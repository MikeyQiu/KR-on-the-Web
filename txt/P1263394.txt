8
1
0
2
 
y
a
M
 
7
 
 
]

V
C
.
s
c
[
 
 
2
v
7
7
1
2
0
.
2
1
6
1
:
v
i
X
r
a

Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring

Seungjun Nah

Tae Hyun Kim

Kyoung Mu Lee

Department of ECE, ASRI, Seoul National University, 08826, Seoul, Korea
{seungjun.nah, lliger9}@gmail.com, kyoungmu@snu.ac.kr

Abstract

blur model [28, 10, 13, 11].

Non-uniform blind deblurring for general dynamic
scenes is a challenging computer vision problem as blurs
arise not only from multiple object motions but also from
camera shake, scene depth variation. To remove these
complicated motion blurs, conventional energy optimiza-
tion based methods rely on simple assumptions such that
blur kernel is partially uniform or locally linear. More-
over, recent machine learning based methods also depend
on synthetic blur datasets generated under these assump-
tions. This makes conventional deblurring methods fail to
remove blurs where blur kernel is difﬁcult to approximate or
parameterize (e.g. object motion boundaries). In this work,
we propose a multi-scale convolutional neural network that
restores sharp images in an end-to-end manner where blur
is caused by various sources. Together, we present multi-
scale loss function that mimics conventional coarse-to-ﬁne
approaches. Furthermore, we propose a new large-scale
dataset that provides pairs of realistic blurry image and the
corresponding ground truth sharp image that are obtained
by a high-speed camera. With the proposed model trained
on this dataset, we demonstrate empirically that our method
achieves the state-of-the-art performance in dynamic scene
deblurring not only qualitatively, but also quantitatively.

1. Introduction

Motion blur is one of the most commonly arising types
of artifacts when taking photos. Shakes of camera and fast
object motions degrade image quality to undesired blurry
images. Furthermore, various causes such as depth varia-
tion, occlusion in motion boundaries make blurs even more
complex. Single image deblurring problem is to estimate
the unknown sharp image given a blurry image. Earlier
studies focused on removing blurs caused by simple transla-
tional or rotational camera motions. More recent works try
to handle general non-uniform blurs caused by depth vari-
ation, camera shakes and object motions in dynamic envi-
ronments. Most of these approaches are based on following

B = KS + n,

(1)

where B, S and n are vectorized blurry image, latent
sharp image, and noise, respectively. K is a large sparse
matrix whose rows each contain a local blur kernel acting
on S to generate a blurry pixel. In practice, blur kernel is
unknown. Thus, blind deblurring methods try to estimate
latent sharp image S and blur kernel K simultaneously.

Finding blur kernel for every pixel is a severely ill-posed
problem. Thus, some approaches tried to parametrize blur
models with simple assumptions on the sources of blurs. In
[28, 10], they assumed that blur is caused by 3D camera
motion only. However, in dynamic scenes, the kernel es-
timation is more challenging as there are multiple moving
objects as well as camera motion. Thus, Kim et al. [14] pro-
posed a dynamic scene deblurring method that jointly seg-
ments and deblurs a non-uniformly blurred image, allowing
the estimation of complex (non-linear) kernel within a seg-
ment. In addition, Kim and Lee [15] approximated the blur
kernel to be locally linear and proposed an approach that es-
timates both the latent image and the locally linear motions
jointly. However, these blur kernel approximations are still
inaccurate, especially in the cases of abrupt motion discon-
tinuities and occlusions. Note that such erroneous kernel
estimation directly affects the quality of the latent image,
resulting in undesired ringing artifacts.

Recently, CNNs (Convolutional Neural Networks) have
been applied in numerous computer vision problems in-
cluding deblurring problem and showed promising results
[29, 25, 26, 1]. Since no pairs of real blurry image and
ground truth sharp image are available for supervised learn-
ing, they commonly used blurry images generated by con-
volving synthetic blur kernels. In [29, 25, 1], synthesized
blur images with uniform blur kernel are used for training.
And, in [26], classiﬁcation CNN is trained to estimate lo-
cally linear blur kernels. Thus, CNN-based models are still
suited only to some speciﬁc types of blurs, and there are
restrictions on more common spatially varying blurs.

1

(a)

(b)

(c)

Figure 1. (a) Input blurry image. (b) Result of Sun et al. [26]. (c) Our deblurring result. Our results show clear object boundaries without
artifacts.

Therefore, all the existing methods still have many prob-
lems before they could be generalized and used in practice.
These are mainly due to the use of simple and unrealis-
tic blur kernel models. Thus, to solve those problems, in
this work, we propose a novel end-to-end deep learning ap-
proach for dynamic scene deblurring.

First, we propose a multi-scale CNN that directly re-
stores latent images without assuming any restricted blur
kernel model. Especially, the multi-scale architecture is
designed to mimic conventional coarse-to-ﬁne optimization
methods. Unlike other approaches, our method does not es-
timate explicit blur kernels. Accordingly, our method is free
from artifacts that arise from kernel estimation errors. Sec-
ond, we train the proposed model with a multi-scale loss
that is appropriate for coarse-to-ﬁne architecture that en-
hances convergence greatly. In addition, we further improve
the results by employing adversarial loss [9]. Third, we pro-
pose a new realistic blurry image dataset with ground truth
sharp images. To obtain kernel model-free dataset for train-
ing, we employ the dataset acquisition method introduced
in [17]. As the blurring process can be modeled by the in-
tegration of sharp images during shutter time [17, 21, 16],
we captured a sequence of sharp frames of a dynamic scene
with a high-speed camera and averaged them to generate a
blurry image by considering gamma correction.

By training with the proposed dataset and adding proper
augmentation, our model can handle general local blur ker-
nel implicitly. As the loss term optimizes the result to
resemble the ground truth, it even restores occluded re-
gions where blur kernel is extremely complex as shown in
Fig. 1. We trained our model with millions of pairs of image
patches and achieved signiﬁcant improvements in dynamic
scene deblurring. Extensive experimental results demon-
strate that the performance of the proposed method is far

superior to those of the state-of-the-art dynamic scene de-
blurring methods in both qualitative and quantitative evalu-
ations.

1.1. Related Works

There are several approaches that employed CNNs for

deblurring [29, 26, 25, 1].

Xu et al. [29] proposed an image deconvolution CNN to
deblur a blurry image in a non-blind setting. They built a
network based on the separable kernel property that the (in-
verse) blur kernel can be decomposed into a small number
of signiﬁcant ﬁlters. Additionally, they incorporated the de-
noising network [7] to reduce visual artifacts such as noise
and color saturation by concatenating the module at the end
of their proposed network.

On the other hand, Schuler et al. [25] proposed a blind
deblurring method with CNN. Their proposed network
mimics conventional optimization-based deblurring meth-
ods and iterates the feature extraction, kernel estimation,
and the latent image estimation steps in a coarse-to-ﬁne
manner. To obtain pairs of sharp and blurry images for net-
work training, they generated uniform blur kernels using a
Gaussian process and synthesized lots of blurry images by
convolving them to the sharp images collected from the Im-
ageNet dataset [3]. However, they reported performance
limits for large blurs due to their suboptimal architecture.

Similarly to the work of Couzinie-Devy et al. [2], Sun
et al. [26] proposed a sequential deblurring approach. First,
they generated pairs of blurry and sharp patches with 73
candidate blur kernels. Next, they trained classiﬁcation
CNN to measure the likelihood of a speciﬁc blur kernel of
a local patch. And then smoothly varying blur kernel is ob-
tained by optimizing an energy model that is composed of
the CNN likelihoods and smoothness priors. Final latent

image estimation is performed with conventional optimiza-
tion method [30].

Note that all these methods require an accurate kernel
estimation step for restoring the latent sharp image. In con-
trast, our proposed model is learned to produce the latent
image directly without estimating blur kernels.

In other computer vision tasks, several forms of coarse-
to-ﬁne architecture or multi-scale architecture were ap-
plied [8, 6, 4, 23, 5]. However, not all multi-scale CNNs
are designed to produce optimal results, similarly to [25].
In depth estimation, optical ﬂow estimation, etc., networks
usually produce outputs having smaller resolution com-
pared to input image resolution [8, 6, 5]. These methods
have difﬁculties in handling long-range dependency even if
multi-scale architecture is used.

Therefore, we make a multi-scale architecture that pre-
serves ﬁne-grained detail information as well as long-range
dependency from coarser scales. Furthermore, we make
sure intermediate level networks help the ﬁnal stage in an
explicit way by training network with multi-scale losses.

1.2. Kernel Free Learning for Dynamic Scene De 

blurring

Conventionally, it was essential to ﬁnd blur kernel before
estimating latent image. CNN based methods were no ex-
ception [25, 26]. However, estimating kernel involves sev-
eral problems. First, assuming simple kernel convolution
cannot model several challenging cases such as occluded re-
gions or depth variations. Second, kernel estimation process
is subtle and sensitive to noise and saturation, unless blur
model is carefully designed. Furthermore, incorrectly esti-
mated kernels give rise to artifacts in latent images. Third,
ﬁnding spatially varying kernel for every pixel in dynamic
scene requires a huge amount of memory and computation.
Therefore, we adopt kernel-free methods in both blur
dataset generation and latent image estimation. In blurry
image generation, we follow to approximate camera imag-
ing process, rather than assuming speciﬁc motions, instead
of ﬁnding or designing complex blur kernel. We capture
successive sharp frames and integrate to simulate blurring
process. The detailed procedure is described in section 2.
Note that our dataset is composed of blurry and sharp image
pairs only, and that the local kernel information is implic-
itly embedded in it. In Fig. 2, our kernel-free blurry image is
compared with a conventional synthesized image with uni-
form blur kernel. Notably, the blur image generated by our
method exhibits realistic and spatially varying blurs caused
by the moving person and the static background, while the
blur image synthesized by conventional method does not.
For latent image estimation, we do not assume blur sources
and train the model solely on our blurry and sharp image
pairs. Thus, our proposed method does not suffer from
kernel-related problems in deblurring.

2. Blur Dataset

Instead of modeling a kernel to convolve on a sharp im-
age, we choose to record the sharp information to be inte-
grated over time for blur image generation. As camera sen-
sor receives light during the exposure, sharp image stimu-
lation at every time is accumulated, generating blurry im-
age [13]. The integrated signal is then transformed into
pixel value by nonlinear CRF (Camera Response Function).
Thus, the process could be approximated by accumulating
signals from high-speed video frames.

Blur accumulation process can be modeled as follows.

B = g

S(t)dt

≃ g

1
T

 

T

t=0

Z

1
M

 

M−1

i=0
X

!

S[i]

,

(2)

!

where T and S(t) denote the exposure time and the sen-
sor signal of a sharp image at time t, respectively. Simi-
larly, M , S[i] are the number of sampled frames and the
i-th sharp frame signal captured during the exposure time,
respectively. g is the CRF that maps a sharp latent signal
S(t) into an observed image ˆS(t) such that ˆS(t) = g(S(t)),
or ˆS[i] = g(S[i]). In practice, we only have observed video
frames while the original signal and the CRF is unknown.

It is known that non-uniform deblurring becomes signif-
icantly difﬁcult when nonlinear CRF is involved, and non-
linearity should be taken into account. However, currently,
there are no CRF estimation techniques available for an im-
age with spatially varying blur [27]. When the ground truth
CRF is not given, a common practical method is to approxi-
mate CRF as a gamma curve with γ = 2.2 as follows, since
it is known as an aproximated average of known CRFs [27].

g(x) = x1/γ.

(3)

Thus, by correcting the gamma function, we obtain the
latent frame signal S[i] from the observed image ˆS[i] by
S[i] = g−1( ˆS[i]), and then synthesize the corresponding
blur image B by using (2).

We used GOPRO4 Hero Black camera to generate our
dataset. We took 240 fps videos with GOPRO camera and
then averaged varying number (7 - 13) of successive latent
frames to produce blurs of different strengths. For example,
averaging 15 frames simulates a photo taken at 1/16 shut-
ter speed, while corresponding sharp image shutter speed
is 1/240. Notably, the sharp latent image corresponding to
each blurry one is deﬁned as the mid-frame among the sharp
frames that are used to make the blurry image. Finally, our
dataset is composed of 3214 pairs of blurry and sharp im-
ages at 1280x720 resolution. The proposed GOPRO dataset
is publicly available on our website 1.

1https://github.com/SeungjunNah/DeepDeblur_release

(a)

(b)

(c)

Figure 2. (a) Ground truth sharp image. (b) Blurry image generated by convolving a uniform blur kernel. (c) Blurry image by averaging
sharp frames. In this case, blur is mostly caused by person motion, leaving the background as it is. The blur kernel is non-uniform, complex
shaped. However, when the blurry image is synthesized by convolution with a uniform kernel, the background also gets blurred as if blur
was caused by camera shake. To model dynamic scene blur, our kernel-free method is required.

INPUT

CONV

BN

ReLU

CONV

BN

ReLU

OUTPUT

(a)

3. Proposed Method

In our model, ﬁner scale image deblurring is aided by
coarser scale features. To exploit coarse and middle level
information while preserving ﬁne level information at the
same time, input and output to our network take the form of
Gaussian pyramids. Note that most of other coarse-to-ﬁne
networks take a single image as input and output.

3.1. Model Architecture

In addition to the multi-scale architecture, we employ a
slightly modiﬁed version of residual network structure [12]
as a building block of our model. Using residual network
structure enables deeper architecture compared to a plain
CNN. Also, as blurry and sharp image pairs are similar in
values, it is efﬁcient to let parameters learn the difference
only. We found that removing the rectiﬁed linear unit af-
ter the shortcut connection of the original residual building
block boosts the convergence speed at training time. We de-
note the modiﬁed building block as ResBlock. The original
and our modiﬁed building block are compared in Fig. 3.

By stacking enough number of convolution layers with
ResBlocks, the receptive ﬁeld at each scale is expanded.
Details are described in the following paragraphs. For sake
of consistency, we deﬁne scale levels in the order of de-
creasing resolution (i.e.
level 1 for ﬁnest scale). Unless
denoted otherwise, we use total K = 3 scales. At training
time, we set the resolution of the input and output Gaussian
pyramid patches to be {256 × 256, 128 × 128, 64 × 64}.
The scale ratio between consecutive scales is 0.5. For all
convolution layers, we set the ﬁlter size to be 5 × 5. As our
model is fully convolutional, at test time, the patch size may
vary as the GPU memory allows. The overall architecture
is shown in Fig. 4.

INPUT

CONV

ReLU

CONV

OUTPUT

(b)

Figure 3. (a) Original residual network building block. (b) Mod-
iﬁed building block of our network. We did not use batch nor-
malization layers since we trained model with mini-batch of size
2, which is smaller than usual for batch normalization. We found
removing rectiﬁed linear unit just before the block output is bene-
ﬁcial in terms of performance empirically.

(cid:1776)(cid:2778)

(cid:1776)(cid:2779)

(cid:1776)(cid:2780)

CONV

ResBlock

ResBlock

CONV

CONV

ResBlock

ResBlock

CONV

. . .

Up
Conv

. . .

Up
Conv

. . .

Backprop

(cid:1786)(cid:2778)

(cid:1786)(cid:2779)

(cid:1793)(cid:2778)

(cid:1793)(cid:2779)

Backprop

Backprop

CONV

ResBlock

ResBlock

CONV

Figure 4. Multi-scale network architecture. Bk, Lk, Sk denote blurry and latent, and ground truth sharp images, respectively. Subscript
k denotes k-th scale level in the Gaussian pyramid, which is downsampled to 1/2k scale. Our model takes a blurry image pyramid as the
input and outputs an estimated latent image pyramid. Every intermediate scale output is trained to be sharp. At test time, original scale
image is chosen as the ﬁnal result.

(cid:1786)(cid:2780)

(cid:1793)(cid:2780)

Coarsest level network

Finer level network

At the front of the network locates the coarsest level net-
work. The ﬁrst convolution layer transforms 1/4 resolu-
tion, 64 × 64 size image into 64 feature maps. Then, 19
ResBlocks are stacked followed by last convolution layer
that transforms the feature map into input dimension. Every
convolution layer preserves resolution with zero padding. In
total, there are 40 convolution layers. The number of con-
volution layers at each scale level is determined so that total
model should have 120 convolution layers. Thus, the coars-
est level network has receptive ﬁeld large enough to cover
the whole patch. At the end of the stage, the coarsest level
latent sharp image is generated. Moreover, information
from the coarsest level output is delivered to the next stage
where ﬁner scale network is. To convert a coarsest output
to ﬁt the input size of the next ﬁner scale, the output patch
passes an upconvolution [22] layer, while other multi-scale
methods use reshaping [8] or upsampling [4, 6, 23]. Since
the sharp and blurry patches share low-frequency informa-
tion, learning suitable feature with upconvolution helps to
remove redundancy. In our experiment, using upconvolu-
tion showed better performance than upsampling. Then, the
upconvolution feature is concatenated with the ﬁner scale
blurry patch as an input.

Finer level networks basically have the same structure as in
the coarsest level network. However, the ﬁrst convolution
layer takes the sharp feature from the previous stage as well
as its own blurry input image, in a concatenated form. Every
convolution ﬁlter size is 5 × 5 with the same number of
feature maps as in the coarsest level. Except for the last
ﬁnest scale, there is an upconvolution layer before the next
stage. At the ﬁnest scale, the original resolution sharp image
is restored.

3.2. Training

Our model is trained on the proposed GOPRO dataset.
Among 3214 pairs, 2103 pairs were used for training and
remainings were used for the test. To prevent our network
from overﬁtting, several data augmentation techniques are
involved. In terms of geometric transformations, patches
are randomly ﬂipped horizontally and vertically, rotated by
90 degrees. For color, RGB channels are randomly per-
muted. To take image degradations into account, satura-
tion in HSV colorspace is multiplied by a random num-
ber within [0.5, 1.5]. Also, Gaussian random noise is
added to blurry images. To make our network be robust
against different strengths of noise, standard deviation of

noise is also randomly sampled from Gaussian distribution,
N (0, (2/255)2). Then, value outside [0, 1] is clipped. Fi-
nally, 0.5 is subtracted to set input and output value range
zero-centered, having range [-0.5, 0.5].

In optimizing the network parameters, we trained the
model in a combination of two losses, multi-scale content
loss and adversarial loss.

Multi-scale content loss

Basically, the coarse-to-ﬁne approach desires that every in-
termediate output becomes the sharp image of the corre-
sponding scale. Thus, we train our network so that inter-
mediate outputs should form a Gaussian pyramid of sharp
images. MSE criterion is applied to every level of pyramids.
Hence, the loss function is deﬁned as follows:

Lcont =

kLk − Skk2,

(4)

1
2K

K

Xk=1

1
ckwkhk

where Lk, Sk denote the model output and ground truth im-
age at scale level k, respectively. The loss at each scale is
normalized by the number of channels ck, width wk, and
the height hk (i.e. the total number of elements).

Adversarial loss

Recently, adversarial networks are reported to generate
sharp realistic images [9, 4, 24]. Following the architec-
ture introduced in [24], we build discriminator as in Ta-
ble 1. Discriminator takes the output of the ﬁnest scale or
the ground truth sharp image as input and classiﬁes if it is
deblurred image or sharp image.

The adversarial loss is deﬁned as follows.

Ladv =

[log D(S)]+

E
S∼psharp(S)
E
B∼pblurry (B)

[log(1 − D(G(B)))],

(5)

where G and D denote the generator, that is our multi-
scale deblurring network in Fig. 4 and the discriminator
(classiﬁer), respectively. When training, G tries to minimize
the adversarial loss while D tries to maximize it.

Finally, by combining the multi-scale content loss and
adversarial loss, the generator network and discriminator
network is jointly trained. Thus, our ﬁnal loss term is

#

1

2

3

4

5

6

7

8

9

10

11

Layer Weight dimension

Stride

conv

conv

conv

conv

conv

conv

conv

conv

conv

fc

32 × 3 × 5 × 5

64 × 32 × 5 × 5

64 × 64 × 5 × 5

128 × 64 × 5 × 5

128 × 128 × 5 × 5

256 × 128 × 5 × 5

256 × 256 × 5 × 5

512 × 256 × 5 × 5

512 × 512 × 4 × 4

512 × 1 × 1 × 1

sigmoid

-

2

1

2

1

4

1

4

1

4

-

-

Table 1. Model parameters of the discriminator. Every convolution
layers are activated with LeakyReLU layer.

rate is decreased to 1/10 of the previous learning rate. Total
training takes 9 × 105 iterations to converge.

4. Experimental Results

We implemented our model with torch7 library. All the
following experiments were performed in a desktop with i7-
6700K CPU and NVIDIA GTX Titan X (Maxwell) GPU.

4.1. GOPRO Dataset

We evaluate the performance of our model in the pro-
posed GOPRO dataset. Our test dataset consists of 1111
pairs, which is approximately 1/3 of the total dataset. We
compare the results with those of the state-of-the-art meth-
ods [15, 26] in both qualitative and quantitative ways. Our
results show signiﬁcant improvement in terms of image
quality. Some deblurring results are shown in Fig. 5. We no-
tice from the results of Sun et al. [26], deblurring is not suc-
cessful on the regions where blurs are nonlinearly shaped or
located at the boundary of motion. Kim and Lee [15]’s re-
sults also fail in cases where strong edges are not found. In
contrast, our results are free from those kernel-estimation
related problems. Table 2, shows the quantitative evalua-
tion results of the competing methods and ours with differ-
ent scale level k in terms of PSNR, SSIM over the test data.
Also, the runtime is compared. We observe that our system
with K = 2 produces the best results in terms of both PSNR
and SSIM, while K = 3 is the fastest.

Ltotal = Lcont + λ × Ladv,
where the weight constant λ = 1 × 10−4.
We used ADAM [18] optimizer with a mini-batch size
2 for training. The learning rate is adaptively tuned begin-
ning from 5 × 10−5. After 3 × 105 iterations, the learning

(6)

4.2. K¨ohler Dataset

K¨ohler dataset [19] consists of 4 latent images and 12
differently blurred images for each of them. The blurs are
caused by replaying recorded 6D camera motion, assum-
ing linear CRF. We report the quantitative results on this

Figure 5. Test results on the GOPRO dataset. From top to bottom: Blurry images, results of Sun et al. [26], results of Kim and Lee [15],
and results of the proposed method.

Figure 6. Deblurring results on the dataset [20]. The top row shows results of results of Sun et al. [26] and the bottom row shows our
results.

Ours

4.3. Dataset of Lai et al.

Measure

[26]

[15]

K = 1 K = 2 K = 3

PSNR

SSIM

24.64

23.64

28.93

29.23

29.08

0.8429

0.8239

0.9100

0.9162

0.9135

Runtime

20 min

1 hr

7.21 s

4.33 s

3.09 s

Table 2. Quantitative deblurring performance comparison on the
GOPRO dataset. K denotes the scale level.

dataset in Table 3. Our model is trained by setting g as iden-
tity function in (2). We note that our system with K = 3
produces the best results in PSNR, and the system K = 2
exhibits the best MSSIM result.

Measure

[26]

[15]

Ours

K = 1 K = 2 K = 3

PSNR

25.22

24.68

25.74

26.02

26.48

MSSIM 0.7735

0.7937

0.8042

0.8116

0.8079

Lai et al.

[20] generated synthetic dataset by convolv-
ing nonuniform blur kernels and imposing several common
degradations. They also recorded 6D camera trajectories
to generate blur kernels. However, their blurry images and
sharp images are not aligned in the way of our dataset, mak-
ing simple image quality measures such as PSNR and SSIM
less correlated with perceptual quality. Thus, we show qual-
itative comparisons in Fig. 6. Clearly, our results avoid ring-
ing artifacts while preserving details such as wave ripple.

5. Conclusion

In this paper, we proposed a blind deblurring neural net-
work for sharp image estimation. Unlike previous stud-
ies, our model avoids problems related to kernel estimation.
The proposed model follows a coarse-to-ﬁne approach and
is trained in multi-scale space. We also constructed a re-
alistic ground-truth blur dataset, enabling efﬁcient super-
vised learning and rigorous evaluation. Experimental re-
sults show that our approach outperforms the state-of-the-
art methods in both qualitative and quantitative ways while
being much faster.

This project is partially funded by Microsoft Research

Asia.

Table 3. Quantitative comparison on the K¨ohler dataset. The
dataset has its own evaluation code, thus we report multi-scale
SSIM instead of SSIM.

Acknowledgement

[19] R. K¨ohler, M. Hirsch, B. Mohler, B. Sch¨olkopf, and
S. Harmeling.
Recording and playback of camera
shake: Benchmarking blind deconvolution with a real-world
database. In ECCV, pages 27–40. Springer, 2012. 6

[20] W.-S. Lai, J.-B. Huang, Z. Hu, N. Ahuja, and M.-H. Yang. A
comparative study for single image blind deblurring. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 1701–1709, 2016. 8, 16

[21] Y. Li, S. B. Kang, N. Joshi, S. M. Seitz, and D. P. Hutten-
locher. Generating sharp panoramas from motion-blurred
videos. In CVPR, 2010. 2

[22] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In CVPR, pages 3431–
3440, 2015. 5

[23] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale
video prediction beyond mean square error. arXiv preprint
arXiv:1511.05440, 2015. 3, 5

[24] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-
sentation learning with deep convolutional generative adver-
sarial networks. arXiv preprint arXiv:1511.06434, 2015. 6

[25] C. J. Schuler, M. Hirsch, S. Harmeling, and B. Sch¨olkopf.
IEEE transactions on pattern analysis

Learning to deblur.
and machine intelligence, 38(7):1439–1451, 2016. 1, 2, 3

[26] J. Sun, W. Cao, Z. Xu, and J. Ponce. Learning a convolu-
tional neural network for non-uniform motion blur removal.
In CVPR, pages 769–777. IEEE, 2015. 1, 2, 3, 6, 7, 8, 13, 19
[27] Y.-W. Tai, X. Chen, S. Kim, S. J. Kim, F. Li, J. Yang, J. Yu,
Y. Matsushita, and M. S. Brown. Nonlinear camera response
functions and image deblurring: Theoretical analysis and
practice. PAMI, 35(10):2498–2512, 2013. 3

[28] O. Whyte, J. Sivic, A. Zisserman, and J. Ponce. Non-uniform

deblurring for shaken images. 2010. 1

[29] L. Xu, J. S. Ren, C. Liu, and J. Jia. Deep convolutional neural
In Advances in Neural
network for image deconvolution.
Information Processing Systems, pages 1790–1798, 2014. 1,
2

[30] D. Zoran and Y. Weiss. From learning models of natural
image patches to whole image restoration. In ICCV, pages
479–486. IEEE, 2011. 3

References

[1] A. Chakrabarti. A neural approach to blind motion deblur-

ring. In ECCV, 2016. 1, 2

[2] F. Couzinie-Devy, J. Sun, K. Alahari, and J. Ponce. Learning
to estimate and remove non-uniform image blur. In CVPR,
2013. 2

[3] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, pages 248–255. IEEE, 2009. 2

[4] E. L. Denton, S. Chintala, R. Fergus, et al. Deep genera-
tive image models using a laplacian pyramid of adversarial
In Advances in Neural Information Processing
networks.
Systems, pages 1486–1494, 2015. 3, 5, 6

[5] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas,
V. Golkov, P. van der Smagt, D. Cremers, and T. Brox.
Flownet: Learning optical ﬂow with convolutional networks.
In CVPR, pages 2758–2766, 2015. 3

[6] D. Eigen and R. Fergus. Predicting depth, surface normals
and semantic labels with a common multi-scale convolu-
In ICCV, pages 2650–2658, 2015. 3,
tional architecture.
5

[7] D. Eigen, D. Krishnan, and R. Fergus. Restoring an image
taken through a window covered with dirt or rain. In ICCV,
pages 633–640, 2013. 2

[8] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction
from a single image using a multi-scale deep network.
In
Advances in Neural Information Ppocessing Ssytems, pages
2366–2374, 2014. 3, 5

[9] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in Neural Information
Processing Systems, pages 2672–2680, 2014. 2, 6

[10] A. Gupta, N. Joshi, C. L. Zitnick, M. Cohen, and B. Curless.
Single image deblurring using motion density functions. In
ECCV, pages 171–184. Springer, 2010. 1

[11] S. Harmeling, H. Michael, and B. Sch¨olkopf.

Space-
variant single-image blind deconvolution for removing cam-
In Advances in Neural Information Processing
era shake.
Systems, pages 829–837, 2010. 1

[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, pages 770–778, 2016. 4

[13] M. Hirsch, C. J. Schuler, S. Harmeling, and B. Sch¨olkopf.
Fast removal of non-uniform camera shake. In ICCV, 2011.
1, 3

[14] T. H. Kim, B. Ahn, and K. M. Lee. Dynamic scene deblur-

ring. In ICCV, 2013. 1

[15] T. H. Kim and K. M. Lee. Segmentation-free dynamic scene

deblurring. In CVPR, 2014. 1, 6, 7, 8, 13, 19

[16] T. H. Kim and K. M. Lee. Generalized video deblurring for

dynamic scenes. In CVPR, 2015. 2

[17] T. H. Kim, S. Nah, and K. M. Lee. Dynamic scene deblurring
using a locally adaptive linear blur model. arXiv preprint
arXiv:1603.04265, 2016. 2

[18] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. arXiv preprint arXiv:1412.6980, 2014. 6

In this appendix, we present more comparative experimental results to demonstrate the effectiveness of our proposed

A. Appendix

deblurring method.

A.1. Comparison of loss function

In section 3.2, we employed a loss function that combines both the multi-scale content loss (MSE) and the adversarial loss
for training our network. We examine the effect of the adversarial loss term quantitatively and qualitatively. The PSNR and
SSIM results are shown in table A.1. From this results, we observe that adding adversarial loss does not increases PSNR, but
increase SSIM, which means that it encourages to generate more natural and structure preserving images.

Table A.1. Quantitative deblurring performance comparison of loss used to optimize our model (K = 3, λ = 1 × 10−4). Evaluated on
the GOPRO test dataset assuming linear CRF.

Loss

PSNR

SSIM

Lcont(M SE) Lcont + λLadv

28.62

0.9094

28.45

0.9170

Fig. A.1 and A.2 show some qualitative comparisons between the results of our network trained with Lcont and Lcont +

λLadv.

Blurry image

Deblurred image (MSE)

Deblurred image (MSE + Adversarial)

Figure A.1. Visual comparison of results from our model trained with different loss functions. The blurry image is from our proposed
dataset.

Blurry image

Deblurred image (MSE)

Deblurred image (MSE + Adversarial)

Figure A.2. Visual comparison of results from our model trained with different loss functions. The blurry image is from our proposed
dataset.

A.2. Comparison on GOPRO dataset

Lee [15], Sun et al. [26], and ours.

We provide qualitative results on our GOPRO test dataset. Fig. A.3 and A.4 shows the deblurring results of Kim and

Blurry image

Kim and Lee [15]

Sun et al. [27]

Ours

Figure A.3. Visual comparison with other methods. The blurry image is from our proposed dataset.

Blurry image

Kim and Lee [15]

Sun et al. [27]

Ours

Figure A.4. Visual comparison with other methods. The blurry image is from our proposed dataset.

A.3. Comparison on Lai et al. [20] dataset

We provide qualitative results on the dataset of Lai et al. [?]. The Lai et al. dataset is composed of synthetic and real
blurry images, and we showed the deblurring result of a synthetically generated blurry image in section 4.3. We present the
qualitative deblurring results of competing methods on real images in Fig. A.5 and A.6.

Blurry image

Kim and Lee [15]

Sun et al. [27]

Ours

Figure A.5. Visual comparison with other methods. The blurry image is a real image in Lai et al. [?]

Blurry image

Kim and Lee [15]

Sun et al. [27]

Ours

Figure A.6. Visual comparison with other methods. The blurry image is a real image in Lai et al. [?]

A.4. Comparison on real dynamic scenes

Finally, we further present deblurring results on real dynamic scenes. The blurry scenes are captured by a SONY RX100
M4 camera. The qualitative deblurring results of Kim and Lee [15], Sun et al. [26] and ours are compared in Fig. A.7 and
A.8.

Blurry image

Kim and Lee [15]

Sun et al. [27]

Ours

Figure A.7. Visual comparison with other methods. The blurry image is a real dynamic scene.

Blurry image

Kim and Lee [15]

Sun et al. [27]

Ours

Figure A.8. Visual comparison with other methods. The blurry image is a real dynamic scene.

8
1
0
2
 
y
a
M
 
7
 
 
]

V
C
.
s
c
[
 
 
2
v
7
7
1
2
0
.
2
1
6
1
:
v
i
X
r
a

Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring

Seungjun Nah

Tae Hyun Kim

Kyoung Mu Lee

Department of ECE, ASRI, Seoul National University, 08826, Seoul, Korea
{seungjun.nah, lliger9}@gmail.com, kyoungmu@snu.ac.kr

Abstract

blur model [28, 10, 13, 11].

Non-uniform blind deblurring for general dynamic
scenes is a challenging computer vision problem as blurs
arise not only from multiple object motions but also from
camera shake, scene depth variation. To remove these
complicated motion blurs, conventional energy optimiza-
tion based methods rely on simple assumptions such that
blur kernel is partially uniform or locally linear. More-
over, recent machine learning based methods also depend
on synthetic blur datasets generated under these assump-
tions. This makes conventional deblurring methods fail to
remove blurs where blur kernel is difﬁcult to approximate or
parameterize (e.g. object motion boundaries). In this work,
we propose a multi-scale convolutional neural network that
restores sharp images in an end-to-end manner where blur
is caused by various sources. Together, we present multi-
scale loss function that mimics conventional coarse-to-ﬁne
approaches. Furthermore, we propose a new large-scale
dataset that provides pairs of realistic blurry image and the
corresponding ground truth sharp image that are obtained
by a high-speed camera. With the proposed model trained
on this dataset, we demonstrate empirically that our method
achieves the state-of-the-art performance in dynamic scene
deblurring not only qualitatively, but also quantitatively.

1. Introduction

Motion blur is one of the most commonly arising types
of artifacts when taking photos. Shakes of camera and fast
object motions degrade image quality to undesired blurry
images. Furthermore, various causes such as depth varia-
tion, occlusion in motion boundaries make blurs even more
complex. Single image deblurring problem is to estimate
the unknown sharp image given a blurry image. Earlier
studies focused on removing blurs caused by simple transla-
tional or rotational camera motions. More recent works try
to handle general non-uniform blurs caused by depth vari-
ation, camera shakes and object motions in dynamic envi-
ronments. Most of these approaches are based on following

B = KS + n,

(1)

where B, S and n are vectorized blurry image, latent
sharp image, and noise, respectively. K is a large sparse
matrix whose rows each contain a local blur kernel acting
on S to generate a blurry pixel. In practice, blur kernel is
unknown. Thus, blind deblurring methods try to estimate
latent sharp image S and blur kernel K simultaneously.

Finding blur kernel for every pixel is a severely ill-posed
problem. Thus, some approaches tried to parametrize blur
models with simple assumptions on the sources of blurs. In
[28, 10], they assumed that blur is caused by 3D camera
motion only. However, in dynamic scenes, the kernel es-
timation is more challenging as there are multiple moving
objects as well as camera motion. Thus, Kim et al. [14] pro-
posed a dynamic scene deblurring method that jointly seg-
ments and deblurs a non-uniformly blurred image, allowing
the estimation of complex (non-linear) kernel within a seg-
ment. In addition, Kim and Lee [15] approximated the blur
kernel to be locally linear and proposed an approach that es-
timates both the latent image and the locally linear motions
jointly. However, these blur kernel approximations are still
inaccurate, especially in the cases of abrupt motion discon-
tinuities and occlusions. Note that such erroneous kernel
estimation directly affects the quality of the latent image,
resulting in undesired ringing artifacts.

Recently, CNNs (Convolutional Neural Networks) have
been applied in numerous computer vision problems in-
cluding deblurring problem and showed promising results
[29, 25, 26, 1]. Since no pairs of real blurry image and
ground truth sharp image are available for supervised learn-
ing, they commonly used blurry images generated by con-
volving synthetic blur kernels. In [29, 25, 1], synthesized
blur images with uniform blur kernel are used for training.
And, in [26], classiﬁcation CNN is trained to estimate lo-
cally linear blur kernels. Thus, CNN-based models are still
suited only to some speciﬁc types of blurs, and there are
restrictions on more common spatially varying blurs.

1

(a)

(b)

(c)

Figure 1. (a) Input blurry image. (b) Result of Sun et al. [26]. (c) Our deblurring result. Our results show clear object boundaries without
artifacts.

Therefore, all the existing methods still have many prob-
lems before they could be generalized and used in practice.
These are mainly due to the use of simple and unrealis-
tic blur kernel models. Thus, to solve those problems, in
this work, we propose a novel end-to-end deep learning ap-
proach for dynamic scene deblurring.

First, we propose a multi-scale CNN that directly re-
stores latent images without assuming any restricted blur
kernel model. Especially, the multi-scale architecture is
designed to mimic conventional coarse-to-ﬁne optimization
methods. Unlike other approaches, our method does not es-
timate explicit blur kernels. Accordingly, our method is free
from artifacts that arise from kernel estimation errors. Sec-
ond, we train the proposed model with a multi-scale loss
that is appropriate for coarse-to-ﬁne architecture that en-
hances convergence greatly. In addition, we further improve
the results by employing adversarial loss [9]. Third, we pro-
pose a new realistic blurry image dataset with ground truth
sharp images. To obtain kernel model-free dataset for train-
ing, we employ the dataset acquisition method introduced
in [17]. As the blurring process can be modeled by the in-
tegration of sharp images during shutter time [17, 21, 16],
we captured a sequence of sharp frames of a dynamic scene
with a high-speed camera and averaged them to generate a
blurry image by considering gamma correction.

By training with the proposed dataset and adding proper
augmentation, our model can handle general local blur ker-
nel implicitly. As the loss term optimizes the result to
resemble the ground truth, it even restores occluded re-
gions where blur kernel is extremely complex as shown in
Fig. 1. We trained our model with millions of pairs of image
patches and achieved signiﬁcant improvements in dynamic
scene deblurring. Extensive experimental results demon-
strate that the performance of the proposed method is far

superior to those of the state-of-the-art dynamic scene de-
blurring methods in both qualitative and quantitative evalu-
ations.

1.1. Related Works

There are several approaches that employed CNNs for

deblurring [29, 26, 25, 1].

Xu et al. [29] proposed an image deconvolution CNN to
deblur a blurry image in a non-blind setting. They built a
network based on the separable kernel property that the (in-
verse) blur kernel can be decomposed into a small number
of signiﬁcant ﬁlters. Additionally, they incorporated the de-
noising network [7] to reduce visual artifacts such as noise
and color saturation by concatenating the module at the end
of their proposed network.

On the other hand, Schuler et al. [25] proposed a blind
deblurring method with CNN. Their proposed network
mimics conventional optimization-based deblurring meth-
ods and iterates the feature extraction, kernel estimation,
and the latent image estimation steps in a coarse-to-ﬁne
manner. To obtain pairs of sharp and blurry images for net-
work training, they generated uniform blur kernels using a
Gaussian process and synthesized lots of blurry images by
convolving them to the sharp images collected from the Im-
ageNet dataset [3]. However, they reported performance
limits for large blurs due to their suboptimal architecture.

Similarly to the work of Couzinie-Devy et al. [2], Sun
et al. [26] proposed a sequential deblurring approach. First,
they generated pairs of blurry and sharp patches with 73
candidate blur kernels. Next, they trained classiﬁcation
CNN to measure the likelihood of a speciﬁc blur kernel of
a local patch. And then smoothly varying blur kernel is ob-
tained by optimizing an energy model that is composed of
the CNN likelihoods and smoothness priors. Final latent

image estimation is performed with conventional optimiza-
tion method [30].

Note that all these methods require an accurate kernel
estimation step for restoring the latent sharp image. In con-
trast, our proposed model is learned to produce the latent
image directly without estimating blur kernels.

In other computer vision tasks, several forms of coarse-
to-ﬁne architecture or multi-scale architecture were ap-
plied [8, 6, 4, 23, 5]. However, not all multi-scale CNNs
are designed to produce optimal results, similarly to [25].
In depth estimation, optical ﬂow estimation, etc., networks
usually produce outputs having smaller resolution com-
pared to input image resolution [8, 6, 5]. These methods
have difﬁculties in handling long-range dependency even if
multi-scale architecture is used.

Therefore, we make a multi-scale architecture that pre-
serves ﬁne-grained detail information as well as long-range
dependency from coarser scales. Furthermore, we make
sure intermediate level networks help the ﬁnal stage in an
explicit way by training network with multi-scale losses.

1.2. Kernel Free Learning for Dynamic Scene De 

blurring

Conventionally, it was essential to ﬁnd blur kernel before
estimating latent image. CNN based methods were no ex-
ception [25, 26]. However, estimating kernel involves sev-
eral problems. First, assuming simple kernel convolution
cannot model several challenging cases such as occluded re-
gions or depth variations. Second, kernel estimation process
is subtle and sensitive to noise and saturation, unless blur
model is carefully designed. Furthermore, incorrectly esti-
mated kernels give rise to artifacts in latent images. Third,
ﬁnding spatially varying kernel for every pixel in dynamic
scene requires a huge amount of memory and computation.
Therefore, we adopt kernel-free methods in both blur
dataset generation and latent image estimation. In blurry
image generation, we follow to approximate camera imag-
ing process, rather than assuming speciﬁc motions, instead
of ﬁnding or designing complex blur kernel. We capture
successive sharp frames and integrate to simulate blurring
process. The detailed procedure is described in section 2.
Note that our dataset is composed of blurry and sharp image
pairs only, and that the local kernel information is implic-
itly embedded in it. In Fig. 2, our kernel-free blurry image is
compared with a conventional synthesized image with uni-
form blur kernel. Notably, the blur image generated by our
method exhibits realistic and spatially varying blurs caused
by the moving person and the static background, while the
blur image synthesized by conventional method does not.
For latent image estimation, we do not assume blur sources
and train the model solely on our blurry and sharp image
pairs. Thus, our proposed method does not suffer from
kernel-related problems in deblurring.

2. Blur Dataset

Instead of modeling a kernel to convolve on a sharp im-
age, we choose to record the sharp information to be inte-
grated over time for blur image generation. As camera sen-
sor receives light during the exposure, sharp image stimu-
lation at every time is accumulated, generating blurry im-
age [13]. The integrated signal is then transformed into
pixel value by nonlinear CRF (Camera Response Function).
Thus, the process could be approximated by accumulating
signals from high-speed video frames.

Blur accumulation process can be modeled as follows.

B = g

S(t)dt

≃ g

1
T

 

T

t=0

Z

1
M

 

M−1

i=0
X

!

S[i]

,

(2)

!

where T and S(t) denote the exposure time and the sen-
sor signal of a sharp image at time t, respectively. Simi-
larly, M , S[i] are the number of sampled frames and the
i-th sharp frame signal captured during the exposure time,
respectively. g is the CRF that maps a sharp latent signal
S(t) into an observed image ˆS(t) such that ˆS(t) = g(S(t)),
or ˆS[i] = g(S[i]). In practice, we only have observed video
frames while the original signal and the CRF is unknown.

It is known that non-uniform deblurring becomes signif-
icantly difﬁcult when nonlinear CRF is involved, and non-
linearity should be taken into account. However, currently,
there are no CRF estimation techniques available for an im-
age with spatially varying blur [27]. When the ground truth
CRF is not given, a common practical method is to approxi-
mate CRF as a gamma curve with γ = 2.2 as follows, since
it is known as an aproximated average of known CRFs [27].

g(x) = x1/γ.

(3)

Thus, by correcting the gamma function, we obtain the
latent frame signal S[i] from the observed image ˆS[i] by
S[i] = g−1( ˆS[i]), and then synthesize the corresponding
blur image B by using (2).

We used GOPRO4 Hero Black camera to generate our
dataset. We took 240 fps videos with GOPRO camera and
then averaged varying number (7 - 13) of successive latent
frames to produce blurs of different strengths. For example,
averaging 15 frames simulates a photo taken at 1/16 shut-
ter speed, while corresponding sharp image shutter speed
is 1/240. Notably, the sharp latent image corresponding to
each blurry one is deﬁned as the mid-frame among the sharp
frames that are used to make the blurry image. Finally, our
dataset is composed of 3214 pairs of blurry and sharp im-
ages at 1280x720 resolution. The proposed GOPRO dataset
is publicly available on our website 1.

1https://github.com/SeungjunNah/DeepDeblur_release

(a)

(b)

(c)

Figure 2. (a) Ground truth sharp image. (b) Blurry image generated by convolving a uniform blur kernel. (c) Blurry image by averaging
sharp frames. In this case, blur is mostly caused by person motion, leaving the background as it is. The blur kernel is non-uniform, complex
shaped. However, when the blurry image is synthesized by convolution with a uniform kernel, the background also gets blurred as if blur
was caused by camera shake. To model dynamic scene blur, our kernel-free method is required.

INPUT

CONV

BN

ReLU

CONV

BN

ReLU

OUTPUT

(a)

3. Proposed Method

In our model, ﬁner scale image deblurring is aided by
coarser scale features. To exploit coarse and middle level
information while preserving ﬁne level information at the
same time, input and output to our network take the form of
Gaussian pyramids. Note that most of other coarse-to-ﬁne
networks take a single image as input and output.

3.1. Model Architecture

In addition to the multi-scale architecture, we employ a
slightly modiﬁed version of residual network structure [12]
as a building block of our model. Using residual network
structure enables deeper architecture compared to a plain
CNN. Also, as blurry and sharp image pairs are similar in
values, it is efﬁcient to let parameters learn the difference
only. We found that removing the rectiﬁed linear unit af-
ter the shortcut connection of the original residual building
block boosts the convergence speed at training time. We de-
note the modiﬁed building block as ResBlock. The original
and our modiﬁed building block are compared in Fig. 3.

By stacking enough number of convolution layers with
ResBlocks, the receptive ﬁeld at each scale is expanded.
Details are described in the following paragraphs. For sake
of consistency, we deﬁne scale levels in the order of de-
creasing resolution (i.e.
level 1 for ﬁnest scale). Unless
denoted otherwise, we use total K = 3 scales. At training
time, we set the resolution of the input and output Gaussian
pyramid patches to be {256 × 256, 128 × 128, 64 × 64}.
The scale ratio between consecutive scales is 0.5. For all
convolution layers, we set the ﬁlter size to be 5 × 5. As our
model is fully convolutional, at test time, the patch size may
vary as the GPU memory allows. The overall architecture
is shown in Fig. 4.

INPUT

CONV

ReLU

CONV

OUTPUT

(b)

Figure 3. (a) Original residual network building block. (b) Mod-
iﬁed building block of our network. We did not use batch nor-
malization layers since we trained model with mini-batch of size
2, which is smaller than usual for batch normalization. We found
removing rectiﬁed linear unit just before the block output is bene-
ﬁcial in terms of performance empirically.

(cid:1776)(cid:2778)

(cid:1776)(cid:2779)

(cid:1776)(cid:2780)

CONV

ResBlock

ResBlock

CONV

CONV

ResBlock

ResBlock

CONV

. . .

Up
Conv

. . .

Up
Conv

. . .

Backprop

(cid:1786)(cid:2778)

(cid:1786)(cid:2779)

(cid:1793)(cid:2778)

(cid:1793)(cid:2779)

Backprop

Backprop

CONV

ResBlock

ResBlock

CONV

Figure 4. Multi-scale network architecture. Bk, Lk, Sk denote blurry and latent, and ground truth sharp images, respectively. Subscript
k denotes k-th scale level in the Gaussian pyramid, which is downsampled to 1/2k scale. Our model takes a blurry image pyramid as the
input and outputs an estimated latent image pyramid. Every intermediate scale output is trained to be sharp. At test time, original scale
image is chosen as the ﬁnal result.

(cid:1786)(cid:2780)

(cid:1793)(cid:2780)

Coarsest level network

Finer level network

At the front of the network locates the coarsest level net-
work. The ﬁrst convolution layer transforms 1/4 resolu-
tion, 64 × 64 size image into 64 feature maps. Then, 19
ResBlocks are stacked followed by last convolution layer
that transforms the feature map into input dimension. Every
convolution layer preserves resolution with zero padding. In
total, there are 40 convolution layers. The number of con-
volution layers at each scale level is determined so that total
model should have 120 convolution layers. Thus, the coars-
est level network has receptive ﬁeld large enough to cover
the whole patch. At the end of the stage, the coarsest level
latent sharp image is generated. Moreover, information
from the coarsest level output is delivered to the next stage
where ﬁner scale network is. To convert a coarsest output
to ﬁt the input size of the next ﬁner scale, the output patch
passes an upconvolution [22] layer, while other multi-scale
methods use reshaping [8] or upsampling [4, 6, 23]. Since
the sharp and blurry patches share low-frequency informa-
tion, learning suitable feature with upconvolution helps to
remove redundancy. In our experiment, using upconvolu-
tion showed better performance than upsampling. Then, the
upconvolution feature is concatenated with the ﬁner scale
blurry patch as an input.

Finer level networks basically have the same structure as in
the coarsest level network. However, the ﬁrst convolution
layer takes the sharp feature from the previous stage as well
as its own blurry input image, in a concatenated form. Every
convolution ﬁlter size is 5 × 5 with the same number of
feature maps as in the coarsest level. Except for the last
ﬁnest scale, there is an upconvolution layer before the next
stage. At the ﬁnest scale, the original resolution sharp image
is restored.

3.2. Training

Our model is trained on the proposed GOPRO dataset.
Among 3214 pairs, 2103 pairs were used for training and
remainings were used for the test. To prevent our network
from overﬁtting, several data augmentation techniques are
involved. In terms of geometric transformations, patches
are randomly ﬂipped horizontally and vertically, rotated by
90 degrees. For color, RGB channels are randomly per-
muted. To take image degradations into account, satura-
tion in HSV colorspace is multiplied by a random num-
ber within [0.5, 1.5]. Also, Gaussian random noise is
added to blurry images. To make our network be robust
against different strengths of noise, standard deviation of

noise is also randomly sampled from Gaussian distribution,
N (0, (2/255)2). Then, value outside [0, 1] is clipped. Fi-
nally, 0.5 is subtracted to set input and output value range
zero-centered, having range [-0.5, 0.5].

In optimizing the network parameters, we trained the
model in a combination of two losses, multi-scale content
loss and adversarial loss.

Multi-scale content loss

Basically, the coarse-to-ﬁne approach desires that every in-
termediate output becomes the sharp image of the corre-
sponding scale. Thus, we train our network so that inter-
mediate outputs should form a Gaussian pyramid of sharp
images. MSE criterion is applied to every level of pyramids.
Hence, the loss function is deﬁned as follows:

Lcont =

kLk − Skk2,

(4)

1
2K

K

Xk=1

1
ckwkhk

where Lk, Sk denote the model output and ground truth im-
age at scale level k, respectively. The loss at each scale is
normalized by the number of channels ck, width wk, and
the height hk (i.e. the total number of elements).

Adversarial loss

Recently, adversarial networks are reported to generate
sharp realistic images [9, 4, 24]. Following the architec-
ture introduced in [24], we build discriminator as in Ta-
ble 1. Discriminator takes the output of the ﬁnest scale or
the ground truth sharp image as input and classiﬁes if it is
deblurred image or sharp image.

The adversarial loss is deﬁned as follows.

Ladv =

[log D(S)]+

E
S∼psharp(S)
E
B∼pblurry (B)

[log(1 − D(G(B)))],

(5)

where G and D denote the generator, that is our multi-
scale deblurring network in Fig. 4 and the discriminator
(classiﬁer), respectively. When training, G tries to minimize
the adversarial loss while D tries to maximize it.

Finally, by combining the multi-scale content loss and
adversarial loss, the generator network and discriminator
network is jointly trained. Thus, our ﬁnal loss term is

#

1

2

3

4

5

6

7

8

9

10

11

Layer Weight dimension

Stride

conv

conv

conv

conv

conv

conv

conv

conv

conv

fc

32 × 3 × 5 × 5

64 × 32 × 5 × 5

64 × 64 × 5 × 5

128 × 64 × 5 × 5

128 × 128 × 5 × 5

256 × 128 × 5 × 5

256 × 256 × 5 × 5

512 × 256 × 5 × 5

512 × 512 × 4 × 4

512 × 1 × 1 × 1

sigmoid

-

2

1

2

1

4

1

4

1

4

-

-

Table 1. Model parameters of the discriminator. Every convolution
layers are activated with LeakyReLU layer.

rate is decreased to 1/10 of the previous learning rate. Total
training takes 9 × 105 iterations to converge.

4. Experimental Results

We implemented our model with torch7 library. All the
following experiments were performed in a desktop with i7-
6700K CPU and NVIDIA GTX Titan X (Maxwell) GPU.

4.1. GOPRO Dataset

We evaluate the performance of our model in the pro-
posed GOPRO dataset. Our test dataset consists of 1111
pairs, which is approximately 1/3 of the total dataset. We
compare the results with those of the state-of-the-art meth-
ods [15, 26] in both qualitative and quantitative ways. Our
results show signiﬁcant improvement in terms of image
quality. Some deblurring results are shown in Fig. 5. We no-
tice from the results of Sun et al. [26], deblurring is not suc-
cessful on the regions where blurs are nonlinearly shaped or
located at the boundary of motion. Kim and Lee [15]’s re-
sults also fail in cases where strong edges are not found. In
contrast, our results are free from those kernel-estimation
related problems. Table 2, shows the quantitative evalua-
tion results of the competing methods and ours with differ-
ent scale level k in terms of PSNR, SSIM over the test data.
Also, the runtime is compared. We observe that our system
with K = 2 produces the best results in terms of both PSNR
and SSIM, while K = 3 is the fastest.

Ltotal = Lcont + λ × Ladv,
where the weight constant λ = 1 × 10−4.
We used ADAM [18] optimizer with a mini-batch size
2 for training. The learning rate is adaptively tuned begin-
ning from 5 × 10−5. After 3 × 105 iterations, the learning

(6)

4.2. K¨ohler Dataset

K¨ohler dataset [19] consists of 4 latent images and 12
differently blurred images for each of them. The blurs are
caused by replaying recorded 6D camera motion, assum-
ing linear CRF. We report the quantitative results on this

Figure 5. Test results on the GOPRO dataset. From top to bottom: Blurry images, results of Sun et al. [26], results of Kim and Lee [15],
and results of the proposed method.

Figure 6. Deblurring results on the dataset [20]. The top row shows results of results of Sun et al. [26] and the bottom row shows our
results.

Ours

4.3. Dataset of Lai et al.

Measure

[26]

[15]

K = 1 K = 2 K = 3

PSNR

SSIM

24.64

23.64

28.93

29.23

29.08

0.8429

0.8239

0.9100

0.9162

0.9135

Runtime

20 min

1 hr

7.21 s

4.33 s

3.09 s

Table 2. Quantitative deblurring performance comparison on the
GOPRO dataset. K denotes the scale level.

dataset in Table 3. Our model is trained by setting g as iden-
tity function in (2). We note that our system with K = 3
produces the best results in PSNR, and the system K = 2
exhibits the best MSSIM result.

Measure

[26]

[15]

Ours

K = 1 K = 2 K = 3

PSNR

25.22

24.68

25.74

26.02

26.48

MSSIM 0.7735

0.7937

0.8042

0.8116

0.8079

Lai et al.

[20] generated synthetic dataset by convolv-
ing nonuniform blur kernels and imposing several common
degradations. They also recorded 6D camera trajectories
to generate blur kernels. However, their blurry images and
sharp images are not aligned in the way of our dataset, mak-
ing simple image quality measures such as PSNR and SSIM
less correlated with perceptual quality. Thus, we show qual-
itative comparisons in Fig. 6. Clearly, our results avoid ring-
ing artifacts while preserving details such as wave ripple.

5. Conclusion

In this paper, we proposed a blind deblurring neural net-
work for sharp image estimation. Unlike previous stud-
ies, our model avoids problems related to kernel estimation.
The proposed model follows a coarse-to-ﬁne approach and
is trained in multi-scale space. We also constructed a re-
alistic ground-truth blur dataset, enabling efﬁcient super-
vised learning and rigorous evaluation. Experimental re-
sults show that our approach outperforms the state-of-the-
art methods in both qualitative and quantitative ways while
being much faster.

This project is partially funded by Microsoft Research

Asia.

Table 3. Quantitative comparison on the K¨ohler dataset. The
dataset has its own evaluation code, thus we report multi-scale
SSIM instead of SSIM.

Acknowledgement

[19] R. K¨ohler, M. Hirsch, B. Mohler, B. Sch¨olkopf, and
S. Harmeling.
Recording and playback of camera
shake: Benchmarking blind deconvolution with a real-world
database. In ECCV, pages 27–40. Springer, 2012. 6

[20] W.-S. Lai, J.-B. Huang, Z. Hu, N. Ahuja, and M.-H. Yang. A
comparative study for single image blind deblurring. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 1701–1709, 2016. 8, 16

[21] Y. Li, S. B. Kang, N. Joshi, S. M. Seitz, and D. P. Hutten-
locher. Generating sharp panoramas from motion-blurred
videos. In CVPR, 2010. 2

[22] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In CVPR, pages 3431–
3440, 2015. 5

[23] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale
video prediction beyond mean square error. arXiv preprint
arXiv:1511.05440, 2015. 3, 5

[24] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-
sentation learning with deep convolutional generative adver-
sarial networks. arXiv preprint arXiv:1511.06434, 2015. 6

[25] C. J. Schuler, M. Hirsch, S. Harmeling, and B. Sch¨olkopf.
IEEE transactions on pattern analysis

Learning to deblur.
and machine intelligence, 38(7):1439–1451, 2016. 1, 2, 3

[26] J. Sun, W. Cao, Z. Xu, and J. Ponce. Learning a convolu-
tional neural network for non-uniform motion blur removal.
In CVPR, pages 769–777. IEEE, 2015. 1, 2, 3, 6, 7, 8, 13, 19
[27] Y.-W. Tai, X. Chen, S. Kim, S. J. Kim, F. Li, J. Yang, J. Yu,
Y. Matsushita, and M. S. Brown. Nonlinear camera response
functions and image deblurring: Theoretical analysis and
practice. PAMI, 35(10):2498–2512, 2013. 3

[28] O. Whyte, J. Sivic, A. Zisserman, and J. Ponce. Non-uniform

deblurring for shaken images. 2010. 1

[29] L. Xu, J. S. Ren, C. Liu, and J. Jia. Deep convolutional neural
In Advances in Neural
network for image deconvolution.
Information Processing Systems, pages 1790–1798, 2014. 1,
2

[30] D. Zoran and Y. Weiss. From learning models of natural
image patches to whole image restoration. In ICCV, pages
479–486. IEEE, 2011. 3

References

[1] A. Chakrabarti. A neural approach to blind motion deblur-

ring. In ECCV, 2016. 1, 2

[2] F. Couzinie-Devy, J. Sun, K. Alahari, and J. Ponce. Learning
to estimate and remove non-uniform image blur. In CVPR,
2013. 2

[3] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, pages 248–255. IEEE, 2009. 2

[4] E. L. Denton, S. Chintala, R. Fergus, et al. Deep genera-
tive image models using a laplacian pyramid of adversarial
In Advances in Neural Information Processing
networks.
Systems, pages 1486–1494, 2015. 3, 5, 6

[5] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas,
V. Golkov, P. van der Smagt, D. Cremers, and T. Brox.
Flownet: Learning optical ﬂow with convolutional networks.
In CVPR, pages 2758–2766, 2015. 3

[6] D. Eigen and R. Fergus. Predicting depth, surface normals
and semantic labels with a common multi-scale convolu-
In ICCV, pages 2650–2658, 2015. 3,
tional architecture.
5

[7] D. Eigen, D. Krishnan, and R. Fergus. Restoring an image
taken through a window covered with dirt or rain. In ICCV,
pages 633–640, 2013. 2

[8] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction
from a single image using a multi-scale deep network.
In
Advances in Neural Information Ppocessing Ssytems, pages
2366–2374, 2014. 3, 5

[9] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in Neural Information
Processing Systems, pages 2672–2680, 2014. 2, 6

[10] A. Gupta, N. Joshi, C. L. Zitnick, M. Cohen, and B. Curless.
Single image deblurring using motion density functions. In
ECCV, pages 171–184. Springer, 2010. 1

[11] S. Harmeling, H. Michael, and B. Sch¨olkopf.

Space-
variant single-image blind deconvolution for removing cam-
In Advances in Neural Information Processing
era shake.
Systems, pages 829–837, 2010. 1

[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, pages 770–778, 2016. 4

[13] M. Hirsch, C. J. Schuler, S. Harmeling, and B. Sch¨olkopf.
Fast removal of non-uniform camera shake. In ICCV, 2011.
1, 3

[14] T. H. Kim, B. Ahn, and K. M. Lee. Dynamic scene deblur-

ring. In ICCV, 2013. 1

[15] T. H. Kim and K. M. Lee. Segmentation-free dynamic scene

deblurring. In CVPR, 2014. 1, 6, 7, 8, 13, 19

[16] T. H. Kim and K. M. Lee. Generalized video deblurring for

dynamic scenes. In CVPR, 2015. 2

[17] T. H. Kim, S. Nah, and K. M. Lee. Dynamic scene deblurring
using a locally adaptive linear blur model. arXiv preprint
arXiv:1603.04265, 2016. 2

[18] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. arXiv preprint arXiv:1412.6980, 2014. 6

In this appendix, we present more comparative experimental results to demonstrate the effectiveness of our proposed

A. Appendix

deblurring method.

A.1. Comparison of loss function

In section 3.2, we employed a loss function that combines both the multi-scale content loss (MSE) and the adversarial loss
for training our network. We examine the effect of the adversarial loss term quantitatively and qualitatively. The PSNR and
SSIM results are shown in table A.1. From this results, we observe that adding adversarial loss does not increases PSNR, but
increase SSIM, which means that it encourages to generate more natural and structure preserving images.

Table A.1. Quantitative deblurring performance comparison of loss used to optimize our model (K = 3, λ = 1 × 10−4). Evaluated on
the GOPRO test dataset assuming linear CRF.

Loss

PSNR

SSIM

Lcont(M SE) Lcont + λLadv

28.62

0.9094

28.45

0.9170

Fig. A.1 and A.2 show some qualitative comparisons between the results of our network trained with Lcont and Lcont +

λLadv.

Blurry image

Deblurred image (MSE)

Deblurred image (MSE + Adversarial)

Figure A.1. Visual comparison of results from our model trained with different loss functions. The blurry image is from our proposed
dataset.

Blurry image

Deblurred image (MSE)

Deblurred image (MSE + Adversarial)

Figure A.2. Visual comparison of results from our model trained with different loss functions. The blurry image is from our proposed
dataset.

A.2. Comparison on GOPRO dataset

Lee [15], Sun et al. [26], and ours.

We provide qualitative results on our GOPRO test dataset. Fig. A.3 and A.4 shows the deblurring results of Kim and

Blurry image

Kim and Lee [15]

Sun et al. [27]

Ours

Figure A.3. Visual comparison with other methods. The blurry image is from our proposed dataset.

Blurry image

Kim and Lee [15]

Sun et al. [27]

Ours

Figure A.4. Visual comparison with other methods. The blurry image is from our proposed dataset.

A.3. Comparison on Lai et al. [20] dataset

We provide qualitative results on the dataset of Lai et al. [?]. The Lai et al. dataset is composed of synthetic and real
blurry images, and we showed the deblurring result of a synthetically generated blurry image in section 4.3. We present the
qualitative deblurring results of competing methods on real images in Fig. A.5 and A.6.

Blurry image

Kim and Lee [15]

Sun et al. [27]

Ours

Figure A.5. Visual comparison with other methods. The blurry image is a real image in Lai et al. [?]

Blurry image

Kim and Lee [15]

Sun et al. [27]

Ours

Figure A.6. Visual comparison with other methods. The blurry image is a real image in Lai et al. [?]

A.4. Comparison on real dynamic scenes

Finally, we further present deblurring results on real dynamic scenes. The blurry scenes are captured by a SONY RX100
M4 camera. The qualitative deblurring results of Kim and Lee [15], Sun et al. [26] and ours are compared in Fig. A.7 and
A.8.

Blurry image

Kim and Lee [15]

Sun et al. [27]

Ours

Figure A.7. Visual comparison with other methods. The blurry image is a real dynamic scene.

Blurry image

Kim and Lee [15]

Sun et al. [27]

Ours

Figure A.8. Visual comparison with other methods. The blurry image is a real dynamic scene.

8
1
0
2
 
y
a
M
 
7
 
 
]

V
C
.
s
c
[
 
 
2
v
7
7
1
2
0
.
2
1
6
1
:
v
i
X
r
a

Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring

Seungjun Nah

Tae Hyun Kim

Kyoung Mu Lee

Department of ECE, ASRI, Seoul National University, 08826, Seoul, Korea
{seungjun.nah, lliger9}@gmail.com, kyoungmu@snu.ac.kr

Abstract

blur model [28, 10, 13, 11].

Non-uniform blind deblurring for general dynamic
scenes is a challenging computer vision problem as blurs
arise not only from multiple object motions but also from
camera shake, scene depth variation. To remove these
complicated motion blurs, conventional energy optimiza-
tion based methods rely on simple assumptions such that
blur kernel is partially uniform or locally linear. More-
over, recent machine learning based methods also depend
on synthetic blur datasets generated under these assump-
tions. This makes conventional deblurring methods fail to
remove blurs where blur kernel is difﬁcult to approximate or
parameterize (e.g. object motion boundaries). In this work,
we propose a multi-scale convolutional neural network that
restores sharp images in an end-to-end manner where blur
is caused by various sources. Together, we present multi-
scale loss function that mimics conventional coarse-to-ﬁne
approaches. Furthermore, we propose a new large-scale
dataset that provides pairs of realistic blurry image and the
corresponding ground truth sharp image that are obtained
by a high-speed camera. With the proposed model trained
on this dataset, we demonstrate empirically that our method
achieves the state-of-the-art performance in dynamic scene
deblurring not only qualitatively, but also quantitatively.

1. Introduction

Motion blur is one of the most commonly arising types
of artifacts when taking photos. Shakes of camera and fast
object motions degrade image quality to undesired blurry
images. Furthermore, various causes such as depth varia-
tion, occlusion in motion boundaries make blurs even more
complex. Single image deblurring problem is to estimate
the unknown sharp image given a blurry image. Earlier
studies focused on removing blurs caused by simple transla-
tional or rotational camera motions. More recent works try
to handle general non-uniform blurs caused by depth vari-
ation, camera shakes and object motions in dynamic envi-
ronments. Most of these approaches are based on following

B = KS + n,

(1)

where B, S and n are vectorized blurry image, latent
sharp image, and noise, respectively. K is a large sparse
matrix whose rows each contain a local blur kernel acting
on S to generate a blurry pixel. In practice, blur kernel is
unknown. Thus, blind deblurring methods try to estimate
latent sharp image S and blur kernel K simultaneously.

Finding blur kernel for every pixel is a severely ill-posed
problem. Thus, some approaches tried to parametrize blur
models with simple assumptions on the sources of blurs. In
[28, 10], they assumed that blur is caused by 3D camera
motion only. However, in dynamic scenes, the kernel es-
timation is more challenging as there are multiple moving
objects as well as camera motion. Thus, Kim et al. [14] pro-
posed a dynamic scene deblurring method that jointly seg-
ments and deblurs a non-uniformly blurred image, allowing
the estimation of complex (non-linear) kernel within a seg-
ment. In addition, Kim and Lee [15] approximated the blur
kernel to be locally linear and proposed an approach that es-
timates both the latent image and the locally linear motions
jointly. However, these blur kernel approximations are still
inaccurate, especially in the cases of abrupt motion discon-
tinuities and occlusions. Note that such erroneous kernel
estimation directly affects the quality of the latent image,
resulting in undesired ringing artifacts.

Recently, CNNs (Convolutional Neural Networks) have
been applied in numerous computer vision problems in-
cluding deblurring problem and showed promising results
[29, 25, 26, 1]. Since no pairs of real blurry image and
ground truth sharp image are available for supervised learn-
ing, they commonly used blurry images generated by con-
volving synthetic blur kernels. In [29, 25, 1], synthesized
blur images with uniform blur kernel are used for training.
And, in [26], classiﬁcation CNN is trained to estimate lo-
cally linear blur kernels. Thus, CNN-based models are still
suited only to some speciﬁc types of blurs, and there are
restrictions on more common spatially varying blurs.

1

(a)

(b)

(c)

Figure 1. (a) Input blurry image. (b) Result of Sun et al. [26]. (c) Our deblurring result. Our results show clear object boundaries without
artifacts.

Therefore, all the existing methods still have many prob-
lems before they could be generalized and used in practice.
These are mainly due to the use of simple and unrealis-
tic blur kernel models. Thus, to solve those problems, in
this work, we propose a novel end-to-end deep learning ap-
proach for dynamic scene deblurring.

First, we propose a multi-scale CNN that directly re-
stores latent images without assuming any restricted blur
kernel model. Especially, the multi-scale architecture is
designed to mimic conventional coarse-to-ﬁne optimization
methods. Unlike other approaches, our method does not es-
timate explicit blur kernels. Accordingly, our method is free
from artifacts that arise from kernel estimation errors. Sec-
ond, we train the proposed model with a multi-scale loss
that is appropriate for coarse-to-ﬁne architecture that en-
hances convergence greatly. In addition, we further improve
the results by employing adversarial loss [9]. Third, we pro-
pose a new realistic blurry image dataset with ground truth
sharp images. To obtain kernel model-free dataset for train-
ing, we employ the dataset acquisition method introduced
in [17]. As the blurring process can be modeled by the in-
tegration of sharp images during shutter time [17, 21, 16],
we captured a sequence of sharp frames of a dynamic scene
with a high-speed camera and averaged them to generate a
blurry image by considering gamma correction.

By training with the proposed dataset and adding proper
augmentation, our model can handle general local blur ker-
nel implicitly. As the loss term optimizes the result to
resemble the ground truth, it even restores occluded re-
gions where blur kernel is extremely complex as shown in
Fig. 1. We trained our model with millions of pairs of image
patches and achieved signiﬁcant improvements in dynamic
scene deblurring. Extensive experimental results demon-
strate that the performance of the proposed method is far

superior to those of the state-of-the-art dynamic scene de-
blurring methods in both qualitative and quantitative evalu-
ations.

1.1. Related Works

There are several approaches that employed CNNs for

deblurring [29, 26, 25, 1].

Xu et al. [29] proposed an image deconvolution CNN to
deblur a blurry image in a non-blind setting. They built a
network based on the separable kernel property that the (in-
verse) blur kernel can be decomposed into a small number
of signiﬁcant ﬁlters. Additionally, they incorporated the de-
noising network [7] to reduce visual artifacts such as noise
and color saturation by concatenating the module at the end
of their proposed network.

On the other hand, Schuler et al. [25] proposed a blind
deblurring method with CNN. Their proposed network
mimics conventional optimization-based deblurring meth-
ods and iterates the feature extraction, kernel estimation,
and the latent image estimation steps in a coarse-to-ﬁne
manner. To obtain pairs of sharp and blurry images for net-
work training, they generated uniform blur kernels using a
Gaussian process and synthesized lots of blurry images by
convolving them to the sharp images collected from the Im-
ageNet dataset [3]. However, they reported performance
limits for large blurs due to their suboptimal architecture.

Similarly to the work of Couzinie-Devy et al. [2], Sun
et al. [26] proposed a sequential deblurring approach. First,
they generated pairs of blurry and sharp patches with 73
candidate blur kernels. Next, they trained classiﬁcation
CNN to measure the likelihood of a speciﬁc blur kernel of
a local patch. And then smoothly varying blur kernel is ob-
tained by optimizing an energy model that is composed of
the CNN likelihoods and smoothness priors. Final latent

image estimation is performed with conventional optimiza-
tion method [30].

Note that all these methods require an accurate kernel
estimation step for restoring the latent sharp image. In con-
trast, our proposed model is learned to produce the latent
image directly without estimating blur kernels.

In other computer vision tasks, several forms of coarse-
to-ﬁne architecture or multi-scale architecture were ap-
plied [8, 6, 4, 23, 5]. However, not all multi-scale CNNs
are designed to produce optimal results, similarly to [25].
In depth estimation, optical ﬂow estimation, etc., networks
usually produce outputs having smaller resolution com-
pared to input image resolution [8, 6, 5]. These methods
have difﬁculties in handling long-range dependency even if
multi-scale architecture is used.

Therefore, we make a multi-scale architecture that pre-
serves ﬁne-grained detail information as well as long-range
dependency from coarser scales. Furthermore, we make
sure intermediate level networks help the ﬁnal stage in an
explicit way by training network with multi-scale losses.

1.2. Kernel Free Learning for Dynamic Scene De 

blurring

Conventionally, it was essential to ﬁnd blur kernel before
estimating latent image. CNN based methods were no ex-
ception [25, 26]. However, estimating kernel involves sev-
eral problems. First, assuming simple kernel convolution
cannot model several challenging cases such as occluded re-
gions or depth variations. Second, kernel estimation process
is subtle and sensitive to noise and saturation, unless blur
model is carefully designed. Furthermore, incorrectly esti-
mated kernels give rise to artifacts in latent images. Third,
ﬁnding spatially varying kernel for every pixel in dynamic
scene requires a huge amount of memory and computation.
Therefore, we adopt kernel-free methods in both blur
dataset generation and latent image estimation. In blurry
image generation, we follow to approximate camera imag-
ing process, rather than assuming speciﬁc motions, instead
of ﬁnding or designing complex blur kernel. We capture
successive sharp frames and integrate to simulate blurring
process. The detailed procedure is described in section 2.
Note that our dataset is composed of blurry and sharp image
pairs only, and that the local kernel information is implic-
itly embedded in it. In Fig. 2, our kernel-free blurry image is
compared with a conventional synthesized image with uni-
form blur kernel. Notably, the blur image generated by our
method exhibits realistic and spatially varying blurs caused
by the moving person and the static background, while the
blur image synthesized by conventional method does not.
For latent image estimation, we do not assume blur sources
and train the model solely on our blurry and sharp image
pairs. Thus, our proposed method does not suffer from
kernel-related problems in deblurring.

2. Blur Dataset

Instead of modeling a kernel to convolve on a sharp im-
age, we choose to record the sharp information to be inte-
grated over time for blur image generation. As camera sen-
sor receives light during the exposure, sharp image stimu-
lation at every time is accumulated, generating blurry im-
age [13]. The integrated signal is then transformed into
pixel value by nonlinear CRF (Camera Response Function).
Thus, the process could be approximated by accumulating
signals from high-speed video frames.

Blur accumulation process can be modeled as follows.

B = g

S(t)dt

≃ g

1
T

 

T

t=0

Z

1
M

 

M−1

i=0
X

!

S[i]

,

(2)

!

where T and S(t) denote the exposure time and the sen-
sor signal of a sharp image at time t, respectively. Simi-
larly, M , S[i] are the number of sampled frames and the
i-th sharp frame signal captured during the exposure time,
respectively. g is the CRF that maps a sharp latent signal
S(t) into an observed image ˆS(t) such that ˆS(t) = g(S(t)),
or ˆS[i] = g(S[i]). In practice, we only have observed video
frames while the original signal and the CRF is unknown.

It is known that non-uniform deblurring becomes signif-
icantly difﬁcult when nonlinear CRF is involved, and non-
linearity should be taken into account. However, currently,
there are no CRF estimation techniques available for an im-
age with spatially varying blur [27]. When the ground truth
CRF is not given, a common practical method is to approxi-
mate CRF as a gamma curve with γ = 2.2 as follows, since
it is known as an aproximated average of known CRFs [27].

g(x) = x1/γ.

(3)

Thus, by correcting the gamma function, we obtain the
latent frame signal S[i] from the observed image ˆS[i] by
S[i] = g−1( ˆS[i]), and then synthesize the corresponding
blur image B by using (2).

We used GOPRO4 Hero Black camera to generate our
dataset. We took 240 fps videos with GOPRO camera and
then averaged varying number (7 - 13) of successive latent
frames to produce blurs of different strengths. For example,
averaging 15 frames simulates a photo taken at 1/16 shut-
ter speed, while corresponding sharp image shutter speed
is 1/240. Notably, the sharp latent image corresponding to
each blurry one is deﬁned as the mid-frame among the sharp
frames that are used to make the blurry image. Finally, our
dataset is composed of 3214 pairs of blurry and sharp im-
ages at 1280x720 resolution. The proposed GOPRO dataset
is publicly available on our website 1.

1https://github.com/SeungjunNah/DeepDeblur_release

(a)

(b)

(c)

Figure 2. (a) Ground truth sharp image. (b) Blurry image generated by convolving a uniform blur kernel. (c) Blurry image by averaging
sharp frames. In this case, blur is mostly caused by person motion, leaving the background as it is. The blur kernel is non-uniform, complex
shaped. However, when the blurry image is synthesized by convolution with a uniform kernel, the background also gets blurred as if blur
was caused by camera shake. To model dynamic scene blur, our kernel-free method is required.

INPUT

CONV

BN

ReLU

CONV

BN

ReLU

OUTPUT

(a)

3. Proposed Method

In our model, ﬁner scale image deblurring is aided by
coarser scale features. To exploit coarse and middle level
information while preserving ﬁne level information at the
same time, input and output to our network take the form of
Gaussian pyramids. Note that most of other coarse-to-ﬁne
networks take a single image as input and output.

3.1. Model Architecture

In addition to the multi-scale architecture, we employ a
slightly modiﬁed version of residual network structure [12]
as a building block of our model. Using residual network
structure enables deeper architecture compared to a plain
CNN. Also, as blurry and sharp image pairs are similar in
values, it is efﬁcient to let parameters learn the difference
only. We found that removing the rectiﬁed linear unit af-
ter the shortcut connection of the original residual building
block boosts the convergence speed at training time. We de-
note the modiﬁed building block as ResBlock. The original
and our modiﬁed building block are compared in Fig. 3.

By stacking enough number of convolution layers with
ResBlocks, the receptive ﬁeld at each scale is expanded.
Details are described in the following paragraphs. For sake
of consistency, we deﬁne scale levels in the order of de-
creasing resolution (i.e.
level 1 for ﬁnest scale). Unless
denoted otherwise, we use total K = 3 scales. At training
time, we set the resolution of the input and output Gaussian
pyramid patches to be {256 × 256, 128 × 128, 64 × 64}.
The scale ratio between consecutive scales is 0.5. For all
convolution layers, we set the ﬁlter size to be 5 × 5. As our
model is fully convolutional, at test time, the patch size may
vary as the GPU memory allows. The overall architecture
is shown in Fig. 4.

INPUT

CONV

ReLU

CONV

OUTPUT

(b)

Figure 3. (a) Original residual network building block. (b) Mod-
iﬁed building block of our network. We did not use batch nor-
malization layers since we trained model with mini-batch of size
2, which is smaller than usual for batch normalization. We found
removing rectiﬁed linear unit just before the block output is bene-
ﬁcial in terms of performance empirically.

(cid:1776)(cid:2778)

(cid:1776)(cid:2779)

(cid:1776)(cid:2780)

CONV

ResBlock

ResBlock

CONV

CONV

ResBlock

ResBlock

CONV

. . .

Up
Conv

. . .

Up
Conv

. . .

Backprop

(cid:1786)(cid:2778)

(cid:1786)(cid:2779)

(cid:1793)(cid:2778)

(cid:1793)(cid:2779)

Backprop

Backprop

CONV

ResBlock

ResBlock

CONV

Figure 4. Multi-scale network architecture. Bk, Lk, Sk denote blurry and latent, and ground truth sharp images, respectively. Subscript
k denotes k-th scale level in the Gaussian pyramid, which is downsampled to 1/2k scale. Our model takes a blurry image pyramid as the
input and outputs an estimated latent image pyramid. Every intermediate scale output is trained to be sharp. At test time, original scale
image is chosen as the ﬁnal result.

(cid:1786)(cid:2780)

(cid:1793)(cid:2780)

Coarsest level network

Finer level network

At the front of the network locates the coarsest level net-
work. The ﬁrst convolution layer transforms 1/4 resolu-
tion, 64 × 64 size image into 64 feature maps. Then, 19
ResBlocks are stacked followed by last convolution layer
that transforms the feature map into input dimension. Every
convolution layer preserves resolution with zero padding. In
total, there are 40 convolution layers. The number of con-
volution layers at each scale level is determined so that total
model should have 120 convolution layers. Thus, the coars-
est level network has receptive ﬁeld large enough to cover
the whole patch. At the end of the stage, the coarsest level
latent sharp image is generated. Moreover, information
from the coarsest level output is delivered to the next stage
where ﬁner scale network is. To convert a coarsest output
to ﬁt the input size of the next ﬁner scale, the output patch
passes an upconvolution [22] layer, while other multi-scale
methods use reshaping [8] or upsampling [4, 6, 23]. Since
the sharp and blurry patches share low-frequency informa-
tion, learning suitable feature with upconvolution helps to
remove redundancy. In our experiment, using upconvolu-
tion showed better performance than upsampling. Then, the
upconvolution feature is concatenated with the ﬁner scale
blurry patch as an input.

Finer level networks basically have the same structure as in
the coarsest level network. However, the ﬁrst convolution
layer takes the sharp feature from the previous stage as well
as its own blurry input image, in a concatenated form. Every
convolution ﬁlter size is 5 × 5 with the same number of
feature maps as in the coarsest level. Except for the last
ﬁnest scale, there is an upconvolution layer before the next
stage. At the ﬁnest scale, the original resolution sharp image
is restored.

3.2. Training

Our model is trained on the proposed GOPRO dataset.
Among 3214 pairs, 2103 pairs were used for training and
remainings were used for the test. To prevent our network
from overﬁtting, several data augmentation techniques are
involved. In terms of geometric transformations, patches
are randomly ﬂipped horizontally and vertically, rotated by
90 degrees. For color, RGB channels are randomly per-
muted. To take image degradations into account, satura-
tion in HSV colorspace is multiplied by a random num-
ber within [0.5, 1.5]. Also, Gaussian random noise is
added to blurry images. To make our network be robust
against different strengths of noise, standard deviation of

noise is also randomly sampled from Gaussian distribution,
N (0, (2/255)2). Then, value outside [0, 1] is clipped. Fi-
nally, 0.5 is subtracted to set input and output value range
zero-centered, having range [-0.5, 0.5].

In optimizing the network parameters, we trained the
model in a combination of two losses, multi-scale content
loss and adversarial loss.

Multi-scale content loss

Basically, the coarse-to-ﬁne approach desires that every in-
termediate output becomes the sharp image of the corre-
sponding scale. Thus, we train our network so that inter-
mediate outputs should form a Gaussian pyramid of sharp
images. MSE criterion is applied to every level of pyramids.
Hence, the loss function is deﬁned as follows:

Lcont =

kLk − Skk2,

(4)

1
2K

K

Xk=1

1
ckwkhk

where Lk, Sk denote the model output and ground truth im-
age at scale level k, respectively. The loss at each scale is
normalized by the number of channels ck, width wk, and
the height hk (i.e. the total number of elements).

Adversarial loss

Recently, adversarial networks are reported to generate
sharp realistic images [9, 4, 24]. Following the architec-
ture introduced in [24], we build discriminator as in Ta-
ble 1. Discriminator takes the output of the ﬁnest scale or
the ground truth sharp image as input and classiﬁes if it is
deblurred image or sharp image.

The adversarial loss is deﬁned as follows.

Ladv =

[log D(S)]+

E
S∼psharp(S)
E
B∼pblurry (B)

[log(1 − D(G(B)))],

(5)

where G and D denote the generator, that is our multi-
scale deblurring network in Fig. 4 and the discriminator
(classiﬁer), respectively. When training, G tries to minimize
the adversarial loss while D tries to maximize it.

Finally, by combining the multi-scale content loss and
adversarial loss, the generator network and discriminator
network is jointly trained. Thus, our ﬁnal loss term is

#

1

2

3

4

5

6

7

8

9

10

11

Layer Weight dimension

Stride

conv

conv

conv

conv

conv

conv

conv

conv

conv

fc

32 × 3 × 5 × 5

64 × 32 × 5 × 5

64 × 64 × 5 × 5

128 × 64 × 5 × 5

128 × 128 × 5 × 5

256 × 128 × 5 × 5

256 × 256 × 5 × 5

512 × 256 × 5 × 5

512 × 512 × 4 × 4

512 × 1 × 1 × 1

sigmoid

-

2

1

2

1

4

1

4

1

4

-

-

Table 1. Model parameters of the discriminator. Every convolution
layers are activated with LeakyReLU layer.

rate is decreased to 1/10 of the previous learning rate. Total
training takes 9 × 105 iterations to converge.

4. Experimental Results

We implemented our model with torch7 library. All the
following experiments were performed in a desktop with i7-
6700K CPU and NVIDIA GTX Titan X (Maxwell) GPU.

4.1. GOPRO Dataset

We evaluate the performance of our model in the pro-
posed GOPRO dataset. Our test dataset consists of 1111
pairs, which is approximately 1/3 of the total dataset. We
compare the results with those of the state-of-the-art meth-
ods [15, 26] in both qualitative and quantitative ways. Our
results show signiﬁcant improvement in terms of image
quality. Some deblurring results are shown in Fig. 5. We no-
tice from the results of Sun et al. [26], deblurring is not suc-
cessful on the regions where blurs are nonlinearly shaped or
located at the boundary of motion. Kim and Lee [15]’s re-
sults also fail in cases where strong edges are not found. In
contrast, our results are free from those kernel-estimation
related problems. Table 2, shows the quantitative evalua-
tion results of the competing methods and ours with differ-
ent scale level k in terms of PSNR, SSIM over the test data.
Also, the runtime is compared. We observe that our system
with K = 2 produces the best results in terms of both PSNR
and SSIM, while K = 3 is the fastest.

Ltotal = Lcont + λ × Ladv,
where the weight constant λ = 1 × 10−4.
We used ADAM [18] optimizer with a mini-batch size
2 for training. The learning rate is adaptively tuned begin-
ning from 5 × 10−5. After 3 × 105 iterations, the learning

(6)

4.2. K¨ohler Dataset

K¨ohler dataset [19] consists of 4 latent images and 12
differently blurred images for each of them. The blurs are
caused by replaying recorded 6D camera motion, assum-
ing linear CRF. We report the quantitative results on this

Figure 5. Test results on the GOPRO dataset. From top to bottom: Blurry images, results of Sun et al. [26], results of Kim and Lee [15],
and results of the proposed method.

Figure 6. Deblurring results on the dataset [20]. The top row shows results of results of Sun et al. [26] and the bottom row shows our
results.

Ours

4.3. Dataset of Lai et al.

Measure

[26]

[15]

K = 1 K = 2 K = 3

PSNR

SSIM

24.64

23.64

28.93

29.23

29.08

0.8429

0.8239

0.9100

0.9162

0.9135

Runtime

20 min

1 hr

7.21 s

4.33 s

3.09 s

Table 2. Quantitative deblurring performance comparison on the
GOPRO dataset. K denotes the scale level.

dataset in Table 3. Our model is trained by setting g as iden-
tity function in (2). We note that our system with K = 3
produces the best results in PSNR, and the system K = 2
exhibits the best MSSIM result.

Measure

[26]

[15]

Ours

K = 1 K = 2 K = 3

PSNR

25.22

24.68

25.74

26.02

26.48

MSSIM 0.7735

0.7937

0.8042

0.8116

0.8079

Lai et al.

[20] generated synthetic dataset by convolv-
ing nonuniform blur kernels and imposing several common
degradations. They also recorded 6D camera trajectories
to generate blur kernels. However, their blurry images and
sharp images are not aligned in the way of our dataset, mak-
ing simple image quality measures such as PSNR and SSIM
less correlated with perceptual quality. Thus, we show qual-
itative comparisons in Fig. 6. Clearly, our results avoid ring-
ing artifacts while preserving details such as wave ripple.

5. Conclusion

In this paper, we proposed a blind deblurring neural net-
work for sharp image estimation. Unlike previous stud-
ies, our model avoids problems related to kernel estimation.
The proposed model follows a coarse-to-ﬁne approach and
is trained in multi-scale space. We also constructed a re-
alistic ground-truth blur dataset, enabling efﬁcient super-
vised learning and rigorous evaluation. Experimental re-
sults show that our approach outperforms the state-of-the-
art methods in both qualitative and quantitative ways while
being much faster.

This project is partially funded by Microsoft Research

Asia.

Table 3. Quantitative comparison on the K¨ohler dataset. The
dataset has its own evaluation code, thus we report multi-scale
SSIM instead of SSIM.

Acknowledgement

[19] R. K¨ohler, M. Hirsch, B. Mohler, B. Sch¨olkopf, and
S. Harmeling.
Recording and playback of camera
shake: Benchmarking blind deconvolution with a real-world
database. In ECCV, pages 27–40. Springer, 2012. 6

[20] W.-S. Lai, J.-B. Huang, Z. Hu, N. Ahuja, and M.-H. Yang. A
comparative study for single image blind deblurring. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 1701–1709, 2016. 8, 16

[21] Y. Li, S. B. Kang, N. Joshi, S. M. Seitz, and D. P. Hutten-
locher. Generating sharp panoramas from motion-blurred
videos. In CVPR, 2010. 2

[22] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In CVPR, pages 3431–
3440, 2015. 5

[23] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale
video prediction beyond mean square error. arXiv preprint
arXiv:1511.05440, 2015. 3, 5

[24] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-
sentation learning with deep convolutional generative adver-
sarial networks. arXiv preprint arXiv:1511.06434, 2015. 6

[25] C. J. Schuler, M. Hirsch, S. Harmeling, and B. Sch¨olkopf.
IEEE transactions on pattern analysis

Learning to deblur.
and machine intelligence, 38(7):1439–1451, 2016. 1, 2, 3

[26] J. Sun, W. Cao, Z. Xu, and J. Ponce. Learning a convolu-
tional neural network for non-uniform motion blur removal.
In CVPR, pages 769–777. IEEE, 2015. 1, 2, 3, 6, 7, 8, 13, 19
[27] Y.-W. Tai, X. Chen, S. Kim, S. J. Kim, F. Li, J. Yang, J. Yu,
Y. Matsushita, and M. S. Brown. Nonlinear camera response
functions and image deblurring: Theoretical analysis and
practice. PAMI, 35(10):2498–2512, 2013. 3

[28] O. Whyte, J. Sivic, A. Zisserman, and J. Ponce. Non-uniform

deblurring for shaken images. 2010. 1

[29] L. Xu, J. S. Ren, C. Liu, and J. Jia. Deep convolutional neural
In Advances in Neural
network for image deconvolution.
Information Processing Systems, pages 1790–1798, 2014. 1,
2

[30] D. Zoran and Y. Weiss. From learning models of natural
image patches to whole image restoration. In ICCV, pages
479–486. IEEE, 2011. 3

References

[1] A. Chakrabarti. A neural approach to blind motion deblur-

ring. In ECCV, 2016. 1, 2

[2] F. Couzinie-Devy, J. Sun, K. Alahari, and J. Ponce. Learning
to estimate and remove non-uniform image blur. In CVPR,
2013. 2

[3] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, pages 248–255. IEEE, 2009. 2

[4] E. L. Denton, S. Chintala, R. Fergus, et al. Deep genera-
tive image models using a laplacian pyramid of adversarial
In Advances in Neural Information Processing
networks.
Systems, pages 1486–1494, 2015. 3, 5, 6

[5] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas,
V. Golkov, P. van der Smagt, D. Cremers, and T. Brox.
Flownet: Learning optical ﬂow with convolutional networks.
In CVPR, pages 2758–2766, 2015. 3

[6] D. Eigen and R. Fergus. Predicting depth, surface normals
and semantic labels with a common multi-scale convolu-
In ICCV, pages 2650–2658, 2015. 3,
tional architecture.
5

[7] D. Eigen, D. Krishnan, and R. Fergus. Restoring an image
taken through a window covered with dirt or rain. In ICCV,
pages 633–640, 2013. 2

[8] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction
from a single image using a multi-scale deep network.
In
Advances in Neural Information Ppocessing Ssytems, pages
2366–2374, 2014. 3, 5

[9] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in Neural Information
Processing Systems, pages 2672–2680, 2014. 2, 6

[10] A. Gupta, N. Joshi, C. L. Zitnick, M. Cohen, and B. Curless.
Single image deblurring using motion density functions. In
ECCV, pages 171–184. Springer, 2010. 1

[11] S. Harmeling, H. Michael, and B. Sch¨olkopf.

Space-
variant single-image blind deconvolution for removing cam-
In Advances in Neural Information Processing
era shake.
Systems, pages 829–837, 2010. 1

[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, pages 770–778, 2016. 4

[13] M. Hirsch, C. J. Schuler, S. Harmeling, and B. Sch¨olkopf.
Fast removal of non-uniform camera shake. In ICCV, 2011.
1, 3

[14] T. H. Kim, B. Ahn, and K. M. Lee. Dynamic scene deblur-

ring. In ICCV, 2013. 1

[15] T. H. Kim and K. M. Lee. Segmentation-free dynamic scene

deblurring. In CVPR, 2014. 1, 6, 7, 8, 13, 19

[16] T. H. Kim and K. M. Lee. Generalized video deblurring for

dynamic scenes. In CVPR, 2015. 2

[17] T. H. Kim, S. Nah, and K. M. Lee. Dynamic scene deblurring
using a locally adaptive linear blur model. arXiv preprint
arXiv:1603.04265, 2016. 2

[18] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. arXiv preprint arXiv:1412.6980, 2014. 6

In this appendix, we present more comparative experimental results to demonstrate the effectiveness of our proposed

A. Appendix

deblurring method.

A.1. Comparison of loss function

In section 3.2, we employed a loss function that combines both the multi-scale content loss (MSE) and the adversarial loss
for training our network. We examine the effect of the adversarial loss term quantitatively and qualitatively. The PSNR and
SSIM results are shown in table A.1. From this results, we observe that adding adversarial loss does not increases PSNR, but
increase SSIM, which means that it encourages to generate more natural and structure preserving images.

Table A.1. Quantitative deblurring performance comparison of loss used to optimize our model (K = 3, λ = 1 × 10−4). Evaluated on
the GOPRO test dataset assuming linear CRF.

Loss

PSNR

SSIM

Lcont(M SE) Lcont + λLadv

28.62

0.9094

28.45

0.9170

Fig. A.1 and A.2 show some qualitative comparisons between the results of our network trained with Lcont and Lcont +

λLadv.

Blurry image

Deblurred image (MSE)

Deblurred image (MSE + Adversarial)

Figure A.1. Visual comparison of results from our model trained with different loss functions. The blurry image is from our proposed
dataset.

Blurry image

Deblurred image (MSE)

Deblurred image (MSE + Adversarial)

Figure A.2. Visual comparison of results from our model trained with different loss functions. The blurry image is from our proposed
dataset.

A.2. Comparison on GOPRO dataset

Lee [15], Sun et al. [26], and ours.

We provide qualitative results on our GOPRO test dataset. Fig. A.3 and A.4 shows the deblurring results of Kim and

Blurry image

Kim and Lee [15]

Sun et al. [27]

Ours

Figure A.3. Visual comparison with other methods. The blurry image is from our proposed dataset.

Blurry image

Kim and Lee [15]

Sun et al. [27]

Ours

Figure A.4. Visual comparison with other methods. The blurry image is from our proposed dataset.

A.3. Comparison on Lai et al. [20] dataset

We provide qualitative results on the dataset of Lai et al. [?]. The Lai et al. dataset is composed of synthetic and real
blurry images, and we showed the deblurring result of a synthetically generated blurry image in section 4.3. We present the
qualitative deblurring results of competing methods on real images in Fig. A.5 and A.6.

Blurry image

Kim and Lee [15]

Sun et al. [27]

Ours

Figure A.5. Visual comparison with other methods. The blurry image is a real image in Lai et al. [?]

Blurry image

Kim and Lee [15]

Sun et al. [27]

Ours

Figure A.6. Visual comparison with other methods. The blurry image is a real image in Lai et al. [?]

A.4. Comparison on real dynamic scenes

Finally, we further present deblurring results on real dynamic scenes. The blurry scenes are captured by a SONY RX100
M4 camera. The qualitative deblurring results of Kim and Lee [15], Sun et al. [26] and ours are compared in Fig. A.7 and
A.8.

Blurry image

Kim and Lee [15]

Sun et al. [27]

Ours

Figure A.7. Visual comparison with other methods. The blurry image is a real dynamic scene.

Blurry image

Kim and Lee [15]

Sun et al. [27]

Ours

Figure A.8. Visual comparison with other methods. The blurry image is a real dynamic scene.

8
1
0
2
 
y
a
M
 
7
 
 
]

V
C
.
s
c
[
 
 
2
v
7
7
1
2
0
.
2
1
6
1
:
v
i
X
r
a

Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring

Seungjun Nah

Tae Hyun Kim

Kyoung Mu Lee

Department of ECE, ASRI, Seoul National University, 08826, Seoul, Korea
{seungjun.nah, lliger9}@gmail.com, kyoungmu@snu.ac.kr

Abstract

blur model [28, 10, 13, 11].

Non-uniform blind deblurring for general dynamic
scenes is a challenging computer vision problem as blurs
arise not only from multiple object motions but also from
camera shake, scene depth variation. To remove these
complicated motion blurs, conventional energy optimiza-
tion based methods rely on simple assumptions such that
blur kernel is partially uniform or locally linear. More-
over, recent machine learning based methods also depend
on synthetic blur datasets generated under these assump-
tions. This makes conventional deblurring methods fail to
remove blurs where blur kernel is difﬁcult to approximate or
parameterize (e.g. object motion boundaries). In this work,
we propose a multi-scale convolutional neural network that
restores sharp images in an end-to-end manner where blur
is caused by various sources. Together, we present multi-
scale loss function that mimics conventional coarse-to-ﬁne
approaches. Furthermore, we propose a new large-scale
dataset that provides pairs of realistic blurry image and the
corresponding ground truth sharp image that are obtained
by a high-speed camera. With the proposed model trained
on this dataset, we demonstrate empirically that our method
achieves the state-of-the-art performance in dynamic scene
deblurring not only qualitatively, but also quantitatively.

1. Introduction

Motion blur is one of the most commonly arising types
of artifacts when taking photos. Shakes of camera and fast
object motions degrade image quality to undesired blurry
images. Furthermore, various causes such as depth varia-
tion, occlusion in motion boundaries make blurs even more
complex. Single image deblurring problem is to estimate
the unknown sharp image given a blurry image. Earlier
studies focused on removing blurs caused by simple transla-
tional or rotational camera motions. More recent works try
to handle general non-uniform blurs caused by depth vari-
ation, camera shakes and object motions in dynamic envi-
ronments. Most of these approaches are based on following

B = KS + n,

(1)

where B, S and n are vectorized blurry image, latent
sharp image, and noise, respectively. K is a large sparse
matrix whose rows each contain a local blur kernel acting
on S to generate a blurry pixel. In practice, blur kernel is
unknown. Thus, blind deblurring methods try to estimate
latent sharp image S and blur kernel K simultaneously.

Finding blur kernel for every pixel is a severely ill-posed
problem. Thus, some approaches tried to parametrize blur
models with simple assumptions on the sources of blurs. In
[28, 10], they assumed that blur is caused by 3D camera
motion only. However, in dynamic scenes, the kernel es-
timation is more challenging as there are multiple moving
objects as well as camera motion. Thus, Kim et al. [14] pro-
posed a dynamic scene deblurring method that jointly seg-
ments and deblurs a non-uniformly blurred image, allowing
the estimation of complex (non-linear) kernel within a seg-
ment. In addition, Kim and Lee [15] approximated the blur
kernel to be locally linear and proposed an approach that es-
timates both the latent image and the locally linear motions
jointly. However, these blur kernel approximations are still
inaccurate, especially in the cases of abrupt motion discon-
tinuities and occlusions. Note that such erroneous kernel
estimation directly affects the quality of the latent image,
resulting in undesired ringing artifacts.

Recently, CNNs (Convolutional Neural Networks) have
been applied in numerous computer vision problems in-
cluding deblurring problem and showed promising results
[29, 25, 26, 1]. Since no pairs of real blurry image and
ground truth sharp image are available for supervised learn-
ing, they commonly used blurry images generated by con-
volving synthetic blur kernels. In [29, 25, 1], synthesized
blur images with uniform blur kernel are used for training.
And, in [26], classiﬁcation CNN is trained to estimate lo-
cally linear blur kernels. Thus, CNN-based models are still
suited only to some speciﬁc types of blurs, and there are
restrictions on more common spatially varying blurs.

1

(a)

(b)

(c)

Figure 1. (a) Input blurry image. (b) Result of Sun et al. [26]. (c) Our deblurring result. Our results show clear object boundaries without
artifacts.

Therefore, all the existing methods still have many prob-
lems before they could be generalized and used in practice.
These are mainly due to the use of simple and unrealis-
tic blur kernel models. Thus, to solve those problems, in
this work, we propose a novel end-to-end deep learning ap-
proach for dynamic scene deblurring.

First, we propose a multi-scale CNN that directly re-
stores latent images without assuming any restricted blur
kernel model. Especially, the multi-scale architecture is
designed to mimic conventional coarse-to-ﬁne optimization
methods. Unlike other approaches, our method does not es-
timate explicit blur kernels. Accordingly, our method is free
from artifacts that arise from kernel estimation errors. Sec-
ond, we train the proposed model with a multi-scale loss
that is appropriate for coarse-to-ﬁne architecture that en-
hances convergence greatly. In addition, we further improve
the results by employing adversarial loss [9]. Third, we pro-
pose a new realistic blurry image dataset with ground truth
sharp images. To obtain kernel model-free dataset for train-
ing, we employ the dataset acquisition method introduced
in [17]. As the blurring process can be modeled by the in-
tegration of sharp images during shutter time [17, 21, 16],
we captured a sequence of sharp frames of a dynamic scene
with a high-speed camera and averaged them to generate a
blurry image by considering gamma correction.

By training with the proposed dataset and adding proper
augmentation, our model can handle general local blur ker-
nel implicitly. As the loss term optimizes the result to
resemble the ground truth, it even restores occluded re-
gions where blur kernel is extremely complex as shown in
Fig. 1. We trained our model with millions of pairs of image
patches and achieved signiﬁcant improvements in dynamic
scene deblurring. Extensive experimental results demon-
strate that the performance of the proposed method is far

superior to those of the state-of-the-art dynamic scene de-
blurring methods in both qualitative and quantitative evalu-
ations.

1.1. Related Works

There are several approaches that employed CNNs for

deblurring [29, 26, 25, 1].

Xu et al. [29] proposed an image deconvolution CNN to
deblur a blurry image in a non-blind setting. They built a
network based on the separable kernel property that the (in-
verse) blur kernel can be decomposed into a small number
of signiﬁcant ﬁlters. Additionally, they incorporated the de-
noising network [7] to reduce visual artifacts such as noise
and color saturation by concatenating the module at the end
of their proposed network.

On the other hand, Schuler et al. [25] proposed a blind
deblurring method with CNN. Their proposed network
mimics conventional optimization-based deblurring meth-
ods and iterates the feature extraction, kernel estimation,
and the latent image estimation steps in a coarse-to-ﬁne
manner. To obtain pairs of sharp and blurry images for net-
work training, they generated uniform blur kernels using a
Gaussian process and synthesized lots of blurry images by
convolving them to the sharp images collected from the Im-
ageNet dataset [3]. However, they reported performance
limits for large blurs due to their suboptimal architecture.

Similarly to the work of Couzinie-Devy et al. [2], Sun
et al. [26] proposed a sequential deblurring approach. First,
they generated pairs of blurry and sharp patches with 73
candidate blur kernels. Next, they trained classiﬁcation
CNN to measure the likelihood of a speciﬁc blur kernel of
a local patch. And then smoothly varying blur kernel is ob-
tained by optimizing an energy model that is composed of
the CNN likelihoods and smoothness priors. Final latent

image estimation is performed with conventional optimiza-
tion method [30].

Note that all these methods require an accurate kernel
estimation step for restoring the latent sharp image. In con-
trast, our proposed model is learned to produce the latent
image directly without estimating blur kernels.

In other computer vision tasks, several forms of coarse-
to-ﬁne architecture or multi-scale architecture were ap-
plied [8, 6, 4, 23, 5]. However, not all multi-scale CNNs
are designed to produce optimal results, similarly to [25].
In depth estimation, optical ﬂow estimation, etc., networks
usually produce outputs having smaller resolution com-
pared to input image resolution [8, 6, 5]. These methods
have difﬁculties in handling long-range dependency even if
multi-scale architecture is used.

Therefore, we make a multi-scale architecture that pre-
serves ﬁne-grained detail information as well as long-range
dependency from coarser scales. Furthermore, we make
sure intermediate level networks help the ﬁnal stage in an
explicit way by training network with multi-scale losses.

1.2. Kernel Free Learning for Dynamic Scene De 

blurring

Conventionally, it was essential to ﬁnd blur kernel before
estimating latent image. CNN based methods were no ex-
ception [25, 26]. However, estimating kernel involves sev-
eral problems. First, assuming simple kernel convolution
cannot model several challenging cases such as occluded re-
gions or depth variations. Second, kernel estimation process
is subtle and sensitive to noise and saturation, unless blur
model is carefully designed. Furthermore, incorrectly esti-
mated kernels give rise to artifacts in latent images. Third,
ﬁnding spatially varying kernel for every pixel in dynamic
scene requires a huge amount of memory and computation.
Therefore, we adopt kernel-free methods in both blur
dataset generation and latent image estimation. In blurry
image generation, we follow to approximate camera imag-
ing process, rather than assuming speciﬁc motions, instead
of ﬁnding or designing complex blur kernel. We capture
successive sharp frames and integrate to simulate blurring
process. The detailed procedure is described in section 2.
Note that our dataset is composed of blurry and sharp image
pairs only, and that the local kernel information is implic-
itly embedded in it. In Fig. 2, our kernel-free blurry image is
compared with a conventional synthesized image with uni-
form blur kernel. Notably, the blur image generated by our
method exhibits realistic and spatially varying blurs caused
by the moving person and the static background, while the
blur image synthesized by conventional method does not.
For latent image estimation, we do not assume blur sources
and train the model solely on our blurry and sharp image
pairs. Thus, our proposed method does not suffer from
kernel-related problems in deblurring.

2. Blur Dataset

Instead of modeling a kernel to convolve on a sharp im-
age, we choose to record the sharp information to be inte-
grated over time for blur image generation. As camera sen-
sor receives light during the exposure, sharp image stimu-
lation at every time is accumulated, generating blurry im-
age [13]. The integrated signal is then transformed into
pixel value by nonlinear CRF (Camera Response Function).
Thus, the process could be approximated by accumulating
signals from high-speed video frames.

Blur accumulation process can be modeled as follows.

B = g

S(t)dt

≃ g

1
T

 

T

t=0

Z

1
M

 

M−1

i=0
X

!

S[i]

,

(2)

!

where T and S(t) denote the exposure time and the sen-
sor signal of a sharp image at time t, respectively. Simi-
larly, M , S[i] are the number of sampled frames and the
i-th sharp frame signal captured during the exposure time,
respectively. g is the CRF that maps a sharp latent signal
S(t) into an observed image ˆS(t) such that ˆS(t) = g(S(t)),
or ˆS[i] = g(S[i]). In practice, we only have observed video
frames while the original signal and the CRF is unknown.

It is known that non-uniform deblurring becomes signif-
icantly difﬁcult when nonlinear CRF is involved, and non-
linearity should be taken into account. However, currently,
there are no CRF estimation techniques available for an im-
age with spatially varying blur [27]. When the ground truth
CRF is not given, a common practical method is to approxi-
mate CRF as a gamma curve with γ = 2.2 as follows, since
it is known as an aproximated average of known CRFs [27].

g(x) = x1/γ.

(3)

Thus, by correcting the gamma function, we obtain the
latent frame signal S[i] from the observed image ˆS[i] by
S[i] = g−1( ˆS[i]), and then synthesize the corresponding
blur image B by using (2).

We used GOPRO4 Hero Black camera to generate our
dataset. We took 240 fps videos with GOPRO camera and
then averaged varying number (7 - 13) of successive latent
frames to produce blurs of different strengths. For example,
averaging 15 frames simulates a photo taken at 1/16 shut-
ter speed, while corresponding sharp image shutter speed
is 1/240. Notably, the sharp latent image corresponding to
each blurry one is deﬁned as the mid-frame among the sharp
frames that are used to make the blurry image. Finally, our
dataset is composed of 3214 pairs of blurry and sharp im-
ages at 1280x720 resolution. The proposed GOPRO dataset
is publicly available on our website 1.

1https://github.com/SeungjunNah/DeepDeblur_release

(a)

(b)

(c)

Figure 2. (a) Ground truth sharp image. (b) Blurry image generated by convolving a uniform blur kernel. (c) Blurry image by averaging
sharp frames. In this case, blur is mostly caused by person motion, leaving the background as it is. The blur kernel is non-uniform, complex
shaped. However, when the blurry image is synthesized by convolution with a uniform kernel, the background also gets blurred as if blur
was caused by camera shake. To model dynamic scene blur, our kernel-free method is required.

INPUT

CONV

BN

ReLU

CONV

BN

ReLU

OUTPUT

(a)

3. Proposed Method

In our model, ﬁner scale image deblurring is aided by
coarser scale features. To exploit coarse and middle level
information while preserving ﬁne level information at the
same time, input and output to our network take the form of
Gaussian pyramids. Note that most of other coarse-to-ﬁne
networks take a single image as input and output.

3.1. Model Architecture

In addition to the multi-scale architecture, we employ a
slightly modiﬁed version of residual network structure [12]
as a building block of our model. Using residual network
structure enables deeper architecture compared to a plain
CNN. Also, as blurry and sharp image pairs are similar in
values, it is efﬁcient to let parameters learn the difference
only. We found that removing the rectiﬁed linear unit af-
ter the shortcut connection of the original residual building
block boosts the convergence speed at training time. We de-
note the modiﬁed building block as ResBlock. The original
and our modiﬁed building block are compared in Fig. 3.

By stacking enough number of convolution layers with
ResBlocks, the receptive ﬁeld at each scale is expanded.
Details are described in the following paragraphs. For sake
of consistency, we deﬁne scale levels in the order of de-
creasing resolution (i.e.
level 1 for ﬁnest scale). Unless
denoted otherwise, we use total K = 3 scales. At training
time, we set the resolution of the input and output Gaussian
pyramid patches to be {256 × 256, 128 × 128, 64 × 64}.
The scale ratio between consecutive scales is 0.5. For all
convolution layers, we set the ﬁlter size to be 5 × 5. As our
model is fully convolutional, at test time, the patch size may
vary as the GPU memory allows. The overall architecture
is shown in Fig. 4.

INPUT

CONV

ReLU

CONV

OUTPUT

(b)

Figure 3. (a) Original residual network building block. (b) Mod-
iﬁed building block of our network. We did not use batch nor-
malization layers since we trained model with mini-batch of size
2, which is smaller than usual for batch normalization. We found
removing rectiﬁed linear unit just before the block output is bene-
ﬁcial in terms of performance empirically.

(cid:1776)(cid:2778)

(cid:1776)(cid:2779)

(cid:1776)(cid:2780)

CONV

ResBlock

ResBlock

CONV

CONV

ResBlock

ResBlock

CONV

. . .

Up
Conv

. . .

Up
Conv

. . .

Backprop

(cid:1786)(cid:2778)

(cid:1786)(cid:2779)

(cid:1793)(cid:2778)

(cid:1793)(cid:2779)

Backprop

Backprop

CONV

ResBlock

ResBlock

CONV

Figure 4. Multi-scale network architecture. Bk, Lk, Sk denote blurry and latent, and ground truth sharp images, respectively. Subscript
k denotes k-th scale level in the Gaussian pyramid, which is downsampled to 1/2k scale. Our model takes a blurry image pyramid as the
input and outputs an estimated latent image pyramid. Every intermediate scale output is trained to be sharp. At test time, original scale
image is chosen as the ﬁnal result.

(cid:1786)(cid:2780)

(cid:1793)(cid:2780)

Coarsest level network

Finer level network

At the front of the network locates the coarsest level net-
work. The ﬁrst convolution layer transforms 1/4 resolu-
tion, 64 × 64 size image into 64 feature maps. Then, 19
ResBlocks are stacked followed by last convolution layer
that transforms the feature map into input dimension. Every
convolution layer preserves resolution with zero padding. In
total, there are 40 convolution layers. The number of con-
volution layers at each scale level is determined so that total
model should have 120 convolution layers. Thus, the coars-
est level network has receptive ﬁeld large enough to cover
the whole patch. At the end of the stage, the coarsest level
latent sharp image is generated. Moreover, information
from the coarsest level output is delivered to the next stage
where ﬁner scale network is. To convert a coarsest output
to ﬁt the input size of the next ﬁner scale, the output patch
passes an upconvolution [22] layer, while other multi-scale
methods use reshaping [8] or upsampling [4, 6, 23]. Since
the sharp and blurry patches share low-frequency informa-
tion, learning suitable feature with upconvolution helps to
remove redundancy. In our experiment, using upconvolu-
tion showed better performance than upsampling. Then, the
upconvolution feature is concatenated with the ﬁner scale
blurry patch as an input.

Finer level networks basically have the same structure as in
the coarsest level network. However, the ﬁrst convolution
layer takes the sharp feature from the previous stage as well
as its own blurry input image, in a concatenated form. Every
convolution ﬁlter size is 5 × 5 with the same number of
feature maps as in the coarsest level. Except for the last
ﬁnest scale, there is an upconvolution layer before the next
stage. At the ﬁnest scale, the original resolution sharp image
is restored.

3.2. Training

Our model is trained on the proposed GOPRO dataset.
Among 3214 pairs, 2103 pairs were used for training and
remainings were used for the test. To prevent our network
from overﬁtting, several data augmentation techniques are
involved. In terms of geometric transformations, patches
are randomly ﬂipped horizontally and vertically, rotated by
90 degrees. For color, RGB channels are randomly per-
muted. To take image degradations into account, satura-
tion in HSV colorspace is multiplied by a random num-
ber within [0.5, 1.5]. Also, Gaussian random noise is
added to blurry images. To make our network be robust
against different strengths of noise, standard deviation of

noise is also randomly sampled from Gaussian distribution,
N (0, (2/255)2). Then, value outside [0, 1] is clipped. Fi-
nally, 0.5 is subtracted to set input and output value range
zero-centered, having range [-0.5, 0.5].

In optimizing the network parameters, we trained the
model in a combination of two losses, multi-scale content
loss and adversarial loss.

Multi-scale content loss

Basically, the coarse-to-ﬁne approach desires that every in-
termediate output becomes the sharp image of the corre-
sponding scale. Thus, we train our network so that inter-
mediate outputs should form a Gaussian pyramid of sharp
images. MSE criterion is applied to every level of pyramids.
Hence, the loss function is deﬁned as follows:

Lcont =

kLk − Skk2,

(4)

1
2K

K

Xk=1

1
ckwkhk

where Lk, Sk denote the model output and ground truth im-
age at scale level k, respectively. The loss at each scale is
normalized by the number of channels ck, width wk, and
the height hk (i.e. the total number of elements).

Adversarial loss

Recently, adversarial networks are reported to generate
sharp realistic images [9, 4, 24]. Following the architec-
ture introduced in [24], we build discriminator as in Ta-
ble 1. Discriminator takes the output of the ﬁnest scale or
the ground truth sharp image as input and classiﬁes if it is
deblurred image or sharp image.

The adversarial loss is deﬁned as follows.

Ladv =

[log D(S)]+

E
S∼psharp(S)
E
B∼pblurry (B)

[log(1 − D(G(B)))],

(5)

where G and D denote the generator, that is our multi-
scale deblurring network in Fig. 4 and the discriminator
(classiﬁer), respectively. When training, G tries to minimize
the adversarial loss while D tries to maximize it.

Finally, by combining the multi-scale content loss and
adversarial loss, the generator network and discriminator
network is jointly trained. Thus, our ﬁnal loss term is

#

1

2

3

4

5

6

7

8

9

10

11

Layer Weight dimension

Stride

conv

conv

conv

conv

conv

conv

conv

conv

conv

fc

32 × 3 × 5 × 5

64 × 32 × 5 × 5

64 × 64 × 5 × 5

128 × 64 × 5 × 5

128 × 128 × 5 × 5

256 × 128 × 5 × 5

256 × 256 × 5 × 5

512 × 256 × 5 × 5

512 × 512 × 4 × 4

512 × 1 × 1 × 1

sigmoid

-

2

1

2

1

4

1

4

1

4

-

-

Table 1. Model parameters of the discriminator. Every convolution
layers are activated with LeakyReLU layer.

rate is decreased to 1/10 of the previous learning rate. Total
training takes 9 × 105 iterations to converge.

4. Experimental Results

We implemented our model with torch7 library. All the
following experiments were performed in a desktop with i7-
6700K CPU and NVIDIA GTX Titan X (Maxwell) GPU.

4.1. GOPRO Dataset

We evaluate the performance of our model in the pro-
posed GOPRO dataset. Our test dataset consists of 1111
pairs, which is approximately 1/3 of the total dataset. We
compare the results with those of the state-of-the-art meth-
ods [15, 26] in both qualitative and quantitative ways. Our
results show signiﬁcant improvement in terms of image
quality. Some deblurring results are shown in Fig. 5. We no-
tice from the results of Sun et al. [26], deblurring is not suc-
cessful on the regions where blurs are nonlinearly shaped or
located at the boundary of motion. Kim and Lee [15]’s re-
sults also fail in cases where strong edges are not found. In
contrast, our results are free from those kernel-estimation
related problems. Table 2, shows the quantitative evalua-
tion results of the competing methods and ours with differ-
ent scale level k in terms of PSNR, SSIM over the test data.
Also, the runtime is compared. We observe that our system
with K = 2 produces the best results in terms of both PSNR
and SSIM, while K = 3 is the fastest.

Ltotal = Lcont + λ × Ladv,
where the weight constant λ = 1 × 10−4.
We used ADAM [18] optimizer with a mini-batch size
2 for training. The learning rate is adaptively tuned begin-
ning from 5 × 10−5. After 3 × 105 iterations, the learning

(6)

4.2. K¨ohler Dataset

K¨ohler dataset [19] consists of 4 latent images and 12
differently blurred images for each of them. The blurs are
caused by replaying recorded 6D camera motion, assum-
ing linear CRF. We report the quantitative results on this

Figure 5. Test results on the GOPRO dataset. From top to bottom: Blurry images, results of Sun et al. [26], results of Kim and Lee [15],
and results of the proposed method.

Figure 6. Deblurring results on the dataset [20]. The top row shows results of results of Sun et al. [26] and the bottom row shows our
results.

Ours

4.3. Dataset of Lai et al.

Measure

[26]

[15]

K = 1 K = 2 K = 3

PSNR

SSIM

24.64

23.64

28.93

29.23

29.08

0.8429

0.8239

0.9100

0.9162

0.9135

Runtime

20 min

1 hr

7.21 s

4.33 s

3.09 s

Table 2. Quantitative deblurring performance comparison on the
GOPRO dataset. K denotes the scale level.

dataset in Table 3. Our model is trained by setting g as iden-
tity function in (2). We note that our system with K = 3
produces the best results in PSNR, and the system K = 2
exhibits the best MSSIM result.

Measure

[26]

[15]

Ours

K = 1 K = 2 K = 3

PSNR

25.22

24.68

25.74

26.02

26.48

MSSIM 0.7735

0.7937

0.8042

0.8116

0.8079

Lai et al.

[20] generated synthetic dataset by convolv-
ing nonuniform blur kernels and imposing several common
degradations. They also recorded 6D camera trajectories
to generate blur kernels. However, their blurry images and
sharp images are not aligned in the way of our dataset, mak-
ing simple image quality measures such as PSNR and SSIM
less correlated with perceptual quality. Thus, we show qual-
itative comparisons in Fig. 6. Clearly, our results avoid ring-
ing artifacts while preserving details such as wave ripple.

5. Conclusion

In this paper, we proposed a blind deblurring neural net-
work for sharp image estimation. Unlike previous stud-
ies, our model avoids problems related to kernel estimation.
The proposed model follows a coarse-to-ﬁne approach and
is trained in multi-scale space. We also constructed a re-
alistic ground-truth blur dataset, enabling efﬁcient super-
vised learning and rigorous evaluation. Experimental re-
sults show that our approach outperforms the state-of-the-
art methods in both qualitative and quantitative ways while
being much faster.

This project is partially funded by Microsoft Research

Asia.

Table 3. Quantitative comparison on the K¨ohler dataset. The
dataset has its own evaluation code, thus we report multi-scale
SSIM instead of SSIM.

Acknowledgement

[19] R. K¨ohler, M. Hirsch, B. Mohler, B. Sch¨olkopf, and
S. Harmeling.
Recording and playback of camera
shake: Benchmarking blind deconvolution with a real-world
database. In ECCV, pages 27–40. Springer, 2012. 6

[20] W.-S. Lai, J.-B. Huang, Z. Hu, N. Ahuja, and M.-H. Yang. A
comparative study for single image blind deblurring. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 1701–1709, 2016. 8, 16

[21] Y. Li, S. B. Kang, N. Joshi, S. M. Seitz, and D. P. Hutten-
locher. Generating sharp panoramas from motion-blurred
videos. In CVPR, 2010. 2

[22] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In CVPR, pages 3431–
3440, 2015. 5

[23] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale
video prediction beyond mean square error. arXiv preprint
arXiv:1511.05440, 2015. 3, 5

[24] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-
sentation learning with deep convolutional generative adver-
sarial networks. arXiv preprint arXiv:1511.06434, 2015. 6

[25] C. J. Schuler, M. Hirsch, S. Harmeling, and B. Sch¨olkopf.
IEEE transactions on pattern analysis

Learning to deblur.
and machine intelligence, 38(7):1439–1451, 2016. 1, 2, 3

[26] J. Sun, W. Cao, Z. Xu, and J. Ponce. Learning a convolu-
tional neural network for non-uniform motion blur removal.
In CVPR, pages 769–777. IEEE, 2015. 1, 2, 3, 6, 7, 8, 13, 19
[27] Y.-W. Tai, X. Chen, S. Kim, S. J. Kim, F. Li, J. Yang, J. Yu,
Y. Matsushita, and M. S. Brown. Nonlinear camera response
functions and image deblurring: Theoretical analysis and
practice. PAMI, 35(10):2498–2512, 2013. 3

[28] O. Whyte, J. Sivic, A. Zisserman, and J. Ponce. Non-uniform

deblurring for shaken images. 2010. 1

[29] L. Xu, J. S. Ren, C. Liu, and J. Jia. Deep convolutional neural
In Advances in Neural
network for image deconvolution.
Information Processing Systems, pages 1790–1798, 2014. 1,
2

[30] D. Zoran and Y. Weiss. From learning models of natural
image patches to whole image restoration. In ICCV, pages
479–486. IEEE, 2011. 3

References

[1] A. Chakrabarti. A neural approach to blind motion deblur-

ring. In ECCV, 2016. 1, 2

[2] F. Couzinie-Devy, J. Sun, K. Alahari, and J. Ponce. Learning
to estimate and remove non-uniform image blur. In CVPR,
2013. 2

[3] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, pages 248–255. IEEE, 2009. 2

[4] E. L. Denton, S. Chintala, R. Fergus, et al. Deep genera-
tive image models using a laplacian pyramid of adversarial
In Advances in Neural Information Processing
networks.
Systems, pages 1486–1494, 2015. 3, 5, 6

[5] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas,
V. Golkov, P. van der Smagt, D. Cremers, and T. Brox.
Flownet: Learning optical ﬂow with convolutional networks.
In CVPR, pages 2758–2766, 2015. 3

[6] D. Eigen and R. Fergus. Predicting depth, surface normals
and semantic labels with a common multi-scale convolu-
In ICCV, pages 2650–2658, 2015. 3,
tional architecture.
5

[7] D. Eigen, D. Krishnan, and R. Fergus. Restoring an image
taken through a window covered with dirt or rain. In ICCV,
pages 633–640, 2013. 2

[8] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction
from a single image using a multi-scale deep network.
In
Advances in Neural Information Ppocessing Ssytems, pages
2366–2374, 2014. 3, 5

[9] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in Neural Information
Processing Systems, pages 2672–2680, 2014. 2, 6

[10] A. Gupta, N. Joshi, C. L. Zitnick, M. Cohen, and B. Curless.
Single image deblurring using motion density functions. In
ECCV, pages 171–184. Springer, 2010. 1

[11] S. Harmeling, H. Michael, and B. Sch¨olkopf.

Space-
variant single-image blind deconvolution for removing cam-
In Advances in Neural Information Processing
era shake.
Systems, pages 829–837, 2010. 1

[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, pages 770–778, 2016. 4

[13] M. Hirsch, C. J. Schuler, S. Harmeling, and B. Sch¨olkopf.
Fast removal of non-uniform camera shake. In ICCV, 2011.
1, 3

[14] T. H. Kim, B. Ahn, and K. M. Lee. Dynamic scene deblur-

ring. In ICCV, 2013. 1

[15] T. H. Kim and K. M. Lee. Segmentation-free dynamic scene

deblurring. In CVPR, 2014. 1, 6, 7, 8, 13, 19

[16] T. H. Kim and K. M. Lee. Generalized video deblurring for

dynamic scenes. In CVPR, 2015. 2

[17] T. H. Kim, S. Nah, and K. M. Lee. Dynamic scene deblurring
using a locally adaptive linear blur model. arXiv preprint
arXiv:1603.04265, 2016. 2

[18] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. arXiv preprint arXiv:1412.6980, 2014. 6

In this appendix, we present more comparative experimental results to demonstrate the effectiveness of our proposed

A. Appendix

deblurring method.

A.1. Comparison of loss function

In section 3.2, we employed a loss function that combines both the multi-scale content loss (MSE) and the adversarial loss
for training our network. We examine the effect of the adversarial loss term quantitatively and qualitatively. The PSNR and
SSIM results are shown in table A.1. From this results, we observe that adding adversarial loss does not increases PSNR, but
increase SSIM, which means that it encourages to generate more natural and structure preserving images.

Table A.1. Quantitative deblurring performance comparison of loss used to optimize our model (K = 3, λ = 1 × 10−4). Evaluated on
the GOPRO test dataset assuming linear CRF.

Loss

PSNR

SSIM

Lcont(M SE) Lcont + λLadv

28.62

0.9094

28.45

0.9170

Fig. A.1 and A.2 show some qualitative comparisons between the results of our network trained with Lcont and Lcont +

λLadv.

Blurry image

Deblurred image (MSE)

Deblurred image (MSE + Adversarial)

Figure A.1. Visual comparison of results from our model trained with different loss functions. The blurry image is from our proposed
dataset.

Blurry image

Deblurred image (MSE)

Deblurred image (MSE + Adversarial)

Figure A.2. Visual comparison of results from our model trained with different loss functions. The blurry image is from our proposed
dataset.

A.2. Comparison on GOPRO dataset

Lee [15], Sun et al. [26], and ours.

We provide qualitative results on our GOPRO test dataset. Fig. A.3 and A.4 shows the deblurring results of Kim and

Blurry image

Kim and Lee [15]

Sun et al. [27]

Ours

Figure A.3. Visual comparison with other methods. The blurry image is from our proposed dataset.

Blurry image

Kim and Lee [15]

Sun et al. [27]

Ours

Figure A.4. Visual comparison with other methods. The blurry image is from our proposed dataset.

A.3. Comparison on Lai et al. [20] dataset

We provide qualitative results on the dataset of Lai et al. [?]. The Lai et al. dataset is composed of synthetic and real
blurry images, and we showed the deblurring result of a synthetically generated blurry image in section 4.3. We present the
qualitative deblurring results of competing methods on real images in Fig. A.5 and A.6.

Blurry image

Kim and Lee [15]

Sun et al. [27]

Ours

Figure A.5. Visual comparison with other methods. The blurry image is a real image in Lai et al. [?]

Blurry image

Kim and Lee [15]

Sun et al. [27]

Ours

Figure A.6. Visual comparison with other methods. The blurry image is a real image in Lai et al. [?]

A.4. Comparison on real dynamic scenes

Finally, we further present deblurring results on real dynamic scenes. The blurry scenes are captured by a SONY RX100
M4 camera. The qualitative deblurring results of Kim and Lee [15], Sun et al. [26] and ours are compared in Fig. A.7 and
A.8.

Blurry image

Kim and Lee [15]

Sun et al. [27]

Ours

Figure A.7. Visual comparison with other methods. The blurry image is a real dynamic scene.

Blurry image

Kim and Lee [15]

Sun et al. [27]

Ours

Figure A.8. Visual comparison with other methods. The blurry image is a real dynamic scene.

