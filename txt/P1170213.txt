8
1
0
2
 
v
o
N
 
0
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
7
7
2
4
0
.
1
1
8
1
:
v
i
X
r
a

Anomaly Detection via Graphical Lasso

Haitao Liu1, Randy C. Paﬀenroth 2, Jian Zou 2, and Chong Zhou1

1 Data Science Program
2 Mathematical Sciences
{hliu5,cpaffenroth,jzou,czhou2}@wpi.edu
Worcester Polytechnic Institute

Abstract

Anomalies and outliers are common in real-world data, and they can arise from
many sources, such as sensor faults. Accordingly, anomaly detection is important
both for analyzing the anomalies themselves and for cleaning the data for further
analysis of its ambient structure. Nonetheless, a precise deﬁnition of anomalies is
important for automated detection and herein we approach such problems from
the perspective of detecting sparse latent eﬀects embedded in large collections of
noisy data. Standard Graphical Lasso based techniques can identify the condi-
tional dependency structure of a collection of random variables based on their
sample covariance matrix. However, classic Graphical Lasso is sensitive to out-
liers in the sample covariance matrix. In particular, several outliers in a sample
covariance matrix can destroy the sparsity of its inverse. Accordingly, we propose
a novel optimization problem that is similar in spirit to Robust Principal Compo-
nent Analysis (RPCA) and splits the sample covariance matrix M into two parts,
M = F + S, where F is the cleaned sample covariance whose inverse is sparse and
computable by Graphical Lasso, and S contains the outliers in M . We accomplish
this decomposition by adding an additional (cid:96)1 penalty to classic Graphical Lasso,
and name it “Robust Graphical Lasso (Rglasso)”. Moreover, we propose an Al-
ternating Direction Method of Multipliers (ADMM) solution to the optimization
problem which scales to large numbers of unknowns. We evaluate our algorithm
on both real and synthetic datasets, obtaining interpretable results and outper-
forming the standard robust Minimum Covariance Determinant (MCD) method
and Robust Principal Component Analysis (RPCA) regarding both accuracy and
speed.

Keywords: Anomaly Detection, Graphical Lasso, Robust PCA, Latent Structure

1

Introduction

Gaussian Graphical Models (GGMs) are widely used to study network structures and
collections of noisy data [Whittaker, 2009, Lauritzen, 1996]. Such models employ an
undirected graph to represent random variables and their relationships. If an entry Θij

1

of the inverse of the covariance matrix of a collection of random variables (also called the
information matrix or the precision matrix) is 0, then variables i and j are conditionally
linearly independent (i.e., linearly independent given all the other variables), and this fact
is represented graphically by having no edge between the nodes representing variables
In other words,
i and j [Hammersley and Cliﬀord, 1971, Speed and Kiiveri, 1986].
while a covariance matrix encodes the linear predictability between pairs of random
variables, the information matrix encodes the improvement in linear predictability by
the addition of the given random variable, above and beyond that provided by the other
random variables. Such graphical models are used in many application domains including
causal inference, image processing, computing vision, speech recognition, analyzing gene
networks, and ﬁnancial analytics, to name but a few [Liu et al., 2017, Zou and Huang,
2016].

However, when the data is high-dimensional and noisy, as is often the case in many
modern applications, a direct inversion of the sample covariance matrix will often lead
to a dense graph structure that masks the underlying sparse conditional dependencies
[Meinshausen and B¨uhlmann, 2006, Yuan and Lin, 2007]. Graphical Lasso [Friedman
et al., 2008, Danaher et al., 2014, Yang et al., 2015, Hallac et al., 2017b,a] is the most pop-
ular model for recovering sparse information matrices from noisy data, and it proceeds by
balancing a maximum likelihood principle against a sparsity-inducing (cid:96)1 regularization.
However, Graphical Lasso requires an uncorrupted sample covariance matrix as in-
put, and even a few large anomalies (as opposed to small noise) can mask the sparsity
structure of the information matrix. Accordingly, in this paper, we provide a novel
framework to identify such sparsity structures that represent latent relationships.
In
particular, our work can be thought of as a generalization of current work in Robust
Principal Component Analysis (RPCA) [Cand`es et al., 2011, Wright et al., 2009, Lin
et al., 2010, Paﬀenroth et al., 2013, Sun et al., 2013] to problems of conditional de-
pendency. At a high level, RPCA proceeds based on the observation that a sum of a
low-rank matrix and a sparse matrix is, generally speaking, neither low-rank nor sparse.
However, under appropriate regularity conditions, one can detect whether a given ma-
trix M = L + S is a sum of a low-rank matrix L and a sparse matrix S, and compute a
unique decomposition of this form. Similarly, the sum of a sparse matrix and a matrix
whose inverse is sparse is, generally speaking, a matrix which is neither sparse nor has a
sparse inverse. However, as we demonstrate here, one can detect whether a given matrix
M = S1 + S−1
is a sum of a sparse matrix S1 and a matrix F whose inverse is sparse
2
S2 = F −1, and compute a unique decomposition of this form. Finally, we demonstrate
the interpretability of the results of applying our proposed algorithm on high-frequency
ﬁnancial data.

1.1 Related Work

Covariance estimation is very sensitive to outliers [Johnstone, 2001] and some researchers
have estimated robust covariance matrices based on the Mahalanobis distances or shape
bias of a range of existing robust covariance matrix estimators [Rousseeuw and Driessen,
1999, Filzmoser et al., 2014, Hubert et al., 2014, Yanyuan Ma, 2001]. Some other robust
covariance estimation procedures are based on cell-wise contamination data [Alqallaf
et al., 2009, ¨Ollerer and Croux, 2015, Tarr et al., 2016] and these estimated robust

2

covariance matrices are used as input to Graphical Lasso. Unlike these two-stage pro-
cedures, some scholars estimate the information matrix directly through a one-stage
optimization with the assumption that the data follows certain distributions [Yang and
Lozano, 2015, Hirose et al., 2017, Finegold and Drton, 2009]. However, the aforemen-
tioned robust procedures for Graphical Lasso discard outliers and focus on the estimation
of covariance or information matrices rather than on anomaly detection. The minimum
covariance determinate (MCD) estimator is employed to construct robust Mahalanobis
distances to identify local multivariate outliers [Filzmoser et al., 2014] and this is the
method in the literature closest to the methods we propose. Another close method
is RPCA, which decomposes the raw data matrix into a low-rank matrix L represent-
ing the cleaned data and a sparse matrix S containing the outliers. In this paper, we
propose a novel one-stage model to detect the outliers, which contain the hidden cor-
relations among the variables. We demonstrate the eﬀectiveness and eﬃciency of our
model on three synthetic datasets, and superiority of our method over the MCD and
RPCA method.

1.2 Contribution

In this paper, we propose a novel algorithm that combines ideas from Graphical Lasso
and RPCA to detect anomalies in large collections of noisy data.
In particular, our
algorithm is similar to RPCA, in that we split the observed sample covariance matrix
M into two parts, M = F + S1, where F is the cleaned sample covariance matrix used as
the input to Graphical Lasso and S1 contains the outliers in M . However, our method
diﬀers from extent approaches in that we leverage an additional (cid:96)1 penalty applied to
the input sample covariance matrix (as inspired by RPCA) in addition to the (cid:96)1 penalty
classically applied by Graphical Lasso to the computed information matrix. We call our
new model the “Robust Graphical Lasso (Rglasso)”, and it has several advantages over
current approaches.

• Our method provides a robust procedure that provides a clean sample covariance
to Graphical Lasso even when the original sample covariance matrix has been
contaminated by large outliers.

• Our method can be implemented using a fast Alternating Direction Method of
Multipliers (ADMM) [Boyd et al., 2011] optimization procedure that scales to
large problems with many random variables.

• Unlike Robust Principal Component Analysis (RPCA), we decompose the raw
data matrix into a sparse matrix S1 containing the outliers and a full-rank matrix
F , whose inverse is a sparse matrix S2 = F −1.

2 Background

In this section, we provide some key ideas from Robust Principal Component Analysis
that computes a unique decomposition of a matrix in the form of a sum of a low-rank
matrix and a sparse matrix. We then show how this framework can be generalized

3

to conditional dependency learning and Graphic Lasso, which is a popular model for
retrieving sparse conditionally dependencies in the literature.

Notation Throughout this paper, we use the following notational conventions: Σ is a
covariance matrix, Θ = Σ−1 is an information matrix, X are vectors, V and E are sets,
and L, S and M are matrices.

2.1 Robust Principal Component Analysis

Robust Principal Component Analysis (RPCA) is an extension of Principal Component
Analysis (PCA) that attempts to reduce the sensitivity of PCA to outliers by decom-
posing the raw data into outliers and the rest of the data [Hotelling, 1933, Eckart and
Young, 1936, Jolliﬀe, 1986]. After decomposition, the cleaned data can be accurately
approximated by a low-rank subspace [Cand`es et al., 2011]. In particular, the decom-
position of RPCA is computed by solving the following optimization problem [Cand`es
et al., 2011]:

minimize (cid:107)L(cid:107)∗ + λ(cid:107)S(cid:107)1
s.t. M = L + S,

(1)

where the matrix M is the input data, the matrix L is a low-rank matrix representing
the cleaned data, the matrix S contains element-wise outliers, (cid:107) · (cid:107)∗ is the nuclear norm
(i.e., the sum of the singular values of the matrix) and (cid:107) · (cid:107)1 is the one norm (i.e., the
sum of the absolute values of the entries). RPCA can detect whether a given matrix
M is a sum of a low-rank matrix L and a sparse matrix S by minimizing the nuclear
norm of L and the (cid:96)1 norm of S (1). The nuclear norm encourages the low-rankness of
L by summing the singular values of L. The (cid:96)1 norm encourages the sparsity of S by
summing of the absolute values of the entries. The unique decomposition of this form
can be achieved without knowing the true rank of L and support of S, assuming some
mild regularity conditions [Cand`es et al., 2011, Lin et al., 2010, Wright et al., 2009,
Paﬀenroth et al., 2013]. Our description of RPCA follows the notation and outline of
Zhou and Paﬀenroth [Zhou and Paﬀenroth, 2017].

As we will detail, our problem is similar to, but more complicated than problem
(1). However, the more complicated problem can still be solved using a combination
of ideas involving Alternating Direction Method of Multipliers [Boyd et al., 2011] and
other oﬀ-the-shelf solvers such as the proximal solvers for the (cid:96)1 penalties [Parikh et al.,
2014].

2.2 Graphical Lasso

Gaussian Graphical models [Speed and Kiiveri, 1986] estimate conditional dependency of
a collection of random variables that are represented by an undirected graph G = (V, E),
where V contains p vertices corresponding to p random variables and E is an edge
set E = ei,j ∈ V|i (cid:54)= j describing the conditional independence relationship among the
random variables. Any absent edge between vi and vj, which indicates the variables
X (i) and X (j) are conditionally independent, corresponds to a zero in the inverse of the
covariance matrix of the variables (also called the information matrix) [Whittaker, 2009,
Lauritzen, 1996]. Such conditional independence is computed by way of the Gaussian

4

(2)

(3)

likelihood estimation in the following optimization problem [Yuan and Lin, 2007]:

arg min
Θ(cid:31)0

− log|Θ| + tr(M Θ),

where log| · | is the log determinant, tr(·) denotes the trace, M is the observed sample
covariance matrix, and Θ is the information matrix which is positive deﬁnite.

The standard approach to enforcing the sparsity of the information matrix is to add
an (cid:96)1 penalty to the information matrix to form a likelihood-penalty estimation, namely
Graphic Lasso [Friedman et al., 2008], as in the following optimization problem:

arg min
Θ(cid:31)0

− log|Θ| + tr(M Θ) + ρ(cid:107)Θ(cid:107)1,

where ρ controls the sparsity of the estimated information matrix Θ. When ρ equals
zero, the problem is equivalent to the maximum log-likelihood function (2). As ρ grows,
more entries in the information matrix Θ are encouraged to be zero, which leads to a
more sparse graph structure. When ρ goes to inﬁnity, all the oﬀ-diagonal entries in the
information matrix will be zero, meaning all the variables are mutually independent of
each other. 1

3 Problem Deﬁnition

Consider multivariate observations sampled from a distribution x ∼ N (0, Σ0), where Σ0
is the contaminated population covariance matrix. With such samples, we can compute
the observed contaminated sample covariance matrix M . In some applications, M is
provided directly, and we don’t need to compute it from the original data. Such an ob-
served corrupted sample covariance matrix M is not a good estimator of the covariance
matrix of the population anymore, and cannot be processed by Graphical Lasso since
Graphical Lasso is very sensitive to the outliers in the sample covariance matrix.
In
particular, even a few large anomalies can mask the underlying sparsity structure of the
information matrix. Here, we aim to detect the sparse latent eﬀects embedded in the
observed corrupted sample covariance M rather than estimate the underlying covariance
matrix Σ and its inverse Θ = Σ−1. Accordingly, we formulate the optimization problem
to detect such hidden eﬀects by a combination of RPCA and Graphic Lasso. Such a
combination takes advantage of the anomaly detection ability of RPCA to give a clean
sample covariance matrix as input to Graphic Lasso. Just as in RPCA, our method
isolates the anomalies in the corrupted sample covariance matrix into a sparse matrix,
and the remaining part is noise-free, whose inverse has a clear sparse structure. Specif-
ically, we split a corrupted sample covariance matrix M into two parts M = F + S,
where F represents correlations of the bulk of the data and we consider the inverse of
F , estimated by Θ, as the “information matrix” of the bulk of the data, of which the
sparse structure is clearly captured by Graphic Lasso, and S contains the anomalies in
M . The corruption level is balanced by the (cid:96)1 norm of S as compared to the loss of
Graphic Lasso, as in the following optimization problem:

1Note, the diagonal will also be zero, but that is not essential for our analysis.

5

arg min
Θ(cid:31)0,L(cid:23)0,S

− log|Θ| + tr(F Θ) + ρ(cid:107)Θ(cid:107)1 + λ(cid:107)S(cid:107)1

s.t. M = F + S.

(4)

In problem (4), Θ is the information matrix, which is positive-deﬁnite, and M is the
observed corrupted sample covariance matrix. F , which is also positive-semideﬁnite, is
the recovered clean sample covariance matrix whose inverse is sparse and computable
by Graphical Lasso. S is the sparse matrix containing the hidden correlation in M . ρ
and λ play an essential role in tuning the sparsity of the graph and the corruption level.
ρ controls the sparsity of the graph structure by enforcing the entries of the information
matrix Θ to be zero. A choice of a larger value of ρ leads to a more sparse graph
structure. λ plays a role in splitting M into F and S by encouraging the sparsity of
S. The sparsity increases as the value of λ grows. When λ = 0, problem (4) is just
standard Graphical Lasso.

Note, problem (4) is not a convex optimization problem.

In particular, the term
tr(F Θ) is bi-convex. Accordingly, there are not current theoretical guarantees that our
ADMM procedure will always return the global minimizer. However, as an empirical
matter, we still achieve high-quality solutions (see Section 5 and 6).

4 Proposed Algorithm

This algorithm description is inspired by Hallac et al. [Hallac et al., 2017a].

To solve problem (4) eﬃciently, we propose an alternating direction method of mul-
tipliers (ADMM) algorithm [Boyd et al., 2011]. The idea of ADMM is to split a complex
problem up into several subproblems. Each time, we only need to optimize a subprob-
lem, which it is easy to handle, with all the other problems ﬁxed. Then, we can iterate
to solve each subproblem and update its solution until the user-deﬁned converge crite-
rion is reached. In this section, we develop analytical and closed-form solutions to the
separable subproblems. These subproblem solutions are fast and easy to implement. To
do so, we rewrite problem (4) by introducing a new variable Z to replace the Θ in (cid:96)1
norm so that we minimize the log-likelihood and (cid:96)1 norm of Θ separately. The convex
optimization problem becomes

arg min
Θ(cid:31)0,F (cid:23)0,S,Z

− log|Θ| + tr(F Θ) + ρ(cid:107)Z(cid:107)1 + λ(cid:107)S(cid:107)1

s.t. M = F + S
Z = Θ.

To include the two constraints into the minimization target in (5), we write the

corresponding augmented Lagrangian [Hestenes, 1969] as

arg min
Θ(cid:31)0,F (cid:23)0,S
−Z + U1(cid:107)2

− log|Θ| + tr(F Θ) + ρ(cid:107)Z(cid:107)1 + λ(cid:107)S(cid:107)1 +

(cid:107)Θ

F + < U2, M − F − S > +

(cid:107)M − F − S(cid:107)2
F .

µ2
2

µ1
2

Writing (6) in the forms of proximal operators [Parikh et al., 2014] allows us to take

(5)

(6)

6

advantage of well-known properties to ﬁnd closed-form updates for each variable in the
ADMM subproblems. Problem (6) is equivalent to

− log|Θ| + tr(F Θ) + ρ(cid:107)Z(cid:107)1 + λ(cid:107)S(cid:107)1

+

(cid:107)Θ − Z + U1(cid:107)2

F +

U2 + M − F − S(cid:107)2
F .

µ2
2

(cid:107)

1
µ2

arg min
Θ(cid:31)0,F (cid:23)0,S
µ1
2

Proximal Operators. For a real-valued function f (X), the proximal operators is
deﬁned as

X k+1 := proxφf (X k)
proxφf (X k) = arg min f (x) + (1/2φ)(cid:107)X − X k(cid:107)2
F ,

where k is the iteration counter. The update value of X should balance minimizing the
function f (X) against being close to the previous iterate X k.
ADMM Subproblems. As shown in Algorithm 1 below, our ADMM algorithm
divides problem (7) into four subproblems. Primal variables Θ, Z, F and S in the four
subproblems are alternately optimized. After each iteration, the scaled dual variable
U1 and U2 are also updated. The algorithm operates over the following six steps until
converge criteria are satisﬁed.

(7)

(8)

Θk+1 :=arg min

− log|Θ| + tr(F kΘ) +

(cid:107)Θ − Z k + U k

1 (cid:107)2
F

µ1
2

(cid:107)Θk+1 − Z + U k

1 (cid:107)2
F

Θ(cid:31)0

Z k+1 :=arg min

ρ(cid:107)Z(cid:107)1 +

µ1
2
Z
:= U k
1 + Θk+1 − Z k+1

U k+1
1

F k+1 :=arg min

(cid:107)

1
µ2

F (cid:23)0

tr(F Θk+1) +

µ2
2
1
µ2
S
:= U k
2 + M − F k+1 − Sk+1

λ(cid:107)S(cid:107)1 +

µ2
2

(cid:107)

Sk+1 :=arg min

U k+1
2

U k

2 + M − F − Sk(cid:107)2
F

U k

2 + M − F k+1 − S(cid:107)2
F

Convergence Criteria. We set two stopping criteria, ∆1 = (cid:107)Θk+1 − Θk(cid:107)F /(cid:107)Θk(cid:107)F and
∆2 = (cid:107)M − F − S(cid:107)F /(cid:107)M (cid:107)F , where criterion ∆1 measures whether the graph structure
is stable, and criterion ∆2 evaluates the stability of splitting the M into an F and an S.
Algorithm Convergence. In the setting of two primal variables, the convergence of
ADMM is guaranteed [Boyd et al., 2011, Mota et al., 2011]. The convergence of ADMM
with more than two primal variables is an ongoing area of research [Mohan et al., 2014].
Although there is no theory to guarantee the convergence of ADMM algorithm with
more than two primal variables, it is often observed to converge in practice. In our case,
our ADMM algorithm involves four primal variables, and we observe that our ADMM
algorithm converges well (see Section 5).
1 Θ-Update. Θ-minimization has an analytical solution [Boyd et al., 2011, Witten and
Tibshirani, 2009, Danaher et al., 2014].

Θk+1 =

Q(D +

D2 + 4µ1I)QT ,

(cid:112)

1
2µ1

(9)

7

where QDQT is the eigenvalue decomposition of µ1(Z − U1) − F [Boyd et al., 2011].
2 Z-Update. We use element-wise soft thresholding [Parikh et al., 2014, Boyd et al.,
2011] to minimize Z (see Algorithm 2 for the details to update Z).

3 F -Update. Take tr(F Θk+1) + µ2
to be zero.

2 (cid:107) 1
µ2

Z k+1 := Sof tρ/µ1(Θk+1

ij + U k

1,ij).

(10)

U k

2 + M − F − Sk(cid:107)2
F

(cid:1) and set the ﬁrst derivative

Rearrange,

Θk+1 − µ2(

U k

2 + M − F − Sk) = 0.

1
µ2

1
µ2

F =

U k

2 + M − Sk −

Θk+1.

1
µ2

To satisfy the constraint that F is positive-semideﬁnite, we project F to the positive-
semideﬁnite cone.
[Wen et al., 2010],
It is not the same as the idea of Wen et al.
but is similar in spirit. Speciﬁcally, we let QDQT denote the spectral decomposition of
1
Θk+1. D+ and D are the corresponding non-negative and negative
µ2
eigenvalues.

2 + M − Sk − 1
µ2

U k

We can get the projection of F as FSP D

QDQT = (cid:2)Q1 Q2

(cid:3)

(cid:20)D+

0
0 D

(cid:21)

(cid:21) (cid:20)QT
1
QT
2

FSP D := Q1D+QT
1 .

4 S-Update. We use element-wise soft thresholding to minimize S (see Algorithm 2
for the details to update S).

Sk+1 := Sof tλ/µ2(Mij − F k+1

ij + U k

2,ij).

Algorithm Initialization2. Since problem (4) is not convex (see Section 3), the ini-
tialization of the internal variables of the algorithm is very important. Based on our
experiments and observations, our algorithm has returned high-quality results when we
initialize F = 0 and S = M − F (see details in Algorithm 1).

(11)

(12)

5 Numerical Study

In this section, we evaluate our ADMM algorithm under three graphical structures on
synthetic datasets with diﬀerent anomaly setups. Further, we compare the performance
of our proposed algorithm with Minimum Covariance Determinant (MCD) [Rousseeuw
and Driessen, 1999] and Robust Principal Component Analysis (RPCA) [Cand`es et al.,
2011] on the simulated data.

2Our code is on github https://github.com/lht1949/AnomalyDetection/blob/master/Rglasso.

py.

8

Algorithm 1 Robust Graphical Lasso
The input of the algorithm:

• The empirical sample covariance matrix M ∈ Rm×m.

• The tuning parameter ρ penalizes the information matrix Θ.

• The tuning parameter λ penalizes the sparse latent eﬀects matrix S .

The algorithm will use the following internal variables:

• S ∈ Rm×m, F ∈ Rm×m, Z ∈ Rm×m, U1 ∈ Rm×m, U2 ∈ Rm×m, µ1 ∈ R, µ2 ∈ R,

β ∈ R, ρ ∈ R, λ ∈ R, converged ∈ {True, False}

Initialize the variables: F = 0, Z = 0, U1 = 0, U2 = 0, S = M − F , µ1 = 0.2, µ2 = 0.2,
β = 1.2 and converged = False.

While (not converged):

1. Update the values of Θ, Z , F and S :

(a) Θ = Find Optimal Θ(Z, U1, F , µ1)
(b) Z = Sof tρ/µ1(Θ + U1)
(c) F = Find Optimal F(M , U2, S, Θ,µ2)
(d) S = Sof tλ/µ2(M , U2, F , λ, µ2)

2. Update the Lagrange multipliers, µ1 and µ2 :

U1 = U1 + Θ − Z
U2 = U2 + M − F − S
µ1 = µ1 * β
µ2 = µ2 * β

3. Check for convergence:

∆1 = (cid:107)Θk+1 − Θk(cid:107)F /(cid:107)Θk(cid:107)F
∆2 = (cid:107)M − F − S(cid:107)F /(cid:107)M (cid:107)F
If ∆1 < (cid:15) and ∆2 < (cid:15):

converged = True

Return ˆF = F , ˆS = S, ˆΘ = Θ

9

Algorithm 2 Soft Thresholding Operation on Z and S

1. Z k+1 = Sof tρ/µ1(Θk+1

ij + U k

1,ij)

(a) if Θk+1

ij + U k
Z k+1 = Θk+1

1,ij > ρ/µ1:

ij + U k

1,ij − ρ/µ1

(b) if Θk+1

ij + U k

1,ij| < ρ/µ1

Z k+1 = 0

(c) if Θk+1

ij + U k
Z k+1 = Θk+1

1,ij < −ρ/µ1

ij + U k

1,ij + ρ/µ1

2. Sk+1 = Sof tλ/µ2(Mij − F k+1

ij + U k

2,ij)

(a) if Mij − F k+1

ij + U k
Sk+1 = Mij − F k+1
ij + U k

(b) if |Mij − F k+1
Sk+1 = 0
(c) if Mij − F k+1

ij + U k
Sk+1 = Mij − F k+1

2,ij > λ/µ2:

ij + U k

2,ij − λ/µ2

2,ij| < λ/µ2:

2,ij < −λ/µ2:

ij + U k

2,ij + λ/µ2

Graph Structure Setup. We test our algorithm under three network structures. The
ﬁrst two structures follow the setup of [Yuan and Lin, 2007]. The third structure is
intended to simulate anomalies with random placement.

Network Structure 1: Θ has the structure that θii = 1, θi,i−1 = θi−1,i = 0.5.

Network Structure 2: Θ has the structure that θii = 1, θi,i−1 = θi−1,i = 0.5,

θi,i−2 = θi−2,i = 0.25.

Network Structure 3: Θ has the structure that θii = 1, the positions of the oﬀ-

diagonal entries are random, and 5% of entries are non-zero.

Anomalies Setup. The true anomaly matrix S0 is structured such that each row
(column) has three entries. Speciﬁcally, each variable only correlates with at most two
variables and at least one variable. The entries of the anomaly matrix are drawn from
a normal distribution N (µ, 10). We ﬁrst study the case that µ is ﬁxed at 1000, then we
allow the value of µ to change.

Data Generation. The contaminated data is drawn from the multivariate distribution
of N (0, Σ0), where Σ0 = Θ−1 + S0 is the contaminated covariance. The observed sample
covariance M of the contaminated data can be computed as input to our proposed
algorithm.
Experiment Result. In the following results, we set the number of variable p = 200
and the sample size n = 100, 000.

(1) Fix µ = 1000. We compare F1 scores of the detected anomaly matrices S under

10

the three network structures (structure 1, 2, 3). Figure 1 indicates that the value of λ
plays a critical role in detecting the edges of the true anomaly matrix S0 correctly. When
the choice of λ is small, our algorithm detects a dense anomaly matrix S, whose F1 score
is low due to overestimation of the number of edges since the true S0 is sparse. As the
value of λ grows, the sparsity of the detected anomaly matrix S increase correspondingly.
For a certain range of values of λ, our algorithm correctly identiﬁes the edges of the true
S0. As the value of λ continues to grow, all the entries of S turn to be zero, and F1 score
drops to zero correspondently. In addition, we can see that the curves under structures
1, 2, 3 grow at diﬀerent speeds from zero to one. It indicates that our algorithm has
diﬀerent sensitivities to λ at the curve increasing phase under the three structures. It is
notable that all the three curves start to drop at the same value of λ. This is due to the
magnitude of the anomalies dominating the magnitude of the contaminated covariance.

Figure 1: This ﬁgure shows that comparison of F1 scores of the detected anomaly matrix
S under three structures. The entries of the true anomaly matrix S0 are drawn from
normal distribution N (1000, 10).

ρ
0.001
0.005
0.01
0.05
0.1
1
2
4

F1 Score of S
0.995
0.995
0.995
0.997
0.997
0.997
0.997
0.997

∆1
8.97E-08
8.11E-08
7.92E-08
9.17E-08
7.06E-08
6.41E-08
7.50E-08
8.97E-08

∆2
7.30E-14
4.86E-14
4.06E-14
4.09E-14
2.55E-14
8.92E-17
3.99E-17
1.13E-18

Iterations
50
51
52
53
54
70
75
80

Table 1: Numerical results of structure 1 with λ = 4. F1 score of the detected anomaly
matrix S is very robust to the choice of ρ, which controls the sparsity of the information
matrix.

Table 1, 2, and 3 show the numerical results of three structures with ﬁxed values of
λ. All the F1 scores of the detected anomaly matrix S are very robust to the choice
of ρ, which controls the sparsity of the information matrix. In addition, our algorithm

11

ρ
0.001
0.005
0.01
0.05
0.1
1
2
4

F1 Score of S
0.998
0.998
0.998
0.998
0.998
0.998
0.998
0.998

∆1
7.15E-08
8.73E-08
8.73E-08
7.79E-08
6.57E-08
9.20E-08
6.76E-08
8.08E-08

∆2
1.02E-14
1.64E-14
1.64E-14
5.02E-15
4.45E-15
7.00E-18
3.78E-20
2.05E-19

Iterations
49
49
49
50
53
68
74
79

ρ
0.001
0.005
0.01
0.05
0.1
1

F1 Score of S
0.998
0.998
0.998
0.998
0.998
0.998

∆1
7.83E-08
7.19E-08
7.44E-08
8.39E-08
7.13E-08
8.21E-08

∆2
3.76E-12
3.69E-12
3.66E-12
1.07E-12
1.07E-12
2.84E-29

Iterations
55
56
56
62
62
79

Table 2: Numerical results of structure 2 with λ = 1.98. F1 score of the detected
anomaly matrix S is very robust to the choice of ρ, which controls the sparsity of the
information matrix.

Table 3: Numerical results of structure 3 with λ = 36. F1 score of the detected anomaly
matrix S is very robust to the choice of ρ, which controls the sparsity of the information
matrix.

converges with a very high accuracy under all three structures. The convergence criterion
∆2 is much smaller than convergence criterion ∆1. In addition, the number of iterations
the algorithm takes to satisfy the two convergence criteria is less than 100. As the choice
of ρ grows, the number of iterations grows slowly.

(2) Vary µ. In this part, we allow the magnitude, µ, of the true anomalies to change.
The F1 score curves of our detected anomaly matrix under the diﬀerent magnitude of
the true anomalies are presented in Figure 2. In general, as the value of µ increases,
our algorithm can detect the anomalies more accurately in all the structure settings.
Speciﬁcally, our algorithm can separate the anomalies of much smaller magnitude from
the observed corrupted sample covariance correctly under the structure 1 and 2 than
structure 3 setting.

12

Figure 2: This ﬁgure shows that comparison of F1 score of anomaly matrix S under
three structures (The information matrices of structure 1 and 2 have tridiagonal and
ﬁve-diagonal structure. The information matrices of structure 3 is random with 5%
non-zero entries). The entries of the true anomaly matrix S0 are drawn from normal
distribution N (µ, 10), and µ varies.

Next, to investigate the identiﬁability of the true covariance matrix Σ and the true
anomaly matrix S0, we examine the distributions of the entries of the true covariance
matrix and the true anomaly matrix under the three structure settings. Figure 3(a), 3(b)
and 3(c) show that the distributions of the entries of the true covariance matrix on the
top (labeled by blue color) and the true anomaly matrix at the bottom (labeled by orange
color). Since the anomaly matrix is sparse with only 600 non-zero entries (out of total
40,000 entries), and the covariance matrix is dense with around 40,000 non-zero entries,
the distribution is dominated by the values of the entries of the covariance matrix, and
the distribution of the values of entries of the anomaly matrix is hidden. The magniﬁed
distributions in Figure 3(d), 3(e) and 3(f) give clearer pictures of the distributions of the
values of the entries of the anomaly matrix. We set µ = 50, 30 and 200 for structures
1, 2 and 3 correspondingly. Under such µ settings, we can identify the anomalies with
an F1 score around 0.7. The distribution of the entries of the covariance matrix under
structure 1 is bell-shaped, and there are around three thousand entries with values close
to or larger than that of the anomaly matrix. This suggests that our anomaly detection
algorithm performs very well when the magnitude of the anomalies is smaller than that
of the truth covariance under the structure 1 setup. Figure 3(b) shows that all the values
of entries of the covariance matrix are very close to zero under the mode 2 setup. If we
magnify the distribution of the entries of the anomaly matrix, Figure 3(e) indicates that
our algorithm performs well under structure 2 with the condition that the magnitude
of the anomalies is much larger than that of the true covariance. Under the structure
3 settings, the distribution of the values of the entries of the true covariance is a long
thin tail with majority values close to zero. The magniﬁed distribution in Figure 3(f)
shows that the magnitude of the anomalies is comparable to the magnitude of the true
covariance matrix.

13

(a) Distribution under structure
1 with µ = 50.

(b) Distribution under structure
2 with µ = 30.

(c) Distribution under structure
3 with µ = 200.

(d) Magniﬁed distribution un-
der structure 1.

(e) Magniﬁed distribution under
structure 2.

(f) Magniﬁed distribution under
structure 3.

Figure 3: The distributions of the entries of the true covariance Σ and the true anomaly
matrix S0 are labeled by blue and orange color correspondingly, and the entries of the
anomaly matrix are drawn from N (µ, 10). The performances of our algorithm are
considered well under structure 1 ((a), (d)) and 3 ((c), (f)). The performance under
structure 2 ((b), (e)) is not considered as well as structure 1 and 3.

(a) Accuracy comparison over diﬀerent p with ﬁxed
n.

(b) Time cost comparison over diﬀerent p with
ﬁxed n.

Figure 4: Accuracy and time cost comparison of our proposed method (Rglasso) v.s.
MCD method over diﬀerent numbers of features (p) with ﬁxed sample size (n = 10, 000)
((a) and (b)). The comparison is under the structure 1 setting with µ = 1000. The
comparison is done in Python 2.7.14 on the MacBook Pro (Mid 2015) with processor 2.2
GHz Intel Core i7, Memory 16 GB 1600 MHz DDR3, and macOS High Sierra (Version
10.13.3). We use the MCD implementation in the package of scikit-learn (Version 0.19.0),
and implement RPCA according to the algorithm of [Cand`es et al., 2011].

Time Cost and Accuracy. We compare our proposed algorithm with MCD and
RPCA method under the structure 1 setup with µ = 1000. The comparison is done in
Python 2.7.14 on the MacBook Pro (Mid 2015) with processor 2.2 GHz Intel Core i7,
Memory 16 GB 1600 MHz DDR3, and macOS High Sierra (Version 10.13.3). We use
the MCD implementation in the package of scikit-learn 0.19.0. and implement RPCA

14

according to the algorithm of [Cand`es et al., 2011]. Figure 4(a) and 4(b) show the results
over diﬀerent numbers of features (p) with ﬁxed sample size (n = 10, 000). In the case
that p is small, MCD and RPCA can detect the anomalies with relatively high accuracy.
As the value of p grows, the accuracy of MCD and RPCA drop quickly to close to zero.
RPCA performs poorly since it decomposes the input matrix into a low-rank matrix and
a sparse matrix, however, the true input matrix is composed of a full-rank matrix and
a sparse matrix. In contrast, the performance of our algorithm is very stable with F1
score close to one. As to computational cost, our algorithm is much faster than MCD
and RPCA as the value of p grows (see Figure 4(b)). RPCA converges slowly due to
the wrong decomposition of the imput matrix.

6 Case study with application in ﬁnancial data

Detecting anomalies in stock prices is an interesting application of our methods since
this usually implies the hidden connections between diﬀerent companies. Such a sparse
hidden graph structure reveals the latent correlation of stocks, and it can help improve
portfolio allocation and better hedge risks by avoiding stocks with such hidden connec-
tions. We can infer hidden graph structures 3 by applying our algorithm to the stock
prices through their log returns. We study the intraday high-frequency transactions 4
(millisecond) of 94 stocks 5 in S&P 100 [Liu et al., 2018]. Speciﬁcally, we use 1-minute
averaged log returns of the stock prices from these (millisecond) transactions on August
21, 2013. The dimension of the data matrix is 389 by 94. We compute the sample
covariance of the 94 stocks, which is the input to our algorithm.

Figure 5 shows the latent network structure our algorithm identiﬁes. We can see that
the stocks with the most connections are in the center of the graph. In particular, the
stocks in the red circle are all in the ﬁnance industry. These ﬁnance companies provide
loans, credit lines or other ﬁnancial services to other companies. Therefore, they have
the most edges connected with other companies. Such wide connections result in a
relatively dense graph structure.

We further examine the data by excluding the ﬁnance stocks (MS, C, ALL, JPM,
COF, BAC, USB, MET, GS, WFC and BK) in the red circle. The sparsity of the hidden
graph structure we detect improves in Figure 6. SPG (Simon Property Group) and DD
(DuPont) have the most edges and locate in the center since SPG (Simon Property
Group), as a real estate company, is correlated with other companies via oﬃce building
renting, warehouse cost and etc., and DD (DuPont) as a conglomerate involves many
industries.

3The hidden graph structure is detected anomaly matrix S. Figure 5, 6, and 7 are visualizations by

treating S as adjacent matrices.

4The stock prices of the transactions are download from Wharton Research Data Services (WRDS).
5We use the data in Liu et al. [Liu et al., 2018]

15

Figure 5: Latent network structure our algorithm identiﬁes using 94 stocks in S&P 100.

Figure 6: Latent network structure our algorithm identiﬁes using 83 stocks (excluding
the stocks in ﬁnance industry) in S&P 100.

Figure 7 shows the hidden structure of FDX (FedEx), which is extracted from Figure
6. FDX (FedEx) has hidden connections with companies in 3 major industries: energy
industry such as SLB (Schlumberger Business Consulting), FCX (Freeport-McMoRan
Inc.), APC (Anadarko Petroleum), APA (Apache Corporation) and DVN (Devon Energy
Corp), since energy consumption is the biggest cost of FDX (FedEx) as a transporta-
tion company; transportation industry such as SO (Southern Company), NSC (Norfolk

16

Figure 7: Latent network structure of FDX (FedEx), which is extracted from Figure 6.

Southern), UNP (Union Paciﬁc Corporation) and UPS (United Parcel Service), since
these companies are FDX (FedEx)’s competitors; automaker/aircraft maker industry
such as F (Ford Motor) and UTX (United Technologies), since FDX (FedEx) may need
to purchase trucks or airplanes from automaker/aircraft makers as transportation tools.

7 Conclusion and Future Work

In this paper, we propose a Robust Graphical Lasso (Rglasso) to detect sparse latent
eﬀects via Graphical Lasso. The algorithm is similar in spirit to Robust Principal Com-
ponent Analysis (RPCA). Moreover, we provide an Alternating Direction Method of
Multipliers (ADMM) solution to the optimization problem which scales to large prob-
lems with many random variables. We evaluate the proposed algorithm on both real
and synthetic datasets, obtaining interpretable results and outperforming the standard
robust minimum covariance determinant (MCD) and RPCA method in terms of both
accuracy and speed.
In future work, we will explore more graphical structures and
anomaly settings to understand our algorithm working conditions. In addition, we will
modify the loss function and constraints to estimate the information matrix. Moreover,
since we have demonstrated that our algorithm provides high-quality results both in
synthetic and real data, we will explore the theoretical foundation for the convergence
of the algorithm to a global minimizer.

References

F. Alqallaf, S. V. Aelst, V. J. Yohai, and R. H. Zamar. Propagation of outliers in
ISSN 00905364.

multivariate data. The Annals of Statistics, 37(1):311–331, 2009.
URL http://www.jstor.org/stable/25464750.

S. Boyd, N. Parikh, E. Chu, B. Peleato, J. Eckstein, et al. Distributed optimization and

17

statistical learning via the alternating direction method of multipliers. Foundations
and Trends in Machine Learning, 3(1):1–122, 2011.

E. J. Cand`es, X. Li, Y. Ma, and J. Wright. Robust principal component analysis?
ISSN 0004-5411. doi:

Journal of the ACM (JACM), 58(3):11:1–11:37, June 2011.
10.1145/1970392.1970395. URL http://doi.acm.org/10.1145/1970392.1970395.

P. Danaher, P. Wang, and D. M. Witten. The joint graphical lasso for inverse covariance
estimation across multiple classes. Journal of the Royal Statistical Society: Series B
(Statistical Methodology), 76(2):373–397, 2014.

C. Eckart and G. Young. The approximation of one matrix by another of lower rank.

Psychometrika, 1(3):211–218, 1936.

P. Filzmoser, A. Ruiz-Gazen, and C. Thomas-Agnan. Identiﬁcation of local multivariate

outliers. Statistical Papers, 55(1):29–47, 2014.

M. A. Finegold and M. Drton. Robust graphical modeling with t-distributions.

In
Proceedings of the Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence,
pages 169–176. AUAI Press, 2009.

J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the
graphical lasso. Biostatistics, 9(3):432–441, 2008. doi: 10.1093/biostatistics/kxm045.
URL +http://dx.doi.org/10.1093/biostatistics/kxm045.

D. Hallac, Y. Park, S. Boyd, and J. Leskovec. Network inference via the time-varying
graphical lasso. In Proceedings of the 23rd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pages 205–213. ACM, 2017a.

D. Hallac, S. Vare, S. Boyd, and J. Leskovec. Toeplitz inverse covariance-based clus-
tering of multivariate time series data.
In Proceedings of the 23rd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, pages 215–223.
ACM, 2017b.

J. Hammersley and P. Cliﬀord. Markov ﬁelds on ﬁnite graphs and lattices, 1971. URL

http://www.statslab.cam.ac.uk/~grg/books/hammfest/hamm-cliff.pdf.

M. R. Hestenes. Multiplier and gradient methods. Journal of optimization theory and

applications, 4(5):303–320, 1969.

K. Hirose, H. Fujisawa, and J. Sese. Robust sparse gaussian graphical modeling. Journal
of Multivariate Analysis, 161(Supplement C):172 – 190, 2017. ISSN 0047-259X. doi:
https://doi.org/10.1016/j.jmva.2017.07.012.

H. Hotelling. Analysis of a complex of statistical variables into principal components.

Journal of educational psychology, 24(6):417, 1933.

M. Hubert, P. Rousseeuw, and K. Vakili. Shape bias of robust covariance estimators:

an empirical study. Statistical Papers, 55(1):15–28, 2014.

18

I. M. Johnstone. On the distribution of the largest eigenvalue in principal components
analysis. The Annals of Statistics, 29(2):295–327, 2001. ISSN 00905364. URL http:
//www.jstor.org/stable/2674106.

I. T. Jolliﬀe. Principal Component Analysis and Factor Analysis. Springer, New York,

NY, 1986.

S. L. Lauritzen. Graphical Models. Clarendon Press, Oxford, United Kingdom, 1996.

Z. Lin, M. Chen, and Y. Ma. The augmented lagrange multiplier method for exact
recovery of corrupted low-rank matrices. arXiv preprint arXiv:1009.5055, 09, 2010.
URL https://arxiv.org/abs/1009.5055.

H. Liu, J. Zou, and N. Ravishanker. Multiple day biclustering of high-frequency ﬁnancial

time series. Stat, 7(1):e176, 2018.

X. Liu, X. Kong, and A. B. Ragin. Uniﬁed and contrasting graphical lasso for brain
network discovery. In Proceedings of the 2017 SIAM International Conference on Data
Mining, pages 180–188. SIAM, 2017.

N. Meinshausen and P. B¨uhlmann. High-dimensional graphs and variable selection with

the lasso. The annals of statistics, 34:1436–1462, 2006.

K. Mohan, P. London, M. Fazel, D. Witten, and S.-I. Lee. Node-based learning of
multiple gaussian graphical models. The Journal of Machine Learning Research, 15
(1):445–488, 2014.

J. F. Mota, J. M. Xavier, P. M. Aguiar, and M. P¨uschel. A proof of convergence
for the alternating direction method of multipliers applied to polyhedral-constrained
functions. arXiv preprint arXiv:1112.2295, 2011.

V. ¨Ollerer and C. Croux. Robust high-dimensional precision matrix estimation.

In
Modern Nonparametric, Robust and Multivariate Methods, pages 325–350. Springer,
2015.

R. Paﬀenroth, P. Du Toit, R. Nong, L. Scharf, A. P. Jayasumana, and V. Bandara.
Space-time signal processing for distributed pattern detection in sensor networks.
IEEE Journal of Selected Topics in Signal Processing, 7(1):38–49, 2013.

N. Parikh, S. Boyd, et al. Proximal algorithms. Foundations and Trends R(cid:13) in Optimiza-

tion, 1(3):127–239, 2014.

P. J. Rousseeuw and K. V. Driessen. A fast algorithm for the minimum covariance

determinant estimator. Technometrics, 41(3):212–223, 1999.

T. P. Speed and H. T. Kiiveri. Gaussian markov distributions over ﬁnite graphs. The
Annals of Statistics, 14(1):138–150, 1986. ISSN 00905364. URL http://www.jstor.
org/stable/2241271.

Q. Sun, S. Xiang, and J. Ye. Robust principal component analysis via capped norms.
In Proceedings of the 19th ACM SIGKDD international conference on Knowledge
discovery and data mining, pages 311–319. ACM, 2013.

19

G. Tarr, S. M¨uller, and N. C. Weber. Robust estimation of precision matrices under cell-
wise contamination. Computational Statistics and Data Analysis, 93:404–420, 2016.

Z. Wen, D. Goldfarb, and W. Yin. Alternating direction augmented lagrangian methods
for semideﬁnite programming. Mathematical Programming Computation, 2(3-4):203–
230, 2010.

J. Whittaker. Graphical Models in Applied Multivariate Statistics. Wiley Publishing, w

New York, NY, 2009. ISBN 0470743662, 9780470743669.

D. M. Witten and R. Tibshirani. Covariance-regularized regression and classiﬁcation for
high dimensional problems. Journal of the Royal Statistical Society: Series B (Statis-
tical Methodology), 71(3):615–636, 2009. ISSN 1467-9868. doi: 10.1111/j.1467-9868.
2009.00699.x. URL http://dx.doi.org/10.1111/j.1467-9868.2009.00699.x.

J. Wright, A. Ganesh, S. Rao, Y. Peng, and Y. Ma. Robust principal component analysis:
Exact recovery of corrupted low-rank matrices via convex optimization. In Y. Bengio,
D. Schuurmans, J. D. Laﬀerty, C. K. I. Williams, and A. Culotta, editors, Advances
in Neural Information Processing Systems 22, pages 2080–2088. Curran Associates,
Inc., 2009.

E. Yang and A. C. Lozano. Robust gaussian graphical modeling with the trimmed
graphical lasso. In Advances in Neural Information Processing Systems, pages 2602–
2610, 2015.

S. Yang, Q. Sun, S. Ji, P. Wonka, I. Davidson, and J. Ye. Structural graphical lasso
for learning mouse brain connectivity.
In Proceedings of the 21th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, pages 1385–1394.
ACM, 2015.

M. G. G. Yanyuan Ma. Highly robust estimation of dispersion matrices. Journal of

Multivariate Analysis, 78:11–36, July 2001.

M. Yuan and Y. Lin. Model selection and estimation in the gaussian graphical model.
Biometrika, 94(1):19–35, 2007. doi: 10.1093/biomet/asm018. URL +http://dx.doi.
org/10.1093/biomet/asm018.

C. Zhou and R. C. Paﬀenroth. Anomaly detection with robust deep autoencoders.
In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 665–674. ACM, 2017.

J. Zou and C. Huang. Eﬃcient portfolio allocation with sparse volatility estimation
for high-frequency ﬁnancial data. In Big Data (Big Data), 2016 IEEE International
Conference on, pages 2332–2341. IEEE, 2016.

20

