Learning Single-View 3D Reconstruction with
Limited Pose Supervision

Guandao Yang1, Yin Cui1,2, Serge Belongie1,2, Bharath Hariharan1

1 Department of Computer Science, Cornell University
2 Cornell Tech

Abstract. It is expensive to label images with 3D structure or precise
camera pose. Yet, this is precisely the kind of annotation required to train
single-view 3D reconstruction models. In contrast, unlabeled images or
images with just category labels are easy to acquire, but few current
models can use this weak supervision. We present a uniﬁed framework
that can combine both types of supervision: a small amount of cam-
era pose annotations are used to enforce pose-invariance and view-point
consistency, and unlabeled images combined with an adversarial loss are
used to enforce the realism of rendered, generated models. We use this
uniﬁed framework to measure the impact of each form of supervision in
three paradigms: semi-supervised, multi-task, and transfer learning. We
show that with a combination of these ideas, we can train single-view
reconstruction models that improve up to 7 points in performance (AP)
when using only 1% pose annotated training data.

Keywords: single-image 3d-reconstruction, few-shot learning, GANs

1

Introduction

The ability to understand 3D structure from single images is a hallmark of the
human visual system and a crucial step in visual reasoning and interaction. Of
course, a single image by itself does not have enough information to allow 3D
reconstruction, and a machine vision system must rely on some prior over shape:
all cars have wheels, for example. The crucial question is how a machine vision
system can acquire such priors.

One possibility is to leverage datasets of 3D shapes [4], but obtaining such
a dataset for a wide variety of categories requires either 3D modeling exper-
tise or 3D scanning tools and is therefore expensive. Another option, extensively
explored recently [27,21], is to show the machine many diﬀerent views of a multi-
tude of objects from calibrated cameras. The machine can then use photometric
consistency between rendered views of hypothesized shape and the correspond-
ing view of the real object as a learning signal. Although more tractable than
collecting 3D models, this approach is still very expensive in practice: one needs
to either physically acquire thousands of objects and place them on a turntable,
or ask human annotators to annotate images in the wild with both the camera
parameters and the precise instance that the image depicts. The assumption

2

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

Model

p11

p12

p13

p11

p12

p13

p21

p22

p23

p21

p22

p23

Images from same category with 
annotated camera poses

Images across categories with annotated 
camera poses

Collection of images of same category 
without pose annotation

Fig. 1. We propose a uniﬁed framework for single-view 3D reconstruction. Our model
can be trained with diﬀerent types of data, including pose-annotated images from the
same object category or across multiple categories, and unlabeled images.

that multiple, calibrated views of thousands of objects are available is also bi-
ologically implausible: a human infant must physically interact with objects to
acquire such training data, but most humans can understand airplane shape very
easily despite having played with very few airplanes.

Our goal in this paper is to learn eﬀective single-view 3D reconstruction
models when calibrated multi-view images are available for very few objects. To
do so we look at two additional sources of information. First, what if we had a
large collection of images of a category but without any annotation of the precise
instance or pose? Such a dataset is easy to acquire by simply downloading images
of this category from the web (Fig. 1, lower right). While it might be hard to
extract 3D information from such images, they can capture the distribution of the
visual appearance of objects from this category. Second, we look at annotations
from other semantic classes (Fig. 1, lower middle). These other classes might not
tell us about the nuances of a particular class, but they can still help delineate
what shapes in general look like. For example, most shapes are compact, smooth,
tend to be convex, etc.

This paper presents a framework that can eﬀectively use all these sources
of information. First, we design a uniﬁed model architecture and loss functions
that combine pose supervision with weaker supervision from unlabeled images.
Then, we use our model and training framework to evaluate and compare many
training paradigms and forms of supervision to come up with the best way of
using a small number of pose annotations eﬀectively. In particular, we show that:

1. Images without instance or pose annotations are indeed useful and can pro-
vide signiﬁcant gains in performance (up to 5 points in AP). At the same
time a little bit of pose supervision (< 50 objects) gives a large gain (> 20
points AP) when compared to not using pose information at all.

Learning Single-View 3D Reconstruction with Limited Pose Supervision

3

2. Category-agnostic priors obtained by pooling training data across classes
work just as well as, but not better than, category-speciﬁc priors trained on
each class individually.

3. Fine-tuning category-agnostic models for a novel semantic class using a small
amount (i.e. only 1%) of pose supervision signiﬁcantly improves performance
(up to 7 points in AP).

4. When faced with a novel category with nothing but a tiny set of pose-
annotated images, a category-agnostic model trained on pooled data and
ﬁne-tuned on the category of interest outperforms a baseline trained on only
the novel category by an enormous margin (up to 20 points in AP).

In summary, our results convincingly show large accuracy gains to be accrued
from combining multiple sources of data (unlabeled or labeled from diﬀerent
classes) with a single uniﬁed model.

2 Related Work

Despite many successes in reconstructing 3D scenes from multiple images [22,1],
doing it on a single image remains challenging. Classic work on single-image
3D reconstruction relies on having access to images labeled with the 3D struc-
ture [19]. This is also true for many recent deep learning approaches [8,5,24,18,6].
To get away from this need for precise 3D models, some work leverages key-
point and silhouette annotations [23,12]. More recent approaches assume multi-
ple views with calibrated cameras for training [27,10,21,17], and design training
loss functions that leverage photometric consistency and/or enforce invariance
to pose. Among these, our encoder-decoder architecture is similar to the one
proposed in PTN [27], but our model is trained end-to-end and is additionally
able to leverage unlabeled images to deal with limited supervision. In terms of
the required supervision, Tulsiani et al. [20] remove the requirement for pose
annotations but still require images to be annotated with the instance they
correspond to. PrGAN [7] reduces the supervision requirement further by only
using unlabeled images. As we show in this paper, this makes the problem need-
lessly challenging, while adding small amounts of pose supervision leads to large
accuracy gains.

Recovering 3D structure from a single image requires strong priors about
shape, and another line of work has focused on better capturing the manifold
of shape. Classic work has used low-dimensional parametric models [12,3]. More
recently, the rediscovery of convolutional networks has led to a resurgence in
interest in deep generative models. Wu et al. [26] used deep belief nets to model
3D shapes while Rezende et al. [17] consider variants of variational autoencoders.
Generative adversarial networks or GANs [9] can also be used to build generative
models of shapes [25]. The challenge is to train them without 3D data: Gadelha
et al. [7] show that this is indeed possible. While we use an adversarial loss as
they suggest, our generator is trained jointly with an encoder end-to-end on a
combination of pose-supervised and unlabeled images.

4

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

Chairs
Tables

Airplanes

Cars

R - Rotation

T - Translate

Image

Category

Instance ID

Camera Pose

3D Shape

Fig. 2. Diﬀerent forms of training annotations for single-view 3D reconstruction.
Note that some annotations (e.g. category) are cheaper to obtain than others (e.g.
3D shapes); and conversely some oﬀer a better training signal than others.

3 Training Paradigms

For single-view 3D reconstruction, we consider four types of annotations for an
image as illustrated in Fig 2. Our goal is to minimize the need for the more
expensive annotations (instance ID, camera pose and 3D shape). Towards this
end, we look at three diﬀerent training paradigms.

3.1 Semi-supervised single-category

In this setting, we assume all images are from a single category. Noting the
fact that camera pose and model-instance annotations are diﬃcult to collect
in the wild, we restrict to a semi-supervised setting where only some of the
images are labeled with camera pose and most of them are unlabeled. Formally,
we are given a dataset of images annotated with both camera pose and the
instance ID: Xl = {(xij, pij, i)}i,j, where xij represents the j-th image of the
i-th instance when projected with camera pose pij. We also have a dataset
without any annotation: Xu = {xi}i. The goal is to use Xl and Xu to learn a
category-speciﬁc model for single image 3D reconstruction.

3.2 Semi-supervised multi-category

An alternative to building a separate model for each category is to build a
category-agnostic model. This allows one to combine training data across multi-
ple categories, and even use training images that do not have any category labels.
Thus, instead of a separate labeled training set X c
for each category c, here we
l
l ∪ X c2
only assume a combined dataset X multi
. Similarly,
we assume access to an unlabeled set of images X multi
(now without category
labels). Note that this multi-category setting is harder than the single-category
since it introduces cross-category confusion, but it also allows the model to learn
category-agnostic shape information across diﬀerent categories.

l ∪ · · · ∪ X cn

= X c1

u

l

l

3.3 Few-shot transfer learning

Collecting a large dataset that can cover all categories we would ever encounter
is infeasible. Therefore, we also need a way to adapt a pre-trained model to a new

Learning Single-View 3D Reconstruction with Limited Pose Supervision

5

category. This strategy can also be used for adapting a category-agnostic model
to a speciﬁc category. We assume that for this adaptation, a dataset X (new)
containing a very small number of images with pose and instance annotations
(< 100) are available for the category of interest. We also assume that the semi-
supervised multi-category dataset described above is available as a pre-training
dataset: X pre

and X pre

u = X multi

l = X multi

u

.

l

l

4 A Uniﬁed Framework

We need a model and a training framework that can utilize both images with
pose and instance annotations, and images without any labels. The former set
of images can be used to enforce the consistency of the predicted 3D shape
across views, as well as the similarity between the rendered 3D shape and the
corresponding view of the real object. The latter set of images can only provide
constraints on the realism of the generated shapes. To capture all these con-
straints, we propose a uniﬁed model architecture with three main components:

1. An Encoder E that takes an image (silhouette) as input and produces a

2. A Generator G that takes a latent representation of shape as input and

latent representation of shape.

produces a voxel grid.

3. A Discriminator D that tries to distinguish between rendered views of the

voxel output by the generator and views of the real objects.

In addition, we make use of a “projector” module P that takes a voxel and a
viewpoint as input, and it renders the voxel from the inputted viewpoint. We
use a diﬀerentiable projector similar to the one in PrGAN [7]. We extend it to
perspective projection. P has no trainable parameters.

The training process alternates between an iteration on images labeled with
pose and instance, and an iteration on unlabeled images. The two sets of itera-
tions use diﬀerent loss functions but update the same model.

4.1 Training on pose-annotated images

In each pass on the annotated images, the encoder is provided with pairs of
images xi1, xi2 of the same 3D object i taken from diﬀerent camera poses p1 and
p2. The encoder E embeds each image into latent vectors z1, z2. The generator
(decoder) G is tasked with predicting the 3D voxel grid from z1 and z2.

The 3D voxel grid produced by the generator should be: 1) a good recon-
struction of the object and 2) invariant to the pose of the input image [27]. This
requires that the latent shape representation also be invariant to the camera
pose of the input image. To ensure the pose invariance of the learned latent rep-
resentation z1, the predicted 3D voxel from z1 should be able to reconstruct the
second input image when projected to the second viewpoint p2, and vice versa.

With these intuitions in mind, we explore the following three losses.

6

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

G

G

Proj

Proj

p1

p2

E

z1

E

z2

p

z

G

Proj

Gaussian

D

 
YES / NO 
Is the image generated? 

Fig. 3. Overview of the proposed model architecture. An encoder E and a generator
G with pose consistency (on the top) learn from images with pose supervision, and a
discriminator D (on the bottom) helps G to learn from unlabeled images. Notice that
two encoders E and three generator G in the diagram all share parameters, respectively.

Reconstruction loss: The predicted 3D model, when projected with a cer-
tain camera pose, should be consistent with the ground truth image projected
from that camera pose. More speciﬁcally, let (x1, p1) and (x2, p2) be two pairs
of image-pose pair sampled from a 3D-model, then the voxel reconstructed from
E(x1) should produce the same image as x2 if projected from camera pose p2.
Same for the other view. Let P (v, p) represent the image generated by project-
ing voxel v using camera pose p. We deﬁne the reconstruction loss to address
this consistency requirement as:

Lrecon = kP (G(E(x2)), p1) − x1k1+2 + kP (G(E(x1)), p2) − x2k1+2

(1)

where k · k1+2 = k · k1 + k · k2 is the summation of ℓ1 and ℓ2 reconstruction losses.
Such reconstruction loss has been used in prior work [27]. We add ℓ1 loss since
ℓ1 loss could better cope with sparse vectors such as silhouette images.

Pose-invariance loss on representations: Given two randomly sampled
views of an object, the encoder E should be able to embed their latent repre-
sentations close by, irrespective of pose. Therefore, we deﬁne a pose-invariance
loss on the latent representations:

Lpinv = kE(x1) − E(x2)k2

Pose-invariance loss on voxels: Similarly, the 3D voxel output recon-
structed by the generator G from two diﬀerent views of the same object should
be the same. Thus, we introduce a voxel-based pose invariance loss:

Lvinv = kG(E(x1)) − G(E(x2))k1

(2)

(3)

Learning Single-View 3D Reconstruction with Limited Pose Supervision

7

Losses are illustrated by the dashed lines in Fig. 3. Each training step on the

images with pose annotations tries to minimize the combined supervised loss:

Lsupervised = Lrecon + αLpinv + βLvinv

(4)

where α and β are weights for Lpinv and Lvinv, respectively. We use α = β = 0.1
in all of our experiments.

4.2 Training on unlabeled images

In order to learn from unlabeled images, we use an adversarial loss, as illustrated
in the bottom of Fig. 3. The intuition is to let the generator G learn to generate
3D voxel grids. When projected from a random viewpoint, the 3D voxel grid
should be able to produce an image that is indistinguishable from a real image.
Another advantage of an adversarial loss is regularization, as in the McRecon
approach [10]. Speciﬁcally, we ﬁrst sample a vector z ∼ N (0, I) and a viewpoint
p uniformly sampled from the range of camera poses observed in the training
set. Then the generator G will take the latent vector z and reconstruct a 3D
shape. This 3D shape will be projected to an image using the random pose p.
No matter which camera pose we project, the projected image should look like
an image sampled from the dataset. We update the generator and discriminator
by using an adversarial loss similar to the one used by PrGAN [7]:

LD = Ez,p[log(1 − D(P (G(z), p)))] + Ex∼X [log D(x)]
LG = −Ez,p[log D(P (G(z), p))]

(5)

(6)

Note that instead of normally distributed z vectors, one could also use the
encoder output on sampled training images. However, encouraging G to produce
meaningful shapes even on noise input might force G to capture shape priors.

4.3

Implementation details

The detailed architectures of encoder, generator and discriminator are illustrated
in Fig. 4. In the projector (not shown in Fig. 4), we ﬁrst rotate the voxelized 3D
model by its center and then use perspective projection to produce the image
according to the camera pose. The whole model is trained end-to-end by alternat-
ing between iterations on pose-annotated and unlabeled images. We use Adam
optimizer [13] with learning rates of 10−3, 10−4, and 10−4 for encoder, gener-
ator, and discriminator respectively. While training using the adversarial loss,
we used the gradient penalty introduced by DRAGAN [14] to improve training
stability. Codes are available at https://github.com/stevenygd/3d-recon.

5 Experiments

5.1 Dataset

We use voxelized 32 × 32 × 32 3D shapes from the ShapeNetCore [4] dataset. We
look at 10 categories: airplanes, cars, chairs, displays, phones, speakers,

8

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

Fig. 4. Model Architectures of encoder, generator and discriminator. Conv: Convo-
lution,BN: Batch Normalization [11],LN: Layer Normalization [2],L-ReLU:leaky ReLU
with slope of 0.2 [15], ConvT:Transposed Convolution that is often used in generation
tasks [16,25]. FC, k :Fully-Connected layers with k outputs. The discriminator outputs
a probability that the image is generated.

Table 1. Comparison between synthetic datasets used in prior work and ours. The key
diﬀerence is the amount of pose annotations available during training. We experiment
with multiple settings.

Dataset Properties MVC [20] McRecon [10] PTN [27]
Input image
64x64/RGB 127x127/RGB 64x64/RGB
Supervision image 64x64/Mask 127x127/Mask 32x32/Mask
Supervision level
Pose annotations
#views per image 5
Pose selection

2D + U3D
100%
Unavailable
Random

2D
100%
8-24
Fixed discrete Random

Ours
32x32/Grayscale
32x32/Mask
2D
0-100%
5

2D
100%

Random

tables, benches, vessels, and cabinets. For each category, we use ShapeNet’s
default split for training, validation, and test. While generating the training
images, we ﬁrst rotate the voxelized 3D model around its center using a rotation
vector r = [rx, ry, rz], where rx ∈ [−20◦, 40◦] and ry ∈ [0◦, 360◦] are uniformly
sampled rotation angles of Altitude and Azimuth; we always set rz = 0. We
then project the rotated 3D voxel into a binary mask as the image for training,
validation, and testing, where the rotation vector r is the camera pose. For
each 3D shape, we generate 5 masks from diﬀerent camera poses. During the
experiments, we also want to restrict the amount of pose supervision. A model is
trained with r% of pose supervision if r% of model instances are annotated with
poses. We will explore 100%, 50%, 10%, and 1% of pose annotations in diﬀerent
settings. All training images, no matter whether they have pose annotations or
not, are used as unlabeled images in all settings.

Learning Single-View 3D Reconstruction with Limited Pose Supervision

9

Note that our data settings are diﬀerent from prior work, and indeed the
settings in prior works diﬀer from each other. We use input images with the low-
est resolution (32 × 32) and no color cues (grayscale) compared to the synthetic
dataset from Tulsiani et al. [20], McRecon [10], and PTN [27]. We use fewer
viewpoints than PTN [27], and our viewpoints are sampled randomly, making it
a more diﬃcult task. Our data setting only provides 2D supervision with camera
pose, which is diﬀerent from McRecon [10] that also used unlabeled 3D super-
vision (U3D). The precise data setting is orthogonal to our focus, which is on
combining pose supervision and unlabeled images. As such, we select the set-
ting with less information provided when compared to prior works. A detailed
comparison is presented in Table 1.

5.2 Evaluation metrics

To evaluate the performance of our model, we use the Intersection-over-Union
(IoU) between the ground truth voxel grid and the predicted one, averaged over
all objects. Computing the IoU requires thresholding the probability output of
voxels from the generator. As suggested by Tulsiani et al. [20], we sweep over
thresholds and report the maximum average IoU. We also report IoU0.4 and
IoU0.5 for comparison with previous work, and the Average Precision (AP).

5.3 Semi-supervised single-category

We use 6 categories: airplanes, benches, cars, chairs, sofas, and tables for
single-category experiments under semi-supervised setting. In this setting, we
train a separate model for each category. We experiment with varying amounts
of pose supervision from 0% to 100%.

Comparison with prior work: We ﬁrst compare with prior work that uses
full pose/instance supervision. We train our models with 50% of the images an-
notated with instance and pose. The models are trained for 20,000 iterations
with early stopping (i.e., keeping the model with the best performance in val-
idation set). Performance comparisons are shown in Table 2. The performance
of our model is comparable with prior work across multiple metrics. The results
suggest that while using only 50% of pose supervisions, our model outperforms
McRecon [10] and MVC [20], but it performs worse than PTN [27] in terms
of IoU0.5. However, note that due to diﬀerences in the setting across diﬀerent
approaches, the numbers are not exactly commensurate.

Are unlabeled images useful? We next ask if using unlabeled images and
an adversarial loss to provide additional supervision and regularization is useful.
We compare three models: 1) a model trained with both pose-annotated and
unlabeled images; 2) a model trained on just the pose-annotated images; and
3) a model trained on only the unlabeled images. In the third case, since the
model doesn’t have data to train the encoder, we adopt the training scheme
of PrGAN [7] by ﬁrst training the generator G and discriminator D together
as a GAN, and then using the generator to train an encoder E once the GAN

10

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

Table 2. Comparison between our model and prior work on single-view 3D recon-
struction. All models are trained with images from a single category. Our model’s
performance is comparable with prior models while using only 50% pose supervision.

MVC [20] McRecon [10] PTN [27] Ours (50% pose annotations)

Category

airplanes
benches
cars
chairs
sofas
tables

IoU
0.55
-
0.75
0.42
-
-

AP
0.59
0.39
0.82
0.48
0.56
0.46

IoU0.4
0.37
0.30
0.56
0.35
0.38
0.35

IoU0.5
-
-
-
0.49
-
-

IoU
0.57
0.36
0.78
0.44
0.54
0.44

AP
0.75
0.48
0.92
0.60
0.69
0.63

IoU0.4 IoU0.5
0.56
0.57
0.35
0.35
0.77
0.77
0.43
0.42
0.53
0.52
0.43
0.42

Fig. 5. Comparison between three variations of our models trained with: 1) combined
pose-annotated and unlabeled images, 2) pose-annotated images only, and 3) unla-
beled images only. Our model is able to leverage both data with pose annotation and
unlabeled data. Unlabeled data is especially helpful in the case of limited supervision.

training is done. We compare these models on the chair category with diﬀerent
amounts of pose supervision. Results are presented in Fig. 5.

First, compared to the purely unsupervised approach (0 pose supervision),
when only 1% of the data has pose annotations (45 models, 225 images), perfor-
mance increases signiﬁcantly. This suggests that pose supervision is necessary,
and that our model could successfully leverage such supervision to make better
predictions. Second, the model that combines pose annotations with unlabeled
images outperforms the one that uses only pose-annotated images. The lesser
the pose annotation available, the larger the gain, indicating that an adversarial
loss on unlabeled images is useful especially in the case when pose supervisions
and viewpoints are limited (≤ 10%). Third, given enough pose supervision (50%
or even 100%), the performance gap between the pose-supervision-only model
and the combined model is greatly reduced. This suggests that when there are
enough images with pose annotations, leveraging unlabeled data is unnecessary.

Learning Single-View 3D Reconstruction with Limited Pose Supervision

11

Table 3. Performance of category-agnostic models under diﬀerent amount of pose
supervision. Using same amount of supervision (50%), the performance of category-
agnostic model is on par with its category-speciﬁc counterpart, indicating that we
don’t need category supervision.

Test categories

airplanes
cars
chairs
displays
phones
speakers
tables
Mean

50% single
IoU AP
0.75
0.57
0.92
0.78
0.60
0.44
0.61
0.44
0.69
0.55
0.73
0.58
0.63
0.44
0.70
0.54

Pose supervision and problem setting
10% multi

1% multi
100% multi 50% multi
IoU AP IoU AP IoU AP IoU AP
0.49 0.63
0.58
0.71 0.81
0.79
0.31 0.39
0.45
0.26 0.32
0.43
0.42 0.52
0.55
0.45 0.58
0.59
0.29 0.39
0.46
0.42 0.52
0.55

0.76
0.93
0.57
0.59
0.72
0.74
0.63
0.71

0.73
0.93
0.57
0.58
0.73
0.73
0.61
0.70

0.57
0.78
0.44
0.43
0.56
0.59
0.45
0.55

0.54
0.78
0.41
0.36
0.50
0.55
0.40
0.51

0.75
0.93
0.54
0.49
0.64
0.69
0.54
0.65

5.4 Semi-supervised multi-category

We next experiment with a category-agnostic model on combined training data
from 7 categories : airplanes, cars, chairs, displays, phones, speakers,
and tables. This experiment is also conducted with diﬀerent amount of pose
annotations. Results are reported in Table 3.

In general, using more pose supervision yields better performance of category-
agnostic model. With the same amount of pose supervision (50%) for each cat-
egory, the category-agnostic model achieves similar performance compared with
the category-speciﬁc models. This suggests that the model is able to remedy the
removal of category information by learning a category-agnostic representation.

5.5 Few-shot transfer learning

What happens when a new class comes along that the system has not seen
before? In this case, the model should be able to transfer the knowledge it has
acquired and adapt it to the new class with very limited annotated training data.
To evaluate if this is possible, we use the category-agnostic model, pre-trained
on the dataset described in Sec 5.4, and adapt it to three unseen categories:
benches, vessels, and carbinets. For each of the novel categories, only 1%
of the pose-annotated data is provided. As a result, each novel category usually
has about 13 3D-shapes or about 65 pose-annotated images.

We compare three models in this experiment. From scratch: a model trained
from scratch on the given novel category without using any pre-training; Out-
of-Category [27]: the pre-trained category-agnostic model directly applied on
the novel classes without any additional training; and Fine-tuning: a pre-
trained category-agnostic model ﬁne-tuned on the given novel category. The
ﬁne-tuning is done by ﬁxing the encoder and training the generator only using
pose-annotated images for a few iterations. We used the same training strategy

12

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

B enches

Cabinets

Vessels

F rom scratch
Out-of-Category
F ine-tuning

F rom scratch
Out-of-Category
F ine-tuning

0.34

0.32

0.30

0.28

U
o

I

0.26

0.24

0.22

0.20

0.45

0.40

P
A

0.35

0.30

0.25

0.50

0.45

0.40

U
o

I

0.35

0.30

0.65

0.60

0.55

P
A

0.50

0.45

0.40

F rom scratch
Out-of-Category
F ine-tuning

Vessels

F rom scratch
Out-of-Category
F ine-tuning

F rom scratch
Out-of-Category
F ine-tuning

F rom scratch
Out-of-Category
F ine-tuning

0.46

0.44

0.42

U
o

I

0.40

0.38

0.675

0.650

0.625

0.600

P
A

0.575

0.550

0.525

0.500

0.475

1%

10%

50%

100%

1%

10%

50%

100%

1%

10%

50%

100%

B enches

Cabinets

1%

10%

50%

100%

1%

10%

50%

100%

1%

10%

50%

100%

P r e - t r a i n i n g P ose S upervision

P r e - t r a i n i n g P ose S upervision

P r e - t r a i n i n g P ose S upervision

Fig. 6. Few-shot transfer learning on novel categories. Each column represents the
performance on a novel category (IoU in top row and AP in bottom row). Notice that
the horizontal axis shows the amount of pose annotated supervision in pre-training.

Table 4. Comparing diﬀerent training strategies on chairs with 1% pose annotations.
Fine-tuning a category-agnostic model on the target category works the best.

IoU
AP

S, P
0.2913
0.3800

S, U S, P+U M
0.3175
0.2065
0.4162
0.2180

0.3104
0.3859

FT
0.3250
0.4247

as mentioned in 4.3 for all three models. In this experiment, we varies the amount
of pose annotations used for pre-traning. The results are shown in Fig. 6.

First, we observe that ﬁne-tuning a pre-trained model for a novel category
performs much better than training from scratch without pre-training. This sug-
gests that transferring the knowledge learned from a pre-trained model is essen-
tial for few-shot learning on new categories. Second, compared with the out-
of-category baseline, ﬁne-tuning improves the performance a lot upon directly
using the pre-trained model, especially in the case of limited pose supervision.
This indicates that our model is able to quickly adapt to a novel category with
few training examples via ﬁne-tuning.

5.6 How best to use limited annotation?

We now have all the ingredients necessary to answer the question: given a very
small number of pose annotations, what is the best way to train a single-view
3D reconstruction model?

Table 4 compares multiple training strategies on chairs: using just the pose-
annotated images of chairs (S, P), using just unlabeled images of chairs (S,

Learning Single-View 3D Reconstruction with Limited Pose Supervision

13

Fig. 7. 3D shape generation on the validation set. The top row shows input images
(32 × 32 grayscale). The corresponding ground truth voxels and generated ones are
presented in the middle row and bottom row, respectively. The models are trained
with semi-supervised single-category setting with 50% pose supervision.

Fig. 8. Interpolation within-category (top 3 rows) and cross-category (bottom 3 rows).
Given the latent vector of the left most shape z1 and the right-most shape z2, inter-
mediate shapes correspond to G(z1 + α(z2 − z1)), where α ∈ [0, 1].

U), using both pose-annotated and unlabeled images of chairs (S, P+U),
combining multiple categories to train a category-agnostic model (M), and ﬁne-
tuning a category-agnostic model for chairs (FT). The ﬁne-tuned model works
best, indicating that it is best to combine both pose-annotated and unlabeled
images, to leverage multiple categories and to retain category-speciﬁcity.

14

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

Fig. 9. Latent space arithmetic.

Fig. 10. Shape predictions from models with diﬀerent amount of pose supervisions.
From left to right: input image, ground truth voxel, and then shapes from models
presented in Fig. 5. P : training with pose annotation; S : training with unlabeled data.
The percentage indicates the amount of pose annotation.

5.7 Qualitative Results

Fig. 7 shows some qualitative results from our category-speciﬁc model trained
with 50% pose annotations. In addition to single-image 3D reconstruction, our
model learns a meaningful representation of shape, as shown by the ability to
do interpolation and arithmetic in the latent space (Fig. 8, 9).

The qualitative impact of reducing annotations is shown in Fig. 10. When the
amount of supervision is reduced, one sees a signiﬁcant amount of noise in the
3D reconstructions, which seems to reduce when unlabeled images are included.

6 Conclusions

In conclusion, we propose a uniﬁed and end-to-end model to use both images
labeled with camera pose and unlabeled images as supervision for single view 3D
reconstruction, and evaluate diﬀerent training strategies when annotations are
limited. Our experiments show that one can train a single-view reconstruction
model with few pose annotations when leveraging unlabeled data. Future work
will include conﬁrming and extending these results on more practical settings
with high resolution RGB images and arbitrary camera locations.

Learning Single-View 3D Reconstruction with Limited Pose Supervision

15

References

1. Agarwal, S., Furukawa, Y., Snavely, N., Simon, I., Curless, B., Seitz, S.M., Szeliski,

R.: Building rome in a day. Communications of the ACM (2011) 3

2. Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer normalization. arXiv preprint

arXiv:1607.06450 (2016) 8

3. Blanz, V., Vetter, T.: A morphable model for the synthesis of 3d faces. In: Pro-
ceedings of the 26th annual conference on Computer graphics and interactive tech-
niques. ACM Press/Addison-Wesley Publishing Co. (1999) 3

4. Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z.,
Savarese, S., Savva, M., Song, S., Su, H., Xiao, J., Yi, L., Yu, F.: ShapeNet:
An Information-Rich 3D Model Repository. Tech. Rep. arXiv:1512.03012 [cs.GR],
Stanford University — Princeton University — Toyota Technological Institute at
Chicago (2015) 1, 7

5. Choy, C.B., Xu, D., Gwak, J., Chen, K., Savarese, S.: 3d-r2n2: A uniﬁed approach

for single and multi-view 3d object reconstruction. In: ECCV (2016) 3

6. Fan, H., Su, H., Guibas, L.J.: A point set generation network for 3d object recon-

struction from a single image. In: CVPR. vol. 2, p. 6 (2017) 3

7. Gadelha, M., Maji, S., Wang, R.: 3d shape induction from 2d views of multiple

objects. In: 3DV (2017) 3, 5, 7, 9

8. Girdhar, R., Fouhey, D.F., Rodriguez, M., Gupta, A.: Learning a predictable and

generative vector representation for objects. In: ECCV (2016) 3

9. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,

Courville, A., Bengio, Y.: Generative adversarial nets. In: NIPS (2014) 3

10. Gwak, J., Choy, C.B., Garg, A., Chandraker, M., Savarese, S.: Weakly supervised
generative adversarial networks for 3d reconstruction. In: 3DV (2017) 3, 7, 8, 9,
10

11. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by

reducing internal covariate shift. In: ICML (2015) 8

12. Kar, A., Tulsiani, S., Carreira, J., Malik, J.: Category-speciﬁc object reconstruction

from a single image. In: CVPR (2015) 3

13. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: ICLR

(2015) 7

(2018) 7

14. Kodali, N., Hays, J., Abernethy, J., Kira, Z.: On convergence and stability of gans

15. Maas, A.L., Hannun, A.Y., Ng, A.Y.: Rectiﬁer nonlinearities improve neural net-

work acoustic models. In: ICML (2013) 8

16. Radford, A., Metz, L., Chintala, S.: Unsupervised representation learning with

deep convolutional generative adversarial networks. In: ICLR (2016) 8

17. Rezende, D.J., Eslami, S.A., Mohamed, S., Battaglia, P., Jaderberg, M., Heess, N.:

Unsupervised learning of 3d structure from images. In: NIPS (2016) 3

18. Rock, J., Gupta, T., Thorsen, J., Gwak, J., Shin, D., Hoiem, D.: Completing 3d

object shape from one depth image. In: CVPR (2015) 3

19. Saxena, A., Sun, M., Ng, A.Y.: Make3d: Learning 3d scene structure from a single

still image. PAMI (2009) 3

20. Tulsiani, S., Efros, A.A., Malik, J.: Multi-view consistency as supervisory signal

for learning shape and pose prediction. In: CVPR (2018) 3, 8, 9, 10

21. Tulsiani, S., Zhou, T., Efros, A.A., Malik, J.: Multi-view supervision for single-view

reconstruction via diﬀerentiable ray consistency. In: CVPR (2017) 1, 3

16

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

22. Ullman, S.: The interpretation of structure from motion. In: Proc. R. Soc. Lond.

23. Vicente, S., Carreira, J., Agapito, L., Batista, J.: Reconstructing pascal voc. In:

B. The Royal Society (1979) 3

CVPR (2014) 3

24. Wu, J., Xue, T., Lim, J.J., Tian, Y., Tenenbaum, J.B., Torralba, A., Freeman,

W.T.: Single image 3d interpreter network. In: ECCV (2016) 3

25. Wu, J., Zhang, C., Xue, T., Freeman, W.T., Tenenbaum, J.B.: Learning a prob-
abilistic latent space of object shapes via 3d generative-adversarial modeling. In:
NIPS (2016) 3, 8

26. Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., Xiao, J.: 3d shapenets:

A deep representation for volumetric shapes. In: CVPR (2015) 3

27. Yan, X., Yang, J., Yumer, E., Guo, Y., Lee, H.: Perspective transformer nets:
Learning single-view 3d object reconstruction without 3d supervision. In: NIPS
(2016) 1, 3, 5, 6, 8, 9, 10, 11

Learning Single-View 3D Reconstruction with
Limited Pose Supervision

Guandao Yang1, Yin Cui1,2, Serge Belongie1,2, Bharath Hariharan1

1 Department of Computer Science, Cornell University
2 Cornell Tech

Abstract. It is expensive to label images with 3D structure or precise
camera pose. Yet, this is precisely the kind of annotation required to train
single-view 3D reconstruction models. In contrast, unlabeled images or
images with just category labels are easy to acquire, but few current
models can use this weak supervision. We present a uniﬁed framework
that can combine both types of supervision: a small amount of cam-
era pose annotations are used to enforce pose-invariance and view-point
consistency, and unlabeled images combined with an adversarial loss are
used to enforce the realism of rendered, generated models. We use this
uniﬁed framework to measure the impact of each form of supervision in
three paradigms: semi-supervised, multi-task, and transfer learning. We
show that with a combination of these ideas, we can train single-view
reconstruction models that improve up to 7 points in performance (AP)
when using only 1% pose annotated training data.

Keywords: single-image 3d-reconstruction, few-shot learning, GANs

1

Introduction

The ability to understand 3D structure from single images is a hallmark of the
human visual system and a crucial step in visual reasoning and interaction. Of
course, a single image by itself does not have enough information to allow 3D
reconstruction, and a machine vision system must rely on some prior over shape:
all cars have wheels, for example. The crucial question is how a machine vision
system can acquire such priors.

One possibility is to leverage datasets of 3D shapes [4], but obtaining such
a dataset for a wide variety of categories requires either 3D modeling exper-
tise or 3D scanning tools and is therefore expensive. Another option, extensively
explored recently [27,21], is to show the machine many diﬀerent views of a multi-
tude of objects from calibrated cameras. The machine can then use photometric
consistency between rendered views of hypothesized shape and the correspond-
ing view of the real object as a learning signal. Although more tractable than
collecting 3D models, this approach is still very expensive in practice: one needs
to either physically acquire thousands of objects and place them on a turntable,
or ask human annotators to annotate images in the wild with both the camera
parameters and the precise instance that the image depicts. The assumption

2

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

Model

p11

p12

p13

p11

p12

p13

p21

p22

p23

p21

p22

p23

Images from same category with 
annotated camera poses

Images across categories with annotated 
camera poses

Collection of images of same category 
without pose annotation

Fig. 1. We propose a uniﬁed framework for single-view 3D reconstruction. Our model
can be trained with diﬀerent types of data, including pose-annotated images from the
same object category or across multiple categories, and unlabeled images.

that multiple, calibrated views of thousands of objects are available is also bi-
ologically implausible: a human infant must physically interact with objects to
acquire such training data, but most humans can understand airplane shape very
easily despite having played with very few airplanes.

Our goal in this paper is to learn eﬀective single-view 3D reconstruction
models when calibrated multi-view images are available for very few objects. To
do so we look at two additional sources of information. First, what if we had a
large collection of images of a category but without any annotation of the precise
instance or pose? Such a dataset is easy to acquire by simply downloading images
of this category from the web (Fig. 1, lower right). While it might be hard to
extract 3D information from such images, they can capture the distribution of the
visual appearance of objects from this category. Second, we look at annotations
from other semantic classes (Fig. 1, lower middle). These other classes might not
tell us about the nuances of a particular class, but they can still help delineate
what shapes in general look like. For example, most shapes are compact, smooth,
tend to be convex, etc.

This paper presents a framework that can eﬀectively use all these sources
of information. First, we design a uniﬁed model architecture and loss functions
that combine pose supervision with weaker supervision from unlabeled images.
Then, we use our model and training framework to evaluate and compare many
training paradigms and forms of supervision to come up with the best way of
using a small number of pose annotations eﬀectively. In particular, we show that:

1. Images without instance or pose annotations are indeed useful and can pro-
vide signiﬁcant gains in performance (up to 5 points in AP). At the same
time a little bit of pose supervision (< 50 objects) gives a large gain (> 20
points AP) when compared to not using pose information at all.

Learning Single-View 3D Reconstruction with Limited Pose Supervision

3

2. Category-agnostic priors obtained by pooling training data across classes
work just as well as, but not better than, category-speciﬁc priors trained on
each class individually.

3. Fine-tuning category-agnostic models for a novel semantic class using a small
amount (i.e. only 1%) of pose supervision signiﬁcantly improves performance
(up to 7 points in AP).

4. When faced with a novel category with nothing but a tiny set of pose-
annotated images, a category-agnostic model trained on pooled data and
ﬁne-tuned on the category of interest outperforms a baseline trained on only
the novel category by an enormous margin (up to 20 points in AP).

In summary, our results convincingly show large accuracy gains to be accrued
from combining multiple sources of data (unlabeled or labeled from diﬀerent
classes) with a single uniﬁed model.

2 Related Work

Despite many successes in reconstructing 3D scenes from multiple images [22,1],
doing it on a single image remains challenging. Classic work on single-image
3D reconstruction relies on having access to images labeled with the 3D struc-
ture [19]. This is also true for many recent deep learning approaches [8,5,24,18,6].
To get away from this need for precise 3D models, some work leverages key-
point and silhouette annotations [23,12]. More recent approaches assume multi-
ple views with calibrated cameras for training [27,10,21,17], and design training
loss functions that leverage photometric consistency and/or enforce invariance
to pose. Among these, our encoder-decoder architecture is similar to the one
proposed in PTN [27], but our model is trained end-to-end and is additionally
able to leverage unlabeled images to deal with limited supervision. In terms of
the required supervision, Tulsiani et al. [20] remove the requirement for pose
annotations but still require images to be annotated with the instance they
correspond to. PrGAN [7] reduces the supervision requirement further by only
using unlabeled images. As we show in this paper, this makes the problem need-
lessly challenging, while adding small amounts of pose supervision leads to large
accuracy gains.

Recovering 3D structure from a single image requires strong priors about
shape, and another line of work has focused on better capturing the manifold
of shape. Classic work has used low-dimensional parametric models [12,3]. More
recently, the rediscovery of convolutional networks has led to a resurgence in
interest in deep generative models. Wu et al. [26] used deep belief nets to model
3D shapes while Rezende et al. [17] consider variants of variational autoencoders.
Generative adversarial networks or GANs [9] can also be used to build generative
models of shapes [25]. The challenge is to train them without 3D data: Gadelha
et al. [7] show that this is indeed possible. While we use an adversarial loss as
they suggest, our generator is trained jointly with an encoder end-to-end on a
combination of pose-supervised and unlabeled images.

4

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

Chairs
Tables

Airplanes

Cars

R - Rotation

T - Translate

Image

Category

Instance ID

Camera Pose

3D Shape

Fig. 2. Diﬀerent forms of training annotations for single-view 3D reconstruction.
Note that some annotations (e.g. category) are cheaper to obtain than others (e.g.
3D shapes); and conversely some oﬀer a better training signal than others.

3 Training Paradigms

For single-view 3D reconstruction, we consider four types of annotations for an
image as illustrated in Fig 2. Our goal is to minimize the need for the more
expensive annotations (instance ID, camera pose and 3D shape). Towards this
end, we look at three diﬀerent training paradigms.

3.1 Semi-supervised single-category

In this setting, we assume all images are from a single category. Noting the
fact that camera pose and model-instance annotations are diﬃcult to collect
in the wild, we restrict to a semi-supervised setting where only some of the
images are labeled with camera pose and most of them are unlabeled. Formally,
we are given a dataset of images annotated with both camera pose and the
instance ID: Xl = {(xij, pij, i)}i,j, where xij represents the j-th image of the
i-th instance when projected with camera pose pij. We also have a dataset
without any annotation: Xu = {xi}i. The goal is to use Xl and Xu to learn a
category-speciﬁc model for single image 3D reconstruction.

3.2 Semi-supervised multi-category

An alternative to building a separate model for each category is to build a
category-agnostic model. This allows one to combine training data across multi-
ple categories, and even use training images that do not have any category labels.
Thus, instead of a separate labeled training set X c
for each category c, here we
l
l ∪ X c2
only assume a combined dataset X multi
. Similarly,
we assume access to an unlabeled set of images X multi
(now without category
labels). Note that this multi-category setting is harder than the single-category
since it introduces cross-category confusion, but it also allows the model to learn
category-agnostic shape information across diﬀerent categories.

l ∪ · · · ∪ X cn

= X c1

u

l

l

3.3 Few-shot transfer learning

Collecting a large dataset that can cover all categories we would ever encounter
is infeasible. Therefore, we also need a way to adapt a pre-trained model to a new

Learning Single-View 3D Reconstruction with Limited Pose Supervision

5

category. This strategy can also be used for adapting a category-agnostic model
to a speciﬁc category. We assume that for this adaptation, a dataset X (new)
containing a very small number of images with pose and instance annotations
(< 100) are available for the category of interest. We also assume that the semi-
supervised multi-category dataset described above is available as a pre-training
dataset: X pre

and X pre

u = X multi

l = X multi

u

.

l

l

4 A Uniﬁed Framework

We need a model and a training framework that can utilize both images with
pose and instance annotations, and images without any labels. The former set
of images can be used to enforce the consistency of the predicted 3D shape
across views, as well as the similarity between the rendered 3D shape and the
corresponding view of the real object. The latter set of images can only provide
constraints on the realism of the generated shapes. To capture all these con-
straints, we propose a uniﬁed model architecture with three main components:

1. An Encoder E that takes an image (silhouette) as input and produces a

2. A Generator G that takes a latent representation of shape as input and

latent representation of shape.

produces a voxel grid.

3. A Discriminator D that tries to distinguish between rendered views of the

voxel output by the generator and views of the real objects.

In addition, we make use of a “projector” module P that takes a voxel and a
viewpoint as input, and it renders the voxel from the inputted viewpoint. We
use a diﬀerentiable projector similar to the one in PrGAN [7]. We extend it to
perspective projection. P has no trainable parameters.

The training process alternates between an iteration on images labeled with
pose and instance, and an iteration on unlabeled images. The two sets of itera-
tions use diﬀerent loss functions but update the same model.

4.1 Training on pose-annotated images

In each pass on the annotated images, the encoder is provided with pairs of
images xi1, xi2 of the same 3D object i taken from diﬀerent camera poses p1 and
p2. The encoder E embeds each image into latent vectors z1, z2. The generator
(decoder) G is tasked with predicting the 3D voxel grid from z1 and z2.

The 3D voxel grid produced by the generator should be: 1) a good recon-
struction of the object and 2) invariant to the pose of the input image [27]. This
requires that the latent shape representation also be invariant to the camera
pose of the input image. To ensure the pose invariance of the learned latent rep-
resentation z1, the predicted 3D voxel from z1 should be able to reconstruct the
second input image when projected to the second viewpoint p2, and vice versa.

With these intuitions in mind, we explore the following three losses.

6

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

G

G

Proj

Proj

p1

p2

E

z1

E

z2

p

z

G

Proj

Gaussian

D

 
YES / NO 
Is the image generated? 

Fig. 3. Overview of the proposed model architecture. An encoder E and a generator
G with pose consistency (on the top) learn from images with pose supervision, and a
discriminator D (on the bottom) helps G to learn from unlabeled images. Notice that
two encoders E and three generator G in the diagram all share parameters, respectively.

Reconstruction loss: The predicted 3D model, when projected with a cer-
tain camera pose, should be consistent with the ground truth image projected
from that camera pose. More speciﬁcally, let (x1, p1) and (x2, p2) be two pairs
of image-pose pair sampled from a 3D-model, then the voxel reconstructed from
E(x1) should produce the same image as x2 if projected from camera pose p2.
Same for the other view. Let P (v, p) represent the image generated by project-
ing voxel v using camera pose p. We deﬁne the reconstruction loss to address
this consistency requirement as:

Lrecon = kP (G(E(x2)), p1) − x1k1+2 + kP (G(E(x1)), p2) − x2k1+2

(1)

where k · k1+2 = k · k1 + k · k2 is the summation of ℓ1 and ℓ2 reconstruction losses.
Such reconstruction loss has been used in prior work [27]. We add ℓ1 loss since
ℓ1 loss could better cope with sparse vectors such as silhouette images.

Pose-invariance loss on representations: Given two randomly sampled
views of an object, the encoder E should be able to embed their latent repre-
sentations close by, irrespective of pose. Therefore, we deﬁne a pose-invariance
loss on the latent representations:

Lpinv = kE(x1) − E(x2)k2

Pose-invariance loss on voxels: Similarly, the 3D voxel output recon-
structed by the generator G from two diﬀerent views of the same object should
be the same. Thus, we introduce a voxel-based pose invariance loss:

Lvinv = kG(E(x1)) − G(E(x2))k1

(2)

(3)

Learning Single-View 3D Reconstruction with Limited Pose Supervision

7

Losses are illustrated by the dashed lines in Fig. 3. Each training step on the

images with pose annotations tries to minimize the combined supervised loss:

Lsupervised = Lrecon + αLpinv + βLvinv

(4)

where α and β are weights for Lpinv and Lvinv, respectively. We use α = β = 0.1
in all of our experiments.

4.2 Training on unlabeled images

In order to learn from unlabeled images, we use an adversarial loss, as illustrated
in the bottom of Fig. 3. The intuition is to let the generator G learn to generate
3D voxel grids. When projected from a random viewpoint, the 3D voxel grid
should be able to produce an image that is indistinguishable from a real image.
Another advantage of an adversarial loss is regularization, as in the McRecon
approach [10]. Speciﬁcally, we ﬁrst sample a vector z ∼ N (0, I) and a viewpoint
p uniformly sampled from the range of camera poses observed in the training
set. Then the generator G will take the latent vector z and reconstruct a 3D
shape. This 3D shape will be projected to an image using the random pose p.
No matter which camera pose we project, the projected image should look like
an image sampled from the dataset. We update the generator and discriminator
by using an adversarial loss similar to the one used by PrGAN [7]:

LD = Ez,p[log(1 − D(P (G(z), p)))] + Ex∼X [log D(x)]
LG = −Ez,p[log D(P (G(z), p))]

(5)

(6)

Note that instead of normally distributed z vectors, one could also use the
encoder output on sampled training images. However, encouraging G to produce
meaningful shapes even on noise input might force G to capture shape priors.

4.3

Implementation details

The detailed architectures of encoder, generator and discriminator are illustrated
in Fig. 4. In the projector (not shown in Fig. 4), we ﬁrst rotate the voxelized 3D
model by its center and then use perspective projection to produce the image
according to the camera pose. The whole model is trained end-to-end by alternat-
ing between iterations on pose-annotated and unlabeled images. We use Adam
optimizer [13] with learning rates of 10−3, 10−4, and 10−4 for encoder, gener-
ator, and discriminator respectively. While training using the adversarial loss,
we used the gradient penalty introduced by DRAGAN [14] to improve training
stability. Codes are available at https://github.com/stevenygd/3d-recon.

5 Experiments

5.1 Dataset

We use voxelized 32 × 32 × 32 3D shapes from the ShapeNetCore [4] dataset. We
look at 10 categories: airplanes, cars, chairs, displays, phones, speakers,

8

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

Fig. 4. Model Architectures of encoder, generator and discriminator. Conv: Convo-
lution,BN: Batch Normalization [11],LN: Layer Normalization [2],L-ReLU:leaky ReLU
with slope of 0.2 [15], ConvT:Transposed Convolution that is often used in generation
tasks [16,25]. FC, k :Fully-Connected layers with k outputs. The discriminator outputs
a probability that the image is generated.

Table 1. Comparison between synthetic datasets used in prior work and ours. The key
diﬀerence is the amount of pose annotations available during training. We experiment
with multiple settings.

Dataset Properties MVC [20] McRecon [10] PTN [27]
Input image
64x64/RGB 127x127/RGB 64x64/RGB
Supervision image 64x64/Mask 127x127/Mask 32x32/Mask
Supervision level
Pose annotations
#views per image 5
Pose selection

2D + U3D
100%
Unavailable
Random

2D
100%
8-24
Fixed discrete Random

Ours
32x32/Grayscale
32x32/Mask
2D
0-100%
5

2D
100%

Random

tables, benches, vessels, and cabinets. For each category, we use ShapeNet’s
default split for training, validation, and test. While generating the training
images, we ﬁrst rotate the voxelized 3D model around its center using a rotation
vector r = [rx, ry, rz], where rx ∈ [−20◦, 40◦] and ry ∈ [0◦, 360◦] are uniformly
sampled rotation angles of Altitude and Azimuth; we always set rz = 0. We
then project the rotated 3D voxel into a binary mask as the image for training,
validation, and testing, where the rotation vector r is the camera pose. For
each 3D shape, we generate 5 masks from diﬀerent camera poses. During the
experiments, we also want to restrict the amount of pose supervision. A model is
trained with r% of pose supervision if r% of model instances are annotated with
poses. We will explore 100%, 50%, 10%, and 1% of pose annotations in diﬀerent
settings. All training images, no matter whether they have pose annotations or
not, are used as unlabeled images in all settings.

Learning Single-View 3D Reconstruction with Limited Pose Supervision

9

Note that our data settings are diﬀerent from prior work, and indeed the
settings in prior works diﬀer from each other. We use input images with the low-
est resolution (32 × 32) and no color cues (grayscale) compared to the synthetic
dataset from Tulsiani et al. [20], McRecon [10], and PTN [27]. We use fewer
viewpoints than PTN [27], and our viewpoints are sampled randomly, making it
a more diﬃcult task. Our data setting only provides 2D supervision with camera
pose, which is diﬀerent from McRecon [10] that also used unlabeled 3D super-
vision (U3D). The precise data setting is orthogonal to our focus, which is on
combining pose supervision and unlabeled images. As such, we select the set-
ting with less information provided when compared to prior works. A detailed
comparison is presented in Table 1.

5.2 Evaluation metrics

To evaluate the performance of our model, we use the Intersection-over-Union
(IoU) between the ground truth voxel grid and the predicted one, averaged over
all objects. Computing the IoU requires thresholding the probability output of
voxels from the generator. As suggested by Tulsiani et al. [20], we sweep over
thresholds and report the maximum average IoU. We also report IoU0.4 and
IoU0.5 for comparison with previous work, and the Average Precision (AP).

5.3 Semi-supervised single-category

We use 6 categories: airplanes, benches, cars, chairs, sofas, and tables for
single-category experiments under semi-supervised setting. In this setting, we
train a separate model for each category. We experiment with varying amounts
of pose supervision from 0% to 100%.

Comparison with prior work: We ﬁrst compare with prior work that uses
full pose/instance supervision. We train our models with 50% of the images an-
notated with instance and pose. The models are trained for 20,000 iterations
with early stopping (i.e., keeping the model with the best performance in val-
idation set). Performance comparisons are shown in Table 2. The performance
of our model is comparable with prior work across multiple metrics. The results
suggest that while using only 50% of pose supervisions, our model outperforms
McRecon [10] and MVC [20], but it performs worse than PTN [27] in terms
of IoU0.5. However, note that due to diﬀerences in the setting across diﬀerent
approaches, the numbers are not exactly commensurate.

Are unlabeled images useful? We next ask if using unlabeled images and
an adversarial loss to provide additional supervision and regularization is useful.
We compare three models: 1) a model trained with both pose-annotated and
unlabeled images; 2) a model trained on just the pose-annotated images; and
3) a model trained on only the unlabeled images. In the third case, since the
model doesn’t have data to train the encoder, we adopt the training scheme
of PrGAN [7] by ﬁrst training the generator G and discriminator D together
as a GAN, and then using the generator to train an encoder E once the GAN

10

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

Table 2. Comparison between our model and prior work on single-view 3D recon-
struction. All models are trained with images from a single category. Our model’s
performance is comparable with prior models while using only 50% pose supervision.

MVC [20] McRecon [10] PTN [27] Ours (50% pose annotations)

Category

airplanes
benches
cars
chairs
sofas
tables

IoU
0.55
-
0.75
0.42
-
-

AP
0.59
0.39
0.82
0.48
0.56
0.46

IoU0.4
0.37
0.30
0.56
0.35
0.38
0.35

IoU0.5
-
-
-
0.49
-
-

IoU
0.57
0.36
0.78
0.44
0.54
0.44

AP
0.75
0.48
0.92
0.60
0.69
0.63

IoU0.4 IoU0.5
0.56
0.57
0.35
0.35
0.77
0.77
0.43
0.42
0.53
0.52
0.43
0.42

Fig. 5. Comparison between three variations of our models trained with: 1) combined
pose-annotated and unlabeled images, 2) pose-annotated images only, and 3) unla-
beled images only. Our model is able to leverage both data with pose annotation and
unlabeled data. Unlabeled data is especially helpful in the case of limited supervision.

training is done. We compare these models on the chair category with diﬀerent
amounts of pose supervision. Results are presented in Fig. 5.

First, compared to the purely unsupervised approach (0 pose supervision),
when only 1% of the data has pose annotations (45 models, 225 images), perfor-
mance increases signiﬁcantly. This suggests that pose supervision is necessary,
and that our model could successfully leverage such supervision to make better
predictions. Second, the model that combines pose annotations with unlabeled
images outperforms the one that uses only pose-annotated images. The lesser
the pose annotation available, the larger the gain, indicating that an adversarial
loss on unlabeled images is useful especially in the case when pose supervisions
and viewpoints are limited (≤ 10%). Third, given enough pose supervision (50%
or even 100%), the performance gap between the pose-supervision-only model
and the combined model is greatly reduced. This suggests that when there are
enough images with pose annotations, leveraging unlabeled data is unnecessary.

Learning Single-View 3D Reconstruction with Limited Pose Supervision

11

Table 3. Performance of category-agnostic models under diﬀerent amount of pose
supervision. Using same amount of supervision (50%), the performance of category-
agnostic model is on par with its category-speciﬁc counterpart, indicating that we
don’t need category supervision.

Test categories

airplanes
cars
chairs
displays
phones
speakers
tables
Mean

50% single
IoU AP
0.75
0.57
0.92
0.78
0.60
0.44
0.61
0.44
0.69
0.55
0.73
0.58
0.63
0.44
0.70
0.54

Pose supervision and problem setting
10% multi

1% multi
100% multi 50% multi
IoU AP IoU AP IoU AP IoU AP
0.49 0.63
0.58
0.71 0.81
0.79
0.31 0.39
0.45
0.26 0.32
0.43
0.42 0.52
0.55
0.45 0.58
0.59
0.29 0.39
0.46
0.42 0.52
0.55

0.73
0.93
0.57
0.58
0.73
0.73
0.61
0.70

0.76
0.93
0.57
0.59
0.72
0.74
0.63
0.71

0.57
0.78
0.44
0.43
0.56
0.59
0.45
0.55

0.54
0.78
0.41
0.36
0.50
0.55
0.40
0.51

0.75
0.93
0.54
0.49
0.64
0.69
0.54
0.65

5.4 Semi-supervised multi-category

We next experiment with a category-agnostic model on combined training data
from 7 categories : airplanes, cars, chairs, displays, phones, speakers,
and tables. This experiment is also conducted with diﬀerent amount of pose
annotations. Results are reported in Table 3.

In general, using more pose supervision yields better performance of category-
agnostic model. With the same amount of pose supervision (50%) for each cat-
egory, the category-agnostic model achieves similar performance compared with
the category-speciﬁc models. This suggests that the model is able to remedy the
removal of category information by learning a category-agnostic representation.

5.5 Few-shot transfer learning

What happens when a new class comes along that the system has not seen
before? In this case, the model should be able to transfer the knowledge it has
acquired and adapt it to the new class with very limited annotated training data.
To evaluate if this is possible, we use the category-agnostic model, pre-trained
on the dataset described in Sec 5.4, and adapt it to three unseen categories:
benches, vessels, and carbinets. For each of the novel categories, only 1%
of the pose-annotated data is provided. As a result, each novel category usually
has about 13 3D-shapes or about 65 pose-annotated images.

We compare three models in this experiment. From scratch: a model trained
from scratch on the given novel category without using any pre-training; Out-
of-Category [27]: the pre-trained category-agnostic model directly applied on
the novel classes without any additional training; and Fine-tuning: a pre-
trained category-agnostic model ﬁne-tuned on the given novel category. The
ﬁne-tuning is done by ﬁxing the encoder and training the generator only using
pose-annotated images for a few iterations. We used the same training strategy

12

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

B enches

Cabinets

Vessels

F rom scratch
Out-of-Category
F ine-tuning

F rom scratch
Out-of-Category
F ine-tuning

0.34

0.32

0.30

0.28

U
o

I

0.26

0.24

0.22

0.20

0.45

0.40

P
A

0.35

0.30

0.25

0.50

0.45

0.40

U
o

I

0.35

0.30

0.65

0.60

0.55

P
A

0.50

0.45

0.40

F rom scratch
Out-of-Category
F ine-tuning

Vessels

F rom scratch
Out-of-Category
F ine-tuning

F rom scratch
Out-of-Category
F ine-tuning

F rom scratch
Out-of-Category
F ine-tuning

0.46

0.44

0.42

U
o

I

0.40

0.38

0.675

0.650

0.625

0.600

P
A

0.575

0.550

0.525

0.500

0.475

1%

10%

50%

100%

1%

10%

50%

100%

1%

10%

50%

100%

B enches

Cabinets

1%

10%

50%

100%

1%

10%

50%

100%

1%

10%

50%

100%

P r e - t r a i n i n g P ose S upervision

P r e - t r a i n i n g P ose S upervision

P r e - t r a i n i n g P ose S upervision

Fig. 6. Few-shot transfer learning on novel categories. Each column represents the
performance on a novel category (IoU in top row and AP in bottom row). Notice that
the horizontal axis shows the amount of pose annotated supervision in pre-training.

Table 4. Comparing diﬀerent training strategies on chairs with 1% pose annotations.
Fine-tuning a category-agnostic model on the target category works the best.

IoU
AP

S, P
0.2913
0.3800

S, U S, P+U M
0.3175
0.2065
0.4162
0.2180

0.3104
0.3859

FT
0.3250
0.4247

as mentioned in 4.3 for all three models. In this experiment, we varies the amount
of pose annotations used for pre-traning. The results are shown in Fig. 6.

First, we observe that ﬁne-tuning a pre-trained model for a novel category
performs much better than training from scratch without pre-training. This sug-
gests that transferring the knowledge learned from a pre-trained model is essen-
tial for few-shot learning on new categories. Second, compared with the out-
of-category baseline, ﬁne-tuning improves the performance a lot upon directly
using the pre-trained model, especially in the case of limited pose supervision.
This indicates that our model is able to quickly adapt to a novel category with
few training examples via ﬁne-tuning.

5.6 How best to use limited annotation?

We now have all the ingredients necessary to answer the question: given a very
small number of pose annotations, what is the best way to train a single-view
3D reconstruction model?

Table 4 compares multiple training strategies on chairs: using just the pose-
annotated images of chairs (S, P), using just unlabeled images of chairs (S,

Learning Single-View 3D Reconstruction with Limited Pose Supervision

13

Fig. 7. 3D shape generation on the validation set. The top row shows input images
(32 × 32 grayscale). The corresponding ground truth voxels and generated ones are
presented in the middle row and bottom row, respectively. The models are trained
with semi-supervised single-category setting with 50% pose supervision.

Fig. 8. Interpolation within-category (top 3 rows) and cross-category (bottom 3 rows).
Given the latent vector of the left most shape z1 and the right-most shape z2, inter-
mediate shapes correspond to G(z1 + α(z2 − z1)), where α ∈ [0, 1].

U), using both pose-annotated and unlabeled images of chairs (S, P+U),
combining multiple categories to train a category-agnostic model (M), and ﬁne-
tuning a category-agnostic model for chairs (FT). The ﬁne-tuned model works
best, indicating that it is best to combine both pose-annotated and unlabeled
images, to leverage multiple categories and to retain category-speciﬁcity.

14

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

Fig. 9. Latent space arithmetic.

Fig. 10. Shape predictions from models with diﬀerent amount of pose supervisions.
From left to right: input image, ground truth voxel, and then shapes from models
presented in Fig. 5. P : training with pose annotation; S : training with unlabeled data.
The percentage indicates the amount of pose annotation.

5.7 Qualitative Results

Fig. 7 shows some qualitative results from our category-speciﬁc model trained
with 50% pose annotations. In addition to single-image 3D reconstruction, our
model learns a meaningful representation of shape, as shown by the ability to
do interpolation and arithmetic in the latent space (Fig. 8, 9).

The qualitative impact of reducing annotations is shown in Fig. 10. When the
amount of supervision is reduced, one sees a signiﬁcant amount of noise in the
3D reconstructions, which seems to reduce when unlabeled images are included.

6 Conclusions

In conclusion, we propose a uniﬁed and end-to-end model to use both images
labeled with camera pose and unlabeled images as supervision for single view 3D
reconstruction, and evaluate diﬀerent training strategies when annotations are
limited. Our experiments show that one can train a single-view reconstruction
model with few pose annotations when leveraging unlabeled data. Future work
will include conﬁrming and extending these results on more practical settings
with high resolution RGB images and arbitrary camera locations.

Learning Single-View 3D Reconstruction with Limited Pose Supervision

15

References

1. Agarwal, S., Furukawa, Y., Snavely, N., Simon, I., Curless, B., Seitz, S.M., Szeliski,

R.: Building rome in a day. Communications of the ACM (2011) 3

2. Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer normalization. arXiv preprint

arXiv:1607.06450 (2016) 8

3. Blanz, V., Vetter, T.: A morphable model for the synthesis of 3d faces. In: Pro-
ceedings of the 26th annual conference on Computer graphics and interactive tech-
niques. ACM Press/Addison-Wesley Publishing Co. (1999) 3

4. Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z.,
Savarese, S., Savva, M., Song, S., Su, H., Xiao, J., Yi, L., Yu, F.: ShapeNet:
An Information-Rich 3D Model Repository. Tech. Rep. arXiv:1512.03012 [cs.GR],
Stanford University — Princeton University — Toyota Technological Institute at
Chicago (2015) 1, 7

5. Choy, C.B., Xu, D., Gwak, J., Chen, K., Savarese, S.: 3d-r2n2: A uniﬁed approach

for single and multi-view 3d object reconstruction. In: ECCV (2016) 3

6. Fan, H., Su, H., Guibas, L.J.: A point set generation network for 3d object recon-

struction from a single image. In: CVPR. vol. 2, p. 6 (2017) 3

7. Gadelha, M., Maji, S., Wang, R.: 3d shape induction from 2d views of multiple

objects. In: 3DV (2017) 3, 5, 7, 9

8. Girdhar, R., Fouhey, D.F., Rodriguez, M., Gupta, A.: Learning a predictable and

generative vector representation for objects. In: ECCV (2016) 3

9. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,

Courville, A., Bengio, Y.: Generative adversarial nets. In: NIPS (2014) 3

10. Gwak, J., Choy, C.B., Garg, A., Chandraker, M., Savarese, S.: Weakly supervised
generative adversarial networks for 3d reconstruction. In: 3DV (2017) 3, 7, 8, 9,
10

11. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by

reducing internal covariate shift. In: ICML (2015) 8

12. Kar, A., Tulsiani, S., Carreira, J., Malik, J.: Category-speciﬁc object reconstruction

from a single image. In: CVPR (2015) 3

13. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: ICLR

(2015) 7

(2018) 7

14. Kodali, N., Hays, J., Abernethy, J., Kira, Z.: On convergence and stability of gans

15. Maas, A.L., Hannun, A.Y., Ng, A.Y.: Rectiﬁer nonlinearities improve neural net-

work acoustic models. In: ICML (2013) 8

16. Radford, A., Metz, L., Chintala, S.: Unsupervised representation learning with

deep convolutional generative adversarial networks. In: ICLR (2016) 8

17. Rezende, D.J., Eslami, S.A., Mohamed, S., Battaglia, P., Jaderberg, M., Heess, N.:

Unsupervised learning of 3d structure from images. In: NIPS (2016) 3

18. Rock, J., Gupta, T., Thorsen, J., Gwak, J., Shin, D., Hoiem, D.: Completing 3d

object shape from one depth image. In: CVPR (2015) 3

19. Saxena, A., Sun, M., Ng, A.Y.: Make3d: Learning 3d scene structure from a single

still image. PAMI (2009) 3

20. Tulsiani, S., Efros, A.A., Malik, J.: Multi-view consistency as supervisory signal

for learning shape and pose prediction. In: CVPR (2018) 3, 8, 9, 10

21. Tulsiani, S., Zhou, T., Efros, A.A., Malik, J.: Multi-view supervision for single-view

reconstruction via diﬀerentiable ray consistency. In: CVPR (2017) 1, 3

16

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

22. Ullman, S.: The interpretation of structure from motion. In: Proc. R. Soc. Lond.

23. Vicente, S., Carreira, J., Agapito, L., Batista, J.: Reconstructing pascal voc. In:

B. The Royal Society (1979) 3

CVPR (2014) 3

24. Wu, J., Xue, T., Lim, J.J., Tian, Y., Tenenbaum, J.B., Torralba, A., Freeman,

W.T.: Single image 3d interpreter network. In: ECCV (2016) 3

25. Wu, J., Zhang, C., Xue, T., Freeman, W.T., Tenenbaum, J.B.: Learning a prob-
abilistic latent space of object shapes via 3d generative-adversarial modeling. In:
NIPS (2016) 3, 8

26. Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., Xiao, J.: 3d shapenets:

A deep representation for volumetric shapes. In: CVPR (2015) 3

27. Yan, X., Yang, J., Yumer, E., Guo, Y., Lee, H.: Perspective transformer nets:
Learning single-view 3d object reconstruction without 3d supervision. In: NIPS
(2016) 1, 3, 5, 6, 8, 9, 10, 11

Learning Single-View 3D Reconstruction with
Limited Pose Supervision

Guandao Yang1, Yin Cui1,2, Serge Belongie1,2, Bharath Hariharan1

1 Department of Computer Science, Cornell University
2 Cornell Tech

Abstract. It is expensive to label images with 3D structure or precise
camera pose. Yet, this is precisely the kind of annotation required to train
single-view 3D reconstruction models. In contrast, unlabeled images or
images with just category labels are easy to acquire, but few current
models can use this weak supervision. We present a uniﬁed framework
that can combine both types of supervision: a small amount of cam-
era pose annotations are used to enforce pose-invariance and view-point
consistency, and unlabeled images combined with an adversarial loss are
used to enforce the realism of rendered, generated models. We use this
uniﬁed framework to measure the impact of each form of supervision in
three paradigms: semi-supervised, multi-task, and transfer learning. We
show that with a combination of these ideas, we can train single-view
reconstruction models that improve up to 7 points in performance (AP)
when using only 1% pose annotated training data.

Keywords: single-image 3d-reconstruction, few-shot learning, GANs

1

Introduction

The ability to understand 3D structure from single images is a hallmark of the
human visual system and a crucial step in visual reasoning and interaction. Of
course, a single image by itself does not have enough information to allow 3D
reconstruction, and a machine vision system must rely on some prior over shape:
all cars have wheels, for example. The crucial question is how a machine vision
system can acquire such priors.

One possibility is to leverage datasets of 3D shapes [4], but obtaining such
a dataset for a wide variety of categories requires either 3D modeling exper-
tise or 3D scanning tools and is therefore expensive. Another option, extensively
explored recently [27,21], is to show the machine many diﬀerent views of a multi-
tude of objects from calibrated cameras. The machine can then use photometric
consistency between rendered views of hypothesized shape and the correspond-
ing view of the real object as a learning signal. Although more tractable than
collecting 3D models, this approach is still very expensive in practice: one needs
to either physically acquire thousands of objects and place them on a turntable,
or ask human annotators to annotate images in the wild with both the camera
parameters and the precise instance that the image depicts. The assumption

2

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

Model

p11

p12

p13

p11

p12

p13

p21

p22

p23

p21

p22

p23

Images from same category with 
annotated camera poses

Images across categories with annotated 
camera poses

Collection of images of same category 
without pose annotation

Fig. 1. We propose a uniﬁed framework for single-view 3D reconstruction. Our model
can be trained with diﬀerent types of data, including pose-annotated images from the
same object category or across multiple categories, and unlabeled images.

that multiple, calibrated views of thousands of objects are available is also bi-
ologically implausible: a human infant must physically interact with objects to
acquire such training data, but most humans can understand airplane shape very
easily despite having played with very few airplanes.

Our goal in this paper is to learn eﬀective single-view 3D reconstruction
models when calibrated multi-view images are available for very few objects. To
do so we look at two additional sources of information. First, what if we had a
large collection of images of a category but without any annotation of the precise
instance or pose? Such a dataset is easy to acquire by simply downloading images
of this category from the web (Fig. 1, lower right). While it might be hard to
extract 3D information from such images, they can capture the distribution of the
visual appearance of objects from this category. Second, we look at annotations
from other semantic classes (Fig. 1, lower middle). These other classes might not
tell us about the nuances of a particular class, but they can still help delineate
what shapes in general look like. For example, most shapes are compact, smooth,
tend to be convex, etc.

This paper presents a framework that can eﬀectively use all these sources
of information. First, we design a uniﬁed model architecture and loss functions
that combine pose supervision with weaker supervision from unlabeled images.
Then, we use our model and training framework to evaluate and compare many
training paradigms and forms of supervision to come up with the best way of
using a small number of pose annotations eﬀectively. In particular, we show that:

1. Images without instance or pose annotations are indeed useful and can pro-
vide signiﬁcant gains in performance (up to 5 points in AP). At the same
time a little bit of pose supervision (< 50 objects) gives a large gain (> 20
points AP) when compared to not using pose information at all.

Learning Single-View 3D Reconstruction with Limited Pose Supervision

3

2. Category-agnostic priors obtained by pooling training data across classes
work just as well as, but not better than, category-speciﬁc priors trained on
each class individually.

3. Fine-tuning category-agnostic models for a novel semantic class using a small
amount (i.e. only 1%) of pose supervision signiﬁcantly improves performance
(up to 7 points in AP).

4. When faced with a novel category with nothing but a tiny set of pose-
annotated images, a category-agnostic model trained on pooled data and
ﬁne-tuned on the category of interest outperforms a baseline trained on only
the novel category by an enormous margin (up to 20 points in AP).

In summary, our results convincingly show large accuracy gains to be accrued
from combining multiple sources of data (unlabeled or labeled from diﬀerent
classes) with a single uniﬁed model.

2 Related Work

Despite many successes in reconstructing 3D scenes from multiple images [22,1],
doing it on a single image remains challenging. Classic work on single-image
3D reconstruction relies on having access to images labeled with the 3D struc-
ture [19]. This is also true for many recent deep learning approaches [8,5,24,18,6].
To get away from this need for precise 3D models, some work leverages key-
point and silhouette annotations [23,12]. More recent approaches assume multi-
ple views with calibrated cameras for training [27,10,21,17], and design training
loss functions that leverage photometric consistency and/or enforce invariance
to pose. Among these, our encoder-decoder architecture is similar to the one
proposed in PTN [27], but our model is trained end-to-end and is additionally
able to leverage unlabeled images to deal with limited supervision. In terms of
the required supervision, Tulsiani et al. [20] remove the requirement for pose
annotations but still require images to be annotated with the instance they
correspond to. PrGAN [7] reduces the supervision requirement further by only
using unlabeled images. As we show in this paper, this makes the problem need-
lessly challenging, while adding small amounts of pose supervision leads to large
accuracy gains.

Recovering 3D structure from a single image requires strong priors about
shape, and another line of work has focused on better capturing the manifold
of shape. Classic work has used low-dimensional parametric models [12,3]. More
recently, the rediscovery of convolutional networks has led to a resurgence in
interest in deep generative models. Wu et al. [26] used deep belief nets to model
3D shapes while Rezende et al. [17] consider variants of variational autoencoders.
Generative adversarial networks or GANs [9] can also be used to build generative
models of shapes [25]. The challenge is to train them without 3D data: Gadelha
et al. [7] show that this is indeed possible. While we use an adversarial loss as
they suggest, our generator is trained jointly with an encoder end-to-end on a
combination of pose-supervised and unlabeled images.

4

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

Chairs
Tables

Airplanes

Cars

R - Rotation

T - Translate

Image

Category

Instance ID

Camera Pose

3D Shape

Fig. 2. Diﬀerent forms of training annotations for single-view 3D reconstruction.
Note that some annotations (e.g. category) are cheaper to obtain than others (e.g.
3D shapes); and conversely some oﬀer a better training signal than others.

3 Training Paradigms

For single-view 3D reconstruction, we consider four types of annotations for an
image as illustrated in Fig 2. Our goal is to minimize the need for the more
expensive annotations (instance ID, camera pose and 3D shape). Towards this
end, we look at three diﬀerent training paradigms.

3.1 Semi-supervised single-category

In this setting, we assume all images are from a single category. Noting the
fact that camera pose and model-instance annotations are diﬃcult to collect
in the wild, we restrict to a semi-supervised setting where only some of the
images are labeled with camera pose and most of them are unlabeled. Formally,
we are given a dataset of images annotated with both camera pose and the
instance ID: Xl = {(xij, pij, i)}i,j, where xij represents the j-th image of the
i-th instance when projected with camera pose pij. We also have a dataset
without any annotation: Xu = {xi}i. The goal is to use Xl and Xu to learn a
category-speciﬁc model for single image 3D reconstruction.

3.2 Semi-supervised multi-category

An alternative to building a separate model for each category is to build a
category-agnostic model. This allows one to combine training data across multi-
ple categories, and even use training images that do not have any category labels.
Thus, instead of a separate labeled training set X c
for each category c, here we
l
l ∪ X c2
only assume a combined dataset X multi
. Similarly,
we assume access to an unlabeled set of images X multi
(now without category
labels). Note that this multi-category setting is harder than the single-category
since it introduces cross-category confusion, but it also allows the model to learn
category-agnostic shape information across diﬀerent categories.

l ∪ · · · ∪ X cn

= X c1

u

l

l

3.3 Few-shot transfer learning

Collecting a large dataset that can cover all categories we would ever encounter
is infeasible. Therefore, we also need a way to adapt a pre-trained model to a new

Learning Single-View 3D Reconstruction with Limited Pose Supervision

5

category. This strategy can also be used for adapting a category-agnostic model
to a speciﬁc category. We assume that for this adaptation, a dataset X (new)
containing a very small number of images with pose and instance annotations
(< 100) are available for the category of interest. We also assume that the semi-
supervised multi-category dataset described above is available as a pre-training
dataset: X pre

and X pre

u = X multi

l = X multi

u

.

l

l

4 A Uniﬁed Framework

We need a model and a training framework that can utilize both images with
pose and instance annotations, and images without any labels. The former set
of images can be used to enforce the consistency of the predicted 3D shape
across views, as well as the similarity between the rendered 3D shape and the
corresponding view of the real object. The latter set of images can only provide
constraints on the realism of the generated shapes. To capture all these con-
straints, we propose a uniﬁed model architecture with three main components:

1. An Encoder E that takes an image (silhouette) as input and produces a

2. A Generator G that takes a latent representation of shape as input and

latent representation of shape.

produces a voxel grid.

3. A Discriminator D that tries to distinguish between rendered views of the

voxel output by the generator and views of the real objects.

In addition, we make use of a “projector” module P that takes a voxel and a
viewpoint as input, and it renders the voxel from the inputted viewpoint. We
use a diﬀerentiable projector similar to the one in PrGAN [7]. We extend it to
perspective projection. P has no trainable parameters.

The training process alternates between an iteration on images labeled with
pose and instance, and an iteration on unlabeled images. The two sets of itera-
tions use diﬀerent loss functions but update the same model.

4.1 Training on pose-annotated images

In each pass on the annotated images, the encoder is provided with pairs of
images xi1, xi2 of the same 3D object i taken from diﬀerent camera poses p1 and
p2. The encoder E embeds each image into latent vectors z1, z2. The generator
(decoder) G is tasked with predicting the 3D voxel grid from z1 and z2.

The 3D voxel grid produced by the generator should be: 1) a good recon-
struction of the object and 2) invariant to the pose of the input image [27]. This
requires that the latent shape representation also be invariant to the camera
pose of the input image. To ensure the pose invariance of the learned latent rep-
resentation z1, the predicted 3D voxel from z1 should be able to reconstruct the
second input image when projected to the second viewpoint p2, and vice versa.

With these intuitions in mind, we explore the following three losses.

6

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

G

G

Proj

Proj

p1

p2

E

z1

E

z2

p

z

G

Proj

Gaussian

D

 
YES / NO 
Is the image generated? 

Fig. 3. Overview of the proposed model architecture. An encoder E and a generator
G with pose consistency (on the top) learn from images with pose supervision, and a
discriminator D (on the bottom) helps G to learn from unlabeled images. Notice that
two encoders E and three generator G in the diagram all share parameters, respectively.

Reconstruction loss: The predicted 3D model, when projected with a cer-
tain camera pose, should be consistent with the ground truth image projected
from that camera pose. More speciﬁcally, let (x1, p1) and (x2, p2) be two pairs
of image-pose pair sampled from a 3D-model, then the voxel reconstructed from
E(x1) should produce the same image as x2 if projected from camera pose p2.
Same for the other view. Let P (v, p) represent the image generated by project-
ing voxel v using camera pose p. We deﬁne the reconstruction loss to address
this consistency requirement as:

Lrecon = kP (G(E(x2)), p1) − x1k1+2 + kP (G(E(x1)), p2) − x2k1+2

(1)

where k · k1+2 = k · k1 + k · k2 is the summation of ℓ1 and ℓ2 reconstruction losses.
Such reconstruction loss has been used in prior work [27]. We add ℓ1 loss since
ℓ1 loss could better cope with sparse vectors such as silhouette images.

Pose-invariance loss on representations: Given two randomly sampled
views of an object, the encoder E should be able to embed their latent repre-
sentations close by, irrespective of pose. Therefore, we deﬁne a pose-invariance
loss on the latent representations:

Lpinv = kE(x1) − E(x2)k2

Pose-invariance loss on voxels: Similarly, the 3D voxel output recon-
structed by the generator G from two diﬀerent views of the same object should
be the same. Thus, we introduce a voxel-based pose invariance loss:

Lvinv = kG(E(x1)) − G(E(x2))k1

(2)

(3)

Learning Single-View 3D Reconstruction with Limited Pose Supervision

7

Losses are illustrated by the dashed lines in Fig. 3. Each training step on the

images with pose annotations tries to minimize the combined supervised loss:

Lsupervised = Lrecon + αLpinv + βLvinv

(4)

where α and β are weights for Lpinv and Lvinv, respectively. We use α = β = 0.1
in all of our experiments.

4.2 Training on unlabeled images

In order to learn from unlabeled images, we use an adversarial loss, as illustrated
in the bottom of Fig. 3. The intuition is to let the generator G learn to generate
3D voxel grids. When projected from a random viewpoint, the 3D voxel grid
should be able to produce an image that is indistinguishable from a real image.
Another advantage of an adversarial loss is regularization, as in the McRecon
approach [10]. Speciﬁcally, we ﬁrst sample a vector z ∼ N (0, I) and a viewpoint
p uniformly sampled from the range of camera poses observed in the training
set. Then the generator G will take the latent vector z and reconstruct a 3D
shape. This 3D shape will be projected to an image using the random pose p.
No matter which camera pose we project, the projected image should look like
an image sampled from the dataset. We update the generator and discriminator
by using an adversarial loss similar to the one used by PrGAN [7]:

LD = Ez,p[log(1 − D(P (G(z), p)))] + Ex∼X [log D(x)]
LG = −Ez,p[log D(P (G(z), p))]

(5)

(6)

Note that instead of normally distributed z vectors, one could also use the
encoder output on sampled training images. However, encouraging G to produce
meaningful shapes even on noise input might force G to capture shape priors.

4.3

Implementation details

The detailed architectures of encoder, generator and discriminator are illustrated
in Fig. 4. In the projector (not shown in Fig. 4), we ﬁrst rotate the voxelized 3D
model by its center and then use perspective projection to produce the image
according to the camera pose. The whole model is trained end-to-end by alternat-
ing between iterations on pose-annotated and unlabeled images. We use Adam
optimizer [13] with learning rates of 10−3, 10−4, and 10−4 for encoder, gener-
ator, and discriminator respectively. While training using the adversarial loss,
we used the gradient penalty introduced by DRAGAN [14] to improve training
stability. Codes are available at https://github.com/stevenygd/3d-recon.

5 Experiments

5.1 Dataset

We use voxelized 32 × 32 × 32 3D shapes from the ShapeNetCore [4] dataset. We
look at 10 categories: airplanes, cars, chairs, displays, phones, speakers,

8

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

Fig. 4. Model Architectures of encoder, generator and discriminator. Conv: Convo-
lution,BN: Batch Normalization [11],LN: Layer Normalization [2],L-ReLU:leaky ReLU
with slope of 0.2 [15], ConvT:Transposed Convolution that is often used in generation
tasks [16,25]. FC, k :Fully-Connected layers with k outputs. The discriminator outputs
a probability that the image is generated.

Table 1. Comparison between synthetic datasets used in prior work and ours. The key
diﬀerence is the amount of pose annotations available during training. We experiment
with multiple settings.

Dataset Properties MVC [20] McRecon [10] PTN [27]
Input image
64x64/RGB 127x127/RGB 64x64/RGB
Supervision image 64x64/Mask 127x127/Mask 32x32/Mask
Supervision level
Pose annotations
#views per image 5
Pose selection

2D + U3D
100%
Unavailable
Random

2D
100%
8-24
Fixed discrete Random

Ours
32x32/Grayscale
32x32/Mask
2D
0-100%
5

2D
100%

Random

tables, benches, vessels, and cabinets. For each category, we use ShapeNet’s
default split for training, validation, and test. While generating the training
images, we ﬁrst rotate the voxelized 3D model around its center using a rotation
vector r = [rx, ry, rz], where rx ∈ [−20◦, 40◦] and ry ∈ [0◦, 360◦] are uniformly
sampled rotation angles of Altitude and Azimuth; we always set rz = 0. We
then project the rotated 3D voxel into a binary mask as the image for training,
validation, and testing, where the rotation vector r is the camera pose. For
each 3D shape, we generate 5 masks from diﬀerent camera poses. During the
experiments, we also want to restrict the amount of pose supervision. A model is
trained with r% of pose supervision if r% of model instances are annotated with
poses. We will explore 100%, 50%, 10%, and 1% of pose annotations in diﬀerent
settings. All training images, no matter whether they have pose annotations or
not, are used as unlabeled images in all settings.

Learning Single-View 3D Reconstruction with Limited Pose Supervision

9

Note that our data settings are diﬀerent from prior work, and indeed the
settings in prior works diﬀer from each other. We use input images with the low-
est resolution (32 × 32) and no color cues (grayscale) compared to the synthetic
dataset from Tulsiani et al. [20], McRecon [10], and PTN [27]. We use fewer
viewpoints than PTN [27], and our viewpoints are sampled randomly, making it
a more diﬃcult task. Our data setting only provides 2D supervision with camera
pose, which is diﬀerent from McRecon [10] that also used unlabeled 3D super-
vision (U3D). The precise data setting is orthogonal to our focus, which is on
combining pose supervision and unlabeled images. As such, we select the set-
ting with less information provided when compared to prior works. A detailed
comparison is presented in Table 1.

5.2 Evaluation metrics

To evaluate the performance of our model, we use the Intersection-over-Union
(IoU) between the ground truth voxel grid and the predicted one, averaged over
all objects. Computing the IoU requires thresholding the probability output of
voxels from the generator. As suggested by Tulsiani et al. [20], we sweep over
thresholds and report the maximum average IoU. We also report IoU0.4 and
IoU0.5 for comparison with previous work, and the Average Precision (AP).

5.3 Semi-supervised single-category

We use 6 categories: airplanes, benches, cars, chairs, sofas, and tables for
single-category experiments under semi-supervised setting. In this setting, we
train a separate model for each category. We experiment with varying amounts
of pose supervision from 0% to 100%.

Comparison with prior work: We ﬁrst compare with prior work that uses
full pose/instance supervision. We train our models with 50% of the images an-
notated with instance and pose. The models are trained for 20,000 iterations
with early stopping (i.e., keeping the model with the best performance in val-
idation set). Performance comparisons are shown in Table 2. The performance
of our model is comparable with prior work across multiple metrics. The results
suggest that while using only 50% of pose supervisions, our model outperforms
McRecon [10] and MVC [20], but it performs worse than PTN [27] in terms
of IoU0.5. However, note that due to diﬀerences in the setting across diﬀerent
approaches, the numbers are not exactly commensurate.

Are unlabeled images useful? We next ask if using unlabeled images and
an adversarial loss to provide additional supervision and regularization is useful.
We compare three models: 1) a model trained with both pose-annotated and
unlabeled images; 2) a model trained on just the pose-annotated images; and
3) a model trained on only the unlabeled images. In the third case, since the
model doesn’t have data to train the encoder, we adopt the training scheme
of PrGAN [7] by ﬁrst training the generator G and discriminator D together
as a GAN, and then using the generator to train an encoder E once the GAN

10

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

Table 2. Comparison between our model and prior work on single-view 3D recon-
struction. All models are trained with images from a single category. Our model’s
performance is comparable with prior models while using only 50% pose supervision.

MVC [20] McRecon [10] PTN [27] Ours (50% pose annotations)

Category

airplanes
benches
cars
chairs
sofas
tables

IoU
0.55
-
0.75
0.42
-
-

AP
0.59
0.39
0.82
0.48
0.56
0.46

IoU0.4
0.37
0.30
0.56
0.35
0.38
0.35

IoU0.5
-
-
-
0.49
-
-

IoU
0.57
0.36
0.78
0.44
0.54
0.44

AP
0.75
0.48
0.92
0.60
0.69
0.63

IoU0.4 IoU0.5
0.56
0.57
0.35
0.35
0.77
0.77
0.43
0.42
0.53
0.52
0.43
0.42

Fig. 5. Comparison between three variations of our models trained with: 1) combined
pose-annotated and unlabeled images, 2) pose-annotated images only, and 3) unla-
beled images only. Our model is able to leverage both data with pose annotation and
unlabeled data. Unlabeled data is especially helpful in the case of limited supervision.

training is done. We compare these models on the chair category with diﬀerent
amounts of pose supervision. Results are presented in Fig. 5.

First, compared to the purely unsupervised approach (0 pose supervision),
when only 1% of the data has pose annotations (45 models, 225 images), perfor-
mance increases signiﬁcantly. This suggests that pose supervision is necessary,
and that our model could successfully leverage such supervision to make better
predictions. Second, the model that combines pose annotations with unlabeled
images outperforms the one that uses only pose-annotated images. The lesser
the pose annotation available, the larger the gain, indicating that an adversarial
loss on unlabeled images is useful especially in the case when pose supervisions
and viewpoints are limited (≤ 10%). Third, given enough pose supervision (50%
or even 100%), the performance gap between the pose-supervision-only model
and the combined model is greatly reduced. This suggests that when there are
enough images with pose annotations, leveraging unlabeled data is unnecessary.

Learning Single-View 3D Reconstruction with Limited Pose Supervision

11

Table 3. Performance of category-agnostic models under diﬀerent amount of pose
supervision. Using same amount of supervision (50%), the performance of category-
agnostic model is on par with its category-speciﬁc counterpart, indicating that we
don’t need category supervision.

Test categories

airplanes
cars
chairs
displays
phones
speakers
tables
Mean

50% single
IoU AP
0.75
0.57
0.92
0.78
0.60
0.44
0.61
0.44
0.69
0.55
0.73
0.58
0.63
0.44
0.70
0.54

Pose supervision and problem setting
10% multi

1% multi
100% multi 50% multi
IoU AP IoU AP IoU AP IoU AP
0.49 0.63
0.58
0.71 0.81
0.79
0.31 0.39
0.45
0.26 0.32
0.43
0.42 0.52
0.55
0.45 0.58
0.59
0.29 0.39
0.46
0.42 0.52
0.55

0.76
0.93
0.57
0.59
0.72
0.74
0.63
0.71

0.73
0.93
0.57
0.58
0.73
0.73
0.61
0.70

0.57
0.78
0.44
0.43
0.56
0.59
0.45
0.55

0.54
0.78
0.41
0.36
0.50
0.55
0.40
0.51

0.75
0.93
0.54
0.49
0.64
0.69
0.54
0.65

5.4 Semi-supervised multi-category

We next experiment with a category-agnostic model on combined training data
from 7 categories : airplanes, cars, chairs, displays, phones, speakers,
and tables. This experiment is also conducted with diﬀerent amount of pose
annotations. Results are reported in Table 3.

In general, using more pose supervision yields better performance of category-
agnostic model. With the same amount of pose supervision (50%) for each cat-
egory, the category-agnostic model achieves similar performance compared with
the category-speciﬁc models. This suggests that the model is able to remedy the
removal of category information by learning a category-agnostic representation.

5.5 Few-shot transfer learning

What happens when a new class comes along that the system has not seen
before? In this case, the model should be able to transfer the knowledge it has
acquired and adapt it to the new class with very limited annotated training data.
To evaluate if this is possible, we use the category-agnostic model, pre-trained
on the dataset described in Sec 5.4, and adapt it to three unseen categories:
benches, vessels, and carbinets. For each of the novel categories, only 1%
of the pose-annotated data is provided. As a result, each novel category usually
has about 13 3D-shapes or about 65 pose-annotated images.

We compare three models in this experiment. From scratch: a model trained
from scratch on the given novel category without using any pre-training; Out-
of-Category [27]: the pre-trained category-agnostic model directly applied on
the novel classes without any additional training; and Fine-tuning: a pre-
trained category-agnostic model ﬁne-tuned on the given novel category. The
ﬁne-tuning is done by ﬁxing the encoder and training the generator only using
pose-annotated images for a few iterations. We used the same training strategy

12

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

B enches

Cabinets

Vessels

F rom scratch
Out-of-Category
F ine-tuning

F rom scratch
Out-of-Category
F ine-tuning

0.34

0.32

0.30

0.28

U
o

I

0.26

0.24

0.22

0.20

0.45

0.40

P
A

0.35

0.30

0.25

0.50

0.45

0.40

U
o

I

0.35

0.30

0.65

0.60

0.55

P
A

0.50

0.45

0.40

F rom scratch
Out-of-Category
F ine-tuning

Vessels

F rom scratch
Out-of-Category
F ine-tuning

F rom scratch
Out-of-Category
F ine-tuning

F rom scratch
Out-of-Category
F ine-tuning

0.46

0.44

0.42

U
o

I

0.40

0.38

0.675

0.650

0.625

0.600

P
A

0.575

0.550

0.525

0.500

0.475

1%

10%

50%

100%

1%

10%

50%

100%

1%

10%

50%

100%

B enches

Cabinets

1%

10%

50%

100%

1%

10%

50%

100%

1%

10%

50%

100%

P r e - t r a i n i n g P ose S upervision

P r e - t r a i n i n g P ose S upervision

P r e - t r a i n i n g P ose S upervision

Fig. 6. Few-shot transfer learning on novel categories. Each column represents the
performance on a novel category (IoU in top row and AP in bottom row). Notice that
the horizontal axis shows the amount of pose annotated supervision in pre-training.

Table 4. Comparing diﬀerent training strategies on chairs with 1% pose annotations.
Fine-tuning a category-agnostic model on the target category works the best.

IoU
AP

S, P
0.2913
0.3800

S, U S, P+U M
0.3175
0.2065
0.4162
0.2180

0.3104
0.3859

FT
0.3250
0.4247

as mentioned in 4.3 for all three models. In this experiment, we varies the amount
of pose annotations used for pre-traning. The results are shown in Fig. 6.

First, we observe that ﬁne-tuning a pre-trained model for a novel category
performs much better than training from scratch without pre-training. This sug-
gests that transferring the knowledge learned from a pre-trained model is essen-
tial for few-shot learning on new categories. Second, compared with the out-
of-category baseline, ﬁne-tuning improves the performance a lot upon directly
using the pre-trained model, especially in the case of limited pose supervision.
This indicates that our model is able to quickly adapt to a novel category with
few training examples via ﬁne-tuning.

5.6 How best to use limited annotation?

We now have all the ingredients necessary to answer the question: given a very
small number of pose annotations, what is the best way to train a single-view
3D reconstruction model?

Table 4 compares multiple training strategies on chairs: using just the pose-
annotated images of chairs (S, P), using just unlabeled images of chairs (S,

Learning Single-View 3D Reconstruction with Limited Pose Supervision

13

Fig. 7. 3D shape generation on the validation set. The top row shows input images
(32 × 32 grayscale). The corresponding ground truth voxels and generated ones are
presented in the middle row and bottom row, respectively. The models are trained
with semi-supervised single-category setting with 50% pose supervision.

Fig. 8. Interpolation within-category (top 3 rows) and cross-category (bottom 3 rows).
Given the latent vector of the left most shape z1 and the right-most shape z2, inter-
mediate shapes correspond to G(z1 + α(z2 − z1)), where α ∈ [0, 1].

U), using both pose-annotated and unlabeled images of chairs (S, P+U),
combining multiple categories to train a category-agnostic model (M), and ﬁne-
tuning a category-agnostic model for chairs (FT). The ﬁne-tuned model works
best, indicating that it is best to combine both pose-annotated and unlabeled
images, to leverage multiple categories and to retain category-speciﬁcity.

14

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

Fig. 9. Latent space arithmetic.

Fig. 10. Shape predictions from models with diﬀerent amount of pose supervisions.
From left to right: input image, ground truth voxel, and then shapes from models
presented in Fig. 5. P : training with pose annotation; S : training with unlabeled data.
The percentage indicates the amount of pose annotation.

5.7 Qualitative Results

Fig. 7 shows some qualitative results from our category-speciﬁc model trained
with 50% pose annotations. In addition to single-image 3D reconstruction, our
model learns a meaningful representation of shape, as shown by the ability to
do interpolation and arithmetic in the latent space (Fig. 8, 9).

The qualitative impact of reducing annotations is shown in Fig. 10. When the
amount of supervision is reduced, one sees a signiﬁcant amount of noise in the
3D reconstructions, which seems to reduce when unlabeled images are included.

6 Conclusions

In conclusion, we propose a uniﬁed and end-to-end model to use both images
labeled with camera pose and unlabeled images as supervision for single view 3D
reconstruction, and evaluate diﬀerent training strategies when annotations are
limited. Our experiments show that one can train a single-view reconstruction
model with few pose annotations when leveraging unlabeled data. Future work
will include conﬁrming and extending these results on more practical settings
with high resolution RGB images and arbitrary camera locations.

Learning Single-View 3D Reconstruction with Limited Pose Supervision

15

References

1. Agarwal, S., Furukawa, Y., Snavely, N., Simon, I., Curless, B., Seitz, S.M., Szeliski,

R.: Building rome in a day. Communications of the ACM (2011) 3

2. Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer normalization. arXiv preprint

arXiv:1607.06450 (2016) 8

3. Blanz, V., Vetter, T.: A morphable model for the synthesis of 3d faces. In: Pro-
ceedings of the 26th annual conference on Computer graphics and interactive tech-
niques. ACM Press/Addison-Wesley Publishing Co. (1999) 3

4. Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z.,
Savarese, S., Savva, M., Song, S., Su, H., Xiao, J., Yi, L., Yu, F.: ShapeNet:
An Information-Rich 3D Model Repository. Tech. Rep. arXiv:1512.03012 [cs.GR],
Stanford University — Princeton University — Toyota Technological Institute at
Chicago (2015) 1, 7

5. Choy, C.B., Xu, D., Gwak, J., Chen, K., Savarese, S.: 3d-r2n2: A uniﬁed approach

for single and multi-view 3d object reconstruction. In: ECCV (2016) 3

6. Fan, H., Su, H., Guibas, L.J.: A point set generation network for 3d object recon-

struction from a single image. In: CVPR. vol. 2, p. 6 (2017) 3

7. Gadelha, M., Maji, S., Wang, R.: 3d shape induction from 2d views of multiple

objects. In: 3DV (2017) 3, 5, 7, 9

8. Girdhar, R., Fouhey, D.F., Rodriguez, M., Gupta, A.: Learning a predictable and

generative vector representation for objects. In: ECCV (2016) 3

9. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,

Courville, A., Bengio, Y.: Generative adversarial nets. In: NIPS (2014) 3

10. Gwak, J., Choy, C.B., Garg, A., Chandraker, M., Savarese, S.: Weakly supervised
generative adversarial networks for 3d reconstruction. In: 3DV (2017) 3, 7, 8, 9,
10

11. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by

reducing internal covariate shift. In: ICML (2015) 8

12. Kar, A., Tulsiani, S., Carreira, J., Malik, J.: Category-speciﬁc object reconstruction

from a single image. In: CVPR (2015) 3

13. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: ICLR

(2015) 7

(2018) 7

14. Kodali, N., Hays, J., Abernethy, J., Kira, Z.: On convergence and stability of gans

15. Maas, A.L., Hannun, A.Y., Ng, A.Y.: Rectiﬁer nonlinearities improve neural net-

work acoustic models. In: ICML (2013) 8

16. Radford, A., Metz, L., Chintala, S.: Unsupervised representation learning with

deep convolutional generative adversarial networks. In: ICLR (2016) 8

17. Rezende, D.J., Eslami, S.A., Mohamed, S., Battaglia, P., Jaderberg, M., Heess, N.:

Unsupervised learning of 3d structure from images. In: NIPS (2016) 3

18. Rock, J., Gupta, T., Thorsen, J., Gwak, J., Shin, D., Hoiem, D.: Completing 3d

object shape from one depth image. In: CVPR (2015) 3

19. Saxena, A., Sun, M., Ng, A.Y.: Make3d: Learning 3d scene structure from a single

still image. PAMI (2009) 3

20. Tulsiani, S., Efros, A.A., Malik, J.: Multi-view consistency as supervisory signal

for learning shape and pose prediction. In: CVPR (2018) 3, 8, 9, 10

21. Tulsiani, S., Zhou, T., Efros, A.A., Malik, J.: Multi-view supervision for single-view

reconstruction via diﬀerentiable ray consistency. In: CVPR (2017) 1, 3

16

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

22. Ullman, S.: The interpretation of structure from motion. In: Proc. R. Soc. Lond.

23. Vicente, S., Carreira, J., Agapito, L., Batista, J.: Reconstructing pascal voc. In:

B. The Royal Society (1979) 3

CVPR (2014) 3

24. Wu, J., Xue, T., Lim, J.J., Tian, Y., Tenenbaum, J.B., Torralba, A., Freeman,

W.T.: Single image 3d interpreter network. In: ECCV (2016) 3

25. Wu, J., Zhang, C., Xue, T., Freeman, W.T., Tenenbaum, J.B.: Learning a prob-
abilistic latent space of object shapes via 3d generative-adversarial modeling. In:
NIPS (2016) 3, 8

26. Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., Xiao, J.: 3d shapenets:

A deep representation for volumetric shapes. In: CVPR (2015) 3

27. Yan, X., Yang, J., Yumer, E., Guo, Y., Lee, H.: Perspective transformer nets:
Learning single-view 3d object reconstruction without 3d supervision. In: NIPS
(2016) 1, 3, 5, 6, 8, 9, 10, 11

Learning Single-View 3D Reconstruction with
Limited Pose Supervision

Guandao Yang1, Yin Cui1,2, Serge Belongie1,2, Bharath Hariharan1

1 Department of Computer Science, Cornell University
2 Cornell Tech

Abstract. It is expensive to label images with 3D structure or precise
camera pose. Yet, this is precisely the kind of annotation required to train
single-view 3D reconstruction models. In contrast, unlabeled images or
images with just category labels are easy to acquire, but few current
models can use this weak supervision. We present a uniﬁed framework
that can combine both types of supervision: a small amount of cam-
era pose annotations are used to enforce pose-invariance and view-point
consistency, and unlabeled images combined with an adversarial loss are
used to enforce the realism of rendered, generated models. We use this
uniﬁed framework to measure the impact of each form of supervision in
three paradigms: semi-supervised, multi-task, and transfer learning. We
show that with a combination of these ideas, we can train single-view
reconstruction models that improve up to 7 points in performance (AP)
when using only 1% pose annotated training data.

Keywords: single-image 3d-reconstruction, few-shot learning, GANs

1

Introduction

The ability to understand 3D structure from single images is a hallmark of the
human visual system and a crucial step in visual reasoning and interaction. Of
course, a single image by itself does not have enough information to allow 3D
reconstruction, and a machine vision system must rely on some prior over shape:
all cars have wheels, for example. The crucial question is how a machine vision
system can acquire such priors.

One possibility is to leverage datasets of 3D shapes [4], but obtaining such
a dataset for a wide variety of categories requires either 3D modeling exper-
tise or 3D scanning tools and is therefore expensive. Another option, extensively
explored recently [27,21], is to show the machine many diﬀerent views of a multi-
tude of objects from calibrated cameras. The machine can then use photometric
consistency between rendered views of hypothesized shape and the correspond-
ing view of the real object as a learning signal. Although more tractable than
collecting 3D models, this approach is still very expensive in practice: one needs
to either physically acquire thousands of objects and place them on a turntable,
or ask human annotators to annotate images in the wild with both the camera
parameters and the precise instance that the image depicts. The assumption

2

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

Model

p11

p12

p13

p11

p12

p13

p21

p22

p23

p21

p22

p23

Images from same category with 
annotated camera poses

Images across categories with annotated 
camera poses

Collection of images of same category 
without pose annotation

Fig. 1. We propose a uniﬁed framework for single-view 3D reconstruction. Our model
can be trained with diﬀerent types of data, including pose-annotated images from the
same object category or across multiple categories, and unlabeled images.

that multiple, calibrated views of thousands of objects are available is also bi-
ologically implausible: a human infant must physically interact with objects to
acquire such training data, but most humans can understand airplane shape very
easily despite having played with very few airplanes.

Our goal in this paper is to learn eﬀective single-view 3D reconstruction
models when calibrated multi-view images are available for very few objects. To
do so we look at two additional sources of information. First, what if we had a
large collection of images of a category but without any annotation of the precise
instance or pose? Such a dataset is easy to acquire by simply downloading images
of this category from the web (Fig. 1, lower right). While it might be hard to
extract 3D information from such images, they can capture the distribution of the
visual appearance of objects from this category. Second, we look at annotations
from other semantic classes (Fig. 1, lower middle). These other classes might not
tell us about the nuances of a particular class, but they can still help delineate
what shapes in general look like. For example, most shapes are compact, smooth,
tend to be convex, etc.

This paper presents a framework that can eﬀectively use all these sources
of information. First, we design a uniﬁed model architecture and loss functions
that combine pose supervision with weaker supervision from unlabeled images.
Then, we use our model and training framework to evaluate and compare many
training paradigms and forms of supervision to come up with the best way of
using a small number of pose annotations eﬀectively. In particular, we show that:

1. Images without instance or pose annotations are indeed useful and can pro-
vide signiﬁcant gains in performance (up to 5 points in AP). At the same
time a little bit of pose supervision (< 50 objects) gives a large gain (> 20
points AP) when compared to not using pose information at all.

Learning Single-View 3D Reconstruction with Limited Pose Supervision

3

2. Category-agnostic priors obtained by pooling training data across classes
work just as well as, but not better than, category-speciﬁc priors trained on
each class individually.

3. Fine-tuning category-agnostic models for a novel semantic class using a small
amount (i.e. only 1%) of pose supervision signiﬁcantly improves performance
(up to 7 points in AP).

4. When faced with a novel category with nothing but a tiny set of pose-
annotated images, a category-agnostic model trained on pooled data and
ﬁne-tuned on the category of interest outperforms a baseline trained on only
the novel category by an enormous margin (up to 20 points in AP).

In summary, our results convincingly show large accuracy gains to be accrued
from combining multiple sources of data (unlabeled or labeled from diﬀerent
classes) with a single uniﬁed model.

2 Related Work

Despite many successes in reconstructing 3D scenes from multiple images [22,1],
doing it on a single image remains challenging. Classic work on single-image
3D reconstruction relies on having access to images labeled with the 3D struc-
ture [19]. This is also true for many recent deep learning approaches [8,5,24,18,6].
To get away from this need for precise 3D models, some work leverages key-
point and silhouette annotations [23,12]. More recent approaches assume multi-
ple views with calibrated cameras for training [27,10,21,17], and design training
loss functions that leverage photometric consistency and/or enforce invariance
to pose. Among these, our encoder-decoder architecture is similar to the one
proposed in PTN [27], but our model is trained end-to-end and is additionally
able to leverage unlabeled images to deal with limited supervision. In terms of
the required supervision, Tulsiani et al. [20] remove the requirement for pose
annotations but still require images to be annotated with the instance they
correspond to. PrGAN [7] reduces the supervision requirement further by only
using unlabeled images. As we show in this paper, this makes the problem need-
lessly challenging, while adding small amounts of pose supervision leads to large
accuracy gains.

Recovering 3D structure from a single image requires strong priors about
shape, and another line of work has focused on better capturing the manifold
of shape. Classic work has used low-dimensional parametric models [12,3]. More
recently, the rediscovery of convolutional networks has led to a resurgence in
interest in deep generative models. Wu et al. [26] used deep belief nets to model
3D shapes while Rezende et al. [17] consider variants of variational autoencoders.
Generative adversarial networks or GANs [9] can also be used to build generative
models of shapes [25]. The challenge is to train them without 3D data: Gadelha
et al. [7] show that this is indeed possible. While we use an adversarial loss as
they suggest, our generator is trained jointly with an encoder end-to-end on a
combination of pose-supervised and unlabeled images.

4

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

Chairs
Tables

Airplanes

Cars

R - Rotation

T - Translate

Image

Category

Instance ID

Camera Pose

3D Shape

Fig. 2. Diﬀerent forms of training annotations for single-view 3D reconstruction.
Note that some annotations (e.g. category) are cheaper to obtain than others (e.g.
3D shapes); and conversely some oﬀer a better training signal than others.

3 Training Paradigms

For single-view 3D reconstruction, we consider four types of annotations for an
image as illustrated in Fig 2. Our goal is to minimize the need for the more
expensive annotations (instance ID, camera pose and 3D shape). Towards this
end, we look at three diﬀerent training paradigms.

3.1 Semi-supervised single-category

In this setting, we assume all images are from a single category. Noting the
fact that camera pose and model-instance annotations are diﬃcult to collect
in the wild, we restrict to a semi-supervised setting where only some of the
images are labeled with camera pose and most of them are unlabeled. Formally,
we are given a dataset of images annotated with both camera pose and the
instance ID: Xl = {(xij, pij, i)}i,j, where xij represents the j-th image of the
i-th instance when projected with camera pose pij. We also have a dataset
without any annotation: Xu = {xi}i. The goal is to use Xl and Xu to learn a
category-speciﬁc model for single image 3D reconstruction.

3.2 Semi-supervised multi-category

An alternative to building a separate model for each category is to build a
category-agnostic model. This allows one to combine training data across multi-
ple categories, and even use training images that do not have any category labels.
Thus, instead of a separate labeled training set X c
for each category c, here we
l
l ∪ X c2
only assume a combined dataset X multi
. Similarly,
we assume access to an unlabeled set of images X multi
(now without category
labels). Note that this multi-category setting is harder than the single-category
since it introduces cross-category confusion, but it also allows the model to learn
category-agnostic shape information across diﬀerent categories.

l ∪ · · · ∪ X cn

= X c1

u

l

l

3.3 Few-shot transfer learning

Collecting a large dataset that can cover all categories we would ever encounter
is infeasible. Therefore, we also need a way to adapt a pre-trained model to a new

Learning Single-View 3D Reconstruction with Limited Pose Supervision

5

category. This strategy can also be used for adapting a category-agnostic model
to a speciﬁc category. We assume that for this adaptation, a dataset X (new)
containing a very small number of images with pose and instance annotations
(< 100) are available for the category of interest. We also assume that the semi-
supervised multi-category dataset described above is available as a pre-training
dataset: X pre

and X pre

u = X multi

l = X multi

u

.

l

l

4 A Uniﬁed Framework

We need a model and a training framework that can utilize both images with
pose and instance annotations, and images without any labels. The former set
of images can be used to enforce the consistency of the predicted 3D shape
across views, as well as the similarity between the rendered 3D shape and the
corresponding view of the real object. The latter set of images can only provide
constraints on the realism of the generated shapes. To capture all these con-
straints, we propose a uniﬁed model architecture with three main components:

1. An Encoder E that takes an image (silhouette) as input and produces a

2. A Generator G that takes a latent representation of shape as input and

latent representation of shape.

produces a voxel grid.

3. A Discriminator D that tries to distinguish between rendered views of the

voxel output by the generator and views of the real objects.

In addition, we make use of a “projector” module P that takes a voxel and a
viewpoint as input, and it renders the voxel from the inputted viewpoint. We
use a diﬀerentiable projector similar to the one in PrGAN [7]. We extend it to
perspective projection. P has no trainable parameters.

The training process alternates between an iteration on images labeled with
pose and instance, and an iteration on unlabeled images. The two sets of itera-
tions use diﬀerent loss functions but update the same model.

4.1 Training on pose-annotated images

In each pass on the annotated images, the encoder is provided with pairs of
images xi1, xi2 of the same 3D object i taken from diﬀerent camera poses p1 and
p2. The encoder E embeds each image into latent vectors z1, z2. The generator
(decoder) G is tasked with predicting the 3D voxel grid from z1 and z2.

The 3D voxel grid produced by the generator should be: 1) a good recon-
struction of the object and 2) invariant to the pose of the input image [27]. This
requires that the latent shape representation also be invariant to the camera
pose of the input image. To ensure the pose invariance of the learned latent rep-
resentation z1, the predicted 3D voxel from z1 should be able to reconstruct the
second input image when projected to the second viewpoint p2, and vice versa.

With these intuitions in mind, we explore the following three losses.

6

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

G

G

Proj

Proj

p1

p2

E

z1

E

z2

p

z

G

Proj

Gaussian

D

 
YES / NO 
Is the image generated? 

Fig. 3. Overview of the proposed model architecture. An encoder E and a generator
G with pose consistency (on the top) learn from images with pose supervision, and a
discriminator D (on the bottom) helps G to learn from unlabeled images. Notice that
two encoders E and three generator G in the diagram all share parameters, respectively.

Reconstruction loss: The predicted 3D model, when projected with a cer-
tain camera pose, should be consistent with the ground truth image projected
from that camera pose. More speciﬁcally, let (x1, p1) and (x2, p2) be two pairs
of image-pose pair sampled from a 3D-model, then the voxel reconstructed from
E(x1) should produce the same image as x2 if projected from camera pose p2.
Same for the other view. Let P (v, p) represent the image generated by project-
ing voxel v using camera pose p. We deﬁne the reconstruction loss to address
this consistency requirement as:

Lrecon = kP (G(E(x2)), p1) − x1k1+2 + kP (G(E(x1)), p2) − x2k1+2

(1)

where k · k1+2 = k · k1 + k · k2 is the summation of ℓ1 and ℓ2 reconstruction losses.
Such reconstruction loss has been used in prior work [27]. We add ℓ1 loss since
ℓ1 loss could better cope with sparse vectors such as silhouette images.

Pose-invariance loss on representations: Given two randomly sampled
views of an object, the encoder E should be able to embed their latent repre-
sentations close by, irrespective of pose. Therefore, we deﬁne a pose-invariance
loss on the latent representations:

Lpinv = kE(x1) − E(x2)k2

Pose-invariance loss on voxels: Similarly, the 3D voxel output recon-
structed by the generator G from two diﬀerent views of the same object should
be the same. Thus, we introduce a voxel-based pose invariance loss:

Lvinv = kG(E(x1)) − G(E(x2))k1

(2)

(3)

Learning Single-View 3D Reconstruction with Limited Pose Supervision

7

Losses are illustrated by the dashed lines in Fig. 3. Each training step on the

images with pose annotations tries to minimize the combined supervised loss:

Lsupervised = Lrecon + αLpinv + βLvinv

(4)

where α and β are weights for Lpinv and Lvinv, respectively. We use α = β = 0.1
in all of our experiments.

4.2 Training on unlabeled images

In order to learn from unlabeled images, we use an adversarial loss, as illustrated
in the bottom of Fig. 3. The intuition is to let the generator G learn to generate
3D voxel grids. When projected from a random viewpoint, the 3D voxel grid
should be able to produce an image that is indistinguishable from a real image.
Another advantage of an adversarial loss is regularization, as in the McRecon
approach [10]. Speciﬁcally, we ﬁrst sample a vector z ∼ N (0, I) and a viewpoint
p uniformly sampled from the range of camera poses observed in the training
set. Then the generator G will take the latent vector z and reconstruct a 3D
shape. This 3D shape will be projected to an image using the random pose p.
No matter which camera pose we project, the projected image should look like
an image sampled from the dataset. We update the generator and discriminator
by using an adversarial loss similar to the one used by PrGAN [7]:

LD = Ez,p[log(1 − D(P (G(z), p)))] + Ex∼X [log D(x)]
LG = −Ez,p[log D(P (G(z), p))]

(5)

(6)

Note that instead of normally distributed z vectors, one could also use the
encoder output on sampled training images. However, encouraging G to produce
meaningful shapes even on noise input might force G to capture shape priors.

4.3

Implementation details

The detailed architectures of encoder, generator and discriminator are illustrated
in Fig. 4. In the projector (not shown in Fig. 4), we ﬁrst rotate the voxelized 3D
model by its center and then use perspective projection to produce the image
according to the camera pose. The whole model is trained end-to-end by alternat-
ing between iterations on pose-annotated and unlabeled images. We use Adam
optimizer [13] with learning rates of 10−3, 10−4, and 10−4 for encoder, gener-
ator, and discriminator respectively. While training using the adversarial loss,
we used the gradient penalty introduced by DRAGAN [14] to improve training
stability. Codes are available at https://github.com/stevenygd/3d-recon.

5 Experiments

5.1 Dataset

We use voxelized 32 × 32 × 32 3D shapes from the ShapeNetCore [4] dataset. We
look at 10 categories: airplanes, cars, chairs, displays, phones, speakers,

8

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

Fig. 4. Model Architectures of encoder, generator and discriminator. Conv: Convo-
lution,BN: Batch Normalization [11],LN: Layer Normalization [2],L-ReLU:leaky ReLU
with slope of 0.2 [15], ConvT:Transposed Convolution that is often used in generation
tasks [16,25]. FC, k :Fully-Connected layers with k outputs. The discriminator outputs
a probability that the image is generated.

Table 1. Comparison between synthetic datasets used in prior work and ours. The key
diﬀerence is the amount of pose annotations available during training. We experiment
with multiple settings.

Dataset Properties MVC [20] McRecon [10] PTN [27]
Input image
64x64/RGB 127x127/RGB 64x64/RGB
Supervision image 64x64/Mask 127x127/Mask 32x32/Mask
Supervision level
Pose annotations
#views per image 5
Pose selection

2D + U3D
100%
Unavailable
Random

2D
100%
8-24
Fixed discrete Random

Ours
32x32/Grayscale
32x32/Mask
2D
0-100%
5

2D
100%

Random

tables, benches, vessels, and cabinets. For each category, we use ShapeNet’s
default split for training, validation, and test. While generating the training
images, we ﬁrst rotate the voxelized 3D model around its center using a rotation
vector r = [rx, ry, rz], where rx ∈ [−20◦, 40◦] and ry ∈ [0◦, 360◦] are uniformly
sampled rotation angles of Altitude and Azimuth; we always set rz = 0. We
then project the rotated 3D voxel into a binary mask as the image for training,
validation, and testing, where the rotation vector r is the camera pose. For
each 3D shape, we generate 5 masks from diﬀerent camera poses. During the
experiments, we also want to restrict the amount of pose supervision. A model is
trained with r% of pose supervision if r% of model instances are annotated with
poses. We will explore 100%, 50%, 10%, and 1% of pose annotations in diﬀerent
settings. All training images, no matter whether they have pose annotations or
not, are used as unlabeled images in all settings.

Learning Single-View 3D Reconstruction with Limited Pose Supervision

9

Note that our data settings are diﬀerent from prior work, and indeed the
settings in prior works diﬀer from each other. We use input images with the low-
est resolution (32 × 32) and no color cues (grayscale) compared to the synthetic
dataset from Tulsiani et al. [20], McRecon [10], and PTN [27]. We use fewer
viewpoints than PTN [27], and our viewpoints are sampled randomly, making it
a more diﬃcult task. Our data setting only provides 2D supervision with camera
pose, which is diﬀerent from McRecon [10] that also used unlabeled 3D super-
vision (U3D). The precise data setting is orthogonal to our focus, which is on
combining pose supervision and unlabeled images. As such, we select the set-
ting with less information provided when compared to prior works. A detailed
comparison is presented in Table 1.

5.2 Evaluation metrics

To evaluate the performance of our model, we use the Intersection-over-Union
(IoU) between the ground truth voxel grid and the predicted one, averaged over
all objects. Computing the IoU requires thresholding the probability output of
voxels from the generator. As suggested by Tulsiani et al. [20], we sweep over
thresholds and report the maximum average IoU. We also report IoU0.4 and
IoU0.5 for comparison with previous work, and the Average Precision (AP).

5.3 Semi-supervised single-category

We use 6 categories: airplanes, benches, cars, chairs, sofas, and tables for
single-category experiments under semi-supervised setting. In this setting, we
train a separate model for each category. We experiment with varying amounts
of pose supervision from 0% to 100%.

Comparison with prior work: We ﬁrst compare with prior work that uses
full pose/instance supervision. We train our models with 50% of the images an-
notated with instance and pose. The models are trained for 20,000 iterations
with early stopping (i.e., keeping the model with the best performance in val-
idation set). Performance comparisons are shown in Table 2. The performance
of our model is comparable with prior work across multiple metrics. The results
suggest that while using only 50% of pose supervisions, our model outperforms
McRecon [10] and MVC [20], but it performs worse than PTN [27] in terms
of IoU0.5. However, note that due to diﬀerences in the setting across diﬀerent
approaches, the numbers are not exactly commensurate.

Are unlabeled images useful? We next ask if using unlabeled images and
an adversarial loss to provide additional supervision and regularization is useful.
We compare three models: 1) a model trained with both pose-annotated and
unlabeled images; 2) a model trained on just the pose-annotated images; and
3) a model trained on only the unlabeled images. In the third case, since the
model doesn’t have data to train the encoder, we adopt the training scheme
of PrGAN [7] by ﬁrst training the generator G and discriminator D together
as a GAN, and then using the generator to train an encoder E once the GAN

10

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

Table 2. Comparison between our model and prior work on single-view 3D recon-
struction. All models are trained with images from a single category. Our model’s
performance is comparable with prior models while using only 50% pose supervision.

MVC [20] McRecon [10] PTN [27] Ours (50% pose annotations)

Category

airplanes
benches
cars
chairs
sofas
tables

IoU
0.55
-
0.75
0.42
-
-

AP
0.59
0.39
0.82
0.48
0.56
0.46

IoU0.4
0.37
0.30
0.56
0.35
0.38
0.35

IoU0.5
-
-
-
0.49
-
-

IoU
0.57
0.36
0.78
0.44
0.54
0.44

AP
0.75
0.48
0.92
0.60
0.69
0.63

IoU0.4 IoU0.5
0.56
0.57
0.35
0.35
0.77
0.77
0.43
0.42
0.53
0.52
0.43
0.42

Fig. 5. Comparison between three variations of our models trained with: 1) combined
pose-annotated and unlabeled images, 2) pose-annotated images only, and 3) unla-
beled images only. Our model is able to leverage both data with pose annotation and
unlabeled data. Unlabeled data is especially helpful in the case of limited supervision.

training is done. We compare these models on the chair category with diﬀerent
amounts of pose supervision. Results are presented in Fig. 5.

First, compared to the purely unsupervised approach (0 pose supervision),
when only 1% of the data has pose annotations (45 models, 225 images), perfor-
mance increases signiﬁcantly. This suggests that pose supervision is necessary,
and that our model could successfully leverage such supervision to make better
predictions. Second, the model that combines pose annotations with unlabeled
images outperforms the one that uses only pose-annotated images. The lesser
the pose annotation available, the larger the gain, indicating that an adversarial
loss on unlabeled images is useful especially in the case when pose supervisions
and viewpoints are limited (≤ 10%). Third, given enough pose supervision (50%
or even 100%), the performance gap between the pose-supervision-only model
and the combined model is greatly reduced. This suggests that when there are
enough images with pose annotations, leveraging unlabeled data is unnecessary.

Learning Single-View 3D Reconstruction with Limited Pose Supervision

11

Table 3. Performance of category-agnostic models under diﬀerent amount of pose
supervision. Using same amount of supervision (50%), the performance of category-
agnostic model is on par with its category-speciﬁc counterpart, indicating that we
don’t need category supervision.

Test categories

airplanes
cars
chairs
displays
phones
speakers
tables
Mean

50% single
IoU AP
0.75
0.57
0.92
0.78
0.60
0.44
0.61
0.44
0.69
0.55
0.73
0.58
0.63
0.44
0.70
0.54

Pose supervision and problem setting
10% multi

1% multi
100% multi 50% multi
IoU AP IoU AP IoU AP IoU AP
0.49 0.63
0.58
0.71 0.81
0.79
0.31 0.39
0.45
0.26 0.32
0.43
0.42 0.52
0.55
0.45 0.58
0.59
0.29 0.39
0.46
0.42 0.52
0.55

0.76
0.93
0.57
0.59
0.72
0.74
0.63
0.71

0.73
0.93
0.57
0.58
0.73
0.73
0.61
0.70

0.57
0.78
0.44
0.43
0.56
0.59
0.45
0.55

0.54
0.78
0.41
0.36
0.50
0.55
0.40
0.51

0.75
0.93
0.54
0.49
0.64
0.69
0.54
0.65

5.4 Semi-supervised multi-category

We next experiment with a category-agnostic model on combined training data
from 7 categories : airplanes, cars, chairs, displays, phones, speakers,
and tables. This experiment is also conducted with diﬀerent amount of pose
annotations. Results are reported in Table 3.

In general, using more pose supervision yields better performance of category-
agnostic model. With the same amount of pose supervision (50%) for each cat-
egory, the category-agnostic model achieves similar performance compared with
the category-speciﬁc models. This suggests that the model is able to remedy the
removal of category information by learning a category-agnostic representation.

5.5 Few-shot transfer learning

What happens when a new class comes along that the system has not seen
before? In this case, the model should be able to transfer the knowledge it has
acquired and adapt it to the new class with very limited annotated training data.
To evaluate if this is possible, we use the category-agnostic model, pre-trained
on the dataset described in Sec 5.4, and adapt it to three unseen categories:
benches, vessels, and carbinets. For each of the novel categories, only 1%
of the pose-annotated data is provided. As a result, each novel category usually
has about 13 3D-shapes or about 65 pose-annotated images.

We compare three models in this experiment. From scratch: a model trained
from scratch on the given novel category without using any pre-training; Out-
of-Category [27]: the pre-trained category-agnostic model directly applied on
the novel classes without any additional training; and Fine-tuning: a pre-
trained category-agnostic model ﬁne-tuned on the given novel category. The
ﬁne-tuning is done by ﬁxing the encoder and training the generator only using
pose-annotated images for a few iterations. We used the same training strategy

12

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

B enches

Cabinets

Vessels

F rom scratch
Out-of-Category
F ine-tuning

F rom scratch
Out-of-Category
F ine-tuning

0.34

0.32

0.30

0.28

U
o

I

0.26

0.24

0.22

0.20

0.45

0.40

P
A

0.35

0.30

0.25

0.50

0.45

0.40

U
o

I

0.35

0.30

0.65

0.60

0.55

P
A

0.50

0.45

0.40

F rom scratch
Out-of-Category
F ine-tuning

Vessels

F rom scratch
Out-of-Category
F ine-tuning

F rom scratch
Out-of-Category
F ine-tuning

F rom scratch
Out-of-Category
F ine-tuning

0.46

0.44

0.42

U
o

I

0.40

0.38

0.675

0.650

0.625

0.600

P
A

0.575

0.550

0.525

0.500

0.475

1%

10%

50%

100%

1%

10%

50%

100%

1%

10%

50%

100%

B enches

Cabinets

1%

10%

50%

100%

1%

10%

50%

100%

1%

10%

50%

100%

P r e - t r a i n i n g P ose S upervision

P r e - t r a i n i n g P ose S upervision

P r e - t r a i n i n g P ose S upervision

Fig. 6. Few-shot transfer learning on novel categories. Each column represents the
performance on a novel category (IoU in top row and AP in bottom row). Notice that
the horizontal axis shows the amount of pose annotated supervision in pre-training.

Table 4. Comparing diﬀerent training strategies on chairs with 1% pose annotations.
Fine-tuning a category-agnostic model on the target category works the best.

IoU
AP

S, P
0.2913
0.3800

S, U S, P+U M
0.3175
0.2065
0.4162
0.2180

0.3104
0.3859

FT
0.3250
0.4247

as mentioned in 4.3 for all three models. In this experiment, we varies the amount
of pose annotations used for pre-traning. The results are shown in Fig. 6.

First, we observe that ﬁne-tuning a pre-trained model for a novel category
performs much better than training from scratch without pre-training. This sug-
gests that transferring the knowledge learned from a pre-trained model is essen-
tial for few-shot learning on new categories. Second, compared with the out-
of-category baseline, ﬁne-tuning improves the performance a lot upon directly
using the pre-trained model, especially in the case of limited pose supervision.
This indicates that our model is able to quickly adapt to a novel category with
few training examples via ﬁne-tuning.

5.6 How best to use limited annotation?

We now have all the ingredients necessary to answer the question: given a very
small number of pose annotations, what is the best way to train a single-view
3D reconstruction model?

Table 4 compares multiple training strategies on chairs: using just the pose-
annotated images of chairs (S, P), using just unlabeled images of chairs (S,

Learning Single-View 3D Reconstruction with Limited Pose Supervision

13

Fig. 7. 3D shape generation on the validation set. The top row shows input images
(32 × 32 grayscale). The corresponding ground truth voxels and generated ones are
presented in the middle row and bottom row, respectively. The models are trained
with semi-supervised single-category setting with 50% pose supervision.

Fig. 8. Interpolation within-category (top 3 rows) and cross-category (bottom 3 rows).
Given the latent vector of the left most shape z1 and the right-most shape z2, inter-
mediate shapes correspond to G(z1 + α(z2 − z1)), where α ∈ [0, 1].

U), using both pose-annotated and unlabeled images of chairs (S, P+U),
combining multiple categories to train a category-agnostic model (M), and ﬁne-
tuning a category-agnostic model for chairs (FT). The ﬁne-tuned model works
best, indicating that it is best to combine both pose-annotated and unlabeled
images, to leverage multiple categories and to retain category-speciﬁcity.

14

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

Fig. 9. Latent space arithmetic.

Fig. 10. Shape predictions from models with diﬀerent amount of pose supervisions.
From left to right: input image, ground truth voxel, and then shapes from models
presented in Fig. 5. P : training with pose annotation; S : training with unlabeled data.
The percentage indicates the amount of pose annotation.

5.7 Qualitative Results

Fig. 7 shows some qualitative results from our category-speciﬁc model trained
with 50% pose annotations. In addition to single-image 3D reconstruction, our
model learns a meaningful representation of shape, as shown by the ability to
do interpolation and arithmetic in the latent space (Fig. 8, 9).

The qualitative impact of reducing annotations is shown in Fig. 10. When the
amount of supervision is reduced, one sees a signiﬁcant amount of noise in the
3D reconstructions, which seems to reduce when unlabeled images are included.

6 Conclusions

In conclusion, we propose a uniﬁed and end-to-end model to use both images
labeled with camera pose and unlabeled images as supervision for single view 3D
reconstruction, and evaluate diﬀerent training strategies when annotations are
limited. Our experiments show that one can train a single-view reconstruction
model with few pose annotations when leveraging unlabeled data. Future work
will include conﬁrming and extending these results on more practical settings
with high resolution RGB images and arbitrary camera locations.

Learning Single-View 3D Reconstruction with Limited Pose Supervision

15

References

1. Agarwal, S., Furukawa, Y., Snavely, N., Simon, I., Curless, B., Seitz, S.M., Szeliski,

R.: Building rome in a day. Communications of the ACM (2011) 3

2. Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer normalization. arXiv preprint

arXiv:1607.06450 (2016) 8

3. Blanz, V., Vetter, T.: A morphable model for the synthesis of 3d faces. In: Pro-
ceedings of the 26th annual conference on Computer graphics and interactive tech-
niques. ACM Press/Addison-Wesley Publishing Co. (1999) 3

4. Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z.,
Savarese, S., Savva, M., Song, S., Su, H., Xiao, J., Yi, L., Yu, F.: ShapeNet:
An Information-Rich 3D Model Repository. Tech. Rep. arXiv:1512.03012 [cs.GR],
Stanford University — Princeton University — Toyota Technological Institute at
Chicago (2015) 1, 7

5. Choy, C.B., Xu, D., Gwak, J., Chen, K., Savarese, S.: 3d-r2n2: A uniﬁed approach

for single and multi-view 3d object reconstruction. In: ECCV (2016) 3

6. Fan, H., Su, H., Guibas, L.J.: A point set generation network for 3d object recon-

struction from a single image. In: CVPR. vol. 2, p. 6 (2017) 3

7. Gadelha, M., Maji, S., Wang, R.: 3d shape induction from 2d views of multiple

objects. In: 3DV (2017) 3, 5, 7, 9

8. Girdhar, R., Fouhey, D.F., Rodriguez, M., Gupta, A.: Learning a predictable and

generative vector representation for objects. In: ECCV (2016) 3

9. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,

Courville, A., Bengio, Y.: Generative adversarial nets. In: NIPS (2014) 3

10. Gwak, J., Choy, C.B., Garg, A., Chandraker, M., Savarese, S.: Weakly supervised
generative adversarial networks for 3d reconstruction. In: 3DV (2017) 3, 7, 8, 9,
10

11. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by

reducing internal covariate shift. In: ICML (2015) 8

12. Kar, A., Tulsiani, S., Carreira, J., Malik, J.: Category-speciﬁc object reconstruction

from a single image. In: CVPR (2015) 3

13. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: ICLR

(2015) 7

(2018) 7

14. Kodali, N., Hays, J., Abernethy, J., Kira, Z.: On convergence and stability of gans

15. Maas, A.L., Hannun, A.Y., Ng, A.Y.: Rectiﬁer nonlinearities improve neural net-

work acoustic models. In: ICML (2013) 8

16. Radford, A., Metz, L., Chintala, S.: Unsupervised representation learning with

deep convolutional generative adversarial networks. In: ICLR (2016) 8

17. Rezende, D.J., Eslami, S.A., Mohamed, S., Battaglia, P., Jaderberg, M., Heess, N.:

Unsupervised learning of 3d structure from images. In: NIPS (2016) 3

18. Rock, J., Gupta, T., Thorsen, J., Gwak, J., Shin, D., Hoiem, D.: Completing 3d

object shape from one depth image. In: CVPR (2015) 3

19. Saxena, A., Sun, M., Ng, A.Y.: Make3d: Learning 3d scene structure from a single

still image. PAMI (2009) 3

20. Tulsiani, S., Efros, A.A., Malik, J.: Multi-view consistency as supervisory signal

for learning shape and pose prediction. In: CVPR (2018) 3, 8, 9, 10

21. Tulsiani, S., Zhou, T., Efros, A.A., Malik, J.: Multi-view supervision for single-view

reconstruction via diﬀerentiable ray consistency. In: CVPR (2017) 1, 3

16

Guandao Yang, Yin Cui, Serge Belongie, Bharath Hariharan

22. Ullman, S.: The interpretation of structure from motion. In: Proc. R. Soc. Lond.

23. Vicente, S., Carreira, J., Agapito, L., Batista, J.: Reconstructing pascal voc. In:

B. The Royal Society (1979) 3

CVPR (2014) 3

24. Wu, J., Xue, T., Lim, J.J., Tian, Y., Tenenbaum, J.B., Torralba, A., Freeman,

W.T.: Single image 3d interpreter network. In: ECCV (2016) 3

25. Wu, J., Zhang, C., Xue, T., Freeman, W.T., Tenenbaum, J.B.: Learning a prob-
abilistic latent space of object shapes via 3d generative-adversarial modeling. In:
NIPS (2016) 3, 8

26. Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., Xiao, J.: 3d shapenets:

A deep representation for volumetric shapes. In: CVPR (2015) 3

27. Yan, X., Yang, J., Yumer, E., Guo, Y., Lee, H.: Perspective transformer nets:
Learning single-view 3d object reconstruction without 3d supervision. In: NIPS
(2016) 1, 3, 5, 6, 8, 9, 10, 11

