8
1
0
2
 
n
u
J
 
1
 
 
]

C
O
.
h
t
a
m

[
 
 
1
v
0
7
3
0
0
.
6
0
8
1
:
v
i
X
r
a

Workshop track - ICLR 2018

NONLINEAR ACCELERATION OF CNNS

Damien Scieur∗, Edouard Oyallon†, Alexandre d’Aspremont∗ and Francis Bach∗
∗DI, Ecole Normale Sup´erieure, UMR CNRS 8548, INRIA, PSL Research University.
†CVN, CentraleSup´elec, Universit´e Paris-Saclay; Galen team, INRIA Saclay

ABSTRACT

Regularized Nonlinear Acceleration (RNA) can improve the rate of convergence
of many optimization schemes such as gradient descent, SAGA or SVRG, estimat-
ing the optimum using a nonlinear average of past iterates. Until now, its analysis
was limited to convex problems, but empirical observations show that RNA may
be extended to a broader setting. Here, we investigate the beneﬁts of nonlinear
acceleration when applied to the training of neural networks, in particular for the
task of image recognition on the CIFAR10 and ImageNet data sets. In our ex-
periments, with minimal modiﬁcations to existing frameworks, RNA speeds up
convergence and improves testing error on standard CNNs.

1

INTRODUCTION

Successful deep Convolutional Neural Networks (CNNs) for large-scale classiﬁcation are typically
optimized through a variant of the stochastic gradient descent (SGD) algorithm (Krizhevsky et al.,
2012). Reﬁning this optimization scheme is a complicated task and requires a signiﬁcant amount of
engineering whose mathematical foundations are not well understood (Wilson et al., 2017). Here,
we propose to wrap an adhoc acceleration technique known as Regularized Nonlinear Acceleration
algorithm (RNA) (Scieur et al., 2016), around existing CNN training frameworks. RNA is generic
as it does not depend on the optimization algorithm, but simply requires several successive iter-
ates of gradient based methods, which involves a minimal adaptation in many frameworks. This
meta-algorithm has been applied successfully to gradient descent in the smooth and strongly convex
cases, with convergence and rate guarantees recently derived in (Scieur et al., 2016). Recent works
(Scieur et al., 2017) further show that it improves standard stochastic optimization schemes such as
SAGA or SVRG (Defazio et al., 2014; Johnson & Zhang, 2013), which indicates it may also be a
strong candidate as an accelerated method in stochastic non convex cases.

RNA is an ideal meta-learning algorithm for deep CNNs, because contrary to many acceleration
methods Lin et al. (2015); G¨uler (1992), optimization can be performed off-line and does not in-
volve any potentially expensive extra-learning process. This means one can focus on acceleration a
posteriori. RNA related numerical computations are not expensive, and form a simple linear system
from a well-chosen linear combination of several optimization steps. This system is usually very
small relative to the number of parameters, so the cost of acceleration grows linearly with respect to
this number.

Here, we study applications applications of RNA to several recent architectures, like ResNet
(He et al., 2016), applied to classical challenging datasets, like CIFAR10 or ImageNet. Our con-
tributions are are twofold: ﬁrst we demonstrate that it is often possible to achieve an accuracy
similar to the ﬁnal epoch in half the time; second, we show that RNA slightly improves the test clas-
siﬁcation performance, at no additional numerical cost. We provide an implementation that can be
incorporated using only few lines of code around many standard Python deep learning frameworks,
like PyTorch1.

1Code can be found here:

https://github.com/windows7lover/RegularizedNonlinearAcceleration

1

Workshop track - ICLR 2018

2 ACCELERATING WITH REGULARIZED NONLINEAR ACCELERATION

This section brieﬂy describes the RNA procedure and we refer the reader to Scieur et al. (2016)
for more extensive explanations and theoretical guarantees. For the sake of simplicity, we consider
an iterate sequence {θk}0≤k≤K of K + 1 elements of Rd produced from the successive steps of
an iterative optimization algorithm. For example, each θk could correspond to the parameters of a
neural network at epoch i, trained via a gradient descent algorithm, i.e.

θk+1 = θk − η∇f (θk),
with η the step size (or learning rate) of the algorithm. Local minimization of f is naturally achieved
by θ∗ where ∇f (θ∗) = 0. RNA aims to linearly combine the parameters θk into an estimate ˆθ

ˆθ =

X
k≤K

ckθk s.t.

ck = 1,

X
k≤K

(1)

(2)

(3)

so that ∇f (ˆθ) becomes smaller. In other terms, RNA output ˆθ which solves approximately

∇f (cid:16)Pk≤K ckθk(cid:17) (cid:13)
minc (cid:13)
(cid:13)
(cid:13)

2

subject to

Pk≤K ck = 1.

In the next subsection, we describe the algorithm and intuitively explain the acceleration mechanism
when using the optimization method (1), because this restrictive setting makes the analysis simpler.

2.1 REGULARIZED NONLINEAR ACCELERATION ALGORITHM

In practice, solving (2) is a difﬁcult task. Instead, we will assume that the function f is approximately
quadratic in the neighbourhood of {θk}k≤K. This approximation is common in optimization for the
design of (quasi-)second order methods, such as the Newton’s method or BFGS. Thus, ∇f can be
considered almost as an afﬁne function, which means:

−∇f (cid:16)Pk≤K ckθk(cid:17) ≈

Pk≤K ck∇f (θk) .

From a ﬁnite difference scheme, one can easily recover {∇f (θk)}k from the iterates in (1), because
for any k, we have −η∇f (θk) = (θk+1 − θk). As linearized iterates of a ﬂow tends to be aligned,
minimizing the ℓ2-norm of (3) requires incorporating some regularization to avoid ill-conditioning

minc kRck2 + λkck2

subject to

Pk≤K ck = 1,

where R = [θ1 − θ0, . . . , θK − θK−1]. This exactly corresponds to the combination of steps 2
and 3 of Algorithm 1. Similar ideas hold is the stochastic case (Scieur et al., 2017), under limited
assumption on the signal to noise ratio.

Algorithm 1 Regularized Nonlinear Acceleration (RNA), (and computational complexity).
Input: Sequence of vectors {θ0, θ1, . . . , θK} ∈ Rd , regularization parameter λ > 0.
1: Compute R = [θ1 − θ0, . . . , θK − θK−1]
2: Solve (RT R + λI)z = 1.
3: Normalize c = z/(
Output: ˆθ =

Pk≤K zk).

O(K)
O(K 2d + K 3)
O(K)
O(Kd)

Pk≤K ckθk.

2.2 PRACTICAL USAGE

We produced a software package based on PyTorch that includes in minimal modiﬁcations of exist-
ing standard procedures. As claimed, the RNA procedure does not require any access to the data,
but simply stores regularly some model parameters in a buffer. On the ﬂy acceleration on CPU is
achievable, since one step of RNA is roughly equivalent to squaring a matrix of size d× K, to form a
K × K matrix and solve the corresponding system. K is typically 10 in the experiments that follow.

3 APPLICATIONS TO CNNS

We now describe the performance of our method on classical CNN training problems.

2

Workshop track - ICLR 2018

30

25

20

15

10

5

0

75

70

65

60

55

50

45

40

35

30

Cifar10: % Top-1 Error (Validation Set)

Imagenet: % Top-1 Error (Validation Set)

VGG-19
VGG-19 + RNA
Resnet-18
Resnet-18 + RNA
Densenet-121
Densenet-121 + RNA

AlexNet
AlexNet + RNA
Resnet-18
Resnet-18 + RNA

50

100

150

200

250

300

350

1

30

60

90

Epoch

Epoch

Figure 1: Comparison of Top-1 error between vanilla and extrapolated network.

3.1 CLASSIFICATION PIPELINES

Because the RNA algorithm is generic, it can be easily applied to many different existing CNN train-
ing codes. We used our method with various CNNs on CIFAR10 and ImageNet; the ﬁrst dataset con-
sists of 50k RGB images of size 32 × 32 whereas the latter is more challenging with 1.2M images of
size 224 × 224. Data augmentation via random translation is applied. In both cases, we trained our
CNN via SGD with momentum 0.9 and a weight decay of 10−5, until convergence. The initial learn-
ing rate is 0.1 (0.01 for VGG and AlexNet), and is decreased by 10 at epoch 150, 250 and 30, 60, 90
respectively for CIFAR and ImageNet. For ImageNet, we used AlexNet (Krizhevsky et al., 2012)
and ResNet (He et al., 2016) because they are standard architectures in computer vision. For the
CIFAR dataset, we used the standard VGG, ResNet and DenseNet (Huang et al., 2017). AlexNet
is trained with drop-out (Srivastava et al., 2014) on its fully connected layers, whereas the others
CNNs are trained with batch-normalization (Ioffe & Szegedy, 2015).

In these experiments, each θk corresponds to the parameters resulting from one pass on the data. We
apply successively, off-line, at each epoch j the RNA on {θj−K+1, ...θj} and report the accuracy
obtained by the extrapolated CNN on the validation set. Here, we ﬁx K = 10 and λ = 10−8.

3.2 NUMERICAL RESULTS

Figure 1 reports performance on the validation set of the vanilla and extrapolated CNN via RNA,
at each epoch. Observe that RNA accuracy convergence is smoother than on the original CNNs
which shows an effective variance reduction. In addition, we observe the impact of acceleration: the
accelerated networks quickly present good generalization performance, even competitive with the
best one. Note that several iterations after a learning rate drop are necessary to obtain acceleration
because this corresponds to a brutal change in the optimization. Furthermore, selecting the hyper-
parameter λ can be tricky: for example, a larger λ removes the outlier validation performance at
epoch 40 of Figure 1, for ResNet-18 on ImageNet. Here, we have deliberately chosen to use generic
parameters to make the comparison as fair as possible, but more sophisticated adaptive strategies
has been discussed by Scieur et al. (2017).

Tables 3.2 reports the lowest validation error of the vanilla architectures compared to their extrap-
olated counterpart. Off-line optimization by RNA has only slightly improved ﬁnal accuracy, but
these improvements have been obtained after the training procedure, in an ofﬂine fashion, without
extra-learning.

Network
VGG
Resnet18
Densenet 121

Vanilla
+RNA
6.18% 5.86%
4.71% 4.64%
4.50% 4.42%

Network Vanilla
AlexNet
Resnet18

+RNA
43.72% 43.72%
30.11% 29.64%

Table 1: Lowest Top-1 error on CIFAR10 (Left) and ImageNet (Right)

3

Workshop track - ICLR 2018

ACKNOWLEDGMENTS

We acknowledge support from the European Unions Seventh Framework Programme (FP7-
PEOPLE-2013-ITN) under grant agreement n.607290 SpaRTaN and from the European Research
Council (grant SEQUOIA 724063). Alexandre d’Aspremont was partially supported by the data
science joint research initiative with the fonds AXA pour la recherche and Kamet Ventures. Edouard
Oyallon was partially supported by a postdoctoral grant from DPEI of Inria (AAR 2017POD057)
for the collaboration with CWI.

REFERENCES

Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in neural information
processing systems, pp. 1646–1654, 2014.

Osman G¨uler. New proximal point algorithms for convex minimization. SIAM Journal on Opti-

mization, 2(4):649–664, 1992.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
770–778, 2016.

Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, volume 1, pp. 3, 2017.

Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by

reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.

Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in neural information processing systems, pp. 315–323, 2013.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097–1105,
2012.

Hongzhou Lin, Julien Mairal, and Zaid Harchaoui. A universal catalyst for ﬁrst-order optimization.

In Advances in Neural Information Processing Systems, pp. 3384–3392, 2015.

Damien Scieur, Alexandre d’Aspremont, and Francis Bach. Regularized nonlinear acceleration. In

Advances In Neural Information Processing Systems, pp. 712–720, 2016.

Damien Scieur, Francis Bach, and Alexandre d’Aspremont. Nonlinear acceleration of stochastic

algorithms. In Advances in Neural Information Processing Systems, pp. 3985–3994, 2017.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine Learning
Research, 15(1):1929–1958, 2014.

Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
In Advances in Neural Information

value of adaptive gradient methods in machine learning.
Processing Systems, pp. 4151–4161, 2017.

4

8
1
0
2
 
n
u
J
 
1
 
 
]

C
O
.
h
t
a
m

[
 
 
1
v
0
7
3
0
0
.
6
0
8
1
:
v
i
X
r
a

Workshop track - ICLR 2018

NONLINEAR ACCELERATION OF CNNS

Damien Scieur∗, Edouard Oyallon†, Alexandre d’Aspremont∗ and Francis Bach∗
∗DI, Ecole Normale Sup´erieure, UMR CNRS 8548, INRIA, PSL Research University.
†CVN, CentraleSup´elec, Universit´e Paris-Saclay; Galen team, INRIA Saclay

ABSTRACT

Regularized Nonlinear Acceleration (RNA) can improve the rate of convergence
of many optimization schemes such as gradient descent, SAGA or SVRG, estimat-
ing the optimum using a nonlinear average of past iterates. Until now, its analysis
was limited to convex problems, but empirical observations show that RNA may
be extended to a broader setting. Here, we investigate the beneﬁts of nonlinear
acceleration when applied to the training of neural networks, in particular for the
task of image recognition on the CIFAR10 and ImageNet data sets. In our ex-
periments, with minimal modiﬁcations to existing frameworks, RNA speeds up
convergence and improves testing error on standard CNNs.

1

INTRODUCTION

Successful deep Convolutional Neural Networks (CNNs) for large-scale classiﬁcation are typically
optimized through a variant of the stochastic gradient descent (SGD) algorithm (Krizhevsky et al.,
2012). Reﬁning this optimization scheme is a complicated task and requires a signiﬁcant amount of
engineering whose mathematical foundations are not well understood (Wilson et al., 2017). Here,
we propose to wrap an adhoc acceleration technique known as Regularized Nonlinear Acceleration
algorithm (RNA) (Scieur et al., 2016), around existing CNN training frameworks. RNA is generic
as it does not depend on the optimization algorithm, but simply requires several successive iter-
ates of gradient based methods, which involves a minimal adaptation in many frameworks. This
meta-algorithm has been applied successfully to gradient descent in the smooth and strongly convex
cases, with convergence and rate guarantees recently derived in (Scieur et al., 2016). Recent works
(Scieur et al., 2017) further show that it improves standard stochastic optimization schemes such as
SAGA or SVRG (Defazio et al., 2014; Johnson & Zhang, 2013), which indicates it may also be a
strong candidate as an accelerated method in stochastic non convex cases.

RNA is an ideal meta-learning algorithm for deep CNNs, because contrary to many acceleration
methods Lin et al. (2015); G¨uler (1992), optimization can be performed off-line and does not in-
volve any potentially expensive extra-learning process. This means one can focus on acceleration a
posteriori. RNA related numerical computations are not expensive, and form a simple linear system
from a well-chosen linear combination of several optimization steps. This system is usually very
small relative to the number of parameters, so the cost of acceleration grows linearly with respect to
this number.

Here, we study applications applications of RNA to several recent architectures, like ResNet
(He et al., 2016), applied to classical challenging datasets, like CIFAR10 or ImageNet. Our con-
tributions are are twofold: ﬁrst we demonstrate that it is often possible to achieve an accuracy
similar to the ﬁnal epoch in half the time; second, we show that RNA slightly improves the test clas-
siﬁcation performance, at no additional numerical cost. We provide an implementation that can be
incorporated using only few lines of code around many standard Python deep learning frameworks,
like PyTorch1.

1Code can be found here:

https://github.com/windows7lover/RegularizedNonlinearAcceleration

1

Workshop track - ICLR 2018

2 ACCELERATING WITH REGULARIZED NONLINEAR ACCELERATION

This section brieﬂy describes the RNA procedure and we refer the reader to Scieur et al. (2016)
for more extensive explanations and theoretical guarantees. For the sake of simplicity, we consider
an iterate sequence {θk}0≤k≤K of K + 1 elements of Rd produced from the successive steps of
an iterative optimization algorithm. For example, each θk could correspond to the parameters of a
neural network at epoch i, trained via a gradient descent algorithm, i.e.

θk+1 = θk − η∇f (θk),
with η the step size (or learning rate) of the algorithm. Local minimization of f is naturally achieved
by θ∗ where ∇f (θ∗) = 0. RNA aims to linearly combine the parameters θk into an estimate ˆθ

ˆθ =

X
k≤K

ckθk s.t.

ck = 1,

X
k≤K

(1)

(2)

(3)

so that ∇f (ˆθ) becomes smaller. In other terms, RNA output ˆθ which solves approximately

∇f (cid:16)Pk≤K ckθk(cid:17) (cid:13)
minc (cid:13)
(cid:13)
(cid:13)

2

subject to

Pk≤K ck = 1.

In the next subsection, we describe the algorithm and intuitively explain the acceleration mechanism
when using the optimization method (1), because this restrictive setting makes the analysis simpler.

2.1 REGULARIZED NONLINEAR ACCELERATION ALGORITHM

In practice, solving (2) is a difﬁcult task. Instead, we will assume that the function f is approximately
quadratic in the neighbourhood of {θk}k≤K. This approximation is common in optimization for the
design of (quasi-)second order methods, such as the Newton’s method or BFGS. Thus, ∇f can be
considered almost as an afﬁne function, which means:

−∇f (cid:16)Pk≤K ckθk(cid:17) ≈

Pk≤K ck∇f (θk) .

From a ﬁnite difference scheme, one can easily recover {∇f (θk)}k from the iterates in (1), because
for any k, we have −η∇f (θk) = (θk+1 − θk). As linearized iterates of a ﬂow tends to be aligned,
minimizing the ℓ2-norm of (3) requires incorporating some regularization to avoid ill-conditioning

minc kRck2 + λkck2

subject to

Pk≤K ck = 1,

where R = [θ1 − θ0, . . . , θK − θK−1]. This exactly corresponds to the combination of steps 2
and 3 of Algorithm 1. Similar ideas hold is the stochastic case (Scieur et al., 2017), under limited
assumption on the signal to noise ratio.

Algorithm 1 Regularized Nonlinear Acceleration (RNA), (and computational complexity).
Input: Sequence of vectors {θ0, θ1, . . . , θK} ∈ Rd , regularization parameter λ > 0.
1: Compute R = [θ1 − θ0, . . . , θK − θK−1]
2: Solve (RT R + λI)z = 1.
3: Normalize c = z/(
Output: ˆθ =

Pk≤K zk).

O(K)
O(K 2d + K 3)
O(K)
O(Kd)

Pk≤K ckθk.

2.2 PRACTICAL USAGE

We produced a software package based on PyTorch that includes in minimal modiﬁcations of exist-
ing standard procedures. As claimed, the RNA procedure does not require any access to the data,
but simply stores regularly some model parameters in a buffer. On the ﬂy acceleration on CPU is
achievable, since one step of RNA is roughly equivalent to squaring a matrix of size d× K, to form a
K × K matrix and solve the corresponding system. K is typically 10 in the experiments that follow.

3 APPLICATIONS TO CNNS

We now describe the performance of our method on classical CNN training problems.

2

Workshop track - ICLR 2018

30

25

20

15

10

5

0

75

70

65

60

55

50

45

40

35

30

Cifar10: % Top-1 Error (Validation Set)

Imagenet: % Top-1 Error (Validation Set)

VGG-19
VGG-19 + RNA
Resnet-18
Resnet-18 + RNA
Densenet-121
Densenet-121 + RNA

AlexNet
AlexNet + RNA
Resnet-18
Resnet-18 + RNA

50

100

150

200

250

300

350

1

30

60

90

Epoch

Epoch

Figure 1: Comparison of Top-1 error between vanilla and extrapolated network.

3.1 CLASSIFICATION PIPELINES

Because the RNA algorithm is generic, it can be easily applied to many different existing CNN train-
ing codes. We used our method with various CNNs on CIFAR10 and ImageNet; the ﬁrst dataset con-
sists of 50k RGB images of size 32 × 32 whereas the latter is more challenging with 1.2M images of
size 224 × 224. Data augmentation via random translation is applied. In both cases, we trained our
CNN via SGD with momentum 0.9 and a weight decay of 10−5, until convergence. The initial learn-
ing rate is 0.1 (0.01 for VGG and AlexNet), and is decreased by 10 at epoch 150, 250 and 30, 60, 90
respectively for CIFAR and ImageNet. For ImageNet, we used AlexNet (Krizhevsky et al., 2012)
and ResNet (He et al., 2016) because they are standard architectures in computer vision. For the
CIFAR dataset, we used the standard VGG, ResNet and DenseNet (Huang et al., 2017). AlexNet
is trained with drop-out (Srivastava et al., 2014) on its fully connected layers, whereas the others
CNNs are trained with batch-normalization (Ioffe & Szegedy, 2015).

In these experiments, each θk corresponds to the parameters resulting from one pass on the data. We
apply successively, off-line, at each epoch j the RNA on {θj−K+1, ...θj} and report the accuracy
obtained by the extrapolated CNN on the validation set. Here, we ﬁx K = 10 and λ = 10−8.

3.2 NUMERICAL RESULTS

Figure 1 reports performance on the validation set of the vanilla and extrapolated CNN via RNA,
at each epoch. Observe that RNA accuracy convergence is smoother than on the original CNNs
which shows an effective variance reduction. In addition, we observe the impact of acceleration: the
accelerated networks quickly present good generalization performance, even competitive with the
best one. Note that several iterations after a learning rate drop are necessary to obtain acceleration
because this corresponds to a brutal change in the optimization. Furthermore, selecting the hyper-
parameter λ can be tricky: for example, a larger λ removes the outlier validation performance at
epoch 40 of Figure 1, for ResNet-18 on ImageNet. Here, we have deliberately chosen to use generic
parameters to make the comparison as fair as possible, but more sophisticated adaptive strategies
has been discussed by Scieur et al. (2017).

Tables 3.2 reports the lowest validation error of the vanilla architectures compared to their extrap-
olated counterpart. Off-line optimization by RNA has only slightly improved ﬁnal accuracy, but
these improvements have been obtained after the training procedure, in an ofﬂine fashion, without
extra-learning.

Network
VGG
Resnet18
Densenet 121

Vanilla
+RNA
6.18% 5.86%
4.71% 4.64%
4.50% 4.42%

Network Vanilla
AlexNet
Resnet18

+RNA
43.72% 43.72%
30.11% 29.64%

Table 1: Lowest Top-1 error on CIFAR10 (Left) and ImageNet (Right)

3

Workshop track - ICLR 2018

ACKNOWLEDGMENTS

We acknowledge support from the European Unions Seventh Framework Programme (FP7-
PEOPLE-2013-ITN) under grant agreement n.607290 SpaRTaN and from the European Research
Council (grant SEQUOIA 724063). Alexandre d’Aspremont was partially supported by the data
science joint research initiative with the fonds AXA pour la recherche and Kamet Ventures. Edouard
Oyallon was partially supported by a postdoctoral grant from DPEI of Inria (AAR 2017POD057)
for the collaboration with CWI.

REFERENCES

Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in neural information
processing systems, pp. 1646–1654, 2014.

Osman G¨uler. New proximal point algorithms for convex minimization. SIAM Journal on Opti-

mization, 2(4):649–664, 1992.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
770–778, 2016.

Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, volume 1, pp. 3, 2017.

Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by

reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.

Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in neural information processing systems, pp. 315–323, 2013.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097–1105,
2012.

Hongzhou Lin, Julien Mairal, and Zaid Harchaoui. A universal catalyst for ﬁrst-order optimization.

In Advances in Neural Information Processing Systems, pp. 3384–3392, 2015.

Damien Scieur, Alexandre d’Aspremont, and Francis Bach. Regularized nonlinear acceleration. In

Advances In Neural Information Processing Systems, pp. 712–720, 2016.

Damien Scieur, Francis Bach, and Alexandre d’Aspremont. Nonlinear acceleration of stochastic

algorithms. In Advances in Neural Information Processing Systems, pp. 3985–3994, 2017.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine Learning
Research, 15(1):1929–1958, 2014.

Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
In Advances in Neural Information

value of adaptive gradient methods in machine learning.
Processing Systems, pp. 4151–4161, 2017.

4

