9
1
0
2
 
r
a

M
 
7
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
3
8
6
2
1
.
0
1
8
1
:
v
i
X
r
a

Pseudo-Bayesian Learning with Kernel Fourier Transform as Prior

Ga¨el Letarte1
gael.letarte.1@ulaval.ca

Emilie Morvant2
emilie.morvant@univ-st-etienne.fr
1 D´epartement d’informatique et de g´enie logiciel, Universit´e Laval, Qu´ebec, Canada
2 Univ Lyon, UJM-Saint-Etienne, CNRS, Institut d’Optique Graduate School,
Laboratoire Hubert Curien UMR 5516, Saint-Etienne, France
3 ´Equipe-projet Modal, Inria Lille - Nord Europe, Villeneuve d’Ascq, France

Pascal Germain3
pascal.germain@inria.fr

Abstract

We revisit Rahimi and Recht (2007)’s ker-
nel random Fourier features (RFF) method
through the lens of the PAC-Bayesian theory.
While the primary goal of RFF is to approx-
imate a kernel, we look at the Fourier trans-
form as a prior distribution over trigonomet-
ric hypotheses. It naturally suggests learning
a posterior on these hypotheses. We derive
generalization bounds that are optimized by
learning a pseudo-posterior obtained from a
closed-form expression. Based on this study,
we consider two learning strategies: The ﬁrst
one ﬁnds a compact landmarks-based repre-
sentation of the data where each landmark
is given by a distribution-tailored similar-
ity measure, while the second one provides
a PAC-Bayesian justiﬁcation to the kernel
alignment method of Sinha and Duchi (2016).

1 INTRODUCTION

Kernel methods (Shawe-Taylor and Cristianini, 2004),
such as support vector machines (Boser et al., 1992;
Vapnik, 1998), map data in a high dimension space in
which a linear predictor can solve the learning prob-
lem at hand. The mapping space is not directly com-
puted and the linear predictor is represented implicitly
thanks to a kernel function. This is the powerful kernel
trick: the kernel function computes the scalar product
between two data points in this high dimension space.
However, kernel methods notoriously suﬀer from two
drawbacks. On the ﬁrst hand, computing all the scalar

Proceedings of the 22nd International Conference on Ar-
tiﬁcial Intelligence and Statistics (AISTATS) 2019, Naha,
Okinawa, Japan. PMLR: Volume 89. Copyright 2019 by
the author(s).

products for all the learning samples is costly: O(n2)
for many kernel-based methods, where n is the num-
ber of training data point. On the other hand, one
has to select a kernel function adapted to the learning
problem for the algorithm to succeed.

The ﬁrst of these drawbacks has motivated the de-
velopment of approximation methods making kernel
methods more scalable, such as Nystr¨om approxima-
tion (Williams and Seeger, 2001; Drineas and Ma-
honey, 2005) that constructs a low-rank approxima-
tion of the Gram matrix1 and is data dependent, or
random Fourier features (RFF) (Rahimi and Recht,
2007) that approximates the kernel with random fea-
tures based on the Fourier transform and is not data
dependent (a comparison between the two approaches
have been conducted by Yang et al., 2012).
In this
paper, we revisit the latter technique.

We start from the observation that a predictor based
on kernel Fourier features can be interpreted as a
weighted combination of those features according to a
data independent distribution deﬁned by the Fourier
transform. We introduce an original viewpoint, where
this distribution is interpreted as a prior distribution
over a space of weak hypotheses—each hypothesis be-
ing a simple trigonometric function obtained by the
Fourier decomposition. This suggests that one can
improve the approximation by adapting this distribu-
tion in regards to data points: we aim at learning a
posterior distribution. By this means, our study pro-
poses strategies to learn a representation to the data.
While this representation is not as ﬂexible and power-
ful than the ones that can be learned by deep neural
networks (Goodfellow et al., 2016), we think that it is
worthwhile to study this strategy to eventually solve
the second drawback of kernel methods that currently
heavily rely on the kernel choice. This in mind, while
the majority of work related to random Fourier fea-

1The Gram matrix is the n × n matrix constituted by

all the kernel values computed on the learning samples.

Pseudo-Bayesian Learning with Kernel Fourier Transform as Prior

tures focus on the study and improvement of the ker-
nel approximation, we propose here a reinterpretation
in the light of the PAC-Bayesian theory (McAllester,
1999; Catoni, 2007). We derive generalization bounds
that can be straightforwardly optimized by learning a
pseudo-posterior thanks to a closed-form expression.

The rest of the paper is organized as follows. Sec-
tion 2 recalls the RFF setting. Section 3 expresses the
Fourier transform as a prior leading (i) to a ﬁrst PAC-
Bayesian analysis and a landmarks-based algorithm
in Section 4, (ii) to another PAC-Bayesian analysis
in Section 5 allowing to justify the kernel alignment
method of Sinha and Duchi (2016) and to propose a
greedy kernel learning method. Then Section 6 pro-
vides experiments to show the usefulness of our work.

2 RANDOM FOURIER FEATURES

Problem setting. Consider a classiﬁcation problem
where we want to learn a predictor f : Rd → Y ,
from a d-dimensional space to a discrete output space
(e.g., Y = {0, 1, . . . , |Y |−1}). The learning algorithm
is given a training set S = {(xi, yi)}n
i=1 ∼ Dn of n
i.i.d. samples, where D denotes the data generating
distribution over Rd × Y . We consider a positive-
semideﬁnite (PSD) kernel k : Rd × Rd → [−1, 1]. Ker-
nel machines learn predictors of the form

f (x) =

αik(xi, x) ,

(1)

n
(cid:88)

i=1

by optimizing the values of vector α ∈ Rn.

Fourier features. When n is large, running a ker-
nel machine algorithm (like SVM or kernel ridge re-
gression) is expensive in memory and running time.
To circumvent this problem, Rahimi and Recht (2007)
introduced the random Fourier features as a way to
approximate the value of a shift-invariant kernel, i.e.,
relying on the value of δ = x−x(cid:48) ∈ Rd, which we write

k(δ) = k(x − x(cid:48)) = k(x, x(cid:48))

interchangeably. Let the distribution p(ω) be the
Fourier transform of the shift-invariant kernel k,

p(ω) =

k(δ) e−i ω·δd δ .

(2)

(cid:90)

1
(2π)d

Rd

Now, by writing k as the inverse of the Fourier trans-
form p, and using trigonometric identities, we obtain:

k(x − x(cid:48)) =

(cid:90)

Rd

= E
ω∼p

= E
ω∼p

(cid:104)

p(ω)ei ω·(x−x(cid:48))d ω = E
ei ω·(x−x(cid:48))
ω∼p
cos (cid:0)ω · (x−x(cid:48))(cid:1)+i sin (cid:0)ω · (x−x(cid:48))(cid:1)(cid:105)
cos (cid:0)ω · (x − x(cid:48))(cid:1) .

(3)

Rahimi and Recht (2007) suggest expressing the above
cos(cid:0)ω · (x−x(cid:48))(cid:1) as a product of two features. One way
to achieve this is to map every input example into
zω(x) = (cid:0) cos(ω · x), sin(ω · x)(cid:1) .
The random variable zω(x) · zω(x(cid:48)), with ω drawn
from p, is an unbiased estimate of k(x − x(cid:48)). Indeed,
we recover from Equation (3) and Equation (4):

(4)

zω(x) · zω(x(cid:48))

E
ω∼p

(cid:104)

(cid:105)
cos(ω · x) cos(ω · x(cid:48)) + sin(ω · x) sin(ω · x(cid:48))
cos (cid:0)ω · (x − x(cid:48))(cid:1) .

= E
ω∼p

= E
ω∼p

To reduce the variance in the estimation of k(x−x(cid:48)),
the idea is
from p:
to sample D points
ω1, ω2, . . . , ωD. Then, each training sample xi ∈ Rd
is mapped to a new feature vector in R2D :

i.i.d.

φ(xi) =

cos(ω1 · xi) , . . . , cos(ωD · xi) ,

(cid:16)

1
√
D

sin(ω1 · xi) , . . . , sin(ωD · xi)

(5)
(cid:17)

.

Thus, we have k(x − x(cid:48)) ≈ φ(x) · φ(x(cid:48)) when D is
“large enough”. This provides a decomposition of the
PSD kernel k that diﬀers from the classical one (as dis-
cussed in Bach, 2017). By learning a linear predictor
on the transformed training set S (cid:55)→ {(φ(xi), yi)}n
i=1
through an algorithm like a linear SVM, we recover
a predictor equivalent to the one learned by a ker-
nelized algorithm. That is, we learn a weight vector
w = (w1, . . . , w2D) ∈ R2D and we predict the label of a
sample x ∈ Rd by computing, in place of Equation (1),

f (x) =

wj φj(x) .

(6)

2D
(cid:88)

j=1

3 THE FOURIER TRANSFORM AS

A PRIOR DISTRIBUTION

As described in the previous section, the random
Fourier features trick has been introduced to reduce
the running time of kernel learning algorithms. Con-
sequently, most of the subsequent work study and/or
improve the properties of the kernel approximation
(e.g., Yu et al., 2016; Rudi and Rosasco, 2017; Bach,
2017; Choromanski et al., 2018) with some notable ex-
ceptions, as the kernel learning algorithms of Yang
et al. (2015), Sinha and Duchi (2016), and Oliva et al.
(2016), that we discuss and relate to our approach in
Section 5.

We aim at reinterpreting the Fourier transform—i.e.,
the distribution p of Equation (2)—as a prior distri-
bution over the feature space. It can be seen as an al-
ternative representation of the prior knowledge that is

Ga¨el Letarte, Emilie Morvant, Pascal Germain

encoded in the choice of a speciﬁc kernel function, that
we denote kp from now on. In accordance with Equa-
tion (3), each feature obtained from a vector ω ∈ Rd
can be seen as a hypothesis

hω(δ) := cos(ω · δ) .

Henceforth, the kernel is interpreted as a predictor per-
forming a p-weighed aggregation of weak hypotheses.
This alternative interpretation of distribution p as a
prior over hypotheses naturally suggests to learn a pos-
terior distribution over the same hypotheses. That is,
we seek a distribution q giving rise to a new kernel

kq(δ) := E
ω∼q

hω(δ) .

In order to assess the quality of the kernel kq, we de-
ﬁne a loss function based on the consideration that
its output should be high when two samples share the
same label, and low otherwise. Hence, we evaluate
the kernel on two samples (x, y) ∼ D and (x(cid:48), y(cid:48)) ∼ D
through the linear loss

(cid:96)(cid:0)kq(δ), λ(cid:1) :=

1 − λ kq(δ)
2

,

(7)

where δ = x−x(cid:48) denotes a pairwise distance and λ
denotes the pairwise similarity measure:

λ = λ(y, y(cid:48)) :=

(cid:26) 1
−1

if y = y(cid:48),
otherwise.

Furthermore, we deﬁne the kernel alignment general-
ization loss L∆(kq) on a “pairwise” probability distri-
bution ∆, deﬁned over Rd×[−1, 1] as

L∆(kq) := E

(δ,λ)∼∆

(cid:96)(cid:0)kq(δ), λ(cid:1) .

(8)

Note that any data generating distribution D over
input-output spaces Rd × Y automatically gives rise
to a “pairwise” distribution ∆D. By a slight abuse
of notation, we write LD(kq) the corresponding gen-
eralization loss, and the associated kernel alignment
empirical loss is deﬁned as

(cid:98)LS(kq) :=

1
n(n − 1)

n
(cid:88)

i,j=1,i(cid:54)=j

(cid:96)(cid:0)kq(δij), λij

(cid:1) ,

(9)

where for a pair of examples {(xi, yi), (xj, yj)} ∈ S2
we have δij := (xi − xj) and λij := λ(yi, yj).

Starting from this reinterpretation of the Fourier
transform, we provide in the rest of the paper two
PAC-Bayesian analyses. The ﬁrst one (Section 4)
is obtained by combining n PAC-Bayesian bounds:
instead of considering all the possible pairs of data
points, we ﬁx one point and we study the generaliza-
tion ability for all the pairs involving it. The second
analysis (Section 5) is based on the fact that the loss
can be expressed as a second-order U-statistics.

4 PAC-BAYESIAN ANALYSIS AND

LANDMARKS

Due to the linearity of the loss function (cid:96), we can
rewrite the loss of kq as the q-average loss of every
hypothesis. Indeed, Equation (8) becomes

LD(kq) =

E
(δ,λ)∼∆D

E
ω∼q

hω(δ), λ

(cid:16)
(cid:96)

(cid:17)

= E
ω∼q

E
(δ,λ)∼∆D

(cid:96)(hω(δ), λ) = E
ω∼q

LD(hω) .

The above q-expectation of losses LD(hω) turns out to
be the quantity bounded by most PAC-Bayesian gen-
eralization theorems (sometimes referred as the Gibbs
risk in the literature), excepted that such results usu-
ally apply to the loss over samples instead of distances.
Hence, we use PAC-Bayesian bounds to obtain gener-
alization guarantees on LD(kq) from its empirical es-
timate of Equation (9), that we can rewrite as

(cid:98)LS(kq) =

1
n2−n

(cid:16)

n
(cid:88)

(cid:96)
i,j=1;i(cid:54)=j

E
ω∼q

hω(δ), λij

= E
ω∼q

(cid:98)LS(hω) .

(cid:17)

However the classical PAC-Bayesian theorems cannot
be applied directly to bound LD(kq), as the empir-
ical loss (cid:98)LS(kq) would require to be computed from
i.i.d. observations of ∆D. Instead, the empirical loss
involves dependent samples, as it is computed from
n2−n pairs formed by n elements from D.

4.1 First Order KL-Bound

A straightforward approach to apply classical PAC-
Bayesian results is to bound separately the loss asso-
ciated with each training sample. That is, for each
(xi, yi) ∈ S, we deﬁne

Li

D(hω) := E

(x,y)∼D

(cid:16)
(cid:96)

hω(xi − x), λ(yi, y)

, (10)

(cid:17)

and (cid:98)Li

S(hω) :=

1
n−1

n
(cid:88)

(cid:16)
(cid:96)
j=1,j(cid:54)=i

hω(xi − xj), λ(yi, yj)

.

(cid:17)

Thus, the next theorem gives a generalization guar-
antee on Li
D(kq) relying namely on the empirical es-
timate (cid:98)Li
S(kq) and the Kullback-Leibler divergence
KL(q(cid:107)p) = Eω∼q ln q(ω)
p(ω) between the prior p and the
learned posterior q. Note that the statement of Theo-
rem 1 is obtained straightforwardly from Alquier et al.
(2016, Theorem 4.1 and Lemma 1), but can be recov-
ered easily from Lever et al. (2013).

Theorem 1. For t > 0, i ∈ {1, . . . , n}, and a prior
distribution p over Rd, with probability 1−ε over the
choice of S ∼ Dn, we have for all q on Rd :

Li

D(kq) ≤ (cid:98)Li

S(kq) +

KL(q(cid:107)p) +

(cid:18)

1
t

t2
2(n−1)

+ ln

(cid:19)

.

1
ε

Pseudo-Bayesian Learning with Kernel Fourier Transform as Prior

By the union bound, and using the fact that LD(kq) =
E(xi,yi)∼D Li
D(kq), we prove the following corollary in
the supplementary material.

representation of the input space, mapping the data-
points into compact feature vectors, from which we
can learn a simple predictor.

Corollary 2. For t > 0 and a prior distribution p over
Rd, with probability 1−ε over the choice of S ∼ Dn,
we have for all q on Rd :

LD(kq) ≤ (cid:98)LS(kq)+

KL(q(cid:107)p) +

(cid:18)

2
t

t2
2(n−1)

+ ln

(cid:19)

.

n+1
ε

for KL-bounds. Since

Pseudo-Posterior
the
above result is valid for any distribution q, one can
compute the bound for any learned posterior distri-
bution. Note that the bound promotes the mini-
mization of a trade-oﬀ—parameterized by a constant
t—between the empirical
loss (cid:98)LS(kq) and the KL-
divergence between the prior p and the posterior q:

(cid:98)LS(kq) +

KL(q(cid:107)p) .

2
t

It is well-known that for ﬁxed t, p and S, the minimum
bound value is obtained with the pseudo-Bayesian pos-
terior q∗, such that for ω ∈ Rd,

q∗(ω) =

p(ω) exp

−τ (cid:98)LS(hω)

(11)

(cid:16)

(cid:17)

,

1
Z

where τ := 1
2 t and Z is a normalization constant.2
Note also Corollary 2’s bound converges to the gener-
(cid:16)(cid:113) ln n
n

alization loss LD(kq) at rate O
√

for the param-

(cid:17)

eter choice t =

n ln n.

Due to the continuity of the feature space, the pseudo-
posterior of Equation (11) is hard to compute. To es-
timate it, one may make use of Monte Carlo (e.g.,
Dalalyan and Tsybakov, 2012) or variational Bayes
methods (e.g., Alquier et al., 2016). In this work, we
explore a simpler method: we work solely from a dis-
crete probability space.

4.2 Landmarks-Based Learning

We now propose to leverage on the fact that Theo-
rem 1 bounds the kernel function for the distances to
a single data point, instead of learning a kernel glob-
ally for every data point as in Corollary 2. We thus
aim at learning a collection of kernels (which we can
also interpret as similarity functions) for a subset of
the training points. We call landmarks these training
points. The aim of this approach is to learn a new

2This trade-oﬀ is the same one involved in some other
PAC-Bayesian bounds for i.i.d. data (e.g., Catoni, 2007).
As discussed in Zhang (2006); Gr¨unwald (2012); Germain
et al. (2016), there is a similarity between the minimization
of such PAC-Bayes bounds and the Bayes update rule.

m}D

Concretely, along with the learning sample S of n ex-
amples i.i.d. from D, we consider a landmarks sample
L = {(xl, yl)}nL
l=1 of nL points i.i.d. from D, and a
prior Fourier transform distribution p. For each land-
mark (xl, yl) ∈ L, let sample D points from p, denoted
m=1 ∼ pD. Then, consider a uniform dis-
ΩL = {ωl
tribution P on the discrete hypothesis set ΩL, such
that P (ωl
m · δ). We aim
at learning a set of kernels {(cid:98)kQl }nL
l=1, where each (cid:98)kQl
is obtained from a distinct xl ∈ L with a ﬁxed pa-
rameter β > 0, by computing the pseudo-posterior
distribution Ql given by

m(δ) := cos(ωl

D and hl

m) = 1

Ql

m =

1
Zl

(cid:16)

√

exp

− β

n (cid:98)Ll

S(hl

m)

(cid:17)

,

(12)

√

for m=1, . . . , D ; Zl being the normalization constant.
Note that Equation (12) gives the minimum of The-
n. That is, β = 1 corresponds
orem 1 with t = β
to the regime where the bound converges. Moreover,
similarly to Corollary 2, generalization guarantees are
obtained simultaneously for the nL computed distri-
butions thanks to the union bound and Theorem 1.
Thus, with probability 1−ε, for all {Ql}nL

l=1:

Ll

D((cid:98)kQl ) ≤ (cid:98)Ll

S((cid:98)kQl )+

(cid:18)
KL(Ql(cid:107)P )+

t2
2(n−1)

+ ln

(cid:19)

,

nL
ε

1
t

where KL(Ql(cid:107)P ) = ln D + (cid:80)D

j=1 Ql

j ln Ql
j.

Once all pseudo-posterior are computed thanks to
Equation (12), our landmarks-based approach is to
map samples x ∈ Rd to nL similarity features:
(cid:17)

(cid:16)

ψ(x) :=

(cid:98)kQ1(x1−x), . . . , (cid:98)kQnL (xnL −x)

,

(13)

and to learn a linear predictor on the transformed
training set. Note that, this mapping is not a kernel
map anymore and is somehow similar to the mapping
proposed by Balcan et al. (2008b,a); Zantedeschi et al.
(2018) for a similarity function that is more general
than a kernel but ﬁxed for each landmark.

5 LEARNING KERNEL

(REVISITED)

In this section, we present PAC-Bayesian theorems
that directly bound the kernel alignment generaliza-
tion loss LD(kq) on a “pairwise” probability distri-
bution ∆D—as deﬁned by Equation (8)—even if the
empirical loss (cid:98)LD(kq) is computed on dependent sam-
ples. These bounds suggest a kernel alignment (or
kernel learning) strategy similar to the one of Sinha

Ga¨el Letarte, Emilie Morvant, Pascal Germain

and Duchi (2016). We stress that our guarantees hold
solely for the kernel alignment loss, but not for the
predictor trained with this kernel. Hence, our pro-
posed algorithm learns a kernel independently of the
prediction method to be used downstream. This is in
contrast with the one-step frameworks of Yang et al.
(2015) and Oliva et al. (2016), which learn a mixture
of random kernel features in a fully Bayesian way; they
rely on a data-generating model, whereas our approach
assumes only that the observations are i.i.d.

5.1 Second Order KL-bound

n2−n

that
The following result
is based on the fact
(cid:80)n
(cid:98)LS(hω) := 1
i(cid:54)=j (cid:96)(hω(δij), λij) is an unbiased
second-order estimator of E(δ,λ)∼∆D (cid:96)(hω(δ), λ), al-
lowing us to build on the PAC-Bayesian analysis for
U-statistics of Lever et al. (2013, Theorem 7). Indeed,
the next theorem gives a generalization guarantee on
the kernel alignment loss LD(kq).
Theorem 3 (Lever et al. 2013). For t > 0 and a prior
distribution p over Rd, with probability 1−ε over the
choice of S ∼ Dn, we have for all q on Rd :

LD(kq) ≤ (cid:98)LS(kq) +

KL(q(cid:107)p) +

+ ln

(cid:18)

1
t

t2
2n

(cid:19)

.

1
(cid:15)

Except for some constant terms, the above Theorem 3
is similar to Corollary 2. Indeed, both are minimized
by the same pseudo-posterior q∗ (Equation 11, with
τ := t for Theorem 3). Interestingly, we get rid of the
ln(n + 1) term of Corollary 2, making in Theorem 3’s
bound to converge at rate O( 1√

n.

√

n ) when t =

5.2 Second Order Bounds for f -Divergences

In the following, we build on a recent result of Alquier
and Guedj (2018) to express a new family of PAC-
Bayesian bounds for our dependent samples, where the
KL term is replaced by other f -divergences.

Given a convex function f such that f (1)=0, a
f -divergence is given by Df (q(cid:107)p) := Eω∼p f (cid:0) q(ω)
(cid:1).
The following theorem applies to f -divergences such
that f (x) = xµ − 1.
Theorem 4. For µ > 1 and a prior distribution p over
Rd, with probability 1−ε over the choice of S ∼ Dn,
we have for all q on Rd :

p(ω)

LD(kq) ≤ (cid:98)LS(kq)
(cid:17)µ−1(cid:16)




(cid:16) 1
√
2

n

+

Dµ(q(cid:107)p) + 1

(cid:17) 1

µ (cid:0) 1
ε

(cid:1)1− 1

µ

if 1 < µ ≤ 2,

(cid:1)1− 1

µ(cid:16)



(cid:0) 1
4n

Dµ(q(cid:107)p) + 1

(cid:17) 1

µ (cid:0) 1
ε

(cid:1)1− 1

µ

if µ > 2,

where Dµ(q(cid:107)p) := E
ω∼p

(cid:19)µ

(cid:18) q(ω)
p(ω)

− 1 .

Proof. Let Mµ := E
ω∼p

E
S(cid:48)∼Dn

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)LD(hω) − (cid:98)LS(cid:48)(hω)
(cid:12)

µ

.

We start from Alquier and Guedj (2018, Theorem 1):

LD(kq) ≤ (cid:98)LS(kq)+

(cid:19)1− 1

µ (cid:16)

Dµ(q(cid:107)p)+1

. (14)

(cid:17) 1

µ

Let us show Mµ ≤

for 1 < µ ≤ 2 :

Mµ = E
ω∼p

E
S(cid:48)∼Dn

LD(hω) − (cid:98)LS(cid:48)(hω)

(cid:17)2(cid:21) µ

2

(cid:17)2(cid:21) µ

2

≤ E
ω∼p

E
S(cid:48)∼Dn

LD(hω) − (cid:98)LS(cid:48)(hω)

(15)

(cid:18)Mµ
ε
(cid:16) 1
√
2

n

(cid:17)µ

(cid:20)(cid:16)

(cid:16)

(cid:20)

(cid:104)

= E
ω∼p

≤ E
ω∼p

Var
S(cid:48)∼Dn
(cid:21) µ
(cid:20) 1
4n

2

(cid:105) µ

2

(LS(cid:48)(hω))

=

(cid:20) 1
4n

(cid:21) µ

2

.

(16)

Line (15) is obtained by Jensen’s inequality (since
0 < µ
2 ≤ 1), and the inequality of Line (16) is proven
by Lemma 6 of the supplementary material. Note that
the latter is based on the Efron-Stein inequality and
Boucheron et al. (2013, Corollary 3.2).

The ﬁrst case of Theorem 4 statement (1 < µ ≤ 2) is
obtained by inserting Line (16) in Equation (14). The
second case (µ > 2) is obtained by upper-bounding
Mµ by M2 = 1

4n , as |LD(hω) − (cid:98)LS(cid:48)(hω)| ≤ 1 .

As a particular case, with µ = 2, we obtain from Theo-
rem 4 a bound that relies on the chi-square divergence
χ2(q(cid:107)p) = Eω∼p
− 1 .
Corollary 5. Given a prior distribution p over Rd,
with probability 1−ε over the choice of S ∼ Dn, we
have for all q on Rd :

(cid:0) q(ω)
p(ω)

(cid:1)2

LD(kq) ≤ (cid:98)LS(kq) +

(cid:114)

χ2(q(cid:107)p) + 1
4 n ε

.

It is noteworthy that the above result looks alike other
PAC-Bayesian bounds based on the chi-square diver-
gence in the i.i.d. setting, as the one of Honorio and
Jaakkola (2014, Lemma 7), B´egin et al. (2016, Corol-
lary 10) or Alquier and Guedj (2018, Corollary 1).
Interestingly, the latter has been introduced to han-
dle unbounded (possibly heavy-tailed) losses, and one
could also extend our Corollary 5 to this setting.

5.3 PAC-Bayesian Interpretation of Kernel

Alignment Optimization

Sinha and Duchi (2016) propose a kernel learning al-
gorithm that weights random kernel features. To do
so, their algorithm solves a kernel alignment problem.
As explained below, this method is coherent with the
PAC-Bayesian theory exposed by our current work.

Pseudo-Bayesian Learning with Kernel Fourier Transform as Prior

Kernel alignment algorithm. Let us consider a
Fourier transform distribution p, from which N points
are sampled, denoted Ω = {ωm}N
m=1 ∼ pN . Then,
consider a uniform distribution P on the discrete hy-
pothesis set Ω, such that P (ωm) = 1
N and hm(δ) :=
cos(ωm · δ). Given a dataset S, and constant parame-
ters µ > 1, ρ > 0, the optimization algorithm proposed
by Sinha and Duchi solves the following problem.

maximize
Q∈RN
+

n
(cid:88)

n
(cid:88)

N
(cid:88)

i=1

j=1

m=1

λij

Qmhm(δij) ,

(17)

such that

Qm=1 and Dµ(Q(cid:107)P ) ≤ ρ .

(18)

N
(cid:88)

m=1

The iterative procedure proposed by Sinha and Duchi
ﬁnds an (cid:15)-suboptimal solution to the above problem
in O(N log( 1
(cid:15) )) steps. The solution provides a learned
kernel (cid:98)kQ(δ) := 1
N

m=1 Qmhm(δ).

(cid:80)N

Sinha and Duchi propose to use the above alignment
method to reduce the number of features needed com-
pared to the classical RFF procedure (as described in
Section 2). Albeit this method is a kernel learning one,
empirical experiments show that with a large num-
ber of random features, the classical RFF procedure
achieves as good prediction accuracy. However, one
can draw (with replacement) D < N features from Ω
according to Q. For a relatively small D, learning a
linear predictor on the random feature vector (such
as the one presented by Equation 5) obtained from Q
achieves better results than the classical RFF method
on the same number D of random features.

PAC-Bayesian interpretation. The optimization
problem of Equations (17–18) deals with the same
trade-oﬀ as the one promoted by Theorem 4.
In-
deed, maximizing Equation (17) amounts to minimiz-
ing (cid:98)LS(kq), and the constraint of Equation (18) con-
trols the f -divergence Dµ(Q(cid:107)P ), which is the same
complexity measure involved in Theorem 4. Further-
more, the empirical experiments performed by Sinha
and Duchi (2016) focus on the χ2-divergence (case
µ=2), which corresponds to tackling the trade-oﬀ ex-
pressed by Corollary 5.

5.4 Greedy Kernel Learning

The method proposed by Sinha and Duchi (2016) can
easily be adapted to minimize the bound of Theorem 3
instead of the bound of Theorem 4. We describe this
kernel learning procedure below.

Given a Fourier transform prior distribution p, let sam-
m=1 ∼ pN . Let P (ωm) = 1
ple N points Ω = {ωm}N
N
and hm(δ) := cos(ωm · δ). Given a dataset S, and

constant parameters β > 0, compute the following
pseudo-posterior for m = 1, . . . , N :

Qm =

exp

− β

n (cid:98)LS(hm)

(19)

(cid:16)

√

(cid:17)

.

1
Z

Then, we sample with replacement D < N features
from Ω according to the pseudo-posterior Q. The sam-
pled features are used to map every x ∈ Rd of the
training set into a new vector φ(x) ∈ R2D according
to Equation (5). The latter transformed dataset is
then given as input to a linear learning procedure.

In summary, this learning method is strongly inspired
by the one described in Section 5.3, but the poste-
rior computation phase is faster, as we beneﬁt from a
closed-form expression (Equation 19). Once (cid:98)LS(hm)
is computed for all hm,3 we can vary the parameter β
and get a new posterior in O(N ) steps.

6 EXPERIMENTS

All experiments use a Gaussian (a.k.a. RBF) kernel
2σ2 (cid:107)x − x(cid:48)(cid:107)2(cid:1) , for
of variance σ2: kσ(x, x(cid:48)) = exp (cid:0) − 1
which the Fourier transform is given by

pσ(ω) =

(cid:17) d

2

(cid:16) σ2
2π

e− 1

2 σ2(cid:107)ω(cid:107)2

= N (ω; 0, 1

σ2 I) .

(20)

Apart from the toy experiment of Figure 1, the ex-
periments on real data are conducted by splitting the
available data into a training set, a validation set and
a test set. The kernel parameter σ is chosen among
{10−7, 10−6, . . . , 102} by running an RBF SVM on the
training set and keeping the parameter having the best
accuracy score on the validation set. That is, this σ
deﬁnes the prior distribution given by Equation (20)
for all our pseudo-Bayesian methods. Unless otherwise
speciﬁed, all the other parameters are selected using
the validation set. More details about the experimen-
tal procedure are given in the supplementary material.

6.1 Landmarks-Based Learning

Toy experiment. To get some insight from the
landmarks-based procedure of Section 4.2, we gen-
erate a 2D dataset Stoy,
illustrated by Fig-
ure 1. We randomly select ﬁve training points
L={x1, x2, x3, x4, x5} ⊂ Stoy, and compare two pro-
cedures, described below.

RBF-Landmarks: Learn a linear SVM on the empirical
kernel map given by the ﬁve RBF kernels centered
on L. That is, each x ∈ Stoy is mapped such that

(cid:16)

x (cid:55)→

kσ(x1, x), kσ(x2, x), kσ(x3, x), kσ(x4, x), kσ(x5, x)

.

(cid:17)

3We show in supplementary material (section A.2) that

each (cid:98)LS(hm) can be computed in O(n) steps.

Ga¨el Letarte, Emilie Morvant, Pascal Germain

Figure 1: First row shows selected RBF-Landmarks kernel outputs, while second row shows the corresponding
learned similarity measures on random Fourier features (PB-Landmarks). The rightmost column displays the
classiﬁcation learned by a linear SVM over the mapped dataset.

Dataset

ads
adult
breast
farm
mnist17
mnist49
mnist56

landmarks-based

SVM RBF

PB PBβ=1 PBD=64

4.90

3.50

4.88

3.05 10.98

5.12
19.70 19.60 17.99 17.99
3.50
15.73 14.19
0.32
2.09
1.55

6.99
11.58 17.47
0.74
0.34
1.16
2.26
0.55 0.97

0.42
1.80
1.06

5.00
17.99
2.80
15.73
0.32
2.50
1.03

Table 1: Test error of the landmarks-based approach.

ing points as landmarks (from 1% to 25%), and we
compare the classiﬁcation error of a linear SVM on
the mapping obtained by the original RBF functions
(as in the RBF-Landmarks method above), with the
mapping obtained by learning the landmarks posterior
distributions (PB-Landmark method). We also com-
pare the case where the landmarks are selected at ran-
dom among the training data (curves postﬁxed “-R”),
to another scenario where we use the centroids ob-
tained with a k-Means clustering as landmarks (curves
postﬁxed “-C”). Note that, the latter case is not rig-
orously backed by our PAC-Bayesian theorems, since
the choice of landmarks is now dependent of the whole
observed training set. The results show that the clas-
siﬁcation error of both cases are similar, but the clus-
tering strategy leads to a more stable behavior, proba-
bly since the landmarks are more representative of the
original space. Moreover, the pseudo-Bayesian method
improves the results on almost all datasets.

Table 1 compares the error rate of an SVM (trained
along with the full Gram matrix and a properly se-
lected σ on the validation set) with four landmarks-
based approaches: (RBF) the landmarks are RBF ker-
nel of parameter σ; (PB) the PB-Landmarks approach
where the number of features per landmarks D and

Figure 2: Behavior of the landmarks-based approach
according to the percentage of training points selected
as landmarks on the dataset “ads”.

PB-Landmarks: Generate 20 random samples accord-
ing to the Fourier transform of Equation (20). For ev-
ery landmark of L, learn a similarity measure thanks
to Equation (12) (with β = 1), minimizing the PAC-
Bayesian bound. We thus obtain ﬁve posterior distri-
butions Q1, Q2, Q3, Q4, Q5, and learn a linear SVM on
the mapped training set obtained by Equation (13).

Hence, the RBF-Landmarks method corresponds to
the prior, from which we learn a posterior by land-
marks by the PB-Landmarks procedure. Right-most
plots of Figure 1 show that the PB-Landmarks set-
ting successfully ﬁnds a representation from which the
linear SVM can predict well.

Experiments on real data. We conduct similar
experiments as the above one on seven real binary
classiﬁcation datasets. Figure 2 studies the behav-
ior of the approaches according to the number of se-
lected landmarks. We select a percentage of the train-

Pseudo-Bayesian Learning with Kernel Fourier Transform as Prior

(a) ads

(b) farm

(c) mnist49

Figure 3: Train and test error of the kernel learning approaches according to the number of random features D.

the β parameter are selected using the validation set;
(PBβ=1) the PB-Landmarks approach where β=1 is
ﬁxed and D is selected by validation; and (PBD=64)
the PB-Landmarks approach where D=64 is ﬁxed and
β is selected by validation. For all landmarks-based
approaches, we select the landmarks by clustering, and
use 10% of the training set size as the number of land-
marks; we want to study the methods in the regime
where it provides relatively compact representations.
We observe that learning the posterior improves the
RBF-Landmarks (except on “mnist56”) and that the
validation of both β and D parameters are not manda-
tory to obtain satisfactory results. The SVM RBF
is better than all landmarks-based approaches on 4
datasets out of 7, but requires a far less compact rep-
resentation of the data as it uses the full Gram matrix.

6.2 Greedy Kernel Learning

Figure 3 presents a study of the kernel learning method
detailed in Section 5.4, inspired from the one of Sinha
and Duchi (2016). We ﬁrst generate N =20000 random
features according to pσ as given by Equation (4), and
we learn a posterior using two strategies: (OKRFF)
the original optimized kernel of Sinha and Duchi given
by Equations (17-18), where ρ is selected on the val-
idation set; and (PBRFF) the pseudo-posterior given
by Equation (19) where β is selected on the valida-
tion set. For both obtained posteriors, we subsample
an increasing number of features D ∈ [1, 5000] to cre-
ate the mapping given by Equation (5), on which we
learn a linear SVM. We also compare to (RFF) the
standard random Fourier features as described in Sec-
tion 2, with D randomly selected features according to
the prior pσ.

We see that our PBRFF approach behaves similarly as
OKRFF, with a slight advantage for the latter. How-
ever, we recall that computing the posterior of former
method is faster. Both kernel learning methods have
better accuracy than the classical RFF algorithm for
a small number of random features, and similar ones
for a large number of random features.

7 CONCLUSION & PERSPECTIVES

We elaborated an original viewpoint of the random
Fourier features, proposed by Rahimi and Recht (2007)
to approximate a kernel. By looking at the Fourier
transform as a prior distribution over trigonometric
functions, we present two kinds of generalization the-
orems that bound a kernel alignment loss. Based
on classical ﬁrst-order PAC-Bayesian results, we de-
rived a landmarks-based strategy that learns a com-
pact representation of the data. Then, we proposed
two second-order generalization bounds. The ﬁrst one
is based on the U-statistic theorem of Lever et al.
(2013). The second one is a new PAC-Bayesian
theorem for f -divergences (replacing the usual KL-
divergence term). We show that the latter bound pro-
vides a theoretical justiﬁcation to the kernel alignment
method of Sinha and Duchi (2016), and we also empir-
ically evaluate a similar but simpler algorithm where
the alignment distribution is obtained by the PAC-
Bayesian pseudo-posterior closed-form expression.

Our current guarantees hold solely for the kernel align-
ment loss, and not for the predictor trained with this
kernel. An important research direction is to extend
the guarantees to the ﬁnal predictor, which could in
turn be the bedrock of a new one-step learning pro-
cedure (in the vein of Yang et al., 2015; Oliva et al.,
2016). Other research directions include the study of
the RKHS associated with the learned kernel, and the
extension of our study to wavelet transforms (Mallat,
2008). Furthermore, considering the Fourier transform
of a kernel as a (pseudo-)Bayesian prior might lead to
other original contributions. Among them, we foresee
new perspectives on representation and metric learn-
ing, namely for unsupervised learning.

Acknowledgments. P. Germain wants to thank
Francis Bach for insightful preliminary discussions.
This work was supported in part by the French Project
APRIORI ANR-18-CE23-0015 and in part by NSERC.
This research was enabled in part by support provided
by Compute Canada (www.computecanada.ca).

Ga¨el Letarte, Emilie Morvant, Pascal Germain

References

Pierre Alquier and Benjamin Guedj. Simpler PAC-
Bayesian bounds for hostile data. Machine Learning,
107(5), 2018.

Pierre Alquier, James Ridgway, and Nicolas Chopin.
On the properties of variational approximations of
gibbs posteriors. Journal of Machine Learning Re-
search, 17, 2016.

Francis R. Bach. On the equivalence between ker-
nel quadrature rules and random feature expansions.
Journal of Machine Learning Research, 18, 2017.

Maria-Florina Balcan, Avrim Blum, and Nathan Sre-
bro. Improved guarantees for learning via similarity
functions. In COLT, 2008a.

Maria-Florina Balcan, Avrim Blum, and Nathan Sre-
bro. A theory of learning with similarity functions.
Machine Learning, 72(1-2):89–112, 2008b.

Luc B´egin, Pascal Germain, Fran¸cois Laviolette, and
Jean-Francis Roy. PAC-Bayesian bounds based on
the R´enyi divergence. In AISTATS, 2016.

Bernhard E. Boser, Isabelle Guyon, and Vladimir Vap-
nik. A training algorithm for optimal margin classi-
ﬁers. In COLT, 1992.

St´ephane Boucheron, G´abor Lugosi, and Pascal Mas-
sart. Concentration inequalities : a nonasymptotic
theory of independence. Oxford university press,
2013. ISBN 978-0-19-953525-5.

Olivier Catoni. PAC-Bayesian supervised classiﬁca-
tion: the thermodynamics of statistical learning, vol-
ume 56. Inst. of Mathematical Statistic, 2007.

Krzysztof Choromanski, Mark Rowland, Tam´as
Sarl´os, Vikas Sindhwani, Richard E. Turner, and
Adrian Weller. The geometry of random features.
In AISTATS, 2018.

Arnak S. Dalalyan and Alexandre B. Tsybakov. Sparse
regression learning by aggregation and langevin
monte-carlo. J. Comput. Syst. Sci., 78(5), 2012.

Petros Drineas and Michael W Mahoney. On the
nystr¨om method for approximating a gram matrix
for improved kernel-based learning. Journal of Ma-
chine Learning Research, 6(Dec), 2005.

Pascal Germain, Francis R. Bach, Alexandre Lacoste,
and Simon Lacoste-Julien. PAC-Bayesian theory
meets Bayesian inference. In NIPS, 2016.

Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
Deep Learning. MIT Press, 2016. http://www.
deeplearningbook.org.

Peter Gr¨unwald. The safe Bayesian - learning the
learning rate via the mixability gap. In ALT, 2012.

Jean Honorio and Tommi S. Jaakkola. Tight bounds
for the expected risk of linear classiﬁers and pac-
bayes ﬁnite-sample guarantees. In AISTATS, 2014.

Guy Lever, Fran¸cois Laviolette, and John Shawe-
through
Tighter PAC-Bayes bounds
Taylor.
distribution-dependent priors. Theor. Comput. Sci.,
473, 2013.

St´ephane Mallat. A Wavelet Tour of Signal Process-

ing, 3rd Edition. Academic Press, 2008.

David McAllester. Some PAC-Bayesian theorems. Ma-

chine Learning, 37(3), 1999.

Junier B Oliva, Avinava Dubey, Andrew G Wilson,
Barnab´as P´oczos, Jeﬀ Schneider, and Eric P Xing.
Bayesian nonparametric kernel-learning.
In AIS-
TATS, 2016.

Ali Rahimi and Benjamin Recht. Random features for

large-scale kernel machines. In NIPS, 2007.

Alessandro Rudi and Lorenzo Rosasco. Generaliza-
tion properties of learning with random features. In
NIPS, 2017.

John Shawe-Taylor and Nello Cristianini. Kernel
Methods for Pattern Analysis. Cambridge Univer-
sity Press, 2004.

Aman Sinha and John C. Duchi. Learning kernels with

random features. In NIPS, 2016.

Vladimir Vapnik. Statistical learning theory. Wiley,

1998.

Christopher K. I. Williams and Matthias Seeger. Using
the Nystr¨om method to speed up kernel machines.
In NIPS. 2001.

Tianbao Yang, Yu-feng Li, Mehrdad Mahdavi, Rong
Jin, and Zhi-Hua Zhou. Nystr¨om method vs ran-
dom fourier features: A theoretical and empirical
comparison. In NIPS. 2012.

Zichao Yang, Andrew Gordon Wilson, Alexander J.
Smola, and Le Song. A la carte - learning fast ker-
nels. In AISTATS, 2015.

Felix

X.

Yu,

Ananda

Suresh,
Krzysztof Marcin Choromanski, Daniel N.
Holtmann-Rice, and Sanjiv Kumar.
Orthogo-
nal random features. In NIPS, 2016.

Theertha

Valentina Zantedeschi, R´emi Emonet, and Marc Seb-
ban. Fast and provably eﬀective multi-view classiﬁ-
cation with landmark-based svm. In ECML-PKDD,
2018.

Tong Zhang.

Information-theoretic upper and lower
bounds for statistical estimation. IEEE Trans. In-
formation Theory, 52(4), 2006.

A Supplementary Material

A.1 Mathematical Results

Corollary 2. For t > 0 and a prior distribution p over Rd, with probability 1−ε over the choice of S ∼ Dn,
we have for all q on Rd :

LD(kq) ≤ (cid:98)LS(kq) +

KL(q(cid:107)p) +

(cid:18)

2
t

t2
2(n − 1)

+ ln

(cid:19)

.

n + 1
ε

Proof. We want to bound

LD(kq) = E

(x,y)∼D

E
(x(cid:48),y(cid:48))∼D
L(cid:48)
D(kq) ,

=

E
(x(cid:48),y(cid:48))∼D

(cid:16)

E
ω∼q

(cid:96)

hω(x − x(cid:48)), λ(y, y(cid:48))

(cid:17)

D(kq) is the alignment loss of the kernel kq centered on (x(cid:48), y(cid:48)) ∼ D (see Equation (10)).
where L(cid:48)
Let t > 0 and p a distribution on Rd. By applying the PAC-Bayesian theorem, with ε0 ∈ (0, 1), we have

∀q on Rd : LD(kq) ≤

Li

D(kq) +

KL(q(cid:107)p) +

+ ln

≥ 1 − (cid:15)0 .

1
n

n
(cid:88)

i=1

(cid:34)

1
t

t2
2n

Moreover, we have that for each i ∈ {1, . . . , n}, with a εi ∈ (0, 1), we have

∀q on Rd : Li

D(kq) ≤ (cid:98)Li

S(kq) +

KL(q(cid:107)p) +

+ ln

≥ 1 − (cid:15)i .

(cid:34)

1
t

t2
2(n − 1)

(cid:32)

Pr
S∼Dn

(cid:32)

Pr
S∼Dn

(cid:35)(cid:33)

(cid:35)(cid:33)

1
ε0

1
εi

By combining above probabilistic results with ε0 = ε1 = · · · = εn = ε
least 1 − ε ,

n+1 , we obtain that, with probability at

LD(kq) =

L(cid:48)

D(kq)

E
(x(cid:48),y(cid:48))∼D
(cid:34)
(cid:98)Li

n
(cid:88)

1
n

i=1

≤

S(kq) +

KL(q(cid:107)p) +

(cid:20)

1
t

t2
2(n − 1)

+ ln

n + 1
ε

(cid:21)(cid:35)

(cid:20)

1
t

+

KL(q(cid:107)p) +

+ ln

(cid:21)

n + 1
ε

= (cid:98)LS(kq) +

KL(q(cid:107)p) +

+

KL(q(cid:107)p) +

+ ln

(cid:20)

(cid:20)

1
t
2
t

t2
2(n − 1)
t2
2(n − 1)

+ ln

+ ln

(cid:21)

(cid:21)

n + 1
ε
n + 1
ε

.

(cid:20)

1
t

≤ (cid:98)LS(kq) +

KL(q(cid:107)p) +

t2
2n

(cid:21)

n + 1
ε

t2
2n

Lemma 6. For any data-generating distribution D:

Var
S(cid:48)∼Dn

(LS(cid:48)(hω)) ≤

1
4n

.

Proof. Given S(cid:48) = {(xi, yi)}n

i=1 ∼ Dn, we denote

Fω

(cid:0)S(cid:48)) := Fω

(cid:0)(x1, y1), . . . , (xn, yn)(cid:1) := LS(cid:48)(hω) =

1
n(n − 1)

n
(cid:88)

(cid:16)
(cid:96)

i(cid:54)=j

hω(xi − xj), λ(yi, yj)

.

(cid:17)

The function Fω above has the bounded diﬀerences property. That is, for each i ∈ {1, . . . , n} :

sup
S(cid:48),x∗∈Rd,y∗∈Y

(cid:12)
(cid:12)Fω

(cid:0)(x1, y1), . . . , (xn, yn)(cid:1) − Fω

(cid:0)(x1, y1), . . . , (xi−1, yi−1), (x∗, y∗), (xi+1, yi+1), . . . , (xn, yn)(cid:1)(cid:12)

(cid:12) ≤

1
n

,

Thus, we apply the Efron-Stein inequality (following Boucheron et al., 2013, Corollary 3.2) to obtain

Ga¨el Letarte, Emilie Morvant, Pascal Germain

Var
S(cid:48)∼Dn

(Fω(S(cid:48))) ≤

(cid:19)2

1
4

n
(cid:88)

i=1

(cid:18) 1
n

=

1
4n

.

A.2 Kernel Alignment Loss Computation

The kernel learning algorithms presented in Section 5 require to compute the empirical kernel alignment loss for
each hypothesis hω, given by

(cid:98)LS(hω) =

(cid:96)(hω(δij), λij) .

(21)

1
n(n−1)

n
(cid:88)

i(cid:54)=j

A naive implementation of Equation (21) would need O(n2) steps. Propositions 7 and 8 below show how to
rewrite Equation (21) in a form that needs O(n) steps. Proposition 7 is dedicated to the binary classiﬁcation,
and is equivalent to the computation method proposed by Sinha and Duchi (2016). By Proposition 8, we extend
the result to the multi-classiﬁcation case.
Proposition 7 (Binary classiﬁcation). When S = (xi, yi)n

i=1 ∈ (Rd × {−1, 1})n, we have

(cid:98)LS(hω) =

n
2(n−1)

−

1
2n(n−1)

(cid:32) n
(cid:88)





i=1

yi cos(ω · xi)

+

yi sin(ω · xi)

(cid:33)2

(cid:32) n
(cid:88)

i=1

(cid:33)2
 .

That is, in the binary classiﬁcation case (y ∈ {−1, 1}), one can compute the empirical alignment loss (cid:98)LS(hω) in
O(n) steps.

Proof. Using the cosine trigonometric identity

n
(cid:88)

i(cid:54)=j

λijhω(xi − xj) =

yiyj cos(ω · (xi−xj)) − n

yiyj (cos(ω · xi) cos(ω · xj) + sin(ω · xi) sin(ω · xj)) − n

yi cos(ω · xi)

+

yi sin(ω · xi)

− n

(cid:33)2

(cid:32) n
(cid:88)

i=1

(cid:33)2

Thus,

(cid:98)LS(hω) =

1
n(n−1)

(cid:96)(hω(δij), λij)

n
(cid:88)

n
(cid:88)

i=1

j=1

n
(cid:88)

n
(cid:88)

j=1

i=1
(cid:32) n
(cid:88)

i=1

=

=

n
(cid:88)

i(cid:54)=j
n
(cid:88)

i(cid:54)=j

1
2

1
2

=

1
n(n−1)

1 − λijhω(δij)
2

=

−

1
2n(n−1)

λijhω(xi−xj)

n
(cid:88)

i(cid:54)=j


(cid:32) n
(cid:88)

i=1


=

−

1
2n(n−1)



=

n
2(n−1)

−

1
2n(n−1)



(cid:32) n
(cid:88)

i=1

yi cos(ω · xi)

+

yi sin(ω · xi)

− n



(cid:33)2

(cid:32) n
(cid:88)

i=1

(cid:33)2

(cid:32) n
(cid:88)

i=1

(cid:33)2



(cid:33)2
 .

yi cos(ω · xi)

+

yi sin(ω · xi)

Pseudo-Bayesian Learning with Kernel Fourier Transform as Prior

Proposition 8 (Multi-class classiﬁcation). When S = (xi, yi)n

i=1 ∈ (Rd × {1, . . . , L})n, we have

(cid:98)LS(hω) =

n
2(n−1)

−

1
2n(n−1)



2

L
(cid:88)

y=1

(c2

y + s2

y) −

(cid:33)2

cy

−

(cid:32) L
(cid:88)

y=1

(cid:32) L
(cid:88)

y=1

(cid:33)2
 ,

sy

cy :=

cos(ω · x)

and

sy :=

sin(ω · x) .

(cid:88)

x∈Sy

(cid:88)

x∈Sy

That is, in the multi-class classiﬁcation case with L classes (y ∈ {1, . . . , L})n), one can compute the empirical
alignment loss (cid:98)LS(hω) in O(n) steps.

with

Proof.

n
(cid:88)

i(cid:54)=j

λijhω(xi − xj) =

λij cos(ω · (xi−xj)) − n

n
(cid:88)

n
(cid:88)

i=1

j=1

n
(cid:88)

n
(cid:88)

i=1
n
(cid:88)

j=1
n
(cid:88)

i=1

j=1

=

(2I[yi = yj] − 1) cos(ω · (xi−xj)) − n

= 2

I[yi = yj] cos(ω · (xi−xj)) −

cos(ω · (xi−xj)) − n

n
(cid:88)

n
(cid:88)

i=1

j=1

Let’s denote Sy := {xi|(xi, y) ∈ S}. We have

n
(cid:88)

n
(cid:88)

i=1

j=1

I[yi = yj] cos(ω · (xi−xj)) =

cos(ω · (x−x(cid:48)))

L
(cid:88)

y=1

L
(cid:88)

y=1

(cid:88)

(cid:88)

x(cid:48)∈Sy

x∈Sy








(cid:88)

x∈Sy

=

cos(ω · x)



+



sin(ω · x)





2



(cid:88)

x∈Sy

2




 ,

and

Thus, we can rewrite

n
(cid:88)

n
(cid:88)

i=1

j=1

cos(ω · (xi−xj)) =

cos(ω · x)



+



sin(ω · x)



.





L
(cid:88)

(cid:88)

y=1

x∈Sy


2




2

L
(cid:88)

(cid:88)

y=1

x∈Sy

n
(cid:88)

i(cid:54)=j

λijhω(xi − xj) = 2

(c2

y + s2

y) −

cy

−

sy

− n .

L
(cid:88)

y=1

(cid:32) L
(cid:88)

y=1

(cid:33)2

(cid:33)2

(cid:32) L
(cid:88)

y=1

Ga¨el Letarte, Emilie Morvant, Pascal Germain

Therefore,

(cid:98)LS(hω) =

1
n(n−1)

(cid:96)(hω(δij), λij)

=

1
n(n−1)

1 − λijhω(δij)
2

n
(cid:88)

i(cid:54)=j
n
(cid:88)

i(cid:54)=j

λijhω(xi−xj)

=

−

1
2n(n−1)

=

−

1
2n(n−1)

1
2

1
2

n
(cid:88)

i(cid:54)=j


2

L
(cid:88)

y=1

=

n
2(n−1)

−

1
2n(n−1)

(c2

y + s2

y) −

(cid:33)2

cy

−

(cid:32) L
(cid:88)

y=1

(cid:32) L
(cid:88)

y=1

(cid:33)2

(cid:32) L
(cid:88)

y=1

(c2

y + s2

y) −

cy

−



2

L
(cid:88)

y=1

(cid:33)2



sy

− n



(cid:33)2
 .

sy

(cid:32) L
(cid:88)

y=1

A.3 Experiments

Implementation details. The code used to run the experiments is available at:

https://github.com/gletarte/pbrff

In Section 6 we use the following datasets:

ads http://archive.ics.uci.edu/ml/datasets/Internet+Advertisements
The ﬁrst 4 features which have missing values are removed.

adult https://archive.ics.uci.edu/ml/datasets/Adult

breast https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic).

farm https://archive.ics.uci.edu/ml/datasets/Farm+Ads

mnist http://yann.lecun.com/exdb/mnist/

As Sinha and Duchi (2016), binary classiﬁcation tasks are compiled with the following digits pairs: 1 vs. 7,
4 vs. 9, and 5 vs. 6.

We split the datasets into training and testing sets with a 75/25 ratio except for adult which has a training/test
split already computed. We then use 20% of the training set for validation. Table 2 presents an overview. We
use the following parameter values range for selection on the validation set:

• C ∈ {10−5, 10−4, . . . , 104}

• σ ∈ {10−7, 10−6, . . . , 102}

• ρ ∈ {10−4N, 10−3N, . . . , 100N }

• β ∈ {10−3, 10−2, . . . , 103}

• D ∈ {8, 16, 32, 64, 128}

Dataset ntrain nvalid

ntest

d

ads
adult
breast
farm
mnist17
mnist49
mnist56

1967
26048
340
2485
9101
8268
7912

492

820
6513 16281
143

1554
108
30
1036 54877
784
3793
784
3446
784
3298

86
622
2276
2068
1979

Table 2: Datasets overview.

Supplementary experiments. Figures 4 and 6 present extra results obtained for the landmarks-based learning
experiments (Subsection 6.1). Figure 5 gives extra results for the greedy kernel learning experiment (Subsec-
tion 6.2).

Pseudo-Bayesian Learning with Kernel Fourier Transform as Prior

Figure 4: Repetition of Figure 1’s experiment, with another toy dataset. First row shows selected RBF-
Landmarks kernel outputs, while second row shows the corresponding learned similarity measures on random
Fourier features (PB-Landmarks). The rightmost column displays the classiﬁcation learned by a linear SVM
over the mapped dataset.

(a) mnist56

(b) mnist17

(c) adult

(d) breast

Figure 5: Train and test error of the kernel learning approaches according to the number of random features D
on the remaining 4 datasets (not reported by Figure 3).

Ga¨el Letarte, Emilie Morvant, Pascal Germain

(a) mnist17

(b) farm

(c) mnist56

(d) adult

(e) mnist49

(f) breast

Figure 6: Behavior of the landmarks-based approach according to the percentage of training points selected as
landmarks on the remaining 6 datasets (not reported by Figure 2).

