Jointly Predicting Predicates and Arguments
in Neural Semantic Role Labeling

Luheng He

Kenton Lee

Omer Levy

Luke Zettlemoyer

Paul G. Allen School of Computer Science & Engineering
University of Washington, Seattle WA
{luheng, kentonl, omerlevy, lsz}@cs.washington.edu

8
1
0
2
 
g
u
A
 
3
1
 
 
]
L
C
.
s
c
[
 
 
2
v
7
8
7
4
0
.
5
0
8
1
:
v
i
X
r
a

Abstract

Recent BIO-tagging-based neural seman-
tic role labeling models are very high per-
forming, but assume gold predicates as
part of the input and cannot incorporate
span-level features. We propose an end-
to-end approach for jointly predicting all
predicates, arguments spans, and the rela-
tions between them. The model makes in-
dependent decisions about what relation-
ship, if any, holds between every possi-
ble word-span pair, and learns contextu-
alized span representations that provide
rich, shared input features for each deci-
sion. Experiments demonstrate that this
approach sets a new state of the art on
PropBank SRL without gold predicates.1

1

Introduction

Semantic role labeling (SRL) captures predicate-
argument relations, such as “who did what to
whom.” Recent high-performing SRL models (He
et al., 2017; Marcheggiani et al., 2017; Tan et al.,
2018) are BIO-taggers, labeling argument spans
for a single predicate at a time (as shown in Fig-
ure 1). They are typically only evaluated with gold
predicates, and must be pipelined with error-prone
predicate identiﬁcation models for deployment.

We propose an end-to-end approach for predict-
ing all the predicates and their argument spans in
one forward pass. Our model builds on a recent
coreference resolution model (Lee et al., 2017),
by making central use of learned, contextualized
span representations. We use these representations
to predict SRL graphs directly over text spans.
Each edge is identiﬁed by independently predict-
ing which role, if any, holds between every possi-
ble pair of text spans, while using aggressive beam

1Code and models: https://github.com/luheng/lsgn

Figure 1: A comparison of our span-graph struc-
ture (top) versus BIO-based SRL (bottom).

pruning for efﬁciency. The ﬁnal graph is simply
the union of predicted SRL roles (edges) and their
associated text spans (nodes).

Our span-graph formulation overcomes a key
limitation of semi-markov and BIO-based mod-
els (Kong et al., 2016; Zhou and Xu, 2015; Yang
and Mitchell, 2017; He et al., 2017; Tan et al.,
2018):
it can model overlapping spans across
different predicates in the same output structure
(see Figure 1). The span representations also
generalize the token-level representations in BIO-
based models, letting the model dynamically de-
cide which spans and roles to include, without
using previously standard syntactic features (Pun-
yakanok et al., 2008; FitzGerald et al., 2015).

To the best of our knowledge, this is the ﬁrst
span-based SRL model that does not assume that
predicates are given.
In this more realistic set-
ting, where the predicate must be predicted, our
model achieves state-of-the-art performance on
It also reinforces the strong perfor-
PropBank.
mance of similar span embedding methods for
coreference (Lee et al., 2017), suggesting that this
style of models could be used for other span-span
relation tasks, such as syntactic parsing (Stern
et al., 2017), relation extraction (Miwa and Bansal,
2016), and QA-SRL (FitzGerald et al., 2018).

2 Model

We consider the space of possible predicates to
be all the tokens in the input sentence, and the
space of arguments to be all continuous spans.Our
model decides what relation exists between each
predicate-argument pair (including no relation).

Formally, given a sequence X = w1, . . . , wn,
we wish to predict a set of labeled predicate-
argument relations Y ⊆ P × A × L, where P =
{w1, . . . , wn} is the set of all tokens (predicates),
A = {(wi, . . . , wj) | 1 ≤ i ≤ j ≤ n} contains
all the spans (arguments), and L is the space of
semantic role labels, including a null label (cid:15) indi-
cating no relation. The ﬁnal SRL output would be
all the non-empty relations {(p, a, l) ∈ Y | l (cid:54)= (cid:15)}.
We then deﬁne a set of random variables, where
each random variable yp,a corresponds to a predi-
cate p ∈ P and an argument a ∈ A, taking value
from the discrete label space L. The random vari-
ables yp,a are conditionally independent of each
other given the input X:

P (Y | X) =

P (yp,a | X)

(1)

(cid:89)

p∈P,a∈A

P (yp,a = l | X) =

exp(φ(p, a, l))

exp(φ(p, a, l(cid:48)))

(cid:80)
l(cid:48)∈L

(2)

Where φ(p, a, l) is a scoring function for a pos-
sible (predicate, argument, label) combination. φ
is decomposed into two unary scores on the pred-
icate and the argument (deﬁned in Section 3), as
well as a label-speciﬁc score for the relation:

φ(p, a, l) = Φa(a) + Φp(p) + Φ(l)

rel (a, p)

(3)

The score for the null label is set to a constant:
φ(p, a, (cid:15)) = 0, similar to logistic regression.

Learning For each input X, we minimize the
negative log likelihood of the gold structure Y ∗:

J (X) = − log P (Y ∗ | X)

(4)

Beam pruning As our model deals with O(n2)
possible argument spans and O(n) possible pred-
icates, it needs to consider O(n3|L|) possible re-
lations, which is computationally impractical. To
overcome this issue, we deﬁne two beams Ba and
Bp for storing the candidate arguments and pred-
icates, respectively. The candidates in each beam
are ranked by their unary score (Φa or Φp). The
sizes of the beams are limited by λan and λpn. El-
ements that fall out of the beam do not participate

in computing the edge factors Φ(l)
rel , reducing the
overall number of relational factors evaluated by
the model to O(n2|L|). We also limit the max-
imum width of spans to a ﬁxed number W (e.g.
W = 30), further reducing the number of com-
puted unary factors to O(n).

3 Neural Architecture

Our model builds contextualized representations
for argument spans a and predicate words p based
on BiLSTM outputs (Figure 2) and uses feed-
forward networks to compute the factor scores in
φ(p, a, l) described in Section 2 (Figure 3).

Word-level contexts The bottom layer con-
sists of pre-trained word embeddings con-
catenated with character-based representations,
for each token wi, we have xi =
i.e.
[WORDEMB(wi); CHARCNN(wi)]. We then con-
textualize each xi using an m-layered bidirec-
tional LSTM with highway connections (Zhang
et al., 2016), which we denote as ¯xi.

Argument and predicate representation We
build contextualized representations for all can-
didate arguments a ∈ A and predicates p ∈
P. The argument representation contains the fol-
lowing: end points from the BiLSTM outputs
(¯xSTART(a), ¯xEND(a)), a soft head word xh(a), and
embedded span width features f (a), similar to Lee
et al. (2017). The predicate representation is sim-
ply the BiLSTM output at the position INDEX(p).

g(a) =[¯xSTART(a); ¯xEND(a); xh(a); f (a)]
g(p) =¯xINDEX(p)

(5)

(6)

The soft head representation xh(a) is an attention
mechanism over word inputs x in the argument
span, where the weights e(a) are computed via a
linear layer over the BiLSTM outputs ¯x.
xh(a) = xSTART(a):END(a)e(s)(cid:124)
e(a) = SOFTMAX(w(cid:124)

e ¯xSTART(a):END(a))

(8)

(7)

xSTART(a):END(a) is a shorthand for stacking a list
of vectors xt, where START(a) ≤ t ≤ END(a).

Scoring The scoring functions Φ are imple-
mented with feed-forward networks based on the
predicate and argument representations g:

Φa(a) =w(cid:124)
Φp(p) =w(cid:124)
rel (a, p) =w(l)(cid:124)
Φ(l)

a MLPa(g(a))
p MLPp(g(p))

r MLPr([g(a); g(p)])

(9)

(10)

(11)

Many tourists

tourists visit Disney

Disney to

to meet

meet their favorite

favorite cartoon characters

Span head (xh)

+

+

+

+

+

+

Span representation (g)

Bidirectional LSTM
(¯xm)

Word & character
representation (x)

Many

tourists

visit

Disney

to

meet

their

favorite

cartoon

characters

Figure 2: Building the argument span representations g(a) from BiLSTM outputs. For clarity, we only
show one BiLSTM layer and a small subset of the arguments.

φ(Many tourists, meet, (cid:15)) = 0

φ(Many tourists,
meet, ARG0)

φ(Many tourists,
meet, ARG1)

Softmax
P (yp,a = l | X)

Combined
score φ(p, a, l)

Label score Φ(l)
rel
Unary scores Φa, Φp

Span
representation (g)

Many tourists

meet

Figure 3: The span-pair classiﬁer takes in predi-
cate and argument representations as inputs, and
computes a softmax over the label space L.

4 Experiments

We experiment on the CoNLL 2005 (Carreras and
M`arquez, 2005) and CoNLL 2012 (OntoNotes 5.0,
(Pradhan et al., 2013)) benchmarks, using two
SRL setups: end-to-end and gold predicates. In
the end-to-end setup, a system takes a tokenized
sentence as input, and predicts all the predicates
and their arguments. Systems are evaluated on the
micro-averaged F1 for correctly predicting (pred-
icate, argument span, label) tuples. For compari-
son with previous systems, we also report results
with gold predicates, in which the complete set of
predicates in the input sentence is given as well.
Other experimental setups and hyperparameteres
are listed in Appendix A.1.

ELMo embeddings To further improve perfor-
mance, we also add ELMo word representations
(Peters et al., 2018) to the BiLSTM input (in the
+ELMo rows). Since the contextualized represen-
tations ELMo provides can be applied to most pre-
vious neural systems, the improvement is orthog-
In Table 1 and 2, we
onal to our contribution.
organize all the results into two categories:
the
comparable single model systems, and the mod-

els augmented with ELMo or ensembling (in the
PoE rows).

End-to-end results As shown in Table 1,2 our
joint model outperforms the previous best pipeline
system (He et al., 2017) by an F1 difference of
anywhere between 1.3 and 6.0 in every setting.
The improvement is larger on the Brown test set,
which is out-of-domain, and the CoNLL 2012 test
set, which contains nominal predicates. On all
datasets, our model is able to predict over 40% of
the sentences completely correctly.

Results with gold predicates To compare with
additional previous systems, we also conduct ex-
periments with gold predicates by constraining
our predicate beam to be gold predicates only.
As shown in Table 2, our model signiﬁcantly
out-performs He et al. (2017), but falls short of
Tan et al. (2018), a very recent attention-based
(Vaswani et al., 2017) BIO-tagging model that was
developed concurrently with our work. By adding
the contextualized ELMo representations, we are
able to out-perform all previous systems, includ-
ing Peters et al. (2018), which applies ELMo to
the SRL model introduced in He et al. (2017).

5 Analysis

Our model’s architecture differs signiﬁcantly from
previous BIO systems in terms of both input and
decision space. To better understand our model’s
strengths and weaknesses, we perform three anal-
yses following Lee et al. (2017) and He et al.
(2017), studying (1) the effectiveness of beam

2For the end-to-end setting on CoNLL 2012, we used a
subset of the train/dev data from previous work due to noise
in the dataset; the dev result is not directly comparable. See
Appendix A.2 for detailed explanation.

CoNLL 05 In-domain (WSJ)

Out-of-domain (Brown)

CoNLL 2012 (OntoNotes)

End-to-End

Dev. F1

P

Ours+ELMo
He et al. (2017)PoE

Ours
He et al. (2017)

85.3
81.5

81.6
80.3

84.8
82.0

81.2
80.2

R

87.2
83.4

83.9
82.3

F1

86.0
82.7

82.5
81.2

P

73.9
69.7

69.7
67.6

R

78.4
70.5

71.9
69.6

F1

Dev. F1

P

76.1
70.1

70.8
68.5

83.0
77.2

79.4
75.5

81.9
80.2

79.4
78.6

R

84.0
76.6

80.1
75.1

F1

82.9
78.4

79.8
76.8

Table 1: End-to-end SRL results for CoNLL 2005 and CoNLL 2012, compared to previous systems.
CoNLL 05 contains two test sets: WSJ (in-domain) and Brown (out-of-domain).

WSJ Brown OntoNotes

Arg. Beam

by ambulance
her mother ... ambulance
her mother
Priscilla
should
transported by ambulance
Priscilla says .... ambulance
ambulance

Φa

2.5
2.2
2.2
1.9
1.8
-0.3
-2.2
-3.2

Pred. Beam

says
transported
ambulance
been

Φp

0.1
0.0
-8.3
-11.3

Ours+ELMo
Peters et al. (2018)+ELMo
Tan et al. (2018)PoE
He et al. (2017)PoE
FitzGerald et al. (2015)PoE

Ours
Tan et al. (2018)
He et al. (2017)
Yang and Mitchell (2017)
Zhou and Xu (2015)

87.4
-
86.1
84.6
80.3

83.9
84.8
83.1
81.9
82.8

80.4
-
74.8
73.6
72.2

73.7
74.1
72.1
72.0
69.4

85.5
84.6
83.9
83.4
80.1

82.1
82.7
81.7
-
81.1

Table 2: Experiment results with gold predicates.

pruning, (2) the ability to capture long-range de-
pendencies, (3) agreement with syntactic spans,
and (4) the ability to predict globally consistent
SRL structures. The analyses are performed on
the development sets without using ELMo embed-
dings. 3

Effectiveness of beam pruning Figure 4 shows
the predicate and argument spans kept in the beam,
sorted with their unary scores. Our model efﬁ-
ciently prunes unlikely argument spans and pred-
icates, signiﬁcantly reduces the number of edges
it needs to consider. Figure 5 shows the recall of
predicate words on the CoNLL 2012 development
set. By retaining λp = 0.4 predicates per word,
we are able to keep over 99.7% argument-bearing
predicates. Compared to having a part-of-speech
tagger (POS:X in Figure 5), our joint beam prun-
ing allowing the model to have a soft trade-off be-
tween efﬁciency and recall.4

Long-distance dependencies Figure 6 shows
the performance breakdown by binned distance
between arguments to the given predicates. Our
model is better at accurately predicting arguments
that are farther away from the predicates, even

3For comparability with prior work, analyses (2)-(4) are

performed on the CoNLL 05 dev set with gold predicates.

4The predicate ID accuracy of our model is not compa-
rable with that reported in He et al. (2017), since our model
does not predict non-argument-bearing predicates.

Figure 4: Top: The candidate arguments and pred-
icates in the argument beam Ba and predicate
beam Bp after pruning, along with their unary
scores. Bottom: Predicted SRL relations with two
identiﬁed predicates and their arguments.

compared to an ensemble model (He et al., 2017)
that has a higher overall F1. This is very likely
due to architectural differences; in a BIO tagger,
predicate information passes through many LSTM
timesteps before reaching a long-distance argu-
ment, whereas our architecture enables direct con-
nections between all predicates-arguments pairs.

Agreement with syntax As mentioned in He
et al. (2017), their BIO-based SRL system has
good agreement with gold syntactic span bound-
aries (94.3%) but falls short of previous syntax-
based systems (Punyakanok et al., 2004). By
directly modeling span information, our model
achieves comparable syntactic agreement (95.0%)
to Punyakanok et al. (2004) without explicitly
modeling syntax.

Global consistency On the other hand, our
model suffers from global consistency issues. For
example, on the CoNLL 2005 test set, our model
has lower complete-predicate accuracy (62.6%)
than the BIO systems (He et al., 2017; Tan et al.,
2018) (64.3%-66.4%). Table 3 shows its viola-

100

90

80

70

60

)

%

(

l
l
a
c
e
R

POS:Verb
POS:Verb+Noun
POS:Verb+Noun+Adj
Ours:Predicate beam

0.1

0.2

0.3

0.4

0.5

0.6

0.7

Spans per word λ

Figure 5: Recall of gold argument-bearing predi-
cates on the CoNLL 2012 development data as we
increase the number of predicates kept per word.
POS:X shows the gold predicate recall from using
certain pos-tags identiﬁed by the NLTK part-of-
speech tagger (Bird, 2006).

%
1
F

85
80
75
70
65
60

Ours
He (PoE)
He
Punyakanok

0

1-2

3-6

7-max

Distance (num. words in between)

Figure 6: F1 by surface distance between pred-
icates and arguments, showing degrading perfor-
mance on long-range arguments.

tions of global structural constraints5 compared to
previous systems. Our model made more con-
straint violations compared to previous systems.
For example, our model predicts duplicate core
arguments6 (shown in the U column in Table 3)
more often than previous work. This is due to the
fact that our model uses independent classiﬁers to
label each predicate-argument pair, making it difﬁ-
cult for them to implicitly track the decisions made
for several arguments with the same predicate.

The Ours+decode row in Table 3 shows SRL
performance after enforcing the U-constraint us-
ing dynamic programming (T¨ackstr¨om et al.,
2015) at decoding time. Constrained decoding at
test time is effective at eliminating all the core-role
inconsistencies (shown in the U-column), but did
not bring signiﬁcant gain on the end result (shown

SRL-Violations

Model/Oracle SRL F1 Syn %

Gold

Ours+decode
Ours
He (PoE)
He
Punyakanok

100.0

82.4
82.3
82.7
81.6
77.4

98.7

95.1
95.0
94.3
94.0
95.3

U

24

0
69
37
48
0

C

0

8
7
3
4
0

R

61

104
105
68
73
0

Table 3: Comparison on the CoNLL 05 devel-
opment set against previous systems in terms
of unlabeled agreement with gold constituency
(Syn%) and each type of SRL-constraints viola-
tions (Unique core roles, Continuation roles and
Reference roles).

in SRL F1), which only evaluates the piece-wise
predicate-argument structures.

6 Conclusion and Future Work

We proposed a new SRL model that is able to
jointly predict all predicates and argument spans,
generalized from a recent coreference system (Lee
et al., 2017). Compared to previous BIO systems,
our new model supports joint predicate identiﬁ-
cation and is able to incorporate span-level fea-
tures. Empirically, the model does better at long-
range dependencies and agreement with syntactic
boundaries, but is weaker at global consistency,
due to our strong independence assumption.

In the future, we could incorporate higher-order
inference methods (Lee et al., 2018) to relax this
assumption. It would also be interesting to com-
bine our span-based architecture with the self-
attention layers (Tan et al., 2018; Strubell et al.,
2018) for more effective contextualization.

Acknowledgments

This research was supported in part by the ARO
(W911NF-16-1-0121),
the NSF (IIS-1252835,
IIS-1562364), a gift from Tencent, and an Allen
Distinguished Investigator Award. We thank Eun-
sol Choi, Dipanjan Das, Nicholas Fitzgerald, Ariel
Holtzman, Julian Michael, Noah Smith, Swabha
Swayamdipta, and our anonymous reviewers for
helpful feedback.

5Punyakanok et al. (2008) described a list of global con-
straints for SRL systems, e.g., there can be at most one core
argument of each type for each predicate.

6Arguments with labels ARG0,ARG1,. . . ,ARG5 and AA.

References

In ACL.

Steven Bird. 2006. Nltk: the natural language toolkit.

Xavier Carreras and Llu´ıs M`arquez. 2005.

Introduc-
tion to the conll-2005 shared task: Semantic role la-
beling. In CoNLL.

Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics.

Nicholas FitzGerald, Julian Michael, Luheng He, and
Luke Zettlemoyer. 2018. Large-scale qa-srl parsing.
In ACL.

Nicholas FitzGerald, Oscar T¨ackstr¨om, Kuzman
Ganchev, and Dipanjan Das. 2015. Semantic role
labeling with neural network factors. In EMNLP.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In NIPS.

Luheng He, Kenton Lee, Mike Lewis, and Luke S.
Zettlemoyer. 2017. Deep semantic role labeling:
What works and what’s next. In ACL.

Diederik Kingma and Jimmy Ba. 2015. Adam: A

Method for Stochastic Optimization. In ICLR.

Lingpeng Kong, Chris Dyer, and Noah A Smith. 2016.
Segmental recurrent neural networks. In ICLR.

Kenton Lee, Luheng He, Mike Lewis, and Luke S.
Zettlemoyer. 2017. End-to-end neural coreference
resolution. In EMNLP.

Andrew M Saxe, James L McClelland, and Surya Gan-
guli. 2014. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. arXiv
preprint.

Mitchell Stern, Jacob Andreas, and Dan Klein. 2017. A
minimal span-based neural constituency parser. In
ACL.

Patrick Verga, Daniel Andor,
Emma Strubell,
David Weiss,
and Andrew McCallum. 2018.
Linguistically-Informed Self-Attention for Seman-
tic Role Labeling. arXiv preprint.

Oscar T¨ackstr¨om, Kuzman Ganchev, and Dipanjan
Das. 2015. Efﬁcient inference and structured learn-
ing for semantic role labeling. Transactions of the
Association for Computational Linguistics.

Zhixing Tan, Mingxuan Wang, Jun Xie, Yidong Chen,
and Xiaodong Shi. 2018. Deep semantic role label-
ing with self-attention. In AAAI.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NIPS.

Kenton Lee, Luheng He, and Luke Zettlemoyer. 2018.
Higher-order coreference resolution with coarse-to-
ﬁne inference. In NAACL.

Bishan Yang and Tom M. Mitchell. 2017. A joint
sequential and relational model for frame-semantic
parsing. In EMNLP.

Diego Marcheggiani, Anton Frolov, and Ivan Titov.
2017. A simple and accurate syntax-agnostic neural
model for dependency-based semantic role labeling.
In CoNLL.

Yu Zhang, Guoguo Chen, Dong Yu, Kaisheng Yaco,
Sanjeev Khudanpur, and James Glass. 2016. High-
way long short-term memory rnns for distant speech
recognition. In ICASSP.

Makoto Miwa and Mohit Bansal. 2016. End-to-end re-
lation extraction using lstms on sequences and tree
structures. In ACL.

Vinod Nair and Geoffrey E Hinton. 2010. Rectiﬁed
linear units improve restricted boltzmann machines.
In ICML.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In EMNLP.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In NAACL.

Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Hwee Tou Ng, Anders Bj¨orkelund, Olga Uryupina,
Yuchen Zhang, and Zhi Zhong. 2013. Towards ro-
bust linguistic analysis using ontonotes. In CoNLL.

Vasin Punyakanok, Dan Roth, Wen tau Yih, Dav Zi-
mak, and Yuancheng Tu. 2004. Semantic role la-
beling via generalized inference over classiﬁers. In
CoNLL.

Jie Zhou and Wei Xu. 2015. End-to-end learning of
semantic role labeling using recurrent neural net-
works. In ACL.

A Supplemental Material

A.1 Hyperparameters

Representation sizes The word embeddings are
ﬁxed 300-dimensional GloVe embeddings (Pen-
nington et al., 2014) (context window size of 2
for head word embeddings, and window size of 10
for LSTM inputs), normalized to be unit vectors.
Out-of-vocabulary words are represented by a vec-
tor of zeros. In the character CNN, characters are
represented as learned 8-dimensional embeddings.
The convolutions have window sizes of 3, 4, and 5
characters, each consisting of 50 ﬁlters.

Network sizes We use 3 stacked bidirectional
LSTMs with highway connections and 200 dimen-
sional hidden states. Each MLP consists of two
hidden layers with 150 dimensions and rectiﬁed
linear units (Nair and Hinton, 2010).

CoNLL 2012

OntoNotes5

Train

Dev

Test

Train

Dev

Docs
Sentences
Predicates

2.8
75
189

0.3
9.6
24

0.3
9.5
24

11
116
253

1.5
16
35

Table 4: Data statistics (in number of thousands)
for the CoNLL 2012 split and the train/dev split of
OntoNotes5.

Inference We model spans up to length 30. We
use λa = 0.8 for pruning arguments, λp = 0.4
for pruning predicates. At decoding time, we
use dynamic programming (a simpliﬁed version of
T¨ackstr¨om et al. (2015)) to predict a set of non-
overlapping arguments for each predicate 7.

Training We use Adam (Kingma and Ba, 2015)
with initial learning rate 0.001 and decay rate of
0.1% every 100 steps. The LSTM weights are ini-
tialized with random orthonormal matrices (Saxe
et al., 2014). We apply 0.5 dropout to the word
embeddings and character CNN outputs and 0.2
dropout to all hidden layers and feature embed-
dings. In the LSTMs, we use variational dropout
masks that are shared across timesteps (Gal and
Ghahramani, 2016), with 0.4 dropout rate.

Batching At training time, we randomly shufﬂe
all the documents and then batch at sentence level.
Each batch contains at most 40 sentences and 700
words. All models are trained for at most 320,000
steps with early stopping on the development set,
which takes less than 48 hours on a single Titan X
GPU.

A.2 OntoNotes Data Statistics

Table 4 shows the data statistics on various splits
of OntoNotes. We found that some sentences
in the OntoNotes 5.0 train/dev split have missing
predicates, which is unsuitable for training end-
to-end SRL systems. Therefore, our end-to-end
SRL models are trained on the smaller but cleaner
CoNLL 2012 splits. For experiments with gold
predicates, we use the full OntoNotes 5.0 train/dev
split and the CoNLL 2012 test set, following pre-
vious work.

7This is mainly a constraint enforced by the ofﬁcial

CoNLL evaluation script.

Jointly Predicting Predicates and Arguments
in Neural Semantic Role Labeling

Luheng He

Kenton Lee

Omer Levy

Luke Zettlemoyer

Paul G. Allen School of Computer Science & Engineering
University of Washington, Seattle WA
{luheng, kentonl, omerlevy, lsz}@cs.washington.edu

8
1
0
2
 
g
u
A
 
3
1
 
 
]
L
C
.
s
c
[
 
 
2
v
7
8
7
4
0
.
5
0
8
1
:
v
i
X
r
a

Abstract

Recent BIO-tagging-based neural seman-
tic role labeling models are very high per-
forming, but assume gold predicates as
part of the input and cannot incorporate
span-level features. We propose an end-
to-end approach for jointly predicting all
predicates, arguments spans, and the rela-
tions between them. The model makes in-
dependent decisions about what relation-
ship, if any, holds between every possi-
ble word-span pair, and learns contextu-
alized span representations that provide
rich, shared input features for each deci-
sion. Experiments demonstrate that this
approach sets a new state of the art on
PropBank SRL without gold predicates.1

1

Introduction

Semantic role labeling (SRL) captures predicate-
argument relations, such as “who did what to
whom.” Recent high-performing SRL models (He
et al., 2017; Marcheggiani et al., 2017; Tan et al.,
2018) are BIO-taggers, labeling argument spans
for a single predicate at a time (as shown in Fig-
ure 1). They are typically only evaluated with gold
predicates, and must be pipelined with error-prone
predicate identiﬁcation models for deployment.

We propose an end-to-end approach for predict-
ing all the predicates and their argument spans in
one forward pass. Our model builds on a recent
coreference resolution model (Lee et al., 2017),
by making central use of learned, contextualized
span representations. We use these representations
to predict SRL graphs directly over text spans.
Each edge is identiﬁed by independently predict-
ing which role, if any, holds between every possi-
ble pair of text spans, while using aggressive beam

1Code and models: https://github.com/luheng/lsgn

Figure 1: A comparison of our span-graph struc-
ture (top) versus BIO-based SRL (bottom).

pruning for efﬁciency. The ﬁnal graph is simply
the union of predicted SRL roles (edges) and their
associated text spans (nodes).

Our span-graph formulation overcomes a key
limitation of semi-markov and BIO-based mod-
els (Kong et al., 2016; Zhou and Xu, 2015; Yang
and Mitchell, 2017; He et al., 2017; Tan et al.,
2018):
it can model overlapping spans across
different predicates in the same output structure
(see Figure 1). The span representations also
generalize the token-level representations in BIO-
based models, letting the model dynamically de-
cide which spans and roles to include, without
using previously standard syntactic features (Pun-
yakanok et al., 2008; FitzGerald et al., 2015).

To the best of our knowledge, this is the ﬁrst
span-based SRL model that does not assume that
predicates are given.
In this more realistic set-
ting, where the predicate must be predicted, our
model achieves state-of-the-art performance on
It also reinforces the strong perfor-
PropBank.
mance of similar span embedding methods for
coreference (Lee et al., 2017), suggesting that this
style of models could be used for other span-span
relation tasks, such as syntactic parsing (Stern
et al., 2017), relation extraction (Miwa and Bansal,
2016), and QA-SRL (FitzGerald et al., 2018).

2 Model

We consider the space of possible predicates to
be all the tokens in the input sentence, and the
space of arguments to be all continuous spans.Our
model decides what relation exists between each
predicate-argument pair (including no relation).

Formally, given a sequence X = w1, . . . , wn,
we wish to predict a set of labeled predicate-
argument relations Y ⊆ P × A × L, where P =
{w1, . . . , wn} is the set of all tokens (predicates),
A = {(wi, . . . , wj) | 1 ≤ i ≤ j ≤ n} contains
all the spans (arguments), and L is the space of
semantic role labels, including a null label (cid:15) indi-
cating no relation. The ﬁnal SRL output would be
all the non-empty relations {(p, a, l) ∈ Y | l (cid:54)= (cid:15)}.
We then deﬁne a set of random variables, where
each random variable yp,a corresponds to a predi-
cate p ∈ P and an argument a ∈ A, taking value
from the discrete label space L. The random vari-
ables yp,a are conditionally independent of each
other given the input X:

P (Y | X) =

P (yp,a | X)

(1)

(cid:89)

p∈P,a∈A

P (yp,a = l | X) =

exp(φ(p, a, l))

exp(φ(p, a, l(cid:48)))

(cid:80)
l(cid:48)∈L

(2)

Where φ(p, a, l) is a scoring function for a pos-
sible (predicate, argument, label) combination. φ
is decomposed into two unary scores on the pred-
icate and the argument (deﬁned in Section 3), as
well as a label-speciﬁc score for the relation:

φ(p, a, l) = Φa(a) + Φp(p) + Φ(l)

rel (a, p)

(3)

The score for the null label is set to a constant:
φ(p, a, (cid:15)) = 0, similar to logistic regression.

Learning For each input X, we minimize the
negative log likelihood of the gold structure Y ∗:

J (X) = − log P (Y ∗ | X)

(4)

Beam pruning As our model deals with O(n2)
possible argument spans and O(n) possible pred-
icates, it needs to consider O(n3|L|) possible re-
lations, which is computationally impractical. To
overcome this issue, we deﬁne two beams Ba and
Bp for storing the candidate arguments and pred-
icates, respectively. The candidates in each beam
are ranked by their unary score (Φa or Φp). The
sizes of the beams are limited by λan and λpn. El-
ements that fall out of the beam do not participate

in computing the edge factors Φ(l)
rel , reducing the
overall number of relational factors evaluated by
the model to O(n2|L|). We also limit the max-
imum width of spans to a ﬁxed number W (e.g.
W = 30), further reducing the number of com-
puted unary factors to O(n).

3 Neural Architecture

Our model builds contextualized representations
for argument spans a and predicate words p based
on BiLSTM outputs (Figure 2) and uses feed-
forward networks to compute the factor scores in
φ(p, a, l) described in Section 2 (Figure 3).

Word-level contexts The bottom layer con-
sists of pre-trained word embeddings con-
catenated with character-based representations,
for each token wi, we have xi =
i.e.
[WORDEMB(wi); CHARCNN(wi)]. We then con-
textualize each xi using an m-layered bidirec-
tional LSTM with highway connections (Zhang
et al., 2016), which we denote as ¯xi.

Argument and predicate representation We
build contextualized representations for all can-
didate arguments a ∈ A and predicates p ∈
P. The argument representation contains the fol-
lowing: end points from the BiLSTM outputs
(¯xSTART(a), ¯xEND(a)), a soft head word xh(a), and
embedded span width features f (a), similar to Lee
et al. (2017). The predicate representation is sim-
ply the BiLSTM output at the position INDEX(p).

g(a) =[¯xSTART(a); ¯xEND(a); xh(a); f (a)]
g(p) =¯xINDEX(p)

(5)

(6)

The soft head representation xh(a) is an attention
mechanism over word inputs x in the argument
span, where the weights e(a) are computed via a
linear layer over the BiLSTM outputs ¯x.
xh(a) = xSTART(a):END(a)e(s)(cid:124)
e(a) = SOFTMAX(w(cid:124)

e ¯xSTART(a):END(a))

(8)

(7)

xSTART(a):END(a) is a shorthand for stacking a list
of vectors xt, where START(a) ≤ t ≤ END(a).

Scoring The scoring functions Φ are imple-
mented with feed-forward networks based on the
predicate and argument representations g:

Φa(a) =w(cid:124)
Φp(p) =w(cid:124)
rel (a, p) =w(l)(cid:124)
Φ(l)

a MLPa(g(a))
p MLPp(g(p))

r MLPr([g(a); g(p)])

(9)

(10)

(11)

Many tourists

tourists visit Disney

Disney to

to meet

meet their favorite

favorite cartoon characters

Span head (xh)

+

+

+

+

+

+

Span representation (g)

Bidirectional LSTM
(¯xm)

Word & character
representation (x)

Many

tourists

visit

Disney

to

meet

their

favorite

cartoon

characters

Figure 2: Building the argument span representations g(a) from BiLSTM outputs. For clarity, we only
show one BiLSTM layer and a small subset of the arguments.

φ(Many tourists, meet, (cid:15)) = 0

φ(Many tourists,
meet, ARG0)

φ(Many tourists,
meet, ARG1)

Softmax
P (yp,a = l | X)

Combined
score φ(p, a, l)

Label score Φ(l)
rel
Unary scores Φa, Φp

Span
representation (g)

Many tourists

meet

Figure 3: The span-pair classiﬁer takes in predi-
cate and argument representations as inputs, and
computes a softmax over the label space L.

4 Experiments

We experiment on the CoNLL 2005 (Carreras and
M`arquez, 2005) and CoNLL 2012 (OntoNotes 5.0,
(Pradhan et al., 2013)) benchmarks, using two
SRL setups: end-to-end and gold predicates. In
the end-to-end setup, a system takes a tokenized
sentence as input, and predicts all the predicates
and their arguments. Systems are evaluated on the
micro-averaged F1 for correctly predicting (pred-
icate, argument span, label) tuples. For compari-
son with previous systems, we also report results
with gold predicates, in which the complete set of
predicates in the input sentence is given as well.
Other experimental setups and hyperparameteres
are listed in Appendix A.1.

ELMo embeddings To further improve perfor-
mance, we also add ELMo word representations
(Peters et al., 2018) to the BiLSTM input (in the
+ELMo rows). Since the contextualized represen-
tations ELMo provides can be applied to most pre-
vious neural systems, the improvement is orthog-
In Table 1 and 2, we
onal to our contribution.
organize all the results into two categories:
the
comparable single model systems, and the mod-

els augmented with ELMo or ensembling (in the
PoE rows).

End-to-end results As shown in Table 1,2 our
joint model outperforms the previous best pipeline
system (He et al., 2017) by an F1 difference of
anywhere between 1.3 and 6.0 in every setting.
The improvement is larger on the Brown test set,
which is out-of-domain, and the CoNLL 2012 test
set, which contains nominal predicates. On all
datasets, our model is able to predict over 40% of
the sentences completely correctly.

Results with gold predicates To compare with
additional previous systems, we also conduct ex-
periments with gold predicates by constraining
our predicate beam to be gold predicates only.
As shown in Table 2, our model signiﬁcantly
out-performs He et al. (2017), but falls short of
Tan et al. (2018), a very recent attention-based
(Vaswani et al., 2017) BIO-tagging model that was
developed concurrently with our work. By adding
the contextualized ELMo representations, we are
able to out-perform all previous systems, includ-
ing Peters et al. (2018), which applies ELMo to
the SRL model introduced in He et al. (2017).

5 Analysis

Our model’s architecture differs signiﬁcantly from
previous BIO systems in terms of both input and
decision space. To better understand our model’s
strengths and weaknesses, we perform three anal-
yses following Lee et al. (2017) and He et al.
(2017), studying (1) the effectiveness of beam

2For the end-to-end setting on CoNLL 2012, we used a
subset of the train/dev data from previous work due to noise
in the dataset; the dev result is not directly comparable. See
Appendix A.2 for detailed explanation.

CoNLL 05 In-domain (WSJ)

Out-of-domain (Brown)

CoNLL 2012 (OntoNotes)

End-to-End

Dev. F1

P

Ours+ELMo
He et al. (2017)PoE

Ours
He et al. (2017)

85.3
81.5

81.6
80.3

84.8
82.0

81.2
80.2

R

87.2
83.4

83.9
82.3

F1

86.0
82.7

82.5
81.2

P

73.9
69.7

69.7
67.6

R

78.4
70.5

71.9
69.6

F1

Dev. F1

P

76.1
70.1

70.8
68.5

83.0
77.2

79.4
75.5

81.9
80.2

79.4
78.6

R

84.0
76.6

80.1
75.1

F1

82.9
78.4

79.8
76.8

Table 1: End-to-end SRL results for CoNLL 2005 and CoNLL 2012, compared to previous systems.
CoNLL 05 contains two test sets: WSJ (in-domain) and Brown (out-of-domain).

WSJ Brown OntoNotes

Arg. Beam

by ambulance
her mother ... ambulance
her mother
Priscilla
should
transported by ambulance
Priscilla says .... ambulance
ambulance

Φa

2.5
2.2
2.2
1.9
1.8
-0.3
-2.2
-3.2

Pred. Beam

says
transported
ambulance
been

Φp

0.1
0.0
-8.3
-11.3

Ours+ELMo
Peters et al. (2018)+ELMo
Tan et al. (2018)PoE
He et al. (2017)PoE
FitzGerald et al. (2015)PoE

Ours
Tan et al. (2018)
He et al. (2017)
Yang and Mitchell (2017)
Zhou and Xu (2015)

87.4
-
86.1
84.6
80.3

83.9
84.8
83.1
81.9
82.8

80.4
-
74.8
73.6
72.2

73.7
74.1
72.1
72.0
69.4

85.5
84.6
83.9
83.4
80.1

82.1
82.7
81.7
-
81.1

Table 2: Experiment results with gold predicates.

pruning, (2) the ability to capture long-range de-
pendencies, (3) agreement with syntactic spans,
and (4) the ability to predict globally consistent
SRL structures. The analyses are performed on
the development sets without using ELMo embed-
dings. 3

Effectiveness of beam pruning Figure 4 shows
the predicate and argument spans kept in the beam,
sorted with their unary scores. Our model efﬁ-
ciently prunes unlikely argument spans and pred-
icates, signiﬁcantly reduces the number of edges
it needs to consider. Figure 5 shows the recall of
predicate words on the CoNLL 2012 development
set. By retaining λp = 0.4 predicates per word,
we are able to keep over 99.7% argument-bearing
predicates. Compared to having a part-of-speech
tagger (POS:X in Figure 5), our joint beam prun-
ing allowing the model to have a soft trade-off be-
tween efﬁciency and recall.4

Long-distance dependencies Figure 6 shows
the performance breakdown by binned distance
between arguments to the given predicates. Our
model is better at accurately predicting arguments
that are farther away from the predicates, even

3For comparability with prior work, analyses (2)-(4) are

performed on the CoNLL 05 dev set with gold predicates.

4The predicate ID accuracy of our model is not compa-
rable with that reported in He et al. (2017), since our model
does not predict non-argument-bearing predicates.

Figure 4: Top: The candidate arguments and pred-
icates in the argument beam Ba and predicate
beam Bp after pruning, along with their unary
scores. Bottom: Predicted SRL relations with two
identiﬁed predicates and their arguments.

compared to an ensemble model (He et al., 2017)
that has a higher overall F1. This is very likely
due to architectural differences; in a BIO tagger,
predicate information passes through many LSTM
timesteps before reaching a long-distance argu-
ment, whereas our architecture enables direct con-
nections between all predicates-arguments pairs.

Agreement with syntax As mentioned in He
et al. (2017), their BIO-based SRL system has
good agreement with gold syntactic span bound-
aries (94.3%) but falls short of previous syntax-
based systems (Punyakanok et al., 2004). By
directly modeling span information, our model
achieves comparable syntactic agreement (95.0%)
to Punyakanok et al. (2004) without explicitly
modeling syntax.

Global consistency On the other hand, our
model suffers from global consistency issues. For
example, on the CoNLL 2005 test set, our model
has lower complete-predicate accuracy (62.6%)
than the BIO systems (He et al., 2017; Tan et al.,
2018) (64.3%-66.4%). Table 3 shows its viola-

100

90

80

70

60

)

%

(

l
l
a
c
e
R

POS:Verb
POS:Verb+Noun
POS:Verb+Noun+Adj
Ours:Predicate beam

0.1

0.2

0.3

0.4

0.5

0.6

0.7

Spans per word λ

Figure 5: Recall of gold argument-bearing predi-
cates on the CoNLL 2012 development data as we
increase the number of predicates kept per word.
POS:X shows the gold predicate recall from using
certain pos-tags identiﬁed by the NLTK part-of-
speech tagger (Bird, 2006).

%
1
F

85
80
75
70
65
60

Ours
He (PoE)
He
Punyakanok

0

1-2

3-6

7-max

Distance (num. words in between)

Figure 6: F1 by surface distance between pred-
icates and arguments, showing degrading perfor-
mance on long-range arguments.

tions of global structural constraints5 compared to
previous systems. Our model made more con-
straint violations compared to previous systems.
For example, our model predicts duplicate core
arguments6 (shown in the U column in Table 3)
more often than previous work. This is due to the
fact that our model uses independent classiﬁers to
label each predicate-argument pair, making it difﬁ-
cult for them to implicitly track the decisions made
for several arguments with the same predicate.

The Ours+decode row in Table 3 shows SRL
performance after enforcing the U-constraint us-
ing dynamic programming (T¨ackstr¨om et al.,
2015) at decoding time. Constrained decoding at
test time is effective at eliminating all the core-role
inconsistencies (shown in the U-column), but did
not bring signiﬁcant gain on the end result (shown

SRL-Violations

Model/Oracle SRL F1 Syn %

Gold

Ours+decode
Ours
He (PoE)
He
Punyakanok

100.0

82.4
82.3
82.7
81.6
77.4

98.7

95.1
95.0
94.3
94.0
95.3

U

24

0
69
37
48
0

C

0

8
7
3
4
0

R

61

104
105
68
73
0

Table 3: Comparison on the CoNLL 05 devel-
opment set against previous systems in terms
of unlabeled agreement with gold constituency
(Syn%) and each type of SRL-constraints viola-
tions (Unique core roles, Continuation roles and
Reference roles).

in SRL F1), which only evaluates the piece-wise
predicate-argument structures.

6 Conclusion and Future Work

We proposed a new SRL model that is able to
jointly predict all predicates and argument spans,
generalized from a recent coreference system (Lee
et al., 2017). Compared to previous BIO systems,
our new model supports joint predicate identiﬁ-
cation and is able to incorporate span-level fea-
tures. Empirically, the model does better at long-
range dependencies and agreement with syntactic
boundaries, but is weaker at global consistency,
due to our strong independence assumption.

In the future, we could incorporate higher-order
inference methods (Lee et al., 2018) to relax this
assumption. It would also be interesting to com-
bine our span-based architecture with the self-
attention layers (Tan et al., 2018; Strubell et al.,
2018) for more effective contextualization.

Acknowledgments

This research was supported in part by the ARO
(W911NF-16-1-0121),
the NSF (IIS-1252835,
IIS-1562364), a gift from Tencent, and an Allen
Distinguished Investigator Award. We thank Eun-
sol Choi, Dipanjan Das, Nicholas Fitzgerald, Ariel
Holtzman, Julian Michael, Noah Smith, Swabha
Swayamdipta, and our anonymous reviewers for
helpful feedback.

5Punyakanok et al. (2008) described a list of global con-
straints for SRL systems, e.g., there can be at most one core
argument of each type for each predicate.

6Arguments with labels ARG0,ARG1,. . . ,ARG5 and AA.

References

In ACL.

Steven Bird. 2006. Nltk: the natural language toolkit.

Xavier Carreras and Llu´ıs M`arquez. 2005.

Introduc-
tion to the conll-2005 shared task: Semantic role la-
beling. In CoNLL.

Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics.

Nicholas FitzGerald, Julian Michael, Luheng He, and
Luke Zettlemoyer. 2018. Large-scale qa-srl parsing.
In ACL.

Nicholas FitzGerald, Oscar T¨ackstr¨om, Kuzman
Ganchev, and Dipanjan Das. 2015. Semantic role
labeling with neural network factors. In EMNLP.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In NIPS.

Luheng He, Kenton Lee, Mike Lewis, and Luke S.
Zettlemoyer. 2017. Deep semantic role labeling:
What works and what’s next. In ACL.

Diederik Kingma and Jimmy Ba. 2015. Adam: A

Method for Stochastic Optimization. In ICLR.

Lingpeng Kong, Chris Dyer, and Noah A Smith. 2016.
Segmental recurrent neural networks. In ICLR.

Kenton Lee, Luheng He, Mike Lewis, and Luke S.
Zettlemoyer. 2017. End-to-end neural coreference
resolution. In EMNLP.

Andrew M Saxe, James L McClelland, and Surya Gan-
guli. 2014. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. arXiv
preprint.

Mitchell Stern, Jacob Andreas, and Dan Klein. 2017. A
minimal span-based neural constituency parser. In
ACL.

Patrick Verga, Daniel Andor,
Emma Strubell,
David Weiss,
and Andrew McCallum. 2018.
Linguistically-Informed Self-Attention for Seman-
tic Role Labeling. arXiv preprint.

Oscar T¨ackstr¨om, Kuzman Ganchev, and Dipanjan
Das. 2015. Efﬁcient inference and structured learn-
ing for semantic role labeling. Transactions of the
Association for Computational Linguistics.

Zhixing Tan, Mingxuan Wang, Jun Xie, Yidong Chen,
and Xiaodong Shi. 2018. Deep semantic role label-
ing with self-attention. In AAAI.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NIPS.

Kenton Lee, Luheng He, and Luke Zettlemoyer. 2018.
Higher-order coreference resolution with coarse-to-
ﬁne inference. In NAACL.

Bishan Yang and Tom M. Mitchell. 2017. A joint
sequential and relational model for frame-semantic
parsing. In EMNLP.

Diego Marcheggiani, Anton Frolov, and Ivan Titov.
2017. A simple and accurate syntax-agnostic neural
model for dependency-based semantic role labeling.
In CoNLL.

Yu Zhang, Guoguo Chen, Dong Yu, Kaisheng Yaco,
Sanjeev Khudanpur, and James Glass. 2016. High-
way long short-term memory rnns for distant speech
recognition. In ICASSP.

Makoto Miwa and Mohit Bansal. 2016. End-to-end re-
lation extraction using lstms on sequences and tree
structures. In ACL.

Vinod Nair and Geoffrey E Hinton. 2010. Rectiﬁed
linear units improve restricted boltzmann machines.
In ICML.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In EMNLP.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In NAACL.

Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Hwee Tou Ng, Anders Bj¨orkelund, Olga Uryupina,
Yuchen Zhang, and Zhi Zhong. 2013. Towards ro-
bust linguistic analysis using ontonotes. In CoNLL.

Vasin Punyakanok, Dan Roth, Wen tau Yih, Dav Zi-
mak, and Yuancheng Tu. 2004. Semantic role la-
beling via generalized inference over classiﬁers. In
CoNLL.

Jie Zhou and Wei Xu. 2015. End-to-end learning of
semantic role labeling using recurrent neural net-
works. In ACL.

A Supplemental Material

A.1 Hyperparameters

Representation sizes The word embeddings are
ﬁxed 300-dimensional GloVe embeddings (Pen-
nington et al., 2014) (context window size of 2
for head word embeddings, and window size of 10
for LSTM inputs), normalized to be unit vectors.
Out-of-vocabulary words are represented by a vec-
tor of zeros. In the character CNN, characters are
represented as learned 8-dimensional embeddings.
The convolutions have window sizes of 3, 4, and 5
characters, each consisting of 50 ﬁlters.

Network sizes We use 3 stacked bidirectional
LSTMs with highway connections and 200 dimen-
sional hidden states. Each MLP consists of two
hidden layers with 150 dimensions and rectiﬁed
linear units (Nair and Hinton, 2010).

CoNLL 2012

OntoNotes5

Train

Dev

Test

Train

Dev

Docs
Sentences
Predicates

2.8
75
189

0.3
9.6
24

0.3
9.5
24

11
116
253

1.5
16
35

Table 4: Data statistics (in number of thousands)
for the CoNLL 2012 split and the train/dev split of
OntoNotes5.

Inference We model spans up to length 30. We
use λa = 0.8 for pruning arguments, λp = 0.4
for pruning predicates. At decoding time, we
use dynamic programming (a simpliﬁed version of
T¨ackstr¨om et al. (2015)) to predict a set of non-
overlapping arguments for each predicate 7.

Training We use Adam (Kingma and Ba, 2015)
with initial learning rate 0.001 and decay rate of
0.1% every 100 steps. The LSTM weights are ini-
tialized with random orthonormal matrices (Saxe
et al., 2014). We apply 0.5 dropout to the word
embeddings and character CNN outputs and 0.2
dropout to all hidden layers and feature embed-
dings. In the LSTMs, we use variational dropout
masks that are shared across timesteps (Gal and
Ghahramani, 2016), with 0.4 dropout rate.

Batching At training time, we randomly shufﬂe
all the documents and then batch at sentence level.
Each batch contains at most 40 sentences and 700
words. All models are trained for at most 320,000
steps with early stopping on the development set,
which takes less than 48 hours on a single Titan X
GPU.

A.2 OntoNotes Data Statistics

Table 4 shows the data statistics on various splits
of OntoNotes. We found that some sentences
in the OntoNotes 5.0 train/dev split have missing
predicates, which is unsuitable for training end-
to-end SRL systems. Therefore, our end-to-end
SRL models are trained on the smaller but cleaner
CoNLL 2012 splits. For experiments with gold
predicates, we use the full OntoNotes 5.0 train/dev
split and the CoNLL 2012 test set, following pre-
vious work.

7This is mainly a constraint enforced by the ofﬁcial

CoNLL evaluation script.

