4
1
0
2
 
v
o
N
5
2
 
 
]
L
M

 

.
t
a
t
s
[
 
 
5
v
7
5
8
5
.
2
1
3
1
:
v
i
X
r
a

A Generative Product-of-Filters Model of Audio

Dawen Liang∗
Columbia University
New York, NY 10027
dliang@ee.columbia.edu

Matthew D. Hoffman
Adobe Research
San Francisco, CA 94103
mathoffm@adobe.com

Gautham J. Mysore
Adobe Research
San Francisco, CA 94103
gmysore@adobe.com

Abstract

We propose the product-of-ﬁlters (PoF) model, a generative model that decom-
poses audio spectra as sparse linear combinations of “ﬁlters” in the log-spectral
domain. PoF makes similar assumptions to those used in the classic homomorphic
ﬁltering approach to signal processing, but replaces decompositions built of basic
signal processing operations with a learned decomposition based on statistical in-
ference. This paper formulates the PoF model and derives a mean-ﬁeld method
for posterior inference and a variational EM algorithm to estimate the model’s free
parameters. We demonstrate PoF’s potential for audio processing on a bandwidth
expansion task, and show that PoF can serve as an effective unsupervised feature
extractor for a speaker identiﬁcation task.

1 Introduction

Some of the most successful approaches to audio signal processing of the last ﬁfty years have been
based on decomposing complicated systems into an excitation signal and some number of simpler
linear systems. One of the simplest (and most widely used) examples is linear predictive coding
(LPC), which uses a simple autoregressive model to decompose an audio signal into an excitation
signal and linear ﬁlter [17]. More broadly, homomorphic ﬁltering methods such as cepstral analysis
[16] try to decompose complicated linear systems into a set of simpler linear systems that can then
be analyzed, interpreted, and manipulated independently.

One reason that this broad approach has been successful is because it is consistent with the way
many real-world objects actually generate sound. An important example is the human voice: human
vocal sounds are generated by running vibrations generated by the vocal folds through the rest of
the vocal tract (tongue, lips, jaw, etc.), which approximately linearly ﬁlters the vibrations that come
from the larynx or lungs.

Traditional approaches typically rely on hand-designed decompositions built of basic operations
such as Fourier transforms, discrete cosine transforms, and least-squares solvers. In this paper we
take a more data-driven approach, and derive a generative product-of-ﬁlters (PoF) model that learns
a statistical-inference-based decomposition that is tuned to be appropriate to the data being ana-
lyzed. Like traditional homomorphic ﬁltering approaches, PoF decomposes audio spectra as linear
combinations of ﬁlters in the log-spectral domain. Unlike previous approaches, these ﬁlters are
learned from data rather than selected from convenient families such as orthogonal cosines, and the
PoF model learns a sparsity-inducing prior that prefers decompositions that use relatively few ﬁlters
to explain each observed spectrum. The result when applied to speech data is that PoF discovers
some ﬁlters that model excitation signals and some that model the various ﬁltering operations that
the vocal tract can perform. Given a set of these learned ﬁlters, PoF can infer how much each ﬁl-
ter contributed to a given audio magnitude spectrum, resulting in a sparse, compact, interpretable
representation.

∗This work was performed while Dawen Liang was an intern at Adobe Research.

1

The rest of the paper proceeds as follows. First, we formally introduce the product-of-ﬁlters (PoF)
model, and give more rigorous intuitions about the assumptions that it makes. Next, we review
some previous work and show how it relates to the PoF model. Then, we derive a mean-ﬁeld
variational inference algorithm that allows us to do approximate posterior inference in PoF, as well
as a variational EM algorithm that ﬁts the model’s free parameters to data. Finally, we demonstrate
PoF’s potential for audio processing on a bandwidth expansion task, where it achieves better results
than a recently proposed technique based on non-negative matrix factorization (NMF). We also
evaluate PoF as an unsupervised feature extractor, and ﬁnd that the representation learned by the
model yields higher accuracy on a speaker identiﬁcation task than the widely used mel-frequency
cepstral coefﬁcient (MFCC) representation.

2 Product-of-Filters Model

We are interested in modeling audio spectrograms, which are collections of Fourier magnitude spec-
tra W taken from some set of audio signals, where W is an F × T non-negative matrix; the cell
Wf t gives the magnitude of the audio signal at frequency bin f and time window (often called a
frame) t. Each column of W is the magnitude of the fast Fourier transform (FFT) of a short window
of an audio signal, within which the spectral characteristics of the signal are assumed to be roughly
stationary.

The motivation for our model comes from the widely used homomorphic ﬁltering approach to audio
and speech signal processing [16], where a short window of audio w[n] is modeled as a convolution
between an excitation signal e[n] (which might come from a speaker’s vocal folds) and the impulse
response h[n] of a series of linear ﬁlters (such as might be implemented by a speaker’s vocal tract):

In the spectral domain after taking the FFT, this is equivalent to:

w[n] = (e ∗ h)[n]

|W[k]| = |E[k]| ◦ |H[k]| = exp{log |E[k]| + log |H[k]|}

where ◦ denotes element-wise multiplication and | · | denotes the magnitude of a complex value pro-
duced by the FFT. Thus, the convolution between these two signals corresponds to a simple addition
of their log-spectra. Another attractive feature is the symmetry between the excitation signal e[n]
and the impulse response h[n] of the vocal-tract ﬁlter—convolution commutes, so mathematically
(if not physiologically) the vocal tract could just as well be exciting the “ﬁlter” implemented by
vocal folds.

We will likewise model the observed magnitude spectra as a product of ﬁlters. We assume each
observed log-spectrum is approximately obtained by linearly combining elements from a pool of L
log-ﬁlters1 U ≡ [u1|u2| · · · |uL] ∈ RF ×L:

log Wf t ≈

lUf lalt,

P

where alt denotes the activation of ﬁlter ul in frame t. We will impose some sparsity on the activa-
tions, to allow us to encode the intuition that not all ﬁlters are always active. This assumption ex-
pands on the expressive power of the simple excitation-ﬁlter model of equation 1; we could recover
that model by partitioning the ﬁlters into “excitations” and “vocal tracts”, requiring that exactly one
“excitation ﬁlter” be active in each frame, and combining the weighted effects of all “vocal tract
ﬁlters” into a single ﬁlter.

We have two main reasons for relaxing the classic excitation-ﬁlter model to include more than two
ﬁlters, one computational and one statistical. The statistical rationale is that the parameters that
deﬁne the human voice (pitch, tongue position, etc.) are inherently continuous, and so a very large
dictionary of excitations and ﬁlters might be necessary to explain observed inter- and intra-speaker
variability with the classic model. The computational rationale is that clustering models (which
might try to determine which excitation is active) can be more fraught with local optima than fac-
torial models such as ours (which tries to determine how active each ﬁlter is), and there is some
precedent for relaxing clustering models into factorial models [3].

1We will use the term “ﬁlter” when referring to U for the rest of the paper.

(1)

(2)

(3)

2

Formally, we deﬁne the product-of-ﬁlters model:

alt ∼ Gamma(αl, αl)

Wf t ∼ Gamma

γf , γf / exp(

(cid:16)

lUf lalt)
(cid:17)

P

where γf is the frequency-dependent noise level. We restrict the activations at (but not the ﬁlters
ul) to be non-negative; if we allowed negative alt, then the the ﬁlters would be inverted, reducing
the interpretability of the model.

Under this model

(4)

(5)

E[alt] = 1
E[Wf t] = exp(

lUf lalt).

P

For l ∈ {1, 2, · · · , L}, αl controls the sparseness of the activations associated with ﬁlter ul; smaller
values of αl indicate that ﬁlter ul is used more rarely. From a generative point of view, one can view
the model as ﬁrst drawing activations alt from a sparse prior, then applying multiplicative gamma
l Uf lalt). A graphical model representation
noise with expected value 1 to the expected value exp(
of the PoF model is shown in Figure 1.

P

wt

T

U, γ

at

α

Figure 1: Graphical model representation of the PoF model.

In this paper we focus on speech applications, but the homomorphic ﬁltering approach has also been
successfully applied to model other kinds of sounds such as musical instruments. For example, [13]
treat the effect of the random excitation, string, and body as a chain of linear systems, which can
therefore be modeled as a product of ﬁlters.

3 Related Work

The PoF model can be interpreted as a matrix factorization model, where we are trying to decompose
the log-spectrogram. A closely related model is non-negative matrix factorization (NMF) [14] and
its variations, e.g., NMF with sparseness constrains [10], convex NMF [4], and fully Bayesian NMF
[2]. In NMF, a F × T non-negative matrix W is approximately decomposed into the product of
two non-negative matrices: a F × K matrix V (often called the dictionary) and a K × T matrix H
(often called the activations). NMF is widely used to analyze audio spectrograms [6, 19], largely
due to its additivity property and the parts-based representation that it induces. It also often provides
a semantically meaningful interpretation. For example, given the NMF decomposition of a piano
sonata, the components in the dictionary are likely to correspond to notes with different pitches, and
the activations will indicate when and how strongly each note is played. NMF’s ability to isolate
energy coming from different sources in mixed recordings has made it a popular tool for addressing
source separation problems.

Although both models decompose audio spectra using a linear combination of dictionary elements,
NMF and PoF make fundamentally different modeling assumptions. NMF models each frame of a
spectrogram as an additive combination of dictionary elements, which approximately corresponds
to modeling the corresponding time-domain signal as a summation of parts. On the other hand, PoF
models each frame of the spectrogram as a product of ﬁlters (sum of log-ﬁlters), which corresponds
to modeling the corresponding time-domain signal as a convolution of ﬁlters. NMF is well suited to
decomposing polyphonic sounds into mixtures of independent sources, whereas PoF is well suited
to decomposing monophonic sounds into simpler systems.

In the compressive sensing literature, there has been a great deal of work on matrix factorization
and dictionary learning by solving an optimization problem with sparseness constraints—adding
ℓ1 norm penalty as a convex relaxation of ℓ0 norm penalty [5]. Online algorithms have also been

3

proposed to handle large data sets [15].
In principle, we could have formulated the PoF model
similarly, using an ℓ1 penalty and convex optimization in place of a gamma prior and Bayesian
inference. However, in such a formulation it is unclear how we might ﬁt the L hyperparameters α
controlling the amount of sparsity in the activations associated with each ﬁlter. We found that PoF
best explains speech training data when each ﬁlter ul has its own sparsity hyperparameter αl, and
performing cross-validation to select so many hyperparameters would be impractical.

4 Inference and Parameter Estimation

From Figure 1, we can see that there are two computational problems that will arise when using the
PoF model. First, given ﬁxed U, α, and γ and input spectrum wt, we must (approximately) compute
the posterior distribution p(at|wt, U, α, γ). This will enable us to ﬁt the PoF model to unseen data
and obtain a different representation in the latent ﬁlter space. Second, given a collection of training
spectra W = {wt}1:T , we want to ﬁnd the maximum likelihood estimates of the free parameters U,
α, and γ. In this section, we will tackle these two problems respectively. The detailed derivations
can be found in the appendix. The source code in Python is available on Github2.

4.1 Mean-Field Posterior Inference

The posterior p(at|wt, U, α, γ) is intractable to compute due to the nonconjugacy of the model.
Therefore, we employ mean-ﬁeld variational inference [12].

Variational inference is a deterministic alternative to Markov Chain Monte Carlo (MCMC) meth-
ods. The basic idea behind variational inference is to choose a tractable family of variational dis-
tributions q(at) to approximate the intractable posterior p(at|wt, U, α, γ), so that the Kullback-
Leibler (KL) divergence between the variational distribution and the true posterior KL(qakpa|W) is
minimized. In particular, we are using the mean-ﬁeld family which is completely factorized, i.e.,
q(at) =
l q(alt). For each alt, we choose a variational distribution from the same family as alt’s
prior distribution: q(alt) = Gamma(alt; νa
t and ρa
t are free parameters that we will tune to
minimize the KL divergence between q and the posterior.

lt). ν a

lt, ρa

Q

We can lower bound the marginal likelihood of the input spectrum wt:

log p(wt|U, α, γ)

≥ Eq[log p(wt, at|U, α, γ)] − Eq[log q(at)]

△
= L(ν a

t , ρa
t )

lt/ρa
To compute the variational lower bound L(ν a
lt
and Eq[log alt] = ψ(νa
lt, where ψ(·) is the digamma function, are both easy to compute.
For Eq[exp(−Uf lalt)], we seek for the moment-generating function of a gamma-distributed random
variable and obtain the expectation as:

t ), the necessary expectations Eq[alt] = νa

lt) − log ρa

t , ρa

Eq[exp(−Uf lalt)] =

−νa
lt

1 + Uf l
ρa
lt (cid:17)

(cid:16)

for Uf l > −ρa

lt, and +∞ otherwise3.

The nonconjugacy and the exponents in the likelihood model preclude optimizing the lower bound
by traditional closed-form coordinate ascent updates. Instead, we compute the gradient of L(ν a
t , ρa
t )
with respect to variational parameters ν a
t and use Limited-memory BFGS (L-BFGS) to opti-
mize the variational lower bound, which guarantees to ﬁnd a local optimum and optimal variational
parameters { ˆν a

t and ρa

t , ˆρa

t }.

Note that in the posterior inference, the optimization problem is independent for different frame t.
Therefore, given input spectra {wt}1:T , we can break down the whole problem into T independent
sub-problems which can be solved in parallel.

2https://github.com/dawenl/pof
3Technically the expectation for Uf l ≤ −ρa

the variational lower bound goes to −∞ and the optimization can be carried out seamlessly.

lt is undeﬁned. Here we treat it as +∞ so that when Uf l ≤ −ρa
lt

(6)

(7)

4

4.2 Parameter Estimation

Given a collection of training audio spectra {wt}1:T , we carry out parameter estimation for the PoF
model by ﬁnding the maximum likelihood estimates of the free parameters U, α, and γ, approxi-
mately marginalizing out at.

We formally deﬁne the parameter estimation problem as

ˆU, ˆα, ˆγ = arg max

log p(wt|U, α, γ)

U,α,γ Xt

(8)

= arg max

log

U,α,γ Xt

Zat

p(wt, at|U, α, γ)dat

This problem can be solved by variational Expectation-Maximization (EM) which maximizes a
lower bound on marginal likelihood in equation 6 with respect to the variational parameters, and
then for the ﬁxed values of variational parameters, maximizes the lower bound with respect to the
model’s free parameters U, α, and γ.

E-step For each wt where t = 1, 2, · · · , T , perform posterior inference by optimizing the values
of the variational parameters { ˆν a

t }. This is done as described in Section 4.1.

t , ˆρa

M-step Maximize the variational lower bound in equation 6, which is equivalent to maximizing
the following objective:

Q(U, α, γ) =

Eq[log p(wt, at|U, α, γ)]

(9)

t

P

This is accomplished by ﬁnding the maximum likelihood estimates using the expected sufﬁcient
statistics for each at that were computed in the E-step. There are no simple closed-form updates for
the M-step. Therefore, we compute the gradient of Q(U, α, γ) with respect to U, α, γ, respectively,
and use L-BFGS to optimize the bound in equation 9.
The most time-consuming part for M-step is updating U, which is a F × L matrix. However,
note that the optimization problem is independent for different frequency bins f ∈ {1, 2, · · · , F }.
Therefore, we can update U by optimizing each row independently, and in parallel if desired.

5 Evaluation

We conducted experiments to assess the PoF model on two different tasks. We evaluate PoF’s ability
to infer missing data in the bandwidth expansion task. We also explore the potential of the PoF model
as an unsupervised feature extractor for the speaker identiﬁcation task.
Both tasks require pre-trained parameters U, α, and γ, which we learned from the TIMIT Speech
Corpus [7]. It contains speech sampled at 16000 Hz from 630 speakers of eight major dialects of
American English, each reading ten phonetically rich sentences. The parameters were learned from
20 randomly selected speakers (10 males and 10 females). We performed a 1024-point FFT with
Hann window and 50% overlap, thus the number of frequency bins was F = 513. We performed
the experiments on magnitude spectrograms except where speciﬁed otherwise.

We tried different model orders L ∈ {10, 20, · · · , 80} and evaluated the lower bound on the marginal
likelihood log p(wt|U, α, γ) in equation 6. In general, larger L will give us a larger variational
lower bound and will be slower to train. In our experiments, we set L = 50 as a compromise
between performance and computational efﬁciency. We initialized all the variational parameters ν a
t
and ρa
t with random draws from a gamma distribution with shape parameter 100 and inverse-scale
parameter 100. This yields a diffuse and smooth initial variational posterior, which helped avoid bad
local optima. We ran variational EM until the variational lower bound increased by less than 0.01%.

Figure 2 demonstrates some representative ﬁlters learned from the PoF model with L = 50. The six
ﬁlters ul associated with the largest values of αl are shown in Figure 2a, and the six ﬁlters associated
with the smallest values of αl are shown in Figure 2b. Small values of αl indicate a prior preference
to use the associated ﬁlters less frequently, since the Gamma(αl, αl) prior places more mass near
0 when αl is smaller. The ﬁlters in Figure 2b, which are used relatively rarely, tend to have the

5

)
B
d
(
 
e
d
u
t
i
n
g
a
M

10
5
0
−5
−10
−15
−20
−25

)
B
d
(
 
e
d
u
t
i
n
g
a
M

20
15
10
5
0
−5
−10
−15
−20

)
B
d
(
 
e
d
u
t
i
n
g
a
M

40

20

0

−20

−40

)
B
d
(
 
e
d
u
t
i
n
g
a
M

30
20
10
0
−10
−20
−30

or

α = 1.68

α = 1.37

α = 1.06

6
4
2
0
−2
−4
−6
−8
−10
−12

15
10
5
0
−5
−10
−15
−20

60
40
20
0
−20
−40
−60

30
20
10
0
−10
−20
−30

0

2

6

8

0

2

6

8

0

2

6

8

4
Frequency (kHz)

α = 0.96

4
Frequency (kHz)

α = 0.94

4
Frequency (kHz)

α = 0.89

0

2

6

8

0

2

6

8

0

2

6

8

4
Frequency (kHz)

4
Frequency (kHz)

4
Frequency (kHz)

(a) The top 6 ﬁlters ul with the largest αl values (shown above each plot).

α = 0.01

α = 0.01

α = 0.02

0

2

6

8

0

2

6

8

0

2

6

8

4
Frequency (kHz)

α = 0.02

4
Frequency (kHz)

α = 0.02

4
Frequency (kHz)

α = 0.02

0

2

6

8

0

2

6

8

0

2

6

8

4
Frequency (kHz)

4
Frequency (kHz)

4
Frequency (kHz)

(b) The top 6 ﬁlters ul with the smallest αl values (shown above each plot).

Figure 2: The representative ﬁlters learned from the PoF model with L = 50.

10
5
0
−5
−10
−15
−20
−25

25
20
15
10
5
0
−5
−10
−15
−20

25
20
15
10
5
0
−5
−10
−15
−20

25
20
15
10
5
0
−5
−10
−15
−20

strong harmonic structure displayed by the log-spectra of periodic signals, while the ﬁlters in Figure
2a tend to vary more smoothly, suggesting that they are being used to model the ﬁltering induced
by the vocal tract. The periodic “excitation” ﬁlters tend to be used more rarely, which is consistent
with the fact that normally there is not more than one excitation signal contributing to a speaker’s
voice. (Very few people can speak or sing more than one pitch simultaneously.) The model has more
freedom to use several of the coarser “vocal tract” ﬁlters per spectrum, which is consistent with the
fact that several aspects of the vocal tract may be combined to ﬁlter the excitation signal generated
by a speaker’s vocal folds.

Despite the non-convexity inherent to all dictionary-learning problems, which causes the details
of the ﬁlters vary from run to run, training with multiple random restarts and different speakers
produced little impact on the ﬁlters that the PoF model learned; in all cases with different model
order L, we found the same “excitation/ﬁlter” structure, similar to what is shown in Figure 2.

5.1 Bandwidth Expansion

We demonstrate PoF’s potential in audio processing applications on a bandwidth expansion task,
where the goal is to infer the contents of a full-bandwidth signal given only the contents of a band-
limited version of that signal. Bandwidth expansion has applications to restoration of low-quality
audio such as might be recorded from a telephone or cheap microphone.
Given the parameters U, α, and γ ﬁt to full-bandwidth training data, we can treat the bandwidth
expansion problem as a missing data problem. Given spectra from a band-limited recording Wbl =
{wbl
t ) over the activations at associated
with the band-limited signal, for t = {1, 2, · · · , T }. We can approximate this posterior using the
variational inference algorithm from Section 4.1 by only using the band-limited part of U and γ.
Then we can reconstruct the full-bandwidth spectra by combining the inferred {at}1:T with the
full-bandwidth U. Following the model formulation in equation 4, we might either estimate the
full-bandwidth spectra using

t }1:T , the model implies a posterior distribution p(at|wbl

Eq[W fb

f t] =

Eq[exp(Uf lalt)]

Eq[W fb

f t] = exp{

lUf l · Eq[alt]}.

l

Q

P

6

(10)

(11)

We use equation 11, both because it is more stable and because human auditory perception is log-
arithmic; if we are summarizing the posterior distribution with a point estimate, the expectation on
the log-spectral domain is more perceptually natural.

As a comparison, NMF is widely used for bandwidth expansion [1, 18, 20]. The full-bandwidth
training spectra Wtrain, which are also used to learn the parameters U, α, and γ for the PoF model,
are decomposed by NMF as Wtrain ≈ VH, where V is the dictionary and H is the activation. Then
given the band-limited spectra Wbl, we can use the band-limited part of V to infer the activation
Hbl. Finally, we can reconstruct the full-bandwidth spectra by computing VHbl.

Based on how the loss function is deﬁned, there can be different types of NMF models: KL-NMF
[14] which is based on Kullback-Leibler divergence, and IS-NMF [6] which is based on Itakura-
Saito divergence, are among the most commonly used NMF decomposition models in audio signal
processing. We compare the PoF model with both KL-NMF and IS-NMF with different model
orders K = 25, 50, and 100. We used the standard multiplicative updates for NMF and stopped the
iterations when the decrease in the cost function was less than 0.01%. For IS-NMF, we used power
spectra instead of magnitude spectra, since the power spectrum representation is more consistent
with the statistical assumptions that underlie the Itakura-Saito divergence.

We randomly selected 10 speakers (5 male and 5 female) from TIMIT that do not overlap with the
speakers used to ﬁt the model parameters U, α, and γ, and took 3 sentences from each speaker
as test data. We cut off all the contents below 400 Hz and above 3400 Hz to obtain band-limited
recordings of approximately telephone-quality speech.

In previous NMF-based bandwidth expansion work [1, 18, 20], all experiments are done in a speaker-
dependent setting, which means the model is trained from the target speaker. What we are doing
here, on the other hand, is speaker-independent: we use no prior knowledge about the speciﬁc
speaker whose speech is being restored4. To our knowledge, little if any work has been done on
speaker-independent bandwidth expansion based on NMF decompositions.

To evaluate the quality of the reconstructed recordings, we used the composite objective measure
[11] and short-time objective intelligibility [21] metrics. These metrics measure different aspects of
the “distance” between the reconstructed speech and the original speech. The composite objective
measure (will be abbreviated as OVRL, as it reﬂects the overall sound quality) was originally pro-
posed as a quality measure for speech enhancement. It aggregates different basic objective measures
and has been shown to correlate with humans’ perceptions of audio quality. OVRL is based on the
predicted perceptual auditory rating and is in the range of 1 to 5 (1: bad; 2: poor; 3: fair; 4: good; 5:
excellent). The short-time objective intelligibility measure (STOI) is a function of the clean speech
and reconstructed speech, which correlates with the intelligibility of the reconstructed speech, that
is, it predicts the ability of listeners to understand what words are being spoken rather than per-
ceived sound quality. STOI is computed as the average correlation coefﬁcient from 15 one-third
octave bands across frames, thus theoretically should be in the range of -1 to 1, where larger values
indicate higher expected intelligibility. However, in practice, even when we ﬁlled out the missing
contents with random noise, the STOI is 0.306 ± 0.016, which can be interpreted as a practical
lower bound on the test data.

The average OVRL and STOI with two standard errors5 across 30 sentences for different methods,
along with these from the band-limited input speech as baseline, are reported in Table 1. We can see
that NMF improves STOI by a small amount, and PoF improves it slightly more, but the improve-
ment in both cases is fairly small. This may be because the band-limited input speech already has
a relatively high STOI (telephone-quality speech is fairly intelligible). On the other hand, PoF pro-
duces better predicted perceived sound quality as measured by OVRL than KL-NMF and IS-NMF
by a large margin regardless of the model order K, improving the sound quality from poor-to-fair
(2.71 OVRL) to fair-to-good (3.25 OVRL).

4When we conducted speaker-dependent experiments, both PoF and NMF produced nearly ceiling-level
results. Thus we only report results on the harder and more practically relevant speaker-independent problem.

5For both OVRL and STOI, we used the MATLAB implementation from the original authors.

7

Table 1: Average OVRL (composite objective measure) and STOI (short-time objective intelligibil-
ity) score with two standard errors (in parenthesis) for the bandwidth expansion task from different
methods. OVRL is in the range of 1 to 5 [1: bad; 2: poor; 3: fair; 4: good; 5: excellent]. STOI is
the average correlation coefﬁcient, thus theoretically should be in the range of -1 to 1, where larger
values indicate higher expected intelligibility.

KL-NMF

Band-limited input
K=25
K=50
K=100
K=25
K=50
K=100

IS-NMF

PoF

OVRL
1.72 (0.16)
2.60 (0.12)
2.71 (0.14)
2.41 (0.10)
2.43 (0.15)
2.62 (0.12)
2.15 (0.10)
3.25 (0.13)

STOI
0.762 (0.012)
0.786 (0.013)
0.790 (0.013)
0.759 (0.012)
0.779 (0.013)
0.774 (0.014)
0.751 (0.012)
0.804 (0.010)

5.2 Feature Learning and Speaker Identiﬁcation

We explore PoF’s potential as an unsupervised feature extractor. One way to interpret the PoF model
is that it attempts to represent the data in a latent ﬁlter space. Therefore, given spectra {wt}1:T , we
can use the coordinates in the latent ﬁlter space {at}1:T as features. Since we believe the inter- and
intra-speaker variability is well-captured by the PoF model, we use speaker identiﬁcation to evaluate
the effectiveness of these features.

We compare our learned representation with mel-frequency cepstral coefﬁcients (MFCCs), which
are widely used in various speech and audio processing tasks including speaker identiﬁcation.
MFCCs are computed by taking the discrete cosine transform (DCT) on mel-scale log-spectra and
using only the low-order coefﬁcients. PoF can be understood in similar terms; we are also trying
to explain the variability in log-spectra in terms of a linear combination of dictionary elements.
However, instead of using the ﬁxed, orthogonal DCT basis, PoF learns a ﬁlter space that is tuned to
the statistics of the input. Therefore, it seems reasonable to hope that the coefﬁcients at from the
PoF model, which will be abbreviated as PoFC, might compare favorably with MFCCs as a feature
representation.

We evaluated speaker identiﬁcation under the following scenario: identify different speakers from
recordings where each speaker may start and ﬁnish talking at random time, but at any given time
there is only one speaker speaking (like during a meeting). This is very similar to speaker diarization
[8], but here we assume we know a priori the number of speakers and certain amount of training data
is available for each speaker. Our goal in this experiment was to demonstrate that the PoFC repre-
sentation captures information that is missing or difﬁcult to extract from the MFCC representation,
rather than trying to build a state-of-the-art speaker identiﬁcation system.

We randomly selected 10 speakers (5 males and 5 females) from TIMIT outside the training data
we used to learn the free parameters U, α, and γ. We used the ﬁrst 13 DCT coefﬁcients which
is a standard choice for computing MFCC. We obtained the PoFC by doing posterior inference as
described in Section 4.1 and used Eq[at] as a point estimate summary. For both MFCC and PoFC,
we computed the ﬁrst-order and second-order differences and concatenated them with the original
feature.

We treat speaker identiﬁcation problem as a classiﬁcation problem and make predictions for each
frame. We trained a multi-class (one-vs-the-rest) linear SVM using eight sentences from each
speaker and tested with the remaining two sentences, which gave us about 7800 frames of train-
ing data and 1700 frames of test data. The test data was randomly permuted so that the order in
which sentences appear is random, which simulates the aforementioned scenario.

The frame level accuracy is reported in the ﬁrst row of Table 2. We can see PoFC increases the
accuracy by a large margin (from 49.1% to 60.5%). To make use of temporal information, we used a
simple median ﬁlter smoother with length 25, which boosts the performance for both representations
equally; these results are reported in the second row of Table 2.

8

Table 2: 10-speaker identiﬁcation accuracy using PoFC, MFCC, and combined. The ﬁrst row shows
the raw frame-level test accuracy. The second row shows the result after applying a simple median
ﬁlter with length 25 on the frame-level prediction.

Frame-level
Smoothing

MFCC
49.1% 60.5%
74.2% 85.0%

65.5%
89.5%

PoFC MFCC + PoFC

Although MFCCs and PoFCs capture similar information, concatenating both sets of features yields
better accuracy than that obtained by either feature set alone. The results achieved by combining
the features are summarized in the last column of Table 2, which indicates that MFCCs and PoFCs
capture complementary information. These results, which use a relatively simple frame-level classi-
ﬁer, suggest that PoFC could produce even better accuracy when used in a more sophisticated model
(e.g. a deep neural network).

6 Discussion and Future Work

In this paper, we proposed the product-of-ﬁlters (PoF) model, a generative model which makes sim-
ilar assumptions to those used in the classic homomorphic ﬁltering approach to signal processing.
We derived variational inference and parameter estimation algorithms, and demonstrated experi-
mental improvements on a bandwidth expansion task and showed that PoF can serve as an effective
unsupervised feature extractor for speaker identiﬁcation.

In this paper, we derived PoF as a standalone model. However, it can also be used as a building
block and integrated into a larger model, e.g., as a prior for the dictionary in a probabilistic NMF
model.

Although the optimization in the variational EM algorithm can be parallelized, currently we cannot
ﬁt PoF to large-scale speech data on a single machine. Leveraging recent developments in stochastic
variational inference [9], it would be possible to learn the free parameters from a much larger, more
diverse speech corpus, or even from streams of data.

References

[1] D. Bansal, B. Raj, and P. Smaragdis. Bandwidth expansion of narrowband speech using non-

negative matrix factorization. In INTERSPEECH, pages 1505–1508, 2005.

[2] A. T. Cemgil. Bayesian inference for nonnegative matrix factorisation models. Computational

Intelligence and Neuroscience, 2009.

[3] C. Ding and X. He. K-means clustering via principal component analysis. In Proceedings of

the International Conference on Machine learning, page 29. ACM, 2004.

[4] C. Ding, T. Li, and M. I. Jordan. Convex and semi-nonnegative matrix factorizations. Pattern

Analysis and Machine Intelligence, IEEE Transactions on, 32(1):45–55, 2010.

[5] D. L. Donoho and M. Elad. Optimally sparse representation in general (nonorthogonal) dictio-
naries via ℓ1 minimization. Proceedings of the National Academy of Sciences, 100(5):2197–
2202, 2003.

[6] C. F´evotte, N. Bertin, and J.-L. Durrieu. Nonnegative matrix factorization with the Itakura-
Saito divergence: with application to music analysis. Neural Computation, 21(3):793–830,
Mar. 2009.

[7] W. M. Fisher, G. R. Doddington, and K. M. Goudie-Marshall. The DARPA speech recognition
research database: speciﬁcations and status. In Proc. DARPA Workshop on speech recognition,
pages 93–99, 1986.

[8] E.B. Fox, E.B. Sudderth, M.I. Jordan, and A.S. Willsky. A Sticky HDP-HMM with Application

to Speaker Diarization. Annals of Applied Statistics, 5(2A):1020–1056, 2011.

[9] M. Hoffman, D. Blei, C. Wang, and J. Paisley. Stochastic variational inference. Journal of

Machine Learning Research, 14:1303–1347, 2013.

9

[10] P. O. Hoyer. Non-negative matrix factorization with sparseness constraints. The Journal of

Machine Learning Research, 5:1457–1469, 2004.

[11] Y. Hu and P. C. Loizou. Evaluation of objective quality measures for speech enhancement.
Audio, Speech, and Language Processing, IEEE Transactions on, 16(1):229–238, 2008.

[12] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational

methods for graphical models. Machine learning, 37(2):183–233, 1999.

[13] M. Karjalainen, V. V¨alim¨aki, and Z. J´anosy. Towards high-quality sound synthesis of the guitar
and string instruments. In Proceedings of the International Computer Music Conference, pages
56–56, 1993.

[14] D.D. Lee and H.S. Seung. Algorithms for non-negative matrix factorization. Advances in

Neural Information Processing Systems, 13:556–562, 2001.

[15] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online learning for matrix factorization and sparse

coding. The Journal of Machine Learning Research, 11:19–60, 2010.

[16] A. Oppenheim and R. Schafer. Homomorphic analysis of speech. Audio and Electroacoustics,

IEEE Transactions on, 16(2):221–226, 1968.

[17] D. O’Shaughnessy. Linear predictive coding. Potentials, IEEE, 7(1):29–32, 1988.
[18] B. Raj, R. Singh, M. Shashanka, and P. Smaragdis. Bandwidth expansionwith a P´olya urn
model. In Acoustics, Speech and Signal Processing, IEEE International Conference on, vol-
ume 4, pages IV–597. IEEE, 2007.

[19] P. Smaragdis and J. C. Brown. Non-negative matrix factorization for polyphonic music tran-
scription. In Applications of Signal Processing to Audio and Acoustics, IEEE Workshop on.,
pages 177–180. IEEE, 2003.

[20] D. L. Sun and R. Mazumder. Non-negative matrix completion for bandwidth extension: A
In Machine Learning for Signal Processing (MLSP), IEEE

convex optimization approach.
International Workshop on, pages 1–6. IEEE, 2013.

[21] C. H. Taal, R. C. Hendriks, R. Heusdens, and J. Jensen. An algorithm for intelligibility pre-
diction of time–frequency weighted noisy speech. Audio, Speech, and Language Processing,
IEEE Transactions on, 19(7):2125–2136, 2011.

10

A Variational EM for Product-of-Filters Model

A.1 E-step (Posterior Inference)

Following Section 4.1, the variational lower bound for the E-step (equation 6):
log p(wt|U, α, γ)

= log

q(at)

Zat

≥

Zat

q(at) log

p(wt, at|U, α, γ)
q(at)
p(wt, at|U, α, γ)
q(at)

dat

dat

(12)

t , ρa
t )
The second term is the entropy of a gamma-distributed random variable:

= Eq[log p(wt, at|U, α, γ)] − Eq[log q(at)]
≡ L(ν a

− Eq[log q(at)]
νa
lt − log ρa

=

Xl (cid:16)

lt + log Γ(νa

lt) + (1 − νa

lt)ψ(νa
lt)

(cid:17)

For the ﬁrst term, we can keep the parts which depend on {ν a

t , ρa

t }:

Eq[log p(wt, at|U, α, γ)]

= Eq[log p(wt|at, U, γ)] + Eq[log p(at|α)]

= const +

(αl − 1)Eq[log alt] − αlEq[alt]

−

γf

Wf t

Eq[exp(−Uf lalt)] +

Uf lEq[alt]

Xl n

o

Xf

n

Yl

Xl

o

Take the derivative of L(ν a

t , ρa

t ) with respect to νa

lt and ρa
lt:

∂L
∂νa
lt

=

γf

Wf t log(1 +

(cid:26)

Xf

Uf l
ρa
lt

)

L

Yj=1

Eq[exp(−Uf jajt)] −

+(αl − νa

lt)ψ1(νa

lt) + 1 −

Uf l
ρa
lt (cid:27)

αl
ρa
lt

∂L
∂ρa
lt

=

νlt
(ρa
lt)2

(cid:26)

Xf

γf

Uf l − Wf t(1 +

)−1Uf l

Eq[exp(−Uf jajt)]

+αl(

νlt
lt)2 −
(ρa

1
ρa
lt

)

(cid:27)

Uf l
ρa
lt

L

Yj=1

where ψ1(·) is the trigamma function.

A.2 M-step

The objective function for M-step is:

Q(U, α, γ) =

Eq[log p(wt, at|U, α, γ)]

=

Eq[log p(wt|at, U, γ)] + Eq[log p(at|α)]

(13)

Xt

Xt

where

Eq[log p(wt|at, U, γ)]

Xf (cid:16)
Eq[log p(at|α)]

Xl

=

γf log γf − γf

Uf lEq[alt] − log Γ(γf ) + (γf − 1) log Wf t − Wf tγf

Eq[exp(−Uf lalt)]

Yl

(cid:17)

=

αl log αl − log Γ(αl) + (αl − 1)Eq[log alt] − αlEq[alt]

(cid:17)

Xl (cid:16)

11

Take the derivative with respect to U, α, γ, we obtain the following gradients:

∂Q
∂Uf l

∂Q
∂αl

∂Q
∂γf

=

=

=

Xt (cid:16)

Xt (cid:16)

Xt (cid:16)

− Eq[alt] + Wf tEq[alt](1 +

)−(νa

lt+1)

Eq[exp(−Uf jajt)]

(cid:17)

Uf l
ρa
lt

log αl + 1 − ψ(αl) + Eq[log alt] − Eq[alt]

Yj6=l

(cid:17)

log γf −

Uf lEq[alt] + 1 − ψ(γf ) + log Wf t − Wf t

Eq[exp(−Uf lalt)]

Xl

Yl

(14)

(15)

(cid:17)
(16)

Note that the optimization problem for U is independent for different frequency bin f ∈
{1, 2, · · · , F }, as reﬂected by the gradient.

12

4
1
0
2
 
v
o
N
5
2
 
 
]
L
M

 

.
t
a
t
s
[
 
 
5
v
7
5
8
5
.
2
1
3
1
:
v
i
X
r
a

A Generative Product-of-Filters Model of Audio

Dawen Liang∗
Columbia University
New York, NY 10027
dliang@ee.columbia.edu

Matthew D. Hoffman
Adobe Research
San Francisco, CA 94103
mathoffm@adobe.com

Gautham J. Mysore
Adobe Research
San Francisco, CA 94103
gmysore@adobe.com

Abstract

We propose the product-of-ﬁlters (PoF) model, a generative model that decom-
poses audio spectra as sparse linear combinations of “ﬁlters” in the log-spectral
domain. PoF makes similar assumptions to those used in the classic homomorphic
ﬁltering approach to signal processing, but replaces decompositions built of basic
signal processing operations with a learned decomposition based on statistical in-
ference. This paper formulates the PoF model and derives a mean-ﬁeld method
for posterior inference and a variational EM algorithm to estimate the model’s free
parameters. We demonstrate PoF’s potential for audio processing on a bandwidth
expansion task, and show that PoF can serve as an effective unsupervised feature
extractor for a speaker identiﬁcation task.

1 Introduction

Some of the most successful approaches to audio signal processing of the last ﬁfty years have been
based on decomposing complicated systems into an excitation signal and some number of simpler
linear systems. One of the simplest (and most widely used) examples is linear predictive coding
(LPC), which uses a simple autoregressive model to decompose an audio signal into an excitation
signal and linear ﬁlter [17]. More broadly, homomorphic ﬁltering methods such as cepstral analysis
[16] try to decompose complicated linear systems into a set of simpler linear systems that can then
be analyzed, interpreted, and manipulated independently.

One reason that this broad approach has been successful is because it is consistent with the way
many real-world objects actually generate sound. An important example is the human voice: human
vocal sounds are generated by running vibrations generated by the vocal folds through the rest of
the vocal tract (tongue, lips, jaw, etc.), which approximately linearly ﬁlters the vibrations that come
from the larynx or lungs.

Traditional approaches typically rely on hand-designed decompositions built of basic operations
such as Fourier transforms, discrete cosine transforms, and least-squares solvers. In this paper we
take a more data-driven approach, and derive a generative product-of-ﬁlters (PoF) model that learns
a statistical-inference-based decomposition that is tuned to be appropriate to the data being ana-
lyzed. Like traditional homomorphic ﬁltering approaches, PoF decomposes audio spectra as linear
combinations of ﬁlters in the log-spectral domain. Unlike previous approaches, these ﬁlters are
learned from data rather than selected from convenient families such as orthogonal cosines, and the
PoF model learns a sparsity-inducing prior that prefers decompositions that use relatively few ﬁlters
to explain each observed spectrum. The result when applied to speech data is that PoF discovers
some ﬁlters that model excitation signals and some that model the various ﬁltering operations that
the vocal tract can perform. Given a set of these learned ﬁlters, PoF can infer how much each ﬁl-
ter contributed to a given audio magnitude spectrum, resulting in a sparse, compact, interpretable
representation.

∗This work was performed while Dawen Liang was an intern at Adobe Research.

1

The rest of the paper proceeds as follows. First, we formally introduce the product-of-ﬁlters (PoF)
model, and give more rigorous intuitions about the assumptions that it makes. Next, we review
some previous work and show how it relates to the PoF model. Then, we derive a mean-ﬁeld
variational inference algorithm that allows us to do approximate posterior inference in PoF, as well
as a variational EM algorithm that ﬁts the model’s free parameters to data. Finally, we demonstrate
PoF’s potential for audio processing on a bandwidth expansion task, where it achieves better results
than a recently proposed technique based on non-negative matrix factorization (NMF). We also
evaluate PoF as an unsupervised feature extractor, and ﬁnd that the representation learned by the
model yields higher accuracy on a speaker identiﬁcation task than the widely used mel-frequency
cepstral coefﬁcient (MFCC) representation.

2 Product-of-Filters Model

We are interested in modeling audio spectrograms, which are collections of Fourier magnitude spec-
tra W taken from some set of audio signals, where W is an F × T non-negative matrix; the cell
Wf t gives the magnitude of the audio signal at frequency bin f and time window (often called a
frame) t. Each column of W is the magnitude of the fast Fourier transform (FFT) of a short window
of an audio signal, within which the spectral characteristics of the signal are assumed to be roughly
stationary.

The motivation for our model comes from the widely used homomorphic ﬁltering approach to audio
and speech signal processing [16], where a short window of audio w[n] is modeled as a convolution
between an excitation signal e[n] (which might come from a speaker’s vocal folds) and the impulse
response h[n] of a series of linear ﬁlters (such as might be implemented by a speaker’s vocal tract):

In the spectral domain after taking the FFT, this is equivalent to:

w[n] = (e ∗ h)[n]

|W[k]| = |E[k]| ◦ |H[k]| = exp{log |E[k]| + log |H[k]|}

where ◦ denotes element-wise multiplication and | · | denotes the magnitude of a complex value pro-
duced by the FFT. Thus, the convolution between these two signals corresponds to a simple addition
of their log-spectra. Another attractive feature is the symmetry between the excitation signal e[n]
and the impulse response h[n] of the vocal-tract ﬁlter—convolution commutes, so mathematically
(if not physiologically) the vocal tract could just as well be exciting the “ﬁlter” implemented by
vocal folds.

We will likewise model the observed magnitude spectra as a product of ﬁlters. We assume each
observed log-spectrum is approximately obtained by linearly combining elements from a pool of L
log-ﬁlters1 U ≡ [u1|u2| · · · |uL] ∈ RF ×L:

log Wf t ≈

lUf lalt,

P

where alt denotes the activation of ﬁlter ul in frame t. We will impose some sparsity on the activa-
tions, to allow us to encode the intuition that not all ﬁlters are always active. This assumption ex-
pands on the expressive power of the simple excitation-ﬁlter model of equation 1; we could recover
that model by partitioning the ﬁlters into “excitations” and “vocal tracts”, requiring that exactly one
“excitation ﬁlter” be active in each frame, and combining the weighted effects of all “vocal tract
ﬁlters” into a single ﬁlter.

We have two main reasons for relaxing the classic excitation-ﬁlter model to include more than two
ﬁlters, one computational and one statistical. The statistical rationale is that the parameters that
deﬁne the human voice (pitch, tongue position, etc.) are inherently continuous, and so a very large
dictionary of excitations and ﬁlters might be necessary to explain observed inter- and intra-speaker
variability with the classic model. The computational rationale is that clustering models (which
might try to determine which excitation is active) can be more fraught with local optima than fac-
torial models such as ours (which tries to determine how active each ﬁlter is), and there is some
precedent for relaxing clustering models into factorial models [3].

1We will use the term “ﬁlter” when referring to U for the rest of the paper.

(1)

(2)

(3)

2

Formally, we deﬁne the product-of-ﬁlters model:

alt ∼ Gamma(αl, αl)

Wf t ∼ Gamma

γf , γf / exp(

(cid:16)

lUf lalt)
(cid:17)

P

where γf is the frequency-dependent noise level. We restrict the activations at (but not the ﬁlters
ul) to be non-negative; if we allowed negative alt, then the the ﬁlters would be inverted, reducing
the interpretability of the model.

Under this model

(4)

(5)

E[alt] = 1
E[Wf t] = exp(

lUf lalt).

P

For l ∈ {1, 2, · · · , L}, αl controls the sparseness of the activations associated with ﬁlter ul; smaller
values of αl indicate that ﬁlter ul is used more rarely. From a generative point of view, one can view
the model as ﬁrst drawing activations alt from a sparse prior, then applying multiplicative gamma
l Uf lalt). A graphical model representation
noise with expected value 1 to the expected value exp(
of the PoF model is shown in Figure 1.

P

wt

T

U, γ

at

α

Figure 1: Graphical model representation of the PoF model.

In this paper we focus on speech applications, but the homomorphic ﬁltering approach has also been
successfully applied to model other kinds of sounds such as musical instruments. For example, [13]
treat the effect of the random excitation, string, and body as a chain of linear systems, which can
therefore be modeled as a product of ﬁlters.

3 Related Work

The PoF model can be interpreted as a matrix factorization model, where we are trying to decompose
the log-spectrogram. A closely related model is non-negative matrix factorization (NMF) [14] and
its variations, e.g., NMF with sparseness constrains [10], convex NMF [4], and fully Bayesian NMF
[2]. In NMF, a F × T non-negative matrix W is approximately decomposed into the product of
two non-negative matrices: a F × K matrix V (often called the dictionary) and a K × T matrix H
(often called the activations). NMF is widely used to analyze audio spectrograms [6, 19], largely
due to its additivity property and the parts-based representation that it induces. It also often provides
a semantically meaningful interpretation. For example, given the NMF decomposition of a piano
sonata, the components in the dictionary are likely to correspond to notes with different pitches, and
the activations will indicate when and how strongly each note is played. NMF’s ability to isolate
energy coming from different sources in mixed recordings has made it a popular tool for addressing
source separation problems.

Although both models decompose audio spectra using a linear combination of dictionary elements,
NMF and PoF make fundamentally different modeling assumptions. NMF models each frame of a
spectrogram as an additive combination of dictionary elements, which approximately corresponds
to modeling the corresponding time-domain signal as a summation of parts. On the other hand, PoF
models each frame of the spectrogram as a product of ﬁlters (sum of log-ﬁlters), which corresponds
to modeling the corresponding time-domain signal as a convolution of ﬁlters. NMF is well suited to
decomposing polyphonic sounds into mixtures of independent sources, whereas PoF is well suited
to decomposing monophonic sounds into simpler systems.

In the compressive sensing literature, there has been a great deal of work on matrix factorization
and dictionary learning by solving an optimization problem with sparseness constraints—adding
ℓ1 norm penalty as a convex relaxation of ℓ0 norm penalty [5]. Online algorithms have also been

3

proposed to handle large data sets [15].
In principle, we could have formulated the PoF model
similarly, using an ℓ1 penalty and convex optimization in place of a gamma prior and Bayesian
inference. However, in such a formulation it is unclear how we might ﬁt the L hyperparameters α
controlling the amount of sparsity in the activations associated with each ﬁlter. We found that PoF
best explains speech training data when each ﬁlter ul has its own sparsity hyperparameter αl, and
performing cross-validation to select so many hyperparameters would be impractical.

4 Inference and Parameter Estimation

From Figure 1, we can see that there are two computational problems that will arise when using the
PoF model. First, given ﬁxed U, α, and γ and input spectrum wt, we must (approximately) compute
the posterior distribution p(at|wt, U, α, γ). This will enable us to ﬁt the PoF model to unseen data
and obtain a different representation in the latent ﬁlter space. Second, given a collection of training
spectra W = {wt}1:T , we want to ﬁnd the maximum likelihood estimates of the free parameters U,
α, and γ. In this section, we will tackle these two problems respectively. The detailed derivations
can be found in the appendix. The source code in Python is available on Github2.

4.1 Mean-Field Posterior Inference

The posterior p(at|wt, U, α, γ) is intractable to compute due to the nonconjugacy of the model.
Therefore, we employ mean-ﬁeld variational inference [12].

Variational inference is a deterministic alternative to Markov Chain Monte Carlo (MCMC) meth-
ods. The basic idea behind variational inference is to choose a tractable family of variational dis-
tributions q(at) to approximate the intractable posterior p(at|wt, U, α, γ), so that the Kullback-
Leibler (KL) divergence between the variational distribution and the true posterior KL(qakpa|W) is
minimized. In particular, we are using the mean-ﬁeld family which is completely factorized, i.e.,
q(at) =
l q(alt). For each alt, we choose a variational distribution from the same family as alt’s
prior distribution: q(alt) = Gamma(alt; νa
t and ρa
t are free parameters that we will tune to
minimize the KL divergence between q and the posterior.

lt). ν a

lt, ρa

Q

We can lower bound the marginal likelihood of the input spectrum wt:

log p(wt|U, α, γ)

≥ Eq[log p(wt, at|U, α, γ)] − Eq[log q(at)]

△
= L(ν a

t , ρa
t )

lt/ρa
To compute the variational lower bound L(ν a
lt
and Eq[log alt] = ψ(νa
lt, where ψ(·) is the digamma function, are both easy to compute.
For Eq[exp(−Uf lalt)], we seek for the moment-generating function of a gamma-distributed random
variable and obtain the expectation as:

t ), the necessary expectations Eq[alt] = νa

lt) − log ρa

t , ρa

Eq[exp(−Uf lalt)] =

−νa
lt

1 + Uf l
ρa
lt (cid:17)

(cid:16)

for Uf l > −ρa

lt, and +∞ otherwise3.

The nonconjugacy and the exponents in the likelihood model preclude optimizing the lower bound
by traditional closed-form coordinate ascent updates. Instead, we compute the gradient of L(ν a
t , ρa
t )
with respect to variational parameters ν a
t and use Limited-memory BFGS (L-BFGS) to opti-
mize the variational lower bound, which guarantees to ﬁnd a local optimum and optimal variational
parameters { ˆν a

t and ρa

t , ˆρa

t }.

Note that in the posterior inference, the optimization problem is independent for different frame t.
Therefore, given input spectra {wt}1:T , we can break down the whole problem into T independent
sub-problems which can be solved in parallel.

2https://github.com/dawenl/pof
3Technically the expectation for Uf l ≤ −ρa

the variational lower bound goes to −∞ and the optimization can be carried out seamlessly.

lt is undeﬁned. Here we treat it as +∞ so that when Uf l ≤ −ρa
lt

(6)

(7)

4

4.2 Parameter Estimation

Given a collection of training audio spectra {wt}1:T , we carry out parameter estimation for the PoF
model by ﬁnding the maximum likelihood estimates of the free parameters U, α, and γ, approxi-
mately marginalizing out at.

We formally deﬁne the parameter estimation problem as

ˆU, ˆα, ˆγ = arg max

log p(wt|U, α, γ)

U,α,γ Xt

(8)

= arg max

log

U,α,γ Xt

Zat

p(wt, at|U, α, γ)dat

This problem can be solved by variational Expectation-Maximization (EM) which maximizes a
lower bound on marginal likelihood in equation 6 with respect to the variational parameters, and
then for the ﬁxed values of variational parameters, maximizes the lower bound with respect to the
model’s free parameters U, α, and γ.

E-step For each wt where t = 1, 2, · · · , T , perform posterior inference by optimizing the values
of the variational parameters { ˆν a

t }. This is done as described in Section 4.1.

t , ˆρa

M-step Maximize the variational lower bound in equation 6, which is equivalent to maximizing
the following objective:

Q(U, α, γ) =

Eq[log p(wt, at|U, α, γ)]

(9)

t

P

This is accomplished by ﬁnding the maximum likelihood estimates using the expected sufﬁcient
statistics for each at that were computed in the E-step. There are no simple closed-form updates for
the M-step. Therefore, we compute the gradient of Q(U, α, γ) with respect to U, α, γ, respectively,
and use L-BFGS to optimize the bound in equation 9.
The most time-consuming part for M-step is updating U, which is a F × L matrix. However,
note that the optimization problem is independent for different frequency bins f ∈ {1, 2, · · · , F }.
Therefore, we can update U by optimizing each row independently, and in parallel if desired.

5 Evaluation

We conducted experiments to assess the PoF model on two different tasks. We evaluate PoF’s ability
to infer missing data in the bandwidth expansion task. We also explore the potential of the PoF model
as an unsupervised feature extractor for the speaker identiﬁcation task.
Both tasks require pre-trained parameters U, α, and γ, which we learned from the TIMIT Speech
Corpus [7]. It contains speech sampled at 16000 Hz from 630 speakers of eight major dialects of
American English, each reading ten phonetically rich sentences. The parameters were learned from
20 randomly selected speakers (10 males and 10 females). We performed a 1024-point FFT with
Hann window and 50% overlap, thus the number of frequency bins was F = 513. We performed
the experiments on magnitude spectrograms except where speciﬁed otherwise.

We tried different model orders L ∈ {10, 20, · · · , 80} and evaluated the lower bound on the marginal
likelihood log p(wt|U, α, γ) in equation 6. In general, larger L will give us a larger variational
lower bound and will be slower to train. In our experiments, we set L = 50 as a compromise
between performance and computational efﬁciency. We initialized all the variational parameters ν a
t
and ρa
t with random draws from a gamma distribution with shape parameter 100 and inverse-scale
parameter 100. This yields a diffuse and smooth initial variational posterior, which helped avoid bad
local optima. We ran variational EM until the variational lower bound increased by less than 0.01%.

Figure 2 demonstrates some representative ﬁlters learned from the PoF model with L = 50. The six
ﬁlters ul associated with the largest values of αl are shown in Figure 2a, and the six ﬁlters associated
with the smallest values of αl are shown in Figure 2b. Small values of αl indicate a prior preference
to use the associated ﬁlters less frequently, since the Gamma(αl, αl) prior places more mass near
0 when αl is smaller. The ﬁlters in Figure 2b, which are used relatively rarely, tend to have the

5

)
B
d
(
 
e
d
u
t
i
n
g
a
M

10
5
0
−5
−10
−15
−20
−25

)
B
d
(
 
e
d
u
t
i
n
g
a
M

20
15
10
5
0
−5
−10
−15
−20

)
B
d
(
 
e
d
u
t
i
n
g
a
M

40

20

0

−20

−40

)
B
d
(
 
e
d
u
t
i
n
g
a
M

30
20
10
0
−10
−20
−30

or

α = 1.68

α = 1.37

α = 1.06

6
4
2
0
−2
−4
−6
−8
−10
−12

15
10
5
0
−5
−10
−15
−20

60
40
20
0
−20
−40
−60

30
20
10
0
−10
−20
−30

0

2

6

8

0

2

6

8

0

2

6

8

4
Frequency (kHz)

α = 0.96

4
Frequency (kHz)

α = 0.94

4
Frequency (kHz)

α = 0.89

0

2

6

8

0

2

6

8

0

2

6

8

4
Frequency (kHz)

4
Frequency (kHz)

4
Frequency (kHz)

(a) The top 6 ﬁlters ul with the largest αl values (shown above each plot).

α = 0.01

α = 0.01

α = 0.02

0

2

6

8

0

2

6

8

0

2

6

8

4
Frequency (kHz)

α = 0.02

4
Frequency (kHz)

α = 0.02

4
Frequency (kHz)

α = 0.02

0

2

6

8

0

2

6

8

0

2

6

8

4
Frequency (kHz)

4
Frequency (kHz)

4
Frequency (kHz)

(b) The top 6 ﬁlters ul with the smallest αl values (shown above each plot).

Figure 2: The representative ﬁlters learned from the PoF model with L = 50.

10
5
0
−5
−10
−15
−20
−25

25
20
15
10
5
0
−5
−10
−15
−20

25
20
15
10
5
0
−5
−10
−15
−20

25
20
15
10
5
0
−5
−10
−15
−20

strong harmonic structure displayed by the log-spectra of periodic signals, while the ﬁlters in Figure
2a tend to vary more smoothly, suggesting that they are being used to model the ﬁltering induced
by the vocal tract. The periodic “excitation” ﬁlters tend to be used more rarely, which is consistent
with the fact that normally there is not more than one excitation signal contributing to a speaker’s
voice. (Very few people can speak or sing more than one pitch simultaneously.) The model has more
freedom to use several of the coarser “vocal tract” ﬁlters per spectrum, which is consistent with the
fact that several aspects of the vocal tract may be combined to ﬁlter the excitation signal generated
by a speaker’s vocal folds.

Despite the non-convexity inherent to all dictionary-learning problems, which causes the details
of the ﬁlters vary from run to run, training with multiple random restarts and different speakers
produced little impact on the ﬁlters that the PoF model learned; in all cases with different model
order L, we found the same “excitation/ﬁlter” structure, similar to what is shown in Figure 2.

5.1 Bandwidth Expansion

We demonstrate PoF’s potential in audio processing applications on a bandwidth expansion task,
where the goal is to infer the contents of a full-bandwidth signal given only the contents of a band-
limited version of that signal. Bandwidth expansion has applications to restoration of low-quality
audio such as might be recorded from a telephone or cheap microphone.
Given the parameters U, α, and γ ﬁt to full-bandwidth training data, we can treat the bandwidth
expansion problem as a missing data problem. Given spectra from a band-limited recording Wbl =
{wbl
t ) over the activations at associated
with the band-limited signal, for t = {1, 2, · · · , T }. We can approximate this posterior using the
variational inference algorithm from Section 4.1 by only using the band-limited part of U and γ.
Then we can reconstruct the full-bandwidth spectra by combining the inferred {at}1:T with the
full-bandwidth U. Following the model formulation in equation 4, we might either estimate the
full-bandwidth spectra using

t }1:T , the model implies a posterior distribution p(at|wbl

Eq[W fb

f t] =

Eq[exp(Uf lalt)]

Eq[W fb

f t] = exp{

lUf l · Eq[alt]}.

l

Q

P

6

(10)

(11)

We use equation 11, both because it is more stable and because human auditory perception is log-
arithmic; if we are summarizing the posterior distribution with a point estimate, the expectation on
the log-spectral domain is more perceptually natural.

As a comparison, NMF is widely used for bandwidth expansion [1, 18, 20]. The full-bandwidth
training spectra Wtrain, which are also used to learn the parameters U, α, and γ for the PoF model,
are decomposed by NMF as Wtrain ≈ VH, where V is the dictionary and H is the activation. Then
given the band-limited spectra Wbl, we can use the band-limited part of V to infer the activation
Hbl. Finally, we can reconstruct the full-bandwidth spectra by computing VHbl.

Based on how the loss function is deﬁned, there can be different types of NMF models: KL-NMF
[14] which is based on Kullback-Leibler divergence, and IS-NMF [6] which is based on Itakura-
Saito divergence, are among the most commonly used NMF decomposition models in audio signal
processing. We compare the PoF model with both KL-NMF and IS-NMF with different model
orders K = 25, 50, and 100. We used the standard multiplicative updates for NMF and stopped the
iterations when the decrease in the cost function was less than 0.01%. For IS-NMF, we used power
spectra instead of magnitude spectra, since the power spectrum representation is more consistent
with the statistical assumptions that underlie the Itakura-Saito divergence.

We randomly selected 10 speakers (5 male and 5 female) from TIMIT that do not overlap with the
speakers used to ﬁt the model parameters U, α, and γ, and took 3 sentences from each speaker
as test data. We cut off all the contents below 400 Hz and above 3400 Hz to obtain band-limited
recordings of approximately telephone-quality speech.

In previous NMF-based bandwidth expansion work [1, 18, 20], all experiments are done in a speaker-
dependent setting, which means the model is trained from the target speaker. What we are doing
here, on the other hand, is speaker-independent: we use no prior knowledge about the speciﬁc
speaker whose speech is being restored4. To our knowledge, little if any work has been done on
speaker-independent bandwidth expansion based on NMF decompositions.

To evaluate the quality of the reconstructed recordings, we used the composite objective measure
[11] and short-time objective intelligibility [21] metrics. These metrics measure different aspects of
the “distance” between the reconstructed speech and the original speech. The composite objective
measure (will be abbreviated as OVRL, as it reﬂects the overall sound quality) was originally pro-
posed as a quality measure for speech enhancement. It aggregates different basic objective measures
and has been shown to correlate with humans’ perceptions of audio quality. OVRL is based on the
predicted perceptual auditory rating and is in the range of 1 to 5 (1: bad; 2: poor; 3: fair; 4: good; 5:
excellent). The short-time objective intelligibility measure (STOI) is a function of the clean speech
and reconstructed speech, which correlates with the intelligibility of the reconstructed speech, that
is, it predicts the ability of listeners to understand what words are being spoken rather than per-
ceived sound quality. STOI is computed as the average correlation coefﬁcient from 15 one-third
octave bands across frames, thus theoretically should be in the range of -1 to 1, where larger values
indicate higher expected intelligibility. However, in practice, even when we ﬁlled out the missing
contents with random noise, the STOI is 0.306 ± 0.016, which can be interpreted as a practical
lower bound on the test data.

The average OVRL and STOI with two standard errors5 across 30 sentences for different methods,
along with these from the band-limited input speech as baseline, are reported in Table 1. We can see
that NMF improves STOI by a small amount, and PoF improves it slightly more, but the improve-
ment in both cases is fairly small. This may be because the band-limited input speech already has
a relatively high STOI (telephone-quality speech is fairly intelligible). On the other hand, PoF pro-
duces better predicted perceived sound quality as measured by OVRL than KL-NMF and IS-NMF
by a large margin regardless of the model order K, improving the sound quality from poor-to-fair
(2.71 OVRL) to fair-to-good (3.25 OVRL).

4When we conducted speaker-dependent experiments, both PoF and NMF produced nearly ceiling-level
results. Thus we only report results on the harder and more practically relevant speaker-independent problem.

5For both OVRL and STOI, we used the MATLAB implementation from the original authors.

7

Table 1: Average OVRL (composite objective measure) and STOI (short-time objective intelligibil-
ity) score with two standard errors (in parenthesis) for the bandwidth expansion task from different
methods. OVRL is in the range of 1 to 5 [1: bad; 2: poor; 3: fair; 4: good; 5: excellent]. STOI is
the average correlation coefﬁcient, thus theoretically should be in the range of -1 to 1, where larger
values indicate higher expected intelligibility.

KL-NMF

Band-limited input
K=25
K=50
K=100
K=25
K=50
K=100

IS-NMF

PoF

OVRL
1.72 (0.16)
2.60 (0.12)
2.71 (0.14)
2.41 (0.10)
2.43 (0.15)
2.62 (0.12)
2.15 (0.10)
3.25 (0.13)

STOI
0.762 (0.012)
0.786 (0.013)
0.790 (0.013)
0.759 (0.012)
0.779 (0.013)
0.774 (0.014)
0.751 (0.012)
0.804 (0.010)

5.2 Feature Learning and Speaker Identiﬁcation

We explore PoF’s potential as an unsupervised feature extractor. One way to interpret the PoF model
is that it attempts to represent the data in a latent ﬁlter space. Therefore, given spectra {wt}1:T , we
can use the coordinates in the latent ﬁlter space {at}1:T as features. Since we believe the inter- and
intra-speaker variability is well-captured by the PoF model, we use speaker identiﬁcation to evaluate
the effectiveness of these features.

We compare our learned representation with mel-frequency cepstral coefﬁcients (MFCCs), which
are widely used in various speech and audio processing tasks including speaker identiﬁcation.
MFCCs are computed by taking the discrete cosine transform (DCT) on mel-scale log-spectra and
using only the low-order coefﬁcients. PoF can be understood in similar terms; we are also trying
to explain the variability in log-spectra in terms of a linear combination of dictionary elements.
However, instead of using the ﬁxed, orthogonal DCT basis, PoF learns a ﬁlter space that is tuned to
the statistics of the input. Therefore, it seems reasonable to hope that the coefﬁcients at from the
PoF model, which will be abbreviated as PoFC, might compare favorably with MFCCs as a feature
representation.

We evaluated speaker identiﬁcation under the following scenario: identify different speakers from
recordings where each speaker may start and ﬁnish talking at random time, but at any given time
there is only one speaker speaking (like during a meeting). This is very similar to speaker diarization
[8], but here we assume we know a priori the number of speakers and certain amount of training data
is available for each speaker. Our goal in this experiment was to demonstrate that the PoFC repre-
sentation captures information that is missing or difﬁcult to extract from the MFCC representation,
rather than trying to build a state-of-the-art speaker identiﬁcation system.

We randomly selected 10 speakers (5 males and 5 females) from TIMIT outside the training data
we used to learn the free parameters U, α, and γ. We used the ﬁrst 13 DCT coefﬁcients which
is a standard choice for computing MFCC. We obtained the PoFC by doing posterior inference as
described in Section 4.1 and used Eq[at] as a point estimate summary. For both MFCC and PoFC,
we computed the ﬁrst-order and second-order differences and concatenated them with the original
feature.

We treat speaker identiﬁcation problem as a classiﬁcation problem and make predictions for each
frame. We trained a multi-class (one-vs-the-rest) linear SVM using eight sentences from each
speaker and tested with the remaining two sentences, which gave us about 7800 frames of train-
ing data and 1700 frames of test data. The test data was randomly permuted so that the order in
which sentences appear is random, which simulates the aforementioned scenario.

The frame level accuracy is reported in the ﬁrst row of Table 2. We can see PoFC increases the
accuracy by a large margin (from 49.1% to 60.5%). To make use of temporal information, we used a
simple median ﬁlter smoother with length 25, which boosts the performance for both representations
equally; these results are reported in the second row of Table 2.

8

Table 2: 10-speaker identiﬁcation accuracy using PoFC, MFCC, and combined. The ﬁrst row shows
the raw frame-level test accuracy. The second row shows the result after applying a simple median
ﬁlter with length 25 on the frame-level prediction.

Frame-level
Smoothing

MFCC
49.1% 60.5%
74.2% 85.0%

65.5%
89.5%

PoFC MFCC + PoFC

Although MFCCs and PoFCs capture similar information, concatenating both sets of features yields
better accuracy than that obtained by either feature set alone. The results achieved by combining
the features are summarized in the last column of Table 2, which indicates that MFCCs and PoFCs
capture complementary information. These results, which use a relatively simple frame-level classi-
ﬁer, suggest that PoFC could produce even better accuracy when used in a more sophisticated model
(e.g. a deep neural network).

6 Discussion and Future Work

In this paper, we proposed the product-of-ﬁlters (PoF) model, a generative model which makes sim-
ilar assumptions to those used in the classic homomorphic ﬁltering approach to signal processing.
We derived variational inference and parameter estimation algorithms, and demonstrated experi-
mental improvements on a bandwidth expansion task and showed that PoF can serve as an effective
unsupervised feature extractor for speaker identiﬁcation.

In this paper, we derived PoF as a standalone model. However, it can also be used as a building
block and integrated into a larger model, e.g., as a prior for the dictionary in a probabilistic NMF
model.

Although the optimization in the variational EM algorithm can be parallelized, currently we cannot
ﬁt PoF to large-scale speech data on a single machine. Leveraging recent developments in stochastic
variational inference [9], it would be possible to learn the free parameters from a much larger, more
diverse speech corpus, or even from streams of data.

References

[1] D. Bansal, B. Raj, and P. Smaragdis. Bandwidth expansion of narrowband speech using non-

negative matrix factorization. In INTERSPEECH, pages 1505–1508, 2005.

[2] A. T. Cemgil. Bayesian inference for nonnegative matrix factorisation models. Computational

Intelligence and Neuroscience, 2009.

[3] C. Ding and X. He. K-means clustering via principal component analysis. In Proceedings of

the International Conference on Machine learning, page 29. ACM, 2004.

[4] C. Ding, T. Li, and M. I. Jordan. Convex and semi-nonnegative matrix factorizations. Pattern

Analysis and Machine Intelligence, IEEE Transactions on, 32(1):45–55, 2010.

[5] D. L. Donoho and M. Elad. Optimally sparse representation in general (nonorthogonal) dictio-
naries via ℓ1 minimization. Proceedings of the National Academy of Sciences, 100(5):2197–
2202, 2003.

[6] C. F´evotte, N. Bertin, and J.-L. Durrieu. Nonnegative matrix factorization with the Itakura-
Saito divergence: with application to music analysis. Neural Computation, 21(3):793–830,
Mar. 2009.

[7] W. M. Fisher, G. R. Doddington, and K. M. Goudie-Marshall. The DARPA speech recognition
research database: speciﬁcations and status. In Proc. DARPA Workshop on speech recognition,
pages 93–99, 1986.

[8] E.B. Fox, E.B. Sudderth, M.I. Jordan, and A.S. Willsky. A Sticky HDP-HMM with Application

to Speaker Diarization. Annals of Applied Statistics, 5(2A):1020–1056, 2011.

[9] M. Hoffman, D. Blei, C. Wang, and J. Paisley. Stochastic variational inference. Journal of

Machine Learning Research, 14:1303–1347, 2013.

9

[10] P. O. Hoyer. Non-negative matrix factorization with sparseness constraints. The Journal of

Machine Learning Research, 5:1457–1469, 2004.

[11] Y. Hu and P. C. Loizou. Evaluation of objective quality measures for speech enhancement.
Audio, Speech, and Language Processing, IEEE Transactions on, 16(1):229–238, 2008.

[12] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational

methods for graphical models. Machine learning, 37(2):183–233, 1999.

[13] M. Karjalainen, V. V¨alim¨aki, and Z. J´anosy. Towards high-quality sound synthesis of the guitar
and string instruments. In Proceedings of the International Computer Music Conference, pages
56–56, 1993.

[14] D.D. Lee and H.S. Seung. Algorithms for non-negative matrix factorization. Advances in

Neural Information Processing Systems, 13:556–562, 2001.

[15] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online learning for matrix factorization and sparse

coding. The Journal of Machine Learning Research, 11:19–60, 2010.

[16] A. Oppenheim and R. Schafer. Homomorphic analysis of speech. Audio and Electroacoustics,

IEEE Transactions on, 16(2):221–226, 1968.

[17] D. O’Shaughnessy. Linear predictive coding. Potentials, IEEE, 7(1):29–32, 1988.
[18] B. Raj, R. Singh, M. Shashanka, and P. Smaragdis. Bandwidth expansionwith a P´olya urn
model. In Acoustics, Speech and Signal Processing, IEEE International Conference on, vol-
ume 4, pages IV–597. IEEE, 2007.

[19] P. Smaragdis and J. C. Brown. Non-negative matrix factorization for polyphonic music tran-
scription. In Applications of Signal Processing to Audio and Acoustics, IEEE Workshop on.,
pages 177–180. IEEE, 2003.

[20] D. L. Sun and R. Mazumder. Non-negative matrix completion for bandwidth extension: A
In Machine Learning for Signal Processing (MLSP), IEEE

convex optimization approach.
International Workshop on, pages 1–6. IEEE, 2013.

[21] C. H. Taal, R. C. Hendriks, R. Heusdens, and J. Jensen. An algorithm for intelligibility pre-
diction of time–frequency weighted noisy speech. Audio, Speech, and Language Processing,
IEEE Transactions on, 19(7):2125–2136, 2011.

10

A Variational EM for Product-of-Filters Model

A.1 E-step (Posterior Inference)

Following Section 4.1, the variational lower bound for the E-step (equation 6):
log p(wt|U, α, γ)

= log

q(at)

Zat

≥

Zat

q(at) log

p(wt, at|U, α, γ)
q(at)
p(wt, at|U, α, γ)
q(at)

dat

dat

(12)

t , ρa
t )
The second term is the entropy of a gamma-distributed random variable:

= Eq[log p(wt, at|U, α, γ)] − Eq[log q(at)]
≡ L(ν a

− Eq[log q(at)]
νa
lt − log ρa

=

Xl (cid:16)

lt + log Γ(νa

lt) + (1 − νa

lt)ψ(νa
lt)

(cid:17)

For the ﬁrst term, we can keep the parts which depend on {ν a

t , ρa

t }:

Eq[log p(wt, at|U, α, γ)]

= Eq[log p(wt|at, U, γ)] + Eq[log p(at|α)]

= const +

(αl − 1)Eq[log alt] − αlEq[alt]

−

γf

Wf t

Eq[exp(−Uf lalt)] +

Uf lEq[alt]

Xl n

o

Xf

n

Yl

Xl

o

Take the derivative of L(ν a

t , ρa

t ) with respect to νa

lt and ρa
lt:

∂L
∂νa
lt

=

γf

Wf t log(1 +

(cid:26)

Xf

Uf l
ρa
lt

)

L

Yj=1

Eq[exp(−Uf jajt)] −

+(αl − νa

lt)ψ1(νa

lt) + 1 −

Uf l
ρa
lt (cid:27)

αl
ρa
lt

∂L
∂ρa
lt

=

νlt
(ρa
lt)2

(cid:26)

Xf

γf

Uf l − Wf t(1 +

)−1Uf l

Eq[exp(−Uf jajt)]

+αl(

νlt
lt)2 −
(ρa

1
ρa
lt

)

(cid:27)

Uf l
ρa
lt

L

Yj=1

where ψ1(·) is the trigamma function.

A.2 M-step

The objective function for M-step is:

Q(U, α, γ) =

Eq[log p(wt, at|U, α, γ)]

=

Eq[log p(wt|at, U, γ)] + Eq[log p(at|α)]

(13)

Xt

Xt

where

Eq[log p(wt|at, U, γ)]

Xf (cid:16)
Eq[log p(at|α)]

Xl

=

γf log γf − γf

Uf lEq[alt] − log Γ(γf ) + (γf − 1) log Wf t − Wf tγf

Eq[exp(−Uf lalt)]

Yl

(cid:17)

=

αl log αl − log Γ(αl) + (αl − 1)Eq[log alt] − αlEq[alt]

(cid:17)

Xl (cid:16)

11

Take the derivative with respect to U, α, γ, we obtain the following gradients:

∂Q
∂Uf l

∂Q
∂αl

∂Q
∂γf

=

=

=

Xt (cid:16)

Xt (cid:16)

Xt (cid:16)

− Eq[alt] + Wf tEq[alt](1 +

)−(νa

lt+1)

Eq[exp(−Uf jajt)]

(cid:17)

Uf l
ρa
lt

log αl + 1 − ψ(αl) + Eq[log alt] − Eq[alt]

Yj6=l

(cid:17)

log γf −

Uf lEq[alt] + 1 − ψ(γf ) + log Wf t − Wf t

Eq[exp(−Uf lalt)]

Xl

Yl

(14)

(15)

(cid:17)
(16)

Note that the optimization problem for U is independent for different frequency bin f ∈
{1, 2, · · · , F }, as reﬂected by the gradient.

12

4
1
0
2
 
v
o
N
5
2
 
 
]
L
M

 

.
t
a
t
s
[
 
 
5
v
7
5
8
5
.
2
1
3
1
:
v
i
X
r
a

A Generative Product-of-Filters Model of Audio

Dawen Liang∗
Columbia University
New York, NY 10027
dliang@ee.columbia.edu

Matthew D. Hoffman
Adobe Research
San Francisco, CA 94103
mathoffm@adobe.com

Gautham J. Mysore
Adobe Research
San Francisco, CA 94103
gmysore@adobe.com

Abstract

We propose the product-of-ﬁlters (PoF) model, a generative model that decom-
poses audio spectra as sparse linear combinations of “ﬁlters” in the log-spectral
domain. PoF makes similar assumptions to those used in the classic homomorphic
ﬁltering approach to signal processing, but replaces decompositions built of basic
signal processing operations with a learned decomposition based on statistical in-
ference. This paper formulates the PoF model and derives a mean-ﬁeld method
for posterior inference and a variational EM algorithm to estimate the model’s free
parameters. We demonstrate PoF’s potential for audio processing on a bandwidth
expansion task, and show that PoF can serve as an effective unsupervised feature
extractor for a speaker identiﬁcation task.

1 Introduction

Some of the most successful approaches to audio signal processing of the last ﬁfty years have been
based on decomposing complicated systems into an excitation signal and some number of simpler
linear systems. One of the simplest (and most widely used) examples is linear predictive coding
(LPC), which uses a simple autoregressive model to decompose an audio signal into an excitation
signal and linear ﬁlter [17]. More broadly, homomorphic ﬁltering methods such as cepstral analysis
[16] try to decompose complicated linear systems into a set of simpler linear systems that can then
be analyzed, interpreted, and manipulated independently.

One reason that this broad approach has been successful is because it is consistent with the way
many real-world objects actually generate sound. An important example is the human voice: human
vocal sounds are generated by running vibrations generated by the vocal folds through the rest of
the vocal tract (tongue, lips, jaw, etc.), which approximately linearly ﬁlters the vibrations that come
from the larynx or lungs.

Traditional approaches typically rely on hand-designed decompositions built of basic operations
such as Fourier transforms, discrete cosine transforms, and least-squares solvers. In this paper we
take a more data-driven approach, and derive a generative product-of-ﬁlters (PoF) model that learns
a statistical-inference-based decomposition that is tuned to be appropriate to the data being ana-
lyzed. Like traditional homomorphic ﬁltering approaches, PoF decomposes audio spectra as linear
combinations of ﬁlters in the log-spectral domain. Unlike previous approaches, these ﬁlters are
learned from data rather than selected from convenient families such as orthogonal cosines, and the
PoF model learns a sparsity-inducing prior that prefers decompositions that use relatively few ﬁlters
to explain each observed spectrum. The result when applied to speech data is that PoF discovers
some ﬁlters that model excitation signals and some that model the various ﬁltering operations that
the vocal tract can perform. Given a set of these learned ﬁlters, PoF can infer how much each ﬁl-
ter contributed to a given audio magnitude spectrum, resulting in a sparse, compact, interpretable
representation.

∗This work was performed while Dawen Liang was an intern at Adobe Research.

1

The rest of the paper proceeds as follows. First, we formally introduce the product-of-ﬁlters (PoF)
model, and give more rigorous intuitions about the assumptions that it makes. Next, we review
some previous work and show how it relates to the PoF model. Then, we derive a mean-ﬁeld
variational inference algorithm that allows us to do approximate posterior inference in PoF, as well
as a variational EM algorithm that ﬁts the model’s free parameters to data. Finally, we demonstrate
PoF’s potential for audio processing on a bandwidth expansion task, where it achieves better results
than a recently proposed technique based on non-negative matrix factorization (NMF). We also
evaluate PoF as an unsupervised feature extractor, and ﬁnd that the representation learned by the
model yields higher accuracy on a speaker identiﬁcation task than the widely used mel-frequency
cepstral coefﬁcient (MFCC) representation.

2 Product-of-Filters Model

We are interested in modeling audio spectrograms, which are collections of Fourier magnitude spec-
tra W taken from some set of audio signals, where W is an F × T non-negative matrix; the cell
Wf t gives the magnitude of the audio signal at frequency bin f and time window (often called a
frame) t. Each column of W is the magnitude of the fast Fourier transform (FFT) of a short window
of an audio signal, within which the spectral characteristics of the signal are assumed to be roughly
stationary.

The motivation for our model comes from the widely used homomorphic ﬁltering approach to audio
and speech signal processing [16], where a short window of audio w[n] is modeled as a convolution
between an excitation signal e[n] (which might come from a speaker’s vocal folds) and the impulse
response h[n] of a series of linear ﬁlters (such as might be implemented by a speaker’s vocal tract):

In the spectral domain after taking the FFT, this is equivalent to:

w[n] = (e ∗ h)[n]

|W[k]| = |E[k]| ◦ |H[k]| = exp{log |E[k]| + log |H[k]|}

where ◦ denotes element-wise multiplication and | · | denotes the magnitude of a complex value pro-
duced by the FFT. Thus, the convolution between these two signals corresponds to a simple addition
of their log-spectra. Another attractive feature is the symmetry between the excitation signal e[n]
and the impulse response h[n] of the vocal-tract ﬁlter—convolution commutes, so mathematically
(if not physiologically) the vocal tract could just as well be exciting the “ﬁlter” implemented by
vocal folds.

We will likewise model the observed magnitude spectra as a product of ﬁlters. We assume each
observed log-spectrum is approximately obtained by linearly combining elements from a pool of L
log-ﬁlters1 U ≡ [u1|u2| · · · |uL] ∈ RF ×L:

log Wf t ≈

lUf lalt,

P

where alt denotes the activation of ﬁlter ul in frame t. We will impose some sparsity on the activa-
tions, to allow us to encode the intuition that not all ﬁlters are always active. This assumption ex-
pands on the expressive power of the simple excitation-ﬁlter model of equation 1; we could recover
that model by partitioning the ﬁlters into “excitations” and “vocal tracts”, requiring that exactly one
“excitation ﬁlter” be active in each frame, and combining the weighted effects of all “vocal tract
ﬁlters” into a single ﬁlter.

We have two main reasons for relaxing the classic excitation-ﬁlter model to include more than two
ﬁlters, one computational and one statistical. The statistical rationale is that the parameters that
deﬁne the human voice (pitch, tongue position, etc.) are inherently continuous, and so a very large
dictionary of excitations and ﬁlters might be necessary to explain observed inter- and intra-speaker
variability with the classic model. The computational rationale is that clustering models (which
might try to determine which excitation is active) can be more fraught with local optima than fac-
torial models such as ours (which tries to determine how active each ﬁlter is), and there is some
precedent for relaxing clustering models into factorial models [3].

1We will use the term “ﬁlter” when referring to U for the rest of the paper.

(1)

(2)

(3)

2

Formally, we deﬁne the product-of-ﬁlters model:

alt ∼ Gamma(αl, αl)

Wf t ∼ Gamma

γf , γf / exp(

(cid:16)

lUf lalt)
(cid:17)

P

where γf is the frequency-dependent noise level. We restrict the activations at (but not the ﬁlters
ul) to be non-negative; if we allowed negative alt, then the the ﬁlters would be inverted, reducing
the interpretability of the model.

Under this model

(4)

(5)

E[alt] = 1
E[Wf t] = exp(

lUf lalt).

P

For l ∈ {1, 2, · · · , L}, αl controls the sparseness of the activations associated with ﬁlter ul; smaller
values of αl indicate that ﬁlter ul is used more rarely. From a generative point of view, one can view
the model as ﬁrst drawing activations alt from a sparse prior, then applying multiplicative gamma
l Uf lalt). A graphical model representation
noise with expected value 1 to the expected value exp(
of the PoF model is shown in Figure 1.

P

wt

T

U, γ

at

α

Figure 1: Graphical model representation of the PoF model.

In this paper we focus on speech applications, but the homomorphic ﬁltering approach has also been
successfully applied to model other kinds of sounds such as musical instruments. For example, [13]
treat the effect of the random excitation, string, and body as a chain of linear systems, which can
therefore be modeled as a product of ﬁlters.

3 Related Work

The PoF model can be interpreted as a matrix factorization model, where we are trying to decompose
the log-spectrogram. A closely related model is non-negative matrix factorization (NMF) [14] and
its variations, e.g., NMF with sparseness constrains [10], convex NMF [4], and fully Bayesian NMF
[2]. In NMF, a F × T non-negative matrix W is approximately decomposed into the product of
two non-negative matrices: a F × K matrix V (often called the dictionary) and a K × T matrix H
(often called the activations). NMF is widely used to analyze audio spectrograms [6, 19], largely
due to its additivity property and the parts-based representation that it induces. It also often provides
a semantically meaningful interpretation. For example, given the NMF decomposition of a piano
sonata, the components in the dictionary are likely to correspond to notes with different pitches, and
the activations will indicate when and how strongly each note is played. NMF’s ability to isolate
energy coming from different sources in mixed recordings has made it a popular tool for addressing
source separation problems.

Although both models decompose audio spectra using a linear combination of dictionary elements,
NMF and PoF make fundamentally different modeling assumptions. NMF models each frame of a
spectrogram as an additive combination of dictionary elements, which approximately corresponds
to modeling the corresponding time-domain signal as a summation of parts. On the other hand, PoF
models each frame of the spectrogram as a product of ﬁlters (sum of log-ﬁlters), which corresponds
to modeling the corresponding time-domain signal as a convolution of ﬁlters. NMF is well suited to
decomposing polyphonic sounds into mixtures of independent sources, whereas PoF is well suited
to decomposing monophonic sounds into simpler systems.

In the compressive sensing literature, there has been a great deal of work on matrix factorization
and dictionary learning by solving an optimization problem with sparseness constraints—adding
ℓ1 norm penalty as a convex relaxation of ℓ0 norm penalty [5]. Online algorithms have also been

3

proposed to handle large data sets [15].
In principle, we could have formulated the PoF model
similarly, using an ℓ1 penalty and convex optimization in place of a gamma prior and Bayesian
inference. However, in such a formulation it is unclear how we might ﬁt the L hyperparameters α
controlling the amount of sparsity in the activations associated with each ﬁlter. We found that PoF
best explains speech training data when each ﬁlter ul has its own sparsity hyperparameter αl, and
performing cross-validation to select so many hyperparameters would be impractical.

4 Inference and Parameter Estimation

From Figure 1, we can see that there are two computational problems that will arise when using the
PoF model. First, given ﬁxed U, α, and γ and input spectrum wt, we must (approximately) compute
the posterior distribution p(at|wt, U, α, γ). This will enable us to ﬁt the PoF model to unseen data
and obtain a different representation in the latent ﬁlter space. Second, given a collection of training
spectra W = {wt}1:T , we want to ﬁnd the maximum likelihood estimates of the free parameters U,
α, and γ. In this section, we will tackle these two problems respectively. The detailed derivations
can be found in the appendix. The source code in Python is available on Github2.

4.1 Mean-Field Posterior Inference

The posterior p(at|wt, U, α, γ) is intractable to compute due to the nonconjugacy of the model.
Therefore, we employ mean-ﬁeld variational inference [12].

Variational inference is a deterministic alternative to Markov Chain Monte Carlo (MCMC) meth-
ods. The basic idea behind variational inference is to choose a tractable family of variational dis-
tributions q(at) to approximate the intractable posterior p(at|wt, U, α, γ), so that the Kullback-
Leibler (KL) divergence between the variational distribution and the true posterior KL(qakpa|W) is
minimized. In particular, we are using the mean-ﬁeld family which is completely factorized, i.e.,
q(at) =
l q(alt). For each alt, we choose a variational distribution from the same family as alt’s
prior distribution: q(alt) = Gamma(alt; νa
t and ρa
t are free parameters that we will tune to
minimize the KL divergence between q and the posterior.

lt). ν a

lt, ρa

Q

We can lower bound the marginal likelihood of the input spectrum wt:

log p(wt|U, α, γ)

≥ Eq[log p(wt, at|U, α, γ)] − Eq[log q(at)]

△
= L(ν a

t , ρa
t )

lt/ρa
To compute the variational lower bound L(ν a
lt
and Eq[log alt] = ψ(νa
lt, where ψ(·) is the digamma function, are both easy to compute.
For Eq[exp(−Uf lalt)], we seek for the moment-generating function of a gamma-distributed random
variable and obtain the expectation as:

t ), the necessary expectations Eq[alt] = νa

lt) − log ρa

t , ρa

Eq[exp(−Uf lalt)] =

−νa
lt

1 + Uf l
ρa
lt (cid:17)

(cid:16)

for Uf l > −ρa

lt, and +∞ otherwise3.

The nonconjugacy and the exponents in the likelihood model preclude optimizing the lower bound
by traditional closed-form coordinate ascent updates. Instead, we compute the gradient of L(ν a
t , ρa
t )
with respect to variational parameters ν a
t and use Limited-memory BFGS (L-BFGS) to opti-
mize the variational lower bound, which guarantees to ﬁnd a local optimum and optimal variational
parameters { ˆν a

t and ρa

t , ˆρa

t }.

Note that in the posterior inference, the optimization problem is independent for different frame t.
Therefore, given input spectra {wt}1:T , we can break down the whole problem into T independent
sub-problems which can be solved in parallel.

2https://github.com/dawenl/pof
3Technically the expectation for Uf l ≤ −ρa

the variational lower bound goes to −∞ and the optimization can be carried out seamlessly.

lt is undeﬁned. Here we treat it as +∞ so that when Uf l ≤ −ρa
lt

(6)

(7)

4

4.2 Parameter Estimation

Given a collection of training audio spectra {wt}1:T , we carry out parameter estimation for the PoF
model by ﬁnding the maximum likelihood estimates of the free parameters U, α, and γ, approxi-
mately marginalizing out at.

We formally deﬁne the parameter estimation problem as

ˆU, ˆα, ˆγ = arg max

log p(wt|U, α, γ)

U,α,γ Xt

(8)

= arg max

log

U,α,γ Xt

Zat

p(wt, at|U, α, γ)dat

This problem can be solved by variational Expectation-Maximization (EM) which maximizes a
lower bound on marginal likelihood in equation 6 with respect to the variational parameters, and
then for the ﬁxed values of variational parameters, maximizes the lower bound with respect to the
model’s free parameters U, α, and γ.

E-step For each wt where t = 1, 2, · · · , T , perform posterior inference by optimizing the values
of the variational parameters { ˆν a

t }. This is done as described in Section 4.1.

t , ˆρa

M-step Maximize the variational lower bound in equation 6, which is equivalent to maximizing
the following objective:

Q(U, α, γ) =

Eq[log p(wt, at|U, α, γ)]

(9)

t

P

This is accomplished by ﬁnding the maximum likelihood estimates using the expected sufﬁcient
statistics for each at that were computed in the E-step. There are no simple closed-form updates for
the M-step. Therefore, we compute the gradient of Q(U, α, γ) with respect to U, α, γ, respectively,
and use L-BFGS to optimize the bound in equation 9.
The most time-consuming part for M-step is updating U, which is a F × L matrix. However,
note that the optimization problem is independent for different frequency bins f ∈ {1, 2, · · · , F }.
Therefore, we can update U by optimizing each row independently, and in parallel if desired.

5 Evaluation

We conducted experiments to assess the PoF model on two different tasks. We evaluate PoF’s ability
to infer missing data in the bandwidth expansion task. We also explore the potential of the PoF model
as an unsupervised feature extractor for the speaker identiﬁcation task.
Both tasks require pre-trained parameters U, α, and γ, which we learned from the TIMIT Speech
Corpus [7]. It contains speech sampled at 16000 Hz from 630 speakers of eight major dialects of
American English, each reading ten phonetically rich sentences. The parameters were learned from
20 randomly selected speakers (10 males and 10 females). We performed a 1024-point FFT with
Hann window and 50% overlap, thus the number of frequency bins was F = 513. We performed
the experiments on magnitude spectrograms except where speciﬁed otherwise.

We tried different model orders L ∈ {10, 20, · · · , 80} and evaluated the lower bound on the marginal
likelihood log p(wt|U, α, γ) in equation 6. In general, larger L will give us a larger variational
lower bound and will be slower to train. In our experiments, we set L = 50 as a compromise
between performance and computational efﬁciency. We initialized all the variational parameters ν a
t
and ρa
t with random draws from a gamma distribution with shape parameter 100 and inverse-scale
parameter 100. This yields a diffuse and smooth initial variational posterior, which helped avoid bad
local optima. We ran variational EM until the variational lower bound increased by less than 0.01%.

Figure 2 demonstrates some representative ﬁlters learned from the PoF model with L = 50. The six
ﬁlters ul associated with the largest values of αl are shown in Figure 2a, and the six ﬁlters associated
with the smallest values of αl are shown in Figure 2b. Small values of αl indicate a prior preference
to use the associated ﬁlters less frequently, since the Gamma(αl, αl) prior places more mass near
0 when αl is smaller. The ﬁlters in Figure 2b, which are used relatively rarely, tend to have the

5

)
B
d
(
 
e
d
u
t
i
n
g
a
M

10
5
0
−5
−10
−15
−20
−25

)
B
d
(
 
e
d
u
t
i
n
g
a
M

20
15
10
5
0
−5
−10
−15
−20

)
B
d
(
 
e
d
u
t
i
n
g
a
M

40

20

0

−20

−40

)
B
d
(
 
e
d
u
t
i
n
g
a
M

30
20
10
0
−10
−20
−30

or

α = 1.68

α = 1.37

α = 1.06

6
4
2
0
−2
−4
−6
−8
−10
−12

15
10
5
0
−5
−10
−15
−20

60
40
20
0
−20
−40
−60

30
20
10
0
−10
−20
−30

0

2

6

8

0

2

6

8

0

2

6

8

4
Frequency (kHz)

α = 0.96

4
Frequency (kHz)

α = 0.94

4
Frequency (kHz)

α = 0.89

0

2

6

8

0

2

6

8

0

2

6

8

4
Frequency (kHz)

4
Frequency (kHz)

4
Frequency (kHz)

(a) The top 6 ﬁlters ul with the largest αl values (shown above each plot).

α = 0.01

α = 0.01

α = 0.02

0

2

6

8

0

2

6

8

0

2

6

8

4
Frequency (kHz)

α = 0.02

4
Frequency (kHz)

α = 0.02

4
Frequency (kHz)

α = 0.02

0

2

6

8

0

2

6

8

0

2

6

8

4
Frequency (kHz)

4
Frequency (kHz)

4
Frequency (kHz)

(b) The top 6 ﬁlters ul with the smallest αl values (shown above each plot).

Figure 2: The representative ﬁlters learned from the PoF model with L = 50.

10
5
0
−5
−10
−15
−20
−25

25
20
15
10
5
0
−5
−10
−15
−20

25
20
15
10
5
0
−5
−10
−15
−20

25
20
15
10
5
0
−5
−10
−15
−20

strong harmonic structure displayed by the log-spectra of periodic signals, while the ﬁlters in Figure
2a tend to vary more smoothly, suggesting that they are being used to model the ﬁltering induced
by the vocal tract. The periodic “excitation” ﬁlters tend to be used more rarely, which is consistent
with the fact that normally there is not more than one excitation signal contributing to a speaker’s
voice. (Very few people can speak or sing more than one pitch simultaneously.) The model has more
freedom to use several of the coarser “vocal tract” ﬁlters per spectrum, which is consistent with the
fact that several aspects of the vocal tract may be combined to ﬁlter the excitation signal generated
by a speaker’s vocal folds.

Despite the non-convexity inherent to all dictionary-learning problems, which causes the details
of the ﬁlters vary from run to run, training with multiple random restarts and different speakers
produced little impact on the ﬁlters that the PoF model learned; in all cases with different model
order L, we found the same “excitation/ﬁlter” structure, similar to what is shown in Figure 2.

5.1 Bandwidth Expansion

We demonstrate PoF’s potential in audio processing applications on a bandwidth expansion task,
where the goal is to infer the contents of a full-bandwidth signal given only the contents of a band-
limited version of that signal. Bandwidth expansion has applications to restoration of low-quality
audio such as might be recorded from a telephone or cheap microphone.
Given the parameters U, α, and γ ﬁt to full-bandwidth training data, we can treat the bandwidth
expansion problem as a missing data problem. Given spectra from a band-limited recording Wbl =
{wbl
t ) over the activations at associated
with the band-limited signal, for t = {1, 2, · · · , T }. We can approximate this posterior using the
variational inference algorithm from Section 4.1 by only using the band-limited part of U and γ.
Then we can reconstruct the full-bandwidth spectra by combining the inferred {at}1:T with the
full-bandwidth U. Following the model formulation in equation 4, we might either estimate the
full-bandwidth spectra using

t }1:T , the model implies a posterior distribution p(at|wbl

Eq[W fb

f t] =

Eq[exp(Uf lalt)]

Eq[W fb

f t] = exp{

lUf l · Eq[alt]}.

l

Q

P

6

(10)

(11)

We use equation 11, both because it is more stable and because human auditory perception is log-
arithmic; if we are summarizing the posterior distribution with a point estimate, the expectation on
the log-spectral domain is more perceptually natural.

As a comparison, NMF is widely used for bandwidth expansion [1, 18, 20]. The full-bandwidth
training spectra Wtrain, which are also used to learn the parameters U, α, and γ for the PoF model,
are decomposed by NMF as Wtrain ≈ VH, where V is the dictionary and H is the activation. Then
given the band-limited spectra Wbl, we can use the band-limited part of V to infer the activation
Hbl. Finally, we can reconstruct the full-bandwidth spectra by computing VHbl.

Based on how the loss function is deﬁned, there can be different types of NMF models: KL-NMF
[14] which is based on Kullback-Leibler divergence, and IS-NMF [6] which is based on Itakura-
Saito divergence, are among the most commonly used NMF decomposition models in audio signal
processing. We compare the PoF model with both KL-NMF and IS-NMF with different model
orders K = 25, 50, and 100. We used the standard multiplicative updates for NMF and stopped the
iterations when the decrease in the cost function was less than 0.01%. For IS-NMF, we used power
spectra instead of magnitude spectra, since the power spectrum representation is more consistent
with the statistical assumptions that underlie the Itakura-Saito divergence.

We randomly selected 10 speakers (5 male and 5 female) from TIMIT that do not overlap with the
speakers used to ﬁt the model parameters U, α, and γ, and took 3 sentences from each speaker
as test data. We cut off all the contents below 400 Hz and above 3400 Hz to obtain band-limited
recordings of approximately telephone-quality speech.

In previous NMF-based bandwidth expansion work [1, 18, 20], all experiments are done in a speaker-
dependent setting, which means the model is trained from the target speaker. What we are doing
here, on the other hand, is speaker-independent: we use no prior knowledge about the speciﬁc
speaker whose speech is being restored4. To our knowledge, little if any work has been done on
speaker-independent bandwidth expansion based on NMF decompositions.

To evaluate the quality of the reconstructed recordings, we used the composite objective measure
[11] and short-time objective intelligibility [21] metrics. These metrics measure different aspects of
the “distance” between the reconstructed speech and the original speech. The composite objective
measure (will be abbreviated as OVRL, as it reﬂects the overall sound quality) was originally pro-
posed as a quality measure for speech enhancement. It aggregates different basic objective measures
and has been shown to correlate with humans’ perceptions of audio quality. OVRL is based on the
predicted perceptual auditory rating and is in the range of 1 to 5 (1: bad; 2: poor; 3: fair; 4: good; 5:
excellent). The short-time objective intelligibility measure (STOI) is a function of the clean speech
and reconstructed speech, which correlates with the intelligibility of the reconstructed speech, that
is, it predicts the ability of listeners to understand what words are being spoken rather than per-
ceived sound quality. STOI is computed as the average correlation coefﬁcient from 15 one-third
octave bands across frames, thus theoretically should be in the range of -1 to 1, where larger values
indicate higher expected intelligibility. However, in practice, even when we ﬁlled out the missing
contents with random noise, the STOI is 0.306 ± 0.016, which can be interpreted as a practical
lower bound on the test data.

The average OVRL and STOI with two standard errors5 across 30 sentences for different methods,
along with these from the band-limited input speech as baseline, are reported in Table 1. We can see
that NMF improves STOI by a small amount, and PoF improves it slightly more, but the improve-
ment in both cases is fairly small. This may be because the band-limited input speech already has
a relatively high STOI (telephone-quality speech is fairly intelligible). On the other hand, PoF pro-
duces better predicted perceived sound quality as measured by OVRL than KL-NMF and IS-NMF
by a large margin regardless of the model order K, improving the sound quality from poor-to-fair
(2.71 OVRL) to fair-to-good (3.25 OVRL).

4When we conducted speaker-dependent experiments, both PoF and NMF produced nearly ceiling-level
results. Thus we only report results on the harder and more practically relevant speaker-independent problem.

5For both OVRL and STOI, we used the MATLAB implementation from the original authors.

7

Table 1: Average OVRL (composite objective measure) and STOI (short-time objective intelligibil-
ity) score with two standard errors (in parenthesis) for the bandwidth expansion task from different
methods. OVRL is in the range of 1 to 5 [1: bad; 2: poor; 3: fair; 4: good; 5: excellent]. STOI is
the average correlation coefﬁcient, thus theoretically should be in the range of -1 to 1, where larger
values indicate higher expected intelligibility.

KL-NMF

Band-limited input
K=25
K=50
K=100
K=25
K=50
K=100

IS-NMF

PoF

OVRL
1.72 (0.16)
2.60 (0.12)
2.71 (0.14)
2.41 (0.10)
2.43 (0.15)
2.62 (0.12)
2.15 (0.10)
3.25 (0.13)

STOI
0.762 (0.012)
0.786 (0.013)
0.790 (0.013)
0.759 (0.012)
0.779 (0.013)
0.774 (0.014)
0.751 (0.012)
0.804 (0.010)

5.2 Feature Learning and Speaker Identiﬁcation

We explore PoF’s potential as an unsupervised feature extractor. One way to interpret the PoF model
is that it attempts to represent the data in a latent ﬁlter space. Therefore, given spectra {wt}1:T , we
can use the coordinates in the latent ﬁlter space {at}1:T as features. Since we believe the inter- and
intra-speaker variability is well-captured by the PoF model, we use speaker identiﬁcation to evaluate
the effectiveness of these features.

We compare our learned representation with mel-frequency cepstral coefﬁcients (MFCCs), which
are widely used in various speech and audio processing tasks including speaker identiﬁcation.
MFCCs are computed by taking the discrete cosine transform (DCT) on mel-scale log-spectra and
using only the low-order coefﬁcients. PoF can be understood in similar terms; we are also trying
to explain the variability in log-spectra in terms of a linear combination of dictionary elements.
However, instead of using the ﬁxed, orthogonal DCT basis, PoF learns a ﬁlter space that is tuned to
the statistics of the input. Therefore, it seems reasonable to hope that the coefﬁcients at from the
PoF model, which will be abbreviated as PoFC, might compare favorably with MFCCs as a feature
representation.

We evaluated speaker identiﬁcation under the following scenario: identify different speakers from
recordings where each speaker may start and ﬁnish talking at random time, but at any given time
there is only one speaker speaking (like during a meeting). This is very similar to speaker diarization
[8], but here we assume we know a priori the number of speakers and certain amount of training data
is available for each speaker. Our goal in this experiment was to demonstrate that the PoFC repre-
sentation captures information that is missing or difﬁcult to extract from the MFCC representation,
rather than trying to build a state-of-the-art speaker identiﬁcation system.

We randomly selected 10 speakers (5 males and 5 females) from TIMIT outside the training data
we used to learn the free parameters U, α, and γ. We used the ﬁrst 13 DCT coefﬁcients which
is a standard choice for computing MFCC. We obtained the PoFC by doing posterior inference as
described in Section 4.1 and used Eq[at] as a point estimate summary. For both MFCC and PoFC,
we computed the ﬁrst-order and second-order differences and concatenated them with the original
feature.

We treat speaker identiﬁcation problem as a classiﬁcation problem and make predictions for each
frame. We trained a multi-class (one-vs-the-rest) linear SVM using eight sentences from each
speaker and tested with the remaining two sentences, which gave us about 7800 frames of train-
ing data and 1700 frames of test data. The test data was randomly permuted so that the order in
which sentences appear is random, which simulates the aforementioned scenario.

The frame level accuracy is reported in the ﬁrst row of Table 2. We can see PoFC increases the
accuracy by a large margin (from 49.1% to 60.5%). To make use of temporal information, we used a
simple median ﬁlter smoother with length 25, which boosts the performance for both representations
equally; these results are reported in the second row of Table 2.

8

Table 2: 10-speaker identiﬁcation accuracy using PoFC, MFCC, and combined. The ﬁrst row shows
the raw frame-level test accuracy. The second row shows the result after applying a simple median
ﬁlter with length 25 on the frame-level prediction.

Frame-level
Smoothing

MFCC
49.1% 60.5%
74.2% 85.0%

65.5%
89.5%

PoFC MFCC + PoFC

Although MFCCs and PoFCs capture similar information, concatenating both sets of features yields
better accuracy than that obtained by either feature set alone. The results achieved by combining
the features are summarized in the last column of Table 2, which indicates that MFCCs and PoFCs
capture complementary information. These results, which use a relatively simple frame-level classi-
ﬁer, suggest that PoFC could produce even better accuracy when used in a more sophisticated model
(e.g. a deep neural network).

6 Discussion and Future Work

In this paper, we proposed the product-of-ﬁlters (PoF) model, a generative model which makes sim-
ilar assumptions to those used in the classic homomorphic ﬁltering approach to signal processing.
We derived variational inference and parameter estimation algorithms, and demonstrated experi-
mental improvements on a bandwidth expansion task and showed that PoF can serve as an effective
unsupervised feature extractor for speaker identiﬁcation.

In this paper, we derived PoF as a standalone model. However, it can also be used as a building
block and integrated into a larger model, e.g., as a prior for the dictionary in a probabilistic NMF
model.

Although the optimization in the variational EM algorithm can be parallelized, currently we cannot
ﬁt PoF to large-scale speech data on a single machine. Leveraging recent developments in stochastic
variational inference [9], it would be possible to learn the free parameters from a much larger, more
diverse speech corpus, or even from streams of data.

References

[1] D. Bansal, B. Raj, and P. Smaragdis. Bandwidth expansion of narrowband speech using non-

negative matrix factorization. In INTERSPEECH, pages 1505–1508, 2005.

[2] A. T. Cemgil. Bayesian inference for nonnegative matrix factorisation models. Computational

Intelligence and Neuroscience, 2009.

[3] C. Ding and X. He. K-means clustering via principal component analysis. In Proceedings of

the International Conference on Machine learning, page 29. ACM, 2004.

[4] C. Ding, T. Li, and M. I. Jordan. Convex and semi-nonnegative matrix factorizations. Pattern

Analysis and Machine Intelligence, IEEE Transactions on, 32(1):45–55, 2010.

[5] D. L. Donoho and M. Elad. Optimally sparse representation in general (nonorthogonal) dictio-
naries via ℓ1 minimization. Proceedings of the National Academy of Sciences, 100(5):2197–
2202, 2003.

[6] C. F´evotte, N. Bertin, and J.-L. Durrieu. Nonnegative matrix factorization with the Itakura-
Saito divergence: with application to music analysis. Neural Computation, 21(3):793–830,
Mar. 2009.

[7] W. M. Fisher, G. R. Doddington, and K. M. Goudie-Marshall. The DARPA speech recognition
research database: speciﬁcations and status. In Proc. DARPA Workshop on speech recognition,
pages 93–99, 1986.

[8] E.B. Fox, E.B. Sudderth, M.I. Jordan, and A.S. Willsky. A Sticky HDP-HMM with Application

to Speaker Diarization. Annals of Applied Statistics, 5(2A):1020–1056, 2011.

[9] M. Hoffman, D. Blei, C. Wang, and J. Paisley. Stochastic variational inference. Journal of

Machine Learning Research, 14:1303–1347, 2013.

9

[10] P. O. Hoyer. Non-negative matrix factorization with sparseness constraints. The Journal of

Machine Learning Research, 5:1457–1469, 2004.

[11] Y. Hu and P. C. Loizou. Evaluation of objective quality measures for speech enhancement.
Audio, Speech, and Language Processing, IEEE Transactions on, 16(1):229–238, 2008.

[12] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational

methods for graphical models. Machine learning, 37(2):183–233, 1999.

[13] M. Karjalainen, V. V¨alim¨aki, and Z. J´anosy. Towards high-quality sound synthesis of the guitar
and string instruments. In Proceedings of the International Computer Music Conference, pages
56–56, 1993.

[14] D.D. Lee and H.S. Seung. Algorithms for non-negative matrix factorization. Advances in

Neural Information Processing Systems, 13:556–562, 2001.

[15] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online learning for matrix factorization and sparse

coding. The Journal of Machine Learning Research, 11:19–60, 2010.

[16] A. Oppenheim and R. Schafer. Homomorphic analysis of speech. Audio and Electroacoustics,

IEEE Transactions on, 16(2):221–226, 1968.

[17] D. O’Shaughnessy. Linear predictive coding. Potentials, IEEE, 7(1):29–32, 1988.
[18] B. Raj, R. Singh, M. Shashanka, and P. Smaragdis. Bandwidth expansionwith a P´olya urn
model. In Acoustics, Speech and Signal Processing, IEEE International Conference on, vol-
ume 4, pages IV–597. IEEE, 2007.

[19] P. Smaragdis and J. C. Brown. Non-negative matrix factorization for polyphonic music tran-
scription. In Applications of Signal Processing to Audio and Acoustics, IEEE Workshop on.,
pages 177–180. IEEE, 2003.

[20] D. L. Sun and R. Mazumder. Non-negative matrix completion for bandwidth extension: A
In Machine Learning for Signal Processing (MLSP), IEEE

convex optimization approach.
International Workshop on, pages 1–6. IEEE, 2013.

[21] C. H. Taal, R. C. Hendriks, R. Heusdens, and J. Jensen. An algorithm for intelligibility pre-
diction of time–frequency weighted noisy speech. Audio, Speech, and Language Processing,
IEEE Transactions on, 19(7):2125–2136, 2011.

10

A Variational EM for Product-of-Filters Model

A.1 E-step (Posterior Inference)

Following Section 4.1, the variational lower bound for the E-step (equation 6):
log p(wt|U, α, γ)

= log

q(at)

Zat

≥

Zat

q(at) log

p(wt, at|U, α, γ)
q(at)
p(wt, at|U, α, γ)
q(at)

dat

dat

(12)

t , ρa
t )
The second term is the entropy of a gamma-distributed random variable:

= Eq[log p(wt, at|U, α, γ)] − Eq[log q(at)]
≡ L(ν a

− Eq[log q(at)]
νa
lt − log ρa

=

Xl (cid:16)

lt + log Γ(νa

lt) + (1 − νa

lt)ψ(νa
lt)

(cid:17)

For the ﬁrst term, we can keep the parts which depend on {ν a

t , ρa

t }:

Eq[log p(wt, at|U, α, γ)]

= Eq[log p(wt|at, U, γ)] + Eq[log p(at|α)]

= const +

(αl − 1)Eq[log alt] − αlEq[alt]

−

γf

Wf t

Eq[exp(−Uf lalt)] +

Uf lEq[alt]

Xl n

o

Xf

n

Yl

Xl

o

Take the derivative of L(ν a

t , ρa

t ) with respect to νa

lt and ρa
lt:

∂L
∂νa
lt

=

γf

Wf t log(1 +

(cid:26)

Xf

Uf l
ρa
lt

)

L

Yj=1

Eq[exp(−Uf jajt)] −

+(αl − νa

lt)ψ1(νa

lt) + 1 −

Uf l
ρa
lt (cid:27)

αl
ρa
lt

∂L
∂ρa
lt

=

νlt
(ρa
lt)2

(cid:26)

Xf

γf

Uf l − Wf t(1 +

)−1Uf l

Eq[exp(−Uf jajt)]

+αl(

νlt
lt)2 −
(ρa

1
ρa
lt

)

(cid:27)

Uf l
ρa
lt

L

Yj=1

where ψ1(·) is the trigamma function.

A.2 M-step

The objective function for M-step is:

Q(U, α, γ) =

Eq[log p(wt, at|U, α, γ)]

=

Eq[log p(wt|at, U, γ)] + Eq[log p(at|α)]

(13)

Xt

Xt

where

Eq[log p(wt|at, U, γ)]

Xf (cid:16)
Eq[log p(at|α)]

Xl

=

γf log γf − γf

Uf lEq[alt] − log Γ(γf ) + (γf − 1) log Wf t − Wf tγf

Eq[exp(−Uf lalt)]

Yl

(cid:17)

=

αl log αl − log Γ(αl) + (αl − 1)Eq[log alt] − αlEq[alt]

(cid:17)

Xl (cid:16)

11

Take the derivative with respect to U, α, γ, we obtain the following gradients:

∂Q
∂Uf l

∂Q
∂αl

∂Q
∂γf

=

=

=

Xt (cid:16)

Xt (cid:16)

Xt (cid:16)

− Eq[alt] + Wf tEq[alt](1 +

)−(νa

lt+1)

Eq[exp(−Uf jajt)]

(cid:17)

Uf l
ρa
lt

log αl + 1 − ψ(αl) + Eq[log alt] − Eq[alt]

Yj6=l

(cid:17)

log γf −

Uf lEq[alt] + 1 − ψ(γf ) + log Wf t − Wf t

Eq[exp(−Uf lalt)]

Xl

Yl

(14)

(15)

(cid:17)
(16)

Note that the optimization problem for U is independent for different frequency bin f ∈
{1, 2, · · · , F }, as reﬂected by the gradient.

12

4
1
0
2
 
v
o
N
5
2
 
 
]
L
M

 

.
t
a
t
s
[
 
 
5
v
7
5
8
5
.
2
1
3
1
:
v
i
X
r
a

A Generative Product-of-Filters Model of Audio

Dawen Liang∗
Columbia University
New York, NY 10027
dliang@ee.columbia.edu

Matthew D. Hoffman
Adobe Research
San Francisco, CA 94103
mathoffm@adobe.com

Gautham J. Mysore
Adobe Research
San Francisco, CA 94103
gmysore@adobe.com

Abstract

We propose the product-of-ﬁlters (PoF) model, a generative model that decom-
poses audio spectra as sparse linear combinations of “ﬁlters” in the log-spectral
domain. PoF makes similar assumptions to those used in the classic homomorphic
ﬁltering approach to signal processing, but replaces decompositions built of basic
signal processing operations with a learned decomposition based on statistical in-
ference. This paper formulates the PoF model and derives a mean-ﬁeld method
for posterior inference and a variational EM algorithm to estimate the model’s free
parameters. We demonstrate PoF’s potential for audio processing on a bandwidth
expansion task, and show that PoF can serve as an effective unsupervised feature
extractor for a speaker identiﬁcation task.

1 Introduction

Some of the most successful approaches to audio signal processing of the last ﬁfty years have been
based on decomposing complicated systems into an excitation signal and some number of simpler
linear systems. One of the simplest (and most widely used) examples is linear predictive coding
(LPC), which uses a simple autoregressive model to decompose an audio signal into an excitation
signal and linear ﬁlter [17]. More broadly, homomorphic ﬁltering methods such as cepstral analysis
[16] try to decompose complicated linear systems into a set of simpler linear systems that can then
be analyzed, interpreted, and manipulated independently.

One reason that this broad approach has been successful is because it is consistent with the way
many real-world objects actually generate sound. An important example is the human voice: human
vocal sounds are generated by running vibrations generated by the vocal folds through the rest of
the vocal tract (tongue, lips, jaw, etc.), which approximately linearly ﬁlters the vibrations that come
from the larynx or lungs.

Traditional approaches typically rely on hand-designed decompositions built of basic operations
such as Fourier transforms, discrete cosine transforms, and least-squares solvers. In this paper we
take a more data-driven approach, and derive a generative product-of-ﬁlters (PoF) model that learns
a statistical-inference-based decomposition that is tuned to be appropriate to the data being ana-
lyzed. Like traditional homomorphic ﬁltering approaches, PoF decomposes audio spectra as linear
combinations of ﬁlters in the log-spectral domain. Unlike previous approaches, these ﬁlters are
learned from data rather than selected from convenient families such as orthogonal cosines, and the
PoF model learns a sparsity-inducing prior that prefers decompositions that use relatively few ﬁlters
to explain each observed spectrum. The result when applied to speech data is that PoF discovers
some ﬁlters that model excitation signals and some that model the various ﬁltering operations that
the vocal tract can perform. Given a set of these learned ﬁlters, PoF can infer how much each ﬁl-
ter contributed to a given audio magnitude spectrum, resulting in a sparse, compact, interpretable
representation.

∗This work was performed while Dawen Liang was an intern at Adobe Research.

1

The rest of the paper proceeds as follows. First, we formally introduce the product-of-ﬁlters (PoF)
model, and give more rigorous intuitions about the assumptions that it makes. Next, we review
some previous work and show how it relates to the PoF model. Then, we derive a mean-ﬁeld
variational inference algorithm that allows us to do approximate posterior inference in PoF, as well
as a variational EM algorithm that ﬁts the model’s free parameters to data. Finally, we demonstrate
PoF’s potential for audio processing on a bandwidth expansion task, where it achieves better results
than a recently proposed technique based on non-negative matrix factorization (NMF). We also
evaluate PoF as an unsupervised feature extractor, and ﬁnd that the representation learned by the
model yields higher accuracy on a speaker identiﬁcation task than the widely used mel-frequency
cepstral coefﬁcient (MFCC) representation.

2 Product-of-Filters Model

We are interested in modeling audio spectrograms, which are collections of Fourier magnitude spec-
tra W taken from some set of audio signals, where W is an F × T non-negative matrix; the cell
Wf t gives the magnitude of the audio signal at frequency bin f and time window (often called a
frame) t. Each column of W is the magnitude of the fast Fourier transform (FFT) of a short window
of an audio signal, within which the spectral characteristics of the signal are assumed to be roughly
stationary.

The motivation for our model comes from the widely used homomorphic ﬁltering approach to audio
and speech signal processing [16], where a short window of audio w[n] is modeled as a convolution
between an excitation signal e[n] (which might come from a speaker’s vocal folds) and the impulse
response h[n] of a series of linear ﬁlters (such as might be implemented by a speaker’s vocal tract):

In the spectral domain after taking the FFT, this is equivalent to:

w[n] = (e ∗ h)[n]

|W[k]| = |E[k]| ◦ |H[k]| = exp{log |E[k]| + log |H[k]|}

where ◦ denotes element-wise multiplication and | · | denotes the magnitude of a complex value pro-
duced by the FFT. Thus, the convolution between these two signals corresponds to a simple addition
of their log-spectra. Another attractive feature is the symmetry between the excitation signal e[n]
and the impulse response h[n] of the vocal-tract ﬁlter—convolution commutes, so mathematically
(if not physiologically) the vocal tract could just as well be exciting the “ﬁlter” implemented by
vocal folds.

We will likewise model the observed magnitude spectra as a product of ﬁlters. We assume each
observed log-spectrum is approximately obtained by linearly combining elements from a pool of L
log-ﬁlters1 U ≡ [u1|u2| · · · |uL] ∈ RF ×L:

log Wf t ≈

lUf lalt,

P

where alt denotes the activation of ﬁlter ul in frame t. We will impose some sparsity on the activa-
tions, to allow us to encode the intuition that not all ﬁlters are always active. This assumption ex-
pands on the expressive power of the simple excitation-ﬁlter model of equation 1; we could recover
that model by partitioning the ﬁlters into “excitations” and “vocal tracts”, requiring that exactly one
“excitation ﬁlter” be active in each frame, and combining the weighted effects of all “vocal tract
ﬁlters” into a single ﬁlter.

We have two main reasons for relaxing the classic excitation-ﬁlter model to include more than two
ﬁlters, one computational and one statistical. The statistical rationale is that the parameters that
deﬁne the human voice (pitch, tongue position, etc.) are inherently continuous, and so a very large
dictionary of excitations and ﬁlters might be necessary to explain observed inter- and intra-speaker
variability with the classic model. The computational rationale is that clustering models (which
might try to determine which excitation is active) can be more fraught with local optima than fac-
torial models such as ours (which tries to determine how active each ﬁlter is), and there is some
precedent for relaxing clustering models into factorial models [3].

1We will use the term “ﬁlter” when referring to U for the rest of the paper.

(1)

(2)

(3)

2

Formally, we deﬁne the product-of-ﬁlters model:

alt ∼ Gamma(αl, αl)

Wf t ∼ Gamma

γf , γf / exp(

(cid:16)

lUf lalt)
(cid:17)

P

where γf is the frequency-dependent noise level. We restrict the activations at (but not the ﬁlters
ul) to be non-negative; if we allowed negative alt, then the the ﬁlters would be inverted, reducing
the interpretability of the model.

Under this model

(4)

(5)

E[alt] = 1
E[Wf t] = exp(

lUf lalt).

P

For l ∈ {1, 2, · · · , L}, αl controls the sparseness of the activations associated with ﬁlter ul; smaller
values of αl indicate that ﬁlter ul is used more rarely. From a generative point of view, one can view
the model as ﬁrst drawing activations alt from a sparse prior, then applying multiplicative gamma
l Uf lalt). A graphical model representation
noise with expected value 1 to the expected value exp(
of the PoF model is shown in Figure 1.

P

wt

T

U, γ

at

α

Figure 1: Graphical model representation of the PoF model.

In this paper we focus on speech applications, but the homomorphic ﬁltering approach has also been
successfully applied to model other kinds of sounds such as musical instruments. For example, [13]
treat the effect of the random excitation, string, and body as a chain of linear systems, which can
therefore be modeled as a product of ﬁlters.

3 Related Work

The PoF model can be interpreted as a matrix factorization model, where we are trying to decompose
the log-spectrogram. A closely related model is non-negative matrix factorization (NMF) [14] and
its variations, e.g., NMF with sparseness constrains [10], convex NMF [4], and fully Bayesian NMF
[2]. In NMF, a F × T non-negative matrix W is approximately decomposed into the product of
two non-negative matrices: a F × K matrix V (often called the dictionary) and a K × T matrix H
(often called the activations). NMF is widely used to analyze audio spectrograms [6, 19], largely
due to its additivity property and the parts-based representation that it induces. It also often provides
a semantically meaningful interpretation. For example, given the NMF decomposition of a piano
sonata, the components in the dictionary are likely to correspond to notes with different pitches, and
the activations will indicate when and how strongly each note is played. NMF’s ability to isolate
energy coming from different sources in mixed recordings has made it a popular tool for addressing
source separation problems.

Although both models decompose audio spectra using a linear combination of dictionary elements,
NMF and PoF make fundamentally different modeling assumptions. NMF models each frame of a
spectrogram as an additive combination of dictionary elements, which approximately corresponds
to modeling the corresponding time-domain signal as a summation of parts. On the other hand, PoF
models each frame of the spectrogram as a product of ﬁlters (sum of log-ﬁlters), which corresponds
to modeling the corresponding time-domain signal as a convolution of ﬁlters. NMF is well suited to
decomposing polyphonic sounds into mixtures of independent sources, whereas PoF is well suited
to decomposing monophonic sounds into simpler systems.

In the compressive sensing literature, there has been a great deal of work on matrix factorization
and dictionary learning by solving an optimization problem with sparseness constraints—adding
ℓ1 norm penalty as a convex relaxation of ℓ0 norm penalty [5]. Online algorithms have also been

3

proposed to handle large data sets [15].
In principle, we could have formulated the PoF model
similarly, using an ℓ1 penalty and convex optimization in place of a gamma prior and Bayesian
inference. However, in such a formulation it is unclear how we might ﬁt the L hyperparameters α
controlling the amount of sparsity in the activations associated with each ﬁlter. We found that PoF
best explains speech training data when each ﬁlter ul has its own sparsity hyperparameter αl, and
performing cross-validation to select so many hyperparameters would be impractical.

4 Inference and Parameter Estimation

From Figure 1, we can see that there are two computational problems that will arise when using the
PoF model. First, given ﬁxed U, α, and γ and input spectrum wt, we must (approximately) compute
the posterior distribution p(at|wt, U, α, γ). This will enable us to ﬁt the PoF model to unseen data
and obtain a different representation in the latent ﬁlter space. Second, given a collection of training
spectra W = {wt}1:T , we want to ﬁnd the maximum likelihood estimates of the free parameters U,
α, and γ. In this section, we will tackle these two problems respectively. The detailed derivations
can be found in the appendix. The source code in Python is available on Github2.

4.1 Mean-Field Posterior Inference

The posterior p(at|wt, U, α, γ) is intractable to compute due to the nonconjugacy of the model.
Therefore, we employ mean-ﬁeld variational inference [12].

Variational inference is a deterministic alternative to Markov Chain Monte Carlo (MCMC) meth-
ods. The basic idea behind variational inference is to choose a tractable family of variational dis-
tributions q(at) to approximate the intractable posterior p(at|wt, U, α, γ), so that the Kullback-
Leibler (KL) divergence between the variational distribution and the true posterior KL(qakpa|W) is
minimized. In particular, we are using the mean-ﬁeld family which is completely factorized, i.e.,
q(at) =
l q(alt). For each alt, we choose a variational distribution from the same family as alt’s
prior distribution: q(alt) = Gamma(alt; νa
t and ρa
t are free parameters that we will tune to
minimize the KL divergence between q and the posterior.

lt). ν a

lt, ρa

Q

We can lower bound the marginal likelihood of the input spectrum wt:

log p(wt|U, α, γ)

≥ Eq[log p(wt, at|U, α, γ)] − Eq[log q(at)]

△
= L(ν a

t , ρa
t )

lt/ρa
To compute the variational lower bound L(ν a
lt
and Eq[log alt] = ψ(νa
lt, where ψ(·) is the digamma function, are both easy to compute.
For Eq[exp(−Uf lalt)], we seek for the moment-generating function of a gamma-distributed random
variable and obtain the expectation as:

t ), the necessary expectations Eq[alt] = νa

lt) − log ρa

t , ρa

Eq[exp(−Uf lalt)] =

−νa
lt

1 + Uf l
ρa
lt (cid:17)

(cid:16)

for Uf l > −ρa

lt, and +∞ otherwise3.

The nonconjugacy and the exponents in the likelihood model preclude optimizing the lower bound
by traditional closed-form coordinate ascent updates. Instead, we compute the gradient of L(ν a
t , ρa
t )
with respect to variational parameters ν a
t and use Limited-memory BFGS (L-BFGS) to opti-
mize the variational lower bound, which guarantees to ﬁnd a local optimum and optimal variational
parameters { ˆν a

t and ρa

t , ˆρa

t }.

Note that in the posterior inference, the optimization problem is independent for different frame t.
Therefore, given input spectra {wt}1:T , we can break down the whole problem into T independent
sub-problems which can be solved in parallel.

2https://github.com/dawenl/pof
3Technically the expectation for Uf l ≤ −ρa

the variational lower bound goes to −∞ and the optimization can be carried out seamlessly.

lt is undeﬁned. Here we treat it as +∞ so that when Uf l ≤ −ρa
lt

(6)

(7)

4

4.2 Parameter Estimation

Given a collection of training audio spectra {wt}1:T , we carry out parameter estimation for the PoF
model by ﬁnding the maximum likelihood estimates of the free parameters U, α, and γ, approxi-
mately marginalizing out at.

We formally deﬁne the parameter estimation problem as

ˆU, ˆα, ˆγ = arg max

log p(wt|U, α, γ)

U,α,γ Xt

(8)

= arg max

log

U,α,γ Xt

Zat

p(wt, at|U, α, γ)dat

This problem can be solved by variational Expectation-Maximization (EM) which maximizes a
lower bound on marginal likelihood in equation 6 with respect to the variational parameters, and
then for the ﬁxed values of variational parameters, maximizes the lower bound with respect to the
model’s free parameters U, α, and γ.

E-step For each wt where t = 1, 2, · · · , T , perform posterior inference by optimizing the values
of the variational parameters { ˆν a

t }. This is done as described in Section 4.1.

t , ˆρa

M-step Maximize the variational lower bound in equation 6, which is equivalent to maximizing
the following objective:

Q(U, α, γ) =

Eq[log p(wt, at|U, α, γ)]

(9)

t

P

This is accomplished by ﬁnding the maximum likelihood estimates using the expected sufﬁcient
statistics for each at that were computed in the E-step. There are no simple closed-form updates for
the M-step. Therefore, we compute the gradient of Q(U, α, γ) with respect to U, α, γ, respectively,
and use L-BFGS to optimize the bound in equation 9.
The most time-consuming part for M-step is updating U, which is a F × L matrix. However,
note that the optimization problem is independent for different frequency bins f ∈ {1, 2, · · · , F }.
Therefore, we can update U by optimizing each row independently, and in parallel if desired.

5 Evaluation

We conducted experiments to assess the PoF model on two different tasks. We evaluate PoF’s ability
to infer missing data in the bandwidth expansion task. We also explore the potential of the PoF model
as an unsupervised feature extractor for the speaker identiﬁcation task.
Both tasks require pre-trained parameters U, α, and γ, which we learned from the TIMIT Speech
Corpus [7]. It contains speech sampled at 16000 Hz from 630 speakers of eight major dialects of
American English, each reading ten phonetically rich sentences. The parameters were learned from
20 randomly selected speakers (10 males and 10 females). We performed a 1024-point FFT with
Hann window and 50% overlap, thus the number of frequency bins was F = 513. We performed
the experiments on magnitude spectrograms except where speciﬁed otherwise.

We tried different model orders L ∈ {10, 20, · · · , 80} and evaluated the lower bound on the marginal
likelihood log p(wt|U, α, γ) in equation 6. In general, larger L will give us a larger variational
lower bound and will be slower to train. In our experiments, we set L = 50 as a compromise
between performance and computational efﬁciency. We initialized all the variational parameters ν a
t
and ρa
t with random draws from a gamma distribution with shape parameter 100 and inverse-scale
parameter 100. This yields a diffuse and smooth initial variational posterior, which helped avoid bad
local optima. We ran variational EM until the variational lower bound increased by less than 0.01%.

Figure 2 demonstrates some representative ﬁlters learned from the PoF model with L = 50. The six
ﬁlters ul associated with the largest values of αl are shown in Figure 2a, and the six ﬁlters associated
with the smallest values of αl are shown in Figure 2b. Small values of αl indicate a prior preference
to use the associated ﬁlters less frequently, since the Gamma(αl, αl) prior places more mass near
0 when αl is smaller. The ﬁlters in Figure 2b, which are used relatively rarely, tend to have the

5

)
B
d
(
 
e
d
u
t
i
n
g
a
M

10
5
0
−5
−10
−15
−20
−25

)
B
d
(
 
e
d
u
t
i
n
g
a
M

20
15
10
5
0
−5
−10
−15
−20

)
B
d
(
 
e
d
u
t
i
n
g
a
M

40

20

0

−20

−40

)
B
d
(
 
e
d
u
t
i
n
g
a
M

30
20
10
0
−10
−20
−30

or

α = 1.68

α = 1.37

α = 1.06

6
4
2
0
−2
−4
−6
−8
−10
−12

15
10
5
0
−5
−10
−15
−20

60
40
20
0
−20
−40
−60

30
20
10
0
−10
−20
−30

0

2

6

8

0

2

6

8

0

2

6

8

4
Frequency (kHz)

α = 0.96

4
Frequency (kHz)

α = 0.94

4
Frequency (kHz)

α = 0.89

0

2

6

8

0

2

6

8

0

2

6

8

4
Frequency (kHz)

4
Frequency (kHz)

4
Frequency (kHz)

(a) The top 6 ﬁlters ul with the largest αl values (shown above each plot).

α = 0.01

α = 0.01

α = 0.02

0

2

6

8

0

2

6

8

0

2

6

8

4
Frequency (kHz)

α = 0.02

4
Frequency (kHz)

α = 0.02

4
Frequency (kHz)

α = 0.02

0

2

6

8

0

2

6

8

0

2

6

8

4
Frequency (kHz)

4
Frequency (kHz)

4
Frequency (kHz)

(b) The top 6 ﬁlters ul with the smallest αl values (shown above each plot).

Figure 2: The representative ﬁlters learned from the PoF model with L = 50.

10
5
0
−5
−10
−15
−20
−25

25
20
15
10
5
0
−5
−10
−15
−20

25
20
15
10
5
0
−5
−10
−15
−20

25
20
15
10
5
0
−5
−10
−15
−20

strong harmonic structure displayed by the log-spectra of periodic signals, while the ﬁlters in Figure
2a tend to vary more smoothly, suggesting that they are being used to model the ﬁltering induced
by the vocal tract. The periodic “excitation” ﬁlters tend to be used more rarely, which is consistent
with the fact that normally there is not more than one excitation signal contributing to a speaker’s
voice. (Very few people can speak or sing more than one pitch simultaneously.) The model has more
freedom to use several of the coarser “vocal tract” ﬁlters per spectrum, which is consistent with the
fact that several aspects of the vocal tract may be combined to ﬁlter the excitation signal generated
by a speaker’s vocal folds.

Despite the non-convexity inherent to all dictionary-learning problems, which causes the details
of the ﬁlters vary from run to run, training with multiple random restarts and different speakers
produced little impact on the ﬁlters that the PoF model learned; in all cases with different model
order L, we found the same “excitation/ﬁlter” structure, similar to what is shown in Figure 2.

5.1 Bandwidth Expansion

We demonstrate PoF’s potential in audio processing applications on a bandwidth expansion task,
where the goal is to infer the contents of a full-bandwidth signal given only the contents of a band-
limited version of that signal. Bandwidth expansion has applications to restoration of low-quality
audio such as might be recorded from a telephone or cheap microphone.
Given the parameters U, α, and γ ﬁt to full-bandwidth training data, we can treat the bandwidth
expansion problem as a missing data problem. Given spectra from a band-limited recording Wbl =
{wbl
t ) over the activations at associated
with the band-limited signal, for t = {1, 2, · · · , T }. We can approximate this posterior using the
variational inference algorithm from Section 4.1 by only using the band-limited part of U and γ.
Then we can reconstruct the full-bandwidth spectra by combining the inferred {at}1:T with the
full-bandwidth U. Following the model formulation in equation 4, we might either estimate the
full-bandwidth spectra using

t }1:T , the model implies a posterior distribution p(at|wbl

Eq[W fb

f t] =

Eq[exp(Uf lalt)]

Eq[W fb

f t] = exp{

lUf l · Eq[alt]}.

l

Q

P

6

(10)

(11)

We use equation 11, both because it is more stable and because human auditory perception is log-
arithmic; if we are summarizing the posterior distribution with a point estimate, the expectation on
the log-spectral domain is more perceptually natural.

As a comparison, NMF is widely used for bandwidth expansion [1, 18, 20]. The full-bandwidth
training spectra Wtrain, which are also used to learn the parameters U, α, and γ for the PoF model,
are decomposed by NMF as Wtrain ≈ VH, where V is the dictionary and H is the activation. Then
given the band-limited spectra Wbl, we can use the band-limited part of V to infer the activation
Hbl. Finally, we can reconstruct the full-bandwidth spectra by computing VHbl.

Based on how the loss function is deﬁned, there can be different types of NMF models: KL-NMF
[14] which is based on Kullback-Leibler divergence, and IS-NMF [6] which is based on Itakura-
Saito divergence, are among the most commonly used NMF decomposition models in audio signal
processing. We compare the PoF model with both KL-NMF and IS-NMF with different model
orders K = 25, 50, and 100. We used the standard multiplicative updates for NMF and stopped the
iterations when the decrease in the cost function was less than 0.01%. For IS-NMF, we used power
spectra instead of magnitude spectra, since the power spectrum representation is more consistent
with the statistical assumptions that underlie the Itakura-Saito divergence.

We randomly selected 10 speakers (5 male and 5 female) from TIMIT that do not overlap with the
speakers used to ﬁt the model parameters U, α, and γ, and took 3 sentences from each speaker
as test data. We cut off all the contents below 400 Hz and above 3400 Hz to obtain band-limited
recordings of approximately telephone-quality speech.

In previous NMF-based bandwidth expansion work [1, 18, 20], all experiments are done in a speaker-
dependent setting, which means the model is trained from the target speaker. What we are doing
here, on the other hand, is speaker-independent: we use no prior knowledge about the speciﬁc
speaker whose speech is being restored4. To our knowledge, little if any work has been done on
speaker-independent bandwidth expansion based on NMF decompositions.

To evaluate the quality of the reconstructed recordings, we used the composite objective measure
[11] and short-time objective intelligibility [21] metrics. These metrics measure different aspects of
the “distance” between the reconstructed speech and the original speech. The composite objective
measure (will be abbreviated as OVRL, as it reﬂects the overall sound quality) was originally pro-
posed as a quality measure for speech enhancement. It aggregates different basic objective measures
and has been shown to correlate with humans’ perceptions of audio quality. OVRL is based on the
predicted perceptual auditory rating and is in the range of 1 to 5 (1: bad; 2: poor; 3: fair; 4: good; 5:
excellent). The short-time objective intelligibility measure (STOI) is a function of the clean speech
and reconstructed speech, which correlates with the intelligibility of the reconstructed speech, that
is, it predicts the ability of listeners to understand what words are being spoken rather than per-
ceived sound quality. STOI is computed as the average correlation coefﬁcient from 15 one-third
octave bands across frames, thus theoretically should be in the range of -1 to 1, where larger values
indicate higher expected intelligibility. However, in practice, even when we ﬁlled out the missing
contents with random noise, the STOI is 0.306 ± 0.016, which can be interpreted as a practical
lower bound on the test data.

The average OVRL and STOI with two standard errors5 across 30 sentences for different methods,
along with these from the band-limited input speech as baseline, are reported in Table 1. We can see
that NMF improves STOI by a small amount, and PoF improves it slightly more, but the improve-
ment in both cases is fairly small. This may be because the band-limited input speech already has
a relatively high STOI (telephone-quality speech is fairly intelligible). On the other hand, PoF pro-
duces better predicted perceived sound quality as measured by OVRL than KL-NMF and IS-NMF
by a large margin regardless of the model order K, improving the sound quality from poor-to-fair
(2.71 OVRL) to fair-to-good (3.25 OVRL).

4When we conducted speaker-dependent experiments, both PoF and NMF produced nearly ceiling-level
results. Thus we only report results on the harder and more practically relevant speaker-independent problem.

5For both OVRL and STOI, we used the MATLAB implementation from the original authors.

7

Table 1: Average OVRL (composite objective measure) and STOI (short-time objective intelligibil-
ity) score with two standard errors (in parenthesis) for the bandwidth expansion task from different
methods. OVRL is in the range of 1 to 5 [1: bad; 2: poor; 3: fair; 4: good; 5: excellent]. STOI is
the average correlation coefﬁcient, thus theoretically should be in the range of -1 to 1, where larger
values indicate higher expected intelligibility.

KL-NMF

Band-limited input
K=25
K=50
K=100
K=25
K=50
K=100

IS-NMF

PoF

OVRL
1.72 (0.16)
2.60 (0.12)
2.71 (0.14)
2.41 (0.10)
2.43 (0.15)
2.62 (0.12)
2.15 (0.10)
3.25 (0.13)

STOI
0.762 (0.012)
0.786 (0.013)
0.790 (0.013)
0.759 (0.012)
0.779 (0.013)
0.774 (0.014)
0.751 (0.012)
0.804 (0.010)

5.2 Feature Learning and Speaker Identiﬁcation

We explore PoF’s potential as an unsupervised feature extractor. One way to interpret the PoF model
is that it attempts to represent the data in a latent ﬁlter space. Therefore, given spectra {wt}1:T , we
can use the coordinates in the latent ﬁlter space {at}1:T as features. Since we believe the inter- and
intra-speaker variability is well-captured by the PoF model, we use speaker identiﬁcation to evaluate
the effectiveness of these features.

We compare our learned representation with mel-frequency cepstral coefﬁcients (MFCCs), which
are widely used in various speech and audio processing tasks including speaker identiﬁcation.
MFCCs are computed by taking the discrete cosine transform (DCT) on mel-scale log-spectra and
using only the low-order coefﬁcients. PoF can be understood in similar terms; we are also trying
to explain the variability in log-spectra in terms of a linear combination of dictionary elements.
However, instead of using the ﬁxed, orthogonal DCT basis, PoF learns a ﬁlter space that is tuned to
the statistics of the input. Therefore, it seems reasonable to hope that the coefﬁcients at from the
PoF model, which will be abbreviated as PoFC, might compare favorably with MFCCs as a feature
representation.

We evaluated speaker identiﬁcation under the following scenario: identify different speakers from
recordings where each speaker may start and ﬁnish talking at random time, but at any given time
there is only one speaker speaking (like during a meeting). This is very similar to speaker diarization
[8], but here we assume we know a priori the number of speakers and certain amount of training data
is available for each speaker. Our goal in this experiment was to demonstrate that the PoFC repre-
sentation captures information that is missing or difﬁcult to extract from the MFCC representation,
rather than trying to build a state-of-the-art speaker identiﬁcation system.

We randomly selected 10 speakers (5 males and 5 females) from TIMIT outside the training data
we used to learn the free parameters U, α, and γ. We used the ﬁrst 13 DCT coefﬁcients which
is a standard choice for computing MFCC. We obtained the PoFC by doing posterior inference as
described in Section 4.1 and used Eq[at] as a point estimate summary. For both MFCC and PoFC,
we computed the ﬁrst-order and second-order differences and concatenated them with the original
feature.

We treat speaker identiﬁcation problem as a classiﬁcation problem and make predictions for each
frame. We trained a multi-class (one-vs-the-rest) linear SVM using eight sentences from each
speaker and tested with the remaining two sentences, which gave us about 7800 frames of train-
ing data and 1700 frames of test data. The test data was randomly permuted so that the order in
which sentences appear is random, which simulates the aforementioned scenario.

The frame level accuracy is reported in the ﬁrst row of Table 2. We can see PoFC increases the
accuracy by a large margin (from 49.1% to 60.5%). To make use of temporal information, we used a
simple median ﬁlter smoother with length 25, which boosts the performance for both representations
equally; these results are reported in the second row of Table 2.

8

Table 2: 10-speaker identiﬁcation accuracy using PoFC, MFCC, and combined. The ﬁrst row shows
the raw frame-level test accuracy. The second row shows the result after applying a simple median
ﬁlter with length 25 on the frame-level prediction.

Frame-level
Smoothing

MFCC
49.1% 60.5%
74.2% 85.0%

65.5%
89.5%

PoFC MFCC + PoFC

Although MFCCs and PoFCs capture similar information, concatenating both sets of features yields
better accuracy than that obtained by either feature set alone. The results achieved by combining
the features are summarized in the last column of Table 2, which indicates that MFCCs and PoFCs
capture complementary information. These results, which use a relatively simple frame-level classi-
ﬁer, suggest that PoFC could produce even better accuracy when used in a more sophisticated model
(e.g. a deep neural network).

6 Discussion and Future Work

In this paper, we proposed the product-of-ﬁlters (PoF) model, a generative model which makes sim-
ilar assumptions to those used in the classic homomorphic ﬁltering approach to signal processing.
We derived variational inference and parameter estimation algorithms, and demonstrated experi-
mental improvements on a bandwidth expansion task and showed that PoF can serve as an effective
unsupervised feature extractor for speaker identiﬁcation.

In this paper, we derived PoF as a standalone model. However, it can also be used as a building
block and integrated into a larger model, e.g., as a prior for the dictionary in a probabilistic NMF
model.

Although the optimization in the variational EM algorithm can be parallelized, currently we cannot
ﬁt PoF to large-scale speech data on a single machine. Leveraging recent developments in stochastic
variational inference [9], it would be possible to learn the free parameters from a much larger, more
diverse speech corpus, or even from streams of data.

References

[1] D. Bansal, B. Raj, and P. Smaragdis. Bandwidth expansion of narrowband speech using non-

negative matrix factorization. In INTERSPEECH, pages 1505–1508, 2005.

[2] A. T. Cemgil. Bayesian inference for nonnegative matrix factorisation models. Computational

Intelligence and Neuroscience, 2009.

[3] C. Ding and X. He. K-means clustering via principal component analysis. In Proceedings of

the International Conference on Machine learning, page 29. ACM, 2004.

[4] C. Ding, T. Li, and M. I. Jordan. Convex and semi-nonnegative matrix factorizations. Pattern

Analysis and Machine Intelligence, IEEE Transactions on, 32(1):45–55, 2010.

[5] D. L. Donoho and M. Elad. Optimally sparse representation in general (nonorthogonal) dictio-
naries via ℓ1 minimization. Proceedings of the National Academy of Sciences, 100(5):2197–
2202, 2003.

[6] C. F´evotte, N. Bertin, and J.-L. Durrieu. Nonnegative matrix factorization with the Itakura-
Saito divergence: with application to music analysis. Neural Computation, 21(3):793–830,
Mar. 2009.

[7] W. M. Fisher, G. R. Doddington, and K. M. Goudie-Marshall. The DARPA speech recognition
research database: speciﬁcations and status. In Proc. DARPA Workshop on speech recognition,
pages 93–99, 1986.

[8] E.B. Fox, E.B. Sudderth, M.I. Jordan, and A.S. Willsky. A Sticky HDP-HMM with Application

to Speaker Diarization. Annals of Applied Statistics, 5(2A):1020–1056, 2011.

[9] M. Hoffman, D. Blei, C. Wang, and J. Paisley. Stochastic variational inference. Journal of

Machine Learning Research, 14:1303–1347, 2013.

9

[10] P. O. Hoyer. Non-negative matrix factorization with sparseness constraints. The Journal of

Machine Learning Research, 5:1457–1469, 2004.

[11] Y. Hu and P. C. Loizou. Evaluation of objective quality measures for speech enhancement.
Audio, Speech, and Language Processing, IEEE Transactions on, 16(1):229–238, 2008.

[12] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational

methods for graphical models. Machine learning, 37(2):183–233, 1999.

[13] M. Karjalainen, V. V¨alim¨aki, and Z. J´anosy. Towards high-quality sound synthesis of the guitar
and string instruments. In Proceedings of the International Computer Music Conference, pages
56–56, 1993.

[14] D.D. Lee and H.S. Seung. Algorithms for non-negative matrix factorization. Advances in

Neural Information Processing Systems, 13:556–562, 2001.

[15] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online learning for matrix factorization and sparse

coding. The Journal of Machine Learning Research, 11:19–60, 2010.

[16] A. Oppenheim and R. Schafer. Homomorphic analysis of speech. Audio and Electroacoustics,

IEEE Transactions on, 16(2):221–226, 1968.

[17] D. O’Shaughnessy. Linear predictive coding. Potentials, IEEE, 7(1):29–32, 1988.
[18] B. Raj, R. Singh, M. Shashanka, and P. Smaragdis. Bandwidth expansionwith a P´olya urn
model. In Acoustics, Speech and Signal Processing, IEEE International Conference on, vol-
ume 4, pages IV–597. IEEE, 2007.

[19] P. Smaragdis and J. C. Brown. Non-negative matrix factorization for polyphonic music tran-
scription. In Applications of Signal Processing to Audio and Acoustics, IEEE Workshop on.,
pages 177–180. IEEE, 2003.

[20] D. L. Sun and R. Mazumder. Non-negative matrix completion for bandwidth extension: A
In Machine Learning for Signal Processing (MLSP), IEEE

convex optimization approach.
International Workshop on, pages 1–6. IEEE, 2013.

[21] C. H. Taal, R. C. Hendriks, R. Heusdens, and J. Jensen. An algorithm for intelligibility pre-
diction of time–frequency weighted noisy speech. Audio, Speech, and Language Processing,
IEEE Transactions on, 19(7):2125–2136, 2011.

10

A Variational EM for Product-of-Filters Model

A.1 E-step (Posterior Inference)

Following Section 4.1, the variational lower bound for the E-step (equation 6):
log p(wt|U, α, γ)

= log

q(at)

Zat

≥

Zat

q(at) log

p(wt, at|U, α, γ)
q(at)
p(wt, at|U, α, γ)
q(at)

dat

dat

(12)

t , ρa
t )
The second term is the entropy of a gamma-distributed random variable:

= Eq[log p(wt, at|U, α, γ)] − Eq[log q(at)]
≡ L(ν a

− Eq[log q(at)]
νa
lt − log ρa

=

Xl (cid:16)

lt + log Γ(νa

lt) + (1 − νa

lt)ψ(νa
lt)

(cid:17)

For the ﬁrst term, we can keep the parts which depend on {ν a

t , ρa

t }:

Eq[log p(wt, at|U, α, γ)]

= Eq[log p(wt|at, U, γ)] + Eq[log p(at|α)]

= const +

(αl − 1)Eq[log alt] − αlEq[alt]

−

γf

Wf t

Eq[exp(−Uf lalt)] +

Uf lEq[alt]

Xl n

o

Xf

n

Yl

Xl

o

Take the derivative of L(ν a

t , ρa

t ) with respect to νa

lt and ρa
lt:

∂L
∂νa
lt

=

γf

Wf t log(1 +

(cid:26)

Xf

Uf l
ρa
lt

)

L

Yj=1

Eq[exp(−Uf jajt)] −

+(αl − νa

lt)ψ1(νa

lt) + 1 −

Uf l
ρa
lt (cid:27)

αl
ρa
lt

∂L
∂ρa
lt

=

νlt
(ρa
lt)2

(cid:26)

Xf

γf

Uf l − Wf t(1 +

)−1Uf l

Eq[exp(−Uf jajt)]

+αl(

νlt
lt)2 −
(ρa

1
ρa
lt

)

(cid:27)

Uf l
ρa
lt

L

Yj=1

where ψ1(·) is the trigamma function.

A.2 M-step

The objective function for M-step is:

Q(U, α, γ) =

Eq[log p(wt, at|U, α, γ)]

=

Eq[log p(wt|at, U, γ)] + Eq[log p(at|α)]

(13)

Xt

Xt

where

Eq[log p(wt|at, U, γ)]

Xf (cid:16)
Eq[log p(at|α)]

Xl

=

γf log γf − γf

Uf lEq[alt] − log Γ(γf ) + (γf − 1) log Wf t − Wf tγf

Eq[exp(−Uf lalt)]

Yl

(cid:17)

=

αl log αl − log Γ(αl) + (αl − 1)Eq[log alt] − αlEq[alt]

(cid:17)

Xl (cid:16)

11

Take the derivative with respect to U, α, γ, we obtain the following gradients:

∂Q
∂Uf l

∂Q
∂αl

∂Q
∂γf

=

=

=

Xt (cid:16)

Xt (cid:16)

Xt (cid:16)

− Eq[alt] + Wf tEq[alt](1 +

)−(νa

lt+1)

Eq[exp(−Uf jajt)]

(cid:17)

Uf l
ρa
lt

log αl + 1 − ψ(αl) + Eq[log alt] − Eq[alt]

Yj6=l

(cid:17)

log γf −

Uf lEq[alt] + 1 − ψ(γf ) + log Wf t − Wf t

Eq[exp(−Uf lalt)]

Xl

Yl

(14)

(15)

(cid:17)
(16)

Note that the optimization problem for U is independent for different frequency bin f ∈
{1, 2, · · · , F }, as reﬂected by the gradient.

12

