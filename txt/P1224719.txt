DWM: A Decomposable Winograd Method for Convolution Acceleration

Di Huang,1,2,3 Xishan Zhang,1∗ Rui Zhang,1 Tian Zhi,1 Deyuan He,1,2,3 Jiaming Guo,1,2,3
Chang Liu,1,2,3 Qi Guo,1 Zidong Du,1 Shaoli Liu,3 Tianshi Chen,3 Yunji Chen1,2,4,5,6
1SKL of Computer Architecture, Institute of Computing Technology, CAS
2University of Chinese Academy of Sciences, 3Cambricon Tech. Ltd
4Institute of Brain-Intelligence Technology, Zhangjiang Laboratory
5Shanghai Research Center for Brian Science and Brain-Inspired Instelligence
6CAS Center for Excellence in Brain Science and Intelligence Technology
{huangdi18s, zhangxishan, zhangrui, zhitian, hedeyuan18s, guojiaming18s, liuchang18s, guoqi, duzidong, cyj}@ict.ac.cn
{liushaoli, tchen}@cambricon.com

0
2
0
2
 
b
e
F
 
3
 
 
]

G
L
.
s
c
[
 
 
1
v
2
5
5
0
0
.
2
0
0
2
:
v
i
X
r
a

Abstract

Winograd’s minimal ﬁltering algorithm has been widely used
in Convolutional Neural Networks (CNNs) to reduce the
number of multiplications for faster processing. However, it
is only effective on convolutions with kernel size as 3x3 and
stride as 1, because it suffers from signiﬁcantly increased
FLOPs and numerical accuracy problem for kernel size larger
than 3x3 and fails on convolution with stride larger than 1.
In this paper, we propose a novel Decomposable Winograd
Method (DWM), which breaks through the limitation of orig-
inal Winograd’s minimal ﬁltering algorithm to a wide and
general convolutions. DWM decomposes kernels with large
size or large stride to several small kernels with stride as 1 for
further applying Winograd method, so that DWM can reduce
the number of multiplications while keeping the numerical
accuracy. It enables the fast exploring of larger kernel size
and larger stride value in CNNs for high performance and
accuracy and even the potential for new CNNs. Comparing
against the original Winograd, the proposed DWM is able to
support all kinds of convolutions with a speedup of ∼2, with-
out affecting the numerical accuracy.

Introduction
Deep Convolutional Neural Networks (CNNs) have shown
excellent performance on many machine learning tasks but
has been plagued by the huge amount of computations
for a long time. Recently, CNNs are increasingly larger
to achieve better accuracy but also increase the amount of
computations. E.g., AlexNet (Krizhevsky, Sutskever, and
Hinton 2012), has 7 × 108 multiplications, VGG-16 (Si-
monyan and Zisserman 2014) contains 1.5 × 1010 multipli-
cations, and SENet-154 (Hu, Shen, and Sun 2018) contains
2.1 × 1010 multiplications. The massive amount of compu-
tations blocks the application of CNNs. Therefore, reducing
the multiplications in CNNs is essential and meaningful.

Lavin (2016) applied Winograd’s minimal ﬁltering algo-
rithm (Winograd 1980) to half reduce the number of multi-
plications in convolutions. Unfortunately, Winograd’s mini-
mal ﬁltering algorithm is only effective on 3×3 kernels with

∗Xishan Zhang (zhangxishan@ict.ac.cn) is the corresponding

author.
Copyright c(cid:13) 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

(a)

(b)

Figure 1: (a): The speedup comparison between two acceler-
ating algorithms, measured by FLOPs. Baseline is the regu-
lar convolution result of FP64; (b): The numerical accuracy
of two accelerating algorithms, which is the mean squared
error (MSE), scaled by log10(error) + 10 to make it posi-
tive.

stride 1. When the kernels are larger than 3 × 3, the trans-
form matrices of Winograd’s minimal ﬁltering algorithm
will introduce much more decimal multiplications causing
precision and efﬁciency problems. Another problem is that
Winograd’s minimal ﬁltering algorithm cannot be used on
convolutions with stride > 1. Since convolutions of kernel
size larger than 3 × 3 and stride > 1 are frequently used in
CNNs, these two restrictions severely limited the application
of Winograd’s minimal ﬁltering algorithm.

To tackle the above drawbacks, we propose the Decom-
posable Winograd Method (DWM) to extend the Winograd’s

minimal ﬁltering algorithm into the cases of large kernels
and stride > 1. First, for kernels larger than 3, DWM decom-
poses the original kernel into several kernels smaller than 3,
on which we can apply Winograd’s minimal ﬁltering algo-
rithm separately. Thus, for large kernels situation, DWM can
still reduce the number of multiplications by 50% and keep
the numerical accuracy the same as the original convolution.
Second, for stride larger than 1 situation, DWM splits the
kernels into several parts to apply Winograd’s minimal ﬁlter-
ing algorithm. E.g., with stride 2, DWM is equal to split the
polynomials of Winograd into odd ones and even ones and
compute them respectively. Therefore, DWM break through
the kernel size and stride restriction of Winograd’s minimal
ﬁltering algorithm. DWM has the advantages of both com-
putation and numerical accuracy. DWM can efﬁciently re-
duce the multiplications of regular convolutions. As shown
in Figure 1(a), with the kernel size increasing from 3 to 11,
the speedup of Winograd’s minimal ﬁltering algorithm de-
creases seriously, while the speedup of DWM keep to ∼ 2 .
Furthermore, DWM can keep the ﬁnal result totally equiv-
alent to the result of regular convolutions, which makes it
suitable to be applied to actual products. As shown in Fig-
ure 1(b), with the kernel size increasing from 3 to 11, the ac-
curacy error of DWM keep small but the error of Winograd’s
minimal ﬁltering algorithm increases quickly. These advan-
tages of DWM enables the fast exploring of larger kernel
size and larger stride value in CNNs for high performance
and accuracy and even the potential for new CNNs. Exper-
iments show that DWM achieves ∼ 2× acceleration while
keeping the numerical error under E-07, which is close to
the numerical accuracy of FP32 convolution.

The contribution of this paper has three aspects:

• We identify the limitation of the Winograd’s minimal
ﬁltering algorithm that it suffers from signiﬁcantly in-
creased FLOPs and numerical accuracy problem for ker-
nel size larger than 3x3 and fails on convolution with
stride larger than 1.

• We propose a novel DWM method which decomposes
kernels with large size or large stride to several small ker-
nels with stride as 1 for further apply Winograd method,
so that DWM can reduce the number of multiplications
while keeping the numerical accuracy.

• We evaluate the proposed DWM method on convolutions
with kernel size varying from 3 to 11 and stride from 1 to
2. Experimental results show that DWM is able to support
all kinds of convolutions with a speedup of ∼2, without
affecting the numerical accuracy.

Related Works
So far, with the success of convolutional neuron networks,
many researchers focus on accelerate convolution using
linear algebra property. Cong and Xiao (2014) save 47%
amount of multiplications by utilizing the linear algebra
property at the sub-matrix block level. Mathieu et al. (1997)
ﬁrst proposed the method of using Fast Fourier Trans-
form (FFT) to reduce the computation of convolution op-
erations. After that,
the FFT algorithm was reﬁned by
Vasilache (2014). Their two GPU implementations called

cuFFT and fbfft outperformed the convolution library made
by NVIDIA at that time. Lavin (2016) exploited several
element-level linear algebra methods to reduce the num-
ber of multiplications, including Winograd’s minimal ﬁl-
tering algorithm (Winograd 1980) and FFT. Nowadays,
cuDNN (Chetlur et al. 2014), a state-of-art deep learning li-
brary, includes both the Winograd algorithm and the FFT
algorithm as their convolution implementation.

Some researchers made their efforts to overcome the de-
fects of the Winograd algorithm. Some of them tried to over-
come the incompatibility of the Winograd algorithm and
model sparsiﬁcation. Li (2017) proposed a method to sparse
native Winograd coefﬁcients and obtains sparsity level be-
yond 90% with only 0.1% accuracy loss. Liu (2018) moved
ReLU operation into the Winograd transformation and then
pruned the weights in the Winograd domain. This approach
reduced the number of multiplications by 10.8× with loss of
accuracy less than 0.1%. Choi (2018) proposed a prune prin-
ciple to make the data keep sparse after the Winograd trans-
formation. Some researchers attempted to solve the numer-
ical accuracy and kernel size problem. Barabasz (2019) in-
vestigated a wider range of Winograd algorithms for DNNs,
which signiﬁcantly improve ﬂoating-point accuracy in many
cases. In fp16, this approach has given up to 6.5 times bet-
ter image recognition accuracy in one important case. Vin-
cent (2017) decreased the numerical error of large tile Wino-
grad algorithm by selecting the polynomial points. Meng
and Brothers (2019) extended the Winograd algorithm to
larger tiles by introducing complex numbers during the
Winograd transformation. Other researchers focused on the
hardware implementation of the Winograd algorithm. Due
to the low memory ceiling of GPU hardware, the Winograd
algorithm can be used to speed up CPU convolution opera-
tions and achieves 5 to 25-fold improvement in throughput
compared to previous state-of-art implementations (Budden
et al. 2017). Besides, for the mobile CPU acceleration, the
Winograd algorithm achieves up to 60% performance im-
provements in the full network compared to im2col/im2row
based optimization techniques (Maji et al. 2019).

Different from the researches mentioned above, our meth-
ods focus on the Winograd algorithm itself instead of the
combination of the Winograd algorithm and other methods
such as sparsiﬁcation. By extending the Winograd algorithm
to a much wider situation, we efﬁciently reduce the number
of multiplications while keeping the calculation’s numerical
accuracy stably high.

Preliminary on the Winograd Algorithm

The Winograd Algorithm
As an equivalent problem of multi-dimensional FIR ﬁlters
problem, convolution can be implemented more efﬁciently
using Winograd minimal ﬁltering algorithm (Lavin and Gray
2016). Denoting the result of computing m outputs with an
r-tap FIR ﬁlter as F (m, r), the corresponding convolution
algorithm for it requires m + r − 1 multiplications.

The original Winograd algorithm is derived from the rela-
tionship between polynomial multiplication and 1-D convo-
lutions using the Chinese Remainder Theorem(CRT) (Wino-

grad 1980). For the ﬁxed m and r, the whole algorithm con-
tains three ﬁxed transformation matrices: A, B and G.

Considering the original 1-D situation, the r element ﬁlter
g(x) and l = m + r − 1 element input signal d(x) can be
represented as polynomials (0 < r < l):

g(x) = gr−1xr−1 + gr−2xr−2 + ... + g1x + g0,
d(x) = dl−1xl−1 + dl−2xl−2 + ... + d1x + d0,
then the result of convolution g(x) ∗ d(x) can be obtained
by calculating the coefﬁcients of polynomial multiplication
y(x) = g(x)d(x).
(2)
Applying CRT, we can get three transformation matrices
A, B and G, and the process of doing convolution can be
formulated as the following:

(1)

Y = AT [(Gg) (cid:12) (BT d)].
(3)
where Y denotes the convolution output and (cid:12) denotes
element-wise multiplication. For 2-D convolutions, we can
nest the F(m, r) with itself, and then get

Y = AT [(GgGT ) (cid:12) (BT dB)]A.
(4)
From equation (4), we can derive the gradient of neuron
(denoted as ∇d) and the gradient of weight (denoted as ∇g)
of Winograd algorithm using the chain rule:

∇d = B[(A∇Y AT ) (cid:12) (GgGT )]BT ,
∇g = GT [(A∇Y AT ) (cid:12) (BT dB)]G,

(5)

where ∇Y is the gradient passed from the next layer.

Drawbacks
Large Kernel Size The beneﬁt of the Winograd algorithm
comes from the simplicity of transformation matrices. For
example, applying the Winograd algorithm, the transforma-
tion matrices of F (2, 3) are shown as follows:




BT =

G =









0
0 −1
1
0
1
1
0
1
0 −1
0
0 −1
1
0

0
1
1/2
1/2
1/2 −1/2
0

0


 ,




 ,

0
1/2
1/2
1
(cid:21)

AT =

(cid:20) 1
0

1
0
1
1 −1 −1

.

(6)



However, considering F (2, 5) as an example, the transfor-
mation matrices becomes something like

4
0 −5
0 −4 −4
0
0 −2 −1
0
0

0
1
4 −4 −1
2
2 −1 −2
0 −5
4

0
0
0
0
0
1

1
1
1
1
1
0

BT =















,

G =










0

0

0

1/4
−1/6
−1/6
1/24
1/12
1/24 −1/12
0

0
−1/6 −1/6 −1/6 −1/6
1/6 −1/6
1/6 −1/6
2/3
1/6
1/3
2/3
1/6 −1/3
1
0

0

0










,

AT =

(cid:20) 1
0

1
1
1 −1

1
1
2 −2

(cid:21)

.

0
1

The huge number of decimals in transformation matrices
makes the Winograd transformation not only high consump-
tion, but also less accurate.

Stride > 1 Another problem of the Winograd algorithm
is that it cannot be applied to stride > 1 convolutions, for
it is derived from polynomial multiplication which indicates
stride 1 convolutions. Therefore, although the Winograd al-
gorithm can implement convolutions much more efﬁciently,
it is always used on 3 × 3 and stride 1 convolutions only.

The Decomposable Winograd Method
In this section, we propose a series of techniques called
the Decomposable Winograd Method(DWM) to apply the
Winograd algorithm mentioned above on more general cases
like larger kernels and stride > 1.

Forward
Large Kernel Size As we denote before, g(x) represents
the 1-D convolution ﬁlter and thus r represents the size of
convolution ﬁlter. Observing the derivation process above,
we ﬁnd that when r becomes larger, g(x) in equation (1)
can be split into several small polynomials, to which we can
apply the original Winograd algorithm:






g(0)(x) = g2x2 + g1x + g0
g(1)(x) = (g5x2 + g4x + g3)x3

.

(8)

...

r−1 mod 3
(cid:88)

i=0

(cid:98)r/3(cid:99)
(cid:88)

i=0

g((cid:98)r/3(cid:99))(x) =

gr−i−1x(r−1 mod 3)−ix3(cid:98)r/3(cid:99)

Then from g(x) = (cid:80)(cid:98)r/3(cid:99)

i=0 g(i)(x) we can get

y(x) = g(x)d(x) =

[g(i)(x)d(x)] =

yi(x),

(9)

(cid:98)r/3(cid:99)
(cid:88)

i=0

We can apply F (2, 2) or F (2, 3) on each yi(x) separately,
with half multiplication reduced. As for 2-D convolution,
we can split the large kernel into small parts, and then ap-
ply Winograd algorithm to each part separately. The whole
process is illustrated by Figure 3, which shows that we can
process a common large kernel convolution in ﬁve steps:
• Splitting. Split the convolution kernel into several
parts whose kernel size belows 3 × 3, and then pre-
pare the input signal by slicing the redundant edges.
This method is shown in Figure 2a.

• Transformation. Apply corresponding Winograd
transformation BT (·)B and G(·)GT (in F (2, 2) or
F (2, 3)) on each part.

• Calculation. Do element-wise multiplication and

• Detransformation. Do the AT (·)A inverse transfor-
mation to change the intermidiate results to spatial
domain.

• Aggregation. Sum the calculation results of each
part, which gives the ﬁnal result that equivalent to
the original convolution.

(7)

channel-wise summation.

(a)

(b)

(cid:40) g(0)(x) = g2x2 + g1x + g0
g(1)(x) = (g4x + g3)x3

,

(10)

ments can be split into

g(0)(x) =

gs∗ixs∗i

Figure 2: (a): Splitting a 5 × 5 and stride 1 convolution into
four smaller convolutions; (b): Splitting a 5 × 5 and stride
2 convolution into four stride 1 convolutions. ’*’ denotes
doing convolution with corresponding parts.

For example, when r = 5, we can split it into two parts

and g(x) = g(0)(x) + g(1)(x). Then we get:

y(x) = g(x)d(x) = g(0)(x)d(x) + g(1)(x)d(x).

(11)

For g(0)(x)d(x), we can apply F (2, 3) to it, and for
g(1)(x)d(x), we can apply F (2, 2). When it comes to a 2-
D 5 × 5 convolution case, we can split the kernel into 4
parts:3 × 3, 3 × 2, 2 × 3 and 2 × 2, just as the method shown
in Figure 2a. This method’s advantage is that using F (2, 2)
and F (2, 3) instead of larger ones not only reduces the mul-
tiplications of Winograd transformation efﬁciently but also
keeps the Winograd algorithm’s accuracy because of the few
amounts of multiplications in transformation matrices. We
will further illustrate this advantage in the experiment part.

Stride > 1 Normal polynomial multiplication indicates
stride 1 convolution, but we can surmount this barrier by
grouping the polynomial into several parts. Denoting convo-
lution stride as s, 1-D convolution kernel g(x) with r ele-

Figure 3: The process of doing convolution using Winograd
algorithm. The four dotted frames denote convolution pro-
cedure of the four split parts, and each procedure can be di-
vided into ﬁve steps: splitting, transformation, calculation,
detransformation and aggregation.

(cid:98)(r−1)/s(cid:99)
(cid:88)

i=0

(cid:98)(r−2)/s(cid:99)
(cid:88)

i=0

...






g(1)(x) =

gs∗i+1xs∗i+1

.

(12)

g(s−1)(x) =

gs∗i+s−1xs∗i+s−1

(cid:98)(r−s−1)/s(cid:99)
(cid:88)

i=0

The input signal d(x) with l elements can be split into
d(0)(x), d(1)(x), ..., d(s−1)(x)similarly.

Then we get several stride 1 convolutions which can
be represented by polynomials multiplications y(i)(x) =
g(i)(x)d(i)(x) and y(x) = (cid:80) y(i)(x). By applying Wino-
grad algorithm to them, we have reduced the multiplications
of 1-D convolutions with stride > 1 successfully. For 2-D
convolutions, we nest the 1-D convolution methods. This
process also contains ﬁve parts: splitting, transformation,
calculation, detransformation and aggregation, which is sim-
ilar to Figure 3. The splitting method is illustrated by Fig-
ure 2b.

For instance, when we are dealing with stride 2 convolu-
tions, we can group the convolution kernel g(x) and input
signal d(x) by their degree’s parity, and then get

g(0)(x) =

g2ix2i

g(1)(x) =

g2i+1x2i+1

(13)











(cid:98)(r−1)/2(cid:99)
(cid:88)

i=0

(cid:98)(r−2)/2(cid:99)
(cid:88)

i=0

(cid:98)(l−1)/2(cid:99)
(cid:88)

i=0

(cid:98)(l−2)/2(cid:99)
(cid:88)

and

d(0)(x) =

d2ix2i

.

(14)

d(1)(x) =

d2i+1x2i+1

i=0
In a 2-D case, a 5×5 stride 2 convolution on 7×7 activation
can be splitted into doing four stride 1 convolutions: 3 × 3,
3 × 2, 2 × 3 and 2 × 2. Details are shown in Figure 2b.

Occasionally, we need to do a stride > 2 convolution with
large kernel size, then we can combine the two techniques
mentioned above. By applying these two techniques, we can
optimize all kinds of convolutions with the Winograd algo-
rithm, reducing the number of multiplications by half.

Backward
Using equation (5), we can apply DWM in the process of
calculating gradients, and this will also reduce the number of
multiplications by half. From SGD algorithm, we can derive
that
(t+1) = G(g(t) + ∇g(t))GT
gwino

= Gg(t)GT + G(GT [(A∇Y(t)AT ) (cid:12) (BT d(t)B)]G)GT ,
(15)
where Wwino denotes weight in Winograd form and t de-
notes iteration. We should notice that GGT (cid:54)= E, so we
keep weight W in the normal form (or called ’spatial do-
main’) instead of Winograd form (or called ’frequency do-
main’) during the training process in order to make DWM
be the equivalent substitute of normal convolution. Consid-
ering DWM, the two techniques mentioned above, we can
derive the corresponding back propagation rules. Denoting
output as Y , the two techniques have the same form in the
aggregation step:

Y =

(cid:88)

Y (i).

From the derivative rules we know that for part j:

∇Y (j) = ∇(j)Y (j) + ∇(j) (cid:88)

Y (i)

= ∇(j) (cid:88)

Y (i) = ∇(j)Y.

i(cid:54)=j

i

i

(16)

(17)

This means that we don’t need to store each output Y (i) for
the backward. Furthermore, it is easy to derive that in the
splitting step, the backward acts as follows:

∇d =

(cid:88)

∇d(i), ∇g =

(cid:88)

∇g(i),

(18)

where i denotes different parts produced by DWM.

Comparison and Discussion

We are not the ﬁrst one who tries to solve the large kernel
size problem that appeared during applying the Winograd
algorithm. Lu et al. (Lu et al. 2017) tried to implement the
Winograd algorithm on FPGAs, and they solved large ker-
nel problems by padding the kernel. However, the padded
elements will be ﬁlled with non-zero values after the Wino-
grad transformation, which means that paddings will bring
lots of extra calculations. For DWM, we ﬁnd a way that pre-
cisely separates the convolution operations without padding.
This method avoids any redundant ﬂoating-point multiplica-
tions during Winograd transformation, and thus achieves the
best acceleration without any numerical accuracy loss.

Furthermore, with the rise of neural architecture search
(NAS), convolutions with larger kernel sizes such as 5 × 5
or 7 × 7 have become more popular than ever. Networks
like ProxylessNAS (Cai, Zhu, and Han 2018) show that dif-
ferent computation environments will breed different neural
architectures. For example, when we want to implement our
neural networks on GPUs rather than on mobile CPUs, large
kernel sizes may be more suitable choices than small ones
because of the computation parallelism of GPUs. From be-
ginning to the end, the architecture of neural networks fol-
lows the most popular hardware, not the other way round.
Hence, we believe that reducing the computation overhead
of neural networks is worthy in any case.

Experiments

Setup

All the results were tested on NVIDIA V100 GPU. We have
implemented DWM on TensorFlow (Abadi et al. 2016) and
PyTorch (Paszke et al. 2017). On both platforms, the imple-
mented DWM performs like a plug-and-play operator, which
makes it convenient to use during inference and training. We
tested the numerical accuracy of the algorithms by doing
convolution with the standard normal distribution random
numbers on one single layer, and then calculate the mean
squared error (MSE) with FP64 convolution results. The nu-
merical accuracy on a single layer was tested on both two
platforms, and the results were the same. For all single layer
tests, the batch size is set to 256 and the layers are the same
padded. We measured the accuracy and FLOPs of networks
based on PyTorch. The traditional model architectures are
gained from TorchVision, and the ProxylessNAS (Cai, Zhu,
and Han 2018) analysis is based on the ofﬁcial PyTorch im-
plementation. The Networks’ accuracy was measured on Im-
ageNet 2012 (Russakovsky et al. 2015).

Numerical Accuracy of Single Layer

We estimated the mean squared error (MSE) between sev-
eral methods and the FP64 results by doing a forward con-
volution. The input signal and convolution weights are ran-
dom numbers generated by standard normal distribution us-
ing Numpy, set seed 11. We assumed two application sit-
uations: larger feature map (set 28 × 28) with fewer chan-
nels (set 128) and smaller feature map (set 14 × 14) with
more channels (set 256), which is consistent with the reality.

Table 1: Mean squared error (MSE) between different acceleration algorithms and the FP64 result by running a forward convolu-
tion, i.e FP64 convolution result is the baseline. H/W means the size of featur map, and FP32/FP16 indicates doing convolution
in FP32/FP16 format.

Kernel Size H/W Channel

Winograd FP32 Winograd FP16 DWM FP32 DWM FP16

3x3
3x3
5x5
5x5
7x7
7x7
9x9
9x9
11x11
11x11

14
28
14
28
14
28
14
28
14
28

256
128
256
128
256
128
256
128
256
128

Filters
256
128
256
128
256
128
256
128
256
128

FP32
2.21E-08
1.11E-10
1.05E-02
3.15E-10
6.13E-10
5.61E-10
9.90E-10
8.52E-10
1.47E-09
1.15E-09

FP16
2.71E-04
1.41E-04
6.93E-04
3.80E-04
1.25E-03
7.16E-04
1.90E-03
1.14E-03
2.60E-03
1.63E-03

5.24E-10
1.43E-10
7.14E-08
2.00E-08
1.47E-03
4.24E-04
5.31E-03
1.62E-03
1.48E-02
4.35E-03

1.44E-03
7.48E-04
1.07E-01
5.78E-02
NaN
NaN
NaN
NaN
NaN
NaN

5.32E-10
1.47E-10
1.47E-09
4.33E-10
2.97E-09
8.86E-10
3.67E-09
1.18E-09
5.30E-09
1.81E-09

3.42E-02
9.08E-03
9.72E-02
2.83E-02
1.97E-01
5.88E-02
2.36E-01
7.33E-02
3.46E-01
1.15E-01

Table 2: Top-1 accuracy, FLOPs and speedup of several acclerating algorithm on different networks. Origin means the original
top-1 accuracy and FLOPs of networks.

Network

AlexNet (Krizhevsky, Sutskever, and Hinton 2012)
GoogLeNet (Szegedy et al. 2015)
Inception-V3 (Szegedy et al. 2016)
ResNet-152 (He et al. 2016)
DenseNet-161 (Huang et al. 2017)
ProxylessGPU (Cai, Zhu, and Han 2018)
ProxylessMobile (Cai, Zhu, and Han 2018)

Origin

Acc
56.52
69.79
69.54
78.31
77.14
75.08
74.59

GFLOPs
0.71
1.51
2.86
11.62
7.88
0.49
0.35

Acc
56.51
69.79
69.47
78.31
77.13
74.77
74.47

Winograd
GFLOPs
0.56
0.97
2.34
8.78
6.32
0.49
0.34

speedup
1.28
1.55
1.22
1.32
1.25
1.01
1.01

Acc
56.51
69.77
69.46
78.31
77.12
75.06
74.57

DWM
GFLOPs
0.45
0.92
1.92
8.60
6.23
0.47
0.33

speedup
1.57
1.65
1.49
1.35
1.26
1.05
1.06

As shown in Table 1, (1) DWM has better numerical accu-
racy than the traditional Winograd algorithm almost in all
situations. It is obvious that the traditional Winograd algo-
rithm faced with a serious numerical accuracy problem as
the kernel size grows up. When the kernel size is 7 × 7 or
larger, the error of the FP32 Winograd algorithm approaches
FP16’s, which may cause accuracy problems. By contrast,
DWM’s numerical error stays at a low level, which is close
to the result of FP32, meaning that it can be applied to all
kinds of convolution operations without any problem. (2)
When using FP16, the traditional Winograd algorithm may
get an overﬂow. This may be caused by the intermediate re-
sults of the Winograd transformation. When kernel size be-
comes larger, the transformation matrices will be ﬁlled with
large numbers, and these numbers may make the results of
matrix multiplication become too large to be represented in
FP16. This also shows the advantages of using DWM in-
stead of the traditional Winograd algorithm. Furthermore,
the FP16 DWM is implemented without any optimization,
which means the numerical accuracy of it still can be im-
proved. (3) In some situations, DWM FP32 is more accurate
than FP32 ones. This is reasonable because DWM consumes
fewer multiplications, which may make it has better numer-
ical accuracy.

FLOPs Estimation on Single Layer

We calculated the FLOPs of convolutions with different ker-
nel sizes and 2 kinds of stride. Due to the decimals in the
transformation matrix in the traditional Winograd algorithm,

the FLOPs caused by transformation cannot be ignored, and
thus we only eliminate the inﬂuence of 0, ±1, ±2n and ± 1
2n
which can be easily implemented by shifting. As shown
in Table 3, we can get the following results: (1) DWM
cost less computation than the traditional Winograd algo-
rithm in all situations. As the kernel size becomes larger, the
FLOPs of traditional Winograd algorithm increases heav-
ily, and most FLOPs concentrates on the Winograd transfor-
mation because the transformation process becomes a non-
sparsity matrix multiplication. On the contrary, the speedup
of DWM keeps steady for its simple transformation matri-
ces. (2) DWM speeds up stride 2 convolutions by 2 ×, which
cannot be achieved by the traditional method. Not surpris-
ingly, due to the splitting method of DWM, the speedup of
stride 2 convolution still holds stably. These advantages lead
to stably speedup on almost all kinds of convolutions.

We also tested the actual runtime of convolution opera-
tions based on naive implementations of different kinds of
convolutions. The result was tested by nvprof, an NVIDIA
proﬁling tool. According to Table 4, we can conclude that:
(1) DWM outperforms the original Winograd algorithm, es-
pecially in large kernel situation. (2) DWM is faster than
cuDNN in some situations. Actually, when the size of the
feature map increases to around 100, cuDNN performs bet-
ter than DWM (not presented due to the space left). The in-
stability of cuDNN may be caused by some other acceler-
ating algorithms. Hence, both DWM and cuDNN have their
advantages. Furthermore, the naive DWM implementation
can be optimized by kernel fusion, soft pipeline and some

Table 3: The speedup of several acclerating algorithms on different kinds of convolutions. We assume that the output size is
ﬁxed to 14×14.

Kernel Size

Stride

Direct
FLOPs
1.76E+03
4.90E+03
9.60E+03
1.59E+04
2.37E+04
1.76E+03
4.90E+03
9.60E+03
1.59E+04
2.37E+04

Winograd

DWM

FLOPs
784
1.48E+04
9.72E+04
3.16E+05
1.07E+06
N/A
N/A
N/A
N/A
N/A

speedup
2.25
0.33
0.10
0.05
0.02
N/A
N/A
N/A
N/A
N/A

FLOPs
784
2.40E+03
4.90E+03
7.06E+03
1.10E+04
1.23E+03
2.40E+03
4.90E+03
8.28E+03
1.10E+04

speedup
2.25
2.04
1.96
2.25
2.15
1.44
2.04
1.96
1.92
2.15

1
1
1
1
1
2
2
2
2
2

3x3
5x5
7x7
9x9
11x11
3x3
5x5
7x7
9x9
11x11

Table 4: The actual runtime of several acclerating algorithms on different kinds of convolutions tested by nvprof. The batch
size, the channels and the ﬁlters are 256. The input size is ﬁxed to 14×14.

Kernel Size DWM(ms) Winograd(ms)

3x3
5x5
7x7
9x9
11x11

3.67
11.37
22.83
30.67
48.29

3.35
69.70
133.34
248.71
349.33

cuDNN(ms) Wino/DWM cuDNN/DWM
0.91
6.13
5.84
8.11
7.23

2.80
11.26
24.51
50.92
94.63

0.76
0.99
1.07
1.66
1.96

other optimization techniques unlike the simple traditional
3x3 Winograd algorithm. Since we mainly focuses on speed
up Winograd through the algorithm aspect in this paper, the
implementation optimization of DWM is our future work.

Total Analysis on Networks

Finally, we analyzed several representative networks. This
analysis includes the comparison of total FLOPs of the net-
work and the top-1 accuracy of inference on the ImageNet
2012 dataset. The top-1 accuracy of the two accelerating al-
gorithms is tested using FP16. As shown in Table 2, we can
conclude that: (1) the top-1 acc is very close. This result is
not surprising, because Table 1 shows that only large kernel
size will make DWM and the traditional Winograd algorithm
different. Thus, the networks most of which consist of 3 × 3
kernel convolutions like will get similar inference results.
(2) When there are more large kernels in the network such
as 11 × 11 in AlexNet and 1 × 5/7 or 5/7 × 1 in GoogLeNet
and Inception-v3, DWM performs better.

However, in some cases such as ResNet-152, most of the
computation is produced by 3 × 3 kernels. Worse, when it
comes to some modern NAS architectures, the calculation is
concentrated on 1 × 1 kernels because of the separable con-
volution. Over 90% amount of calculation of convolutional
neural networks is caused by convolution operations. Hence,
most of the architectures are designed or searched based
on separable convolutions to reduce the amouont of calcu-
lation. Although separable convolutions can cut the FLOPs
down effectively, it reduces the representation of the original
neural networks. When the computing power grows up and
more fast convolution algorithms are invented, FLOPs will
not be the main consideration of architecture designing.

Conclusion
In this paper, we propose a novel DWM to extend Wino-
grad’s minimal ﬁltering algorithm to a wide and general
convolutions. Winograd’s minimal ﬁltering algorithm has
been widely used to reduce the number of multiplications
for faster processing. However, it has the drawbacks of suf-
ferring from signiﬁcantly increased FLOPs and numerical
accuracy problem for kernel size larger than 3x3 and failing
on convolution with stride larger than 1, so it is only effec-
tive on convolutions with kernel size as 3x3 and stride as 1.
To solve this problems, we propose DWM to break through
the limitation of original Winograd’s minimal ﬁltering algo-
rithm on convolutions of large kernel and large stride. DWM
decomposes kernels with large size or large stride to several
small kernels with stride as 1 for further applying Winograd
method, so that DWM can reduce the number of multipli-
cations while keeping the numerical accuracy. Experimental
results show that the proposed DWM is able to support all
kinks of convolutions with a speedup of ∼2, without affect-
ing the numerical accuracy. These good properties of DWM
enables the fast exploring of larger kernel size and larger
stride value in CNNs for high performance, accuracy and
even the potential for new CNNs.

Acknowledgments
This work is partially supported by the National Key
Research and Development Program of China (under
Grant 2017YFA0700902), the NSF of China (under Grants
61432016, 61532016, 61672491, 61602441, 61602446,
61732002, 61702478, 61732007, 61732020 and 61906179),
Beijing Natural Science Foundation (JQ18013), the 973 Pro-
gram of China (under Grant 2015CB358800), National Sci-

ence and Technology Major Project (2018ZX01031102), the
Transformation and Transfer of Scientiﬁc and Technologi-
cal Achievements of Chinese Academy of Sciences (KFJ-
HGZX-013), Key Research Projects in Frontier Science of
Chinese Academy of Sciences (QYZDB-SSW-JSC001) ,
Strategic Priority Research Program of Chinese Academy
of Science (XDB32050200, XDC01020000) and Standard-
ization Research Project of Chinese Academy of Sciences
(BZ201800001).

References
[Abadi et al. 2016] Abadi, M.; Barham, P.; Chen, J.; Chen,
Z.; Davis, A.; Dean, J.; Devin, M.; Ghemawat, S.; Irving, G.;
Isard, M.; et al. 2016. Tensorﬂow: A system for large-scale
In 12th {USENIX} Symposium on Op-
machine learning.
erating Systems Design and Implementation ({OSDI} 16),
265–283.
[Barabasz and Gregg 2019] Barabasz, B., and Gregg, D.
2019. Winograd convolution for dnns: Beyond linear poli-
nomials. arXiv preprint arXiv:1905.05233.
[Budden et al. 2017] Budden, D.; Matveev, A.; Santurkar, S.;
Chaudhuri, S. R.; and Shavit, N. 2017. Deep tensor convo-
In Proceedings of the 34th Interna-
lution on multicores.
tional Conference on Machine Learning-Volume 70, 615–
624. JMLR. org.
[Cai, Zhu, and Han 2018] Cai, H.; Zhu, L.; and Han, S.
2018. Proxylessnas: Direct neural architecture search on tar-
get task and hardware. arXiv preprint arXiv:1812.00332.
[Chetlur et al. 2014] Chetlur, S.; Woolley, C.; Vandermersch,
P.; Cohen, J.; Tran, J.; Catanzaro, B.; and Shelhamer, E.
2014. cudnn: Efﬁcient primitives for deep learning. arXiv
preprint arXiv:1410.0759.
[Choi, El-Khamy, and Lee 2018] Choi, Y.; El-Khamy, M.;
and Lee, J. 2018. Compression of deep convolutional neu-
ral networks under joint sparsity constraints. arXiv preprint
arXiv:1805.08303.
[Cong and Xiao 2014] Cong, J., and Xiao, B. 2014. Min-
imizing computation in convolutional neural networks.
In
International conference on artiﬁcial neural networks, 281–
290. Springer.
[He et al. 2016] He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, 770–778.
[Hu, Shen, and Sun 2018] Hu, J.; Shen, L.; and Sun, G.
2018. Squeeze-and-excitation networks. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition, 7132–7141.
[Huang et al. 2017] Huang, G.; Liu, Z.; Van Der Maaten, L.;
and Weinberger, K. Q. 2017. Densely connected convolu-
tional networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, 4700–4708.
[Krizhevsky, Sutskever, and Hinton 2012] Krizhevsky, A.;
Sutskever, I.; and Hinton, G. E. 2012. Imagenet classiﬁca-
tion with deep convolutional neural networks. In Advances
in neural information processing systems, 1097–1105.

[Lavin and Gray 2016] Lavin, A., and Gray, S. 2016. Fast
algorithms for convolutional neural networks. In Proceed-
ings of the IEEE Conference on Computer Vision and Pat-
tern Recognition, 4013–4021.
[Li, Park, and Tang 2017] Li, S.; Park, J.; and Tang, P. T. P.
2017. Enabling sparse winograd convolution by native prun-
ing. arXiv preprint arXiv:1702.08597.
[Liu et al. 2018] Liu, X.; Pool, J.; Han, S.; and Dally, W. J.
2018. Efﬁcient sparse-winograd convolutional neural net-
works. arXiv preprint arXiv:1802.06367.
[Lu et al. 2017] Lu, L.; Liang, Y.; Xiao, Q.; and Yan, S. 2017.
Evaluating fast algorithms for convolutional neural networks
on fpgas. In 2017 IEEE 25th Annual International Sympo-
sium on Field-Programmable Custom Computing Machines
(FCCM), 101–108. IEEE.
[Madisetti 1997] Madisetti, V. 1997. The digital signal pro-
cessing handbook. CRC press.
[Maji et al. 2019] Maji, P.; Mundy, A.; Dasika, G.; Beu, J.;
Mattina, M.; and Mullins, R. 2019. Efﬁcient winograd
or cook-toom convolution kernel implementation on widely
used mobile cpus. arXiv preprint arXiv:1903.01521.
[Meng and Brothers 2019] Meng, L., and Brothers, J. 2019.
Efﬁcient winograd convolution via integer arithmetic. arXiv
preprint arXiv:1901.01965.
[Paszke et al. 2017] Paszke, A.; Gross, S.; Chintala, S.;
Chanan, G.; Yang, E.; DeVito, Z.; Lin, Z.; Desmaison, A.;
Antiga, L.; and Lerer, A. 2017. Automatic differentiation in
PyTorch. In NIPS Autodiff Workshop.
[Russakovsky et al. 2015] Russakovsky, O.; Deng, J.; Su, H.;
Krause, J.; Satheesh, S.; Ma, S.; Huang, Z.; Karpathy, A.;
Khosla, A.; Bernstein, M.; Berg, A. C.; and Fei-Fei, L. 2015.
In-
ImageNet Large Scale Visual Recognition Challenge.
ternational Journal of Computer Vision (IJCV) 115(3):211–
252.
[Simonyan and Zisserman 2014] Simonyan, K., and Zisser-
man, A. 2014. Very deep convolutional networks for large-
scale image recognition. arXiv preprint arXiv:1409.1556.
[Szegedy et al. 2015] Szegedy, C.; Liu, W.; Jia, Y.; Sermanet,
P.; Reed, S.; Anguelov, D.; Erhan, D.; Vanhoucke, V.; and
Rabinovich, A. 2015. Going deeper with convolutions. In
Proceedings of the IEEE conference on computer vision and
pattern recognition, 1–9.
[Szegedy et al. 2016] Szegedy, C.; Vanhoucke, V.; Ioffe, S.;
Shlens, J.; and Wojna, Z. 2016. Rethinking the incep-
tion architecture for computer vision. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, 2818–2826.
[Vasilache et al. 2014] Vasilache, N.; Johnson, J.; Mathieu,
M.; Chintala, S.; Piantino, S.; and LeCun, Y. 2014. Fast
convolutional nets with fbfft: A gpu performance evaluation.
arXiv preprint arXiv:1412.7580.
[Vincent et al. 2017] Vincent, K.; Stephano, K.; Frumkin,
M.; Ginsburg, B.; and Demouth, J. 2017. On improving
the numerical stability of winograd convolutions.
[Winograd 1980] Winograd, S. 1980. Arithmetic complexity
of computations, volume 33. Siam.

