Structured Receptive Fields in CNNs

J¨orn-Henrik Jacobsen1, Jan van Gemert1,2, Zhongyu Lou1, Arnold W. M. Smeulders1
1University of Amsterdam, The Netherlands
2TU Delft, The Netherlands
{j.jacobsen,z.lou,a.w.m.smeulders}@uva.nl, j.c.vangemert@tudelft.nl

6
1
0
2
 
y
a
M
 
3
1
 
 
]

V
C
.
s
c
[
 
 
2
v
1
7
9
2
0
.
5
0
6
1
:
v
i
X
r
a

Abstract

Learning powerful feature representations with CNNs is
hard when training data are limited. Pre-training is one
way to overcome this, but it requires large datasets sufﬁ-
ciently similar to the target domain. Another option is to de-
sign priors into the model, which can range from tuned hy-
perparameters to fully engineered representations like Scat-
tering Networks. We combine these ideas into structured
receptive ﬁeld networks, a model which has a ﬁxed ﬁlter
basis and yet retains the ﬂexibility of CNNs. This ﬂexibil-
ity is achieved by expressing receptive ﬁelds in CNNs as a
weighted sum over a ﬁxed basis which is similar in spirit
to Scattering Networks. The key difference is that we learn
arbitrary effective ﬁlter sets from the basis rather than mod-
eling the ﬁlters. This approach explicitly connects clas-
sical multiscale image analysis with general CNNs. With
structured receptive ﬁeld networks, we improve consider-
ably over unstructured CNNs for small and medium dataset
scenarios as well as over Scattering for large datasets. We
validate our ﬁndings on ILSVRC2012, Cifar-10, Cifar-100
and MNIST. As a realistic small dataset example, we show
state-of-the-art classiﬁcation results on popular 3D MRI
brain-disease datasets where pre-training is difﬁcult due to
a lack of large public datasets in a similar domain.

1. Introduction

Where convolutional networks have appeared enor-
mously powerful in the classiﬁcation of images when ample
data are available [14], we focus on smaller image datasets.
We propose structuring receptive ﬁelds in CNNs as linear
combinations of basis functions to train them with fewer
image data.

The common approach to smaller datasets is to per-
form pre-training on a large dataset, usually ImageNet [30].
Where CNNs generalize well to domains similar to the do-
main where the pre-training came from [28, 41], the per-
formance decreases signiﬁcantly when moving away from
the pre-training domain [41, 38]. We aim to make learning
more effective for smaller sets by restricting CNNs param-

Figure 1: A subset of ﬁlters of the ﬁrst structured recep-
tive ﬁeld CNN layer as trained on 100-class ILSVRC2012
and the Gaussian derivative basis they are learned from. The
network learns scaled and rotated versions of zero, ﬁrst, sec-
ond and third order ﬁlters. Furthermore, the ﬁlters learn to
recombine the different input color channels which is a cru-
cial property of CNNs.

eter spaces. Since all images are spatially coherent and hu-
man observers are considered to only cast local variations
up to a certain order as meaningful [11, 18] our key as-
sumption is that it is unnecessary to learn these properties
in the network. When visualizing the intermediate layers
of a trained network, see e.g. [40] and Figure 2, it becomes
evident that the ﬁlters as learned in a CNN are locally coher-
ent and as a consequence can be decomposed into a smooth
compact ﬁlter basis [12].

We aim to maintain the CNN’s capacity to learn general
variances and invariances in arbitrary images. Following
from our assumptions, the demand is posed on the ﬁlter set
that i) a linear combination of a ﬁnite basis set is capable of
forming any arbitrary ﬁlter necessary for the task at hand,
as illustrated in Figure 1 and ii) that we preserve the full
learning capacity of the network. For i) we choose the fam-
ily of Gaussian ﬁlters and its smooth derivatives for which
it has been proven [12] that 3-rd or 4-th order is sufﬁcient
to capture all local image variation perceivable by humans.
According to scale-space theory [11, 36], the Gaussian fam-
ily constitutes the Taylor expansion of the image function
which guarantees completeness. For ii) we maintain back-
propagation parameter optimization in the network, now
applied to learning the weights by which the ﬁlters are
summed into the effective ﬁlter set.

Figure 2: Filters randomly sampled from all layers of the
GoogLenet model [34], from left to right layer number in-
creases. Without being forced to do so, the model exhibits
spatial coherence (seen as smooth functions almost every-
where) after being trained on ILSVRC2012. This behaviour
reﬂects the spatial coherence of the input feature maps even
in the highest layers.

Similarly motivated, the Scattering Transform [2, 20,
31], a special type of CNN, uses a complete set of wavelet
ﬁlters ordered in a cascade. However, different from a clas-
sical CNN, the ﬁlters parameters are not learned by back-
propagation but rather they are ﬁxed from the start and the
whole network structure is motivated by signal processing
principles.
In the Scattering Network the choice of local
and global invariances are tailored to the type of images
speciﬁcally. In the Scattering Transform invariance to group
actions beyond local translation and deformation requires
explicit design [20] with the regards to the variability en-
countered in the target domain such as translation [2], rota-
tion [31] or scale. As a consequence, when the desired in-
variance groups are known a priori, Scattering delivers very
effective networks.
Our paper takes the best of two worlds. On the one hand,
we adopt the Scattering principle of using ﬁxed ﬁlter bases
as a function prior in the network. But on the other hand, we
maintain from plain CNNs the capacity to learn arbitrary ef-
fective ﬁlter combinations to form complex invariances and
equivariances.

Our main contributions are:

• Deriving the structured receptive ﬁeld network
(RFNN) from ﬁrst principles by formulating ﬁlter
learning as a linear decomposition onto a ﬁlter ba-
sis, unifying CNNs and multiscale image analysis in
a learnable model.

• Combining the strengths of Scattering and CNNs. We
do well on both domains: i) small datasets where Scat-
tering is best but CNNs are weak; ii) complex datasets
where CNNs excel but Scattering is weak.

• State-of-the-art classiﬁcation results on a small dataset
where pre-training is
The task is
Alzheimer’s disease classiﬁcation on two widely used
brain MRI datasets. We outperform all published re-
sults on the ADNI dataset.

infeasible.

2. Related Work

2.1. Scale-space: the deep structure of images

Scale-space theory [36] provides a model for the struc-
ture of images by steadily convolving the image with ﬁlters
of increasing scale, effectively reducing the resolution in
each scale step. While details of the image will slowly dis-
appear, the order by which they do so will uniquely encode
the deep structure of the image [11]. Gaussian ﬁlters have
the advantage in that they do not introduce any artifacts [18]
in the image while Gaussian derivative ﬁlters form a com-
plete and stable basis to decompose locally any realistic im-
age. The set of responses to the derivative ﬁlters describing
one patch is called the N-jet [5].

In the same vein, CNNs can be perceived to also model
the deep structure of images, this time in a non-linear fash-
ion. The pooling layers in a CNN effectively reduce reso-
lution of input feature maps. Viewed from the top of the
network down, the spatial extent of a convolution kernel is
increased in each layer by a factor 2, where a 5x5 kernel at
the higher layer measures 10x10 pixels on the layer below.
The deep structure in a CNN models the image on several
discrete levels of resolution simultaneously, precisely in line
with Scale-space theory.

Where CNNs typically reduce resolution by max pooling
in a non-linear fashion, Scale-space offers a linear theory
for continuous reduction of resolution. Scale-space theory
treats an image as a function of the mathematical apparatus
to reveal the local image structure. In this paper, we exploit
the descriptive power of Scale-space theory to decompose
the image locally on a ﬁxed ﬁlter basis of multiple scales.

2.2. CNNs and their parameters

CNNs [15] have large numbers of parameters to
learn [13]. This is their strength as they can solve ex-
tremely complicated problems [13, 35]. At the same time,
their number of unrestricted parameters is a limiting factor
in terms of the large amounts of data needed to train. To
prevent overﬁtting, which is an issue even when training on
large datasets like the million images of the ILSVRC2012
challenge [30], usually regularization is imposed with meth-
ods like dropout [33] and weight decay [22]. Regulariza-
tion is essential to achieving good performance. In cases
where limited training data are available, CNN training
quickly overﬁts regardless and the learned representations
do not generalize well. Transfer learning from models pre-
trained in similar domains to the new domain is necessary
to achieve competitive results [23]. One thing pre-training
on large datasets provides is knowledge about properties in-
herent to all natural images, such as spatial coherence and
In this paper, we
robustness to uninformative variability.
aim to design these properties into CNNs to improve gener-
alization when limited training data are available.

2.3. The Scattering representation

To reduce model complexity we draw inspiration from
the elegant convolutional Scattering Network [2, 20, 31].
Scattering uses a multi-layer cascade of a pre-deﬁned
wavelet ﬁlter bank with nonlinearity and pooling operators.
It computes a locally translation-invariant image represen-
tation, stable to deformations while avoiding information
loss by recovering wavelet coefﬁcients in successive lay-
ers. No learning is used in the image representation: all
relevant combinations of the ﬁlters are fed into an SVM-
classiﬁer yielding state-of-the-art results on small dataset
classiﬁcation. Scattering is particularly well-suited to small
datasets because it refrains from feature learning. Since all
ﬁlter combinations are pre-deﬁned, their effectiveness is in-
dependent of dataset size.
In this paper, we also beneﬁt
from a ﬁxed ﬁlter bank. In contrast to Scattering, we learn
linear combinations of a ﬁlter basis into effective ﬁlters and
non-linear combinations thereof.

The wavelet ﬁlterbank of Scattering is carefully designed
to sample a range of rotations and scales. These ﬁlters
and their properties are grounded in wavelet theory [19]
and exhibit precisely formulated properties. By using in-
terpretable ﬁlters, Scattering can design invariance to ﬁnite
groups such as translation [2], scale and rotation [31]. Hard
coding the invariance into the network is effective when
the problem and its invariants are known precisely, but for
many applications this is rarely the case. When the vari-
ability is unknown, additional Scattering paths have to be
computed, stored and processed exhaustively before classi-
ﬁcation. This leads to a well-structured but very high di-
mensional parameter space. In this paper, we use a Gaus-
sian derivatives basis as the ﬁlter bank, ﬁrmly grounded in
scale-space theory [11, 18, 36]. Our approach incorporates
learning effective ﬁlter combinations from the very begin-
ning, which allows for a compact representation of the prob-
lem at hand.

2.4. Recent CNNs

Restriction of parameter spaces has led to some major
advances in recent CNNs performance. Network in Net-
work [17] and GoogleNet [34] illustrate that fully connected
layers, which constitute most of Alexnet’s parameters, can
be replaced by a global average pooling layer reducing the
number of parameters in the fully connected layers to virtu-
ally zero. The number of parameters in the convolution lay-
ers is increased to enhance the expressiveness of each layers
features. Overall the total number of parameters is not nec-
essarily decreased, but the function space is restricted, al-
lowing for bigger models while classiﬁcation accuracy im-
proves [17, 34].

The VGG Network [32] improves over Alexnet in a dif-
ferent way. The convolution layers parameter spaces are
restricted by splitting each 5x5 convolution layer into two

3x3 convolution layers. 5x5 convolutions and 2 subsequent
3x3 convolutions have the same effective receptive ﬁeld size
while each receptive ﬁeld has 18 instead of 25 trainable pa-
rameters. This regularization enables learning larger mod-
els that are less prone to overﬁtting. In this paper, we follow
a different approach in restricting the free parameter space
without reducing ﬁlter size.

3. Deep Receptive Field Networks

3.1. Structured receptive ﬁelds

In our structured receptive ﬁeld networks we make
the relationship between Scale-space and CNNs explicit.
Whereas normal CNNs treat images and their ﬁlters as pixel
values, we aim for a CNN that treats images as functions in
Scale-space. Thus, the learned convolution kernels become
functions as well. We therefore approximate an arbitrary
CNN ﬁlter F (x) with a Taylor expansion around a up to
order M

F (x) =

(x − a)m.

(1)

M
(cid:88)

m=0

F m(a)
m!

Scale-space allows us to use differential operators on im-
ages, due to linearity of convolution we are able to compute
the exact derivatives of the scaled underlying function by
convolution with derivatives of the Gaussian kernel

G(.; σ) ∗ F (x) =

N
(cid:88)

m=0

(Gm(.; σ) ∗ F )(a)
m!

(x − a)m,

(2)

where ∗ denotes convolution, G(.; σ) is a Gaussian kernel
with scale σ and Gm(.; σ) is the mth order Gaussian deriva-
tive with respect to it’s spatial variable. Thus, a convolu-
tion with a basis of weighted Gaussian derivatives receptive
ﬁelds is the functional equivalent to pixel values in a stan-
dard CNN operating on a scaled inﬁnitely differentiable ver-
sion of the image.

To construct the full basis set in practice, one can show
that the Hermite polynomials emerge from a sequence of
Gaussian derivatives up to order M [29]. A Gaussian
derivative of arbitrary order can be obtained from the or-
thogonal Hermite polynomials Hm through pointwise mul-
tiplication with a Gaussian envelope

Gm(.; σ) = (−1)m 1
√

σm Hm(

σ

x
√

2

) ◦ G(x; σ).

(3)

The resulting operators allow computation of an image’s
local geometry at scale σ and location x up to any order
of precision M . This basis is thus a complete set. Each
derivative corresponds to an independent degree of free-
dom, making it also a minimal set.
Thus, an RFNN is a general CNN when a complete
polynomial up to inﬁnite order is considered. We restrict

the basis based on the requirement that one can construct
quadrature pair ﬁlters as suggested by Scattering and by
evidence from Scale-space theory [12] that considers all
orders up to a maximum of 4, as it has been suggested that
orders beyond that do not carry any information meaningful
to visual perception.

3.2. Transformation properties of the basis

The isotropic Gaussian derivatives exhibit multiple de-
sirable properties. It is possible to create complex multi-
orientation pyramids that constitute wavelet representations
similar to the Morlet Wavelet pyramids used in Scattering
Networks [2]. A complex multiresolution ﬁlterbank can
be constructed from a dilated and rotated Gaussian deriva-
tive quadrature. The exact dilated versions of an arbitrary
Gaussian derivative Gm can be obtained through convolu-
tion with a Gaussian kernel of scale σ = n according to

(cid:112)

Gm(.;

j2 + n2) = Gm(.; j) ∗ G(.; n).

(4)

Arbitrary rotations of Gaussian derivative kernels can be ob-
tained from a minimal set of basis ﬁlters without the need to
rotate the basis itself. This property is referred to as steer-
ability [6]. Steerability is a property of all functions that can
be expressed in a polynomial in x and y times an isotropic
Gaussian. This certainly holds for the Gaussian derivatives
according to equation 3. For example a quadrature pair of
2nd and 3rd order Gaussian derivatives Gxx and Gxxx ro-
tated by an angle θ can be obtained from a minimal 3 and 4
x-y separable basis set given by

Gxx

θ = cos2(θ)Gxx − 2 cos(θ) sin(θ)Gxy + sin2(θ)Gyy
θ = cos3(θ)Gxxx − 3 cos2(θ) sin(θ)Gxxy
+3 cos(θ) sin2(θ)Gxyy − sin3(θ)Gyyy

Gxxx

A general derivation of the minimal basis set necessary for
steering arbitrary orders can be found in [6]. Note that the
anisotropic case can be constructed in analogous manner
according to [26]. This renders Scattering as a special case
of the RFNN for ﬁxed angles and scales, given a proper
choice of pooling operations and possibly skip connections
to closely resemble the architecture described in [2].
In
practice this allows for seamless integration of the Scatter-
ing concept into CNNs to achieve a variety of hybrid archi-
tectures.

3.3. Learning basis ﬁlter parameters

Learning a feature representation boils down to convo-
lution kernel learning. Where a classical CNN learns pixel
values of the convolutional kernel, a RFNN learns Gaus-
sian derivative basis function weights that combine to a

Algorithm 1 RFNN Learning - updating the parameters αl
ij
between input map indexed by i and output map indexed by
j of layer l in the Mini-batch Gradient Decent framework.
1: Input: input feature maps ol−1

for each training sam-
ple (computed for the previous layer, ol−1 is the input
image when l = 1), corresponding ground-truth labels
{y1, y2, . . . , yK}, the basic kernels {φ1, φ2, . . . , φM },
previous parameter αl

i

ij.

2: compute the convolution {ζ1, ζ2, . . . , ζm} of {ol−1

i}

respect to the basic kernels {φ1, φ2, . . . , φM }

3: obtain the output map ol

j = αl

ij1 · ζ1 + αl

ij2 · ζ2 + ... +

αl

ijM · ζM
4: compute the δl

map ol
j

tion

jn for each output neuron n of the output

5: compute the derivative ψ(cid:48)(tl

jn) of the activation func-

6: compute the gradient ∂E
∂αl
ij
ij = αl

7: update parameter αl
is the learning rate

respect to the weights αl
ij
K · (cid:80)K
ij − r · 1

k=1[ ∂E
∂αl
ij

]k, r

8: Output: αl

ij, the output feature maps ol
j

(5)

Figure 3: An illustration of the basic building block in an
RFNN network. A linear comibination of a limited basis
ﬁlter set φm yields an arbitrary number of effective ﬁlters.
The weights αij are learned by the network.

convolution kernel function. A 2D ﬁlter kernel function
F (x, y) in all layers, is a linear combination of i unique
(non-symmetric) Gaussian derivative basis functions φ

F (x, y) = α1φ1 + · · · + αnφi,

(6)

where α1, ..., αi are the parameters being learned.

We learn the ﬁlter’s weights α by mini-batch stochas-
tic gradient descent and compute the derivatives of the loss
function E with respect to the parameters α through back-
propagation. It is straightforward to show the independence
between the basis weights α and the actual basis (see Ap-
pendix for derivation). Thus, we formulate the basis learn-

ing as a combination of a ﬁxed basis layer with a 1x1 convo-
lution layer that has a kernel depth equal to the basis order.
Propagation through the 1x1 layer is done as in any CNN
while propagation through the basis layer is achieved by
a convolution with ﬂipped versions of the Gaussian ﬁlters.
This makes it straightforward to include into any existing
deep learning framework. The basic structured receptive
ﬁeld building block is illustrated in ﬁgure 3, showing how
each effective ﬁlter is composed out of multiple basis ﬁlters.
Note that the linearity of convolution allows us to never ac-
tually compute the effective ﬁlters. Convolving with effec-
tive ﬁlters is the same as convolving with the basis and then
recombining the feature maps, allowing for efﬁcient imple-
mentation. Algorithm 1 shows how the parameters are up-
dated.

3.4. The network

In this work, we choose the Network in Network (NiN)
architecture [17] as the basis into which we integrate the
structured receptive ﬁelds.
It is particularly suited for an
analysis of the RFNN approach, as the absence of a fully
connected layer ensures all parameters to be fully concerned
with re-combining basis ﬁlter outcomes of the current layer.
At the same time, it is powerful, similar in spirit to the state
of the art Googlenet [34], while being comparably small and
fast to train.

NiN alternates one spatial convolution layer with 1x1
convolutions and pooling. The 1x1 layers form non-linear
combinations of the spatial convolution layers outputs. This
procedure is repeated four times in 16 layers, with different
number of ﬁlters and kernel sizes for the spatial convolu-
tion layer. The ﬁnal pooling layer is a global average pool-
ing layer. Each convolution layer is followed by a rectiﬁer
nonlinearity. Details on the different NiNs for Cifar and
Imagenet can be found in the Caffe model zoo [9].

In the RFNN version of the Network in Network model,
the basis layer including the Gaussian derivatives set is re-
placing the spatial convolution layer and corresponds to φm
in equation 6. Thus, each basis convolution layer has a num-
ber of ﬁlters depending on order and scale of the chosen
basis set. The basis set is ﬁxed: no parameters are learned
in this layer. The linear re-combination of the ﬁlter basis is
done by the subsequent 1x1 convolution layer, correspond-
ing to αij in equation 6. Note that there is no non-linearity
between φm and αij layer in the RFNN case, as the com-
binations of the ﬁlters are linear. Thus the RFNN model is
almost identical to the standard Network in Network.
We evaluate the model with and without multiple scales σs.
When including scale, we extract 4 scales, as the original
model includes 3 pooling steps and thus operates on 4 scales
In the ﬁrst layer we directly compute 4 scales,
at least.
sampled continuously with σs = 2s where s = scale as
done in [2]. In each subsequent layer we discard the lowest

scale. The dimensionality reduction by max pooling renders
it meaningless to insert the lowest scale of the previous layer
into the ﬁlter basis set as it is already covered by the pyra-
midal structure of the network. This enables us to save on
basis ﬁlters in the higher layers of the network. In conclu-
sion we reduce the total number of 2D ﬁlters in the network
from 520,000 in the standard Network in Network to be-
tween 12 and 144 in the RF Network in Network (RFNiN),
while retaining the models expressiveness as shown in the
experimental section.

4. Experiments

The experiments are partitioned into four parts.

i) We
show insight in the proposed model to investigate design
choices; ii) we show that our model combines the strengths
of Scattering and CNNs; iii) we show structured recep-
tive ﬁelds improve classiﬁcation performance when limit-
ing training data; iv) we show a 3D version of our model
that outperforms the state-of-the-art, including a 3D-CNN,
on two brain MRI classiﬁcation datasets where large pre-
training datasets are not available. We use the Caffe li-
brary [9] and Theano [1] where we added RFNN as a sepa-
rate module. Code is available on github1.

4.1. Experiment 1: Model insight

The RFNN used in this section is the structured recep-
tive ﬁeld version of the Network in Network (RFNiN) in-
troduced in section 3.3. We gain insight into the model by
evaluating the scale and order of the basis ﬁlters. In addi-
tion, we analyze the performance compared to the standard
Network in Network (NiN) [17] and Alexnet [13] and show
that our proposed model is not merely a change in architec-
ture. To allow overnight experiments we use the 100 largest
classes of the ILSVRC2012 ImageNet classiﬁcation chal-
lenge [30]. Selection is done by folder size, as more than
100 classes have 1,300 images in them, yielding a dataset
size of 130,000 images. This is a real-world medium sized
dataset in a domain where CNNs excel.

Experimental setup. The Network in Network (NiN)
model and our Structured Receptive Field Network in
Network (RFNiN) model are based on the training deﬁ-
nitions provided by the Caffe model zoo [9]. Training is
done with the standard procedure on Imagenet. We use
stochastic gradient descent, a momentum of 0.9, a weight
decay of 0.0005. The images are re-sized to 256x256,
mirrored during training and the dataset mean is subtracted.
The base learning rate was decreased by a factor of 10,
according to the reduction from 1,000 to 100 classes, to
ensure proper scaling of the weight updates, NiN didn’t
converge with the original learning rate. We decreased it
by a factor of 10 after 50,000 iterations and again by the

1https://github.com/jhjacobsen/RFNN

Top-1

2DFilters #Params

ILSVRC2012-100 Subset

Method
RFNiN 1st-order
RFNiN 2nd-order
RFNiN 3rd-order
RFNiN 4th-order
RFNiN-Scale 1st-order
RFNiN-Scale 2nd-order
RFNiN-Scale 3rd-order
RFNiN-Scale 4th-order

44.83% 12
61.24% 24
63.64% 40
62.92% 60
57.21% 24
67.56% 54
69.65% 94
68.95% 144

Network in Network
Alexnet

67.30% 520k
54.86% 370k

1.8M
3.4M
5.5M
8.1M
2.2M
4.2M
6.8M
10.1M

8.2M
60.0M

Table 1: Results on 100 Biggest ILSVRC2012 classes: The
table shows RFNiN with 1st, 2nd, 3rd and 4th order ﬁlters
in the whole network. Row 1-4 are applying basis ﬁlters in
all layers on a scale of σ=1. RFNiN-scale in row 5-8 ap-
plies basis ﬁlters on 4 scales, where σ=1,2,4,8. The results
show that a 3rd order basis is sufﬁcient while incorporating
scale into the network gives a big gain in performance. The
RFNiN is able to outperform the same Network in Network
architecture.

same factor after 75,000 iterations. The networks were
trained for 100,000 iterations. Results are computed as
the mean Top-1 classiﬁcation accuracy on the validation set.

Filter basis order. In table 1, the ﬁrst four rows show
the result of RFNiN architectures with 1st to 4th order
Gaussian derivative basis ﬁlter set comprised of 12 to 60
individual Gaussian derivative ﬁlters in all layers of the net-
work. In these experiments the value of σ=1, ﬁxed for all
ﬁlters and all layers. Comparing ﬁrst to fourth order ﬁlter
basis in table 1, we conclude that third order is sufﬁcient,
outperforming ﬁrst and second order as predicted by Scale-
space theory [12]. The fourth order does not add any more
gain.

Filter scale. The RFNiN-Scale entries of table 1 show
the classiﬁcation result up to fourth order now with 4 differ-
ent scales, σ=1, 2, 4, 8 for the lowest layer, σ=1, 2, 4 for the
second layer, σ=1, 2 for the third, and σ=1 for the fourth.
This implies that the basis ﬁlter set expands from 24 up to
144 ﬁlters in total in the network. Comparing the use of sin-
gle scale ﬁlters in the network to dilated copies of the ﬁlters
with varying scale indicates that a considerable gain can be
achieved by including ﬁlters with different scales. This ob-
servation is supported by Scattering [2], showing that the
multiple scales can directly be extracted from the ﬁrst layer
on. In fact, normal CNNs are also capable of similar be-
havior, as positive valued low-pass ﬁlter feature maps are
not affected by rectiﬁer nonlinearities [31]. Thus, scale can

Figure 4: Mean of ﬁlter weights and variances per layer for
15 basis ﬁlters with no scale, as trained on ILSVRC2012-
100 subset. Note that the lower order ﬁlters have the highest
weights while zero-order ﬁlters are most effective in higher
layers for combinations of lower responses.

directly be computed from the ﬁrst layer onwards, which
yields a much smaller set of basis ﬁlters and fewer convo-
lutions needed in the higher layers. Note that number of
parameters is not directly correlated with performance.

Analysis of network layers. For the network RFNiN
4th-order Figure 4 provides an overview of the range of ba-
sis weights per effective ﬁlters in all layers, where the x-
axis indexes the spatial derivative index and y-axis the mean
value plus standard deviation of weights per layer over all
effective ﬁlter kernels. The ﬁgure indicates that weights de-
crease towards higher orders as expected. Furthermore zero
order ﬁlters have relatively high weights in higher layers,
which hints to passing on scaled incoming features.

Comparison to Network in Network. The champion
RFNiN in table 1 slightly outperforms the Network in
Network with the same setting and training circumstances
while only having 94 instead 520,000 spatial ﬁlters in the
network in total. Note that the number of parameters is rel-
atively similar though, as the scale component increases the
number of basis functions per ﬁlter signiﬁcantly. The result
shows that our basis representation is sufﬁcient for complex
tasks like Imagenet.

Refactorize Network in Network. To illustrate that our
proposed model is not merely a change in architecture we
compare to a third architecture. We remove the Gaussian
basis and we re-factorize the NiN such that it becomes iden-
tical to RFNiN. Both have almost the same number of pa-
rameters, but the NiN-factorize has a freely learnable ba-
sis. Re-factorizing only the ﬁrst layer and leaving the rest
of the network as in the original NiN, in table 2 we show
that a Gaussian basis is superior to a learned basis. When
re-factorizing all layers, RFNiN-Scale 3rd-order results are
superior by far to the identical NiN-factorize All Layers.

Model

Basis #Params Top-1

Model

Cifar-10 Cifar-100

NiN-refactor Layer 1
RFNiN-refactor Layer 1 Gauss

Free

NiN-refactor All Layers
RFNiN-Scale 3rd-order

Free
Gauss

7.47M
7.47M

6.87M
6.83M

64.10%
68.63%

38.02%
69.65%

Table 2: Classiﬁcation on ILSVRC2012-100 to illustrate in-
ﬂuence of factorization on performance. The results show
that the advantage of the Gaussian basis is substantial and
our results are not merely due to a change in architecture.

Roto-Trans Scattering
RFNiN

82.30% 56.80%
86.31% 63.81%

RCNN

91.31% 68.25%

Table 3: Comparison against Scattering on a large complex
domain. State-of-the-art comparison is given by RCNN.
RFNiN outperforms Scattering by large margins.

recently introduced Deep Roto-Translation Scattering ap-
proach [24], a powerful variant of Scattering networks ex-
plicitly modeling invariance under the action of small rota-
tions. This is a domain where CNNs excel and learning of
complex image variabilities is key.

The RFNiN is again a variant of the standard NiN for
It is similar to the model in experiment 1, just
Cifar-10.
that it has one basis layer, two 1x1 convolution layers and
one pooling layer less and the units in the 1x1 convolution
layers are 192 in the whole network. Furthermore, we show
performance of the state-of-the-art recurrent convolutional
networks (RCNNs) [16] for comparison.

The results in Table 3 show a considerable improvement
on Cifar-10 and Cifar-100 when comparing RFNiN to
Roto-Translation Scattering [24], which was designed
speciﬁcally for this dataset. RCNNs performance is consid-
erably higher as they follow a different approach to which
structured receptive ﬁelds can also be applied if desired.

RFNNs are robust to dataset size. From these experi-
ments, we conclude that RFNNs combine the best of both
worlds. We outperform CNNs and compete with Scatter-
ing when training data is limited as exempliﬁed on subsets
of MNIST. We capture complex image variabilities beyond
the capabilities of Scattering representations as exempliﬁed
on the datasets Cifar-10 and Cifar-100 despite operating in a
similarly smooth parameter space on a receptive ﬁeld level.

4.3. Experiment 3: Limiting datasize

To demonstrate the effectiveness of the RF variant com-
pared to the Network in Network, we reduce the number of
classes in the ILSVRC2012-dataset from 1000 to 100 to 10,
resulting in a reduction of the total number of images on
which the network was trained from 1.2M to 130k to 13k
and subsequent decrease in visual variety to learn from. To
demonstrate performance is not only due to smaller number
of learnable parameters, we evaluate two RFNiN versions.
RFNiN-v1 is RFNiN-Scale 3rd-order from table1. RFNiN-
v2 is one layer deeper and wider [128/128/384/512/1000]
version of the RFNiN-v1, resulting in 3 million additional
parameters, which is 2,5 million more than NiN.

The results in table 4 show that compared to CNNs the

Figure 5: Classiﬁcation performance of the Scattering Net-
work on various subsets of the MNIST dataset.
In com-
parison the state of the art CNN-A from [27]. RFNN de-
notes our receptive ﬁeld network, with the same architecture
as CNN-B. Both are shown, to illustrate that good perfor-
mance of the RFNN is not due to the CNN architecture, but
due to RFNN decomposition. Our RFNN performs on par
with Scattering, substantially outperforming both CNNs.

4.2. Experiment 2: Scattering and RFNNs

Small simple domain. We compare an RFNN to Scat-
tering in classiﬁcation on reduced training sizes of the
MNIST dataset. This is the domain where Scattering out-
performs standard CNNs [2]. We reduce the number of
training samples when training on MNIST as done in [2].
The network architecture and training parameters used in
this section are the same as in [39]. The RFNN contains 3
layers with a third order basis on one scale as a multiscale
basis didn’t provide any gain. Scale and order are deter-
mined on a validation set. Each basis layer is followed by
a layer of αN = 64 1x1 units that linearly re-combine the
basis ﬁlters outcomes. As comparison we re-implement the
same model as a plain CNN. The CNN and Scattering re-
sults on the task are taken from [2, 27].

Results are shown in Figure 5, each number is averaged
over 3 runs. For the experiment on MNIST the gap be-
tween the CNNs and networks with pre-deﬁned ﬁlters in-
creases when training data is reduced, while RFNN and
Scattering perform on par even at the smallest sample size.
Large complex domain. We compare against Scattering
on the Cifar-10 and Cifar-100 datasets, as reported by the

Model

#Params

1000-class

100-class

10-class

3D MRI classiﬁcation

Accuracy TPR

SPC

NiN
RFNiN-v1
RFNiN-v2

7.5M
6.8M
10M

56.78%
50.08%
54.04%

67.30% 76.97%
69.65% 85.00%
70.78% 83.36%

3D-RFNiN (ours)
ICA [37]
Voxel-Direct-D-gm [4]

97.79% 97.14% 98.78%
81.90% 79.50%
80.70%
81.00% 95.00%
-

Table 4: Three classiﬁcation experiments on ILSVRC2012
subsets. Results show that the bigger model (RFNiN-v2)
performs better than RFNiN-Scale 3rd-order (RFNiN-v1) on
the 1000-classes while on 100-class and 10-class, v1 and
v2 perform similar. The gap between RFNiN and NiN in-
creases for fewer classes.

RFNiN performance is better relatively speaking when the
number of samples and thus the visual variety decreases.
For the 13k ILSVRC2012-10 image dataset the gap be-
tween RFNiN and NiN increased to 8.0% from 2.4% for the
130k images in ILSVRC2012-100 while the best RFNiN is
inferior to NiN by 2.98% for the full ILSVRC2012-1000.
This supports our aim that RFNiN is effectively incorpo-
rating natural image priors, yielding a better performance
compared to the standard NiN when training data and va-
riety is limited, even when having more learnable parame-
ters. Truly large datasets seem to contain information not
yet captured by our model.

4.4. Experiment 4: Small realistic data

We apply an RFNiN to 3D brain MRI classiﬁcation for
Alzheimer’s disease [4] on two popular datasets. Neu-
roimaging is a domain where training data is notoriously
small and high dimensional and no truly large open access
databases in a similar domain exist for pre-training.

We use a 3-layer RFNiN with ﬁlters sizes [128,96,96]
with a third order basis in 3 scales σ ∈ {1, 4, 16}. This
time wider spaced, as the brains are very big objects and are
centered due to normalization to MNI space with the FSL
library [8]. Each basis layer is followed by one 1x1 convo-
lution layer. Global average pooling is applied to the ﬁnal
feature maps. The network is implemented in Theano [1]
and trained with Adam [10].

The results are shown in table 5. Note that [7, 25] train
on their own subset and use an order of magnitude more
training data. We follow standard practice [4] and train on
a smaller subset. Nevertheless we outperform all published
methods on the ADNI dataset. The same 3 layer NiN as our
RFNiN model has 84.21% accuracy, more than 10% worse
while being hard to train due to unstable convergence. On
the OASIS AD-126 Alzheimer’s dataset [21], we achieve
an accuracy of 80.26%, compared to 74.10% with a SIFT-
based approach [3]. Thus, we show our RFNiN can ef-
fectively learn comparably deep representations even when
data is scarce and exhibits stable convergence properties.

3D-CNN [25]
NIB [7]

95.70%
94.74%

-
95.24% 94.26%

-

Table 5: Alzheimer’s classiﬁcation with 150 train and test
3D MRI images from the widely used ADNI benchmark.
RFNiN, ICA and Voxel-Direct-D-gm are trained on the sub-
set introduced in [4], 3D-CNN and NIB were trained on
their own subset of ADNI, using an order of magnitude
more training data. RFNiN outperforms all published re-
sults. Reported is accuracy, true positive rate and speciﬁcity.

5. Discussion

The experiments show that structuring convolutional lay-
ers with a ﬁlter basis grounded on Scale-space principles
improves performance when data is limited. The ﬁlter ba-
sis provides regularization especially suited for image data
by restricting the parameter space to smooth features up to
fourth order. The Gaussian derivative basis opens up a new
perspective for reasoning in CNNs, connecting them with
a rich body of prior multiscale image analysis research that
can now be readily incorporated into the models. This is
especially interesting for applications where model insight
and control is key.

We illustrated the effectiveness of RFNNs on multi-
ple subsets of Imagenet, Cifar-10, Cifar-100 and MNIST.
The choice of a third order Gaussian basis is sufﬁcient to
tackle all datasets which is in accordance with prior re-
search [12, 2]. While it remains an open problem to match
the performance of CNNs on very large datasets like the
1000-class ILSVRC2012, our results show that the RFNN
method outperforms CNNs by large margins when data
are scarce.
It can also outperform CNNs on challenging
medium sized datasets while being superior to Scattering
on large datasets despite having more parameters as the
pre-deﬁned basis restriction allows the network to devote
its full capacity to a sensible feature spaces. As a small
data real world example, we verify our claims with 3D MRI
Alzheimer’s disease classiﬁcation on two datasets where we
consistently achieve competitive performance including the
best results on the widely used ADNI dataset.

Acknowledgements. We thank Rein van den Boom-
gaard and Silvia-Laura Pintea for insightful discussions.
This work has been conducted with data from the
Alzheimer’s Disease Neuroimaging Initiative (ADNI) and
the Open Access Series of Imaging Studies (OASIS).

References

[1] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. Good-
fellow, A. Bergeron, N. Bouchard, D. Warde-Farley, and
Y. Bengio. Theano: new features and speed improvements.
arXiv preprint arXiv:1211.5590, 2012.

[2] J. Bruna and S. Mallat. Invariant scattering convolution net-

works. TPAMI, 35(8):1872–1886, 2013.

[3] Y. Chen, J. Storrs, L. Tan, L. J. Mazlack, J.-H. Lee, and
L. J. Lu. Detecting brain structural changes as biomarker
from magnetic resonance images using a local feature based
svm approach. Journal of neuroscience methods, 221:22–31,
2014.

[4] R. Cuingnet, E. Gerardin,

J. Tessieras, G. Auzias,
S. Leh´ericy, M.-O. Habert, M. Chupin, H. Benali, O. Col-
liot, A. D. N. Initiative, et al. Automatic classiﬁcation of
patients with alzheimer’s disease from structural mri: a com-
parison of ten methods using the adni database. neuroimage,
56(2):766–781, 2011.

[5] L. M. Florack, B. M. ter Haar Romeny, J. J. Koenderink,
and M. A. Viergever. Scale and the differential structure of
images. Image and Vision Computing, 10(6):376–388, 1992.

[6] W. T. Freeman and E. H. Adelson. The design and use of

steerable ﬁlters. TPAMI, (9):891–906, 1991.

[7] A. Gupta, M. Ayhan, and A. Maida. Natural image bases to

represent neuroimaging data. In ICML, 2013.

[8] M. Jenkinson, C. F. Beckmann, T. E. Behrens, M. W. Wool-
rich, and S. M. Smith. Fsl. Neuroimage, 62(2):782–790,
2012.

[9] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolu-
tional architecture for fast feature embedding. arXiv preprint
arXiv:1408.5093, 2014.

[10] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. arXiv preprint arXiv:1412.6980, 2014.

[11] J. J. Koenderink. The structure of images. Biological cyber-

netics, 50(5):363–370, 1984.

[12] J. J. Koenderink and A. J. van Doorn. Representation of lo-
cal geometry in the visual system. Biological cybernetics,
55(6):367–375, 1987.

[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012.

Imagenet
In

[14] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature,

521(7553):436–444, 2015.

[15] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278–2324, 1998.

[16] M. Liang and X. Hu. Recurrent convolutional neural network

for object recognition. In CVPR, 2015.

[20] S. Mallat. Group invariant scattering. Communications on

Pure and Applied Mathematics, 65(10):1331–1398, 2012.

[21] D. S. Marcus, T. H. Wang, J. Parker, J. G. Csernansky, J. C.
Morris, and R. L. Buckner. Open access series of imaging
studies (oasis): cross-sectional mri data in young, middle
aged, nondemented, and demented older adults. Journal of
cognitive neuroscience, 19(9):1498–1507, 2007.

[22] J. Moody, S. Hanson, A. Krogh, and J. A. Hertz. A simple
weight decay can improve generalization. NIPS, 1995.
[23] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Learning and
transferring mid-level image representations using convolu-
tional neural networks. In CVPR, 2014.

[24] E. Oyallon and S. Mallat. Deep roto-translation scattering for
object classiﬁcation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 2865–
2873, 2015.

[25] A. Payan and G. Montana. Predicting alzheimer’s disease: a
neuroimaging study with 3d convolutional neural networks.
arXiv preprint arXiv:1502.02506, 2015.

[26] P. Perona. Steerable-scalable kernels for edge detection and
junction analysis. In ECCV, pages 3–18. Springer, 1992.
[27] M. A. Ranzato, F. J. Huang, Y.-L. Boureau, and Y. LeCun.
Unsupervised learning of invariant feature hierarchies with
applications to object recognition. In CVPR. IEEE, 2007.

[28] A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson.
Cnn features off-the-shelf: an astounding baseline for recog-
nition. In CVPR, 2014.

[29] B. M. H. Romeny. Front-end vision and multi-scale image
analysis: multi-scale computer vision theory and applica-
tions, written in mathematica, volume 27. Springer Science
& Business Media, 2008.

[30] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge. IJCV, pages 1–42, 2015.

[31] L. Sifre and S. Mallat. Rotation, scaling and deformation in-
variant scattering for texture discrimination. In CVPR, 2013.

[32] K. Simonyan and A. Zisserman.

Very deep con-
large-scale image recognition.

volutional networks for
arXiv:1409.1556, 2014.

[33] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov. Dropout: A simple way to prevent neural
networks from overﬁtting. JMLR, 2014.

[34] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. arXiv:1409.4842, 2014.

[35] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface:
Closing the gap to human-level performance in face veriﬁca-
tion. In CVPR. IEEE, 2014.

[17] M. Lin, Q. Chen, and S. Yan. Network in network.

[36] A. P. Witkin. Scale-space ﬁltering.

In International Joint

arXiv:1312.4400, 2013.

[18] T. Lindeberg. Scale-space theory in computer vision, volume

256. Springer Science & Business Media, 2013.

[19] S. Mallat. A wavelet tour of signal processing. Academic

press, 1999.

Conference on Artiﬁcial Intelligence, 1983.

[37] W. Yang, R. L. Lui, J.-H. Gao, T. F. Chan, S.-T. Yau, R. A.
Sperling, and X. Huang. Independent component analysis-
Journal of
based classiﬁcation of alzheimer’s mri data.
Alzheimer’s disease: JAD, 24(4):775, 2011.

[38] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How trans-
ferable are features in deep neural networks? In NIPS, 2014.

[39] M. D. Zeiler and R. Fergus. Stochastic pooling for regular-
ization of deep convolutional neural networks. arXiv preprint
arXiv:1301.3557, 2013.

[40] M. D. Zeiler and R. Fergus. Visualizing and understanding

convolutional networks. In ECCV, 2014.

[41] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva.
Learning deep features for scene recognition using places
database. In NIPS, 2014.

Structured Receptive Fields in CNNs

J¨orn-Henrik Jacobsen1, Jan van Gemert1,2, Zhongyu Lou1, Arnold W. M. Smeulders1
1University of Amsterdam, The Netherlands
2TU Delft, The Netherlands
{j.jacobsen,z.lou,a.w.m.smeulders}@uva.nl, j.c.vangemert@tudelft.nl

6
1
0
2
 
y
a
M
 
3
1
 
 
]

V
C
.
s
c
[
 
 
2
v
1
7
9
2
0
.
5
0
6
1
:
v
i
X
r
a

Abstract

Learning powerful feature representations with CNNs is
hard when training data are limited. Pre-training is one
way to overcome this, but it requires large datasets sufﬁ-
ciently similar to the target domain. Another option is to de-
sign priors into the model, which can range from tuned hy-
perparameters to fully engineered representations like Scat-
tering Networks. We combine these ideas into structured
receptive ﬁeld networks, a model which has a ﬁxed ﬁlter
basis and yet retains the ﬂexibility of CNNs. This ﬂexibil-
ity is achieved by expressing receptive ﬁelds in CNNs as a
weighted sum over a ﬁxed basis which is similar in spirit
to Scattering Networks. The key difference is that we learn
arbitrary effective ﬁlter sets from the basis rather than mod-
eling the ﬁlters. This approach explicitly connects clas-
sical multiscale image analysis with general CNNs. With
structured receptive ﬁeld networks, we improve consider-
ably over unstructured CNNs for small and medium dataset
scenarios as well as over Scattering for large datasets. We
validate our ﬁndings on ILSVRC2012, Cifar-10, Cifar-100
and MNIST. As a realistic small dataset example, we show
state-of-the-art classiﬁcation results on popular 3D MRI
brain-disease datasets where pre-training is difﬁcult due to
a lack of large public datasets in a similar domain.

1. Introduction

Where convolutional networks have appeared enor-
mously powerful in the classiﬁcation of images when ample
data are available [14], we focus on smaller image datasets.
We propose structuring receptive ﬁelds in CNNs as linear
combinations of basis functions to train them with fewer
image data.

The common approach to smaller datasets is to per-
form pre-training on a large dataset, usually ImageNet [30].
Where CNNs generalize well to domains similar to the do-
main where the pre-training came from [28, 41], the per-
formance decreases signiﬁcantly when moving away from
the pre-training domain [41, 38]. We aim to make learning
more effective for smaller sets by restricting CNNs param-

Figure 1: A subset of ﬁlters of the ﬁrst structured recep-
tive ﬁeld CNN layer as trained on 100-class ILSVRC2012
and the Gaussian derivative basis they are learned from. The
network learns scaled and rotated versions of zero, ﬁrst, sec-
ond and third order ﬁlters. Furthermore, the ﬁlters learn to
recombine the different input color channels which is a cru-
cial property of CNNs.

eter spaces. Since all images are spatially coherent and hu-
man observers are considered to only cast local variations
up to a certain order as meaningful [11, 18] our key as-
sumption is that it is unnecessary to learn these properties
in the network. When visualizing the intermediate layers
of a trained network, see e.g. [40] and Figure 2, it becomes
evident that the ﬁlters as learned in a CNN are locally coher-
ent and as a consequence can be decomposed into a smooth
compact ﬁlter basis [12].

We aim to maintain the CNN’s capacity to learn general
variances and invariances in arbitrary images. Following
from our assumptions, the demand is posed on the ﬁlter set
that i) a linear combination of a ﬁnite basis set is capable of
forming any arbitrary ﬁlter necessary for the task at hand,
as illustrated in Figure 1 and ii) that we preserve the full
learning capacity of the network. For i) we choose the fam-
ily of Gaussian ﬁlters and its smooth derivatives for which
it has been proven [12] that 3-rd or 4-th order is sufﬁcient
to capture all local image variation perceivable by humans.
According to scale-space theory [11, 36], the Gaussian fam-
ily constitutes the Taylor expansion of the image function
which guarantees completeness. For ii) we maintain back-
propagation parameter optimization in the network, now
applied to learning the weights by which the ﬁlters are
summed into the effective ﬁlter set.

Figure 2: Filters randomly sampled from all layers of the
GoogLenet model [34], from left to right layer number in-
creases. Without being forced to do so, the model exhibits
spatial coherence (seen as smooth functions almost every-
where) after being trained on ILSVRC2012. This behaviour
reﬂects the spatial coherence of the input feature maps even
in the highest layers.

Similarly motivated, the Scattering Transform [2, 20,
31], a special type of CNN, uses a complete set of wavelet
ﬁlters ordered in a cascade. However, different from a clas-
sical CNN, the ﬁlters parameters are not learned by back-
propagation but rather they are ﬁxed from the start and the
whole network structure is motivated by signal processing
principles.
In the Scattering Network the choice of local
and global invariances are tailored to the type of images
speciﬁcally. In the Scattering Transform invariance to group
actions beyond local translation and deformation requires
explicit design [20] with the regards to the variability en-
countered in the target domain such as translation [2], rota-
tion [31] or scale. As a consequence, when the desired in-
variance groups are known a priori, Scattering delivers very
effective networks.
Our paper takes the best of two worlds. On the one hand,
we adopt the Scattering principle of using ﬁxed ﬁlter bases
as a function prior in the network. But on the other hand, we
maintain from plain CNNs the capacity to learn arbitrary ef-
fective ﬁlter combinations to form complex invariances and
equivariances.

Our main contributions are:

• Deriving the structured receptive ﬁeld network
(RFNN) from ﬁrst principles by formulating ﬁlter
learning as a linear decomposition onto a ﬁlter ba-
sis, unifying CNNs and multiscale image analysis in
a learnable model.

• Combining the strengths of Scattering and CNNs. We
do well on both domains: i) small datasets where Scat-
tering is best but CNNs are weak; ii) complex datasets
where CNNs excel but Scattering is weak.

• State-of-the-art classiﬁcation results on a small dataset
where pre-training is
The task is
Alzheimer’s disease classiﬁcation on two widely used
brain MRI datasets. We outperform all published re-
sults on the ADNI dataset.

infeasible.

2. Related Work

2.1. Scale-space: the deep structure of images

Scale-space theory [36] provides a model for the struc-
ture of images by steadily convolving the image with ﬁlters
of increasing scale, effectively reducing the resolution in
each scale step. While details of the image will slowly dis-
appear, the order by which they do so will uniquely encode
the deep structure of the image [11]. Gaussian ﬁlters have
the advantage in that they do not introduce any artifacts [18]
in the image while Gaussian derivative ﬁlters form a com-
plete and stable basis to decompose locally any realistic im-
age. The set of responses to the derivative ﬁlters describing
one patch is called the N-jet [5].

In the same vein, CNNs can be perceived to also model
the deep structure of images, this time in a non-linear fash-
ion. The pooling layers in a CNN effectively reduce reso-
lution of input feature maps. Viewed from the top of the
network down, the spatial extent of a convolution kernel is
increased in each layer by a factor 2, where a 5x5 kernel at
the higher layer measures 10x10 pixels on the layer below.
The deep structure in a CNN models the image on several
discrete levels of resolution simultaneously, precisely in line
with Scale-space theory.

Where CNNs typically reduce resolution by max pooling
in a non-linear fashion, Scale-space offers a linear theory
for continuous reduction of resolution. Scale-space theory
treats an image as a function of the mathematical apparatus
to reveal the local image structure. In this paper, we exploit
the descriptive power of Scale-space theory to decompose
the image locally on a ﬁxed ﬁlter basis of multiple scales.

2.2. CNNs and their parameters

CNNs [15] have large numbers of parameters to
learn [13]. This is their strength as they can solve ex-
tremely complicated problems [13, 35]. At the same time,
their number of unrestricted parameters is a limiting factor
in terms of the large amounts of data needed to train. To
prevent overﬁtting, which is an issue even when training on
large datasets like the million images of the ILSVRC2012
challenge [30], usually regularization is imposed with meth-
ods like dropout [33] and weight decay [22]. Regulariza-
tion is essential to achieving good performance. In cases
where limited training data are available, CNN training
quickly overﬁts regardless and the learned representations
do not generalize well. Transfer learning from models pre-
trained in similar domains to the new domain is necessary
to achieve competitive results [23]. One thing pre-training
on large datasets provides is knowledge about properties in-
herent to all natural images, such as spatial coherence and
In this paper, we
robustness to uninformative variability.
aim to design these properties into CNNs to improve gener-
alization when limited training data are available.

2.3. The Scattering representation

To reduce model complexity we draw inspiration from
the elegant convolutional Scattering Network [2, 20, 31].
Scattering uses a multi-layer cascade of a pre-deﬁned
wavelet ﬁlter bank with nonlinearity and pooling operators.
It computes a locally translation-invariant image represen-
tation, stable to deformations while avoiding information
loss by recovering wavelet coefﬁcients in successive lay-
ers. No learning is used in the image representation: all
relevant combinations of the ﬁlters are fed into an SVM-
classiﬁer yielding state-of-the-art results on small dataset
classiﬁcation. Scattering is particularly well-suited to small
datasets because it refrains from feature learning. Since all
ﬁlter combinations are pre-deﬁned, their effectiveness is in-
dependent of dataset size.
In this paper, we also beneﬁt
from a ﬁxed ﬁlter bank. In contrast to Scattering, we learn
linear combinations of a ﬁlter basis into effective ﬁlters and
non-linear combinations thereof.

The wavelet ﬁlterbank of Scattering is carefully designed
to sample a range of rotations and scales. These ﬁlters
and their properties are grounded in wavelet theory [19]
and exhibit precisely formulated properties. By using in-
terpretable ﬁlters, Scattering can design invariance to ﬁnite
groups such as translation [2], scale and rotation [31]. Hard
coding the invariance into the network is effective when
the problem and its invariants are known precisely, but for
many applications this is rarely the case. When the vari-
ability is unknown, additional Scattering paths have to be
computed, stored and processed exhaustively before classi-
ﬁcation. This leads to a well-structured but very high di-
mensional parameter space. In this paper, we use a Gaus-
sian derivatives basis as the ﬁlter bank, ﬁrmly grounded in
scale-space theory [11, 18, 36]. Our approach incorporates
learning effective ﬁlter combinations from the very begin-
ning, which allows for a compact representation of the prob-
lem at hand.

2.4. Recent CNNs

Restriction of parameter spaces has led to some major
advances in recent CNNs performance. Network in Net-
work [17] and GoogleNet [34] illustrate that fully connected
layers, which constitute most of Alexnet’s parameters, can
be replaced by a global average pooling layer reducing the
number of parameters in the fully connected layers to virtu-
ally zero. The number of parameters in the convolution lay-
ers is increased to enhance the expressiveness of each layers
features. Overall the total number of parameters is not nec-
essarily decreased, but the function space is restricted, al-
lowing for bigger models while classiﬁcation accuracy im-
proves [17, 34].

The VGG Network [32] improves over Alexnet in a dif-
ferent way. The convolution layers parameter spaces are
restricted by splitting each 5x5 convolution layer into two

3x3 convolution layers. 5x5 convolutions and 2 subsequent
3x3 convolutions have the same effective receptive ﬁeld size
while each receptive ﬁeld has 18 instead of 25 trainable pa-
rameters. This regularization enables learning larger mod-
els that are less prone to overﬁtting. In this paper, we follow
a different approach in restricting the free parameter space
without reducing ﬁlter size.

3. Deep Receptive Field Networks

3.1. Structured receptive ﬁelds

In our structured receptive ﬁeld networks we make
the relationship between Scale-space and CNNs explicit.
Whereas normal CNNs treat images and their ﬁlters as pixel
values, we aim for a CNN that treats images as functions in
Scale-space. Thus, the learned convolution kernels become
functions as well. We therefore approximate an arbitrary
CNN ﬁlter F (x) with a Taylor expansion around a up to
order M

F (x) =

(x − a)m.

(1)

M
(cid:88)

m=0

F m(a)
m!

Scale-space allows us to use differential operators on im-
ages, due to linearity of convolution we are able to compute
the exact derivatives of the scaled underlying function by
convolution with derivatives of the Gaussian kernel

G(.; σ) ∗ F (x) =

N
(cid:88)

m=0

(Gm(.; σ) ∗ F )(a)
m!

(x − a)m,

(2)

where ∗ denotes convolution, G(.; σ) is a Gaussian kernel
with scale σ and Gm(.; σ) is the mth order Gaussian deriva-
tive with respect to it’s spatial variable. Thus, a convolu-
tion with a basis of weighted Gaussian derivatives receptive
ﬁelds is the functional equivalent to pixel values in a stan-
dard CNN operating on a scaled inﬁnitely differentiable ver-
sion of the image.

To construct the full basis set in practice, one can show
that the Hermite polynomials emerge from a sequence of
Gaussian derivatives up to order M [29]. A Gaussian
derivative of arbitrary order can be obtained from the or-
thogonal Hermite polynomials Hm through pointwise mul-
tiplication with a Gaussian envelope

Gm(.; σ) = (−1)m 1
√

σm Hm(

σ

x
√

2

) ◦ G(x; σ).

(3)

The resulting operators allow computation of an image’s
local geometry at scale σ and location x up to any order
of precision M . This basis is thus a complete set. Each
derivative corresponds to an independent degree of free-
dom, making it also a minimal set.
Thus, an RFNN is a general CNN when a complete
polynomial up to inﬁnite order is considered. We restrict

the basis based on the requirement that one can construct
quadrature pair ﬁlters as suggested by Scattering and by
evidence from Scale-space theory [12] that considers all
orders up to a maximum of 4, as it has been suggested that
orders beyond that do not carry any information meaningful
to visual perception.

3.2. Transformation properties of the basis

The isotropic Gaussian derivatives exhibit multiple de-
sirable properties. It is possible to create complex multi-
orientation pyramids that constitute wavelet representations
similar to the Morlet Wavelet pyramids used in Scattering
Networks [2]. A complex multiresolution ﬁlterbank can
be constructed from a dilated and rotated Gaussian deriva-
tive quadrature. The exact dilated versions of an arbitrary
Gaussian derivative Gm can be obtained through convolu-
tion with a Gaussian kernel of scale σ = n according to

(cid:112)

Gm(.;

j2 + n2) = Gm(.; j) ∗ G(.; n).

(4)

Arbitrary rotations of Gaussian derivative kernels can be ob-
tained from a minimal set of basis ﬁlters without the need to
rotate the basis itself. This property is referred to as steer-
ability [6]. Steerability is a property of all functions that can
be expressed in a polynomial in x and y times an isotropic
Gaussian. This certainly holds for the Gaussian derivatives
according to equation 3. For example a quadrature pair of
2nd and 3rd order Gaussian derivatives Gxx and Gxxx ro-
tated by an angle θ can be obtained from a minimal 3 and 4
x-y separable basis set given by

Gxx

θ = cos2(θ)Gxx − 2 cos(θ) sin(θ)Gxy + sin2(θ)Gyy
θ = cos3(θ)Gxxx − 3 cos2(θ) sin(θ)Gxxy
+3 cos(θ) sin2(θ)Gxyy − sin3(θ)Gyyy

Gxxx

A general derivation of the minimal basis set necessary for
steering arbitrary orders can be found in [6]. Note that the
anisotropic case can be constructed in analogous manner
according to [26]. This renders Scattering as a special case
of the RFNN for ﬁxed angles and scales, given a proper
choice of pooling operations and possibly skip connections
to closely resemble the architecture described in [2].
In
practice this allows for seamless integration of the Scatter-
ing concept into CNNs to achieve a variety of hybrid archi-
tectures.

3.3. Learning basis ﬁlter parameters

Learning a feature representation boils down to convo-
lution kernel learning. Where a classical CNN learns pixel
values of the convolutional kernel, a RFNN learns Gaus-
sian derivative basis function weights that combine to a

Algorithm 1 RFNN Learning - updating the parameters αl
ij
between input map indexed by i and output map indexed by
j of layer l in the Mini-batch Gradient Decent framework.
1: Input: input feature maps ol−1

for each training sam-
ple (computed for the previous layer, ol−1 is the input
image when l = 1), corresponding ground-truth labels
{y1, y2, . . . , yK}, the basic kernels {φ1, φ2, . . . , φM },
previous parameter αl

i

ij.

2: compute the convolution {ζ1, ζ2, . . . , ζm} of {ol−1

i}

respect to the basic kernels {φ1, φ2, . . . , φM }

3: obtain the output map ol

j = αl

ij1 · ζ1 + αl

ij2 · ζ2 + ... +

αl

ijM · ζM
4: compute the δl

map ol
j

tion

jn for each output neuron n of the output

5: compute the derivative ψ(cid:48)(tl

jn) of the activation func-

6: compute the gradient ∂E
∂αl
ij
ij = αl

7: update parameter αl
is the learning rate

respect to the weights αl
ij
K · (cid:80)K
ij − r · 1

k=1[ ∂E
∂αl
ij

]k, r

8: Output: αl

ij, the output feature maps ol
j

(5)

Figure 3: An illustration of the basic building block in an
RFNN network. A linear comibination of a limited basis
ﬁlter set φm yields an arbitrary number of effective ﬁlters.
The weights αij are learned by the network.

convolution kernel function. A 2D ﬁlter kernel function
F (x, y) in all layers, is a linear combination of i unique
(non-symmetric) Gaussian derivative basis functions φ

F (x, y) = α1φ1 + · · · + αnφi,

(6)

where α1, ..., αi are the parameters being learned.

We learn the ﬁlter’s weights α by mini-batch stochas-
tic gradient descent and compute the derivatives of the loss
function E with respect to the parameters α through back-
propagation. It is straightforward to show the independence
between the basis weights α and the actual basis (see Ap-
pendix for derivation). Thus, we formulate the basis learn-

ing as a combination of a ﬁxed basis layer with a 1x1 convo-
lution layer that has a kernel depth equal to the basis order.
Propagation through the 1x1 layer is done as in any CNN
while propagation through the basis layer is achieved by
a convolution with ﬂipped versions of the Gaussian ﬁlters.
This makes it straightforward to include into any existing
deep learning framework. The basic structured receptive
ﬁeld building block is illustrated in ﬁgure 3, showing how
each effective ﬁlter is composed out of multiple basis ﬁlters.
Note that the linearity of convolution allows us to never ac-
tually compute the effective ﬁlters. Convolving with effec-
tive ﬁlters is the same as convolving with the basis and then
recombining the feature maps, allowing for efﬁcient imple-
mentation. Algorithm 1 shows how the parameters are up-
dated.

3.4. The network

In this work, we choose the Network in Network (NiN)
architecture [17] as the basis into which we integrate the
structured receptive ﬁelds.
It is particularly suited for an
analysis of the RFNN approach, as the absence of a fully
connected layer ensures all parameters to be fully concerned
with re-combining basis ﬁlter outcomes of the current layer.
At the same time, it is powerful, similar in spirit to the state
of the art Googlenet [34], while being comparably small and
fast to train.

NiN alternates one spatial convolution layer with 1x1
convolutions and pooling. The 1x1 layers form non-linear
combinations of the spatial convolution layers outputs. This
procedure is repeated four times in 16 layers, with different
number of ﬁlters and kernel sizes for the spatial convolu-
tion layer. The ﬁnal pooling layer is a global average pool-
ing layer. Each convolution layer is followed by a rectiﬁer
nonlinearity. Details on the different NiNs for Cifar and
Imagenet can be found in the Caffe model zoo [9].

In the RFNN version of the Network in Network model,
the basis layer including the Gaussian derivatives set is re-
placing the spatial convolution layer and corresponds to φm
in equation 6. Thus, each basis convolution layer has a num-
ber of ﬁlters depending on order and scale of the chosen
basis set. The basis set is ﬁxed: no parameters are learned
in this layer. The linear re-combination of the ﬁlter basis is
done by the subsequent 1x1 convolution layer, correspond-
ing to αij in equation 6. Note that there is no non-linearity
between φm and αij layer in the RFNN case, as the com-
binations of the ﬁlters are linear. Thus the RFNN model is
almost identical to the standard Network in Network.
We evaluate the model with and without multiple scales σs.
When including scale, we extract 4 scales, as the original
model includes 3 pooling steps and thus operates on 4 scales
In the ﬁrst layer we directly compute 4 scales,
at least.
sampled continuously with σs = 2s where s = scale as
done in [2]. In each subsequent layer we discard the lowest

scale. The dimensionality reduction by max pooling renders
it meaningless to insert the lowest scale of the previous layer
into the ﬁlter basis set as it is already covered by the pyra-
midal structure of the network. This enables us to save on
basis ﬁlters in the higher layers of the network. In conclu-
sion we reduce the total number of 2D ﬁlters in the network
from 520,000 in the standard Network in Network to be-
tween 12 and 144 in the RF Network in Network (RFNiN),
while retaining the models expressiveness as shown in the
experimental section.

4. Experiments

The experiments are partitioned into four parts.

i) We
show insight in the proposed model to investigate design
choices; ii) we show that our model combines the strengths
of Scattering and CNNs; iii) we show structured recep-
tive ﬁelds improve classiﬁcation performance when limit-
ing training data; iv) we show a 3D version of our model
that outperforms the state-of-the-art, including a 3D-CNN,
on two brain MRI classiﬁcation datasets where large pre-
training datasets are not available. We use the Caffe li-
brary [9] and Theano [1] where we added RFNN as a sepa-
rate module. Code is available on github1.

4.1. Experiment 1: Model insight

The RFNN used in this section is the structured recep-
tive ﬁeld version of the Network in Network (RFNiN) in-
troduced in section 3.3. We gain insight into the model by
evaluating the scale and order of the basis ﬁlters. In addi-
tion, we analyze the performance compared to the standard
Network in Network (NiN) [17] and Alexnet [13] and show
that our proposed model is not merely a change in architec-
ture. To allow overnight experiments we use the 100 largest
classes of the ILSVRC2012 ImageNet classiﬁcation chal-
lenge [30]. Selection is done by folder size, as more than
100 classes have 1,300 images in them, yielding a dataset
size of 130,000 images. This is a real-world medium sized
dataset in a domain where CNNs excel.

Experimental setup. The Network in Network (NiN)
model and our Structured Receptive Field Network in
Network (RFNiN) model are based on the training deﬁ-
nitions provided by the Caffe model zoo [9]. Training is
done with the standard procedure on Imagenet. We use
stochastic gradient descent, a momentum of 0.9, a weight
decay of 0.0005. The images are re-sized to 256x256,
mirrored during training and the dataset mean is subtracted.
The base learning rate was decreased by a factor of 10,
according to the reduction from 1,000 to 100 classes, to
ensure proper scaling of the weight updates, NiN didn’t
converge with the original learning rate. We decreased it
by a factor of 10 after 50,000 iterations and again by the

1https://github.com/jhjacobsen/RFNN

Top-1

2DFilters #Params

ILSVRC2012-100 Subset

Method
RFNiN 1st-order
RFNiN 2nd-order
RFNiN 3rd-order
RFNiN 4th-order
RFNiN-Scale 1st-order
RFNiN-Scale 2nd-order
RFNiN-Scale 3rd-order
RFNiN-Scale 4th-order

44.83% 12
61.24% 24
63.64% 40
62.92% 60
57.21% 24
67.56% 54
69.65% 94
68.95% 144

Network in Network
Alexnet

67.30% 520k
54.86% 370k

1.8M
3.4M
5.5M
8.1M
2.2M
4.2M
6.8M
10.1M

8.2M
60.0M

Table 1: Results on 100 Biggest ILSVRC2012 classes: The
table shows RFNiN with 1st, 2nd, 3rd and 4th order ﬁlters
in the whole network. Row 1-4 are applying basis ﬁlters in
all layers on a scale of σ=1. RFNiN-scale in row 5-8 ap-
plies basis ﬁlters on 4 scales, where σ=1,2,4,8. The results
show that a 3rd order basis is sufﬁcient while incorporating
scale into the network gives a big gain in performance. The
RFNiN is able to outperform the same Network in Network
architecture.

same factor after 75,000 iterations. The networks were
trained for 100,000 iterations. Results are computed as
the mean Top-1 classiﬁcation accuracy on the validation set.

Filter basis order. In table 1, the ﬁrst four rows show
the result of RFNiN architectures with 1st to 4th order
Gaussian derivative basis ﬁlter set comprised of 12 to 60
individual Gaussian derivative ﬁlters in all layers of the net-
work. In these experiments the value of σ=1, ﬁxed for all
ﬁlters and all layers. Comparing ﬁrst to fourth order ﬁlter
basis in table 1, we conclude that third order is sufﬁcient,
outperforming ﬁrst and second order as predicted by Scale-
space theory [12]. The fourth order does not add any more
gain.

Filter scale. The RFNiN-Scale entries of table 1 show
the classiﬁcation result up to fourth order now with 4 differ-
ent scales, σ=1, 2, 4, 8 for the lowest layer, σ=1, 2, 4 for the
second layer, σ=1, 2 for the third, and σ=1 for the fourth.
This implies that the basis ﬁlter set expands from 24 up to
144 ﬁlters in total in the network. Comparing the use of sin-
gle scale ﬁlters in the network to dilated copies of the ﬁlters
with varying scale indicates that a considerable gain can be
achieved by including ﬁlters with different scales. This ob-
servation is supported by Scattering [2], showing that the
multiple scales can directly be extracted from the ﬁrst layer
on. In fact, normal CNNs are also capable of similar be-
havior, as positive valued low-pass ﬁlter feature maps are
not affected by rectiﬁer nonlinearities [31]. Thus, scale can

Figure 4: Mean of ﬁlter weights and variances per layer for
15 basis ﬁlters with no scale, as trained on ILSVRC2012-
100 subset. Note that the lower order ﬁlters have the highest
weights while zero-order ﬁlters are most effective in higher
layers for combinations of lower responses.

directly be computed from the ﬁrst layer onwards, which
yields a much smaller set of basis ﬁlters and fewer convo-
lutions needed in the higher layers. Note that number of
parameters is not directly correlated with performance.

Analysis of network layers. For the network RFNiN
4th-order Figure 4 provides an overview of the range of ba-
sis weights per effective ﬁlters in all layers, where the x-
axis indexes the spatial derivative index and y-axis the mean
value plus standard deviation of weights per layer over all
effective ﬁlter kernels. The ﬁgure indicates that weights de-
crease towards higher orders as expected. Furthermore zero
order ﬁlters have relatively high weights in higher layers,
which hints to passing on scaled incoming features.

Comparison to Network in Network. The champion
RFNiN in table 1 slightly outperforms the Network in
Network with the same setting and training circumstances
while only having 94 instead 520,000 spatial ﬁlters in the
network in total. Note that the number of parameters is rel-
atively similar though, as the scale component increases the
number of basis functions per ﬁlter signiﬁcantly. The result
shows that our basis representation is sufﬁcient for complex
tasks like Imagenet.

Refactorize Network in Network. To illustrate that our
proposed model is not merely a change in architecture we
compare to a third architecture. We remove the Gaussian
basis and we re-factorize the NiN such that it becomes iden-
tical to RFNiN. Both have almost the same number of pa-
rameters, but the NiN-factorize has a freely learnable ba-
sis. Re-factorizing only the ﬁrst layer and leaving the rest
of the network as in the original NiN, in table 2 we show
that a Gaussian basis is superior to a learned basis. When
re-factorizing all layers, RFNiN-Scale 3rd-order results are
superior by far to the identical NiN-factorize All Layers.

Model

Basis #Params Top-1

Model

Cifar-10 Cifar-100

NiN-refactor Layer 1
RFNiN-refactor Layer 1 Gauss

Free

NiN-refactor All Layers
RFNiN-Scale 3rd-order

Free
Gauss

7.47M
7.47M

6.87M
6.83M

64.10%
68.63%

38.02%
69.65%

Table 2: Classiﬁcation on ILSVRC2012-100 to illustrate in-
ﬂuence of factorization on performance. The results show
that the advantage of the Gaussian basis is substantial and
our results are not merely due to a change in architecture.

Roto-Trans Scattering
RFNiN

82.30% 56.80%
86.31% 63.81%

RCNN

91.31% 68.25%

Table 3: Comparison against Scattering on a large complex
domain. State-of-the-art comparison is given by RCNN.
RFNiN outperforms Scattering by large margins.

recently introduced Deep Roto-Translation Scattering ap-
proach [24], a powerful variant of Scattering networks ex-
plicitly modeling invariance under the action of small rota-
tions. This is a domain where CNNs excel and learning of
complex image variabilities is key.

The RFNiN is again a variant of the standard NiN for
It is similar to the model in experiment 1, just
Cifar-10.
that it has one basis layer, two 1x1 convolution layers and
one pooling layer less and the units in the 1x1 convolution
layers are 192 in the whole network. Furthermore, we show
performance of the state-of-the-art recurrent convolutional
networks (RCNNs) [16] for comparison.

The results in Table 3 show a considerable improvement
on Cifar-10 and Cifar-100 when comparing RFNiN to
Roto-Translation Scattering [24], which was designed
speciﬁcally for this dataset. RCNNs performance is consid-
erably higher as they follow a different approach to which
structured receptive ﬁelds can also be applied if desired.

RFNNs are robust to dataset size. From these experi-
ments, we conclude that RFNNs combine the best of both
worlds. We outperform CNNs and compete with Scatter-
ing when training data is limited as exempliﬁed on subsets
of MNIST. We capture complex image variabilities beyond
the capabilities of Scattering representations as exempliﬁed
on the datasets Cifar-10 and Cifar-100 despite operating in a
similarly smooth parameter space on a receptive ﬁeld level.

4.3. Experiment 3: Limiting datasize

To demonstrate the effectiveness of the RF variant com-
pared to the Network in Network, we reduce the number of
classes in the ILSVRC2012-dataset from 1000 to 100 to 10,
resulting in a reduction of the total number of images on
which the network was trained from 1.2M to 130k to 13k
and subsequent decrease in visual variety to learn from. To
demonstrate performance is not only due to smaller number
of learnable parameters, we evaluate two RFNiN versions.
RFNiN-v1 is RFNiN-Scale 3rd-order from table1. RFNiN-
v2 is one layer deeper and wider [128/128/384/512/1000]
version of the RFNiN-v1, resulting in 3 million additional
parameters, which is 2,5 million more than NiN.

The results in table 4 show that compared to CNNs the

Figure 5: Classiﬁcation performance of the Scattering Net-
work on various subsets of the MNIST dataset.
In com-
parison the state of the art CNN-A from [27]. RFNN de-
notes our receptive ﬁeld network, with the same architecture
as CNN-B. Both are shown, to illustrate that good perfor-
mance of the RFNN is not due to the CNN architecture, but
due to RFNN decomposition. Our RFNN performs on par
with Scattering, substantially outperforming both CNNs.

4.2. Experiment 2: Scattering and RFNNs

Small simple domain. We compare an RFNN to Scat-
tering in classiﬁcation on reduced training sizes of the
MNIST dataset. This is the domain where Scattering out-
performs standard CNNs [2]. We reduce the number of
training samples when training on MNIST as done in [2].
The network architecture and training parameters used in
this section are the same as in [39]. The RFNN contains 3
layers with a third order basis on one scale as a multiscale
basis didn’t provide any gain. Scale and order are deter-
mined on a validation set. Each basis layer is followed by
a layer of αN = 64 1x1 units that linearly re-combine the
basis ﬁlters outcomes. As comparison we re-implement the
same model as a plain CNN. The CNN and Scattering re-
sults on the task are taken from [2, 27].

Results are shown in Figure 5, each number is averaged
over 3 runs. For the experiment on MNIST the gap be-
tween the CNNs and networks with pre-deﬁned ﬁlters in-
creases when training data is reduced, while RFNN and
Scattering perform on par even at the smallest sample size.
Large complex domain. We compare against Scattering
on the Cifar-10 and Cifar-100 datasets, as reported by the

Model

#Params

1000-class

100-class

10-class

3D MRI classiﬁcation

Accuracy TPR

SPC

NiN
RFNiN-v1
RFNiN-v2

7.5M
6.8M
10M

56.78%
50.08%
54.04%

67.30% 76.97%
69.65% 85.00%
70.78% 83.36%

3D-RFNiN (ours)
ICA [37]
Voxel-Direct-D-gm [4]

97.79% 97.14% 98.78%
81.90% 79.50%
80.70%
81.00% 95.00%
-

Table 4: Three classiﬁcation experiments on ILSVRC2012
subsets. Results show that the bigger model (RFNiN-v2)
performs better than RFNiN-Scale 3rd-order (RFNiN-v1) on
the 1000-classes while on 100-class and 10-class, v1 and
v2 perform similar. The gap between RFNiN and NiN in-
creases for fewer classes.

RFNiN performance is better relatively speaking when the
number of samples and thus the visual variety decreases.
For the 13k ILSVRC2012-10 image dataset the gap be-
tween RFNiN and NiN increased to 8.0% from 2.4% for the
130k images in ILSVRC2012-100 while the best RFNiN is
inferior to NiN by 2.98% for the full ILSVRC2012-1000.
This supports our aim that RFNiN is effectively incorpo-
rating natural image priors, yielding a better performance
compared to the standard NiN when training data and va-
riety is limited, even when having more learnable parame-
ters. Truly large datasets seem to contain information not
yet captured by our model.

4.4. Experiment 4: Small realistic data

We apply an RFNiN to 3D brain MRI classiﬁcation for
Alzheimer’s disease [4] on two popular datasets. Neu-
roimaging is a domain where training data is notoriously
small and high dimensional and no truly large open access
databases in a similar domain exist for pre-training.

We use a 3-layer RFNiN with ﬁlters sizes [128,96,96]
with a third order basis in 3 scales σ ∈ {1, 4, 16}. This
time wider spaced, as the brains are very big objects and are
centered due to normalization to MNI space with the FSL
library [8]. Each basis layer is followed by one 1x1 convo-
lution layer. Global average pooling is applied to the ﬁnal
feature maps. The network is implemented in Theano [1]
and trained with Adam [10].

The results are shown in table 5. Note that [7, 25] train
on their own subset and use an order of magnitude more
training data. We follow standard practice [4] and train on
a smaller subset. Nevertheless we outperform all published
methods on the ADNI dataset. The same 3 layer NiN as our
RFNiN model has 84.21% accuracy, more than 10% worse
while being hard to train due to unstable convergence. On
the OASIS AD-126 Alzheimer’s dataset [21], we achieve
an accuracy of 80.26%, compared to 74.10% with a SIFT-
based approach [3]. Thus, we show our RFNiN can ef-
fectively learn comparably deep representations even when
data is scarce and exhibits stable convergence properties.

3D-CNN [25]
NIB [7]

95.70%
94.74%

-
95.24% 94.26%

-

Table 5: Alzheimer’s classiﬁcation with 150 train and test
3D MRI images from the widely used ADNI benchmark.
RFNiN, ICA and Voxel-Direct-D-gm are trained on the sub-
set introduced in [4], 3D-CNN and NIB were trained on
their own subset of ADNI, using an order of magnitude
more training data. RFNiN outperforms all published re-
sults. Reported is accuracy, true positive rate and speciﬁcity.

5. Discussion

The experiments show that structuring convolutional lay-
ers with a ﬁlter basis grounded on Scale-space principles
improves performance when data is limited. The ﬁlter ba-
sis provides regularization especially suited for image data
by restricting the parameter space to smooth features up to
fourth order. The Gaussian derivative basis opens up a new
perspective for reasoning in CNNs, connecting them with
a rich body of prior multiscale image analysis research that
can now be readily incorporated into the models. This is
especially interesting for applications where model insight
and control is key.

We illustrated the effectiveness of RFNNs on multi-
ple subsets of Imagenet, Cifar-10, Cifar-100 and MNIST.
The choice of a third order Gaussian basis is sufﬁcient to
tackle all datasets which is in accordance with prior re-
search [12, 2]. While it remains an open problem to match
the performance of CNNs on very large datasets like the
1000-class ILSVRC2012, our results show that the RFNN
method outperforms CNNs by large margins when data
are scarce.
It can also outperform CNNs on challenging
medium sized datasets while being superior to Scattering
on large datasets despite having more parameters as the
pre-deﬁned basis restriction allows the network to devote
its full capacity to a sensible feature spaces. As a small
data real world example, we verify our claims with 3D MRI
Alzheimer’s disease classiﬁcation on two datasets where we
consistently achieve competitive performance including the
best results on the widely used ADNI dataset.

Acknowledgements. We thank Rein van den Boom-
gaard and Silvia-Laura Pintea for insightful discussions.
This work has been conducted with data from the
Alzheimer’s Disease Neuroimaging Initiative (ADNI) and
the Open Access Series of Imaging Studies (OASIS).

References

[1] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. Good-
fellow, A. Bergeron, N. Bouchard, D. Warde-Farley, and
Y. Bengio. Theano: new features and speed improvements.
arXiv preprint arXiv:1211.5590, 2012.

[2] J. Bruna and S. Mallat. Invariant scattering convolution net-

works. TPAMI, 35(8):1872–1886, 2013.

[3] Y. Chen, J. Storrs, L. Tan, L. J. Mazlack, J.-H. Lee, and
L. J. Lu. Detecting brain structural changes as biomarker
from magnetic resonance images using a local feature based
svm approach. Journal of neuroscience methods, 221:22–31,
2014.

[4] R. Cuingnet, E. Gerardin,

J. Tessieras, G. Auzias,
S. Leh´ericy, M.-O. Habert, M. Chupin, H. Benali, O. Col-
liot, A. D. N. Initiative, et al. Automatic classiﬁcation of
patients with alzheimer’s disease from structural mri: a com-
parison of ten methods using the adni database. neuroimage,
56(2):766–781, 2011.

[5] L. M. Florack, B. M. ter Haar Romeny, J. J. Koenderink,
and M. A. Viergever. Scale and the differential structure of
images. Image and Vision Computing, 10(6):376–388, 1992.

[6] W. T. Freeman and E. H. Adelson. The design and use of

steerable ﬁlters. TPAMI, (9):891–906, 1991.

[7] A. Gupta, M. Ayhan, and A. Maida. Natural image bases to

represent neuroimaging data. In ICML, 2013.

[8] M. Jenkinson, C. F. Beckmann, T. E. Behrens, M. W. Wool-
rich, and S. M. Smith. Fsl. Neuroimage, 62(2):782–790,
2012.

[9] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolu-
tional architecture for fast feature embedding. arXiv preprint
arXiv:1408.5093, 2014.

[10] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. arXiv preprint arXiv:1412.6980, 2014.

[11] J. J. Koenderink. The structure of images. Biological cyber-

netics, 50(5):363–370, 1984.

[12] J. J. Koenderink and A. J. van Doorn. Representation of lo-
cal geometry in the visual system. Biological cybernetics,
55(6):367–375, 1987.

[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012.

Imagenet
In

[14] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature,

521(7553):436–444, 2015.

[15] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278–2324, 1998.

[16] M. Liang and X. Hu. Recurrent convolutional neural network

for object recognition. In CVPR, 2015.

[20] S. Mallat. Group invariant scattering. Communications on

Pure and Applied Mathematics, 65(10):1331–1398, 2012.

[21] D. S. Marcus, T. H. Wang, J. Parker, J. G. Csernansky, J. C.
Morris, and R. L. Buckner. Open access series of imaging
studies (oasis): cross-sectional mri data in young, middle
aged, nondemented, and demented older adults. Journal of
cognitive neuroscience, 19(9):1498–1507, 2007.

[22] J. Moody, S. Hanson, A. Krogh, and J. A. Hertz. A simple
weight decay can improve generalization. NIPS, 1995.
[23] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Learning and
transferring mid-level image representations using convolu-
tional neural networks. In CVPR, 2014.

[24] E. Oyallon and S. Mallat. Deep roto-translation scattering for
object classiﬁcation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 2865–
2873, 2015.

[25] A. Payan and G. Montana. Predicting alzheimer’s disease: a
neuroimaging study with 3d convolutional neural networks.
arXiv preprint arXiv:1502.02506, 2015.

[26] P. Perona. Steerable-scalable kernels for edge detection and
junction analysis. In ECCV, pages 3–18. Springer, 1992.
[27] M. A. Ranzato, F. J. Huang, Y.-L. Boureau, and Y. LeCun.
Unsupervised learning of invariant feature hierarchies with
applications to object recognition. In CVPR. IEEE, 2007.

[28] A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson.
Cnn features off-the-shelf: an astounding baseline for recog-
nition. In CVPR, 2014.

[29] B. M. H. Romeny. Front-end vision and multi-scale image
analysis: multi-scale computer vision theory and applica-
tions, written in mathematica, volume 27. Springer Science
& Business Media, 2008.

[30] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge. IJCV, pages 1–42, 2015.

[31] L. Sifre and S. Mallat. Rotation, scaling and deformation in-
variant scattering for texture discrimination. In CVPR, 2013.

[32] K. Simonyan and A. Zisserman.

Very deep con-
large-scale image recognition.

volutional networks for
arXiv:1409.1556, 2014.

[33] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov. Dropout: A simple way to prevent neural
networks from overﬁtting. JMLR, 2014.

[34] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. arXiv:1409.4842, 2014.

[35] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface:
Closing the gap to human-level performance in face veriﬁca-
tion. In CVPR. IEEE, 2014.

[17] M. Lin, Q. Chen, and S. Yan. Network in network.

[36] A. P. Witkin. Scale-space ﬁltering.

In International Joint

arXiv:1312.4400, 2013.

[18] T. Lindeberg. Scale-space theory in computer vision, volume

256. Springer Science & Business Media, 2013.

[19] S. Mallat. A wavelet tour of signal processing. Academic

press, 1999.

Conference on Artiﬁcial Intelligence, 1983.

[37] W. Yang, R. L. Lui, J.-H. Gao, T. F. Chan, S.-T. Yau, R. A.
Sperling, and X. Huang. Independent component analysis-
Journal of
based classiﬁcation of alzheimer’s mri data.
Alzheimer’s disease: JAD, 24(4):775, 2011.

[38] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How trans-
ferable are features in deep neural networks? In NIPS, 2014.

[39] M. D. Zeiler and R. Fergus. Stochastic pooling for regular-
ization of deep convolutional neural networks. arXiv preprint
arXiv:1301.3557, 2013.

[40] M. D. Zeiler and R. Fergus. Visualizing and understanding

convolutional networks. In ECCV, 2014.

[41] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva.
Learning deep features for scene recognition using places
database. In NIPS, 2014.

