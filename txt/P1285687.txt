LCSTS: A Large Scale Chinese Short Text Summarization Dataset

Baotian Hu Qingcai Chen

Fangze Zhu

Intelligent Computing Research Center
Harbin Institute of Technology, Shenzhen Graduate School
{baotianchina,qingcai.chen, zhufangze123}@gmail.com

Abstract

Automatic text summarization is widely
regarded as the highly difﬁcult problem,
partially because of the lack of large
text summarization data set. Due to the
great challenge of constructing the large
scale summaries for full text, in this pa-
per, we introduce a large corpus of Chi-
nese short text summarization dataset con-
structed from the Chinese microblogging
website Sina Weibo, which is released to
the public1. This corpus consists of over
2 million real Chinese short texts with
short summaries given by the author of
each text. We also manually tagged the
relevance of 10,666 short summaries with
their corresponding short texts. Based on
the corpus, we introduce recurrent neural
network for the summary generation and
achieve promising results, which not only
shows the usefulness of the proposed cor-
pus for short text summarization research,
but also provides a baseline for further re-
search on this topic.

1

Introduction

Nowadays, individuals or organizations can eas-
ily share or post information to the public on the
social network. Take the popular Chinese mi-
croblogging website (Sina Weibo) as an example,
the People’s Daily, one of the media in China,
posts more than tens of weibos (analogous to
tweets) each day. Most of these weibos are well-
written and highly informative because of the text
length limitation (less than140 Chinese charac-
ters). Such data is regarded as naturally annotated
web resources (Sun, 2011). If we can mine these
high-quality data from these naturally annotated
web resources, it will be beneﬁcial to the research
that has been hampered by the lack of data.

1http://icrc.hitsz.edu.cn/Article/show/139.html

Figure 1: A Weibo Posted by People’s Daily.

In the Natural Language Processing (NLP)
community, automatic text summarization is a hot
and difﬁcult task. A good summarization system
should understand the whole text and re-organize
the information to generate coherent, informative,
and signiﬁcantly short summaries which convey
important information of the original text (Hovy
and Lin, 1998), (Martins, 2007). Most of tradi-
tional abstractive summarization methods divide
the process into two phrases (Bing et al., 2015).
First, key textual elements are extracted from the
original text by using unsupervised methods or lin-
guistic knowledge. And then, unclear extracted
components are rewritten or paraphrased to pro-
duce a concise summary of the original text by
using linguistic rules or language generation tech-
niques. Although extensive researches have been
done, the linguistic quality of abstractive sum-
mary is still far from satisfactory. Recently, deep
learning methods have shown potential abilities
to learn representation (Hu et al., 2014; Zhou et
al., 2015) and generate language (Bahdanau et al.,
2014; Sutskever et al., 2014) from large scale data
by utilizing GPUs. Many researchers realize that
we are closer to generate abstractive summariza-
tions by using the deep learning methods. How-
ever, the publicly available and high-quality large
scale summarization data set is still very rare and
not easy to be constructed manually. For exam-
ple, the popular document summarization dataset
DUC2, TAC3 and TREC4 have only hundreds of
human written English text summarizations. The
In this pa-
problem is even worse for Chinese.

2http://duc.nist.gov/data.html
3http://www.nist.gov/tac/2015/KBP/
4http://trec.nist.gov/

6
1
0
2
 
b
e
F
 
9
1
 
 
]
L
C
.
s
c
[
 
 
4
v
5
6
8
5
0
.
6
0
5
1
:
v
i
X
r
a

Figure 2: Diagram of the process for creating the dataset.

per, we take one step back and focus on construct-
ing LCSTS, the Large-scale Chinese Short Text
Summarization dataset by utilizing the naturally
annotated web resources on Sina Weibo. Figure 1
shows one weibo posted by the People’s Daily. In
order to convey the import information to the pub-
lic quickly, it also writes a very informative and
short summary (in the blue circle) of the news.
Our goal is to mine a large scale, high-quality short
text summarization dataset from these texts.

This paper makes the following contributions:
(1) We introduce a large scale Chinese short text
summarization dataset. To our knowledge, it is
the largest one to date; (2) We provide standard
splits for the dataset into large scale training set
and human labeled test set which will be easier for
benchmarking the related methods; (3) We explore
the properties of the dataset and sample 10,666
instances for manually checking and scoring the
quality of the dataset; (4) We perform recurrent
neural network based encoder-decoder method on
the dataset to generate summary and get promis-
ing results, which can be used as one baseline of
the task.

2 Related Work

Our work is related to recent works on automatic
text summarization and natural language process-
ing based on naturally annotated web resources,
which are brieﬂy introduced as follows.

Automatic Text Summarization in some form
has been studied since 1950. Since then, most re-
searches are related to extractive summarizations
by analyzing the organization of the words in the
document (Nenkova and McKeown, 2011) (Luhn,
1998); Since it needs labeled data sets for su-
pervised machine learning methods and labeling
dataset is very intensive, some researches focused
on the unsupervised methods (Mihalcea, 2004).
The scale of existing data sets are usually very

small (most of them are less than 1000). For
example, DUC2002 dataset contains 567 docu-
ments and each document is provided with two
100-words human summaries. Our work is also
related to the headline generation, which is a task
to generate one sentence of the text it entitles.
Colmenares et.al construct a 1.3 million ﬁnancial
news headline dataset written in English for head-
line generation (Colmenares et al., 2015). How-
ever, the data set is not publicly available.

Naturally Annotated Web Resources based
Natural Language Processing is proposed by
Sun (Sun, 2011). Naturally Annotated Web Re-
sources is the data generated by users for commu-
nicative purposes such as web pages, blogs and
microblogs. We can mine knowledge or useful
data from these raw data by using marks generated
by users unintentionally. Jure et.al track 1.6 mil-
lion mainstream media sites and blogs and mine a
set of novel and persistent temporal patterns in the
news cycle (Leskovec et al., 2009). Sepandar et.al
use the users’ naturally annotated pattern ‘we feel’
and ‘i feel’ to extract the ‘Feeling’ sentence collec-
tion which is used to collect the world’s emotions.
In this work, we use the naturally annotated re-
sources to construct the large scale Chinese short
text summarization data to facilitate the research
on text summarization.

3 Data Collection

A lot of popular Chinese media and organizations
have created accounts on the Sina Weibo. They
use their accounts to post news and information.
These accounts are veriﬁed on the Weibo and la-
beled by a blue ‘V’. In order to guarantee the qual-
ity of the crawled text, we only crawl the veriﬁed
organizations’ weibos which are more likely to be
clean, formal and informative. There are a lot of
human intervention required in each step. The pro-
cess of the data collection is shown as Figure 2 and

summarized as follows:

1) We ﬁrst collect 50 very popular organiza-
tion users as seeds. They come from the domains
of politic, economic, military, movies, game and
etc, such as People’s Daily, the Economic Observe
press, the Ministry of National Defense and etc. 2)
We then crawl fusers followed by these seed users
and ﬁlter them by using human written rules such
as the user must be blue veriﬁed, the number of
followers is more than 1 million and etc. 3) We
use the chosen users and text crawler to crawl their
weibos. 4) we ﬁlter, clean and extract (short text,
summary) pairs. About 100 rules are used to ex-
tract high quality pairs. These rules are concluded
by 5 peoples via carefully investigating of the raw
text. We also remove those paris, whose short text
length is too short (less than 80 characters) and
length of summaries is out of [10,30].

4 Data Properties

The dataset consists of three parts shown as Ta-
ble 1 and the length distributions of texts are
shown as Figure 3.

Part I is the main content of LCSTS that con-
tains 2,400,591 (short text, summary) pairs. These
pairs can be used to train supervised learning
model for summary generation.

Part II contains the 10,666 human labeled
(short text, summary) pairs with the score ranges
from 1 to 5 that indicates the relevance between
the short text and the corresponding summary. ‘1’
denotes “ the least relevant ” and ‘5’ denotes “the
most relevant”. For annotating this part, we recruit
5 volunteers, each pair is only labeled by one an-
notator. These pairs are randomly sampled from
Part I and are used to analysize the distribution of
pairs in the Part I. Figure 4 illustrates examples of
different scores. From the examples we can see
that pairs scored by 3, 4 or 5 are very relevant to
the corresponding summaries. These summaries
are highly informative, concise and signiﬁcantly
short compared to original text. We can also see
that many words in the summary do not appear
in the original text, which indicates the signiﬁcant
difference of our dataset from sentence compres-
sion datasets. The summaries of pairs scored by
1 or 2 are highly abstractive and relatively hard to
conclude the summaries from the short text. They
are more likely to be headlines or comments in-
stead of summaries. The statistics show that the
percent of score 1 and 2 is less than 20% of the

Figure 3: Box plot of lengths for short text(ST),
segmented short
sum-
mary(SUM) and segmented summary(Segmented
SUM). The red line denotes the median, and the
edges of the box the quartiles.

text(Segmented ST),

data, which can be ﬁltered by using trained classi-
ﬁer.

Part III contains 1,106 pairs. For this part, 3
annotators label the same 2000 texts and we ex-
tract the text with common scores. This part is
independent from Part I and Part II. In this work,
we use pairs scored by 3, 4 and 5 of this part as the
test set for short text summary generation task.

Part I

2,400,591

Part II

Part III

Number of Pairs
Human Score 1
Human Score 2
Human Score 3
Human Score 4
Human Score 5
Number of Pairs
Human Score 1
Human Score 2
Human Score 3
Human Score 4
Human Score 5

10,666
942
1,039
2,019
3,128
3,538
1,106
165
216
227
301
197

Table 1: Data Statistics

5 Experiment

Recently, recurrent neural network (RNN) have
shown powerful abilities on speech recogni-
tion (Graves et al., 2013), machine transla-
tion (Sutskever et al., 2014) and automatic dialog
response (Shang et al., 2015). However, there is
rare research on the automatic text summarization
by using deep models. In this section, we use RNN
as encoder and decoder to generate the summary
of short text. We use the Part I as the training set

Figure 5: The graphical depiction of RNN encoder
and decoder framework without context.

Figure 6: The graphical depiction of the RNN en-
coder and decoder framework with context.

text is segmented into Chinese words by using
jieba5. The vocabulary is limited to 50,000. We
adopt two deep architectures, 1) The local con-
text is not used during decoding. We use the
RNN as encoder and it’s last hidden state as the
input of decoder, as shown in Figure 5; 2) The
context is used during decoding, following (Bah-
danau et al., 2014), we use the combination of
all the hidden states of encoder as input of the
decoder, as shown in Figure 6. For the RNN,
we adopt the gated recurrent unit (GRU) which is
proposed by (Chung et al., 2015) and has been
proved comparable to LSTM (Chung et al., 2014).
All the parameters (including the embeddings) of
the two architectures are randomly initialized and
ADADELTA (Zeiler, 2012) is used to update the
learning rate. After the model is trained, the beam
search is used to generate the best summaries in
the process of decoding and the size of beam is set
to 10 in our experiment.

5https://pypi.python.org/pypi/jieba/

Figure 4: Five examples of different scores.

and the subset of Part III, which is scored by 3, 4
and 5, as test set.

Two approaches are used to preprocess the data:
1) character-based method, we take the Chinese
character as input, which will reduce the vocab-
ulary size to 4,000. 2) word-based method, the

model

RNN

RNN context

data
Word
Char
Word
Char

R-1
0.177
0.215
0.268
0.299

R-2
0.085
0.089
0.161
0.174

R-L
0.158
0.186
0.241
0.272

Table 2: The experiment result:
“Word” and
“Char” denote the word-based and character-
based input respectively.

Figure 7: An example of the generated summaries.

For evaluation, we adopt the ROUGE metrics
proposed by (Lin and Hovy, 2003), which has
been proved strongly correlated with human eval-
uations. ROUGE-1, ROUGE-2 and ROUGE-L
are used. Because the standard Rouge package 6
is used for evaluating English summarization sys-
tems, we transform the Chinese words to numeri-
cal IDs to adapt to the systems. All the models are
trained on the GPUs tesla M2090 for about one
week.Table 2 lists the experiment results. As we
can see in Figure 7, the summaries generated by
RNN with context are very close to human written
summaries, which indicates that if we feed enough
data to the RNN encoder and decoder, it may gen-
erate summary almost from scratch.

The results also show that the RNN with con-
text outperforms RNN without context on both
character and word based input. This result indi-
cates that the internal hidden states of the RNN
encoder can be combined to represent the context
of words in summary. And also the performances
of the character-based input outperform the word-
based input. As shown in Figure 8, the summary
generated by RNN with context by inputing the
character-based short text is relatively good, while

6http://www.berouge.com/Pages/default.aspx

Figure 8: An example of the generated summaries
with UNKs.

the the summary generated by RNN with context
on word-based input contains many UNKs. This
may attribute to that the segmentation may lead to
many UNKs in the vocabulary and text such as the
person name and organization name. For exam-
ple, “愿景光电子” is a company name which
is not in the vocabulary of word-based RNN, the
RNN summarizer has to use the UNKs to replace
the “愿景光电子” in the process of decoding.

6 Conclusion and Future Work

We constructed a large-scale Chinese short text
summarization dataset and performed RNN-based
methods on it, which achieved some promising re-
sults. This is just a start of deep models on this
task and there is much room for improvement. We
take the whole short text as one sequence, this may
not be very reasonable, because most of short texts
contain several sentences. A hierarchical RNN (Li
et al., 2015) is one possible direction. The rare
word problem is also very important for the gener-
ation of the summaries, especially when the input
is word-based instead of character-based. It is also
a hot topic in the neural generative models such
as neural translation machine(NTM) (Luong et al.,
2014), which can beneﬁt to this task. We also plan
to construct a large document summarization data
set by using naturally annotated web resources.

Acknowledgments

Industry Development
Shenzhen:

supported by National Natu-
This work is
ral Science Foundation of China:
61473101,
61173075 and 61272383, Strategic Emerg-
Funds
Special
ing
JCYJ20140417172417105,
of
JCYJ20140508161040764
and
JCYJ20140627163809422. We thank to Baolin
Peng, Lin Ma, Li Yu and the anonymous reviewers
for their insightful comments.

References

[Bahdanau et al.2014] Dzmitry Bahdanau, Kyunghyun
Cho, and Yoshua Bengio. 2014. Neural machine
translation by jointly learning to align and translate.
CoRR, abs/1409.0473.

[Bing et al.2015] Lidong Bing, Piji Li, Yi Liao, Wai
Lam, Weiwei Guo, and Rebecca Passonneau.
2015. Abstractive multi-document summarization
via phrase selection and merging. In Proceedings of
the ACL-IJCNLP, pages 1587–1597, Beijing, China,
July. Association for Computational Linguistics.

[Chung et al.2014] Junyoung Chung, C¸ aglar G¨ulc¸ehre,
KyungHyun Cho, and Yoshua Bengio. 2014. Em-
pirical evaluation of gated recurrent neural networks
on sequence modeling. CoRR, abs/1412.3555.

[Chung et al.2015] Junyoung Chung, C¸ aglar G¨ulc¸ehre,
2015.
KyungHyun Cho, and Yoshua Bengio.
Gated feedback recurrent neural networks. CoRR,
abs/1502.02367.

[Colmenares et al.2015] Carlos A. Colmenares, Ma-
rina Litvak, Amin Mantrach, and Fabrizio Sil-
vestri.
2015. Heads: Headline generation as
sequence prediction using an abstract feature-rich
In Proceddings of 2015 Conference of the
space.
North American Chapter of
the Association for
Computational Linguistics–Human Language Tech-
nologies (NAACL HLT 2015).

[Graves et al.2013] Alex Graves, Abdel-rahman Mo-
2013. Speech
hamed, and Geoffrey E. Hinton.
recognition with deep recurrent neural networks.
CoRR, abs/1303.5778.

[Hovy and Lin1998] Eduard Hovy and Chin-Yew Lin.
1998. Automated text summarization and the sum-
In Proceedings of a Workshop on
marist system.
Held at Baltimore, Maryland: October 13-15, 1998,
TIPSTER ’98, pages 197–214, Stroudsburg, PA,
USA. Association for Computational Linguistics.

[Hu et al.2014] Baotian Hu, Zhengdong Lu, Hang Li,
and Qingcai Chen.
2014. Convolutional neural
network architectures for matching natural language
sentences. In Advances in Neural Information Pro-
cessing Systems 27, pages 2042–2050. Curran Asso-
ciates, Inc.

[Leskovec et al.2009] Jure Leskovec, Lars Backstrom,
and Jon Kleinberg. 2009. Meme-tracking and the
In Proceedings of
dynamics of the news cycle.
the 15th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ’09,
pages 497–506.

[Li et al.2015] Jiwei Li, Minh-Thang Luong, and Dan
Jurafsky. 2015. A hierarchical neural autoencoder

for paragraphs and documents.
ACL.

In Proceedings of

[Lin and Hovy2003] Chin-Yew Lin and E.H. Hovy.
2003. Automatic evaluation of summaries using
In Proceedings
n-gram co-occurrence statistics.
of 2003 Language Technology Conference (HLT-
NAACL 2003), Edmonton, Canada.

[Luhn1998] H. P. Luhn. 1998. The automatic creation
of literature abstracts. IBM Journal of Research and
Development, 2(2):159–165.

[Luong et al.2014] Thang Luong,

Ilya Sutskever,
Quoc V. Le, Oriol Vinyals, and Wojciech Zaremba.
2014. Addressing the rare word problem in neural
machine translation. CoRR, abs/1410.8206.

[Martins2007] Dipanjan Das Andr F.T. Martins. 2007.
A survey on automatic text summarization. Techni-
cal report, CMU.

[Mihalcea2004] Rada Mihalcea. 2004. Graph-based
ranking algorithms for sentence extraction, applied
to text summarization. In Proceedings of the 42nd
Annual Meeting of the Association for Computa-
tional Linguistics, companion volume, Spain.

[Nenkova and McKeown2011] Ani Nenkova and Kath-
leen McKeown. 2011. Automatic summarization.
Foundations and Trend in Information Retrieval,
5(2-3):103–233.

[Shang et al.2015] Lifeng Shang, Zhengdong Lu, and
Hang Li. 2015. Neural responding machine for
short-text conversation. CoRR, abs/1503.02364.

[Sun2011] Mao Song Sun. 2011. Natural language
procesing based on naturaly annotated web re-
sources. Journal of Chinese Information Process-
ing, 25(6):26–32.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals,
and Quoc V. V Le. 2014. Sequence to sequence
learning with neural networks. In Advances in Neu-
ral Information Processing Systems 27, pages 3104–
3112.

[Zeiler2012] Matthew D. Zeiler. 2012. ADADELTA:
CoRR,

an adaptive learning rate method.
abs/1212.5701.

[Zhou et al.2015] Xiaoqiang Zhou, Baotian Hu, Qing-
cai Chen, Buzhou Tang, and Xiaolong Wang. 2015.
Answer sequence learning with neural networks for
answer selection in community question answering.
In Proceedings of the ACL-IJCNLP, pages 713–718,
Beijing, China, July. Association for Computational
Linguistics.

