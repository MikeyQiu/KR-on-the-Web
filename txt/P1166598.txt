7
1
0
2
 
c
e
D
 
4
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
1
7
9
3
0
.
4
0
7
1
:
v
i
X
r
a

On the Effects of Batch and Weight Normalization in Generative Adversarial
Networks

Sitao Xiang1
Hao Li1, 2, 3
1University of Southern California
2Pinscreen
3USC Institute for Creative Technologies
hao@hao-li.com
sitaoxia@usc.edu

Abstract

Generative adversarial networks (GANs) are highly ef-
fective unsupervised learning frameworks that can generate
very sharp data, even for data such as images with complex,
highly multimodal distributions. However GANs are known
to be very hard to train, suffering from problems such as
mode collapse and disturbing visual artifacts. Batch nor-
malization (BN) techniques have been introduced to address
the training. Though BN accelerates the training in the be-
ginning, our experiments show that the use of BN can be
unstable and negatively impact the quality of the trained
model. The evaluation of BN and numerous other recent
schemes for improving GAN training is hindered by the lack
of an effective objective quality measure for GAN models.
To address these issues, we ﬁrst introduce a weight normal-
ization (WN) approach for GAN training that signiﬁcantly
improves the stability, efﬁciency and the quality of the gen-
erated samples. To allow a methodical evaluation, we intro-
duce squared Euclidean reconstruction error on a test set
as a new objective measure, to assess training performance
in terms of speed, stability, and quality of generated sam-
ples. Our experiments with a standard DCGAN architecture
on commonly used datasets (CelebA, LSUN bedroom, and
CIFAR-10) indicate that training using WN is generally su-
perior to BN for GANs, achiving 10% lower mean squared
loss for reconstruction and signiﬁcantly better qualitative
results than BN. We further demonstrate the stability of WN
on a 21-layer ResNet trained with the CelebA data set.

1. Introduction

Despite their prevalent use, the effects of Batch Nor-
malization (BN) [7] in Generative Adversarial Networks
(GAN) [5] have not been examined carefully. Popular-
ized by the inﬂuential DCGAN architecture [14], the use of
BN in GANs is typically justiﬁed by its perceived training

speedup and stability, but the generated samples often suffer
from visual artifacts and limited variations (mode collapse).
The lack of evidence that BN always improves GAN train-
ing is partly due to the unavailability of quality measures
for GAN models. Being puzzled by this technique, we pro-
pose a methodical evaluation of GAN models and assess
their abilities to generate large variations of samples (mode
coverage). The idea is to hold out a portion of the dataset
as a test dataset and try to ﬁnd the latent code that generates
the closest approximation to these test images. For each
test image, we optimize for the latent code by gradient de-
scent for a ﬁxed number of iterations. The average squared
Euclidean distance between the test samples and the recon-
structed ones is used as a measure of the quality of GANs.
Our experiments show that the reconstruction error cor-
relates with the visual quality of the generated samples, and
while still time consuming, this approach is more efﬁcient
than existing log-likelihood-based evaluation methods. Our
evaluation technique is therefore convenient for monitoring
the progress during training. We show that BN generally ac-
celerates training in early stages, and can increase the suc-
cess rate of GAN training for certain datasets and network
structures where a model without any normalization could
often fail. In many cases though, BN can cause the stability
and generalization power of the model to decrease drasti-
cally. Following the work of Salimans and Kingma [16] and
Arpit et al. [2], we introduce a modiﬁed Weight Normal-
ization (WN) technique for GAN training. Using the same
sets of experiments, we found that our WN approach can
achieve faster and more stable training than BN, as well as
generate equal or higher quality samples than GAN models
without normalization. We believe that our proposed WN
technique is superior than BN in the context of GANs.

2. Related Work

Batch Normalization. Batch Normalization (BN) [7] is a
technique to accelerate the training of deep neural networks

1

and has been shown to be effective in various applications.
In the context of GANs, it ﬁrst appeared in LAPGAN by
Denton et al. [4] (for generator only), and made popular
by the inﬂuential DCGAN architecture by Radford et al.
[14] (for both generator and discriminator). It has since be-
come a common practice, as listed in this overview of GAN
techniques [3] and used in many GAN architectures (e.g.
WGAN [1] and EBGAN [19]). To summarize, BN takes a
batch of samples {x1, x2, . . . , xm} and computes the fol-
lowing:

yi =

xi − µB
σB

· γ + β ,

(1)

where µB and σB are the means and standard deviations
of the input batch and γ and β are the learned parameters.
As a result, the output will always have a mean β and a
standard deviation γ, regardless of the input distribution.
Most importantly, the gradients must be back-propagated
through the computation of µB and σB.

Weight Normalization. Weight Normalization (WN) is a
more recent normalization technique proposed by Salimans
and Kingma [16]. For a linear layer

y = WT x + b ,

where x ∈ Rn, y ∈ Rm, W ∈ Rn×m and b ∈ Rm, weight
normalization performs a reparameterization W with V ∈
Rn×m and g ∈ Rm:

(2)

(3)

wi =

· vi

,

gi
||vi||2

where wi and vi are the i-th column of W and V, respec-
tively. As with BN, the computation of ||vi||2 is taken into
account when computing the gradient with respect to V.

Although presented as a reparameterization that modi-
ﬁes the curvature of the loss function, the main idea is to
simply divide the weight vectors by their norms. A very
similar idea has been proposed around the same time, un-
der “normalization propogation” (NormProp) by Arpit et
al. [2]. While the effectiveness of this technique has been
illustrated on various experiments in [16] and [2], they did
not investigate this acceleration approach for GANs. As de-
tailed in Section 3, we propose a modiﬁed version of Weight
Normalization to improve the training of GAN models.

GAN Evaluation.
In earlier GAN-related works, with a
lack of quantitative measures, visual inspection has been a
commonly used method.
In addition to inspecting visual
quality, this has also been used to show that the model did
not overﬁt, by interpolation in latent space (e.g. [4]) and
by ﬁnding closest training sample to generated samples and
point out their difference (e.g. [5]).

Various quantitative measures has since been proposed.
A commonly used one is estimating the log-likelihood of
the training set in the generator’s distribution, by generating
a large amount of samples and ﬁtting a Gaussian Parzen
window (e.g. [4, 11]). As discussed by Theis et al. [17], this
is not particularly effective, as the amount of samples that
need to be generated for accurate log-likelihood estimation
is intractable.

Another measure is Inception score, proposed by Sali-
mans et al. [15], based on the assumption that a good gener-
ative model should be able to generate meaningful objects.
A limitation of this approach is that, the inception model is
pretrained on another image classiﬁcation task, usually for
natural objects. Thus, it is only useful as a measure of GAN
quality trained on images on similar objects. The quality of
GANs has also been evaluated indirectly, e.g. by measur-
ing the classiﬁcation accuracy using features extracted by a
GAN discriminator [14]. Our proposed measure of recon-
struction loss is most similar to that used by Metz et al. [13].
We discuss differences in section 4.

3. Weight Normalization for GAN Training

We propose a modiﬁed formulation of the weight nor-
malization approach introduced by Salimans and Kingma
[16]. A notable deﬁciency of the original WN technique is
that, in its simplest form, it does not normalize the mean
value of the input. In [16] this is solved by augmenting WN
with a version of BN that only normalizes the mean of the
input but not the variance. While their experiments showed
an improved performance for the CIFAR-10 classiﬁcation
task compared to plain WN, it gave worse results for several
of our experiments (See appendix D.2). Hence, we chose to
not include this augmentation in our approach.

In [2], the authors attempt to solve this problem by en-
forcing a zero-mean, unit-variance distribution throughout
the network. In their method, the scale and bias are ﬁrst
ﬁxed as g = 1 and b = 0, that is,

y =

wT x
||w||2

.

(4)

For simplicity, we consider here a single output neuron.
The training data is normalized so that the input to the net-
work has zero mean and unit variance. The mean and vari-
ance of the output of each nonlinear layer (ReLU in this
case) is evaluated in closed form under the assumption that
the input to the preceding linear layer is from a multivariate
standard normal distribution. The mean and variance is then
used to correct the distribution of the output:

y =

ReLU

(cid:20)

1
σ

(cid:19)

(cid:18) wT x
||w||2

(cid:21)

− µ

,

(5)

where

(cid:114) 1
2π

(cid:115)

(cid:18)

1
2

(cid:19)

1
π

µ =

and σ =

1 −

(6)

Note that we can now separate out the restricted weight
normalized layer from equation 9. We call the remaining
part “Translated ReLU (TReLU)”:

are the mean and standard deviation of the distributions
after ReLU when x is from a multivariate standard normal
distribution. The output y in equation 5 would also have
zero mean and unit variance.

Notice that this ad-hoc ﬁx does not really achieve its
goal. Firstly, as mentioned in [2], the closed form mean and
variance is only an approximation since the correctness of
this derivation requires the input to be normal distributed,
which does not strictly hold beyond the ﬁrst layer. More
critically, after deriving equation 5 and ﬁxing µ and σ, akin
to Batch Normalization, they argue that an afﬁne transfor-
mation needs to be learned after the weight-normalized lin-
ear layer and before the succeeding non-linear layer, in or-
der to avoid decreasing the set of functions that can be rep-
resented by the network. The formulation then becomes:

y =

ReLU

+ β

− µ

.

(7)

(cid:34)

1
σ

(cid:32)

γ (cid:0)wT x(cid:1)
||w||2

(cid:33)

(cid:35)

Their derivation for µ and σ is for the restricted case,
when there is no learned afﬁne transformation, i.e. when
γ = 1 and β = 0. When this restriction is relaxed, the
result would be invalid, even if the i.i.d. normal condition
on x does hold. We could make µ and σ functions of γ and
β to ﬁx this error, but the back-propagation computation
would be overly complex, since these functions also need
to be taken into account.

As we cannot hope to strictly enforce a zero-mean unit-
variance distribution, we propose to use a simpler approx-
imation instead. Note that with ReLU-like nonlinearity
(i.e. ReLU, leaky ReLU and parametric ReLU) we have
ReLU(ax) = a · ReLU(x) when a ≥ 0. In equation 7,
when γ < 0, we can always invert the direction of w and
take the negative of γ. Hence, without loss of generality,
we can assume γ ≥ 0. Then equivalently, equation 7 can be
written as

y =

ReLU

(cid:20)

γ
σ

(cid:18) wT x
||w||2

+

β
γ

(cid:19)

−

(cid:21)

µ
γ

.

(8)

The purpose of µ and σ is to cancel out the mean and
variance introduced by ReLU and the afﬁne transformation
(i.e. β and γ). Instead of deriving a complex formula, we
simply set µ = β and σ = γ, and re-formulate the equation
using α = − β

γ . Equation 8 becomes:

γ = − µ

y = ReLU

− α

+ α .

(9)

(cid:19)

(cid:18) wT x
||w||2

TReLUα(x) =ReLU(x − α) + α
(cid:40)

=

x (x ≥ α)
α (x < α)

,

(10)

(11)

where α is a learned parameter. It is more commonly re-
ferred to as a “threshold layer”, deﬁned by y = max{x, α},
but here the threshold is learned. We chose this name to re-
ﬂect the fact that other ReLU-like nonlinear functions can
be used to give translated leaky and parametric ReLU lay-
ers. Here, we “translate” the data by −α, apply the non-
linear function, then “translate” the data “back” (by α). By
using TReLU instead of adding bias to the previous layer,
we prevent (to a certain degree) the introduction of a large
mean into the distribution.

This simpliﬁcation effectively negates the learned afﬁne
transformation, which seemingly would reduce the set of
functions that can be represented by the network. We ar-
gue, however, that allowing the learning of an afﬁne trans-
formation at the last weight-normalized layer recovers the
expressiveness of the entire stack of layers (see appendix A
for proof). From now on, “strict weight-normalized layers”
will refer to layers without afﬁne transformations (Equation
4), while layers with a learned afﬁne transformation

y =

· γ + β

wT x
||w||2

(12)

are referred as “afﬁne weight-normalized layers”. These

are collectively called “weight-normalized layers”.

4. Evaluation Method

For many generative models, the reconstruction error on
the training set is often explicitly optimized in some form
(e.g., Variational Autoencoders [8]). Even when this is not
the case as in GANs, it is natural to evaluate the model with
a reconstruction loss (squared Euclidean distance) mea-
sured on a test set. In the case of GANs, given a generator
G and a set of test samples X = {x(1), x(2), . . . , x(m)}, the
reconstruction loss of G on X is deﬁned as

Lrec(G, X) =

||G(z) − x(i)||2
2

.

(13)

1
m

m
(cid:88)

i=1

min
z

In the case of images, we normalize for different image
sizes by considering per pixel, per color channel reconstruc-
tion loss, thus we divide the loss by 3wh where w and h are
the width and height of the training images. Since there

is no way to directly infer the optimal z from x, we use
an alternative method: starting from an all-zero vector, we
perform gradient descent on the latent code to ﬁnd one that
minimizes the squared Euclidean distance between the sam-
ple generated from the code and the target one. Because the
code is optimized instead of computed from a feed-forward
network, the evaluation process is time-consuming. Thus,
we avoid performing this evaluation at every training iter-
ation when monitoring the training process, and only use
a reduced number of samples and gradient descent steps.
Only for the ﬁnal trained model, we perform an extensive
evaluation on a larger test set, with a larger number of steps.
This method is very similar to that proposed by Metz et
al. [13]. There are two important differences: in [13] the
samples used for reconstruction come from the training set,
while we take the samples from a separate test set. Intu-
itively, in order to generate the test samples that are not in
the training set, the generator must learn the distribution of
the training samples, but not memorize and overﬁt on them.
Such an effect would not be achieved if the test samples
come from the training set.

Furthermore, [13] uses L-BFGS for optimization on the
latent code. L-BFGS is known to give good and fast op-
timization for problems that are not too high-dimension,
which suits the setting of this problem well. However, its
effectiveness is sensitive to many of its parameters. We
were not able to ﬁnd a combination of parameters that con-
sistently work well under the various experiment settings.
This also made it harder to justify the choice of parameters
since for the different models we would like to compare the
best parameters may be very different.

Instead we use RMSProp. It may not be the fastest opti-
mization method for this problem, but we found it to work
well under our settings, and altering the parameters (learn-
ing rate and number of steps) generally affect the recon-
struction result of different models in the same way, which
makes comparison easier.

5. Experiments

We conducted experiments on image generation tasks,
with quantitative analysis on DCGAN-based architecture
on CelebA, LSUN bedroom and CIFAR-10 datasets, and
qualitative results with a 21-layer ResNet on CelebA. The
CelebA experiments are detailed here. Due to limited space,
we only show some generated and reconstructed samples on
LSUN and CIFAR-10 here, and discuss the settings, qual-
itative and quantitative results along with more samples in
Appendices D.3 and D.4.

5.1. DCGAN Setup

For CelebA [10], we using central 160 × 160 patches.
(1) trained

We compared three DCGAN-based models:

without any normalization as a reference (the non-
normalized or “vanilla” model), (2) with Batch Normaliza-
tion (“BN model”), and (3) with our formulation of Weight
Normalization (“WN model”). The network is structured in
the following way: for the discriminator, we use successive
convolution layers with kernel size 4, stride 2, padding 1
and output features doubling that of the previous layer, start-
ing from 64 features in the ﬁrst layer. We add convolution
layers until the spatial size of the feature map is sufﬁciently
small (5×5). We then add one ﬁnal convolution layer with
stride 1, zero padding and kernel size 5 (equaling the size
of the last feature map). For the generator, we reverse this
structure and use transposed convolution layers.

As per common practice, Batch Normalization is not
applied to the ﬁrst
layer of the discriminator, nor to
the last layers of both the discriminator and generator.
Weight normalization is used for every layer. For the
last layer of both discriminator and generator, we use
afﬁne weight-normalized layers (AWNConv) while for ev-
ery other layer we use strict weight-normalized layers
(SWNConv). Parametric ReLU (PReLU) is used for vanilla
and batch-normalized models and Translated Parametric
ReLU (TPReLU) for weight-normalized models. Slope and
bias parameters are learned per-channel. The length of the
code is 256 for all models. The architectures are summa-
rized in table 1. Additional details regarding the imple-
mentation of weight normalized layers are discussed in ap-
pendix B.

All models are optimized with RMSProp [18], with a
learning rate of 10−4, α = 0.9, ε = 10−6 and a batch
size of 32. Speciﬁcally for the BN model, we use sepa-
rate batches for true samples and generated samples when
training the discriminator, as suggested by [3]. After each
parameter update, we clip the learned slope of parametric
ReLU layers to [0, 1]. There are a total of 202,599 images
in CelebA dataset. We randomly selected 2,000 images for
evaluation and used the rest for training. During the train-
ing, we perform a “running evaluation” for every 500 train-
ing iterations, on a randomly selected and ﬁxed subset of
200 test samples. The optimal code is found by performing
gradient descent for 50 steps, starting from a zero vector.
Again we use RMSProp, with a learning rate of 0.01.

For each model, the best performing network during the
training is saved and used for ﬁnal evaluation. In the ﬁnal
evaluation, we use all 2,000 test samples and perform gra-
dient descent for 2,000 steps. For BN model, we use its
inference mode. In addition, we also use the “converged”
model for evaluation, in case the model does converge but
gives notably worse running reconstruction than the optimal
recorded model. However, this did not occur in the main ex-
periment. We consider that training has converged if both
the running reconstruction loss and the generated samples
stay stable for a sufﬁcient amount of time.

Table 1. Network structure for discriminators (top) and genera-
tors (bottom). First three columns: type of layers for vanilla, BN
and WN models, respectively. Fourth column: kernel size, stride,
padding and number of output channels of convolution layer.

vanilla
Conv
-
PReLU
Conv
-
PReLU
Conv
-
PReLU
Conv
-
PReLU
Conv
-
PReLU
Conv
Sigmoid
vanilla
Conv
-
PReLU
Conv
-
PReLU
Conv
-
PReLU
Conv
-
PReLU
Conv
-
PReLU
Conv
Sigmoid

BN
Conv
-
PReLU
Conv
BN
PReLU
Conv
BN
PReLU
Conv
BN
PReLU
Conv
BN
PReLU
Conv
Sigmoid
BN
Conv
BN
PReLU
Conv
BN
PReLU
Conv
BN
PReLU
Conv
BN
PReLU
Conv
BN
PReLU
Conv
Sigmoid

WN
SWNConv
-
TPReLU
SWNConv
-
TPReLU
SWNConv
-
TPReLU
SWNConv
-
TPReLU
SWNConv
-
TPReLU
AWNConv
Sigmoid
WN
SWNConv
-
TPReLU
SWNConv
-
TPReLU
SWNConv
-
TPReLU
SWNConv
-
TPReLU
SWNConv
-
TPReLU
AWNConv
Sigmoid

4, 2, 1, 64

4, 2, 1, 128

4, 2, 1, 256

4, 2, 1, 512

4, 2, 1, 1024

5, 1, 0, 1

5, 1, 0, 1024

4, 2, 1, 512

4, 2, 1, 256

4, 2, 1, 128

4, 2, 1, 64

4, 2, 1, 3

We observed mode collapse issues with both the vanilla
and BN models. To reduce the possibility that these obser-
vations are caused by random factors, we repeat the training
procedure for these models three times. We present the re-
sults from the best training instances and additional ones of
the vanilla and BN models can be found in Appendix D.1.

5.2. Reconstruction

The running reconstruction loss of the three models is
shown in Figure 1 for the ﬁrst 150,000 iterations. The gen-
erated samples from both the vanilla and BN models have
collapsed. The WN model was trained to 700,000 iterations
and is considered to have converged (see Appendix C for

Figure 1. Running reconstruction loss during training.

Table 2. Optimal reconstruction loss of the models

Model Optimal iteration Running loss
vanilla
BN
WN

0.014509
0.017199
0.013010

30,500
30,500
463,000

Final loss
0.006171
0.006355
0.005524

the prolonged training).

The lowest running reconstruction loss recorded dur-
ing training, the iteration at which this minimum loss is
achieved, and the ﬁnal reconstruction loss for each model
is listed in table 2. WN achieves about 10.5% lower ﬁnal
reconstruction loss than the vanilla model, while for BN the
loss is 3% higher. We can also see from the loss curve that,
until the vanilla model collapses, BN never achieved a bet-
ter reconstruction loss.

We also provide qualitative results of the reconstructions.
Selected reconstructed samples are compared to the original
test samples in ﬁgure 2. These samples are selected such
that all three models give reasonable results. Random sam-
ples can be found in Appendix C. The WN model captures
details (e.g. facial expression, texture of hair, subtle color
variation) much more faithfully. Samples reconstructed by
the BN model are signiﬁcantly blurrier and affected by arti-
facts.

5.3. Stability

As shown in Figure 1, the reconstruction of vanilla and
BN models started to get worse relatively early on during
their training, after achieving their optimal reconstruction
loss. For the vanilla model, the loss went up slowly, then in
a relatively short time around iteration 135,000, the gener-
ator collapses and produces the same output, which caused
the reconstruction loss to increase suddenly. For the BN
model, at around 40,000 iterations, the loss started to show
excessive ﬂuctuation. Our WN model however, kept im-
proving steadily until 300,000 iterations and then remained
largely stable.

Figure 2. Selected ﬁnal reconstruction results. From left to right
in each group: test sample, vanilla reconstruction, BN reconstruc-
tion, WN reconstruction. All images best viewed enlarged.

Figure 4. Evolution of samples during early stage of training.
Top 3 rows: vanilla; Middle 3 rows: BN; Bottom 3 rows: WN.
Columns are samples from iterations 100, 200, 300, 400, 500, 600,
800, 1000, 1200, 1500, 2000, 2500, 3000, 4000 and 5000.

Figure 3. Evolution of samples during training. Top 3 rows:
vanilla; Middle 3 rows: BN; Bottom 3 rows: WN. Columns: every
10,000 iterations from 10,000 to 150,000.

Figure 5. Random generated samples on CIFAR-10. Left to right:
vanilla, BN, WN.

We can also visualize this (in)stability by checking sam-
ples generated from the same code at different iterations, as
shown in Figure 3. The WN model is noticeably more sta-
ble as samples generated from the same code remain mostly
constant across a time scale of 100,000 iterations, and the
generated samples are slowly improving, while the other
two models produce more random variations. Additional
visual analysis and samples can be found in Appendix C.

5.4. Training Speed

We compare the training speed of the three models by
assessing their generated samples during early stages of
the training, as illustrated in Figure 4.
It is evident that
Batch Normalization does accelerate training and the effect
of Weight Normalization is comparable. Notice that our
WN model can already produce a human face in only 100
iterations. This accelerated training is mostly useful as a
fast sanity check, when monitoring the training progress of
deep neural networks. As shown in Figure, the visual qual-
ity of the samples generated by the three models are compa-
rable at 10,000 iterations, and none of the models achieve a

noticeably faster progression than the other. In addition, the
ability to generate visually plausible samples earlier on does
not necessarily translate into an overall faster improvement
of the reconstruction. Notice that BN allows a higher learn-
ing rate. The training of the vanilla and WN models often
fail with a learning rate of 0.0002, while the BN model can
still be trainable with a learning rate of 0.001. However,
we found that an increased learning rate did not accelerate
the training of the BN model. Instead, it further harms the
stability of the model.

5.5. Results on LSUN and CIFAR-10

Figures 5 through 8 show random generated and recon-
structed samples of the three models on CIFAR-10 and
LSUN bedroom datasets. The vanilla model failed to train
on LSUN, so only results for the BN and WN models are
shown.

5.6. ResNet Setup

Residual Networks [6] are becoming increasingly pop-
ular for image classiﬁcation. While it has been used in

Figure 6. Random reconstructions on CIFAR-10. In each group,
left to right: test sample, vanilla, BN, WN.

Figure 7. Random generated samples on LSUN. Left: BN, right:
WN.

Figure 8. Random reconstructions on LSUN. In each group, left
to right: test sample, BN, WN.

GANs, the setting is usually an image-to-image translation
task, e.g.
image super-resolution [9]. Direct image gen-
eration from noise with ResNet has not been particularly
successful. Here we test our method on a 21-layer residual
network.

Our block structure is as follows: we base our design on
the basic blocks from [6]. On the shortcut branch, we use an
optional average pooling, present when the stride is 2, fol-
lowed by an optional convolution with kernel size 1, present
when the number of input features does not equal the num-
ber of output features. On the residue branch, we use Conv-
BN-PReLU-Conv-BN structure for the BN model and re-

move batch normalization layers for the vanilla model. The
two branches are then summed, then a ﬁnal PReLU layer is
applied to the result.

In the WN model, all convolutions are replaced with the
strict weight normalized version and PReLU layers are re-
placed with the translated version. There is some compli-
cation when summing the two branches in the WN model,
see Appendex B for more details. The ﬁrst convolution on
the residue branch has kernel size 3 or 4 when the stride
is 1 or 2 respectively. the second convolution always have
kernel size 3. The two convolutions always have padding
1. In the generator, all convolutions are replaced with trans-
posed convolutions, and the average pooling on the shortcut
branch is replaced with a nearest neighbour upscaling.

The discriminator network consists of 5 levels, with each
level consisting of a stride 2 block followed by a stride 1
block with the same number of output features, for a total
of 10 residue blocks and thus 20 layers. Then a ﬁnal con-
volution with kernel size 5 and no padding is added, as in
the DCGAN models above, for a total of 21 layers. Since
the network is much deeper, to save computation time, we
reduced the number of features to (64, 128, 256, 384, 512)
and dimension of the latent space to 128. Again, the gener-
ator is a mirror image of the discriminator. We also reduced
the batch size to 16 and learning rate to 2 × 10−5.

5.7. ResNet Results

During 70,000 iterations of training, the vanilla model
and the BN model were never able to generate more than
a handful of different samples (random samples from itera-
tion 70,000 shown in ﬁgure 9) and were extremely unstable
(evolution of samples for every 10,000 iterations shown in
ﬁgure 10). The WN model was trained to iteration 300,000
without major issues, and was able to generate samples with
high quality and diversity. The best running reconstruction
loss was 0.016906, achieved at iteration 195,000. Random
samples from that iteration are shown in ﬁgure 11.

We do point out however, that with continued training
after around iteration 200,000 we observe some degradation
of sample quality in the weight normalized ResNet model,
in similar ways as in the vanilla DCGAN model examined
in Appendix C. This indicates that Weight Normalization
in itself may not be sufﬁcient to guarantee the stability of
the network. But it is not our goal to compete with other
techniques and ﬁnd a complete solution to the instability of
GAN training. Rather, since our method does not propose
different training loss (e.g. least squares in LSGAN [12]) or
protocol (e.g. batch discrimination) or favour a particular
architecture (e.g. autoencoder-based, in EBGAN [19]), our
method is complementary to these existing GAN training
improvement techniques, and can be combined with any of
these to further improve the quality of GANs.

Our method is not compatible with methods that oper-

Figure 9. Random samples from vanilla and BN ResNet model,
at iteration 70,000. Top 3 rows: vanilla; Bottom 3 rows: BN.

Figure 10. Evolution of samples during ResNet training. Top 3
rows: vanilla; Middle 3 rows: BN; Bottom 3 rows: WN. Columns
are samples from 10,000 to 70,000 iterations at intervals of 10,000
iterations. It is hardly recognizable but samples in the same row
are indeed generated from the same code.

ate on the weights, notably weight clipping in Wasserstein
GAN [1]. But for this particular case, weight normalization
provides an alternative way to weight clipping to enforce
Lipschitz continuity, as discussed in Appendix E.

6. Conclusion

We introduced weight normalization for the training
of GANs using an alternative formulation than the orig-

Figure 11. Random samples from WN ResNet model, at iteration
195,000.

inal work of [16] and [2], which achieves superior train-
ing performance. We also presented an evaluation method
for GANs based on the mean squared Euclidean distance
between the test samples and the closest generated ones,
which are synthesized via gradient descent on a latent code.
We trained and analyzed variants of DCGAN [14] with
different normalization methods for image generation on
datasets of multiple scales. We found that batch-normalized
models perform worse in reconstructing test samples and
In particular, both recon-
are less stable during training.
struction errors and the visual quality can be deteriorated
by BN. However, our formulation of weight normalization
improves both reconstruction quality and training stability
considerably. We further demonstrate the stabilizing power
of weight normalization by successful training of a residual
GAN that is considerably deeper. Based on our extensive
evaluations, we believe that weight normalization should be
used instead of batch normalization when training genera-
tive adversarial networks.

References

[1] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein
gan. arXiv preprint arXiv:1701.07875, 2017. 2, 8, 17
[2] D. Arpit, Y. Zhou, B. U. Kota, and V. Govindaraju.
Normalization propagation: A parametric technique
for removing internal covariate shift in deep networks.
arXiv preprint arXiv:1603.01431, 2016. 1, 2, 3, 8
[3] S. Chintala, E. Denton, M. Arjovsky, and M. Math-
ieu. How to train a gan? tips and tricks to make

gans work. https://github.com/soumith/
ganhacks, 2016. Accessed: 2017-02-26. 2, 4
[4] E. L. Denton, S. Chintala, R. Fergus, et al. Deep gen-
erative image models using a laplacian pyramid of ad-
versarial networks. In Advances in neural information
processing systems, pages 1486–1494, 2015. 2

[5] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Ben-
gio. Generative adversarial nets. In Advances in neu-
ral information processing systems, pages 2672–2680,
2014. 1, 2

[6] K. He, X. Zhang, S. Ren, and J. Sun. Deep resid-
ual learning for image recognition. In Proceedings of
the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016. 6, 7

[7] S. Ioffe and C. Szegedy. Batch normalization: Accel-
erating deep network training by reducing internal co-
variate shift. arXiv preprint arXiv:1502.03167, 2015.
1

[8] D. P. Kingma and M. Welling. Auto-encoding varia-
tional bayes. arXiv preprint arXiv:1312.6114, 2013.
3

[9] C. Ledig, L. Theis, F. Husz´ar, J. Caballero, A. Cun-
ningham, A. Acosta, A. Aitken, A. Tejani, J. Totz,
Z. Wang, et al. Photo-realistic single image super-
resolution using a generative adversarial network.
arXiv preprint arXiv:1609.04802, 2016. 7

[10] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learn-
ing face attributes in the wild. In Proceedings of In-
ternational Conference on Computer Vision (ICCV),
December 2015. 4

[11] A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and
B. Frey. Adversarial autoencoders. arXiv preprint
arXiv:1511.05644, 2015. 2

[12] X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. P.
Smolley. Least squares generative adversarial net-
works. arXiv preprint ArXiv:1611.04076, 2016. 7
[13] L. Metz, B. Poole, D. Pfau, and J. Sohl-Dickstein. Un-
rolled generative adversarial networks. arXiv preprint
arXiv:1611.02163, 2016. 2, 4

[14] A. Radford, L. Metz, and S. Chintala.

Unsu-
pervised representation learning with deep convolu-
tional generative adversarial networks. arXiv preprint
arXiv:1511.06434, 2015. 1, 2, 8

[15] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung,
A. Radford, and X. Chen.
Improved techniques for
training gans. In Advances in Neural Information Pro-
cessing Systems, pages 2234–2242, 2016. 2

[16] T. Salimans and D. P. Kingma. Weight normalization:
A simple reparameterization to accelerate training of
deep neural networks. In Advances in Neural Infor-
mation Processing Systems, pages 901–901, 2016. 1,
2, 8, 14

[17] L. Theis, A. v. d. Oord, and M. Bethge. A note on

the evaluation of generative models. arXiv preprint
arXiv:1511.01844, 2015. 2

[18] T. Tieleman and G. Hinton. Lecture 6.5-rmsprop: Di-
vide the gradient by a running average of its recent
magnitude. COURSERA: Neural networks for ma-
chine learning, 4(2), 2012. 4

[19] J. Zhao, M. Mathieu, and Y. LeCun.

Energy-
based generative adversarial network. arXiv preprint
arXiv:1609.03126, 2016. 2, 7

A. Proof

for The Equivalence Between a
Non-Normalized and a Strict Weight-
Normalized Network with One Afﬁne
Weight-Normalized Layer at the End

Consider two networks with (2n + 1) layers each. The
ﬁrst network is a non-normalized network, where layers
(2k + 1) (0 ≤ k ≤ n) are linear layers and layers 2k (1 ≤
k ≤ n) are ReLU layers. The second network is a weight-
normalized network, where layers (2k +1) (0 ≤ k ≤ n−1)
are strict weight-normalized layers, layer (2n + 1) is an
afﬁne weight-normalized layer, and layers 2k (1 ≤ k ≤ n)
are translated ReLU layers.

We make the following claim:

Claim 1. The aforementioned two networks are capable of
representing the same set of functions.

First, we prove that a linear-and-ReLU combination is
equivalent to a strict weight-normalized-and-TReLU com-
bination, if both are augmented by a learned afﬁne transfor-
mation at the end.

Lemma 2. A linear layer, followed by a ReLU layer, fol-
lowed by an afﬁne transformation, is equivalent to a strict
weight-normalized layer, followed by a TReLU layer, then
by an afﬁne transformation.

Proof. For simplicity we consider the case where the ﬁrst
layer has only one output neuron. Then, a linear layer, fol-
lowed by a ReLU layer, followed by an afﬁne transforma-
tion, becomes

y = ReLU (cid:0)wT x + α(cid:1) · γ + β

(14)

followed by a
while a strict weight-normalized layer,
TReLU layer, followed by an afﬁne transformation would
be

y = TReLUα(cid:48)

· γ(cid:48) + β(cid:48)

(15)

(cid:32)

(cid:33)

w(cid:48)T x
||w(cid:48)||

where w, w(cid:48), α, α(cid:48), β, β(cid:48), γ and γ(cid:48) are learned parameters.

The transformation


w(cid:48) = w

α(cid:48) = − α
||w||
β(cid:48) = β + α · γ

γ(cid:48) = ||w|| · γ

and

(16)





w = w(cid:48)
α = −||w(cid:48)|| · α(cid:48)
β = β(cid:48) + α(cid:48) · γ(cid:48)
γ = γ(cid:48)
||w(cid:48)||

establishes a one-to-one correspondence between these two
forms.

We make the following observations:

Lemma 3. A linear layer preceded by an afﬁne transfor-
mation is equivalent to a single linear layer.

Lemma 4. A linear layer is equivalent to an afﬁne weight-
normalized layer.

These proofs are trivial. Now we demonstrate the proof

of claim 1 by transforming network 1 to network 2:

We perform the following procedure for each k from
1 to (n − 1): ﬁrst, we add an afﬁne transformation be-
tween the ReLU layer 2k and the linear layer (2k + 1).
We then exchange the linear layer (2k − 1) and the ReLU
layer 2k with a strict weight-normalized layer and a TReLU
layer. The additional linear transformations are then re-
moved. By adding and removing an afﬁne transformation
does not change the expressiveness of the network since a
linear layer succeeds it. With an afﬁne layer in place, the ex-
change would not change the expressiveness of the network
either.

Finally, we change the last linear layer (2n + 1) to an

afﬁne weight-normalized layer.

B. Implementation Details

Here we provide some implementation details regarding

the weight normalized layers.

Note that for strided and transposed convolutional layers,
each element in the output tensor receives an input from
only a subset of the ci×kw ×kh elements in the input tensor,
which corresponds to the kernel, where ci is the number
of input features and kw and kh are the kernel width and
Ideally, we should perform weight normalization
height.
for each of these different subsets of weights separately. In
our experiments, we use a simple trick: we compute the
norm of the weight for the full kernel as a whole, and divide
dw × dh where dw and dh are horizontal and
the norm by
vertical strides. This norm is used to normalize the weight
in all different subsets.

√

The ﬁrst layer in the generator deserves some special
treatment. While it can be seen as a transposed convolu-
tional layer, (since the spatial size of the input is 1 × 1), it
can also be viewed as a fully connected layer (with shared
bias between output elements from the same feature map).

These two views do make a difference when Weight Nor-
malization is in place: similar to the case above, each output
element actually receives an input from a subset of ci ele-
ments instead of ci × kw × kh elements, corresponding to
the kernel. Hence, it is more appropriate to implement this
layer as a weight normalized fully connected layer, which
is what we did in our experiments.

√

We also found that weight initialization of the ﬁrst layer
of the generator had an impact on the effect of WN. In
our experiments, initial weights are drawn uniformly from
ci] where ci is the size of the input
[−0.01/
which is just the length of the latent code. For the con-
volutional layers, initial weights are drawn uniformly from
[−1/

ci · kw · kh], as usual.

ci · kw · kh, 1/

ci, 0.01/

√

√

√

When computing the norm, a numerical stability term
ε = 10−6 is added to the sum of squared weights before
taking the square root.

Recall that the purpose of weight normalization is to nor-
malize the mean and variance of the output of a linear layer.
In the strict weight normalized case (equation 4), if each di-
mension of the input vector x is independently drawn from
a distribution with expected value 0 and variance 1, then the
output y will also have expected value 0 and variance 1.

In a residue block, if the shortcut branch and the residue
branch are simply summed, the output distribution will have
variance 2, and the normalizing effect would be lost. One
2, but
possible ﬁx to this is to simply divide the sum by
this will cause the shortcut branch to vanish as the network
grows deeper, which defeats the purpose of residual net-
works. Our solution is what we call “weight normalized
addition”. Consider ﬁrst the simplest case of adding two
variables. We take

√

y =

w1x1 + w2x2
(cid:112)w2
1 + w2
2

(17)

where w1 and w2 are learned weights. This will preserve
the normalizing effect. At the same time, if we set the ini-
tial weight to 0 on the residue branch and 1 on the shortcut
branch, the shortcut branch will be able to “go through” the
network at the beginning, thus the beneﬁts of residual net-
works are also preserved.

When adding two convolutional feature maps, we learn
pairs of weights for each feature channel and share the
weights across spatial locations.

C. Additional Samples and Analysis

Here we show addtitional generated samples from the
three models. In addition, we talk about certain issues with
GAN training that can only be detected by visually inspect-
ing large amounts of samples.

loss. But notice how the diversity of the samples has de-
creased: the lower half of the ﬁgure is dominated by yellow
and brown colors and are darker than the upper half. Even
more subtle, the lower half has less variation in facial ex-
pressions. The gender diversity is also decreasing: some
clear male faces in the upper half become more feminine in
the lower half.

When training beyond a certain amount of iterations, the
samples start to evolve toward the same direction. While
the samples are still different, similar changes can be ob-
served in each iteration. In this process, the difference be-
tween samples is gradually lost. This corresponds to the
slow and steady increase of the reconstruction loss. At
around 130,000 iterations, this process suddenly accelerates
and then the model collapses at a certain point, as shown in
ﬁgure 12.

Interestingly, this appears to be like a reversed behav-
ior of an early stage training. In particular, training usually
starts with a code that generates a similar output and chang-
ing in a similar way until the synthesized samples start to
gain diversity.

C.2. Batch-Normalized Model

Figure 14 shows samples generated by the BN model
from the same set of random codes, at iteration 30,500 (op-
timal iteration) and 110,000. At ﬁrst sight, it does not show
a decrease in diversity as with the vanilla model. But after
comparing the samples carefully, we can discover certain
repeatedly-occurring features. To see this more clearly, in
Figure 15 we picked and rearranged several samples from
Figure 14.

We identiﬁed two groups from samples generated at iter-
ation 110,000. Within each group, while the appearance of
the face varies considerably, almost the exact same expres-
sion is produced. When comparing these samples from ones
that are generated from the same code at iteration 30,500,
we found that a second group is new, while the ﬁrst group
has already existed for a long period of time. This indicates
that the BN model had limited diversity even at its optimal
iteration.

This is a indication of a different cause for mode col-
lapse: as the training progresses, certain features become
dominant. While most samples stay different, more and
more start to acquire these dominating features. In the ex-
treme case, only a handful of different possible outputs re-
main in the end.

Alternatively, mode collapse can suddenly happen, as is
the case for the “failed” training instance 3 in Appendix
D.1. We show the samples that are generated when the
model collapses in Figure 16. Notice that there was no clear
sign of decreased diversity prior to the sudden collapse.

Figure 12. Samples generated by vanilla model, every 100 itera-
tions from 134,000 to 135,400

Figure 13. Random samples generated by vanilla model. Upper
half: iteration 30,500. Lower half: Iteration 120,000.

C.1. Vanilla Model

Figure 13 shows samples generated by the vanilla model
from the same set of random codes, at iteration 30,500 (op-
timal iteration) and 120,000. Samples generated at 120,000
iterations are visually superior on average if each sample
is inspected individually, despite a higher reconstruction

Figure 16. Samples generated by BN model instance 3, every 100
iterations from 38,300 to 39,200

Figure 14. Random samples generated by BN model. Upper half:
iteration 30,500. Lower half: Iteration 110,000.

Figure 15. Selected samples from ﬁgure 14. Row 1 and 3 are
from iteration 30,500. Row 2 and 4 are corresponding samples
from iteration 110,000.

Figure 17. Random samples generated by WN model at iteration
463,000

C.3. Weight-Normalized Model

Figure 17 illustrates random samples generated by the
optimal WN model. In terms of diversity, it is not ideal, as

Figure 18. Running reconstruction loss during training of WN
model

Table 3. Optimal reconstruction loss of repeated training of
vanilla model.

Instance Optimal iteration Running loss
vanilla-1
vanilla-2
vanilla-3

0.014509
0.014703
0.014734

30,500
34,500
31,000

the samples still show a lack of color variation and an un-
balanced gender ratio compared to the ground truth distribu-
tion. However, they show more variations than the vanilla
model and less subtle recurring features compared to the
BN model. The individual samples are of higher quality on
average as well. Also note the relative low rate of “failed”
or highly implausible samples.

Figure 18 shows

the running reconstruction loss
recorded during the whole training process of the WN
model. The loss remains nearly constant after 300,000 iter-
ations, which demonstrates the stability of the WN model.

C.4. Random Reconstructed Samples

Figure 19 shows a random selection of reconstructed

samples.

D. Additional Experiments

D.1. Additional Training Instances of Vanilla and

BN Models

Figures 20 and 21, and Tables 3 and 4 show the recon-
struction loss recorded during all training instances of the
vanilla and batch-normalized models.

We can see that the three instances of vanilla model gave
almost identical loss curves. They achieved similar optimal
loss at similar times, and the mode collapse also happened
around the same time.

In addition to the instability observed for each training
instance, the batch-normalized models also showed “meta-

Figure 19. Randomly reconstructed test samples. From left to
right in each group: test sample, vanilla reconstruction, BN recon-
struction, WN reconstruction.

Figure 20. Running reconstruction loss of repeated training of
vanilla model.

instability” as the behavior differed considerably between
each training instance. Notably, in the third instance, mode
collapse happened very early on. The training did recover

Figure 21. Running reconstruction loss of repeated training of
batch-normalized model.

Figure 22. Running reconstruction loss during training of addi-
tional models.

Table 4. Optimal reconstruction loss of repeated training of batch-
normalized model.

Instance Optimal iteration Running loss

BN-1
BN-2
BN-3

37,000
30,500
16,000

0.017349
0.017199
0.018810

Table 5. Optimal reconstruction loss of additional models.
Optimal iteration Running loss

Model
afﬁne-WN
WN+mean-BN
WN

51,000
19,500
463,000

0.014034
0.016639
0.013010

to some extent, but the model was never able to regain the
same sampling diversity as before the mode collapse.

D.2. Additional Models

For completeness, we also compare different formula-
tions of Weight Normalization. The ﬁrst one is a full-afﬁne
WN model, constructed from the WN model by replac-
ing all strict weight-normalized layers by afﬁne weight-
normalized layers and all TPReLU layers by PReLU lay-
ers. The second one is a model with Weight Normalization
plus mean-only Batch Normalization, as used in [16], con-
structed by taking the afﬁne WN model and adding mean-
only Batch Normalization at those places where regular
Batch Normalization layers would be used in the BN model.
The reconstruction loss for the ﬁrst 150,000 steps are
shown in Figure 22 and Table 5. The results of the WN
model are included for comparison.

The WN with mean-only BN model achieved worse re-
construction than the vanilla model. In addition, although
less severe than the BN model, it results in similar ﬂuctua-
tions of the BN model. We believe that the major advantage
of WN over BN is its independence from batch statistics.

Figure 23. Running reconstruction loss during training of the WN
and afﬁne-WN models

By adding mean-only BN, this dependency is re-introduced,
which harms the stability of the model.

We are also interested in the comparison between the
WN model and the afﬁne-WN model, as it compares our
formulation of Weight Normalization against the one orig-
inally one in [16]. For this purpose, we trained the afﬁne-
WN model using also 700,000 iterations. Figure 23 shows
the comparison of their reconstruction loss during training.

After making a quick descent in the beginning, the run-
ning reconstruction loss of the afﬁne-WN model start to
increase steadily. Unlike the vanilla model, the generated
samples kept stable and are of high-quality. Hence, we
evaluate both the optimal model and the model at iteration
700,000. The results are compared with the WN model in
table 6.

Surprisingly, the afﬁne-WN model at iteration 700,000
yields equally good 2,000-step reconstructions as with iter-
ation 51,000, when the model achieved optimal 50-step re-
construction. Both of them, however, are about 7.5% worse
than the WN model.

Table 6. Optimal and converged reconstruction loss of WN and
afﬁne-WN models

Table 7. Optimal and converged reconstruction loss of the models
on CIFAR-10

Iteration Running loss
Model
afﬁne-WN
51,000
afﬁne-WN 700,000
463,000

0.014034
0.020478
0.013010

WN

Final loss
0.005941
0.005939
0.005525

Model
vanilla
BN
WN
vanilla
BN
WN

Iteration Running loss
387,000
1,000
50,000
500,000
500,000
500,000

0.010382
0.017987
0.010906
0.010948
0.019287
0.014195

Final loss
0.003413
0.004904
0.003509
0.003414
0.005421
0.003269

Figure 24. Reconstruction loss during training on CIFAR-10.

D.3. Experiments on CIFAR-10 Dataset

There are 60,000 images (training plus validation) of size
32×32 in the CIFAR-10 dataset. We construct models in
similar ways as for CelebA, but begin with 96 output chan-
nels for the ﬁrst convolutional layer and stop further con-
volutions when the spatial size of the feature map reaches
4×4. The length of the code (256) and training batch size
(32) remains the same.

We use 58,000 images for training and 2,000 images for
evaluation. During training, evaluation is performed every
1,000 training iterations on 400 images, with 50 gradient
descent steps. Final evaluation is performed on the whole
test set with 2,000 gradient descent steps.

BN is still the worst model compared to the vanilla and
WN models. Now the WN model achieves optimal loss
early on, but then becomes worse. On the other hand, the
vanilla model keeps improving. However, both models con-
verges, as shown by the ﬂat section in the loss curve, be-
tween iterations 400,000 and 500,000. So we take these
two models at iteration 500,000 for evaluation in additional
to the optimal 50-step models. The BN model does not ac-
tually converge, as the rapidly changing “recurring feature”,
discussed in Section C, occurs. For completeness however,
we also take the BN model from iteration 500,000 for eval-
uation.

The seemingly worse 500,000-iteration WN model
turned out to give the best ﬁnal reconstruction result. A
more careful examination of the reconstruction process re-
vealed that the 500,000-iteration WN model achieved bet-

Figure 25. CIFAR-10 samples. Top to bottom: vanilla model,
BN model, WN model. Left: optimal iteration. Right: iteration
500,000.

ter reconstruction than the optimal vanilla model at around
400th reconstruction step. We acknowledge that this ex-
poses a weakness of our evaluation method: performing
the reconstruction for too few steps may give inaccurate
results, while too many steps would be time-consuming,
which makes it unsuitable for training process monitoring.
Figure 25 shows random samples generated by the three

Figure 26. CIFAR-10 reconstructions. Each group, left to right:
test sample, vanilla model, BN model, WN model.

models, at their optimal iteration and at iteration 500,000.
While the visual quality of samples from all models are
good, the results are consistent in terms of diversity with
the analysis in Appendix C. The vanilla samples look dull
and are dominated by one color (green); the BN samples
show a recurring feature (marked with red border).

Figure 26 shows random test samples and reconstructed

ones.

D.4. Experiments on LSUN Bedroom Dataset

There are 3,033,042 images in the bedroom class of the
LSUN dataset, with images having 256 pixels on the shorter
side. Unlike many published results on this dataset, we
use the full-sized images. We crop with centered 256×256
patches but do not down-sample the image. We construct
models in a similar way as with CelebA, but stop further
convolution when the spatial size of the feature map reaches
4×4 and use a code length of 512. Due to the large size of
the images and the network, we reduce the batch sizes to 12
to save computation.

We use 2,000 images for evaluation and the rest for train-
ing. During training, evaluation is performed every 1,000
iterations on 200 images, with 50 gradient descent steps. Fi-
nal evaluation is performed on the whole test set with 2,000
gradient descent steps.

For the vanilla model, training fails constantly, even
when we reduce the learning rate by a factor of 10 (to 10−5),
so only the BN and WN models are compared here. The BN
model collapsed at iteration 330,800. The WN model was
trained with 600,000 iterations. The reconstruction loss is
shown in Figure 27 and Table 8.

Random samples generated by the two models are shown
in Figures 28 and 29. Reconstruction of random samples are
shown in ﬁgure 30.

In the BN samples, recurring tile-like artifacts are ob-

Figure 27. Reconstruction loss during training on LSUN bedroom
dataset.

Table 8. Reconstruction loss of the models on LSUN bedroom
dataset

Model Optimal iteration Running loss

BN
WN

125,000
478,000

0.020943
0.016266

Final loss
0.011051
0.008546

Figure 28. LSUN samples generated by the BN model at iteration
125,000

served. The best quality samples that are generated by the
BN model are arguably sharper and cleaner, while the WN
model reproduces details more accurately.

E. Connection to Wasserstein GAN

For Wasserstein GANs [1], the discriminator is replaced
with a critic, that is K-Lipschitz-continuous for some con-
stant K and only depends on the structure of the network.
To achieve this, they clipped the parameters of the critic net-
work to a small window [−0.01, 0.01] after each parameter
update during training.

We claim that our weight-normalized discriminator is

Lipschitz-continuous with a small modiﬁcation:

Claim 1. The weight-normalized discriminator proposed in
this paper is K-Lipschitz-continuous for some constant K
if the sigmoid layer is removed and the only afﬁne weight-
normalized layer is replaced by a strict weight-normalized
layer.

To see this, we ﬁrst prove the following lemma:

Lemma 2. For a strict weight-normalized layer

yi =

T x
wi
||wi||
where x ∈ Rn, y ∈ Rm, W ∈ Rn×m is the weight
matrix and wi the i-th column of W. If the loss function of
the network is L, then

(18)

,

n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂L
∂xi

(cid:12)
(cid:12)
(cid:12)
(cid:12)

√

≤

n ·

m
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂L
∂yi

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(19)

Proof.

n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂L
∂xi

(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

m
(cid:88)

(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂yj
∂xi

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂L
∂yj

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

=

=

≤

i=1

m
(cid:88)

j=1

m
(cid:88)

j=1
m
(cid:88)

j=1

j=1
(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂L
∂yj

∂L
∂yj

(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂L
∂yj

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

·

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:33)

∂yj
∂xi

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:33)

|wij|
||wj||

n
(cid:88)

i=1

n
(cid:88)

i=1

(cid:19)

√

·

n

√

=

n ·

m
(cid:88)

j=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂L
∂yj

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Figure 29. LSUN samples generated by the WN model at iteration
299,000

Figure 30. LSUN reconstructions. Each group, left to right: test
sample, BN model, WN model.

For a strict weight-normalized convolution layer with cI
n in

input channels and kernel size kW × kH , change
cI · kW · kH .
inequality 19 to

√

√

Note that in our implementation, the learned slope of
parametric ReLU layers are clipped to [0, 1], so the follow-
ing becomes obvious:

Lemma 3. For a TPReLU layer with input x and output y
in Rn,

n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂L
∂xi

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂L
∂yi

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(20)

Now it is easy to see that claim 1 is true since for each
layer, the sum of absolute value of gradients grows by at
most a constant factor. So with such a modiﬁcation, our
discriminator changes into a WGAN critic.

