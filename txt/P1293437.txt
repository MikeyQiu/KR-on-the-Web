Wasserstein Distance Guided Representation Learning
for Domain Adaptation

Jian Shen, Yanru Qu, Weinan Zhang∗, Yong Yu
Shanghai Jiao Tong University
{rockyshen, kevinqu, wnzhang, yyu}@apex.sjtu.edu.cn

8
1
0
2
 
r
a

M
 
9
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
7
1
2
1
0
.
7
0
7
1
:
v
i
X
r
a

Abstract

Domain adaptation aims at generalizing a high-performance
learner on a target domain via utilizing the knowledge dis-
tilled from a source domain which has a different but re-
lated data distribution. One solution to domain adaptation
is to learn domain invariant feature representations while
the learned representations should also be discriminative in
prediction. To learn such representations, domain adaptation
frameworks usually include a domain invariant representation
learning approach to measure and reduce the domain discrep-
ancy, as well as a discriminator for classiﬁcation. Inspired by
Wasserstein GAN, in this paper we propose a novel approach
to learn domain invariant feature representations, namely
Wasserstein Distance Guided Representation Learning (WD-
GRL). WDGRL utilizes a neural network, denoted by the
domain critic, to estimate empirical Wasserstein distance be-
tween the source and target samples and optimizes the feature
extractor network to minimize the estimated Wasserstein dis-
tance in an adversarial manner. The theoretical advantages of
Wasserstein distance for domain adaptation lie in its gradi-
ent property and promising generalization bound. Empirical
studies on common sentiment and image classiﬁcation adap-
tation datasets demonstrate that our proposed WDGRL out-
performs the state-of-the-art domain invariant representation
learning approaches.

Introduction
Domain adaptation deﬁnes the problem when the target do-
main labeled data is insufﬁcient, while the source domain
has much more labeled data. Even though the source and
target domains have different marginal distributions (Ben-
David et al. 2007; Pan and Yang 2010), domain adaptation
aims at utilizing the knowledge distilled from the source
domain to help target domain learning. In practice, unsu-
pervised domain adaptation is concerned and studied more
commonly since manual annotation is often expensive or
time-consuming. Faced with the covariate shift and the lack
of annotations, conventional machine learning methods may
fail to learn a high-performance model.

To effectively transfer a classiﬁer across different do-
mains, different methods have been proposed, including
instance reweighting (Mansour, Mohri, and Rostamizadeh

∗Weinan Zhang is the corresponding author.

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

2009), subsampling (Chen, Chen, and Weinberger 2011),
feature mapping (Tzeng et al. 2014) and weight regular-
ization (Rozantsev, Salzmann, and Fua 2016). Among these
methods feature mapping has shown great success recently,
which projects the data from different domains to a com-
mon latent space where the feature representations are do-
main invariant. Recently, deep neural networks, as a great
tool to automatically learn effective data representations,
have been leveraged in learning knowledge-transferable fea-
ture representations for domain adaptation (Glorot, Bordes,
and Bengio 2011; Chen et al. 2012; Zhuang et al. 2015;
Long et al. 2015; Ganin et al. 2016).

On the other hand, generative adversarial nets (GANs)
(Goodfellow et al. 2014) are heavily studied during recent
years, which play a minimax game between two adversar-
ial networks: the discriminator is trained to distinguish real
data from the generated data, while the generator learns to
generate high-quality data to fool the discriminator. It is in-
tuitive to employ this minimax game for domain adaptation
to make the source and target feature representations in-
distinguishable. These adversarial adaptation methods have
become a popular solution to reduce domain discrepancy
through an adversarial objective with respect to a domain
classiﬁer (Ganin et al. 2016; Tzeng et al. 2017). However,
when the domain classiﬁer network can perfectly distinguish
target representations from source ones, there will be a gra-
dient vanishing problem. A more reasonable solution would
be to replace the domain discrepancy measure with Wasser-
stein distance, which provides more stable gradients even if
two distributions are distant (Arjovsky, Chintala, and Bottou
2017).

In this paper, we propose a domain invariant representa-
tion learning approach to reduce domain discrepancy for do-
main adaptation, namely Wasserstein Distance Guided Rep-
resentation Learning (WDGRL), inspired by recently pro-
posed Wasserstein GAN (Arjovsky, Chintala, and Bottou
2017). WDGRL trains a domain critic network to estimate
the empirical Wasserstein distance between the source and
target feature representations. The feature extractor network
will then be optimized to minimize the estimated Wasser-
stein distance in an adversarial manner. By iterative adver-
sarial training, we ﬁnally learn feature representations in-
variant to the covariate shift between domains. Additionally,
WDGRL can be easily adopted in existing domain adap-

tation frameworks (Tzeng et al. 2014; Long et al. 2015;
Zhuang et al. 2015; Long et al. 2016; Bousmalis et al. 2016)
by replacing the representation learning approaches. Em-
pirical studies on common domain adaptation benchmarks
demonstrate that WDGRL outperforms the state-of-the-art
representation learning approaches for domain adaptation.
Furthermore, the visualization of learned representations
clearly shows that WDGRL successfully uniﬁes two domain
distributions, as well as maintains obvious label discrimina-
tion.

Related Works
Domain adaptation is a popular subject in transfer learn-
ing (Pan and Yang 2010). It concerns covariate shift be-
tween two data distributions, usually labeled source data
and unlabeled target data. Solutions to domain adapta-
tion problems can be mainly categorized into three types:
i). Instance-based methods, which reweight/subsample the
source samples to match the distribution of the target do-
main, thus training on the reweighted source samples guar-
antees classiﬁers with transferability (Huang et al. 2007;
Chen, Weinberger, and Blitzer 2011; Chu, De la Torre, and
Cohn 2013). ii). Parameter-based methods, which trans-
fer knowledge through shared or regularized parameters of
source and target domain learners, or by combining mul-
tiple reweighted source learners to form an improved tar-
get learner (Duan, Xu, and Chang 2012; Rozantsev, Salz-
mann, and Fua 2016). iii). The last but the most popu-
lar and effective methods are feature-based, which can be
further categorized into two groups (Weiss, Khoshgoftaar,
and Wang 2016). Asymmetric feature-based methods trans-
form the features of one domain to more closely match
another domain (Hoffman et al. 2014; Kandemir 2015;
Courty et al. 2017) while symmetric feature-based methods
map different domains to a common latent space where the
feature distributions are close.

Recently, deep learning has been regarded as a powerful
way to learn feature representations for domain adaptation.
Symmetric feature-based methods are more widely studied
since it can be easily incorporated into deep neural networks
(Chen et al. 2012; Zhuang et al. 2015; Long et al. 2015;
Ganin et al. 2016; Bousmalis et al. 2016; Luo et al. 2017).
Among symmetric feature-based methods, minimizing the
maximum mean discrepancy (MMD) (Gretton et al. 2012)
metric is effective to minimize the divergence of two dis-
tributions. MMD is a nonparametric metric that measures
the distribution divergence between the mean embeddings
of two distributions in reproducing kernel Hilbert space
(RKHS). The deep domain confusion (DDC) method (Tzeng
et al. 2014) utilized MMD metric in the last fully connected
layer in addition to the regular classiﬁcation loss to learn rep-
resentations that are both domain invariant and discrimina-
tive. Deep adaptation network (DAN) (Long et al. 2015) was
proposed to enhance the feature transferability by minimiz-
ing multi-kernel MMD in several task-speciﬁc layers. On the
other hand, correlation alignment (CORAL) method (Sun,
Feng, and Saenko 2016) was proposed to align the second-
order statistics of the source and target distributions with a
linear transformation and (Sun and Saenko 2016) extended

CORAL and proposed Deep CORAL to learn a nonlinear
transformation that aligns correlations of layer activations in
deep neural networks.

Another class of symmetric feature-based methods uses
an adversarial objective to reduce domain discrepancy. Mo-
tivated by theory in (Ben-David et al. 2007; Ben-David et
al. 2010) suggesting that a good cross-domain representa-
tion contains no discriminative information about the origin
(i.e. domain) of the input, domain adversarial neural network
(DANN) (Ajakan et al. 2014; Ganin et al. 2016) was pro-
posed to learn domain invariant features by a minimax game
between the domain classiﬁer and the feature extractor. In
order to back-propagate the gradients computed from the
domain classiﬁer, DANN employs a gradient reversal layer
(GRL). On the other hand, (Tzeng et al. 2017) proposed a
general framework for adversarial adaptation by choosing
adversarial loss type with respect to the domain classiﬁer
and the weight sharing strategy. Our proposed WDGRL can
also be viewed as an adversarial adaptation method since it
evaluates and minimizes the empirical Wasserstein distance
in an adversarial manner. Our WDGRL differs from previ-
ous adversarial methods: i). WDGRL adopts an iterative ad-
versarial training strategy, ii). WDGRL adopts Wasserstein
distance as the adversarial loss which has gradient superior-
ity.

Another related work for domain adaptation is optimal
transport (Courty, Flamary, and Tuia 2014; Courty et al.
2017), which is equivalent to Wasserstein distance. And
(Redko, Habrard, and Sebban 2016) gave a theoretical anal-
ysis that Wasserstein distance can guarantee generalization
for domain adaptation. Though these works utilized Wasser-
stein distance in domain adaptation, there are distinct differ-
ences between WDGRL and the previous ones: these works
are asymmetric feature-based methods which design a trans-
formation from source representations to target ones based
on optimal transport while WDGRL is a symmetric method
that projects both domains to a common latent space to learn
domain invariant features. And WDGRL can be integrated
into other symmetric feature-based adaptation frameworks.
Besides learning shared representations, domain separa-
tion network (DSN) (Bousmalis et al. 2016) was proposed
to explicitly separate private representations for each domain
and shared ones between the source and target domains. The
private representations were learned by deﬁning a difference
loss via a soft orthogonality constraint between the shared
and private representations while the shared representations
were learned by DANN or MMD mentioned above. With the
help of reconstruction through private and shared represen-
tations together, the classiﬁer trained on the shared represen-
tations can better generalize across domains. Since our work
focuses on learning the shared representations, it can also be
integrated into DSN easily.

Wasserstein Metric
Before we introduce our domain invariant feature represen-
tation learning approach, we ﬁrst give a brief introduction of
the Wasserstein metric. The Wasserstein metric is a distance
measure between probability distributions on a given metric
space (M, ρ), where ρ(x, y) is a distance function for two

instances x and y in the set M . The p-th Wasserstein dis-
tance between two Borel probability measures P and Q is
deﬁned as

Wp(P, Q) =

(cid:90)

(cid:16)

inf
µ∈Γ(P,Q)

ρ(x, y)pdµ(x, y)

(cid:17)1/p

,

(1)

where P, Q ∈ {P : (cid:82) ρ(x, y)pdP(x) < ∞, ∀y ∈ M } are
two probability measures on M with ﬁnite p-th moment and
Γ(P, Q) is the set of all measures on M × M with marginals
P and Q. Wasserstein metric arises in the problem of optimal
transport: µ(x, y) can be viewed as a randomized policy for
transporting a unit quantity of some material from a random
location x to another location y while satisfying the marginal
constraint x ∼ P and y ∼ Q. If the cost of transporting a unit
of material from x ∈ P to y ∈ Q is given by ρ(x, y)p, then
Wp(P, Q) is the minimum expected transport cost.

The Kantorovich-Rubinstein theorem shows that when M
is separable, the dual representation of the ﬁrst Wasserstein
distance (Earth-Mover distance) can be written as a form of
integral probability metric (Villani 2008)

W1(P, Q) = sup

Ex∼P[f (x)] − Ex∼Q[f (x)],

(2)

(cid:107)f (cid:107)L≤1

where the Lipschitz semi-norm is deﬁned as (cid:107)f (cid:107)L =
sup |f (x) − f (y)|/ρ(x, y). In this paper, for simplicity,
Wasserstein distance represents the ﬁrst Wasserstein dis-
tance.

Wasserstein Distance Guided
Reprensentation Learning

i , ys

i )}ns

Problem Deﬁnition
In unsupervised domain adaptation problem, we have a la-
i=1 of ns samples
beled source dataset X s = {(xs
from the source domain Ds which is assumed sufﬁcient to
train an accurate classiﬁer, and an unlabeled target dataset
X t = {xt
j=1 of nt samples from the target domain Dt.
It is assumed that the two domains share the same feature
space but follow different marginal data distributions, Pxs
and Pxt respectively. The goal is to learn a transferable clas-
siﬁer η(x) to minimize target risk (cid:15)t = Pr(x,y)∼Dt[η(x) (cid:54)=
y] using all the given data.

j}nt

Domain Invariant Representation Learning
The challenge of unsupervised domain adaptation mainly
lies in the fact that two domains have different data distribu-
tions. Thus the model trained with source domain data may
be highly biased in the target domain. To solve this problem,
we propose a new approach to learn feature representations
invariant to the change of domains by minimizing empirical
Wasserstein distance between the source and target repre-
sentations through adversarial training.

In our adversarial representation learning approach, there
is a feature extractor which can be implemented by a neural
network. The feature extractor is supposed to learn the do-
main invariant feature representations from both domains.
Given an instance x ∈ Rm from either domain, the feature
extractor learns a function fg : Rm → Rd that maps the

instance to a d-dimensional representation with correspond-
ing network parameter θg. And then in order to reduce the
discrepancy between the source and target domains, we use
the domain critic, as suggested in (Arjovsky, Chintala, and
Bottou 2017), whose goal is to estimate the Wasserstein dis-
tance between the source and target representation distribu-
tions. Given a feature representation h = fg(x) computed
by the feature extractor, the domain critic learns a function
fw : Rd → R that maps the feature representation to a real
number with parameter θw. Then the Wasserstein distance
between two representation distributions Phs and Pht, where
hs = fg(xs) and ht = fg(xt), can be computed according
to Eq. (2)
W1(Phs , Pht ) = sup

hs [fw(h)] − EP

ht [fw(h)]

EP

(cid:107)fw (cid:107)L≤1

= sup

(cid:107)fw (cid:107)L≤1

EP

xs [fw(fg(x))] − EP

xt [fw(fg(x))].

(3)
If the parameterized family of domain critic functions {fw}
are all 1-Lipschitz, then we can approximate the empirical
Wasserstein distance by maximizing the domain critic loss
Lwd with respect to parameter θw
1
ns

Lwd(xs, xt) =

fw(fg(xs))−

fw(fg(xt)).

1
nt

(cid:88)

(cid:88)

xs∈X s

xt∈X t

(4)
Here comes the question of enforcing the Lipschitz con-
straint. (Arjovsky, Chintala, and Bottou 2017) proposed to
clip the weights of domain critic within a compact space
[−c, c] after each gradient update. However (Gulrajani et al.
2017) pointed out that weight clipping will cause capacity
underuse and gradient vanishing or exploding problems. As
suggested in (Gulrajani et al. 2017), a more reasonable way
is to enforce gradient penalty Lgrad for the domain critic
parameter θw

Lgrad(ˆh) = ((cid:107)∇ˆhfw(ˆh)(cid:107)2 − 1)2,
(5)
where the feature representations ˆh at which to penalize the
gradients are deﬁned not only at the source and target rep-
resentations but also at the random points along the straight
line between source and target representation pairs. So we
can ﬁnally estimate the empirical Wasserstein distance by
solving the problem

max
θw

{Lwd − γLgrad}

(6)

where γ is the balancing coefﬁcient.

Since the Wasserstein distance is continuous and differ-
entiable almost everywhere, we can ﬁrst train the domain
critic to optimality. Then by ﬁxing the optimal parameter of
domain critic and minimizing the estimator of Wasserstein
distance, the feature extractor network can learn feature rep-
resentations with domain discrepancy reduced. Up to now
the representation learning can be achieved by solving the
minimax problem

min
θg

max
θw

{Lwd − γLgrad}

(7)

where γ should be set 0 when optimizing the minimum op-
eration since the gradient penalty should not guide the rep-
resentation learning process. By iteratively learning feature

Algorithm 1 Wasserstein Distance Guided Representation
Learning Combining with Discriminator
Require: source data X s; target data X t; minibatch size m; critic
training step n; coefﬁcient γ, λ; learning rate for domain critic
α1; learning rate for classiﬁcation and feature learning α2
1: Initialize feature extractor, domain critic, discriminator with

random weights θg, θw, θc

2: repeat
3:
4:
5:
6:

Sample minibatch {xs
for t = 1, ..., n do

i , ys

i }m

i=1, {xt

i}m

i=1 from X s and X t

hs ← fg(xs), ht ← fg(xt)
Sample h as the random points along straight lines be-
tween hs and ht pairs
ˆh ← {hs, ht, h}
θw ← θw + α1∇θw [Lwd(xs, xt) − γLgrad(ˆh)]

7:
8:
9:
10:
11:
12: until θg, θw, θc converge

end for
θc ← θc − α2∇θc Lc(xs, ys)
θg ← θg − α2∇θg [Lc(xs, ys) + Lwd(xs, xt)]

Note that this algorithm can be trained by the standard
back-propagation with two iterative steps. In a mini-batch
containing labeled source data and unlabeled target data, we
ﬁrst train the domain critic network to optimality by optimiz-
ing the max operator via gradient ascent and then update the
feature extractor by minimizing the classiﬁcation loss com-
puted by labeled source data and the estimated Wasserstein
distance simultaneously. The learned representations can be
domain invariant and target discriminative since the parame-
ter θg receives the gradients from both the domain critic and
the discriminator loss.

Theoretical Analysis
In this section, we give some theoretical analysis about the
advantages of using Wasserstein distance for domain adap-
tation.

Gradient Superiority
In domain adaptation, to minimize
the divergence between the data distributions Pxs and Pxt,
the symmetric feature-based methods learn a transformation
function to map the data from the original space to a com-
mon latent space with a distance measure. There are two sit-
uations after the mapping: i). The two mapped feature dis-
tributions have supports that lie on low dimensional man-
ifolds (Narayanan and Mitter 2010) in the latent space. In
such situation, there will be a gradient vanishing problem if
adopting the domain classiﬁer to make data indistinguish-
able while Wasserstein distance could provide reliable gra-
dients (Arjovsky, Chintala, and Bottou 2017). ii). The fea-
ture representations may ﬁll in the whole space since the
feature mapping usually reduces dimensionality. However,
if a data point lies in the regions where the probability of
one distribution could be ignored compared with the other
distribution, it makes no contributions to the gradients with
traditional cross-entropy loss since the gradient computed by
this data point is almost 0. If we adopt Wasserstein distance
as the distance measure, stable gradients can be provided
wherever. So theoretically in either situation, WDGRL can
perform better than previous adversarial adaptation methods

Figure 1: WDGRL Combining with Discriminator.

representations with lower Wasserstein distance, the adver-
sarial objective can ﬁnally learn domain invariant feature
representations.

Combining with Discriminator
As mentioned above, our ﬁnal goal is to learn a high-
performance classiﬁer for the target domain. However, the
process of WDGRL is in an unsupervised setting, which
may result in that the learned domain invariant representa-
tions are not discriminative enough. Hence it is necessary
to incorporate the supervision signals of source domain data
into the representation learning process as in DANN (Ganin
et al. 2016). Next we further introduce the combination of
the representation learning approaches and a discriminator,
of which the overview framework is given by Figure 1. A de-
tailed algorithm of the combination is given in Algorithm 1.
We further add several layers as the discriminator after the
feature extractor network. Since WDGRL guarantees trans-
ferability of the learned representations, the shared discrim-
inator can be directly applied to target domain prediction
when training ﬁnished. The objective of the discriminator
fc : Rd → Rl is to compute the softmax prediction with
parameter θc where l is the number of classes. The discrim-
inator loss function is deﬁned as the cross-entropy between
the predicted probabilistic distribution and the one-hot en-
coding of the class labels given the labeled source data:

1
ns

ns
(cid:88)

l
(cid:88)

i=1

k=1

Lc(xs, ys) = −

1(ys

i = k) · log fc(fg(xs

i ))k,

i = k) is the indicator function and fc(fg(xs

(8)
where 1(ys
i ))k
corresponds to the k-th dimension value of the distribution
fc(fg(xs
i )). By combining the discriminator loss, we attain
our ﬁnal objective function

(cid:110)

min
θg,θc

Lc + λ max
θw

(cid:104)
Lwd − γLgrad

(cid:105)(cid:111)
,

(9)

where λ is the coefﬁcient that controls the balance between
discriminative and transferable feature learning and γ should
be set 0 when optimizing the minimum operator.

(Ganin et al. 2016; Tzeng et al. 2017).

Proof.

Generalization Bound (Redko, Habrard, and Sebban
2016) proved that the target error can be bounded by the
Wasserstein distance for empirical measures. However, the
generalization bound exists when assuming the hypothesis
class is a unit ball in RKHS and the transport cost function
is RKHS distance. In this paper we prove the generalization
bound in terms of the Kantorovich-Rubinstein dual formu-
lation under a different assumption.

We ﬁrst formalize some notations that will be used in the
following statements. Let X be an instance set and {0, 1}
be the label set for binary classiﬁcation. We denote by µs
the distribution of source instances on X and use µt for the
target domain. We denote that two domains have the same
labeling function f : X → [0, 1] which is always assumed
to hold in domain adaptation problem. A hypothesis class H
is a set of predictor functions, ∀h ∈ H, h : X → [0, 1]. The
probability according to the distribution µs that a hypothesis
h disagrees with the labeling function f (which can also be
a hypothesis) is deﬁned as (cid:15)s(h, f ) = Ex∈µs[|h(x) − f (x)|].
We use the shorthand (cid:15)s(h) = (cid:15)s(h, f ) and (cid:15)t(h) is de-
ﬁned the same. We now present the Lemma that introduces
Wasserstein distance to relate the source and target errors.

Lemma 1. Let µs, µt ∈ P(X ) be two probability measures.
Assume the hypotheses h ∈ H are all K-Lipschitz continu-
ous for some K. Then the following holds

(cid:15)t(h, h(cid:48)) ≤ (cid:15)s(h, h(cid:48)) + 2KW1(µs, µt)

(10)

for every hypothesis h, h(cid:48) ∈ H.

Proof. We ﬁrst prove that for every K-Lipschitz continuous
hypotheses h, h(cid:48) ∈ H, |h − h(cid:48)| is 2K-Lipschitz continuous.
Using the triangle inequality, we have

|h(x)−h(cid:48)(x)| ≤ |h(x)−h(y)|+|h(y)−h(cid:48)(x)|

≤ |h(x)−h(y)|+|h(y)−h(cid:48)(y)|+|h(cid:48)(x)−h(cid:48)(y)|
(11)

and thus for every x, y ∈ X ,

|h(x)−h(cid:48)(x)|−|h(y)−h(cid:48)(y)|
ρ(x, y)

≤

|h(x)−h(y)|+|h(cid:48)(x)−h(cid:48)(y)|
ρ(x, y)

≤ 2K.

Then for every hypothesis h, h(cid:48), we have

(cid:15)t(h, h(cid:48))−(cid:15)s(h, h(cid:48)) = Eµt [|h(x)−h(cid:48)(x)|]−Eµs [|h(x)−h(cid:48)(x)|]

Eµt [f (x)]−Eµs [f (x)]

≤ sup

(cid:107)f (cid:107)L≤2K

= 2KW1(µs, µt)

(12)

(13)

Theorem 1. Under the assumption of Lemma 1, for every
h ∈ H the following holds

(cid:15)t(h) ≤ (cid:15)s(h) + 2KW1(µs, µt) + λ

(14)

where λ is the combined error of the ideal hypothesis h∗ that
minimizes the combined error (cid:15)s(h) + (cid:15)t(h).

(cid:15)t(h) ≤ (cid:15)t(h∗) + (cid:15)t(h∗, h)

= (cid:15)t(h∗) + (cid:15)s(h, h∗) + (cid:15)t(h∗, h) − (cid:15)s(h, h∗)
≤ (cid:15)t(h∗) + (cid:15)s(h, h∗) + 2KW1(µs, µt)
≤ (cid:15)t(h∗) + (cid:15)s(h) + (cid:15)s(h∗) + 2KW1(µs, µt)
= (cid:15)s(h) + 2KW1(µs, µt) + λ

(15)

Thus the generalization bound of applying Wasserstein
distance between domain distributions has been proved,
while the proof of using empirical measures on the source
and target domain samples can be further proved according
to Theorem 2.1 in (Bolley, Guillin, and Villani 2007) as the
same way in (Redko, Habrard, and Sebban 2016).

The assumption made here is to specify the hypothesis
class is K-Lipschitz continuous for some K. While it may
seem too restrictive, in fact the hypotheses are always imple-
mented by neural networks where the basic linear mapping
functions and the activation functions such as sigmoid and
relu are all Lipschitz continuous, so the assumption is not
that strong and can be fulﬁlled. And the weights in neural
networks are always regularized to avoid overﬁtting which
means the constant K will not be too large. Compared with
the proof in (Redko, Habrard, and Sebban 2016) the assump-
tions are different and can be used for different cases.

Application to Adaptation Frameworks
WDGRL can be integrated into existing feature-based do-
main adaptation frameworks (Tzeng et al. 2014; Long et al.
2015; Zhuang et al. 2015; Long et al. 2016; Bousmalis et al.
2016). These frameworks are all symmetric feature-based
and aim to learn domain invariant feature representations
for adaptation using divergence measures such as MMD
and DANN. We provide a promising alternative WDGRL
to learn domain invariant representations, which can replace
the MMD or DANN. We should point out that although WD-
GRL has gradient advantage over DANN, it takes more time
to estimate the Wasserstein distance. Although we only ap-
ply WDGRL on one hidden layer, it can also be applied on
multilayer structures as implemented in (Long et al. 2015).

Experiments
In this section, we evaluate the efﬁcacy of our approach
on sentiment and image classiﬁcation adaptation datasets.
Compared with other domain invariant representation learn-
ing approaches, WDGRL achieves better performance on
average. Furthermore, we visualize the feature representa-
tions learned by these approaches for an empirical analysis.

Datasets
Amazon review benchmark dataset. The Amazon review
dataset1 (Blitzer et al. 2007) is one of the most widely used
benchmarks for domain adaptation and sentiment analysis.
It is collected from product reviews from Amazon.com and
contains four types (domains), namely books (B), DVDs

1https://www.cs.jhu.edu/˜mdredze/datasets/sentiment/

(D), electronics (E) and kitchen appliances (K). For each
domain, there are 2,000 labeled reviews and approximately
4,000 unlabeled reviews (varying slightly across domains)
and the classes are balanced. In our experiments, for easy
computation, we follow (Chen et al. 2012) to use the 5,000
most frequent terms of unigrams and bigrams as the input
and totally A2

4 = 12 adaptation tasks are constructed.

Ofﬁce-Caltech object recognition dataset. The Ofﬁce-
Caltech dataset2 released by (Gong et al. 2012) is com-
prised of 10 common categories shared by the Ofﬁce-31 and
Caltech-256 datasets. In our experiments, we construct 12
tasks across 4 domains: Amazon (A), Webcam (W), DSLR
(D) and Caltech (C), with 958, 295, 157 and 1,123 im-
age samples respectively. In our experiments, Decaf features
are used as the input. Decaf features (Donahue et al. 2014)
are the 4096-dimensional FC7-layer hidden activations ex-
tracted by the deep convolutional neural network AlexNet.

Compared Approaches
We mainly compare our proposed approach with domain
adversarial neural network (DANN) (Ganin et al. 2016),
maximum mean discrepancy metric (MMD) (Gretton et al.
2012) and deep correlation alignment (CORAL) (Sun and
Saenko 2016) since these approaches and our proposed WD-
GRL all aim at learning the domain invariant feature repre-
sentations, which are crucial to reduce the domain discrep-
ancy. Other domain adaptation frameworks (Bousmalis et al.
2016; Tzeng et al. 2014; Long et al. 2015; Long et al. 2016;
Zhuang et al. 2015) are not included in the comparison, be-
cause these frameworks focus on adaptation architecture de-
sign and all compared approaches can be easily integrated
into these frameworks.

S-only: As an empirical lower bound, we train a model
using the labeled source data only, and test it on the target
test data directly.

MMD: The MMD metric is a measurement of the di-
vergence between two probability distributions from their
samples by computing the distance of mean embeddings in
RKHS.

DANN: DANN is an adversarial representation learning
approach that a domain classiﬁer aims at distinguishing the
learned source/target features while the feature extractor
tries to confuse the domain classiﬁer. The minimax opti-
mization is solved via a gradient reversal layer (GRL).

CORAL: Deep correlation alignment minimizes domain
discrepancy by aligning the second-order statistics of the
source and target distributions and can be applied to the layer
activations in neural networks.

Implementation Details
We implement all our experiments3 using TensorFlow and
the models are all trained with Adam optimizer. We follow
the evaluation protocol in (Long et al. 2013) and evaluate all
compared approaches through grid search on the hyperpa-
rameter space, and report the best results of each approach.
For each approach we use a batch size of 64 samples in total

2https://cs.stanford.edu/˜jhoffman/domainadapt/
3Experiment code: https://github.com/RockySJ/WDGRL.

Table 1: Performance (accuracy %) on Amazon review
dataset.

B → D
B → E
B → K
D → B
D → E
D → K
E → B
E → D
E → K
K → B
K → D
K → E
AVG

S-only MMD DANN CORAL WDGRL
81.09
75.23
77.78
76.46
76.24
79.68
73.37
73.79
86.64
72.12
75.79
85.92
77.84

82.57
80.95
83.55
79.93
82.59
84.15
75.72
77.69
87.37
75.83
78.05
86.27
81.22

82.74
82.93
84.81
80.81
83.49
85.35
76.91
78.08
87.87
76.95
79.11
86.83
82.16

82.07
78.98
82.76
79.35
81.64
83.41
75.95
77.58
86.63
75.81
78.53
86.11
80.74

83.05
83.28
85.45
80.72
83.58
86.24
77.22
78.28
88.16
77.16
79.89
86.29
82.43

with 32 samples from each domain, and a ﬁxed learning rate
10−4. All compared approaches are combined with a dis-
criminator to learn both domain invariant and discriminative
representations and to conduct the classiﬁcation task.

We use standard multi-layer perceptron (MLP) as the ba-
sic network architecture. MLP is sufﬁcient to handle all the
problems in our experiments. For Amazon review dataset
the network is designed with one hidden layer of 500 nodes,
relu activation function and softmax output function, while
the network for Ofﬁce-Caltech dataset has two hidden layers
of 500 and 100 nodes. For each dataset the same network
architecture is used for all compared approaches and these
approaches are all applied on the last hidden layer.

For the MMD experiments we follow the suggestions of
(Bousmalis et al. 2016) and use a linear combination of 19
RBF kernels with the standard deviation parameters ranging
from 10−6 to 106. As for DANN implementation, we add a
gradient reversal layer (GRL) and then a domain classiﬁer
with one hidden layer of 100 nodes. And the CORAL ap-
proach computes a distance between the second-order statis-
tics (covariances) of the source and target features and the
distance is deﬁned as the squared Frobenius norm. For each
approach, the corresponding loss term is added to the clas-
siﬁcation loss with a coefﬁcient for the trade-off. And the
coefﬁcients are tuned different to achieve the best results for
each approach.

Our approach is easy to implement according to Algo-
rithm 1. In our experiments, the domain critic network is de-
signed with a hidden layer of 100 nodes. The training steps
n is 5 which is chosen for fast computation and sufﬁcient op-
timization guarantee for the domain critic, and the learning
rate for the domain critic is 10−4. We penalize the gradients
not only at source/target representations but also at the ran-
dom points along the straight line between the source and
target pairs and the coefﬁcient γ is set to 10 as suggested in
(Gulrajani et al. 2017).

Results and Discussion

Amazon review benchmark dataset. The challenge of
cross domain sentiment analysis lies in the distribution shift
as different words are used in different domains. Table 1

Table 2: Performance (accuracy %) on Ofﬁce-Caltech
dataset with Decaf features.

S-only MMD DANN CORAL WDGRL
A → C
84.55
A → D
81.05
A → W 75.59
W → A
79.82
W → D
98.25
W → C
79.67
D → A
84.56
D → W 96.84
D → C
80.49
C → A
92.35
C → W 84.21
C → D
87.72
85.44
AVG

87.80
82.46
77.81
82.98
100
81.30
84.70
98.95
82.11
93.27
89.47
91.23
87.67

88.62
90.53
91.58
92.22
100
88.62
90.11
98.95
87.80
93.14
91.58
91.23
92.03

86.18
91.23
90.53
88.39
100
88.62
85.75
97.89
85.37
93.01
92.63
89.47
90.76

86.99
93.68
89.47
93.67
100
89.43
91.69
97.89
90.24
93.54
91.58
94.74
92.74

shows the detailed comparison results of these approaches
in 12 transfer tasks. As we can see, our proposed WDGRL
outperforms all other compared approaches in 10 out of 12
domain adaptation tasks, and it achieves the second high-
est scores in the remaining 2 tasks. We ﬁnd that as adver-
sarial adaptation approaches, WDGRL outperforms DANN,
which is consistent with our theoretical analysis that WD-
GRL has more reliable gradients. MMD and CORAL are
both non-parametric and have lower computational cost than
WDGRL, while their classiﬁcation performances are also
lower than WDGRL.

Ofﬁce-Caltech object recognition dataset. Table 2
shows the results of our experiments on Ofﬁce-Caltech
dataset. We observe that our approach achieves better per-
formance than other compared approaches on most tasks.
Ofﬁce-Caltech dataset is small since there are only hundreds
of images in one domain and it is a 10-class classiﬁcation
problem. Thus we can draw a conclusion that the empiri-
cal Wasserstein distance can also be applied to small-scale
datasets adaptation effectively. We note that CORAL per-
forms better than MMD in Amazon review dataset while it
performs worse than MMD in Ofﬁce-Caltech dataset. A pos-
sible reason is that the reasonable covariance alignment ap-
proach requires large samples. On the other hand, we can see
that these different approaches have different performances
on different adaptation tasks.

Feature Visualization

We randomly choose the D→E domain adaptation task of
Amazon review dataset and plot in Figure 2 the t-SNE visu-
alization following (Donahue et al. 2014; Long et al. 2016)
to visualize the learned feature representations. In these ﬁg-
ures, red and blue points represent positive and negative
samples of the source domain, purple and green points rep-
resent positive and negative samples of the target domain. A
transferable feature mapping should cluster red (blue) and
purple (green) points together, and meanwhile classiﬁcation
can be easily conducted between purple and green points.
We can see that almost all approaches learn discriminative
and domain invariant feature representations to some extent.
And representations learned by WDGRL are more transfer-

(a) t-SNE of DANN features

(b) t-SNE of MMD features

(c) t-SNE of CORAL features

(d) t-SNE of WDGRL features

Figure 2: Feature visualization of the D→E task in Amazon
review dataset.

able since the classes between the source and target domains
align better and the region where purple and green points
mix together is smaller.

Conclusions

In this paper, we propose a new adversarial approach WD-
GRL to learn domain invariant feature representations for
domain adaptation. WDGRL can effectively reduce the do-
main discrepancy taking advantage of the gradient property
of Wasserstein distance and the transferability is guaran-
teed by the generalization bound. Our proposed approach
could be further integrated into other domain adaptation
frameworks (Bousmalis et al. 2016; Tzeng et al. 2014;
Long et al. 2015; Long et al. 2016; Zhuang et al. 2015) to
attain better transferability. Empirical results on sentiment
and image classiﬁcation domain adaptation datasets demon-
strate that WDGRL outperforms the state-of-the-art domain
invariant feature learning approaches. From feature visual-
ization, one can easily observe that WDGRL yields domain
invariant yet target-discriminative feature representations. In
future work, we will investigate more sophisticated architec-
tures for tasks on image data as well as integrate WDGRL
into existing adaptation frameworks.

Acknowledgement

This work is ﬁnancially supported by NSFC (61702327) and
Shanghai Sailing Program (17YF1428200).

References
[Ajakan et al. 2014] Ajakan, H.; Germain, P.; Larochelle, H.;
Laviolette, F.; and Marchand, M. 2014. Domain-adversarial
neural networks. arXiv:1412.4446.
[Arjovsky, Chintala, and Bottou 2017] Arjovsky, M.; Chin-
tala, S.; and Bottou, L.
2017. Wasserstein gan.
arXiv:1701.07875.
[Ben-David et al. 2007] Ben-David, S.; Blitzer, J.; Crammer,
K.; and Pereira, F. 2007. Analysis of representations for
domain adaptation. In NIPS.
[Ben-David et al. 2010] Ben-David, S.; Blitzer, J.; Crammer,
K.; Kulesza, A.; Pereira, F.; and Vaughan, J. W. 2010. A
theory of learning from different domains. Machine learn-
ing.
[Blitzer et al. 2007] Blitzer, J.; Dredze, M.; Pereira, F.; et al.
2007. Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classiﬁcation. In ACL.
[Bolley, Guillin, and Villani 2007] Bolley, F.; Guillin, A.;
and Villani, C. 2007. Quantitative concentration inequalities
for empirical measures on non-compact spaces. Probability
Theory and Related Fields.
[Bousmalis et al. 2016] Bousmalis, K.; Trigeorgis, G.; Sil-
berman, N.; Krishnan, D.; and Erhan, D. 2016. Domain
separation networks. In NIPS.
[Chen et al. 2012] Chen, M.; Xu, Z.; Weinberger, K.; and
Sha, F. 2012. Marginalized denoising autoencoders for do-
main adaptation. arXiv:1206.4683.
[Chen, Chen, and Weinberger 2011] Chen, M.; Chen, Y.; and
Weinberger, K. Q. 2011. Automatic feature decomposition
for single view co-training. In ICML.
[Chen, Weinberger, and Blitzer 2011] Chen, M.; Wein-
berger, K. Q.; and Blitzer, J. 2011. Co-training for domain
adaptation. In NIPS.
[Chu, De la Torre, and Cohn 2013] Chu, W.-S.; De la Torre,
F.; and Cohn, J. F. 2013. Selective transfer machine for
personalized facial action unit detection. In CVPR.
[Courty et al. 2017] Courty, N.; Flamary, R.; Tuia, D.; and
Rakotomamonjy, A. 2017. Optimal transport for domain
adaptation. IEEE transactions on pattern analysis and ma-
chine intelligence.
[Courty, Flamary, and Tuia 2014] Courty, N.; Flamary, R.;
and Tuia, D. 2014. Domain adaptation with regularized
optimal transport. In ECML/PKDD.
[Donahue et al. 2014] Donahue, J.; Jia, Y.; Vinyals, O.; Hoff-
man, J.; Zhang, N.; Tzeng, E.; and Darrell, T. 2014. Decaf:
A deep convolutional activation feature for generic visual
recognition. In ICML.
[Duan, Xu, and Chang 2012] Duan, L.; Xu, D.; and Chang,
S.-F. 2012. Exploiting web images for event recognition
in consumer videos: A multiple source domain adaptation
approach. In CVPR. IEEE.
[Ganin et al. 2016] Ganin, Y.; Ustinova, E.; Ajakan, H.; Ger-
main, P.; Larochelle, H.; Laviolette, F.; Marchand, M.; and
Lempitsky, V. 2016. Domain-adversarial training of neural
networks. JMLR.

[Glorot, Bordes, and Bengio 2011] Glorot, X.; Bordes, A.;
and Bengio, Y. 2011. Domain adaptation for large-scale sen-
timent classiﬁcation: A deep learning approach. In ICML.
[Gong et al. 2012] Gong, B.; Shi, Y.; Sha, F.; and Grauman,
K. 2012. Geodesic ﬂow kernel for unsupervised domain
adaptation. In CVPR. IEEE.
[Goodfellow et al. 2014] Goodfellow, I.; Pouget-Abadie, J.;
Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair, S.; Courville,
A.; and Bengio, Y. 2014. Generative adversarial nets. In
NIPS.
[Gretton et al. 2012] Gretton, A.; Borgwardt, K. M.; Rasch,
M. J.; Sch¨olkopf, B.; and Smola, A. 2012. A kernel two-
sample test. JMLR.
[Gulrajani et al. 2017] Gulrajani, I.; Ahmed, F.; Arjovsky,
M.; Dumoulin, V.; and Courville, A. 2017. Improved train-
ing of wasserstein gans. arXiv:1704.00028.
[Hoffman et al. 2014] Hoffman, J.; Rodner, E.; Donahue, J.;
Kulis, B.; and Saenko, K. 2014. Asymmetric and cate-
gory invariant feature transformations for domain adapta-
tion. IJCV.
[Huang et al. 2007] Huang, J.; Smola, A. J.; Gretton, A.;
Borgwardt, K. M.; Sch¨olkopf, B.; et al. 2007. Correcting
sample selection bias by unlabeled data. NIPS.
[Kandemir 2015] Kandemir, M. 2015. Asymmetric transfer
learning with deep gaussian processes. In ICML.
[Long et al. 2013] Long, M.; Wang, J.; Ding, G.; Sun, J.; and
Yu, P. S. 2013. Transfer feature learning with joint distri-
bution adaptation. In The IEEE International Conference on
Computer Vision (ICCV).
[Long et al. 2015] Long, M.; Cao, Y.; Wang, J.; and Jordan,
M. 2015. Learning transferable features with deep adapta-
tion networks. In ICML.
[Long et al. 2016] Long, M.; Wang, J.; Cao, Y.; Sun, J.; and
Philip, S. Y. 2016. Deep learning of transferable represen-
tation for scalable domain adaptation. TKDE.
[Luo et al. 2017] Luo, L.; Wang, X.; Hu, S.; Wang, C.; Tang,
Y.; and Chen, L. 2017. Close yet distinctive domain adapta-
tion. arXiv:1704.04235.
[Mansour, Mohri, and Rostamizadeh 2009] Mansour,
Y.;
Mohri, M.; and Rostamizadeh, A. 2009. Domain adaptation
with multiple sources. In NIPS.
[Narayanan and Mitter 2010] Narayanan, H., and Mitter, S.
2010. Sample complexity of testing the manifold hypothe-
sis. In NIPS.
[Pan and Yang 2010] Pan, S. J., and Yang, Q. 2010. A survey
on transfer learning. IEEE Transactions on knowledge and
data engineering.
[Redko, Habrard, and Sebban 2016] Redko, I.; Habrard, A.;
and Sebban, M. 2016. Theoretical analysis of domain adap-
tation with optimal transport. arXiv:1610.04420.
[Rozantsev, Salzmann, and Fua 2016] Rozantsev, A.; Salz-
mann, M.; and Fua, P. 2016. Beyond sharing weights for
deep domain adaptation. arXiv:1603.06432.
[Sun and Saenko 2016] Sun, B., and Saenko, K. 2016. Deep

coral: Correlation alignment for deep domain adaptation. In
ECCV 2016 Workshops. Springer.
[Sun, Feng, and Saenko 2016] Sun, B.; Feng, J.; and Saenko,
K. 2016. Return of frustratingly easy domain adaptation. In
AAAI.
[Tzeng et al. 2014] Tzeng, E.; Hoffman, J.; Zhang, N.;
Saenko, K.; and Darrell, T. 2014. Deep domain confusion:
Maximizing for domain invariance. arXiv:1412.3474.
[Tzeng et al. 2017] Tzeng, E.; Hoffman, J.; Saenko, K.; and
Darrell, T. 2017. Adversarial discriminative domain adapta-
tion. arXiv:1702.05464.
[Villani 2008] Villani, C. 2008. Optimal transport: old and
new.
[Weiss, Khoshgoftaar, and Wang 2016] Weiss, K.; Khosh-
goftaar, T. M.; and Wang, D. 2016. A survey of transfer
learning. Journal of Big Data.
[Zhuang et al. 2015] Zhuang, F.; Cheng, X.; Luo, P.; Pan,
S. J.; and He, Q. 2015. Supervised representation learning:
Transfer learning with deep autoencoders. In IJCAI.

Appendix

Gradient Superiority
Here we would like to prove the gradient priority of Wasserstein distance over cross-entropy in the situation where the mapped
feature distributions ﬁll in the whole feature space. For simplicity, we take two normal distributions as an example and the
conclusion still holds in the high-dimensional space. Fig 3 shows the two normal distributions and the whole space is divided
into 3 regions where the probability of source data lying in region A is high while that of target data is extremely low. The
situation is just opposite in region C and in region B two distributions differ a little.

Figure 3: Gaussian Example

We use the same notation here as above. We assume that source data are labeled 1 while target data are labeled 0 and a
domain classiﬁer is used to help learn the domain invariant representations. So given one instance (x, y) from either domain,
the feature extractor minimizes the following objective which could be viewed as the negative of cross-entropy between the
domain label y and its corresponding prediction σ(fd(fg(x)))

LD(x, y) = y log σ(fd(fg(x))) + (1 − y) log(1 − σ(fd(fg(x))))
(16)
where σ is the sigmoid function and fd is the logit computed by the domain classiﬁer network. Then the gradient of LD with
respect to θg can be computed according to the chain rule, i.e. ∂LD
∂θg

. The ﬁrst term can be directly computed

= ∂LD
∂fd

∂fg
∂θg

∂fd
∂fg

∂LD
∂fd

= y − σ(fd(fg))

As we know, the optimal domain classiﬁer is σ(f ∗
p(h)+q(h) where h = fg(x) and p(h) represents the source feature
distribution and q(h) represents the target feature distribution. So if one source instance lies in region A, it provides gradient of
almost 0. The same result holds for target samples lying in region C. So these points make no contribution to the gradient and
thus the divergence between feature distributions couldn’t be reduced effectively.

d (h)) = p(h)

Now we consider Wasserstein distance as the loss function

LW = Ex∼Pxs [fw(fg(x))] − Ex∼P

xt [fw(fg(x))].

The gradient of LW with respect to θg can be computed according to the chain rule, i.e. ∂LW
∂θg
domain data x ∼ Pxs , ∂LW
∂fw
always provide stable gradients wherever data is.

= 1; while for target domain data x ∼ Pxt, ∂LW
∂fw

∂fw
∂fg
= −1. Therefore Wasserstein distance can

. So for source

= ∂LW
∂fw

∂fg
∂θg

Generalization Bound
We now continue from the Theorem 1 in the paper to prove that target error can be bounded by the Wasserstein distance for
empirical measures on the source and target samples. we ﬁrst present a statement showing the convergence of the empirical
measure to the true Wasserstein distance.
Theorem 2. ((Bolley, Guillin, and Villani 2007), Theorem 2.1; (Redko, Habrard, and Sebban 2016), Theorem 1) Let µ be a
probability measure in Rd satisfying T1(λ) inequality. Let ˆµ = 1
i=1 δxi be its associated empirical deﬁned on a sample of
N
independent variables {xi}N
i=1 drawn from µ. Then for any d(cid:48) > d and λ(cid:48) < λ there exists some constant N0 depending on d(cid:48)
and some square exponential moment of µ such that for any (cid:15) > 0 and N ≥ N0max(ε−(d+2), 1)

(cid:80)N

P[W1(µ, ˆµ) > ε] ≤ exp(cid:0) −

N ε2(cid:1)

λ(cid:48)
2

(17)

(18)

(19)

where d(cid:48), λ(cid:48) can be calculated explicitly.

Now we can follow the Theorem 1 and Theorem 2 to prove that target error can be bounded by the Wasserstein distance for
empirical measures on the source and target samples as the process of the proof of the Theorem 3. in (Redko, Habrard, and
Sebban 2016).
Theorem 3. Under the assumption of Lemma 1, let two probability measures satisfy T1(λ) inequality, Xs and Xt be two
samples of size Ns and Nt drawn i.i.d from µs and µt resepectively. Let ˆµs = 1
be the
Ns
associated empirical measures. Then for any d(cid:48) > d and λ(cid:48) < λ there exists some constant N0 depending on d(cid:48) such that for
any δ > 0 and min(N s, N t) ≥ N0max(δ−(d(cid:48)+2), 1) with probability at least 1 − δ for all h the followingt holds:

and ˆµt = 1
Nt

i=1 δxs

i=1 δxt

(cid:80)Ns

(cid:80)Nt

i

i

(cid:15)t(h) ≤ (cid:15)s(h) + 2KW1( ˆµs, ˆµt) + λ + 2K

2log

(20)

(cid:115)

(cid:19)

(cid:18) 1
δ

/λ(cid:48)

(cid:18)(cid:114) 1
Ns

+

(cid:19)

(cid:114) 1
Nt

where λ is the combined error of the ideal hypothesis h∗ that minimizes the combined error of (cid:15)s(h) + (cid:15)t(h).

Proof.

(cid:15)t(h) ≤ (cid:15)s(h) + 2KW1(µs, µt) + λ

≤ (cid:15)s(h) + 2KW1(µs, ˆµs) + 2KW1( ˆµs, µt) + λ
(cid:18) 1
δ

≤ (cid:15)s(h) + 2K

2log

(cid:115)

(cid:19)

≤ (cid:15)s(h) + 2KW1( ˆµs, ˆµt) + λ + 2K

2log

(cid:115)

(cid:19)

(cid:18) 1
δ

/λ(cid:48)

(cid:18)(cid:114) 1
Ns

+

(cid:19)

(cid:114) 1
Nt

/Nsλ(cid:48) + 2KW1( ˆµs, ˆµt) + 2KW1( ˆµt, µt) + λ

(21)

More Experiment Results
Synthetic data. We generate a synthetic dataset to show the superior gradient advantage of WDGRL over DANN. In the paper,
we claim that when two representation distributions are distant or have regions they differ a lot, DANN will have gradient
vanishing problem while WDGRL still provides the stable gradient. It is a little difﬁcult to fully realize such situations, so
we design a rather restrictive experiment. However, this toy experiment does verify DANN may fail in some situations while
WDGRL can work. We visualize the data input in Figure 4(a) with 2000 samples for each domain. And from Figure 4(b) we
ﬁnd that if we adopt DANN the domain classiﬁer can distinguish two domain data well and the DANN loss decreases to nearly
0 as the training process continues. In such situation, the domain classiﬁer can provide poor gradient. As shown in 4(c), our
WDGRL approach can effectively classify the target data while DANN fails.

(a) input visualization

(b) DANN loss and accuracy

(c) Performance on target domain

Figure 4: Synthetic experiment.

Ofﬁce-Caltech dataset with SURF features. Table 3 shows the result of our experiments on Ofﬁce-Caltech dataset with

SURF features.

Email spam ﬁltering dataset. The email spam ﬁltering dataset 4 released by ECML/PKDD 2006 discovery challenge con-
tains 4 separate user inboxes. From public inbox (source domain) 4,000 labeled training samples were collected, among which

4http://www.ecmlpkdd2006.org/challenge.html

Table 3: Performance (accuracy %) on Ofﬁce-Caltech dataset with Decaf features

S-only MMD DANN D-CORAL WDGRL
A → C
43.19
A → D
35.03
A → W 35.23
W → A
30.06
W → D
80.25
W → C
30.19
C → W 36.95
C → A
52.92
C → D
45.86
D → W 69.50
D → A
31.21
D → C
30.37
43.4
AVG

44.08
41.40
37.29
34.13
84.71
30.72
40.34
54.80
47.13
73.56
32.46
30.72
45.95

45.86
44.59
40.68
32.15
81.53
31.08
42.37
55.22
48.41
76.95
35.60
32.59
47.25

44.97
41.40
38.64
34.13
82.80
32.68
43.39
54.91
47.77
74.24
31.63
32.24
46.57

44.97
40.13
38.31
34.86
84.08
33.30
40.00
53.44
47.13
73.90
31.52
31.52
46.10

half samples are spam emails and the other half non-spam ones. The test samples were collected from 3 private inboxes (target
domains), each of which consists of 2,500 samples. In our experiments, 3 cross-domain tasks are constructed from the public
inbox to the private inboxes. We choose the 5,067 most frequent terms as features and 4 test samples were deleted as a result of
not containing any of these terms. Experimenting on the 3 tasks by transferring from public to private groups of private inboxes
u1 ∼ u3, we found our method does achieve better performance than MMD, DANN and D-CORAL, which is demonstrated in
Table 4. We can see from this result that all these approaches can reach the goal of learning the transferable features for they all
outperform the source only baseline at least 9%. Among them, MMD and DANN achieve almost the same performance while
WDGRL further boosts the performance by a rate of 2.90%.

Table 4: Performance (Accuracy %) on email spam dataset
S only MMD DANN D-CORAL WDGRL
69.63
76.01
81.24
75.63

P → u1
P → u2
P → u3
AVG

83.27
85.74
91.92
86.98

79.71
83.83
89.80
84.45

80.95
85.98
94.08
87.00

85.67
88.26
95.76
89.90

Newsgroup classiﬁcation dataset. The 20 newsgroups dataset 5 is a collection of 18,774 newsgroup documents across 6
top categories and 20 subcategories in a hierarchical structure. In our experiments, we adopt a similar setting as (?). The task
is to classify top categories and the four largest top categories (comp, rec, sci, talk) are chosen for evaluation. Speciﬁcally, for
each top category, the largest subcategory is selected as the source domain while the second largest subcategory is chosen as
the target domain. Moreover, the largest category comp is considered as the positive class and one of the three other categories
as the negative class.

The distribution shift across newsgroups is caused by category speciﬁc words. Notice the construction of our domain adap-
tation tasks which aim to classify the top categories while the adaptation exists between the subcategories. It makes sense that
there exist more differences among top categories than those among subcategories which implies that classiﬁcation is not that
sensitive to the subcategories and thus enables the ease of domain adaptation. Table 5 gives the information of performance on
the 20newsgroup dataset from which we can ﬁnd that the comparison methods are almost neck and neck, which is consistent
with our previous observation.

Table 5: Performance (Accuracy %) on 20 newsgroup dataset

C vs. R
C vs. S
C vs. T
AVG

S only MMD DANN D-CORAL WDGRL
81.62
74.01
94.44
83.36

97.85
87.52
96.96
94.11

98.10
90.57
97.75
95.47

97.57
84.20
97.22
93.00

98.35
91.33
97.62
95.77

5http://qwone.com/˜jason/20Newsgroups/

Wasserstein Distance Guided Representation Learning
for Domain Adaptation

Jian Shen, Yanru Qu, Weinan Zhang∗, Yong Yu
Shanghai Jiao Tong University
{rockyshen, kevinqu, wnzhang, yyu}@apex.sjtu.edu.cn

8
1
0
2
 
r
a

M
 
9
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
7
1
2
1
0
.
7
0
7
1
:
v
i
X
r
a

Abstract

Domain adaptation aims at generalizing a high-performance
learner on a target domain via utilizing the knowledge dis-
tilled from a source domain which has a different but re-
lated data distribution. One solution to domain adaptation
is to learn domain invariant feature representations while
the learned representations should also be discriminative in
prediction. To learn such representations, domain adaptation
frameworks usually include a domain invariant representation
learning approach to measure and reduce the domain discrep-
ancy, as well as a discriminator for classiﬁcation. Inspired by
Wasserstein GAN, in this paper we propose a novel approach
to learn domain invariant feature representations, namely
Wasserstein Distance Guided Representation Learning (WD-
GRL). WDGRL utilizes a neural network, denoted by the
domain critic, to estimate empirical Wasserstein distance be-
tween the source and target samples and optimizes the feature
extractor network to minimize the estimated Wasserstein dis-
tance in an adversarial manner. The theoretical advantages of
Wasserstein distance for domain adaptation lie in its gradi-
ent property and promising generalization bound. Empirical
studies on common sentiment and image classiﬁcation adap-
tation datasets demonstrate that our proposed WDGRL out-
performs the state-of-the-art domain invariant representation
learning approaches.

Introduction
Domain adaptation deﬁnes the problem when the target do-
main labeled data is insufﬁcient, while the source domain
has much more labeled data. Even though the source and
target domains have different marginal distributions (Ben-
David et al. 2007; Pan and Yang 2010), domain adaptation
aims at utilizing the knowledge distilled from the source
domain to help target domain learning. In practice, unsu-
pervised domain adaptation is concerned and studied more
commonly since manual annotation is often expensive or
time-consuming. Faced with the covariate shift and the lack
of annotations, conventional machine learning methods may
fail to learn a high-performance model.

To effectively transfer a classiﬁer across different do-
mains, different methods have been proposed, including
instance reweighting (Mansour, Mohri, and Rostamizadeh

∗Weinan Zhang is the corresponding author.

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

2009), subsampling (Chen, Chen, and Weinberger 2011),
feature mapping (Tzeng et al. 2014) and weight regular-
ization (Rozantsev, Salzmann, and Fua 2016). Among these
methods feature mapping has shown great success recently,
which projects the data from different domains to a com-
mon latent space where the feature representations are do-
main invariant. Recently, deep neural networks, as a great
tool to automatically learn effective data representations,
have been leveraged in learning knowledge-transferable fea-
ture representations for domain adaptation (Glorot, Bordes,
and Bengio 2011; Chen et al. 2012; Zhuang et al. 2015;
Long et al. 2015; Ganin et al. 2016).

On the other hand, generative adversarial nets (GANs)
(Goodfellow et al. 2014) are heavily studied during recent
years, which play a minimax game between two adversar-
ial networks: the discriminator is trained to distinguish real
data from the generated data, while the generator learns to
generate high-quality data to fool the discriminator. It is in-
tuitive to employ this minimax game for domain adaptation
to make the source and target feature representations in-
distinguishable. These adversarial adaptation methods have
become a popular solution to reduce domain discrepancy
through an adversarial objective with respect to a domain
classiﬁer (Ganin et al. 2016; Tzeng et al. 2017). However,
when the domain classiﬁer network can perfectly distinguish
target representations from source ones, there will be a gra-
dient vanishing problem. A more reasonable solution would
be to replace the domain discrepancy measure with Wasser-
stein distance, which provides more stable gradients even if
two distributions are distant (Arjovsky, Chintala, and Bottou
2017).

In this paper, we propose a domain invariant representa-
tion learning approach to reduce domain discrepancy for do-
main adaptation, namely Wasserstein Distance Guided Rep-
resentation Learning (WDGRL), inspired by recently pro-
posed Wasserstein GAN (Arjovsky, Chintala, and Bottou
2017). WDGRL trains a domain critic network to estimate
the empirical Wasserstein distance between the source and
target feature representations. The feature extractor network
will then be optimized to minimize the estimated Wasser-
stein distance in an adversarial manner. By iterative adver-
sarial training, we ﬁnally learn feature representations in-
variant to the covariate shift between domains. Additionally,
WDGRL can be easily adopted in existing domain adap-

tation frameworks (Tzeng et al. 2014; Long et al. 2015;
Zhuang et al. 2015; Long et al. 2016; Bousmalis et al. 2016)
by replacing the representation learning approaches. Em-
pirical studies on common domain adaptation benchmarks
demonstrate that WDGRL outperforms the state-of-the-art
representation learning approaches for domain adaptation.
Furthermore, the visualization of learned representations
clearly shows that WDGRL successfully uniﬁes two domain
distributions, as well as maintains obvious label discrimina-
tion.

Related Works
Domain adaptation is a popular subject in transfer learn-
ing (Pan and Yang 2010). It concerns covariate shift be-
tween two data distributions, usually labeled source data
and unlabeled target data. Solutions to domain adapta-
tion problems can be mainly categorized into three types:
i). Instance-based methods, which reweight/subsample the
source samples to match the distribution of the target do-
main, thus training on the reweighted source samples guar-
antees classiﬁers with transferability (Huang et al. 2007;
Chen, Weinberger, and Blitzer 2011; Chu, De la Torre, and
Cohn 2013). ii). Parameter-based methods, which trans-
fer knowledge through shared or regularized parameters of
source and target domain learners, or by combining mul-
tiple reweighted source learners to form an improved tar-
get learner (Duan, Xu, and Chang 2012; Rozantsev, Salz-
mann, and Fua 2016). iii). The last but the most popu-
lar and effective methods are feature-based, which can be
further categorized into two groups (Weiss, Khoshgoftaar,
and Wang 2016). Asymmetric feature-based methods trans-
form the features of one domain to more closely match
another domain (Hoffman et al. 2014; Kandemir 2015;
Courty et al. 2017) while symmetric feature-based methods
map different domains to a common latent space where the
feature distributions are close.

Recently, deep learning has been regarded as a powerful
way to learn feature representations for domain adaptation.
Symmetric feature-based methods are more widely studied
since it can be easily incorporated into deep neural networks
(Chen et al. 2012; Zhuang et al. 2015; Long et al. 2015;
Ganin et al. 2016; Bousmalis et al. 2016; Luo et al. 2017).
Among symmetric feature-based methods, minimizing the
maximum mean discrepancy (MMD) (Gretton et al. 2012)
metric is effective to minimize the divergence of two dis-
tributions. MMD is a nonparametric metric that measures
the distribution divergence between the mean embeddings
of two distributions in reproducing kernel Hilbert space
(RKHS). The deep domain confusion (DDC) method (Tzeng
et al. 2014) utilized MMD metric in the last fully connected
layer in addition to the regular classiﬁcation loss to learn rep-
resentations that are both domain invariant and discrimina-
tive. Deep adaptation network (DAN) (Long et al. 2015) was
proposed to enhance the feature transferability by minimiz-
ing multi-kernel MMD in several task-speciﬁc layers. On the
other hand, correlation alignment (CORAL) method (Sun,
Feng, and Saenko 2016) was proposed to align the second-
order statistics of the source and target distributions with a
linear transformation and (Sun and Saenko 2016) extended

CORAL and proposed Deep CORAL to learn a nonlinear
transformation that aligns correlations of layer activations in
deep neural networks.

Another class of symmetric feature-based methods uses
an adversarial objective to reduce domain discrepancy. Mo-
tivated by theory in (Ben-David et al. 2007; Ben-David et
al. 2010) suggesting that a good cross-domain representa-
tion contains no discriminative information about the origin
(i.e. domain) of the input, domain adversarial neural network
(DANN) (Ajakan et al. 2014; Ganin et al. 2016) was pro-
posed to learn domain invariant features by a minimax game
between the domain classiﬁer and the feature extractor. In
order to back-propagate the gradients computed from the
domain classiﬁer, DANN employs a gradient reversal layer
(GRL). On the other hand, (Tzeng et al. 2017) proposed a
general framework for adversarial adaptation by choosing
adversarial loss type with respect to the domain classiﬁer
and the weight sharing strategy. Our proposed WDGRL can
also be viewed as an adversarial adaptation method since it
evaluates and minimizes the empirical Wasserstein distance
in an adversarial manner. Our WDGRL differs from previ-
ous adversarial methods: i). WDGRL adopts an iterative ad-
versarial training strategy, ii). WDGRL adopts Wasserstein
distance as the adversarial loss which has gradient superior-
ity.

Another related work for domain adaptation is optimal
transport (Courty, Flamary, and Tuia 2014; Courty et al.
2017), which is equivalent to Wasserstein distance. And
(Redko, Habrard, and Sebban 2016) gave a theoretical anal-
ysis that Wasserstein distance can guarantee generalization
for domain adaptation. Though these works utilized Wasser-
stein distance in domain adaptation, there are distinct differ-
ences between WDGRL and the previous ones: these works
are asymmetric feature-based methods which design a trans-
formation from source representations to target ones based
on optimal transport while WDGRL is a symmetric method
that projects both domains to a common latent space to learn
domain invariant features. And WDGRL can be integrated
into other symmetric feature-based adaptation frameworks.
Besides learning shared representations, domain separa-
tion network (DSN) (Bousmalis et al. 2016) was proposed
to explicitly separate private representations for each domain
and shared ones between the source and target domains. The
private representations were learned by deﬁning a difference
loss via a soft orthogonality constraint between the shared
and private representations while the shared representations
were learned by DANN or MMD mentioned above. With the
help of reconstruction through private and shared represen-
tations together, the classiﬁer trained on the shared represen-
tations can better generalize across domains. Since our work
focuses on learning the shared representations, it can also be
integrated into DSN easily.

Wasserstein Metric
Before we introduce our domain invariant feature represen-
tation learning approach, we ﬁrst give a brief introduction of
the Wasserstein metric. The Wasserstein metric is a distance
measure between probability distributions on a given metric
space (M, ρ), where ρ(x, y) is a distance function for two

instances x and y in the set M . The p-th Wasserstein dis-
tance between two Borel probability measures P and Q is
deﬁned as

Wp(P, Q) =

(cid:90)

(cid:16)

inf
µ∈Γ(P,Q)

ρ(x, y)pdµ(x, y)

(cid:17)1/p

,

(1)

where P, Q ∈ {P : (cid:82) ρ(x, y)pdP(x) < ∞, ∀y ∈ M } are
two probability measures on M with ﬁnite p-th moment and
Γ(P, Q) is the set of all measures on M × M with marginals
P and Q. Wasserstein metric arises in the problem of optimal
transport: µ(x, y) can be viewed as a randomized policy for
transporting a unit quantity of some material from a random
location x to another location y while satisfying the marginal
constraint x ∼ P and y ∼ Q. If the cost of transporting a unit
of material from x ∈ P to y ∈ Q is given by ρ(x, y)p, then
Wp(P, Q) is the minimum expected transport cost.

The Kantorovich-Rubinstein theorem shows that when M
is separable, the dual representation of the ﬁrst Wasserstein
distance (Earth-Mover distance) can be written as a form of
integral probability metric (Villani 2008)

W1(P, Q) = sup

Ex∼P[f (x)] − Ex∼Q[f (x)],

(2)

(cid:107)f (cid:107)L≤1

where the Lipschitz semi-norm is deﬁned as (cid:107)f (cid:107)L =
sup |f (x) − f (y)|/ρ(x, y). In this paper, for simplicity,
Wasserstein distance represents the ﬁrst Wasserstein dis-
tance.

Wasserstein Distance Guided
Reprensentation Learning

i , ys

i )}ns

Problem Deﬁnition
In unsupervised domain adaptation problem, we have a la-
i=1 of ns samples
beled source dataset X s = {(xs
from the source domain Ds which is assumed sufﬁcient to
train an accurate classiﬁer, and an unlabeled target dataset
X t = {xt
j=1 of nt samples from the target domain Dt.
It is assumed that the two domains share the same feature
space but follow different marginal data distributions, Pxs
and Pxt respectively. The goal is to learn a transferable clas-
siﬁer η(x) to minimize target risk (cid:15)t = Pr(x,y)∼Dt[η(x) (cid:54)=
y] using all the given data.

j}nt

Domain Invariant Representation Learning
The challenge of unsupervised domain adaptation mainly
lies in the fact that two domains have different data distribu-
tions. Thus the model trained with source domain data may
be highly biased in the target domain. To solve this problem,
we propose a new approach to learn feature representations
invariant to the change of domains by minimizing empirical
Wasserstein distance between the source and target repre-
sentations through adversarial training.

In our adversarial representation learning approach, there
is a feature extractor which can be implemented by a neural
network. The feature extractor is supposed to learn the do-
main invariant feature representations from both domains.
Given an instance x ∈ Rm from either domain, the feature
extractor learns a function fg : Rm → Rd that maps the

instance to a d-dimensional representation with correspond-
ing network parameter θg. And then in order to reduce the
discrepancy between the source and target domains, we use
the domain critic, as suggested in (Arjovsky, Chintala, and
Bottou 2017), whose goal is to estimate the Wasserstein dis-
tance between the source and target representation distribu-
tions. Given a feature representation h = fg(x) computed
by the feature extractor, the domain critic learns a function
fw : Rd → R that maps the feature representation to a real
number with parameter θw. Then the Wasserstein distance
between two representation distributions Phs and Pht, where
hs = fg(xs) and ht = fg(xt), can be computed according
to Eq. (2)
W1(Phs , Pht ) = sup

hs [fw(h)] − EP

ht [fw(h)]

EP

(cid:107)fw (cid:107)L≤1

= sup

(cid:107)fw (cid:107)L≤1

EP

xs [fw(fg(x))] − EP

xt [fw(fg(x))].

(3)
If the parameterized family of domain critic functions {fw}
are all 1-Lipschitz, then we can approximate the empirical
Wasserstein distance by maximizing the domain critic loss
Lwd with respect to parameter θw
1
ns

Lwd(xs, xt) =

fw(fg(xs))−

fw(fg(xt)).

1
nt

(cid:88)

(cid:88)

xs∈X s

xt∈X t

(4)
Here comes the question of enforcing the Lipschitz con-
straint. (Arjovsky, Chintala, and Bottou 2017) proposed to
clip the weights of domain critic within a compact space
[−c, c] after each gradient update. However (Gulrajani et al.
2017) pointed out that weight clipping will cause capacity
underuse and gradient vanishing or exploding problems. As
suggested in (Gulrajani et al. 2017), a more reasonable way
is to enforce gradient penalty Lgrad for the domain critic
parameter θw

Lgrad(ˆh) = ((cid:107)∇ˆhfw(ˆh)(cid:107)2 − 1)2,
(5)
where the feature representations ˆh at which to penalize the
gradients are deﬁned not only at the source and target rep-
resentations but also at the random points along the straight
line between source and target representation pairs. So we
can ﬁnally estimate the empirical Wasserstein distance by
solving the problem

max
θw

{Lwd − γLgrad}

(6)

where γ is the balancing coefﬁcient.

Since the Wasserstein distance is continuous and differ-
entiable almost everywhere, we can ﬁrst train the domain
critic to optimality. Then by ﬁxing the optimal parameter of
domain critic and minimizing the estimator of Wasserstein
distance, the feature extractor network can learn feature rep-
resentations with domain discrepancy reduced. Up to now
the representation learning can be achieved by solving the
minimax problem

min
θg

max
θw

{Lwd − γLgrad}

(7)

where γ should be set 0 when optimizing the minimum op-
eration since the gradient penalty should not guide the rep-
resentation learning process. By iteratively learning feature

Algorithm 1 Wasserstein Distance Guided Representation
Learning Combining with Discriminator
Require: source data X s; target data X t; minibatch size m; critic
training step n; coefﬁcient γ, λ; learning rate for domain critic
α1; learning rate for classiﬁcation and feature learning α2
1: Initialize feature extractor, domain critic, discriminator with

random weights θg, θw, θc

2: repeat
3:
4:
5:
6:

Sample minibatch {xs
for t = 1, ..., n do

i , ys

i }m

i=1, {xt

i}m

i=1 from X s and X t

hs ← fg(xs), ht ← fg(xt)
Sample h as the random points along straight lines be-
tween hs and ht pairs
ˆh ← {hs, ht, h}
θw ← θw + α1∇θw [Lwd(xs, xt) − γLgrad(ˆh)]

7:
8:
9:
10:
11:
12: until θg, θw, θc converge

end for
θc ← θc − α2∇θc Lc(xs, ys)
θg ← θg − α2∇θg [Lc(xs, ys) + Lwd(xs, xt)]

Note that this algorithm can be trained by the standard
back-propagation with two iterative steps. In a mini-batch
containing labeled source data and unlabeled target data, we
ﬁrst train the domain critic network to optimality by optimiz-
ing the max operator via gradient ascent and then update the
feature extractor by minimizing the classiﬁcation loss com-
puted by labeled source data and the estimated Wasserstein
distance simultaneously. The learned representations can be
domain invariant and target discriminative since the parame-
ter θg receives the gradients from both the domain critic and
the discriminator loss.

Theoretical Analysis
In this section, we give some theoretical analysis about the
advantages of using Wasserstein distance for domain adap-
tation.

Gradient Superiority
In domain adaptation, to minimize
the divergence between the data distributions Pxs and Pxt,
the symmetric feature-based methods learn a transformation
function to map the data from the original space to a com-
mon latent space with a distance measure. There are two sit-
uations after the mapping: i). The two mapped feature dis-
tributions have supports that lie on low dimensional man-
ifolds (Narayanan and Mitter 2010) in the latent space. In
such situation, there will be a gradient vanishing problem if
adopting the domain classiﬁer to make data indistinguish-
able while Wasserstein distance could provide reliable gra-
dients (Arjovsky, Chintala, and Bottou 2017). ii). The fea-
ture representations may ﬁll in the whole space since the
feature mapping usually reduces dimensionality. However,
if a data point lies in the regions where the probability of
one distribution could be ignored compared with the other
distribution, it makes no contributions to the gradients with
traditional cross-entropy loss since the gradient computed by
this data point is almost 0. If we adopt Wasserstein distance
as the distance measure, stable gradients can be provided
wherever. So theoretically in either situation, WDGRL can
perform better than previous adversarial adaptation methods

Figure 1: WDGRL Combining with Discriminator.

representations with lower Wasserstein distance, the adver-
sarial objective can ﬁnally learn domain invariant feature
representations.

Combining with Discriminator
As mentioned above, our ﬁnal goal is to learn a high-
performance classiﬁer for the target domain. However, the
process of WDGRL is in an unsupervised setting, which
may result in that the learned domain invariant representa-
tions are not discriminative enough. Hence it is necessary
to incorporate the supervision signals of source domain data
into the representation learning process as in DANN (Ganin
et al. 2016). Next we further introduce the combination of
the representation learning approaches and a discriminator,
of which the overview framework is given by Figure 1. A de-
tailed algorithm of the combination is given in Algorithm 1.
We further add several layers as the discriminator after the
feature extractor network. Since WDGRL guarantees trans-
ferability of the learned representations, the shared discrim-
inator can be directly applied to target domain prediction
when training ﬁnished. The objective of the discriminator
fc : Rd → Rl is to compute the softmax prediction with
parameter θc where l is the number of classes. The discrim-
inator loss function is deﬁned as the cross-entropy between
the predicted probabilistic distribution and the one-hot en-
coding of the class labels given the labeled source data:

1
ns

ns
(cid:88)

l
(cid:88)

i=1

k=1

Lc(xs, ys) = −

1(ys

i = k) · log fc(fg(xs

i ))k,

i = k) is the indicator function and fc(fg(xs

(8)
where 1(ys
i ))k
corresponds to the k-th dimension value of the distribution
fc(fg(xs
i )). By combining the discriminator loss, we attain
our ﬁnal objective function

(cid:110)

min
θg,θc

Lc + λ max
θw

(cid:104)
Lwd − γLgrad

(cid:105)(cid:111)
,

(9)

where λ is the coefﬁcient that controls the balance between
discriminative and transferable feature learning and γ should
be set 0 when optimizing the minimum operator.

(Ganin et al. 2016; Tzeng et al. 2017).

Proof.

Generalization Bound (Redko, Habrard, and Sebban
2016) proved that the target error can be bounded by the
Wasserstein distance for empirical measures. However, the
generalization bound exists when assuming the hypothesis
class is a unit ball in RKHS and the transport cost function
is RKHS distance. In this paper we prove the generalization
bound in terms of the Kantorovich-Rubinstein dual formu-
lation under a different assumption.

We ﬁrst formalize some notations that will be used in the
following statements. Let X be an instance set and {0, 1}
be the label set for binary classiﬁcation. We denote by µs
the distribution of source instances on X and use µt for the
target domain. We denote that two domains have the same
labeling function f : X → [0, 1] which is always assumed
to hold in domain adaptation problem. A hypothesis class H
is a set of predictor functions, ∀h ∈ H, h : X → [0, 1]. The
probability according to the distribution µs that a hypothesis
h disagrees with the labeling function f (which can also be
a hypothesis) is deﬁned as (cid:15)s(h, f ) = Ex∈µs[|h(x) − f (x)|].
We use the shorthand (cid:15)s(h) = (cid:15)s(h, f ) and (cid:15)t(h) is de-
ﬁned the same. We now present the Lemma that introduces
Wasserstein distance to relate the source and target errors.

Lemma 1. Let µs, µt ∈ P(X ) be two probability measures.
Assume the hypotheses h ∈ H are all K-Lipschitz continu-
ous for some K. Then the following holds

(cid:15)t(h, h(cid:48)) ≤ (cid:15)s(h, h(cid:48)) + 2KW1(µs, µt)

(10)

for every hypothesis h, h(cid:48) ∈ H.

Proof. We ﬁrst prove that for every K-Lipschitz continuous
hypotheses h, h(cid:48) ∈ H, |h − h(cid:48)| is 2K-Lipschitz continuous.
Using the triangle inequality, we have

|h(x)−h(cid:48)(x)| ≤ |h(x)−h(y)|+|h(y)−h(cid:48)(x)|

≤ |h(x)−h(y)|+|h(y)−h(cid:48)(y)|+|h(cid:48)(x)−h(cid:48)(y)|
(11)

and thus for every x, y ∈ X ,

|h(x)−h(cid:48)(x)|−|h(y)−h(cid:48)(y)|
ρ(x, y)

≤

|h(x)−h(y)|+|h(cid:48)(x)−h(cid:48)(y)|
ρ(x, y)

≤ 2K.

Then for every hypothesis h, h(cid:48), we have

(cid:15)t(h, h(cid:48))−(cid:15)s(h, h(cid:48)) = Eµt [|h(x)−h(cid:48)(x)|]−Eµs [|h(x)−h(cid:48)(x)|]

Eµt [f (x)]−Eµs [f (x)]

≤ sup

(cid:107)f (cid:107)L≤2K

= 2KW1(µs, µt)

(12)

(13)

Theorem 1. Under the assumption of Lemma 1, for every
h ∈ H the following holds

(cid:15)t(h) ≤ (cid:15)s(h) + 2KW1(µs, µt) + λ

(14)

where λ is the combined error of the ideal hypothesis h∗ that
minimizes the combined error (cid:15)s(h) + (cid:15)t(h).

(cid:15)t(h) ≤ (cid:15)t(h∗) + (cid:15)t(h∗, h)

= (cid:15)t(h∗) + (cid:15)s(h, h∗) + (cid:15)t(h∗, h) − (cid:15)s(h, h∗)
≤ (cid:15)t(h∗) + (cid:15)s(h, h∗) + 2KW1(µs, µt)
≤ (cid:15)t(h∗) + (cid:15)s(h) + (cid:15)s(h∗) + 2KW1(µs, µt)
= (cid:15)s(h) + 2KW1(µs, µt) + λ

(15)

Thus the generalization bound of applying Wasserstein
distance between domain distributions has been proved,
while the proof of using empirical measures on the source
and target domain samples can be further proved according
to Theorem 2.1 in (Bolley, Guillin, and Villani 2007) as the
same way in (Redko, Habrard, and Sebban 2016).

The assumption made here is to specify the hypothesis
class is K-Lipschitz continuous for some K. While it may
seem too restrictive, in fact the hypotheses are always imple-
mented by neural networks where the basic linear mapping
functions and the activation functions such as sigmoid and
relu are all Lipschitz continuous, so the assumption is not
that strong and can be fulﬁlled. And the weights in neural
networks are always regularized to avoid overﬁtting which
means the constant K will not be too large. Compared with
the proof in (Redko, Habrard, and Sebban 2016) the assump-
tions are different and can be used for different cases.

Application to Adaptation Frameworks
WDGRL can be integrated into existing feature-based do-
main adaptation frameworks (Tzeng et al. 2014; Long et al.
2015; Zhuang et al. 2015; Long et al. 2016; Bousmalis et al.
2016). These frameworks are all symmetric feature-based
and aim to learn domain invariant feature representations
for adaptation using divergence measures such as MMD
and DANN. We provide a promising alternative WDGRL
to learn domain invariant representations, which can replace
the MMD or DANN. We should point out that although WD-
GRL has gradient advantage over DANN, it takes more time
to estimate the Wasserstein distance. Although we only ap-
ply WDGRL on one hidden layer, it can also be applied on
multilayer structures as implemented in (Long et al. 2015).

Experiments
In this section, we evaluate the efﬁcacy of our approach
on sentiment and image classiﬁcation adaptation datasets.
Compared with other domain invariant representation learn-
ing approaches, WDGRL achieves better performance on
average. Furthermore, we visualize the feature representa-
tions learned by these approaches for an empirical analysis.

Datasets
Amazon review benchmark dataset. The Amazon review
dataset1 (Blitzer et al. 2007) is one of the most widely used
benchmarks for domain adaptation and sentiment analysis.
It is collected from product reviews from Amazon.com and
contains four types (domains), namely books (B), DVDs

1https://www.cs.jhu.edu/˜mdredze/datasets/sentiment/

(D), electronics (E) and kitchen appliances (K). For each
domain, there are 2,000 labeled reviews and approximately
4,000 unlabeled reviews (varying slightly across domains)
and the classes are balanced. In our experiments, for easy
computation, we follow (Chen et al. 2012) to use the 5,000
most frequent terms of unigrams and bigrams as the input
and totally A2

4 = 12 adaptation tasks are constructed.

Ofﬁce-Caltech object recognition dataset. The Ofﬁce-
Caltech dataset2 released by (Gong et al. 2012) is com-
prised of 10 common categories shared by the Ofﬁce-31 and
Caltech-256 datasets. In our experiments, we construct 12
tasks across 4 domains: Amazon (A), Webcam (W), DSLR
(D) and Caltech (C), with 958, 295, 157 and 1,123 im-
age samples respectively. In our experiments, Decaf features
are used as the input. Decaf features (Donahue et al. 2014)
are the 4096-dimensional FC7-layer hidden activations ex-
tracted by the deep convolutional neural network AlexNet.

Compared Approaches
We mainly compare our proposed approach with domain
adversarial neural network (DANN) (Ganin et al. 2016),
maximum mean discrepancy metric (MMD) (Gretton et al.
2012) and deep correlation alignment (CORAL) (Sun and
Saenko 2016) since these approaches and our proposed WD-
GRL all aim at learning the domain invariant feature repre-
sentations, which are crucial to reduce the domain discrep-
ancy. Other domain adaptation frameworks (Bousmalis et al.
2016; Tzeng et al. 2014; Long et al. 2015; Long et al. 2016;
Zhuang et al. 2015) are not included in the comparison, be-
cause these frameworks focus on adaptation architecture de-
sign and all compared approaches can be easily integrated
into these frameworks.

S-only: As an empirical lower bound, we train a model
using the labeled source data only, and test it on the target
test data directly.

MMD: The MMD metric is a measurement of the di-
vergence between two probability distributions from their
samples by computing the distance of mean embeddings in
RKHS.

DANN: DANN is an adversarial representation learning
approach that a domain classiﬁer aims at distinguishing the
learned source/target features while the feature extractor
tries to confuse the domain classiﬁer. The minimax opti-
mization is solved via a gradient reversal layer (GRL).

CORAL: Deep correlation alignment minimizes domain
discrepancy by aligning the second-order statistics of the
source and target distributions and can be applied to the layer
activations in neural networks.

Implementation Details
We implement all our experiments3 using TensorFlow and
the models are all trained with Adam optimizer. We follow
the evaluation protocol in (Long et al. 2013) and evaluate all
compared approaches through grid search on the hyperpa-
rameter space, and report the best results of each approach.
For each approach we use a batch size of 64 samples in total

2https://cs.stanford.edu/˜jhoffman/domainadapt/
3Experiment code: https://github.com/RockySJ/WDGRL.

Table 1: Performance (accuracy %) on Amazon review
dataset.

B → D
B → E
B → K
D → B
D → E
D → K
E → B
E → D
E → K
K → B
K → D
K → E
AVG

S-only MMD DANN CORAL WDGRL
81.09
75.23
77.78
76.46
76.24
79.68
73.37
73.79
86.64
72.12
75.79
85.92
77.84

82.57
80.95
83.55
79.93
82.59
84.15
75.72
77.69
87.37
75.83
78.05
86.27
81.22

82.74
82.93
84.81
80.81
83.49
85.35
76.91
78.08
87.87
76.95
79.11
86.83
82.16

82.07
78.98
82.76
79.35
81.64
83.41
75.95
77.58
86.63
75.81
78.53
86.11
80.74

83.05
83.28
85.45
80.72
83.58
86.24
77.22
78.28
88.16
77.16
79.89
86.29
82.43

with 32 samples from each domain, and a ﬁxed learning rate
10−4. All compared approaches are combined with a dis-
criminator to learn both domain invariant and discriminative
representations and to conduct the classiﬁcation task.

We use standard multi-layer perceptron (MLP) as the ba-
sic network architecture. MLP is sufﬁcient to handle all the
problems in our experiments. For Amazon review dataset
the network is designed with one hidden layer of 500 nodes,
relu activation function and softmax output function, while
the network for Ofﬁce-Caltech dataset has two hidden layers
of 500 and 100 nodes. For each dataset the same network
architecture is used for all compared approaches and these
approaches are all applied on the last hidden layer.

For the MMD experiments we follow the suggestions of
(Bousmalis et al. 2016) and use a linear combination of 19
RBF kernels with the standard deviation parameters ranging
from 10−6 to 106. As for DANN implementation, we add a
gradient reversal layer (GRL) and then a domain classiﬁer
with one hidden layer of 100 nodes. And the CORAL ap-
proach computes a distance between the second-order statis-
tics (covariances) of the source and target features and the
distance is deﬁned as the squared Frobenius norm. For each
approach, the corresponding loss term is added to the clas-
siﬁcation loss with a coefﬁcient for the trade-off. And the
coefﬁcients are tuned different to achieve the best results for
each approach.

Our approach is easy to implement according to Algo-
rithm 1. In our experiments, the domain critic network is de-
signed with a hidden layer of 100 nodes. The training steps
n is 5 which is chosen for fast computation and sufﬁcient op-
timization guarantee for the domain critic, and the learning
rate for the domain critic is 10−4. We penalize the gradients
not only at source/target representations but also at the ran-
dom points along the straight line between the source and
target pairs and the coefﬁcient γ is set to 10 as suggested in
(Gulrajani et al. 2017).

Results and Discussion

Amazon review benchmark dataset. The challenge of
cross domain sentiment analysis lies in the distribution shift
as different words are used in different domains. Table 1

Table 2: Performance (accuracy %) on Ofﬁce-Caltech
dataset with Decaf features.

S-only MMD DANN CORAL WDGRL
A → C
84.55
A → D
81.05
A → W 75.59
W → A
79.82
W → D
98.25
W → C
79.67
D → A
84.56
D → W 96.84
D → C
80.49
C → A
92.35
C → W 84.21
C → D
87.72
85.44
AVG

87.80
82.46
77.81
82.98
100
81.30
84.70
98.95
82.11
93.27
89.47
91.23
87.67

88.62
90.53
91.58
92.22
100
88.62
90.11
98.95
87.80
93.14
91.58
91.23
92.03

86.18
91.23
90.53
88.39
100
88.62
85.75
97.89
85.37
93.01
92.63
89.47
90.76

86.99
93.68
89.47
93.67
100
89.43
91.69
97.89
90.24
93.54
91.58
94.74
92.74

shows the detailed comparison results of these approaches
in 12 transfer tasks. As we can see, our proposed WDGRL
outperforms all other compared approaches in 10 out of 12
domain adaptation tasks, and it achieves the second high-
est scores in the remaining 2 tasks. We ﬁnd that as adver-
sarial adaptation approaches, WDGRL outperforms DANN,
which is consistent with our theoretical analysis that WD-
GRL has more reliable gradients. MMD and CORAL are
both non-parametric and have lower computational cost than
WDGRL, while their classiﬁcation performances are also
lower than WDGRL.

Ofﬁce-Caltech object recognition dataset. Table 2
shows the results of our experiments on Ofﬁce-Caltech
dataset. We observe that our approach achieves better per-
formance than other compared approaches on most tasks.
Ofﬁce-Caltech dataset is small since there are only hundreds
of images in one domain and it is a 10-class classiﬁcation
problem. Thus we can draw a conclusion that the empiri-
cal Wasserstein distance can also be applied to small-scale
datasets adaptation effectively. We note that CORAL per-
forms better than MMD in Amazon review dataset while it
performs worse than MMD in Ofﬁce-Caltech dataset. A pos-
sible reason is that the reasonable covariance alignment ap-
proach requires large samples. On the other hand, we can see
that these different approaches have different performances
on different adaptation tasks.

Feature Visualization

We randomly choose the D→E domain adaptation task of
Amazon review dataset and plot in Figure 2 the t-SNE visu-
alization following (Donahue et al. 2014; Long et al. 2016)
to visualize the learned feature representations. In these ﬁg-
ures, red and blue points represent positive and negative
samples of the source domain, purple and green points rep-
resent positive and negative samples of the target domain. A
transferable feature mapping should cluster red (blue) and
purple (green) points together, and meanwhile classiﬁcation
can be easily conducted between purple and green points.
We can see that almost all approaches learn discriminative
and domain invariant feature representations to some extent.
And representations learned by WDGRL are more transfer-

(a) t-SNE of DANN features

(b) t-SNE of MMD features

(c) t-SNE of CORAL features

(d) t-SNE of WDGRL features

Figure 2: Feature visualization of the D→E task in Amazon
review dataset.

able since the classes between the source and target domains
align better and the region where purple and green points
mix together is smaller.

Conclusions

In this paper, we propose a new adversarial approach WD-
GRL to learn domain invariant feature representations for
domain adaptation. WDGRL can effectively reduce the do-
main discrepancy taking advantage of the gradient property
of Wasserstein distance and the transferability is guaran-
teed by the generalization bound. Our proposed approach
could be further integrated into other domain adaptation
frameworks (Bousmalis et al. 2016; Tzeng et al. 2014;
Long et al. 2015; Long et al. 2016; Zhuang et al. 2015) to
attain better transferability. Empirical results on sentiment
and image classiﬁcation domain adaptation datasets demon-
strate that WDGRL outperforms the state-of-the-art domain
invariant feature learning approaches. From feature visual-
ization, one can easily observe that WDGRL yields domain
invariant yet target-discriminative feature representations. In
future work, we will investigate more sophisticated architec-
tures for tasks on image data as well as integrate WDGRL
into existing adaptation frameworks.

Acknowledgement

This work is ﬁnancially supported by NSFC (61702327) and
Shanghai Sailing Program (17YF1428200).

References
[Ajakan et al. 2014] Ajakan, H.; Germain, P.; Larochelle, H.;
Laviolette, F.; and Marchand, M. 2014. Domain-adversarial
neural networks. arXiv:1412.4446.
[Arjovsky, Chintala, and Bottou 2017] Arjovsky, M.; Chin-
tala, S.; and Bottou, L.
2017. Wasserstein gan.
arXiv:1701.07875.
[Ben-David et al. 2007] Ben-David, S.; Blitzer, J.; Crammer,
K.; and Pereira, F. 2007. Analysis of representations for
domain adaptation. In NIPS.
[Ben-David et al. 2010] Ben-David, S.; Blitzer, J.; Crammer,
K.; Kulesza, A.; Pereira, F.; and Vaughan, J. W. 2010. A
theory of learning from different domains. Machine learn-
ing.
[Blitzer et al. 2007] Blitzer, J.; Dredze, M.; Pereira, F.; et al.
2007. Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classiﬁcation. In ACL.
[Bolley, Guillin, and Villani 2007] Bolley, F.; Guillin, A.;
and Villani, C. 2007. Quantitative concentration inequalities
for empirical measures on non-compact spaces. Probability
Theory and Related Fields.
[Bousmalis et al. 2016] Bousmalis, K.; Trigeorgis, G.; Sil-
berman, N.; Krishnan, D.; and Erhan, D. 2016. Domain
separation networks. In NIPS.
[Chen et al. 2012] Chen, M.; Xu, Z.; Weinberger, K.; and
Sha, F. 2012. Marginalized denoising autoencoders for do-
main adaptation. arXiv:1206.4683.
[Chen, Chen, and Weinberger 2011] Chen, M.; Chen, Y.; and
Weinberger, K. Q. 2011. Automatic feature decomposition
for single view co-training. In ICML.
[Chen, Weinberger, and Blitzer 2011] Chen, M.; Wein-
berger, K. Q.; and Blitzer, J. 2011. Co-training for domain
adaptation. In NIPS.
[Chu, De la Torre, and Cohn 2013] Chu, W.-S.; De la Torre,
F.; and Cohn, J. F. 2013. Selective transfer machine for
personalized facial action unit detection. In CVPR.
[Courty et al. 2017] Courty, N.; Flamary, R.; Tuia, D.; and
Rakotomamonjy, A. 2017. Optimal transport for domain
adaptation. IEEE transactions on pattern analysis and ma-
chine intelligence.
[Courty, Flamary, and Tuia 2014] Courty, N.; Flamary, R.;
and Tuia, D. 2014. Domain adaptation with regularized
optimal transport. In ECML/PKDD.
[Donahue et al. 2014] Donahue, J.; Jia, Y.; Vinyals, O.; Hoff-
man, J.; Zhang, N.; Tzeng, E.; and Darrell, T. 2014. Decaf:
A deep convolutional activation feature for generic visual
recognition. In ICML.
[Duan, Xu, and Chang 2012] Duan, L.; Xu, D.; and Chang,
S.-F. 2012. Exploiting web images for event recognition
in consumer videos: A multiple source domain adaptation
approach. In CVPR. IEEE.
[Ganin et al. 2016] Ganin, Y.; Ustinova, E.; Ajakan, H.; Ger-
main, P.; Larochelle, H.; Laviolette, F.; Marchand, M.; and
Lempitsky, V. 2016. Domain-adversarial training of neural
networks. JMLR.

[Glorot, Bordes, and Bengio 2011] Glorot, X.; Bordes, A.;
and Bengio, Y. 2011. Domain adaptation for large-scale sen-
timent classiﬁcation: A deep learning approach. In ICML.
[Gong et al. 2012] Gong, B.; Shi, Y.; Sha, F.; and Grauman,
K. 2012. Geodesic ﬂow kernel for unsupervised domain
adaptation. In CVPR. IEEE.
[Goodfellow et al. 2014] Goodfellow, I.; Pouget-Abadie, J.;
Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair, S.; Courville,
A.; and Bengio, Y. 2014. Generative adversarial nets. In
NIPS.
[Gretton et al. 2012] Gretton, A.; Borgwardt, K. M.; Rasch,
M. J.; Sch¨olkopf, B.; and Smola, A. 2012. A kernel two-
sample test. JMLR.
[Gulrajani et al. 2017] Gulrajani, I.; Ahmed, F.; Arjovsky,
M.; Dumoulin, V.; and Courville, A. 2017. Improved train-
ing of wasserstein gans. arXiv:1704.00028.
[Hoffman et al. 2014] Hoffman, J.; Rodner, E.; Donahue, J.;
Kulis, B.; and Saenko, K. 2014. Asymmetric and cate-
gory invariant feature transformations for domain adapta-
tion. IJCV.
[Huang et al. 2007] Huang, J.; Smola, A. J.; Gretton, A.;
Borgwardt, K. M.; Sch¨olkopf, B.; et al. 2007. Correcting
sample selection bias by unlabeled data. NIPS.
[Kandemir 2015] Kandemir, M. 2015. Asymmetric transfer
learning with deep gaussian processes. In ICML.
[Long et al. 2013] Long, M.; Wang, J.; Ding, G.; Sun, J.; and
Yu, P. S. 2013. Transfer feature learning with joint distri-
bution adaptation. In The IEEE International Conference on
Computer Vision (ICCV).
[Long et al. 2015] Long, M.; Cao, Y.; Wang, J.; and Jordan,
M. 2015. Learning transferable features with deep adapta-
tion networks. In ICML.
[Long et al. 2016] Long, M.; Wang, J.; Cao, Y.; Sun, J.; and
Philip, S. Y. 2016. Deep learning of transferable represen-
tation for scalable domain adaptation. TKDE.
[Luo et al. 2017] Luo, L.; Wang, X.; Hu, S.; Wang, C.; Tang,
Y.; and Chen, L. 2017. Close yet distinctive domain adapta-
tion. arXiv:1704.04235.
[Mansour, Mohri, and Rostamizadeh 2009] Mansour,
Y.;
Mohri, M.; and Rostamizadeh, A. 2009. Domain adaptation
with multiple sources. In NIPS.
[Narayanan and Mitter 2010] Narayanan, H., and Mitter, S.
2010. Sample complexity of testing the manifold hypothe-
sis. In NIPS.
[Pan and Yang 2010] Pan, S. J., and Yang, Q. 2010. A survey
on transfer learning. IEEE Transactions on knowledge and
data engineering.
[Redko, Habrard, and Sebban 2016] Redko, I.; Habrard, A.;
and Sebban, M. 2016. Theoretical analysis of domain adap-
tation with optimal transport. arXiv:1610.04420.
[Rozantsev, Salzmann, and Fua 2016] Rozantsev, A.; Salz-
mann, M.; and Fua, P. 2016. Beyond sharing weights for
deep domain adaptation. arXiv:1603.06432.
[Sun and Saenko 2016] Sun, B., and Saenko, K. 2016. Deep

coral: Correlation alignment for deep domain adaptation. In
ECCV 2016 Workshops. Springer.
[Sun, Feng, and Saenko 2016] Sun, B.; Feng, J.; and Saenko,
K. 2016. Return of frustratingly easy domain adaptation. In
AAAI.
[Tzeng et al. 2014] Tzeng, E.; Hoffman, J.; Zhang, N.;
Saenko, K.; and Darrell, T. 2014. Deep domain confusion:
Maximizing for domain invariance. arXiv:1412.3474.
[Tzeng et al. 2017] Tzeng, E.; Hoffman, J.; Saenko, K.; and
Darrell, T. 2017. Adversarial discriminative domain adapta-
tion. arXiv:1702.05464.
[Villani 2008] Villani, C. 2008. Optimal transport: old and
new.
[Weiss, Khoshgoftaar, and Wang 2016] Weiss, K.; Khosh-
goftaar, T. M.; and Wang, D. 2016. A survey of transfer
learning. Journal of Big Data.
[Zhuang et al. 2015] Zhuang, F.; Cheng, X.; Luo, P.; Pan,
S. J.; and He, Q. 2015. Supervised representation learning:
Transfer learning with deep autoencoders. In IJCAI.

Appendix

Gradient Superiority
Here we would like to prove the gradient priority of Wasserstein distance over cross-entropy in the situation where the mapped
feature distributions ﬁll in the whole feature space. For simplicity, we take two normal distributions as an example and the
conclusion still holds in the high-dimensional space. Fig 3 shows the two normal distributions and the whole space is divided
into 3 regions where the probability of source data lying in region A is high while that of target data is extremely low. The
situation is just opposite in region C and in region B two distributions differ a little.

Figure 3: Gaussian Example

We use the same notation here as above. We assume that source data are labeled 1 while target data are labeled 0 and a
domain classiﬁer is used to help learn the domain invariant representations. So given one instance (x, y) from either domain,
the feature extractor minimizes the following objective which could be viewed as the negative of cross-entropy between the
domain label y and its corresponding prediction σ(fd(fg(x)))

LD(x, y) = y log σ(fd(fg(x))) + (1 − y) log(1 − σ(fd(fg(x))))
(16)
where σ is the sigmoid function and fd is the logit computed by the domain classiﬁer network. Then the gradient of LD with
respect to θg can be computed according to the chain rule, i.e. ∂LD
∂θg

. The ﬁrst term can be directly computed

= ∂LD
∂fd

∂fg
∂θg

∂fd
∂fg

∂LD
∂fd

= y − σ(fd(fg))

As we know, the optimal domain classiﬁer is σ(f ∗
p(h)+q(h) where h = fg(x) and p(h) represents the source feature
distribution and q(h) represents the target feature distribution. So if one source instance lies in region A, it provides gradient of
almost 0. The same result holds for target samples lying in region C. So these points make no contribution to the gradient and
thus the divergence between feature distributions couldn’t be reduced effectively.

d (h)) = p(h)

Now we consider Wasserstein distance as the loss function

LW = Ex∼Pxs [fw(fg(x))] − Ex∼P

xt [fw(fg(x))].

The gradient of LW with respect to θg can be computed according to the chain rule, i.e. ∂LW
∂θg
domain data x ∼ Pxs , ∂LW
∂fw
always provide stable gradients wherever data is.

= 1; while for target domain data x ∼ Pxt, ∂LW
∂fw

∂fw
∂fg
= −1. Therefore Wasserstein distance can

. So for source

= ∂LW
∂fw

∂fg
∂θg

Generalization Bound
We now continue from the Theorem 1 in the paper to prove that target error can be bounded by the Wasserstein distance for
empirical measures on the source and target samples. we ﬁrst present a statement showing the convergence of the empirical
measure to the true Wasserstein distance.
Theorem 2. ((Bolley, Guillin, and Villani 2007), Theorem 2.1; (Redko, Habrard, and Sebban 2016), Theorem 1) Let µ be a
probability measure in Rd satisfying T1(λ) inequality. Let ˆµ = 1
i=1 δxi be its associated empirical deﬁned on a sample of
N
independent variables {xi}N
i=1 drawn from µ. Then for any d(cid:48) > d and λ(cid:48) < λ there exists some constant N0 depending on d(cid:48)
and some square exponential moment of µ such that for any (cid:15) > 0 and N ≥ N0max(ε−(d+2), 1)

(cid:80)N

P[W1(µ, ˆµ) > ε] ≤ exp(cid:0) −

N ε2(cid:1)

λ(cid:48)
2

(17)

(18)

(19)

where d(cid:48), λ(cid:48) can be calculated explicitly.

Now we can follow the Theorem 1 and Theorem 2 to prove that target error can be bounded by the Wasserstein distance for
empirical measures on the source and target samples as the process of the proof of the Theorem 3. in (Redko, Habrard, and
Sebban 2016).
Theorem 3. Under the assumption of Lemma 1, let two probability measures satisfy T1(λ) inequality, Xs and Xt be two
samples of size Ns and Nt drawn i.i.d from µs and µt resepectively. Let ˆµs = 1
be the
Ns
associated empirical measures. Then for any d(cid:48) > d and λ(cid:48) < λ there exists some constant N0 depending on d(cid:48) such that for
any δ > 0 and min(N s, N t) ≥ N0max(δ−(d(cid:48)+2), 1) with probability at least 1 − δ for all h the followingt holds:

and ˆµt = 1
Nt

i=1 δxs

i=1 δxt

(cid:80)Ns

(cid:80)Nt

i

i

(cid:15)t(h) ≤ (cid:15)s(h) + 2KW1( ˆµs, ˆµt) + λ + 2K

2log

(20)

(cid:115)

(cid:19)

(cid:18) 1
δ

/λ(cid:48)

(cid:18)(cid:114) 1
Ns

+

(cid:19)

(cid:114) 1
Nt

where λ is the combined error of the ideal hypothesis h∗ that minimizes the combined error of (cid:15)s(h) + (cid:15)t(h).

Proof.

(cid:15)t(h) ≤ (cid:15)s(h) + 2KW1(µs, µt) + λ

≤ (cid:15)s(h) + 2KW1(µs, ˆµs) + 2KW1( ˆµs, µt) + λ
(cid:18) 1
δ

≤ (cid:15)s(h) + 2K

2log

(cid:115)

(cid:19)

≤ (cid:15)s(h) + 2KW1( ˆµs, ˆµt) + λ + 2K

2log

(cid:115)

(cid:19)

(cid:18) 1
δ

/λ(cid:48)

(cid:18)(cid:114) 1
Ns

+

(cid:19)

(cid:114) 1
Nt

/Nsλ(cid:48) + 2KW1( ˆµs, ˆµt) + 2KW1( ˆµt, µt) + λ

(21)

More Experiment Results
Synthetic data. We generate a synthetic dataset to show the superior gradient advantage of WDGRL over DANN. In the paper,
we claim that when two representation distributions are distant or have regions they differ a lot, DANN will have gradient
vanishing problem while WDGRL still provides the stable gradient. It is a little difﬁcult to fully realize such situations, so
we design a rather restrictive experiment. However, this toy experiment does verify DANN may fail in some situations while
WDGRL can work. We visualize the data input in Figure 4(a) with 2000 samples for each domain. And from Figure 4(b) we
ﬁnd that if we adopt DANN the domain classiﬁer can distinguish two domain data well and the DANN loss decreases to nearly
0 as the training process continues. In such situation, the domain classiﬁer can provide poor gradient. As shown in 4(c), our
WDGRL approach can effectively classify the target data while DANN fails.

(a) input visualization

(b) DANN loss and accuracy

(c) Performance on target domain

Figure 4: Synthetic experiment.

Ofﬁce-Caltech dataset with SURF features. Table 3 shows the result of our experiments on Ofﬁce-Caltech dataset with

SURF features.

Email spam ﬁltering dataset. The email spam ﬁltering dataset 4 released by ECML/PKDD 2006 discovery challenge con-
tains 4 separate user inboxes. From public inbox (source domain) 4,000 labeled training samples were collected, among which

4http://www.ecmlpkdd2006.org/challenge.html

Table 3: Performance (accuracy %) on Ofﬁce-Caltech dataset with Decaf features

S-only MMD DANN D-CORAL WDGRL
A → C
43.19
A → D
35.03
A → W 35.23
W → A
30.06
W → D
80.25
W → C
30.19
C → W 36.95
C → A
52.92
C → D
45.86
D → W 69.50
D → A
31.21
D → C
30.37
43.4
AVG

44.08
41.40
37.29
34.13
84.71
30.72
40.34
54.80
47.13
73.56
32.46
30.72
45.95

45.86
44.59
40.68
32.15
81.53
31.08
42.37
55.22
48.41
76.95
35.60
32.59
47.25

44.97
41.40
38.64
34.13
82.80
32.68
43.39
54.91
47.77
74.24
31.63
32.24
46.57

44.97
40.13
38.31
34.86
84.08
33.30
40.00
53.44
47.13
73.90
31.52
31.52
46.10

half samples are spam emails and the other half non-spam ones. The test samples were collected from 3 private inboxes (target
domains), each of which consists of 2,500 samples. In our experiments, 3 cross-domain tasks are constructed from the public
inbox to the private inboxes. We choose the 5,067 most frequent terms as features and 4 test samples were deleted as a result of
not containing any of these terms. Experimenting on the 3 tasks by transferring from public to private groups of private inboxes
u1 ∼ u3, we found our method does achieve better performance than MMD, DANN and D-CORAL, which is demonstrated in
Table 4. We can see from this result that all these approaches can reach the goal of learning the transferable features for they all
outperform the source only baseline at least 9%. Among them, MMD and DANN achieve almost the same performance while
WDGRL further boosts the performance by a rate of 2.90%.

Table 4: Performance (Accuracy %) on email spam dataset
S only MMD DANN D-CORAL WDGRL
69.63
76.01
81.24
75.63

P → u1
P → u2
P → u3
AVG

83.27
85.74
91.92
86.98

79.71
83.83
89.80
84.45

80.95
85.98
94.08
87.00

85.67
88.26
95.76
89.90

Newsgroup classiﬁcation dataset. The 20 newsgroups dataset 5 is a collection of 18,774 newsgroup documents across 6
top categories and 20 subcategories in a hierarchical structure. In our experiments, we adopt a similar setting as (?). The task
is to classify top categories and the four largest top categories (comp, rec, sci, talk) are chosen for evaluation. Speciﬁcally, for
each top category, the largest subcategory is selected as the source domain while the second largest subcategory is chosen as
the target domain. Moreover, the largest category comp is considered as the positive class and one of the three other categories
as the negative class.

The distribution shift across newsgroups is caused by category speciﬁc words. Notice the construction of our domain adap-
tation tasks which aim to classify the top categories while the adaptation exists between the subcategories. It makes sense that
there exist more differences among top categories than those among subcategories which implies that classiﬁcation is not that
sensitive to the subcategories and thus enables the ease of domain adaptation. Table 5 gives the information of performance on
the 20newsgroup dataset from which we can ﬁnd that the comparison methods are almost neck and neck, which is consistent
with our previous observation.

Table 5: Performance (Accuracy %) on 20 newsgroup dataset

C vs. R
C vs. S
C vs. T
AVG

S only MMD DANN D-CORAL WDGRL
81.62
74.01
94.44
83.36

98.10
90.57
97.75
95.47

97.85
87.52
96.96
94.11

97.57
84.20
97.22
93.00

98.35
91.33
97.62
95.77

5http://qwone.com/˜jason/20Newsgroups/

