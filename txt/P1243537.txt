0
2
0
2
 
b
e
F
 
4
2
 
 
]
S
A
.
s
s
e
e
[
 
 
2
v
8
8
6
8
0
.
2
0
0
2
:
v
i
X
r
a

AN EMPIRICAL STUDY OF CONV-TASNET

Berkan Kadıo˘glu ‡ (cid:63) Michael Horgan† (cid:63)

Xiaoyu Liu† (cid:63)

Jordi Pons† Dan Darcy†

Vivek Kumar†

‡ Electrical and Computer Engineering Department, Northeastern University
† Dolby Laboratories

ABSTRACT

Conv-TasNet is a recently proposed waveform-based deep neural
network that achieves state-of-the-art performance in speech source
separation. Its architecture consists of a learnable encoder/decoder
and a separator that operates on top of this learned space. Various
improvements have been proposed to Conv-TasNet. However, they
mostly focus on the separator, leaving its encoder/decoder as a (shal-
low) linear operator. In this paper, we conduct an empirical study of
Conv-TasNet and propose an enhancement to the encoder/decoder
that is based on a (deep) non-linear variant of it. In addition, we ex-
periment with the larger and more diverse LibriTTS dataset and in-
vestigate the generalization capabilities of the studied models when
trained on a much larger dataset. We propose cross-dataset evalua-
tion that includes assessing separations from the WSJ0-2mix, Lib-
riTTS and VCTK databases. Our results show that enhancements to
the encoder/decoder can improve average SI-SNR performance by
more than 1 dB. Furthermore, we offer insights into the generaliza-
tion capabilities of Conv-TasNet and the potential value of improve-
ments to the encoder/decoder.

Index Terms— Speech source separation, Conv-TasNet, deep

encoder/decoder, generalization, end-to-end.

and in [15] a clustering mechanism is integrated into the separa-
tor. Interestingly, only a few works touch on the encoder/decoder of
Conv-TasNet. In a multi-channel setting [16,17] a second encoder is
used to learn phase differences between channels, and in [15] a mag-
nitude STFT is appended to the learned encoder transform. As seen,
most previous works use a (shallow) linear encoder/decoder. To the
best of our knowledge, only [18] used a deep encoder/decoder for a
Conv-TasNet inspired model for speech enhancement, which has not
been extended to or fully tested for speech source separation.

In this work, we conduct an empirical study of Conv-TasNet,
which is formally introduced in Section 2. Our contributions focus
on two areas: architectural improvements to the encoder/decoder,
and a study of the generalization capabilities of the developed mod-
els. In Section 3, we introduce the deep encoder/decoder we propose
and we discuss several variants of this structure. In Section 4.1, we
evaluate the studied models against the WSJ0-2mix database to gain
insights on the performance of each variant. In Section 4.2, we ex-
plore the impact of using a larger, more diverse training set and we
study the generalization capabilities of the trained models via em-
ploying a cross-dataset evaluation. In Section 4.3, we compare the
performance of the proposed deep encoder/decoder to several state-
of-the-art separators. We conclude our discussion in Section 5.

1. INTRODUCTION

2. REVIEW OF CONV-TASNET

With the recent advent of deep learning, speech separation methods
have experienced steadfast success in difﬁcult scenarios where, e.g.,
prior information about the speakers is not available. Depending on
the models’ input/output, one can roughly categorize these methods
into spectrogram- and waveform-based models. Spectrogram-based
models, despite their success in the past [1–3], have limitations:
(i) they discard phase information via simply estimating masks that
operate over the magnitude or power spectrogram; (ii) they tend to
employ the noisy phase of the mixture for reconstructing the clean
source; and (iii) they employ a generic transform (like STFT) which
might not be optimal for the task at hand. Although several works in-
vestigate how to address the above-mentioned limitations [4–7], re-
cent publications have reported promising results by tackling source
separation directly in the waveform domain [8–14].

The Conv-TasNet [8] architecture, the work on which this paper
builds, is one such end-to-end neural network that achieves state-
of-the-art performance in speech source separation. Its architecture
consists of two parts: an encoder/decoder, and a separator. Re-
cently, several improvements have been proposed to this architec-
ture. However, most focus has been devoted to its separator. For
example, in [10, 11] a parallel and multi-scale separator is proposed,

Single-channel multi-speaker speech separation aims to separate C
individual speech sources sc ∈ RT , where c ∈ {1, 2,
. . . , C},
from a single-channel mixture of speech x ∈ RT where T is the
length of the waveform and x = (cid:80)C
c=1 sc. Conv-TasNet [8] is an
end-to-end fully convolutional network proposed for this purpose.
Fig. 1 illustrates the two main modules in Conv-TasNet: an en-
code/decoder pair, and a separator.

Fig. 1. The building blocks of Conv-TasNet.

The encoder linearly maps a mixture waveform into a learned latent
space. Speciﬁcally, a mixture waveform is segmented into K over-
lapping frames xk ∈ RL, k = 1, 2, . . . , K, each of length L with
stride S. Then, the linear transform is deﬁned as:

E = UX,

(1)

(cid:63) Work done during Berkan Kadıo˘glu’s internship at Dolby Laborato-

ries. Berkan, Michael and Xiaoyu equally contributed to this work.

where X ∈ RL×K stores all frames in columns, U ∈ RN ×L con-
tains N learnable basis ﬁlters in its rows, and E ∈ RN ×K is the

