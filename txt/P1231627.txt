8
1
0
2
 
l
u
J
 
8
 
 
]
L
C
.
s
c
[
 
 
5
v
4
6
3
2
0
.
5
0
7
1
:
v
i
X
r
a

Supervised Learning of Universal Sentence Representations from
Natural Language Inference Data

Alexis Conneau
Facebook AI Research
aconneau@fb.com

Douwe Kiela
Facebook AI Research
dkiela@fb.com

Holger Schwenk
Facebook AI Research
schwenk@fb.com

Lo¨ıc Barrault
LIUM, Universit´e Le Mans
loic.barrault@univ-lemans.fr

Antoine Bordes
Facebook AI Research
abordes@fb.com

Abstract

Many modern NLP systems rely on word
embeddings, previously trained in an un-
supervised manner on large corpora, as
base features. Efforts to obtain embed-
dings for larger chunks of text, such as
sentences, have however not been so suc-
cessful. Several attempts at learning unsu-
pervised representations of sentences have
not reached satisfactory enough perfor-
mance to be widely adopted. In this paper,
we show how universal sentence represen-
tations trained using the supervised data of
the Stanford Natural Language Inference
datasets can consistently outperform unsu-
pervised methods like SkipThought vec-
tors (Kiros et al., 2015) on a wide range
of transfer tasks. Much like how com-
puter vision uses ImageNet to obtain fea-
tures, which can then be transferred to
other tasks, our work tends to indicate the
suitability of natural language inference
for transfer learning to other NLP tasks.
Our encoder is publicly available1.

1 Introduction

Distributed representations of words (or word
embeddings) (Bengio et al., 2003; Collobert et al.,
2011; Mikolov et al., 2013; Pennington et al.,
2014) have shown to provide useful features for
various tasks in natural language processing and
computer vision. While there seems to be a con-
sensus concerning the usefulness of word embed-
dings and how to learn them, this is not yet clear
with regard to representations that carry the mean-
ing of a full sentence. That is, how to capture the
relationships among multiple words and phrases in
a single vector remains an question to be solved.

In this paper, we study the task of learning
universal representations of sentences, i.e., a sen-
tence encoder model that is trained on a large cor-
pus and subsequently transferred to other tasks.
Two questions need to be solved in order to build
such an encoder, namely: what is the prefer-
able neural network architecture; and how and
on what task should such a network be trained.
Following existing work on learning word em-
beddings, most current approaches consider learn-
ing sentence encoders in an unsupervised manner
like SkipThought (Kiros et al., 2015) or FastSent
(Hill et al., 2016). Here, we investigate whether
supervised learning can be leveraged instead, tak-
ing inspiration from previous results in computer
vision, where many models are pretrained on the
ImageNet (Deng et al., 2009) before being trans-
ferred. We compare sentence embeddings trained
on various supervised tasks, and show that sen-
tence embeddings generated from models trained
on a natural language inference (NLI) task reach
the best results in terms of transfer accuracy. We
hypothesize that the suitability of NLI as a train-
ing task is caused by the fact that it is a high-level
understanding task that involves reasoning about
the semantic relationships within sentences.

Unlike in computer vision, where convolutional
neural networks are predominant, there are mul-
tiple ways to encode a sentence using neural net-
works. Hence, we investigate the impact of the
sentence encoding architecture on representational
transferability, and compare convolutional, recur-
rent and even simpler word composition schemes.
Our experiments show that an encoder based on a
bi-directional LSTM architecture with max pool-
ing,
trained on the Stanford Natural Language
Inference (SNLI) dataset (Bowman et al., 2015),
yields state-of-the-art sentence embeddings com-
pared to all existing alternative unsupervised ap-
proaches like SkipThought or FastSent, while be-

1https://www.github.com/facebookresearch/InferSent

ing much faster to train. We establish this ﬁnding
on a broad and diverse set of transfer tasks that
measures the ability of sentence representations to
capture general and useful information.

2 Related work

Transfer learning using supervised features has
been successful in several computer vision ap-
plications (Razavian et al., 2014). Striking ex-
amples include face recognition (Taigman et al.,
2014) and visual question answering (Antol et al.,
2015), where image features trained on ImageNet
(Deng et al., 2009) and word embeddings trained
on large unsupervised corpora are combined.

In contrast, most approaches for sentence repre-
sentation learning are unsupervised, arguably be-
cause the NLP community has not yet found the
best supervised task for embedding the semantics
of a whole sentence. Another reason is that neu-
ral networks are very good at capturing the bi-
ases of the task on which they are trained, but
can easily forget the overall information or seman-
tics of the input data by specializing too much
on these biases. Learning models on large un-
supervised task makes it harder for the model to
specialize. Littwin and Wolf (2016) showed that
co-adaptation of encoders and classiﬁers, when
trained end-to-end, can negatively impact the gen-
eralization power of image features generated by
an encoder. They propose a loss that incorporates
multiple orthogonal classiﬁers to counteract this
effect.

Recent work on generating sentence embed-
dings range from models that compose word
embeddings (Le and Mikolov, 2014; Arora et al.,
2017; Wieting et al., 2016) to more complex neu-
ral network architectures. SkipThought vectors
(Kiros et al., 2015) propose an objective func-
tion that adapts the skip-gram model for words
(Mikolov et al., 2013) to the sentence level. By en-
coding a sentence to predict the sentences around
it, and using the features in a linear model, they
were able to demonstrate good performance on 8
transfer tasks. They further obtained better results
using layer-norm regularization of their model in
(Ba et al., 2016). Hill et al. (2016) showed that
the task on which sentence embeddings are trained
signiﬁcantly impacts their quality.

In addition to unsupervised methods, they in-
cluded supervised training in their comparison—
namely, on machine translation data (using the

WMT’14 English/French and English/German
pairs), dictionary deﬁnitions and image captioning
data (see also Kiela et al. (2017)) from the COCO
dataset (Lin et al., 2014). These models obtained
signiﬁcantly lower results compared to the unsu-
pervised Skip-Thought approach.

Recent work has explored training sentence en-
coders on the SNLI corpus and applying them on
the SICK corpus (Marelli et al., 2014), either us-
ing multi-task learning or pretraining (Mou et al.,
2016; Bowman et al., 2015). The results were in-
conclusive and did not reach the same level as sim-
pler approaches that directly learn a classiﬁer on
top of unsupervised sentence embeddings instead
(Arora et al., 2017). To our knowledge, this work
is the ﬁrst attempt to fully exploit the SNLI cor-
pus for building generic sentence encoders. As we
show in our experiments, we are able to consis-
tently outperform unsupervised approaches, even
if our models are trained on much less (but human-
annotated) data.

3 Approach

This work combines two research directions,
which we describe in what follows. First, we ex-
plain how the NLI task can be used to train univer-
sal sentence encoding models using the SNLI task.
We subsequently describe the architectures that we
investigated for the sentence encoder, which, in
our opinion, covers a suitable range of sentence
encoders currently in use. Speciﬁcally, we exam-
ine standard recurrent models such as LSTMs and
GRUs, for which we investigate mean and max-
pooling over the hidden representations; a self-
attentive network that incorporates different views
of the sentence; and a hierarchical convolutional
network that can be seen as a tree-based method
that blends different levels of abstraction.

3.1 The Natural Language Inference task

The SNLI dataset consists of 570k human-
generated English sentence pairs, manually la-
beled with one of three categories: entailment,
contradiction and neutral. It captures natural lan-
guage inference, also known in previous incarna-
tions as Recognizing Textual Entailment (RTE),
and constitutes one of the largest high-quality la-
beled resources explicitly constructed in order to
require understanding sentence semantics. We hy-
pothesize that the semantic nature of NLI makes
it a good candidate for learning universal sentence

embeddings in a supervised way. That is, we aim
to demonstrate that sentence encoders trained on
natural language inference are able to learn sen-
tence representations that capture universally use-
ful features.

3-way softmax

fully-connected layers

(u, v, |u − v|, u ∗ v)

u

v

sentence encoder
with premise input

sentence encoder
with hypothesis input

Figure 1: Generic NLI training scheme.

Models can be trained on SNLI in two differ-
ent ways: (i) sentence encoding-based models that
explicitly separate the encoding of the individual
sentences and (ii) joint methods that allow to use
encoding of both sentences (to use cross-features
or attention from one sentence to the other).

Since our goal is to train a generic sentence en-
coder, we adopt the ﬁrst setting. As illustrated in
Figure 1, a typical architecture of this kind uses a
shared sentence encoder that outputs a representa-
tion for the premise u and the hypothesis v. Once
the sentence vectors are generated, 3 matching
methods are applied to extract relations between
u and v : (i) concatenation of the two representa-
tions (u, v); (ii) element-wise product u ∗ v; and
(iii) absolute element-wise difference |u − v|. The
resulting vector, which captures information from
both the premise and the hypothesis, is fed into
a 3-class classiﬁer consisting of multiple fully-
connected layers culminating in a softmax layer.

3.2 Sentence encoder architectures

A wide variety of neural networks for encod-
ing sentences into ﬁxed-size representations ex-
ists, and it is not yet clear which one best cap-
tures generically useful information. We com-
pare 7 different architectures: standard recurrent
encoders with either Long Short-Term Memory
(LSTM) or Gated Recurrent Units (GRU), con-
catenation of last hidden states of forward and
backward GRU, Bi-directional LSTMs (BiLSTM)
with either mean or max pooling, self-attentive

network and hierarchical convolutional networks.

3.2.1 LSTM and GRU
Our ﬁrst, and simplest,
encoders apply re-
current neural networks using either LSTM
(Hochreiter and Schmidhuber, 1997) or GRU
(Cho et al., 2014) modules, as in sequence to se-
quence encoders (Sutskever et al., 2014).
For
a sequence of T words (w1, . . . , wT ), the net-
work computes a set of T hidden representations
−−−−→
h1, . . . , hT , with ht =
LSTM(w1, . . . , wT ) (or
using GRU units instead). A sentence is repre-
sented by the last hidden vector, hT .

We also consider a model BiGRU-last that con-
catenates the last hidden state of a forward GRU,
and the last hidden state of a backward GRU to
have the same architecture as for SkipThought
vectors.

3.2.2 BiLSTM with mean/max pooling
For a sequence of T words {wt}t=1,...,T , a bidirec-
tional LSTM computes a set of T vectors {ht}t.
For t ∈ [1, . . . , T ], ht, is the concatenation of a
forward LSTM and a backward LSTM that read
the sentences in two opposite directions:

−→
ht =
←−
ht =

ht = [

−−−−→
LSTMt(w1, . . . , wT )
←−−−−
LSTMt(w1, . . . , wT )
−→
ht ,

←−
ht ]

We experiment with two ways of combining the
varying number of {ht}t to form a ﬁxed-size vec-
tor, either by selecting the maximum value over
each dimension of the hidden units (max pool-
ing) (Collobert and Weston, 2008) or by consider-
ing the average of the representations (mean pool-
ing).

u :

x x

… …
x

x

max-pooling

x

←−
h1

−→
h1

x

←−
h2

−→
h2

x

←−
h3

−→
h3

x

←−
h4

−→
h4

w1

The

w2

movie

w3

was

w4

great

Figure 2: Bi-LSTM max-pooling network.

3.2.3 Self-attentive network
The self-attentive sentence encoder (Liu et al.,
2016; Lin et al., 2017) uses an attention mecha-
nism over the hidden states of a BiLSTM to gen-
erate a representation u of an input sentence. The
attention mechanism is deﬁned as :

representation ui is computed by a max-pooling
operation over the feature maps (see Figure 4).

u :

u1

u2

u3

u4

αi =

¯hi = tanh(W hi + bw)
¯hT
i uw
e
¯hT
i uw
Pi e
u = X
t

αihi

where {h1, . . . , hT } are the output hidden vec-
tors of a BiLSTM. These are fed to an afﬁne trans-
formation (W , bw) which outputs a set of keys
(¯h1, . . . , ¯hT ). The {αi} represent the score of
similarity between the keys and a learned con-
text query vector uw. These weights are used
to produce the ﬁnal representation u, which is a
weighted linear combination of the hidden vectors.
Following Lin et al. (2017) we use a self-
attentive network with multiple views of the input
sentence, so that the model can learn which part of
the sentence is important for the given task. Con-
w, u4
cretely, we have 4 context vectors u1
w
which generate 4 representations that are then con-
catenated to obtain the sentence representation u.
Figure 3 illustrates this architecture.

w, u2

w, u3

α1

α2

uw

u

←−
h2

−→
h2

α3

←−
h3

−→
h3

α4

←−
h4

−→
h4

w2

movie

w3

was

w4

great

←−
h1

−→
h1

w1

The

Figure 3: Inner Attention network architecture.

3.2.4 Hierarchical ConvNet
One of the currently best performing models on
classiﬁcation tasks is a convolutional architecture
termed AdaSent (Zhao et al., 2015), which con-
catenates different representations of the sentences
at different level of abstractions. Inspired by this
architecture, we introduce a faster version consist-
ing of 4 convolutional layers. At every layer, a

convolutional layer

…

convolutional layer

…

convolutional layer

…

max-pooling

u4

max-pooling

u3

max-pooling

u2

…

…

…

x

x
x

x

x

x

max-pooling

x u1

x
x
x

x

x

convolutional layer

This

is

the

movie

of

all

time

great
est

Figure 4: Hierarchical ConvNet architecture.

The ﬁnal representation u = [u1, u2, u3, u4]
concatenates representations at different levels of
the input sentence. The model thus captures hi-
erarchical abstractions of an input sentence in a
ﬁxed-size representation.

3.3 Training details

For all our models trained on SNLI, we use SGD
with a learning rate of 0.1 and a weight decay of
0.99. At each epoch, we divide the learning rate
by 5 if the dev accuracy decreases. We use mini-
batches of size 64 and training is stopped when the
learning rate goes under the threshold of 10−5. For
the classiﬁer, we use a multi-layer perceptron with
1 hidden-layer of 512 hidden units. We use open-
source GloVe vectors trained on Common Crawl
840B with 300 dimensions as ﬁxed word embed-
dings.

4 Evaluation of sentence representations

Our aim is to obtain general-purpose sentence
embeddings that capture generic information that
is useful for a broad set of tasks. To evalu-
ate the quality of these representations, we use
them as features in 12 transfer tasks. We present
our sentence-embedding evaluation procedure in

N task

name
11k
MR
4k
CR
SUBJ
10k
MPQA 11k
6k
TREC
70k
SST

sentiment (movies)
product reviews
subjectivity/objectivity
opinion polarity
question-type
sentiment (movies)

C examples
2
2
2
2
6
2

”Too slow for a younger crowd , too shallow for an older one.” (neg)
”We tried it out christmas night and it worked great .” (pos)
”A movie that doesn’t aim too high , but doesn’t need to.” (subj)
”don’t want”; ”would like to tell”; (neg, pos)
”What are the twin cities ?” (LOC:city)
”Audrey Tautou has a knack for picking roles that magnify her [..]” (pos)

Table 1: Classiﬁcation tasks. C is the number of class and N is the number of samples.

this section. We constructed a sentence evalu-
ation tool2 called SentEval (Conneau and Kiela,
2018) to automate evaluation on all
the tasks
mentioned in this paper. The tool uses Adam
(Kingma and Ba, 2014) to ﬁt a logistic regression
classiﬁer, with batch size 64.

Binary and multi-class classiﬁcation We use
a set of binary classiﬁcation tasks (see Table 1)
that covers various types of sentence classiﬁca-
tion,
including sentiment analysis (MR, SST),
question-type (TREC), product reviews (CR), sub-
jectivity/objectivity (SUBJ) and opinion polarity
(MPQA). We generate sentence vectors and train
a logistic regression on top. A linear classiﬁer re-
quires fewer parameters than an MLP and is thus
suitable for small datasets, where transfer learning
is especially well-suited. We tune the L2 penalty
of the logistic regression with grid-search on the
validation set.

Entailment and semantic relatedness We also
evaluate on the SICK dataset for both entailment
(SICK-E) and semantic relatedness (SICK-R). We
use the same matching methods as in SNLI and
learn a Logistic Regression on top of the joint rep-
resentation. For semantic relatedness evaluation,
we follow the approach of (Tai et al., 2015) and
learn to predict the probability distribution of re-
latedness scores. We report Pearson correlation.

STS14 - Semantic Textual Similarity While
semantic relatedness is supervised in the case
of SICK-R, we also evaluate our embeddings
on the 6 unsupervised SemEval tasks of STS14
(Agirre et al., 2014). This dataset includes sub-
sets of news articles, forum discussions, image de-
scriptions and headlines from news articles con-
taining pairs of sentences (lower-cased), labeled
with a similarity score between 0 and 5. These
tasks evaluate how the cosine distance between
two sentences correlate with a human-labeled sim-
ilarity score through Pearson and Spearman corre-

lations.

Paraphrase detection The Microsoft Research
Paraphrase Corpus is composed of pairs of sen-
tences which have been extracted from news
sources on the Web. Sentence pairs have been
human-annotated according to whether they cap-
ture a paraphrase/semantic equivalence relation-
ship. We use the same approach as with SICK-E,
except that our classiﬁer has only 2 classes.

Caption-Image retrieval The caption-image
retrieval task evaluates joint image and language
feature models (Hodosh et al., 2013; Lin et al.,
2014). The goal is either to rank a large collec-
tion of images by their relevance with respect to a
given query caption (Image Retrieval), or ranking
captions by their relevance for a given query image
(Caption Retrieval). We use a pairwise ranking-
loss Lcir(x, y):

max(0, α − s(V y, U x) + s(V y, U xk)) +

max(0, α − s(U x, V y) + s(U x, V yk′))

X
y

X
k

X
x

X
k′

where (x, y) consists of an image y with one
of its associated captions x, (yk)k and (yk′)k′ are
negative examples of the ranking loss, α is the
margin and s corresponds to the cosine similarity.
U and V are learned linear transformations that
project the caption x and the image y to the same
embedding space. We use a margin α = 0.2 and
30 contrastive terms. We use the same splits as
in (Karpathy and Fei-Fei, 2015), i.e., we use 113k
images from the COCO dataset (each containing
5 captions) for training, 5k images for validation
and 5k images for test. For evaluation, we split the
5k images in 5 random sets of 1k images on which
we compute Recall@K, with K ∈ {1, 5, 10} and
median (Med r) over the 5 splits. For fair compari-
son, we also report SkipThought results in our set-
ting, using 2048-dimensional pretrained ResNet-
101 (He et al., 2016) with 113k training images.

2https://www.github.com/facebookresearch/SentEval

name
SNLI

task
NLI

N premise

560k

”Two women are embracing while
holding to go packages.”

SICK-E NLI

10k A man is typing on a machine used

SICK-R

STS

10k

STS14

STS

4.5k

for stenography
”A man is singing a song and play-
ing the guitar”
”Liquid ammonia leak kills 15 in
Shanghai”

The man isn’t operating a steno-
graph
”A man is opening a package that
contains headphones”
”Liquid ammonia leak kills at least
15 in Shanghai”

contradiction

1.6

4.6

hypothesis
”Two woman are holding packages.”

label
entailment

Table 2: Natural Language Inference and Semantic Textual Similarity tasks. NLI labels are contra-
diction, neutral and entailment. STS labels are scores between 0 and 5.

Model

LSTM
GRU
BiGRU-last
BiLSTM-Mean
Inner-attention
HConvNet
BiLSTM-Max

dim
2048
4096
4096
4096
4096
4096
4096

NLI

Transfer

dev
81.9
82.4
81.3
79.0
82.3
83.7
85.0

test micro macro
78.6
80.7
80.9
81.8
81.7
80.9
81.7
78.2
81.0
82.5
80.9
83.4
83.7
84.5

79.5
81.7
82.9
83.1
82.1
82.0
85.2

Table 3: Performance of sentence encoder ar-
chitectures on SNLI and (aggregated) transfer
tasks. Dimensions of embeddings were selected
according to best aggregated scores (see Figure 5).

Figure 5: Transfer performance w.r.t. embed-
ding size using the micro aggregation method.

5 Empirical results

In this section, we refer to ”micro” and ”macro”
averages of development set (dev) results on trans-
fer tasks whose metrics is accuracy: we compute a
”macro” aggregated score that corresponds to the
classical average of dev accuracies, and the ”mi-
cro” score that is a sum of the dev accuracies,
weighted by the number of dev samples.

5.1 Architecture impact

Model We observe in Table 3 that different mod-
els trained on the same NLI corpus lead to differ-
ent transfer tasks results. The BiLSTM-4096 with
the max-pooling operation performs best on both
SNLI and transfer tasks. Looking at the micro and

macro averages, we see that it performs signiﬁ-
cantly better than the other models LSTM, GRU,
BiGRU-last, BiLSTM-Mean, inner-attention and
the hierarchical-ConvNet.

Table 3 also shows that better performance on
the training task does not necessarily translate in
better results on the transfer tasks like when com-
paring inner-attention and BiLSTM-Mean for in-
stance.

We hypothesize that some models are likely to
over-specialize and adapt too well to the biases of
a dataset without capturing general-purpose infor-
mation of the input sentence. For example, the
inner-attention model has the ability to focus only
on certain parts of a sentence that are useful for
the SNLI task, but not necessarily for the transfer
tasks. On the other hand, BiLSTM-Mean does not
make sharp choices on which part of the sentence
is more important than others. The difference be-
tween the results seems to come from the different
abilities of the models to incorporate general in-
formation while not focusing too much on speciﬁc
features useful for the task at hand.

For a given model, the transfer quality is also
sensitive to the optimization algorithm: when
training with Adam instead of SGD, we observed
that the BiLSTM-max converged faster on SNLI
(5 epochs instead of 10), but obtained worse re-
sults on the transfer tasks, most likely because of
the model and classiﬁer’s increased capability to
over-specialize on the training task.

Embedding size Figure 5 compares the over-
all performance of different architectures, showing
the evolution of micro averaged performance with
regard to the embedding size.

Since it is easier to linearly separate in high di-
mension, especially with logistic regression, it is
not surprising that increased embedding sizes lead
to increased performance for almost all models.
However, this is particularly true for some mod-

Model

MR CR SUBJ MPQA SST TREC MRPC

SICK-R SICK-E STS14

79.2
66.9
78.0
-
79.8
81.0
78.5
77.4
81.3

Unsupervised representation training (unordered sentences)
82.4
73.7
Unigram-TFIDF
70.7
60.2
ParagraphVec (DBOW)
86.9
74.6
SDAE
SIF (GloVe + WR)
-
-
word2vec BOW†
88.3
77.7
fastText BOW†
87.8
78.3
GloVe BOW†
78.7
87.6
GloVe Positional Encoding† 78.3
87.1
BiLSTM-Max (untrained)†
88.7
77.5
Unsupervised representation training (ordered sentences)
80.6
81.5
87.1
89.3

FastSent
FastSent+AE
SkipThought
SkipThought-LN

90.3
76.3
90.8
-
90.9
92.4
91.6
91.1
89.6

88.7
88.8
93.6
93.7

78.4
76.7
80.1
83.1

70.8
71.8
76.5
79.4

85.0
59.4
78.4
-

-
-
-
82.2
79.7 83.6
81.9 84.8
79.8 83.6
80.6 83.3
80.7 85.8

-
-

76.8
80.4
82.0 92.2
82.9 88.4

Supervised representation training

70.8
87.2
81.5
-
89.5
89.8
90.2

77.4
90.7
84.9
-
90.2
92.1
92.4

69.3
78.7
70.1
-
83.7
84.6
86.3

CaptionRep (bow)
61.9
76.7
DictRep (bow)
NMT En-to-Fr
64.7
Paragram-phrase
-
BiLSTM-Max (on SST)†
(*)
BiLSTM-Max (on SNLI)†
79.9
BiLSTM-Max (on AllNLI)† 81.1
Supervised methods (directly trained for each task – no transfer)
83.1
-
-
-
-

Naive Bayes - SVM
AdaSent
TF-KLD
Illinois-LH
Dependency Tree-LSTM

86.3
93.3
-
-
-

93.2
95.5
-
-
-

81.8
86.3
-
-
-

79.4
83.1
-
-
-

72.2
-
81.0
-
82.8
-
-
79.7
(*)
86.0
83.3 88.7
84.6 88.2

-
92.4
-
-
-

73.6/81.7
72.9/81.1
73.7/80.7
-
72.5/81.4
73.9/82.0
72.1/80.9
72.5/81.2
73.2/81.6

72.2/80.3
71.2/79.1
73.0/82.0
-

73.6/81.9
68.4/76.8
69.1/77.1
-
72.7/80.9
75.1/82.3
76.2/83.1

-
-
80.4/85.9
-
-

-
-
-
-
0.803
0.815
0.800
0.799
0.860

-
-
0.858
0.858

-
-
-
0.849
0.863
0.885
0.884

-
-
-
-
0.868

-
-
-
84.6
78.7
78.3
78.6
77.9
83.4

-
-
82.3
79.5

-
-

83.1
83.1
86.3
86.3

-
-
-
84.5
-

.58/.57
.42/.43
.37/.38
.69/ -
.65/.64
.63/.62
.54/.56
.51/.54
.39/.48

.63/.64
.62/.62
.29/.35
.44/.45

.46/.42
.67/.70
.43/.42
.71/ -
.55/.54
.68/.65
.70/.67

-
-
-
-
-

Table 4: Transfer test results for various architectures trained in different ways. Underlined
are best results for transfer learning approaches, in bold are best results among the models trained
† indicates methods that we trained, other transfer models have been extracted
in the same way.
from (Hill et al., 2016). For best published supervised methods (no transfer), we consider AdaSent
(Zhao et al., 2015), TF-KLD (Ji and Eisenstein, 2013), Tree-LSTM (Tai et al., 2015) and Illinois-LH
system (Lai and Hockenmaier, 2014). (*) Our model trained on SST obtained 83.4 for MR and 86.0 for
SST (MR and SST come from the same source), which we do not put in the tables for fair comparison
with transfer methods.

els (BiLSTM-Max, HConvNet, inner-att), which
demonstrate unequal abilities to incorporate more
information as the size grows. We hypothesize
that such networks are able to incorporate infor-
mation that is not directly relevant to the objective
task (results on SNLI are relatively stable with re-
gard to embedding size) but that can nevertheless
be useful as features for transfer tasks.

5.2 Task transfer

We report in Table 4 transfer tasks results for
different architectures trained in different ways.
We group models by the nature of the data
on which they were trained. The ﬁrst group

corresponds to models trained with unsuper-
vised unordered sentences. This includes bag-
of-words models such as word2vec-SkipGram,
the Unigram-TFIDF model, the Paragraph Vector
model (Le and Mikolov, 2014), the Sequential De-
noising Auto-Encoder (SDAE) (Hill et al., 2016)
and the SIF model (Arora et al., 2017), all trained
on the Toronto book corpus (Zhu et al., 2015). The
second group consists of models trained with un-
supervised ordered sentences such as FastSent and
SkipThought (also trained on the Toronto book
corpus). We also include the FastSent variant
“FastSent+AE” and the SkipThought-LN version
that uses layer normalization. We report results

Model
Direct supervision of sentence representations
m-CNN
m-CNNENS
Order-embeddings
Pre-trained sentence representations
+ VGG19 (82k)
SkipThought
SkipThought
+ ResNet101 (113k)
BiLSTM-Max (on SNLI) + ResNet101 (113k)
BiLSTM-Max (on AllNLI) + ResNet101 (113k)

(Ma et al., 2015)
(Ma et al., 2015)
(Vendrov et al., 2016)

Caption Retrieval

Image Retrieval

R@1 R@5 R@10 Med r R@1 R@5 R@10 Med r

38.3
42.8
46.7

33.8
37.9
42.4
42.6

-
-
-

67.7
72.2
76.1
75.3

81.0
84.1
88.9

82.1
84.3
87.0
87.3

2
2
2

3
2
2
2

27.4
32.6
37.9

25.9
30.6
33.2
33.9

-
-
-

60.0
66.2
69.7
69.7

79.5
82.8
85.9

74.6
81.0
83.6
83.8

3
3
2

4
3
3
3

Table 5: COCO retrieval results. SkipThought is trained either using 82k training samples with VGG19
features, or with 113k samples and ResNet-101 features (our setting). We report the average results on 5
splits of 1k test images.

from models trained on supervised data in the third
group, and also report some results of supervised
methods trained directly on each task for compar-
ison with transfer learning approaches.

Comparison with SkipThought The
best
performing sentence encoder
to date is the
SkipThought-LN model, which was trained on
a very large corpora of ordered sentences. With
much less data (570k compared to 64M sentences)
but with high-quality supervision from the SNLI
dataset, we are able to consistently outperform
the results obtained by SkipThought vectors. We
train our model in less than a day on a single GPU
compared to the best SkipThought-LN network
trained for a month. Our BiLSTM-max trained
on SNLI performs much better than released
SkipThought vectors on MR, CR, MPQA, SST,
MRPC-accuracy, SICK-R, SICK-E and STS14
(see Table 4). Except for the SUBJ dataset, it
also performs better than SkipThought-LN on
MR, CR and MPQA. We also observe by looking
at the STS14 results that the cosine metrics in
our embedding space is much more semantically
informative than in SkipThought embedding
space (pearson score of 0.68 compared to 0.29
and 0.44 for ST and ST-LN). We hypothesize
that this is namely linked to the matching method
of SNLI models which incorporates a notion
of distance (element-wise product and absolute
difference) during training.

NLI as a supervised training set Our ﬁndings
indicate that our model trained on SNLI obtains
much better overall results than models trained
on other supervised tasks such as COCO, dictio-
nary deﬁnitions, NMT, PPDB (Ganitkevitch et al.,
2013) and SST. For SST, we tried exactly the same

models as for SNLI; it is worth noting that SST is
smaller than NLI. Our representations constitute
higher-quality features for both classiﬁcation and
similarity tasks. One explanation is that the natu-
ral language inference task constrains the model to
encode the semantic information of the input sen-
tence, and that the information required to perform
NLI is generally discriminative and informative.

Domain adaptation on SICK tasks Our trans-
fer learning approach obtains better results than
previous state-of-the-art on the SICK task - can
be seen as an out-domain version of SNLI - for
both entailment and relatedness. We obtain a pear-
son score of 0.885 on SICK-R while (Tai et al.,
2015) obtained 0.868, and we obtain 86.3% test
accuracy on SICK-E while previous best hand-
engineered models (Lai and Hockenmaier, 2014)
obtained 84.5%. We also signiﬁcantly outper-
formed previous transfer learning approaches on
SICK-E (Bowman et al., 2015) that used the pa-
rameters of an LSTM model trained on SNLI to
ﬁne-tune on SICK (80.8% accuracy). We hypothe-
size that our embeddings already contain the infor-
mation learned from the in-domain task, and that
learning only the classiﬁer limits the number of
parameters learned on the small out-domain task.

Image-caption retrieval results
In Table 5, we
report results for the COCO image-caption re-
trieval task. We report the mean recalls of 5 ran-
dom splits of 1K test images. When trained with
ResNet features and 30k more training data, the
SkipThought vectors perform signiﬁcantly better
than the original setting, going from 33.8 to 37.9
for caption retrieval R@1, and from 25.9 to 30.6
on image retrieval R@1. Our approach pushes
the results even further, from 37.9 to 42.4 on cap-

tion retrieval, and 30.6 to 33.2 on image retrieval.
These results are comparable to previous approach
of (Ma et al., 2015) that did not do transfer but di-
rectly learned the sentence encoding on the image-
caption retrieval task. This supports the claim that
pre-trained representations such as ResNet image
features and our sentence embeddings can achieve
competitive results compared to features learned
directly on the objective task.

MultiGenre NLI The MultiNLI
corpus
(Williams et al., 2017) was
recently released
as a multi-genre version of SNLI. With 433K
sentence pairs, MultiNLI improves upon SNLI
in its coverage:
it contains ten distinct genres
of written and spoken English, covering most
of the complexity of the language. We augment
Table 4 with our model trained on both SNLI
and MultiNLI (AllNLI). We observe a signiﬁcant
boost
in performance overall compared to the
model trained only on SLNI. Our model even
reaches AdaSent performance on CR, suggesting
that having a larger coverage for the training task
helps learn even better general representations.
On semantic textual similarity STS14, we are
also competitive with PPDB based paragram-
phrase embeddings with a pearson score of 0.70.
tasks
Interestingly, on caption-related transfer
such as the COCO image caption retrieval task,
training our sentence encoder on other genres
from MultiNLI does not degrade the performance
compared to the model trained only SNLI (which
contains mostly captions), which conﬁrms the
generalization power of our embeddings.

6 Conclusion

This paper studies the effects of training sentence
embeddings with supervised data by testing on
12 different transfer tasks. We showed that mod-
els learned on NLI can perform better than mod-
els trained in unsupervised conditions or on other
supervised tasks. By exploring various architec-
tures, we showed that a BiLSTM network with
max pooling makes the best current universal sen-
tence encoding methods, outperforming existing
approaches like SkipThought vectors.

We believe that this work only scratches the sur-
face of possible combinations of models and tasks
for learning generic sentence embeddings. Larger
datasets that rely on natural language understand-
ing for sentences could bring sentence embedding
quality to the next level.

References

Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. 2014. Semeval-2014 task 10: Multilingual
In Proceedings of the
semantic textual similarity.
8th International Workshop on Semantic Evalua-
tion (SemEval 2014). Association for Computational
Linguistics and Dublin City University, Dublin, Ire-
land, pages 81–91.

Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
garet Mitchell, Dhruv Batra, C Lawrence Zitnick,
and Devi Parikh. 2015. Vqa: Visual question an-
swering. In Proceedings of the IEEE International
Conference on Computer Vision. pages 2425–2433.

Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.
A simple but tough-to-beat baseline for sentence em-
International Conference on Learning
beddings.
Representations .

Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-

ton. 2016. Layer normalization. NIPS .

Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Research 3:1137–1155.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of EMNLP.

Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. In Eighth Workshop on Syntax, Semantics
and Structure in Statistical Translation (SSST-8).

Ronan Collobert and Jason Weston. 2008. A uniﬁed
architecture for natural language processing: Deep
In Pro-
neural networks with multitask learning.
ceedings of the 25th international conference on
Machine learning. ACM, pages 160–167.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
Journal of Machine Learning Research
scratch.
12(Aug):2493–2537.

Alexis Conneau and Douwe Kiela. 2018. Senteval: An
evaluation toolkit for universal sentence representa-
tions. LREC .

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hi-
erarchical image database. In Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Con-
ference on. IEEE, pages 248–255.

Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013.
PPDB: The paraphrase
database. In Proceedings of NAACL-HLT. Atlanta,
Georgia, pages 758–764.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Conference on Computer Vision and Pat-
tern Recognition (CVPR). IEEE, page 8.

Etai Littwin and Lior Wolf. 2016. The multiverse loss
for robust transfer learning. In Proceedings of the
IEEE Conference on Computer Vision and Pattern
Recognition. pages 3957–3966.

Felix Hill, Kyunghyun Cho, and Anna Korhonen.
Learning distributed representations of
arXiv preprint

2016.
sentences from unlabelled data.
arXiv:1602.03483 .

Yang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang.
2016. Learning natural language inference using
bidirectional lstm model and inner-attention. arXiv
preprint arXiv:1605.09090 .

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation

Long short-term memory.
9(8):1735–1780.

Miach Hodosh, Peter Young, and Julia Hockenmaier.
2013. Framing image description as a ranking task:
Data, models and evaluation metrics. Journal of Ar-
tiﬁcial Intelligence Research 47:853–899.

Yangfeng Ji and Jacob Eisenstein. 2013. Discrimina-
tive improvements to distributional sentence simi-
In Proceedings of the 2013 Conference on
larity.
Empirical Methods in Natural Language Process-
ing. pages 891–896.

Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-
semantic alignments for generating image descrip-
In Proceedings of the IEEE Conference on
tions.
Computer Vision and Pattern Recognition. pages
3128–3137.

Douwe Kiela, Alexis Conneau, Allan Jabri, and Max-
imilian Nickel. 2017. Learning visually grounded
sentence representations. Proceedings of NAACL
2018 .

Diederik P. Kingma and Jimmy Ba. 2014. Adam: A
In Proceed-
method for stochastic optimization.
ings of the 3rd International Conference on Learn-
ing Representations (ICLR).

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
In
and Sanja Fidler. 2015. Skip-thought vectors.
Advances in neural information processing systems.
pages 3294–3302.

Alice Lai and Julia Hockenmaier. 2014. Illinois-lh: A
denotational and distributional approach to seman-
tics. Proc. SemEval 2:5.

Quoc V Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In ICML.
volume 14, pages 1188–1196.

Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar,
and C Lawrence Zitnick. 2014. Microsoft coco:
In European Confer-
Common objects in context.
ence on Computer Vision. pages 740–755.

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding. International Conference on Learning
Representations .

Lin Ma, Zhengdong Lu, Lifeng Shang, and Hang Li.
2015. Multimodal convolutional neural networks
In Proceedings
for matching image and sentence.
of the IEEE International Conference on Computer
Vision. pages 2623–2631.

Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. A sick cure for the evaluation of com-
In Pro-
positional distributional semantic models.
ceedings of LREC. pages 216–223.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
In Advances in neural information processing
ity.
systems. pages 3111–3119.

Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu,
Lu Zhang, and Zhi Jin. 2016. How transferable are
neural networks in nlp applications? arXiv preprint
arXiv:1603.06111 .

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP. volume 14, pages 1532–
1543.

Ali Sharif Razavian, Hossein Azizpour, Josephine Sul-
livan, and Stefan Carlsson. 2014. Cnn features off-
the-shelf: an astounding baseline for recognition. In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition Workshops. pages
806–813.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems. pages 3104–3112.

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. Proceedings of ACL .

Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato,
and Lior Wolf. 2014. Deepface: Closing the gap
to human-level performance in face veriﬁcation. In
Conference on Computer Vision and Pattern Recog-
nition (CVPR). IEEE, page 8.

Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel
Urtasun. 2016. Order-embeddings of images and
International Conference on Learning
language.
Representations .

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2016. Towards universal paraphrastic sen-
International Conference on
tence embeddings.
Learning Representations .

Adina Williams, Nikita Nangia, and Samuel R Bow-
man. 2017. A broad-coverage challenge corpus for
sentence understanding through inference. arXiv
preprint arXiv:1704.05426 .

Han Zhao, Zhengdong Lu, and Pascal Poupart. 2015.
Self-adaptive hierarchical sentence model. In Pro-
ceedings of the 24th International Conference on Ar-
tiﬁcial Intelligence. AAAI Press, IJCAI’15, pages
4069–4076.

Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-
dinov, Raquel Urtasun, Antonio Torralba, and Sanja
Fidler. 2015. Aligning books and movies: Towards
story-like visual explanations by watching movies
In Proceedings of the IEEE
and reading books.
international conference on computer vision. pages
19–27.

Appendix

Max-pooling visualization for BiLSTM-max
trained and untrained Our
representations
were trained to focus on parts of a sentence such
that a classiﬁer can easily tell the difference be-
tween contradictory, neutral or entailed sentences.
In Table 8 and Table 9, we investigate how
the max-pooling operation selects the information
from the hidden states of the BiLSTM, for our
trained and untrained BiLSTM-max models (for
both models, word embeddings are initialized with
GloVe vectors).

For each time step t, we report the number of
times the max-pooling operation selected the hid-
den state ht (which can be seen as a sentence rep-
resentation centered around word wt).

Without any training, the max-pooling is rather
even across hidden states, although it seems to fo-
cus consistently more on the ﬁrst and last hidden
states. When trained, the model learns to focus on
speciﬁc words that carry most of the meaning of
the sentence without any explicit attention mecha-
nism.

Note that each hidden state also incorporates in-
formation from the sentence at different levels, ex-
plaining why the trained model also incorporates
information from all hidden states.

Figure 6: Pair of entailed sentences A: Visualiza-
tion of max-pooling for BiLSTM-max 4096 un-
trained.

Figure 7: Pair of entailed sentences A: Visual-
ization of max-pooling for BiLSTM-max 4096
trained on NLI.

Figure 8: Pair of entailed sentences B: Visualiza-
tion of max-pooling for BiLSTM-max 4096 un-
trained.

Figure 9: Pair of entailed sentences B: Visual-
ization of max-pooling for BiLSTM-max 4096
trained on NLI.

8
1
0
2
 
l
u
J
 
8
 
 
]
L
C
.
s
c
[
 
 
5
v
4
6
3
2
0
.
5
0
7
1
:
v
i
X
r
a

Supervised Learning of Universal Sentence Representations from
Natural Language Inference Data

Alexis Conneau
Facebook AI Research
aconneau@fb.com

Douwe Kiela
Facebook AI Research
dkiela@fb.com

Holger Schwenk
Facebook AI Research
schwenk@fb.com

Lo¨ıc Barrault
LIUM, Universit´e Le Mans
loic.barrault@univ-lemans.fr

Antoine Bordes
Facebook AI Research
abordes@fb.com

Abstract

Many modern NLP systems rely on word
embeddings, previously trained in an un-
supervised manner on large corpora, as
base features. Efforts to obtain embed-
dings for larger chunks of text, such as
sentences, have however not been so suc-
cessful. Several attempts at learning unsu-
pervised representations of sentences have
not reached satisfactory enough perfor-
mance to be widely adopted. In this paper,
we show how universal sentence represen-
tations trained using the supervised data of
the Stanford Natural Language Inference
datasets can consistently outperform unsu-
pervised methods like SkipThought vec-
tors (Kiros et al., 2015) on a wide range
of transfer tasks. Much like how com-
puter vision uses ImageNet to obtain fea-
tures, which can then be transferred to
other tasks, our work tends to indicate the
suitability of natural language inference
for transfer learning to other NLP tasks.
Our encoder is publicly available1.

1 Introduction

Distributed representations of words (or word
embeddings) (Bengio et al., 2003; Collobert et al.,
2011; Mikolov et al., 2013; Pennington et al.,
2014) have shown to provide useful features for
various tasks in natural language processing and
computer vision. While there seems to be a con-
sensus concerning the usefulness of word embed-
dings and how to learn them, this is not yet clear
with regard to representations that carry the mean-
ing of a full sentence. That is, how to capture the
relationships among multiple words and phrases in
a single vector remains an question to be solved.

In this paper, we study the task of learning
universal representations of sentences, i.e., a sen-
tence encoder model that is trained on a large cor-
pus and subsequently transferred to other tasks.
Two questions need to be solved in order to build
such an encoder, namely: what is the prefer-
able neural network architecture; and how and
on what task should such a network be trained.
Following existing work on learning word em-
beddings, most current approaches consider learn-
ing sentence encoders in an unsupervised manner
like SkipThought (Kiros et al., 2015) or FastSent
(Hill et al., 2016). Here, we investigate whether
supervised learning can be leveraged instead, tak-
ing inspiration from previous results in computer
vision, where many models are pretrained on the
ImageNet (Deng et al., 2009) before being trans-
ferred. We compare sentence embeddings trained
on various supervised tasks, and show that sen-
tence embeddings generated from models trained
on a natural language inference (NLI) task reach
the best results in terms of transfer accuracy. We
hypothesize that the suitability of NLI as a train-
ing task is caused by the fact that it is a high-level
understanding task that involves reasoning about
the semantic relationships within sentences.

Unlike in computer vision, where convolutional
neural networks are predominant, there are mul-
tiple ways to encode a sentence using neural net-
works. Hence, we investigate the impact of the
sentence encoding architecture on representational
transferability, and compare convolutional, recur-
rent and even simpler word composition schemes.
Our experiments show that an encoder based on a
bi-directional LSTM architecture with max pool-
ing,
trained on the Stanford Natural Language
Inference (SNLI) dataset (Bowman et al., 2015),
yields state-of-the-art sentence embeddings com-
pared to all existing alternative unsupervised ap-
proaches like SkipThought or FastSent, while be-

1https://www.github.com/facebookresearch/InferSent

ing much faster to train. We establish this ﬁnding
on a broad and diverse set of transfer tasks that
measures the ability of sentence representations to
capture general and useful information.

2 Related work

Transfer learning using supervised features has
been successful in several computer vision ap-
plications (Razavian et al., 2014). Striking ex-
amples include face recognition (Taigman et al.,
2014) and visual question answering (Antol et al.,
2015), where image features trained on ImageNet
(Deng et al., 2009) and word embeddings trained
on large unsupervised corpora are combined.

In contrast, most approaches for sentence repre-
sentation learning are unsupervised, arguably be-
cause the NLP community has not yet found the
best supervised task for embedding the semantics
of a whole sentence. Another reason is that neu-
ral networks are very good at capturing the bi-
ases of the task on which they are trained, but
can easily forget the overall information or seman-
tics of the input data by specializing too much
on these biases. Learning models on large un-
supervised task makes it harder for the model to
specialize. Littwin and Wolf (2016) showed that
co-adaptation of encoders and classiﬁers, when
trained end-to-end, can negatively impact the gen-
eralization power of image features generated by
an encoder. They propose a loss that incorporates
multiple orthogonal classiﬁers to counteract this
effect.

Recent work on generating sentence embed-
dings range from models that compose word
embeddings (Le and Mikolov, 2014; Arora et al.,
2017; Wieting et al., 2016) to more complex neu-
ral network architectures. SkipThought vectors
(Kiros et al., 2015) propose an objective func-
tion that adapts the skip-gram model for words
(Mikolov et al., 2013) to the sentence level. By en-
coding a sentence to predict the sentences around
it, and using the features in a linear model, they
were able to demonstrate good performance on 8
transfer tasks. They further obtained better results
using layer-norm regularization of their model in
(Ba et al., 2016). Hill et al. (2016) showed that
the task on which sentence embeddings are trained
signiﬁcantly impacts their quality.

In addition to unsupervised methods, they in-
cluded supervised training in their comparison—
namely, on machine translation data (using the

WMT’14 English/French and English/German
pairs), dictionary deﬁnitions and image captioning
data (see also Kiela et al. (2017)) from the COCO
dataset (Lin et al., 2014). These models obtained
signiﬁcantly lower results compared to the unsu-
pervised Skip-Thought approach.

Recent work has explored training sentence en-
coders on the SNLI corpus and applying them on
the SICK corpus (Marelli et al., 2014), either us-
ing multi-task learning or pretraining (Mou et al.,
2016; Bowman et al., 2015). The results were in-
conclusive and did not reach the same level as sim-
pler approaches that directly learn a classiﬁer on
top of unsupervised sentence embeddings instead
(Arora et al., 2017). To our knowledge, this work
is the ﬁrst attempt to fully exploit the SNLI cor-
pus for building generic sentence encoders. As we
show in our experiments, we are able to consis-
tently outperform unsupervised approaches, even
if our models are trained on much less (but human-
annotated) data.

3 Approach

This work combines two research directions,
which we describe in what follows. First, we ex-
plain how the NLI task can be used to train univer-
sal sentence encoding models using the SNLI task.
We subsequently describe the architectures that we
investigated for the sentence encoder, which, in
our opinion, covers a suitable range of sentence
encoders currently in use. Speciﬁcally, we exam-
ine standard recurrent models such as LSTMs and
GRUs, for which we investigate mean and max-
pooling over the hidden representations; a self-
attentive network that incorporates different views
of the sentence; and a hierarchical convolutional
network that can be seen as a tree-based method
that blends different levels of abstraction.

3.1 The Natural Language Inference task

The SNLI dataset consists of 570k human-
generated English sentence pairs, manually la-
beled with one of three categories: entailment,
contradiction and neutral. It captures natural lan-
guage inference, also known in previous incarna-
tions as Recognizing Textual Entailment (RTE),
and constitutes one of the largest high-quality la-
beled resources explicitly constructed in order to
require understanding sentence semantics. We hy-
pothesize that the semantic nature of NLI makes
it a good candidate for learning universal sentence

embeddings in a supervised way. That is, we aim
to demonstrate that sentence encoders trained on
natural language inference are able to learn sen-
tence representations that capture universally use-
ful features.

3-way softmax

fully-connected layers

(u, v, |u − v|, u ∗ v)

u

v

sentence encoder
with premise input

sentence encoder
with hypothesis input

Figure 1: Generic NLI training scheme.

Models can be trained on SNLI in two differ-
ent ways: (i) sentence encoding-based models that
explicitly separate the encoding of the individual
sentences and (ii) joint methods that allow to use
encoding of both sentences (to use cross-features
or attention from one sentence to the other).

Since our goal is to train a generic sentence en-
coder, we adopt the ﬁrst setting. As illustrated in
Figure 1, a typical architecture of this kind uses a
shared sentence encoder that outputs a representa-
tion for the premise u and the hypothesis v. Once
the sentence vectors are generated, 3 matching
methods are applied to extract relations between
u and v : (i) concatenation of the two representa-
tions (u, v); (ii) element-wise product u ∗ v; and
(iii) absolute element-wise difference |u − v|. The
resulting vector, which captures information from
both the premise and the hypothesis, is fed into
a 3-class classiﬁer consisting of multiple fully-
connected layers culminating in a softmax layer.

3.2 Sentence encoder architectures

A wide variety of neural networks for encod-
ing sentences into ﬁxed-size representations ex-
ists, and it is not yet clear which one best cap-
tures generically useful information. We com-
pare 7 different architectures: standard recurrent
encoders with either Long Short-Term Memory
(LSTM) or Gated Recurrent Units (GRU), con-
catenation of last hidden states of forward and
backward GRU, Bi-directional LSTMs (BiLSTM)
with either mean or max pooling, self-attentive

network and hierarchical convolutional networks.

3.2.1 LSTM and GRU
Our ﬁrst, and simplest,
encoders apply re-
current neural networks using either LSTM
(Hochreiter and Schmidhuber, 1997) or GRU
(Cho et al., 2014) modules, as in sequence to se-
quence encoders (Sutskever et al., 2014).
For
a sequence of T words (w1, . . . , wT ), the net-
work computes a set of T hidden representations
−−−−→
h1, . . . , hT , with ht =
LSTM(w1, . . . , wT ) (or
using GRU units instead). A sentence is repre-
sented by the last hidden vector, hT .

We also consider a model BiGRU-last that con-
catenates the last hidden state of a forward GRU,
and the last hidden state of a backward GRU to
have the same architecture as for SkipThought
vectors.

3.2.2 BiLSTM with mean/max pooling
For a sequence of T words {wt}t=1,...,T , a bidirec-
tional LSTM computes a set of T vectors {ht}t.
For t ∈ [1, . . . , T ], ht, is the concatenation of a
forward LSTM and a backward LSTM that read
the sentences in two opposite directions:

−→
ht =
←−
ht =

ht = [

−−−−→
LSTMt(w1, . . . , wT )
←−−−−
LSTMt(w1, . . . , wT )
−→
ht ,

←−
ht ]

We experiment with two ways of combining the
varying number of {ht}t to form a ﬁxed-size vec-
tor, either by selecting the maximum value over
each dimension of the hidden units (max pool-
ing) (Collobert and Weston, 2008) or by consider-
ing the average of the representations (mean pool-
ing).

u :

x x

… …
x

x

max-pooling

x

←−
h1

−→
h1

x

←−
h2

−→
h2

x

←−
h3

−→
h3

x

←−
h4

−→
h4

w1

The

w2

movie

w3

was

w4

great

Figure 2: Bi-LSTM max-pooling network.

3.2.3 Self-attentive network
The self-attentive sentence encoder (Liu et al.,
2016; Lin et al., 2017) uses an attention mecha-
nism over the hidden states of a BiLSTM to gen-
erate a representation u of an input sentence. The
attention mechanism is deﬁned as :

representation ui is computed by a max-pooling
operation over the feature maps (see Figure 4).

u :

u1

u2

u3

u4

αi =

¯hi = tanh(W hi + bw)
¯hT
i uw
e
¯hT
i uw
Pi e
u = X
t

αihi

where {h1, . . . , hT } are the output hidden vec-
tors of a BiLSTM. These are fed to an afﬁne trans-
formation (W , bw) which outputs a set of keys
(¯h1, . . . , ¯hT ). The {αi} represent the score of
similarity between the keys and a learned con-
text query vector uw. These weights are used
to produce the ﬁnal representation u, which is a
weighted linear combination of the hidden vectors.
Following Lin et al. (2017) we use a self-
attentive network with multiple views of the input
sentence, so that the model can learn which part of
the sentence is important for the given task. Con-
w, u4
cretely, we have 4 context vectors u1
w
which generate 4 representations that are then con-
catenated to obtain the sentence representation u.
Figure 3 illustrates this architecture.

w, u2

w, u3

α1

α2

uw

u

←−
h2

−→
h2

α3

←−
h3

−→
h3

α4

←−
h4

−→
h4

w2

movie

w3

was

w4

great

←−
h1

−→
h1

w1

The

Figure 3: Inner Attention network architecture.

3.2.4 Hierarchical ConvNet
One of the currently best performing models on
classiﬁcation tasks is a convolutional architecture
termed AdaSent (Zhao et al., 2015), which con-
catenates different representations of the sentences
at different level of abstractions. Inspired by this
architecture, we introduce a faster version consist-
ing of 4 convolutional layers. At every layer, a

convolutional layer

…

convolutional layer

…

convolutional layer

…

max-pooling

u4

max-pooling

u3

max-pooling

u2

…

…

…

x

x
x

x

x

x

max-pooling

x u1

x
x
x

x

x

convolutional layer

This

is

the

movie

of

all

time

great
est

Figure 4: Hierarchical ConvNet architecture.

The ﬁnal representation u = [u1, u2, u3, u4]
concatenates representations at different levels of
the input sentence. The model thus captures hi-
erarchical abstractions of an input sentence in a
ﬁxed-size representation.

3.3 Training details

For all our models trained on SNLI, we use SGD
with a learning rate of 0.1 and a weight decay of
0.99. At each epoch, we divide the learning rate
by 5 if the dev accuracy decreases. We use mini-
batches of size 64 and training is stopped when the
learning rate goes under the threshold of 10−5. For
the classiﬁer, we use a multi-layer perceptron with
1 hidden-layer of 512 hidden units. We use open-
source GloVe vectors trained on Common Crawl
840B with 300 dimensions as ﬁxed word embed-
dings.

4 Evaluation of sentence representations

Our aim is to obtain general-purpose sentence
embeddings that capture generic information that
is useful for a broad set of tasks. To evalu-
ate the quality of these representations, we use
them as features in 12 transfer tasks. We present
our sentence-embedding evaluation procedure in

N task

name
11k
MR
4k
CR
SUBJ
10k
MPQA 11k
6k
TREC
70k
SST

sentiment (movies)
product reviews
subjectivity/objectivity
opinion polarity
question-type
sentiment (movies)

C examples
2
2
2
2
6
2

”Too slow for a younger crowd , too shallow for an older one.” (neg)
”We tried it out christmas night and it worked great .” (pos)
”A movie that doesn’t aim too high , but doesn’t need to.” (subj)
”don’t want”; ”would like to tell”; (neg, pos)
”What are the twin cities ?” (LOC:city)
”Audrey Tautou has a knack for picking roles that magnify her [..]” (pos)

Table 1: Classiﬁcation tasks. C is the number of class and N is the number of samples.

this section. We constructed a sentence evalu-
ation tool2 called SentEval (Conneau and Kiela,
2018) to automate evaluation on all
the tasks
mentioned in this paper. The tool uses Adam
(Kingma and Ba, 2014) to ﬁt a logistic regression
classiﬁer, with batch size 64.

Binary and multi-class classiﬁcation We use
a set of binary classiﬁcation tasks (see Table 1)
that covers various types of sentence classiﬁca-
tion,
including sentiment analysis (MR, SST),
question-type (TREC), product reviews (CR), sub-
jectivity/objectivity (SUBJ) and opinion polarity
(MPQA). We generate sentence vectors and train
a logistic regression on top. A linear classiﬁer re-
quires fewer parameters than an MLP and is thus
suitable for small datasets, where transfer learning
is especially well-suited. We tune the L2 penalty
of the logistic regression with grid-search on the
validation set.

Entailment and semantic relatedness We also
evaluate on the SICK dataset for both entailment
(SICK-E) and semantic relatedness (SICK-R). We
use the same matching methods as in SNLI and
learn a Logistic Regression on top of the joint rep-
resentation. For semantic relatedness evaluation,
we follow the approach of (Tai et al., 2015) and
learn to predict the probability distribution of re-
latedness scores. We report Pearson correlation.

STS14 - Semantic Textual Similarity While
semantic relatedness is supervised in the case
of SICK-R, we also evaluate our embeddings
on the 6 unsupervised SemEval tasks of STS14
(Agirre et al., 2014). This dataset includes sub-
sets of news articles, forum discussions, image de-
scriptions and headlines from news articles con-
taining pairs of sentences (lower-cased), labeled
with a similarity score between 0 and 5. These
tasks evaluate how the cosine distance between
two sentences correlate with a human-labeled sim-
ilarity score through Pearson and Spearman corre-

lations.

Paraphrase detection The Microsoft Research
Paraphrase Corpus is composed of pairs of sen-
tences which have been extracted from news
sources on the Web. Sentence pairs have been
human-annotated according to whether they cap-
ture a paraphrase/semantic equivalence relation-
ship. We use the same approach as with SICK-E,
except that our classiﬁer has only 2 classes.

Caption-Image retrieval The caption-image
retrieval task evaluates joint image and language
feature models (Hodosh et al., 2013; Lin et al.,
2014). The goal is either to rank a large collec-
tion of images by their relevance with respect to a
given query caption (Image Retrieval), or ranking
captions by their relevance for a given query image
(Caption Retrieval). We use a pairwise ranking-
loss Lcir(x, y):

max(0, α − s(V y, U x) + s(V y, U xk)) +

max(0, α − s(U x, V y) + s(U x, V yk′))

X
y

X
k

X
x

X
k′

where (x, y) consists of an image y with one
of its associated captions x, (yk)k and (yk′)k′ are
negative examples of the ranking loss, α is the
margin and s corresponds to the cosine similarity.
U and V are learned linear transformations that
project the caption x and the image y to the same
embedding space. We use a margin α = 0.2 and
30 contrastive terms. We use the same splits as
in (Karpathy and Fei-Fei, 2015), i.e., we use 113k
images from the COCO dataset (each containing
5 captions) for training, 5k images for validation
and 5k images for test. For evaluation, we split the
5k images in 5 random sets of 1k images on which
we compute Recall@K, with K ∈ {1, 5, 10} and
median (Med r) over the 5 splits. For fair compari-
son, we also report SkipThought results in our set-
ting, using 2048-dimensional pretrained ResNet-
101 (He et al., 2016) with 113k training images.

2https://www.github.com/facebookresearch/SentEval

name
SNLI

task
NLI

N premise

560k

”Two women are embracing while
holding to go packages.”

SICK-E NLI

10k A man is typing on a machine used

SICK-R

STS

10k

STS14

STS

4.5k

for stenography
”A man is singing a song and play-
ing the guitar”
”Liquid ammonia leak kills 15 in
Shanghai”

The man isn’t operating a steno-
graph
”A man is opening a package that
contains headphones”
”Liquid ammonia leak kills at least
15 in Shanghai”

contradiction

1.6

4.6

hypothesis
”Two woman are holding packages.”

label
entailment

Table 2: Natural Language Inference and Semantic Textual Similarity tasks. NLI labels are contra-
diction, neutral and entailment. STS labels are scores between 0 and 5.

Model

LSTM
GRU
BiGRU-last
BiLSTM-Mean
Inner-attention
HConvNet
BiLSTM-Max

dim
2048
4096
4096
4096
4096
4096
4096

NLI

Transfer

dev
81.9
82.4
81.3
79.0
82.3
83.7
85.0

test micro macro
78.6
80.7
80.9
81.8
81.7
80.9
81.7
78.2
81.0
82.5
80.9
83.4
83.7
84.5

79.5
81.7
82.9
83.1
82.1
82.0
85.2

Table 3: Performance of sentence encoder ar-
chitectures on SNLI and (aggregated) transfer
tasks. Dimensions of embeddings were selected
according to best aggregated scores (see Figure 5).

Figure 5: Transfer performance w.r.t. embed-
ding size using the micro aggregation method.

5 Empirical results

In this section, we refer to ”micro” and ”macro”
averages of development set (dev) results on trans-
fer tasks whose metrics is accuracy: we compute a
”macro” aggregated score that corresponds to the
classical average of dev accuracies, and the ”mi-
cro” score that is a sum of the dev accuracies,
weighted by the number of dev samples.

5.1 Architecture impact

Model We observe in Table 3 that different mod-
els trained on the same NLI corpus lead to differ-
ent transfer tasks results. The BiLSTM-4096 with
the max-pooling operation performs best on both
SNLI and transfer tasks. Looking at the micro and

macro averages, we see that it performs signiﬁ-
cantly better than the other models LSTM, GRU,
BiGRU-last, BiLSTM-Mean, inner-attention and
the hierarchical-ConvNet.

Table 3 also shows that better performance on
the training task does not necessarily translate in
better results on the transfer tasks like when com-
paring inner-attention and BiLSTM-Mean for in-
stance.

We hypothesize that some models are likely to
over-specialize and adapt too well to the biases of
a dataset without capturing general-purpose infor-
mation of the input sentence. For example, the
inner-attention model has the ability to focus only
on certain parts of a sentence that are useful for
the SNLI task, but not necessarily for the transfer
tasks. On the other hand, BiLSTM-Mean does not
make sharp choices on which part of the sentence
is more important than others. The difference be-
tween the results seems to come from the different
abilities of the models to incorporate general in-
formation while not focusing too much on speciﬁc
features useful for the task at hand.

For a given model, the transfer quality is also
sensitive to the optimization algorithm: when
training with Adam instead of SGD, we observed
that the BiLSTM-max converged faster on SNLI
(5 epochs instead of 10), but obtained worse re-
sults on the transfer tasks, most likely because of
the model and classiﬁer’s increased capability to
over-specialize on the training task.

Embedding size Figure 5 compares the over-
all performance of different architectures, showing
the evolution of micro averaged performance with
regard to the embedding size.

Since it is easier to linearly separate in high di-
mension, especially with logistic regression, it is
not surprising that increased embedding sizes lead
to increased performance for almost all models.
However, this is particularly true for some mod-

Model

MR CR SUBJ MPQA SST TREC MRPC

SICK-R SICK-E STS14

79.2
66.9
78.0
-
79.8
81.0
78.5
77.4
81.3

Unsupervised representation training (unordered sentences)
82.4
73.7
Unigram-TFIDF
70.7
60.2
ParagraphVec (DBOW)
86.9
74.6
SDAE
SIF (GloVe + WR)
-
-
word2vec BOW†
88.3
77.7
fastText BOW†
87.8
78.3
GloVe BOW†
78.7
87.6
GloVe Positional Encoding† 78.3
87.1
BiLSTM-Max (untrained)†
88.7
77.5
Unsupervised representation training (ordered sentences)
80.6
81.5
87.1
89.3

FastSent
FastSent+AE
SkipThought
SkipThought-LN

90.3
76.3
90.8
-
90.9
92.4
91.6
91.1
89.6

88.7
88.8
93.6
93.7

78.4
76.7
80.1
83.1

70.8
71.8
76.5
79.4

85.0
59.4
78.4
-

-
-
-
82.2
79.7 83.6
81.9 84.8
79.8 83.6
80.6 83.3
80.7 85.8

-
-

76.8
80.4
82.0 92.2
82.9 88.4

Supervised representation training

70.8
87.2
81.5
-
89.5
89.8
90.2

77.4
90.7
84.9
-
90.2
92.1
92.4

69.3
78.7
70.1
-
83.7
84.6
86.3

CaptionRep (bow)
61.9
76.7
DictRep (bow)
NMT En-to-Fr
64.7
Paragram-phrase
-
BiLSTM-Max (on SST)†
(*)
BiLSTM-Max (on SNLI)†
79.9
BiLSTM-Max (on AllNLI)† 81.1
Supervised methods (directly trained for each task – no transfer)
83.1
-
-
-
-

Naive Bayes - SVM
AdaSent
TF-KLD
Illinois-LH
Dependency Tree-LSTM

86.3
93.3
-
-
-

93.2
95.5
-
-
-

79.4
83.1
-
-
-

81.8
86.3
-
-
-

72.2
-
81.0
-
82.8
-
-
79.7
(*)
86.0
83.3 88.7
84.6 88.2

-
92.4
-
-
-

73.6/81.7
72.9/81.1
73.7/80.7
-
72.5/81.4
73.9/82.0
72.1/80.9
72.5/81.2
73.2/81.6

72.2/80.3
71.2/79.1
73.0/82.0
-

73.6/81.9
68.4/76.8
69.1/77.1
-
72.7/80.9
75.1/82.3
76.2/83.1

-
-
80.4/85.9
-
-

-
-
-
-
0.803
0.815
0.800
0.799
0.860

-
-
0.858
0.858

-
-
-
0.849
0.863
0.885
0.884

-
-
-
-
0.868

-
-
-
84.6
78.7
78.3
78.6
77.9
83.4

-
-
82.3
79.5

-
-

83.1
83.1
86.3
86.3

-
-
-
84.5
-

.58/.57
.42/.43
.37/.38
.69/ -
.65/.64
.63/.62
.54/.56
.51/.54
.39/.48

.63/.64
.62/.62
.29/.35
.44/.45

.46/.42
.67/.70
.43/.42
.71/ -
.55/.54
.68/.65
.70/.67

-
-
-
-
-

Table 4: Transfer test results for various architectures trained in different ways. Underlined
are best results for transfer learning approaches, in bold are best results among the models trained
† indicates methods that we trained, other transfer models have been extracted
in the same way.
from (Hill et al., 2016). For best published supervised methods (no transfer), we consider AdaSent
(Zhao et al., 2015), TF-KLD (Ji and Eisenstein, 2013), Tree-LSTM (Tai et al., 2015) and Illinois-LH
system (Lai and Hockenmaier, 2014). (*) Our model trained on SST obtained 83.4 for MR and 86.0 for
SST (MR and SST come from the same source), which we do not put in the tables for fair comparison
with transfer methods.

els (BiLSTM-Max, HConvNet, inner-att), which
demonstrate unequal abilities to incorporate more
information as the size grows. We hypothesize
that such networks are able to incorporate infor-
mation that is not directly relevant to the objective
task (results on SNLI are relatively stable with re-
gard to embedding size) but that can nevertheless
be useful as features for transfer tasks.

5.2 Task transfer

We report in Table 4 transfer tasks results for
different architectures trained in different ways.
We group models by the nature of the data
on which they were trained. The ﬁrst group

corresponds to models trained with unsuper-
vised unordered sentences. This includes bag-
of-words models such as word2vec-SkipGram,
the Unigram-TFIDF model, the Paragraph Vector
model (Le and Mikolov, 2014), the Sequential De-
noising Auto-Encoder (SDAE) (Hill et al., 2016)
and the SIF model (Arora et al., 2017), all trained
on the Toronto book corpus (Zhu et al., 2015). The
second group consists of models trained with un-
supervised ordered sentences such as FastSent and
SkipThought (also trained on the Toronto book
corpus). We also include the FastSent variant
“FastSent+AE” and the SkipThought-LN version
that uses layer normalization. We report results

Model
Direct supervision of sentence representations
m-CNN
m-CNNENS
Order-embeddings
Pre-trained sentence representations
+ VGG19 (82k)
SkipThought
SkipThought
+ ResNet101 (113k)
BiLSTM-Max (on SNLI) + ResNet101 (113k)
BiLSTM-Max (on AllNLI) + ResNet101 (113k)

(Ma et al., 2015)
(Ma et al., 2015)
(Vendrov et al., 2016)

Caption Retrieval

Image Retrieval

R@1 R@5 R@10 Med r R@1 R@5 R@10 Med r

38.3
42.8
46.7

33.8
37.9
42.4
42.6

-
-
-

67.7
72.2
76.1
75.3

81.0
84.1
88.9

82.1
84.3
87.0
87.3

2
2
2

3
2
2
2

27.4
32.6
37.9

25.9
30.6
33.2
33.9

-
-
-

60.0
66.2
69.7
69.7

79.5
82.8
85.9

74.6
81.0
83.6
83.8

3
3
2

4
3
3
3

Table 5: COCO retrieval results. SkipThought is trained either using 82k training samples with VGG19
features, or with 113k samples and ResNet-101 features (our setting). We report the average results on 5
splits of 1k test images.

from models trained on supervised data in the third
group, and also report some results of supervised
methods trained directly on each task for compar-
ison with transfer learning approaches.

Comparison with SkipThought The
best
performing sentence encoder
to date is the
SkipThought-LN model, which was trained on
a very large corpora of ordered sentences. With
much less data (570k compared to 64M sentences)
but with high-quality supervision from the SNLI
dataset, we are able to consistently outperform
the results obtained by SkipThought vectors. We
train our model in less than a day on a single GPU
compared to the best SkipThought-LN network
trained for a month. Our BiLSTM-max trained
on SNLI performs much better than released
SkipThought vectors on MR, CR, MPQA, SST,
MRPC-accuracy, SICK-R, SICK-E and STS14
(see Table 4). Except for the SUBJ dataset, it
also performs better than SkipThought-LN on
MR, CR and MPQA. We also observe by looking
at the STS14 results that the cosine metrics in
our embedding space is much more semantically
informative than in SkipThought embedding
space (pearson score of 0.68 compared to 0.29
and 0.44 for ST and ST-LN). We hypothesize
that this is namely linked to the matching method
of SNLI models which incorporates a notion
of distance (element-wise product and absolute
difference) during training.

NLI as a supervised training set Our ﬁndings
indicate that our model trained on SNLI obtains
much better overall results than models trained
on other supervised tasks such as COCO, dictio-
nary deﬁnitions, NMT, PPDB (Ganitkevitch et al.,
2013) and SST. For SST, we tried exactly the same

models as for SNLI; it is worth noting that SST is
smaller than NLI. Our representations constitute
higher-quality features for both classiﬁcation and
similarity tasks. One explanation is that the natu-
ral language inference task constrains the model to
encode the semantic information of the input sen-
tence, and that the information required to perform
NLI is generally discriminative and informative.

Domain adaptation on SICK tasks Our trans-
fer learning approach obtains better results than
previous state-of-the-art on the SICK task - can
be seen as an out-domain version of SNLI - for
both entailment and relatedness. We obtain a pear-
son score of 0.885 on SICK-R while (Tai et al.,
2015) obtained 0.868, and we obtain 86.3% test
accuracy on SICK-E while previous best hand-
engineered models (Lai and Hockenmaier, 2014)
obtained 84.5%. We also signiﬁcantly outper-
formed previous transfer learning approaches on
SICK-E (Bowman et al., 2015) that used the pa-
rameters of an LSTM model trained on SNLI to
ﬁne-tune on SICK (80.8% accuracy). We hypothe-
size that our embeddings already contain the infor-
mation learned from the in-domain task, and that
learning only the classiﬁer limits the number of
parameters learned on the small out-domain task.

Image-caption retrieval results
In Table 5, we
report results for the COCO image-caption re-
trieval task. We report the mean recalls of 5 ran-
dom splits of 1K test images. When trained with
ResNet features and 30k more training data, the
SkipThought vectors perform signiﬁcantly better
than the original setting, going from 33.8 to 37.9
for caption retrieval R@1, and from 25.9 to 30.6
on image retrieval R@1. Our approach pushes
the results even further, from 37.9 to 42.4 on cap-

tion retrieval, and 30.6 to 33.2 on image retrieval.
These results are comparable to previous approach
of (Ma et al., 2015) that did not do transfer but di-
rectly learned the sentence encoding on the image-
caption retrieval task. This supports the claim that
pre-trained representations such as ResNet image
features and our sentence embeddings can achieve
competitive results compared to features learned
directly on the objective task.

MultiGenre NLI The MultiNLI
corpus
(Williams et al., 2017) was
recently released
as a multi-genre version of SNLI. With 433K
sentence pairs, MultiNLI improves upon SNLI
in its coverage:
it contains ten distinct genres
of written and spoken English, covering most
of the complexity of the language. We augment
Table 4 with our model trained on both SNLI
and MultiNLI (AllNLI). We observe a signiﬁcant
boost
in performance overall compared to the
model trained only on SLNI. Our model even
reaches AdaSent performance on CR, suggesting
that having a larger coverage for the training task
helps learn even better general representations.
On semantic textual similarity STS14, we are
also competitive with PPDB based paragram-
phrase embeddings with a pearson score of 0.70.
tasks
Interestingly, on caption-related transfer
such as the COCO image caption retrieval task,
training our sentence encoder on other genres
from MultiNLI does not degrade the performance
compared to the model trained only SNLI (which
contains mostly captions), which conﬁrms the
generalization power of our embeddings.

6 Conclusion

This paper studies the effects of training sentence
embeddings with supervised data by testing on
12 different transfer tasks. We showed that mod-
els learned on NLI can perform better than mod-
els trained in unsupervised conditions or on other
supervised tasks. By exploring various architec-
tures, we showed that a BiLSTM network with
max pooling makes the best current universal sen-
tence encoding methods, outperforming existing
approaches like SkipThought vectors.

We believe that this work only scratches the sur-
face of possible combinations of models and tasks
for learning generic sentence embeddings. Larger
datasets that rely on natural language understand-
ing for sentences could bring sentence embedding
quality to the next level.

References

Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. 2014. Semeval-2014 task 10: Multilingual
In Proceedings of the
semantic textual similarity.
8th International Workshop on Semantic Evalua-
tion (SemEval 2014). Association for Computational
Linguistics and Dublin City University, Dublin, Ire-
land, pages 81–91.

Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
garet Mitchell, Dhruv Batra, C Lawrence Zitnick,
and Devi Parikh. 2015. Vqa: Visual question an-
swering. In Proceedings of the IEEE International
Conference on Computer Vision. pages 2425–2433.

Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.
A simple but tough-to-beat baseline for sentence em-
International Conference on Learning
beddings.
Representations .

Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-

ton. 2016. Layer normalization. NIPS .

Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Research 3:1137–1155.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of EMNLP.

Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. In Eighth Workshop on Syntax, Semantics
and Structure in Statistical Translation (SSST-8).

Ronan Collobert and Jason Weston. 2008. A uniﬁed
architecture for natural language processing: Deep
In Pro-
neural networks with multitask learning.
ceedings of the 25th international conference on
Machine learning. ACM, pages 160–167.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
Journal of Machine Learning Research
scratch.
12(Aug):2493–2537.

Alexis Conneau and Douwe Kiela. 2018. Senteval: An
evaluation toolkit for universal sentence representa-
tions. LREC .

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hi-
erarchical image database. In Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Con-
ference on. IEEE, pages 248–255.

Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013.
PPDB: The paraphrase
database. In Proceedings of NAACL-HLT. Atlanta,
Georgia, pages 758–764.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Conference on Computer Vision and Pat-
tern Recognition (CVPR). IEEE, page 8.

Etai Littwin and Lior Wolf. 2016. The multiverse loss
for robust transfer learning. In Proceedings of the
IEEE Conference on Computer Vision and Pattern
Recognition. pages 3957–3966.

Felix Hill, Kyunghyun Cho, and Anna Korhonen.
Learning distributed representations of
arXiv preprint

2016.
sentences from unlabelled data.
arXiv:1602.03483 .

Yang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang.
2016. Learning natural language inference using
bidirectional lstm model and inner-attention. arXiv
preprint arXiv:1605.09090 .

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation

Long short-term memory.
9(8):1735–1780.

Miach Hodosh, Peter Young, and Julia Hockenmaier.
2013. Framing image description as a ranking task:
Data, models and evaluation metrics. Journal of Ar-
tiﬁcial Intelligence Research 47:853–899.

Yangfeng Ji and Jacob Eisenstein. 2013. Discrimina-
tive improvements to distributional sentence simi-
In Proceedings of the 2013 Conference on
larity.
Empirical Methods in Natural Language Process-
ing. pages 891–896.

Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-
semantic alignments for generating image descrip-
In Proceedings of the IEEE Conference on
tions.
Computer Vision and Pattern Recognition. pages
3128–3137.

Douwe Kiela, Alexis Conneau, Allan Jabri, and Max-
imilian Nickel. 2017. Learning visually grounded
sentence representations. Proceedings of NAACL
2018 .

Diederik P. Kingma and Jimmy Ba. 2014. Adam: A
In Proceed-
method for stochastic optimization.
ings of the 3rd International Conference on Learn-
ing Representations (ICLR).

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
In
and Sanja Fidler. 2015. Skip-thought vectors.
Advances in neural information processing systems.
pages 3294–3302.

Alice Lai and Julia Hockenmaier. 2014. Illinois-lh: A
denotational and distributional approach to seman-
tics. Proc. SemEval 2:5.

Quoc V Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In ICML.
volume 14, pages 1188–1196.

Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar,
and C Lawrence Zitnick. 2014. Microsoft coco:
In European Confer-
Common objects in context.
ence on Computer Vision. pages 740–755.

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding. International Conference on Learning
Representations .

Lin Ma, Zhengdong Lu, Lifeng Shang, and Hang Li.
2015. Multimodal convolutional neural networks
In Proceedings
for matching image and sentence.
of the IEEE International Conference on Computer
Vision. pages 2623–2631.

Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. A sick cure for the evaluation of com-
In Pro-
positional distributional semantic models.
ceedings of LREC. pages 216–223.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
In Advances in neural information processing
ity.
systems. pages 3111–3119.

Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu,
Lu Zhang, and Zhi Jin. 2016. How transferable are
neural networks in nlp applications? arXiv preprint
arXiv:1603.06111 .

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP. volume 14, pages 1532–
1543.

Ali Sharif Razavian, Hossein Azizpour, Josephine Sul-
livan, and Stefan Carlsson. 2014. Cnn features off-
the-shelf: an astounding baseline for recognition. In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition Workshops. pages
806–813.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems. pages 3104–3112.

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. Proceedings of ACL .

Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato,
and Lior Wolf. 2014. Deepface: Closing the gap
to human-level performance in face veriﬁcation. In
Conference on Computer Vision and Pattern Recog-
nition (CVPR). IEEE, page 8.

Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel
Urtasun. 2016. Order-embeddings of images and
International Conference on Learning
language.
Representations .

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2016. Towards universal paraphrastic sen-
International Conference on
tence embeddings.
Learning Representations .

Adina Williams, Nikita Nangia, and Samuel R Bow-
man. 2017. A broad-coverage challenge corpus for
sentence understanding through inference. arXiv
preprint arXiv:1704.05426 .

Han Zhao, Zhengdong Lu, and Pascal Poupart. 2015.
Self-adaptive hierarchical sentence model. In Pro-
ceedings of the 24th International Conference on Ar-
tiﬁcial Intelligence. AAAI Press, IJCAI’15, pages
4069–4076.

Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-
dinov, Raquel Urtasun, Antonio Torralba, and Sanja
Fidler. 2015. Aligning books and movies: Towards
story-like visual explanations by watching movies
In Proceedings of the IEEE
and reading books.
international conference on computer vision. pages
19–27.

Appendix

Max-pooling visualization for BiLSTM-max
trained and untrained Our
representations
were trained to focus on parts of a sentence such
that a classiﬁer can easily tell the difference be-
tween contradictory, neutral or entailed sentences.
In Table 8 and Table 9, we investigate how
the max-pooling operation selects the information
from the hidden states of the BiLSTM, for our
trained and untrained BiLSTM-max models (for
both models, word embeddings are initialized with
GloVe vectors).

For each time step t, we report the number of
times the max-pooling operation selected the hid-
den state ht (which can be seen as a sentence rep-
resentation centered around word wt).

Without any training, the max-pooling is rather
even across hidden states, although it seems to fo-
cus consistently more on the ﬁrst and last hidden
states. When trained, the model learns to focus on
speciﬁc words that carry most of the meaning of
the sentence without any explicit attention mecha-
nism.

Note that each hidden state also incorporates in-
formation from the sentence at different levels, ex-
plaining why the trained model also incorporates
information from all hidden states.

Figure 6: Pair of entailed sentences A: Visualiza-
tion of max-pooling for BiLSTM-max 4096 un-
trained.

Figure 7: Pair of entailed sentences A: Visual-
ization of max-pooling for BiLSTM-max 4096
trained on NLI.

Figure 8: Pair of entailed sentences B: Visualiza-
tion of max-pooling for BiLSTM-max 4096 un-
trained.

Figure 9: Pair of entailed sentences B: Visual-
ization of max-pooling for BiLSTM-max 4096
trained on NLI.

