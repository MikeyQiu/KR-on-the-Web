Riemannian Optimization for Skip-Gram Negative Sampling

Alexander Fonarev1,2,4,*, Oleksii Hrinchuk1,2,3,*,
Gleb Gusev2,3, Pavel Serdyukov2, and Ivan Oseledets1,5

1Skolkovo Institute of Science and Technology, Moscow, Russia
2Yandex LLC, Moscow, Russia
3Moscow Institute of Physics and Technology, Moscow, Russia
4SBDA Group, Dublin, Ireland
5Institute of Numerical Mathematics, Russian Academy of Sciences, Moscow, Russia

Abstract

Skip-Gram Negative Sampling (SGNS)
word embedding model, well known by its
implementation in “word2vec” software,
is usually optimized by stochastic gradi-
ent descent. However, the optimization of
SGNS objective can be viewed as a prob-
lem of searching for a good matrix with
the low-rank constraint. The most stan-
dard way to solve this type of problems is
to apply Riemannian optimization frame-
work to optimize the SGNS objective over
the manifold of required low-rank matri-
ces.
In this paper, we propose an algo-
rithm that optimizes SGNS objective us-
ing Riemannian optimization and demon-
strates its superiority over popular com-
petitors, such as the original method to
train SGNS and SVD over SPPMI matrix.

1

Introduction

In this paper, we consider the problem of embed-
ding words into a low-dimensional space in order
to measure the semantic similarity between them.
As an example, how to ﬁnd whether the word
“table” is semantically more similar to the word
“stool” than to the word “sky”? That is achieved
by constructing a low-dimensional vector repre-
sentation for each word and measuring similarity
between the words as the similarity between the
corresponding vectors.

One of the most popular word embedding mod-
els (Mikolov et al., 2013) is a discriminative neu-
ral network that optimizes Skip-Gram Negative
Sampling (SGNS) objective (see Equation 3). It
aims at predicting whether two words can be found
close to each other within a text. As shown in Sec-
tion 2, the process of word embeddings training

∗The ﬁrst two authors contributed equally to this work

using SGNS can be divided into two general steps
with clear objectives:

Step 1. Search for a low-rank matrix X that pro-
vides a good SGNS objective value;

Step 2. Search for a good low-rank representation
X = W C(cid:62) in terms of linguistic metrics,
where W is a matrix of word embeddings
and C is a matrix of so-called context em-
beddings.

Unfortunately, most previous approaches mixed
these two steps into a single one, what entails a not
completely correct formulation of the optimization
problem. For example, popular approaches to train
embeddings (including the original “word2vec”
implementation) do not take into account that the
objective from Step 1 depends only on the prod-
uct X = W C(cid:62): instead of straightforward com-
puting of the derivative w.r.t. X, these methods
are explicitly based on the derivatives w.r.t. W
and C, what complicates the optimization proce-
dure. Moreover, such approaches do not take into
account that parametrization W C(cid:62) of matrix X is
non-unique and Step 2 is required. Indeed, for any
invertible matrix S, we have

X = W1C(cid:62)

1 = W1SS−1C(cid:62)

1 = W2C(cid:62)
2 ,

1 and W2C(cid:62)

therefore, solutions W1C(cid:62)
2 are equally
good in terms of the SGNS objective but entail
different cosine similarities between embeddings
and, as a result, different performance in terms of
linguistic metrics (see Section 4.2 for details).

A successful attempt to follow the above de-
scribed steps, which outperforms the original
SGNS optimization approach in terms of various
linguistic tasks, was proposed in (Levy and Gold-
berg, 2014). In order to obtain a low-rank matrix
X on Step 1, the method reduces the dimensional-
ity of Shifted Positive Pointwise Mutual Informa-

7
1
0
2
 
r
p
A
 
6
2
 
 
]
L
C
.
s
c
[
 
 
1
v
9
5
0
8
0
.
4
0
7
1
:
v
i
X
r
a

tion (SPPMI) matrix via Singular Value Decom-
position (SVD). On Step 2, it computes embed-
dings W and C via a simple formula that depends
on the factors obtained by SVD. However, this
method has one important limitation: SVD pro-
vides a solution to a surrogate optimization prob-
lem, which has no direct relation to the SGNS ob-
jective. In fact, SVD minimizes the Mean Squared
Error (MSE) between X and SPPMI matrix, what
does not lead to minimization of SGNS objec-
tive in general (see Section 6.1 and Section 4.2
in (Levy and Goldberg, 2014) for details).

These issues bring us to the main idea of
our paper: while keeping the low-rank matrix
search setup on Step 1, optimize the original
SGNS objective directly. This leads to an opti-
mization problem over matrix X with the low-
rank constraint, which is often (Mishra et al.,
2014) solved by applying Riemannian optimiza-
tion framework (Udriste, 1994). In our paper, we
use the projector-splitting algorithm (Lubich and
Oseledets, 2014), which is easy to implement and
has low computational complexity. Of course,
Step 2 may be improved as well, but we regard
this as a direction of future work.

As a result, our approach achieves the signif-
icant improvement in terms of SGNS optimiza-
tion on Step 1 and, moreover, the improvement
on Step 1 entails the improvement on Step 2 in
terms of linguistic metrics. That is why,
the
proposed two-step decomposition of the problem
makes sense, what, most importantly, opens the
way to applying even more advanced approaches
based on it (e.g., more advanced Riemannian opti-
mization techniques for Step 1 or a more sophisti-
cated treatment of Step 2).

To summarize, the main contributions of our pa-

per are:

• We reformulated the problem of SGNS word
embedding learning as a two-step procedure
with clear objectives;

• For Step 1, we developed an algorithm based
on Riemannian optimization framework that
optimizes SGNS objective over low-rank ma-
trix X directly;

• Our algorithm outperforms state-of-the-art
competitors in terms of SGNS objective
and the semantic similarity linguistic met-
ric (Levy and Goldberg, 2014; Mikolov et al.,
2013; Schnabel et al., 2015).

2 Problem Setting

2.1 Skip-Gram Negative Sampling

In this paper, we consider
the Skip-Gram
Negative Sampling (SGNS) word embedding
model (Mikolov et al., 2013), which is a prob-
abilistic discriminative model. Assume we have
a text corpus given as a sequence of words
w1, . . . , wn, where n may be larger than 1012 and
wi ∈ VW belongs to a vocabulary of words VW . A
context c ∈ VC of the word wi is a word from set
{wi−L, ..., wi−1, wi+1, ..., wi+L} for some ﬁxed
window size L. Let w, c ∈ Rd be the word em-
beddings of word w and context c, respectively.
Assume they are speciﬁed by the following map-
pings:

W : VW → Rd,

C : VC → Rd.

The ultimate goal of SGNS word embedding train-
ing is to ﬁt good mappings W and C.

Let D be a multiset of all word-context pairs
observed in the corpus. In the SGNS model, the
probability that word-context pair (w, c) is ob-
served in the corpus is modeled as a following
dsitribution:

P (#(w, c) (cid:54)= 0|w, c) =

= σ((cid:104)w, c(cid:105)) =

1
1 + exp(−(cid:104)w, c(cid:105))

,

(1)

where #(w, c) is the number of
times the
pair (w, c) appears in D and (cid:104)x, y(cid:105) is the scalar
product of vectors x and y. Number d is a hyper-
parameter that adjusts the ﬂexibility of the model.
It usually takes values from tens to hundreds.

In order to collect a training set, we take all
pairs (w, c) from D as positive examples and k
randomly generated pairs (w, c) as negative ones.
The number of times the word w and the context c
appear in D can be computed as

#(w) =

#(w, c),

#(c) =

#(w, c)

(cid:88)

c∈Vc

(cid:88)

w∈Vw

accordingly. Then negative examples are gener-
ated from the distribution deﬁned by #(c) coun-
ters:

PD(c) =

#(c)
|D|

.

In this way, we have a model maximizing the
following logarithmic likelihood objective for all
word-context pairs (w, c):

lwc = #(w, c)(log σ((cid:104)w, c(cid:105))+

+k · Ec(cid:48)∼PD log σ(−(cid:104)w, c(cid:48)(cid:105))).

(2)

In order to maximize the objective over all obser-
vations for each pair (w, c), we arrive at the fol-
lowing SGNS optimization problem over all pos-
sible mappings W and C:

2.2.2 Matrix Notation
Denote |VW | as n and |VC| as m. Let W ∈ Rn×d
and C ∈ Rm×d be matrices, where each row w ∈
Rd of matrix W is the word embedding of the cor-
responding word w and each row c ∈ Rd of ma-
trix C is the context embedding of the correspond-
ing context c. Then the elements of the product of
these matrices

X = W C(cid:62)

(cid:88)

(cid:88)

l =

w∈VW

c∈VC

(#(w, c)(log σ((cid:104)w, c(cid:105))+

are the scalar products xw,c of all pairs (w, c):

(3)

X = (xw,c), w ∈ VW , c ∈ VC.

+k · Ec(cid:48)∼PD log σ(−(cid:104)w, c(cid:48)(cid:105)))) → max
W,C

.

Usually, this optimization is done via the stochas-
tic gradient descent procedure that is performed
during passing through the corpus (Mikolov et al.,
2013; Rong, 2014).

2.2 Optimization over Low-Rank Matrices

Relying on the prospect proposed in (Levy and
Goldberg, 2014), let us show that the optimization
problem given by (3) can be considered as a prob-
lem of searching for a matrix that maximizes a
certain objective function and has the rank-d con-
straint (Step 1 in the scheme described in Sec-
tion 1).

2.2.1 SGNS Loss Function
the
As shown in (Levy and Goldberg, 2014),
logarithmic likelihood (3) can be represented
as the sum of lw,c(w, c) over all pairs (w, c),
where lw,c(w, c) has the following form:

lw,c(w, c) = #(w, c) log σ((cid:104)w, c(cid:105))+

+k

#(w)#(c)
|D|

log σ(−(cid:104)w, c(cid:105)).

(4)

A crucial observation is that this loss function de-
pends only on the scalar product (cid:104)w, c(cid:105) but not on
embeddings w and c separately:

lw,c(w, c) = fw,c(xw,c),

where

fw,c(xw,c) = aw,c log σ(xw,c)+bw,c log σ(−xw,c),

and xw,c is the scalar product (cid:104)w, c(cid:105), and

aw,c = #(w, c),

bw,c = k

#(w)#(c)
|D|

are constants.

Note that this matrix has rank d, because X equals
to the product of two matrices with sizes (n × d)
and (d × m). Now we can write SGNS objective
given by (3) as a function of X:

F (X) =

(cid:88)

(cid:88)

w∈VW

c∈VC

fw,c(xw,c), F : Rn×m → R.

This arrives us at the following proposition:

Proposition 1 SGNS optimization problem given
by (3) can be rewritten in the following con-
strained form:

(5)

(6)

F (X),

maximize
X∈Rn×m
subject to X ∈ Md,

where Md is the manifold (Udriste, 1994) of all
matrices in Rn×m with rank d:

Md = {X ∈ Rn×m : rank(X) = d}.

The key idea of this paper is to solve the opti-
mization problem given by (6) via the framework
of Riemannian optimization, which we introduce
in Section 3.

Important to note that this prospect does not
suppose the optimization over parameters W
and C directly. This entails the optimization in
the space with ((n + m − d) · d) degrees of free-
dom (Mukherjee et al., 2015) instead of ((n + m) ·
d), what simpliﬁes the optimization process (see
Section 5 for the experimental results).

2.3 Computing Embeddings from a

Low-Rank Solution

Once X is found, we need to recover W and C
such that X = W C(cid:62) (Step 2 in the scheme
described in Section 1). This problem does not

have a unique solution, since if (W, C) satisfy this
equation, then W S−1 and CS(cid:62) satisfy it as well
for any non-singular matrix S. Moreover, different
solutions may achieve different values of the lin-
guistic metrics (see Section 4.2 for details). While
our paper focuses on Step 1, we use, for Step 2,
a heuristic approach that was proposed in (Levy
et al., 2015) and it shows good results in practice.
We compute SVD of X in the form

X = U ΣV (cid:62),

where U and V have orthonormal columns, and Σ
is the diagonal matrix, and use

√

√

W = U

Σ, C = V

Σ

as matrices of embeddings.

A simple justiﬁcation of this solution is the fol-
lowing: we need to map words into vectors in a
way that similar words would have similar embed-
dings in terms of cosine similarities:

cos(w1, w2) =

(cid:104)w1, w2(cid:105)
(cid:107)w1(cid:107) · (cid:107)w2(cid:107)

.

It is reasonable to assume that two words are sim-
ilar, if they share contexts. Therefore, we can esti-
mate the similarity of two words w1, w2 as

s(w1, w2) =

xw1,c · xw2,c,

(cid:88)

c∈VC

what is the element of the matrix XX (cid:62) with in-
dices (w1, w2). Note that

XX (cid:62) = U ΣV (cid:62)V ΣU (cid:62) = U Σ2U (cid:62).

If we choose W = U Σ, we exactly ob-
tain (cid:104)w1, w2(cid:105) = s(w1, w2), since W W (cid:62) =
XX (cid:62) in this case. That is, the cosine similar-
ity of the embeddings w1, w2 coincides with the
intuitive similarity s(w1, w2). However, scaling
Σ instead of Σ was shown in (Levy et al.,
by
2015) to be a better solution in experiments.

√

3 Proposed Method

3.1 Riemannian Optimization

idea

3.1.1 General Scheme
The main
optimiza-
tion (Udriste, 1994) is to consider (6) as a
constrained optimization problem. Assume we
have an approximated solution Xi on a current

of Riemannian

step of the optimization process, where i is the
step number. In order to improve Xi, the next step
of the standard gradient ascent outputs the point

Xi + ∇F (Xi),

where ∇F (Xi) is the gradient of objective F at
the point Xi. Note that the gradient ∇F (Xi)
can be naturally considered as a matrix in Rn×m.
Point Xi + ∇F (Xi) leaves the manifold Md,
because its rank is generally greater than d.
That is why Riemannian optimization methods
map point Xi + ∇F (Xi) back to manifold Md.
The standard Riemannian gradient method ﬁrst
projects the gradient step onto the tangent space
at the current point Xi and then retracts it back to
the manifold:

Xi+1 = R (PTM (Xi + ∇F (Xi))),

where R is the retraction operator, and PTM is the
projection onto the tangent space.

Although the optimization problem is non-
convex, Riemannian optimization methods show
good performance on it. Theoretical properties
and convergence guarantees of such methods are
discussed in (Wei et al., 2016) more thoroughly.

3.1.2 Projector-Splitting Algorithm
In our paper, we use a simpliﬁed version of such
approach that retracts point Xi + ∇F (Xi) directly
to the manifold and does not require projection
onto the tangent space PTM as illustrated in Fig-
ure 1:

Xi+1 = R(Xi + ∇F (Xi)).

Intuitively, retractor R ﬁnds a rank-d matrix on
the manifold Md that is similar to high-rank ma-
trix Xi + ∇F (Xi) in terms of Frobenius norm.
How can we do it? The most straightforward way
to reduce the rank of Xi + ∇F (Xi) is to perform
the SVD, which keeps d largest singular values of
it:

1: Ui+1, Si+1, V (cid:62)
2: Xi+1 ← Ui+1Si+1V (cid:62)

i+1.

i+1 ← SVD(Xi + ∇F (Xi)),

(7)

it is computationally expensive.

However,
In-
stead of this approach, we use the projector-
splitting method (Lubich and Oseledets, 2014),
which is a second-order retraction onto the man-
ifold (for details, see the review (Absil and Os-
Its practical implementation is
eledets, 2015)).

which means that the gradient will be large if S is
close to singular. The projector-splitting scheme
is free from this problem.

3.2 Algorithm

In case of SGNS objective given by (5), an element
of gradient ∇F has the form:

(∇F (X))w,c =

∂fw,c(xw,c)
∂xw,c

=

= #(w, c) · σ (−xw,c) − k

· σ (xw,c) .

#(w)#(c)
|D|

To make the method more ﬂexible in terms of con-
vergence properties, we additionally use λ ∈ R,
which is a step size parameter.
In this case, re-
tractor R returns Xi + λ∇F (Xi) instead of Xi +
∇F (Xi) onto the manifold.

The whole optimization procedure is summa-

rized in Algorithm 1.

4 Experimental Setup

4.1 Training Models

We compare our method (“RO-SGNS” in the ta-
bles) performance to two baselines: SGNS embed-
dings optimized via Stochastic Gradient Descent,
implemented in the original “word2vec”, (“SGD-
SGNS” in the tables) (Mikolov et al., 2013) and
embeddings obtained by SVD over SPPMI ma-
trix (“SVD-SPPMI” in the tables) (Levy and Gold-
berg, 2014). We have also experimented with the
blockwise alternating optimization over factors W
and C, but the results are almost the same to SGD
results, that is why we do not to include them into
the paper. The source code of our experiments is
available online1.

The models were trained on English Wikipedia
“enwik9” corpus2, which was previously used in
most papers on this topic. Like in previous stud-
ies, we counted only the words which occur more
than 200 times in the training corpus (Levy and
Goldberg, 2014; Mikolov et al., 2013). As a re-
sult, we obtained a vocabulary of 24292 unique
tokens (set of words VW and set of contexts VC
are equal). The size of the context window was set
to 5 for all experiments, as it was done in (Levy
and Goldberg, 2014; Mikolov et al., 2013). We
conduct three series of experiments: for dimen-
sionality d = 100, d = 200, and d = 500.

1https://github.com/AlexGrinch/ro_sgns
2http://mattmahoney.net/dc/textdata

Figure 1: Geometric interpretation of one step
of projector-splitting optimization procedure: the
gradient step an the retraction of the high-rank ma-
trix Xi +∇F (Xi) to the manifold of low-rank ma-
trices Md.

also quite intuitive: instead of computing the full
SVD of Xi + ∇F (Xi) according to the gradi-
ent projection method, we use just one step of the
block power numerical method (Bentbib and Kan-
ber, 2015) which computes the SVD, what reduces
the computational complexity.

Let us keep the current point in the following

factorized form:

,

Xi = UiSiV (cid:62)
(8)
i
where matrices Ui ∈ Rn×d and Vi ∈ Rm×d have d
orthonormal columns and Si ∈ Rd×d. Then we
need to perform two QR-decompositions to retract
point Xi + ∇F (Xi) back to the manifold:

1: Ui+1, Si+1 ← QR ((Xi + ∇F (Xi))Vi) ,
(cid:16)
2: Vi+1, S(cid:62)
3: Xi+1 ← Ui+1Si+1V (cid:62)

(Xi + ∇F (Xi))(cid:62)Ui+1

i+1 ← QR

i+1.

(cid:17)

,

In this way, we always keep the solution Xi+1 =
Ui+1Si+1V (cid:62)
i+1 on the manifold Md and in the
form (8).
What

is important, we only need to com-
pute ∇F (Xi), so the gradients with respect to
U , S and V are never computed explicitly, thus
avoiding the subtle case where S is close to singu-
lar (so-called singular (critical) point on the man-
Indeed, the gradient with respect to U
ifold).
(while keeping the orthogonality constraints) can
be written (Koch and Lubich, 2007) as:

∂F
∂U

=

∂F
∂X

V S−1,

Algorithm 1 Riemannian Optimization for SGNS
Require: Dimentionality d, initialization W0 and C0, step size λ, gradient function ∇F : Rn×m →

# get an initial point at the manifold
# compute the ﬁrst point satisfying the low-rank constraint

Ui, Si ← QR ((Xi−1 + λ∇F (Xi−1))Vi−1)
(cid:1)
i ← QR (cid:0)(Xi−1 + λ∇F (Xi−1))(cid:62)Ui
Vi, S(cid:62)

# perform one step of the block power method

# update the point at the manifold

Rn×m, number of iterations K

Ensure: Factor W ∈ Rn×d
1: X0 ← W0C(cid:62)
0
2: U0, S0, V (cid:62)
3: for i ← 1, . . . , K do
4:

0 ← SVD(X0)

5:
6: Xi ← UiSiV (cid:62)
i
7: end for
8: U, Σ, V (cid:62) ← SVD(XK)
√
9: W ← U
Σ
10: return W

# compute word embeddings

Optimization step size is chosen to be small
enough to avoid huge gradient values. However,
thorough choice of λ does not result in a signiﬁ-
cant difference in performance (this parameter was
tuned on the training data only, the exact values
used in experiments are reported below).

4.2 Evaluation

We evaluate word embeddings via the word simi-
larity task. We use the following popular datasets
“wordsim-353” ((Finkelstein
for this purpose:
et al., 2001); 3 datasets), “simlex-999” (Hill et al.,
2016) and “men” (Bruni et al., 2014). Original
“wordsim-353” dataset is a mixture of the word
pairs for both word similarity and word related-
ness tasks. This dataset was split (Agirre et al.,
2009) into two intersecting parts: “wordsim-sim”
(“ws-sim” in the tables) and “wordsim-rel” (“ws-
rel” in the tables) to separate the words from dif-
ferent tasks. In our experiments, we use both of
them on a par with the full version of “wordsim-
353” (“ws-full” in the tables). Each dataset con-
tains word pairs together with assessor-assigned
similarity scores for each pair. As a quality mea-
sure, we use Spearman’s correlation between these
human ratings and cosine similarities for each pair.
We call this quality metric linguistic in our paper.

5 Results of Experiments

First of all, we compare the value of SGNS objec-
tive obtained by the methods. The comparison is
demonstrated in Table 1.

We see that SGD-SGNS and SVD-SPPMI
methods provide quite similar results, however,
the proposed method obtains signiﬁcantly better

d = 100
−1.68
SGD-SGNS
SVD-SPPMI −1.65
−1.44
RO-SGNS

d = 200
−1.67
−1.65
−1.43

d = 500
−1.63
−1.62
−1.41

Table 1: Comparison of SGNS values (multiplied
by 10−9) obtained by the models. Larger is better.

SGNS values, what proves the feasibility of us-
ing Riemannian optimization framework in SGNS
optimization problem.
It is interesting to note
that SVD-SPPMI method, which does not opti-
mize SGNS objective directly, obtains better re-
sults than SGD-SGNS method, which aims at opti-
mizing SGNS. This fact additionally conﬁrms the
idea described in Section 2.2.2 that the indepen-
dent optimization over parameters W and C may
decrease the performance.

However, the target performance measure of
embedding models is the correlation between se-
mantic similarity and human assessment (Sec-
tion 4.2). Table 2 presents the comparison of the
methods in terms of it. We see that our method
outperforms the competitors on all datasets except
for “men” dataset where it obtains slightly worse
results. Moreover, it is important that the higher
dimension entails higher performance gain of our
method in comparison to the competitors.

To understand how our model improves or de-
grades the performance in comparison to the base-
line, we found several words, whose neighbors in
terms of cosine distance change signiﬁcantly. Ta-
ble 3 demonstrates neighbors of the words “ﬁve”,
“he” and “main” for both SVD-SPPMI and RO-
SGNS models. A neighbor is marked bold if we
suppose that it has similar semantic meaning to the

Dim. d Algorithm

d = 100

d = 200

d = 500

SGD-SGNS
SVD-SPPMI
RO-SGNS
SGD-SGNS
SVD-SPPMI
RO-SGNS
SGD-SGNS
SVD-SPPMI
RO-SGNS

ws-sim ws-rel ws-full
0.662
0.570
0.719
0.669
0.585
0.722
0.677
0.597
0.729
0.677
0.584
0.733
0.694
0.625
0.747
0.708
0.647
0.757
0.688
0.600
0.738
0.707
0.639
0.765
0.715
0.654
0.767

simlex men
0.645
0.288
0.317
0.686
0.683
0.322
0.664
0.317
0.347
0.710
0.701
0.353
0.712
0.350
0.380
0.737
0.732
0.383

Table 2: Comparison of the methods in terms of the semantic similarity task. Each entry represents the
Spearman’s correlation between predicted similarities and the manually assessed ones.

ﬁve

he

main

SVD-SPPMI

RO-SGNS

SVD-SPPMI

RO-SGNS

SVD-SPPMI

RO-SGNS

Neighbors
lb
kg
mm
mk
lbf
per

Dist.
0.748
0.731
0.670
0.651
0.650
0.644

Neighbors
four
three
six
seven
eight
and

Dist.
0.999
0.999
0.997
0.997
0.996
0.985

Neighbors
she
was
promptly
having
dumbledore
him

Dist.
0.918
0.797
0.742
0.731
0.731
0.730

Neighbors
when
had
was
who
she
by

Dist.
0.904
0.903
0.901
0.892
0.884
0.880

Neighbors
major
busiest
principal
nearest
connecting
linking

Dist.
0.631
0.621
0.607
0.607
0.591
0.588

Neighbors
major
important
line
external
principal
primary

Dist.
0.689
0.661
0.631
0.624
0.618
0.612

Table 3: Examples of the semantic neighbors obtained for words “ﬁve”, “he” and “main”.

SGD-SGNS

RO-SGNS

usa

SVD-SPPMI

Neighbors
akron
midwest
burbank
nevada
arizona
uk
youngstown
utah
milwaukee
headquartered

Dist.
0.536
0.535
0.534
0.534
0.533
0.532
0.532
0.530
0.530
0.527

Neighbors
wisconsin
delaware
ohio
northeast
cities
southwest
places
counties
maryland
dakota

Dist.
0.700
0.693
0.691
0.690
0.688
0.684
0.684
0.681
0.680
0.674

Neighbors
georgia
delaware
maryland
illinois
madison
arkansas
dakota
tennessee
northeast
nebraska

Dist.
0.707
0.706
0.705
0.704
0.703
0.699
0.690
0.689
0.687
0.686

Table 4: Examples of the semantic neighbors
from 11th to 20th obtained for the word “usa” by
all three methods. Top-10 neighbors for all three
methods are exact names of states.

source word. First of all, we notice that our model
produces much better neighbors of the words de-
scribing digits or numbers (see word “ﬁve” as
an example). Similar situation happens for many
other words, e.g. in case of “main” — the nearest
neighbors contain 4 similar words for our model
instead of 2 in case of SVD-SPPMI. The neigh-
bourhood of “he” contains less semantically sim-
ilar words in case of our model. However, it ﬁl-
ters out irrelevant words, such as “promptly” and
“dumbledore”.

Table 4 contains the nearest words to the word
“usa” from 11th to 20th. We marked names of
USA states bold and did not represent top-10 near-
est words as they are exactly names of states for
all three models. Some non-bold words are ar-
guably relevant as they present large USA cities

(“akron”, “burbank”, “madison”) or geographi-
cal regions of several states (“midwest”, “north-
east”, “southwest”), but there are also some com-
pletely irrelevant words (“uk”, “cities”, “places”)
presented by ﬁrst two models.

Our experiments show that the optimal number
of iterations K in the optimization procedure and
step size λ depend on the particular value of d.
For d = 100, we have K = 7, λ = 5 · 10−5, for
d = 200, we have K = 8, λ = 5 · 10−5, and for
d = 500, we have K = 2, λ = 10−4. Moreover,
the best results were obtained when SVD-SPPMI
embeddings were used as an initialization of Rie-
mannian optimization process.

Figure 2 illustrates how the correlation between
semantic similarity and human assessment scores
changes through iterations of our method. Optimal
value of K is the same for both whole testing set
and its 10-fold subsets chosen for cross-validation.
The idea to stop optimization procedure on some
iteration is also discussed in (Lai et al., 2015).

Training of

the same dimensional models
(d = 500) on English Wikipedia corpus using
SGD-SGNS, SVD-SPPMI, RO-SGNS took 20
minutes, 10 minutes and 70 minutes respectively.
Our method works slower, but not signiﬁcantly.
Moreover, since we were not focused on the code
efﬁciency optimization, this time can be reduced.

Figure 2: Illustration of why it is important to choose the optimal iteration and stop optimization proce-
dure after it. The graphs show semantic similarity metric in dependence on the iteration of optimization
procedure. The embeddings obtained by SVD-SPPMI method were used as initialization. Parameters:
d = 200, λ = 5 · 10−5.

6 Related Work

6.1 Word Embeddings

Skip-Gram Negative Sampling was introduced
in (Mikolov et al., 2013). The “negative sampling”
approach is thoroughly described in (Goldberg and
Levy, 2014), and the learning method is explained
in (Rong, 2014). There are several open-source
implementations of SGNS neural network, which
is widely known as “word2vec”. 12

As shown in Section 2.2, Skip-Gram Negative
Sampling optimization can be reformulated as a
problem of searching for a low-rank matrix. In or-
der to be able to use out-of-the-box SVD for this
task, the authors of (Levy and Goldberg, 2014)
used the surrogate version of SGNS as the objec-
tive function. There are two general assumptions
made in their algorithm that distinguish it from the
SGNS optimization:

1. SVD optimizes Mean Squared Error (MSE)
objective instead of SGNS loss function.

2. In order to avoid inﬁnite elements in SPMI
matrix, it is transformed in ad-hoc manner
(SPPMI matrix) before applying SVD.

This makes the objective not interpretable in terms
of the original task (3). As mentioned in (Levy
and Goldberg, 2014), SGNS objective weighs dif-
ferent (w, c) pairs differently, unlike the SVD,
which works with the same weight for all pairs
and may entail the performance fall. The compre-
hensive explanation of the relation between SGNS
and SVD-SPPMI methods is provided in (Keerthi
et al., 2015). (Lai et al., 2015; Levy et al., 2015)

1Original Google word2vec:

https://code.

google.com/archive/p/word2vec/

2Gensim word2vec:

https://radimrehurek.

com/gensim/models/word2vec.html

give a good overview of highly practical methods
to improve these word embedding models.

6.2 Riemannian Optimization

An introduction to optimization over Riemannian
manifolds can be found in (Udriste, 1994). The
overview of retractions of high rank matrices to
low-rank manifolds is provided in (Absil and Os-
eledets, 2015). The projector-splitting algorithm
was introduced in (Lubich and Oseledets, 2014),
and also was mentioned in (Absil and Oseledets,
2015) as “Lie-Trotter retraction”.

Riemannian optimization is succesfully applied
for example,
to various data science problems:
matrix completion (Vandereycken, 2013), large-
scale recommender systems (Tan et al., 2014), and
tensor completion (Kressner et al., 2014).

7 Conclusions

In our paper, we proposed the general two-step
scheme of training SGNS word embedding model
and introduced the algorithm that performs the
search of a solution in the low-rank form via
Riemannian optimization framework. We also
demonstrated the superiority of our method by
providing experimental comparison to existing
state-of-the-art approaches.

Possible direction of future work is to apply
more advanced optimization techniques to the
Step 1 of the scheme proposed in Section 1 and to
explore the Step 2 — obtaining embeddings with
a given low-rank matrix.

Acknowledgments

This research was supported by the Ministry of
Education and Science of the Russian Federation
(grant 14.756.31.0001).

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In NIPS. pages 3111–3119.

Bamdev Mishra, Gilles Meyer, Silv`ere Bonnabel, and
Rodolphe Sepulchre. 2014. Fixed-rank matrix fac-
torizations and riemannian low-rank optimization.
Computational Statistics 29(3-4):591–621.

A Mukherjee, K Chen, N Wang, and J Zhu. 2015. On
the degrees of freedom of reduced-rank estimators
in multivariate regression. Biometrika 102(2):457–
477.

Xin Rong. 2014. word2vec parameter learning ex-

plained. arXiv preprint arXiv:1411.2738 .

Tobias Schnabel, Igor Labutov, David Mimno, and
Thorsten Joachims. 2015. Evaluation methods for
unsupervised word embeddings. In EMNLP.

Mingkui Tan, Ivor W Tsang, Li Wang, Bart Vanderey-
cken, and Sinno Jialin Pan. 2014. Riemannian pur-
suit for big matrix recovery. In ICML. volume 32,
pages 1539–1547.

Constantin Udriste. 1994. Convex functions and opti-
mization methods on Riemannian manifolds, volume
297. Springer Science & Business Media.

Bart Vandereycken. 2013. Low-rank matrix comple-
tion by riemannian optimization. SIAM Journal on
Optimization 23(2):1214–1236.

Ke Wei, Jian-Feng Cai, Tony F Chan, and Shingyu Le-
ung. 2016. Guarantees of riemannian optimization
for low rank matrix recovery. SIAM Journal on Ma-
trix Analysis and Applications 37(3):1198–1222.

References

P-A Absil and Ivan V Oseledets. 2015. Low-rank re-
tractions: a survey and new results. Computational
Optimization and Applications 62(1):5–29.

Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distribu-
In NAACL.
tional and wordnet-based approaches.
pages 19–27.

AH Bentbib and A Kanber. 2015.

Block power
method for svd decomposition. Analele Stiintiﬁce
Ale Unversitatii Ovidius Constanta-Seria Matemat-
ica 23(2):45–58.

Elia Bruni, Nam-Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. J. Artif. Intell.
Res.(JAIR) 49(1-47).

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In WWW. pages 406–414.

Yoav Goldberg and Omer Levy. 2014. word2vec
explained:
deriving mikolov et al.’s negative-
sampling word-embedding method. arXiv preprint
arXiv:1402.3722 .

Felix Hill, Roi Reichart, and Anna Korhonen. 2016.
Simlex-999: Evaluating semantic models with (gen-
uine) similarity estimation. Computational Linguis-
tics .

S Sathiya Keerthi, Tobias Schnabel, and Rajiv Khanna.
2015. Towards a better understanding of predict and
count models. arXiv preprint arXiv:1511.02024 .

Othmar Koch and Christian Lubich. 2007. Dynami-
cal low-rank approximation. SIAM J. Matrix Anal.
Appl. 29(2):434–454.

Daniel Kressner, Michael Steinlechner, and Bart Van-
dereycken. 2014. Low-rank tensor completion by
riemannian optimization. BIT Numerical Mathe-
matics 54(2):447–468.

Siwei Lai, Kang Liu, Shi He, and Jun Zhao. 2015. How
to generate a good word embedding? arXiv preprint
arXiv:1507.05523 .

Omer Levy and Yoav Goldberg. 2014. Neural word
In

embedding as implicit matrix factorization.
NIPS. pages 2177–2185.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. ACL 3:211–225.

Christian Lubich and Ivan V Oseledets. 2014. A
projector-splitting integrator for dynamical
low-
rank approximation. BIT Numerical Mathematics
54(1):171–188.

