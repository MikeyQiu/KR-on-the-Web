0
2
0
2
 
r
p
A
 
2
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
3
8
6
0
0
.
1
1
8
1
:
v
i
X
r
a

Quasi-random sampling for multivariate distributions via
generative neural networks

Marius Hofert1, Avinash Prasad2, Mu Zhu3

2020-04-06

Abstract
Generative moment matching networks (GMMNs) are introduced for generating quasi-random
samples from multivariate models with any underlying copula in order to compute estimates under
variance reduction. So far, quasi-random sampling for multivariate distributions required a careful
design, exploiting speciﬁc properties (such as conditional distributions) of the implied parametric
copula or the underlying quasi-Monte Carlo (QMC) point set, and was only tractable for a small
number of models. Utilizing GMMNs allows one to construct quasi-random samples for a much larger
variety of multivariate distributions without such restrictions, including empirical ones from real data
with dependence structures not well captured by parametric copulas. Once trained on pseudo-random
samples from a parametric model or on real data, these neural networks only require a multivariate
standard uniform randomized QMC point set as input and are thus fast in estimating expectations
of interest under dependence with variance reduction. Numerical examples are considered to
demonstrate the approach, including applications inspired by risk management practice. All results
are reproducible with the demos GMMN_QMC_paper, GMMN_QMC_data and GMMN_QMC_timings as part
of the R package gnn.

Keywords
Maximum mean discrepancy, generative moment matching networks, quasi-random numbers, copulas,
sums of dependent random variables, expected shortfall.

MSC2010
62H99, 65C60, 60E05, 00A72, 65C10.

1 Introduction

Let X = (X1, . . . , Xd) be a d-dimensional random vector with distribution function FX and continuous
margins FX1, . . . , FXd. It is not a trivial task in general to generate quasi-random samples X1, ..., Xn
from FX , i.e., samples that mimic realizations from FX but preserve low-discrepancy in the sense of
being locally more homogeneous with fewer “gaps” or “clusters” (Cambou et al., 2017, Section 4.2).

By Sklar’s Theorem, we always have the decomposition

FX (x) = C(FX1(x1), . . . , FXd(xd)), x = (x1, . . . , xd) ∈ Rd,
where C : [0, 1]d → [0, 1] is the unique underlying copula (Nelsen, 2006; Joe, 2014). Since, in
distribution, X = F −1
(ud)), we shall mostly focus
on the problem of generating quasi-random samples U1, ..., Un from C rather than X1, ..., Xn from
FX , as the latter are easily obtained from the former.

X (U ) for U ∼ C and F −1

X (u) = (F −1
X1

(u1), . . . , F −1
Xd

(1)

1Department of Statistics and Actuarial Science, University of Waterloo, 200 University Avenue West, Waterloo, ON,
N2L 3G1, marius.hofert@uwaterloo.ca. The author acknowledges support from NSERC (Grant RGPIN-5010-2015).
2Department of Statistics and Actuarial Science, University of Waterloo, 200 University Avenue West, Waterloo, ON,
N2L 3G1, a2prasad@uwaterloo.ca. The author acknowledges support from NSERC (PGS D Scholarship).
3Department of Statistics and Actuarial Science, University of Waterloo, 200 University Avenue West, Waterloo, ON,
N2L 3G1, mu.zhu@uwaterloo.ca. The author acknowledges support from NSERC (RGPIN-2016-03876).

1

1 Introduction

1.1 Existing diﬃculty

For the independence copula, C(u) = u1 · . . . · ud, quasi-random samples can be obtained simply by
using randomized quasi-Monte Carlo (RQMC) point sets such as randomized Sobol’ or generalized
Halton sequences (see, e.g., Lemieux, 2009, Chapter 5).

Recently, Cambou et al. (2017) demonstrated that for a limited number of copulas C (normal, t
or Clayton copulas), one can obtain quasi-random samples by transforming RQMC point sets with
the inverse Rosenblatt transform of C (Rosenblatt, 1952); this method is known as the conditional
distribution method (CDM) — see, e.g., Embrechts et al. (2003) or Hofert (2010, p. 45). Cambou et al.
(2017) also showed that transformations to quasi-random copula samples may exist for copulas with
a suﬃciently simple stochastic representation. For most copulas, the latter is not the case and the
CDM is numerically intractable. In other words, there exists no universal and numerically tractable
transformation from RQMC point sets to quasi-random samples from copulas. For the majority
of copula models, including grouped normal variance mixture copulas, Archimax copulas, nested
Archimedean copulas or extreme-value copulas, we simply do not know how to generate quasi-random
samples from them.

1.2 Our contribution

The main contribution of this paper is to introduce a new approach for quasi-random sampling from
FX with any underlying copula C, using generative neural networks. Even when we do not know
the distribution FX , our approach can still provide quasi-random samples from the corresponding
empirical distribution ˆFX as long as we have a dataset from FX . This is especially useful when the
dependence structure in the data cannot be adequately captured by a readily available parametric
copula; see Section 5 where we present a real-data example to show how useful our approach can be in
this case where no adequate copula model is known in the ﬁrst place.

Speciﬁcally, let fθ denote a neural network (NN) parameterized by θ. We train fθ so that, given
a p-dimensional input Z ∼ FZ with independent components Z1, . . . , Zp from known distributions
FZ1, . . . , FZp, the trained NN can generate d-dimensional output from the desired distribution, f ˆθ(Z) ∼
FX , where ˆθ denotes the parameter vector of the trained NN. We can thus turn a uniform RQMC
point set, {˜v1, . . . , ˜vn}, into a quasi-random sample from FX by letting

Yi = f ˆθ ◦ F −1

Z (˜vi),

i = 1, . . . , n,

(2)

where F −1

Z (u) = (F −1
Z1

(u1), . . . , F −1
Zp

(up)).

1.3 Assessment

The theoretical properties of quasi-randomness (or low-discrepancy) under dependence are hard to
assess; see Cambou et al. (2017) and Appendix A. In low-dimensional cases (see Section 3.2), we use
data visualization tools to assess the quality of the generated quasi-random samples, such as contour
plots (or level curves) showing that the empirical copula of our GMMN quasi-random samples is closer
to the true target copula than that of GMMN pseudo-random samples. In higher-dimensional cases
(see Section 3.3), we use a Cramér-von-Mises goodness-of-ﬁt statistic to make the same point.

Since the main application of quasi-random sampling is to obtain low-variance Monte-Carlo estimates

of

µ = E(Ψ(X)) = E(cid:0)Ψ(F −1

X (U ))(cid:1)

for X ∼ FX , U ∼ C

(3)

2

2 Quasi-random GMMN samples

ˆµMC

n =

Ψ(F −1

X (Ui)),

1
n

n
X

i=1

for an integrable Ψ : Rd → R, we also assess our method in such a speciﬁc context. The Monte Carlo
estimator approximates this expectation by

(4)

(5)

where U1, . . . , Un
approximate E(cid:0)Ψ(F −1

ind.∼ C. Using NN-generated quasi-random samples Y1, . . . , Yn from (2), we can
X (U ))(cid:1) by

ˆµNN

n =

Ψ(Yi) =

Ψ(f ˆθ ◦ F −1

Z (˜vi)).

1
n

n
X

i=1

1
n

n
X

i=1

Theoretically (Section 2.3 and Appendix A), we establish various guarantees that the estimation error
of (3) by (5) will be small as long as both f ˆθ and Ψ are suﬃciently smooth; we also establish the
corresponding convergence rates. Empirically (Section 4), we verify that (5) indeed has lower variance
and converges faster than (4).

Although being the main focus in this paper, let us stress that estimating expectations such as (3)
is not the only application of quasi-random sampling. For example, quasi-random sampling can also
be used for estimating quantiles for the distribution of a sum of dependent random variables.

All results presented in this paper (and more) are reproducible with the demos GMMN_QMC_paper,

GMMN_QMC_data and GMMN_QMC_timings as part of the R package gnn.

2 Quasi-random GMMN samples

2.1 Generative moment matching networks

In this paper, we work with the multi-layer perceptron (MLP), which is regarded as the quintessential
neural network (NN). Let L be the number of (hidden) layers in the NN and, for each l = 0, . . . , L + 1,
let dl be the dimension of layer l, that is, the number of neurons in layer l. In this notation, layer l = 0
refers to the input layer which consists of the input z ∈ Rp for d0 = p, and layer l = L + 1 refers to
the output layer which consists of the output y ∈ Rd for dL+1 = d. Layers l = 1, . . . , L + 1 can be
described in terms of the output al−1 ∈ Rdl−1 of layer l − 1 via

a0 = z ∈ Rd0,
al = fl(al−1) = φl(Wlal−1 + bl) ∈ Rdl,
y = aL+1 ∈ RdL+1,

l = 1, . . . , L + 1,

with weight matrices Wl ∈ Rdl×dl−1, bias vectors bl ∈ Rdl and activation functions φl; note that for
vector inputs the activation function φl is understood to be applied componentwise.

The NN fθ : Rp → Rd can then be written as the composition

fθ = fL+1 ◦ fL ◦ · · · ◦ f2 ◦ f1,

with its (ﬂattened) parameter vector given by θ = (W1, . . . , WL+1, b1, . . . , bL+1). To ﬁt θ, we use the
backpropagation algorithm (a stochastic gradient descent) based on a cost function E. Conceptually, E
computes a distance between the target output x ∈ Rd and the actual output y = y(z) ∈ Rd predicted
by the NN; what is actually computed is a sample version of E based on a subsample (the so-called
mini-batch), see Section 2.2.

3

2 Quasi-random GMMN samples

The expressive power of NNs is primarily characterized by the universal approximation theorem; see
Goodfellow et al. (2016, Chapter 6). In particular, given suitable activation functions, a single hidden
layer NN with a ﬁnite number of neurons can approximate any continuous function on a compact
subset of the multidimensional Euclidean space; see Nielsen (2015, Chapter 4) for a visual account of
the validity of the universal approximation theorem. Cybenko (1989) ﬁrst proposed such universal
approximation results for the sigmoid activation function φl(x) = 1/(1 + e−x) and Hornik (1991,
Theorem 1) then generalized the results to include arbitrary bounded and non-constant activation
functions. In recent years, the rectiﬁed linear unit (ReLU) φl(x) = max{0, x} has become the most
popular activation function for eﬃciently training NNs. This unbounded activation function does not
satisfy the assumptions of the universal approximation theorem in Hornik (1991). However, there have
since been numerous theoretical investigations into the expressive power of NNs with ReLU activation
functions; see, for example, Pascanu et al. (2013), Montufar et al. (2014) or R. Arora et al. (2016).
In particular, for certain conditions on the number of layers and neurons in the NN, R. Arora et al.
(2016) provide a similar universal approximation theorem for NNs with ReLU activation functions.

Li et al. (2015) and Dziugaite et al. (2015) simultaneously introduced a type of generative neural
network known as the generative moment matching network (GMMN) or the Maximum Mean Discrep-
ancy (MMD) net. GMMNs are NNs fθ of the above form which utilize a (kernel) maximum mean
discrepancy statistic as the cost function (see later). Conceptually, they can be thought of as parametric
maps of a given sample Z = (Z1, . . . , Zp) from an input distribution FZ to a sample X = (X1, . . . , Xd)
from the target distribution FX . As is standard in the literature, we assume independence among
the components of Z = (Z1, . . . , Zp). Typical choices for the distribution of the Zj’s are U(0, 1) or
N(0, 1). The objective is then to generate samples from the target distribution via the trained GMMN
f ˆθ. The MMD nets introduced in Dziugaite et al. (2015) are almost identical to GMMNs but with a
slight diﬀerence in the training procedure; additionally, Dziugaite et al. (2015) provided a theoretical
framework for analyzing optimization algorithms with (kernel) MMD cost functions.

2.2 Cost function and training of GMMNs

To learn fθ (or, statistically speaking, to estimate the parameter vector θ) we assume that we have
ntrn training data points X1, . . . , Xntrn from X, either in the form of a pseudo-random sample from
FX or as real data. Based on a sample Z1, . . . , Zntrn from the input distribution, the GMMN generates
the output sample Y1, . . . , Yntrn, where Yi = fθ(Zi), i = 1, . . . , ntrn. Stacking X1, . . . , Xntrn into an
ntrn × d matrix X and likewise Y1, . . . , Yntrn into Y , we are thus interested in whether the two samples
X and Y come from the same distribution.

To this end, GMMNs utilize as cost function E the maximum mean discrepancy (MMD) statistic
from the kernel two-sample test introduced by Gretton et al. (2007), whose sample version is given by

MMD(X, Y ) =

(K(Xi1, Xi2) − 2K(Xi1, Yi2) + K(Yi1, Yi2)),

(6)

v
u
u
t

1
n2

trn

ntrnX

ntrnX

i1=1

i2=1

where K(·, ·) : Rd × Rd → R denotes a kernel (similarity) function. If K(·, ·) is a so-called universal
kernel function (e.g., Gaussian or Laplace), then it can be shown (Gretton et al., 2007; Gretton et al.,
2012) that the MMD converges in probability to 0 for ntrn → ∞ if and only if Y = X in distribution.
This makes the MMD with a universal kernel function an intuitive choice as cost function for training
fθ to learn a random number generator from a multivariate distribution FX ; our choice of kernel K is
addressed in Section 3.1. For the Gaussian kernel in particular, expanding the exponential, one can see
that minimizing the MMD can be interpreted as matching all the moments of the distributions of X

4

2 Quasi-random GMMN samples

and Y which gives an interpretation of the MMD in this case. Note that, in comparison to a quadratic
cost function, all pairs of observations enter the MMD, which turns out to be a crucial property for
learning a random number generator of FX .

Computing MMD(X, Y ) in (6) requires one to evaluate the kernel for all (cid:0)ntrn
2

(cid:1) pairs of observations,
which is memory-prohibitive for even moderately large ntrn. As suggested by Li et al. (2015), we thus
adopt a mini-batch optimization procedure. Instead of directly optimizing the MMD for the entire
training dataset, we partition the data into batches of size nbat and use the batches sequentially to
update the parameters θ of the GMMN with the Adam optimizer of Kingma and Ba (2014). Rather
than following the gradient at each iterative step, the Adam optimizer essentially uses a “memory-
sticking gradient” — a weighted combination of the current gradient and past gradients from earlier
iterations. After all the training data are exhausted, i.e., roughly after (ntrn/nbat)-many batches or
gradient steps, one epoch of the training of the GMMN is completed. The overall training procedure
is considered completed after nepo epochs. The training of the GMMN can thus be summarized as
follows:

Algorithm 2.1 (Training GMMNs)
1) Fix the number nepo of epochs and the batch size 1 ≤ nbat ≤ ntrn per epoch, where nbat
is assumed to divide ntrn.
Initialize the epoch counter k = 0 and the GMMN’s parameter
vector θ; we follow Glorot and Bengio (2010) and initialize the components of θ as Wl ∼
U(−p6/(dl + dl−1), p6/(dl + dl−1))dl×dl−1 and bl = 0 for l = 1, . . . , L + 1.

2) For epoch k = 1, . . . , nepo, do:

2.1) Randomly partition the input distribution sample Z1, . . . , Zntrn and training sample X1, . . . , Xntrn

into corresponding ntrn/nbat non-overlapping batches Z(b)
b = 1, . . . , ntrn/nbat, of size nbat each.

1 , . . . , Z(b)

nbat and X (b)

1 , . . . , X (b)

nbat,

2.2) For batch b = 1, . . . , ntrn/nbat, do:

2.2.1) Compute the GMMN output Y (b)

i = fθ(Z(b)

i

), i = 1, . . . , nbat.

2.2.2) Compute the gradient ∂
and Y (b) (stacking Y (b)

∂θ MMD(X (b), Y (b)) from the samples X (b) (stacking X (b)
, . . . , Y (b)

nbat) via automatic diﬀerentiation.

1

1 , . . . , X (b)

nbat)

2.2.3) Take a gradient step to update θ with the Adam optimizer popularized by Kingma and

Ba (2014, Algorithm 1).

3) Return ˆθ = θ; the ﬁtted GMMN is then f ˆθ.

2.3 Pseudo- and quasi-random sampling by GMMN

The following algorithm describes how to obtain a pseudo-random sample of Y via the trained GMMN
f ˆθ from a pseudo-random sample Z ∼ FZ.
Algorithm 2.2 (Pseudo-random sampling by GMMN)
1) Fix the number ngen of samples to generate from Y .

ind.∼ FZ, i = 1, . . . , ngen, for example, via Zi = F −1

Z (U 0

i ), i = 1, . . . , ngen, where

2) Draw Zi
1, . . . , U 0

U 0

ngen

ind.∼ U(0, 1)p.

3) Return Yi = f ˆθ(Zi), i = 1, . . . , ngen; to obtain a sample from C, return the pseudo-observations of

Y1, . . . , Yngen (Genest, Ghoudi, et al., 1995).

5

2 Quasi-random GMMN samples

To obtain quasi-random samples from FX with underlying copula C, we replace U 0

ind.∼
U(0, 1)p in Algorithm 2.2 by an RQMC point set to obtain the following algorithm; the randomization
is done to obtain unbiased QMC estimators and estimates of their variances.

1, . . . , U 0

ngen

Algorithm 2.3 (Quasi-random sampling by GMMN)
1) Fix the number ngen of samples to generate from Y .
2) Compute an RQMC point set ˜Pngen = {˜v1, . . . , ˜vngen} (for example, a randomized Sobol’ or a

generalized Halton sequence) and Zi = F −1

Z (˜vi), i = 1, . . . , ngen.

3) Return Yi = f ˆθ(Zi), i = 1, . . . , ngen; to obtain a sample from C, return the pseudo-observations of

Y1, . . . , Yngen.
Note that ˜Pngen mimics U(0, 1)p, and not C. As mentioned in the introduction, Cambou et al. (2017)
presented transformations to convert ˜Pngen to samples which mimic samples from C but locally provide
a more homogeneous coverage. Unfortunately, these transformations are only available for a few speciﬁc
cases of C and their numerical evaluation in a fast and robust way is even more challenging.

To avoid these problems, we suggest to utilize the GMMN f ˆθ trained as in Algorithm 2.1. Besides
a straightforward evaluation, this allows us to generate quasi-random samples from FX with any
underlying copula C, by training a GMMN on pseudo-random samples generated from FX . Alterna-
tively, quasi-random samples which follow the same empirical distribution as any given dataset can be
obtained by training a GMMN on the given dataset itself. An additional advantage is that GMMNs
provide a suﬃciently smooth map from the RQMC point set to the target distribution which helps
preserve the low-discrepancy of the point set upon transformation and hence guarantees the improved
performance of RQMC estimators compared to the MC estimator (see Section 4 and Appendix A).

With the mapping F −1

Z (u) = (F −1
Z1
GMMN f ˆθ at hand, deﬁne a transform

(u1), . . . , F −1
Zp

(up)) to the input distribution and the trained

q(u) = f ˆθ ◦ F −1

Z (u), u ∈ (0, 1)p.

Based on the RQMC point set ˜Pngen = {˜v1, . . . , ˜vngen} of size ngen, we can then obtain quasi-random
samples by

(compare with (2)) and deﬁne a GMMN RQMC estimator of (3) by

Yi = q(˜vi),

i = 1, . . . , ngen,

ˆµNN
ngen =

1
ngen

ngen
X

i=1

1
ngen

ngen
X

i=1

Ψ(Yi) =

Ψ(q(˜vi)) =

1
ngen

ngen
X

i=1

Ψ(cid:0)f ˆθ(F −1

Z (˜vi))(cid:1).

We thus have the approximations

E(Ψ(X)) ≈ E(Ψ(Y )) ≈ ˆµNN

ngen.

(7)

(8)

The error in the ﬁrst approximation is small if the GMMN is trained well and the error in the second
approximation is small if the unbiased estimator ˆµNN
ngen has a small variance. The primary bottleneck in
this setup is the error in the ﬁrst approximation which is determined by the size ntrn of the training
dataset and, in particular, by the batch size nbat which is the major factor determining training
eﬃciency of the GMMN we found in all our numerical studies. Given a suﬃciently large nbat and,
by extension, ntrn, the GMMN is trained well, which renders the ﬁrst approximation error in (8)

6

3 GMMN pseudo- and quasi-random samples for copula models

negligible. However, in practice the batch size nbat is constrained by the quadratically increasing
memory demands to compute the MMD cost function of the GMMN. For a theoretical result regarding
this approximation error, see Dziugaite et al. (2015) where a bound on the error between optimizing a
sample version and a population version of MMD(X, Y ) was investigated. Finally, let us note that the
task of GMMN training and generation are separate steps which ensures that, once trained, generating
quasi-random GMMN samples is comparably fast; see Appendix B.

The error in the second approximation in (8) is small if the composite function Ψ ◦ q is suﬃciently
smooth. The transform q is suﬃciently smooth for GMMNs f ˆθ constructed using standard activation
functions and commonly used input distributions; see the discussion following Corollary A.4. Given
gen(log ngen)p−1) for the variance
a suﬃciently smooth Ψ, we can establish a rate of convergence O(n−3
(and O(n−3/2
gen (log ngen)(p−1)/2) for the approximation error) of the GMMN RQMC estimator ˆµNN
ngen
constructed using scrambling as randomization; see Appendix A.4.1. With a stronger assumption
on the behavior of the composite function Ψ ◦ q, we can show that the Koksma–Hlawka bound on
i=1 Ψ(q(vi)) and E(Ψ(Y ))
the error between the (non-randomized) GMMN QMC estimator
is satisﬁed which in turn implies a rate of convergence O(n−1
gen(log ngen)p) for the (non-randomized)
GMMN QMC estimator; see Appendix A.2. If the Koksma–Hlawka bound holds, we can also establish
a rate of convergence O(n−2
gen(log ngen)2p) for the variance of GMMN RQMC estimators constructed
using the digital shift method as randomization technique; see Appendix A.4.2.

Pngen

1
ngen

3 GMMN pseudo- and quasi-random samples for copula models

In this section we assess the quality of pseudo-random samples and quasi-random samples generated
from GMMNs. In both cases we train GMMNs on pseudo-random samples U1, . . . , Untrn ∼ C from
the respective copula C. We start by addressing key implementation details and hyperparameters
of Algorithm 2.1 that we used in all examples thereafter. By utilizing this algorithm to train fθ for
a wide variety of copula families, we then investigate the quality of the samples Y1, . . . , Yngen, once
generated by Algorithm 2.2 and once by Algorithm 2.3.

3.1 GMMN architecture, choice of kernel and training setup

We ﬁnd a single hidden layer architecture (L = 1) to be suﬃcient for all the examples we considered.
This is because, in this paper, we largely consider the cases of d ∈ {2, ..., 10}. Learning an entire
distribution nonparametrically for d > 10 would most likely require L > 1, but it would also require
a much larger sample size ntrn and become much more challenging computationally for GMMNs —
(cid:1) evaluations. After experimentation, we
recall from Section 2.2 that the cost function requires (cid:0)ntrn
2
ﬁx d1 = 300, φ1 to be ReLU (it oﬀers computational eﬃciency via non-expensive and non-vanishing
gradients) and φ2 to be sigmoid (to obtain outputs in [0, 1]d).

To avoid the need of ﬁne-tuning the bandwidth parameter, we follow Li et al. (2015) and use a
mixture of Gaussian kernels with diﬀerent bandwidth parameters as our kernel function for the MMD
statistic in (6); speciﬁcally,

K(x, y) =

K(x, y; σi),

(9)

where nkrn denotes the number of mixture components and K(x, y; σ) = exp(−kx − yk2
2/(2σ2)) is the
Gaussian kernel with bandwidth parameter σ > 0. After experimentation, we ﬁx nkrn = 6 and choose
(σ1, . . . , σ6) = (0.001, 0.01, 0.15, 0.25, 0.50, 0.75); note that copula samples are in [0, 1]d.

nkrnX

i=1

7

3 GMMN pseudo- and quasi-random samples for copula models

Unless otherwise speciﬁed, we use the following setup across all examples. We use ntrn = 60 000
training data points and ﬁnd this to be suﬃciently large to obtain reliable f ˆθ. As dimension of the
input distribution FZ, we choose p = d, that is, the GMMN fθ is set to be a d-to-d transformation.
For FZ, we choose Z ∼ N(0, Id), where Id denotes the identity matrix in Rd×d, so Z consists of
independent standard normal random variables; this choice worked better than U(0, 1)d in practice
despite the fact that N(0, Id) does not satisfy the assumptions of Proposition A.1. We choose a batch
size of nbat = 5000 in Algorithm 2.1; this decision is motivated from a practical trade-oﬀ that a small
nbat will lead to poor estimates of the population MMD cost function but a large nbat will incur
quadratically growing memory requirements due to (6). As the number of epochs we choose nepo = 300
which is generally suﬃcient in our experiments to obtain accurate results. The tuning parameters of
the Adam optimizer is set to the default values reported in Kingma and Ba (2014).

All results in this section, Section 4 and Appendix A.4.2 are reproducible with the demo GMMN_QMC_paper

of the R package gnn. Our implementation utilizes the R packages keras and tensorflow which serve
as R interfaces to the corresponding namesake Python libraries. Furthermore, all GMMN training
is carried out on one NVIDIA Tesla P100 GPU. To generate the RQMC point set in Algorithm 2.3,
we use scrambled nets (Owen, 1995); see also Appendix A.3. Speciﬁcally, we use the implementation
sobol(, randomize = "Owen") from the R package qrng. Finally, our choice of R as programming
language for this work was motivated by the fact that contributed packages providing functionality for
copula modeling and quasi-random number generation — two of the three major ﬁelds of research
(besides machine learning) this work touches upon — exist in R.

3.2 Visual assessments of GMMN samples

In this section we primarily focus on the bivariate case but include an example involving a trivariate
copula; for higher-dimensional copulas, see Sections 3.3 and 4. For all one-parameter copulas considered,
the single parameter will be chosen such that Kendall’s tau, denoted by τ , is equal to 0.25 (weak
dependence), 0.50 (moderate dependence) or 0.75 (strong dependence); clearly, this only applies to
copula families where there is a one-to-one mapping between the copula parameter and τ .

3.2.1 t, Archimedean copulas and their associated mixtures

First, we consider Student t copulas, Archimedean copulas, and their mixtures.

ν (u1), . . . , t−1

Student t copulas are prominent members of the elliptical class of copulas and are given by
C(u) = tν,P (t−1
ν (ud)), u ∈ [0, 1]d, where tν,P denotes the distribution function of the
d-dimensional t distribution with ν degrees of freedom, location vector 0 and correlation matrix P ,
and t−1
ν denotes the quantile function of the univariate t distribution with ν degrees of freedom. For
all t copulas considered in this work, we ﬁx ν = 4. Student t copulas have explicit inverse Rosenblatt
transforms, so one can utilize the CDM for generating quasi-random samples from them (Cambou
et al., 2017).

Archimedean copulas are copulas of the form

C(u) = ψ(ψ−1(u1), . . . , ψ−1(ud)), u ∈ [0, 1]d,

for an Archimedean generator ψ which is a continuous, decreasing function ψ : [0, ∞] → [0, 1] that
satisﬁes ψ(0) = 1, ψ(∞) = limt→∞ ψ(t) = 0 and that is strictly decreasing on [0, inf t : ψ(t) = 0].
Examples of Archimedean generators include ψC(t) = (1 + t)−1/θ (for θ > 0) and ψG(t) = exp(−t1/θ)
(for θ ≥ 1), generating Clayton and Gumbel copulas, respectively. While the inverse Rosenblatt
transform and thus the CDM is available analytically for Clayton copulas, this is not the case for

8

3 GMMN pseudo- and quasi-random samples for copula models

Gumbel; in Appendix B we used numerical root ﬁnding to include the latter case for the purpose of
timings only.

We additionally consider equally-weighted two-component mixture copulas in which one component
is a 90-degree-rotated t4 copula with τ = 0.50 and the other component is either a Clayton copula
(τ = 0.50) or a Gumbel copula (τ = 0.50). The two mixture copula models are referred to as
Clayton-t(90) and Gumbel-t(90) copulas, respectively.

The top rows of Figures 1–3 display contour plots of true t, Clayton and Gumbel copulas respectively,
with τ = 0.25 (left), 0.50 (middle) and 0.75 (right) along with contours of empirical copulas based
on GMMN pseudo-random and GMMN quasi-random samples corresponding to each true copula
C. The top row of Figure 4 displays similar plots for Clayton-t(90) (left) and Gumbel-t(90) (right)
copulas. In each plot, across all ﬁgures described above, we observe that the contour of the empirical
copula based on GMMN pseudo-random samples is visually fairly similar to the contour of C, thus
indicating that the 11 GMMNs have been trained suﬃciently well. We also see that the contours of
the empirical copulas based on GMMN quasi-random samples better approximate the contours of C
than the contours of the empirical copulas based on the corresponding pseudo-random samples. This
observation indicates that, at least visually, the 11 GMMN transforms (corresponding to each C) have
preserved the low-discrepancy of the input RQMC point sets.

The bottom rows of Figures 1–4 display Rosenblatt transformed GMMN quasi-random samples,
corresponding to each of the 11 true copulas C under consideration. The Rosenblatt transform
for a bivariate copula C maps (U1, U2) ∼ C to (R1, R2) = (U1, C2|1(U2 | U1)), where C2|1(u2 | u1)
denotes the conditional distribution function of U2 given U1 = u1 under C. We exploit the fact
that (R1, R2) ∼ U(0, 1)2 if and only if (U1, U2) ∼ C. Moreover, Rosenblatt transforming the GMMN
quasi-random samples should yield a more homogeneous coverage of [0, 1]2. From each of the scatter
plots in Figures 1–4, we observe no signiﬁcant departure from U(0, 1)2, thus indicating that the
GMMNs have learned suﬃcient approximations to the corresponding true copulas C. Furthermore, the
lack of gaps or clusters in the scatter plots provides some visual conﬁrmation of the low-discrepancy of
the Rosenblatt-transformed GMMN quasi-random samples.

3.2.2 Nested Archimedean, Marshall–Olkin and mixture copulas

Next, we consider more complex copulas such as nested Archimedean copulas and Marshall–Olkin
copulas. We also re-consider the two mixture copulas introduced in the previous section along with an
additional mixture copula. To better showcase the complexity of these dependence structures, we use
scatter plots instead of contour plots to display copula and GMMN-generated samples. We omit the
plots containing the Rosenblatt transformed samples since they are harder to obtain for the copulas
we investigate in this section.

Nested Archimedean copulas (NACs) are Archimedean copulas with arguments possibly replaced
by other NACs; see McNeil (2008) or Hofert (2012). In particular, this class of copulas allows us to
construct asymmetric extensions of Archimedean copulas. Important to note here is that NACs are
copulas for which there is no known (tractable) CDM. To demonstrate the ability of GMMNs to capture
such dependence structures, we consider the simplest three-dimensional copula for visualization and
investigate higher-dimensional NACs in Sections 3.3 and 4. The three-dimensional NAC we consider
here is

C(u) = C0(C1(u1, u2), u3), u ∈ [0, 1]3,

(10)

where C0 is a Clayton copula with Kendall’s tau τ0 = 0.25 and C1 is a Clayton copula with Kendall’s
tau τ1 = 0.50. In Sections 3.3 and 4, we will present examples of ﬁve- and ten-dimensional NACs.

9

3 GMMN pseudo- and quasi-random samples for copula models

Figure 1 Top row contains contour plots of true t4 copulas with τ = 0.25 (left), 0.50 (middle) and 0.75
(right) along with the corresponding contour plots of empirical copulas based on GMMN
pseudo-random and GMMN quasi-random samples (respectively, GMMN PRS and GMMN
QRS), both of size ngen = 1000. Bottom row contains Rosenblatt-transformed GMMN QRS
corresponding to the same three t4 copulas.

10

3 GMMN pseudo- and quasi-random samples for copula models

Figure 2 Top row contains contour plots of true Clayton copulas with τ = 0.25 (left), 0.50 (middle)
and 0.75 (right) along with the corresponding contour plots of empirical copulas based on
GMMN PRS and GMMN QRS, both of size ngen = 1000. Bottom row contains Rosenblatt-
transformed GMMN QRS corresponding to the same three Clayton copulas.

11

3 GMMN pseudo- and quasi-random samples for copula models

Figure 3 Top row contains contour plots of true Gumbel copulas with τ = 0.25 (left), 0.50 (middle)
and 0.75 (right) along with the corresponding contour plots of empirical copulas based on
GMMN PRS and GMMN QRS, both of size ngen = 1000. Bottom row contains Rosenblatt-
transformed GMMN QRS corresponding to the same three Gumbel copulas.

12

3 GMMN pseudo- and quasi-random samples for copula models

Figure 4 Top row contains contour plots of true Clayton-t(90) (left) and Gumbel-t(90) (right) mixture
copulas along with the corresponding contour plots of empirical copulas based on GMMN PRS
and GMMN QRS, both of size ngen = 1000. Bottom row contains Rosenblatt-transformed
GMMN QRS corresponding to the same two mixture copulas.

13

3 GMMN pseudo- and quasi-random samples for copula models

Bivariate Marshall–Olkin copulas are of the form

C(u1, u2) = min{u1−α1

u2, u1u1−α2

},

1

2

u1, u2 ∈ [0, 1],

where α1, α2 ∈ [0, 1]. A notable feature of Marshall–Olkin copulas is that they have both an absolutely
continuous component and a singular component. In particular, the singular component is determined
by all points which satisfy uα1
2 . Accurately capturing this singular component may present a
diﬀerent challenge for GMMNs, which is why we included this copula despite the fact that there also
exists a CDM for this copula; see Cambou et al. (2017). As an example for visual assessment, we
consider a Marshall–Olkin copula with α1 = 0.75 and α2 = 0.60.

1 = uα2

We also consider three mixture models, all of which are equally weighted two-component mixture
copulas with one component being a 90-degree-rotated t4 copula with τ = 0.50. The ﬁrst two models are
the Clayton-t(90) and Gumbel-t(90) mixture copulas as previously introduced. The second component
in the third model is a Marshall–Olkin copula with parameters α1 = 0.75 and α2 = 0.60. We refer to
this third model as the MO-t(90) copula.

Figures 5–7 display pseudo-random samples (left column) from a (2, 1)-nested Clayton copula as
in (10), a MO copula, and the three mixture copulas, respectively, along with GMMN pseudo-random
samples (middle column) and GMMN quasi-random samples (right column) corresponding to each
copula C. The similarity between the GMMN pseudo-random samples in the middle column and the
pseudo-random samples in the left column indicate that the copulas C were learned suﬃciently well by
their corresponding GMMNs. Note that in the case of the nested Clayton copula example, we can only
comment on how well the bivariate margins of the copula C were learned. From the right columns,
we can mainly observe that the GMMN quasi-random samples contain less gaps and clusters when
compared with the corresponding pseudo-random and GMMN pseudo-random samples. The fact that
GMMNs were capable of learning the main features, including the singular components, of the MO
copula and the MO-t(90) mixture copula is particularly noteworthy given how challenging it seems to
be to learn a Lebesgue null set from a ﬁnite amount of samples.

Figure 5 Pseudo-random samples (PRS; left), GMMN pseudo-random samples (GMMN PRS; middle)
and GMMN quasi-random samples (GMMN QRS; right), all of size ngen = 1000, from a
(2,1)-nested Clayton copula as in (10) with τ0 = 0.25 and τ1 = 0.50.

3.3 Assessment of GMMN samples by the Cramér-von Mises statistic

After a purely visual inspection of the generated samples, we now assess the quality of GMMN
pseudo-random and GMMN quasi-random samples more formally with the help of a goodness-of-ﬁt

14

3 GMMN pseudo- and quasi-random samples for copula models

Figure 6 PRS (left), GMMN PRS (middle) and GMMN QRS (right), all of size ngen = 1000, from a
Marshall–Olkin copula with α1 = 0.75 and α2 = 0.60 (Kendall’s tau equals 0.5).

statistic. Since bivariate copulas have been investigated in detail in the previous section, we focus on
higher-dimensional copulas in this section.

Speciﬁcally, we use the Cramér–von Mises statistic (Genest, Rémillard, et al., 2009),

Z

Sngen =

[0,1]d

ngen(Cngen(u) − C(u))2 dCngen(u),

where the empirical copula

Cngen(u) =

1{ ˆUi1 ≤ u1, . . . , ˆUid ≤ ud}, u ∈ [0, 1]d,

(11)

1
ngen

ngen
X

i=1

is the empirical distribution function of the pseudo-observations. For ngen = 1000 and each copula C,
we compute B = 100 realizations of Sngen three times — once for the case where ˆUi, i = 1, . . . , ngen, are
pseudo-observations of the true underlying copula (as benchmark), once for GMMN pseudo-random
samples and once for GMMN quasi-random samples. We then use box plots to depict the distribution
of Sngen in each case. Figure 8 displays these box plots for t4 (top row), Clayton (middle row) and
Gumbel (bottom row) copulas of dimensions d = 5 (left column), d = 10 (right column) and τ = 0.50.
Similarly, Figure 9 displays such box plots for d-dimensional nested Clayton (left column) and nested
Gumbel (right column) copulas for d = 3 (top row), d = 5 (middle row) and d = 10 (bottom row).
The three-dimensional NACs have a structure as given by (10) with τ0 = 0.25 and τ1 = 0.50; the ﬁve-
dimensional NACs have structure C0(C1(u1, u2), C2(u3, u4, u5)) with corresponding τ0 = 0.25, τ1 = 0.50
and τ2 = 0.75; and the ten-dimensional NACs have structure C0(C1(u1, . . . , u5), C2(u6, . . . , u10)) with
corresponding τ0 = 0.25, τ1 = 0.50 and τ2 = 0.75.

We can observe from both ﬁgures that the distributions of Sngen for pseudo-random samples from
C and from the GMMN are similar, with slightly higher Sngen values for the GMMN pseudo-random
samples, especially for d = 10. Additionally, we can observe that the distribution of Sngen based on the
GMMN quasi-random samples is closer to zero than that of the GMMN pseudo-random samples. This
provides some evidence that the low-discrepancy of input RQMC points sets has been preserved under
the respective (trained) GMMN transform.

We also see that Sngen values based on the GMMN quasi-random samples are clearly lower than
Sngen values based on the copula pseudo-random samples, with the exception of (some) copulas for
d = 10 where the distributions of Sngen are more similar.

15

3 GMMN pseudo- and quasi-random samples for copula models

Figure 7 PRS (left column), GMMN PRS (middle column) and GMMN QRS (right column), all of
size ngen = 1000, from a Clayton-t(90) (top row), Gumbel-t(90) (middle row) and a MO-t(90)
mixture (bottom row) copula.

16

3 GMMN pseudo- and quasi-random samples for copula models

Figure 8 Box plots based on B = 100 realization of Sngen computed from (i) a pseudo-random sample
(PRS) of C (denoted by Copula PRS), (ii) a GMMN pseudo-random sample (denoted by
GMMN PRS) and (iii) a GMMN quasi-random sample (denoted by GMMN QRS) — all
of size ngen = 1000 — for a t4 (top row), Clayton (middle row) and Gumbel (bottom row)
copula with τ = 0.5 as well as d = 5 (left column) and d = 10 (right column).

17

3 GMMN pseudo- and quasi-random samples for copula models

Figure 9 As Figure 8 but for nested Clayton (left column) and nested Gumbel (right column) copulas

and for d = 3 (top row), d = 5 (middle row) and d = 10 (bottom row).

18

4 Convergence analysis of the RQMC estimator

4 Convergence analysis of the RQMC estimator

In this section we numerically investigate the variance-reduction properties of the GMMN RQMC
ngen in (7) for two functions Ψ and transforms q = f ˆθ ◦ Φ−1 corresponding to diﬀerent
estimator ˆµNN
copulas C. We compare ˆµNN
ngen with estimators based on standard copula pseudo-random and, where
available, copula quasi-random samples. For the latter, we follow Cambou et al. (2017) but note that
quasi-random sampling procedures are only available for some of the copulas we consider here; for
others, the procedures are either too slow (e.g., for Gumbel copulas; see Appendix B) or not known at
all (e.g., for nested Clayton or Gumbel copulas).

We consider two diﬀerent types of functions Ψ. The ﬁrst is a test function primarily used in the
QMC literature to test the performance of ˆµNN
ngen in terms of its ability to preserve the low-discrepancy
of ˜Pngen. The second function Ψ is motivated from a practical application in risk management. For
both functions, standard deviation estimates will be computed to compare convergence rates, based on
B = 25 randomized point sets ˜Pngen for each of ngen ∈ {210, 210.5, . . . , 218} to help roughly gauge the
convergence rate for all estimators. Furthermore, regression coeﬃcients α (obtained by regressing the
logarithm of the standard deviation on the logarithm of ngen) are computed and displayed to allow for
an easy comparison of the corresponding convergence rates O(n−α
gen) with the theoretical convergence
rate O(n−0.5
gen ) of the Monte Carlo estimator’s standard deviation. For RQMC estimators one can
expect α to be larger than 0.5, but with an upper bound of 1.5 − ε, where ε increases with dimension
d; see Theorem A.3 for further details.

4.1 A test function

The test function we consider is the Sobol’ g function (Faure and Lemieux, 2009) based on the
Rosenblatt transform and is given by

Ψ1(U ) =

d
Y

j=1

|4Rj − 2| + j
1 + j

,

where R1 = U1 and, for j = 2, . . . , d and if U ∼ C, Rj = Cj|1,...,j−1(Uj | Uj−1, . . . , U1) denotes the
conditional distribution function of Uj given U1, . . . , Uj−1.

Figure 10 shows plots of standard deviation estimates for estimating E(Ψ1(U )) for t4 copulas (top
row), Clayton (middle row) and Gumbel (bottom row) copulas in dimensions d = 2 (left column),
d = 5 (middle column) and d = 10 (right column). For the t4 and Clayton copulas we numerically
compare the eﬃciency of the GMMN RQMC estimator (with legend label “GMMN QRS”) with the
copula RQMC estimator based on the CDM method (with legend label “Copula QRS”) and the MC
estimator (with legend label “Copula PRS”). For the Gumbel copula, however, the CDM approach
(“Copula QRS”) is computationally not feasible; see Section 3.2.1 and Appendix B. The legend of each
plot also provides the regression coeﬃcient α which indicates the convergence rate of each estimator.
From Figure 10, we observe that the GMMN RQMC estimator clearly outperforms the MC estimator.
Naturally, so does the copula RQMC estimator for the copulas for which it is available. On the one hand,
the rate of convergence of the GMMN RQMC estimator reduces with increasing copula dimensions;
see also the decreasing regression coeﬃcients α when moving from the two- to the ten-dimensional
case. On the other hand, the GMMN RQMC estimator still outperforms the MC estimator.

19

4 Convergence analysis of the RQMC estimator

Figure 10 Standard deviation estimates based on B = 25 replications for estimating E(Ψ1(U )),
expectation of the Sobol’ g function, via MC based on a pseudo-random sample (PRS),
via the copula RQMC estimator (whenever available; rows 1–2 only) and via the GMMN
RQMC estimator. Note that in rows 1–3, d ∈ {2, 5, 10}.

20

5 A real-data example

4.2 An example from risk management practice

Consider modeling the dependence of d risk-factor changes (for example, logarithmic returns) of a
portfolio; see McNeil et al. (2015, Chapters 2, 6 and 7). We now demonstrate the eﬃciency of our
GMMN RQMC estimator by considering the expected shortfall of the aggregate loss, a popular risk
measure in quantitative risk management practice.

Speciﬁcally, if X = (X1, . . . , Xd) denotes a random vector of risk-factor changes with N(0, 1) margins,

the aggregate loss is S = Pd

j=1 Xj. The expected shortfall ES0.99 at level 0.99 of S is given by

ES0.99(S) =

1
1 − 0.99

Z 1

0.99

S (u) du = E(cid:0)S | S > F −1
F −1

S (0.99)(cid:1) = E(Ψ2(X)),

where F −1
model the dependence between the components of X.

S denotes the quantile function of S. As done previously, various copulas will be used to

Figure 11 shows plots of standard deviation estimates for estimating E(Ψ2(X)). The ﬁrst three rows
contain results for the same copula models as considered in Section 4.1. The fourth row contains results
for nested Gumbel copula models with dimension d = 3 (left column), d = 5 (middle column) and
d = 10 (right column). The speciﬁc hierarchical structures and parameterization have been described
earlier in Section 3.3; note that there is no quasi-random sampling procedure known for these copulas.
We can observe from the plots that the GMMN RQMC estimator outperforms the MC estimator.
Similar as before, we see a decrease in the convergence rate of the GMMN RQMC estimator as the
copula dimension increases, although it still outperforms the MC estimator.

5 A real-data example

In this section, we present examples based on real-data to show how our method can be useful in
practice. To this end, we consider applications from ﬁnance and risk management. Such applications
often involve the modeling of dependent multivariate return data in order to estimate various quantities
µ of interest. In this context, utilizing GMMNs for dependence modeling can yield two key advantages.
Firstly, GMMNs are highly ﬂexible and hence can model dependence structures not adequately
captured by prominent parametric copula models; see, e.g., Hofert and Oldford (2018) for the latter
point. Secondly, as demonstrated in Sections 3 and 4, one can readily generate GMMN quasi-random
samples to achieve variance reduction when estimating µ; this is especially advantageous as oftentimes
oversimpliﬁed parametric models are chosen just so that this can be achieved. In this section, we
model asset portfolios consisting of S&P 500 constituents to showcase these advantages. All results are
reproducible with the demo GMMN_QMC_data of the R package gnn.

5.1 Portfolios of S&P 500 constituents

We consider 10 constituent time series from the S&P 500 in the time period from 1995-01-01 to
2015-12-31. The selected constituents include three stocks from the information technology sector —
Intel Corp. (INTC), Oracle Corp. (ORCL) and International Business Machines Corp. (IBM); three
stocks from the ﬁnancial sector — Capital One Financial Corp. (COF), JPMorgan Chase & Co.
(JPM) and American International Group Inc (AIG); and four stocks from the industrial sector — 3M
Company (MMM), Boeing Company (BA), General Electric (GE) and Caterpillar Inc. (CAT). We
also investigate sub-portfolios of stocks with dimensions d = 5 (consisting of INTC, ORCL, IBM, COF
and AIG) and d = 3 (consisting of INTC, IBM and AIG). The data are obtained from the R package
qrmdata.

21

5 A real-data example

Figure 11 Standard deviation estimates based on B = 25 replications for estimating E(Ψ2(X)), the
expected shortfall ES0.99(S), via MC based on a PRS, via the copula RQMC (whenever
available; rows 1–2 only) and via the GMMN RQMC estimator. Note that in rows 1–3,
d ∈ {2, 5, 10}, whereas in row 4, d ∈ {3, 5, 10}.

22

5 A real-data example

To account for marginal temporal dependencies, we follow the copula–GARCH approach (Jondeau
and Rockinger, 2006; Patton, 2006) and model each marginal time series of log-returns by a ARMA(1, 1)–
GARCH(1, 1) model with standardized t innovation distributions (deGARCHing). We then extract the
marginal standardized residuals (i.e., the realizations of the standardized t innovations) and compute,
for each of the three portfolios, their pseudo-observations for the purpose of modeling the cross-sectional
dependence among the corresponding portfolio’s log-return series.

5.2 Assessing the ﬁt of dependence models

As models for the pseudo-observations of each of the three portfolios we use prominent parametric
copulas (Gumbel, Clayton, exchangeable normal, unstructured normal, exchangeable t and unstructured
t) and GMMNs of the same architecture and with the same training setup as detailed in Section 3.1.
The rather small number of training data points (ntrn = 5287) allows us to use nbat = ntrn here and
hence directly train with the entire dataset. All parametric copulas are ﬁtted using the maximum
pseudo-likelihood method; see Hofert, Kojadinovic, et al. (2018, Section 4.1.2).

To evaluate the ﬁt of a dependence model, we use a Cramér-von-Mises type test statistic introduced
by Rémillard and Scaillet (2009) to assess the equality of two empirical copulas. This statistic is
deﬁned as

Sntrn,ngen =

Z

(cid:18)s

[0,1]d

1
ngen

+

1
ntrn

(cid:19)−1(cid:18)

Cngen(u) − Cntrn(u)

du,

(cid:19)2

where Cngen(u) and Cntrn(u) are the empirical copulas, deﬁned according to (11), of the ngen samples
generated from the ﬁtted dependence model and the ntrn pseudo-observations used to ﬁt the dependence
model, respectively. For how Sntrn,ngen is evaluated, see Rémillard and Scaillet (2009, Section 2).

For each of the three portfolios and each of the seven dependence models considered, we compute
B realizations of Sntrn,ngen based on ngen = 10 000 pseudo-random samples generated from the ﬁtted
dependence model and the ntrn = 5287 pseudo-observations of each portfolio considered. Figure 12
displays box plots depicting the distribution of Sntrn,ngen for each portfolio and dependence model.
Across all three portfolios, we can observe that the distribution of Sntrn,ngen based on GMMN models
is concentrated closer to zero than those of each ﬁtted parametric copula. In fact, the diﬀerence in
distributions of Sntrn,ngen realizations between GMMN models and the best parametric copula model
(a t-copula with unstructured correlation matrix) is most noticeable for d = 10, where an adequate ﬁt
becomes more challenging for these parametric copulas. For each of the three portfolios, a GMMN
provides the best ﬁt. Hence, we use these ﬁtted GMMNs to model the underlying dependence structure
for the three portfolios in each of three applications considered next.

5.3 Assessing the variance reduction eﬀect

In three applications we study the variance reduction eﬀect of our GMMN RQMC estimator ˆµNN
ngen
computed from quasi-random samples in comparison to the GMMN MC estimator ˆµNN,MC
computed
from pseudo-random samples.

ngen

Our ﬁrst application concerns the estimation of the expected shortfall µ = ES0.99(S) for S = Pd

j=1 Xj
as in Section 4.2, where the margins of X = (X1, . . . , Xd) are now the ﬁtted standardized t distributions
as obtained by deGARCHing and the dependence structure is the previously ﬁtted GMMN. This
is a classical task in risk management practice according to the Basel guidelines. As a second
application we consider a capital allocation problem which concerns estimating how to allocate an
amount of risk capital (e.g., computed as ES0.99(S)) to each of d business lines. Without loss of

23

5 A real-data example

Figure 12 Box plots based on B = 100 realizations of Sntrn,ngen computed for portfolios of dimensions
d = 3 (left), d = 5 (middle) and d = 10 (right) and for each ﬁtted dependence model using
a pseudo-random sample of size ngen = 10 000 from each corresponding ﬁtted model.

generality, we consider one business line, the ﬁrst, and estimate the expected shortfall contribution
µ = AC1,0.99 = E(X1 | S > F −1
S (0.99)) according to the Euler principle; see McNeil et al. (2015,
Section 8.5). Our third application comes from ﬁnance and concerns the estimation of the expected
payoﬀ µ = E(exp(−r(T − t)) max{(Pd
j=1 ST,j) − K, 0}) of a European basked call option, where r
denotes the continuously compounded annual risk-free interest rate, t denotes the current time point, T
the maturity in years and K the strike price. We assume a Black–Scholes framework for the marginal
stock prices (ST,1, . . . , ST,d) at maturity T , so ST,j ∼ LN(log(St,j)+(r −σ2
j (T −t)), where
St,j denotes the last available stock price of the jth constituent (i.e., the close price on 2015-12-31) and
σj denotes the volatility of the jth constituent (estimated by the standard deviation of its log-returns
over the time period from 2014-01-01 to 2015-12-31). The dependence structure of (ST,1, . . . , ST,d) is
modeled by the previously ﬁtted GMMN. Furthermore, we chose t = 0 to be the last available point in
the data period considered (i.e., 2015-12-31), T = 1 and r = 0.01. The strike prices K were chosen
about 0.5% higher than the average stock price of all stocks in the respective portfolio at t = 0.

j /2)(T −t), σ2

ngen

(with x-axis label “GMMN PRS”) and of ˆµNN

For each of the three portfolios and for each of the three expectations µ considered, we compute
and the GMMN RQMC estimator ˆµNN
B = 200 realizations of the GMMN MC estimator ˆµNN,MC
ngen,
using ngen = 105 samples for both estimators. Figure 13 displays box plots of these realizations of
ˆµNN,MC
ngen (with x-axis label “GMMN QRS”) for ES0.99
ngen
(left column), AC1,0.99 (middle column), the expected payoﬀ of the basket call option (right column)
and for the portfolio in dimension d = 3 (top row), d = 5 (middle row) and d = 10 (bottom row).
Additionally, to quantify the variance reduction eﬀect of ˆµNN
, we report in the secondary
y-axis of each box plot the estimated variance reduction factor (VRF) — namely, the sample variance
of ˆµNN,MC
ngen — and the corresponding improvement in percentages.
ngen
ngen is able to reduce the variance in all considered applications and
across all dimensions. While variance reduction is diminished in higher dimensions (d = 10), the GMMN
RQMC estimator is still immensely useful in estimating expectations µ for three reasons. Firstly, as
demonstrated in the previous section, GMMNs best ﬁt the underlying dependence structure of the
data. Secondly, unlike many parametric copulas, we can generate quasi-random samples independent of
the type of dependence structure observed in the data. Finally, we can generate GMMN quasi-random
samples at no additional cost over GMMN pseudo-random samples; see also Appendix B.

From Figure 13, we observe that ˆµNN

over the sample variance of ˆµNN

ngen over ˆµNN,MC

ngen

24

5 A real-data example

Figure 13 Box plots based on B = 200 realizations of the GMMN MC estimator ˆµNN,MC

(label
“GMMN PRS”) and the GMMN RQMC estimator ˆµNN
ngen (label “GMMN QRS”) of ES0.99
(left column), AC1,0.99 (middle column) and the expected payoﬀ of a basket call (right
column) for portfolio with dimensions d = 3 (top row), d = 5 (middle row) and d = 10
(bottom), using ngen = 105 samples for both estimators.

ngen

25

6 Discussion

6 Discussion

This work has been inspired by the simple question of how to obtain quasi-random samples for a
large variety of multivariate distributions. Until recently, this was only possible for a few multivariate
distributions with speciﬁc underlying copulas.
In general, for the vast majority of multivariate
distributions, obtaining quasi-random samples is a hard problem (Cambou et al., 2017). Our approach
based on GMMNs provides a ﬁrst universal method for doing so.
It depends on ﬁrst learning a
generator f ˆθ such that, given Z (with independent components from some known distribution such
as the standard uniform or standard normal), f ˆθ(Z) follows the targeted multivariate distribution.
Conditional on this ﬁrst step being successful, we can then replace Z with F −1
Z (˜vi), i = 1, . . . , ngen,
where {˜v1, . . . , ˜vngen} is an RQMC point set, to generate quasi-random samples from X.

It is generally diﬃcult to assess the low-discrepancy property of non-uniform quasi-random samples.
To evaluate the quality of our GMMN quasi-random samples, we used visualization tools (Section 3.2),
goodness-of-ﬁt statistics (Section 3.3), and investigated variance reduction eﬀects (Section 4) when
estimating µ = E(Ψ(X)) for a test function and for expected shortfall. As dependence structures
among the components of X, we included various known copulas, some of which allowed for quasi-
random sampling which allowed us to statistically assess the performance of our GMMN quasi-random
samples. However, we emphasize that the key feature of our method is that, given a suﬃciently large
dataset with dependence structure not well described by any known parametric copula model for
which quasi-random sampling is available, we are now able to generate quasi-random samples from
its empirical distribution. We demonstrated this with a real dataset in Section 5. Not only does
a GMMN provide the best ﬁtting model in this application, allowing us to avoid the tedious and
often computationally challenging search that is typically required in classical copula modeling for an
adequate dependence model, we also obtain, at no additional cost, quasi-random samples from this
GMMN — a whole other challenge in classical copula modeling. This universality and computability
is an attractive feature of GMMNs for multivariate modeling.

However, this does not mean that the problem of quasi-random sampling for multivariate distributions
is completely solved. In high dimensions learning an entire distribution is a hard problem, and so is
learning the generator f ˆθ. At a superﬁcial level, the literature on generative NNs — and the many
headlines covering them — may give the impression that such NNs are now capable of generating
samples from very high-dimensional distributions. This, of course, is not true; see, for example,
Arjovsky et al. (2017), S. Arora et al. (2018), and Tolstikhin et al. (2017). In particular, while available
evidence is convincing that any speciﬁc generated sample f ˆθ(Z1), typically an image, can be very
realistic in the sense that it looks just like a typical training sample, this is not the same as saying
that the entire collection of generated samples {f ˆθ(Z1), f ˆθ(Z2), . . . } will have the same distribution as
the training sample. The latter is a much harder problem to solve. Unlike widely cited generative NNs
such as variational autoencoders and generative adversarial networks, GMMNs are capable of learning
entire distributions, because they rely on the MMD-criterion as the cost function rather than, for
example, the mean squared error which does not measure the discrepancy between entire distributions.
Even so, this still does not mean GMMNs are practical for very high dimensions yet, simply because
the fundamental curse of dimensionality cannot be avoided easily. At the moment, it is simply not
realistic to hope that one can learn an entire distribution in high dimensions from a training sample of
only moderate size.

Going forward there are two primary impediments to quasi-random sampling from higher-dimensional
copulas and distributions. Firstly, the problem of distribution learning via generative NNs remains
a challenging task. We may also consider using other goodness-of-ﬁt statistics for multivariate
distributions rather than the MMD as the cost function (provided that the statistic is diﬀerentiable in

26

A Analyzing GMMN QMC and GMMN RQMC estimators

order to train a generative NN). Secondly, we discovered from our empirical investigation in Section 4
that the convergence rates of GMMN RQMC estimators decrease with increasing dimension. Preserving
the low-discrepancy of RQMC point sets upon transformations in high dimensions remains an open
problem in this regard.

A Analyzing GMMN QMC and GMMN RQMC estimators

A.1 QMC point sets

The idea behind quasi-random numbers is to replace pseudo-U (0, 1)p random numbers with low-
discrepancy points sets Pngen to produce a more homogeneous coverage of [0, 1]p in comparison to
pseudo-random numbers. That is, with respect to a certain discrepancy measure, the empirical
distribution of the Pngen is closer to the uniform distribution U(0, 1)p than a pseudo-random sample.
Established notions of the discrepancy of a point set Pngen are as follows. The discrepancy function

of Pngen in an interval I = [0, b) = Qp

j=1[0, bj), bj ∈ (0, 1], j = 1, . . . , p, is deﬁned by

where λ(I) is the p-dimensional Lebesgue measure of I. Thus the discrepancy function is the diﬀerence
between the number of points of Pngen in I and the probability of a p-dimensional standard uniform
random vector to fall in I. For A = {[0, b) : b ∈ (0, 1]p}, the star discrepancy of Pngen is deﬁned by

D(I; Pngen) =

1{vi∈I} − λ(I),

1
ngen

ngen
X

i=1

D∗(Pngen) = sup
I∈A

|D(I; Pngen)|.

If Pngen satisﬁes the condition D∗(Pngen) ∈ O(n−1
(Lemieux, 2009, p. 143).

gen(log ngen)p), it is called a low-discrepancy sequence

There are diﬀerent approaches to construct (deterministic) low-discrepancy sequences; see Lemieux
(2009, Chapters 5–6). The two main approaches involve either lattices (grids which behave well under
projections) or digital nets/sequences. In our numerical investigations presented in Sections 3–4, we
worked with a type of digital net constructed using the Sobol’ sequence; see Sobol’ (1967).

A.2 Analyzing the GMMN QMC estimator

In this section, we will derive conditions under which the (non-randomized) GMMN QMC estimator

1
ngen

ngen
X

i=1

Ψ(q(vi)) =

h(vi),

1
ngen

ngen
X

i=1

where q = f ˆθ ◦ F −1
the following analysis, we need to further assume that supp(FX ) and supp(FY ) are bounded.
The Koksma–Hlawka inequality (Niederreiter, 1992) for a function g : [0, 1]p → R says that

Z , has a small error when approximating E(Ψ(Y )). In

Z and h = Ψ ◦ q = Ψ ◦ f ˆθ ◦ F −1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
ngen

ngen
X

i=1

(cid:12)
(cid:12)
g(vi) − E(g(U 0))
(cid:12)
(cid:12)
(cid:12)

≤ V (g)D∗(Pngen),

where U 0 ∼ U(0, 1)p and the variation V (g) is understood in the sense of Hardy and Krause; we refer to
the right-hand side of the inequality as Koksma–Hlawka bound. From this Koksma–Hlawka inequality,

27

A Analyzing GMMN QMC and GMMN RQMC estimators

1
ngen

we can establish that if g has ﬁnite bounded variation, that is V (g) < ∞, then the convergence rate
for

i=1 g(vi) is determined by D∗(Pngen) = O(n−1

ngen(log ngen)p).

Pngen

Pngen

We can use the Koksma–Hlawka inequality to analyze the convergence of the GMMN QMC estimator
1
i=1 Ψ(yi) of E(Ψ(Y )), where yi = q(vi), i = 1, . . . , ngen and Y ∼ FY , by establishing the
ngen
conditions under which V (h) is bounded. To that end, consider the following proposition.

Proposition A.1 (Suﬃcient conditions for ﬁniteness of the Koksma–Hlawka bound)
Assume that supp(FY ) is bounded and all appearing partial derivatives of q and Ψ exist and are
continuous. Consider q = f ˆθ ◦ F −1
Z , the point set Pngen = {v1, . . . , vngen} ⊆ [0, 1)p and let yi = q(vi),
i = 1, . . . , ngen, denote the GMMN quasi-random sample. Suppose that
1) Ψ(y) < ∞ for all y ∈ supp(FY ) and

∂|β|1Ψ(y)
∂β1y1 . . . ∂βdyd

< ∞, y ∈ supp(FY ),

for all β = (β1, . . . , βd) ⊆ {0, . . . , d}d and |β|1 = Pd

j=1 βj ≤ d;

2) there exists an M > 0 such that | Dk F −1
Zj

k-fold derivative of its argument;

| ≤ M , for each k, j = 1, . . . , p, where Dk denotes the

3) there exists, for each layer l = 1, . . . , L + 1 of the NN f ˆθ, an Nl > 0 such that | Dk φl| ≤ Nl for all

k = 1, . . . , p; and

4) the parameter vector ˆθ = (cW1, . . . , cWL+1, ˆb1, . . . , ˆbL+1) of the ﬁtted NN is bounded.
Then there exists a constant c independent of ngen, but possibly depending on Ψ, ˆθ, M and
N1, . . . , NL+1, such that

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
ngen

ngen
X

i=1

Ψ(yi) − E(Ψ(Y ))

≤ cD∗(Pngen).

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Proof. To begin with note that for any q such that q(U 0) ∼ FY , we know that Y is in distribution
equal to q(U 0) and thus E(Ψ(Y )) = E(cid:0)Ψ(q(U 0))(cid:1) = E(h(U 0)). Based on this property, we can obtain
the Koksma–Hlawka bound V (h)D∗(Pngen) for the change of variable h.

Following Lemieux (2009, Section 5.6.1), we can derive an expression for V (h). To this end, let

V (j)(h; α) =

Z

[0,1)j

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂jh(α)(vα1, . . . , vαj )
∂vαj . . . ∂vα1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

dvα1 . . . dvαj ,

where h(α)(vα1, . . . , vαj ) = h(˜v1, . . . , ˜vp) for ˜vk = vk if k ∈ {α1, . . . , αj} and ˜vk = 1 otherwise. Then

V (h) =

V (j)(h; α),

p
X

X

j=1

α:|α|1=j

(12)

where the inner sum is taken over all α = (α1, . . . , αj) with {α1, . . . , αj} ⊆ {1, . . . , p} — see also
Niederreiter (1992, pp. 19–20), E. Hlawka (1961, eq. (4)) and Hlawka and Mück (1972, eq. (4’)).
Following Hlawka and Mück (1972) and Constantine and Savits (1996, Theorem 2.1), we then have

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂jh(α)(vα1, . . . , vαj )
∂vαj . . . ∂vα1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= X
1≤|β|1≤j

∂|β|1Ψ
∂β1y1 . . . ∂βdyd

j
X

i=1

X

cκ
(κ,k)∈πi(κ,k)

i
Y

∂|κm|1q(α)
km

(vα1, . . . , vαj )

∂κmj vαj . . . ∂κm1vα1

m=1

,

(13)

28

A Analyzing GMMN QMC and GMMN RQMC estimators

where β ∈ Nd
0 and where πi(κ, k) denotes the set of pairs (κ, k) such that k = (k1, . . . , ki) ∈ {1, . . . , d}i
and κ = (κ1, . . . , κi) with κm = (κm1, . . . , κmj) ∈ {0, 1}j, m = 1, . . . , i, and Pi
m=1 κmi = 1 for
i = 1, . . . , j; see Constantine and Savits (1996) for more details on πi(κ, k) and the constants
cκ. Furthermore, for index j = 1, . . . , d, q(α)
(vα1, . . . , vαj ) = qj(˜v1, . . . , ˜vp) and qj(˜v1, . . . , ˜vp) =
φL+1(cWL+1j·aL + ˆbL+1), where al = φl(cWlal−1 + ˆbl) for l = 1, . . . , L with a0 = F −1
Z (˜v) and where
cWL+1j· denotes the jth row of WL+1.

j

Based on the decomposition in (13), a suﬃcient condition to ensure that V (h) < ∞ is that all

products of the form

∂|β|1Ψ
∂β1y1 . . . ∂βdyd

i
Y

∂|κm|1q(α)
km

(vα1, . . . , vαj )

∂κmj vαj . . . ∂κm1vα1

m=1

,

i = 1, . . . , j,

are integrable.

To that end, Assumptions 2)–4) imply that all mixed partial derivatives of q = f ˆθ ◦ F −1

Z are bounded.
By the assumption of continuous partial derivatives of q, this implies that ﬁnite products of the form

i
Y

∂|κm|1q(α)
km

(vα1, . . . , vαj )

∂κmj vαj . . . ∂κm1vα1

m=1

,

i = 1, . . . , j,

are integrable. By Assumption 1), decomposition (13) and Hölder’s inequality, the quantity in (12) is
bounded. This implies that h has bounded variation, so that the Koksma–Hlawka bound is ﬁnite.

The following remark provides insights into Assumptions 2)–4) of Proposition A.1.

Remark A.2
U(a, b)p for a < b, which is a popular choice for the input distribution, clearly satisﬁes Assumption 2)
in Proposition A.1. Assumption 3) is satisﬁed for various commonly used activation functions, such as:
1) Sigmoid. If φl(x) = 1/(1 + e−x) for layer l, then Nl = 1.
2) ReLU. If φl(x) = max{0, x} for layer l, then Nl = 1. In this case, only the ﬁrst derivative is (partly)
non-zero. Additionally, note that the ReLU activation function is not diﬀerentiable at x = 0.
However, even if φl = max{0, x} for all l = 1, . . . , L + 1, the set of all pointwise discontinuities of
the mixed partial derivatives of q is a null set. Hence, the discontinuities do not jeopardize the
proof of Proposition A.1.

3) Softplus. If φl(x) = log(1 + ex) for layer l, then Nl = 1. The Softplus activation function can be

used as a smooth approximation of the ReLU activation function.

4) Linear. If φl(x) = x for layer l, then Nl = 1. Only the ﬁrst derivative is non-zero.
5) Tanh. If φl(x) = tanh(x) for layer l, then Nl = 1.
6) Scaled exponential linear unit (SELU); see Klambauer et al. (2017). If, for layer l,

(λα(exp(−x) − 1),

φl(x) =

λx,

if x < 0,

if x ≥ 0,

where λ and α are prespeciﬁed constants, then Nl = max{λ, λα, 1}. The same argument about
discontinuities made with the ReLU activation function applies equally well to the case of the SELU
activation function.

29

A Analyzing GMMN QMC and GMMN RQMC estimators

Assumption 4) of Proposition A.1 is satisﬁed in practice because NNs are always trained with
regularization on the parameters, which means ˆθ always lies in a compact set. Additionally note that
in the general case where q is characterized by a composition of NN layers and F −1
Z with a diﬀerent
(but standard) activation function in each layer, all partial derivatives of q exist and are continuous.
Moreover, for the activation functions and input distributions listed above, all mixed partial derivatives
of q are bounded.

A.3 RQMC point sets

In Monte Carlo applications, we need to randomize the low-discrepancy sequence Pngen to obtain
unbiased estimators and variance estimates. To that end, we can randomize Pngen via a U 0 ∼ U(0, 1)p to
obtain a randomized point set ˜Pngen = ˜Pngen(U 0) = {˜v1, . . . , ˜vngen}, where ˜vi = r(U 0, vi), i = 1, . . . , ngen,
for a certain randomization function r. A simple randomization to obtain an RQMC point set is to
consider ˜vi = (vi + U 0) mod 1, i = 1, . . . , ngen, for U 0 ∼ U(0, 1)p, a so-called random shift; see Cranley
and Patterson (1976).

In practice, more sophisticated alternatives to the random shift are often used. One such slightly
more sophisticated randomization scheme is the digital shift method; see Lemieux (2009, Chapter 6)
and Cambou et al. (2017). In the same vein as the random shift, one adds a random uniform shift
to points in Pngen, but with operations in Zb, where b is the base in which the digital net is deﬁned,
rather than simply adding two real numbers. We use ˜P ds
ngen to denote the RQMC point set obtained
using the digital shift method.

Another randomization approach is to scramble the digital net. This technique was originally
proposed by Owen (1995). In particular, the type of scrambling we work with is referred to as the
nested uniform scrambling (or full random scrambling) method; see Owen (2003). Since we primarily
use this method throughout the paper, ˜Pngen will denote speciﬁcally the RQMC point set obtained using
scrambling. The digital shift method is more computationally eﬃcient in comparison to scrambling
but because the distortion of the deterministic point set is fairly simple in the digital shift method,
there are bad functions one can construct such that the variance of the RQMC estimator is larger than
that of the corresponding MC estimator; see Lemieux (2009, Chapter 6). Furthermore, when RQMC
points are constructed with scrambling, we can justify (see Appendix A.4) that an improved rate of
O(n−3
gen (log ngen)(p−1)/2) on the
root mean squared error (RMSE) scale, which is more directly comparable to the convergence rate of
O(n−1
gen(log ngen)p) implied by the Koksma-Hlawka bound for the mean absolute error (MAE) of ˆµNN
ngen
using QMC points (see Appendix A.2). Hence, even though the aforementioned bad functions do not
often arise in practice, we primarily work with the scrambling randomization method to construct
our RQMC point sets. Both the scrambling and the digital shift methods are available in the R
package qrng and can be accessed via sobol(, randomize = "Owen") and sobol(, randomize =
"digital.shift") respectively.

gen(log ngen)p−1) is achievable for Var(ˆµNN

ngen); this translates to O(n−3/2

The randomization schemes discussed above preserve the low-discrepancy property of Pngen and
the estimators of interest obtained using each type of RQMC point set are unbiased. Computing the
estimator based on B such randomized point sets and computing the sample variance of the resulting
B estimates then allows us to estimate the variance of the estimator of interest.

30

A Analyzing GMMN QMC and GMMN RQMC estimators

A.4 Analyzing the GMMN RQMC estimator

A.4.1 GMMN RQMC estimators constructed with scrambled nets

Pngen

1
ngen

i=1 g(˜v) based on scrambled nets, Owen (1997) initially derived a
For RQMC estimators
convergence rate for the variance of the estimators under a certain smoothness condition on g, where
g : [0, 1]p → R. Owen (2008) then generalized his earlier result to allow a weaker smoothness condition
for a larger class of scrambled nets. Speciﬁcally, if ˜Pngen = {˜v1, . . . , ˜vngen} is a so-called relaxed
scrambled (λ, q, m, p)-net in base b with bounded gain coeﬃcients — for example, Sobol’ sequences
randomized using nested uniform sampling belong to this class — then we have the following result as
a direct consequence of Owen (2008).

Theorem A.3 (Owen (2008))
If all the mixed partial derivatives (up to order p) of g exist and are continuous, then

Var

(cid:18) 1

ngen

ngen
X

i=1

(cid:19)

g(˜vi)

= O(n−3

gen(log ngen)p−1).

Proof. See Owen (2008, Theorem 3).

Now, for the GMMN RQMC estimator, ˆµNN

ngen = 1
ngen
below naturally follows from Theorem A.3 with some added analysis of the composite function h.

i=1 Ψ(q(˜vi)) = 1
ngen

i=1 h(˜vi), the corollary

Pngen

Pngen

Corollary A.4
If all the mixed partial derivatives (up to order p) of h = Ψ ◦ q = Ψ ◦ f ˆθ ◦ F −1
then Var(ˆµNN

gen(log ngen)p−1).

ngen) = O(n−3

Z exist and are continuous,

To analyze the mixed partial derivatives of h, it suﬃces to analyze each component function

separately.

Dk F −1
Zj
k, j = 1, . . . , p.

For popular choices of input distributions (such as U(a, b)p for a < b or N(0, 1)p), the k-fold derivative
exists and is continuous (on [a, b] or R depending on the choice of input distribution) for each

For each layer l = 1, . . . , L + 1 of the NN f ˆθ, Dk φl exists and is continuous for k = 1, . . . , p —
provided that we use (standard) activation functions; see Remark A.2 for further details on suitable
activation functions. For NNs constructed using some popular activation functions such as the ReLU
and SELU, note that the set of all pointwise discontinuities of the mixed partial derivatives of f ˆθ is a
set of Lebesgue measure zero and hence the proof of Theorem A.3 holds. Alternatively, we can use the
softplus activation function as a smoother approximation of ReLU. Now in the most general case of
NNs f ˆθ being composed of layers with diﬀerent (but standard) activation functions, all mixed partial
derivatives (up to order p) of f ˆθ exist and are continuous almost everywhere.

Finally, it is certainly true that, for many functionals Ψ that we care about in practice, such as
those considered in Sections 4 and 5.3, all of its mixed partial derivatives (up to order p) exist and are
continuous almost everywhere on Rd.

A.4.2 GMMN RQMC estimators constructed with digitally shifted nets

For GMMN RQMC estimators ˆµNN,ds
can obtain an expression for Var(ˆµNN,ds
integrable; see Cambou et al. (2017, Proposition 6).

ngen

constructed using digitally shifted RQMC point sets ˜P ds
ngen, we
ngen ) under the condition that the composite function h is square

With added assumptions on the smoothness of h, one can obtain improved convergence rates
ngen ). For example, under the assumptions of Proposition A.1,

(compared to MC estimators) for Var(ˆµNN,ds

31

B Run time

h has ﬁnite bounded variation in the sense of Hardy–Krause, which implies that Var(ˆµNN,ds
O(n−2

gen(log ngen)2p); see L’Ecuyer (2016).

ngen ) =

In practice, we observe that GMMN RQMC estimators constructed using both scrambled and
digitally shifted nets achieve very similar convergence rates despite diﬀerences in the theoretical
convergence rates. To that end, Figure 14 shows plots of standard deviation estimates for estimating
E(Ψ2(X)) where we use the RQMC point sets ˜P ds
ngen for the same copula models as considered for
Figure 11 (which is based on GMMN RQMC estimators constructed using scramble nets) in Section 4.
The approximate convergence rates as implied by the regression coeﬃcients α displayed in both ﬁgures
are very similar across the various examples.

B Run time

Run time depends on factors such as the hardware used, the current workload, the algorithm imple-
mented, the programming language used, the implementation style, compiler ﬂags, whether garbage
collection was used, etc. There is not even a unique concept of time (system vs user vs elapsed time).
Although none of our code was optimized for run time, we still report on various timings here, measured
in elapsed time also known as wall-clock time.

B.1 Training and sampling

The results in this section are reproducible with the demo GMMN_QMC_timings of the R package gnn.
Table 1 shows elapsed times in minutes for training a GMMN on training data from t4, Clayton (C),
Gumbel (G) and nested Gumbel (NG) copulas in dimensions 2, 3, 5 and 10 as described in Sections 3.2
and 3.3. As is reasonable, the measured times are only aﬀected by the dimension, not by the type of
dependence model.

C

d = 2, 3

d = 5

d = 10

t4
C
G
NG

5.52
5.52
5.52
6.01

7.01
7.00
7.01
7.01

9.46
9.45
9.46
9.44

Table 1 Elapsed times in minutes for training GMMNs of the same architecture as used in Sections 3.2
and 3.3 with nepo = 300, ntrn = 60 000 and nbat = 5000 on respective copula samples; training
was done on one NVIDIA Tesla P100 GPU.

Table 2 contains elapsed times for generating ngen = 105 observations from the respective dependence
model and sampling method on two diﬀerent machines, once on the NVIDIA Tesla P100 GPU used
for training and once locally on a 2018 2.7 GHz Quad-Core Intel Core i7 processor. The results for
the copula-based pseudo-random sampling method are averaged over 100 repetitions. The results for
the copula-based quasi-random sampling method are obtained as follows. If the conditional copulas
involved in applying the inverse Rosenblatt transform of the respective copula model are not available
analytically nor numerically, NA is reported; this applies to the nested Gumbel copula. And if they are
only available numerically (by root ﬁnding), then a reduced sample size of 1000 is used and the reported
run times were obtained by scaling up to ngen by multiplication with 100; this applies to the Gumbel

32

B Run time

Figure 14 Standard deviation estimates based on B = 25 replications for estimating E(Ψ2(X)) via
MC based on a pseudo-random sample (PRS), via the copula RQMC estimator (whenever
available; rows 1–2 only) and via the GMMN RQMC estimator (based on digitally shifted
nets). Note that in rows 1–3, d ∈ {2, 5, 10}, whereas in row 4, d ∈ {3, 5, 10}.

33

B Run time

copula. We also measured run times for ngen = 106 and ngen = 107 and they scale proportionally as
one would expect.

2018 2.7 GHz Quad-Core Intel Core i7

NVIDIA Tesla P100 GPU

Copula

GMMN

Copula

GMMN

d

C

PRS

QRS

PRS

QRS

PRS

QRS

PRS

QRS

2
2
2

5
5
5

0.0642
t4
0.0144
C
G 0.0348
(2,1) NG 0.0633
0.1410
t4
0.0425
C
G 0.0523
(2,3) NG 0.0989
t4
0.2766
0.0734
C
G 0.0807
(5,5) NG 0.1324

10
10
10

0.4420
0.0230
374.8000
NA
1.4830
0.0580
1529.5000
NA
3.6080
0.1190
3579.6000
NA

1.2960
1.3110
1.3470
1.3360
1.4490
1.3890
1.3930
1.4020
1.5870
1.6430
1.5500
1.5470

1.2720
1.3140
1.3310
1.3260
1.3830
1.5060
1.3860
1.4070
1.6720
1.6630
1.5290
1.5280

0.1045
0.0308
0.0669
0.1369
0.2567
0.0936
0.1161
0.2167
0.4917
0.1806
0.1984
0.3087

0.8210
0.0400
687.7000
NA
3.0150
0.1110
2939.9000
NA
6.5290
0.2320
7119.6000
NA

3.6140
3.6290
3.6400
3.6560
3.7330
3.7670
3.7380
3.9450
3.9530
3.9680
4.0060
3.9740

3.5820
3.5820
3.5750
3.6530
3.6960
3.6980
3.7010
3.7210
4.0990
4.1910
3.9370
3.9530

Table 2 Elapsed times in seconds for generating samples of size ngen = 105.

We see from Table 2 that quasi-random sampling from speciﬁc copulas is available and can be fast,
e.g., for t4 and Clayton copulas. However, we already see that quasi-random sampling gets more
time-consuming for larger d. For other copulas, such as Gumbel copulas, it can be much more time
consuming. Furthermore, as seen from the nested case and as is currently the case for most copula
models, a quasi-random sampling procedure is not even available. By contrast, on the same machine,
GMMNs show very close run times, are barely aﬀected by the dimension and are not aﬀected by the
type of dependence model. For d = 10, the GMMN quasi-random sampling procedure even outperforms
the t4 quasi-random sampling procedure for which the conditional copulas are analytically available;
for d = 5 the two procedures perform on par, depending on the machine used.

This highlights the universality of using neural networks for dependence modeling purposes. As
an example, say a risk management application such as estimating expected shortfall with variance
reduction is based on a t4 copula and a regulator requires us to change the model to a Gumbel copula
for stress testing purposes. Suddenly run time increases substantially. Also, if the regulator decides to
incorporate hierarchies (as was more easily done for the t4 model due to its correlation matrix) by
utilizing a nested Gumbel copula, then there is suddenly no quasi-random sampling procedure known
anymore. It is one of the biggest drawbacks of parametric copula models in applications that the level
of diﬃculty of carrying out important statistical tasks such as sampling, ﬁtting and goodness-of-ﬁt can
largely depend on the class of copulas used. These problems are eliminated with neural networks as
dependence models.

B.2 Fitting and training times for the real-data applications

We now brieﬂy present the run times for ﬁtting the parametric copula models and training the GMMNs
used in Section 5. Recall that we considered three dimensions d ∈ {3, 5, 10} and that ﬁtting, respectively
training, was only required once for each dimension, independently of the number of applications

34

References

considered.

Table 3 contains the elapsed times in seconds. Recall from Section 5 that GMMNs provided the
best ﬁt, followed by the unstructured t copula. The latter is in general a popular parametric copula
model in practice; see Fischer et al. (2009). Comparing the last two columns of Table 3, we see that
ﬁtting the t copula is comparably fast for d = 3, however, already for d = 5, run time for training a
GMMN is on par. For d = 10, training a GMMN is signiﬁcantly faster.

d Gumbel Clayton Normal (ex) Normal (un)

t (ex)

t (un) GMMN

3
5
10

1.078
1.291
1.344

0.437
0.455
0.531

0.388
0.435
0.981

1.000
6.071
82.982

3.315
5.932
11.966

9.064
41.131
783.406

43.336
41.235
55.555

Table 3 Elapsed times in seconds for ﬁtting the respective parametric copula model and training the
GMMN on one NVIDIA Tesla P100 GPU for the applications presented in Section 5.

B.3 TensorFlow vs R

Finally, let us stress again what we initially said, namely, that run time depends on many factors. In
particular, one typically relies on TensorFlow for the feed-forward step of input through the GMMN,
which creates overhead especially for smaller data size n. In the demo GMMN_QMC_timings, we also
provide a pure R implementation for this step for GMMNs considered in this work.

For each of d ∈ {2, 5, 10}, we randomly initialize B = 10 GMMNs as in Algorithm 2.1 and average
the elapsed times of their feed-forward steps when passing through data of size n (chosen equidistant
in log-scale from 10 to 106) from the input distribution, once with TensorFlow, and once with our own
R implementation. We then divide the averaged run times of the R implementation by the ones of the
TensorFlow implementation. Whenever the ratio is smaller (larger) than one, the R implementation is
faster (slower) than the TensorFlow implementation. We ran this experiment once locally on the 2018
2.7 GHz Quad-Core Intel Core i7 processor and once on the NVIDIA Tesla P100 GPU. The results are
depicted on the left and on the right plot in Figure 15, respectively. Depending on the machine used,
the R implementation can be signiﬁcantly faster, especially for small n.

References

Arjovsky, Martin, Chintala, Soumith, and Bottou, Léon (2017), Wasserstein generative adversarial

networks, International Conference on Machine Learning, 214–223.

Arora, R., Basu, A., Mianjy, P., and Mukherjee, A. (2016), Understanding deep neural networks with

rectiﬁed linear units, https://arxiv.org/abs/1611.01491 (11/01/2018).

Arora, Sanjeev, Risteski, Andrej, and Zhang, Yi (2018), Do GANs learn the distribution? Some Theory

and Empirics, International Conference on Learning Representations, ICLR.

Cambou, Mathieu, Hofert, Marius, and Lemieux, Christiane (2017), Quasi-random numbers for copula

models, Statistics and Computing, 27(5), 1307–1329.

Constantine, G. and Savits, T. (1996), A multivariate Faa di Bruno formula with applications,

Transactions of the American Mathematical Society, 348(2), 503–520.

Cranley, R. and Patterson, T. N. L. (1976), Randomization of Number Theoretic Methods for Multiple

Integration, SIAM Journal on Numerical Analysis, 13(6), 904–914.

35

References

Figure 15 Ratio of averaged elapsed times of an R implementation over the TensorFlow implementation
when evaluating randomly initialized GMMNs, once run on a 2018 2.7 GHz Quad-Core
Intel Core i7 processor (left) and once on an NVIDIA Tesla P100 GPU (right).

Cybenko, George (1989), Approximation by superpositions of a sigmoidal function, Mathematics of

Control, Signals and Systems, 2(4), 303–314.

Dziugaite, Gintare Karolina, Roy, Daniel M, and Ghahramani, Zoubin (2015), Training generative
neural networks via maximum mean discrepancy optimization, Proceedings of the Thirty-First
Conference on Uncertainty in Artiﬁcial Intelligence, AUAI Press, 258–267, http://www.auai.org/
uai2015/proceedings/papers/230.pdf (08/24/2019).

E. Hlawka (1961), Über die Diskrepanz mehrdimensionaler Folgen mod 1, Mathematische Zeitschrift,

77, 273–284.

Embrechts, P., Lindskog, F., and McNeil, A. J. (2003), Modelling Dependence with Copulas and
Applications to Risk Management, Handbook of Heavy Tailed Distributions in Finance, ed. by S.
Rachev, Elsevier, 329–384.

Faure, H. and Lemieux, C. (2009), Generalized Halton sequence in 2008: A comparative study, ACM

Transactions on Modeling and Computer Simulation, 19, Article 15.

Fischer, M., Köck, C., Schlüter, S., and Weigert, F. (2009), An empirical analysis of multivariate copula

models, Quantitative Finance, 9(7), 839–854.

Genest, C., Ghoudi, K., and Rivest, L.-P. (1995), A semiparametric estimation procedure of dependence

parameters in multivariate families of distributions, Biometrika, 82(3), 543–552.

Genest, Christian, Rémillard, Bruno, and Beaudoin, David (2009), Goodness-of-ﬁt tests for copulas: A

review and a power study, Insurance: Mathematics and economics, 44(2), 199–213.

Glorot, Xavier and Bengio, Yoshua (2010), Understanding the diﬃculty of training deep feedforward
neural networks, Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence
and Statistics, 249–256.

Goodfellow, Ian, Bengio, Yoshua, and Courville, Aaron (2016), Deep learning, vol. 1, MIT press

Cambridge.

36

References

Gretton, A., Borgwardt, K., Rasch, M. J., Schölkopf, B., and Smola, A. J. (2007), A kernel method for

the two-sample-problem, Advances in Neural Information Processing Systems, 513–520.

Gretton, A., Borgwardt, K., Rasch, M. J., Schölkopf, B., and Smola, A. J. (2012), A kernel two-sample

test, Journal of Machine Learning Research, 13(Mar), 723–773.

Hlawka, E. and Mück, R. (1972), Über eine Transformation von gleichverteilten Folgen II, Computing,

9(2), 127–138.

Hofert, M. (2010), Sampling Nested Archimedean Copulas with Applications to CDO Pricing, PhD
thesis, Südwestdeutscher Verlag für Hochschulschriften AG & Co. KG, ISBN 978-3-8381-1656-3.
Hofert, M., Kojadinovic, I., Mächler, M., and Yan, J. (2018), Elements of Copula Modeling with
R, Springer Use R! Series, ISBN 978-3-319-89635-9, doi:10.1007/978- 3- 319- 89635- 9, http:
//www.springer.com/de/book/9783319896342 (03/15/2018).

Hofert, M. and Oldford, R. W. (2018), Visualizing Dependence in High-dimensional Data: An Ap-
plication to S&P 500 Constituent Data, Econometrics and Statistics, 8, 161–183, doi:10.1016/j.
ecosta.2017.03.007.

Hofert, Marius (2012), A stochastic representation and sampling algorithm for nested Archimedean
copulas, Journal of Statistical Computation and Simulation, 82(9), 1239–1255, doi : 10 . 1080 /
00949655.2011.574632.

Hornik, Kurt (1991), Approximation capabilities of multilayer feedforward networks, Neural Networks,

4(2), 251–257.

Joe, Harry (2014), Dependence modeling with copulas, Chapman and Hall/CRC.
Jondeau, E. and Rockinger, M. (2006), The copula–GARCH model of conditional dependencies: An
international stock market application, Journal of International Money and Finance, 25, 827–853.
Kingma, Diederik P and Ba, Jimmy (2014), Adam: A method for stochastic optimization, https:

//arxiv.org/abs/1412.6980 (11/01/2018).

Klambauer, Günter, Unterthiner, Thomas, Mayr, Andreas, and Hochreiter, Sepp (2017), Self-normalizing

neural networks, Advances in Neural Information Processing Systems, 971–980.

L’Ecuyer, Pierre (2016), Randomized quasi-Monte Carlo: An introduction for practitioners, Inter-
national Conference on Monte Carlo and Quasi-Monte Carlo Methods in Scientiﬁc Computing,
Springer, 29–52.

Lemieux, Christiane (2009), Monte Carlo and Quasi–Monte Carlo Sampling, Springer.
Li, Y., Swersky, K., and Zemel, R. (2015), Generative moment matching networks, International

McNeil, A. J. (2008), Sampling nested Archimedean copulas, Journal of Statistical Computation and

McNeil, A. J., Frey, R., and Embrechts, P. (2015), Quantitative Risk Management: Concepts, Techniques,

Conference on Machine Learning, 1718–1727.

Simulation, 78(6), 567–581.

Tools, 2nd ed., Princeton University Press.

Montufar, Guido F, Pascanu, Razvan, Cho, Kyunghyun, and Bengio, Yoshua (2014), On the number
of linear regions of deep neural networks, Advances in Neural Information Processing Systems,
2924–2932.

Nelsen, R. B. (2006), An Introduction to Copulas, Springer-Verlag.
Niederreiter, Harald (1992), Random number generation and quasi-Monte Carlo methods, vol. 63,

SIAM.

Nielsen, M. A. (2015), Neural Networks and Deep Learning, http://neuralnetworksanddeeplearning.

com, Determination Press, (09/18/2018).

Owen, Art B (1995), Randomly permuted (t, m, s)-nets and (t, s)-sequences, Monte Carlo and

Quasi-Monte Carlo Methods in Scientiﬁc Computing, Springer, 299–317.

37

References

Owen, Art B (1997), Monte Carlo variance of scrambled net quadrature, SIAM Journal on Numerical

Owen, Art B (2003), Variance and discrepancy with alternative scramblings, ACM Transactions of

Analysis, 34(5), 1884–1910.

Modeling and Computer Simulation, 13(4).

2319–2343.

Owen, Art B (2008), Local antithetic sampling with scrambled nets, The Annals of Statistics, 36(5),

Pascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua (2013), On the diﬃculty of training recurrent

neural networks, International Conference on Machine Learning, 1310–1318.

Patton, A. J. (2006), Modelling Asymmetric Exchange Rate Dependence, International Economic

Review, 47(2), 527–556, http://public.econ.duke.edu/~ap172/Patton_IER_2006.pdf.

Rémillard, Bruno and Scaillet, Olivier (2009), Testing for equality between two copulas, Journal of

Rosenblatt, Murray (1952), Remarks on a multivariate transformation, The Annals of Mathematical

Multivariate Analysis, 100(3), 377–386.

Statistics, 23(3), 470–472.

Sobol’, Il’ya Meerovich (1967), On the distribution of points in a cube and the approximate evaluation

of integrals, Zhurnal Vychislitel’noi Matematiki i Matematicheskoi Fiziki, 7(4), 784–802.

Tolstikhin, Ilya O, Gelly, Sylvain, Bousquet, Olivier, Simon-Gabriel, Carl-Johann, and Schölkopf,
Bernhard (2017), Adagan: Boosting generative models, Advances in Neural Information Processing
Systems, 5424–5433.

38

