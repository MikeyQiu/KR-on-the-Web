 
 
Using the Tsetlin Machine to Learn Human-Interpretable Rules for 
High-Accuracy Text Categorization with Medical Applications

 

Geir Thore Berge1,2,3, Ole-Christoffer Granmo1, Tor Oddbjørn Tveit1,3,4, Morten Goodwin1*, 
Lei Jiao1*, and Bernt Viggo Matheussen1* 
1 Centre for Artificial Intelligence Research, University of Agder, Grimstad, Norway 
2 Department of Information Systems, University of Agder, Kristiansand, Norway 
3 Department of Technology and eHealth, Sørlandet Hospital Trust, Kristiansand, Norway 
4 Department of Anesthesia and Intensive Care, Sørlandet Hospital Trust, Kristiansand, Norway 
geir.thore.berge@sshf.no 
 
 

Abstract 
Medical applications challenge today's text categorization tech-
niques by demanding both high accuracy and ease-of-interpre-
tation. Although deep learning has provided a leap ahead in ac-
curacy, this leap comes at the sacrifice of interpretability.  To 
address this accuracy-interpretability challenge, we here intro-
duce, for the first time, a text categorization approach that lev-
erages the recently introduced Tsetlin Machine. In all brevity, 
we represent the terms of a text as propositional variables. From 
these, we capture categories using simple propositional formu-
lae, such as:  if “rash” and “reaction” and “penicillin” then Al-
lergy.  The  Tsetlin  Machine  learns  these  formulae  from  a  la-
belled text, utilizing conjunctive clauses to represent the partic-
ular facets of each category.  Indeed, even the absence of terms 
(negated features) can be used for categorization purposes. Our 
empirical comparison with Naïve Bayes, decision trees, linear 
support  vector  machines  (SVMs),  random  forest,  long  short-
term memory (LSTM) neural networks, and other techniques, is 
quite  conclusive.  The  Tsetlin Machine either  performs  on  par 
with or outperforms all of the evaluated methods on both the 20 
Newsgroups and IMDb datasets, as well as on a non-public clin-
ical dataset. On average, the Tsetlin Machine delivers the best 
recall and precision scores across the datasets. Finally, our GPU 
implementation of the Tsetlin Machine executes 5 to 15 times 
faster than the CPU implementation, depending on the dataset. 
We thus believe that our novel approach can have a significant 
impact on a wide range of text analysis applications, forming a 
promising starting point for deeper natural language understand-
ing with the Tsetlin Machine. 

Introduction   

Understanding  natural  language  text  involves  interpreting 
linguistic constructs at multiple levels of abstraction:  words 
                                                
* The last three authors have been listed alphabetically. 
 
 

forming phrases, which interact to form sentences, which in 
turn are interweaved into paragraphs that carry implicit and 
explicit meaning  (Norvig  1987;  Zhang,  Zhao,  and  LeCun 
2015). Because of the complexity inherent in the formation 
of  natural  language,  text  understanding  has  traditionally 
been a difficult area for machine learning algorithms (Linell 
1982). Medical text understanding is no exception, both due 
to the intricate nature of medical language, and due to the 
need for transparency, through human-interpretable proce-
dures  and  outcomes  (Berge,  Granmo,  and  Tveit  2017;  Y. 
Wang et al. 2017). 
  Although deep learning in the form of convolutional neu-
ral networks (CNN), recurrent neural networks (RNNs), and 
Long Short Term Memory (LSTM) recently has provided a 
leap  ahead  in  text  categorization  accuracy  (Zhang,  Zhao, 
and LeCun 2015; Conneau et al. 2017; Schmidhuber 2015; 
Subramanian et al. 2018), this leap has come at the expense 
of interpretability and computational complexity (Miotto et 
al. 2017). Simpler techniques such as Naïve Bayes, logistic 
regression,  decision  trees,  random  forest,  k-nearest  neigh-
bors  (kNN),  and  Support  Vector  Machine  (SVM)  are  still 
widely used, arguably because they are simple and efficient, 
yet, provide reasonable accuracy, in particular when data is 
limited (Y. Wang et al. 2017; Valdes et al. 2016). 

Challenges  Addressed  in  This  Paper:  Despite  recent 
successes, natural language continues to challenge machine 
learning (Schütze, Manning, and Raghavan 2008; M. Wu et 
al. 2017). 

First of all, realistic vocabularies can be surprisingly rich, 
leading to high-dimensional input spaces. Depending on the 
feature  selection  strategy,  text  categorization  has  to  deal 

with thousands to tens of thousands (or even more) of fea-
tures. Even though natural language text typically contains 
high volumes of duplicated words (e.g., stop words), few are 
irrelevant  (Zipf  1932).  Low  frequency  terms  further  carry 
considerable information that may be relevant for categori-
zation (Joachims 1998). As a result, a vocabulary can be so 
diverse that documents covering the same category may not 
even  share  a  single  medium-frequency  term  (Joachims 
2002).  As  an  example,  the  Electronic  Health  Records 
(EHRs) dataset that we target in the present paper abounds 
with  jargon,  misspellings,  acronyms,  abbreviations,  and  a 
mixture  of  Latin,  English,  and  mother  tongue  (Berge, 
Granmo, and Tveit 2017). 

Another challenge is the inherently complex composition 
of  natural  language  (Miotto  et  al.  2017;  Linell  1982).  For 
example, determining whether a patient is allergic to drugs 
or not based on EHRs has proven to be very hard without 
introducing  hand-crafted  rules  (Berge,  Granmo,  and  Tveit 
2017).  Rather  than  relying  on  the  additivity  of  individual 
features to discriminate between categories, full sentences, 
paragraphs and even complete records must be considered 
in  context.  Indeed,  the  temporal  dynamics  of  EHRs  is  a 
strong  confounding  variable.  Simultaneously,  EHRs  may 
also  contain  ambiguous  considerations  and  speculations 
which must be treated as false positives despite their simi-
larity to true positives on the individual feature level (Berge, 
Granmo, and Tveit 2017; Uzuner et al. 2011). 

A  third  challenge  is  interpretability.  While  rule-based 
methods such as decision trees are particularly easy to un-
derstand,  other  techniques  tend  to  be  more  complex  (Y. 
Wang et al. 2017; Mittelstadt et al. 2016). Neural networks 
are  arguably  among  the  most  complicated,  and  have  been 
criticized for being “black boxes” (Miotto et al. 2017). That 
is, it is very difficult for humans to understand how they pro-
duce  their  results.  Even  trained  data  scientists  may  have 
problems  explaining  their  outcomes,  or  such  explanations 
may demand comprehensive and time-consuming analysis. 
It has been argued that as doctors or nurses are expected to 
justify  their  decisions,  so  should  machine  learning  algo-
rithms  be  able  to  explain  their  decisions  (Y.  Wang  et  al. 
2017; Mittelstadt et al. 2016). Recent progress on post pro-
cessing attempts to address this issue (M. Wu et al. 2017). 
However, such approaches add to the already inherent com-
plexity of neural networks by introducing a new abstraction 
layer. They are further yet not capable of providing both an 
exhaustive and compact explanation of the reasoning going 
on inside a neural network.  

Paper  Contributions:  In  this  paper,  we  introduce  the 
first  approach  to  text  categorization  that  leverages  the  re-
cently  introduced  Tsetlin  Machine  (Granmo  2018).  The 
Tsetlin Machine has caught significant interest because it fa-
cilitates human understandable pattern recognition by com-
posing  patterns  in  propositional  logic.  Yet,  it  has  outper-
formed  state-of-the-art  pattern  recognition  techniques  in 

Figure 1. Example of bit vector representation of document sen-
tences, taken from the medical dataset used in one of the experi-
ments. 

benchmarks involving pattern discrimination, image recog-
nition,  and  optimal  move  prediction  for  board  games 
(Granmo 2018).  

 The Tsetlin Machine builds on the Tsetlin Automaton, a 
pioneering solution to the multi-armed bandit problem de-
veloped  by  M.  L.  Tsetlin  in  the  Soviet  Union  in  the  late 
1950s/early 1960s (Tsetlin 1961; Granmo 2018). 

At the heart of the Tsetlin Machine we find a novel game 
theoretic  scheme  that  orchestrates  decentralized  teams  of 
Tsetlin Automata. The orchestration guides the Tsetlin Au-
tomata to jointly learn arbitrarily complex propositional for-
mulae  in  conjunctive  normal  form,  capturing  the  various 
facets of the patterns faced (Granmo 2018). Such formulae 
have turned out to be particularly suited for human interpre-
tation, while still allowing complex non-linear patterns to be 
formed (T. Wang et al. 2017).  

Apart from being simple to interpret, the Tsetlin Machine 
uses  bits  to  represent  both  inputs,  patterns  and  outputs. 
Recognition of patterns, in turn, merely involves decentral-
ized manipulation of bits. This provides the Tsetlin Machine 
with  an  advantage  computationally,  compared  to  methods 
that rely on more complex mathematical modelling, in par-
ticular when implemented on a GPU. 

Paper Organization: The paper is organized as follows.  
In Section 2, we provide the details of our Tsetlin Machine 
based  approach  to  text  categorization.  We  further  demon-
strate  how  the  conjunctive  clauses  are  formed  to  capture 
complex patterns in natural language. Then, in Section 3, we 
evaluate our framework empirically on both the 20 News-
groups  and  the  IMDB  movie  review  (IMDb)  datasets,  as 
well as on a non-public clinical dataset, contrasted against 
selected state-of-the-art techniques. Finally, in Section 4, we 
conclude and provide pointers for further research, includ-
ing contextual language models for the Tsetlin Machine. 

Text Categorization with the Tsetlin Machine 
In this section, we first present our Tsetlin Machine based 
framework  for  text  categorization.  To  highlight  the  key 
characteristics of the framework, we provide a running ex-
ample  from  the  medical  domain  –  detection  of  Allergy  in 
Electronic  Health  Records  (EHRs). Thereafter,  we  present 
the  learning  procedure  itself,  demonstrating  step-by-step 
how  the  Tsetlin  Machine  extracts  sophisticated  linguistic 
patterns from labelled documents and simultaneously com-
bats overfitting. 

Representing Text as Propositional Variables 
The Tsetlin Machine takes a vector of k propositional vari-
ables, X = [x1, x2, …, xk], as input, each taking the value 0 or 
1 (or equivalently, False or True). As illustrated in Figure 1, 
we represent text (sentences, paragraphs, or in our case, doc-
uments) as follows. First, we form a vocabulary, V, consist-
ing of the unique terms found in the target text corpus, D. A 
propositional  variable,  xo,  is  then  introduced  to  represent 
each  term,  toÎV.    This  allows  us  to  model  a  document, 
DÎD, as a vector, X, of |V| propositional variables (stored 
as a vector of bits). The vector tells us which terms are pre-
sent and which are absent in D. 

Example:  For  the topic  Allergy, terms  like “penicillin” 
and “reacts” would for instance be of importance, providing 
the propositional variables x“penicillin” and x“reacts”. 

 The coarse text representation we use here removes the 
information on the ordering of terms. For more fine-grained 
modelling of language, a document can be broken down into 
paragraphs and sentences, as explored in the paper conclu-
sion. More refined vocabularies can also be built by apply-
ing basic NLP-preprocessing techniques, such as case low-
ering,  removing  non-informative  punctuation,  tokenizing 
into  n-grams,  and  taking  advantage  of  sentence  and  para-
graph boundaries (Kovačević et al. 2013). 

Linguistic Patterns in Conjunctive Normal Form 
To  build  patterns  that  accurately  capture  categories,  we 
compose  propositional  formulae  in  conjunctive  normal 
form.  These  relate  the  propositional  variables  into  com-
pounds that trigger on evidence for or against a category. 

Each compound, i.e., a conjunctive clause, is built by em-
ploying the conjunction operator on the propositional varia-
bles and their negations (referred to as literals): 

C"(𝐗) = 𝑥()Ù	𝑥(+Ù	 … 	Ù	𝑥(-Ù	¬𝑥(-.)Ù	 … 	Ù	¬𝑥(/. 
Here, qu are indexes from {1,…, k} that identify which prop-
ositional  variables take  part in the conjunction, as  well as 
their role (negated or unnegated). 
  Example: For instance, the clause “rash” Ù “reaction” Ù 
“penicillin”, can potentially act as evidence (a vote) for the 
category Allergy. 

The beauty of the conjunctive normal form is that it can 
capture  arbitrarily  refined  patterns  by  looking  at  multiple 
terms in conjunction, forming a global view of the input vec-
tor.  This  as  opposed  to  modelling  features  independently, 
such as in linear models like the Naïve Bayes Classifier. 
  Finally,  note  the  similarity  between  the  conjunctive 
clauses in our model and the Boolean model of information 
retrieval,  where  Boolean  expressions  of  terms  are  used  to 
form queries by means of the AND-, OR-, and NOT opera-
tors (Schütze, Manning, and Raghavan 2008). 
 
 
 

Figure  2.  The  Tsetlin  Machine  architecture,  introducing  clause 
polarity, a summation operator collecting "votes", and a threshold 
function arbitrating the final output. 

Categorization: Adding up Evidence from Clauses 
As illustrated in Figure 2, in order to provide a rich and ro-
bust representation of each category, a Tsetlin Machine uti-
lizes multiple clauses, C = {C1, …, Cm}. Each clause is fur-
ther assigned a polarity (+/-). We use odd indexed clauses to 
capture patterns that provide evidence on the presence of a 
category (positive polarity), while even indexed clauses cap-
ture evidence on the absence of the category (negative po-
larity). 

As further depicted in Figure 2, evidence obtained from 

clauses are aggregated by summation: 

												f1(X) = 34 C"(X)

567

"87,:,…

5
; − 34 C"(X)

; 

"8=,>,…

 

 

The  concluding  categorization  is  finally  decided  from  the 
sign of få(X): 

y = 1 if få(X) > 0 otherwise 0. 

That is, only when the number of clauses providing a pos-
itive output outweigh those with negative output is the doc-
ument assigned the target category (y = 1, i.e., Allergy in our 
case). 

Altogether,  this  means  that  the  Tsetlin  Machine  both 
learns what a category looks like, and what it does not look 
like. By summing evidence for and against the category, the 
threshold mechanism arbitrates the final decision. 
  Example: Recall our clause: “rash” Ù “reaction” Ù “pen-
icillin”. If the terms “rash”, “reaction”, and “penicillin” are 
all  present  in  the  document  being  categorized,  the  clause 
would evaluate to 1. This outcome would count as evidence 
for Allergy if the clause has positive polarity, and if none of 
the clauses with negative polarity evaluates to 1, the docu-
ment would be assigned the category Allergy (y = 1).    

Learning the Composition of Clauses 
We have now presented the structure of our framework for 
text categorization. However, a critical part remains. Learn-
ing  propositional  formula  from  data  has  been  notoriously 
difficult (Valiant 1984), with the space of candidate formu-
lae growing exponentially with the number of propositional 
variables involved (Granmo 2018). 

Figure  3.  Six  Tsetlin  Automata  with  100  states  per  action. Each 
automaton learns to either Exclude or Include a candidate literal 
(a term or its negation) in a clause. 

This exponential  growth  becomes  particularly  severe in 
text categorization involving extensive vocabularies. A text 
categorization  problem  may  consist  of  thousands,  or  even 
tens of thousands, of propositional variables. The full range 
of possible propositional formulae is thus immense. As a re-
sult, extreme care must be taken during training to maximize 
the precision and recall of the categorization, while simulta-
neously combating overfitting. 

The pioneering nature of the Tsetlin Machine for text cat-
egorization can be summarized as follows. The Tsetlin Ma-
chine addresses  the above challenges by  decomposing the 
problem into a large number of simple decisions. These de-
cisions are coordinated  by a novel  decentralized  game  be-
tween independent Tsetlin  Automata  –  one  automaton  per 
clause, per literal (Granmo 2018).  Each automaton decides 
whether to Include or Exclude the assigned candidate literal 
in the given clause. That is, whether a literal should take part 
in a clause is collectively regulated by the competing pres-
ence of multiple candidate literals. 

This learning is performed on-line, one training example, 
@XA, yCD at a time. The input vector X = [x1, x2, …, xo] dictates 
which terms are present and which are absent in the current 
document. The target, yC, to be predicted is simply the cate-
gory of the document (Allergy or No Allergy in our case). 

The Tsetlin Automaton: The Tsetlin Automaton that we 
use can be defined as a quintuple (Thathachar and Narendra 
1989): {F, a, b, F(×,×), G(×)}. F = {f1, f2, …, f2N} is the set 
of internal states. a = {aExclude, aInclude} is the set of automa-
ton actions. b = {bInaction, bPenalty, bReward}is the set of inputs 
that can be given to the automaton. An output function G[ft] 
determines the next action performed by the automaton  
given the current automaton state: (1) G[ft] = aExclude for 1  
≤ t ≤ N; and (2) G[ft] = aInclude for N+1 ≤ t ≤ 2N. Finally, 
a transition function F[ft, bu] determines the new automaton  
state  from:  (1)  the  current  automaton  state  and  (2)  the  re-
sponse  of the  environment to the  action  performed  by the 
automaton. In all brevity, we have: (1) ft+1 = F[ft, bPenalty] 
for 1 ≤ t ≤ N; (2) ft-1 = F[ft, bPenalty] for N+1 ≤ t ≤ 2N; (3) 
ft-1  = F[ft, bReward]  for  1  <  t ≤  N; ft+1 = F[ft, bReward]  for 
N+1  ≤  t  <  2N;  and  (5) ft  =  F[ft,  ×]  otherwise  (Granmo 
2018). The crucial issue is to design automata that can learn 
the optimal action when interacting with the environment. 

Example:  Figure  3  depicts  six  Tsetlin  Automata  with 
N=100  states  per  action,  collaborating  to  build  a  single 
clause. The three Tsetlin Automata to the left (TA) control 

Table 1. The Type I Feedback Table has been designed to combat 
false negative  output.  It  is triggered  when  a document  correctly 
evaluates to true (i.e., both output value and target value is 1), or 
wrongly to false (i.e., output value is 0, while target value is 1). 

the terms “to”, “Voltaren” and “reacts”, while the three to 
the right (TA’) control the negation of the same terms. Those 
moving away from the central states in the figure (“Volta-
ren”,  “reacts”,  ¬“reacts”,  ¬“to”,  ¬“Voltaren”)  have  re-
ceived a bReward, while those moving towards the center have 
received a bPenalty. 

Remark:  Note  that  a  state  near  1  conveys  very  strong 
support for aExclude, while a state close to 2N means strong 
support for aInclude. The center reflects uncertainty. 

The Tsetlin Machine Game: The whole team of Tsetlin 
Automata, across all  the  categories  and  clauses,  is  orches-
trated  by  means  of  a  novel  game  (Granmo  2018).  In  this 
game, the Tsetlin Automata partake as independent players. 
Two simple tables specify the complete multi-dimensional 
game matrix, where the Tsetlin Automata are the decision 
makers: (1) Type I Feedback and (2) Type II Feedback. Each 
round of the game plays out as follows: 
1.  The  arrival  of  a  labelled  document  @XA, yCD  signals  the 

start of new game round. 

2.  Each Tsetlin Automaton decides whether to include or 
exclude its designated literal, leading to a new configu-
ration of clauses C. Recall that each decision is based 
on the state of the Tsetlin Automaton, as exemplified in 
Figure 3. 

3.  Each clause,  C" ∈ C, is then evaluated with XA as input. 
4.  The final output, y, of the Tsetlin Machine is decided 
by thresholding få(X), compared with the target output 
yC. 

5.  Each Tsetlin Automaton is independently and randomly 
given  either  Reward,  Inaction,  Penalty  feedback,  ac-
cording to Table 1 or Table 2: 

a. 

If  yC = 1  (true  positive  or false  negative)  the 
Type I Feedback Table is activated, while the 
Type  II Feedback  Table  is activated  if  yC = 0 
and y = 1 (false positive). 

b.  As  seen  in  the  tables, the probability of  each 
kind of feedback is decided by the current ac-
tion  of  the  automaton  (either  include  or  ex-
clude), the  value  of  the  target clause, C",  and 
the value of its assigned literal 𝑥I / ¬𝑥I. 

6.  After  all  the  Tsetlin  Automata  has  received  feedback 
and  updated  their  states  accordingly,  exemplified  in 
Figure 3, a new round starts. 

Remark:  The  computational  simplicity  and  small 
memory footprint of Tsetlin Automata, combined with the 
decentralized  nature  of  the  above  game,  make  the  Tsetlin 
Machine particularly attractive for execution on GPUs. This 
is explored further in the empirical results section.   

Detailed Walkthrough of the Learning Steps 
Figure 4 provides a step-by-step walkthrough that exempli-
fies how the Tsetlin Machine is able to gradually learn the  
target concept Allergy, building and refining clauses, docu-
ment-by-document.  For the  sake  of clarity,  we  focus  on  a 
substring of each document, despite the fact that the steps  
we now describe happen for each and every term of the vo-
cabulary, for each document processed. We explain the fig-
ure from left to right, starting with Example 1 (from the top). 
Processing of Document 1: The first document, D1, con-
tains the substring “reacts to Voltaren”. Each of these terms 
are associated with propositional variables, say, x29123, x32232, 
and x37372. Again, for clarity, we focus on two of the clauses 
associated  with  the  target  concept,  C1(X)  and  C2(X),  both 
with positive polarity. Each clause may potentially include 
any of the propositional variables, as well as their negation. 
Thus,  we  have  two  Tsetlin  Automata  for  each  term,  for 
every  clause.  The  first  Tsetlin  Automaton  in  the  pair  (re-
ferred to as TA) decides whether to Exclude or Include the 
propositional variable as is. The second one (referred to as 
TA’) decides whether to Exclude or Include the negation of 
the variable. 

Starting  from the left,  the  columns TA and TA’ list  the 
state of each of the associated automata controlling C1(X), 
before they have been updated.  As seen, the term “reacts” 
is already Included in C1(X) because the state of the corre-
sponding  automaton  is  greater  than  or  equal  to  N+1  (i.e., 
101). It is further included with confidence because the state 
is almost at the extreme end, i.e., 2N. The other two terms, 
“to”  and  “Voltaren”,  are  excluded.  “to”  is  excluded  with 
confidence (state 1) and “Voltaren” is excluded with uncer-
tainty (state 100). In other words, the clause takes the form 
C1(X) = x29123, initially. 

 With y=1, document D1 has been flagged as an example 
of  the  target category.    Accordingly, the  Feedback  Type  I 
Table, described previously, is activated as depicted in the 
figure, and the new updated states of the Tsetlin Automata 
are listed next. As seen, after the update, “Voltaren” is now 
also included in the clause, increasing categorization preci-
sion: C1(X) = x29123 Ù x37372. 

Processing  of  Document  3:  Further  in  the  processing, 
another document, D3 , with target y = 1 comes along. This 
time C1(X) evaluates to 0, because “Voltaren” is absent from 
D3. However, another clause C2(X) = x29123 happens to only 
include “reacts”, and thus evaluates to 1. This in turn, trig-
gers Feedback Type I for clause C2(X), again leading to an 
update of the Tsetlin Automata states. As seen, the “Apocil-
lin” is now included in the clause because the associated  

 

Table 2. The Type II Feedback Table combats false positive output. 
It is triggered when a document has wrongly evaluated to false, i.e. 
output value is 1, while the target value is 0. 
 
As shown in Figure 3, a Tsetlin automaton that receives a 
Reward moves away from the center (the Tsetlin automaton 
becomes more confident in the action it currently selects). 
Conversely, when receiving a Penalty, the state of a Tsetlin 
Automaton shifts towards the center states, N/N+1 (100 or 
101), signaling increased uncertainty.  

The Type I Feedback Table is activated when a document 
from  is  either  correctly  assigned  the  target  category  (true 
positive), or mistakenly ignored (false negative). The feed-
back given by this table has two effects. Firstly, clauses are 
refined by introducing more literals from the document. Left 
alone, this effect will make the clause memorize the docu-
ment,  by  including  every  literal.  However,  this  effect  is 
countered by the second effect. The second effect is weaker 
by a factor of s, seeking to make all clauses evaluate to 1, 
whenever a document of the target category appears (and the 
clause  has  positive  polarity).  This  effect  counters  overfit-
ting. Indeed, the “granularity” of the clauses produced can 
be finely controlled by means of s. 

The second table, the Type II Feedback Table, is activated 
when a document is wrongly assigned the target category (a 
false  positive categorization).  The  effect  of this  table  is  to 
introduce literals that render the clauses false whenever they 
face a document of the incorrect category. 

Both of the tables are thus interacting, making the whole 
game of Tsetlin Automata converge towards robust pattern 
recognition configurations. 

Note that in order to effectively utilize sparse pattern rep-
resentation capacity (a constrained number of clauses), we 
use a  threshold  value  T as target  for  the  summation f1(X). 
That  is,  the  probability  of  activating  Type  I  Feedback  is: 
(T − max	(−T, min	(T, f1(X)))/2T,  while  Type  II  Feedback  is 
activated  with  probability:  (T + max	(−T, min	(T, f1(X)))/2T. 
If the votes accumulate to a total of +/- T or more, neither 
rewards or penalties are handed out to the involved Tsetlin 
Automata. The threshold mechanism thus helps to alleviate 
the vanishing signal-to-noise ratio problem, as it effectively 
removes clauses (and their literals) that already are able to 
capture a pattern (supporting the correct classification) from 
further feedback (Granmo 2018). 

Figure 4. Step-by-step example of Tsetlin Machine-based learning, demonstrating how clauses are composed and refined. 

 
 

Tsetlin Automaton changed action by going from state 100 
to 101:  C2(X) = x29123 Ù x1057. 

Processing of Document 7: Document D7 is an example 
of  a  document  not  belonging  to  the  category,  with  y  =  0. 
This example highlights the difficulty of text categorization 
task at hand. Indeed, C1(X) evaluates to 1 because the terms 
“reacts” and “Voltaren” are present, leading to a false posi-
tive categorization. However, this activates Feedback Type 
II. This time, clause C1(X) is updated by attempting to mod-
ify the clause so that it evaluates to 0 instead. This is done 
by  penalizing  all  exclude  actions  that  would  have  led  the 
clause to be evaluated 0, if one of them had been included 
instead. The most uncertain of those being penalized is the 
literal that negates “not”. After the feedback, this literal is 
now being included in the clause: C1(X) = x29123 Ù x37372 Ù 
¬x25789.  In  this  case,  the  false  positive  categorization  was 
eliminated by this simple change. In general, however, the 
game  needs  to  played  for  several  epochs  before  all  of  the 
clauses find their final form. 

Experiments  

In this section, we evaluate the Tsetlin Machine empirically, 
in comparison with selected text categorization techniques. 
To this end, we use two public datasets and one proprietary: 
1)  The  20  Newsgroups  dataset  (20  Newsgroups  Data  Set 
n.d.; Lang 1995); 2) The IMDb dataset (Maas et al. 2011); 
and 3) A clinical dataset with authentic EHRs from a hospi-
tal (Berge, Granmo, and Tveit 2017). The two first datasets 
facilitate comparison with previous research, while the third 
focuses on real-life performance in a challenging applica- 

 
tion area. The evaluation metrics we employ are macro-av-
eraged accuracy,  recall,  precision, and  F-measure.  Results 
are presented as mean percentage, with 95% confidence in-
tervals. Our CUDA and C source code for text classification 
at 
with 
https://github.com/cair/TextUnderstandingTsetlinMachine. 

the  Tsetlin  Machine 

found 

can 

be 

Experimental Setup 
The  experiments  were  conducted  on  a  server  (Intel  i7-
8700K 12-core CPU, 2GB SSD, and 64GB of memory) with 
Ubuntu  18.04  LTS  x64  installed.  We  adopted  the  Python 
(with  Cython  C/C++  extensions)  or  CUDA/GPU  capable 
C++ version of the Multi-Class Tsetlin Machine to run the  
experiments.  We  used  implementations  from  the  Scikit-
learn, Keras, and Tensorflow libraries to produce results for 
the  IMDb  dataset. This  included  Naïve  Bayes,  logistic  re-
gression,  decision  tree,  Random  forest,  linear  SVM,  kNN, 
multilayer perceptron (MLP), LSTM, LSTM CNN, Bidirec-
tional  LSTM  (Bi-LSTM)  and  Bi-LSTM  CNN.	For  the  20 
Newsgroups  and  the  clinical  dataset  experiments  we  used 
Weka (v3.9), except for StarSpace (L. Wu et al. 2017). The 
classifiers we employed in Weka were Naïve Bayes, deci-
sion tree (J48/C4.5-based decision tree algorithm, Logistic 
regression  (Multinomial  logistic  regression),  Random  for-
est,  kNN  (Instance-Based  k  - a  nearest  neighbors variant), 
SMO (Sequential Minimal Optimization - an SVM variant), 
and MLP (DL4jMlp). NLP-preprocessing in Weka and Py-
thon  (NLTK  and  Scikit-learn  libraries)  was  performed  to 
replicate  the  described  Tsetlin  Machine  configuration  for 
each of the experiments. This involved running the String-
ToWordVector filter in Weka to perform cleaning, tokeni-
zation of the input data into unigrams and quadrigrams, and 

the  InfoGainAttributeEval  method  to  determine  the  most 
significant attributes. We used the authors’ published train-
ing/test  data  split  (50%-50%)  for  the  experiments  on  the 
IMDb  dataset  (Maas  et  al.  2011).  For  the  20  Newsgroups 
and the clinical we performed 10-fold cross-validation, re-
peated  10  times  to  produce  representative  averages  with 
small  variance.  An  80%-20%  split  was  used  for  the  20 
Newsgroups dataset, and a 90%-10% split was implemented 
for the clinical dataset. 
 
The 20 Newsgroups Dataset 
The results for the modified2 20 Newsgroups dataset with 4 
classes are reported in Table 1. The Tsetlin Machine we em-
ploy here was configured using 8006 selected features, 1600 
clauses, and 100 states. Furthermore, we used an s-value of 
8.0 and a Threshold T of 25. The Tsetlin automata was run 
for 100 epochs per fold. 

As seen in Table 3, The Tsetlin Machine outperforms all 
of the approaches for text categorization we evaluated, apart 
from  the  recall  of  the  Support  Vector  Machine  (SVM), 
which  was  identical  to  the  recall  of  the  Tsetlin  Machine. 
Note that a more or less identical dataset has previously been 
used to evaluate different Convolutional Neural Network ar-
chitectures (CNN; CNN-topic, CNN-channel, CNN-model, 
CNN-concat) (Xu et al. 2016) producing accuracy scores in 
the range 94.8-95.5%. 

 
Precision  Recall 
Method 
 
98±0.6 
86.1±1.5 
Naïve Bayes 
 
99.6±0.3 
Logistic regression  99.3±0.4 
 
98.9±0.4 
99.6±0.2 
Decision tree 
 
99.3±0.4 
91±1.7 
Random forest 
 
96.4±1.0 
69.8±2.3 
kNN 
 
99.8±0.2 
99.7±0.2 
SVM 
 
96.6±1.7 
97.7±1.4 
MLP 
Tsetlin Machine 
 
99.8±0.2 
99.8±0.2 
Table 3. Average classification results for the 20 Newsgroup da-
taset. 

F-measure 
91.6±0.9 
99.4±0.2 
99.2±0.2 
94.9±0.9 
80.9±1.5 
99.7±0.2 
97.1±1.1 
99.8±0.2 

Above, accuracy was maximized by means of careful fea-
ture  selection.  To  investigate  scalability  and  robustness, 
however, we also performed experiments without feature se-
lection for the Tsetlin Machine, giving a total of 25 840 fea-
tures. Using respectively 5168 and 10 336 clauses (the other 
parameters were kept the same), we were able to maintain 
an average accuracy of 99.8% for both cases. Accordingly, 
the Tsetlin  Machine  appears to  generalize  well in  high  di-
mensional  feature  spaces,  potentially  eliminating  the  need 
for  feature  selection.  This  would  make  the  application  of 
text categorization considerably easier. 
The IMDb Dataset 
Table 4 shows the results for the IMDb dataset. The Tsetlin 
Machine was here configured to use 5 000 selected features, 
                                                
2  This  dataset  is  derived  from  the  original  20  Newsgroups  dataset  (the 
"19997" version) and contains 16 000 newsgroup documents. For compar-
ison with Xu et al. (Xu et al. 2016), we strived to use a similar setup; i.e. 

10 000 clauses, and 500 states. Furthermore, we used an s-
value of 27.0, a Threshold T of 20, and ran the Tsetlin Ma-
chine for 200 epochs. Similar to the 20 Newsgroups dataset, 
the reported results are from the final epoch. 

As  observed  in  Table  4, the Tsetlin  Machine  again  out-
performed the other methods. The macro averaged accuracy 
of the Tsetlin Machine in this experiment was 89.2± 0.2,  

86.1±0.0 
87.6±0.0 
68.4±0.0 
86.1±0.0 
63.5±0.0 
89.1±0.0 
84.0±0.0 
84.3±4.8 
86.8±1.9 
83.9±5.4 
87.5±2.4 
88.2±1.5 

F-measure 
86.0±0.0 
87.1±0.0 
69.7±0.0 
86.0±0.0 
60.8±0.0 
88.5±0.0 
83.0±0.0 
85.6±3.0 
88.1±0.5 
85.5±2.8 
87.9±0.9 
89.1±0.4 

Precision  Recall 
85.9±0.0 
86.5±0.0 
71.1±0.0 
85.9±0.0 
58.4±0.0 
88.0±0.0 
82.0±0.0 
87.2±3.5 
89.5±1.1 
87.6±4.1 
88.3±1.2 
90.1±1.0 

Method 
Multinom. Naïve Bayes 
Logistic regression 
Decision tree 
Random forest 
kNN 
Linear SVM 
MLP  
LSTM 
LSTM CNN 
Bi-LSTM 
Bi-LSTM CNN 
Tsetlin Machine 
Table 4. Average classification results for the IMDb dataset. 
  
with 89.7% being the highest accuracy over all the epochs 
and folds. To facilitate a statistically robust comparison of 
the  different  techniques,  the  results  in  the  table  are  mean 
scores from 100 independent runs, together with 95% con-
fidence intervals. By reporting the mean, our results are by 
nature lower than what one achieves by e.g. reporting the 5th 
best accuracy over for instance 100 trials. 

Utilizing  a  convolutional  neural  network  (CNN)  and  a 
Long  short-term  memory  (LSTM)  network  to  classify  the 
same  data,  Yenter  and  Verma  (Yenter  and  Verma  2017) 
achieved maximum accuracy scores of between 89.22% and 
89.5%  on  five  different  models,  and  thus  concluded  that 
their proposed model outperformed prior relevant research 
(Rahman,  Mohammed,  and  Al  Azad  2016;  Tripathy, 
Agrawal, and Rath 2016; Maas et al. 2011). Recent years, 
several authors have also reported even better results by us-
ing  ensemble  methods  or  by  introducing  combinations  of 
techniques such as entropy minimization loss, adversarial, 
and  virtual  adversarial  training  (S.  Wang  and  Manning 
2012;  Mesnil  et  al. 2014;  Zhao  2016; Johnson  and  Zhang 
2015; Dieng et al. 2016; Miyato, Dai, and Goodfellow 2016; 
Sachan,  Zaheer,  and  Salakhutdinov  2018;  Radford, 
Jozefowicz,  and  Sutskever  2017;  Gray,  Radford,  and 
Kingma  2017;  Howard  and  Ruder  2018).  For  example, 
Wang and Manning reported an accuracy of 91.22% by im-
plementing SVM with NB features (NBSVM-bi) (S. Wang 
and  Manning  2012).  Using  deep  learning,  Miyato  et  al. 
(Miyato,  Dai,  and  Goodfellow  2016)  were able  to  achieve 
an accuracy score of 94.09% by using adversarial training 
methods. Finally, by implementing a mixed objective func-

the sub-categories has been grouped into the most common categories com-
puters, recreation, science and politics. 

tion (L-mixed) and word embedding, Singh Sachan et al. ac-
complished an accuracy score of 95.68% (Sachan, Zaheer, 
and Salakhutdinov 2018). 
 
In our further work, we intend to investigate how we can 
enhance our relatively plain modelling of text (presence and 
absence of terms) to further increase accuracy. One option 
is for example to introduce pretrained word embedding. 
The Clinical Dataset 
Table 5 displays results for the clinical dataset. Besides re-
porting categorization accuracy results, we also report total 
run time in hours. The complexity of the dataset is reflected 
in the high standard deviations produced by all of the algo-
rithms.  To  produce  the  final  results  presented  here,  the 
Tsetlin Machine was configured to use 38876 features, 500 
clauses, 100 states, s-value of 3.0, Threshold T of 25, and to 
run 100 epochs per fold. 

Method 
Naïve Bayes 
Logistic regression 
Decision tree 
Random forest 
kNN 
SVM 
MLP 
StarSpace 
Tsetlin Machine 
Table 5. Average classification results for the clinical dataset.  

F-measure 
 Recall 
69.9±6.6 
 76.2±9.1 
69.6±7.2 
 66.9±9.1 
 57.3±12.0  57.9±8.7 
 56.1±10.3  58.9±8.4 
 36.8±10.8  47.1±11.4 
 59.7±10.0  63.7±8.0 
 67.1±12.3  67.2±10.9 
 59.8±9.7 
59.3±9.0 
 70.2±11.6  69.3±8.1 

  Precision 
  65±7.2 
  73.4±9.1 
  59.7±9.1 
  63±9.3 
  68±14.6 
  69.3±8.4 
  70.3±17.6 
  61.7±13.2 
  70.3±10.8 

 
 
 
 
 
 
 
 
 
 

When considering the F-measure results, the Tsetlin Ma-
chine on this classification task has to yield by a very small 
margin to Naïve Bayes in first place and Logistic regression 
in  second  place.  Interestingly,  these  two  linear  algorithms 
outperform all  of  the  other  methods, including  the  usually 
more capable nonlinear algorithms (e.g., SVM, neural net-
work, and the Tsetlin Machine). A peak accuracy of 79.1% 
was  achieved in epoch  294  when  running the Tsetlin  Ma-
chine for an extended total of 2000 epochs, possibly reflect-
ing temporary overfitting on the test set (in later epochs, the 
accuracy results stabilized around 70%). It is reasonable to 
believe that the Naive Bayes and Logistic regression classi-
fiers in this experiment gain from their simple (linear) struc-
ture, preventing them from fitting training data too closely 
(Ng and Jordan 2002). Similar to Naïve Bayes, the Tsetlin 
Machine has also demonstrated accuracy advantages when 
faced with a lack of data, which to some extent may possibly 
explain why the Tsetlin Machine is able to outperform the 
other nonlinear capable algorithms employed in this experi-
ment (Granmo 2018). 

Note that we for all of the above experiments have also 
used  a  GPU  implementation  of  the  Tsetlin  Machine.  The 
GPU version executes 5 to 15 times faster than the CPU im-
plementation, depending on the dataset. 

Although we conducted extensive hyperparameter tuning 
for the Tsetlin Machine (manual grid search) and the other 

evaluated algorithms (e.g., autotuning by use of the CVPa-
rameterSelection metafilter in Weka), even better configu-
rations may potentially exist. We have, however, strived to 
put an equal effort into optimizing each approach to facili-
tate a fair comparison. 

Conclusion and Future Work 

This paper proposed a text categorization approach based 
on the recently introduced Tsetlin  Machine.  In all  brevity, 
we represent the terms of a text as propositional variables. 
From  these,  we  capture  categories  using  simple  proposi-
tional  formulae  that  are  easy  to  interpret  for  humans. The 
Tsetlin Machine learns these formulae from a labelled text, 
utilizing conjunctive clauses to represent the particular fac-
ets of each category.  Our empirical results were quite con-
clusive. The Tsetlin Machine either performed on par with 
or  outperformed  all  of  the  evaluated  methods  on  both  the 
Newsgroups and IMDb datasets, as well as on a non-public 
clinical dataset. On average, the Tsetlin Machine delivered 
the best recall and precision scores across the datasets.  

In our further work, we plan to examine how to use the 
Tsetlin Machine for unsupervised learning of word embed-
dings.  Furthermore,  we  will  investigate  how  the  sparse 
structure of documents can be taken advantage of to speed 
up learning. We further plan to leverage local context win-
dows to learn structure in paragraphs and sentences for more 
precise text representation. Finally, based on our promising 
results  we  envision  implementing  the  Tsetlin  Machine  in 
clinical decision support systems (Berge, Granmo, and Tveit 
2017).  In  particular,  we  are  interested  in  how  structured 
medical data can be combined with the medical narrative to 
support precision medicine. 

All in all, we believe that our novel Tsetlin Machine based 
approach can have a significant impact on a wide range of 
text analysis applications. Furthermore, we believe that the 
approach  and  results  presented  in  this  paper  can  form  a 
promising starting point for deeper natural language under-
standing.  

 

References 
20 Newsgroups Data Set. n.d. Accessed September 6, 2018. 
http://qwone.com/~jason/20Newsgroups/. 
Berge, Geir Thore, Ole-Christoffer Granmo, and Tor Oddbjørn 
Tveit. 2017. Combining Unsupervised, Supervised, and Rule-
Based Algorithms for Text Mining of Electronic Health Records-
A Clinical Decision Support System for Identifying and Classify-
ing Allergies of Concern for Anesthesia During Surgery. In 
ISD2017 Proceedings. AIS. 
Conneau, Alexis, Holger Schwenk, Loïc Barrault, and Yann 
Lecun. 2017. Very Deep Convolutional Networks for Text Classi-
fication. In European Chapter of the Association for Computa-
tional Linguistics EACL’17. 
Dieng, Adji B., Chong Wang, Jianfeng Gao, and John Paisley. 
2016. Topicrnn: A Recurrent Neural Network with Long-Range 
Semantic Dependency. ArXiv Preprint ArXiv:1611.01702. 

Granmo, Ole-Christoffer. 2018. The Tsetlin Machine-A Game 
Theoretic Bandit Driven Approach to Optimal Pattern Recogni-
tion with Propositional Logic. ArXiv Preprint ArXiv:1804.01508. 
Gray, Scott, Alec Radford, and Diederik P. Kingma. 2017. GPU 
Kernels for Block-Sparse Weights. Technical report, OpenAI. 
Howard, Jeremy, and Sebastian Ruder. 2018. Universal Language 
Model Fine-Tuning for Text Classification. In Proceedings of the 
56th Annual Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), 1:328–339. 
Joachims, Thorsten. 1998. Text Categorization with Support Vec-
tor Machines: Learning with Many Relevant Features. In Euro-
pean Conference on Machine Learning, 137–142. Springer. 
Joachims, Thorsten. 2002. A Statistical Learning Model of Text 
Classification for SVMs. In Learning to Classify Text Using Sup-
port Vector Machines, 45–74. Springer. 
Johnson, Rie, and Tong Zhang. 2015. Effective Use of Word Or-
der for Text Categorization with Convolutional Neural Networks. 
In Proceedings of the 2015 Conference of the North American 
Chapter of the Association for Computational Linguistics: Human 
Language Technologies, 103–112. 
Kovačević, Aleksandar, Azad Dehghan, Michele Filannino, John 
A. Keane, and Goran Nenadic. 2013. Combining Rules and Ma-
chine Learning for Extraction of Temporal Expressions and 
Events from Clinical Narratives. Journal of the American Medical 
Informatics Association 20 (5): 859–866. 
Lang, Ken. 1995. Newsweeder: Learning to Filter Netnews. In 
Machine Learning Proceedings 1995, 331–339. Elsevier. 
Linell, P. 1982. The Written Language Bias in Linguistics Studies 
in Communication 2 (SIC 2). Department of Communication 
Studies, Link Oping University. 
Maas, Andrew L., Raymond E. Daly, Peter T. Pham, Dan Huang, 
Andrew Y. Ng, and Christopher Potts. 2011. Learning Word Vec-
tors for Sentiment Analysis. In Proceedings of the 49th Annual 
Meeting of the Association for Computational Linguistics: Hu-
man Language Technologies-Volume 1, 142–150. Association for 
Computational Linguistics. 
Mesnil, Grégoire, Tomas Mikolov, Marc’Aurelio Ranzato, and 
Yoshua Bengio. 2014. Ensemble of Generative and Discrimina-
tive Techniques for Sentiment Analysis of Movie Reviews. CoRR 
abs/1412.5335. 
Miotto, Riccardo, Fei Wang, Shuang Wang, Xiaoqian Jiang, and 
Joel T. Dudley. 2017. Deep Learning for Healthcare: Review, 
Opportunities and Challenges. Briefings in Bioinformatics. 
Mittelstadt, Brent Daniel, Patrick Allo, Mariarosaria Taddeo, 
Sandra Wachter, and Luciano Floridi. 2016. The Ethics of Algo-
rithms: Mapping the Debate. Big Data & Society 3 (2): 
2053951716679679. 
Miyato, Takeru, Andrew M. Dai, and Ian Goodfellow. 2016. Ad-
versarial Training Methods for Semi-Supervised Text Classifica-
tion. ArXiv Preprint ArXiv:1605.07725. 
Ng, Andrew Y., and Michael I. Jordan. 2002. On Discriminative 
vs. Generative Classifiers: A Comparison of Logistic Regression 
and Naive Bayes. In Advances in Neural Information Processing 
Systems, 841–848. 
Norvig, Peter. 1987. Inference in Text Understanding. In AAAI, 
561–565. 

Radford, Alec, Rafal Jozefowicz, and Ilya Sutskever. 2017. 
Learning to Generate Reviews and Discovering Sentiment. ArXiv 
Preprint ArXiv:1704.01444. 
Rahman, Lamia, Nabeel Mohammed, and Abul Kalam Al Azad. 
2016. A New LSTM Model by Introducing Biological Cell State. 
In Electrical Engineering and Information Communication Tech-
nology (ICEEICT), 2016 3rd International Conference On, 1–6. 
IEEE. 
Sachan, Devendra Singh, Manzil Zaheer, and Ruslan Salakhutdi-
nov. 2018. Revisiting LSTM Networks for Semi-Supervised Text 
Classification via Mixed Objective Function. 
Schmidhuber, Jürgen. 2015. Deep Learning in Neural Networks: 
An Overview. Neural Networks 61: 85–117. 
Schütze, Hinrich, Christopher D. Manning, and Prabhakar 
Raghavan. 2008. Introduction to Information Retrieval. Vol. 39. 
Cambridge University Press. 
Subramanian, Sandeep, Adam Trischler, Yoshua Bengio, and 
Christopher J. Pal. 2018. Learning General Purpose Distributed 
Sentence Representations via Large Scale Multi-Task Learning. 
ArXiv Preprint ArXiv:1804.00079. 
Thathachar, M. A., and Kumpati Narendra. 1989. Learning Au-
tomata, an Introduction. Printicc Hail Me, 19S9. 
Tripathy, Abinash, Ankit Agrawal, and Santanu Kumar Rath. 
2016. Classification of Sentiment Reviews Using N-Gram Ma-
chine Learning Approach. Expert Systems with Applications 57: 
117–126. 
Tsetlin, Michael L. 1961. On Behaviour of Finite Automata in 
Random Medium. Avtom I Telemekhanika 22: 1345–1354. 
Uzuner, Özlem, Brett R. South, Shuying Shen, and Scott L. Du-
Vall. 2011. 2010 I2b2/VA Challenge on Concepts, Assertions, 
and Relations in Clinical Text. Journal of the American Medical 
Informatics Association 18 (5): 552–556. 
Valdes, Gilmer, José Marcio Luna, Eric Eaton, Charles B. 
Simone II, Lyle H. Ungar, and Timothy D. Solberg. 2016. Medi-
Boost: A Patient Stratification Tool for Interpretable Decision 
Making in the Era of Precision Medicine. Scientific Reports 6: 
37854. 
Valiant, Leslie G. 1984. A Theory of the Learnable. Communica-
tions of the ACM 27 (11): 1134–1142. 
Wang, Sida, and Christopher D. Manning. 2012. Baselines and 
Bigrams: Simple, Good Sentiment and Topic Classification. In 
Proceedings of the 50th Annual Meeting of the Association for 
Computational Linguistics: Short Papers-Volume 2, 90–94. Asso-
ciation for Computational Linguistics. 
Wang, Tong, Cynthia Rudin, Finale Doshi-Velez, Yimin Liu, Er-
ica Klampfl, and Perry MacNeille. 2017. A Bayesian Framework 
for Learning Rule Sets for Interpretable Classification. The Jour-
nal of Machine Learning Research 18 (1): 2357–2393. 
Wang, Yanshan, Liwei Wang, Majid Rastegar-Mojarad, Sungrim 
Moon, Feichen Shen, Naveed Afzal, Sijia Liu, Yuqun Zeng, 
Saeed Mehrabi, and Sunghwan Sohn. 2017. Clinical Information 
Extraction Applications: A Literature Review. Journal of Bio-
medical Informatics. 
Wu, Ledell, Adam Fisch, Sumit Chopra, Keith Adams, Antoine 
Bordes, and Jason Weston. 2017. StarSpace: Embed All The 
Things! ArXiv Preprint ArXiv:1709.03856. 

Wu, Mike, Michael C. Hughes, Sonali Parbhoo, Maurizio Zazzi, 
Volker Roth, and Finale Doshi-Velez. 2017. Beyond Sparsity: 
Tree Regularization of Deep Models for Interpretability. ArXiv 
Preprint ArXiv:1711.06178. 
Xu, Haotian, Ming Dong, Dongxiao Zhu, Alexander Kotov, April 
Idalski Carcone, and Sylvie Naar-King. 2016. Text Classification 
with Topic-Based Word Embedding and Convolutional Neural 
Networks. In Proceedings of the 7th ACM-BCB, 88–97. ACM. 
Yenter, Alec, and Abhishek Verma. 2017. Deep CNN-LSTM 
with Combined Kernels from Multiple Branches for IMDb Re-
view Sentiment Analysis. In Ubiquitous Computing, Electronics 
and Mobile Communication Conference (UEMCON), 2017 IEEE 
8th Annual, 540–546. IEEE. 
Zhang, Xiang, Junbo Zhao, and Yann LeCun. 2015. Character-
Level Convolutional Networks for Text Classification. In Ad-
vances in Neural Information Processing Systems, 649–657. 
Zhao, Z. 2016. Learning Document Embeddings by Predicting N-
Grams for Sentiment Classification of Long Movie Reviews. In 
Accepted as a Workshop Contribution, ICLR. 
Zipf, George Kingsley. 1932. Selected Studies of the Principle of 
Relative Frequency in Language. 
 

