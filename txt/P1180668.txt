8
1
0
2
 
v
o
N
 
2
 
 
]

G
L
.
s
c
[
 
 
1
v
6
6
8
0
0
.
1
1
8
1
:
v
i
X
r
a

Efﬁcient Neural Network Robustness Certiﬁcation
with General Activation Functions

Huan Zhang1,3,†,∗ Tsui-Wei Weng2,†

Pin-Yu Chen3 Cho-Jui Hsieh1 Luca Daniel2

1University of California, Los Angeles, Los Angeles CA 90095
2Massachusetts Institute of Technology, Cambridge, MA 02139
3IBM Research, Yorktown Heights, NY 10598
huan@huan-zhang.com, twweng@mit.edu
pin-yu.chen@ibm.com, chohsieh@cs.ucla.edu, dluca@mit.edu

Abstract

Finding minimum distortion of adversarial examples and thus certifying robustness
in neural network classiﬁers for given data points is known to be a challenging
problem. Nevertheless, recently it has been shown to be possible to give a non-
trivial certiﬁed lower bound of minimum adversarial distortion, and some recent
progress has been made towards this direction by exploiting the piece-wise linear
nature of ReLU activations. However, a generic robustness certiﬁcation for general
activation functions still remains largely unexplored. To address this issue, in
this paper we introduce CROWN, a general framework to certify robustness of
neural networks with general activation functions for given input data points. The
novelty in our algorithm consists of bounding a given activation function with linear
and quadratic functions, hence allowing it to tackle general activation functions
including but not limited to four popular choices: ReLU, tanh, sigmoid and arctan.
In addition, we facilitate the search for a tighter certiﬁed lower bound by adaptively
selecting appropriate surrogates for each neuron activation. Experimental results
show that CROWN on ReLU networks can notably improve the certiﬁed lower
bounds compared to the current state-of-the-art algorithm Fast-Lin, while having
comparable computational efﬁciency. Furthermore, CROWN also demonstrates
its effectiveness and ﬂexibility on networks with general activation functions,
including tanh, sigmoid and arctan.

1

Introduction

While neural networks (NNs) have achieved remarkable performance and accomplished unprece-
dented breakthroughs in many machine learning tasks, recent studies have highlighted their lack of
robustness against adversarial perturbations [1, 2]. For example, in image learning tasks such as object
classiﬁcation [3, 4, 5, 6] or content captioning [7], visually indistinguishable adversarial examples can
be easily crafted from natural images to alter a NN’s prediction result. Beyond the white-box attack
setting where the target model is entirely transparent, visually imperceptible adversarial perturbations
can also be generated in the black-box setting by only using the prediction results of the target model
[8, 9, 10, 11]. In addition, real-life adversarial examples have been made possible through the lens
of realizing physical perturbations [12, 13, 14]. As NNs are becoming a core technique deployed in
a wide range of applications, including safety-critical tasks, certifying robustness of a NN against
adversarial perturbations has become an important research topic in machine learning.

∗Work done during internship at IBM Research.

†Equal contribution.

Correspondence to Huan Zhang <huan@huan-zhang.com> and Tsui-Wei Weng <twweng@mit.edu>

32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.

Given a NN (possibly with a deep and complicated network architecture), we are interested in
certifying the (local) robustness of an arbitrary natural example x0 by ensuring all its neighborhood
has the same inference outcome (e.g., consistent top-1 prediction). In this paper, the neighborhood
of x0 is characterized by an (cid:96)p ball centered at x0, for any p ≥ 1. Geometrically speaking, the
minimum distance of a misclassiﬁed nearby example to x0 is the least adversary strength (a.k.a.
minimum adversarial distortion) required to alter the target model’s prediction, which is also the
largest possible robustness certiﬁcate for x0. Unfortunately, ﬁnding the minimum distortion of
adversarial examples in NNs with Rectiﬁed Linear Unit (ReLU) activations, which is one of the
most widely used activation functions, is known to be an NP-complete problem [15, 16]. This makes
formal veriﬁcation techniques such as Reluplex [15] computationally demanding even for small-sized
NNs and suffer from scalability issues.

Although certifying the largest possible robustness is challenging for ReLU networks, the piece-
wise linear nature of ReLUs can be exploited to efﬁciently compute a non-trivial certiﬁed lower
bound of the minimum distortion [17, 18, 19, 20]. Beyond ReLU, one fundamental problem that
remains largely unexplored is how to generalize the robustness certiﬁcation technique to other popular
activation functions that are not piece-wise linear, such as tanh and sigmoid, and how to motivate and
certify the design of other activation functions towards improved robustness. In this paper, we tackle
the preceding problem by proposing an efﬁcient robustness certiﬁcation framework for NNs with
general activation functions. Our main contributions in this paper are summarized as follows:

• We propose a generic analysis framework CROWN for certifying NNs using linear or quadratic
upper and lower bounds for general activation functions that are not necessarily piece-wise linear.
• Unlike previous work [20], CROWN allows ﬂexible selections of upper/lower bounds for activation
functions, enabling an adaptive scheme that helps to reduce approximation errors. Our experiments
show that CROWN achieves up to 26% improvements in certiﬁed lower bounds compared to [20].
• Our algorithm is efﬁcient and can scale to large NNs with various activation functions. For a NN
with over 10,000 neurons, we can give a certiﬁed lower bound in about 1 minute on 1 CPU core.

2 Background and Related Work

For ReLU networks, ﬁnding the minimum adversarial distortion for a given input data point x0 can
be cast as a mixed integer linear programming (MILP) problem [21, 22, 23]. Reluplex [15, 24] uses
a satisﬁable modulo theory (SMT) to encode ReLU activations into linear constraints. Similarly,
Planet [25] uses satisﬁability (SAT) solvers. However, due to the NP-completeness for solving such a
problem [15], these methods can only ﬁnd minimum distortion for very small networks. It can take
Reluplex several hours to ﬁnd the minimum distortion of an example for a ReLU network with 5
inputs, 5 outputs and 300 neurons[15].

On the other hand, a computationally feasible alternative of robustness certiﬁcate is to provide a
non-trivial and certiﬁed lower bound of minimum distortion. Some analytical lower bounds based on
operator norms on the weight matrices [3] or the Jacobian matrix in NNs [17] do not exploit special
property of ReLU and thus can be very loose [20]. The bounds in [26, 27] are based on the local
Lipschitz constant. [26] assumes a continuous differentiable NN and hence excludes ReLU networks;
a closed form lower-bound is also hard to derive for networks beyond 2 layers. [27] applies to ReLU
networks and uses Extreme Value Theory to provide an estimated lower bound (CLEVER score).
Although the CLEVER score is capable of reﬂecting the level of robustness in different NNs and is
scalable to large networks, it is not a certiﬁed lower bound. On the other hand, [18] use the idea of a
convex outer adversarial polytope in ReLU networks to compute a certiﬁed lower bound by relaxing
the MILP certiﬁcation problem to linear programing (LP). [19] apply semideﬁnite programming for
robustness certiﬁcation in ReLU networks but their approach is limited to NNs with one hidden layer.
[20] exploit the ReLU property to bound the activation function (or the local Lipschitz constant) and
provide efﬁcient algorithms (Fast-Lin and Fast-Lip) for computing a certiﬁed lower bound, achieving
state-of-the-art performance. A recent work [28] uses abstract transformations to zonotopes for
proving robustness property for ReLU networks. Nonetheless, there are still some applications
demand non-ReLU activations, e.g. RNN and LSTM, thus a general framework that can efﬁciently
compute non-trivial and certiﬁed lower bounds for NNs with general activation functions is of great
importance. We aim at ﬁlling this gap and propose CROWN that can perform efﬁcient robustness
certiﬁcation to NNs with general activation functions. Table 1 summarizes the differences of other
approaches and CROWN. Note that a recent work [29] based on solving Lagrangian dual can also
handle general activation functions but it trades off the quality of robustness bound with scalability.

2

Table 1: Comparison of methods for providing adversarial robustness certiﬁcation in NNs.
Scalability Beyond ReLU
Non-trivial bound Multi-layer
Method
(cid:88)
(cid:88)
×
Szegedy et. al. [3]
(cid:88)
×
×
Reluplex [15], Planet [25]
(cid:88)
Hein & Andriushchenko [26] (cid:88)
differentiable*
(cid:88)
×
×
Raghunathan et al. [19]
(cid:88)
(cid:88)
×
Kolter and Wong [18]
(cid:88)
(cid:88)
×
Fast-lin / Fast-lip [20]
(cid:88) (general)
(cid:88)
(cid:88)
CROWN (ours)
* Continuously differentiable activation function required (soft-plus is demonstrated in [26])

(cid:88)
(cid:88)
×
×
(cid:88)
(cid:88)
(cid:88)

Some recent works (such as robust optimization based adversarial training [30] or region-based
classiﬁcation [31]) empirically exhibit strong robustness against several adversarial attacks, which
is beyond the scope of provable robustness certiﬁcation. In addition, Sinha et al. [16] provide
distributional robustness certiﬁcation based on Wasserstein distance between data distributions, which
is different from the local (cid:96)p ball robustness model considered in this paper.

3 CROWN: A general framework for certifying neural networks

Overview of our results.
In this section, we present a general framework CROWN for efﬁciently
computing a certiﬁed lower bound of minimum adversarial distortion given any input data point x0
with general activation functions in larger NNs. We ﬁrst provide principles in Section 3.1 to derive
output bounds of NNs when the inputs are perturbed within an (cid:96)p ball and each neuron has different
(adaptive) linear approximation bounds on its activation function. In Section 3.2, we demonstrate how
to provide robustness certiﬁcation for four widely-used activation functions (ReLU, tanh, sigmoid
and arctan) using CROWN. In particular, we show that the state-of-the-art Fast-Lin algorithm is a
special case under the CROWN framework and that the adaptive selections of approximation bounds
allow CROWN to achieve a tighter (larger) certiﬁed lower bound (see Section 4). In Section 3.3, we
further highlight the ﬂexibility of CROWN to incorporate quadratic approximations on the activation
functions in addition to the linear approximations described in Section 3.1.

3.1 General framework

Notations. For an m-layer neural network with an input vector x ∈ Rn0 , let the number of
neurons in each layer be nk, ∀k ∈ [m], where [i] denotes set {1, 2, · · · , i}. Let the k-th layer weight
matrix be W(k) ∈ Rnk×nk−1 and bias vector be b(k) ∈ Rnk , and let Φk : Rn0 → Rnk be the
operator mapping from input to layer k. We have Φk(x) = σ(W(k)Φk−1(x) + b(k)), ∀k ∈ [m − 1],
where σ(·) is the coordinate-wise activation function. While our methodology is applicable to any
activation function of interest, we emphasize on four most widely-used activation functions, namely
ReLU: σ(y) = max(y, 0), hyperbolic tangent: σ(y) = tanh(y), sigmoid: σ(y) = 1/(1 + e−y)
and arctan: σ(y) = arctan(y). Note that the input Φ0(x) = x, and the vector output of the NN is
f (x) = Φm(x) = W(m)Φm−1(x)+b(m). The j-th output element is denoted as fj(x) = [Φm(x)]j.

r = W(k)

, where y(k)

Input perturbation and pre-activation bounds. Let x0 ∈ Rn0 be a given data point, and let the
perturbed input vector x be within an (cid:15)-bounded (cid:96)p-ball centered at x0, i.e., x ∈ Bp(x0, (cid:15)), where
Bp(x0, (cid:15)) := {x | (cid:107)x − x0(cid:107)p ≤ (cid:15)}. For the r-th neuron in k-th layer, let its pre-activation input be
y(k)
and W(k)
r,: denotes the r-th row of matrix W(k). When
r
x0 is perturbed within an (cid:15)-bounded (cid:96)p-ball, let l(k)
, u(k)
r ∈ R be the pre-activation lower bound and
, i.e. l(k)
upper bound of y(k)
Below, we ﬁrst deﬁne the linear upper bounds and lower bounds of activation functions in Deﬁni-
tion 3.1, which are the key to derive explicit output bounds for an m-layer neural network with general
activation functions. The formal statement of the explicit output bounds is shown in Theorem 3.2.

r,: Φk−1(x) + b(k)

r ≤ u(k)

r ≤ y(k)

r

r

r

r

.

Deﬁnition 3.1 (Linear bounds on activation function). For the r-th neuron in k-th layer with pre-
U,r, h(k)
activation bounds l(k)
L,r :
R → R, h(k)
L,r(y) ≤ σ(y) ≤
U,r(y), y ∈ [l(k)
h(k)

r and the activation function σ(y), deﬁne two linear functions h(k)
,u(k)
L,r(y + β(k)
L,r(y) = α(k)
U,r), h(k)
U,r(y + β(k)
U,r(y) = α(k)
U,r, α(k)
], ∀k ∈ [m − 1], r ∈ [nk] and α(k)
, u(k)
r

L,r), such that h(k)

L,r ∈ R+, β(k)

U,r, β(k)

L,r ∈ R.

r

r

3

r

and u(k)

U,r, β(k)

L,r, β(k)

U,r, α(k)

L,r depend on l(k)

Note that the parameters α(k)
u(k)
r we may choose different parameters. Also, for ease of exposition, in this paper we restrict
α(k)
U,r, α(k)
L,r ≥ 0. However, Theorem 3.2 can be easily generalized to the case of negative α(k)
U,r, α(k)
L,r.
Theorem 3.2 (Explicit output bounds of neural network f ). Given an m-layer neural network
: Rn0 → R such
function f : Rn0 → Rnm , there exists two explicit functions f L
that ∀j ∈ [nm], ∀x ∈ Bp(x0, (cid:15)), the inequality f L
m
(cid:88)

j : Rn0 → R and f U
j
j (x) ≤ fj(x) ≤ f U
j (x) holds true, where
m
(cid:88)

, i.e. for different l(k)

and

r

r

j (x) = Λ(0)
f U

j,: x +

Λ(k)

j,: (b(k) + ∆(k)
:,j ),

j (x) = Ω(0)
f L

j,: x +

Ω(k)

j,: (b(k) + Θ(k)
:,j ),

(1)

k=1

k=1

Λ(k−1)
j,:

=

(cid:26)e(cid:62)

j
(Λ(k)

j,: W(k)) (cid:12) λ(k−1)

j,:

if k = m + 1;
if k ∈ [m].

Ω(k−1)
j,:

=

(cid:26)e(cid:62)

j
(Ω(k)

j,: W(k)) (cid:12) ω(k−1)

j,:

if k = m + 1;
if k ∈ [m].

and ∀i ∈ [nk], we deﬁne four matrices λ(k), ω(k), ∆(k), Θ(k) ∈ Rnm×nk :

λ(k)
j,i =

∆(k)

i,j =




α(k)
U,i
α(k)
L,i

1

β(k)

U,i
β(k)
L,i
0



if k (cid:54)= 0, Λ(k+1)
if k (cid:54)= 0, Λ(k+1)
if k = 0.
if k (cid:54)= m, Λ(k+1)
if k (cid:54)= m, Λ(k+1)
if k = m.

j,: W(k+1)
j,: W(k+1)

:,i

:,i

≥ 0;
< 0;

ω(k)
j,i =

j,: W(k+1)
j,: W(k+1)

:,i

:,i

≥ 0;
< 0;

Θ(k)

i,j =




α(k)
L,i
α(k)
U,i
1
β(k)
L,i
β(k)
U,i

0





if k (cid:54)= 0, Ω(k+1)
if k (cid:54)= 0, Ω(k+1)
if k = 0.
if k (cid:54)= m, Ω(k+1)
if k (cid:54)= m, Ω(k+1)
if k = m.

j,: W(k+1)
j,: W(k+1)

:,i

:,i

≥ 0;
< 0;

j,: W(k+1)
j,: W(k+1)

:,i

:,i

≥ 0;
< 0;

and (cid:12) is the Hadamard product and ej ∈ Rnm is a standard unit vector at jth coordinate .

U,r and h(k)

Theorem 3.2 illustrates how a NN function fj(x) can be bounded by two linear functions f U
j (x)
j (x) when the activation function of each neuron is bounded by two linear functions h(k)
and f L
U,r
and h(k)
L,r in Deﬁnition 3.1. The central idea is to unwrap the activation functions layer by layer
by considering the signs of the associated (equivalent) weights of each neuron and apply the two
linear bounds h(k)
L,r. As we demonstrate in the proof, when we replace the activation
functions with the corresponding linear upper bounds and lower bounds at the layer m − 1, we
can then deﬁne equivalent weights and biases based on the parameters of h(m−1)
(e.g.
Λ(k), ∆(k), Ω(k), Θ(k) are related to the terms α(k)
L,r, respectively) and then repeat
the procedure to “back-propagate” to the input layer. This allows us to obtain f U
j (x) in
(1). The formal proof of Theorem 3.2 is in Appendix A. Note that for a neuron r in layer k, the slopes
of its linear upper and lower bounds α(k)

L,r can be different. This implies:

and h(m−1)
L,r

j (x) and f L

U,r, α(k)

U,r, α(k)

L,r, β(k)

U,r, β(k)

U,r

1. Fast-Lin [20] is a special case of our framework as they require the slopes α(k)

L,r to be the
same; and it only applies to ReLU networks (cf. Sec. 3.2). In Fast-Lin, Λ(0) and Ω(0) are identical.
2. Our CROWN framework allows adaptive selections on the linear approximation when computing
certiﬁed lower bounds of minimum adversarial distortion, which is the main contributor to improve
the certiﬁed lower bound as demonstrated in the experiments in Section 4.

U,r, α(k)

j (x), and minimum, i.e. minx∈Bp(x0,(cid:15)) f L

Global bounds. More importantly, since the input x ∈ Bp(x0, (cid:15)), we can take the maximum, i.e.
maxx∈Bp(x0,(cid:15)) f U
j (x), as a pair of global upper and lower
bound of fj(x) – which in fact has closed-form solutions because f U
j (x) are two linear
functions and x ∈ Bp(x0, (cid:15)) is a convex norm constraint. This result is formally presented below and
its proof is given in Appendix B.
Corollary 3.3 (Closed-form global bounds). Given a data point x0 ∈ Rn0, (cid:96)p ball parameters p ≥ 1
and (cid:15) > 0. For an m-layer neural network function f : Rn0 → Rnm , there exists two ﬁxed values γL
j
and γU
j ≤ fj(x) ≤ γU
j
holds true, where

j such that ∀x ∈ Bp(x0, (cid:15)) and ∀j ∈ [nm], 1/q = 1 − 1/p, the inequality γL

j (x) and f L

j = (cid:15)(cid:107)Λ(0)
γU

j,: (cid:107)q + Λ(0)

j,: x0 +

Λ(k)

j,: (b(k) + ∆(k)

:,j ), γL

j = −(cid:15)(cid:107)Ω(0)

j,: (cid:107)q + Ω(0)

j,: x0 +

Ω(k)

j,: (b(k) + Θ(k)

:,j ).

m
(cid:88)

k=1

m
(cid:88)

k=1

(2)

4

Table 2: Linear upper bound parameters of various activation functions: h(k)

U,r(y) = α(k)

Upper bound h(k)
U,r
for activation function

ReLU

r ∈ S +
k
β(k)
U,r

0

α(k)
U,r

1

r ∈ S −
k

α(k)
U,r

0

β(k)
U,r

0

Sigmoid, tanh

(denoted as σ(y))
* If α(k)

σ(cid:48)(d)

− d *

σ(d)
α(k)
U,r
r ≤ d ≤ u(k)
r )

(l(k)

r )−σ(l(k)
σ(u(k)
r )
r −l(k)
u(k)

r

σ(l(k)
r )
α(k)
U,r

− l(k)
r

U,r is close to 0, we suggest to calculate the intercept directly, α(k)

U,r · β(k)

issues in implementation. Same for other similar cases.

(cid:5) Alternatively, if the solution d ≥ u(k)

, then we can set α(k)

r

r )−σ(l(k)
U,r = σ(u(k)
r )
r −l(k)
u(k)

r

.

U,r(y + β(k)
U,r)
r ∈ S ±
k
β(k)
U,r
−l(k)
r
, e.g. a = u
σ(l(k)
− l(k)
r )
r
α(k)
U,r

u

α(k)
U,r

a
(a ≥ u

u
σ(cid:48)(d)

(k)
r
(k)
(k)
r −l
r

(k)
r
(k)
(k)
r −l
r

)

(k)
r )

( σ(d)−σ(l
d−l
U,r = σ(d) − α(k)

(k)
r

− σ(cid:48)(d) = 0, d ≥ 0 ) (cid:5)

U,rd, to avoid numerical

Table 3: Linear lower bound parameters of various activation functions: h(k)

L,r(y) = α(k)

Lower bound h(k)
L,r
for activation function
ReLU

r ∈ S +
k

α(k)
L,r
1

β(k)
L,r
0

r ∈ S −
k
β(k)
L,r
0

α(k)
L,r
0

α(k)
L,r
a

L,r(y + β(k)
L,r)
r ∈ S ±
k
β(k)
L,r
0

(k)
r
(k)
(k)
r −l
r
− u(k)
r

u
σ(u(k)
r )
α(k)
L,r

(0 ≤ a ≤ 1, e.g. a = u

, 0, 1)

Sigmoid, tanh

(denoted as σ(y))

r )−σ(l(k)
σ(u(k)
r )
r −l(k)
u(k)

r

σ(l(k)
r )
α(k)
L,r

− l(k)
r

σ(cid:48)(d)

σ(d)
α(k)
L,r

− d

σ(cid:48)(d)

† Alternatively, if the solution d ≤ l(k)

, then we can set α(k)

r

r ≤ d ≤ u(k)
r )

(l(k)
L,r = σ(u(k)
r )−σ(l(k)
r )
r −l(k)
u(k)

r

.

(k)
r )

( σ(d)−σ(u
d−u

(k)
r

− σ(cid:48)(d) = 0, d ≤ 0 ) †

Certiﬁed lower bound of minimum distortion. Given an input example x0 and an m-layer NN,
let c be the predicted class of x0 and t (cid:54)= c be the targeted attack class. We aim to use the uniform
bounds established in Corollary 3.3 to obtain the largest possible lower bound ˜(cid:15)t and ˜(cid:15) of targeted
and untargeted attacks respectively, which can be formulated as follows:

˜(cid:15)t = max

(cid:15) s.t. γL

c ((cid:15)) − γU

(cid:15)

t ((cid:15)) > 0 and ˜(cid:15) = min
t(cid:54)=c

˜(cid:15)t.

We note that although there is a linear (cid:15) term in (2), other terms such as Λ(k), ∆(k) and Ω(k), Θ(k)
also implicitly depend on (cid:15). This is because the parameters α(k)
, u(k)
,
i
which may vary with (cid:15); thus the values in Λ(k), ∆(k), Ω(k), Θ(k) depend on (cid:15). It is therefore difﬁcult
to obtain an explicit expression of γL
t ((cid:15)) in terms of (cid:15). Fortunately, we can still perform
a binary search to obtain ˜(cid:15)t with Corollary 3.3. More precisely, we ﬁrst initialize (cid:15) at some ﬁxed
positive value and apply Corollary 3.3 repeatedly to obtain l(k)
from k = 1 to m and
r
r ∈ [nk]. We then check if the condition γL
t > 0 is satisﬁed. If so, we increase (cid:15); otherwise,
we decrease (cid:15); and we repeat the procedure until a given tolerance level is met.2

L,i depend on l(k)

c ((cid:15)) − γU

U,i , α(k)

U,i, β(k)

L,i, β(k)

and u(k)

c − γU

r

i

Time Complexity. With Corollary 3.3, we can compute analytic output bounds efﬁciently without
resorting to any optimization solvers for general (cid:96)p distortion, and the time complexity for an m-layer
ReLU network is polynomial time in contrast to Reluplex or Mixed-Integer Optimization-based
approach [22, 23] where SMT and MIO solvers are exponential-time. For an m layer network with n
neurons per layer and n outputs, time complexity of CROWN is O(m2n3). Forming Λ(0) and Ω(0)
for the m-th layer involves multiplications of layer weights in a similar cost of forward propagation in
O(mn3) time. Also, the bounds for all previous k ∈ [m − 1] layers need to be computed beforehand
in O(kn3) time; thus the total time complexity is O(m2n3).

3.2 Case studies: CROWN for ReLU, tanh, sigmoid and arctan activations

In Section 3.1 we showed that as long as one can identify two linear functions hU (y), hL(y) to bound
a general activation function σ(y) for each neuron, we can use Corollary 3.3 with a binary search

2The bound can be further improved by considering g(x) := fc(x) − ft(x) and replacing the last layer’s

weights by W(m)

c,: − W(m)
t,:

. This is also used by [20].

5

(a) r ∈ S +
k

(b) r ∈ S −
k

(c) r ∈ S ±
k

Figure 1: σ(y) = tanh. Green lines are the upper bounds h(k)

U,r; red lines are the lower bounds h(k)
L,r.

to obtain certiﬁed lower bounds of minimum distortion. In this section, we illustrate how to ﬁnd
parameters α(k)
L,r of hU (y), hL(y) for four most widely used activation functions:
ReLU, tanh, sigmoid and arctan. Other activations, including but not limited to leaky ReLU, ELU
and softplus, can be easily incorporated into our CROWN framework following a similar procedure.

L,r and β(k)

U,r, α(k)

U,r, β(k)

r

r

k , S ±

k , S −

and u(k)

k is S +

r }, and S −

r < 0 < u(k)

k = {r ∈ [nk] | 0 ≤ l(k)

k , S ±
k and S −
k = {r ∈ [nk] | l(k)

, we deﬁne a partition
k } of set [nk] such that every neuron in k-th layer belongs to exactly one of the three
r ≤ u(k)
k = {r ∈
r ≤ 0}. For neurons in each partitioned
L,r in terms of l(k)
. As
and u(k)

Segmenting activation functions. Based on the signs of l(k)
{S +
sets. The formal deﬁnition of S +
[nk] | l(k)
and u(k)
set, we deﬁne corresponding upper bound h(k)
U,r and lower bound h(k)
we will see shortly, segmenting the activation functions based on l(k)
is useful to bound a
given activation function. We note there are multiple ways of segmenting the activation functions and
deﬁning the partitioned sets (e.g. based on the values of l(k)
rather than their signs), and we can
easily incorporate this into our framework to provide the corresponding explicit output bounds for
the new partition sets. In the case study, we consider S +
k for the four activations, as this
partition reﬂects the curvature of tanh, sigmoid and arctan functions and activation states of ReLU.

k and S −

r ≤ u(k)

r }, S ±

k , S ±

, u(k)
r

r

r

r

r

r

r

, σ(l(k)

r )) as left end-point and (u(k)

Bounding tanh/sigmoid/arctan. For tanh activation, σ(y) = 1−e−2y
1+e−2y ; for sigmoid activation,
σ(y) = 1
1+e−y ; for arctan activation, σ(y) = arctan(y). All functions are convex on one side
U,r and h(k)
(y < 0) and concave on the other side (y > 0), thus the same rules can be used to ﬁnd h(k)
L,r.
Below we call (l(k)
r )) as right end-point. For r ∈ S +
k ,
since σ(y) is concave, we can let h(k)
], and let
h(k)
L,r pass the two end-points. Similarly, σ(y) is concave for r ∈ S +
L,r be any
tangent line of σ(y) at point d ∈ [l(k)
U,r pass the two end-points. Lastly, for r ∈ S ±
k ,
we can let h(k)
U,r be the tangent line that passes the left end-point and (d, σ(d)) where d ≥ 0 and h(k)
U,r
be the tangent line that passes the right end-point and (d, σ(d)) where d ≤ 0. The value of d for
transcendental functions can be found using a binary search. The plots of upper and lower bounds for
tanh and sigmoid are in Figure 1 and 3 (in Appendix). Plots for arctan are similar and so omitted.

, σ(u(k)
U,r be any tangent line of σ(y) at point d ∈ [l(k)

, u(k)
r
k , thus we can let h(k)

] and let h(k)

, u(k)
r

r

r

r

U,r = h(k)
k , we can set h(k)

L,r = y; if r ∈ S −
U,r = u(k)

k , we have σ(y) = 0, and thus we can set h(k)
(y − l(k)

Bounding ReLU. For ReLU activation, σ(y) = max(0, y). If r ∈ S +
we can set h(k)
if r ∈ S ±
r
r −l(k)
leads to the linear lower bound used in Fast-Lin [20]. Thus, Fast-Lin is a special case under our
framework. We propose to adaptively choose a, where we set a = 1 when u(k)
| and a = 0
r
when u(k)
L,r = ay and σ(y) (which
reﬂects the gap between the lower bound and the ReLU function) is always minimized. As shown in
our experiments, the adaptive selection of h(k)
helps to achieve a
tighter certiﬁed lower bound. Figure 4 (in Appendix) illustrates the idea discussed here.

k , we have σ(y) = y and so
L,r = 0;

|. In this way, the area between the lower bound h(k)

L,r = ay, 0 ≤ a ≤ 1. Setting a = u(k)

L,r based on the value of u(k)

r ) and h(k)

U,r = h(k)

r < |l(k)

r ≥ |l(k)

r
r −l(k)

and l(k)
r

u(k)

u(k)

r

r

r

r

6

Summary. We summarized the above analysis on choosing valid linear functions h(k)
L,r in
Table 2 and 3. In general, as long as h(k)
U,r and h(k)
L,r are identiﬁed for the activation functions, we can
use Corollary 3.3 to compute certiﬁed lower bounds for general activation functions. Note that there
remain many other choices of h(k)
L,r as valid upper/lower bounds of σ(y), but ideally, we
would like them to be close to σ(y) in order to achieve a tighter lower bound of minimum distortion.

U,r and h(k)

U,r and h(k)

3.3 Extension to quadratic bounds

In addition to the linear bounds on activation functions, the proposed CROWN framework can
U,r and h(k)
also incorporate quadratic bounds by adding a quadratic term to h(k)
U,ry2 +
α(k)
L,ry2 + α(k)
U,r(y + β(k)
U,r, η(k)
L,r ∈ R. Following the procedure
of unwrapping the activation functions at the layer m − 1, we show in Appendix D that the output
upper bound and lower bound with quadratic approximations are:

L,r), where η(k)

L,r(y) = η(k)

U,r(y) = η(k)

L,r(y + β(k)

U,r), h(k)

L,r: h(k)

j (x) = Φm−2(x)(cid:62)Q(m−1)
f U
j (x) = Φm−2(x)(cid:62)Q(m−1)
f L

U

L

Φm−2(x) + 2p(m−1)
Φm−2(x) + 2p(m−1)

U

Φm−2(x) + s(m−1)
Φm−2(x) + s(m−1)

U

,

,

L

L

(3)

(4)

U

L

U,i

L,i

, if W(m)

j,i η(m−1)

j,i η(m−1)

j,i ≥ 0 or W(m)

= W(m−1)(cid:62)D(m−1)

U W(m−1), Q(m−1)

= W(m−1)(cid:62)D(m−1)
, and s(m−1)
L

L W(m−1), p(m−1)
where Q(m−1)
,
U
p(m−1)
, s(m−1)
are deﬁned in Appendix D due to page limit. When m = 2, Φm−2(x) =
U
L
x and we can directly optimize over x ∈ Bp(x0, (cid:15)); otherwise, we can use the post activation
bounds of layer m − 2 as the constraints. D(m−1)
in (3) is a diagonal matrix with i-th entry being
U
W(m)
j,i < 0. Thus, in general Q(m−1)
, if W(m)
is indeﬁnite,
resulting in a non-convex optimization when ﬁnding the global bounds as in Corollary 3.3. Fortunately,
by properly choosing the quadratic bounds, we can make the problem maxx∈Bp(x0,(cid:15)) f U
j (x) into a
convex Quadratic Programming problem; for example, we can let η(m−1)
j,i > 0
and let η(m−1)
have only negative and zero diagonals for the maximization
problem – this is equivalent to applying a linear upper bound and a quadratic lower bound to bound the
, we let η(m−1)
activation function. Similarly, for D(m−1)
> 0
to make D(m−1)
j (x) is convex.
We can solve this convex program with projected gradient descent (PGD) for x ∈ Bp(x0, (cid:15)) and
Armijo line search. Empirically, we ﬁnd that PGD usually converges within a few iterations.

U,i
have non-negative diagonals and hence the problem minx∈Bp(x0,(cid:15)) f L

j,i < 0 and let η(m−1)

> 0 to make D(m−1)

= 0 for all W(m)

= 0 for all W(m)

L,i

L,i

U,i

L

L

U

U

4 Experiments

Methods. For ReLU networks, CROWN-Ada is CROWN with adaptive linear bounds (Sec. 3.2),
CROWN-Quad is CROWN with quadratic bounds (Sec. 3.3). Fast-Lin and Fast-Lip are state-of-the-art
fast certiﬁed lower bound proposed in [20]. Reluplex can solve the exact minimum adversarial
distortion but is only computationally feasible for very small networks. LP-Full is based on the LP
formulation in [18] and we solve LPs for each neuron exactly to achieve the best possible bound.
For networks with other activation functions, CROWN-general is our proposed method.

Model and Dataset. We evaluate CROWN and other baselines on multi-layer perceptron (MLP)
models trained on MNIST and CIFAR-10 datasets. We denote a feed-forward network with m layers
and n neurons per layer as m × [n]. For models with ReLU activation, we use pretrained models
provided by [20] and also evaluate the same set of 100 random test images and random attack targets
as in [20] (according to their released code) to make our results comparable. For training NN models
with other activation functions, we search for best learning rate and weight decay parameters to
achieve a similar level of accuracy as ReLU models.

Implementation and Setup. We implement our algorithm using Python (numpy with numba). Most
computations in our method are matrix operations that can be automatically parallelized by the BLAS
library; however, we set the number of BLAS threads to 1 for a fair comparison to other methods. Ex-
periments were conducted on an Intel Skylake server CPU running at 2.0 GHz on Google Cloud. Our
code is available at https://github.com/huanzhang12/CROWN-Robustness-Certification

7

(a) MNIST 2 × [20], (cid:96)2

(b) MNIST 2 × [20], (cid:96)∞ (c) MNIST 3 × [20], (cid:96)2

(d) MNIST 3 × [20], (cid:96)∞

Figure 2: Certiﬁed lower bounds and minimum distortion comparisons for (cid:96)2 and (cid:96)∞ distortions. Left
y-axis is distortion and right y-axis (black line) is computation time (seconds, logarithmic scale). On
the top of ﬁgures are the avg. CLEVER score and the upper bound found by C&W attack [6]. From
left to right in (a)-(d): CROWN-Ada, (CROWN-Quad), Fast-Lin, Fast-Lip, LP-Full and (Reluplex).

Table 4: Comparison of certiﬁed lower bounds on large ReLU networks. Bounds are the average over
100 images (skipped misclassiﬁed images) with random attack targets. Percentage improvements are
calculated against Fast-Lin as Fast-Lip is worse than Fast-Lin.
Certiﬁed Bounds

Improvement (%)

Network

MNIST
4 × [1024]

CIFAR-10
7 × [1024]

(cid:96)p norm Fast-Lin
1.57649
0.18891
0.00823
0.86468
0.05937
0.00134

(cid:96)1
(cid:96)2
(cid:96)∞
(cid:96)1
(cid:96)2
(cid:96)∞

Fast-Lip CROWN-Ada CROWN-Ada vs Fast-Lin
0.72800
0.06487
0.00264
0.09239
0.00407
0.00008

1.88217
0.22811
0.00997
1.09067
0.07496
0.00169

+19%
+21%
+21%
+26%
+26%
+26%

Average Computation Time (sec)
Fast-Lin
Fast-Lip CROWN-Ada
1.80
1.78
1.53
13.21
12.57
8.98

3.54
3.79
3.57
22.43
21.82
16.66

2.04
1.96
2.17
19.76
18.71
20.34

Table 5: Comparison of certiﬁed lower bounds by CROWN-Ada on ReLU networks and CROWN-
general on networks with tanh, sigmoid and arctan activations. CIFAR models with sigmoid activa-
tions achieve much worse accuracy than other networks and are thus excluded.

Network

MNIST
3 × [1024]

CIFAR-10
6 × [2048]

(cid:96)p norm
(cid:96)1
(cid:96)2
(cid:96)∞
(cid:96)1
(cid:96)2
(cid:96)∞

Certiﬁed Bounds by CROWN-Ada and CROWN-general
ReLU
3.00231
0.50841
0.02576
0.91201
0.05245
0.00114

arctan
2.33246
0.30345
0.01363
0.46198
0.02515
0.00055

sigmoid
2.94239
0.44471
0.02122
-
-
-

tanh
2.48407
0.27287
0.01182
0.44059
0.02538
0.00055

Average Computation Time (sec)
arctan
ReLU
1.70
1.25
1.75
1.26
1.77
1.37
83.80
71.62
83.12
71.51
58.04
49.28

sigmoid
1.68
1.61
1.76
-
-
-

tanh
1.61
1.76
1.78
89.77
84.22
59.72

Results on Small Networks. Figure 2 shows the certiﬁed lower bound for (cid:96)2 and (cid:96)∞ distortions
found by different algorithms on small networks, where Reluplex is feasible and we can observe the
gap between different certiﬁed lower bounds and the true minimum adversarial distortion. Reluplex
and LP-Full are orders of magnitudes slower than other methods (note the logarithmic scale on
right y-axis), and CROWN-Quad (for 2-layer) and CROWN-Ada achieve the largest lower bounds.
Improvements of CROWN-Ada over Fast-Lin are more signiﬁcant in larger NNs, as we show below.

Results on Large ReLU Networks. Table 4 demonstrates the lower bounds found by different
algorithms for all common (cid:96)p norms. CROWN-Ada signiﬁcantly outperforms Fast-Lin and Fast-Lip,
while the computation time increased by less than 2X over Fast-Lin, and is comparable with Fast-Lip.
See Appendix for results on more networks.

Results on Different Activations. Table 7 compares the certiﬁed lower bound computed by CROWN-
general for four activation functions and different (cid:96)p norm on large networks. CROWN-general is
able to certify non-trivial lower bounds for all four activation functions efﬁciently. Comparing to
CROWN-Ada on ReLU networks, certifying general activations that are not piece-wise linear only
incurs about 20% additional computational overhead.

5 Conclusion

We have presented a general framework CROWN to efﬁciently compute a certiﬁed lower bound
of minimum distortion in neural networks for any given data point x0. CROWN features adaptive
bounds for improved robustness certiﬁcation and applies to general activation functions. Moreover,
experiments show that (1) CROWN outperforms state-of-the-art baselines on ReLU networks and (2)
CROWN can efﬁciently certify non-trivial lower bounds for large networks with over 10K neurons
and with different activation functions.

8

This work was supported in part by NSF IIS-1719097, Intel faculty award, Google Cloud Credits for
Research Program and GPUs donated by NVIDIA. Tsui-Wei Weng and Luca Daniel are partially
supported by MIT-IBM Watson AI Lab and MIT-Skoltech program.

Acknowledgement

References

[1] A. Fawzi, S.-M. Moosavi-Dezfooli, and P. Frossard, “The robustness of deep networks: A
geometrical perspective,” IEEE Signal Processing Magazine, vol. 34, no. 6, pp. 50–62, 2017.

[2] B. Biggio and F. Roli, “Wild patterns: Ten years after the rise of adversarial machine learning,”

arXiv preprint arXiv:1712.03141, 2017.

[3] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus,

“Intriguing properties of neural networks,” arXiv preprint arXiv:1312.6199, 2013.

[4] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing adversarial examples,”

ICLR, 2015.

[5] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, “Deepfool: a simple and accurate method
to fool deep neural networks,” in IEEE Conference on Computer Vision and Pattern Recognition,
2016, pp. 2574–2582.

[6] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural networks,” in IEEE

Symposium on Security and Privacy (SP), 2017, pp. 39–57.

[7] H. Chen, H. Zhang, P.-Y. Chen, J. Yi, and C.-J. Hsieh, “Show-and-fool: Crafting adversarial

examples for neural image captioning,” arXiv preprint arXiv:1712.02051, 2017.

[8] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and A. Swami, “Practical black-box
attacks against machine learning,” in ACM Asia Conference on Computer and Communications
Security, 2017, pp. 506–519.

[9] Y. Liu, X. Chen, C. Liu, and D. Song, “Delving into transferable adversarial examples and

black-box attacks,” ICLR, 2017.

[10] P.-Y. Chen, H. Zhang, Y. Sharma, J. Yi, and C.-J. Hsieh, “ZOO: Zeroth order optimization
based black-box attacks to deep neural networks without training substitute models,” in ACM
Workshop on Artiﬁcial Intelligence and Security, 2017, pp. 15–26.

[11] W. Brendel, J. Rauber, and M. Bethge, “Decision-based adversarial attacks: Reliable attacks

against black-box machine learning models,” ICLR, 2018.

[12] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial examples in the physical world,” arXiv

preprint arXiv:1607.02533, 2016.

[13] I. Evtimov, K. Eykholt, E. Fernandes, T. Kohno, B. Li, A. Prakash, A. Rahmati, and D. Song,
“Robust physical-world attacks on machine learning models,” arXiv preprint arXiv:1707.08945,
2017.

[14] A. Athalye and I. Sutskever, “Synthesizing robust adversarial examples,” arXiv preprint

arXiv:1707.07397, 2017.

[15] G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer, “Reluplex: An efﬁcient smt
solver for verifying deep neural networks,” in International Conference on Computer Aided
Veriﬁcation. Springer, 2017, pp. 97–117.

[16] A. Sinha, H. Namkoong, and J. Duchi, “Certiﬁable distributional robustness with principled

[17] J. Peck, J. Roels, B. Goossens, and Y. Saeys, “Lower bounds on the robustness to adversarial

[18] J. Z. Kolter and E. Wong, “Provable defenses against adversarial examples via the convex outer

[19] A. Raghunathan, J. Steinhardt, and P. Liang, “Certiﬁed defenses against adversarial examples,”

adversarial training,” ICLR, 2018.

perturbations,” in NIPS, 2017.

adversarial polytope,” ICML, 2018.

ICLR, 2018.

9

[20] T.-W. Weng, H. Zhang, H. Chen, Z. Song, C.-J. Hsieh, D. Boning, I. S. Dhillon, and L. Daniel,

“Towards fast computation of certiﬁed robustness for relu networks,” ICML, 2018.

[21] A. Lomuscio and L. Maganti, “An approach to reachability analysis for feed-forward relu neural

networks,” arXiv preprint arXiv:1706.07351, 2017.

[22] C.-H. Cheng, G. Nührenberg, and H. Ruess, “Maximum resilience of artiﬁcial neural networks,”
in International Symposium on Automated Technology for Veriﬁcation and Analysis. Springer,
2017, pp. 251–268.

[23] M. Fischetti and J. Jo, “Deep neural networks as 0-1 mixed integer linear programs: A feasibility

study,” arXiv preprint arXiv:1712.06174, 2017.

[24] N. Carlini, G. Katz, C. Barrett, and D. L. Dill, “Provably minimally-distorted adversarial

examples,” arXiv preprint arXiv:1709.10207, 2017.

[25] R. Ehlers, “Formal veriﬁcation of piece-wise linear feed-forward neural networks,” in Interna-
tional Symposium on Automated Technology for Veriﬁcation and Analysis. Springer, 2017, pp.
269–286.

[26] M. Hein and M. Andriushchenko, “Formal guarantees on the robustness of a classiﬁer against

adversarial manipulation,” in NIPS, 2017.

[27] T.-W. Weng, H. Zhang, P.-Y. Chen, J. Yi, D. Su, Y. Gao, C.-J. Hsieh, and L. Daniel, “Evaluating

the robustness of neural networks: An extreme value theory approach,” ICLR, 2018.

[28] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, and M. Vechev, “Ai2:
Safety and robustness certiﬁcation of neural networks with abstract interpretation,” in IEEE
Symposium on Security and Privacy (SP), vol. 00, 2018, pp. 948–963.

[29] K. Dvijotham, R. Stanforth, S. Gowal, T. Mann, and P. Kohli, “A dual approach to scalable

veriﬁcation of deep networks,” UAI, 2018.

[30] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards deep learning models

resistant to adversarial attacks,” ICLR, 2018.

[31] X. Cao and N. Z. Gong, “Mitigating evasion attacks to deep neural networks via region-based
classiﬁcation,” in ACM Annual Computer Security Applications Conference, 2017, pp. 278–287.
[32] P.-Y. Chen, Y. Sharma, H. Zhang, J. Yi, and C.-J. Hsieh, “EAD: elastic-net attacks to deep

neural networks via adversarial examples,” AAAI, 2018.

10

A Proof of Theorem 3.2

Given an m-layer neural network function f : Rn0 → Rnm with pre-activation bounds l(k) and u(k)
for x ∈ Bp(x0, (cid:15)) and ∀k ∈ [m − 1], let the pre-activation inputs for the i-th neuron at layer m − 1
be y(m−1)
. The j-th output of the neural network is the following:
i

:= W(m−1)
i,:

i

Φm−2(x) + b(m−1)
nm−1
(cid:88)

fj(x) =

W(m)

j,i [Φm−1(x)]i + b(m)

j

,

i=1
nm−1
(cid:88)

i=1
(cid:88)

=

=

W(m)

j,i σ(y(m−1)

i

) + b(m)
j

,

W(m)

j,i σ(y(m−1)

i

)

+

(cid:88)

W(m)

j,i σ(y(m−1)

i

)

+b(m)
j

.

j,i ≥0

W(m)
(cid:124)

(cid:123)(cid:122)
F1

j,i <0

W(m)
(cid:124)

(cid:125)

(cid:123)(cid:122)
F2

(cid:125)

Assume the activation function σ(y) is bounded by two linear functions h(m−1)
tion 3.1, we have

U,i

, h(m−1)
L,i

in Deﬁni-

h(m−1)
L,i

(y(m−1)
i

) ≤ σ(y(m−1)
i

) ≤ h(m−1)
U,i

(y(m−1)
i

).

to the i-th neuron is non-negative (the terms in F1 bracket), we

) ≤ W(m)

j,i σ(y(m−1)

i

) ≤ W(m)
j,i

· h(m−1)
U,i

(y(m−1)
i

);

) ≤ W(m)

j,i σ(y(m−1)

i

) ≤ W(m)
j,i

· h(m−1)
L,i

(y(m−1)
i

).

(x) be an upper bound of fj(x). To compute f U,m−1

(x), (6), (7) and
j,i ≥ 0 terms in (6), the upper bound is the right-
j,i < 0 terms in (6), the upper bound is the RHS in (8). Thus,

j

Thus, if the associated weight W(m)
j,i
have

W(m)
j,i

· h(m−1)
L,i
otherwise (the terms in F2 bracket), we have
· h(m−1)
U,i

(y(m−1)
i

(y(m−1)
i

W(m)
j,i

j

Upper bound. Let f U,m−1
(8) are the key equations. Precisely, for the W(m)
hand-side (RHS) in (7); and for the W(m)
we obtain:
f U,m−1
j
(cid:88)

(cid:88)

(x)

W(m)
j,i

· h(m−1)
U,i

(y(m−1)
i

) +

=

W(m)
j,i

· h(m−1)
L,i

(y(m−1)
i

) + b(m)
j

,

(9)

W(m)

j,i α(m−1)

U,i

(y(m−1)
i

+ β(m−1)
U,i

) +

W(m)

j,i α(m−1)

L,i

(y(m−1)
i

+ β(m−1)
L,i

) + b(m)
j

,

W(m)

j,i <0

(cid:88)

W(m)

j,i <0

W(m)

j,i λ(m−1)

j,i

(y(m−1)
i

+ ∆(m−1)
i,j

) + b(m)
j

,

Λ(m−1)
j,i

W(m−1)
i,r

[Φm−2(x)]r + b(m−1)

i

+ ∆(m−1)
i,j

) + b(m)
j

,

W(m−1)
i,r

[Φm−2(x)]r) +

Λ(m−1)
j,i

(b(m−1)
i

+ ∆(m−1)
i,j

) + b(m)
j

, (13)

(cid:33)

Λ(m−1)

j,i W(m−1)

i,r

[Φm−2(x)]r +

Λ(m−1)
j,i

(b(m−1)
i

+ ∆(m−1)
i,j

) + b(m)
j

,

nm−1
(cid:88)

i=1

(cid:32)nm−1
(cid:88)

i=1

W(m)
j,i ≥0
(cid:88)

W(m)

j,i ≥0

=

=

=

=

=

nm−1
(cid:88)

i=1
nm−1
(cid:88)

i=1
nm−1
(cid:88)

i=1
nm−2
(cid:88)

=

nm−2
(cid:88)

r=1

Λ(m−1)
j,i

(cid:32)nm−1
(cid:88)

r=1

i=1

nm−2
(cid:88)
(

r=1
nm−2
(cid:88)
(

r=1

(5)

(6)

(7)

(8)

(10)

(11)

(12)

(cid:33)

(14)

(15)

˜W(m−1)
j,r

[Φm−2(x)]r + ˜b(m−1)

.

j

11

From (9) to (10), we replace h(m−1)
(11), we use variables λ(m−1)
in the parentheses:

U,i
and ∆(m−1)
j,i

(y(m−1)
i

j,i

(y(m−1)
) and h(m−1)
i
to denote the slopes in front of y(m−1)

L,i

i

) by their deﬁnitions; from (10) to

and the intercepts

λ(m−1)
j,i

=

(cid:40)

α(m−1)
U,i
α(m−1)
L,i

(cid:40)

if W(m)
if W(m)

j,i ≥ 0
j,i < 0

( ⇐⇒ Λ(m)
( ⇐⇒ Λ(m)

j,: W(m)
j,: W(m)

:,i ≥ 0);
:,i < 0);

(16)

=

∆(m−1)
i,j

j,i ≥ 0
j,i < 0

( ⇐⇒ Λ(m)
( ⇐⇒ Λ(m)

if W(m)
j,: W(m)
j,: W(m)
if W(m)
with its deﬁnition and let Λ(m−1)

β(m−1)
U,i
β(m−1)
L,i
From (11) to (12), we replace y(m−1)
. We
further let Λ(m)
j (the standard unit vector with the only non-zero jth element equal to 1), and
thus we can rewrite the conditions of W(m)
. From (12) to (13), we
j,i
collect the constant terms that are not related to x. From (13) to (14), we swap the summation order
of i and r, and the coefﬁcients in front of [Φm−2(x)]r can be combined into a new equivalent weight
˜W(m−1)
j,r

and the constant term can combined into a new equivalent bias ˜b(m−1)

in (16) and (17) as Λ(m)

:,i ≥ 0);
:,i < 0).

j,i λ(m−1)

j,: W(m)

j,: = e(cid:62)

:= W(m)

in (15):

(17)

j,i

j,i

:,i

j

i

˜W(m−1)
j,r

=

Λ(m−1)

j,i W(m−1)

i,r

= Λ(m−1)

j,: W(m−1)

:,r

,

nm−1
(cid:88)

i=1
nm−1
(cid:88)

i=1

˜b(m−1)

j

=

Λ(m−1)
j,i

(b(m−1)
i

+ ∆(m−1)
i,j

) + b(m)

j = Λ(m−1)

j,:

(b(m−1) + ∆(m−1)

:,j

) + b(m)
j

.

Notice that after deﬁning the new equivalent weight ˜W(m−1)
(x)
in (15) and fj(x) in (5) are in the same form. Thus, we can repeat the above procedure again to
obtain an upper bound of f U,m−1

and equivalent bias ˜b(m−1)

(x), i.e. f U,m−2

, f U,m−1

(x):

j,r

j

j

λ(m−2)
j,i

(b(m−2) + ∆(m−2)

j
Λ(m−2)
j,i

˜W(m−2)
j,r
˜b(m−2)
(cid:40)

j
α(m−2)
U,i
α(m−2)
L,i
(cid:40)
β(m−2)
U,i
β(m−2)
L,i

λ(m−2)
j,i
j,: W(m−1)
j,: W(m−2)

:,r

:,i

j
= ˜W(m−1)
j,i
= Λ(m−1)
= Λ(m−2)
= Λ(m−2)
j,:
if ˜W(m−1)
j,i
if ˜W(m−1)
j,i
if ˜W(m−1)
j,i
if ˜W(m−1)
j,i

≥ 0
< 0

≥ 0
< 0

λ(m−2)
j,i

=

∆(m−2)
i,j

=

:,j

) + ˜b(m−1)
j
( ⇐⇒ Λ(m−1)
( ⇐⇒ Λ(m−1)
( ⇐⇒ Λ(m−1)
( ⇐⇒ Λ(m−1)

:,i

:,i

≥ 0);
< 0);

j,: W(m−1)
j,: W(m−1)
j,: W(m−1)
j,: W(m−1)
(x), where fj(x) ≤ f U,m−1
j
(x), and we have

≥ 0);
< 0).

:,i

:,i

(x) ≤

and repeat again iteratively until obtain the ﬁnal upper bound f U,1
f U,m−2
j

(x). We let fj(x) denote the ﬁnal upper bound f U,1

(x) ≤ . . . ≤ f U,1

j

j

j

j (x) = Λ(0)
f U

j,: x +

Λ(k)

j,: (b(k) + ∆(k)
:,j )

and ((cid:12) is the Hadamard product)

and ∀i ∈ [nk],

Λ(k−1)
j,:

=

(cid:40)

e(cid:62)
j
(Λ(k)

j,: W(k)) (cid:12) λ(k−1)

j,:

if k = m + 1;
if k ∈ [m].

λ(k)
j,i =






α(k)
U,i
α(k)
L,i
1

if k ∈ [m − 1], Λ(k+1)
if k ∈ [m − 1], Λ(k+1)
if k = 0.

j,: W(k+1)
j,: W(k+1)

:,i

:,i

≥ 0;
< 0;

m
(cid:88)

k=1

12

∆(k)

i,j =




β(k)
U,i
β(k)
L,i

0

if k ∈ [m − 1], Λ(k+1)
if k ∈ [m − 1], Λ(k+1)
if k = m.

j,: W(k+1)
j,: W(k+1)

:,i

:,i

≥ 0;
< 0;

Lower bound. The above derivations of upper bound can be applied similarly to deriving lower
bounds of fj(x), and the only difference is now we need to use the LHS of (7) and (8) (rather
than RHS when deriving upper bound) to bound the two terms in (6). Thus, following the same
procedure in deriving the upper bounds, we can iteratively unwrap the activation functions and obtain
a ﬁnal lower bound f L,1
(x). Let
j (x) = f L,1
f L

(x), where fj(x) ≥ f L,m−1

(x) ≥ . . . ≥ f L,1

(x) ≥ f L,m−2

(x), we have:

j

j

j

j

j

and ∀i ∈ [nk],

j (x) = Ω(0)
f L

j,: x +

Ω(k)

j,: (b(k) + Θ(k)
:,j )

m
(cid:88)

k=1

Ω(k−1)
j,:

=

(cid:40)

e(cid:62)
j
(Ω(k)

j,: W(k)) (cid:12) ω(k−1)

j,:

if k = m + 1;
if k ∈ [m].

ω(k)
j,i =

Θ(k)

i,j =




α(k)
L,i
α(k)
U,i

1

β(k)

L,i
β(k)
U,i
0



if k ∈ [m − 1], Ω(k+1)
if k ∈ [m − 1], Ω(k+1)
if k = 0.
if k ∈ [m − 1], Ω(k+1)
if k ∈ [m − 1], Ω(k+1)
if k = m.

j,: W(k+1)
j,: W(k+1)

:,i

:,i

≥ 0;
< 0;

j,: W(k+1)
j,: W(k+1)

:,i

:,i

≥ 0;
< 0;

j,i and ω(k)

j,i only differs in the conditions of selecting α(k)

U,i or α(k)

L,i; similarly for ∆(k)

i,j and

Indeed, λ(k)
Θ(k)
i,j .

B Proof of Corollary 3.3

Deﬁnition B.1 (Dual norm). Let (cid:107) · (cid:107) be a norm on Rn. The associated dual norm, denoted as (cid:107) · (cid:107)∗,
is deﬁned as

(cid:107)a(cid:107)∗ = {sup
y

a(cid:62)y | (cid:107)y(cid:107) ≤ 1}.

Global upper bound. Our goal is to ﬁnd a global upper and lower bound for the m-th layer network
output fj(x), ∀x ∈ Bp(x0, (cid:15)). By Theorem 3.2, for x ∈ Bp(x0, (cid:15)), we have f L
j (x) ≤ fj(x) ≤
j (x) and f U
f U
j (x),
and we have

:= maxx∈Bp(x0,(cid:15)) f U

:,j ). Thus deﬁne γU
j

j,: (b(k) + ∆(k)

j (x) = Λ(0)

j,: x + (cid:80)m

k=1 Λ(k)

fj(x) ≤ f U

j (x) ≤ max

j (x) = γU
f U
j ,

x∈Bp(x0,(cid:15))

since ∀x ∈ Bp(x0, (cid:15)). In particular,

max
x∈Bp(x0,(cid:15))

f U
j (x) = max

x∈Bp(x0,(cid:15))

(cid:34)
Λ(0)

j,: x +

m
(cid:88)

(cid:35)
j,: (b(k) + ∆(k)
:,j )

Λ(k)

(cid:20)

=

max
x∈Bp(x0,(cid:15))

(cid:20)

= (cid:15)

max
y∈Bp(0,1)

Λ(0)

j,: x

+

Λ(k)

j,: (b(k) + ∆(k)
:,j )

Λ(0)

j,: y

+ Λ(0)

j,: x0 +

Λ(k)

j,: (b(k) + ∆(k)
:,j )

m
(cid:88)

k=1

= (cid:15)(cid:107)Λ(0)

j,: (cid:107)q + Λ(0)

j,: x0 +

Λ(k)

j,: (b(k) + ∆(k)
:,j ).

(18)

(19)

(20)

k=1
m
(cid:88)

k=1

(cid:21)

(cid:21)

m
(cid:88)

k=1

13

From (18) to (19), let y := x−x0
and use the fact that (cid:96)q norm is dual of (cid:96)p norm for p, q ∈ [1, ∞].

(cid:15)

, and thus y ∈ Bp(0, 1). From (19) to (20), apply Deﬁnition B.1

Global lower bound. Similarly, let γL

j := minx∈Bp(x0,(cid:15)) f L

j (x), we have

fj(x) ≥ f L

j (x) ≥ min

j (x) = γL
f L
j .

x∈Bp(x0,(cid:15))

j (x) = Ω(0)

j,: x + (cid:80)m

k=1 Ω(k)

j,: (b(k) + Θ(k)

:,j ), we can derive γL

j (similar to the derivation of

Since f L
γU
j ) below:

min
x∈Bp(x0,(cid:15))

f L
j (x) = min

x∈Bp(x0,(cid:15))

Ω(0)

j,: x +

Ω(k)

j,: (b(k) + Θ(k)
:,j )

(cid:34)

(cid:35)

(cid:20)

=

min
x∈Bp(x0,(cid:15))

(cid:21)
j,: x

Ω(0)

+

Ω(k)

j,: (b(k) + Θ(k)
:,j )

(cid:20)

= −(cid:15)

max
y∈Bp(0,1)

−Ω(0)

j,: y

+ Ω(0)

j,: x0 +

Ω(k)

j,: (b(k) + Θ(k)
:,j )

m
(cid:88)

k=1

= −(cid:15)(cid:107)Ω(0)

j,: (cid:107)q + Ω(0)

j,: x0 +

Ω(k)

j,: (b(k) + Θ(k)
:,j ).

m
(cid:88)

k=1
m
(cid:88)

k=1
(cid:21)

m
(cid:88)

k=1

Thus, we have

(global upper bound) γU

j = (cid:15)(cid:107)Λ(0)

j,: (cid:107)q + Λ(0)

j,: x0 +

Λ(k)

j,: (b(k) + ∆(k)
:,j ),

(global lower bound) γL

j = −(cid:15)(cid:107)Ω(0)

j,: (cid:107)q + Ω(0)

j,: x0 +

Ω(k)

j,: (b(k) + Θ(k)
:,j ),

m
(cid:88)

k=1

m
(cid:88)

k=1

C Illustration of linear upper and lower bounds on sigmoid activation

function.

(a) r ∈ S +
k

(b) r ∈ S −
k

(c) r ∈ S ±
k

Figure 3: The linear upper and lower bounds for σ(y) = sigmoid

D f U

j (x) and f L

j (x) by Quadratic approximation

Upper bound. Let f U
mations, we can still apply (7) and (8) except that h(k)
quadratic functions:

j (x) be an upper bound of fj(x). To compute f U

U,r(y) and h(k)

j (x) with quadratic approxi-
L,r(y) are replaced by the following

U,r(y) = η(k)
h(k)

U,ry2 + α(k)

U,r(y + β(k)

U,r), h(k)

L,r(y) = η(k)

L,ry2 + α(k)

L,r(y + β(k)

L,r).

14

(a) r ∈ S +
k

(b) r ∈ S −
k

(c) r ∈ S ±
k

Figure 4: The linear upper and lower bounds for σ(y) = ReLU. For the cases (a) and (b), the
linear upper bound and lower bound are exactly the function σ(y) in the region (grey-shaded). For
(c), we plot three out of many choices of lower bound, and they are h(k)
L,r(y) = 0 (dashed-dotted),
L,r(y) = u(k)
L,r(y) = y (dashed), and h(k)
h(k)

y (dotted).

r
r −l(k)

u(k)

r

Therefore,
f U
j (x) =

(cid:88)

W(m)

j,i ≥0

=

nm−1
(cid:88)

i=1

W(m)
j,i

· h(m−1)
U,i

(y(m−1)
i

) +

W(m)
j,i

· h(m−1)
L,i

(y(m−1)
i

) + b(m)
j

,

(21)

(cid:88)

W(m)

j,i <0

(cid:16)

W(m)
j,i

τ (m−1)
j,i

y(m−1)2
i

+ λ(m−1)
j,i

(y(m−1)
i

+ ∆(m−1)
i,j

+ b(m)
j

,

(cid:17)
)

= y(m−1)(cid:62)diag(q(m−1)
U,j
= Φm−2(x)(cid:62)Q(m−1)

)y(m−1) + Λ(m−1)
Φm−2(x) + 2p(m−1)

y(m−1) + W(m)
Φm−2(x) + s(m−1)

j,: ∆(m−1)
.

:,j

j,:

,

U

U

U

From (21) to (22), we replace h(m−1)

U,i

(y(m−1)
i
(cid:40)

) and h(m−1)

L,i
, α(m−1)
U,i
, α(m−1)
L,i

(y(m−1)
i
, β(m−1)
U,i
, β(m−1)
L,i

(τ (m−1)
j,i

, λ(m−1)
j,i

, ∆(m−1)
i,j

(η(m−1)
U,i
(η(m−1)
L,i
From (22) to (23), we let q(m−1)
j,: (cid:12) τ (m−1)
, and write in the matrix form. From (23)
j,i
to (24), we substitute y(m−1) by its deﬁnition: y(m−1) = W(m−1)Φ(m−2)(x) + b(m−1) and then
collect the quadratic terms, linear terms and constant terms of Φ(m−2)(x), where

j,i ≥ 0;
j,i < 0.

if W(m)
if W(m)

= W(m)

) =

)
)

U,j

) by their deﬁnitions and let

Q(m−1)
U
p(m−1)
U
s(m−1)
U

)W(m−1),

= W(m−1)(cid:62)diag(q(m−1)
U,j
= b(m−1)(cid:62) (cid:12) q(m−1)
= p(m−1)
U

b(m−1) + W(m)

U,j + Λ(m−1)

j,:

,
j,: ∆(m−1)

:,j

.

Lower bound. Similar to the above derivation, we can simply swap h(k)
bound f L

j (x):

U,r and h(k)

L,r and obtain lower

f L
j (x) =

(cid:88)

W(m)
j,i

· h(m−1)
U,i

(y(m−1)
i

) +

(cid:88)

W(m)
j,i

· h(m−1)
L,i

(y(m−1)
i

) + b(m)
j

,

W(m)

j,i <0

= Φm−2(x)(cid:62)Q(m−1)

L

W(m)
Φm−2(x) + 2p(m−1)

L

j,i ≥0

Φm−2(x) + s(m−1)

,

L

where

and

Q(m−1)
L
p(m−1)
U
s(m−1)
U

= W(m−1)(cid:62)diag(q(m−1)
L,j
= b(m−1)(cid:62) (cid:12) q(m−1)
= p(m−1)
U

b(m−1) + W(m)

)W(m−1), q(m−1)
L,j
p(m−1)
L
, s(m−1)
L

,
j,: ∆(m−1)

U,j + Λ(m−1)

:,j

j,:

;

j,i

= W(m)
j,: (cid:12) ν(m−1)
= b(m−1)(cid:62) (cid:12) q(m−1)
= p(m−1)
L

L,j + Ω(m−1)

j,:

;
j,: Θ(m−1)

:,j

,

b(m−1) + W(m)

(ν(m−1)
j,i

, ω(m−1)
j,i

, Θ(m−1)
i,j

) =

(cid:40)

(η(m−1)
L,i
(η(m−1)
U,i

, α(m−1)
L,i
, α(m−1)
U,i

, β(m−1)
L,i
, β(m−1)
U,i

)
)

if W(m)
if W(m)

j,i ≥ 0;
j,i < 0.

15

(22)

(23)

(24)

(25)

(26)

(27)

(28)

E Additional Experimental Results

E.1 Results on CROWN-Ada

Table 6: Comparison of our proposed certiﬁed lower bounds for ReLU with adaptive lower bounds
(CROWN-Ada), Fast-Lin and Fast-Lip and Op-nrom. LP-full and Reluplex cannot ﬁnish within a
reasonable amount of time for all the networks reported here. We also include Op-norm, where we
directly compute the operator norm (for example, for p = 2 it is the spectral norm) for each layer
and use their products as a global Lipschitz constant and then compute the robustness lower bound.
CLEVER is an estimated robustness lower bound, and attacking algorithms (including CW [6] and
EAD [32]) provide upper bounds of the minimum adversarial distortion. For each norm, we consider
the robustness against three targeted attack classes: the runner-up class (with the second largest
probability), a random class and the least likely class. It is clear that CROWN-Ada notably improves
the lower bound comparing to Fast-Lin, especially for larger and deeper networks, the improvements
can be up to 28%.

Uncertiﬁed

[27]

Attacks
CLEVER CW/EAD

Time per Image (Avg.)
Lower Bounds

[20]

Our bound

Networks

Conﬁg

p

Target

[20]

MNIST
2 × [1024]

MNIST
3 × [1024]

MNIST
4 × [1024]

CIFAR
5 × [2048]

CIFAR
6 × [2048]

CIFAR
7 × [1024]

∞

∞

∞

∞

∞

2

1

2

1

2

1

2

1

2

1

2

1

∞

runner-up
rand
least
runner-up
rand
least
runner-up
rand
least
runner-up
rand
least
runner-up
rand
least
runner-up
rand
least
runner-up
rand
least
runner-up
rand
least
runner-up
rand
least
runner-up
rand
least
runner-up
rand
least
runner-up
rand
least
runner-up
rand
least
runner-up
rand
least
runner-up
rand
least
runner-up
rand
least
runner-up
rand
least
runner-up
rand
least

Fast-Lin
0.02256
0.03083
0.03854
0.46034
0.63299
0.79263
2.78786
3.88241
4.90809
0.01830
0.02216
0.02432
0.35867
0.43892
0.48361
2.08887
2.59898
2.87560
0.00715
0.00823
0.00899
0.16338
0.18891
0.20671
1.33794
1.57649
1.73874
0.00137
0.00170
0.00188
0.06122
0.07654
0.08456
0.93836
1.18928
1.31904
0.00075
0.00090
0.00095
0.03462
0.04129
0.04387
0.59636
0.72178
0.77179
0.00119
0.00134
0.00141
0.05279
0.05937
0.06249
0.76648
0.86468
0.91127

[3]

Lower Bounds (certiﬁed)

Lower bounds and upper bounds (Avg.)
improvements
over
Fast-Lin
+9.4%
+8.8%
+9.5%
+8.9%
+8.2%
+9.0%
+8.2%
+7.6%
+8.6%
+15.5%
+16.2%
+16.6%
+15.1%
+15.8%
+16.1%
+14.6%
+15.5%
+15.9%
+20.4%
+21.1%
+21.9%
+19.9%
+20.8%
+21.5%
+18.2%
+19.4%
+20.3%
+21.9%
+24.7%
+25.5%
+22.0%
+24.5%
+25.2%
+21.3%
+23.9%
+24.7%
+25.3%
+26.7%
+28.4%
+24.6%
+27.0%
+28.0%
+23.6%
+26.4%
+27.4%
+24.4%
+26.1%
+27.0%
+24.4%
+26.3%
+27.1%
+24.2%
+26.1%
+27.0%

Our algorithm
Fast-Lip Op norm CROWN-Ada
0.01802
0.02512
0.03128
0.42027
0.59033
0.73133
3.46500
5.10000
6.36600
0.01021
0.01236
0.01384
0.22120
0.26980
0.30147
1.80150
2.25950
2.50000
0.00219
0.00264
0.00304
0.05244
0.06487
0.07440
0.58480
0.72800
0.82800
0.00020
0.00030
0.00036
0.00948
0.01417
0.01778
0.22632
0.31984
0.38887
0.00005
0.00007
0.00008
0.00228
0.00331
0.00385
0.05647
0.08212
0.09397
0.00006
0.00008
0.00010
0.00308
0.00407
0.00474
0.07028
0.09239
0.10639

0.02467
0.03353
0.04221
0.50110
0.68506
0.86377
3.01633
4.17760
5.33261
0.02114
0.02576
0.02835
0.41295
0.50841
0.56167
2.39443
3.00231
3.33231
0.00861
0.00997
0.01096
0.19594
0.22811
0.25119
1.58151
1.88217
2.09157
0.00167
0.00212
0.00236
0.07466
0.09527
0.10588
1.13799
1.47393
1.64452
0.00094
0.00114
0.00122
0.04314
0.05245
0.05615
0.73727
0.91201
0.98331
0.00148
0.00169
0.00179
0.06569
0.07496
0.07943
0.95204
1.09067
1.15687

0.00159
0.00263
0.00369
0.24327
0.40201
0.56509
0.20601
0.35957
0.48774
0.00004
0.00007
0.00009
0.06626
0.10233
0.13256
0.00734
0.01133
0.01499
0.00001
0.00001
0.00001
0.11015
0.17734
0.23710
0.00114
0.00183
0.00244
0.00000
0.00000
0.00000
0.00156
0.00333
0.00489
0.00000
0.00000
0.00001
0.00000
0.00000
0.00000
0.00476
0.01079
0.01574
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00020
0.00029
0.00038
0.00000
0.00000
0.00000

0.0447
0.0708
0.0925
0.8104
1.2841
1.6716
4.5970
7.4186
9.9847
0.0509
0.0717
0.0825
0.8402
1.2441
1.4401
4.8370
7.2177
8.3523
0.0485
0.0793
0.1028
0.8689
1.4231
1.8864
5.2685
8.9764
11.867
0.0062
0.0147
0.0208
0.2712
0.6399
0.9169
4.0755
9.7145
12.793
0.0054
0.0131
0.0199
0.2394
0.5860
0.8756
3.3569
8.2507
12.603
0.0062
0.0112
0.0148
0.2661
0.5145
0.6253
4.815
8.630
11.44

0.0856
0.1291
0.1731
1.1874
1.8779
2.4556
9.5295
17.259
23.933
0.1037
0.1484
0.1777
1.3513
2.0387
2.4916
10.159
17.796
22.395
0.08635
0.1303
0.1680
1.2422
1.8921
2.4451
10.079
17.200
23.910
0.00950
0.02351
0.03416
0.3778
0.9497
1.4379
7.6529
21.643
34.497
0.00770
0.01866
0.02868
0.2979
0.7635
1.2111
6.0112
17.160
28.958
0.0102
0.0218
0.0333
0.3943
0.9730
1.3709
7.9987
22.180
31.529

Fast-Lin
163 ms
176 ms
176 ms
154 ms
141 ms
152 ms
159 ms
168 ms
179 ms
805 ms
782 ms
792 ms
732 ms
711 ms
723 ms
685 ms
743 ms
729 ms
1.54 s
1.53 s
1.74 s
1.79 s
1.78 s
1.98 s
1.87 s
1.80 s
1.94 s
18.2 s
19.6 s
20.4 s
24.2 s
26.0 s
25.0 s
24.7 s
25.7 s
26.0 s
27.6 s
28.1 s
28.1 s
37.0 s
40.0 s
40.0 s
37.2 s
39.5 s
40.7 s
8.98 s
8.98 s
8.81 s
12.7 s
12.6 s
12.9 s
12.8 s
13.2 s
13.3 s

Fast-Lip CROWN-Ada
179 ms
213 ms
251 ms
184 ms
212 ms
291 ms
989 ms
1.15 s
1.37 s
1.28 s
859 ms
684 ms
1.06 s
696 ms
655 ms
2.36 s
2.69 s
3.08 s
3.42 s
2.17 s
2.00 s
2.58 s
1.96 s
2.01 s
1.93 s
2.04 s
2.40 s
38.2 s
48.2 s
50.5 s
39.4 s
31.2 s
33.2 s
45.1 s
36.2 s
31.7 s
64.7 s
72.3 s
76.3 s
60.7 s
56.8 s
56.3 s
65.6 s
53.5 s
42.1 s
20.1 s
20.3 s
22.1 s
20.9 s
18.7 s
20.7 s
21.0 s
19.8 s
17.6 s

128 ms
166 ms
143 ms
110 ms
133 ms
116 ms
136 ms
157 ms
144 ms
1.33 s
1.37 s
1.37 s
1.26 s
1.26 s
1.25 s
1.15 s
1.25 s
1.31 s
3.23 s
3.57 s
3.87 s
3.52 s
3.79 s
4.01 s
3.34 s
3.54 s
3.72 s
33.1 s
36.7 s
38.6 s
41.0 s
42.5 s
44.4 s
40.5 s
44.0 s
44.9 s
47.3 s
49.3 s
49.4 s
65.8 s
71.5 s
72.5 s
66.8 s
71.6 s
72.5 s
16.2 s
16.7 s
17.4 s
20.7 s
21.8 s
22.2 s
21.9 s
22.4 s
22.9 s

16

E.2 Results on CROWN-general

Table 7: Comparison of certiﬁed lower bounds by CROWN-Ada on ReLU networks and CROWN-
general on networks with tanh, sigmoid and arctan activations. CIFAR models with sigmoid activa-
tions achieve much worse accuracy than other networks and are thus excluded. For each norm, we
consider the robustness against three targeted attack classes: the runner-up class (with the second
largest probability), a random class and the least likely class.

Network

Certiﬁed Bounds by CROWN-general Average Computation Time (sec)

(cid:96)p norm

MNIST
3 × [1024]

MNIST
4 × [1024]

MNIST
5 × [1024]

CIFAR-10
5 × [2048]

CIFAR-10
6 × [2048]

CIFAR-10
7 × [1024]

target
runner-up
random
least
runner-up
random
least
runner-up
random
least
runner-up
random
least
runner-up
random
least
runner-up
random
least
runner-up
random
least
runner-up
random
least
runner-up
random
least
runner-up
random
least
runner-up
random
least
runner-up
random
least
runner-up
random
least
runner-up
random
least
runner-up
random
least
runner-up
random
least
runner-up
random
least
runner-up
random
least

tanh
0.0164
0.0230
0.0306
0.3546
0.5023
0.6696
2.4600
3.5550
4.8215
0.0091
0.0118
0.0147
0.2086
0.2729
0.3399
1.8296
2.4841
3.1261
0.0060
0.0073
0.0084
0.1369
0.1660
0.1909
1.1242
1.3952
1.6231
0.0005
0.0008
0.0010
0.0219
0.0368
0.0460
0.3744
0.6384
0.8051
0.0004
0.0006
0.0006
0.0177
0.0254
0.0294
0.3043
0.4406
0.5129
0.0006
0.0008
0.0008
0.0260
0.0344
0.0376
0.3826
0.5087
0.5595

sigmoid
0.0225
0.0325
0.0424
0.4515
0.6552
0.8576
2.7953
4.0854
5.4528
0.0162
0.0212
0.0243
0.3389
0.4447
0.5064
2.2397
2.9424
3.3486
0.0150
0.0202
0.0230
0.3153
0.4254
0.4849
2.0616
2.8082
3.2201
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-

(cid:96)∞

(cid:96)2

(cid:96)1

(cid:96)∞

(cid:96)2

(cid:96)1

(cid:96)∞

(cid:96)2

(cid:96)1

(cid:96)∞

(cid:96)2

(cid:96)1

(cid:96)∞

(cid:96)2

(cid:96)1

(cid:96)∞

(cid:96)2

(cid:96)1

arctan
0.0169
0.0240
0.0314
0.3616
0.5178
0.6769
2.4299
3.5995
4.7548
0.0107
0.0136
0.0165
0.2348
0.3034
0.3690
1.7481
2.3325
2.8881
0.0062
0.0077
0.0091
0.1426
0.1774
0.2096
1.2388
1.5842
1.9026
0.0006
0.0009
0.0011
0.0256
0.0406
0.0497
0.4491
0.7264
0.8955
0.0003
0.0006
0.0007
0.0163
0.0251
0.0306
0.2925
0.4620
0.5665
0.0005
0.0007
0.0008
0.0225
0.0317
0.0371
0.3648
0.5244
0.6171

tanh
0.3374
0.3185
0.3129
0.3139
0.3044
0.3869
0.2940
0.3277
0.3201
1.6794
1.7783
1.8908
1.6416
1.7589
1.8206
1.5506
1.6149
1.7762
3.9916
3.5068
3.9076
4.1634
4.1468
4.5045
4.4911
4.4543
4.4674
37.3918
38.0841
39.1638
47.4896
54.0104
55.8924
46.4041
54.2138
56.2512
59.5020
59.7220
60.8031
78.8801
84.2228
86.2997
78.7486
89.7717
87.2094
20.8612
21.4550
21.3406
27.9442
30.3782
30.7492
28.1898
29.6373
31.3457

sigmoid
0.3213
0.3388
0.3586
0.3110
0.3183
0.3495
0.3339
0.3306
0.3915
1.7902
1.7597
1.8483
1.7606
1.7518
1.7929
1.6052
1.7015
1.7902
4.4614
4.4069
4.6283
4.3311
4.1797
4.4773
3.9944
4.0839
4.5508
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-

arctan
0.3148
0.3128
0.3156
0.3005
0.2931
0.2676
0.3053
0.3109
0.3254
1.7099
1.7667
1.7930
1.8267
1.6945
1.8264
1.6704
1.6847
1.8345
3.7635
3.7387
3.9730
4.1039
4.0898
4.5497
4.4436
4.2609
4.5154
37.1383
37.9199
39.4041
48.3390
52.7471
56.3877
47.1640
51.6295
55.6069
58.2473
58.0388
60.9790
72.1884
83.1202
86.9320
70.2496
83.7972
86.6502
20.5169
21.2134
21.1804
27.0240
29.8086
30.7321
27.1238
30.5106
30.6481

17

