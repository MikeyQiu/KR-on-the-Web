8
1
0
2
 
p
e
S
 
8
1
 
 
]

G
L
.
s
c
[
 
 
2
v
6
4
5
4
0
.
9
0
7
1
:
v
i
X
r
a

NORMALIZED DIRECTION-PRESERVING ADAM

Zijun Zhang
Department of Computer Science
University of Calgary
zijun.zhang@ucalgary.ca

Lin Ma
School of Computer Science
Wuhan University
linmawhu@gmail.com

Zongpeng Li
Department of Computer Science
University of Calgary
zongpeng@ucalgary.ca

Chuan Wu
Department of Computer Science
The University of Hong Kong
cwu@cs.hku.hk

ABSTRACT

Adaptive optimization algorithms, such as Adam and RMSprop, have shown bet-
ter optimization performance than stochastic gradient descent (SGD) in some sce-
narios. However, recent studies show that they often lead to worse generalization
performance than SGD, especially for training deep neural networks (DNNs). In
this work, we identify the reasons that Adam generalizes worse than SGD, and
develop a variant of Adam to eliminate the generalization gap. The proposed
method, normalized direction-preserving Adam (ND-Adam), enables more pre-
cise control of the direction and step size for updating weight vectors, leading to
signiﬁcantly improved generalization performance. Following a similar rationale,
we further improve the generalization performance in classiﬁcation tasks by regu-
larizing the softmax logits. By bridging the gap between SGD and Adam, we also
hope to shed light on why certain optimization algorithms generalize better than
others.

1

INTRODUCTION

In contrast with the growing complexity of neural network architectures (Szegedy et al., 2015; He
et al., 2016; Hu et al., 2018), the training methods remain relatively simple. Most practical opti-
mization methods for deep neural networks (DNNs) are based on the stochastic gradient descent
(SGD) algorithm. However, the learning rate of SGD, as a hyperparameter, is often difﬁcult to tune,
since the magnitudes of different parameters vary widely, and adjustment is required throughout the
training process.

To tackle this problem, several adaptive variants of SGD were developed, including Adagrad (Duchi
et al., 2011), Adadelta (Zeiler, 2012), RMSprop (Tieleman & Hinton, 2012), Adam (Kingma & Ba,
2015). These algorithms aim to adapt the learning rate to different parameters automatically, based
on the statistics of gradient. Although they usually simplify learning rate settings, and lead to faster
convergence, it is observed that their generalization performance tend to be signiﬁcantly worse than
that of SGD in some scenarios (Wilson et al., 2017). This intriguing phenomenon may explain why
SGD (possibly with momentum) is still prevalent in training state-of-the-art deep models, especially
feedforward DNNs (Szegedy et al., 2015; He et al., 2016; Hu et al., 2018). Furthermore, recent
work has shown that DNNs are capable of ﬁtting noise data (Zhang et al., 2017), suggesting that
their generalization capabilities are not the mere result of DNNs themselves, but are entwined with
optimization (Arpit et al., 2017).

This work aims to bridge the gap between SGD and Adam in terms of the generalization perfor-
mance. To this end, we identify two problems that may degrade the generalization performance
of Adam, and show how these problems are (partially) avoided by using SGD with L2 weight de-
cay. First, the updates of SGD lie in the span of historical gradients, whereas it is not the case for
Adam. This difference has been discussed in rather recent literature (Wilson et al., 2017), where
the authors show that adaptive methods can ﬁnd drastically different but worse solutions than SGD.

Second, while the magnitudes of Adam parameter updates are invariant to rescaling of the gradient,
the effect of the updates on the same overall network function still varies with the magnitudes of pa-
rameters. As a result, the effective learning rates of weight vectors tend to decrease during training,
which leads to sharp local minima that do not generalize well (Hochreiter & Schmidhuber, 1997).

To address these two problems of Adam, we propose the normalized direction-preserving Adam
(ND-Adam) algorithm, which controls the update direction and step size in a more precise way. We
show that ND-Adam is able to achieve signiﬁcantly better generalization performance than vanilla
Adam, and matches that of SGD in image classiﬁcation tasks.

We summarize our contributions as follows:

•

•

•

We observe that the directions of Adam parameter updates are different from that of SGD,
i.e., Adam does not preserve the directions of gradients as SGD does. We ﬁx the problem
by adapting the learning rate to each weight vector, instead of each individual weight, such
that the direction of the gradient is preserved.
For both Adam and SGD without L2 weight decay, we observe that the magnitude of each
vector’s direction change depends on its L2-norm. We show that, using SGD with L2
weight decay implicitly normalizes the weight vectors, and thus remove the dependence
in an approximate manner. We ﬁx the problem for Adam by explicitly normalizing each
weight vector, and by optimizing only its direction, such that the effective learning rate can
be precisely controlled.
We further demonstrate that, without proper regularization, the learning signal backpropa-
gated from the softmax layer may vary with the overall magnitude of the logits in an unde-
sirable way. Based on the observation, we apply batch normalization or L2-regularization
to the logits, which further improves the generalization performance in classiﬁcation tasks.

In essence, our proposed methods, ND-Adam and regularized softmax, improve the generalization
performance of Adam by enabling more precise control over the directions of parameter updates,
the learning rates, and the learning signals.

The remainder of this paper is organized as follows. In Sec. 2, we identify two problems of Adam,
and show how SGD with L2 weight decay partially avoids these problems. In Sec. 3, we further
discuss and develop ND-Adam as a solution to the two problems. In Sec. 4, we propose regularized
softmax to improve the learning signal backpropagated from the softmax layer. We provide em-
pirical evidence for our analysis, and evaluate the performance of the proposed methods in Sec. 5.
1

2 BACKGROUND AND MOTIVATION

2.1 ADAPTIVE MOMENT ESTIMATION (ADAM)

Adaptive moment estimation (Adam) (Kingma & Ba, 2015) is a stochastic optimization method
that applies individual adaptive learning rates to different parameters, based on the estimates of the
Rn, Adam
ﬁrst and second moments of the gradients. Speciﬁcally, for n trainable parameters, θ
maintains a running average of the ﬁrst and second moments of the gradient w.r.t. each parameter as

∈

mt = β1mt−1 + (1

β1) gt,

−
β2) g2
t .

and

(1b)
−
Rn denote respectively the ﬁrst and second
Here, t denotes the time step, mt ∈
R are the corresponding decay factors. Kingma & Ba (2015)
moments, and β1 ∈
further notice that, since m0 and v0 are initialized to 0’s, they are biased towards zero during the
initial time steps, especially when the decay factors are large (i.e., close to 1). Thus, for computing
the next update, they need to be corrected as

vt = β2vt−1 + (1
Rn and vt ∈

R and β2 ∈

(1a)

(2)

1Code is available at https://github.com/zj10/ND-Adam.

ˆmt =

, ˆvt =

mt

βt
1

1

−

vt

1

−

,

βt
2

where βt

1, βt

2 are the t-th powers of β1, β2 respectively. Then, we can update each parameter as
αt
√ˆvt + (cid:15)

θt = θt−1 −

ˆmt,

(3)

where αt is the global learning rate, and (cid:15) is a small constant to avoid division by zero. Note the
above computations between vectors are element-wise.

A distinguishing merit of Adam is that the magnitudes of parameter updates are invariant to rescaling
of the gradient, as shown by the adaptive learning rate term, αt/ (cid:0)√ˆvt + (cid:15)(cid:1). However, there are two
potential problems when applying Adam to DNNs.

First, in some scenarios, DNNs trained with Adam generalize worse than that trained with stochas-
tic gradient descent (SGD) (Wilson et al., 2017). Zhang et al. (2017) demonstrate that over-
parameterized DNNs are capable of memorizing the entire dataset, no matter if it is natural data
or meaningless noise data, and thus suggest much of the generalization power of DNNs comes from
the training algorithm, e.g., SGD and its variants. It coincides with another recent work (Wilson
et al., 2017), which shows that simple SGD often yields better generalization performance than
adaptive gradient methods, such as Adam. As pointed out by the latter, the difference in the gen-
eralization performance may result from the different directions of updates. Speciﬁcally, for each
hidden unit, the SGD update of its input weight vector can only lie in the span of all possible input
vectors, which, however, is not the case for Adam due to the individually adapted learning rates. We
refer to this problem as the direction missing problem.

Second, while batch normalization (Ioffe & Szegedy, 2015) can signiﬁcantly accelerate the con-
vergence of DNNs, the input weights and the scaling factor of each hidden unit can be scaled in
inﬁnitely many (but consistent) ways, without changing the function implemented by the hidden
unit. Thus, for different magnitudes of an input weight vector, the updates given by Adam can have
different effects on the overall network function, which is undesirable. Furthermore, even when
batch normalization is not used, a network using linear rectiﬁers (e.g., ReLU, leaky ReLU) as acti-
vation functions, is still subject to ill-conditioning of the parameterization (Glorot et al., 2011), and
hence the same problem. We refer to this problem as the ill-conditioning problem.

2.2 L2 WEIGHT DECAY

L2 weight decay is a regularization technique frequently used with SGD. It often has a signiﬁcant
effect on the generalization performance of DNNs. Despite its simplicity and crucial role in the
training process, how L2 weight decay works in DNNs remains to be explained. A common jus-
tiﬁcation is that L2 weight decay can be introduced by placing a Gaussian prior upon the weights,
when the objective is to ﬁnd the maximum a posteriori (MAP) weights (Blundell et al.). How-
ever, as discussed in Sec. 2.1, the magnitudes of input weight vectors are irrelevant in terms of the
overall network function, in some common scenarios, rendering the variance of the Gaussian prior
meaningless.

We propose to view L2 weight decay in neural networks as a form of weight normalization, which
may better explain its effect on the generalization performance. Consider a neural network trained
with the following loss function:

(cid:101)L (θ;

) = L (θ;

) +

D

D

λ
2

(cid:88)

i∈N

2
2 ,

wi(cid:107)
(cid:107)

D

) is the original loss function speciﬁed by the task,

where L (θ;
is
the set of all hidden units, and wi denotes the input weights of hidden unit i, which is included in the
trainable parameters, θ. For simplicity, we consider SGD updates without momentum. Therefore,
the update of wi at each time step is

is a batch of training data,

N

D

∆wi =

∂ (cid:101)L
∂wi

α

−

=

α

−

(cid:18) ∂L
∂wi

(cid:19)

+ λwi

,

where α is the learning rate. As we can see from Eq. (5), the gradient magnitude of the L2 penalty is
proportional to
wi(cid:107)2 to an equilibrium
wi(cid:107)2, thus forms a negative feedback loop that stabilizes
(cid:107)
wi(cid:107)2 tends to increase or decrease dramatically at the beginning of
value. Empirically, we ﬁnd that
(cid:107)

(cid:107)

(4)

(5)

the training, and then varies mildly within a small range, which indicates
In practice, we usually have
0.
wi ·
Let l(cid:107)wi and l⊥wi be the vector projection and rejection of ∂L
∂wi

∆wi(cid:107)2 /
(cid:107)

wi(cid:107)2 (cid:28)
(cid:107)

∆wi ≈

wi + ∆wi(cid:107)2.
(cid:107)
1, thus ∆wi is approximately orthogonal to wi, i.e.

wi(cid:107)2 ≈ (cid:107)

on wi, which are deﬁned as

l(cid:107)wi =

(cid:18) ∂L

∂wi ·

wi
wi(cid:107)2

(cid:19) wi
wi(cid:107)2
(cid:107)

, l⊥wi =

∂L
∂wi −

l(cid:107)wi.

(cid:107)
From Eq. (5) and (6), it is easy to show

(6)

(7)

∆wi(cid:107)2
(cid:107)
wi(cid:107)2 ≈
(cid:107)

l⊥wi(cid:107)2
(cid:107)
(cid:13)
(cid:13)
(cid:13)l(cid:107)wi
(cid:13)2

αλ.

As discussed in Sec. 2.1, when batch normalization is used, or when linear rectiﬁers are used as
wi(cid:107)2 becomes irrelevant; it is the direction of wi that actually
activation functions, the magnitude of
(cid:107)
makes a difference in the overall network function. If L2 weight decay is not applied, the magnitude
wi(cid:107)2 increases during the training process, which can
of wi’s direction change will decrease as
potentially lead to overﬁtting (discussed in detail in Sec. 3.2). On the other hand, Eq. (7) shows that
L2 weight decay implicitly normalizes the weights, such that the magnitude of wi’s direction change
does not depend on
wi(cid:107)2, and can be tuned by the product of α and λ. In the following, we refer
(cid:107)
to
∆wi(cid:107)2 /

wi(cid:107)2 as the effective learning rate of wi.
(cid:107)

While L2 weight decay produces the normalization effect in an implicit and approximate way, we
will show that explicitly doing so enables more precise control of the effective learning rate.

(cid:107)

(cid:107)

3 NORMALIZED DIRECTION-PRESERVING ADAM

We ﬁrst present the normalized direction-preserving Adam (ND-Adam) algorithm, which essentially
improves the optimization of the input weights of hidden units, while employing the vanilla Adam
algorithm to update other parameters. Speciﬁcally, we divide the trainable parameters, θ, into two
. Then we update θv and θs by
sets, θv and θs, such that θv =
different rules, as described by Alg. 1. The learning rates for the two sets of parameters are denoted
by αv

, and θs =

wi|
{

∈ N }

θv

{

\

}

θ

i

t , respectively.

t and αs

In Alg. 1, computing gt (wi) and wi,t may take slightly more time compared to Adam, which how-
ever is negligible in practice. On the other hand, to estimate the second order moment of each
Rn, Adam maintains n scalars, whereas ND-Adam requires only one scalar, vt (wi), and thus
wi ∈
reduces the memory overhead of Adam.

In the following, we address the direction missing problem and the ill-conditioning problem dis-
cussed in Sec. 2.1, and explain Alg. 1 in detail. We show how the proposed algorithm jointly solves
the two problems, as well as its relation to other normalization schemes.

3.1 PRESERVING GRADIENT DIRECTIONS

Assuming the stationarity of a hidden unit’s input distribution, the SGD update (possibly with mo-
mentum) of the input weight vector is a linear combination of historical gradients, and thus can
only lie in the span of the input vectors. Consequently, the input weight vector itself will eventually
converge to the same subspace.

In contrast, the Adam algorithm adapts the global learning rate to each scalar parameter indepen-
dently, such that the gradient of each parameter is normalized by a running average of its magnitudes,
which changes the direction of the gradient. To preserve the direction of the gradient w.r.t. each input
weight vector, we generalize the learning rate adaptation scheme from scalars to vectors.

Let gt (wi), mt (wi), vt (wi) be the counterparts of gt, mt, vt for vector wi. Since Eq. (1a) is
a linear combination of historical gradients, it can be extended to vectors without any change; or
equivalently, we can rewrite it for each vector as

mt (wi) = β1mt−1 (wi) + (1

β1) gt (wi) .

−

(8)

*/

*/

*/

*/

(9)

(10)

(11)

Algorithm 1: Normalized direction-preserving Adam
/* Initialization
t
←
for i

0;

do
∈ N
wi,0(cid:107)2;
wi,0/
wi,0 ←
(cid:107)
0;
m0 (wi)
0;
v0 (wi)

←
←

/* Perform T iterations of training
while t < T do
t + 1;
t
←
/* Update θv
for i

do

−

∈ N
¯gt (wi)
gt (wi)
mt (wi)
vt (wi)
ˆmt (wi)
ˆvt (wi)
¯wi,t ←
wi,t ←

∂L/∂wi;
¯gt (wi)
(¯gt (wi)
·
β1mt−1 (wi) + (1
β2vt−1 (wi) + (1
−
βt
1);
mt (wi) / (1
−
βt
2);
vt (wi) / (1
−
t ˆmt (wi) /

←
←
←
←
←
←
αv
wi,t−1 −
¯wi,t(cid:107)2;
¯wi,t/
(cid:107)
/* Update θs using Adam
AdamUpdate (cid:0)θs
θs
t ←
return θT ;

t−1; αs

t , β1, β2

(cid:1);

wi,t−1) wi,t−1;
β1) gt (wi);
2
2;
gt (wi)
(cid:107)
(cid:107)

−
β2)

(cid:16)(cid:112)

ˆvt (wi) + (cid:15)

(cid:17)

;

We then extend Eq. (1b) as

vt (wi) = β2vt−1 (wi) + (1

β2)

gt (wi)
(cid:107)
(cid:107)

−

2
2 ,

i.e., instead of estimating the average gradient magnitude for each individual parameter, we estimate
2
2 for each vector wi. In addition, we modify Eq. (2) and (3) accordingly as
the average of

gt (wi)
(cid:107)
(cid:107)

and

ˆmt (wi) =

, ˆvt (wi) =

mt (wi)
βt
1
1

−

wi,t = wi,t−1 −

(cid:112)

αv
t
ˆvt (wi) + (cid:15)

vt (wi)
βt
1
2

−

,

ˆmt (wi) .

Here, ˆmt (wi) is a vector with the same dimension as wi, whereas ˆvt (wi) is a scalar. Therefore,
when applying Eq. (11), the direction of the update is the negative direction of ˆmt (wi), and thus is
in the span of the historical gradients of wi.

Despite the empirical success of SGD, a question remains as to why it is desirable to constrain the
input weights in the span of the input vectors. A possible explanation is related to the manifold
hypothesis, which suggests that real-world data presented in high dimensional spaces (e.g., images,
audios, text) concentrates on manifolds of much lower dimensionality (Cayton, 2005; Narayanan &
Mitter, 2010). In fact, commonly used activation functions, such as (leaky) ReLU, sigmoid, tanh,
can only be activated (not saturating or having small gradients) by a portion of the input vectors, in
whose span the input weights lie upon convergence. Assuming the local linearity of the manifolds
of data or hidden-layer representations, constraining the input weights in the subspace that contains
that portion of the input vectors, encourages the hidden units to form local coordinate systems on
the corresponding manifold, which can lead to good representations (Rifai et al., 2011).

3.2 SPHERICAL WEIGHT OPTIMIZATION

The ill-conditioning problem occurs when the magnitude change of an input weight vector can be
compensated by other parameters, such as the scaling factor of batch normalization, or the output

weight vector, without affecting the overall network function. Consequently, suppose we have two
DNNs that parameterize the same function, but with some of the input weight vectors having differ-
ent magnitudes, applying the same SGD or Adam update rule will, in general, change the network
functions in different ways. Thus, the ill-conditioning problem makes the training process inconsis-
tent and difﬁcult to control.

More importantly, when the weights are not properly regularized (e.g., without using L2 weight
decay), the magnitude of wi’s direction change will decrease as
wi(cid:107)2 increases during the training
process. As a result, the effective learning rate for wi tends to decrease faster than expected. The
gradient noise introduced by large learning rates is crucial to avoid sharp minima (Smith & Le,
2018). And it is well known that sharp minima generalize worse than ﬂat minima (Hochreiter &
Schmidhuber, 1997).

(cid:107)

As shown in Sec. 2.2, when combined with SGD, L2 weight decay can alleviate the ill-conditioning
problem by implicitly and approximately normalizing the weights. However, the approximation
fails when
wi(cid:107)2 is far from the equilibrium due to improper initialization, or drastic changes in
(cid:107)
the magnitudes of the weight vectors. In addition, due to the direction missing problem, naively
applying L2 weight decay to Adam does not yield the same effect as it does on SGD. In concurrent
work, Loshchilov & Hutter (2017a) address the problem by decoupling the weight decay and the
optimization steps taken w.r.t. the loss function. However, their experimental results indicate that
improving L2 weight decay alone cannot eliminate the generalization gap between Adam and SGD.

The ill-conditioning problem is also addressed by Neyshabur et al. (2015), by employing a geometry
invariant to rescaling of weights. However, their proposed methods do not preserve the direction of
gradient.

To address the ill-conditioning problem in a more principled way, we restrict the L2-norm of each
wi to 1, and only optimize its direction. In other words, instead of optimizing wi in a n-dimensional
1)-dimensional unit sphere. Speciﬁcally, we ﬁrst compute the raw
space, we optimize wi on a (n
gradient w.r.t. wi, ¯gt (wi) = ∂L/∂wi, and project the gradient onto the unit sphere as

−

Here,

gt (wi) = ¯gt (wi)

wi,t−1) wi,t−1.
wi,t−1(cid:107)2 = 1. Then we follow Eq. (8)-(10), and replace (11) with
ˆmt (wi) , and wi,t =

(¯gt (wi)

−

·

(cid:107)

¯wi,t = wi,t−1 −

(cid:112)

αv
t
ˆvt (wi) + (cid:15)

¯wi,t
¯wi,t(cid:107)2
(cid:107)

.

In Eq. (12), we keep only the component that is orthogonal to wi,t−1. However, ˆmt (wi) is not
wi(cid:107)2 can still
necessarily orthogonal as well; moreover, even when ˆmt (wi) is orthogonal to wi,t−1,
increase according to the Pythagorean theorem. Therefore, we explicitly normalize wi,t in Eq. (13),
to ensure
wi,t(cid:107)2 = 1 after each update. Also note that, since wi,t−1 is a linear combination of
(cid:107)
its historical gradients, gt (wi) still lies in the span of the historical gradients after the projection in
Eq. (12).

(cid:107)

Compared to SGD with L2 weight decay, spherical weight optimization explicitly normalizes the
weight vectors, such that each update to the weight vectors only changes their directions, and strictly
keeps the magnitudes constant. As a result, the effective learning rate of a weight vector is

(12)

(13)

(14)

∆wi,t(cid:107)2
(cid:107)
wi,t−1(cid:107)2 ≈
(cid:107)

ˆmt (wi)
(cid:107)2
(cid:107)
(cid:112)
ˆvt (wi)

αv
t ,

which enables precise control over the learning rate of wi through a single hyperparameter, αv
t ,
rather than two as required by Eq. (7).

Note that it is possible to control the effective learning rate more precisely, by normalizing ˆmt (wi)
(cid:107)2, instead of by (cid:112)
ˆvt (wi). However, by doing so, we lose information provided
with
ˆmt (wi)
(cid:107)
by
In addition, since ˆmt (wi) is less noisy than gt (wi),
(cid:107)2 at different time steps.
ˆmt (wi)
(cid:107)
(cid:112)
ˆvt (wi) becomes small near convergence, which is considered a desirable property
(cid:107)2 /
ˆmt (wi)
(cid:107)
of Adam (Kingma & Ba, 2015). Thus, we keep the gradient normalization scheme intact.

We note the difference between various gradient normalization schemes and the normalization
scheme employed by spherical weight optimization. As shown in Eq. (11), ND-Adam general-
izes the gradient normalization scheme of Adam, and thus both Adam and ND-Adam normalize

the gradient by a running average of its magnitude. This, and other similar schemes (Hazan et al.,
2015; Yu et al., 2017) make the optimization less susceptible to vanishing and exploding gradients.
The proposed spherical weight optimization serves a different purpose. It normalizes each weight
vector and projects the gradient onto a unit sphere, such that the effective learning rate can be con-
trolled more precisely. Moreover, it provides robustness to improper weight initialization, since the
magnitude of each weight vector is kept constant.

For nonlinear activation functions (without batch normalization), such as sigmoid and tanh, an extra
scaling factor is needed for each hidden unit to express functions that require unnormalized weight
), the activation of hidden
vectors. For instance, given an input vector x
·
unit i is then given by

Rn, and a nonlinearity φ (

(15)
where γi is the scaling factor, and bi is the bias. Consequently, normalizing weight vectors does not
limit the expressiveness of models.

x + bi) ,

∈
yi = φ (γiwi ·

3.3 RELATION TO WEIGHT NORMALIZATION AND BATCH NORMALIZATION

wi(cid:107)2 (cid:54)
(cid:107)

A related normalization and reparameterization scheme, weight normalization (Salimans & Kingma,
2016), has been developed as an alternative to batch normalization, aiming to accelerate the conver-
gence of SGD optimization. We note the difference between spherical weight optimization and
weight normalization. First, the weight vector of each hidden unit is not directly normalized in
= 1 in general. At training time, the activation of hidden unit i is
weight normalization, i.e,

(cid:18) γi
wi(cid:107)2
which is equivalent to Eq. (15) for the forward pass. For the backward pass, the effective learning
wi(cid:107)2 in weight normalization, hence it does not solve the ill-conditioning
rate still depends on
(cid:107)
problem. At inference time, both of these two schemes can merge wi and γi into a single equivalent
weight vector, w(cid:48)

yi = φ

x + bi

wi ·

(16)

i = γiwi, or w(cid:48)

wi.

(cid:19)

(cid:107)

,

i = γi
(cid:107)wi(cid:107)2

While spherical weight optimization naturally encompasses weight normalization, it can further
beneﬁt from batch normalization. When combined with batch normalization, Eq. (15) evolves into

yi = φ (γi BN (wi ·

x) + bi) ,

(17)

where BN (
) represents the transformation done by batch normalization without scaling and shift-
·
ing. Here, γi serves as the scaling factor for both the normalized weight vector and batch normal-
ization.

4 REGULARIZED SOFTMAX

For multi-class classiﬁcation tasks, the softmax function is the de facto activation function for the
output layer. Despite its simplicity and intuitive probabilistic interpretation, we observe a related
problem to the ill-conditioning problem we have addressed. Similar to how different magnitudes
of weight vectors result in different updates to the same network function, the learning signal back-
propagated from the softmax layer varies with the overall magnitude of the logits.

Speciﬁcally, when using cross entropy as the surrogate loss with one-hot target vectors, the predic-
tion is considered correct as long as arg maxc∈C (zc) is the target class, where zc is the logit before
. Thus, the logits can be positively scaled
the softmax activation, corresponding to category c
together without changing the predictions, whereas the cross entropy and its derivatives will vary
with the scaling factor. Concretely, denoting the scaling factor by η, the gradient w.r.t. each logit is
(cid:21)

∈ C

(cid:20)

∂L
∂zˆc

= η

(cid:80)

exp (ηzˆc)
c∈C exp (ηzc) −

1

, and

∂L
∂z¯c

=

(cid:80)

η exp (ηz¯c)
c∈C exp (ηzc)

,

(18)

where ˆc is the target class, and ¯c

∈ C\ {

.
ˆc
}

For Adam and ND-Adam, since the gradient w.r.t. each scalar or vector are normalized, the absolute
magnitudes of Eq. (18) are irrelevant. Instead, the relative magnitudes make a difference here. When

η is small, we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂L/∂z¯c
∂L/∂zˆc

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

lim
η→0

1

,

1

|C| −

which indicates that, when the magnitude of the logits is small, softmax encourages the logit of the
target class to increase, while equally penalizing that of the other classes, regardless of the difference
in ˆz
. However, it is more reasonable to penalize more the logits that are
ˆz
}
closer to ˆz, which are more likely to cause misclassiﬁcation.

¯z for different ¯z

∈ C\ {

−

On the other end of the spectrum, assuming no two digits are the same, we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂L/∂z¯c(cid:48)
∂L/∂zˆc

(cid:12)
(cid:12)
(cid:12)
(cid:12)

lim
η→∞

= 1, lim
η→∞

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂L/∂z¯c(cid:48)(cid:48)
∂L/∂zˆc

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= 0,

∈ C\ {

ˆc, ¯c(cid:48)
where ¯c(cid:48) = arg maxc∈C\{ˆc} (zc), and ¯c(cid:48)(cid:48)
. Eq. (20) indicates that, when the magnitude
}
of the logits is large, softmax penalizes only the largest logit of the non-target classes. In this case,
although the logit that is most likely to cause misclassiﬁcation is strongly penalized, the logits of
other non-target classes are ignored. As a result, the logits of the non-target classes tend to be similar
at convergence, ignoring the fact that some classes are closer to each other than the others. The latter
case is related to the saturation problem of softmax discussed in the literature (Oland et al., 2017),
where they focus on the problem of small absolute gradient magnitude, which nevertheless does not
affect Adam and ND-Adam.

We propose two methods to exploit the prior knowledge that the magnitude of the logits should
not be too small or too large. First, we can apply batch normalization to the logits. But instead
of setting γc’s as trainable variables, we consider them as a single hyperparameter, γC, such that
. Tuning the value of γC can lead to a better trade-off between the two extremes
γc = γC,
described by Eq. (19) and (20). We observe in practice that the optimal value of γC tends to be the
same for different optimizers or different network widths, but varies with network depth. We refer
to this method as batch-normalized softmax (BN-Softmax).

c
∀

∈ C

Alternatively, since the magnitude of the logits tends to grow larger than expected (in order to mini-
mize the cross entropy), we can apply L2-regularization to the logits by adding the following penalty
to the loss function:

(19)

(20)

(21)

LC =

λC
2

(cid:88)

c∈C

z2
c ,

where λC is a hyperparameter to be tuned. Different from BN-Softmax, λC can also be shared by
different networks of different depths.

5 EXPERIMENTS

In this section, we provide empirical evidence for the analysis in Sec. 2.2, and evaluate the perfor-
mance of ND-Adam and regularized softmax on CIFAR-10 and CIFAR-100.

5.1 THE EFFECT OF L2 WEIGHT DECAY

To empirically examine the effect of L2 weight decay, we train a wide residual network (WRN)
(Zagoruyko & Komodakis, 2016b) of 22 layers, with a width of 7.5 times that of a vanilla ResNet.
Using the notation suggested by Zagoruyko & Komodakis (2016b), we refer to this network as
WRN-22-7.5. We train the network on the CIFAR-10 dataset (Krizhevsky & Hinton, 2009), with
a small modiﬁcation to the original WRN architecture, and with a different learning rate anneal-
ing schedule. Speciﬁcally, for simplicity and slightly better performance, we replace the last
fully connected layer with a convolutional layer with 10 output feature maps.
i.e., we change
the layers after the last residual block from BN-ReLU-GlobalAvgPool-FC-Softmax to
BN-ReLU-Conv-GlobalAvgPool-Softmax. In addition, for clearer comparisons, the learn-
ing rate is annealed according to a cosine function without restart (Loshchilov & Hutter, 2017b;
Gastaldi, 2017). We train the model for 80k iterations with a batch size of 128, similar to the set-
tings used by Zagoruyko & Komodakis (Zagoruyko & Komodakis, 2016b). The experiments are
based on a TensorFlow implementation of WRN (Wu, 2016).

i and ∆wp

i , where ∆wl

As a common practice, we use SGD with a momentum of 0.9, the analysis for which is similar to
that in Sec. 2.2. Due to the linearity of derivatives and momentum, ∆wi can be decomposed as
i + ∆wp
∆wi = ∆wl
i are the components corresponding to the original loss
), and the L2 penalty term (see Eq. (4)), respectively. Fig. 1a shows the ratio between
function, L (
·
i (cid:107)2, which indicates how the tendency of ∆wl
the scalar projection of ∆wl
i to
i and
i . Note that ∆wp
wi(cid:107)2 is compensated by ∆wp
increase
i points to the negative direction of wi,
even when momentum is used, since the direction change of wi is slow. As shown in Fig. 1a, at the
beginning of the training, ∆wp
wi(cid:107)2 to its equilibrium value. During
(cid:107)
i , and ∆wp
i on ∆wp
the middle stage of the training, the projection of ∆wl
i almost cancel each other.
Then, towards the end of the training, the gradient of wi diminishes rapidly, making ∆wp
i dominant
again. Therefore, Eq. (7) holds more accurately during the middle stage of the training.

i dominants and quickly adjusts

i on ∆wp

∆wp

(cid:107)

(cid:107)

∆wi(cid:107)2 /
(cid:107)

In Fig. 1b, we show how the effective learning rate varies in different hyperparameter settings. By
wi(cid:107)2 is expected to remain the same as long as αλ stays constant, which is
Eq. (7),
(cid:107)
conﬁrmed by the fact that the curve for α0 = 0.1, λ = 0.001 overlaps with that for α0 = 0.05, λ =
0.002. However, comparing the curve for α0 = 0.1, λ = 0.001, with that for α0 = 0.1, λ =
wi(cid:107)2 does not change proportionally to αλ. On
0.0005, we can see that the value of
(cid:107)
the other hand, by using ND-Adam, we can control the value of
wi(cid:107)2 more precisely by
(cid:107)
adjusting the learning rate for weight vectors, αv. For the same training step, changes in αv lead to
approximately proportional changes in
wi(cid:107)2, as shown by the two curves corresponding
to ND-Adam in Fig. 1b.

∆wi(cid:107)2 /
(cid:107)

∆wi(cid:107)2 /
(cid:107)

∆wi(cid:107)2 /

(cid:107)

(cid:107)

(a) Scalar projection of ∆wl
(cid:107)∆wp

i (cid:107)2.

i on ∆wp

i normalized by

(b) Relative magnitudes of weight updates, or effective
learning rates.

Figure 1: An illustration of how L2 weight decay and ND-Adam control the effective learning rate.
The results are obtained from the 5th layer of the network, and other layers show similar results.

5.2 PERFORMANCE EVALUATION

To compare the generalization performance of SGD, Adam, and ND-Adam, we train the same WRN-
22-7.5 network on the CIFAR-10 and CIFAR-100 datasets. For SGD and ND-Adam, we ﬁrst tune
the hyperparameters for SGD (α0 = 0.1, λ = 0.001, momentum 0.9), then tune the initial learning
rate of ND-Adam for weight vectors to match the effective learning rate to that of SGD, i.e., αv
0 =
0.05, as shown in Fig. 1b. While L2 weight decay can greatly affect the performance of SGD, it does
not noticeably beneﬁt Adam in our experiments. For Adam and ND-Adam, β1 and β2 are set to the
default values of Adam, i.e., β1 = 0.9, β2 = 0.999. Although the learning rate of Adam is usually
set to a constant value, we observe better performance with the cosine learning rate schedule. The
initial learning rate of Adam (α0), and that of ND-Adam for scalar parameters (αs
0) are both tuned
to 0.001. We use horizontal ﬂips and random crops for data augmentation, and no dropout is used.

We ﬁrst experiment with the use of trainable scaling parameters (γi) of batch normalization. As
shown in Fig. 2, at convergence, the test accuracies of ND-Adam are signiﬁcantly improved upon
that of vanilla Adam, and matches that of SGD. Note that at the early stage of training, the test accu-
racies of Adam increase more rapidly than that of ND-Adam and SGD. However, the test accuracies
remain at a high level afterwards, which indicates that Adam tends to quickly ﬁnd and get stuck in
bad local minima that do not generalize well.

The average results of 3 runs are summarized in the ﬁrst part of Table 1. Interestingly, compared
to SGD, ND-Adam shows slightly better performance on CIFAR-10, but worse performance on
CIFAR-100. This inconsistency may be related to the problem of softmax discussed in Sec. 4, that
there is a lack of proper control over the magnitude of the logits. But overall, given comparable ef-
fective learning rates, ND-Adam and SGD show similar generalization performance. In this sense,
the effective learning rate is a more natural learning rate measure than the learning rate hyperparam-
eter.

Figure 2: Test accuracies of the same network
trained with SGD, Adam, and ND-Adam. De-
tails are shown in the ﬁrst part of Table 1.

Figure 3: Magnitudes of softmax logits in differ-
ent settings. Results of WRN-22-7.5 networks
trained on CIFAR-10.

Next, we repeat the experiments with the use of BN-Softmax. As discussed in Sec. 3.2, γi’s can be
removed from a linear rectiﬁer network, without changing the overall network function. Although
this property does not strictly hold for residual networks due to the skip connections, we observe that
when BN-Softmax is used, simply removing the scaling factors results in slightly better performance
for all three algorithms. Thus, we only report results for this setting. The scaling factor of the logits,
γC, is set to 2.5 for CIFAR-10, and 1 for CIFAR-100.

As shown in the second part of Table 1, while we obtain the best generalization performance with
ND-Adam, the improvement is most prominent for Adam, and is relatively small for SGD. This
discrepancy can be explained by comparing the magnitudes of softmax logits without regularization.
As shown in Fig. 3, the magnitude of logits corresponding to Adam is much larger than that of ND-
Adam and SGD, and therefore beneﬁts more from the regularization.

Table 1: Test error rates of WRN-22-7.5 net-
works on CIFAR-10 and CIFAR-100. Based on
a TensorFlow implementation of WRN.

CIFAR-10
Error (%)

CIFAR-100
Error (%)

BN w/ scaling factors

Method

SGD
Adam
ND-Adam

SGD
Adam
ND-Adam

BN w/o scaling factors, BN-Softmax

4.61
6.14
4.53

4.49
5.43
4.14

20.60
25.51
21.45

20.18
22.48
19.90

Table 2: Test error
rates of WRN-22-7.5
and WRN-28-10 networks on CIFAR-10 and
CIFAR-100. Based on the original implemen-
tation of WRN.

Method

SGD
ND-Adam

SGD
ND-Adam

CIFAR-10
Error (%)

CIFAR-100
Error (%)

WRN-22-7.5

WRN-28-10

3.84
3.70

3.80
3.70

19.24
19.30

18.48
18.42

While the TensorFlow implementation we use already provides an adequate test bed, we notice
that it is different from the original implementation of WRN in several aspects. For instance, they
use different nonlinearities (leaky ReLU vs. ReLU), and use different skip connections for down-
sampling (average pooling vs. strided convolution). A subtle yet important difference is that, L2-

regularization is applied not only to weight vectors, but also to the scales and biases of batch normal-
ization in the original implementation, which leads to better generalization performance. For further
comparison between SGD and ND-Adam, we reimplement ND-Adam and test its performance on a
PyTorch version of the original implementation (Zagoruyko & Komodakis, 2016a).

Due to the aforementioned differences, we use a slightly different hyperparameter setting in this
experiment. Speciﬁcally, for SGD λ is set to 5e
6 (L2-
4, while for ND-Adam λ is set to 5e
regularization for biases), and both αs
0 are set to 0.04. In this case, regularizing softmax
does not yield improved performance for SGD, since the L2-regularization applied to γi’s and the
last layer weights can serve a similar purpose. Thus, we only apply L2-regularized softmax for
ND-Adam with λC = 0.001. The average results of 3 runs are summarized in Table 2. Note that the
performance of SGD for WRN-28-10 is slightly better than that reported with the original imple-
mentation (i.e., 4.00 and 19.25), due to the modiﬁcations described in Sec. 5.1. In this experiment,
SGD and ND-Adam show almost identical generalization performance.

0 and αv

−

−

6 CONCLUSION

We introduced ND-Adam, a tailored version of Adam for training DNNs, to bridge the general-
ization gap between Adam and SGD. ND-Adam is designed to preserve the direction of gradient
for each weight vector, and produce the regularization effect of L2 weight decay in a more precise
and principled way. We further introduced regularized softmax, which limits the magnitude of soft-
max logits to provide better learning signals. Combining ND-Adam and regularized softmax, we
show through experiments signiﬁcantly improved generalization performance, eliminating the gap
between Adam and SGD. From a high-level view, our analysis and empirical results suggest the
need for more precise control over the training process of DNNs.

REFERENCES

Devansh Arpit, Stanisław Jastrz˛ebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxin-
der S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look
at memorization in deep networks. In International Conference on Machine Learning, 2017.

Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in

neural networks. In International Conference on Machine Learning.

Lawrence Cayton. Algorithms for manifold learning. Univ. of California at San Diego Tech. Rep,

pp. 1–17, 2005.

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and

stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011.

Xavier Gastaldi. Shake-shake regularization of 3-branch residual networks. In Workshop of Inter-

national Conference on Learning Representations, 2017.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectiﬁer neural networks.

In

International Conference on Artiﬁcial Intelligence and Statistics, pp. 315–323, 2011.

Elad Hazan, Kﬁr Levy, and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex opti-

mization. In Advances in Neural Information Processing Systems, pp. 1594–1602, 2015.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016.

Sepp Hochreiter and Jürgen Schmidhuber. Flat minima. Neural Computation, 9(1):1–42, 1997.

Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In IEEE Conference on Computer

Vision and Pattern Recognition, 2018.

Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning, pp. 448–456,
2015.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International

Conference on Learning Representations, 2015.

Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-

nical report, University of Toronto, 2009.

Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. arXiv preprint

arXiv:1711.05101, 2017a.

Ilya Loshchilov and Frank Hutter. Sgdr: stochastic gradient descent with restarts. In International

Conference on Learning Representations, 2017b.

Hariharan Narayanan and Sanjoy Mitter. Sample complexity of testing the manifold hypothesis. In

Advances in Neural Information Processing Systems, pp. 1786–1794, 2010.

Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized opti-
mization in deep neural networks. In Advances in Neural Information Processing Systems, pp.
2422–2430, 2015.

Anders Oland, Aayush Bansal, Roger B Dannenberg, and Bhiksha Raj. Be careful what you
arXiv preprint

backpropagate: A case for linear output activations & gradient boosting.
arXiv:1707.04199, 2017.

Salah Rifai, Yann N Dauphin, Pascal Vincent, Yoshua Bengio, and Xavier Muller. The manifold
tangent classiﬁer. In Advances in Neural Information Processing Systems, pp. 2294–2302, 2011.

Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to accel-
erate training of deep neural networks. In Advances in Neural Information Processing Systems,
pp. 901–909, 2016.

Samuel L Smith and Quoc V Le. A bayesian perspective on generalization and stochastic gradient

descent. In International Conference on Learning Representations, 2018.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–9, 2015.

Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5—RmsProp: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.

Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht. The
In Advances in Neural In-

marginal value of adaptive gradient methods in machine learning.
formation Processing Systems, 2017.

Neal Wu. A tensorﬂow implementation of wide residual networks, 2016. URL https://

github.com/tensorflow/models/tree/master/research/resnet.

Adams Wei Yu, Qihang Lin, Ruslan Salakhutdinov, and Jaime Carbonell. Normalized gradient with
adaptive stepsize method for deep neural network training. arXiv preprint arXiv:1707.04822,
2017.

Sergey Zagoruyko and Nikos Komodakis. A pytorch implementation of wide residual networks,
2016a. URL https://github.com/szagoruyko/wide-residual-networks.

Sergey Zagoruyko and Nikos Komodakis.

Wide residual networks.

arXiv preprint

arXiv:1605.07146, 2016b.

2012.

Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,

Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning Rep-
resentations, 2017.

8
1
0
2
 
p
e
S
 
8
1
 
 
]

G
L
.
s
c
[
 
 
2
v
6
4
5
4
0
.
9
0
7
1
:
v
i
X
r
a

NORMALIZED DIRECTION-PRESERVING ADAM

Zijun Zhang
Department of Computer Science
University of Calgary
zijun.zhang@ucalgary.ca

Lin Ma
School of Computer Science
Wuhan University
linmawhu@gmail.com

Zongpeng Li
Department of Computer Science
University of Calgary
zongpeng@ucalgary.ca

Chuan Wu
Department of Computer Science
The University of Hong Kong
cwu@cs.hku.hk

ABSTRACT

Adaptive optimization algorithms, such as Adam and RMSprop, have shown bet-
ter optimization performance than stochastic gradient descent (SGD) in some sce-
narios. However, recent studies show that they often lead to worse generalization
performance than SGD, especially for training deep neural networks (DNNs). In
this work, we identify the reasons that Adam generalizes worse than SGD, and
develop a variant of Adam to eliminate the generalization gap. The proposed
method, normalized direction-preserving Adam (ND-Adam), enables more pre-
cise control of the direction and step size for updating weight vectors, leading to
signiﬁcantly improved generalization performance. Following a similar rationale,
we further improve the generalization performance in classiﬁcation tasks by regu-
larizing the softmax logits. By bridging the gap between SGD and Adam, we also
hope to shed light on why certain optimization algorithms generalize better than
others.

1

INTRODUCTION

In contrast with the growing complexity of neural network architectures (Szegedy et al., 2015; He
et al., 2016; Hu et al., 2018), the training methods remain relatively simple. Most practical opti-
mization methods for deep neural networks (DNNs) are based on the stochastic gradient descent
(SGD) algorithm. However, the learning rate of SGD, as a hyperparameter, is often difﬁcult to tune,
since the magnitudes of different parameters vary widely, and adjustment is required throughout the
training process.

To tackle this problem, several adaptive variants of SGD were developed, including Adagrad (Duchi
et al., 2011), Adadelta (Zeiler, 2012), RMSprop (Tieleman & Hinton, 2012), Adam (Kingma & Ba,
2015). These algorithms aim to adapt the learning rate to different parameters automatically, based
on the statistics of gradient. Although they usually simplify learning rate settings, and lead to faster
convergence, it is observed that their generalization performance tend to be signiﬁcantly worse than
that of SGD in some scenarios (Wilson et al., 2017). This intriguing phenomenon may explain why
SGD (possibly with momentum) is still prevalent in training state-of-the-art deep models, especially
feedforward DNNs (Szegedy et al., 2015; He et al., 2016; Hu et al., 2018). Furthermore, recent
work has shown that DNNs are capable of ﬁtting noise data (Zhang et al., 2017), suggesting that
their generalization capabilities are not the mere result of DNNs themselves, but are entwined with
optimization (Arpit et al., 2017).

This work aims to bridge the gap between SGD and Adam in terms of the generalization perfor-
mance. To this end, we identify two problems that may degrade the generalization performance
of Adam, and show how these problems are (partially) avoided by using SGD with L2 weight de-
cay. First, the updates of SGD lie in the span of historical gradients, whereas it is not the case for
Adam. This difference has been discussed in rather recent literature (Wilson et al., 2017), where
the authors show that adaptive methods can ﬁnd drastically different but worse solutions than SGD.

Second, while the magnitudes of Adam parameter updates are invariant to rescaling of the gradient,
the effect of the updates on the same overall network function still varies with the magnitudes of pa-
rameters. As a result, the effective learning rates of weight vectors tend to decrease during training,
which leads to sharp local minima that do not generalize well (Hochreiter & Schmidhuber, 1997).

To address these two problems of Adam, we propose the normalized direction-preserving Adam
(ND-Adam) algorithm, which controls the update direction and step size in a more precise way. We
show that ND-Adam is able to achieve signiﬁcantly better generalization performance than vanilla
Adam, and matches that of SGD in image classiﬁcation tasks.

We summarize our contributions as follows:

•

•

•

We observe that the directions of Adam parameter updates are different from that of SGD,
i.e., Adam does not preserve the directions of gradients as SGD does. We ﬁx the problem
by adapting the learning rate to each weight vector, instead of each individual weight, such
that the direction of the gradient is preserved.
For both Adam and SGD without L2 weight decay, we observe that the magnitude of each
vector’s direction change depends on its L2-norm. We show that, using SGD with L2
weight decay implicitly normalizes the weight vectors, and thus remove the dependence
in an approximate manner. We ﬁx the problem for Adam by explicitly normalizing each
weight vector, and by optimizing only its direction, such that the effective learning rate can
be precisely controlled.
We further demonstrate that, without proper regularization, the learning signal backpropa-
gated from the softmax layer may vary with the overall magnitude of the logits in an unde-
sirable way. Based on the observation, we apply batch normalization or L2-regularization
to the logits, which further improves the generalization performance in classiﬁcation tasks.

In essence, our proposed methods, ND-Adam and regularized softmax, improve the generalization
performance of Adam by enabling more precise control over the directions of parameter updates,
the learning rates, and the learning signals.

The remainder of this paper is organized as follows. In Sec. 2, we identify two problems of Adam,
and show how SGD with L2 weight decay partially avoids these problems. In Sec. 3, we further
discuss and develop ND-Adam as a solution to the two problems. In Sec. 4, we propose regularized
softmax to improve the learning signal backpropagated from the softmax layer. We provide em-
pirical evidence for our analysis, and evaluate the performance of the proposed methods in Sec. 5.
1

2 BACKGROUND AND MOTIVATION

2.1 ADAPTIVE MOMENT ESTIMATION (ADAM)

Adaptive moment estimation (Adam) (Kingma & Ba, 2015) is a stochastic optimization method
that applies individual adaptive learning rates to different parameters, based on the estimates of the
Rn, Adam
ﬁrst and second moments of the gradients. Speciﬁcally, for n trainable parameters, θ
maintains a running average of the ﬁrst and second moments of the gradient w.r.t. each parameter as

∈

mt = β1mt−1 + (1

β1) gt,

−
β2) g2
t .

and

(1b)
−
Rn denote respectively the ﬁrst and second
Here, t denotes the time step, mt ∈
R are the corresponding decay factors. Kingma & Ba (2015)
moments, and β1 ∈
further notice that, since m0 and v0 are initialized to 0’s, they are biased towards zero during the
initial time steps, especially when the decay factors are large (i.e., close to 1). Thus, for computing
the next update, they need to be corrected as

vt = β2vt−1 + (1
Rn and vt ∈

R and β2 ∈

(1a)

(2)

1Code is available at https://github.com/zj10/ND-Adam.

ˆmt =

, ˆvt =

mt

βt
1

1

−

vt

1

−

,

βt
2

where βt

1, βt

2 are the t-th powers of β1, β2 respectively. Then, we can update each parameter as
αt
√ˆvt + (cid:15)

θt = θt−1 −

ˆmt,

(3)

where αt is the global learning rate, and (cid:15) is a small constant to avoid division by zero. Note the
above computations between vectors are element-wise.

A distinguishing merit of Adam is that the magnitudes of parameter updates are invariant to rescaling
of the gradient, as shown by the adaptive learning rate term, αt/ (cid:0)√ˆvt + (cid:15)(cid:1). However, there are two
potential problems when applying Adam to DNNs.

First, in some scenarios, DNNs trained with Adam generalize worse than that trained with stochas-
tic gradient descent (SGD) (Wilson et al., 2017). Zhang et al. (2017) demonstrate that over-
parameterized DNNs are capable of memorizing the entire dataset, no matter if it is natural data
or meaningless noise data, and thus suggest much of the generalization power of DNNs comes from
the training algorithm, e.g., SGD and its variants. It coincides with another recent work (Wilson
et al., 2017), which shows that simple SGD often yields better generalization performance than
adaptive gradient methods, such as Adam. As pointed out by the latter, the difference in the gen-
eralization performance may result from the different directions of updates. Speciﬁcally, for each
hidden unit, the SGD update of its input weight vector can only lie in the span of all possible input
vectors, which, however, is not the case for Adam due to the individually adapted learning rates. We
refer to this problem as the direction missing problem.

Second, while batch normalization (Ioffe & Szegedy, 2015) can signiﬁcantly accelerate the con-
vergence of DNNs, the input weights and the scaling factor of each hidden unit can be scaled in
inﬁnitely many (but consistent) ways, without changing the function implemented by the hidden
unit. Thus, for different magnitudes of an input weight vector, the updates given by Adam can have
different effects on the overall network function, which is undesirable. Furthermore, even when
batch normalization is not used, a network using linear rectiﬁers (e.g., ReLU, leaky ReLU) as acti-
vation functions, is still subject to ill-conditioning of the parameterization (Glorot et al., 2011), and
hence the same problem. We refer to this problem as the ill-conditioning problem.

2.2 L2 WEIGHT DECAY

L2 weight decay is a regularization technique frequently used with SGD. It often has a signiﬁcant
effect on the generalization performance of DNNs. Despite its simplicity and crucial role in the
training process, how L2 weight decay works in DNNs remains to be explained. A common jus-
tiﬁcation is that L2 weight decay can be introduced by placing a Gaussian prior upon the weights,
when the objective is to ﬁnd the maximum a posteriori (MAP) weights (Blundell et al.). How-
ever, as discussed in Sec. 2.1, the magnitudes of input weight vectors are irrelevant in terms of the
overall network function, in some common scenarios, rendering the variance of the Gaussian prior
meaningless.

We propose to view L2 weight decay in neural networks as a form of weight normalization, which
may better explain its effect on the generalization performance. Consider a neural network trained
with the following loss function:

(cid:101)L (θ;

) = L (θ;

) +

D

D

λ
2

(cid:88)

i∈N

2
2 ,

wi(cid:107)
(cid:107)

D

) is the original loss function speciﬁed by the task,

where L (θ;
is
the set of all hidden units, and wi denotes the input weights of hidden unit i, which is included in the
trainable parameters, θ. For simplicity, we consider SGD updates without momentum. Therefore,
the update of wi at each time step is

is a batch of training data,

N

D

∆wi =

∂ (cid:101)L
∂wi

α

−

=

α

−

(cid:18) ∂L
∂wi

(cid:19)

+ λwi

,

where α is the learning rate. As we can see from Eq. (5), the gradient magnitude of the L2 penalty is
proportional to
wi(cid:107)2 to an equilibrium
wi(cid:107)2, thus forms a negative feedback loop that stabilizes
(cid:107)
wi(cid:107)2 tends to increase or decrease dramatically at the beginning of
value. Empirically, we ﬁnd that
(cid:107)

(cid:107)

(4)

(5)

the training, and then varies mildly within a small range, which indicates
In practice, we usually have
0.
wi ·
Let l(cid:107)wi and l⊥wi be the vector projection and rejection of ∂L
∂wi

∆wi(cid:107)2 /
(cid:107)

wi(cid:107)2 (cid:28)
(cid:107)

∆wi ≈

wi + ∆wi(cid:107)2.
(cid:107)
1, thus ∆wi is approximately orthogonal to wi, i.e.

wi(cid:107)2 ≈ (cid:107)

on wi, which are deﬁned as

l(cid:107)wi =

(cid:18) ∂L

∂wi ·

wi
wi(cid:107)2

(cid:19) wi
wi(cid:107)2
(cid:107)

, l⊥wi =

∂L
∂wi −

l(cid:107)wi.

(cid:107)
From Eq. (5) and (6), it is easy to show

(6)

(7)

∆wi(cid:107)2
(cid:107)
wi(cid:107)2 ≈
(cid:107)

l⊥wi(cid:107)2
(cid:107)
(cid:13)
(cid:13)
(cid:13)l(cid:107)wi
(cid:13)2

αλ.

As discussed in Sec. 2.1, when batch normalization is used, or when linear rectiﬁers are used as
wi(cid:107)2 becomes irrelevant; it is the direction of wi that actually
activation functions, the magnitude of
(cid:107)
makes a difference in the overall network function. If L2 weight decay is not applied, the magnitude
wi(cid:107)2 increases during the training process, which can
of wi’s direction change will decrease as
potentially lead to overﬁtting (discussed in detail in Sec. 3.2). On the other hand, Eq. (7) shows that
L2 weight decay implicitly normalizes the weights, such that the magnitude of wi’s direction change
does not depend on
wi(cid:107)2, and can be tuned by the product of α and λ. In the following, we refer
(cid:107)
to
∆wi(cid:107)2 /

wi(cid:107)2 as the effective learning rate of wi.
(cid:107)

While L2 weight decay produces the normalization effect in an implicit and approximate way, we
will show that explicitly doing so enables more precise control of the effective learning rate.

(cid:107)

(cid:107)

3 NORMALIZED DIRECTION-PRESERVING ADAM

We ﬁrst present the normalized direction-preserving Adam (ND-Adam) algorithm, which essentially
improves the optimization of the input weights of hidden units, while employing the vanilla Adam
algorithm to update other parameters. Speciﬁcally, we divide the trainable parameters, θ, into two
. Then we update θv and θs by
sets, θv and θs, such that θv =
different rules, as described by Alg. 1. The learning rates for the two sets of parameters are denoted
by αv

, and θs =

wi|
{

∈ N }

θv

{

\

}

θ

i

t , respectively.

t and αs

In Alg. 1, computing gt (wi) and wi,t may take slightly more time compared to Adam, which how-
ever is negligible in practice. On the other hand, to estimate the second order moment of each
Rn, Adam maintains n scalars, whereas ND-Adam requires only one scalar, vt (wi), and thus
wi ∈
reduces the memory overhead of Adam.

In the following, we address the direction missing problem and the ill-conditioning problem dis-
cussed in Sec. 2.1, and explain Alg. 1 in detail. We show how the proposed algorithm jointly solves
the two problems, as well as its relation to other normalization schemes.

3.1 PRESERVING GRADIENT DIRECTIONS

Assuming the stationarity of a hidden unit’s input distribution, the SGD update (possibly with mo-
mentum) of the input weight vector is a linear combination of historical gradients, and thus can
only lie in the span of the input vectors. Consequently, the input weight vector itself will eventually
converge to the same subspace.

In contrast, the Adam algorithm adapts the global learning rate to each scalar parameter indepen-
dently, such that the gradient of each parameter is normalized by a running average of its magnitudes,
which changes the direction of the gradient. To preserve the direction of the gradient w.r.t. each input
weight vector, we generalize the learning rate adaptation scheme from scalars to vectors.

Let gt (wi), mt (wi), vt (wi) be the counterparts of gt, mt, vt for vector wi. Since Eq. (1a) is
a linear combination of historical gradients, it can be extended to vectors without any change; or
equivalently, we can rewrite it for each vector as

mt (wi) = β1mt−1 (wi) + (1

β1) gt (wi) .

−

(8)

*/

*/

*/

*/

(9)

(10)

(11)

Algorithm 1: Normalized direction-preserving Adam
/* Initialization
t
←
for i

0;

do
∈ N
wi,0(cid:107)2;
wi,0/
wi,0 ←
(cid:107)
0;
m0 (wi)
0;
v0 (wi)

←
←

/* Perform T iterations of training
while t < T do
t + 1;
t
←
/* Update θv
for i

do

−

∈ N
¯gt (wi)
gt (wi)
mt (wi)
vt (wi)
ˆmt (wi)
ˆvt (wi)
¯wi,t ←
wi,t ←

∂L/∂wi;
¯gt (wi)
(¯gt (wi)
·
β1mt−1 (wi) + (1
β2vt−1 (wi) + (1
−
βt
1);
mt (wi) / (1
−
βt
2);
vt (wi) / (1
−
t ˆmt (wi) /

←
←
←
←
←
←
αv
wi,t−1 −
¯wi,t(cid:107)2;
¯wi,t/
(cid:107)
/* Update θs using Adam
AdamUpdate (cid:0)θs
θs
t ←
return θT ;

t−1; αs

t , β1, β2

(cid:1);

wi,t−1) wi,t−1;
β1) gt (wi);
2
2;
gt (wi)
(cid:107)
(cid:107)

−
β2)

(cid:16)(cid:112)

ˆvt (wi) + (cid:15)

(cid:17)

;

We then extend Eq. (1b) as

vt (wi) = β2vt−1 (wi) + (1

β2)

gt (wi)
(cid:107)
(cid:107)

−

2
2 ,

i.e., instead of estimating the average gradient magnitude for each individual parameter, we estimate
2
2 for each vector wi. In addition, we modify Eq. (2) and (3) accordingly as
the average of

gt (wi)
(cid:107)
(cid:107)

and

ˆmt (wi) =

, ˆvt (wi) =

mt (wi)
βt
1
1

−

wi,t = wi,t−1 −

(cid:112)

αv
t
ˆvt (wi) + (cid:15)

vt (wi)
βt
1
2

−

,

ˆmt (wi) .

Here, ˆmt (wi) is a vector with the same dimension as wi, whereas ˆvt (wi) is a scalar. Therefore,
when applying Eq. (11), the direction of the update is the negative direction of ˆmt (wi), and thus is
in the span of the historical gradients of wi.

Despite the empirical success of SGD, a question remains as to why it is desirable to constrain the
input weights in the span of the input vectors. A possible explanation is related to the manifold
hypothesis, which suggests that real-world data presented in high dimensional spaces (e.g., images,
audios, text) concentrates on manifolds of much lower dimensionality (Cayton, 2005; Narayanan &
Mitter, 2010). In fact, commonly used activation functions, such as (leaky) ReLU, sigmoid, tanh,
can only be activated (not saturating or having small gradients) by a portion of the input vectors, in
whose span the input weights lie upon convergence. Assuming the local linearity of the manifolds
of data or hidden-layer representations, constraining the input weights in the subspace that contains
that portion of the input vectors, encourages the hidden units to form local coordinate systems on
the corresponding manifold, which can lead to good representations (Rifai et al., 2011).

3.2 SPHERICAL WEIGHT OPTIMIZATION

The ill-conditioning problem occurs when the magnitude change of an input weight vector can be
compensated by other parameters, such as the scaling factor of batch normalization, or the output

weight vector, without affecting the overall network function. Consequently, suppose we have two
DNNs that parameterize the same function, but with some of the input weight vectors having differ-
ent magnitudes, applying the same SGD or Adam update rule will, in general, change the network
functions in different ways. Thus, the ill-conditioning problem makes the training process inconsis-
tent and difﬁcult to control.

More importantly, when the weights are not properly regularized (e.g., without using L2 weight
decay), the magnitude of wi’s direction change will decrease as
wi(cid:107)2 increases during the training
process. As a result, the effective learning rate for wi tends to decrease faster than expected. The
gradient noise introduced by large learning rates is crucial to avoid sharp minima (Smith & Le,
2018). And it is well known that sharp minima generalize worse than ﬂat minima (Hochreiter &
Schmidhuber, 1997).

(cid:107)

As shown in Sec. 2.2, when combined with SGD, L2 weight decay can alleviate the ill-conditioning
problem by implicitly and approximately normalizing the weights. However, the approximation
fails when
wi(cid:107)2 is far from the equilibrium due to improper initialization, or drastic changes in
(cid:107)
the magnitudes of the weight vectors. In addition, due to the direction missing problem, naively
applying L2 weight decay to Adam does not yield the same effect as it does on SGD. In concurrent
work, Loshchilov & Hutter (2017a) address the problem by decoupling the weight decay and the
optimization steps taken w.r.t. the loss function. However, their experimental results indicate that
improving L2 weight decay alone cannot eliminate the generalization gap between Adam and SGD.

The ill-conditioning problem is also addressed by Neyshabur et al. (2015), by employing a geometry
invariant to rescaling of weights. However, their proposed methods do not preserve the direction of
gradient.

To address the ill-conditioning problem in a more principled way, we restrict the L2-norm of each
wi to 1, and only optimize its direction. In other words, instead of optimizing wi in a n-dimensional
1)-dimensional unit sphere. Speciﬁcally, we ﬁrst compute the raw
space, we optimize wi on a (n
gradient w.r.t. wi, ¯gt (wi) = ∂L/∂wi, and project the gradient onto the unit sphere as

−

Here,

gt (wi) = ¯gt (wi)

wi,t−1) wi,t−1.
wi,t−1(cid:107)2 = 1. Then we follow Eq. (8)-(10), and replace (11) with
ˆmt (wi) , and wi,t =

(¯gt (wi)

−

·

(cid:107)

¯wi,t = wi,t−1 −

(cid:112)

αv
t
ˆvt (wi) + (cid:15)

¯wi,t
¯wi,t(cid:107)2
(cid:107)

.

In Eq. (12), we keep only the component that is orthogonal to wi,t−1. However, ˆmt (wi) is not
wi(cid:107)2 can still
necessarily orthogonal as well; moreover, even when ˆmt (wi) is orthogonal to wi,t−1,
increase according to the Pythagorean theorem. Therefore, we explicitly normalize wi,t in Eq. (13),
to ensure
wi,t(cid:107)2 = 1 after each update. Also note that, since wi,t−1 is a linear combination of
(cid:107)
its historical gradients, gt (wi) still lies in the span of the historical gradients after the projection in
Eq. (12).

(cid:107)

Compared to SGD with L2 weight decay, spherical weight optimization explicitly normalizes the
weight vectors, such that each update to the weight vectors only changes their directions, and strictly
keeps the magnitudes constant. As a result, the effective learning rate of a weight vector is

(12)

(13)

(14)

∆wi,t(cid:107)2
(cid:107)
wi,t−1(cid:107)2 ≈
(cid:107)

ˆmt (wi)
(cid:107)2
(cid:107)
(cid:112)
ˆvt (wi)

αv
t ,

which enables precise control over the learning rate of wi through a single hyperparameter, αv
t ,
rather than two as required by Eq. (7).

Note that it is possible to control the effective learning rate more precisely, by normalizing ˆmt (wi)
(cid:107)2, instead of by (cid:112)
ˆvt (wi). However, by doing so, we lose information provided
with
ˆmt (wi)
(cid:107)
by
In addition, since ˆmt (wi) is less noisy than gt (wi),
(cid:107)2 at different time steps.
ˆmt (wi)
(cid:107)
(cid:112)
ˆvt (wi) becomes small near convergence, which is considered a desirable property
(cid:107)2 /
ˆmt (wi)
(cid:107)
of Adam (Kingma & Ba, 2015). Thus, we keep the gradient normalization scheme intact.

We note the difference between various gradient normalization schemes and the normalization
scheme employed by spherical weight optimization. As shown in Eq. (11), ND-Adam general-
izes the gradient normalization scheme of Adam, and thus both Adam and ND-Adam normalize

the gradient by a running average of its magnitude. This, and other similar schemes (Hazan et al.,
2015; Yu et al., 2017) make the optimization less susceptible to vanishing and exploding gradients.
The proposed spherical weight optimization serves a different purpose. It normalizes each weight
vector and projects the gradient onto a unit sphere, such that the effective learning rate can be con-
trolled more precisely. Moreover, it provides robustness to improper weight initialization, since the
magnitude of each weight vector is kept constant.

For nonlinear activation functions (without batch normalization), such as sigmoid and tanh, an extra
scaling factor is needed for each hidden unit to express functions that require unnormalized weight
), the activation of hidden
vectors. For instance, given an input vector x
·
unit i is then given by

Rn, and a nonlinearity φ (

(15)
where γi is the scaling factor, and bi is the bias. Consequently, normalizing weight vectors does not
limit the expressiveness of models.

x + bi) ,

∈
yi = φ (γiwi ·

3.3 RELATION TO WEIGHT NORMALIZATION AND BATCH NORMALIZATION

wi(cid:107)2 (cid:54)
(cid:107)

A related normalization and reparameterization scheme, weight normalization (Salimans & Kingma,
2016), has been developed as an alternative to batch normalization, aiming to accelerate the conver-
gence of SGD optimization. We note the difference between spherical weight optimization and
weight normalization. First, the weight vector of each hidden unit is not directly normalized in
= 1 in general. At training time, the activation of hidden unit i is
weight normalization, i.e,

(cid:18) γi
wi(cid:107)2
which is equivalent to Eq. (15) for the forward pass. For the backward pass, the effective learning
wi(cid:107)2 in weight normalization, hence it does not solve the ill-conditioning
rate still depends on
(cid:107)
problem. At inference time, both of these two schemes can merge wi and γi into a single equivalent
weight vector, w(cid:48)

yi = φ

x + bi

wi ·

(16)

i = γiwi, or w(cid:48)

wi.

(cid:19)

(cid:107)

,

i = γi
(cid:107)wi(cid:107)2

While spherical weight optimization naturally encompasses weight normalization, it can further
beneﬁt from batch normalization. When combined with batch normalization, Eq. (15) evolves into

yi = φ (γi BN (wi ·

x) + bi) ,

(17)

where BN (
) represents the transformation done by batch normalization without scaling and shift-
·
ing. Here, γi serves as the scaling factor for both the normalized weight vector and batch normal-
ization.

4 REGULARIZED SOFTMAX

For multi-class classiﬁcation tasks, the softmax function is the de facto activation function for the
output layer. Despite its simplicity and intuitive probabilistic interpretation, we observe a related
problem to the ill-conditioning problem we have addressed. Similar to how different magnitudes
of weight vectors result in different updates to the same network function, the learning signal back-
propagated from the softmax layer varies with the overall magnitude of the logits.

Speciﬁcally, when using cross entropy as the surrogate loss with one-hot target vectors, the predic-
tion is considered correct as long as arg maxc∈C (zc) is the target class, where zc is the logit before
. Thus, the logits can be positively scaled
the softmax activation, corresponding to category c
together without changing the predictions, whereas the cross entropy and its derivatives will vary
with the scaling factor. Concretely, denoting the scaling factor by η, the gradient w.r.t. each logit is
(cid:21)

∈ C

(cid:20)

∂L
∂zˆc

= η

(cid:80)

exp (ηzˆc)
c∈C exp (ηzc) −

1

, and

∂L
∂z¯c

=

(cid:80)

η exp (ηz¯c)
c∈C exp (ηzc)

,

(18)

where ˆc is the target class, and ¯c

∈ C\ {

.
ˆc
}

For Adam and ND-Adam, since the gradient w.r.t. each scalar or vector are normalized, the absolute
magnitudes of Eq. (18) are irrelevant. Instead, the relative magnitudes make a difference here. When

η is small, we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂L/∂z¯c
∂L/∂zˆc

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

lim
η→0

1

,

1

|C| −

which indicates that, when the magnitude of the logits is small, softmax encourages the logit of the
target class to increase, while equally penalizing that of the other classes, regardless of the difference
in ˆz
. However, it is more reasonable to penalize more the logits that are
ˆz
}
closer to ˆz, which are more likely to cause misclassiﬁcation.

¯z for different ¯z

∈ C\ {

−

On the other end of the spectrum, assuming no two digits are the same, we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂L/∂z¯c(cid:48)
∂L/∂zˆc

(cid:12)
(cid:12)
(cid:12)
(cid:12)

lim
η→∞

= 1, lim
η→∞

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂L/∂z¯c(cid:48)(cid:48)
∂L/∂zˆc

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= 0,

∈ C\ {

ˆc, ¯c(cid:48)
where ¯c(cid:48) = arg maxc∈C\{ˆc} (zc), and ¯c(cid:48)(cid:48)
. Eq. (20) indicates that, when the magnitude
}
of the logits is large, softmax penalizes only the largest logit of the non-target classes. In this case,
although the logit that is most likely to cause misclassiﬁcation is strongly penalized, the logits of
other non-target classes are ignored. As a result, the logits of the non-target classes tend to be similar
at convergence, ignoring the fact that some classes are closer to each other than the others. The latter
case is related to the saturation problem of softmax discussed in the literature (Oland et al., 2017),
where they focus on the problem of small absolute gradient magnitude, which nevertheless does not
affect Adam and ND-Adam.

We propose two methods to exploit the prior knowledge that the magnitude of the logits should
not be too small or too large. First, we can apply batch normalization to the logits. But instead
of setting γc’s as trainable variables, we consider them as a single hyperparameter, γC, such that
. Tuning the value of γC can lead to a better trade-off between the two extremes
γc = γC,
described by Eq. (19) and (20). We observe in practice that the optimal value of γC tends to be the
same for different optimizers or different network widths, but varies with network depth. We refer
to this method as batch-normalized softmax (BN-Softmax).

c
∀

∈ C

Alternatively, since the magnitude of the logits tends to grow larger than expected (in order to mini-
mize the cross entropy), we can apply L2-regularization to the logits by adding the following penalty
to the loss function:

(19)

(20)

(21)

LC =

λC
2

(cid:88)

c∈C

z2
c ,

where λC is a hyperparameter to be tuned. Different from BN-Softmax, λC can also be shared by
different networks of different depths.

5 EXPERIMENTS

In this section, we provide empirical evidence for the analysis in Sec. 2.2, and evaluate the perfor-
mance of ND-Adam and regularized softmax on CIFAR-10 and CIFAR-100.

5.1 THE EFFECT OF L2 WEIGHT DECAY

To empirically examine the effect of L2 weight decay, we train a wide residual network (WRN)
(Zagoruyko & Komodakis, 2016b) of 22 layers, with a width of 7.5 times that of a vanilla ResNet.
Using the notation suggested by Zagoruyko & Komodakis (2016b), we refer to this network as
WRN-22-7.5. We train the network on the CIFAR-10 dataset (Krizhevsky & Hinton, 2009), with
a small modiﬁcation to the original WRN architecture, and with a different learning rate anneal-
ing schedule. Speciﬁcally, for simplicity and slightly better performance, we replace the last
fully connected layer with a convolutional layer with 10 output feature maps.
i.e., we change
the layers after the last residual block from BN-ReLU-GlobalAvgPool-FC-Softmax to
BN-ReLU-Conv-GlobalAvgPool-Softmax. In addition, for clearer comparisons, the learn-
ing rate is annealed according to a cosine function without restart (Loshchilov & Hutter, 2017b;
Gastaldi, 2017). We train the model for 80k iterations with a batch size of 128, similar to the set-
tings used by Zagoruyko & Komodakis (Zagoruyko & Komodakis, 2016b). The experiments are
based on a TensorFlow implementation of WRN (Wu, 2016).

i and ∆wp

i , where ∆wl

As a common practice, we use SGD with a momentum of 0.9, the analysis for which is similar to
that in Sec. 2.2. Due to the linearity of derivatives and momentum, ∆wi can be decomposed as
i + ∆wp
∆wi = ∆wl
i are the components corresponding to the original loss
), and the L2 penalty term (see Eq. (4)), respectively. Fig. 1a shows the ratio between
function, L (
·
i (cid:107)2, which indicates how the tendency of ∆wl
the scalar projection of ∆wl
i to
i and
i . Note that ∆wp
wi(cid:107)2 is compensated by ∆wp
increase
i points to the negative direction of wi,
even when momentum is used, since the direction change of wi is slow. As shown in Fig. 1a, at the
beginning of the training, ∆wp
wi(cid:107)2 to its equilibrium value. During
(cid:107)
i , and ∆wp
i on ∆wp
the middle stage of the training, the projection of ∆wl
i almost cancel each other.
Then, towards the end of the training, the gradient of wi diminishes rapidly, making ∆wp
i dominant
again. Therefore, Eq. (7) holds more accurately during the middle stage of the training.

i dominants and quickly adjusts

i on ∆wp

∆wp

(cid:107)

(cid:107)

∆wi(cid:107)2 /
(cid:107)

In Fig. 1b, we show how the effective learning rate varies in different hyperparameter settings. By
wi(cid:107)2 is expected to remain the same as long as αλ stays constant, which is
Eq. (7),
(cid:107)
conﬁrmed by the fact that the curve for α0 = 0.1, λ = 0.001 overlaps with that for α0 = 0.05, λ =
0.002. However, comparing the curve for α0 = 0.1, λ = 0.001, with that for α0 = 0.1, λ =
wi(cid:107)2 does not change proportionally to αλ. On
0.0005, we can see that the value of
(cid:107)
the other hand, by using ND-Adam, we can control the value of
wi(cid:107)2 more precisely by
(cid:107)
adjusting the learning rate for weight vectors, αv. For the same training step, changes in αv lead to
approximately proportional changes in
wi(cid:107)2, as shown by the two curves corresponding
to ND-Adam in Fig. 1b.

∆wi(cid:107)2 /
(cid:107)

∆wi(cid:107)2 /
(cid:107)

∆wi(cid:107)2 /

(cid:107)

(cid:107)

(a) Scalar projection of ∆wl
(cid:107)∆wp

i (cid:107)2.

i on ∆wp

i normalized by

(b) Relative magnitudes of weight updates, or effective
learning rates.

Figure 1: An illustration of how L2 weight decay and ND-Adam control the effective learning rate.
The results are obtained from the 5th layer of the network, and other layers show similar results.

5.2 PERFORMANCE EVALUATION

To compare the generalization performance of SGD, Adam, and ND-Adam, we train the same WRN-
22-7.5 network on the CIFAR-10 and CIFAR-100 datasets. For SGD and ND-Adam, we ﬁrst tune
the hyperparameters for SGD (α0 = 0.1, λ = 0.001, momentum 0.9), then tune the initial learning
rate of ND-Adam for weight vectors to match the effective learning rate to that of SGD, i.e., αv
0 =
0.05, as shown in Fig. 1b. While L2 weight decay can greatly affect the performance of SGD, it does
not noticeably beneﬁt Adam in our experiments. For Adam and ND-Adam, β1 and β2 are set to the
default values of Adam, i.e., β1 = 0.9, β2 = 0.999. Although the learning rate of Adam is usually
set to a constant value, we observe better performance with the cosine learning rate schedule. The
initial learning rate of Adam (α0), and that of ND-Adam for scalar parameters (αs
0) are both tuned
to 0.001. We use horizontal ﬂips and random crops for data augmentation, and no dropout is used.

We ﬁrst experiment with the use of trainable scaling parameters (γi) of batch normalization. As
shown in Fig. 2, at convergence, the test accuracies of ND-Adam are signiﬁcantly improved upon
that of vanilla Adam, and matches that of SGD. Note that at the early stage of training, the test accu-
racies of Adam increase more rapidly than that of ND-Adam and SGD. However, the test accuracies
remain at a high level afterwards, which indicates that Adam tends to quickly ﬁnd and get stuck in
bad local minima that do not generalize well.

The average results of 3 runs are summarized in the ﬁrst part of Table 1. Interestingly, compared
to SGD, ND-Adam shows slightly better performance on CIFAR-10, but worse performance on
CIFAR-100. This inconsistency may be related to the problem of softmax discussed in Sec. 4, that
there is a lack of proper control over the magnitude of the logits. But overall, given comparable ef-
fective learning rates, ND-Adam and SGD show similar generalization performance. In this sense,
the effective learning rate is a more natural learning rate measure than the learning rate hyperparam-
eter.

Figure 2: Test accuracies of the same network
trained with SGD, Adam, and ND-Adam. De-
tails are shown in the ﬁrst part of Table 1.

Figure 3: Magnitudes of softmax logits in differ-
ent settings. Results of WRN-22-7.5 networks
trained on CIFAR-10.

Next, we repeat the experiments with the use of BN-Softmax. As discussed in Sec. 3.2, γi’s can be
removed from a linear rectiﬁer network, without changing the overall network function. Although
this property does not strictly hold for residual networks due to the skip connections, we observe that
when BN-Softmax is used, simply removing the scaling factors results in slightly better performance
for all three algorithms. Thus, we only report results for this setting. The scaling factor of the logits,
γC, is set to 2.5 for CIFAR-10, and 1 for CIFAR-100.

As shown in the second part of Table 1, while we obtain the best generalization performance with
ND-Adam, the improvement is most prominent for Adam, and is relatively small for SGD. This
discrepancy can be explained by comparing the magnitudes of softmax logits without regularization.
As shown in Fig. 3, the magnitude of logits corresponding to Adam is much larger than that of ND-
Adam and SGD, and therefore beneﬁts more from the regularization.

Table 1: Test error rates of WRN-22-7.5 net-
works on CIFAR-10 and CIFAR-100. Based on
a TensorFlow implementation of WRN.

CIFAR-10
Error (%)

CIFAR-100
Error (%)

BN w/ scaling factors

Method

SGD
Adam
ND-Adam

SGD
Adam
ND-Adam

BN w/o scaling factors, BN-Softmax

4.61
6.14
4.53

4.49
5.43
4.14

20.60
25.51
21.45

20.18
22.48
19.90

Table 2: Test error
rates of WRN-22-7.5
and WRN-28-10 networks on CIFAR-10 and
CIFAR-100. Based on the original implemen-
tation of WRN.

Method

SGD
ND-Adam

SGD
ND-Adam

CIFAR-10
Error (%)

CIFAR-100
Error (%)

WRN-22-7.5

WRN-28-10

3.84
3.70

3.80
3.70

19.24
19.30

18.48
18.42

While the TensorFlow implementation we use already provides an adequate test bed, we notice
that it is different from the original implementation of WRN in several aspects. For instance, they
use different nonlinearities (leaky ReLU vs. ReLU), and use different skip connections for down-
sampling (average pooling vs. strided convolution). A subtle yet important difference is that, L2-

regularization is applied not only to weight vectors, but also to the scales and biases of batch normal-
ization in the original implementation, which leads to better generalization performance. For further
comparison between SGD and ND-Adam, we reimplement ND-Adam and test its performance on a
PyTorch version of the original implementation (Zagoruyko & Komodakis, 2016a).

Due to the aforementioned differences, we use a slightly different hyperparameter setting in this
experiment. Speciﬁcally, for SGD λ is set to 5e
6 (L2-
4, while for ND-Adam λ is set to 5e
regularization for biases), and both αs
0 are set to 0.04. In this case, regularizing softmax
does not yield improved performance for SGD, since the L2-regularization applied to γi’s and the
last layer weights can serve a similar purpose. Thus, we only apply L2-regularized softmax for
ND-Adam with λC = 0.001. The average results of 3 runs are summarized in Table 2. Note that the
performance of SGD for WRN-28-10 is slightly better than that reported with the original imple-
mentation (i.e., 4.00 and 19.25), due to the modiﬁcations described in Sec. 5.1. In this experiment,
SGD and ND-Adam show almost identical generalization performance.

0 and αv

−

−

6 CONCLUSION

We introduced ND-Adam, a tailored version of Adam for training DNNs, to bridge the general-
ization gap between Adam and SGD. ND-Adam is designed to preserve the direction of gradient
for each weight vector, and produce the regularization effect of L2 weight decay in a more precise
and principled way. We further introduced regularized softmax, which limits the magnitude of soft-
max logits to provide better learning signals. Combining ND-Adam and regularized softmax, we
show through experiments signiﬁcantly improved generalization performance, eliminating the gap
between Adam and SGD. From a high-level view, our analysis and empirical results suggest the
need for more precise control over the training process of DNNs.

REFERENCES

Devansh Arpit, Stanisław Jastrz˛ebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxin-
der S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look
at memorization in deep networks. In International Conference on Machine Learning, 2017.

Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in

neural networks. In International Conference on Machine Learning.

Lawrence Cayton. Algorithms for manifold learning. Univ. of California at San Diego Tech. Rep,

pp. 1–17, 2005.

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and

stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011.

Xavier Gastaldi. Shake-shake regularization of 3-branch residual networks. In Workshop of Inter-

national Conference on Learning Representations, 2017.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectiﬁer neural networks.

In

International Conference on Artiﬁcial Intelligence and Statistics, pp. 315–323, 2011.

Elad Hazan, Kﬁr Levy, and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex opti-

mization. In Advances in Neural Information Processing Systems, pp. 1594–1602, 2015.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016.

Sepp Hochreiter and Jürgen Schmidhuber. Flat minima. Neural Computation, 9(1):1–42, 1997.

Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In IEEE Conference on Computer

Vision and Pattern Recognition, 2018.

Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning, pp. 448–456,
2015.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International

Conference on Learning Representations, 2015.

Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-

nical report, University of Toronto, 2009.

Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. arXiv preprint

arXiv:1711.05101, 2017a.

Ilya Loshchilov and Frank Hutter. Sgdr: stochastic gradient descent with restarts. In International

Conference on Learning Representations, 2017b.

Hariharan Narayanan and Sanjoy Mitter. Sample complexity of testing the manifold hypothesis. In

Advances in Neural Information Processing Systems, pp. 1786–1794, 2010.

Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized opti-
mization in deep neural networks. In Advances in Neural Information Processing Systems, pp.
2422–2430, 2015.

Anders Oland, Aayush Bansal, Roger B Dannenberg, and Bhiksha Raj. Be careful what you
arXiv preprint

backpropagate: A case for linear output activations & gradient boosting.
arXiv:1707.04199, 2017.

Salah Rifai, Yann N Dauphin, Pascal Vincent, Yoshua Bengio, and Xavier Muller. The manifold
tangent classiﬁer. In Advances in Neural Information Processing Systems, pp. 2294–2302, 2011.

Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to accel-
erate training of deep neural networks. In Advances in Neural Information Processing Systems,
pp. 901–909, 2016.

Samuel L Smith and Quoc V Le. A bayesian perspective on generalization and stochastic gradient

descent. In International Conference on Learning Representations, 2018.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–9, 2015.

Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5—RmsProp: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.

Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht. The
In Advances in Neural In-

marginal value of adaptive gradient methods in machine learning.
formation Processing Systems, 2017.

Neal Wu. A tensorﬂow implementation of wide residual networks, 2016. URL https://

github.com/tensorflow/models/tree/master/research/resnet.

Adams Wei Yu, Qihang Lin, Ruslan Salakhutdinov, and Jaime Carbonell. Normalized gradient with
adaptive stepsize method for deep neural network training. arXiv preprint arXiv:1707.04822,
2017.

Sergey Zagoruyko and Nikos Komodakis. A pytorch implementation of wide residual networks,
2016a. URL https://github.com/szagoruyko/wide-residual-networks.

Sergey Zagoruyko and Nikos Komodakis.

Wide residual networks.

arXiv preprint

arXiv:1605.07146, 2016b.

2012.

Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,

Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning Rep-
resentations, 2017.

8
1
0
2
 
p
e
S
 
8
1
 
 
]

G
L
.
s
c
[
 
 
2
v
6
4
5
4
0
.
9
0
7
1
:
v
i
X
r
a

NORMALIZED DIRECTION-PRESERVING ADAM

Zijun Zhang
Department of Computer Science
University of Calgary
zijun.zhang@ucalgary.ca

Lin Ma
School of Computer Science
Wuhan University
linmawhu@gmail.com

Zongpeng Li
Department of Computer Science
University of Calgary
zongpeng@ucalgary.ca

Chuan Wu
Department of Computer Science
The University of Hong Kong
cwu@cs.hku.hk

ABSTRACT

Adaptive optimization algorithms, such as Adam and RMSprop, have shown bet-
ter optimization performance than stochastic gradient descent (SGD) in some sce-
narios. However, recent studies show that they often lead to worse generalization
performance than SGD, especially for training deep neural networks (DNNs). In
this work, we identify the reasons that Adam generalizes worse than SGD, and
develop a variant of Adam to eliminate the generalization gap. The proposed
method, normalized direction-preserving Adam (ND-Adam), enables more pre-
cise control of the direction and step size for updating weight vectors, leading to
signiﬁcantly improved generalization performance. Following a similar rationale,
we further improve the generalization performance in classiﬁcation tasks by regu-
larizing the softmax logits. By bridging the gap between SGD and Adam, we also
hope to shed light on why certain optimization algorithms generalize better than
others.

1

INTRODUCTION

In contrast with the growing complexity of neural network architectures (Szegedy et al., 2015; He
et al., 2016; Hu et al., 2018), the training methods remain relatively simple. Most practical opti-
mization methods for deep neural networks (DNNs) are based on the stochastic gradient descent
(SGD) algorithm. However, the learning rate of SGD, as a hyperparameter, is often difﬁcult to tune,
since the magnitudes of different parameters vary widely, and adjustment is required throughout the
training process.

To tackle this problem, several adaptive variants of SGD were developed, including Adagrad (Duchi
et al., 2011), Adadelta (Zeiler, 2012), RMSprop (Tieleman & Hinton, 2012), Adam (Kingma & Ba,
2015). These algorithms aim to adapt the learning rate to different parameters automatically, based
on the statistics of gradient. Although they usually simplify learning rate settings, and lead to faster
convergence, it is observed that their generalization performance tend to be signiﬁcantly worse than
that of SGD in some scenarios (Wilson et al., 2017). This intriguing phenomenon may explain why
SGD (possibly with momentum) is still prevalent in training state-of-the-art deep models, especially
feedforward DNNs (Szegedy et al., 2015; He et al., 2016; Hu et al., 2018). Furthermore, recent
work has shown that DNNs are capable of ﬁtting noise data (Zhang et al., 2017), suggesting that
their generalization capabilities are not the mere result of DNNs themselves, but are entwined with
optimization (Arpit et al., 2017).

This work aims to bridge the gap between SGD and Adam in terms of the generalization perfor-
mance. To this end, we identify two problems that may degrade the generalization performance
of Adam, and show how these problems are (partially) avoided by using SGD with L2 weight de-
cay. First, the updates of SGD lie in the span of historical gradients, whereas it is not the case for
Adam. This difference has been discussed in rather recent literature (Wilson et al., 2017), where
the authors show that adaptive methods can ﬁnd drastically different but worse solutions than SGD.

Second, while the magnitudes of Adam parameter updates are invariant to rescaling of the gradient,
the effect of the updates on the same overall network function still varies with the magnitudes of pa-
rameters. As a result, the effective learning rates of weight vectors tend to decrease during training,
which leads to sharp local minima that do not generalize well (Hochreiter & Schmidhuber, 1997).

To address these two problems of Adam, we propose the normalized direction-preserving Adam
(ND-Adam) algorithm, which controls the update direction and step size in a more precise way. We
show that ND-Adam is able to achieve signiﬁcantly better generalization performance than vanilla
Adam, and matches that of SGD in image classiﬁcation tasks.

We summarize our contributions as follows:

•

•

•

We observe that the directions of Adam parameter updates are different from that of SGD,
i.e., Adam does not preserve the directions of gradients as SGD does. We ﬁx the problem
by adapting the learning rate to each weight vector, instead of each individual weight, such
that the direction of the gradient is preserved.
For both Adam and SGD without L2 weight decay, we observe that the magnitude of each
vector’s direction change depends on its L2-norm. We show that, using SGD with L2
weight decay implicitly normalizes the weight vectors, and thus remove the dependence
in an approximate manner. We ﬁx the problem for Adam by explicitly normalizing each
weight vector, and by optimizing only its direction, such that the effective learning rate can
be precisely controlled.
We further demonstrate that, without proper regularization, the learning signal backpropa-
gated from the softmax layer may vary with the overall magnitude of the logits in an unde-
sirable way. Based on the observation, we apply batch normalization or L2-regularization
to the logits, which further improves the generalization performance in classiﬁcation tasks.

In essence, our proposed methods, ND-Adam and regularized softmax, improve the generalization
performance of Adam by enabling more precise control over the directions of parameter updates,
the learning rates, and the learning signals.

The remainder of this paper is organized as follows. In Sec. 2, we identify two problems of Adam,
and show how SGD with L2 weight decay partially avoids these problems. In Sec. 3, we further
discuss and develop ND-Adam as a solution to the two problems. In Sec. 4, we propose regularized
softmax to improve the learning signal backpropagated from the softmax layer. We provide em-
pirical evidence for our analysis, and evaluate the performance of the proposed methods in Sec. 5.
1

2 BACKGROUND AND MOTIVATION

2.1 ADAPTIVE MOMENT ESTIMATION (ADAM)

Adaptive moment estimation (Adam) (Kingma & Ba, 2015) is a stochastic optimization method
that applies individual adaptive learning rates to different parameters, based on the estimates of the
Rn, Adam
ﬁrst and second moments of the gradients. Speciﬁcally, for n trainable parameters, θ
maintains a running average of the ﬁrst and second moments of the gradient w.r.t. each parameter as

∈

mt = β1mt−1 + (1

β1) gt,

−
β2) g2
t .

and

(1b)
−
Rn denote respectively the ﬁrst and second
Here, t denotes the time step, mt ∈
R are the corresponding decay factors. Kingma & Ba (2015)
moments, and β1 ∈
further notice that, since m0 and v0 are initialized to 0’s, they are biased towards zero during the
initial time steps, especially when the decay factors are large (i.e., close to 1). Thus, for computing
the next update, they need to be corrected as

vt = β2vt−1 + (1
Rn and vt ∈

R and β2 ∈

(1a)

(2)

1Code is available at https://github.com/zj10/ND-Adam.

ˆmt =

, ˆvt =

mt

βt
1

1

−

vt

1

−

,

βt
2

where βt

1, βt

2 are the t-th powers of β1, β2 respectively. Then, we can update each parameter as
αt
√ˆvt + (cid:15)

θt = θt−1 −

ˆmt,

(3)

where αt is the global learning rate, and (cid:15) is a small constant to avoid division by zero. Note the
above computations between vectors are element-wise.

A distinguishing merit of Adam is that the magnitudes of parameter updates are invariant to rescaling
of the gradient, as shown by the adaptive learning rate term, αt/ (cid:0)√ˆvt + (cid:15)(cid:1). However, there are two
potential problems when applying Adam to DNNs.

First, in some scenarios, DNNs trained with Adam generalize worse than that trained with stochas-
tic gradient descent (SGD) (Wilson et al., 2017). Zhang et al. (2017) demonstrate that over-
parameterized DNNs are capable of memorizing the entire dataset, no matter if it is natural data
or meaningless noise data, and thus suggest much of the generalization power of DNNs comes from
the training algorithm, e.g., SGD and its variants. It coincides with another recent work (Wilson
et al., 2017), which shows that simple SGD often yields better generalization performance than
adaptive gradient methods, such as Adam. As pointed out by the latter, the difference in the gen-
eralization performance may result from the different directions of updates. Speciﬁcally, for each
hidden unit, the SGD update of its input weight vector can only lie in the span of all possible input
vectors, which, however, is not the case for Adam due to the individually adapted learning rates. We
refer to this problem as the direction missing problem.

Second, while batch normalization (Ioffe & Szegedy, 2015) can signiﬁcantly accelerate the con-
vergence of DNNs, the input weights and the scaling factor of each hidden unit can be scaled in
inﬁnitely many (but consistent) ways, without changing the function implemented by the hidden
unit. Thus, for different magnitudes of an input weight vector, the updates given by Adam can have
different effects on the overall network function, which is undesirable. Furthermore, even when
batch normalization is not used, a network using linear rectiﬁers (e.g., ReLU, leaky ReLU) as acti-
vation functions, is still subject to ill-conditioning of the parameterization (Glorot et al., 2011), and
hence the same problem. We refer to this problem as the ill-conditioning problem.

2.2 L2 WEIGHT DECAY

L2 weight decay is a regularization technique frequently used with SGD. It often has a signiﬁcant
effect on the generalization performance of DNNs. Despite its simplicity and crucial role in the
training process, how L2 weight decay works in DNNs remains to be explained. A common jus-
tiﬁcation is that L2 weight decay can be introduced by placing a Gaussian prior upon the weights,
when the objective is to ﬁnd the maximum a posteriori (MAP) weights (Blundell et al.). How-
ever, as discussed in Sec. 2.1, the magnitudes of input weight vectors are irrelevant in terms of the
overall network function, in some common scenarios, rendering the variance of the Gaussian prior
meaningless.

We propose to view L2 weight decay in neural networks as a form of weight normalization, which
may better explain its effect on the generalization performance. Consider a neural network trained
with the following loss function:

(cid:101)L (θ;

) = L (θ;

) +

D

D

λ
2

(cid:88)

i∈N

2
2 ,

wi(cid:107)
(cid:107)

D

) is the original loss function speciﬁed by the task,

where L (θ;
is
the set of all hidden units, and wi denotes the input weights of hidden unit i, which is included in the
trainable parameters, θ. For simplicity, we consider SGD updates without momentum. Therefore,
the update of wi at each time step is

is a batch of training data,

N

D

∆wi =

∂ (cid:101)L
∂wi

α

−

=

α

−

(cid:18) ∂L
∂wi

(cid:19)

+ λwi

,

where α is the learning rate. As we can see from Eq. (5), the gradient magnitude of the L2 penalty is
proportional to
wi(cid:107)2 to an equilibrium
wi(cid:107)2, thus forms a negative feedback loop that stabilizes
(cid:107)
wi(cid:107)2 tends to increase or decrease dramatically at the beginning of
value. Empirically, we ﬁnd that
(cid:107)

(cid:107)

(4)

(5)

the training, and then varies mildly within a small range, which indicates
In practice, we usually have
0.
wi ·
Let l(cid:107)wi and l⊥wi be the vector projection and rejection of ∂L
∂wi

∆wi(cid:107)2 /
(cid:107)

wi(cid:107)2 (cid:28)
(cid:107)

∆wi ≈

wi + ∆wi(cid:107)2.
(cid:107)
1, thus ∆wi is approximately orthogonal to wi, i.e.

wi(cid:107)2 ≈ (cid:107)

on wi, which are deﬁned as

l(cid:107)wi =

(cid:18) ∂L

∂wi ·

wi
wi(cid:107)2

(cid:19) wi
wi(cid:107)2
(cid:107)

, l⊥wi =

∂L
∂wi −

l(cid:107)wi.

(cid:107)
From Eq. (5) and (6), it is easy to show

(6)

(7)

∆wi(cid:107)2
(cid:107)
wi(cid:107)2 ≈
(cid:107)

l⊥wi(cid:107)2
(cid:107)
(cid:13)
(cid:13)
(cid:13)l(cid:107)wi
(cid:13)2

αλ.

As discussed in Sec. 2.1, when batch normalization is used, or when linear rectiﬁers are used as
wi(cid:107)2 becomes irrelevant; it is the direction of wi that actually
activation functions, the magnitude of
(cid:107)
makes a difference in the overall network function. If L2 weight decay is not applied, the magnitude
wi(cid:107)2 increases during the training process, which can
of wi’s direction change will decrease as
potentially lead to overﬁtting (discussed in detail in Sec. 3.2). On the other hand, Eq. (7) shows that
L2 weight decay implicitly normalizes the weights, such that the magnitude of wi’s direction change
does not depend on
wi(cid:107)2, and can be tuned by the product of α and λ. In the following, we refer
(cid:107)
to
∆wi(cid:107)2 /

wi(cid:107)2 as the effective learning rate of wi.
(cid:107)

While L2 weight decay produces the normalization effect in an implicit and approximate way, we
will show that explicitly doing so enables more precise control of the effective learning rate.

(cid:107)

(cid:107)

3 NORMALIZED DIRECTION-PRESERVING ADAM

We ﬁrst present the normalized direction-preserving Adam (ND-Adam) algorithm, which essentially
improves the optimization of the input weights of hidden units, while employing the vanilla Adam
algorithm to update other parameters. Speciﬁcally, we divide the trainable parameters, θ, into two
. Then we update θv and θs by
sets, θv and θs, such that θv =
different rules, as described by Alg. 1. The learning rates for the two sets of parameters are denoted
by αv

, and θs =

wi|
{

∈ N }

θv

{

\

}

θ

i

t , respectively.

t and αs

In Alg. 1, computing gt (wi) and wi,t may take slightly more time compared to Adam, which how-
ever is negligible in practice. On the other hand, to estimate the second order moment of each
Rn, Adam maintains n scalars, whereas ND-Adam requires only one scalar, vt (wi), and thus
wi ∈
reduces the memory overhead of Adam.

In the following, we address the direction missing problem and the ill-conditioning problem dis-
cussed in Sec. 2.1, and explain Alg. 1 in detail. We show how the proposed algorithm jointly solves
the two problems, as well as its relation to other normalization schemes.

3.1 PRESERVING GRADIENT DIRECTIONS

Assuming the stationarity of a hidden unit’s input distribution, the SGD update (possibly with mo-
mentum) of the input weight vector is a linear combination of historical gradients, and thus can
only lie in the span of the input vectors. Consequently, the input weight vector itself will eventually
converge to the same subspace.

In contrast, the Adam algorithm adapts the global learning rate to each scalar parameter indepen-
dently, such that the gradient of each parameter is normalized by a running average of its magnitudes,
which changes the direction of the gradient. To preserve the direction of the gradient w.r.t. each input
weight vector, we generalize the learning rate adaptation scheme from scalars to vectors.

Let gt (wi), mt (wi), vt (wi) be the counterparts of gt, mt, vt for vector wi. Since Eq. (1a) is
a linear combination of historical gradients, it can be extended to vectors without any change; or
equivalently, we can rewrite it for each vector as

mt (wi) = β1mt−1 (wi) + (1

β1) gt (wi) .

−

(8)

*/

*/

*/

*/

(9)

(10)

(11)

Algorithm 1: Normalized direction-preserving Adam
/* Initialization
t
←
for i

0;

do
∈ N
wi,0(cid:107)2;
wi,0/
wi,0 ←
(cid:107)
0;
m0 (wi)
0;
v0 (wi)

←
←

/* Perform T iterations of training
while t < T do
t + 1;
t
←
/* Update θv
for i

do

−

∈ N
¯gt (wi)
gt (wi)
mt (wi)
vt (wi)
ˆmt (wi)
ˆvt (wi)
¯wi,t ←
wi,t ←

∂L/∂wi;
¯gt (wi)
(¯gt (wi)
·
β1mt−1 (wi) + (1
β2vt−1 (wi) + (1
−
βt
1);
mt (wi) / (1
−
βt
2);
vt (wi) / (1
−
t ˆmt (wi) /

←
←
←
←
←
←
αv
wi,t−1 −
¯wi,t(cid:107)2;
¯wi,t/
(cid:107)
/* Update θs using Adam
AdamUpdate (cid:0)θs
θs
t ←
return θT ;

t−1; αs

t , β1, β2

(cid:1);

wi,t−1) wi,t−1;
β1) gt (wi);
2
2;
gt (wi)
(cid:107)
(cid:107)

−
β2)

(cid:16)(cid:112)

ˆvt (wi) + (cid:15)

(cid:17)

;

We then extend Eq. (1b) as

vt (wi) = β2vt−1 (wi) + (1

β2)

gt (wi)
(cid:107)
(cid:107)

−

2
2 ,

i.e., instead of estimating the average gradient magnitude for each individual parameter, we estimate
2
2 for each vector wi. In addition, we modify Eq. (2) and (3) accordingly as
the average of

gt (wi)
(cid:107)
(cid:107)

and

ˆmt (wi) =

, ˆvt (wi) =

mt (wi)
βt
1
1

−

wi,t = wi,t−1 −

(cid:112)

αv
t
ˆvt (wi) + (cid:15)

vt (wi)
βt
1
2

−

,

ˆmt (wi) .

Here, ˆmt (wi) is a vector with the same dimension as wi, whereas ˆvt (wi) is a scalar. Therefore,
when applying Eq. (11), the direction of the update is the negative direction of ˆmt (wi), and thus is
in the span of the historical gradients of wi.

Despite the empirical success of SGD, a question remains as to why it is desirable to constrain the
input weights in the span of the input vectors. A possible explanation is related to the manifold
hypothesis, which suggests that real-world data presented in high dimensional spaces (e.g., images,
audios, text) concentrates on manifolds of much lower dimensionality (Cayton, 2005; Narayanan &
Mitter, 2010). In fact, commonly used activation functions, such as (leaky) ReLU, sigmoid, tanh,
can only be activated (not saturating or having small gradients) by a portion of the input vectors, in
whose span the input weights lie upon convergence. Assuming the local linearity of the manifolds
of data or hidden-layer representations, constraining the input weights in the subspace that contains
that portion of the input vectors, encourages the hidden units to form local coordinate systems on
the corresponding manifold, which can lead to good representations (Rifai et al., 2011).

3.2 SPHERICAL WEIGHT OPTIMIZATION

The ill-conditioning problem occurs when the magnitude change of an input weight vector can be
compensated by other parameters, such as the scaling factor of batch normalization, or the output

weight vector, without affecting the overall network function. Consequently, suppose we have two
DNNs that parameterize the same function, but with some of the input weight vectors having differ-
ent magnitudes, applying the same SGD or Adam update rule will, in general, change the network
functions in different ways. Thus, the ill-conditioning problem makes the training process inconsis-
tent and difﬁcult to control.

More importantly, when the weights are not properly regularized (e.g., without using L2 weight
decay), the magnitude of wi’s direction change will decrease as
wi(cid:107)2 increases during the training
process. As a result, the effective learning rate for wi tends to decrease faster than expected. The
gradient noise introduced by large learning rates is crucial to avoid sharp minima (Smith & Le,
2018). And it is well known that sharp minima generalize worse than ﬂat minima (Hochreiter &
Schmidhuber, 1997).

(cid:107)

As shown in Sec. 2.2, when combined with SGD, L2 weight decay can alleviate the ill-conditioning
problem by implicitly and approximately normalizing the weights. However, the approximation
fails when
wi(cid:107)2 is far from the equilibrium due to improper initialization, or drastic changes in
(cid:107)
the magnitudes of the weight vectors. In addition, due to the direction missing problem, naively
applying L2 weight decay to Adam does not yield the same effect as it does on SGD. In concurrent
work, Loshchilov & Hutter (2017a) address the problem by decoupling the weight decay and the
optimization steps taken w.r.t. the loss function. However, their experimental results indicate that
improving L2 weight decay alone cannot eliminate the generalization gap between Adam and SGD.

The ill-conditioning problem is also addressed by Neyshabur et al. (2015), by employing a geometry
invariant to rescaling of weights. However, their proposed methods do not preserve the direction of
gradient.

To address the ill-conditioning problem in a more principled way, we restrict the L2-norm of each
wi to 1, and only optimize its direction. In other words, instead of optimizing wi in a n-dimensional
1)-dimensional unit sphere. Speciﬁcally, we ﬁrst compute the raw
space, we optimize wi on a (n
gradient w.r.t. wi, ¯gt (wi) = ∂L/∂wi, and project the gradient onto the unit sphere as

−

Here,

gt (wi) = ¯gt (wi)

wi,t−1) wi,t−1.
wi,t−1(cid:107)2 = 1. Then we follow Eq. (8)-(10), and replace (11) with
ˆmt (wi) , and wi,t =

(¯gt (wi)

−

·

(cid:107)

¯wi,t = wi,t−1 −

(cid:112)

αv
t
ˆvt (wi) + (cid:15)

¯wi,t
¯wi,t(cid:107)2
(cid:107)

.

In Eq. (12), we keep only the component that is orthogonal to wi,t−1. However, ˆmt (wi) is not
wi(cid:107)2 can still
necessarily orthogonal as well; moreover, even when ˆmt (wi) is orthogonal to wi,t−1,
increase according to the Pythagorean theorem. Therefore, we explicitly normalize wi,t in Eq. (13),
to ensure
wi,t(cid:107)2 = 1 after each update. Also note that, since wi,t−1 is a linear combination of
(cid:107)
its historical gradients, gt (wi) still lies in the span of the historical gradients after the projection in
Eq. (12).

(cid:107)

Compared to SGD with L2 weight decay, spherical weight optimization explicitly normalizes the
weight vectors, such that each update to the weight vectors only changes their directions, and strictly
keeps the magnitudes constant. As a result, the effective learning rate of a weight vector is

(12)

(13)

(14)

∆wi,t(cid:107)2
(cid:107)
wi,t−1(cid:107)2 ≈
(cid:107)

ˆmt (wi)
(cid:107)2
(cid:107)
(cid:112)
ˆvt (wi)

αv
t ,

which enables precise control over the learning rate of wi through a single hyperparameter, αv
t ,
rather than two as required by Eq. (7).

Note that it is possible to control the effective learning rate more precisely, by normalizing ˆmt (wi)
(cid:107)2, instead of by (cid:112)
ˆvt (wi). However, by doing so, we lose information provided
with
ˆmt (wi)
(cid:107)
by
In addition, since ˆmt (wi) is less noisy than gt (wi),
(cid:107)2 at different time steps.
ˆmt (wi)
(cid:107)
(cid:112)
ˆvt (wi) becomes small near convergence, which is considered a desirable property
(cid:107)2 /
ˆmt (wi)
(cid:107)
of Adam (Kingma & Ba, 2015). Thus, we keep the gradient normalization scheme intact.

We note the difference between various gradient normalization schemes and the normalization
scheme employed by spherical weight optimization. As shown in Eq. (11), ND-Adam general-
izes the gradient normalization scheme of Adam, and thus both Adam and ND-Adam normalize

the gradient by a running average of its magnitude. This, and other similar schemes (Hazan et al.,
2015; Yu et al., 2017) make the optimization less susceptible to vanishing and exploding gradients.
The proposed spherical weight optimization serves a different purpose. It normalizes each weight
vector and projects the gradient onto a unit sphere, such that the effective learning rate can be con-
trolled more precisely. Moreover, it provides robustness to improper weight initialization, since the
magnitude of each weight vector is kept constant.

For nonlinear activation functions (without batch normalization), such as sigmoid and tanh, an extra
scaling factor is needed for each hidden unit to express functions that require unnormalized weight
), the activation of hidden
vectors. For instance, given an input vector x
·
unit i is then given by

Rn, and a nonlinearity φ (

(15)
where γi is the scaling factor, and bi is the bias. Consequently, normalizing weight vectors does not
limit the expressiveness of models.

x + bi) ,

∈
yi = φ (γiwi ·

3.3 RELATION TO WEIGHT NORMALIZATION AND BATCH NORMALIZATION

wi(cid:107)2 (cid:54)
(cid:107)

A related normalization and reparameterization scheme, weight normalization (Salimans & Kingma,
2016), has been developed as an alternative to batch normalization, aiming to accelerate the conver-
gence of SGD optimization. We note the difference between spherical weight optimization and
weight normalization. First, the weight vector of each hidden unit is not directly normalized in
= 1 in general. At training time, the activation of hidden unit i is
weight normalization, i.e,

(cid:18) γi
wi(cid:107)2
which is equivalent to Eq. (15) for the forward pass. For the backward pass, the effective learning
wi(cid:107)2 in weight normalization, hence it does not solve the ill-conditioning
rate still depends on
(cid:107)
problem. At inference time, both of these two schemes can merge wi and γi into a single equivalent
weight vector, w(cid:48)

yi = φ

x + bi

wi ·

(16)

i = γiwi, or w(cid:48)

wi.

(cid:19)

(cid:107)

,

i = γi
(cid:107)wi(cid:107)2

While spherical weight optimization naturally encompasses weight normalization, it can further
beneﬁt from batch normalization. When combined with batch normalization, Eq. (15) evolves into

yi = φ (γi BN (wi ·

x) + bi) ,

(17)

where BN (
) represents the transformation done by batch normalization without scaling and shift-
·
ing. Here, γi serves as the scaling factor for both the normalized weight vector and batch normal-
ization.

4 REGULARIZED SOFTMAX

For multi-class classiﬁcation tasks, the softmax function is the de facto activation function for the
output layer. Despite its simplicity and intuitive probabilistic interpretation, we observe a related
problem to the ill-conditioning problem we have addressed. Similar to how different magnitudes
of weight vectors result in different updates to the same network function, the learning signal back-
propagated from the softmax layer varies with the overall magnitude of the logits.

Speciﬁcally, when using cross entropy as the surrogate loss with one-hot target vectors, the predic-
tion is considered correct as long as arg maxc∈C (zc) is the target class, where zc is the logit before
. Thus, the logits can be positively scaled
the softmax activation, corresponding to category c
together without changing the predictions, whereas the cross entropy and its derivatives will vary
with the scaling factor. Concretely, denoting the scaling factor by η, the gradient w.r.t. each logit is
(cid:21)

∈ C

(cid:20)

∂L
∂zˆc

= η

(cid:80)

exp (ηzˆc)
c∈C exp (ηzc) −

1

, and

∂L
∂z¯c

=

(cid:80)

η exp (ηz¯c)
c∈C exp (ηzc)

,

(18)

where ˆc is the target class, and ¯c

∈ C\ {

.
ˆc
}

For Adam and ND-Adam, since the gradient w.r.t. each scalar or vector are normalized, the absolute
magnitudes of Eq. (18) are irrelevant. Instead, the relative magnitudes make a difference here. When

η is small, we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂L/∂z¯c
∂L/∂zˆc

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

lim
η→0

1

,

1

|C| −

which indicates that, when the magnitude of the logits is small, softmax encourages the logit of the
target class to increase, while equally penalizing that of the other classes, regardless of the difference
in ˆz
. However, it is more reasonable to penalize more the logits that are
ˆz
}
closer to ˆz, which are more likely to cause misclassiﬁcation.

¯z for different ¯z

∈ C\ {

−

On the other end of the spectrum, assuming no two digits are the same, we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂L/∂z¯c(cid:48)
∂L/∂zˆc

(cid:12)
(cid:12)
(cid:12)
(cid:12)

lim
η→∞

= 1, lim
η→∞

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂L/∂z¯c(cid:48)(cid:48)
∂L/∂zˆc

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= 0,

∈ C\ {

ˆc, ¯c(cid:48)
where ¯c(cid:48) = arg maxc∈C\{ˆc} (zc), and ¯c(cid:48)(cid:48)
. Eq. (20) indicates that, when the magnitude
}
of the logits is large, softmax penalizes only the largest logit of the non-target classes. In this case,
although the logit that is most likely to cause misclassiﬁcation is strongly penalized, the logits of
other non-target classes are ignored. As a result, the logits of the non-target classes tend to be similar
at convergence, ignoring the fact that some classes are closer to each other than the others. The latter
case is related to the saturation problem of softmax discussed in the literature (Oland et al., 2017),
where they focus on the problem of small absolute gradient magnitude, which nevertheless does not
affect Adam and ND-Adam.

We propose two methods to exploit the prior knowledge that the magnitude of the logits should
not be too small or too large. First, we can apply batch normalization to the logits. But instead
of setting γc’s as trainable variables, we consider them as a single hyperparameter, γC, such that
. Tuning the value of γC can lead to a better trade-off between the two extremes
γc = γC,
described by Eq. (19) and (20). We observe in practice that the optimal value of γC tends to be the
same for different optimizers or different network widths, but varies with network depth. We refer
to this method as batch-normalized softmax (BN-Softmax).

c
∀

∈ C

Alternatively, since the magnitude of the logits tends to grow larger than expected (in order to mini-
mize the cross entropy), we can apply L2-regularization to the logits by adding the following penalty
to the loss function:

(19)

(20)

(21)

LC =

λC
2

(cid:88)

c∈C

z2
c ,

where λC is a hyperparameter to be tuned. Different from BN-Softmax, λC can also be shared by
different networks of different depths.

5 EXPERIMENTS

In this section, we provide empirical evidence for the analysis in Sec. 2.2, and evaluate the perfor-
mance of ND-Adam and regularized softmax on CIFAR-10 and CIFAR-100.

5.1 THE EFFECT OF L2 WEIGHT DECAY

To empirically examine the effect of L2 weight decay, we train a wide residual network (WRN)
(Zagoruyko & Komodakis, 2016b) of 22 layers, with a width of 7.5 times that of a vanilla ResNet.
Using the notation suggested by Zagoruyko & Komodakis (2016b), we refer to this network as
WRN-22-7.5. We train the network on the CIFAR-10 dataset (Krizhevsky & Hinton, 2009), with
a small modiﬁcation to the original WRN architecture, and with a different learning rate anneal-
ing schedule. Speciﬁcally, for simplicity and slightly better performance, we replace the last
fully connected layer with a convolutional layer with 10 output feature maps.
i.e., we change
the layers after the last residual block from BN-ReLU-GlobalAvgPool-FC-Softmax to
BN-ReLU-Conv-GlobalAvgPool-Softmax. In addition, for clearer comparisons, the learn-
ing rate is annealed according to a cosine function without restart (Loshchilov & Hutter, 2017b;
Gastaldi, 2017). We train the model for 80k iterations with a batch size of 128, similar to the set-
tings used by Zagoruyko & Komodakis (Zagoruyko & Komodakis, 2016b). The experiments are
based on a TensorFlow implementation of WRN (Wu, 2016).

i and ∆wp

i , where ∆wl

As a common practice, we use SGD with a momentum of 0.9, the analysis for which is similar to
that in Sec. 2.2. Due to the linearity of derivatives and momentum, ∆wi can be decomposed as
i + ∆wp
∆wi = ∆wl
i are the components corresponding to the original loss
), and the L2 penalty term (see Eq. (4)), respectively. Fig. 1a shows the ratio between
function, L (
·
i (cid:107)2, which indicates how the tendency of ∆wl
the scalar projection of ∆wl
i to
i and
i . Note that ∆wp
wi(cid:107)2 is compensated by ∆wp
increase
i points to the negative direction of wi,
even when momentum is used, since the direction change of wi is slow. As shown in Fig. 1a, at the
beginning of the training, ∆wp
wi(cid:107)2 to its equilibrium value. During
(cid:107)
i , and ∆wp
i on ∆wp
the middle stage of the training, the projection of ∆wl
i almost cancel each other.
Then, towards the end of the training, the gradient of wi diminishes rapidly, making ∆wp
i dominant
again. Therefore, Eq. (7) holds more accurately during the middle stage of the training.

i dominants and quickly adjusts

i on ∆wp

∆wp

(cid:107)

(cid:107)

∆wi(cid:107)2 /
(cid:107)

In Fig. 1b, we show how the effective learning rate varies in different hyperparameter settings. By
wi(cid:107)2 is expected to remain the same as long as αλ stays constant, which is
Eq. (7),
(cid:107)
conﬁrmed by the fact that the curve for α0 = 0.1, λ = 0.001 overlaps with that for α0 = 0.05, λ =
0.002. However, comparing the curve for α0 = 0.1, λ = 0.001, with that for α0 = 0.1, λ =
wi(cid:107)2 does not change proportionally to αλ. On
0.0005, we can see that the value of
(cid:107)
the other hand, by using ND-Adam, we can control the value of
wi(cid:107)2 more precisely by
(cid:107)
adjusting the learning rate for weight vectors, αv. For the same training step, changes in αv lead to
approximately proportional changes in
wi(cid:107)2, as shown by the two curves corresponding
to ND-Adam in Fig. 1b.

∆wi(cid:107)2 /
(cid:107)

∆wi(cid:107)2 /
(cid:107)

∆wi(cid:107)2 /

(cid:107)

(cid:107)

(a) Scalar projection of ∆wl
(cid:107)∆wp

i (cid:107)2.

i on ∆wp

i normalized by

(b) Relative magnitudes of weight updates, or effective
learning rates.

Figure 1: An illustration of how L2 weight decay and ND-Adam control the effective learning rate.
The results are obtained from the 5th layer of the network, and other layers show similar results.

5.2 PERFORMANCE EVALUATION

To compare the generalization performance of SGD, Adam, and ND-Adam, we train the same WRN-
22-7.5 network on the CIFAR-10 and CIFAR-100 datasets. For SGD and ND-Adam, we ﬁrst tune
the hyperparameters for SGD (α0 = 0.1, λ = 0.001, momentum 0.9), then tune the initial learning
rate of ND-Adam for weight vectors to match the effective learning rate to that of SGD, i.e., αv
0 =
0.05, as shown in Fig. 1b. While L2 weight decay can greatly affect the performance of SGD, it does
not noticeably beneﬁt Adam in our experiments. For Adam and ND-Adam, β1 and β2 are set to the
default values of Adam, i.e., β1 = 0.9, β2 = 0.999. Although the learning rate of Adam is usually
set to a constant value, we observe better performance with the cosine learning rate schedule. The
initial learning rate of Adam (α0), and that of ND-Adam for scalar parameters (αs
0) are both tuned
to 0.001. We use horizontal ﬂips and random crops for data augmentation, and no dropout is used.

We ﬁrst experiment with the use of trainable scaling parameters (γi) of batch normalization. As
shown in Fig. 2, at convergence, the test accuracies of ND-Adam are signiﬁcantly improved upon
that of vanilla Adam, and matches that of SGD. Note that at the early stage of training, the test accu-
racies of Adam increase more rapidly than that of ND-Adam and SGD. However, the test accuracies
remain at a high level afterwards, which indicates that Adam tends to quickly ﬁnd and get stuck in
bad local minima that do not generalize well.

The average results of 3 runs are summarized in the ﬁrst part of Table 1. Interestingly, compared
to SGD, ND-Adam shows slightly better performance on CIFAR-10, but worse performance on
CIFAR-100. This inconsistency may be related to the problem of softmax discussed in Sec. 4, that
there is a lack of proper control over the magnitude of the logits. But overall, given comparable ef-
fective learning rates, ND-Adam and SGD show similar generalization performance. In this sense,
the effective learning rate is a more natural learning rate measure than the learning rate hyperparam-
eter.

Figure 2: Test accuracies of the same network
trained with SGD, Adam, and ND-Adam. De-
tails are shown in the ﬁrst part of Table 1.

Figure 3: Magnitudes of softmax logits in differ-
ent settings. Results of WRN-22-7.5 networks
trained on CIFAR-10.

Next, we repeat the experiments with the use of BN-Softmax. As discussed in Sec. 3.2, γi’s can be
removed from a linear rectiﬁer network, without changing the overall network function. Although
this property does not strictly hold for residual networks due to the skip connections, we observe that
when BN-Softmax is used, simply removing the scaling factors results in slightly better performance
for all three algorithms. Thus, we only report results for this setting. The scaling factor of the logits,
γC, is set to 2.5 for CIFAR-10, and 1 for CIFAR-100.

As shown in the second part of Table 1, while we obtain the best generalization performance with
ND-Adam, the improvement is most prominent for Adam, and is relatively small for SGD. This
discrepancy can be explained by comparing the magnitudes of softmax logits without regularization.
As shown in Fig. 3, the magnitude of logits corresponding to Adam is much larger than that of ND-
Adam and SGD, and therefore beneﬁts more from the regularization.

Table 1: Test error rates of WRN-22-7.5 net-
works on CIFAR-10 and CIFAR-100. Based on
a TensorFlow implementation of WRN.

CIFAR-10
Error (%)

CIFAR-100
Error (%)

BN w/ scaling factors

Method

SGD
Adam
ND-Adam

SGD
Adam
ND-Adam

BN w/o scaling factors, BN-Softmax

4.61
6.14
4.53

4.49
5.43
4.14

20.60
25.51
21.45

20.18
22.48
19.90

Table 2: Test error
rates of WRN-22-7.5
and WRN-28-10 networks on CIFAR-10 and
CIFAR-100. Based on the original implemen-
tation of WRN.

Method

SGD
ND-Adam

SGD
ND-Adam

CIFAR-10
Error (%)

CIFAR-100
Error (%)

WRN-22-7.5

WRN-28-10

3.84
3.70

3.80
3.70

19.24
19.30

18.48
18.42

While the TensorFlow implementation we use already provides an adequate test bed, we notice
that it is different from the original implementation of WRN in several aspects. For instance, they
use different nonlinearities (leaky ReLU vs. ReLU), and use different skip connections for down-
sampling (average pooling vs. strided convolution). A subtle yet important difference is that, L2-

regularization is applied not only to weight vectors, but also to the scales and biases of batch normal-
ization in the original implementation, which leads to better generalization performance. For further
comparison between SGD and ND-Adam, we reimplement ND-Adam and test its performance on a
PyTorch version of the original implementation (Zagoruyko & Komodakis, 2016a).

Due to the aforementioned differences, we use a slightly different hyperparameter setting in this
experiment. Speciﬁcally, for SGD λ is set to 5e
6 (L2-
4, while for ND-Adam λ is set to 5e
regularization for biases), and both αs
0 are set to 0.04. In this case, regularizing softmax
does not yield improved performance for SGD, since the L2-regularization applied to γi’s and the
last layer weights can serve a similar purpose. Thus, we only apply L2-regularized softmax for
ND-Adam with λC = 0.001. The average results of 3 runs are summarized in Table 2. Note that the
performance of SGD for WRN-28-10 is slightly better than that reported with the original imple-
mentation (i.e., 4.00 and 19.25), due to the modiﬁcations described in Sec. 5.1. In this experiment,
SGD and ND-Adam show almost identical generalization performance.

0 and αv

−

−

6 CONCLUSION

We introduced ND-Adam, a tailored version of Adam for training DNNs, to bridge the general-
ization gap between Adam and SGD. ND-Adam is designed to preserve the direction of gradient
for each weight vector, and produce the regularization effect of L2 weight decay in a more precise
and principled way. We further introduced regularized softmax, which limits the magnitude of soft-
max logits to provide better learning signals. Combining ND-Adam and regularized softmax, we
show through experiments signiﬁcantly improved generalization performance, eliminating the gap
between Adam and SGD. From a high-level view, our analysis and empirical results suggest the
need for more precise control over the training process of DNNs.

REFERENCES

Devansh Arpit, Stanisław Jastrz˛ebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxin-
der S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look
at memorization in deep networks. In International Conference on Machine Learning, 2017.

Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in

neural networks. In International Conference on Machine Learning.

Lawrence Cayton. Algorithms for manifold learning. Univ. of California at San Diego Tech. Rep,

pp. 1–17, 2005.

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and

stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011.

Xavier Gastaldi. Shake-shake regularization of 3-branch residual networks. In Workshop of Inter-

national Conference on Learning Representations, 2017.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectiﬁer neural networks.

In

International Conference on Artiﬁcial Intelligence and Statistics, pp. 315–323, 2011.

Elad Hazan, Kﬁr Levy, and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex opti-

mization. In Advances in Neural Information Processing Systems, pp. 1594–1602, 2015.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016.

Sepp Hochreiter and Jürgen Schmidhuber. Flat minima. Neural Computation, 9(1):1–42, 1997.

Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In IEEE Conference on Computer

Vision and Pattern Recognition, 2018.

Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning, pp. 448–456,
2015.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International

Conference on Learning Representations, 2015.

Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-

nical report, University of Toronto, 2009.

Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. arXiv preprint

arXiv:1711.05101, 2017a.

Ilya Loshchilov and Frank Hutter. Sgdr: stochastic gradient descent with restarts. In International

Conference on Learning Representations, 2017b.

Hariharan Narayanan and Sanjoy Mitter. Sample complexity of testing the manifold hypothesis. In

Advances in Neural Information Processing Systems, pp. 1786–1794, 2010.

Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized opti-
mization in deep neural networks. In Advances in Neural Information Processing Systems, pp.
2422–2430, 2015.

Anders Oland, Aayush Bansal, Roger B Dannenberg, and Bhiksha Raj. Be careful what you
arXiv preprint

backpropagate: A case for linear output activations & gradient boosting.
arXiv:1707.04199, 2017.

Salah Rifai, Yann N Dauphin, Pascal Vincent, Yoshua Bengio, and Xavier Muller. The manifold
tangent classiﬁer. In Advances in Neural Information Processing Systems, pp. 2294–2302, 2011.

Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to accel-
erate training of deep neural networks. In Advances in Neural Information Processing Systems,
pp. 901–909, 2016.

Samuel L Smith and Quoc V Le. A bayesian perspective on generalization and stochastic gradient

descent. In International Conference on Learning Representations, 2018.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–9, 2015.

Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5—RmsProp: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.

Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht. The
In Advances in Neural In-

marginal value of adaptive gradient methods in machine learning.
formation Processing Systems, 2017.

Neal Wu. A tensorﬂow implementation of wide residual networks, 2016. URL https://

github.com/tensorflow/models/tree/master/research/resnet.

Adams Wei Yu, Qihang Lin, Ruslan Salakhutdinov, and Jaime Carbonell. Normalized gradient with
adaptive stepsize method for deep neural network training. arXiv preprint arXiv:1707.04822,
2017.

Sergey Zagoruyko and Nikos Komodakis. A pytorch implementation of wide residual networks,
2016a. URL https://github.com/szagoruyko/wide-residual-networks.

Sergey Zagoruyko and Nikos Komodakis.

Wide residual networks.

arXiv preprint

arXiv:1605.07146, 2016b.

2012.

Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,

Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning Rep-
resentations, 2017.

8
1
0
2
 
p
e
S
 
8
1
 
 
]

G
L
.
s
c
[
 
 
2
v
6
4
5
4
0
.
9
0
7
1
:
v
i
X
r
a

NORMALIZED DIRECTION-PRESERVING ADAM

Zijun Zhang
Department of Computer Science
University of Calgary
zijun.zhang@ucalgary.ca

Lin Ma
School of Computer Science
Wuhan University
linmawhu@gmail.com

Zongpeng Li
Department of Computer Science
University of Calgary
zongpeng@ucalgary.ca

Chuan Wu
Department of Computer Science
The University of Hong Kong
cwu@cs.hku.hk

ABSTRACT

Adaptive optimization algorithms, such as Adam and RMSprop, have shown bet-
ter optimization performance than stochastic gradient descent (SGD) in some sce-
narios. However, recent studies show that they often lead to worse generalization
performance than SGD, especially for training deep neural networks (DNNs). In
this work, we identify the reasons that Adam generalizes worse than SGD, and
develop a variant of Adam to eliminate the generalization gap. The proposed
method, normalized direction-preserving Adam (ND-Adam), enables more pre-
cise control of the direction and step size for updating weight vectors, leading to
signiﬁcantly improved generalization performance. Following a similar rationale,
we further improve the generalization performance in classiﬁcation tasks by regu-
larizing the softmax logits. By bridging the gap between SGD and Adam, we also
hope to shed light on why certain optimization algorithms generalize better than
others.

1

INTRODUCTION

In contrast with the growing complexity of neural network architectures (Szegedy et al., 2015; He
et al., 2016; Hu et al., 2018), the training methods remain relatively simple. Most practical opti-
mization methods for deep neural networks (DNNs) are based on the stochastic gradient descent
(SGD) algorithm. However, the learning rate of SGD, as a hyperparameter, is often difﬁcult to tune,
since the magnitudes of different parameters vary widely, and adjustment is required throughout the
training process.

To tackle this problem, several adaptive variants of SGD were developed, including Adagrad (Duchi
et al., 2011), Adadelta (Zeiler, 2012), RMSprop (Tieleman & Hinton, 2012), Adam (Kingma & Ba,
2015). These algorithms aim to adapt the learning rate to different parameters automatically, based
on the statistics of gradient. Although they usually simplify learning rate settings, and lead to faster
convergence, it is observed that their generalization performance tend to be signiﬁcantly worse than
that of SGD in some scenarios (Wilson et al., 2017). This intriguing phenomenon may explain why
SGD (possibly with momentum) is still prevalent in training state-of-the-art deep models, especially
feedforward DNNs (Szegedy et al., 2015; He et al., 2016; Hu et al., 2018). Furthermore, recent
work has shown that DNNs are capable of ﬁtting noise data (Zhang et al., 2017), suggesting that
their generalization capabilities are not the mere result of DNNs themselves, but are entwined with
optimization (Arpit et al., 2017).

This work aims to bridge the gap between SGD and Adam in terms of the generalization perfor-
mance. To this end, we identify two problems that may degrade the generalization performance
of Adam, and show how these problems are (partially) avoided by using SGD with L2 weight de-
cay. First, the updates of SGD lie in the span of historical gradients, whereas it is not the case for
Adam. This difference has been discussed in rather recent literature (Wilson et al., 2017), where
the authors show that adaptive methods can ﬁnd drastically different but worse solutions than SGD.

Second, while the magnitudes of Adam parameter updates are invariant to rescaling of the gradient,
the effect of the updates on the same overall network function still varies with the magnitudes of pa-
rameters. As a result, the effective learning rates of weight vectors tend to decrease during training,
which leads to sharp local minima that do not generalize well (Hochreiter & Schmidhuber, 1997).

To address these two problems of Adam, we propose the normalized direction-preserving Adam
(ND-Adam) algorithm, which controls the update direction and step size in a more precise way. We
show that ND-Adam is able to achieve signiﬁcantly better generalization performance than vanilla
Adam, and matches that of SGD in image classiﬁcation tasks.

We summarize our contributions as follows:

•

•

•

We observe that the directions of Adam parameter updates are different from that of SGD,
i.e., Adam does not preserve the directions of gradients as SGD does. We ﬁx the problem
by adapting the learning rate to each weight vector, instead of each individual weight, such
that the direction of the gradient is preserved.
For both Adam and SGD without L2 weight decay, we observe that the magnitude of each
vector’s direction change depends on its L2-norm. We show that, using SGD with L2
weight decay implicitly normalizes the weight vectors, and thus remove the dependence
in an approximate manner. We ﬁx the problem for Adam by explicitly normalizing each
weight vector, and by optimizing only its direction, such that the effective learning rate can
be precisely controlled.
We further demonstrate that, without proper regularization, the learning signal backpropa-
gated from the softmax layer may vary with the overall magnitude of the logits in an unde-
sirable way. Based on the observation, we apply batch normalization or L2-regularization
to the logits, which further improves the generalization performance in classiﬁcation tasks.

In essence, our proposed methods, ND-Adam and regularized softmax, improve the generalization
performance of Adam by enabling more precise control over the directions of parameter updates,
the learning rates, and the learning signals.

The remainder of this paper is organized as follows. In Sec. 2, we identify two problems of Adam,
and show how SGD with L2 weight decay partially avoids these problems. In Sec. 3, we further
discuss and develop ND-Adam as a solution to the two problems. In Sec. 4, we propose regularized
softmax to improve the learning signal backpropagated from the softmax layer. We provide em-
pirical evidence for our analysis, and evaluate the performance of the proposed methods in Sec. 5.
1

2 BACKGROUND AND MOTIVATION

2.1 ADAPTIVE MOMENT ESTIMATION (ADAM)

Adaptive moment estimation (Adam) (Kingma & Ba, 2015) is a stochastic optimization method
that applies individual adaptive learning rates to different parameters, based on the estimates of the
Rn, Adam
ﬁrst and second moments of the gradients. Speciﬁcally, for n trainable parameters, θ
maintains a running average of the ﬁrst and second moments of the gradient w.r.t. each parameter as

∈

mt = β1mt−1 + (1

β1) gt,

−
β2) g2
t .

and

(1b)
−
Rn denote respectively the ﬁrst and second
Here, t denotes the time step, mt ∈
R are the corresponding decay factors. Kingma & Ba (2015)
moments, and β1 ∈
further notice that, since m0 and v0 are initialized to 0’s, they are biased towards zero during the
initial time steps, especially when the decay factors are large (i.e., close to 1). Thus, for computing
the next update, they need to be corrected as

vt = β2vt−1 + (1
Rn and vt ∈

R and β2 ∈

(1a)

(2)

1Code is available at https://github.com/zj10/ND-Adam.

ˆmt =

, ˆvt =

mt

βt
1

1

−

vt

1

−

,

βt
2

where βt

1, βt

2 are the t-th powers of β1, β2 respectively. Then, we can update each parameter as
αt
√ˆvt + (cid:15)

θt = θt−1 −

ˆmt,

(3)

where αt is the global learning rate, and (cid:15) is a small constant to avoid division by zero. Note the
above computations between vectors are element-wise.

A distinguishing merit of Adam is that the magnitudes of parameter updates are invariant to rescaling
of the gradient, as shown by the adaptive learning rate term, αt/ (cid:0)√ˆvt + (cid:15)(cid:1). However, there are two
potential problems when applying Adam to DNNs.

First, in some scenarios, DNNs trained with Adam generalize worse than that trained with stochas-
tic gradient descent (SGD) (Wilson et al., 2017). Zhang et al. (2017) demonstrate that over-
parameterized DNNs are capable of memorizing the entire dataset, no matter if it is natural data
or meaningless noise data, and thus suggest much of the generalization power of DNNs comes from
the training algorithm, e.g., SGD and its variants. It coincides with another recent work (Wilson
et al., 2017), which shows that simple SGD often yields better generalization performance than
adaptive gradient methods, such as Adam. As pointed out by the latter, the difference in the gen-
eralization performance may result from the different directions of updates. Speciﬁcally, for each
hidden unit, the SGD update of its input weight vector can only lie in the span of all possible input
vectors, which, however, is not the case for Adam due to the individually adapted learning rates. We
refer to this problem as the direction missing problem.

Second, while batch normalization (Ioffe & Szegedy, 2015) can signiﬁcantly accelerate the con-
vergence of DNNs, the input weights and the scaling factor of each hidden unit can be scaled in
inﬁnitely many (but consistent) ways, without changing the function implemented by the hidden
unit. Thus, for different magnitudes of an input weight vector, the updates given by Adam can have
different effects on the overall network function, which is undesirable. Furthermore, even when
batch normalization is not used, a network using linear rectiﬁers (e.g., ReLU, leaky ReLU) as acti-
vation functions, is still subject to ill-conditioning of the parameterization (Glorot et al., 2011), and
hence the same problem. We refer to this problem as the ill-conditioning problem.

2.2 L2 WEIGHT DECAY

L2 weight decay is a regularization technique frequently used with SGD. It often has a signiﬁcant
effect on the generalization performance of DNNs. Despite its simplicity and crucial role in the
training process, how L2 weight decay works in DNNs remains to be explained. A common jus-
tiﬁcation is that L2 weight decay can be introduced by placing a Gaussian prior upon the weights,
when the objective is to ﬁnd the maximum a posteriori (MAP) weights (Blundell et al.). How-
ever, as discussed in Sec. 2.1, the magnitudes of input weight vectors are irrelevant in terms of the
overall network function, in some common scenarios, rendering the variance of the Gaussian prior
meaningless.

We propose to view L2 weight decay in neural networks as a form of weight normalization, which
may better explain its effect on the generalization performance. Consider a neural network trained
with the following loss function:

(cid:101)L (θ;

) = L (θ;

) +

D

D

λ
2

(cid:88)

i∈N

2
2 ,

wi(cid:107)
(cid:107)

D

) is the original loss function speciﬁed by the task,

where L (θ;
is
the set of all hidden units, and wi denotes the input weights of hidden unit i, which is included in the
trainable parameters, θ. For simplicity, we consider SGD updates without momentum. Therefore,
the update of wi at each time step is

is a batch of training data,

N

D

∆wi =

∂ (cid:101)L
∂wi

α

−

=

α

−

(cid:18) ∂L
∂wi

(cid:19)

+ λwi

,

where α is the learning rate. As we can see from Eq. (5), the gradient magnitude of the L2 penalty is
proportional to
wi(cid:107)2 to an equilibrium
wi(cid:107)2, thus forms a negative feedback loop that stabilizes
(cid:107)
wi(cid:107)2 tends to increase or decrease dramatically at the beginning of
value. Empirically, we ﬁnd that
(cid:107)

(cid:107)

(4)

(5)

the training, and then varies mildly within a small range, which indicates
In practice, we usually have
0.
wi ·
Let l(cid:107)wi and l⊥wi be the vector projection and rejection of ∂L
∂wi

∆wi(cid:107)2 /
(cid:107)

wi(cid:107)2 (cid:28)
(cid:107)

∆wi ≈

wi + ∆wi(cid:107)2.
(cid:107)
1, thus ∆wi is approximately orthogonal to wi, i.e.

wi(cid:107)2 ≈ (cid:107)

on wi, which are deﬁned as

l(cid:107)wi =

(cid:18) ∂L

∂wi ·

wi
wi(cid:107)2

(cid:19) wi
wi(cid:107)2
(cid:107)

, l⊥wi =

∂L
∂wi −

l(cid:107)wi.

(cid:107)
From Eq. (5) and (6), it is easy to show

(6)

(7)

∆wi(cid:107)2
(cid:107)
wi(cid:107)2 ≈
(cid:107)

l⊥wi(cid:107)2
(cid:107)
(cid:13)
(cid:13)
(cid:13)l(cid:107)wi
(cid:13)2

αλ.

As discussed in Sec. 2.1, when batch normalization is used, or when linear rectiﬁers are used as
wi(cid:107)2 becomes irrelevant; it is the direction of wi that actually
activation functions, the magnitude of
(cid:107)
makes a difference in the overall network function. If L2 weight decay is not applied, the magnitude
wi(cid:107)2 increases during the training process, which can
of wi’s direction change will decrease as
potentially lead to overﬁtting (discussed in detail in Sec. 3.2). On the other hand, Eq. (7) shows that
L2 weight decay implicitly normalizes the weights, such that the magnitude of wi’s direction change
does not depend on
wi(cid:107)2, and can be tuned by the product of α and λ. In the following, we refer
(cid:107)
to
∆wi(cid:107)2 /

wi(cid:107)2 as the effective learning rate of wi.
(cid:107)

While L2 weight decay produces the normalization effect in an implicit and approximate way, we
will show that explicitly doing so enables more precise control of the effective learning rate.

(cid:107)

(cid:107)

3 NORMALIZED DIRECTION-PRESERVING ADAM

We ﬁrst present the normalized direction-preserving Adam (ND-Adam) algorithm, which essentially
improves the optimization of the input weights of hidden units, while employing the vanilla Adam
algorithm to update other parameters. Speciﬁcally, we divide the trainable parameters, θ, into two
. Then we update θv and θs by
sets, θv and θs, such that θv =
different rules, as described by Alg. 1. The learning rates for the two sets of parameters are denoted
by αv

, and θs =

wi|
{

∈ N }

θv

{

\

}

θ

i

t , respectively.

t and αs

In Alg. 1, computing gt (wi) and wi,t may take slightly more time compared to Adam, which how-
ever is negligible in practice. On the other hand, to estimate the second order moment of each
Rn, Adam maintains n scalars, whereas ND-Adam requires only one scalar, vt (wi), and thus
wi ∈
reduces the memory overhead of Adam.

In the following, we address the direction missing problem and the ill-conditioning problem dis-
cussed in Sec. 2.1, and explain Alg. 1 in detail. We show how the proposed algorithm jointly solves
the two problems, as well as its relation to other normalization schemes.

3.1 PRESERVING GRADIENT DIRECTIONS

Assuming the stationarity of a hidden unit’s input distribution, the SGD update (possibly with mo-
mentum) of the input weight vector is a linear combination of historical gradients, and thus can
only lie in the span of the input vectors. Consequently, the input weight vector itself will eventually
converge to the same subspace.

In contrast, the Adam algorithm adapts the global learning rate to each scalar parameter indepen-
dently, such that the gradient of each parameter is normalized by a running average of its magnitudes,
which changes the direction of the gradient. To preserve the direction of the gradient w.r.t. each input
weight vector, we generalize the learning rate adaptation scheme from scalars to vectors.

Let gt (wi), mt (wi), vt (wi) be the counterparts of gt, mt, vt for vector wi. Since Eq. (1a) is
a linear combination of historical gradients, it can be extended to vectors without any change; or
equivalently, we can rewrite it for each vector as

mt (wi) = β1mt−1 (wi) + (1

β1) gt (wi) .

−

(8)

*/

*/

*/

*/

(9)

(10)

(11)

Algorithm 1: Normalized direction-preserving Adam
/* Initialization
t
←
for i

0;

do
∈ N
wi,0(cid:107)2;
wi,0/
wi,0 ←
(cid:107)
0;
m0 (wi)
0;
v0 (wi)

←
←

/* Perform T iterations of training
while t < T do
t + 1;
t
←
/* Update θv
for i

do

−

∈ N
¯gt (wi)
gt (wi)
mt (wi)
vt (wi)
ˆmt (wi)
ˆvt (wi)
¯wi,t ←
wi,t ←

∂L/∂wi;
¯gt (wi)
(¯gt (wi)
·
β1mt−1 (wi) + (1
β2vt−1 (wi) + (1
−
βt
1);
mt (wi) / (1
−
βt
2);
vt (wi) / (1
−
t ˆmt (wi) /

←
←
←
←
←
←
αv
wi,t−1 −
¯wi,t(cid:107)2;
¯wi,t/
(cid:107)
/* Update θs using Adam
AdamUpdate (cid:0)θs
θs
t ←
return θT ;

t−1; αs

t , β1, β2

(cid:1);

wi,t−1) wi,t−1;
β1) gt (wi);
2
2;
gt (wi)
(cid:107)
(cid:107)

−
β2)

(cid:16)(cid:112)

ˆvt (wi) + (cid:15)

(cid:17)

;

We then extend Eq. (1b) as

vt (wi) = β2vt−1 (wi) + (1

β2)

gt (wi)
(cid:107)
(cid:107)

−

2
2 ,

i.e., instead of estimating the average gradient magnitude for each individual parameter, we estimate
2
2 for each vector wi. In addition, we modify Eq. (2) and (3) accordingly as
the average of

gt (wi)
(cid:107)
(cid:107)

and

ˆmt (wi) =

, ˆvt (wi) =

mt (wi)
βt
1
1

−

wi,t = wi,t−1 −

(cid:112)

αv
t
ˆvt (wi) + (cid:15)

vt (wi)
βt
1
2

−

,

ˆmt (wi) .

Here, ˆmt (wi) is a vector with the same dimension as wi, whereas ˆvt (wi) is a scalar. Therefore,
when applying Eq. (11), the direction of the update is the negative direction of ˆmt (wi), and thus is
in the span of the historical gradients of wi.

Despite the empirical success of SGD, a question remains as to why it is desirable to constrain the
input weights in the span of the input vectors. A possible explanation is related to the manifold
hypothesis, which suggests that real-world data presented in high dimensional spaces (e.g., images,
audios, text) concentrates on manifolds of much lower dimensionality (Cayton, 2005; Narayanan &
Mitter, 2010). In fact, commonly used activation functions, such as (leaky) ReLU, sigmoid, tanh,
can only be activated (not saturating or having small gradients) by a portion of the input vectors, in
whose span the input weights lie upon convergence. Assuming the local linearity of the manifolds
of data or hidden-layer representations, constraining the input weights in the subspace that contains
that portion of the input vectors, encourages the hidden units to form local coordinate systems on
the corresponding manifold, which can lead to good representations (Rifai et al., 2011).

3.2 SPHERICAL WEIGHT OPTIMIZATION

The ill-conditioning problem occurs when the magnitude change of an input weight vector can be
compensated by other parameters, such as the scaling factor of batch normalization, or the output

weight vector, without affecting the overall network function. Consequently, suppose we have two
DNNs that parameterize the same function, but with some of the input weight vectors having differ-
ent magnitudes, applying the same SGD or Adam update rule will, in general, change the network
functions in different ways. Thus, the ill-conditioning problem makes the training process inconsis-
tent and difﬁcult to control.

More importantly, when the weights are not properly regularized (e.g., without using L2 weight
decay), the magnitude of wi’s direction change will decrease as
wi(cid:107)2 increases during the training
process. As a result, the effective learning rate for wi tends to decrease faster than expected. The
gradient noise introduced by large learning rates is crucial to avoid sharp minima (Smith & Le,
2018). And it is well known that sharp minima generalize worse than ﬂat minima (Hochreiter &
Schmidhuber, 1997).

(cid:107)

As shown in Sec. 2.2, when combined with SGD, L2 weight decay can alleviate the ill-conditioning
problem by implicitly and approximately normalizing the weights. However, the approximation
fails when
wi(cid:107)2 is far from the equilibrium due to improper initialization, or drastic changes in
(cid:107)
the magnitudes of the weight vectors. In addition, due to the direction missing problem, naively
applying L2 weight decay to Adam does not yield the same effect as it does on SGD. In concurrent
work, Loshchilov & Hutter (2017a) address the problem by decoupling the weight decay and the
optimization steps taken w.r.t. the loss function. However, their experimental results indicate that
improving L2 weight decay alone cannot eliminate the generalization gap between Adam and SGD.

The ill-conditioning problem is also addressed by Neyshabur et al. (2015), by employing a geometry
invariant to rescaling of weights. However, their proposed methods do not preserve the direction of
gradient.

To address the ill-conditioning problem in a more principled way, we restrict the L2-norm of each
wi to 1, and only optimize its direction. In other words, instead of optimizing wi in a n-dimensional
1)-dimensional unit sphere. Speciﬁcally, we ﬁrst compute the raw
space, we optimize wi on a (n
gradient w.r.t. wi, ¯gt (wi) = ∂L/∂wi, and project the gradient onto the unit sphere as

−

Here,

gt (wi) = ¯gt (wi)

wi,t−1) wi,t−1.
wi,t−1(cid:107)2 = 1. Then we follow Eq. (8)-(10), and replace (11) with
ˆmt (wi) , and wi,t =

(¯gt (wi)

−

·

(cid:107)

¯wi,t = wi,t−1 −

(cid:112)

αv
t
ˆvt (wi) + (cid:15)

¯wi,t
¯wi,t(cid:107)2
(cid:107)

.

In Eq. (12), we keep only the component that is orthogonal to wi,t−1. However, ˆmt (wi) is not
wi(cid:107)2 can still
necessarily orthogonal as well; moreover, even when ˆmt (wi) is orthogonal to wi,t−1,
increase according to the Pythagorean theorem. Therefore, we explicitly normalize wi,t in Eq. (13),
to ensure
wi,t(cid:107)2 = 1 after each update. Also note that, since wi,t−1 is a linear combination of
(cid:107)
its historical gradients, gt (wi) still lies in the span of the historical gradients after the projection in
Eq. (12).

(cid:107)

Compared to SGD with L2 weight decay, spherical weight optimization explicitly normalizes the
weight vectors, such that each update to the weight vectors only changes their directions, and strictly
keeps the magnitudes constant. As a result, the effective learning rate of a weight vector is

(12)

(13)

(14)

∆wi,t(cid:107)2
(cid:107)
wi,t−1(cid:107)2 ≈
(cid:107)

ˆmt (wi)
(cid:107)2
(cid:107)
(cid:112)
ˆvt (wi)

αv
t ,

which enables precise control over the learning rate of wi through a single hyperparameter, αv
t ,
rather than two as required by Eq. (7).

Note that it is possible to control the effective learning rate more precisely, by normalizing ˆmt (wi)
(cid:107)2, instead of by (cid:112)
ˆvt (wi). However, by doing so, we lose information provided
with
ˆmt (wi)
(cid:107)
by
In addition, since ˆmt (wi) is less noisy than gt (wi),
(cid:107)2 at different time steps.
ˆmt (wi)
(cid:107)
(cid:112)
ˆvt (wi) becomes small near convergence, which is considered a desirable property
(cid:107)2 /
ˆmt (wi)
(cid:107)
of Adam (Kingma & Ba, 2015). Thus, we keep the gradient normalization scheme intact.

We note the difference between various gradient normalization schemes and the normalization
scheme employed by spherical weight optimization. As shown in Eq. (11), ND-Adam general-
izes the gradient normalization scheme of Adam, and thus both Adam and ND-Adam normalize

the gradient by a running average of its magnitude. This, and other similar schemes (Hazan et al.,
2015; Yu et al., 2017) make the optimization less susceptible to vanishing and exploding gradients.
The proposed spherical weight optimization serves a different purpose. It normalizes each weight
vector and projects the gradient onto a unit sphere, such that the effective learning rate can be con-
trolled more precisely. Moreover, it provides robustness to improper weight initialization, since the
magnitude of each weight vector is kept constant.

For nonlinear activation functions (without batch normalization), such as sigmoid and tanh, an extra
scaling factor is needed for each hidden unit to express functions that require unnormalized weight
), the activation of hidden
vectors. For instance, given an input vector x
·
unit i is then given by

Rn, and a nonlinearity φ (

(15)
where γi is the scaling factor, and bi is the bias. Consequently, normalizing weight vectors does not
limit the expressiveness of models.

x + bi) ,

∈
yi = φ (γiwi ·

3.3 RELATION TO WEIGHT NORMALIZATION AND BATCH NORMALIZATION

wi(cid:107)2 (cid:54)
(cid:107)

A related normalization and reparameterization scheme, weight normalization (Salimans & Kingma,
2016), has been developed as an alternative to batch normalization, aiming to accelerate the conver-
gence of SGD optimization. We note the difference between spherical weight optimization and
weight normalization. First, the weight vector of each hidden unit is not directly normalized in
= 1 in general. At training time, the activation of hidden unit i is
weight normalization, i.e,

(cid:18) γi
wi(cid:107)2
which is equivalent to Eq. (15) for the forward pass. For the backward pass, the effective learning
wi(cid:107)2 in weight normalization, hence it does not solve the ill-conditioning
rate still depends on
(cid:107)
problem. At inference time, both of these two schemes can merge wi and γi into a single equivalent
weight vector, w(cid:48)

yi = φ

x + bi

wi ·

(16)

i = γiwi, or w(cid:48)

wi.

(cid:19)

(cid:107)

,

i = γi
(cid:107)wi(cid:107)2

While spherical weight optimization naturally encompasses weight normalization, it can further
beneﬁt from batch normalization. When combined with batch normalization, Eq. (15) evolves into

yi = φ (γi BN (wi ·

x) + bi) ,

(17)

where BN (
) represents the transformation done by batch normalization without scaling and shift-
·
ing. Here, γi serves as the scaling factor for both the normalized weight vector and batch normal-
ization.

4 REGULARIZED SOFTMAX

For multi-class classiﬁcation tasks, the softmax function is the de facto activation function for the
output layer. Despite its simplicity and intuitive probabilistic interpretation, we observe a related
problem to the ill-conditioning problem we have addressed. Similar to how different magnitudes
of weight vectors result in different updates to the same network function, the learning signal back-
propagated from the softmax layer varies with the overall magnitude of the logits.

Speciﬁcally, when using cross entropy as the surrogate loss with one-hot target vectors, the predic-
tion is considered correct as long as arg maxc∈C (zc) is the target class, where zc is the logit before
. Thus, the logits can be positively scaled
the softmax activation, corresponding to category c
together without changing the predictions, whereas the cross entropy and its derivatives will vary
with the scaling factor. Concretely, denoting the scaling factor by η, the gradient w.r.t. each logit is
(cid:21)

∈ C

(cid:20)

∂L
∂zˆc

= η

(cid:80)

exp (ηzˆc)
c∈C exp (ηzc) −

1

, and

∂L
∂z¯c

=

(cid:80)

η exp (ηz¯c)
c∈C exp (ηzc)

,

(18)

where ˆc is the target class, and ¯c

∈ C\ {

.
ˆc
}

For Adam and ND-Adam, since the gradient w.r.t. each scalar or vector are normalized, the absolute
magnitudes of Eq. (18) are irrelevant. Instead, the relative magnitudes make a difference here. When

η is small, we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂L/∂z¯c
∂L/∂zˆc

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

lim
η→0

1

,

1

|C| −

which indicates that, when the magnitude of the logits is small, softmax encourages the logit of the
target class to increase, while equally penalizing that of the other classes, regardless of the difference
in ˆz
. However, it is more reasonable to penalize more the logits that are
ˆz
}
closer to ˆz, which are more likely to cause misclassiﬁcation.

¯z for different ¯z

∈ C\ {

−

On the other end of the spectrum, assuming no two digits are the same, we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂L/∂z¯c(cid:48)
∂L/∂zˆc

(cid:12)
(cid:12)
(cid:12)
(cid:12)

lim
η→∞

= 1, lim
η→∞

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂L/∂z¯c(cid:48)(cid:48)
∂L/∂zˆc

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= 0,

∈ C\ {

ˆc, ¯c(cid:48)
where ¯c(cid:48) = arg maxc∈C\{ˆc} (zc), and ¯c(cid:48)(cid:48)
. Eq. (20) indicates that, when the magnitude
}
of the logits is large, softmax penalizes only the largest logit of the non-target classes. In this case,
although the logit that is most likely to cause misclassiﬁcation is strongly penalized, the logits of
other non-target classes are ignored. As a result, the logits of the non-target classes tend to be similar
at convergence, ignoring the fact that some classes are closer to each other than the others. The latter
case is related to the saturation problem of softmax discussed in the literature (Oland et al., 2017),
where they focus on the problem of small absolute gradient magnitude, which nevertheless does not
affect Adam and ND-Adam.

We propose two methods to exploit the prior knowledge that the magnitude of the logits should
not be too small or too large. First, we can apply batch normalization to the logits. But instead
of setting γc’s as trainable variables, we consider them as a single hyperparameter, γC, such that
. Tuning the value of γC can lead to a better trade-off between the two extremes
γc = γC,
described by Eq. (19) and (20). We observe in practice that the optimal value of γC tends to be the
same for different optimizers or different network widths, but varies with network depth. We refer
to this method as batch-normalized softmax (BN-Softmax).

c
∀

∈ C

Alternatively, since the magnitude of the logits tends to grow larger than expected (in order to mini-
mize the cross entropy), we can apply L2-regularization to the logits by adding the following penalty
to the loss function:

(19)

(20)

(21)

LC =

λC
2

(cid:88)

c∈C

z2
c ,

where λC is a hyperparameter to be tuned. Different from BN-Softmax, λC can also be shared by
different networks of different depths.

5 EXPERIMENTS

In this section, we provide empirical evidence for the analysis in Sec. 2.2, and evaluate the perfor-
mance of ND-Adam and regularized softmax on CIFAR-10 and CIFAR-100.

5.1 THE EFFECT OF L2 WEIGHT DECAY

To empirically examine the effect of L2 weight decay, we train a wide residual network (WRN)
(Zagoruyko & Komodakis, 2016b) of 22 layers, with a width of 7.5 times that of a vanilla ResNet.
Using the notation suggested by Zagoruyko & Komodakis (2016b), we refer to this network as
WRN-22-7.5. We train the network on the CIFAR-10 dataset (Krizhevsky & Hinton, 2009), with
a small modiﬁcation to the original WRN architecture, and with a different learning rate anneal-
ing schedule. Speciﬁcally, for simplicity and slightly better performance, we replace the last
fully connected layer with a convolutional layer with 10 output feature maps.
i.e., we change
the layers after the last residual block from BN-ReLU-GlobalAvgPool-FC-Softmax to
BN-ReLU-Conv-GlobalAvgPool-Softmax. In addition, for clearer comparisons, the learn-
ing rate is annealed according to a cosine function without restart (Loshchilov & Hutter, 2017b;
Gastaldi, 2017). We train the model for 80k iterations with a batch size of 128, similar to the set-
tings used by Zagoruyko & Komodakis (Zagoruyko & Komodakis, 2016b). The experiments are
based on a TensorFlow implementation of WRN (Wu, 2016).

i and ∆wp

i , where ∆wl

As a common practice, we use SGD with a momentum of 0.9, the analysis for which is similar to
that in Sec. 2.2. Due to the linearity of derivatives and momentum, ∆wi can be decomposed as
i + ∆wp
∆wi = ∆wl
i are the components corresponding to the original loss
), and the L2 penalty term (see Eq. (4)), respectively. Fig. 1a shows the ratio between
function, L (
·
i (cid:107)2, which indicates how the tendency of ∆wl
the scalar projection of ∆wl
i to
i and
i . Note that ∆wp
wi(cid:107)2 is compensated by ∆wp
increase
i points to the negative direction of wi,
even when momentum is used, since the direction change of wi is slow. As shown in Fig. 1a, at the
beginning of the training, ∆wp
wi(cid:107)2 to its equilibrium value. During
(cid:107)
i , and ∆wp
i on ∆wp
the middle stage of the training, the projection of ∆wl
i almost cancel each other.
Then, towards the end of the training, the gradient of wi diminishes rapidly, making ∆wp
i dominant
again. Therefore, Eq. (7) holds more accurately during the middle stage of the training.

i dominants and quickly adjusts

i on ∆wp

∆wp

(cid:107)

(cid:107)

∆wi(cid:107)2 /
(cid:107)

In Fig. 1b, we show how the effective learning rate varies in different hyperparameter settings. By
wi(cid:107)2 is expected to remain the same as long as αλ stays constant, which is
Eq. (7),
(cid:107)
conﬁrmed by the fact that the curve for α0 = 0.1, λ = 0.001 overlaps with that for α0 = 0.05, λ =
0.002. However, comparing the curve for α0 = 0.1, λ = 0.001, with that for α0 = 0.1, λ =
wi(cid:107)2 does not change proportionally to αλ. On
0.0005, we can see that the value of
(cid:107)
the other hand, by using ND-Adam, we can control the value of
wi(cid:107)2 more precisely by
(cid:107)
adjusting the learning rate for weight vectors, αv. For the same training step, changes in αv lead to
approximately proportional changes in
wi(cid:107)2, as shown by the two curves corresponding
to ND-Adam in Fig. 1b.

∆wi(cid:107)2 /
(cid:107)

∆wi(cid:107)2 /
(cid:107)

∆wi(cid:107)2 /

(cid:107)

(cid:107)

(a) Scalar projection of ∆wl
(cid:107)∆wp

i (cid:107)2.

i on ∆wp

i normalized by

(b) Relative magnitudes of weight updates, or effective
learning rates.

Figure 1: An illustration of how L2 weight decay and ND-Adam control the effective learning rate.
The results are obtained from the 5th layer of the network, and other layers show similar results.

5.2 PERFORMANCE EVALUATION

To compare the generalization performance of SGD, Adam, and ND-Adam, we train the same WRN-
22-7.5 network on the CIFAR-10 and CIFAR-100 datasets. For SGD and ND-Adam, we ﬁrst tune
the hyperparameters for SGD (α0 = 0.1, λ = 0.001, momentum 0.9), then tune the initial learning
rate of ND-Adam for weight vectors to match the effective learning rate to that of SGD, i.e., αv
0 =
0.05, as shown in Fig. 1b. While L2 weight decay can greatly affect the performance of SGD, it does
not noticeably beneﬁt Adam in our experiments. For Adam and ND-Adam, β1 and β2 are set to the
default values of Adam, i.e., β1 = 0.9, β2 = 0.999. Although the learning rate of Adam is usually
set to a constant value, we observe better performance with the cosine learning rate schedule. The
initial learning rate of Adam (α0), and that of ND-Adam for scalar parameters (αs
0) are both tuned
to 0.001. We use horizontal ﬂips and random crops for data augmentation, and no dropout is used.

We ﬁrst experiment with the use of trainable scaling parameters (γi) of batch normalization. As
shown in Fig. 2, at convergence, the test accuracies of ND-Adam are signiﬁcantly improved upon
that of vanilla Adam, and matches that of SGD. Note that at the early stage of training, the test accu-
racies of Adam increase more rapidly than that of ND-Adam and SGD. However, the test accuracies
remain at a high level afterwards, which indicates that Adam tends to quickly ﬁnd and get stuck in
bad local minima that do not generalize well.

The average results of 3 runs are summarized in the ﬁrst part of Table 1. Interestingly, compared
to SGD, ND-Adam shows slightly better performance on CIFAR-10, but worse performance on
CIFAR-100. This inconsistency may be related to the problem of softmax discussed in Sec. 4, that
there is a lack of proper control over the magnitude of the logits. But overall, given comparable ef-
fective learning rates, ND-Adam and SGD show similar generalization performance. In this sense,
the effective learning rate is a more natural learning rate measure than the learning rate hyperparam-
eter.

Figure 2: Test accuracies of the same network
trained with SGD, Adam, and ND-Adam. De-
tails are shown in the ﬁrst part of Table 1.

Figure 3: Magnitudes of softmax logits in differ-
ent settings. Results of WRN-22-7.5 networks
trained on CIFAR-10.

Next, we repeat the experiments with the use of BN-Softmax. As discussed in Sec. 3.2, γi’s can be
removed from a linear rectiﬁer network, without changing the overall network function. Although
this property does not strictly hold for residual networks due to the skip connections, we observe that
when BN-Softmax is used, simply removing the scaling factors results in slightly better performance
for all three algorithms. Thus, we only report results for this setting. The scaling factor of the logits,
γC, is set to 2.5 for CIFAR-10, and 1 for CIFAR-100.

As shown in the second part of Table 1, while we obtain the best generalization performance with
ND-Adam, the improvement is most prominent for Adam, and is relatively small for SGD. This
discrepancy can be explained by comparing the magnitudes of softmax logits without regularization.
As shown in Fig. 3, the magnitude of logits corresponding to Adam is much larger than that of ND-
Adam and SGD, and therefore beneﬁts more from the regularization.

Table 1: Test error rates of WRN-22-7.5 net-
works on CIFAR-10 and CIFAR-100. Based on
a TensorFlow implementation of WRN.

CIFAR-10
Error (%)

CIFAR-100
Error (%)

BN w/ scaling factors

Method

SGD
Adam
ND-Adam

SGD
Adam
ND-Adam

BN w/o scaling factors, BN-Softmax

4.61
6.14
4.53

4.49
5.43
4.14

20.60
25.51
21.45

20.18
22.48
19.90

Table 2: Test error
rates of WRN-22-7.5
and WRN-28-10 networks on CIFAR-10 and
CIFAR-100. Based on the original implemen-
tation of WRN.

Method

SGD
ND-Adam

SGD
ND-Adam

CIFAR-10
Error (%)

CIFAR-100
Error (%)

WRN-22-7.5

WRN-28-10

3.84
3.70

3.80
3.70

19.24
19.30

18.48
18.42

While the TensorFlow implementation we use already provides an adequate test bed, we notice
that it is different from the original implementation of WRN in several aspects. For instance, they
use different nonlinearities (leaky ReLU vs. ReLU), and use different skip connections for down-
sampling (average pooling vs. strided convolution). A subtle yet important difference is that, L2-

regularization is applied not only to weight vectors, but also to the scales and biases of batch normal-
ization in the original implementation, which leads to better generalization performance. For further
comparison between SGD and ND-Adam, we reimplement ND-Adam and test its performance on a
PyTorch version of the original implementation (Zagoruyko & Komodakis, 2016a).

Due to the aforementioned differences, we use a slightly different hyperparameter setting in this
experiment. Speciﬁcally, for SGD λ is set to 5e
6 (L2-
4, while for ND-Adam λ is set to 5e
regularization for biases), and both αs
0 are set to 0.04. In this case, regularizing softmax
does not yield improved performance for SGD, since the L2-regularization applied to γi’s and the
last layer weights can serve a similar purpose. Thus, we only apply L2-regularized softmax for
ND-Adam with λC = 0.001. The average results of 3 runs are summarized in Table 2. Note that the
performance of SGD for WRN-28-10 is slightly better than that reported with the original imple-
mentation (i.e., 4.00 and 19.25), due to the modiﬁcations described in Sec. 5.1. In this experiment,
SGD and ND-Adam show almost identical generalization performance.

0 and αv

−

−

6 CONCLUSION

We introduced ND-Adam, a tailored version of Adam for training DNNs, to bridge the general-
ization gap between Adam and SGD. ND-Adam is designed to preserve the direction of gradient
for each weight vector, and produce the regularization effect of L2 weight decay in a more precise
and principled way. We further introduced regularized softmax, which limits the magnitude of soft-
max logits to provide better learning signals. Combining ND-Adam and regularized softmax, we
show through experiments signiﬁcantly improved generalization performance, eliminating the gap
between Adam and SGD. From a high-level view, our analysis and empirical results suggest the
need for more precise control over the training process of DNNs.

REFERENCES

Devansh Arpit, Stanisław Jastrz˛ebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxin-
der S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look
at memorization in deep networks. In International Conference on Machine Learning, 2017.

Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in

neural networks. In International Conference on Machine Learning.

Lawrence Cayton. Algorithms for manifold learning. Univ. of California at San Diego Tech. Rep,

pp. 1–17, 2005.

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and

stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011.

Xavier Gastaldi. Shake-shake regularization of 3-branch residual networks. In Workshop of Inter-

national Conference on Learning Representations, 2017.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectiﬁer neural networks.

In

International Conference on Artiﬁcial Intelligence and Statistics, pp. 315–323, 2011.

Elad Hazan, Kﬁr Levy, and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex opti-

mization. In Advances in Neural Information Processing Systems, pp. 1594–1602, 2015.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016.

Sepp Hochreiter and Jürgen Schmidhuber. Flat minima. Neural Computation, 9(1):1–42, 1997.

Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In IEEE Conference on Computer

Vision and Pattern Recognition, 2018.

Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning, pp. 448–456,
2015.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International

Conference on Learning Representations, 2015.

Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-

nical report, University of Toronto, 2009.

Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. arXiv preprint

arXiv:1711.05101, 2017a.

Ilya Loshchilov and Frank Hutter. Sgdr: stochastic gradient descent with restarts. In International

Conference on Learning Representations, 2017b.

Hariharan Narayanan and Sanjoy Mitter. Sample complexity of testing the manifold hypothesis. In

Advances in Neural Information Processing Systems, pp. 1786–1794, 2010.

Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized opti-
mization in deep neural networks. In Advances in Neural Information Processing Systems, pp.
2422–2430, 2015.

Anders Oland, Aayush Bansal, Roger B Dannenberg, and Bhiksha Raj. Be careful what you
arXiv preprint

backpropagate: A case for linear output activations & gradient boosting.
arXiv:1707.04199, 2017.

Salah Rifai, Yann N Dauphin, Pascal Vincent, Yoshua Bengio, and Xavier Muller. The manifold
tangent classiﬁer. In Advances in Neural Information Processing Systems, pp. 2294–2302, 2011.

Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to accel-
erate training of deep neural networks. In Advances in Neural Information Processing Systems,
pp. 901–909, 2016.

Samuel L Smith and Quoc V Le. A bayesian perspective on generalization and stochastic gradient

descent. In International Conference on Learning Representations, 2018.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–9, 2015.

Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5—RmsProp: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.

Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht. The
In Advances in Neural In-

marginal value of adaptive gradient methods in machine learning.
formation Processing Systems, 2017.

Neal Wu. A tensorﬂow implementation of wide residual networks, 2016. URL https://

github.com/tensorflow/models/tree/master/research/resnet.

Adams Wei Yu, Qihang Lin, Ruslan Salakhutdinov, and Jaime Carbonell. Normalized gradient with
adaptive stepsize method for deep neural network training. arXiv preprint arXiv:1707.04822,
2017.

Sergey Zagoruyko and Nikos Komodakis. A pytorch implementation of wide residual networks,
2016a. URL https://github.com/szagoruyko/wide-residual-networks.

Sergey Zagoruyko and Nikos Komodakis.

Wide residual networks.

arXiv preprint

arXiv:1605.07146, 2016b.

2012.

Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,

Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning Rep-
resentations, 2017.

