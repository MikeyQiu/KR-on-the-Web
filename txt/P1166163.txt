mGPfusion: Predicting protein stability changes with
Gaussian process kernel learning and data fusion

Emmi Jokinen1, Markus Heinonen1,2 and Harri L¨ahdesm¨aki1
1Department of Computer Science, Aalto University, 02150 Espoo, Finland
2Helsinki Institute for Information Technology, Finland

Abstract

Motivation: Proteins are commonly used by biochemical industry for numerous processes. Reﬁning these proteins’ properties via
mutations causes stability eﬀects as well. Accurate computational method to predict how mutations aﬀect protein stability are necessary
to facilitate eﬃcient protein design. However, accuracy of predictive models is ultimately constrained by the limited availability of
experimental data.
Results: We have developed mGPfusion, a novel Gaussian process (GP) method for predicting protein’s stability changes upon single
and multiple mutations. This method complements the limited experimental data with large amounts of molecular simulation data.
We introduce a Bayesian data fusion model that re-calibrates the experimental and in silico data sources and then learns a predictive
GP model from the combined data. Our protein-speciﬁc model requires experimental data only regarding the protein of interest and
performs well even with few experimental measurements. The mGPfusion models proteins by contact maps and infers the stability
eﬀects caused by mutations with a mixture of graph kernels. Our results show that mGPfusion outperforms state-of-the-art methods in
predicting protein stability on a dataset of 15 diﬀerent proteins and that incorporating molecular simulation data improves the model
learning and prediction accuracy.
Availability: Software implementation and datasets are available at github.com/emmijokinen/mgpfusion
Contact: emmi.jokinen@aalto.ﬁ

8
1
0
2
 
r
a

M
 
3
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
2
5
8
2
0
.
2
0
8
1
:
v
i
X
r
a

1 Introduction

Proteins are used in various applications by pharmaceutical, food, fuel,
and many other industries and their usage is growing steadily (Kirk
et al., 2002; Sanchez and Demain, 2010). Proteins have important ad-
vantages over chemical catalysts, as they are derived from renewable
resources, are biodegradable and are often highly selective (Cherry and
Fidantsef, 2003). Protein engineering is used to further improve the
properties of proteins, for example to enhance their catalytic activity,
modify their substrate speciﬁcity or to improve their thermostability
(Rapley and Walker, 2000). Increasing the stability is an important as-
pect of protein engineering, as the proteins used in industry should be
stable in the industrial process conditions, which often involve higher
than ambient temperature and non-aqueous solvents (Bommarius et al.,
2011). The properties of a protein are modiﬁed by introducing alter-
ations to its amino acid sequence. Mutations in general tend to be
destabilising, and if too many destabilising mutations are implemented,
the protein may not remain functional without compensatory stabilis-
ing mutations (Tokuriki and Tawﬁk, 2009).

The stability of a protein can be deﬁned as the diﬀerence in Gibbs
energy ∆G between the folded and unfolded (or native and denatu-
rated) state of the protein. More precisely, the Gibbs energy diﬀerence
determines the thermodynamic stability ∆Gt of the protein, as it does
not take into account the kinetic stability ∆Gk which determines the
energy needed for the transition between the folded and unfolded states
(Anslyn and Dougherty, 2006) (see Supplementary Figure S1). Here we
will consider only the thermodynamic stability and from now on it will
be referred to merely as stability ∆G.

The eﬀect of mutations can be deﬁned by the change they cause
to the Gibbs energy ∆G, denoted as ∆∆G (Pace and Scholtz, 1997).
To comprehend the signiﬁcance of stability changes upon mutations,
we can consider globular proteins, the most common type of enzymes,
whose polypeptide chain is folded up in a compact ball-like shape with
an irregular surface (Alberts et al., 2007). These proteins are only
marginally stable and the diﬀerence in Gibbs energy between the folded
and unfolded state is only about 5–15 kcal/mol, which is not much more
than the energy of a single hydrogen bond that is about 2–5 kcal/mol
(Branden and Tooze, 1999). Therefore, even one mutation that breaks
a hydrogen bond can prevent a protein from folding properly.

The protein stability can be measured with many techniques, in-
cluding thermal, urea and guanidinium chloride (GdmCl) denaturation

curves that are determined as the fraction of unfolded proteins at dif-
ferent temperatures or at diﬀerent concentrations of urea or GdmCl
(Pace and Shaw, 2000). Some of the experimentally measured sta-
bility changes upon mutations have been gathered in thermodynamic
databases such as Protherm (Kumar et al., 2006).

A variety of computational methods have been introduced to predict
the stability changes upon mutations more eﬀortlessly than through ex-
perimental measurements. These methods utilise physics or knowledge-
based potentials (Leaver-Fay et al., 2011), their combinations, or diﬀer-
ent machine learning methods. The machine learning methods utilise
support vector machines (SVM) (Capriotti et al., 2005b, 2008; Chen
et al., 2013; Cheng et al., 2006; Folkman et al., 2014; Liu and Kang,
2012; Pires et al., 2014a), random forests (Tian et al., 2010; Wainreb
et al., 2011), neural networks (Dehouck et al., 2009; Giollo et al., 2014),
and Gaussian processes (Pires et al., 2014b). However, it has been as-
sessed that although on average many of these methods provide good
results, they tend to fail on details (Potapov et al., 2009). In addition,
many of these methods are able to predict the stability eﬀects only for
single-point mutations.

We introduce mGPfusion (mutation Gaussian Processes with data
fusion), a method for predicting stability eﬀects of both point and mul-
tiple mutations. mGPfusion is a protein-speciﬁc model – in contrast to
earlier stability predictors that aim to estimate arbitrary protein struc-
ture or sequence stabilities – and achieves markedly higher accuracy
while utilising data only from a single protein at a time. In contrast
to earlier works that only use experimental data to train their models,
we also combine exhaustive Rosetta (Leaver-Fay et al., 2011) simulated
point mutation in silico stabilities to our training data.

A key part of mGPfusion is the automatic scaling of simulated data
to better match the experimental data distribution based on those vari-
ants that have both experimental and simulated stability values. Fur-
thermore, we estimate a variance resulting from the scaling, which
places a higher uncertainty on very destabilising simulations. Our
Gaussian process model then utilises the joint dataset with their es-
timated heteroscedastic variances and uses a mixture of graph kernels
to assess the stability eﬀects caused by changes in amino acid sequence
according to 21 substitution models. Our experiments on a novel 15
protein dataset show a state-of-the-art stability prediction performance,
which is also sustained when there is access only to a very few experi-
mental stability measurements.

1

Figure 1: Pipeline illustration for mGPfusion. a) M = 21 substitution matrices utilise diﬀerent information sources and give scores to pairwise amino
acid substitutions. b) The wild-type structures from Protein Data Bank are modelled as contact graphs. c) The graph kernel measures similarity of two
sequences by a substitution model S over all positions p and their neighbourhoods nbs(p) in the contact graph. d) Each substitution matrix is used to
create a separate covariance matrix. e) Multiple kernel learning (MKL) is used for ﬁnding the optimal combination of the base kernels. The kernel matrix
measures variant similarities. f ) Experimentally measured ∆∆G values yE are gathered from Protherm and Rosetta’s ddg monomer application is used to
simulate the stability eﬀects yS for all single point mutations. g) Bayesian scaling for the simulated values yS at the x-axis. Possible scalings are coloured
with green and the chosen scaling from yS into scaled values ˜yS is marked by black dots. The scaling is ﬁtted to a subset of experimentally measured
stabilities yE (circles). h) The stability predictive GP model is trained using experimental and simulated data through the kernel matrix.

2 Methods

Following Pires et al. (2014b) we choose a Bayesian model family of
Gaussian processes for prediction of mutation eﬀects on protein sta-
bility due to its inherent ability to handle uncertainty in a principle
way. Bayesian modelling is a natural approach for combining the ex-
perimental and simulated data distribution, while it is also suitable for
learning the underlying mixture of substitution models that governs the
mutational process.

The pipeline for mGPfusion is presented in Figure 1. The ﬁrst
part of mGPfusion consists of collection of in silico and experimental
datasets discussed in Section 2.1, the scaling of the in silico dataset in
Section 2.2 and the fusion of these two datasets in Section 2.3. The sec-
ond part consists of the Gaussian process model described in Section 2.4
with detailed description of the graph kernels in Sections 2.5-2.6 and
model inference in Section 2.7. Finally, the evaluation criteria used are
described in Section 2.8.

teins that fulﬁlled these requirements are listed in Table 1. We aver-
aged the stability values for proteins with multiple measurements and
ignored mutations to residues not present in their 3D structures. These
data sets are available at github.com/emmijokinen/mgpfusion.

Protein (organism)

PDB

mutations
point

point (sim)

all

2LZM 349
T4 Lysozyme (Enterobacteria phage T4)
182
1BNI
Barnase (Bacillus amyloliquefaciens)
124
1VQB
Gene V protein (Escherichia virus m13)
116
1LZI
Glycosyltransferase A (Homo sapiens)
98
2CI2
Chymotrypsin inhibitor 2 (Hordeum vulgare)
89
1PGA
Protein G (Streptococcus sp. gx7805)
83
2RN2
Ribonuclease H (Escheria coli)
80
1CSP
Cold shock protein B (Bacillus subtilis)
80
1BVC
Apomyoglobin (Physeter catodon)
63
4LYZ
Hen egg white lysozyme (Gallus gallus)
57
Ribonuclease A (Bos taurus)
1RTB
56
Peptidyl-prolyl cis-trans isomerase (Homo sapiens) 1PIN
53
1RN1
Ribonuclease T1 isozyme (Aspergillus oryzae)
54
1RGG
Ribonuclease (Streptomyces auerofaciens)
53
1BPI
Bovine pancreatic trypsin inhibitor (Bos taurus)

264
163
92
114
77
34
65
50
56
50
50
56
48
45
47

3116
2052
1634
2470
1235
1064
2945
1273
2907
2451
2356
2907
1957
1824
1102

2.1 Experimental and in silico data

total

1537

1211

31293

Protherm is a database of numerical thermodynamic parameters for
proteins and their mutants (Kumar et al., 2006). From Protherm we
gathered all proteins with at least 50 unique mutations whose ∆∆G
has been measured by thermal denaturation, and where a PDB code
for a 3D structure of the protein was reported. We required the pro-
teins to have at least 50 unique mutations, so that we would have a
representative test set and get suﬃciently reliable estimates of predic-
tion accuracy on individual proteins and examine how the amount of
experimental training data aﬀects the accuracy of the model. The 3D
structures are necessary for obtaining the connections between residues.
We collected the 3D structures with the reported PDB codes from the
Protein Databank, www.rcsb.org (Berman et al., 2000). The 15 pro-

Table 1: The 15 protein data from ProTherm database with counts of point
mutations, all mutations, and of simulated point mutation stability changes.

We also generate simulated data of the stability eﬀects of all possible
single mutations of the proteins. Our method can utilise any simulated
stability values. We used the “ddG monomer” application of Rosetta
3.6 (Leaver-Fay et al., 2011) using the high-resolution backrub-based
protocol 16 recommended in Kellogg et al. (2011). The predictions yS
made with Rosetta are given in Rosetta Energy Units (REU). Kellogg
et al. (2011) suggest transformation 0.57yS for converting the predic-
tions into physical units. The simulated data scaled this way is not
as accurate as the experimental data, the correlation and root mean

2

square error (rmse) with respect to the experimental data are shown
for all proteins in Table 2 and for individual proteins in Supplementary
Table S2, on rows labelled Rosetta. For this reason, we use instead a
Bayesian scaling described in the next section and diﬀerent noise models
for the experimental and simulated data, described in Section 2.3.

For each of the 15 proteins,

let xi = (xi1, . . . , xiM ) denote its
M -length variant i with positions p labelled with residues xip ∈
{A, R, N, . . . , V }. We denote the wild-type protein as x0. We collect
15 separate sets of simulated and experimental data. We denote the
NE )T with
NE experimental variants of each protein as XE = (xE
the corresponding experimental stability values yE = (yE
NE )T ∈
RNE . Similarly, we denote the NS simulated observations as XS =
(xS

1 , . . . , xE

1 , . . . , yE

NS )T and yS = (yS

NS )T ∈ RNS .

1 , . . . , xS

1 , . . . , yS

2.2 Bayesian scaling of in silico data

The described transformation from REU to physical units may not
be optimal for all proteins. We therefore applied instead a linear-
exponential scaling function to obtain scaled Rosetta simulated sta-
bilities ˜yS,

˜yS = g(yS | θj) = ajecj yS

+ bjyS + dj.

(1)

This scaling transforms the Rosetta simulations yS for each protein
j = 1, . . . , 15 to correspond better to the experimental data. The pa-
rameters θj = (aj, bj, cj, dj) deﬁne the weight aj and steepness cj of the
exponential term, while the linear term has slope bj and intercept dj.
To avoid overﬁtting, we perform Bayesian linear regression and start
by deﬁning parameter prior p(θj) = p(aj)p(bj)p(cj)p(dj) that reﬂects
our beliefs about realistic scalings having only moderate steepness:

and stabilities y = (y0, yE, ˜yS) of size RN where N = 1 + NE + NS is
the total number of simulated and experimental data points, including
the wild-type. We assume heteroscedastic additive noise models for the
three information sources

y0 = f (x0) + ε0,
(cid:1) + εE
i = f (cid:0)xE
yE
i ,
(cid:1) + εS
i = f (cid:0)xS
˜yS
i ,

i

i

(cid:1)

(cid:1)

0

ε0 ∼ N (cid:0)0, σ2
i ∼ N (cid:0)0, σ2
εE
(cid:16)
εS
i ∼ N

E

0, (σE + σS + tσT (i))2(cid:17)

,

(5)

where the observed values are noisy versions of the underlying ‘true’
stability function f (x) corrupted by zero-mean noise with data source
speciﬁc variances. We learn a Gaussian process based stability function
f (x) in the next Section.

The Equations (5) encode that the experimental data are corrupted
by a global experimental noise variance σ2
E. The simulated stabilities
are additionally corrupted by a global Rosetta simulator error variance
σ2
S, and by the value-dependent transformation variance tσ2
T (i) scaled
by parameter t. The model then encapsulates that we trust the Rosetta
data less than the experimental data. By deﬁnition, the ∆∆G of the
wild-type is zero (y0 = 0) with very small assumed error, σ0 = 10−6.
Note that σ2
T are ﬁxed by equation (4), while we infer the optimal
values for the remaining three free parameters (σE, σR, t) (See Section
2.4). The parameters σ2

S are assigned priors

E and σ2

σE ∼ Gamma(σE|αE, βE)
σS ∼ Gamma(σR|αS, βS).

(6)

The values of these hyperparameters are shown in Supplementary Ta-
ble S1.

p(aj) = Gamma(aj | αa, βa)
p(bj) = Beta(1/2 · bj | αb, βb)
p(cj) = Beta(10/3 · cj | αc, βc)
p(dj) = N (dj | µd, σ2

d).

2.4 Gaussian processes

(2)

We use a Gaussian process (GP) function f to predict the stability
f (x) ∈ R of a protein variant x. Gaussian processes are a family of
non-parametric, non-linear Bayesian models (Rasmussen and Williams,
2006). A zero-mean GP prior

The empirically selected hyperparameter values are listed in Supple-
mentary Table S1 and the priors are illustrated in Figure S2.

We compute the posterior for θj using the subset of simulated data

that have corresponding experimentally measured data:

p(θj|yE, yS) ∝

(cid:89)

N (cid:0)yE

i

| g(yS

i |θj), σ2
n

(cid:1) p(θj).

i:xi∈XE ∩XS

The product iterates over all NE∩S simulated ∆∆G’s that have a
matching experimentally observed value. The σ2
n is the scaling error
variance, which was set to σ2
n = 0.5. The parameters θ for each protein
were sampled using a random walk Metropolis-Hastings MCMC algo-
rithm (the mhsample function in Matlab) for NM C = 10000 samples
with a burn-in set to 500. The proposal distribution was selected to be
a symmetric uniform distribution such that [as+1, bs+1, cs+1, ds+1] ∼
U (as ± 0.4, bs ± 0.04, cs ± 0.04, ds ± 0.4). Given the sample of scaling
parameters (θ(s)
s=1 , we deﬁne the scaled simulated data as the aver-
age scaling over the MCMC sample, and record also the sample scaling
variance

j )NM C

˜yS
i =

g(yS

i |θ(s)
j )

1
NM C

1
NM C

NM C(cid:88)

s=1
NM C(cid:88)

(cid:16)

s=1

σ2
T (i) =

g(yS

i |θ(s)

j ) − ˜yS
i

(cid:17)2

.

See Figure 1 g) for an illustration of the scaling. We collect the scaled
simulated value and its variance from each simulated point into vectors
˜yS = (˜yS

T (NS)) ∈ RNS .

T (1), . . . , σ2

NS ) and σ2

1 , . . . , ˜yS

T = (σ2

deﬁnes a distribution over functions f (x) whose mean and covariance
are

f (x) ∼ GP (0, k(x, x(cid:48))) ,

E[f (x)] = 0
cov[f (x), f (x(cid:48))] = k(x, x(cid:48)).

For any collection of protein variants X = x1, . . . , xN , the function val-
ues follow a multivariate normal distribution f ∼ N (0, KXX ), where
f = (f (x1), . . . , f (xN ))T ∈ RN , and where KXX ∈ RN ×N with
[KXX ]ij = k(xi, xj). The key property of Gaussian processes is that
they encode functions that predict similar stability values f (x), f (x(cid:48))
for protein variants x, x(cid:48) that are similar, as encoded by the kernel
k(x, x(cid:48)). The key part of GP modelling is then to infer a kernel that
measures the mutation’s eﬀects to the stability.

Let a dataset of noisy stability values from two sources be y ∈ RN ,
the corresponding protein structures X = (xi)N
i=1, and a new protein
variant x∗ whose stability we wish to predict. A Gaussian process de-
ﬁnes a joint distribution over the observed values y of variants X, and
the unknown function value f (x∗) of the unseen variant x∗,
(cid:21)(cid:19)

(cid:18)

(cid:21)

(cid:20) y
f (x∗)

∼ N

0,

(cid:20)KXX + diag(σ2)
k∗X

kX∗
k(x∗, x∗)

,

∗X ∈ RN is a kernel vector with elements k(xi, x∗) for
where kX∗ = kT
T )2)T
all i = 1, . . . , N , and where σ2 = (σ2
collects ﬁnal variances of the data points from equations (5). Here
the exponents are elementwise. The conditional distribution gives the
posterior distribution of the stability prediction as

E1T , (σE1T + σS1T + tσT

0, σ2

f (x∗)|(X, y) ∼ N (cid:0)µ(x∗), σ2(x∗)(cid:1) ,

2.3 Data fusion and noise models

where the prediction mean and variance are

For each protein j, we organise its experimental data (XE, yE) and
transformed simulated data (XS, ˜yS) along with the wild-type infor-
mation (x0, y0) into a single joint dataset of variants X = (x0, XE, XS)

(cid:0)KXX + diag(σ2)(cid:1)−1 y,

µ(x∗) = k∗X
σ2(x∗) = k(x∗, x∗) − k∗X

(cid:0)KXX + diag(σ2)(cid:1)−1 kX∗.

(3)

(4)

3

Hence, in GP regression the stability predictions µ(x∗) ± σ(x∗) will
come with uncertainty estimates.

ring amino acids and scaled them between zero and one as

2.5 Graph kernel

Next, we consider how to compute the similarity function k(x, x(cid:48)) be-
tween two variants of the same protein structure. We will encode the
3D structural information of the two protein variants as a contact map
and measure their similarity by the formalism of graph kernels (Vish-
wanathan et al., 2010).

We consider two residues to be in contact if their closest atoms are
within 5 ˚A of each other in the PDB structure, which is illustrated in
Figure 1 b). All variants of the same protein have the same length, with
only diﬀerent residues at mutating positions. Furthermore, we assume
that all variants share the wild-type protein contact map.

To compare protein variants, we construct a weighted decomposition
kernel (WDK) (Menchetti et al., 2005) between two protein variants
x = (x1, . . . , xM ) and x(cid:48) = (x(cid:48)

M ) of length M ,

1, . . . , x(cid:48)

k(x, x(cid:48)) =


S(xp, x(cid:48)
p)

M
(cid:88)

p=1



(cid:88)

S(xl, x(cid:48)
l)

,

l∈nbs(p)

(7)

where nbs(p) deﬁnes the set of neighbouring positions to position p,
and S is a substitution matrix. The kernel iterates over all positions
p and compares for each of them their residues through a substitution
matrix S(xp, x(cid:48)
p). Furthermore, the similarity of the residues at each
position is multiplied by the average similarity of the residues at its
neighbouring positions S(xl, x(cid:48)
l). Hence, the kernel deﬁnes the simi-
larity of two protein variants as the average position and neighbour-
hood similarity over all positions. The kernel matrix is normalised
so that for two data points, the normalised kernel
p) =
k(xp, x(cid:48)
p, x(cid:48)
p), as deﬁned by Shawe-Taylor and Cris-
tianini (2004). The kernel is illustrated in Figure 1 c).

k(xp, xp)k(x(cid:48)

is ˆk(xp, x(cid:48)

p)/

(cid:113)

The above WDK kernel allows us to compare the eﬀects of multiple
simultaneous mutations. However, as the wild type protein structure
is used for all of the protein variants, changes that the mutations may
cause to the protein structure are not taken into consideration. This
may cause problems if mutations that alter the protein structure sig-
niﬁcantly are introduced – especially if many of them are introduced
simultaneously. On the other hand, substitution matrices that have
their basis in sequence comparisons, should take these eﬀects into ac-
count to some extend as these kinds of mutations are usually highly
destabilising and do not occur often in nature. In the next section, we
will discuss how we utilise diﬀerent substitution matrices with multiple
kernel learning.

The BLOSUM substitution models have been a common choice for
protein models (Giguere et al., 2013), while mixtures of substitution
models were proposed by Cichonska et al. (2017). BLOSUM matrices
score amino acid substitutions by their appearances throughout evolu-
tion, as they compare the frequencies of diﬀerent mutations in similar
blocks of sequences (Henikoﬀ and Henikoﬀ, 1992). However, there are
also diﬀerent ways to score amino acids substitutions, such as chemical
similarity and neighbourhood selectivity (Tomii and Kanehisa, 1996).
When the stability eﬀects of mutations are evaluated, the frequency of
an amino acid substitution in nature may not be the most important
factor.

To take into account diﬀerent measures of similarity between amino
acids, we employed a set of 21 amino acid substitution matrices gath-
ered from AAindex21 (Kawashima et al., 2008). AAindex2 currently
contains 94 substitution matrices. From these we selected those that
had no gaps concerning substitutions between the 20 naturally occur-

1http://www.genome.jp/aaindex/
2http://www.cs.ubc.ca/~schmidtm/Software/minConf.html

4

S =

S0 − min(S0) + 1
max(S0) − min(S0) + 1

.

Out of these matrices, we only chose those 23 matrices that were pos-
itive semideﬁnite. Furthermore, there were two pairs of matrices that
were extremely similar, and we only selected one matrix from each pair,
ending up with 21 substitution matrices. These substitution matrices
are used together with Equation 7 for computing 21 base kernel matri-
ces. Finally, MKL is used to ﬁnd an optimal combination of the base
kernels of form

(8)

(9)

Kφ =

wmK (γm)
m ,

21
(cid:88)

m=1

where wm is a kernel speciﬁc weight, γm is an (elementwise) exponent.
The elementwise exponent retains the SDP property of Kφ (Shawe-
Taylor and Cristianini, 2004). We observe empirically that the optimal
kernel weights wm tend to be sparse (See Figure 2).

The selected substitution matrices are listed in Figure 2. These ma-
trices have diﬀerent basis and through multiple kernel learning (MKL)
our model learns which of these are important for inferring the stability
eﬀects that mutations cause on diﬀerent proteins. The ﬁgure illustrates
this by showing the average weights of the base kernel matrices obtained
via the multiple kernel learning.

2.7 Parameter inference

The complete model has ﬁve parameters φ = (σE, σS, t, w, γ) to in-
fer, of which the variance parameters (σE, σS, t) parameterise the joint
data variance σ2
φ, while the MKL parameters w = (w1, . . . , w21) and
γ = (γ1, . . . , γ21) parameterise the kernel matrix Kφ. In a Gaussian
process model these can be jointly optimised by the marginal (log)
likelihood with priors

log p(y|φ)p(σE)p(σR) = log

p(y|f , φ)p(f |φ)p(σE)p(σR)df

(cid:90)

∝ −

yT (Kφ + diag(σ2

1
2
+ log Gamma(σE|αE, βE) + log Gamma(σS|αS, βS),

log |Kφ + diag(σ2

φ))−1y −

φ)|

1
2

(10)

which automatically balances model ﬁt (the square term) and the
model complexity (the determinant) to avoid overﬁtting (Rasmussen
and Williams, 2006). The parameters can be optimised by maximis-
ing the marginal log likelihood (10) using gradient ascent, since the
marginal likelihood can be diﬀerentiated analytically (see Supplemen-
tary Equations S1 and S2). We utilised a limited-memory projected
quasi-Newton algorithm (minConf TMP2), described by (Schmidt et al.,
2009).

We chose to evaluate the accuracy of our predictions using the same
metrics that have been used by many others – correlation ρ between the
predicted and experimentally measured ∆∆G values (Capriotti et al.,
2005a; Dehouck et al., 2009; Kellogg et al., 2011; Pires et al., 2014b;
Potapov et al., 2009) and the root mean square error (rmse) (Dehouck
et al., 2009; Pires et al., 2014a,b), which are determined in the Supple-
mentary Equations S3 and S4. We use marginal likelihood maximisa-
tion to infer model parameters and perform cross-validation to evaluate
the model performance on test data. Below we only report evaluation
metrics obtained from the test sets not used at any stage of the model
learning or data transformation sampling.

3 Results

In this section we evaluate the performance of mGPfusion on predict-
ing stability eﬀects of mutations, and compare it to the state-of-the-
art prediction methods mCSM, PoPMuSiC and Rosetta. Rosetta is a

2.6 Substitution matrices and multiple kernel

2.8 Evaluation criteria

learning

Figure 2: Average weights for kernels utilising the described substitution matrices from AAindex2, when GP models were trained with mutation level
cross-validation. Basis for the substitution matrices are obtained from (Tomii and Kanehisa, 1996). ∗ were added to AAindex2 in a later release, and their
basis were not determined by Tomii and Kanehisa (1996).

molecular modelling software whose ddg monomer module can directly
simulate the stability changes ∆∆G of a protein upon mutations. PoP-
MuSic and mCSM are machine learning models that predict stability
based on protein variant features. We run Rosetta locally, and use
mCSM and PoPMuSiC models through their web servers3. This may
give these methods an advantage over mGPfusion since parts of our
testing data were likely used within their training data.

We compare four diﬀerent variants of our method: mGPfusion that
uses both simulated data and MKL, “mGPfusion, only B62” that uses
simulated data but incorporates only one kernel matrix (BLOSUM62
substitution matrix), mGP model that uses MKL but does not use simu-
lated data, and “mGP, only B62” that uses only the base GP model but
does not incorporate simulated data and uses only the BLOSUM62 sub-
stitution matrix. In addition, we experiment on transforming Rosetta
predictions with the Bayesian scaling. We perform the experiments
for the 15 proteins separately using either position or mutation level
(leave-one-out) cross-validation regarding the methods mGP, mGPfu-
sion and the Bayesian scaling of Rosetta. Pires et al. (2014b) used
protein and position level cross-validation to evaluate their model. In
protein level cross-validation all mutations in a protein are either in
the test or training set exclusively. When we train our model using
protein level cross-validation, we use no experimental data and rely
only on the simulated data. Position level cross-validation is deﬁned
so that all mutations in a position are either in the test or training set
exclusively. However, datasets in Pires et al. (2014b) contained only
point mutations and therefore we had to extend the deﬁnition to also
include multiple mutations. In position level cross-validation we train
one model for each position using only the part of data that has a
wild-type residue in that position. Therefore, in position level cross-
validation we construct a test set that contains all protein variants that
have a mutation at position p and use as training set all the protein
variants that have a wild-type residue at that position. Dehouck et al.
(2009) evaluated their models by randomly selecting training and test
sets so that each mutation was exclusively in one of the sets, but both
sets could contain mutations from the same position of the same pro-

3biosig.unimelb.edu.au/mcsm and

omictools.com/popmusic-tool

tein. We call this mutation level cross-validation. When we use all
available experimental data with mutation level cross-validation, this
corresponds to leave-one-out cross-validation.

3.1 Predicting point mutations

Table 2 summarises the average prediction performance over all 15 pro-
teins for all compared methods, types of mutations and cross-validation
types. We ﬁrst compare the performances on single point mutations,
where mGPfusion and mGP achieve the highest performance with
ρ = 0.81 and rmse = 1.07 kcal/mol, and ρ = 0.81 and rmse = 1.04
kcal/mol, respectively with mutation level cross-validation. With only
one kernel utilising the BLOSUM62 matrix instead of MKL, the per-
formance decreases slightly, but the competing methods are still out-
performed, as mCSM achieves ρ = 0.64 and rmse = 1.37 kcal/mol,
PoPMuSic ρ = 0.61 and Rosetta ρ = 0.55. Applying Bayesian scaling
on Rosetta simulator improves the performance of standard Rosetta
from ρ = 0.55 to ρ = 0.65 and decreases the rmse from 1.63 kcal/mol
to 1.35 kcal/mol, which is interestingly even slightly better than the
performances of mCSCM and PoPMuSiC.

With position level cross-validation mGPfusion achieves the high-
est performance of ρ = 0.70 and rmse = 1.26 kcal/mol, likely due
to having still access to simulated variants from that position, since
they are always available to the learner. Without simulation data,
the baseline machine learning model mGP performance decreases to
ρ = 0.51 and rmse = 1.54 kcal/mol, thus demonstrating the impor-
tance of the data fusion. Cross-validation could not be performed for
the oﬀ-the-shelf methods mCSM and PoPMuSiC. Even still, mGPfusion
(trained with one or multiple kernels) outperforms competing state-of-
the-art methods and achieves markedly higher prediction performance
as quantiﬁed by both mutation and position level cross-validations.
Also mGP outperforms these methods when quantiﬁed by mutation
level cross-validation. With protein level cross-validation mGPfusion
achieves slightly better results than Rosetta.

5

Method
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled

Point mutations
cross-validation level
prot.
pos.
mut.
0.56
0.70
0.81
0.56
0.69
0.79
-
0.51
0.81
-
0.34
0.76
-
0.63
0.65

Correlation ρ
Multiple mutations
cross-validation level
prot.
pos.
mut.
0.49
0.61
0.88
0.50
0.64
0.86
-
0.52
0.86
-
0.55
0.86
-
0.39
0.51

All mutations
cross-validation level
prot.
pos.
mut.
0.52
0.64
0.83
0.52
0.66
0.82
-
0.50
0.83
-
0.49
0.80
-
0.48
0.60

Point mutations
cross-validation level
prot.
pos.
mut.
1.61
1.26
1.07
1.62
1.30
1.11
-
1.54
1.04
-
1.95
1.26
-
1.38
1.35

rmse
Multiple mutations
cross-validation level
prot.
pos.
mut.
2.53
2.45
1.33
2.50
2.40
1.43
-
2.65
1.44
-
2.56
1.45
-
2.99
2.49

All mutations
cross-validation level

mut.
1.13
1.18
1.14
1.30
1.66

pos.
1.87
1.85
2.09
2.23
2.22

prot.
1.84
1.84
-
-
-

Predictions from oﬀ-the-shelf implementations with no cross-validation

Rosetta
mCSM
PoPMuSiC

0.55
0.61
0.64

0.40
-
-

0.49
-
-

1.63
1.40
1.37

2.74
-
-

1.92
-
-

Table 2: Comparison of diﬀerent methods on the 15 protein dataset with respect to ρ and rmse. Mutation, position, and protein are referred to as mut.,
pos., and prot., respectively. Predictions from oﬀ-the-shelf implementations of Rosetta, mCSM and PoPMuSiC are used directly without cross-validation.

3.2 Predicting multiple mutations

Next, we tested stability prediction accuracies for variants containing
either single or multiple mutations. Figure 3 shows a scatter plot of
mGPfusion predictions for all 1537 single and multiple mutation vari-
ants (covering all 15 proteins) against the experimental ∆∆G values
using the mutation level (leave-one-out) cross-validation. The points
are coloured by the number of simultaneous mutations in the variants,
with 326 variants having at least 2 mutations (See Table 1). Fig-
ure 3 illustrates the mGPfusion’s overall high accuracy of ρ = 0.83
and rmse = 1.13 kcal/mol on both single and multiple mutations (See
Table 2). Scatter plots for the individual proteins can be found in Sup-
plementary Figure S3. Dehouck et al. (2009) suggested that considering
the predictive power after removal of most badly predicted stability ef-
fects of mutations may give more relevant evaluation, as some of the
experimental measurements may have been made in non-physiological
conditions or aﬀected by signiﬁcant error, associated with a poorly re-
solved structure, or indexed incorrectly in the database. They thus
reported correlation and rmse of the predictions after excluding 10 %
of the predictions with most negative impacts on the correlation co-
eﬃcient. Pires et al. (2014b) also reported their accuracy after 10 %
outlier removal. If we remove the 10% worst predicted stability eﬀects
from the combined predictions, we achieve correlation ρ of 0.92 and
rmse of 0.67 kcal/mol. We report these results for all the methods
in Supplementary Table S3 and also present the error distribution in
Supplementary Figure S5.

Figure 3: Scatter plot for the mutation level (leave-one-out) predictions made
for all 15 proteins (See Table 1). The colour indicates the number of simul-
taneous mutations.

The high accuracy is retained for variants with multiple mutations
as well (ρ = 0.88 and rmse = 1.33 kcal/mol, see Tables 2 and S2).
Table 3 lists mGPfusion’s rmse for diﬀerent number of simultaneous
mutations. The model accuracy in fact improves up to 6 mutations.
This is explained by the training set often containing the same single
point mutations that appear in variants with multiple mutations. The
model can then infer the combined eﬀect of pointwise mutations. The
model seems to fail when predicting the eﬀects of 7-9 simultaneous mu-

6

tations. Most of these mutations (8/12) are for Ribonuclease (1RGG)
and their eﬀects seem to be exceptionally diﬃcult to predict. This may
be because only few of the point mutations that are part of the multiple
mutations are present in the training data. However, these mutations
seem to be exceptionally diﬃcult to predict for Rosetta as well, which
could indicate that the experimental measurements concerning these
mutations are not quite accurate. PoPMuSiC and mCSM are unable
to predict multiple mutations, while Rosetta supports them, but its
rmse accuracy decreases already with two mutations.

mutations
occurences

mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta

1
1211

1.07
1.11
1.04
1.26
1.35
1.63

2
207

1.06
1.12
1.03
0.96
2.10
2.27

3
52

0.80
0.77
0.61
0.65
1.92
2.11

4
42

0.51
0.59
0.50
0.83
2.94
3.78

5
4

0.40
0.29
0.18
0.26
2.29
2.93

6
8

1.01
1.14
0.92
1.14
2.32
2.21

7
3

3.02
3.00
3.23
2.95
2.93
2.92

8
3

5.89
6.78
6.18
6.90
6.75
5.80

9
6

5.16
5.56
6.75
6.57
7.28
7.45

10
1

0.25
0.11
0.08
0.05
2.69
3.42

Table 3: Root-mean-square errors for diﬀerent number of simultaneous
mutations for all 15 proteins, with models trained by leave-one-out cross-
validation. Rosetta is added for comparison.

With multiple mutations, the decrease in performance between the
position and mutation level cross-validations becomes clearer than with
single mutations. With the position level cross-validation the stability
eﬀects of multiple mutations are predicted multiple times, which partly
explains this loss of accuracy. For example, the eﬀects of mutants with
nine diﬀerent simultaneous mutations, which were the most diﬃcult
cases in the mutation level cross-validation, are predicted nine times.
Surprisingly, mGPfusion trained with protein level cross-validation
achieves higher correlation and smaller errors than Rosetta; mGPfusion
utilising simulated ∆∆G values for only single mutations, can predict
the stability eﬀects of multiple mutations better than Rosetta.

3.3 Uncertainty of the predictions

Gaussian processes provide a mean µ(x) and a standard deviation σ(x)
for the stability prediction of a protein variant x. The standard de-
viation allows estimation of the prediction accuracy even without test
data. Figure 1 h) visualises the uncertainty of a few predictions made
for the protein G (1PGA) when mutation level cross-validation is used.
The estimated standard deviation allows a user to automatically iden-
tify low quality predictions that can appear e.g. in parts of the input
protein space from which less data is included in model training. Con-
versely, in order to minimise the amount of uncertainty in the mGPfu-
sion predictions, estimated standard deviation can be used to guide next
experiments. The probabilistic nature of the predictions also admits an
alternative error measure of negative log probability density (NLPD)
nlpd = − (cid:80)N
i=1 log p(yi|µ(xi), σ2(xi)), which can naturally take into
account the prediction variance.

3.4 Eﬀect of training set size

The results presented in Sections 3.1–3.3 used all available data for
training with cross-validation to obtain unbiased performance mea-
sures. The inclusion of thousands of simulated variants allows the model

to learn accurate models with less experimentally measured variants.
Hence, we study how the mGPfusion model with or without simulated
data performs with reduced number of experimental observations. To
facilitate this, we randomly selected subsets of experimental data of
size 0, 10, 20, and so on. We learned the mGP and mGPfusion models
with these reduced experimental data sets while always using the full
simulated data sets. This also allows us to estimate how the models
work with diﬀerent number of cross-validation folds. For example, the
point of a learning curve which utilises 2/3 or 4/5 of the training data
correspond to an average of multiple 3-fold or 5-fold cross-validations,
respectively.

The learning curve in Figure 4a) shows how the averaged correlation
for protein 2LZM improves when the size of the experimental data set
increases. The right-most values at N = 348 are obtained with leave-
one-out cross-validation. The inclusion of simulated data in mGPfusion
(dark blue line) consistently improves the performance of mGP, which
is trained without simulated data. Figure 4b) illustrate the diﬀerence
in root mean square error. Learning curves for all proteins listed in
Table 1 can be found from the Supplementary Figures S6-S8. When
the number of experimental samples is zero, the mGPfusion model is
trained solely using the simulated data with scaling 0.57yS, and the
mGP model predicts the stability eﬀect of every mutation as zero. The
last point on the learning curves is obtained with mutation level cross-
validation (see Tables 2 and S2).

proteins mGPfusion model trained with only one kernel that utilises
BLOSUM62, provides approximately as good results as the mGPfu-
sion model trained with multiple kernels. However, with many of the
proteins, utilising just BLOSUM62 does not seem to be suﬃcient and
the accuracy of the model can be improved by using diﬀerent substi-
tution matrices. Prior knowledge of appropriate substitution models
for each protein could enable creation of accurate prediction models
with just one substitution model, but the MKL seems to be a good
tool for selecting suitable substitution models when such knowledge is
not available. It seems that the data fusion and number or relevance
of used substitution matrices can compensate each other – the learning
curves show, that the diﬀerence between mGPfusion models trained
with one or multiple kernels is smaller than the diﬀerence between the
mGP models utilising one or multiple kernels. This indicates that if ad-
ditional simulated data is exploited, the use of multiple or appropriate
substitution models is not as important than without the data fusion.
On the other hand, if data fusion is not applied, the use of MKL can
more signiﬁcantly improve the accuracy of the mGP model.

3.6 Eﬀect of

the Bayesian transformation on

Rosetta

The Bayesian scaling of simulated Rosetta values, proposed in Sec-
tion 2.2, improves the match of Rosetta simulated values to empirical
∆∆G values even without using the Gaussian process framework. The
Bayesian scaling improves the performance of standard Rosetta sim-
ulations from ρ = 0.55 and rmse = 1.63 kcal/mol to ρ = 0.65 and
rmse = 1.35 kcal/mol (see Table 2 and Supplementary Table S2). This
shows that the scaling proposed by Kellogg et al. (2011) indeed is not
always the optimal scaling and signiﬁcant improvements can be gained
by optimising the scaling using a set of training data.

Figure 1 g) visualises the Bayesian scaling for protein 1PGA, where
the very destabilising ∆∆G values are dampened by the scaling (black
dots) to less extreme values by matching the scaled simulated values to
the experimental points (blue circles). The black dots along the scaling
curve indicate the grid of point mutations after transformation. The
scaling variance σ2
T is indicated by the green region’s vertical width,
and on the right panel. The scaling tends to dampen very small values
into less extreme stabilities, while it also estimates higher uncertainties
for stability values further away from ∆∆G = 0. However, the scalings
vary between diﬀerent proteins, as can be seen from the transformations
for each of the 15 proteins presented in Supplementary Figure S9.

Figure 4: a) Correlation and b) root mean square error of predictions made
by models with diﬀerent number of experimental training samples for T4
Lysozyme (2LZM). The results of Rosetta, mCSM and PoPMuSiC are in-
variant to training data (because mCSM and PoPMuSiC are pre-trained),
and are thus constant lines. For both ﬁgures, an average of 100 randomly
selected training sets is taken at each point.

3.5 Eﬀect of data fusion and multiple substitution

matrices

In the beginning of the learning curves, when only little training data is
available, mGPfusion quite consistently outperforms the mGP model,
demonstrating that the additional simulated data improves the predic-
tion accuracy. However, when more training data becomes available,
the performance of mGP model is almost as good or sometimes even
better than the performance of the mGPfusion model. This shows that
if enough training data is available, it is not necessary to simulate addi-
tional data in order to obtain accurate predictions. Table 2 also shows,
that the data fusion can compensate the lack of relevant training data
– with the mGPfusion models that utilise the additional data, the de-
crease in accuracy is smaller when position level cross-validation is used
instead of mutation level cross-validation, than with the mGP models.
The varying weights for the base kernels between diﬀerent pro-
teins (shown in Figure 2) already illustrated that diﬀerent proteins
beneﬁt from diﬀerent similarity measures for amino acid substitutions.
The learning curves also support this observation – with some of the

4 Conclusions

We present a novel method mGPfusion for predicting stability eﬀects of
both single and multiple simultaneous mutations. mGPfusion utilises
structural information in form of contact maps and integrates that with
amino acid residues and combines both experimental and comprehen-
In con-
sive simulated measurements of mutations’ stability eﬀects.
trast to earlier general-purpose stability models, mGPfusion model is
protein-speciﬁc by design, which improves the accuracy but necessitates
having a set of experimental measurements from the protein. In prac-
tise small datasets of 10–20 experimental observations were found to
provide state-of-the-art accuracy models when supplemented by large
simulation datasets.

An important advantage over most state-of-the-art machine learn-
ing methods is that mGPfusion is able to predict the eﬀects of multiple
simultaneous mutations in addition to single point mutations. Our
experiments show that mGPfusion is reliable in predicting up to six
simultaneous mutations in our dataset. Furthermore, the Gaussian
process framework provide a way to estimate the (un)certainty of the
predictions even without a separate test set. We additionally proposed
a novel Bayesian scaling method to re-calibrate simulated protein sta-
bility values against experimental observations. This is a crucial part of
the mGPfusion model, and also alone improved protein-speciﬁc Rosetta
stability predictions by calibrating them using experimental data.

mGPfusion is best suited for a situation, where a protein is thor-
oughly experimented on and accurate predictions for stability eﬀects

7

upon mutations are needed. It takes some time to set up the frame-
work and train the model, but after that new predictions can be made
in fractions of a second. The most time-consuming part is running the
simulations with Rosetta, at least when the most accurate protocol 16
is used. Simulating all 19 possible point mutations for one position
took about 12 hours, but simulations for diﬀerent positions can be run
on parallel. The time needed for training the prediction model depends
on the amount of experimental and simulated training data. With no
simulated data, the training time ranged from few seconds to few min-
utes. With data fusion and a single kernel, the training time was under
an hour. With data fusion and MKL with 21 kernels, the training time
was from a few minutes to a day.

Acknowledgements

We acknowledge the computational resources provided by the Aalto
Science-IT.

Funding

This work has been supported by the Academy of Finland Center of Ex-
cellence in Systems Immunology and Physiology, the Academy of Fin-
land grants no. 260403 and 299915, and the Finnish Funding Agency
for Innovation Tekes (grant no 40128/14, Living Factories).
References

Alberts, B., Johnson, A., Lewis, J., Raﬀ, M., Roberts, K., and Walter, P. (2007). Molec-

ular biology of the cell. Garland Science, 5 edition.

Anslyn, E. V. and Dougherty, D. A. (2006). Modern physical organic chemistry. Uni-

versity Science Books.

Berman, H. M., Westbrook, J., Feng, Z., Gilliland, G., Bhat, T., Weissig, H., Shindyalov,
I. N., and Bourne, P. E. (2000). The protein data bank. Nucleic acids research, 28(1),
235–242.

Bommarius, A. S., Blum, J. K., and Abrahamson, M. J. (2011). Status of protein en-
gineering for biocatalysts: how to design an industrially useful biocatalyst. Current
opinion in chemical biology, 15(2), 194–200.

Branden, C. and Tooze, J. (1999). Introduction to protein structure. Garland, 2 edition.

Giollo, M., Martin, A. J., Walsh, I., Ferrari, C., and Tosatto, S. C. (2014). NeEMO: a
method using residue interaction networks to improve prediction of protein stability
upon mutation. BMC genomics, 15(4), 1.

Henikoﬀ, S. and Henikoﬀ, J. G. (1992). Amino acid substitution matrices from protein
blocks. Proceedings of the National Academy of Sciences, 89(22), 10915–10919.

Kawashima, S., Pokarowski, P., Pokarowska, M., Kolinski, A., Katayama, T., and Kane-
hisa, M. (2008). AAindex: amino acid index database, progress report 2008. Nucleic
acids research, 36(suppl 1), D202–D205.

Kellogg, E. H., Leaver-Fay, A., and Baker, D. (2011). Role of conformational sampling
in computing mutation-induced changes in protein structure and stability. Proteins:
Structure, Function, and Bioinformatics, 79(3), 830–838.

Kirk, O., Borchert, T. V., and Fuglsang, C. C. (2002). Industrial enzyme applications.

Current opinion in biotechnology, 13(4), 345–351.

Kumar, M. S., Bava, K. A., Gromiha, M. M., Prabakaran, P., Kitajima, K., Uedaira,
H., and Sarai, A. (2006). ProTherm and ProNIT: thermodynamic databases for pro-
teins and protein–nucleic acid interactions. Nucleic Acids Research, 34(suppl 1),
D204–D206.

Leaver-Fay, A., Tyka, M., Lewis, S. M., Lange, O. F., Thompson, J., Jacak, R., Kaufman,
K., Renfrew, P. D., Smith, C. A., Sheﬄer, W., et al. (2011). ROSETTA3: an object-
oriented software suite for the simulation and design of macromolecules. Methods in
enzymology, 487, 545.

Liu, J. and Kang, X. (2012). Grading amino acid properties increased accuracies of single

point mutation on protein stability prediction. BMC bioinformatics, 13(1), 1.

Menchetti, S., Costa, F., and Frasconi, P. (2005). Weighted decomposition kernels. In
Proceedings of the 22nd international conference on Machine learning, pages 585–
592. ACM.

Pace, C. N. and Scholtz, J. M. (1997). Measuring the conformational stability of a pro-

tein. Protein structure: A practical approach, 2, 299–321.

Pace, C. N. and Shaw, K. L. (2000). Linear extrapolation method of analyzing solvent
denaturation curves. Proteins: Structure, Function, and Bioinformatics, 41(S4),
1–7.

Pires, D. E., Ascher, D. B., and Blundell, T. L. (2014a). DUET: a server for predicting
eﬀects of mutations on protein stability using an integrated computational approach.
Nucleic acids research, page gku411.

Pires, D. E., Ascher, D. B., and Blundell, T. L. (2014b). mCSM: predicting the eﬀects of
mutations in proteins using graph-based signatures. Bioinformatics, 30(3), 335–342.

Potapov, V., Cohen, M., and Schreiber, G. (2009). Assessing computational methods for
predicting protein stability upon mutation: good on average but not in the details.
Protein Engineering Design and Selection, 22(9), 553–560.

Capriotti, E., Fariselli, P., and Casadio, R. (2005a). I-Mutant2.0: predicting stability
changes upon mutation from the protein sequence or structure. Nucleic acids research,
33(suppl 2), W306–W310.

Rapley, R. and Walker, J. M. (2000). Molecular Biology and Biotechnology. Royal

Society of Chemistry, 4 edition.

Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian processes for machine learn-

Capriotti, E., Fariselli, P., Calabrese, R., and Casadio, R. (2005b). Predicting pro-
tein stability changes from sequences using support vector machines. Bioinformatics,
21(suppl 2), ii54–ii58.

ing. The MIT Press.

Capriotti, E., Fariselli, P., Rossi, I., and Casadio, R. (2008). A three-state prediction of

single point mutations on protein stability changes. BMC bioinformatics, 9(2).

Chen, C.-W., Lin, J., and Chu, Y.-W. (2013). iStable: oﬀ-the-shelf predictor integration

for predicting protein stability changes. BMC bioinformatics, 14(2).

Cheng, J., Randall, A., and Baldi, P. (2006). Prediction of protein stability changes for
single-site mutations using support vector machines. Proteins: Structure, Function,
and Bioinformatics, 62(4), 1125–1132.

Cherry, J. R. and Fidantsef, A. L. (2003). Directed evolution of industrial enzymes: an

update. Current opinion in biotechnology, 14(4), 438–443.

Cichonska, A., Ravikumar, B., Parri, E., Timonen, S., Pahikkala, T., Airola, A., Wenner-
berg, K., Rousu, J., and Aittokallio, T. (2017). Computational-experimental approach
to drug-target interaction mapping: A case study on kinase inhibitors. PLoS compu-
tational biology, 13(8), e1005678.

Sanchez, S. and Demain, A. L. (2010). Enzymes and bioconversions of industrial, pharma-
ceutical, and biotechnological signiﬁcance. Organic Process Research & Development,
15(1), 224–230.

Schmidt, M. W., Berg, E., Friedlander, M. P., and Murphy, K. P. (2009). Optimizing
costly functions with simple constraints: A limited-memory projected quasi-newton
algorithm. In International Conference on Artiﬁcial Intelligence and Statistics, page
None.

Shawe-Taylor, J. and Cristianini, N. (2004). Kernel methods for pattern analysis. Cam-

bridge university press.

Tian, J., Wu, N., Chu, X., and Fan, Y. (2010). Predicting changes in protein thermosta-
bility brought about by single- or multi-site mutations. BMC bioinformatics, 11(1),
1.

Tokuriki, N. and Tawﬁk, D. S. (2009). Stability eﬀects of mutations and protein evolv-

ability. Current opinion in structural biology, 19(5), 596–604.

Dehouck, Y., Grosﬁls, A., Folch, B., Gilis, D., Bogaerts, P., and Rooman, M. (2009). Fast
and accurate predictions of protein stability changes upon mutations using statistical
potentials and neural networks: PoPMuSiC-2.0. Bioinformatics, 25(19), 2537–2543.

Tomii, K. and Kanehisa, M. (1996). Analysis of amino acid indices and mutation matrices
for sequence comparison and structure prediction of proteins. Protein Engineering,
Design and Selection, 9(1), 27–36.

Folkman, L., Stantic, B., and Sattar, A. (2014). Feature-based multiple models improve
classiﬁcation of mutation-induced stability changes. BMC genomics, 15(Suppl 4).

Vishwanathan, S. V. N., Schraudolph, N. N., Kondor, R., and Borgwardt, K. M. (2010).

Graph kernels. The Journal of Machine Learning Research, 11, 1201–1242.

Giguere, S., Marchand, M., Laviolette, F., Drouin, A., and Corbeil, J. (2013). Learn-
ing a peptide-protein binding aﬃnity predictor with kernel ridge regression. BMC
bioinformatics, 14(1), 82.

Wainreb, G., Wolf, L., Ashkenazy, H., Dehouck, Y., and Ben-Tal, N. (2011). Protein
stability: a single recorded mutation aids in predicting the eﬀects of other mutations
in the same amino acid site. Bioinformatics, 27(23), 3286–3292.

8

Supplementary material

Figure S5: The stability of a protein is determined by the thermodynamic and kinetic stabilities, ∆Gt and ∆Gk, respectively. We only consider the
thermodynamic stability.

Figure S6: Priors presented by Equation 2. Here µd = −1.5, the most likely value for −a. Other hyperparameter values are presented in Table S4.

Table S4: Values for the hyperparameters used in the priors of a, b, c, d, σE and σS presented in Equations 2 and 6, respectively.

a

b

c

d

σE

σS

αa = 2
βa = 1.5

αb = 1.3
βb = 2

αc = 2
βc = 5

µd = −a
σd = 0.15

αE = 2.5
βE = 0.02

αS = 50
βS = 0.007

9

The partial derivatives of the marginal likelihood with respect to the parameters φ are obtained from Equation (9) as follows:

where α = K −1

φ y, Kφ is determined as

and the partial derivatives of Kφ with respect to the optimised parameters are

∂
∂φj

log p(y|X, φ) =

yT K −1

∂Kφ
∂φj

K −1

φ y −

(cid:19)

K −1
φ

∂Kφ
∂φj

1
2
1
2

φ
(cid:18)(cid:16)

=

tr

ααT − K −1

φ

(cid:18)

(cid:19)

,

tr

1
2
(cid:17) ∂Kφ
∂θj

Kφ =

wmK γm

m + diag





σ0
σE1NE
σE1NE + σS1NS + tσT


2



M
(cid:88)

m=1

∂Kφ
∂σE

∂Kφ
∂σR

∂Kφ
∂t

∂Kφ
∂wm
∂Kφ
∂γm







0
2σE1NE
2 (σE1NE + σS1NS + tσT )
0
0NE
2 (σE1NE + σS1NS + tσT )
0
0NE
2 (σE1NE + σS1NS + t) σT













=diag



=diag



=diag



=K γm
m

=wmK γm

m log Km

(cid:80)N∗

i=1(yi − ¯y)(µ(xi) − ¯µ)

i=1(yi − ¯y)2 (cid:80)N∗

i=1(µ(xi) − ¯µ)2

ρ =

(cid:113)(cid:80)N∗
(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
N∗

N∗(cid:88)

i=1

rmse =

(yi − µ(xi))2,

Correlation ρ and root-mean-square error rmse for the predictions are determined as

(S11)

(S12)

(S13)

(S14)

(S15)

(S16)

(S17)

(S18)

where ¯y is the mean of the experimentally measured values, µ(xi) is prediction mean, ¯µ is the average of all prediction means, and N∗ is the
number of predictions.

10

Figure S7: Mutation-level predictions for all 15 proteins presented in Table 1. The predictions are coloured by the number of simultaneous mutations.

11

Figure S8: Position-level predictions for all 15 proteins. When the eﬀects of a mutant are predicted multiple times, they are connected by a line.

12

Table S5: (Continues on the next page)Comparison of diﬀerent methods on the 15 protein dataset with respect to ρ and rmse. Oﬀ-the-shelf implementations
of Rosetta, mCSM and PoPMuSiC are used directly without cross-validation.

Protein Method

2LZM

1BNI

1VQB

1LZ1

2CI2

1PGA

2RN2

1CSP

mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC

0.58
0.59
-
-
-

0.75
0.73
0.75
0.71
0.57

0.49
0.50
-
-
-

0.67
0.65
0.79
0.79
0.47

0.62
0.62
-
-
-

0.77
0.74
0.81
0.61
0.59

Point mutations
cross-validation level
prot.
pos.
mut.
0.75
0.80
0.87
0.75
0.77
0.86
-
0.59
0.86
-
0.37
0.75
0.73
-
0.74
0.75
0.57
0.71
0.64
0.61
0.65
0.48
0.58
0.62
0.60
0.66
0.50
0.53
0.12
0.29
0.46
0.49
0.53
0.51
0.59
0.56
0.39
-0.31
0.55
0.59
0.67
0.64
0.72
0.67
0.61
0.74
0.60
0.63
0.74
0.75
0.47
0.59
0.61
-0.46
0.59
0.69
-0.10
0.28
0.58
0.59
0.12
0.09
0.64
0.70
0.71
0.71
0.23
0.22
-0.06
-0.37
0.20
0.33
0.42
0.48

0.85
0.86
0.88
0.87
0.23

0.33
0.34
-
-
-

0.69
0.71
-
-
-

0.71
0.71
-
-
-

0.79
0.82
0.77
0.83
0.66

0.68
0.82
0.62
0.57
0.69

0.73
0.69
0.65
0.51
0.60

0.64
0.63
-
-
-

0.75
0.73
-
-
-

0.39
0.40
-
-
-

0.93
0.91
0.96
0.97
0.71

0.86
0.84
0.86
0.87
0.17

-1.00
-1.00
-
-
-

1.00
1.00
1.00
1.00
-1.00

Correlation ρ
Multiple mutations
cross-validation level
prot.
pos.
mut.
0.64
0.83
0.96
0.64
0.87
0.96
-
0.85
0.97
-
0.77
0.94
0.66
-
0.68
0.68
-
-
0.70
0.79
0.82
0.85
0.29
0.18
-
-
0.83
0.82
0.70
0.75
0.68
0.73
-
-
0.11
0.05
0.56
0.42
-1.00
-1.00
-
-
0.87
0.86
0.79
0.71
0.61
0.62
-
-
0.35
0.60
-0.24
-0.08
0.07
0.03
-
-
0.60
0.60
0.42
0.42
0.50
0.47
-
-
0.73
0.69
0.75
0.71
0.69
0.68
-
-

0.92
0.91
0.94
0.92
0.68

0.78
0.76
0.75
0.77
0.48

0.90
0.76
0.93
0.92
0.09

0.48
0.59
-
-
-

0.61
0.62
-
-
-

0.32
0.62
-
-
-

0.95
0.92
0.92
0.92
0.61

0.85
0.86
-
-
-

0.77
0.76
0.78
0.74
0.53

0.76
0.75
0.85
0.85
0.57

0.77
0.74
0.80
0.63
0.53

0.52
0.53
-
-
-

0.60
0.58
-
-
-

0.57
0.57
-
-
-

All mutations
cross-validation level
prot.
pos.
mut.
0.68
0.76
0.90
0.69
0.82
0.90
-
0.72
0.90
-
0.61
0.82
0.65
-
0.70
0.71
-
-
0.55
0.60
0.63
0.41
0.45
0.56
-
-
0.69
0.69
0.50
0.55
0.59
0.59
-
-
0.57
0.56
0.47
0.21
0.46
0.55
-
-
0.81
0.79
0.72
0.63
0.63
0.65
-
-
0.43
0.53
-0.14
0.06
0.11
0.28
-
-
0.53
0.53
0.22
0.23
0.57
0.65
-
-
0.75
0.72
0.77
0.72
0.64
0.60
-
-

0.38
0.54
-
-
-

0.50
0.70
-
-
-

0.70
0.70
-
-
-

0.92
0.91
0.94
0.92
0.59

0.79
0.81
0.77
0.82
0.62

0.85
0.81
0.84
0.81
0.24

0.71
0.72
-
-
-

0.82
0.79
0.76
0.68
0.63

1.06
1.06
-
-
-

0.83
0.86
0.81
0.91
0.99

2.25
2.25
-
-
-

1.71
1.75
1.41
1.50
1.99

1.69
1.69
-
-
-

1.21
1.27
1.08
1.61
1.58

mut.
0.82
0.84
0.82
1.12
1.05

Point mutations
cross-validation level
prot.
pos.
1.12
1.02
1.12
1.08
-
1.34
-
1.93
1.06
-
1.13
1.27
1.11
1.37
1.45
1.40
2.32
1.58
1.70
1.62
1.53
1.94
1.94
2.41
2.89
2.00
2.26
2.24
2.29
0.99
1.03
1.15
1.43
1.01
1.04
0.97
0.95
0.90
0.97
1.02
1.39
1.00
1.09
0.86
0.85
1.54
1.22
1.58
1.73
1.42
1.70
1.94
1.89
1.21
1.22
1.45
1.45
1.13
1.07
1.04
1.16
1.04
1.04
1.07
1.12
1.06
1.11
1.02
0.99

1.10
1.10
-
-
-

0.64
0.65
0.54
0.60
1.04

1.26
0.87
1.40
1.53
1.21

1.64
1.06
-
-
-

0.91
0.87
0.93
0.82
1.09

1.05
1.59
-
-
-

0.85
0.91
0.95
1.16
1.00

1.07
1.07
-
-
-

0.56
1.10
0.13
0.27
3.23

1.15
1.35
0.70
0.63
1.90

1.28
1.24
1.32
1.15
2.51

3.75
3.51
-
-
-

2.06
2.43
-
-
-

2.17
2.14
-
-
-

mut.
0.57
0.59
0.48
0.78
1.65

rmse
Multiple mutations
cross-validation level
prot.
pos.
2.14
1.53
2.11
1.16
-
1.24
-
1.44
1.84
-
1.93
-
-
2.49
2.03
2.00
1.67
2.90
2.33
-
-
1.62
1.82
1.97
2.30
1.96
2.06
-
-
2.40
2.41
1.65
2.36
3.25
3.41
-
-
0.80
1.01
1.01
1.40
1.27
1.30
-
-
2.09
0.95
3.01
2.07
3.09
3.51
-
-
1.01
0.95
1.21
1.20
1.08
1.25
-
-
1.66
1.87
1.51
1.59
2.29
1.92
-
-

2.58
2.13
-
-
-

2.74
0.88
-
-
-

0.91
1.18
-
-
-

1.21
1.12
-
-
-

0.91
0.96
0.76
0.86
2.19

0.53
0.69
0.45
0.47
2.81

0.67
0.69
0.74
0.68
1.20

0.55
0.71
0.66
0.71
1.27

All mutations
cross-validation level
prot.
1.43
1.42
-
-
-

mut.
0.76
0.79
0.75
1.05
1.23

pos.
1.30
1.13
1.29
1.70
1.51

1.22
1.27
1.11
1.57
1.70

1.59
1.66
1.27
1.33
1.97

0.83
0.87
0.80
0.90
1.07

0.80
0.87
0.90
1.08
1.06

0.88
0.83
0.94
1.02
2.33

0.86
0.83
0.89
0.80
1.12

0.75
0.78
0.63
0.71
1.58

1.37
-
-
1.67
1.60
1.55
2.20
1.94

1.77
-
-
1.82
1.89
2.24
2.66
1.99

2.21
-
-
1.07
1.10
1.17
1.47
1.15

1.12
-
-
0.86
0.99
1.02
1.39
1.11

1.13
-
-
2.00
1.13
2.81
2.01
2.87

2.95
-
-
1.14
1.13
1.36
1.36
1.11

1.11
-
-
1.45
1.60
1.36
1.43
1.92

1.47
-
-

1.75
1.75
-
-
-

2.20
2.30
-
-
-

1.16
1.14
-
-
-

1.10
1.08
-
-
-

2.38
1.02
-
-
-

1.02
1.51
-
-
-

1.80
1.57
-
-
-

13

Table S5: (Continued) Comparison of diﬀerent methods on the 15 protein dataset with respect to ρ and rmse. Oﬀ-the-shelf implementations of Rosetta,
mCSM and PoPMuSiC are used directly without cross-validation.

Protein Method

1BVC

4LYZ

1RTB

1PIN∗

1RN1

1RGG

1BPI

total

mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC

0.34
0.34
-
-
-

0.61
0.59
0.65
0.62
0.29

0.53
0.53
-
-
-

0.64
0.59
0.61
0.23
0.50

0.70
0.70
-
-
-

0.92
0.89
0.92
0.91
0.65

Point mutations
cross-validation level
prot.
pos.
mut.
0.48
0.43
0.41
0.48
0.48
0.50
-
-0.12
-0.05
-
0.06
-0.05
-
0.40
0.42
0.47
0.47
0.60
0.27
0.30
-0.05
0.27
0.28
0.33
0.55
0.59
0.81
0.79
0.69
0.58
0.61
0.69
0.68
0.72
0.49
0.51
0.40
0.25
0.49
0.53
0.72
0.60
0.62
0.58
0.04
0.27
0.57
0.67
0.76
0.62
0.51
0.52
-0.61
-0.53
0.43
0.60
0.77
0.65
0.67
0.65
0.57
0.58
0.52
0.51
0.71
0.72
0.70
0.69
0.51
0.34
0.63
0.55
0.61
0.64

0.67
0.67
-
-
-

0.83
0.81
0.74
0.77
0.63

0.68
0.64
0.56
0.43
0.52

0.69
0.69
0.71
0.72
0.58

0.61
0.61
-
-
-

0.51
0.51
-
-
-

0.81
0.79
0.81
0.76
0.65

0.56
0.56
-
-
-

Correlation ρ
Multiple mutations
cross-validation level
prot.
pos.
mut.
-0.63
-0.25
-0.28
-0.66
-0.25
-0.25
-
-0.21
-0.10
-
-0.21
-0.11
-
-0.57
-0.63
-0.65
-
-
0.57
0.64
0.46
0.47
0.68
0.71
-
-
0.74
0.73
0.30
0.58
0.73
0.73
-
-
-

0.78
0.73
0.78
0.48
0.73

0.95
0.96
0.96
0.97
0.70

0.75
0.67
-
-
-

0.66
0.64
-
-
-

-
-
-
-
-

-0.19
-0.01
-
-
-

0.96
0.93
-
-
-

-0.06
0.21
-
-
-

0.49
0.50
-
-
-

-
-
-
-
-

0.97
0.96
0.99
0.99
0.21

0.42
0.22
0.19
0.08
0.78

-0.39
-0.53
-0.29
-0.55
0.23

0.88
0.86
0.86
0.86
0.51

-

-

-
-

-
-
-
0.38
0.42
0.62
0.50
0.18
0.20
-
-
0.66
0.30
0.08
0.07
0.86
0.77
-
-
-0.02
0.33
0.52
0.46
-0.00
-0.00
-
-
0.61
0.64
0.52
0.55
0.39
0.40
-
-

0.65
0.64
0.68
0.64
0.33

0.64
0.59
0.61
0.23
0.50

0.86
0.86
0.76
0.85
0.67

0.35
0.35
-
-
-

0.53
0.53
-
-
-

0.68
0.69
-
-
-

All mutations
cross-validation level
prot.
pos.
mut.
0.09
-0.09
0.08
0.14
-0.07
0.14
-
-0.23
-0.13
-
-0.20
-0.14
-
-0.09
0.09
0.14
-
-
0.34
0.40
0.18
0.21
0.34
0.35
-
-
0.71
0.75
0.18
0.16
0.65
0.70
-
-
0.49
0.51
0.40
0.25
0.49
0.53
0.72
0.60
0.55
0.51
0.04
0.10
0.54
0.65
-
-
0.52
0.33
-0.05
-0.00
0.51
0.39
-
-
-0.13
0.05
0.84
0.70
-0.08
0.13
-
-
0.64
0.66
0.50
0.49
0.48
0.49
-
-

0.12
0.12
-
-
-

0.65
0.66
-
-
-

0.73
0.55
-
-
-

0.52
0.52
-
-
-

0.84
0.81
0.75
0.78
0.61

0.52
0.40
0.35
0.30
0.42

0.85
0.81
0.83
0.76
0.08

0.83
0.82
0.83
0.80
0.60

3.46
3.50
-
-
-

1.47
1.48
1.38
1.48
2.26

1.00
1.00
-
-
-

0.47
0.50
0.49
0.70
0.54

2.44
2.44
-
-
-

1.25
1.48
1.26
1.67
1.99

mut.
0.74
0.70
0.99
0.99
0.75

Point mutations
cross-validation level
prot.
pos.
1.65
0.72
1.65
0.71
-
1.00
-
0.99
-
0.76
1.67
1.00
0.85
1.65
1.64
1.78
1.84
2.22
3.61
1.43
1.45
1.73
1.85
2.62
3.37
2.08
2.45
2.33
2.22
0.53
0.54
0.58
0.76
0.54
0.99
0.59
0.70
1.17
1.24
1.75
1.78
1.18
1.20
0.97
1.14
1.70
1.68
2.24
2.01
1.76
1.58
1.36
1.54
1.28
1.32
1.43
1.73
1.52
1.80
1.26
1.31
1.26
1.30
1.54
1.95
1.38
1.63
1.40
1.37

0.85
0.92
1.06
1.00
1.12

1.18
1.18
-
-
-

1.41
1.42
1.38
1.53
1.57

1.56
1.56
-
-
-

1.27
1.32
1.43
1.68
1.52

1.61
1.62
-
-
-

1.83
1.82
-
-
-

1.07
1.11
1.04
1.26
1.35

0.69
0.57
0.36
0.31
1.22

2.35
2.17
-
-
-

mut.
1.64
1.62
1.30
1.29
1.65

rmse
Multiple mutations
cross-validation level
prot.
pos.
2.70
1.48
2.43
1.47
-
1.35
-
1.33
-
1.45
2.45
-
-
1.02
0.93
1.15
1.16
1.29
2.05
-
-
2.24
1.81
3.39
2.11
1.62
2.01
-
-
-
-
-
-
-

1.71
1.90
-
-
-

2.22
1.82
3.44
2.12
1.63

-
-
-
-
-

-
-
-
-
-

0.19
0.35
0.12
0.13
0.98

5.67
6.21
6.81
6.89
7.08

2.17
2.86
1.72
2.52
7.43

1.33
1.43
1.44
1.45
2.49

-
-
0.91
0.77
0.58
0.82
1.01
1.24
-
-
5.95
6.14
6.88
6.77
7.24
6.90
-
-
7.66
7.07
4.19
6.58
7.43
7.68
-
-
2.45
2.40
2.65
2.56
2.99
2.74
-
-

1.28
0.99
-
-
-

4.20
5.43
-
-
-

7.69
7.70
-
-
-

2.53
2.50
-
-
-

All mutations
cross-validation level
prot.
2.02
1.92
-
-
-

mut.
1.09
1.06
1.09
1.09
1.10

pos.
1.21
1.20
1.21
1.20
1.20

1.35
1.35
1.24
1.33
2.09

1.40
1.53
1.69
1.73
1.95

0.47
0.50
0.49
0.70
0.54

0.81
0.88
1.01
0.95
1.10

2.65
2.85
3.05
3.14
3.23

1.40
1.57
1.47
1.80
2.88

1.13
1.18
1.14
1.30
1.66

1.94
-
-
1.45
1.43
1.59
1.63
1.94

3.35
-
-
1.85
1.84
2.81
3.14
1.99

2.40
-
-
0.53
0.54
0.58
0.76
0.54

0.99
0.59
0.70

1.13
1.17
1.61
1.66
1.15

1.20
-
-
4.75
4.90
5.53
5.41
5.74

3.16
-
-
3.64
3.40
2.28
3.34
3.62

3.09
-
-
1.87
1.85
2.09
2.23
2.22

1.92
-
-

3.26
3.27
-
-
-

2.36
2.38
-
-
-

1.00
1.00
-
-
-

1.19
1.17
-
-
-

2.23
2.64
-
-
-

3.11
3.11
-
-
-

1.84
1.84
-
-
-

∗ dataset for 1PIN contained no multiple mutations.

14

Table S6: Comparison of diﬀerent methods on the 15 protein dataset with respect to ρ and rmse after removing 10% of predictions with largest errors.
Mutation, position, and protein are referred to as mut., pos., and prot., respectively. Oﬀ-the-shelf implementations of Rosetta, mCSM and PoPMuSiC are
used directly without cross-validation.

Results after 10 %
outlier removal

Method
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled

Point mutations
cross-validation level
prot.
pos.
0.75
0.77
0.75
0.73
-
0.57
-
0.27
-
0.78

mut.
0.87
0.86
0.89
0.86
0.80

Correlation ρ
Multiple mutations
cross-validation level
prot.
pos.
0.63
0.84
0.65
0.84
-
0.76
-
0.76
-
0.71

mut.
0.97
0.96
0.98
0.97
0.73

Oﬀ-the-shelf implementations with no cross-validation

All mutations
cross-validation level
prot.
pos.
0.71
0.81
0.72
0.80
-
0.74
-
0.66
-
0.75

mut.
0.92
0.91
0.93
0.91
0.78

Point mutations
cross-validation level
prot.
pos.
0.93
0.85
0.93
0.85
-
1.01
-
1.30
-
0.87

mut.
0.69
0.69
0.65
0.73
0.84

rmse
Multiple mutations
cross-validation level
prot.
pos.
1.90
1.55
1.85
1.43
-
1.66
-
1.48
-
1.99

mut.
0.62
0.64
0.49
0.54
1.67

All mutations
cross-validation level
prot.
1.13
1.12
-
-
-

mut.
0.67
0.67
0.62
0.69
1.00

pos.
1.15
1.10
1.23
1.37
1.31

Rosetta
mCSM
PoPMuSiC

0.75
0.71
0.73

0.67
-
-

0.73
-
-

0.94
0.89
0.86

1.85
-
-

1.11
-
-

Figure S9: rmse with diﬀerent amount of predictions, when predictions are sorted by the error. Position level cross-validation was used for mGPfusion,
mGP and Rosetta scaled.

15

Figure S10: Learning curves.

16

Figure S11: Learning curves.

17

Figure S12: Learning curves.

18

Figure S13: Transformations for all 15 proteins presented in Table 1. The red circles mark the simulated ∆∆G-values yS with respect to the experimental
measured ∆∆G-values yE. Thin black lines show possible transformations for yS, whereas the thick black line shows the selected transformation from yS
to ˜yS.

19

mGPfusion: Predicting protein stability changes with
Gaussian process kernel learning and data fusion

Emmi Jokinen1, Markus Heinonen1,2 and Harri L¨ahdesm¨aki1
1Department of Computer Science, Aalto University, 02150 Espoo, Finland
2Helsinki Institute for Information Technology, Finland

Abstract

Motivation: Proteins are commonly used by biochemical industry for numerous processes. Reﬁning these proteins’ properties via
mutations causes stability eﬀects as well. Accurate computational method to predict how mutations aﬀect protein stability are necessary
to facilitate eﬃcient protein design. However, accuracy of predictive models is ultimately constrained by the limited availability of
experimental data.
Results: We have developed mGPfusion, a novel Gaussian process (GP) method for predicting protein’s stability changes upon single
and multiple mutations. This method complements the limited experimental data with large amounts of molecular simulation data.
We introduce a Bayesian data fusion model that re-calibrates the experimental and in silico data sources and then learns a predictive
GP model from the combined data. Our protein-speciﬁc model requires experimental data only regarding the protein of interest and
performs well even with few experimental measurements. The mGPfusion models proteins by contact maps and infers the stability
eﬀects caused by mutations with a mixture of graph kernels. Our results show that mGPfusion outperforms state-of-the-art methods in
predicting protein stability on a dataset of 15 diﬀerent proteins and that incorporating molecular simulation data improves the model
learning and prediction accuracy.
Availability: Software implementation and datasets are available at github.com/emmijokinen/mgpfusion
Contact: emmi.jokinen@aalto.ﬁ

8
1
0
2
 
r
a

M
 
3
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
2
5
8
2
0
.
2
0
8
1
:
v
i
X
r
a

1 Introduction

Proteins are used in various applications by pharmaceutical, food, fuel,
and many other industries and their usage is growing steadily (Kirk
et al., 2002; Sanchez and Demain, 2010). Proteins have important ad-
vantages over chemical catalysts, as they are derived from renewable
resources, are biodegradable and are often highly selective (Cherry and
Fidantsef, 2003). Protein engineering is used to further improve the
properties of proteins, for example to enhance their catalytic activity,
modify their substrate speciﬁcity or to improve their thermostability
(Rapley and Walker, 2000). Increasing the stability is an important as-
pect of protein engineering, as the proteins used in industry should be
stable in the industrial process conditions, which often involve higher
than ambient temperature and non-aqueous solvents (Bommarius et al.,
2011). The properties of a protein are modiﬁed by introducing alter-
ations to its amino acid sequence. Mutations in general tend to be
destabilising, and if too many destabilising mutations are implemented,
the protein may not remain functional without compensatory stabilis-
ing mutations (Tokuriki and Tawﬁk, 2009).

The stability of a protein can be deﬁned as the diﬀerence in Gibbs
energy ∆G between the folded and unfolded (or native and denatu-
rated) state of the protein. More precisely, the Gibbs energy diﬀerence
determines the thermodynamic stability ∆Gt of the protein, as it does
not take into account the kinetic stability ∆Gk which determines the
energy needed for the transition between the folded and unfolded states
(Anslyn and Dougherty, 2006) (see Supplementary Figure S1). Here we
will consider only the thermodynamic stability and from now on it will
be referred to merely as stability ∆G.

The eﬀect of mutations can be deﬁned by the change they cause
to the Gibbs energy ∆G, denoted as ∆∆G (Pace and Scholtz, 1997).
To comprehend the signiﬁcance of stability changes upon mutations,
we can consider globular proteins, the most common type of enzymes,
whose polypeptide chain is folded up in a compact ball-like shape with
an irregular surface (Alberts et al., 2007). These proteins are only
marginally stable and the diﬀerence in Gibbs energy between the folded
and unfolded state is only about 5–15 kcal/mol, which is not much more
than the energy of a single hydrogen bond that is about 2–5 kcal/mol
(Branden and Tooze, 1999). Therefore, even one mutation that breaks
a hydrogen bond can prevent a protein from folding properly.

The protein stability can be measured with many techniques, in-
cluding thermal, urea and guanidinium chloride (GdmCl) denaturation

curves that are determined as the fraction of unfolded proteins at dif-
ferent temperatures or at diﬀerent concentrations of urea or GdmCl
(Pace and Shaw, 2000). Some of the experimentally measured sta-
bility changes upon mutations have been gathered in thermodynamic
databases such as Protherm (Kumar et al., 2006).

A variety of computational methods have been introduced to predict
the stability changes upon mutations more eﬀortlessly than through ex-
perimental measurements. These methods utilise physics or knowledge-
based potentials (Leaver-Fay et al., 2011), their combinations, or diﬀer-
ent machine learning methods. The machine learning methods utilise
support vector machines (SVM) (Capriotti et al., 2005b, 2008; Chen
et al., 2013; Cheng et al., 2006; Folkman et al., 2014; Liu and Kang,
2012; Pires et al., 2014a), random forests (Tian et al., 2010; Wainreb
et al., 2011), neural networks (Dehouck et al., 2009; Giollo et al., 2014),
and Gaussian processes (Pires et al., 2014b). However, it has been as-
sessed that although on average many of these methods provide good
results, they tend to fail on details (Potapov et al., 2009). In addition,
many of these methods are able to predict the stability eﬀects only for
single-point mutations.

We introduce mGPfusion (mutation Gaussian Processes with data
fusion), a method for predicting stability eﬀects of both point and mul-
tiple mutations. mGPfusion is a protein-speciﬁc model – in contrast to
earlier stability predictors that aim to estimate arbitrary protein struc-
ture or sequence stabilities – and achieves markedly higher accuracy
while utilising data only from a single protein at a time. In contrast
to earlier works that only use experimental data to train their models,
we also combine exhaustive Rosetta (Leaver-Fay et al., 2011) simulated
point mutation in silico stabilities to our training data.

A key part of mGPfusion is the automatic scaling of simulated data
to better match the experimental data distribution based on those vari-
ants that have both experimental and simulated stability values. Fur-
thermore, we estimate a variance resulting from the scaling, which
places a higher uncertainty on very destabilising simulations. Our
Gaussian process model then utilises the joint dataset with their es-
timated heteroscedastic variances and uses a mixture of graph kernels
to assess the stability eﬀects caused by changes in amino acid sequence
according to 21 substitution models. Our experiments on a novel 15
protein dataset show a state-of-the-art stability prediction performance,
which is also sustained when there is access only to a very few experi-
mental stability measurements.

1

Figure 1: Pipeline illustration for mGPfusion. a) M = 21 substitution matrices utilise diﬀerent information sources and give scores to pairwise amino
acid substitutions. b) The wild-type structures from Protein Data Bank are modelled as contact graphs. c) The graph kernel measures similarity of two
sequences by a substitution model S over all positions p and their neighbourhoods nbs(p) in the contact graph. d) Each substitution matrix is used to
create a separate covariance matrix. e) Multiple kernel learning (MKL) is used for ﬁnding the optimal combination of the base kernels. The kernel matrix
measures variant similarities. f ) Experimentally measured ∆∆G values yE are gathered from Protherm and Rosetta’s ddg monomer application is used to
simulate the stability eﬀects yS for all single point mutations. g) Bayesian scaling for the simulated values yS at the x-axis. Possible scalings are coloured
with green and the chosen scaling from yS into scaled values ˜yS is marked by black dots. The scaling is ﬁtted to a subset of experimentally measured
stabilities yE (circles). h) The stability predictive GP model is trained using experimental and simulated data through the kernel matrix.

2 Methods

Following Pires et al. (2014b) we choose a Bayesian model family of
Gaussian processes for prediction of mutation eﬀects on protein sta-
bility due to its inherent ability to handle uncertainty in a principle
way. Bayesian modelling is a natural approach for combining the ex-
perimental and simulated data distribution, while it is also suitable for
learning the underlying mixture of substitution models that governs the
mutational process.

The pipeline for mGPfusion is presented in Figure 1. The ﬁrst
part of mGPfusion consists of collection of in silico and experimental
datasets discussed in Section 2.1, the scaling of the in silico dataset in
Section 2.2 and the fusion of these two datasets in Section 2.3. The sec-
ond part consists of the Gaussian process model described in Section 2.4
with detailed description of the graph kernels in Sections 2.5-2.6 and
model inference in Section 2.7. Finally, the evaluation criteria used are
described in Section 2.8.

teins that fulﬁlled these requirements are listed in Table 1. We aver-
aged the stability values for proteins with multiple measurements and
ignored mutations to residues not present in their 3D structures. These
data sets are available at github.com/emmijokinen/mgpfusion.

Protein (organism)

PDB

mutations
point

point (sim)

all

2LZM 349
T4 Lysozyme (Enterobacteria phage T4)
182
1BNI
Barnase (Bacillus amyloliquefaciens)
124
1VQB
Gene V protein (Escherichia virus m13)
116
1LZI
Glycosyltransferase A (Homo sapiens)
98
2CI2
Chymotrypsin inhibitor 2 (Hordeum vulgare)
89
1PGA
Protein G (Streptococcus sp. gx7805)
83
2RN2
Ribonuclease H (Escheria coli)
80
1CSP
Cold shock protein B (Bacillus subtilis)
80
1BVC
Apomyoglobin (Physeter catodon)
63
4LYZ
Hen egg white lysozyme (Gallus gallus)
57
Ribonuclease A (Bos taurus)
1RTB
56
Peptidyl-prolyl cis-trans isomerase (Homo sapiens) 1PIN
53
1RN1
Ribonuclease T1 isozyme (Aspergillus oryzae)
54
1RGG
Ribonuclease (Streptomyces auerofaciens)
53
1BPI
Bovine pancreatic trypsin inhibitor (Bos taurus)

264
163
92
114
77
34
65
50
56
50
50
56
48
45
47

3116
2052
1634
2470
1235
1064
2945
1273
2907
2451
2356
2907
1957
1824
1102

2.1 Experimental and in silico data

total

1537

1211

31293

Protherm is a database of numerical thermodynamic parameters for
proteins and their mutants (Kumar et al., 2006). From Protherm we
gathered all proteins with at least 50 unique mutations whose ∆∆G
has been measured by thermal denaturation, and where a PDB code
for a 3D structure of the protein was reported. We required the pro-
teins to have at least 50 unique mutations, so that we would have a
representative test set and get suﬃciently reliable estimates of predic-
tion accuracy on individual proteins and examine how the amount of
experimental training data aﬀects the accuracy of the model. The 3D
structures are necessary for obtaining the connections between residues.
We collected the 3D structures with the reported PDB codes from the
Protein Databank, www.rcsb.org (Berman et al., 2000). The 15 pro-

Table 1: The 15 protein data from ProTherm database with counts of point
mutations, all mutations, and of simulated point mutation stability changes.

We also generate simulated data of the stability eﬀects of all possible
single mutations of the proteins. Our method can utilise any simulated
stability values. We used the “ddG monomer” application of Rosetta
3.6 (Leaver-Fay et al., 2011) using the high-resolution backrub-based
protocol 16 recommended in Kellogg et al. (2011). The predictions yS
made with Rosetta are given in Rosetta Energy Units (REU). Kellogg
et al. (2011) suggest transformation 0.57yS for converting the predic-
tions into physical units. The simulated data scaled this way is not
as accurate as the experimental data, the correlation and root mean

2

square error (rmse) with respect to the experimental data are shown
for all proteins in Table 2 and for individual proteins in Supplementary
Table S2, on rows labelled Rosetta. For this reason, we use instead a
Bayesian scaling described in the next section and diﬀerent noise models
for the experimental and simulated data, described in Section 2.3.

For each of the 15 proteins,

let xi = (xi1, . . . , xiM ) denote its
M -length variant i with positions p labelled with residues xip ∈
{A, R, N, . . . , V }. We denote the wild-type protein as x0. We collect
15 separate sets of simulated and experimental data. We denote the
NE )T with
NE experimental variants of each protein as XE = (xE
the corresponding experimental stability values yE = (yE
NE )T ∈
RNE . Similarly, we denote the NS simulated observations as XS =
(xS

1 , . . . , xE

1 , . . . , yE

NS )T and yS = (yS

NS )T ∈ RNS .

1 , . . . , xS

1 , . . . , yS

2.2 Bayesian scaling of in silico data

The described transformation from REU to physical units may not
be optimal for all proteins. We therefore applied instead a linear-
exponential scaling function to obtain scaled Rosetta simulated sta-
bilities ˜yS,

˜yS = g(yS | θj) = ajecj yS

+ bjyS + dj.

(1)

This scaling transforms the Rosetta simulations yS for each protein
j = 1, . . . , 15 to correspond better to the experimental data. The pa-
rameters θj = (aj, bj, cj, dj) deﬁne the weight aj and steepness cj of the
exponential term, while the linear term has slope bj and intercept dj.
To avoid overﬁtting, we perform Bayesian linear regression and start
by deﬁning parameter prior p(θj) = p(aj)p(bj)p(cj)p(dj) that reﬂects
our beliefs about realistic scalings having only moderate steepness:

and stabilities y = (y0, yE, ˜yS) of size RN where N = 1 + NE + NS is
the total number of simulated and experimental data points, including
the wild-type. We assume heteroscedastic additive noise models for the
three information sources

y0 = f (x0) + ε0,
(cid:1) + εE
i = f (cid:0)xE
yE
i ,
(cid:1) + εS
i = f (cid:0)xS
˜yS
i ,

i

i

(cid:1)

(cid:1)

0

ε0 ∼ N (cid:0)0, σ2
i ∼ N (cid:0)0, σ2
εE
(cid:16)
εS
i ∼ N

E

0, (σE + σS + tσT (i))2(cid:17)

,

(5)

where the observed values are noisy versions of the underlying ‘true’
stability function f (x) corrupted by zero-mean noise with data source
speciﬁc variances. We learn a Gaussian process based stability function
f (x) in the next Section.

The Equations (5) encode that the experimental data are corrupted
by a global experimental noise variance σ2
E. The simulated stabilities
are additionally corrupted by a global Rosetta simulator error variance
σ2
S, and by the value-dependent transformation variance tσ2
T (i) scaled
by parameter t. The model then encapsulates that we trust the Rosetta
data less than the experimental data. By deﬁnition, the ∆∆G of the
wild-type is zero (y0 = 0) with very small assumed error, σ0 = 10−6.
Note that σ2
T are ﬁxed by equation (4), while we infer the optimal
values for the remaining three free parameters (σE, σR, t) (See Section
2.4). The parameters σ2

S are assigned priors

E and σ2

σE ∼ Gamma(σE|αE, βE)
σS ∼ Gamma(σR|αS, βS).

(6)

The values of these hyperparameters are shown in Supplementary Ta-
ble S1.

p(aj) = Gamma(aj | αa, βa)
p(bj) = Beta(1/2 · bj | αb, βb)
p(cj) = Beta(10/3 · cj | αc, βc)
p(dj) = N (dj | µd, σ2

d).

2.4 Gaussian processes

(2)

We use a Gaussian process (GP) function f to predict the stability
f (x) ∈ R of a protein variant x. Gaussian processes are a family of
non-parametric, non-linear Bayesian models (Rasmussen and Williams,
2006). A zero-mean GP prior

The empirically selected hyperparameter values are listed in Supple-
mentary Table S1 and the priors are illustrated in Figure S2.

We compute the posterior for θj using the subset of simulated data

that have corresponding experimentally measured data:

p(θj|yE, yS) ∝

(cid:89)

N (cid:0)yE

i

| g(yS

i |θj), σ2
n

(cid:1) p(θj).

i:xi∈XE ∩XS

The product iterates over all NE∩S simulated ∆∆G’s that have a
matching experimentally observed value. The σ2
n is the scaling error
variance, which was set to σ2
n = 0.5. The parameters θ for each protein
were sampled using a random walk Metropolis-Hastings MCMC algo-
rithm (the mhsample function in Matlab) for NM C = 10000 samples
with a burn-in set to 500. The proposal distribution was selected to be
a symmetric uniform distribution such that [as+1, bs+1, cs+1, ds+1] ∼
U (as ± 0.4, bs ± 0.04, cs ± 0.04, ds ± 0.4). Given the sample of scaling
parameters (θ(s)
s=1 , we deﬁne the scaled simulated data as the aver-
age scaling over the MCMC sample, and record also the sample scaling
variance

j )NM C

˜yS
i =

g(yS

i |θ(s)
j )

1
NM C

1
NM C

NM C(cid:88)

s=1
NM C(cid:88)

(cid:16)

s=1

σ2
T (i) =

g(yS

i |θ(s)

j ) − ˜yS
i

(cid:17)2

.

See Figure 1 g) for an illustration of the scaling. We collect the scaled
simulated value and its variance from each simulated point into vectors
˜yS = (˜yS

T (NS)) ∈ RNS .

T (1), . . . , σ2

NS ) and σ2

1 , . . . , ˜yS

T = (σ2

deﬁnes a distribution over functions f (x) whose mean and covariance
are

f (x) ∼ GP (0, k(x, x(cid:48))) ,

E[f (x)] = 0
cov[f (x), f (x(cid:48))] = k(x, x(cid:48)).

For any collection of protein variants X = x1, . . . , xN , the function val-
ues follow a multivariate normal distribution f ∼ N (0, KXX ), where
f = (f (x1), . . . , f (xN ))T ∈ RN , and where KXX ∈ RN ×N with
[KXX ]ij = k(xi, xj). The key property of Gaussian processes is that
they encode functions that predict similar stability values f (x), f (x(cid:48))
for protein variants x, x(cid:48) that are similar, as encoded by the kernel
k(x, x(cid:48)). The key part of GP modelling is then to infer a kernel that
measures the mutation’s eﬀects to the stability.

Let a dataset of noisy stability values from two sources be y ∈ RN ,
the corresponding protein structures X = (xi)N
i=1, and a new protein
variant x∗ whose stability we wish to predict. A Gaussian process de-
ﬁnes a joint distribution over the observed values y of variants X, and
the unknown function value f (x∗) of the unseen variant x∗,
(cid:21)(cid:19)

(cid:18)

(cid:21)

(cid:20) y
f (x∗)

∼ N

0,

(cid:20)KXX + diag(σ2)
k∗X

kX∗
k(x∗, x∗)

,

∗X ∈ RN is a kernel vector with elements k(xi, x∗) for
where kX∗ = kT
T )2)T
all i = 1, . . . , N , and where σ2 = (σ2
collects ﬁnal variances of the data points from equations (5). Here
the exponents are elementwise. The conditional distribution gives the
posterior distribution of the stability prediction as

E1T , (σE1T + σS1T + tσT

0, σ2

f (x∗)|(X, y) ∼ N (cid:0)µ(x∗), σ2(x∗)(cid:1) ,

2.3 Data fusion and noise models

where the prediction mean and variance are

For each protein j, we organise its experimental data (XE, yE) and
transformed simulated data (XS, ˜yS) along with the wild-type infor-
mation (x0, y0) into a single joint dataset of variants X = (x0, XE, XS)

(cid:0)KXX + diag(σ2)(cid:1)−1 y,

µ(x∗) = k∗X
σ2(x∗) = k(x∗, x∗) − k∗X

(cid:0)KXX + diag(σ2)(cid:1)−1 kX∗.

(3)

(4)

3

Hence, in GP regression the stability predictions µ(x∗) ± σ(x∗) will
come with uncertainty estimates.

ring amino acids and scaled them between zero and one as

2.5 Graph kernel

Next, we consider how to compute the similarity function k(x, x(cid:48)) be-
tween two variants of the same protein structure. We will encode the
3D structural information of the two protein variants as a contact map
and measure their similarity by the formalism of graph kernels (Vish-
wanathan et al., 2010).

We consider two residues to be in contact if their closest atoms are
within 5 ˚A of each other in the PDB structure, which is illustrated in
Figure 1 b). All variants of the same protein have the same length, with
only diﬀerent residues at mutating positions. Furthermore, we assume
that all variants share the wild-type protein contact map.

To compare protein variants, we construct a weighted decomposition
kernel (WDK) (Menchetti et al., 2005) between two protein variants
x = (x1, . . . , xM ) and x(cid:48) = (x(cid:48)

M ) of length M ,

1, . . . , x(cid:48)

k(x, x(cid:48)) =


S(xp, x(cid:48)
p)

M
(cid:88)

p=1



(cid:88)

S(xl, x(cid:48)
l)

,

l∈nbs(p)

(7)

where nbs(p) deﬁnes the set of neighbouring positions to position p,
and S is a substitution matrix. The kernel iterates over all positions
p and compares for each of them their residues through a substitution
matrix S(xp, x(cid:48)
p). Furthermore, the similarity of the residues at each
position is multiplied by the average similarity of the residues at its
neighbouring positions S(xl, x(cid:48)
l). Hence, the kernel deﬁnes the simi-
larity of two protein variants as the average position and neighbour-
hood similarity over all positions. The kernel matrix is normalised
so that for two data points, the normalised kernel
p) =
k(xp, x(cid:48)
p, x(cid:48)
p), as deﬁned by Shawe-Taylor and Cris-
tianini (2004). The kernel is illustrated in Figure 1 c).

k(xp, xp)k(x(cid:48)

is ˆk(xp, x(cid:48)

p)/

(cid:113)

The above WDK kernel allows us to compare the eﬀects of multiple
simultaneous mutations. However, as the wild type protein structure
is used for all of the protein variants, changes that the mutations may
cause to the protein structure are not taken into consideration. This
may cause problems if mutations that alter the protein structure sig-
niﬁcantly are introduced – especially if many of them are introduced
simultaneously. On the other hand, substitution matrices that have
their basis in sequence comparisons, should take these eﬀects into ac-
count to some extend as these kinds of mutations are usually highly
destabilising and do not occur often in nature. In the next section, we
will discuss how we utilise diﬀerent substitution matrices with multiple
kernel learning.

The BLOSUM substitution models have been a common choice for
protein models (Giguere et al., 2013), while mixtures of substitution
models were proposed by Cichonska et al. (2017). BLOSUM matrices
score amino acid substitutions by their appearances throughout evolu-
tion, as they compare the frequencies of diﬀerent mutations in similar
blocks of sequences (Henikoﬀ and Henikoﬀ, 1992). However, there are
also diﬀerent ways to score amino acids substitutions, such as chemical
similarity and neighbourhood selectivity (Tomii and Kanehisa, 1996).
When the stability eﬀects of mutations are evaluated, the frequency of
an amino acid substitution in nature may not be the most important
factor.

To take into account diﬀerent measures of similarity between amino
acids, we employed a set of 21 amino acid substitution matrices gath-
ered from AAindex21 (Kawashima et al., 2008). AAindex2 currently
contains 94 substitution matrices. From these we selected those that
had no gaps concerning substitutions between the 20 naturally occur-

1http://www.genome.jp/aaindex/
2http://www.cs.ubc.ca/~schmidtm/Software/minConf.html

4

S =

S0 − min(S0) + 1
max(S0) − min(S0) + 1

.

Out of these matrices, we only chose those 23 matrices that were pos-
itive semideﬁnite. Furthermore, there were two pairs of matrices that
were extremely similar, and we only selected one matrix from each pair,
ending up with 21 substitution matrices. These substitution matrices
are used together with Equation 7 for computing 21 base kernel matri-
ces. Finally, MKL is used to ﬁnd an optimal combination of the base
kernels of form

(8)

(9)

Kφ =

wmK (γm)
m ,

21
(cid:88)

m=1

where wm is a kernel speciﬁc weight, γm is an (elementwise) exponent.
The elementwise exponent retains the SDP property of Kφ (Shawe-
Taylor and Cristianini, 2004). We observe empirically that the optimal
kernel weights wm tend to be sparse (See Figure 2).

The selected substitution matrices are listed in Figure 2. These ma-
trices have diﬀerent basis and through multiple kernel learning (MKL)
our model learns which of these are important for inferring the stability
eﬀects that mutations cause on diﬀerent proteins. The ﬁgure illustrates
this by showing the average weights of the base kernel matrices obtained
via the multiple kernel learning.

2.7 Parameter inference

The complete model has ﬁve parameters φ = (σE, σS, t, w, γ) to in-
fer, of which the variance parameters (σE, σS, t) parameterise the joint
data variance σ2
φ, while the MKL parameters w = (w1, . . . , w21) and
γ = (γ1, . . . , γ21) parameterise the kernel matrix Kφ. In a Gaussian
process model these can be jointly optimised by the marginal (log)
likelihood with priors

log p(y|φ)p(σE)p(σR) = log

p(y|f , φ)p(f |φ)p(σE)p(σR)df

(cid:90)

∝ −

yT (Kφ + diag(σ2

1
2
+ log Gamma(σE|αE, βE) + log Gamma(σS|αS, βS),

log |Kφ + diag(σ2

φ))−1y −

φ)|

1
2

(10)

which automatically balances model ﬁt (the square term) and the
model complexity (the determinant) to avoid overﬁtting (Rasmussen
and Williams, 2006). The parameters can be optimised by maximis-
ing the marginal log likelihood (10) using gradient ascent, since the
marginal likelihood can be diﬀerentiated analytically (see Supplemen-
tary Equations S1 and S2). We utilised a limited-memory projected
quasi-Newton algorithm (minConf TMP2), described by (Schmidt et al.,
2009).

We chose to evaluate the accuracy of our predictions using the same
metrics that have been used by many others – correlation ρ between the
predicted and experimentally measured ∆∆G values (Capriotti et al.,
2005a; Dehouck et al., 2009; Kellogg et al., 2011; Pires et al., 2014b;
Potapov et al., 2009) and the root mean square error (rmse) (Dehouck
et al., 2009; Pires et al., 2014a,b), which are determined in the Supple-
mentary Equations S3 and S4. We use marginal likelihood maximisa-
tion to infer model parameters and perform cross-validation to evaluate
the model performance on test data. Below we only report evaluation
metrics obtained from the test sets not used at any stage of the model
learning or data transformation sampling.

3 Results

In this section we evaluate the performance of mGPfusion on predict-
ing stability eﬀects of mutations, and compare it to the state-of-the-
art prediction methods mCSM, PoPMuSiC and Rosetta. Rosetta is a

2.6 Substitution matrices and multiple kernel

2.8 Evaluation criteria

learning

Figure 2: Average weights for kernels utilising the described substitution matrices from AAindex2, when GP models were trained with mutation level
cross-validation. Basis for the substitution matrices are obtained from (Tomii and Kanehisa, 1996). ∗ were added to AAindex2 in a later release, and their
basis were not determined by Tomii and Kanehisa (1996).

molecular modelling software whose ddg monomer module can directly
simulate the stability changes ∆∆G of a protein upon mutations. PoP-
MuSic and mCSM are machine learning models that predict stability
based on protein variant features. We run Rosetta locally, and use
mCSM and PoPMuSiC models through their web servers3. This may
give these methods an advantage over mGPfusion since parts of our
testing data were likely used within their training data.

We compare four diﬀerent variants of our method: mGPfusion that
uses both simulated data and MKL, “mGPfusion, only B62” that uses
simulated data but incorporates only one kernel matrix (BLOSUM62
substitution matrix), mGP model that uses MKL but does not use simu-
lated data, and “mGP, only B62” that uses only the base GP model but
does not incorporate simulated data and uses only the BLOSUM62 sub-
stitution matrix. In addition, we experiment on transforming Rosetta
predictions with the Bayesian scaling. We perform the experiments
for the 15 proteins separately using either position or mutation level
(leave-one-out) cross-validation regarding the methods mGP, mGPfu-
sion and the Bayesian scaling of Rosetta. Pires et al. (2014b) used
protein and position level cross-validation to evaluate their model. In
protein level cross-validation all mutations in a protein are either in
the test or training set exclusively. When we train our model using
protein level cross-validation, we use no experimental data and rely
only on the simulated data. Position level cross-validation is deﬁned
so that all mutations in a position are either in the test or training set
exclusively. However, datasets in Pires et al. (2014b) contained only
point mutations and therefore we had to extend the deﬁnition to also
include multiple mutations. In position level cross-validation we train
one model for each position using only the part of data that has a
wild-type residue in that position. Therefore, in position level cross-
validation we construct a test set that contains all protein variants that
have a mutation at position p and use as training set all the protein
variants that have a wild-type residue at that position. Dehouck et al.
(2009) evaluated their models by randomly selecting training and test
sets so that each mutation was exclusively in one of the sets, but both
sets could contain mutations from the same position of the same pro-

3biosig.unimelb.edu.au/mcsm and

omictools.com/popmusic-tool

tein. We call this mutation level cross-validation. When we use all
available experimental data with mutation level cross-validation, this
corresponds to leave-one-out cross-validation.

3.1 Predicting point mutations

Table 2 summarises the average prediction performance over all 15 pro-
teins for all compared methods, types of mutations and cross-validation
types. We ﬁrst compare the performances on single point mutations,
where mGPfusion and mGP achieve the highest performance with
ρ = 0.81 and rmse = 1.07 kcal/mol, and ρ = 0.81 and rmse = 1.04
kcal/mol, respectively with mutation level cross-validation. With only
one kernel utilising the BLOSUM62 matrix instead of MKL, the per-
formance decreases slightly, but the competing methods are still out-
performed, as mCSM achieves ρ = 0.64 and rmse = 1.37 kcal/mol,
PoPMuSic ρ = 0.61 and Rosetta ρ = 0.55. Applying Bayesian scaling
on Rosetta simulator improves the performance of standard Rosetta
from ρ = 0.55 to ρ = 0.65 and decreases the rmse from 1.63 kcal/mol
to 1.35 kcal/mol, which is interestingly even slightly better than the
performances of mCSCM and PoPMuSiC.

With position level cross-validation mGPfusion achieves the high-
est performance of ρ = 0.70 and rmse = 1.26 kcal/mol, likely due
to having still access to simulated variants from that position, since
they are always available to the learner. Without simulation data,
the baseline machine learning model mGP performance decreases to
ρ = 0.51 and rmse = 1.54 kcal/mol, thus demonstrating the impor-
tance of the data fusion. Cross-validation could not be performed for
the oﬀ-the-shelf methods mCSM and PoPMuSiC. Even still, mGPfusion
(trained with one or multiple kernels) outperforms competing state-of-
the-art methods and achieves markedly higher prediction performance
as quantiﬁed by both mutation and position level cross-validations.
Also mGP outperforms these methods when quantiﬁed by mutation
level cross-validation. With protein level cross-validation mGPfusion
achieves slightly better results than Rosetta.

5

Method
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled

Point mutations
cross-validation level
prot.
pos.
mut.
0.56
0.70
0.81
0.56
0.69
0.79
-
0.51
0.81
-
0.34
0.76
-
0.63
0.65

Correlation ρ
Multiple mutations
cross-validation level
prot.
pos.
mut.
0.49
0.61
0.88
0.50
0.64
0.86
-
0.52
0.86
-
0.55
0.86
-
0.39
0.51

All mutations
cross-validation level
prot.
pos.
mut.
0.52
0.64
0.83
0.52
0.66
0.82
-
0.50
0.83
-
0.49
0.80
-
0.48
0.60

Point mutations
cross-validation level
prot.
pos.
mut.
1.61
1.26
1.07
1.62
1.30
1.11
-
1.54
1.04
-
1.95
1.26
-
1.38
1.35

rmse
Multiple mutations
cross-validation level
prot.
pos.
mut.
2.53
2.45
1.33
2.50
2.40
1.43
-
2.65
1.44
-
2.56
1.45
-
2.99
2.49

All mutations
cross-validation level

mut.
1.13
1.18
1.14
1.30
1.66

pos.
1.87
1.85
2.09
2.23
2.22

prot.
1.84
1.84
-
-
-

Predictions from oﬀ-the-shelf implementations with no cross-validation

Rosetta
mCSM
PoPMuSiC

0.55
0.61
0.64

0.40
-
-

0.49
-
-

1.63
1.40
1.37

2.74
-
-

1.92
-
-

Table 2: Comparison of diﬀerent methods on the 15 protein dataset with respect to ρ and rmse. Mutation, position, and protein are referred to as mut.,
pos., and prot., respectively. Predictions from oﬀ-the-shelf implementations of Rosetta, mCSM and PoPMuSiC are used directly without cross-validation.

3.2 Predicting multiple mutations

Next, we tested stability prediction accuracies for variants containing
either single or multiple mutations. Figure 3 shows a scatter plot of
mGPfusion predictions for all 1537 single and multiple mutation vari-
ants (covering all 15 proteins) against the experimental ∆∆G values
using the mutation level (leave-one-out) cross-validation. The points
are coloured by the number of simultaneous mutations in the variants,
with 326 variants having at least 2 mutations (See Table 1). Fig-
ure 3 illustrates the mGPfusion’s overall high accuracy of ρ = 0.83
and rmse = 1.13 kcal/mol on both single and multiple mutations (See
Table 2). Scatter plots for the individual proteins can be found in Sup-
plementary Figure S3. Dehouck et al. (2009) suggested that considering
the predictive power after removal of most badly predicted stability ef-
fects of mutations may give more relevant evaluation, as some of the
experimental measurements may have been made in non-physiological
conditions or aﬀected by signiﬁcant error, associated with a poorly re-
solved structure, or indexed incorrectly in the database. They thus
reported correlation and rmse of the predictions after excluding 10 %
of the predictions with most negative impacts on the correlation co-
eﬃcient. Pires et al. (2014b) also reported their accuracy after 10 %
outlier removal. If we remove the 10% worst predicted stability eﬀects
from the combined predictions, we achieve correlation ρ of 0.92 and
rmse of 0.67 kcal/mol. We report these results for all the methods
in Supplementary Table S3 and also present the error distribution in
Supplementary Figure S5.

Figure 3: Scatter plot for the mutation level (leave-one-out) predictions made
for all 15 proteins (See Table 1). The colour indicates the number of simul-
taneous mutations.

The high accuracy is retained for variants with multiple mutations
as well (ρ = 0.88 and rmse = 1.33 kcal/mol, see Tables 2 and S2).
Table 3 lists mGPfusion’s rmse for diﬀerent number of simultaneous
mutations. The model accuracy in fact improves up to 6 mutations.
This is explained by the training set often containing the same single
point mutations that appear in variants with multiple mutations. The
model can then infer the combined eﬀect of pointwise mutations. The
model seems to fail when predicting the eﬀects of 7-9 simultaneous mu-

6

tations. Most of these mutations (8/12) are for Ribonuclease (1RGG)
and their eﬀects seem to be exceptionally diﬃcult to predict. This may
be because only few of the point mutations that are part of the multiple
mutations are present in the training data. However, these mutations
seem to be exceptionally diﬃcult to predict for Rosetta as well, which
could indicate that the experimental measurements concerning these
mutations are not quite accurate. PoPMuSiC and mCSM are unable
to predict multiple mutations, while Rosetta supports them, but its
rmse accuracy decreases already with two mutations.

mutations
occurences

mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta

1
1211

1.07
1.11
1.04
1.26
1.35
1.63

2
207

1.06
1.12
1.03
0.96
2.10
2.27

3
52

0.80
0.77
0.61
0.65
1.92
2.11

4
42

0.51
0.59
0.50
0.83
2.94
3.78

5
4

0.40
0.29
0.18
0.26
2.29
2.93

6
8

1.01
1.14
0.92
1.14
2.32
2.21

7
3

3.02
3.00
3.23
2.95
2.93
2.92

8
3

5.89
6.78
6.18
6.90
6.75
5.80

9
6

5.16
5.56
6.75
6.57
7.28
7.45

10
1

0.25
0.11
0.08
0.05
2.69
3.42

Table 3: Root-mean-square errors for diﬀerent number of simultaneous
mutations for all 15 proteins, with models trained by leave-one-out cross-
validation. Rosetta is added for comparison.

With multiple mutations, the decrease in performance between the
position and mutation level cross-validations becomes clearer than with
single mutations. With the position level cross-validation the stability
eﬀects of multiple mutations are predicted multiple times, which partly
explains this loss of accuracy. For example, the eﬀects of mutants with
nine diﬀerent simultaneous mutations, which were the most diﬃcult
cases in the mutation level cross-validation, are predicted nine times.
Surprisingly, mGPfusion trained with protein level cross-validation
achieves higher correlation and smaller errors than Rosetta; mGPfusion
utilising simulated ∆∆G values for only single mutations, can predict
the stability eﬀects of multiple mutations better than Rosetta.

3.3 Uncertainty of the predictions

Gaussian processes provide a mean µ(x) and a standard deviation σ(x)
for the stability prediction of a protein variant x. The standard de-
viation allows estimation of the prediction accuracy even without test
data. Figure 1 h) visualises the uncertainty of a few predictions made
for the protein G (1PGA) when mutation level cross-validation is used.
The estimated standard deviation allows a user to automatically iden-
tify low quality predictions that can appear e.g. in parts of the input
protein space from which less data is included in model training. Con-
versely, in order to minimise the amount of uncertainty in the mGPfu-
sion predictions, estimated standard deviation can be used to guide next
experiments. The probabilistic nature of the predictions also admits an
alternative error measure of negative log probability density (NLPD)
nlpd = − (cid:80)N
i=1 log p(yi|µ(xi), σ2(xi)), which can naturally take into
account the prediction variance.

3.4 Eﬀect of training set size

The results presented in Sections 3.1–3.3 used all available data for
training with cross-validation to obtain unbiased performance mea-
sures. The inclusion of thousands of simulated variants allows the model

to learn accurate models with less experimentally measured variants.
Hence, we study how the mGPfusion model with or without simulated
data performs with reduced number of experimental observations. To
facilitate this, we randomly selected subsets of experimental data of
size 0, 10, 20, and so on. We learned the mGP and mGPfusion models
with these reduced experimental data sets while always using the full
simulated data sets. This also allows us to estimate how the models
work with diﬀerent number of cross-validation folds. For example, the
point of a learning curve which utilises 2/3 or 4/5 of the training data
correspond to an average of multiple 3-fold or 5-fold cross-validations,
respectively.

The learning curve in Figure 4a) shows how the averaged correlation
for protein 2LZM improves when the size of the experimental data set
increases. The right-most values at N = 348 are obtained with leave-
one-out cross-validation. The inclusion of simulated data in mGPfusion
(dark blue line) consistently improves the performance of mGP, which
is trained without simulated data. Figure 4b) illustrate the diﬀerence
in root mean square error. Learning curves for all proteins listed in
Table 1 can be found from the Supplementary Figures S6-S8. When
the number of experimental samples is zero, the mGPfusion model is
trained solely using the simulated data with scaling 0.57yS, and the
mGP model predicts the stability eﬀect of every mutation as zero. The
last point on the learning curves is obtained with mutation level cross-
validation (see Tables 2 and S2).

proteins mGPfusion model trained with only one kernel that utilises
BLOSUM62, provides approximately as good results as the mGPfu-
sion model trained with multiple kernels. However, with many of the
proteins, utilising just BLOSUM62 does not seem to be suﬃcient and
the accuracy of the model can be improved by using diﬀerent substi-
tution matrices. Prior knowledge of appropriate substitution models
for each protein could enable creation of accurate prediction models
with just one substitution model, but the MKL seems to be a good
tool for selecting suitable substitution models when such knowledge is
not available. It seems that the data fusion and number or relevance
of used substitution matrices can compensate each other – the learning
curves show, that the diﬀerence between mGPfusion models trained
with one or multiple kernels is smaller than the diﬀerence between the
mGP models utilising one or multiple kernels. This indicates that if ad-
ditional simulated data is exploited, the use of multiple or appropriate
substitution models is not as important than without the data fusion.
On the other hand, if data fusion is not applied, the use of MKL can
more signiﬁcantly improve the accuracy of the mGP model.

3.6 Eﬀect of

the Bayesian transformation on

Rosetta

The Bayesian scaling of simulated Rosetta values, proposed in Sec-
tion 2.2, improves the match of Rosetta simulated values to empirical
∆∆G values even without using the Gaussian process framework. The
Bayesian scaling improves the performance of standard Rosetta sim-
ulations from ρ = 0.55 and rmse = 1.63 kcal/mol to ρ = 0.65 and
rmse = 1.35 kcal/mol (see Table 2 and Supplementary Table S2). This
shows that the scaling proposed by Kellogg et al. (2011) indeed is not
always the optimal scaling and signiﬁcant improvements can be gained
by optimising the scaling using a set of training data.

Figure 1 g) visualises the Bayesian scaling for protein 1PGA, where
the very destabilising ∆∆G values are dampened by the scaling (black
dots) to less extreme values by matching the scaled simulated values to
the experimental points (blue circles). The black dots along the scaling
curve indicate the grid of point mutations after transformation. The
scaling variance σ2
T is indicated by the green region’s vertical width,
and on the right panel. The scaling tends to dampen very small values
into less extreme stabilities, while it also estimates higher uncertainties
for stability values further away from ∆∆G = 0. However, the scalings
vary between diﬀerent proteins, as can be seen from the transformations
for each of the 15 proteins presented in Supplementary Figure S9.

Figure 4: a) Correlation and b) root mean square error of predictions made
by models with diﬀerent number of experimental training samples for T4
Lysozyme (2LZM). The results of Rosetta, mCSM and PoPMuSiC are in-
variant to training data (because mCSM and PoPMuSiC are pre-trained),
and are thus constant lines. For both ﬁgures, an average of 100 randomly
selected training sets is taken at each point.

3.5 Eﬀect of data fusion and multiple substitution

matrices

In the beginning of the learning curves, when only little training data is
available, mGPfusion quite consistently outperforms the mGP model,
demonstrating that the additional simulated data improves the predic-
tion accuracy. However, when more training data becomes available,
the performance of mGP model is almost as good or sometimes even
better than the performance of the mGPfusion model. This shows that
if enough training data is available, it is not necessary to simulate addi-
tional data in order to obtain accurate predictions. Table 2 also shows,
that the data fusion can compensate the lack of relevant training data
– with the mGPfusion models that utilise the additional data, the de-
crease in accuracy is smaller when position level cross-validation is used
instead of mutation level cross-validation, than with the mGP models.
The varying weights for the base kernels between diﬀerent pro-
teins (shown in Figure 2) already illustrated that diﬀerent proteins
beneﬁt from diﬀerent similarity measures for amino acid substitutions.
The learning curves also support this observation – with some of the

4 Conclusions

We present a novel method mGPfusion for predicting stability eﬀects of
both single and multiple simultaneous mutations. mGPfusion utilises
structural information in form of contact maps and integrates that with
amino acid residues and combines both experimental and comprehen-
In con-
sive simulated measurements of mutations’ stability eﬀects.
trast to earlier general-purpose stability models, mGPfusion model is
protein-speciﬁc by design, which improves the accuracy but necessitates
having a set of experimental measurements from the protein. In prac-
tise small datasets of 10–20 experimental observations were found to
provide state-of-the-art accuracy models when supplemented by large
simulation datasets.

An important advantage over most state-of-the-art machine learn-
ing methods is that mGPfusion is able to predict the eﬀects of multiple
simultaneous mutations in addition to single point mutations. Our
experiments show that mGPfusion is reliable in predicting up to six
simultaneous mutations in our dataset. Furthermore, the Gaussian
process framework provide a way to estimate the (un)certainty of the
predictions even without a separate test set. We additionally proposed
a novel Bayesian scaling method to re-calibrate simulated protein sta-
bility values against experimental observations. This is a crucial part of
the mGPfusion model, and also alone improved protein-speciﬁc Rosetta
stability predictions by calibrating them using experimental data.

mGPfusion is best suited for a situation, where a protein is thor-
oughly experimented on and accurate predictions for stability eﬀects

7

upon mutations are needed. It takes some time to set up the frame-
work and train the model, but after that new predictions can be made
in fractions of a second. The most time-consuming part is running the
simulations with Rosetta, at least when the most accurate protocol 16
is used. Simulating all 19 possible point mutations for one position
took about 12 hours, but simulations for diﬀerent positions can be run
on parallel. The time needed for training the prediction model depends
on the amount of experimental and simulated training data. With no
simulated data, the training time ranged from few seconds to few min-
utes. With data fusion and a single kernel, the training time was under
an hour. With data fusion and MKL with 21 kernels, the training time
was from a few minutes to a day.

Acknowledgements

We acknowledge the computational resources provided by the Aalto
Science-IT.

Funding

This work has been supported by the Academy of Finland Center of Ex-
cellence in Systems Immunology and Physiology, the Academy of Fin-
land grants no. 260403 and 299915, and the Finnish Funding Agency
for Innovation Tekes (grant no 40128/14, Living Factories).
References

Alberts, B., Johnson, A., Lewis, J., Raﬀ, M., Roberts, K., and Walter, P. (2007). Molec-

ular biology of the cell. Garland Science, 5 edition.

Anslyn, E. V. and Dougherty, D. A. (2006). Modern physical organic chemistry. Uni-

versity Science Books.

Berman, H. M., Westbrook, J., Feng, Z., Gilliland, G., Bhat, T., Weissig, H., Shindyalov,
I. N., and Bourne, P. E. (2000). The protein data bank. Nucleic acids research, 28(1),
235–242.

Bommarius, A. S., Blum, J. K., and Abrahamson, M. J. (2011). Status of protein en-
gineering for biocatalysts: how to design an industrially useful biocatalyst. Current
opinion in chemical biology, 15(2), 194–200.

Branden, C. and Tooze, J. (1999). Introduction to protein structure. Garland, 2 edition.

Giollo, M., Martin, A. J., Walsh, I., Ferrari, C., and Tosatto, S. C. (2014). NeEMO: a
method using residue interaction networks to improve prediction of protein stability
upon mutation. BMC genomics, 15(4), 1.

Henikoﬀ, S. and Henikoﬀ, J. G. (1992). Amino acid substitution matrices from protein
blocks. Proceedings of the National Academy of Sciences, 89(22), 10915–10919.

Kawashima, S., Pokarowski, P., Pokarowska, M., Kolinski, A., Katayama, T., and Kane-
hisa, M. (2008). AAindex: amino acid index database, progress report 2008. Nucleic
acids research, 36(suppl 1), D202–D205.

Kellogg, E. H., Leaver-Fay, A., and Baker, D. (2011). Role of conformational sampling
in computing mutation-induced changes in protein structure and stability. Proteins:
Structure, Function, and Bioinformatics, 79(3), 830–838.

Kirk, O., Borchert, T. V., and Fuglsang, C. C. (2002). Industrial enzyme applications.

Current opinion in biotechnology, 13(4), 345–351.

Kumar, M. S., Bava, K. A., Gromiha, M. M., Prabakaran, P., Kitajima, K., Uedaira,
H., and Sarai, A. (2006). ProTherm and ProNIT: thermodynamic databases for pro-
teins and protein–nucleic acid interactions. Nucleic Acids Research, 34(suppl 1),
D204–D206.

Leaver-Fay, A., Tyka, M., Lewis, S. M., Lange, O. F., Thompson, J., Jacak, R., Kaufman,
K., Renfrew, P. D., Smith, C. A., Sheﬄer, W., et al. (2011). ROSETTA3: an object-
oriented software suite for the simulation and design of macromolecules. Methods in
enzymology, 487, 545.

Liu, J. and Kang, X. (2012). Grading amino acid properties increased accuracies of single

point mutation on protein stability prediction. BMC bioinformatics, 13(1), 1.

Menchetti, S., Costa, F., and Frasconi, P. (2005). Weighted decomposition kernels. In
Proceedings of the 22nd international conference on Machine learning, pages 585–
592. ACM.

Pace, C. N. and Scholtz, J. M. (1997). Measuring the conformational stability of a pro-

tein. Protein structure: A practical approach, 2, 299–321.

Pace, C. N. and Shaw, K. L. (2000). Linear extrapolation method of analyzing solvent
denaturation curves. Proteins: Structure, Function, and Bioinformatics, 41(S4),
1–7.

Pires, D. E., Ascher, D. B., and Blundell, T. L. (2014a). DUET: a server for predicting
eﬀects of mutations on protein stability using an integrated computational approach.
Nucleic acids research, page gku411.

Pires, D. E., Ascher, D. B., and Blundell, T. L. (2014b). mCSM: predicting the eﬀects of
mutations in proteins using graph-based signatures. Bioinformatics, 30(3), 335–342.

Potapov, V., Cohen, M., and Schreiber, G. (2009). Assessing computational methods for
predicting protein stability upon mutation: good on average but not in the details.
Protein Engineering Design and Selection, 22(9), 553–560.

Capriotti, E., Fariselli, P., and Casadio, R. (2005a). I-Mutant2.0: predicting stability
changes upon mutation from the protein sequence or structure. Nucleic acids research,
33(suppl 2), W306–W310.

Rapley, R. and Walker, J. M. (2000). Molecular Biology and Biotechnology. Royal

Society of Chemistry, 4 edition.

Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian processes for machine learn-

Capriotti, E., Fariselli, P., Calabrese, R., and Casadio, R. (2005b). Predicting pro-
tein stability changes from sequences using support vector machines. Bioinformatics,
21(suppl 2), ii54–ii58.

ing. The MIT Press.

Capriotti, E., Fariselli, P., Rossi, I., and Casadio, R. (2008). A three-state prediction of

single point mutations on protein stability changes. BMC bioinformatics, 9(2).

Chen, C.-W., Lin, J., and Chu, Y.-W. (2013). iStable: oﬀ-the-shelf predictor integration

for predicting protein stability changes. BMC bioinformatics, 14(2).

Cheng, J., Randall, A., and Baldi, P. (2006). Prediction of protein stability changes for
single-site mutations using support vector machines. Proteins: Structure, Function,
and Bioinformatics, 62(4), 1125–1132.

Cherry, J. R. and Fidantsef, A. L. (2003). Directed evolution of industrial enzymes: an

update. Current opinion in biotechnology, 14(4), 438–443.

Cichonska, A., Ravikumar, B., Parri, E., Timonen, S., Pahikkala, T., Airola, A., Wenner-
berg, K., Rousu, J., and Aittokallio, T. (2017). Computational-experimental approach
to drug-target interaction mapping: A case study on kinase inhibitors. PLoS compu-
tational biology, 13(8), e1005678.

Sanchez, S. and Demain, A. L. (2010). Enzymes and bioconversions of industrial, pharma-
ceutical, and biotechnological signiﬁcance. Organic Process Research & Development,
15(1), 224–230.

Schmidt, M. W., Berg, E., Friedlander, M. P., and Murphy, K. P. (2009). Optimizing
costly functions with simple constraints: A limited-memory projected quasi-newton
algorithm. In International Conference on Artiﬁcial Intelligence and Statistics, page
None.

Shawe-Taylor, J. and Cristianini, N. (2004). Kernel methods for pattern analysis. Cam-

bridge university press.

Tian, J., Wu, N., Chu, X., and Fan, Y. (2010). Predicting changes in protein thermosta-
bility brought about by single- or multi-site mutations. BMC bioinformatics, 11(1),
1.

Tokuriki, N. and Tawﬁk, D. S. (2009). Stability eﬀects of mutations and protein evolv-

ability. Current opinion in structural biology, 19(5), 596–604.

Dehouck, Y., Grosﬁls, A., Folch, B., Gilis, D., Bogaerts, P., and Rooman, M. (2009). Fast
and accurate predictions of protein stability changes upon mutations using statistical
potentials and neural networks: PoPMuSiC-2.0. Bioinformatics, 25(19), 2537–2543.

Tomii, K. and Kanehisa, M. (1996). Analysis of amino acid indices and mutation matrices
for sequence comparison and structure prediction of proteins. Protein Engineering,
Design and Selection, 9(1), 27–36.

Folkman, L., Stantic, B., and Sattar, A. (2014). Feature-based multiple models improve
classiﬁcation of mutation-induced stability changes. BMC genomics, 15(Suppl 4).

Vishwanathan, S. V. N., Schraudolph, N. N., Kondor, R., and Borgwardt, K. M. (2010).

Graph kernels. The Journal of Machine Learning Research, 11, 1201–1242.

Giguere, S., Marchand, M., Laviolette, F., Drouin, A., and Corbeil, J. (2013). Learn-
ing a peptide-protein binding aﬃnity predictor with kernel ridge regression. BMC
bioinformatics, 14(1), 82.

Wainreb, G., Wolf, L., Ashkenazy, H., Dehouck, Y., and Ben-Tal, N. (2011). Protein
stability: a single recorded mutation aids in predicting the eﬀects of other mutations
in the same amino acid site. Bioinformatics, 27(23), 3286–3292.

8

Supplementary material

Figure S5: The stability of a protein is determined by the thermodynamic and kinetic stabilities, ∆Gt and ∆Gk, respectively. We only consider the
thermodynamic stability.

Figure S6: Priors presented by Equation 2. Here µd = −1.5, the most likely value for −a. Other hyperparameter values are presented in Table S4.

Table S4: Values for the hyperparameters used in the priors of a, b, c, d, σE and σS presented in Equations 2 and 6, respectively.

a

b

c

d

σE

σS

αa = 2
βa = 1.5

αb = 1.3
βb = 2

αc = 2
βc = 5

µd = −a
σd = 0.15

αE = 2.5
βE = 0.02

αS = 50
βS = 0.007

9

The partial derivatives of the marginal likelihood with respect to the parameters φ are obtained from Equation (9) as follows:

where α = K −1

φ y, Kφ is determined as

and the partial derivatives of Kφ with respect to the optimised parameters are

∂
∂φj

log p(y|X, φ) =

yT K −1

∂Kφ
∂φj

K −1

φ y −

(cid:19)

K −1
φ

∂Kφ
∂φj

1
2
1
2

φ
(cid:18)(cid:16)

=

tr

ααT − K −1

φ

(cid:18)

(cid:19)

,

tr

1
2
(cid:17) ∂Kφ
∂θj

Kφ =

wmK γm

m + diag





σ0
σE1NE
σE1NE + σS1NS + tσT


2



M
(cid:88)

m=1

∂Kφ
∂σE

∂Kφ
∂σR

∂Kφ
∂t

∂Kφ
∂wm
∂Kφ
∂γm







0
2σE1NE
2 (σE1NE + σS1NS + tσT )
0
0NE
2 (σE1NE + σS1NS + tσT )
0
0NE
2 (σE1NE + σS1NS + t) σT













=diag



=diag



=diag



=K γm
m

=wmK γm

m log Km

(cid:80)N∗

i=1(yi − ¯y)(µ(xi) − ¯µ)

i=1(yi − ¯y)2 (cid:80)N∗

i=1(µ(xi) − ¯µ)2

ρ =

(cid:113)(cid:80)N∗
(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
N∗

N∗(cid:88)

i=1

rmse =

(yi − µ(xi))2,

Correlation ρ and root-mean-square error rmse for the predictions are determined as

(S11)

(S12)

(S13)

(S14)

(S15)

(S16)

(S17)

(S18)

where ¯y is the mean of the experimentally measured values, µ(xi) is prediction mean, ¯µ is the average of all prediction means, and N∗ is the
number of predictions.

10

Figure S7: Mutation-level predictions for all 15 proteins presented in Table 1. The predictions are coloured by the number of simultaneous mutations.

11

Figure S8: Position-level predictions for all 15 proteins. When the eﬀects of a mutant are predicted multiple times, they are connected by a line.

12

Table S5: (Continues on the next page)Comparison of diﬀerent methods on the 15 protein dataset with respect to ρ and rmse. Oﬀ-the-shelf implementations
of Rosetta, mCSM and PoPMuSiC are used directly without cross-validation.

Protein Method

2LZM

1BNI

1VQB

1LZ1

2CI2

1PGA

2RN2

1CSP

mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC

0.58
0.59
-
-
-

0.75
0.73
0.75
0.71
0.57

0.77
0.74
0.81
0.61
0.59

0.49
0.50
-
-
-

0.67
0.65
0.79
0.79
0.47

0.62
0.62
-
-
-

Point mutations
cross-validation level
prot.
pos.
mut.
0.75
0.80
0.87
0.75
0.77
0.86
-
0.59
0.86
-
0.37
0.75
0.73
-
0.74
0.75
0.57
0.71
0.64
0.61
0.65
0.48
0.58
0.62
0.60
0.66
0.50
0.53
0.12
0.29
0.46
0.49
0.53
0.51
0.59
0.56
0.39
-0.31
0.55
0.59
0.67
0.64
0.72
0.67
0.61
0.74
0.60
0.63
0.74
0.75
0.47
0.59
0.61
-0.46
0.59
0.69
-0.10
0.28
0.58
0.59
0.12
0.09
0.64
0.70
0.71
0.71
0.23
0.22
-0.06
-0.37
0.20
0.33
0.42
0.48

0.33
0.34
-
-
-

0.85
0.86
0.88
0.87
0.23

0.71
0.71
-
-
-

0.68
0.82
0.62
0.57
0.69

0.69
0.71
-
-
-

0.79
0.82
0.77
0.83
0.66

0.73
0.69
0.65
0.51
0.60

0.64
0.63
-
-
-

0.39
0.40
-
-
-

0.75
0.73
-
-
-

0.86
0.84
0.86
0.87
0.17

0.93
0.91
0.96
0.97
0.71

-1.00
-1.00
-
-
-

1.00
1.00
1.00
1.00
-1.00

Correlation ρ
Multiple mutations
cross-validation level
prot.
pos.
mut.
0.64
0.83
0.96
0.64
0.87
0.96
-
0.85
0.97
-
0.77
0.94
0.66
-
0.68
0.68
-
-
0.70
0.79
0.82
0.85
0.29
0.18
-
-
0.83
0.82
0.70
0.75
0.68
0.73
-
-
0.11
0.05
0.56
0.42
-1.00
-1.00
-
-
0.87
0.86
0.79
0.71
0.61
0.62
-
-
0.35
0.60
-0.24
-0.08
0.07
0.03
-
-
0.60
0.60
0.42
0.42
0.50
0.47
-
-
0.73
0.69
0.75
0.71
0.69
0.68
-
-

0.92
0.91
0.94
0.92
0.68

0.90
0.76
0.93
0.92
0.09

0.78
0.76
0.75
0.77
0.48

0.48
0.59
-
-
-

0.61
0.62
-
-
-

0.32
0.62
-
-
-

0.95
0.92
0.92
0.92
0.61

0.85
0.86
-
-
-

0.77
0.76
0.78
0.74
0.53

0.77
0.74
0.80
0.63
0.53

0.76
0.75
0.85
0.85
0.57

0.52
0.53
-
-
-

0.57
0.57
-
-
-

0.60
0.58
-
-
-

All mutations
cross-validation level
prot.
pos.
mut.
0.68
0.76
0.90
0.69
0.82
0.90
-
0.72
0.90
-
0.61
0.82
0.65
-
0.70
0.71
-
-
0.55
0.60
0.63
0.41
0.45
0.56
-
-
0.69
0.69
0.50
0.55
0.59
0.59
-
-
0.57
0.56
0.47
0.21
0.46
0.55
-
-
0.81
0.79
0.72
0.63
0.63
0.65
-
-
0.43
0.53
-0.14
0.06
0.11
0.28
-
-
0.53
0.53
0.22
0.23
0.57
0.65
-
-
0.75
0.72
0.77
0.72
0.64
0.60
-
-

0.38
0.54
-
-
-

0.70
0.70
-
-
-

0.50
0.70
-
-
-

0.92
0.91
0.94
0.92
0.59

0.79
0.81
0.77
0.82
0.62

0.85
0.81
0.84
0.81
0.24

0.71
0.72
-
-
-

0.82
0.79
0.76
0.68
0.63

0.83
0.86
0.81
0.91
0.99

1.06
1.06
-
-
-

1.69
1.69
-
-
-

2.25
2.25
-
-
-

1.71
1.75
1.41
1.50
1.99

1.21
1.27
1.08
1.61
1.58

mut.
0.82
0.84
0.82
1.12
1.05

Point mutations
cross-validation level
prot.
pos.
1.12
1.02
1.12
1.08
-
1.34
-
1.93
1.06
-
1.13
1.27
1.11
1.37
1.45
1.40
2.32
1.58
1.70
1.62
1.53
1.94
1.94
2.41
2.89
2.00
2.26
2.24
2.29
0.99
1.03
1.15
1.43
1.01
1.04
0.97
0.95
0.90
0.97
1.02
1.39
1.00
1.09
0.86
0.85
1.54
1.22
1.58
1.73
1.42
1.70
1.94
1.89
1.21
1.22
1.45
1.45
1.13
1.07
1.04
1.16
1.04
1.04
1.07
1.12
1.06
1.11
1.02
0.99

0.64
0.65
0.54
0.60
1.04

1.10
1.10
-
-
-

0.91
0.87
0.93
0.82
1.09

1.05
1.59
-
-
-

1.26
0.87
1.40
1.53
1.21

1.64
1.06
-
-
-

0.85
0.91
0.95
1.16
1.00

1.07
1.07
-
-
-

0.56
1.10
0.13
0.27
3.23

1.15
1.35
0.70
0.63
1.90

1.28
1.24
1.32
1.15
2.51

3.75
3.51
-
-
-

2.17
2.14
-
-
-

2.06
2.43
-
-
-

mut.
0.57
0.59
0.48
0.78
1.65

rmse
Multiple mutations
cross-validation level
prot.
pos.
2.14
1.53
2.11
1.16
-
1.24
-
1.44
1.84
-
1.93
-
-
2.49
2.03
2.00
1.67
2.90
2.33
-
-
1.62
1.82
1.97
2.30
1.96
2.06
-
-
2.40
2.41
1.65
2.36
3.25
3.41
-
-
0.80
1.01
1.01
1.40
1.27
1.30
-
-
2.09
0.95
3.01
2.07
3.09
3.51
-
-
1.01
0.95
1.21
1.20
1.08
1.25
-
-
1.66
1.87
1.51
1.59
2.29
1.92
-
-

2.58
2.13
-
-
-

0.91
1.18
-
-
-

2.74
0.88
-
-
-

1.21
1.12
-
-
-

0.91
0.96
0.76
0.86
2.19

0.67
0.69
0.74
0.68
1.20

0.53
0.69
0.45
0.47
2.81

0.55
0.71
0.66
0.71
1.27

All mutations
cross-validation level
prot.
1.43
1.42
-
-
-

mut.
0.76
0.79
0.75
1.05
1.23

pos.
1.30
1.13
1.29
1.70
1.51

1.22
1.27
1.11
1.57
1.70

1.59
1.66
1.27
1.33
1.97

0.83
0.87
0.80
0.90
1.07

0.80
0.87
0.90
1.08
1.06

0.88
0.83
0.94
1.02
2.33

0.86
0.83
0.89
0.80
1.12

0.75
0.78
0.63
0.71
1.58

1.37
-
-
1.67
1.60
1.55
2.20
1.94

1.77
-
-
1.82
1.89
2.24
2.66
1.99

2.21
-
-
1.07
1.10
1.17
1.47
1.15

1.12
-
-
0.86
0.99
1.02
1.39
1.11

1.13
-
-
2.00
1.13
2.81
2.01
2.87

2.95
-
-
1.14
1.13
1.36
1.36
1.11

1.11
-
-
1.45
1.60
1.36
1.43
1.92

1.47
-
-

1.75
1.75
-
-
-

2.20
2.30
-
-
-

1.16
1.14
-
-
-

1.10
1.08
-
-
-

2.38
1.02
-
-
-

1.02
1.51
-
-
-

1.80
1.57
-
-
-

13

Table S5: (Continued) Comparison of diﬀerent methods on the 15 protein dataset with respect to ρ and rmse. Oﬀ-the-shelf implementations of Rosetta,
mCSM and PoPMuSiC are used directly without cross-validation.

Protein Method

1BVC

4LYZ

1RTB

1PIN∗

1RN1

1RGG

1BPI

total

mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC

0.34
0.34
-
-
-

0.61
0.59
0.65
0.62
0.29

0.53
0.53
-
-
-

0.64
0.59
0.61
0.23
0.50

0.70
0.70
-
-
-

0.92
0.89
0.92
0.91
0.65

Point mutations
cross-validation level
prot.
pos.
mut.
0.48
0.43
0.41
0.48
0.48
0.50
-
-0.12
-0.05
-
0.06
-0.05
-
0.40
0.42
0.47
0.47
0.60
0.27
0.30
-0.05
0.27
0.28
0.33
0.55
0.59
0.81
0.79
0.69
0.58
0.61
0.69
0.68
0.72
0.49
0.51
0.40
0.25
0.49
0.53
0.72
0.60
0.62
0.58
0.04
0.27
0.57
0.67
0.76
0.62
0.51
0.52
-0.61
-0.53
0.43
0.60
0.77
0.65
0.67
0.65
0.57
0.58
0.52
0.51
0.71
0.72
0.70
0.69
0.51
0.34
0.63
0.55
0.61
0.64

0.67
0.67
-
-
-

0.83
0.81
0.74
0.77
0.63

0.81
0.79
0.81
0.76
0.65

0.68
0.64
0.56
0.43
0.52

0.61
0.61
-
-
-

0.69
0.69
0.71
0.72
0.58

0.51
0.51
-
-
-

0.56
0.56
-
-
-

Correlation ρ
Multiple mutations
cross-validation level
prot.
pos.
mut.
-0.63
-0.25
-0.28
-0.66
-0.25
-0.25
-
-0.21
-0.10
-
-0.21
-0.11
-
-0.57
-0.63
-0.65
-
-
0.57
0.64
0.46
0.47
0.68
0.71
-
-
0.74
0.73
0.30
0.58
0.73
0.73
-
-
-

0.78
0.73
0.78
0.48
0.73

0.95
0.96
0.96
0.97
0.70

0.75
0.67
-
-
-

0.66
0.64
-
-
-

-
-
-
-
-

-0.19
-0.01
-
-
-

0.96
0.93
-
-
-

-0.06
0.21
-
-
-

0.49
0.50
-
-
-

-
-
-
-
-

0.97
0.96
0.99
0.99
0.21

0.42
0.22
0.19
0.08
0.78

-0.39
-0.53
-0.29
-0.55
0.23

0.88
0.86
0.86
0.86
0.51

-

-

-
-

-
-
-
0.38
0.42
0.62
0.50
0.18
0.20
-
-
0.66
0.30
0.08
0.07
0.86
0.77
-
-
-0.02
0.33
0.52
0.46
-0.00
-0.00
-
-
0.61
0.64
0.52
0.55
0.39
0.40
-
-

0.65
0.64
0.68
0.64
0.33

0.64
0.59
0.61
0.23
0.50

0.86
0.86
0.76
0.85
0.67

0.35
0.35
-
-
-

0.68
0.69
-
-
-

0.53
0.53
-
-
-

All mutations
cross-validation level
prot.
pos.
mut.
0.09
-0.09
0.08
0.14
-0.07
0.14
-
-0.23
-0.13
-
-0.20
-0.14
-
-0.09
0.09
0.14
-
-
0.34
0.40
0.18
0.21
0.34
0.35
-
-
0.71
0.75
0.18
0.16
0.65
0.70
-
-
0.49
0.51
0.40
0.25
0.49
0.53
0.72
0.60
0.55
0.51
0.04
0.10
0.54
0.65
-
-
0.52
0.33
-0.05
-0.00
0.51
0.39
-
-
-0.13
0.05
0.84
0.70
-0.08
0.13
-
-
0.64
0.66
0.50
0.49
0.48
0.49
-
-

0.12
0.12
-
-
-

0.65
0.66
-
-
-

0.73
0.55
-
-
-

0.52
0.52
-
-
-

0.84
0.81
0.75
0.78
0.61

0.52
0.40
0.35
0.30
0.42

0.85
0.81
0.83
0.76
0.08

0.83
0.82
0.83
0.80
0.60

3.46
3.50
-
-
-

1.47
1.48
1.38
1.48
2.26

0.47
0.50
0.49
0.70
0.54

2.44
2.44
-
-
-

1.25
1.48
1.26
1.67
1.99

1.00
1.00
-
-
-

mut.
0.74
0.70
0.99
0.99
0.75

Point mutations
cross-validation level
prot.
pos.
1.65
0.72
1.65
0.71
-
1.00
-
0.99
-
0.76
1.67
1.00
0.85
1.65
1.64
1.78
1.84
2.22
3.61
1.43
1.45
1.73
1.85
2.62
3.37
2.08
2.45
2.33
2.22
0.53
0.54
0.58
0.76
0.54
0.99
0.59
0.70
1.17
1.24
1.75
1.78
1.18
1.20
0.97
1.14
1.70
1.68
2.24
2.01
1.76
1.58
1.36
1.54
1.28
1.32
1.43
1.73
1.52
1.80
1.26
1.31
1.26
1.30
1.54
1.95
1.38
1.63
1.40
1.37

0.85
0.92
1.06
1.00
1.12

1.18
1.18
-
-
-

1.41
1.42
1.38
1.53
1.57

1.56
1.56
-
-
-

1.27
1.32
1.43
1.68
1.52

1.83
1.82
-
-
-

1.07
1.11
1.04
1.26
1.35

1.61
1.62
-
-
-

0.69
0.57
0.36
0.31
1.22

2.35
2.17
-
-
-

mut.
1.64
1.62
1.30
1.29
1.65

rmse
Multiple mutations
cross-validation level
prot.
pos.
2.70
1.48
2.43
1.47
-
1.35
-
1.33
-
1.45
2.45
-
-
1.02
0.93
1.15
1.16
1.29
2.05
-
-
2.24
1.81
3.39
2.11
1.62
2.01
-
-
-
-
-
-
-

1.71
1.90
-
-
-

2.22
1.82
3.44
2.12
1.63

-
-
-
-
-

-
-
-
-
-

0.19
0.35
0.12
0.13
0.98

5.67
6.21
6.81
6.89
7.08

2.17
2.86
1.72
2.52
7.43

1.33
1.43
1.44
1.45
2.49

-
-
0.91
0.77
0.58
0.82
1.01
1.24
-
-
5.95
6.14
6.88
6.77
7.24
6.90
-
-
7.66
7.07
4.19
6.58
7.43
7.68
-
-
2.45
2.40
2.65
2.56
2.99
2.74
-
-

1.28
0.99
-
-
-

4.20
5.43
-
-
-

7.69
7.70
-
-
-

2.53
2.50
-
-
-

All mutations
cross-validation level
prot.
2.02
1.92
-
-
-

mut.
1.09
1.06
1.09
1.09
1.10

pos.
1.21
1.20
1.21
1.20
1.20

1.35
1.35
1.24
1.33
2.09

1.40
1.53
1.69
1.73
1.95

0.47
0.50
0.49
0.70
0.54

0.81
0.88
1.01
0.95
1.10

2.65
2.85
3.05
3.14
3.23

1.40
1.57
1.47
1.80
2.88

1.13
1.18
1.14
1.30
1.66

1.94
-
-
1.45
1.43
1.59
1.63
1.94

3.35
-
-
1.85
1.84
2.81
3.14
1.99

2.40
-
-
0.53
0.54
0.58
0.76
0.54

0.99
0.59
0.70

1.13
1.17
1.61
1.66
1.15

1.20
-
-
4.75
4.90
5.53
5.41
5.74

3.16
-
-
3.64
3.40
2.28
3.34
3.62

3.09
-
-
1.87
1.85
2.09
2.23
2.22

1.92
-
-

3.26
3.27
-
-
-

2.36
2.38
-
-
-

1.00
1.00
-
-
-

1.19
1.17
-
-
-

2.23
2.64
-
-
-

3.11
3.11
-
-
-

1.84
1.84
-
-
-

∗ dataset for 1PIN contained no multiple mutations.

14

Table S6: Comparison of diﬀerent methods on the 15 protein dataset with respect to ρ and rmse after removing 10% of predictions with largest errors.
Mutation, position, and protein are referred to as mut., pos., and prot., respectively. Oﬀ-the-shelf implementations of Rosetta, mCSM and PoPMuSiC are
used directly without cross-validation.

Results after 10 %
outlier removal

Method
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled

Point mutations
cross-validation level
prot.
pos.
0.75
0.77
0.75
0.73
-
0.57
-
0.27
-
0.78

mut.
0.87
0.86
0.89
0.86
0.80

Correlation ρ
Multiple mutations
cross-validation level
prot.
pos.
0.63
0.84
0.65
0.84
-
0.76
-
0.76
-
0.71

mut.
0.97
0.96
0.98
0.97
0.73

Oﬀ-the-shelf implementations with no cross-validation

All mutations
cross-validation level
prot.
pos.
0.71
0.81
0.72
0.80
-
0.74
-
0.66
-
0.75

mut.
0.92
0.91
0.93
0.91
0.78

Point mutations
cross-validation level
prot.
pos.
0.93
0.85
0.93
0.85
-
1.01
-
1.30
-
0.87

mut.
0.69
0.69
0.65
0.73
0.84

rmse
Multiple mutations
cross-validation level
prot.
pos.
1.90
1.55
1.85
1.43
-
1.66
-
1.48
-
1.99

mut.
0.62
0.64
0.49
0.54
1.67

All mutations
cross-validation level
prot.
1.13
1.12
-
-
-

mut.
0.67
0.67
0.62
0.69
1.00

pos.
1.15
1.10
1.23
1.37
1.31

Rosetta
mCSM
PoPMuSiC

0.75
0.71
0.73

0.67
-
-

0.73
-
-

0.94
0.89
0.86

1.85
-
-

1.11
-
-

Figure S9: rmse with diﬀerent amount of predictions, when predictions are sorted by the error. Position level cross-validation was used for mGPfusion,
mGP and Rosetta scaled.

15

Figure S10: Learning curves.

16

Figure S11: Learning curves.

17

Figure S12: Learning curves.

18

Figure S13: Transformations for all 15 proteins presented in Table 1. The red circles mark the simulated ∆∆G-values yS with respect to the experimental
measured ∆∆G-values yE. Thin black lines show possible transformations for yS, whereas the thick black line shows the selected transformation from yS
to ˜yS.

19

mGPfusion: Predicting protein stability changes with
Gaussian process kernel learning and data fusion

Emmi Jokinen1, Markus Heinonen1,2 and Harri L¨ahdesm¨aki1
1Department of Computer Science, Aalto University, 02150 Espoo, Finland
2Helsinki Institute for Information Technology, Finland

Abstract

Motivation: Proteins are commonly used by biochemical industry for numerous processes. Reﬁning these proteins’ properties via
mutations causes stability eﬀects as well. Accurate computational method to predict how mutations aﬀect protein stability are necessary
to facilitate eﬃcient protein design. However, accuracy of predictive models is ultimately constrained by the limited availability of
experimental data.
Results: We have developed mGPfusion, a novel Gaussian process (GP) method for predicting protein’s stability changes upon single
and multiple mutations. This method complements the limited experimental data with large amounts of molecular simulation data.
We introduce a Bayesian data fusion model that re-calibrates the experimental and in silico data sources and then learns a predictive
GP model from the combined data. Our protein-speciﬁc model requires experimental data only regarding the protein of interest and
performs well even with few experimental measurements. The mGPfusion models proteins by contact maps and infers the stability
eﬀects caused by mutations with a mixture of graph kernels. Our results show that mGPfusion outperforms state-of-the-art methods in
predicting protein stability on a dataset of 15 diﬀerent proteins and that incorporating molecular simulation data improves the model
learning and prediction accuracy.
Availability: Software implementation and datasets are available at github.com/emmijokinen/mgpfusion
Contact: emmi.jokinen@aalto.ﬁ

8
1
0
2
 
r
a

M
 
3
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
2
5
8
2
0
.
2
0
8
1
:
v
i
X
r
a

1 Introduction

Proteins are used in various applications by pharmaceutical, food, fuel,
and many other industries and their usage is growing steadily (Kirk
et al., 2002; Sanchez and Demain, 2010). Proteins have important ad-
vantages over chemical catalysts, as they are derived from renewable
resources, are biodegradable and are often highly selective (Cherry and
Fidantsef, 2003). Protein engineering is used to further improve the
properties of proteins, for example to enhance their catalytic activity,
modify their substrate speciﬁcity or to improve their thermostability
(Rapley and Walker, 2000). Increasing the stability is an important as-
pect of protein engineering, as the proteins used in industry should be
stable in the industrial process conditions, which often involve higher
than ambient temperature and non-aqueous solvents (Bommarius et al.,
2011). The properties of a protein are modiﬁed by introducing alter-
ations to its amino acid sequence. Mutations in general tend to be
destabilising, and if too many destabilising mutations are implemented,
the protein may not remain functional without compensatory stabilis-
ing mutations (Tokuriki and Tawﬁk, 2009).

The stability of a protein can be deﬁned as the diﬀerence in Gibbs
energy ∆G between the folded and unfolded (or native and denatu-
rated) state of the protein. More precisely, the Gibbs energy diﬀerence
determines the thermodynamic stability ∆Gt of the protein, as it does
not take into account the kinetic stability ∆Gk which determines the
energy needed for the transition between the folded and unfolded states
(Anslyn and Dougherty, 2006) (see Supplementary Figure S1). Here we
will consider only the thermodynamic stability and from now on it will
be referred to merely as stability ∆G.

The eﬀect of mutations can be deﬁned by the change they cause
to the Gibbs energy ∆G, denoted as ∆∆G (Pace and Scholtz, 1997).
To comprehend the signiﬁcance of stability changes upon mutations,
we can consider globular proteins, the most common type of enzymes,
whose polypeptide chain is folded up in a compact ball-like shape with
an irregular surface (Alberts et al., 2007). These proteins are only
marginally stable and the diﬀerence in Gibbs energy between the folded
and unfolded state is only about 5–15 kcal/mol, which is not much more
than the energy of a single hydrogen bond that is about 2–5 kcal/mol
(Branden and Tooze, 1999). Therefore, even one mutation that breaks
a hydrogen bond can prevent a protein from folding properly.

The protein stability can be measured with many techniques, in-
cluding thermal, urea and guanidinium chloride (GdmCl) denaturation

curves that are determined as the fraction of unfolded proteins at dif-
ferent temperatures or at diﬀerent concentrations of urea or GdmCl
(Pace and Shaw, 2000). Some of the experimentally measured sta-
bility changes upon mutations have been gathered in thermodynamic
databases such as Protherm (Kumar et al., 2006).

A variety of computational methods have been introduced to predict
the stability changes upon mutations more eﬀortlessly than through ex-
perimental measurements. These methods utilise physics or knowledge-
based potentials (Leaver-Fay et al., 2011), their combinations, or diﬀer-
ent machine learning methods. The machine learning methods utilise
support vector machines (SVM) (Capriotti et al., 2005b, 2008; Chen
et al., 2013; Cheng et al., 2006; Folkman et al., 2014; Liu and Kang,
2012; Pires et al., 2014a), random forests (Tian et al., 2010; Wainreb
et al., 2011), neural networks (Dehouck et al., 2009; Giollo et al., 2014),
and Gaussian processes (Pires et al., 2014b). However, it has been as-
sessed that although on average many of these methods provide good
results, they tend to fail on details (Potapov et al., 2009). In addition,
many of these methods are able to predict the stability eﬀects only for
single-point mutations.

We introduce mGPfusion (mutation Gaussian Processes with data
fusion), a method for predicting stability eﬀects of both point and mul-
tiple mutations. mGPfusion is a protein-speciﬁc model – in contrast to
earlier stability predictors that aim to estimate arbitrary protein struc-
ture or sequence stabilities – and achieves markedly higher accuracy
while utilising data only from a single protein at a time. In contrast
to earlier works that only use experimental data to train their models,
we also combine exhaustive Rosetta (Leaver-Fay et al., 2011) simulated
point mutation in silico stabilities to our training data.

A key part of mGPfusion is the automatic scaling of simulated data
to better match the experimental data distribution based on those vari-
ants that have both experimental and simulated stability values. Fur-
thermore, we estimate a variance resulting from the scaling, which
places a higher uncertainty on very destabilising simulations. Our
Gaussian process model then utilises the joint dataset with their es-
timated heteroscedastic variances and uses a mixture of graph kernels
to assess the stability eﬀects caused by changes in amino acid sequence
according to 21 substitution models. Our experiments on a novel 15
protein dataset show a state-of-the-art stability prediction performance,
which is also sustained when there is access only to a very few experi-
mental stability measurements.

1

Figure 1: Pipeline illustration for mGPfusion. a) M = 21 substitution matrices utilise diﬀerent information sources and give scores to pairwise amino
acid substitutions. b) The wild-type structures from Protein Data Bank are modelled as contact graphs. c) The graph kernel measures similarity of two
sequences by a substitution model S over all positions p and their neighbourhoods nbs(p) in the contact graph. d) Each substitution matrix is used to
create a separate covariance matrix. e) Multiple kernel learning (MKL) is used for ﬁnding the optimal combination of the base kernels. The kernel matrix
measures variant similarities. f ) Experimentally measured ∆∆G values yE are gathered from Protherm and Rosetta’s ddg monomer application is used to
simulate the stability eﬀects yS for all single point mutations. g) Bayesian scaling for the simulated values yS at the x-axis. Possible scalings are coloured
with green and the chosen scaling from yS into scaled values ˜yS is marked by black dots. The scaling is ﬁtted to a subset of experimentally measured
stabilities yE (circles). h) The stability predictive GP model is trained using experimental and simulated data through the kernel matrix.

2 Methods

Following Pires et al. (2014b) we choose a Bayesian model family of
Gaussian processes for prediction of mutation eﬀects on protein sta-
bility due to its inherent ability to handle uncertainty in a principle
way. Bayesian modelling is a natural approach for combining the ex-
perimental and simulated data distribution, while it is also suitable for
learning the underlying mixture of substitution models that governs the
mutational process.

The pipeline for mGPfusion is presented in Figure 1. The ﬁrst
part of mGPfusion consists of collection of in silico and experimental
datasets discussed in Section 2.1, the scaling of the in silico dataset in
Section 2.2 and the fusion of these two datasets in Section 2.3. The sec-
ond part consists of the Gaussian process model described in Section 2.4
with detailed description of the graph kernels in Sections 2.5-2.6 and
model inference in Section 2.7. Finally, the evaluation criteria used are
described in Section 2.8.

teins that fulﬁlled these requirements are listed in Table 1. We aver-
aged the stability values for proteins with multiple measurements and
ignored mutations to residues not present in their 3D structures. These
data sets are available at github.com/emmijokinen/mgpfusion.

Protein (organism)

PDB

mutations
point

point (sim)

all

2LZM 349
T4 Lysozyme (Enterobacteria phage T4)
182
1BNI
Barnase (Bacillus amyloliquefaciens)
124
1VQB
Gene V protein (Escherichia virus m13)
116
1LZI
Glycosyltransferase A (Homo sapiens)
98
2CI2
Chymotrypsin inhibitor 2 (Hordeum vulgare)
89
1PGA
Protein G (Streptococcus sp. gx7805)
83
2RN2
Ribonuclease H (Escheria coli)
80
1CSP
Cold shock protein B (Bacillus subtilis)
80
1BVC
Apomyoglobin (Physeter catodon)
63
4LYZ
Hen egg white lysozyme (Gallus gallus)
57
Ribonuclease A (Bos taurus)
1RTB
56
Peptidyl-prolyl cis-trans isomerase (Homo sapiens) 1PIN
53
1RN1
Ribonuclease T1 isozyme (Aspergillus oryzae)
54
1RGG
Ribonuclease (Streptomyces auerofaciens)
53
1BPI
Bovine pancreatic trypsin inhibitor (Bos taurus)

264
163
92
114
77
34
65
50
56
50
50
56
48
45
47

3116
2052
1634
2470
1235
1064
2945
1273
2907
2451
2356
2907
1957
1824
1102

2.1 Experimental and in silico data

total

1537

1211

31293

Protherm is a database of numerical thermodynamic parameters for
proteins and their mutants (Kumar et al., 2006). From Protherm we
gathered all proteins with at least 50 unique mutations whose ∆∆G
has been measured by thermal denaturation, and where a PDB code
for a 3D structure of the protein was reported. We required the pro-
teins to have at least 50 unique mutations, so that we would have a
representative test set and get suﬃciently reliable estimates of predic-
tion accuracy on individual proteins and examine how the amount of
experimental training data aﬀects the accuracy of the model. The 3D
structures are necessary for obtaining the connections between residues.
We collected the 3D structures with the reported PDB codes from the
Protein Databank, www.rcsb.org (Berman et al., 2000). The 15 pro-

Table 1: The 15 protein data from ProTherm database with counts of point
mutations, all mutations, and of simulated point mutation stability changes.

We also generate simulated data of the stability eﬀects of all possible
single mutations of the proteins. Our method can utilise any simulated
stability values. We used the “ddG monomer” application of Rosetta
3.6 (Leaver-Fay et al., 2011) using the high-resolution backrub-based
protocol 16 recommended in Kellogg et al. (2011). The predictions yS
made with Rosetta are given in Rosetta Energy Units (REU). Kellogg
et al. (2011) suggest transformation 0.57yS for converting the predic-
tions into physical units. The simulated data scaled this way is not
as accurate as the experimental data, the correlation and root mean

2

square error (rmse) with respect to the experimental data are shown
for all proteins in Table 2 and for individual proteins in Supplementary
Table S2, on rows labelled Rosetta. For this reason, we use instead a
Bayesian scaling described in the next section and diﬀerent noise models
for the experimental and simulated data, described in Section 2.3.

For each of the 15 proteins,

let xi = (xi1, . . . , xiM ) denote its
M -length variant i with positions p labelled with residues xip ∈
{A, R, N, . . . , V }. We denote the wild-type protein as x0. We collect
15 separate sets of simulated and experimental data. We denote the
NE )T with
NE experimental variants of each protein as XE = (xE
the corresponding experimental stability values yE = (yE
NE )T ∈
RNE . Similarly, we denote the NS simulated observations as XS =
(xS

1 , . . . , xE

1 , . . . , yE

NS )T and yS = (yS

NS )T ∈ RNS .

1 , . . . , xS

1 , . . . , yS

2.2 Bayesian scaling of in silico data

The described transformation from REU to physical units may not
be optimal for all proteins. We therefore applied instead a linear-
exponential scaling function to obtain scaled Rosetta simulated sta-
bilities ˜yS,

˜yS = g(yS | θj) = ajecj yS

+ bjyS + dj.

(1)

This scaling transforms the Rosetta simulations yS for each protein
j = 1, . . . , 15 to correspond better to the experimental data. The pa-
rameters θj = (aj, bj, cj, dj) deﬁne the weight aj and steepness cj of the
exponential term, while the linear term has slope bj and intercept dj.
To avoid overﬁtting, we perform Bayesian linear regression and start
by deﬁning parameter prior p(θj) = p(aj)p(bj)p(cj)p(dj) that reﬂects
our beliefs about realistic scalings having only moderate steepness:

and stabilities y = (y0, yE, ˜yS) of size RN where N = 1 + NE + NS is
the total number of simulated and experimental data points, including
the wild-type. We assume heteroscedastic additive noise models for the
three information sources

y0 = f (x0) + ε0,
(cid:1) + εE
i = f (cid:0)xE
yE
i ,
(cid:1) + εS
i = f (cid:0)xS
˜yS
i ,

i

i

(cid:1)

(cid:1)

0

ε0 ∼ N (cid:0)0, σ2
i ∼ N (cid:0)0, σ2
εE
(cid:16)
εS
i ∼ N

E

0, (σE + σS + tσT (i))2(cid:17)

,

(5)

where the observed values are noisy versions of the underlying ‘true’
stability function f (x) corrupted by zero-mean noise with data source
speciﬁc variances. We learn a Gaussian process based stability function
f (x) in the next Section.

The Equations (5) encode that the experimental data are corrupted
by a global experimental noise variance σ2
E. The simulated stabilities
are additionally corrupted by a global Rosetta simulator error variance
σ2
S, and by the value-dependent transformation variance tσ2
T (i) scaled
by parameter t. The model then encapsulates that we trust the Rosetta
data less than the experimental data. By deﬁnition, the ∆∆G of the
wild-type is zero (y0 = 0) with very small assumed error, σ0 = 10−6.
Note that σ2
T are ﬁxed by equation (4), while we infer the optimal
values for the remaining three free parameters (σE, σR, t) (See Section
2.4). The parameters σ2

S are assigned priors

E and σ2

σE ∼ Gamma(σE|αE, βE)
σS ∼ Gamma(σR|αS, βS).

(6)

The values of these hyperparameters are shown in Supplementary Ta-
ble S1.

p(aj) = Gamma(aj | αa, βa)
p(bj) = Beta(1/2 · bj | αb, βb)
p(cj) = Beta(10/3 · cj | αc, βc)
p(dj) = N (dj | µd, σ2

d).

2.4 Gaussian processes

(2)

We use a Gaussian process (GP) function f to predict the stability
f (x) ∈ R of a protein variant x. Gaussian processes are a family of
non-parametric, non-linear Bayesian models (Rasmussen and Williams,
2006). A zero-mean GP prior

The empirically selected hyperparameter values are listed in Supple-
mentary Table S1 and the priors are illustrated in Figure S2.

We compute the posterior for θj using the subset of simulated data

that have corresponding experimentally measured data:

p(θj|yE, yS) ∝

(cid:89)

N (cid:0)yE

i

| g(yS

i |θj), σ2
n

(cid:1) p(θj).

i:xi∈XE ∩XS

The product iterates over all NE∩S simulated ∆∆G’s that have a
matching experimentally observed value. The σ2
n is the scaling error
variance, which was set to σ2
n = 0.5. The parameters θ for each protein
were sampled using a random walk Metropolis-Hastings MCMC algo-
rithm (the mhsample function in Matlab) for NM C = 10000 samples
with a burn-in set to 500. The proposal distribution was selected to be
a symmetric uniform distribution such that [as+1, bs+1, cs+1, ds+1] ∼
U (as ± 0.4, bs ± 0.04, cs ± 0.04, ds ± 0.4). Given the sample of scaling
parameters (θ(s)
s=1 , we deﬁne the scaled simulated data as the aver-
age scaling over the MCMC sample, and record also the sample scaling
variance

j )NM C

˜yS
i =

g(yS

i |θ(s)
j )

1
NM C

1
NM C

NM C(cid:88)

s=1
NM C(cid:88)

(cid:16)

s=1

σ2
T (i) =

g(yS

i |θ(s)

j ) − ˜yS
i

(cid:17)2

.

See Figure 1 g) for an illustration of the scaling. We collect the scaled
simulated value and its variance from each simulated point into vectors
˜yS = (˜yS

T (NS)) ∈ RNS .

T (1), . . . , σ2

NS ) and σ2

1 , . . . , ˜yS

T = (σ2

deﬁnes a distribution over functions f (x) whose mean and covariance
are

f (x) ∼ GP (0, k(x, x(cid:48))) ,

E[f (x)] = 0
cov[f (x), f (x(cid:48))] = k(x, x(cid:48)).

For any collection of protein variants X = x1, . . . , xN , the function val-
ues follow a multivariate normal distribution f ∼ N (0, KXX ), where
f = (f (x1), . . . , f (xN ))T ∈ RN , and where KXX ∈ RN ×N with
[KXX ]ij = k(xi, xj). The key property of Gaussian processes is that
they encode functions that predict similar stability values f (x), f (x(cid:48))
for protein variants x, x(cid:48) that are similar, as encoded by the kernel
k(x, x(cid:48)). The key part of GP modelling is then to infer a kernel that
measures the mutation’s eﬀects to the stability.

Let a dataset of noisy stability values from two sources be y ∈ RN ,
the corresponding protein structures X = (xi)N
i=1, and a new protein
variant x∗ whose stability we wish to predict. A Gaussian process de-
ﬁnes a joint distribution over the observed values y of variants X, and
the unknown function value f (x∗) of the unseen variant x∗,
(cid:21)(cid:19)

(cid:18)

(cid:21)

(cid:20) y
f (x∗)

∼ N

0,

(cid:20)KXX + diag(σ2)
k∗X

kX∗
k(x∗, x∗)

,

∗X ∈ RN is a kernel vector with elements k(xi, x∗) for
where kX∗ = kT
T )2)T
all i = 1, . . . , N , and where σ2 = (σ2
collects ﬁnal variances of the data points from equations (5). Here
the exponents are elementwise. The conditional distribution gives the
posterior distribution of the stability prediction as

E1T , (σE1T + σS1T + tσT

0, σ2

f (x∗)|(X, y) ∼ N (cid:0)µ(x∗), σ2(x∗)(cid:1) ,

2.3 Data fusion and noise models

where the prediction mean and variance are

For each protein j, we organise its experimental data (XE, yE) and
transformed simulated data (XS, ˜yS) along with the wild-type infor-
mation (x0, y0) into a single joint dataset of variants X = (x0, XE, XS)

(cid:0)KXX + diag(σ2)(cid:1)−1 y,

µ(x∗) = k∗X
σ2(x∗) = k(x∗, x∗) − k∗X

(cid:0)KXX + diag(σ2)(cid:1)−1 kX∗.

(3)

(4)

3

Hence, in GP regression the stability predictions µ(x∗) ± σ(x∗) will
come with uncertainty estimates.

ring amino acids and scaled them between zero and one as

2.5 Graph kernel

Next, we consider how to compute the similarity function k(x, x(cid:48)) be-
tween two variants of the same protein structure. We will encode the
3D structural information of the two protein variants as a contact map
and measure their similarity by the formalism of graph kernels (Vish-
wanathan et al., 2010).

We consider two residues to be in contact if their closest atoms are
within 5 ˚A of each other in the PDB structure, which is illustrated in
Figure 1 b). All variants of the same protein have the same length, with
only diﬀerent residues at mutating positions. Furthermore, we assume
that all variants share the wild-type protein contact map.

To compare protein variants, we construct a weighted decomposition
kernel (WDK) (Menchetti et al., 2005) between two protein variants
x = (x1, . . . , xM ) and x(cid:48) = (x(cid:48)

M ) of length M ,

1, . . . , x(cid:48)

k(x, x(cid:48)) =


S(xp, x(cid:48)
p)

M
(cid:88)

p=1



(cid:88)

S(xl, x(cid:48)
l)

,

l∈nbs(p)

(7)

where nbs(p) deﬁnes the set of neighbouring positions to position p,
and S is a substitution matrix. The kernel iterates over all positions
p and compares for each of them their residues through a substitution
matrix S(xp, x(cid:48)
p). Furthermore, the similarity of the residues at each
position is multiplied by the average similarity of the residues at its
neighbouring positions S(xl, x(cid:48)
l). Hence, the kernel deﬁnes the simi-
larity of two protein variants as the average position and neighbour-
hood similarity over all positions. The kernel matrix is normalised
so that for two data points, the normalised kernel
p) =
k(xp, x(cid:48)
p, x(cid:48)
p), as deﬁned by Shawe-Taylor and Cris-
tianini (2004). The kernel is illustrated in Figure 1 c).

k(xp, xp)k(x(cid:48)

is ˆk(xp, x(cid:48)

p)/

(cid:113)

The above WDK kernel allows us to compare the eﬀects of multiple
simultaneous mutations. However, as the wild type protein structure
is used for all of the protein variants, changes that the mutations may
cause to the protein structure are not taken into consideration. This
may cause problems if mutations that alter the protein structure sig-
niﬁcantly are introduced – especially if many of them are introduced
simultaneously. On the other hand, substitution matrices that have
their basis in sequence comparisons, should take these eﬀects into ac-
count to some extend as these kinds of mutations are usually highly
destabilising and do not occur often in nature. In the next section, we
will discuss how we utilise diﬀerent substitution matrices with multiple
kernel learning.

The BLOSUM substitution models have been a common choice for
protein models (Giguere et al., 2013), while mixtures of substitution
models were proposed by Cichonska et al. (2017). BLOSUM matrices
score amino acid substitutions by their appearances throughout evolu-
tion, as they compare the frequencies of diﬀerent mutations in similar
blocks of sequences (Henikoﬀ and Henikoﬀ, 1992). However, there are
also diﬀerent ways to score amino acids substitutions, such as chemical
similarity and neighbourhood selectivity (Tomii and Kanehisa, 1996).
When the stability eﬀects of mutations are evaluated, the frequency of
an amino acid substitution in nature may not be the most important
factor.

To take into account diﬀerent measures of similarity between amino
acids, we employed a set of 21 amino acid substitution matrices gath-
ered from AAindex21 (Kawashima et al., 2008). AAindex2 currently
contains 94 substitution matrices. From these we selected those that
had no gaps concerning substitutions between the 20 naturally occur-

1http://www.genome.jp/aaindex/
2http://www.cs.ubc.ca/~schmidtm/Software/minConf.html

4

S =

S0 − min(S0) + 1
max(S0) − min(S0) + 1

.

Out of these matrices, we only chose those 23 matrices that were pos-
itive semideﬁnite. Furthermore, there were two pairs of matrices that
were extremely similar, and we only selected one matrix from each pair,
ending up with 21 substitution matrices. These substitution matrices
are used together with Equation 7 for computing 21 base kernel matri-
ces. Finally, MKL is used to ﬁnd an optimal combination of the base
kernels of form

(8)

(9)

Kφ =

wmK (γm)
m ,

21
(cid:88)

m=1

where wm is a kernel speciﬁc weight, γm is an (elementwise) exponent.
The elementwise exponent retains the SDP property of Kφ (Shawe-
Taylor and Cristianini, 2004). We observe empirically that the optimal
kernel weights wm tend to be sparse (See Figure 2).

The selected substitution matrices are listed in Figure 2. These ma-
trices have diﬀerent basis and through multiple kernel learning (MKL)
our model learns which of these are important for inferring the stability
eﬀects that mutations cause on diﬀerent proteins. The ﬁgure illustrates
this by showing the average weights of the base kernel matrices obtained
via the multiple kernel learning.

2.7 Parameter inference

The complete model has ﬁve parameters φ = (σE, σS, t, w, γ) to in-
fer, of which the variance parameters (σE, σS, t) parameterise the joint
data variance σ2
φ, while the MKL parameters w = (w1, . . . , w21) and
γ = (γ1, . . . , γ21) parameterise the kernel matrix Kφ. In a Gaussian
process model these can be jointly optimised by the marginal (log)
likelihood with priors

log p(y|φ)p(σE)p(σR) = log

p(y|f , φ)p(f |φ)p(σE)p(σR)df

(cid:90)

∝ −

yT (Kφ + diag(σ2

1
2
+ log Gamma(σE|αE, βE) + log Gamma(σS|αS, βS),

log |Kφ + diag(σ2

φ))−1y −

φ)|

1
2

(10)

which automatically balances model ﬁt (the square term) and the
model complexity (the determinant) to avoid overﬁtting (Rasmussen
and Williams, 2006). The parameters can be optimised by maximis-
ing the marginal log likelihood (10) using gradient ascent, since the
marginal likelihood can be diﬀerentiated analytically (see Supplemen-
tary Equations S1 and S2). We utilised a limited-memory projected
quasi-Newton algorithm (minConf TMP2), described by (Schmidt et al.,
2009).

We chose to evaluate the accuracy of our predictions using the same
metrics that have been used by many others – correlation ρ between the
predicted and experimentally measured ∆∆G values (Capriotti et al.,
2005a; Dehouck et al., 2009; Kellogg et al., 2011; Pires et al., 2014b;
Potapov et al., 2009) and the root mean square error (rmse) (Dehouck
et al., 2009; Pires et al., 2014a,b), which are determined in the Supple-
mentary Equations S3 and S4. We use marginal likelihood maximisa-
tion to infer model parameters and perform cross-validation to evaluate
the model performance on test data. Below we only report evaluation
metrics obtained from the test sets not used at any stage of the model
learning or data transformation sampling.

3 Results

In this section we evaluate the performance of mGPfusion on predict-
ing stability eﬀects of mutations, and compare it to the state-of-the-
art prediction methods mCSM, PoPMuSiC and Rosetta. Rosetta is a

2.6 Substitution matrices and multiple kernel

2.8 Evaluation criteria

learning

Figure 2: Average weights for kernels utilising the described substitution matrices from AAindex2, when GP models were trained with mutation level
cross-validation. Basis for the substitution matrices are obtained from (Tomii and Kanehisa, 1996). ∗ were added to AAindex2 in a later release, and their
basis were not determined by Tomii and Kanehisa (1996).

molecular modelling software whose ddg monomer module can directly
simulate the stability changes ∆∆G of a protein upon mutations. PoP-
MuSic and mCSM are machine learning models that predict stability
based on protein variant features. We run Rosetta locally, and use
mCSM and PoPMuSiC models through their web servers3. This may
give these methods an advantage over mGPfusion since parts of our
testing data were likely used within their training data.

We compare four diﬀerent variants of our method: mGPfusion that
uses both simulated data and MKL, “mGPfusion, only B62” that uses
simulated data but incorporates only one kernel matrix (BLOSUM62
substitution matrix), mGP model that uses MKL but does not use simu-
lated data, and “mGP, only B62” that uses only the base GP model but
does not incorporate simulated data and uses only the BLOSUM62 sub-
stitution matrix. In addition, we experiment on transforming Rosetta
predictions with the Bayesian scaling. We perform the experiments
for the 15 proteins separately using either position or mutation level
(leave-one-out) cross-validation regarding the methods mGP, mGPfu-
sion and the Bayesian scaling of Rosetta. Pires et al. (2014b) used
protein and position level cross-validation to evaluate their model. In
protein level cross-validation all mutations in a protein are either in
the test or training set exclusively. When we train our model using
protein level cross-validation, we use no experimental data and rely
only on the simulated data. Position level cross-validation is deﬁned
so that all mutations in a position are either in the test or training set
exclusively. However, datasets in Pires et al. (2014b) contained only
point mutations and therefore we had to extend the deﬁnition to also
include multiple mutations. In position level cross-validation we train
one model for each position using only the part of data that has a
wild-type residue in that position. Therefore, in position level cross-
validation we construct a test set that contains all protein variants that
have a mutation at position p and use as training set all the protein
variants that have a wild-type residue at that position. Dehouck et al.
(2009) evaluated their models by randomly selecting training and test
sets so that each mutation was exclusively in one of the sets, but both
sets could contain mutations from the same position of the same pro-

3biosig.unimelb.edu.au/mcsm and

omictools.com/popmusic-tool

tein. We call this mutation level cross-validation. When we use all
available experimental data with mutation level cross-validation, this
corresponds to leave-one-out cross-validation.

3.1 Predicting point mutations

Table 2 summarises the average prediction performance over all 15 pro-
teins for all compared methods, types of mutations and cross-validation
types. We ﬁrst compare the performances on single point mutations,
where mGPfusion and mGP achieve the highest performance with
ρ = 0.81 and rmse = 1.07 kcal/mol, and ρ = 0.81 and rmse = 1.04
kcal/mol, respectively with mutation level cross-validation. With only
one kernel utilising the BLOSUM62 matrix instead of MKL, the per-
formance decreases slightly, but the competing methods are still out-
performed, as mCSM achieves ρ = 0.64 and rmse = 1.37 kcal/mol,
PoPMuSic ρ = 0.61 and Rosetta ρ = 0.55. Applying Bayesian scaling
on Rosetta simulator improves the performance of standard Rosetta
from ρ = 0.55 to ρ = 0.65 and decreases the rmse from 1.63 kcal/mol
to 1.35 kcal/mol, which is interestingly even slightly better than the
performances of mCSCM and PoPMuSiC.

With position level cross-validation mGPfusion achieves the high-
est performance of ρ = 0.70 and rmse = 1.26 kcal/mol, likely due
to having still access to simulated variants from that position, since
they are always available to the learner. Without simulation data,
the baseline machine learning model mGP performance decreases to
ρ = 0.51 and rmse = 1.54 kcal/mol, thus demonstrating the impor-
tance of the data fusion. Cross-validation could not be performed for
the oﬀ-the-shelf methods mCSM and PoPMuSiC. Even still, mGPfusion
(trained with one or multiple kernels) outperforms competing state-of-
the-art methods and achieves markedly higher prediction performance
as quantiﬁed by both mutation and position level cross-validations.
Also mGP outperforms these methods when quantiﬁed by mutation
level cross-validation. With protein level cross-validation mGPfusion
achieves slightly better results than Rosetta.

5

Method
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled

Point mutations
cross-validation level
prot.
pos.
mut.
0.56
0.70
0.81
0.56
0.69
0.79
-
0.51
0.81
-
0.34
0.76
-
0.63
0.65

Correlation ρ
Multiple mutations
cross-validation level
prot.
pos.
mut.
0.49
0.61
0.88
0.50
0.64
0.86
-
0.52
0.86
-
0.55
0.86
-
0.39
0.51

All mutations
cross-validation level
prot.
pos.
mut.
0.52
0.64
0.83
0.52
0.66
0.82
-
0.50
0.83
-
0.49
0.80
-
0.48
0.60

Point mutations
cross-validation level
prot.
pos.
mut.
1.61
1.26
1.07
1.62
1.30
1.11
-
1.54
1.04
-
1.95
1.26
-
1.38
1.35

rmse
Multiple mutations
cross-validation level
prot.
pos.
mut.
2.53
2.45
1.33
2.50
2.40
1.43
-
2.65
1.44
-
2.56
1.45
-
2.99
2.49

All mutations
cross-validation level

mut.
1.13
1.18
1.14
1.30
1.66

pos.
1.87
1.85
2.09
2.23
2.22

prot.
1.84
1.84
-
-
-

Predictions from oﬀ-the-shelf implementations with no cross-validation

Rosetta
mCSM
PoPMuSiC

0.55
0.61
0.64

0.40
-
-

0.49
-
-

1.63
1.40
1.37

2.74
-
-

1.92
-
-

Table 2: Comparison of diﬀerent methods on the 15 protein dataset with respect to ρ and rmse. Mutation, position, and protein are referred to as mut.,
pos., and prot., respectively. Predictions from oﬀ-the-shelf implementations of Rosetta, mCSM and PoPMuSiC are used directly without cross-validation.

3.2 Predicting multiple mutations

Next, we tested stability prediction accuracies for variants containing
either single or multiple mutations. Figure 3 shows a scatter plot of
mGPfusion predictions for all 1537 single and multiple mutation vari-
ants (covering all 15 proteins) against the experimental ∆∆G values
using the mutation level (leave-one-out) cross-validation. The points
are coloured by the number of simultaneous mutations in the variants,
with 326 variants having at least 2 mutations (See Table 1). Fig-
ure 3 illustrates the mGPfusion’s overall high accuracy of ρ = 0.83
and rmse = 1.13 kcal/mol on both single and multiple mutations (See
Table 2). Scatter plots for the individual proteins can be found in Sup-
plementary Figure S3. Dehouck et al. (2009) suggested that considering
the predictive power after removal of most badly predicted stability ef-
fects of mutations may give more relevant evaluation, as some of the
experimental measurements may have been made in non-physiological
conditions or aﬀected by signiﬁcant error, associated with a poorly re-
solved structure, or indexed incorrectly in the database. They thus
reported correlation and rmse of the predictions after excluding 10 %
of the predictions with most negative impacts on the correlation co-
eﬃcient. Pires et al. (2014b) also reported their accuracy after 10 %
outlier removal. If we remove the 10% worst predicted stability eﬀects
from the combined predictions, we achieve correlation ρ of 0.92 and
rmse of 0.67 kcal/mol. We report these results for all the methods
in Supplementary Table S3 and also present the error distribution in
Supplementary Figure S5.

Figure 3: Scatter plot for the mutation level (leave-one-out) predictions made
for all 15 proteins (See Table 1). The colour indicates the number of simul-
taneous mutations.

The high accuracy is retained for variants with multiple mutations
as well (ρ = 0.88 and rmse = 1.33 kcal/mol, see Tables 2 and S2).
Table 3 lists mGPfusion’s rmse for diﬀerent number of simultaneous
mutations. The model accuracy in fact improves up to 6 mutations.
This is explained by the training set often containing the same single
point mutations that appear in variants with multiple mutations. The
model can then infer the combined eﬀect of pointwise mutations. The
model seems to fail when predicting the eﬀects of 7-9 simultaneous mu-

6

tations. Most of these mutations (8/12) are for Ribonuclease (1RGG)
and their eﬀects seem to be exceptionally diﬃcult to predict. This may
be because only few of the point mutations that are part of the multiple
mutations are present in the training data. However, these mutations
seem to be exceptionally diﬃcult to predict for Rosetta as well, which
could indicate that the experimental measurements concerning these
mutations are not quite accurate. PoPMuSiC and mCSM are unable
to predict multiple mutations, while Rosetta supports them, but its
rmse accuracy decreases already with two mutations.

mutations
occurences

mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta

1
1211

1.07
1.11
1.04
1.26
1.35
1.63

2
207

1.06
1.12
1.03
0.96
2.10
2.27

3
52

0.80
0.77
0.61
0.65
1.92
2.11

4
42

0.51
0.59
0.50
0.83
2.94
3.78

5
4

0.40
0.29
0.18
0.26
2.29
2.93

6
8

1.01
1.14
0.92
1.14
2.32
2.21

7
3

3.02
3.00
3.23
2.95
2.93
2.92

8
3

5.89
6.78
6.18
6.90
6.75
5.80

9
6

5.16
5.56
6.75
6.57
7.28
7.45

10
1

0.25
0.11
0.08
0.05
2.69
3.42

Table 3: Root-mean-square errors for diﬀerent number of simultaneous
mutations for all 15 proteins, with models trained by leave-one-out cross-
validation. Rosetta is added for comparison.

With multiple mutations, the decrease in performance between the
position and mutation level cross-validations becomes clearer than with
single mutations. With the position level cross-validation the stability
eﬀects of multiple mutations are predicted multiple times, which partly
explains this loss of accuracy. For example, the eﬀects of mutants with
nine diﬀerent simultaneous mutations, which were the most diﬃcult
cases in the mutation level cross-validation, are predicted nine times.
Surprisingly, mGPfusion trained with protein level cross-validation
achieves higher correlation and smaller errors than Rosetta; mGPfusion
utilising simulated ∆∆G values for only single mutations, can predict
the stability eﬀects of multiple mutations better than Rosetta.

3.3 Uncertainty of the predictions

Gaussian processes provide a mean µ(x) and a standard deviation σ(x)
for the stability prediction of a protein variant x. The standard de-
viation allows estimation of the prediction accuracy even without test
data. Figure 1 h) visualises the uncertainty of a few predictions made
for the protein G (1PGA) when mutation level cross-validation is used.
The estimated standard deviation allows a user to automatically iden-
tify low quality predictions that can appear e.g. in parts of the input
protein space from which less data is included in model training. Con-
versely, in order to minimise the amount of uncertainty in the mGPfu-
sion predictions, estimated standard deviation can be used to guide next
experiments. The probabilistic nature of the predictions also admits an
alternative error measure of negative log probability density (NLPD)
nlpd = − (cid:80)N
i=1 log p(yi|µ(xi), σ2(xi)), which can naturally take into
account the prediction variance.

3.4 Eﬀect of training set size

The results presented in Sections 3.1–3.3 used all available data for
training with cross-validation to obtain unbiased performance mea-
sures. The inclusion of thousands of simulated variants allows the model

to learn accurate models with less experimentally measured variants.
Hence, we study how the mGPfusion model with or without simulated
data performs with reduced number of experimental observations. To
facilitate this, we randomly selected subsets of experimental data of
size 0, 10, 20, and so on. We learned the mGP and mGPfusion models
with these reduced experimental data sets while always using the full
simulated data sets. This also allows us to estimate how the models
work with diﬀerent number of cross-validation folds. For example, the
point of a learning curve which utilises 2/3 or 4/5 of the training data
correspond to an average of multiple 3-fold or 5-fold cross-validations,
respectively.

The learning curve in Figure 4a) shows how the averaged correlation
for protein 2LZM improves when the size of the experimental data set
increases. The right-most values at N = 348 are obtained with leave-
one-out cross-validation. The inclusion of simulated data in mGPfusion
(dark blue line) consistently improves the performance of mGP, which
is trained without simulated data. Figure 4b) illustrate the diﬀerence
in root mean square error. Learning curves for all proteins listed in
Table 1 can be found from the Supplementary Figures S6-S8. When
the number of experimental samples is zero, the mGPfusion model is
trained solely using the simulated data with scaling 0.57yS, and the
mGP model predicts the stability eﬀect of every mutation as zero. The
last point on the learning curves is obtained with mutation level cross-
validation (see Tables 2 and S2).

proteins mGPfusion model trained with only one kernel that utilises
BLOSUM62, provides approximately as good results as the mGPfu-
sion model trained with multiple kernels. However, with many of the
proteins, utilising just BLOSUM62 does not seem to be suﬃcient and
the accuracy of the model can be improved by using diﬀerent substi-
tution matrices. Prior knowledge of appropriate substitution models
for each protein could enable creation of accurate prediction models
with just one substitution model, but the MKL seems to be a good
tool for selecting suitable substitution models when such knowledge is
not available. It seems that the data fusion and number or relevance
of used substitution matrices can compensate each other – the learning
curves show, that the diﬀerence between mGPfusion models trained
with one or multiple kernels is smaller than the diﬀerence between the
mGP models utilising one or multiple kernels. This indicates that if ad-
ditional simulated data is exploited, the use of multiple or appropriate
substitution models is not as important than without the data fusion.
On the other hand, if data fusion is not applied, the use of MKL can
more signiﬁcantly improve the accuracy of the mGP model.

3.6 Eﬀect of

the Bayesian transformation on

Rosetta

The Bayesian scaling of simulated Rosetta values, proposed in Sec-
tion 2.2, improves the match of Rosetta simulated values to empirical
∆∆G values even without using the Gaussian process framework. The
Bayesian scaling improves the performance of standard Rosetta sim-
ulations from ρ = 0.55 and rmse = 1.63 kcal/mol to ρ = 0.65 and
rmse = 1.35 kcal/mol (see Table 2 and Supplementary Table S2). This
shows that the scaling proposed by Kellogg et al. (2011) indeed is not
always the optimal scaling and signiﬁcant improvements can be gained
by optimising the scaling using a set of training data.

Figure 1 g) visualises the Bayesian scaling for protein 1PGA, where
the very destabilising ∆∆G values are dampened by the scaling (black
dots) to less extreme values by matching the scaled simulated values to
the experimental points (blue circles). The black dots along the scaling
curve indicate the grid of point mutations after transformation. The
scaling variance σ2
T is indicated by the green region’s vertical width,
and on the right panel. The scaling tends to dampen very small values
into less extreme stabilities, while it also estimates higher uncertainties
for stability values further away from ∆∆G = 0. However, the scalings
vary between diﬀerent proteins, as can be seen from the transformations
for each of the 15 proteins presented in Supplementary Figure S9.

Figure 4: a) Correlation and b) root mean square error of predictions made
by models with diﬀerent number of experimental training samples for T4
Lysozyme (2LZM). The results of Rosetta, mCSM and PoPMuSiC are in-
variant to training data (because mCSM and PoPMuSiC are pre-trained),
and are thus constant lines. For both ﬁgures, an average of 100 randomly
selected training sets is taken at each point.

3.5 Eﬀect of data fusion and multiple substitution

matrices

In the beginning of the learning curves, when only little training data is
available, mGPfusion quite consistently outperforms the mGP model,
demonstrating that the additional simulated data improves the predic-
tion accuracy. However, when more training data becomes available,
the performance of mGP model is almost as good or sometimes even
better than the performance of the mGPfusion model. This shows that
if enough training data is available, it is not necessary to simulate addi-
tional data in order to obtain accurate predictions. Table 2 also shows,
that the data fusion can compensate the lack of relevant training data
– with the mGPfusion models that utilise the additional data, the de-
crease in accuracy is smaller when position level cross-validation is used
instead of mutation level cross-validation, than with the mGP models.
The varying weights for the base kernels between diﬀerent pro-
teins (shown in Figure 2) already illustrated that diﬀerent proteins
beneﬁt from diﬀerent similarity measures for amino acid substitutions.
The learning curves also support this observation – with some of the

4 Conclusions

We present a novel method mGPfusion for predicting stability eﬀects of
both single and multiple simultaneous mutations. mGPfusion utilises
structural information in form of contact maps and integrates that with
amino acid residues and combines both experimental and comprehen-
In con-
sive simulated measurements of mutations’ stability eﬀects.
trast to earlier general-purpose stability models, mGPfusion model is
protein-speciﬁc by design, which improves the accuracy but necessitates
having a set of experimental measurements from the protein. In prac-
tise small datasets of 10–20 experimental observations were found to
provide state-of-the-art accuracy models when supplemented by large
simulation datasets.

An important advantage over most state-of-the-art machine learn-
ing methods is that mGPfusion is able to predict the eﬀects of multiple
simultaneous mutations in addition to single point mutations. Our
experiments show that mGPfusion is reliable in predicting up to six
simultaneous mutations in our dataset. Furthermore, the Gaussian
process framework provide a way to estimate the (un)certainty of the
predictions even without a separate test set. We additionally proposed
a novel Bayesian scaling method to re-calibrate simulated protein sta-
bility values against experimental observations. This is a crucial part of
the mGPfusion model, and also alone improved protein-speciﬁc Rosetta
stability predictions by calibrating them using experimental data.

mGPfusion is best suited for a situation, where a protein is thor-
oughly experimented on and accurate predictions for stability eﬀects

7

upon mutations are needed. It takes some time to set up the frame-
work and train the model, but after that new predictions can be made
in fractions of a second. The most time-consuming part is running the
simulations with Rosetta, at least when the most accurate protocol 16
is used. Simulating all 19 possible point mutations for one position
took about 12 hours, but simulations for diﬀerent positions can be run
on parallel. The time needed for training the prediction model depends
on the amount of experimental and simulated training data. With no
simulated data, the training time ranged from few seconds to few min-
utes. With data fusion and a single kernel, the training time was under
an hour. With data fusion and MKL with 21 kernels, the training time
was from a few minutes to a day.

Acknowledgements

We acknowledge the computational resources provided by the Aalto
Science-IT.

Funding

This work has been supported by the Academy of Finland Center of Ex-
cellence in Systems Immunology and Physiology, the Academy of Fin-
land grants no. 260403 and 299915, and the Finnish Funding Agency
for Innovation Tekes (grant no 40128/14, Living Factories).
References

Alberts, B., Johnson, A., Lewis, J., Raﬀ, M., Roberts, K., and Walter, P. (2007). Molec-

ular biology of the cell. Garland Science, 5 edition.

Anslyn, E. V. and Dougherty, D. A. (2006). Modern physical organic chemistry. Uni-

versity Science Books.

Berman, H. M., Westbrook, J., Feng, Z., Gilliland, G., Bhat, T., Weissig, H., Shindyalov,
I. N., and Bourne, P. E. (2000). The protein data bank. Nucleic acids research, 28(1),
235–242.

Bommarius, A. S., Blum, J. K., and Abrahamson, M. J. (2011). Status of protein en-
gineering for biocatalysts: how to design an industrially useful biocatalyst. Current
opinion in chemical biology, 15(2), 194–200.

Branden, C. and Tooze, J. (1999). Introduction to protein structure. Garland, 2 edition.

Giollo, M., Martin, A. J., Walsh, I., Ferrari, C., and Tosatto, S. C. (2014). NeEMO: a
method using residue interaction networks to improve prediction of protein stability
upon mutation. BMC genomics, 15(4), 1.

Henikoﬀ, S. and Henikoﬀ, J. G. (1992). Amino acid substitution matrices from protein
blocks. Proceedings of the National Academy of Sciences, 89(22), 10915–10919.

Kawashima, S., Pokarowski, P., Pokarowska, M., Kolinski, A., Katayama, T., and Kane-
hisa, M. (2008). AAindex: amino acid index database, progress report 2008. Nucleic
acids research, 36(suppl 1), D202–D205.

Kellogg, E. H., Leaver-Fay, A., and Baker, D. (2011). Role of conformational sampling
in computing mutation-induced changes in protein structure and stability. Proteins:
Structure, Function, and Bioinformatics, 79(3), 830–838.

Kirk, O., Borchert, T. V., and Fuglsang, C. C. (2002). Industrial enzyme applications.

Current opinion in biotechnology, 13(4), 345–351.

Kumar, M. S., Bava, K. A., Gromiha, M. M., Prabakaran, P., Kitajima, K., Uedaira,
H., and Sarai, A. (2006). ProTherm and ProNIT: thermodynamic databases for pro-
teins and protein–nucleic acid interactions. Nucleic Acids Research, 34(suppl 1),
D204–D206.

Leaver-Fay, A., Tyka, M., Lewis, S. M., Lange, O. F., Thompson, J., Jacak, R., Kaufman,
K., Renfrew, P. D., Smith, C. A., Sheﬄer, W., et al. (2011). ROSETTA3: an object-
oriented software suite for the simulation and design of macromolecules. Methods in
enzymology, 487, 545.

Liu, J. and Kang, X. (2012). Grading amino acid properties increased accuracies of single

point mutation on protein stability prediction. BMC bioinformatics, 13(1), 1.

Menchetti, S., Costa, F., and Frasconi, P. (2005). Weighted decomposition kernels. In
Proceedings of the 22nd international conference on Machine learning, pages 585–
592. ACM.

Pace, C. N. and Scholtz, J. M. (1997). Measuring the conformational stability of a pro-

tein. Protein structure: A practical approach, 2, 299–321.

Pace, C. N. and Shaw, K. L. (2000). Linear extrapolation method of analyzing solvent
denaturation curves. Proteins: Structure, Function, and Bioinformatics, 41(S4),
1–7.

Pires, D. E., Ascher, D. B., and Blundell, T. L. (2014a). DUET: a server for predicting
eﬀects of mutations on protein stability using an integrated computational approach.
Nucleic acids research, page gku411.

Pires, D. E., Ascher, D. B., and Blundell, T. L. (2014b). mCSM: predicting the eﬀects of
mutations in proteins using graph-based signatures. Bioinformatics, 30(3), 335–342.

Potapov, V., Cohen, M., and Schreiber, G. (2009). Assessing computational methods for
predicting protein stability upon mutation: good on average but not in the details.
Protein Engineering Design and Selection, 22(9), 553–560.

Capriotti, E., Fariselli, P., and Casadio, R. (2005a). I-Mutant2.0: predicting stability
changes upon mutation from the protein sequence or structure. Nucleic acids research,
33(suppl 2), W306–W310.

Rapley, R. and Walker, J. M. (2000). Molecular Biology and Biotechnology. Royal

Society of Chemistry, 4 edition.

Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian processes for machine learn-

Capriotti, E., Fariselli, P., Calabrese, R., and Casadio, R. (2005b). Predicting pro-
tein stability changes from sequences using support vector machines. Bioinformatics,
21(suppl 2), ii54–ii58.

ing. The MIT Press.

Capriotti, E., Fariselli, P., Rossi, I., and Casadio, R. (2008). A three-state prediction of

single point mutations on protein stability changes. BMC bioinformatics, 9(2).

Chen, C.-W., Lin, J., and Chu, Y.-W. (2013). iStable: oﬀ-the-shelf predictor integration

for predicting protein stability changes. BMC bioinformatics, 14(2).

Cheng, J., Randall, A., and Baldi, P. (2006). Prediction of protein stability changes for
single-site mutations using support vector machines. Proteins: Structure, Function,
and Bioinformatics, 62(4), 1125–1132.

Cherry, J. R. and Fidantsef, A. L. (2003). Directed evolution of industrial enzymes: an

update. Current opinion in biotechnology, 14(4), 438–443.

Cichonska, A., Ravikumar, B., Parri, E., Timonen, S., Pahikkala, T., Airola, A., Wenner-
berg, K., Rousu, J., and Aittokallio, T. (2017). Computational-experimental approach
to drug-target interaction mapping: A case study on kinase inhibitors. PLoS compu-
tational biology, 13(8), e1005678.

Sanchez, S. and Demain, A. L. (2010). Enzymes and bioconversions of industrial, pharma-
ceutical, and biotechnological signiﬁcance. Organic Process Research & Development,
15(1), 224–230.

Schmidt, M. W., Berg, E., Friedlander, M. P., and Murphy, K. P. (2009). Optimizing
costly functions with simple constraints: A limited-memory projected quasi-newton
algorithm. In International Conference on Artiﬁcial Intelligence and Statistics, page
None.

Shawe-Taylor, J. and Cristianini, N. (2004). Kernel methods for pattern analysis. Cam-

bridge university press.

Tian, J., Wu, N., Chu, X., and Fan, Y. (2010). Predicting changes in protein thermosta-
bility brought about by single- or multi-site mutations. BMC bioinformatics, 11(1),
1.

Tokuriki, N. and Tawﬁk, D. S. (2009). Stability eﬀects of mutations and protein evolv-

ability. Current opinion in structural biology, 19(5), 596–604.

Dehouck, Y., Grosﬁls, A., Folch, B., Gilis, D., Bogaerts, P., and Rooman, M. (2009). Fast
and accurate predictions of protein stability changes upon mutations using statistical
potentials and neural networks: PoPMuSiC-2.0. Bioinformatics, 25(19), 2537–2543.

Tomii, K. and Kanehisa, M. (1996). Analysis of amino acid indices and mutation matrices
for sequence comparison and structure prediction of proteins. Protein Engineering,
Design and Selection, 9(1), 27–36.

Folkman, L., Stantic, B., and Sattar, A. (2014). Feature-based multiple models improve
classiﬁcation of mutation-induced stability changes. BMC genomics, 15(Suppl 4).

Vishwanathan, S. V. N., Schraudolph, N. N., Kondor, R., and Borgwardt, K. M. (2010).

Graph kernels. The Journal of Machine Learning Research, 11, 1201–1242.

Giguere, S., Marchand, M., Laviolette, F., Drouin, A., and Corbeil, J. (2013). Learn-
ing a peptide-protein binding aﬃnity predictor with kernel ridge regression. BMC
bioinformatics, 14(1), 82.

Wainreb, G., Wolf, L., Ashkenazy, H., Dehouck, Y., and Ben-Tal, N. (2011). Protein
stability: a single recorded mutation aids in predicting the eﬀects of other mutations
in the same amino acid site. Bioinformatics, 27(23), 3286–3292.

8

Supplementary material

Figure S5: The stability of a protein is determined by the thermodynamic and kinetic stabilities, ∆Gt and ∆Gk, respectively. We only consider the
thermodynamic stability.

Figure S6: Priors presented by Equation 2. Here µd = −1.5, the most likely value for −a. Other hyperparameter values are presented in Table S4.

Table S4: Values for the hyperparameters used in the priors of a, b, c, d, σE and σS presented in Equations 2 and 6, respectively.

a

b

c

d

σE

σS

αa = 2
βa = 1.5

αb = 1.3
βb = 2

αc = 2
βc = 5

µd = −a
σd = 0.15

αE = 2.5
βE = 0.02

αS = 50
βS = 0.007

9

The partial derivatives of the marginal likelihood with respect to the parameters φ are obtained from Equation (9) as follows:

where α = K −1

φ y, Kφ is determined as

and the partial derivatives of Kφ with respect to the optimised parameters are

∂
∂φj

log p(y|X, φ) =

yT K −1

∂Kφ
∂φj

K −1

φ y −

(cid:19)

K −1
φ

∂Kφ
∂φj

1
2
1
2

φ
(cid:18)(cid:16)

=

tr

ααT − K −1

φ

(cid:18)

(cid:19)

,

tr

1
2
(cid:17) ∂Kφ
∂θj

Kφ =

wmK γm

m + diag





σ0
σE1NE
σE1NE + σS1NS + tσT


2



M
(cid:88)

m=1

∂Kφ
∂σE

∂Kφ
∂σR

∂Kφ
∂t

∂Kφ
∂wm
∂Kφ
∂γm







0
2σE1NE
2 (σE1NE + σS1NS + tσT )
0
0NE
2 (σE1NE + σS1NS + tσT )
0
0NE
2 (σE1NE + σS1NS + t) σT













=diag



=diag



=diag



=K γm
m

=wmK γm

m log Km

(cid:80)N∗

i=1(yi − ¯y)(µ(xi) − ¯µ)

i=1(yi − ¯y)2 (cid:80)N∗

i=1(µ(xi) − ¯µ)2

ρ =

(cid:113)(cid:80)N∗
(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
N∗

N∗(cid:88)

i=1

rmse =

(yi − µ(xi))2,

Correlation ρ and root-mean-square error rmse for the predictions are determined as

(S11)

(S12)

(S13)

(S14)

(S15)

(S16)

(S17)

(S18)

where ¯y is the mean of the experimentally measured values, µ(xi) is prediction mean, ¯µ is the average of all prediction means, and N∗ is the
number of predictions.

10

Figure S7: Mutation-level predictions for all 15 proteins presented in Table 1. The predictions are coloured by the number of simultaneous mutations.

11

Figure S8: Position-level predictions for all 15 proteins. When the eﬀects of a mutant are predicted multiple times, they are connected by a line.

12

Table S5: (Continues on the next page)Comparison of diﬀerent methods on the 15 protein dataset with respect to ρ and rmse. Oﬀ-the-shelf implementations
of Rosetta, mCSM and PoPMuSiC are used directly without cross-validation.

Protein Method

2LZM

1BNI

1VQB

1LZ1

2CI2

1PGA

2RN2

1CSP

mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC

0.75
0.73
0.75
0.71
0.57

0.58
0.59
-
-
-

0.67
0.65
0.79
0.79
0.47

0.62
0.62
-
-
-

0.77
0.74
0.81
0.61
0.59

0.49
0.50
-
-
-

Point mutations
cross-validation level
prot.
pos.
mut.
0.75
0.80
0.87
0.75
0.77
0.86
-
0.59
0.86
-
0.37
0.75
0.73
-
0.74
0.75
0.57
0.71
0.64
0.61
0.65
0.48
0.58
0.62
0.60
0.66
0.50
0.53
0.12
0.29
0.46
0.49
0.53
0.51
0.59
0.56
0.39
-0.31
0.55
0.59
0.67
0.64
0.72
0.67
0.61
0.74
0.60
0.63
0.74
0.75
0.47
0.59
0.61
-0.46
0.59
0.69
-0.10
0.28
0.58
0.59
0.12
0.09
0.64
0.70
0.71
0.71
0.23
0.22
-0.06
-0.37
0.20
0.33
0.42
0.48

0.33
0.34
-
-
-

0.85
0.86
0.88
0.87
0.23

0.71
0.71
-
-
-

0.69
0.71
-
-
-

0.79
0.82
0.77
0.83
0.66

0.68
0.82
0.62
0.57
0.69

0.64
0.63
-
-
-

0.73
0.69
0.65
0.51
0.60

0.39
0.40
-
-
-

0.75
0.73
-
-
-

0.86
0.84
0.86
0.87
0.17

0.93
0.91
0.96
0.97
0.71

-1.00
-1.00
-
-
-

1.00
1.00
1.00
1.00
-1.00

Correlation ρ
Multiple mutations
cross-validation level
prot.
pos.
mut.
0.64
0.83
0.96
0.64
0.87
0.96
-
0.85
0.97
-
0.77
0.94
0.66
-
0.68
0.68
-
-
0.70
0.79
0.82
0.85
0.29
0.18
-
-
0.83
0.82
0.70
0.75
0.68
0.73
-
-
0.11
0.05
0.56
0.42
-1.00
-1.00
-
-
0.87
0.86
0.79
0.71
0.61
0.62
-
-
0.35
0.60
-0.24
-0.08
0.07
0.03
-
-
0.60
0.60
0.42
0.42
0.50
0.47
-
-
0.73
0.69
0.75
0.71
0.69
0.68
-
-

0.92
0.91
0.94
0.92
0.68

0.78
0.76
0.75
0.77
0.48

0.90
0.76
0.93
0.92
0.09

0.48
0.59
-
-
-

0.61
0.62
-
-
-

0.32
0.62
-
-
-

0.95
0.92
0.92
0.92
0.61

0.85
0.86
-
-
-

0.77
0.76
0.78
0.74
0.53

0.77
0.74
0.80
0.63
0.53

0.76
0.75
0.85
0.85
0.57

0.52
0.53
-
-
-

0.57
0.57
-
-
-

0.60
0.58
-
-
-

All mutations
cross-validation level
prot.
pos.
mut.
0.68
0.76
0.90
0.69
0.82
0.90
-
0.72
0.90
-
0.61
0.82
0.65
-
0.70
0.71
-
-
0.55
0.60
0.63
0.41
0.45
0.56
-
-
0.69
0.69
0.50
0.55
0.59
0.59
-
-
0.57
0.56
0.47
0.21
0.46
0.55
-
-
0.81
0.79
0.72
0.63
0.63
0.65
-
-
0.43
0.53
-0.14
0.06
0.11
0.28
-
-
0.53
0.53
0.22
0.23
0.57
0.65
-
-
0.75
0.72
0.77
0.72
0.64
0.60
-
-

0.38
0.54
-
-
-

0.70
0.70
-
-
-

0.50
0.70
-
-
-

0.92
0.91
0.94
0.92
0.59

0.79
0.81
0.77
0.82
0.62

0.85
0.81
0.84
0.81
0.24

0.71
0.72
-
-
-

0.82
0.79
0.76
0.68
0.63

0.83
0.86
0.81
0.91
0.99

1.06
1.06
-
-
-

1.69
1.69
-
-
-

1.21
1.27
1.08
1.61
1.58

1.71
1.75
1.41
1.50
1.99

2.25
2.25
-
-
-

mut.
0.82
0.84
0.82
1.12
1.05

Point mutations
cross-validation level
prot.
pos.
1.12
1.02
1.12
1.08
-
1.34
-
1.93
1.06
-
1.13
1.27
1.11
1.37
1.45
1.40
2.32
1.58
1.70
1.62
1.53
1.94
1.94
2.41
2.89
2.00
2.26
2.24
2.29
0.99
1.03
1.15
1.43
1.01
1.04
0.97
0.95
0.90
0.97
1.02
1.39
1.00
1.09
0.86
0.85
1.54
1.22
1.58
1.73
1.42
1.70
1.94
1.89
1.21
1.22
1.45
1.45
1.13
1.07
1.04
1.16
1.04
1.04
1.07
1.12
1.06
1.11
1.02
0.99

0.64
0.65
0.54
0.60
1.04

1.10
1.10
-
-
-

0.91
0.87
0.93
0.82
1.09

1.05
1.59
-
-
-

1.26
0.87
1.40
1.53
1.21

1.64
1.06
-
-
-

0.85
0.91
0.95
1.16
1.00

1.07
1.07
-
-
-

0.56
1.10
0.13
0.27
3.23

1.28
1.24
1.32
1.15
2.51

1.15
1.35
0.70
0.63
1.90

3.75
3.51
-
-
-

2.17
2.14
-
-
-

2.06
2.43
-
-
-

mut.
0.57
0.59
0.48
0.78
1.65

rmse
Multiple mutations
cross-validation level
prot.
pos.
2.14
1.53
2.11
1.16
-
1.24
-
1.44
1.84
-
1.93
-
-
2.49
2.03
2.00
1.67
2.90
2.33
-
-
1.62
1.82
1.97
2.30
1.96
2.06
-
-
2.40
2.41
1.65
2.36
3.25
3.41
-
-
0.80
1.01
1.01
1.40
1.27
1.30
-
-
2.09
0.95
3.01
2.07
3.09
3.51
-
-
1.01
0.95
1.21
1.20
1.08
1.25
-
-
1.66
1.87
1.51
1.59
2.29
1.92
-
-

2.58
2.13
-
-
-

0.91
1.18
-
-
-

2.74
0.88
-
-
-

1.21
1.12
-
-
-

0.91
0.96
0.76
0.86
2.19

0.67
0.69
0.74
0.68
1.20

0.53
0.69
0.45
0.47
2.81

0.55
0.71
0.66
0.71
1.27

All mutations
cross-validation level
prot.
1.43
1.42
-
-
-

mut.
0.76
0.79
0.75
1.05
1.23

pos.
1.30
1.13
1.29
1.70
1.51

1.22
1.27
1.11
1.57
1.70

1.59
1.66
1.27
1.33
1.97

0.83
0.87
0.80
0.90
1.07

0.80
0.87
0.90
1.08
1.06

0.88
0.83
0.94
1.02
2.33

0.86
0.83
0.89
0.80
1.12

0.75
0.78
0.63
0.71
1.58

1.37
-
-
1.67
1.60
1.55
2.20
1.94

1.77
-
-
1.82
1.89
2.24
2.66
1.99

2.21
-
-
1.07
1.10
1.17
1.47
1.15

1.12
-
-
0.86
0.99
1.02
1.39
1.11

1.13
-
-
2.00
1.13
2.81
2.01
2.87

2.95
-
-
1.14
1.13
1.36
1.36
1.11

1.11
-
-
1.45
1.60
1.36
1.43
1.92

1.47
-
-

1.75
1.75
-
-
-

2.20
2.30
-
-
-

1.16
1.14
-
-
-

1.10
1.08
-
-
-

2.38
1.02
-
-
-

1.02
1.51
-
-
-

1.80
1.57
-
-
-

13

Table S5: (Continued) Comparison of diﬀerent methods on the 15 protein dataset with respect to ρ and rmse. Oﬀ-the-shelf implementations of Rosetta,
mCSM and PoPMuSiC are used directly without cross-validation.

Protein Method

1BVC

4LYZ

1RTB

1PIN∗

1RN1

1RGG

1BPI

total

mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC

0.34
0.34
-
-
-

0.61
0.59
0.65
0.62
0.29

0.64
0.59
0.61
0.23
0.50

0.70
0.70
-
-
-

0.92
0.89
0.92
0.91
0.65

0.53
0.53
-
-
-

Point mutations
cross-validation level
prot.
pos.
mut.
0.48
0.43
0.41
0.48
0.48
0.50
-
-0.12
-0.05
-
0.06
-0.05
-
0.40
0.42
0.47
0.47
0.60
0.27
0.30
-0.05
0.27
0.28
0.33
0.55
0.59
0.81
0.79
0.69
0.58
0.61
0.69
0.68
0.72
0.49
0.51
0.40
0.25
0.49
0.53
0.72
0.60
0.62
0.58
0.04
0.27
0.57
0.67
0.76
0.62
0.51
0.52
-0.61
-0.53
0.43
0.60
0.77
0.65
0.67
0.65
0.57
0.58
0.52
0.51
0.71
0.72
0.70
0.69
0.51
0.34
0.63
0.55
0.61
0.64

0.67
0.67
-
-
-

0.83
0.81
0.74
0.77
0.63

0.69
0.69
0.71
0.72
0.58

0.68
0.64
0.56
0.43
0.52

0.61
0.61
-
-
-

0.51
0.51
-
-
-

0.81
0.79
0.81
0.76
0.65

0.56
0.56
-
-
-

Correlation ρ
Multiple mutations
cross-validation level
prot.
pos.
mut.
-0.63
-0.25
-0.28
-0.66
-0.25
-0.25
-
-0.21
-0.10
-
-0.21
-0.11
-
-0.57
-0.63
-0.65
-
-
0.57
0.64
0.46
0.47
0.68
0.71
-
-
0.74
0.73
0.30
0.58
0.73
0.73
-
-
-

0.78
0.73
0.78
0.48
0.73

0.95
0.96
0.96
0.97
0.70

0.75
0.67
-
-
-

0.66
0.64
-
-
-

-
-
-
-
-

-0.19
-0.01
-
-
-

0.96
0.93
-
-
-

-0.06
0.21
-
-
-

0.49
0.50
-
-
-

-
-
-
-
-

0.97
0.96
0.99
0.99
0.21

0.42
0.22
0.19
0.08
0.78

-0.39
-0.53
-0.29
-0.55
0.23

0.88
0.86
0.86
0.86
0.51

-

-

-
-

-
-
-
0.38
0.42
0.62
0.50
0.18
0.20
-
-
0.66
0.30
0.08
0.07
0.86
0.77
-
-
-0.02
0.33
0.52
0.46
-0.00
-0.00
-
-
0.61
0.64
0.52
0.55
0.39
0.40
-
-

0.65
0.64
0.68
0.64
0.33

0.86
0.86
0.76
0.85
0.67

0.64
0.59
0.61
0.23
0.50

0.35
0.35
-
-
-

0.68
0.69
-
-
-

0.53
0.53
-
-
-

All mutations
cross-validation level
prot.
pos.
mut.
0.09
-0.09
0.08
0.14
-0.07
0.14
-
-0.23
-0.13
-
-0.20
-0.14
-
-0.09
0.09
0.14
-
-
0.34
0.40
0.18
0.21
0.34
0.35
-
-
0.71
0.75
0.18
0.16
0.65
0.70
-
-
0.49
0.51
0.40
0.25
0.49
0.53
0.72
0.60
0.55
0.51
0.04
0.10
0.54
0.65
-
-
0.52
0.33
-0.05
-0.00
0.51
0.39
-
-
-0.13
0.05
0.84
0.70
-0.08
0.13
-
-
0.64
0.66
0.50
0.49
0.48
0.49
-
-

0.12
0.12
-
-
-

0.65
0.66
-
-
-

0.73
0.55
-
-
-

0.52
0.52
-
-
-

0.84
0.81
0.75
0.78
0.61

0.52
0.40
0.35
0.30
0.42

0.85
0.81
0.83
0.76
0.08

0.83
0.82
0.83
0.80
0.60

3.46
3.50
-
-
-

1.47
1.48
1.38
1.48
2.26

0.47
0.50
0.49
0.70
0.54

2.44
2.44
-
-
-

1.25
1.48
1.26
1.67
1.99

1.00
1.00
-
-
-

mut.
0.74
0.70
0.99
0.99
0.75

Point mutations
cross-validation level
prot.
pos.
1.65
0.72
1.65
0.71
-
1.00
-
0.99
-
0.76
1.67
1.00
0.85
1.65
1.64
1.78
1.84
2.22
3.61
1.43
1.45
1.73
1.85
2.62
3.37
2.08
2.45
2.33
2.22
0.53
0.54
0.58
0.76
0.54
0.99
0.59
0.70
1.17
1.24
1.75
1.78
1.18
1.20
0.97
1.14
1.70
1.68
2.24
2.01
1.76
1.58
1.36
1.54
1.28
1.32
1.43
1.73
1.52
1.80
1.26
1.31
1.26
1.30
1.54
1.95
1.38
1.63
1.40
1.37

0.85
0.92
1.06
1.00
1.12

1.18
1.18
-
-
-

1.56
1.56
-
-
-

1.41
1.42
1.38
1.53
1.57

1.27
1.32
1.43
1.68
1.52

1.07
1.11
1.04
1.26
1.35

1.83
1.82
-
-
-

1.61
1.62
-
-
-

0.69
0.57
0.36
0.31
1.22

2.35
2.17
-
-
-

mut.
1.64
1.62
1.30
1.29
1.65

rmse
Multiple mutations
cross-validation level
prot.
pos.
2.70
1.48
2.43
1.47
-
1.35
-
1.33
-
1.45
2.45
-
-
1.02
0.93
1.15
1.16
1.29
2.05
-
-
2.24
1.81
3.39
2.11
1.62
2.01
-
-
-
-
-
-
-

1.71
1.90
-
-
-

2.22
1.82
3.44
2.12
1.63

-
-
-
-
-

-
-
-
-
-

0.19
0.35
0.12
0.13
0.98

5.67
6.21
6.81
6.89
7.08

2.17
2.86
1.72
2.52
7.43

1.33
1.43
1.44
1.45
2.49

-
-
0.91
0.77
0.58
0.82
1.01
1.24
-
-
5.95
6.14
6.88
6.77
7.24
6.90
-
-
7.66
7.07
4.19
6.58
7.43
7.68
-
-
2.45
2.40
2.65
2.56
2.99
2.74
-
-

1.28
0.99
-
-
-

4.20
5.43
-
-
-

7.69
7.70
-
-
-

2.53
2.50
-
-
-

All mutations
cross-validation level
prot.
2.02
1.92
-
-
-

mut.
1.09
1.06
1.09
1.09
1.10

pos.
1.21
1.20
1.21
1.20
1.20

1.35
1.35
1.24
1.33
2.09

1.40
1.53
1.69
1.73
1.95

0.47
0.50
0.49
0.70
0.54

0.81
0.88
1.01
0.95
1.10

2.65
2.85
3.05
3.14
3.23

1.40
1.57
1.47
1.80
2.88

1.13
1.18
1.14
1.30
1.66

1.94
-
-
1.45
1.43
1.59
1.63
1.94

3.35
-
-
1.85
1.84
2.81
3.14
1.99

2.40
-
-
0.53
0.54
0.58
0.76
0.54

0.99
0.59
0.70

1.13
1.17
1.61
1.66
1.15

1.20
-
-
4.75
4.90
5.53
5.41
5.74

3.16
-
-
3.64
3.40
2.28
3.34
3.62

3.09
-
-
1.87
1.85
2.09
2.23
2.22

1.92
-
-

3.26
3.27
-
-
-

2.36
2.38
-
-
-

1.00
1.00
-
-
-

1.19
1.17
-
-
-

2.23
2.64
-
-
-

3.11
3.11
-
-
-

1.84
1.84
-
-
-

∗ dataset for 1PIN contained no multiple mutations.

14

Table S6: Comparison of diﬀerent methods on the 15 protein dataset with respect to ρ and rmse after removing 10% of predictions with largest errors.
Mutation, position, and protein are referred to as mut., pos., and prot., respectively. Oﬀ-the-shelf implementations of Rosetta, mCSM and PoPMuSiC are
used directly without cross-validation.

Results after 10 %
outlier removal

Method
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled

Point mutations
cross-validation level
prot.
pos.
0.75
0.77
0.75
0.73
-
0.57
-
0.27
-
0.78

mut.
0.87
0.86
0.89
0.86
0.80

Correlation ρ
Multiple mutations
cross-validation level
prot.
pos.
0.63
0.84
0.65
0.84
-
0.76
-
0.76
-
0.71

mut.
0.97
0.96
0.98
0.97
0.73

Oﬀ-the-shelf implementations with no cross-validation

All mutations
cross-validation level
prot.
pos.
0.71
0.81
0.72
0.80
-
0.74
-
0.66
-
0.75

mut.
0.92
0.91
0.93
0.91
0.78

Point mutations
cross-validation level
prot.
pos.
0.93
0.85
0.93
0.85
-
1.01
-
1.30
-
0.87

mut.
0.69
0.69
0.65
0.73
0.84

rmse
Multiple mutations
cross-validation level
prot.
pos.
1.90
1.55
1.85
1.43
-
1.66
-
1.48
-
1.99

mut.
0.62
0.64
0.49
0.54
1.67

All mutations
cross-validation level
prot.
1.13
1.12
-
-
-

mut.
0.67
0.67
0.62
0.69
1.00

pos.
1.15
1.10
1.23
1.37
1.31

Rosetta
mCSM
PoPMuSiC

0.75
0.71
0.73

0.67
-
-

0.73
-
-

0.94
0.89
0.86

1.85
-
-

1.11
-
-

Figure S9: rmse with diﬀerent amount of predictions, when predictions are sorted by the error. Position level cross-validation was used for mGPfusion,
mGP and Rosetta scaled.

15

Figure S10: Learning curves.

16

Figure S11: Learning curves.

17

Figure S12: Learning curves.

18

Figure S13: Transformations for all 15 proteins presented in Table 1. The red circles mark the simulated ∆∆G-values yS with respect to the experimental
measured ∆∆G-values yE. Thin black lines show possible transformations for yS, whereas the thick black line shows the selected transformation from yS
to ˜yS.

19

mGPfusion: Predicting protein stability changes with
Gaussian process kernel learning and data fusion

Emmi Jokinen1, Markus Heinonen1,2 and Harri L¨ahdesm¨aki1
1Department of Computer Science, Aalto University, 02150 Espoo, Finland
2Helsinki Institute for Information Technology, Finland

Abstract

Motivation: Proteins are commonly used by biochemical industry for numerous processes. Reﬁning these proteins’ properties via
mutations causes stability eﬀects as well. Accurate computational method to predict how mutations aﬀect protein stability are necessary
to facilitate eﬃcient protein design. However, accuracy of predictive models is ultimately constrained by the limited availability of
experimental data.
Results: We have developed mGPfusion, a novel Gaussian process (GP) method for predicting protein’s stability changes upon single
and multiple mutations. This method complements the limited experimental data with large amounts of molecular simulation data.
We introduce a Bayesian data fusion model that re-calibrates the experimental and in silico data sources and then learns a predictive
GP model from the combined data. Our protein-speciﬁc model requires experimental data only regarding the protein of interest and
performs well even with few experimental measurements. The mGPfusion models proteins by contact maps and infers the stability
eﬀects caused by mutations with a mixture of graph kernels. Our results show that mGPfusion outperforms state-of-the-art methods in
predicting protein stability on a dataset of 15 diﬀerent proteins and that incorporating molecular simulation data improves the model
learning and prediction accuracy.
Availability: Software implementation and datasets are available at github.com/emmijokinen/mgpfusion
Contact: emmi.jokinen@aalto.ﬁ

8
1
0
2
 
r
a

M
 
3
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
2
5
8
2
0
.
2
0
8
1
:
v
i
X
r
a

1 Introduction

Proteins are used in various applications by pharmaceutical, food, fuel,
and many other industries and their usage is growing steadily (Kirk
et al., 2002; Sanchez and Demain, 2010). Proteins have important ad-
vantages over chemical catalysts, as they are derived from renewable
resources, are biodegradable and are often highly selective (Cherry and
Fidantsef, 2003). Protein engineering is used to further improve the
properties of proteins, for example to enhance their catalytic activity,
modify their substrate speciﬁcity or to improve their thermostability
(Rapley and Walker, 2000). Increasing the stability is an important as-
pect of protein engineering, as the proteins used in industry should be
stable in the industrial process conditions, which often involve higher
than ambient temperature and non-aqueous solvents (Bommarius et al.,
2011). The properties of a protein are modiﬁed by introducing alter-
ations to its amino acid sequence. Mutations in general tend to be
destabilising, and if too many destabilising mutations are implemented,
the protein may not remain functional without compensatory stabilis-
ing mutations (Tokuriki and Tawﬁk, 2009).

The stability of a protein can be deﬁned as the diﬀerence in Gibbs
energy ∆G between the folded and unfolded (or native and denatu-
rated) state of the protein. More precisely, the Gibbs energy diﬀerence
determines the thermodynamic stability ∆Gt of the protein, as it does
not take into account the kinetic stability ∆Gk which determines the
energy needed for the transition between the folded and unfolded states
(Anslyn and Dougherty, 2006) (see Supplementary Figure S1). Here we
will consider only the thermodynamic stability and from now on it will
be referred to merely as stability ∆G.

The eﬀect of mutations can be deﬁned by the change they cause
to the Gibbs energy ∆G, denoted as ∆∆G (Pace and Scholtz, 1997).
To comprehend the signiﬁcance of stability changes upon mutations,
we can consider globular proteins, the most common type of enzymes,
whose polypeptide chain is folded up in a compact ball-like shape with
an irregular surface (Alberts et al., 2007). These proteins are only
marginally stable and the diﬀerence in Gibbs energy between the folded
and unfolded state is only about 5–15 kcal/mol, which is not much more
than the energy of a single hydrogen bond that is about 2–5 kcal/mol
(Branden and Tooze, 1999). Therefore, even one mutation that breaks
a hydrogen bond can prevent a protein from folding properly.

The protein stability can be measured with many techniques, in-
cluding thermal, urea and guanidinium chloride (GdmCl) denaturation

curves that are determined as the fraction of unfolded proteins at dif-
ferent temperatures or at diﬀerent concentrations of urea or GdmCl
(Pace and Shaw, 2000). Some of the experimentally measured sta-
bility changes upon mutations have been gathered in thermodynamic
databases such as Protherm (Kumar et al., 2006).

A variety of computational methods have been introduced to predict
the stability changes upon mutations more eﬀortlessly than through ex-
perimental measurements. These methods utilise physics or knowledge-
based potentials (Leaver-Fay et al., 2011), their combinations, or diﬀer-
ent machine learning methods. The machine learning methods utilise
support vector machines (SVM) (Capriotti et al., 2005b, 2008; Chen
et al., 2013; Cheng et al., 2006; Folkman et al., 2014; Liu and Kang,
2012; Pires et al., 2014a), random forests (Tian et al., 2010; Wainreb
et al., 2011), neural networks (Dehouck et al., 2009; Giollo et al., 2014),
and Gaussian processes (Pires et al., 2014b). However, it has been as-
sessed that although on average many of these methods provide good
results, they tend to fail on details (Potapov et al., 2009). In addition,
many of these methods are able to predict the stability eﬀects only for
single-point mutations.

We introduce mGPfusion (mutation Gaussian Processes with data
fusion), a method for predicting stability eﬀects of both point and mul-
tiple mutations. mGPfusion is a protein-speciﬁc model – in contrast to
earlier stability predictors that aim to estimate arbitrary protein struc-
ture or sequence stabilities – and achieves markedly higher accuracy
while utilising data only from a single protein at a time. In contrast
to earlier works that only use experimental data to train their models,
we also combine exhaustive Rosetta (Leaver-Fay et al., 2011) simulated
point mutation in silico stabilities to our training data.

A key part of mGPfusion is the automatic scaling of simulated data
to better match the experimental data distribution based on those vari-
ants that have both experimental and simulated stability values. Fur-
thermore, we estimate a variance resulting from the scaling, which
places a higher uncertainty on very destabilising simulations. Our
Gaussian process model then utilises the joint dataset with their es-
timated heteroscedastic variances and uses a mixture of graph kernels
to assess the stability eﬀects caused by changes in amino acid sequence
according to 21 substitution models. Our experiments on a novel 15
protein dataset show a state-of-the-art stability prediction performance,
which is also sustained when there is access only to a very few experi-
mental stability measurements.

1

Figure 1: Pipeline illustration for mGPfusion. a) M = 21 substitution matrices utilise diﬀerent information sources and give scores to pairwise amino
acid substitutions. b) The wild-type structures from Protein Data Bank are modelled as contact graphs. c) The graph kernel measures similarity of two
sequences by a substitution model S over all positions p and their neighbourhoods nbs(p) in the contact graph. d) Each substitution matrix is used to
create a separate covariance matrix. e) Multiple kernel learning (MKL) is used for ﬁnding the optimal combination of the base kernels. The kernel matrix
measures variant similarities. f ) Experimentally measured ∆∆G values yE are gathered from Protherm and Rosetta’s ddg monomer application is used to
simulate the stability eﬀects yS for all single point mutations. g) Bayesian scaling for the simulated values yS at the x-axis. Possible scalings are coloured
with green and the chosen scaling from yS into scaled values ˜yS is marked by black dots. The scaling is ﬁtted to a subset of experimentally measured
stabilities yE (circles). h) The stability predictive GP model is trained using experimental and simulated data through the kernel matrix.

2 Methods

Following Pires et al. (2014b) we choose a Bayesian model family of
Gaussian processes for prediction of mutation eﬀects on protein sta-
bility due to its inherent ability to handle uncertainty in a principle
way. Bayesian modelling is a natural approach for combining the ex-
perimental and simulated data distribution, while it is also suitable for
learning the underlying mixture of substitution models that governs the
mutational process.

The pipeline for mGPfusion is presented in Figure 1. The ﬁrst
part of mGPfusion consists of collection of in silico and experimental
datasets discussed in Section 2.1, the scaling of the in silico dataset in
Section 2.2 and the fusion of these two datasets in Section 2.3. The sec-
ond part consists of the Gaussian process model described in Section 2.4
with detailed description of the graph kernels in Sections 2.5-2.6 and
model inference in Section 2.7. Finally, the evaluation criteria used are
described in Section 2.8.

teins that fulﬁlled these requirements are listed in Table 1. We aver-
aged the stability values for proteins with multiple measurements and
ignored mutations to residues not present in their 3D structures. These
data sets are available at github.com/emmijokinen/mgpfusion.

Protein (organism)

PDB

mutations
point

point (sim)

all

2LZM 349
T4 Lysozyme (Enterobacteria phage T4)
182
1BNI
Barnase (Bacillus amyloliquefaciens)
124
1VQB
Gene V protein (Escherichia virus m13)
116
1LZI
Glycosyltransferase A (Homo sapiens)
98
2CI2
Chymotrypsin inhibitor 2 (Hordeum vulgare)
89
1PGA
Protein G (Streptococcus sp. gx7805)
83
2RN2
Ribonuclease H (Escheria coli)
80
1CSP
Cold shock protein B (Bacillus subtilis)
80
1BVC
Apomyoglobin (Physeter catodon)
63
4LYZ
Hen egg white lysozyme (Gallus gallus)
57
Ribonuclease A (Bos taurus)
1RTB
56
Peptidyl-prolyl cis-trans isomerase (Homo sapiens) 1PIN
53
1RN1
Ribonuclease T1 isozyme (Aspergillus oryzae)
54
1RGG
Ribonuclease (Streptomyces auerofaciens)
53
1BPI
Bovine pancreatic trypsin inhibitor (Bos taurus)

264
163
92
114
77
34
65
50
56
50
50
56
48
45
47

3116
2052
1634
2470
1235
1064
2945
1273
2907
2451
2356
2907
1957
1824
1102

2.1 Experimental and in silico data

total

1537

1211

31293

Protherm is a database of numerical thermodynamic parameters for
proteins and their mutants (Kumar et al., 2006). From Protherm we
gathered all proteins with at least 50 unique mutations whose ∆∆G
has been measured by thermal denaturation, and where a PDB code
for a 3D structure of the protein was reported. We required the pro-
teins to have at least 50 unique mutations, so that we would have a
representative test set and get suﬃciently reliable estimates of predic-
tion accuracy on individual proteins and examine how the amount of
experimental training data aﬀects the accuracy of the model. The 3D
structures are necessary for obtaining the connections between residues.
We collected the 3D structures with the reported PDB codes from the
Protein Databank, www.rcsb.org (Berman et al., 2000). The 15 pro-

Table 1: The 15 protein data from ProTherm database with counts of point
mutations, all mutations, and of simulated point mutation stability changes.

We also generate simulated data of the stability eﬀects of all possible
single mutations of the proteins. Our method can utilise any simulated
stability values. We used the “ddG monomer” application of Rosetta
3.6 (Leaver-Fay et al., 2011) using the high-resolution backrub-based
protocol 16 recommended in Kellogg et al. (2011). The predictions yS
made with Rosetta are given in Rosetta Energy Units (REU). Kellogg
et al. (2011) suggest transformation 0.57yS for converting the predic-
tions into physical units. The simulated data scaled this way is not
as accurate as the experimental data, the correlation and root mean

2

square error (rmse) with respect to the experimental data are shown
for all proteins in Table 2 and for individual proteins in Supplementary
Table S2, on rows labelled Rosetta. For this reason, we use instead a
Bayesian scaling described in the next section and diﬀerent noise models
for the experimental and simulated data, described in Section 2.3.

For each of the 15 proteins,

let xi = (xi1, . . . , xiM ) denote its
M -length variant i with positions p labelled with residues xip ∈
{A, R, N, . . . , V }. We denote the wild-type protein as x0. We collect
15 separate sets of simulated and experimental data. We denote the
NE )T with
NE experimental variants of each protein as XE = (xE
the corresponding experimental stability values yE = (yE
NE )T ∈
RNE . Similarly, we denote the NS simulated observations as XS =
(xS

1 , . . . , xE

1 , . . . , yE

NS )T and yS = (yS

NS )T ∈ RNS .

1 , . . . , xS

1 , . . . , yS

2.2 Bayesian scaling of in silico data

The described transformation from REU to physical units may not
be optimal for all proteins. We therefore applied instead a linear-
exponential scaling function to obtain scaled Rosetta simulated sta-
bilities ˜yS,

˜yS = g(yS | θj) = ajecj yS

+ bjyS + dj.

(1)

This scaling transforms the Rosetta simulations yS for each protein
j = 1, . . . , 15 to correspond better to the experimental data. The pa-
rameters θj = (aj, bj, cj, dj) deﬁne the weight aj and steepness cj of the
exponential term, while the linear term has slope bj and intercept dj.
To avoid overﬁtting, we perform Bayesian linear regression and start
by deﬁning parameter prior p(θj) = p(aj)p(bj)p(cj)p(dj) that reﬂects
our beliefs about realistic scalings having only moderate steepness:

and stabilities y = (y0, yE, ˜yS) of size RN where N = 1 + NE + NS is
the total number of simulated and experimental data points, including
the wild-type. We assume heteroscedastic additive noise models for the
three information sources

y0 = f (x0) + ε0,
(cid:1) + εE
i = f (cid:0)xE
yE
i ,
(cid:1) + εS
i = f (cid:0)xS
˜yS
i ,

i

i

(cid:1)

(cid:1)

0

ε0 ∼ N (cid:0)0, σ2
i ∼ N (cid:0)0, σ2
εE
(cid:16)
εS
i ∼ N

E

0, (σE + σS + tσT (i))2(cid:17)

,

(5)

where the observed values are noisy versions of the underlying ‘true’
stability function f (x) corrupted by zero-mean noise with data source
speciﬁc variances. We learn a Gaussian process based stability function
f (x) in the next Section.

The Equations (5) encode that the experimental data are corrupted
by a global experimental noise variance σ2
E. The simulated stabilities
are additionally corrupted by a global Rosetta simulator error variance
σ2
S, and by the value-dependent transformation variance tσ2
T (i) scaled
by parameter t. The model then encapsulates that we trust the Rosetta
data less than the experimental data. By deﬁnition, the ∆∆G of the
wild-type is zero (y0 = 0) with very small assumed error, σ0 = 10−6.
Note that σ2
T are ﬁxed by equation (4), while we infer the optimal
values for the remaining three free parameters (σE, σR, t) (See Section
2.4). The parameters σ2

S are assigned priors

E and σ2

σE ∼ Gamma(σE|αE, βE)
σS ∼ Gamma(σR|αS, βS).

(6)

The values of these hyperparameters are shown in Supplementary Ta-
ble S1.

p(aj) = Gamma(aj | αa, βa)
p(bj) = Beta(1/2 · bj | αb, βb)
p(cj) = Beta(10/3 · cj | αc, βc)
p(dj) = N (dj | µd, σ2

d).

2.4 Gaussian processes

(2)

We use a Gaussian process (GP) function f to predict the stability
f (x) ∈ R of a protein variant x. Gaussian processes are a family of
non-parametric, non-linear Bayesian models (Rasmussen and Williams,
2006). A zero-mean GP prior

The empirically selected hyperparameter values are listed in Supple-
mentary Table S1 and the priors are illustrated in Figure S2.

We compute the posterior for θj using the subset of simulated data

that have corresponding experimentally measured data:

p(θj|yE, yS) ∝

(cid:89)

N (cid:0)yE

i

| g(yS

i |θj), σ2
n

(cid:1) p(θj).

i:xi∈XE ∩XS

The product iterates over all NE∩S simulated ∆∆G’s that have a
matching experimentally observed value. The σ2
n is the scaling error
variance, which was set to σ2
n = 0.5. The parameters θ for each protein
were sampled using a random walk Metropolis-Hastings MCMC algo-
rithm (the mhsample function in Matlab) for NM C = 10000 samples
with a burn-in set to 500. The proposal distribution was selected to be
a symmetric uniform distribution such that [as+1, bs+1, cs+1, ds+1] ∼
U (as ± 0.4, bs ± 0.04, cs ± 0.04, ds ± 0.4). Given the sample of scaling
parameters (θ(s)
s=1 , we deﬁne the scaled simulated data as the aver-
age scaling over the MCMC sample, and record also the sample scaling
variance

j )NM C

˜yS
i =

g(yS

i |θ(s)
j )

1
NM C

1
NM C

NM C(cid:88)

s=1
NM C(cid:88)

(cid:16)

s=1

σ2
T (i) =

g(yS

i |θ(s)

j ) − ˜yS
i

(cid:17)2

.

See Figure 1 g) for an illustration of the scaling. We collect the scaled
simulated value and its variance from each simulated point into vectors
˜yS = (˜yS

T (NS)) ∈ RNS .

T (1), . . . , σ2

NS ) and σ2

1 , . . . , ˜yS

T = (σ2

deﬁnes a distribution over functions f (x) whose mean and covariance
are

f (x) ∼ GP (0, k(x, x(cid:48))) ,

E[f (x)] = 0
cov[f (x), f (x(cid:48))] = k(x, x(cid:48)).

For any collection of protein variants X = x1, . . . , xN , the function val-
ues follow a multivariate normal distribution f ∼ N (0, KXX ), where
f = (f (x1), . . . , f (xN ))T ∈ RN , and where KXX ∈ RN ×N with
[KXX ]ij = k(xi, xj). The key property of Gaussian processes is that
they encode functions that predict similar stability values f (x), f (x(cid:48))
for protein variants x, x(cid:48) that are similar, as encoded by the kernel
k(x, x(cid:48)). The key part of GP modelling is then to infer a kernel that
measures the mutation’s eﬀects to the stability.

Let a dataset of noisy stability values from two sources be y ∈ RN ,
the corresponding protein structures X = (xi)N
i=1, and a new protein
variant x∗ whose stability we wish to predict. A Gaussian process de-
ﬁnes a joint distribution over the observed values y of variants X, and
the unknown function value f (x∗) of the unseen variant x∗,
(cid:21)(cid:19)

(cid:18)

(cid:21)

(cid:20) y
f (x∗)

∼ N

0,

(cid:20)KXX + diag(σ2)
k∗X

kX∗
k(x∗, x∗)

,

∗X ∈ RN is a kernel vector with elements k(xi, x∗) for
where kX∗ = kT
T )2)T
all i = 1, . . . , N , and where σ2 = (σ2
collects ﬁnal variances of the data points from equations (5). Here
the exponents are elementwise. The conditional distribution gives the
posterior distribution of the stability prediction as

E1T , (σE1T + σS1T + tσT

0, σ2

f (x∗)|(X, y) ∼ N (cid:0)µ(x∗), σ2(x∗)(cid:1) ,

2.3 Data fusion and noise models

where the prediction mean and variance are

For each protein j, we organise its experimental data (XE, yE) and
transformed simulated data (XS, ˜yS) along with the wild-type infor-
mation (x0, y0) into a single joint dataset of variants X = (x0, XE, XS)

(cid:0)KXX + diag(σ2)(cid:1)−1 y,

µ(x∗) = k∗X
σ2(x∗) = k(x∗, x∗) − k∗X

(cid:0)KXX + diag(σ2)(cid:1)−1 kX∗.

(3)

(4)

3

Hence, in GP regression the stability predictions µ(x∗) ± σ(x∗) will
come with uncertainty estimates.

ring amino acids and scaled them between zero and one as

2.5 Graph kernel

Next, we consider how to compute the similarity function k(x, x(cid:48)) be-
tween two variants of the same protein structure. We will encode the
3D structural information of the two protein variants as a contact map
and measure their similarity by the formalism of graph kernels (Vish-
wanathan et al., 2010).

We consider two residues to be in contact if their closest atoms are
within 5 ˚A of each other in the PDB structure, which is illustrated in
Figure 1 b). All variants of the same protein have the same length, with
only diﬀerent residues at mutating positions. Furthermore, we assume
that all variants share the wild-type protein contact map.

To compare protein variants, we construct a weighted decomposition
kernel (WDK) (Menchetti et al., 2005) between two protein variants
x = (x1, . . . , xM ) and x(cid:48) = (x(cid:48)

M ) of length M ,

1, . . . , x(cid:48)

k(x, x(cid:48)) =


S(xp, x(cid:48)
p)

M
(cid:88)

p=1



(cid:88)

S(xl, x(cid:48)
l)

,

l∈nbs(p)

(7)

where nbs(p) deﬁnes the set of neighbouring positions to position p,
and S is a substitution matrix. The kernel iterates over all positions
p and compares for each of them their residues through a substitution
matrix S(xp, x(cid:48)
p). Furthermore, the similarity of the residues at each
position is multiplied by the average similarity of the residues at its
neighbouring positions S(xl, x(cid:48)
l). Hence, the kernel deﬁnes the simi-
larity of two protein variants as the average position and neighbour-
hood similarity over all positions. The kernel matrix is normalised
so that for two data points, the normalised kernel
p) =
k(xp, x(cid:48)
p, x(cid:48)
p), as deﬁned by Shawe-Taylor and Cris-
tianini (2004). The kernel is illustrated in Figure 1 c).

k(xp, xp)k(x(cid:48)

is ˆk(xp, x(cid:48)

p)/

(cid:113)

The above WDK kernel allows us to compare the eﬀects of multiple
simultaneous mutations. However, as the wild type protein structure
is used for all of the protein variants, changes that the mutations may
cause to the protein structure are not taken into consideration. This
may cause problems if mutations that alter the protein structure sig-
niﬁcantly are introduced – especially if many of them are introduced
simultaneously. On the other hand, substitution matrices that have
their basis in sequence comparisons, should take these eﬀects into ac-
count to some extend as these kinds of mutations are usually highly
destabilising and do not occur often in nature. In the next section, we
will discuss how we utilise diﬀerent substitution matrices with multiple
kernel learning.

The BLOSUM substitution models have been a common choice for
protein models (Giguere et al., 2013), while mixtures of substitution
models were proposed by Cichonska et al. (2017). BLOSUM matrices
score amino acid substitutions by their appearances throughout evolu-
tion, as they compare the frequencies of diﬀerent mutations in similar
blocks of sequences (Henikoﬀ and Henikoﬀ, 1992). However, there are
also diﬀerent ways to score amino acids substitutions, such as chemical
similarity and neighbourhood selectivity (Tomii and Kanehisa, 1996).
When the stability eﬀects of mutations are evaluated, the frequency of
an amino acid substitution in nature may not be the most important
factor.

To take into account diﬀerent measures of similarity between amino
acids, we employed a set of 21 amino acid substitution matrices gath-
ered from AAindex21 (Kawashima et al., 2008). AAindex2 currently
contains 94 substitution matrices. From these we selected those that
had no gaps concerning substitutions between the 20 naturally occur-

1http://www.genome.jp/aaindex/
2http://www.cs.ubc.ca/~schmidtm/Software/minConf.html

4

S =

S0 − min(S0) + 1
max(S0) − min(S0) + 1

.

Out of these matrices, we only chose those 23 matrices that were pos-
itive semideﬁnite. Furthermore, there were two pairs of matrices that
were extremely similar, and we only selected one matrix from each pair,
ending up with 21 substitution matrices. These substitution matrices
are used together with Equation 7 for computing 21 base kernel matri-
ces. Finally, MKL is used to ﬁnd an optimal combination of the base
kernels of form

(8)

(9)

Kφ =

wmK (γm)
m ,

21
(cid:88)

m=1

where wm is a kernel speciﬁc weight, γm is an (elementwise) exponent.
The elementwise exponent retains the SDP property of Kφ (Shawe-
Taylor and Cristianini, 2004). We observe empirically that the optimal
kernel weights wm tend to be sparse (See Figure 2).

The selected substitution matrices are listed in Figure 2. These ma-
trices have diﬀerent basis and through multiple kernel learning (MKL)
our model learns which of these are important for inferring the stability
eﬀects that mutations cause on diﬀerent proteins. The ﬁgure illustrates
this by showing the average weights of the base kernel matrices obtained
via the multiple kernel learning.

2.7 Parameter inference

The complete model has ﬁve parameters φ = (σE, σS, t, w, γ) to in-
fer, of which the variance parameters (σE, σS, t) parameterise the joint
data variance σ2
φ, while the MKL parameters w = (w1, . . . , w21) and
γ = (γ1, . . . , γ21) parameterise the kernel matrix Kφ. In a Gaussian
process model these can be jointly optimised by the marginal (log)
likelihood with priors

log p(y|φ)p(σE)p(σR) = log

p(y|f , φ)p(f |φ)p(σE)p(σR)df

(cid:90)

∝ −

yT (Kφ + diag(σ2

1
2
+ log Gamma(σE|αE, βE) + log Gamma(σS|αS, βS),

log |Kφ + diag(σ2

φ))−1y −

φ)|

1
2

(10)

which automatically balances model ﬁt (the square term) and the
model complexity (the determinant) to avoid overﬁtting (Rasmussen
and Williams, 2006). The parameters can be optimised by maximis-
ing the marginal log likelihood (10) using gradient ascent, since the
marginal likelihood can be diﬀerentiated analytically (see Supplemen-
tary Equations S1 and S2). We utilised a limited-memory projected
quasi-Newton algorithm (minConf TMP2), described by (Schmidt et al.,
2009).

We chose to evaluate the accuracy of our predictions using the same
metrics that have been used by many others – correlation ρ between the
predicted and experimentally measured ∆∆G values (Capriotti et al.,
2005a; Dehouck et al., 2009; Kellogg et al., 2011; Pires et al., 2014b;
Potapov et al., 2009) and the root mean square error (rmse) (Dehouck
et al., 2009; Pires et al., 2014a,b), which are determined in the Supple-
mentary Equations S3 and S4. We use marginal likelihood maximisa-
tion to infer model parameters and perform cross-validation to evaluate
the model performance on test data. Below we only report evaluation
metrics obtained from the test sets not used at any stage of the model
learning or data transformation sampling.

3 Results

In this section we evaluate the performance of mGPfusion on predict-
ing stability eﬀects of mutations, and compare it to the state-of-the-
art prediction methods mCSM, PoPMuSiC and Rosetta. Rosetta is a

2.6 Substitution matrices and multiple kernel

2.8 Evaluation criteria

learning

Figure 2: Average weights for kernels utilising the described substitution matrices from AAindex2, when GP models were trained with mutation level
cross-validation. Basis for the substitution matrices are obtained from (Tomii and Kanehisa, 1996). ∗ were added to AAindex2 in a later release, and their
basis were not determined by Tomii and Kanehisa (1996).

molecular modelling software whose ddg monomer module can directly
simulate the stability changes ∆∆G of a protein upon mutations. PoP-
MuSic and mCSM are machine learning models that predict stability
based on protein variant features. We run Rosetta locally, and use
mCSM and PoPMuSiC models through their web servers3. This may
give these methods an advantage over mGPfusion since parts of our
testing data were likely used within their training data.

We compare four diﬀerent variants of our method: mGPfusion that
uses both simulated data and MKL, “mGPfusion, only B62” that uses
simulated data but incorporates only one kernel matrix (BLOSUM62
substitution matrix), mGP model that uses MKL but does not use simu-
lated data, and “mGP, only B62” that uses only the base GP model but
does not incorporate simulated data and uses only the BLOSUM62 sub-
stitution matrix. In addition, we experiment on transforming Rosetta
predictions with the Bayesian scaling. We perform the experiments
for the 15 proteins separately using either position or mutation level
(leave-one-out) cross-validation regarding the methods mGP, mGPfu-
sion and the Bayesian scaling of Rosetta. Pires et al. (2014b) used
protein and position level cross-validation to evaluate their model. In
protein level cross-validation all mutations in a protein are either in
the test or training set exclusively. When we train our model using
protein level cross-validation, we use no experimental data and rely
only on the simulated data. Position level cross-validation is deﬁned
so that all mutations in a position are either in the test or training set
exclusively. However, datasets in Pires et al. (2014b) contained only
point mutations and therefore we had to extend the deﬁnition to also
include multiple mutations. In position level cross-validation we train
one model for each position using only the part of data that has a
wild-type residue in that position. Therefore, in position level cross-
validation we construct a test set that contains all protein variants that
have a mutation at position p and use as training set all the protein
variants that have a wild-type residue at that position. Dehouck et al.
(2009) evaluated their models by randomly selecting training and test
sets so that each mutation was exclusively in one of the sets, but both
sets could contain mutations from the same position of the same pro-

3biosig.unimelb.edu.au/mcsm and

omictools.com/popmusic-tool

tein. We call this mutation level cross-validation. When we use all
available experimental data with mutation level cross-validation, this
corresponds to leave-one-out cross-validation.

3.1 Predicting point mutations

Table 2 summarises the average prediction performance over all 15 pro-
teins for all compared methods, types of mutations and cross-validation
types. We ﬁrst compare the performances on single point mutations,
where mGPfusion and mGP achieve the highest performance with
ρ = 0.81 and rmse = 1.07 kcal/mol, and ρ = 0.81 and rmse = 1.04
kcal/mol, respectively with mutation level cross-validation. With only
one kernel utilising the BLOSUM62 matrix instead of MKL, the per-
formance decreases slightly, but the competing methods are still out-
performed, as mCSM achieves ρ = 0.64 and rmse = 1.37 kcal/mol,
PoPMuSic ρ = 0.61 and Rosetta ρ = 0.55. Applying Bayesian scaling
on Rosetta simulator improves the performance of standard Rosetta
from ρ = 0.55 to ρ = 0.65 and decreases the rmse from 1.63 kcal/mol
to 1.35 kcal/mol, which is interestingly even slightly better than the
performances of mCSCM and PoPMuSiC.

With position level cross-validation mGPfusion achieves the high-
est performance of ρ = 0.70 and rmse = 1.26 kcal/mol, likely due
to having still access to simulated variants from that position, since
they are always available to the learner. Without simulation data,
the baseline machine learning model mGP performance decreases to
ρ = 0.51 and rmse = 1.54 kcal/mol, thus demonstrating the impor-
tance of the data fusion. Cross-validation could not be performed for
the oﬀ-the-shelf methods mCSM and PoPMuSiC. Even still, mGPfusion
(trained with one or multiple kernels) outperforms competing state-of-
the-art methods and achieves markedly higher prediction performance
as quantiﬁed by both mutation and position level cross-validations.
Also mGP outperforms these methods when quantiﬁed by mutation
level cross-validation. With protein level cross-validation mGPfusion
achieves slightly better results than Rosetta.

5

Method
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled

Point mutations
cross-validation level
prot.
pos.
mut.
0.56
0.70
0.81
0.56
0.69
0.79
-
0.51
0.81
-
0.34
0.76
-
0.63
0.65

Correlation ρ
Multiple mutations
cross-validation level
prot.
pos.
mut.
0.49
0.61
0.88
0.50
0.64
0.86
-
0.52
0.86
-
0.55
0.86
-
0.39
0.51

All mutations
cross-validation level
prot.
pos.
mut.
0.52
0.64
0.83
0.52
0.66
0.82
-
0.50
0.83
-
0.49
0.80
-
0.48
0.60

Point mutations
cross-validation level
prot.
pos.
mut.
1.61
1.26
1.07
1.62
1.30
1.11
-
1.54
1.04
-
1.95
1.26
-
1.38
1.35

rmse
Multiple mutations
cross-validation level
prot.
pos.
mut.
2.53
2.45
1.33
2.50
2.40
1.43
-
2.65
1.44
-
2.56
1.45
-
2.99
2.49

All mutations
cross-validation level

mut.
1.13
1.18
1.14
1.30
1.66

pos.
1.87
1.85
2.09
2.23
2.22

prot.
1.84
1.84
-
-
-

Predictions from oﬀ-the-shelf implementations with no cross-validation

Rosetta
mCSM
PoPMuSiC

0.55
0.61
0.64

0.40
-
-

0.49
-
-

1.63
1.40
1.37

2.74
-
-

1.92
-
-

Table 2: Comparison of diﬀerent methods on the 15 protein dataset with respect to ρ and rmse. Mutation, position, and protein are referred to as mut.,
pos., and prot., respectively. Predictions from oﬀ-the-shelf implementations of Rosetta, mCSM and PoPMuSiC are used directly without cross-validation.

3.2 Predicting multiple mutations

Next, we tested stability prediction accuracies for variants containing
either single or multiple mutations. Figure 3 shows a scatter plot of
mGPfusion predictions for all 1537 single and multiple mutation vari-
ants (covering all 15 proteins) against the experimental ∆∆G values
using the mutation level (leave-one-out) cross-validation. The points
are coloured by the number of simultaneous mutations in the variants,
with 326 variants having at least 2 mutations (See Table 1). Fig-
ure 3 illustrates the mGPfusion’s overall high accuracy of ρ = 0.83
and rmse = 1.13 kcal/mol on both single and multiple mutations (See
Table 2). Scatter plots for the individual proteins can be found in Sup-
plementary Figure S3. Dehouck et al. (2009) suggested that considering
the predictive power after removal of most badly predicted stability ef-
fects of mutations may give more relevant evaluation, as some of the
experimental measurements may have been made in non-physiological
conditions or aﬀected by signiﬁcant error, associated with a poorly re-
solved structure, or indexed incorrectly in the database. They thus
reported correlation and rmse of the predictions after excluding 10 %
of the predictions with most negative impacts on the correlation co-
eﬃcient. Pires et al. (2014b) also reported their accuracy after 10 %
outlier removal. If we remove the 10% worst predicted stability eﬀects
from the combined predictions, we achieve correlation ρ of 0.92 and
rmse of 0.67 kcal/mol. We report these results for all the methods
in Supplementary Table S3 and also present the error distribution in
Supplementary Figure S5.

Figure 3: Scatter plot for the mutation level (leave-one-out) predictions made
for all 15 proteins (See Table 1). The colour indicates the number of simul-
taneous mutations.

The high accuracy is retained for variants with multiple mutations
as well (ρ = 0.88 and rmse = 1.33 kcal/mol, see Tables 2 and S2).
Table 3 lists mGPfusion’s rmse for diﬀerent number of simultaneous
mutations. The model accuracy in fact improves up to 6 mutations.
This is explained by the training set often containing the same single
point mutations that appear in variants with multiple mutations. The
model can then infer the combined eﬀect of pointwise mutations. The
model seems to fail when predicting the eﬀects of 7-9 simultaneous mu-

6

tations. Most of these mutations (8/12) are for Ribonuclease (1RGG)
and their eﬀects seem to be exceptionally diﬃcult to predict. This may
be because only few of the point mutations that are part of the multiple
mutations are present in the training data. However, these mutations
seem to be exceptionally diﬃcult to predict for Rosetta as well, which
could indicate that the experimental measurements concerning these
mutations are not quite accurate. PoPMuSiC and mCSM are unable
to predict multiple mutations, while Rosetta supports them, but its
rmse accuracy decreases already with two mutations.

mutations
occurences

mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta

1
1211

1.07
1.11
1.04
1.26
1.35
1.63

2
207

1.06
1.12
1.03
0.96
2.10
2.27

3
52

0.80
0.77
0.61
0.65
1.92
2.11

4
42

0.51
0.59
0.50
0.83
2.94
3.78

5
4

0.40
0.29
0.18
0.26
2.29
2.93

6
8

1.01
1.14
0.92
1.14
2.32
2.21

7
3

3.02
3.00
3.23
2.95
2.93
2.92

8
3

5.89
6.78
6.18
6.90
6.75
5.80

9
6

5.16
5.56
6.75
6.57
7.28
7.45

10
1

0.25
0.11
0.08
0.05
2.69
3.42

Table 3: Root-mean-square errors for diﬀerent number of simultaneous
mutations for all 15 proteins, with models trained by leave-one-out cross-
validation. Rosetta is added for comparison.

With multiple mutations, the decrease in performance between the
position and mutation level cross-validations becomes clearer than with
single mutations. With the position level cross-validation the stability
eﬀects of multiple mutations are predicted multiple times, which partly
explains this loss of accuracy. For example, the eﬀects of mutants with
nine diﬀerent simultaneous mutations, which were the most diﬃcult
cases in the mutation level cross-validation, are predicted nine times.
Surprisingly, mGPfusion trained with protein level cross-validation
achieves higher correlation and smaller errors than Rosetta; mGPfusion
utilising simulated ∆∆G values for only single mutations, can predict
the stability eﬀects of multiple mutations better than Rosetta.

3.3 Uncertainty of the predictions

Gaussian processes provide a mean µ(x) and a standard deviation σ(x)
for the stability prediction of a protein variant x. The standard de-
viation allows estimation of the prediction accuracy even without test
data. Figure 1 h) visualises the uncertainty of a few predictions made
for the protein G (1PGA) when mutation level cross-validation is used.
The estimated standard deviation allows a user to automatically iden-
tify low quality predictions that can appear e.g. in parts of the input
protein space from which less data is included in model training. Con-
versely, in order to minimise the amount of uncertainty in the mGPfu-
sion predictions, estimated standard deviation can be used to guide next
experiments. The probabilistic nature of the predictions also admits an
alternative error measure of negative log probability density (NLPD)
nlpd = − (cid:80)N
i=1 log p(yi|µ(xi), σ2(xi)), which can naturally take into
account the prediction variance.

3.4 Eﬀect of training set size

The results presented in Sections 3.1–3.3 used all available data for
training with cross-validation to obtain unbiased performance mea-
sures. The inclusion of thousands of simulated variants allows the model

to learn accurate models with less experimentally measured variants.
Hence, we study how the mGPfusion model with or without simulated
data performs with reduced number of experimental observations. To
facilitate this, we randomly selected subsets of experimental data of
size 0, 10, 20, and so on. We learned the mGP and mGPfusion models
with these reduced experimental data sets while always using the full
simulated data sets. This also allows us to estimate how the models
work with diﬀerent number of cross-validation folds. For example, the
point of a learning curve which utilises 2/3 or 4/5 of the training data
correspond to an average of multiple 3-fold or 5-fold cross-validations,
respectively.

The learning curve in Figure 4a) shows how the averaged correlation
for protein 2LZM improves when the size of the experimental data set
increases. The right-most values at N = 348 are obtained with leave-
one-out cross-validation. The inclusion of simulated data in mGPfusion
(dark blue line) consistently improves the performance of mGP, which
is trained without simulated data. Figure 4b) illustrate the diﬀerence
in root mean square error. Learning curves for all proteins listed in
Table 1 can be found from the Supplementary Figures S6-S8. When
the number of experimental samples is zero, the mGPfusion model is
trained solely using the simulated data with scaling 0.57yS, and the
mGP model predicts the stability eﬀect of every mutation as zero. The
last point on the learning curves is obtained with mutation level cross-
validation (see Tables 2 and S2).

proteins mGPfusion model trained with only one kernel that utilises
BLOSUM62, provides approximately as good results as the mGPfu-
sion model trained with multiple kernels. However, with many of the
proteins, utilising just BLOSUM62 does not seem to be suﬃcient and
the accuracy of the model can be improved by using diﬀerent substi-
tution matrices. Prior knowledge of appropriate substitution models
for each protein could enable creation of accurate prediction models
with just one substitution model, but the MKL seems to be a good
tool for selecting suitable substitution models when such knowledge is
not available. It seems that the data fusion and number or relevance
of used substitution matrices can compensate each other – the learning
curves show, that the diﬀerence between mGPfusion models trained
with one or multiple kernels is smaller than the diﬀerence between the
mGP models utilising one or multiple kernels. This indicates that if ad-
ditional simulated data is exploited, the use of multiple or appropriate
substitution models is not as important than without the data fusion.
On the other hand, if data fusion is not applied, the use of MKL can
more signiﬁcantly improve the accuracy of the mGP model.

3.6 Eﬀect of

the Bayesian transformation on

Rosetta

The Bayesian scaling of simulated Rosetta values, proposed in Sec-
tion 2.2, improves the match of Rosetta simulated values to empirical
∆∆G values even without using the Gaussian process framework. The
Bayesian scaling improves the performance of standard Rosetta sim-
ulations from ρ = 0.55 and rmse = 1.63 kcal/mol to ρ = 0.65 and
rmse = 1.35 kcal/mol (see Table 2 and Supplementary Table S2). This
shows that the scaling proposed by Kellogg et al. (2011) indeed is not
always the optimal scaling and signiﬁcant improvements can be gained
by optimising the scaling using a set of training data.

Figure 1 g) visualises the Bayesian scaling for protein 1PGA, where
the very destabilising ∆∆G values are dampened by the scaling (black
dots) to less extreme values by matching the scaled simulated values to
the experimental points (blue circles). The black dots along the scaling
curve indicate the grid of point mutations after transformation. The
scaling variance σ2
T is indicated by the green region’s vertical width,
and on the right panel. The scaling tends to dampen very small values
into less extreme stabilities, while it also estimates higher uncertainties
for stability values further away from ∆∆G = 0. However, the scalings
vary between diﬀerent proteins, as can be seen from the transformations
for each of the 15 proteins presented in Supplementary Figure S9.

Figure 4: a) Correlation and b) root mean square error of predictions made
by models with diﬀerent number of experimental training samples for T4
Lysozyme (2LZM). The results of Rosetta, mCSM and PoPMuSiC are in-
variant to training data (because mCSM and PoPMuSiC are pre-trained),
and are thus constant lines. For both ﬁgures, an average of 100 randomly
selected training sets is taken at each point.

3.5 Eﬀect of data fusion and multiple substitution

matrices

In the beginning of the learning curves, when only little training data is
available, mGPfusion quite consistently outperforms the mGP model,
demonstrating that the additional simulated data improves the predic-
tion accuracy. However, when more training data becomes available,
the performance of mGP model is almost as good or sometimes even
better than the performance of the mGPfusion model. This shows that
if enough training data is available, it is not necessary to simulate addi-
tional data in order to obtain accurate predictions. Table 2 also shows,
that the data fusion can compensate the lack of relevant training data
– with the mGPfusion models that utilise the additional data, the de-
crease in accuracy is smaller when position level cross-validation is used
instead of mutation level cross-validation, than with the mGP models.
The varying weights for the base kernels between diﬀerent pro-
teins (shown in Figure 2) already illustrated that diﬀerent proteins
beneﬁt from diﬀerent similarity measures for amino acid substitutions.
The learning curves also support this observation – with some of the

4 Conclusions

We present a novel method mGPfusion for predicting stability eﬀects of
both single and multiple simultaneous mutations. mGPfusion utilises
structural information in form of contact maps and integrates that with
amino acid residues and combines both experimental and comprehen-
In con-
sive simulated measurements of mutations’ stability eﬀects.
trast to earlier general-purpose stability models, mGPfusion model is
protein-speciﬁc by design, which improves the accuracy but necessitates
having a set of experimental measurements from the protein. In prac-
tise small datasets of 10–20 experimental observations were found to
provide state-of-the-art accuracy models when supplemented by large
simulation datasets.

An important advantage over most state-of-the-art machine learn-
ing methods is that mGPfusion is able to predict the eﬀects of multiple
simultaneous mutations in addition to single point mutations. Our
experiments show that mGPfusion is reliable in predicting up to six
simultaneous mutations in our dataset. Furthermore, the Gaussian
process framework provide a way to estimate the (un)certainty of the
predictions even without a separate test set. We additionally proposed
a novel Bayesian scaling method to re-calibrate simulated protein sta-
bility values against experimental observations. This is a crucial part of
the mGPfusion model, and also alone improved protein-speciﬁc Rosetta
stability predictions by calibrating them using experimental data.

mGPfusion is best suited for a situation, where a protein is thor-
oughly experimented on and accurate predictions for stability eﬀects

7

upon mutations are needed. It takes some time to set up the frame-
work and train the model, but after that new predictions can be made
in fractions of a second. The most time-consuming part is running the
simulations with Rosetta, at least when the most accurate protocol 16
is used. Simulating all 19 possible point mutations for one position
took about 12 hours, but simulations for diﬀerent positions can be run
on parallel. The time needed for training the prediction model depends
on the amount of experimental and simulated training data. With no
simulated data, the training time ranged from few seconds to few min-
utes. With data fusion and a single kernel, the training time was under
an hour. With data fusion and MKL with 21 kernels, the training time
was from a few minutes to a day.

Acknowledgements

We acknowledge the computational resources provided by the Aalto
Science-IT.

Funding

This work has been supported by the Academy of Finland Center of Ex-
cellence in Systems Immunology and Physiology, the Academy of Fin-
land grants no. 260403 and 299915, and the Finnish Funding Agency
for Innovation Tekes (grant no 40128/14, Living Factories).
References

Alberts, B., Johnson, A., Lewis, J., Raﬀ, M., Roberts, K., and Walter, P. (2007). Molec-

ular biology of the cell. Garland Science, 5 edition.

Anslyn, E. V. and Dougherty, D. A. (2006). Modern physical organic chemistry. Uni-

versity Science Books.

Berman, H. M., Westbrook, J., Feng, Z., Gilliland, G., Bhat, T., Weissig, H., Shindyalov,
I. N., and Bourne, P. E. (2000). The protein data bank. Nucleic acids research, 28(1),
235–242.

Bommarius, A. S., Blum, J. K., and Abrahamson, M. J. (2011). Status of protein en-
gineering for biocatalysts: how to design an industrially useful biocatalyst. Current
opinion in chemical biology, 15(2), 194–200.

Branden, C. and Tooze, J. (1999). Introduction to protein structure. Garland, 2 edition.

Giollo, M., Martin, A. J., Walsh, I., Ferrari, C., and Tosatto, S. C. (2014). NeEMO: a
method using residue interaction networks to improve prediction of protein stability
upon mutation. BMC genomics, 15(4), 1.

Henikoﬀ, S. and Henikoﬀ, J. G. (1992). Amino acid substitution matrices from protein
blocks. Proceedings of the National Academy of Sciences, 89(22), 10915–10919.

Kawashima, S., Pokarowski, P., Pokarowska, M., Kolinski, A., Katayama, T., and Kane-
hisa, M. (2008). AAindex: amino acid index database, progress report 2008. Nucleic
acids research, 36(suppl 1), D202–D205.

Kellogg, E. H., Leaver-Fay, A., and Baker, D. (2011). Role of conformational sampling
in computing mutation-induced changes in protein structure and stability. Proteins:
Structure, Function, and Bioinformatics, 79(3), 830–838.

Kirk, O., Borchert, T. V., and Fuglsang, C. C. (2002). Industrial enzyme applications.

Current opinion in biotechnology, 13(4), 345–351.

Kumar, M. S., Bava, K. A., Gromiha, M. M., Prabakaran, P., Kitajima, K., Uedaira,
H., and Sarai, A. (2006). ProTherm and ProNIT: thermodynamic databases for pro-
teins and protein–nucleic acid interactions. Nucleic Acids Research, 34(suppl 1),
D204–D206.

Leaver-Fay, A., Tyka, M., Lewis, S. M., Lange, O. F., Thompson, J., Jacak, R., Kaufman,
K., Renfrew, P. D., Smith, C. A., Sheﬄer, W., et al. (2011). ROSETTA3: an object-
oriented software suite for the simulation and design of macromolecules. Methods in
enzymology, 487, 545.

Liu, J. and Kang, X. (2012). Grading amino acid properties increased accuracies of single

point mutation on protein stability prediction. BMC bioinformatics, 13(1), 1.

Menchetti, S., Costa, F., and Frasconi, P. (2005). Weighted decomposition kernels. In
Proceedings of the 22nd international conference on Machine learning, pages 585–
592. ACM.

Pace, C. N. and Scholtz, J. M. (1997). Measuring the conformational stability of a pro-

tein. Protein structure: A practical approach, 2, 299–321.

Pace, C. N. and Shaw, K. L. (2000). Linear extrapolation method of analyzing solvent
denaturation curves. Proteins: Structure, Function, and Bioinformatics, 41(S4),
1–7.

Pires, D. E., Ascher, D. B., and Blundell, T. L. (2014a). DUET: a server for predicting
eﬀects of mutations on protein stability using an integrated computational approach.
Nucleic acids research, page gku411.

Pires, D. E., Ascher, D. B., and Blundell, T. L. (2014b). mCSM: predicting the eﬀects of
mutations in proteins using graph-based signatures. Bioinformatics, 30(3), 335–342.

Potapov, V., Cohen, M., and Schreiber, G. (2009). Assessing computational methods for
predicting protein stability upon mutation: good on average but not in the details.
Protein Engineering Design and Selection, 22(9), 553–560.

Capriotti, E., Fariselli, P., and Casadio, R. (2005a). I-Mutant2.0: predicting stability
changes upon mutation from the protein sequence or structure. Nucleic acids research,
33(suppl 2), W306–W310.

Rapley, R. and Walker, J. M. (2000). Molecular Biology and Biotechnology. Royal

Society of Chemistry, 4 edition.

Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian processes for machine learn-

Capriotti, E., Fariselli, P., Calabrese, R., and Casadio, R. (2005b). Predicting pro-
tein stability changes from sequences using support vector machines. Bioinformatics,
21(suppl 2), ii54–ii58.

ing. The MIT Press.

Capriotti, E., Fariselli, P., Rossi, I., and Casadio, R. (2008). A three-state prediction of

single point mutations on protein stability changes. BMC bioinformatics, 9(2).

Chen, C.-W., Lin, J., and Chu, Y.-W. (2013). iStable: oﬀ-the-shelf predictor integration

for predicting protein stability changes. BMC bioinformatics, 14(2).

Cheng, J., Randall, A., and Baldi, P. (2006). Prediction of protein stability changes for
single-site mutations using support vector machines. Proteins: Structure, Function,
and Bioinformatics, 62(4), 1125–1132.

Cherry, J. R. and Fidantsef, A. L. (2003). Directed evolution of industrial enzymes: an

update. Current opinion in biotechnology, 14(4), 438–443.

Cichonska, A., Ravikumar, B., Parri, E., Timonen, S., Pahikkala, T., Airola, A., Wenner-
berg, K., Rousu, J., and Aittokallio, T. (2017). Computational-experimental approach
to drug-target interaction mapping: A case study on kinase inhibitors. PLoS compu-
tational biology, 13(8), e1005678.

Sanchez, S. and Demain, A. L. (2010). Enzymes and bioconversions of industrial, pharma-
ceutical, and biotechnological signiﬁcance. Organic Process Research & Development,
15(1), 224–230.

Schmidt, M. W., Berg, E., Friedlander, M. P., and Murphy, K. P. (2009). Optimizing
costly functions with simple constraints: A limited-memory projected quasi-newton
algorithm. In International Conference on Artiﬁcial Intelligence and Statistics, page
None.

Shawe-Taylor, J. and Cristianini, N. (2004). Kernel methods for pattern analysis. Cam-

bridge university press.

Tian, J., Wu, N., Chu, X., and Fan, Y. (2010). Predicting changes in protein thermosta-
bility brought about by single- or multi-site mutations. BMC bioinformatics, 11(1),
1.

Tokuriki, N. and Tawﬁk, D. S. (2009). Stability eﬀects of mutations and protein evolv-

ability. Current opinion in structural biology, 19(5), 596–604.

Dehouck, Y., Grosﬁls, A., Folch, B., Gilis, D., Bogaerts, P., and Rooman, M. (2009). Fast
and accurate predictions of protein stability changes upon mutations using statistical
potentials and neural networks: PoPMuSiC-2.0. Bioinformatics, 25(19), 2537–2543.

Tomii, K. and Kanehisa, M. (1996). Analysis of amino acid indices and mutation matrices
for sequence comparison and structure prediction of proteins. Protein Engineering,
Design and Selection, 9(1), 27–36.

Folkman, L., Stantic, B., and Sattar, A. (2014). Feature-based multiple models improve
classiﬁcation of mutation-induced stability changes. BMC genomics, 15(Suppl 4).

Vishwanathan, S. V. N., Schraudolph, N. N., Kondor, R., and Borgwardt, K. M. (2010).

Graph kernels. The Journal of Machine Learning Research, 11, 1201–1242.

Giguere, S., Marchand, M., Laviolette, F., Drouin, A., and Corbeil, J. (2013). Learn-
ing a peptide-protein binding aﬃnity predictor with kernel ridge regression. BMC
bioinformatics, 14(1), 82.

Wainreb, G., Wolf, L., Ashkenazy, H., Dehouck, Y., and Ben-Tal, N. (2011). Protein
stability: a single recorded mutation aids in predicting the eﬀects of other mutations
in the same amino acid site. Bioinformatics, 27(23), 3286–3292.

8

Supplementary material

Figure S5: The stability of a protein is determined by the thermodynamic and kinetic stabilities, ∆Gt and ∆Gk, respectively. We only consider the
thermodynamic stability.

Figure S6: Priors presented by Equation 2. Here µd = −1.5, the most likely value for −a. Other hyperparameter values are presented in Table S4.

Table S4: Values for the hyperparameters used in the priors of a, b, c, d, σE and σS presented in Equations 2 and 6, respectively.

a

b

c

d

σE

σS

αa = 2
βa = 1.5

αb = 1.3
βb = 2

αc = 2
βc = 5

µd = −a
σd = 0.15

αE = 2.5
βE = 0.02

αS = 50
βS = 0.007

9

The partial derivatives of the marginal likelihood with respect to the parameters φ are obtained from Equation (9) as follows:

where α = K −1

φ y, Kφ is determined as

and the partial derivatives of Kφ with respect to the optimised parameters are

∂
∂φj

log p(y|X, φ) =

yT K −1

∂Kφ
∂φj

K −1

φ y −

(cid:19)

K −1
φ

∂Kφ
∂φj

1
2
1
2

φ
(cid:18)(cid:16)

=

tr

ααT − K −1

φ

(cid:18)

(cid:19)

,

tr

1
2
(cid:17) ∂Kφ
∂θj

Kφ =

wmK γm

m + diag





σ0
σE1NE
σE1NE + σS1NS + tσT


2



M
(cid:88)

m=1

∂Kφ
∂σE

∂Kφ
∂σR

∂Kφ
∂t

∂Kφ
∂wm
∂Kφ
∂γm







0
2σE1NE
2 (σE1NE + σS1NS + tσT )
0
0NE
2 (σE1NE + σS1NS + tσT )
0
0NE
2 (σE1NE + σS1NS + t) σT













=diag



=diag



=diag



=K γm
m

=wmK γm

m log Km

(cid:80)N∗

i=1(yi − ¯y)(µ(xi) − ¯µ)

i=1(yi − ¯y)2 (cid:80)N∗

i=1(µ(xi) − ¯µ)2

ρ =

(cid:113)(cid:80)N∗
(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
N∗

N∗(cid:88)

i=1

rmse =

(yi − µ(xi))2,

Correlation ρ and root-mean-square error rmse for the predictions are determined as

(S11)

(S12)

(S13)

(S14)

(S15)

(S16)

(S17)

(S18)

where ¯y is the mean of the experimentally measured values, µ(xi) is prediction mean, ¯µ is the average of all prediction means, and N∗ is the
number of predictions.

10

Figure S7: Mutation-level predictions for all 15 proteins presented in Table 1. The predictions are coloured by the number of simultaneous mutations.

11

Figure S8: Position-level predictions for all 15 proteins. When the eﬀects of a mutant are predicted multiple times, they are connected by a line.

12

Table S5: (Continues on the next page)Comparison of diﬀerent methods on the 15 protein dataset with respect to ρ and rmse. Oﬀ-the-shelf implementations
of Rosetta, mCSM and PoPMuSiC are used directly without cross-validation.

Protein Method

2LZM

1BNI

1VQB

1LZ1

2CI2

1PGA

2RN2

1CSP

mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC

0.58
0.59
-
-
-

0.75
0.73
0.75
0.71
0.57

0.49
0.50
-
-
-

0.62
0.62
-
-
-

0.67
0.65
0.79
0.79
0.47

0.77
0.74
0.81
0.61
0.59

Point mutations
cross-validation level
prot.
pos.
mut.
0.75
0.80
0.87
0.75
0.77
0.86
-
0.59
0.86
-
0.37
0.75
0.73
-
0.74
0.75
0.57
0.71
0.64
0.61
0.65
0.48
0.58
0.62
0.60
0.66
0.50
0.53
0.12
0.29
0.46
0.49
0.53
0.51
0.59
0.56
0.39
-0.31
0.55
0.59
0.67
0.64
0.72
0.67
0.61
0.74
0.60
0.63
0.74
0.75
0.47
0.59
0.61
-0.46
0.59
0.69
-0.10
0.28
0.58
0.59
0.12
0.09
0.64
0.70
0.71
0.71
0.23
0.22
-0.06
-0.37
0.20
0.33
0.42
0.48

0.85
0.86
0.88
0.87
0.23

0.33
0.34
-
-
-

0.69
0.71
-
-
-

0.71
0.71
-
-
-

0.79
0.82
0.77
0.83
0.66

0.68
0.82
0.62
0.57
0.69

0.73
0.69
0.65
0.51
0.60

0.64
0.63
-
-
-

0.75
0.73
-
-
-

0.39
0.40
-
-
-

0.86
0.84
0.86
0.87
0.17

0.93
0.91
0.96
0.97
0.71

-1.00
-1.00
-
-
-

1.00
1.00
1.00
1.00
-1.00

Correlation ρ
Multiple mutations
cross-validation level
prot.
pos.
mut.
0.64
0.83
0.96
0.64
0.87
0.96
-
0.85
0.97
-
0.77
0.94
0.66
-
0.68
0.68
-
-
0.70
0.79
0.82
0.85
0.29
0.18
-
-
0.83
0.82
0.70
0.75
0.68
0.73
-
-
0.11
0.05
0.56
0.42
-1.00
-1.00
-
-
0.87
0.86
0.79
0.71
0.61
0.62
-
-
0.35
0.60
-0.24
-0.08
0.07
0.03
-
-
0.60
0.60
0.42
0.42
0.50
0.47
-
-
0.73
0.69
0.75
0.71
0.69
0.68
-
-

0.92
0.91
0.94
0.92
0.68

0.90
0.76
0.93
0.92
0.09

0.78
0.76
0.75
0.77
0.48

0.48
0.59
-
-
-

0.61
0.62
-
-
-

0.32
0.62
-
-
-

0.95
0.92
0.92
0.92
0.61

0.85
0.86
-
-
-

0.77
0.76
0.78
0.74
0.53

0.77
0.74
0.80
0.63
0.53

0.76
0.75
0.85
0.85
0.57

0.52
0.53
-
-
-

0.57
0.57
-
-
-

0.60
0.58
-
-
-

All mutations
cross-validation level
prot.
pos.
mut.
0.68
0.76
0.90
0.69
0.82
0.90
-
0.72
0.90
-
0.61
0.82
0.65
-
0.70
0.71
-
-
0.55
0.60
0.63
0.41
0.45
0.56
-
-
0.69
0.69
0.50
0.55
0.59
0.59
-
-
0.57
0.56
0.47
0.21
0.46
0.55
-
-
0.81
0.79
0.72
0.63
0.63
0.65
-
-
0.43
0.53
-0.14
0.06
0.11
0.28
-
-
0.53
0.53
0.22
0.23
0.57
0.65
-
-
0.75
0.72
0.77
0.72
0.64
0.60
-
-

0.38
0.54
-
-
-

0.50
0.70
-
-
-

0.70
0.70
-
-
-

0.92
0.91
0.94
0.92
0.59

0.79
0.81
0.77
0.82
0.62

0.85
0.81
0.84
0.81
0.24

0.71
0.72
-
-
-

0.82
0.79
0.76
0.68
0.63

1.06
1.06
-
-
-

0.83
0.86
0.81
0.91
0.99

1.21
1.27
1.08
1.61
1.58

1.69
1.69
-
-
-

1.71
1.75
1.41
1.50
1.99

2.25
2.25
-
-
-

mut.
0.82
0.84
0.82
1.12
1.05

Point mutations
cross-validation level
prot.
pos.
1.12
1.02
1.12
1.08
-
1.34
-
1.93
1.06
-
1.13
1.27
1.11
1.37
1.45
1.40
2.32
1.58
1.70
1.62
1.53
1.94
1.94
2.41
2.89
2.00
2.26
2.24
2.29
0.99
1.03
1.15
1.43
1.01
1.04
0.97
0.95
0.90
0.97
1.02
1.39
1.00
1.09
0.86
0.85
1.54
1.22
1.58
1.73
1.42
1.70
1.94
1.89
1.21
1.22
1.45
1.45
1.13
1.07
1.04
1.16
1.04
1.04
1.07
1.12
1.06
1.11
1.02
0.99

1.10
1.10
-
-
-

0.64
0.65
0.54
0.60
1.04

1.26
0.87
1.40
1.53
1.21

1.05
1.59
-
-
-

0.91
0.87
0.93
0.82
1.09

1.64
1.06
-
-
-

1.07
1.07
-
-
-

0.85
0.91
0.95
1.16
1.00

0.56
1.10
0.13
0.27
3.23

1.15
1.35
0.70
0.63
1.90

1.28
1.24
1.32
1.15
2.51

3.75
3.51
-
-
-

2.17
2.14
-
-
-

2.06
2.43
-
-
-

mut.
0.57
0.59
0.48
0.78
1.65

rmse
Multiple mutations
cross-validation level
prot.
pos.
2.14
1.53
2.11
1.16
-
1.24
-
1.44
1.84
-
1.93
-
-
2.49
2.03
2.00
1.67
2.90
2.33
-
-
1.62
1.82
1.97
2.30
1.96
2.06
-
-
2.40
2.41
1.65
2.36
3.25
3.41
-
-
0.80
1.01
1.01
1.40
1.27
1.30
-
-
2.09
0.95
3.01
2.07
3.09
3.51
-
-
1.01
0.95
1.21
1.20
1.08
1.25
-
-
1.66
1.87
1.51
1.59
2.29
1.92
-
-

2.58
2.13
-
-
-

0.91
1.18
-
-
-

2.74
0.88
-
-
-

1.21
1.12
-
-
-

0.91
0.96
0.76
0.86
2.19

0.53
0.69
0.45
0.47
2.81

0.67
0.69
0.74
0.68
1.20

0.55
0.71
0.66
0.71
1.27

All mutations
cross-validation level
prot.
1.43
1.42
-
-
-

mut.
0.76
0.79
0.75
1.05
1.23

pos.
1.30
1.13
1.29
1.70
1.51

1.22
1.27
1.11
1.57
1.70

1.59
1.66
1.27
1.33
1.97

0.83
0.87
0.80
0.90
1.07

0.80
0.87
0.90
1.08
1.06

0.88
0.83
0.94
1.02
2.33

0.86
0.83
0.89
0.80
1.12

0.75
0.78
0.63
0.71
1.58

1.37
-
-
1.67
1.60
1.55
2.20
1.94

1.77
-
-
1.82
1.89
2.24
2.66
1.99

2.21
-
-
1.07
1.10
1.17
1.47
1.15

1.12
-
-
0.86
0.99
1.02
1.39
1.11

1.13
-
-
2.00
1.13
2.81
2.01
2.87

2.95
-
-
1.14
1.13
1.36
1.36
1.11

1.11
-
-
1.45
1.60
1.36
1.43
1.92

1.47
-
-

1.75
1.75
-
-
-

2.20
2.30
-
-
-

1.16
1.14
-
-
-

1.10
1.08
-
-
-

2.38
1.02
-
-
-

1.02
1.51
-
-
-

1.80
1.57
-
-
-

13

Table S5: (Continued) Comparison of diﬀerent methods on the 15 protein dataset with respect to ρ and rmse. Oﬀ-the-shelf implementations of Rosetta,
mCSM and PoPMuSiC are used directly without cross-validation.

Protein Method

1BVC

4LYZ

1RTB

1PIN∗

1RN1

1RGG

1BPI

total

mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC

0.34
0.34
-
-
-

0.61
0.59
0.65
0.62
0.29

0.64
0.59
0.61
0.23
0.50

0.70
0.70
-
-
-

0.92
0.89
0.92
0.91
0.65

0.53
0.53
-
-
-

Point mutations
cross-validation level
prot.
pos.
mut.
0.48
0.43
0.41
0.48
0.48
0.50
-
-0.12
-0.05
-
0.06
-0.05
-
0.40
0.42
0.47
0.47
0.60
0.27
0.30
-0.05
0.27
0.28
0.33
0.55
0.59
0.81
0.79
0.69
0.58
0.61
0.69
0.68
0.72
0.49
0.51
0.40
0.25
0.49
0.53
0.72
0.60
0.62
0.58
0.04
0.27
0.57
0.67
0.76
0.62
0.51
0.52
-0.61
-0.53
0.43
0.60
0.77
0.65
0.67
0.65
0.57
0.58
0.52
0.51
0.71
0.72
0.70
0.69
0.51
0.34
0.63
0.55
0.61
0.64

0.67
0.67
-
-
-

0.83
0.81
0.74
0.77
0.63

0.69
0.69
0.71
0.72
0.58

0.68
0.64
0.56
0.43
0.52

0.61
0.61
-
-
-

0.51
0.51
-
-
-

0.81
0.79
0.81
0.76
0.65

0.56
0.56
-
-
-

Correlation ρ
Multiple mutations
cross-validation level
prot.
pos.
mut.
-0.63
-0.25
-0.28
-0.66
-0.25
-0.25
-
-0.21
-0.10
-
-0.21
-0.11
-
-0.57
-0.63
-0.65
-
-
0.57
0.64
0.46
0.47
0.68
0.71
-
-
0.74
0.73
0.30
0.58
0.73
0.73
-
-
-

0.78
0.73
0.78
0.48
0.73

0.95
0.96
0.96
0.97
0.70

0.75
0.67
-
-
-

0.66
0.64
-
-
-

-
-
-
-
-

-0.19
-0.01
-
-
-

0.96
0.93
-
-
-

-0.06
0.21
-
-
-

0.49
0.50
-
-
-

-
-
-
-
-

0.97
0.96
0.99
0.99
0.21

0.42
0.22
0.19
0.08
0.78

-0.39
-0.53
-0.29
-0.55
0.23

0.88
0.86
0.86
0.86
0.51

-

-

-
-

-
-
-
0.38
0.42
0.62
0.50
0.18
0.20
-
-
0.66
0.30
0.08
0.07
0.86
0.77
-
-
-0.02
0.33
0.52
0.46
-0.00
-0.00
-
-
0.61
0.64
0.52
0.55
0.39
0.40
-
-

0.65
0.64
0.68
0.64
0.33

0.86
0.86
0.76
0.85
0.67

0.64
0.59
0.61
0.23
0.50

0.35
0.35
-
-
-

0.68
0.69
-
-
-

0.53
0.53
-
-
-

All mutations
cross-validation level
prot.
pos.
mut.
0.09
-0.09
0.08
0.14
-0.07
0.14
-
-0.23
-0.13
-
-0.20
-0.14
-
-0.09
0.09
0.14
-
-
0.34
0.40
0.18
0.21
0.34
0.35
-
-
0.71
0.75
0.18
0.16
0.65
0.70
-
-
0.49
0.51
0.40
0.25
0.49
0.53
0.72
0.60
0.55
0.51
0.04
0.10
0.54
0.65
-
-
0.52
0.33
-0.05
-0.00
0.51
0.39
-
-
-0.13
0.05
0.84
0.70
-0.08
0.13
-
-
0.64
0.66
0.50
0.49
0.48
0.49
-
-

0.12
0.12
-
-
-

0.65
0.66
-
-
-

0.73
0.55
-
-
-

0.52
0.52
-
-
-

0.84
0.81
0.75
0.78
0.61

0.52
0.40
0.35
0.30
0.42

0.85
0.81
0.83
0.76
0.08

0.83
0.82
0.83
0.80
0.60

3.46
3.50
-
-
-

1.47
1.48
1.38
1.48
2.26

2.44
2.44
-
-
-

1.25
1.48
1.26
1.67
1.99

1.00
1.00
-
-
-

0.47
0.50
0.49
0.70
0.54

mut.
0.74
0.70
0.99
0.99
0.75

Point mutations
cross-validation level
prot.
pos.
1.65
0.72
1.65
0.71
-
1.00
-
0.99
-
0.76
1.67
1.00
0.85
1.65
1.64
1.78
1.84
2.22
3.61
1.43
1.45
1.73
1.85
2.62
3.37
2.08
2.45
2.33
2.22
0.53
0.54
0.58
0.76
0.54
0.99
0.59
0.70
1.17
1.24
1.75
1.78
1.18
1.20
0.97
1.14
1.70
1.68
2.24
2.01
1.76
1.58
1.36
1.54
1.28
1.32
1.43
1.73
1.52
1.80
1.26
1.31
1.26
1.30
1.54
1.95
1.38
1.63
1.40
1.37

0.85
0.92
1.06
1.00
1.12

1.18
1.18
-
-
-

1.41
1.42
1.38
1.53
1.57

1.56
1.56
-
-
-

1.27
1.32
1.43
1.68
1.52

1.61
1.62
-
-
-

1.83
1.82
-
-
-

1.07
1.11
1.04
1.26
1.35

0.69
0.57
0.36
0.31
1.22

2.35
2.17
-
-
-

mut.
1.64
1.62
1.30
1.29
1.65

rmse
Multiple mutations
cross-validation level
prot.
pos.
2.70
1.48
2.43
1.47
-
1.35
-
1.33
-
1.45
2.45
-
-
1.02
0.93
1.15
1.16
1.29
2.05
-
-
2.24
1.81
3.39
2.11
1.62
2.01
-
-
-
-
-
-
-

1.71
1.90
-
-
-

2.22
1.82
3.44
2.12
1.63

-
-
-
-
-

-
-
-
-
-

0.19
0.35
0.12
0.13
0.98

5.67
6.21
6.81
6.89
7.08

2.17
2.86
1.72
2.52
7.43

1.33
1.43
1.44
1.45
2.49

-
-
0.91
0.77
0.58
0.82
1.01
1.24
-
-
5.95
6.14
6.88
6.77
7.24
6.90
-
-
7.66
7.07
4.19
6.58
7.43
7.68
-
-
2.45
2.40
2.65
2.56
2.99
2.74
-
-

1.28
0.99
-
-
-

4.20
5.43
-
-
-

7.69
7.70
-
-
-

2.53
2.50
-
-
-

All mutations
cross-validation level
prot.
2.02
1.92
-
-
-

mut.
1.09
1.06
1.09
1.09
1.10

pos.
1.21
1.20
1.21
1.20
1.20

1.35
1.35
1.24
1.33
2.09

1.40
1.53
1.69
1.73
1.95

0.47
0.50
0.49
0.70
0.54

0.81
0.88
1.01
0.95
1.10

2.65
2.85
3.05
3.14
3.23

1.40
1.57
1.47
1.80
2.88

1.13
1.18
1.14
1.30
1.66

1.94
-
-
1.45
1.43
1.59
1.63
1.94

3.35
-
-
1.85
1.84
2.81
3.14
1.99

2.40
-
-
0.53
0.54
0.58
0.76
0.54

0.99
0.59
0.70

1.13
1.17
1.61
1.66
1.15

1.20
-
-
4.75
4.90
5.53
5.41
5.74

3.16
-
-
3.64
3.40
2.28
3.34
3.62

3.09
-
-
1.87
1.85
2.09
2.23
2.22

1.92
-
-

3.26
3.27
-
-
-

2.36
2.38
-
-
-

1.00
1.00
-
-
-

1.19
1.17
-
-
-

2.23
2.64
-
-
-

3.11
3.11
-
-
-

1.84
1.84
-
-
-

∗ dataset for 1PIN contained no multiple mutations.

14

Table S6: Comparison of diﬀerent methods on the 15 protein dataset with respect to ρ and rmse after removing 10% of predictions with largest errors.
Mutation, position, and protein are referred to as mut., pos., and prot., respectively. Oﬀ-the-shelf implementations of Rosetta, mCSM and PoPMuSiC are
used directly without cross-validation.

Results after 10 %
outlier removal

Method
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled

Point mutations
cross-validation level
prot.
pos.
0.75
0.77
0.75
0.73
-
0.57
-
0.27
-
0.78

mut.
0.87
0.86
0.89
0.86
0.80

Correlation ρ
Multiple mutations
cross-validation level
prot.
pos.
0.63
0.84
0.65
0.84
-
0.76
-
0.76
-
0.71

mut.
0.97
0.96
0.98
0.97
0.73

Oﬀ-the-shelf implementations with no cross-validation

All mutations
cross-validation level
prot.
pos.
0.71
0.81
0.72
0.80
-
0.74
-
0.66
-
0.75

mut.
0.92
0.91
0.93
0.91
0.78

Point mutations
cross-validation level
prot.
pos.
0.93
0.85
0.93
0.85
-
1.01
-
1.30
-
0.87

mut.
0.69
0.69
0.65
0.73
0.84

rmse
Multiple mutations
cross-validation level
prot.
pos.
1.90
1.55
1.85
1.43
-
1.66
-
1.48
-
1.99

mut.
0.62
0.64
0.49
0.54
1.67

All mutations
cross-validation level
prot.
1.13
1.12
-
-
-

mut.
0.67
0.67
0.62
0.69
1.00

pos.
1.15
1.10
1.23
1.37
1.31

Rosetta
mCSM
PoPMuSiC

0.75
0.71
0.73

0.67
-
-

0.73
-
-

0.94
0.89
0.86

1.85
-
-

1.11
-
-

Figure S9: rmse with diﬀerent amount of predictions, when predictions are sorted by the error. Position level cross-validation was used for mGPfusion,
mGP and Rosetta scaled.

15

Figure S10: Learning curves.

16

Figure S11: Learning curves.

17

Figure S12: Learning curves.

18

Figure S13: Transformations for all 15 proteins presented in Table 1. The red circles mark the simulated ∆∆G-values yS with respect to the experimental
measured ∆∆G-values yE. Thin black lines show possible transformations for yS, whereas the thick black line shows the selected transformation from yS
to ˜yS.

19

mGPfusion: Predicting protein stability changes with
Gaussian process kernel learning and data fusion

Emmi Jokinen1, Markus Heinonen1,2 and Harri L¨ahdesm¨aki1
1Department of Computer Science, Aalto University, 02150 Espoo, Finland
2Helsinki Institute for Information Technology, Finland

Abstract

Motivation: Proteins are commonly used by biochemical industry for numerous processes. Reﬁning these proteins’ properties via
mutations causes stability eﬀects as well. Accurate computational method to predict how mutations aﬀect protein stability are necessary
to facilitate eﬃcient protein design. However, accuracy of predictive models is ultimately constrained by the limited availability of
experimental data.
Results: We have developed mGPfusion, a novel Gaussian process (GP) method for predicting protein’s stability changes upon single
and multiple mutations. This method complements the limited experimental data with large amounts of molecular simulation data.
We introduce a Bayesian data fusion model that re-calibrates the experimental and in silico data sources and then learns a predictive
GP model from the combined data. Our protein-speciﬁc model requires experimental data only regarding the protein of interest and
performs well even with few experimental measurements. The mGPfusion models proteins by contact maps and infers the stability
eﬀects caused by mutations with a mixture of graph kernels. Our results show that mGPfusion outperforms state-of-the-art methods in
predicting protein stability on a dataset of 15 diﬀerent proteins and that incorporating molecular simulation data improves the model
learning and prediction accuracy.
Availability: Software implementation and datasets are available at github.com/emmijokinen/mgpfusion
Contact: emmi.jokinen@aalto.ﬁ

8
1
0
2
 
r
a

M
 
3
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
2
5
8
2
0
.
2
0
8
1
:
v
i
X
r
a

1 Introduction

Proteins are used in various applications by pharmaceutical, food, fuel,
and many other industries and their usage is growing steadily (Kirk
et al., 2002; Sanchez and Demain, 2010). Proteins have important ad-
vantages over chemical catalysts, as they are derived from renewable
resources, are biodegradable and are often highly selective (Cherry and
Fidantsef, 2003). Protein engineering is used to further improve the
properties of proteins, for example to enhance their catalytic activity,
modify their substrate speciﬁcity or to improve their thermostability
(Rapley and Walker, 2000). Increasing the stability is an important as-
pect of protein engineering, as the proteins used in industry should be
stable in the industrial process conditions, which often involve higher
than ambient temperature and non-aqueous solvents (Bommarius et al.,
2011). The properties of a protein are modiﬁed by introducing alter-
ations to its amino acid sequence. Mutations in general tend to be
destabilising, and if too many destabilising mutations are implemented,
the protein may not remain functional without compensatory stabilis-
ing mutations (Tokuriki and Tawﬁk, 2009).

The stability of a protein can be deﬁned as the diﬀerence in Gibbs
energy ∆G between the folded and unfolded (or native and denatu-
rated) state of the protein. More precisely, the Gibbs energy diﬀerence
determines the thermodynamic stability ∆Gt of the protein, as it does
not take into account the kinetic stability ∆Gk which determines the
energy needed for the transition between the folded and unfolded states
(Anslyn and Dougherty, 2006) (see Supplementary Figure S1). Here we
will consider only the thermodynamic stability and from now on it will
be referred to merely as stability ∆G.

The eﬀect of mutations can be deﬁned by the change they cause
to the Gibbs energy ∆G, denoted as ∆∆G (Pace and Scholtz, 1997).
To comprehend the signiﬁcance of stability changes upon mutations,
we can consider globular proteins, the most common type of enzymes,
whose polypeptide chain is folded up in a compact ball-like shape with
an irregular surface (Alberts et al., 2007). These proteins are only
marginally stable and the diﬀerence in Gibbs energy between the folded
and unfolded state is only about 5–15 kcal/mol, which is not much more
than the energy of a single hydrogen bond that is about 2–5 kcal/mol
(Branden and Tooze, 1999). Therefore, even one mutation that breaks
a hydrogen bond can prevent a protein from folding properly.

The protein stability can be measured with many techniques, in-
cluding thermal, urea and guanidinium chloride (GdmCl) denaturation

curves that are determined as the fraction of unfolded proteins at dif-
ferent temperatures or at diﬀerent concentrations of urea or GdmCl
(Pace and Shaw, 2000). Some of the experimentally measured sta-
bility changes upon mutations have been gathered in thermodynamic
databases such as Protherm (Kumar et al., 2006).

A variety of computational methods have been introduced to predict
the stability changes upon mutations more eﬀortlessly than through ex-
perimental measurements. These methods utilise physics or knowledge-
based potentials (Leaver-Fay et al., 2011), their combinations, or diﬀer-
ent machine learning methods. The machine learning methods utilise
support vector machines (SVM) (Capriotti et al., 2005b, 2008; Chen
et al., 2013; Cheng et al., 2006; Folkman et al., 2014; Liu and Kang,
2012; Pires et al., 2014a), random forests (Tian et al., 2010; Wainreb
et al., 2011), neural networks (Dehouck et al., 2009; Giollo et al., 2014),
and Gaussian processes (Pires et al., 2014b). However, it has been as-
sessed that although on average many of these methods provide good
results, they tend to fail on details (Potapov et al., 2009). In addition,
many of these methods are able to predict the stability eﬀects only for
single-point mutations.

We introduce mGPfusion (mutation Gaussian Processes with data
fusion), a method for predicting stability eﬀects of both point and mul-
tiple mutations. mGPfusion is a protein-speciﬁc model – in contrast to
earlier stability predictors that aim to estimate arbitrary protein struc-
ture or sequence stabilities – and achieves markedly higher accuracy
while utilising data only from a single protein at a time. In contrast
to earlier works that only use experimental data to train their models,
we also combine exhaustive Rosetta (Leaver-Fay et al., 2011) simulated
point mutation in silico stabilities to our training data.

A key part of mGPfusion is the automatic scaling of simulated data
to better match the experimental data distribution based on those vari-
ants that have both experimental and simulated stability values. Fur-
thermore, we estimate a variance resulting from the scaling, which
places a higher uncertainty on very destabilising simulations. Our
Gaussian process model then utilises the joint dataset with their es-
timated heteroscedastic variances and uses a mixture of graph kernels
to assess the stability eﬀects caused by changes in amino acid sequence
according to 21 substitution models. Our experiments on a novel 15
protein dataset show a state-of-the-art stability prediction performance,
which is also sustained when there is access only to a very few experi-
mental stability measurements.

1

Figure 1: Pipeline illustration for mGPfusion. a) M = 21 substitution matrices utilise diﬀerent information sources and give scores to pairwise amino
acid substitutions. b) The wild-type structures from Protein Data Bank are modelled as contact graphs. c) The graph kernel measures similarity of two
sequences by a substitution model S over all positions p and their neighbourhoods nbs(p) in the contact graph. d) Each substitution matrix is used to
create a separate covariance matrix. e) Multiple kernel learning (MKL) is used for ﬁnding the optimal combination of the base kernels. The kernel matrix
measures variant similarities. f ) Experimentally measured ∆∆G values yE are gathered from Protherm and Rosetta’s ddg monomer application is used to
simulate the stability eﬀects yS for all single point mutations. g) Bayesian scaling for the simulated values yS at the x-axis. Possible scalings are coloured
with green and the chosen scaling from yS into scaled values ˜yS is marked by black dots. The scaling is ﬁtted to a subset of experimentally measured
stabilities yE (circles). h) The stability predictive GP model is trained using experimental and simulated data through the kernel matrix.

2 Methods

Following Pires et al. (2014b) we choose a Bayesian model family of
Gaussian processes for prediction of mutation eﬀects on protein sta-
bility due to its inherent ability to handle uncertainty in a principle
way. Bayesian modelling is a natural approach for combining the ex-
perimental and simulated data distribution, while it is also suitable for
learning the underlying mixture of substitution models that governs the
mutational process.

The pipeline for mGPfusion is presented in Figure 1. The ﬁrst
part of mGPfusion consists of collection of in silico and experimental
datasets discussed in Section 2.1, the scaling of the in silico dataset in
Section 2.2 and the fusion of these two datasets in Section 2.3. The sec-
ond part consists of the Gaussian process model described in Section 2.4
with detailed description of the graph kernels in Sections 2.5-2.6 and
model inference in Section 2.7. Finally, the evaluation criteria used are
described in Section 2.8.

teins that fulﬁlled these requirements are listed in Table 1. We aver-
aged the stability values for proteins with multiple measurements and
ignored mutations to residues not present in their 3D structures. These
data sets are available at github.com/emmijokinen/mgpfusion.

Protein (organism)

PDB

mutations
point

point (sim)

all

2LZM 349
T4 Lysozyme (Enterobacteria phage T4)
182
1BNI
Barnase (Bacillus amyloliquefaciens)
124
1VQB
Gene V protein (Escherichia virus m13)
116
1LZI
Glycosyltransferase A (Homo sapiens)
98
2CI2
Chymotrypsin inhibitor 2 (Hordeum vulgare)
89
1PGA
Protein G (Streptococcus sp. gx7805)
83
2RN2
Ribonuclease H (Escheria coli)
80
1CSP
Cold shock protein B (Bacillus subtilis)
80
1BVC
Apomyoglobin (Physeter catodon)
63
4LYZ
Hen egg white lysozyme (Gallus gallus)
57
Ribonuclease A (Bos taurus)
1RTB
56
Peptidyl-prolyl cis-trans isomerase (Homo sapiens) 1PIN
53
1RN1
Ribonuclease T1 isozyme (Aspergillus oryzae)
54
1RGG
Ribonuclease (Streptomyces auerofaciens)
53
1BPI
Bovine pancreatic trypsin inhibitor (Bos taurus)

264
163
92
114
77
34
65
50
56
50
50
56
48
45
47

3116
2052
1634
2470
1235
1064
2945
1273
2907
2451
2356
2907
1957
1824
1102

2.1 Experimental and in silico data

total

1537

1211

31293

Protherm is a database of numerical thermodynamic parameters for
proteins and their mutants (Kumar et al., 2006). From Protherm we
gathered all proteins with at least 50 unique mutations whose ∆∆G
has been measured by thermal denaturation, and where a PDB code
for a 3D structure of the protein was reported. We required the pro-
teins to have at least 50 unique mutations, so that we would have a
representative test set and get suﬃciently reliable estimates of predic-
tion accuracy on individual proteins and examine how the amount of
experimental training data aﬀects the accuracy of the model. The 3D
structures are necessary for obtaining the connections between residues.
We collected the 3D structures with the reported PDB codes from the
Protein Databank, www.rcsb.org (Berman et al., 2000). The 15 pro-

Table 1: The 15 protein data from ProTherm database with counts of point
mutations, all mutations, and of simulated point mutation stability changes.

We also generate simulated data of the stability eﬀects of all possible
single mutations of the proteins. Our method can utilise any simulated
stability values. We used the “ddG monomer” application of Rosetta
3.6 (Leaver-Fay et al., 2011) using the high-resolution backrub-based
protocol 16 recommended in Kellogg et al. (2011). The predictions yS
made with Rosetta are given in Rosetta Energy Units (REU). Kellogg
et al. (2011) suggest transformation 0.57yS for converting the predic-
tions into physical units. The simulated data scaled this way is not
as accurate as the experimental data, the correlation and root mean

2

square error (rmse) with respect to the experimental data are shown
for all proteins in Table 2 and for individual proteins in Supplementary
Table S2, on rows labelled Rosetta. For this reason, we use instead a
Bayesian scaling described in the next section and diﬀerent noise models
for the experimental and simulated data, described in Section 2.3.

For each of the 15 proteins,

let xi = (xi1, . . . , xiM ) denote its
M -length variant i with positions p labelled with residues xip ∈
{A, R, N, . . . , V }. We denote the wild-type protein as x0. We collect
15 separate sets of simulated and experimental data. We denote the
NE )T with
NE experimental variants of each protein as XE = (xE
the corresponding experimental stability values yE = (yE
NE )T ∈
RNE . Similarly, we denote the NS simulated observations as XS =
(xS

1 , . . . , xE

1 , . . . , yE

NS )T and yS = (yS

NS )T ∈ RNS .

1 , . . . , xS

1 , . . . , yS

2.2 Bayesian scaling of in silico data

The described transformation from REU to physical units may not
be optimal for all proteins. We therefore applied instead a linear-
exponential scaling function to obtain scaled Rosetta simulated sta-
bilities ˜yS,

˜yS = g(yS | θj) = ajecj yS

+ bjyS + dj.

(1)

This scaling transforms the Rosetta simulations yS for each protein
j = 1, . . . , 15 to correspond better to the experimental data. The pa-
rameters θj = (aj, bj, cj, dj) deﬁne the weight aj and steepness cj of the
exponential term, while the linear term has slope bj and intercept dj.
To avoid overﬁtting, we perform Bayesian linear regression and start
by deﬁning parameter prior p(θj) = p(aj)p(bj)p(cj)p(dj) that reﬂects
our beliefs about realistic scalings having only moderate steepness:

and stabilities y = (y0, yE, ˜yS) of size RN where N = 1 + NE + NS is
the total number of simulated and experimental data points, including
the wild-type. We assume heteroscedastic additive noise models for the
three information sources

y0 = f (x0) + ε0,
(cid:1) + εE
i = f (cid:0)xE
yE
i ,
(cid:1) + εS
i = f (cid:0)xS
˜yS
i ,

i

i

(cid:1)

(cid:1)

0

ε0 ∼ N (cid:0)0, σ2
i ∼ N (cid:0)0, σ2
εE
(cid:16)
εS
i ∼ N

E

0, (σE + σS + tσT (i))2(cid:17)

,

(5)

where the observed values are noisy versions of the underlying ‘true’
stability function f (x) corrupted by zero-mean noise with data source
speciﬁc variances. We learn a Gaussian process based stability function
f (x) in the next Section.

The Equations (5) encode that the experimental data are corrupted
by a global experimental noise variance σ2
E. The simulated stabilities
are additionally corrupted by a global Rosetta simulator error variance
σ2
S, and by the value-dependent transformation variance tσ2
T (i) scaled
by parameter t. The model then encapsulates that we trust the Rosetta
data less than the experimental data. By deﬁnition, the ∆∆G of the
wild-type is zero (y0 = 0) with very small assumed error, σ0 = 10−6.
Note that σ2
T are ﬁxed by equation (4), while we infer the optimal
values for the remaining three free parameters (σE, σR, t) (See Section
2.4). The parameters σ2

S are assigned priors

E and σ2

σE ∼ Gamma(σE|αE, βE)
σS ∼ Gamma(σR|αS, βS).

(6)

The values of these hyperparameters are shown in Supplementary Ta-
ble S1.

p(aj) = Gamma(aj | αa, βa)
p(bj) = Beta(1/2 · bj | αb, βb)
p(cj) = Beta(10/3 · cj | αc, βc)
p(dj) = N (dj | µd, σ2

d).

2.4 Gaussian processes

(2)

We use a Gaussian process (GP) function f to predict the stability
f (x) ∈ R of a protein variant x. Gaussian processes are a family of
non-parametric, non-linear Bayesian models (Rasmussen and Williams,
2006). A zero-mean GP prior

The empirically selected hyperparameter values are listed in Supple-
mentary Table S1 and the priors are illustrated in Figure S2.

We compute the posterior for θj using the subset of simulated data

that have corresponding experimentally measured data:

p(θj|yE, yS) ∝

(cid:89)

N (cid:0)yE

i

| g(yS

i |θj), σ2
n

(cid:1) p(θj).

i:xi∈XE ∩XS

The product iterates over all NE∩S simulated ∆∆G’s that have a
matching experimentally observed value. The σ2
n is the scaling error
variance, which was set to σ2
n = 0.5. The parameters θ for each protein
were sampled using a random walk Metropolis-Hastings MCMC algo-
rithm (the mhsample function in Matlab) for NM C = 10000 samples
with a burn-in set to 500. The proposal distribution was selected to be
a symmetric uniform distribution such that [as+1, bs+1, cs+1, ds+1] ∼
U (as ± 0.4, bs ± 0.04, cs ± 0.04, ds ± 0.4). Given the sample of scaling
parameters (θ(s)
s=1 , we deﬁne the scaled simulated data as the aver-
age scaling over the MCMC sample, and record also the sample scaling
variance

j )NM C

˜yS
i =

g(yS

i |θ(s)
j )

1
NM C

1
NM C

NM C(cid:88)

s=1
NM C(cid:88)

(cid:16)

s=1

σ2
T (i) =

g(yS

i |θ(s)

j ) − ˜yS
i

(cid:17)2

.

See Figure 1 g) for an illustration of the scaling. We collect the scaled
simulated value and its variance from each simulated point into vectors
˜yS = (˜yS

T (NS)) ∈ RNS .

T (1), . . . , σ2

NS ) and σ2

1 , . . . , ˜yS

T = (σ2

deﬁnes a distribution over functions f (x) whose mean and covariance
are

f (x) ∼ GP (0, k(x, x(cid:48))) ,

E[f (x)] = 0
cov[f (x), f (x(cid:48))] = k(x, x(cid:48)).

For any collection of protein variants X = x1, . . . , xN , the function val-
ues follow a multivariate normal distribution f ∼ N (0, KXX ), where
f = (f (x1), . . . , f (xN ))T ∈ RN , and where KXX ∈ RN ×N with
[KXX ]ij = k(xi, xj). The key property of Gaussian processes is that
they encode functions that predict similar stability values f (x), f (x(cid:48))
for protein variants x, x(cid:48) that are similar, as encoded by the kernel
k(x, x(cid:48)). The key part of GP modelling is then to infer a kernel that
measures the mutation’s eﬀects to the stability.

Let a dataset of noisy stability values from two sources be y ∈ RN ,
the corresponding protein structures X = (xi)N
i=1, and a new protein
variant x∗ whose stability we wish to predict. A Gaussian process de-
ﬁnes a joint distribution over the observed values y of variants X, and
the unknown function value f (x∗) of the unseen variant x∗,
(cid:21)(cid:19)

(cid:18)

(cid:21)

(cid:20) y
f (x∗)

∼ N

0,

(cid:20)KXX + diag(σ2)
k∗X

kX∗
k(x∗, x∗)

,

∗X ∈ RN is a kernel vector with elements k(xi, x∗) for
where kX∗ = kT
T )2)T
all i = 1, . . . , N , and where σ2 = (σ2
collects ﬁnal variances of the data points from equations (5). Here
the exponents are elementwise. The conditional distribution gives the
posterior distribution of the stability prediction as

E1T , (σE1T + σS1T + tσT

0, σ2

f (x∗)|(X, y) ∼ N (cid:0)µ(x∗), σ2(x∗)(cid:1) ,

2.3 Data fusion and noise models

where the prediction mean and variance are

For each protein j, we organise its experimental data (XE, yE) and
transformed simulated data (XS, ˜yS) along with the wild-type infor-
mation (x0, y0) into a single joint dataset of variants X = (x0, XE, XS)

(cid:0)KXX + diag(σ2)(cid:1)−1 y,

µ(x∗) = k∗X
σ2(x∗) = k(x∗, x∗) − k∗X

(cid:0)KXX + diag(σ2)(cid:1)−1 kX∗.

(3)

(4)

3

Hence, in GP regression the stability predictions µ(x∗) ± σ(x∗) will
come with uncertainty estimates.

ring amino acids and scaled them between zero and one as

2.5 Graph kernel

Next, we consider how to compute the similarity function k(x, x(cid:48)) be-
tween two variants of the same protein structure. We will encode the
3D structural information of the two protein variants as a contact map
and measure their similarity by the formalism of graph kernels (Vish-
wanathan et al., 2010).

We consider two residues to be in contact if their closest atoms are
within 5 ˚A of each other in the PDB structure, which is illustrated in
Figure 1 b). All variants of the same protein have the same length, with
only diﬀerent residues at mutating positions. Furthermore, we assume
that all variants share the wild-type protein contact map.

To compare protein variants, we construct a weighted decomposition
kernel (WDK) (Menchetti et al., 2005) between two protein variants
x = (x1, . . . , xM ) and x(cid:48) = (x(cid:48)

M ) of length M ,

1, . . . , x(cid:48)

k(x, x(cid:48)) =


S(xp, x(cid:48)
p)

M
(cid:88)

p=1



(cid:88)

S(xl, x(cid:48)
l)

,

l∈nbs(p)

(7)

where nbs(p) deﬁnes the set of neighbouring positions to position p,
and S is a substitution matrix. The kernel iterates over all positions
p and compares for each of them their residues through a substitution
matrix S(xp, x(cid:48)
p). Furthermore, the similarity of the residues at each
position is multiplied by the average similarity of the residues at its
neighbouring positions S(xl, x(cid:48)
l). Hence, the kernel deﬁnes the simi-
larity of two protein variants as the average position and neighbour-
hood similarity over all positions. The kernel matrix is normalised
so that for two data points, the normalised kernel
p) =
k(xp, x(cid:48)
p, x(cid:48)
p), as deﬁned by Shawe-Taylor and Cris-
tianini (2004). The kernel is illustrated in Figure 1 c).

k(xp, xp)k(x(cid:48)

is ˆk(xp, x(cid:48)

p)/

(cid:113)

The above WDK kernel allows us to compare the eﬀects of multiple
simultaneous mutations. However, as the wild type protein structure
is used for all of the protein variants, changes that the mutations may
cause to the protein structure are not taken into consideration. This
may cause problems if mutations that alter the protein structure sig-
niﬁcantly are introduced – especially if many of them are introduced
simultaneously. On the other hand, substitution matrices that have
their basis in sequence comparisons, should take these eﬀects into ac-
count to some extend as these kinds of mutations are usually highly
destabilising and do not occur often in nature. In the next section, we
will discuss how we utilise diﬀerent substitution matrices with multiple
kernel learning.

The BLOSUM substitution models have been a common choice for
protein models (Giguere et al., 2013), while mixtures of substitution
models were proposed by Cichonska et al. (2017). BLOSUM matrices
score amino acid substitutions by their appearances throughout evolu-
tion, as they compare the frequencies of diﬀerent mutations in similar
blocks of sequences (Henikoﬀ and Henikoﬀ, 1992). However, there are
also diﬀerent ways to score amino acids substitutions, such as chemical
similarity and neighbourhood selectivity (Tomii and Kanehisa, 1996).
When the stability eﬀects of mutations are evaluated, the frequency of
an amino acid substitution in nature may not be the most important
factor.

To take into account diﬀerent measures of similarity between amino
acids, we employed a set of 21 amino acid substitution matrices gath-
ered from AAindex21 (Kawashima et al., 2008). AAindex2 currently
contains 94 substitution matrices. From these we selected those that
had no gaps concerning substitutions between the 20 naturally occur-

1http://www.genome.jp/aaindex/
2http://www.cs.ubc.ca/~schmidtm/Software/minConf.html

4

S =

S0 − min(S0) + 1
max(S0) − min(S0) + 1

.

Out of these matrices, we only chose those 23 matrices that were pos-
itive semideﬁnite. Furthermore, there were two pairs of matrices that
were extremely similar, and we only selected one matrix from each pair,
ending up with 21 substitution matrices. These substitution matrices
are used together with Equation 7 for computing 21 base kernel matri-
ces. Finally, MKL is used to ﬁnd an optimal combination of the base
kernels of form

(8)

(9)

Kφ =

wmK (γm)
m ,

21
(cid:88)

m=1

where wm is a kernel speciﬁc weight, γm is an (elementwise) exponent.
The elementwise exponent retains the SDP property of Kφ (Shawe-
Taylor and Cristianini, 2004). We observe empirically that the optimal
kernel weights wm tend to be sparse (See Figure 2).

The selected substitution matrices are listed in Figure 2. These ma-
trices have diﬀerent basis and through multiple kernel learning (MKL)
our model learns which of these are important for inferring the stability
eﬀects that mutations cause on diﬀerent proteins. The ﬁgure illustrates
this by showing the average weights of the base kernel matrices obtained
via the multiple kernel learning.

2.7 Parameter inference

The complete model has ﬁve parameters φ = (σE, σS, t, w, γ) to in-
fer, of which the variance parameters (σE, σS, t) parameterise the joint
data variance σ2
φ, while the MKL parameters w = (w1, . . . , w21) and
γ = (γ1, . . . , γ21) parameterise the kernel matrix Kφ. In a Gaussian
process model these can be jointly optimised by the marginal (log)
likelihood with priors

log p(y|φ)p(σE)p(σR) = log

p(y|f , φ)p(f |φ)p(σE)p(σR)df

(cid:90)

∝ −

yT (Kφ + diag(σ2

1
2
+ log Gamma(σE|αE, βE) + log Gamma(σS|αS, βS),

log |Kφ + diag(σ2

φ))−1y −

φ)|

1
2

(10)

which automatically balances model ﬁt (the square term) and the
model complexity (the determinant) to avoid overﬁtting (Rasmussen
and Williams, 2006). The parameters can be optimised by maximis-
ing the marginal log likelihood (10) using gradient ascent, since the
marginal likelihood can be diﬀerentiated analytically (see Supplemen-
tary Equations S1 and S2). We utilised a limited-memory projected
quasi-Newton algorithm (minConf TMP2), described by (Schmidt et al.,
2009).

We chose to evaluate the accuracy of our predictions using the same
metrics that have been used by many others – correlation ρ between the
predicted and experimentally measured ∆∆G values (Capriotti et al.,
2005a; Dehouck et al., 2009; Kellogg et al., 2011; Pires et al., 2014b;
Potapov et al., 2009) and the root mean square error (rmse) (Dehouck
et al., 2009; Pires et al., 2014a,b), which are determined in the Supple-
mentary Equations S3 and S4. We use marginal likelihood maximisa-
tion to infer model parameters and perform cross-validation to evaluate
the model performance on test data. Below we only report evaluation
metrics obtained from the test sets not used at any stage of the model
learning or data transformation sampling.

3 Results

In this section we evaluate the performance of mGPfusion on predict-
ing stability eﬀects of mutations, and compare it to the state-of-the-
art prediction methods mCSM, PoPMuSiC and Rosetta. Rosetta is a

2.6 Substitution matrices and multiple kernel

2.8 Evaluation criteria

learning

Figure 2: Average weights for kernels utilising the described substitution matrices from AAindex2, when GP models were trained with mutation level
cross-validation. Basis for the substitution matrices are obtained from (Tomii and Kanehisa, 1996). ∗ were added to AAindex2 in a later release, and their
basis were not determined by Tomii and Kanehisa (1996).

molecular modelling software whose ddg monomer module can directly
simulate the stability changes ∆∆G of a protein upon mutations. PoP-
MuSic and mCSM are machine learning models that predict stability
based on protein variant features. We run Rosetta locally, and use
mCSM and PoPMuSiC models through their web servers3. This may
give these methods an advantage over mGPfusion since parts of our
testing data were likely used within their training data.

We compare four diﬀerent variants of our method: mGPfusion that
uses both simulated data and MKL, “mGPfusion, only B62” that uses
simulated data but incorporates only one kernel matrix (BLOSUM62
substitution matrix), mGP model that uses MKL but does not use simu-
lated data, and “mGP, only B62” that uses only the base GP model but
does not incorporate simulated data and uses only the BLOSUM62 sub-
stitution matrix. In addition, we experiment on transforming Rosetta
predictions with the Bayesian scaling. We perform the experiments
for the 15 proteins separately using either position or mutation level
(leave-one-out) cross-validation regarding the methods mGP, mGPfu-
sion and the Bayesian scaling of Rosetta. Pires et al. (2014b) used
protein and position level cross-validation to evaluate their model. In
protein level cross-validation all mutations in a protein are either in
the test or training set exclusively. When we train our model using
protein level cross-validation, we use no experimental data and rely
only on the simulated data. Position level cross-validation is deﬁned
so that all mutations in a position are either in the test or training set
exclusively. However, datasets in Pires et al. (2014b) contained only
point mutations and therefore we had to extend the deﬁnition to also
include multiple mutations. In position level cross-validation we train
one model for each position using only the part of data that has a
wild-type residue in that position. Therefore, in position level cross-
validation we construct a test set that contains all protein variants that
have a mutation at position p and use as training set all the protein
variants that have a wild-type residue at that position. Dehouck et al.
(2009) evaluated their models by randomly selecting training and test
sets so that each mutation was exclusively in one of the sets, but both
sets could contain mutations from the same position of the same pro-

3biosig.unimelb.edu.au/mcsm and

omictools.com/popmusic-tool

tein. We call this mutation level cross-validation. When we use all
available experimental data with mutation level cross-validation, this
corresponds to leave-one-out cross-validation.

3.1 Predicting point mutations

Table 2 summarises the average prediction performance over all 15 pro-
teins for all compared methods, types of mutations and cross-validation
types. We ﬁrst compare the performances on single point mutations,
where mGPfusion and mGP achieve the highest performance with
ρ = 0.81 and rmse = 1.07 kcal/mol, and ρ = 0.81 and rmse = 1.04
kcal/mol, respectively with mutation level cross-validation. With only
one kernel utilising the BLOSUM62 matrix instead of MKL, the per-
formance decreases slightly, but the competing methods are still out-
performed, as mCSM achieves ρ = 0.64 and rmse = 1.37 kcal/mol,
PoPMuSic ρ = 0.61 and Rosetta ρ = 0.55. Applying Bayesian scaling
on Rosetta simulator improves the performance of standard Rosetta
from ρ = 0.55 to ρ = 0.65 and decreases the rmse from 1.63 kcal/mol
to 1.35 kcal/mol, which is interestingly even slightly better than the
performances of mCSCM and PoPMuSiC.

With position level cross-validation mGPfusion achieves the high-
est performance of ρ = 0.70 and rmse = 1.26 kcal/mol, likely due
to having still access to simulated variants from that position, since
they are always available to the learner. Without simulation data,
the baseline machine learning model mGP performance decreases to
ρ = 0.51 and rmse = 1.54 kcal/mol, thus demonstrating the impor-
tance of the data fusion. Cross-validation could not be performed for
the oﬀ-the-shelf methods mCSM and PoPMuSiC. Even still, mGPfusion
(trained with one or multiple kernels) outperforms competing state-of-
the-art methods and achieves markedly higher prediction performance
as quantiﬁed by both mutation and position level cross-validations.
Also mGP outperforms these methods when quantiﬁed by mutation
level cross-validation. With protein level cross-validation mGPfusion
achieves slightly better results than Rosetta.

5

Method
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled

Point mutations
cross-validation level
prot.
pos.
mut.
0.56
0.70
0.81
0.56
0.69
0.79
-
0.51
0.81
-
0.34
0.76
-
0.63
0.65

Correlation ρ
Multiple mutations
cross-validation level
prot.
pos.
mut.
0.49
0.61
0.88
0.50
0.64
0.86
-
0.52
0.86
-
0.55
0.86
-
0.39
0.51

All mutations
cross-validation level
prot.
pos.
mut.
0.52
0.64
0.83
0.52
0.66
0.82
-
0.50
0.83
-
0.49
0.80
-
0.48
0.60

Point mutations
cross-validation level
prot.
pos.
mut.
1.61
1.26
1.07
1.62
1.30
1.11
-
1.54
1.04
-
1.95
1.26
-
1.38
1.35

rmse
Multiple mutations
cross-validation level
prot.
pos.
mut.
2.53
2.45
1.33
2.50
2.40
1.43
-
2.65
1.44
-
2.56
1.45
-
2.99
2.49

All mutations
cross-validation level

mut.
1.13
1.18
1.14
1.30
1.66

pos.
1.87
1.85
2.09
2.23
2.22

prot.
1.84
1.84
-
-
-

Predictions from oﬀ-the-shelf implementations with no cross-validation

Rosetta
mCSM
PoPMuSiC

0.55
0.61
0.64

0.40
-
-

0.49
-
-

1.63
1.40
1.37

2.74
-
-

1.92
-
-

Table 2: Comparison of diﬀerent methods on the 15 protein dataset with respect to ρ and rmse. Mutation, position, and protein are referred to as mut.,
pos., and prot., respectively. Predictions from oﬀ-the-shelf implementations of Rosetta, mCSM and PoPMuSiC are used directly without cross-validation.

3.2 Predicting multiple mutations

Next, we tested stability prediction accuracies for variants containing
either single or multiple mutations. Figure 3 shows a scatter plot of
mGPfusion predictions for all 1537 single and multiple mutation vari-
ants (covering all 15 proteins) against the experimental ∆∆G values
using the mutation level (leave-one-out) cross-validation. The points
are coloured by the number of simultaneous mutations in the variants,
with 326 variants having at least 2 mutations (See Table 1). Fig-
ure 3 illustrates the mGPfusion’s overall high accuracy of ρ = 0.83
and rmse = 1.13 kcal/mol on both single and multiple mutations (See
Table 2). Scatter plots for the individual proteins can be found in Sup-
plementary Figure S3. Dehouck et al. (2009) suggested that considering
the predictive power after removal of most badly predicted stability ef-
fects of mutations may give more relevant evaluation, as some of the
experimental measurements may have been made in non-physiological
conditions or aﬀected by signiﬁcant error, associated with a poorly re-
solved structure, or indexed incorrectly in the database. They thus
reported correlation and rmse of the predictions after excluding 10 %
of the predictions with most negative impacts on the correlation co-
eﬃcient. Pires et al. (2014b) also reported their accuracy after 10 %
outlier removal. If we remove the 10% worst predicted stability eﬀects
from the combined predictions, we achieve correlation ρ of 0.92 and
rmse of 0.67 kcal/mol. We report these results for all the methods
in Supplementary Table S3 and also present the error distribution in
Supplementary Figure S5.

Figure 3: Scatter plot for the mutation level (leave-one-out) predictions made
for all 15 proteins (See Table 1). The colour indicates the number of simul-
taneous mutations.

The high accuracy is retained for variants with multiple mutations
as well (ρ = 0.88 and rmse = 1.33 kcal/mol, see Tables 2 and S2).
Table 3 lists mGPfusion’s rmse for diﬀerent number of simultaneous
mutations. The model accuracy in fact improves up to 6 mutations.
This is explained by the training set often containing the same single
point mutations that appear in variants with multiple mutations. The
model can then infer the combined eﬀect of pointwise mutations. The
model seems to fail when predicting the eﬀects of 7-9 simultaneous mu-

6

tations. Most of these mutations (8/12) are for Ribonuclease (1RGG)
and their eﬀects seem to be exceptionally diﬃcult to predict. This may
be because only few of the point mutations that are part of the multiple
mutations are present in the training data. However, these mutations
seem to be exceptionally diﬃcult to predict for Rosetta as well, which
could indicate that the experimental measurements concerning these
mutations are not quite accurate. PoPMuSiC and mCSM are unable
to predict multiple mutations, while Rosetta supports them, but its
rmse accuracy decreases already with two mutations.

mutations
occurences

mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta

1
1211

1.07
1.11
1.04
1.26
1.35
1.63

2
207

1.06
1.12
1.03
0.96
2.10
2.27

3
52

0.80
0.77
0.61
0.65
1.92
2.11

4
42

0.51
0.59
0.50
0.83
2.94
3.78

5
4

0.40
0.29
0.18
0.26
2.29
2.93

6
8

1.01
1.14
0.92
1.14
2.32
2.21

7
3

3.02
3.00
3.23
2.95
2.93
2.92

8
3

5.89
6.78
6.18
6.90
6.75
5.80

9
6

5.16
5.56
6.75
6.57
7.28
7.45

10
1

0.25
0.11
0.08
0.05
2.69
3.42

Table 3: Root-mean-square errors for diﬀerent number of simultaneous
mutations for all 15 proteins, with models trained by leave-one-out cross-
validation. Rosetta is added for comparison.

With multiple mutations, the decrease in performance between the
position and mutation level cross-validations becomes clearer than with
single mutations. With the position level cross-validation the stability
eﬀects of multiple mutations are predicted multiple times, which partly
explains this loss of accuracy. For example, the eﬀects of mutants with
nine diﬀerent simultaneous mutations, which were the most diﬃcult
cases in the mutation level cross-validation, are predicted nine times.
Surprisingly, mGPfusion trained with protein level cross-validation
achieves higher correlation and smaller errors than Rosetta; mGPfusion
utilising simulated ∆∆G values for only single mutations, can predict
the stability eﬀects of multiple mutations better than Rosetta.

3.3 Uncertainty of the predictions

Gaussian processes provide a mean µ(x) and a standard deviation σ(x)
for the stability prediction of a protein variant x. The standard de-
viation allows estimation of the prediction accuracy even without test
data. Figure 1 h) visualises the uncertainty of a few predictions made
for the protein G (1PGA) when mutation level cross-validation is used.
The estimated standard deviation allows a user to automatically iden-
tify low quality predictions that can appear e.g. in parts of the input
protein space from which less data is included in model training. Con-
versely, in order to minimise the amount of uncertainty in the mGPfu-
sion predictions, estimated standard deviation can be used to guide next
experiments. The probabilistic nature of the predictions also admits an
alternative error measure of negative log probability density (NLPD)
nlpd = − (cid:80)N
i=1 log p(yi|µ(xi), σ2(xi)), which can naturally take into
account the prediction variance.

3.4 Eﬀect of training set size

The results presented in Sections 3.1–3.3 used all available data for
training with cross-validation to obtain unbiased performance mea-
sures. The inclusion of thousands of simulated variants allows the model

to learn accurate models with less experimentally measured variants.
Hence, we study how the mGPfusion model with or without simulated
data performs with reduced number of experimental observations. To
facilitate this, we randomly selected subsets of experimental data of
size 0, 10, 20, and so on. We learned the mGP and mGPfusion models
with these reduced experimental data sets while always using the full
simulated data sets. This also allows us to estimate how the models
work with diﬀerent number of cross-validation folds. For example, the
point of a learning curve which utilises 2/3 or 4/5 of the training data
correspond to an average of multiple 3-fold or 5-fold cross-validations,
respectively.

The learning curve in Figure 4a) shows how the averaged correlation
for protein 2LZM improves when the size of the experimental data set
increases. The right-most values at N = 348 are obtained with leave-
one-out cross-validation. The inclusion of simulated data in mGPfusion
(dark blue line) consistently improves the performance of mGP, which
is trained without simulated data. Figure 4b) illustrate the diﬀerence
in root mean square error. Learning curves for all proteins listed in
Table 1 can be found from the Supplementary Figures S6-S8. When
the number of experimental samples is zero, the mGPfusion model is
trained solely using the simulated data with scaling 0.57yS, and the
mGP model predicts the stability eﬀect of every mutation as zero. The
last point on the learning curves is obtained with mutation level cross-
validation (see Tables 2 and S2).

proteins mGPfusion model trained with only one kernel that utilises
BLOSUM62, provides approximately as good results as the mGPfu-
sion model trained with multiple kernels. However, with many of the
proteins, utilising just BLOSUM62 does not seem to be suﬃcient and
the accuracy of the model can be improved by using diﬀerent substi-
tution matrices. Prior knowledge of appropriate substitution models
for each protein could enable creation of accurate prediction models
with just one substitution model, but the MKL seems to be a good
tool for selecting suitable substitution models when such knowledge is
not available. It seems that the data fusion and number or relevance
of used substitution matrices can compensate each other – the learning
curves show, that the diﬀerence between mGPfusion models trained
with one or multiple kernels is smaller than the diﬀerence between the
mGP models utilising one or multiple kernels. This indicates that if ad-
ditional simulated data is exploited, the use of multiple or appropriate
substitution models is not as important than without the data fusion.
On the other hand, if data fusion is not applied, the use of MKL can
more signiﬁcantly improve the accuracy of the mGP model.

3.6 Eﬀect of

the Bayesian transformation on

Rosetta

The Bayesian scaling of simulated Rosetta values, proposed in Sec-
tion 2.2, improves the match of Rosetta simulated values to empirical
∆∆G values even without using the Gaussian process framework. The
Bayesian scaling improves the performance of standard Rosetta sim-
ulations from ρ = 0.55 and rmse = 1.63 kcal/mol to ρ = 0.65 and
rmse = 1.35 kcal/mol (see Table 2 and Supplementary Table S2). This
shows that the scaling proposed by Kellogg et al. (2011) indeed is not
always the optimal scaling and signiﬁcant improvements can be gained
by optimising the scaling using a set of training data.

Figure 1 g) visualises the Bayesian scaling for protein 1PGA, where
the very destabilising ∆∆G values are dampened by the scaling (black
dots) to less extreme values by matching the scaled simulated values to
the experimental points (blue circles). The black dots along the scaling
curve indicate the grid of point mutations after transformation. The
scaling variance σ2
T is indicated by the green region’s vertical width,
and on the right panel. The scaling tends to dampen very small values
into less extreme stabilities, while it also estimates higher uncertainties
for stability values further away from ∆∆G = 0. However, the scalings
vary between diﬀerent proteins, as can be seen from the transformations
for each of the 15 proteins presented in Supplementary Figure S9.

Figure 4: a) Correlation and b) root mean square error of predictions made
by models with diﬀerent number of experimental training samples for T4
Lysozyme (2LZM). The results of Rosetta, mCSM and PoPMuSiC are in-
variant to training data (because mCSM and PoPMuSiC are pre-trained),
and are thus constant lines. For both ﬁgures, an average of 100 randomly
selected training sets is taken at each point.

3.5 Eﬀect of data fusion and multiple substitution

matrices

In the beginning of the learning curves, when only little training data is
available, mGPfusion quite consistently outperforms the mGP model,
demonstrating that the additional simulated data improves the predic-
tion accuracy. However, when more training data becomes available,
the performance of mGP model is almost as good or sometimes even
better than the performance of the mGPfusion model. This shows that
if enough training data is available, it is not necessary to simulate addi-
tional data in order to obtain accurate predictions. Table 2 also shows,
that the data fusion can compensate the lack of relevant training data
– with the mGPfusion models that utilise the additional data, the de-
crease in accuracy is smaller when position level cross-validation is used
instead of mutation level cross-validation, than with the mGP models.
The varying weights for the base kernels between diﬀerent pro-
teins (shown in Figure 2) already illustrated that diﬀerent proteins
beneﬁt from diﬀerent similarity measures for amino acid substitutions.
The learning curves also support this observation – with some of the

4 Conclusions

We present a novel method mGPfusion for predicting stability eﬀects of
both single and multiple simultaneous mutations. mGPfusion utilises
structural information in form of contact maps and integrates that with
amino acid residues and combines both experimental and comprehen-
In con-
sive simulated measurements of mutations’ stability eﬀects.
trast to earlier general-purpose stability models, mGPfusion model is
protein-speciﬁc by design, which improves the accuracy but necessitates
having a set of experimental measurements from the protein. In prac-
tise small datasets of 10–20 experimental observations were found to
provide state-of-the-art accuracy models when supplemented by large
simulation datasets.

An important advantage over most state-of-the-art machine learn-
ing methods is that mGPfusion is able to predict the eﬀects of multiple
simultaneous mutations in addition to single point mutations. Our
experiments show that mGPfusion is reliable in predicting up to six
simultaneous mutations in our dataset. Furthermore, the Gaussian
process framework provide a way to estimate the (un)certainty of the
predictions even without a separate test set. We additionally proposed
a novel Bayesian scaling method to re-calibrate simulated protein sta-
bility values against experimental observations. This is a crucial part of
the mGPfusion model, and also alone improved protein-speciﬁc Rosetta
stability predictions by calibrating them using experimental data.

mGPfusion is best suited for a situation, where a protein is thor-
oughly experimented on and accurate predictions for stability eﬀects

7

upon mutations are needed. It takes some time to set up the frame-
work and train the model, but after that new predictions can be made
in fractions of a second. The most time-consuming part is running the
simulations with Rosetta, at least when the most accurate protocol 16
is used. Simulating all 19 possible point mutations for one position
took about 12 hours, but simulations for diﬀerent positions can be run
on parallel. The time needed for training the prediction model depends
on the amount of experimental and simulated training data. With no
simulated data, the training time ranged from few seconds to few min-
utes. With data fusion and a single kernel, the training time was under
an hour. With data fusion and MKL with 21 kernels, the training time
was from a few minutes to a day.

Acknowledgements

We acknowledge the computational resources provided by the Aalto
Science-IT.

Funding

This work has been supported by the Academy of Finland Center of Ex-
cellence in Systems Immunology and Physiology, the Academy of Fin-
land grants no. 260403 and 299915, and the Finnish Funding Agency
for Innovation Tekes (grant no 40128/14, Living Factories).
References

Alberts, B., Johnson, A., Lewis, J., Raﬀ, M., Roberts, K., and Walter, P. (2007). Molec-

ular biology of the cell. Garland Science, 5 edition.

Anslyn, E. V. and Dougherty, D. A. (2006). Modern physical organic chemistry. Uni-

versity Science Books.

Berman, H. M., Westbrook, J., Feng, Z., Gilliland, G., Bhat, T., Weissig, H., Shindyalov,
I. N., and Bourne, P. E. (2000). The protein data bank. Nucleic acids research, 28(1),
235–242.

Bommarius, A. S., Blum, J. K., and Abrahamson, M. J. (2011). Status of protein en-
gineering for biocatalysts: how to design an industrially useful biocatalyst. Current
opinion in chemical biology, 15(2), 194–200.

Branden, C. and Tooze, J. (1999). Introduction to protein structure. Garland, 2 edition.

Giollo, M., Martin, A. J., Walsh, I., Ferrari, C., and Tosatto, S. C. (2014). NeEMO: a
method using residue interaction networks to improve prediction of protein stability
upon mutation. BMC genomics, 15(4), 1.

Henikoﬀ, S. and Henikoﬀ, J. G. (1992). Amino acid substitution matrices from protein
blocks. Proceedings of the National Academy of Sciences, 89(22), 10915–10919.

Kawashima, S., Pokarowski, P., Pokarowska, M., Kolinski, A., Katayama, T., and Kane-
hisa, M. (2008). AAindex: amino acid index database, progress report 2008. Nucleic
acids research, 36(suppl 1), D202–D205.

Kellogg, E. H., Leaver-Fay, A., and Baker, D. (2011). Role of conformational sampling
in computing mutation-induced changes in protein structure and stability. Proteins:
Structure, Function, and Bioinformatics, 79(3), 830–838.

Kirk, O., Borchert, T. V., and Fuglsang, C. C. (2002). Industrial enzyme applications.

Current opinion in biotechnology, 13(4), 345–351.

Kumar, M. S., Bava, K. A., Gromiha, M. M., Prabakaran, P., Kitajima, K., Uedaira,
H., and Sarai, A. (2006). ProTherm and ProNIT: thermodynamic databases for pro-
teins and protein–nucleic acid interactions. Nucleic Acids Research, 34(suppl 1),
D204–D206.

Leaver-Fay, A., Tyka, M., Lewis, S. M., Lange, O. F., Thompson, J., Jacak, R., Kaufman,
K., Renfrew, P. D., Smith, C. A., Sheﬄer, W., et al. (2011). ROSETTA3: an object-
oriented software suite for the simulation and design of macromolecules. Methods in
enzymology, 487, 545.

Liu, J. and Kang, X. (2012). Grading amino acid properties increased accuracies of single

point mutation on protein stability prediction. BMC bioinformatics, 13(1), 1.

Menchetti, S., Costa, F., and Frasconi, P. (2005). Weighted decomposition kernels. In
Proceedings of the 22nd international conference on Machine learning, pages 585–
592. ACM.

Pace, C. N. and Scholtz, J. M. (1997). Measuring the conformational stability of a pro-

tein. Protein structure: A practical approach, 2, 299–321.

Pace, C. N. and Shaw, K. L. (2000). Linear extrapolation method of analyzing solvent
denaturation curves. Proteins: Structure, Function, and Bioinformatics, 41(S4),
1–7.

Pires, D. E., Ascher, D. B., and Blundell, T. L. (2014a). DUET: a server for predicting
eﬀects of mutations on protein stability using an integrated computational approach.
Nucleic acids research, page gku411.

Pires, D. E., Ascher, D. B., and Blundell, T. L. (2014b). mCSM: predicting the eﬀects of
mutations in proteins using graph-based signatures. Bioinformatics, 30(3), 335–342.

Potapov, V., Cohen, M., and Schreiber, G. (2009). Assessing computational methods for
predicting protein stability upon mutation: good on average but not in the details.
Protein Engineering Design and Selection, 22(9), 553–560.

Capriotti, E., Fariselli, P., and Casadio, R. (2005a). I-Mutant2.0: predicting stability
changes upon mutation from the protein sequence or structure. Nucleic acids research,
33(suppl 2), W306–W310.

Rapley, R. and Walker, J. M. (2000). Molecular Biology and Biotechnology. Royal

Society of Chemistry, 4 edition.

Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian processes for machine learn-

Capriotti, E., Fariselli, P., Calabrese, R., and Casadio, R. (2005b). Predicting pro-
tein stability changes from sequences using support vector machines. Bioinformatics,
21(suppl 2), ii54–ii58.

ing. The MIT Press.

Capriotti, E., Fariselli, P., Rossi, I., and Casadio, R. (2008). A three-state prediction of

single point mutations on protein stability changes. BMC bioinformatics, 9(2).

Chen, C.-W., Lin, J., and Chu, Y.-W. (2013). iStable: oﬀ-the-shelf predictor integration

for predicting protein stability changes. BMC bioinformatics, 14(2).

Cheng, J., Randall, A., and Baldi, P. (2006). Prediction of protein stability changes for
single-site mutations using support vector machines. Proteins: Structure, Function,
and Bioinformatics, 62(4), 1125–1132.

Cherry, J. R. and Fidantsef, A. L. (2003). Directed evolution of industrial enzymes: an

update. Current opinion in biotechnology, 14(4), 438–443.

Cichonska, A., Ravikumar, B., Parri, E., Timonen, S., Pahikkala, T., Airola, A., Wenner-
berg, K., Rousu, J., and Aittokallio, T. (2017). Computational-experimental approach
to drug-target interaction mapping: A case study on kinase inhibitors. PLoS compu-
tational biology, 13(8), e1005678.

Sanchez, S. and Demain, A. L. (2010). Enzymes and bioconversions of industrial, pharma-
ceutical, and biotechnological signiﬁcance. Organic Process Research & Development,
15(1), 224–230.

Schmidt, M. W., Berg, E., Friedlander, M. P., and Murphy, K. P. (2009). Optimizing
costly functions with simple constraints: A limited-memory projected quasi-newton
algorithm. In International Conference on Artiﬁcial Intelligence and Statistics, page
None.

Shawe-Taylor, J. and Cristianini, N. (2004). Kernel methods for pattern analysis. Cam-

bridge university press.

Tian, J., Wu, N., Chu, X., and Fan, Y. (2010). Predicting changes in protein thermosta-
bility brought about by single- or multi-site mutations. BMC bioinformatics, 11(1),
1.

Tokuriki, N. and Tawﬁk, D. S. (2009). Stability eﬀects of mutations and protein evolv-

ability. Current opinion in structural biology, 19(5), 596–604.

Dehouck, Y., Grosﬁls, A., Folch, B., Gilis, D., Bogaerts, P., and Rooman, M. (2009). Fast
and accurate predictions of protein stability changes upon mutations using statistical
potentials and neural networks: PoPMuSiC-2.0. Bioinformatics, 25(19), 2537–2543.

Tomii, K. and Kanehisa, M. (1996). Analysis of amino acid indices and mutation matrices
for sequence comparison and structure prediction of proteins. Protein Engineering,
Design and Selection, 9(1), 27–36.

Folkman, L., Stantic, B., and Sattar, A. (2014). Feature-based multiple models improve
classiﬁcation of mutation-induced stability changes. BMC genomics, 15(Suppl 4).

Vishwanathan, S. V. N., Schraudolph, N. N., Kondor, R., and Borgwardt, K. M. (2010).

Graph kernels. The Journal of Machine Learning Research, 11, 1201–1242.

Giguere, S., Marchand, M., Laviolette, F., Drouin, A., and Corbeil, J. (2013). Learn-
ing a peptide-protein binding aﬃnity predictor with kernel ridge regression. BMC
bioinformatics, 14(1), 82.

Wainreb, G., Wolf, L., Ashkenazy, H., Dehouck, Y., and Ben-Tal, N. (2011). Protein
stability: a single recorded mutation aids in predicting the eﬀects of other mutations
in the same amino acid site. Bioinformatics, 27(23), 3286–3292.

8

Supplementary material

Figure S5: The stability of a protein is determined by the thermodynamic and kinetic stabilities, ∆Gt and ∆Gk, respectively. We only consider the
thermodynamic stability.

Figure S6: Priors presented by Equation 2. Here µd = −1.5, the most likely value for −a. Other hyperparameter values are presented in Table S4.

Table S4: Values for the hyperparameters used in the priors of a, b, c, d, σE and σS presented in Equations 2 and 6, respectively.

a

b

c

d

σE

σS

αa = 2
βa = 1.5

αb = 1.3
βb = 2

αc = 2
βc = 5

µd = −a
σd = 0.15

αE = 2.5
βE = 0.02

αS = 50
βS = 0.007

9

The partial derivatives of the marginal likelihood with respect to the parameters φ are obtained from Equation (9) as follows:

where α = K −1

φ y, Kφ is determined as

and the partial derivatives of Kφ with respect to the optimised parameters are

∂
∂φj

log p(y|X, φ) =

yT K −1

∂Kφ
∂φj

K −1

φ y −

(cid:19)

K −1
φ

∂Kφ
∂φj

1
2
1
2

φ
(cid:18)(cid:16)

=

tr

ααT − K −1

φ

(cid:18)

(cid:19)

,

tr

1
2
(cid:17) ∂Kφ
∂θj

Kφ =

wmK γm

m + diag





σ0
σE1NE
σE1NE + σS1NS + tσT


2



M
(cid:88)

m=1

∂Kφ
∂σE

∂Kφ
∂σR

∂Kφ
∂t

∂Kφ
∂wm
∂Kφ
∂γm







0
2σE1NE
2 (σE1NE + σS1NS + tσT )
0
0NE
2 (σE1NE + σS1NS + tσT )
0
0NE
2 (σE1NE + σS1NS + t) σT













=diag



=diag



=diag



=K γm
m

=wmK γm

m log Km

(cid:80)N∗

i=1(yi − ¯y)(µ(xi) − ¯µ)

i=1(yi − ¯y)2 (cid:80)N∗

i=1(µ(xi) − ¯µ)2

ρ =

(cid:113)(cid:80)N∗
(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
N∗

N∗(cid:88)

i=1

rmse =

(yi − µ(xi))2,

Correlation ρ and root-mean-square error rmse for the predictions are determined as

(S11)

(S12)

(S13)

(S14)

(S15)

(S16)

(S17)

(S18)

where ¯y is the mean of the experimentally measured values, µ(xi) is prediction mean, ¯µ is the average of all prediction means, and N∗ is the
number of predictions.

10

Figure S7: Mutation-level predictions for all 15 proteins presented in Table 1. The predictions are coloured by the number of simultaneous mutations.

11

Figure S8: Position-level predictions for all 15 proteins. When the eﬀects of a mutant are predicted multiple times, they are connected by a line.

12

Table S5: (Continues on the next page)Comparison of diﬀerent methods on the 15 protein dataset with respect to ρ and rmse. Oﬀ-the-shelf implementations
of Rosetta, mCSM and PoPMuSiC are used directly without cross-validation.

Protein Method

2LZM

1BNI

1VQB

1LZ1

2CI2

1PGA

2RN2

1CSP

mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC

0.58
0.59
-
-
-

0.75
0.73
0.75
0.71
0.57

0.49
0.50
-
-
-

0.62
0.62
-
-
-

0.67
0.65
0.79
0.79
0.47

0.77
0.74
0.81
0.61
0.59

Point mutations
cross-validation level
prot.
pos.
mut.
0.75
0.80
0.87
0.75
0.77
0.86
-
0.59
0.86
-
0.37
0.75
0.73
-
0.74
0.75
0.57
0.71
0.64
0.61
0.65
0.48
0.58
0.62
0.60
0.66
0.50
0.53
0.12
0.29
0.46
0.49
0.53
0.51
0.59
0.56
0.39
-0.31
0.55
0.59
0.67
0.64
0.72
0.67
0.61
0.74
0.60
0.63
0.74
0.75
0.47
0.59
0.61
-0.46
0.59
0.69
-0.10
0.28
0.58
0.59
0.12
0.09
0.64
0.70
0.71
0.71
0.23
0.22
-0.06
-0.37
0.20
0.33
0.42
0.48

0.85
0.86
0.88
0.87
0.23

0.33
0.34
-
-
-

0.79
0.82
0.77
0.83
0.66

0.68
0.82
0.62
0.57
0.69

0.71
0.71
-
-
-

0.69
0.71
-
-
-

0.64
0.63
-
-
-

0.73
0.69
0.65
0.51
0.60

0.75
0.73
-
-
-

0.39
0.40
-
-
-

0.93
0.91
0.96
0.97
0.71

0.86
0.84
0.86
0.87
0.17

-1.00
-1.00
-
-
-

1.00
1.00
1.00
1.00
-1.00

Correlation ρ
Multiple mutations
cross-validation level
prot.
pos.
mut.
0.64
0.83
0.96
0.64
0.87
0.96
-
0.85
0.97
-
0.77
0.94
0.66
-
0.68
0.68
-
-
0.70
0.79
0.82
0.85
0.29
0.18
-
-
0.83
0.82
0.70
0.75
0.68
0.73
-
-
0.11
0.05
0.56
0.42
-1.00
-1.00
-
-
0.87
0.86
0.79
0.71
0.61
0.62
-
-
0.35
0.60
-0.24
-0.08
0.07
0.03
-
-
0.60
0.60
0.42
0.42
0.50
0.47
-
-
0.73
0.69
0.75
0.71
0.69
0.68
-
-

0.92
0.91
0.94
0.92
0.68

0.78
0.76
0.75
0.77
0.48

0.90
0.76
0.93
0.92
0.09

0.48
0.59
-
-
-

0.32
0.62
-
-
-

0.61
0.62
-
-
-

0.95
0.92
0.92
0.92
0.61

0.85
0.86
-
-
-

0.77
0.76
0.78
0.74
0.53

0.76
0.75
0.85
0.85
0.57

0.77
0.74
0.80
0.63
0.53

0.52
0.53
-
-
-

0.60
0.58
-
-
-

0.57
0.57
-
-
-

All mutations
cross-validation level
prot.
pos.
mut.
0.68
0.76
0.90
0.69
0.82
0.90
-
0.72
0.90
-
0.61
0.82
0.65
-
0.70
0.71
-
-
0.55
0.60
0.63
0.41
0.45
0.56
-
-
0.69
0.69
0.50
0.55
0.59
0.59
-
-
0.57
0.56
0.47
0.21
0.46
0.55
-
-
0.81
0.79
0.72
0.63
0.63
0.65
-
-
0.43
0.53
-0.14
0.06
0.11
0.28
-
-
0.53
0.53
0.22
0.23
0.57
0.65
-
-
0.75
0.72
0.77
0.72
0.64
0.60
-
-

0.38
0.54
-
-
-

0.50
0.70
-
-
-

0.70
0.70
-
-
-

0.92
0.91
0.94
0.92
0.59

0.85
0.81
0.84
0.81
0.24

0.79
0.81
0.77
0.82
0.62

0.71
0.72
-
-
-

0.82
0.79
0.76
0.68
0.63

0.83
0.86
0.81
0.91
0.99

1.06
1.06
-
-
-

2.25
2.25
-
-
-

1.71
1.75
1.41
1.50
1.99

1.69
1.69
-
-
-

1.21
1.27
1.08
1.61
1.58

mut.
0.82
0.84
0.82
1.12
1.05

Point mutations
cross-validation level
prot.
pos.
1.12
1.02
1.12
1.08
-
1.34
-
1.93
1.06
-
1.13
1.27
1.11
1.37
1.45
1.40
2.32
1.58
1.70
1.62
1.53
1.94
1.94
2.41
2.89
2.00
2.26
2.24
2.29
0.99
1.03
1.15
1.43
1.01
1.04
0.97
0.95
0.90
0.97
1.02
1.39
1.00
1.09
0.86
0.85
1.54
1.22
1.58
1.73
1.42
1.70
1.94
1.89
1.21
1.22
1.45
1.45
1.13
1.07
1.04
1.16
1.04
1.04
1.07
1.12
1.06
1.11
1.02
0.99

0.64
0.65
0.54
0.60
1.04

1.10
1.10
-
-
-

1.64
1.06
-
-
-

1.26
0.87
1.40
1.53
1.21

1.05
1.59
-
-
-

0.91
0.87
0.93
0.82
1.09

0.85
0.91
0.95
1.16
1.00

1.07
1.07
-
-
-

0.56
1.10
0.13
0.27
3.23

1.15
1.35
0.70
0.63
1.90

1.28
1.24
1.32
1.15
2.51

3.75
3.51
-
-
-

2.06
2.43
-
-
-

2.17
2.14
-
-
-

mut.
0.57
0.59
0.48
0.78
1.65

rmse
Multiple mutations
cross-validation level
prot.
pos.
2.14
1.53
2.11
1.16
-
1.24
-
1.44
1.84
-
1.93
-
-
2.49
2.03
2.00
1.67
2.90
2.33
-
-
1.62
1.82
1.97
2.30
1.96
2.06
-
-
2.40
2.41
1.65
2.36
3.25
3.41
-
-
0.80
1.01
1.01
1.40
1.27
1.30
-
-
2.09
0.95
3.01
2.07
3.09
3.51
-
-
1.01
0.95
1.21
1.20
1.08
1.25
-
-
1.66
1.87
1.51
1.59
2.29
1.92
-
-

2.58
2.13
-
-
-

2.74
0.88
-
-
-

0.91
1.18
-
-
-

1.21
1.12
-
-
-

0.91
0.96
0.76
0.86
2.19

0.53
0.69
0.45
0.47
2.81

0.67
0.69
0.74
0.68
1.20

0.55
0.71
0.66
0.71
1.27

All mutations
cross-validation level
prot.
1.43
1.42
-
-
-

mut.
0.76
0.79
0.75
1.05
1.23

pos.
1.30
1.13
1.29
1.70
1.51

1.22
1.27
1.11
1.57
1.70

1.59
1.66
1.27
1.33
1.97

0.83
0.87
0.80
0.90
1.07

0.80
0.87
0.90
1.08
1.06

0.88
0.83
0.94
1.02
2.33

0.86
0.83
0.89
0.80
1.12

0.75
0.78
0.63
0.71
1.58

1.37
-
-
1.67
1.60
1.55
2.20
1.94

1.77
-
-
1.82
1.89
2.24
2.66
1.99

2.21
-
-
1.07
1.10
1.17
1.47
1.15

1.12
-
-
0.86
0.99
1.02
1.39
1.11

1.13
-
-
2.00
1.13
2.81
2.01
2.87

2.95
-
-
1.14
1.13
1.36
1.36
1.11

1.11
-
-
1.45
1.60
1.36
1.43
1.92

1.47
-
-

1.75
1.75
-
-
-

2.20
2.30
-
-
-

1.16
1.14
-
-
-

1.10
1.08
-
-
-

2.38
1.02
-
-
-

1.02
1.51
-
-
-

1.80
1.57
-
-
-

13

Table S5: (Continued) Comparison of diﬀerent methods on the 15 protein dataset with respect to ρ and rmse. Oﬀ-the-shelf implementations of Rosetta,
mCSM and PoPMuSiC are used directly without cross-validation.

Protein Method

1BVC

4LYZ

1RTB

1PIN∗

1RN1

1RGG

1BPI

total

mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC

0.61
0.59
0.65
0.62
0.29

0.34
0.34
-
-
-

0.92
0.89
0.92
0.91
0.65

0.64
0.59
0.61
0.23
0.50

0.70
0.70
-
-
-

0.53
0.53
-
-
-

Point mutations
cross-validation level
prot.
pos.
mut.
0.48
0.43
0.41
0.48
0.48
0.50
-
-0.12
-0.05
-
0.06
-0.05
-
0.40
0.42
0.47
0.47
0.60
0.27
0.30
-0.05
0.27
0.28
0.33
0.55
0.59
0.81
0.79
0.69
0.58
0.61
0.69
0.68
0.72
0.49
0.51
0.40
0.25
0.49
0.53
0.72
0.60
0.62
0.58
0.04
0.27
0.57
0.67
0.76
0.62
0.51
0.52
-0.61
-0.53
0.43
0.60
0.77
0.65
0.67
0.65
0.57
0.58
0.52
0.51
0.71
0.72
0.70
0.69
0.51
0.34
0.63
0.55
0.61
0.64

0.67
0.67
-
-
-

0.83
0.81
0.74
0.77
0.63

0.69
0.69
0.71
0.72
0.58

0.68
0.64
0.56
0.43
0.52

0.61
0.61
-
-
-

0.51
0.51
-
-
-

0.81
0.79
0.81
0.76
0.65

0.56
0.56
-
-
-

Correlation ρ
Multiple mutations
cross-validation level
prot.
pos.
mut.
-0.63
-0.25
-0.28
-0.66
-0.25
-0.25
-
-0.21
-0.10
-
-0.21
-0.11
-
-0.57
-0.63
-0.65
-
-
0.57
0.64
0.46
0.47
0.68
0.71
-
-
0.74
0.73
0.30
0.58
0.73
0.73
-
-
-

0.78
0.73
0.78
0.48
0.73

0.95
0.96
0.96
0.97
0.70

0.75
0.67
-
-
-

0.66
0.64
-
-
-

-
-
-
-
-

-0.19
-0.01
-
-
-

0.96
0.93
-
-
-

-0.06
0.21
-
-
-

0.49
0.50
-
-
-

-
-
-
-
-

0.97
0.96
0.99
0.99
0.21

0.42
0.22
0.19
0.08
0.78

-0.39
-0.53
-0.29
-0.55
0.23

0.88
0.86
0.86
0.86
0.51

-

-

-
-

-
-
-
0.38
0.42
0.62
0.50
0.18
0.20
-
-
0.66
0.30
0.08
0.07
0.86
0.77
-
-
-0.02
0.33
0.52
0.46
-0.00
-0.00
-
-
0.61
0.64
0.52
0.55
0.39
0.40
-
-

0.65
0.64
0.68
0.64
0.33

0.86
0.86
0.76
0.85
0.67

0.64
0.59
0.61
0.23
0.50

0.35
0.35
-
-
-

0.68
0.69
-
-
-

0.53
0.53
-
-
-

All mutations
cross-validation level
prot.
pos.
mut.
0.09
-0.09
0.08
0.14
-0.07
0.14
-
-0.23
-0.13
-
-0.20
-0.14
-
-0.09
0.09
0.14
-
-
0.34
0.40
0.18
0.21
0.34
0.35
-
-
0.71
0.75
0.18
0.16
0.65
0.70
-
-
0.49
0.51
0.40
0.25
0.49
0.53
0.72
0.60
0.55
0.51
0.04
0.10
0.54
0.65
-
-
0.52
0.33
-0.05
-0.00
0.51
0.39
-
-
-0.13
0.05
0.84
0.70
-0.08
0.13
-
-
0.64
0.66
0.50
0.49
0.48
0.49
-
-

0.12
0.12
-
-
-

0.65
0.66
-
-
-

0.73
0.55
-
-
-

0.52
0.52
-
-
-

0.84
0.81
0.75
0.78
0.61

0.52
0.40
0.35
0.30
0.42

0.85
0.81
0.83
0.76
0.08

0.83
0.82
0.83
0.80
0.60

3.46
3.50
-
-
-

1.47
1.48
1.38
1.48
2.26

0.47
0.50
0.49
0.70
0.54

2.44
2.44
-
-
-

1.25
1.48
1.26
1.67
1.99

1.00
1.00
-
-
-

mut.
0.74
0.70
0.99
0.99
0.75

Point mutations
cross-validation level
prot.
pos.
1.65
0.72
1.65
0.71
-
1.00
-
0.99
-
0.76
1.67
1.00
0.85
1.65
1.64
1.78
1.84
2.22
3.61
1.43
1.45
1.73
1.85
2.62
3.37
2.08
2.45
2.33
2.22
0.53
0.54
0.58
0.76
0.54
0.99
0.59
0.70
1.17
1.24
1.75
1.78
1.18
1.20
0.97
1.14
1.70
1.68
2.24
2.01
1.76
1.58
1.36
1.54
1.28
1.32
1.43
1.73
1.52
1.80
1.26
1.31
1.26
1.30
1.54
1.95
1.38
1.63
1.40
1.37

0.85
0.92
1.06
1.00
1.12

1.18
1.18
-
-
-

1.56
1.56
-
-
-

1.41
1.42
1.38
1.53
1.57

1.27
1.32
1.43
1.68
1.52

1.83
1.82
-
-
-

1.07
1.11
1.04
1.26
1.35

1.61
1.62
-
-
-

0.69
0.57
0.36
0.31
1.22

2.35
2.17
-
-
-

mut.
1.64
1.62
1.30
1.29
1.65

rmse
Multiple mutations
cross-validation level
prot.
pos.
2.70
1.48
2.43
1.47
-
1.35
-
1.33
-
1.45
2.45
-
-
1.02
0.93
1.15
1.16
1.29
2.05
-
-
2.24
1.81
3.39
2.11
1.62
2.01
-
-
-
-
-
-
-

1.71
1.90
-
-
-

2.22
1.82
3.44
2.12
1.63

-
-
-
-
-

-
-
-
-
-

0.19
0.35
0.12
0.13
0.98

5.67
6.21
6.81
6.89
7.08

2.17
2.86
1.72
2.52
7.43

1.33
1.43
1.44
1.45
2.49

-
-
0.91
0.77
0.58
0.82
1.01
1.24
-
-
5.95
6.14
6.88
6.77
7.24
6.90
-
-
7.66
7.07
4.19
6.58
7.43
7.68
-
-
2.45
2.40
2.65
2.56
2.99
2.74
-
-

1.28
0.99
-
-
-

4.20
5.43
-
-
-

7.69
7.70
-
-
-

2.53
2.50
-
-
-

All mutations
cross-validation level
prot.
2.02
1.92
-
-
-

mut.
1.09
1.06
1.09
1.09
1.10

pos.
1.21
1.20
1.21
1.20
1.20

1.35
1.35
1.24
1.33
2.09

1.40
1.53
1.69
1.73
1.95

0.47
0.50
0.49
0.70
0.54

0.81
0.88
1.01
0.95
1.10

2.65
2.85
3.05
3.14
3.23

1.40
1.57
1.47
1.80
2.88

1.13
1.18
1.14
1.30
1.66

1.94
-
-
1.45
1.43
1.59
1.63
1.94

3.35
-
-
1.85
1.84
2.81
3.14
1.99

2.40
-
-
0.53
0.54
0.58
0.76
0.54

0.99
0.59
0.70

1.13
1.17
1.61
1.66
1.15

1.20
-
-
4.75
4.90
5.53
5.41
5.74

3.16
-
-
3.64
3.40
2.28
3.34
3.62

3.09
-
-
1.87
1.85
2.09
2.23
2.22

1.92
-
-

3.26
3.27
-
-
-

2.36
2.38
-
-
-

1.00
1.00
-
-
-

1.19
1.17
-
-
-

2.23
2.64
-
-
-

3.11
3.11
-
-
-

1.84
1.84
-
-
-

∗ dataset for 1PIN contained no multiple mutations.

14

Table S6: Comparison of diﬀerent methods on the 15 protein dataset with respect to ρ and rmse after removing 10% of predictions with largest errors.
Mutation, position, and protein are referred to as mut., pos., and prot., respectively. Oﬀ-the-shelf implementations of Rosetta, mCSM and PoPMuSiC are
used directly without cross-validation.

Results after 10 %
outlier removal

Method
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled

Point mutations
cross-validation level
prot.
pos.
0.75
0.77
0.75
0.73
-
0.57
-
0.27
-
0.78

mut.
0.87
0.86
0.89
0.86
0.80

Correlation ρ
Multiple mutations
cross-validation level
prot.
pos.
0.63
0.84
0.65
0.84
-
0.76
-
0.76
-
0.71

mut.
0.97
0.96
0.98
0.97
0.73

Oﬀ-the-shelf implementations with no cross-validation

All mutations
cross-validation level
prot.
pos.
0.71
0.81
0.72
0.80
-
0.74
-
0.66
-
0.75

mut.
0.92
0.91
0.93
0.91
0.78

Point mutations
cross-validation level
prot.
pos.
0.93
0.85
0.93
0.85
-
1.01
-
1.30
-
0.87

mut.
0.69
0.69
0.65
0.73
0.84

rmse
Multiple mutations
cross-validation level
prot.
pos.
1.90
1.55
1.85
1.43
-
1.66
-
1.48
-
1.99

mut.
0.62
0.64
0.49
0.54
1.67

All mutations
cross-validation level
prot.
1.13
1.12
-
-
-

mut.
0.67
0.67
0.62
0.69
1.00

pos.
1.15
1.10
1.23
1.37
1.31

Rosetta
mCSM
PoPMuSiC

0.75
0.71
0.73

0.67
-
-

0.73
-
-

0.94
0.89
0.86

1.85
-
-

1.11
-
-

Figure S9: rmse with diﬀerent amount of predictions, when predictions are sorted by the error. Position level cross-validation was used for mGPfusion,
mGP and Rosetta scaled.

15

Figure S10: Learning curves.

16

Figure S11: Learning curves.

17

Figure S12: Learning curves.

18

Figure S13: Transformations for all 15 proteins presented in Table 1. The red circles mark the simulated ∆∆G-values yS with respect to the experimental
measured ∆∆G-values yE. Thin black lines show possible transformations for yS, whereas the thick black line shows the selected transformation from yS
to ˜yS.

19

mGPfusion: Predicting protein stability changes with
Gaussian process kernel learning and data fusion

Emmi Jokinen1, Markus Heinonen1,2 and Harri L¨ahdesm¨aki1
1Department of Computer Science, Aalto University, 02150 Espoo, Finland
2Helsinki Institute for Information Technology, Finland

Abstract

Motivation: Proteins are commonly used by biochemical industry for numerous processes. Reﬁning these proteins’ properties via
mutations causes stability eﬀects as well. Accurate computational method to predict how mutations aﬀect protein stability are necessary
to facilitate eﬃcient protein design. However, accuracy of predictive models is ultimately constrained by the limited availability of
experimental data.
Results: We have developed mGPfusion, a novel Gaussian process (GP) method for predicting protein’s stability changes upon single
and multiple mutations. This method complements the limited experimental data with large amounts of molecular simulation data.
We introduce a Bayesian data fusion model that re-calibrates the experimental and in silico data sources and then learns a predictive
GP model from the combined data. Our protein-speciﬁc model requires experimental data only regarding the protein of interest and
performs well even with few experimental measurements. The mGPfusion models proteins by contact maps and infers the stability
eﬀects caused by mutations with a mixture of graph kernels. Our results show that mGPfusion outperforms state-of-the-art methods in
predicting protein stability on a dataset of 15 diﬀerent proteins and that incorporating molecular simulation data improves the model
learning and prediction accuracy.
Availability: Software implementation and datasets are available at github.com/emmijokinen/mgpfusion
Contact: emmi.jokinen@aalto.ﬁ

8
1
0
2
 
r
a

M
 
3
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
2
5
8
2
0
.
2
0
8
1
:
v
i
X
r
a

1 Introduction

Proteins are used in various applications by pharmaceutical, food, fuel,
and many other industries and their usage is growing steadily (Kirk
et al., 2002; Sanchez and Demain, 2010). Proteins have important ad-
vantages over chemical catalysts, as they are derived from renewable
resources, are biodegradable and are often highly selective (Cherry and
Fidantsef, 2003). Protein engineering is used to further improve the
properties of proteins, for example to enhance their catalytic activity,
modify their substrate speciﬁcity or to improve their thermostability
(Rapley and Walker, 2000). Increasing the stability is an important as-
pect of protein engineering, as the proteins used in industry should be
stable in the industrial process conditions, which often involve higher
than ambient temperature and non-aqueous solvents (Bommarius et al.,
2011). The properties of a protein are modiﬁed by introducing alter-
ations to its amino acid sequence. Mutations in general tend to be
destabilising, and if too many destabilising mutations are implemented,
the protein may not remain functional without compensatory stabilis-
ing mutations (Tokuriki and Tawﬁk, 2009).

The stability of a protein can be deﬁned as the diﬀerence in Gibbs
energy ∆G between the folded and unfolded (or native and denatu-
rated) state of the protein. More precisely, the Gibbs energy diﬀerence
determines the thermodynamic stability ∆Gt of the protein, as it does
not take into account the kinetic stability ∆Gk which determines the
energy needed for the transition between the folded and unfolded states
(Anslyn and Dougherty, 2006) (see Supplementary Figure S1). Here we
will consider only the thermodynamic stability and from now on it will
be referred to merely as stability ∆G.

The eﬀect of mutations can be deﬁned by the change they cause
to the Gibbs energy ∆G, denoted as ∆∆G (Pace and Scholtz, 1997).
To comprehend the signiﬁcance of stability changes upon mutations,
we can consider globular proteins, the most common type of enzymes,
whose polypeptide chain is folded up in a compact ball-like shape with
an irregular surface (Alberts et al., 2007). These proteins are only
marginally stable and the diﬀerence in Gibbs energy between the folded
and unfolded state is only about 5–15 kcal/mol, which is not much more
than the energy of a single hydrogen bond that is about 2–5 kcal/mol
(Branden and Tooze, 1999). Therefore, even one mutation that breaks
a hydrogen bond can prevent a protein from folding properly.

The protein stability can be measured with many techniques, in-
cluding thermal, urea and guanidinium chloride (GdmCl) denaturation

curves that are determined as the fraction of unfolded proteins at dif-
ferent temperatures or at diﬀerent concentrations of urea or GdmCl
(Pace and Shaw, 2000). Some of the experimentally measured sta-
bility changes upon mutations have been gathered in thermodynamic
databases such as Protherm (Kumar et al., 2006).

A variety of computational methods have been introduced to predict
the stability changes upon mutations more eﬀortlessly than through ex-
perimental measurements. These methods utilise physics or knowledge-
based potentials (Leaver-Fay et al., 2011), their combinations, or diﬀer-
ent machine learning methods. The machine learning methods utilise
support vector machines (SVM) (Capriotti et al., 2005b, 2008; Chen
et al., 2013; Cheng et al., 2006; Folkman et al., 2014; Liu and Kang,
2012; Pires et al., 2014a), random forests (Tian et al., 2010; Wainreb
et al., 2011), neural networks (Dehouck et al., 2009; Giollo et al., 2014),
and Gaussian processes (Pires et al., 2014b). However, it has been as-
sessed that although on average many of these methods provide good
results, they tend to fail on details (Potapov et al., 2009). In addition,
many of these methods are able to predict the stability eﬀects only for
single-point mutations.

We introduce mGPfusion (mutation Gaussian Processes with data
fusion), a method for predicting stability eﬀects of both point and mul-
tiple mutations. mGPfusion is a protein-speciﬁc model – in contrast to
earlier stability predictors that aim to estimate arbitrary protein struc-
ture or sequence stabilities – and achieves markedly higher accuracy
while utilising data only from a single protein at a time. In contrast
to earlier works that only use experimental data to train their models,
we also combine exhaustive Rosetta (Leaver-Fay et al., 2011) simulated
point mutation in silico stabilities to our training data.

A key part of mGPfusion is the automatic scaling of simulated data
to better match the experimental data distribution based on those vari-
ants that have both experimental and simulated stability values. Fur-
thermore, we estimate a variance resulting from the scaling, which
places a higher uncertainty on very destabilising simulations. Our
Gaussian process model then utilises the joint dataset with their es-
timated heteroscedastic variances and uses a mixture of graph kernels
to assess the stability eﬀects caused by changes in amino acid sequence
according to 21 substitution models. Our experiments on a novel 15
protein dataset show a state-of-the-art stability prediction performance,
which is also sustained when there is access only to a very few experi-
mental stability measurements.

1

Figure 1: Pipeline illustration for mGPfusion. a) M = 21 substitution matrices utilise diﬀerent information sources and give scores to pairwise amino
acid substitutions. b) The wild-type structures from Protein Data Bank are modelled as contact graphs. c) The graph kernel measures similarity of two
sequences by a substitution model S over all positions p and their neighbourhoods nbs(p) in the contact graph. d) Each substitution matrix is used to
create a separate covariance matrix. e) Multiple kernel learning (MKL) is used for ﬁnding the optimal combination of the base kernels. The kernel matrix
measures variant similarities. f ) Experimentally measured ∆∆G values yE are gathered from Protherm and Rosetta’s ddg monomer application is used to
simulate the stability eﬀects yS for all single point mutations. g) Bayesian scaling for the simulated values yS at the x-axis. Possible scalings are coloured
with green and the chosen scaling from yS into scaled values ˜yS is marked by black dots. The scaling is ﬁtted to a subset of experimentally measured
stabilities yE (circles). h) The stability predictive GP model is trained using experimental and simulated data through the kernel matrix.

2 Methods

Following Pires et al. (2014b) we choose a Bayesian model family of
Gaussian processes for prediction of mutation eﬀects on protein sta-
bility due to its inherent ability to handle uncertainty in a principle
way. Bayesian modelling is a natural approach for combining the ex-
perimental and simulated data distribution, while it is also suitable for
learning the underlying mixture of substitution models that governs the
mutational process.

The pipeline for mGPfusion is presented in Figure 1. The ﬁrst
part of mGPfusion consists of collection of in silico and experimental
datasets discussed in Section 2.1, the scaling of the in silico dataset in
Section 2.2 and the fusion of these two datasets in Section 2.3. The sec-
ond part consists of the Gaussian process model described in Section 2.4
with detailed description of the graph kernels in Sections 2.5-2.6 and
model inference in Section 2.7. Finally, the evaluation criteria used are
described in Section 2.8.

teins that fulﬁlled these requirements are listed in Table 1. We aver-
aged the stability values for proteins with multiple measurements and
ignored mutations to residues not present in their 3D structures. These
data sets are available at github.com/emmijokinen/mgpfusion.

Protein (organism)

PDB

mutations
point

point (sim)

all

2LZM 349
T4 Lysozyme (Enterobacteria phage T4)
182
1BNI
Barnase (Bacillus amyloliquefaciens)
124
1VQB
Gene V protein (Escherichia virus m13)
116
1LZI
Glycosyltransferase A (Homo sapiens)
98
2CI2
Chymotrypsin inhibitor 2 (Hordeum vulgare)
89
1PGA
Protein G (Streptococcus sp. gx7805)
83
2RN2
Ribonuclease H (Escheria coli)
80
1CSP
Cold shock protein B (Bacillus subtilis)
80
1BVC
Apomyoglobin (Physeter catodon)
63
4LYZ
Hen egg white lysozyme (Gallus gallus)
57
Ribonuclease A (Bos taurus)
1RTB
56
Peptidyl-prolyl cis-trans isomerase (Homo sapiens) 1PIN
53
1RN1
Ribonuclease T1 isozyme (Aspergillus oryzae)
54
1RGG
Ribonuclease (Streptomyces auerofaciens)
53
1BPI
Bovine pancreatic trypsin inhibitor (Bos taurus)

264
163
92
114
77
34
65
50
56
50
50
56
48
45
47

3116
2052
1634
2470
1235
1064
2945
1273
2907
2451
2356
2907
1957
1824
1102

2.1 Experimental and in silico data

total

1537

1211

31293

Protherm is a database of numerical thermodynamic parameters for
proteins and their mutants (Kumar et al., 2006). From Protherm we
gathered all proteins with at least 50 unique mutations whose ∆∆G
has been measured by thermal denaturation, and where a PDB code
for a 3D structure of the protein was reported. We required the pro-
teins to have at least 50 unique mutations, so that we would have a
representative test set and get suﬃciently reliable estimates of predic-
tion accuracy on individual proteins and examine how the amount of
experimental training data aﬀects the accuracy of the model. The 3D
structures are necessary for obtaining the connections between residues.
We collected the 3D structures with the reported PDB codes from the
Protein Databank, www.rcsb.org (Berman et al., 2000). The 15 pro-

Table 1: The 15 protein data from ProTherm database with counts of point
mutations, all mutations, and of simulated point mutation stability changes.

We also generate simulated data of the stability eﬀects of all possible
single mutations of the proteins. Our method can utilise any simulated
stability values. We used the “ddG monomer” application of Rosetta
3.6 (Leaver-Fay et al., 2011) using the high-resolution backrub-based
protocol 16 recommended in Kellogg et al. (2011). The predictions yS
made with Rosetta are given in Rosetta Energy Units (REU). Kellogg
et al. (2011) suggest transformation 0.57yS for converting the predic-
tions into physical units. The simulated data scaled this way is not
as accurate as the experimental data, the correlation and root mean

2

square error (rmse) with respect to the experimental data are shown
for all proteins in Table 2 and for individual proteins in Supplementary
Table S2, on rows labelled Rosetta. For this reason, we use instead a
Bayesian scaling described in the next section and diﬀerent noise models
for the experimental and simulated data, described in Section 2.3.

For each of the 15 proteins,

let xi = (xi1, . . . , xiM ) denote its
M -length variant i with positions p labelled with residues xip ∈
{A, R, N, . . . , V }. We denote the wild-type protein as x0. We collect
15 separate sets of simulated and experimental data. We denote the
NE )T with
NE experimental variants of each protein as XE = (xE
the corresponding experimental stability values yE = (yE
NE )T ∈
RNE . Similarly, we denote the NS simulated observations as XS =
(xS

1 , . . . , xE

1 , . . . , yE

NS )T and yS = (yS

NS )T ∈ RNS .

1 , . . . , xS

1 , . . . , yS

2.2 Bayesian scaling of in silico data

The described transformation from REU to physical units may not
be optimal for all proteins. We therefore applied instead a linear-
exponential scaling function to obtain scaled Rosetta simulated sta-
bilities ˜yS,

˜yS = g(yS | θj) = ajecj yS

+ bjyS + dj.

(1)

This scaling transforms the Rosetta simulations yS for each protein
j = 1, . . . , 15 to correspond better to the experimental data. The pa-
rameters θj = (aj, bj, cj, dj) deﬁne the weight aj and steepness cj of the
exponential term, while the linear term has slope bj and intercept dj.
To avoid overﬁtting, we perform Bayesian linear regression and start
by deﬁning parameter prior p(θj) = p(aj)p(bj)p(cj)p(dj) that reﬂects
our beliefs about realistic scalings having only moderate steepness:

and stabilities y = (y0, yE, ˜yS) of size RN where N = 1 + NE + NS is
the total number of simulated and experimental data points, including
the wild-type. We assume heteroscedastic additive noise models for the
three information sources

y0 = f (x0) + ε0,
(cid:1) + εE
i = f (cid:0)xE
yE
i ,
(cid:1) + εS
i = f (cid:0)xS
˜yS
i ,

i

i

(cid:1)

(cid:1)

0

ε0 ∼ N (cid:0)0, σ2
i ∼ N (cid:0)0, σ2
εE
(cid:16)
εS
i ∼ N

E

0, (σE + σS + tσT (i))2(cid:17)

,

(5)

where the observed values are noisy versions of the underlying ‘true’
stability function f (x) corrupted by zero-mean noise with data source
speciﬁc variances. We learn a Gaussian process based stability function
f (x) in the next Section.

The Equations (5) encode that the experimental data are corrupted
by a global experimental noise variance σ2
E. The simulated stabilities
are additionally corrupted by a global Rosetta simulator error variance
σ2
S, and by the value-dependent transformation variance tσ2
T (i) scaled
by parameter t. The model then encapsulates that we trust the Rosetta
data less than the experimental data. By deﬁnition, the ∆∆G of the
wild-type is zero (y0 = 0) with very small assumed error, σ0 = 10−6.
Note that σ2
T are ﬁxed by equation (4), while we infer the optimal
values for the remaining three free parameters (σE, σR, t) (See Section
2.4). The parameters σ2

S are assigned priors

E and σ2

σE ∼ Gamma(σE|αE, βE)
σS ∼ Gamma(σR|αS, βS).

(6)

The values of these hyperparameters are shown in Supplementary Ta-
ble S1.

p(aj) = Gamma(aj | αa, βa)
p(bj) = Beta(1/2 · bj | αb, βb)
p(cj) = Beta(10/3 · cj | αc, βc)
p(dj) = N (dj | µd, σ2

d).

2.4 Gaussian processes

(2)

We use a Gaussian process (GP) function f to predict the stability
f (x) ∈ R of a protein variant x. Gaussian processes are a family of
non-parametric, non-linear Bayesian models (Rasmussen and Williams,
2006). A zero-mean GP prior

The empirically selected hyperparameter values are listed in Supple-
mentary Table S1 and the priors are illustrated in Figure S2.

We compute the posterior for θj using the subset of simulated data

that have corresponding experimentally measured data:

p(θj|yE, yS) ∝

(cid:89)

N (cid:0)yE

i

| g(yS

i |θj), σ2
n

(cid:1) p(θj).

i:xi∈XE ∩XS

The product iterates over all NE∩S simulated ∆∆G’s that have a
matching experimentally observed value. The σ2
n is the scaling error
variance, which was set to σ2
n = 0.5. The parameters θ for each protein
were sampled using a random walk Metropolis-Hastings MCMC algo-
rithm (the mhsample function in Matlab) for NM C = 10000 samples
with a burn-in set to 500. The proposal distribution was selected to be
a symmetric uniform distribution such that [as+1, bs+1, cs+1, ds+1] ∼
U (as ± 0.4, bs ± 0.04, cs ± 0.04, ds ± 0.4). Given the sample of scaling
parameters (θ(s)
s=1 , we deﬁne the scaled simulated data as the aver-
age scaling over the MCMC sample, and record also the sample scaling
variance

j )NM C

˜yS
i =

g(yS

i |θ(s)
j )

1
NM C

1
NM C

NM C(cid:88)

s=1
NM C(cid:88)

(cid:16)

s=1

σ2
T (i) =

g(yS

i |θ(s)

j ) − ˜yS
i

(cid:17)2

.

See Figure 1 g) for an illustration of the scaling. We collect the scaled
simulated value and its variance from each simulated point into vectors
˜yS = (˜yS

T (NS)) ∈ RNS .

T (1), . . . , σ2

NS ) and σ2

1 , . . . , ˜yS

T = (σ2

deﬁnes a distribution over functions f (x) whose mean and covariance
are

f (x) ∼ GP (0, k(x, x(cid:48))) ,

E[f (x)] = 0
cov[f (x), f (x(cid:48))] = k(x, x(cid:48)).

For any collection of protein variants X = x1, . . . , xN , the function val-
ues follow a multivariate normal distribution f ∼ N (0, KXX ), where
f = (f (x1), . . . , f (xN ))T ∈ RN , and where KXX ∈ RN ×N with
[KXX ]ij = k(xi, xj). The key property of Gaussian processes is that
they encode functions that predict similar stability values f (x), f (x(cid:48))
for protein variants x, x(cid:48) that are similar, as encoded by the kernel
k(x, x(cid:48)). The key part of GP modelling is then to infer a kernel that
measures the mutation’s eﬀects to the stability.

Let a dataset of noisy stability values from two sources be y ∈ RN ,
the corresponding protein structures X = (xi)N
i=1, and a new protein
variant x∗ whose stability we wish to predict. A Gaussian process de-
ﬁnes a joint distribution over the observed values y of variants X, and
the unknown function value f (x∗) of the unseen variant x∗,
(cid:21)(cid:19)

(cid:18)

(cid:21)

(cid:20) y
f (x∗)

∼ N

0,

(cid:20)KXX + diag(σ2)
k∗X

kX∗
k(x∗, x∗)

,

∗X ∈ RN is a kernel vector with elements k(xi, x∗) for
where kX∗ = kT
T )2)T
all i = 1, . . . , N , and where σ2 = (σ2
collects ﬁnal variances of the data points from equations (5). Here
the exponents are elementwise. The conditional distribution gives the
posterior distribution of the stability prediction as

E1T , (σE1T + σS1T + tσT

0, σ2

f (x∗)|(X, y) ∼ N (cid:0)µ(x∗), σ2(x∗)(cid:1) ,

2.3 Data fusion and noise models

where the prediction mean and variance are

For each protein j, we organise its experimental data (XE, yE) and
transformed simulated data (XS, ˜yS) along with the wild-type infor-
mation (x0, y0) into a single joint dataset of variants X = (x0, XE, XS)

(cid:0)KXX + diag(σ2)(cid:1)−1 y,

µ(x∗) = k∗X
σ2(x∗) = k(x∗, x∗) − k∗X

(cid:0)KXX + diag(σ2)(cid:1)−1 kX∗.

(3)

(4)

3

Hence, in GP regression the stability predictions µ(x∗) ± σ(x∗) will
come with uncertainty estimates.

ring amino acids and scaled them between zero and one as

2.5 Graph kernel

Next, we consider how to compute the similarity function k(x, x(cid:48)) be-
tween two variants of the same protein structure. We will encode the
3D structural information of the two protein variants as a contact map
and measure their similarity by the formalism of graph kernels (Vish-
wanathan et al., 2010).

We consider two residues to be in contact if their closest atoms are
within 5 ˚A of each other in the PDB structure, which is illustrated in
Figure 1 b). All variants of the same protein have the same length, with
only diﬀerent residues at mutating positions. Furthermore, we assume
that all variants share the wild-type protein contact map.

To compare protein variants, we construct a weighted decomposition
kernel (WDK) (Menchetti et al., 2005) between two protein variants
x = (x1, . . . , xM ) and x(cid:48) = (x(cid:48)

M ) of length M ,

1, . . . , x(cid:48)

k(x, x(cid:48)) =


S(xp, x(cid:48)
p)

M
(cid:88)

p=1



(cid:88)

S(xl, x(cid:48)
l)

,

l∈nbs(p)

(7)

where nbs(p) deﬁnes the set of neighbouring positions to position p,
and S is a substitution matrix. The kernel iterates over all positions
p and compares for each of them their residues through a substitution
matrix S(xp, x(cid:48)
p). Furthermore, the similarity of the residues at each
position is multiplied by the average similarity of the residues at its
neighbouring positions S(xl, x(cid:48)
l). Hence, the kernel deﬁnes the simi-
larity of two protein variants as the average position and neighbour-
hood similarity over all positions. The kernel matrix is normalised
so that for two data points, the normalised kernel
p) =
k(xp, x(cid:48)
p, x(cid:48)
p), as deﬁned by Shawe-Taylor and Cris-
tianini (2004). The kernel is illustrated in Figure 1 c).

k(xp, xp)k(x(cid:48)

is ˆk(xp, x(cid:48)

p)/

(cid:113)

The above WDK kernel allows us to compare the eﬀects of multiple
simultaneous mutations. However, as the wild type protein structure
is used for all of the protein variants, changes that the mutations may
cause to the protein structure are not taken into consideration. This
may cause problems if mutations that alter the protein structure sig-
niﬁcantly are introduced – especially if many of them are introduced
simultaneously. On the other hand, substitution matrices that have
their basis in sequence comparisons, should take these eﬀects into ac-
count to some extend as these kinds of mutations are usually highly
destabilising and do not occur often in nature. In the next section, we
will discuss how we utilise diﬀerent substitution matrices with multiple
kernel learning.

The BLOSUM substitution models have been a common choice for
protein models (Giguere et al., 2013), while mixtures of substitution
models were proposed by Cichonska et al. (2017). BLOSUM matrices
score amino acid substitutions by their appearances throughout evolu-
tion, as they compare the frequencies of diﬀerent mutations in similar
blocks of sequences (Henikoﬀ and Henikoﬀ, 1992). However, there are
also diﬀerent ways to score amino acids substitutions, such as chemical
similarity and neighbourhood selectivity (Tomii and Kanehisa, 1996).
When the stability eﬀects of mutations are evaluated, the frequency of
an amino acid substitution in nature may not be the most important
factor.

To take into account diﬀerent measures of similarity between amino
acids, we employed a set of 21 amino acid substitution matrices gath-
ered from AAindex21 (Kawashima et al., 2008). AAindex2 currently
contains 94 substitution matrices. From these we selected those that
had no gaps concerning substitutions between the 20 naturally occur-

1http://www.genome.jp/aaindex/
2http://www.cs.ubc.ca/~schmidtm/Software/minConf.html

4

S =

S0 − min(S0) + 1
max(S0) − min(S0) + 1

.

Out of these matrices, we only chose those 23 matrices that were pos-
itive semideﬁnite. Furthermore, there were two pairs of matrices that
were extremely similar, and we only selected one matrix from each pair,
ending up with 21 substitution matrices. These substitution matrices
are used together with Equation 7 for computing 21 base kernel matri-
ces. Finally, MKL is used to ﬁnd an optimal combination of the base
kernels of form

(8)

(9)

Kφ =

wmK (γm)
m ,

21
(cid:88)

m=1

where wm is a kernel speciﬁc weight, γm is an (elementwise) exponent.
The elementwise exponent retains the SDP property of Kφ (Shawe-
Taylor and Cristianini, 2004). We observe empirically that the optimal
kernel weights wm tend to be sparse (See Figure 2).

The selected substitution matrices are listed in Figure 2. These ma-
trices have diﬀerent basis and through multiple kernel learning (MKL)
our model learns which of these are important for inferring the stability
eﬀects that mutations cause on diﬀerent proteins. The ﬁgure illustrates
this by showing the average weights of the base kernel matrices obtained
via the multiple kernel learning.

2.7 Parameter inference

The complete model has ﬁve parameters φ = (σE, σS, t, w, γ) to in-
fer, of which the variance parameters (σE, σS, t) parameterise the joint
data variance σ2
φ, while the MKL parameters w = (w1, . . . , w21) and
γ = (γ1, . . . , γ21) parameterise the kernel matrix Kφ. In a Gaussian
process model these can be jointly optimised by the marginal (log)
likelihood with priors

log p(y|φ)p(σE)p(σR) = log

p(y|f , φ)p(f |φ)p(σE)p(σR)df

(cid:90)

∝ −

yT (Kφ + diag(σ2

1
2
+ log Gamma(σE|αE, βE) + log Gamma(σS|αS, βS),

log |Kφ + diag(σ2

φ))−1y −

φ)|

1
2

(10)

which automatically balances model ﬁt (the square term) and the
model complexity (the determinant) to avoid overﬁtting (Rasmussen
and Williams, 2006). The parameters can be optimised by maximis-
ing the marginal log likelihood (10) using gradient ascent, since the
marginal likelihood can be diﬀerentiated analytically (see Supplemen-
tary Equations S1 and S2). We utilised a limited-memory projected
quasi-Newton algorithm (minConf TMP2), described by (Schmidt et al.,
2009).

We chose to evaluate the accuracy of our predictions using the same
metrics that have been used by many others – correlation ρ between the
predicted and experimentally measured ∆∆G values (Capriotti et al.,
2005a; Dehouck et al., 2009; Kellogg et al., 2011; Pires et al., 2014b;
Potapov et al., 2009) and the root mean square error (rmse) (Dehouck
et al., 2009; Pires et al., 2014a,b), which are determined in the Supple-
mentary Equations S3 and S4. We use marginal likelihood maximisa-
tion to infer model parameters and perform cross-validation to evaluate
the model performance on test data. Below we only report evaluation
metrics obtained from the test sets not used at any stage of the model
learning or data transformation sampling.

3 Results

In this section we evaluate the performance of mGPfusion on predict-
ing stability eﬀects of mutations, and compare it to the state-of-the-
art prediction methods mCSM, PoPMuSiC and Rosetta. Rosetta is a

2.6 Substitution matrices and multiple kernel

2.8 Evaluation criteria

learning

Figure 2: Average weights for kernels utilising the described substitution matrices from AAindex2, when GP models were trained with mutation level
cross-validation. Basis for the substitution matrices are obtained from (Tomii and Kanehisa, 1996). ∗ were added to AAindex2 in a later release, and their
basis were not determined by Tomii and Kanehisa (1996).

molecular modelling software whose ddg monomer module can directly
simulate the stability changes ∆∆G of a protein upon mutations. PoP-
MuSic and mCSM are machine learning models that predict stability
based on protein variant features. We run Rosetta locally, and use
mCSM and PoPMuSiC models through their web servers3. This may
give these methods an advantage over mGPfusion since parts of our
testing data were likely used within their training data.

We compare four diﬀerent variants of our method: mGPfusion that
uses both simulated data and MKL, “mGPfusion, only B62” that uses
simulated data but incorporates only one kernel matrix (BLOSUM62
substitution matrix), mGP model that uses MKL but does not use simu-
lated data, and “mGP, only B62” that uses only the base GP model but
does not incorporate simulated data and uses only the BLOSUM62 sub-
stitution matrix. In addition, we experiment on transforming Rosetta
predictions with the Bayesian scaling. We perform the experiments
for the 15 proteins separately using either position or mutation level
(leave-one-out) cross-validation regarding the methods mGP, mGPfu-
sion and the Bayesian scaling of Rosetta. Pires et al. (2014b) used
protein and position level cross-validation to evaluate their model. In
protein level cross-validation all mutations in a protein are either in
the test or training set exclusively. When we train our model using
protein level cross-validation, we use no experimental data and rely
only on the simulated data. Position level cross-validation is deﬁned
so that all mutations in a position are either in the test or training set
exclusively. However, datasets in Pires et al. (2014b) contained only
point mutations and therefore we had to extend the deﬁnition to also
include multiple mutations. In position level cross-validation we train
one model for each position using only the part of data that has a
wild-type residue in that position. Therefore, in position level cross-
validation we construct a test set that contains all protein variants that
have a mutation at position p and use as training set all the protein
variants that have a wild-type residue at that position. Dehouck et al.
(2009) evaluated their models by randomly selecting training and test
sets so that each mutation was exclusively in one of the sets, but both
sets could contain mutations from the same position of the same pro-

3biosig.unimelb.edu.au/mcsm and

omictools.com/popmusic-tool

tein. We call this mutation level cross-validation. When we use all
available experimental data with mutation level cross-validation, this
corresponds to leave-one-out cross-validation.

3.1 Predicting point mutations

Table 2 summarises the average prediction performance over all 15 pro-
teins for all compared methods, types of mutations and cross-validation
types. We ﬁrst compare the performances on single point mutations,
where mGPfusion and mGP achieve the highest performance with
ρ = 0.81 and rmse = 1.07 kcal/mol, and ρ = 0.81 and rmse = 1.04
kcal/mol, respectively with mutation level cross-validation. With only
one kernel utilising the BLOSUM62 matrix instead of MKL, the per-
formance decreases slightly, but the competing methods are still out-
performed, as mCSM achieves ρ = 0.64 and rmse = 1.37 kcal/mol,
PoPMuSic ρ = 0.61 and Rosetta ρ = 0.55. Applying Bayesian scaling
on Rosetta simulator improves the performance of standard Rosetta
from ρ = 0.55 to ρ = 0.65 and decreases the rmse from 1.63 kcal/mol
to 1.35 kcal/mol, which is interestingly even slightly better than the
performances of mCSCM and PoPMuSiC.

With position level cross-validation mGPfusion achieves the high-
est performance of ρ = 0.70 and rmse = 1.26 kcal/mol, likely due
to having still access to simulated variants from that position, since
they are always available to the learner. Without simulation data,
the baseline machine learning model mGP performance decreases to
ρ = 0.51 and rmse = 1.54 kcal/mol, thus demonstrating the impor-
tance of the data fusion. Cross-validation could not be performed for
the oﬀ-the-shelf methods mCSM and PoPMuSiC. Even still, mGPfusion
(trained with one or multiple kernels) outperforms competing state-of-
the-art methods and achieves markedly higher prediction performance
as quantiﬁed by both mutation and position level cross-validations.
Also mGP outperforms these methods when quantiﬁed by mutation
level cross-validation. With protein level cross-validation mGPfusion
achieves slightly better results than Rosetta.

5

Method
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled

Point mutations
cross-validation level
prot.
pos.
mut.
0.56
0.70
0.81
0.56
0.69
0.79
-
0.51
0.81
-
0.34
0.76
-
0.63
0.65

Correlation ρ
Multiple mutations
cross-validation level
prot.
pos.
mut.
0.49
0.61
0.88
0.50
0.64
0.86
-
0.52
0.86
-
0.55
0.86
-
0.39
0.51

All mutations
cross-validation level
prot.
pos.
mut.
0.52
0.64
0.83
0.52
0.66
0.82
-
0.50
0.83
-
0.49
0.80
-
0.48
0.60

Point mutations
cross-validation level
prot.
pos.
mut.
1.61
1.26
1.07
1.62
1.30
1.11
-
1.54
1.04
-
1.95
1.26
-
1.38
1.35

rmse
Multiple mutations
cross-validation level
prot.
pos.
mut.
2.53
2.45
1.33
2.50
2.40
1.43
-
2.65
1.44
-
2.56
1.45
-
2.99
2.49

All mutations
cross-validation level

mut.
1.13
1.18
1.14
1.30
1.66

pos.
1.87
1.85
2.09
2.23
2.22

prot.
1.84
1.84
-
-
-

Predictions from oﬀ-the-shelf implementations with no cross-validation

Rosetta
mCSM
PoPMuSiC

0.55
0.61
0.64

0.40
-
-

0.49
-
-

1.63
1.40
1.37

2.74
-
-

1.92
-
-

Table 2: Comparison of diﬀerent methods on the 15 protein dataset with respect to ρ and rmse. Mutation, position, and protein are referred to as mut.,
pos., and prot., respectively. Predictions from oﬀ-the-shelf implementations of Rosetta, mCSM and PoPMuSiC are used directly without cross-validation.

3.2 Predicting multiple mutations

Next, we tested stability prediction accuracies for variants containing
either single or multiple mutations. Figure 3 shows a scatter plot of
mGPfusion predictions for all 1537 single and multiple mutation vari-
ants (covering all 15 proteins) against the experimental ∆∆G values
using the mutation level (leave-one-out) cross-validation. The points
are coloured by the number of simultaneous mutations in the variants,
with 326 variants having at least 2 mutations (See Table 1). Fig-
ure 3 illustrates the mGPfusion’s overall high accuracy of ρ = 0.83
and rmse = 1.13 kcal/mol on both single and multiple mutations (See
Table 2). Scatter plots for the individual proteins can be found in Sup-
plementary Figure S3. Dehouck et al. (2009) suggested that considering
the predictive power after removal of most badly predicted stability ef-
fects of mutations may give more relevant evaluation, as some of the
experimental measurements may have been made in non-physiological
conditions or aﬀected by signiﬁcant error, associated with a poorly re-
solved structure, or indexed incorrectly in the database. They thus
reported correlation and rmse of the predictions after excluding 10 %
of the predictions with most negative impacts on the correlation co-
eﬃcient. Pires et al. (2014b) also reported their accuracy after 10 %
outlier removal. If we remove the 10% worst predicted stability eﬀects
from the combined predictions, we achieve correlation ρ of 0.92 and
rmse of 0.67 kcal/mol. We report these results for all the methods
in Supplementary Table S3 and also present the error distribution in
Supplementary Figure S5.

Figure 3: Scatter plot for the mutation level (leave-one-out) predictions made
for all 15 proteins (See Table 1). The colour indicates the number of simul-
taneous mutations.

The high accuracy is retained for variants with multiple mutations
as well (ρ = 0.88 and rmse = 1.33 kcal/mol, see Tables 2 and S2).
Table 3 lists mGPfusion’s rmse for diﬀerent number of simultaneous
mutations. The model accuracy in fact improves up to 6 mutations.
This is explained by the training set often containing the same single
point mutations that appear in variants with multiple mutations. The
model can then infer the combined eﬀect of pointwise mutations. The
model seems to fail when predicting the eﬀects of 7-9 simultaneous mu-

6

tations. Most of these mutations (8/12) are for Ribonuclease (1RGG)
and their eﬀects seem to be exceptionally diﬃcult to predict. This may
be because only few of the point mutations that are part of the multiple
mutations are present in the training data. However, these mutations
seem to be exceptionally diﬃcult to predict for Rosetta as well, which
could indicate that the experimental measurements concerning these
mutations are not quite accurate. PoPMuSiC and mCSM are unable
to predict multiple mutations, while Rosetta supports them, but its
rmse accuracy decreases already with two mutations.

mutations
occurences

mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta

1
1211

1.07
1.11
1.04
1.26
1.35
1.63

2
207

1.06
1.12
1.03
0.96
2.10
2.27

3
52

0.80
0.77
0.61
0.65
1.92
2.11

4
42

0.51
0.59
0.50
0.83
2.94
3.78

5
4

0.40
0.29
0.18
0.26
2.29
2.93

6
8

1.01
1.14
0.92
1.14
2.32
2.21

7
3

3.02
3.00
3.23
2.95
2.93
2.92

8
3

5.89
6.78
6.18
6.90
6.75
5.80

9
6

5.16
5.56
6.75
6.57
7.28
7.45

10
1

0.25
0.11
0.08
0.05
2.69
3.42

Table 3: Root-mean-square errors for diﬀerent number of simultaneous
mutations for all 15 proteins, with models trained by leave-one-out cross-
validation. Rosetta is added for comparison.

With multiple mutations, the decrease in performance between the
position and mutation level cross-validations becomes clearer than with
single mutations. With the position level cross-validation the stability
eﬀects of multiple mutations are predicted multiple times, which partly
explains this loss of accuracy. For example, the eﬀects of mutants with
nine diﬀerent simultaneous mutations, which were the most diﬃcult
cases in the mutation level cross-validation, are predicted nine times.
Surprisingly, mGPfusion trained with protein level cross-validation
achieves higher correlation and smaller errors than Rosetta; mGPfusion
utilising simulated ∆∆G values for only single mutations, can predict
the stability eﬀects of multiple mutations better than Rosetta.

3.3 Uncertainty of the predictions

Gaussian processes provide a mean µ(x) and a standard deviation σ(x)
for the stability prediction of a protein variant x. The standard de-
viation allows estimation of the prediction accuracy even without test
data. Figure 1 h) visualises the uncertainty of a few predictions made
for the protein G (1PGA) when mutation level cross-validation is used.
The estimated standard deviation allows a user to automatically iden-
tify low quality predictions that can appear e.g. in parts of the input
protein space from which less data is included in model training. Con-
versely, in order to minimise the amount of uncertainty in the mGPfu-
sion predictions, estimated standard deviation can be used to guide next
experiments. The probabilistic nature of the predictions also admits an
alternative error measure of negative log probability density (NLPD)
nlpd = − (cid:80)N
i=1 log p(yi|µ(xi), σ2(xi)), which can naturally take into
account the prediction variance.

3.4 Eﬀect of training set size

The results presented in Sections 3.1–3.3 used all available data for
training with cross-validation to obtain unbiased performance mea-
sures. The inclusion of thousands of simulated variants allows the model

to learn accurate models with less experimentally measured variants.
Hence, we study how the mGPfusion model with or without simulated
data performs with reduced number of experimental observations. To
facilitate this, we randomly selected subsets of experimental data of
size 0, 10, 20, and so on. We learned the mGP and mGPfusion models
with these reduced experimental data sets while always using the full
simulated data sets. This also allows us to estimate how the models
work with diﬀerent number of cross-validation folds. For example, the
point of a learning curve which utilises 2/3 or 4/5 of the training data
correspond to an average of multiple 3-fold or 5-fold cross-validations,
respectively.

The learning curve in Figure 4a) shows how the averaged correlation
for protein 2LZM improves when the size of the experimental data set
increases. The right-most values at N = 348 are obtained with leave-
one-out cross-validation. The inclusion of simulated data in mGPfusion
(dark blue line) consistently improves the performance of mGP, which
is trained without simulated data. Figure 4b) illustrate the diﬀerence
in root mean square error. Learning curves for all proteins listed in
Table 1 can be found from the Supplementary Figures S6-S8. When
the number of experimental samples is zero, the mGPfusion model is
trained solely using the simulated data with scaling 0.57yS, and the
mGP model predicts the stability eﬀect of every mutation as zero. The
last point on the learning curves is obtained with mutation level cross-
validation (see Tables 2 and S2).

proteins mGPfusion model trained with only one kernel that utilises
BLOSUM62, provides approximately as good results as the mGPfu-
sion model trained with multiple kernels. However, with many of the
proteins, utilising just BLOSUM62 does not seem to be suﬃcient and
the accuracy of the model can be improved by using diﬀerent substi-
tution matrices. Prior knowledge of appropriate substitution models
for each protein could enable creation of accurate prediction models
with just one substitution model, but the MKL seems to be a good
tool for selecting suitable substitution models when such knowledge is
not available. It seems that the data fusion and number or relevance
of used substitution matrices can compensate each other – the learning
curves show, that the diﬀerence between mGPfusion models trained
with one or multiple kernels is smaller than the diﬀerence between the
mGP models utilising one or multiple kernels. This indicates that if ad-
ditional simulated data is exploited, the use of multiple or appropriate
substitution models is not as important than without the data fusion.
On the other hand, if data fusion is not applied, the use of MKL can
more signiﬁcantly improve the accuracy of the mGP model.

3.6 Eﬀect of

the Bayesian transformation on

Rosetta

The Bayesian scaling of simulated Rosetta values, proposed in Sec-
tion 2.2, improves the match of Rosetta simulated values to empirical
∆∆G values even without using the Gaussian process framework. The
Bayesian scaling improves the performance of standard Rosetta sim-
ulations from ρ = 0.55 and rmse = 1.63 kcal/mol to ρ = 0.65 and
rmse = 1.35 kcal/mol (see Table 2 and Supplementary Table S2). This
shows that the scaling proposed by Kellogg et al. (2011) indeed is not
always the optimal scaling and signiﬁcant improvements can be gained
by optimising the scaling using a set of training data.

Figure 1 g) visualises the Bayesian scaling for protein 1PGA, where
the very destabilising ∆∆G values are dampened by the scaling (black
dots) to less extreme values by matching the scaled simulated values to
the experimental points (blue circles). The black dots along the scaling
curve indicate the grid of point mutations after transformation. The
scaling variance σ2
T is indicated by the green region’s vertical width,
and on the right panel. The scaling tends to dampen very small values
into less extreme stabilities, while it also estimates higher uncertainties
for stability values further away from ∆∆G = 0. However, the scalings
vary between diﬀerent proteins, as can be seen from the transformations
for each of the 15 proteins presented in Supplementary Figure S9.

Figure 4: a) Correlation and b) root mean square error of predictions made
by models with diﬀerent number of experimental training samples for T4
Lysozyme (2LZM). The results of Rosetta, mCSM and PoPMuSiC are in-
variant to training data (because mCSM and PoPMuSiC are pre-trained),
and are thus constant lines. For both ﬁgures, an average of 100 randomly
selected training sets is taken at each point.

3.5 Eﬀect of data fusion and multiple substitution

matrices

In the beginning of the learning curves, when only little training data is
available, mGPfusion quite consistently outperforms the mGP model,
demonstrating that the additional simulated data improves the predic-
tion accuracy. However, when more training data becomes available,
the performance of mGP model is almost as good or sometimes even
better than the performance of the mGPfusion model. This shows that
if enough training data is available, it is not necessary to simulate addi-
tional data in order to obtain accurate predictions. Table 2 also shows,
that the data fusion can compensate the lack of relevant training data
– with the mGPfusion models that utilise the additional data, the de-
crease in accuracy is smaller when position level cross-validation is used
instead of mutation level cross-validation, than with the mGP models.
The varying weights for the base kernels between diﬀerent pro-
teins (shown in Figure 2) already illustrated that diﬀerent proteins
beneﬁt from diﬀerent similarity measures for amino acid substitutions.
The learning curves also support this observation – with some of the

4 Conclusions

We present a novel method mGPfusion for predicting stability eﬀects of
both single and multiple simultaneous mutations. mGPfusion utilises
structural information in form of contact maps and integrates that with
amino acid residues and combines both experimental and comprehen-
In con-
sive simulated measurements of mutations’ stability eﬀects.
trast to earlier general-purpose stability models, mGPfusion model is
protein-speciﬁc by design, which improves the accuracy but necessitates
having a set of experimental measurements from the protein. In prac-
tise small datasets of 10–20 experimental observations were found to
provide state-of-the-art accuracy models when supplemented by large
simulation datasets.

An important advantage over most state-of-the-art machine learn-
ing methods is that mGPfusion is able to predict the eﬀects of multiple
simultaneous mutations in addition to single point mutations. Our
experiments show that mGPfusion is reliable in predicting up to six
simultaneous mutations in our dataset. Furthermore, the Gaussian
process framework provide a way to estimate the (un)certainty of the
predictions even without a separate test set. We additionally proposed
a novel Bayesian scaling method to re-calibrate simulated protein sta-
bility values against experimental observations. This is a crucial part of
the mGPfusion model, and also alone improved protein-speciﬁc Rosetta
stability predictions by calibrating them using experimental data.

mGPfusion is best suited for a situation, where a protein is thor-
oughly experimented on and accurate predictions for stability eﬀects

7

upon mutations are needed. It takes some time to set up the frame-
work and train the model, but after that new predictions can be made
in fractions of a second. The most time-consuming part is running the
simulations with Rosetta, at least when the most accurate protocol 16
is used. Simulating all 19 possible point mutations for one position
took about 12 hours, but simulations for diﬀerent positions can be run
on parallel. The time needed for training the prediction model depends
on the amount of experimental and simulated training data. With no
simulated data, the training time ranged from few seconds to few min-
utes. With data fusion and a single kernel, the training time was under
an hour. With data fusion and MKL with 21 kernels, the training time
was from a few minutes to a day.

Acknowledgements

We acknowledge the computational resources provided by the Aalto
Science-IT.

Funding

This work has been supported by the Academy of Finland Center of Ex-
cellence in Systems Immunology and Physiology, the Academy of Fin-
land grants no. 260403 and 299915, and the Finnish Funding Agency
for Innovation Tekes (grant no 40128/14, Living Factories).
References

Alberts, B., Johnson, A., Lewis, J., Raﬀ, M., Roberts, K., and Walter, P. (2007). Molec-

ular biology of the cell. Garland Science, 5 edition.

Anslyn, E. V. and Dougherty, D. A. (2006). Modern physical organic chemistry. Uni-

versity Science Books.

Berman, H. M., Westbrook, J., Feng, Z., Gilliland, G., Bhat, T., Weissig, H., Shindyalov,
I. N., and Bourne, P. E. (2000). The protein data bank. Nucleic acids research, 28(1),
235–242.

Bommarius, A. S., Blum, J. K., and Abrahamson, M. J. (2011). Status of protein en-
gineering for biocatalysts: how to design an industrially useful biocatalyst. Current
opinion in chemical biology, 15(2), 194–200.

Branden, C. and Tooze, J. (1999). Introduction to protein structure. Garland, 2 edition.

Giollo, M., Martin, A. J., Walsh, I., Ferrari, C., and Tosatto, S. C. (2014). NeEMO: a
method using residue interaction networks to improve prediction of protein stability
upon mutation. BMC genomics, 15(4), 1.

Henikoﬀ, S. and Henikoﬀ, J. G. (1992). Amino acid substitution matrices from protein
blocks. Proceedings of the National Academy of Sciences, 89(22), 10915–10919.

Kawashima, S., Pokarowski, P., Pokarowska, M., Kolinski, A., Katayama, T., and Kane-
hisa, M. (2008). AAindex: amino acid index database, progress report 2008. Nucleic
acids research, 36(suppl 1), D202–D205.

Kellogg, E. H., Leaver-Fay, A., and Baker, D. (2011). Role of conformational sampling
in computing mutation-induced changes in protein structure and stability. Proteins:
Structure, Function, and Bioinformatics, 79(3), 830–838.

Kirk, O., Borchert, T. V., and Fuglsang, C. C. (2002). Industrial enzyme applications.

Current opinion in biotechnology, 13(4), 345–351.

Kumar, M. S., Bava, K. A., Gromiha, M. M., Prabakaran, P., Kitajima, K., Uedaira,
H., and Sarai, A. (2006). ProTherm and ProNIT: thermodynamic databases for pro-
teins and protein–nucleic acid interactions. Nucleic Acids Research, 34(suppl 1),
D204–D206.

Leaver-Fay, A., Tyka, M., Lewis, S. M., Lange, O. F., Thompson, J., Jacak, R., Kaufman,
K., Renfrew, P. D., Smith, C. A., Sheﬄer, W., et al. (2011). ROSETTA3: an object-
oriented software suite for the simulation and design of macromolecules. Methods in
enzymology, 487, 545.

Liu, J. and Kang, X. (2012). Grading amino acid properties increased accuracies of single

point mutation on protein stability prediction. BMC bioinformatics, 13(1), 1.

Menchetti, S., Costa, F., and Frasconi, P. (2005). Weighted decomposition kernels. In
Proceedings of the 22nd international conference on Machine learning, pages 585–
592. ACM.

Pace, C. N. and Scholtz, J. M. (1997). Measuring the conformational stability of a pro-

tein. Protein structure: A practical approach, 2, 299–321.

Pace, C. N. and Shaw, K. L. (2000). Linear extrapolation method of analyzing solvent
denaturation curves. Proteins: Structure, Function, and Bioinformatics, 41(S4),
1–7.

Pires, D. E., Ascher, D. B., and Blundell, T. L. (2014a). DUET: a server for predicting
eﬀects of mutations on protein stability using an integrated computational approach.
Nucleic acids research, page gku411.

Pires, D. E., Ascher, D. B., and Blundell, T. L. (2014b). mCSM: predicting the eﬀects of
mutations in proteins using graph-based signatures. Bioinformatics, 30(3), 335–342.

Potapov, V., Cohen, M., and Schreiber, G. (2009). Assessing computational methods for
predicting protein stability upon mutation: good on average but not in the details.
Protein Engineering Design and Selection, 22(9), 553–560.

Capriotti, E., Fariselli, P., and Casadio, R. (2005a). I-Mutant2.0: predicting stability
changes upon mutation from the protein sequence or structure. Nucleic acids research,
33(suppl 2), W306–W310.

Rapley, R. and Walker, J. M. (2000). Molecular Biology and Biotechnology. Royal

Society of Chemistry, 4 edition.

Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian processes for machine learn-

Capriotti, E., Fariselli, P., Calabrese, R., and Casadio, R. (2005b). Predicting pro-
tein stability changes from sequences using support vector machines. Bioinformatics,
21(suppl 2), ii54–ii58.

ing. The MIT Press.

Capriotti, E., Fariselli, P., Rossi, I., and Casadio, R. (2008). A three-state prediction of

single point mutations on protein stability changes. BMC bioinformatics, 9(2).

Chen, C.-W., Lin, J., and Chu, Y.-W. (2013). iStable: oﬀ-the-shelf predictor integration

for predicting protein stability changes. BMC bioinformatics, 14(2).

Cheng, J., Randall, A., and Baldi, P. (2006). Prediction of protein stability changes for
single-site mutations using support vector machines. Proteins: Structure, Function,
and Bioinformatics, 62(4), 1125–1132.

Cherry, J. R. and Fidantsef, A. L. (2003). Directed evolution of industrial enzymes: an

update. Current opinion in biotechnology, 14(4), 438–443.

Cichonska, A., Ravikumar, B., Parri, E., Timonen, S., Pahikkala, T., Airola, A., Wenner-
berg, K., Rousu, J., and Aittokallio, T. (2017). Computational-experimental approach
to drug-target interaction mapping: A case study on kinase inhibitors. PLoS compu-
tational biology, 13(8), e1005678.

Sanchez, S. and Demain, A. L. (2010). Enzymes and bioconversions of industrial, pharma-
ceutical, and biotechnological signiﬁcance. Organic Process Research & Development,
15(1), 224–230.

Schmidt, M. W., Berg, E., Friedlander, M. P., and Murphy, K. P. (2009). Optimizing
costly functions with simple constraints: A limited-memory projected quasi-newton
algorithm. In International Conference on Artiﬁcial Intelligence and Statistics, page
None.

Shawe-Taylor, J. and Cristianini, N. (2004). Kernel methods for pattern analysis. Cam-

bridge university press.

Tian, J., Wu, N., Chu, X., and Fan, Y. (2010). Predicting changes in protein thermosta-
bility brought about by single- or multi-site mutations. BMC bioinformatics, 11(1),
1.

Tokuriki, N. and Tawﬁk, D. S. (2009). Stability eﬀects of mutations and protein evolv-

ability. Current opinion in structural biology, 19(5), 596–604.

Dehouck, Y., Grosﬁls, A., Folch, B., Gilis, D., Bogaerts, P., and Rooman, M. (2009). Fast
and accurate predictions of protein stability changes upon mutations using statistical
potentials and neural networks: PoPMuSiC-2.0. Bioinformatics, 25(19), 2537–2543.

Tomii, K. and Kanehisa, M. (1996). Analysis of amino acid indices and mutation matrices
for sequence comparison and structure prediction of proteins. Protein Engineering,
Design and Selection, 9(1), 27–36.

Folkman, L., Stantic, B., and Sattar, A. (2014). Feature-based multiple models improve
classiﬁcation of mutation-induced stability changes. BMC genomics, 15(Suppl 4).

Vishwanathan, S. V. N., Schraudolph, N. N., Kondor, R., and Borgwardt, K. M. (2010).

Graph kernels. The Journal of Machine Learning Research, 11, 1201–1242.

Giguere, S., Marchand, M., Laviolette, F., Drouin, A., and Corbeil, J. (2013). Learn-
ing a peptide-protein binding aﬃnity predictor with kernel ridge regression. BMC
bioinformatics, 14(1), 82.

Wainreb, G., Wolf, L., Ashkenazy, H., Dehouck, Y., and Ben-Tal, N. (2011). Protein
stability: a single recorded mutation aids in predicting the eﬀects of other mutations
in the same amino acid site. Bioinformatics, 27(23), 3286–3292.

8

Supplementary material

Figure S5: The stability of a protein is determined by the thermodynamic and kinetic stabilities, ∆Gt and ∆Gk, respectively. We only consider the
thermodynamic stability.

Figure S6: Priors presented by Equation 2. Here µd = −1.5, the most likely value for −a. Other hyperparameter values are presented in Table S4.

Table S4: Values for the hyperparameters used in the priors of a, b, c, d, σE and σS presented in Equations 2 and 6, respectively.

a

b

c

d

σE

σS

αa = 2
βa = 1.5

αb = 1.3
βb = 2

αc = 2
βc = 5

µd = −a
σd = 0.15

αE = 2.5
βE = 0.02

αS = 50
βS = 0.007

9

The partial derivatives of the marginal likelihood with respect to the parameters φ are obtained from Equation (9) as follows:

where α = K −1

φ y, Kφ is determined as

and the partial derivatives of Kφ with respect to the optimised parameters are

∂
∂φj

log p(y|X, φ) =

yT K −1

∂Kφ
∂φj

K −1

φ y −

(cid:19)

K −1
φ

∂Kφ
∂φj

1
2
1
2

φ
(cid:18)(cid:16)

=

tr

ααT − K −1

φ

(cid:18)

(cid:19)

,

tr

1
2
(cid:17) ∂Kφ
∂θj

Kφ =

wmK γm

m + diag





σ0
σE1NE
σE1NE + σS1NS + tσT


2



M
(cid:88)

m=1

∂Kφ
∂σE

∂Kφ
∂σR

∂Kφ
∂t

∂Kφ
∂wm
∂Kφ
∂γm







0
2σE1NE
2 (σE1NE + σS1NS + tσT )
0
0NE
2 (σE1NE + σS1NS + tσT )
0
0NE
2 (σE1NE + σS1NS + t) σT













=diag



=diag



=diag



=K γm
m

=wmK γm

m log Km

(cid:80)N∗

i=1(yi − ¯y)(µ(xi) − ¯µ)

i=1(yi − ¯y)2 (cid:80)N∗

i=1(µ(xi) − ¯µ)2

ρ =

(cid:113)(cid:80)N∗
(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
N∗

N∗(cid:88)

i=1

rmse =

(yi − µ(xi))2,

Correlation ρ and root-mean-square error rmse for the predictions are determined as

(S11)

(S12)

(S13)

(S14)

(S15)

(S16)

(S17)

(S18)

where ¯y is the mean of the experimentally measured values, µ(xi) is prediction mean, ¯µ is the average of all prediction means, and N∗ is the
number of predictions.

10

Figure S7: Mutation-level predictions for all 15 proteins presented in Table 1. The predictions are coloured by the number of simultaneous mutations.

11

Figure S8: Position-level predictions for all 15 proteins. When the eﬀects of a mutant are predicted multiple times, they are connected by a line.

12

Table S5: (Continues on the next page)Comparison of diﬀerent methods on the 15 protein dataset with respect to ρ and rmse. Oﬀ-the-shelf implementations
of Rosetta, mCSM and PoPMuSiC are used directly without cross-validation.

Protein Method

2LZM

1BNI

1VQB

1LZ1

2CI2

1PGA

2RN2

1CSP

mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC

0.58
0.59
-
-
-

0.75
0.73
0.75
0.71
0.57

0.49
0.50
-
-
-

0.67
0.65
0.79
0.79
0.47

0.62
0.62
-
-
-

0.77
0.74
0.81
0.61
0.59

Point mutations
cross-validation level
prot.
pos.
mut.
0.75
0.80
0.87
0.75
0.77
0.86
-
0.59
0.86
-
0.37
0.75
0.73
-
0.74
0.75
0.57
0.71
0.64
0.61
0.65
0.48
0.58
0.62
0.60
0.66
0.50
0.53
0.12
0.29
0.46
0.49
0.53
0.51
0.59
0.56
0.39
-0.31
0.55
0.59
0.67
0.64
0.72
0.67
0.61
0.74
0.60
0.63
0.74
0.75
0.47
0.59
0.61
-0.46
0.59
0.69
-0.10
0.28
0.58
0.59
0.12
0.09
0.64
0.70
0.71
0.71
0.23
0.22
-0.06
-0.37
0.20
0.33
0.42
0.48

0.85
0.86
0.88
0.87
0.23

0.33
0.34
-
-
-

0.68
0.82
0.62
0.57
0.69

0.79
0.82
0.77
0.83
0.66

0.69
0.71
-
-
-

0.71
0.71
-
-
-

0.73
0.69
0.65
0.51
0.60

0.64
0.63
-
-
-

0.75
0.73
-
-
-

0.39
0.40
-
-
-

0.93
0.91
0.96
0.97
0.71

0.86
0.84
0.86
0.87
0.17

-1.00
-1.00
-
-
-

1.00
1.00
1.00
1.00
-1.00

Correlation ρ
Multiple mutations
cross-validation level
prot.
pos.
mut.
0.64
0.83
0.96
0.64
0.87
0.96
-
0.85
0.97
-
0.77
0.94
0.66
-
0.68
0.68
-
-
0.70
0.79
0.82
0.85
0.29
0.18
-
-
0.83
0.82
0.70
0.75
0.68
0.73
-
-
0.11
0.05
0.56
0.42
-1.00
-1.00
-
-
0.87
0.86
0.79
0.71
0.61
0.62
-
-
0.35
0.60
-0.24
-0.08
0.07
0.03
-
-
0.60
0.60
0.42
0.42
0.50
0.47
-
-
0.73
0.69
0.75
0.71
0.69
0.68
-
-

0.92
0.91
0.94
0.92
0.68

0.90
0.76
0.93
0.92
0.09

0.78
0.76
0.75
0.77
0.48

0.48
0.59
-
-
-

0.32
0.62
-
-
-

0.61
0.62
-
-
-

0.95
0.92
0.92
0.92
0.61

0.85
0.86
-
-
-

0.77
0.76
0.78
0.74
0.53

0.76
0.75
0.85
0.85
0.57

0.77
0.74
0.80
0.63
0.53

0.52
0.53
-
-
-

0.60
0.58
-
-
-

0.57
0.57
-
-
-

All mutations
cross-validation level
prot.
pos.
mut.
0.68
0.76
0.90
0.69
0.82
0.90
-
0.72
0.90
-
0.61
0.82
0.65
-
0.70
0.71
-
-
0.55
0.60
0.63
0.41
0.45
0.56
-
-
0.69
0.69
0.50
0.55
0.59
0.59
-
-
0.57
0.56
0.47
0.21
0.46
0.55
-
-
0.81
0.79
0.72
0.63
0.63
0.65
-
-
0.43
0.53
-0.14
0.06
0.11
0.28
-
-
0.53
0.53
0.22
0.23
0.57
0.65
-
-
0.75
0.72
0.77
0.72
0.64
0.60
-
-

0.38
0.54
-
-
-

0.50
0.70
-
-
-

0.70
0.70
-
-
-

0.92
0.91
0.94
0.92
0.59

0.85
0.81
0.84
0.81
0.24

0.79
0.81
0.77
0.82
0.62

0.71
0.72
-
-
-

0.82
0.79
0.76
0.68
0.63

1.06
1.06
-
-
-

0.83
0.86
0.81
0.91
0.99

2.25
2.25
-
-
-

1.71
1.75
1.41
1.50
1.99

1.21
1.27
1.08
1.61
1.58

1.69
1.69
-
-
-

mut.
0.82
0.84
0.82
1.12
1.05

Point mutations
cross-validation level
prot.
pos.
1.12
1.02
1.12
1.08
-
1.34
-
1.93
1.06
-
1.13
1.27
1.11
1.37
1.45
1.40
2.32
1.58
1.70
1.62
1.53
1.94
1.94
2.41
2.89
2.00
2.26
2.24
2.29
0.99
1.03
1.15
1.43
1.01
1.04
0.97
0.95
0.90
0.97
1.02
1.39
1.00
1.09
0.86
0.85
1.54
1.22
1.58
1.73
1.42
1.70
1.94
1.89
1.21
1.22
1.45
1.45
1.13
1.07
1.04
1.16
1.04
1.04
1.07
1.12
1.06
1.11
1.02
0.99

0.64
0.65
0.54
0.60
1.04

1.10
1.10
-
-
-

1.64
1.06
-
-
-

1.26
0.87
1.40
1.53
1.21

0.91
0.87
0.93
0.82
1.09

1.05
1.59
-
-
-

0.85
0.91
0.95
1.16
1.00

1.07
1.07
-
-
-

0.56
1.10
0.13
0.27
3.23

1.15
1.35
0.70
0.63
1.90

1.28
1.24
1.32
1.15
2.51

3.75
3.51
-
-
-

2.06
2.43
-
-
-

2.17
2.14
-
-
-

mut.
0.57
0.59
0.48
0.78
1.65

rmse
Multiple mutations
cross-validation level
prot.
pos.
2.14
1.53
2.11
1.16
-
1.24
-
1.44
1.84
-
1.93
-
-
2.49
2.03
2.00
1.67
2.90
2.33
-
-
1.62
1.82
1.97
2.30
1.96
2.06
-
-
2.40
2.41
1.65
2.36
3.25
3.41
-
-
0.80
1.01
1.01
1.40
1.27
1.30
-
-
2.09
0.95
3.01
2.07
3.09
3.51
-
-
1.01
0.95
1.21
1.20
1.08
1.25
-
-
1.66
1.87
1.51
1.59
2.29
1.92
-
-

2.58
2.13
-
-
-

2.74
0.88
-
-
-

0.91
1.18
-
-
-

1.21
1.12
-
-
-

0.91
0.96
0.76
0.86
2.19

0.53
0.69
0.45
0.47
2.81

0.67
0.69
0.74
0.68
1.20

0.55
0.71
0.66
0.71
1.27

All mutations
cross-validation level
prot.
1.43
1.42
-
-
-

mut.
0.76
0.79
0.75
1.05
1.23

pos.
1.30
1.13
1.29
1.70
1.51

1.22
1.27
1.11
1.57
1.70

1.59
1.66
1.27
1.33
1.97

0.83
0.87
0.80
0.90
1.07

0.80
0.87
0.90
1.08
1.06

0.88
0.83
0.94
1.02
2.33

0.86
0.83
0.89
0.80
1.12

0.75
0.78
0.63
0.71
1.58

1.37
-
-
1.67
1.60
1.55
2.20
1.94

1.77
-
-
1.82
1.89
2.24
2.66
1.99

2.21
-
-
1.07
1.10
1.17
1.47
1.15

1.12
-
-
0.86
0.99
1.02
1.39
1.11

1.13
-
-
2.00
1.13
2.81
2.01
2.87

2.95
-
-
1.14
1.13
1.36
1.36
1.11

1.11
-
-
1.45
1.60
1.36
1.43
1.92

1.47
-
-

1.75
1.75
-
-
-

2.20
2.30
-
-
-

1.16
1.14
-
-
-

1.10
1.08
-
-
-

2.38
1.02
-
-
-

1.02
1.51
-
-
-

1.80
1.57
-
-
-

13

Table S5: (Continued) Comparison of diﬀerent methods on the 15 protein dataset with respect to ρ and rmse. Oﬀ-the-shelf implementations of Rosetta,
mCSM and PoPMuSiC are used directly without cross-validation.

Protein Method

1BVC

4LYZ

1RTB

1PIN∗

1RN1

1RGG

1BPI

total

mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC

0.34
0.34
-
-
-

0.61
0.59
0.65
0.62
0.29

0.64
0.59
0.61
0.23
0.50

0.70
0.70
-
-
-

0.53
0.53
-
-
-

0.92
0.89
0.92
0.91
0.65

Point mutations
cross-validation level
prot.
pos.
mut.
0.48
0.43
0.41
0.48
0.48
0.50
-
-0.12
-0.05
-
0.06
-0.05
-
0.40
0.42
0.47
0.47
0.60
0.27
0.30
-0.05
0.27
0.28
0.33
0.55
0.59
0.81
0.79
0.69
0.58
0.61
0.69
0.68
0.72
0.49
0.51
0.40
0.25
0.49
0.53
0.72
0.60
0.62
0.58
0.04
0.27
0.57
0.67
0.76
0.62
0.51
0.52
-0.61
-0.53
0.43
0.60
0.77
0.65
0.67
0.65
0.57
0.58
0.52
0.51
0.71
0.72
0.70
0.69
0.51
0.34
0.63
0.55
0.61
0.64

0.67
0.67
-
-
-

0.83
0.81
0.74
0.77
0.63

0.61
0.61
-
-
-

0.51
0.51
-
-
-

0.81
0.79
0.81
0.76
0.65

0.56
0.56
-
-
-

0.69
0.69
0.71
0.72
0.58

0.68
0.64
0.56
0.43
0.52

Correlation ρ
Multiple mutations
cross-validation level
prot.
pos.
mut.
-0.63
-0.25
-0.28
-0.66
-0.25
-0.25
-
-0.21
-0.10
-
-0.21
-0.11
-
-0.57
-0.63
-0.65
-
-
0.57
0.64
0.46
0.47
0.68
0.71
-
-
0.74
0.73
0.30
0.58
0.73
0.73
-
-
-

0.78
0.73
0.78
0.48
0.73

0.95
0.96
0.96
0.97
0.70

0.75
0.67
-
-
-

0.66
0.64
-
-
-

-
-
-
-
-

-0.19
-0.01
-
-
-

0.96
0.93
-
-
-

-0.06
0.21
-
-
-

0.49
0.50
-
-
-

-
-
-
-
-

0.97
0.96
0.99
0.99
0.21

0.42
0.22
0.19
0.08
0.78

-0.39
-0.53
-0.29
-0.55
0.23

0.88
0.86
0.86
0.86
0.51

-

-

-
-

-
-
-
0.38
0.42
0.62
0.50
0.18
0.20
-
-
0.66
0.30
0.08
0.07
0.86
0.77
-
-
-0.02
0.33
0.52
0.46
-0.00
-0.00
-
-
0.61
0.64
0.52
0.55
0.39
0.40
-
-

0.65
0.64
0.68
0.64
0.33

0.86
0.86
0.76
0.85
0.67

0.64
0.59
0.61
0.23
0.50

0.35
0.35
-
-
-

0.68
0.69
-
-
-

0.53
0.53
-
-
-

All mutations
cross-validation level
prot.
pos.
mut.
0.09
-0.09
0.08
0.14
-0.07
0.14
-
-0.23
-0.13
-
-0.20
-0.14
-
-0.09
0.09
0.14
-
-
0.34
0.40
0.18
0.21
0.34
0.35
-
-
0.71
0.75
0.18
0.16
0.65
0.70
-
-
0.49
0.51
0.40
0.25
0.49
0.53
0.72
0.60
0.55
0.51
0.04
0.10
0.54
0.65
-
-
0.52
0.33
-0.05
-0.00
0.51
0.39
-
-
-0.13
0.05
0.84
0.70
-0.08
0.13
-
-
0.64
0.66
0.50
0.49
0.48
0.49
-
-

0.12
0.12
-
-
-

0.65
0.66
-
-
-

0.52
0.52
-
-
-

0.73
0.55
-
-
-

0.84
0.81
0.75
0.78
0.61

0.85
0.81
0.83
0.76
0.08

0.83
0.82
0.83
0.80
0.60

0.52
0.40
0.35
0.30
0.42

3.46
3.50
-
-
-

1.47
1.48
1.38
1.48
2.26

2.44
2.44
-
-
-

1.25
1.48
1.26
1.67
1.99

1.00
1.00
-
-
-

0.47
0.50
0.49
0.70
0.54

mut.
0.74
0.70
0.99
0.99
0.75

Point mutations
cross-validation level
prot.
pos.
1.65
0.72
1.65
0.71
-
1.00
-
0.99
-
0.76
1.67
1.00
0.85
1.65
1.64
1.78
1.84
2.22
3.61
1.43
1.45
1.73
1.85
2.62
3.37
2.08
2.45
2.33
2.22
0.53
0.54
0.58
0.76
0.54
0.99
0.59
0.70
1.17
1.24
1.75
1.78
1.18
1.20
0.97
1.14
1.70
1.68
2.24
2.01
1.76
1.58
1.36
1.54
1.28
1.32
1.43
1.73
1.52
1.80
1.26
1.31
1.26
1.30
1.54
1.95
1.38
1.63
1.40
1.37

0.85
0.92
1.06
1.00
1.12

1.18
1.18
-
-
-

1.27
1.32
1.43
1.68
1.52

1.61
1.62
-
-
-

1.83
1.82
-
-
-

1.07
1.11
1.04
1.26
1.35

1.41
1.42
1.38
1.53
1.57

1.56
1.56
-
-
-

0.69
0.57
0.36
0.31
1.22

2.35
2.17
-
-
-

mut.
1.64
1.62
1.30
1.29
1.65

rmse
Multiple mutations
cross-validation level
prot.
pos.
2.70
1.48
2.43
1.47
-
1.35
-
1.33
-
1.45
2.45
-
-
1.02
0.93
1.15
1.16
1.29
2.05
-
-
2.24
1.81
3.39
2.11
1.62
2.01
-
-
-
-
-
-
-

1.71
1.90
-
-
-

2.22
1.82
3.44
2.12
1.63

-
-
-
-
-

-
-
-
-
-

0.19
0.35
0.12
0.13
0.98

5.67
6.21
6.81
6.89
7.08

2.17
2.86
1.72
2.52
7.43

1.33
1.43
1.44
1.45
2.49

-
-
0.91
0.77
0.58
0.82
1.01
1.24
-
-
5.95
6.14
6.88
6.77
7.24
6.90
-
-
7.66
7.07
4.19
6.58
7.43
7.68
-
-
2.45
2.40
2.65
2.56
2.99
2.74
-
-

1.28
0.99
-
-
-

4.20
5.43
-
-
-

7.69
7.70
-
-
-

2.53
2.50
-
-
-

All mutations
cross-validation level
prot.
2.02
1.92
-
-
-

mut.
1.09
1.06
1.09
1.09
1.10

pos.
1.21
1.20
1.21
1.20
1.20

1.35
1.35
1.24
1.33
2.09

1.40
1.53
1.69
1.73
1.95

0.47
0.50
0.49
0.70
0.54

0.81
0.88
1.01
0.95
1.10

2.65
2.85
3.05
3.14
3.23

1.40
1.57
1.47
1.80
2.88

1.13
1.18
1.14
1.30
1.66

1.94
-
-
1.45
1.43
1.59
1.63
1.94

3.35
-
-
1.85
1.84
2.81
3.14
1.99

2.40
-
-
0.53
0.54
0.58
0.76
0.54

0.99
0.59
0.70

1.13
1.17
1.61
1.66
1.15

1.20
-
-
4.75
4.90
5.53
5.41
5.74

3.16
-
-
3.64
3.40
2.28
3.34
3.62

3.09
-
-
1.87
1.85
2.09
2.23
2.22

1.92
-
-

3.26
3.27
-
-
-

2.36
2.38
-
-
-

1.00
1.00
-
-
-

1.19
1.17
-
-
-

2.23
2.64
-
-
-

3.11
3.11
-
-
-

1.84
1.84
-
-
-

∗ dataset for 1PIN contained no multiple mutations.

14

Table S6: Comparison of diﬀerent methods on the 15 protein dataset with respect to ρ and rmse after removing 10% of predictions with largest errors.
Mutation, position, and protein are referred to as mut., pos., and prot., respectively. Oﬀ-the-shelf implementations of Rosetta, mCSM and PoPMuSiC are
used directly without cross-validation.

Results after 10 %
outlier removal

Method
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled

Point mutations
cross-validation level
prot.
pos.
0.75
0.77
0.75
0.73
-
0.57
-
0.27
-
0.78

mut.
0.87
0.86
0.89
0.86
0.80

Correlation ρ
Multiple mutations
cross-validation level
prot.
pos.
0.63
0.84
0.65
0.84
-
0.76
-
0.76
-
0.71

mut.
0.97
0.96
0.98
0.97
0.73

Oﬀ-the-shelf implementations with no cross-validation

All mutations
cross-validation level
prot.
pos.
0.71
0.81
0.72
0.80
-
0.74
-
0.66
-
0.75

mut.
0.92
0.91
0.93
0.91
0.78

Point mutations
cross-validation level
prot.
pos.
0.93
0.85
0.93
0.85
-
1.01
-
1.30
-
0.87

mut.
0.69
0.69
0.65
0.73
0.84

rmse
Multiple mutations
cross-validation level
prot.
pos.
1.90
1.55
1.85
1.43
-
1.66
-
1.48
-
1.99

mut.
0.62
0.64
0.49
0.54
1.67

All mutations
cross-validation level
prot.
1.13
1.12
-
-
-

mut.
0.67
0.67
0.62
0.69
1.00

pos.
1.15
1.10
1.23
1.37
1.31

Rosetta
mCSM
PoPMuSiC

0.75
0.71
0.73

0.67
-
-

0.73
-
-

0.94
0.89
0.86

1.85
-
-

1.11
-
-

Figure S9: rmse with diﬀerent amount of predictions, when predictions are sorted by the error. Position level cross-validation was used for mGPfusion,
mGP and Rosetta scaled.

15

Figure S10: Learning curves.

16

Figure S11: Learning curves.

17

Figure S12: Learning curves.

18

Figure S13: Transformations for all 15 proteins presented in Table 1. The red circles mark the simulated ∆∆G-values yS with respect to the experimental
measured ∆∆G-values yE. Thin black lines show possible transformations for yS, whereas the thick black line shows the selected transformation from yS
to ˜yS.

19

mGPfusion: Predicting protein stability changes with
Gaussian process kernel learning and data fusion

Emmi Jokinen1, Markus Heinonen1,2 and Harri L¨ahdesm¨aki1
1Department of Computer Science, Aalto University, 02150 Espoo, Finland
2Helsinki Institute for Information Technology, Finland

Abstract

Motivation: Proteins are commonly used by biochemical industry for numerous processes. Reﬁning these proteins’ properties via
mutations causes stability eﬀects as well. Accurate computational method to predict how mutations aﬀect protein stability are necessary
to facilitate eﬃcient protein design. However, accuracy of predictive models is ultimately constrained by the limited availability of
experimental data.
Results: We have developed mGPfusion, a novel Gaussian process (GP) method for predicting protein’s stability changes upon single
and multiple mutations. This method complements the limited experimental data with large amounts of molecular simulation data.
We introduce a Bayesian data fusion model that re-calibrates the experimental and in silico data sources and then learns a predictive
GP model from the combined data. Our protein-speciﬁc model requires experimental data only regarding the protein of interest and
performs well even with few experimental measurements. The mGPfusion models proteins by contact maps and infers the stability
eﬀects caused by mutations with a mixture of graph kernels. Our results show that mGPfusion outperforms state-of-the-art methods in
predicting protein stability on a dataset of 15 diﬀerent proteins and that incorporating molecular simulation data improves the model
learning and prediction accuracy.
Availability: Software implementation and datasets are available at github.com/emmijokinen/mgpfusion
Contact: emmi.jokinen@aalto.ﬁ

8
1
0
2
 
r
a

M
 
3
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
2
5
8
2
0
.
2
0
8
1
:
v
i
X
r
a

1 Introduction

Proteins are used in various applications by pharmaceutical, food, fuel,
and many other industries and their usage is growing steadily (Kirk
et al., 2002; Sanchez and Demain, 2010). Proteins have important ad-
vantages over chemical catalysts, as they are derived from renewable
resources, are biodegradable and are often highly selective (Cherry and
Fidantsef, 2003). Protein engineering is used to further improve the
properties of proteins, for example to enhance their catalytic activity,
modify their substrate speciﬁcity or to improve their thermostability
(Rapley and Walker, 2000). Increasing the stability is an important as-
pect of protein engineering, as the proteins used in industry should be
stable in the industrial process conditions, which often involve higher
than ambient temperature and non-aqueous solvents (Bommarius et al.,
2011). The properties of a protein are modiﬁed by introducing alter-
ations to its amino acid sequence. Mutations in general tend to be
destabilising, and if too many destabilising mutations are implemented,
the protein may not remain functional without compensatory stabilis-
ing mutations (Tokuriki and Tawﬁk, 2009).

The stability of a protein can be deﬁned as the diﬀerence in Gibbs
energy ∆G between the folded and unfolded (or native and denatu-
rated) state of the protein. More precisely, the Gibbs energy diﬀerence
determines the thermodynamic stability ∆Gt of the protein, as it does
not take into account the kinetic stability ∆Gk which determines the
energy needed for the transition between the folded and unfolded states
(Anslyn and Dougherty, 2006) (see Supplementary Figure S1). Here we
will consider only the thermodynamic stability and from now on it will
be referred to merely as stability ∆G.

The eﬀect of mutations can be deﬁned by the change they cause
to the Gibbs energy ∆G, denoted as ∆∆G (Pace and Scholtz, 1997).
To comprehend the signiﬁcance of stability changes upon mutations,
we can consider globular proteins, the most common type of enzymes,
whose polypeptide chain is folded up in a compact ball-like shape with
an irregular surface (Alberts et al., 2007). These proteins are only
marginally stable and the diﬀerence in Gibbs energy between the folded
and unfolded state is only about 5–15 kcal/mol, which is not much more
than the energy of a single hydrogen bond that is about 2–5 kcal/mol
(Branden and Tooze, 1999). Therefore, even one mutation that breaks
a hydrogen bond can prevent a protein from folding properly.

The protein stability can be measured with many techniques, in-
cluding thermal, urea and guanidinium chloride (GdmCl) denaturation

curves that are determined as the fraction of unfolded proteins at dif-
ferent temperatures or at diﬀerent concentrations of urea or GdmCl
(Pace and Shaw, 2000). Some of the experimentally measured sta-
bility changes upon mutations have been gathered in thermodynamic
databases such as Protherm (Kumar et al., 2006).

A variety of computational methods have been introduced to predict
the stability changes upon mutations more eﬀortlessly than through ex-
perimental measurements. These methods utilise physics or knowledge-
based potentials (Leaver-Fay et al., 2011), their combinations, or diﬀer-
ent machine learning methods. The machine learning methods utilise
support vector machines (SVM) (Capriotti et al., 2005b, 2008; Chen
et al., 2013; Cheng et al., 2006; Folkman et al., 2014; Liu and Kang,
2012; Pires et al., 2014a), random forests (Tian et al., 2010; Wainreb
et al., 2011), neural networks (Dehouck et al., 2009; Giollo et al., 2014),
and Gaussian processes (Pires et al., 2014b). However, it has been as-
sessed that although on average many of these methods provide good
results, they tend to fail on details (Potapov et al., 2009). In addition,
many of these methods are able to predict the stability eﬀects only for
single-point mutations.

We introduce mGPfusion (mutation Gaussian Processes with data
fusion), a method for predicting stability eﬀects of both point and mul-
tiple mutations. mGPfusion is a protein-speciﬁc model – in contrast to
earlier stability predictors that aim to estimate arbitrary protein struc-
ture or sequence stabilities – and achieves markedly higher accuracy
while utilising data only from a single protein at a time. In contrast
to earlier works that only use experimental data to train their models,
we also combine exhaustive Rosetta (Leaver-Fay et al., 2011) simulated
point mutation in silico stabilities to our training data.

A key part of mGPfusion is the automatic scaling of simulated data
to better match the experimental data distribution based on those vari-
ants that have both experimental and simulated stability values. Fur-
thermore, we estimate a variance resulting from the scaling, which
places a higher uncertainty on very destabilising simulations. Our
Gaussian process model then utilises the joint dataset with their es-
timated heteroscedastic variances and uses a mixture of graph kernels
to assess the stability eﬀects caused by changes in amino acid sequence
according to 21 substitution models. Our experiments on a novel 15
protein dataset show a state-of-the-art stability prediction performance,
which is also sustained when there is access only to a very few experi-
mental stability measurements.

1

Figure 1: Pipeline illustration for mGPfusion. a) M = 21 substitution matrices utilise diﬀerent information sources and give scores to pairwise amino
acid substitutions. b) The wild-type structures from Protein Data Bank are modelled as contact graphs. c) The graph kernel measures similarity of two
sequences by a substitution model S over all positions p and their neighbourhoods nbs(p) in the contact graph. d) Each substitution matrix is used to
create a separate covariance matrix. e) Multiple kernel learning (MKL) is used for ﬁnding the optimal combination of the base kernels. The kernel matrix
measures variant similarities. f ) Experimentally measured ∆∆G values yE are gathered from Protherm and Rosetta’s ddg monomer application is used to
simulate the stability eﬀects yS for all single point mutations. g) Bayesian scaling for the simulated values yS at the x-axis. Possible scalings are coloured
with green and the chosen scaling from yS into scaled values ˜yS is marked by black dots. The scaling is ﬁtted to a subset of experimentally measured
stabilities yE (circles). h) The stability predictive GP model is trained using experimental and simulated data through the kernel matrix.

2 Methods

Following Pires et al. (2014b) we choose a Bayesian model family of
Gaussian processes for prediction of mutation eﬀects on protein sta-
bility due to its inherent ability to handle uncertainty in a principle
way. Bayesian modelling is a natural approach for combining the ex-
perimental and simulated data distribution, while it is also suitable for
learning the underlying mixture of substitution models that governs the
mutational process.

The pipeline for mGPfusion is presented in Figure 1. The ﬁrst
part of mGPfusion consists of collection of in silico and experimental
datasets discussed in Section 2.1, the scaling of the in silico dataset in
Section 2.2 and the fusion of these two datasets in Section 2.3. The sec-
ond part consists of the Gaussian process model described in Section 2.4
with detailed description of the graph kernels in Sections 2.5-2.6 and
model inference in Section 2.7. Finally, the evaluation criteria used are
described in Section 2.8.

teins that fulﬁlled these requirements are listed in Table 1. We aver-
aged the stability values for proteins with multiple measurements and
ignored mutations to residues not present in their 3D structures. These
data sets are available at github.com/emmijokinen/mgpfusion.

Protein (organism)

PDB

mutations
point

point (sim)

all

2LZM 349
T4 Lysozyme (Enterobacteria phage T4)
182
1BNI
Barnase (Bacillus amyloliquefaciens)
124
1VQB
Gene V protein (Escherichia virus m13)
116
1LZI
Glycosyltransferase A (Homo sapiens)
98
2CI2
Chymotrypsin inhibitor 2 (Hordeum vulgare)
89
1PGA
Protein G (Streptococcus sp. gx7805)
83
2RN2
Ribonuclease H (Escheria coli)
80
1CSP
Cold shock protein B (Bacillus subtilis)
80
1BVC
Apomyoglobin (Physeter catodon)
63
4LYZ
Hen egg white lysozyme (Gallus gallus)
57
Ribonuclease A (Bos taurus)
1RTB
56
Peptidyl-prolyl cis-trans isomerase (Homo sapiens) 1PIN
53
1RN1
Ribonuclease T1 isozyme (Aspergillus oryzae)
54
1RGG
Ribonuclease (Streptomyces auerofaciens)
53
1BPI
Bovine pancreatic trypsin inhibitor (Bos taurus)

264
163
92
114
77
34
65
50
56
50
50
56
48
45
47

3116
2052
1634
2470
1235
1064
2945
1273
2907
2451
2356
2907
1957
1824
1102

2.1 Experimental and in silico data

total

1537

1211

31293

Protherm is a database of numerical thermodynamic parameters for
proteins and their mutants (Kumar et al., 2006). From Protherm we
gathered all proteins with at least 50 unique mutations whose ∆∆G
has been measured by thermal denaturation, and where a PDB code
for a 3D structure of the protein was reported. We required the pro-
teins to have at least 50 unique mutations, so that we would have a
representative test set and get suﬃciently reliable estimates of predic-
tion accuracy on individual proteins and examine how the amount of
experimental training data aﬀects the accuracy of the model. The 3D
structures are necessary for obtaining the connections between residues.
We collected the 3D structures with the reported PDB codes from the
Protein Databank, www.rcsb.org (Berman et al., 2000). The 15 pro-

Table 1: The 15 protein data from ProTherm database with counts of point
mutations, all mutations, and of simulated point mutation stability changes.

We also generate simulated data of the stability eﬀects of all possible
single mutations of the proteins. Our method can utilise any simulated
stability values. We used the “ddG monomer” application of Rosetta
3.6 (Leaver-Fay et al., 2011) using the high-resolution backrub-based
protocol 16 recommended in Kellogg et al. (2011). The predictions yS
made with Rosetta are given in Rosetta Energy Units (REU). Kellogg
et al. (2011) suggest transformation 0.57yS for converting the predic-
tions into physical units. The simulated data scaled this way is not
as accurate as the experimental data, the correlation and root mean

2

square error (rmse) with respect to the experimental data are shown
for all proteins in Table 2 and for individual proteins in Supplementary
Table S2, on rows labelled Rosetta. For this reason, we use instead a
Bayesian scaling described in the next section and diﬀerent noise models
for the experimental and simulated data, described in Section 2.3.

For each of the 15 proteins,

let xi = (xi1, . . . , xiM ) denote its
M -length variant i with positions p labelled with residues xip ∈
{A, R, N, . . . , V }. We denote the wild-type protein as x0. We collect
15 separate sets of simulated and experimental data. We denote the
NE )T with
NE experimental variants of each protein as XE = (xE
the corresponding experimental stability values yE = (yE
NE )T ∈
RNE . Similarly, we denote the NS simulated observations as XS =
(xS

1 , . . . , xE

1 , . . . , yE

NS )T and yS = (yS

NS )T ∈ RNS .

1 , . . . , xS

1 , . . . , yS

2.2 Bayesian scaling of in silico data

The described transformation from REU to physical units may not
be optimal for all proteins. We therefore applied instead a linear-
exponential scaling function to obtain scaled Rosetta simulated sta-
bilities ˜yS,

˜yS = g(yS | θj) = ajecj yS

+ bjyS + dj.

(1)

This scaling transforms the Rosetta simulations yS for each protein
j = 1, . . . , 15 to correspond better to the experimental data. The pa-
rameters θj = (aj, bj, cj, dj) deﬁne the weight aj and steepness cj of the
exponential term, while the linear term has slope bj and intercept dj.
To avoid overﬁtting, we perform Bayesian linear regression and start
by deﬁning parameter prior p(θj) = p(aj)p(bj)p(cj)p(dj) that reﬂects
our beliefs about realistic scalings having only moderate steepness:

and stabilities y = (y0, yE, ˜yS) of size RN where N = 1 + NE + NS is
the total number of simulated and experimental data points, including
the wild-type. We assume heteroscedastic additive noise models for the
three information sources

y0 = f (x0) + ε0,
(cid:1) + εE
i = f (cid:0)xE
yE
i ,
(cid:1) + εS
i = f (cid:0)xS
˜yS
i ,

i

i

(cid:1)

(cid:1)

0

ε0 ∼ N (cid:0)0, σ2
i ∼ N (cid:0)0, σ2
εE
(cid:16)
εS
i ∼ N

E

0, (σE + σS + tσT (i))2(cid:17)

,

(5)

where the observed values are noisy versions of the underlying ‘true’
stability function f (x) corrupted by zero-mean noise with data source
speciﬁc variances. We learn a Gaussian process based stability function
f (x) in the next Section.

The Equations (5) encode that the experimental data are corrupted
by a global experimental noise variance σ2
E. The simulated stabilities
are additionally corrupted by a global Rosetta simulator error variance
σ2
S, and by the value-dependent transformation variance tσ2
T (i) scaled
by parameter t. The model then encapsulates that we trust the Rosetta
data less than the experimental data. By deﬁnition, the ∆∆G of the
wild-type is zero (y0 = 0) with very small assumed error, σ0 = 10−6.
Note that σ2
T are ﬁxed by equation (4), while we infer the optimal
values for the remaining three free parameters (σE, σR, t) (See Section
2.4). The parameters σ2

S are assigned priors

E and σ2

σE ∼ Gamma(σE|αE, βE)
σS ∼ Gamma(σR|αS, βS).

(6)

The values of these hyperparameters are shown in Supplementary Ta-
ble S1.

p(aj) = Gamma(aj | αa, βa)
p(bj) = Beta(1/2 · bj | αb, βb)
p(cj) = Beta(10/3 · cj | αc, βc)
p(dj) = N (dj | µd, σ2

d).

2.4 Gaussian processes

(2)

We use a Gaussian process (GP) function f to predict the stability
f (x) ∈ R of a protein variant x. Gaussian processes are a family of
non-parametric, non-linear Bayesian models (Rasmussen and Williams,
2006). A zero-mean GP prior

The empirically selected hyperparameter values are listed in Supple-
mentary Table S1 and the priors are illustrated in Figure S2.

We compute the posterior for θj using the subset of simulated data

that have corresponding experimentally measured data:

p(θj|yE, yS) ∝

(cid:89)

N (cid:0)yE

i

| g(yS

i |θj), σ2
n

(cid:1) p(θj).

i:xi∈XE ∩XS

The product iterates over all NE∩S simulated ∆∆G’s that have a
matching experimentally observed value. The σ2
n is the scaling error
variance, which was set to σ2
n = 0.5. The parameters θ for each protein
were sampled using a random walk Metropolis-Hastings MCMC algo-
rithm (the mhsample function in Matlab) for NM C = 10000 samples
with a burn-in set to 500. The proposal distribution was selected to be
a symmetric uniform distribution such that [as+1, bs+1, cs+1, ds+1] ∼
U (as ± 0.4, bs ± 0.04, cs ± 0.04, ds ± 0.4). Given the sample of scaling
parameters (θ(s)
s=1 , we deﬁne the scaled simulated data as the aver-
age scaling over the MCMC sample, and record also the sample scaling
variance

j )NM C

˜yS
i =

g(yS

i |θ(s)
j )

1
NM C

1
NM C

NM C(cid:88)

s=1
NM C(cid:88)

(cid:16)

s=1

σ2
T (i) =

g(yS

i |θ(s)

j ) − ˜yS
i

(cid:17)2

.

See Figure 1 g) for an illustration of the scaling. We collect the scaled
simulated value and its variance from each simulated point into vectors
˜yS = (˜yS

T (NS)) ∈ RNS .

T (1), . . . , σ2

NS ) and σ2

1 , . . . , ˜yS

T = (σ2

deﬁnes a distribution over functions f (x) whose mean and covariance
are

f (x) ∼ GP (0, k(x, x(cid:48))) ,

E[f (x)] = 0
cov[f (x), f (x(cid:48))] = k(x, x(cid:48)).

For any collection of protein variants X = x1, . . . , xN , the function val-
ues follow a multivariate normal distribution f ∼ N (0, KXX ), where
f = (f (x1), . . . , f (xN ))T ∈ RN , and where KXX ∈ RN ×N with
[KXX ]ij = k(xi, xj). The key property of Gaussian processes is that
they encode functions that predict similar stability values f (x), f (x(cid:48))
for protein variants x, x(cid:48) that are similar, as encoded by the kernel
k(x, x(cid:48)). The key part of GP modelling is then to infer a kernel that
measures the mutation’s eﬀects to the stability.

Let a dataset of noisy stability values from two sources be y ∈ RN ,
the corresponding protein structures X = (xi)N
i=1, and a new protein
variant x∗ whose stability we wish to predict. A Gaussian process de-
ﬁnes a joint distribution over the observed values y of variants X, and
the unknown function value f (x∗) of the unseen variant x∗,
(cid:21)(cid:19)

(cid:18)

(cid:21)

(cid:20) y
f (x∗)

∼ N

0,

(cid:20)KXX + diag(σ2)
k∗X

kX∗
k(x∗, x∗)

,

∗X ∈ RN is a kernel vector with elements k(xi, x∗) for
where kX∗ = kT
T )2)T
all i = 1, . . . , N , and where σ2 = (σ2
collects ﬁnal variances of the data points from equations (5). Here
the exponents are elementwise. The conditional distribution gives the
posterior distribution of the stability prediction as

E1T , (σE1T + σS1T + tσT

0, σ2

f (x∗)|(X, y) ∼ N (cid:0)µ(x∗), σ2(x∗)(cid:1) ,

2.3 Data fusion and noise models

where the prediction mean and variance are

For each protein j, we organise its experimental data (XE, yE) and
transformed simulated data (XS, ˜yS) along with the wild-type infor-
mation (x0, y0) into a single joint dataset of variants X = (x0, XE, XS)

(cid:0)KXX + diag(σ2)(cid:1)−1 y,

µ(x∗) = k∗X
σ2(x∗) = k(x∗, x∗) − k∗X

(cid:0)KXX + diag(σ2)(cid:1)−1 kX∗.

(3)

(4)

3

Hence, in GP regression the stability predictions µ(x∗) ± σ(x∗) will
come with uncertainty estimates.

ring amino acids and scaled them between zero and one as

2.5 Graph kernel

Next, we consider how to compute the similarity function k(x, x(cid:48)) be-
tween two variants of the same protein structure. We will encode the
3D structural information of the two protein variants as a contact map
and measure their similarity by the formalism of graph kernels (Vish-
wanathan et al., 2010).

We consider two residues to be in contact if their closest atoms are
within 5 ˚A of each other in the PDB structure, which is illustrated in
Figure 1 b). All variants of the same protein have the same length, with
only diﬀerent residues at mutating positions. Furthermore, we assume
that all variants share the wild-type protein contact map.

To compare protein variants, we construct a weighted decomposition
kernel (WDK) (Menchetti et al., 2005) between two protein variants
x = (x1, . . . , xM ) and x(cid:48) = (x(cid:48)

M ) of length M ,

1, . . . , x(cid:48)

k(x, x(cid:48)) =


S(xp, x(cid:48)
p)

M
(cid:88)

p=1



(cid:88)

S(xl, x(cid:48)
l)

,

l∈nbs(p)

(7)

where nbs(p) deﬁnes the set of neighbouring positions to position p,
and S is a substitution matrix. The kernel iterates over all positions
p and compares for each of them their residues through a substitution
matrix S(xp, x(cid:48)
p). Furthermore, the similarity of the residues at each
position is multiplied by the average similarity of the residues at its
neighbouring positions S(xl, x(cid:48)
l). Hence, the kernel deﬁnes the simi-
larity of two protein variants as the average position and neighbour-
hood similarity over all positions. The kernel matrix is normalised
so that for two data points, the normalised kernel
p) =
k(xp, x(cid:48)
p, x(cid:48)
p), as deﬁned by Shawe-Taylor and Cris-
tianini (2004). The kernel is illustrated in Figure 1 c).

k(xp, xp)k(x(cid:48)

is ˆk(xp, x(cid:48)

p)/

(cid:113)

The above WDK kernel allows us to compare the eﬀects of multiple
simultaneous mutations. However, as the wild type protein structure
is used for all of the protein variants, changes that the mutations may
cause to the protein structure are not taken into consideration. This
may cause problems if mutations that alter the protein structure sig-
niﬁcantly are introduced – especially if many of them are introduced
simultaneously. On the other hand, substitution matrices that have
their basis in sequence comparisons, should take these eﬀects into ac-
count to some extend as these kinds of mutations are usually highly
destabilising and do not occur often in nature. In the next section, we
will discuss how we utilise diﬀerent substitution matrices with multiple
kernel learning.

The BLOSUM substitution models have been a common choice for
protein models (Giguere et al., 2013), while mixtures of substitution
models were proposed by Cichonska et al. (2017). BLOSUM matrices
score amino acid substitutions by their appearances throughout evolu-
tion, as they compare the frequencies of diﬀerent mutations in similar
blocks of sequences (Henikoﬀ and Henikoﬀ, 1992). However, there are
also diﬀerent ways to score amino acids substitutions, such as chemical
similarity and neighbourhood selectivity (Tomii and Kanehisa, 1996).
When the stability eﬀects of mutations are evaluated, the frequency of
an amino acid substitution in nature may not be the most important
factor.

To take into account diﬀerent measures of similarity between amino
acids, we employed a set of 21 amino acid substitution matrices gath-
ered from AAindex21 (Kawashima et al., 2008). AAindex2 currently
contains 94 substitution matrices. From these we selected those that
had no gaps concerning substitutions between the 20 naturally occur-

1http://www.genome.jp/aaindex/
2http://www.cs.ubc.ca/~schmidtm/Software/minConf.html

4

S =

S0 − min(S0) + 1
max(S0) − min(S0) + 1

.

Out of these matrices, we only chose those 23 matrices that were pos-
itive semideﬁnite. Furthermore, there were two pairs of matrices that
were extremely similar, and we only selected one matrix from each pair,
ending up with 21 substitution matrices. These substitution matrices
are used together with Equation 7 for computing 21 base kernel matri-
ces. Finally, MKL is used to ﬁnd an optimal combination of the base
kernels of form

(8)

(9)

Kφ =

wmK (γm)
m ,

21
(cid:88)

m=1

where wm is a kernel speciﬁc weight, γm is an (elementwise) exponent.
The elementwise exponent retains the SDP property of Kφ (Shawe-
Taylor and Cristianini, 2004). We observe empirically that the optimal
kernel weights wm tend to be sparse (See Figure 2).

The selected substitution matrices are listed in Figure 2. These ma-
trices have diﬀerent basis and through multiple kernel learning (MKL)
our model learns which of these are important for inferring the stability
eﬀects that mutations cause on diﬀerent proteins. The ﬁgure illustrates
this by showing the average weights of the base kernel matrices obtained
via the multiple kernel learning.

2.7 Parameter inference

The complete model has ﬁve parameters φ = (σE, σS, t, w, γ) to in-
fer, of which the variance parameters (σE, σS, t) parameterise the joint
data variance σ2
φ, while the MKL parameters w = (w1, . . . , w21) and
γ = (γ1, . . . , γ21) parameterise the kernel matrix Kφ. In a Gaussian
process model these can be jointly optimised by the marginal (log)
likelihood with priors

log p(y|φ)p(σE)p(σR) = log

p(y|f , φ)p(f |φ)p(σE)p(σR)df

(cid:90)

∝ −

yT (Kφ + diag(σ2

1
2
+ log Gamma(σE|αE, βE) + log Gamma(σS|αS, βS),

log |Kφ + diag(σ2

φ))−1y −

φ)|

1
2

(10)

which automatically balances model ﬁt (the square term) and the
model complexity (the determinant) to avoid overﬁtting (Rasmussen
and Williams, 2006). The parameters can be optimised by maximis-
ing the marginal log likelihood (10) using gradient ascent, since the
marginal likelihood can be diﬀerentiated analytically (see Supplemen-
tary Equations S1 and S2). We utilised a limited-memory projected
quasi-Newton algorithm (minConf TMP2), described by (Schmidt et al.,
2009).

We chose to evaluate the accuracy of our predictions using the same
metrics that have been used by many others – correlation ρ between the
predicted and experimentally measured ∆∆G values (Capriotti et al.,
2005a; Dehouck et al., 2009; Kellogg et al., 2011; Pires et al., 2014b;
Potapov et al., 2009) and the root mean square error (rmse) (Dehouck
et al., 2009; Pires et al., 2014a,b), which are determined in the Supple-
mentary Equations S3 and S4. We use marginal likelihood maximisa-
tion to infer model parameters and perform cross-validation to evaluate
the model performance on test data. Below we only report evaluation
metrics obtained from the test sets not used at any stage of the model
learning or data transformation sampling.

3 Results

In this section we evaluate the performance of mGPfusion on predict-
ing stability eﬀects of mutations, and compare it to the state-of-the-
art prediction methods mCSM, PoPMuSiC and Rosetta. Rosetta is a

2.6 Substitution matrices and multiple kernel

2.8 Evaluation criteria

learning

Figure 2: Average weights for kernels utilising the described substitution matrices from AAindex2, when GP models were trained with mutation level
cross-validation. Basis for the substitution matrices are obtained from (Tomii and Kanehisa, 1996). ∗ were added to AAindex2 in a later release, and their
basis were not determined by Tomii and Kanehisa (1996).

molecular modelling software whose ddg monomer module can directly
simulate the stability changes ∆∆G of a protein upon mutations. PoP-
MuSic and mCSM are machine learning models that predict stability
based on protein variant features. We run Rosetta locally, and use
mCSM and PoPMuSiC models through their web servers3. This may
give these methods an advantage over mGPfusion since parts of our
testing data were likely used within their training data.

We compare four diﬀerent variants of our method: mGPfusion that
uses both simulated data and MKL, “mGPfusion, only B62” that uses
simulated data but incorporates only one kernel matrix (BLOSUM62
substitution matrix), mGP model that uses MKL but does not use simu-
lated data, and “mGP, only B62” that uses only the base GP model but
does not incorporate simulated data and uses only the BLOSUM62 sub-
stitution matrix. In addition, we experiment on transforming Rosetta
predictions with the Bayesian scaling. We perform the experiments
for the 15 proteins separately using either position or mutation level
(leave-one-out) cross-validation regarding the methods mGP, mGPfu-
sion and the Bayesian scaling of Rosetta. Pires et al. (2014b) used
protein and position level cross-validation to evaluate their model. In
protein level cross-validation all mutations in a protein are either in
the test or training set exclusively. When we train our model using
protein level cross-validation, we use no experimental data and rely
only on the simulated data. Position level cross-validation is deﬁned
so that all mutations in a position are either in the test or training set
exclusively. However, datasets in Pires et al. (2014b) contained only
point mutations and therefore we had to extend the deﬁnition to also
include multiple mutations. In position level cross-validation we train
one model for each position using only the part of data that has a
wild-type residue in that position. Therefore, in position level cross-
validation we construct a test set that contains all protein variants that
have a mutation at position p and use as training set all the protein
variants that have a wild-type residue at that position. Dehouck et al.
(2009) evaluated their models by randomly selecting training and test
sets so that each mutation was exclusively in one of the sets, but both
sets could contain mutations from the same position of the same pro-

3biosig.unimelb.edu.au/mcsm and

omictools.com/popmusic-tool

tein. We call this mutation level cross-validation. When we use all
available experimental data with mutation level cross-validation, this
corresponds to leave-one-out cross-validation.

3.1 Predicting point mutations

Table 2 summarises the average prediction performance over all 15 pro-
teins for all compared methods, types of mutations and cross-validation
types. We ﬁrst compare the performances on single point mutations,
where mGPfusion and mGP achieve the highest performance with
ρ = 0.81 and rmse = 1.07 kcal/mol, and ρ = 0.81 and rmse = 1.04
kcal/mol, respectively with mutation level cross-validation. With only
one kernel utilising the BLOSUM62 matrix instead of MKL, the per-
formance decreases slightly, but the competing methods are still out-
performed, as mCSM achieves ρ = 0.64 and rmse = 1.37 kcal/mol,
PoPMuSic ρ = 0.61 and Rosetta ρ = 0.55. Applying Bayesian scaling
on Rosetta simulator improves the performance of standard Rosetta
from ρ = 0.55 to ρ = 0.65 and decreases the rmse from 1.63 kcal/mol
to 1.35 kcal/mol, which is interestingly even slightly better than the
performances of mCSCM and PoPMuSiC.

With position level cross-validation mGPfusion achieves the high-
est performance of ρ = 0.70 and rmse = 1.26 kcal/mol, likely due
to having still access to simulated variants from that position, since
they are always available to the learner. Without simulation data,
the baseline machine learning model mGP performance decreases to
ρ = 0.51 and rmse = 1.54 kcal/mol, thus demonstrating the impor-
tance of the data fusion. Cross-validation could not be performed for
the oﬀ-the-shelf methods mCSM and PoPMuSiC. Even still, mGPfusion
(trained with one or multiple kernels) outperforms competing state-of-
the-art methods and achieves markedly higher prediction performance
as quantiﬁed by both mutation and position level cross-validations.
Also mGP outperforms these methods when quantiﬁed by mutation
level cross-validation. With protein level cross-validation mGPfusion
achieves slightly better results than Rosetta.

5

Method
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled

Point mutations
cross-validation level
prot.
pos.
mut.
0.56
0.70
0.81
0.56
0.69
0.79
-
0.51
0.81
-
0.34
0.76
-
0.63
0.65

Correlation ρ
Multiple mutations
cross-validation level
prot.
pos.
mut.
0.49
0.61
0.88
0.50
0.64
0.86
-
0.52
0.86
-
0.55
0.86
-
0.39
0.51

All mutations
cross-validation level
prot.
pos.
mut.
0.52
0.64
0.83
0.52
0.66
0.82
-
0.50
0.83
-
0.49
0.80
-
0.48
0.60

Point mutations
cross-validation level
prot.
pos.
mut.
1.61
1.26
1.07
1.62
1.30
1.11
-
1.54
1.04
-
1.95
1.26
-
1.38
1.35

rmse
Multiple mutations
cross-validation level
prot.
pos.
mut.
2.53
2.45
1.33
2.50
2.40
1.43
-
2.65
1.44
-
2.56
1.45
-
2.99
2.49

All mutations
cross-validation level

mut.
1.13
1.18
1.14
1.30
1.66

pos.
1.87
1.85
2.09
2.23
2.22

prot.
1.84
1.84
-
-
-

Predictions from oﬀ-the-shelf implementations with no cross-validation

Rosetta
mCSM
PoPMuSiC

0.55
0.61
0.64

0.40
-
-

0.49
-
-

1.63
1.40
1.37

2.74
-
-

1.92
-
-

Table 2: Comparison of diﬀerent methods on the 15 protein dataset with respect to ρ and rmse. Mutation, position, and protein are referred to as mut.,
pos., and prot., respectively. Predictions from oﬀ-the-shelf implementations of Rosetta, mCSM and PoPMuSiC are used directly without cross-validation.

3.2 Predicting multiple mutations

Next, we tested stability prediction accuracies for variants containing
either single or multiple mutations. Figure 3 shows a scatter plot of
mGPfusion predictions for all 1537 single and multiple mutation vari-
ants (covering all 15 proteins) against the experimental ∆∆G values
using the mutation level (leave-one-out) cross-validation. The points
are coloured by the number of simultaneous mutations in the variants,
with 326 variants having at least 2 mutations (See Table 1). Fig-
ure 3 illustrates the mGPfusion’s overall high accuracy of ρ = 0.83
and rmse = 1.13 kcal/mol on both single and multiple mutations (See
Table 2). Scatter plots for the individual proteins can be found in Sup-
plementary Figure S3. Dehouck et al. (2009) suggested that considering
the predictive power after removal of most badly predicted stability ef-
fects of mutations may give more relevant evaluation, as some of the
experimental measurements may have been made in non-physiological
conditions or aﬀected by signiﬁcant error, associated with a poorly re-
solved structure, or indexed incorrectly in the database. They thus
reported correlation and rmse of the predictions after excluding 10 %
of the predictions with most negative impacts on the correlation co-
eﬃcient. Pires et al. (2014b) also reported their accuracy after 10 %
outlier removal. If we remove the 10% worst predicted stability eﬀects
from the combined predictions, we achieve correlation ρ of 0.92 and
rmse of 0.67 kcal/mol. We report these results for all the methods
in Supplementary Table S3 and also present the error distribution in
Supplementary Figure S5.

Figure 3: Scatter plot for the mutation level (leave-one-out) predictions made
for all 15 proteins (See Table 1). The colour indicates the number of simul-
taneous mutations.

The high accuracy is retained for variants with multiple mutations
as well (ρ = 0.88 and rmse = 1.33 kcal/mol, see Tables 2 and S2).
Table 3 lists mGPfusion’s rmse for diﬀerent number of simultaneous
mutations. The model accuracy in fact improves up to 6 mutations.
This is explained by the training set often containing the same single
point mutations that appear in variants with multiple mutations. The
model can then infer the combined eﬀect of pointwise mutations. The
model seems to fail when predicting the eﬀects of 7-9 simultaneous mu-

6

tations. Most of these mutations (8/12) are for Ribonuclease (1RGG)
and their eﬀects seem to be exceptionally diﬃcult to predict. This may
be because only few of the point mutations that are part of the multiple
mutations are present in the training data. However, these mutations
seem to be exceptionally diﬃcult to predict for Rosetta as well, which
could indicate that the experimental measurements concerning these
mutations are not quite accurate. PoPMuSiC and mCSM are unable
to predict multiple mutations, while Rosetta supports them, but its
rmse accuracy decreases already with two mutations.

mutations
occurences

mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta

1
1211

1.07
1.11
1.04
1.26
1.35
1.63

2
207

1.06
1.12
1.03
0.96
2.10
2.27

3
52

0.80
0.77
0.61
0.65
1.92
2.11

4
42

0.51
0.59
0.50
0.83
2.94
3.78

5
4

0.40
0.29
0.18
0.26
2.29
2.93

6
8

1.01
1.14
0.92
1.14
2.32
2.21

7
3

3.02
3.00
3.23
2.95
2.93
2.92

8
3

5.89
6.78
6.18
6.90
6.75
5.80

9
6

5.16
5.56
6.75
6.57
7.28
7.45

10
1

0.25
0.11
0.08
0.05
2.69
3.42

Table 3: Root-mean-square errors for diﬀerent number of simultaneous
mutations for all 15 proteins, with models trained by leave-one-out cross-
validation. Rosetta is added for comparison.

With multiple mutations, the decrease in performance between the
position and mutation level cross-validations becomes clearer than with
single mutations. With the position level cross-validation the stability
eﬀects of multiple mutations are predicted multiple times, which partly
explains this loss of accuracy. For example, the eﬀects of mutants with
nine diﬀerent simultaneous mutations, which were the most diﬃcult
cases in the mutation level cross-validation, are predicted nine times.
Surprisingly, mGPfusion trained with protein level cross-validation
achieves higher correlation and smaller errors than Rosetta; mGPfusion
utilising simulated ∆∆G values for only single mutations, can predict
the stability eﬀects of multiple mutations better than Rosetta.

3.3 Uncertainty of the predictions

Gaussian processes provide a mean µ(x) and a standard deviation σ(x)
for the stability prediction of a protein variant x. The standard de-
viation allows estimation of the prediction accuracy even without test
data. Figure 1 h) visualises the uncertainty of a few predictions made
for the protein G (1PGA) when mutation level cross-validation is used.
The estimated standard deviation allows a user to automatically iden-
tify low quality predictions that can appear e.g. in parts of the input
protein space from which less data is included in model training. Con-
versely, in order to minimise the amount of uncertainty in the mGPfu-
sion predictions, estimated standard deviation can be used to guide next
experiments. The probabilistic nature of the predictions also admits an
alternative error measure of negative log probability density (NLPD)
nlpd = − (cid:80)N
i=1 log p(yi|µ(xi), σ2(xi)), which can naturally take into
account the prediction variance.

3.4 Eﬀect of training set size

The results presented in Sections 3.1–3.3 used all available data for
training with cross-validation to obtain unbiased performance mea-
sures. The inclusion of thousands of simulated variants allows the model

to learn accurate models with less experimentally measured variants.
Hence, we study how the mGPfusion model with or without simulated
data performs with reduced number of experimental observations. To
facilitate this, we randomly selected subsets of experimental data of
size 0, 10, 20, and so on. We learned the mGP and mGPfusion models
with these reduced experimental data sets while always using the full
simulated data sets. This also allows us to estimate how the models
work with diﬀerent number of cross-validation folds. For example, the
point of a learning curve which utilises 2/3 or 4/5 of the training data
correspond to an average of multiple 3-fold or 5-fold cross-validations,
respectively.

The learning curve in Figure 4a) shows how the averaged correlation
for protein 2LZM improves when the size of the experimental data set
increases. The right-most values at N = 348 are obtained with leave-
one-out cross-validation. The inclusion of simulated data in mGPfusion
(dark blue line) consistently improves the performance of mGP, which
is trained without simulated data. Figure 4b) illustrate the diﬀerence
in root mean square error. Learning curves for all proteins listed in
Table 1 can be found from the Supplementary Figures S6-S8. When
the number of experimental samples is zero, the mGPfusion model is
trained solely using the simulated data with scaling 0.57yS, and the
mGP model predicts the stability eﬀect of every mutation as zero. The
last point on the learning curves is obtained with mutation level cross-
validation (see Tables 2 and S2).

proteins mGPfusion model trained with only one kernel that utilises
BLOSUM62, provides approximately as good results as the mGPfu-
sion model trained with multiple kernels. However, with many of the
proteins, utilising just BLOSUM62 does not seem to be suﬃcient and
the accuracy of the model can be improved by using diﬀerent substi-
tution matrices. Prior knowledge of appropriate substitution models
for each protein could enable creation of accurate prediction models
with just one substitution model, but the MKL seems to be a good
tool for selecting suitable substitution models when such knowledge is
not available. It seems that the data fusion and number or relevance
of used substitution matrices can compensate each other – the learning
curves show, that the diﬀerence between mGPfusion models trained
with one or multiple kernels is smaller than the diﬀerence between the
mGP models utilising one or multiple kernels. This indicates that if ad-
ditional simulated data is exploited, the use of multiple or appropriate
substitution models is not as important than without the data fusion.
On the other hand, if data fusion is not applied, the use of MKL can
more signiﬁcantly improve the accuracy of the mGP model.

3.6 Eﬀect of

the Bayesian transformation on

Rosetta

The Bayesian scaling of simulated Rosetta values, proposed in Sec-
tion 2.2, improves the match of Rosetta simulated values to empirical
∆∆G values even without using the Gaussian process framework. The
Bayesian scaling improves the performance of standard Rosetta sim-
ulations from ρ = 0.55 and rmse = 1.63 kcal/mol to ρ = 0.65 and
rmse = 1.35 kcal/mol (see Table 2 and Supplementary Table S2). This
shows that the scaling proposed by Kellogg et al. (2011) indeed is not
always the optimal scaling and signiﬁcant improvements can be gained
by optimising the scaling using a set of training data.

Figure 1 g) visualises the Bayesian scaling for protein 1PGA, where
the very destabilising ∆∆G values are dampened by the scaling (black
dots) to less extreme values by matching the scaled simulated values to
the experimental points (blue circles). The black dots along the scaling
curve indicate the grid of point mutations after transformation. The
scaling variance σ2
T is indicated by the green region’s vertical width,
and on the right panel. The scaling tends to dampen very small values
into less extreme stabilities, while it also estimates higher uncertainties
for stability values further away from ∆∆G = 0. However, the scalings
vary between diﬀerent proteins, as can be seen from the transformations
for each of the 15 proteins presented in Supplementary Figure S9.

Figure 4: a) Correlation and b) root mean square error of predictions made
by models with diﬀerent number of experimental training samples for T4
Lysozyme (2LZM). The results of Rosetta, mCSM and PoPMuSiC are in-
variant to training data (because mCSM and PoPMuSiC are pre-trained),
and are thus constant lines. For both ﬁgures, an average of 100 randomly
selected training sets is taken at each point.

3.5 Eﬀect of data fusion and multiple substitution

matrices

In the beginning of the learning curves, when only little training data is
available, mGPfusion quite consistently outperforms the mGP model,
demonstrating that the additional simulated data improves the predic-
tion accuracy. However, when more training data becomes available,
the performance of mGP model is almost as good or sometimes even
better than the performance of the mGPfusion model. This shows that
if enough training data is available, it is not necessary to simulate addi-
tional data in order to obtain accurate predictions. Table 2 also shows,
that the data fusion can compensate the lack of relevant training data
– with the mGPfusion models that utilise the additional data, the de-
crease in accuracy is smaller when position level cross-validation is used
instead of mutation level cross-validation, than with the mGP models.
The varying weights for the base kernels between diﬀerent pro-
teins (shown in Figure 2) already illustrated that diﬀerent proteins
beneﬁt from diﬀerent similarity measures for amino acid substitutions.
The learning curves also support this observation – with some of the

4 Conclusions

We present a novel method mGPfusion for predicting stability eﬀects of
both single and multiple simultaneous mutations. mGPfusion utilises
structural information in form of contact maps and integrates that with
amino acid residues and combines both experimental and comprehen-
In con-
sive simulated measurements of mutations’ stability eﬀects.
trast to earlier general-purpose stability models, mGPfusion model is
protein-speciﬁc by design, which improves the accuracy but necessitates
having a set of experimental measurements from the protein. In prac-
tise small datasets of 10–20 experimental observations were found to
provide state-of-the-art accuracy models when supplemented by large
simulation datasets.

An important advantage over most state-of-the-art machine learn-
ing methods is that mGPfusion is able to predict the eﬀects of multiple
simultaneous mutations in addition to single point mutations. Our
experiments show that mGPfusion is reliable in predicting up to six
simultaneous mutations in our dataset. Furthermore, the Gaussian
process framework provide a way to estimate the (un)certainty of the
predictions even without a separate test set. We additionally proposed
a novel Bayesian scaling method to re-calibrate simulated protein sta-
bility values against experimental observations. This is a crucial part of
the mGPfusion model, and also alone improved protein-speciﬁc Rosetta
stability predictions by calibrating them using experimental data.

mGPfusion is best suited for a situation, where a protein is thor-
oughly experimented on and accurate predictions for stability eﬀects

7

upon mutations are needed. It takes some time to set up the frame-
work and train the model, but after that new predictions can be made
in fractions of a second. The most time-consuming part is running the
simulations with Rosetta, at least when the most accurate protocol 16
is used. Simulating all 19 possible point mutations for one position
took about 12 hours, but simulations for diﬀerent positions can be run
on parallel. The time needed for training the prediction model depends
on the amount of experimental and simulated training data. With no
simulated data, the training time ranged from few seconds to few min-
utes. With data fusion and a single kernel, the training time was under
an hour. With data fusion and MKL with 21 kernels, the training time
was from a few minutes to a day.

Acknowledgements

We acknowledge the computational resources provided by the Aalto
Science-IT.

Funding

This work has been supported by the Academy of Finland Center of Ex-
cellence in Systems Immunology and Physiology, the Academy of Fin-
land grants no. 260403 and 299915, and the Finnish Funding Agency
for Innovation Tekes (grant no 40128/14, Living Factories).
References

Alberts, B., Johnson, A., Lewis, J., Raﬀ, M., Roberts, K., and Walter, P. (2007). Molec-

ular biology of the cell. Garland Science, 5 edition.

Anslyn, E. V. and Dougherty, D. A. (2006). Modern physical organic chemistry. Uni-

versity Science Books.

Berman, H. M., Westbrook, J., Feng, Z., Gilliland, G., Bhat, T., Weissig, H., Shindyalov,
I. N., and Bourne, P. E. (2000). The protein data bank. Nucleic acids research, 28(1),
235–242.

Bommarius, A. S., Blum, J. K., and Abrahamson, M. J. (2011). Status of protein en-
gineering for biocatalysts: how to design an industrially useful biocatalyst. Current
opinion in chemical biology, 15(2), 194–200.

Branden, C. and Tooze, J. (1999). Introduction to protein structure. Garland, 2 edition.

Giollo, M., Martin, A. J., Walsh, I., Ferrari, C., and Tosatto, S. C. (2014). NeEMO: a
method using residue interaction networks to improve prediction of protein stability
upon mutation. BMC genomics, 15(4), 1.

Henikoﬀ, S. and Henikoﬀ, J. G. (1992). Amino acid substitution matrices from protein
blocks. Proceedings of the National Academy of Sciences, 89(22), 10915–10919.

Kawashima, S., Pokarowski, P., Pokarowska, M., Kolinski, A., Katayama, T., and Kane-
hisa, M. (2008). AAindex: amino acid index database, progress report 2008. Nucleic
acids research, 36(suppl 1), D202–D205.

Kellogg, E. H., Leaver-Fay, A., and Baker, D. (2011). Role of conformational sampling
in computing mutation-induced changes in protein structure and stability. Proteins:
Structure, Function, and Bioinformatics, 79(3), 830–838.

Kirk, O., Borchert, T. V., and Fuglsang, C. C. (2002). Industrial enzyme applications.

Current opinion in biotechnology, 13(4), 345–351.

Kumar, M. S., Bava, K. A., Gromiha, M. M., Prabakaran, P., Kitajima, K., Uedaira,
H., and Sarai, A. (2006). ProTherm and ProNIT: thermodynamic databases for pro-
teins and protein–nucleic acid interactions. Nucleic Acids Research, 34(suppl 1),
D204–D206.

Leaver-Fay, A., Tyka, M., Lewis, S. M., Lange, O. F., Thompson, J., Jacak, R., Kaufman,
K., Renfrew, P. D., Smith, C. A., Sheﬄer, W., et al. (2011). ROSETTA3: an object-
oriented software suite for the simulation and design of macromolecules. Methods in
enzymology, 487, 545.

Liu, J. and Kang, X. (2012). Grading amino acid properties increased accuracies of single

point mutation on protein stability prediction. BMC bioinformatics, 13(1), 1.

Menchetti, S., Costa, F., and Frasconi, P. (2005). Weighted decomposition kernels. In
Proceedings of the 22nd international conference on Machine learning, pages 585–
592. ACM.

Pace, C. N. and Scholtz, J. M. (1997). Measuring the conformational stability of a pro-

tein. Protein structure: A practical approach, 2, 299–321.

Pace, C. N. and Shaw, K. L. (2000). Linear extrapolation method of analyzing solvent
denaturation curves. Proteins: Structure, Function, and Bioinformatics, 41(S4),
1–7.

Pires, D. E., Ascher, D. B., and Blundell, T. L. (2014a). DUET: a server for predicting
eﬀects of mutations on protein stability using an integrated computational approach.
Nucleic acids research, page gku411.

Pires, D. E., Ascher, D. B., and Blundell, T. L. (2014b). mCSM: predicting the eﬀects of
mutations in proteins using graph-based signatures. Bioinformatics, 30(3), 335–342.

Potapov, V., Cohen, M., and Schreiber, G. (2009). Assessing computational methods for
predicting protein stability upon mutation: good on average but not in the details.
Protein Engineering Design and Selection, 22(9), 553–560.

Capriotti, E., Fariselli, P., and Casadio, R. (2005a). I-Mutant2.0: predicting stability
changes upon mutation from the protein sequence or structure. Nucleic acids research,
33(suppl 2), W306–W310.

Rapley, R. and Walker, J. M. (2000). Molecular Biology and Biotechnology. Royal

Society of Chemistry, 4 edition.

Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian processes for machine learn-

Capriotti, E., Fariselli, P., Calabrese, R., and Casadio, R. (2005b). Predicting pro-
tein stability changes from sequences using support vector machines. Bioinformatics,
21(suppl 2), ii54–ii58.

ing. The MIT Press.

Capriotti, E., Fariselli, P., Rossi, I., and Casadio, R. (2008). A three-state prediction of

single point mutations on protein stability changes. BMC bioinformatics, 9(2).

Chen, C.-W., Lin, J., and Chu, Y.-W. (2013). iStable: oﬀ-the-shelf predictor integration

for predicting protein stability changes. BMC bioinformatics, 14(2).

Cheng, J., Randall, A., and Baldi, P. (2006). Prediction of protein stability changes for
single-site mutations using support vector machines. Proteins: Structure, Function,
and Bioinformatics, 62(4), 1125–1132.

Cherry, J. R. and Fidantsef, A. L. (2003). Directed evolution of industrial enzymes: an

update. Current opinion in biotechnology, 14(4), 438–443.

Cichonska, A., Ravikumar, B., Parri, E., Timonen, S., Pahikkala, T., Airola, A., Wenner-
berg, K., Rousu, J., and Aittokallio, T. (2017). Computational-experimental approach
to drug-target interaction mapping: A case study on kinase inhibitors. PLoS compu-
tational biology, 13(8), e1005678.

Sanchez, S. and Demain, A. L. (2010). Enzymes and bioconversions of industrial, pharma-
ceutical, and biotechnological signiﬁcance. Organic Process Research & Development,
15(1), 224–230.

Schmidt, M. W., Berg, E., Friedlander, M. P., and Murphy, K. P. (2009). Optimizing
costly functions with simple constraints: A limited-memory projected quasi-newton
algorithm. In International Conference on Artiﬁcial Intelligence and Statistics, page
None.

Shawe-Taylor, J. and Cristianini, N. (2004). Kernel methods for pattern analysis. Cam-

bridge university press.

Tian, J., Wu, N., Chu, X., and Fan, Y. (2010). Predicting changes in protein thermosta-
bility brought about by single- or multi-site mutations. BMC bioinformatics, 11(1),
1.

Tokuriki, N. and Tawﬁk, D. S. (2009). Stability eﬀects of mutations and protein evolv-

ability. Current opinion in structural biology, 19(5), 596–604.

Dehouck, Y., Grosﬁls, A., Folch, B., Gilis, D., Bogaerts, P., and Rooman, M. (2009). Fast
and accurate predictions of protein stability changes upon mutations using statistical
potentials and neural networks: PoPMuSiC-2.0. Bioinformatics, 25(19), 2537–2543.

Tomii, K. and Kanehisa, M. (1996). Analysis of amino acid indices and mutation matrices
for sequence comparison and structure prediction of proteins. Protein Engineering,
Design and Selection, 9(1), 27–36.

Folkman, L., Stantic, B., and Sattar, A. (2014). Feature-based multiple models improve
classiﬁcation of mutation-induced stability changes. BMC genomics, 15(Suppl 4).

Vishwanathan, S. V. N., Schraudolph, N. N., Kondor, R., and Borgwardt, K. M. (2010).

Graph kernels. The Journal of Machine Learning Research, 11, 1201–1242.

Giguere, S., Marchand, M., Laviolette, F., Drouin, A., and Corbeil, J. (2013). Learn-
ing a peptide-protein binding aﬃnity predictor with kernel ridge regression. BMC
bioinformatics, 14(1), 82.

Wainreb, G., Wolf, L., Ashkenazy, H., Dehouck, Y., and Ben-Tal, N. (2011). Protein
stability: a single recorded mutation aids in predicting the eﬀects of other mutations
in the same amino acid site. Bioinformatics, 27(23), 3286–3292.

8

Supplementary material

Figure S5: The stability of a protein is determined by the thermodynamic and kinetic stabilities, ∆Gt and ∆Gk, respectively. We only consider the
thermodynamic stability.

Figure S6: Priors presented by Equation 2. Here µd = −1.5, the most likely value for −a. Other hyperparameter values are presented in Table S4.

Table S4: Values for the hyperparameters used in the priors of a, b, c, d, σE and σS presented in Equations 2 and 6, respectively.

a

b

c

d

σE

σS

αa = 2
βa = 1.5

αb = 1.3
βb = 2

αc = 2
βc = 5

µd = −a
σd = 0.15

αE = 2.5
βE = 0.02

αS = 50
βS = 0.007

9

The partial derivatives of the marginal likelihood with respect to the parameters φ are obtained from Equation (9) as follows:

where α = K −1

φ y, Kφ is determined as

and the partial derivatives of Kφ with respect to the optimised parameters are

∂
∂φj

log p(y|X, φ) =

yT K −1

∂Kφ
∂φj

K −1

φ y −

(cid:19)

K −1
φ

∂Kφ
∂φj

1
2
1
2

φ
(cid:18)(cid:16)

=

tr

ααT − K −1

φ

(cid:18)

(cid:19)

,

tr

1
2
(cid:17) ∂Kφ
∂θj

Kφ =

wmK γm

m + diag





σ0
σE1NE
σE1NE + σS1NS + tσT


2



M
(cid:88)

m=1

∂Kφ
∂σE

∂Kφ
∂σR

∂Kφ
∂t

∂Kφ
∂wm
∂Kφ
∂γm







0
2σE1NE
2 (σE1NE + σS1NS + tσT )
0
0NE
2 (σE1NE + σS1NS + tσT )
0
0NE
2 (σE1NE + σS1NS + t) σT













=diag



=diag



=diag



=K γm
m

=wmK γm

m log Km

(cid:80)N∗

i=1(yi − ¯y)(µ(xi) − ¯µ)

i=1(yi − ¯y)2 (cid:80)N∗

i=1(µ(xi) − ¯µ)2

ρ =

(cid:113)(cid:80)N∗
(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
N∗

N∗(cid:88)

i=1

rmse =

(yi − µ(xi))2,

Correlation ρ and root-mean-square error rmse for the predictions are determined as

(S11)

(S12)

(S13)

(S14)

(S15)

(S16)

(S17)

(S18)

where ¯y is the mean of the experimentally measured values, µ(xi) is prediction mean, ¯µ is the average of all prediction means, and N∗ is the
number of predictions.

10

Figure S7: Mutation-level predictions for all 15 proteins presented in Table 1. The predictions are coloured by the number of simultaneous mutations.

11

Figure S8: Position-level predictions for all 15 proteins. When the eﬀects of a mutant are predicted multiple times, they are connected by a line.

12

Table S5: (Continues on the next page)Comparison of diﬀerent methods on the 15 protein dataset with respect to ρ and rmse. Oﬀ-the-shelf implementations
of Rosetta, mCSM and PoPMuSiC are used directly without cross-validation.

Protein Method

2LZM

1BNI

1VQB

1LZ1

2CI2

1PGA

2RN2

1CSP

mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC

0.75
0.73
0.75
0.71
0.57

0.58
0.59
-
-
-

0.49
0.50
-
-
-

0.67
0.65
0.79
0.79
0.47

0.62
0.62
-
-
-

0.77
0.74
0.81
0.61
0.59

Point mutations
cross-validation level
prot.
pos.
mut.
0.75
0.80
0.87
0.75
0.77
0.86
-
0.59
0.86
-
0.37
0.75
0.73
-
0.74
0.75
0.57
0.71
0.64
0.61
0.65
0.48
0.58
0.62
0.60
0.66
0.50
0.53
0.12
0.29
0.46
0.49
0.53
0.51
0.59
0.56
0.39
-0.31
0.55
0.59
0.67
0.64
0.72
0.67
0.61
0.74
0.60
0.63
0.74
0.75
0.47
0.59
0.61
-0.46
0.59
0.69
-0.10
0.28
0.58
0.59
0.12
0.09
0.64
0.70
0.71
0.71
0.23
0.22
-0.06
-0.37
0.20
0.33
0.42
0.48

0.85
0.86
0.88
0.87
0.23

0.33
0.34
-
-
-

0.69
0.71
-
-
-

0.79
0.82
0.77
0.83
0.66

0.71
0.71
-
-
-

0.68
0.82
0.62
0.57
0.69

0.64
0.63
-
-
-

0.73
0.69
0.65
0.51
0.60

0.75
0.73
-
-
-

0.39
0.40
-
-
-

0.93
0.91
0.96
0.97
0.71

0.86
0.84
0.86
0.87
0.17

-1.00
-1.00
-
-
-

1.00
1.00
1.00
1.00
-1.00

Correlation ρ
Multiple mutations
cross-validation level
prot.
pos.
mut.
0.64
0.83
0.96
0.64
0.87
0.96
-
0.85
0.97
-
0.77
0.94
0.66
-
0.68
0.68
-
-
0.70
0.79
0.82
0.85
0.29
0.18
-
-
0.83
0.82
0.70
0.75
0.68
0.73
-
-
0.11
0.05
0.56
0.42
-1.00
-1.00
-
-
0.87
0.86
0.79
0.71
0.61
0.62
-
-
0.35
0.60
-0.24
-0.08
0.07
0.03
-
-
0.60
0.60
0.42
0.42
0.50
0.47
-
-
0.73
0.69
0.75
0.71
0.69
0.68
-
-

0.92
0.91
0.94
0.92
0.68

0.90
0.76
0.93
0.92
0.09

0.78
0.76
0.75
0.77
0.48

0.48
0.59
-
-
-

0.32
0.62
-
-
-

0.61
0.62
-
-
-

0.95
0.92
0.92
0.92
0.61

0.85
0.86
-
-
-

0.77
0.76
0.78
0.74
0.53

0.76
0.75
0.85
0.85
0.57

0.77
0.74
0.80
0.63
0.53

0.52
0.53
-
-
-

0.60
0.58
-
-
-

0.57
0.57
-
-
-

All mutations
cross-validation level
prot.
pos.
mut.
0.68
0.76
0.90
0.69
0.82
0.90
-
0.72
0.90
-
0.61
0.82
0.65
-
0.70
0.71
-
-
0.55
0.60
0.63
0.41
0.45
0.56
-
-
0.69
0.69
0.50
0.55
0.59
0.59
-
-
0.57
0.56
0.47
0.21
0.46
0.55
-
-
0.81
0.79
0.72
0.63
0.63
0.65
-
-
0.43
0.53
-0.14
0.06
0.11
0.28
-
-
0.53
0.53
0.22
0.23
0.57
0.65
-
-
0.75
0.72
0.77
0.72
0.64
0.60
-
-

0.38
0.54
-
-
-

0.50
0.70
-
-
-

0.70
0.70
-
-
-

0.92
0.91
0.94
0.92
0.59

0.85
0.81
0.84
0.81
0.24

0.79
0.81
0.77
0.82
0.62

0.71
0.72
-
-
-

0.82
0.79
0.76
0.68
0.63

0.83
0.86
0.81
0.91
0.99

1.06
1.06
-
-
-

2.25
2.25
-
-
-

1.71
1.75
1.41
1.50
1.99

1.21
1.27
1.08
1.61
1.58

1.69
1.69
-
-
-

mut.
0.82
0.84
0.82
1.12
1.05

Point mutations
cross-validation level
prot.
pos.
1.12
1.02
1.12
1.08
-
1.34
-
1.93
1.06
-
1.13
1.27
1.11
1.37
1.45
1.40
2.32
1.58
1.70
1.62
1.53
1.94
1.94
2.41
2.89
2.00
2.26
2.24
2.29
0.99
1.03
1.15
1.43
1.01
1.04
0.97
0.95
0.90
0.97
1.02
1.39
1.00
1.09
0.86
0.85
1.54
1.22
1.58
1.73
1.42
1.70
1.94
1.89
1.21
1.22
1.45
1.45
1.13
1.07
1.04
1.16
1.04
1.04
1.07
1.12
1.06
1.11
1.02
0.99

0.64
0.65
0.54
0.60
1.04

1.10
1.10
-
-
-

1.26
0.87
1.40
1.53
1.21

1.64
1.06
-
-
-

0.91
0.87
0.93
0.82
1.09

1.05
1.59
-
-
-

0.85
0.91
0.95
1.16
1.00

1.07
1.07
-
-
-

0.56
1.10
0.13
0.27
3.23

1.15
1.35
0.70
0.63
1.90

1.28
1.24
1.32
1.15
2.51

3.75
3.51
-
-
-

2.06
2.43
-
-
-

2.17
2.14
-
-
-

mut.
0.57
0.59
0.48
0.78
1.65

rmse
Multiple mutations
cross-validation level
prot.
pos.
2.14
1.53
2.11
1.16
-
1.24
-
1.44
1.84
-
1.93
-
-
2.49
2.03
2.00
1.67
2.90
2.33
-
-
1.62
1.82
1.97
2.30
1.96
2.06
-
-
2.40
2.41
1.65
2.36
3.25
3.41
-
-
0.80
1.01
1.01
1.40
1.27
1.30
-
-
2.09
0.95
3.01
2.07
3.09
3.51
-
-
1.01
0.95
1.21
1.20
1.08
1.25
-
-
1.66
1.87
1.51
1.59
2.29
1.92
-
-

2.58
2.13
-
-
-

2.74
0.88
-
-
-

0.91
1.18
-
-
-

1.21
1.12
-
-
-

0.91
0.96
0.76
0.86
2.19

0.53
0.69
0.45
0.47
2.81

0.67
0.69
0.74
0.68
1.20

0.55
0.71
0.66
0.71
1.27

All mutations
cross-validation level
prot.
1.43
1.42
-
-
-

mut.
0.76
0.79
0.75
1.05
1.23

pos.
1.30
1.13
1.29
1.70
1.51

1.22
1.27
1.11
1.57
1.70

1.59
1.66
1.27
1.33
1.97

0.83
0.87
0.80
0.90
1.07

0.80
0.87
0.90
1.08
1.06

0.88
0.83
0.94
1.02
2.33

0.86
0.83
0.89
0.80
1.12

0.75
0.78
0.63
0.71
1.58

1.37
-
-
1.67
1.60
1.55
2.20
1.94

1.77
-
-
1.82
1.89
2.24
2.66
1.99

2.21
-
-
1.07
1.10
1.17
1.47
1.15

1.12
-
-
0.86
0.99
1.02
1.39
1.11

1.13
-
-
2.00
1.13
2.81
2.01
2.87

2.95
-
-
1.14
1.13
1.36
1.36
1.11

1.11
-
-
1.45
1.60
1.36
1.43
1.92

1.47
-
-

1.75
1.75
-
-
-

2.20
2.30
-
-
-

1.16
1.14
-
-
-

1.10
1.08
-
-
-

2.38
1.02
-
-
-

1.02
1.51
-
-
-

1.80
1.57
-
-
-

13

Table S5: (Continued) Comparison of diﬀerent methods on the 15 protein dataset with respect to ρ and rmse. Oﬀ-the-shelf implementations of Rosetta,
mCSM and PoPMuSiC are used directly without cross-validation.

Protein Method

1BVC

4LYZ

1RTB

1PIN∗

1RN1

1RGG

1BPI

total

mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled
Rosetta
mCSM
PoPMuSiC

0.61
0.59
0.65
0.62
0.29

0.34
0.34
-
-
-

0.92
0.89
0.92
0.91
0.65

0.53
0.53
-
-
-

0.64
0.59
0.61
0.23
0.50

0.70
0.70
-
-
-

Point mutations
cross-validation level
prot.
pos.
mut.
0.48
0.43
0.41
0.48
0.48
0.50
-
-0.12
-0.05
-
0.06
-0.05
-
0.40
0.42
0.47
0.47
0.60
0.27
0.30
-0.05
0.27
0.28
0.33
0.55
0.59
0.81
0.79
0.69
0.58
0.61
0.69
0.68
0.72
0.49
0.51
0.40
0.25
0.49
0.53
0.72
0.60
0.62
0.58
0.04
0.27
0.57
0.67
0.76
0.62
0.51
0.52
-0.61
-0.53
0.43
0.60
0.77
0.65
0.67
0.65
0.57
0.58
0.52
0.51
0.71
0.72
0.70
0.69
0.51
0.34
0.63
0.55
0.61
0.64

0.67
0.67
-
-
-

0.83
0.81
0.74
0.77
0.63

0.69
0.69
0.71
0.72
0.58

0.81
0.79
0.81
0.76
0.65

0.68
0.64
0.56
0.43
0.52

0.61
0.61
-
-
-

0.51
0.51
-
-
-

0.56
0.56
-
-
-

Correlation ρ
Multiple mutations
cross-validation level
prot.
pos.
mut.
-0.63
-0.25
-0.28
-0.66
-0.25
-0.25
-
-0.21
-0.10
-
-0.21
-0.11
-
-0.57
-0.63
-0.65
-
-
0.57
0.64
0.46
0.47
0.68
0.71
-
-
0.74
0.73
0.30
0.58
0.73
0.73
-
-
-

0.78
0.73
0.78
0.48
0.73

0.95
0.96
0.96
0.97
0.70

0.75
0.67
-
-
-

0.66
0.64
-
-
-

-
-
-
-
-

-0.19
-0.01
-
-
-

0.96
0.93
-
-
-

-0.06
0.21
-
-
-

0.49
0.50
-
-
-

-
-
-
-
-

0.97
0.96
0.99
0.99
0.21

0.42
0.22
0.19
0.08
0.78

-0.39
-0.53
-0.29
-0.55
0.23

0.88
0.86
0.86
0.86
0.51

-

-

-
-

-
-
-
0.38
0.42
0.62
0.50
0.18
0.20
-
-
0.66
0.30
0.08
0.07
0.86
0.77
-
-
-0.02
0.33
0.52
0.46
-0.00
-0.00
-
-
0.61
0.64
0.52
0.55
0.39
0.40
-
-

0.65
0.64
0.68
0.64
0.33

0.64
0.59
0.61
0.23
0.50

0.86
0.86
0.76
0.85
0.67

0.35
0.35
-
-
-

0.53
0.53
-
-
-

0.68
0.69
-
-
-

All mutations
cross-validation level
prot.
pos.
mut.
0.09
-0.09
0.08
0.14
-0.07
0.14
-
-0.23
-0.13
-
-0.20
-0.14
-
-0.09
0.09
0.14
-
-
0.34
0.40
0.18
0.21
0.34
0.35
-
-
0.71
0.75
0.18
0.16
0.65
0.70
-
-
0.49
0.51
0.40
0.25
0.49
0.53
0.72
0.60
0.55
0.51
0.04
0.10
0.54
0.65
-
-
0.52
0.33
-0.05
-0.00
0.51
0.39
-
-
-0.13
0.05
0.84
0.70
-0.08
0.13
-
-
0.64
0.66
0.50
0.49
0.48
0.49
-
-

0.12
0.12
-
-
-

0.65
0.66
-
-
-

0.73
0.55
-
-
-

0.52
0.52
-
-
-

0.84
0.81
0.75
0.78
0.61

0.52
0.40
0.35
0.30
0.42

0.85
0.81
0.83
0.76
0.08

0.83
0.82
0.83
0.80
0.60

3.46
3.50
-
-
-

1.47
1.48
1.38
1.48
2.26

1.00
1.00
-
-
-

0.47
0.50
0.49
0.70
0.54

2.44
2.44
-
-
-

1.25
1.48
1.26
1.67
1.99

mut.
0.74
0.70
0.99
0.99
0.75

Point mutations
cross-validation level
prot.
pos.
1.65
0.72
1.65
0.71
-
1.00
-
0.99
-
0.76
1.67
1.00
0.85
1.65
1.64
1.78
1.84
2.22
3.61
1.43
1.45
1.73
1.85
2.62
3.37
2.08
2.45
2.33
2.22
0.53
0.54
0.58
0.76
0.54
0.99
0.59
0.70
1.17
1.24
1.75
1.78
1.18
1.20
0.97
1.14
1.70
1.68
2.24
2.01
1.76
1.58
1.36
1.54
1.28
1.32
1.43
1.73
1.52
1.80
1.26
1.31
1.26
1.30
1.54
1.95
1.38
1.63
1.40
1.37

0.85
0.92
1.06
1.00
1.12

1.18
1.18
-
-
-

1.41
1.42
1.38
1.53
1.57

1.56
1.56
-
-
-

1.27
1.32
1.43
1.68
1.52

1.83
1.82
-
-
-

1.07
1.11
1.04
1.26
1.35

1.61
1.62
-
-
-

0.69
0.57
0.36
0.31
1.22

2.35
2.17
-
-
-

mut.
1.64
1.62
1.30
1.29
1.65

rmse
Multiple mutations
cross-validation level
prot.
pos.
2.70
1.48
2.43
1.47
-
1.35
-
1.33
-
1.45
2.45
-
-
1.02
0.93
1.15
1.16
1.29
2.05
-
-
2.24
1.81
3.39
2.11
1.62
2.01
-
-
-
-
-
-
-

1.71
1.90
-
-
-

2.22
1.82
3.44
2.12
1.63

-
-
-
-
-

-
-
-
-
-

0.19
0.35
0.12
0.13
0.98

5.67
6.21
6.81
6.89
7.08

2.17
2.86
1.72
2.52
7.43

1.33
1.43
1.44
1.45
2.49

-
-
0.91
0.77
0.58
0.82
1.01
1.24
-
-
5.95
6.14
6.88
6.77
7.24
6.90
-
-
7.66
7.07
4.19
6.58
7.43
7.68
-
-
2.45
2.40
2.65
2.56
2.99
2.74
-
-

1.28
0.99
-
-
-

4.20
5.43
-
-
-

7.69
7.70
-
-
-

2.53
2.50
-
-
-

All mutations
cross-validation level
prot.
2.02
1.92
-
-
-

mut.
1.09
1.06
1.09
1.09
1.10

pos.
1.21
1.20
1.21
1.20
1.20

1.35
1.35
1.24
1.33
2.09

1.40
1.53
1.69
1.73
1.95

0.47
0.50
0.49
0.70
0.54

0.81
0.88
1.01
0.95
1.10

2.65
2.85
3.05
3.14
3.23

1.40
1.57
1.47
1.80
2.88

1.13
1.18
1.14
1.30
1.66

1.94
-
-
1.45
1.43
1.59
1.63
1.94

3.35
-
-
1.85
1.84
2.81
3.14
1.99

2.40
-
-
0.53
0.54
0.58
0.76
0.54

0.99
0.59
0.70

1.13
1.17
1.61
1.66
1.15

1.20
-
-
4.75
4.90
5.53
5.41
5.74

3.16
-
-
3.64
3.40
2.28
3.34
3.62

3.09
-
-
1.87
1.85
2.09
2.23
2.22

1.92
-
-

3.26
3.27
-
-
-

2.36
2.38
-
-
-

1.00
1.00
-
-
-

1.19
1.17
-
-
-

2.23
2.64
-
-
-

3.11
3.11
-
-
-

1.84
1.84
-
-
-

∗ dataset for 1PIN contained no multiple mutations.

14

Table S6: Comparison of diﬀerent methods on the 15 protein dataset with respect to ρ and rmse after removing 10% of predictions with largest errors.
Mutation, position, and protein are referred to as mut., pos., and prot., respectively. Oﬀ-the-shelf implementations of Rosetta, mCSM and PoPMuSiC are
used directly without cross-validation.

Results after 10 %
outlier removal

Method
mGPfusion
mGPfusion, only B62
mGP
mGP, only B62
Rosetta scaled

Point mutations
cross-validation level
prot.
pos.
0.75
0.77
0.75
0.73
-
0.57
-
0.27
-
0.78

mut.
0.87
0.86
0.89
0.86
0.80

Correlation ρ
Multiple mutations
cross-validation level
prot.
pos.
0.63
0.84
0.65
0.84
-
0.76
-
0.76
-
0.71

mut.
0.97
0.96
0.98
0.97
0.73

Oﬀ-the-shelf implementations with no cross-validation

All mutations
cross-validation level
prot.
pos.
0.71
0.81
0.72
0.80
-
0.74
-
0.66
-
0.75

mut.
0.92
0.91
0.93
0.91
0.78

Point mutations
cross-validation level
prot.
pos.
0.93
0.85
0.93
0.85
-
1.01
-
1.30
-
0.87

mut.
0.69
0.69
0.65
0.73
0.84

rmse
Multiple mutations
cross-validation level
prot.
pos.
1.90
1.55
1.85
1.43
-
1.66
-
1.48
-
1.99

mut.
0.62
0.64
0.49
0.54
1.67

All mutations
cross-validation level
prot.
1.13
1.12
-
-
-

mut.
0.67
0.67
0.62
0.69
1.00

pos.
1.15
1.10
1.23
1.37
1.31

Rosetta
mCSM
PoPMuSiC

0.75
0.71
0.73

0.67
-
-

0.73
-
-

0.94
0.89
0.86

1.85
-
-

1.11
-
-

Figure S9: rmse with diﬀerent amount of predictions, when predictions are sorted by the error. Position level cross-validation was used for mGPfusion,
mGP and Rosetta scaled.

15

Figure S10: Learning curves.

16

Figure S11: Learning curves.

17

Figure S12: Learning curves.

18

Figure S13: Transformations for all 15 proteins presented in Table 1. The red circles mark the simulated ∆∆G-values yS with respect to the experimental
measured ∆∆G-values yE. Thin black lines show possible transformations for yS, whereas the thick black line shows the selected transformation from yS
to ˜yS.

19

