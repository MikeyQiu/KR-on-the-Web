On the interplay between noise and curvature
and its eﬀect on optimization and generalization

0
2
0
2
 
r
p
A
 
6
 
 
]

G
L
.
s
c
[
 
 
2
v
4
7
7
7
0
.
6
0
9
1
:
v
i
X
r
a

Valentin Thomas
Mila, Universit´e de Montr´eal

Fabian Pedregosa
Google Research, Brain Team

Bart van Merri¨enboer
Google Research, Brain Team

Pierre-Antoine Mangazol
Google Research, Brain Team

Yoshua Bengio
Mila, Universit´e de Montr´eal
CIFAR Senior Fellow

Nicolas Le Roux
Google Research, Brain Team
Mila, McGill University

Abstract

distribution ˆp over x and minimize the training loss

The speed at which one can minimize an ex-
pected loss using stochastic methods depends
on two properties: the curvature of the loss
and the variance of the gradients. While most
previous works focus on one or the other of
these properties, we explore how their inter-
action aﬀects optimization speed. Further,
as the ultimate goal is good generalization
performance, we clarify how both curvature
and noise are relevant to properly estimate
the generalization gap. Realizing that the
limitations of some existing works stems from
a confusion between these matrices, we also
clarify the distinction between the Fisher ma-
trix, the Hessian, and the covariance matrix
of the gradients.

1 Introduction

Training a machine learning model is often cast as the
minimization of a smooth function f over parameters
θ in Rd. More precisely, we aim at ﬁnding a minimum
of an expected loss, i.e.

θ∗ ∈ arg min

Ep[(cid:96)(θ, x)] ,

θ

(1)

where the expectation is under the data distribution
x ∼ p. In practice, we only have access to an empirical

Proceedings of the 23rdInternational Conference on Artiﬁcial
Intelligence and Statistics (AISTATS) 2020, Palermo, Italy.
PMLR: Volume 108. Copyright 2020 by the author(s).

ˆθ∗ ∈ arg min

E ˆp[(cid:96)(θ, x)]

= arg min

f (θ) .

θ

θ

(2)

(3)

To minimize this function, we assume access to an ora-
cle which, for every value of θ and x, returns both (cid:96)(θ, x)
and its derivative with respect to θ, i.e., ∇θ(cid:96)(θ, x).
Given this oracle, stochastic gradient iteratively per-
forms the following update: θt+1 = θt − αt∇(cid:96)(θt, x) 1
where {αt}t≥0 is a sequence of stepsizes.

Two questions arise: First, how quickly do we converge
to ˆθ∗ and how is this speed aﬀected by properties of (cid:96)
and ˆp? Second, what is Ep[(cid:96)(ˆθ∗, x)]?

It is known that the former is inﬂuenced by two quan-
tities: the curvature of the function, either measured
through its smoothness constant or its condition num-
ber, and the noise on the gradients, usually measured
through a bound on E ˆp[(cid:107)∇(cid:96)(θ, x)(cid:107)2]. For instance,
when f is µ-strongly convex, L-smooth, and the noise
is bounded, i.e. E ˆp[(cid:107)∇(cid:96)(θ, x)(cid:107)2] ≤ c, then stochas-
tic gradient with a constant stepsize α will converge
linearly to a ball (Schmidt, 2014). Calling ∆ the sub-
optimality, i.e. ∆k = f (θk) − f (ˆθ∗), we have

E[∆k] ≤ (1 − 2αµ)k∆0 +

(4)

Lαc
4µ

.

This implies that as k → ∞, the expected suboptimal-
ity depends on both the curvature through µ and L,
and on the noise through c. However, bounding curva-
ture and noise using constants rather than full matrices
hides the dependencies between these two quantities.
We also observed that, because existing works replace
the full noise matrix with a constant when deriving
convergence rates, that matrix is poorly understood

1We omit the subscripts when clear from context.

On the interplay between noise and curvature

and is often confused with a curvature matrix. This
confusion remains when discussing the generalization
properties of a model. Indeed, the generalization gap
stems from a discrepancy between the empirical and
the true data distribution. An estimator of this gap
must thus include an estimate of that discrepancy in ad-
dition to an estimate of the impact of an inﬁnitesimal
discrepancy on the loss. The former can be charac-
terized as noise and the latter as curvature. Hence,
attempts at estimating the generalization gap using
only the curvature (Keskar et al., 2017; Novak et al.,
2018) are bound to fail as do not characterize the size
or geometry of the discrepancy.

In this work, we make the following contributions:

• We provide theoretical and empirical evidence of
the similarities and diﬀerences surrounding the
curvatures matrices; the Fisher F and the Hessian
H, and the noise matrix, C;

• We brieﬂy expand the convergence results
of Schmidt (2014), theoretically and empirically
highlighting the importance of the relationship
between noise and curvature for strongly convex
functions and quadratics;

• We make the connection with an old estimator of
the generalization gap, the Takeuchi Information
Criterion, and show how its use of both curvature
and noise yields a superior estimator to other com-
monly used ones, such as ﬂatness or sensitivity for
neural networks.

2 Information matrices: deﬁnitions,

similarities, and diﬀerences

Before delving into the impact of the information ma-
trices for optimization and generalization, we start by
recalling their deﬁnitions. We shall see that, despite
having similar formulations, they encode diﬀerent infor-
mation. We then provide insights on their similarities
and diﬀerences.

We discuss here two information matrices associated
with curvature, the Fisher matrix F and the Hessian
H, and one associated with noise, the gradients’ un-
centered covariance C. In particular, while F and H
are well understood, C is often misinterpreted. For
instance, it is often called “empirical Fisher” (Martens,
2014) despite bearing no relationship to F, the true
Fisher. This confusion can have dire consequences and
optimizers using C as approximation to F can have
arbitrarily poor performance (Kunstner et al., 2019).

To present these matrices, we consider the case of
maximum likelihood estimation (MLE). We have access
to a set of samples (x, y) ∈ X × Y where x is the input
and y the target. We deﬁne p : X × Y (cid:55)→ R as the

data distribution and qθ : X × Y (cid:55)→ R such that
qθ(x, y) = p(x)qθ(y|x) as the model distribution2.
For each sample (x, y) ∼ p, our loss is the negative log-
likelihood (cid:96)(θ, y, x) = − log qθ(y|x). Note that all the
deﬁnitions and results in this section are valid whether
we use the true data distribution p or the empirical ˆp.

Matrices H, F and C are then deﬁned as:

H(θ) = Ep

C(θ) = Ep

F(θ) = Eqθ

= Eqθ

(cid:96)(θ, y, x)

(cid:21)
(cid:20) ∂2
∂θ∂θ(cid:62) (cid:96)(θ, y, x)
(cid:20) ∂
∂θ
(cid:20) ∂
∂θ
(cid:21)
(cid:20) ∂2
∂θ∂θ(cid:62) (cid:96)(θ, y, x)

∂
∂θ
∂
∂θ

(cid:96)(θ, y, x)

.

(cid:21)

(cid:21)

(cid:96)(θ, y, x)(cid:62)

(cid:96)(θ, y, x)(cid:62)

(5)

(6)

(7)

(8)

We observe the following: a) The deﬁnition of H and
C involves the data distribution, in contrast with the
deﬁnition of F, which involves the model distribution;
b) If qθ = p, all matrices are equal. Furthermore, as
noted by Martens (2014), H = F whenever the matrix
of second derivatives does not depend on y, a property
shared in particular by all generalized linear models.

As said above, H, F, and C characterize diﬀerent prop-
erties of the optimization problem. H and F are curva-
ture matrices and describe the geometry of the space
around the current point. C, on the other hand, is
a “noise matrix” and represents the sensitivity of the
gradient to the particular sample.3

We now explore in more details their similarities and
diﬀerences.

2.1 Bounds between H, F and C

The following proposition bounds the distance between
the information matrices:

Proposition 1 (Distance between H, F and C)
Assuming the second moments of the Fisher are
bounded above, i.e. Eqθ [||∇2
θ(cid:96)(θ, x, y)||2] ≤ β1 and
Eqθ [||∇θ(cid:96)(θ, x, y)∇θ(cid:96)(θ, x, y)(cid:62)||2] ≤ β2, we have

||F − H||2 ≤ β1 Dχ2 (p||qθ) ,
||F − C||2 ≤ β2 Dχ2 (p||qθ) ,
||C − H||2 ≤ (cid:0)β1 + β2

(cid:1) Dχ2(p||qθ) .

2qθ(y|x) are the softmax activations of a neural network

in the classiﬁcation setting.

3Technically, it is S, the centered covariance matrix,
rather than C which plays that role but the two are similar
close to a stationary point.

Thomas, Pedregosa, van Merri¨enboer, Mangazol, Bengio, Le Roux

where Dχ2 (p||qθ) = (cid:82)(cid:82) (p(x,y)−qθ(x,y))2
divergence and || · || is the Frobenius norm.

qθ(x,y)

dydx is the χ2

In that case, for θ = θ∗

All the proofs are in the appendix.

In particular, when p = qθ we recover that F = H = C.
At ﬁrst glance, one could assume that, as the model
is trained and qθ approaches p, these matrices become
more similar. The χ2 divergence is however poorly
correlated with the loss of the model. One sample
where the prediction of the model is much smaller
than the true distribution can dramatically impact
the χ2 divergence and thus the distance between the
information matrices. We show in Section 5.3 how
these distances evolve when training a deep network.

2.2 C does not approximate F

C is often referred to as the “empirical Fisher” matrix,
implying that it is an approximation to the true Fisher
matrix F (Martens, 2014). In some recent works (Liang
et al., 2017; George et al., 2018) the empirical Fisher
matrix is used instead of the Fisher matrix. However,
in the general case, there is no guarantee that C will
approximate F, even in the limit of inﬁnite samples. We
now give a simple example highlighting their diﬀerent
roles:

X

Example 1 (Mean regression) Let
=
(xi)i=1,...,N be an i.i.d sequence of random vari-
ables. The task is to estimate µ = E[x] by minimizing
the loss (cid:96)(θ) = 1
n=1 ||xn − θ||2. The minimum
2N
is attained at θMLE = 1
n=1 xn. This estimator is
N
consistent and converges to µ at rate O( 1√
N

(cid:80)N

(cid:80)N

).

This problem is an MLE problem if we deﬁne qθ(x) =
N (x; θ, Id). In this case, we have

H(θMLE) = F(θMLE) = Id

, C(θMLE) = (cid:98)Σx , (9)

where (cid:98)Σx is the empirical covariance of the xi’s. We see
that, even in the limit of inﬁnite data, the covariance
C does not converge to the actual Fisher matrix nor
the Hessian. Hence we shall and will not refer to C as
the “empirical Fisher” matrix.

In some other settings, however, one can expect a
stronger correlation between C and H:

Example 2 (Ordinary Least Squares) Let us assume
we have a data distribution (xn, yn) so that
n θ∗ + (cid:15)n,

(cid:15)n ∼ N (0, Σ)

yn = x(cid:62)

(10)

with yn ∈ Rp and xn, (cid:15)n ∈ Rd, θ ∈ Rp×d. We train a
model to minimize the sum of squares residual

min
θ

1
2N

N
(cid:88)

n=1

||yn − x(cid:62)

n θ||2 .

(11)

H = F = Ep[xx(cid:62)]

, C = Ep[xΣx(cid:62)]

(12)

First, we observe that the Hessian and the Fisher are
equal, for all θ. Second, when the input/output co-
variance matrix is isotropic Σ = σ2I, then we have
C ∝ H = F.

3 Information matrices in

optimization

Now that the distinction between these matrices has
been clariﬁed, we can explain how their interplay af-
fects optimization. In this section, we oﬀer theoretical
results, as well as a small empirical study, on the impact
of the noise geometry on convergence rates.

3.1 Convergence rates

We start here by expanding the the result of Schmidt
(2014) to full matrices, expliciting how the interplay
of noise and curvature aﬀects optimization. We then
build a toy experiment to validate the results.

3.1.1 General setting

Proposition 2 (Function value) Let f be a twice-
diﬀerentiable function. Assume f is µ-strongly convex
and there are two matrices H and S such that for all
θ, θ(cid:48):

f (θ(cid:48)) ≤ f (θ) + ∇f (θ)(cid:62)(θ(cid:48) − θ) +

(θ(cid:48) − θ)(cid:62)H(θ(cid:48) − θ)

Ep[∇(cid:96)(θ, x)∇(cid:96)(θ, x)(cid:62)] (cid:52) S + ∇f (θ)∇f (θ)(cid:62) .

1
2

Then stochastic gradient with stepsize α and positive
deﬁnite preconditioning matrix M satisﬁes

E[∆k] ≤ (1 − 2αµM µ)k∆0 +

Tr(HMSM) ,

α
4µM µ

where µM is the smallest eigenvalue of M − α

2 M(cid:62)HM.

S is the centered covariance matrix of the stochastic
gradients and, if f is quadratic, then H is the Hessian.

3.1.2 Centered and uncentered covariance

Proposition 2, as well as most results on optimization,
uses a bound on the uncentered covariance of the gra-
dients. The result is that the noise must be lower far
away from the optimum, where the gradients are high.
Thus, it seems more natural to deﬁne convergence rates
as a function of the centered gradients’ covariance S,
although these results are usually weaker as a conse-
quence of the relaxed assumption. For the remainder

On the interplay between noise and curvature

of this section, focused on the quadratic case, we will
use S. Note that the two matrices are equal at any
ﬁrst-order stationary point.

Rather than using a preconditioner, another popular
method to reduce the impact of the curvature is Polyak
momentum, deﬁned as

Centered covariance matrices have been used in the
past to derive convergence rates, for instance by Bach
and Moulines (2013); Flammarion and Bach (2015);
Dieuleveut et al. (2016). These works also include a
dependence on the geometry of the noise since their
constraint is of the form S (cid:52) σ2H. In particular, if
S and H are not aligned, σ2 must be larger for the
inequality to hold.

3.1.3 Quadratic functions

v0 = 0 ,

vt = γvt−1 + ∇θ(cid:96)(θt, xt)

θt+1 = θt − αvt .

Proposition 5 (Limit cycle of momentum) If f
is a quadratic function and H and C are simultaneously
diagonalizable, then Polyak momentum with parameter
γ and stepsize α yields

E[∆t] = α
2

(1+γ)

(1−γ) Tr (cid:0)(2(1 + γ)I − αH)−1S(cid:1) + O(e−t) .
(14)

Proposition 3 (Quadratic case) Assuming
minimize a quadratic function

we

4 Generalization

f (θ) = 1

2 (θ − ˆθ∗)(cid:62)H(θ − ˆθ∗)

only having access to a noisy estimate of the gradient
g(θ) ∼ ∇f (θ) + (cid:15) with (cid:15) a zero-mean random variable
with covariance E[(cid:15)(cid:15)(cid:62)] = S, then the iterates obtained
using stochastic gradient with stepsize α and precondi-
tioning psd matrix M satisfy

E[θk − ˆθ∗] = (1 − αMH)k(θ0 − ˆθ∗) .

the covariance of
Further,
E[(θk − ˆθ∗)(θk − ˆθ∗)(cid:62)] satisﬁes

the iterates Σk =

Σk+1 = (I − αMH)Σk(I − αMH)(cid:62) + α2MSM(cid:62) .

In particular, the stationary distribution of the iterates
has a covariance Σ∞ which veriﬁes the equation
Σ∞HM + MHΣ∞ = αM(cid:0)S + HΣ∞H(cid:1)M .

Recently, analyzing the stationary distribution of SGD
by modelling its dynamics as a stochastic diﬀerential
equation (SDE) has gained traction in the machine
learning community (Chaudhari and Soatto, 2017; Jas-
trzebski et al., 2017). Worthy of note, Mandt et al.
(2017); Zhu et al. (2018) do not make assumptions about
the structure of the noise matrix S. Our proposition
above extends and corrects some of their results as it
does not rely on the continuous-time approximation of
the dynamics of SGD. Indeed, as pointed out in Yaida
(2018), most of the works using the continuous-time
approximation implicitly make the confusion between
centered S and uncentered C covariance matrices of
the gradients.

Proposition 4 (Limit cycle of SG) If
a
quadratic function and H, C and M are simultane-
ously diagonalizable,
then stochastic gradient with
symmetric positive deﬁnite preconditioner M and
stepsize α yields

is

f

E[∆t] = α

2 Tr((2I − αMH)−1MS) + O(e−t) .

(13)

So far, we focused on the impact of the interplay be-
tween curvature and noise in the optimization setting.
However, optimization, i.e. reaching low loss on the
training set, is generally not the ultimate goal as one
would rather reach a low test loss. The diﬀerence be-
tween training and test loss is called the generalization
gap and estimating it has been the focus of many au-
thors (Keskar et al., 2017; Neyshabur et al., 2017; Liang
et al., 2017; Novak et al., 2018; Rangamani et al., 2019).

We believe there is a fundamental misunderstanding in
several of these works, stemming from the confusion
between curvature and noise. Rather than proposing
a new metric, we empirically show how the Takeuchi
information criterion (TIC: Takeuchi, 1976) addresses
these misunderstandings. It makes use of both the Hes-
sian of the loss with respect to the parameters, H, and
the uncentered covariance of the gradients, C. While
the former represents the curvature of the loss, i.e., the
sensitivity of the gradient to a change in parameter
space, the latter represents the sensitivity of the gradi-
ent to a change in inputs. As the generalization gap is
a direct consequence of the discrepancy between train-
ing and test sets, the inﬂuence of C is natural. Thus,
our result further reinforces the idea that the Hessian
cannot by itself be used to estimate the generalization
gap, an observation already made by Dinh et al. (2017),
among others.

4.1 Takeuchi information criterion

In the simplest case of a well speciﬁed least squares
regression problem, an unbiased estimator of the gen-
eralization gap is the AIC (Akaike, 1974), which is
simply the number of degrees of freedom divided by
the number of samples: ˆG(θ) = 1
N d where d is the
dimensionality of θ. This estimator is valid locally
around the maximum likelihood parameters computed
on the training data. However, these assumptions do

Thomas, Pedregosa, van Merri¨enboer, Mangazol, Bengio, Le Roux

not hold in most cases, leading to the number of pa-
rameters being a poor predictor of the generalization
gap (Novak et al., 2018). When dealing with maximum
likelihood estimation (MLE) in misspeciﬁed models, a
more general formula for estimating the gap is given
by the Takeuchi information criterion (TIC: Takeuchi,
1976):

ˆG =

Tr(H(ˆθ∗)−1C(ˆθ∗)) ,

(15)

1
N

where ˆθ∗ is a local optimum. Note that H and C here
are the hessian and covariance of the gradients matrices
computed on the true data distribution.

This criterion is not new in the domain of machine learn-
ing. It was rediscovered by Murata et al. (1994) and
similar criteria have been proposed since then (Beirami
et al., 2017; Wang et al., 2018). However, as far as we
know, no experimental validation of this criterion has
been carried out for deep networks. Indeed, for deep
networks, H is highly degenerate, most of its eigen-
values being close to 0 (Sagun et al., 2016). In this
work, the Takeuchi information criterion is computed,
in the degenerate case, by only taking into account
the eigenvalues of the Hessian of signiﬁcant magnitude.
In practice, we cut all the eigenvalues smaller than a
constant times the biggest eigenvalue and perform the
inversion on that subspace. Details can be found in
appendix B.1.

Interestingly, the term Tr(H−1C) appeared in several
works before, whether as an upper bound on the sub-
optimality (Flammarion and Bach, 2015) or as a num-
ber of iterates required to reach a certain suboptimal-
ity (Bottou and Bousquet, 2008). Sadly, it is hard to
estimate for large networks but we propose an eﬃcient
approximation in Section 5.5.1.

4.2 Limitations of ﬂatness and sensitivity

We highlight here two commonly used estimators of
the generalization gap as they provide good examples
of failure modes that can occur when not taking the
noise into account. This is not to mean that these
estimators cannot be useful for the models that are
common nowadays, rather that they are bound to fail
in some cases.

Flatness (Hochreiter and Schmidhuber, 1997) links
the spectrum of the Hessian at a local optimum with the
generalization gap. This correlation, observed again
by Keskar et al. (2017), was already shown to not
hold in general (Dinh et al., 2017). As we showed in
Section 2.2, the Hessian does not capture the covariance
of the data, which is linked to the generalization gap
through the central-limit theorem.

Sensitivity (Novak et al., 2018) links the generaliza-

tion gap to the derivative of the loss with respect to
the input. The underlying idea is that we can expect
some discrepancy between train and test data, which
will induce changes in the output and a potentially
higher test loss. However, penalizing the norm of the
Jacobian assumes that changes between train and test
data will be isotropic. In practice, we can expect data
to vary more along some directions, which is not re-
ﬂected in the sensitivity. In the extreme case where
the test data is exactly the same as the training data,
the generalization gap will be 0, which will again not
be captured by the sensitivity. In practice, whitening
the data makes the sensitivity appropriate, save for a
scaling factor, as we will see in the experiments.

5 Experiments

We now provide experimental validation of all the re-
sults in this paper. We start by analyzing the distance
between information matrices, ﬁrst showing its poor
correlation with the training loss, then showing that
these matrices appear to be remarkably aligned, albeit
with diﬀerent scales, when training deep networks on
standard datasets.

5.1 Discrepancies between C, H and F

5.1.1 Experimental setup

For comparing the similarities and discrepencies be-
tween the information matrices, we tested

• 5 diﬀerent architectures:

logistic regression, a 1-
hidden layer and 2-hidden layer fully connected net-
work, and 2 small convolutional neural networks
(CNNs, one with batch normalization (Ioﬀe and
Szegedy, 2015) and one without);

• 3 datasets: MNIST, CIFAR-10, SVHN;
• 3 learning rates: 10−2, 5 · 10−3, and 10−3, using SGD

with momentum µ = 0.9;

• 2 batch sizes: 64, 512;
• 5 dataset sizes: 5k, 10k, 20k, 25k, and 50k.

We train for 750k steps and compute the metrics every
75k steps. To be able to compute all the information
matrices exacly, we reduced the input dimension by
converting all images to greyscale and resizing them
to 7 × 7 pixels. While this makes the classiﬁcation
task more challenging, our neural networks still exhibit
the behaviour of larger ones by their ability to ﬁt the
training set, even with random labels. Details and
additional ﬁgures can be found in appendix B.2.

5.2 Comparing Fisher and empirical Fisher

Figure 1 shows the squared Frobenius norm between
F and C (on training data) for many architectures,

On the interplay between noise and curvature

datasets, at various stages of the optimization. We see
that, while the two matrices eventually coincide on the
training set for some models, the convergence is very
weak as even low training errors can lead to a large
discrepancy between these two matrices. In practice,
C and F might be signiﬁcantly diﬀerent, even when
computed on the training set.

5.4 Impact of noise on second-order methods

Section 3 extended existing results to take the geometry
of the noise and the curvature into account. Here, we
show how the geometry of the noise, and in particular
its relationship to the Hessian, can make or break
second-order methods in the stochastic setting. To be
clear, we assume here that we have access to the full
Hessian and do not address the issue of estimating it
from noisy samples.

2 θ(cid:62)Hθ with θ ∈ R20
We assume a quadratic (cid:96)(θ) = 1
and H ∈ R20×20 a diagonal matrix such that Hii = i2
with a condition number d2 = 400. At each timestep,
we have access to an oracle that outputs a noisy gradi-
ent, Hθt + (cid:15) with (cid:15) drawn from a zero-mean Gaussian
with covariance S. Note here that S is the centered
covariance of the gradients. We consider three settings:
a) S = α1H; b) S = I; c) S = α−1H−1 where the
constants α1 and α−1 are chosen such that Tr(S) = d.
Hence, these three settings are indistinguishable from
the point of view of the rate of Schmidt (2014).

In this simpliﬁed setting, we get an analytic formula
for the variance at each timestep and we can compute
the exact number of steps t such that E[∆t] falls below
a suboptimality threshold. To vary the impact of the
noise, we compute the number of steps for three diﬀer-
ent thresholds: a) (cid:15) = 1; b) (cid:15) = 0.1; c) (cid:15) = 0.01. For
each algorithm and each noise, the stepsize is optimized
to minimize the number of steps required to reach the
threshold.

The results are in Table 1. We see that, while Stochastic
gradient and momentum are insensitive to the geom-
etry of the noise for small (cid:15), Newton method is not
and degrades when the noise is large in low curvature
directions. For (cid:15) = 10−2 and S ∝ H−1, Newton is
worse than SG, a phenomenon that is not captured
by the bounds of Bottou and Bousquet (2008) since
they do not take the structure of the curvature and the
noise into account. We also see that the advantage of
Polyak momentum over stochastic gradient disappears
when the suboptimality is small, i.e. when the noise is
large compared to the signal.

Also worthy of notice is the ﬁxed stepsize required to
achieve suboptimality (cid:15), as shown in Table 2. While
it hardly depends on the geometry of the noise for SG
and Polyak, Newton method requires much smaller
stepsizes when S is anticorrelated with H to avoid
amplifying the noise.

5.5 The TIC and the generalization gap

We now empirically test the quality of the TIC as an
estimator of the generalization gap in deep networks.
Following Neyshabur et al. (2017) we assess the be-

Figure 1: Squared Frobenius norm between ¯F and ¯C
(computed on the training distribution). Even for some
low training losses, there can be a signiﬁcant diﬀerence
between the two matrices.

5.3 Comparing H, F and C

In this subsection, we analyze the similarities and dif-
ferences between the information matrices. We will
focus on the scale similarity r, deﬁned as the ratio of
traces, and the angle similarity s, deﬁned as the cosine
between matrices. Note that having both r(A, B) = 1
and s(A, B) = 1 implies A = B.

Figure 2: Scale and angle similarities between informa-
tion matrices.

Figure 2 shows the scale (left) and angle (right) simi-
larities between the three pairs of matrices during the
optimization of all models used in ﬁgure 4. We can see
that H is not aligned with C nor F at the beginning
of the optimization but this changes quickly. Then, all
three matrices reach a very high cosine similarity, much
higher than we would obtain for two random low-rank
matrices. For the scaling, C is “larger” than the other
two while F and H are very close to each other. Thus,
as in the least squares case, we have C ∝

∼ F ≈ H.

Thomas, Pedregosa, van Merri¨enboer, Mangazol, Bengio, Le Roux

(cid:15) Method

100

10−1

10−2

SG
Newton
Polyak
SG
Newton
Polyak
SG
Newton
Polyak

β = 1
44
3
36
288
3
119
2090
29
1743

β = 0
43
2
36
253
28
111
1941
315
1727

β = −1
42
19
34
207
225
97
1731
2663
1705

Table 1: Number of updates required to reach subopti-
mality of (cid:15) for various methods and S ∝ Hβ.

(cid:15) Method

100

10−1

10−2

SG
Newton
Polyak
SG
Newton
Polyak
SG
Newton
Polyak

β = 1
5 · 10−3
1 · 100
5 · 10−3
4 · 10−3
1 · 100
2 · 10−3
1 · 10−3
2 · 10−1
3 · 10−4

β = 0
5 · 10−3
1 · 100
4 · 10−3
4 · 10−3
2 · 100
2 · 10−3
1 · 10−3
2 · 10−2
3 · 10−4

β = −1
5 · 10−3
2 · 10−1
5 · 10−3
5 · 10−3
3 · 10−2
3 · 10−3
2 · 10−3
3 · 10−3
3 · 10−4

Table 2: Stepsizes achieving suboptimality (cid:15) in the
fewest updates for various methods and S ∝ Hβ.

haviour of our generalization gap estimator by varying
(1) the number of parameters in a model and (2) the
label randomization ratio.

Experiments are performed using a fully connected
feedforward network with a single hidden layer trained
on a subset of 2k samples of SVHN (Netzer et al., 2011).
In Figure 3a we vary the number of units in the hidden
layer without label randomization while in Figure 3b
we vary the label randomization ratio with a ﬁxed
architecture. Each point is computed using 3 diﬀerent
random number generator seeds. The neural networks
are trained for 750k steps. The conﬁdence intervals
are provided using bootstrapping to estimate a 95%
conﬁdence interval. The Hessian, covariance matrices
and sensitivity are computed on a subset of size 5k of
the test data. Details can be found in Appendix B.2.

We now study the ability of the TIC across a wide va-
riety of models, datasets, and hyperparameters. More
speciﬁcally, we compare the TIC to the generaliza-
tion gap for: The experiments of Figure 4 are per-
formed with the experimental setup presented in sub-
section 5.1.1. Figure 4a shows that the TIC using
H and C computed over the test set is an excellent
estimator of the generalization gap. For comparison,
we also show in Figure 4b the generalization gap as a
function of H computed over the test set. We see that,

(a) Varying hidden layer size.

(b) Varying the label ran-
domization level.

Figure 3: Comparing the TIC to other estimators of
the generalization gap on SVHN. The TIC matches the
generalization gap more closely than both the AIC and
the sensitivity.

even when using the test set, the correlation is much
weaker than with the TIC.

(a) Gap vs. TIC.

(b) Gap vs. ﬂatness.

Figure 4: Generalization gap as a function of the
Takeuchi information criterion (left) and the trace of
the Hessian on the test set (right) for many architec-
tures, datasets, and hyperparameters. Correlation is
perfect if all points lie on a line. We see that the
Hessian cannot by itself capture the generalization gap.

5.5.1 Eﬃcient approximations to the TIC

Although the TIC is a good estimate of the generaliza-
tion gap, it can be expensive to compute on large mod-
els. Following our theoretical and empirical analysis of
the proximity of H and F, we propose two approxima-
tions to the TIC: Tr(F−1C) and Tr(C)/ Tr(F). They
are easier to compute as the F is in general easier to
compute than H and the second does not require any
matrix inversion.

Using the same experimental setting as in 5.5, we ob-
serve in Figure 5 that the replacing H with F leads
to almost no loss in predictive performance. On the
other hand, the ratio of the traces works best when the
generalization gap is high and tends to overestimate it
when it is small.

Intuition on Tr(C)/ Tr(F): it is not clear right away

On the interplay between noise and curvature

Elisseeﬀ, 2002), where a full training with a slightly
diﬀerent training set has been replaced with a local
search.

6 Conclusion and open questions

We clariﬁed the relationship between information ma-
trices used in optimization. While their diﬀerences
seem obvious in retrospect, the widespread confusion
makes these messages necessary. Indeed, several well-
known algorithms, such as Adam (Kingma and Ba,
2014), claiming to use second-order information about
the loss to accelerate training seem instead to be using
the covariance matrix of the gradients. Equipped with
this new understanding of the diﬀerence between the
curvature and noise information matrices, one might
wonder if the success of these methods is not due to
variance reduction instead. If so, one should be able to
combine variance reduction and geometry adaptation,
an idea attempted by Le Roux et al. (2011).

We also showed how, in certain settings, the geometry
of the noise could aﬀect the performance of second-
order methods. While Polyak momentum is aﬀected by
the scale of the noise, its performance is independent of
the geometry, similar to stochastic gradient but unlike
Newton method. However, empirical results indicate
that common loss functions are in the regime favorable
to second-order methods.

Finally, we investigated whether the Takeuchi informa-
tion criterion is relevant for estimating the generaliza-
tion gap in neural networks. We provided evidence that
this complexity measures involving the information ma-
trices is predictive of the generalization performance.

We hope this study will clarify the interplay of the noise
and curvature in common machine learning settings,
potentially giving rise to new optimization algorithms
as well as new methods to estimate the generalization
gap.

Acknowledgments

We would like to thank Gauthier Gidel, Reyhane Askari
and Giancarlo Kerg for reviewing an earlier version
of this paper. We also thank Aristide Baratin for
insightful discussions. Valentin Thomas acknowledges
funding from the Open Philantropy project.

References

Akaike, H. (1974). A new look at the statistical model
identiﬁcation. IEEE transactions on automatic con-
trol, 19(6):716–723.

Bach, F. and Moulines, E. (2013). Non-strongly-convex
smooth stochastic approximation with convergence

Figure 5: Generalization gap as a function of two
approximations to the Takeuchi Information Criterion:
Tr(F−1C) (left) and Tr(C)/ Tr(F) (right).

why the ratio of traces might be a interesting quan-
tity. However, as observed in ﬁgure 2, C and F are
remarkably aligned, but there remains a scaling factor.
If we had C = αF, then Tr(F−1C) = kα where k
is the dimension of the invertible subspace of F and
Tr(C)/ Tr(F) = dα where d is the dimensionality of
θ. So, up to a multiplicative constant (or an oﬀset
in log scale), we can expect these two quantities to
exhibit similarities. Notice that on ﬁgure 5, this oﬀset
does appear and is diﬀerent for every dataset (MNIST
has the smallest one, then SVHN and CIFAR10, just
slightly bigger).

5.6 The importance of the noise in

estimating the generalization gap

For a given model, the generalization gap captures
the discrepancy that exists between the training set
and the data distribution. Hence, estimating that
gap involves the evaluation of the uncertainty around
the data distribution. The TIC uses C to capture
that uncertainty but other measures probably exist.
However, estimators which do not estimate it are bound
to have failure modes. For instance, by using the
square norm of the derivative of the loss with respect
to the input, the sensitivity implicitly assumes that
the uncertainty around the inputs is isotropic and will
fail should the data be heavily concentrated in a low-
dimensional subspace. It would be interesting to adapt
the sensitivity to take the covariance of the inputs into
account.

Another aspect worth mentioning is that estimators
such as the margin assume that the classiﬁer is ﬁxed
but the data is a random variable. Then, the margin
quantiﬁes the probability that a new datapoint would
fall on the other side of the decision boundary. By
contrast, the TIC assumes that the data are ﬁxed but
that the classiﬁer is a random variable. It estimates the
probability that a classiﬁer trained on slightly diﬀerent
data would classify a training point incorrectly. In that,
it echoes the uniform stability theory (Bousquet and

Thomas, Pedregosa, van Merri¨enboer, Mangazol, Bengio, Le Roux

rate o (1/n). In Advances in Neural Information
Processing Systems, pages 773–781.

Beirami, A., Razaviyayn, M., Shahrampour, S., and
Tarokh, V. (2017). On optimal generalizability in
parametric learning. In Advances in Neural Infor-
mation Processing Systems, pages 3455–3465.

Bottou, L. and Bousquet, O. (2008). The tradeoﬀs of
large scale learning. In Advances in neural informa-
tion processing systems, pages 161–168.

Bousquet, O. and Elisseeﬀ, A. (2002). Stability and
generalization. Journal of machine learning research,
2(Mar):499–526.

Chaudhari, P. and Soatto, S. (2017). Stochastic gradi-
ent descent performs variational inference, converges
to limit cycles for deep networks. arXiv preprint
arXiv:1710.11029.

Dieuleveut, A., Bach, F., et al. (2016). Nonparametric
stochastic approximation with large step-sizes. The
Annals of Statistics, 44(4):1363–1399.

Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y.
(2017).
Sharp minima can generalize for deep
nets. International Conference on Machine Learning
(ICML).

Flammarion, N. and Bach, F. (2015). From averaging to
acceleration, there is only a step-size. In Conference
on Learning Theory, pages 658–695.

George, T., Laurent, C., Bouthillier, X., Ballas, N., and
Vincent, P. (2018). Fast approximate natural gra-
dient descent in a kronecker factored eigenbasis. In
Advances in Neural Information Processing Systems,
pages 9550–9560.

Hochreiter, S. and Schmidhuber, J. (1997). Flat min-

ima. Neural Computation, 9(1):1–42.

Ioﬀe, S. and Szegedy, C. (2015). Batch normalization:
Accelerating deep network training by reducing inter-
nal covariate shift. arXiv preprint arXiv:1502.03167.

Jastrzebski, S., Kenton, Z., Arpit, D., Ballas, N., Fis-
cher, A., Bengio, Y., and Storkey, A. (2017). Three
factors inﬂuencing minima in sgd. arXiv preprint
arXiv:1711.04623.

Keskar, N. S., Mudigere, D., Nocedal, J., Smelyan-
skiy, M., and Tang, P. T. P. (2017). On large-batch
training for deep learning: Generalization gap and
sharp minima. International Conference on Learning
Representations(ICLR).

Kingma, D. P. and Ba, J. (2014). Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Le Roux, N., Bengio, Y., and Fitzgibbon, A. (2011).
Improving ﬁrst and second-order methods by model-
ing uncertainty. Optimization for Machine Learning,
page 403.

Liang, T., Poggio, T., Rakhlin, A., and Stokes, J.
(2017). Fisher-rao metric, geometry, and complexity
of neural networks. arXiv preprint arXiv:1711.01530.

Mandt, S., Hoﬀman, M. D., and Blei, D. M. (2017).
Stochastic gradient descent as approximate bayesian
inference. The Journal of Machine Learning Re-
search, 18(1):4873–4907.

Martens, J. (2014). New insights and perspectives
on the natural gradient method. arXiv preprint
arXiv:1412.1193.

Murata, N., Yoshizawa, S., and Amari, S.-i. (1994). Net-
work information criterion-determining the number
of hidden units for an artiﬁcial neural network model.
IEEE Transactions on Neural Networks, 5(6):865–
872.

Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu,
B., and Ng, A. Y. (2011). Reading digits in natural
images with unsupervised feature learning. Neural
Information Processing Systems (NeurIPS).

Neyshabur, B., Bhojanapalli, S., McAllester, D., and
Srebro, N. (2017). Exploring generalization in deep
learning. In Advances in Neural Information Pro-
cessing Systems, pages 5947–5956.

Novak, R., Bahri, Y., Abolaﬁa, D. A., Pennington, J.,
and Sohl-Dickstein, J. (2018). Sensitivity and gen-
eralization in neural networks: an empirical study.
International Conference on Learning Representa-
tions (ICLR).

Rangamani, A., Nguyen, N. H., Kumar, A., Phan, D.,
Chin, S. H., and Tran, T. D. (2019). A scale invariant
ﬂatness measure for deep network minima. arXiv
preprint arXiv:1902.02434.

Sagun, L., Bottou, L., and LeCun, Y. (2016). Eigenval-
ues of the hessian in deep learning: Singularity and
beyond. arXiv preprint arXiv:1611.07476.

Schmidt, M. (2014). Convergence rate of stochastic
gradient with constant step size. Technical report,
UBC.

Takeuchi, K. (1976). The distribution of information
statistics and the criterion of goodness of ﬁt of models.
Mathematical Science, 153:12–18.

Wang, S., Zhou, W., Lu, H., Maleki, A., and Mirrokni,
V. (2018). Approximate leave-one-out for fast pa-
rameter tuning in high dimensions. arXiv preprint
arXiv:1807.02694.

Kunstner, F., Balles, L., and Hennig, P. (2019). Limi-
tations of the empirical ﬁsher approximation. arXiv
preprint arXiv:1905.12558.

Yaida, S. (2018). Fluctuation-dissipation relations
arXiv preprint

for stochastic gradient descent.
arXiv:1810.00004.

On the interplay between noise and curvature

Zhu, Z., Wu, J., Yu, B., Wu, L., and Ma, J. (2018). The
anisotropic noise in stochastic gradient descent: Its
behavior of escaping from minima and regularization
eﬀects. arXiv preprint arXiv:1803.00195.

(cid:90)

(cid:90)

Thomas, Pedregosa, van Merri¨enboer, Mangazol, Bengio, Le Roux

A Proofs

A.1 Bounds between H, F and C

A.1.1 Bounds with backward χ2 divergence

|Fij − Hij|2 = |

qθ(x, y)(cid:0)∇2

θ(cid:96)(x, y)(cid:1)

ijd(x, y) −

p(x, y)(cid:0)∇2

θ(cid:96)(x, y)(cid:1)

ijd(x, y)|2

(cid:90)

(cid:90)

= |

= |

(cid:0)qθ(x, y) − p(x, y)(cid:1)(cid:0)∇2

θ(cid:96)(x, y)(cid:1)

ijd(x, y)|2

(cid:90) (cid:0)qθ(x, y) − p(x, y)(cid:1)
(cid:112)p(x, y)

(cid:0)(cid:112)p(x, y)∇2

θ(cid:96)(x, y)(cid:1)

ijd(x, y)|2

≤

(cid:90) (cid:0)qθ(x, y) − p(x, y)(cid:1)2
p(x, y)

(cid:90)

d(x, y)

p(x, y)(cid:0)∇2

θ(cid:96)(x, y)(cid:1)2

ijd(x, y)

= Dχ2(qθ||p) Ep[(cid:0)∇2
Where we used Cauchy-Schwarz inequality and Dχ2 denotes the χ2 divergence.

θ(cid:96)(x, y)(cid:1)2
ij]

Where H(x, y) (cid:44) ∇2

θ(cid:96)(x, y) is the empirical hessian for one sample and the || · ||2 is the Frobenius norm.

||F − H||2 ≤ Dχ2 (qθ||p) Ep[||H(x, y)||2
2]

In the same way

(cid:90)

|Fij − Cij|2 = |

qθ(x, y)(cid:0)∇θ(cid:96)(x, y)∇θ(cid:96)(x, y)(cid:62)(cid:1)

ijd(x, y) −

p(x, y)(cid:0)∇θ(cid:96)(x, y)∇θ(cid:96)(x, y)(cid:62)(cid:1)

ijd(x, y)|2

≤ Dχ2(qθ||p) Ep[(cid:0)∇θ(cid:96)(x, y)∇θ(cid:96)(x, y)(cid:62)(cid:1)2
ij]

For C(x, y) (cid:44) ∇θ(cid:96)(x, y)∇θ(cid:96)(x, y)(cid:62) we have

||F − C||2 ≤ Dχ2 (qθ||p) Ep[||C(x, y)||2]

Hence

||C − H||2 ≤ Dχ2(qθ||p) Ep[||C(x, y)||2 + ||H(x, y)||2]

A.1.2 Bounds with forward χ2 divergence

Note that in the above proof, breaking the integral in two with Cauchy-Schwarz inequality could have been done
using

|Fij − Hij|2 = |

(cid:90) (cid:0)qθ(x, y) − p(x, y)(cid:1)
(cid:112)qθ(x, y)

(cid:0)(cid:112)qθ(x, y)∇2

θ(cid:96)(x, y)(cid:1)

ijd(x, y)|2

≤

(cid:90) (cid:0)qθ(x, y) − p(x, y)(cid:1)2
qθ(x, y)

(cid:90)

d(x, y)

= Dχ2(p||qθ) Eqθ [(cid:0)∇2

θ(cid:96)(x, y)(cid:1)2
ij]

qθ(x, y)(cid:0)∇2

θ(cid:96)(x, y)(cid:1)2

ijd(x, y)

Similarly

Thus

|Fij − Cij|2 ≤ Dχ2(p||qθ) Eqθ [(cid:0)∇θ(cid:96)(x, y)∇θ(cid:96)(x, y)(cid:62)(cid:1)2
ij]

||C − H||2 ≤ Dχ2 (p||qθ) Eqθ [||C(x, y)||2 + ||H(x, y)||2]

On the interplay between noise and curvature

A.1.3 Proof of Proposition 2

From the upper bound assumption we have

f (θk+1) ≤ f (θk) + ∇f (θk)(cid:62)(θk+1 − θk) + 1

2 (θk+1 − θk)(cid:62)H(θk+1 − θk)

= f (θk) − α∇f (θk)(cid:62)M∇(cid:96)(θk, x) + α2

2 ∇(cid:96)(θk, x)(cid:62)M(cid:62)HM∇(cid:96)(θk, x) .

Subtracting f (θ∗) from both sides and taking conditional expectation we have

E[f (θk+1) − f (ˆθ∗)] ≤ f (θk) − f (ˆθ∗) − α∇f (θk)(cid:62)ME(cid:2)∇(cid:96)(θk, x)(cid:3) + α2

E(cid:2) Tr (cid:0)M(cid:62)HM∇(cid:96)(θk, x)∇(cid:96)(θk, x)(cid:62)(cid:1)(cid:3)

2

≤ f (θk) − f (ˆθ∗) − α∇f (θk)(cid:62)M∇f (θk) + α2
= f (θk) − f (ˆθ∗) − α∇f (θk)(cid:62)(M −

2 Tr (cid:0)M(cid:62)HM(C + ∇f (θk)∇f (θk))(cid:1)
2 Tr (cid:0)M(cid:62)HMC)(cid:1) ,

M(cid:62)HM)∇f (θk) + α2

α
2

where in the second inequality we have used the covariance bound.
For µM I (cid:52) M − α
2 M(cid:62)HM and using the strong convexity bound 1

E[f (θk+1) − f (ˆθ∗)] ≤ f (θk) − f (ˆθ∗) − αµM ∇f (θk)(cid:62)∇f (θk) + α2

2µ (cid:107)∇f (θ)(cid:107)2 ≥ f (θ) − f (ˆθ∗), we can simplify to
2 Tr (cid:0)M(cid:62)HMC(cid:1)

≤ f (θk) − f (ˆθ∗) − 2αµM µ(cid:0)f (θk) − f (ˆθ∗)(cid:1) + α2
= (cid:0)1 − 2αµM µ(cid:1)(cid:0)f (θk) − f (ˆθ∗)(cid:1) + α2

2 Tr (cid:0)M(cid:62)HMC(cid:1)

2 Tr (cid:0)M(cid:62)HMC(cid:1)

Assuming αµM µ ≤ 1

2 , we have (cid:80)k

i=0

(cid:0)1 − 2αµM µ(cid:1)i

≤ (cid:80)∞
i=0

(cid:0)1 − 2αµM µ(cid:1)i

= 1

2αµM µ . Therefore

E[f (θk+1) − f (ˆθ∗)] ≤ f (θk) − f (ˆθ∗) − αµM ∇f (θk)(cid:62)∇f (θk) + α2

2 Tr (cid:0)M(cid:62)HMC(cid:1)

≤ f (θk) − f (ˆθ∗) − 2αµM µ(cid:0)f (θk) − f (ˆθ∗)(cid:1) + α2
= (cid:0)1 − 2αµM µ(cid:1)(cid:0)f (θk) − f (ˆθ∗)(cid:1) + α2

2 Tr (cid:0)M(cid:62)HMC(cid:1)

2 Tr (cid:0)M(cid:62)HMC(cid:1)

Assuming αµM µ ≤ 1
and chaining inequalities we then have

2 , we have (cid:80)k

i=0

(cid:0)1 − 2αµM µ(cid:1)i

≤ (cid:80)∞
i=0

(cid:0)1 − 2αµM µ(cid:1)i

= 1

2αµM µ . Taking full expectations

E[f (θk) − f (ˆθ∗)] ≤ (cid:0)1 − 2αµM µ(cid:1)k(cid:0)f (θ0) − f (ˆθ∗)(cid:1) + α

4µM µ Tr (cid:0)M(cid:62)HMC(cid:1) .

This concludes the proof.

A.1.4 Convergence to limit cycles in the quadratic case

For SGD with constant stepsize α and preconditioner M, the update equation on the parameters is

In our quadratic case, ∇f (θt) = H(θt − θ∗) with E[(cid:15)t] = 0 and E[(cid:15)t(cid:15)(cid:62)

t ] = S. By deﬁning δt = E[θt − θ∗], we have

θt+1 = θt − αM(∇f (θt) + (cid:15)t)

δt+1 = (I − αMH)δt

= (I − αMH)t+1δ0

This concludes the ﬁrst result of proposition on the quadratic case.

Thomas, Pedregosa, van Merri¨enboer, Mangazol, Bengio, Le Roux

By deﬁning, Σt = E[(θt − θ∗)(θt − θ∗)(cid:62)], we get

Σt+1 = Σt − E(cid:2)αM(cid:0)H(θt − θ∗) + (cid:15)t
(cid:1)(cid:62)

(cid:1)(θt − θ∗)(cid:62)(cid:3)
− αE(cid:2)(θt − θ∗)(cid:0)θt − θ∗ + (cid:15)t
HM(cid:62)(cid:3)
+ α2E(cid:2)MH(θt − θ∗)(θt − θ∗)(cid:62)HM(cid:62)(cid:3)
+ α2E(cid:2)M(cid:15)t(cid:15)(cid:62)
= Σt − αMHΣt − αΣtHM(cid:62) + α2MHΣtHM(cid:62) + α2MSM(cid:62)
= (I − αMH)Σt(I − αMH)(cid:62) + α2MSM(cid:62)

t M(cid:62)(cid:3)

A.2 Expected suboptimality for SG and Polyak momentum on quadratic functions

We detail here the computation of the expected suboptimality at each timestep when optimizing a quadratic
function with a diagonal Hessian when the noise is also diagonal. Note that all these results apply if H and S are
simultaneously diagonalizable by a change of basis.

We assume that f is a quadratic with Hessian H and that, at each time step, we receive a gradient perturbed by
a random variable (cid:15) with E[(cid:15)] = 0, E[(cid:15)(cid:15)(cid:62)] = S. Further, we shall assume that H and S are both diagonal. With
these assumptions, the optimization occurs in each dimension independently and we can thus focus on a single
dimension. We will denote by h and c the hessian and noise variance along that direction.

A.2.1 Proof of proposition 4

We can compare this result to the same setting where we use stochastic gradient with a diagonal preconditioning
matrix M. Then we get

and

si = (1 − αMiiHii)2si + α2M2

iiSii

si =

αMiiSii
2Hii − αMiiH2
ii

,

E[f (θt) − f (ˆθ∗)] =

1
2

(cid:88)

i

αMiiSii
2 − αMiiHii

+ O(e−t) .

Generalizing to simultaneously diagonalizable matrices, we get

E[f (θt) − f (ˆθ∗)] =

Tr((2I − αMH)−1MS) + O(e−t) .

α
2

A.2.2 Proof of proposition 5

Polyak momentum update equations are:

Using the quadratic assumption, we can rewrite

vt = γvt−1 + ∇f (θt) + (cid:15)

θt+1 = θt − αvt .

vt+1 = γvt + ∇f (θt+1) + (cid:15)
= γvt + hθt+1 + (cid:15)
= γvt + hθt − αhvt + (cid:15) ,

and the full update can be written in matrix form

(cid:20) θt
vt

(cid:21)

=

(cid:20) 1

−α

h γ − αh

(cid:21) (cid:20) θt−1
vt−1

(cid:21)

+

(cid:20) 0
(cid:15)

(cid:21)

(16)

(17)

(18)

(19)

(20)

(21)

(22)

(23)

(24)

(25)

(26)

(27)

On the interplay between noise and curvature

Denoting P =

(cid:20) 1

−α

(cid:21)

h γ − αh

and St = [

(cid:20) θt
vt

(cid:21) (cid:20) θt
vt

(cid:21)T

, we have

E[St|St−1] = P St−1P T +

(cid:20) 0
0

(cid:21)

.

0
c

If there is a limit cycle for

, it will satisfy

(cid:21)

(cid:20) θt
vt

S = P SP T +

(cid:20) 0
0

(cid:21)

.

0
c

Writing S =

, we have

(cid:20) sθ
svθ

(cid:21)

svθ
sv

The ﬁrst equation gives svθ = α

Finally, the second equation gives

and

sθ = sθ − 2αsvθ + α2sv
sv = h2sθ + 2h(γ − αh)svθ + (γ − αh)2sv + c
svθ = hsθ + (γ − 2αh)svθ − α(γ − αh)sv .

2 sv and the last one becomes
α
2

sv = hsθ + (γ − 2αh)

α
2

sθ =

α(1 + γ)
2h

sv .

sv − α(γ − αh)sv

+ 2h(γ − αh)

+ (γ − αh)2

sv + c

(cid:19)

α
2

sv =

sv =

(cid:18)
h2 α(1 + γ)
2h
c
(1 − γ) (cid:0)1 + γ − αh

(cid:1)

2

sθ =

α(1 + γ)c
h(1 − γ)(2 + 2γ − αh)

.

Adding all dimensions together and multiplying by the Hessian to get the value function, we get

E[f (θt) − f (ˆθ∗)] =

1
2

(cid:88)

i

α(1 + γ)Sii
(1 − γ)(2 + 2γ − αHii)

+ O(e−t) .

Generalizing to simultaneously diagonalizable matrices, we get

E[f (θt) − f (ˆθ∗)] =

α
2

(1 + γ)
(1 − γ)

Tr (cid:0)(2(1 + γ)I − αH)−1S(cid:1) + O(e−t) .

(28)

A.2.3 Comparison between stochastic gradient and Polyak momentum in the large noise regime

When the desired suboptimality is small, it requires a small α and the two suboptimality can be approximated by

f (θt) − f (ˆθ∗) ≈

αSii
(1 − γ)

+ o(1)

f (θt) − f (ˆθ∗) ≈

αSii + o(1) ,

1
4

1
4

(cid:88)

i
(cid:88)

i

(Momentum)

(Stochastic gradient)

and we see that momentum needs a stepsize α that is (1 − γ) times that of stochastic gradient to achieve the
same suboptimality, countering any gain. This is what we see in Table 2.

Thomas, Pedregosa, van Merri¨enboer, Mangazol, Bengio, Le Roux

B Experimental details

B.1 Details on the Hessian inverse

As H is highly degenerate in neural networks, we compute an inverse of H by cutting all the eigenvalues smaller
than 10−3 × λmax where λmax is the biggest eigenvalue of H. We observed that 10−3 and 10−3 were reasonable
constants for selecting the eigenvalues of signiﬁcant magnitude. Using smaller constant sometimes lead to very
noisy estimates of the TIC while using a bigger constant would lead to severe underestimation of the criterion.

B.2 Details on the large scale experiments

These details apply for the experiments conducted in subsection 5.5, ﬁgure 4 and all ﬁgures in subsection 5.1.

We remind the reader the setup.

• 5 diﬀerent architectures: logistic regression, a 1-hidden layer and 2-hidden layer fully connected network, and
2 small convolutional neural networks (CNNs, one with batch normalization (Ioﬀe and Szegedy, 2015) and
one without);

• 3 datasets: MNIST, CIFAR-10, SVHN;
• 3 learning rates: 10−2, 5 · 10−3, 10−3 using vanilla SGD with momentum µ = 0.9;
• 2 batch sizes: 64, 512;
• 5 dataset sizes: 5k, 10k, 20k, 25k, 50k.

We train for 750k steps and compute our metrics every 75k steps.

Data preprocessing: We choose to greyscale, resize to 7 × 7 pixels and normalize all the images in the 3
datasets used (CIFAR-10, MNIST and SVHN). This way, we can design architectures with a relatively low number
of parameters.

Architectures:

• mlp: This one is a one hidden layer MLP. Input size is 7 × 7 = 49 and output size is 10. The default number

of hidden units is 70. We use ReLU activations.

• big mlp: The architecture is the same as above but with one additional hidden layer.
• logreg: This is simple a 49 × 10 linear classiﬁer.
• cnn: It is a small CNN with 3 layers. A ﬁrst conv layer with kernel 3 × 3, 0 padding and 15 channels. The
next layer has 20 channels and same parameters. The last layer has 10 channels and directly outputs the
class scores.

• cnn bn: Same architecture as above, except for a spatial batch-norm after the second layer.

B.3 Details on experiments of subsection 5.5

For these experiments we train one hidden layer MLPs on SVHN. Each points is computed by training three
times with three diﬀerent random seed until convergence. In ﬁgure 3a, the labels are kept without corruption and
we vary the hidden size layer by using {8, 10, 16, 20, 25, 30, 40, 50, 60, 70, 80, 100} hidden units in the hidden layer.

In ﬁgure 3b, we ﬁx the number of hidden units to 70 but we vary the labels corruption percentage from 0% to
100% (included) by increments of 10%.

The networks are trained for 150k gradients steps with a learning rate of 5e−3 and a batch size of 256. We used
a subset of 2000 samples of SVHN to remain in the highly overparametrized regime, our networks were able to ﬁt
random data.

On the interplay between noise and curvature

(a) Varying hidden layer size.

(b) Varying label randomization level.

Figure 6: The train and test errors associated with the experiments 3a and 3b. We see that while we use small
networks, they are still able to ﬁt the data completely provided we use more than 20 hidden units. This behavior
mirrors the one of bigger networks.

