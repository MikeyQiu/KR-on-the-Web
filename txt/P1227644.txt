9
1
0
2
 
y
a
M
 
7
2
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
1
7
0
1
1
.
5
0
9
1
:
v
i
X
r
a

Learning step sizes for unfolded sparse coding

Pierre Ablin∗

Thomas Moreau∗
Alexandre Gramfort

Mathurin Massias

Inria, Université Paris-Saclay
Saclay, France
∗: both authors contributed equally.

May 28, 2019

Abstract

Sparse coding is typically solved by iterative optimization techniques,
such as the Iterative Shrinkage-Thresholding Algorithm (ISTA). Unfolding
and learning weights of ISTA using neural networks is a practical way to
accelerate estimation. In this paper, we study the selection of adapted
step sizes for ISTA. We show that a simple step size strategy can improve
the convergence rate of ISTA by leveraging the sparsity of the iterates.
However, it is impractical in most large-scale applications. Therefore,
we propose a network architecture where only the step sizes of ISTA are
learned. We demonstrate that for a large class of unfolded algorithms,
if the algorithm converges to the solution of the Lasso, its last layers
correspond to ISTA with learned step sizes. Experiments show that our
method is competitive with state-of-the-art networks when the solutions
are sparse enough.

1

Introduction

The resolution of convex optimization problems by iterative algorithms has
become a key part of machine learning and signal processing pipelines. Amongst
these problems, special attention has been devoted to the Lasso (Tibshirani, 1996),
due to the attractive sparsity properties of its solution (see Hastie et al. 2015 for
Rn×m and a
an extensive review). For a given input x
regularization parameter λ > 0 , the Lasso problem is

Rn , a dictionary D

∈

∈

z∗(x)

arg min
z∈Rm

∈

Fx(z) with Fx(z) (cid:44) 1
2 (cid:107)

x

Dz

2 + λ
(cid:107)
(cid:107)

z

1 .
(cid:107)

−

(1)

A variety of algorithms exist to solve Problem (1), e.g. proximal coordinate
descent (Tseng, 2001; Friedman et al., 2007), Least Angle Regression (Efron

1

et al., 2004) or proximal splitting methods (Combettes and Bauschke, 2011).
The focus of this paper is on the Iterative Shrinkage-Thresholding Algorithm
(ISTA, Daubechies et al. 2004), which is a proximal-gradient method applied to
Problem (1). ISTA starts from z(0) = 0 and iterates

(cid:18)

1
L

z(t+1) = ST

z(t)

D(cid:62)(Dz(t)

x),

(2)

−
where ST is the soft-thresholding operator deﬁned as ST(x, u) (cid:44) sign(x) max(
|−
u, 0) , and L is the greatest eigenvalue of D(cid:62)D . In the general case, ISTA
converges at rate 1/t , which can be improved to the optimal rate 1/t2 (Nes-
terov, 1983). However, this optimality stands in the worst possible case, and
linear rates are achievable in practice (Liang et al., 2014).

x
|

−

(cid:19)

λ
L

,

A popular line of research to improve the speed of Lasso solvers is to try to identify
the support of z∗ , in order to diminish the size of the optimization problem (El
Ghaoui et al., 2012; Ndiaye et al., 2017; Johnson and Guestrin, 2015; Massias
et al., 2018). Once the support is identiﬁed, larger steps can also be taken,
leading to improved rates for ﬁrst order algorithms (Liang et al., 2014; Poon
et al., 2018; Sun et al., 2019).

However, these techniques only consider the case where a single Lasso prob-
N
lem is solved. When one wants to solve the Lasso for many samples
i=1 –
e.g. in dictionary learning (Olshausen and Field, 1997) – it is proposed by Gre-
gor and Le Cun (2010) to learn a T -layers neural network of parameters Θ ,
ΦΘ : Rn
z∗(x) . This Learned-ISTA (LISTA) algo-
rithm yields better solution estimates than ISTA on new samples for the same
number of iterations/layers. This idea has led to a profusion of literature (sum-
marized in Table A.1 in appendix). Recently, it has been hinted by Zhang and
Ghanem (2018); Ito et al. (2018); Liu et al. (2019) that only a few well-chosen
parameters can be learned while retaining the performances of LISTA.

Rm such that ΦΘ(x)

→

xi

(cid:39)

{

}

In this article, we study strategies for LISTA where only step sizes are learned.
In Section 3, we propose Oracle-ISTA, an analytic strategy to obtain larger step
sizes in ISTA. We show that the proposed algorithm’s convergence rate can be
much better than that of ISTA. However, it requires computing a large number
of Lipschitz constants which is a burden in high dimension. This motivates the
introduction of Step-LISTA (SLISTA) networks in Section 4, where only a step
size parameter is learned per layer. As a theoretical justiﬁcation, we show in
Theorem 4.4 that the last layers of any deep LISTA network converging on the
Lasso must correspond to ISTA iterations with learned step sizes. We validate
the soundness of this approach with numerical experiments in Section 5.

2 Notation and Framework

Notation The (cid:96)2 norm on Rn is
M
The Frobenius matrix norm is
(cid:107)

. For p

p is the (cid:96)p norm.
(cid:107) · (cid:107)
F . The identity matrix of size m is Idm .
(cid:107)

(cid:107) · (cid:107)

[1,

∞

] ,

∈

2

−

−

x), βλ) .

αD(cid:62)(Dz

αW (cid:62)(Dz

Iterations are denoted z(t) . λ > 0 is
ST is the soft-thresholding operator.
the regularization parameter. The Lasso cost function is Fx . ψα(z, x) is one
iteration of ISTA with step α: ψα(z, x) = ST(z
x), αλ) . φθ(z, x)
is one iteration of a LISTA layer with parameters θ = (W, α, β): φθ(z, x) =
ST(z

−
Rm , the support is
. Given z
The set of integers between 1 and m is
∈
(cid:74)
Rn×m is the
supp(z) =
, DS
0, m
1, m
1, m
∈ (cid:74)
(cid:75)
(cid:75)
(cid:75)
matrix containing the columns of D indexed by S. We denote LS, the greatest
eigenvalue of D(cid:62)
x)
|
networks parameters are between brackets, e.g. Θ =
function is sign(x) = 1 if x > 0,

⊂ (cid:74)
S DS. The equicorrelation set is E =

j (Dz∗
1, m
:
−
(cid:75)
. Neural
∞ = 1
}
T −1
t=0 . The sign

. The equiregularization set is
= λ
}

∈ (cid:74)
D(cid:62)x
(cid:107)
(cid:107)
α(t), β(t)
{

1 if x < 0 and 0 is x = 0 .

1, m
(cid:75)
. For S

j
{
Rn :

} ⊂ (cid:74)

∞ =

D(cid:62)

= 0

: zj

−

∈

∈

B

x

{

}

{

j

|

−

Framework This paragraph recalls some properties of the Lasso. Lemma 2.1
gives the ﬁrst-order optimality conditions for the Lasso.
Lemma 2.1 (Optimality for the Lasso). The Karush-Kuhn-Tucker (KKT)
conditions read

z∗

∈

arg min Fx

j
⇔ ∀

, D(cid:62)
1, m
(cid:75)

j (x

−

∈ (cid:74)

Dz∗)

λ∂

z∗
j |

|

∈

=

(cid:40)

λ sign z∗
,
j }
{
λ, λ],
[
−

if z∗
= 0 ,
j (cid:54)
if z∗
j = 0 .
(3)

(cid:107)

D(cid:62)x

∞ , it holds arg min Fx =
(cid:107)

Deﬁning λmax (cid:44)
λmax . For some
results in Section 3, we will need the following assumption on the dictionary
D:
Assumption 2.2 (Uniqueness assumption). D is such that the solution of
Problem (1) is unique for all λ and x i.e. arg min Fx =

} ⇔

0
{

z∗

≥

λ

.

{

}

Assumption 2.2 may seem stringent since whenever m > n , Fx is not strictly con-
vex. However, it was shown in Tibshirani (2013, Lemma 4) – with earlier results
from Rosset et al. 2004 – that if D is sampled from a continuous distribution,
Assumption 2.2 holds for D with probability one.
Deﬁnition 2.3 (Equicorrelation set). The KKT conditions motivate the intro-
duction of the equicorrelation set E (cid:44)
, since
j
|
{
j = 0 , i.e. E contains the support of any solution z∗ .
z∗
j /
∈
When Assumption 2.2 holds, we have E = supp(z∗) (Tibshirani, 2013, Lemma
16).

1, m
(cid:75)

j (Dz∗

D(cid:62)
|

E =

= λ

∈ (cid:74)

⇒

x)

−

}

:

We consider samples x in the equiregularization set

∞ (cid:44)

x

Rn :

D(cid:62)x
(cid:107)

∞ = 1
}

,

B

{
which is the set of x such that λmax(x) = 1 . Therefore, when λ
is z∗(x) = 0 for all x
this reason, we assume 0 < λ < 1 in the following.

∞ , and when λ < 1 , z∗(x)

∈ B

∈

(cid:107)

= 0 for all x

≥

1 , the solution
∞ . For

∈ B

(4)

3

3 Better step sizes for ISTA

The Lasso objective is the sum of a L-smooth function, 1
function with an explicit proximal operator, λ
for this problem, with the sequence of step sizes (α(t)) consists in iterating

2 , and a
−
1 . Proximal gradient descent

(cid:107) · (cid:107)

2 (cid:107)

· (cid:107)

D

x

z(t+1) = ST

z(t)

α(t)D(cid:62)(Dz(t)

(cid:16)

−

x), λα(t)(cid:17)

.

−

ISTA follows these iterations with a constant step size α(t) = 1/L . In the follow-
ing, denote ψα(z, x) (cid:44) ST(z
x), αλ). One iteration of ISTA can
be cast as a majorization-minimization step (Beck and Teboulle, 2009). Indeed,
for all z

αD(cid:62)(Dz(t)

Rm ,

−

−

∈
Fx(z) = 1
2 (cid:107)

x

Dz(t)

2 + (z
(cid:107)

−

−

z(t))(cid:62)D(cid:62)(Dz(t)

x) + 1

D(z

−

2 (cid:107)

z(t))

2 + λ
(cid:107)
(cid:107)

z

1
(cid:107)

−

x

1
2 (cid:107)
(cid:124)

−

≤

Dz(t)

2 + (z
(cid:107)

−

z(t))(cid:62)D(cid:62)(Dz(t)

x) + L

−
(cid:123)(cid:122)
(cid:44) Qx,L(z, z(t))

z
2 (cid:107)

−

z(t)

2 + λ

(cid:107)

,

z
(cid:107)

1
(cid:107)
(cid:125)

(7)

2 . The

where we have used the inequality (z
minimizer of Qx,L(

z(t)
z(t))(cid:62)D(cid:62)D(z
, z(t)) is ψ1/L(z(t), x), which is the next ISTA step.
·

z
L
(cid:107)

z(t))

−

≤

−

−

(cid:107)

Oracle-ISTA: an accelerated ISTA with larger step sizes Since the
iterates are sparse, this approach can be reﬁned. For S
, let us deﬁne
the S-smoothness of D as

1, m
(cid:75)

⊂ (cid:74)

LS (cid:44) max

z(cid:62)D(cid:62)Dz, s.t.

z

z
(cid:107)

(cid:107)

= 1 and supp(z)

S ,

(8)

⊂

with the convention L∅ = L . Note that LS is the greatest eigenvalue of D(cid:62)
S DS
L ,
where DS
since L is the solution of Equation (8) without support constraint. Assume
supp(z(t))

Rn×|S| is the columns of D indexed by S . For all S , LS

S . Combining Equations (6) and (8), we have

≤

∈

⊂

z s.t. supp(z)

S, Fx(z)

Qx,LS (z, z(t)) .

⊂

≤

∀

The minimizer of the r.h.s is z = ψ1/LS (z(t), x) . Furthermore, the r.h.s.
is
a tighter upper bound than the one given in Equation (7) (see illustration
in Figure 1). Therefore, using z(t+1) = ψ1/LS (z(t), x) minimizes a tighter upper
bound, provided that the following condition holds

(5)

(6)

(9)

((cid:63))

supp(z(t+1))

S .

⊂

4

Figure 1: Majorization illustration. If z(t)
, z(t)) is a tighter
has support S , Qx,LS (
·
, z(t)) on the
upper bound of Fx than Qx,L(
·
set of points of support S .

Algorithm 1: Oracle-ISTA (OISTA) with larger step sizes
Input: Dictionary D , target x , number of iterations T
z(0) = 0
for t = 0, . . . , T

1 do

−
Compute S = supp(z(t)) and LS using an oracle ;
Set y(t+1) = ψ1/LS (z(t), x) ;
if Condition (cid:63) : supp(y(t+1))
⊂
else Set z(t+1) = ψ1/L(z(t), x) ;

S then Set z(t+1) = y(t+1) ;

Output: Sparse code z(T )

≤

Oracle-ISTA (OISTA) is an accelerated version of ISTA which leverages the
sparsity of the iterates in order to use larger step sizes. The method is summarized
in Algorithm 1. OISTA computes y(t+1) = ψ1/Ls (z(t), x) , using the larger step
size 1/LS , and checks if it satisﬁes the support Condition (cid:63). When the condition
is satisﬁed, the step can be safely accepted. In particular Equation (9) yields
Fx(y(t+1))
Fx(z(t)) . Otherwise, the algorithm falls back to the regular ISTA
iteration with the smaller step size. Hence, each iteration of the algorithm
is guaranteed to decrease Fx . The following proposition shows that OISTA
converges in iterates, achieves ﬁnite support identiﬁcation, and eventually reaches
a safe regime where Condition (cid:63) is always true.
Proposition 3.1 (Convergence, ﬁnite-time support identiﬁcation and safe
regime). When Assumption 2.2 holds, the sequence (z(t)) generated by the algo-
rithm converges to z∗ = arg min Fx .
Further, there exists an iteration T ∗ such that for t
supp(z∗) (cid:44) S∗ and Condition (cid:63) is always statisﬁed.

T ∗ , supp(z(t)) =

≥

Sketch of proof (full proof in Subsection B.1). Using Zangwill’s global conver-
gence theorem (Zangwill, 1969), we show that all accumulation points of (z(t))
are solutions of Lasso. Since the solution is assumed unique, (z(t)) converges to
z∗ . Then, we show that the algorithm achieves ﬁnite-support identiﬁcation with
a technique inspired by Hale et al. (2008). The algorithm gets arbitrary close to
z∗ , eventually with the same support. We ﬁnally show that in a neighborhood
of z∗ , the set of points of support S∗ is stable by ψ1/LS (
, x) . The algorithm
·
eventually reaches this region, and then Condition (cid:63) is true.

5

It follows that the algorithm enjoys the usual ISTA convergence results replacing
L with LS∗ .
Proposition 3.2 (Rates of convergence). For t > T ∗ , Fx(z(t))
LS∗
If additionally inf (cid:107)z(cid:107)=1 (cid:107)
is
Fx(z∗)
Fx(z(t))
(1

2 = µ∗ > 0 , then the convergence rate for t

(cid:107)z∗−z(T ∗ )(cid:107)2
2(t−T ∗)

(Fx(z(T ∗))

Fx(z∗)) .

Fx(z∗)

DS∗ z

(cid:107)
µ∗
LS∗ )t−T ∗

T ∗

≥

≤

−

.

−

≤

−

−

Sketch of proof (full proof in Subsection B.2). After iteration T ∗ , OISTA is
S∗ . This function is
equivalent to ISTA applied on Fx(z) restricted to z
LS∗ -smooth, and µ∗-strongly convex if µ∗ > 0 . Therefore, the classical ISTA
rates apply with improved condition number.

∈

and in the µ-strongly convex case (1

These two rates are tighter than the usual ISTA rates – in the convex case
L (cid:107)z∗(cid:107)2
Fx(z∗)) (Beck and
2t
Teboulle, 2009). Finally, the same way ISTA converges in one iteration when D
is orthogonal (D(cid:62)D = Idm), OISTA converges in one iteration if S∗ is identiﬁed
and DS∗ is orthogonal.
Proposition 3.3. Assume D(cid:62)

. Then, z(T ∗+1) = z∗ .

µ∗
L )t(Fx(0)

S∗ DS∗ = LS∗ Id|S∗|

−

−

Proof. For z s.t. supp(z) = S∗ , Fx(z) = Qx,LS (z, z(T ∗)) . Hence, the OISTA
step minimizes Fx .

Quantiﬁcation of the rates improvement in a Gaussian setting The
following proposition gives an asymptotic value for LS
Rn×m are i.i.d centered
Proposition 3.4. Assume that the entries of D
Gaussian variables with variance 1 . Assume that S consists of k integers chosen
uniformly at random in
with linear ratios
γ, k/m
m/n

L in a simple setting.

1, m
(cid:74)
(cid:75)
ζ . Then

. Assume that k, m, n

∞

→

+

∈

→

→

LS
L →

(cid:18) 1 + √ζγ
1 + √γ

(cid:19)2

.

(10)

This is a direct application of the Marchenko-Pastur law (Marchenko and Pas-
tur, 1967). The law is illustrated on a toy dataset in Figure D.1. In Proposi-
tion 3.4, γ is the ratio between the number of atoms and number of dimensions,
1 . In an overcomplete setting, we
and the average size of S is described by ζ
have γ
ζL . Therefore,
if z∗ is very sparse (ζ
1), the convergence rates of Proposition 3.2 are much
better than those of ISTA.

1 , yielding the approximation of Equation (10): LS

(cid:29)

(cid:28)

≤

(cid:39)

6

Example Figure 2 compares the OISTA, ISTA, and FISTA on a toy problem.
The improved rate of convergence of OISTA is illustrated. Further comparisons
are displayed in Figure D.2 for diﬀerent regularization parameters λ . While this
demonstrates a much faster rate of convergence, it requires computing several
Lipschitz constants LS , which is cumbersome in high dimension. This motivates
the next section, where we propose to learn those steps.

Figure 2: Convergence curves of
OISTA, ISTA, and FISTA on a toy
problem with n = 10 , m = 50 ,
λ = 0.5 . The bottom ﬁgure dis-
plays the (normalized) steps taken
by OISTA at each iteration. Full
experimental setup described in Ap-
pendix D.

4 Learning unfolded algorithms

Network architectures At each step, ISTA performs a linear operation to
compute an update in the direction of the gradient D(cid:62)(Dz(t)
x) and then an
element-wise non linearity with the soft-thresholding operator ST . The whole
algorithm can be summarized as a recurrent neural network (RNN), presented
in Figure 3a. Gregor and Le Cun (2010) introduced Learned-ISTA (LISTA), a
neural network constructed by unfolding this RNN T times and learning the
weights associated to each layer. The unfolded network, presented in Figure 3b,
iterates z(t+1) = ST(W (t)
z z(t), λβ(t)) . It outputs exactly the same vector
as T iterations of ISTA when W (t)
L .
Empirically, this network is able to output a better estimate of the sparse code
solution with fewer operations.

and β(t) = 1

L , W (t)

x x + W (t)

x = D(cid:62)

z = Idm

D(cid:62)D
L

−

−

x

Wx

z∗
x

W (0)
x

Wz

W (1)
x

W (2)
x

W (1)
z

W (2)
z

z(3)

(a) ISTA - Recurrent Neural Net-
work

(b) LISTA - Unfolded network with T = 3

Figure 3: Network architecture for ISTA (left) and LISTA (right).

Due to the expression of the gradient, Chen et al. (2018) proposed to consider
only a subclass of the previous networks, where the weights Wx and Wz are

7

coupled via Wz = Idm
−
following. A layer of LISTA is a function φθ : Rm
R+
θ = (W, α, β)

x D . This is the architecture we consider in the
Rm parametrized by
Rn

Rn×m

R+

→

×

∗ such that

W (cid:62)

∈

×

∗ ×
φθ(z, x) = ST(z

Given a set of T layer parameters Θ(T ) =
ΦΘ(T ) : Rn

−
θ(t)
, the LISTA network
{
Rm is ΦΘ(T ) (x) = z(T )(x) where z(t)(x) is deﬁned by recursion

T −1
t=0

−

}

αW (cid:62)(Dz

x), βλ) .

(11)

→

z(0)(x) = 0, and

z(t+1)(x) = φθ(t)(z(t)(x), x)

for t

0, T

∈ (cid:74)

.

1
(cid:75)

−

(12)

L yields the same outputs as T iterations of

Taking W = D , α = β = 1
ISTA.

To alleviate the need to learn the large matrices W (t), Liu et al. (2019) proposed
to use a shared analytic matrix WALISTA for all layers. The matrix is computed
in a preprocessing stage by

WALISTA = arg min

W (cid:62)D

2
F
(cid:107)

W (cid:107)

s.t.

diag(W (cid:62)D) = 111m .

(13)

Then, only the parameters (α(t), β(t)) are learned. This eﬀectively reduces the
number of parameters from (nm + 2)
T . However, we will see that
ALISTA fails in our setup.

T to 2

×

×

Step-LISTA With regards to the study on step sizes for ISTA in Section 3, we
propose to learn approximation of ISTA step sizes for the input distribution using
the LISTA framework. The resulting network, dubbed Step-LISTA (SLISTA),
has T parameters ΘSLISTA =

T −1
t=0 , and follows the iterations:
}

α(t)

{

z(t+1)(x) = ST(z(t)(x)

α(t)D(cid:62)(Dz(t)(x)

x), α(t)λ) .

(14)

−

−

This is equivalent to a coupling in the LISTA parameters: a LISTA layer
θ = (W, α, β) corresponds to a SLISTA layer if and only if α
β W = D. This
network aims at learning good step sizes, like the ones used in OISTA, without
the computational burden of computing Lipschitz constants. The number of
parameters compared to the classical LISTA architecture ΘLISTA is greatly
diminished, making the network easier to train. Learning curves are shown
in Figure ?? in appendix. Figure 4 displays the learned steps of a SLISTA
network on a toy example. The network learns larger step-sizes as the 1/LS’s
increase.

Training the network We consider the framework where the network learns
∞ in an unsupervised way. Given a distribution p on
to solve the Lasso on

∞ , the network is trained by solving

B

B

˜Θ(T )

arg min

Θ(T ) L

∈

(Θ(T )) (cid:44) Ex∼p[Fx(ΦΘ(T )(x))] .

(15)

8

Figure 4: Steps learned with a 20 layers
SLISTA network on a 10
20 problem. For
each layer t and each training sample x, we
compute the support S(x, t) of z(t)(x). The
brown curves display the quantiles of the dis-
tribution of 1/LS(x,t) for each layer t . Full
experimental setup described in Appendix D.

×

Most of the literature on learned optimization train the network with a dif-
ferent supervised objective (Gregor and Le Cun, 2010; Xin et al., 2016; Chen
et al., 2018; Liu et al., 2019). Given a set of pairs (xi, zi) , the supervised
zi
approach tries to learn the parameters of the network such that ΦΘ(xi)
2 . This training procedure diﬀers critically
e.g. by minimizing
(cid:107)
from ours. For instance, ISTA does not converge for the supervised problem in
general while it does for the unsupervised one. As Proposition 4.1 shows, the
unsupervised approach allows to learn to minimize the Lasso cost function Fx .

ΦΘ(xi)

zi

(cid:39)

−

(cid:107)

Proposition 4.1 (Pointwise convergence). Let ˜Θ(T ) found by solving Prob-
lem (15).
For x

∞ such that p(x) > 0 , Fx(Φ ˜Θ(T ) (x))

x almost everywhere.

F ∗

−−−−−→T →+∞

∈ B

ISTA the parameters corresponding to ISTA i.e. θ(t)

Proof. Let Θ(T )
For all T , we have Ex∼p[F ∗
(x))] .
x ]
Since ISTA converges uniformly on any compact, the right hand term goes to
Ex∼p[F ∗
F ∗
0 .
x ]
This implies almost sure convergence of Fx(Φ ˜Θ(T ) (x))
x to 0 since it is
non-negative.

x ] . Therefore, by the squeeze theorem, Ex∼p[Fx(Φ ˜Θ(T ) (x))

Ex∼p[Fx(Φ ˜Θ(T )(x))]

Ex∼p[Fx(ΦΘ(T )

F ∗

→

ISTA = (D, 1/L, 1/L) .

≤

≤

−

−

ISTA

Asymptotical weight coupling theorem In this paragraph, we show the
main result of this paper: any LISTA network minimizing Fx on
∞ reduces
to SLISTA in its deep layers (Theorem 4.4). It relies on the following Lem-
mas.
Lemma 4.2 (Stability of solutions around Dj). Let D
with non-duplicated unit-normed columns. Let c (cid:44) maxl(cid:54)=j
for all j
(1

Rn×m be a dictionary
D(cid:62)
< 1 . Then
|
j ε = 0 , the vector

ε
(cid:107)
λ)ej minimizes Fx for x = Dj + ε .

Rm such that

c) and D(cid:62)

1, m
(cid:75)

< λ(1

and ε

l Dj

∈ (cid:74)

−

∈

∈

B

(cid:107)

|

−

It can be proven by verifying the KKT conditions (3) for (1
Subsection C.1.
Rn×m be a dictionary with non-
Lemma 4.3 (Weight coupling). Let D
duplicated unit-normed columns. Let θ = (W, α, β) a set of parameters. Assume
Rm
that all the couples (z∗(x), x)
arg min Fx(z) verify
∈
× B
φθ(z∗(x), x) = z∗(x). Then, α
β W = D .

∞ such that z∗(x)

λ)ej , detailed in

−

∈

∈

9

, consider x =
1, m
Sketch of proof (full proof in Subsection C.2). For j
(cid:75)
Dj +ε , with ε(cid:62)Dj = 0 . For
∞ and ε veriﬁes the hypothe-
sis of Lemma 4.2, therefore z∗ = (1
arg min Fx . Writing φθ(z∗, x) = z∗
−
for the j-th coordinate yields αW (cid:62)
j (λDj + ε) = λβ . We can then verify that
(αW (cid:62)
j )(λDj +ε) = 0 . This stands for any ε orthogonal to Dj and of norm
small enough. Simple linear algebra shows that this implies αWj

small enough, x
λ)ej

βDj = 0 .

βD(cid:62)

j −

∈ (cid:74)

∈ B

∈

(cid:107)

(cid:107)

ε

−

Lemma 4.3 states that the Lasso solutions are ﬁxed points of a LISTA layer only
if this layer corresponds to a step size for ISTA. The following theorem extends
the lemma by continuity, and shows that the deep layers of any converging
LISTA network must tend toward a SLISTA layer.
Rn×m be a dictionary with non-duplicated unit-normed
Theorem 4.4. Let D
T
columns. Let Θ(T ) =
t=0 be the parameters of a sequence of LISTA networks
}
such that the transfer function of the layer t is z(t+1) = φθ(t)(z(t), x) . Assume
that

∈
θ(t)

{

(i) the sequence of parameters converges i.e. θ(t)

θ∗ = (W ∗, α∗, β∗) ,

−−−→t→∞

(ii) the output of the network converges toward a solution z∗(x) of the Lasso (1)

uniformly over the equiregularization set
z∗(x)

0 .

B

∞ , i.e.

supx∈B∞ (cid:107)

ΦΘ(T ) (x)

−

(cid:107) −−−−→T →∞
β∗ W ∗ = D .

Then α∗

Sketch of proof (full proof in Subsection C.3). Let ε > 0 , and x
the triangular inequality, we have

∈ B

∞ . Using

φθ∗ (z∗, x)

z∗

(cid:107)

−

(cid:107) ≤ (cid:107)

φθ∗ (z∗, x)

φθ(t)(z(t), x)
(cid:107)

−

+

φθ(t)(z(t), x)
(cid:107)

−

z∗

(16)
(cid:107)

Since the z(t) and θ(t) converge, they are valued over a compact set K. The
φθ(z, x) is continuous, piecewise-linear. It is therefore
function f : (z, x, θ)
(cid:55)→
φθ∗ (z∗, x)
ε for t large
Lipschitz on K. Hence, we have
(cid:107)
enough. Since φθ(t)(z(t), x) = z(t+1) and z(t)
ε for
t large enough. Finally, φθ∗ (z∗, x) = z∗ . Lemma 4.3 allows to conclude.

(cid:107) ≤
φθ(t) (z(t), x)

φθ(t) (z(t), x)

−
z∗ ,

(cid:107) ≤

z∗

→

−

(cid:107)

Figure 5: Illustration of Theorem 4.4: for
deep layers of LISTA, we have
−
β(t)D
0 , indicating that the network
ultimately only learns a step size. Full ex-
perimental setup described in Appendix D.

α(t)W (t)

F
(cid:107)

→

(cid:107)

Theorem 4.4 means that the deep layers of any LISTA network that converges to
solutions of the Lasso correspond to SLISTA iterations: W (t) aligns with D , and
α(t), β(t) get coupled. This is illustrated in Figure 5, where a 40-layers LISTA

10

×

20 problem with λ = 0.1 . As predicted by the
network is trained on a 10
theorem, α(t)
β(t) W (t)
D . The last layers only learn a step size. This is consistent
with the observation of Moreau and Bruna (2017) which shows that the deep
layers of LISTA stay close to ISTA. Further, Theorem 4.4 also shows that it is
hopeless to optimize the unsupervised objective (15) with WALISTA (13), since
this matrix is not aligned with D .

→

5 Numerical Experiments

This section provides numerical arguments to compare SLISTA to LISTA and
ISTA. All the experiments were run using Python (Python Software Founda-
tion, 2017) and pytorch (Paszke et al., 2017). The code to reproduce the ﬁgures
is available online1.

Network comparisons We compare the proposed approach SLISTA to state-
of-the-art learned methods LISTA (Chen et al., 2018) and ALISTA (Liu et al., 2019)
on synthetic and semi-real cases.

Rn×m of Gaussian i.i.d. entries is
In the synthetic case, a dictionary D
generated. Each column is then normalized to one. A set of Gaussian i.i.d.
Rn is drawn. The input samples are obtained as xi =
samples (˜xi)N
˜xi/
∞ , so that for all i , xi
D(cid:62) ˜xi
∞ . We set m = 256 and
(cid:107)
n = 64.

i=1 ∈
∈ B

∈ B

∈

∞

(cid:107)

For the semi-real case, we used the digits dataset from scikit-learn (Pedregosa
et al., 2011) which consists of 8
8 images of handwritten digits from 0 to 9 .
We sample m = 256 samples at random from this dataset and normalize it do
generate our dictionary D . Compared to the simulated Gaussian dictionary,
this dictionary has a much richer correlation structure, which is known to imper
the performances of learned algorithms (Moreau and Bruna, 2017). The input
distribution is generated as in the simulated case.

×

(15) on a training set
The networks are trained by minimizing the empirical loss
of size Ntrain = 10, 000 and we report the loss on a test set of size Ntest = 10, 000 .
Further details on training are in Appendix D.

L

Figure 6 shows the test curves for diﬀerent levels of regularization λ = 0.1 and
0.8. SLISTA performs best for high λ, even for challenging semi-real dictionary
D . In a low regularization setting, LISTA performs best as SLISTA cannot learn
larger steps due to the low sparsity of the solution. In this unsupervised setting,
ALISTA does not converge in accordance with Theorem 4.4.

1 The code can be found in supplementary materials.

11

Figure 6: Test loss of ISTA, ALISTA, LISTA and SLISTA on simulated and
semi-real data for diﬀerent regularization parameters.

6 Conclusion

We showed that using larger step sizes is an eﬃcient strategy to accelerate ISTA
for sparse solution of the Lasso. In order to make this approach practical, we
proposed SLISTA, a neural network architecture which learns such step sizes.
Theorem 4.4 shows that the deepest layers of any converging LISTA architecture
must converge to a SLISTA layer. Numerical experiments show that SLISTA
outperforms LISTA in a high sparsity setting. An major beneﬁt of our approach
is that it preserves the dictionary. We plan on leveraging this property to apply
SLISTA in convolutional or wavelet cases, where the structure of the dictionary
allows for fast multiplications.

References

Jonas Adler, Axel Ringh, Ozan Öktem, and Johan Karlsson. Learning to solve
inverse problems using Wasserstein loss. preprint ArXiv, 1710.10898, 2017.

Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm
for linear inverse problems. SIAM journal on imaging sciences, 2(1):183–202,
2009.

Mark Borgerding, Philip Schniter, and Sundeep Rangan. AMP-inspired deep
networks for sparse linear inverse problems. IEEE Transactions on Signal
Processing, 65(16):4293–4308, 2017.

Xiaohan Chen, Jialin Liu, Zhangyang Wang, and Wotao Yin. Theoretical linear
convergence of unfolded ISTA and its practical weights and thresholds. In
Advances in Neural Information Processing Systems (NIPS), pages 9061–9071,
2018.

Patrick L Combettes and Heinz H. Bauschke. Convex Analysis and Monotone
Operator Theory in Hilbert Spaces. Springer, 2011. ISBN 9788578110796. doi:
10.1017/CBO9781107415324.004.

12

Ingrid Daubechies, Michel Defrise, and Christine De Mol. An iterative thresh-
olding algorithm for linear inverse problems with a sparsity constraint. Com-
munications on Pure and Applied Mathematics, 57(11):1413–1457, 2004.

Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. Least

angle regression. Ann. Statist., 32(2):407–499, 2004.

Laurent El Ghaoui, Vivian Viallon, and Tarek Rabbani. Safe feature elimination

in sparse supervised learning. J. Paciﬁc Optim., 8(4):667–698, 2012.

Jerome Friedman, Trevor Hastie, Holger Höﬂing, and Robert Tibshirani. Path-
wise coordinate optimization. The Annals of Applied Statistics, 1(2):302–332,
2007.

Raja Giryes, Yonina C. Eldar, Alex M. Bronstein, and Guillermo Sapiro. Trade-
oﬀs between convergence speed and reconstruction accuracy in inverse problems.
IEEE Transaction on Signal Processing, 66(7):1676–1690, 2018.

Karol Gregor and Yann Le Cun. Learning Fast Approximations of Sparse Coding.
In International Conference on Machine Learning (ICML), pages 399–406,
2010.

Elaine Hale, Wotao Yin, and Yin Zhang. Fixed-point continuation for (cid:96)1-
minimization: Methodology and convergence. SIAM J. Optim., 19(3):1107–
1130, 2008.

Trevor Hastie, Robert Tibshirani, and Martin Wainwright. Statistical Learning

with Sparsity: The Lasso and Generalizations. CRC Press, 2015.

John R. Hershey, Jonathan Le Roux, and Felix Weninger. Deep unfolding: Model-
based inspiration of novel deep architectures. preprint ArXiv, 1409.2574, 2014.

Daisuke Ito, Satoshi Takabe, and Tadashi Wadayama. Trainable ISTA for
sparse signal recovery. In IEEE International Conference on Communications
Workshops, pages 1–6, 2018.

Tyler Johnson and Carlos Guestrin. Blitz: A principled meta-algorithm for
scaling sparse optimization. In International Conference on Machine Learning
(ICML), pages 1171–1179, 2015.

Jingwei Liang, Jalal Fadili, and Gabriel Peyré. Local linear convergence of
forward–backward under partial smoothness. In Advances in Neural Informa-
tion Processing Systems, pages 1970–1978, 2014.

Jialin Liu, Xiaohan Chen, Zhangyang Wang, and Wotao Yin. ALISTA: Analytic
weights are as good as learned weigths in LISTA. In International Conference
on Learning Representation (ICLR), 2019.

Vladimir A Marchenko and Leonid Andreevich Pastur. Distribution of eigenvalues
for some sets of random matrices. Mathematics of the USSR-Sbornik, 1(4):
457, 1967.

13

Mathurin Massias, Alexandre Gramfort, and Joseph Salmon. Celer: a Fast
Solver for the Lasso with Dual Extrapolation. In International Conference on
Machine Learning (ICML), 2018.

Thomas Moreau and Joan Bruna. Understanding neural sparse coding with
matrix factorization. In International Conference on Learning Representation
(ICLR), 2017.

Eugene Ndiaye, Olivier Fercoq, Alexandre Gramfort, and Joseph Salmon. Gap
safe screening rules for sparsity enforcing penalties. J. Mach. Learn. Res., 18
(128):1–33, 2017.

Yurii Nesterov. A method for solving a convex programming problem with rate

of convergence O(1/k2). Soviet Math. Doklady, 269(3):543–547, 1983.

Bruno A. Olshausen and David J Field. Sparse coding with an incomplete basis

set: a strategy employed by V1, 1997.

Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang,
Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
Automatic diﬀerentiation in PyTorch. In NIPS Autodiﬀ Workshop, 2017.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel,
M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn:
Machine learning in Python. Journal of Machine Learning Research, 12:
2825–2830, 2011.

Clarice Poon, Jingwei Liang, and Carola-Bibiane Schönlieb. Local convergence
properties of SAGA and prox-SVRG and acceleration. In International Con-
ference on Machine Learning (ICML), 2018.

Python Software Foundation.
http://python.org/, 2017.

Python Language Reference, version 3.6.

Saharon Rosset, Ji Zhu, and Trevor Hastie. Boosting as a regularized path to a

maximum margin classiﬁer. J. Mach. Learn. Res., 5:941–973, 2004.

Pablo Sprechmann, Alex M. Bronstein, and Guillermo Sapiro. Learning eﬃcient
structured sparse models. In International Conference on Machine Learning
(ICML), pages 615–622, 2012.

Pablo Sprechmann, Roee Litman, and TB Yakar. Eﬃcient supervised sparse
analysis and synthesis operators. In Advances in Neural Information Processing
Systems (NIPS), pages 908–916, 2013.

Yifan Sun, Halyun Jeong, Julie Nutini, and Mark Schmidt. Are we there yet?
manifold identiﬁcation of gradient-related proximal methods. In Proceedings
of Machine Learning Research, volume 89 of Proceedings of Machine Learning
Research, pages 1110–1119. PMLR, 2019.

14

Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of
the Royal Statistical Society: Series B (Methodological), 58(1):267–288, 1996.

Ryan Tibshirani. The lasso problem and uniqueness. Electron. J. Stat., 7:

1456–1490, 2013.

Paul Tseng. Convergence of a block coordinate descent method for nondiﬀeren-

tiable minimization. J. Optim. Theory Appl., 109(3):475–494, 2001.

Zhangyang Wang, Qing Ling, and Thomas S. Huang. Learning deep (cid:96)0 encoders.

In AAAI Conference on Artiﬁcial Intelligence, pages 2194–2200, 2015.

Bo Xin, Yizhou Wang, Wen Gao, and David Wipf. Maximal sparsity with deep
networks? In Advances in Neural Information Processing Systems (NIPS),
pages 4340–4348, 2016.

Yan Yang, Jian Sun, Huibin Li, and Zongben Xu. Deep ADMM-Net for com-
pressive censing MRI. In Advances in Neural Information Processing Systems
(NIPS), pages 10–18, 2017.

Willard I Zangwill. Convergence conditions for nonlinear programming algo-

rithms. Management Science, 16(1):1–13, 1969.

Jian Zhang and Bernard Ghanem. ISTA-Net: Interpretable optimization-inspired
deep network for image compressive sensing.
In IEEE Computer Society
Conference on Computer Vision and Pattern Recognition, pages 1828–1837,
2018.

15

A Unfolded optimization algorithms literature sum-

mary

In Table A.1, we summarize the proliﬁc literature on learned unfolded opti-
mization procedures for sparse recovery. A particular focus is set on the chosen
training loss training which is either supervised, with a regression of zi from the
input xi for a given training set (xi, zi), or unsupervised, where the objective is
to minimize the Lasso cost function Fx for each training point x.

Table A.1: Neural network for sparse coding

Reference

Base Algo

Train Loss

Remarks

Gregor and Le Cun (2010)
Sprechmann et al. (2012)
Sprechmann et al. (2013)
Hershey et al. (2014)
Wang et al. (2015)
Xin et al. (2016)
Giryes et al. (2018)
Yang et al. (2017)

ISTA / CD
Block CD
ADMM
NMF
IHT
IHT
PGD/IHT
ADMM

supervised
unsupervised
supervised
supervised
supervised
supervised
supervised
supervised

Adler et al. (2017)

ADMM

supervised

Borgerding et al. (2017)
Moreau and Bruna (2017)

Chen et al. (2018)

AMP
ISTA

ISTA

supervised
unsupervised

supervised

Ito et al. (2018)

ISTA

supervised

Zhang and Ghanem (2018)

PGD

supervised

Liu et al. (2019)

Proposed

ISTA

ISTA

supervised

unsupervised

Coupled
weights

×
×
N/A

×
×
/(cid:88)
×
N/A
N/A

N/A

×
×
(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

–
Group (cid:96)1
–
NMF
Hard-thresholding
Hard-thresholding
Group (cid:96)1
–
Wasserstein
distance with z∗
–
–
Linear
convergence rate
MMSE shrinkage
non-linearity
Sparsity of
Wavelet
coeﬃcients
Analytic weight
WALISTA
–

B Proofs of Section 3’s results

B.1 Proof of Proposition 3.1

We consider that the solution of the Lasso is unique, following the result of
Tibshirani (2013)[Lemmas 4 and 16] when the entries of D and x come from a

16

continuous distribution.
Proposition 3.1 (Convergence, ﬁnite-time support identiﬁcation and safe
regime). When Assumption 2.2 holds, the sequence (z(t)) generated by the algo-
rithm converges to z∗ = arg min Fx .
Further, there exists an iteration T ∗ such that for t
supp(z∗) (cid:44) S∗ and Condition (cid:63) is always statisﬁed.

T ∗ , supp(z(t)) =

≥

Proof. Let z(t) be the sequence of iterates produced by Algorithm 1. We have a
descent function

Fx(z(t+1))

Fx(z(t))

−

γ
2 (cid:107)

≤ −

z(t+1)

z(t)

−

2
(cid:107)

≤ −

min

Dj
(cid:107)
2

(cid:107)

(cid:107)

z(t+1)

z(t)

−

2 , (17)
(cid:107)

where γ = LS if Condition (cid:63) is met, and L otherwise. Additionally, the iterates
are bounded because Fx(z(t)) decreases at each iteration and Fx is coercive.
Hence we can apply Zangwill’s Global Convergence Theorem (Zangwill, 1969).
Any z∗ accumulation point of (z(t))t∈N is a minimizer of Fx .

Since we only consider the case where the minimizer is unique, the bounded
sequence (z(t))t∈N has a unique accumulation point, thus converges to z∗ .

The support identiﬁcation is a simpliﬁcation of a result of Hale et al. (2008), we
include it here for completeness.

Lemma B.1 (Approximation of the soft-thresholding). Let z
(cid:15) small enough, we have

∈

R, ν > 0 . For

ST(z + (cid:15), ν) =

max(0, (cid:15)) sign(z) ,

(18)

0 ,






z + (cid:15)

ν sign z ,

−

< ν ,

= ν ,

> ν .

if

if

if

z
|
z
|
z
|

|

|

|

Let ρ > 0 be such that Equation (18) holds for ν = λ/L , every (cid:15) < ρ , and every
z = z∗

x) .

1

L D(cid:62)

j (Dz∗

−

j −

Let t
also have

∈

1

L D(cid:62)D)(cid:15) , we

−

N such that z(t) = z∗ + (cid:15) , with

ρ . With (cid:15)(cid:48) (cid:44) (Id

(cid:15)
(cid:107)

(cid:107) ≤

|

.

1

1

1

=

x)

L D(cid:62)

j (Dz∗

1, m
(cid:75)
x)
|

< λ/L hence ST(z∗

(cid:107) ≤
z∗
j −
|
x) + (cid:15)(cid:48)
−
z∗
j −
|
x) + (cid:15)(cid:48)
−

ρ . Let j
∈ (cid:74)
L D(cid:62)
j (Dz∗
−
j, λ/L) = 0 .
j + λ
z∗
j (Dz∗
=
|
−
j, λ/L) = sign z∗
j .

(cid:15)(cid:48)
(cid:107)
If j /
E ,
∈
1
j (Dz∗
L D(cid:62)
E ,
If j
∈
1
L D(cid:62)
j (Dz∗
The same reasoning can be applied with ρ(cid:48) such that Equation (18) holds for
ν = λ/LS∗ , every (cid:15) < ρ(cid:48) , and every z = z∗
j (Dz∗
x). If we introduce
1
ρ(cid:48) , in the ball of center z∗
η > 0 such that
LS∗ D
−
and radius η , the iteration with step size LS∗ identiﬁes the support.

> λ/L , and sign ST(z∗

L sign z∗
j |

1
L∗
S
D)(cid:15)

j −
(cid:62)

L D(cid:62)

⇒ (cid:107)

j −

j −

η =

D(cid:62)

(cid:107) ≤

(cid:107) ≤

(Id

x)

−

−

(cid:107)

(cid:15)

|

|

17

S∗ DS∗ is non expansive on vectors which support
Additionnally, since Id
is S∗ , the iterations with the step LS∗ never leave this ball once they have
entered it.

−

LS∗ D(cid:62)

1

Therefore, once the iterates enter

(z∗, min(η, ρ)) , Condition (cid:63) is always satisﬁed.

B

B.2 Proof of Proposition 3.2

.

(cid:107)z∗−z(T ∗ )(cid:107)2
2(t−T ∗)

Proposition 3.2 (Rates of convergence). For t > T ∗ , Fx(z(t))
LS∗
If additionally inf (cid:107)z(cid:107)=1 (cid:107)
is
Fx(z∗)
Fx(z(t))
(1

(Fx(z(T ∗))

Fx(z∗)) .

(cid:107)
µ∗
LS∗ )t−T ∗

DS∗ z

2 = µ∗ > 0 , then the convergence rate for t

−

−

≤

−

−

Fx(z∗)

≤

T ∗

≥

≥

T ∗ , the iterates support is S∗ and the objective function is
Proof. For t
LS∗ -smooth instead of L-smooth. It is also µ∗ strongly convex if µ∗ > 0 . The
obtained rates are a classical result of the proximal gradient descent method in
these cases.

C Proof of Section 4’s Lemmas

C.1 Proof of Lemma 4.2

Lemma 4.2 (Stability of solutions around Dj). Let D
with non-duplicated unit-normed columns. Let c (cid:44) maxl(cid:54)=j
for all j
(1

ε
(cid:107)
λ)ej minimizes Fx for x = Dj + ε .

Rm such that

1, m
(cid:75)

< λ(1

and ε

∈ (cid:74)

−

∈

∈

(cid:107)

c) and D(cid:62)

Rn×m be a dictionary
D(cid:62)
< 1 . Then
|
j ε = 0 , the vector

l Dj

|

−

Proof. Let j
For notation simplicity, we denote z∗ = z∗(Dj

j be a vector such that
ε) .

1, m
(cid:75)

and let ε

∈ (cid:74)

∈

∩

ε
(cid:107)

(cid:107)

Rm

D⊥

< λ(1

c) .

−

D(cid:62)

j (Dz∗

Dj

ε) = D(cid:62)
j (

λDj

−
λ > 0 . For the other coeﬃcients l

−

−

−

−
1, m

λ =

λ sign z∗

j ,

−

j

, we have

−
ε) =

since 1

−

l (Dz∗

D(cid:62)
|

−

Dj

ε)

|

−

(19)

(20)

(21)

(22)

(23)

(24)

(25)

,

(cid:75) \ {

∈ (cid:74)
}
D(cid:62)
l (
λDj
ε)
|
|
−
−
l Dj + D(cid:62)
λD(cid:62)
l ε)
|
|
D(cid:62)
D(cid:62)
l ε
+
l Dj
λ
|
|
|
Dl
,
λc +
ε
(cid:107)

|
ε
(cid:107)(cid:107)
(cid:107)
< λ ,

λc +

(cid:107)

(cid:107)

=

=

≤

≤

≤

,

,

18

Therefore, (1
λ)ej .

−

λ)ej veriﬁes the KKT conditions (3) and z∗(Dj + ε) = (1

−

C.2 Proof of Lemma 4.3

Rn×m be a dictionary with non-
Lemma 4.3 (Weight coupling). Let D
duplicated unit-normed columns. Let θ = (W, α, β) a set of parameters. Assume
Rm
that all the couples (z∗(x), x)
arg min Fx(z) verify
× B
∈
φθ(z∗(x), x) = z∗(x). Then, α
β W = D .

∞ such that z∗(x)

∈

∈

∈ B

Proof. Let x
Lasso at level λ > 0 . Let j
(3) gives

∞ be an input vector and z∗(x)
1, m
∈ (cid:74)
(cid:75)
j (Dz∗(x)
D(cid:62)

x) =

−

λ .

−

Suppose that z∗(x) is a ﬁxed point of the layer, then we have

Rm be a solution for the
∈
be such that z∗
j > 0 . The KKT conditions

ST(z∗

j (x)

αW (cid:62)

j (Dz∗(x)

x), λβ) = z∗

j (x) > 0 .

−

−

By deﬁnition, ST(a, b) > 0 implies that a > b and ST(a, b) = a

b . Thus,

−

j (Dz∗(x)

x)

λβ = z∗

j (x)

−

z∗
j (x)
αW (cid:62)
αW (cid:62)

αW (cid:62)
j (Dz∗(x)
j (Dz∗(x)

−

−
−
x) + λβ = 0

x)

βD(cid:62)

(αWj

−
βDj)(cid:62)(Dz∗(x)

−

j (Dz∗(x)
x) = 0 .

−

⇔

⇔

x) = 0

by (26)

−

⇔

(0, λ(1

−
As the relation (31) must hold for all x
D⊥
ε
. Indeed, in this case,
j
(cid:107)
the conditions of Lemma 4.2, and thus z∗ = (1
−
(Dj + ε)) = 0

D(cid:62)(Dj + ε)
λ)ej , i.e.

(αWj

∈ B

∈ B

c))

−

∩

∞ , it is true for all Dj + ε for all
∞ = 1 . D veriﬁes
(cid:107)

−

βDj)(cid:62)(D(1
(αWj

λ)ej
−
−
βDj)(cid:62) (

λDj

ε) = 0

−
βDj)(cid:62)Dj = 0 , and therefore Eq. (33) becomes
βDj)(cid:62)ε = 0 for all ε small enough and orthogonal to Dj , which implies
βDj = 0 and concludes our proof.

Taking ε = 0 yields (αWj
(αWj
αWj

−

−

−

−
−

(26)

(27)

(28)

(29)

(30)

(31)

(32)

(33)

C.3 Proof of Theorem 4.4

Rn×m be a dictionary with non-duplicated unit-normed
Theorem 4.4. Let D
T
columns. Let Θ(T ) =
t=0 be the parameters of a sequence of LISTA networks
}
such that the transfer function of the layer t is z(t+1) = φθ(t)(z(t), x) . Assume
that

∈
θ(t)

{

(i) the sequence of parameters converges i.e. θ(t)

θ∗ = (W ∗, α∗, β∗) ,

−−−→t→∞

19

(ii) the output of the network converges toward a solution z∗(x) of the Lasso (1)

uniformly over the equiregularization set
z∗(x)

0 .

B

∞ , i.e.

supx∈B∞ (cid:107)

ΦΘ(T ) (x)

−

(cid:107) −−−−→T →∞
β∗ W ∗ = D .

Then α∗

Proof. For simplicity of the notation, we will drop the x variable whenever
possible, i.e. z∗ = z∗(x) and φθ(z) = φθ(z, x) . We denote z(t) = ΦΘ(t) (x) the
output of the network with t layers.
Let (cid:15) > 0 . By hypothesis (i), there exists T0 such that for all t

T0 ,

(cid:107) ≤
By hypothesis (ii), , there exists T1 such that for all t

| ≤

−

−

W ∗

(cid:15)

α(t)
|

α∗

(cid:15)

β(t)
|

−

β∗

| ≤

W (t)
(cid:107)

T1 and all x

∞ ,

∈ B

≥

≥
(cid:15) .

z(t)
(cid:107)

−

z∗

(cid:15) .

(cid:107) ≤
max(T0, T1) . Using (35), we have

≥
z(t+1)

Let x

∞ be an input vector and t

∈ B

z(t+1)
(cid:107)

z(t)

∈

−

N and θ∗

(cid:107) ≤ (cid:107)
By (i), there exist a compact set
all t
z∗ = arg minz Fx(z) , we have λ
(cid:107)
a compact set
We consider the function f (z, x, θ) = ST(z
set
set. It is thus L-Lipschitz and thus

∈ K

×K

2
K

×B

1
(cid:107)

2 .

≤

K

∞

z

z∗

+

z(t)

z∗

2(cid:15)

(36)

−
(cid:107)
Rn×m
. The input x is taken in a compact set

(cid:107) ≤
∗ s.t. θ(t)

(cid:107)
R+

1
K

R+

−

×

⊂
Fx(z∗)

∗ ×
Fx(0) =

1 for
∈ K
∞ and as
thus z∗ is also in

B

≤

x

(cid:107)

(cid:107)

x), β) on the compact
1 . This function is continuous and piece-wise linear on a compact

αW (cid:62)(Dz

−

−

(cid:107)

φθ(t) (z(t))
φθ∗ (z∗)
(cid:107)

−

−

φθ(t)(z∗)
φθ(t)(z∗)

(cid:107) ≤

(cid:107) ≤

z(t)
L
(cid:107)
θ(t)
L
(cid:107)

−

−

z∗
θ∗

(cid:107) ≤

(cid:107) ≤

L(cid:15)

L(cid:15)

Using these inequalities, we get

φθ∗ (z∗, x)
(cid:107)

−

z∗

(cid:107) ≤ (cid:107)
(cid:124)

φθ∗ (z∗)

φθ(t) (z∗)
(cid:107)
(cid:125)

−
(cid:123)(cid:122)
<L(cid:15) by (38)

+

φθ(t)(z∗)
(cid:107)
(cid:124)

φθ(t) (z(t))
(cid:107)
(cid:125)

(39)

−
(cid:123)(cid:122)
<L(cid:15) by (37)
z∗

+

φθ(t)(z(t))
(cid:107)
(cid:123)(cid:122)
(cid:124)
<2(cid:15) by (36)

−

z(t)

+

(cid:107)
(cid:125)

z(t)
(cid:107)
(cid:107)
−
(cid:125)
(cid:123)(cid:122)
(cid:124)
<(cid:15) by (35)

(2L + 3)(cid:15) .

≤

As this result holds for all (cid:15) > 0 and all x
x

∞ . We can apply the Lemma 4.3 to conclude this proof.

∈ B

∞ , we have φθ∗ (z∗) = z∗ for all

∈ B

(34)

(35)

(37)

(38)

(40)

20

D Experimental setups and supplementary ﬁg-

ures

Dictionary generation: Unless speciﬁed otherwise, to generate synthetic
dictionaries, we ﬁrst draw a random i.i.d. Gaussian matrix ˆD
Rn×m. The
dictionary is obtained by normalizing the columns: Dij = 1

∈
ˆDij.

(cid:107) ˆDi:(cid:107)

Samples generation: The samples x are generated as follows: Random i.i.d.
Rn are generated. We then normalize them: x =
Gaussian samples ˆx
ˆx, so that x

1
(cid:107)D(cid:62) ˆx(cid:107)∞

∈
∈ B

∞.

Training the networks Since the loss function and the network are continuous
but non-diﬀerentiable, we use sub-gradient descent for training. The sub-gradient
of the cost function with respect to the parameters of the network is computed
by automatic diﬀerentiation. We use full-batch sub-gradient descent with a
backtracking procedure to ﬁnd a suitable learning rate. To verify that we do
not overﬁt the training set, we always check that the test loss and train loss are
comparable.

Main text ﬁgures setup

•

•

•

Figure 2: We generate a random dictionary of size 10
λ = 0.5, and a random sample x
for 10000 iterations.

50. We take
×
x is computed by iterating ISTA

∞. F ∗

∈ B

20. We take
Figure 4: We generate a random dictionary of size 10
λ = 0.2. We generate a training set of N = 1000 samples (xi)1000
∞.
A 20 layers SLISTA network is trained by gradient descent on these data.
We report the learned step sizes. For each layer t of the network and
each training sample x, we compute the support at the output of the t-th
layer, S(x, t) = supp(z(t)(x)). For each t, we display the quantiles of the
distribution of the (1/LS(xi,t))1000
i=1 .

i=1 ∈ B

×

Figure 5: A random 10
20 dictionary is generated. We take 1000 training
samples, and λ = 0.05. A 40 layers LISTA network is trained by gradient
descent on those samples. We report the quantity
F for
(cid:107)
each layer t.

α(t)W (t)
(cid:107)

β(t)D

×

−

21

Supplementary experiments

.

Figure D.1:
Illustration of Proposition 3.4.
A toy Gaussian dictionary is generated with
n = 200 , m = 600 so that γ = 3 . We compute
its Lipschitz constant L . For ζ between 0 and
1 , we extract
columns at random and
compute the corresponding Lipschitz constant
LS . The plot shows an almost perfect ﬁt be-
tween the empirical law and the theoretical
limit (10).

ζm
(cid:99)

(cid:98)

Figure D.2: Comparison between ISTA, FISTA and Oracle-ISTA for diﬀerent
levels of regularization on a Gaussian dictionnary, with n = 100 and m = 200.
We report the average number of iterations taken to reach a point z such that
x + 10−13. The experiment is repeated 10 times, starting from random
Fx(z) < F ∗
points in
∞. OISTA is always faster than ISTA, and is faster than FISTA for
B
high regularization.

Figure D.3: Learning curves of SLISTA and
LISTA. Random Gaussian dictionaries with
n = 10 and m = 20 are generated. We take
λ = 0.3. Networks with 10 layers are ﬁt on
those dictionaries, and their test loss is reported
for diﬀerent number of training samples. The
process is repeated 100 times; the curves shown
display the median of the test-loss.

22

