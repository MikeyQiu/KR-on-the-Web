7
1
0
2
 
n
u
J
 
2
2
 
 
]

V
C
.
s
c
[
 
 
1
v
2
2
5
7
0
.
6
0
7
1
:
v
i
X
r
a

Deep Hashing Network for Unsupervised Domain Adaptation

Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, Sethuraman Panchanathan
Center for Cognitive Ubiquitous Computing, Arizona State University, Tempe, AZ, USA
{hemanthv, jeusebio, shayok.chakraborty, panch}@asu.edu

Abstract

In recent years, deep neural networks have emerged as a
dominant machine learning tool for a wide variety of appli-
cation domains. However, training a deep neural network
requires a large amount of labeled data, which is an expen-
sive process in terms of time, labor and human expertise.
Domain adaptation or transfer learning algorithms address
this challenge by leveraging labeled data in a different, but
related source domain, to develop a model for the target
domain. Further, the explosive growth of digital data has
posed a fundamental challenge concerning its storage and
retrieval. Due to its storage and retrieval efﬁciency, recent
years have witnessed a wide application of hashing in a
variety of computer vision applications. In this paper, we
ﬁrst introduce a new dataset, Ofﬁce-Home, to evaluate do-
main adaptation algorithms. The dataset contains images
of a variety of everyday objects from multiple domains. We
then propose a novel deep learning framework that can ex-
ploit labeled source data and unlabeled target data to learn
informative hash codes, to accurately classify unseen tar-
get data. To the best of our knowledge, this is the ﬁrst
research effort to exploit the feature learning capabilities
of deep neural networks to learn representative hash codes
to address the domain adaptation problem. Our extensive
empirical studies on multiple transfer tasks corroborate the
usefulness of the framework in learning efﬁcient hash codes
which outperform existing competitive baselines for unsu-
pervised domain adaptation.

1. Introduction

Deep learning algorithms automatically learn a discrim-
inating set of features and have depicted commendable per-
formance in a variety of computer vision applications. Un-
fortunately, training a deep model necessitates a large vol-
ume of labeled data, which can be time consuming and ex-
pensive to acquire. However, labeled data from a differ-
ent, but related domain is often available, which has mo-
tivated the development of algorithms which can leverage

labeled data in a source domain to develop a machine learn-
ing model for the target domain. Learning a discrimina-
tive model in the presence of the shift between training and
test distributions is known as transfer learning or domain
adaptation [17]. Unsupervised domain adaptation is a chal-
lenging setting, where labeled data is available only in the
source domain; no labeled data is available in the target
domain. Conventional shallow transfer learning methods
develop their models in two stages, feature extraction fol-
lowed by domain adaptation. The features are ﬁxed and
then a model is trained to align the source and target do-
mains [16, 20, 33, 38, 42, 43, 44]. On the other hand, deep
transfer learning procedures exploit the feature learning ca-
pabilities of deep networks to learn transferable feature rep-
resentations for domain adaptation and have demonstrated
impressive empirical performance [17, 18, 31, 34, 46].

The explosive growth of digital data in the modern era
has posed fundamental challenges regarding their storage,
retrieval and computational requirements. Against this
backdrop, hashing has emerged as one of the most popu-
lar and effective techniques due to its fast query speed and
low memory cost [48]. Hashing techniques transform high
dimensional data into compact binary codes and generate
similar binary codes for similar data items. Motivated by
this fact, we propose to train a deep neural network to out-
put binary hash codes (instead of probability values), which
can be used for classiﬁcation. We see two advantages to es-
timating a hash value instead of a standard probability vec-
tor in the ﬁnal layer of the network: (i) the hash values are
used to develop a unique loss function for target data in the
absence of labels and (ii) during prediction, the hash value
of a test sample can be compared against the hash values
of the training samples to arrive at a more robust category
prediction.

In this paper, we ﬁrst introduce a new dataset, Ofﬁce-
Home, which we use to evaluate our algorithm. The Ofﬁce-
Home dataset is an object recognition dataset which con-
tains images from 4 domains. It has around 15, 500 images
organized into 65 categories. We further propose a novel
deep learning framework called Domain Adaptive Hash-

1

ing (DAH) to learn informative hash codes to address the
problem of unsupervised domain adaptation. We propose
a unique loss function to train the deep network with the
following components: (i) supervised hash loss for labeled
source data, which ensures that source samples belonging
to the same class have similar hash codes; (ii) unsuper-
vised entropy loss for unlabeled target data, which imposes
each target sample to align closely with exactly one of the
source categories and be distinct from the other categories
and (iii) a loss based on multi-kernel Maximum Mean Dis-
crepancy (MK-MMD), which seeks to learn transferable
features within the layers of the network to minimize the
distribution difference between the source and target do-
mains. Figure 1 illustrates the different layers of the DAH
and the components of the loss function.

2. Related Work

There have been many approaches to address the prob-
lem of domain-shift in unsupervised domain adaptation.
One straightforward approach is,
to modify a classiﬁer
trained for the source data by adapting it to classify target
data [1, 4] or learn a transformation matrix to linearly trans-
form the source data, so that it is aligned with the target
[27, 42]. Some other procedures re-weight the data points
in the source domain, to select source data that is similar
to the target, when training a domain adaptive classiﬁer,
[9, 10, 19]. A standard procedure to reduce domain discrep-
ancy is, to project the source and target data to a common
subspace, thereby aligning their principal axes [16, 44].
Reducing domain disparity through nonlinear alignment of
data has been possible with Maximum Mean Discrepancy
(MMD) - a measure that provides the distribution differ-
ence between two datasets in a reproducing-kernel Hilbert
space [13]. Kernel-PCA based methods apply the MMD to
achieve nonlinear alignment of domains [32, 33, 38]. Man-
ifold based approaches are also popular in domain adapta-
tion for computer vision, where the subspace of a domain is
treated as a point on the manifold and transformations are
learned to align two domains [20, 23]. A survey of popular
domain adaptation techniques for computer vision is pro-
vided in [41] and a more generic survey of transfer learning
approaches can be found in [39].

All of the above techniques can be termed as shallow
learning procedures, since the models are learned using pre-
determined features. In recent years deep learning has be-
come very successful at learning highly discriminative fea-
tures for computer vision applications [8]. Deep learning
systems like deep CNNs learn representations of data that
capture underlying factors of variation between different
tasks in a multi-task transfer learning setting [3]. These rep-
resentations also disentangle the factors of variation allow-
ing for the transfer of knowledge between tasks [12, 18, 37].
Yosinski et al. [49] demonstrated how the lower layers of a

network produce generic features and the upper layers out-
put task speciﬁc features. Based on this, deep learning pro-
cedures for domain adaptation train networks to learn trans-
ferable features in the fully connected ﬁnal layers of a net-
work [31, 46]. In other approaches to deep domain adapta-
tion, Ganin et al. [17] trained domain adversarial networks
to learn features that make the source and target domain in-
distinguishable and Long et al. [34], trained a network to
do both feature adaptation and classiﬁer adaptation using
residual transfer networks.

Unsupervised hashing techniques have been developed
to extract unique hash codes for efﬁcient storage and re-
trieval of data [22, 25]. Neural network based hashing has
led the way in state-of-the-art unsupervised hashing tech-
niques [7, 11, 14]. The closest work incorporating hash-
ing and adaptation appears in cross-modal hashing, where
deep hashing techniques embed multi-modal data and learn
hash codes for two related domains, like text and images
[5, 6, 29]. However, these algorithms are not unsupervised
and they are mainly applied to extract common hash codes
for multi-modal data for retrieval purposes. To the best of
our knowledge, there has been no work in unsupervised
domain adaptation using deep hashing networks. We now
present the Domain Adaptive Hashing (DAH) network for
unsupervised domain adaptation through deep hashing.

3. Domain Adaptive Hashing Networks

i }ns

In unsupervised domain adaptation, we consider data
from two domains; source and target. The source consists
i , ys
of labeled data, Ds = {xs
i=1 and the target has only
i}nt
i=1. The data points x∗
unlabeled data Dt = {xt
i belong to
X, where X is some input space. The corresponding labels
are represented by y∗
i ∈ Y := {1, . . . , C}. The paradigm of
domain adaptive learning attempts to address the problem of
domain-shift in the data, where the data distributions of the
source and target are different, i.e. Ps(X, Y ) 6= Pt(X, Y ).
The domain-shift notwithstanding, our goal is to train a
deep neural network classiﬁer ψ(.), that can predict the la-
bels {ˆyt

i=1, for the target data.

i }nt

We implement the neural network as a deep CNN which
consists of 5 convolution layers conv1 - conv5 and 3 fully
connected layers fc6 - fc8 followed by a loss layer. In our
model, we introduce a hashing layer hash-fc8 in place of
the standard fc8 layer to learn a binary code hi, for every
data point xi, where hi ∈ {−1, +1}d. The hash-fc8 layer
is driven by two loss functions, (i) supervised hash loss for
the source data, (ii) unsupervised entropy loss for the target
data. The supervised hash loss ensures hash values that are
distinct and discriminatory, i.e. if xi and xj belong to the
same category, their hash values hi and hj are similar and
different otherwise. The unsupervised entropy loss aligns
the target hash values with source hash values based on the
similarity of their feature representations. The output of the

is the output representation of x∗

layer l, where u∗,l
i for the
i
lth layer. The ﬁnal layer outputs are denoted as Us and U t.
The MK-MMD measure d2
k(.) is the multi-kernel maximum
mean discrepancy between the source and target representa-
tions, [24]. For a nonlinear mapping φ(.) associated with a
reproducing kernel Hilbert space Hk and kernel k(.), where
k(x, y) = hφ(x), φ(y)i, the MMD is deﬁned as,

2

.

Hk

(2)

d2
k(U

l
s, U

l
t) =

E[φ(us,l)] − E[φ(ut,l)]
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
The characteristic kernel k(.), is determined as a convex
(cid:12)
combination of κ PSD kernels, {km}κ
m=1, K :=
k : k =
κ
. We set
m=1 βm = 1, βm ≥ 0, ∀m
(cid:8)
βm = 1/κ according to [34] and it works well in practice.
P
3.2. Supervised Hashing for Source Data

κ
m=1 βmkm,

P

(cid:12)
(cid:12)
(cid:12)

(cid:9)

2 (d − h⊤

The Hamming distance for a pair of hash values hi and
hj has a unique relationship with the dot product hhi, hj i,
given by: distH (hi, hj) = 1
i hj), where d is the
hash length. The dot product hhi, hji can be treated as
a similarity measure for the hash codes. Larger the value
of the dot product (high similarity), smaller is the distance
distH and smaller the dot product (low similarity), larger is
the distance distH . Let sij ∈ {0, 1} be the similarity be-
tween xi and xj. If xi and xj belong to the same category,
sij = 1 and 0, otherwise. The probability of similarity be-
tween xi and xj given the corresponding hash values hi
and hj, can be expressed as a likelihood function, given by,

p(sij |hi, hj) =

σ(h⊤
i hj),
1 − σ(h⊤

(

i hj),

sij = 1
sij = 0,

(3)

1

1+e−x is the sigmoid function. As the
where, σ(x) =
dot product hhi, hji increases, the probability of p(sij =
1|hi, hj ) also increases, i.e., xi and xj belong to the same
category. As the dot product decreases, the probability
p(sij = 1|hi, hj) also decreases, i.e., xi and xj belong
to different categories. We construct the (ns × ns) similar-
ity matrix S = {sij}, for the source data with the provided
labels, where sij = 1 if xi and xj belong to the same cat-
egory and 0, otherwise. Let H = {hi}ns
i=1 be the set of
source data hash values. If the elements of H are assumed
to be i.i.d., the negative log likelihood of the similarity ma-
trix S given H can be written as,

L(H) = −log p(S|H)

min
H

= −

sij h⊤

i hj − log

1 + exp(h⊤

i hj )

.

Xsij ∈S (cid:16)

(cid:0)

(cid:1)(cid:17)
(4)

Figure 1: The Domain Adaptive Hash (DAH) network that out-
puts hash codes for the source and the target. The network is
trained with a batch of source and target data. The convolution
layers conv1 - conv5 and the fully connected layers fc6 and fc7 are
ﬁne tuned from the VGG-F network. The MK-MMD loss trains
the DAH to learn feature representations which align the source
and the target. The hash-fc8 layer is trained to output vectors of d
dimensions. The supervised hash loss drives the DAH to estimate
a unique hash value for each object category. The unsupervised
entropy loss aligns the target hash values to their corresponding
source categories. Best viewed in color.
network is represented as ψ(x), where ψ(x) ∈ Rd, which
we convert to a hash code h = sgn(ψ(x)), where sgn(.)
is the sign function. Once the network has been trained,
the probability of x being assigned a label y is given by
f (x) = p(y|h). We train the network using Ds and Dt and
predict the target data labels ˆyt

∗ using f (.).

In order to address the issue of domain-shift, we need to
align the feature representations of the target and the source.
We do that by reducing the domain discrepancy between the
source and target feature representations at multiple layers
of the network. In the following subsections, we discuss
the design of the domain adaptive hash (DAH) network in
detail.

3.1. Reducing Domain Disparity

Deep learning methods have been very successful in do-
main adaptation with state-of-the-art algorithms [17, 31, 34,
46] in recent years. The feature representations transition
from generic to task-speciﬁc as one goes up the layers of
a deep CNN [49]. The convolution layers conv1 to conv5
have been shown to be generic and so, readily transferable,
whereas the fully connected layers are more task-speciﬁc
and need to be adapted before they can be transferred. In
the DAH algorithm, we attempt to minimize the MK-MMD
loss to reduce the domain difference between the source
and target feature representations for fully connected lay-
ers, F = {fc6, fc7, fc8}. Such a loss function has been used
in previous research [31, 34]. The multi-layer MK-MMD
loss is given by,

M(U s, U t) =

d2
k(U

l
s, U

l
t),

(1)

l

where, U
i=1 are the set
of output representations for the source and target data at

s = {us,l

i }ns

t = {ut,l

i }nt

By minimizing Equation (4), we can determine hash val-
ues H for the source data which are consistent with the

Xl∈F
l
i=1 and U

similarity matrix S. The hash loss has been used in pre-
vious research for supervised hashing [30, 50]. Equation
(4) is a discrete optimization problem that is challenging to
solve. We introduce a relaxation on the discrete constraint
hi ∈ {−1, +1}d by instead solving for ui ∈ Rd, where
Us = {ui}ns
i=1 is the output of the network and ui = ψ(xi)
(the superscript denoting the domain has been dropped for
ease of representation). However, the continuous relaxation
gives rise to (i) approximation error, when hhi, hj i is sub-
stituted with hui, uji and, (ii) quantization error, when the
resulting real codes ui are binarized [50]. We account for
the approximation error by having a tanh(.) as the ﬁnal ac-
tivation layer of the neural network, so that the components
of ui are bounded between −1 and +1. In addition, we also
introduce a quantization loss ||ui − sgn(ui)||2
2 along the
lines of [22], where sgn(.) is the sign function. The contin-
uous optimization problem for supervised hashing can now
be outlined;

min
U s

L(U s) = −

sij u⊤

i uj − log

1 + exp(u⊤

i uj)

+

ui − sgn(ui)

Xsij ∈S (cid:16)
ns

i=1
X

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:0)
2
2.

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:1)(cid:17)
(5)

3.3. Unsupervised Hashing for Target Data

k=1

{usj

k }K

In the absence of target data labels, we use the similarity
measure hui, uji, to guide the network to learn discrimina-
tive hash values for the target data. An ideal target output
ut
i, needs to be similar to many of the source outputs from
the jth category
. We assume without loss of
generality, K source data points for every category j where,
(cid:1)
j ∈ {1, . . . , C} and usj
k is the kth source output from cat-
egory j. In addition, ut
i must be dissimilar to most other
source outputs usl
k belonging to a different category (j 6= l).
Enforcing similarity with all the K data points makes for a
more robust target data category assignment. We outline
a probability measure to capture this intuition. Let pij be
the probability that input target data point xi is assigned to
category j where,

(cid:0)

pij =

K
k=1 exp(ut
i
K
k=1 exp(ut
i

⊤usj
k )
⊤usl
k )

C
P
l=1

(6)

P

P

The exp(.) has been introduced for ease of differentiabil-
j pij = 1. When the
ity and the denominator ensures
target data point output is similar to one category only and
dissimilar to all the other categories, the probability vec-
tor pi = [pi1, . . . , piC ]T tends to be a one-hot vector. A
one-hot vector can be viewed as a low entropy realization
of pi. We can therefore envisage all the pi to be one-hot
vectors (low entropy probability vectors), where the target
data point outputs are similar to source data point outputs in
one and only one category. To this end we introduce a loss

P

to capture the entropy of the target probability vectors. The
entropy loss for the network outputs is given by,

H(Us, Ut) = −

pijlog(pij)

(7)

1
nt

nt

C

i=1
X

j=1
X

Minimizing the entropy loss gives us probability vectors pi
that tend to be one-hot vectors, i.e., the target data point
outputs are similar to source data point outputs from any
one category only. Enforcing similarity with K source data
points from a category, guarantees that the hash values are
determined based on a common similarity between multiple
source category data points and the target data point.

3.4. Domain Adaptive Hash Network

We propose a model for deep unsupervised domain adap-
tation based on hashing (DAH) that incorporates unsuper-
vised domain adaptation between the source and the target
(1), supervised hashing for the source (5) and unsupervised
hashing for the target (7) in a deep convolutional neural net-
work. The DAH network is trained to minimize

min
U

J = L(U s) + γM(Us, U t) + ηH(U s, U t),

(8)

where, U := {U s ∪ Ut} and (γ, η) control the importance
of domain adaptation (1) and target entropy loss (7) respec-
tively. The hash values H are obtained from the output of
the network using H = sgn(U). The loss terms (5) and
(7) are determined in the ﬁnal layer of the network with the
network output U. The MK-MMD loss (1) is determined
l
l
between layer outputs {U
t} at each of the fully con-
s, U
nected layers F = {fc6, fc7, fc8}, where we adopt the lin-
ear time estimate for the unbiased MK-MMD as described
in [24] and [31]. The DAH is trained using standard back-
propagation. The detailed derivation of the derivative of (8)
w.r.t. U is provided in the supplementary material.
Network Architecture: Owing to the paucity of images
in a domain adaptation setting, we circumvent the need to
train a deep CNN with millions of images by adapting the
pre-trained VGG-F [8] network to the DAH. The VGG-F
has been trained on the ImageNet 2012 dataset and it con-
sists of 5 convolution layers (conv1 - conv5) and 3 fully
connected layers (fc6, fc7, fc8). We introduce the hashing
layer hash-fc8 that outputs vectors in Rd in the place of fc8.
To account for the hashing approximation, we introduced
a tanh() layer. However, we encounter the issue of van-
ishing gradients [26] when using tanh() as it saturates with
large inputs. We therefore preface the tanh() with a batch
normalization layer which prevents the tanh() from saturat-
ing. In effect, hash-fc8 := {fc8 → batch-norm → tanh()}.
The hash-fc8 provides greater stability when ﬁne-tuning the
learning rates than the deep hashing networks [30, 50]. Fig-
ure 1 illustrates the proposed DAH network.

Table 1: Statistics for the Ofﬁce-Home dataset. Min: # is the
minimum number of images amongst all the categories, Min: Size
and Max: Size are the minimum and maximum image sizes across
all categories and Acc. is the classiﬁcation accuracy.

Domain.
Art
Clipart
Product
Real-World

Min: # Min: Size
117×85 pix.
18×18 pix.
75×63 pix.
88×80 pix.

15
39
38
23

Max: Size
4384×2686 pix.
2400×2400 pix.
2560×2560 pix.
6500×4900 pix.

Acc
44.99±1.85
53.95±1.45
66.41±1.18
59.70±1.04

4. The Ofﬁce-Home Dataset

Supervised deep learning models require a large volume
of labeled training data. Unfortunately, existing datasets
for vision-based domain adaptation are limited in their
size and are not suitable for validating deep learning al-
gorithms. The standard datasets for vision based domain
adaptation are, facial expression datasets CKPlus [35] and
MMI [40], digit datasets SVHN [36], USPS and MNIST[28],
head pose recognition datasets PIE [33], object recogni-
tion datasets COIL[33], Ofﬁce [42] and Ofﬁce-Caltech [20].
These datasets were created before deep-learning became
popular and are insufﬁcient for training and evaluating deep
learning based domain adaptation approaches. For instance,
the object-recognition dataset Ofﬁce has 4110 images across
31 categories and Ofﬁce-Caltech has 2533 images across 10
categories.

We release the Ofﬁce-Home dataset for domain adap-
tation based object recognition, that can be used to evalu-
ate deep learning algorithms for domain adaptation. The
Ofﬁce-Home dataset consists of 4 domains, with each do-
main containing images from 65 categories of everyday ob-
jects and a total of around 15, 500 images. The domains
include, Art: artistic depictions of objects in the form of
sketches, paintings, ornamentation, etc.; Clipart: collec-
tion of clipart images; Product: images of objects with-
out a background, akin to the Amazon category in Ofﬁce
dataset; Real-World: images of objects captured with a
regular camera.

Public domain images were downloaded from web-
sites like www.deviantart.com and www.ﬂickr.com to cre-
ate the Art and Real-World domains. Clipart im-
ages were gathered from multiple clipart websites. The
Product domain images were exclusively collected from
www.amazon.com using web-crawlers. The collected im-
ages were manually ﬁltered on the basis of quality, size and
content. The dataset has an average of around 70 images
per category and a maximum of 99 images in a category.
The primary challenge in creating this dataset was acquir-
ing sufﬁcient number of public domain images across all
the 4 domains. Figure 2 depicts a sampling of 16 categories
from the Ofﬁce-Home dataset and Table 1 outlines some
meta data for the dataset. The Acc. column in the Table
1 refers to classiﬁcation accuracies using the LIBLINEAR
SVM [15] classiﬁer (5-fold cross validation) with deep fea-

tures extracted using the VGG-F network. The dataset is
publicly available for research 1.

5. Experiments

In this section we conduct extensive experiments to
evaluate the DAH algorithm. Since we propose a do-
main adaptation technique based on hashing, we evalu-
ate objection recognition accuracies for unsupervised do-
main adaptation and also study the discriminatory capabil-
ity of the learned hash codes for unsupervised domain adap-
tive hashing. The implementation details are available at
https://github.com/hemanthdv/da-hash

5.1. Datasets

Ofﬁce [42]: This is currently the most popular benchmark
dataset for object recognition in the domain adaptation com-
puter vision community. The dataset consists of images of
everyday objects in an ofﬁce environment. It has 3 domains;
Amazon (A), Dslr (D) and Webcam (W). The dataset has
around 4, 100 images with a majority of the images (2816
images) in the Amazon domain. We adopt the common
evaluation protocol of different pairs of transfer tasks for
this dataset [31, 34]. We consider 6 transfer tasks for all
combinations of source and target pairs for the 3 domains.
Ofﬁce-Home: We introduce this new dataset and evaluate
it in a similar manner to the Ofﬁce dataset. We consider 12
transfer tasks for the Art (Ar), Clipart (Cl), Product
(Pr) and Real-World (Rw) domains for all combinations
of source and target for the 4 domains. Considering all the
different pairs of transfer enables us to evaluate the inherent
bias between the domains in a comprehensive manner [45].

5.2. Implementation Details

We implement the DAH using the MatConvnet frame-
work [47]. Since we train a pre-trained VGG-F, we ﬁne-
tune the weights of conv1-conv5, fc6 and fc7. We set
their learning rates to 1/10th the learning rate of hash-fc8.
We vary the learning rate between 10−4 to 10−5 over 300
epochs with a momentum 0.9 and weight decay 5 × 10−4.
We set K = 5 (number of samples from a category). Since
we have 31 categories in the Ofﬁce dataset, we get a source
batch size of 31 × 5 = 155. For the target batch, we ran-
domly select 155 samples. The total batch size turns out to
be 310. For the Ofﬁce-Home dataset, with K = 5 and 65
categories, we get a batch size of 650. We set d = 64 (hash
code length) for all our experiments. Since there is imbal-
ance in the number of like and unlike pairs in S, we set the
values in similarity matrix Si,j ∈ {0, 10}. Increasing the
similarity weight of like-pairs improves the performance of
DAH. For the entropy loss, we set η = 1. For the MK-
MMD loss, we follow the heuristics mentioned in [24], to

1https://hemanthdv.github.io/officehome-dataset/

Figure 2: Sample images from the Ofﬁce-Home dataset. The dataset consists of images of everyday objects organized into 4 domains;
Art: paintings, sketches and/or artistic depictions, Clipart: clipart images, Product: images without background and Real-World:
regular images captured with a camera. The ﬁgure displays examples from 16 of the 65 categories.

determine the parameters. We estimate γ, by validating a
binary domain classiﬁer to distinguish between source and
target data points and select γ which gives largest error on a
validation set. For MMD, we use a Gaussian kernel with a
bandwidth σ given by the median of the pairwise distances
in the training data. To incorporate the multi-kernel, we
vary the bandwidth σm ∈ [2−8σ, 28σ] with a multiplicative
factor of 2. We deﬁne the target classiﬁer f (xt
i) = p(y|ht
i)
in terms of 6. The target data point is assigned to the class
with the largest probability, with ˆyi = maxj(pij ) using the
hash codes for the source and the target.

5.3. Unsupervised Domain Adaptation

In this section, we study the performance of the DAH
for unsupervised domain adaptation, where labeled data is
available only in the source domain and no labeled data is
available in the target domain. We compare the DAH with
state-of-the-art domain adaptation methods: (i) Geodesic
Flow Kernel (GFK) [20], (ii) Transfer Component Analy-
sis (TCA) [38], (iii) Correlation Alignment (CORAL) [44]
and (iv) Joint Distribution Adaptation (JDA) [33]. We also
compare the DAH with state-of-the-art deep learning meth-
ods for domain adaptation: (v) Deep Adaptation Network
(DAN) [31] and (vi) Domain Adversarial Neural Network
(DANN) [17]. For all of the shallow learning methods,
we extract and use deep features from the fc7 layer of the
VGG-F network that was pre-trained on the ImageNet 2012
dataset. We also evaluate the effect of the entropy loss on
hashing for the DAH. The DAH-e is the DAH algorithm
where η is set to zero, which implies that the target hash
values are not driven to align with the source categories.
We follow the standard protocol for unsupervised domain
adaptation, where all the labeled source data and all the un-
labeled target data is used for training.
Results and Discussion: The results are reported for the
target classiﬁcation in each of the transfer tasks in Tables 2
and 3, where accuracies denote the percentage of correctly

Table 2: Recognition accuracies (%) for domain adaptation exper-
iments on the Ofﬁce dataset. {Amazon (A), Dslr (D), Webcam
(W)}. A→W implies A is source and W is target.

Expt.
GFK
TCA
CORAL
JDA
DAN
DANN
DAH-e
DAH

A→D A→W D→A D→W W→A W→D
93.17
48.59
96.79
51.00
98.59
54.42
97.79
59.24
99.40
67.04
99.40
72.89
96.99
66.27
98.80
66.47

52.08
49.43
51.70
58.62
67.80
72.70
66.16
68.30

41.83
48.12
48.26
51.35
50.36
56.25
55.97
55.54

89.18
93.08
95.97
96.86
95.85
96.48
94.59
96.10

49.04
48.83
47.27
52.34
52.33
53.20
53.91
53.02

Avg.
62.32
64.54
66.04
69.37
72.13
75.15
72.31
73.04

classiﬁed target data samples. We present results with hash
length d = 64 bits. The DAH algorithm consistently out-
performs the baselines across all the domains for the Ofﬁce-
Home dataset. However, DANN marginally surpasses DAH
for the Ofﬁce dataset, prompting us to reason that domain
adversarial training is more effective than DAH when the
categories are fewer in number. Since domain alignment is
category agnostic, it is possible that the aligned domains are
not classiﬁcation friendly in the presence of large number
of categories. When the number of categories is large, as in
Ofﬁce-Home, DAH does best at extracting transferable fea-
tures to achieve higher accuracies. We also note that DAH
delivers better performance than DAH-e; thus, minimizing
the entropy on the target data through 7 aids in improved
alignment of the source and target samples, which boosts
the accuracy.
Feature Analysis: We also study the feature representa-
tions of the penultimate layer (fc7) outputs using t-SNE em-
beddings as in [12]. Figure 3a depicts the A-distance be-
tween domain pairs using Deep (VGG-F), DAN and DAH
[2] deﬁned A-distance as the
features. Ben-David et al.
distance between two domains that can be viewed as the
discrepancy between two domains. Although it is difﬁcult
to estimate its exact value, an approximate distance mea-
sure is given by 2(1 − 2ǫ), where ǫ is the generalization
error for a binary classiﬁer trained to distinguish between
the two domains. We used a LIBLINEAR SVM [15] clas-

Table 3: Recognition accuracies (%) for domain adaptation experiments on the Ofﬁce-Home dataset. {Art (Ar), Clipart (Cl),
Product (Pr), Real-World (Rw)}. Ar→Cl implies Ar is source and Cl is target.

Expt.
GFK
TCA
CORAL
JDA
DAN
DANN
DAH-e
DAH

Ar→Cl Ar→Pr Ar→Rw Cl→Ar Cl→Pr Cl→Rw Pr→Ar Pr→Cl Pr→Rw Rw→Ar Rw→Cl Rw→Pr Avg.
32.40
24.52
21.60
30.34
21.92
19.93
37.91
27.77
27.10
36.97
25.96
25.34
43.46
29.07
30.66
44.94
30.49
33.33
42.69
29.87
29.23
45.54
29.91
31.64

34.20
31.74
40.33
40.90
49.78
49.76
47.49
52.79

34.94
31.36
40.03
40.19
47.59
49.13
48.23
51.93

21.63
19.00
26.08
24.52
32.83
32.26
33.79
34.69

42.92
42.12
50.61
49.25
56.70
56.76
55.63
60.71

25.73
23.64
30.54
32.72
34.05
38.14
38.76
39.63

50.89
48.68
57.11
55.35
62.73
64.65
59.07
62.54

32.88
30.74
38.48
35.10
43.58
44.71
41.16
44.99

38.83
35.71
44.32
42.94
54.13
54.42
48.29
51.73

31.72
32.08
36.16
35.98
42.17
42.96
35.71
40.75

28.96
27.15
36.36
35.35
38.25
42.66
44.99
45.13

siﬁer with 5-fold cross-validation to estimate ǫ. Figure 3a
indicates that the DAH features have the least discrepancy
between the source and target compared to DAN and Deep
features. This is also conﬁrmed with the t-SNE embeddings
in Figures 3b-3d. The Deep features show very little over-
lap between the domains and the categories depict minimal
clustering. Domain overlap and clustering improves as we
move to DAN and DAH features, with DAH providing the
best visualizations. This corroborates the efﬁcacy of the
DAH algorithm to exploit the feature learning capabilities
of deep neural networks to learn representative hash codes
to address domain adaptation.

5.4. Unsupervised Domain Adaptive Hashing

In this section, we study the performance of our algo-
rithm to generate compact and efﬁcient hash codes from the
data for classifying unseen test instances, when no labels
are available. This problem has been addressed in the litera-
ture, with promising empirical results [7, 11, 21]. However,
in a real-world setting, labels may be available from a dif-
ferent, but related (source) domain; a strategy to utilize the
labeled data from the source domain to learn representative
hash codes for the target domain is therefore of immense
practical importance. Our work is the ﬁrst to identify and
address this problem. We consider the following scenar-
ios to address this real-world challenge: (i) No labels are
available for a given dataset and the hash codes need to be
learned in a completely unsupervised manner. We evaluate
against baseline unsupervised hashing methods (ITQ) [22]
and (KMeans) [25] and also state-of-the-art methods for
unsupervised hashing (BA) [7] and (BDNN) [11]. (ii) La-
beled data is available from a different, but related source
domain. A hashing model is trained on the labeled source
data and is used to learn hash codes for the target data. We
refer to this method as NoDA, as no domain adaptation is
performed. We used the deep pairwise-supervised hashing
(DPSH) algorithm [30] to train a deep network with the
source data and applied the network to generate hash codes
for the target data.
(iii) Labeled data is available from a
different, but related source domain and we use our DAH
formulation to learn hash codes for the target domain by
(iv) Labeled data is available
reducing domain disparity.

Table 4: Mean average precision @64 bits. For the NoDA and
DAH results, Art is the source domain for Clipart, Product
and Real-World and Clipart is the source domain for Art.
Similarly, Amazon and Webcam are source target pairs.

Expt.
Amazon
Webcam
Art
Clipart
Product
Real-World
Avg.

NoDA ITQ KMeans
0.465
0.324
0.652
0.511
0.191
0.155
0.195
0.160
0.393
0.239
0.323
0.281
0.370
0.278

0.403
0.558
0.170
0.178
0.341
0.279
0.322

BA
0.367
0.480
0.156
0.179
0.349
0.273
0.301

BDNN DAH SuH
0.830
0.582
0.491
0.939
0.717
0.656
0.492
0.302
0.193
0.622
0.333
0.206
0.774
0.414
0.407
0.586
0.533
0.336
0.707
0.480
0.382

in the target domain. This method falls under supervised
hashing (SuH) (as it uses labeled data in the target domain
to learn hash codes in the same domain) and denotes the
It is included to com-
upper bound on the performance.
pare the performance of unsupervised hashing algorithms
relative to the supervised algorithm. We used the DPSH al-
gorithm [30] to train a deep network on the target data and
used it to generate hash codes on a validation subset.

Results and Discussion: We applied the precision-recall
curves and the mean average precision (mAP) measures to
evaluate the efﬁcacy of the hashing methods, similar to pre-
vious research [7, 11, 21]. The results are depicted in Fig-
ures 4 and 5 (precision-recall curves) and Table 4 (mAP
values), where we present hashing with code length d = 64
bits. Hashing performance with d = 16 bits also follows
a similar trend and is presented in the supplementary mate-
rial. For the sake of brevity, we drop the results with Dslr
as it is very similar to Webcam, with little domain differ-
ence. We note that the NoDA has the poorest performance
due to domain mismatch. This demonstrates that domain
disparity needs to be considered before deploying a hashing
network to extract hash codes. The unsupervised hashing
methods ITQ, KMeans, BA and BDNN perform slightly
better compared to NoDA. The proposed DAH algorithm
encompasses hash code learning and domain adaptation in
a single integrated framework. It is thus able to leverage
the labeled data in the source domain in a meaningful man-
ner to learn efﬁcient hash codes for the target domain. This
accounts for its improved performance, as is evident in Fig-
ures 4 and 5 and Table 4. The supervised hashing technique
(SuH) uses labels from the target and therefore depicts the

e
c
n
a
t
s
i
D
A

-

1.5

0.5

2

1

0

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Deep
DAN
DAH

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Ar -> Cl Ar -> Pr Ar -> Rw

(a) A-Distance

(b) Deep Features (Ar,Cl)

(c) DAN Features (Ar,Cl)

(d) DAH Features (Ar,Cl)

Figure 3: Feature analysis of fc7 layer. (a) A-distances for Deep, DAN and DAH, (b), (c) and (d) t-SNE embeddings for 10 categories
from Art (•) and Clipart(+) domains. Best viewed in color.

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

0.2

0.4

0.6

0.8

1

0.2

0.4

0.6

0.8

1

0.2

0.4

0.6

0.8

1

0.2

0.4

0.6

0.8

1

Recall
 

(a) Art

Recall
 

(b) Clipart

Recall
 

(c) Product

Recall
 

(d) Real-World

Figure 4: Precision-Recall curves @64 bits for the Ofﬁce-Home dataset. Comparison of hashing without domain adaptation (NoDA),
shallow unsupervised hashing (ITQ, KMeans), state-of-the-art deep unsupervised hashing (BA, BDNN), unsupervised domain adaptive
hashing (DAH) and supervised hashing (SuH). Best viewed in color.

real-world setting.

6. Conclusions

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

0.2

0.4

0.6

0.8

1

0.2

0.4

0.6

0.8

1

Recall
 

(a) Amazon

Recall
 

(b) Webcam

Figure 5: Precision-Recall curves @64 bits for the Ofﬁce dataset.
Comparison of hashing without domain adaptation (NoDA), shal-
low unsupervised hashing (ITQ, KMeans), state-of-the-art deep
unsupervised hashing (BA, BDNN), unsupervised domain adap-
tive hashing (DAH) and supervised hashing (SuH). Best viewed
in color.

best performance. The proposed DAH framework consis-
tently delivers the best performance relative to SuH when
compared with the other hashing procedures. This demon-
strates the merit of our framework in learning representa-
tive hash codes by utilizing labeled data from a different
domain. Such a framework will be immensely useful in a

In this paper, we have proposed a novel domain adap-
tive hashing (DAH) framework which exploits the feature
learning capabilities of deep neural networks to learn efﬁ-
cient hash codes for unsupervised domain adaptation. The
DAH framework solves two important practical problems:
category assignment with weak supervision or insufﬁcient
labels (through domain adaptation) and the estimation of
hash codes in an unsupervised setting (hash codes for target
data). Thus, two practical challenges are addressed through
a single integrated framework. This research is the ﬁrst
of its kind to integrate hash code learning with unsuper-
vised domain adaptation. We also introduced a new dataset,
Ofﬁce-Home, which can be used to further research in do-
main adaptation.
Acknowledgements: This material is based upon work
supported by the National Science Foundation (NSF) un-
der Grant No:1116360. Any opinions, ﬁndings, and con-
clusions or recommendations expressed in this material are
those of the authors and do not necessarily reﬂect the views
of the NSF.

References

[1] Y. Aytar and A. Zisserman. Tabula rasa: Model transfer for

object category detection. In IEEE ICCV, 2011. 2

[2] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira,
and J. W. Vaughan. A theory of learning from different do-
mains. Machine learning, 79(1-2):151–175, 2010. 6

[3] Y. Bengio, A. Courville, and P. Vincent. Representation
learning: A review and new perspectives. IEEE transactions
on pattern analysis and machine intelligence, 35(8):1798–
1828, 2013. 2

[4] L. Bruzzone and M. Marconcini. Domain adaptation prob-
lems: A dasvm classiﬁcation technique and a circular valida-
tion strategy. IEEE, PAMI, 32(5):770–787, 2010. 2

[5] Y. Cao, M. Long, J. Wang, Q. Yang, and P. S. Yu. Deep
visual-semantic hashing for cross-modal retrieval. In ACM-
SIGKDD, 2016. 2

[6] Z. Cao, M. Long, and Q. Yang. Transitive hashing network
for heterogeneous multimedia retrieval. In AAAI, 2016. 2
[7] M. A. Carreira-Perpin´an and R. Raziperchikolaei. Hashing
with binary autoencoders. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
557–566, 2015. 2, 7

[8] K. Chatﬁeld, K. Simonyan, A. Vedaldi, and A. Zisserman.
Return of the devil in the details: Delving deep into convo-
lutional nets. In BMVC, 2014. 2, 4

[9] R. Chattopadhyay, Q. Sun, W. Fan, I. Davidson, S. Pan-
chanathan, and J. Ye. Multisource domain adaptation and
its application to early detection of fatigue. ACM Transac-
tions on Knowledge Discovery from Data (TKDD), 6(4):18,
2012. 2

[10] W.-S. Chu, F. De la Torre, and J. F. Cohn. Selective transfer
machine for personalized facial action unit detection. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 3515–3522, 2013. 2

[11] T.-T. Do, A.-D. Doan, and N.-M. Cheung. Learning to hash
with binary deep neural network. In European Conference
on Computer Vision, pages 219–234. Springer, 2016. 2, 7

[12] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,
E. Tzeng, and T. Darrell. Decaf: A deep convolutional acti-
vation feature for generic visual recognition. In ICML, pages
647–655, 2014. 2, 6

[13] L. Duan, I. W. Tsang, and D. Xu. Domain transfer multiple
kernel learning. IEEE PAMI, 34(3):465–479, 2012. 2
[14] V. Erin Liong, J. Lu, G. Wang, P. Moulin, and J. Zhou. Deep
hashing for compact binary codes learning. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 2475–2483, 2015. 2

[15] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-
J. Lin. Liblinear: A library for large linear classiﬁcation.
Journal of machine learning research, 9(Aug):1871–1874,
2008. 5, 6

[16] B. Fernando, A. Habrard, M. Sebban, and T. Tuytelaars. Un-
supervised visual domain adaptation using subspace align-
ment. In CVPR, pages 2960–2967, 2013. 1, 2

[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,
F. Laviolette, M. Marchand, and V. Lempitsky. Domain-

adversarial training of neural networks. Journal of Machine
Learning Research, 17(59):1–35, 2016. 1, 2, 3, 6

[18] X. Glorot, A. Bordes, and Y. Bengio. Domain adaptation
for large-scale sentiment classiﬁcation: A deep learning ap-
proach. In Proceedings of the 28th International Conference
on Machine Learning (ICML-11), pages 513–520, 2011. 1,
2

[19] B. Gong, K. Grauman, and F. Sha. Connecting the dots
with landmarks: Discriminatively learning domain-invariant
features for unsupervised domain adaptation. In ICML (1),
pages 222–230, 2013. 2

[20] B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic ﬂow
kernel for unsupervised domain adaptation. In IEEE CVPR,
2012. 1, 2, 5, 6

[21] Y. Gong and S. Lazebnik.

Iterative quantization: A pro-
In Computer
crustean approach to learning binary codes.
Vision and Pattern Recognition (CVPR), 2011 IEEE Confer-
ence on, pages 817–824. IEEE, 2011. 7

[22] Y. Gong, S. Lazebnik, A. Gordo, and F. Perronnin.

Itera-
tive quantization: A procrustean approach to learning binary
IEEE Transactions
codes for large-scale image retrieval.
on Pattern Analysis and Machine Intelligence, 35(12):2916–
2929, 2013. 2, 4, 7

[23] R. Gopalan, R. Li, and R. Chellappa. Domain adaptation for
object recognition: An unsupervised approach. In 2011 in-
ternational conference on computer vision, pages 999–1006.
IEEE, 2011. 2

[24] A. Gretton, D. Sejdinovic, H. Strathmann, S. Balakrishnan,
M. Pontil, K. Fukumizu, and B. K. Sriperumbudur. Optimal
kernel choice for large-scale two-sample tests. In Advances
in neural information processing systems, pages 1205–1213,
2012. 3, 4, 5, 11

[25] K. He, F. Wen, and J. Sun. K-means hashing: An afﬁnity-
preserving quantization method for learning binary compact
codes. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 2938–2945, 2013. 2, 7
[26] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber.
the difﬁculty of learning

Gradient ﬂow in recurrent nets:
long-term dependencies, 2001. 4

[27] J. Hoffman, E. Rodner, J. Donahue, K. Saenko, and T. Dar-
rell. Efﬁcient learning of domain-invariant image represen-
tations. In ICLR, 2013. 2

[28] K. Jarrett, K. Kavukcuoglu, Y. Lecun, et al. What is the
best multi-stage architecture for object recognition? In 2009
IEEE 12th International Conference on Computer Vision,
pages 2146–2153. IEEE, 2009. 5

[29] Q.-Y. Jiang and W.-J. Li. Deep cross-modal hashing. arXiv

preprint arXiv:1602.02255, 2016. 2

[30] W.-J. Li, S. Wang, and W.-C. Kang. Feature learning based
In IJCAI,

deep supervised hashing with pairwise labels.
2016, 2016. 4, 7

[31] M. Long, Y. Cao, J. Wang, and M. Jordan. Learning transfer-
able features with deep adaptation networks. In ICML, pages
97–105, 2015. 1, 2, 3, 4, 5, 6

[32] M. Long, J. Wang, G. Ding, J. Sun, and P. Yu. Transfer joint
In CVPR,

matching for unsupervised domain adaptation.
pages 1410–1417, 2014. 2

[50] H. Zhu, M. Long, J. Wang, and Y. Cao. Deep hashing net-
work for efﬁcient similarity retrieval. In Thirtieth AAAI Con-
ference on Artiﬁcial Intelligence, 2016. 4

[33] M. Long, J. Wang, G. Ding, J. Sun, and P. S. Yu. Transfer
In Pro-
feature learning with joint distribution adaptation.
ceedings of the IEEE International Conference on Computer
Vision, pages 2200–2207, 2013. 1, 2, 5, 6

[34] M. Long, H. Zhu, J. Wang, and M. I. Jordan. Unsupervised
domain adaptation with residual transfer networks. In NIPS,
2016. 1, 2, 3, 5

[35] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and
I. Matthews. The extended cohn-kanade dataset (ck+): A
complete dataset for action unit and emotion-speciﬁed ex-
pression. In CVPR, pages 94–101. IEEE, 2010. 5

[36] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.
Ng. Reading digits in natural images with unsupervised fea-
ture learning. In NIPS Workshop on Deep Learning and Un-
supervised Feature Learning 2011, 2011. 5

[37] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Learning and
transferring mid-level image representations using convolu-
In Proceedings of the IEEE con-
tional neural networks.
ference on computer vision and pattern recognition, pages
1717–1724, 2014. 2

[38] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain
adaptation via transfer component analysis. Neural Net-
works, IEEE Trans. on, 22(2):199–210, 2011. 1, 2, 6
[39] S. J. Pan and Q. Yang. A survey on transfer learning. IEEE

TKDE, 22(10):1345–1359, 2010. 2

[40] M. Pantic, M. Valstar, R. Rademaker, and L. Maat. Web-
In ICME.

based database for facial expression analysis.
IEEE, 2005. 5

[41] V. M. Patel, R. Gopalan, R. Li, and R. Chellappa. Visual do-
main adaptation: A survey of recent advances. IEEE signal
processing magazine, 32(3):53–69, 2015. 2

[42] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting vi-
sual category models to new domains. In ECCV, 2010. 1, 2,
5

[43] S. Shekhar, V. M. Patel, H. V. Nguyen, and R. Chellappa.
In Proceedings
Generalized domain-adaptive dictionaries.
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 361–368, 2013. 1

[44] B. Sun, J. Feng, and K. Saenko. Return of frustratingly easy
domain adaptation. In ICCV, TASK-CV, 2015. 1, 2, 6
[45] A. Torralba and A. A. Efros. Unbiased look at dataset bias.
In Computer Vision and Pattern Recognition (CVPR), 2011
IEEE Conference on, pages 1521–1528. IEEE, 2011. 5
[46] E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko. Simultane-
ous deep transfer across domains and tasks. In Proceedings
of the IEEE International Conference on Computer Vision,
pages 4068–4076, 2015. 1, 2, 3

[47] A. Vedaldi and K. Lenc. Matconvnet – convolutional neural
networks for matlab. In Proceeding of the ACM Int. Conf. on
Multimedia, 2015. 5

[48] J. Wang, H. T. Shen, J. Song, and J. Ji. Hashing for similarity
search: A survey. arXiv preprint arXiv:1408.2927, 2014. 1
[49] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How trans-
ferable are features in deep neural networks? In Advances
in neural information processing systems, pages 3320–3328,
2014. 2, 3

Supplementary Material

7. Loss Function Derivative

In this section we outline the derivative of Equation 8 for the backpropagation algorithm;

min
U

J = L(U s) + γM(Us, U t) + ηH(U s, U t),

where, U := {Us ∪ Ut} and (γ, η) control the importance of domain adaptation (1) and target entropy loss (7) respectively.
In the following subsections, we outline the derivative of the individual terms w.r.t. the input U.

7.1. Derivative for MK MMD

M(U s, U t) =

d2
k(U

l
s, U

l
t),

Xl∈F

d2
k(U

l
s, U

l
t) =

E[φ(us,l)] − E[φ(ut,l)]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2

.

Hk

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

We implement the linear MK-MMD loss according to [24]. For this derivation, we consider the loss at just one layer. The
derivative for the MK-MMD loss at every other layer can be derived in a similar manner. The output of ith source data point
at layer l is represented as ui and the output of the ith target data point is represented as vi. For ease of representation, we
drop the superscripts for the source (s), the target (t) and the layer (l). Unlike the conventional MMD loss which is O(n2),
the MK-MMD loss outlined in [24] is O(n) and can be estimated online (does not require all the data). The loss is calculated
over every batch of data points during the back-propagation. Let n be the number of source data points U := {ui}n
i=1 and the
number of target data points V := {vi}n
i=1 in the batch. We assume equal number of source and target data points in a batch
and that n is even. The MK-MMD is deﬁned over a set of 4 data points wi = [u2i−1, u2i, v2i−1, v2i], ∀i ∈ {1, 2, . . . , n/2}.
The MK-MMD is given by,

M(U, V) =

hm(wi),

βm

1
n/2

κ

m=1
X

n/2

i=1
X

where, κ is the number of kernels and βm = 1/κ is the weight for each kernel and,

hm(wi) = km(u2i−1, u2i) + km(v2i−1, v2i) − km(u2i−1, v2i) − km(u2i, v2i−1),

(10)

where, km(x, y) = exp

. Re-writing the MK-MMD in terms of the kernels, we have,

− ||x−y||
σm

2
2

(cid:1)
n/2

κ

(cid:0)

2
nκ

m=1
X

i=1
X

(cid:2)

M(U, V) =

km(u2i−1, u2i) + km(v2i−1, v2i) − km(u2i−1, v2i) − km(u2i, v2i−1)

,

(11)

11

(cid:3)

(8)

(1)

(2)

(9)

We now outline the derivative of 11 w.r.t. source output uq and target output vq. The derivative is,

km(u2i−1, u2i).(u2i−1 − u2i).(I{q = 2i} − I{q = 2i − 1})

∂M
∂uq

=

2
nκ

κ

n/2

m=1
X

i=1 h
X

∂M
∂vq

=

2
nκ

κ

n/2

m=1
X

i=1 h
X

2
σm

+

2
σm

2
σm

−

2
σm

km(u2i−1, v2i).(u2i−1 − v2i).I{q = 2i − 1} +

km(u2i, v2i−1).(u2i − v2i−1).I{q = 2i}

,

where, I{.} is the indicator function which is 1 if the condition is true, else it is false. The derivative w.r.t. the target data
output vq is,

km(v2i−1, v2i).(v2i−1 − v2i).(I{q = 2i} − I{q = 2i − 1})

km(u2i−1, v2i).(u2i−1 − v2i).I{q = 2i} −

km(u2i, v2i−1).(u2i − v2i−1).I{q = 2i − 1}

,

2
σm

2
σm

7.2. Derivative for Supervised Hash Loss

The supervised hash loss is given by,

(cid:1)(cid:17)

i

(cid:1)

min
U s

L(U s) = −

siju⊤

i uj − log

1 + exp(u⊤

i uj)

+

ui − sgn(ui)

Xsij ∈S (cid:16)
ns

i=1
X

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:0)
2
2.

(cid:12)
(cid:12)

(cid:12)
(cid:12)

The partial derivative of 5 w.r.t. source data output up is given by,

∂L
∂uq

=

Xsij ∈S h
1

I{i = q}

σ(u⊤

i uj) − sij

uj + I{j = q}

σ(u⊤

i uj) − sij

ui

+ 2(uq − sgn(uq))

(14)

(cid:0)
1+exp(−x) . We assume sgn(.) to be a constant and avoid the differentiability issues with sgn(.) at 0. Since

(cid:1)

(cid:0)

(cid:1)

where, σ(x) =
the S is symmetric, we can reduce the derivative to,

∂L
∂uq

=

ns

2
j=1 h
X

(cid:0)

σ(u⊤

q uj) − sqj

uj

+ 2

uq − sgn(uq)

.

i

(cid:1)

(cid:0)

7.3. Derivative for Unsupervised Entropy Loss

We outline the derivative of dH

dU in the following section, where H is deﬁned as,

H(Us, Ut) = −

pij log(pij)

1
nt

nt

C

i=1
X

j=1
X

and pij is the probability of target data output ut

i belonging to category j, given by

For ease of representation, we will denote the target output ut
P
kth source data point in the jth category usj
the news terms as,

k as uj

pij =

C
P
l=1

K
k=1 exp(ut
i

⊤usj
k )
⊤usl
K
k′=1 exp(ut
k′ )
i
i as vi and drop the superscript t. Similarly, we will denote the
P
k, by dropping the domain superscript. We deﬁne the probability pij with

(6)

(12)

i

(13)

i

(5)

(15)

(7)

(16)

pij =

K
k=1 exp(vi

⊤uj
k)
K
⊤ul
k′=1 exp(vi

k′ )

C
P
l=1

P

P

Further, we simplify by replacing exp(v⊤

i uj

k) with exp(i, jk). Equation 16 can now be represented as,

pij =

K
k=1 exp(i, jk)

C
P
l=1

K
k′=1 exp(i, lk′)

We drop the outer summations (along with the -ve sign) and will reintroduce it at a later time. The entropy loss can be
re-phrased using log( a

P

P

b ) = log(a) - log(b) as,

Hij =

K
k=1 exp(i, jk)

C
P
l=1

K
k′=1 exp(i, lk′)
K
k=1 exp(i, jk)

P
−

P
C
P
l=1
for the target and ∂Hij
∂up
q

P

P

K
k′=1 exp(i, lk′)

log

K
k=1 exp(i, jk)

(cid:0) P
log

C
l=1

(cid:0) P

P

(cid:1)

K
k′=1 exp(i, lk′)
(cid:1)

for the source. We refer to ∂up

q for a consistent reference to source

We need to estimate both, ∂Hij
∂vi
data. The derivative ∂Hij
∂up
q

for 18 is,

∂Hij
∂up

(cid:20)

q (cid:21)18

=

vi
l,k′ exp(i, lk′)

k I{j=p,

k=q }exp(i, jk).log

k exp(i, jk)

+

k I{j=p,

k=q }exp(i, jk)

P

h P

− pijexp(i, pq)log

(cid:0) P
k exp(i, jk)

(cid:1)

P

(cid:0) P

,

(cid:1)i

where, I{.} is an indicator function which is 1 only when both the conditions within are true, else it is 0. The derivative ∂Hij
∂up
q
for 19 is,

∂Hij
∂up

(cid:20)

q (cid:21)19

= −

vi
l,k′ exp(i, lk′)

P

k I{j=p,

k=q }exp(i, jk).log

h P

− pij exp(i, pq)log

l,k′ exp(i, lk′)
(cid:1)

(cid:0) P

l,k′ exp(i, lk′)

+ pijexp(i, pq)

Expressing ∂Hij
∂up
q

=

∂Hij
∂up
q

+

∂Hij
∂up
q

, and deﬁning ¯pijk = exp(i,jk)

(cid:0) P

(cid:1)i
Pl,k′ exp(i,lk′) the derivative w.r.t. the source is,

18
i
=vi

19
i
h
k I{j=p,
k=q }¯pijk.log

h
∂Hij
∂up
q

k exp(i, jk)

+

k I{j=p,

k=q }¯pijk

h P
− pij ¯pipqlog

(cid:0) P
k exp(i, jk)

−

(cid:1)
k I{j=p,
k=q }¯pijk.log

P

− pij ¯pipq + pij ¯pipqlog

(cid:0) P

l,k′ exp(i, lk′)
(cid:1)
P

l,k′ exp(i, lk′)
(cid:1)

(cid:0) P

=vi

k I{j=p,

k=q }¯pijklog(pij ) − pij ¯pipqlog(pij ) +

(cid:0) P

k I{j=p,

k=q }¯pijk − pij ¯pipq

(cid:1)i

=vi

h P
log(pij ) + 1

k I{j=p,

k=q }¯pijk − pij ¯pipq

P

i

The derivative of H w.r.t the source output up

(cid:0)

(cid:1)h P
q is given by,

i

nt

C

∂H
∂up
q

= −

1
nt

i=1
X
for 18 as,

j=1
X

(cid:0)

vi

log(pij) + 1

k I{j=p,

k=q }¯pijk − pij ¯pipq

(cid:1)h P

i

We now outline the derivative ∂H
∂vi

∂Hij
∂vi (cid:21)18

(cid:20)

=

1
l,k′ exp(i, lk′)

P

k exp(i, jk)

k exp(i, jk)uj

k +

k exp(i, jk)uj

k

(cid:0) P

1
l,k′ exp(i, lk′)

(cid:1) P

k exp(i, jk)log

P
k exp(i, jk)

P

(cid:0) P

(cid:1) P

l,k′ exp(i, lk′)ul
k′

,

(26)

i

log

h

−

P

(17)

(18)

(19)

(20)

(21)

(22)

(23)

(24)

(25)

and the derivative ∂H
∂vi

for 19 as,

∂Hij
∂vi (cid:21)19

(cid:20)

= −

1
l,k′ exp(i, lk′)

P

log

h

−

P
19
i

l,k′ exp(i, lk′)

k exp(i, jk)uj

k + Pk exp(i,jk)
Pl,k′ exp(i,lk′)

l,k′ exp(i, lk′)ul
k′

(cid:0) P

1
l,k′ exp(i, lk′)

(cid:1) P

k exp(i, jk)log

l,k′ exp(i, lk′)

l,k′ exp(i, lk′)ul
k′

,

(27)

P

(cid:0) P

i

P

(cid:1) P

Expressing ∂Hij
∂vi

=

∂Hij
∂vi

+

∂Hij
∂vi

, we get,

∂Hij
∂vi

=

18
i

h

h

1
l,k′ exp(i, lk′)

log

k exp(i, jk)

k exp(i, jk)uj

k − log

l,k′ exp(i, lk′)

k exp(i, jk)uj

k

(cid:0) P

(cid:1) P

h
(cid:0) P
k exp(i, jk)uj
k − pij

+
P

− pij log
P

k exp(i, jk)

P

(cid:1) P

l,k′ exp(i, lk′)ul
k′
l,k′ exp(i, lk′)ul

=

log

(cid:0) P
k exp(i, jk)

h
+

k ¯pijkuj
(cid:0) P

k − pij

(cid:1) P

k − log

(cid:1) P
k ¯pijkuj
l,k′ ¯pijk′ ul
k′
l,k′ ¯pijk′ ul

k′ + pijlog

l,k′ exp(i, lk′)

(cid:0) P

l,k′ exp(i, lk′)
k ¯pijkuj

k

(cid:1) P

l,k′ exp(i, lk′)ul
k′

(28)

i

(cid:0) P

(cid:1) P

− pij log
P

=

(cid:0) P
log(pij) + 1

=

log(pij) + 1
(cid:0)

P

k exp(i, jk)
k ¯pijkuj
(cid:1) P
k −
k ¯pijkuj
k − pij
(cid:0)

(cid:1) P

log(pij) + 1

pij
l,k′ ¯pijk′ ul
(cid:1)
P
k′

k′ + pijlog

l,k′ exp(i, lk′)

l,k′ ¯pijk′ ul
k′

l,k′ ¯pijk′ ul
(cid:0) P
k′

(cid:1) P

i

The derivative of H w.r.t. target output vq is given by,
P

(cid:1)(cid:0) P

(cid:0)

(cid:1)

∂H
∂vq

= −

1
nt

C

j=1
X

(cid:0)

log(pqj) + 1

k ¯pqjkuj

k − pqj

l,k′ ¯pqjk′ ul
k′

The derivative of H w.r.t. the source outputs is given by 25 and w.r.t. the target outputs is given by 32.

(cid:1)(cid:0) P

P

(cid:1)

8. Unsupervised Domain Adaptation: Additional Results

(29)

(30)

(31)

(32)

In the main paper we had presented results for unsupervised domain adaptation based object recognition with d = 64 bits.
Here, we outline the classiﬁcation results with d = 16 (DAH-16) and d = 128 (DAH-128) bits for the Ofﬁce-Home dataset
in Table 5. We also present the (DAH-64), DAN and DANN results for comparison. There is an increase in the average
recognition accuracy for d = 128 bits compared to d = 64 bits because of the increased capacity in representation. As
expected, d = 16 has a lower recognition accuracy.
Table 5: Recognition accuracies (%) for domain adaptation experiments on the Ofﬁce-Home dataset. {Art (Ar), Clipart (Cl),
Product (Pr), Real-World (Rw)}. Ar→Cl implies Ar is source and Cl is target.

Expt.
DAN
DANN
DAH-16
DAH-64
DAH-128

Ar→Cl Ar→Pr Ar→Rw Cl→Ar Cl→Pr Cl→Rw Pr→Ar Pr→Cl Pr→Rw Rw→Ar Rw→Cl Rw→Pr Avg.
43.46
29.07
30.66
44.94
30.49
33.33
31.36
20.11
23.83
45.54
29.91
31.64
46.26
30.94
32.58

42.17
42.96
30.32
40.75
40.64

54.13
54.42
40.14
51.73
52.40

62.73
64.65
37.46
62.54
64.97

43.58
44.71
32.63
44.99
45.65

32.83
32.26
25.67
34.69
35.72

34.05
38.14
27.72
39.63
41.31

47.59
49.13
38.79
51.93
52.80

49.78
49.76
33.26
52.79
52.12

56.70
56.76
40.90
60.71
59.31

38.25
42.66
25.54
45.13
46.67

9. Unsupervised Domain Adaptive Hashing: Additional Results

We provide the unsupervised domain adaptive hashing results for d = 16 and d = 128 bits in Figures 6 and 7 respectively.
In Tables 6 and 7, we outline the corresponding mAP values. The notations are along the lines outlined in the main paper. We
observe similar trends for both d = 16 and d = 128 bits compared to d = 64 bits. It is interesting to note that with increase
in bit size d, the mAP does not necessarily increase. Table 7 (d = 64) has its mAP values lower than those for d = 64 (see
main paper) for all the hashing methods. This indicates that merely increasing the hash code length does not always improve
mAP scores. Also, the mAP values for Real-World for d = 128 bits has DAH performing better than SuH. This indicates
that in some cases domain adaptation helps in learning a better generalized model.

Table 6: Mean average precision @16 bits. For the NoDA and DAH results, Art is the source domain for Clipart, Product and
Real-World and Clipart is the source domain for Art.

Table 7: Mean average precision @128 bits. For the NoDA and DAH results, Art is the source domain for Clipart, Product and
Real-World and Clipart is the source domain for Art.

Expt.
Art
Clipart
Product
Real-World
Avg.

NoDA ITQ KMeans
0.147
0.102
0.120
0.110
0.253
0.134
0.225
0.193
0.186
0.135

0.133
0.116
0.241
0.195
0.171

BA
0.131
0.123
0.253
0.216
0.181

BDNN DAH SuH
0.381
0.207
0.151
0.412
0.211
0.138
0.459
0.257
0.313
0.400
0.371
0.248
0.413
0.262
0.212

Expt.
Art
Clipart
Product
Real-World
Avg.

NoDA ITQ KMeans
0.202
0.154
0.210
0.186
0.416
0.279
0.343
0.308
0.293
0.232

0.175
0.196
0.356
0.289
0.254

BA
0.148
0.187
0.336
0.258
0.232

BDNN DAH SuH
0.444
0.314
0.207
0.346
0.350
0.213
0.792
0.424
0.432
0.458
0.544
0.348
0.510
0.408
0.300

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.5
Recall
 

(a) Art

0.5
Recall
 

(b) Clipart

0.5
Recall
 

(c) Product

0.5
Recall
 

(d) Real-World

Figure 6: Precision-Recall curves @16 bits for the Ofﬁce-Home dataset. Comparison of hashing without domain adaptation (NoDA),
shallow unsupervised hashing (ITQ, KMeans), state-of-the-art deep unsupervised hashing (BA, BDNN), unsupervised domain adaptive
hashing (DAH) and supervised hashing (SuH). Best viewed in color.

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

n
o
i
s
i
c
e
r
P

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

n
o
i
s
i
c
e
r
P

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

n
o
i
s
i
c
e
r
P

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.5
Recall
 

(a) Art

0.5
Recall
 

(b) Clipart

0.5
Recall
 

(c) Product

0.5
Recall
 

(d) Real-World

Figure 7: Precision-Recall curves @128 bits for the Ofﬁce-Home dataset. Comparison of hashing without domain adaptation (NoDA),
shallow unsupervised hashing (ITQ, KMeans), state-of-the-art deep unsupervised hashing (BA, BDNN), unsupervised domain adaptive
hashing (DAH) and supervised hashing (SuH). Best viewed in color.

7
1
0
2
 
n
u
J
 
2
2
 
 
]

V
C
.
s
c
[
 
 
1
v
2
2
5
7
0
.
6
0
7
1
:
v
i
X
r
a

Deep Hashing Network for Unsupervised Domain Adaptation

Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, Sethuraman Panchanathan
Center for Cognitive Ubiquitous Computing, Arizona State University, Tempe, AZ, USA
{hemanthv, jeusebio, shayok.chakraborty, panch}@asu.edu

Abstract

In recent years, deep neural networks have emerged as a
dominant machine learning tool for a wide variety of appli-
cation domains. However, training a deep neural network
requires a large amount of labeled data, which is an expen-
sive process in terms of time, labor and human expertise.
Domain adaptation or transfer learning algorithms address
this challenge by leveraging labeled data in a different, but
related source domain, to develop a model for the target
domain. Further, the explosive growth of digital data has
posed a fundamental challenge concerning its storage and
retrieval. Due to its storage and retrieval efﬁciency, recent
years have witnessed a wide application of hashing in a
variety of computer vision applications. In this paper, we
ﬁrst introduce a new dataset, Ofﬁce-Home, to evaluate do-
main adaptation algorithms. The dataset contains images
of a variety of everyday objects from multiple domains. We
then propose a novel deep learning framework that can ex-
ploit labeled source data and unlabeled target data to learn
informative hash codes, to accurately classify unseen tar-
get data. To the best of our knowledge, this is the ﬁrst
research effort to exploit the feature learning capabilities
of deep neural networks to learn representative hash codes
to address the domain adaptation problem. Our extensive
empirical studies on multiple transfer tasks corroborate the
usefulness of the framework in learning efﬁcient hash codes
which outperform existing competitive baselines for unsu-
pervised domain adaptation.

1. Introduction

Deep learning algorithms automatically learn a discrim-
inating set of features and have depicted commendable per-
formance in a variety of computer vision applications. Un-
fortunately, training a deep model necessitates a large vol-
ume of labeled data, which can be time consuming and ex-
pensive to acquire. However, labeled data from a differ-
ent, but related domain is often available, which has mo-
tivated the development of algorithms which can leverage

labeled data in a source domain to develop a machine learn-
ing model for the target domain. Learning a discrimina-
tive model in the presence of the shift between training and
test distributions is known as transfer learning or domain
adaptation [17]. Unsupervised domain adaptation is a chal-
lenging setting, where labeled data is available only in the
source domain; no labeled data is available in the target
domain. Conventional shallow transfer learning methods
develop their models in two stages, feature extraction fol-
lowed by domain adaptation. The features are ﬁxed and
then a model is trained to align the source and target do-
mains [16, 20, 33, 38, 42, 43, 44]. On the other hand, deep
transfer learning procedures exploit the feature learning ca-
pabilities of deep networks to learn transferable feature rep-
resentations for domain adaptation and have demonstrated
impressive empirical performance [17, 18, 31, 34, 46].

The explosive growth of digital data in the modern era
has posed fundamental challenges regarding their storage,
retrieval and computational requirements. Against this
backdrop, hashing has emerged as one of the most popu-
lar and effective techniques due to its fast query speed and
low memory cost [48]. Hashing techniques transform high
dimensional data into compact binary codes and generate
similar binary codes for similar data items. Motivated by
this fact, we propose to train a deep neural network to out-
put binary hash codes (instead of probability values), which
can be used for classiﬁcation. We see two advantages to es-
timating a hash value instead of a standard probability vec-
tor in the ﬁnal layer of the network: (i) the hash values are
used to develop a unique loss function for target data in the
absence of labels and (ii) during prediction, the hash value
of a test sample can be compared against the hash values
of the training samples to arrive at a more robust category
prediction.

In this paper, we ﬁrst introduce a new dataset, Ofﬁce-
Home, which we use to evaluate our algorithm. The Ofﬁce-
Home dataset is an object recognition dataset which con-
tains images from 4 domains. It has around 15, 500 images
organized into 65 categories. We further propose a novel
deep learning framework called Domain Adaptive Hash-

1

ing (DAH) to learn informative hash codes to address the
problem of unsupervised domain adaptation. We propose
a unique loss function to train the deep network with the
following components: (i) supervised hash loss for labeled
source data, which ensures that source samples belonging
to the same class have similar hash codes; (ii) unsuper-
vised entropy loss for unlabeled target data, which imposes
each target sample to align closely with exactly one of the
source categories and be distinct from the other categories
and (iii) a loss based on multi-kernel Maximum Mean Dis-
crepancy (MK-MMD), which seeks to learn transferable
features within the layers of the network to minimize the
distribution difference between the source and target do-
mains. Figure 1 illustrates the different layers of the DAH
and the components of the loss function.

2. Related Work

There have been many approaches to address the prob-
lem of domain-shift in unsupervised domain adaptation.
One straightforward approach is,
to modify a classiﬁer
trained for the source data by adapting it to classify target
data [1, 4] or learn a transformation matrix to linearly trans-
form the source data, so that it is aligned with the target
[27, 42]. Some other procedures re-weight the data points
in the source domain, to select source data that is similar
to the target, when training a domain adaptive classiﬁer,
[9, 10, 19]. A standard procedure to reduce domain discrep-
ancy is, to project the source and target data to a common
subspace, thereby aligning their principal axes [16, 44].
Reducing domain disparity through nonlinear alignment of
data has been possible with Maximum Mean Discrepancy
(MMD) - a measure that provides the distribution differ-
ence between two datasets in a reproducing-kernel Hilbert
space [13]. Kernel-PCA based methods apply the MMD to
achieve nonlinear alignment of domains [32, 33, 38]. Man-
ifold based approaches are also popular in domain adapta-
tion for computer vision, where the subspace of a domain is
treated as a point on the manifold and transformations are
learned to align two domains [20, 23]. A survey of popular
domain adaptation techniques for computer vision is pro-
vided in [41] and a more generic survey of transfer learning
approaches can be found in [39].

All of the above techniques can be termed as shallow
learning procedures, since the models are learned using pre-
determined features. In recent years deep learning has be-
come very successful at learning highly discriminative fea-
tures for computer vision applications [8]. Deep learning
systems like deep CNNs learn representations of data that
capture underlying factors of variation between different
tasks in a multi-task transfer learning setting [3]. These rep-
resentations also disentangle the factors of variation allow-
ing for the transfer of knowledge between tasks [12, 18, 37].
Yosinski et al. [49] demonstrated how the lower layers of a

network produce generic features and the upper layers out-
put task speciﬁc features. Based on this, deep learning pro-
cedures for domain adaptation train networks to learn trans-
ferable features in the fully connected ﬁnal layers of a net-
work [31, 46]. In other approaches to deep domain adapta-
tion, Ganin et al. [17] trained domain adversarial networks
to learn features that make the source and target domain in-
distinguishable and Long et al. [34], trained a network to
do both feature adaptation and classiﬁer adaptation using
residual transfer networks.

Unsupervised hashing techniques have been developed
to extract unique hash codes for efﬁcient storage and re-
trieval of data [22, 25]. Neural network based hashing has
led the way in state-of-the-art unsupervised hashing tech-
niques [7, 11, 14]. The closest work incorporating hash-
ing and adaptation appears in cross-modal hashing, where
deep hashing techniques embed multi-modal data and learn
hash codes for two related domains, like text and images
[5, 6, 29]. However, these algorithms are not unsupervised
and they are mainly applied to extract common hash codes
for multi-modal data for retrieval purposes. To the best of
our knowledge, there has been no work in unsupervised
domain adaptation using deep hashing networks. We now
present the Domain Adaptive Hashing (DAH) network for
unsupervised domain adaptation through deep hashing.

3. Domain Adaptive Hashing Networks

i }ns

In unsupervised domain adaptation, we consider data
from two domains; source and target. The source consists
i , ys
of labeled data, Ds = {xs
i=1 and the target has only
i}nt
i=1. The data points x∗
unlabeled data Dt = {xt
i belong to
X, where X is some input space. The corresponding labels
are represented by y∗
i ∈ Y := {1, . . . , C}. The paradigm of
domain adaptive learning attempts to address the problem of
domain-shift in the data, where the data distributions of the
source and target are different, i.e. Ps(X, Y ) 6= Pt(X, Y ).
The domain-shift notwithstanding, our goal is to train a
deep neural network classiﬁer ψ(.), that can predict the la-
bels {ˆyt

i=1, for the target data.

i }nt

We implement the neural network as a deep CNN which
consists of 5 convolution layers conv1 - conv5 and 3 fully
connected layers fc6 - fc8 followed by a loss layer. In our
model, we introduce a hashing layer hash-fc8 in place of
the standard fc8 layer to learn a binary code hi, for every
data point xi, where hi ∈ {−1, +1}d. The hash-fc8 layer
is driven by two loss functions, (i) supervised hash loss for
the source data, (ii) unsupervised entropy loss for the target
data. The supervised hash loss ensures hash values that are
distinct and discriminatory, i.e. if xi and xj belong to the
same category, their hash values hi and hj are similar and
different otherwise. The unsupervised entropy loss aligns
the target hash values with source hash values based on the
similarity of their feature representations. The output of the

is the output representation of x∗

layer l, where u∗,l
i for the
i
lth layer. The ﬁnal layer outputs are denoted as Us and U t.
The MK-MMD measure d2
k(.) is the multi-kernel maximum
mean discrepancy between the source and target representa-
tions, [24]. For a nonlinear mapping φ(.) associated with a
reproducing kernel Hilbert space Hk and kernel k(.), where
k(x, y) = hφ(x), φ(y)i, the MMD is deﬁned as,

2

.

Hk

(2)

d2
k(U

l
s, U

l
t) =

E[φ(us,l)] − E[φ(ut,l)]
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
The characteristic kernel k(.), is determined as a convex
(cid:12)
combination of κ PSD kernels, {km}κ
m=1, K :=
k : k =
κ
. We set
m=1 βm = 1, βm ≥ 0, ∀m
(cid:8)
βm = 1/κ according to [34] and it works well in practice.
P
3.2. Supervised Hashing for Source Data

κ
m=1 βmkm,

P

(cid:12)
(cid:12)
(cid:12)

(cid:9)

2 (d − h⊤

The Hamming distance for a pair of hash values hi and
hj has a unique relationship with the dot product hhi, hj i,
given by: distH (hi, hj) = 1
i hj), where d is the
hash length. The dot product hhi, hji can be treated as
a similarity measure for the hash codes. Larger the value
of the dot product (high similarity), smaller is the distance
distH and smaller the dot product (low similarity), larger is
the distance distH . Let sij ∈ {0, 1} be the similarity be-
tween xi and xj. If xi and xj belong to the same category,
sij = 1 and 0, otherwise. The probability of similarity be-
tween xi and xj given the corresponding hash values hi
and hj, can be expressed as a likelihood function, given by,

p(sij |hi, hj) =

σ(h⊤
i hj),
1 − σ(h⊤

(

i hj),

sij = 1
sij = 0,

(3)

1

1+e−x is the sigmoid function. As the
where, σ(x) =
dot product hhi, hji increases, the probability of p(sij =
1|hi, hj ) also increases, i.e., xi and xj belong to the same
category. As the dot product decreases, the probability
p(sij = 1|hi, hj) also decreases, i.e., xi and xj belong
to different categories. We construct the (ns × ns) similar-
ity matrix S = {sij}, for the source data with the provided
labels, where sij = 1 if xi and xj belong to the same cat-
egory and 0, otherwise. Let H = {hi}ns
i=1 be the set of
source data hash values. If the elements of H are assumed
to be i.i.d., the negative log likelihood of the similarity ma-
trix S given H can be written as,

L(H) = −log p(S|H)

min
H

= −

sij h⊤

i hj − log

1 + exp(h⊤

i hj )

.

Xsij ∈S (cid:16)

(cid:0)

(cid:1)(cid:17)
(4)

Figure 1: The Domain Adaptive Hash (DAH) network that out-
puts hash codes for the source and the target. The network is
trained with a batch of source and target data. The convolution
layers conv1 - conv5 and the fully connected layers fc6 and fc7 are
ﬁne tuned from the VGG-F network. The MK-MMD loss trains
the DAH to learn feature representations which align the source
and the target. The hash-fc8 layer is trained to output vectors of d
dimensions. The supervised hash loss drives the DAH to estimate
a unique hash value for each object category. The unsupervised
entropy loss aligns the target hash values to their corresponding
source categories. Best viewed in color.
network is represented as ψ(x), where ψ(x) ∈ Rd, which
we convert to a hash code h = sgn(ψ(x)), where sgn(.)
is the sign function. Once the network has been trained,
the probability of x being assigned a label y is given by
f (x) = p(y|h). We train the network using Ds and Dt and
predict the target data labels ˆyt

∗ using f (.).

In order to address the issue of domain-shift, we need to
align the feature representations of the target and the source.
We do that by reducing the domain discrepancy between the
source and target feature representations at multiple layers
of the network. In the following subsections, we discuss
the design of the domain adaptive hash (DAH) network in
detail.

3.1. Reducing Domain Disparity

Deep learning methods have been very successful in do-
main adaptation with state-of-the-art algorithms [17, 31, 34,
46] in recent years. The feature representations transition
from generic to task-speciﬁc as one goes up the layers of
a deep CNN [49]. The convolution layers conv1 to conv5
have been shown to be generic and so, readily transferable,
whereas the fully connected layers are more task-speciﬁc
and need to be adapted before they can be transferred. In
the DAH algorithm, we attempt to minimize the MK-MMD
loss to reduce the domain difference between the source
and target feature representations for fully connected lay-
ers, F = {fc6, fc7, fc8}. Such a loss function has been used
in previous research [31, 34]. The multi-layer MK-MMD
loss is given by,

M(U s, U t) =

d2
k(U

l
s, U

l
t),

(1)

l

where, U
i=1 are the set
of output representations for the source and target data at

s = {us,l

i }ns

t = {ut,l

i }nt

By minimizing Equation (4), we can determine hash val-
ues H for the source data which are consistent with the

Xl∈F
l
i=1 and U

similarity matrix S. The hash loss has been used in pre-
vious research for supervised hashing [30, 50]. Equation
(4) is a discrete optimization problem that is challenging to
solve. We introduce a relaxation on the discrete constraint
hi ∈ {−1, +1}d by instead solving for ui ∈ Rd, where
Us = {ui}ns
i=1 is the output of the network and ui = ψ(xi)
(the superscript denoting the domain has been dropped for
ease of representation). However, the continuous relaxation
gives rise to (i) approximation error, when hhi, hj i is sub-
stituted with hui, uji and, (ii) quantization error, when the
resulting real codes ui are binarized [50]. We account for
the approximation error by having a tanh(.) as the ﬁnal ac-
tivation layer of the neural network, so that the components
of ui are bounded between −1 and +1. In addition, we also
introduce a quantization loss ||ui − sgn(ui)||2
2 along the
lines of [22], where sgn(.) is the sign function. The contin-
uous optimization problem for supervised hashing can now
be outlined;

min
U s

L(U s) = −

sij u⊤

i uj − log

1 + exp(u⊤

i uj)

+

ui − sgn(ui)

Xsij ∈S (cid:16)
ns

i=1
X

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:0)
2
2.

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:1)(cid:17)
(5)

3.3. Unsupervised Hashing for Target Data

k=1

{usj

k }K

In the absence of target data labels, we use the similarity
measure hui, uji, to guide the network to learn discrimina-
tive hash values for the target data. An ideal target output
ut
i, needs to be similar to many of the source outputs from
the jth category
. We assume without loss of
generality, K source data points for every category j where,
(cid:1)
j ∈ {1, . . . , C} and usj
k is the kth source output from cat-
egory j. In addition, ut
i must be dissimilar to most other
source outputs usl
k belonging to a different category (j 6= l).
Enforcing similarity with all the K data points makes for a
more robust target data category assignment. We outline
a probability measure to capture this intuition. Let pij be
the probability that input target data point xi is assigned to
category j where,

(cid:0)

pij =

K
k=1 exp(ut
i
K
k=1 exp(ut
i

⊤usj
k )
⊤usl
k )

C
P
l=1

(6)

P

P

The exp(.) has been introduced for ease of differentiabil-
j pij = 1. When the
ity and the denominator ensures
target data point output is similar to one category only and
dissimilar to all the other categories, the probability vec-
tor pi = [pi1, . . . , piC ]T tends to be a one-hot vector. A
one-hot vector can be viewed as a low entropy realization
of pi. We can therefore envisage all the pi to be one-hot
vectors (low entropy probability vectors), where the target
data point outputs are similar to source data point outputs in
one and only one category. To this end we introduce a loss

P

to capture the entropy of the target probability vectors. The
entropy loss for the network outputs is given by,

H(Us, Ut) = −

pijlog(pij)

(7)

1
nt

nt

C

i=1
X

j=1
X

Minimizing the entropy loss gives us probability vectors pi
that tend to be one-hot vectors, i.e., the target data point
outputs are similar to source data point outputs from any
one category only. Enforcing similarity with K source data
points from a category, guarantees that the hash values are
determined based on a common similarity between multiple
source category data points and the target data point.

3.4. Domain Adaptive Hash Network

We propose a model for deep unsupervised domain adap-
tation based on hashing (DAH) that incorporates unsuper-
vised domain adaptation between the source and the target
(1), supervised hashing for the source (5) and unsupervised
hashing for the target (7) in a deep convolutional neural net-
work. The DAH network is trained to minimize

min
U

J = L(U s) + γM(Us, U t) + ηH(U s, U t),

(8)

where, U := {U s ∪ Ut} and (γ, η) control the importance
of domain adaptation (1) and target entropy loss (7) respec-
tively. The hash values H are obtained from the output of
the network using H = sgn(U). The loss terms (5) and
(7) are determined in the ﬁnal layer of the network with the
network output U. The MK-MMD loss (1) is determined
l
l
between layer outputs {U
t} at each of the fully con-
s, U
nected layers F = {fc6, fc7, fc8}, where we adopt the lin-
ear time estimate for the unbiased MK-MMD as described
in [24] and [31]. The DAH is trained using standard back-
propagation. The detailed derivation of the derivative of (8)
w.r.t. U is provided in the supplementary material.
Network Architecture: Owing to the paucity of images
in a domain adaptation setting, we circumvent the need to
train a deep CNN with millions of images by adapting the
pre-trained VGG-F [8] network to the DAH. The VGG-F
has been trained on the ImageNet 2012 dataset and it con-
sists of 5 convolution layers (conv1 - conv5) and 3 fully
connected layers (fc6, fc7, fc8). We introduce the hashing
layer hash-fc8 that outputs vectors in Rd in the place of fc8.
To account for the hashing approximation, we introduced
a tanh() layer. However, we encounter the issue of van-
ishing gradients [26] when using tanh() as it saturates with
large inputs. We therefore preface the tanh() with a batch
normalization layer which prevents the tanh() from saturat-
ing. In effect, hash-fc8 := {fc8 → batch-norm → tanh()}.
The hash-fc8 provides greater stability when ﬁne-tuning the
learning rates than the deep hashing networks [30, 50]. Fig-
ure 1 illustrates the proposed DAH network.

Table 1: Statistics for the Ofﬁce-Home dataset. Min: # is the
minimum number of images amongst all the categories, Min: Size
and Max: Size are the minimum and maximum image sizes across
all categories and Acc. is the classiﬁcation accuracy.

Domain.
Art
Clipart
Product
Real-World

Min: # Min: Size
117×85 pix.
18×18 pix.
75×63 pix.
88×80 pix.

15
39
38
23

Max: Size
4384×2686 pix.
2400×2400 pix.
2560×2560 pix.
6500×4900 pix.

Acc
44.99±1.85
53.95±1.45
66.41±1.18
59.70±1.04

4. The Ofﬁce-Home Dataset

Supervised deep learning models require a large volume
of labeled training data. Unfortunately, existing datasets
for vision-based domain adaptation are limited in their
size and are not suitable for validating deep learning al-
gorithms. The standard datasets for vision based domain
adaptation are, facial expression datasets CKPlus [35] and
MMI [40], digit datasets SVHN [36], USPS and MNIST[28],
head pose recognition datasets PIE [33], object recogni-
tion datasets COIL[33], Ofﬁce [42] and Ofﬁce-Caltech [20].
These datasets were created before deep-learning became
popular and are insufﬁcient for training and evaluating deep
learning based domain adaptation approaches. For instance,
the object-recognition dataset Ofﬁce has 4110 images across
31 categories and Ofﬁce-Caltech has 2533 images across 10
categories.

We release the Ofﬁce-Home dataset for domain adap-
tation based object recognition, that can be used to evalu-
ate deep learning algorithms for domain adaptation. The
Ofﬁce-Home dataset consists of 4 domains, with each do-
main containing images from 65 categories of everyday ob-
jects and a total of around 15, 500 images. The domains
include, Art: artistic depictions of objects in the form of
sketches, paintings, ornamentation, etc.; Clipart: collec-
tion of clipart images; Product: images of objects with-
out a background, akin to the Amazon category in Ofﬁce
dataset; Real-World: images of objects captured with a
regular camera.

Public domain images were downloaded from web-
sites like www.deviantart.com and www.ﬂickr.com to cre-
ate the Art and Real-World domains. Clipart im-
ages were gathered from multiple clipart websites. The
Product domain images were exclusively collected from
www.amazon.com using web-crawlers. The collected im-
ages were manually ﬁltered on the basis of quality, size and
content. The dataset has an average of around 70 images
per category and a maximum of 99 images in a category.
The primary challenge in creating this dataset was acquir-
ing sufﬁcient number of public domain images across all
the 4 domains. Figure 2 depicts a sampling of 16 categories
from the Ofﬁce-Home dataset and Table 1 outlines some
meta data for the dataset. The Acc. column in the Table
1 refers to classiﬁcation accuracies using the LIBLINEAR
SVM [15] classiﬁer (5-fold cross validation) with deep fea-

tures extracted using the VGG-F network. The dataset is
publicly available for research 1.

5. Experiments

In this section we conduct extensive experiments to
evaluate the DAH algorithm. Since we propose a do-
main adaptation technique based on hashing, we evalu-
ate objection recognition accuracies for unsupervised do-
main adaptation and also study the discriminatory capabil-
ity of the learned hash codes for unsupervised domain adap-
tive hashing. The implementation details are available at
https://github.com/hemanthdv/da-hash

5.1. Datasets

Ofﬁce [42]: This is currently the most popular benchmark
dataset for object recognition in the domain adaptation com-
puter vision community. The dataset consists of images of
everyday objects in an ofﬁce environment. It has 3 domains;
Amazon (A), Dslr (D) and Webcam (W). The dataset has
around 4, 100 images with a majority of the images (2816
images) in the Amazon domain. We adopt the common
evaluation protocol of different pairs of transfer tasks for
this dataset [31, 34]. We consider 6 transfer tasks for all
combinations of source and target pairs for the 3 domains.
Ofﬁce-Home: We introduce this new dataset and evaluate
it in a similar manner to the Ofﬁce dataset. We consider 12
transfer tasks for the Art (Ar), Clipart (Cl), Product
(Pr) and Real-World (Rw) domains for all combinations
of source and target for the 4 domains. Considering all the
different pairs of transfer enables us to evaluate the inherent
bias between the domains in a comprehensive manner [45].

5.2. Implementation Details

We implement the DAH using the MatConvnet frame-
work [47]. Since we train a pre-trained VGG-F, we ﬁne-
tune the weights of conv1-conv5, fc6 and fc7. We set
their learning rates to 1/10th the learning rate of hash-fc8.
We vary the learning rate between 10−4 to 10−5 over 300
epochs with a momentum 0.9 and weight decay 5 × 10−4.
We set K = 5 (number of samples from a category). Since
we have 31 categories in the Ofﬁce dataset, we get a source
batch size of 31 × 5 = 155. For the target batch, we ran-
domly select 155 samples. The total batch size turns out to
be 310. For the Ofﬁce-Home dataset, with K = 5 and 65
categories, we get a batch size of 650. We set d = 64 (hash
code length) for all our experiments. Since there is imbal-
ance in the number of like and unlike pairs in S, we set the
values in similarity matrix Si,j ∈ {0, 10}. Increasing the
similarity weight of like-pairs improves the performance of
DAH. For the entropy loss, we set η = 1. For the MK-
MMD loss, we follow the heuristics mentioned in [24], to

1https://hemanthdv.github.io/officehome-dataset/

Figure 2: Sample images from the Ofﬁce-Home dataset. The dataset consists of images of everyday objects organized into 4 domains;
Art: paintings, sketches and/or artistic depictions, Clipart: clipart images, Product: images without background and Real-World:
regular images captured with a camera. The ﬁgure displays examples from 16 of the 65 categories.

determine the parameters. We estimate γ, by validating a
binary domain classiﬁer to distinguish between source and
target data points and select γ which gives largest error on a
validation set. For MMD, we use a Gaussian kernel with a
bandwidth σ given by the median of the pairwise distances
in the training data. To incorporate the multi-kernel, we
vary the bandwidth σm ∈ [2−8σ, 28σ] with a multiplicative
factor of 2. We deﬁne the target classiﬁer f (xt
i) = p(y|ht
i)
in terms of 6. The target data point is assigned to the class
with the largest probability, with ˆyi = maxj(pij ) using the
hash codes for the source and the target.

5.3. Unsupervised Domain Adaptation

In this section, we study the performance of the DAH
for unsupervised domain adaptation, where labeled data is
available only in the source domain and no labeled data is
available in the target domain. We compare the DAH with
state-of-the-art domain adaptation methods: (i) Geodesic
Flow Kernel (GFK) [20], (ii) Transfer Component Analy-
sis (TCA) [38], (iii) Correlation Alignment (CORAL) [44]
and (iv) Joint Distribution Adaptation (JDA) [33]. We also
compare the DAH with state-of-the-art deep learning meth-
ods for domain adaptation: (v) Deep Adaptation Network
(DAN) [31] and (vi) Domain Adversarial Neural Network
(DANN) [17]. For all of the shallow learning methods,
we extract and use deep features from the fc7 layer of the
VGG-F network that was pre-trained on the ImageNet 2012
dataset. We also evaluate the effect of the entropy loss on
hashing for the DAH. The DAH-e is the DAH algorithm
where η is set to zero, which implies that the target hash
values are not driven to align with the source categories.
We follow the standard protocol for unsupervised domain
adaptation, where all the labeled source data and all the un-
labeled target data is used for training.
Results and Discussion: The results are reported for the
target classiﬁcation in each of the transfer tasks in Tables 2
and 3, where accuracies denote the percentage of correctly

Table 2: Recognition accuracies (%) for domain adaptation exper-
iments on the Ofﬁce dataset. {Amazon (A), Dslr (D), Webcam
(W)}. A→W implies A is source and W is target.

Expt.
GFK
TCA
CORAL
JDA
DAN
DANN
DAH-e
DAH

A→D A→W D→A D→W W→A W→D
93.17
48.59
96.79
51.00
98.59
54.42
97.79
59.24
99.40
67.04
99.40
72.89
96.99
66.27
98.80
66.47

52.08
49.43
51.70
58.62
67.80
72.70
66.16
68.30

41.83
48.12
48.26
51.35
50.36
56.25
55.97
55.54

89.18
93.08
95.97
96.86
95.85
96.48
94.59
96.10

49.04
48.83
47.27
52.34
52.33
53.20
53.91
53.02

Avg.
62.32
64.54
66.04
69.37
72.13
75.15
72.31
73.04

classiﬁed target data samples. We present results with hash
length d = 64 bits. The DAH algorithm consistently out-
performs the baselines across all the domains for the Ofﬁce-
Home dataset. However, DANN marginally surpasses DAH
for the Ofﬁce dataset, prompting us to reason that domain
adversarial training is more effective than DAH when the
categories are fewer in number. Since domain alignment is
category agnostic, it is possible that the aligned domains are
not classiﬁcation friendly in the presence of large number
of categories. When the number of categories is large, as in
Ofﬁce-Home, DAH does best at extracting transferable fea-
tures to achieve higher accuracies. We also note that DAH
delivers better performance than DAH-e; thus, minimizing
the entropy on the target data through 7 aids in improved
alignment of the source and target samples, which boosts
the accuracy.
Feature Analysis: We also study the feature representa-
tions of the penultimate layer (fc7) outputs using t-SNE em-
beddings as in [12]. Figure 3a depicts the A-distance be-
tween domain pairs using Deep (VGG-F), DAN and DAH
[2] deﬁned A-distance as the
features. Ben-David et al.
distance between two domains that can be viewed as the
discrepancy between two domains. Although it is difﬁcult
to estimate its exact value, an approximate distance mea-
sure is given by 2(1 − 2ǫ), where ǫ is the generalization
error for a binary classiﬁer trained to distinguish between
the two domains. We used a LIBLINEAR SVM [15] clas-

Table 3: Recognition accuracies (%) for domain adaptation experiments on the Ofﬁce-Home dataset. {Art (Ar), Clipart (Cl),
Product (Pr), Real-World (Rw)}. Ar→Cl implies Ar is source and Cl is target.

Expt.
GFK
TCA
CORAL
JDA
DAN
DANN
DAH-e
DAH

Ar→Cl Ar→Pr Ar→Rw Cl→Ar Cl→Pr Cl→Rw Pr→Ar Pr→Cl Pr→Rw Rw→Ar Rw→Cl Rw→Pr Avg.
32.40
24.52
21.60
30.34
21.92
19.93
37.91
27.77
27.10
36.97
25.96
25.34
43.46
29.07
30.66
44.94
30.49
33.33
42.69
29.87
29.23
45.54
29.91
31.64

34.20
31.74
40.33
40.90
49.78
49.76
47.49
52.79

34.94
31.36
40.03
40.19
47.59
49.13
48.23
51.93

21.63
19.00
26.08
24.52
32.83
32.26
33.79
34.69

42.92
42.12
50.61
49.25
56.70
56.76
55.63
60.71

25.73
23.64
30.54
32.72
34.05
38.14
38.76
39.63

50.89
48.68
57.11
55.35
62.73
64.65
59.07
62.54

32.88
30.74
38.48
35.10
43.58
44.71
41.16
44.99

38.83
35.71
44.32
42.94
54.13
54.42
48.29
51.73

31.72
32.08
36.16
35.98
42.17
42.96
35.71
40.75

28.96
27.15
36.36
35.35
38.25
42.66
44.99
45.13

siﬁer with 5-fold cross-validation to estimate ǫ. Figure 3a
indicates that the DAH features have the least discrepancy
between the source and target compared to DAN and Deep
features. This is also conﬁrmed with the t-SNE embeddings
in Figures 3b-3d. The Deep features show very little over-
lap between the domains and the categories depict minimal
clustering. Domain overlap and clustering improves as we
move to DAN and DAH features, with DAH providing the
best visualizations. This corroborates the efﬁcacy of the
DAH algorithm to exploit the feature learning capabilities
of deep neural networks to learn representative hash codes
to address domain adaptation.

5.4. Unsupervised Domain Adaptive Hashing

In this section, we study the performance of our algo-
rithm to generate compact and efﬁcient hash codes from the
data for classifying unseen test instances, when no labels
are available. This problem has been addressed in the litera-
ture, with promising empirical results [7, 11, 21]. However,
in a real-world setting, labels may be available from a dif-
ferent, but related (source) domain; a strategy to utilize the
labeled data from the source domain to learn representative
hash codes for the target domain is therefore of immense
practical importance. Our work is the ﬁrst to identify and
address this problem. We consider the following scenar-
ios to address this real-world challenge: (i) No labels are
available for a given dataset and the hash codes need to be
learned in a completely unsupervised manner. We evaluate
against baseline unsupervised hashing methods (ITQ) [22]
and (KMeans) [25] and also state-of-the-art methods for
unsupervised hashing (BA) [7] and (BDNN) [11]. (ii) La-
beled data is available from a different, but related source
domain. A hashing model is trained on the labeled source
data and is used to learn hash codes for the target data. We
refer to this method as NoDA, as no domain adaptation is
performed. We used the deep pairwise-supervised hashing
(DPSH) algorithm [30] to train a deep network with the
source data and applied the network to generate hash codes
for the target data.
(iii) Labeled data is available from a
different, but related source domain and we use our DAH
formulation to learn hash codes for the target domain by
(iv) Labeled data is available
reducing domain disparity.

Table 4: Mean average precision @64 bits. For the NoDA and
DAH results, Art is the source domain for Clipart, Product
and Real-World and Clipart is the source domain for Art.
Similarly, Amazon and Webcam are source target pairs.

Expt.
Amazon
Webcam
Art
Clipart
Product
Real-World
Avg.

NoDA ITQ KMeans
0.465
0.324
0.652
0.511
0.191
0.155
0.195
0.160
0.393
0.239
0.323
0.281
0.370
0.278

0.403
0.558
0.170
0.178
0.341
0.279
0.322

BA
0.367
0.480
0.156
0.179
0.349
0.273
0.301

BDNN DAH SuH
0.830
0.582
0.491
0.939
0.717
0.656
0.492
0.302
0.193
0.622
0.333
0.206
0.774
0.414
0.407
0.586
0.533
0.336
0.707
0.480
0.382

in the target domain. This method falls under supervised
hashing (SuH) (as it uses labeled data in the target domain
to learn hash codes in the same domain) and denotes the
It is included to com-
upper bound on the performance.
pare the performance of unsupervised hashing algorithms
relative to the supervised algorithm. We used the DPSH al-
gorithm [30] to train a deep network on the target data and
used it to generate hash codes on a validation subset.

Results and Discussion: We applied the precision-recall
curves and the mean average precision (mAP) measures to
evaluate the efﬁcacy of the hashing methods, similar to pre-
vious research [7, 11, 21]. The results are depicted in Fig-
ures 4 and 5 (precision-recall curves) and Table 4 (mAP
values), where we present hashing with code length d = 64
bits. Hashing performance with d = 16 bits also follows
a similar trend and is presented in the supplementary mate-
rial. For the sake of brevity, we drop the results with Dslr
as it is very similar to Webcam, with little domain differ-
ence. We note that the NoDA has the poorest performance
due to domain mismatch. This demonstrates that domain
disparity needs to be considered before deploying a hashing
network to extract hash codes. The unsupervised hashing
methods ITQ, KMeans, BA and BDNN perform slightly
better compared to NoDA. The proposed DAH algorithm
encompasses hash code learning and domain adaptation in
a single integrated framework. It is thus able to leverage
the labeled data in the source domain in a meaningful man-
ner to learn efﬁcient hash codes for the target domain. This
accounts for its improved performance, as is evident in Fig-
ures 4 and 5 and Table 4. The supervised hashing technique
(SuH) uses labels from the target and therefore depicts the

e
c
n
a
t
s
i
D
A

-

1.5

0.5

2

1

0

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Deep
DAN
DAH

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Ar -> Cl Ar -> Pr Ar -> Rw

(a) A-Distance

(b) Deep Features (Ar,Cl)

(c) DAN Features (Ar,Cl)

(d) DAH Features (Ar,Cl)

Figure 3: Feature analysis of fc7 layer. (a) A-distances for Deep, DAN and DAH, (b), (c) and (d) t-SNE embeddings for 10 categories
from Art (•) and Clipart(+) domains. Best viewed in color.

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

0.2

0.4

0.6

0.8

1

0.2

0.4

0.6

0.8

1

0.2

0.4

0.6

0.8

1

0.2

0.4

0.6

0.8

1

Recall
 

(a) Art

Recall
 

(b) Clipart

Recall
 

(c) Product

Recall
 

(d) Real-World

Figure 4: Precision-Recall curves @64 bits for the Ofﬁce-Home dataset. Comparison of hashing without domain adaptation (NoDA),
shallow unsupervised hashing (ITQ, KMeans), state-of-the-art deep unsupervised hashing (BA, BDNN), unsupervised domain adaptive
hashing (DAH) and supervised hashing (SuH). Best viewed in color.

real-world setting.

6. Conclusions

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

0.2

0.4

0.6

0.8

1

0.2

0.4

0.6

0.8

1

Recall
 

(a) Amazon

Recall
 

(b) Webcam

Figure 5: Precision-Recall curves @64 bits for the Ofﬁce dataset.
Comparison of hashing without domain adaptation (NoDA), shal-
low unsupervised hashing (ITQ, KMeans), state-of-the-art deep
unsupervised hashing (BA, BDNN), unsupervised domain adap-
tive hashing (DAH) and supervised hashing (SuH). Best viewed
in color.

best performance. The proposed DAH framework consis-
tently delivers the best performance relative to SuH when
compared with the other hashing procedures. This demon-
strates the merit of our framework in learning representa-
tive hash codes by utilizing labeled data from a different
domain. Such a framework will be immensely useful in a

In this paper, we have proposed a novel domain adap-
tive hashing (DAH) framework which exploits the feature
learning capabilities of deep neural networks to learn efﬁ-
cient hash codes for unsupervised domain adaptation. The
DAH framework solves two important practical problems:
category assignment with weak supervision or insufﬁcient
labels (through domain adaptation) and the estimation of
hash codes in an unsupervised setting (hash codes for target
data). Thus, two practical challenges are addressed through
a single integrated framework. This research is the ﬁrst
of its kind to integrate hash code learning with unsuper-
vised domain adaptation. We also introduced a new dataset,
Ofﬁce-Home, which can be used to further research in do-
main adaptation.
Acknowledgements: This material is based upon work
supported by the National Science Foundation (NSF) un-
der Grant No:1116360. Any opinions, ﬁndings, and con-
clusions or recommendations expressed in this material are
those of the authors and do not necessarily reﬂect the views
of the NSF.

References

[1] Y. Aytar and A. Zisserman. Tabula rasa: Model transfer for

object category detection. In IEEE ICCV, 2011. 2

[2] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira,
and J. W. Vaughan. A theory of learning from different do-
mains. Machine learning, 79(1-2):151–175, 2010. 6

[3] Y. Bengio, A. Courville, and P. Vincent. Representation
learning: A review and new perspectives. IEEE transactions
on pattern analysis and machine intelligence, 35(8):1798–
1828, 2013. 2

[4] L. Bruzzone and M. Marconcini. Domain adaptation prob-
lems: A dasvm classiﬁcation technique and a circular valida-
tion strategy. IEEE, PAMI, 32(5):770–787, 2010. 2

[5] Y. Cao, M. Long, J. Wang, Q. Yang, and P. S. Yu. Deep
visual-semantic hashing for cross-modal retrieval. In ACM-
SIGKDD, 2016. 2

[6] Z. Cao, M. Long, and Q. Yang. Transitive hashing network
for heterogeneous multimedia retrieval. In AAAI, 2016. 2
[7] M. A. Carreira-Perpin´an and R. Raziperchikolaei. Hashing
with binary autoencoders. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
557–566, 2015. 2, 7

[8] K. Chatﬁeld, K. Simonyan, A. Vedaldi, and A. Zisserman.
Return of the devil in the details: Delving deep into convo-
lutional nets. In BMVC, 2014. 2, 4

[9] R. Chattopadhyay, Q. Sun, W. Fan, I. Davidson, S. Pan-
chanathan, and J. Ye. Multisource domain adaptation and
its application to early detection of fatigue. ACM Transac-
tions on Knowledge Discovery from Data (TKDD), 6(4):18,
2012. 2

[10] W.-S. Chu, F. De la Torre, and J. F. Cohn. Selective transfer
machine for personalized facial action unit detection. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 3515–3522, 2013. 2

[11] T.-T. Do, A.-D. Doan, and N.-M. Cheung. Learning to hash
with binary deep neural network. In European Conference
on Computer Vision, pages 219–234. Springer, 2016. 2, 7

[12] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,
E. Tzeng, and T. Darrell. Decaf: A deep convolutional acti-
vation feature for generic visual recognition. In ICML, pages
647–655, 2014. 2, 6

[13] L. Duan, I. W. Tsang, and D. Xu. Domain transfer multiple
kernel learning. IEEE PAMI, 34(3):465–479, 2012. 2
[14] V. Erin Liong, J. Lu, G. Wang, P. Moulin, and J. Zhou. Deep
hashing for compact binary codes learning. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 2475–2483, 2015. 2

[15] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-
J. Lin. Liblinear: A library for large linear classiﬁcation.
Journal of machine learning research, 9(Aug):1871–1874,
2008. 5, 6

[16] B. Fernando, A. Habrard, M. Sebban, and T. Tuytelaars. Un-
supervised visual domain adaptation using subspace align-
ment. In CVPR, pages 2960–2967, 2013. 1, 2

[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,
F. Laviolette, M. Marchand, and V. Lempitsky. Domain-

adversarial training of neural networks. Journal of Machine
Learning Research, 17(59):1–35, 2016. 1, 2, 3, 6

[18] X. Glorot, A. Bordes, and Y. Bengio. Domain adaptation
for large-scale sentiment classiﬁcation: A deep learning ap-
proach. In Proceedings of the 28th International Conference
on Machine Learning (ICML-11), pages 513–520, 2011. 1,
2

[19] B. Gong, K. Grauman, and F. Sha. Connecting the dots
with landmarks: Discriminatively learning domain-invariant
features for unsupervised domain adaptation. In ICML (1),
pages 222–230, 2013. 2

[20] B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic ﬂow
kernel for unsupervised domain adaptation. In IEEE CVPR,
2012. 1, 2, 5, 6

[21] Y. Gong and S. Lazebnik.

Iterative quantization: A pro-
In Computer
crustean approach to learning binary codes.
Vision and Pattern Recognition (CVPR), 2011 IEEE Confer-
ence on, pages 817–824. IEEE, 2011. 7

[22] Y. Gong, S. Lazebnik, A. Gordo, and F. Perronnin.

Itera-
tive quantization: A procrustean approach to learning binary
IEEE Transactions
codes for large-scale image retrieval.
on Pattern Analysis and Machine Intelligence, 35(12):2916–
2929, 2013. 2, 4, 7

[23] R. Gopalan, R. Li, and R. Chellappa. Domain adaptation for
object recognition: An unsupervised approach. In 2011 in-
ternational conference on computer vision, pages 999–1006.
IEEE, 2011. 2

[24] A. Gretton, D. Sejdinovic, H. Strathmann, S. Balakrishnan,
M. Pontil, K. Fukumizu, and B. K. Sriperumbudur. Optimal
kernel choice for large-scale two-sample tests. In Advances
in neural information processing systems, pages 1205–1213,
2012. 3, 4, 5, 11

[25] K. He, F. Wen, and J. Sun. K-means hashing: An afﬁnity-
preserving quantization method for learning binary compact
codes. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 2938–2945, 2013. 2, 7
[26] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber.
the difﬁculty of learning

Gradient ﬂow in recurrent nets:
long-term dependencies, 2001. 4

[27] J. Hoffman, E. Rodner, J. Donahue, K. Saenko, and T. Dar-
rell. Efﬁcient learning of domain-invariant image represen-
tations. In ICLR, 2013. 2

[28] K. Jarrett, K. Kavukcuoglu, Y. Lecun, et al. What is the
best multi-stage architecture for object recognition? In 2009
IEEE 12th International Conference on Computer Vision,
pages 2146–2153. IEEE, 2009. 5

[29] Q.-Y. Jiang and W.-J. Li. Deep cross-modal hashing. arXiv

preprint arXiv:1602.02255, 2016. 2

[30] W.-J. Li, S. Wang, and W.-C. Kang. Feature learning based
In IJCAI,

deep supervised hashing with pairwise labels.
2016, 2016. 4, 7

[31] M. Long, Y. Cao, J. Wang, and M. Jordan. Learning transfer-
able features with deep adaptation networks. In ICML, pages
97–105, 2015. 1, 2, 3, 4, 5, 6

[32] M. Long, J. Wang, G. Ding, J. Sun, and P. Yu. Transfer joint
In CVPR,

matching for unsupervised domain adaptation.
pages 1410–1417, 2014. 2

[50] H. Zhu, M. Long, J. Wang, and Y. Cao. Deep hashing net-
work for efﬁcient similarity retrieval. In Thirtieth AAAI Con-
ference on Artiﬁcial Intelligence, 2016. 4

[33] M. Long, J. Wang, G. Ding, J. Sun, and P. S. Yu. Transfer
In Pro-
feature learning with joint distribution adaptation.
ceedings of the IEEE International Conference on Computer
Vision, pages 2200–2207, 2013. 1, 2, 5, 6

[34] M. Long, H. Zhu, J. Wang, and M. I. Jordan. Unsupervised
domain adaptation with residual transfer networks. In NIPS,
2016. 1, 2, 3, 5

[35] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and
I. Matthews. The extended cohn-kanade dataset (ck+): A
complete dataset for action unit and emotion-speciﬁed ex-
pression. In CVPR, pages 94–101. IEEE, 2010. 5

[36] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.
Ng. Reading digits in natural images with unsupervised fea-
ture learning. In NIPS Workshop on Deep Learning and Un-
supervised Feature Learning 2011, 2011. 5

[37] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Learning and
transferring mid-level image representations using convolu-
In Proceedings of the IEEE con-
tional neural networks.
ference on computer vision and pattern recognition, pages
1717–1724, 2014. 2

[38] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain
adaptation via transfer component analysis. Neural Net-
works, IEEE Trans. on, 22(2):199–210, 2011. 1, 2, 6
[39] S. J. Pan and Q. Yang. A survey on transfer learning. IEEE

TKDE, 22(10):1345–1359, 2010. 2

[40] M. Pantic, M. Valstar, R. Rademaker, and L. Maat. Web-
In ICME.

based database for facial expression analysis.
IEEE, 2005. 5

[41] V. M. Patel, R. Gopalan, R. Li, and R. Chellappa. Visual do-
main adaptation: A survey of recent advances. IEEE signal
processing magazine, 32(3):53–69, 2015. 2

[42] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting vi-
sual category models to new domains. In ECCV, 2010. 1, 2,
5

[43] S. Shekhar, V. M. Patel, H. V. Nguyen, and R. Chellappa.
In Proceedings
Generalized domain-adaptive dictionaries.
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 361–368, 2013. 1

[44] B. Sun, J. Feng, and K. Saenko. Return of frustratingly easy
domain adaptation. In ICCV, TASK-CV, 2015. 1, 2, 6
[45] A. Torralba and A. A. Efros. Unbiased look at dataset bias.
In Computer Vision and Pattern Recognition (CVPR), 2011
IEEE Conference on, pages 1521–1528. IEEE, 2011. 5
[46] E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko. Simultane-
ous deep transfer across domains and tasks. In Proceedings
of the IEEE International Conference on Computer Vision,
pages 4068–4076, 2015. 1, 2, 3

[47] A. Vedaldi and K. Lenc. Matconvnet – convolutional neural
networks for matlab. In Proceeding of the ACM Int. Conf. on
Multimedia, 2015. 5

[48] J. Wang, H. T. Shen, J. Song, and J. Ji. Hashing for similarity
search: A survey. arXiv preprint arXiv:1408.2927, 2014. 1
[49] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How trans-
ferable are features in deep neural networks? In Advances
in neural information processing systems, pages 3320–3328,
2014. 2, 3

Supplementary Material

7. Loss Function Derivative

In this section we outline the derivative of Equation 8 for the backpropagation algorithm;

min
U

J = L(U s) + γM(Us, U t) + ηH(U s, U t),

where, U := {Us ∪ Ut} and (γ, η) control the importance of domain adaptation (1) and target entropy loss (7) respectively.
In the following subsections, we outline the derivative of the individual terms w.r.t. the input U.

7.1. Derivative for MK MMD

M(U s, U t) =

d2
k(U

l
s, U

l
t),

Xl∈F

d2
k(U

l
s, U

l
t) =

E[φ(us,l)] − E[φ(ut,l)]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2

.

Hk

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

We implement the linear MK-MMD loss according to [24]. For this derivation, we consider the loss at just one layer. The
derivative for the MK-MMD loss at every other layer can be derived in a similar manner. The output of ith source data point
at layer l is represented as ui and the output of the ith target data point is represented as vi. For ease of representation, we
drop the superscripts for the source (s), the target (t) and the layer (l). Unlike the conventional MMD loss which is O(n2),
the MK-MMD loss outlined in [24] is O(n) and can be estimated online (does not require all the data). The loss is calculated
over every batch of data points during the back-propagation. Let n be the number of source data points U := {ui}n
i=1 and the
number of target data points V := {vi}n
i=1 in the batch. We assume equal number of source and target data points in a batch
and that n is even. The MK-MMD is deﬁned over a set of 4 data points wi = [u2i−1, u2i, v2i−1, v2i], ∀i ∈ {1, 2, . . . , n/2}.
The MK-MMD is given by,

M(U, V) =

hm(wi),

βm

1
n/2

κ

m=1
X

n/2

i=1
X

where, κ is the number of kernels and βm = 1/κ is the weight for each kernel and,

hm(wi) = km(u2i−1, u2i) + km(v2i−1, v2i) − km(u2i−1, v2i) − km(u2i, v2i−1),

(10)

where, km(x, y) = exp

. Re-writing the MK-MMD in terms of the kernels, we have,

− ||x−y||
σm

2
2

(cid:1)
n/2

κ

(cid:0)

2
nκ

m=1
X

i=1
X

(cid:2)

M(U, V) =

km(u2i−1, u2i) + km(v2i−1, v2i) − km(u2i−1, v2i) − km(u2i, v2i−1)

,

(11)

11

(cid:3)

(8)

(1)

(2)

(9)

We now outline the derivative of 11 w.r.t. source output uq and target output vq. The derivative is,

km(u2i−1, u2i).(u2i−1 − u2i).(I{q = 2i} − I{q = 2i − 1})

∂M
∂uq

=

2
nκ

κ

n/2

m=1
X

i=1 h
X

∂M
∂vq

=

2
nκ

κ

n/2

m=1
X

i=1 h
X

2
σm

+

2
σm

2
σm

−

2
σm

km(u2i−1, v2i).(u2i−1 − v2i).I{q = 2i − 1} +

km(u2i, v2i−1).(u2i − v2i−1).I{q = 2i}

,

where, I{.} is the indicator function which is 1 if the condition is true, else it is false. The derivative w.r.t. the target data
output vq is,

km(v2i−1, v2i).(v2i−1 − v2i).(I{q = 2i} − I{q = 2i − 1})

km(u2i−1, v2i).(u2i−1 − v2i).I{q = 2i} −

km(u2i, v2i−1).(u2i − v2i−1).I{q = 2i − 1}

,

2
σm

2
σm

7.2. Derivative for Supervised Hash Loss

The supervised hash loss is given by,

(cid:1)(cid:17)

i

(cid:1)

min
U s

L(U s) = −

siju⊤

i uj − log

1 + exp(u⊤

i uj)

+

ui − sgn(ui)

Xsij ∈S (cid:16)
ns

i=1
X

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:0)
2
2.

(cid:12)
(cid:12)

(cid:12)
(cid:12)

The partial derivative of 5 w.r.t. source data output up is given by,

∂L
∂uq

=

Xsij ∈S h
1

I{i = q}

σ(u⊤

i uj) − sij

uj + I{j = q}

σ(u⊤

i uj) − sij

ui

+ 2(uq − sgn(uq))

(14)

(cid:0)
1+exp(−x) . We assume sgn(.) to be a constant and avoid the differentiability issues with sgn(.) at 0. Since

(cid:1)

(cid:0)

(cid:1)

where, σ(x) =
the S is symmetric, we can reduce the derivative to,

∂L
∂uq

=

ns

2
j=1 h
X

(cid:0)

σ(u⊤

q uj) − sqj

uj

+ 2

uq − sgn(uq)

.

i

(cid:1)

(cid:0)

7.3. Derivative for Unsupervised Entropy Loss

We outline the derivative of dH

dU in the following section, where H is deﬁned as,

H(Us, Ut) = −

pij log(pij)

1
nt

nt

C

i=1
X

j=1
X

and pij is the probability of target data output ut

i belonging to category j, given by

For ease of representation, we will denote the target output ut
P
kth source data point in the jth category usj
the news terms as,

k as uj

pij =

C
P
l=1

K
k=1 exp(ut
i

⊤usj
k )
⊤usl
K
k′=1 exp(ut
k′ )
i
i as vi and drop the superscript t. Similarly, we will denote the
P
k, by dropping the domain superscript. We deﬁne the probability pij with

(6)

(12)

i

(13)

i

(5)

(15)

(7)

(16)

pij =

K
k=1 exp(vi

⊤uj
k)
K
⊤ul
k′=1 exp(vi

k′ )

C
P
l=1

P

P

Further, we simplify by replacing exp(v⊤

i uj

k) with exp(i, jk). Equation 16 can now be represented as,

pij =

K
k=1 exp(i, jk)

C
P
l=1

K
k′=1 exp(i, lk′)

We drop the outer summations (along with the -ve sign) and will reintroduce it at a later time. The entropy loss can be
re-phrased using log( a

P

P

b ) = log(a) - log(b) as,

Hij =

K
k=1 exp(i, jk)

C
P
l=1

K
k′=1 exp(i, lk′)
K
k=1 exp(i, jk)

P
−

P
C
P
l=1
for the target and ∂Hij
∂up
q

P

P

K
k′=1 exp(i, lk′)

log

K
k=1 exp(i, jk)

(cid:0) P
log

C
l=1

(cid:0) P

P

(cid:1)

K
k′=1 exp(i, lk′)
(cid:1)

for the source. We refer to ∂up

q for a consistent reference to source

We need to estimate both, ∂Hij
∂vi
data. The derivative ∂Hij
∂up
q

for 18 is,

∂Hij
∂up

(cid:20)

q (cid:21)18

=

vi
l,k′ exp(i, lk′)

k I{j=p,

k=q }exp(i, jk).log

k exp(i, jk)

+

k I{j=p,

k=q }exp(i, jk)

P

h P

− pijexp(i, pq)log

(cid:0) P
k exp(i, jk)

(cid:1)

P

(cid:0) P

,

(cid:1)i

where, I{.} is an indicator function which is 1 only when both the conditions within are true, else it is 0. The derivative ∂Hij
∂up
q
for 19 is,

∂Hij
∂up

(cid:20)

q (cid:21)19

= −

vi
l,k′ exp(i, lk′)

P

k I{j=p,

k=q }exp(i, jk).log

h P

− pij exp(i, pq)log

l,k′ exp(i, lk′)
(cid:1)

(cid:0) P

l,k′ exp(i, lk′)

+ pijexp(i, pq)

Expressing ∂Hij
∂up
q

=

∂Hij
∂up
q

+

∂Hij
∂up
q

, and deﬁning ¯pijk = exp(i,jk)

(cid:0) P

(cid:1)i
Pl,k′ exp(i,lk′) the derivative w.r.t. the source is,

18
i
=vi

19
i
h
k I{j=p,
k=q }¯pijk.log

h
∂Hij
∂up
q

k exp(i, jk)

+

k I{j=p,

k=q }¯pijk

h P
− pij ¯pipqlog

(cid:0) P
k exp(i, jk)

−

(cid:1)
k I{j=p,
k=q }¯pijk.log

P

− pij ¯pipq + pij ¯pipqlog

(cid:0) P

l,k′ exp(i, lk′)
(cid:1)
P

l,k′ exp(i, lk′)
(cid:1)

(cid:0) P

=vi

k I{j=p,

k=q }¯pijklog(pij ) − pij ¯pipqlog(pij ) +

(cid:0) P

k I{j=p,

k=q }¯pijk − pij ¯pipq

(cid:1)i

=vi

h P
log(pij ) + 1

k I{j=p,

k=q }¯pijk − pij ¯pipq

P

i

The derivative of H w.r.t the source output up

(cid:0)

(cid:1)h P
q is given by,

i

nt

C

∂H
∂up
q

= −

1
nt

i=1
X
for 18 as,

j=1
X

(cid:0)

vi

log(pij) + 1

k I{j=p,

k=q }¯pijk − pij ¯pipq

(cid:1)h P

i

We now outline the derivative ∂H
∂vi

∂Hij
∂vi (cid:21)18

(cid:20)

=

1
l,k′ exp(i, lk′)

P

k exp(i, jk)

k exp(i, jk)uj

k +

k exp(i, jk)uj

k

(cid:0) P

1
l,k′ exp(i, lk′)

(cid:1) P

k exp(i, jk)log

P
k exp(i, jk)

P

(cid:0) P

(cid:1) P

l,k′ exp(i, lk′)ul
k′

,

(26)

i

log

h

−

P

(17)

(18)

(19)

(20)

(21)

(22)

(23)

(24)

(25)

and the derivative ∂H
∂vi

for 19 as,

∂Hij
∂vi (cid:21)19

(cid:20)

= −

1
l,k′ exp(i, lk′)

P

log

h

−

P
19
i

l,k′ exp(i, lk′)

k exp(i, jk)uj

k + Pk exp(i,jk)
Pl,k′ exp(i,lk′)

l,k′ exp(i, lk′)ul
k′

(cid:0) P

1
l,k′ exp(i, lk′)

(cid:1) P

k exp(i, jk)log

l,k′ exp(i, lk′)

l,k′ exp(i, lk′)ul
k′

,

(27)

P

(cid:0) P

i

P

(cid:1) P

Expressing ∂Hij
∂vi

=

∂Hij
∂vi

+

∂Hij
∂vi

, we get,

∂Hij
∂vi

=

18
i

h

h

1
l,k′ exp(i, lk′)

log

k exp(i, jk)

k exp(i, jk)uj

k − log

l,k′ exp(i, lk′)

k exp(i, jk)uj

k

(cid:0) P

(cid:1) P

h
(cid:0) P
k exp(i, jk)uj
k − pij

+
P

− pij log
P

k exp(i, jk)

P

(cid:1) P

l,k′ exp(i, lk′)ul
k′
l,k′ exp(i, lk′)ul

=

log

(cid:0) P
k exp(i, jk)

h
+

k ¯pijkuj
(cid:0) P

k − pij

(cid:1) P

k − log

(cid:1) P
k ¯pijkuj
l,k′ ¯pijk′ ul
k′
l,k′ ¯pijk′ ul

k′ + pijlog

l,k′ exp(i, lk′)

(cid:0) P

l,k′ exp(i, lk′)
k ¯pijkuj

k

(cid:1) P

l,k′ exp(i, lk′)ul
k′

(28)

i

(cid:0) P

(cid:1) P

− pij log
P

=

(cid:0) P
log(pij) + 1

=

log(pij) + 1
(cid:0)

P

k exp(i, jk)
k ¯pijkuj
(cid:1) P
k −
k ¯pijkuj
k − pij
(cid:0)

(cid:1) P

log(pij) + 1

pij
l,k′ ¯pijk′ ul
(cid:1)
P
k′

k′ + pijlog

l,k′ exp(i, lk′)

l,k′ ¯pijk′ ul
k′

l,k′ ¯pijk′ ul
(cid:0) P
k′

(cid:1) P

i

The derivative of H w.r.t. target output vq is given by,
P

(cid:1)(cid:0) P

(cid:0)

(cid:1)

∂H
∂vq

= −

1
nt

C

j=1
X

(cid:0)

log(pqj) + 1

k ¯pqjkuj

k − pqj

l,k′ ¯pqjk′ ul
k′

The derivative of H w.r.t. the source outputs is given by 25 and w.r.t. the target outputs is given by 32.

(cid:1)(cid:0) P

P

(cid:1)

8. Unsupervised Domain Adaptation: Additional Results

(29)

(30)

(31)

(32)

In the main paper we had presented results for unsupervised domain adaptation based object recognition with d = 64 bits.
Here, we outline the classiﬁcation results with d = 16 (DAH-16) and d = 128 (DAH-128) bits for the Ofﬁce-Home dataset
in Table 5. We also present the (DAH-64), DAN and DANN results for comparison. There is an increase in the average
recognition accuracy for d = 128 bits compared to d = 64 bits because of the increased capacity in representation. As
expected, d = 16 has a lower recognition accuracy.
Table 5: Recognition accuracies (%) for domain adaptation experiments on the Ofﬁce-Home dataset. {Art (Ar), Clipart (Cl),
Product (Pr), Real-World (Rw)}. Ar→Cl implies Ar is source and Cl is target.

Expt.
DAN
DANN
DAH-16
DAH-64
DAH-128

Ar→Cl Ar→Pr Ar→Rw Cl→Ar Cl→Pr Cl→Rw Pr→Ar Pr→Cl Pr→Rw Rw→Ar Rw→Cl Rw→Pr Avg.
43.46
29.07
30.66
44.94
30.49
33.33
31.36
20.11
23.83
45.54
29.91
31.64
46.26
30.94
32.58

42.17
42.96
30.32
40.75
40.64

54.13
54.42
40.14
51.73
52.40

62.73
64.65
37.46
62.54
64.97

43.58
44.71
32.63
44.99
45.65

32.83
32.26
25.67
34.69
35.72

47.59
49.13
38.79
51.93
52.80

49.78
49.76
33.26
52.79
52.12

34.05
38.14
27.72
39.63
41.31

56.70
56.76
40.90
60.71
59.31

38.25
42.66
25.54
45.13
46.67

9. Unsupervised Domain Adaptive Hashing: Additional Results

We provide the unsupervised domain adaptive hashing results for d = 16 and d = 128 bits in Figures 6 and 7 respectively.
In Tables 6 and 7, we outline the corresponding mAP values. The notations are along the lines outlined in the main paper. We
observe similar trends for both d = 16 and d = 128 bits compared to d = 64 bits. It is interesting to note that with increase
in bit size d, the mAP does not necessarily increase. Table 7 (d = 64) has its mAP values lower than those for d = 64 (see
main paper) for all the hashing methods. This indicates that merely increasing the hash code length does not always improve
mAP scores. Also, the mAP values for Real-World for d = 128 bits has DAH performing better than SuH. This indicates
that in some cases domain adaptation helps in learning a better generalized model.

Table 6: Mean average precision @16 bits. For the NoDA and DAH results, Art is the source domain for Clipart, Product and
Real-World and Clipart is the source domain for Art.

Table 7: Mean average precision @128 bits. For the NoDA and DAH results, Art is the source domain for Clipart, Product and
Real-World and Clipart is the source domain for Art.

Expt.
Art
Clipart
Product
Real-World
Avg.

NoDA ITQ KMeans
0.147
0.102
0.120
0.110
0.253
0.134
0.225
0.193
0.186
0.135

0.133
0.116
0.241
0.195
0.171

BA
0.131
0.123
0.253
0.216
0.181

BDNN DAH SuH
0.381
0.207
0.151
0.412
0.211
0.138
0.459
0.257
0.313
0.400
0.371
0.248
0.413
0.262
0.212

Expt.
Art
Clipart
Product
Real-World
Avg.

NoDA ITQ KMeans
0.202
0.154
0.210
0.186
0.416
0.279
0.343
0.308
0.293
0.232

0.175
0.196
0.356
0.289
0.254

BA
0.148
0.187
0.336
0.258
0.232

BDNN DAH SuH
0.444
0.314
0.207
0.346
0.350
0.213
0.792
0.424
0.432
0.458
0.544
0.348
0.510
0.408
0.300

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.5
Recall
 

(a) Art

0.5
Recall
 

(b) Clipart

0.5
Recall
 

(c) Product

0.5
Recall
 

(d) Real-World

Figure 6: Precision-Recall curves @16 bits for the Ofﬁce-Home dataset. Comparison of hashing without domain adaptation (NoDA),
shallow unsupervised hashing (ITQ, KMeans), state-of-the-art deep unsupervised hashing (BA, BDNN), unsupervised domain adaptive
hashing (DAH) and supervised hashing (SuH). Best viewed in color.

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

n
o
i
s
i
c
e
r
P

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

n
o
i
s
i
c
e
r
P

0.5

0.4

0.9

0.8

0.7

0.6

0.3

0.2

0.1

0

0

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

n
o
i
s
i
c
e
r
P

0.5

0.4

0.9

0.8

0.7

0.6

0.3

0.2

0.1

0

0

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.5
Recall
 

(a) Art

0.5
Recall
 

(b) Clipart

0.5
Recall
 

(c) Product

0.5
Recall
 

(d) Real-World

Figure 7: Precision-Recall curves @128 bits for the Ofﬁce-Home dataset. Comparison of hashing without domain adaptation (NoDA),
shallow unsupervised hashing (ITQ, KMeans), state-of-the-art deep unsupervised hashing (BA, BDNN), unsupervised domain adaptive
hashing (DAH) and supervised hashing (SuH). Best viewed in color.

7
1
0
2
 
n
u
J
 
2
2
 
 
]

V
C
.
s
c
[
 
 
1
v
2
2
5
7
0
.
6
0
7
1
:
v
i
X
r
a

Deep Hashing Network for Unsupervised Domain Adaptation

Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, Sethuraman Panchanathan
Center for Cognitive Ubiquitous Computing, Arizona State University, Tempe, AZ, USA
{hemanthv, jeusebio, shayok.chakraborty, panch}@asu.edu

Abstract

In recent years, deep neural networks have emerged as a
dominant machine learning tool for a wide variety of appli-
cation domains. However, training a deep neural network
requires a large amount of labeled data, which is an expen-
sive process in terms of time, labor and human expertise.
Domain adaptation or transfer learning algorithms address
this challenge by leveraging labeled data in a different, but
related source domain, to develop a model for the target
domain. Further, the explosive growth of digital data has
posed a fundamental challenge concerning its storage and
retrieval. Due to its storage and retrieval efﬁciency, recent
years have witnessed a wide application of hashing in a
variety of computer vision applications. In this paper, we
ﬁrst introduce a new dataset, Ofﬁce-Home, to evaluate do-
main adaptation algorithms. The dataset contains images
of a variety of everyday objects from multiple domains. We
then propose a novel deep learning framework that can ex-
ploit labeled source data and unlabeled target data to learn
informative hash codes, to accurately classify unseen tar-
get data. To the best of our knowledge, this is the ﬁrst
research effort to exploit the feature learning capabilities
of deep neural networks to learn representative hash codes
to address the domain adaptation problem. Our extensive
empirical studies on multiple transfer tasks corroborate the
usefulness of the framework in learning efﬁcient hash codes
which outperform existing competitive baselines for unsu-
pervised domain adaptation.

1. Introduction

Deep learning algorithms automatically learn a discrim-
inating set of features and have depicted commendable per-
formance in a variety of computer vision applications. Un-
fortunately, training a deep model necessitates a large vol-
ume of labeled data, which can be time consuming and ex-
pensive to acquire. However, labeled data from a differ-
ent, but related domain is often available, which has mo-
tivated the development of algorithms which can leverage

labeled data in a source domain to develop a machine learn-
ing model for the target domain. Learning a discrimina-
tive model in the presence of the shift between training and
test distributions is known as transfer learning or domain
adaptation [17]. Unsupervised domain adaptation is a chal-
lenging setting, where labeled data is available only in the
source domain; no labeled data is available in the target
domain. Conventional shallow transfer learning methods
develop their models in two stages, feature extraction fol-
lowed by domain adaptation. The features are ﬁxed and
then a model is trained to align the source and target do-
mains [16, 20, 33, 38, 42, 43, 44]. On the other hand, deep
transfer learning procedures exploit the feature learning ca-
pabilities of deep networks to learn transferable feature rep-
resentations for domain adaptation and have demonstrated
impressive empirical performance [17, 18, 31, 34, 46].

The explosive growth of digital data in the modern era
has posed fundamental challenges regarding their storage,
retrieval and computational requirements. Against this
backdrop, hashing has emerged as one of the most popu-
lar and effective techniques due to its fast query speed and
low memory cost [48]. Hashing techniques transform high
dimensional data into compact binary codes and generate
similar binary codes for similar data items. Motivated by
this fact, we propose to train a deep neural network to out-
put binary hash codes (instead of probability values), which
can be used for classiﬁcation. We see two advantages to es-
timating a hash value instead of a standard probability vec-
tor in the ﬁnal layer of the network: (i) the hash values are
used to develop a unique loss function for target data in the
absence of labels and (ii) during prediction, the hash value
of a test sample can be compared against the hash values
of the training samples to arrive at a more robust category
prediction.

In this paper, we ﬁrst introduce a new dataset, Ofﬁce-
Home, which we use to evaluate our algorithm. The Ofﬁce-
Home dataset is an object recognition dataset which con-
tains images from 4 domains. It has around 15, 500 images
organized into 65 categories. We further propose a novel
deep learning framework called Domain Adaptive Hash-

1

ing (DAH) to learn informative hash codes to address the
problem of unsupervised domain adaptation. We propose
a unique loss function to train the deep network with the
following components: (i) supervised hash loss for labeled
source data, which ensures that source samples belonging
to the same class have similar hash codes; (ii) unsuper-
vised entropy loss for unlabeled target data, which imposes
each target sample to align closely with exactly one of the
source categories and be distinct from the other categories
and (iii) a loss based on multi-kernel Maximum Mean Dis-
crepancy (MK-MMD), which seeks to learn transferable
features within the layers of the network to minimize the
distribution difference between the source and target do-
mains. Figure 1 illustrates the different layers of the DAH
and the components of the loss function.

2. Related Work

There have been many approaches to address the prob-
lem of domain-shift in unsupervised domain adaptation.
One straightforward approach is,
to modify a classiﬁer
trained for the source data by adapting it to classify target
data [1, 4] or learn a transformation matrix to linearly trans-
form the source data, so that it is aligned with the target
[27, 42]. Some other procedures re-weight the data points
in the source domain, to select source data that is similar
to the target, when training a domain adaptive classiﬁer,
[9, 10, 19]. A standard procedure to reduce domain discrep-
ancy is, to project the source and target data to a common
subspace, thereby aligning their principal axes [16, 44].
Reducing domain disparity through nonlinear alignment of
data has been possible with Maximum Mean Discrepancy
(MMD) - a measure that provides the distribution differ-
ence between two datasets in a reproducing-kernel Hilbert
space [13]. Kernel-PCA based methods apply the MMD to
achieve nonlinear alignment of domains [32, 33, 38]. Man-
ifold based approaches are also popular in domain adapta-
tion for computer vision, where the subspace of a domain is
treated as a point on the manifold and transformations are
learned to align two domains [20, 23]. A survey of popular
domain adaptation techniques for computer vision is pro-
vided in [41] and a more generic survey of transfer learning
approaches can be found in [39].

All of the above techniques can be termed as shallow
learning procedures, since the models are learned using pre-
determined features. In recent years deep learning has be-
come very successful at learning highly discriminative fea-
tures for computer vision applications [8]. Deep learning
systems like deep CNNs learn representations of data that
capture underlying factors of variation between different
tasks in a multi-task transfer learning setting [3]. These rep-
resentations also disentangle the factors of variation allow-
ing for the transfer of knowledge between tasks [12, 18, 37].
Yosinski et al. [49] demonstrated how the lower layers of a

network produce generic features and the upper layers out-
put task speciﬁc features. Based on this, deep learning pro-
cedures for domain adaptation train networks to learn trans-
ferable features in the fully connected ﬁnal layers of a net-
work [31, 46]. In other approaches to deep domain adapta-
tion, Ganin et al. [17] trained domain adversarial networks
to learn features that make the source and target domain in-
distinguishable and Long et al. [34], trained a network to
do both feature adaptation and classiﬁer adaptation using
residual transfer networks.

Unsupervised hashing techniques have been developed
to extract unique hash codes for efﬁcient storage and re-
trieval of data [22, 25]. Neural network based hashing has
led the way in state-of-the-art unsupervised hashing tech-
niques [7, 11, 14]. The closest work incorporating hash-
ing and adaptation appears in cross-modal hashing, where
deep hashing techniques embed multi-modal data and learn
hash codes for two related domains, like text and images
[5, 6, 29]. However, these algorithms are not unsupervised
and they are mainly applied to extract common hash codes
for multi-modal data for retrieval purposes. To the best of
our knowledge, there has been no work in unsupervised
domain adaptation using deep hashing networks. We now
present the Domain Adaptive Hashing (DAH) network for
unsupervised domain adaptation through deep hashing.

3. Domain Adaptive Hashing Networks

i }ns

In unsupervised domain adaptation, we consider data
from two domains; source and target. The source consists
i , ys
of labeled data, Ds = {xs
i=1 and the target has only
i}nt
i=1. The data points x∗
unlabeled data Dt = {xt
i belong to
X, where X is some input space. The corresponding labels
are represented by y∗
i ∈ Y := {1, . . . , C}. The paradigm of
domain adaptive learning attempts to address the problem of
domain-shift in the data, where the data distributions of the
source and target are different, i.e. Ps(X, Y ) 6= Pt(X, Y ).
The domain-shift notwithstanding, our goal is to train a
deep neural network classiﬁer ψ(.), that can predict the la-
bels {ˆyt

i=1, for the target data.

i }nt

We implement the neural network as a deep CNN which
consists of 5 convolution layers conv1 - conv5 and 3 fully
connected layers fc6 - fc8 followed by a loss layer. In our
model, we introduce a hashing layer hash-fc8 in place of
the standard fc8 layer to learn a binary code hi, for every
data point xi, where hi ∈ {−1, +1}d. The hash-fc8 layer
is driven by two loss functions, (i) supervised hash loss for
the source data, (ii) unsupervised entropy loss for the target
data. The supervised hash loss ensures hash values that are
distinct and discriminatory, i.e. if xi and xj belong to the
same category, their hash values hi and hj are similar and
different otherwise. The unsupervised entropy loss aligns
the target hash values with source hash values based on the
similarity of their feature representations. The output of the

is the output representation of x∗

layer l, where u∗,l
i for the
i
lth layer. The ﬁnal layer outputs are denoted as Us and U t.
The MK-MMD measure d2
k(.) is the multi-kernel maximum
mean discrepancy between the source and target representa-
tions, [24]. For a nonlinear mapping φ(.) associated with a
reproducing kernel Hilbert space Hk and kernel k(.), where
k(x, y) = hφ(x), φ(y)i, the MMD is deﬁned as,

2

.

Hk

(2)

d2
k(U

l
s, U

l
t) =

E[φ(us,l)] − E[φ(ut,l)]
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
The characteristic kernel k(.), is determined as a convex
(cid:12)
combination of κ PSD kernels, {km}κ
m=1, K :=
k : k =
κ
. We set
m=1 βm = 1, βm ≥ 0, ∀m
(cid:8)
βm = 1/κ according to [34] and it works well in practice.
P
3.2. Supervised Hashing for Source Data

κ
m=1 βmkm,

P

(cid:12)
(cid:12)
(cid:12)

(cid:9)

2 (d − h⊤

The Hamming distance for a pair of hash values hi and
hj has a unique relationship with the dot product hhi, hj i,
given by: distH (hi, hj) = 1
i hj), where d is the
hash length. The dot product hhi, hji can be treated as
a similarity measure for the hash codes. Larger the value
of the dot product (high similarity), smaller is the distance
distH and smaller the dot product (low similarity), larger is
the distance distH . Let sij ∈ {0, 1} be the similarity be-
tween xi and xj. If xi and xj belong to the same category,
sij = 1 and 0, otherwise. The probability of similarity be-
tween xi and xj given the corresponding hash values hi
and hj, can be expressed as a likelihood function, given by,

p(sij |hi, hj) =

σ(h⊤
i hj),
1 − σ(h⊤

(

i hj),

sij = 1
sij = 0,

(3)

1

1+e−x is the sigmoid function. As the
where, σ(x) =
dot product hhi, hji increases, the probability of p(sij =
1|hi, hj ) also increases, i.e., xi and xj belong to the same
category. As the dot product decreases, the probability
p(sij = 1|hi, hj) also decreases, i.e., xi and xj belong
to different categories. We construct the (ns × ns) similar-
ity matrix S = {sij}, for the source data with the provided
labels, where sij = 1 if xi and xj belong to the same cat-
egory and 0, otherwise. Let H = {hi}ns
i=1 be the set of
source data hash values. If the elements of H are assumed
to be i.i.d., the negative log likelihood of the similarity ma-
trix S given H can be written as,

L(H) = −log p(S|H)

min
H

= −

sij h⊤

i hj − log

1 + exp(h⊤

i hj )

.

Xsij ∈S (cid:16)

(cid:0)

(cid:1)(cid:17)
(4)

Figure 1: The Domain Adaptive Hash (DAH) network that out-
puts hash codes for the source and the target. The network is
trained with a batch of source and target data. The convolution
layers conv1 - conv5 and the fully connected layers fc6 and fc7 are
ﬁne tuned from the VGG-F network. The MK-MMD loss trains
the DAH to learn feature representations which align the source
and the target. The hash-fc8 layer is trained to output vectors of d
dimensions. The supervised hash loss drives the DAH to estimate
a unique hash value for each object category. The unsupervised
entropy loss aligns the target hash values to their corresponding
source categories. Best viewed in color.
network is represented as ψ(x), where ψ(x) ∈ Rd, which
we convert to a hash code h = sgn(ψ(x)), where sgn(.)
is the sign function. Once the network has been trained,
the probability of x being assigned a label y is given by
f (x) = p(y|h). We train the network using Ds and Dt and
predict the target data labels ˆyt

∗ using f (.).

In order to address the issue of domain-shift, we need to
align the feature representations of the target and the source.
We do that by reducing the domain discrepancy between the
source and target feature representations at multiple layers
of the network. In the following subsections, we discuss
the design of the domain adaptive hash (DAH) network in
detail.

3.1. Reducing Domain Disparity

Deep learning methods have been very successful in do-
main adaptation with state-of-the-art algorithms [17, 31, 34,
46] in recent years. The feature representations transition
from generic to task-speciﬁc as one goes up the layers of
a deep CNN [49]. The convolution layers conv1 to conv5
have been shown to be generic and so, readily transferable,
whereas the fully connected layers are more task-speciﬁc
and need to be adapted before they can be transferred. In
the DAH algorithm, we attempt to minimize the MK-MMD
loss to reduce the domain difference between the source
and target feature representations for fully connected lay-
ers, F = {fc6, fc7, fc8}. Such a loss function has been used
in previous research [31, 34]. The multi-layer MK-MMD
loss is given by,

M(U s, U t) =

d2
k(U

l
s, U

l
t),

(1)

l

where, U
i=1 are the set
of output representations for the source and target data at

s = {us,l

i }ns

t = {ut,l

i }nt

By minimizing Equation (4), we can determine hash val-
ues H for the source data which are consistent with the

Xl∈F
l
i=1 and U

similarity matrix S. The hash loss has been used in pre-
vious research for supervised hashing [30, 50]. Equation
(4) is a discrete optimization problem that is challenging to
solve. We introduce a relaxation on the discrete constraint
hi ∈ {−1, +1}d by instead solving for ui ∈ Rd, where
Us = {ui}ns
i=1 is the output of the network and ui = ψ(xi)
(the superscript denoting the domain has been dropped for
ease of representation). However, the continuous relaxation
gives rise to (i) approximation error, when hhi, hj i is sub-
stituted with hui, uji and, (ii) quantization error, when the
resulting real codes ui are binarized [50]. We account for
the approximation error by having a tanh(.) as the ﬁnal ac-
tivation layer of the neural network, so that the components
of ui are bounded between −1 and +1. In addition, we also
introduce a quantization loss ||ui − sgn(ui)||2
2 along the
lines of [22], where sgn(.) is the sign function. The contin-
uous optimization problem for supervised hashing can now
be outlined;

min
U s

L(U s) = −

sij u⊤

i uj − log

1 + exp(u⊤

i uj)

+

ui − sgn(ui)

Xsij ∈S (cid:16)
ns

i=1
X

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:0)
2
2.

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:1)(cid:17)
(5)

3.3. Unsupervised Hashing for Target Data

k=1

{usj

k }K

In the absence of target data labels, we use the similarity
measure hui, uji, to guide the network to learn discrimina-
tive hash values for the target data. An ideal target output
ut
i, needs to be similar to many of the source outputs from
the jth category
. We assume without loss of
generality, K source data points for every category j where,
(cid:1)
j ∈ {1, . . . , C} and usj
k is the kth source output from cat-
egory j. In addition, ut
i must be dissimilar to most other
source outputs usl
k belonging to a different category (j 6= l).
Enforcing similarity with all the K data points makes for a
more robust target data category assignment. We outline
a probability measure to capture this intuition. Let pij be
the probability that input target data point xi is assigned to
category j where,

(cid:0)

pij =

K
k=1 exp(ut
i
K
k=1 exp(ut
i

⊤usj
k )
⊤usl
k )

C
P
l=1

(6)

P

P

The exp(.) has been introduced for ease of differentiabil-
j pij = 1. When the
ity and the denominator ensures
target data point output is similar to one category only and
dissimilar to all the other categories, the probability vec-
tor pi = [pi1, . . . , piC ]T tends to be a one-hot vector. A
one-hot vector can be viewed as a low entropy realization
of pi. We can therefore envisage all the pi to be one-hot
vectors (low entropy probability vectors), where the target
data point outputs are similar to source data point outputs in
one and only one category. To this end we introduce a loss

P

to capture the entropy of the target probability vectors. The
entropy loss for the network outputs is given by,

H(Us, Ut) = −

pijlog(pij)

(7)

1
nt

nt

C

i=1
X

j=1
X

Minimizing the entropy loss gives us probability vectors pi
that tend to be one-hot vectors, i.e., the target data point
outputs are similar to source data point outputs from any
one category only. Enforcing similarity with K source data
points from a category, guarantees that the hash values are
determined based on a common similarity between multiple
source category data points and the target data point.

3.4. Domain Adaptive Hash Network

We propose a model for deep unsupervised domain adap-
tation based on hashing (DAH) that incorporates unsuper-
vised domain adaptation between the source and the target
(1), supervised hashing for the source (5) and unsupervised
hashing for the target (7) in a deep convolutional neural net-
work. The DAH network is trained to minimize

min
U

J = L(U s) + γM(Us, U t) + ηH(U s, U t),

(8)

where, U := {U s ∪ Ut} and (γ, η) control the importance
of domain adaptation (1) and target entropy loss (7) respec-
tively. The hash values H are obtained from the output of
the network using H = sgn(U). The loss terms (5) and
(7) are determined in the ﬁnal layer of the network with the
network output U. The MK-MMD loss (1) is determined
l
l
between layer outputs {U
t} at each of the fully con-
s, U
nected layers F = {fc6, fc7, fc8}, where we adopt the lin-
ear time estimate for the unbiased MK-MMD as described
in [24] and [31]. The DAH is trained using standard back-
propagation. The detailed derivation of the derivative of (8)
w.r.t. U is provided in the supplementary material.
Network Architecture: Owing to the paucity of images
in a domain adaptation setting, we circumvent the need to
train a deep CNN with millions of images by adapting the
pre-trained VGG-F [8] network to the DAH. The VGG-F
has been trained on the ImageNet 2012 dataset and it con-
sists of 5 convolution layers (conv1 - conv5) and 3 fully
connected layers (fc6, fc7, fc8). We introduce the hashing
layer hash-fc8 that outputs vectors in Rd in the place of fc8.
To account for the hashing approximation, we introduced
a tanh() layer. However, we encounter the issue of van-
ishing gradients [26] when using tanh() as it saturates with
large inputs. We therefore preface the tanh() with a batch
normalization layer which prevents the tanh() from saturat-
ing. In effect, hash-fc8 := {fc8 → batch-norm → tanh()}.
The hash-fc8 provides greater stability when ﬁne-tuning the
learning rates than the deep hashing networks [30, 50]. Fig-
ure 1 illustrates the proposed DAH network.

Table 1: Statistics for the Ofﬁce-Home dataset. Min: # is the
minimum number of images amongst all the categories, Min: Size
and Max: Size are the minimum and maximum image sizes across
all categories and Acc. is the classiﬁcation accuracy.

Domain.
Art
Clipart
Product
Real-World

Min: # Min: Size
117×85 pix.
18×18 pix.
75×63 pix.
88×80 pix.

15
39
38
23

Max: Size
4384×2686 pix.
2400×2400 pix.
2560×2560 pix.
6500×4900 pix.

Acc
44.99±1.85
53.95±1.45
66.41±1.18
59.70±1.04

4. The Ofﬁce-Home Dataset

Supervised deep learning models require a large volume
of labeled training data. Unfortunately, existing datasets
for vision-based domain adaptation are limited in their
size and are not suitable for validating deep learning al-
gorithms. The standard datasets for vision based domain
adaptation are, facial expression datasets CKPlus [35] and
MMI [40], digit datasets SVHN [36], USPS and MNIST[28],
head pose recognition datasets PIE [33], object recogni-
tion datasets COIL[33], Ofﬁce [42] and Ofﬁce-Caltech [20].
These datasets were created before deep-learning became
popular and are insufﬁcient for training and evaluating deep
learning based domain adaptation approaches. For instance,
the object-recognition dataset Ofﬁce has 4110 images across
31 categories and Ofﬁce-Caltech has 2533 images across 10
categories.

We release the Ofﬁce-Home dataset for domain adap-
tation based object recognition, that can be used to evalu-
ate deep learning algorithms for domain adaptation. The
Ofﬁce-Home dataset consists of 4 domains, with each do-
main containing images from 65 categories of everyday ob-
jects and a total of around 15, 500 images. The domains
include, Art: artistic depictions of objects in the form of
sketches, paintings, ornamentation, etc.; Clipart: collec-
tion of clipart images; Product: images of objects with-
out a background, akin to the Amazon category in Ofﬁce
dataset; Real-World: images of objects captured with a
regular camera.

Public domain images were downloaded from web-
sites like www.deviantart.com and www.ﬂickr.com to cre-
ate the Art and Real-World domains. Clipart im-
ages were gathered from multiple clipart websites. The
Product domain images were exclusively collected from
www.amazon.com using web-crawlers. The collected im-
ages were manually ﬁltered on the basis of quality, size and
content. The dataset has an average of around 70 images
per category and a maximum of 99 images in a category.
The primary challenge in creating this dataset was acquir-
ing sufﬁcient number of public domain images across all
the 4 domains. Figure 2 depicts a sampling of 16 categories
from the Ofﬁce-Home dataset and Table 1 outlines some
meta data for the dataset. The Acc. column in the Table
1 refers to classiﬁcation accuracies using the LIBLINEAR
SVM [15] classiﬁer (5-fold cross validation) with deep fea-

tures extracted using the VGG-F network. The dataset is
publicly available for research 1.

5. Experiments

In this section we conduct extensive experiments to
evaluate the DAH algorithm. Since we propose a do-
main adaptation technique based on hashing, we evalu-
ate objection recognition accuracies for unsupervised do-
main adaptation and also study the discriminatory capabil-
ity of the learned hash codes for unsupervised domain adap-
tive hashing. The implementation details are available at
https://github.com/hemanthdv/da-hash

5.1. Datasets

Ofﬁce [42]: This is currently the most popular benchmark
dataset for object recognition in the domain adaptation com-
puter vision community. The dataset consists of images of
everyday objects in an ofﬁce environment. It has 3 domains;
Amazon (A), Dslr (D) and Webcam (W). The dataset has
around 4, 100 images with a majority of the images (2816
images) in the Amazon domain. We adopt the common
evaluation protocol of different pairs of transfer tasks for
this dataset [31, 34]. We consider 6 transfer tasks for all
combinations of source and target pairs for the 3 domains.
Ofﬁce-Home: We introduce this new dataset and evaluate
it in a similar manner to the Ofﬁce dataset. We consider 12
transfer tasks for the Art (Ar), Clipart (Cl), Product
(Pr) and Real-World (Rw) domains for all combinations
of source and target for the 4 domains. Considering all the
different pairs of transfer enables us to evaluate the inherent
bias between the domains in a comprehensive manner [45].

5.2. Implementation Details

We implement the DAH using the MatConvnet frame-
work [47]. Since we train a pre-trained VGG-F, we ﬁne-
tune the weights of conv1-conv5, fc6 and fc7. We set
their learning rates to 1/10th the learning rate of hash-fc8.
We vary the learning rate between 10−4 to 10−5 over 300
epochs with a momentum 0.9 and weight decay 5 × 10−4.
We set K = 5 (number of samples from a category). Since
we have 31 categories in the Ofﬁce dataset, we get a source
batch size of 31 × 5 = 155. For the target batch, we ran-
domly select 155 samples. The total batch size turns out to
be 310. For the Ofﬁce-Home dataset, with K = 5 and 65
categories, we get a batch size of 650. We set d = 64 (hash
code length) for all our experiments. Since there is imbal-
ance in the number of like and unlike pairs in S, we set the
values in similarity matrix Si,j ∈ {0, 10}. Increasing the
similarity weight of like-pairs improves the performance of
DAH. For the entropy loss, we set η = 1. For the MK-
MMD loss, we follow the heuristics mentioned in [24], to

1https://hemanthdv.github.io/officehome-dataset/

Figure 2: Sample images from the Ofﬁce-Home dataset. The dataset consists of images of everyday objects organized into 4 domains;
Art: paintings, sketches and/or artistic depictions, Clipart: clipart images, Product: images without background and Real-World:
regular images captured with a camera. The ﬁgure displays examples from 16 of the 65 categories.

determine the parameters. We estimate γ, by validating a
binary domain classiﬁer to distinguish between source and
target data points and select γ which gives largest error on a
validation set. For MMD, we use a Gaussian kernel with a
bandwidth σ given by the median of the pairwise distances
in the training data. To incorporate the multi-kernel, we
vary the bandwidth σm ∈ [2−8σ, 28σ] with a multiplicative
factor of 2. We deﬁne the target classiﬁer f (xt
i) = p(y|ht
i)
in terms of 6. The target data point is assigned to the class
with the largest probability, with ˆyi = maxj(pij ) using the
hash codes for the source and the target.

5.3. Unsupervised Domain Adaptation

In this section, we study the performance of the DAH
for unsupervised domain adaptation, where labeled data is
available only in the source domain and no labeled data is
available in the target domain. We compare the DAH with
state-of-the-art domain adaptation methods: (i) Geodesic
Flow Kernel (GFK) [20], (ii) Transfer Component Analy-
sis (TCA) [38], (iii) Correlation Alignment (CORAL) [44]
and (iv) Joint Distribution Adaptation (JDA) [33]. We also
compare the DAH with state-of-the-art deep learning meth-
ods for domain adaptation: (v) Deep Adaptation Network
(DAN) [31] and (vi) Domain Adversarial Neural Network
(DANN) [17]. For all of the shallow learning methods,
we extract and use deep features from the fc7 layer of the
VGG-F network that was pre-trained on the ImageNet 2012
dataset. We also evaluate the effect of the entropy loss on
hashing for the DAH. The DAH-e is the DAH algorithm
where η is set to zero, which implies that the target hash
values are not driven to align with the source categories.
We follow the standard protocol for unsupervised domain
adaptation, where all the labeled source data and all the un-
labeled target data is used for training.
Results and Discussion: The results are reported for the
target classiﬁcation in each of the transfer tasks in Tables 2
and 3, where accuracies denote the percentage of correctly

Table 2: Recognition accuracies (%) for domain adaptation exper-
iments on the Ofﬁce dataset. {Amazon (A), Dslr (D), Webcam
(W)}. A→W implies A is source and W is target.

Expt.
GFK
TCA
CORAL
JDA
DAN
DANN
DAH-e
DAH

A→D A→W D→A D→W W→A W→D
93.17
48.59
96.79
51.00
98.59
54.42
97.79
59.24
99.40
67.04
99.40
72.89
96.99
66.27
98.80
66.47

52.08
49.43
51.70
58.62
67.80
72.70
66.16
68.30

41.83
48.12
48.26
51.35
50.36
56.25
55.97
55.54

89.18
93.08
95.97
96.86
95.85
96.48
94.59
96.10

49.04
48.83
47.27
52.34
52.33
53.20
53.91
53.02

Avg.
62.32
64.54
66.04
69.37
72.13
75.15
72.31
73.04

classiﬁed target data samples. We present results with hash
length d = 64 bits. The DAH algorithm consistently out-
performs the baselines across all the domains for the Ofﬁce-
Home dataset. However, DANN marginally surpasses DAH
for the Ofﬁce dataset, prompting us to reason that domain
adversarial training is more effective than DAH when the
categories are fewer in number. Since domain alignment is
category agnostic, it is possible that the aligned domains are
not classiﬁcation friendly in the presence of large number
of categories. When the number of categories is large, as in
Ofﬁce-Home, DAH does best at extracting transferable fea-
tures to achieve higher accuracies. We also note that DAH
delivers better performance than DAH-e; thus, minimizing
the entropy on the target data through 7 aids in improved
alignment of the source and target samples, which boosts
the accuracy.
Feature Analysis: We also study the feature representa-
tions of the penultimate layer (fc7) outputs using t-SNE em-
beddings as in [12]. Figure 3a depicts the A-distance be-
tween domain pairs using Deep (VGG-F), DAN and DAH
[2] deﬁned A-distance as the
features. Ben-David et al.
distance between two domains that can be viewed as the
discrepancy between two domains. Although it is difﬁcult
to estimate its exact value, an approximate distance mea-
sure is given by 2(1 − 2ǫ), where ǫ is the generalization
error for a binary classiﬁer trained to distinguish between
the two domains. We used a LIBLINEAR SVM [15] clas-

Table 3: Recognition accuracies (%) for domain adaptation experiments on the Ofﬁce-Home dataset. {Art (Ar), Clipart (Cl),
Product (Pr), Real-World (Rw)}. Ar→Cl implies Ar is source and Cl is target.

Expt.
GFK
TCA
CORAL
JDA
DAN
DANN
DAH-e
DAH

Ar→Cl Ar→Pr Ar→Rw Cl→Ar Cl→Pr Cl→Rw Pr→Ar Pr→Cl Pr→Rw Rw→Ar Rw→Cl Rw→Pr Avg.
32.40
24.52
21.60
30.34
21.92
19.93
37.91
27.77
27.10
36.97
25.96
25.34
43.46
29.07
30.66
44.94
30.49
33.33
42.69
29.87
29.23
45.54
29.91
31.64

34.20
31.74
40.33
40.90
49.78
49.76
47.49
52.79

34.94
31.36
40.03
40.19
47.59
49.13
48.23
51.93

21.63
19.00
26.08
24.52
32.83
32.26
33.79
34.69

42.92
42.12
50.61
49.25
56.70
56.76
55.63
60.71

25.73
23.64
30.54
32.72
34.05
38.14
38.76
39.63

50.89
48.68
57.11
55.35
62.73
64.65
59.07
62.54

32.88
30.74
38.48
35.10
43.58
44.71
41.16
44.99

38.83
35.71
44.32
42.94
54.13
54.42
48.29
51.73

31.72
32.08
36.16
35.98
42.17
42.96
35.71
40.75

28.96
27.15
36.36
35.35
38.25
42.66
44.99
45.13

siﬁer with 5-fold cross-validation to estimate ǫ. Figure 3a
indicates that the DAH features have the least discrepancy
between the source and target compared to DAN and Deep
features. This is also conﬁrmed with the t-SNE embeddings
in Figures 3b-3d. The Deep features show very little over-
lap between the domains and the categories depict minimal
clustering. Domain overlap and clustering improves as we
move to DAN and DAH features, with DAH providing the
best visualizations. This corroborates the efﬁcacy of the
DAH algorithm to exploit the feature learning capabilities
of deep neural networks to learn representative hash codes
to address domain adaptation.

5.4. Unsupervised Domain Adaptive Hashing

In this section, we study the performance of our algo-
rithm to generate compact and efﬁcient hash codes from the
data for classifying unseen test instances, when no labels
are available. This problem has been addressed in the litera-
ture, with promising empirical results [7, 11, 21]. However,
in a real-world setting, labels may be available from a dif-
ferent, but related (source) domain; a strategy to utilize the
labeled data from the source domain to learn representative
hash codes for the target domain is therefore of immense
practical importance. Our work is the ﬁrst to identify and
address this problem. We consider the following scenar-
ios to address this real-world challenge: (i) No labels are
available for a given dataset and the hash codes need to be
learned in a completely unsupervised manner. We evaluate
against baseline unsupervised hashing methods (ITQ) [22]
and (KMeans) [25] and also state-of-the-art methods for
unsupervised hashing (BA) [7] and (BDNN) [11]. (ii) La-
beled data is available from a different, but related source
domain. A hashing model is trained on the labeled source
data and is used to learn hash codes for the target data. We
refer to this method as NoDA, as no domain adaptation is
performed. We used the deep pairwise-supervised hashing
(DPSH) algorithm [30] to train a deep network with the
source data and applied the network to generate hash codes
for the target data.
(iii) Labeled data is available from a
different, but related source domain and we use our DAH
formulation to learn hash codes for the target domain by
(iv) Labeled data is available
reducing domain disparity.

Table 4: Mean average precision @64 bits. For the NoDA and
DAH results, Art is the source domain for Clipart, Product
and Real-World and Clipart is the source domain for Art.
Similarly, Amazon and Webcam are source target pairs.

Expt.
Amazon
Webcam
Art
Clipart
Product
Real-World
Avg.

NoDA ITQ KMeans
0.465
0.324
0.652
0.511
0.191
0.155
0.195
0.160
0.393
0.239
0.323
0.281
0.370
0.278

0.403
0.558
0.170
0.178
0.341
0.279
0.322

BA
0.367
0.480
0.156
0.179
0.349
0.273
0.301

BDNN DAH SuH
0.830
0.582
0.491
0.939
0.717
0.656
0.492
0.302
0.193
0.622
0.333
0.206
0.774
0.414
0.407
0.586
0.533
0.336
0.707
0.480
0.382

in the target domain. This method falls under supervised
hashing (SuH) (as it uses labeled data in the target domain
to learn hash codes in the same domain) and denotes the
It is included to com-
upper bound on the performance.
pare the performance of unsupervised hashing algorithms
relative to the supervised algorithm. We used the DPSH al-
gorithm [30] to train a deep network on the target data and
used it to generate hash codes on a validation subset.

Results and Discussion: We applied the precision-recall
curves and the mean average precision (mAP) measures to
evaluate the efﬁcacy of the hashing methods, similar to pre-
vious research [7, 11, 21]. The results are depicted in Fig-
ures 4 and 5 (precision-recall curves) and Table 4 (mAP
values), where we present hashing with code length d = 64
bits. Hashing performance with d = 16 bits also follows
a similar trend and is presented in the supplementary mate-
rial. For the sake of brevity, we drop the results with Dslr
as it is very similar to Webcam, with little domain differ-
ence. We note that the NoDA has the poorest performance
due to domain mismatch. This demonstrates that domain
disparity needs to be considered before deploying a hashing
network to extract hash codes. The unsupervised hashing
methods ITQ, KMeans, BA and BDNN perform slightly
better compared to NoDA. The proposed DAH algorithm
encompasses hash code learning and domain adaptation in
a single integrated framework. It is thus able to leverage
the labeled data in the source domain in a meaningful man-
ner to learn efﬁcient hash codes for the target domain. This
accounts for its improved performance, as is evident in Fig-
ures 4 and 5 and Table 4. The supervised hashing technique
(SuH) uses labels from the target and therefore depicts the

e
c
n
a
t
s
i
D
A

-

1.5

0.5

2

1

0

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Deep
DAN
DAH

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Ar -> Cl Ar -> Pr Ar -> Rw

(a) A-Distance

(b) Deep Features (Ar,Cl)

(c) DAN Features (Ar,Cl)

(d) DAH Features (Ar,Cl)

Figure 3: Feature analysis of fc7 layer. (a) A-distances for Deep, DAN and DAH, (b), (c) and (d) t-SNE embeddings for 10 categories
from Art (•) and Clipart(+) domains. Best viewed in color.

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

0.2

0.4

0.6

0.8

1

0.2

0.4

0.6

0.8

1

0.2

0.4

0.6

0.8

1

0.2

0.4

0.6

0.8

1

Recall
 

(a) Art

Recall
 

(b) Clipart

Recall
 

(c) Product

Recall
 

(d) Real-World

Figure 4: Precision-Recall curves @64 bits for the Ofﬁce-Home dataset. Comparison of hashing without domain adaptation (NoDA),
shallow unsupervised hashing (ITQ, KMeans), state-of-the-art deep unsupervised hashing (BA, BDNN), unsupervised domain adaptive
hashing (DAH) and supervised hashing (SuH). Best viewed in color.

real-world setting.

6. Conclusions

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

0.2

0.4

0.6

0.8

1

0.2

0.4

0.6

0.8

1

Recall
 

(a) Amazon

Recall
 

(b) Webcam

Figure 5: Precision-Recall curves @64 bits for the Ofﬁce dataset.
Comparison of hashing without domain adaptation (NoDA), shal-
low unsupervised hashing (ITQ, KMeans), state-of-the-art deep
unsupervised hashing (BA, BDNN), unsupervised domain adap-
tive hashing (DAH) and supervised hashing (SuH). Best viewed
in color.

best performance. The proposed DAH framework consis-
tently delivers the best performance relative to SuH when
compared with the other hashing procedures. This demon-
strates the merit of our framework in learning representa-
tive hash codes by utilizing labeled data from a different
domain. Such a framework will be immensely useful in a

In this paper, we have proposed a novel domain adap-
tive hashing (DAH) framework which exploits the feature
learning capabilities of deep neural networks to learn efﬁ-
cient hash codes for unsupervised domain adaptation. The
DAH framework solves two important practical problems:
category assignment with weak supervision or insufﬁcient
labels (through domain adaptation) and the estimation of
hash codes in an unsupervised setting (hash codes for target
data). Thus, two practical challenges are addressed through
a single integrated framework. This research is the ﬁrst
of its kind to integrate hash code learning with unsuper-
vised domain adaptation. We also introduced a new dataset,
Ofﬁce-Home, which can be used to further research in do-
main adaptation.
Acknowledgements: This material is based upon work
supported by the National Science Foundation (NSF) un-
der Grant No:1116360. Any opinions, ﬁndings, and con-
clusions or recommendations expressed in this material are
those of the authors and do not necessarily reﬂect the views
of the NSF.

References

[1] Y. Aytar and A. Zisserman. Tabula rasa: Model transfer for

object category detection. In IEEE ICCV, 2011. 2

[2] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira,
and J. W. Vaughan. A theory of learning from different do-
mains. Machine learning, 79(1-2):151–175, 2010. 6

[3] Y. Bengio, A. Courville, and P. Vincent. Representation
learning: A review and new perspectives. IEEE transactions
on pattern analysis and machine intelligence, 35(8):1798–
1828, 2013. 2

[4] L. Bruzzone and M. Marconcini. Domain adaptation prob-
lems: A dasvm classiﬁcation technique and a circular valida-
tion strategy. IEEE, PAMI, 32(5):770–787, 2010. 2

[5] Y. Cao, M. Long, J. Wang, Q. Yang, and P. S. Yu. Deep
visual-semantic hashing for cross-modal retrieval. In ACM-
SIGKDD, 2016. 2

[6] Z. Cao, M. Long, and Q. Yang. Transitive hashing network
for heterogeneous multimedia retrieval. In AAAI, 2016. 2
[7] M. A. Carreira-Perpin´an and R. Raziperchikolaei. Hashing
with binary autoencoders. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
557–566, 2015. 2, 7

[8] K. Chatﬁeld, K. Simonyan, A. Vedaldi, and A. Zisserman.
Return of the devil in the details: Delving deep into convo-
lutional nets. In BMVC, 2014. 2, 4

[9] R. Chattopadhyay, Q. Sun, W. Fan, I. Davidson, S. Pan-
chanathan, and J. Ye. Multisource domain adaptation and
its application to early detection of fatigue. ACM Transac-
tions on Knowledge Discovery from Data (TKDD), 6(4):18,
2012. 2

[10] W.-S. Chu, F. De la Torre, and J. F. Cohn. Selective transfer
machine for personalized facial action unit detection. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 3515–3522, 2013. 2

[11] T.-T. Do, A.-D. Doan, and N.-M. Cheung. Learning to hash
with binary deep neural network. In European Conference
on Computer Vision, pages 219–234. Springer, 2016. 2, 7

[12] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,
E. Tzeng, and T. Darrell. Decaf: A deep convolutional acti-
vation feature for generic visual recognition. In ICML, pages
647–655, 2014. 2, 6

[13] L. Duan, I. W. Tsang, and D. Xu. Domain transfer multiple
kernel learning. IEEE PAMI, 34(3):465–479, 2012. 2
[14] V. Erin Liong, J. Lu, G. Wang, P. Moulin, and J. Zhou. Deep
hashing for compact binary codes learning. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 2475–2483, 2015. 2

[15] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-
J. Lin. Liblinear: A library for large linear classiﬁcation.
Journal of machine learning research, 9(Aug):1871–1874,
2008. 5, 6

[16] B. Fernando, A. Habrard, M. Sebban, and T. Tuytelaars. Un-
supervised visual domain adaptation using subspace align-
ment. In CVPR, pages 2960–2967, 2013. 1, 2

[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,
F. Laviolette, M. Marchand, and V. Lempitsky. Domain-

adversarial training of neural networks. Journal of Machine
Learning Research, 17(59):1–35, 2016. 1, 2, 3, 6

[18] X. Glorot, A. Bordes, and Y. Bengio. Domain adaptation
for large-scale sentiment classiﬁcation: A deep learning ap-
proach. In Proceedings of the 28th International Conference
on Machine Learning (ICML-11), pages 513–520, 2011. 1,
2

[19] B. Gong, K. Grauman, and F. Sha. Connecting the dots
with landmarks: Discriminatively learning domain-invariant
features for unsupervised domain adaptation. In ICML (1),
pages 222–230, 2013. 2

[20] B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic ﬂow
kernel for unsupervised domain adaptation. In IEEE CVPR,
2012. 1, 2, 5, 6

[21] Y. Gong and S. Lazebnik.

Iterative quantization: A pro-
In Computer
crustean approach to learning binary codes.
Vision and Pattern Recognition (CVPR), 2011 IEEE Confer-
ence on, pages 817–824. IEEE, 2011. 7

[22] Y. Gong, S. Lazebnik, A. Gordo, and F. Perronnin.

Itera-
tive quantization: A procrustean approach to learning binary
IEEE Transactions
codes for large-scale image retrieval.
on Pattern Analysis and Machine Intelligence, 35(12):2916–
2929, 2013. 2, 4, 7

[23] R. Gopalan, R. Li, and R. Chellappa. Domain adaptation for
object recognition: An unsupervised approach. In 2011 in-
ternational conference on computer vision, pages 999–1006.
IEEE, 2011. 2

[24] A. Gretton, D. Sejdinovic, H. Strathmann, S. Balakrishnan,
M. Pontil, K. Fukumizu, and B. K. Sriperumbudur. Optimal
kernel choice for large-scale two-sample tests. In Advances
in neural information processing systems, pages 1205–1213,
2012. 3, 4, 5, 11

[25] K. He, F. Wen, and J. Sun. K-means hashing: An afﬁnity-
preserving quantization method for learning binary compact
codes. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 2938–2945, 2013. 2, 7
[26] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber.
the difﬁculty of learning

Gradient ﬂow in recurrent nets:
long-term dependencies, 2001. 4

[27] J. Hoffman, E. Rodner, J. Donahue, K. Saenko, and T. Dar-
rell. Efﬁcient learning of domain-invariant image represen-
tations. In ICLR, 2013. 2

[28] K. Jarrett, K. Kavukcuoglu, Y. Lecun, et al. What is the
best multi-stage architecture for object recognition? In 2009
IEEE 12th International Conference on Computer Vision,
pages 2146–2153. IEEE, 2009. 5

[29] Q.-Y. Jiang and W.-J. Li. Deep cross-modal hashing. arXiv

preprint arXiv:1602.02255, 2016. 2

[30] W.-J. Li, S. Wang, and W.-C. Kang. Feature learning based
In IJCAI,

deep supervised hashing with pairwise labels.
2016, 2016. 4, 7

[31] M. Long, Y. Cao, J. Wang, and M. Jordan. Learning transfer-
able features with deep adaptation networks. In ICML, pages
97–105, 2015. 1, 2, 3, 4, 5, 6

[32] M. Long, J. Wang, G. Ding, J. Sun, and P. Yu. Transfer joint
In CVPR,

matching for unsupervised domain adaptation.
pages 1410–1417, 2014. 2

[50] H. Zhu, M. Long, J. Wang, and Y. Cao. Deep hashing net-
work for efﬁcient similarity retrieval. In Thirtieth AAAI Con-
ference on Artiﬁcial Intelligence, 2016. 4

[33] M. Long, J. Wang, G. Ding, J. Sun, and P. S. Yu. Transfer
In Pro-
feature learning with joint distribution adaptation.
ceedings of the IEEE International Conference on Computer
Vision, pages 2200–2207, 2013. 1, 2, 5, 6

[34] M. Long, H. Zhu, J. Wang, and M. I. Jordan. Unsupervised
domain adaptation with residual transfer networks. In NIPS,
2016. 1, 2, 3, 5

[35] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and
I. Matthews. The extended cohn-kanade dataset (ck+): A
complete dataset for action unit and emotion-speciﬁed ex-
pression. In CVPR, pages 94–101. IEEE, 2010. 5

[36] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.
Ng. Reading digits in natural images with unsupervised fea-
ture learning. In NIPS Workshop on Deep Learning and Un-
supervised Feature Learning 2011, 2011. 5

[37] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Learning and
transferring mid-level image representations using convolu-
In Proceedings of the IEEE con-
tional neural networks.
ference on computer vision and pattern recognition, pages
1717–1724, 2014. 2

[38] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain
adaptation via transfer component analysis. Neural Net-
works, IEEE Trans. on, 22(2):199–210, 2011. 1, 2, 6
[39] S. J. Pan and Q. Yang. A survey on transfer learning. IEEE

TKDE, 22(10):1345–1359, 2010. 2

[40] M. Pantic, M. Valstar, R. Rademaker, and L. Maat. Web-
In ICME.

based database for facial expression analysis.
IEEE, 2005. 5

[41] V. M. Patel, R. Gopalan, R. Li, and R. Chellappa. Visual do-
main adaptation: A survey of recent advances. IEEE signal
processing magazine, 32(3):53–69, 2015. 2

[42] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting vi-
sual category models to new domains. In ECCV, 2010. 1, 2,
5

[43] S. Shekhar, V. M. Patel, H. V. Nguyen, and R. Chellappa.
In Proceedings
Generalized domain-adaptive dictionaries.
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 361–368, 2013. 1

[44] B. Sun, J. Feng, and K. Saenko. Return of frustratingly easy
domain adaptation. In ICCV, TASK-CV, 2015. 1, 2, 6
[45] A. Torralba and A. A. Efros. Unbiased look at dataset bias.
In Computer Vision and Pattern Recognition (CVPR), 2011
IEEE Conference on, pages 1521–1528. IEEE, 2011. 5
[46] E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko. Simultane-
ous deep transfer across domains and tasks. In Proceedings
of the IEEE International Conference on Computer Vision,
pages 4068–4076, 2015. 1, 2, 3

[47] A. Vedaldi and K. Lenc. Matconvnet – convolutional neural
networks for matlab. In Proceeding of the ACM Int. Conf. on
Multimedia, 2015. 5

[48] J. Wang, H. T. Shen, J. Song, and J. Ji. Hashing for similarity
search: A survey. arXiv preprint arXiv:1408.2927, 2014. 1
[49] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How trans-
ferable are features in deep neural networks? In Advances
in neural information processing systems, pages 3320–3328,
2014. 2, 3

Supplementary Material

7. Loss Function Derivative

In this section we outline the derivative of Equation 8 for the backpropagation algorithm;

min
U

J = L(U s) + γM(Us, U t) + ηH(U s, U t),

where, U := {Us ∪ Ut} and (γ, η) control the importance of domain adaptation (1) and target entropy loss (7) respectively.
In the following subsections, we outline the derivative of the individual terms w.r.t. the input U.

7.1. Derivative for MK MMD

M(U s, U t) =

d2
k(U

l
s, U

l
t),

Xl∈F

d2
k(U

l
s, U

l
t) =

E[φ(us,l)] − E[φ(ut,l)]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2

.

Hk

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

We implement the linear MK-MMD loss according to [24]. For this derivation, we consider the loss at just one layer. The
derivative for the MK-MMD loss at every other layer can be derived in a similar manner. The output of ith source data point
at layer l is represented as ui and the output of the ith target data point is represented as vi. For ease of representation, we
drop the superscripts for the source (s), the target (t) and the layer (l). Unlike the conventional MMD loss which is O(n2),
the MK-MMD loss outlined in [24] is O(n) and can be estimated online (does not require all the data). The loss is calculated
over every batch of data points during the back-propagation. Let n be the number of source data points U := {ui}n
i=1 and the
number of target data points V := {vi}n
i=1 in the batch. We assume equal number of source and target data points in a batch
and that n is even. The MK-MMD is deﬁned over a set of 4 data points wi = [u2i−1, u2i, v2i−1, v2i], ∀i ∈ {1, 2, . . . , n/2}.
The MK-MMD is given by,

M(U, V) =

hm(wi),

βm

1
n/2

κ

m=1
X

n/2

i=1
X

where, κ is the number of kernels and βm = 1/κ is the weight for each kernel and,

hm(wi) = km(u2i−1, u2i) + km(v2i−1, v2i) − km(u2i−1, v2i) − km(u2i, v2i−1),

(10)

where, km(x, y) = exp

. Re-writing the MK-MMD in terms of the kernels, we have,

− ||x−y||
σm

2
2

(cid:1)
n/2

κ

(cid:0)

2
nκ

m=1
X

i=1
X

(cid:2)

M(U, V) =

km(u2i−1, u2i) + km(v2i−1, v2i) − km(u2i−1, v2i) − km(u2i, v2i−1)

,

(11)

11

(cid:3)

(8)

(1)

(2)

(9)

We now outline the derivative of 11 w.r.t. source output uq and target output vq. The derivative is,

km(u2i−1, u2i).(u2i−1 − u2i).(I{q = 2i} − I{q = 2i − 1})

∂M
∂uq

=

2
nκ

κ

n/2

m=1
X

i=1 h
X

∂M
∂vq

=

2
nκ

κ

n/2

m=1
X

i=1 h
X

2
σm

+

2
σm

2
σm

−

2
σm

km(u2i−1, v2i).(u2i−1 − v2i).I{q = 2i − 1} +

km(u2i, v2i−1).(u2i − v2i−1).I{q = 2i}

,

where, I{.} is the indicator function which is 1 if the condition is true, else it is false. The derivative w.r.t. the target data
output vq is,

km(v2i−1, v2i).(v2i−1 − v2i).(I{q = 2i} − I{q = 2i − 1})

km(u2i−1, v2i).(u2i−1 − v2i).I{q = 2i} −

km(u2i, v2i−1).(u2i − v2i−1).I{q = 2i − 1}

,

2
σm

2
σm

7.2. Derivative for Supervised Hash Loss

The supervised hash loss is given by,

(cid:1)(cid:17)

i

(cid:1)

min
U s

L(U s) = −

siju⊤

i uj − log

1 + exp(u⊤

i uj)

+

ui − sgn(ui)

Xsij ∈S (cid:16)
ns

i=1
X

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:0)
2
2.

(cid:12)
(cid:12)

(cid:12)
(cid:12)

The partial derivative of 5 w.r.t. source data output up is given by,

∂L
∂uq

=

Xsij ∈S h
1

I{i = q}

σ(u⊤

i uj) − sij

uj + I{j = q}

σ(u⊤

i uj) − sij

ui

+ 2(uq − sgn(uq))

(14)

(cid:0)
1+exp(−x) . We assume sgn(.) to be a constant and avoid the differentiability issues with sgn(.) at 0. Since

(cid:0)

(cid:1)

(cid:1)

where, σ(x) =
the S is symmetric, we can reduce the derivative to,

∂L
∂uq

=

ns

2
j=1 h
X

(cid:0)

σ(u⊤

q uj) − sqj

uj

+ 2

uq − sgn(uq)

.

i

(cid:1)

(cid:0)

7.3. Derivative for Unsupervised Entropy Loss

We outline the derivative of dH

dU in the following section, where H is deﬁned as,

H(Us, Ut) = −

pij log(pij)

1
nt

nt

C

i=1
X

j=1
X

and pij is the probability of target data output ut

i belonging to category j, given by

For ease of representation, we will denote the target output ut
P
kth source data point in the jth category usj
the news terms as,

k as uj

pij =

C
P
l=1

K
k=1 exp(ut
i

⊤usj
k )
⊤usl
K
k′=1 exp(ut
k′ )
i
i as vi and drop the superscript t. Similarly, we will denote the
P
k, by dropping the domain superscript. We deﬁne the probability pij with

(6)

(12)

i

(13)

i

(5)

(15)

(7)

(16)

pij =

K
k=1 exp(vi

⊤uj
k)
K
⊤ul
k′=1 exp(vi

k′ )

C
P
l=1

P

P

Further, we simplify by replacing exp(v⊤

i uj

k) with exp(i, jk). Equation 16 can now be represented as,

pij =

K
k=1 exp(i, jk)

C
P
l=1

K
k′=1 exp(i, lk′)

We drop the outer summations (along with the -ve sign) and will reintroduce it at a later time. The entropy loss can be
re-phrased using log( a

P

P

b ) = log(a) - log(b) as,

Hij =

K
k=1 exp(i, jk)

C
P
l=1

K
k′=1 exp(i, lk′)
K
k=1 exp(i, jk)

P
−

P
C
P
l=1
for the target and ∂Hij
∂up
q

P

P

K
k′=1 exp(i, lk′)

log

K
k=1 exp(i, jk)

(cid:0) P
log

C
l=1

(cid:0) P

P

(cid:1)

K
k′=1 exp(i, lk′)
(cid:1)

for the source. We refer to ∂up

q for a consistent reference to source

We need to estimate both, ∂Hij
∂vi
data. The derivative ∂Hij
∂up
q

for 18 is,

∂Hij
∂up

(cid:20)

q (cid:21)18

=

vi
l,k′ exp(i, lk′)

k I{j=p,

k=q }exp(i, jk).log

k exp(i, jk)

+

k I{j=p,

k=q }exp(i, jk)

P

h P

− pijexp(i, pq)log

(cid:0) P
k exp(i, jk)

(cid:1)

P

(cid:0) P

,

(cid:1)i

where, I{.} is an indicator function which is 1 only when both the conditions within are true, else it is 0. The derivative ∂Hij
∂up
q
for 19 is,

∂Hij
∂up

(cid:20)

q (cid:21)19

= −

vi
l,k′ exp(i, lk′)

P

k I{j=p,

k=q }exp(i, jk).log

h P

− pij exp(i, pq)log

l,k′ exp(i, lk′)
(cid:1)

(cid:0) P

l,k′ exp(i, lk′)

+ pijexp(i, pq)

Expressing ∂Hij
∂up
q

=

∂Hij
∂up
q

+

∂Hij
∂up
q

, and deﬁning ¯pijk = exp(i,jk)

(cid:0) P

(cid:1)i
Pl,k′ exp(i,lk′) the derivative w.r.t. the source is,

18
i
=vi

19
i
h
k I{j=p,
k=q }¯pijk.log

h
∂Hij
∂up
q

k exp(i, jk)

+

k I{j=p,

k=q }¯pijk

h P
− pij ¯pipqlog

(cid:0) P
k exp(i, jk)

−

(cid:1)
k I{j=p,
k=q }¯pijk.log

P

− pij ¯pipq + pij ¯pipqlog

(cid:0) P

l,k′ exp(i, lk′)
(cid:1)
P

l,k′ exp(i, lk′)
(cid:1)

(cid:0) P

=vi

k I{j=p,

k=q }¯pijklog(pij ) − pij ¯pipqlog(pij ) +

(cid:0) P

k I{j=p,

k=q }¯pijk − pij ¯pipq

(cid:1)i

=vi

h P
log(pij ) + 1

k I{j=p,

k=q }¯pijk − pij ¯pipq

P

i

The derivative of H w.r.t the source output up

(cid:0)

(cid:1)h P
q is given by,

i

nt

C

∂H
∂up
q

= −

1
nt

i=1
X
for 18 as,

j=1
X

(cid:0)

vi

log(pij) + 1

k I{j=p,

k=q }¯pijk − pij ¯pipq

(cid:1)h P

i

We now outline the derivative ∂H
∂vi

∂Hij
∂vi (cid:21)18

(cid:20)

=

1
l,k′ exp(i, lk′)

P

k exp(i, jk)

k exp(i, jk)uj

k +

k exp(i, jk)uj

k

(cid:0) P

1
l,k′ exp(i, lk′)

(cid:1) P

k exp(i, jk)log

P
k exp(i, jk)

P

(cid:0) P

(cid:1) P

l,k′ exp(i, lk′)ul
k′

,

(26)

i

log

h

−

P

(17)

(18)

(19)

(20)

(21)

(22)

(23)

(24)

(25)

and the derivative ∂H
∂vi

for 19 as,

∂Hij
∂vi (cid:21)19

(cid:20)

= −

1
l,k′ exp(i, lk′)

P

log

h

−

P
19
i

l,k′ exp(i, lk′)

k exp(i, jk)uj

k + Pk exp(i,jk)
Pl,k′ exp(i,lk′)

l,k′ exp(i, lk′)ul
k′

(cid:0) P

1
l,k′ exp(i, lk′)

(cid:1) P

k exp(i, jk)log

l,k′ exp(i, lk′)

l,k′ exp(i, lk′)ul
k′

,

(27)

P

(cid:0) P

i

P

(cid:1) P

Expressing ∂Hij
∂vi

=

∂Hij
∂vi

+

∂Hij
∂vi

, we get,

∂Hij
∂vi

=

18
i

h

h

1
l,k′ exp(i, lk′)

log

k exp(i, jk)

k exp(i, jk)uj

k − log

l,k′ exp(i, lk′)

k exp(i, jk)uj

k

(cid:0) P

(cid:1) P

h
(cid:0) P
k exp(i, jk)uj
k − pij

+
P

− pij log
P

k exp(i, jk)

P

(cid:1) P

l,k′ exp(i, lk′)ul
k′
l,k′ exp(i, lk′)ul

=

log

(cid:0) P
k exp(i, jk)

h
+

k ¯pijkuj
(cid:0) P

k − pij

(cid:1) P

k − log

(cid:1) P
k ¯pijkuj
l,k′ ¯pijk′ ul
k′
l,k′ ¯pijk′ ul

k′ + pijlog

l,k′ exp(i, lk′)

(cid:0) P

l,k′ exp(i, lk′)
k ¯pijkuj

k

(cid:1) P

l,k′ exp(i, lk′)ul
k′

(28)

i

(cid:0) P

(cid:1) P

− pij log
P

=

(cid:0) P
log(pij) + 1

=

log(pij) + 1
(cid:0)

P

k exp(i, jk)
k ¯pijkuj
(cid:1) P
k −
k ¯pijkuj
k − pij
(cid:0)

(cid:1) P

log(pij) + 1

pij
l,k′ ¯pijk′ ul
(cid:1)
P
k′

k′ + pijlog

l,k′ exp(i, lk′)

l,k′ ¯pijk′ ul
k′

l,k′ ¯pijk′ ul
(cid:0) P
k′

(cid:1) P

i

The derivative of H w.r.t. target output vq is given by,
P

(cid:1)(cid:0) P

(cid:0)

(cid:1)

∂H
∂vq

= −

1
nt

C

j=1
X

(cid:0)

log(pqj) + 1

k ¯pqjkuj

k − pqj

l,k′ ¯pqjk′ ul
k′

The derivative of H w.r.t. the source outputs is given by 25 and w.r.t. the target outputs is given by 32.

(cid:1)(cid:0) P

P

(cid:1)

8. Unsupervised Domain Adaptation: Additional Results

(29)

(30)

(31)

(32)

In the main paper we had presented results for unsupervised domain adaptation based object recognition with d = 64 bits.
Here, we outline the classiﬁcation results with d = 16 (DAH-16) and d = 128 (DAH-128) bits for the Ofﬁce-Home dataset
in Table 5. We also present the (DAH-64), DAN and DANN results for comparison. There is an increase in the average
recognition accuracy for d = 128 bits compared to d = 64 bits because of the increased capacity in representation. As
expected, d = 16 has a lower recognition accuracy.
Table 5: Recognition accuracies (%) for domain adaptation experiments on the Ofﬁce-Home dataset. {Art (Ar), Clipart (Cl),
Product (Pr), Real-World (Rw)}. Ar→Cl implies Ar is source and Cl is target.

Expt.
DAN
DANN
DAH-16
DAH-64
DAH-128

Ar→Cl Ar→Pr Ar→Rw Cl→Ar Cl→Pr Cl→Rw Pr→Ar Pr→Cl Pr→Rw Rw→Ar Rw→Cl Rw→Pr Avg.
43.46
29.07
30.66
44.94
30.49
33.33
31.36
20.11
23.83
45.54
29.91
31.64
46.26
30.94
32.58

42.17
42.96
30.32
40.75
40.64

54.13
54.42
40.14
51.73
52.40

62.73
64.65
37.46
62.54
64.97

43.58
44.71
32.63
44.99
45.65

32.83
32.26
25.67
34.69
35.72

34.05
38.14
27.72
39.63
41.31

49.78
49.76
33.26
52.79
52.12

47.59
49.13
38.79
51.93
52.80

56.70
56.76
40.90
60.71
59.31

38.25
42.66
25.54
45.13
46.67

9. Unsupervised Domain Adaptive Hashing: Additional Results

We provide the unsupervised domain adaptive hashing results for d = 16 and d = 128 bits in Figures 6 and 7 respectively.
In Tables 6 and 7, we outline the corresponding mAP values. The notations are along the lines outlined in the main paper. We
observe similar trends for both d = 16 and d = 128 bits compared to d = 64 bits. It is interesting to note that with increase
in bit size d, the mAP does not necessarily increase. Table 7 (d = 64) has its mAP values lower than those for d = 64 (see
main paper) for all the hashing methods. This indicates that merely increasing the hash code length does not always improve
mAP scores. Also, the mAP values for Real-World for d = 128 bits has DAH performing better than SuH. This indicates
that in some cases domain adaptation helps in learning a better generalized model.

Table 6: Mean average precision @16 bits. For the NoDA and DAH results, Art is the source domain for Clipart, Product and
Real-World and Clipart is the source domain for Art.

Table 7: Mean average precision @128 bits. For the NoDA and DAH results, Art is the source domain for Clipart, Product and
Real-World and Clipart is the source domain for Art.

Expt.
Art
Clipart
Product
Real-World
Avg.

NoDA ITQ KMeans
0.147
0.102
0.120
0.110
0.253
0.134
0.225
0.193
0.186
0.135

0.133
0.116
0.241
0.195
0.171

BA
0.131
0.123
0.253
0.216
0.181

BDNN DAH SuH
0.381
0.207
0.151
0.412
0.211
0.138
0.459
0.257
0.313
0.400
0.371
0.248
0.413
0.262
0.212

Expt.
Art
Clipart
Product
Real-World
Avg.

NoDA ITQ KMeans
0.202
0.154
0.210
0.186
0.416
0.279
0.343
0.308
0.293
0.232

0.175
0.196
0.356
0.289
0.254

BA
0.148
0.187
0.336
0.258
0.232

BDNN DAH SuH
0.444
0.314
0.207
0.346
0.350
0.213
0.792
0.424
0.432
0.458
0.544
0.348
0.510
0.408
0.300

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.5
Recall
 

(a) Art

0.5
Recall
 

(b) Clipart

0.5
Recall
 

(c) Product

0.5
Recall
 

(d) Real-World

Figure 6: Precision-Recall curves @16 bits for the Ofﬁce-Home dataset. Comparison of hashing without domain adaptation (NoDA),
shallow unsupervised hashing (ITQ, KMeans), state-of-the-art deep unsupervised hashing (BA, BDNN), unsupervised domain adaptive
hashing (DAH) and supervised hashing (SuH). Best viewed in color.

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

n
o
i
s
i
c
e
r
P

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

n
o
i
s
i
c
e
r
P

0.5

0.4

0.9

0.8

0.7

0.6

0.3

0.2

0.1

0

0

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

n
o
i
s
i
c
e
r
P

0.5

0.4

0.9

0.8

0.7

0.6

0.3

0.2

0.1

0

0

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.5
Recall
 

(a) Art

0.5
Recall
 

(b) Clipart

0.5
Recall
 

(c) Product

0.5
Recall
 

(d) Real-World

Figure 7: Precision-Recall curves @128 bits for the Ofﬁce-Home dataset. Comparison of hashing without domain adaptation (NoDA),
shallow unsupervised hashing (ITQ, KMeans), state-of-the-art deep unsupervised hashing (BA, BDNN), unsupervised domain adaptive
hashing (DAH) and supervised hashing (SuH). Best viewed in color.

7
1
0
2
 
n
u
J
 
2
2
 
 
]

V
C
.
s
c
[
 
 
1
v
2
2
5
7
0
.
6
0
7
1
:
v
i
X
r
a

Deep Hashing Network for Unsupervised Domain Adaptation

Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, Sethuraman Panchanathan
Center for Cognitive Ubiquitous Computing, Arizona State University, Tempe, AZ, USA
{hemanthv, jeusebio, shayok.chakraborty, panch}@asu.edu

Abstract

In recent years, deep neural networks have emerged as a
dominant machine learning tool for a wide variety of appli-
cation domains. However, training a deep neural network
requires a large amount of labeled data, which is an expen-
sive process in terms of time, labor and human expertise.
Domain adaptation or transfer learning algorithms address
this challenge by leveraging labeled data in a different, but
related source domain, to develop a model for the target
domain. Further, the explosive growth of digital data has
posed a fundamental challenge concerning its storage and
retrieval. Due to its storage and retrieval efﬁciency, recent
years have witnessed a wide application of hashing in a
variety of computer vision applications. In this paper, we
ﬁrst introduce a new dataset, Ofﬁce-Home, to evaluate do-
main adaptation algorithms. The dataset contains images
of a variety of everyday objects from multiple domains. We
then propose a novel deep learning framework that can ex-
ploit labeled source data and unlabeled target data to learn
informative hash codes, to accurately classify unseen tar-
get data. To the best of our knowledge, this is the ﬁrst
research effort to exploit the feature learning capabilities
of deep neural networks to learn representative hash codes
to address the domain adaptation problem. Our extensive
empirical studies on multiple transfer tasks corroborate the
usefulness of the framework in learning efﬁcient hash codes
which outperform existing competitive baselines for unsu-
pervised domain adaptation.

1. Introduction

Deep learning algorithms automatically learn a discrim-
inating set of features and have depicted commendable per-
formance in a variety of computer vision applications. Un-
fortunately, training a deep model necessitates a large vol-
ume of labeled data, which can be time consuming and ex-
pensive to acquire. However, labeled data from a differ-
ent, but related domain is often available, which has mo-
tivated the development of algorithms which can leverage

labeled data in a source domain to develop a machine learn-
ing model for the target domain. Learning a discrimina-
tive model in the presence of the shift between training and
test distributions is known as transfer learning or domain
adaptation [17]. Unsupervised domain adaptation is a chal-
lenging setting, where labeled data is available only in the
source domain; no labeled data is available in the target
domain. Conventional shallow transfer learning methods
develop their models in two stages, feature extraction fol-
lowed by domain adaptation. The features are ﬁxed and
then a model is trained to align the source and target do-
mains [16, 20, 33, 38, 42, 43, 44]. On the other hand, deep
transfer learning procedures exploit the feature learning ca-
pabilities of deep networks to learn transferable feature rep-
resentations for domain adaptation and have demonstrated
impressive empirical performance [17, 18, 31, 34, 46].

The explosive growth of digital data in the modern era
has posed fundamental challenges regarding their storage,
retrieval and computational requirements. Against this
backdrop, hashing has emerged as one of the most popu-
lar and effective techniques due to its fast query speed and
low memory cost [48]. Hashing techniques transform high
dimensional data into compact binary codes and generate
similar binary codes for similar data items. Motivated by
this fact, we propose to train a deep neural network to out-
put binary hash codes (instead of probability values), which
can be used for classiﬁcation. We see two advantages to es-
timating a hash value instead of a standard probability vec-
tor in the ﬁnal layer of the network: (i) the hash values are
used to develop a unique loss function for target data in the
absence of labels and (ii) during prediction, the hash value
of a test sample can be compared against the hash values
of the training samples to arrive at a more robust category
prediction.

In this paper, we ﬁrst introduce a new dataset, Ofﬁce-
Home, which we use to evaluate our algorithm. The Ofﬁce-
Home dataset is an object recognition dataset which con-
tains images from 4 domains. It has around 15, 500 images
organized into 65 categories. We further propose a novel
deep learning framework called Domain Adaptive Hash-

1

ing (DAH) to learn informative hash codes to address the
problem of unsupervised domain adaptation. We propose
a unique loss function to train the deep network with the
following components: (i) supervised hash loss for labeled
source data, which ensures that source samples belonging
to the same class have similar hash codes; (ii) unsuper-
vised entropy loss for unlabeled target data, which imposes
each target sample to align closely with exactly one of the
source categories and be distinct from the other categories
and (iii) a loss based on multi-kernel Maximum Mean Dis-
crepancy (MK-MMD), which seeks to learn transferable
features within the layers of the network to minimize the
distribution difference between the source and target do-
mains. Figure 1 illustrates the different layers of the DAH
and the components of the loss function.

2. Related Work

There have been many approaches to address the prob-
lem of domain-shift in unsupervised domain adaptation.
One straightforward approach is,
to modify a classiﬁer
trained for the source data by adapting it to classify target
data [1, 4] or learn a transformation matrix to linearly trans-
form the source data, so that it is aligned with the target
[27, 42]. Some other procedures re-weight the data points
in the source domain, to select source data that is similar
to the target, when training a domain adaptive classiﬁer,
[9, 10, 19]. A standard procedure to reduce domain discrep-
ancy is, to project the source and target data to a common
subspace, thereby aligning their principal axes [16, 44].
Reducing domain disparity through nonlinear alignment of
data has been possible with Maximum Mean Discrepancy
(MMD) - a measure that provides the distribution differ-
ence between two datasets in a reproducing-kernel Hilbert
space [13]. Kernel-PCA based methods apply the MMD to
achieve nonlinear alignment of domains [32, 33, 38]. Man-
ifold based approaches are also popular in domain adapta-
tion for computer vision, where the subspace of a domain is
treated as a point on the manifold and transformations are
learned to align two domains [20, 23]. A survey of popular
domain adaptation techniques for computer vision is pro-
vided in [41] and a more generic survey of transfer learning
approaches can be found in [39].

All of the above techniques can be termed as shallow
learning procedures, since the models are learned using pre-
determined features. In recent years deep learning has be-
come very successful at learning highly discriminative fea-
tures for computer vision applications [8]. Deep learning
systems like deep CNNs learn representations of data that
capture underlying factors of variation between different
tasks in a multi-task transfer learning setting [3]. These rep-
resentations also disentangle the factors of variation allow-
ing for the transfer of knowledge between tasks [12, 18, 37].
Yosinski et al. [49] demonstrated how the lower layers of a

network produce generic features and the upper layers out-
put task speciﬁc features. Based on this, deep learning pro-
cedures for domain adaptation train networks to learn trans-
ferable features in the fully connected ﬁnal layers of a net-
work [31, 46]. In other approaches to deep domain adapta-
tion, Ganin et al. [17] trained domain adversarial networks
to learn features that make the source and target domain in-
distinguishable and Long et al. [34], trained a network to
do both feature adaptation and classiﬁer adaptation using
residual transfer networks.

Unsupervised hashing techniques have been developed
to extract unique hash codes for efﬁcient storage and re-
trieval of data [22, 25]. Neural network based hashing has
led the way in state-of-the-art unsupervised hashing tech-
niques [7, 11, 14]. The closest work incorporating hash-
ing and adaptation appears in cross-modal hashing, where
deep hashing techniques embed multi-modal data and learn
hash codes for two related domains, like text and images
[5, 6, 29]. However, these algorithms are not unsupervised
and they are mainly applied to extract common hash codes
for multi-modal data for retrieval purposes. To the best of
our knowledge, there has been no work in unsupervised
domain adaptation using deep hashing networks. We now
present the Domain Adaptive Hashing (DAH) network for
unsupervised domain adaptation through deep hashing.

3. Domain Adaptive Hashing Networks

i }ns

In unsupervised domain adaptation, we consider data
from two domains; source and target. The source consists
i , ys
of labeled data, Ds = {xs
i=1 and the target has only
i}nt
i=1. The data points x∗
unlabeled data Dt = {xt
i belong to
X, where X is some input space. The corresponding labels
are represented by y∗
i ∈ Y := {1, . . . , C}. The paradigm of
domain adaptive learning attempts to address the problem of
domain-shift in the data, where the data distributions of the
source and target are different, i.e. Ps(X, Y ) 6= Pt(X, Y ).
The domain-shift notwithstanding, our goal is to train a
deep neural network classiﬁer ψ(.), that can predict the la-
bels {ˆyt

i=1, for the target data.

i }nt

We implement the neural network as a deep CNN which
consists of 5 convolution layers conv1 - conv5 and 3 fully
connected layers fc6 - fc8 followed by a loss layer. In our
model, we introduce a hashing layer hash-fc8 in place of
the standard fc8 layer to learn a binary code hi, for every
data point xi, where hi ∈ {−1, +1}d. The hash-fc8 layer
is driven by two loss functions, (i) supervised hash loss for
the source data, (ii) unsupervised entropy loss for the target
data. The supervised hash loss ensures hash values that are
distinct and discriminatory, i.e. if xi and xj belong to the
same category, their hash values hi and hj are similar and
different otherwise. The unsupervised entropy loss aligns
the target hash values with source hash values based on the
similarity of their feature representations. The output of the

is the output representation of x∗

layer l, where u∗,l
i for the
i
lth layer. The ﬁnal layer outputs are denoted as Us and U t.
The MK-MMD measure d2
k(.) is the multi-kernel maximum
mean discrepancy between the source and target representa-
tions, [24]. For a nonlinear mapping φ(.) associated with a
reproducing kernel Hilbert space Hk and kernel k(.), where
k(x, y) = hφ(x), φ(y)i, the MMD is deﬁned as,

2

.

Hk

(2)

d2
k(U

l
s, U

l
t) =

E[φ(us,l)] − E[φ(ut,l)]
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
The characteristic kernel k(.), is determined as a convex
(cid:12)
combination of κ PSD kernels, {km}κ
m=1, K :=
k : k =
κ
. We set
m=1 βm = 1, βm ≥ 0, ∀m
(cid:8)
βm = 1/κ according to [34] and it works well in practice.
P
3.2. Supervised Hashing for Source Data

κ
m=1 βmkm,

P

(cid:12)
(cid:12)
(cid:12)

(cid:9)

2 (d − h⊤

The Hamming distance for a pair of hash values hi and
hj has a unique relationship with the dot product hhi, hj i,
given by: distH (hi, hj) = 1
i hj), where d is the
hash length. The dot product hhi, hji can be treated as
a similarity measure for the hash codes. Larger the value
of the dot product (high similarity), smaller is the distance
distH and smaller the dot product (low similarity), larger is
the distance distH . Let sij ∈ {0, 1} be the similarity be-
tween xi and xj. If xi and xj belong to the same category,
sij = 1 and 0, otherwise. The probability of similarity be-
tween xi and xj given the corresponding hash values hi
and hj, can be expressed as a likelihood function, given by,

p(sij |hi, hj) =

σ(h⊤
i hj),
1 − σ(h⊤

(

i hj),

sij = 1
sij = 0,

(3)

1

1+e−x is the sigmoid function. As the
where, σ(x) =
dot product hhi, hji increases, the probability of p(sij =
1|hi, hj ) also increases, i.e., xi and xj belong to the same
category. As the dot product decreases, the probability
p(sij = 1|hi, hj) also decreases, i.e., xi and xj belong
to different categories. We construct the (ns × ns) similar-
ity matrix S = {sij}, for the source data with the provided
labels, where sij = 1 if xi and xj belong to the same cat-
egory and 0, otherwise. Let H = {hi}ns
i=1 be the set of
source data hash values. If the elements of H are assumed
to be i.i.d., the negative log likelihood of the similarity ma-
trix S given H can be written as,

L(H) = −log p(S|H)

min
H

= −

sij h⊤

i hj − log

1 + exp(h⊤

i hj )

.

Xsij ∈S (cid:16)

(cid:0)

(cid:1)(cid:17)
(4)

Figure 1: The Domain Adaptive Hash (DAH) network that out-
puts hash codes for the source and the target. The network is
trained with a batch of source and target data. The convolution
layers conv1 - conv5 and the fully connected layers fc6 and fc7 are
ﬁne tuned from the VGG-F network. The MK-MMD loss trains
the DAH to learn feature representations which align the source
and the target. The hash-fc8 layer is trained to output vectors of d
dimensions. The supervised hash loss drives the DAH to estimate
a unique hash value for each object category. The unsupervised
entropy loss aligns the target hash values to their corresponding
source categories. Best viewed in color.
network is represented as ψ(x), where ψ(x) ∈ Rd, which
we convert to a hash code h = sgn(ψ(x)), where sgn(.)
is the sign function. Once the network has been trained,
the probability of x being assigned a label y is given by
f (x) = p(y|h). We train the network using Ds and Dt and
predict the target data labels ˆyt

∗ using f (.).

In order to address the issue of domain-shift, we need to
align the feature representations of the target and the source.
We do that by reducing the domain discrepancy between the
source and target feature representations at multiple layers
of the network. In the following subsections, we discuss
the design of the domain adaptive hash (DAH) network in
detail.

3.1. Reducing Domain Disparity

Deep learning methods have been very successful in do-
main adaptation with state-of-the-art algorithms [17, 31, 34,
46] in recent years. The feature representations transition
from generic to task-speciﬁc as one goes up the layers of
a deep CNN [49]. The convolution layers conv1 to conv5
have been shown to be generic and so, readily transferable,
whereas the fully connected layers are more task-speciﬁc
and need to be adapted before they can be transferred. In
the DAH algorithm, we attempt to minimize the MK-MMD
loss to reduce the domain difference between the source
and target feature representations for fully connected lay-
ers, F = {fc6, fc7, fc8}. Such a loss function has been used
in previous research [31, 34]. The multi-layer MK-MMD
loss is given by,

M(U s, U t) =

d2
k(U

l
s, U

l
t),

(1)

l

where, U
i=1 are the set
of output representations for the source and target data at

s = {us,l

i }ns

t = {ut,l

i }nt

By minimizing Equation (4), we can determine hash val-
ues H for the source data which are consistent with the

Xl∈F
l
i=1 and U

similarity matrix S. The hash loss has been used in pre-
vious research for supervised hashing [30, 50]. Equation
(4) is a discrete optimization problem that is challenging to
solve. We introduce a relaxation on the discrete constraint
hi ∈ {−1, +1}d by instead solving for ui ∈ Rd, where
Us = {ui}ns
i=1 is the output of the network and ui = ψ(xi)
(the superscript denoting the domain has been dropped for
ease of representation). However, the continuous relaxation
gives rise to (i) approximation error, when hhi, hj i is sub-
stituted with hui, uji and, (ii) quantization error, when the
resulting real codes ui are binarized [50]. We account for
the approximation error by having a tanh(.) as the ﬁnal ac-
tivation layer of the neural network, so that the components
of ui are bounded between −1 and +1. In addition, we also
introduce a quantization loss ||ui − sgn(ui)||2
2 along the
lines of [22], where sgn(.) is the sign function. The contin-
uous optimization problem for supervised hashing can now
be outlined;

min
U s

L(U s) = −

sij u⊤

i uj − log

1 + exp(u⊤

i uj)

+

ui − sgn(ui)

Xsij ∈S (cid:16)
ns

i=1
X

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:0)
2
2.

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:1)(cid:17)
(5)

3.3. Unsupervised Hashing for Target Data

k=1

{usj

k }K

In the absence of target data labels, we use the similarity
measure hui, uji, to guide the network to learn discrimina-
tive hash values for the target data. An ideal target output
ut
i, needs to be similar to many of the source outputs from
the jth category
. We assume without loss of
generality, K source data points for every category j where,
(cid:1)
j ∈ {1, . . . , C} and usj
k is the kth source output from cat-
egory j. In addition, ut
i must be dissimilar to most other
source outputs usl
k belonging to a different category (j 6= l).
Enforcing similarity with all the K data points makes for a
more robust target data category assignment. We outline
a probability measure to capture this intuition. Let pij be
the probability that input target data point xi is assigned to
category j where,

(cid:0)

pij =

K
k=1 exp(ut
i
K
k=1 exp(ut
i

⊤usj
k )
⊤usl
k )

C
P
l=1

(6)

P

P

The exp(.) has been introduced for ease of differentiabil-
j pij = 1. When the
ity and the denominator ensures
target data point output is similar to one category only and
dissimilar to all the other categories, the probability vec-
tor pi = [pi1, . . . , piC ]T tends to be a one-hot vector. A
one-hot vector can be viewed as a low entropy realization
of pi. We can therefore envisage all the pi to be one-hot
vectors (low entropy probability vectors), where the target
data point outputs are similar to source data point outputs in
one and only one category. To this end we introduce a loss

P

to capture the entropy of the target probability vectors. The
entropy loss for the network outputs is given by,

H(Us, Ut) = −

pijlog(pij)

(7)

1
nt

nt

C

i=1
X

j=1
X

Minimizing the entropy loss gives us probability vectors pi
that tend to be one-hot vectors, i.e., the target data point
outputs are similar to source data point outputs from any
one category only. Enforcing similarity with K source data
points from a category, guarantees that the hash values are
determined based on a common similarity between multiple
source category data points and the target data point.

3.4. Domain Adaptive Hash Network

We propose a model for deep unsupervised domain adap-
tation based on hashing (DAH) that incorporates unsuper-
vised domain adaptation between the source and the target
(1), supervised hashing for the source (5) and unsupervised
hashing for the target (7) in a deep convolutional neural net-
work. The DAH network is trained to minimize

min
U

J = L(U s) + γM(Us, U t) + ηH(U s, U t),

(8)

where, U := {U s ∪ Ut} and (γ, η) control the importance
of domain adaptation (1) and target entropy loss (7) respec-
tively. The hash values H are obtained from the output of
the network using H = sgn(U). The loss terms (5) and
(7) are determined in the ﬁnal layer of the network with the
network output U. The MK-MMD loss (1) is determined
l
l
between layer outputs {U
t} at each of the fully con-
s, U
nected layers F = {fc6, fc7, fc8}, where we adopt the lin-
ear time estimate for the unbiased MK-MMD as described
in [24] and [31]. The DAH is trained using standard back-
propagation. The detailed derivation of the derivative of (8)
w.r.t. U is provided in the supplementary material.
Network Architecture: Owing to the paucity of images
in a domain adaptation setting, we circumvent the need to
train a deep CNN with millions of images by adapting the
pre-trained VGG-F [8] network to the DAH. The VGG-F
has been trained on the ImageNet 2012 dataset and it con-
sists of 5 convolution layers (conv1 - conv5) and 3 fully
connected layers (fc6, fc7, fc8). We introduce the hashing
layer hash-fc8 that outputs vectors in Rd in the place of fc8.
To account for the hashing approximation, we introduced
a tanh() layer. However, we encounter the issue of van-
ishing gradients [26] when using tanh() as it saturates with
large inputs. We therefore preface the tanh() with a batch
normalization layer which prevents the tanh() from saturat-
ing. In effect, hash-fc8 := {fc8 → batch-norm → tanh()}.
The hash-fc8 provides greater stability when ﬁne-tuning the
learning rates than the deep hashing networks [30, 50]. Fig-
ure 1 illustrates the proposed DAH network.

Table 1: Statistics for the Ofﬁce-Home dataset. Min: # is the
minimum number of images amongst all the categories, Min: Size
and Max: Size are the minimum and maximum image sizes across
all categories and Acc. is the classiﬁcation accuracy.

Domain.
Art
Clipart
Product
Real-World

Min: # Min: Size
117×85 pix.
18×18 pix.
75×63 pix.
88×80 pix.

15
39
38
23

Max: Size
4384×2686 pix.
2400×2400 pix.
2560×2560 pix.
6500×4900 pix.

Acc
44.99±1.85
53.95±1.45
66.41±1.18
59.70±1.04

4. The Ofﬁce-Home Dataset

Supervised deep learning models require a large volume
of labeled training data. Unfortunately, existing datasets
for vision-based domain adaptation are limited in their
size and are not suitable for validating deep learning al-
gorithms. The standard datasets for vision based domain
adaptation are, facial expression datasets CKPlus [35] and
MMI [40], digit datasets SVHN [36], USPS and MNIST[28],
head pose recognition datasets PIE [33], object recogni-
tion datasets COIL[33], Ofﬁce [42] and Ofﬁce-Caltech [20].
These datasets were created before deep-learning became
popular and are insufﬁcient for training and evaluating deep
learning based domain adaptation approaches. For instance,
the object-recognition dataset Ofﬁce has 4110 images across
31 categories and Ofﬁce-Caltech has 2533 images across 10
categories.

We release the Ofﬁce-Home dataset for domain adap-
tation based object recognition, that can be used to evalu-
ate deep learning algorithms for domain adaptation. The
Ofﬁce-Home dataset consists of 4 domains, with each do-
main containing images from 65 categories of everyday ob-
jects and a total of around 15, 500 images. The domains
include, Art: artistic depictions of objects in the form of
sketches, paintings, ornamentation, etc.; Clipart: collec-
tion of clipart images; Product: images of objects with-
out a background, akin to the Amazon category in Ofﬁce
dataset; Real-World: images of objects captured with a
regular camera.

Public domain images were downloaded from web-
sites like www.deviantart.com and www.ﬂickr.com to cre-
ate the Art and Real-World domains. Clipart im-
ages were gathered from multiple clipart websites. The
Product domain images were exclusively collected from
www.amazon.com using web-crawlers. The collected im-
ages were manually ﬁltered on the basis of quality, size and
content. The dataset has an average of around 70 images
per category and a maximum of 99 images in a category.
The primary challenge in creating this dataset was acquir-
ing sufﬁcient number of public domain images across all
the 4 domains. Figure 2 depicts a sampling of 16 categories
from the Ofﬁce-Home dataset and Table 1 outlines some
meta data for the dataset. The Acc. column in the Table
1 refers to classiﬁcation accuracies using the LIBLINEAR
SVM [15] classiﬁer (5-fold cross validation) with deep fea-

tures extracted using the VGG-F network. The dataset is
publicly available for research 1.

5. Experiments

In this section we conduct extensive experiments to
evaluate the DAH algorithm. Since we propose a do-
main adaptation technique based on hashing, we evalu-
ate objection recognition accuracies for unsupervised do-
main adaptation and also study the discriminatory capabil-
ity of the learned hash codes for unsupervised domain adap-
tive hashing. The implementation details are available at
https://github.com/hemanthdv/da-hash

5.1. Datasets

Ofﬁce [42]: This is currently the most popular benchmark
dataset for object recognition in the domain adaptation com-
puter vision community. The dataset consists of images of
everyday objects in an ofﬁce environment. It has 3 domains;
Amazon (A), Dslr (D) and Webcam (W). The dataset has
around 4, 100 images with a majority of the images (2816
images) in the Amazon domain. We adopt the common
evaluation protocol of different pairs of transfer tasks for
this dataset [31, 34]. We consider 6 transfer tasks for all
combinations of source and target pairs for the 3 domains.
Ofﬁce-Home: We introduce this new dataset and evaluate
it in a similar manner to the Ofﬁce dataset. We consider 12
transfer tasks for the Art (Ar), Clipart (Cl), Product
(Pr) and Real-World (Rw) domains for all combinations
of source and target for the 4 domains. Considering all the
different pairs of transfer enables us to evaluate the inherent
bias between the domains in a comprehensive manner [45].

5.2. Implementation Details

We implement the DAH using the MatConvnet frame-
work [47]. Since we train a pre-trained VGG-F, we ﬁne-
tune the weights of conv1-conv5, fc6 and fc7. We set
their learning rates to 1/10th the learning rate of hash-fc8.
We vary the learning rate between 10−4 to 10−5 over 300
epochs with a momentum 0.9 and weight decay 5 × 10−4.
We set K = 5 (number of samples from a category). Since
we have 31 categories in the Ofﬁce dataset, we get a source
batch size of 31 × 5 = 155. For the target batch, we ran-
domly select 155 samples. The total batch size turns out to
be 310. For the Ofﬁce-Home dataset, with K = 5 and 65
categories, we get a batch size of 650. We set d = 64 (hash
code length) for all our experiments. Since there is imbal-
ance in the number of like and unlike pairs in S, we set the
values in similarity matrix Si,j ∈ {0, 10}. Increasing the
similarity weight of like-pairs improves the performance of
DAH. For the entropy loss, we set η = 1. For the MK-
MMD loss, we follow the heuristics mentioned in [24], to

1https://hemanthdv.github.io/officehome-dataset/

Figure 2: Sample images from the Ofﬁce-Home dataset. The dataset consists of images of everyday objects organized into 4 domains;
Art: paintings, sketches and/or artistic depictions, Clipart: clipart images, Product: images without background and Real-World:
regular images captured with a camera. The ﬁgure displays examples from 16 of the 65 categories.

determine the parameters. We estimate γ, by validating a
binary domain classiﬁer to distinguish between source and
target data points and select γ which gives largest error on a
validation set. For MMD, we use a Gaussian kernel with a
bandwidth σ given by the median of the pairwise distances
in the training data. To incorporate the multi-kernel, we
vary the bandwidth σm ∈ [2−8σ, 28σ] with a multiplicative
factor of 2. We deﬁne the target classiﬁer f (xt
i) = p(y|ht
i)
in terms of 6. The target data point is assigned to the class
with the largest probability, with ˆyi = maxj(pij ) using the
hash codes for the source and the target.

5.3. Unsupervised Domain Adaptation

In this section, we study the performance of the DAH
for unsupervised domain adaptation, where labeled data is
available only in the source domain and no labeled data is
available in the target domain. We compare the DAH with
state-of-the-art domain adaptation methods: (i) Geodesic
Flow Kernel (GFK) [20], (ii) Transfer Component Analy-
sis (TCA) [38], (iii) Correlation Alignment (CORAL) [44]
and (iv) Joint Distribution Adaptation (JDA) [33]. We also
compare the DAH with state-of-the-art deep learning meth-
ods for domain adaptation: (v) Deep Adaptation Network
(DAN) [31] and (vi) Domain Adversarial Neural Network
(DANN) [17]. For all of the shallow learning methods,
we extract and use deep features from the fc7 layer of the
VGG-F network that was pre-trained on the ImageNet 2012
dataset. We also evaluate the effect of the entropy loss on
hashing for the DAH. The DAH-e is the DAH algorithm
where η is set to zero, which implies that the target hash
values are not driven to align with the source categories.
We follow the standard protocol for unsupervised domain
adaptation, where all the labeled source data and all the un-
labeled target data is used for training.
Results and Discussion: The results are reported for the
target classiﬁcation in each of the transfer tasks in Tables 2
and 3, where accuracies denote the percentage of correctly

Table 2: Recognition accuracies (%) for domain adaptation exper-
iments on the Ofﬁce dataset. {Amazon (A), Dslr (D), Webcam
(W)}. A→W implies A is source and W is target.

Expt.
GFK
TCA
CORAL
JDA
DAN
DANN
DAH-e
DAH

A→D A→W D→A D→W W→A W→D
93.17
48.59
96.79
51.00
98.59
54.42
97.79
59.24
99.40
67.04
99.40
72.89
96.99
66.27
98.80
66.47

52.08
49.43
51.70
58.62
67.80
72.70
66.16
68.30

41.83
48.12
48.26
51.35
50.36
56.25
55.97
55.54

89.18
93.08
95.97
96.86
95.85
96.48
94.59
96.10

49.04
48.83
47.27
52.34
52.33
53.20
53.91
53.02

Avg.
62.32
64.54
66.04
69.37
72.13
75.15
72.31
73.04

classiﬁed target data samples. We present results with hash
length d = 64 bits. The DAH algorithm consistently out-
performs the baselines across all the domains for the Ofﬁce-
Home dataset. However, DANN marginally surpasses DAH
for the Ofﬁce dataset, prompting us to reason that domain
adversarial training is more effective than DAH when the
categories are fewer in number. Since domain alignment is
category agnostic, it is possible that the aligned domains are
not classiﬁcation friendly in the presence of large number
of categories. When the number of categories is large, as in
Ofﬁce-Home, DAH does best at extracting transferable fea-
tures to achieve higher accuracies. We also note that DAH
delivers better performance than DAH-e; thus, minimizing
the entropy on the target data through 7 aids in improved
alignment of the source and target samples, which boosts
the accuracy.
Feature Analysis: We also study the feature representa-
tions of the penultimate layer (fc7) outputs using t-SNE em-
beddings as in [12]. Figure 3a depicts the A-distance be-
tween domain pairs using Deep (VGG-F), DAN and DAH
[2] deﬁned A-distance as the
features. Ben-David et al.
distance between two domains that can be viewed as the
discrepancy between two domains. Although it is difﬁcult
to estimate its exact value, an approximate distance mea-
sure is given by 2(1 − 2ǫ), where ǫ is the generalization
error for a binary classiﬁer trained to distinguish between
the two domains. We used a LIBLINEAR SVM [15] clas-

Table 3: Recognition accuracies (%) for domain adaptation experiments on the Ofﬁce-Home dataset. {Art (Ar), Clipart (Cl),
Product (Pr), Real-World (Rw)}. Ar→Cl implies Ar is source and Cl is target.

Expt.
GFK
TCA
CORAL
JDA
DAN
DANN
DAH-e
DAH

Ar→Cl Ar→Pr Ar→Rw Cl→Ar Cl→Pr Cl→Rw Pr→Ar Pr→Cl Pr→Rw Rw→Ar Rw→Cl Rw→Pr Avg.
32.40
24.52
21.60
30.34
21.92
19.93
37.91
27.77
27.10
36.97
25.96
25.34
43.46
29.07
30.66
44.94
30.49
33.33
42.69
29.87
29.23
45.54
29.91
31.64

34.20
31.74
40.33
40.90
49.78
49.76
47.49
52.79

34.94
31.36
40.03
40.19
47.59
49.13
48.23
51.93

21.63
19.00
26.08
24.52
32.83
32.26
33.79
34.69

25.73
23.64
30.54
32.72
34.05
38.14
38.76
39.63

42.92
42.12
50.61
49.25
56.70
56.76
55.63
60.71

50.89
48.68
57.11
55.35
62.73
64.65
59.07
62.54

32.88
30.74
38.48
35.10
43.58
44.71
41.16
44.99

38.83
35.71
44.32
42.94
54.13
54.42
48.29
51.73

31.72
32.08
36.16
35.98
42.17
42.96
35.71
40.75

28.96
27.15
36.36
35.35
38.25
42.66
44.99
45.13

siﬁer with 5-fold cross-validation to estimate ǫ. Figure 3a
indicates that the DAH features have the least discrepancy
between the source and target compared to DAN and Deep
features. This is also conﬁrmed with the t-SNE embeddings
in Figures 3b-3d. The Deep features show very little over-
lap between the domains and the categories depict minimal
clustering. Domain overlap and clustering improves as we
move to DAN and DAH features, with DAH providing the
best visualizations. This corroborates the efﬁcacy of the
DAH algorithm to exploit the feature learning capabilities
of deep neural networks to learn representative hash codes
to address domain adaptation.

5.4. Unsupervised Domain Adaptive Hashing

In this section, we study the performance of our algo-
rithm to generate compact and efﬁcient hash codes from the
data for classifying unseen test instances, when no labels
are available. This problem has been addressed in the litera-
ture, with promising empirical results [7, 11, 21]. However,
in a real-world setting, labels may be available from a dif-
ferent, but related (source) domain; a strategy to utilize the
labeled data from the source domain to learn representative
hash codes for the target domain is therefore of immense
practical importance. Our work is the ﬁrst to identify and
address this problem. We consider the following scenar-
ios to address this real-world challenge: (i) No labels are
available for a given dataset and the hash codes need to be
learned in a completely unsupervised manner. We evaluate
against baseline unsupervised hashing methods (ITQ) [22]
and (KMeans) [25] and also state-of-the-art methods for
unsupervised hashing (BA) [7] and (BDNN) [11]. (ii) La-
beled data is available from a different, but related source
domain. A hashing model is trained on the labeled source
data and is used to learn hash codes for the target data. We
refer to this method as NoDA, as no domain adaptation is
performed. We used the deep pairwise-supervised hashing
(DPSH) algorithm [30] to train a deep network with the
source data and applied the network to generate hash codes
for the target data.
(iii) Labeled data is available from a
different, but related source domain and we use our DAH
formulation to learn hash codes for the target domain by
(iv) Labeled data is available
reducing domain disparity.

Table 4: Mean average precision @64 bits. For the NoDA and
DAH results, Art is the source domain for Clipart, Product
and Real-World and Clipart is the source domain for Art.
Similarly, Amazon and Webcam are source target pairs.

Expt.
Amazon
Webcam
Art
Clipart
Product
Real-World
Avg.

NoDA ITQ KMeans
0.465
0.324
0.652
0.511
0.191
0.155
0.195
0.160
0.393
0.239
0.323
0.281
0.370
0.278

0.403
0.558
0.170
0.178
0.341
0.279
0.322

BA
0.367
0.480
0.156
0.179
0.349
0.273
0.301

BDNN DAH SuH
0.830
0.582
0.491
0.939
0.717
0.656
0.492
0.302
0.193
0.622
0.333
0.206
0.774
0.414
0.407
0.586
0.533
0.336
0.707
0.480
0.382

in the target domain. This method falls under supervised
hashing (SuH) (as it uses labeled data in the target domain
to learn hash codes in the same domain) and denotes the
It is included to com-
upper bound on the performance.
pare the performance of unsupervised hashing algorithms
relative to the supervised algorithm. We used the DPSH al-
gorithm [30] to train a deep network on the target data and
used it to generate hash codes on a validation subset.

Results and Discussion: We applied the precision-recall
curves and the mean average precision (mAP) measures to
evaluate the efﬁcacy of the hashing methods, similar to pre-
vious research [7, 11, 21]. The results are depicted in Fig-
ures 4 and 5 (precision-recall curves) and Table 4 (mAP
values), where we present hashing with code length d = 64
bits. Hashing performance with d = 16 bits also follows
a similar trend and is presented in the supplementary mate-
rial. For the sake of brevity, we drop the results with Dslr
as it is very similar to Webcam, with little domain differ-
ence. We note that the NoDA has the poorest performance
due to domain mismatch. This demonstrates that domain
disparity needs to be considered before deploying a hashing
network to extract hash codes. The unsupervised hashing
methods ITQ, KMeans, BA and BDNN perform slightly
better compared to NoDA. The proposed DAH algorithm
encompasses hash code learning and domain adaptation in
a single integrated framework. It is thus able to leverage
the labeled data in the source domain in a meaningful man-
ner to learn efﬁcient hash codes for the target domain. This
accounts for its improved performance, as is evident in Fig-
ures 4 and 5 and Table 4. The supervised hashing technique
(SuH) uses labels from the target and therefore depicts the

e
c
n
a
t
s
i
D
A

-

1.5

0.5

2

1

0

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Deep
DAN
DAH

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Ar -> Cl Ar -> Pr Ar -> Rw

(a) A-Distance

(b) Deep Features (Ar,Cl)

(c) DAN Features (Ar,Cl)

(d) DAH Features (Ar,Cl)

Figure 3: Feature analysis of fc7 layer. (a) A-distances for Deep, DAN and DAH, (b), (c) and (d) t-SNE embeddings for 10 categories
from Art (•) and Clipart(+) domains. Best viewed in color.

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

0.2

0.4

0.6

0.8

1

0.2

0.4

0.6

0.8

1

0.2

0.4

0.6

0.8

1

0.2

0.4

0.6

0.8

1

Recall
 

(a) Art

Recall
 

(b) Clipart

Recall
 

(c) Product

Recall
 

(d) Real-World

Figure 4: Precision-Recall curves @64 bits for the Ofﬁce-Home dataset. Comparison of hashing without domain adaptation (NoDA),
shallow unsupervised hashing (ITQ, KMeans), state-of-the-art deep unsupervised hashing (BA, BDNN), unsupervised domain adaptive
hashing (DAH) and supervised hashing (SuH). Best viewed in color.

real-world setting.

6. Conclusions

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

0.2

0.4

0.6

0.8

1

0.2

0.4

0.6

0.8

1

Recall
 

(a) Amazon

Recall
 

(b) Webcam

Figure 5: Precision-Recall curves @64 bits for the Ofﬁce dataset.
Comparison of hashing without domain adaptation (NoDA), shal-
low unsupervised hashing (ITQ, KMeans), state-of-the-art deep
unsupervised hashing (BA, BDNN), unsupervised domain adap-
tive hashing (DAH) and supervised hashing (SuH). Best viewed
in color.

best performance. The proposed DAH framework consis-
tently delivers the best performance relative to SuH when
compared with the other hashing procedures. This demon-
strates the merit of our framework in learning representa-
tive hash codes by utilizing labeled data from a different
domain. Such a framework will be immensely useful in a

In this paper, we have proposed a novel domain adap-
tive hashing (DAH) framework which exploits the feature
learning capabilities of deep neural networks to learn efﬁ-
cient hash codes for unsupervised domain adaptation. The
DAH framework solves two important practical problems:
category assignment with weak supervision or insufﬁcient
labels (through domain adaptation) and the estimation of
hash codes in an unsupervised setting (hash codes for target
data). Thus, two practical challenges are addressed through
a single integrated framework. This research is the ﬁrst
of its kind to integrate hash code learning with unsuper-
vised domain adaptation. We also introduced a new dataset,
Ofﬁce-Home, which can be used to further research in do-
main adaptation.
Acknowledgements: This material is based upon work
supported by the National Science Foundation (NSF) un-
der Grant No:1116360. Any opinions, ﬁndings, and con-
clusions or recommendations expressed in this material are
those of the authors and do not necessarily reﬂect the views
of the NSF.

References

[1] Y. Aytar and A. Zisserman. Tabula rasa: Model transfer for

object category detection. In IEEE ICCV, 2011. 2

[2] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira,
and J. W. Vaughan. A theory of learning from different do-
mains. Machine learning, 79(1-2):151–175, 2010. 6

[3] Y. Bengio, A. Courville, and P. Vincent. Representation
learning: A review and new perspectives. IEEE transactions
on pattern analysis and machine intelligence, 35(8):1798–
1828, 2013. 2

[4] L. Bruzzone and M. Marconcini. Domain adaptation prob-
lems: A dasvm classiﬁcation technique and a circular valida-
tion strategy. IEEE, PAMI, 32(5):770–787, 2010. 2

[5] Y. Cao, M. Long, J. Wang, Q. Yang, and P. S. Yu. Deep
visual-semantic hashing for cross-modal retrieval. In ACM-
SIGKDD, 2016. 2

[6] Z. Cao, M. Long, and Q. Yang. Transitive hashing network
for heterogeneous multimedia retrieval. In AAAI, 2016. 2
[7] M. A. Carreira-Perpin´an and R. Raziperchikolaei. Hashing
with binary autoencoders. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
557–566, 2015. 2, 7

[8] K. Chatﬁeld, K. Simonyan, A. Vedaldi, and A. Zisserman.
Return of the devil in the details: Delving deep into convo-
lutional nets. In BMVC, 2014. 2, 4

[9] R. Chattopadhyay, Q. Sun, W. Fan, I. Davidson, S. Pan-
chanathan, and J. Ye. Multisource domain adaptation and
its application to early detection of fatigue. ACM Transac-
tions on Knowledge Discovery from Data (TKDD), 6(4):18,
2012. 2

[10] W.-S. Chu, F. De la Torre, and J. F. Cohn. Selective transfer
machine for personalized facial action unit detection. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 3515–3522, 2013. 2

[11] T.-T. Do, A.-D. Doan, and N.-M. Cheung. Learning to hash
with binary deep neural network. In European Conference
on Computer Vision, pages 219–234. Springer, 2016. 2, 7

[12] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,
E. Tzeng, and T. Darrell. Decaf: A deep convolutional acti-
vation feature for generic visual recognition. In ICML, pages
647–655, 2014. 2, 6

[13] L. Duan, I. W. Tsang, and D. Xu. Domain transfer multiple
kernel learning. IEEE PAMI, 34(3):465–479, 2012. 2
[14] V. Erin Liong, J. Lu, G. Wang, P. Moulin, and J. Zhou. Deep
hashing for compact binary codes learning. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 2475–2483, 2015. 2

[15] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-
J. Lin. Liblinear: A library for large linear classiﬁcation.
Journal of machine learning research, 9(Aug):1871–1874,
2008. 5, 6

[16] B. Fernando, A. Habrard, M. Sebban, and T. Tuytelaars. Un-
supervised visual domain adaptation using subspace align-
ment. In CVPR, pages 2960–2967, 2013. 1, 2

[17] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,
F. Laviolette, M. Marchand, and V. Lempitsky. Domain-

adversarial training of neural networks. Journal of Machine
Learning Research, 17(59):1–35, 2016. 1, 2, 3, 6

[18] X. Glorot, A. Bordes, and Y. Bengio. Domain adaptation
for large-scale sentiment classiﬁcation: A deep learning ap-
proach. In Proceedings of the 28th International Conference
on Machine Learning (ICML-11), pages 513–520, 2011. 1,
2

[19] B. Gong, K. Grauman, and F. Sha. Connecting the dots
with landmarks: Discriminatively learning domain-invariant
features for unsupervised domain adaptation. In ICML (1),
pages 222–230, 2013. 2

[20] B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic ﬂow
kernel for unsupervised domain adaptation. In IEEE CVPR,
2012. 1, 2, 5, 6

[21] Y. Gong and S. Lazebnik.

Iterative quantization: A pro-
In Computer
crustean approach to learning binary codes.
Vision and Pattern Recognition (CVPR), 2011 IEEE Confer-
ence on, pages 817–824. IEEE, 2011. 7

[22] Y. Gong, S. Lazebnik, A. Gordo, and F. Perronnin.

Itera-
tive quantization: A procrustean approach to learning binary
IEEE Transactions
codes for large-scale image retrieval.
on Pattern Analysis and Machine Intelligence, 35(12):2916–
2929, 2013. 2, 4, 7

[23] R. Gopalan, R. Li, and R. Chellappa. Domain adaptation for
object recognition: An unsupervised approach. In 2011 in-
ternational conference on computer vision, pages 999–1006.
IEEE, 2011. 2

[24] A. Gretton, D. Sejdinovic, H. Strathmann, S. Balakrishnan,
M. Pontil, K. Fukumizu, and B. K. Sriperumbudur. Optimal
kernel choice for large-scale two-sample tests. In Advances
in neural information processing systems, pages 1205–1213,
2012. 3, 4, 5, 11

[25] K. He, F. Wen, and J. Sun. K-means hashing: An afﬁnity-
preserving quantization method for learning binary compact
codes. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 2938–2945, 2013. 2, 7
[26] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber.
the difﬁculty of learning

Gradient ﬂow in recurrent nets:
long-term dependencies, 2001. 4

[27] J. Hoffman, E. Rodner, J. Donahue, K. Saenko, and T. Dar-
rell. Efﬁcient learning of domain-invariant image represen-
tations. In ICLR, 2013. 2

[28] K. Jarrett, K. Kavukcuoglu, Y. Lecun, et al. What is the
best multi-stage architecture for object recognition? In 2009
IEEE 12th International Conference on Computer Vision,
pages 2146–2153. IEEE, 2009. 5

[29] Q.-Y. Jiang and W.-J. Li. Deep cross-modal hashing. arXiv

preprint arXiv:1602.02255, 2016. 2

[30] W.-J. Li, S. Wang, and W.-C. Kang. Feature learning based
In IJCAI,

deep supervised hashing with pairwise labels.
2016, 2016. 4, 7

[31] M. Long, Y. Cao, J. Wang, and M. Jordan. Learning transfer-
able features with deep adaptation networks. In ICML, pages
97–105, 2015. 1, 2, 3, 4, 5, 6

[32] M. Long, J. Wang, G. Ding, J. Sun, and P. Yu. Transfer joint
In CVPR,

matching for unsupervised domain adaptation.
pages 1410–1417, 2014. 2

[50] H. Zhu, M. Long, J. Wang, and Y. Cao. Deep hashing net-
work for efﬁcient similarity retrieval. In Thirtieth AAAI Con-
ference on Artiﬁcial Intelligence, 2016. 4

[33] M. Long, J. Wang, G. Ding, J. Sun, and P. S. Yu. Transfer
In Pro-
feature learning with joint distribution adaptation.
ceedings of the IEEE International Conference on Computer
Vision, pages 2200–2207, 2013. 1, 2, 5, 6

[34] M. Long, H. Zhu, J. Wang, and M. I. Jordan. Unsupervised
domain adaptation with residual transfer networks. In NIPS,
2016. 1, 2, 3, 5

[35] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and
I. Matthews. The extended cohn-kanade dataset (ck+): A
complete dataset for action unit and emotion-speciﬁed ex-
pression. In CVPR, pages 94–101. IEEE, 2010. 5

[36] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.
Ng. Reading digits in natural images with unsupervised fea-
ture learning. In NIPS Workshop on Deep Learning and Un-
supervised Feature Learning 2011, 2011. 5

[37] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Learning and
transferring mid-level image representations using convolu-
In Proceedings of the IEEE con-
tional neural networks.
ference on computer vision and pattern recognition, pages
1717–1724, 2014. 2

[38] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain
adaptation via transfer component analysis. Neural Net-
works, IEEE Trans. on, 22(2):199–210, 2011. 1, 2, 6
[39] S. J. Pan and Q. Yang. A survey on transfer learning. IEEE

TKDE, 22(10):1345–1359, 2010. 2

[40] M. Pantic, M. Valstar, R. Rademaker, and L. Maat. Web-
In ICME.

based database for facial expression analysis.
IEEE, 2005. 5

[41] V. M. Patel, R. Gopalan, R. Li, and R. Chellappa. Visual do-
main adaptation: A survey of recent advances. IEEE signal
processing magazine, 32(3):53–69, 2015. 2

[42] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting vi-
sual category models to new domains. In ECCV, 2010. 1, 2,
5

[43] S. Shekhar, V. M. Patel, H. V. Nguyen, and R. Chellappa.
In Proceedings
Generalized domain-adaptive dictionaries.
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 361–368, 2013. 1

[44] B. Sun, J. Feng, and K. Saenko. Return of frustratingly easy
domain adaptation. In ICCV, TASK-CV, 2015. 1, 2, 6
[45] A. Torralba and A. A. Efros. Unbiased look at dataset bias.
In Computer Vision and Pattern Recognition (CVPR), 2011
IEEE Conference on, pages 1521–1528. IEEE, 2011. 5
[46] E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko. Simultane-
ous deep transfer across domains and tasks. In Proceedings
of the IEEE International Conference on Computer Vision,
pages 4068–4076, 2015. 1, 2, 3

[47] A. Vedaldi and K. Lenc. Matconvnet – convolutional neural
networks for matlab. In Proceeding of the ACM Int. Conf. on
Multimedia, 2015. 5

[48] J. Wang, H. T. Shen, J. Song, and J. Ji. Hashing for similarity
search: A survey. arXiv preprint arXiv:1408.2927, 2014. 1
[49] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How trans-
ferable are features in deep neural networks? In Advances
in neural information processing systems, pages 3320–3328,
2014. 2, 3

Supplementary Material

7. Loss Function Derivative

In this section we outline the derivative of Equation 8 for the backpropagation algorithm;

min
U

J = L(U s) + γM(Us, U t) + ηH(U s, U t),

where, U := {Us ∪ Ut} and (γ, η) control the importance of domain adaptation (1) and target entropy loss (7) respectively.
In the following subsections, we outline the derivative of the individual terms w.r.t. the input U.

7.1. Derivative for MK MMD

M(U s, U t) =

d2
k(U

l
s, U

l
t),

Xl∈F

d2
k(U

l
s, U

l
t) =

E[φ(us,l)] − E[φ(ut,l)]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2

.

Hk

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

We implement the linear MK-MMD loss according to [24]. For this derivation, we consider the loss at just one layer. The
derivative for the MK-MMD loss at every other layer can be derived in a similar manner. The output of ith source data point
at layer l is represented as ui and the output of the ith target data point is represented as vi. For ease of representation, we
drop the superscripts for the source (s), the target (t) and the layer (l). Unlike the conventional MMD loss which is O(n2),
the MK-MMD loss outlined in [24] is O(n) and can be estimated online (does not require all the data). The loss is calculated
over every batch of data points during the back-propagation. Let n be the number of source data points U := {ui}n
i=1 and the
number of target data points V := {vi}n
i=1 in the batch. We assume equal number of source and target data points in a batch
and that n is even. The MK-MMD is deﬁned over a set of 4 data points wi = [u2i−1, u2i, v2i−1, v2i], ∀i ∈ {1, 2, . . . , n/2}.
The MK-MMD is given by,

M(U, V) =

hm(wi),

βm

1
n/2

κ

m=1
X

n/2

i=1
X

where, κ is the number of kernels and βm = 1/κ is the weight for each kernel and,

hm(wi) = km(u2i−1, u2i) + km(v2i−1, v2i) − km(u2i−1, v2i) − km(u2i, v2i−1),

(10)

where, km(x, y) = exp

. Re-writing the MK-MMD in terms of the kernels, we have,

− ||x−y||
σm

2
2

(cid:1)
n/2

κ

(cid:0)

2
nκ

m=1
X

i=1
X

(cid:2)

M(U, V) =

km(u2i−1, u2i) + km(v2i−1, v2i) − km(u2i−1, v2i) − km(u2i, v2i−1)

,

(11)

11

(cid:3)

(8)

(1)

(2)

(9)

We now outline the derivative of 11 w.r.t. source output uq and target output vq. The derivative is,

km(u2i−1, u2i).(u2i−1 − u2i).(I{q = 2i} − I{q = 2i − 1})

∂M
∂uq

=

2
nκ

κ

n/2

m=1
X

i=1 h
X

∂M
∂vq

=

2
nκ

κ

n/2

m=1
X

i=1 h
X

2
σm

+

2
σm

2
σm

−

2
σm

km(u2i−1, v2i).(u2i−1 − v2i).I{q = 2i − 1} +

km(u2i, v2i−1).(u2i − v2i−1).I{q = 2i}

,

where, I{.} is the indicator function which is 1 if the condition is true, else it is false. The derivative w.r.t. the target data
output vq is,

km(v2i−1, v2i).(v2i−1 − v2i).(I{q = 2i} − I{q = 2i − 1})

km(u2i−1, v2i).(u2i−1 − v2i).I{q = 2i} −

km(u2i, v2i−1).(u2i − v2i−1).I{q = 2i − 1}

,

2
σm

2
σm

7.2. Derivative for Supervised Hash Loss

The supervised hash loss is given by,

(cid:1)(cid:17)

i

(cid:1)

min
U s

L(U s) = −

siju⊤

i uj − log

1 + exp(u⊤

i uj)

+

ui − sgn(ui)

Xsij ∈S (cid:16)
ns

i=1
X

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:0)
2
2.

(cid:12)
(cid:12)

(cid:12)
(cid:12)

The partial derivative of 5 w.r.t. source data output up is given by,

∂L
∂uq

=

Xsij ∈S h
1

I{i = q}

σ(u⊤

i uj) − sij

uj + I{j = q}

σ(u⊤

i uj) − sij

ui

+ 2(uq − sgn(uq))

(14)

(cid:0)
1+exp(−x) . We assume sgn(.) to be a constant and avoid the differentiability issues with sgn(.) at 0. Since

(cid:1)

(cid:0)

(cid:1)

where, σ(x) =
the S is symmetric, we can reduce the derivative to,

∂L
∂uq

=

ns

2
j=1 h
X

(cid:0)

σ(u⊤

q uj) − sqj

uj

+ 2

uq − sgn(uq)

.

i

(cid:1)

(cid:0)

7.3. Derivative for Unsupervised Entropy Loss

We outline the derivative of dH

dU in the following section, where H is deﬁned as,

H(Us, Ut) = −

pij log(pij)

1
nt

nt

C

i=1
X

j=1
X

and pij is the probability of target data output ut

i belonging to category j, given by

For ease of representation, we will denote the target output ut
P
kth source data point in the jth category usj
the news terms as,

k as uj

pij =

C
P
l=1

K
k=1 exp(ut
i

⊤usj
k )
⊤usl
K
k′=1 exp(ut
k′ )
i
i as vi and drop the superscript t. Similarly, we will denote the
P
k, by dropping the domain superscript. We deﬁne the probability pij with

(6)

(12)

i

(13)

i

(5)

(15)

(7)

(16)

pij =

K
k=1 exp(vi

⊤uj
k)
K
⊤ul
k′=1 exp(vi

k′ )

C
P
l=1

P

P

Further, we simplify by replacing exp(v⊤

i uj

k) with exp(i, jk). Equation 16 can now be represented as,

pij =

K
k=1 exp(i, jk)

C
P
l=1

K
k′=1 exp(i, lk′)

We drop the outer summations (along with the -ve sign) and will reintroduce it at a later time. The entropy loss can be
re-phrased using log( a

P

P

b ) = log(a) - log(b) as,

Hij =

K
k=1 exp(i, jk)

C
P
l=1

K
k′=1 exp(i, lk′)
K
k=1 exp(i, jk)

P
−

P
C
P
l=1
for the target and ∂Hij
∂up
q

P

P

K
k′=1 exp(i, lk′)

log

K
k=1 exp(i, jk)

(cid:0) P
log

C
l=1

(cid:0) P

P

(cid:1)

K
k′=1 exp(i, lk′)
(cid:1)

for the source. We refer to ∂up

q for a consistent reference to source

We need to estimate both, ∂Hij
∂vi
data. The derivative ∂Hij
∂up
q

for 18 is,

∂Hij
∂up

(cid:20)

q (cid:21)18

=

vi
l,k′ exp(i, lk′)

k I{j=p,

k=q }exp(i, jk).log

k exp(i, jk)

+

k I{j=p,

k=q }exp(i, jk)

P

h P

− pijexp(i, pq)log

(cid:0) P
k exp(i, jk)

(cid:1)

P

(cid:0) P

,

(cid:1)i

where, I{.} is an indicator function which is 1 only when both the conditions within are true, else it is 0. The derivative ∂Hij
∂up
q
for 19 is,

∂Hij
∂up

(cid:20)

q (cid:21)19

= −

vi
l,k′ exp(i, lk′)

P

k I{j=p,

k=q }exp(i, jk).log

h P

− pij exp(i, pq)log

l,k′ exp(i, lk′)
(cid:1)

(cid:0) P

l,k′ exp(i, lk′)

+ pijexp(i, pq)

Expressing ∂Hij
∂up
q

=

∂Hij
∂up
q

+

∂Hij
∂up
q

, and deﬁning ¯pijk = exp(i,jk)

(cid:0) P

(cid:1)i
Pl,k′ exp(i,lk′) the derivative w.r.t. the source is,

18
i
=vi

19
i
h
k I{j=p,
k=q }¯pijk.log

h
∂Hij
∂up
q

k exp(i, jk)

+

k I{j=p,

k=q }¯pijk

h P
− pij ¯pipqlog

(cid:0) P
k exp(i, jk)

−

(cid:1)
k I{j=p,
k=q }¯pijk.log

P

− pij ¯pipq + pij ¯pipqlog

(cid:0) P

l,k′ exp(i, lk′)
(cid:1)
P

l,k′ exp(i, lk′)
(cid:1)

(cid:0) P

=vi

k I{j=p,

k=q }¯pijklog(pij ) − pij ¯pipqlog(pij ) +

(cid:0) P

k I{j=p,

k=q }¯pijk − pij ¯pipq

(cid:1)i

=vi

h P
log(pij ) + 1

k I{j=p,

k=q }¯pijk − pij ¯pipq

P

i

The derivative of H w.r.t the source output up

(cid:0)

(cid:1)h P
q is given by,

i

nt

C

∂H
∂up
q

= −

1
nt

i=1
X
for 18 as,

j=1
X

(cid:0)

vi

log(pij) + 1

k I{j=p,

k=q }¯pijk − pij ¯pipq

(cid:1)h P

i

We now outline the derivative ∂H
∂vi

∂Hij
∂vi (cid:21)18

(cid:20)

=

1
l,k′ exp(i, lk′)

P

k exp(i, jk)

k exp(i, jk)uj

k +

k exp(i, jk)uj

k

(cid:0) P

1
l,k′ exp(i, lk′)

(cid:1) P

k exp(i, jk)log

P
k exp(i, jk)

P

(cid:0) P

(cid:1) P

l,k′ exp(i, lk′)ul
k′

,

(26)

i

log

h

−

P

(17)

(18)

(19)

(20)

(21)

(22)

(23)

(24)

(25)

and the derivative ∂H
∂vi

for 19 as,

∂Hij
∂vi (cid:21)19

(cid:20)

= −

1
l,k′ exp(i, lk′)

P

log

h

−

P
19
i

l,k′ exp(i, lk′)

k exp(i, jk)uj

k + Pk exp(i,jk)
Pl,k′ exp(i,lk′)

l,k′ exp(i, lk′)ul
k′

(cid:0) P

1
l,k′ exp(i, lk′)

(cid:1) P

k exp(i, jk)log

l,k′ exp(i, lk′)

l,k′ exp(i, lk′)ul
k′

,

(27)

P

(cid:0) P

i

P

(cid:1) P

Expressing ∂Hij
∂vi

=

∂Hij
∂vi

+

∂Hij
∂vi

, we get,

∂Hij
∂vi

=

18
i

h

h

1
l,k′ exp(i, lk′)

log

k exp(i, jk)

k exp(i, jk)uj

k − log

l,k′ exp(i, lk′)

k exp(i, jk)uj

k

(cid:0) P

(cid:1) P

h
(cid:0) P
k exp(i, jk)uj
k − pij

+
P

− pij log
P

k exp(i, jk)

P

(cid:1) P

l,k′ exp(i, lk′)ul
k′
l,k′ exp(i, lk′)ul

=

log

(cid:0) P
k exp(i, jk)

h
+

k ¯pijkuj
(cid:0) P

k − pij

(cid:1) P

k − log

(cid:1) P
k ¯pijkuj
l,k′ ¯pijk′ ul
k′
l,k′ ¯pijk′ ul

k′ + pijlog

l,k′ exp(i, lk′)

(cid:0) P

l,k′ exp(i, lk′)
k ¯pijkuj

k

(cid:1) P

l,k′ exp(i, lk′)ul
k′

(28)

i

(cid:0) P

(cid:1) P

− pij log
P

=

(cid:0) P
log(pij) + 1

=

log(pij) + 1
(cid:0)

P

k exp(i, jk)
k ¯pijkuj
(cid:1) P
k −
k ¯pijkuj
k − pij
(cid:0)

(cid:1) P

log(pij) + 1

pij
l,k′ ¯pijk′ ul
(cid:1)
P
k′

k′ + pijlog

l,k′ exp(i, lk′)

l,k′ ¯pijk′ ul
k′

l,k′ ¯pijk′ ul
(cid:0) P
k′

(cid:1) P

i

The derivative of H w.r.t. target output vq is given by,
P

(cid:1)(cid:0) P

(cid:0)

(cid:1)

∂H
∂vq

= −

1
nt

C

j=1
X

(cid:0)

log(pqj) + 1

k ¯pqjkuj

k − pqj

l,k′ ¯pqjk′ ul
k′

The derivative of H w.r.t. the source outputs is given by 25 and w.r.t. the target outputs is given by 32.

(cid:1)(cid:0) P

P

(cid:1)

8. Unsupervised Domain Adaptation: Additional Results

(29)

(30)

(31)

(32)

In the main paper we had presented results for unsupervised domain adaptation based object recognition with d = 64 bits.
Here, we outline the classiﬁcation results with d = 16 (DAH-16) and d = 128 (DAH-128) bits for the Ofﬁce-Home dataset
in Table 5. We also present the (DAH-64), DAN and DANN results for comparison. There is an increase in the average
recognition accuracy for d = 128 bits compared to d = 64 bits because of the increased capacity in representation. As
expected, d = 16 has a lower recognition accuracy.
Table 5: Recognition accuracies (%) for domain adaptation experiments on the Ofﬁce-Home dataset. {Art (Ar), Clipart (Cl),
Product (Pr), Real-World (Rw)}. Ar→Cl implies Ar is source and Cl is target.

Expt.
DAN
DANN
DAH-16
DAH-64
DAH-128

Ar→Cl Ar→Pr Ar→Rw Cl→Ar Cl→Pr Cl→Rw Pr→Ar Pr→Cl Pr→Rw Rw→Ar Rw→Cl Rw→Pr Avg.
43.46
29.07
30.66
44.94
30.49
33.33
31.36
20.11
23.83
45.54
29.91
31.64
46.26
30.94
32.58

42.17
42.96
30.32
40.75
40.64

54.13
54.42
40.14
51.73
52.40

62.73
64.65
37.46
62.54
64.97

43.58
44.71
32.63
44.99
45.65

32.83
32.26
25.67
34.69
35.72

47.59
49.13
38.79
51.93
52.80

49.78
49.76
33.26
52.79
52.12

34.05
38.14
27.72
39.63
41.31

56.70
56.76
40.90
60.71
59.31

38.25
42.66
25.54
45.13
46.67

9. Unsupervised Domain Adaptive Hashing: Additional Results

We provide the unsupervised domain adaptive hashing results for d = 16 and d = 128 bits in Figures 6 and 7 respectively.
In Tables 6 and 7, we outline the corresponding mAP values. The notations are along the lines outlined in the main paper. We
observe similar trends for both d = 16 and d = 128 bits compared to d = 64 bits. It is interesting to note that with increase
in bit size d, the mAP does not necessarily increase. Table 7 (d = 64) has its mAP values lower than those for d = 64 (see
main paper) for all the hashing methods. This indicates that merely increasing the hash code length does not always improve
mAP scores. Also, the mAP values for Real-World for d = 128 bits has DAH performing better than SuH. This indicates
that in some cases domain adaptation helps in learning a better generalized model.

Table 6: Mean average precision @16 bits. For the NoDA and DAH results, Art is the source domain for Clipart, Product and
Real-World and Clipart is the source domain for Art.

Table 7: Mean average precision @128 bits. For the NoDA and DAH results, Art is the source domain for Clipart, Product and
Real-World and Clipart is the source domain for Art.

Expt.
Art
Clipart
Product
Real-World
Avg.

NoDA ITQ KMeans
0.147
0.102
0.120
0.110
0.253
0.134
0.225
0.193
0.186
0.135

0.133
0.116
0.241
0.195
0.171

BA
0.131
0.123
0.253
0.216
0.181

BDNN DAH SuH
0.381
0.207
0.151
0.412
0.211
0.138
0.459
0.257
0.313
0.400
0.371
0.248
0.413
0.262
0.212

Expt.
Art
Clipart
Product
Real-World
Avg.

NoDA ITQ KMeans
0.202
0.154
0.210
0.186
0.416
0.279
0.343
0.308
0.293
0.232

0.175
0.196
0.356
0.289
0.254

BA
0.148
0.187
0.336
0.258
0.232

BDNN DAH SuH
0.444
0.314
0.207
0.346
0.350
0.213
0.792
0.424
0.432
0.458
0.544
0.348
0.510
0.408
0.300

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.5
Recall
 

(a) Art

0.5
Recall
 

(b) Clipart

0.5
Recall
 

(c) Product

0.5
Recall
 

(d) Real-World

Figure 6: Precision-Recall curves @16 bits for the Ofﬁce-Home dataset. Comparison of hashing without domain adaptation (NoDA),
shallow unsupervised hashing (ITQ, KMeans), state-of-the-art deep unsupervised hashing (BA, BDNN), unsupervised domain adaptive
hashing (DAH) and supervised hashing (SuH). Best viewed in color.

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

n
o
i
s
i
c
e
r
P

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

n
o
i
s
i
c
e
r
P

0.5

0.4

0.9

0.8

0.7

0.6

0.3

0.2

0.1

0

0

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

n
o
i
s
i
c
e
r
P

0.5

0.4

0.9

0.8

0.7

0.6

0.3

0.2

0.1

0

0

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

NoDA
ITQ
KMeans
BA
BDNN
DAH
SuH

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.5
Recall
 

(a) Art

0.5
Recall
 

(b) Clipart

0.5
Recall
 

(c) Product

0.5
Recall
 

(d) Real-World

Figure 7: Precision-Recall curves @128 bits for the Ofﬁce-Home dataset. Comparison of hashing without domain adaptation (NoDA),
shallow unsupervised hashing (ITQ, KMeans), state-of-the-art deep unsupervised hashing (BA, BDNN), unsupervised domain adaptive
hashing (DAH) and supervised hashing (SuH). Best viewed in color.

