Noname manuscript No.
(will be inserted by the editor)

Fuzzy Approach Topic Discovery in Health and Medical
Corpora

Amir Karami · Aryya Gangopadhyay · Bin Zhou · Hadi Kharrazi

7
1
0
2
 
y
a
M
 
6
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
5
9
9
0
0
.
5
0
7
1
:
v
i
X
r
a

Received: date / Accepted: date

Abstract The majority of medical documents and elec-
tronic health records (EHRs) are in text format that
poses a challenge for data processing and ﬁnding rel-
evant documents. Looking for ways to automatically
retrieve the enormous amount of health and medical
knowledge has always been an intriguing topic. Pow-
erful methods have been developed in recent years to
make the text processing automatic. One of the popular
approaches to retrieve information based on discovering
the themes in health & medical corpora is topic model-
ing; however, this approach still needs new perspectives.
In this research we describe fuzzy latent semantic anal-
ysis (FLSA), a novel approach in topic modeling using
fuzzy perspective. FLSA can handle health & medical
corpora redundancy issue and provides a new method to
estimate the number of topics. The quantitative evalu-
ations show that FLSA produces superior performance
and features to latent Dirichlet allocation (LDA), the
most popular topic model.

Keywords Text Mining · Topic Model · Medical ·
Health · Fuzzy Approach

Amir Karami
School of Library and Information Science, University of
South Carolina
Tel.: +1-803-777-0197
Fax: +1-803-777-7938
E-mail: karami@sc.edu

Aryya Gangopadhyay and Bin Zhou
Information Systems Department, University of Maryland
Baltimore County

Hadi Kharrazi
Bloomberge School of Public Health, Johns Hopkins Univer-
sity

1 Introduction

There is a growing need to analyze large collections of
electronic documents. Moreover, very large-scale scien-
tiﬁc data management and analysis is one of the data-
intensive challenges identiﬁed by National Science Foun-
dation (NSF) as an area for future study [10]. Large
collections of electronic documents abound as our col-
lective knowledge continues to be digitized and stored,
requiring new tools for organization, search, indexing,
and browsing. As a consequence, ﬁnding relevant doc-
uments has become more diﬃcult for experts. In par-
ticular, large scale health and medical text data histor-
ically has been generated and stored. For example, the
total number of papers published on PubMed website
is more than 6 million papers in 20151 and the annual
average number of US hospital discharges is more than
30 million records [33, 34]. This huge amount of text
data and EHRs is a great motivation for companies to
save $450 billion a year using advanced data analytical
approaches2.

Developing eﬃcient techniques for discovering the
hidden structure in large complicated health and med-
ical data sets, and using that structure to answer ques-
tions about those data, is at the core of big health and
medical data science research. Substantial resources were
allocated in developing new data analytic methods and
tools. However, retrieving big health and medical text
data is a major current challenge.

One of the popular methods in medical text data
representation is bag-of-words (BOW). This technique
represents documents based on the frequency of words

1http://www.ncbi.nlm.nih.gov/pubmed
2http://www.mckinsey.com/industries/healthcare-

systems-and-services/our-insights/the-big-data-
revolution-in-us-health-care

P (T |D)

2.1 Topic Modeling in Health and Medical

2

with a matrix like A.

W ord1 W ord2 W ord3 W ord4

Document1

A =

Document2

Document3





3
0
0

1
1
0

4
0
3





0
0
0

For example, matrix A shows that word 3 appeared
3 times in document 3. However, this matrix is a sparse
matrix for large number of documents [27]. Sparsity
means that there are a lot of words in a corpus; however,
one document covers a small percentage of all words.
Therefore, most elements are zero in BOW matrix [1].
Topic modeling is a popular method to address spar-
sity and high dimensionality issues. This method was
originally introduced as a text analysis technique that
the objects are documents and the features are the fre-
quency of terms. The output of topic modeling is two
matrices. The ﬁrst one is the probability of words for
each topic or P (W |T ) and the second one is the proba-
bility of topics for each document or P (T |D) (Figure 1).

Documents(D)

T opics(T )

Documents(D)

)

W
(
s
d
r
o
W

W × D

→ P (W |T )

)

W
(
s
d
r
o
W

)
T
(
s
c
i
p
o
T

Fig. 1 Matrix Interpretation of Topic Modeling

The words with higher probability in W ords×T opic
matrix discloses semantic structure. T opics×Documents
matrix reduces the number of dimension from the num-
ber of words in BOW approach to the number of topics.
For example, suppose that there are 100 topics in a cor-
pus with 5000 documents and 10,000 words. Topic mod-
eling converts W × D matrix to two matrices: Words ×
Topics matrix with 10,000 rows and 100 columns, and
Topics × Documents matrix with 100 rows and 5000
columns. It is worth mentioning that the second non-
sparse matrix is used for document classiﬁcation and
clustering with 100 topics as the number of features in-
stead of 10,000 words as the number of features in the
BOW approach. Document classiﬁcation and clustering
problems categorized labeled and unlabeled documents
based on extracted features (topics) from documents
using topic modeling.

Topic modeling is an eﬀective method for health
and medical text mining; however, due to the inten-
sive amount of available data, there is still the need to
improve the performance of this approach. In addition,
copy and paste (redundancy) has a negative impact on
topic modeling [8] and previous work has shown that
most of medical notes are redundant [47].

Amir Karami et al.

In this research, we propose Fuzzy Latent Semantic
Analysis (FLSA) model for health and medical text
mining. This model shows better performance in both
redundant and non-redundant document and can help
topic models estimating number of topics in corpus. The
remainder of this paper is organized as follows. In the
related work section, we review the related research. In
the methodology section, we provide more details for
FLSA. An empirical study was conducted to verify the
eﬀectiveness of FLSA. Finally, we provide an illustra-
tive example for FLSA, and present a summary and
future directions in the last two sections.

2 Related Work

Text mining can be deﬁned as the methods of machine
learning and statistics with the goal of recognizing pat-
terns and disclosing the hidden information in text data
[23]. In this section, we review key concepts, and health
and medical applications of topic modeling and fuzzy
clustering (FC).

There are two main approaches in text mining: su-
pervised and unsupervised. The goal of supervised ap-
proach is to disclose hidden structure in labeled datasets
and the goal of unsupervised approach is to discover
patterns in unlabeled datasets. The most popular tech-
niques in supervised and unsupervised approaches are
classiﬁcation and clustering, correspondingly. The pur-
pose of classiﬁcation is to train a corpus with prede-
ﬁned labels and assign a label to a new document [38].
Clustering assigns a cluster to each document in a cor-
pus based on similarity in a cluster and dissimilarity
between clusters. Among text mining techniques, topic
modeling is one of popular unsupervised methods with
a wide range of applications from SMS spam detection
[30] to image tagging [48].

Topic modeling deﬁnes each topic as probability dis-
tribution over words and a document as probability dis-
tribution over topics. In health and medical text min-
ing, latent Dirichlet allocation (LDA) shows better per-
formance than other topic models [46].

2.2 Health and Medical Applications of LDA

LDA is a generative probabilistic model based on a
three-level hierarchical Bayesian model. LDA assumes
that documents contain latent topics and each topic can
be represented by a distribution across words [6].

LDA has a wide range of health and medical appli-
cations such as predicting protein-protein relationships

Fuzzy Approach Topic Discovery in Health and Medical Corpora

3

based on the literature knowledge [4], discovering rele-
vant clinical concepts and structures in patients’ health
records [3], identifying patterns of clinical events in a co-
hort of brain cancer patients [2], and analyzing time-to-
event outcomes [11]. The discovery of clinical pathway
(CP) patterns is a method for revealing the structure,
semantics, and dynamics of CPs to provide clinicians
with explicit knowledge used to guide treatment activi-
ties of individual patients. LDA has used for CPs to ﬁnd
treatment behaviors of patients [24], to predict clinical
order patterns, and to model various treatment activ-
ities [7] and their occurring time stamps in CPs [12].
LDA has also customized to determine patient mor-
tality [18], and to discover knowledge from modeling
disease and patient characteristics [43]. Redundancy-
aware LDA (Red-LDA) is one of the versions of LDA
for handling redundancy issue in medical documents
and has shown better performance than LDA [9].

2.3 Health and Medical Applications of FC

There are two major clustering approaches: hard and
fuzzy (soft). In hard clustering, every object may belong
to exactly one cluster but, in fuzzy clustering (FC), the
membership is fuzzy and objects may belong to several
clusters [26]. Among fuzzy clustering techniques, fuzzy
C-means (FCM) is the most popular model [5]. FCM is
based on minimizing the overall distance from a cluster
prototype to each datum.

Fuzzy clustering has used in predicting the response
to treatment with citalopram in alcohol dependence
[40], analyzing diabetic neuropathy [13], detecting early
diabetic retinopathy [50], characterizing stroke subtypes
and coexisting causes of ischemic stroke [20, 22, 21], im-
proving decision-making in radiation therapy [41], and
detecting cancer such as breast cancer [19]. In addition,
fuzzy clustering was used to improve ultrasound imag-
ing technique [39] and analyze microarray data [17].

Although there are a lot of fuzzy clustering applica-
tions in health and medical domains especially in im-
age processing, this approach has not been considered
for topic modeling yet. This paper proposes a new ap-
proach to provide a bridge between fuzzy clustering and
topic modeling to analyze big health and medical cor-
pora.

3 Methodology

as a new approach in topic modeling and will be val-
idated through a series of experiments, conducted on
health and medical text data.

Although LDA has shown a better performance than
other topic models, redundancy has negative eﬀect on
LDA performance [8, 14]. The reason is that, for ex-
ample, words like w1w2w3 in a document such as dk =
{w1w2w3w6w9} are copied to a document like dp =
{w7w8} to be dp = {w1w2w3w7w8}. w1w2w3 should be
assigned to the same topic by LDA but it is possible
to be assigned by LDA to diﬀerent topics. FLSA has a
potential to handle the redundancy issue, estimate the
optimum number of topics, and provide better perfor-
mance than its competitors.

3.1 Fuzzy Logic and Fuzzy Clustering

The traditional reasoning has precise character that is
yes-or-no (true-or-false) rather than more-or-less [52].
Fuzzy logic added a new extension to move from the
classical logic, 0 or 1, to the truth values between zero
and one, [0,1]) [49] (Figure 2).

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

Fig. 2 Fuzzy Logic Spectrum

Fuzzy logic assumes that if X is a collection of data
points represented by x, then a fuzzy set A in X is a
set of order pairs, A = {(x, µA(x)|x ∈ X)}.

µA(x) is the membership function which maps X to
the membership space M which is between 0 and 1 [28].
The goal of most clustering algorithms is to minimize
the objective function J that measures the quality of
clusters to ﬁnd the optimum J which is the sum of the
squared distances between each cluster center and each
data point.

The main goal of fuzzy models is to formulate uncer-
tainty for applications such as decision-making [32, 28].
For example, a voter decides to select some candidates
among a set of candidates in an election. The voter
has diﬀerent preferences in terms of economic, foreign
policy, health, etc. Based on the preferences, the dis-
tance between each candidate’s plans and the voter’s
preferences can be changed. These preferences can be
formulated and measured in fuzzy clustering with µ,
degree of membership.

3.2 FLSA

In this part, we describe our method, fuzzy latent se-
mantic analysis (FLSA), for uncovering latent semantic
features from text documents. FLSA treats fuzzy view

FLSA assumes that documents and words can be fuzzy
clustered and each cluster is a topic. For example, given

4

Amir Karami et al.

dna

genome

human

dna

genome

human

infectious

bacteria

disease

infectious

bacteria

disease

T1

T2

T3

T opics

organisms

species

evolution

organisms

species

evolution

N

D

Fig. 3 This topic model example correspends to the fuzzy process.

a corpus FLSA discovers topic 1 including dna, genome,
and human words with “Genetics” theme, topic 2 in-
cluding infectious, bacteria, and disease words with “Dis-
ease” theme, and topic 3 including organisms, species,
and evolution words with “Evolution” theme (Figure 3).
In this process, words are assigned a fuzzy degree of
membership with respect to each cluster (topic). The
color of circles shows the magnitude of membership
from low (light Grey) to high (dim Grey).

The main goal of FLSA is to ﬁnd two matrices:
P (T |D) and P (W |T ) mentioned in Figure 1. FLSA has
seven steps using Local Term Weighting (LTW), Global
Term Weighting (GTM), and Fuzzy Clustering (FC):

Step 1. The ﬁrst step is to calculate LTW. Among
diﬀerent LTW methods, we use term frequency (TF) as
it is the most popular method.

Step 2. The next step is to calculate GTW. We ex-
plore four GTW methods in this research including En-
tropy, Inverse Document Frequency (IDF), Probabilistic
Inverse Document Frequency (ProbIDF), and Normal
(Table 1):

– Entropy gives higher weight to the terms with less

frequency in few documents [15].

– IDF assigns higher weights to rare terms and lower

weights to common terms [42].

– Normal is used to correct discrepancies in document
lengths and also normalize the document vectors
[37].

– ProbIDF is similar to IDF and assigns very low neg-
ative weight for the terms occurring in every docu-
ment [37]

Symbol tfij deﬁnes the number of times word i oc-
curs in document j. With m words and n documents,

we need to ﬁnd b(tfij) and pij for calculating the four
mentioned GTW methods:

b(tfij) =

(cid:26) 1 tfij > 0
0 tfij = 0

pij =

tfij
j tfij

(cid:80)

(1)

(2)

Name

Entropy
IDF

Normal

ProbIDF

Formula
(cid:80)

1 +
log2
1√(cid:80)

log2

j pij log2(pij )
log2 n

(cid:80)

n
j tfij

j tf 2
ij
n−(cid:80)
(cid:80)

j b(tfij )

j b(tfij )

Table 1 GTW Methods

The outputs of this step are the document term ma-
trices with applied TF-Entropy, TF-IDF, TF-Normal,
and TF-ProbIDF methods.

Step 3. We use Fuzzy C-means (FCM) in this re-
search to fuzzy cluster the documented represented by
the four mentioned GTW methods. FCM minimizes an
objective function by considering constraints:

M in Jq(µ, V, X) =

(µkj)qDIS2
kj

(3)

c
(cid:88)

n
(cid:88)

k=1

j=1

Fuzzy Approach Topic Discovery in Health and Medical Corpora

subject to:

0 ≤ µkj ≤ 1;

c
(cid:88)

k=1

µkj = 1

0 <

µkj < n;

n
(cid:88)

j=1

Where:

n= number of data
c= number of clusters (topics)
µkj= membership value
q= fuzziﬁer, 1 < q ≤ ∞
V = cluster center vector
DISkj = d(xj, vk)= distance between xj and vk

By optimizing eq.3:

µij =

1
l=1( DISkj
DISlj

(cid:80)c

2
q−1

)

vi =

(cid:80)n

j=1(µkj)qxj
(cid:80)n
j=1(µkj)q

The iterations in the clustering algorithm continue
till the maximum changes in µkj becomes less than or
equal to a pre-speciﬁed threshold with O(n) computa-
tional time complexity.

We use µkj as the membership degrees for each doc-
ument (D) with respect to each of topics (clusters). The
value of µkj is between 0 and 1 that can be interpreted
as P(Tk|Dj) or probability of topic k in document j.
This step ﬁnds T opics × Documents matrix and uses it
along with the following steps to ﬁnd W ords × T opics
matrix in Figure 1. It is worth mentioning that FLAS’s
steps are dependent and integrated. For example, the
documents (D) and the topics (T) in step 3 are the same
topics and documents in steps 5,6 and 7.

To avoid the negative impact of high dimensional-
ity of the four mentioned matrices in step 2, we use
singular value decomposition (SVD), which is a pop-
ular method [16] to reduce the data dimension before
using fuzzy clustering. We select two dimensions, as the
minimum number of dimensions, for SVD to have a fast
process.

(4)

(5)

(6)

(7)

(8)

5

(9)

(10)

(11)

(12)

Step 4. We use document-term matrices with the
GTW methods in step 2 (W ords × Documents matrix)
to ﬁnd P(Dj) or probability of document j:

P (Dj) =

(cid:80)m

i=1(Wi, Dj)
(cid:80)n

j=1(Wi, Dj)

(cid:80)m

i=1

Step 5. The next step has two parts. The ﬁrst part
is to ﬁnd P(Dj|Tk) or probability of document j in
topic k using P (Tk|Dj) in step 3 and P (Dj) in step 4:

P (Dj, Tk) = P (Tk|Dj) × P (Dj)

Then normalizing P (D, T ) in each topic:

P (Dj|Tk) =

P (Dj, Tk)
j=1 P (Dj, Tk)

(cid:80)n

Step 6: We use document-term matrices with the GTW
methods in step 2 to ﬁnd P(Wi|Dj) or probability of
word i in document j:

P (Wi|Dj) =

P (Wi, Dj)
i=1 P (Wi, Dj)

(cid:80)m

Step 7: The ﬁnal step is to ﬁnd P(Wi|Tk) or proba-
bility of word i in topic k (T opics × Documents matrix
in Figure 1) using P (Dj|Tk) in step 5 and P (Wi|Dj) in
step 6:

P (Wi|Tk) =

P (Wi|Dj) × P (Dj|Tk)

(13)

n
(cid:88)

j=1

FLSA is ﬂexible to work with all dimensionality re-
duction techniques [16] such as principal component
analysis (PCA) [25] and all fuzzy clustering techniques
such as self-organizing map (SOM) [36].

Fuzzy c-means, as it is the core of FLSA, can be ap-
plied on both discrete and continuous data. This feature
enables FLSA to use a wide range of LTW and GTW
methods and to be used for other machine learning and
data science applications such as image processing [35].
Moreover, optimization nature of fuzzy clustering pro-
vides a solution to estimate the optimum number of
topics.

4 Experiments

In this section, we evaluate FLSA against LDA by doc-
ument classiﬁcation using Random Forest, document
clustering using k-means, document modeling using log-
likelihood, and execution time test. We also evaluate
FLSA against RedLDA by document modeling on re-
dundant documents.

6

Amir Karami et al.

We use ﬁve datasets, the Matlab package for Chib-
style estimation of log-likelihood1, the FCM Matlab
package2 with its default settings including 100 iter-
ations and 1e-5 as the minimum improvement in ob-
jective function between two consecutive iterations, the
Weka tool3 for classiﬁcation evaluation, the MALLET
package4 with its default settings for implementing LDA,
and the Python package for implementing RedLDA5.
The source code for FLSA will be available in the ﬁrst
author’s website6,7 in R and Matlab platforms.

4.1 Datasets

We leverage ﬁve available health and medical datasets
in this research (Table 2):

– The ﬁrst dataset8 is MuchMore Springer Bilingual
Corpus (M-Dataset) which is a labeled corpus of En-
glish scientiﬁc medical abstracts from the Springer
website. In this research, we use the ﬁrst 2 journals
including: Arthroscopy and Federal Health Stan-
dard Sheet.

– The second dataset9 is an unlabeled corpus of 2,434

nursing notes (N-Dataset).

– The third dataset10 is Ohsumed Collection (O-Dataset)
that is a labeled corpus of medical abstracts from
the MeSH categories including Bacterial Infections
and Mycoses, and Virus Diseases.

– The fourth dataset (T-Dataset) is health news from
Twitter11. We collected the tweets from the health
related Twitter accounts including cbchealth, every-
dayhealth, foxnewshealth, goodhealth, kaiserhealthnews,
latimeshealth, msnhealthnews, NBChealth, nprhealth,
usnewshealth, bbchealth, cnnhealth, gdnhealthcare, ny-
timeshealth, reuters health, and wsjhealth from Au-
gust 2011 to December 201412.

– The ﬁfth dataset is R-Datasets13 are the synthesis
text documents to track the negative eﬀect of redun-
dancy in documents [8]. These datasets are subsets

1http://www.cs.umass.edu/~wallach/code/etm/
2http://www.mathworks.com/help/fuzzy/fcm.html
3http://www.cs.waikato.ac.nz/ml/weka/
4http://mallet.cs.umass.edu/
5https://sourceforge.net/projects/redlda/
6https://sites.google.com/site/karamihomepage/
7https://github.com/amir-karami
8http://muchmore.dfki.de/resources1.htm
9http://physionet.org/
10http://disi.unitn.it/moschitti/corpora/ohsumed-

first-20000-docs.tar.gz

11www.twitter.com
12https://github.com/amir-karami/Health-News-

Tweets-Data

13https://sourceforge.net/projects/
corpusredundanc/files/?source=navbar

of a larger dataset called WSJ which has a collec-
tion of the abstracts of Wall Street Journal. In this
dataset, 1300 abstracts were sampled between 1 and
5 times in a uniform manner for 11 times to elimi-
nate bias from random sampling.

4.2 Document Classiﬁcation

The ﬁrst evaluation measure is document classiﬁcation
using two labeled datasets, M-Dataset and O-Dataset.
Both datasets have two classes (labels), M-Dataset with
Arthroscopy and Federal Health Standard Sheet classes,
and O-Dataset with Bacterial Infections and Mycoses,
and Virus Diseases classes.

Document classiﬁcation problem assigns a document
to a class and this problem needs to extract features
from text data. To avoid high dimensionality of BOW
approach for document classiﬁcation, topic modeling re-
duces the number of features by clustering the mean-
ingful related words as a topic. To avoid any possible
bias, we track the performance of FLSA against LDA
with the 10-fold cross validation method that the data
is broken into 10 subsets for 10 iterations. Each of the
subsets is selected for testing and the rest of sets are se-
lected for training. We use 50, 100, 150, and 200 topics
as the input features of documents for Random Forest
method as it is one of the popular and high performance
classiﬁcation methods [44]. The output of Random For-
est is presented as a confusion matrix (Table 7) with the
following deﬁnitions:

– True Negative (TN) is the number of correct pre-

dictions that an instance is negative.

– False Negative (FN) is the number of incorrect of

predictions that an instance negative.

– False Positive (FP) is the number of incorrect pre-

dictions that an instance is positive.

– True Positive (TP) is the number of correct predic-

tions that an instance is positive.

Predicted
Negative Positive

Actual

Negative
Positive

TN
FN

FP
TP

Table 7 Confusion Matrix

For evaluating the performance of the classiﬁcation
algorithm, we use accuracy (ACC), F-measure, Matthews
Correlation Coeﬃcient (MCC), and the area under ROC
(AUC). The evaluation metrics are deﬁned based on the
confusion matrix, as shown in equations 14 - 18:

Fuzzy Approach Topic Discovery in Health and Medical Corpora

7

Dataset Name #Documents
M-Dataset
N-Dataset
O-Dataset
T-Dataset
R-Dataset-1
R-Dataset-2
R-Dataset-3
R-Dataset-4
R-Dataset-5
R-Dataset-6
R-Dataset-7
R-Dataset-8
R-Dataset-9
R-Dataset-10
R-Dataset-11

1,527
1,607
2,092
58,927
2,288
3,310
3,254
3,162
3,211
3,124
3,251
3,212
3,257
3,258
3,284

#Term Tokens
245,931
299,449
198,998
395,635
754,801
785,467
757,603
705,100
751,496
727,687
747,067
747,956
755,261
739,041
764,658

#Unique Terms
14,411
11,059
15,768
25,310
3,265
23,408
22,982
22,264
23,046
22,944
22,609
22,961
22,929
22,913
23,134

Avg Term Per Document Description
96.3
124.8
95.1
6.7
231.2
236.4
232.8
222.9
234.1
232.9
229.8
232.6
231.9
226.8
232.8

Medical Papers
Nursing Notes
Medical Papers
Tweets
News
News
News
News
News
News
News
News
News
News
News

Method

F-Measure MCC ROC #Topics

Method

F-Measure MCC ROC #Topics

Table 2 Basic Statistics for Datasets

LDA
FLSA(Entropy)
FLSA(IDF)
FLSA(Normal)
FLSA(ProbIDF)

Acc
%
90.05
97.66
95.90
91.22
97.66

0.9
0.977
0.959
0.912
0.977

0.969
0.799
0.953 0.99
0.982
0.917
0.971
0.824
0.987
0.953

50
50
50
50
50

LDA
FLSA(Entropy)
FLSA(IDF)
FLSA(Normal)
FLSA(ProbIDF)

Table 3 M-Datatset Classiﬁcation - 50 and 100 Topics

LDA
FLSA(Entropy)
FLSA(IDF)
FLSA(Normal)
FLSA(ProbIDF)

150
0.867
0.536
150
0.995
0.917
150
0.991
0.953
0.905
150
0.992
0.941 0.991 150

LDA
FLSA(Entropy)
FLSA(IDF)
FLSA(Normal)
FLSA(ProbIDF)

Table 4 M-Datatset Classiﬁcation - 150 and 200 Topics

Method

F-Measure MCC ROC #Topics

Method

F-Measure MCC ROC #Topics

Method

F-Measure MCC ROC #Topics

Method

F-Measure MCC ROC #Topics

Acc
%
77.19
95.90
97.66
95.32
97.07

Acc
%
75.38
75.21
75.90
71.25
74.87

Acc
%
72.80
74.87
76.59
72.46
75.04

0.77
0.959
0.977
0.953
0.971

0.72
0.741
0.746
0.677
0.735

0.662
0.735
0.752
0.691
0.735

LDA
FLSA(Entropy)
FLSA(IDF)
FLSA(Normal)
FLSA(ProbIDF)

LDA
FLSA(Entropy)
FLSA(IDF)
FLSA(Normal)
FLSA(ProbIDF)

0.66
0.737

50
0.282
0.331
50
0.343 0.727 50
50
0.63
0.153
50
0.711
0.314

LDA
FLSA(Entropy)
FLSA(IDF)
FLSA(Normal)
FLSA(ProbIDF)

150
0.636
0.136
0.312
150
0.723
0.358 0.732 150
150
0.668
0.194
150
0.726
0.313

LDA
FLSA(Entropy)
FLSA(IDF)
FLSA(Normal)
FLSA(ProbIDF)

Table 6 O-Datatset Classiﬁcation - 150 and 200 Topics

Acc
%
78.36
96.49
98.24
92.39
97.66

Acc
%
82.45
97.076
97.66
92.39
97.66

Acc
%
72.97
76.24
74.35
71.08
74.52

Acc
%
71.08
75.21
74.18
71.94
74.87

0.78
0.965
0.982
0.924
0.977

0.822
0.971
0.977
0.924
0.977

0.682
0.747
0.726
0.694
0.724

0.648
0.74
0.725
0.683
0.729

100
0.895
0.561
0.929
100
0.986
0.964 0.996 100
100
0.984
0.846
100
0.994
0.953

200
0.894
0.646
200
0.992
0.941
200
0.984
0.953
0.846
200
0.982
0.953 0.985 200

0.179
100
0.657
0.344 0.732 100
100
0.712
0.288
100
0.617
0.201
100
0.684
0.283

0.079
200
0.63
0.326 0.731 200
200
0.713
0.285
200
0.657
0.172
200
0.719
0.298

Table 5 O-Datatset Classiﬁcation- 50 and 100 Topics

Method

F-Measure MCC ROC #Topics

Method

F-Measure MCC ROC #Topics

P recision(P ) =

T P
T P + F P

Recall(R) =

T P
T P + F N

Accuracy(Acc) =

T P + T N
T P + T N + F P + F N

(14)

F − measure = 2 ×

P × R
P + R

(17)

ROC curves plot FP on the X axis vs. TP on the
Y axis to ﬁnd the trade oﬀ between them; therefore,
the ROC is closer to the upper left indicating better
performance (Figure 6).

MCC is used to determine the quality of classiﬁ-
cation methods, ranging between -1 (the worst perfor-

(15)

(16)

8

Amir Karami et al.

·104

·104

2

3

4

5

6

7

8

2

3

4

5

6

7

8

# Clusters

# Clusters

F LSA(Entropy)

F LSA(IDF )

F LSA(P robIDF )

F LSA(N ormal)

LDA

Fig. 4 Calinski-Harabasz - 50 Topics Left Figure and 100 Topics Right Figure

·104

·104

x
e
d
n
I

z
s
a
b
a
r
a
H

-
i
k
s
n

i
l
a
C

x
e
d
n
I

z
s
a
b
a
r
a
H

-
i
k
s
n

i
l
a
C

1.5

0.5

1

0

1.5

0.5

1

0

x
e
d
n
I

z
s
a
b
a
r
a
H

-
i
k
s
n

i
l
a
C

x
e
d
n
I

z
s
a
b
a
r
a
H

-
i
k
s
n

i
l
a
C

1.5

0.5

1

0

1.5

0.5

1

0

2

3

4

5

6

7

8

2

3

4

5

6

7

8

# Clusters

# Clusters

F LSA(Entropy)

F LSA(IDF )

F LSA(P robIDF )

F LSA(N ormal)

LDA

Fig. 5 Calinski-Harabasz - 150 Topics Left Figure and 200 Topics Right Figure

TP

FP

Fig. 6 ROC

MCC=

√

mance) and +1 (the best performance).

(T P ×T N )−(F N ×F P )

(T P +F P )×(T P +F N )×(T N +F P )×(T N +F N )

(18)

This experiment shows that FLSA with Entropy,
IDF, Normal, and ProbIDF show better performance
than LDA with diﬀerent numbers of topics (Tables 3 -
6). The highest performance in each table is shown in
bold format.

4.3 Document Clustering

The second evaluation is document clustering using un-
labeled N-Dataset. Internal and external validation are

two major methods for clustering validation; however,
comparison between these two major methods shows
that internal validation is more precise [45]. We evaluate
diﬀerent numbers of topics and clusters with Calinski-
Harabasz (CH) index, as one of the popular internal
validation methods, using K-means with 500 iterations.
CH index evaluates the cluster validity based on the av-
erage of the sum of squared error cluster between and
within clusters. Higher CH index indicates better clus-
tering.

We track the performance of FLSAs and LDA using
diﬀerent numbers of clusters ranging from 2 to 8 with
diﬀerent numbers of topics including 50, 100, 150, and
200. CH index shows that FLSAs have better perfor-
mance than LDA with the diﬀerent ranges of features
and clusters (Figures 4 & 5). The gap between FLSAs
and LDA does not change signiﬁcantly with diﬀerent
numbers of topics and clusters.

4.4 Redundancy Issue

The next experiment explores the eﬀect of redundancy
issue using the ﬁfth dataset. R-Datasets are not a med-
ical corpus; however, they were created as a synthetic
redundant corpus without having privacy issue to mea-
sure the eﬀect of redundancy issue [8]. We select pub-
licly available unlabeled R-Datasets to make the eval-

Fuzzy Approach Topic Discovery in Health and Medical Corpora

9

uation process easier for possible future research. We
compare FLSAs with not only LDA but also RedLDA,
as it was developed to handle redundancy issue in med-
ical text data [9].

We train LDA, RedLDA and FLSA models on R-
Datasets to compare the generalization performance of
the models. We compute the log-likelihood on a held-
out test set to evaluate the models. A higher log-likelihood
score indicates better generalization performance. Fig-
ure 7 shows the average log-likelihood of the R-Datasets
with diﬀerent numbers of topics from 25 to 350. This
experiment indicates that FLSAs performs better than
RedLDA and LDA on redundant documents.

)
s
d
n
o
c
e
S
(

e
m
T

i

400

300

200

100

0

·105

−7.6

−7.8

−8

d
o
o
h
i
l
e
k
i
L
-
g
o
L

0

100

200

300

Number of Topics

Entropy

IDF

N ormal

P robIDF

RedLDA

LDA

Fig. 7 Likelihood Comparison for R-Datasets

0

100

200

300

400

Number of Topics

Entropy

IDF

N ormal

P robIDF

LDA

Fig. 8 Execution Time for T-Dataset

5 Example

Documents and papers in health and medical domains
contain terms and words that need expertise to under-
stand them. Therefore, we show an illustrative exam-
ple with T-Dataset containing tweets with more under-
standable words in this section. First, we run FLSA on
diﬀerent numbers of topics to estimate optimum num-
ber of topics using the objective function ,Jq (Figure 9).
This estimation feature of fuzzy clustering is on of ad-
vantageous of FLSA. The elbow or the knee part of the
ﬁgure shows ∼125 as the number of topics1.

)
J
(

n
o
i
t
c
n
u
F

e
v
i
t
c
e
j
b
O

·10−2

6

4

2

0

0

100

200

300

400

Number of Topics

Fig. 9 Number of Topics Estimation

4.5 Execution Time

In this section, we compare the speed of FLSA in com-
parison with LDA using T-Dataset, the biggest dataset
in this paper. The major process in topic modeling is
based on a joint probability distribution over hidden
topics and the observed words to infer the word with
higher probability in each topic by using the posterior
distribution. The most popular approximate method for
LDA is collapsed Gibbs sampling applied in the experi-
ments. All the inference algorithms require multiple it-
erations which increase the computational cost linearly
with the number of documents, topics, words, and iter-
ations [51]. Figure 8 shows that the time performance
of FLSAs is stable with an increase in the numbers of
topics and considerably better than LDA.

Table 8 shows a sample of topics that are about fam-

ily, cancer, Ebola, Alzheimer, nursing, and teenagers:

– T83 is about the beneﬁt of nursing black teens with

dementia.

– T42 is about the huge costs of teens pregnancy.
– T33 is reporting a teen’s invention to save Alzheimer’s

patients and improve the quality of their lives.
– T101 is about the government’s need to hire more

nurses in diﬀerent cities.

– T85 is about the recipes for family having diabetes

specially for women and kids.

– T107 is about using Ebola for cyberbullying.
– T71 is about the role of psychiatrists against Ebola.

1https://www.cs.princeton.edu/courses/archive/

spring07/cos424/scribe_notes/0306.pdf

10

Amir Karami et al.

– T25 is about sneezing as a risk for spreading Ebola
and the relation between Ebola and lung cancer.

6 Conclusion

The vast array of health and medical text data repre-
sents a valuable resource that can be analyzed to ad-
vance state-of-the-art medicine and health. Large elec-
tronic health and medical archives such as PubMed pro-
vide an extremely useful service to the scholarly com-
munity. However, the needs of readers go beyond a sim-
ple keyword search.

Topic modeling is one of the popular unsupervised
methods to automatically discover a hidden thematic
structure in a large collection of unstructured health
and medical documents. This discovered structure fa-
cilitates browsing, searching, and summarizing the col-
lection.

Fuzzy perspective is a machine learning approach
that has been used more in medical image processing
than text processing. Existing techniques of topic mod-
eling are based on two main approaches: linear alge-
bra and statistical distributions; however, this paper
proposes FLSA to utilize fuzzy perspective for disclos-
ing latent semantic features of health and medical text
data.

FLSA is a new competitor to the established topic
models such as LDA and has the ﬂexibility to work with
a wide range of dimension reduction and fuzzy clus-
tering techniques. FLSA also works with both discrete
and continuous data, estimates the optimum number of
topics, and avoids the negative eﬀect of the redundancy
issue in health and medical corpora.

In our future work, we will develop dynamic and
hierarchal topic models using fuzzy perspective. In ad-
dition, FLSA will be applied on social media data to
track public opinions and will be used for online review
and SMS spam detection [29, 31].

References

1. C. C. Aggarwal and C. Zhai. An introduction to
In Mining Text Data, pages 1–10.

text mining.
Springer, 2012.

2. C. Arnold and W. Speier. A topic model of clinical
reports.
In Proceedings of the 35th international
ACM SIGIR conference on Research and devel-
opment in information retrieval, pages 1031–1032.
ACM, 2012.

3. C. W. Arnold, S. M. El-Saden, A. A. Bui, and
R. Taira. Clinical case-based retrieval using latent

topic analysis. In AMIA Annual Symposium Pro-
ceedings, volume 2010, page 26. American Medical
Informatics Association, 2010.

4. T. Asou and K. Eguchi. Predicting protein-protein
relationships from literature using collapsed varia-
tional latent dirichlet allocation. In Proceedings of
the 2nd international workshop on Data and text
mining in bioinformatics, pages 77–80. ACM, 2008.
5. J. C. Bezdek. Pattern recognition with fuzzy objec-
tive function algorithms. Kluwer Academic Pub-
lishers, 1981.

6. D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent
dirichlet allocation. Journal of machine Learning
research, 3:993–1022, 2003.

7. J. H. Chen, M. K. Goldstein, S. M. Asch,
L. Mackey, and R. B. Altman. Predicting inpatient
clinical order patterns with probabilistic topic mod-
els vs conventional order sets. Journal of the Amer-
ican Medical Informatics Association, page ocw136,
2016.

8. R. Cohen, M. Elhadad, and N. Elhadad. Redun-
dancy in electronic health record corpora: analysis,
impact on text mining performance and mitigation
strategies. BMC bioinformatics, 14(1):10, 2013.
9. R. Cohen, I. Aviram, M. Elhadad, and N. El-
hadad. Redundancy-aware topic modeling for pa-
tient record notes. PloS one, 9(2):e87555, 2014.
10. N. Council. Future directions for nsf advanced com-
puting infrastructure to support u.s. science and en-
gineering in 2017-2020: Interim report, 2016. The
National Academies Press Washington, DC.

11. J. A. Dawson and C. Kendziorski.

Survival-
supervised latent dirichlet allocation models for ge-
nomic analysis of time-to-event outcomes. arXiv
preprint arXiv:1202.5999, 2012.

12. G. Defossez, A. Rollet, O. Dameron, and P. In-
grand. Temporal representation of care trajectories
of cancer patients using data from a regional in-
formation system: an application in breast cancer.
BMC medical informatics and decision making, 14
(1):24, 2014.

13. L. Di Lascio, A. Gisolﬁ, A. Albunia, G. Galardi,
and F. Meschi. A fuzzy-based methodology for the
analysis of diabetic neuropathy. Fuzzy Sets and
Systems, 129(2):203–228, 2002.

14. D. Downey, O. Etzioni, and S. Soderland. A proba-
bilistic model of redundancy in information extrac-
tion. Technical report, DTIC Document, 2006.
15. S. Dumais. Enhancing performance in latent se-

mantic indexing (lsi) retrieval, 1992.

16. I. K. Fodor. A survey of dimension reduction tech-

niques, 2002.

Fuzzy Approach Topic Discovery in Health and Medical Corpora

11

T83
teen
nursing
dementia
beneﬁt
ready
babies
race
black
worst
meet

T42
sick
pregnant
billion
clinical
dying
stories
rare
access
role
hope

T33
teens
put
popular
save
alzheimer
leave
run
docs
quality
tied

T101
deadly
government
nurses
aﬀordable
clinic
premiums
ban
city
warns
deadline

T85
diabetes
week
ﬁnd
recipes
work
low
kids
rt
women
life

T71
rt
health
ebola
pms
poached
upset
study

T107
rt
health
ebola
lightning
study
welcomes
vape
cyberbullying tension
visually
tremors

psychiatrists
embryo

T25
rt
health
ebola
study
cancer
care
amp
sneeze
poo
risk

Table 8 A Sample of Topics

17. A. P. Gasch and M. B. Eisen. Exploring the condi-
tional coregulation of yeast gene expression through
fuzzy k-means clustering. Genome Biol, 3(11):1–22,
2002.

18. M. Ghassemi, T. Naumann, F. Doshi-Velez,
N. Brimmer, R. Joshi, A. Rumshisky, and
P. Szolovits. Unfolding physiological state: mortal-
ity modelling in intensive care units. In Proceedings
of the 20th ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 75–
84. ACM, 2014.

19. A. E. Hassanien. Intelligent data analysis of breast
cancer based on rough set theory.
International
Journal on Artiﬁcial Intelligence Tools, 12(04):465–
479, 2003.

20. C. M. Helgason and T. H. Jobe. The fuzzy cube
and causal eﬃcacy: representation of concomitant
mechanisms in stroke. Neural Networks, 11(3):549–
555, 1998.

21. C. M. Helgason and T. H. Jobe. Causal interac-
tions, fuzzy sets and cerebrovascular accident: the
limits of evidence-based medicine and the advent
of complexity-based medicine. Neuroepidemiology,
18(2):64–74, 1999.

22. C. M. Helgason, D. Malik, S.-C. Cheng, T. H. Jobe,
and J. N. Mordeson. Statistical versus fuzzy mea-
sures of variable interaction in patients with stroke.
Neuroepidemiology, 20(2):77–84, 2001.

23. A. Hotho, A. N¨urnberger, and G. Paaß. A brief

survey of text mining. volume 20, 2005.

24. Z. Huang, W. Dong, H. Duan, and H. Li. Similarity
measure between patient traces for clinical pathway
analysis: problem, method, and applications. IEEE
journal of biomedical and health informatics, 18(1):
4–14, 2014.

25. I. Jolliﬀe. Principal component analysis. Wiley On-

line Library, 2002.

26. A. Karami. Fuzzy Topic Modeling for Medical Cor-
pora. PhD thesis, UNIVERSITY OF MARYLAND,
BALTIMORE COUNTY, 2015.

27. A. Karami and A. Gangopadhyay. Fftm: A fuzzy
feature transformation method for medical docu-
ments.
In Proceedings of the Conference of the
Association for Computational Linguistics (ACL),
volume 128, 2014.

28. A. Karami and Z. Guo. A fuzzy logic multi-
criteria decision framework for selecting it service
In Proceedings of the Hawaii Inter-
providers.
national Conference on System Science (HICSS),
pages 1118–1127. IEEE, 2012.

29. A. Karami and B. Zhou. Online review spam de-
tection by new linguistic features. iConference 2015
Proceedings, 2015.

30. A. Karami and L. Zhou. Exploiting latent content
based features for the detection of static sms spams.
the 77th annual meeting of the association for in-
formation science and technology (ASIST), 2014.

31. A. Karami and L. Zhou. Improving static sms spam
detection by using new content-based features. the
20th americas conference on information systems
(AMCIS), 2014.

32. A. Karami, H. R. Yazdani, H. S. Beiryaie, and
N. Hosseinzadeh. A risk based model for is out-
In 2nd IEEE Interna-
sourcing vendor selection.
tional Conference on Information and Financial
Engineering (ICIFE), pages 250–254. IEEE, 2010.
33. A. Karami, A. Gangopadhyay, B. Zhou, and
H. Kharrazi. Flatm: A fuzzy logic approach topic
model for medical documents.
In Proceedings of
the Annual Meeting of the North American Fuzzy
Information Processing Society (NAFIPS). IEEE,
2015.

34. A. Karami, A. Gangopadhyay, B. Zhou, and
H. Kharrazi. A fuzzy approach model for uncov-
ering hidden latent semantic structure in medical
text collections. In Proceedings of the iConference,
2015.

35. J. Keller, R. Krisnapuram, and N. R. Pal. Fuzzy
models and algorithms for pattern recognition and
image processing, volume 4. Springer Science &
Business Media, 2005.

12

Amir Karami et al.

36. T. Kohonen. The self-organizing map. Proceedings

pages 573–576. ACM, 2009.

of the IEEE, 78(9):1464–1480, 1990.

37. T. G. Kolda. Limited-memory matrix methods

with applications. 1998.

38. T. M. Mitchell. Machine learning. wcb, 1997.
39. W. K. Moon, S.-C. Chang, C.-S. Huang, and R.-
F. Chang. Breast tumor classiﬁcation using fuzzy
clustering for breast elastography. Ultrasound in
medicine & biology, 37(5):700–708, 2011.

40. C. A. Naranjo, K. E. Bremner, M. Bazoon, and I. B.
Turksen. Using fuzzy logic to predict response to
citalopram in alcohol dependence. Clinical Phar-
macology & Therapeutics, 62(2):209–224, 1997.

41. E.

I. Papageorgiou, C. D. Stylios, and P. P.
Groumpos. An integrated two-level hierarchical
system for decision making in radiation therapy
based on fuzzy cognitive maps.
IEEE Trans-
actions on Biomedical Engineering, 50(12):1326–
1339, 2003.

42. K. Papineni. Why inverse document frequency?
In Proceedings of the second meeting of the North
American Chapter of the Association for Com-
putational Linguistics on Language technologies,
pages 1–8. Association for Computational Linguis-
tics, 2001.

43. R. Pivovarov, A. J. Perotte, E. Grave, J. Angio-
lillo, C. H. Wiggins, and N. Elhadad. Learning
probabilistic phenotypes from heterogeneous ehr
data. Journal of biomedical informatics, 58:156–
165, 2015.

44. Y. Qi, Z. Bar-Joseph, and J. Klein-Seetharaman.
Evaluation of diﬀerent biological data and compu-
tational classiﬁcation methods for use in protein in-
teraction prediction. Proteins: Structure, Function,
and Bioinformatics, 63(3):490–500, 2006.

45. E. Rend´on, I. Abundez, A. Arizmendi, and E. M.
Quiroz. Internal versus external cluster validation
indexes.
International Journal of computers and
communications, 5(1):27–34, 2011.

46. E. Sarioglu, H.-A. Choi, and K. Yadav. Clinical re-
port classiﬁcation using natural language process-
ing and topic modeling. In Proceedings of the IEEE
International Conference on Machine Learning and
Applications (ICMLA), volume 2, pages 204–209,
2012.

47. J. O. Wrenn, D. M. Stein, S. Bakken, and P. D.
Stetson. Quantifying clinical narrative redundancy
in an electronic health record. Journal of the Amer-
ican Medical Informatics Association, 17(1):49–53,
2010.

48. H. Xu, J. Wang, X.-S. Hua, and S. Li. Tag re-
ﬁnement by regularized lda. In Proceedings of the
17th ACM international conference on Multimedia,

49. L. A. Zadeh. Outline of a new approach to the
analysis of complex systems and decision processes.
IEEE Transactions on Systems, Man and Cyber-
netics, (1):28–44, 1973.

50. G. Zahlmann, B. Kochner, I. Ugi, D. Schuhmann,
B. Liesenfeld, A. Wegner, M. Obermaier, and
M. Mertz. Hybrid fuzzy image processing for situa-
tion assessment [diabetic retinopathy]. IEEE Engi-
neering in Medicine and Biology Magazine, 19(1):
76–83, 2000.

51. J. Zeng, Z.-Q. Liu, and X.-Q. Cao. A new approach
arXiv preprint

to speeding up topic modeling.
arXiv:1204.0170, 2012.

52. H.-J. Zimmermann. Fuzzy set theory. Wiley In-
terdisciplinary Reviews: Computational Statistics,
2(3):317–332, 2010.

Amir Karami is an Assistant Professor in the School

of Library and Information Science and a Faculty As-
sociate at the Arnold School of Public Health at the
University of South Carolina. He is currently working
on developing fuzzy text mining techniques and their
applications in medical, health, and social science.

Aryya Gangopadhyay is a Professor and the Chair
of Information Systems Department at the University
of Maryland Baltimore County. His research interests
are in the areas of databases and data mining. Cur-
rently, he is focused on privacy preserving data min-
ing, spatio-temporal data mining, and data mining for
health informatics.

Bin Zhou is an Assistant Professor in the Depart-
ment of Information Systems at the University of Mary-
land, Baltimore County. His recent research projects
include designing eﬃcient search techniques on large-
scale data, detecting malicious activities that manipu-
late the Web search results, and protecting users pri-
vacy in large social networks.

Hadi Kharrazi is an Assistant Professor in the
Johns Hopkins Bloomberg School of Public Health with
a joint appointment at the Johns Hopkins School of
Medicine. His research interest is in contextualizing CDSS
in PHI platforms to be utilized at diﬀerent HIT levels
of managed care such as EHR platforms or consumer
health informatics solutions.

Noname manuscript No.
(will be inserted by the editor)

Fuzzy Approach Topic Discovery in Health and Medical
Corpora

Amir Karami · Aryya Gangopadhyay · Bin Zhou · Hadi Kharrazi

7
1
0
2
 
y
a
M
 
6
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
5
9
9
0
0
.
5
0
7
1
:
v
i
X
r
a

Received: date / Accepted: date

Abstract The majority of medical documents and elec-
tronic health records (EHRs) are in text format that
poses a challenge for data processing and ﬁnding rel-
evant documents. Looking for ways to automatically
retrieve the enormous amount of health and medical
knowledge has always been an intriguing topic. Pow-
erful methods have been developed in recent years to
make the text processing automatic. One of the popular
approaches to retrieve information based on discovering
the themes in health & medical corpora is topic model-
ing; however, this approach still needs new perspectives.
In this research we describe fuzzy latent semantic anal-
ysis (FLSA), a novel approach in topic modeling using
fuzzy perspective. FLSA can handle health & medical
corpora redundancy issue and provides a new method to
estimate the number of topics. The quantitative evalu-
ations show that FLSA produces superior performance
and features to latent Dirichlet allocation (LDA), the
most popular topic model.

Keywords Text Mining · Topic Model · Medical ·
Health · Fuzzy Approach

Amir Karami
School of Library and Information Science, University of
South Carolina
Tel.: +1-803-777-0197
Fax: +1-803-777-7938
E-mail: karami@sc.edu

Aryya Gangopadhyay and Bin Zhou
Information Systems Department, University of Maryland
Baltimore County

Hadi Kharrazi
Bloomberge School of Public Health, Johns Hopkins Univer-
sity

1 Introduction

There is a growing need to analyze large collections of
electronic documents. Moreover, very large-scale scien-
tiﬁc data management and analysis is one of the data-
intensive challenges identiﬁed by National Science Foun-
dation (NSF) as an area for future study [10]. Large
collections of electronic documents abound as our col-
lective knowledge continues to be digitized and stored,
requiring new tools for organization, search, indexing,
and browsing. As a consequence, ﬁnding relevant doc-
uments has become more diﬃcult for experts. In par-
ticular, large scale health and medical text data histor-
ically has been generated and stored. For example, the
total number of papers published on PubMed website
is more than 6 million papers in 20151 and the annual
average number of US hospital discharges is more than
30 million records [33, 34]. This huge amount of text
data and EHRs is a great motivation for companies to
save $450 billion a year using advanced data analytical
approaches2.

Developing eﬃcient techniques for discovering the
hidden structure in large complicated health and med-
ical data sets, and using that structure to answer ques-
tions about those data, is at the core of big health and
medical data science research. Substantial resources were
allocated in developing new data analytic methods and
tools. However, retrieving big health and medical text
data is a major current challenge.

One of the popular methods in medical text data
representation is bag-of-words (BOW). This technique
represents documents based on the frequency of words

1http://www.ncbi.nlm.nih.gov/pubmed
2http://www.mckinsey.com/industries/healthcare-

systems-and-services/our-insights/the-big-data-
revolution-in-us-health-care

P (T |D)

2.1 Topic Modeling in Health and Medical

2

with a matrix like A.

W ord1 W ord2 W ord3 W ord4

Document1

A =

Document2

Document3





3
0
0

1
1
0

4
0
3





0
0
0

For example, matrix A shows that word 3 appeared
3 times in document 3. However, this matrix is a sparse
matrix for large number of documents [27]. Sparsity
means that there are a lot of words in a corpus; however,
one document covers a small percentage of all words.
Therefore, most elements are zero in BOW matrix [1].
Topic modeling is a popular method to address spar-
sity and high dimensionality issues. This method was
originally introduced as a text analysis technique that
the objects are documents and the features are the fre-
quency of terms. The output of topic modeling is two
matrices. The ﬁrst one is the probability of words for
each topic or P (W |T ) and the second one is the proba-
bility of topics for each document or P (T |D) (Figure 1).

Documents(D)

T opics(T )

Documents(D)

)

W
(
s
d
r
o
W

W × D

→ P (W |T )

)

W
(
s
d
r
o
W

)
T
(
s
c
i
p
o
T

Fig. 1 Matrix Interpretation of Topic Modeling

The words with higher probability in W ords×T opic
matrix discloses semantic structure. T opics×Documents
matrix reduces the number of dimension from the num-
ber of words in BOW approach to the number of topics.
For example, suppose that there are 100 topics in a cor-
pus with 5000 documents and 10,000 words. Topic mod-
eling converts W × D matrix to two matrices: Words ×
Topics matrix with 10,000 rows and 100 columns, and
Topics × Documents matrix with 100 rows and 5000
columns. It is worth mentioning that the second non-
sparse matrix is used for document classiﬁcation and
clustering with 100 topics as the number of features in-
stead of 10,000 words as the number of features in the
BOW approach. Document classiﬁcation and clustering
problems categorized labeled and unlabeled documents
based on extracted features (topics) from documents
using topic modeling.

Topic modeling is an eﬀective method for health
and medical text mining; however, due to the inten-
sive amount of available data, there is still the need to
improve the performance of this approach. In addition,
copy and paste (redundancy) has a negative impact on
topic modeling [8] and previous work has shown that
most of medical notes are redundant [47].

Amir Karami et al.

In this research, we propose Fuzzy Latent Semantic
Analysis (FLSA) model for health and medical text
mining. This model shows better performance in both
redundant and non-redundant document and can help
topic models estimating number of topics in corpus. The
remainder of this paper is organized as follows. In the
related work section, we review the related research. In
the methodology section, we provide more details for
FLSA. An empirical study was conducted to verify the
eﬀectiveness of FLSA. Finally, we provide an illustra-
tive example for FLSA, and present a summary and
future directions in the last two sections.

2 Related Work

Text mining can be deﬁned as the methods of machine
learning and statistics with the goal of recognizing pat-
terns and disclosing the hidden information in text data
[23]. In this section, we review key concepts, and health
and medical applications of topic modeling and fuzzy
clustering (FC).

There are two main approaches in text mining: su-
pervised and unsupervised. The goal of supervised ap-
proach is to disclose hidden structure in labeled datasets
and the goal of unsupervised approach is to discover
patterns in unlabeled datasets. The most popular tech-
niques in supervised and unsupervised approaches are
classiﬁcation and clustering, correspondingly. The pur-
pose of classiﬁcation is to train a corpus with prede-
ﬁned labels and assign a label to a new document [38].
Clustering assigns a cluster to each document in a cor-
pus based on similarity in a cluster and dissimilarity
between clusters. Among text mining techniques, topic
modeling is one of popular unsupervised methods with
a wide range of applications from SMS spam detection
[30] to image tagging [48].

Topic modeling deﬁnes each topic as probability dis-
tribution over words and a document as probability dis-
tribution over topics. In health and medical text min-
ing, latent Dirichlet allocation (LDA) shows better per-
formance than other topic models [46].

2.2 Health and Medical Applications of LDA

LDA is a generative probabilistic model based on a
three-level hierarchical Bayesian model. LDA assumes
that documents contain latent topics and each topic can
be represented by a distribution across words [6].

LDA has a wide range of health and medical appli-
cations such as predicting protein-protein relationships

Fuzzy Approach Topic Discovery in Health and Medical Corpora

3

based on the literature knowledge [4], discovering rele-
vant clinical concepts and structures in patients’ health
records [3], identifying patterns of clinical events in a co-
hort of brain cancer patients [2], and analyzing time-to-
event outcomes [11]. The discovery of clinical pathway
(CP) patterns is a method for revealing the structure,
semantics, and dynamics of CPs to provide clinicians
with explicit knowledge used to guide treatment activi-
ties of individual patients. LDA has used for CPs to ﬁnd
treatment behaviors of patients [24], to predict clinical
order patterns, and to model various treatment activ-
ities [7] and their occurring time stamps in CPs [12].
LDA has also customized to determine patient mor-
tality [18], and to discover knowledge from modeling
disease and patient characteristics [43]. Redundancy-
aware LDA (Red-LDA) is one of the versions of LDA
for handling redundancy issue in medical documents
and has shown better performance than LDA [9].

2.3 Health and Medical Applications of FC

There are two major clustering approaches: hard and
fuzzy (soft). In hard clustering, every object may belong
to exactly one cluster but, in fuzzy clustering (FC), the
membership is fuzzy and objects may belong to several
clusters [26]. Among fuzzy clustering techniques, fuzzy
C-means (FCM) is the most popular model [5]. FCM is
based on minimizing the overall distance from a cluster
prototype to each datum.

Fuzzy clustering has used in predicting the response
to treatment with citalopram in alcohol dependence
[40], analyzing diabetic neuropathy [13], detecting early
diabetic retinopathy [50], characterizing stroke subtypes
and coexisting causes of ischemic stroke [20, 22, 21], im-
proving decision-making in radiation therapy [41], and
detecting cancer such as breast cancer [19]. In addition,
fuzzy clustering was used to improve ultrasound imag-
ing technique [39] and analyze microarray data [17].

Although there are a lot of fuzzy clustering applica-
tions in health and medical domains especially in im-
age processing, this approach has not been considered
for topic modeling yet. This paper proposes a new ap-
proach to provide a bridge between fuzzy clustering and
topic modeling to analyze big health and medical cor-
pora.

3 Methodology

as a new approach in topic modeling and will be val-
idated through a series of experiments, conducted on
health and medical text data.

Although LDA has shown a better performance than
other topic models, redundancy has negative eﬀect on
LDA performance [8, 14]. The reason is that, for ex-
ample, words like w1w2w3 in a document such as dk =
{w1w2w3w6w9} are copied to a document like dp =
{w7w8} to be dp = {w1w2w3w7w8}. w1w2w3 should be
assigned to the same topic by LDA but it is possible
to be assigned by LDA to diﬀerent topics. FLSA has a
potential to handle the redundancy issue, estimate the
optimum number of topics, and provide better perfor-
mance than its competitors.

3.1 Fuzzy Logic and Fuzzy Clustering

The traditional reasoning has precise character that is
yes-or-no (true-or-false) rather than more-or-less [52].
Fuzzy logic added a new extension to move from the
classical logic, 0 or 1, to the truth values between zero
and one, [0,1]) [49] (Figure 2).

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

Fig. 2 Fuzzy Logic Spectrum

Fuzzy logic assumes that if X is a collection of data
points represented by x, then a fuzzy set A in X is a
set of order pairs, A = {(x, µA(x)|x ∈ X)}.

µA(x) is the membership function which maps X to
the membership space M which is between 0 and 1 [28].
The goal of most clustering algorithms is to minimize
the objective function J that measures the quality of
clusters to ﬁnd the optimum J which is the sum of the
squared distances between each cluster center and each
data point.

The main goal of fuzzy models is to formulate uncer-
tainty for applications such as decision-making [32, 28].
For example, a voter decides to select some candidates
among a set of candidates in an election. The voter
has diﬀerent preferences in terms of economic, foreign
policy, health, etc. Based on the preferences, the dis-
tance between each candidate’s plans and the voter’s
preferences can be changed. These preferences can be
formulated and measured in fuzzy clustering with µ,
degree of membership.

3.2 FLSA

In this part, we describe our method, fuzzy latent se-
mantic analysis (FLSA), for uncovering latent semantic
features from text documents. FLSA treats fuzzy view

FLSA assumes that documents and words can be fuzzy
clustered and each cluster is a topic. For example, given

4

Amir Karami et al.

dna

genome

human

dna

genome

human

infectious

bacteria

disease

infectious

bacteria

disease

T1

T2

T3

T opics

organisms

species

evolution

organisms

species

evolution

N

D

Fig. 3 This topic model example correspends to the fuzzy process.

a corpus FLSA discovers topic 1 including dna, genome,
and human words with “Genetics” theme, topic 2 in-
cluding infectious, bacteria, and disease words with “Dis-
ease” theme, and topic 3 including organisms, species,
and evolution words with “Evolution” theme (Figure 3).
In this process, words are assigned a fuzzy degree of
membership with respect to each cluster (topic). The
color of circles shows the magnitude of membership
from low (light Grey) to high (dim Grey).

The main goal of FLSA is to ﬁnd two matrices:
P (T |D) and P (W |T ) mentioned in Figure 1. FLSA has
seven steps using Local Term Weighting (LTW), Global
Term Weighting (GTM), and Fuzzy Clustering (FC):

Step 1. The ﬁrst step is to calculate LTW. Among
diﬀerent LTW methods, we use term frequency (TF) as
it is the most popular method.

Step 2. The next step is to calculate GTW. We ex-
plore four GTW methods in this research including En-
tropy, Inverse Document Frequency (IDF), Probabilistic
Inverse Document Frequency (ProbIDF), and Normal
(Table 1):

– Entropy gives higher weight to the terms with less

frequency in few documents [15].

– IDF assigns higher weights to rare terms and lower

weights to common terms [42].

– Normal is used to correct discrepancies in document
lengths and also normalize the document vectors
[37].

– ProbIDF is similar to IDF and assigns very low neg-
ative weight for the terms occurring in every docu-
ment [37]

Symbol tfij deﬁnes the number of times word i oc-
curs in document j. With m words and n documents,

we need to ﬁnd b(tfij) and pij for calculating the four
mentioned GTW methods:

b(tfij) =

(cid:26) 1 tfij > 0
0 tfij = 0

pij =

tfij
j tfij

(cid:80)

(1)

(2)

Name

Entropy
IDF

Normal

ProbIDF

Formula
(cid:80)

1 +
log2
1√(cid:80)

log2

j pij log2(pij )
log2 n

(cid:80)

n
j tfij

j tf 2
ij
n−(cid:80)
(cid:80)

j b(tfij )

j b(tfij )

Table 1 GTW Methods

The outputs of this step are the document term ma-
trices with applied TF-Entropy, TF-IDF, TF-Normal,
and TF-ProbIDF methods.

Step 3. We use Fuzzy C-means (FCM) in this re-
search to fuzzy cluster the documented represented by
the four mentioned GTW methods. FCM minimizes an
objective function by considering constraints:

M in Jq(µ, V, X) =

(µkj)qDIS2
kj

(3)

c
(cid:88)

n
(cid:88)

k=1

j=1

Fuzzy Approach Topic Discovery in Health and Medical Corpora

subject to:

0 ≤ µkj ≤ 1;

c
(cid:88)

k=1

µkj = 1

0 <

µkj < n;

n
(cid:88)

j=1

Where:

n= number of data
c= number of clusters (topics)
µkj= membership value
q= fuzziﬁer, 1 < q ≤ ∞
V = cluster center vector
DISkj = d(xj, vk)= distance between xj and vk

By optimizing eq.3:

µij =

1
l=1( DISkj
DISlj

(cid:80)c

2
q−1

)

vi =

(cid:80)n

j=1(µkj)qxj
(cid:80)n
j=1(µkj)q

The iterations in the clustering algorithm continue
till the maximum changes in µkj becomes less than or
equal to a pre-speciﬁed threshold with O(n) computa-
tional time complexity.

We use µkj as the membership degrees for each doc-
ument (D) with respect to each of topics (clusters). The
value of µkj is between 0 and 1 that can be interpreted
as P(Tk|Dj) or probability of topic k in document j.
This step ﬁnds T opics × Documents matrix and uses it
along with the following steps to ﬁnd W ords × T opics
matrix in Figure 1. It is worth mentioning that FLAS’s
steps are dependent and integrated. For example, the
documents (D) and the topics (T) in step 3 are the same
topics and documents in steps 5,6 and 7.

To avoid the negative impact of high dimensional-
ity of the four mentioned matrices in step 2, we use
singular value decomposition (SVD), which is a pop-
ular method [16] to reduce the data dimension before
using fuzzy clustering. We select two dimensions, as the
minimum number of dimensions, for SVD to have a fast
process.

(4)

(5)

(6)

(7)

(8)

5

(9)

(10)

(11)

(12)

Step 4. We use document-term matrices with the
GTW methods in step 2 (W ords × Documents matrix)
to ﬁnd P(Dj) or probability of document j:

P (Dj) =

(cid:80)m

i=1(Wi, Dj)
(cid:80)n

j=1(Wi, Dj)

(cid:80)m

i=1

Step 5. The next step has two parts. The ﬁrst part
is to ﬁnd P(Dj|Tk) or probability of document j in
topic k using P (Tk|Dj) in step 3 and P (Dj) in step 4:

P (Dj, Tk) = P (Tk|Dj) × P (Dj)

Then normalizing P (D, T ) in each topic:

P (Dj|Tk) =

P (Dj, Tk)
j=1 P (Dj, Tk)

(cid:80)n

Step 6: We use document-term matrices with the GTW
methods in step 2 to ﬁnd P(Wi|Dj) or probability of
word i in document j:

P (Wi|Dj) =

P (Wi, Dj)
i=1 P (Wi, Dj)

(cid:80)m

Step 7: The ﬁnal step is to ﬁnd P(Wi|Tk) or proba-
bility of word i in topic k (T opics × Documents matrix
in Figure 1) using P (Dj|Tk) in step 5 and P (Wi|Dj) in
step 6:

P (Wi|Tk) =

P (Wi|Dj) × P (Dj|Tk)

(13)

n
(cid:88)

j=1

FLSA is ﬂexible to work with all dimensionality re-
duction techniques [16] such as principal component
analysis (PCA) [25] and all fuzzy clustering techniques
such as self-organizing map (SOM) [36].

Fuzzy c-means, as it is the core of FLSA, can be ap-
plied on both discrete and continuous data. This feature
enables FLSA to use a wide range of LTW and GTW
methods and to be used for other machine learning and
data science applications such as image processing [35].
Moreover, optimization nature of fuzzy clustering pro-
vides a solution to estimate the optimum number of
topics.

4 Experiments

In this section, we evaluate FLSA against LDA by doc-
ument classiﬁcation using Random Forest, document
clustering using k-means, document modeling using log-
likelihood, and execution time test. We also evaluate
FLSA against RedLDA by document modeling on re-
dundant documents.

6

Amir Karami et al.

We use ﬁve datasets, the Matlab package for Chib-
style estimation of log-likelihood1, the FCM Matlab
package2 with its default settings including 100 iter-
ations and 1e-5 as the minimum improvement in ob-
jective function between two consecutive iterations, the
Weka tool3 for classiﬁcation evaluation, the MALLET
package4 with its default settings for implementing LDA,
and the Python package for implementing RedLDA5.
The source code for FLSA will be available in the ﬁrst
author’s website6,7 in R and Matlab platforms.

4.1 Datasets

We leverage ﬁve available health and medical datasets
in this research (Table 2):

– The ﬁrst dataset8 is MuchMore Springer Bilingual
Corpus (M-Dataset) which is a labeled corpus of En-
glish scientiﬁc medical abstracts from the Springer
website. In this research, we use the ﬁrst 2 journals
including: Arthroscopy and Federal Health Stan-
dard Sheet.

– The second dataset9 is an unlabeled corpus of 2,434

nursing notes (N-Dataset).

– The third dataset10 is Ohsumed Collection (O-Dataset)
that is a labeled corpus of medical abstracts from
the MeSH categories including Bacterial Infections
and Mycoses, and Virus Diseases.

– The fourth dataset (T-Dataset) is health news from
Twitter11. We collected the tweets from the health
related Twitter accounts including cbchealth, every-
dayhealth, foxnewshealth, goodhealth, kaiserhealthnews,
latimeshealth, msnhealthnews, NBChealth, nprhealth,
usnewshealth, bbchealth, cnnhealth, gdnhealthcare, ny-
timeshealth, reuters health, and wsjhealth from Au-
gust 2011 to December 201412.

– The ﬁfth dataset is R-Datasets13 are the synthesis
text documents to track the negative eﬀect of redun-
dancy in documents [8]. These datasets are subsets

1http://www.cs.umass.edu/~wallach/code/etm/
2http://www.mathworks.com/help/fuzzy/fcm.html
3http://www.cs.waikato.ac.nz/ml/weka/
4http://mallet.cs.umass.edu/
5https://sourceforge.net/projects/redlda/
6https://sites.google.com/site/karamihomepage/
7https://github.com/amir-karami
8http://muchmore.dfki.de/resources1.htm
9http://physionet.org/
10http://disi.unitn.it/moschitti/corpora/ohsumed-

first-20000-docs.tar.gz

11www.twitter.com
12https://github.com/amir-karami/Health-News-

Tweets-Data

13https://sourceforge.net/projects/
corpusredundanc/files/?source=navbar

of a larger dataset called WSJ which has a collec-
tion of the abstracts of Wall Street Journal. In this
dataset, 1300 abstracts were sampled between 1 and
5 times in a uniform manner for 11 times to elimi-
nate bias from random sampling.

4.2 Document Classiﬁcation

The ﬁrst evaluation measure is document classiﬁcation
using two labeled datasets, M-Dataset and O-Dataset.
Both datasets have two classes (labels), M-Dataset with
Arthroscopy and Federal Health Standard Sheet classes,
and O-Dataset with Bacterial Infections and Mycoses,
and Virus Diseases classes.

Document classiﬁcation problem assigns a document
to a class and this problem needs to extract features
from text data. To avoid high dimensionality of BOW
approach for document classiﬁcation, topic modeling re-
duces the number of features by clustering the mean-
ingful related words as a topic. To avoid any possible
bias, we track the performance of FLSA against LDA
with the 10-fold cross validation method that the data
is broken into 10 subsets for 10 iterations. Each of the
subsets is selected for testing and the rest of sets are se-
lected for training. We use 50, 100, 150, and 200 topics
as the input features of documents for Random Forest
method as it is one of the popular and high performance
classiﬁcation methods [44]. The output of Random For-
est is presented as a confusion matrix (Table 7) with the
following deﬁnitions:

– True Negative (TN) is the number of correct pre-

dictions that an instance is negative.

– False Negative (FN) is the number of incorrect of

predictions that an instance negative.

– False Positive (FP) is the number of incorrect pre-

dictions that an instance is positive.

– True Positive (TP) is the number of correct predic-

tions that an instance is positive.

Predicted
Negative Positive

Actual

Negative
Positive

TN
FN

FP
TP

Table 7 Confusion Matrix

For evaluating the performance of the classiﬁcation
algorithm, we use accuracy (ACC), F-measure, Matthews
Correlation Coeﬃcient (MCC), and the area under ROC
(AUC). The evaluation metrics are deﬁned based on the
confusion matrix, as shown in equations 14 - 18:

Fuzzy Approach Topic Discovery in Health and Medical Corpora

7

Dataset Name #Documents
M-Dataset
N-Dataset
O-Dataset
T-Dataset
R-Dataset-1
R-Dataset-2
R-Dataset-3
R-Dataset-4
R-Dataset-5
R-Dataset-6
R-Dataset-7
R-Dataset-8
R-Dataset-9
R-Dataset-10
R-Dataset-11

1,527
1,607
2,092
58,927
2,288
3,310
3,254
3,162
3,211
3,124
3,251
3,212
3,257
3,258
3,284

#Term Tokens
245,931
299,449
198,998
395,635
754,801
785,467
757,603
705,100
751,496
727,687
747,067
747,956
755,261
739,041
764,658

#Unique Terms
14,411
11,059
15,768
25,310
3,265
23,408
22,982
22,264
23,046
22,944
22,609
22,961
22,929
22,913
23,134

Avg Term Per Document Description
96.3
124.8
95.1
6.7
231.2
236.4
232.8
222.9
234.1
232.9
229.8
232.6
231.9
226.8
232.8

Medical Papers
Nursing Notes
Medical Papers
Tweets
News
News
News
News
News
News
News
News
News
News
News

Method

F-Measure MCC ROC #Topics

Method

F-Measure MCC ROC #Topics

Table 2 Basic Statistics for Datasets

LDA
FLSA(Entropy)
FLSA(IDF)
FLSA(Normal)
FLSA(ProbIDF)

Acc
%
90.05
97.66
95.90
91.22
97.66

0.9
0.977
0.959
0.912
0.977

0.969
0.799
0.953 0.99
0.982
0.917
0.971
0.824
0.987
0.953

50
50
50
50
50

LDA
FLSA(Entropy)
FLSA(IDF)
FLSA(Normal)
FLSA(ProbIDF)

Table 3 M-Datatset Classiﬁcation - 50 and 100 Topics

LDA
FLSA(Entropy)
FLSA(IDF)
FLSA(Normal)
FLSA(ProbIDF)

150
0.867
0.536
150
0.995
0.917
150
0.991
0.953
0.905
150
0.992
0.941 0.991 150

LDA
FLSA(Entropy)
FLSA(IDF)
FLSA(Normal)
FLSA(ProbIDF)

Table 4 M-Datatset Classiﬁcation - 150 and 200 Topics

Method

F-Measure MCC ROC #Topics

Method

F-Measure MCC ROC #Topics

Method

F-Measure MCC ROC #Topics

Method

F-Measure MCC ROC #Topics

Acc
%
77.19
95.90
97.66
95.32
97.07

Acc
%
75.38
75.21
75.90
71.25
74.87

Acc
%
72.80
74.87
76.59
72.46
75.04

0.77
0.959
0.977
0.953
0.971

0.72
0.741
0.746
0.677
0.735

0.662
0.735
0.752
0.691
0.735

LDA
FLSA(Entropy)
FLSA(IDF)
FLSA(Normal)
FLSA(ProbIDF)

LDA
FLSA(Entropy)
FLSA(IDF)
FLSA(Normal)
FLSA(ProbIDF)

0.66
0.737

50
0.282
0.331
50
0.343 0.727 50
50
0.63
0.153
50
0.711
0.314

LDA
FLSA(Entropy)
FLSA(IDF)
FLSA(Normal)
FLSA(ProbIDF)

150
0.636
0.136
0.312
150
0.723
0.358 0.732 150
150
0.668
0.194
150
0.726
0.313

LDA
FLSA(Entropy)
FLSA(IDF)
FLSA(Normal)
FLSA(ProbIDF)

Table 6 O-Datatset Classiﬁcation - 150 and 200 Topics

Acc
%
78.36
96.49
98.24
92.39
97.66

Acc
%
82.45
97.076
97.66
92.39
97.66

Acc
%
72.97
76.24
74.35
71.08
74.52

Acc
%
71.08
75.21
74.18
71.94
74.87

0.78
0.965
0.982
0.924
0.977

0.822
0.971
0.977
0.924
0.977

0.682
0.747
0.726
0.694
0.724

0.648
0.74
0.725
0.683
0.729

100
0.895
0.561
0.929
100
0.986
0.964 0.996 100
100
0.984
0.846
100
0.994
0.953

200
0.894
0.646
200
0.992
0.941
200
0.984
0.953
0.846
200
0.982
0.953 0.985 200

0.179
100
0.657
0.344 0.732 100
100
0.712
0.288
100
0.617
0.201
100
0.684
0.283

0.079
200
0.63
0.326 0.731 200
200
0.713
0.285
200
0.657
0.172
200
0.719
0.298

Table 5 O-Datatset Classiﬁcation- 50 and 100 Topics

Method

F-Measure MCC ROC #Topics

Method

F-Measure MCC ROC #Topics

P recision(P ) =

T P
T P + F P

Recall(R) =

T P
T P + F N

Accuracy(Acc) =

T P + T N
T P + T N + F P + F N

(14)

F − measure = 2 ×

P × R
P + R

(17)

ROC curves plot FP on the X axis vs. TP on the
Y axis to ﬁnd the trade oﬀ between them; therefore,
the ROC is closer to the upper left indicating better
performance (Figure 6).

MCC is used to determine the quality of classiﬁ-
cation methods, ranging between -1 (the worst perfor-

(15)

(16)

8

Amir Karami et al.

·104

·104

2

3

4

5

6

7

8

2

3

4

5

6

7

8

# Clusters

# Clusters

F LSA(Entropy)

F LSA(IDF )

F LSA(P robIDF )

F LSA(N ormal)

LDA

Fig. 4 Calinski-Harabasz - 50 Topics Left Figure and 100 Topics Right Figure

·104

·104

x
e
d
n
I

z
s
a
b
a
r
a
H

-
i
k
s
n

i
l
a
C

x
e
d
n
I

z
s
a
b
a
r
a
H

-
i
k
s
n

i
l
a
C

1.5

0.5

1

0

1.5

0.5

1

0

x
e
d
n
I

z
s
a
b
a
r
a
H

-
i
k
s
n

i
l
a
C

x
e
d
n
I

z
s
a
b
a
r
a
H

-
i
k
s
n

i
l
a
C

1.5

0.5

1

0

1.5

0.5

1

0

2

3

4

5

6

7

8

2

3

4

5

6

7

8

# Clusters

# Clusters

F LSA(Entropy)

F LSA(IDF )

F LSA(P robIDF )

F LSA(N ormal)

LDA

Fig. 5 Calinski-Harabasz - 150 Topics Left Figure and 200 Topics Right Figure

TP

FP

Fig. 6 ROC

MCC=

√

mance) and +1 (the best performance).

(T P ×T N )−(F N ×F P )

(T P +F P )×(T P +F N )×(T N +F P )×(T N +F N )

(18)

This experiment shows that FLSA with Entropy,
IDF, Normal, and ProbIDF show better performance
than LDA with diﬀerent numbers of topics (Tables 3 -
6). The highest performance in each table is shown in
bold format.

4.3 Document Clustering

The second evaluation is document clustering using un-
labeled N-Dataset. Internal and external validation are

two major methods for clustering validation; however,
comparison between these two major methods shows
that internal validation is more precise [45]. We evaluate
diﬀerent numbers of topics and clusters with Calinski-
Harabasz (CH) index, as one of the popular internal
validation methods, using K-means with 500 iterations.
CH index evaluates the cluster validity based on the av-
erage of the sum of squared error cluster between and
within clusters. Higher CH index indicates better clus-
tering.

We track the performance of FLSAs and LDA using
diﬀerent numbers of clusters ranging from 2 to 8 with
diﬀerent numbers of topics including 50, 100, 150, and
200. CH index shows that FLSAs have better perfor-
mance than LDA with the diﬀerent ranges of features
and clusters (Figures 4 & 5). The gap between FLSAs
and LDA does not change signiﬁcantly with diﬀerent
numbers of topics and clusters.

4.4 Redundancy Issue

The next experiment explores the eﬀect of redundancy
issue using the ﬁfth dataset. R-Datasets are not a med-
ical corpus; however, they were created as a synthetic
redundant corpus without having privacy issue to mea-
sure the eﬀect of redundancy issue [8]. We select pub-
licly available unlabeled R-Datasets to make the eval-

Fuzzy Approach Topic Discovery in Health and Medical Corpora

9

uation process easier for possible future research. We
compare FLSAs with not only LDA but also RedLDA,
as it was developed to handle redundancy issue in med-
ical text data [9].

We train LDA, RedLDA and FLSA models on R-
Datasets to compare the generalization performance of
the models. We compute the log-likelihood on a held-
out test set to evaluate the models. A higher log-likelihood
score indicates better generalization performance. Fig-
ure 7 shows the average log-likelihood of the R-Datasets
with diﬀerent numbers of topics from 25 to 350. This
experiment indicates that FLSAs performs better than
RedLDA and LDA on redundant documents.

)
s
d
n
o
c
e
S
(

e
m
T

i

400

300

200

100

0

·105

−7.6

−7.8

−8

d
o
o
h
i
l
e
k
i
L
-
g
o
L

0

100

200

300

Number of Topics

Entropy

IDF

N ormal

P robIDF

RedLDA

LDA

Fig. 7 Likelihood Comparison for R-Datasets

0

100

200

300

400

Number of Topics

Entropy

IDF

N ormal

P robIDF

LDA

Fig. 8 Execution Time for T-Dataset

5 Example

Documents and papers in health and medical domains
contain terms and words that need expertise to under-
stand them. Therefore, we show an illustrative exam-
ple with T-Dataset containing tweets with more under-
standable words in this section. First, we run FLSA on
diﬀerent numbers of topics to estimate optimum num-
ber of topics using the objective function ,Jq (Figure 9).
This estimation feature of fuzzy clustering is on of ad-
vantageous of FLSA. The elbow or the knee part of the
ﬁgure shows ∼125 as the number of topics1.

)
J
(

n
o
i
t
c
n
u
F

e
v
i
t
c
e
j
b
O

·10−2

6

4

2

0

0

100

200

300

400

Number of Topics

Fig. 9 Number of Topics Estimation

4.5 Execution Time

In this section, we compare the speed of FLSA in com-
parison with LDA using T-Dataset, the biggest dataset
in this paper. The major process in topic modeling is
based on a joint probability distribution over hidden
topics and the observed words to infer the word with
higher probability in each topic by using the posterior
distribution. The most popular approximate method for
LDA is collapsed Gibbs sampling applied in the experi-
ments. All the inference algorithms require multiple it-
erations which increase the computational cost linearly
with the number of documents, topics, words, and iter-
ations [51]. Figure 8 shows that the time performance
of FLSAs is stable with an increase in the numbers of
topics and considerably better than LDA.

Table 8 shows a sample of topics that are about fam-

ily, cancer, Ebola, Alzheimer, nursing, and teenagers:

– T83 is about the beneﬁt of nursing black teens with

dementia.

– T42 is about the huge costs of teens pregnancy.
– T33 is reporting a teen’s invention to save Alzheimer’s

patients and improve the quality of their lives.
– T101 is about the government’s need to hire more

nurses in diﬀerent cities.

– T85 is about the recipes for family having diabetes

specially for women and kids.

– T107 is about using Ebola for cyberbullying.
– T71 is about the role of psychiatrists against Ebola.

1https://www.cs.princeton.edu/courses/archive/

spring07/cos424/scribe_notes/0306.pdf

10

Amir Karami et al.

– T25 is about sneezing as a risk for spreading Ebola
and the relation between Ebola and lung cancer.

6 Conclusion

The vast array of health and medical text data repre-
sents a valuable resource that can be analyzed to ad-
vance state-of-the-art medicine and health. Large elec-
tronic health and medical archives such as PubMed pro-
vide an extremely useful service to the scholarly com-
munity. However, the needs of readers go beyond a sim-
ple keyword search.

Topic modeling is one of the popular unsupervised
methods to automatically discover a hidden thematic
structure in a large collection of unstructured health
and medical documents. This discovered structure fa-
cilitates browsing, searching, and summarizing the col-
lection.

Fuzzy perspective is a machine learning approach
that has been used more in medical image processing
than text processing. Existing techniques of topic mod-
eling are based on two main approaches: linear alge-
bra and statistical distributions; however, this paper
proposes FLSA to utilize fuzzy perspective for disclos-
ing latent semantic features of health and medical text
data.

FLSA is a new competitor to the established topic
models such as LDA and has the ﬂexibility to work with
a wide range of dimension reduction and fuzzy clus-
tering techniques. FLSA also works with both discrete
and continuous data, estimates the optimum number of
topics, and avoids the negative eﬀect of the redundancy
issue in health and medical corpora.

In our future work, we will develop dynamic and
hierarchal topic models using fuzzy perspective. In ad-
dition, FLSA will be applied on social media data to
track public opinions and will be used for online review
and SMS spam detection [29, 31].

References

1. C. C. Aggarwal and C. Zhai. An introduction to
In Mining Text Data, pages 1–10.

text mining.
Springer, 2012.

2. C. Arnold and W. Speier. A topic model of clinical
reports.
In Proceedings of the 35th international
ACM SIGIR conference on Research and devel-
opment in information retrieval, pages 1031–1032.
ACM, 2012.

3. C. W. Arnold, S. M. El-Saden, A. A. Bui, and
R. Taira. Clinical case-based retrieval using latent

topic analysis. In AMIA Annual Symposium Pro-
ceedings, volume 2010, page 26. American Medical
Informatics Association, 2010.

4. T. Asou and K. Eguchi. Predicting protein-protein
relationships from literature using collapsed varia-
tional latent dirichlet allocation. In Proceedings of
the 2nd international workshop on Data and text
mining in bioinformatics, pages 77–80. ACM, 2008.
5. J. C. Bezdek. Pattern recognition with fuzzy objec-
tive function algorithms. Kluwer Academic Pub-
lishers, 1981.

6. D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent
dirichlet allocation. Journal of machine Learning
research, 3:993–1022, 2003.

7. J. H. Chen, M. K. Goldstein, S. M. Asch,
L. Mackey, and R. B. Altman. Predicting inpatient
clinical order patterns with probabilistic topic mod-
els vs conventional order sets. Journal of the Amer-
ican Medical Informatics Association, page ocw136,
2016.

8. R. Cohen, M. Elhadad, and N. Elhadad. Redun-
dancy in electronic health record corpora: analysis,
impact on text mining performance and mitigation
strategies. BMC bioinformatics, 14(1):10, 2013.
9. R. Cohen, I. Aviram, M. Elhadad, and N. El-
hadad. Redundancy-aware topic modeling for pa-
tient record notes. PloS one, 9(2):e87555, 2014.
10. N. Council. Future directions for nsf advanced com-
puting infrastructure to support u.s. science and en-
gineering in 2017-2020: Interim report, 2016. The
National Academies Press Washington, DC.

11. J. A. Dawson and C. Kendziorski.

Survival-
supervised latent dirichlet allocation models for ge-
nomic analysis of time-to-event outcomes. arXiv
preprint arXiv:1202.5999, 2012.

12. G. Defossez, A. Rollet, O. Dameron, and P. In-
grand. Temporal representation of care trajectories
of cancer patients using data from a regional in-
formation system: an application in breast cancer.
BMC medical informatics and decision making, 14
(1):24, 2014.

13. L. Di Lascio, A. Gisolﬁ, A. Albunia, G. Galardi,
and F. Meschi. A fuzzy-based methodology for the
analysis of diabetic neuropathy. Fuzzy Sets and
Systems, 129(2):203–228, 2002.

14. D. Downey, O. Etzioni, and S. Soderland. A proba-
bilistic model of redundancy in information extrac-
tion. Technical report, DTIC Document, 2006.
15. S. Dumais. Enhancing performance in latent se-

mantic indexing (lsi) retrieval, 1992.

16. I. K. Fodor. A survey of dimension reduction tech-

niques, 2002.

Fuzzy Approach Topic Discovery in Health and Medical Corpora

11

T83
teen
nursing
dementia
beneﬁt
ready
babies
race
black
worst
meet

T42
sick
pregnant
billion
clinical
dying
stories
rare
access
role
hope

T33
teens
put
popular
save
alzheimer
leave
run
docs
quality
tied

T101
deadly
government
nurses
aﬀordable
clinic
premiums
ban
city
warns
deadline

T85
diabetes
week
ﬁnd
recipes
work
low
kids
rt
women
life

T71
rt
health
ebola
pms
poached
upset
study

T107
rt
health
ebola
lightning
study
welcomes
vape
cyberbullying tension
visually
tremors

psychiatrists
embryo

T25
rt
health
ebola
study
cancer
care
amp
sneeze
poo
risk

Table 8 A Sample of Topics

17. A. P. Gasch and M. B. Eisen. Exploring the condi-
tional coregulation of yeast gene expression through
fuzzy k-means clustering. Genome Biol, 3(11):1–22,
2002.

18. M. Ghassemi, T. Naumann, F. Doshi-Velez,
N. Brimmer, R. Joshi, A. Rumshisky, and
P. Szolovits. Unfolding physiological state: mortal-
ity modelling in intensive care units. In Proceedings
of the 20th ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 75–
84. ACM, 2014.

19. A. E. Hassanien. Intelligent data analysis of breast
cancer based on rough set theory.
International
Journal on Artiﬁcial Intelligence Tools, 12(04):465–
479, 2003.

20. C. M. Helgason and T. H. Jobe. The fuzzy cube
and causal eﬃcacy: representation of concomitant
mechanisms in stroke. Neural Networks, 11(3):549–
555, 1998.

21. C. M. Helgason and T. H. Jobe. Causal interac-
tions, fuzzy sets and cerebrovascular accident: the
limits of evidence-based medicine and the advent
of complexity-based medicine. Neuroepidemiology,
18(2):64–74, 1999.

22. C. M. Helgason, D. Malik, S.-C. Cheng, T. H. Jobe,
and J. N. Mordeson. Statistical versus fuzzy mea-
sures of variable interaction in patients with stroke.
Neuroepidemiology, 20(2):77–84, 2001.

23. A. Hotho, A. N¨urnberger, and G. Paaß. A brief

survey of text mining. volume 20, 2005.

24. Z. Huang, W. Dong, H. Duan, and H. Li. Similarity
measure between patient traces for clinical pathway
analysis: problem, method, and applications. IEEE
journal of biomedical and health informatics, 18(1):
4–14, 2014.

25. I. Jolliﬀe. Principal component analysis. Wiley On-

line Library, 2002.

26. A. Karami. Fuzzy Topic Modeling for Medical Cor-
pora. PhD thesis, UNIVERSITY OF MARYLAND,
BALTIMORE COUNTY, 2015.

27. A. Karami and A. Gangopadhyay. Fftm: A fuzzy
feature transformation method for medical docu-
ments.
In Proceedings of the Conference of the
Association for Computational Linguistics (ACL),
volume 128, 2014.

28. A. Karami and Z. Guo. A fuzzy logic multi-
criteria decision framework for selecting it service
In Proceedings of the Hawaii Inter-
providers.
national Conference on System Science (HICSS),
pages 1118–1127. IEEE, 2012.

29. A. Karami and B. Zhou. Online review spam de-
tection by new linguistic features. iConference 2015
Proceedings, 2015.

30. A. Karami and L. Zhou. Exploiting latent content
based features for the detection of static sms spams.
the 77th annual meeting of the association for in-
formation science and technology (ASIST), 2014.

31. A. Karami and L. Zhou. Improving static sms spam
detection by using new content-based features. the
20th americas conference on information systems
(AMCIS), 2014.

32. A. Karami, H. R. Yazdani, H. S. Beiryaie, and
N. Hosseinzadeh. A risk based model for is out-
In 2nd IEEE Interna-
sourcing vendor selection.
tional Conference on Information and Financial
Engineering (ICIFE), pages 250–254. IEEE, 2010.
33. A. Karami, A. Gangopadhyay, B. Zhou, and
H. Kharrazi. Flatm: A fuzzy logic approach topic
model for medical documents.
In Proceedings of
the Annual Meeting of the North American Fuzzy
Information Processing Society (NAFIPS). IEEE,
2015.

34. A. Karami, A. Gangopadhyay, B. Zhou, and
H. Kharrazi. A fuzzy approach model for uncov-
ering hidden latent semantic structure in medical
text collections. In Proceedings of the iConference,
2015.

35. J. Keller, R. Krisnapuram, and N. R. Pal. Fuzzy
models and algorithms for pattern recognition and
image processing, volume 4. Springer Science &
Business Media, 2005.

12

Amir Karami et al.

36. T. Kohonen. The self-organizing map. Proceedings

pages 573–576. ACM, 2009.

of the IEEE, 78(9):1464–1480, 1990.

37. T. G. Kolda. Limited-memory matrix methods

with applications. 1998.

38. T. M. Mitchell. Machine learning. wcb, 1997.
39. W. K. Moon, S.-C. Chang, C.-S. Huang, and R.-
F. Chang. Breast tumor classiﬁcation using fuzzy
clustering for breast elastography. Ultrasound in
medicine & biology, 37(5):700–708, 2011.

40. C. A. Naranjo, K. E. Bremner, M. Bazoon, and I. B.
Turksen. Using fuzzy logic to predict response to
citalopram in alcohol dependence. Clinical Phar-
macology & Therapeutics, 62(2):209–224, 1997.

41. E.

I. Papageorgiou, C. D. Stylios, and P. P.
Groumpos. An integrated two-level hierarchical
system for decision making in radiation therapy
based on fuzzy cognitive maps.
IEEE Trans-
actions on Biomedical Engineering, 50(12):1326–
1339, 2003.

42. K. Papineni. Why inverse document frequency?
In Proceedings of the second meeting of the North
American Chapter of the Association for Com-
putational Linguistics on Language technologies,
pages 1–8. Association for Computational Linguis-
tics, 2001.

43. R. Pivovarov, A. J. Perotte, E. Grave, J. Angio-
lillo, C. H. Wiggins, and N. Elhadad. Learning
probabilistic phenotypes from heterogeneous ehr
data. Journal of biomedical informatics, 58:156–
165, 2015.

44. Y. Qi, Z. Bar-Joseph, and J. Klein-Seetharaman.
Evaluation of diﬀerent biological data and compu-
tational classiﬁcation methods for use in protein in-
teraction prediction. Proteins: Structure, Function,
and Bioinformatics, 63(3):490–500, 2006.

45. E. Rend´on, I. Abundez, A. Arizmendi, and E. M.
Quiroz. Internal versus external cluster validation
indexes.
International Journal of computers and
communications, 5(1):27–34, 2011.

46. E. Sarioglu, H.-A. Choi, and K. Yadav. Clinical re-
port classiﬁcation using natural language process-
ing and topic modeling. In Proceedings of the IEEE
International Conference on Machine Learning and
Applications (ICMLA), volume 2, pages 204–209,
2012.

47. J. O. Wrenn, D. M. Stein, S. Bakken, and P. D.
Stetson. Quantifying clinical narrative redundancy
in an electronic health record. Journal of the Amer-
ican Medical Informatics Association, 17(1):49–53,
2010.

48. H. Xu, J. Wang, X.-S. Hua, and S. Li. Tag re-
ﬁnement by regularized lda. In Proceedings of the
17th ACM international conference on Multimedia,

49. L. A. Zadeh. Outline of a new approach to the
analysis of complex systems and decision processes.
IEEE Transactions on Systems, Man and Cyber-
netics, (1):28–44, 1973.

50. G. Zahlmann, B. Kochner, I. Ugi, D. Schuhmann,
B. Liesenfeld, A. Wegner, M. Obermaier, and
M. Mertz. Hybrid fuzzy image processing for situa-
tion assessment [diabetic retinopathy]. IEEE Engi-
neering in Medicine and Biology Magazine, 19(1):
76–83, 2000.

51. J. Zeng, Z.-Q. Liu, and X.-Q. Cao. A new approach
arXiv preprint

to speeding up topic modeling.
arXiv:1204.0170, 2012.

52. H.-J. Zimmermann. Fuzzy set theory. Wiley In-
terdisciplinary Reviews: Computational Statistics,
2(3):317–332, 2010.

Amir Karami is an Assistant Professor in the School

of Library and Information Science and a Faculty As-
sociate at the Arnold School of Public Health at the
University of South Carolina. He is currently working
on developing fuzzy text mining techniques and their
applications in medical, health, and social science.

Aryya Gangopadhyay is a Professor and the Chair
of Information Systems Department at the University
of Maryland Baltimore County. His research interests
are in the areas of databases and data mining. Cur-
rently, he is focused on privacy preserving data min-
ing, spatio-temporal data mining, and data mining for
health informatics.

Bin Zhou is an Assistant Professor in the Depart-
ment of Information Systems at the University of Mary-
land, Baltimore County. His recent research projects
include designing eﬃcient search techniques on large-
scale data, detecting malicious activities that manipu-
late the Web search results, and protecting users pri-
vacy in large social networks.

Hadi Kharrazi is an Assistant Professor in the
Johns Hopkins Bloomberg School of Public Health with
a joint appointment at the Johns Hopkins School of
Medicine. His research interest is in contextualizing CDSS
in PHI platforms to be utilized at diﬀerent HIT levels
of managed care such as EHR platforms or consumer
health informatics solutions.

