4
1
0
2
 
v
o
N
 
1
1
 
 
]

G
L
.
s
c
[
 
 
1
v
9
1
9
2
.
1
1
4
1
:
v
i
X
r
a

Bounded Regret for Finite-Armed Structured Bandits

Tor Lattimore
Department of Computing Science
University of Alberta, Canada
tlattimo@ualberta.ca

R´emi Munos
INRIA
Lille, France1
remi.munos@inria.fr

Abstract

We study a new type of K-armed bandit problem where the expected return of
one arm may depend on the returns of other arms. We present a new algorithm
for this general class of problems and show that under certain circumstances it
is possible to achieve ﬁnite expected cumulative regret. We also give problem-
dependent lower bounds on the cumulative regret showing that at least in special
cases the new algorithm is nearly optimal.

1 Introduction

The multi-armed bandit problem is a reinforcement learning problem with K actions. At each time-
step a learner must choose an action i after which it receives a reward distributed with mean µi. The
goal is to maximise the cumulative reward. This is perhaps the simplest setting in which the well-
known exploration/exploitation dilemma becomes apparent, with a learner being forced to choose
between exploring arms about which she has little information, and exploiting by choosing the arm
that currently appears optimal.

(a)

(b)

(c)

1

0

µ

We consider a general class of K-
armed bandit problems where the ex-
pected return of each arm may be de-
pendent on other arms. This model
has already been considered when the
dependencies are linear [18] and also
in the general setting studied here
[12, 1]. Let Θ ∋ θ∗ be an arbitrary
parameter space and deﬁne the expected return of arm i by µi(θ∗) ∈ R. The learner is permitted
to know the functions µ1 · · · µK, but not the true parameter θ∗. The unknown parameter θ∗ deter-
mines the mean reward for each arm. The performance of a learner is measured by the (expected)
cumulative regret, which is the difference between the expected return of the optimal policy and the
n
t=1 µIt (θ∗) where It is
(expected) return of the learner’s policy. Rn := n maxi∈1···K µi(θ∗) −
the arm chosen at time-step t.

0
1
Figure 1: Examples

−1

−1

−1

−1

0

1

0

1

θ

A motivating example is as follows. Suppose a long-running company must decide each week
whether or not to purchase some new form of advertising with unknown expected returns. The
problem may be formulated using the new setting by letting K = 2 and Θ = [−∞, ∞]. We assume
the base-line performance without purchasing the advertising is known and so deﬁne µ1(θ) = 0 for
all θ. The expected return of choosing to advertise is µ2(θ) = θ (see Figure (b) above).

Our main contribution is a new algorithm based on UCB [6] for the structured bandit problem with
strong problem-dependent guarantees on the regret. The key improvement over UCB is that the new
algorithm enjoys ﬁnite regret in many cases while UCB suffers logarithmic regret unless all arms
have the same return. For example, in (a) and (c) above we show that ﬁnite regret is possible for all

P

1Current afﬁliation: Google DeepMind.

1

θ∗, while in the advertising problem ﬁnite regret is attainable if θ∗ ≥ 0. The improved algorithm
exploits the known structure and so avoids the famous negative results by Lai and Robbins [17]. One
insight from this work is that knowing the return of the optimal arm and a bound on the minimum
gap is not the only information that leads to the possibility of ﬁnite regret. In the examples given
above neither quantity is known, but the assumed structure is nevertheless sufﬁcient for ﬁnite regret.

Despite the enormous literature on bandits, as far as we are aware this is the ﬁrst time this setting
has been considered with the aim of achieving ﬁnite regret. There has been substantial work on
exploiting various kinds of structure to reduce an otherwise impossible problem to one where sub-
linear (or even logarithmic) regret is possible [19, 4, 10, and references therein], but the focus is
usually on efﬁciently dealing with large action spaces rather than sub-logarithmic/ﬁnite regret. The
most comparable previous work studies the case where both the return of the best arm and a bound
on the minimum gap between the best arm and some sub-optimal arm is known [11, 9], which
extended the permutation bandits studied by Lai and Robbins [16] and more general results by
the same authors [15]. Also relevant is the paper by Agrawal et. al. [1], which studied a similar
setting, but where Θ was ﬁnite. Graves and Lai [12] extended the aforementioned contribution to
continuous parameter spaces (and also to MDPs). Their work differs from ours in a number of
ways. Most notably, their objective is to compute exactly the asymptotically optimal regret in the
case where ﬁnite regret is not possible. In the case where ﬁnite regret is possible they prove only that
the optimal regret is sub-logarithmic, and do not present any explicit bounds on the actual regret.
Aside from this the results depend on the parameter space being a metric space and they assume that
the optimal policy is locally constant about the true parameter.

2 Notation

General. Most of our notation is common with [8]. The indicator function is denoted by 1{expr}
and is 1 if expr is true and 0 otherwise. We use log for the natural logarithm. Logical and/or
are denoted by ∧ and ∨ respectively. Deﬁne function ω(x) = min {y ∈ N : z ≥ x log z, ∀z ≥ y},
which satisﬁes log ω(x) ∈ O(log x). In fact, limx→∞ log(ω(x))/ log(x) = 1.

Bandits. Let Θ be a set. A K-armed structured bandit is characterised by a set of functions
µk : Θ → R where µk(θ) is the expected return of arm k ∈ A := {1, · · · , K} given un-
known parameter θ. We deﬁne the mean of the optimal arm by the function µ∗ : Θ → R with
µ∗(θ) := maxi µi(θ). The true unknown parameter that determines the means is θ∗ ∈ Θ. The best
arm is i∗ := arg maxi µi(θ∗). The arm chosen at time-step t is denoted by It while Xi,s is the sth
reward obtained when sampling from arm i. We denote the number of times arm i has been chosen
at time-step t by Ti(t). The empiric estimate of the mean of arm i based on the ﬁrst s samples is
ˆµi,s. We deﬁne the gap between the means of the best arm and arm i by ∆i := µ∗(θ∗) − µi(θ∗).
The set of sub-optimal arms is A′ := {i ∈ A : ∆i > 0}. The minimum gap is ∆min := mini∈A′ ∆i
while the maximum gap is ∆max := maxi∈A ∆i. The cumulative regret is deﬁned

Rn :=

µ∗(θ∗) −

µIt =

∆It

n

n

t=1
X

t=1
X

n

t=1
X

Note quantities like ∆i and i∗ depend on θ∗, which is omitted from the notation. As is rather
common we assume that the returns are sub-gaussian, which means that if X is the return sampled
from some arm, then ln E exp(λ(X − EX)) ≤ λ2σ2/2. As usual we assume that σ2 is known and
does not depend on the arm. If X1 · · · Xn are sampled independently from some arm with mean µ
n
t=1 Xt, then the following maximal concentration inequality is well-known.
and Sn =

P

P

max
1≤t≤n

|St − tµ| ≥ ε

≤ 2 exp

−

(cid:18)
A straight-forward corollary is that P {|ˆµi,n − µi| ≥ ε} ≤ 2 exp

(cid:26)

(cid:27)

ε2
2nσ2

−

(cid:18)

.

.
(cid:19)

(cid:19)
ε2n
2σ2

It is an important point that Θ is completely arbitrary. The classic multi-armed bandit can be ob-
tained by setting Θ = RK and µk(θ) = θk, which removes all dependencies between the arms. The
setting where the optimal expected return is known to be zero and a bound on ∆i ≥ ε is known can
be regained by choosing Θ = (−∞, −ε]K × {1, · · · , K} and µk(θ1, · · · , θK, i) = θk1{k 6= i}. We
do not demand that µk : Θ → R be continuous, or even that Θ be endowed with a topology.

2

3 Structured UCB

We propose a new algorithm called UCB-S that is a straight-forward modiﬁcation of UCB [6], but
where the known structure of the problem is exploited. At each time-step it constructs a conﬁdence
interval about the mean of each arm. From this a subspace ˜Θt ⊆ Θ is constructed, which contains
the true parameter θ with high probability. The algorithm takes the optimistic action over all θ ∈ ˜Θt.
Algorithm 1 UCB-S

1: Input: functions µ1, · · · , µk : Θ → [0, 1]
2: for t ∈ 1, . . . , ∞ do

3:

4:
5:
6:
7:
8:

Deﬁne conﬁdence set ˜Θt ←

˜θ : ∀i,

µi(˜θ) − ˆµi,Ti(t−1)

<

if ˜Θt = ∅ then

Choose arm arbitrarily

else

(

(cid:12)
(cid:12)
(cid:12)

Optimistic arm is i ← arg maxi sup˜θ∈ ˜Θt
Choose arm i

µi(˜θ)

ασ2 log t
Ti(t − 1) )

s

(cid:12)
(cid:12)
(cid:12)

Remark 1. The choice of arm when ˜Θt = ∅ does not affect the regret bounds in this paper. In
practice, it is possible to simply increase t without taking an action, but this complicates the analysis.
In many cases the true parameter θ∗ is never identiﬁed in the sense that we do not expect that
˜Θt → {θ∗}. The computational complexity of UCB-S depends on the difﬁculty of computing ˜Θt
and computing the optimistic arm within this set. This is efﬁcient in simple cases, like when µk is
piecewise linear, but may be intractable for complex functions.

4 Theorems

We present two main theorems bounding the regret of the UCB-S algorithm. The ﬁrst is for arbitrary
θ∗, which leads to a logarithmic bound on the regret comparable to that obtained for UCB by [6].
The analysis is slightly different because UCB-S maintains upper and lower conﬁdence bounds
and selects its actions optimistically from the model class, rather than by maximising the upper
conﬁdence bound as UCB does.

Theorem 2. If α > 2 and θ ∈ Θ, then the algorithm UCB-S suffers an expected regret of at most

ERn ≤

2∆maxK(α − 1)
α − 2

+

8ασ2 log n
∆i

Xi∈A′

+

∆i

i
X

If the samples from the optimal arm are sufﬁcient to learn the optimal action, then ﬁnite regret is
possible. In Section 6 we give something of a converse by showing that if knowing the mean of the
optimal arm is insufﬁcient to act optimally, then logarithmic regret is unavoidable.

Theorem 3. Let α = 4 and assume there exists an ε > 0 such that

(∀θ ∈ Θ)

|µi∗ (θ∗) − µi∗ (θ)| < ε =⇒ ∀i 6= i∗, µi∗ (θ) > µi(θ).

(1)

Then ERn ≤

with ω∗ := max

32σ2 log ω∗
∆i
8σ2αK
ε2

Xi∈A′ (cid:18)
ω

(cid:26)

(cid:18)

+ ∆i

+ 3∆maxK +

(cid:19)
8σ2αK
∆2

.
min (cid:19)(cid:27)

, ω

(cid:19)

(cid:18)

∆maxK 3
ω∗

,

Remark 4. For small ε and large n the expected regret looks like ERn ∈ O
(for small n the regret is, of course, even smaller).

K

log

1
ε

 

i=1
X

(cid:1)∆i !
(cid:0)

The explanation of the bound is as follows.
If at some time-step t it holds that all conﬁdence
intervals contain the truth and the width of the conﬁdence interval about i∗ drops below ε, then by
the condition in Equation (1) it holds that i∗ is the optimistic arm within ˜Θt. In this case UCB-S

3

suffers no regret at this time-step. Since the number of samples of each sub-optimal arm grows at
most logarithmically by the proof of Theorem 2, the number of samples of the best arm must grow
linearly. Therefore the number of time-steps before best arm has been pulled O(ε−2) times is also
O(ε−2). After this point the algorithm suffers only a constant cumulative penalty for the possibility
that the conﬁdence intervals do not contain the truth, which is ﬁnite for suitably chosen values of α.
Note that Agrawal et. al. [1] had essentially the same condition to achieve ﬁnite regret as (1), but
speciﬁed to the case where Θ is ﬁnite.

An interesting question is raised by comparing the bound in Theorem 3 to those given by Bubeck
et. al. [11] where if the expected return of the best arm is known and ε is a known bound on the
minimum gap, then a regret bound of

log

2∆i
ε

(cid:1)∆i
(cid:0)

O

 

Xi∈A′  

1 + log log

1
ε

(cid:19)!!

(cid:18)
is achieved. If ε is close to ∆i, then this bound is an improvement over the bound given by Theorem
3, although our theorem is more general. The improved UCB algorithm [7] enjoys a bound on the
expected regret of O(
i ). If we follow the same reasoning as above we obtain a
bound comparable to (2). Unfortunately though, the extension of the improved UCB algorithm to
the structured setting is rather challenging with the main obstruction being the extreme growth of
the phases used by improved UCB. Reﬁning the phases leads to super-logarithmic regret, a problem
we ultimately failed to resolve. Nevertheless we feel that there is some hope of obtaining a bound
like (2) in this setting.

log n∆2

i∈A′

1
∆i

P

(2)

Before the proofs of Theorems 2 and 3 we give some example structured bandits and indicate the
regions where the conditions for Theorem 3 are (not) met. Areas where Theorem 3 can be applied
to obtain ﬁnite regret are unshaded while those with logarithmic regret are shaded.

(a)

(b)

(c)

Key:

−1

−1

0
(d)

0
(e)

θ

1
(f)

1

−1

1

−1

0

a hidden message

µ

1

0

1

0

µ

−1

µ1
µ2
µ3

θ

−1

0

1

−1

0

2

3

4

5

6

1

−1 1
Figure 2: Examples

(a) The conditions for Theorem 3 are met for all θ 6= 0, but for θ = 0 the regret strictly vanishes for

all policies, which means that the regret is bounded by ERn ∈ O(1{θ∗ 6= 0} 1

|θ∗| log 1

|θ∗| ).

(b) Action 2 is uninformative and not globally optimal so Theorem 3 does not apply for θ < 1/2
where this action is optimal. For θ > 0 the optimal action is 1, when the conditions are met and
ﬁnite regret is again achieved.

ERn ∈ O

1{θ∗ < 0}

+ 1{θ∗ > 0}

log n
|θ∗|

(cid:18)

log 1
θ∗
θ∗

.

(cid:19)

(c) The conditions for Theorem 3 are again met for all non-zero θ∗, which leads as in (a) to a regret

of ERn ∈ O(1{θ∗ 6= 0} 1

|θ∗| log 1

|θ∗| ).

Examples (d) and (e) illustrate the potential complexity of the regions in which ﬁnite regret is pos-
sible. Note especially that in (e) the regret for θ∗ = 1
2 is logarithmic in the horizon, but ﬁnite for θ∗
arbitrarily close. Example (f) is a permutation bandit with 3 arms where it can be clearly seen that
the conditions of Theorem 3 are satisﬁed.

4

5 Proof of Theorems 2 and 3

We start by bounding the probability that some mean does not lie inside the conﬁdence set.
Lemma 5. P {Ft = 1} ≤ 2Kt exp(−α log(t)) where

Ft = 1

∃i : |ˆµi,Ti(t−1) − µi| ≥

(

2ασ2 log t
Ti(t − 1) )

.

s

Proof. We use the concentration guarantees:

2ασ2 log t
Ti(t − 1) )

s

P {Ft = 1}

(a)
= P

∃i :

µi(θ∗) − ˆµi,Ti(t−1)

≥

(

(cid:12)
(cid:12)

K

P

i=1
X
K

(b)
≤

(c)
≤

(
(cid:12)
(cid:12)
t
P

i=1
X

s=1
X

(

µi(θ∗) − ˆµi,Ti(t−1)

≥

(cid:12)
(cid:12)
|µi(θ∗) − ˆµi,s| ≥

(cid:12)
(cid:12)
2ασ2 log t
Ti(t − 1) )

s

2ασ2 log t
s

)

(d)
≤

r

K

t

i=1
X

s=1
X

2 exp(−α log t)

(e)
= 2Kt1−α

where (a) follows from the deﬁnition of Ft. (b) by the union bound. (c) also follows from the union
bound and is the standard trick to deal with the random variable Ti(t − 1). (d) follows from the
concentration inequalities for sub-gaussian random variables. (e) is trivial.
Proof of Theorem 2. Let i be an arm with ∆i > 0 and suppose that It = i. Then either Ft is true or

8σ2α log n
∆2
i
Note that if Ft does not hold then the true parameter lies within the conﬁdence set, θ∗ ∈ ˜Θt. Suppose
on the contrary that Ft and (3) are both false.

Ti(t − 1) <

=: ui(n)

(3)

(cid:25)

(cid:24)

µi∗ (˜θ)

(a)
≥ µ∗(θ∗)

(b)
= µi(θ∗) + ∆i

(c)
> ∆i + ˆµi,Ti(t−1) −

max
˜θ∈ ˜Θt

2σ2α log t
Ti(t − 1)

s

(d)
≥ ˆµi,Ti(t−1) +

2ασ2 log t
Ti(t − 1)

(e)
≥ max
˜θ∈ ˜Θt

s

µi(˜θ),

where (a) follows since θ∗ ∈ ˜Θt. (b) is the deﬁnition of the gap. (c) since Ft is false. (d) is true
because (3) is false. Therefore arm i is not taken. We now bound the expected number of times that
arm i is played within the ﬁrst n time-steps by

ETi(n)

(a)
= E

1{It = i}

(b)
≤ ui(n) + E

n

n

1{It = i ∧ (3) is false}

t=1
X
(c)
≤ ui(n) + E

n

t=ui+1
X

1{Ft = 1 ∧ It = i}

t=ui+1
X

where (a) follows from the linearity of expectation and deﬁnition of Ti(n). (b) by Equation (3) and
the deﬁnition of ui(n) and expectation. (c) is true by recalling that playing arm i at time-step t
implies that either Ft or (3) must be true. Therefore

ERn ≤

∆i

ui(n) + E

1{Ft = 1 ∧ It = i}

≤

∆iui(n) + ∆maxE

1{Ft = 1}

!

Xi∈A′

n

t=1
X

(4)

Xi∈A′

 

n

t=ui+1
X

Bounding the second summation

n

E

t=1
X

1{Ft = 1}

(a)
=

P {Ft = 1}

n

t=1
X

n

(b)
≤

t=1
X

2Kt1−α

(c)
≤

2K(α − 1)
α − 2

5

where (a) follows by exchanging the expectation and sum and because the expectation of an indicator
function can be written as the probability of the event. (b) by Lemma 5 and (c) is trivial. Substituting
into (4) leads to

ERn ≤

2∆maxK(α − 1)
α − 2

+

8ασ2 log n
∆i

Xi∈A′

+

∆i.

i
X

Before the proof of Theorem 3 we need a high-probability bound on the number of times arm i is
pulled, which is proven along the lines of similar results by [5].

Lemma 6. Let i ∈ A′ be some sub-optimal arm. If z > ui(n), then P {Ti(n) > z} ≤

2Kz2−α
α − 2

.

Proof. As in the proof of Theorem 2, if t ≤ n and Ft is false and Ti(t − 1) > ui(n) ≥ ui(t), then
arm i is not chosen. Therefore

P {Ti(n) > z} ≤

n

P {Ft = 1}

(a)
≤

n

2Kt1−α

(b)
≤ 2K

where (a) follows from Lemma 5 and (b) and (c) are trivial.

t=z+1
X

t=z+1
X

n

t1−αdt

(c)
≤

2Kz2−α
α − 2

z
Z

Lemma 7. Assume the conditions of Theorem 3 and additionally that Ti∗ (t − 1) ≥
Ft is false. Then It = i∗.
Proof. Since Ft is false, for ˜θ ∈ ˜Θt we have:

l

8ασ2 log t
ε2

and

m

|µi∗ (˜θ) − µi∗ (θ∗)|

(a)
≤ |µi∗ (˜θ) − ˆµi∗,Ti(t−1)| + |ˆµi∗,Ti(t−1) − µi∗ (θ∗)|

(b)
< 2

2σ2α log t
Ti∗ (t − 1)

(c)
≤ ε

s

where (a) is the triangle inequality. (b) follows by the deﬁnition of the conﬁdence interval and
because Ft is false. (c) by the assumed lower bound on Ti∗ (t − 1). Therefore by (1), for all ˜θ ∈ ˜Θt
it holds that the best arm is i∗. Finally, since Ft is false, θ∗ ∈ ˜Θt, which means that ˜Θt 6= ∅.
Therefore It = i∗ as required.
Proof of Theorem 3. Let ω∗ be some constant to be chosen later. Then the regret may be written as

ERn ≤ E

∆i1{It = i} + ∆maxE

1{It 6= i∗} .

(5)

∗

ω

K

t=1
X

i=1
X

n

t=ω∗+1
X

The ﬁrst summation is bounded as in the proof of Theorem 2 by

∗

ω

E

∆i1{It = i} ≤

∆i +

8ασ2 log ω∗
∆i

∗

ω

t=1
X
We now bound the second sum in (5) and choose ω∗. By Lemma 6, if n
K > ui(n), then

t=1
X

Xi∈A

Xi∈A′ (cid:18)

(cid:19)

+

P {Ft = 1} .

(6)

P

Ti(n) >

n
K

≤

2K
α − 2

K
n

(cid:18)

α−2

.

n
8σ2αK
ε2

, ω

o
8σ2αK
∆2
min

(cid:17)

(cid:16)

(cid:17)o

(cid:19)
K > ui(t) for all i 6= i∗ and t

K ≥

. Then t

(7)

Suppose t ≥ ω∗ := max
8σ2α log t
ε2

(cid:16)
. By the union bound

n

ω

P

Ti∗ (t) <

(cid:26)

8σ2α log t
ε2

(a)
≤ P

Ti∗ (t) <

(cid:27)

(cid:26)

(b)
≤ P

t
K

(cid:27)

(cid:26)

∃i : Ti(t) >

(c)
<

2K 2
α − 2

K
t

t
K

(cid:27)

α−2

(cid:18)

(cid:19)

(8)

6

where (a) is true since t
Now if Ti(t) ≥ 8σ2α log t

ε2

n

E

t=ω∗+1
X

K ≥ 8σ2α log t

ε2

. (b) since

K
i=1 Ti(t) = t. (c) by the union bound and (7).

and Ft is false, then the chosen arm is i∗. Therefore
P

1{It 6= i∗} ≤

P {Ft = 1} +

P

Ti(t − 1) <

n

n

8σ2α log t
ε2

(cid:27)

(a)
≤

(b)
≤

t=ω∗+1
X
n

t=ω∗+1
X
n

t=ω∗+1
X

P {Ft = 1} +

P {Ft = 1} +

(cid:26)
n

t=ω∗+1
X
2K 2
α − 2

α−2

K
t

t=ω∗+1 (cid:18)
X
2K 2
(α − 2)(α − 3)

(cid:19)
K
ω∗

(cid:18)

(cid:19)

α−3

(9)

where (a) follows from (8) and (b) by straight-forward calculus. Therefore by combining (5), (6)
and (9) we obtain

ERn ≤

≤

Xi:∆i>0

Xi:∆i>0

8σ2α log ω∗
∆2
i

8σ2α log ω∗
∆2
i

∆i

∆i

(cid:24)

(cid:24)

+

+

(cid:25)

(cid:25)

2∆maxK 2
(α − 2)(α − 3)

2∆maxK 2
(α − 2)(α − 3)

K
ω∗

K
ω∗

(cid:18)

(cid:19)

(cid:18)

(cid:19)

α−3

n

+ ∆max

P {Ft = 1}

α−3

+

t=1
X
2∆maxK(α − 1)
α − 2

Setting α = 4 leads to ERn ≤

K

i=1 (cid:18)
X

32σ2 log ω∗
∆i

+ ∆i

+ 3∆maxK +

(cid:19)

∆maxK 3
ω∗

.

6 Lower Bounds and Ambiguous Examples

We prove lower bounds for two illustrative examples of structured bandits. Some previous work
is also relevant. The famous paper by Lai and Robbins [17] shows that the bound of Theorem 2
cannot in general be greatly improved. Many of the techniques here are borrowed from Bubeck et.
al. [11]. Given a ﬁxed algorithm and varying θ we denote the regret and expectation by Rn(θ) and
Eθ respectively. Returns are assumed to be sampled from a normal distribution with unit variance,
so that σ2 = 1.
(a)

(d)

(b)

(c)

Key:

∆

∆

µ1
µ2

−1

0

1

−1

0

−1

0

1

a hidden message

−1

1
1
Figure 3: Counter-examples

0

Theorem 8. Given the structured bandit depicted in Figure 3.(a) or Figure 2.(c), then for all θ > 0
and all algorithms the regret satisﬁes max {E−θRn(−θ), EθRn(θ)} ≥ 1
8θ for sufﬁciently large n.
Proof. The proof uses the same technique as the proof of Theorem 5 in the paper by [11]. Fix an
algorithm and let Pθ,t be the probability measure on the space of outcomes up to time-step t under
the bandit determined by parameter θ.

E−θRn(−θ) + EθRn(θ)

(a)
= 2θ

E−θ

1{It = 1} + Eθ

1{It = 2}

n

n

 

n

t=1
X
(P−θ,t {It = 1} + Pθ,t {It = 2})

t=1
X

n

(c)
≥ θ

(b)
= 2θ

!

exp (− KL(P−θ,t, Pθ,t))

1

0

µ

−1

t=1
X
n

t=1
X

(d)
= θ

exp

−4tθ2

(e)
≥

1
8θ

(cid:0)

(cid:1)

7

t=1
X

where (a) follows since 2|θ| is the gap between the expected returns of the two arms given parameter
θ and by the deﬁnition of the regret. (b) by replacing the expectations with probabilities. (c) follows
from Lemma 4 by [11] where KL(P−θ,t, Pθ,t) is the relative entropy between measures P−θ,t and
Pθ,t. (d) is true by computing the relative entropy between two normals with unit variance and
means separated by 2θ, which is 4θ2. (e) holds for sufﬁciently large n.
Theorem 9. Let Θ, {µ1, µ2} be a structured bandit where returns are sampled from a normal dis-
tribution with unit variance. Assume there exists a pair θ1, θ2 ∈ Θ and constant ∆ > 0 such that
µ1(θ1) = µ1(θ2) and µ1(θ1) ≥ µ2(θ1) + ∆ and µ2(θ2) ≥ µ1(θ2) + ∆. Then the following hold:

(1) Eθ1 Rn(θ1) ≥ 1+log 2n∆2

8∆

− 1
2

Eθ2 Rn(θ2)

(2) Eθ2 Rn(θ2) ≥ n∆

2 exp (−4Eθ1Rn(θ1)∆) − Eθ1 Rn(θ1)

A natural example where the conditions are satisﬁed is depicted in Figure 3.(b) and by choosing θ1 =
−1, θ2 = 1. We know from Theorem 3 that UCB-S enjoys ﬁnite regret of Eθ2Rn(θ2) ∈ O( 1
∆ log 1
∆ )
and logarithmic regret Eθ1 Rn(θ1) ∈ O( 1
∆ log n). Part 1 of Theorem 9 shows that if we demand
ﬁnite regret Eθ2 Rn(θ2) ∈ O(1), then the regret Eθ1 Rn(θ1) is necessarily logarithmic. On the other
hand, part 2 shows that if we demand Eθ1Rn(θ1) ∈ o(log(n)), then the regret Eθ2 Rn(θ2) ∈ Ω(n).
Therefore the trade-off made by UCB-S essentially cannot be improved.
Proof of Theorem 9. Again, we make use of the techniques of [11].

Eθ1Rn(θ1) + Eθ2Rn(θ2)

(a)
≥ ∆ (Eθ1T2(n) + Eθ2T1(n))

(b)
≥ ∆

(Pθ1 {It = 2} + Pθ2 {It = 1})

n

t=1
X

(c)
≥

(e)
≥

∆
2

n∆
2

n

t=1
X
(d)
≥

n∆
2

(f )
≥

n∆
2

exp (− KL(Pθ1,t, Pθ2,t))

exp (− KL(Pθ1,n, Pθ2,n))

exp

−4∆2Eθ1T2(n)

exp (−4∆Eθ1Rn(θ1))

(10)

where (a) follows from the deﬁnition of the regret and the bandits used. (b) by the deﬁnition of
Tk(n). (c) by Lemma 4 of [11]. (d) since the relative entropy KL(Pθ1,t, Pθ2,t) is increasing with
t. (e) By checking that KL(Pθ1,n, Pθ2,n) = 4∆2Eθ1 T2(n). (f) by substituting the deﬁnition of the
regret. Now part 2 is completed by rearranging (10). For part 1 we also rearrange (10) to obtain

(cid:0)

(cid:1)

Eθ1 Rn(θ1) ≥

exp (−4∆Eθ1Rn(θ1)) − Eθ2 Rn(θ2)

n∆
2

Letting x = Eθ1Rn(θ1) and using the constraint above we obtain:

x ≥

+

x
2

n∆
2

1
2

(cid:18)

exp (−4∆x) − Eθ2Rn(θ2)

.

(cid:19)

But by simple calculus the function on the right hand side is minimised for x = 1
which leads to

4∆ log(2n∆2),

Eθ1 Rn(θ1) ≥

log(2n∆2)
8∆

+

1
8∆

1
2

−

Eθ2 Rn(θ2).

Discussion of Figure 3.(c/d). In both examples there is an ambiguous region for which the lower
bound (Theorem 9) does not show that logarithmic regret is unavoidable, but where Theorem 3
cannot be applied to show that UCB-S achieves ﬁnite regret. We managed to show that ﬁnite regret
is possible in both cases by using a different algorithm. For (c) we could construct a carefully
tuned algorithm for which the regret was at most O(1) if θ ≤ 0 and O( 1
θ ) otherwise. This
result contradicts a claim by Bubeck et. al. [11, Thm. 8]. Additional discussion of the ambiguous
case in general, as well as this speciﬁc example, may be found in the supplementary material. One
observation is that unbridled optimism is the cause of the failure of UCB-S in these cases. This is
illustrated by Figure 3.(d) with θ ≤ 0. No matter how narrow the conﬁdence interval about µ1, if
the second action has not been taken sufﬁciently often, then there will still be some belief that θ > 0
is possible where the second action is optimistic, which leads to logarithmic regret. Adapting the
algorithm to be slightly risk averse solves this problem.

θ log log 1

8

7 Experiments

We tested Algorithm 1 on a selection of structured bandits depicted in Figure 2 and compared to
UCB [6, 8]. Rewards were sampled from normal distributions with unit variances. For UCB we
chose α = 2, while we used the theoretically justiﬁed α = 4 for Algorithm 1. All code is available
in the supplementary material. Each data-point is the average of 500 independent samples with the
blue crosses and red squares indicating the regret of UCB-S and UCB respectively.

200

100

)
θ
(
n
R
θ
ˆE

200

100

)
θ
(
n
R
θ
ˆE

0

0

0
−0.2 −0.1

0.1

0.2

0

θ

5e4
n

1e5

K = 2, µ1(θ) = θ, µ2(θ) = −θ,
n = 50 000 (see Figure 2.(a))

K = 2, µ1(θ) = θ, µ2(θ) = −θ,
θ = 0.04 (see Figure 2.(a))

K = 2, µ1(θ) = 0, µ2(θ) = θ,
n = 50 000 (see Figure 2.(b))

The results show that Algorithm 1 typically out-performs regu-
lar UCB. The exception is the top right experiment where UCB
performs slightly better for θ < 0. This is not surprising, since
in this case the structured version of UCB cannot exploit the ad-
ditional structure and suffers due to worse constant factors. On
the other hand, if θ > 0, then UCB endures logarithmic regret
and performs signiﬁcantly worse than its structured counterpart.
The superiority of Algorithm 1 would be accentuated in the top
left and bottom right experiments by increasing the horizon.

8 Conclusion

)
θ
(
n
R
θ
ˆE

400

200

0

−1

)
θ
(
n
R
θ
ˆE

150

100

50

0

−1

0

θ

0

θ

1

1

K = 2, µ1(θ) = θ1{θ > 0},
µ2(θ) = −θ1{θ < 0},
n = 50 000 (see Figure 2.(c))

The limitation of the new approach is that the proof techniques and algorithm are most suited to
the case where the number of actions is relatively small. Generalising the techniques to large action
spaces is therefore an important open problem. There is still a small gap between the upper and
lower bounds, and the lower bounds have only been proven for special examples. Proving a general
problem-dependent lower bound is an interesting question, but probably extremely challenging given
the ﬂexibility of the setting. We are also curious to know if there exist problems for which the optimal
regret is somewhere between ﬁnite and logarithmic. Another question is that of how to deﬁne
Thompson sampling for structured bandits. Thompson sampling has recently attracted a great deal of
attention [13, 2, 14, 3, 9], but so far we are unable even to deﬁne an algorithm resembling Thompson
sampling for the general structured bandit problem. Not only because we have not endowed Θ
with a topology, but also because choosing a reasonable prior seems rather problem-dependent. An
advantage of our approach is that we do not rely on knowing the distribution of the rewards while
with one notable exception [9] this is required for Thompson sampling.

Acknowledgements. Tor Lattimore was supported by the Google Australia Fellowship for Ma-
chine Learning and the Alberta Innovates Technology Futures, NSERC. The majority of this work
was completed while R´emi Munos was visiting Microsoft Research, New England. This research
was partially supported by the European Community’s Seventh Framework Programme under grant
agreements no. 270327 (project CompLACS).

References

[1] Rajeev Agrawal, Demosthenis Teneketzis, and Venkatachalam Anantharam. Asymptotically
efﬁcient adaptive allocation schemes for controlled markov chains: Finite parameter space.
Automatic Control, IEEE Transactions on, 34(12):1249–1259, 1989.

[2] Shipra Agrawal and Navin Goyal. Analysis of Thompson sampling for the multi-armed bandit

problem. In In Proceedings of the 25th Annual Conference on Learning Theory, 2012.

[3] Shipra Agrawal and Navin Goyal. Further optimal regret bounds for thompson sampling. In
In Proceedings of the 16th International Conference on Artiﬁcial Intelligence and Statistics,
volume 31, pages 99–107, 2013.

9

[4] Kareem Amin, Michael Kearns, and Umar Syed. Bandits, query learning, and the haystack
dimension. Journal of Machine Learning Research-Proceedings Track, 19:87–106, 2011.
[5] Jean-Yves Audibert, R´emi Munos, and Csaba Szepesv´ari. Variance estimates and exploration
function in multi-armed bandit. Technical report, research report 07-31, Certis-Ecole des Ponts,
2007.

[6] Peter Auer, Nicol´o Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed

bandit problem. Machine Learning, 47:235–256, 2002.

[7] Peter Auer and Ronald Ortner. UCB revisited: Improved regret bounds for the stochastic

multi-armed bandit problem. Periodica Mathematica Hungarica, 61(1-2):55–65, 2010.

[8] S´ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic
Multi-armed Bandit Problems. Foundations and Trends in Machine Learning. Now Publishers
Incorporated, 2012.

[9] S´ebastien Bubeck and Che-Yu Liu. Prior-free and prior-dependent regret bounds for thompson
sampling. In Advances in Neural Information Processing Systems, pages 638–646, 2013.
[10] S´ebastien Bubeck, R´emi Munos, Gilles Stoltz, and Csaba Szepesv´ari. Online optimization in

X-armed bandits. In NIPS, pages 201–208, 2008.

[11] S´ebastien Bubeck, Vianney Perchet, and Philippe Rigollet. Bounded regret in stochastic multi-

armed bandits. In In Proceedings of the 26th Annual Conference on Learning Theory, 2013.

[12] Todd L Graves and Tze Leung Lai. Asymptotically efﬁcient adaptive choice of control laws in

controlled Markov chains. SIAM journal on control and optimization, 35(3):715–743, 1997.

[13] Emilie Kaufmann, Nathaniel Korda, and R´emi Munos. Thompson sampling: An asymptoti-
cally optimal ﬁnite-time analysis. In Algorithmic Learning Theory, pages 199–213. Springer,
2012.

[14] Nathaniel Korda, Emilie Kaufmann, and R´emi Munos. Thompson sampling for 1-dimensional
In Advances in Neural Information Processing Systems, pages

exponential family bandits.
1448–1456, 2013.

[15] Tze Leung Lai and Herbert Robbins. Asymptotically optimal allocation of treatments in se-
quential experiments. In T. J. Santner and A. C. Tamhane, editors, Design of Experiments:
Ranking and Selection, pages 127–142. 1984.

[16] Tze Leung Lai and Herbert Robbins. Optimal sequential sampling from two populations.

Proceedings of the National Academy of Sciences, 81(4):1284–1286, 1984.

[17] Tze Leung Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Ad-

vances in applied mathematics, 6(1):4–22, 1985.

[18] Adam J Mersereau, Paat Rusmevichientong, and John N Tsitsiklis. A structured multiarmed
bandit problem and the greedy policy. Automatic Control, IEEE Transactions on, 54(12):2787–
2802, 2009.

[19] Dan Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic
exploration. In Advances in Neural Information Processing Systems, pages 2256–2264, 2013.

A Ambiguous Case

We assume for convenience that K = 2 and µ1(θ) 6= µ2(θ) for all θ ∈ Θ. The second assumption
is non-restrictive, since an algorithm cannot perform badly on the θ for which µ1(θ) = µ2(θ), so we
can simply remove these points from the parameter space. Now Θ can be partitioned into three sets
according to whether or not ﬁnite regret is expected by Theorem 3, or impossible by Theorem 9.

Θeasy :=
Θhard :=
Θamb := Θ − Θeasy − Θhard

(cid:8)

(cid:8)

θ ∈ Θ : ∃ε > 0 such that
θ ∈ Θ : ∃θ′ ∈ Θ such that µi∗(θ)(θ) = µi∗(θ)(θ′) and i∗(θ′) 6= i∗(θ)

µi∗(θ)(θ′) − µi∗(θ)(θ′)

< ε =⇒ i∗(θ′) = i∗(θ)

(cid:9)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:9)

The topic of this section is to study whether or not ﬁnite regret is possible on Θamb, and what
sacriﬁces need to be made in order to achieve this. Some examples are given in Figure 4. Note that
(a) was considered by Bubeck et. al. [11, Thm. 8] and will receive special attention here.

10

(a)

(b)

(c)

(d)

Key:

µ1

µ2

1

0

µ

−1

−1

0
Θamb = [−1, 0]
Θeasy = (0, 1]

1

−1

0
Θamb = [−1, 1]
Θeasy = ∅

1

−1

0
Θamb = [−1, 0]
Θeasy = (0, 1]

1

−1

0
Θamb = [−1, 0]
Θeasy = (0, 1]

1

a hidden message

Figure 4: Ambiguous examples

The main theorem is this section shows that ﬁnite regret is indeed possible for many θ ∈ Θamb
without incurring signiﬁcant additional regret for θ ∈ Θeasy and retaining logarithmic regret for
θ ∈ Θhard. The following algorithm is similar to Algorithm 1, but favours actions which may
be optimal for some plausible ambiguous θ. Theorems will be given subsequently, but proofs are
omitted.
Algorithm 2
1: Input: functions µ1, · · · , µk : Θ → [0, 1], {βt}∞
t=1
2: κ1 = 0
3: for t ∈ 1, . . . , ∞ do

4:

5:
6:

7:

8:

Deﬁne conﬁdence set: ˜Θt ←

˜θ : ∀i,

µi(˜θ) − ˆµi,Ti(t−1)

<

if κt = 0 ∧ ∃ ˜Θt ∩ Θamb 6= ∅ then

Choose θ ∈ ˜Θt ∩ Θamb arbitrarily and set κt = i∗(θ)

(

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

Choose It = arg max

sup
θ∈ ˜Θt

k

κt+1 ← 1{It = κt}

µk(θ) + 1{k = κt}

βt log t
Tk(t − 1)

s

ασ2 log t
Ti(t − 1) )

s

Theorem 10. Suppose K = 2, θ ∈ Θ and i∗ = 1 and βt = log log t and ∆ := µ1(θ) − µ2(θ).
Then Algorithm 2 satisﬁes:

EθRn(θ)/ log n < ∞.
1. lim supn→∞
2. If θ ∈ Θeasy, then EθRn(θ) ∈ O( 1

∆ (log 1

∆ )(log log 1

∆ )).

3. If θ is such that

lim
δ→0

sup
θ′:|µ1(θ)−µ1(θ′)|<δ

µ2(θ′) − µ1(θ′)
|µ1(θ) − µ1(θ′)|

< ∞,

(11)

then limn→∞ EθRn(θ) < ∞.

Remark 11. The condition (11) not satisﬁed for θ ∈ Θhard, since in this case there exists some
θ′ with µ1(θ) = µ1(θ′) but where µ2(θ′) − µ1(θ′) > 0. The condition may not be satisﬁed even
for θ ∈ Θamb. See, for example, Figure 4.(c). The condition is satisﬁed for all other ambiguous
θ for the problems shown in Figure 4.(a,b,d) where the risk µ2(θ′) − µ1(θ′) decreases linearly as
µ1(θ) − µ1(θ′) converges to zero.

The following theorem shows that you cannot get ﬁnite regret for the ambiguous case where (11) is
not satisﬁed without making sacriﬁces in the easy case.
Theorem 12. Suppose θ ∈ Θamb with i∗(θ) = 1 and EθRn(θ) ∈ O(1). Then there exists a constant
c > 0 such that for each θ′ with i∗(θ′) = 2 we have

Eθ′ Rn(θ′) ≥ c

µ2(θ′) − µ1(θ′)
(µ1(θ) − µ1(θ′))2 .

Therefore if the condition (3) in the statement of Theorem 10 is not satisﬁed for some ambiguous θ,
then we can construct a sequence {θ′}∞
(µ2(θ′

i=1 such that limi→∞ µ1(θ′

i) = µ1(θ) and where

i) − µ1(θ′))Eθ′

i) = ∞,

Rn(θ′

i

lim
i→∞

11

which means that the regret must grow faster than the inverse of the gap. The situation becomes
worse the faster the quantity below diverges to inﬁnity.

sup
θ′:|µ1(θ)−µ1(θ′)|<δ

µ2(θ′) − µ1(θ)
|µ1(θ) − µ1(θ′)|

.

In summary, ﬁnite regret is often possible in the ambiguous case, but may lead to worse regret
guarantees in the easy case. Ultimately we are not sure how to optimise these trade-offs and there
are still many interesting unanswered questions.

Analysis of Figure 4.(a)

We now consider a case of special interest that was previously studied by Bubeck et. al. [11] and
is depicted in Figure 4.(a). The structured bandit falls into the ambiguous case when θ ≤ 0, since
no interval about µ1(θ) = 0 is sufﬁcient to rule out the possibility that the second action is in fact
optimal. Nevertheless, using a carefully crafted algorithm we show that the optimal regret is smaller
than one might expect. The new algorithm operates in phases, choosing each action a certain number
of times. If all evidence points to the ﬁrst action being best, then this is taken until its optimality
is proven to be implausible, while otherwise the second action is taken. The algorithm is heavily
biased towards choosing the ﬁrst action where estimation is more challenging, and where the cost
of an error tends to be smaller.
Algorithm 3

1: α ← 5
2: for ℓ ∈ 2, . . . , ∞ do
3:
4:
5:
6:

n1,ℓ = 2ℓ and n2,ℓ = ℓ2
Choose each arm k ∈ {1, 2} exactly nk,ℓ times and let ˆµk,ℓ,nk,ℓ be the average return
s ← 0
if ˆµ1,ℓ,n1,ℓ ≥ −

log log n1,ℓ and ˆµ2,n2,ℓ < −1/2 then

α
n1,ℓ

// Iterate over phases

while ˆµ1,ℓ,n1,ℓ+s ≥ −

q

α log log(n1,ℓ+s)
n1,ℓ+s

do

Choose action 1 and s ← s + 1 and ˆµ1,ℓ,n1,ℓ+s is average return of arm 1 this phase

q

7:

8:

9:
10:
11:

else

while ˆµ2,ℓ,n2,ℓ+s ≥ − 1

2 do

Choose action 2 and s ← s + 1 and ˆµ2,ℓ,n2,ℓ+s is average return of arm 2 this phase

Theorem 13. Let Θ = [−1, 1] and µ1(0) = −θ1{θ > 0} and µ2(θ) = −1{θ ≤ 0}. Assume
returns are normally distributed with unit variance. Then Algorithm 3 suffers regret bounded by
1

EθRn(θ) ∈

θ log log 1

θ

O
O(1)
(cid:0)

(cid:26)

(cid:1)

if θ > 0
otherwise.

Remark 14. Theorem 13 contradicts a result by Bubeck et. al. [11, Thm. 8], which states that for
any algorithm

But by Theorem 13 there exists an algorithm for which

max

E0Rn(0), sup
θ>0

(cid:26)

θ · EθRn(θ)

∈ Ω (log n) .

(cid:27)

1
θ

(cid:27)(cid:19)

max

E0Rn(0), sup
θ>0

(cid:26)

sup
θ>0

(cid:18)

(cid:26)

(cid:27)

θ · EθRn(θ)

∈ O

min

θ2n, log log

= O(log log n).

We are currently unsure whether or not the dependence on log log 1
θ can be dropped from the bound
given in Theorem 13. Note that Theorem 3 cannot be applied when θ = 0, so Algorithm 1 suffers
logarithmic regret in this case. Algorithm 3 is carefully tuned and exploits the asymmetry in the
problem. It is possible that the result of Bubeck et. al. can be saved in spirit by using the symmetric
structured bandit depicted in Figure 4.(b). This would still only give a worst-case bound and does
not imply that ﬁnite problem-dependent regret is impossible.
Proof of Theorem 13. It is enough to consider only θ ∈ [0, 1], since the returns on the arms is
constant for θ ∈ [−1, 0]. We let L be the number phases (times that the outer loop is executed) and
Tℓ be the number of times the sub-optimal action is taken in the ℓth phase. Recall that ˆµk,ℓ,t denotes
the empirical estimate of µ based on t samples taken in the ℓth phase.

12

Step 1: Decomposing the regret

The regret is decomposed:

(θ = 0) :

E0Rn(0) = E0

Tℓ =

P0 {L ≥ ℓ} E0[Tℓ|L ≥ ℓ]

(θ > 0) :

EθRn(θ) = θEθ

Tℓ = θ

Pθ {L ≥ ℓ} Eθ[Tℓ|L ≥ ℓ]

L

∞

Xℓ=0
L

Xℓ=0

∞

Xℓ=0

Xℓ=0

Step 2: Bounding Eθ[Tℓ|L ≥ ℓ]

We need to consider the cases when θ = 0 and θ > 0 separately. If s ≥ 1, then

P0 {Tℓ ≥ n2,ℓ + s|L ≥ ℓ}

ˆµ2,ℓ,n2,ℓ+s−1 ≥ −

ˆµ2,ℓ,n2,ℓ+s−1 − µ2(0) ≥

(a)
≤ P0

(c)
≤ exp

(cid:26)

(cid:18)

1
2

−

(n2,ℓ + s − 1)

(b)
= P0

(d)
≤ exp

1
2

(cid:27)

(cid:19)

(cid:26)

−

s
2

,

(cid:16)

(cid:17)

where (a) follows since if the second action is chosen more than n2,ℓ times in the ℓth phase, then
that phase ends when ˆµ2,ℓ,t < − 1
2 , (b) by noting that µ2(0) = −1, (c) follows from the standard
concentration inequality and the fact that unit variance is assumed, (d) since n2,ℓ ≥ 1. Therefore by
Lemma 17 we have that E0[Tℓ|L ≥ ℓ] ≤ n2,ℓ + 2e1/2. Now assume θ > 0 and deﬁne

ω2(x) = min {z : y ≥ x log log y, ∀y ≥ z} ,

which satisﬁes ω2(x) ∈ O(x log log x). If n1,ℓ + s − 1 ≥ ω2

Pθ {Tℓ ≥ n1,ℓ + s|L ≥ ℓ}

ˆµ1,ℓ,n1,ℓ+s−1 ≥ −

log log(n1,ℓ + s − 1)

(a)
≤ Pθ

(cid:26)

ˆµ1,ℓ,n1,ℓ+s−1 − µ1(θ) ≥ θ −

4α
θ2

, then

(cid:0)

(cid:1)
α
nℓ,1 + s − 1

r
log log(n1,ℓ + s − 1)

α
nℓ,1 + s − 1
θ2
8

r
(d)
≤ exp

−

(cid:18)

(cid:9)

(cid:27)

(e)
≤ exp

(cid:19)

θ2
8

−

s

,

(cid:18)

(cid:19)

ˆµ1,ℓ,n1,ℓ+s−1 − µ1(θ) ≥ θ/2

(n1,ℓ + s − 1)

(b)
= Pθ

(c)
≤ Pθ

(cid:26)

(cid:8)

where (a) follows since if the ﬁrst arm (which is now sub-optimal) is chosen more than n1,ℓ times,
then the phase ends if ˆµ1,ℓ,t drops below the conﬁdence interval. (b) since µ1(θ) = −θ. (c) since
. (d) by the usual concentration inequality and (e) since n1,ℓ ≥ 1. Another
n1,ℓ + s − 1 ≥ ω2
application of Lemma 17 yields

4α
θ2

1
2

(cid:27)

(cid:27)

(cid:0)

(cid:1)

Eθ[Tℓ|L ≥ ℓ] ≤ max

n1,ℓ, ω2

4α
θ2

+

8eθ2/8
θ2

,

(cid:26)

(cid:18)

(cid:19)(cid:27)

where the max appears because we demanded that n1,ℓ + s − 1 ≥ ω2
each phase the ﬁrst action is taken at least n1,ℓ times before the phase can end.

and since at the start of

4α
θ2

(cid:0)

(cid:1)

Bounding the number of phases

Again we consider the cases when θ = 0 and θ ≥ 0 separately.

P0 {L > ℓ}

(a)
≤ P0

ˆµ2,ℓ,n2,ℓ ≥ −

∨ ∃s : ˆµ1,ℓ,n1,ℓ+s ≤ −

log log(n1,ℓ + s)

(cid:26)

(cid:26)

(b)
≤ P0

(c)
≤ exp

−

n2,ℓ
8

(cid:16)

(cid:17)

1
2

1
2

(cid:27)

(cid:26)

ˆµ2,ℓ,n2,ℓ ≥ −

+ P0

∃s : ˆµ1,ℓ,n1,ℓ+s ≤ −

log log(n1,ℓ + s)

+ P0

∃s : ˆµ1,ℓ,n1,ℓ+s ≤ −

log log(n1,ℓ + s)

,

α
n1,ℓ + s

r

r

α
n1,ℓ + s

r

α
n1,ℓ + s

(cid:27)

(cid:27)

(cid:27)

(12)

(cid:26)

13

where (a) is true since the ℓth phase will not end if ˆµ2,ℓ,n2,ℓ < −1/2 and if ˆµ1,ℓ,t never drops below
the conﬁdence interval. (b) follows from the union bound and (c) by the concentration inequality.
The second term is bounded using the maximal inequality and the peeling technique.

P0

∃s : ˆµ1,ℓ,n1,ℓ+s ≤ −

log log(n1,ℓ + s)

α
n1,ℓ + s

r

P0

∃t : 2kn1,ℓ ≤ t ≤ 2k+1n1,ℓ ∧ ˆµ1,ℓ,t ≤ −

log log t

P0

∃t ≤ 2k+1n1,ℓ : ˆµ1,ℓ,t ≤ −

(cid:27)

r

α
t

α

n1,ℓ2k log log 2kn1,ℓ

r
∞

(cid:27)

(cid:27)
α (e)
≤

(a)
≤

(b)
≤

(c)
≤

(cid:26)

∞

Xk=0
∞

Xk=0
∞

Xk=0

(cid:26)

(cid:26)

(cid:0)

exp

−α log log

2kn1,ℓ

(d)
=

(cid:0)

(cid:1)(cid:1)

Xk=0 (cid:18)

1
log 2k + log n1,ℓ (cid:19)

α−1

2
log 2

1
ℓ log 2

(cid:18)

(cid:19)

where (a) follows by the union bound, (b) by bounding t in the interval 2kn1,ℓ ≤ t ≤ 2k+1n1,ℓ. (c)
follows from the maximal inequality. (d) is trivial while (e) follows by approximating the sum by an
integral. By combining with (12) we obtain

P0 {L > ℓ} ≤ exp

−

n2,ℓ
8

+

2
log 2

1
ℓ log 2

(cid:18)

(cid:19)

(cid:16)

(cid:17)

α−1

= exp

−

+

ℓ
8

(cid:18)

(cid:19)

2
log 2

1
ℓ log 2

(cid:18)

(cid:19)

α−1

.

More straight-forwardly, if θ > 0, then

Pθ {L > ℓ} ≤ Pθ

∃s : ˆµ2,n2,ℓ+s < −

≤ 5 exp

−

1
2

(cid:27)

n2,ℓ
16

,

(cid:17)

(cid:16)

(cid:26)

where in the last inequality we used Lemma 16 and naive bounding.

Putting it together

We now combine the results of the previous components to obtain the required bound on the regret.
Recall that α = 5.

(θ = 0) :

E0Rn(0) = Eθ

Tℓ =

P0 {L ≥ ℓ} E0[Tℓ|L ≥ ℓ]

∞

∞

(θ > 0) :

Tℓ = θ

P {L ≥ ℓ} E[Tℓ|L ≥ ℓ]

Xℓ=2  
EθRn(θ) = θEθ

∞

Xℓ=2
∞

≤

=

Xℓ=2

∞

Xℓ=2  

∞

Xℓ=2
n2,ℓ
8

exp

−

exp

−

(cid:16)

(cid:18)

(cid:17)

+

ℓ
8

(cid:19)
∞

+

2
log 2

1
ℓ log 2

(cid:18)

2
log 2

1
ℓ log 2

(cid:18)

(cid:19)

α−1

!

(cid:16)

(cid:19)
α−1

n2,ℓ + 2e1/2

(cid:17)

ℓ2 + 2e1/2

∈ O(1)

!

(cid:16)

(cid:17)

Xℓ=1
n2,ℓ
16

 

(cid:17)

≤ 5θ

exp

−

max

n1,ℓ, ω2

Xℓ=2
θ · ω2

(cid:16)
α
θ2

(cid:16)

(cid:16)

(cid:17)(cid:17)

∈ O

= O

log log

1
θ

(cid:18)

1
θ

.

(cid:19)

4α
θ2

+

8eθ2/8
θ2

!

(cid:26)

(cid:18)

(cid:19)(cid:27)

B Technical Lemmas

Lemma 15. Deﬁne functions ω and ω2 by

ω(x) := min {z > 1 : y ≥ x log y, ∀y ≥ z}
ω2(x) := min {z > e : y ≥ x log log y, ∀y ≥ z} .

Then ω(x) ∈ O (x log x) and ω2(x) ∈ O (x log log x).

14

Lemma 16. Let {Xi}∞
sub-gaussian constant. Deﬁne ˆµt = 1
t

i=1 be sampled from some sub-gaussian distributed arm with mean µ and unit
t
s=1 Xs. Then for s ≥ 6/∆2 we have
1
log 2

P {∃t ≥ s : ˆµt − µ ≥ ∆} ≤ p +

1
1 − p

log

P

where p = exp

− s∆2
4
(cid:16)

.

(cid:17)

Proof. We assume without loss of generality that µ = 0 and use a peeling argument combined with
Azuma’s maximal inequality

P {∃t > s : ˆµt ≥ ∆}

∃k ∈ N, 2ks ≤ t < 2k+1s : ˆµt ≥ ∆

(a)
= P
(b)
= P
(c)
≤ P
∞
(cid:8)

(cid:8)

(d)
≤

Xk=0
∞

(e)
≤

(f )
≤

(h)
=

∃k ∈ N, 2ks ≤ t < 2k+1s : tˆµt ≥ t∆
(cid:8)
(cid:9)

(cid:9)
∃k ∈ N, 2ks ≤ t < 2k+1s : tˆµt ≥ 2ks∆

P

∃2ks ≤ t < 2k+1s : tˆµt ≥ 2ks∆/2

(cid:8)
∃t < 2k+1s : tˆµt ≥ 2ks∆/2

P

(cid:9)

(cid:9)

Xk=0
∞

(cid:8)

exp

−

(g)
=

2

2ks∆
2k+1s !
(cid:1)
2k

(i)
≤ p +

1
2 (cid:0)
s∆2
4

(cid:19)

 

(cid:18)

exp

−

Xk=0
∞

Xk=0

(cid:9)

∞

exp

−

(cid:18)

2ks∆2
4

(cid:19)

Xk=0
1
log 2

log

1
1 − p

where (a) follows by splitting the sum over an exponential grid. (b) by comparing cumulative dif-
ferences rather than the means. (c) since t > 2ks. (d) by the union bound over all k. (e) follows by
increasing the range. (f) by Azuma’s maximal inequality. (g) and (h) are true by straight-forward
arithmetic while (i) follows from Lemma 18.
Lemma 17. Suppose z is a positive random variable and for some α > 0 it holds for all natural
numbers k that P {z ≥ k} ≤ exp(−kα). Then Ez ≤ eα
α
Proof. Let δ ∈ (0, 1). Then

1
δ

P

z ≥ log

≤ P

z ≥

log

≤ exp

−α

log

(cid:26)

(cid:27)

(cid:22)

(cid:26)
≤ eα · exp

−α log

(cid:18)
(cid:22)
= eα · δα.

1
δ

(cid:23)(cid:27)

1
δ

(cid:23)(cid:19)

To complete the proof we use a standard identity to bound the expectation

Ez ≤

1

P

1
δ

0
Z

z ≥ log

dδ ≤ eα

δα−1dδ =

eα
α

.

Lemma 18. Let p ∈ (0, 7/10). Then

p2k

≤ p +

1
log 2

1
1 − p

.

Proof. Splitting the sum and comparing to an integral yields:

(cid:26)

∞

Xk=0

1
δ

(cid:18)

(cid:19)(cid:19)

1

0
Z

log

∞

Xk=0

p2k (a)

= p +

∞

p2k (b)

≤ p +

∞

p2k

dk

(c)
= p +

∞

pu/udu

1
log 2

1
Z

(d)
≤ p +

Xk=1
1
log 2

∞

u=1
X

pu/u

(e)
= p +

1
log 2

log

1
1 − p

where (a) follows by splitting the sum. (b) by noting that p2k
is monotone decreasing and comparing
to an integral. (c) by substituting u = 2k. (d) by reverting back to a sum. (e) follows from a standard
formula.

(cid:18)

1
δ

(cid:27)

0
Z

15

C Table of Notation

K
Θ
θ∗
It
Ti(n)
Xi,s
∆i
∆min
∆max
A
A′
Rn
Rn(θ)
µi(θ)
ˆµi,s
µ∗(θ)
i∗
i∗(θ)
ω(x)
ω2(x)
Ft

α
σ2

ui(n)

number of arms
parameter space
unknown parameter θ∗ ∈ Θ
arm played at time-step t
number times arm i has been played after time-step n
sth reward obtained when playing arm i
gap between the means of the best arm and the ith arm
minimum gap, ∆min := mini:∆i>0 ∆i
maximum gap, ∆max := maxi ∆i
set of arms A := {1, 2, · · · , K}
set of suboptimal arms A := {i : ∆i > 0}
regret at time-step n given unknown true parameter θ∗
regret at time-step n given parameter θ
mean of arm i given θ
empiric estimate of the mean of arm i after s plays
maximum return at θ. µ∗(θ) := maxi µi(θ)
optimal arm given θ∗
optimal arm given θ
minimum value y such that z ≥ x log z for all z ≥ y
minimum value y such that z ≥ x log log z for all z ≥ y
event that the true value of some mean is outside the conﬁdence interval about
the empiric estimate at time-step t
parameter controlling how exploring the algorithm UCB-S is
known parameter controlling the tails of the distributions governing the return
of the arms
critical number of samples for arm i. ui(n) :=

8σ2α log n
∆2
i

l

m

16

