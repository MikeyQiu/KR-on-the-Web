Dating Documents using Graph Convolution Networks

Shikhar Vashishth
IISc Bangalore
shikhar@iisc.ac.in

Shib Sankar Dasgupta
IISc Bangalore
shibd@iisc.ac.in

Swayambhu Nath Ray
IISc Bangalore
swayambhuray@iisc.ac.in

Partha Talukdar
IISc Bangalore
ppt@iisc.ac.in

9
1
0
2
 
b
e
F
 
1
 
 
]
L
C
.
s
c
[
 
 
1
v
5
7
1
0
0
.
2
0
9
1
:
v
i
X
r
a

Abstract

for many
Document date is essential
important
tasks, such as document re-
trieval, summarization, event detection,
etc. While existing approaches for these
tasks assume accurate knowledge of the
document date, this is not always avail-
able, especially for arbitrary documents
from the Web. Document Dating is a
challenging problem which requires infer-
ence over the temporal structure of the
document. Prior document dating sys-
tems have largely relied on handcrafted
features while ignoring such document-
internal structures. In this paper, we pro-
pose NeuralDater, a Graph Convolutional
Network (GCN) based document dating
approach which jointly exploits syntactic
and temporal graph structures of docu-
ment in a principled way. To the best of
our knowledge, this is the ﬁrst applica-
tion of deep learning for the problem of
document dating. Through extensive ex-
periments on real-world datasets, we ﬁnd
that NeuralDater signiﬁcantly outperforms
state-of-the-art baseline by 19% absolute
(45% relative) accuracy points.

1

Introduction

Date of a document, also referred to as the Doc-
ument Creation Time (DCT), is at the core of
many important tasks, such as, information re-
trieval (Olson et al., 1999; Li and Croft, 2003;
Dakka et al., 2008), temporal reasoning (Mani and
Wilson, 2000; Llid´o et al., 2001), text summariza-
tion (Wan, 2007), event detection (Allan et al.,
1998), and analysis of historical text (de Jong
et al., 2005a), among others. In all such tasks, the
document date is assumed to be available and also

Figure 1: Top: An example document annotated with syntac-
tic and temporal dependencies. In order to predict the right
value of 1999 for the Document Creation Time (DCT), infer-
ence over these document structures is necessary. Bottom:
Document date prediction by two state-of-the-art-baselines
and NeuralDater, the method proposed in this paper. While
the two previous methods are getting misled by the tempo-
ral expression (1995) in the document, NeuralDater is able to
use the syntactic and temporal structure of the document to
predict the right value (1999).

accurate – a strong assumption, especially for ar-
bitrary documents from the Web. Thus, there is
a need to automatically predict the date of a docu-
ment based on its content. This problem is referred
to as Document Dating.

Initial attempts on automatic document dating
started with generative models by (de Jong et al.,
2005b). This model is later improved by (Kan-
habua and Nørv˚ag, 2008a) who incorporate addi-
tional features such as POS tags, collocations, etc.
Chambers (2012) shows signiﬁcant improvement
over these prior efforts through their discrimina-
tive models using handcrafted temporal features.
Kotsakos et al. (2014) propose a statistical ap-
proach for document dating exploiting term bursti-

Figure 2: Overview of NeuralDater. NeuralDater exploits syntactic and temporal structure in a document to learn effective
representation, which in turn are used to predict the document time. NeuralDater uses a Bi-directional LSTM (Bi-LSTM), two
Graph Convolution Networks (GCN) – one over the dependency tree and the other over the document’s temporal graph – along
with a softmax classiﬁer, all trained end-to-end jointly. Please see Section 4 for more details.

ness (Lappas et al., 2009).

Document dating is a challenging problem
which requires extensive reasoning over the tem-
poral structure of the document. Let us moti-
vate this through an example shown in Figure 1.
In the document, four years after plays a crucial
role in identifying the creation time of the docu-
ment. The existing approaches give higher conﬁ-
dence for timestamp immediate to the year men-
tion 1995. NeuralDater exploits the syntactic and
temporal structure of the document to predict the
right timestamp (1999) for the document. With the
exception of (Chambers, 2012), all prior works on
the document dating problem ignore such infor-
mative temporal structure within the document.

Research in document event extraction and or-
dering have made it possible to extract such tem-
poral structures involving events,
temporal ex-
pressions, and the (unknown) document date in
a document (Mirza and Tonelli, 2016; Chambers
et al., 2014). While methods to perform reason-
ing over such structures exist (Verhagen et al.,
2007, 2010; UzZaman et al., 2013; Llorens et al.,
2015; Pustejovsky et al., 2003), none of them have
exploited advances in deep learning (Krizhevsky
et al., 2012; Hinton et al., 2012; Goodfellow et al.,
In particular, recently proposed Graph
2016).
Convolution Networks (GCN) (Defferrard et al.,
2016; Kipf and Welling, 2017) have emerged as a

way to learn graph representation while encoding
structural information and constraints represented
by the graph. We adapt GCNs for the document
dating problem and make the following contribu-
tions:

• We propose NeuralDater, a Graph Convolu-
tion Network (GCN)-based approach for doc-
ument dating. To the best of our knowledge,
this is the ﬁrst application of GCNs, and more
broadly deep neural network-based methods,
for the document dating problem.

• NeuralDater is the ﬁrst document dating ap-
proach which exploits syntactic as well tem-
poral structure of the document, all within a
principled joint model.

• Through extensive experiments on multiple
real-world datasets, we demonstrate Neu-
ralDater’s effectiveness over state-of-the-art
baselines.

NeuralDater’s source code and datasets used
in the paper are available at http://github.
com/malllabiisc/NeuralDater.

2 Related Work

Automatic Document Dating: de Jong et al.
(2005b) propose the ﬁrst approach for automat-
ing document dating through a statistical language

model. Kanhabua and Nørv˚ag (2008a) further ex-
tend this work by incorporating semantic-based
preprocessing and temporal entropy (Kanhabua
and Nørv˚ag, 2008b) based term-weighting. Cham-
bers (2012) proposes a MaxEnt based discrimina-
tive model trained on hand-crafted temporal fea-
tures. He also proposes a model to learn proba-
bilistic constraints between year mentions and the
actual creation time of the document. We draw
inspiration from his work for exploiting temporal
reasoning for document dating. Kotsakos et al.
(2014) propose a purely statistical method which
considers lexical similarity alongside burstiness
(Lappas et al., 2009) of terms for dating docu-
ments. To the best of our knowledge, NeuralDater,
our proposed method, is the ﬁrst method to utilize
deep learning techniques for the document dating
problem.

Event Ordering Systems: Temporal ordering
of events is a vast research topic in NLP. The
problem is posed as a temporal relation classiﬁ-
cation between two given temporal entities. Ma-
chine Learned classiﬁers and well crafted linguis-
tic features for this task are used in (Chambers
et al., 2007; Mirza and Tonelli, 2014). D’Souza
and Ng (2013) use a hybrid approach by adding
437 hand-crafted rules. Chambers and Jurafsky
(2008); Yoshikawa et al. (2009) try to classify with
many more temporal constraints, while utilizing
integer linear programming and Markov logic.

CAEVO, a CAscading EVent Ordering archi-
tecture (Chambers et al., 2014) use sieve-based ar-
chitecture (Lee et al., 2013) for temporal event or-
dering for the ﬁrst time. They mix multiple learn-
ers according to their precision based ranks and
use transitive closure for maintaining consistency
of temporal graph. Mirza and Tonelli (2016) re-
cently propose CATENA (CAusal and TEmporal
relation extraction from NAtural language texts),
the ﬁrst integrated system for the temporal and
causal relations extraction between pre-annotated
events and time expressions. They also incorpo-
rate sieve-based architecture which outperforms
existing methods in temporal relation classiﬁca-
tion domain. We make use of CATENA for tem-
poral graph construction in our work.

Graph Convolutional Networks

(GCN):
GCNs generalize Convolutional Neural Network
(CNN) over graphs. GCN is introduced by (Bruna
et al., 2014), and later extended by (Defferrard
et al., 2016) with efﬁcient localized ﬁlter approx-

imation in spectral domain. Kipf and Welling
(2017) propose a ﬁrst-order approximation of
localized ﬁlters through layer-wise propagation
rule. GCNs over syntactic dependency trees
have been recently exploited in the ﬁeld of
semantic-role labeling (Marcheggiani and Titov,
2017), neural machine translation (Bastings et al.,
2017a), event detection (Bastings et al., 2017b),
relation extraction (Vashishth et al., 2018). In our
work, we successfully use GCNs for document
dating.

3 Background: Graph Convolution

Networks (GCN)

In this section, we provide an overview of Graph
Convolution Networks (GCN) (Kipf and Welling,
2017). GCN learns an embedding for each node of
the graph it is applied over. We ﬁrst present GCN
for undirected graphs and then move onto GCN
for directed graph setting.

3.1 GCN on Undirected Graph

Let G = (V, E) be an undirected graph, where V
is a set of n vertices and E the set of edges. The
input feature matrix X ∈ Rn×m whose rows are
input representation of node u, xu ∈ Rm, ∀u ∈ V.
The output hidden representation hv ∈ Rd of a
node v after a single layer of graph convolution
operation can be obtained by considering only the
immediate neighbors of v. This can be formulated
as:

hv = f

(W xu + b)

 , ∀v ∈ V.







(cid:88)

u∈N (v)

Here, model parameters W ∈ Rd×m and b ∈ Rd
are learned in a task-speciﬁc setting using ﬁrst-
order gradient optimization. N (v) refers to the set
of neighbors of v and f is any non-linear activa-
tion function. We have used ReLU as the activa-
tion function in this paper1.

In order to capture nodes many hops away, mul-
tiple GCN layers may be stacked one on top of an-
other. In particular, hk+1
, representation of node
v
v after kth GCN layer can be formulated as:

hk+1
v = f

(cid:88)

(cid:16)

W khk

u + bk(cid:17)

 , ∀v ∈ V.







u∈N (v)

where hk

u is the input to the kth layer.

1ReLU: f (x) = max(0, x)

3.2 GCN on Labeled and Directed Graph

In this section, we consider GCN formulation over
graphs where each edge is labeled as well as di-
In this setting, an edge from node u to
rected.
v with label l(u, v) is denoted as (u, v, l(u, v)).
While a few recent works focus on GCN over di-
rected graphs (Yasunaga et al., 2017; Marcheg-
giani and Titov, 2017), none of them consider la-
beled edges. We handle both direction and label by
incorporating label and direction speciﬁc ﬁlters.

Based on the assumption that the information
in a directed edge need not only propagate along
its direction, following (Marcheggiani and Titov,
2017) we deﬁne an updated edge set E (cid:48) which ex-
pands the original set E by incorporating inverse,
as well self-loop edges.

E (cid:48) = E ∪ {(v, u, l(u, v)−1) | (u, v, l(u, v)) ∈ E}
(1)

∪ {(u, u, (cid:62)) | u ∈ V)}.

Here, l(u, v)−1 is the inverse edge label corre-
sponding to label l(u, v), and (cid:62) is a special empty
relation symbol for self-loop edges. We now de-
as the embedding of node v after kth
ﬁne hk+1
GCN layer applied over the directed and labeled
graph as:

v

hk+1
v = f

W k

l(u,v)hk

u + bk

l(u,v)





(cid:88)

(cid:16)

u∈N (v)

(cid:17)



 .

(2)

We note that the parameters W k
in this case are edge label speciﬁc.

l(u,v) and bk

l(u,v)

3.3

Incorporating Edge Importance

In many practical settings, we may not want to
give equal importance to all the edges. For exam-
ple, in case of automatically constructed graphs,
some of the edges may be erroneous and we may
want to automatically learn to discard them. Edge-
wise gating may be used in a GCN to give im-
portance to relevant edges and subdue the noisy
ones. Bastings et al. (2017b); Marcheggiani and
Titov (2017) used gating for similar reasons and
obtained high performance gain. At kth layer,
we compute gating value for a particular edge
(u, v, l(u, v)) as:

(cid:16)

gk
u,v = σ

u · ˆwk
hk

l(u,v) + ˆbk

l(u,v)

(cid:17)

,

where, σ(·) is the sigmoid function, ˆwk
l(u,v) and
ˆbk
l(u,v) are label speciﬁc gating parameters. Thus,

gating helps to make the model robust to the noisy
labels and directions of the input graphs. GCN
embedding of a node while incorporating edge
gating may be computed as follows.

hk+1
v = f

(cid:16)

gk
u,v ×

W k

l(u,v)hk

u + bk

l(u,v)

(cid:17)



 .





(cid:88)

u∈N (v)

4 NeuralDater Overview

The Documents Dating problem may be cast as a
multi-class classiﬁcation problem (Kotsakos et al.,
In this section, we
2014; Chambers, 2012).
present an overview of NeuralDater, the document
dating system proposed in this paper. Architec-
tural overview of NeuralDater is shown in Figure
2.

NeuralDater is a deep learning-based multi-
class classiﬁcation system. It takes in a document
as input and returns its predicted date as output by
exploiting the syntactic and temporal structure of
document.

NeuralDater network consists of three layers
which learn an embedding for the Document Cre-
ation Time (DCT) node corresponding to the doc-
ument. This embedding is then fed to a soft-
max classiﬁer which produces a distribution over
timestamps. Following prior research (Chambers,
2012; Kotsakos et al., 2014), we work with year
granularity for the experiments in this paper. We,
however, note that NeuralDater can be trained for
ﬁner granularity with appropriate training data.
The NeuralDater network is trained end-to-end us-
ing training data. We brieﬂy present NeuralDater’s
various components below. Each component is de-
scribed in greater detail in subsequent sections.

• Context Embedding:

In this layer, Neu-
ralDater uses a Bi-directional LSTM (Bi-
LSTM) to learn embedding for each token in
the document. Bi-LSTMs have been shown
to be quite effective in capturing local context
inside token embeddings (Sutskever et al.,
2014).

• Syntactic Embedding: In this step, Neural-
Dater revises token embeddings from the pre-
vious step by running a GCN over the depen-
dency parses of sentences in the document.
We refer to this GCN as Syntactic GCN or
S-GCN. While the Bi-LSTM captures imme-
diate local context in token embeddings, S-

GCN augments them by capturing syntactic
context.

• Temporal Embedding:

In this step, Neu-
ralDater further reﬁnes embeddings learned
by S-GCN to incorporate cues from temporal
structure of event and times in the document.
NeuralDater uses state-of-the-art causal and
temporal relation extraction algorithm (Mirza
and Tonelli, 2016) for extracting temporal
graph for each document. A GCN is then run
over this temporal graph to reﬁne the embed-
dings from the previous layer. We refer to this
GCN as Temporal GCN or T-GCN. In this
step, a special DCT node is introduced whose
embedding is also learned by the T-GCN.

• Classiﬁer: Embedding of the DCT node
along with average pooled embeddings
learned by S-GCN are fed to a fully con-
nected softmax classiﬁer which makes the ﬁ-
nal prediction about the date of the document.

Even though the previous discussion is pre-
sented in a sequential manner, the whole network
is trained in a joint end-to-end manner using back-
propagation.

5 NeuralDater Details

In this section, we present detailed description of
various components of NeuralDater.

5.1 Context Embedding (Bi-LSTM)

Let us consider a document D with n tokens
w1, w2, ..., wn. We ﬁrst represent each token by
a k-dimensional word embedding. For the exper-
iments in this paper, we use GloVe (Pennington
et al., 2014) embeddings. These token embed-
dings are stacked together to get the document
representation X ∈ Rn×k. We then employ a
Bi-directional LSTM (Bi-LSTM) (Hochreiter and
Schmidhuber, 1997) on the input matrix X to ob-
tain contextual embedding for each token. After
stacking contextual embedding of all these tokens,
we get the new document representation matrix
Hcntx ∈ Rn×rcntx.
In this new representation,
each token is represented in a rcntx-dimensional
space. Our choice of LSTMs for learning con-
textual embeddings for tokens is motivated by the
previous success of LSTMs in this task (Sutskever
et al., 2014).

5.2 Syntactic Embedding (S-GCN)

While the Bi-LSTM is effective at capturing im-
mediate local context of a token, it may not be as
effective in capturing longer range dependencies
among words in a sentence. For example, in Fig-
ure 1, we would like the embedding of token ap-
proved to be directly affected by govt, even though
they are not immediate neighbors. A dependency
parse may be used to capture such longer-range
connections.
In fact, similar features were ex-
ploited by (Chambers, 2012) for the document dat-
ing problem. NeuralDater captures such longer-
range information by using another GCN run over
the syntactic structure of the document. We de-
scribe this in detail below.

The context embedding, Hcntx ∈ Rn×rcntx
learned in the previous step is used as input to
this layer. For a given document, we ﬁrst extract
its syntactic dependency structure by applying the
Stanford CoreNLP’s dependency parser (Manning
et al., 2014) on each sentence in the document in-
dividually. We now employ the Graph Convolu-
tion Network (GCN) over this dependency graph
using the GCN formulation presented in Section
3.2. We call this GCN the Syntactic GCN or S-
GCN, as mentioned in Section 4.

Since S-GCN operates over the dependency
graph and uses Equation 2 for updating embed-
dings, the number of parameters in S-GCN is di-
rectly proportional to the number of dependency
Stanford CoreNLP’s dependency
edge types.
parser returns 55 different dependency edge types.
This large number of edge types is going to sig-
niﬁcantly over-parameterize S-GCN, thereby in-
creasing the possibility of overﬁtting. In order to
address this, we use only three edge types in S-
GCN. For each edge connecting nodes wi and wj
in E (cid:48) (see Equation 1), we determine its new type
L(wi, wj) as follows:

• L(wi, wj) =→ if (wi, wj, l(wi, wj)) ∈ E (cid:48),
i.e., if the edge is an original dependency
parse edge

• L(wi, wj) =← if (wi, wj, l(wi, wj)−1) ∈ E (cid:48),

i.e., if the edges is an inverse edge

• L(wi, wj) = (cid:62) if (wi, wj, (cid:62)) ∈ E (cid:48), i.e., if

the edge is a self-loop with wi = wj

S-GCN now estimates embedding hsyn
wi ∈ Rrsyn
for each token wi in the document using the for-

mulation shown below.

(cid:32)

(cid:80)

hsyn
wi = f

(cid:16)

wj ∈N (wi)

WL(wi,wj )hcntx

wj + bL(wi,wj )

(cid:33)

(cid:17)

Datasets

# Docs Start Year End Year

APW
NYT

675k
647k

1995
1987

2010
1996

Please note S-GCN’s use of the new edge types
L(wi, wj) above, instead of the l(wi, wj) types
used in Equation 2. By stacking embeddings for
all the tokens together, we get the new embedding
matrix Hsyn ∈ Rn×rsyn representing the docu-
ment.

AveragePooling: We obtain an embedding havg
D
for the whole document by average pooling of ev-
ery token representation.

havg
D =

1
n

n
(cid:88)

i=1

hsyn
wi .

(3)

5.3 Temporal Embedding (T-GCN)

In this layer, NeuralDater exploits temporal struc-
ture of the document to learn an embedding for the
Document Creation Time (DCT) node of the doc-
ument. First, we describe the construction of tem-
poral graph, followed by GCN-based embedding
learning over this graph.

Temporal Graph Construction: NeuralDater
uses Stanford’s SUTime tagger (Chang and Man-
ning, 2012) for date normalization and the event
extraction classiﬁer of (Chambers et al., 2014) for
event detection. The annotated document is then
passed to CATENA (Mirza and Tonelli, 2016),
current state-of-the-art temporal and causal rela-
tion extraction algorithm,
to obtain a temporal
graph for each document. Since our task is to pre-
dict the creation time of a given document, we
supply DCT as unknown to CATENA. We hy-
pothesize that the temporal relations extracted in
absence of DCT are helpful for document dating
and we indeed ﬁnd this to be true, as shown in
Section 7. Temporal graph is a directed graph,
where nodes correspond to events, time mentions,
and the Document Creation Time (DCT). Edges in
this graph represent causal and temporal relation-
ships between them. Each edge is attributed with
a label representing the type of the temporal rela-
tion. CATENA outputs 9 different types of tempo-
ral relations, out of which we selected ﬁve types,
viz., AFTER, BEFORE, SAME, INCLUDES, and
IS INCLUDED. The remaining four types were
ignored as they were substantially infrequent.

Please note that the temporal graph may involve
only a small number of tokens in the document.

Table 1: Details of datasets used. Please see Section 6 for
details.

For example, in the temporal graph in Figure 2,
there are a total of 5 nodes: two temporal expres-
sion nodes (1995 and four years after), two event
nodes (adopted and approved), and a special DCT
node. This graph also consists of temporal rela-
tion edges such as (four years after, approved, BE-
FORE).

Temporal Graph Convolution: NeuralDater
employs a GCN over the temporal graph con-
structed above. We refer to this GCN as the Tem-
poral GCN or T-GCN, as mentioned in Section
4. T-GCN is based on the GCN formulation pre-
sented in Section 3.2. Unlike S-GCN, here we
consider label and direction speciﬁc parameters as
the temporal graph consists of only ﬁve types of
edges.

Let nT be the number of nodes in the temporal
graph. Starting with Hsyn (Section 5.2), T-GCN
learns a rtemp-dimensional embedding for each
node in the temporal graph. Stacking all these em-
beddings together, we get the embedding matrix
Htemp ∈ RnT ×rtemp. T-GCN embeds the tempo-
ral constraints induced by the temporal graph in
htemp
DCT ∈ Rrtemp, embedding of the DCT node of
the document.

5.4 Classiﬁer
Finally, the DCT embedding htemp
DCT and average-
pooled syntactic representation havg
D (see Equation
3) of document D are concatenated and fed to a
fully connected feed forward network followed by
a softmax. This allows the NeuralDater to exploit
context, syntactic, and temporal structure of the
document to predict the ﬁnal document date y.

havg+temp
D

= [htemp

DCT ; havg
D ]

p(y|D) = Softmax(W · havg+temp

+ b).

D

6 Experimental Setup

Datasets: We experiment on Associated Press
Worldstream (APW) and New York Times (NYT)
sections of Gigaword corpus (Parker et al., 2011).
The original dataset contains around 3 million

documents of APW and 2 million documents of
NYT from span of multiple years. From both
sections, we randomly sample around 650k doc-
uments while maintaining balance among years.
Documents belonging to years with substantially
fewer documents are omitted. Details of the
dataset can be found in Table 1. For train, test and
validation splits, the dataset was randomly divided
in 80:10:10 ratio.

Evaluation Criteria: Given a document, the
model needs to predict the year in which the docu-
ment was published. We measure performance in
terms of overall accuracy of the model.

Baselines: For evaluating NeuralDater, we

compared against the following methods:

• BurstySimDater Kotsakos et al.

(2014):
This is a purely statistical method which uses
lexical similarity and term burstiness (Lappas
et al., 2009) for dating documents in arbitrary
length time frame. For our experiments, we
took the time frame length as 1 year. Please
refer to (Kotsakos et al., 2014) for more de-
tails.

based

• MaxEnt-Time-NER: Maximum Entropy
(MaxEnt)
on
hand-crafted temporal and Named Entity
Recognizer (NER) based features. More
details in (Chambers, 2012).

classiﬁer

trained

• MaxEnt-Joint: Refers to MaxEnt-Time-
NER combined with year mention classiﬁer
as described in (Chambers, 2012).

• MaxEnt-Uni-Time: MaxEnt based discrim-
inative model which takes bag-of-words rep-
resentation of input document with normal-
ized time expression as its features.

• CNN: A Convolution Neural Network
(CNN)
(LeCun et al., 1999) based text
classiﬁcation model proposed by (Kim,
2014), which attained state-of-the-art results
in several domains.

• NeuralDater: Our proposed method, refer

Section 4.

Method

APW NYT

BurstySimDater
45.9
MaxEnt-Time+NER 52.5
52.5
MaxEnt-Joint
57.5
MaxEnt-Uni-Time
56.3
CNN
64.1
NeuralDater

38.5
42.3
42.5
50.5
50.4
58.9

Table 2: Accuracies of different methods on APW and NYT
datasets for the document dating problem (higher is better).
NeuralDater signiﬁcantly outperforms all other competitive
baselines. This is our main result. Please see Section 7.1 for
more details.

Figure 3: Mean absolute deviation (in years; lower is bet-
ter) between a model’s top prediction and the true year in
the APW dataset. We ﬁnd that NeuralDater, the proposed
method, achieves the least deviation. Please see Section 7.1
for details.

Method

Accuracy

T-GCN
S-GCN + T-GCN (K = 1)
S-GCN + T-GCN (K = 2)
S-GCN + T-GCN (K = 3)

Bi-LSTM
Bi-LSTM + CNN
Bi-LSTM + T-GCN
Bi-LSTM + S-GCN + T-GCN (no gate)
Bi-LSTM + S-GCN + T-GCN (K = 1)
Bi-LSTM + S-GCN + T-GCN (K = 2)
Bi-LSTM + S-GCN + T-GCN (K = 3)

57.3
57.8
58.8
59.1

58.6
59.0
60.5
62.7
64.1
63.8
63.3

Table 3: Accuracies of different ablated methods on the APW
dataset. Overall, we observe that incorporation of context
(Bi-LSTM), syntactic structure (S-GCN) and temporal struc-
ture (T-GCN) in NeuralDater achieves the best performance.
Please see Section 7.1 for details.

GCNs and BiLSTM with 0.8 dropout. We used
Adam (Kingma and Ba, 2014) with 0.001 learn-
ing rate for training.

Hyperparameters: By default, edge gating
(Section 3.3) is used in all GCNs. The parameter
K represents the number of layers in T-GCN (Sec-
tion 5.3). We use 300-dimensional GloVe embed-
dings and 128-dimensional hidden state for both

7 Results

7.1 Performance Comparison

In order to evaluate the effectiveness of Neu-
ralDater, our proposed method, we compare it

against existing document dating systems and text
classiﬁcation models. The ﬁnal results are sum-
marized in Table 2. Overall, we ﬁnd that Neu-
ralDater outperforms all other methods with a
signiﬁcant margin on both datasets. Compared
to the previous state-of-the-art in document dat-
ing, BurstySimDater (Kotsakos et al., 2014), we
get 19% average absolute improvement in accu-
racy across both datasets. We observe only a
slight gain in the performance of MaxEnt-based
model (MaxEnt-Time+NER) of (Chambers, 2012)
on combining with temporal constraint reasoner
(MaxEnt-Joint). This may be attributed to the
fact that the model utilizes only year mentions in
the document, thus ignoring other relevant signals
which might be relevant to the task. BurstySim-
Dater performs considerably better in terms of pre-
cision compared to the other baselines, although
it signiﬁcantly underperforms in accuracy. We
note that NeuralDater outperforms all these prior
models both in terms of precision and accuracy.
We ﬁnd that even generic deep-learning based text
classiﬁcation models, such as CNN (Kim, 2014),
are quite effective for the problem. However,
since such a model doesn’t give speciﬁc attention
to temporal features in the document, its perfor-
mance remains limited. From Figure 3, we ob-
serve that NeuralDater’s top prediction achieves
on average the lowest deviation from the true year.

7.2 Ablation Comparisons

For demonstrating the efﬁcacy of GCNs and BiL-
STM for the problem, we evaluate different ab-
lated variants of NeuralDater on the APW dataset.
Speciﬁcally, we validate the importance of us-
ing syntactic and temporal GCNs and the effect
of eliminating BiLSTM from the model. Over-
all results are summarized in Table 3. The ﬁrst
block of rows in the table corresponds to the case
when BiLSTM layer is excluded from Neural-
Dater, while the second block denotes the case
when BiLSTM is included. We also experiment
with multiple stacked layers of T-GCN (denoted
by K) to observe its effect on the performance of
the model.

We observe that embeddings from Syntactic
GCN (S-GCN) are much better than plain GloVe
embeddings for T-GCN as S-GCN encodes the
syntactic neighborhood information in event and
time embeddings which makes them more relevant
for document dating task.

Figure 4: Evaluating performance of different methods on
dating documents with and without time mentions. Please
see Section 7.3 for details.

Overall, we observe that including BiLSTM
in the model improves performance signiﬁcantly.
Single BiLSTM model outperforms all the mod-
els listed in the ﬁrst block of Table 3. Also, some
gain in performance is observed on increasing the
number of T-GCN layers (K) in absence of BiL-
STM, although the same does not follow when
BiLSTM is included in the model. This observa-
tion is consistent with (Marcheggiani and Titov,
2017), as multiple GCN layers become redundant
in the presence of BiLSTM. We also ﬁnd that elim-
inating edge gating from our best model deterio-
rates its overall performance.

In summary, these results validate our thesis
that joint incorporation of syntactic and temporal
structure of a document in NeuralDater results in
improved performance.

7.3 Discussion and Error Analysis

In this section, we list some of our observations
while trying to identify pros and cons of Neural-
Dater, our proposed method. We divided the de-
velopment split of the APW dataset into two sets
– those with and without any mention of time ex-
pressions (year). We apply NeuralDater and other
methods to these two sets of documents and re-
port accuracies in Figure 4. We ﬁnd that overall,
NeuralDater performs better in comparison to the
existing baselines in both scenarios. Even though
the performance of NeuralDater degrades in the
absence of time mentions, its performance is still
the best relatively. Based on other analysis, we
ﬁnd that NeuralDater fails to identify timestamp
of documents reporting local infrequent incidents
without explicit time mention. NeuralDater be-
comes confused in the presence of multiple mis-
leading time mentions; it also loses out on docu-
ments discussing events which are outside the time
range of the text on which the model was trained.
In future, we plan to eliminate these pitfalls by

incorporating additional signals from Knowledge
Graphs about entities mentioned in the document.
We also plan to utilize free text temporal expres-
sion (Kuzey et al., 2016) in documents for improv-
ing performance on this problem.

8 Conclusion

We propose NeuralDater, a Graph Convolutional
Network (GCN) based method for document dat-
ing which exploits syntactic and temporal struc-
tures in the document in a principled way. To the
best of our knowledge, this is the ﬁrst applica-
tion of deep learning techniques for the problem of
document dating. Through extensive experiments
on real-world datasets, we demonstrate the effec-
tiveness of NeuralDater over existing state-of-the-
art approaches. We are hopeful that the representa-
tion learning techniques explored in this paper will
inspire further development and adoption of such
techniques in the temporal information processing
research community.

Acknowledgements

We thank the anonymous reviewers for their con-
structive comments. This work is supported in part
by the Ministry of Human Resource Development
(Government of India) and by a gift from Google.

References

In Proceedings of

James Allan, Ron Papka, and Victor Lavrenko.
1998. On-line new event detection and track-
In-
ing.
ternational ACM SIGIR Conference on Research
and Development in Information Retrieval. ACM,
New York, NY, USA, SIGIR ’98, pages 37–45.
https://doi.org/10.1145/290941.290954.

the 21st Annual

Joost Bastings,

Ivan Titov, Wilker Aziz, Diego
Marcheggiani, and Khalil Simaan. 2017a. Graph
convolutional encoders for syntax-aware neural ma-
chine translation. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing. Association for Computational Lin-
guistics, Copenhagen, Denmark, pages 1957–1967.
https://www.aclweb.org/anthology/D17-1209.

national Conference on Learning Representations
(ICLR2014), CBLS, April 2014.

Nathanael Chambers. 2012.

In Proceedings of

Learning from their

Labeling documents
time
with timestamps:
the 50th An-
expressions.
the Association for Compu-
nual Meeting of
tational Linguistics:
- Volume
1. Association for Computational Linguistics,
Stroudsburg, PA, USA, ACL ’12, pages 98–106.
http://dl.acm.org/citation.cfm?id=2390524.2390539.

Long Papers

Nathanael Chambers, Taylor Cassidy, Bill McDow-
ell, and Steven Bethard. 2014. Dense event order-
ing with a multi-pass architecture. Transactions of
the Association of Computational Linguistics 2:273–
284. http://www.aclweb.org/anthology/Q14-1022.

Nathanael Chambers and Dan Jurafsky. 2008. Jointly
combining implicit constraints improves temporal
ordering. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Association for Computational Linguistics, Strouds-
burg, PA, USA, EMNLP ’08, pages 698–706.
http://dl.acm.org/citation.cfm?id=1613715.1613803.

Classifying temporal
In Proceedings of

Nathanael Chambers, Shan Wang, and Dan Ju-
rafsky. 2007.
relations
the 45th
between events.
the ACL on Interactive
Annual Meeting of
Poster and Demonstration Sessions. Associa-
Strouds-
tion for Computational Linguistics,
burg, PA, USA, ACL ’07,
pages 173–176.
http://dl.acm.org/citation.cfm?id=1557769.1557820.

Angel X. Chang and Christopher Manning. 2012.
recognizing and nor-
Sutime: A library for
In Proceedings of
malizing time expressions.
the Eighth International Conference on Language
Resources and Evaluation (LREC-2012). Euro-
pean Language Resources Association (ELRA).
http://www.aclweb.org/anthology/L12-1122.

Wisam Dakka, Luis Gravano, and Panagiotis G. Ipeiro-
tis. 2008. Answering general time sensitive queries.
In Proceedings of the 17th ACM Conference on
Information and Knowledge Management. ACM,
New York, NY, USA, CIKM ’08, pages 1437–1438.
https://doi.org/10.1145/1458082.1458320.

Franciska M.G. de Jong, H. Rode, and Djoerd Hiem-
Temporal Language Models for
stra. 2005a.
the Disclosure of Historical Text, KNAW, pages
161–168.
Imported from EWI/DB PMS [db-
utwente:inpr:0000003683].

Joost Bastings,

Ivan Titov, Wilker Aziz, Diego
Marcheggiani, and Khalil Sima’an. 2017b. Graph
convolutional encoders for syntax-aware neural
CoRR abs/1704.04675.
machine translation.
http://arxiv.org/abs/1704.04675.

Franciska M.G. de Jong, H. Rode, and Djoerd Hiem-
Temporal Language Models for
stra. 2005b.
the Disclosure of Historical Text, KNAW, pages
161–168.
Imported from EWI/DB PMS [db-
utwente:inpr:0000003683].

Joan Bruna, Wojciech Zaremba, Arthur Szlam, and
Spectral networks and lo-
In Inter-

Yann Lecun. 2014.
cally connected networks on graphs.

Micha¨el Defferrard, Xavier Bresson, and Pierre Van-
dergheynst. 2016. Convolutional neural networks
In
on graphs with fast localized spectral ﬁltering.

Proceedings of the 30th International Conference
on Neural Information Processing Systems. Curran
Associates Inc., USA, NIPS’16, pages 3844–3852.
http://dl.acm.org/citation.cfm?id=3157382.3157527.

SIGIR Conference on Research &#38; Devel-
in Information Retrieval. ACM, New
opment
York, NY, USA, SIGIR ’14, pages 1003–1006.
https://doi.org/10.1145/2600428.2609495.

Jennifer D’Souza and Vincent Ng. 2013. Classifying
temporal relations with rich linguistic knowledge.
In Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
Association for Computational Linguistics, pages
918–927.
http://www.aclweb.org/anthology/N13-
1112.

Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
2016. Deep Learning. MIT Press. http://www.
deeplearningbook.org.

G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. r. Mohamed,
N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N.
Sainath, and B. Kingsbury. 2012. Deep neural
networks for acoustic modeling in speech recog-
nition: The shared views of four research groups.
IEEE Signal Processing Magazine 29(6):82–97.
https://doi.org/10.1109/MSP.2012.2205597.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long
short-term memory. Neural Comput. 9(8):1735–
1780. https://doi.org/10.1162/neco.1997.9.8.1735.

Nattiya Kanhabua and Kjetil Nørv˚ag. 2008a. Improv-
ing temporal language models for determining time
of non-timestamped documents. In Proceedings of
the 12th European Conference on Research and Ad-
vanced Technology for Digital Libraries. Springer-
Verlag, Berlin, Heidelberg, ECDL ’08, pages 358–
370.

Nattiya Kanhabua and Kjetil Nørv˚ag. 2008b. Improv-
ing temporal language models for determining time
In International
of non-timestamped documents.
Conference on Theory and Practice of Digital Li-
braries. Springer, pages 358–370.

Yoon Kim. 2014.

Convolutional neural networks
In Proceedings of the
for sentence classiﬁcation.
2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP). Association
for Computational Linguistics, pages 1746–1751.
https://doi.org/10.3115/v1/D14-1181.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
CoRR

A method for stochastic optimization.
abs/1412.6980.

Thomas N. Kipf and Max Welling. 2017.

Semi-
supervised classiﬁcation with graph convolutional
networks. In International Conference on Learning
Representations (ICLR).

Dimitrios Kotsakos, Theodoros Lappas, Dimitrios
Kotzias, Dimitrios Gunopulos, Nattiya Kan-
habua, and Kjetil Nørv˚ag. 2014. A burstiness-
aware
In
approach for document dating.
the 37th International ACM
Proceedings of

Ilya Sutskever, and Geoffrey E.
Alex Krizhevsky,
Imagenet classiﬁcation with deep
Hinton. 2012.
In Proceedings
convolutional neural networks.
of
the 25th International Conference on Neural
Information Processing Systems - Volume 1. Curran
Associates Inc., USA, NIPS’12, pages 1097–1105.
http://dl.acm.org/citation.cfm?id=2999134.2999257.

Jannik Str¨otgen, and
Erdal Kuzey, Vinay Setty,
As time goes by:
Gerhard Weikum. 2016.
Comprehensive tagging of textual phrases with
the 25th
temporal scopes.
International Conference on World Wide Web.
International World Wide Web Conferences
Steering Committee, Republic and Canton of
Geneva, Switzerland, WWW ’16, pages 915–925.
https://doi.org/10.1145/2872427.2883055.

In Proceedings of

Theodoros Lappas, Benjamin Arai, Manolis Platakis,
Dimitrios Kotsakos, and Dimitrios Gunopulos.
On burstiness-aware search for docu-
2009.
the 15th
In Proceedings of
ment sequences.
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining. ACM, New
York, NY, USA, KDD ’09, pages 477–486.
https://doi.org/10.1145/1557019.1557075.

Yann LeCun,

Patrick Haffner,

L´eon Bottou,
recogni-
and Yoshua Bengio. 1999.
In Shape,
tion with gradient-based learning.
Contour and Grouping in Computer Vision.
Springer-Verlag, London, UK, UK, pages 319–.
http://dl.acm.org/citation.cfm?id=646469.691875.

Object

Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan
Jurafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Comput. Linguist. 39(4):885–916.

Xiaoyan Li and W. Bruce Croft. 2003.

Time-
In Proceedings of
based language models.
the Twelfth International Conference on Informa-
tion and Knowledge Management. ACM, New
York, NY, USA, CIKM ’03, pages 469–475.
https://doi.org/10.1145/956863.956951.

D. Llid´o, R. Berlanga, and M. J. Aramburu. 2001.
Extracting temporal references to assign document
event-time periods*.
In Heinrich C. Mayr, Jiri
Lazansky, Gerald Quirchmayr, and Pavel Vogel,
editors, Database and Expert Systems Applica-
tions. Springer Berlin Heidelberg, Berlin, Heidel-
berg, pages 62–71.

Hector Llorens, Nathanael Chambers, Naushad Uz-
Zaman, Nasrin Mostafazadeh, James Allen, and
James Pustejovsky. 2015. Semeval-2015 task 5:

Qa tempeval-evaluating temporal information un-
In Proceed-
derstanding with question answering.
ings of the 9th International Workshop on Semantic
Evaluation (SemEval 2015). pages 792–800.

Inderjeet Mani and George Wilson. 2000. Robust tem-
poral processing of news. In Proceedings of the 38th
Annual Meeting on Association for Computational
Linguistics. Association for Computational Linguis-
tics, Stroudsburg, PA, USA, ACL ’00, pages 69–76.
https://doi.org/10.3115/1075218.1075228.

Jenny

Steven

Christopher D. Manning, Mihai Surdeanu,
Finkel,

John
Bauer,
J. Bethard,
and David McClosky. 2014.
The Stanford
language processing toolkit.
CoreNLP natural
In Association for Computational Linguistics
(ACL) System Demonstrations. pages 55–60.
http://www.aclweb.org/anthology/P/P14/P14-5010.

Diego Marcheggiani and Ivan Titov. 2017. Encod-
ing sentences with graph convolutional networks
for semantic role labeling. CoRR abs/1703.04826.
http://arxiv.org/abs/1703.04826.

Paramita Mirza and Sara Tonelli. 2014. Classifying
temporal relations with simple features. In Proceed-
ings of the 14th Conference of the European Chap-
ter of the Association for Computational Linguistics.
Association for Computational Linguistics, pages
308–317. https://doi.org/10.3115/v1/E14-1033.

Paramita Mirza and Sara Tonelli. 2016.

Catena:
Causal and temporal relation extraction from nat-
In Proceedings of COLING
ural language texts.
2016, the 26th International Conference on Compu-
tational Linguistics: Technical Papers. The COL-
ING 2016 Organizing Committee, pages 64–75.
http://www.aclweb.org/anthology/C16-1007.

MA Olson, K Bostic, MI Seltzer, and DB Berkeley.
1999. Usenix annual technical conference, freenix
track.

Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English gigaword ﬁfth edi-
tion ldc2011t07. dvd. Philadelphia: Linguistic Data
Consortium .

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP). pages 1532–
1543. http://www.aclweb.org/anthology/D14-1162.

James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, et al. 2003. The timebank corpus. In Corpus
linguistics. Lancaster, UK., volume 2003, page 40.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.
Sequence to sequence learning with
2014.
the 27th
In Proceedings of
neural networks.
International Conference on Neural Information

Processing Systems - Volume 2. MIT Press, Cam-
bridge, MA, USA, NIPS’14, pages 3104–3112.
http://dl.acm.org/citation.cfm?id=2969033.2969173.

Naushad UzZaman, Hector Llorens, Leon Derczyn-
ski, James Allen, Marc Verhagen, and James Puste-
jovsky. 2013. Semeval-2013 task 1: Tempeval-3:
Evaluating time expressions, events, and temporal
In Second Joint Conference on Lexical
relations.
and Computational Semantics (* SEM), Volume 2:
Proceedings of the Seventh International Workshop
on Semantic Evaluation (SemEval 2013). volume 2,
pages 1–9.

Shikhar Vashishth, Rishabh Joshi, Sai Suman Prayaga,
Chiranjib Bhattacharyya, and Partha Talukdar. 2018.
Reside: Improving distantly-supervised neural rela-
tion extraction using side information. In Proceed-
ings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, pages 1257–1266.
http://aclweb.org/anthology/D18-1157.

Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval temporal
relation identiﬁcation. In Proceedings of the 4th in-
ternational workshop on semantic evaluations. As-
sociation for Computational Linguistics, pages 75–
80.

Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th international
workshop on semantic evaluation. Association for
Computational Linguistics, pages 57–62.

Xiaojun Wan. 2007.

In Proceedings of

Timedtextrank: Adding the
temporal dimension to multi-document summariza-
the 30th Annual In-
tion.
ternational ACM SIGIR Conference on Research
and Development in Information Retrieval. ACM,
New York, NY, USA, SIGIR ’07, pages 867–868.
https://doi.org/10.1145/1277741.1277949.

Pareek,

Michihiro Yasunaga, Rui Zhang, Kshitijh Meelu,
Ayush
and
Krishnan
Dragomir R. Radev. 2017. Graph-based neural
In Proceedings
multi-document summarization.
of CoNLL-2017. Association for Computational
Linguistics.

Srinivasan,

Katsumasa Yoshikawa, Sebastian Riedel, Masayuki
Asahara, and Yuji Matsumoto. 2009.
Jointly
identifying temporal relations with markov logic.
In Proceedings of
the
47th Annual Meeting of
the ACL and the 4th
International Joint Conference on Natural Lan-
guage Processing of
the AFNLP. Association
for Computational Linguistics, pages 405–413.
http://www.aclweb.org/anthology/P09-1046.

the Joint Conference of

