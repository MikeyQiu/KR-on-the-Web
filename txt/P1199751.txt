Commonsense for Generative Multi-Hop Question Answering Tasks

Lisa Bauer∗

Yicheng Wang∗
UNC Chapel Hill
{lbauer6, yicheng, mbansal}@cs.unc.edu

Mohit Bansal

9
1
0
2
 
n
u
J
 
1
 
 
]
L
C
.
s
c
[
 
 
3
v
9
0
3
6
0
.
9
0
8
1
:
v
i
X
r
a

Abstract

Reading comprehension QA tasks have seen
a recent surge in popularity, yet most works
have focused on fact-ﬁnding extractive QA.
We instead focus on a more challenging multi-
hop generative task (NarrativeQA), which re-
quires the model to reason, gather, and synthe-
size disjoint pieces of information within the
context to generate an answer. This type of
multi-step reasoning also often requires under-
standing implicit relations, which humans re-
solve via external, background commonsense
knowledge. We ﬁrst present a strong genera-
tive baseline that uses a multi-attention mech-
anism to perform multiple hops of reasoning
and a pointer-generator decoder to synthesize
the answer. This model performs substan-
tially better than previous generative models,
and is competitive with current state-of-the-
art span prediction models. We next intro-
duce a novel system for selecting grounded
multi-hop relational commonsense informa-
tion from ConceptNet via a pointwise mutual
information and term-frequency based scor-
ing function. Finally, we effectively use this
extracted commonsense information to ﬁll in
gaps of reasoning between context hops, using
a selectively-gated attention mechanism. This
boosts the model’s performance signiﬁcantly
(also veriﬁed via human evaluation), establish-
ing a new state-of-the-art for the task. We
also show promising initial results of the gen-
eralizability of our background knowledge en-
hancements by demonstrating some improve-
ment on QAngaroo-WikiHop, another multi-
hop reasoning dataset.

1

Introduction

In this paper, we explore the task of machine
reading comprehension (MRC) based QA. This

∗ Equal contribution (published at EMNLP 2018).
We publicly release all our code, models, and data at:

https://github.com/yicheng-w/CommonSenseMultiHopQA

task tests a model’s natural language understand-
ing capabilities by asking it to answer a question
based on a passage of relevant content. Much
progress has been made in reasoning-based MRC-
QA on the bAbI dataset (Weston et al., 2016),
which contains questions that require the combi-
nation of multiple disjoint pieces of evidence in
the context. However, due to its synthetic nature,
bAbI evidences have smaller lexicons and sim-
pler passage structures when compared to human-
generated text.

There also have been several attempts at the
MRC-QA task on human-generated text. Large
scale datasets such as CNN/DM (Hermann et al.,
2015) and SQuAD (Rajpurkar et al., 2016) have
made the training of end-to-end neural models
possible. However, these datasets are fact-based
and do not place heavy emphasis on multi-hop rea-
soning capabilities. More recent datasets such as
QAngaroo (Welbl et al., 2018) have prompted a
strong focus on multi-hop reasoning in very long
texts. However, QAngaroo is an extractive dataset
where answers are guaranteed to be spans within
the context; hence, this is more focused on fact
ﬁnding and linking, and does not require models
to synthesize and generate new information.

We focus on the recently published Narra-
tiveQA generative dataset (Koˇcisk`y et al., 2018)
that contains questions requiring multi-hop rea-
soning for long, complex stories and other nar-
ratives, which requires the model to go beyond
fact linking and to synthesize non-span answers.
Hence, models that perform well on previous rea-
soning tasks (Dhingra et al., 2018) have had lim-
ited success on this dataset. In this paper, we ﬁrst
propose the Multi-Hop Pointer-Generator Model
(MHPGM), a strong baseline model that uses mul-
tiple hops of bidirectional attention, self-attention,
and a pointer-generator decoder to effectively read
and reason within a long passage and synthesize

a coherent response. Our model achieves 41.49
Rouge-L and 17.33 METEOR on the summary
subtask of NarrativeQA, substantially better than
the performance of previous generative models.

Next,

to address the issue that understand-
ing human-generated text and performing long-
distance reasoning on it often involves intermittent
access to missing hops of external commonsense
(background) knowledge, we present an algorithm
for selecting useful, grounded multi-hop relational
knowledge paths from ConceptNet (Speer and
Havasi, 2012) via a pointwise mutual information
(PMI) and term-frequency-based scoring func-
tion. We then present a novel method of insert-
ing these selected commonsense paths between
the hops of document-context reasoning within
our model, via the Necessary and Optional Infor-
mation Cell (NOIC), which employs a selectively-
gated attention mechanism that utilizes common-
sense information to effectively ﬁll in gaps of in-
ference. With these additions, we further improve
performance on the NarrativeQA dataset, achiev-
ing 44.16 Rouge-L and 19.03 METEOR (also ver-
iﬁed via human evaluation). We also provide man-
ual analysis on the effectiveness of our common-
sense selection algorithm.

Finally,

to show the generalizability of our
multi-hop reasoning and commonsense methods,
we show some promising initial results via the ad-
dition of commonsense information over the base-
line on QAngaroo-WikiHop (Welbl et al., 2018),
an extractive dataset for multi-hop reasoning from
a different domain.

2 Related Work

Machine Reading Comprehension: MRC has
long been a task used to assess a model’s ability
to understand and reason about language. Large
scale datasets such as CNN/Daily Mail
(Her-
mann et al., 2015) and SQuAD (Rajpurkar et al.,
2016) have encouraged the development of many
advanced, high performing attention-based neural
models (Seo et al., 2017; Dhingra et al., 2017).
Concurrently, datasets such as bAbI (Weston et al.,
2016) have focused speciﬁcally on multi-step rea-
soning by requiring the model to reason with
disjoint pieces of information. On this task,
it has been shown that iteratively updating the
query representation with information from the
context can effectively emulate multi-step reason-
ing (Sukhbaatar et al., 2015).

More recently, there has been an increase in
multi-paragraph, multi-hop inference QA datasets
such as QAngaroo (Welbl et al., 2018) and Narra-
tiveQA (Koˇcisk`y et al., 2018). These datasets have
much longer contexts than previous datasets, and
answering a question often requires the synthesis
of multiple discontiguous pieces of evidence.
It
has been shown that models designed for previ-
ous tasks (Seo et al., 2017; Kadlec et al., 2016)
have limited success on these new datasets.
In
our work, we expand upon Gated Attention Net-
work (Dhingra et al., 2017) to create a baseline
model better suited for complex MRC datasets
such as NarrativeQA by improving its attention
and gating mechanisms, expanding its generation
capabilities, and allowing access to external com-
monsense for connecting implicit relations.

Commonsense/Background Knowledge: Com-
monsense or background knowledge has been
tasks including opinion min-
used for several
ing (Cambria et al., 2010), sentiment analy-
sis (Poria et al., 2015, 2016), handwritten text
recognition (Wang et al., 2013), and more re-
cently, dialogue (Young et al., 2018; Ghazvinine-
jad et al., 2018). These approaches add com-
monsense knowledge as relation triples or fea-
tures from external databases. Recently, large-
scale graphical commonsense databases such as
ConceptNet (Speer and Havasi, 2012) use graph-
ical structure to express intricate relations be-
tween concepts, but effective goal-oriented graph
traversal has not been extensively used in previous
commonsense incorporation efforts. Knowledge-
base QA is a task in which systems are asked to
ﬁnd answers to questions by traversing knowledge
graphs (Bollacker et al., 2008). Knowledge path
extraction has been shown to be effective at the
task (Bordes et al., 2014; Bao et al., 2016). We ap-
ply these techniques to MRC-QA by using them to
extract useful commonsense knowledge paths that
fully utilize the graphical nature of databases such
as ConceptNet (Speer and Havasi, 2012).

Incorporation of External Knowledge: There
have been several attempts at using external
knowledge to boost model performance on a vari-
ety of tasks: Chen et al. (2018) showed that adding
lexical information from semantic databases such
as WordNet improves performance on NLI; Xu
et al. (2017) used a gated recall-LSTM mechanism
to incorporate commonsense information into to-
ken representations in dialogue.

In MRC, Weissenborn et al. (2017) integrated
external background knowledge into an NLU
model by using contextually-reﬁned word em-
beddings which integrated information from Con-
ceptNet (single-hop relations mapped to unstruc-
tured text) via a single layer bidirectional LSTM.
Concurrently to our work, Mihaylov and Frank
(2018) showed improvements on a cloze-style task
by incorporating commonsense knowledge via a
context-to-commonsense attention, where com-
monsense relations were extracted as triples. This
work represented commonsense relations as key-
value pairs and combined context representation
and commonsense via a static gate.

Differing from previous works, we employ
multi-hop commonsense paths (multiple con-
nected edges within ConceptNet graph that give
us information beyond a single relationship triple)
to help with our MRC model. Moreover, we use
this in tandem with our multi-hop reasoning archi-
tecture to incorporate different aspects of the com-
monsense relationship path at each hop, in order
to bridge different inference gaps in the multi-hop
QA task. Additionally, our model performs syn-
thesis with its external, background knowledge as
it generates, rather than extracts, its answer.

3 Methods

3.1 Multi-Hop Pointer-Generator Baseline

1, wa

2, . . . , wa

1 , wC
2 , . . . , wQ

the context, X C = {wC
1 , wQ

We ﬁrst rigorously state the problem of genera-
tive QA as follows: given two sequences of input
2 , . . . , wC
n }
tokens:
and the query, X Q = {wQ
m}, the
system should generate a series of answer tokens
X a = {wa
p}. As outlined in previous
sections, an effective generative QA model needs
to be able to perform several hops of reasoning
It would also
over long and complex passages.
need to be able to generate coherent statements to
answer complex questions while having the abil-
ity to copy rare words such as speciﬁc entities
from the reading context. With these in mind, we
propose the Multi-Hop Pointer-Generator Model
(MHPGM) baseline, a novel combination of previ-
ous works with the following major components:

• Embedding Layer: The tokens are embedded
into both learned word embeddings and pre-
trained context-aware embeddings (ELMo (Pe-
ters et al., 2018)).

• Reasoning Layer: The embedded context is
then passed through k reasoning cells, each

of which iteratively updates the context repre-
sentation with information from the query via
BiDAF attention (Seo et al., 2017), emulating a
single reasoning step within the multi-step rea-
soning process.

• Self-Attention Layer: The context representa-
tion is passed through a layer of self-attention
(Cheng et al., 2016) to resolve long-term depen-
dencies and co-reference within the context.

• Pointer-Generator Decoding Layer:

A
attention-pointer-generator decoder (See et al.,
2017) that attends on and potentially copies
from the context is used to create the answer.

The overall model is illustrated in Fig. 1, and

the layers are described in further detail below.
Embedding layer: We embed each word from the
context and question with a learned embedding
space of dimension d. We also obtain context-
aware embeddings for each word via the pre-
trained embedding from language models (ELMo)
(1024 dimensions). The embedded representation
for each word in the context or question, eC
i or
eQ
i ∈ Rd+1024, is the concatenation of its learned
word embedding and ELMo embedding.
Reasoning layer: Our reasoning layer is com-
posed of k reasoning cells (see Fig. 1), where each
incrementally updates the context representation.
The tth reasoning cell’s inputs are the previous
step’s output ({ct−1
}n
i=1) and the embedded ques-
tion ({eQ
i }m
i=1). It ﬁrst creates step-speciﬁc con-
text and query encodings via cell-speciﬁc bidirec-
tional LSTMs:

i

ut = BiLSTM(ct−1);

vt = BiLSTM(eQ)

Then, we use bidirectional attention (Seo et al.,
2017) to emulate a hop of reasoning by focusing
on relevant aspects of the context. Speciﬁcally, we
ﬁrst compute context-to-query attention:

ij = W t
St

1ut

i + W t

2vt

j + W t

3(ut

i (cid:12) vt
j)

pt
ij =

exp(St
ij)
k=1 exp(St

ik)

(cid:80)m

(cq)t

i =

ijvt
pt
j

m
(cid:88)

j=1

2, W t

1, W t

where W t
3 are trainable parameters, and
(cid:12) is elementwise multiplication. We then com-
pute a query-to-context attention vector:

mt

i = max
1≤j≤m

St
ij

Figure 1: Architecture for our Multi-Hop Pointer-Generator Model, and the NOIC commonsense reasoning cell.

pt
i =

exp(mt
i)
j=1 exp(mt
j)

(cid:80)n

qc

t =

iut
pt
i

n
(cid:88)

i=1

We then obtain the updated context representation:

i = [ut
ct

i; (cq)t

i; ut

i (cid:12) (cq)t

i; qc

t (cid:12) (cq)t
i]

where ; is concatenation, ct is the cell’s output.

The initial input of the reasoning layer is the
embedded context representation, i.e., c0 = eC,
and the ﬁnal output of the reasoning layer is the
output of the last cell, ck.
Self-Attention Layer: As the ﬁnal layer before
answer generation, we utilize a residual static self-
attention mechanism (Clark and Gardner, 2018) to
help the model process long contexts with long-
term dependencies. The input of this layer is the
output of the last reasoning cell, ck. We ﬁrst pass
this representation through a fully-connected layer
and then a bi-directional LSTM to obtain another
representation of the context cSA. We obtain the
self attention representation c(cid:48):

ij = W4cSA
SSA

i + W5cSA

j + W6(cSA

i (cid:12) cSA

j

)

pSA
ij =

exp(SSA
ij )
k=1 exp(SSA
ik )

(cid:80)n

n
(cid:88)

c(cid:48)

i =

ij cSA
pSA
j

j=1
where W4, W5, and W6 are trainable parameters.
The output of the self-attention layer is gener-

ated by another layer of bidirectional LSTM.

c(cid:48)(cid:48) = BiLSTM([c(cid:48); cSA; c(cid:48) (cid:12) cSA]

Finally, we add this residually to ck to obtain the
encoded context c = ck + c(cid:48)(cid:48).
Pointer-Generator Decoding Layer: Similar to
the work of See et al. (2017), we use a pointer-
generator model attending on (and potentially
copying from) the context.

At decoding step t, the decoder receives the in-
put xt (embedded representation of last timestep’s
output), the last time step’s hidden state st−1 and
context vector at−1. The decoder computes the
current hidden state st as:

st = LSTM([xt; at−1], st−1)

This hidden state is then used to compute a proba-
bility distribution over the generative vocabulary:

Pgen = softmax(Wgenst + bgen)

We

employ Bahdanau

attention mecha-
nism (Bahdanau et al., 2015) to attend over the
context (c being the output of self-attention layer):
αi = v(cid:124) tanh(Wcci + Wsst + battn)

Dataset

Outside Knowledge Required

WikiHop
NarrativeQA

11%
42%

Table 1: Qualitative analysis of commonsense require-
ments. WikiHop results are from Welbl et al. (2018);
NarrativeQA results are from our manual analysis (on
the validation set).

about a question. We remedy this issue by intro-
ducing grounded commonsense (background) in-
formation using relations between concepts from
ConceptNet (Speer and Havasi, 2012)1 that help
inference by introducing useful connections be-
tween concepts in the context and question.

Due to the size of the semantic network and
the large amount of unnecessary information, we
need an effective way of selecting relations which
provides novel information while being grounded
by the context-query pair. Our commonsense se-
lection strategy is twofold: (1) collect potentially
relevant concepts via a tree construction method
aimed at selecting with high recall candidate rea-
soning paths, and (2) rank and ﬁlter these paths to
ensure both the quality and variety of added infor-
mation via a 3-step scoring strategy (initial node
scoring, cumulative node scoring, and path selec-
tion). We will refer to Fig. 2 as a running example
throughout this section.2

3.2.1 Tree Construction
Given context C and question Q, we want to con-
struct paths grounded in the pair that emulate rea-
soning steps required to answer the question. In
this section, we build ‘prototype’ paths by con-
structing trees rooted in concepts in the query with
the following branching steps3 to emulate multi-
hop reasoning process. For each concept c1 in the
question, we do:
Direct Interaction: In the ﬁrst level, we select re-
lations r1 from ConceptNet that directly link c1
to a concept within the context, c2 ∈ C, e.g., in
Fig. 2, we have lady → church, lady → mother,
lady → person.
Multi-Hop: We then select relations in Concept-
Net r2 that link c2 to another concept in the con-
text, c3 ∈ C. This emulates a potential reason-

1A semantic network where the nodes are individual con-
cepts (words or phrases) and the edges describe directed re-
lations between them (e.g., (cid:104)island, UsedFor, vacation(cid:105)).

2We release all our commonsense extraction code and
the extracted commonsense data at: https://github.com/
yicheng-w/CommonSenseMultiHopQA

3If we are unable to ﬁnd a relation that satisﬁes the condi-

tion, we keep the steps up to and including the node.

Figure 2: Commonsense selection approach.

ˆαi =

exp(αi)
j=1 exp(αj)

(cid:80)n

at =

ˆαici

n
(cid:88)

i=1

We utilize a pointer mechanism that allows the
decoder to directly copy tokens from the context
based on ˆαi. We calculate a selection distribution
psel ∈ R2, where psel
is the probability of gener-
1
ating a token from Pgen and psel
is the probability
2
of copying a word from the context:

o = σ(Waat + Wxxt + Wsst + bptr)

psel = softmax(o)

Our ﬁnal output distribution at timestep t is a
weighted sum of the generative distribution and
the copy distribution:

Pt(w) = psel

1 Pgen(w) + psel
2

(cid:88)

ˆαi

i:wC

i =w

3.2 Commonsense Selection and

Representation

In QA tasks that require multiple hops of reason-
ing, the model often needs knowledge of relations
not directly stated in the context to reach the cor-
rect conclusion. In the datasets we consider, man-
ual analysis shows that external knowledge is fre-
quently needed for inference (see Table 1).

Even with a large amount of training data, it
is very unlikely that a model is able to learn ev-
ery nuanced relation between concepts and ap-
ply the correct ones (as in Fig. 2) when reasoning

ing hop within the context of the MRC task, e.g.,
church → house, mother → daughter, person →
lover.
Outside Knowledge: We then allow an uncon-
strained hop into c3’s neighbors in ConceptNet,
getting to c4 ∈ nbh(c3) via r3 (nbh(v) is the set
of nodes that can be reached from v in one hop).
This emulates the gathering of useful external in-
formation to complete paths within the context,
e.g., house → child, daughter → child.
Context-Grounding: To ensure that the exter-
nal knowledge is indeed helpful to the task, and
also to explicitly link 2nd degree neighbor con-
cepts within the context, we ﬁnish the process by
grounding it again into context by connecting c4
to c5 ∈ C via r4, e.g., child → their.

3.2.2 Rank and Filter

This tree building process collects a large number
of potentially relevant and useful paths. However,
this step also introduces a large amount of noise.
For example, given the question and full context
(not depicted in the ﬁgure) in Fig. 2, we obtain
the path “between → hard → being → cottage →
country” using our tree building method, which is
not relevant to our question. Therefore, to improve
the precision of useful concepts, we rank these
knowledge paths by their relevance and ﬁlter out
noise using the following 3-step scoring method:
Initial Node Scoring: We want to select paths
to the context,
with nodes that are important
in order to provide the most useful common-
sense relations. We approximate importance and
saliency for concepts in the context by their term-
frequency, under the heuristic that important con-
cepts occur more frequently. Thus we score c ∈
{c2, c3, c5} by: score(c) = count(c)/|C|, where
|C| is the context length and count() is the num-
ber of times a concept appears in the context. In
Fig. 2, this ensures that concepts like daughter are
scored highly due to their frequency in the context.
For c4, we use a special scoring function as it is
an unconstrained hop into ConceptNet. We want
c4 to be a logically consistent next step in reason-
ing following the path of c1 to c3, e.g., in Fig. 2, we
see that child is a logically consistent next step af-
ter the partial path of mother → daughter. We ap-
proximate this based on the heuristic that logically
consistent paths occur more frequently. Therefore,
we score this node via Pointwise Mutual Informa-
tion (PMI) between the partial path c1−3 and c4:
PMI(c4, c1−3) = log(P(c4, c1−3)/P(c4)P(c1−3)),

where

P(c4, c1−3) =

# of paths connecting c1, c2, c3, c4
# of distinct paths of length 4

P(c4) =

# of nodes that can reach c4
|ConceptNet|

P(c1−3) =

# of paths connecting c1, c2, c3
# of paths of length 3

Further, it is well known that PMI has high
sensitivity to low-frequency values,
thus we
use normalized PMI (NPMI) (Bouma, 2009):
score(c4) = PMI(c4, c1−3)/(− log P(c4, c1−3)).

Since the branching at each juncture represents
a hop in the multi-hop reasoning process, and hops
at different levels or with different parent nodes do
not ‘compete’ with each other, we normalize each
node’s score against its siblings:

n-score(c) = softmaxsiblings(c)(score(c)).

Cumulative Node Scoring: We want to add com-
monsense paths consisting of multiple hops of
relevant information, thus we re-score each node
based not only on its relevance and saliency but
also that of its tree descendants.

We do this by computing a cumulative node
score from the bottom up, where at the leaf nodes,
we have c-score = n-score, and for cl not a leaf
node, we have c-score(cl) = n-score(cl) + f (cl)
where f of a node is the average of the c-scores of
its top 2 highest scoring children.

For example, given the paths lady → mother →
daughter, lady → mother → married, and lady →
mother → book, we start the cumulative scoring
at the leaf nodes, which in this case are daugh-
ter, married, and book, where daughter and mar-
ried are scored much higher than book due to their
more frequent occurrences. Then, to cumulatively
score mother , we would take the average score of
its two highest scoring children (in this case mar-
ried and daughter) and compound that with the
score of mother itself. Note that the poor scoring
of the irrelevant concept book does not affect the
scoring of mother, which is quite high due to the
concept’s frequent occurrence and the relevance of
its top scoring children.
Path Selection: We select paths in a top-down
breath-ﬁrst fashion in order to add information rel-
evant to different parts of the context. Starting at
the root, we recursively take two of its children
with the highest cumulative scores until we reach
a leaf, selecting up to 24 = 16 paths. For example,

if we were at node mother, this allows us to se-
lect the child node daughter and married over the
child node book. These selected paths, as well as
their partial sub-paths, are what we add as exter-
nal information to the QA model, i.e., we add the
complete path (cid:104)lady, AtLocation, church, Relat-
edTo, house, RelatedTo, child, RelatedTo, their(cid:105),
but also truncated versions of the path, including
(cid:104)lady, AtLocation, church, RelatedTo, house, Re-
latedTo, child(cid:105). We directly give these paths to the
model as sequences of tokens.4

Overall, our sampling strategy provides the
knowledge that a lady can be a mother and that
mother is connected to daughter. This creates
a logical connection between lady and daughter
which helps highlight the importance of our sec-
ond piece of evidence (see Fig. 2). Likewise,
the commonsense information we extracted cre-
ate a similar connection in our third piece of ev-
idence, which states the explicit connection be-
tween daughter and Esther. We also successfully
extract a more story context-centric connection, in
which commonsense provides the knowledge that
a lady is at the location church, which directs to
another piece of evidence in the context. Addition-
ally, this path also encodes a relation between lady
and child, by way of church, which is how lady
and Esther are explicitly connected in the story.

3.3 Commonsense Model Incorporation

i

,

, wCS
2

l } where wCS

Given the list of commonsense logic paths as se-
quences of words: X CS = {wCS
. . . ,
1
wCS
represents the list of tokens
that make up a single path, we ﬁrst embed these
commonsense tokens into the learned embedding
space used by the model, giving us the embedded
ij ∈ Rd. We want to
commonsense tokens, eCS
use these commonsense paths to ﬁll in the gaps
of reasoning between hops of inference. Thus,
we propose Necessary and Optional Information
Cell
(NOIC), a variation of our base reasoning
cell used in the reasoning layer that is capable of
incorporating optional helpful information.

NOIC This cell is an extension to the base rea-
soning cell that allows the model to use common-
sense information to ﬁll in gaps of reasoning. An
example of this is on the bottom left of Fig. 1,
where we see that the cell ﬁrst performs the op-
erations done in the base reasoning cell and then

4In cases where more than one relation can be used to

make a hop, we pick one at random.

adds optional, commonsense information.

At reasoning step t, after obtaining the out-
put of the base reasoning cell, ct, we create a
cell-speciﬁc representation for commonsense in-
formation by concatenating the embedded com-
monsense paths so that each path has a single vec-
tor representation, uCS
. We then project it to the
i
same dimension as ct
i = ReLU(W uCS
i: vCS
i + b)
where W and b are trainable parameters.

We use an attention layer to model the interac-

tion between commonsense and the context:

ij = W CS
SCS

1 ct

i + W CS

2 vCS

j + W CS

3

(ct

i (cid:12) vCS
j

)

pCS
ij =

exp(SCS
ij )
k=1 exp(SCS
ij )

(cid:80)l

cCS
i =

ij vCS
pCS
j

l
(cid:88)

j=1

Finally, we combine this commonsense-aware
context representation with the original ct
i via a
sigmoid gate, since commonsense information is
often not necessary at every step of inference:

zi = σ(Wz[cCS

; ct

i

i] + bz)

(co)t

i + (1 − zi) (cid:12) cCS

i = zi (cid:12) ct
t as the output of the current reasoning
We use co
step instead of ct. As we replace each base rea-
soning cell with NOIC, we selectively incorporate
commonsense at every step of inference.

i

4 Experimental Setup

Datasets: We report results on two multi-hop rea-
soning datasets: generative NarrativeQA (Koˇcisk`y
et al., 2018) (summary subtask) and extractive
QAngaroo WikiHop (Welbl et al., 2018). For
multiple-choice WikiHop, we rank candidate re-
sponses by their generation probability. Similar to
previous works (Dhingra et al., 2018), we use the
non-oracle, unmasked and not-validated dataset.
Evaluation Metrics: We evaluate NarrativeQA
on the metrics proposed by its original authors:
Bleu-1, Bleu-4 (Papineni et al., 2002), ME-
TEOR (Banerjee and Lavie, 2005) and Rouge-
L (Lin, 2004). We also evaluate on CIDEr (Vedan-
tam et al., 2015) which emphasizes annotator con-
sensus. For WikiHop, we evaluate on accuracy.

More dataset, metric, and all other training de-

tails are in the supplementary.

Model

BLEU-1

BLEU-4 METEOR Rouge-L CIDEr

Seq2Seq (Koˇcisk`y et al., 2018)
ASR (Koˇcisk`y et al., 2018)
BiDAF† (Koˇcisk`y et al., 2018)
BiAttn + MRU-LSTM† (Tay et al., 2018)

MHPGM
MHPGM+ NOIC

15.89
23.20
33.72
36.55

40.24
43.63

1.26
6.39
15.53
19.79

17.40
21.07

4.08
7.77
15.38
17.87

17.33
19.03

13.15
22.26
36.30
41.44

41.49
44.16

-
-
-
-

139.23
152.98

Table 2: Results across different metrics on the test set of NarrativeQA-summaries task. † indicates span prediction
models trained on the Rouge-L retrieval oracle.

Model

BiDAF (Welbl et al., 2018)
Coref-GRU (Dhingra et al., 2018)

MHPGM
MHPGM+ NOIC

Dev

42.1
56.0

56.2
58.5

Test

42.9
59.3

57.5
57.9

Table 3: Results of our models on WikiHop dataset,
measured in % accuracy.

5 Results

5.1 Main Experiment

The results of our model on both NarrativeQA and
WikiHop with and without commonsense incorpo-
ration are shown in Table 2 and Table 3. We see
empirically that our model outperforms all gener-
ative models on NarrativeQA, and is competitive
with the top span prediction models. Furthermore,
with the NOIC commonsense integration, we were
able to further improve performance (p < 0.001
on all metrics5), establishing a new state-of-the-art
for the task.

We also see that our model performs rea-
sonably well on WikiHop, and further achieves
promising initial improvements via the addition
of commonsense, hinting at the generalizability
of our approaches. We speculate that the im-
provement is smaller on Wikihop because only
approximately 11% of WikiHop data points re-
quire commonsense and because WikiHop data re-
quires more fact-based commonsense (e.g., from
Freebase (Bollacker et al., 2008)) as opposed to
semantics-based commonsense (e.g., from Con-
ceptNet (Speer and Havasi, 2012)).6

5.2 Model Ablations

We also tested the effectiveness of each compo-
nent of our architecture as well as the effective-

5Stat.

signiﬁcance computed using bootstrap test with
100K iterations (Noreen, 1989; Efron and Tibshirani, 1994).
6All results here are for the standard (non-oracle) un-
masked and not-validated dataset. Welbl et al. (2018) has
reported higher numbers on different data settings which are
not comparable to our results.

ness of adding commonsense information on the
NarrativeQA validation set, with results shown in
Table 4. Experiment 1 and 5 are our models pre-
sented in Table 2. Experiment 2 demonstrates the
importance of multi-hop attention by showing that
if we only allow one hop of attention (even with all
other components of the model, including ELMo
embeddings) the model’s performance decreases
by over 12 Rouge-L points. Experiment 3 and 4
demonstrate the effectiveness of other parts of our
model. We see that ELMo embeddings (Peters
et al., 2018) were also important for the model’s
performance and that self-attention is able to con-
tribute signiﬁcantly to performance on top of other
components of the model. Finally, we see that ef-
fectively introducing external knowledge via our
commonsense selection algorithm and NOIC can
improve performance even further on top of our
strong baseline.

5.3 Commonsense Ablations

We also conducted experiments testing the effec-
tiveness of our commonsense selection and incor-
poration techniques. We ﬁrst tried to naively add
ConceptNet information by initializing the word
embeddings with the ConceptNet-trained embed-
dings, NumberBatch (Speer and Havasi, 2012)
(we also change embedding size from 256 to
300). Then, to verify the effectiveness of our com-
monsense selection and grounding algorithm, we
test our best model on in-domain noise by giv-
ing each context-query pair a set of random rela-
tions grounded in other context-query pairs. This
should teach the model about general common-
sense relations present in the domain of Narra-
tiveQA but does not provide grounding that ﬁlls
in speciﬁc hops of inference. We also experi-
mented with a simpler commonsense extraction
method of using a single hop from the query to
the context. The results of these are shown in
Table 5, where we see that neither NumberBatch
nor random-relationships nor single-hop common-

#

1
2
3
4
5

Ablation

-
k = 1
- ELMo
- Self-Attn
+ NOIC

B-1

42.3
32.5
32.8
37.0
46.0

B-4 M

R

C

18.9
11.7
12.7
16.4
21.9

18.3
12.9
13.6
15.6
20.7

44.9
32.4
33.7
38.6
48.0

151.6
95.7
103.1
125.6
166.6

Table 4: Model ablations on NarrativeQA val-set.

Commonsense

B-1

B-4 M

R

C

None
NumberBatch
Random Rel.
Single Hop
Grounded Rel.

42.3
42.6
43.3
42.1
45.9

18.9
19.6
19.3
19.9
21.9

18.3
18.6
18.6
18.2
20.7

44.9
44.4
45.2
44.0
48.0

151.6
148.1
151.2
148.6
166.6

Table 5: Commonsense ablations on NarrativeQA val-
set.

sense offer statistically signiﬁcant improvements7,
whereas our commonsense selection and incorpo-
ration mechanism improves performance signiﬁ-
cantly across all metrics. We also present several
examples of extracted commonsense and its model
attention visualization in the supplementary.

6 Human Evaluation Analysis

We also conduct human evaluation analysis on
both the quality of the selected commonsense re-
lations, as well as the performance of our ﬁnal
model.
Commonsense Selection: We conducted manual
analysis on a 50 sample subset of the NarrativeQA
test set to check the effectiveness of our common-
sense selection algorithm. Speciﬁcally, given a
context-query pair, as well as the commonsense
selected by our algorithm, we conduct two inde-
pendent evaluations: (1) was any external com-
monsense knowledge necessary for answering the
question?; (2) were the commonsense relations
provided by our algorithm relevant to the ques-
tion? The result for these two evaluations as well
as how they overlap with each other are shown in
Table 6, where we see that 50% of the cases re-
quired external commonsense knowledge, and on
a majority (34%) of those cases our algorithm was
able to select the correct/relevant commonsense
information to ﬁll in gaps of inference. We also
see that in general, our algorithm was able to pro-
vide useful commonsense 48% of the time.
Model Performance: We also conduct human
evaluation to verify that our commonsense incor-
porated model was indeed better than MHPGM.

7The improvement in Rouge-L and METEOR for all three

ablation approaches have p ≥ 0.15 with the bootstrap test.

Commonsense Required
Yes

No

Relevant CS Extracted
Irrelevant CS Extracted

34%
16%

14%
36%

Table 6: NarrativeQA’s commonsense requirements
and effectiveness of commonsense selection algorithm.

MHPGM+NOIC better
MHPGM better
Indistinguishable (Both-good)
Indistinguishable (Both-bad)

23%
15%
41%
21%

Table 7: Human evaluation on the output quality of the
MHPGM+NOIC vs. MHPGM in terms of correctness.

We randomly selected 100 examples from the Nar-
rativeQA test set, along with both models’ pre-
dicted answers, and for each datapoint, we asked
3 external human evaluators (ﬂuent English speak-
ers) to decide (without knowing which model pro-
duced each response) if one is strictly better than
the other, or that they were similar in quality (both-
good or both-bad). As shown in Table 7, we see
that the human evaluation results are in agreement
with that of the automatic evaluation metrics: our
commonsense incorporation has a reasonable im-
pact on the overall correctness of the model. The
inter-annotator agreement had a Fleiss κ = 0.831,
indicating ‘almost-perfect’ agreement between the
annotators (Landis and Koch, 1977).

7 Conclusion

We present an effective reasoning-generative QA
architecture that is a novel combination of previ-
ous work, which uses multiple hops of bidirec-
tional attention and a pointer-generator decoder to
effectively perform multi-hop reasoning and syn-
thesize a coherent and correct answer. Further, we
introduce an algorithm to select grounded, use-
ful paths of commonsense knowledge to ﬁll in
the gaps of inference required for QA, as well a
Necessary and Optional Information Cell (NOIC)
which successfully incorporates this information
during multi-hop reasoning to achieve the new
state-of-the-art on NarrativeQA.

Acknowledgments
We thank the reviewers for their helpful com-
ments. This work was supported by DARPA
(YFA17-D17AP00022), Google Faculty Research
Award, Bloomberg Data Science Research Grant,
and NVidia GPU awards. The views contained in
this article are those of the authors and not of the
funding agency.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR.

Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
In Proceed-
correlation with human judgments.
ings of the ACL Workshop on intrinsic and extrinsic
evaluation measures for machine translation and/or
summarization, pages 65–72.

Junwei Bao, Nan Duan, Zhao Yan, Ming Zhou, and
Tiejun Zhao. 2016. Constraint-based question an-
In Proceedings of
swering with knowledge graph.
COLING 2016, the 26th International Conference
on Computational Linguistics: Technical Papers,
pages 2503–2514.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1247–1250. AcM.

Antoine Bordes, Sumit Chopra, and Jason Weston.
2014. Question answering with subgraph embed-
In Proceedings of the 2014 Conference on
dings.
Empirical Methods in Natural Language Processing
(EMNLP), pages 615–620.

Gerlof Bouma. 2009. Normalized (pointwise) mutual
information in collocation extraction. Proceedings
of GSCL, pages 31–40.

Erik Cambria, Amir Hussain, Tariq Durrani, Catherine
Havasi, Chris Eckl, and James Munro. 2010. Sen-
tic computing for patient centered applications. In
Signal Processing (ICSP), 2010 IEEE 10th Interna-
tional Conference on, pages 1279–1282. IEEE.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Diana
Inkpen, and Si Wei. 2018. Neural natural language
inference models enhanced with external knowl-
edge. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 2406–2417. Associa-
tion for Computational Linguistics.

Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016.
Long short-term memory-networks for machine
reading. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Process-
ing, pages 551–561.

Christopher Clark and Matt Gardner. 2018. Simple
and effective multi-paragraph reading comprehen-
sion. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Long
Papers), pages 845–855.

Bhuwan Dhingra, Qiao Jin, Zhilin Yang, William Co-
hen, and Ruslan Salakhutdinov. 2018. Neural mod-
els for reasoning over multiple mentions using coref-
In Proceedings of the 2018 Conference of
erence.

the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers), volume 2, pages
42–48.

Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang, William
Cohen, and Ruslan Salakhutdinov. 2017. Gated-
In Pro-
attention readers for text comprehension.
ceedings of the 55th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), volume 1, pages 1832–1846.

Bradley Efron and Robert J Tibshirani. 1994. An intro-

duction to the bootstrap. CRC press.

Marjan Ghazvininejad, Chris Brockett, Ming-Wei
Chang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, and
Michel Galley. 2018. A knowledge-grounded neural
conversation model. In AAAI.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems, pages 1693–
1701.

Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and Jan
Kleindienst. 2016. Text understanding with the at-
tention sum reader network. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics, pages 908–918.

Diederik P Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In 3rd Interna-
tional Conference for Learning Representations.

Tom´aˇs Koˇcisk`y, Jonathan Schwarz, Phil Blunsom,
Chris Dyer, Karl Moritz Hermann, G´aabor Melis,
and Edward Grefenstette. 2018. The narrativeqa
Transactions
reading comprehension challenge.
of the Association of Computational Linguistics,
6:317–328.

J Richard Landis and Gary G Koch. 1977. The mea-
surement of observer agreement for categorical data.
biometrics, pages 159–174.

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. Text Summarization
Branches Out.

Todor Mihaylov and Anette Frank. 2018. Knowledge-
able reader: enhancing cloze-style reading compre-
hension with external commonsense knowledge. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 821–832. Association for Com-
putational Linguistics.

Eric W Noreen. 1989. Computer-intensive methods for

testing hypotheses. Wiley New York.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the

40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311–318. Association for
Computational Linguistics.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of NAACL.

Soujanya Poria, Erik Cambria, Alexander Gelbukh,
Federica Bisio, and Amir Hussain. 2015. Sentiment
data ﬂow analysis by means of dynamic linguistic
IEEE Computational Intelligence Maga-
patterns.
zine, 10(4):26–36.

Soujanya Poria, Iti Chaturvedi, Erik Cambria, and Fed-
erica Bisio. 2016. Sentic LDA: Improving on LDA
with semantic similarity for aspect-based sentiment
In Neural Networks (IJCNN), 2016 In-
analysis.
ternational Joint Conference on, pages 4465–4473.
IEEE.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, pages 2383–2392.

Abigail See, Peter J Liu, and Christopher D Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), volume 1,
pages 1073–1083.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2017. Bidirectional attention
ﬂow for machine comprehension. In ICLR.

Robyn Speer and Catherine Havasi. 2012. Represent-
ing general relational knowledge in ConceptNet 5.
In LREC, pages 3679–3686.

Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.
2015. End-to-end memory networks. In Advances
in Neural Information Processing Systems, pages
2440–2448.

Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2018.
Multi-range reasoning for machine comprehension.
arXiv preprint arXiv:1803.09074.

Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. 2015. Cider: Consensus-based image de-
In Proceedings of the IEEE
scription evaluation.
Conference on Computer Vision and Pattern Recog-
nition, pages 4566–4575.

Qiu-Feng Wang, Erik Cambria, Cheng-Lin Liu, and
Amir Hussain. 2013. Common sense knowledge
for handwritten chinese text recognition. Cognitive
Computation, 5(2):234–242.

Dirk Weissenborn, Tom´aˇs Koˇcisk`y, and Chris Dyer.
2017. Dynamic integration of background knowl-
arXiv preprint
edge in neural NLU systems.
arXiv:1706.02596.

Johannes Welbl, Pontus Stenetorp, and Sebastian
Riedel. 2018. Constructing datasets for multi-hop
reading comprehension across documents. Transac-
tions of the Association of Computational Linguis-
tics, 6:287–302.

Jason Weston, Antoine Bordes, Sumit Chopra, Alexan-
der M Rush, Bart van Merri¨enboer, Armand Joulin,
and Tomas Mikolov. 2016. Towards ai-complete
question answering: A set of prerequisite toy tasks.
In ICLR.

Zhen Xu, Bingquan Liu, Baoxun Wang, Chengjie Sun,
and Xiaolong Wang. 2017.
Incorporating loose-
structured knowledge into LSTM with recall gate for
conversation modeling. In International Joint Con-
ference on Neural Networks.

Tom Young, Erik Cambria, Iti Chaturvedi, Minlie
Huang, Hao Zhou, and Subham Biswas. 2018. Aug-
menting end-to-end dialog systems with common-
In Proceedings of the Thirty-
sense knowledge.
Second AAAI Conference on Artiﬁcial Intelligence
(AAAI-18).

A Supplemental Material

A.1 Experimental Setup

Datasets We test our model with and with-
out commonsense addition on two challenging
datasets that require multi-hop reasoning and ex-
ternal knowledge: NarrativeQA (Koˇcisk`y et al.,
2018) and QAngaroo-WikiHop (Welbl et al.,
2018). NarrativeQA is a generative QA dataset
where the passages are either stories or summaries
of stories, and the questions ask about complex
aspects of the narratives such as event timelines,
characters, relations between characters, etc. Each
question has two answers which are generated by
human annotators and usually cannot be found
in the passage directly. We focus on the sum-
mary subtask in this paper, where summaries have
lengths of up to 1000 words.

We also test our model on WikiHop, a fact
based, multi-hop dataset. Questions in WikiHop
often require a model to read several documents
in order to obtain an answer. We focus on the
multiple-choice part of WikiHop, where models
are tasked with picking the correct response from
a pool of candidates. We rank candidate responses
by calculating their generation probability based
on our model. As this is a multi-document QA
task, we ﬁrst rank the candidate documents via TF-
IDF cosine distance with the question, and then
take the top k documents such that their combined
length is less than 1300 words.

paths have high activation, but the activation be-
comes more focused on the passage’s key words
w.r.t. the question, as the number of hops increase.

Evaluation Metrics We evaluate NarrativeQA
on the metrics proposed by its original authors:
Bleu-1, Bleu-4 (Papineni et al., 2002), ME-
TEOR (Banerjee and Lavie, 2005) and Rouge-
L (Lin, 2004). We also evaluate on CIDEr (Vedan-
tam et al., 2015) as it places emphasize on annota-
tor consensus. For WikiHop, we evaluate on accu-
racy.

Training Details
In training for both datasets,
we minimize the negative log probability of gener-
ating the ground-truth answer with the Adam opti-
mizer (Kingma and Ba, 2015) with an initial learn-
ing rate of 0.001, a dropout-rate of 0.2 (dropout is
applied to the input of each RNN layer) and batch
size of 24. We use 256 dimensional word embed-
dings and a hidden size of 128 for all RNNs and
k = 3 hops of multi attention. At inference time
we use greedy decoding to generate the answer.
For both NarrativeQA and WikiHop, we reached
these parameters via tuning on the full, ofﬁcial val-
idation set.

A.2 Commonsense Extraction Examples

In Tables 8, 9, and 10 (see next page), we demon-
strate extracted commonsense examples for ques-
tions that require commonsense to reach an an-
swer. We bold words in the question and in the ex-
tracted commonsense in cases where the common-
sense knowledge explicitly bridges gaps between
implicitly connected words in the context or ques-
tion. The relevant context is also displayed, with
context words that are key to answering the ques-
tion (via commonsense) marked in bold. These are
then followed by a context visualization described
in the next section.

A.3 Commonsense Integration Visualization

We also visualize how much commonsense infor-
mation is integrated into each part of the context
by providing a visualization of the zi value (see
end of Sec. 3.3 of main ﬁle) for i ∈ {1, 2, 3},
which is the gate value signifying how much
commonsense-attention representation is used in
the output context representation. In the follow-
ing examples (next page), we use shades of blue
to represent the average of (1 − zi) at each word
in the context (normalized within each hop), with
deeper blue indicating the use of more common-
sense information. As a general trend, we see that
in the earlier hops, words which are near tokens
that occur in both the context and commonsense

Figure 3: Example 1 visualized activation values of ﬁrst attention hop (1 − z1).

Figure 4: Example 1 visualized activation values of second attention hop (1 − z2).

Commonsense Extraction and Visualization Examples

Question

What shore does Michael’s corpse wash up on?

Context

”..as the play begins nora and cathleen receive word from the priest that a
body, that may be their brother michael, has washed up on shore in donegal,
the island farthest north of their home island of inishmaan..”

Answers

the shore of donegal / donegal

Extracted
Commonsense

up → RelatedTo → wind → Antonym → her → RelatedTo → person
up → RelatedTo → north → RelatedTo → up
wash → RelatedTo → up
up → Antonym → down
wash → RelatedTo → water → PartOf → sea → RelatedTo → ﬁsh
up → RelatedTo → wind
wash → RelatedTo → water → PartOf → sea
shore → RelatedTo → sea
wash → RelatedTo → body
wash → Antonym → making
up → Antonym → down → Antonym → up
wash → RelatedTo → water → PartOf → sea → MadeOf → water
up → RelatedTo → wind → Antonym → her
wash → RelatedTo → water
up → RelatedTo → south
shore → RelatedTo → sea → MadeOf → water → AtLocation → bucket → RelatedTo → horse
wash → RelatedTo → clothing
wash → RelatedTo → water → PartOf → sea → MadeOf → water → PartOf → sea
shore → RelatedTo → sea → MadeOf → water
wash → Antonym → getting
up → RelatedTo → north
corpse → RelatedTo → body
shore → RelatedTo → sea → MadeOf → water → AtLocation → fountain
corpse → RelatedTo → body → RelatedTo → corpse
corpse → RelatedTo → body → RelatedTo → water
wash → HasContext → west
up → RelatedTo → wind → Antonym → her → RelatedTo → person → MadeOf → water
up → RelatedTo → wind → AtLocation → sea
wash → RelatedTo → water → AtLocation → can
shore → RelatedTo → sea → MadeOf → water → AtLocation → bucket
wash → RelatedTo → will
shore → RelatedTo → sea → MadeOf → water → AtLocation → fountain → RelatedTo → water

Table 8: Example 1 selected commonsense paths.

Figure 5: Example 1 visualized activation values of third attention hop (1 − z3).

Question

Context

Answers

What species lives in the nearby mines?

”..the nearby mines are inhabited by a race of goblins..”

the goblins / goblins.

Extracted
Commonsense

species → RelatedTo → kingdom → RelatedTo → queen
species → RelatedTo → kingdom → RelatedTo → queen → UsedFor → people → HasA → feet
mines → FormOf → mine
lives → FormOf → life
mines → FormOf → mine → AtLocation → home → RelatedTo → person
species → RelatedTo → kingdom → RelatedTo → queen → UsedFor → people
species → RelatedTo → kingdom → DerivedFrom → king → RelatedTo → master
species → RelatedTo → kingdom → RelatedTo → queen → RelatedTo → person → Desires → feet
mines → FormOf → mine → AtLocation → home → RelatedTo → line → RelatedTo → thread
species → RelatedTo → kingdom → DerivedFrom → king → RelatedTo → leader → AtLocation
→ company
species → RelatedTo → kingdom
species → RelatedTo → kingdom → DerivedFrom → king → RelatedTo → leader
species → RelatedTo → kingdom → DerivedFrom → king
mines → FormOf → mine → AtLocation → home → RelatedTo → line
species → RelatedTo → race
mines → FormOf → mine → AtLocation → home
species → RelatedTo → kingdom → RelatedTo → queen → RelatedTo → person
species → RelatedTo → kingdom → DerivedFrom → king → RelatedTo → master → RelatedTo
→ young

Table 9: Example 2 selected commonsense paths.

Figure 6: Example 2 visualized activation values of ﬁrst attention hop (1 − z1).

Figure 7: Example 2 visualized activation values of second attention hop (1 − z2).

Figure 8: Example 2 visualized activation values of third attention hop (1 − z3).

Question

What duty does ruth have to fulﬁll when her aunt dies?

Context

”..ruth anvoy, a young american woman with a wealthy father, comes to britain to
visit her widowed aunt lady coxon..”
”..having made a promise to her now-deceased husband, lady coxon has for
years been seeking to bestow a sum of 13,000 pounds upon a talented
intellectual whose potential has been hampered by lack of money. having failed to
ﬁnd such a person, lady coxon tells anvoy that upon her death the money will be
left to her, and she must carry on the quest..”
”..anvoy, having lost nearly all her wealth, has only the 13,000 pounds from
lady coxon, with a moral but not legal obligation to give it away..”
”..she awards the coxon fund to saltram, who lives off it exactly as he lived off
his friends, producing nothing of intellectual value..”

Answers

she must give away the 13,000 pounds to an appropriate recipient. /
bestow 13000 to the appropriate person

Extracted
Commonsense

duty → RelatedTo → moral → Antonym → immoral
duty → RelatedTo → time → IsA → money
duty → RelatedTo → time → IsA → money → AtLocation → church
duty → DistinctFrom → off
duty → RelatedTo → time → IsA → money → CapableOf → pay → bills → MotivatedByGoal
→ must
duty → RelatedTo → time → IsA → money → AtLocation → church → RelatedTo → house
duty → RelatedTo → must → RelatedTo → having → RelatedTo → estate → RelatedTo → real
her → RelatedTo → woman → RelatedTo → lady → RelatedTo → plate → Antonym → her
duty → RelatedTo → moral → RelatedTo → will → RelatedTo → choose → IsA → decide
duty → RelatedTo → must → RelatedTo → having → RelatedTo → estate
duty → RelatedTo → obligation
duty → RelatedTo → moral → RelatedTo → will → IsA → purpose
her → RelatedTo → but → DistinctFrom → only → RelatedTo → child → RelatedTo → particularly
her → RelatedTo → person → RelatedTo → others → RelatedTo → people
her → Antonym → him → RelatedTo → he → RelatedTo → person → Desires → conversation
her → RelatedTo → woman → RelatedTo → lady
her → RelatedTo → woman → RelatedTo → she
duty → RelatedTo → must → RelatedTo → having → RelatedTo → own → RelatedTo → having
her → RelatedTo → person → DistinctFrom → man → Antonym → people
her → RelatedTo → but → DistinctFrom → only → RelatedTo → child
her → Antonym → him → RelatedTo → he → RelatedTo → person
her → Antonym → his → RelatedTo → him → RelatedTo → person

Table 10: Example 3 selected commonsense paths.

Figure 9: Example 3 visualized activation values of ﬁrst attention hop (1 − z1).

Figure 10: Example 3 visualized activation values of second attention hop (1 − z2).

Figure 11: Example 3 visualized activation values of third attention hop (1 − z3).

Commonsense for Generative Multi-Hop Question Answering Tasks

Lisa Bauer∗

Yicheng Wang∗
UNC Chapel Hill
{lbauer6, yicheng, mbansal}@cs.unc.edu

Mohit Bansal

9
1
0
2
 
n
u
J
 
1
 
 
]
L
C
.
s
c
[
 
 
3
v
9
0
3
6
0
.
9
0
8
1
:
v
i
X
r
a

Abstract

Reading comprehension QA tasks have seen
a recent surge in popularity, yet most works
have focused on fact-ﬁnding extractive QA.
We instead focus on a more challenging multi-
hop generative task (NarrativeQA), which re-
quires the model to reason, gather, and synthe-
size disjoint pieces of information within the
context to generate an answer. This type of
multi-step reasoning also often requires under-
standing implicit relations, which humans re-
solve via external, background commonsense
knowledge. We ﬁrst present a strong genera-
tive baseline that uses a multi-attention mech-
anism to perform multiple hops of reasoning
and a pointer-generator decoder to synthesize
the answer. This model performs substan-
tially better than previous generative models,
and is competitive with current state-of-the-
art span prediction models. We next intro-
duce a novel system for selecting grounded
multi-hop relational commonsense informa-
tion from ConceptNet via a pointwise mutual
information and term-frequency based scor-
ing function. Finally, we effectively use this
extracted commonsense information to ﬁll in
gaps of reasoning between context hops, using
a selectively-gated attention mechanism. This
boosts the model’s performance signiﬁcantly
(also veriﬁed via human evaluation), establish-
ing a new state-of-the-art for the task. We
also show promising initial results of the gen-
eralizability of our background knowledge en-
hancements by demonstrating some improve-
ment on QAngaroo-WikiHop, another multi-
hop reasoning dataset.

1

Introduction

In this paper, we explore the task of machine
reading comprehension (MRC) based QA. This

∗ Equal contribution (published at EMNLP 2018).
We publicly release all our code, models, and data at:

https://github.com/yicheng-w/CommonSenseMultiHopQA

task tests a model’s natural language understand-
ing capabilities by asking it to answer a question
based on a passage of relevant content. Much
progress has been made in reasoning-based MRC-
QA on the bAbI dataset (Weston et al., 2016),
which contains questions that require the combi-
nation of multiple disjoint pieces of evidence in
the context. However, due to its synthetic nature,
bAbI evidences have smaller lexicons and sim-
pler passage structures when compared to human-
generated text.

There also have been several attempts at the
MRC-QA task on human-generated text. Large
scale datasets such as CNN/DM (Hermann et al.,
2015) and SQuAD (Rajpurkar et al., 2016) have
made the training of end-to-end neural models
possible. However, these datasets are fact-based
and do not place heavy emphasis on multi-hop rea-
soning capabilities. More recent datasets such as
QAngaroo (Welbl et al., 2018) have prompted a
strong focus on multi-hop reasoning in very long
texts. However, QAngaroo is an extractive dataset
where answers are guaranteed to be spans within
the context; hence, this is more focused on fact
ﬁnding and linking, and does not require models
to synthesize and generate new information.

We focus on the recently published Narra-
tiveQA generative dataset (Koˇcisk`y et al., 2018)
that contains questions requiring multi-hop rea-
soning for long, complex stories and other nar-
ratives, which requires the model to go beyond
fact linking and to synthesize non-span answers.
Hence, models that perform well on previous rea-
soning tasks (Dhingra et al., 2018) have had lim-
ited success on this dataset. In this paper, we ﬁrst
propose the Multi-Hop Pointer-Generator Model
(MHPGM), a strong baseline model that uses mul-
tiple hops of bidirectional attention, self-attention,
and a pointer-generator decoder to effectively read
and reason within a long passage and synthesize

a coherent response. Our model achieves 41.49
Rouge-L and 17.33 METEOR on the summary
subtask of NarrativeQA, substantially better than
the performance of previous generative models.

Next,

to address the issue that understand-
ing human-generated text and performing long-
distance reasoning on it often involves intermittent
access to missing hops of external commonsense
(background) knowledge, we present an algorithm
for selecting useful, grounded multi-hop relational
knowledge paths from ConceptNet (Speer and
Havasi, 2012) via a pointwise mutual information
(PMI) and term-frequency-based scoring func-
tion. We then present a novel method of insert-
ing these selected commonsense paths between
the hops of document-context reasoning within
our model, via the Necessary and Optional Infor-
mation Cell (NOIC), which employs a selectively-
gated attention mechanism that utilizes common-
sense information to effectively ﬁll in gaps of in-
ference. With these additions, we further improve
performance on the NarrativeQA dataset, achiev-
ing 44.16 Rouge-L and 19.03 METEOR (also ver-
iﬁed via human evaluation). We also provide man-
ual analysis on the effectiveness of our common-
sense selection algorithm.

Finally,

to show the generalizability of our
multi-hop reasoning and commonsense methods,
we show some promising initial results via the ad-
dition of commonsense information over the base-
line on QAngaroo-WikiHop (Welbl et al., 2018),
an extractive dataset for multi-hop reasoning from
a different domain.

2 Related Work

Machine Reading Comprehension: MRC has
long been a task used to assess a model’s ability
to understand and reason about language. Large
scale datasets such as CNN/Daily Mail
(Her-
mann et al., 2015) and SQuAD (Rajpurkar et al.,
2016) have encouraged the development of many
advanced, high performing attention-based neural
models (Seo et al., 2017; Dhingra et al., 2017).
Concurrently, datasets such as bAbI (Weston et al.,
2016) have focused speciﬁcally on multi-step rea-
soning by requiring the model to reason with
disjoint pieces of information. On this task,
it has been shown that iteratively updating the
query representation with information from the
context can effectively emulate multi-step reason-
ing (Sukhbaatar et al., 2015).

More recently, there has been an increase in
multi-paragraph, multi-hop inference QA datasets
such as QAngaroo (Welbl et al., 2018) and Narra-
tiveQA (Koˇcisk`y et al., 2018). These datasets have
much longer contexts than previous datasets, and
answering a question often requires the synthesis
of multiple discontiguous pieces of evidence.
It
has been shown that models designed for previ-
ous tasks (Seo et al., 2017; Kadlec et al., 2016)
have limited success on these new datasets.
In
our work, we expand upon Gated Attention Net-
work (Dhingra et al., 2017) to create a baseline
model better suited for complex MRC datasets
such as NarrativeQA by improving its attention
and gating mechanisms, expanding its generation
capabilities, and allowing access to external com-
monsense for connecting implicit relations.

Commonsense/Background Knowledge: Com-
monsense or background knowledge has been
tasks including opinion min-
used for several
ing (Cambria et al., 2010), sentiment analy-
sis (Poria et al., 2015, 2016), handwritten text
recognition (Wang et al., 2013), and more re-
cently, dialogue (Young et al., 2018; Ghazvinine-
jad et al., 2018). These approaches add com-
monsense knowledge as relation triples or fea-
tures from external databases. Recently, large-
scale graphical commonsense databases such as
ConceptNet (Speer and Havasi, 2012) use graph-
ical structure to express intricate relations be-
tween concepts, but effective goal-oriented graph
traversal has not been extensively used in previous
commonsense incorporation efforts. Knowledge-
base QA is a task in which systems are asked to
ﬁnd answers to questions by traversing knowledge
graphs (Bollacker et al., 2008). Knowledge path
extraction has been shown to be effective at the
task (Bordes et al., 2014; Bao et al., 2016). We ap-
ply these techniques to MRC-QA by using them to
extract useful commonsense knowledge paths that
fully utilize the graphical nature of databases such
as ConceptNet (Speer and Havasi, 2012).

Incorporation of External Knowledge: There
have been several attempts at using external
knowledge to boost model performance on a vari-
ety of tasks: Chen et al. (2018) showed that adding
lexical information from semantic databases such
as WordNet improves performance on NLI; Xu
et al. (2017) used a gated recall-LSTM mechanism
to incorporate commonsense information into to-
ken representations in dialogue.

In MRC, Weissenborn et al. (2017) integrated
external background knowledge into an NLU
model by using contextually-reﬁned word em-
beddings which integrated information from Con-
ceptNet (single-hop relations mapped to unstruc-
tured text) via a single layer bidirectional LSTM.
Concurrently to our work, Mihaylov and Frank
(2018) showed improvements on a cloze-style task
by incorporating commonsense knowledge via a
context-to-commonsense attention, where com-
monsense relations were extracted as triples. This
work represented commonsense relations as key-
value pairs and combined context representation
and commonsense via a static gate.

Differing from previous works, we employ
multi-hop commonsense paths (multiple con-
nected edges within ConceptNet graph that give
us information beyond a single relationship triple)
to help with our MRC model. Moreover, we use
this in tandem with our multi-hop reasoning archi-
tecture to incorporate different aspects of the com-
monsense relationship path at each hop, in order
to bridge different inference gaps in the multi-hop
QA task. Additionally, our model performs syn-
thesis with its external, background knowledge as
it generates, rather than extracts, its answer.

3 Methods

3.1 Multi-Hop Pointer-Generator Baseline

1, wa

2, . . . , wa

1 , wC
2 , . . . , wQ

the context, X C = {wC
1 , wQ

We ﬁrst rigorously state the problem of genera-
tive QA as follows: given two sequences of input
2 , . . . , wC
n }
tokens:
and the query, X Q = {wQ
m}, the
system should generate a series of answer tokens
X a = {wa
p}. As outlined in previous
sections, an effective generative QA model needs
to be able to perform several hops of reasoning
It would also
over long and complex passages.
need to be able to generate coherent statements to
answer complex questions while having the abil-
ity to copy rare words such as speciﬁc entities
from the reading context. With these in mind, we
propose the Multi-Hop Pointer-Generator Model
(MHPGM) baseline, a novel combination of previ-
ous works with the following major components:

• Embedding Layer: The tokens are embedded
into both learned word embeddings and pre-
trained context-aware embeddings (ELMo (Pe-
ters et al., 2018)).

• Reasoning Layer: The embedded context is
then passed through k reasoning cells, each

of which iteratively updates the context repre-
sentation with information from the query via
BiDAF attention (Seo et al., 2017), emulating a
single reasoning step within the multi-step rea-
soning process.

• Self-Attention Layer: The context representa-
tion is passed through a layer of self-attention
(Cheng et al., 2016) to resolve long-term depen-
dencies and co-reference within the context.

• Pointer-Generator Decoding Layer:

A
attention-pointer-generator decoder (See et al.,
2017) that attends on and potentially copies
from the context is used to create the answer.

The overall model is illustrated in Fig. 1, and

the layers are described in further detail below.
Embedding layer: We embed each word from the
context and question with a learned embedding
space of dimension d. We also obtain context-
aware embeddings for each word via the pre-
trained embedding from language models (ELMo)
(1024 dimensions). The embedded representation
for each word in the context or question, eC
i or
eQ
i ∈ Rd+1024, is the concatenation of its learned
word embedding and ELMo embedding.
Reasoning layer: Our reasoning layer is com-
posed of k reasoning cells (see Fig. 1), where each
incrementally updates the context representation.
The tth reasoning cell’s inputs are the previous
step’s output ({ct−1
}n
i=1) and the embedded ques-
tion ({eQ
i }m
i=1). It ﬁrst creates step-speciﬁc con-
text and query encodings via cell-speciﬁc bidirec-
tional LSTMs:

i

ut = BiLSTM(ct−1);

vt = BiLSTM(eQ)

Then, we use bidirectional attention (Seo et al.,
2017) to emulate a hop of reasoning by focusing
on relevant aspects of the context. Speciﬁcally, we
ﬁrst compute context-to-query attention:

ij = W t
St

1ut

i + W t

2vt

j + W t

3(ut

i (cid:12) vt
j)

pt
ij =

exp(St
ij)
k=1 exp(St

ik)

(cid:80)m

(cq)t

i =

ijvt
pt
j

m
(cid:88)

j=1

2, W t

1, W t

where W t
3 are trainable parameters, and
(cid:12) is elementwise multiplication. We then com-
pute a query-to-context attention vector:

mt

i = max
1≤j≤m

St
ij

Figure 1: Architecture for our Multi-Hop Pointer-Generator Model, and the NOIC commonsense reasoning cell.

pt
i =

exp(mt
i)
j=1 exp(mt
j)

(cid:80)n

qc

t =

iut
pt
i

n
(cid:88)

i=1

We then obtain the updated context representation:

i = [ut
ct

i; (cq)t

i; ut

i (cid:12) (cq)t

i; qc

t (cid:12) (cq)t
i]

where ; is concatenation, ct is the cell’s output.

The initial input of the reasoning layer is the
embedded context representation, i.e., c0 = eC,
and the ﬁnal output of the reasoning layer is the
output of the last cell, ck.
Self-Attention Layer: As the ﬁnal layer before
answer generation, we utilize a residual static self-
attention mechanism (Clark and Gardner, 2018) to
help the model process long contexts with long-
term dependencies. The input of this layer is the
output of the last reasoning cell, ck. We ﬁrst pass
this representation through a fully-connected layer
and then a bi-directional LSTM to obtain another
representation of the context cSA. We obtain the
self attention representation c(cid:48):

ij = W4cSA
SSA

i + W5cSA

j + W6(cSA

i (cid:12) cSA

j

)

pSA
ij =

exp(SSA
ij )
k=1 exp(SSA
ik )

(cid:80)n

n
(cid:88)

c(cid:48)

i =

ij cSA
pSA
j

j=1
where W4, W5, and W6 are trainable parameters.
The output of the self-attention layer is gener-

ated by another layer of bidirectional LSTM.

c(cid:48)(cid:48) = BiLSTM([c(cid:48); cSA; c(cid:48) (cid:12) cSA]

Finally, we add this residually to ck to obtain the
encoded context c = ck + c(cid:48)(cid:48).
Pointer-Generator Decoding Layer: Similar to
the work of See et al. (2017), we use a pointer-
generator model attending on (and potentially
copying from) the context.

At decoding step t, the decoder receives the in-
put xt (embedded representation of last timestep’s
output), the last time step’s hidden state st−1 and
context vector at−1. The decoder computes the
current hidden state st as:

st = LSTM([xt; at−1], st−1)

This hidden state is then used to compute a proba-
bility distribution over the generative vocabulary:

Pgen = softmax(Wgenst + bgen)

We

employ Bahdanau

attention mecha-
nism (Bahdanau et al., 2015) to attend over the
context (c being the output of self-attention layer):
αi = v(cid:124) tanh(Wcci + Wsst + battn)

Dataset

Outside Knowledge Required

WikiHop
NarrativeQA

11%
42%

Table 1: Qualitative analysis of commonsense require-
ments. WikiHop results are from Welbl et al. (2018);
NarrativeQA results are from our manual analysis (on
the validation set).

about a question. We remedy this issue by intro-
ducing grounded commonsense (background) in-
formation using relations between concepts from
ConceptNet (Speer and Havasi, 2012)1 that help
inference by introducing useful connections be-
tween concepts in the context and question.

Due to the size of the semantic network and
the large amount of unnecessary information, we
need an effective way of selecting relations which
provides novel information while being grounded
by the context-query pair. Our commonsense se-
lection strategy is twofold: (1) collect potentially
relevant concepts via a tree construction method
aimed at selecting with high recall candidate rea-
soning paths, and (2) rank and ﬁlter these paths to
ensure both the quality and variety of added infor-
mation via a 3-step scoring strategy (initial node
scoring, cumulative node scoring, and path selec-
tion). We will refer to Fig. 2 as a running example
throughout this section.2

3.2.1 Tree Construction
Given context C and question Q, we want to con-
struct paths grounded in the pair that emulate rea-
soning steps required to answer the question. In
this section, we build ‘prototype’ paths by con-
structing trees rooted in concepts in the query with
the following branching steps3 to emulate multi-
hop reasoning process. For each concept c1 in the
question, we do:
Direct Interaction: In the ﬁrst level, we select re-
lations r1 from ConceptNet that directly link c1
to a concept within the context, c2 ∈ C, e.g., in
Fig. 2, we have lady → church, lady → mother,
lady → person.
Multi-Hop: We then select relations in Concept-
Net r2 that link c2 to another concept in the con-
text, c3 ∈ C. This emulates a potential reason-

1A semantic network where the nodes are individual con-
cepts (words or phrases) and the edges describe directed re-
lations between them (e.g., (cid:104)island, UsedFor, vacation(cid:105)).

2We release all our commonsense extraction code and
the extracted commonsense data at: https://github.com/
yicheng-w/CommonSenseMultiHopQA

3If we are unable to ﬁnd a relation that satisﬁes the condi-

tion, we keep the steps up to and including the node.

Figure 2: Commonsense selection approach.

ˆαi =

exp(αi)
j=1 exp(αj)

(cid:80)n

at =

ˆαici

n
(cid:88)

i=1

We utilize a pointer mechanism that allows the
decoder to directly copy tokens from the context
based on ˆαi. We calculate a selection distribution
psel ∈ R2, where psel
is the probability of gener-
1
ating a token from Pgen and psel
is the probability
2
of copying a word from the context:

o = σ(Waat + Wxxt + Wsst + bptr)

psel = softmax(o)

Our ﬁnal output distribution at timestep t is a
weighted sum of the generative distribution and
the copy distribution:

Pt(w) = psel

1 Pgen(w) + psel
2

(cid:88)

ˆαi

i:wC

i =w

3.2 Commonsense Selection and

Representation

In QA tasks that require multiple hops of reason-
ing, the model often needs knowledge of relations
not directly stated in the context to reach the cor-
rect conclusion. In the datasets we consider, man-
ual analysis shows that external knowledge is fre-
quently needed for inference (see Table 1).

Even with a large amount of training data, it
is very unlikely that a model is able to learn ev-
ery nuanced relation between concepts and ap-
ply the correct ones (as in Fig. 2) when reasoning

ing hop within the context of the MRC task, e.g.,
church → house, mother → daughter, person →
lover.
Outside Knowledge: We then allow an uncon-
strained hop into c3’s neighbors in ConceptNet,
getting to c4 ∈ nbh(c3) via r3 (nbh(v) is the set
of nodes that can be reached from v in one hop).
This emulates the gathering of useful external in-
formation to complete paths within the context,
e.g., house → child, daughter → child.
Context-Grounding: To ensure that the exter-
nal knowledge is indeed helpful to the task, and
also to explicitly link 2nd degree neighbor con-
cepts within the context, we ﬁnish the process by
grounding it again into context by connecting c4
to c5 ∈ C via r4, e.g., child → their.

3.2.2 Rank and Filter

This tree building process collects a large number
of potentially relevant and useful paths. However,
this step also introduces a large amount of noise.
For example, given the question and full context
(not depicted in the ﬁgure) in Fig. 2, we obtain
the path “between → hard → being → cottage →
country” using our tree building method, which is
not relevant to our question. Therefore, to improve
the precision of useful concepts, we rank these
knowledge paths by their relevance and ﬁlter out
noise using the following 3-step scoring method:
Initial Node Scoring: We want to select paths
to the context,
with nodes that are important
in order to provide the most useful common-
sense relations. We approximate importance and
saliency for concepts in the context by their term-
frequency, under the heuristic that important con-
cepts occur more frequently. Thus we score c ∈
{c2, c3, c5} by: score(c) = count(c)/|C|, where
|C| is the context length and count() is the num-
ber of times a concept appears in the context. In
Fig. 2, this ensures that concepts like daughter are
scored highly due to their frequency in the context.
For c4, we use a special scoring function as it is
an unconstrained hop into ConceptNet. We want
c4 to be a logically consistent next step in reason-
ing following the path of c1 to c3, e.g., in Fig. 2, we
see that child is a logically consistent next step af-
ter the partial path of mother → daughter. We ap-
proximate this based on the heuristic that logically
consistent paths occur more frequently. Therefore,
we score this node via Pointwise Mutual Informa-
tion (PMI) between the partial path c1−3 and c4:
PMI(c4, c1−3) = log(P(c4, c1−3)/P(c4)P(c1−3)),

where

P(c4, c1−3) =

# of paths connecting c1, c2, c3, c4
# of distinct paths of length 4

P(c4) =

# of nodes that can reach c4
|ConceptNet|

P(c1−3) =

# of paths connecting c1, c2, c3
# of paths of length 3

Further, it is well known that PMI has high
sensitivity to low-frequency values,
thus we
use normalized PMI (NPMI) (Bouma, 2009):
score(c4) = PMI(c4, c1−3)/(− log P(c4, c1−3)).

Since the branching at each juncture represents
a hop in the multi-hop reasoning process, and hops
at different levels or with different parent nodes do
not ‘compete’ with each other, we normalize each
node’s score against its siblings:

n-score(c) = softmaxsiblings(c)(score(c)).

Cumulative Node Scoring: We want to add com-
monsense paths consisting of multiple hops of
relevant information, thus we re-score each node
based not only on its relevance and saliency but
also that of its tree descendants.

We do this by computing a cumulative node
score from the bottom up, where at the leaf nodes,
we have c-score = n-score, and for cl not a leaf
node, we have c-score(cl) = n-score(cl) + f (cl)
where f of a node is the average of the c-scores of
its top 2 highest scoring children.

For example, given the paths lady → mother →
daughter, lady → mother → married, and lady →
mother → book, we start the cumulative scoring
at the leaf nodes, which in this case are daugh-
ter, married, and book, where daughter and mar-
ried are scored much higher than book due to their
more frequent occurrences. Then, to cumulatively
score mother , we would take the average score of
its two highest scoring children (in this case mar-
ried and daughter) and compound that with the
score of mother itself. Note that the poor scoring
of the irrelevant concept book does not affect the
scoring of mother, which is quite high due to the
concept’s frequent occurrence and the relevance of
its top scoring children.
Path Selection: We select paths in a top-down
breath-ﬁrst fashion in order to add information rel-
evant to different parts of the context. Starting at
the root, we recursively take two of its children
with the highest cumulative scores until we reach
a leaf, selecting up to 24 = 16 paths. For example,

if we were at node mother, this allows us to se-
lect the child node daughter and married over the
child node book. These selected paths, as well as
their partial sub-paths, are what we add as exter-
nal information to the QA model, i.e., we add the
complete path (cid:104)lady, AtLocation, church, Relat-
edTo, house, RelatedTo, child, RelatedTo, their(cid:105),
but also truncated versions of the path, including
(cid:104)lady, AtLocation, church, RelatedTo, house, Re-
latedTo, child(cid:105). We directly give these paths to the
model as sequences of tokens.4

Overall, our sampling strategy provides the
knowledge that a lady can be a mother and that
mother is connected to daughter. This creates
a logical connection between lady and daughter
which helps highlight the importance of our sec-
ond piece of evidence (see Fig. 2). Likewise,
the commonsense information we extracted cre-
ate a similar connection in our third piece of ev-
idence, which states the explicit connection be-
tween daughter and Esther. We also successfully
extract a more story context-centric connection, in
which commonsense provides the knowledge that
a lady is at the location church, which directs to
another piece of evidence in the context. Addition-
ally, this path also encodes a relation between lady
and child, by way of church, which is how lady
and Esther are explicitly connected in the story.

3.3 Commonsense Model Incorporation

i

,

, wCS
2

l } where wCS

Given the list of commonsense logic paths as se-
quences of words: X CS = {wCS
. . . ,
1
wCS
represents the list of tokens
that make up a single path, we ﬁrst embed these
commonsense tokens into the learned embedding
space used by the model, giving us the embedded
ij ∈ Rd. We want to
commonsense tokens, eCS
use these commonsense paths to ﬁll in the gaps
of reasoning between hops of inference. Thus,
we propose Necessary and Optional Information
Cell
(NOIC), a variation of our base reasoning
cell used in the reasoning layer that is capable of
incorporating optional helpful information.

NOIC This cell is an extension to the base rea-
soning cell that allows the model to use common-
sense information to ﬁll in gaps of reasoning. An
example of this is on the bottom left of Fig. 1,
where we see that the cell ﬁrst performs the op-
erations done in the base reasoning cell and then

4In cases where more than one relation can be used to

make a hop, we pick one at random.

adds optional, commonsense information.

At reasoning step t, after obtaining the out-
put of the base reasoning cell, ct, we create a
cell-speciﬁc representation for commonsense in-
formation by concatenating the embedded com-
monsense paths so that each path has a single vec-
tor representation, uCS
. We then project it to the
i
same dimension as ct
i = ReLU(W uCS
i: vCS
i + b)
where W and b are trainable parameters.

We use an attention layer to model the interac-

tion between commonsense and the context:

ij = W CS
SCS

1 ct

i + W CS

2 vCS

j + W CS

3

(ct

i (cid:12) vCS
j

)

pCS
ij =

exp(SCS
ij )
k=1 exp(SCS
ij )

(cid:80)l

cCS
i =

ij vCS
pCS
j

l
(cid:88)

j=1

Finally, we combine this commonsense-aware
context representation with the original ct
i via a
sigmoid gate, since commonsense information is
often not necessary at every step of inference:

zi = σ(Wz[cCS

; ct

i

i] + bz)

(co)t

i + (1 − zi) (cid:12) cCS

i = zi (cid:12) ct
t as the output of the current reasoning
We use co
step instead of ct. As we replace each base rea-
soning cell with NOIC, we selectively incorporate
commonsense at every step of inference.

i

4 Experimental Setup

Datasets: We report results on two multi-hop rea-
soning datasets: generative NarrativeQA (Koˇcisk`y
et al., 2018) (summary subtask) and extractive
QAngaroo WikiHop (Welbl et al., 2018). For
multiple-choice WikiHop, we rank candidate re-
sponses by their generation probability. Similar to
previous works (Dhingra et al., 2018), we use the
non-oracle, unmasked and not-validated dataset.
Evaluation Metrics: We evaluate NarrativeQA
on the metrics proposed by its original authors:
Bleu-1, Bleu-4 (Papineni et al., 2002), ME-
TEOR (Banerjee and Lavie, 2005) and Rouge-
L (Lin, 2004). We also evaluate on CIDEr (Vedan-
tam et al., 2015) which emphasizes annotator con-
sensus. For WikiHop, we evaluate on accuracy.

More dataset, metric, and all other training de-

tails are in the supplementary.

Model

BLEU-1

BLEU-4 METEOR Rouge-L CIDEr

Seq2Seq (Koˇcisk`y et al., 2018)
ASR (Koˇcisk`y et al., 2018)
BiDAF† (Koˇcisk`y et al., 2018)
BiAttn + MRU-LSTM† (Tay et al., 2018)

MHPGM
MHPGM+ NOIC

15.89
23.20
33.72
36.55

40.24
43.63

1.26
6.39
15.53
19.79

17.40
21.07

4.08
7.77
15.38
17.87

17.33
19.03

13.15
22.26
36.30
41.44

41.49
44.16

-
-
-
-

139.23
152.98

Table 2: Results across different metrics on the test set of NarrativeQA-summaries task. † indicates span prediction
models trained on the Rouge-L retrieval oracle.

Model

BiDAF (Welbl et al., 2018)
Coref-GRU (Dhingra et al., 2018)

MHPGM
MHPGM+ NOIC

Dev

42.1
56.0

56.2
58.5

Test

42.9
59.3

57.5
57.9

Table 3: Results of our models on WikiHop dataset,
measured in % accuracy.

5 Results

5.1 Main Experiment

The results of our model on both NarrativeQA and
WikiHop with and without commonsense incorpo-
ration are shown in Table 2 and Table 3. We see
empirically that our model outperforms all gener-
ative models on NarrativeQA, and is competitive
with the top span prediction models. Furthermore,
with the NOIC commonsense integration, we were
able to further improve performance (p < 0.001
on all metrics5), establishing a new state-of-the-art
for the task.

We also see that our model performs rea-
sonably well on WikiHop, and further achieves
promising initial improvements via the addition
of commonsense, hinting at the generalizability
of our approaches. We speculate that the im-
provement is smaller on Wikihop because only
approximately 11% of WikiHop data points re-
quire commonsense and because WikiHop data re-
quires more fact-based commonsense (e.g., from
Freebase (Bollacker et al., 2008)) as opposed to
semantics-based commonsense (e.g., from Con-
ceptNet (Speer and Havasi, 2012)).6

5.2 Model Ablations

We also tested the effectiveness of each compo-
nent of our architecture as well as the effective-

5Stat.

signiﬁcance computed using bootstrap test with
100K iterations (Noreen, 1989; Efron and Tibshirani, 1994).
6All results here are for the standard (non-oracle) un-
masked and not-validated dataset. Welbl et al. (2018) has
reported higher numbers on different data settings which are
not comparable to our results.

ness of adding commonsense information on the
NarrativeQA validation set, with results shown in
Table 4. Experiment 1 and 5 are our models pre-
sented in Table 2. Experiment 2 demonstrates the
importance of multi-hop attention by showing that
if we only allow one hop of attention (even with all
other components of the model, including ELMo
embeddings) the model’s performance decreases
by over 12 Rouge-L points. Experiment 3 and 4
demonstrate the effectiveness of other parts of our
model. We see that ELMo embeddings (Peters
et al., 2018) were also important for the model’s
performance and that self-attention is able to con-
tribute signiﬁcantly to performance on top of other
components of the model. Finally, we see that ef-
fectively introducing external knowledge via our
commonsense selection algorithm and NOIC can
improve performance even further on top of our
strong baseline.

5.3 Commonsense Ablations

We also conducted experiments testing the effec-
tiveness of our commonsense selection and incor-
poration techniques. We ﬁrst tried to naively add
ConceptNet information by initializing the word
embeddings with the ConceptNet-trained embed-
dings, NumberBatch (Speer and Havasi, 2012)
(we also change embedding size from 256 to
300). Then, to verify the effectiveness of our com-
monsense selection and grounding algorithm, we
test our best model on in-domain noise by giv-
ing each context-query pair a set of random rela-
tions grounded in other context-query pairs. This
should teach the model about general common-
sense relations present in the domain of Narra-
tiveQA but does not provide grounding that ﬁlls
in speciﬁc hops of inference. We also experi-
mented with a simpler commonsense extraction
method of using a single hop from the query to
the context. The results of these are shown in
Table 5, where we see that neither NumberBatch
nor random-relationships nor single-hop common-

#

1
2
3
4
5

Ablation

-
k = 1
- ELMo
- Self-Attn
+ NOIC

B-1

42.3
32.5
32.8
37.0
46.0

B-4 M

R

C

18.9
11.7
12.7
16.4
21.9

18.3
12.9
13.6
15.6
20.7

44.9
32.4
33.7
38.6
48.0

151.6
95.7
103.1
125.6
166.6

Table 4: Model ablations on NarrativeQA val-set.

Commonsense

B-1

B-4 M

R

C

None
NumberBatch
Random Rel.
Single Hop
Grounded Rel.

42.3
42.6
43.3
42.1
45.9

18.9
19.6
19.3
19.9
21.9

18.3
18.6
18.6
18.2
20.7

44.9
44.4
45.2
44.0
48.0

151.6
148.1
151.2
148.6
166.6

Table 5: Commonsense ablations on NarrativeQA val-
set.

sense offer statistically signiﬁcant improvements7,
whereas our commonsense selection and incorpo-
ration mechanism improves performance signiﬁ-
cantly across all metrics. We also present several
examples of extracted commonsense and its model
attention visualization in the supplementary.

6 Human Evaluation Analysis

We also conduct human evaluation analysis on
both the quality of the selected commonsense re-
lations, as well as the performance of our ﬁnal
model.
Commonsense Selection: We conducted manual
analysis on a 50 sample subset of the NarrativeQA
test set to check the effectiveness of our common-
sense selection algorithm. Speciﬁcally, given a
context-query pair, as well as the commonsense
selected by our algorithm, we conduct two inde-
pendent evaluations: (1) was any external com-
monsense knowledge necessary for answering the
question?; (2) were the commonsense relations
provided by our algorithm relevant to the ques-
tion? The result for these two evaluations as well
as how they overlap with each other are shown in
Table 6, where we see that 50% of the cases re-
quired external commonsense knowledge, and on
a majority (34%) of those cases our algorithm was
able to select the correct/relevant commonsense
information to ﬁll in gaps of inference. We also
see that in general, our algorithm was able to pro-
vide useful commonsense 48% of the time.
Model Performance: We also conduct human
evaluation to verify that our commonsense incor-
porated model was indeed better than MHPGM.

7The improvement in Rouge-L and METEOR for all three

ablation approaches have p ≥ 0.15 with the bootstrap test.

Commonsense Required
Yes

No

Relevant CS Extracted
Irrelevant CS Extracted

34%
16%

14%
36%

Table 6: NarrativeQA’s commonsense requirements
and effectiveness of commonsense selection algorithm.

MHPGM+NOIC better
MHPGM better
Indistinguishable (Both-good)
Indistinguishable (Both-bad)

23%
15%
41%
21%

Table 7: Human evaluation on the output quality of the
MHPGM+NOIC vs. MHPGM in terms of correctness.

We randomly selected 100 examples from the Nar-
rativeQA test set, along with both models’ pre-
dicted answers, and for each datapoint, we asked
3 external human evaluators (ﬂuent English speak-
ers) to decide (without knowing which model pro-
duced each response) if one is strictly better than
the other, or that they were similar in quality (both-
good or both-bad). As shown in Table 7, we see
that the human evaluation results are in agreement
with that of the automatic evaluation metrics: our
commonsense incorporation has a reasonable im-
pact on the overall correctness of the model. The
inter-annotator agreement had a Fleiss κ = 0.831,
indicating ‘almost-perfect’ agreement between the
annotators (Landis and Koch, 1977).

7 Conclusion

We present an effective reasoning-generative QA
architecture that is a novel combination of previ-
ous work, which uses multiple hops of bidirec-
tional attention and a pointer-generator decoder to
effectively perform multi-hop reasoning and syn-
thesize a coherent and correct answer. Further, we
introduce an algorithm to select grounded, use-
ful paths of commonsense knowledge to ﬁll in
the gaps of inference required for QA, as well a
Necessary and Optional Information Cell (NOIC)
which successfully incorporates this information
during multi-hop reasoning to achieve the new
state-of-the-art on NarrativeQA.

Acknowledgments
We thank the reviewers for their helpful com-
ments. This work was supported by DARPA
(YFA17-D17AP00022), Google Faculty Research
Award, Bloomberg Data Science Research Grant,
and NVidia GPU awards. The views contained in
this article are those of the authors and not of the
funding agency.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR.

Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
In Proceed-
correlation with human judgments.
ings of the ACL Workshop on intrinsic and extrinsic
evaluation measures for machine translation and/or
summarization, pages 65–72.

Junwei Bao, Nan Duan, Zhao Yan, Ming Zhou, and
Tiejun Zhao. 2016. Constraint-based question an-
In Proceedings of
swering with knowledge graph.
COLING 2016, the 26th International Conference
on Computational Linguistics: Technical Papers,
pages 2503–2514.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1247–1250. AcM.

Antoine Bordes, Sumit Chopra, and Jason Weston.
2014. Question answering with subgraph embed-
In Proceedings of the 2014 Conference on
dings.
Empirical Methods in Natural Language Processing
(EMNLP), pages 615–620.

Gerlof Bouma. 2009. Normalized (pointwise) mutual
information in collocation extraction. Proceedings
of GSCL, pages 31–40.

Erik Cambria, Amir Hussain, Tariq Durrani, Catherine
Havasi, Chris Eckl, and James Munro. 2010. Sen-
tic computing for patient centered applications. In
Signal Processing (ICSP), 2010 IEEE 10th Interna-
tional Conference on, pages 1279–1282. IEEE.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Diana
Inkpen, and Si Wei. 2018. Neural natural language
inference models enhanced with external knowl-
edge. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 2406–2417. Associa-
tion for Computational Linguistics.

Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016.
Long short-term memory-networks for machine
reading. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Process-
ing, pages 551–561.

Christopher Clark and Matt Gardner. 2018. Simple
and effective multi-paragraph reading comprehen-
sion. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Long
Papers), pages 845–855.

Bhuwan Dhingra, Qiao Jin, Zhilin Yang, William Co-
hen, and Ruslan Salakhutdinov. 2018. Neural mod-
els for reasoning over multiple mentions using coref-
In Proceedings of the 2018 Conference of
erence.

the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers), volume 2, pages
42–48.

Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang, William
Cohen, and Ruslan Salakhutdinov. 2017. Gated-
In Pro-
attention readers for text comprehension.
ceedings of the 55th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), volume 1, pages 1832–1846.

Bradley Efron and Robert J Tibshirani. 1994. An intro-

duction to the bootstrap. CRC press.

Marjan Ghazvininejad, Chris Brockett, Ming-Wei
Chang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, and
Michel Galley. 2018. A knowledge-grounded neural
conversation model. In AAAI.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems, pages 1693–
1701.

Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and Jan
Kleindienst. 2016. Text understanding with the at-
tention sum reader network. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics, pages 908–918.

Diederik P Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In 3rd Interna-
tional Conference for Learning Representations.

Tom´aˇs Koˇcisk`y, Jonathan Schwarz, Phil Blunsom,
Chris Dyer, Karl Moritz Hermann, G´aabor Melis,
and Edward Grefenstette. 2018. The narrativeqa
Transactions
reading comprehension challenge.
of the Association of Computational Linguistics,
6:317–328.

J Richard Landis and Gary G Koch. 1977. The mea-
surement of observer agreement for categorical data.
biometrics, pages 159–174.

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. Text Summarization
Branches Out.

Todor Mihaylov and Anette Frank. 2018. Knowledge-
able reader: enhancing cloze-style reading compre-
hension with external commonsense knowledge. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 821–832. Association for Com-
putational Linguistics.

Eric W Noreen. 1989. Computer-intensive methods for

testing hypotheses. Wiley New York.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the

40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311–318. Association for
Computational Linguistics.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of NAACL.

Soujanya Poria, Erik Cambria, Alexander Gelbukh,
Federica Bisio, and Amir Hussain. 2015. Sentiment
data ﬂow analysis by means of dynamic linguistic
IEEE Computational Intelligence Maga-
patterns.
zine, 10(4):26–36.

Soujanya Poria, Iti Chaturvedi, Erik Cambria, and Fed-
erica Bisio. 2016. Sentic LDA: Improving on LDA
with semantic similarity for aspect-based sentiment
In Neural Networks (IJCNN), 2016 In-
analysis.
ternational Joint Conference on, pages 4465–4473.
IEEE.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, pages 2383–2392.

Abigail See, Peter J Liu, and Christopher D Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), volume 1,
pages 1073–1083.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2017. Bidirectional attention
ﬂow for machine comprehension. In ICLR.

Robyn Speer and Catherine Havasi. 2012. Represent-
ing general relational knowledge in ConceptNet 5.
In LREC, pages 3679–3686.

Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.
2015. End-to-end memory networks. In Advances
in Neural Information Processing Systems, pages
2440–2448.

Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2018.
Multi-range reasoning for machine comprehension.
arXiv preprint arXiv:1803.09074.

Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. 2015. Cider: Consensus-based image de-
In Proceedings of the IEEE
scription evaluation.
Conference on Computer Vision and Pattern Recog-
nition, pages 4566–4575.

Qiu-Feng Wang, Erik Cambria, Cheng-Lin Liu, and
Amir Hussain. 2013. Common sense knowledge
for handwritten chinese text recognition. Cognitive
Computation, 5(2):234–242.

Dirk Weissenborn, Tom´aˇs Koˇcisk`y, and Chris Dyer.
2017. Dynamic integration of background knowl-
arXiv preprint
edge in neural NLU systems.
arXiv:1706.02596.

Johannes Welbl, Pontus Stenetorp, and Sebastian
Riedel. 2018. Constructing datasets for multi-hop
reading comprehension across documents. Transac-
tions of the Association of Computational Linguis-
tics, 6:287–302.

Jason Weston, Antoine Bordes, Sumit Chopra, Alexan-
der M Rush, Bart van Merri¨enboer, Armand Joulin,
and Tomas Mikolov. 2016. Towards ai-complete
question answering: A set of prerequisite toy tasks.
In ICLR.

Zhen Xu, Bingquan Liu, Baoxun Wang, Chengjie Sun,
and Xiaolong Wang. 2017.
Incorporating loose-
structured knowledge into LSTM with recall gate for
conversation modeling. In International Joint Con-
ference on Neural Networks.

Tom Young, Erik Cambria, Iti Chaturvedi, Minlie
Huang, Hao Zhou, and Subham Biswas. 2018. Aug-
menting end-to-end dialog systems with common-
In Proceedings of the Thirty-
sense knowledge.
Second AAAI Conference on Artiﬁcial Intelligence
(AAAI-18).

A Supplemental Material

A.1 Experimental Setup

Datasets We test our model with and with-
out commonsense addition on two challenging
datasets that require multi-hop reasoning and ex-
ternal knowledge: NarrativeQA (Koˇcisk`y et al.,
2018) and QAngaroo-WikiHop (Welbl et al.,
2018). NarrativeQA is a generative QA dataset
where the passages are either stories or summaries
of stories, and the questions ask about complex
aspects of the narratives such as event timelines,
characters, relations between characters, etc. Each
question has two answers which are generated by
human annotators and usually cannot be found
in the passage directly. We focus on the sum-
mary subtask in this paper, where summaries have
lengths of up to 1000 words.

We also test our model on WikiHop, a fact
based, multi-hop dataset. Questions in WikiHop
often require a model to read several documents
in order to obtain an answer. We focus on the
multiple-choice part of WikiHop, where models
are tasked with picking the correct response from
a pool of candidates. We rank candidate responses
by calculating their generation probability based
on our model. As this is a multi-document QA
task, we ﬁrst rank the candidate documents via TF-
IDF cosine distance with the question, and then
take the top k documents such that their combined
length is less than 1300 words.

paths have high activation, but the activation be-
comes more focused on the passage’s key words
w.r.t. the question, as the number of hops increase.

Evaluation Metrics We evaluate NarrativeQA
on the metrics proposed by its original authors:
Bleu-1, Bleu-4 (Papineni et al., 2002), ME-
TEOR (Banerjee and Lavie, 2005) and Rouge-
L (Lin, 2004). We also evaluate on CIDEr (Vedan-
tam et al., 2015) as it places emphasize on annota-
tor consensus. For WikiHop, we evaluate on accu-
racy.

Training Details
In training for both datasets,
we minimize the negative log probability of gener-
ating the ground-truth answer with the Adam opti-
mizer (Kingma and Ba, 2015) with an initial learn-
ing rate of 0.001, a dropout-rate of 0.2 (dropout is
applied to the input of each RNN layer) and batch
size of 24. We use 256 dimensional word embed-
dings and a hidden size of 128 for all RNNs and
k = 3 hops of multi attention. At inference time
we use greedy decoding to generate the answer.
For both NarrativeQA and WikiHop, we reached
these parameters via tuning on the full, ofﬁcial val-
idation set.

A.2 Commonsense Extraction Examples

In Tables 8, 9, and 10 (see next page), we demon-
strate extracted commonsense examples for ques-
tions that require commonsense to reach an an-
swer. We bold words in the question and in the ex-
tracted commonsense in cases where the common-
sense knowledge explicitly bridges gaps between
implicitly connected words in the context or ques-
tion. The relevant context is also displayed, with
context words that are key to answering the ques-
tion (via commonsense) marked in bold. These are
then followed by a context visualization described
in the next section.

A.3 Commonsense Integration Visualization

We also visualize how much commonsense infor-
mation is integrated into each part of the context
by providing a visualization of the zi value (see
end of Sec. 3.3 of main ﬁle) for i ∈ {1, 2, 3},
which is the gate value signifying how much
commonsense-attention representation is used in
the output context representation. In the follow-
ing examples (next page), we use shades of blue
to represent the average of (1 − zi) at each word
in the context (normalized within each hop), with
deeper blue indicating the use of more common-
sense information. As a general trend, we see that
in the earlier hops, words which are near tokens
that occur in both the context and commonsense

Figure 3: Example 1 visualized activation values of ﬁrst attention hop (1 − z1).

Figure 4: Example 1 visualized activation values of second attention hop (1 − z2).

Commonsense Extraction and Visualization Examples

Question

What shore does Michael’s corpse wash up on?

Context

”..as the play begins nora and cathleen receive word from the priest that a
body, that may be their brother michael, has washed up on shore in donegal,
the island farthest north of their home island of inishmaan..”

Answers

the shore of donegal / donegal

Extracted
Commonsense

up → RelatedTo → wind → Antonym → her → RelatedTo → person
up → RelatedTo → north → RelatedTo → up
wash → RelatedTo → up
up → Antonym → down
wash → RelatedTo → water → PartOf → sea → RelatedTo → ﬁsh
up → RelatedTo → wind
wash → RelatedTo → water → PartOf → sea
shore → RelatedTo → sea
wash → RelatedTo → body
wash → Antonym → making
up → Antonym → down → Antonym → up
wash → RelatedTo → water → PartOf → sea → MadeOf → water
up → RelatedTo → wind → Antonym → her
wash → RelatedTo → water
up → RelatedTo → south
shore → RelatedTo → sea → MadeOf → water → AtLocation → bucket → RelatedTo → horse
wash → RelatedTo → clothing
wash → RelatedTo → water → PartOf → sea → MadeOf → water → PartOf → sea
shore → RelatedTo → sea → MadeOf → water
wash → Antonym → getting
up → RelatedTo → north
corpse → RelatedTo → body
shore → RelatedTo → sea → MadeOf → water → AtLocation → fountain
corpse → RelatedTo → body → RelatedTo → corpse
corpse → RelatedTo → body → RelatedTo → water
wash → HasContext → west
up → RelatedTo → wind → Antonym → her → RelatedTo → person → MadeOf → water
up → RelatedTo → wind → AtLocation → sea
wash → RelatedTo → water → AtLocation → can
shore → RelatedTo → sea → MadeOf → water → AtLocation → bucket
wash → RelatedTo → will
shore → RelatedTo → sea → MadeOf → water → AtLocation → fountain → RelatedTo → water

Table 8: Example 1 selected commonsense paths.

Figure 5: Example 1 visualized activation values of third attention hop (1 − z3).

Question

Context

Answers

What species lives in the nearby mines?

”..the nearby mines are inhabited by a race of goblins..”

the goblins / goblins.

Extracted
Commonsense

species → RelatedTo → kingdom → RelatedTo → queen
species → RelatedTo → kingdom → RelatedTo → queen → UsedFor → people → HasA → feet
mines → FormOf → mine
lives → FormOf → life
mines → FormOf → mine → AtLocation → home → RelatedTo → person
species → RelatedTo → kingdom → RelatedTo → queen → UsedFor → people
species → RelatedTo → kingdom → DerivedFrom → king → RelatedTo → master
species → RelatedTo → kingdom → RelatedTo → queen → RelatedTo → person → Desires → feet
mines → FormOf → mine → AtLocation → home → RelatedTo → line → RelatedTo → thread
species → RelatedTo → kingdom → DerivedFrom → king → RelatedTo → leader → AtLocation
→ company
species → RelatedTo → kingdom
species → RelatedTo → kingdom → DerivedFrom → king → RelatedTo → leader
species → RelatedTo → kingdom → DerivedFrom → king
mines → FormOf → mine → AtLocation → home → RelatedTo → line
species → RelatedTo → race
mines → FormOf → mine → AtLocation → home
species → RelatedTo → kingdom → RelatedTo → queen → RelatedTo → person
species → RelatedTo → kingdom → DerivedFrom → king → RelatedTo → master → RelatedTo
→ young

Table 9: Example 2 selected commonsense paths.

Figure 6: Example 2 visualized activation values of ﬁrst attention hop (1 − z1).

Figure 7: Example 2 visualized activation values of second attention hop (1 − z2).

Figure 8: Example 2 visualized activation values of third attention hop (1 − z3).

Question

What duty does ruth have to fulﬁll when her aunt dies?

Context

”..ruth anvoy, a young american woman with a wealthy father, comes to britain to
visit her widowed aunt lady coxon..”
”..having made a promise to her now-deceased husband, lady coxon has for
years been seeking to bestow a sum of 13,000 pounds upon a talented
intellectual whose potential has been hampered by lack of money. having failed to
ﬁnd such a person, lady coxon tells anvoy that upon her death the money will be
left to her, and she must carry on the quest..”
”..anvoy, having lost nearly all her wealth, has only the 13,000 pounds from
lady coxon, with a moral but not legal obligation to give it away..”
”..she awards the coxon fund to saltram, who lives off it exactly as he lived off
his friends, producing nothing of intellectual value..”

Answers

she must give away the 13,000 pounds to an appropriate recipient. /
bestow 13000 to the appropriate person

Extracted
Commonsense

duty → RelatedTo → moral → Antonym → immoral
duty → RelatedTo → time → IsA → money
duty → RelatedTo → time → IsA → money → AtLocation → church
duty → DistinctFrom → off
duty → RelatedTo → time → IsA → money → CapableOf → pay → bills → MotivatedByGoal
→ must
duty → RelatedTo → time → IsA → money → AtLocation → church → RelatedTo → house
duty → RelatedTo → must → RelatedTo → having → RelatedTo → estate → RelatedTo → real
her → RelatedTo → woman → RelatedTo → lady → RelatedTo → plate → Antonym → her
duty → RelatedTo → moral → RelatedTo → will → RelatedTo → choose → IsA → decide
duty → RelatedTo → must → RelatedTo → having → RelatedTo → estate
duty → RelatedTo → obligation
duty → RelatedTo → moral → RelatedTo → will → IsA → purpose
her → RelatedTo → but → DistinctFrom → only → RelatedTo → child → RelatedTo → particularly
her → RelatedTo → person → RelatedTo → others → RelatedTo → people
her → Antonym → him → RelatedTo → he → RelatedTo → person → Desires → conversation
her → RelatedTo → woman → RelatedTo → lady
her → RelatedTo → woman → RelatedTo → she
duty → RelatedTo → must → RelatedTo → having → RelatedTo → own → RelatedTo → having
her → RelatedTo → person → DistinctFrom → man → Antonym → people
her → RelatedTo → but → DistinctFrom → only → RelatedTo → child
her → Antonym → him → RelatedTo → he → RelatedTo → person
her → Antonym → his → RelatedTo → him → RelatedTo → person

Table 10: Example 3 selected commonsense paths.

Figure 9: Example 3 visualized activation values of ﬁrst attention hop (1 − z1).

Figure 10: Example 3 visualized activation values of second attention hop (1 − z2).

Figure 11: Example 3 visualized activation values of third attention hop (1 − z3).

Commonsense for Generative Multi-Hop Question Answering Tasks

Lisa Bauer∗

Yicheng Wang∗
UNC Chapel Hill
{lbauer6, yicheng, mbansal}@cs.unc.edu

Mohit Bansal

9
1
0
2
 
n
u
J
 
1
 
 
]
L
C
.
s
c
[
 
 
3
v
9
0
3
6
0
.
9
0
8
1
:
v
i
X
r
a

Abstract

Reading comprehension QA tasks have seen
a recent surge in popularity, yet most works
have focused on fact-ﬁnding extractive QA.
We instead focus on a more challenging multi-
hop generative task (NarrativeQA), which re-
quires the model to reason, gather, and synthe-
size disjoint pieces of information within the
context to generate an answer. This type of
multi-step reasoning also often requires under-
standing implicit relations, which humans re-
solve via external, background commonsense
knowledge. We ﬁrst present a strong genera-
tive baseline that uses a multi-attention mech-
anism to perform multiple hops of reasoning
and a pointer-generator decoder to synthesize
the answer. This model performs substan-
tially better than previous generative models,
and is competitive with current state-of-the-
art span prediction models. We next intro-
duce a novel system for selecting grounded
multi-hop relational commonsense informa-
tion from ConceptNet via a pointwise mutual
information and term-frequency based scor-
ing function. Finally, we effectively use this
extracted commonsense information to ﬁll in
gaps of reasoning between context hops, using
a selectively-gated attention mechanism. This
boosts the model’s performance signiﬁcantly
(also veriﬁed via human evaluation), establish-
ing a new state-of-the-art for the task. We
also show promising initial results of the gen-
eralizability of our background knowledge en-
hancements by demonstrating some improve-
ment on QAngaroo-WikiHop, another multi-
hop reasoning dataset.

1

Introduction

In this paper, we explore the task of machine
reading comprehension (MRC) based QA. This

∗ Equal contribution (published at EMNLP 2018).
We publicly release all our code, models, and data at:

https://github.com/yicheng-w/CommonSenseMultiHopQA

task tests a model’s natural language understand-
ing capabilities by asking it to answer a question
based on a passage of relevant content. Much
progress has been made in reasoning-based MRC-
QA on the bAbI dataset (Weston et al., 2016),
which contains questions that require the combi-
nation of multiple disjoint pieces of evidence in
the context. However, due to its synthetic nature,
bAbI evidences have smaller lexicons and sim-
pler passage structures when compared to human-
generated text.

There also have been several attempts at the
MRC-QA task on human-generated text. Large
scale datasets such as CNN/DM (Hermann et al.,
2015) and SQuAD (Rajpurkar et al., 2016) have
made the training of end-to-end neural models
possible. However, these datasets are fact-based
and do not place heavy emphasis on multi-hop rea-
soning capabilities. More recent datasets such as
QAngaroo (Welbl et al., 2018) have prompted a
strong focus on multi-hop reasoning in very long
texts. However, QAngaroo is an extractive dataset
where answers are guaranteed to be spans within
the context; hence, this is more focused on fact
ﬁnding and linking, and does not require models
to synthesize and generate new information.

We focus on the recently published Narra-
tiveQA generative dataset (Koˇcisk`y et al., 2018)
that contains questions requiring multi-hop rea-
soning for long, complex stories and other nar-
ratives, which requires the model to go beyond
fact linking and to synthesize non-span answers.
Hence, models that perform well on previous rea-
soning tasks (Dhingra et al., 2018) have had lim-
ited success on this dataset. In this paper, we ﬁrst
propose the Multi-Hop Pointer-Generator Model
(MHPGM), a strong baseline model that uses mul-
tiple hops of bidirectional attention, self-attention,
and a pointer-generator decoder to effectively read
and reason within a long passage and synthesize

a coherent response. Our model achieves 41.49
Rouge-L and 17.33 METEOR on the summary
subtask of NarrativeQA, substantially better than
the performance of previous generative models.

Next,

to address the issue that understand-
ing human-generated text and performing long-
distance reasoning on it often involves intermittent
access to missing hops of external commonsense
(background) knowledge, we present an algorithm
for selecting useful, grounded multi-hop relational
knowledge paths from ConceptNet (Speer and
Havasi, 2012) via a pointwise mutual information
(PMI) and term-frequency-based scoring func-
tion. We then present a novel method of insert-
ing these selected commonsense paths between
the hops of document-context reasoning within
our model, via the Necessary and Optional Infor-
mation Cell (NOIC), which employs a selectively-
gated attention mechanism that utilizes common-
sense information to effectively ﬁll in gaps of in-
ference. With these additions, we further improve
performance on the NarrativeQA dataset, achiev-
ing 44.16 Rouge-L and 19.03 METEOR (also ver-
iﬁed via human evaluation). We also provide man-
ual analysis on the effectiveness of our common-
sense selection algorithm.

Finally,

to show the generalizability of our
multi-hop reasoning and commonsense methods,
we show some promising initial results via the ad-
dition of commonsense information over the base-
line on QAngaroo-WikiHop (Welbl et al., 2018),
an extractive dataset for multi-hop reasoning from
a different domain.

2 Related Work

Machine Reading Comprehension: MRC has
long been a task used to assess a model’s ability
to understand and reason about language. Large
scale datasets such as CNN/Daily Mail
(Her-
mann et al., 2015) and SQuAD (Rajpurkar et al.,
2016) have encouraged the development of many
advanced, high performing attention-based neural
models (Seo et al., 2017; Dhingra et al., 2017).
Concurrently, datasets such as bAbI (Weston et al.,
2016) have focused speciﬁcally on multi-step rea-
soning by requiring the model to reason with
disjoint pieces of information. On this task,
it has been shown that iteratively updating the
query representation with information from the
context can effectively emulate multi-step reason-
ing (Sukhbaatar et al., 2015).

More recently, there has been an increase in
multi-paragraph, multi-hop inference QA datasets
such as QAngaroo (Welbl et al., 2018) and Narra-
tiveQA (Koˇcisk`y et al., 2018). These datasets have
much longer contexts than previous datasets, and
answering a question often requires the synthesis
of multiple discontiguous pieces of evidence.
It
has been shown that models designed for previ-
ous tasks (Seo et al., 2017; Kadlec et al., 2016)
have limited success on these new datasets.
In
our work, we expand upon Gated Attention Net-
work (Dhingra et al., 2017) to create a baseline
model better suited for complex MRC datasets
such as NarrativeQA by improving its attention
and gating mechanisms, expanding its generation
capabilities, and allowing access to external com-
monsense for connecting implicit relations.

Commonsense/Background Knowledge: Com-
monsense or background knowledge has been
tasks including opinion min-
used for several
ing (Cambria et al., 2010), sentiment analy-
sis (Poria et al., 2015, 2016), handwritten text
recognition (Wang et al., 2013), and more re-
cently, dialogue (Young et al., 2018; Ghazvinine-
jad et al., 2018). These approaches add com-
monsense knowledge as relation triples or fea-
tures from external databases. Recently, large-
scale graphical commonsense databases such as
ConceptNet (Speer and Havasi, 2012) use graph-
ical structure to express intricate relations be-
tween concepts, but effective goal-oriented graph
traversal has not been extensively used in previous
commonsense incorporation efforts. Knowledge-
base QA is a task in which systems are asked to
ﬁnd answers to questions by traversing knowledge
graphs (Bollacker et al., 2008). Knowledge path
extraction has been shown to be effective at the
task (Bordes et al., 2014; Bao et al., 2016). We ap-
ply these techniques to MRC-QA by using them to
extract useful commonsense knowledge paths that
fully utilize the graphical nature of databases such
as ConceptNet (Speer and Havasi, 2012).

Incorporation of External Knowledge: There
have been several attempts at using external
knowledge to boost model performance on a vari-
ety of tasks: Chen et al. (2018) showed that adding
lexical information from semantic databases such
as WordNet improves performance on NLI; Xu
et al. (2017) used a gated recall-LSTM mechanism
to incorporate commonsense information into to-
ken representations in dialogue.

In MRC, Weissenborn et al. (2017) integrated
external background knowledge into an NLU
model by using contextually-reﬁned word em-
beddings which integrated information from Con-
ceptNet (single-hop relations mapped to unstruc-
tured text) via a single layer bidirectional LSTM.
Concurrently to our work, Mihaylov and Frank
(2018) showed improvements on a cloze-style task
by incorporating commonsense knowledge via a
context-to-commonsense attention, where com-
monsense relations were extracted as triples. This
work represented commonsense relations as key-
value pairs and combined context representation
and commonsense via a static gate.

Differing from previous works, we employ
multi-hop commonsense paths (multiple con-
nected edges within ConceptNet graph that give
us information beyond a single relationship triple)
to help with our MRC model. Moreover, we use
this in tandem with our multi-hop reasoning archi-
tecture to incorporate different aspects of the com-
monsense relationship path at each hop, in order
to bridge different inference gaps in the multi-hop
QA task. Additionally, our model performs syn-
thesis with its external, background knowledge as
it generates, rather than extracts, its answer.

3 Methods

3.1 Multi-Hop Pointer-Generator Baseline

1, wa

2, . . . , wa

1 , wC
2 , . . . , wQ

the context, X C = {wC
1 , wQ

We ﬁrst rigorously state the problem of genera-
tive QA as follows: given two sequences of input
2 , . . . , wC
n }
tokens:
and the query, X Q = {wQ
m}, the
system should generate a series of answer tokens
X a = {wa
p}. As outlined in previous
sections, an effective generative QA model needs
to be able to perform several hops of reasoning
It would also
over long and complex passages.
need to be able to generate coherent statements to
answer complex questions while having the abil-
ity to copy rare words such as speciﬁc entities
from the reading context. With these in mind, we
propose the Multi-Hop Pointer-Generator Model
(MHPGM) baseline, a novel combination of previ-
ous works with the following major components:

• Embedding Layer: The tokens are embedded
into both learned word embeddings and pre-
trained context-aware embeddings (ELMo (Pe-
ters et al., 2018)).

• Reasoning Layer: The embedded context is
then passed through k reasoning cells, each

of which iteratively updates the context repre-
sentation with information from the query via
BiDAF attention (Seo et al., 2017), emulating a
single reasoning step within the multi-step rea-
soning process.

• Self-Attention Layer: The context representa-
tion is passed through a layer of self-attention
(Cheng et al., 2016) to resolve long-term depen-
dencies and co-reference within the context.

• Pointer-Generator Decoding Layer:

A
attention-pointer-generator decoder (See et al.,
2017) that attends on and potentially copies
from the context is used to create the answer.

The overall model is illustrated in Fig. 1, and

the layers are described in further detail below.
Embedding layer: We embed each word from the
context and question with a learned embedding
space of dimension d. We also obtain context-
aware embeddings for each word via the pre-
trained embedding from language models (ELMo)
(1024 dimensions). The embedded representation
for each word in the context or question, eC
i or
eQ
i ∈ Rd+1024, is the concatenation of its learned
word embedding and ELMo embedding.
Reasoning layer: Our reasoning layer is com-
posed of k reasoning cells (see Fig. 1), where each
incrementally updates the context representation.
The tth reasoning cell’s inputs are the previous
step’s output ({ct−1
}n
i=1) and the embedded ques-
tion ({eQ
i }m
i=1). It ﬁrst creates step-speciﬁc con-
text and query encodings via cell-speciﬁc bidirec-
tional LSTMs:

i

ut = BiLSTM(ct−1);

vt = BiLSTM(eQ)

Then, we use bidirectional attention (Seo et al.,
2017) to emulate a hop of reasoning by focusing
on relevant aspects of the context. Speciﬁcally, we
ﬁrst compute context-to-query attention:

ij = W t
St

1ut

i + W t

2vt

j + W t

3(ut

i (cid:12) vt
j)

pt
ij =

exp(St
ij)
k=1 exp(St

ik)

(cid:80)m

(cq)t

i =

ijvt
pt
j

m
(cid:88)

j=1

2, W t

1, W t

where W t
3 are trainable parameters, and
(cid:12) is elementwise multiplication. We then com-
pute a query-to-context attention vector:

mt

i = max
1≤j≤m

St
ij

Figure 1: Architecture for our Multi-Hop Pointer-Generator Model, and the NOIC commonsense reasoning cell.

pt
i =

exp(mt
i)
j=1 exp(mt
j)

(cid:80)n

qc

t =

iut
pt
i

n
(cid:88)

i=1

We then obtain the updated context representation:

i = [ut
ct

i; (cq)t

i; ut

i (cid:12) (cq)t

i; qc

t (cid:12) (cq)t
i]

where ; is concatenation, ct is the cell’s output.

The initial input of the reasoning layer is the
embedded context representation, i.e., c0 = eC,
and the ﬁnal output of the reasoning layer is the
output of the last cell, ck.
Self-Attention Layer: As the ﬁnal layer before
answer generation, we utilize a residual static self-
attention mechanism (Clark and Gardner, 2018) to
help the model process long contexts with long-
term dependencies. The input of this layer is the
output of the last reasoning cell, ck. We ﬁrst pass
this representation through a fully-connected layer
and then a bi-directional LSTM to obtain another
representation of the context cSA. We obtain the
self attention representation c(cid:48):

ij = W4cSA
SSA

i + W5cSA

j + W6(cSA

i (cid:12) cSA

j

)

pSA
ij =

exp(SSA
ij )
k=1 exp(SSA
ik )

(cid:80)n

n
(cid:88)

c(cid:48)

i =

ij cSA
pSA
j

j=1
where W4, W5, and W6 are trainable parameters.
The output of the self-attention layer is gener-

ated by another layer of bidirectional LSTM.

c(cid:48)(cid:48) = BiLSTM([c(cid:48); cSA; c(cid:48) (cid:12) cSA]

Finally, we add this residually to ck to obtain the
encoded context c = ck + c(cid:48)(cid:48).
Pointer-Generator Decoding Layer: Similar to
the work of See et al. (2017), we use a pointer-
generator model attending on (and potentially
copying from) the context.

At decoding step t, the decoder receives the in-
put xt (embedded representation of last timestep’s
output), the last time step’s hidden state st−1 and
context vector at−1. The decoder computes the
current hidden state st as:

st = LSTM([xt; at−1], st−1)

This hidden state is then used to compute a proba-
bility distribution over the generative vocabulary:

Pgen = softmax(Wgenst + bgen)

We

employ Bahdanau

attention mecha-
nism (Bahdanau et al., 2015) to attend over the
context (c being the output of self-attention layer):
αi = v(cid:124) tanh(Wcci + Wsst + battn)

Dataset

Outside Knowledge Required

WikiHop
NarrativeQA

11%
42%

Table 1: Qualitative analysis of commonsense require-
ments. WikiHop results are from Welbl et al. (2018);
NarrativeQA results are from our manual analysis (on
the validation set).

about a question. We remedy this issue by intro-
ducing grounded commonsense (background) in-
formation using relations between concepts from
ConceptNet (Speer and Havasi, 2012)1 that help
inference by introducing useful connections be-
tween concepts in the context and question.

Due to the size of the semantic network and
the large amount of unnecessary information, we
need an effective way of selecting relations which
provides novel information while being grounded
by the context-query pair. Our commonsense se-
lection strategy is twofold: (1) collect potentially
relevant concepts via a tree construction method
aimed at selecting with high recall candidate rea-
soning paths, and (2) rank and ﬁlter these paths to
ensure both the quality and variety of added infor-
mation via a 3-step scoring strategy (initial node
scoring, cumulative node scoring, and path selec-
tion). We will refer to Fig. 2 as a running example
throughout this section.2

3.2.1 Tree Construction
Given context C and question Q, we want to con-
struct paths grounded in the pair that emulate rea-
soning steps required to answer the question. In
this section, we build ‘prototype’ paths by con-
structing trees rooted in concepts in the query with
the following branching steps3 to emulate multi-
hop reasoning process. For each concept c1 in the
question, we do:
Direct Interaction: In the ﬁrst level, we select re-
lations r1 from ConceptNet that directly link c1
to a concept within the context, c2 ∈ C, e.g., in
Fig. 2, we have lady → church, lady → mother,
lady → person.
Multi-Hop: We then select relations in Concept-
Net r2 that link c2 to another concept in the con-
text, c3 ∈ C. This emulates a potential reason-

1A semantic network where the nodes are individual con-
cepts (words or phrases) and the edges describe directed re-
lations between them (e.g., (cid:104)island, UsedFor, vacation(cid:105)).

2We release all our commonsense extraction code and
the extracted commonsense data at: https://github.com/
yicheng-w/CommonSenseMultiHopQA

3If we are unable to ﬁnd a relation that satisﬁes the condi-

tion, we keep the steps up to and including the node.

Figure 2: Commonsense selection approach.

ˆαi =

exp(αi)
j=1 exp(αj)

(cid:80)n

at =

ˆαici

n
(cid:88)

i=1

We utilize a pointer mechanism that allows the
decoder to directly copy tokens from the context
based on ˆαi. We calculate a selection distribution
psel ∈ R2, where psel
is the probability of gener-
1
ating a token from Pgen and psel
is the probability
2
of copying a word from the context:

o = σ(Waat + Wxxt + Wsst + bptr)

psel = softmax(o)

Our ﬁnal output distribution at timestep t is a
weighted sum of the generative distribution and
the copy distribution:

Pt(w) = psel

1 Pgen(w) + psel
2

(cid:88)

ˆαi

i:wC

i =w

3.2 Commonsense Selection and

Representation

In QA tasks that require multiple hops of reason-
ing, the model often needs knowledge of relations
not directly stated in the context to reach the cor-
rect conclusion. In the datasets we consider, man-
ual analysis shows that external knowledge is fre-
quently needed for inference (see Table 1).

Even with a large amount of training data, it
is very unlikely that a model is able to learn ev-
ery nuanced relation between concepts and ap-
ply the correct ones (as in Fig. 2) when reasoning

ing hop within the context of the MRC task, e.g.,
church → house, mother → daughter, person →
lover.
Outside Knowledge: We then allow an uncon-
strained hop into c3’s neighbors in ConceptNet,
getting to c4 ∈ nbh(c3) via r3 (nbh(v) is the set
of nodes that can be reached from v in one hop).
This emulates the gathering of useful external in-
formation to complete paths within the context,
e.g., house → child, daughter → child.
Context-Grounding: To ensure that the exter-
nal knowledge is indeed helpful to the task, and
also to explicitly link 2nd degree neighbor con-
cepts within the context, we ﬁnish the process by
grounding it again into context by connecting c4
to c5 ∈ C via r4, e.g., child → their.

3.2.2 Rank and Filter

This tree building process collects a large number
of potentially relevant and useful paths. However,
this step also introduces a large amount of noise.
For example, given the question and full context
(not depicted in the ﬁgure) in Fig. 2, we obtain
the path “between → hard → being → cottage →
country” using our tree building method, which is
not relevant to our question. Therefore, to improve
the precision of useful concepts, we rank these
knowledge paths by their relevance and ﬁlter out
noise using the following 3-step scoring method:
Initial Node Scoring: We want to select paths
to the context,
with nodes that are important
in order to provide the most useful common-
sense relations. We approximate importance and
saliency for concepts in the context by their term-
frequency, under the heuristic that important con-
cepts occur more frequently. Thus we score c ∈
{c2, c3, c5} by: score(c) = count(c)/|C|, where
|C| is the context length and count() is the num-
ber of times a concept appears in the context. In
Fig. 2, this ensures that concepts like daughter are
scored highly due to their frequency in the context.
For c4, we use a special scoring function as it is
an unconstrained hop into ConceptNet. We want
c4 to be a logically consistent next step in reason-
ing following the path of c1 to c3, e.g., in Fig. 2, we
see that child is a logically consistent next step af-
ter the partial path of mother → daughter. We ap-
proximate this based on the heuristic that logically
consistent paths occur more frequently. Therefore,
we score this node via Pointwise Mutual Informa-
tion (PMI) between the partial path c1−3 and c4:
PMI(c4, c1−3) = log(P(c4, c1−3)/P(c4)P(c1−3)),

where

P(c4, c1−3) =

# of paths connecting c1, c2, c3, c4
# of distinct paths of length 4

P(c4) =

# of nodes that can reach c4
|ConceptNet|

P(c1−3) =

# of paths connecting c1, c2, c3
# of paths of length 3

Further, it is well known that PMI has high
sensitivity to low-frequency values,
thus we
use normalized PMI (NPMI) (Bouma, 2009):
score(c4) = PMI(c4, c1−3)/(− log P(c4, c1−3)).

Since the branching at each juncture represents
a hop in the multi-hop reasoning process, and hops
at different levels or with different parent nodes do
not ‘compete’ with each other, we normalize each
node’s score against its siblings:

n-score(c) = softmaxsiblings(c)(score(c)).

Cumulative Node Scoring: We want to add com-
monsense paths consisting of multiple hops of
relevant information, thus we re-score each node
based not only on its relevance and saliency but
also that of its tree descendants.

We do this by computing a cumulative node
score from the bottom up, where at the leaf nodes,
we have c-score = n-score, and for cl not a leaf
node, we have c-score(cl) = n-score(cl) + f (cl)
where f of a node is the average of the c-scores of
its top 2 highest scoring children.

For example, given the paths lady → mother →
daughter, lady → mother → married, and lady →
mother → book, we start the cumulative scoring
at the leaf nodes, which in this case are daugh-
ter, married, and book, where daughter and mar-
ried are scored much higher than book due to their
more frequent occurrences. Then, to cumulatively
score mother , we would take the average score of
its two highest scoring children (in this case mar-
ried and daughter) and compound that with the
score of mother itself. Note that the poor scoring
of the irrelevant concept book does not affect the
scoring of mother, which is quite high due to the
concept’s frequent occurrence and the relevance of
its top scoring children.
Path Selection: We select paths in a top-down
breath-ﬁrst fashion in order to add information rel-
evant to different parts of the context. Starting at
the root, we recursively take two of its children
with the highest cumulative scores until we reach
a leaf, selecting up to 24 = 16 paths. For example,

if we were at node mother, this allows us to se-
lect the child node daughter and married over the
child node book. These selected paths, as well as
their partial sub-paths, are what we add as exter-
nal information to the QA model, i.e., we add the
complete path (cid:104)lady, AtLocation, church, Relat-
edTo, house, RelatedTo, child, RelatedTo, their(cid:105),
but also truncated versions of the path, including
(cid:104)lady, AtLocation, church, RelatedTo, house, Re-
latedTo, child(cid:105). We directly give these paths to the
model as sequences of tokens.4

Overall, our sampling strategy provides the
knowledge that a lady can be a mother and that
mother is connected to daughter. This creates
a logical connection between lady and daughter
which helps highlight the importance of our sec-
ond piece of evidence (see Fig. 2). Likewise,
the commonsense information we extracted cre-
ate a similar connection in our third piece of ev-
idence, which states the explicit connection be-
tween daughter and Esther. We also successfully
extract a more story context-centric connection, in
which commonsense provides the knowledge that
a lady is at the location church, which directs to
another piece of evidence in the context. Addition-
ally, this path also encodes a relation between lady
and child, by way of church, which is how lady
and Esther are explicitly connected in the story.

3.3 Commonsense Model Incorporation

i

,

, wCS
2

l } where wCS

Given the list of commonsense logic paths as se-
quences of words: X CS = {wCS
. . . ,
1
wCS
represents the list of tokens
that make up a single path, we ﬁrst embed these
commonsense tokens into the learned embedding
space used by the model, giving us the embedded
ij ∈ Rd. We want to
commonsense tokens, eCS
use these commonsense paths to ﬁll in the gaps
of reasoning between hops of inference. Thus,
we propose Necessary and Optional Information
Cell
(NOIC), a variation of our base reasoning
cell used in the reasoning layer that is capable of
incorporating optional helpful information.

NOIC This cell is an extension to the base rea-
soning cell that allows the model to use common-
sense information to ﬁll in gaps of reasoning. An
example of this is on the bottom left of Fig. 1,
where we see that the cell ﬁrst performs the op-
erations done in the base reasoning cell and then

4In cases where more than one relation can be used to

make a hop, we pick one at random.

adds optional, commonsense information.

At reasoning step t, after obtaining the out-
put of the base reasoning cell, ct, we create a
cell-speciﬁc representation for commonsense in-
formation by concatenating the embedded com-
monsense paths so that each path has a single vec-
tor representation, uCS
. We then project it to the
i
same dimension as ct
i = ReLU(W uCS
i: vCS
i + b)
where W and b are trainable parameters.

We use an attention layer to model the interac-

tion between commonsense and the context:

ij = W CS
SCS

1 ct

i + W CS

2 vCS

j + W CS

3

(ct

i (cid:12) vCS
j

)

pCS
ij =

exp(SCS
ij )
k=1 exp(SCS
ij )

(cid:80)l

cCS
i =

ij vCS
pCS
j

l
(cid:88)

j=1

Finally, we combine this commonsense-aware
context representation with the original ct
i via a
sigmoid gate, since commonsense information is
often not necessary at every step of inference:

zi = σ(Wz[cCS

; ct

i

i] + bz)

(co)t

i + (1 − zi) (cid:12) cCS

i = zi (cid:12) ct
t as the output of the current reasoning
We use co
step instead of ct. As we replace each base rea-
soning cell with NOIC, we selectively incorporate
commonsense at every step of inference.

i

4 Experimental Setup

Datasets: We report results on two multi-hop rea-
soning datasets: generative NarrativeQA (Koˇcisk`y
et al., 2018) (summary subtask) and extractive
QAngaroo WikiHop (Welbl et al., 2018). For
multiple-choice WikiHop, we rank candidate re-
sponses by their generation probability. Similar to
previous works (Dhingra et al., 2018), we use the
non-oracle, unmasked and not-validated dataset.
Evaluation Metrics: We evaluate NarrativeQA
on the metrics proposed by its original authors:
Bleu-1, Bleu-4 (Papineni et al., 2002), ME-
TEOR (Banerjee and Lavie, 2005) and Rouge-
L (Lin, 2004). We also evaluate on CIDEr (Vedan-
tam et al., 2015) which emphasizes annotator con-
sensus. For WikiHop, we evaluate on accuracy.

More dataset, metric, and all other training de-

tails are in the supplementary.

Model

BLEU-1

BLEU-4 METEOR Rouge-L CIDEr

Seq2Seq (Koˇcisk`y et al., 2018)
ASR (Koˇcisk`y et al., 2018)
BiDAF† (Koˇcisk`y et al., 2018)
BiAttn + MRU-LSTM† (Tay et al., 2018)

MHPGM
MHPGM+ NOIC

15.89
23.20
33.72
36.55

40.24
43.63

1.26
6.39
15.53
19.79

17.40
21.07

4.08
7.77
15.38
17.87

17.33
19.03

13.15
22.26
36.30
41.44

41.49
44.16

-
-
-
-

139.23
152.98

Table 2: Results across different metrics on the test set of NarrativeQA-summaries task. † indicates span prediction
models trained on the Rouge-L retrieval oracle.

Model

BiDAF (Welbl et al., 2018)
Coref-GRU (Dhingra et al., 2018)

MHPGM
MHPGM+ NOIC

Dev

42.1
56.0

56.2
58.5

Test

42.9
59.3

57.5
57.9

Table 3: Results of our models on WikiHop dataset,
measured in % accuracy.

5 Results

5.1 Main Experiment

The results of our model on both NarrativeQA and
WikiHop with and without commonsense incorpo-
ration are shown in Table 2 and Table 3. We see
empirically that our model outperforms all gener-
ative models on NarrativeQA, and is competitive
with the top span prediction models. Furthermore,
with the NOIC commonsense integration, we were
able to further improve performance (p < 0.001
on all metrics5), establishing a new state-of-the-art
for the task.

We also see that our model performs rea-
sonably well on WikiHop, and further achieves
promising initial improvements via the addition
of commonsense, hinting at the generalizability
of our approaches. We speculate that the im-
provement is smaller on Wikihop because only
approximately 11% of WikiHop data points re-
quire commonsense and because WikiHop data re-
quires more fact-based commonsense (e.g., from
Freebase (Bollacker et al., 2008)) as opposed to
semantics-based commonsense (e.g., from Con-
ceptNet (Speer and Havasi, 2012)).6

5.2 Model Ablations

We also tested the effectiveness of each compo-
nent of our architecture as well as the effective-

5Stat.

signiﬁcance computed using bootstrap test with
100K iterations (Noreen, 1989; Efron and Tibshirani, 1994).
6All results here are for the standard (non-oracle) un-
masked and not-validated dataset. Welbl et al. (2018) has
reported higher numbers on different data settings which are
not comparable to our results.

ness of adding commonsense information on the
NarrativeQA validation set, with results shown in
Table 4. Experiment 1 and 5 are our models pre-
sented in Table 2. Experiment 2 demonstrates the
importance of multi-hop attention by showing that
if we only allow one hop of attention (even with all
other components of the model, including ELMo
embeddings) the model’s performance decreases
by over 12 Rouge-L points. Experiment 3 and 4
demonstrate the effectiveness of other parts of our
model. We see that ELMo embeddings (Peters
et al., 2018) were also important for the model’s
performance and that self-attention is able to con-
tribute signiﬁcantly to performance on top of other
components of the model. Finally, we see that ef-
fectively introducing external knowledge via our
commonsense selection algorithm and NOIC can
improve performance even further on top of our
strong baseline.

5.3 Commonsense Ablations

We also conducted experiments testing the effec-
tiveness of our commonsense selection and incor-
poration techniques. We ﬁrst tried to naively add
ConceptNet information by initializing the word
embeddings with the ConceptNet-trained embed-
dings, NumberBatch (Speer and Havasi, 2012)
(we also change embedding size from 256 to
300). Then, to verify the effectiveness of our com-
monsense selection and grounding algorithm, we
test our best model on in-domain noise by giv-
ing each context-query pair a set of random rela-
tions grounded in other context-query pairs. This
should teach the model about general common-
sense relations present in the domain of Narra-
tiveQA but does not provide grounding that ﬁlls
in speciﬁc hops of inference. We also experi-
mented with a simpler commonsense extraction
method of using a single hop from the query to
the context. The results of these are shown in
Table 5, where we see that neither NumberBatch
nor random-relationships nor single-hop common-

#

1
2
3
4
5

Ablation

-
k = 1
- ELMo
- Self-Attn
+ NOIC

B-1

42.3
32.5
32.8
37.0
46.0

B-4 M

R

C

18.9
11.7
12.7
16.4
21.9

18.3
12.9
13.6
15.6
20.7

44.9
32.4
33.7
38.6
48.0

151.6
95.7
103.1
125.6
166.6

Table 4: Model ablations on NarrativeQA val-set.

Commonsense

B-1

B-4 M

R

C

None
NumberBatch
Random Rel.
Single Hop
Grounded Rel.

42.3
42.6
43.3
42.1
45.9

18.9
19.6
19.3
19.9
21.9

18.3
18.6
18.6
18.2
20.7

44.9
44.4
45.2
44.0
48.0

151.6
148.1
151.2
148.6
166.6

Table 5: Commonsense ablations on NarrativeQA val-
set.

sense offer statistically signiﬁcant improvements7,
whereas our commonsense selection and incorpo-
ration mechanism improves performance signiﬁ-
cantly across all metrics. We also present several
examples of extracted commonsense and its model
attention visualization in the supplementary.

6 Human Evaluation Analysis

We also conduct human evaluation analysis on
both the quality of the selected commonsense re-
lations, as well as the performance of our ﬁnal
model.
Commonsense Selection: We conducted manual
analysis on a 50 sample subset of the NarrativeQA
test set to check the effectiveness of our common-
sense selection algorithm. Speciﬁcally, given a
context-query pair, as well as the commonsense
selected by our algorithm, we conduct two inde-
pendent evaluations: (1) was any external com-
monsense knowledge necessary for answering the
question?; (2) were the commonsense relations
provided by our algorithm relevant to the ques-
tion? The result for these two evaluations as well
as how they overlap with each other are shown in
Table 6, where we see that 50% of the cases re-
quired external commonsense knowledge, and on
a majority (34%) of those cases our algorithm was
able to select the correct/relevant commonsense
information to ﬁll in gaps of inference. We also
see that in general, our algorithm was able to pro-
vide useful commonsense 48% of the time.
Model Performance: We also conduct human
evaluation to verify that our commonsense incor-
porated model was indeed better than MHPGM.

7The improvement in Rouge-L and METEOR for all three

ablation approaches have p ≥ 0.15 with the bootstrap test.

Commonsense Required
Yes

No

Relevant CS Extracted
Irrelevant CS Extracted

34%
16%

14%
36%

Table 6: NarrativeQA’s commonsense requirements
and effectiveness of commonsense selection algorithm.

MHPGM+NOIC better
MHPGM better
Indistinguishable (Both-good)
Indistinguishable (Both-bad)

23%
15%
41%
21%

Table 7: Human evaluation on the output quality of the
MHPGM+NOIC vs. MHPGM in terms of correctness.

We randomly selected 100 examples from the Nar-
rativeQA test set, along with both models’ pre-
dicted answers, and for each datapoint, we asked
3 external human evaluators (ﬂuent English speak-
ers) to decide (without knowing which model pro-
duced each response) if one is strictly better than
the other, or that they were similar in quality (both-
good or both-bad). As shown in Table 7, we see
that the human evaluation results are in agreement
with that of the automatic evaluation metrics: our
commonsense incorporation has a reasonable im-
pact on the overall correctness of the model. The
inter-annotator agreement had a Fleiss κ = 0.831,
indicating ‘almost-perfect’ agreement between the
annotators (Landis and Koch, 1977).

7 Conclusion

We present an effective reasoning-generative QA
architecture that is a novel combination of previ-
ous work, which uses multiple hops of bidirec-
tional attention and a pointer-generator decoder to
effectively perform multi-hop reasoning and syn-
thesize a coherent and correct answer. Further, we
introduce an algorithm to select grounded, use-
ful paths of commonsense knowledge to ﬁll in
the gaps of inference required for QA, as well a
Necessary and Optional Information Cell (NOIC)
which successfully incorporates this information
during multi-hop reasoning to achieve the new
state-of-the-art on NarrativeQA.

Acknowledgments
We thank the reviewers for their helpful com-
ments. This work was supported by DARPA
(YFA17-D17AP00022), Google Faculty Research
Award, Bloomberg Data Science Research Grant,
and NVidia GPU awards. The views contained in
this article are those of the authors and not of the
funding agency.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR.

Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
In Proceed-
correlation with human judgments.
ings of the ACL Workshop on intrinsic and extrinsic
evaluation measures for machine translation and/or
summarization, pages 65–72.

Junwei Bao, Nan Duan, Zhao Yan, Ming Zhou, and
Tiejun Zhao. 2016. Constraint-based question an-
In Proceedings of
swering with knowledge graph.
COLING 2016, the 26th International Conference
on Computational Linguistics: Technical Papers,
pages 2503–2514.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1247–1250. AcM.

Antoine Bordes, Sumit Chopra, and Jason Weston.
2014. Question answering with subgraph embed-
In Proceedings of the 2014 Conference on
dings.
Empirical Methods in Natural Language Processing
(EMNLP), pages 615–620.

Gerlof Bouma. 2009. Normalized (pointwise) mutual
information in collocation extraction. Proceedings
of GSCL, pages 31–40.

Erik Cambria, Amir Hussain, Tariq Durrani, Catherine
Havasi, Chris Eckl, and James Munro. 2010. Sen-
tic computing for patient centered applications. In
Signal Processing (ICSP), 2010 IEEE 10th Interna-
tional Conference on, pages 1279–1282. IEEE.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Diana
Inkpen, and Si Wei. 2018. Neural natural language
inference models enhanced with external knowl-
edge. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 2406–2417. Associa-
tion for Computational Linguistics.

Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016.
Long short-term memory-networks for machine
reading. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Process-
ing, pages 551–561.

Christopher Clark and Matt Gardner. 2018. Simple
and effective multi-paragraph reading comprehen-
sion. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Long
Papers), pages 845–855.

Bhuwan Dhingra, Qiao Jin, Zhilin Yang, William Co-
hen, and Ruslan Salakhutdinov. 2018. Neural mod-
els for reasoning over multiple mentions using coref-
In Proceedings of the 2018 Conference of
erence.

the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers), volume 2, pages
42–48.

Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang, William
Cohen, and Ruslan Salakhutdinov. 2017. Gated-
In Pro-
attention readers for text comprehension.
ceedings of the 55th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), volume 1, pages 1832–1846.

Bradley Efron and Robert J Tibshirani. 1994. An intro-

duction to the bootstrap. CRC press.

Marjan Ghazvininejad, Chris Brockett, Ming-Wei
Chang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, and
Michel Galley. 2018. A knowledge-grounded neural
conversation model. In AAAI.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems, pages 1693–
1701.

Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and Jan
Kleindienst. 2016. Text understanding with the at-
tention sum reader network. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics, pages 908–918.

Diederik P Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In 3rd Interna-
tional Conference for Learning Representations.

Tom´aˇs Koˇcisk`y, Jonathan Schwarz, Phil Blunsom,
Chris Dyer, Karl Moritz Hermann, G´aabor Melis,
and Edward Grefenstette. 2018. The narrativeqa
Transactions
reading comprehension challenge.
of the Association of Computational Linguistics,
6:317–328.

J Richard Landis and Gary G Koch. 1977. The mea-
surement of observer agreement for categorical data.
biometrics, pages 159–174.

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. Text Summarization
Branches Out.

Todor Mihaylov and Anette Frank. 2018. Knowledge-
able reader: enhancing cloze-style reading compre-
hension with external commonsense knowledge. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 821–832. Association for Com-
putational Linguistics.

Eric W Noreen. 1989. Computer-intensive methods for

testing hypotheses. Wiley New York.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the

40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311–318. Association for
Computational Linguistics.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of NAACL.

Soujanya Poria, Erik Cambria, Alexander Gelbukh,
Federica Bisio, and Amir Hussain. 2015. Sentiment
data ﬂow analysis by means of dynamic linguistic
IEEE Computational Intelligence Maga-
patterns.
zine, 10(4):26–36.

Soujanya Poria, Iti Chaturvedi, Erik Cambria, and Fed-
erica Bisio. 2016. Sentic LDA: Improving on LDA
with semantic similarity for aspect-based sentiment
In Neural Networks (IJCNN), 2016 In-
analysis.
ternational Joint Conference on, pages 4465–4473.
IEEE.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, pages 2383–2392.

Abigail See, Peter J Liu, and Christopher D Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), volume 1,
pages 1073–1083.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2017. Bidirectional attention
ﬂow for machine comprehension. In ICLR.

Robyn Speer and Catherine Havasi. 2012. Represent-
ing general relational knowledge in ConceptNet 5.
In LREC, pages 3679–3686.

Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.
2015. End-to-end memory networks. In Advances
in Neural Information Processing Systems, pages
2440–2448.

Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2018.
Multi-range reasoning for machine comprehension.
arXiv preprint arXiv:1803.09074.

Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. 2015. Cider: Consensus-based image de-
In Proceedings of the IEEE
scription evaluation.
Conference on Computer Vision and Pattern Recog-
nition, pages 4566–4575.

Qiu-Feng Wang, Erik Cambria, Cheng-Lin Liu, and
Amir Hussain. 2013. Common sense knowledge
for handwritten chinese text recognition. Cognitive
Computation, 5(2):234–242.

Dirk Weissenborn, Tom´aˇs Koˇcisk`y, and Chris Dyer.
2017. Dynamic integration of background knowl-
arXiv preprint
edge in neural NLU systems.
arXiv:1706.02596.

Johannes Welbl, Pontus Stenetorp, and Sebastian
Riedel. 2018. Constructing datasets for multi-hop
reading comprehension across documents. Transac-
tions of the Association of Computational Linguis-
tics, 6:287–302.

Jason Weston, Antoine Bordes, Sumit Chopra, Alexan-
der M Rush, Bart van Merri¨enboer, Armand Joulin,
and Tomas Mikolov. 2016. Towards ai-complete
question answering: A set of prerequisite toy tasks.
In ICLR.

Zhen Xu, Bingquan Liu, Baoxun Wang, Chengjie Sun,
and Xiaolong Wang. 2017.
Incorporating loose-
structured knowledge into LSTM with recall gate for
conversation modeling. In International Joint Con-
ference on Neural Networks.

Tom Young, Erik Cambria, Iti Chaturvedi, Minlie
Huang, Hao Zhou, and Subham Biswas. 2018. Aug-
menting end-to-end dialog systems with common-
In Proceedings of the Thirty-
sense knowledge.
Second AAAI Conference on Artiﬁcial Intelligence
(AAAI-18).

A Supplemental Material

A.1 Experimental Setup

Datasets We test our model with and with-
out commonsense addition on two challenging
datasets that require multi-hop reasoning and ex-
ternal knowledge: NarrativeQA (Koˇcisk`y et al.,
2018) and QAngaroo-WikiHop (Welbl et al.,
2018). NarrativeQA is a generative QA dataset
where the passages are either stories or summaries
of stories, and the questions ask about complex
aspects of the narratives such as event timelines,
characters, relations between characters, etc. Each
question has two answers which are generated by
human annotators and usually cannot be found
in the passage directly. We focus on the sum-
mary subtask in this paper, where summaries have
lengths of up to 1000 words.

We also test our model on WikiHop, a fact
based, multi-hop dataset. Questions in WikiHop
often require a model to read several documents
in order to obtain an answer. We focus on the
multiple-choice part of WikiHop, where models
are tasked with picking the correct response from
a pool of candidates. We rank candidate responses
by calculating their generation probability based
on our model. As this is a multi-document QA
task, we ﬁrst rank the candidate documents via TF-
IDF cosine distance with the question, and then
take the top k documents such that their combined
length is less than 1300 words.

paths have high activation, but the activation be-
comes more focused on the passage’s key words
w.r.t. the question, as the number of hops increase.

Evaluation Metrics We evaluate NarrativeQA
on the metrics proposed by its original authors:
Bleu-1, Bleu-4 (Papineni et al., 2002), ME-
TEOR (Banerjee and Lavie, 2005) and Rouge-
L (Lin, 2004). We also evaluate on CIDEr (Vedan-
tam et al., 2015) as it places emphasize on annota-
tor consensus. For WikiHop, we evaluate on accu-
racy.

Training Details
In training for both datasets,
we minimize the negative log probability of gener-
ating the ground-truth answer with the Adam opti-
mizer (Kingma and Ba, 2015) with an initial learn-
ing rate of 0.001, a dropout-rate of 0.2 (dropout is
applied to the input of each RNN layer) and batch
size of 24. We use 256 dimensional word embed-
dings and a hidden size of 128 for all RNNs and
k = 3 hops of multi attention. At inference time
we use greedy decoding to generate the answer.
For both NarrativeQA and WikiHop, we reached
these parameters via tuning on the full, ofﬁcial val-
idation set.

A.2 Commonsense Extraction Examples

In Tables 8, 9, and 10 (see next page), we demon-
strate extracted commonsense examples for ques-
tions that require commonsense to reach an an-
swer. We bold words in the question and in the ex-
tracted commonsense in cases where the common-
sense knowledge explicitly bridges gaps between
implicitly connected words in the context or ques-
tion. The relevant context is also displayed, with
context words that are key to answering the ques-
tion (via commonsense) marked in bold. These are
then followed by a context visualization described
in the next section.

A.3 Commonsense Integration Visualization

We also visualize how much commonsense infor-
mation is integrated into each part of the context
by providing a visualization of the zi value (see
end of Sec. 3.3 of main ﬁle) for i ∈ {1, 2, 3},
which is the gate value signifying how much
commonsense-attention representation is used in
the output context representation. In the follow-
ing examples (next page), we use shades of blue
to represent the average of (1 − zi) at each word
in the context (normalized within each hop), with
deeper blue indicating the use of more common-
sense information. As a general trend, we see that
in the earlier hops, words which are near tokens
that occur in both the context and commonsense

Figure 3: Example 1 visualized activation values of ﬁrst attention hop (1 − z1).

Figure 4: Example 1 visualized activation values of second attention hop (1 − z2).

Commonsense Extraction and Visualization Examples

Question

What shore does Michael’s corpse wash up on?

Context

”..as the play begins nora and cathleen receive word from the priest that a
body, that may be their brother michael, has washed up on shore in donegal,
the island farthest north of their home island of inishmaan..”

Answers

the shore of donegal / donegal

Extracted
Commonsense

up → RelatedTo → wind → Antonym → her → RelatedTo → person
up → RelatedTo → north → RelatedTo → up
wash → RelatedTo → up
up → Antonym → down
wash → RelatedTo → water → PartOf → sea → RelatedTo → ﬁsh
up → RelatedTo → wind
wash → RelatedTo → water → PartOf → sea
shore → RelatedTo → sea
wash → RelatedTo → body
wash → Antonym → making
up → Antonym → down → Antonym → up
wash → RelatedTo → water → PartOf → sea → MadeOf → water
up → RelatedTo → wind → Antonym → her
wash → RelatedTo → water
up → RelatedTo → south
shore → RelatedTo → sea → MadeOf → water → AtLocation → bucket → RelatedTo → horse
wash → RelatedTo → clothing
wash → RelatedTo → water → PartOf → sea → MadeOf → water → PartOf → sea
shore → RelatedTo → sea → MadeOf → water
wash → Antonym → getting
up → RelatedTo → north
corpse → RelatedTo → body
shore → RelatedTo → sea → MadeOf → water → AtLocation → fountain
corpse → RelatedTo → body → RelatedTo → corpse
corpse → RelatedTo → body → RelatedTo → water
wash → HasContext → west
up → RelatedTo → wind → Antonym → her → RelatedTo → person → MadeOf → water
up → RelatedTo → wind → AtLocation → sea
wash → RelatedTo → water → AtLocation → can
shore → RelatedTo → sea → MadeOf → water → AtLocation → bucket
wash → RelatedTo → will
shore → RelatedTo → sea → MadeOf → water → AtLocation → fountain → RelatedTo → water

Table 8: Example 1 selected commonsense paths.

Figure 5: Example 1 visualized activation values of third attention hop (1 − z3).

Question

Context

Answers

What species lives in the nearby mines?

”..the nearby mines are inhabited by a race of goblins..”

the goblins / goblins.

Extracted
Commonsense

species → RelatedTo → kingdom → RelatedTo → queen
species → RelatedTo → kingdom → RelatedTo → queen → UsedFor → people → HasA → feet
mines → FormOf → mine
lives → FormOf → life
mines → FormOf → mine → AtLocation → home → RelatedTo → person
species → RelatedTo → kingdom → RelatedTo → queen → UsedFor → people
species → RelatedTo → kingdom → DerivedFrom → king → RelatedTo → master
species → RelatedTo → kingdom → RelatedTo → queen → RelatedTo → person → Desires → feet
mines → FormOf → mine → AtLocation → home → RelatedTo → line → RelatedTo → thread
species → RelatedTo → kingdom → DerivedFrom → king → RelatedTo → leader → AtLocation
→ company
species → RelatedTo → kingdom
species → RelatedTo → kingdom → DerivedFrom → king → RelatedTo → leader
species → RelatedTo → kingdom → DerivedFrom → king
mines → FormOf → mine → AtLocation → home → RelatedTo → line
species → RelatedTo → race
mines → FormOf → mine → AtLocation → home
species → RelatedTo → kingdom → RelatedTo → queen → RelatedTo → person
species → RelatedTo → kingdom → DerivedFrom → king → RelatedTo → master → RelatedTo
→ young

Table 9: Example 2 selected commonsense paths.

Figure 6: Example 2 visualized activation values of ﬁrst attention hop (1 − z1).

Figure 7: Example 2 visualized activation values of second attention hop (1 − z2).

Figure 8: Example 2 visualized activation values of third attention hop (1 − z3).

Question

What duty does ruth have to fulﬁll when her aunt dies?

Context

”..ruth anvoy, a young american woman with a wealthy father, comes to britain to
visit her widowed aunt lady coxon..”
”..having made a promise to her now-deceased husband, lady coxon has for
years been seeking to bestow a sum of 13,000 pounds upon a talented
intellectual whose potential has been hampered by lack of money. having failed to
ﬁnd such a person, lady coxon tells anvoy that upon her death the money will be
left to her, and she must carry on the quest..”
”..anvoy, having lost nearly all her wealth, has only the 13,000 pounds from
lady coxon, with a moral but not legal obligation to give it away..”
”..she awards the coxon fund to saltram, who lives off it exactly as he lived off
his friends, producing nothing of intellectual value..”

Answers

she must give away the 13,000 pounds to an appropriate recipient. /
bestow 13000 to the appropriate person

Extracted
Commonsense

duty → RelatedTo → moral → Antonym → immoral
duty → RelatedTo → time → IsA → money
duty → RelatedTo → time → IsA → money → AtLocation → church
duty → DistinctFrom → off
duty → RelatedTo → time → IsA → money → CapableOf → pay → bills → MotivatedByGoal
→ must
duty → RelatedTo → time → IsA → money → AtLocation → church → RelatedTo → house
duty → RelatedTo → must → RelatedTo → having → RelatedTo → estate → RelatedTo → real
her → RelatedTo → woman → RelatedTo → lady → RelatedTo → plate → Antonym → her
duty → RelatedTo → moral → RelatedTo → will → RelatedTo → choose → IsA → decide
duty → RelatedTo → must → RelatedTo → having → RelatedTo → estate
duty → RelatedTo → obligation
duty → RelatedTo → moral → RelatedTo → will → IsA → purpose
her → RelatedTo → but → DistinctFrom → only → RelatedTo → child → RelatedTo → particularly
her → RelatedTo → person → RelatedTo → others → RelatedTo → people
her → Antonym → him → RelatedTo → he → RelatedTo → person → Desires → conversation
her → RelatedTo → woman → RelatedTo → lady
her → RelatedTo → woman → RelatedTo → she
duty → RelatedTo → must → RelatedTo → having → RelatedTo → own → RelatedTo → having
her → RelatedTo → person → DistinctFrom → man → Antonym → people
her → RelatedTo → but → DistinctFrom → only → RelatedTo → child
her → Antonym → him → RelatedTo → he → RelatedTo → person
her → Antonym → his → RelatedTo → him → RelatedTo → person

Table 10: Example 3 selected commonsense paths.

Figure 9: Example 3 visualized activation values of ﬁrst attention hop (1 − z1).

Figure 10: Example 3 visualized activation values of second attention hop (1 − z2).

Figure 11: Example 3 visualized activation values of third attention hop (1 − z3).

Commonsense for Generative Multi-Hop Question Answering Tasks

Lisa Bauer∗

Yicheng Wang∗
UNC Chapel Hill
{lbauer6, yicheng, mbansal}@cs.unc.edu

Mohit Bansal

9
1
0
2
 
n
u
J
 
1
 
 
]
L
C
.
s
c
[
 
 
3
v
9
0
3
6
0
.
9
0
8
1
:
v
i
X
r
a

Abstract

Reading comprehension QA tasks have seen
a recent surge in popularity, yet most works
have focused on fact-ﬁnding extractive QA.
We instead focus on a more challenging multi-
hop generative task (NarrativeQA), which re-
quires the model to reason, gather, and synthe-
size disjoint pieces of information within the
context to generate an answer. This type of
multi-step reasoning also often requires under-
standing implicit relations, which humans re-
solve via external, background commonsense
knowledge. We ﬁrst present a strong genera-
tive baseline that uses a multi-attention mech-
anism to perform multiple hops of reasoning
and a pointer-generator decoder to synthesize
the answer. This model performs substan-
tially better than previous generative models,
and is competitive with current state-of-the-
art span prediction models. We next intro-
duce a novel system for selecting grounded
multi-hop relational commonsense informa-
tion from ConceptNet via a pointwise mutual
information and term-frequency based scor-
ing function. Finally, we effectively use this
extracted commonsense information to ﬁll in
gaps of reasoning between context hops, using
a selectively-gated attention mechanism. This
boosts the model’s performance signiﬁcantly
(also veriﬁed via human evaluation), establish-
ing a new state-of-the-art for the task. We
also show promising initial results of the gen-
eralizability of our background knowledge en-
hancements by demonstrating some improve-
ment on QAngaroo-WikiHop, another multi-
hop reasoning dataset.

1

Introduction

In this paper, we explore the task of machine
reading comprehension (MRC) based QA. This

∗ Equal contribution (published at EMNLP 2018).
We publicly release all our code, models, and data at:

https://github.com/yicheng-w/CommonSenseMultiHopQA

task tests a model’s natural language understand-
ing capabilities by asking it to answer a question
based on a passage of relevant content. Much
progress has been made in reasoning-based MRC-
QA on the bAbI dataset (Weston et al., 2016),
which contains questions that require the combi-
nation of multiple disjoint pieces of evidence in
the context. However, due to its synthetic nature,
bAbI evidences have smaller lexicons and sim-
pler passage structures when compared to human-
generated text.

There also have been several attempts at the
MRC-QA task on human-generated text. Large
scale datasets such as CNN/DM (Hermann et al.,
2015) and SQuAD (Rajpurkar et al., 2016) have
made the training of end-to-end neural models
possible. However, these datasets are fact-based
and do not place heavy emphasis on multi-hop rea-
soning capabilities. More recent datasets such as
QAngaroo (Welbl et al., 2018) have prompted a
strong focus on multi-hop reasoning in very long
texts. However, QAngaroo is an extractive dataset
where answers are guaranteed to be spans within
the context; hence, this is more focused on fact
ﬁnding and linking, and does not require models
to synthesize and generate new information.

We focus on the recently published Narra-
tiveQA generative dataset (Koˇcisk`y et al., 2018)
that contains questions requiring multi-hop rea-
soning for long, complex stories and other nar-
ratives, which requires the model to go beyond
fact linking and to synthesize non-span answers.
Hence, models that perform well on previous rea-
soning tasks (Dhingra et al., 2018) have had lim-
ited success on this dataset. In this paper, we ﬁrst
propose the Multi-Hop Pointer-Generator Model
(MHPGM), a strong baseline model that uses mul-
tiple hops of bidirectional attention, self-attention,
and a pointer-generator decoder to effectively read
and reason within a long passage and synthesize

a coherent response. Our model achieves 41.49
Rouge-L and 17.33 METEOR on the summary
subtask of NarrativeQA, substantially better than
the performance of previous generative models.

Next,

to address the issue that understand-
ing human-generated text and performing long-
distance reasoning on it often involves intermittent
access to missing hops of external commonsense
(background) knowledge, we present an algorithm
for selecting useful, grounded multi-hop relational
knowledge paths from ConceptNet (Speer and
Havasi, 2012) via a pointwise mutual information
(PMI) and term-frequency-based scoring func-
tion. We then present a novel method of insert-
ing these selected commonsense paths between
the hops of document-context reasoning within
our model, via the Necessary and Optional Infor-
mation Cell (NOIC), which employs a selectively-
gated attention mechanism that utilizes common-
sense information to effectively ﬁll in gaps of in-
ference. With these additions, we further improve
performance on the NarrativeQA dataset, achiev-
ing 44.16 Rouge-L and 19.03 METEOR (also ver-
iﬁed via human evaluation). We also provide man-
ual analysis on the effectiveness of our common-
sense selection algorithm.

Finally,

to show the generalizability of our
multi-hop reasoning and commonsense methods,
we show some promising initial results via the ad-
dition of commonsense information over the base-
line on QAngaroo-WikiHop (Welbl et al., 2018),
an extractive dataset for multi-hop reasoning from
a different domain.

2 Related Work

Machine Reading Comprehension: MRC has
long been a task used to assess a model’s ability
to understand and reason about language. Large
scale datasets such as CNN/Daily Mail
(Her-
mann et al., 2015) and SQuAD (Rajpurkar et al.,
2016) have encouraged the development of many
advanced, high performing attention-based neural
models (Seo et al., 2017; Dhingra et al., 2017).
Concurrently, datasets such as bAbI (Weston et al.,
2016) have focused speciﬁcally on multi-step rea-
soning by requiring the model to reason with
disjoint pieces of information. On this task,
it has been shown that iteratively updating the
query representation with information from the
context can effectively emulate multi-step reason-
ing (Sukhbaatar et al., 2015).

More recently, there has been an increase in
multi-paragraph, multi-hop inference QA datasets
such as QAngaroo (Welbl et al., 2018) and Narra-
tiveQA (Koˇcisk`y et al., 2018). These datasets have
much longer contexts than previous datasets, and
answering a question often requires the synthesis
of multiple discontiguous pieces of evidence.
It
has been shown that models designed for previ-
ous tasks (Seo et al., 2017; Kadlec et al., 2016)
have limited success on these new datasets.
In
our work, we expand upon Gated Attention Net-
work (Dhingra et al., 2017) to create a baseline
model better suited for complex MRC datasets
such as NarrativeQA by improving its attention
and gating mechanisms, expanding its generation
capabilities, and allowing access to external com-
monsense for connecting implicit relations.

Commonsense/Background Knowledge: Com-
monsense or background knowledge has been
tasks including opinion min-
used for several
ing (Cambria et al., 2010), sentiment analy-
sis (Poria et al., 2015, 2016), handwritten text
recognition (Wang et al., 2013), and more re-
cently, dialogue (Young et al., 2018; Ghazvinine-
jad et al., 2018). These approaches add com-
monsense knowledge as relation triples or fea-
tures from external databases. Recently, large-
scale graphical commonsense databases such as
ConceptNet (Speer and Havasi, 2012) use graph-
ical structure to express intricate relations be-
tween concepts, but effective goal-oriented graph
traversal has not been extensively used in previous
commonsense incorporation efforts. Knowledge-
base QA is a task in which systems are asked to
ﬁnd answers to questions by traversing knowledge
graphs (Bollacker et al., 2008). Knowledge path
extraction has been shown to be effective at the
task (Bordes et al., 2014; Bao et al., 2016). We ap-
ply these techniques to MRC-QA by using them to
extract useful commonsense knowledge paths that
fully utilize the graphical nature of databases such
as ConceptNet (Speer and Havasi, 2012).

Incorporation of External Knowledge: There
have been several attempts at using external
knowledge to boost model performance on a vari-
ety of tasks: Chen et al. (2018) showed that adding
lexical information from semantic databases such
as WordNet improves performance on NLI; Xu
et al. (2017) used a gated recall-LSTM mechanism
to incorporate commonsense information into to-
ken representations in dialogue.

In MRC, Weissenborn et al. (2017) integrated
external background knowledge into an NLU
model by using contextually-reﬁned word em-
beddings which integrated information from Con-
ceptNet (single-hop relations mapped to unstruc-
tured text) via a single layer bidirectional LSTM.
Concurrently to our work, Mihaylov and Frank
(2018) showed improvements on a cloze-style task
by incorporating commonsense knowledge via a
context-to-commonsense attention, where com-
monsense relations were extracted as triples. This
work represented commonsense relations as key-
value pairs and combined context representation
and commonsense via a static gate.

Differing from previous works, we employ
multi-hop commonsense paths (multiple con-
nected edges within ConceptNet graph that give
us information beyond a single relationship triple)
to help with our MRC model. Moreover, we use
this in tandem with our multi-hop reasoning archi-
tecture to incorporate different aspects of the com-
monsense relationship path at each hop, in order
to bridge different inference gaps in the multi-hop
QA task. Additionally, our model performs syn-
thesis with its external, background knowledge as
it generates, rather than extracts, its answer.

3 Methods

3.1 Multi-Hop Pointer-Generator Baseline

1, wa

2, . . . , wa

1 , wC
2 , . . . , wQ

the context, X C = {wC
1 , wQ

We ﬁrst rigorously state the problem of genera-
tive QA as follows: given two sequences of input
2 , . . . , wC
n }
tokens:
and the query, X Q = {wQ
m}, the
system should generate a series of answer tokens
X a = {wa
p}. As outlined in previous
sections, an effective generative QA model needs
to be able to perform several hops of reasoning
It would also
over long and complex passages.
need to be able to generate coherent statements to
answer complex questions while having the abil-
ity to copy rare words such as speciﬁc entities
from the reading context. With these in mind, we
propose the Multi-Hop Pointer-Generator Model
(MHPGM) baseline, a novel combination of previ-
ous works with the following major components:

• Embedding Layer: The tokens are embedded
into both learned word embeddings and pre-
trained context-aware embeddings (ELMo (Pe-
ters et al., 2018)).

• Reasoning Layer: The embedded context is
then passed through k reasoning cells, each

of which iteratively updates the context repre-
sentation with information from the query via
BiDAF attention (Seo et al., 2017), emulating a
single reasoning step within the multi-step rea-
soning process.

• Self-Attention Layer: The context representa-
tion is passed through a layer of self-attention
(Cheng et al., 2016) to resolve long-term depen-
dencies and co-reference within the context.

• Pointer-Generator Decoding Layer:

A
attention-pointer-generator decoder (See et al.,
2017) that attends on and potentially copies
from the context is used to create the answer.

The overall model is illustrated in Fig. 1, and

the layers are described in further detail below.
Embedding layer: We embed each word from the
context and question with a learned embedding
space of dimension d. We also obtain context-
aware embeddings for each word via the pre-
trained embedding from language models (ELMo)
(1024 dimensions). The embedded representation
for each word in the context or question, eC
i or
eQ
i ∈ Rd+1024, is the concatenation of its learned
word embedding and ELMo embedding.
Reasoning layer: Our reasoning layer is com-
posed of k reasoning cells (see Fig. 1), where each
incrementally updates the context representation.
The tth reasoning cell’s inputs are the previous
step’s output ({ct−1
}n
i=1) and the embedded ques-
tion ({eQ
i }m
i=1). It ﬁrst creates step-speciﬁc con-
text and query encodings via cell-speciﬁc bidirec-
tional LSTMs:

i

ut = BiLSTM(ct−1);

vt = BiLSTM(eQ)

Then, we use bidirectional attention (Seo et al.,
2017) to emulate a hop of reasoning by focusing
on relevant aspects of the context. Speciﬁcally, we
ﬁrst compute context-to-query attention:

ij = W t
St

1ut

i + W t

2vt

j + W t

3(ut

i (cid:12) vt
j)

pt
ij =

exp(St
ij)
k=1 exp(St

ik)

(cid:80)m

(cq)t

i =

ijvt
pt
j

m
(cid:88)

j=1

2, W t

1, W t

where W t
3 are trainable parameters, and
(cid:12) is elementwise multiplication. We then com-
pute a query-to-context attention vector:

mt

i = max
1≤j≤m

St
ij

Figure 1: Architecture for our Multi-Hop Pointer-Generator Model, and the NOIC commonsense reasoning cell.

pt
i =

exp(mt
i)
j=1 exp(mt
j)

(cid:80)n

qc

t =

iut
pt
i

n
(cid:88)

i=1

We then obtain the updated context representation:

i = [ut
ct

i; (cq)t

i; ut

i (cid:12) (cq)t

i; qc

t (cid:12) (cq)t
i]

where ; is concatenation, ct is the cell’s output.

The initial input of the reasoning layer is the
embedded context representation, i.e., c0 = eC,
and the ﬁnal output of the reasoning layer is the
output of the last cell, ck.
Self-Attention Layer: As the ﬁnal layer before
answer generation, we utilize a residual static self-
attention mechanism (Clark and Gardner, 2018) to
help the model process long contexts with long-
term dependencies. The input of this layer is the
output of the last reasoning cell, ck. We ﬁrst pass
this representation through a fully-connected layer
and then a bi-directional LSTM to obtain another
representation of the context cSA. We obtain the
self attention representation c(cid:48):

ij = W4cSA
SSA

i + W5cSA

j + W6(cSA

i (cid:12) cSA

j

)

pSA
ij =

exp(SSA
ij )
k=1 exp(SSA
ik )

(cid:80)n

n
(cid:88)

c(cid:48)

i =

ij cSA
pSA
j

j=1
where W4, W5, and W6 are trainable parameters.
The output of the self-attention layer is gener-

ated by another layer of bidirectional LSTM.

c(cid:48)(cid:48) = BiLSTM([c(cid:48); cSA; c(cid:48) (cid:12) cSA]

Finally, we add this residually to ck to obtain the
encoded context c = ck + c(cid:48)(cid:48).
Pointer-Generator Decoding Layer: Similar to
the work of See et al. (2017), we use a pointer-
generator model attending on (and potentially
copying from) the context.

At decoding step t, the decoder receives the in-
put xt (embedded representation of last timestep’s
output), the last time step’s hidden state st−1 and
context vector at−1. The decoder computes the
current hidden state st as:

st = LSTM([xt; at−1], st−1)

This hidden state is then used to compute a proba-
bility distribution over the generative vocabulary:

Pgen = softmax(Wgenst + bgen)

We

employ Bahdanau

attention mecha-
nism (Bahdanau et al., 2015) to attend over the
context (c being the output of self-attention layer):
αi = v(cid:124) tanh(Wcci + Wsst + battn)

Dataset

Outside Knowledge Required

WikiHop
NarrativeQA

11%
42%

Table 1: Qualitative analysis of commonsense require-
ments. WikiHop results are from Welbl et al. (2018);
NarrativeQA results are from our manual analysis (on
the validation set).

about a question. We remedy this issue by intro-
ducing grounded commonsense (background) in-
formation using relations between concepts from
ConceptNet (Speer and Havasi, 2012)1 that help
inference by introducing useful connections be-
tween concepts in the context and question.

Due to the size of the semantic network and
the large amount of unnecessary information, we
need an effective way of selecting relations which
provides novel information while being grounded
by the context-query pair. Our commonsense se-
lection strategy is twofold: (1) collect potentially
relevant concepts via a tree construction method
aimed at selecting with high recall candidate rea-
soning paths, and (2) rank and ﬁlter these paths to
ensure both the quality and variety of added infor-
mation via a 3-step scoring strategy (initial node
scoring, cumulative node scoring, and path selec-
tion). We will refer to Fig. 2 as a running example
throughout this section.2

3.2.1 Tree Construction
Given context C and question Q, we want to con-
struct paths grounded in the pair that emulate rea-
soning steps required to answer the question. In
this section, we build ‘prototype’ paths by con-
structing trees rooted in concepts in the query with
the following branching steps3 to emulate multi-
hop reasoning process. For each concept c1 in the
question, we do:
Direct Interaction: In the ﬁrst level, we select re-
lations r1 from ConceptNet that directly link c1
to a concept within the context, c2 ∈ C, e.g., in
Fig. 2, we have lady → church, lady → mother,
lady → person.
Multi-Hop: We then select relations in Concept-
Net r2 that link c2 to another concept in the con-
text, c3 ∈ C. This emulates a potential reason-

1A semantic network where the nodes are individual con-
cepts (words or phrases) and the edges describe directed re-
lations between them (e.g., (cid:104)island, UsedFor, vacation(cid:105)).

2We release all our commonsense extraction code and
the extracted commonsense data at: https://github.com/
yicheng-w/CommonSenseMultiHopQA

3If we are unable to ﬁnd a relation that satisﬁes the condi-

tion, we keep the steps up to and including the node.

Figure 2: Commonsense selection approach.

ˆαi =

exp(αi)
j=1 exp(αj)

(cid:80)n

at =

ˆαici

n
(cid:88)

i=1

We utilize a pointer mechanism that allows the
decoder to directly copy tokens from the context
based on ˆαi. We calculate a selection distribution
psel ∈ R2, where psel
is the probability of gener-
1
ating a token from Pgen and psel
is the probability
2
of copying a word from the context:

o = σ(Waat + Wxxt + Wsst + bptr)

psel = softmax(o)

Our ﬁnal output distribution at timestep t is a
weighted sum of the generative distribution and
the copy distribution:

Pt(w) = psel

1 Pgen(w) + psel
2

(cid:88)

ˆαi

i:wC

i =w

3.2 Commonsense Selection and

Representation

In QA tasks that require multiple hops of reason-
ing, the model often needs knowledge of relations
not directly stated in the context to reach the cor-
rect conclusion. In the datasets we consider, man-
ual analysis shows that external knowledge is fre-
quently needed for inference (see Table 1).

Even with a large amount of training data, it
is very unlikely that a model is able to learn ev-
ery nuanced relation between concepts and ap-
ply the correct ones (as in Fig. 2) when reasoning

ing hop within the context of the MRC task, e.g.,
church → house, mother → daughter, person →
lover.
Outside Knowledge: We then allow an uncon-
strained hop into c3’s neighbors in ConceptNet,
getting to c4 ∈ nbh(c3) via r3 (nbh(v) is the set
of nodes that can be reached from v in one hop).
This emulates the gathering of useful external in-
formation to complete paths within the context,
e.g., house → child, daughter → child.
Context-Grounding: To ensure that the exter-
nal knowledge is indeed helpful to the task, and
also to explicitly link 2nd degree neighbor con-
cepts within the context, we ﬁnish the process by
grounding it again into context by connecting c4
to c5 ∈ C via r4, e.g., child → their.

3.2.2 Rank and Filter

This tree building process collects a large number
of potentially relevant and useful paths. However,
this step also introduces a large amount of noise.
For example, given the question and full context
(not depicted in the ﬁgure) in Fig. 2, we obtain
the path “between → hard → being → cottage →
country” using our tree building method, which is
not relevant to our question. Therefore, to improve
the precision of useful concepts, we rank these
knowledge paths by their relevance and ﬁlter out
noise using the following 3-step scoring method:
Initial Node Scoring: We want to select paths
to the context,
with nodes that are important
in order to provide the most useful common-
sense relations. We approximate importance and
saliency for concepts in the context by their term-
frequency, under the heuristic that important con-
cepts occur more frequently. Thus we score c ∈
{c2, c3, c5} by: score(c) = count(c)/|C|, where
|C| is the context length and count() is the num-
ber of times a concept appears in the context. In
Fig. 2, this ensures that concepts like daughter are
scored highly due to their frequency in the context.
For c4, we use a special scoring function as it is
an unconstrained hop into ConceptNet. We want
c4 to be a logically consistent next step in reason-
ing following the path of c1 to c3, e.g., in Fig. 2, we
see that child is a logically consistent next step af-
ter the partial path of mother → daughter. We ap-
proximate this based on the heuristic that logically
consistent paths occur more frequently. Therefore,
we score this node via Pointwise Mutual Informa-
tion (PMI) between the partial path c1−3 and c4:
PMI(c4, c1−3) = log(P(c4, c1−3)/P(c4)P(c1−3)),

where

P(c4, c1−3) =

# of paths connecting c1, c2, c3, c4
# of distinct paths of length 4

P(c4) =

# of nodes that can reach c4
|ConceptNet|

P(c1−3) =

# of paths connecting c1, c2, c3
# of paths of length 3

Further, it is well known that PMI has high
sensitivity to low-frequency values,
thus we
use normalized PMI (NPMI) (Bouma, 2009):
score(c4) = PMI(c4, c1−3)/(− log P(c4, c1−3)).

Since the branching at each juncture represents
a hop in the multi-hop reasoning process, and hops
at different levels or with different parent nodes do
not ‘compete’ with each other, we normalize each
node’s score against its siblings:

n-score(c) = softmaxsiblings(c)(score(c)).

Cumulative Node Scoring: We want to add com-
monsense paths consisting of multiple hops of
relevant information, thus we re-score each node
based not only on its relevance and saliency but
also that of its tree descendants.

We do this by computing a cumulative node
score from the bottom up, where at the leaf nodes,
we have c-score = n-score, and for cl not a leaf
node, we have c-score(cl) = n-score(cl) + f (cl)
where f of a node is the average of the c-scores of
its top 2 highest scoring children.

For example, given the paths lady → mother →
daughter, lady → mother → married, and lady →
mother → book, we start the cumulative scoring
at the leaf nodes, which in this case are daugh-
ter, married, and book, where daughter and mar-
ried are scored much higher than book due to their
more frequent occurrences. Then, to cumulatively
score mother , we would take the average score of
its two highest scoring children (in this case mar-
ried and daughter) and compound that with the
score of mother itself. Note that the poor scoring
of the irrelevant concept book does not affect the
scoring of mother, which is quite high due to the
concept’s frequent occurrence and the relevance of
its top scoring children.
Path Selection: We select paths in a top-down
breath-ﬁrst fashion in order to add information rel-
evant to different parts of the context. Starting at
the root, we recursively take two of its children
with the highest cumulative scores until we reach
a leaf, selecting up to 24 = 16 paths. For example,

if we were at node mother, this allows us to se-
lect the child node daughter and married over the
child node book. These selected paths, as well as
their partial sub-paths, are what we add as exter-
nal information to the QA model, i.e., we add the
complete path (cid:104)lady, AtLocation, church, Relat-
edTo, house, RelatedTo, child, RelatedTo, their(cid:105),
but also truncated versions of the path, including
(cid:104)lady, AtLocation, church, RelatedTo, house, Re-
latedTo, child(cid:105). We directly give these paths to the
model as sequences of tokens.4

Overall, our sampling strategy provides the
knowledge that a lady can be a mother and that
mother is connected to daughter. This creates
a logical connection between lady and daughter
which helps highlight the importance of our sec-
ond piece of evidence (see Fig. 2). Likewise,
the commonsense information we extracted cre-
ate a similar connection in our third piece of ev-
idence, which states the explicit connection be-
tween daughter and Esther. We also successfully
extract a more story context-centric connection, in
which commonsense provides the knowledge that
a lady is at the location church, which directs to
another piece of evidence in the context. Addition-
ally, this path also encodes a relation between lady
and child, by way of church, which is how lady
and Esther are explicitly connected in the story.

3.3 Commonsense Model Incorporation

i

,

, wCS
2

l } where wCS

Given the list of commonsense logic paths as se-
quences of words: X CS = {wCS
. . . ,
1
wCS
represents the list of tokens
that make up a single path, we ﬁrst embed these
commonsense tokens into the learned embedding
space used by the model, giving us the embedded
ij ∈ Rd. We want to
commonsense tokens, eCS
use these commonsense paths to ﬁll in the gaps
of reasoning between hops of inference. Thus,
we propose Necessary and Optional Information
Cell
(NOIC), a variation of our base reasoning
cell used in the reasoning layer that is capable of
incorporating optional helpful information.

NOIC This cell is an extension to the base rea-
soning cell that allows the model to use common-
sense information to ﬁll in gaps of reasoning. An
example of this is on the bottom left of Fig. 1,
where we see that the cell ﬁrst performs the op-
erations done in the base reasoning cell and then

4In cases where more than one relation can be used to

make a hop, we pick one at random.

adds optional, commonsense information.

At reasoning step t, after obtaining the out-
put of the base reasoning cell, ct, we create a
cell-speciﬁc representation for commonsense in-
formation by concatenating the embedded com-
monsense paths so that each path has a single vec-
tor representation, uCS
. We then project it to the
i
same dimension as ct
i = ReLU(W uCS
i: vCS
i + b)
where W and b are trainable parameters.

We use an attention layer to model the interac-

tion between commonsense and the context:

ij = W CS
SCS

1 ct

i + W CS

2 vCS

j + W CS

3

(ct

i (cid:12) vCS
j

)

pCS
ij =

exp(SCS
ij )
k=1 exp(SCS
ij )

(cid:80)l

cCS
i =

ij vCS
pCS
j

l
(cid:88)

j=1

Finally, we combine this commonsense-aware
context representation with the original ct
i via a
sigmoid gate, since commonsense information is
often not necessary at every step of inference:

zi = σ(Wz[cCS

; ct

i

i] + bz)

(co)t

i + (1 − zi) (cid:12) cCS

i = zi (cid:12) ct
t as the output of the current reasoning
We use co
step instead of ct. As we replace each base rea-
soning cell with NOIC, we selectively incorporate
commonsense at every step of inference.

i

4 Experimental Setup

Datasets: We report results on two multi-hop rea-
soning datasets: generative NarrativeQA (Koˇcisk`y
et al., 2018) (summary subtask) and extractive
QAngaroo WikiHop (Welbl et al., 2018). For
multiple-choice WikiHop, we rank candidate re-
sponses by their generation probability. Similar to
previous works (Dhingra et al., 2018), we use the
non-oracle, unmasked and not-validated dataset.
Evaluation Metrics: We evaluate NarrativeQA
on the metrics proposed by its original authors:
Bleu-1, Bleu-4 (Papineni et al., 2002), ME-
TEOR (Banerjee and Lavie, 2005) and Rouge-
L (Lin, 2004). We also evaluate on CIDEr (Vedan-
tam et al., 2015) which emphasizes annotator con-
sensus. For WikiHop, we evaluate on accuracy.

More dataset, metric, and all other training de-

tails are in the supplementary.

Model

BLEU-1

BLEU-4 METEOR Rouge-L CIDEr

Seq2Seq (Koˇcisk`y et al., 2018)
ASR (Koˇcisk`y et al., 2018)
BiDAF† (Koˇcisk`y et al., 2018)
BiAttn + MRU-LSTM† (Tay et al., 2018)

MHPGM
MHPGM+ NOIC

15.89
23.20
33.72
36.55

40.24
43.63

1.26
6.39
15.53
19.79

17.40
21.07

4.08
7.77
15.38
17.87

17.33
19.03

13.15
22.26
36.30
41.44

41.49
44.16

-
-
-
-

139.23
152.98

Table 2: Results across different metrics on the test set of NarrativeQA-summaries task. † indicates span prediction
models trained on the Rouge-L retrieval oracle.

Model

BiDAF (Welbl et al., 2018)
Coref-GRU (Dhingra et al., 2018)

MHPGM
MHPGM+ NOIC

Dev

42.1
56.0

56.2
58.5

Test

42.9
59.3

57.5
57.9

Table 3: Results of our models on WikiHop dataset,
measured in % accuracy.

5 Results

5.1 Main Experiment

The results of our model on both NarrativeQA and
WikiHop with and without commonsense incorpo-
ration are shown in Table 2 and Table 3. We see
empirically that our model outperforms all gener-
ative models on NarrativeQA, and is competitive
with the top span prediction models. Furthermore,
with the NOIC commonsense integration, we were
able to further improve performance (p < 0.001
on all metrics5), establishing a new state-of-the-art
for the task.

We also see that our model performs rea-
sonably well on WikiHop, and further achieves
promising initial improvements via the addition
of commonsense, hinting at the generalizability
of our approaches. We speculate that the im-
provement is smaller on Wikihop because only
approximately 11% of WikiHop data points re-
quire commonsense and because WikiHop data re-
quires more fact-based commonsense (e.g., from
Freebase (Bollacker et al., 2008)) as opposed to
semantics-based commonsense (e.g., from Con-
ceptNet (Speer and Havasi, 2012)).6

5.2 Model Ablations

We also tested the effectiveness of each compo-
nent of our architecture as well as the effective-

5Stat.

signiﬁcance computed using bootstrap test with
100K iterations (Noreen, 1989; Efron and Tibshirani, 1994).
6All results here are for the standard (non-oracle) un-
masked and not-validated dataset. Welbl et al. (2018) has
reported higher numbers on different data settings which are
not comparable to our results.

ness of adding commonsense information on the
NarrativeQA validation set, with results shown in
Table 4. Experiment 1 and 5 are our models pre-
sented in Table 2. Experiment 2 demonstrates the
importance of multi-hop attention by showing that
if we only allow one hop of attention (even with all
other components of the model, including ELMo
embeddings) the model’s performance decreases
by over 12 Rouge-L points. Experiment 3 and 4
demonstrate the effectiveness of other parts of our
model. We see that ELMo embeddings (Peters
et al., 2018) were also important for the model’s
performance and that self-attention is able to con-
tribute signiﬁcantly to performance on top of other
components of the model. Finally, we see that ef-
fectively introducing external knowledge via our
commonsense selection algorithm and NOIC can
improve performance even further on top of our
strong baseline.

5.3 Commonsense Ablations

We also conducted experiments testing the effec-
tiveness of our commonsense selection and incor-
poration techniques. We ﬁrst tried to naively add
ConceptNet information by initializing the word
embeddings with the ConceptNet-trained embed-
dings, NumberBatch (Speer and Havasi, 2012)
(we also change embedding size from 256 to
300). Then, to verify the effectiveness of our com-
monsense selection and grounding algorithm, we
test our best model on in-domain noise by giv-
ing each context-query pair a set of random rela-
tions grounded in other context-query pairs. This
should teach the model about general common-
sense relations present in the domain of Narra-
tiveQA but does not provide grounding that ﬁlls
in speciﬁc hops of inference. We also experi-
mented with a simpler commonsense extraction
method of using a single hop from the query to
the context. The results of these are shown in
Table 5, where we see that neither NumberBatch
nor random-relationships nor single-hop common-

#

1
2
3
4
5

Ablation

-
k = 1
- ELMo
- Self-Attn
+ NOIC

B-1

42.3
32.5
32.8
37.0
46.0

B-4 M

R

C

18.9
11.7
12.7
16.4
21.9

18.3
12.9
13.6
15.6
20.7

44.9
32.4
33.7
38.6
48.0

151.6
95.7
103.1
125.6
166.6

Table 4: Model ablations on NarrativeQA val-set.

Commonsense

B-1

B-4 M

R

C

None
NumberBatch
Random Rel.
Single Hop
Grounded Rel.

42.3
42.6
43.3
42.1
45.9

18.9
19.6
19.3
19.9
21.9

18.3
18.6
18.6
18.2
20.7

44.9
44.4
45.2
44.0
48.0

151.6
148.1
151.2
148.6
166.6

Table 5: Commonsense ablations on NarrativeQA val-
set.

sense offer statistically signiﬁcant improvements7,
whereas our commonsense selection and incorpo-
ration mechanism improves performance signiﬁ-
cantly across all metrics. We also present several
examples of extracted commonsense and its model
attention visualization in the supplementary.

6 Human Evaluation Analysis

We also conduct human evaluation analysis on
both the quality of the selected commonsense re-
lations, as well as the performance of our ﬁnal
model.
Commonsense Selection: We conducted manual
analysis on a 50 sample subset of the NarrativeQA
test set to check the effectiveness of our common-
sense selection algorithm. Speciﬁcally, given a
context-query pair, as well as the commonsense
selected by our algorithm, we conduct two inde-
pendent evaluations: (1) was any external com-
monsense knowledge necessary for answering the
question?; (2) were the commonsense relations
provided by our algorithm relevant to the ques-
tion? The result for these two evaluations as well
as how they overlap with each other are shown in
Table 6, where we see that 50% of the cases re-
quired external commonsense knowledge, and on
a majority (34%) of those cases our algorithm was
able to select the correct/relevant commonsense
information to ﬁll in gaps of inference. We also
see that in general, our algorithm was able to pro-
vide useful commonsense 48% of the time.
Model Performance: We also conduct human
evaluation to verify that our commonsense incor-
porated model was indeed better than MHPGM.

7The improvement in Rouge-L and METEOR for all three

ablation approaches have p ≥ 0.15 with the bootstrap test.

Commonsense Required
Yes

No

Relevant CS Extracted
Irrelevant CS Extracted

34%
16%

14%
36%

Table 6: NarrativeQA’s commonsense requirements
and effectiveness of commonsense selection algorithm.

MHPGM+NOIC better
MHPGM better
Indistinguishable (Both-good)
Indistinguishable (Both-bad)

23%
15%
41%
21%

Table 7: Human evaluation on the output quality of the
MHPGM+NOIC vs. MHPGM in terms of correctness.

We randomly selected 100 examples from the Nar-
rativeQA test set, along with both models’ pre-
dicted answers, and for each datapoint, we asked
3 external human evaluators (ﬂuent English speak-
ers) to decide (without knowing which model pro-
duced each response) if one is strictly better than
the other, or that they were similar in quality (both-
good or both-bad). As shown in Table 7, we see
that the human evaluation results are in agreement
with that of the automatic evaluation metrics: our
commonsense incorporation has a reasonable im-
pact on the overall correctness of the model. The
inter-annotator agreement had a Fleiss κ = 0.831,
indicating ‘almost-perfect’ agreement between the
annotators (Landis and Koch, 1977).

7 Conclusion

We present an effective reasoning-generative QA
architecture that is a novel combination of previ-
ous work, which uses multiple hops of bidirec-
tional attention and a pointer-generator decoder to
effectively perform multi-hop reasoning and syn-
thesize a coherent and correct answer. Further, we
introduce an algorithm to select grounded, use-
ful paths of commonsense knowledge to ﬁll in
the gaps of inference required for QA, as well a
Necessary and Optional Information Cell (NOIC)
which successfully incorporates this information
during multi-hop reasoning to achieve the new
state-of-the-art on NarrativeQA.

Acknowledgments
We thank the reviewers for their helpful com-
ments. This work was supported by DARPA
(YFA17-D17AP00022), Google Faculty Research
Award, Bloomberg Data Science Research Grant,
and NVidia GPU awards. The views contained in
this article are those of the authors and not of the
funding agency.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR.

Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
In Proceed-
correlation with human judgments.
ings of the ACL Workshop on intrinsic and extrinsic
evaluation measures for machine translation and/or
summarization, pages 65–72.

Junwei Bao, Nan Duan, Zhao Yan, Ming Zhou, and
Tiejun Zhao. 2016. Constraint-based question an-
In Proceedings of
swering with knowledge graph.
COLING 2016, the 26th International Conference
on Computational Linguistics: Technical Papers,
pages 2503–2514.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1247–1250. AcM.

Antoine Bordes, Sumit Chopra, and Jason Weston.
2014. Question answering with subgraph embed-
In Proceedings of the 2014 Conference on
dings.
Empirical Methods in Natural Language Processing
(EMNLP), pages 615–620.

Gerlof Bouma. 2009. Normalized (pointwise) mutual
information in collocation extraction. Proceedings
of GSCL, pages 31–40.

Erik Cambria, Amir Hussain, Tariq Durrani, Catherine
Havasi, Chris Eckl, and James Munro. 2010. Sen-
tic computing for patient centered applications. In
Signal Processing (ICSP), 2010 IEEE 10th Interna-
tional Conference on, pages 1279–1282. IEEE.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Diana
Inkpen, and Si Wei. 2018. Neural natural language
inference models enhanced with external knowl-
edge. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 2406–2417. Associa-
tion for Computational Linguistics.

Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016.
Long short-term memory-networks for machine
reading. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Process-
ing, pages 551–561.

Christopher Clark and Matt Gardner. 2018. Simple
and effective multi-paragraph reading comprehen-
sion. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Long
Papers), pages 845–855.

Bhuwan Dhingra, Qiao Jin, Zhilin Yang, William Co-
hen, and Ruslan Salakhutdinov. 2018. Neural mod-
els for reasoning over multiple mentions using coref-
In Proceedings of the 2018 Conference of
erence.

the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers), volume 2, pages
42–48.

Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang, William
Cohen, and Ruslan Salakhutdinov. 2017. Gated-
In Pro-
attention readers for text comprehension.
ceedings of the 55th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), volume 1, pages 1832–1846.

Bradley Efron and Robert J Tibshirani. 1994. An intro-

duction to the bootstrap. CRC press.

Marjan Ghazvininejad, Chris Brockett, Ming-Wei
Chang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, and
Michel Galley. 2018. A knowledge-grounded neural
conversation model. In AAAI.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems, pages 1693–
1701.

Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and Jan
Kleindienst. 2016. Text understanding with the at-
tention sum reader network. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics, pages 908–918.

Diederik P Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In 3rd Interna-
tional Conference for Learning Representations.

Tom´aˇs Koˇcisk`y, Jonathan Schwarz, Phil Blunsom,
Chris Dyer, Karl Moritz Hermann, G´aabor Melis,
and Edward Grefenstette. 2018. The narrativeqa
Transactions
reading comprehension challenge.
of the Association of Computational Linguistics,
6:317–328.

J Richard Landis and Gary G Koch. 1977. The mea-
surement of observer agreement for categorical data.
biometrics, pages 159–174.

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. Text Summarization
Branches Out.

Todor Mihaylov and Anette Frank. 2018. Knowledge-
able reader: enhancing cloze-style reading compre-
hension with external commonsense knowledge. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 821–832. Association for Com-
putational Linguistics.

Eric W Noreen. 1989. Computer-intensive methods for

testing hypotheses. Wiley New York.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the

40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311–318. Association for
Computational Linguistics.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of NAACL.

Soujanya Poria, Erik Cambria, Alexander Gelbukh,
Federica Bisio, and Amir Hussain. 2015. Sentiment
data ﬂow analysis by means of dynamic linguistic
IEEE Computational Intelligence Maga-
patterns.
zine, 10(4):26–36.

Soujanya Poria, Iti Chaturvedi, Erik Cambria, and Fed-
erica Bisio. 2016. Sentic LDA: Improving on LDA
with semantic similarity for aspect-based sentiment
In Neural Networks (IJCNN), 2016 In-
analysis.
ternational Joint Conference on, pages 4465–4473.
IEEE.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, pages 2383–2392.

Abigail See, Peter J Liu, and Christopher D Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), volume 1,
pages 1073–1083.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2017. Bidirectional attention
ﬂow for machine comprehension. In ICLR.

Robyn Speer and Catherine Havasi. 2012. Represent-
ing general relational knowledge in ConceptNet 5.
In LREC, pages 3679–3686.

Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.
2015. End-to-end memory networks. In Advances
in Neural Information Processing Systems, pages
2440–2448.

Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2018.
Multi-range reasoning for machine comprehension.
arXiv preprint arXiv:1803.09074.

Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. 2015. Cider: Consensus-based image de-
In Proceedings of the IEEE
scription evaluation.
Conference on Computer Vision and Pattern Recog-
nition, pages 4566–4575.

Qiu-Feng Wang, Erik Cambria, Cheng-Lin Liu, and
Amir Hussain. 2013. Common sense knowledge
for handwritten chinese text recognition. Cognitive
Computation, 5(2):234–242.

Dirk Weissenborn, Tom´aˇs Koˇcisk`y, and Chris Dyer.
2017. Dynamic integration of background knowl-
arXiv preprint
edge in neural NLU systems.
arXiv:1706.02596.

Johannes Welbl, Pontus Stenetorp, and Sebastian
Riedel. 2018. Constructing datasets for multi-hop
reading comprehension across documents. Transac-
tions of the Association of Computational Linguis-
tics, 6:287–302.

Jason Weston, Antoine Bordes, Sumit Chopra, Alexan-
der M Rush, Bart van Merri¨enboer, Armand Joulin,
and Tomas Mikolov. 2016. Towards ai-complete
question answering: A set of prerequisite toy tasks.
In ICLR.

Zhen Xu, Bingquan Liu, Baoxun Wang, Chengjie Sun,
and Xiaolong Wang. 2017.
Incorporating loose-
structured knowledge into LSTM with recall gate for
conversation modeling. In International Joint Con-
ference on Neural Networks.

Tom Young, Erik Cambria, Iti Chaturvedi, Minlie
Huang, Hao Zhou, and Subham Biswas. 2018. Aug-
menting end-to-end dialog systems with common-
In Proceedings of the Thirty-
sense knowledge.
Second AAAI Conference on Artiﬁcial Intelligence
(AAAI-18).

A Supplemental Material

A.1 Experimental Setup

Datasets We test our model with and with-
out commonsense addition on two challenging
datasets that require multi-hop reasoning and ex-
ternal knowledge: NarrativeQA (Koˇcisk`y et al.,
2018) and QAngaroo-WikiHop (Welbl et al.,
2018). NarrativeQA is a generative QA dataset
where the passages are either stories or summaries
of stories, and the questions ask about complex
aspects of the narratives such as event timelines,
characters, relations between characters, etc. Each
question has two answers which are generated by
human annotators and usually cannot be found
in the passage directly. We focus on the sum-
mary subtask in this paper, where summaries have
lengths of up to 1000 words.

We also test our model on WikiHop, a fact
based, multi-hop dataset. Questions in WikiHop
often require a model to read several documents
in order to obtain an answer. We focus on the
multiple-choice part of WikiHop, where models
are tasked with picking the correct response from
a pool of candidates. We rank candidate responses
by calculating their generation probability based
on our model. As this is a multi-document QA
task, we ﬁrst rank the candidate documents via TF-
IDF cosine distance with the question, and then
take the top k documents such that their combined
length is less than 1300 words.

paths have high activation, but the activation be-
comes more focused on the passage’s key words
w.r.t. the question, as the number of hops increase.

Evaluation Metrics We evaluate NarrativeQA
on the metrics proposed by its original authors:
Bleu-1, Bleu-4 (Papineni et al., 2002), ME-
TEOR (Banerjee and Lavie, 2005) and Rouge-
L (Lin, 2004). We also evaluate on CIDEr (Vedan-
tam et al., 2015) as it places emphasize on annota-
tor consensus. For WikiHop, we evaluate on accu-
racy.

Training Details
In training for both datasets,
we minimize the negative log probability of gener-
ating the ground-truth answer with the Adam opti-
mizer (Kingma and Ba, 2015) with an initial learn-
ing rate of 0.001, a dropout-rate of 0.2 (dropout is
applied to the input of each RNN layer) and batch
size of 24. We use 256 dimensional word embed-
dings and a hidden size of 128 for all RNNs and
k = 3 hops of multi attention. At inference time
we use greedy decoding to generate the answer.
For both NarrativeQA and WikiHop, we reached
these parameters via tuning on the full, ofﬁcial val-
idation set.

A.2 Commonsense Extraction Examples

In Tables 8, 9, and 10 (see next page), we demon-
strate extracted commonsense examples for ques-
tions that require commonsense to reach an an-
swer. We bold words in the question and in the ex-
tracted commonsense in cases where the common-
sense knowledge explicitly bridges gaps between
implicitly connected words in the context or ques-
tion. The relevant context is also displayed, with
context words that are key to answering the ques-
tion (via commonsense) marked in bold. These are
then followed by a context visualization described
in the next section.

A.3 Commonsense Integration Visualization

We also visualize how much commonsense infor-
mation is integrated into each part of the context
by providing a visualization of the zi value (see
end of Sec. 3.3 of main ﬁle) for i ∈ {1, 2, 3},
which is the gate value signifying how much
commonsense-attention representation is used in
the output context representation. In the follow-
ing examples (next page), we use shades of blue
to represent the average of (1 − zi) at each word
in the context (normalized within each hop), with
deeper blue indicating the use of more common-
sense information. As a general trend, we see that
in the earlier hops, words which are near tokens
that occur in both the context and commonsense

Figure 3: Example 1 visualized activation values of ﬁrst attention hop (1 − z1).

Figure 4: Example 1 visualized activation values of second attention hop (1 − z2).

Commonsense Extraction and Visualization Examples

Question

What shore does Michael’s corpse wash up on?

Context

”..as the play begins nora and cathleen receive word from the priest that a
body, that may be their brother michael, has washed up on shore in donegal,
the island farthest north of their home island of inishmaan..”

Answers

the shore of donegal / donegal

Extracted
Commonsense

up → RelatedTo → wind → Antonym → her → RelatedTo → person
up → RelatedTo → north → RelatedTo → up
wash → RelatedTo → up
up → Antonym → down
wash → RelatedTo → water → PartOf → sea → RelatedTo → ﬁsh
up → RelatedTo → wind
wash → RelatedTo → water → PartOf → sea
shore → RelatedTo → sea
wash → RelatedTo → body
wash → Antonym → making
up → Antonym → down → Antonym → up
wash → RelatedTo → water → PartOf → sea → MadeOf → water
up → RelatedTo → wind → Antonym → her
wash → RelatedTo → water
up → RelatedTo → south
shore → RelatedTo → sea → MadeOf → water → AtLocation → bucket → RelatedTo → horse
wash → RelatedTo → clothing
wash → RelatedTo → water → PartOf → sea → MadeOf → water → PartOf → sea
shore → RelatedTo → sea → MadeOf → water
wash → Antonym → getting
up → RelatedTo → north
corpse → RelatedTo → body
shore → RelatedTo → sea → MadeOf → water → AtLocation → fountain
corpse → RelatedTo → body → RelatedTo → corpse
corpse → RelatedTo → body → RelatedTo → water
wash → HasContext → west
up → RelatedTo → wind → Antonym → her → RelatedTo → person → MadeOf → water
up → RelatedTo → wind → AtLocation → sea
wash → RelatedTo → water → AtLocation → can
shore → RelatedTo → sea → MadeOf → water → AtLocation → bucket
wash → RelatedTo → will
shore → RelatedTo → sea → MadeOf → water → AtLocation → fountain → RelatedTo → water

Table 8: Example 1 selected commonsense paths.

Figure 5: Example 1 visualized activation values of third attention hop (1 − z3).

Question

Context

Answers

What species lives in the nearby mines?

”..the nearby mines are inhabited by a race of goblins..”

the goblins / goblins.

Extracted
Commonsense

species → RelatedTo → kingdom → RelatedTo → queen
species → RelatedTo → kingdom → RelatedTo → queen → UsedFor → people → HasA → feet
mines → FormOf → mine
lives → FormOf → life
mines → FormOf → mine → AtLocation → home → RelatedTo → person
species → RelatedTo → kingdom → RelatedTo → queen → UsedFor → people
species → RelatedTo → kingdom → DerivedFrom → king → RelatedTo → master
species → RelatedTo → kingdom → RelatedTo → queen → RelatedTo → person → Desires → feet
mines → FormOf → mine → AtLocation → home → RelatedTo → line → RelatedTo → thread
species → RelatedTo → kingdom → DerivedFrom → king → RelatedTo → leader → AtLocation
→ company
species → RelatedTo → kingdom
species → RelatedTo → kingdom → DerivedFrom → king → RelatedTo → leader
species → RelatedTo → kingdom → DerivedFrom → king
mines → FormOf → mine → AtLocation → home → RelatedTo → line
species → RelatedTo → race
mines → FormOf → mine → AtLocation → home
species → RelatedTo → kingdom → RelatedTo → queen → RelatedTo → person
species → RelatedTo → kingdom → DerivedFrom → king → RelatedTo → master → RelatedTo
→ young

Table 9: Example 2 selected commonsense paths.

Figure 6: Example 2 visualized activation values of ﬁrst attention hop (1 − z1).

Figure 7: Example 2 visualized activation values of second attention hop (1 − z2).

Figure 8: Example 2 visualized activation values of third attention hop (1 − z3).

Question

What duty does ruth have to fulﬁll when her aunt dies?

Context

”..ruth anvoy, a young american woman with a wealthy father, comes to britain to
visit her widowed aunt lady coxon..”
”..having made a promise to her now-deceased husband, lady coxon has for
years been seeking to bestow a sum of 13,000 pounds upon a talented
intellectual whose potential has been hampered by lack of money. having failed to
ﬁnd such a person, lady coxon tells anvoy that upon her death the money will be
left to her, and she must carry on the quest..”
”..anvoy, having lost nearly all her wealth, has only the 13,000 pounds from
lady coxon, with a moral but not legal obligation to give it away..”
”..she awards the coxon fund to saltram, who lives off it exactly as he lived off
his friends, producing nothing of intellectual value..”

Answers

she must give away the 13,000 pounds to an appropriate recipient. /
bestow 13000 to the appropriate person

Extracted
Commonsense

duty → RelatedTo → moral → Antonym → immoral
duty → RelatedTo → time → IsA → money
duty → RelatedTo → time → IsA → money → AtLocation → church
duty → DistinctFrom → off
duty → RelatedTo → time → IsA → money → CapableOf → pay → bills → MotivatedByGoal
→ must
duty → RelatedTo → time → IsA → money → AtLocation → church → RelatedTo → house
duty → RelatedTo → must → RelatedTo → having → RelatedTo → estate → RelatedTo → real
her → RelatedTo → woman → RelatedTo → lady → RelatedTo → plate → Antonym → her
duty → RelatedTo → moral → RelatedTo → will → RelatedTo → choose → IsA → decide
duty → RelatedTo → must → RelatedTo → having → RelatedTo → estate
duty → RelatedTo → obligation
duty → RelatedTo → moral → RelatedTo → will → IsA → purpose
her → RelatedTo → but → DistinctFrom → only → RelatedTo → child → RelatedTo → particularly
her → RelatedTo → person → RelatedTo → others → RelatedTo → people
her → Antonym → him → RelatedTo → he → RelatedTo → person → Desires → conversation
her → RelatedTo → woman → RelatedTo → lady
her → RelatedTo → woman → RelatedTo → she
duty → RelatedTo → must → RelatedTo → having → RelatedTo → own → RelatedTo → having
her → RelatedTo → person → DistinctFrom → man → Antonym → people
her → RelatedTo → but → DistinctFrom → only → RelatedTo → child
her → Antonym → him → RelatedTo → he → RelatedTo → person
her → Antonym → his → RelatedTo → him → RelatedTo → person

Table 10: Example 3 selected commonsense paths.

Figure 9: Example 3 visualized activation values of ﬁrst attention hop (1 − z1).

Figure 10: Example 3 visualized activation values of second attention hop (1 − z2).

Figure 11: Example 3 visualized activation values of third attention hop (1 − z3).

