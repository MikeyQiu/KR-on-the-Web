Video Object Segmentation using Teacher-Student Adaptation
in a Human Robot Interaction (HRI) Setting

Mennatullah Siam1, Chen Jiang1, Steven Lu1, Laura Petrich1,
Mahmoud Gamal2, Mohamed Elhoseiny3, Martin Jagersand1

9
1
0
2
 
r
a

M
 
2
1
 
 
]

V
C
.
s
c
[
 
 
4
v
3
3
7
7
0
.
0
1
8
1
:
v
i
X
r
a

Abstract— Video object segmentation is an essential task
in robot manipulation to facilitate grasping and learning
affordances. Incremental learning is important for robotics in
unstructured environments. Inspired by the children learning
process, human robot interaction (HRI) can be utilized to teach
robots about the world guided by humans similar to how
children learn from a parent or a teacher. A human teacher can
show potential objects of interest to the robot, which is able
to self adapt to the teaching signal without providing man-
ual segmentation labels. We propose a novel teacher-student
learning paradigm to teach robots about their surrounding
environment. A two-stream motion and appearance ”teacher”
network provides pseudo-labels to adapt an appearance ”stu-
dent” network. The student network is able to segment the
newly learned objects in other scenes, whether they are static
or in motion. We also introduce a carefully designed dataset
that serves the proposed HRI setup, denoted as (I)nteractive
(V)ideo (O)bject (S)egmentation. Our IVOS dataset contains
teaching videos of different objects, and manipulation tasks.
Our proposed adaptation method outperforms the state-of-the-
art on DAVIS and FBMS with 6.8% and 1.2% in F-measure
respectively. It improves over the baseline on IVOS dataset with
46.1% and 25.9% in mIoU.

I. INTRODUCTION

The robotics and vision communities greatly improved
video object segmentation over the recent years. The main
approaches in video object segmentation could be catego-
rized into semi-supervised and unsupervised approaches. In
semi-supervised video object segmentation approaches (e.g.,
[34][2][12], the method is initialized manually by a segmen-
tation mask in the ﬁrst few frames, then the segmented object
is tracked throughout the video sequence. On the other hand,
unsupervised methods [14][32][9][31] attempt to discover
the primary object automatically and segment it through the
video sequence. Motion is one of the fundamental cues that
can help improve unsupervised video object segmentation.
While there has been recent success in deep learning ap-
proaches for segmenting motion (e.g., [32][9][31]), current
approaches depend mainly on prior large-scale training data.
Video semantic segmentation for robotics is widely used
in different applications such as autonomous driving [4][26],
and robot manipulation [5][10]. Object segmentation can aid
in grasping, manipulating objects, and learning object affor-
dances [5]. In robot manipulation, learning to segment new

1Mennatullah Siam, Chen Jian, Steven Lu, Laura Petrich and Mar-
tin Jagersand are with the University of Alberta, Canada. e-mail: men-
natul@ualberta.ca.

2Mahmoud Gamal is with Cairo University, Egypt.
3 Mohamed El-Hoseiny is with Facebook AI Research. e-mail: elho-

seiny@fb.com.

Fig. 1: Overview of the proposed Teacher-Student adaptation
method for video object segmentation. The teacher model
based on motion cues is able to provide pseudo-labels to
adapt
the student model. Blue: conﬁdent positive pixels.
Cyan: ignored region in the adaptation.

objects incrementally, has signiﬁcant importance. Real world
environments have far more objects and more appearance
variation than can be feasibly trained a-priori. Current large-
scale datasets such as Image-Net [15] do not cover this.

A recent

trend in robotics is toward human-centered
artiﬁcial intelligence. Human-centered AI involves learning
by instruction using a human teacher. Such human-robot
interaction (HRI) mimics children being taught novel con-
cepts from few examples [18]. In the robotic setting, a
human teacher demonstrates an object by moving it and
showing different poses, while verbally or textually teaching
its label. The robot is then required to segment the objects
in other settings where it is either static or manipulated by
the human or the robot itself. We demonstrated this HRI
setting in our team submission to the KUKA Innovation
Challenge at the Hannover Fair [25]. This HRI setting has
few differences to conventional video object segmentation:
(1) Abundance of the different poses of the object. (2)
The existence of different instances/classes within the same
category. (3) Different challenges introduced by cluttered
backgrounds, different rigid and non-rigid transformations,
occlusions and illumination changes. In this paper, we focus
on these robotics challenges and provide a new dataset and
a new method to study such a scenario.

We collected a new dataset to benchmark (I)nteractive
(V)ideo (O)bject (S)egmentation in the HRI scenario. The
dataset contains two types of videos: (1) A human teacher

showing different household objects in varying poses for
interactive learning. (2) Videos of the same objects used
in a kitchen setting while serving and eating food. The
objects occur both as static objects and active objects being
manipulated. Manipulation was performed by both humans
and robots. The aim of this dataset is to facilitate incremen-
tal learning and immediate use in a collaborative human-
robot environments, such as assistive robot manipulation.
Datasets that have a similar setting such as ICUBWorld
transformations dataset [22], and the Core50 dataset [17]
were proposed. These datasets include different instances
within the same category. They benchmark solutions to
object recognition in a similar HRI setting but do not provide
segmentation annotations unlike our dataset. Other datasets
were concerned with the activities of daily living such as
the ADL dataset [24]. The dataset was comprised of ego-
centric videos for activities. However, such ADL datasets do
not contain the required teaching videos to match the HRI
setting we are focusing on. Table I summarizes the most
relevant datasets suited to the HRI setting.

The main contribution of our collected IVOS dataset is
providing the manipulation tasks setting with objects being
manipulated by humans or a robot. In addition to providing
segmentation annotation for both teaching videos and ma-
nipulation tasks. It enables researchers to analyze the effect
of different transformations such as translation, scale, and
rotation on the incremental learning of video object segmen-
tation. It acts as a benchmark for interactive video object
segmentation in the HRI setting. It also provides videos of
both human and similarly robot manipulation tasks with the
segmentation annotations along with the corresponding robot
trajectories. Thus, it enables further research in learning robot
trajectories from visual cues with semantics.

We propose a novel teacher-student adaptation method
based on motion cues for video object segmentation. Our
method enables a human teacher to demonstrate objects mov-
ing with different transformations and associates them with
labels. During inference, our approach can learn to segment
the object without manual segmentation annotation. The
teacher model is a fully convolutional network that combines
motion and appearance, denoted as “Motion+Appearance“.
The adapted student model
is a one-stream appearance-
only fully convolutional network denoted as “Appearance“.
Combining motion and appearance in the teacher network
allows the creation of pseudo-labels for adapting the student
network. Our work is inspired from the semi-supervised
on-line method [34]. This work uses manual segmentation
masks for initialization. Instead, our approach tackles a
more challenging problem and does not require manual
segmentation; it relies on the pseudo-labels provided by the
teacher model. Figure 1 shows an overview of the proposed
method. The two main reasons behind using the adaptation
targets from the teacher model is: (1) The student model is
more computationally efﬁcient. The inference and adaptation
time for the teacher model is 1.5x of the student model’s. The
adaptation occurs only once on the ﬁrst frame, then the more
efﬁcient student model can be used for inference. (2) The

TABLE I: Comparison of different datasets. T:Turntable,
H:handheld

Dataset
RGB-D [27]
BIG BIRD [28]
ICUB 28 [21]
ICUB World [22]
Core50 [17]
IVOS

Sess.
-
-
4
6
11
12

Cat.
51
-
7
20
10
12

Obj.
300
100
28
200
50
36

Acq.
T
T
H
H
H
H

Tasks
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:51)

Seg.
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:51)

teacher model can be used to generate pseudo-labels for the
potential object of interest. It does not Urequire the human
to provide manual segmentation mask during the teaching
phase which provides a natural
interface to the human.
Consequently, the adapted student model can segment the
object of the interest whether it is static or moving. If the
adapted model was still dependant on optical ﬂow it will
only be able to recognize the object in motion.

Our proposed method outperforms the state-of-the-art on
the popular DAVIS [23] and FBMS [19] benchmarks with
6.8% and 1.2% in F-measure respectively. On our new
IVOS dataset results show the motion adapted network
outperforms the baseline with 46.1% and 25.9% in mIoU
on Scale/Rotation and Manipulation Tasks respectively. Our
code 1 and IVOS dataset 2 are publicly available. A video
description and demonstration is available at 3. Our main
contributions are :

• Providing a Dataset for Interactive Video Object Seg-
mentation (IVOS) in a Human-Robot Interaction set-
ting, and including manipulation tasks unlike previous
datasets.

• A teacher-student adaptation method is proposed to
learn new objects from a human teacher without pro-
viding manual segmentation labels. We propose a novel
pseudo-label adaptation based on a teacher model that
is dependant on motion. Adaptation with discrete and
continuous pseudo-labels are evaluated to demonstrate
different adaptation methods.

II. IVOS DATASET

the dataset

We collected IVOS for the purpose of benchmarking
(I)nteractive (V)ideo (O)bject (S)egmentation in the HRI
setting. We collect
in two different settings:
(1) Human teaching objects. (2) Manipulation tasks setting.
Unlike previous datasets in human robot interaction IVOS
dataset provides video sequences for manipulation tasks.
In addition to providing segmentation annotation for both
teaching videos and manipulation tasks.

A. Human Teaching Objects

For teaching, videos are collected while a human moves an
object with her hand. The unstructured human hand motion
naturally provides different views of the object and samples

1https://github.com/MSiam/motion_adaptation
2https://msiam.github.io/ivos/
3https://youtu.be/36hMbAs8e0c

Fig. 2: Samples of collected Dataset IVOS, Teaching Objects Setting. (a) Translation split. (b) Scale split. (c) Planar Rotation
split. (d) Out-of-plane Rotation. (e) Non rigid transformations.

(a)

(b)

(c)

(d)

(a)

(b)

(c)

(d)

Fig. 3: Samples of collected IVOS dataset, Robot manipulation Tasks Setting with segmentation annotation. Manipulation
Tasks: (a) Drinking. (b) Stirring. (c) Pouring Water. (d) Pouring Cereal.

different geometric transformations. We provide transforma-
tions such as translation, scale, planar rotation, out-of-plane
rotation, and other transformations such as opening the lid
of a bottle. Two illumination conditions are provided: day-
light and indoor lighting, which sums up to 10 sessions of
recording for both illumination and transformations. Figure 2
shows a sample for the objects being captured under different
transformations with the segmentation masks. In each session
a video for the object held by a human with relatively
cluttered scene background is recorded.

A GRAS-20S4C-C ﬁre-wire camera is used to record the
data along with a Kinect sensor [29]. The collected data is
annotated manually with polygonal masks using the VGG
Image Annotator tool [6]. The ﬁnal teaching videos contains
12 object categories, with a total of 36 instances under
these categories. The detection crops are provided for all
the frames, while the segmentation masks are provided for
20 instances with ∼ 18,000 annotated masks.

B. Manipulation Tasks Setting

The manipulation task benchmark includes two video
categories: one with human manipulation, and the other with
robot manipulation. Activities of Daily Living (ADL) such
as food preparation are the focus for the recorded tasks.
The aim of this benchmark is to further improve perception
systems in robotics for assisted living. Robot trajectories
are created through kinesthetic teaching, and the robot pose
way-points are provided in the dataset. In order to create

typical robot velocity and acceleration, proﬁles trajectories
were generated from these way-points using splines as is
standard in robotics.

The collected sequences are further annotated with seg-
mentation masks similar to the teaching objects setting.
Figure 3 shows some of the recorded frames with ground-
truth annotations. It covers 4 main manipulation tasks:
cutting, pouring, stirring, and drinking for both robot and
human manipulation covering a total of 56 tasks. The dataset
contains ∼ 8,900 frames with segmentation masks, along
with the recorded robot trajectories to enable further research
on how to learn these trajectories from visual cues.

A. Baseline Network Architecture

III. METHOD

The student model in this work is built on the wide ResNet
architecture presented in [36]. The network is comprised
of 16 residual blocks. Dilated convolution [37] is used to
increase the receptive ﬁeld without decreasing the resolution.
The output from the network is bilinearly upsampled to
the initial image resolution. The loss function used is boot-
strapped cross entropy [35], which helps with class imbal-
ance. It computes the cross entropy loss from a fraction of the
hardest pixels. Pre-trained weights on PASCAL dataset for
objectness is used from [34], to help the network generalize
to different objects in the scene. Then it is trained on DAVIS
training set, the student model without adaptation is denoted
as the baseline model throughout the paper.

Fig. 4: Motion Adaptation of fully convolutional residual networks pipeline.

The teacher network incorporates motion from optical
ﬂow, where a two-stream wide ResNet for motion and
appearance is used. Each stream contains 11 residual blocks
for memory efﬁciency reasons. The output feature maps
are combined by multiplying the output activation maps
from both motion and appearance streams. After combining
features another 5 residual blocks are used with dilated con-
volution. The input to the motion stream is the optical ﬂow
computed using [16], and converted into RGB representation
using the Sintel color wheel representation [1].

B. Teacher-Student Adaptation using Pseudo-labels

There is an analogy between this work and the work
in [33], where a student method is learning to mimic a
teacher method. In our work the teacher method is a motion
dependent one, and the student method tries to mimic the
teacher during inference through motion adaptation. The
teacher-student training helps the network understands the
primary object
in the scene in an unsupervised manner.
Unlike the work in [34] that ﬁrst ﬁne-tunes the network
based on the manual segmentation mask then adapts it on-
line with the most conﬁdent pixels. Our method provides a
natural human robot interaction that does not require manual
labelling for initialization.

Our approach provides two different adaptation methods,
adapting based on discrete or continuous labels. The teacher
network pseudo-labels are initially ﬁltered to remove parts
representing the human moving using the output human
segmentation from Mask R-CNN [8]. When discrete labels
are used it is based on pseudo-labels from the conﬁdent
pixels in the teacher network output. Such a method provides
superior accuracy, but on the expense of tuning the param-
eters that determine these conﬁdent pixels. Another method
that utilizes continuous labels adaptation from the teacher
network is also introduced. This method alleviates the need
for any hyper-parameter tuning but on the cost of degraded
accuracy. Figure 4 summarizes the adaptation scheme, and
shows the output pseudo-labels,
the output segmentation
before and after adaptation.

In the case of discrete pseudo-labels, the output probability
maps from the teacher network is further processed in a
similar fashion to the semi-supervised method [34]. Initially
the conﬁdent positive pixels are labeled, then a geometric
distance transform is computed to label the most conﬁdent
negative pixels as shown in Algorithm 1.

Algorithm 1 Motion Adaptation Algorithm.
Input: X: images used for teaching. N: number of samples
used. Mteacher: Teacher Model. Mstudent: Student Model.
Output: `Mstudent: Adapted Student Model.
1: function TEACH(N , X, Mteacher, Mstudent)
2:
3:
4:
5:
6: end function

Pi = Mteacher(Xi)
`Mstudent = Adapt(Pi, Mstudent)

for i in N do

end for

Discrete Labels Adaptation Method

7: function ADAPT(At, Mstudent)
Mask ← IGNORED
8:
pos indices ← (At > POS TH )
9:
dt ← DITANCE TRANSFORM(Mask)
10:
neg indices ← (dt > NEG DT TH )
Mask[pos indices] ← 1, Mask[neg indices] ← 0

11:
12:

return ﬁnetune(Mstudent,Mask)

13: end function

In the case of continuous labels, the output probability
maps are used without further processing. This has the
advantage of not using any hyper-parameters or discrete label
segmentation. It generalizes better to different scenarios on
the expense of degraded accuracy. Inspiring from the relation
between cross entropy and KL-divergence as in equations
1. The cross entropy loss can be viewed as a mean to
decrease the divergence between the true distribution p and
the predicted one q, in addition to the uncertainty implicit in
H(p). In our case the true distribution is the probability maps
from the teacher network, while the predicted is the student

network output. Figure 5 shows the difference between
the pseudo-labels for both discrete and continuous variants.
Conditional random ﬁelds is used as a post-processing step
on DAVIS and FBMS.

(a)

(c)

(b)

(d)

Fig. 5: (a,b) Discrete adaptation targets (pseudo-labels), cyan
is the unknown region, blue is the conﬁdent positive pixels.
(c, d) Continuous adaptation targets.

DKL(p|q) =

pi log

(cid:88)

i

pi
qi

DKL(p|q) =

pi log

− H(p)

(cid:88)

i

1
qi

H(p, q) = H(p) + DKL(p|q)

(1a)

(1b)

(1c)

IV. EXPERIMENTAL RESULTS

A. Experimental Setup

For all experiments the DAVIS training data is used to
train our Appearance model and the Appearance+Motion
model. The optimization method used is Adam [13] with
learning rate 10−6 during training, and 10−5 during on-line
adaptation. In on-line adaptation 15 iterations are used in the
scale/rotation experiments and 50 in the tasks experiments.
Adaptation is only conducted once at the initialization of
the video object segmentation. The positive threshold used
to identify highly conﬁdent positive samples is 0.8, and the
negative threshold distance to the foreground mask is 220
in case of DAVIS benchmark. Since IVOS is recorded in an
indoor setup, a negative distance threshold of 20 is used.

F-measure, and 1% in mIoU. Table III shows quantitative
results on FBMS dataset, where our MotAdapt outperforms
the state of the art with 1.2% in F-measure and 10% in recall.
Figure 6 shows qualitative results on FBMS highlighting
the improvement gain from motion adaptation compared to
LVO [32]. Figure 7 shows qualitative evaluation on DAVIS,
where it demonstrates the beneﬁt from motion adaptation
compared to the baseline (top row), and compared to LVO
[32] and ARP [14] (bottom row).

C. Video Object Segmentation in HRI Setting

Our method is evaluated in the HRI scenario on our
dataset IVOS. The teaching is performed on the translation
sequences, with only the ﬁrst two frames used to generate
pseudo-labels for adaptation. An initial evaluation is con-
ducted on both scale and rotation sequences, in order to
assess the adaptation capability to generalize to different
poses and transformations. Table IV shows the comparison
between the baseline method without adaptation, and the
two variants of motion adaptation on the scale, rotation and
tasks sequences. The discrete and continuous variants for
our motion adaptation outperform the baseline with 54.5%
and 49.3% respectively on the scale sequences. Similarly
on the rotation sequences it outperforms the baseline with
37.7% and 35.7% respectively. The main reason for this
large gap, is that general segmentation methods will segment
all objects in the scene as foreground, while our teaching
that was
method adaptively learns the object of interest
demonstrated by the human teacher.

All manipulation tasks sequences where the category bot-
tle existed is evaluated and cropped to include the working
area. Our method outperforms the baseline on the tasks with
25.9%. The ﬁrst variant of our adaptation method generally
outperforms the second variant with continuous labels adap-
tation. However the second variant has the advantage that it
can work on any setting such as DAVIS and IVOS without
tuning any hyper-parameters. Figure 8 shows the output from
our adaptation method when it is recognized by the robot,
and while the robot has successfully manipulated that object.

V. CONCLUSIONS
In this paper we proposed a novel approach for visual
learning by instruction. Our proposed motion adaptation
(MotAdapt) method provides a natural interface to teaching
robots to segment novel object instances. This enables robots
to manipulate and grasp these objects. Two variants of the
adaptation scheme is experimented with. Our results show
that Mot-Adapt outperforms the state of the art on DAVIS
and FBMS and outperforms the baseline on IVOS dataset.

B. Generic Video Object Segmentation

REFERENCES

In order to evaluate the performance of our proposed
motion adaptation (MotAdapt) method with respect to the
state-of-the-art, we experiment on generic video object seg-
mentation datasets. Table II shows quantitative analysis on
DAVIS benchmark compared to the state-of-the-art unsuper-
vised methods. One of the variants of MotAdapt based on
discrete labels outperforms the state of the art with 6.8% in

[1] S. Baker, D. Scharstein, J. Lewis, S. Roth, M. J. Black, and R. Szeliski,
“A database and evaluation methodology for optical ﬂow,” Interna-
tional Journal of Computer Vision, vol. 92, no. 1, pp. 1–31, 2011.
[2] S. Caelles, K.-K. Maninis, J. Pont-Tuset, L. Leal-Taix´e, D. Cremers,
and L. Van Gool, “One-shot video object segmentation,” arXiv preprint
arXiv:1611.05198, 2016.

[3] J. Cheng, Y.-H. Tsai, S. Wang, and M.-H. Yang, “Segﬂow: Joint learn-
ing for video object segmentation and optical ﬂow,” arXiv preprint
arXiv:1709.06750, 2017.

TABLE II: Quantitative comparison on DAVIS benchmark. MotAdapt-1: Continuous Labels, MotAdapt-2: Discrete Labels.

Measure

J

F

Mean
Recall
Decay
Mean
Recall
Decay

NLC[7]
55.1
55.8
12.6
52.3
51.9
11.4

SFL[3]
67.4
81.4
6.2
66.7
77.1
5.1

LMP [31]
70.0
85.0
1.3
65.9
79.2
2.5

FSeg [9]
70.7
83.5
1.5
65.3
73.8
1.8

LVO [32]
75.9
89.1
0.0
72.1
83.4
1.3

ARP [14]
76.2
91.1
0.0
70.6
83.5
7.9

Baseline MOTAdapt-1 MOTAdapt-2
74.0
85.7
7.0
74.4
81.6
0.0

77.2
87.8
5.0
77.4
84.4
3.3

75.3
87.1
5.0
75.3
83.8
3.3

TABLE III: Quantitative results on FBMS dataset (test set).

Measure
P
R
F

FST [20]
76.3
63.3
69.2

CVOS [30]
83.4
67.9
74.9

83.1
71.5
76.8

81.4
73.9
77.5

LVO[32]
92.1
67.4
77.8

Base
80.8
76.1
78.4

ours
80.7
77.4
79.0

CUT [11] MPNet-V[31]

TABLE IV: mIoU on IVOS over the different transformations and tasks. IVOS dataset teaching is conducted on few samples
from the translation, then evaluating on scale, rotation and manipulation tasks. MotAdapt-1: Continuous Labels. MotAdapt-2:
Discrete Labels.

Model
Baseline
MotAdapt-1
Mot-Adapt-2

Scale
14.5
63.8
69.0

13.8
49.5
51.5

14.7
30.2
40.6

Rotation Manipulation Tasks

O
V
L

t
p
a
d
A
t
o
M

Fig. 6: Qualitative Evaluation on the FBMS dataset. Top: LVO [32]. Bottom: ours.

(a)

(b)

(c)

(d)

Fig. 7: Qualitative evaluation on DAVIS16. (a) LVO [32]. (b) ARP [14]. (c) Baseline. (d) MotAdapt.

(a)

(b)

(c)

(d)

Fig. 8: Qualitative evaluation on IVOS Manipulation Tasks Setting. (a) Teaching Phase, Discrete Labels. (b) Teaching Phase,
Continuous Labels. (c) Inference Phase before manipulation. (d) Inference Phase, during manipulation.

[28] A. Singh, J. Sha, K. S. Narayan, T. Achim, and P. Abbeel, “Bigbird:
instances,” in Robotics and
IEEE,

A large-scale 3d database of object
Automation (ICRA), 2014 IEEE International Conference on.
2014, pp. 509–516.

[29] J. Steward, D. Lichti, J. Chow, R. Ferber, and S. Osis, “Performance
assessment and calibration of the kinect 2.0 time-of-ﬂight range
camera for use in motion capture applications,” in Proceedings of the
Fig Working Week, 2015.

[30] B. Taylor, V. Karasev, and S. Soatto, “Causal video object segmen-
tation from persistence of occlusions,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2015, pp.
4268–4276.

[31] P. Tokmakov, K. Alahari, and C. Schmid, “Learning motion patterns

in videos,” arXiv preprint arXiv:1612.07217, 2016.

[32] ——, “Learning video object segmentation with visual memory,” arXiv

preprint arXiv:1704.05737, 2017.

[33] G. Urban, K. J. Geras, S. E. Kahou, O. Aslan, S. Wang, R. Caruana,
A. Mohamed, M. Philipose, and M. Richardson, “Do deep convolu-
tional nets really need to be deep and convolutional?” arXiv preprint
arXiv:1603.05691, 2016.

[34] P. Voigtlaender and B. Leibe, “Online adaptation of convolutional
neural networks for video object segmentation,” arXiv preprint
arXiv:1706.09364, 2017.

[35] Z. Wu, C. Shen, and A. v. d. Hengel, “Bridging category-level
and instance-level semantic image segmentation,” arXiv preprint
arXiv:1605.06885, 2016.

[36] ——, “Wider or deeper: Revisiting the resnet model for visual

recognition,” arXiv preprint arXiv:1611.10080, 2016.

[37] F. Yu and V. Koltun, “Multi-scale context aggregation by dilated

convolutions,” arXiv preprint arXiv:1511.07122, 2015.

[4] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Be-
nenson, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset
for semantic urban scene understanding,” in Proceedings of the IEEE
conference on computer vision and pattern recognition, 2016, pp.
3213–3223.

[5] T.-T. Do, A. Nguyen, I. Reid, D. G. Caldwell, and N. G. Tsagarakis,
“Affordancenet: An end-to-end deep learning approach for object
affordance detection,” arXiv preprint arXiv:1709.07326, 2017.

[6] A. Dutta, A. Gupta, and A. Zissermann, “VGG image annotator

(VIA),” http://www.robots.ox.ac.uk/ vgg/software/via/, 2016.

[7] A. Faktor and M. Irani, “Video segmentation by non-local consensus

voting.” in BMVC, vol. 2, no. 7, 2014, p. 8.

[8] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick, “Mask r-cnn,” in
Computer Vision (ICCV), 2017 IEEE International Conference on.
IEEE, 2017, pp. 2980–2988.

[9] S. D. Jain, B. Xiong, and K. Grauman, “Fusionseg: Learning to
combine motion and appearance for fully automatic segmention of
generic objects in videos,” arXiv preprint arXiv:1701.05384, 2017.

[10] J. Kenney, T. Buckley, and O. Brock, “Interactive segmentation for
manipulation in unstructured environments,” in Robotics and Automa-
tion, 2009. ICRA’09. IEEE International Conference on.
IEEE, 2009,
pp. 1377–1382.

[11] M. Keuper, B. Andres, and T. Brox, “Motion trajectory segmentation
via minimum cost multicuts,” in Proceedings of the IEEE International
Conference on Computer Vision, 2015, pp. 3271–3279.

[12] A. Khoreva, F. Perazzi, R. Benenson, B. Schiele, and A. Sorkine-
Hornung, “Learning video object segmentation from static images,”
arXiv preprint arXiv:1612.02646, 2016.

[13] D. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

arXiv preprint arXiv:1412.6980, 2014.

[14] Y. J. Koh and C.-S. Kim, “Primary object segmentation in videos based

on region augmentation and reduction.”

[15] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in Advances in neural
information processing systems, 2012, pp. 1097–1105.

[16] C. Liu et al., “Beyond pixels: exploring new representations and
applications for motion analysis,” Ph.D. dissertation, Massachusetts
Institute of Technology, 2009.

[17] V. Lomonaco and D. Maltoni, “Core50: a new dataset and benchmark
for continuous object recognition,” arXiv preprint arXiv:1705.03550,
2017.

[18] E. M. Markman, Categorization and naming in children: Problems of

induction. Mit Press, 1989.

[19] P. Ochs, J. Malik, and T. Brox, “Segmentation of moving objects by
long term video analysis,” IEEE transactions on pattern analysis and
machine intelligence, vol. 36, no. 6, pp. 1187–1200, 2014.

[20] A. Papazoglou and V. Ferrari, “Fast object segmentation in uncon-
strained video,” in Proceedings of the IEEE International Conference
on Computer Vision, 2013, pp. 1777–1784.

[21] G. Pasquale, C. Ciliberto, F. Odone, L. Rosasco, and L. Natale,
“Teaching icub to recognize objects using deep convolutional neural
networks,” in Machine Learning for Interactive Systems, 2015, pp.
21–25.

[22] G. Pasquale, C. Ciliberto, L. Rosasco, and L. Natale, “Object identi-
ﬁcation from few examples by improving the invariance of a deep
convolutional neural network,” in Intelligent Robots and Systems
(IROS), 2016 IEEE/RSJ International Conference on.
IEEE, 2016,
pp. 4904–4911.

[23] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and
A. Sorkine-Hornung, “A benchmark dataset and evaluation methodol-
ogy for video object segmentation,” in Computer Vision and Pattern
Recognition, 2016.

[24] H. Pirsiavash and D. Ramanan, “Detecting activities of daily living in
ﬁrst-person camera views,” in Computer Vision and Pattern Recogni-
tion (CVPR), 2012 IEEE Conference on.
IEEE, 2012, pp. 2847–2854.
[25] K. Robotics, “KUKA Innovation Award Challenge,” https://www.

youtube.com/watch?v=aLcw73dt Oo, 2018.

[26] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez, “The
synthia dataset: A large collection of synthetic images for semantic
segmentation of urban scenes,” in Proceedings of the IEEE conference
on computer vision and pattern recognition, 2016, pp. 3234–3243.

[27] M. Schwarz, H. Schulz, and S. Behnke, “Rgb-d object recognition and
pose estimation based on pre-trained convolutional neural network fea-
tures,” in Robotics and Automation (ICRA), 2015 IEEE International
Conference on.

IEEE, 2015, pp. 1329–1335.

