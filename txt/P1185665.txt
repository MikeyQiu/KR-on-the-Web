6
1
0
2
 
p
e
S
 
6
2
 
 
]
L
C
.
s
c
[
 
 
1
v
6
7
8
7
0
.
9
0
6
1
:
v
i
X
r
a

Lexicon-Free Fingerspelling Recognition from Video:
Data, Models, and Signer Adaptation

Taehwan Kima, Jonathan Keaneb, Weiran Wanga, Hao Tanga, Jason Riggleb, Gregory
Shakhnarovicha, Diane Brentarib, Karen Livescua

aToyota Technological Institute at Chicago
6045 S Kenwood Ave, Chicago, IL 60637, USA
bDepartment of Linguistics, University of Chicago
1115 E. 58th Street, Chicago, IL 60637, USA

Abstract

We study the problem of recognizing video sequences of ﬁngerspelled letters in Amer-
ican Sign Language (ASL). Fingerspelling comprises a signiﬁcant but relatively un-
derstudied part of ASL. Recognizing ﬁngerspelling is challenging for a number of
reasons: It involves quick, small motions that are often highly coarticulated; it ex-
hibits signiﬁcant variation between signers; and there has been a dearth of continuous
ﬁngerspelling data collected. In this work we collect and annotate a new data set of
continuous ﬁngerspelling videos, compare several types of recognizers, and explore
the problem of signer variation. Our best-performing models are segmental (semi-
Markov) conditional random ﬁelds using deep neural network-based features. In the
signer-dependent setting, our recognizers achieve up to about 92% letter accuracy. The
multi-signer setting is much more challenging, but with neural network adaptation we
achieve up to 83% letter accuracies in this setting.

Keywords: American Sign Language, ﬁngerspelling recognition, segmental model,
deep neural network, adaptation

1. Introduction

Sign languages are the primary means of communication for millions of Deaf peo-
ple in the world. In the US, there are about 350,000–500,000 people for whom Ameri-
can Sign Language (ASL) is the primary language [1]. While there has been extensive
research over several decades on automatic recognition and analysis of spoken lan-
guage, much less progress has been made for sign languages. Both signers and non-
signers would beneﬁt from technology that improves communication between these
populations and facilitates search and retrieval in sign language video.

Sign language recognition involves major challenges. The linguistics of sign lan-
guage is less well understood than that of spoken language, hampering both scientiﬁc
and technological progress. Another challenge is the high variability in the appearance
of signers’ bodies and the large number of degrees of freedom. The closely related

1

Figure 1: The ASL ﬁngerspelled alphabet. Reproduced from [2].

computer vision problem of articulated pose estimation and tracking remains largely
unsolved.

Signing in ASL (as in many other sign languages) involves the simultaneous use
of handshape, location, movement, orientation, and non-manual behaviors. There has
now been a signiﬁcant amount of research on sign language recognition, for a number
of sign languages. Prior research has focused more on the larger motions of sign and on
interactions between multiple body parts, and less so on handshape. In some contexts,
however, handshape carries much of the content, and it is this aspect of ASL that we
study here.

Sign language handshape has its own phonology that has been studied and enjoys
a broadly agreed-upon understanding relative to the other manual parameters of move-
ment and place of articulation [3]. Recent linguistic work on sign language phonology
has developed approaches based on articulatory features, related to motions of parts
of the hand [4, 2]. At the same time, computer vision research has studied pose es-
timation and tracking of hands [5], but usually not in the context of a grammar that
constrains the motion. There is therefore a great need to better understand and model
the handshape properties of sign language.

This project focuses mainly on one constrained, but very practical, component of
ASL: ﬁngerspelling. In certain contexts (e.g., when no sign exists for a word such as
a name, to introduce a new word, or for emphasis), signers use ﬁngerspelling: They
spell out the word as a sequence of handshapes or hand trajectories corresponding
to individual letters. Fig. 1 shows the ASL ﬁngerspelled alphabet, and Figs. 2 show
examples of real ﬁngerspelling sequences. We will refer to the handshapes in Fig. 1 as
ﬁngerspelled letters (FS-letters), which are canonical target handshapes for each of the
26 letters, and the starred actual handshapes in Fig. 2 as peak handshapes.

Fingerspelled words arise naturally in the context of technical conversation or con-
versation about current events, such as in Deaf blogs or news sites.1 Automatic ﬁn-
gerspelling recognition could add signiﬁcant value to such resources. Overall, ﬁnger-
spelling comprises 12-35% of ASL [6], depending on the context, and includes 72% of
the handshapes used in ASL [7]. These factors make ﬁngerspelling an excellent testbed
for handshape recognition.

Most previous work on ﬁngerspelling and handshape has focused on restricted con-

1E.g., http://deafvideo.tv, http://aslized.org.

2

Figure 2: Images and ground-truth segmentations of the ﬁngerspelled words T-U-L-I-P and A-R-T pro-
duced by two signers in our data set. Image frames are sub-sampled at the same rate from both signers to
show the true relative speeds. The starred frames indicate manually annotated peak handshapes (peak-HSs)
for each FS-letter (see the text for the full deﬁnition of FS-letter and peak-HS). “<s>” and “</s>” denote
non-signing intervals before/after signing. See Sec. 3, 4 for more details on data collection, annotation, and
segmentation.

ditions such as careful articulation, isolated signs, restricted (20-100 word) vocabular-
ies, or signer-dependent applications [8, 9, 10] (see Section 2 for more on related work).
In such restricted settings, letter error rates (Levenshtein distances between hypothe-
sized and true FS-letter sequences, as a proportion of the number of true FS-letters) of
10% or less have been obtained. In this work we consider lexicon-free ﬁngerspelling
sequences produced by multiple signers. This is a natural setting, since ﬁngerspelling
is often used for names and other “new” terms that do not appear in any closed vocabu-
lary. Our long-term goals are to develop techniques for robust automatic detection and
recognition of ﬁngerspelled words in video, and to enable generalization across signers,
styles, and recording conditions, both in controlled visual settings and “in the wild.”
To this end, we are also interested in developing multi-signer, multi-style corpora of
ﬁngerspelling, as well as efﬁcient annotation schemes for the new data.

The work in this paper represents our ﬁrst steps toward this goal: studio collection
and annotation of a new multi-signer connected ﬁngerspelling data set (Section 3) and
high-quality ﬁngerspelling recognition in the signer-dependent and multi-signer set-
tings (Section 5). The new data set, while small relative to typical speech data sets,
comprises the largest ﬁngerspelling video data set of which we are aware containing
connected multi-signer ﬁngerspelling that is not restricted to any particular lexicon.
Next, we compare several recognition models inspired by automatic speech recogni-
tion, with some custom-made characteristics for ASL (Section 4). We begin with a
tandem hidden Markov model (HMM) approach, where the features are based on pos-

3

teriors of deep neural network (DNN) classiﬁers of letter and handshape phonological
features. We also develop discriminative segmental models, which allow us to intro-
duce more ﬂexible features of ﬁngerspelling segments. In the category of segmental
models, we compare models for rescoring lattices and a ﬁrst-pass segmental model,
and the latter ultimately outperforms the others. Finally, we address the problem of
signer variation via adaptation of the DNN classiﬁers, which allows us to bridge much
of the gap between signer-dependent and signer-independent performance. 2

2. Related work

Automatic sign language recognition has been approached in a variety of ways,
including approaches based on computer vision techniques and ones inspired by au-
tomatic speech recognition. Thorough surveys on the topic are provided by Koller et
al. [14] and Ong and Ranganath [15]. Here we focus on the most closely related work.
A great deal of effort has been aimed at exploiting specialized equipment such as
depth sensors, motion capture, data gloves, or colored gloves in video [16, 17, 18,
19, 20, 21, 22]. This approach is very attractive for designing new communication
interfaces for Deaf individuals. In many settings, however, video is more practical,
and for online or archival recordings is the only choice. In this paper we restrict the
discussion to the video-only setting.

A number of sign language video corpora have been collected [23, 24, 25, 26, 27,
28]. Some of the largest data collection efforts have been for European languages,
such as the Dicta-Sign [29] and SignSpeak [30] projects. For American Sign Lan-
guage, the American Sign Language Lexicon Video Dataset (ASLLVD) [27, 28, 31]
includes recordings of almost 3,000 isolated signs and can be searched via a query-
by-example interface. The National Center for Sign Language and Gesture Resources
(NCSLGR) Corpus includes videos of continuous ASL signing, with over 10,000 sign
tokens including about 1,500 ﬁngerspelling sequences, annotated using the SignStream
linguistic annotation tool [32, 33]. The latter includes the largest previous data set of
ﬁngerspelling sequences of which we are aware. In order to explicitly study ﬁnger-
spelling, however, it is helpful to have a collection that is both larger and annotated
speciﬁcally with ﬁngerspelling in mind. To our knowledge, the data collection and an-
notation we report in this paper (see Sec. 3) is the largest currently available for ASL
ﬁngerspelling.

Sign language recognition from video begins with front-end features. Prior work
has used a variety of visual features, including ones based on estimated position, shape
and movement of the hands and head [34, 35, 36, 37], sometimes combined with ap-
pearance descriptors [38, 39, 40, 4] and color models [41, 42]. In this work, we are
aiming at relatively small motions that are difﬁcult to track a priori, and therefore be-
gin with general image appearance features based on histograms of oriented gradi-
ents (HOG) [43], which have also been used in other recent sign language recognition

2Parts of this work have appeared in our conference papers [11, 12, 13]. This paper includes additional
model comparisons and improvements, different linguistic feature sets, and detailed presentation of the col-
lected data and annotation.

4

work [14, 44].

Much prior work has used hidden Markov model (HMM)-based approaches [45,
46, 39, 14], and this is the starting point for our work as well. Ney and colleagues have
shown that it is possible to borrow many of the standard HMM-based techniques from
automatic speech recognition to obtain good performance on naturalistic German Sign
Language videos [39, 14]. In addition, there have been efforts involving conditional
models [35] and more complex (non-linear-chain) models [47, 48, 49].

As in acoustic and even visual speech recognition, the choice of basic linguis-
tic unit is an important research question. For acoustic speech, the most commonly
used unit is the context-dependent phoneme [50], although other choices such as syl-
lables, articulatory features, and automatically learned units have also been consid-
ered [51, 52]. As the research community started to consider visual speech recog-
nition (“lipreading”), analogous units have been explored: visemes, articulatory fea-
tures, and automatic clusters [53, 54, 55]. While sign language shares some aspects of
spoken language, it has some unique characteristics. A number of linguistically mo-
tivated representations of handshape and motion have been used in some prior work,
e.g., [34, 4, 47, 48, 56, 57, 46, 58, 59], and multiple systems of phonological and pho-
netic features have been developed by linguists [60, 2]. One of the unique aspects of
sign language is that transitional movements occupy a larger portion of the signal than
steady states, and some researchers have developed approaches for explicitly modeling
the transitions as units [61, 62, 63, 64].

A subset of ASL recognition work has focused speciﬁcally on ﬁngerspelling and/or
handshape classiﬁcation [65, 66, 47, 18] and ﬁngerspelling sequence recognition [8, 9,
10]. Letter error rates of 10% or less have been achieved when the recognition is
constrained to a small (up to 100-word) lexicon of allowed sequences. Our work is the
ﬁrst of which we are aware to address the task of lexicon-free ﬁngerspelling sequence
recognition.

The problem of signer adaptation has been addressed in prior work, using tech-
niques borrowed from speaker adaptation for speech recognition, such as maximum
likelihood linear regression and maximum a posteriori estimation [14, 67, 68], and to
explicitly study the contrast between signer-dependent and multi-signer ﬁngerspelling
recognition.

The models we use and develop in this paper are related to prior work in both vision
and speech (unrelated to sign language). Our HMM baselines are tandem models,
analogous to ones developed for speech recognition [69, 70]. The segmental models we
propose are related to segmental conditional random ﬁelds (SCRFs) and their variants,
which have now been applied fairly widely for speech recognition [71, 72, 73, 74, 75].
In natural language processing, semi-Markov CRFs have been used for named entity
recognition [76], where the labeling is binary. Finally, segmental models have been
applied to vision tasks such as classiﬁcation and segmentation of action sequences [77,
78] with a small set of possible activities to choose from, including work on spotting
and recognition of a small vocabulary of (non-ﬁngerspelled) signs in sign language
video [79] or instrumented data capture [80]. One aspect that our work shares with the
speech recognition work is that we have a relatively large set of labels (26 FS-letters
plus non-letter “N/A” labels), and widely varying lengths of segments corresponding
to each label (our data includes segment durations anywhere from 2 to 40 frames),

5

which makes the search space larger and the recognition task more difﬁcult than in the
vision and text tasks. In prior speech recognition work, this computational difﬁculty
has often been addressed by adopting a lattice rescoring approach, where a frame-based
model such as an HMM system generates ﬁrst-pass lattices and a segmental model
rescores them [71, 74]. We compare this approach to an efﬁcient ﬁrst-pass segmental
model [72].

3. Data collection and annotation

We recorded and analyzed videos of 3 native ASL signers and 1 early learner, ﬁn-
gerspelling a total of 3,684 word instances. We annotated the video by identifying the
time of peak articulation (also known as hold, posture, or target) for each ﬁngerspelled
letter. There were 21,453 peaks in total. The following sections describe in detail the
data collection and annotation process.

The data and annotations will be released publicly, in order to make it easy for
others to study ﬁngerspelling more extensively, and to replicate and compare against
our results.

3.1. Video recording

The data was collected across several sessions, each including all of the words on
one word list (the lists are described in detail below). Our use of “words” here includes
both real words and nonsense letter sequences. The signers were presented with an
isolated word on a computer screen. They were asked to ﬁngerspell the word, and then
press either a green button to advance or a red button to repeat the word if they felt they
had made a mistake. For most sessions the signers were asked to ﬁngerspell at a normal,
natural speed.3 Each session lasted 25–40 minutes, with a self-timed rest/stretch break
in the middle of each session.

Three word lists were created and used to collect data. The ﬁrst list had 300 words:
100 names, 100 nouns, and 100 non-English words. These words were chosen to get
examples of many letters in many contexts. The second list consisted of 300 mostly
non-English words in an effort to get examples of each possible letter bigram. The
third list had the 300 most common nouns in the CELEX corpus in order to get a list
of words that are reasonably familiar to the signers. Additionally, a set of carefully
articulated, isolated ﬁngerspelled letters were also collected from each signer. A full
listing of the word lists can be found in [2].

The video was recorded in a laboratory setting, with the signers wearing green or
blue clothing and with the signers set against a green background. For most sessions
the signers sat in a chair with an armrest that they could use if they felt the desire to. In
a small number of sessions the signers were asked to stand rather than sit. Video was
recorded using at least two cameras, each at an approximately 45 degree angle from a
direct frontal view. Each camera recorded video at 1920x540 pixels per ﬁeld4, 60 ﬁelds

3The instructions, given in ASL, were to: “proceed at normal speed and in your natural way of ﬁnger-

spelling.”

4Through our deinterlacing process, additional lines were interpolated to give us a ﬁnal result of

1920x1080 pixels per frame at 60 frames per second

6

per second, interlaced, using the AVCHD format. For purposes of annotation, the video
ﬁles were processed with FFMPEG to deinterlace, crop, resize, and reencode them for
compatibility with the ELAN annotation software [81]. For purposes of recognition
experiments, the videos were kept at full size and deinterlaced only.

The recording settings, including differences in environment and camera placement

across sessions, are illustrated in Fig. 3.

Figure 3: Example video frames from the four signers included in the recognition experiments.

3.2. Annotation

Our annotation method is separated into two main parts: 1. a simple task to identify
approximate times of each peak (peak detection) and 2. a veriﬁcation task to determine
precise timing for each peak (peak veriﬁcation). The ﬁrst is designed to be extremely
quick, allowing multiple annotator judgements to be aggregated together. The second
is more time-consuming and performed by a single annotator.

3.2.1. Peak detection

3–4 human annotators identiﬁed the peak of each letter. For this purpose, we de-
ﬁned a peak as the point where the articulators change direction to proceed on to the
next peak (i.e. where the instantaneous velocity of the articulators is zero or at its min-
imum). This point is typically where the hand most closely resembles the canonical
handshape, although at normal speed the peak handshape is often very different from
canonical. Two FS-letters, -J- and -Z-, are not well-represented in terms of peaks, since
they have movement. For these two FS-letters, annotators were asked to mark a peak at
the point that they could determine that it was one of these two FS-letters. Peak detec-
tion is simple, requiring minimal training; annotators reported that this task was very
intuitive. Most of the annotators at this stage had no exposure to ASL or ﬁngerspelling.

7

The peak times from the multiple annotators were averaged, after aligning the an-
notations to minimize the mean absolute difference in time between the individual
annotators’ peaks. We accounted for misidentiﬁed peaks by penalizing missing or ex-
tra ones in the alignment. Using logs from the recording session, a best guess at the
FS-letter corresponding to each peak was added by aligning the intended FS-letter se-
quence to the peak times (starting at the left edge of the word, matching each peak with
a FS-letter).

3.2.2. Peak veriﬁcation

Finally, a more experienced, second-language learner of ASL or an annotator specif-
ically trained in ﬁngerspelling annotation veriﬁed the location and identity of each peak
from the peak detection stage. For the veriﬁcation stage, we further reﬁned the deﬁ-
nition of peak to the point in time when the handshape conﬁguration is closest to the
canonical handshape for a given FS-letter. If a peak handshape remained stable for
more than one frame, each stable frame was marked. However, for recognition experi-
ments, only the original single peak frame was used.

When using the data for recognizer training, the peak annotations are used to seg-
ment each word into FS-letters: The boundary between consecutive FS-letters is de-
ﬁned as the midpoint between their peak frames. Appendix A provides additional
details about the annotation and analysis conventions, as well as detailed deﬁnitions of
the ﬁngerspelled letters.

FS-letter frequencies in the collected data are given in Tab. 1. A histogram of the

durations of all peaks in our data (excluding outliers) is given in Fig. 4.

Several studies [82, 83, 84, 2] have been conducted looking at how frequently FS-
letters are realized canonically (i.e. as the correct peak handshape) in our data. The
frequency of the canonical peak handshape of the FS-letter depends on a number of
factors (FS-letter identity, speed, signer, etc.). Some FS-letters show up nearly always
as the canonical variant (e.g., -C-), some almost never (-D-), while others have 75%–
85% ( -E- or -O- respectively) canonical peak handshape realizations [83]. Similarly
to the phonetics of spoken language, contextual effects have an impact on the phonetic
conﬁguration of each peak handshape. For example, the pinky extension property of
FS-letters has been shown to spread from peak handshapes that have an extended pinky
to the ones before and after [2]. These coarticulation effects, combined with the quick
motions of ﬁngerspelling, are some of the factors that make the recognition task quite
challenging.

4. Recognition methods

Our task is to take as input a video (a sequence of images) corresponding to a
ﬁngerspelled word, as in Fig. 2, and predict the signed FS-letters. This is a sequence
prediction task analogous to connected phone or word recognition, but there are some
interesting sign language-speciﬁc properties to the data domain. For example, one
striking aspect of ﬁngerspelling sequences, such as those in Fig. 2, is the large amount
of motion and lack of any prolonged “steady state” for each FS-letter. As described in
Sec. 3, each FS-letter is represented by a brief “peak of articulation”, during which the

8

Figure 4: Histogram of peak durations for all peaks in the corpus, excluding outliers. Any peak durations
that were greater than 6 standard deviations more than the mean peak duration were excluded.

hand’s motion is at a minimum and the handshape is the closest to the target handshape
for the FS-letter. This peak is surrounded by longer periods of motion between the
current FS-letter and the previous/next FS-letters.

Another striking property of sign language is the wide variation between signers.
Inspection of data such as Fig. 2 reveals some types of signer variation, including dif-
ferences in speed, hand appearance, and non-signing motion before and after signing.
The speed variation is large: In our data, the ratio between the average per-letter dura-
tion is about 1.8 between the fastest and slowest signers.

We consider signer-dependent, signer-independent, and signer-adapted recognition.
We next describe the recognizers we compare, as well as the techniques we explore for
signer adaptation. All of the recognizers use deep neural network (DNN) classiﬁers of
FS-letters or handshape features.

4.1. Recognizers

In designing recognizers, we keep several considerations in mind. First, the data
set, while large in comparison to prior ﬁngerspelling data sets, is still quite small com-
pared to typical speech data sets. This means that large models with many context-
dependent units are infeasible to train on our data (as conﬁrmed by our initial experi-
ments). We therefore restrict attention here to “mono-letter” models, that is models in
which each unit is a context-independent FS-letter. We also consider the use of artic-
ulatory (phonological and phonetic) feature units, and there is evidence from speech
recognition research that these may be useful in low-data settings [85, 86]. Second,
we would like our models to be able to capture detailed sign language-speciﬁc infor-
mation, such as the dynamic aspects of FS-letters as discussed above; this suggests
the segmental models that we consider below. Finally, we would like our models to
be easy to adapt to new signers. In order to enable this, all of our recognizers use in-
dependently trained deep neural network (DNN) classiﬁers, which can be adapted and

9

peak letter
E
A
I
O
N
R
T
S
L
U
C
D
M
P

freq
0.1124
0.0979
0.0788
0.0685
0.0680
0.0606
0.0581
0.0492
0.0491
0.0376
0.0352
0.0277
0.0276
0.0256

n
2411
2101
1690
1470
1458
1300
1246
1055
1053
807
755
594
593
550

peak letter
G
H
Y
F
K
B
V
W
Z
X
J
?
Q

freq
0.0234
0.0225
0.0225
0.0201
0.0184
0.0170
0.0140
0.0135
0.0122
0.0114
0.0103
0.0101
0.0083

n
501
483
483
432
395
365
300
290
261
245
221
216
178

Table 1: Frequencies and counts of peaks in the corpus

plugged into different sequence models. Our DNNs are trained using an L2-regularized
cross-entropy loss. The inputs are the image features concatenated over a multi-frame
window centered at the current frame, which are fed through several fully connected
layers followed by a softmax output layer with as many units as labels.5

4.1.1. Tandem model

The ﬁrst recognizer we consider is a fairly typical tandem model [69, 70]. Frame-
level image features are fed to seven DNN classiﬁers, one of which predicts the frame’s
FS-letter label and six others which predict handshape phonological features. The
phonological features are deﬁned in Tab. 2, and example frames for values of one
feature are shown in Fig. 5. The classiﬁer outputs and image features are reduced in
dimensionality via PCA and then concatenated. The concatenated features form the
observations in an HMM-based recognizer with Gaussian mixture observation densi-
ties. We use a 3-state HMM for each (context-independent) FS-letter, plus one HMM
each for initial and ﬁnal “silence” (non-signing segments).

In addition, we use a bigram letter language model. In general the language model
of FS-letter sequences is difﬁcult to deﬁne or estimate, since ﬁngerspelling does not
follow the same distribution as English words and there is no large database of natural
ﬁngerspelled sequences on which to train. In addition, in our data set, the words were
selected so as to maximize coverage of letter n-grams and word types rather than fol-
lowing a natural distribution. For this work, the language model is trained using ARPA
CSR-III text, which includes English words and names [87]. The issue of language
modeling for ﬁngerspelling deserves more attention in future work.

5We have also considered convolutional neural networks with raw image pixels as input, but these did

not signiﬁcantly outperform the DNNs on image features. See Sec. 5.3.3.

10

Feature
SF point of
reference
(POR)
SF joints

Deﬁnition/Values
side of the hand where
SFs are located
SIL, radial, ulnar, radial/ulnar

degree of ﬂexion or
extension of SFs
SIL, ﬂexed:base, ﬂexed:nonbase,
ﬂexed:base & nonbase,
stacked, crossed, spread

SF quantity

combinations of SFs
N/A, all, one,
one > all, all > one

SF thumb

SF handpart

thumb position
N/A, unopposed, opposed

internal parts of the hand
SIL, base, palm, ulnar

UF

open/closed
SIL, open, closed

Table 2: Deﬁnition and possible values for phonological features based on [3]. The ﬁrst ﬁve features are
properties of the active ﬁngers (selected ﬁngers, SF); the last feature is the state of the inactive or unselected
ﬁngers (UF). In addition to Brentari’s feature values, we add a SIL (“silence”) value to the features that do
not have an N/A value. For detailed descriptions, see [3].

Figure 5: Example images corresponding to SF thumb = ’unopposed’ (upper row) and SF thumb = ’opposed’
(bottom row).

4.1.2. Segmental CRF

The second recognizer is a segmental CRF (SCRF). SCRFs [76, 71] are conditional
log-linear models with feature functions that can be based on variable-length segments

11

of input frames, allowing for great ﬂexibility in deﬁning feature functions. Formally,
for a sequence of frames o1, o2, . . . , oT , a segmentation of size k is a sequence of
time points 0 = q0, q1, . . . , qk−1, qk = T used to denote time boundaries of segments.
In other words, the i-th segment starts at time qi−1 and ends at qi. The labeling of
segmentation q is a sequence of labels s1, s2, . . . , sk. We will denote the length of
, respectively. For a segmentation of size k,
label sequences and segmentations
= k. A SCRF deﬁnes, over a sequence of labels s given a sequence of frames
s
|
|
o, a probability distribution

s
|
|

=

q

q

|

|

|

|

,

o) =
p(s
|

(cid:16)(cid:80)|q|

(cid:80)

q:|q|=|s| exp
(cid:80)

q(cid:48):|q(cid:48)|=|s(cid:48)| exp

i=1 λ(cid:62)f (si−1, si, qi−1, qi, o)
(cid:16)(cid:80)|q(cid:48)|

i−1, s(cid:48)

i, q(cid:48)

i−1, q(cid:48)

i=1 λ(cid:62)f (s(cid:48)

(cid:17)

i, o)

(cid:80)

s(cid:48)

(cid:17)

where λ is a weight vector, and f (s, s(cid:48), q, q(cid:48), o) is a feature vector. We assume s0 is
the sentence-start string, so that f (s0, s1, q0, q1) is well-deﬁned. In the case of sign
language, it is natural to deﬁne feature functions that are sensitive to the duration and
dynamics of each FS-letter segment.

4.1.3. Rescoring segmental CRF

One common way of using SCRFs is to rescore the outputs from a baseline HMM-
based recognizer, and this is one way we apply SCRFs here. We ﬁrst use the baseline
recognizer, in this case the tandem HMM, to generate lattices of high-scoring segmen-
tations and labelings, and then rescore them with our SCRFs.

We use the same feature functions as in [12], described here for completeness.
Some of the feature functions are quite general to sequence recognition tasks, while
some are tailored speciﬁcally to ﬁngerspelling recognition.

DNN classiﬁer-based feature functions. The ﬁrst set of feature functions measure how
well the frames within a segment match the hypothesized label. For this purpose we
use the same DNN classiﬁers as in the tandem model above.

Let y be a FS-letter and v be the value of a FS-letter or linguistic feature, and g(v
oi)
|
the softmax output of a DNN classiﬁer at frame i for class v. We deﬁne several feature
functions based on aggregating the DNN outputs over a segment in various ways:

mean: f mean

yv

(s, s(cid:48), q, q(cid:48), o) = δ(s(cid:48) = y)

max: f max

yv (s, s(cid:48), q, q(cid:48), o) = δ(s(cid:48) = y)

1
q(cid:48)−q+1

(cid:80)q(cid:48)

i=q g(v

oi)

·
maxi∈{q,q+1,...,q(cid:48)} g(v

|

·

oi)
|

divs: a concatenation of three mean feature functions, each computed over a
third of the segment

divm: a concatenation of three max feature functions, each computed over a third
of the segment

•

•

•

•

These are similar to features used in prior work on SCRFs for ASR with DNN-based
feature functions [88, 89, 72]. We tune the choice of aggregated feature functions on
tuning data in our experiments.

12

Peak detection features. A sequence of ﬁngerspelled FS-letters yields a correspond-
ing sequence of peak handshapes as described above. The peak frame and the frames
around it for each FS-letter tend to have relatively little motion, while the transition
frames between peaks have much more motion. To encourage each predicted FS-letter
segment to have a single peak, we deﬁne letter-speciﬁc “peak detection features” based
on approximate derivatives of the visual descriptors. We compute the approximate
derivative as the l2 norm of the difference between descriptors in every pair of consec-
utive frames, smoothed by averaging over 5-frame windows. Typically, there should
be a single local minimum in this derivative over the span of the segment. We deﬁne
the feature function corresponding to FS-letter y as
(s, s(cid:48), q, q(cid:48), o) = δ(s(cid:48) = y)

δpeak(o, q, q(cid:48))

f peak
y

where δpeak(o, q, q(cid:48)) is 1 if there is exactly one local minimum between time point q
and q(cid:48) and 0 otherwise.

·

Language model feature. The language model feature is a bigram probability of the
FS-letter pair corresponding to an edge:

f lm(s, s(cid:48), q, q(cid:48), o) = pLM (s, s(cid:48)).

where pLM is our smoothed bigram language model.

Baseline consistency feature. To take advantage of the already high-quality baseline
that generated the lattices, we use a baseline feature like the one in [71], which is based
on the 1-best output from the baseline tandem recognizer. The feature has value 1 when
the corresponding segment spans exactly one FS-letter label in the baseline output and
the label matches it:

f bias(s, s(cid:48), q, q(cid:48), o) =






+1

1

if C(q, q(cid:48)) = 1,
and B(q, q(cid:48)) = s(cid:48)
otherwise

−
where C(q, q(cid:48)) is the number of distinct baseline labels in the time span from q to q(cid:48),
B(q, q(cid:48)) is the label corresponding to time span (q, q(cid:48)) when C(q, q(cid:48)) = 1.

4.1.4. First-pass segmental CRF

One of the drawbacks of a rescoring approach is that the quality of the ﬁnal outputs
depends both on the quality of the baseline lattices and the ﬁt between the segmenta-
tions in the baseline lattices and those preferred by the second-pass model. We there-
fore also consider a ﬁrst-pass segmental model, using similar features to the rescoring
model. The difference between a rescoring model and a ﬁrst-pass model lies in the
set of segmentations and labellings to marginalize over. The rescoring model only
marginalizes over the hypotheses generated by the tandem model, while the ﬁrst-pass
model marginalizes over all possible hypotheses. The ﬁrst-pass model thus does not
depend on the tandem HMM. We use a ﬁrst-pass SCRF inspired by the phonetic rec-
ognizer of Tang et al. [72], and the same feature functions as in [72], namely average
DNN outputs over each segment, samples of DNN outputs within the segment, bound-
oi) is the
aries of DNN outputs in each segment, duration and bias. Recall that g(v

|

13

softmax output of a DNN classiﬁer at frame i for class v. We use the same DNN frame
classiﬁers as in the rescoring setting. Also recall that y is a FS-letter and v is the value
of a FS-letter or linguistic feature. The exact deﬁnition of each feature is listed below.

Averages of DNN outputs. These are the same as the mean features used in the rescor-
ing setting.

Samples of DNN outputs. These are samples of the DNN outputs at the mid-points of
three equal-sized sub-segments

f spl
yvi(s, s(cid:48), q, q(cid:48), o) = δ(s(cid:48) = y)g(v

oq+(q(cid:48)−q)i)
|

for i = 16%, 50%, 84%.

Left boundary. Three DNN output vectors around the left boundary of the segment

f l-bndry
yvk

(s, s(cid:48), q, q(cid:48), o) = δ(s(cid:48) = y)g(v

oq+k)
|

for k

1, 0, 1

.
}

∈ {−

Right boundary. Three DNN output vectors around the right boundary of the segment

f r-bndry
yvk

(s, s(cid:48), q, q(cid:48), o) = δ(s(cid:48) = y)g(v

oq(cid:48)+k)

|

for k

1, 0, 1

.
}

∈ {−

Duration. An indicator for the duration of a segment

yk (s, s(cid:48), q, q(cid:48), o) = δ(q(cid:48)
f dur

q = k)δ(s(cid:48) = y)

−

for k

1, 2, . . . , 30

∈ {

.
}

Bias. A constant 1

f bias
y

(s, s(cid:48), q, q(cid:48), o) = 1

δ(s(cid:48) = y).

·

As shown in the above deﬁnitions, all features are lexicalized with the unigram

label, i.e., they are multiplied by an indicator for the hypothesized FS-letter label.

4.2. DNN adaptation

In experiments, we will consider both signer-dependent and signer-independent
recognition. In the latter case, the test signer is not seen in the training set. As we
will see, there is a very large gap between signer-dependent and signer-independent
recognition on our data. We also consider the case where we have some labeled data
from the test signer, but not a sufﬁcient amount for training signer-dependent models.
In this case we consider adapting a signer-independent model toward the test signer.
The most straightforward form of adaptation for our models is to adapt only the DNN
classiﬁers, and then use the adapted ones in pre-trained signer-independent models.
We consider adaptation with carefully annotated adaptation data, using ground-truth

14

Figure 6: Left: Unadapted DNN classiﬁer; middle: adaptation via linear input network and output layer
updating (LIN+UP); right: adaptation via linear input network and linear output network (LIN+LON).

frame-level labels, as well as the setting where only word labels are available but no
frame-level alignments. In the latter case, we obtain frame labels via forced alignment
using the signer-independent model.

Our adaptation approaches are inspired by several DNN adaptation techniques that
have been developed for speech recognition (e.g., [90, 91, 92, 93]). We consider several
approaches, shown in Fig. 6. Two of the approaches are based on linear input networks
(LIN) and linear output networks (LON) [94, 95, 96].
In these techniques most of
the network parameters are ﬁxed to the signer-independent ones, and a limited set of
weights at the input and/or output layers are learned.

In the approach we refer to as LIN+UP in Fig. 6, we apply an afﬁne transformation
WLIN to the static features at each frame, as a pre-processing step before frame con-
catenation, and use the transformed features as input to the trained signer-independent
DNNs. We simultaneously learn WLIN and ﬁne-tune the last (softmax) layer weights
by minimizing the same cross-entropy loss on the adaptation data, initialized with the
signer-independent weights..

The second approach, referred to as LIN+LON in Fig. 6, uses the same adaptation
layer at the input. However, instead of adapting the softmax weights at the top layer,
it removes the softmax output activation and adds a new softmax output layer WLON
trained for the test signer. The new input and output layers are trained jointly with the
same cross-entropy loss.

Finally, we also consider adaptation by ﬁne-tuning all of the DNN weights on the
adaptation data, using as initialization the signer-independent DNN weights (that is,
using the signer-independent DNN as a “warm start”).

5. Experimental Results

We report on experiments using the ﬁngerspelling data from the four ASL signers
described above. We begin by describing some of the front-end details of hand seg-
mentation and feature extraction, followed by experiments with the frame-level DNN
classiﬁers (Sec. 5.1) and FS-letter sequence recognizers (Sec. 5.2).

Hand localization and segmentation As in prior work [9, 11, 12], we used a simple
signer-dependent model for hand detection. First we manually annotated hand regions

15

in 30 frames, and we trained a mixture of Gaussians Phand for the color of the hand
pixels in L*a*b color space, and a single-Gaussian color model P x
bg for every pixel x
in the image excluding pixel values in or near marked hand regions. Given the color
triplet cx = [lx, ax, bx] at pixel x from a test frame, we assign the pixel to the hand if

Phand(cx)πhand > P x

bg(cx)(1

πhand),

−

(1)

where the prior πhand for hand size is estimated from the same 30 training frames.

We clean up the output of this simple model via several ﬁltering steps. First, we
suppress pixels that fall within regions detected as faces by the Viola-Jones face de-
tector [97], since these tend to be false positives. We also suppress pixels that passed
the log-odds test (Eq. 1) but have a low estimated value of Phand, since these tend
to correspond to movements in the scene. Finally, we suppress pixels outside of a
(generous) spatial region where the signing is expected to occur. The largest surviv-
ing connected component of the resulting binary map is treated as a mask that deﬁnes
the detected hand region. Some examples of the resulting hand regions are shown in
Fig. 2. Although this approach currently involves manual annotation for a small num-
ber of frames, it could be fully automated in an interactive system, and may not be
required if given a larger number of training signers.

Handshape descriptors We use histograms of oriented gradients (HOG [43]) as the
visual descriptor (feature vector) for a given hand region. We ﬁrst resize the tight
128 pixels, and then com-
bounding box of the hand region to a canonical size of 128
pute HOG features on a spatial pyramid of regions, 4
16 grids, with
8, and 16
4, 8
eight orientation bins per grid cell, resulting in 2688-dimensional descriptors. Pixels
outside of the hand mask are ignored in this computation. For the HMM-based rec-
ognizers, to speed up computation, these descriptors were projected to at most 200
principal dimensions; the exact dimensionality in each experiment was tuned on a de-
velopment set. For DNN frame classiﬁers, we found that ﬁner grids did not improve
much with increasing complexities, so we use 128-dimensional descriptors.

×
×

×

×

5.1. DNN frame classiﬁcation performance

Since all of our ﬁngerspelling recognition models use DNN frame classiﬁers as a

building block, we ﬁrst examine the performance of the frame classiﬁers.

For training and tuning the DNNs, we use the recognizer training data, split into
90% for DNN training and 10% for DNN tuning. The DNNs are trained for seven tasks
(FS-letter classiﬁcation and classiﬁcation of each of the six phonological features). The
input is the 128-dimensional HOG features concatenated over a 21-frame window. The
DNNs have three hidden layers, each with 3000 ReLUs [98]. Network learning is done
with cross-entropy training with a weight decay penalty of 10−5, via stochastic gradient
descent (SGD) over 100-sample minibatches for up to 30 epochs, with dropout [99] at
a rate of 0.5 at each hidden layer, ﬁxed momentum of 0.95, and initial learning rate
of 0.01, which is halved when held-out accuracy stops improving. We pick the best-
performing epoch on held-out data. The network structure and hyperparameters were
tuned on held-out (signer-independent) data in initial experiments.

16

Figure 7: Frame errors with DNN classiﬁers, with various settings. The horizontal axis labels indicate the
amount of adaptation data (0, 1, 2, 3 = none, 5%, 10%, 20% of the test signer’s data, corresponding to no
adaptation (signer-independent), ∼ 29, ∼ 58, and ∼ 115 words). GT = ground truth labels; FA = forced
alignment labels; FT = ﬁne-tuning. We also added trained DNN on only 20% of the test signer’s data.
Signer-dependent DNN uses 80% of the test signer’s data.

We consider the signer-dependent setting (where the DNN is trained on data from
the test signer), signer-independent setting (where the DNN is trained on data from all
except the test signer), and signer-adapted setting (where the signer-independent DNNs
are adapted using adaptation data from the test signer). For LIN+UP and LIN+LON,
we adapt by running SGD over minibatches of 100 samples with a ﬁxed momentum of
0.9 for up to 20 epochs, with initial learning rate of 0.02 (which is halved when accu-
racy stops improving on the adaptation data). For ﬁne-tuning, we use the same SGD
procedure as for the signer-independent DNNs. We pick the epoch with the highest
accuracy on the adaptation data.

The frame error rates for all settings are given in Fig. 7. For the signer-adapted case,
we consider DNN adaptation with different types and amounts of supervision. The
types of supervision include fully labeled adaptation data (“GT”, for “ground truth”,
in the ﬁgure), where the peak locations for all FS-letters are manually annotated; as
well as adaptation data labeled only with the FS-letter sequence but not the timing

17

information. In the latter case, we use the baseline tandem system to generate forced
alignments (“FA” in the ﬁgure). We consider amounts of adaptation data from 5% to
20% of the test signer’s full data.

These results show that among the adaptation methods, LIN+UP slightly outper-
forms LIN+LON, and ﬁne-tuning outperforms both LIN+UP and LIN+LON. For FS-
letter sequence recognition experiments in the next section, we adapt via ﬁne-tuning
using 20% of the test signer’s data.

We have analyzed the types of errors made by the DNN classiﬁers. One of the main
effects is that all of the signer-independent classiﬁers have a large number of incorrect
predictions of the non-signing classes (<s>, </s>). This may be due to the previously
mentioned observation that non-linguistic gestures are variable and easy to confuse
with signing when given a new signer’s image frames. As the DNNs are adapted, this
is the main type of error that is corrected. Speciﬁcally, of the frames with label <s>
that are misrecognized by the signer-independent DNNs, 18.3% are corrected after
adaptation; of the misrecognized frames labeled </s>, 11.2% are corrected.

5.2. FS-letter recognition experiments

5.2.1. Signer-dependent recognition

Our ﬁrst continuous FS-letter recognition experiments are signer-dependent; that
is, we train and test on the same signer, for each of four signers. For each signer, we
use a 10-fold setup: In each fold, 80% of the data is used as a training set, 10% as a
development set for tuning parameters, and the remaining 10% as a ﬁnal test set. We
independently tune the parameters in each fold. To make the results comparable to the
later adaptation results, we use 8 out of 10 folds to compute the ﬁnal test results and
report the average letter error rate (LER) over those 8 folds. For language models, we
train letter bigram language models from large online dictionaries of varying sizes that
include both English words and names [100]. We use HTK [101] to implement the
baseline HMM-based recognizers and SRILM [102] to train the language models. The
HMM parameters (number of Gaussians per state, size of language model vocabulary,
transition penalty and language model weight), as well as the dimensionality of the
HOG descriptor input and HOG depth, were tuned to minimize development set letter
error rates for the baseline HMM system. The front-end and language model hyperpa-
rameters were then kept ﬁxed for the experiments with SCRFs (in this sense the SCRFs
are slightly disadvantaged). Additional parameters tuned for the SCRF rescoring mod-
els included the N-best list sizes, type of feature functions, choice of language models,
and L1 and L2 regularization parameters. Finally, for the ﬁrst-pass SCRF, we tuned
step size, maximum length of segmentations, and number of training epochs.

Tab. 3 (last row) shows the signer-dependent FS-letter recognition results. SCRF
rescoring improves over the tandem HMM, and the ﬁrst-pass SCRF outperforms both.
Note that in our experimental setup, there is some overlap of word types between
training and test data. This is a realistic setup, since in real applications some of the test
words will have been previously seen and some will be new. However, for comparison,
we have also conducted the same experiments while keeping the training, development,
and test vocabularies disjoint; in this modiﬁed setup, letter error rates increase by about
2-3% overall, but the SCRFs still outperform the other models.

18

Signer
No adapt.
Forced align.
Ground truth
Signer-dep.

1
54.1
30.2
22.0
13.8

Tandem HMM
3
2
62.6
54.7
39.6
38.5
31.6
13.0
26.1
7.1

57.5
36.1
21.4
11.5

4 Mean
57.2
33.6
22.0
14.6

Rescoring SCRF
3
2
61.1
51.2
38.2
36.0
29.5
13.5
19.1
7.0

56.3
34.5
21.4
10.0

4 Mean
55.3
32.0
21.7
11.5

1
52.6
39.5
22.4
10.2

First-pass SCRF
3
2
72.5
53.3
36.5
24.9
24.9
10.6
9.3
7.7

61.4
35.5
18.4
10.1

4 Mean
60.6
30.3
17.3
8.8

1
55.3
24.4
15.2
8.1

Table 3: Letter error rates (%) on four test signers.

5.2.2. Signer-independent recognition

In the signer-independent setting, we would like to recognize FS-letter sequences
from a new signer, given a model trained only on data from other signers. For each
of the four test signers, we train models on the remaining three signers, and report the
performance for each test signer and averaged over the four test signers. For direct
comparison with the signer-dependent experiments, each test signer’s performance is
itself an average over the 8 test folds for that signer.

As shown in the ﬁrst line of Tab. 3, the signer-independent performance of the three
types of recognizers is quite poor, with the rescoring SCRF somewhat outperforming
the tandem HMM and ﬁrst-pass SCRF. The poor performance is perhaps to be expected
with such a small number of training signers.

5.2.3. Signer-adapted recognition

The remainder of Tab. 3 (second and third rows) gives the connected FS-letter
recognition performance obtained with the three types of models using DNNs adapted
via ﬁne-tuning, using different types of adaptation data (ground-truth, GT, vs. forced-
aligned, FA). For all models, we do not retrain the models with the adapted DNNs, but
tune hyperparameters6 on 10% of the test signer’s data. The tuned models are evalu-
ated on an unseen 10% of the test signer’s remaining data; ﬁnally, we repeat this for
eight choices of tuning and test sets, covering the 80% of the test signer’s data that we
do not use for adaptation, and report the mean FS-letter accuracy over the test sets.

As shown in Tab. 3, adaptation improves the performance to up to 30.3% letter
error rate with forced-alignment adaptation labels and up to 17.3% letter error rate with
ground-truth adaptation labels. All of the adapted models improve similarly. However,
interestingly, the ﬁrst-pass SCRF is slightly worse than the others before adaptation
but better (by 4.4% absolute) after ground-truth adaptation. One hypothesis is that the
ﬁrst-pass SCRF is more dependent on the DNN performance, while the tandem model
uses the original image features and the rescoring SCRF uses that tandem model’s
hypotheses. Once the DNNs are adapted, however, the ﬁrst-pass SCRF outperforms
the other models. Fig. 8 shows an example ﬁngerspelling sequence and the hypotheses
of the tandem, rescoring SCRF, and ﬁrst-pass SCRF.

5.3. Extensions and analysis

models.

We next analyze our results and consider potential extensions for improving the

6See [11, 12, 72] for details of the tuning parameters.

19

Figure 8: Sample frames from the word ROAD, with asterisks denoting the peak frame for each FS-letter
and “<s>” and “</s>” denoting periods before the ﬁrst FS-letter and after the last FS-letter; ground-truth
labeling and segmentation based on peak annotations (GT); hypothesized output from the tandem HMM
(Tandem), rescoring SCRF (re-SCRF), and ﬁrst-pass SCRF (1p-SCRF).

5.3.1. Analysis: Could we do better by training entirely on adaptation data?

Until now we have considered adapting the DNNs while using sequence models
(HMMs/SCRFs) trained only on signer-independent data. In this section we consider
alternatives to this adaptation setting. We ﬁx the model to a ﬁrst-pass SCRF and the
adaptation data to 20% of the test signer’s data annotated with ground-truth labels. In
this setting, we consider two alternative ways of using the adaptation data: (1) using
the adaptation data from the test signer to train both the DNNs and sequence model
from scratch, ignoring the signer-independent training set; and (2) training the DNNs
from scratch on the adaptation data, but using the SCRF trained on the training signers.
We compare these options with our best results using the signer-independent SCRF and
DNNs ﬁne-tuned on the adaptation data. The results are shown in Tab. 4.

We ﬁnd that ignoring the signer-independent training set and training both DNNs
and SCRFs from scratch on the test signer (option (1) above) works remarkably well,
better than the signer-independent models and even better than adaptation via forced
alignment (see Tab. 3). However, training the SCRF on the training signers but DNNs
from scratch on the adaptation data (option (2) above) improves performance fur-
ther. However, neither of these outperforms our previous best approach of signer-
independent SCRFs plus DNNs ﬁne-tuned on the adaptation data.

Signer
Accuracy

SCRF + DNNs trained from scratch
4 Mean
3
28.9
25.1

1
23.2

2
18.2

30.1

Sig.-indep. SCRF, DNNs from scratch

1
18.4

2
12.6

3
27.0

4
20.7

Mean
19.7

Sig.-indep. SCRF, ﬁne-tuned DNN
4 Mean
3
24.9
17.3

2
10.6

1
15.2

18.4

Table 4: Letter error rates (%) for different settings of SCRF and DNN training in the signer-adapted case.
Details are given in Section 5.3.1.

5.3.2. Analysis: FS-letter vs. feature DNNs

We next compare the FS-letter DNN classiﬁers and the phonological feature DNN
classiﬁers in the context of the ﬁrst-pass SCRF recognizers. We also consider an al-
ternative sub-letter feature set, in particular a set of phonetic features introduced by
Keane [2], whose feature values are listed in Section Appendix A, Tab. B.9. We use

20

the ﬁrst-pass SCRF with either only FS-letter classiﬁers, only phonetic feature clas-
siﬁers, FS-letter + phonological feature classiﬁers, and FS-letter + phonetic feature
classiﬁers. We do not consider the case of phonological features alone, because they
are not discrimative for some FS-letters. Fig. 9 shows the FS-letter recognition results
for the signer-dependent and signer-adapted settings.

We ﬁnd that using FS-letter classiﬁers alone outperforms the other options in the
signer-dependent setting, achieving 7.7% letter error rate. For signer-adapted recogni-
tion, phonological or phonetic features are helpful in addition to FS-letters for two of
the signers (signers 2 and 4) but not for the other two (signers 1,3); on average, using
FS-letter classiﬁers alone is best in both cases, achieving 16.6% accuracy on average.
In contrast, in earlier work [11] we found that phonological features outperform FS-
letters in the tandem HMM. However, those experiments used a tandem model with
neural networks with a single hidden layer; we conjecture that with more layers, we
are able to do a better job at the more complicated task of FS-letter classiﬁcation.

5.3.3. Analysis: DNNs vs. CNNs

In all experiments thus far, we have used HOG image descriptors fed into fully
connected feedforward DNNs. However, it has recently become common to use convo-
lutional neural networks (CNNs) on raw image pixels without any hand-crafted image
descriptors, which produces improved performance for certain visual recognition tasks
(e.g., [103, 104]). In our case, we have relatively little training data compared with
typical benchmark visual recognition tasks, motivating our choice to preprocess with
image descriptors rather than rely on networks that learn features from raw pixels. To
test this assumption, we compare the performance of FS-letter and phonological fea-
ture classiﬁers based on DNNs and CNNs. We use a signer-dependent setting and the
same 8-fold setup.

×

×

×

×

64

For CNN experiments, our inputs are grayscale 64

T pixels. T is the
number of frames used in the input window, as in our DNNs. The CNNs use 32 kernels
3 ﬁlters with stride 1 for the ﬁrst and second convolutional layers. Next we
of 3
2 pixel window, with stride 2. For the third and
add a max pooling layer with a 2
fourth convolutional layer, we use 64 kernels of 3
3 ﬁlters with stride 1. Again,
we then add a max pooling layer with a 2
2 pixel window, with stride 2. For all
convolutional layers, ReLUs are used for the nonlinearity (as in our DNNs). Finally,
two fully connected layers with 2000 ReLUs are added, followed by a softmax output
layer. We use dropout to prevent overﬁtting, with a 0.25 dropout probability for all
convolutional layers and 0.5 for fully connected layers. Training is done via stochastic
10−6, momentum
gradient descent with learning rate 0.01, learning rate decay 1
0.9, and miini-batches of size 100. The network structure and all other settings were
tuned on held-out data in preliminary experiments. T was also tuned and set to 21. We
implemented the CNNs using Keras [105] with Theano [106].

×

×

×

Fig. 10 shows the results for the signer-dependent setting. We ﬁnd that the perfor-
mances of the DNNs and CNNs are comparable. Speciﬁcally, for FS-letter classiﬁca-
tion, DNNs are slightly better, while for phonological feature classiﬁcation, CNNs are
slightly better. However, in both cases the gaps are very small. These results provide
evidence for our hypothesis that on this data set, CNNs on raw pixels do not provide a
substantial beneﬁt, and we do not use them in further experiments.

21

Figure 9: (Top) Signer-dependent recognition and (bottom) signer-independent recognition with frame an-
notations and adaptation. We compare: FS-letter only, phonetic features only, FS-letters + phonological
features [3] and FS-letters + phonetic features [2].

5.3.4. Improving performance in the force-aligned adaptation case

We next attempt to improve the performance of adaptation in the absence of ground-
truth (manually annotated) frame labels. This is an important setting, since in practice

22

Figure 10: Comparison between DNN and CNN frame classiﬁers, in signer-dependent experiments over four
signers.

it can be very difﬁcult to obtain ground-truth labels at the frame level. Using only the
FS-letter label sequence for the adaptation data, we use the signer-independent tandem
recognizer to get force-aligned frame labels. We then adapt (ﬁne-tune) the DNNs using
the force-aligned adaptation data (as before in the FA case). We then re-align the test
signer’s adaptation data with the adapted recognizer. Finally, we adapt the DNNs again
with the re-aligned data. Throughout this experiment, we do not change the recognizer
but only update the DNNs. Using this iterative realignment approach, we are able to
furthur improve the recognition accuracy in the FA case by about 1.3%, as shown in
Tab. 5.

Signer 1
22.7

Fine-tuning with FA
Signer 3
33.4

Signer 2
26.0

Signer 4 Mean
29.2

34.8

Signer 1
21.9

Fine-tuning with FA + realignment
Signer 3
30.5

Signer 2
25.3

Signer 4 Mean
27.9

34.0

Table 5: Letter error rates (%) with iterated forced-alignment (FA) adaptation.

5.3.5. Improving performance with segmental cascades

Finally, we consider whether we can improve upon the performance of our best
models, the ﬁrst-pass SCRFs, by rescoring their results in a second pass with more
powerful features. We follow the discriminative segmental cascades (DSC) approach
of [72], where a simpler ﬁrst-pass SCRF is used for lattice generation and a second
SCRF, with more computationally demanding features, is used for rescoring.

23

For these experiments we start with the most successful ﬁrst-pass SCRF in the
above experiments, which uses FS-letter DNNs and is adapted with 20% of the test
signer’s data with ground-truth peak handshape labels. For the second-pass SCRF,
we use the ﬁrst-pass score as a feature, and add to it two more complex features: a
segmental DNN, which takes as input an entire hypothesized segment and produces
posterior probabilities for all of the FS-letter classes; and the “peak detection” feature
described in Section 4. We use the same bigram language model as in the tandem
HMM and rescoring SCRF models. For the segmental DNN, the training segments are
given by the ground-truth segmentations derived from manual peak annotations. The
input layer consists of a concatenation of three mean HOG vectors, each averaged over
one third of the segment. We use the same DNN structure and learning strategy as for
the DNN frame classiﬁers.

As shown in Tab. 6, this approach slightly improves the average FS-letter accu-
racy over four signers, from 16.6% in the ﬁrst pass to 16.2% in the second pass.
This improvement, while small, is statistically signiﬁcant at the p=0.05 level (using
the MAPSSWE signiﬁcance test from the NIST Scoring Toolkit [107]). These results
combine our most successful ideas and form our ﬁnal best results for signer-adapted
recognition. For the signer-dependent setting, this approach achieves comparable per-
formance to the ﬁrst-pass SCRF.

Signer 1
7.2

Signer-dependent
Signer 3
8.1

Signer 2
6.5

Signer 4 Mean

8.6

7.6

Signer 1
13.0

Signer-independent
Signer 3
21.7

Signer 2
11.2

Signer 4 Mean
16.2

18.8

Table 6: Letter error rates (%) obtained with a two-pass segmental cascade.

6. Conclusion

This paper tackles the problem of unconstrained ﬁngerspelled letter sequence recog-
nition in ASL, where the FS-letter sequences are not restricted to any closed vocabu-
lary. This problem is challenging due to both the small amount of available training
data and the signiﬁcant variation between signers. Our data collection effort has thus
far produced a set of carefully annotated ﬁngerspelled letter sequences for four native
ASL signers. Our recognition experiments have compared HMM-based and segmental
models with features based on DNN classiﬁers, and have investigated a range of set-
tings including signer-dependent, signer-independent, and signer-adapted. Our main
contributions are:

•

•

We have developed an approach for quick annotation of ﬁngerspelling data using
a two-pass approach, where multiple non-expert annotators quickly ﬁnd candi-
date peak times, which are then automatically combined and veriﬁed by an ASL
expert.

Signer-dependent recognition, even with only a small amount of training data,
is quite successful, reaching letter error rates below 10%. Signer-independent
recognition, in contrast, is quite challenging, with letter error rates around 60%.

24

•

•

•

DNN adaptation allows us to bridge a large part of the gap between signer-
independent and signer-dependent performance.

Our best results are obtained using two-pass discriminative segmental cascades
with features based on frame-level DNN FS-letter classiﬁers. This approach
achieves an average letter error rate of 7.6% in the signer-dependent setting
and 16.2% LER in the signer-adapted setting. The adapted models use signer-
independent SCRFs and DNNs adapted by ﬁne-tuning on the adaptation data.
The adapted results are obtained using about 115 words of adaptation data with
manual annotations of FS-letter peaks.

If an even smaller amount of adaptation data is available, we can still get

improvements from adaptation down to about 30 words of adaptation data.

In the absence of manual frame-level annotations, we can automatically align the
adaptation data and still get a signiﬁcant boost in performance from adaptation.
We can also iteratively improve the performance by re-aligning the data with the
adapted models. Our best adapted models using automatically aligned adaptation
data achieve 27.3% LER.

•

The main types of errors that are addressed by adapting the DNN classiﬁers are
confusions between signing and non-signing segments.

We are continuing to investigate additional sequence recognition approaches as
well as adaptation methods, especially for the case where no manual annotations are
available. Future work will also expand our data collection to a larger number of signers
and to data collected “in the wild”, such as videos from Deaf online media. Finally,
future work will consider jointly detecting and recognizing ﬁngerspelling sequences
embedded within running ASL.

Acknowledgements

We are grateful for the work of several undergraduate assistants for annotation of
the data. This research was funded by a Google Faculty Award and by NSF grants
NSF-1433485 and NSF/BCS-1251807. The opinions expressed in this work are those
of the authors and do not necessarily reﬂect the views of the funding agency.

Appendix A. Fingerspelled handshape deﬁnitions and annotation conventions

Handshape – We deﬁned a handshape as stable if all of digits assumed a position
and maintained it with only minor ﬂuctuations. As soon as any digit moves, the hand-
shape is considered to not be stable anymore. We were conservative with respect to
holds, in that if a digit moves a small amount, but that movement is part of a larger
movement that preceded or followed, that was not considered stable.

•

Fingerspelled (FS)-letters are deﬁned as the target, canonical handshapes that
are linked to the 26 letters of the English alphabet.

25

•

Peak handshapes are the phonetically realized handshapes used in a given se-
quence at the moment when it most closely approximates the FS-letter.

Orientation – Most FS-letters are produced with the palm facing away from the
signer’s body. The few exceptions to this are -G-, -H-, -P-, and -Q-7 where the palm
faces the signer ( -G- and -H-; labeled side in our analysis), or faces down ( -P- and
-Q-; labeled down in our analysis). Because handshape and orientation changes are
not always synchronized, we have annotated handshape stability as a hold, even if the
hand is continuing to undergo an orientation change. Future annotation is necessary
for orientation changes in detail and determine the pattern of stability and motion that
exists there.

Movement – Two FS-letters are described as having movement:

-J- and -Z-. -J-
involves an orientation change, and -Z- traces the path of the letter8. For both of these
FS-letters, again we have annotated a hold to be where the handshape is stable, regard-
less of orientation change, or path movement.

Handshape detail – A detailed (although not exhaustive) description of hand-
shapes is given in table A.7. This is meant to be guidance to annotators, and is intended
to catch the core features for each handshape, allowing for the systematic variation
known to exist in handshape. If some of these features match, but the handshape is
signiﬁcantly different than expected, annotators added a diacritic (+) to note a large
amount of deviance. This is not intended to exhaustively mark all of the deviant hand-
shapes, but only those that should be looked into further. There are some instances
where a peak is found, but no peak was detected. Although we have not analyzed this
systematically, these instances are frequently peak handshapes that are instantaneous,
or peak handshapes that occur extremely close to each other. These peaks are noted
with a different diacritic (*). Finally if two handshapes have compressed to form a
single peak handshape, a digraph is used to annotate the combined peak handshape.
Examples that we have seen so far are -GH-, - IT-, - IN-, - IO-, - IL-, and - CI-. Here
the digraph is simply two FS-letters that seem to make up the single peak; for consis-
tency they should be written in alphabetical order regardless of the orthographic order
of the letters in the word being ﬁngerspelled. See table A.8 for a description of those
found so far.

Appendix B. Phonetic feature deﬁnitions

7These are the FS-letters traditionally described as having different orientation; there are other possibili-

ties that we have found as well: -X- and -Y-.

8This is frequently abbreviated to just a horizontal line, representing the top bar of the z.

26

FS-Letter

description of the most canonical shape

-A-
-B-
-C-
-D-

-E-

-F-
-G-

-H-
-I-

-J-

-K-

-L-

-M-

-N-

-O-

-P-

-Q-

-R-
-S-
-T-

-U-

-V-

-W-

-X-
-Y-
-Z-

all ﬁngers are ﬂexed, with thumb touching the radial side of the hand, or extended
all ﬁngers are extended. The thumb is hyper ﬂexed across the palm
all the ﬁngers are curved.
the index ﬁnger is fully extended. At least the middle ﬁnger is making contact with the thumb: the ring
and pinky may be either ﬂexed, or making contact with the thumb
the thumb is bent and hyper ﬂexed across the palm, the index ﬁnger is bent and may be touching the
thumb. The other ﬁngers may be bent, like the index ﬁnger, or ﬂexed completely.
contact with the index and thumb. The middle, ring, and pinky are all extended
the index ﬁnger is fully extended. All other ﬁngers are ﬂexed. The thumb is either extended fully, or
unextended, against the middle ﬁnger.
index and middle ﬁngers are fully extended. All others are ﬂexed. The thumb is unextended or extended
the pinky is fully extended, all other ﬁngers are ﬂexed. The thumb is either hyperﬂexed, or unexetended,
against the radial side
the pinky is fully extended, all other ﬁngers are ﬂexed. The thumb is either hyperﬂexed, or unexetended,
against the radial side
the index ﬁnger is fully extended, the middle ﬁnger is extended, but bent 90 at the joint closest to the
hand
the index ﬁnger is fully extended, and the thumb is extended away from the hand. All other ﬁngers are
ﬂexed
the index, middle, and ring ﬁngers are closed, or ﬂat-closed over the thumb, which is hyper ﬂexed across
the palm, possibly touching the base of the pinky and ring ﬁngers
the index and middle ﬁngers are closed, or ﬂat-closed over the thumb, which is hyper ﬂexed across the
palm, possibly touching the base of the ring and middle ﬁngers
the thumb and the index ﬁnger are touching in a curved, closed conﬁguration. The other ﬁngers are
either in the same conﬁguration, touching the thumb, or completely ﬂexed.
the index ﬁnger is fully extended, the middle ﬁnger is extended, but bent 90 at the joint closest to the
hand
the index ﬁnger is fully extended. All other ﬁngers are ﬂexed. The thumb is either extended fully, or
unextended, against the middle ﬁnger.
the index and middle ﬁngers are extended and crossed over each other. All other ﬁngers are ﬂexed
all ﬁngers are completely ﬂexed, with thumb hyperﬂexed across the ﬁst
the index ﬁnger is closed, or ﬂat-closed over the thumb, which is hyper ﬂexed across the palm, possibly
touching the base of the middle and index ﬁngers9
the index and middle ﬁngers are completely extended, all other ﬁngers are ﬂexed, the thumb is hyper-
ﬂexed across the palm
the index and middle ﬁngers are completely extended and are spread apart, all other ﬁngers are ﬂexed,
the thumb is hyperﬂexed across the palm
the index, middle, and ring ﬁngers are completely extended and are spread apart, all other ﬁngers are
ﬂexed, the thumb is hyperﬂexed across the palm
the index ﬁnger is bent similar to -E-, all other ﬁngers are completely ﬂexed
the pinky is fully extended, the thumb is hyper extended away from the hand. All other ﬁngers are ﬂexed
the index ﬁnger is fully extended, and all other ﬁngers are ﬂexed

Table A.7: Description of handshapes.

FS-letter

description of the most canonical handshape

-I- and -T-

-G- and -H-
-I- and -N-

-I- and -O-

-I- and -L-
-C- and -I-

the index ﬁnger is closed, or ﬂat-closed over the thumb, which is hyper ﬂexed across the
palm, possibly touching the base of the middle and index ﬁngers, and the pinky is extended.
This should only be used if the hand reaches this conﬁguration at a single frame.
index and middle ﬁngers as well as the thumb are extended. Similar to the CL 3 handshape.
index and middle ﬁngers are (partially) ﬂexed over the thumb, and the pinky is fully ex-
tended.
index, middle, and ring ﬁngers are looped and touching the thumb, and the pinky is fully
extended.
the index and thumb are extended (the thumb is abducted), and the pinky is fully extended.
the index, middle, and ring ﬁngers as well as the thumb are partially extended (also de-
scribed as curved open), and the pinky is fully extended.

Table A.8: Description of digraphs.

27

letter

index

middle

ring

pinky

spread

palm

MCP
90
180
180
180
135
90
180
180
90
90
180
180
90
90
135
180
180
180
90
90
180
180
180
180
90
180
180

PIP MCP
90
180
90
180
90
135
180
180
90
90
180
180
135
135
135
180
180
180
90
135
180
180
180
135
90
180
180

90
180
180
90
135
180
90
180
90
90
90
90
90
90
135
90
90
180
90
90
180
180
180
90
90
90
180

PIP MCP
90
180
90
135
90
180
90
180
90
90
180
90
135
135
135
180
90
180
90
90
180
180
180
90
90
90
180

90
180
180
90
135
180
90
90
90
90
90
90
90
90
135
90
90
90
90
90
90
90
180
90
90
90
90

PIP MCP
90
180
90
90
90
180
90
90
90
90
90
90
135
90
135
90
90
90
90
90
90
90
180
90
90
90
90

90
180
180
90
135
180
90
90
180
180
90
90
90
90
135
90
90
90
90
90
90
90
90
90
180
90
90

PIP
90
180
90
90
90
180
90
90
180
180
90
90
90
90
135
90
90
90
90
90
90
90
90
90
180
90
90

a
b
c
d
e
f
g
h
i
j
k
l
m
n
o
p
q
r
s
t
u
v
w
x
y
z
zz

0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
-1
0
0
0
1
1
0
1
0
1

thumb
PIP
180
180
135
180
90
180
180
180
180
180
180
180
180
180
180
180
180
180
180
180
180
180
180
180
180
180
180

z
90
90
0
45
0
45
90
90
90
90
90
0
90
90
0
90
90
0
45
90
90
90
90
45
0
45
45

y
0
-45
0
0
-45
0
0
-45
-45
-45
0
90
-45
-45
-45
0
0
-45
-45
-45
-45
-45
-45
-45
90
0
0

touch
i
r
-
m
r
i
m
r
r
r
m
-
p
r
m/i
m
m
r
r
m
r
r
p
m
-
m
m

for
for
for
for
for
for
in
in
for
dwn
for
for
for
for
for
dwn
dwn
for
for
for
for
for
for
for
for
for
for

Table B.9: Phonetic features [2]. The numerical values refer to joint angles in each ﬁnger.

28

References

References

[1] R. E. Mitchell, T. A. Young, B. Bachleda, M. A. Karchmer, How many people
use ASL in the United States? Why estimates need updating, Sign Language
Studies 6 (3) (2006) 306–335.

[2] J. Keane, Towards an articulatory model of handshape: What ﬁngerspelling tells
us about the phonetics and phonology of handshape in American Sign Language,
Ph.D. thesis, University of Chicago (December 2014).

[3] D. Brentari, A Prosodic Model of Sign Language Phonology, MIT Press, 1998.

[4] L. Ding, A. M. Martinez, Modelling and recognition of the linguistic compo-
nents in American Sign Language, Image and Vision Computing 27 (12) (2009)
1826–1844.

[5] D. Tang, T.-H. Yu, T.-K. Kim, Real-time articulated hand pose estimation using

semi-supervised transductive regression forests, in: ICCV, 2013.

[6] C. Padden, D. C. Gunsauls, How the alphabet came to be used in a sign language,

Sign Language Studies 4 (1) (2003) 10–33.

[7] D. Brentari, C. Padden, Native and foreign vocabulary in American Sign Lan-
guage: A lexicon with multiple origins, in: Foreign vocabulary in sign lan-
guages: A cross-linguistic investigation of word formation, Lawrence Erlbaum,
Mahwah, NJ, 2001, pp. 87–119.

[8] P. Goh, E.-J. Holden, Dynamic ﬁngerspelling recognition using geometric and

motion features, in: ICIP, 2006.

[9] S. Liwicki, M. Everingham, Automatic recognition of ﬁngerspelled words in
British Sign Language, in: 2nd IEEE Workshop on CVPR for Human Commu-
nicative Behavior Analysis, 2009.

[10] S. Ricco, C. Tomasi, Fingerspelling recognition through classiﬁcation of letter-

to-letter transitions, in: ACCV, 2009.

[11] T. Kim, K. Livescu, G. Shakhnarovich, American Sign Language ﬁngerspelling
recognition with phonological feature-based tandem models, in: Proc. IEEE
Workshop on Spoken Language Technology (SLT), 2012.

[12] T. Kim, G. Shakhnarovich, K. Livescu, Fingerspelling recognition with semi-

Markov conditional random ﬁelds, in: ICCV, 2013.

[13] T. Kim, W. Wang, H. Tang, K. Livescu, Signer-independent ﬁngerspelling recog-

nition with deep neural network adaptation, in: ICASSP, 2016.

[14] O. Koller, J. Forster, H. Ney, Continuous sign language recognition: Towards
large vocabulary statistical recognition systems handling multiple signers, Com-
puter Vision and Image Understanding 141 (2015) 108–125.

29

[15] S. C. Ong, S. Ranganath, Automatic sign language analysis: A survey and the
future beyond lexical meaning, IEEE transactions on pattern analysis and ma-
chine intelligence 27 (6) (2005) 873–891.

[16] C. Oz, M. C. Leu, Recognition of ﬁnger spelling of American Sign Language
with artiﬁcial neural network using position/orientation sensors and data glove,
in: 2nd International Conference on Advances in Neural Networks, 2005.

[17] R. Wang, J. Popovic, Real-time hand-tracking with a color glove, in: SIG-

GRAPH, 2009.

[18] N. Pugeault, R. Bowden, Spelling it out: Real-time ASL ﬁngerspelling recogni-

tion, in: ICCV Workshops, 2011.

[19] C. Keskin, F. Kırac¸, Y. E. Kara, L. Akarun, Hand pose estimation and hand shape
classiﬁcation using multi-layered randomized decision forests, in: ECCV, 2012.

[20] P. Lu, M. Huenerfauth, Collecting and evaluating the cuny asl corpus for re-
search on american sign language animation, Computer Speech and Language
28 (3) (2014) 812–831.

[21] C. Zhang, Y. Tian, Histogram of 3d facets: a depth descriptor for human action
and hand gesture recognition, Computer Vision and Image Understanding 139
(2015) 29–39.

[22] C. Dong, M. C. Lieu, Z. Yin, American Sign Language alphabet recognition

using Microsoft Kinect, in: CVPR Workshops, 2015.

[23] A. M. Martinez, R. B. Wilbur, R. Shay, A. C. Kak, Purdue ASL database for the

recognition of American sign language, in: ICMI, 2002.

[24] P. Dreuw, C. Neidle, V. Athitsos, S. Sclaroff, H. Ney, Benchmark databases for
video-based automatic sign language recognition., in: International Conference
on Language Resources and Evaluation (LREC), 2008.

[25] U. Von Agris, M. Knorr, K.-F. Kraiss, The signiﬁcance of facial features for
automatic sign language recognition, in: Proc. IEEE International Conference
on Automatic Face and Gesture Recognition, 2008.

[26] P. Dreuw, J. Forster, Y. Gweth, D. Stein, H. Ney, G. Martinez, J. V. Llahi,
O. Crasborn, E. Ormel, W. Du, T. Hoyoux, J. Piater, J. M. M. Lazaro, M. Wheat-
ley, SignSpeak - understanding, recognition, and translation of sign languages,
in: Proc. Workshop on the Representation and Processing of Sign Languages:
Corpora and Sign Language Technologies (CSLT), 2010.

[27] V. Athitsos, C. Neidle, S. Sclaroff, J. Nash, A. Stefan, A. Thangali, H. Wang,
Q. Yuan, Large lexicon project: American sign language video corpus and sign
language indexing/retrieval algorithms, in: Workshop on the Representation
and Processing of Sign Languages: Corpora and Sign Language Technologies
(CSLT), 2010.

30

[28] C. Neidle, C. Vogler, A new web interface to facilitate access to corpora: Devel-
opment of the ASLLRP data access interface (DAI), in: LREC Workshop on the
Representation and Processing of Sign Languages: Interactions between Corpus
and Lexicon, 2012.

[29] E. Efthimiou, S.-E. Fotinea, T. Hanke, J. Glauert, R. Bowden, A. Braffort,
C. Collet, P. Maragos, F. Lefebvre-Albaret, Sign language technologies and re-
sources of the Dicta-Sign project, in: LREC Workshop on the Representation
and Processing of Sign Languages: Interactions between Corpus and Lexicon,
2012.

[30] J. Forster, C. Schmidt, T. Hoyoux, O. Koller, U. Zelle, J. H. Piater, H. Ney,
RWTH-PHOENIX-Weather: A large vocabulary sign language recognition and
translation corpus., in: International Conference on Language Resources and
Evaluation (LREC), 2012.

[31] American

Sign

Language

Lexicon

Video

Dataset.

http://www.bu.edu/av/asllrp/dai-asllvd.html.

[32] NCSLGR. http://www.bu.edu/asllrp/ncslgr-for-download/download-info.html.

[33] C. Neidle, S. Sclaroff, V. Athitsos, SignStream: A tool for linguistic and com-
puter vision research on visual-gestural language data, Behavior Research Meth-
ods, Instruments, & Computers 33 (3) (2001) 311–320.

[34] R. Bowden, D. Windridge, T. Kadir, A. Zisserman, M. Brady, A linguistic fea-
ture vector for the visual interpretation of sign language, in: ECCV, 2004.

[35] R. Yang, S. Sarkar, Detecting coarticulation in sign language using conditional
random ﬁelds, in: Proc. Intl. Conf. on Pattern Recognition (ICPR), 2006.

[36] M. Zahedi, P. Dreuw, D. Rybach, T. Deselaers, H. Ney, Geometric features for
improving continuous appearance-based sign language recognition, in: BMVC,
2006.

[37] M. M. Zaki, S. I. Shaheen, Sign language recognition using a combination of
new vision based features, Pattern Recognition Letters 32 (4) (2011) 572–577.

[38] A. Farhadi, D. Forsyth, R. White, Transfer learning in sign language, in: CVPR,

2007.

[39] P. Dreuw, D. Rybach, T. Deselaers, M. Zahedi, , H. Ney, Speech recognition
techniques for a sign language recognition system, in: Proc. Interspeech, 2007.

[40] S. Nayak, S. Sarkar, B. Loeding, Automated extraction of signs from continuous
sign language sentences using iterated conditional modes, in: CVPR, 2009.

[41] P. Buehler, M. Everingham, D. P. Huttenlocher, A. Zisserman, Upper body de-
tection and tracking in extended signing sequences, International Journal of
Computer Vision 95 (2) (2011) 180–197.

31

[42] T. Pﬁster, J. Charles, M. Everingham, A. Zisserman, Automatic and efﬁcient
long term arm and hand tracking for continuous sign language TV broadcasts,
in: BMVC, 2012.

[43] N. Dalal, B. Triggs, Histograms of oriented gradients for human detection, in:

CVPR, 2005.

[44] P. Jangyodsuk, C. Conly, V. Athitsos, Sign language recognition using dynamic
time warping and hand shape distance based on histogram of oriented gradi-
ent features, in: Proc. 7th International Conference on PErvasive Technologies
Related to Assistive Environments, 2014.

[45] T. Starner, J. Weaver, A. Pentland, Real-time american sign language recognition
using desk and wearable computer based video, IEEE Transactions on Pattern
Analysis and Machine Intelligence 20 (12) (1998) 1371–1375.

[46] C. Vogler, D. Metaxas, Parallel hidden Markov models for American Sign Lan-

guage recognition, in: ICCV, 1999.

[47] A. Thangali, J. P. Nash, S. Sclaroff, C. Neidle, Exploiting phonological con-

straints for handshape inference in ASL video, in: CVPR, 2011.

[48] C. Vogler, D. Metaxas, Handshapes and movements: Multiple-channel ASL

recognition, in: Gesture Workshop, 2003.

[49] S. Theodorakis, A. Katsamanis, P. Maragos, Product-HMMs for automatic sign

language recognition, in: ICASSP, 2009.

[50] J. Odell, The use of context in large vocabulary speech recognition, Ph.D. thesis,

University of Cambridge (March 1995).

[51] M. Ostendorf, Moving beyond the ‘beads-on-a-string’ model of speech, in:
Proc. IEEE Workshop on Automatic Speech Recognition and Understanding
(ASRU), 1999.

[52] K. Livescu, E. Fosler-Lussier, F. Metze, Subword modeling for automatic speech
recognition: Past, present, and emerging approaches, IEEE Signal Processing
Magazine 29 (6) (2012) 44–57.

[53] G. Potamianos, C. Neti, J. Luettin, I. Matthews, Audiovisual Speech Processing,
Cambridge University Press, 2015, Ch. Audiovisual automatic speech recogni-
tion, pp. 193–247.

[54] T. J. Hazen, K. Saenko, C.-H. La, J. R. Glass, A segment-based audio-visual
speech recognizer: Data collection, development, and initial experiments, in:
ICMI, ACM, 2004, pp. 235–242.

[55] K. Saenko, K. Livescu, J. Glass, T. Darrell, Multistream articulatory feature-
based models for visual speech recognition, IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence 31 (9) (2009) 1700–1707.

32

[56] C. Vogler, D. Metaxas, A framework for recognizing the simultaneous aspects
of American Sign Language, Computer Vision and Image Understanding 81
(2001) 358–384.

[57] C. Vogler, D. Metaxas, Toward scalability in ASL recognition: Breaking down

signs into phonemes, in: Gesture Workshop, 1999.

[58] V. Pitsikalis, S. Theodorakis, C. Vogler, P. Maragos, Advances in phonetics-
based sub-unit modeling for transcription alignment and sign language recogni-
tion, in: IEEE CVPR Workshop on Gesture Recognition, 2011.

[59] S. Theodorakis, V. Pitsikalis, P. Maragos, Model-level data-driven sub-units for

signs in videos of continuous sign language, in: ICASSP, 2010.

[60] D. Brentari, A prosodic model of sign language phonology, The MIT Press,

1998.

[61] C. Vogler, D. Metaxas, Adapting hidden Markov models for ASL recognition
by using three-dimensional computer vision methods, in: IEEE International
Conference on Systems, Man and Cybernetics, 1997.

[62] R. Yang, S. Sarkar, B. Loeding, Handling movement epenthesis and hand seg-
mentation ambiguities in continuous sign language recognition using nested dy-
namic programming, IEEE Transactions on Pattern Analysis and Machine Intel-
ligence 32 (3) (2010) 462–477.

[63] S. Theodorakis, V. Pitsikalis, P. Maragos, Advances in dynamic-static integra-
tion of manual cues for sign language recognition, in: The 9th International Ges-
ture Workshop: Gesture in Embodied Communication and Human-Computer
Interaction (GW), 2011.

[64] K. Li, Z. Zhou, C.-H. Lee, Sign transition modeling and a scalable solution to
continuous sign language recognition for real-world applications, ACM Trans-
actions on Accessible Computing (TACCESS) 8 (2) (2016) 7.

[65] V. Athitsos, J. Alon, S. Sclaroff, G. Kollios, BoostMap: A method for efﬁcient

approximate similarity rankings, in: CVPR, 2004.

[66] A. Roussos, S. Theodorakis, V. Pitsikalis, P. Maragos, Afﬁne-invariant modeling
of shape-appearance images applied on sign language handshape classiﬁcation,
in: ICIP, 2010.

[67] U. Von Agris, C. Blomer, K.-F. Kraiss, Rapid signer adaptation for continuous
sign language recognition using a combined approach of eigenvoices, MLLR,
and MAP, in: Proc. Intl. Conf. on Pattern Recognition (ICPR), 2008.

[68] S. C. Ong, S. Ranganath, Deciphering gestures with layered meanings and signer
adaptation, in: Proc. IEEE International Conference on Automatic Face and
Gesture Recognition, 2004.

33

[69] H. Hermansky, D. P. W. Ellis, S. Sharma, Tandem connectionist feature extrac-

tion for conventional HMM systems, in: ICASSP, 2000.

[70] F. Gr´ezl, M. Karaﬁ´at, S. Kont´ar, J. Cernocky, Probabilistic and bottle-neck fea-

tures for LVCSR of meetings, in: ICASSP, 2007.

[71] G. Zweig, P. Nguyen, Segmental CRF approach to large vocabulary continuous
speech recognition, in: Proc. IEEE Workshop on Automatic Speech Recognition
and Understanding (ASRU), 2009.

[72] H. Tang, W. Wang, K. Gimpel, K. Livescu, Discriminative segmental cascades
for feature-rich phone recognition, in: Proc. IEEE Workshop on Automatic
Speech Recognition and Understanding (ASRU), 2015.

[73] L. Lu, L. Kong, C. Dyer, N. A. Smith, S. Renals, Segmental recurrent neural
networks for end-to-end speech recognition, arXiv preprint arXiv:1603.00223.

[74] G. Zweig, P. Nguyen, D. Van Compernolle, K. Demuynck, L. Atlas, P. Clark,
G. Sell, M. Wang, F. Sha, H. Hermansky, et al., Speech recognition with seg-
mental conditional random ﬁelds: A summary of the jhu clsp 2010 summer
workshop, in: ICASSP, 2011.

[75] Y. He, E. Fosler-Lussier, Efﬁcient segmental conditional random ﬁelds for phone

recognition, in: Proc. Interspeech, 2012.

[76] S. Sarawagi, W. W. Cohen, Semi-Markov conditional random ﬁelds for infor-

mation extraction, in: NIPS, 2004.

[77] Q. Shi, L. Cheng, L. Wang, A. Smola, Human action segmentation and recog-
nition using discriminative semi-Markov models, International Journal of Com-
puter Vision 93 (1).

[78] T. V. Duong, H. H. Bui, D. Q. Phung, S. Venkatesh, Activity recognition and ab-
normality detection with the switching hidden semi-Markov model, in: CVPR,
2005.

[79] S.-S. Cho, H.-D. Yang, S.-W. Lee, Sign language spotting based on semi-
Markov conditional random ﬁeld, in: Workshop on the Applications of Com-
puter Vision, 2009.

[80] W. W. Kong, S. Ranganath, Towards subject independent continuous sign lan-
guage recognition: A segment and merge approach, Pattern Recognition 47 (3).

[81] H. Sloetjes, P. Wittenburg, Annotation by category: ELAN and ISO DCR., in:
International Conference on Language Resources and Evaluation (LREC), 2008.

[82] J. Keane, D. Brentari, J. Riggle, Coarticulation in ASL ﬁngerspelling, in: An-

nual Meeting of the North East Linguistic Society, 2012.

34

[83] J. Keane, D. Brentari, J. Riggle, Handshape and coarticulation in ASL ﬁnger-
spelling, conference presentation, linguistic Society of America 2012 Annual
Meeting (January 2012).

[84] J. Keane, D. Brentari, J. Riggle, Dispelling prescriptive rules in ASL ﬁnger-
spelling: the case of -E-, poster, theoretical Issues in Sign Language Research
11; London, UK (July 2013).

[85] S. St¨uker, F. Metze, T. Schultz, A. Waibel, Integrating multilingual articulatory

features into speech recognition., in: Proc. Interspeech, 2003.

[86] O. C¸ etin, A. Kantor, S. King, C. Bartels, M. Magimai-doss, J. Frankel,
K. Livescu, An articulatory feature-based tandem approach and factored obser-
vation modeling, in: ICASSP, 2007.

[87] D. Graff, R. Rosenfeld, D. Paul, CSR-III text, http://http://www.ldc.

upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC95T6
(1995).

[88] O. Abdel-Hamid, L. Deng, D. Yu, H. Jiang, Deep segmental neural networks for

speech recognition, in: Proc. Interspeech, 2013.

[89] Y. He, E. Fosler-Lussier, Segmental conditional random ﬁelds with deep neural
networks as acoustic models for ﬁrst-pass word recognition, in: Proc. Inter-
speech, 2015.

[90] H. Liao, Speaker adaptation of context dependent deep neural networks, in:

ICASSP, 2013.

[91] O. Abdel-Hamid, H. Jiang, Fast speaker adaptation of hybrid NN/HMM model
for speech recognition based on discriminative learning of speaker code, in:
ICASSP, 2013.

[92] P. Swietojanski, S. Renals, Learning hidden unit contributions for unsupervised
speaker adaptation of neural network acoustic models, in: Proc. IEEE Workshop
on Spoken Language Technology (SLT), 2014.

[93] R. Doddipatla, M. Hasan, T. Hain, Speaker dependent bottleneck layer training
for speaker adaptation in automatic speech recognition, in: Proc. Interspeech,
2014.

[94] J. Neto, L. Almeida, M. Hochberg, C. Martins, L. Nunes, S. Renals, T. Robin-
son, Speaker-adaptation for hybrid HMM-ANN continuous speech recognition
system, in: Proc. Eurospeech, 1995.

[95] K. Yao, D. Yu, F. Seide, H. Su, L. Deng, Y. Gong, Adaptation of
context-dependent deep neural networks for automatic speech recognition, in:
Proc. IEEE Workshop on Spoken Language Technology (SLT), 2012.

35

[96] B. Li, K. C. Sim, Comparison of discriminative input and output transformations
for speaker adaptation in the hybrid NN/HMM systems, in: Proc. Interspeech,
2010.

[97] P. Viola, M. J. Jones, Rapid object detection using a boosted cascade of simple

features, in: CVPR, 2001.

[98] M. D. Zeiler, M. Ranzato, R. Monga, M. Mao, K. Yang, Q. V. Le, P. Nguyen,
A. Senior, V. Vanhoucke, J. Dean, G. E. Hinton, On rectiﬁed linear units for
speech processing, in: ICASSP, 2013.

[99] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, R. R. Salakhutdinov,
Dropout: A simple way to prevent neural networks from overﬁtting, Journal of
Machine Learing Research 15 (2014) 1929–1958.

[100] http://www.keithv.com/software/csr.

[101] http://htk.eng.cam.ac.uk.

[102] A. Stolcke, J. Zheng, W. Wang, V. Abrash, SRILM at sixteen: update and out-
look, in: Proc. IEEE Workshop on Automatic Speech Recognition and Under-
standing (ASRU), 2011.

[103] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classiﬁcation with deep
convolutional neural networks, in: Advances in neural information processing
systems, 2012, pp. 1097–1105.

[104] K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale

image recognition, arXiv preprint arXiv:1409.1556.

[105] F. Chollet, keras, https://github.com/fchollet/keras (2015).

[106] Theano Development Team, Theano: A Python framework for fast computation

of mathematical expressions, arXiv e-prints abs/1605.02688.
URL http://arxiv.org/abs/1605.02688

[107] SCTK. http://www.nist.gov/speech/tools/index.htm.

36

