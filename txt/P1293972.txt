Uniﬁed Deep Supervised Domain Adaptation and Generalization

Saeid Motiian Marco Piccirilli

Donald A. Adjeroh

Gianfranco Doretto

West Virginia University
Morgantown, WV 26508
{samotiian, mpiccir1, daadjeroh, gidoretto}@mix.wvu.edu

7
1
0
2
 
p
e
S
 
8
2
 
 
]

V
C
.
s
c
[
 
 
1
v
0
9
1
0
1
.
9
0
7
1
:
v
i
X
r
a

Abstract

This work provides a uniﬁed framework for addressing
the problem of visual supervised domain adaptation and
generalization with deep models. The main idea is to exploit
the Siamese architecture to learn an embedding subspace
that is discriminative, and where mapped visual domains
are semantically aligned and yet maximally separated. The
supervised setting becomes attractive especially when only
few target data samples need to be labeled. In this scenario,
alignment and separation of semantic probability distribu-
tions is difﬁcult because of the lack of data. We found that by
reverting to point-wise surrogates of distribution distances
and similarities provides an effective solution. In addition,
the approach has a high “speed” of adaptation, which re-
quires an extremely low number of labeled target training
samples, even one per category can be effective. The ap-
proach is extended to domain generalization. For both ap-
plications the experiments show very promising results.

1. Introduction

Many computer vision applications require enough la-
beled data (target data) for training visual classiﬁers to ad-
dress a speciﬁc task at hand. Whenever target data is either
not available, or it is expensive to collect and/or label it, the
typical approach is to use available datasets (source data),
representative of a closely related task. Since this practice
is known for leading to suboptimal performance, techniques
such as domain adaptation [6] and/or domain generaliza-
tion [5] have been developed to address the issue. Domain
adaptation methods require target data, whereas domain
generalization methods do not. Domain adaptation can be
either supervised [60, 33], unsupervised [38, 38, 61], or
semi-supervised [26, 29, 67]. Unsupervised domain adap-
tation (UDA) is attractive because it does not require target
data to be labeled. Conversely, supervised domain adapta-
tion (SDA) requires labeled target data.

UDA expects large amounts of target data in order to
be effective, and this is emphasized even more when using
deep models. Moreover, given the same amount of target

data, SDA typically outperforms UDA, as we will later ex-
plain. Therefore, especially when target data is scarce, it is
more attractive to use SDA, also because limited amounts
of target data are likely to not be very expensive to label.

In the absence of target data, domain generalization
(DG) exploits several cheaply available datasets (sources),
representing different speciﬁc but closely related tasks. It
then attempts to learn by combining data sources in a way
that produces visual classiﬁers that are less sensitive to the
speciﬁc target data that will need to be processed.

In this work, we introduce a supervised approach for vi-
sual recognition that can be used for both SDA and DG.
The SDA approach requires very few labeled target sam-
ples per category in training. Indeed, even one sample can
signiﬁcantly increase performance, and a few others bring
it closer to a peak, showing a remarkable “speed” of adap-
tation. Moreover, the approach is also robust to adapt-
ing to categories that have no target labeled samples. Al-
though domain adaptation and generalization are closely
related, adaptation techniques are not directly applied to
DG, and viceversa. However, we show that by making sim-
ple changes to our proposed training loss function, and by
maintaining the same architecture, our SDA approach very
effectively extends to DG.

Using basic principles, we analyze how visual classiﬁ-
cation is extended to handle UDA by aligning a source do-
main distribution to a target domain distribution to make
the classiﬁer domain invariant. This leads to observing that
SDA approaches improve upon UDA by making the align-
ment semantic, because they can ensure the alignment of se-
mantically equivalent distributions from different domains.
However, we go one step ahead by suggesting that semantic
distribution separation should further increase performance,
and this leads to the introduction of a classiﬁcation and con-
trastive semantic alignment (CCSA) loss.

We deal with the limited size of target domain samples
by observing that the CCSA loss relies on computing dis-
tances and similarities between distributions (as typically
done in adaptation and generalization approaches). Those
are difﬁcult to represent with limited data. Thus, we revert

1

Figure 1. Deep supervised domain adaptation. In training, the semantic alignment loss minimizes the distance between samples from
different domains but the same class label and the separation loss maximizes the distance between samples from different domains and
class labels. At the same time, the classiﬁcation loss guarantees high classiﬁcation accuracy.

to point-wise surrogates. The resulting approach turns out
to be very effective as shown in the experimental section.

2. Related work

Domain adaptation. Visual recognition algorithms are
trained with data from a source domain, and when they are
tested on a target domain with marginal distribution that dif-
fers from the one of the sources, we experience the visual
domain adaptation (DA) problem (also known as dataset
bias [48, 59, 58], or covariate shift [54]), and observe a per-
formance decrease.

Traditional DA methods attempt to directly minimize
the shift between source and target distributions. We di-
vide them in three categories. The ﬁrst one includes those
that try to ﬁnd a mapping between source and target dis-
tributions [53, 34, 27, 26, 19, 57]. The second one seeks
to ﬁnd a shared latent space for source and target distribu-
tions [40, 2, 44, 21, 22, 47, 43]. The third one regularizes
a classiﬁer trained on a source distribution to work well on
a target distribution [4, 1, 66, 15, 3, 12]. UDA approaches
fall in the ﬁrst and second categories, while SDA methods
could fall either in the second or third category or some-
times both. Recently, [7, 42] have addressed UDA when
an auxiliary data view [36, 43], is available during training,
which is beyond the scope of this work.

Here, we are interested in ﬁnding a shared subspace for
source and target distributions. Among algorithms for sub-
space learning, Siamese networks [11] work well for differ-
ent tasks [14, 55, 35, 63, 9]. Recently, Siamese networks
In [60], which is
have been used for domain adaptation.
an SDA approach, unlabeled and sparsely labeled target do-
main data are used to optimize for domain invariance to fa-
cilitate domain transfer while using a soft label distribution
matching loss. In [56], which is a UDA approach, unlabeled
target data is used to learn a nonlinear transformation that
aligns correlations of layer activations in deep neural net-
works. Some approaches went beyond the Siamase weight-

sharing and used couple networks for DA. [33] uses two
CNN streams, for source and target, fused at the classiﬁer
level. [50] uses a two-streams architecture, for source and
target, with related but not shared weights. Here we use
a Siamese network to learn an embedding such that sam-
ples from the same class are mapped as close as possible to
each other. This semantic alignment objective is similar to
other deep approaches, but unlike them, we explicitly model
and introduce cross-domain class separation forces. More-
over, we do so with very few training samples, which makes
the problem of characterizing distributions challenging, and
this is why we propose to use point-wise surrogates.

Domain generalization. Domain generalization (DG) is a
less investigated problem and is addressed in two ways. In
the ﬁrst one, all information from the training domains or
datasets is aggregated to learn a shared invariant represen-
tation. Speciﬁcally, [5] pulls all of the training data together
[44]
in one dataset, and learns a single SVM classiﬁer.
learns an invariant transformation by minimizing the dis-
similarity across domains. [23], which can be used for SDA
too, ﬁnds a representation that minimizes the mismatch be-
tween domains and maximizes the separability of data. [24]
learns features that are robust to variations across domains.

The second approach to DG is to exploit all information
from the training domains to train a classiﬁer or regulate its
weights [32, 17, 65, 45, 46]. Speciﬁcally, [32] adjusts the
weights of the classiﬁer to work well on an unseen dataset,
and [65] fuses the score of exemplar classiﬁers given any
test sample. While most works use the shallow models, here
we approach DG as in the ﬁrst way, and extend the proposed
SDA approach by training a deep Siamese network to ﬁnd
a shared invariant representation where semantic alignment
as well as separation are explicitly accounted for. To the
best of our knowledge, [24] is the only DG approach using
deep models, and our method is the ﬁrst deep method that
solves both adaptation and generalization.

Figure 2. Deep domain generalization. In training, the semantic alignment loss minimizes the distance between samples from different
domains but the same class label and the separation loss maximizes the distance between samples from different domains and class labels.
At the same time, the classiﬁcation loss guarantees high classiﬁcation accuracy. In testing, the embedding function embeds samples from
unseen distributions to the domain invariant space and the prediction function classiﬁes them (right). In this ﬁgure, different colors represent
different domain distributions and different shapes represent different classes.

3. Supervised DA with Scarce Target Data

i , ys

i )}N

i=1. The feature xs

In this section we describe the model we propose to ad-
dress supervised domain adaptation (SDA), and in the fol-
lowing Section 4 we extend it to address the domain gen-
eralization problem. We are given a training dataset made
of pairs Ds = {(xs
i ∈ X is a re-
alization from a random variable X s, and the label ys
i ∈ Y
is a realization from a random variable Y . In addition, we
are also given the training data Dt = {(xt
i=1, where
i ∈ X is a realization from a random variable X t, and the
xt
labels yt
i ∈ Y. We assume that there is a covariate shift [54]
between X s and X t, i.e., there is a difference between the
probability distributions p(X s) and p(X t). We say that X s
represents the source domain and that X t represents the tar-
get domain. Under this settings the goal is to learn a predic-
tion function f : X → Y that during testing is going to
perform well on data from the target domain.

i )}M

i, yt

The problem formulated thus far is typically referred to
as supervised domain adaptation. In this work we are es-
pecially concerned with the version of this problem where
only very few target labeled samples per class are available.
We aim at handling cases where there is only one target la-
beled sample, and there can even be some classes with no
target samples at all.

3.1. Deep SDA

In the absence of covariate shift a visual classiﬁer f is

trained by minimizing a classiﬁcation loss

LC(f ) = E[(cid:96)(f (X s), Y )] ,

(1)

where E[·] denotes statistical expectation and (cid:96) could be any
appropriate loss function (for example categorical cross-
entropy for multi-class classiﬁcation). When the distribu-
tions of X s and X t are different, a deep model fs trained
with Ds will have reduced performance on the target do-
Increasing it would be trivial by simply training a
main.
new model ft with data Dt. However, Dt is small and deep
models require large amounts of labeled data.

In general, f could be modeled by the composition of
two functions, i.e., f = h ◦ g. Here g : X → Z would
be an embedding from the input space X to a feature or
embedding space Z, and h : Z → Y would be a function
for predicting from the feature space. With this notation we
would have fs = hs ◦ gs and ft = ht ◦ gt, and the SDA
problem would be about ﬁnding the best approximation for
gt and ht, given the constraints on the available data.

The unsupervised DA paradigm (UDA) assumes that Dt
does not have labels. In that case the typical approach as-
sumes that gt = gs = g, and f minimizes (1), while g also
minimizes

LCA(g) = d(p(g(X s)), p(g(X t))) .

(2)

The purpose of (2) is to align the distributions of the fea-
tures in the embedding space, mapped from the source and
the target domains. d is meant to be a metric between dis-
tributions that once aligned, they will no longer allow to tell
whether a feature is coming from the source or the target
domain. For that reason, we refer to (2) as the confusion
alignment loss. A popular choice for d is the Maximum
Mean Discrepancy [28]. In the embedding space Z, fea-
tures are assumed to be domain invariant. Therefore, UDA
methods say that from the feature to the label space it is safe
to assume that ht = hs = h.

Since we are interested in visual recognition, the embed-
ding function g would be modeled by a convolutional neural
network (CNN) with some initial convolutional layers, fol-
lowed by some fully connected layers. In addition, the train-
ing architecture would have two streams, one for source and
the other for target samples. Since gs = gt = g, the CNN
parameters would be shared as in a Siamese architecture. In
addition, the source stream would continue with additional
fully connected layers for modeling h. See Figure 1.

From the above discussion it is clear that in order to per-
form well, UDA needs to align effectively. This can hap-
pen only if distributions are represented by a sufﬁciently
large dataset. Therefore, UDA approaches are in a posi-
tion of weakness because we assume Dt to be small. More-

Figure 3. Visualization of the MNIST-USPS datasets. Left: 2D visualization of the row images of the MNIST-USPS datasets. The
samples from the same class and different domains lie far from each other on the 2D subspace. Middle: 2D visualization of the embedded
images using our base model (without domain adaptation). The samples from the same class and different domains still lie far from each
other on the 2D subspace. Right: 2D visualization of the embedded images using our SDA model. The samples from the same class and
different domains lie very close to each other on the 2D subspace.

over, UDA approaches have also another intrinsic limita-
tion, which is that even with perfect confusion alignment,
there is no guarantee that samples from different domains
but the same class label, would map nearby in the embed-
ding space. This lack of semantic alignment is a major
source of performance reduction.

SDA approaches easily address the semantic alignment

problem by replacing (2) with

LSA(g) =

d(p(g(X s

a)), p(g(X t

a))) ,

(3)

C
(cid:88)

a=1

a = X s|{Y =
where C is the number of class labels, and X s
a = X t|{Y = a} are conditional random vari-
a} and X t
ables. d instead is a suitable distance metric between the
distributions of X s
a in the embedding space. We
refer to (3) as the semantic alignment loss, which clearly
encourages samples from different domains but the same
label, to map nearby in the embedding space.

a and X t

While the analysis above clearly indicates why SDA pro-
vides superior performance than UDA, it also suggests that
deep SDA approaches have not considered that greater per-
formance could be achieved by encouraging class separa-
tion, meaning that samples from different domains and with
different labels, should be mapped as far apart as possible in
the embedding space. This idea means that, in principle, a
semantic alignment less prone to errors should be achieved
by adding to (3) the following term

LS(g) =

k(p(g(X s

a)), p(g(X t

b))) ,

(4)

(cid:88)

a,b|a(cid:54)=b

where k is a suitable similarity metric between the distri-
butions of X s
b in the embedding space, which adds

a and X t

a penalty when the distributions p(g(X s
b))
come close, since they would lead to lower classiﬁcation
accuracy. We refer to (4) as the separation loss.

a)) and p(g(X t

Finally, we suggest that SDA could be approached by

learning a deep model f = h ◦ g such that

LCCSA(f ) = LC(h ◦ g) + LSA(g) + LS(g) .

(5)

We refer to (5) as the classiﬁcation and contrastive semantic
alignment loss. This would allow to set gs = gt = g. The
classiﬁcation network h is trained only with source data, so
hs = h. In addition, to improve performance on the target
domain, ht could be obtained via ﬁne-tuning based on the
few samples in Dt, i.e.,

ht = ﬁne-tuning(h|Dt) .

(6)

Note that the network architecture remains the one in Fig-
ure 1, only with a different loss, and training procedure.

3.2. Handling Scarce Target Data

When the size of the labeled target training dataset Dt
is very small, minimizing the loss (5) becomes a challenge.
The problem is that the semantic alignment loss as well as
the separation loss rely on computing distances and simi-
larities between distributions, and those are very difﬁcult to
represent with as few as one data sample.

Rather than attempting to characterize distributions with
statistics that require enough data, because of the reduced
size of Dt, we compute the distance in the semantic align-
ment loss (3) by computing average pairwise distances be-
tween points in the embedding space, i.e., we compute

d(p(g(X s

a)), p(g(X t

a))) =

d(g(xs

i ), g(xt

j)) ,

(7)

(cid:88)

i,j

available during training, thus representative of an unknown
target domain.

The SDA method in Section 3 treats source and target
datasets Ds and Dt almost symmetrically. In particular, the
embedding g aims at achieving semantic alignment, while
favoring class separation. The only asymmetry is in the pre-
diction function h that is trained only on the source, to be
then ﬁne-tuned on the target.

In domain generalization, we are not interested in adapt-
ing the classiﬁer to the target domain, because it is un-
known. Instead, we want to make sure that the embedding
g maps to a domain invariant space. To do so we consider
every distinct unordered pair of source domains (u, v), rep-
resented by Dsu and Dsv , and, like in SDA, impose the se-
mantic alignment loss (3) as well as the separation loss (4).
Moreover, the losses are summed over every pair in order to
make the map g as domain invariant as possible. Similarly,
the classiﬁer h should be as correct as possible for any of
the mapped samples, to maximize performance on an un-
seen target. This calls for having a fully symmetric learning
for h by training it on all the source domains, meaning that
the classiﬁcation loss (1) is summed over every domain su.
See Figure 2.

The network architecture is still the one in Figure 1, and
we have implemented it with the same choices for distances
and similarities as those made in Section 3.2. However,
since we are summing the losses (3) and (4) over every un-
ordered pair of source domains, there is a quadratic growth
of paired training samples. So, if necessary, rather than pro-
cessing every paired sample, we select them randomly.

5. Experiments

We divide the experiments into two parts, domain adap-
tation and domain generalization. In both sections, we use
benchmark datasets and compare our domain adaptation
model and our domain generalization model, both indicated
as CCSA, with the state-of-the-art.

5.1. Domain Adaptation

We present results using the Ofﬁce dataset [53], the

MNIST dataset [37], and the USPS dataset [31].

5.1.1 Ofﬁce Dataset

The ofﬁce dataset is a standard benchmark dataset for visual
domain adaptation. It contains 31 object classes for three
domains: Amazon, Webcam, and DSLR, indicated as A,
W, and D, for a total of 4,652 images. We consider six
domain shifts using the three domains (A → W, A → D,
W → A, W → D, D → A, and D → W). We performed
different experiments using this dataset.
First experiment. We followed the setting described
in [60]. All classes of the ofﬁce dataset and 5 train-test splits

Figure 4. (a), (b), (c): Improvement of CCSA over the base model.
(d): Average classiﬁcation accuracy for M → U task for different
number of labeled target samples per category (n). It shows that
our model provides signiﬁcant improvement over baselines.

i = yt

where it is assumed ys
j = a. The strength of this ap-
proach is that it allows even a single labeled target sample
to be paired with all the source samples, effectively trying
to semantically align the entire source data with the few tar-
get data. Similarly, we compute the similarities in the sep-
aration loss (4) by computing average pairwise similarities
between points in the embedding space, i.e., we compute

k(p(g(X s

a)), p(g(X t

b))) =

k(g(xs

i ), g(xt

j)) ,

(8)

(cid:88)

i,j

where it is assumed that ys

i = a (cid:54)= yt

j = b.

Moreover, our implementation further assumes that

d(g(xs

i ), g(xt

j)) =

(cid:107)g(xs

i ) − g(xt

j)(cid:107)2 ,

(9)

k(g(xs

i ), g(xt

j)) =

max(0, m − (cid:107)g(xs

i ) − g(xt

j)(cid:107))2(10)

1
2
1
2

where (cid:107) · (cid:107) denotes the Frobenius norm, and m is the mar-
gin that speciﬁes the separability in the embedding space.
Note that with the choices outlined in (9) and (10), the loss
LSA(g)+LS(g) becomes the well known contrastive loss as
deﬁned in [30]. Finally, to balance the classiﬁcation versus
the contrastive semantic alignment portion of the loss (5),
(7) and (8) are normalized and weighted by 1 − γ and (1)
by γ.

4. Extension to Domain Generalization

In visual domain generalization (DG), D labeled datasets
Ds1 , · · · , DsD , representative of D distinct source domains
are given. The goal is to learn from them a visual classiﬁer
f that during testing is going to perform well on data Dt, not

Table 1. Ofﬁce dataset. Classiﬁcation accuracy for domain adaptation over the 31 categories of the Ofﬁce dataset. A, W, and D stand for
Amazon, Webcam, and DSLR domain. Lower Bound is our base model without adaptation.

A → W
A → D
W → A
W → D
D → A
D → W
Average

Lower Bound
61.2 ± 0.9
62.3 ± 0.8
51.6 ± 0.9
95.6 ± 0.7
58.5 ± 0.8
80.1 ± 0.6
68.2

[62]
61.8 ± 0.4
64.4 ± 0.3
52.2 ± 0.4
98.5 ± 0.4
52.1 ± 0.8
95.0 ± 0.5
70.6

Unsupervised
[39]
68.5 ± 0.4
67.0 ± 0.4
53.1 ± 0.3
99.0 ± 0.2
54.0 ± 0.4
96.0 ± 0.3
72.9

[25]
68.7 ± 0.3
67.1 ± 0.3
54.09 ± 0.5
99.0 ± 0.2
56.0 ± 0.5
96.4 ± 0.3
73.6

[60]
82.7 ± 0.8
86.1 ± 1.2
65.0 ± 0.5
97.6 ± 0.2
66.2 ± 0.3
95.7 ± 0.5
82.21

Supervised
[33]
84.5 ± 1.7
86.3 ± 0.8
65.7 ± 1.7
97.5 ± 0.7
66.5 ± 1.0
95.5 ± 0.6
82.68

CCSA
88.2 ± 1.0
89.0 ± 1.2
72.1 ± 1.0
97.6 ± 0.4
71.8 ± 0.5
96.4 ± 0.8
85.8

Table 2. Ofﬁce dataset. Classiﬁcation accuracy for domain adap-
tation over the Ofﬁce dataset when only the labeled target samples
of 15 classes are available during training. Testing is done on all
31 classes. A, W, and D stand for Amazon, Webcam, and DSLR
domain. Lower Bound is our base model without adaptation.

A → W
A → D
W → A
W → D
D → A
D → W
Average

Lower Bound
52.1 ± 0.6
61.6 ± 0.8
34.5 ± 0.9
95.1 ± 0.2
40.1 ± 0.3
89.7 ± 0.8
62.26

[60]
59.3 ± 0.6
68.0 ± 0.5
40.5 ± 0.2
97.5 ± 0.1
43.1 ± 0.2
90.0 ± 0.2
66.4

CCSA
63.3 ± 0.9
70.5 ± 0.6
43.6 ± 1.0
96.2 ± 0.3
42.6 ± 0.6
90.0 ± 0.2
67.83

are considered. For the source domain, 20 examples per cat-
egory for the Amazon domain, and 8 examples per category
for the DSLR and Webcam domains are randomly selected
for training for each split. Also, 3 labeled examples are ran-
domly selected for each category in the target domain for
training for each split. The rest of the target samples are
used for testing. Note that we used the same splits gener-
ated by [60]. We also report the classiﬁcation results of the
SDA algorithm presented in [39] and [33]. In addition to the
SDA algorithms, we report the results of some recent UDA
algorithms. They follow a different experimental protocol
compared to the SDA algorithms, and use all samples of the
target domain in training as unlabeled data together with all
samples of the source domain.

For the embedding function g, we used the convolutional
layers of the VGG-16 architecture [55] followed by 2 fully
connected layers with output size of 1024 and 128, respec-
tively. For the prediction function h, we used a fully con-
nected layer with softmax activation. Similar to [60], we
used the weights pre-trained on the ImageNet dataset [51]
for the convolutional layers, and initialized the fully con-
nected layers using all the source domain data. We then
ﬁne-tuned all the weights using the train-test splits.

Table 1 reports the classiﬁcation accuracy over 31 classes
for the Ofﬁce dataset and shows that CCSA has better per-
formance compared to [60]. Since the difference between
W domain and D domain is not considerable, unsupervised
algorithms work well on D → W and W → D. However,
in the cases when target and source domains are very dif-
ferent (A → W, W → A, A → D, and D → A), CCSA
shows larger margins compared to the second best. This

suggests that CCSA will provide greater alignment gains
when there are bigger domain shifts. Figure 4(a) instead,
shows how much improvement can be obtained with respect
to the base model. This is simply obtained by training g and
h with only the classiﬁcation loss and source training data,
so no adaptation is performed.
Second experiment. We followed the setting described
in [60] when only 10 target labeled samples of 15 classes
of the Ofﬁce dataset are available during training. Similar
to [60], we compute the accuracy on the remaining 16 cat-
egories for which no target data was available during train-
ing. We used the same network structure as in the ﬁrst ex-
periment and the same splits generated by [60].

Table 2 shows that CCSA is effective at transferring in-
formation from the labeled classes to the unlabeled target
classes. Similar to the ﬁrst experiment, CCSA works well
when shifts between domains are larger.
Third experiment. We used the original train-test splits of
the Ofﬁce dataset [53]. The splits are generated in a sim-
ilar manner to the ﬁrst experiment but here instead, only
10 classes are considered (backpack, bike, calculator, head-
phones, keyboard, laptop-computer, monitor, mouse, mug,
and projector).
In order to compare our results with the
state-of-the-art, we used DeCaF-fc6 features [14] and 800-
dimension SURF features as input. For DeCaF-fc6 features
(SURF features) we used 2 fully connected layers with out-
put size of 1024 (512) and 128 (32) with ReLU activation as
the embedding function, and one fully connected layer with
softmax activation as the prediction function. The features
and splits are available on the Ofﬁce dataset webpage 1.

We compared our results with three UDA (GFK [26],
mSDA [8], and RTML [13]) and one SDA (CDML [64])
algorithms under the same settings. Table 3 shows that
CCSA provides an improved accuracy with respect to the
others. Again, greater domain shifts are better compensated
by CCSA . Figure 4(b) shows the improvement of CCSA
over the base model using DeCaF-fc6 features.

5.1.2 MNIST-USPS Datasets

The MNIST (M) and USPS (U) datasets have recently been
used for domain adaptation [20, 50]. They contain images

1https://cs.stanford.edu/˜jhoffman/domainadapt/

Table 3. Ofﬁce dataset. Classiﬁcation accuracy for domain adaptation over the 10 categories of the Ofﬁce dataset. A, W, and D stand for
Amazon, Webcam, and DSLR domain. Lower Bound is our base model with no adaptation.
Lower Bound GFK [26]

CDML [64] RTML [13]

mSDA [8]

CCSA

A → W
A → D
W → A
W → D
D → A
D → W
Average

A → W
A → D
W → A
W → D
D → A
D → W
Average

26.5 ± 3.1
17.5 ± 1.2
25.9 ± 1.0
46.9 ± 1.1
19.3 ± 1.9
48.0 ± 2.1
30.6

78.9 ± 1.8
79.2 ± 2.1
77.3 ± 1.1
96.6 ± 1.0
84.0 ± 1.3
96.7 ± 0.9
85.4

SURF

39.9 ± 0.9
36.2 ± 0.7
29.8 ± 0.6
80.9 ± 0.4
33.2 ± 0.6
79.4 ± 0.6
43.5

73.1 ± 2.8
82.6 ± 2.1
82.6 ± 1.3
98.8 ± 0.9
85.4 ± 0.7
91.3 ± 0.4
85.63

35.5 ± 0.5
29.7 ± 0.7
32.1 ± 0.8
56.6 ± 0.4
33.6 ± 0.8
68.6 ± 0.7
38.4

64.6 ± 4.2
72.6 ± 3.5
71.4 ± 1.7
99.5 ± 0.6
78.8 ± 0.5
97.5 ± 0.4
80.73

37.3 ± 0.7
35.3 ± 0.5
32.4 ± 0.5
77.9 ± 0.9
29.4 ± 0.8
79.4 ± 0.6
43.5

75.9 ± 2.1
81.4 ± 2.6
86.3 ± 1.6
99.4 ± 0.4
88.4 ± 0.5
95.1 ± 0.5
87.75

DeCaF-fc6

43.4 ± 0.9
43.3 ± 0.6
37.5 ± 0.7
91.7 ± 1.1
36.3 ± 0.3
90.5 ± 0.7
49.8

79.5 ± 2.6
83.8 ± 1.7
90.8 ± 1.6
100 ± 0.0
90.6 ± 0.5
98.6 ± 0.3
90.55

71.2 ± 1.3
74.2 ± 1.3
42.9 ± 0.9
85.1 ± 1.0
28.9 ± 1.3
77.3 ± 1.6
63.2

94.5 ± 1.9
97.2 ± 1.0
91.2 ± 0.8
99.6 ± 0.5
91.7 ± 1.0
98.7 ± 0.6
95.4

Table 4. MNIST-USPS datasets. Classiﬁcation accuracy for do-
main adaptation over the MNIST and USPS datasets. M and
U stand for MNIST and USPS domain. Lower Bound is our
base model without adaptation. CCSA - n stands for our method
when we use n labeled target samples per category in training.
M → U
89.4
91.2
65.4
85.0
89.0
90.1
91.4
92.4
93.0
92.9
92.8

Method
ADDA [61]
CoGAN [38]
Lower Bound
CCSA-1
CCSA-2
CCSA-3
CCSA-4
CCSA-5
CCSA-6
CCSA-7
CCSA-8

89.7
90.1
62.0
81.7
85.5
87.9
88.7
90.1
91.3
91.1
91.4

90.1
89.1
58.6
78.4
82.0
85.8
86.1
88.8
89.6
89.4
90.0

U → M Average

Table 5. VLCS dataset. Classiﬁcation accuracy for domain gen-
eralization over the 5 categories of the VLCS dataset. LB (Lower
Bound) is our base model trained without the contrastive semantic
alignment loss. 1NN stands for ﬁrst nearest neighbor.

1NN
L, C, S → V
57.2
V, C, S → L 52.4
V, L, S → C
90.5
V, L, C → S
56.9
C, S → V, L 55.0
C, L → V, S
52.6
V, C → L, S
56.6
Average
60.1

Lower Bound
LB
SVM
59.1
58.4
55.6
55.2
86.1
85.1
54.6
55.2
55.3
55.5
50.9
51.8
60.1
59.9
60.2
60.1

Domain Generalization

UML [17] LRE-SVM [65] SCA [23]
60.5
59.7
88.1
54.8
55.0
52.8
58.8
61.4

56.2
58.5
91.1
58.4
56.4
57.4
55.4
61.5

64.3
59.6
88.9
59.2
59.5
55.9
60.7
64.0

CCSA
67.1
62.1
92.3
59.1
59.3
56.5
60.2
65.0

of digits from 0 to 9. We considered two cross-domain
tasks, M → U and U → M, and followed the experimental
setting in [20, 50], which involves randomly selecting 2000
images from MNIST and 1800 images from USPS. Here,
we randomly selected n labeled samples per class from tar-
get domain data and used them in training. We evaluated
our approach for n ranging from 1 to 8 and repeated each
experiment 10 times (we only show the mean of the accura-
cies because the standard deviation is very small).

Similar to [37], we used 2 convolutional layers with 6
and 16 ﬁlters of 5 × 5 kernels followed by max-pooling
layers and 2 fully connected layers with size 120 and 84

as the embedding function g, and one fully connected layer
with softmax activation as the prediction function h. We
compare our method with 2 recent UDA methods. Those
methods use all target samples in their training stage, while
we only use very few labeled target samples per category in
training.

Table 4 shows the average classiﬁcation accuracy of the
MNIST-USPS datasets. CCSA works well even when only
one target sample per category (n = 1) is available in train-
ing. Also, we can see that by increasing n, the accuracy
quickly converges to the top.
Ablation study. We consider three baselines to compare
with CCSA for M → U task. First, we train the network
with source data and then ﬁne-tune it with available target
data. Second, we train the network using the classiﬁcation
and semantic alignment losses (LCSA(f ) = LC(h ◦ g) +
LSA(g)). Third, we train the network using the classiﬁca-
tion and separation losses (LCS(f ) = LC(h ◦ g) + LS(g)).
Figure 4(d) shows the average accuracies over 10 repeti-
tions. It shows that CSA and CS improve the accuracy over
ﬁne-tuning. Using the semantic alignment loss together
with separation loss (CCSA) shows the best performance.
Visualization. We show how samples lie on the embedding
space using CCSA . First, we considered the row images
of the MNIST and USPS datasets and plotted 2D visualiza-
tion of them using t-SNE algorithm [41]. As Figure 3(Left)
shows the row images of the same class and different do-
mains lie far away from each other in the 2D subspace. For
example, the samples of the class zero of the USPS dataset
(0 U ) are far from the class zero of the MNIST dataset
(0 M ). Second, we trained our base model with no adap-
tation on the MNIST dataset. We then plotted the 2D visu-
alization of the MNIST and USPS samples in the embed-
ding space (output of g, the last fully connected layer). As
Figure 3(Middle) shows, the samples from the same class
and different domains still lie far away from each other in
the 2D subspace. Finally, we trained our SDA model on the

MNIST dataset and 3 labeled samples per class of the USPS
dataset. We then plotted the 2D visualization of the MNIST
and USPS samples in the embedding space (output of g).
As Figure 3(Right) shows, the samples from the same class
and different domains now lie very close to each other in the
2D subspace. Note however, that this is only a 2D visualiza-
tion of high-dimensional data, and Figure 3(Right) may not
perfectly reﬂect how close is the data from the same class,
and how classes are separated.
Weight sharing: There is no restriction whether gt and gs
should share the weights or not. Not sharing weights will
likely lead to overﬁtting, given the reduced amount of target
training data, and weight-sharing can be seen as a regular-
izer. We repeated the experiment for M → U task when
n = 4. Not sharing weights provides the average accuracy
88.6 over 10 repetitions which is less than the average accu-
racy with weight-sharing (see Table 4). A similar behavior
can be seen for other experiments.

5.2. Domain Generalization

We evaluate CCSA on different datasets. The goal is to
show that CCSA is able to learn a domain invariant embed-
ding subspace for visual recognition tasks.

5.3. VLCS Dataset

In this section, we use images of 5 shared object cat-
egories (bird, car, chair, dog, and person), of the PAS-
CAL VOC2007 (V) [16], LabelMe (L) [52], Caltech-101
(C) [18], and SUN09 (S) [10] datasets, which is known as
VLCS dataset [17].

[24, 23] have shown that there are covariate shifts be-
tween the above 4 domains and have developed a DG
method to minimize them. We followed their experi-
mental setting, and randomly divided each domain into a
training set (70%) and a test set (30%) and conducted a
leave-one-domain-out evaluation (4 cross-domain
cases) and a leave-two-domain-out evaluation (3
cross-domain cases). In order to compare our results with
the state-of-the-art, we used DeCaF-fc6 features which are
publicly available 2, and repeated each cross-domain case
20 times and reported the average classiﬁcation accuracy.

We used 2 fully connected layers with output size of
1024 and 128 with ReLU activation as the embedding func-
tion g, and one fully connected layer with softmax activa-
tion as the prediction function h. To create positive and
negative pairs for training our network, for each sample of a
source domain we randomly selected 5 samples from each
remaining source domain, and help in this way to avoid
overﬁtting. However, to train a deeper network together
with convolutional layers, it is enough to create a large
amount of positive and negative pairs.

2http://www.cs.dartmouth.edu/˜chenfang/proj_

page/FXR_iccv13/index.php

Table 6. MNIST dataset. Classiﬁcation accuracy for domain gen-
eralization over the MNIST dataset and its rotated domains.

M15◦ , M30◦ , M45◦ , M60◦ , M75◦ → M
M, M30◦ , M45◦ , M60◦ , M75◦ → M15◦
M, M15◦ , M45◦ , M60◦ , M75◦ → M30◦
M, M15◦ , M30◦ , M60◦ , M75◦ → M45◦
M, M15◦ , M30◦ , M45◦ , M75◦ → M60◦
M, M15◦ , M30◦ , M45◦ , M60◦ → M75◦
Average

CAE [49] MTAE [24] CCSA
84.6
95.6
94.6
82.9
94.8
82.1
89.1

82.5
96.3
93.4
78.6
94.2
80.5
87.5

72.1
95.3
92.6
81.5
92.7
79.3
85.5

We report comparative results in Table 5, where all DG
methods work better than the base model, emphasizing the
need for domain generalization. Our DG method has higher
average performance. Also, note that in order to compare
with the state-of-the-art DG methods, we only used 2 fully
connected layers for our network and precomputed features
as input. However, when using convolutional layers on row
images, we expect our DG model to provide better per-
formance. Figure 4(c) shows the improvement of our DG
model over the base model using DeCaF-fc6 features.

5.4. MNIST Dataset

We followed the setting in [24], and randomly selected a
set M of 100 images per category from the MNIST dataset
(1000 in total). We then rotated each image in M ﬁve
times with 15 degrees intervals, creating ﬁve new domains
M15◦ , M30◦ , M45◦ , M60◦ , and M75◦ . We conducted a
leave-one-domain-out evaluation (6 cross-domain
cases in total). We used the same network of Section 5.1.2,
and we repeated the experiments 10 times. To create pos-
itive and negative pairs for training our network, for each
sample of a source domain we randomly selected 2 samples
from each remaining source domain. We report compar-
ative average accuracies for CCSA and others in Table 6,
showing again a performance improvement.

6. Conclusions

We have introduced a deep model

in combination
with the classiﬁcation and contrastive semantic alignment
(CCSA) loss to address SDA. We have shown that the
CCSA loss can be augmented to address the DG problem
without the need to change the basic model architecture.
However, the approach is general in the sense that the ar-
chitecture sub-components can be changed. We found that
addressing the semantic distribution alignments with point-
wise surrogates of distribution distances and similarities for
SDA and DG works very effectively, even when labeled tar-
get samples are very few. In addition, we found the SDA
accuracy to converge very quickly as more labeled target
samples per category are available.

Acknowledgments

This material is based upon work supported by the Center
for Identiﬁcation Technology Research and the National Science
Foundation under Grant No. 1066197.

References

[1] Y. Aytar and A. Zisserman. Tabula rasa: Model transfer for
object category detection. In Computer Vision (ICCV), 2011
IEEE International Conference on, pages 2252–2259. IEEE,
2011.

[2] M. Baktashmotlagh, M. T. Harandi, B. C. Lovell, and
M. Salzmann. Unsupervised domain adaptation by domain
invariant projection. In IEEE ICCV, pages 769–776, 2013.

[3] C. J. Becker, C. M. Christoudias, and P. Fua. Non-linear
In Advances in Neural

domain adaptation with boosting.
Information Processing Systems, pages 485–493, 2013.
[4] A. Bergamo and L. Torresani. Exploiting weakly-labeled
web images to improve object classiﬁcation: a domain adap-
tation approach. In Advances in Neural Information Process-
ing Systems, pages 181–189, 2010.

[5] G. Blanchard, G. Lee, and C. Scott. Generalizing from sev-
eral related classiﬁcation tasks to a new unlabeled sample. In
Advances in neural information processing systems, pages
2178–2186, 2011.

[6] J. Blitzer, R. McDonald, and F. Pereira. Domain adapta-
tion with structural correspondence learning. In Proceedings
of the 2006 conference on empirical methods in natural lan-
guage processing, pages 120–128. Association for Compu-
tational Linguistics, 2006.

[7] L. Chen, W. Li, and D. Xu. Recognizing RGB images by
In CVPR, pages 1418–1425,

learning from RGB-D data.
June 2014.

[8] M. Chen, Z. Xu, K. Weinberger, and F. Sha. Marginalized de-
noising autoencoders for domain adaptation. arXiv preprint
arXiv:1206.4683, 2012.

[9] Q. Chen, J. Huang, R. Feris, L. M. Brown, J. Dong, and
S. Yan. Deep domain adaptation for describing people based
In Proceedings of the
on ﬁne-grained clothing attributes.
IEEE conference on computer vision and pattern recogni-
tion, pages 5315–5324, 2015.

[10] M. J. Choi, J. J. Lim, A. Torralba, and A. S. Willsky. Exploit-
ing hierarchical context on a large database of object cate-
gories. In Computer vision and pattern recognition (CVPR),
2010 IEEE conference on, pages 129–136. IEEE, 2010.
[11] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similar-
ity metric discriminatively, with application to face veriﬁ-
cation. In Computer Vision and Pattern Recognition, 2005.
CVPR 2005. IEEE Computer Society Conference on, vol-
ume 1, pages 539–546. IEEE, 2005.

[12] H. Daume III and D. Marcu. Domain adaptation for statis-
tical classiﬁers. Journal of Artiﬁcial Intelligence Research,
26:101–126, 2006.

[13] Z. Ding and Y. Fu. Robust transfer metric learning for im-
age classiﬁcation. IEEE Transactions on Image Processing,
26(2):660–670, 2017.

[14] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,
DeCAF: a deep convolu-
In

E. Tzeng, and T. Darrell.
tional activation feature for generic visual recognition.
arXiv:1310.1531, 2013.

[15] L. Duan, I. W. Tsang, D. Xu, and S. J. Maybank. Domain
transfer svm for video concept detection. In Computer Vi-

sion and Pattern Recognition, 2009. CVPR 2009. IEEE Con-
ference on, pages 1375–1381. IEEE, 2009.

[16] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and
A. Zisserman. The pascal visual object classes (voc) chal-
lenge. International journal of computer vision, 88(2):303–
338, 2010.

[17] C. Fang, Y. Xu, and D. N. Rockmore. Unbiased metric learn-
ing: On the utilization of multiple datasets and web images
for softening bias. In International Conference on Computer
Vision, 2013.

[18] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative
visual models from few training examples: An incremental
bayesian approach tested on 101 object categories. Computer
vision and Image understanding, 106(1):59–70, 2007.
[19] B. Fernando, A. Habrard, M. Sebban, and T. Tuytelaars. Un-
supervised visual domain adaptation using subspace align-
ment. In IEEE ICCV, pages 2960–2967, 2013.

[20] B. Fernando, T. Tommasi, and T. Tuytelaarsc. Joint cross-
domain classiﬁcation and subspace learning for unsuper-
vised adaptation. Pattern Recogition Letters, 2015.

[21] Y. Ganin and V. Lempitsky. Unsupervised domain adaptation
by backpropagation. arXiv preprint arXiv:1409.7495, 2014.
[22] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,
F. Laviolette, M. Marchand, and V. Lempitsky. Domain-
adversarial training of neural networks. Journal of Machine
Learning Research, 17(59):1–35, 2016.

[23] M. Ghifary, D. Balduzzi, W. B. Kleijn, and M. Zhang. Scatter
component analysis: A uniﬁed framework for domain adap-
tation and domain generalization. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 2017.

[24] M. Ghifary, W. Bastiaan Kleijn, M. Zhang, and D. Balduzzi.
Domain generalization for object recognition with multi-task
autoencoders. In Proceedings of the IEEE International Con-
ference on Computer Vision, pages 2551–2559, 2015.
[25] M. Ghifary, W. B. Kleijn, M. Zhang, D. Balduzzi, and
W. Li. Deep reconstruction-classiﬁcation networks for un-
supervised domain adaptation. In European Conference on
Computer Vision, pages 597–613. Springer, 2016.

[26] B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic ﬂow ker-
nel for unsupervised domain adaptation. In Computer Vision
and Pattern Recognition (CVPR), 2012 IEEE Conference on,
pages 2066–2073. IEEE, 2012.

[27] R. Gopalan, R. Li, and R. Chellappa. Domain adaptation
for object recognition: An unsupervised approach. In IEEE
ICCV, pages 999–1006, 2011.

[28] A. Gretton, K. M. Borgwardt, M. Rasch, B. Sch¨olkopf, and
A. J. Smola. A kernel method for the two-sample-problem.
In NIPS, 2006.

[29] Y. Guo and M. Xiao. Cross language text classiﬁcation via
subspace co-regularized multi-view learning. In Proceedings
of the 29th International Conference on Machine Learning,
ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1,
2012, 2012.

[30] R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduc-
tion by learning an invariant mapping. In Computer vision
and pattern recognition, 2006 IEEE computer society confer-
ence on, volume 2, pages 1735–1742. IEEE, 2006.

[31] J. J. Hull. A database for handwritten text recognition re-
search. IEEE Transactions on pattern analysis and machine
intelligence, 16(5):550–554, 1994.

[32] A. Khosla, T. Zhou, T. Malisiewicz, A. A. Efros, and A. Tor-
In European
ralba. Undoing the damage of dataset bias.
Conference on Computer Vision, pages 158–171. Springer,
2012.

[33] P. Koniusz, Y. Tas, and F. Porikli. Domain adaptation by mix-
ture of alignments of second- or higher-order scatter tensors.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.

[34] B. Kulis, K. Saenko, and T. Darrell. What you saw is not
what you get: Domain adaptation using asymmetric ker-
In Computer Vision and Pattern Recogni-
nel transforms.
tion (CVPR), 2011 IEEE Conference on, pages 1785–1792.
IEEE, 2011.

[35] B. Kumar, G. Carneiro, I. Reid, et al. Learning local im-
age descriptors with deep siamese and triplet convolutional
networks by minimising global loss functions. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 5385–5394, 2016.

[36] M. Lapin, M. Hein, and B. Schiele. Learning using privi-
leged information: SVM+ and weighted SVM. Neural Net-
works, 53:95–108, 2014.

[37] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceedings
of the IEEE, 86(11):2278–2324, 1998.

[38] M.-Y. Liu and O. Tuzel. Coupled generative adversarial net-
works. In Advances in Neural Information Processing Sys-
tems, pages 469–477, 2016.

[39] M. Long, Y. Cao, J. Wang, and M. I. Jordan. Learning trans-
In ICML,

ferable features with deep adaptation networks.
pages 97–105, 2015.

[40] M. Long, G. Ding, J. Wang, J. Sun, Y. Guo, and P. S. Yu.
Transfer sparse coding for robust image representation. In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 407–414, 2013.

[41] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne.
Journal of Machine Learning Research, 9(Nov):2579–2605,
2008.

[42] S. Motiian and G. Doretto. Information bottleneck domain
adaptation with privileged information for visual recogni-
In European Conference on Computer Vision, pages
tion.
630–647. Springer, 2016.

[43] S. Motiian, M. Piccirilli, D. A. Adjeroh, and G. Doretto. In-
formation bottleneck learning using privileged information
In Proceedings of the IEEE Con-
for visual recognition.
ference on Computer Vision and Pattern Recognition, pages
1496–1505, 2016.

[44] K. Muandet, D. Balduzzi, and B. Sch¨olkopf. Domain gen-
eralization via invariant feature representation. In ICML (1),
pages 10–18, 2013.

[45] L. Niu, W. Li, and D. Xu. Multi-view domain generalization
for visual recognition. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision, pages 4193–4201,
2015.

[46] L. Niu, W. Li, D. Xu, and J. Cai. An exemplar-based multi-
view domain generalization framework for visual recogni-
tion. IEEE Transactions on Neural Networks and Learning
Systems, 2017.

[47] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain
IEEE TNN,

adaptation via transfer component analysis.
22(2):199–210, 2011.

[48] J. Ponce, T. L. Berg, M. Everingham, D. A. Forsyth,
M. Hebert, S. Lazebnik, M. Marszalek, C. Schmid, B. C.
Russell, A. Torralba, et al. Dataset issues in object recog-
In Toward category-level object recognition, pages
nition.
29–48. Springer, 2006.

[49] S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y. Bengio.
Contractive auto-encoders: Explicit invariance during fea-
In Proceedings of the 28th international
ture extraction.
conference on machine learning (ICML-11), pages 833–840,
2011.

[50] A. Rozantsev, M. Salzmann, and P. Fua. Beyond shar-
arXiv preprint

ing weights for deep domain adaptation.
arXiv:1603.06432, 2016.

[51] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge. IJCV, 2015.

[52] B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Free-
man. Labelme: a database and web-based tool for image
annotation. International journal of computer vision, 77(1-
3):157–173, 2008.

[53] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting vi-
sual category models to new domains. In ECCV, pages 213–
226, 2010.

[54] H. Shimodaira. Improving predictive inference under covari-
ate shift by weighting the log-likelihood function. Journal of
Statistical Planning and Inference, 90(2):227–244, 2000.

[55] K. Simonyan and A. Zisserman.

Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014.

[56] B. Sun and K. Saenko. Deep coral: Correlation alignment for
deep domain adaptation. In Computer Vision–ECCV 2016
Workshops, pages 443–450. Springer, 2016.

[57] T. Tommasi, M. Lanzi, P. Russo, and B. Caputo. Learning
the roots of visual domain shift. In Computer Vision–ECCV
2016 Workshops, pages 475–482. Springer, 2016.

[58] T. Tommasi, N. Patricia, B. Caputo, and T. Tuytelaars. A
deeper look at dataset bias. In German Conference on Pattern
Recognition, pages 504–516. Springer, 2015.

[59] A. Torralba and A. A. Efros. Unbiased look at dataset bias.
In Computer Vision and Pattern Recognition (CVPR), 2011
IEEE Conference on, pages 1521–1528, 2011.

[60] E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko. Simultane-
ous deep transfer across domains and tasks. In ICCV, 2015.
[61] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adversarial
discriminative domain adaptation. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), July
2017.

[62] E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell.
Deep domain confusion: Maximizing for domain invariance.
arXiv preprint arXiv:1412.3474, 2014.

[63] R. R. Varior, B. Shuai, J. Lu, D. Xu, and G. Wang. A
siamese long short-term memory architecture for human re-
identiﬁcation. In European Conference on Computer Vision,
pages 135–153. Springer, 2016.

[64] H. Wang, W. Wang, C. Zhang, and F. Xu. Cross-domain
metric learning based on information theory. In AAAI, pages
2099–2105, 2014.

[65] Z. Xu, W. Li, L. Niu, and D. Xu. Exploiting low-rank
structure from latent domains for domain generalization. In
ECCV, pages 628–643, 2014.

[66] J. Yang, R. Yan, and A. G. Hauptmann. Adapting svm classi-
ﬁers to data with shifted distributions. In Data Mining Work-
shops, 2007. ICDM Workshops 2007. Seventh IEEE Interna-
tional Conference on, pages 69–76. IEEE, 2007.

[67] T. Yao, Y. Pan, C.-W. Ngo, H. Li, and T. Mei. Semi-
supervised domain adaptation with subspace learning for vi-
sual recognition. In The IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR), June 2015.

