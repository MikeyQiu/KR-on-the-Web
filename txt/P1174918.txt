Learning Discriminative Features with Multiple Granularities
for Person Re-Identification
Guanshuo Wang1∗, Yufeng Yuan2∗, Xiong Chen2, Jiwei Li2, Xi Zhou1,2
1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University
2CloudWalk Technology
guanshuo.wang@sjtu.edu.cn,{yuanyufeng,chenxiong,lijiwei,zhouxi}@cloudwalk.cn

8
1
0
2
 
g
u
A
 
4
1
 
 
]

V
C
.
s
c
[
 
 
3
v
8
3
4
1
0
.
4
0
8
1
:
v
i
X
r
a

ABSTRACT
The combination of global and partial features has been an es-
sential solution to improve discriminative performances in per-
son re-identification (Re-ID) tasks. Previous part-based methods
mainly focus on locating regions with specific pre-defined seman-
tics to learn local representations, which increases learning diffi-
culty but not efficient or robust to scenarios with large variances.
In this paper, we propose an end-to-end feature learning strategy
integrating discriminative information with various granularities.
We carefully design the Multiple Granularity Network (MGN), a
multi-branch deep network architecture consisting of one branch
for global feature representations and two branches for local fea-
ture representations. Instead of learning on semantic regions, we
uniformly partition the images into several stripes, and vary the
number of parts in different local branches to obtain local fea-
ture representations with multiple granularities. Comprehensive
experiments implemented on the mainstream evaluation datasets
including Market-1501, DukeMTMC-reid and CUHK03 indicate
that our method robustly achieves state-of-the-art performances
and outperforms any existing approaches by a large margin. For
example, on Market-1501 dataset in single query mode, we obtain
a top result of Rank-1/mAP=96.6%/94.2% with this method after
re-ranking.

CCS CONCEPTS
• Computing methodologies → Object identification; Learn-
ing to rank; Supervised learning by classification; Image rep-
resentations; Neural networks;

KEYWORDS
Person re-identification, Feature learning, Multi-branch deep net-
work

ACM Reference Format:
Guanshuo Wang, Yufeng Yuan, Xiong Chen, Jiwei Li, and Xi Zhou. 2018.
Learning Discriminative Features with Multiple Granularities for Person
Re-Identification. In 2018 ACM Multimedia Conference (MM ’18), October

*Equal contribution.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
MM ’18, October 22–26, 2018, Seoul, Republic of Korea
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5665-7/18/10. . . $15.00
https://doi.org/10.1145/3240508.3240552

Figure 1: Body part partitions from coarse to fine granular-
ities. We regard original pedestrian images with the whole
body as the coarsest level of granularity in the left column.
The middle and right column are respectively pedestrian
partitions divided into 2 and 3 stripes from the original im-
ages. The more stripes images are divided into, the finer the
granularity of partitions is.

22–26, 2018, Seoul, Republic of Korea. ACM, New York, NY, USA, 9 pages.
https://doi.org/10.1145/3240508.3240552

1 INTRODUCTION
Person re-identification (Re-ID) is a challenging task to retrieve
a given person among all the gallery pedestrian images captured
across different security cameras. Due to the scene complexity
of images from surveillance videos, the main challenges for per-
son Re-ID come from large variations on persons such as pose,
occlusion, clothes, background clutter, detection failure, etc. The
prosperity of deep convolutional network has introduced more pow-
erful representations with better discrimination and robustness for
pedestrian images, which pushed the performance of Re-ID to a
new level. Some recent deep Re-ID methods [4, 5, 22, 27, 29, 30, 36]
have achieved breakthrough with high-level identification rates
and mean average precision.

The intuitive approach of pedestrian representations is to extract
discriminative features from the whole body on images. The aim
of global feature learning is to capture the most salient clues of ap-
pearance to represent identities of different pedestrians. However,
high complexities for images captured in surveillance scenes usu-
ally restrict the accuracy for feature learning in large scale Re-ID
scenarios. Due to the limited scale and weak diversity of person
Re-ID training datasets, some non-salient or infrequent detailed

information can be easily ignored and make no contribution for bet-
ter discrimination during global feature learning procedure, which
makes global features hard to adapt similar inter-class common
properties or large intra-class differences.

To relieve this dilemma, locating significant body parts from im-
ages to represent local information of identities has been confirmed
to be an effective approach for better Re-ID accuracy in many previ-
ous works. Each located body part region only contains a small per-
centage of local information from the whole body, and at the same
time distraction by other related or unrelated information outside
the regions is actually filtered by locating operations, with which
local features can be learned to concentrate more on identities and
used as an important complement for global features. Part-based
methods for person Re-ID can be divided into three main pathways
according to their part locating methods: 1) Locating part regions
with strong structural information such as empirical knowledge
about human bodies [8, 21, 36, 43] or strong learning-based pose
information [33, 44]; 2) Locating part regions by region proposal
methods [19, 41]; 3) Enhancing features by middle-level attention
on salient partitions [22, 24, 25, 45]. However, obvious limitations
impede the effectiveness of these methods. First, pose or occlusion
variations can affect the reliability of local representation. Second,
these methods almost only focus on specific parts with fixed se-
mantics, but cannot cover all the discriminative information. Last
but not least, most of these methods are not end-to-end learning
process, which increases the complexity and difficulty of feature
learning.

In this paper, we propose a feature learning strategy combining
global and local information in different granularities. As shown in
Figure 1, various numbers of partition stripes introduce a diversity
of content granularity. We define the original image containing
only one whole partition with global information as the coarsest
case, and as the number of partitions increase, features of local
parts can concentrate more on finer discriminative information in
each part stripe, filtering information on the other stripes. Since
deep learning mechanism can capture approximate response pref-
erences on the main body from the whole image, it is also possible
to capture more fine-grained saliency for local features extracted
from smaller part regions. Notice that these part regions are not
necessary to be located partitions with specific semantics, but only
a piece of equally-split stripe on the original images. From the ob-
servation, we find that the granularity of discriminative responses
indeed becomes finer as the number of horizontal stripes increases.
Based on this motivation, we design the Multiple Granularity Net-
work (MGN), a multi-branch network architecture divided into
one global and two local branches with delicated parameters from
the 4th residual stage of the ResNet-50 [13] backbone. In each lo-
cal branch of MGN, we divide globally-pooled feature maps into
different numbers of stripes as part regions to learn local feature
representations independently, referring the methods in [36].

Comparing to the previous part-based methods, our method
only utilize equally-divided parts for local representation, but can
achieve outstanding performance exceeding all previous methods.
Besides, our method is completely a end-to-end learning process,
which is easy for learning and implementation. Extensive exper-
iment results show that our method can achieve state-of-the-art
performances on several mainstream Re-ID datasets, even with

Figure 2: Feature response maps in different granularities
extracted from the last output of different models. The re-
sponse intensity is calculated by the L2-norm of feature vec-
tors from all the spatial locations. Middle Column: a pedes-
trian image. Left Column: global response map by IDE em-
bedding. Right Column: three local response maps corre-
sponding to three split stripes of the origin image, extracted
by part-based model. Best viewed in color.

settings without any additional external data or re-ranking [50]
operation.

2 RELATED WORKS
With the prosperity of deep learning, feature learning by deep
networks has become a common practice in person Re-ID tasks.
[20, 42] first introduce deep siamese network architecture into Re-
ID and combine the body part feature learning, achieving higher
performances comparing to the contemporary hand-crafted meth-
ods. [47] proposes ID-discriminative Embedding (IDE) with simple
ResNet-50 backbones as a baseline of the performance level for
modern deep Re-ID systems. A number of methods are proposed
to improve the performance for deep person Re-ID. In [1, 37], mid-
level features of image pairs are computed to depict interrelation of
local parts with carefully designed mechanism. [39] introduces Do-
main Guided Dropout to enhance the generalization ability across
different domains of pedestrian scenarios. [50] brings re-ranking
strategy into Re-ID tasks to modify the ranking results for accuracy
improvement.

Recently some deep Re-ID methods pushed the performances
to a new level comparing to the former systems. [43] introduces
a part-based alignment matching in training phase with shortest
path programming and mutual learning to improve metric learning
performance. [3, 36] both equally slice the feature maps of input
images into several stripes in vertical orientation. [3] merges slices
of local features with LSTM network and combine with global
features learned from classification metric learning. Instead [36]
directly concatenates the features from local parts as the final repre-
sentation, and applies refined part pooling to modify the mapping
validation of part features. However, according to the report in [43],
these systems just achieve similar performances as human, which
we still need a highway to surpass.

Figure 3: Multiple Granularity Network architecture. The ResNet-50 backbone is split into three branches after res_conv4_1
residual block: Global Branch, Part-2 Branch and Part-3 Branch. During testing, all the reduced features are concatenated
together as the final feature representation of a pedestrian image. Notice that the 1 × 1 convolutions for dimension reduction
and fully connected layers for identity prediction in each branch DO NOT share weights with each other. Each path from the
feature to the specific loss function represents an independent supervisory signal. Best viewed in color.

Among all the strategies for performance improvement, we ar-
gue that combining the local representations from parts of images
is the most effective. As mentioned in Section 1, we summarize
three main pathways for part-based learning: determining regions
according to structural information about human body, locating
body parts by region proposal methods and enhancing features
by spatial attention. In [8, 21, 36], images are all split into several
stripes in horizontal orientation according to intrinsic human body
structure knowledge, on which local feature representations are
learned. [33, 44] utilize structural information of body landmarks
predicted by pose estimation methods to crop more accurate region
areas with semantics. To locate semantic partitions without strongly
learning-based predictors, region proposal methods such as [11, 18]
are employed in some part-based methods [19, 22, 25, 41, 45]. Atten-
tion information can be a powerful complement for discrimination,
which are enhanced in [22, 24, 25]. In our proposed method, we
only use simple horizontal stripes as part regions for local feature
learning but achieve outstanding performances.

Loss functions are used as supervisory signals in feature learning.
In the training phase for deep Re-ID systems, the most common
loss functions are classification losses and metric losses. Softmax
loss is almost the only choice of classification loss function for
its strong robustness to various kinds of multi-class classification
tasks, which can be used individually [1, 19, 22, 25, 36, 39, 41, 47]
or combined with other losses [3, 8, 20, 43] in embedding learning
procedures for Re-ID. For metric losses used in embedding learning

for Re-ID, there are more variants with different ranking metrics.
Contrastive loss [12] is commonly used in siamese-liked networks
[37], which focuses on maximizing the distances between inter-class
pairs and minimizing that between intra-class pairs. Triplet loss
[15, 28] enforces a margin between the intra and inter distances to
the same anchor sample with in a triplet. Based on triplet loss, many
variants [6, 8, 14, 32] are proposed to solve learning or performance
issues in metric learning. We employ a setting of joint learning
with both softmax and triplet losses in our proposed method.

3 MULTIPLE GRANULARITY NETWORK
Figure 2 shows feature response maps of a certain image extracted
from the IDE baseline model [47] and a part-based model based on
IDE. We can observe that even if no explicit attention mechanisms
are imposed to enhance the preferences to some salient compo-
nents, the deep network can still learn the preliminary distinction
of response preferences on different body parts according to their
inherent semantic meanings. However, to eliminate the distraction
of unrelated patterns in pedestrian images with high complexity,
higher responses are just concentrated on the main body of pedes-
trians instead of any concrete body parts with semantic patterns.
As we narrow the area of represented regions and train as a classi-
fication task to learn local features, we can observe that responses
on the local feature maps starts to cluster on some salient semantic
patterns, which also varies with the sizes of represented regions.

Branch

Part No. Map Size

Dims

Feature

Global
Part-2
Part-3

1
2
3

12 × 4
24 × 8
24 × 8

256
256*2+256
256*3+256

f G
д
|1
i =0 }, f P 2
i =0 }, f P 3
|2

д

д

{f P 2
pi
{f P 3
pi

Table 1: Comparison of the settings for three branches in
MGN. Notice that the size of input images is set to 384 × 128.
"Branch" refers to the name of branches. "Part No." refers to
the number of partitions on feature maps. "Map Size" refers
to the size of output feature maps from each branch. "Dims"
refers to the dimensionality and number of features for the
output representations. "Feature" means the symbols for the
output feature representations.

This observation reflects a relationship between the volume of im-
age contents, i.e. the granularity of regions, and capability of deep
networks to focus on specific patterns for representations. We be-
lieve this phenomenon comes from the limitation of information in
restricted regions. In general, comparing from a global image, it is
intuitively hard to discriminate the identities for pedestrians from
a local part region. Supervising signals of the classification task
enforce the features to be correctly classified as the target identity,
which also push the learning procedure trying to explore useful
fine-grained details among limited information.

Actually, local feature learning in previous part-based methods
only introduces a basic granularity diversity of partitions to the
total feature learning procedure with or without empirical prior
knowledge. Assume that there are appropriate levels of granular-
ities, details with most discriminative information might be al-
most concentrated by deep networks. Motivated by the observation
and analysis above, we propose the Multiple Granularity Network
(MGN) architecture to combine global and multi-granularity local
feature learning for more powerful pedestrian representation.

3.1 Network Architecture
The architecture of Multiple Granularity Network is shown in Fig-
ure 3. The backbone of our network is ResNet-50 which helps to
achieve competitive performances in some Re-ID systems [3, 36, 43].
The most obvious modification different from the original version
is that we divide the subsequent part after res_conv4_1 block into
three independent branches, sharing the similar architecture with
the original ResNet-50.

Table 1 lists the settings of these branches. In the upper branch,
we employ down-sampling with a stride-2 convolution layer in
res_conv5_1 block, following a global max-pooling (GMP) [2] op-
eration on the corresponding output feature map and a 1 × 1 con-
volution layer with batch normalization [17] and ReLU to reduce
2048-dim features zG
. This branch learns the global
д
feature representations without any partition information, so we
name this branch as the Global Branch.

to 256-dim fG
д

The middle and lower branches both share the similar network
architecture with Global Branch. The difference is that we employ
no down-sampling operations in res_conv5_1 block to preserve
proper areas of reception fields for local features, and output feature
maps in each branch are uniformly split into several stripes in
horizontal orientation, on which we independently perform the

same following operations as Global Branch to learn local feature
representations. We call these branches Part-N Branch, where N
refers to the number of partitions on the unreduced feature maps,
e.g. the middle and lower branches in Figure 3 can be named as
Part-2 and Part-3 Branch.

During testing phases, to obtain the most powerful discrimina-
tion, all the features reduced to 256-dim are concatenated as the
final feature, combining both the global and local information to
perfect the comprehensiveness for learned features.

3.2 Loss Functions
To unleash the discrimination ability of the learned representations
of this network architecture, we employ softmax loss for classfi-
cation, and triplet loss for metric learning as the loss functions in
training phases, which are both widely used in various deep Re-ID
methods.

For basic discrimination learning, we regard the identification
task as a multi-class classfication problem. For i-th learned features
fi , softmax loss is formulated as:

fi

(1)

i=1

log

= −

N
(cid:213)

Lsof tmax

eWT
yi
k=1 eWT
(cid:205)C
k fi
where Wk
corresponds to a weight vector for class k, with the size
of mini-batch in training process N and the number of classes in the
training dataset C. Different from the traditional softmax loss, the
form we employ here abandons bias terms in linear multi-class clas-
sifiers according to [38], which contributes to better discrimination
performances. Among all the learned embeddings, we employ the
softmax loss to the global features before 1×1 convolution reduction
д } and part features after reduction {f P 2
i=1, f P 3
|2
|3
i=1}.
{zG
pi
pi
д , f P 3
д } are trained
with triplet loss to enhance ranking performances. We use the batch-
hard triplet loss [14], an improved version based on the original
semi-hard triplet loss. This loss function is formulated as follows:

д , zP 2
All the global features after reduction {fG

д , zP 3

д , f P 2

Lt r ipl et

= −

P
(cid:213)

K
(cid:213)

i=1

a=1

[α + max
p=1...K

∥f

(i)
a − f

(i)
p ∥2

∥f

(i)
a − f

(j)
n ∥2]+

(2)

− min
n=1...K
j=1...P
j(cid:44)i

(i)
a , f

(i)
(i)
p , f
n

where f
are the features extracted from anchor, positive
and negative samples receptively, and α is the margin hyperparam-
eter to control the differences of intra and inter distances. Here
positive and negative samples refer to the pedestrians with same
or different identity with the anchor. The candidate triplets are
built by the furthest positive and closest negative sampled pairs,
i.e. the hardest positive and negative pairs in a mini-batch with P
selected identities and K images from each identity. This improved
version of triplet loss enhances the robustness in metric learning,
and further improve the performances at the same time.

In MGN achitecture, to avoid loss weight tuning troubles and
difficulties in convergence, we novelly propose classfication-before-
metric achitecture, which applies the softmax losses to reduced
256-dim local features in Part-2 and Part-3 Branches, and all the
non-reduced global-pooled 2048-dim global features, but applies

triplet losses to all the reduced features, different from existing
methods using triplet losses. This setting is inspired from coarse-to-
fine mechanism, regarding non-reduced features as coarse informa-
tion to learn classification and reduced features as fine information
with learned metric. The proposed setting achieves robust conver-
gence comparing to that of imposing joint effects at the same level
of reduced features. Besides, we employ no triplet loss on local
features. Due to misalignment or other issues, the contents of local
regions might vary dramatically, which makes the triplet loss tend
to corrupt the model during training.

3.3 Discussions
In our proposed Multiple Granularity Network architecture, there
are some issues worth our separate discussion. In this paragraph,
we specifically discuss the issues as follows:

Multi-branch architecture According to our initial motivation
for MGN architecture, it seems to be reasonable that the global and
local representations are both learned in one single branch. We can
directly split the same final feature maps extracted by res_conv5_3
in different numbers of stripes, and apply corresponding supervi-
sory signals as our proposed methods. However, we find this setting
is not efficient for further performance improving. Borrowing the
ideas in [34], the reason might be that the branches sharing the
similar network architecture(mainly the fourth residual stage of
ResNet-50) just response to different levels of detailed informa-
tion on images. Learning features in multiple granularities with
one mixed single branch might dilute the importance of detailed
information. Besides, we try to split the backbone network after
shallower or deeper layers, which also achieve no better perfor-
mances.

Diversity of granularity Three branches in our network ar-
chitecture actually learn representing information with different
perferences. Global Branch with larger reception field and global
max-pooling captures integral but coarse features from the pedes-
trian images, and features learned by Part-2 and Part-3 Branches
without strided convolution and split parts of stripes tend to be
local but fine. The branch with more partitions will learn finer
representation for pedestrian images. Branches learning different
preferences can cooperatively supplement low-level discriminating
information to the common backbone parts, which is the reason
for performance boosting in any single branch.

4 EXPERIMENT
4.1 Implementation
To capture more detailed information from pedestrian images, we
refer to [36] and resize input images to 384×128. We use the weights
of ResNet-50 pretrained on ImageNet [9] to initialize the backbone
and branches of MGN. Notice that different branches in the net-
work are all initialized with the same pretrained weights of the
corresponding layers after the res_conv4_1 block. During training
phases, we only deploy random horizontal flipping to images in
the training dataset for data augmentation. Each mini-batch is sam-
pled with randomly selected P identities and randomly sampled
K images for each identity from the training set to cooperate the
requirement of triplet loss. Here we recommend to set P = 16 and
K = 4 to train our proposed model. For the margin parameter for

triplet loss, we set to 1.2 in all our experiments. We choose SGD as
the optimizer with momentum 0.9. The weight decay factor for L2
regularization is set to 0.0005. As for the learning rate strategy, we
set the initial learning rate to 0.01, and decay the learning rate to
1e-3 and 1e-4 after training for 40 and 60 epochs. The total training
process lasts for 80 epochs. During evaluation, we both extract
the features corresponding to original images and the horizontally
flipped versions, then use the average of these as the final features.
Our model is implemented on PyTorch framework. To conduct a
complete training procedure on Market-1501 dataset, it takes about
2 hours with data-parallel acceleration by two NVIDIA TITAN Xp
GPUs. All our experiments on different datasets follow the settings
above.

4.2 Datasets and Protocols
The experiments to evaluate our proposed method are conducted on
three mainstream Re-ID datasets: Market-1501 [46], DukeMTMC-
reID [49] and CUHK03 [20]. It is necessary to introduce these
datasets and their evaluation protocols before we show our results.
Market-1501 This dataset includes images of 1,501 persons cap-
tured from 6 different cameras. The pedestrians are cropped with
bounding-boxes predicted by DPM detector [10]. The whole dataset
is divided into training set with 12,936 images of 751 persons and
testing set with 3,368 query images and 19,732 gallery images of
750 persons. There are single-query and multiple-query modes in
evaluation, the difference of which is the number of images from the
same identity. In multiple-query mode, all features extracted from
the images of a person captured by the same camera are merged by
avg- or max-pooling, which contains more complete information
than single query mode with only 1 query image.

DukeMTMC-reID This dataset is a subset of the DukeMTMC
[26] used for person re-identification by images. It consists of 36,411
images of 1,812 persons from 8 high-resolution cameras. 16,522
images of 702 persons are randomly selected from the dataset as
the training set, and the remaining 702 persons are divided into the
testing set where contains 2,228 query images and 17,661 gallery
images. It might be the most challenging datasets for person Re-
ID at present, with common situations in high similarity across
persons and large variations within the same identity.

CUHK03 This dataset consists of 14,097 images of 1,467 per-
sons from 6 cameras. Two types of annotations are provided in this
dataset: manually labeled pedestrian bounding boxes and DPM-
detected bounding boxes. Originally the whole dataset is divided
into 20 random splits for cross-validation, which is designed for
hand-crafted methods and very time-consuming to conduct experi-
ments for deep-learning-based methods.

Protocols In our experiments, to evaluate the performances of
Re-ID methods, we report the cumulative matching characteristics
(CMC) at rank-1, rank-5 and rank-10, and mean average preci-
sion (mAP) on all the candidate datasets. On Market-1501 dataset,
we conduct experiments both in single-query and multiple-query
mode. On CUHK03 dataset, to simplify the evaluation procedure
and meanwhile enhance the accuracy of the performance reflected
by the results, we adopt the protocol used in [50].

Methods

Single Query Multiple Query
Rank-1 mAP
Rank-1 mAP

TriNet[14]
JLML[21]
AACN[40]
AOS[16]
DPFL[7]
MLFN[4]
KPM+RSA+HG[30]
PSE+ECN[27]
HA-CNN[22]
DuATM[31]
GSRW[29]
DNN_CRF[5]
PCB+RPP[36]
MGN(Ours)

TriNet(RK)[14]
AOS(RK)[16]
AACN(RK)[40]
PSE+ECN(RK)[27]
MGN(Ours, RK)

84.9
85.1
85.9
86.5
88.6
90.0
90.1
90.4
91.2
91.4
92.7
93.5
93.8
95.7

86.7
88.7
88.7
90.3
96.6

69.1
65.5
66.9
70.4
72.6
74.3
75.3
80.5
75.7
76.6
82.5
81.6
81.6
86.9

81.1
83.3
83.0
84.0
94.2

90.5
89.7
89.8
91.3
92.2
92.3
-
-
93.8
-
-
-
-
96.9

91.8
92.5
92.2
-
97.1

76.4
74.5
75.1
78.3
80.4
82.4
-
-
82.8
-
-
-
-
90.7

87.2
88.6
87.3
-
95.9

Table 2: Comparison of results on Market-1501 with Single
Query setting (SQ) and Multiple Query setting (MQ). "RK"
refers to implementing re-ranking operation.

Methods

Rank-1 mAP

SVDNet[35]
AOS[16]
HA-CNN[22]
GSRW[29]
DuATM[31]
PCB+RPP[36]
PSE+ECN[27]
DNN_CRF[5]
GP-reid[2]

76.7
79.2
80.5
80.7
81.8
83.3
84.5
84.9
85.2

56.8
62.1
63.8
66.4
64.6
69.2
75.7
69.5
72.8

MGN(Ours)

88.7

78.4

Table 3: Comparison of results on DukeMTMC-reID.

Methods

Labeled

Detected

Rank-1 mAP Rank-1 mAP

BOW+XQDA[46]
LOMO+XQDA[23]

IDE[47]
PAN[48]
SVDNet[35]
HA-CNN[22]
MLFN[4]
PCB+RPP[36]

MGN(Ours)

7.9
14.8

22.2
36.9
40.9
44.4
54.7
-

7.3
13.6

21.0
35.0
37.8
41.0
49.2
-

6.4
12.8

21.3
36.3
41.5
41.7
52.8
63.7

6.4
11.5

19.7
34.0
37.3
38.6
47.8
57.5

68.0

67.4

66.8

66.0

Table 4: Comparison of results on CUHK03 with evaluation
protocols in [50].

4.3 Comparison with State-of-the-Art Methods
We compare our proposed method with current state-of-the-art
methods on all the candidate datasets to show our considerable
performance advantage over all the existing competitors. Results
in detail are given as follow:

Market-1501 The results on Market-1501 dataset is shown in Ta-
ble 2. For the special effects of re-ranking method for improvement
on mAP and rank-1 accuracy, we divide the results into two groups
according to whether re-ranking is implemented or not. In single
query mode, PCB+RPP [36] achieved the best published result with-
out re-ranking , but our MGN achieves Rank-1/mAP=95.7%/86.9%,
exceeding the former method by 1.9% in Rank-1 accuracy and 5.3%
in mAP. After implementing re-ranking, the result can be improved
to Rank-1/mAP=96.6%/94.2%, which surpasses all existing methods
by a large margin.

Figure 4 shows top-10 ranking results for some given query
pedestrian images. The first two results shows the great robustness:
regardless of the pose or gait of these captured pedestrian, MGN
features can robustly represent discriminative information of their
identities. The third query image is captured in a low-resolution
condition, losing an amount of important information. However,
from some detailed clues such as the strap of the bag and his black
suits, most of the ranking results are accurate and with high quality.
The last pedestrian shows his back carrying a black backpack, but
we can obtain his captured images in front view in rank-3, 6 and 9.
We attribute this surprising result to the effects of local features,
which establish relationships when some salient parts are lost.

DukeMTMC-reID According to Table 3, our MGN architec-
ture also performs excellently on the challenging DukeMTMC-reID
dataset. GP-reid [2] is a good practice of many useful strategies com-
bined in person Re-ID tasks and achieved the best published result.
MGN achieves state-of-the-art result of Rank-1/mAP=88.7%/78.4%,
outperforming GP-reid by +3.5% in Rank-1 and +5.6% in mAP. Stand-
ing on this level of performance on the most challenging datasets
currently, we believe there are still some issues to be conquered for
further perfect deep Re-ID systems.

CUHK03 As shown in Table 4, our MGN achieves Rank-1/mAP=
68.0%/67.4% on CUHK03 labeled setting and 66.8%/66.0% on CUHK03
detected setting, which outperform all the published results by a
large margin. Here we can observe an obvious gap between results
of labeled and detected conditions. We argue that it reflects an
important affect of detection failure on person Re-ID performance,
which emphasizes the importance of high-performance pedestrian
detectors.

4.4 Effectiveness of Components
To verify effectiveness of each component in MGN, we conduct
several ablation experiments with different component settings
on Market-1501 dataset in single query mode. Notice that other
unrelated settings in each comparative experiment are the same as
MGN implementation in Section 4.1, and we have carefully tuned
all the candidate models and report the best performance with
our settings. Table 5 shows the comparison results in different
settings related to components of MGN. We separately analyze
each component as follows:

Figure 4: Top-10 ranking list for some query images on Market-1501 datasets by MGN. The retrieved images are all from in
the gallery set, but not from the same camera shot. The images with green borders belong to the same identity as the given
query, and that with red borders do not.

Model

Rank-1

Rank-5

Rank-10 mAP

ResNet-50
ResNet-101
ResNet-50+TP

Global (Branch)
Part-2 (Single)
Part-2 (Branch)
Part-3 (Single)
Part-3 (Branch)
G+P2+P3 (Single)

MGN w/o Part-3
MGN w/ Part-4

MGN (Part2+4)
MGN (Part3+4)

MGN w/o TP
MGN

87.5
90.4
88.7

89.8
92.6
94.4
93.1
94.4
94.4

94.4
95.1

94.8
95.0

95.3
95.7

94.9
95.7
96.0

95.8
97.1
97.9
97.6
98.2
97.6

97.9
98.3

98.2
98.1

97.9
98.3

96.7
97.2
97.2

97.5
98.0
98.8
98.7
98.8
98.5

98.7
98.9

98.9
98.8

98.7
99.0

71.4
78.0
75.0

78.5
80.2
83.9
82.1
84.1
85.2

85.7
86.1

85.6
86.1

86.2
86.9

Table 5: Results with different settings on Market-1501
datasets. "TP" refers to triplet loss. "Branch" refers to a sub-
branch of MGN. "Single" refers to a single network with the
same setting as the branch with the corresponding name in
MGN. The model "ResNet-50+TP" can be regarded as "Global
(Single)". "G+P2+P3" refers to an ensemble setting by Global
(Single), Part-2 (Single) and Part-3 (Branch).

MGN vs ResNet-50 Comparing the results by the baseline
ResNet-50 model with our MGN model without triplet loss, we can
observe MGN makes a significant performance improvement from

Rank-1/mAP=87.5%/71.4% to 95.3%/86.2% (+7.8%/14.8%). We also im-
plement the same experiment with ResNet-101 model, which has a
similar scale of weights with MGN. The deeper ResNet-101 network
indeed brings a considerable performance boost (+2.9%/7.4%), but
there is still a large gap with our MGN model, which shows that ex-
tra weights from additional branches are not the main contributors
of the improvement, but the carefully-designed network architec-
ture. Results above prove that our proposed MGN has incredible
capability of feature representations for person Re-ID.

Multiple branch vs Multiple networks The multi-branch set-
ting in one single network is very similar to ensemble of multiple
independent networks, but we believe the cooperation of multiple
branches can achieve a better performance than ensemble learning.
We train three independent ResNet-50 networks separately, each of
which respectively replicates the corresponding configuration of
three branches, i.e. Global, Part-2 and Part-3 Branch in MGN . In our
experiments, we explore the effects of multi-branch architecture in
two aspects. On the one hand, from a global view, we compare the
performance of MGN with the ensemble of three single networks.
The ensemble strategy indeed achieves better performance than
any single participating network, but MGN still outperforms about
1% ∼ 2% on both Rank-1 and mAP. It shows that the cooperation of
branches learns more discriminative feature representations than
independent networks. On the other hand, from a local view, we
respectively compare the performances of features learned by sub-
branches of MGN with single networks in corresponding setting of
branch. As our expect, the features from sub-branches also perform
better than that from single networks. We argue that the mutual
effects between sub-branches complement the blind spots in their
individual learning procedure.

setting with Part-3/Part-4, the former setting causes greater perfor-
mance loss than the later. The reason is that the uniform division
by 2 and 4 introduces no overlap areas between stripes, but the
Part-3/Part-4 setting does. We argue this overlap can introduce the
correlation between different partitions, from which more discrimi-
native information can be learned.

Triplet Loss A number of previous works [3, 8, 20, 43] have
shown the effectiveness of joint training with softmax loss for clas-
sification and triplet loss for metric learning in person Re-ID tasks.
In our experiments, we reproduce the boosting effect with both
ResNet-50 and MGN models. With the help of triplet loss on all
the candidate datasets, we can observe +1.2%/3.6% Rank-1/mAP
improvement to the baseline model, and +0.4%/0.7% Rank-1/mAP
improvement to MGN model. We can observe two interesting ef-
fects from the improvement figures: 1) The improvement on mAP
is more obvious than that on rank-1 accuracy, which proves the
ranking effects of metric learning losses. 2) Triplet loss brings larger
improvement to the baseline model than that to MGN. Comparing
to softmax loss, triplet loss helps to capture more detailed infor-
mation to meet the margin condition [28]. Our proposed network
architecture is initially designed to enhance the local representa-
tion, which dilutes the original effects of triplet loss. Notice that in
the MGN without triplet loss setting (MGN w/o TP), we employ no
softmax loss on 2048-dim features, and alternatively on the reduced
256-dim features.

Feature response with multiple granularity We insist on
our proposed MGN model learns the global and local feature repre-
sentations with multiple levels of granularities. Figure 5 shows some
feature response maps for some input pedestrian images, extracted
from all the top of branches in MGN. All the response maps filter
most of the complex background, which contain no useful infor-
mation about identities for pedestrians. The responses from Global
Branch is mainly focused on main body parts, and the information
on limbs, waists or feet is commonly ignored. On local branches,
the global responses on main body are lost, but concentrate more
on some particular parts. For example, we can observe preferences
on body parts such as shoulders and joints in Part-2 cases. As for
the Part-3 cases, responses are more scattered on body parts, but
some pivotal semantic information is preferred. Notice that on the
Part-3 response map of the last pedestrian images, we can observe
a bright circle region in front of the chest of this pedestrian. It is a
mark of his T-shirt, which can be regarded as a very discriminative
characteristic for his identity.

5 CONCLUSION
In this paper, we propose the Multiple Granularity Network (MGN),
a novel multi-branch deep network for learning discriminative
representations in person re-identification tasks. Each branch in
MGN learns global or local representation with certain granular-
ity of body partition. Our method directly learns local features on
horizontally-split feature stripes, which is completely end-to-end
and introduces no part locating operations such as region proposal
or pose estimation. Extensive experiments have indicated that our
method not only achieves state-of-the-art results on several main-
stream person Re-ID datasets, but also pushes the performance to
an exceptional level comparing to existing methods.

Figure 5: Feature response maps extracted from output lay-
ers of every branches. First column: the original pedestrian
images. Second column: response maps from Global Branch.
Third column: response maps from Part-2 Branch. Fourth
column: response maps from Part-3 Branch. The brighter
the area is, the more concentrated it is. Best viewed in color.

Multi-branch architecture settings The multi-branch deep
network architectures are very common for person Re-ID tasks
[8]. Our proposed MGN exceeds all the previous architectures by
the power of learning representations with multiple granularities.
Based on our proposed architecture, we can intuitively infer many
variants architectures. On the one hand, based on the MGN model,
we can add or reduce the number of local branches. Comparing the
model removed the Part-3 branch with that added a Part-4 branch,
we find that the removal of Part-3 Branch (MGN w/o Part-3) brings
obvious performance degradation by Rank-1/mAP=−0.9%/0.7%, and
the addition of Part-4 Branch (MGN w/ Part-4) introduce no obvious
performance boost. This confirms the necessity and efficiency of
our proposed 3-branch setting. On the other hand, for the Part-N lo-
cal branches, the number of partitions is a hyperparameter effecting
the granularity of learning representations. We divide the feature
maps into 2, 3, 4 in each local branches alternatively, and still find
that the proposed Part-2/Part-3 setting is optimal. The Part-2/Part-4
setting skips the granularity level of Part-3, and brings performance
degradation by Rank-1/mAP=−0.9%/1.3%, which shows the impor-
tance of Part-3 granularity. Comparing the results of Part-2/Part-4

[32] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. 2016. Deep metric

learning via lifted structured feature embedding. In CVPR. 4004–4012.

[33] Chi Su, Jianing Li, Shiliang Zhang, Junliang Xing, Wen Gao, and Qi Tian. 2017.
Pose-driven Deep Convolutional Model for Person Re-identification. In ICCV.
3980–3989.

[34] Yi Sun, Xiaogang Wang, and Xiaoou Tang. 2015. Deeply learned face representa-

tions are sparse, selective, and robust. In CVPR. 2892–2900.

[35] Yifan Sun, Liang Zheng, Weijian Deng, and Shengjin Wang. 2017. SVDNet for

Pedestrian Retrieval. In ICCV. 2590–2600.

[36] Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin Wang. 2018. Beyond Part
Models: Person Retrieval with Refined Part Pooling. In ECCV. In press.

[37] Rahul Rama Varior, Mrinal Haloi, and Gang Wang. 2016. Gated siamese con-
volutional neural network architecture for human re-identification. In ECCV.
Springer, 791–808.

[38] Feng Wang, Xiang Xiang, Jian Cheng, and Alan Loddon Yuille. 2017. Normface:
l2 hypersphere embedding for face verification. In 2017 ACM on Multimedia
Conference. 1041–1049.

[39] Tong Xiao, Hongsheng Li, Wanli Ouyang, and Xiaogang Wang. 2016. Learn-
ing deep feature representations with domain guided dropout for person re-
identification. In CVPR. 1249–1258.

[40] Jing Xu, Rui Zhao, Feng Zhu, Huaming Wang, and Wanli Ouyang. 2018. Attention-
Aware Compositional Network for Person Re-Identification. In CVPR. 2119–2128.
[41] Hantao Yao, Shiliang Zhang, Yongdong Zhang, Jintao Li, and Qi Tian. 2017. Deep
representation learning with part loss for person re-identification. arXiv preprint
arXiv:1707.00798 (2017).

[42] Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. 2014. Deep metric learning for

person re-identification. In ICPR. 34–39.

[43] Xuan Zhang, Hao Luo, Xing Fan, Weilai Xiang, Yixiao Sun, Qiqi Xiao, Wei Jiang,
Chi Zhang, and Jian Sun. 2017. Alignedreid: Surpassing human-level performance
in person re-identification. arXiv preprint arXiv:1711.08184 (2017).

[44] Haiyu Zhao, Maoqing Tian, Shuyang Sun, Jing Shao, Junjie Yan, Shuai Yi, Xi-
aogang Wang, and Xiaoou Tang. 2017. Spindle net: Person re-identification
with human body region guided feature decomposition and fusion. In CVPR.
1077–1085.

[45] Liming Zhao, Xi Li, Jingdong Wang, and Yueting Zhuang. 2017. Deeply-learned

part-aligned representations for person re-identification. In ICCV. 3219–3228.

[46] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and Qi Tian.
2015. Scalable Person Re-identification: A Benchmark. In ICCV. 1116–1124.

[47] Liang Zheng, Yi Yang, and Alexander G Hauptmann. 2016.

Person re-

identification: Past, present and future. arXiv preprint arXiv:1610.02984 (2016).
[48] Zhedong Zheng, Liang Zheng, and Yi Yang. 2017. Pedestrian alignment network
for large-scale person re-identification. arXiv preprint arXiv:1707.00408 (2017).
[49] Zhedong Zheng, Liang Zheng, and Yi Yang. 2017. Unlabeled Samples Generated
by GAN Improve the Person Re-identification Baseline in vitro. In ICCV. 3774–
3782.

[50] Zhun Zhong, Liang Zheng, Donglin Cao, and Shaozi Li. 2017. Re-ranking person

re-identification with k-reciprocal encoding. In CVPR. 3652–3661.

REFERENCES
[1] Ejaz Ahmed, Michael Jones, and Tim K Marks. 2015. An improved deep learning

architecture for person re-identification. In CVPR. 3908–3916.

[2] Jon Almazan, Bojana Gajic, Naila Murray, and Diane Larlus. 2018. Re-ID
done right: towards good practices for person re-identification. arXiv preprint
arXiv:1801.05339 (2018).

[3] Xiang Bai, Mingkun Yang, Tengteng Huang, Zhiyong Dou, Rui Yu, and Yongchao
Xu. 2017. Deep-Person: Learning Discriminative Deep Features for Person Re-
Identification. arXiv preprint arXiv:1711.10658 (2017).

[4] Xiaobin Chang, Timothy M. Hospedales, and Tao Xiang. 2018. Multi-Level

Factorisation Net for Person Re-Identification. In CVPR. 2109–2118.

[5] Dapeng Chen, Dan Xu, Hongsheng Li, Nicu Sebe, and Xiaogang Wang. 2018.
Group Consistent Similarity Learning via Deep CRF for Person Re-Identification.
In CVPR. 8649–8658.

[6] Weihua Chen, Xiaotang Chen, Jianguo Zhang, and Kaiqi Huang. 2017. Beyond
triplet loss: a deep quadruplet network for person re-identification. In CVPR.
403–412.

[7] Yanbei Chen, Xiatian Zhu, and Shaogang Gong. 2017. Person Re-Identification

by Deep Learning Multi-Scale Representations. In ICCV. 2590–2600.

[8] De Cheng, Yihong Gong, Sanping Zhou, Jinjun Wang, and Nanning Zheng. 2016.
Person re-identification by multi-channel parts-based cnn with improved triplet
loss function. In CVPR. 1335–1344.

[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet:

A large-scale hierarchical image database. In CVPR. 248–255.

[10] Pedro Felzenszwalb, David McAllester, and Deva Ramanan. 2008. A discrimina-

tively trained, multiscale, deformable part model. In CVPR. 1–8.

[11] Ross Girshick. 2015. Fast r-cnn. In ICCV. 1440–1448.
[12] Raia Hadsell, Sumit Chopra, and Yann LeCun. 2006. Dimensionality reduction

by learning an invariant mapping. In CVPR, Vol. 2. 1735–1742.

[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual

learning for image recognition. In CVPR. 770–778.

[14] Alexander Hermans, Lucas Beyer, and Bastian Leibe. 2017. In defense of the
triplet loss for person re-identification. arXiv preprint arXiv:1703.07737 (2017).
[15] Elad Hoffer and Nir Ailon. 2015. Deep metric learning using triplet network. In
International Workshop on Similarity-Based Pattern Recognition. Springer, 84–92.
[16] Houjing Huang, Dangwei Li, Zhang Zhang, Xiaotang Chen, and Kaiqi Huang.
2018. Adversarially Occluded Samples for Person Re-Identification. In CVPR.
5098–5107.

[17] Sergey Ioffe and Christian Szegedy. 2015. Batch normalization: Accelerating deep
network training by reducing internal covariate shift. In ICML. 448–456.
[18] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. 2015. Spatial trans-

former networks. In NIPS. 2017–2025.

[19] Dangwei Li, Xiaotang Chen, Zhang Zhang, and Kaiqi Huang. 2017. Learning deep
context-aware features over body and latent parts for person re-identification. In
CVPR. 384–393.

[20] Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang. 2014. Deepreid: Deep filter

pairing neural network for person re-identification. In CVPR. 152–159.

[21] Wei Li, Xiatian Zhu, and Shaogang Gong. 2017. Person re-identification by deep

joint learning of multi-loss classification. In IJCAI. 2194–2200.

[22] Wei Li, Xiatian Zhu, and Shaogang Gong. 2018. Harmonious Attention Network

for Person Re-Identification. In CVPR. 2285–2294.

[23] Shengcai Liao, Yang Hu, Xiangyu Zhu, and Stan Z Li. 2015.

Person re-
identification by local maximal occurrence representation and metric learning.
In CVPR. 2197–2206.

[24] Hao Liu, Jiashi Feng, Meibin Qi, Jianguo Jiang, and Shuicheng Yan. 2017. End-to-
end comparative attention networks for person re-identification. IEEE Transac-
tions on Image Processing 26, 7 (2017), 3492–3506.

[25] Xihui Liu, Haiyu Zhao, Maoqing Tian, Lu Sheng, Jing Shao, Shuai Yi, Junjie
Yan, and Xiaogang Wang. 2017. HydraPlus-Net: Attentive Deep Features for
Pedestrian Analysis. In CVPR. 350–359.

[26] Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara, and Carlo Tomasi.
2016. Performance Measures and a Data Set for Multi-Target, Multi-Camera
Tracking. In ECCV workshop on Benchmarking Multi-Target Tracking. 17–35.
[27] M. Saquib Sarfraz, Arne Schumann, Andreas Eberle, and Rainer Stiefelhagen.
2018. A Pose-Sensitive Embedding for Person Re-Identification With Expanded
Cross Neighborhood Re-Ranking. In CVPR. 420–429.

[28] Florian Schroff, Dmitry Kalenichenko, and James Philbin. 2015. Facenet: A unified

embedding for face recognition and clustering. In CVPR. 815–823.

[29] Yantao Shen, Hongsheng Li, Tong Xiao, Shuai Yi, Dapeng Chen, and Xiaogang
Wang. 2018. Deep Group-Shuffling Random Walk for Person Re-Identification.
In CVPR. 2265–2274.

[30] Yantao Shen, Tong Xiao, Hongsheng Li, Shuai Yi, and Xiaogang Wang. 2018.
End-to-End Deep Kronecker-Product Matching for Person Re-Identification. In
CVPR. 6886–6895.

[31] Jianlou Si, Honggang Zhang, Chun-Guang Li, Jason Kuen, Xiangfei Kong, Alex C.
Kot, and Gang Wang. 2018. Dual Attention Matching Network for Context-Aware
Feature Sequence Based Person Re-Identification. In CVPR. 5363–5372.

