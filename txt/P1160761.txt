Semantics-aware BERT for Language Understanding
Zhuosheng Zhang1,2,3,∗, Yuwei Wu1,2,3,4,*, Hai Zhao1,2,3,†, Zuchao Li1,2,3,
Shuailiang Zhang1,2,3, Xi Zhou5, Xiang Zhou5
1Department of Computer Science and Engineering, Shanghai Jiao Tong University
2Key Laboratory of Shanghai Education Commission for Intelligent Interaction
and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China
3MoE Key Lab of Artiﬁcial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China
4College of Zhiyuan, Shanghai Jiao Tong University, China
5CloudWalk Technology, Shanghai, China
{zhangzs,will8821}@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn

0
2
0
2
 
b
e
F
 
4
 
 
]
L
C
.
s
c
[
 
 
3
v
9
0
2
2
0
.
9
0
9
1
:
v
i
X
r
a

Abstract

The latest work on language representations carefully in-
tegrates contextualized features into language model train-
ing, which enables a series of success especially in vari-
ous machine reading comprehension and natural language in-
ference tasks. However, the existing language representation
models including ELMo, GPT and BERT only exploit plain
context-sensitive features such as character or word embed-
dings. They rarely consider incorporating structured seman-
tic information which can provide rich semantics for lan-
guage representation. To promote natural language under-
standing, we propose to incorporate explicit contextual se-
mantics from pre-trained semantic role labeling, and intro-
duce an improved language representation model, Semantics-
aware BERT (SemBERT), which is capable of explicitly ab-
sorbing contextual semantics over a BERT backbone. Sem-
BERT keeps the convenient usability of its BERT precursor in
a light ﬁne-tuning way without substantial task-speciﬁc modi-
ﬁcations. Compared with BERT, semantics-aware BERT is as
simple in concept but more powerful. It obtains new state-of-
the-art or substantially improves results on ten reading com-
prehension and language inference tasks.

1

Introduction

Recently, deep contextual language model (LM) has been
shown effective for learning universal language representa-
tions, achieving state-of-the-art results in a series of ﬂag-
ship natural language understanding (NLU) tasks. Some
prominent examples are Embedding from Language models
(ELMo) (Peters et al. 2018), Generative Pre-trained Trans-
former (OpenAI GPT) (Radford et al. 2018), Bidirectional
Encoder Representations from Transformers (BERT) (De-
vlin et al. 2018) and Generalized Autoregressive Pretrain-
ing (XLNet) (Yang et al. 2019). Providing ﬁne-grained con-
textual embedding, these pre-trained models could be either
easily applied to downstream models as the encoder or used
for ﬁne-tuning.

∗These authors contribute equally. †Corresponding author. This
paper was partially supported by National Key Research and
Development Program of China (No. 2017YFB0304100) and
Key Projects of National Natural Science Foundation of China
(U1836222 and 61733011).
Copyright c(cid:13) 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Despite the success of those well pre-trained language
models, we argue that current techniques which only fo-
cus on language modeling restrict the power of the pre-
trained representations. The major limitation of existing lan-
guage models lies in only taking plain contextual features
for both representation and training objective, rarely consid-
ering explicit contextual semantic clues. Even though well
pre-trained language models can implicitly represent con-
textual semantics more or less (Clark et al. 2019), they can
be further enhanced by incorporating external knowledge.
To this end, there is a recent trend of incorporating ex-
tra knowledge to pre-trained language models (Zhang et al.
2020b).

A number of studies have found deep learning models
might not really understand the natural language texts (Mu-
drakarta et al. 2018) and vulnerably suffer from adversar-
ial attacks (Jia and Liang 2017). Through their observation,
deep learning models pay great attention to non-signiﬁcant
words and ignore important ones. For attractive question
answering challenge (Rajpurkar et al. 2016), we observe a
number of answers produced by previous models are seman-
tically incomplete (As shown in Section 6.3), which suggests
that the current NLU models suffer from insufﬁcient contex-
tual semantic representation and learning.

Actually, NLU tasks share the similar task purpose as sen-
tence contextual semantic analysis. Brieﬂy, semantic role la-
beling (SRL) over a sentence is to discover who did what to
whom, when and why with respect to the central meaning
of the sentence, which naturally matches the task target of
NLU. For example, in question answering tasks, questions
are usually formed with who, what, how, when and why,
which can be conveniently formulized into the predicate-
argument relationship in terms of contextual semantics.

In human language, a sentence usually involves various
predicate-argument structures, while neural models encode
sentence into embedding representation, with little consider-
ation of the modeling of multiple semantic structures. Thus
we are motivated to enrich the sentence contextual seman-
tics in multiple predicate-speciﬁc argument sequences by
presenting SemBERT: Semantics-aware BERT, which is a
ﬁne-tuned BERT with explicit contextual semantic clues.
The proposed SemBERT learns the representation in a ﬁne-

grained manner and takes both strengths of BERT on plain
context representation and explicit semantics for deeper
meaning representation.

Our model consists of three components: 1) an out-of-
shelf semantic role labeler to annotate the input sentences
with a variety of semantic role labels; 2) an sequence en-
coder where a pre-trained language model is used to build
representation for input raw texts and the semantic role la-
bels are mapped to embedding in parallel; 3) a semantic
integration component to integrate the text representation
with the contextual explicit semantic embedding to obtain
the joint representation for downstream tasks.

The proposed SemBERT will be directly applied to typ-
ical NLU tasks. Our model is evaluated on 11 benchmark
datasets involving natural language inference, question an-
swering, semantic similarity and text classiﬁcation. Sem-
BERT obtains new state-of-the-art on SNLI and also ob-
tains signiﬁcant gains on the GLUE benchmark and SQuAD
2.0. Ablation studies and analysis verify that our intro-
duced explicit semantics is essential to the further perfor-
mance improvement and SemBERT essentially and effec-
tively works as a uniﬁed semantics-enriched language rep-
resentation model1.

2 Background and Related Work

2.1 Language Modeling for NLU
Natural language understanding tasks require a comprehen-
sive understanding of natural languages and the ability to
do further inference and reasoning. A common trend among
NLU studies is that models are becoming more and more
sophisticated with stacked attention mechanisms or large
amount of corpus (Zhang et al. 2018; 2020a; Zhou, Zhang,
and Zhao 2019), resulting in explosive growth of compu-
tational cost. Notably, well pre-trained contextual language
models such as ELMo (Peters et al. 2018), GPT (Radford et
al. 2018) and BERT (Devlin et al. 2018) have been shown
powerful to boost NLU tasks to reach new high perfor-
mance.

Distributed representations have been widely used as a
standard part of NLP models due to the ability to capture
the local co-occurence of words from large scale unlabeled
text (Mikolov et al. 2013). However, these approaches for
learning word vectors only involve a single, context indepen-
dent representation for each word with litter consideration of
contextual encoding in sentence level. Thus recently intro-
duced contextual language models including ELMo, GPT,
BERT and XLNet ﬁll the gap by strengthening the con-
textual sentence modeling for better representation, among
which BERT uses a different pre-training objective, masked
language model, which allows capturing both sides of con-
text, left and right. Besides, BERT also introduces a next
sentence prediction task that jointly pre-trains text-pair rep-
resentations. The latest evaluation shows that BERT is pow-
erful and convenient for downstream NLU tasks.

The major technical improvement over traditional embed-
dings of these newly proposed language models is that they

1The code is publicly available at https://github.com/cooelf/

SemBERT.

focus on extracting context-sensitive features from language
models. When integrating these contextual word embed-
dings with existing task-speciﬁc architectures, ELMo helps
boost several major NLP benchmarks (Peters et al. 2018) in-
cluding question answering on SQuAD, sentiment analysis
(Socher et al. 2013), and named entity recognition (Sang and
De Meulder 2003), while BERT especially shows effective
on language understanding tasks on GLUE, MultiNLI and
SQuAD (Devlin et al. 2018). In this work, we follow this line
of extracting context-sensitive features and take pre-trained
BERT as our backbone encoder for jointly learning explicit
context semantics.

2.2 Explicit Contextual Semantics
Although distributed representations including the latest ad-
vanced pre-trained contextual language models have already
been strengthened by semantics to some extent from linguis-
tic sense (Clark et al. 2019), we argue such implicit seman-
tics may not be enough to support a powerful contextual rep-
resentation for NLU, according to our observation on the se-
mantically incomplete answer span generated by BERT on
SQuAD, which motivates us to directly introduce explicit
semantics.

There are a few formal semantic frames,

including
FrameNet (Baker, Fillmore, and Lowe 1998) and PropBank
(Palmer, Gildea, and Kingsbury 2005), in which the latter is
more popularly implemented in computational linguistics.
Formal semantics generally presents the semantic relation-
ship as predicate-argument structure. For example, given the
following sentence with target verb (predicate) sold, all the
arguments are labeled as follows,

[ARG0 Charlie] [V sold] [ARG1 a book] [ARG2 to Sherry]

[AM −T M P last week].
where ARG0 represents the seller (agent), ARG1 repre-
sents the thing sold (theme), ARG2 represents the buyer (re-
cipient), AM − T M P is an adjunct indicating the timing of
the action and V represents the predicate.

To parse the predicate-argument structure, we have an
NLP task, semantic role labeling (SRL) (Zhao, Chen, and
Kit 2009; Zhao, Zhang, and Kit 2013). Recently, end-to-
end SRL system neural models have been introduced (He
et al. 2017; Li et al. 2019). These studies tackle argument
identiﬁcation and argument classiﬁcation in one shot. He et
al. (2017) presented a deep highway BiLSTM architecture
with constrained decoding, which is simple and effective,
enabling us to select it as our basic semantic role labeler. In-
spired by recent advances, we can easily integrate SRL into
NLU.

3 Semantics-aware BERT
Figure 1 overviews our semantics-aware BERT framework.
We omit rather extensive formulations of BERT and recom-
mend readers to get the details from (Devlin et al. 2018).
SemBERT is designed to be capable of handling multiple se-
quence inputs. In SemBERT, words in the input sequence are
passed to semantic role labeler to fetch multiple predicate-
derived structures of explicit semantics and the correspond-
ing embeddings are aggregated after a linear layer to form

For the text, {reconstructing dormitories will not be approved by cavanaugh}, it will be tokenized to a subword-level sequence, {rec, ##ons, ##tructing, dorm, ##itor, ##ies, will,
not, be, approved, by, ca, ##vana, ##ugh}. Meanwhile, there are two kinds of word-level semantic structures,
[ARG1: reconstructing dormitories] [ARGM-MOD: will] [ARGM-NEG: not] be [V: approved] [ARG0: by cavanaugh]
[V: reconstructing] [ARG1: dormitories] will not be approved by cavanaugh

Figure 1: Semantics-aware BERT. * denotes the pre-trained labeler which will not be ﬁne-tuned in our framework.

the ﬁnal semantic embedding. In parallel, the input sequence
is segmented to subwords (if any) by BERT word-piece tok-
enizer, then the subword representation is transformed back
to word level via a convolutional layer to obtain the contex-
tual word representations. At last, the word representations
and semantic embedding are concatenated to form the joint
representation for downstream tasks.

3.1 Semantic Role Labeling

During the data pre-processing, each sentence is annotated
into several semantic sequences using our pre-trained se-
mantic labeler. We take PropBank style (Palmer, Gildea, and
Kingsbury 2005) of semantic roles to annotate every token
of input sequence with semantic labels. Given a speciﬁc sen-
tence, there would be various predicate-argument structures.
As shown in Figure 1, for the text, [reconstructing dormi-
tories will not be approved by cavanaugh], there are two
semantic structures in the view of the predicates in the sen-
tence.

To disclose the multidimensional semantics, we group the
semantic labels and integrate them with text embeddings in
the next encoding component. The input data ﬂow is de-
picted in Figure 2.

3.2 Encoding

The raw text sequences and semantic role label sequences
are ﬁrstly represented as embedding vectors to feed a pre-
trained BERT. The input sentence X = {x1, . . . , xn} is
a sequence of words of length n, which is ﬁrst tokenized
to word pieces (subword tokens). Then the transformer en-
coder captures the contextual information for each token via
self-attention and produces a sequence of contextual embed-
dings.

1, labeli

For m label sequences related to each predicate, we
have T = {t1, . . . , tm} where ti contains n labels de-
2, ..., labeli
noted as {labeli
n}. Since our labels are
in word-level, the length is equal to the original sentence
length n of X. We regard the semantic signals as embed-
dings and use a lookup table to map these labels to vec-
tors {vi
n} and feed a BiGRU layer to obtain the
label representations for m label sequences in latent space,
n) where 0 < i (cid:54) m. For m
e(ti) = BiGRU (vi
label sequences, let Li denote the label sequences for token
xi, we have e(Li) = {e(t1), . . . , e(tm)}. We concatenate
the m sequences of label representation and feed them to a
fully connected layer to obtain the reﬁned joint representa-

2, . . . , vi

2, ..., vi

1, vi

1, vi

Figure 2: The input representation ﬂow.

tion et in dimension d:

e(cid:48)(Li) = W2 [e(t1), e(t2), . . . , e(tm)] + b2,

et = {e(cid:48)(L1), ..., e(cid:48)(Ln)},

(1)

where W2 and b2 are trainable parameters.

Integration

3.3
This integration module fuses the lexical text embedding and
label representations. As the original pre-trained BERT is
based on a sequence of subwords, while our introduced se-
mantic labels are on words, we need to align these differ-
ent sized sequences. Thus we group the subwords for each
word and use convolutional neural network (CNN) with a
max pooling to obtain the representation in word-level. We
select CNN because of fast speed and our preliminary ex-
periments show that it also gives better results than RNNs
in our concerned tasks where we think the local feature cap-
tured by CNN would be beneﬁcial for subword-derived LM
modeling.

We take one word for example. Supposing that word xi is
made up of a sequence of subwords [s1, s2, ..., sl], where l
is the number of subwords for word xi. Denoting the repre-
sentation of subword sj from BERT as e(sj), we ﬁrst utilize
a Conv1D layer, e(cid:48)
i = W1 [e(si), e(si+1), . . . , e(si+k−1)] +
b1, where W1 and b1 are trainable parameters and k is the
kernel size. We then apply ReLU and max pooling to the
output embedding sequence for xi:

i = ReLU (e(cid:48)
e∗

i), e(xi) = M axP ooling(e∗

1, ..., e∗

l−k+1),

(2)
Therefore, the whole representation for word sequence X is
represented as ew = {e(x1), . . . e(xn)} ∈ Rn×dw where dw
denotes the dimension of word embedding.

The aligned context and distilled semantic embeddings
are then merged by a fusion function h = ew (cid:5) et, where
(cid:5) represents concatenation operation2.

4 Model Implementation
Now, we introduce the speciﬁc implementation parts of our
SemBERT. SemBERT could be a forepart encoder for a wide
range of tasks and could also become an end-to-end model

with only a linear layer for prediction. For simplicity, we
only show the straightforward SemBERT that directly gives
the predictions after ﬁne-tuning3.

4.1 Semantic Role Labeler
To obtain the semantic labels, we use a pre-trained SRL
module to predict all predicates and corresponding argu-
ments in one shot. We implement the semantic role labeler
from Peters et al. (2018), achieving an F1 of 84.6%4 on
English OntoNotes v5.0 benchmark dataset (Pradhan et al.
2013) for the CoNLL-2012 shared task. At test time, we
perform Viterbi decoding to enforce valid spans using BIO
constraints. In our implementation, there are 104 labels in
total. We use O for non-argument words and Verb label for
predicates.

4.2 Task-speciﬁc Fine-tuning
In Section 3, we have described how to obtain the semantics-
aware BERT representations. Here, we show how to adapt
SemBERT to classiﬁcation, regression and span-based MRC
tasks. We transform the fused contextual semantic and LM
representations h to a lower dimension and obtain the pre-
diction distributions. Note that this part is basically the same
as the implementation in BERT without any modiﬁcation, to
avoid extra inﬂuence and focus on the intrinsic performance
of SemBERT. We outline here to keep the completeness of
the implementation.

For classiﬁcation and regression tasks, h is directly passed
to a fully connection layer to get the class logits or score,
respectively. The training objectives are CrossEntropy for
classiﬁcation tasks and Mean Square Error loss for regres-
sion tasks.

For span-based reading comprehension, h is passed to a
fully connection layer to get the start logits s and end log-
its e of all tokens. The score of a candidate span from po-
sition i to position j is deﬁned as si + ej, and the maxi-
mum scoring span where j ≥ i is used as a prediction5. For
prediction, we compare the score of the pooled ﬁrst token
span: snull = s0 + e0 to the score of the best non-null span

3We only use single model for each task without jointly training

and parameter sharing.

2We also tried summation, multiplication and attention mecha-

nisms, but our experiments show that concatenation is the best.

4This result nearly reaches the SOTA in (He et al. 2018).
5All the candidate scores are normanized by softmax.

Method

ALBERT
RoBERTa
XLNET

BiLSTM+ELMo+Attn
GPT
GPT on STILTs
MT-DNN
BERTBASE
BERTLARGE

SemBERTBASE
SemBERTLARGE

Classiﬁcation Natural Language Inference
CoLA SST-2
MNLI
(mc)

(acc) m/mm(acc)

QNLI
(acc)

Semantic Similarity

RTE MRPC QQP
(F1)
(F1)
(acc)

STS-B
(pc)

Score
-
-

69.1
67.8
67.8

36.0
45.4
47.2
61.5
52.1
60.5

57.8
62.3

97.1
96.7
96.8

90.4
91.3
93.1
95.6
93.5
94.9

93.5
94.6

Leaderboard (September, 2019)

91.3/91.0
90.8/90.2
90.2/89.8

99.2
98.9
98.6

In literature (April, 2019)

79.9
88.1
87.2
-
-
92.7

76.4/76.1
82.1/81.4
80.8/80.6
86.7/86.0
84.6/83.4
86.7/85.9
Our implementation
84.4/84.0
87.6/86.3

90.9
94.6

89.2
88.2
86.3

56.8
56.0
69.1
75.5
66.4
70.1

69.3
84.5

93.4
92.1
93.0

84.9
82.3
87.7
90.0
88.9
89.3

88.2
91.2

74.2
90.2
90.3

64.8
70.3
70.1
72.4
71.2
72.1

71.8
72.8

92.5
92.2
91.6

75.1
82.0
85.3
88.3
87.1
87.6

87.3
87.8

89.4
88.5
88.4

70.5
72.8
76.9
82.2
78.3
80.5

80.9
82.9

Table 1: Results on GLUE benchmark. The block In literatures shows the comparable results from (Liu et al. 2019; Radford et
al. 2018) at the time of submitting SemBERT to GLUE (April, 2019).

ˆsi,j = maxj≥i(si + ej). We predict a non-null answer when
ˆsi,j > snull +τ , where the threshold τ is selected on the dev
set to maximize F1.

5 Experiments

5.1 Setup
Our implementation is based on the PyTorch implementa-
tion of BERT6. We use the pre-trained weights of BERT
and follow the same ﬁne-tuning procedure as BERT without
any modiﬁcation, and all the layers are tuned with moderate
model size increasing, as the extra SRL embedding volume
is less than 15% of the original encoder size. We set the ini-
tial learning rate in {8e-6, 1e-5, 2e-5, 3e-5} with warm-up
rate of 0.1 and L2 weight decay of 0.01. The batch size is
selected in {16, 24, 32}. The maximum number of epochs
is set in [2, 5] depending on tasks. Texts are tokenized using
wordpieces, with maximum length of 384 for SQuAD and
128 or 200 for other tasks. The dimension of SRL embed-
ding is set to 10. The default maximum number of predicate-
argument structures m is set to 3.

5.2 Tasks and Datasets
Our evaluation is performed on ten NLU benchmark datasets
involving natural language inference, machine reading com-
prehension, semantic similarity and text classiﬁcation. Some
of these tasks are available from the recently released GLUE
benchmark (Wang et al. 2018), which is a collection of nine
NLU tasks. We also extend our experiments to two widely-
used tasks, SNLI (Bowman et al. 2015) and SQuAD 2.0 (Ra-
jpurkar, Jia, and Liang 2018) to show the superiority.

Reading Comprehension As a widely used MRC bench-
mark dataset, SQuAD 2.0 (Rajpurkar, Jia, and Liang 2018)

combines the 100,000 questions in SQuAD 1.1 (Rajpurkar
et al. 2016) with over 50,000 new, unanswerable questions
that are written adversarially by crowdworkers to look sim-
ilar to answerable ones. For SQuAD 2.0, systems must not
only answer questions when possible, but also abstain from
answering when no answer is supported by the paragraph.

Natural Language Inference Natural Language Infer-
ence involves reading a pair of sentences and judging the re-
lationship between their meanings, such as entailment, neu-
tral and contradiction. We evaluate on 4 diverse datasets, in-
cluding Stanford Natural Language Inference (SNLI) (Bow-
man et al. 2015), Multi-Genre Natural Language Inference
(MNLI) (Nangia et al. 2017), Question Natural Language
Inference (QNLI) (Rajpurkar et al. 2016) and Recognizing
Textual Entailment (RTE) (Bentivogli et al. 2009).

Semantic Similarity Semantic similarity tasks aim to pre-
dict whether two sentences are semantically equivalent or
not. The challenge lies in recognizing rephrasing of con-
cepts, understanding negation, and handling syntactic am-
biguity. Three datasets are used, including Microsoft Para-
phrase corpus (MRPC) (Dolan and Brockett 2005), Quora
Question Pairs (QQP) dataset (Chen et al. 2018) and Seman-
tic Textual Similarity benchmark (STS-B) (Cer et al. 2017).

Classiﬁcation The Corpus of Linguistic Acceptability
(CoLA) (Warstadt, Singh, and Bowman 2018) is used to pre-
dict whether an English sentence is linguistically acceptable
or not. The Stanford Sentiment Treebank (SST-2) (Socher
et al. 2013) provides a dataset for sentiment classiﬁcation
that needs to determine whether the sentiment of a sentence
extracted from movie reviews is positive or negative.

6https://github.com/huggingface/pytorch-pretrained-BERT

7https://nlp.stanford.edu/seminar/details/jdevlin.pdf

Model
#1 BERT + DAE + AoA†
#2 SG-Net†
#3 BERT + NGM + SST†
U-Net (Sun et al. 2018)
RMR + ELMo + Veriﬁer (Hu et al. 2018)

Our implementation

BERTLARGE
SemBERTLARGE
SemBERT∗
LARGE

EM F1
88.6
85.9
87.9
85.2
87.7
85.2
72.6
69.2
74.2
71.7

80.5
82.4
84.8

83.6
85.2
87.9

Table 2: Exact Match (EM) and F1 scores on SQuAD 2.0 test
set for single models. † denotes the top 3 single submissions
from the leaderboard at the time of submitting SemBERT
(11 April, 2019). Most of the top results from the SQuAD
leaderboard do not have public model descriptions available,
and it is allowed to use any public data for system training.
We therefore further adopt synthetic self training7 for data
augmentation, denoted as SemBERT∗

LARGE.

5.3 Results
Table 1 shows results on the GLUE benchmark datasets,
showing SemBERT gives substantial gains over BERT and
outperforms all the previous state-of-the-art models in liter-
ature7. Since SemBERT takes BERT as the backbone with
the same evaluation procedure, the gain is entirely owing to
newly introduced explicit contextual semantics. Though re-
cent dominant models take advance of multi-tasking, knowl-
edge distillation, transfer learning or ensemble, our single
model is lightweight and competitive, even yields better re-
sults with simple design and less parameters. Model parame-
ter comparison is shown in Table 4. We observe that without
multi-task learning like MT-DNN8, our model still achieves
remarkable results.

Particularly, we observe substantial

improvements on
small datasets such as RTE, MRPC, CoLA, which demon-
strates involving explicit semantics helps the model work
better with small training data, which is important for most
NLP tasks as large-scale annotated data is unavailable.

Table 2 shows the results for reading comprehension on
SQuAD 2.0 test set9. SemBERT boosts the strong BERT
baseline essentially on both EM and F1. It also outperforms
all the published works and achieves comparable perfor-
mance with a few unpublished models from the leaderboard.
Table 3 shows SemBERT also achieves a new state-of-
the-art on SNLI benchmark and even outperforms all the en-
semble models10 by a large margin.

7We ﬁnd that MNLI model can be effectively transferred for
RTE and MRPC datasets, thus the models for RTE and MRPC are
ﬁne-tuned base on our MNLI model.

8Since MT-DNN is a multi-task learning framework with
shared parameters on 9 task-speciﬁc layers, we count the 340M
shared parameters for nine times for fair comparison.

9There is a restriction of submission frequency for online

SQuAD 2.0 evaluation, we do not submit our base models.

10https://nlp.stanford.edu/projects/snli/. As ensemble models are
commonly composed of multiple heterogeneous models and re-

Model

In literature

DRCN (Kim et al. 2018)
SJRC (Zhang et al. 2019)
MT-DNN (Liu et al. 2019)†

Our implementation

BERTBASE
SemBERTBASE
BERTLARGE
SemBERTLARGE
BERTWWM
SemBERTWWM

Dev

-
-
92.2

90.8
91.2
91.3
92.0
92.1
92.2

Test

90.1
91.3
91.6

90.7
91.0
91.1
91.6
91.6
91.9

Table 3: Accuracy on SNLI dataset. Previous state-of-the-
art result is marked by †. Both our SemBERT and BERT are
single models, ﬁne-tuned based on the pre-trained models.

Model

MT-DNN
BERT on STILTs
BERT
SemBERT

Params
(M)
3,060
335
335
340

Shared
(M)
340
-
-
-

Rate

9.1
1.0
1.0
1.0

Table 4: Parameter Comparison on LARGE models.
The numbers are from GLUE leaderboard (https://
gluebenchmark.com/leaderboard).

6 Analysis

6.1 Ablation Study

To evaluate the contributions of key factors in our method,
we perform an ablation study on the SNLI and SQuAD 2.0
dev sets as shown in Table 6. Since SemBERT absorbs con-
textual semantics in a deep processing way, we wonder if
a simple and straightforward way integrating such semantic
information may still work, thus we concatenate the SRL
embedding with BERT subword embeddings for a direct
comparison, where the semantic role labels are copied to
the number of subwords for each original word, without
CNN and pooling for word-level alignment. From the re-
sults, we observe that the concatenation would yield an im-
provement, verifying that integrating contextual semantics
would be quite useful for language understanding. However,
SemBERT still outperforms the simple BERT+SRL model
just like the latter outperforms the original BERT by a large
performance margin, which shows that SemBERT works
more effectively for integrating both plain contextual rep-
resentation and contextual semantics at the same time.

6.2 The inﬂuence of the number m

We investigate the inﬂuence of the max number of predicate-
argument structures m by setting it from 1 to 5. Table 7
shows the result. We observe that the modest number of m
would be better.

sources, we exclude them in our table to save space.

Question
What is a very seldom used unit of mass in the metric system?
What is the lone MLS team that belongs to southern California?
How many people does the Greater Los Angeles Area have?

Baseline
The ki
Galaxy
17.5 million

SemBERT
metric slug
LA Galaxy
over 17.5 million

Table 5: The comparison of answers from baseline and our model. In these examples, answers from SemBERT are the same as
the ground truth.

Model

BERTLARGE
BERTLARGE+SRL
SemBERTLARGE

SNLI
Dev
91.3
91.5
92.3

SQuAD 2.0
EM F1
82.4
79.6
83.1
80.3
83.6
80.9

Table 6: Analysis on SNLI and SQuAD 2.0 datasets.

Number
Accuracy

1
91.49

2
91.36

3
91.57

4
91.29

5
91.42

Table 7: The inﬂuence of the max number of predicate-
argument structures m.

6.3 Model Prediction

To have an intuitive observation of the predictions of Sem-
BERT, we show a list of prediction examples on SQuAD 2.0
from baseline BERT and SemBERT11 in Table 5. The com-
parison indicates that our model could extract more seman-
tically accurate answer, yielding more exact match answers
while those from the baseline BERT model are often seman-
tically incomplete. This shows that utilizing explicit seman-
tics is potential to guide the model to produce meaningful
predictions. Intuitively, the advance would attribute to better
awareness of semantic role spans, which guides the model
to learn the patterns like who did what to whom explicitly.

Through the comparison, we observe SemBERT might
beneﬁt from better span segmentation through span-based
SRL labeling. We conduct a case study on our best model of
SQuAD 2.0, by transforming SRL into segmentation tags to
indicate which token is inside or outside the segmented span.
The result is 83.69(EM)/87.02(F1), which shows that the
segmentation indeed works but marginally beneﬁcial com-
pared with our complete architecture.

It is worth noting that we are motivated to use the SRL
signals to help the model to capture the span relationships in-
side sentence, which results in both sides of semantic label
hints and segmentation beneﬁts across semantic role spans
to some extent. The segmentation could also be regarded
as the awareness of semantics even with better semantic
span segmentations. Intuitively, this indicates that our model
evolves from BERT subword-level representation to inter-
mediate word-level and ﬁnal semantic representations.

11Henceforth, we use the SemBERT* model from Table 2 as the

strong and challenging baseline for ablation.

6.4

Infulence of Accuracy of SRL

Our model relies on a semantic role labeler that would inﬂu-
ence the overall model performance. To investigate inﬂuence
of the accuracy of the labeler, we degrade our labeler by ran-
domly turning speciﬁc proportion [0, 20%, 40%] of labels
into random error ones as cascading errors. The F1 scores
of SQuAD are respectively [87.93, 87.31, 87.24]. This ad-
vantage can be attributed to the concatenation operation of
BERT hidden states and SRL representation, in which the
lower dimensional SRL representation (even noisy) would
not affect the former one intensely. This result indicates that
the LM can not only beneﬁt from high-accuracy labeler but
also keep robust against noisy labels.

Besides the wide range of tasks veriﬁed in this work, Sem-
BERT could also be easily adapted to other languages. As
SRL is a fundamental NLP task, it is convenient to train a
labeler for main languages as CoNLL 2009 provides 7 SRL
treebanks. For those without available treebanks, unsuper-
vised SRL methods can be effectively applied. For out-of-
domain issue, the datasets (GLUE and SQuAD) that we are
working on cover quite diverse domains, and experiments
show that our method still works.

7 Conclusion
This paper proposes a novel semantics-aware BERT net-
work architecture for ﬁne-grained language representation.
Experiments on a wide range of NLU tasks including nat-
ural language inference, question answering, machine read-
ing comprehension, semantic similarity and text classiﬁca-
tion show the superiority over the strong baseline BERT.
Our model has surpassed all the published works in all of
the concerned NLU tasks. This work discloses the effec-
tiveness of semantics-aware BERT in natural language un-
derstanding, which demonstrates that explicit contextual se-
mantics can be effectively integrated with state-of-the-art
pre-trained language representation for even better perfor-
mance improvement. Recently, most works focus on heuris-
tically stacking complex mechanisms for performance im-
provement, instead, we hope to shed some lights on fusing
accurate semantic signals for deeper comprehension and in-
ference through a simple but effective method.

References
Baker, C. F.; Fillmore, C. J.; and Lowe, J. B. 1998. The
berkeley framenet project. In COLING.
Bentivogli, L.; Clark, P.; Dagan, I.; and Giampiccolo, D.
2009. The ﬁfth pascal recognizing textual entailment chal-
lenge. In ACL-PASCAL.

Bowman, S. R.; Angeli, G.; Potts, C.; and Manning, C. D.
2015. A large annotated corpus for learning natural language
inference. In EMNLP.
Cer, D.; Diab, M.; Agirre, E.; Lopez-Gazpio, I.; and Specia,
L. 2017. Semeval-2017 task 1: Semantic textual similarity-
multilingual and cross-lingual focused evaluation. arXiv
preprint arXiv:1708.00055.
Chen, Z.; Zhang, H.; Zhang, X.; and Zhao, L. 2018. Quora
question pairs.

Clark, K.; Khandelwal, U.; Levy, O.; and Manning, C. D.
2019. What does BERT look at? an analysis of bert’s atten-
tion. arXiv preprint arXiv:1906.04341.
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.
BERT: Pre-training of deep bidirectional transformers for
language understanding. arXiv preprint arXiv:1810.04805.
Dolan, W. B., and Brockett, C. 2005. Automatically con-
structing a corpus of sentential paraphrases. In IWP2005.
He, L.; Lee, K.; Lewis, M.; Zettlemoyer, L.; He, L.; Lee, K.;
Lewis, M.; Zettlemoyer, L.; He, L.; and Lee, K. 2017. Deep
semantic role labeling: What works and whats next. In ACL.
He, L.; Lee, K.; Levy, O.; and Zettlemoyer, L. 2018. Jointly
predicting predicates and arguments in neural semantic role
labeling. In ACL.
Hu, M.; Peng, Y.; Huang, Z.; Yang, N.; Zhou, M.; et al. 2018.
Read+ verify: Machine reading comprehension with unan-
swerable questions. arXiv preprint arXiv:1808.05759.
Jia, R., and Liang, P. 2017. Adversarial examples for evalu-
ating reading comprehension systems. In EMNLP.
Kim, S.; Hong, J.-H.; Kang, I.; and Kwak, N. 2018. Seman-
tic sentence matching with densely-connected recurrent and
co-attentive information. arXiv preprint arXiv:1805.11360.
Li, Z.; He, S.; Zhao, H.; Zhang, Y.; Zhang, Z.; Zhou, X.;
2019. Dependency or span, end-to-end
and Zhou, X.
In AAAI. arXiv preprint
uniform semantic role labeling.
arXiv:1901.05280.

Liu, X.; He, P.; Chen, W.; and Gao, J. 2019. Multi-task deep
neural networks for natural language understanding. arXiv
preprint arXiv:1901.11504.
Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and
Dean, J. 2013. Distributed representations of words and
phrases and their compositionality. In NIPS.
Mudrakarta, P. K.; Taly, A.; Sundararajan, M.; and Dhamd-
here, K. 2018. Did the model understand the question? In
ACL.
Nangia, N.; Williams, A.; Lazaridou, A.; and Bowman, S. R.
2017. The repeval 2017 shared task: Multi-genre natural lan-
guage inference with sentence representations. In RepEval.
Palmer, M.; Gildea, D.; and Kingsbury, P. 2005. The propo-
sition bank: An annotated corpus of semantic roles. Compu-
tational linguistics 31(1):71–106.
Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark,
C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized
word representations. In NAACL-HLT.

Pradhan, S.; Moschitti, A.; Xue, N.; Ng, H. T.; Bj¨orkelund,
A.; Uryupina, O.; Zhang, Y.; and Zhong, Z. 2013. Towards
robust linguistic analysis using OntoNotes. In CoNLL.
Radford, A.; Narasimhan, K.; Salimans, T.; and Sutskever,
I. 2018. Improving language understanding by generative
pre-training. Technical report.
Rajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.
SQuAD: 100,000+ questions for machine comprehension of
text. In EMNLP.
Rajpurkar, P.; Jia, R.; and Liang, P. 2018. Know what you
don’t know: Unanswerable questions for SQuAD. In ACL.
Sang, E. F., and De Meulder, F. 2003. Introduction to the
conll-2003 shared task: Language-independent named entity
recognition. arXiv preprint cs/0306050.
Socher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning,
C. D.; Ng, A.; and Potts, C. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment treebank.
In EMNLP.
Sun, F.; Li, L.; Qiu, X.; and Liu, Y. 2018. U-net: Machine
reading comprehension with unanswerable questions. arXiv
preprint arXiv:1810.06638.
Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and
Bowman, S. 2018. Glue: A multi-task benchmark and anal-
ysis platform for natural language understanding. In 2018
EMNLP Workshop BlackboxNLP.
Warstadt, A.; Singh, A.; and Bowman, S. R.
Neural network acceptability judgments.
arXiv:1805.12471.
Yang, Z.; Dai, Z.; Yang, Y.; Carbonell, J.; Salakhutdinov,
R.; and Le, Q. V. 2019. XLNet: Generalized autoregres-
sive pretraining for language understanding. arXiv preprint
arXiv:1906.08237.
Zhang, Z.; Li, J.; Zhu, P.; and Zhao, H. 2018. Modeling
multi-turn conversation with deep utterance aggregation. In
COLING. arXiv preprint arXiv:1806.09102.
Zhang, Z.; Wu, Y.; Li, Z.; and Zhao, H. 2019. Explicit
In PACLIC
contextual semantics for text comprehension.
33. arXiv preprint arXiv:1809.02794.
Zhang, S.; Zhao, H.; Wu, Y.; Zhang, Z.; Zhou, X.; and
Zhou, X. 2020a. Dual co-matching network for multi-
In AAAI. arXiv preprint
choice reading comprehension.
arXiv:1901.09381.
Zhang, Z.; Wu, Y.; Zhou, J.; Duan, S.; and Zhao, H. 2020b.
SG-Net: Syntax-guided machine reading comprehension. In
AAAI. arXiv preprint arXiv:1908.05147.
Zhao, H.; Chen, W.; and Kit, C. 2009. Semantic dependency
parsing of nombank and propbank: An efﬁcient integrated
approach via a large-scale feature selection. In EMNLP.
Zhao, H.; Zhang, X.; and Kit, C. 2013. Integrative semantic
dependency parsing via efﬁcient large-scale feature selec-
tion. Journal of Artiﬁcial Intelligence Research 46:203–233.
Zhou, J.; Zhang, Z.; and Zhao, H.
LIMIT-
BERT: Linguistic informed multi-task bert. arXiv preprint
arXiv:1910.14296.

2018.
arXiv preprint

2019.

