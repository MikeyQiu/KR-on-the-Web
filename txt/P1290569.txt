Co-attending Free-form Regions and Detections with Multi-modal Multiplicative
Feature Embedding for Visual Question Answering

Pan Lu† Hongsheng Li‡∗ Wei Zhang(cid:92)

Jianyong Wang† Xiaogang Wang‡∗

† Tsinghua National Laboratory for Information Science and Technology (TNList)
Department of Computer Science, Tsinghua University
‡ Department of Electronic Engineering, The Chinese University of Hong Kong
(cid:92) Shanghai Key Laboratory of Trustworthy Computing, East China Normal University

{lupantech, zhangwei.thu2011}@gmail.com, {hsli, xgwang}@ee.cuhk.edu.hk,

jianyong@mail.tsinghua.edu.cn

7
1
0
2
 
c
e
D
 
2
1
 
 
]

V
C
.
s
c
[
 
 
2
v
4
9
7
6
0
.
1
1
7
1
:
v
i
X
r
a

Abstract

Recently, the Visual Question Answering (VQA) task has
gained increasing attention in artiﬁcial intelligence. Exist-
ing VQA methods mainly adopt the visual attention mech-
anism to associate the input question with corresponding
image regions for effective question answering. The free-
form region based and the detection-based visual attention
mechanisms are mostly investigated, with the former ones
attending free-form image regions and the latter ones at-
tending pre-speciﬁed detection-box regions. We argue that
the two attention mechanisms are able to provide comple-
mentary information and should be effectively integrated
to better solve the VQA problem. In this paper, we pro-
pose a novel deep neural network for VQA that integrates
both attention mechanisms. Our proposed framework effec-
tively fuses features from free-form image regions, detec-
tion boxes, and question representations via a multi-modal
multiplicative feature embedding scheme to jointly attend
question-related free-form image regions and detection boxes
for more accurate question answering. The proposed method
is extensively evaluated on two publicly available datasets,
COCO-QA and VQA, and outperforms state-of-the-art ap-
proaches. Source code is available at https://github.
com/lupantech/dual-mfa-vqa.

1

Introduction

In recent years, multi-modal learning for language and vi-
sion has gained much attention in artiﬁcial intelligence.
Great progress has been achieved for different tasks in-
cluding image captioning (Karpathy and Fei-Fei 2015), vi-
sual question generation (Mostafazadeh et al. 2016; Li et
al. 2017b), video question answering (Ye et al. 2017) and
text-to-image retrieval (Xie, Shen, and Zhu 2016; Li et al.
2017a). The Visual Question Answering (VQA) (Antol et
al. 2015) task has recently emerged as a more challenging
task. The algorithms are required to answer natural language
questions about a given image’s contents. Compared with
the conventional visual-language tasks such as image cap-
tioning and text-to-image retrieval, the VQA task requires
the algorithms to have a better understanding on both the
input image and question in order to infer the answer.

∗Co-corresponding authors.

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Figure 1: Co-attending free-form regions and detection
boxes based on the question, the whole image, and detec-
tion boxes for better utilizing complementary information to
solve the VQA task.

State-of-the-art VQA approaches utilize visual attention
mechanism to relate the question to meaningful image re-
gions for accurate question answering. Most visual attention
mechanisms in VQA can be categorized into free-form re-
gion based methods (Lu et al. 2016; Fukui et al. 2016) and
detection-based methods (Li and Jia 2016; Shih, Singh, and
Hoiem 2016). For the free-form region based methods, the
question features learned by a Long Short-Term Memory
(LSTM) network and the image features learned by a Con-
volutional Neural Network (CNN) are fused by either ad-
dictive, multiplicative or concatenation operations at every
image spatial location. The free-form attention map is ob-
tained by applying a softmax non-linearity operation across
the fused feature map. Since there is no restriction on the ob-
tained attention map, the free-form attention region is able
to attend both global visual context and speciﬁc foreground
objects for inferring answers. However, since there is no re-
striction, the free-form attentive regions might focus on par-
tial objects or irrelevant context sometimes. For instance, for
an question, “What animal do you see?”, a free-form re-
gion attention map might mistakenly focus on only a part
of the foreground cat and generate an answer of “dog”. On
the other hand, for the detection-based attention methods,
the attention mechanism is utilized for relating the question
to pre-speciﬁed detection boxes (e.g., Edge boxes (Zitnick

and Doll´ar 2014)). Instead of applying the softmax operation
over all image spatial locations, the operation is calculated
over all detection boxes. Therefore, the attended regions are
restricted to pre-speciﬁed detection-box regions and such
question-related regions could be more effective for answer-
ing questions about foreground objects. However, such re-
strictions also cause challenges for other types of questions.
For instance, for the question “How is the weather today?”,
there might not exist a detection box in the sky, resulting in
a failure in answering this question.

For better understanding the question and image con-
tents and their relations, a good VQA algorithm needs to
identify global scene attributes, locate objects, identify ob-
ject attributes, quantity and categories to make accurate in-
ference. We argue that the above mentioned two types of
attended mechanisms provide complementary information
and should be effectively integrated in a uniﬁed framework
to take advantages of both the attended free-form regions
and attended detection regions. Take the above mentioned
two questions as examples, the question about animal could
be more effectively answered with detection-based attention
maps, while the question about the weather can be better an-
swered with the free-form region based attention maps.

In this paper, we propose a novel dual-branch deep neu-
ral network for solving the VQA problem that combines
free-form region based and detection-based attention mech-
anisms (see Figure 1). The overall framework consists of two
attention branches, each of which associates the question
with the most relevant free-form regions or with the most
relevant detection regions in the input image. For obtaining
more question-related attention weights for both types of re-
gions, we propose to learn the joint feature representation
of the input question, the whole image, and the detection
boxes with a multiplicative feature embedding scheme. Such
a multiplicative scheme does not share parameters between
the two branches and is shown to result in more robust an-
swering performance than existing methods.

The main contributions of our work can be summarized

as twofold.
• We propose a novel dual-branch deep neural network
that effectively integrates the free-form region based and
detection-based attention mechanisms in a uniﬁed frame-
work;

• In order to better fuse features from different modalities,
a novel multiplicative feature embedding scheme is pro-
posed to learn joint feature representations from the ques-
tion, the whole image, and the detection boxes.

2 Related work
State-of-the-art VQA algorithms are mainly based on deep
neural networks for learning visual-question features and for
predicting the ﬁnal answers. Due to the establishment of the
VQA dataset and the online evaluation platform by (Antol et
al. 2015), there is an increasing number of VQA algorithms
proposed every year.

Multi-modal feature embedding for VQA. The existing
joint feature embedding methods for VQA typically com-
bine the visual and question features learned by deep neural

networks and then solve the task as a multi-class classiﬁca-
tion problem. (Zhou et al. 2015) proposed a simple baseline,
which learns image features with CNN and question features
from LSTM, and concatenated these two features to predict
the answer. Instead of using LSTM for learning question
representations, (Noh, Hongsuck Seo, and Han 2016) used
GRU (Cho et al. 2014) and (Ma, Lu, and Li 2016) trained
CNN for question embedding. Different from the above
mentioned methods addressing the VQA task as a classiﬁca-
tion problem, the work by (Malinowski, Rohrbach, and Fritz
2015) fed both image CNN features and question represen-
tations into an LSTM to generate the answer by sequence-to-
sequence learning. There are also high-order approaches for
multi-modal feature embedding. MCB (Fukui et al. 2016)
and MLB (Kim et al. 2017) both designed bilinear pool-
ing approaches to learn multi-modal feature embedding for
VQA. In (Ben-Younes et al. 2017), a generalized multi-
modal pooling framework is proposed, which shows that
MCB and MLB are its special cases. Inspired by ResNet
(He et al. 2016), (Kim et al. 2016) proposed element-wise
multiplication for the joint residual mappings.

Attention mechanism for VQA. A large quantity of re-
cent works focused on incorporating attention mechanisms
for solving the VQA task. In (Xu and Saenko 2016), the at-
tention weights over different image regions are calculated
based on the semantic similarity between the question and
image regions, and the updated question features are ob-
tained as the weighted sum of the different regions. (Yang et
al. 2016) proposed a multi-stage attention framework, which
stacks the attention modules to search question-related im-
age regions by iterative feature fusion. The attention mech-
anism is not limited to image modality, (Lu et al. 2016) pro-
posed a co-attention mechanism that simultaneously attends
both the question and image with joint visual-question fea-
ture representations.

Since some of the questions are related to objects in the
images, object detection results are explored to replace the
visual features obtained from the whole-image region. (Shih,
Singh, and Hoiem 2016) utilized the attention mechanism to
generate visual features from 100 detection boxes for the
VQA task. Similarly, (Li and Jia 2016) proposed a frame-
work that updates the joint visual-question feature embed-
ding by iteratively attending the top detection boxes.

However, all the attention-based methods focus on one
type of image regions for question-image association (i.e.,
either free-form image regions or detection boxes), and each
of them have limitations on solving certain types of ques-
tions. In contrast, in order to better utilize the complemen-
tary information from both types of image regions, our pro-
posed approach effectively integrates both attention mecha-
nisms in a uniﬁed framework.

3 Method
The overall structure of our proposed deep neural network is
illustrated in Figure 2, which takes the question, the whole
image, and the detection boxes as inputs, and learns to as-
sociate questions to free-form image regions and detection
boxes simultaneously to infer the answer. Our proposed
model for VQA consists of two co-attention branches for

Figure 2: Illustration of the overall network structure for solving the VQA task. The network has two attention branches with
the proposed multiplicative featucre embedding scheme, where one branch attends free-form image regions and another branch
attends detection boxes for encoding question-related visual features.

free-form image regions and for detection boxes, respec-
tively. Before calculating the attention weights for each type
of regions, the joint feature representations of the question,
the whole-image visual features, and the detection box vi-
sual features are obtained via a multiplicative approach. In
this way, the attention weights for each type of regions are
learned from all the three inputs. Importantly, the joint multi-
modal feature embedding has different parameters for each
branch, which leads to better answering accuracy. The re-
sulting visual features from the two branches are fused with
the question representation and then added to obtain the ﬁnal
question-image embedding. A multi-class linear classiﬁer is
adopted for obtaining the ﬁnal predicted answer.

The input feature encoding will be introduced in Section
3.1. Section 3.2 introduces the branch with visual attention
mechanism for associating free-form image regions to the
input questions with our multiplicative feature embedding
scheme, and Section 3.3 describes the another branch with
detection based attention mechanism. The ﬁnal answer pre-
diction will be introduced in Section 3.4.

3.1

Input feature encoding

Encoding whole-image features. We utilize an ImageNet
pre-trained ResNet-152 (He et al. 2016) for learning im-
age visual features. The input image to ResNet is resized to
448 × 448 and the 2048 × 14 × 14 output feature map of the
last convolution layer is used to encode the whole-image vi-
sual features R ∈ R2048×14×14, where 14 × 14 is its spatial
size and 2048 is the number of visual feature channels.

Encoding detection-box features. We adopt the Faster-
RCNN (Ren et al. 2015) framework to obtain object detec-
tion boxes in the input image. For all the object proposals
and their associated detection scores generated by Faster-
RCNN, the non-maximum suppression with an intersection
over union (IoU) 0.3 is applied and the top-ranked 19 detec-
tion boxes are chosen as the detection-box features for our
overall framework. The 4096-dimensional visual features of
Faster-RCNN’s fc7 layer concatenated with boxes’ detec-
tion scores are utilized to encode the visual feature of each

detection box. Let D = [f1, · · · , f19] ∈ R4097×19 denote
the visual features of the top-ranked 19 detection boxes.

Encoding question features. The Gated Recurrent Unit
(GRU) (Cho et al. 2014) is adopted to encode the question
features, which show its effectiveness in recent VQA meth-
ods (Lu et al. 2016; Kim et al. 2017). A GRU cell consists
of an update gate z and a reset gate r. Given a question
q = [q1, ..., qT ], where qt is the one-hot vector of at po-
sition t, and T is the length of the question. We convert
each word qt into a feature vector via a linear transforma-
tion xt = Weqt. At each time step, the word feature vector
qt is sequentially fed into the GRU to encode the input ques-
tion. At each step, the GRU updates the update gate zt and
reset gate rt and outputs a hidden state ht. The GRU update
process operates as

(1)
(2)

(3)

zt = σ(Wzxt + Uzht−1 + bz),
rt = σ(Wrxt + Urht−1 + br),
˜ht = tanh(Whxt + Uh(rt ◦ ht−1) + bh),
ht = zt ◦ ht−1 + (1 − zt) ◦ ˜ht,

(4)
where σ represents the sigmoid activation function. The
weight matrices W , U and bias vector b are learnable pa-
rameters of the GRU.

We take the ﬁnal hidden state hT as the question embed-
ding, i.e., Q = hT ∈ Rk, where k denotes the embedding
length. Following (Li and Jia 2016; Kim et al. 2017), we uti-
lize the pre-trained skip-thought model (Kiros et al. 2015) to
initialize the embedding matrix We in our question language
model. Since the skip-thought model is previously trained on
large text corpus, we are able to transfer external language-
based knowledge to our VQA task by ﬁne-tuning the GRU
from the initial point.

3.2 Learning to attend free-form region visual
features with multiplicative embedding
Our proposed framework has two branches, one for attend-
ing question-related free-form image regions to learn whole-
image features and the other for attending detection-box re-
gions to learn question-related detection features. For each

attention branch, it takes question feature Q, whole-image
feature R and detection-box feature D as inputs, and outputs
the question-attended visual features v1 and v2 for question
answering.

Our free-form region attention branch tries to associate
the input question to relevant regions of the input image.
There is no restriction to the attended regions, which are of
free-form and are able to capture the global visual context
and attributes of the image. Unlike existing VQA methods
that fuse question and image features via simple concatena-
tion (Shih, Singh, and Hoiem 2016) or addition (Lu et al.
2016) to guide the calculation of the attention weights, each
of our attention branch fuses all three types of input features
via a multiplicative feature embedding scheme for utilizing
full information from the inputs. The network structure for
attending visual features with our multiplicative feature em-
bedding scheme is show in Figure 3(a).

Given the question embedding Q ∈ Rk, the whole-image
representation R ∈ R2048×14×14, and the detection repre-
sentation D ∈ R4097×19, we ﬁrst embed them to a 1200-
dimensional common space via the following equations,

R1 = tanh(Wr1R + br1),

D1 =

· 1(tanh(Wd1D + bd1)T ),

1
19

(5)

(6)

Q1 = tanh(Wq1 Q + bq1 ),

(7)
where Wr1 ∈ R1200×2408, Wd1 ∈ R1200×4097, Wq1 ∈
R1200×k are the learnable weight parameters, br1 , bd1 , bq1
∈ R1200 are the bias parameters, 1 ∈ R19 represents an all-
1 vector, and tanh is the hyperbolic tangent function. For
learning the attention weights for the whole-image feature
R, the transformed detection features are averaged across
all detection boxes following Eq. (6) before feature fusion.
After mapping all input features into the 1200-dimensional
common space, the detection feature D1 ∈ R1200 and ques-
tion feature Q1 ∈ R1200, are spatially replicated to a 14×14
grid to form ˜D1 and ˜R1, which match the spatial size of the
whole-image feature R1 ∈ R1200×14×14.

The joint feature representation C1 of the three inputs is
obtained by element-wise multiplication (Hadamard prod-
uct) of ˜Q1, R1 and ˜D1, and followed by a L2 normalization
to constrain the magnitude of the representation,

C1 = Norm2( ˜Q1 ◦ R1 ◦ ˜D1),
(8)
where ◦ indicates element-wise multiplication. The free-
form attention map is then obtained by convolving the joint
feature representation C1 with a 1 × 1 convolution followed
by a softmax operation over the 14 × 14 grid,
a1 = softmax(Wc1 ∗ C1 + bc1),
(9)
where Wc1 ∈ R1200×1×1 and bc1 ∈ R1200 are the learnable
convolution kernel parameters. The attended whole-image
feature over all spatial locations can be calculated by

v1 =

a1(i)R1(i),

(10)

14×14
(cid:88)

i

which can represent the attended whole-image visual feature
that are most related to the input question.

Figure 3: Learning to attend visual features with multi-
modal multiplicative feature embedding for (a) free-form
image regions and for (b) detection boxes.

3.3 Learning to attend detection-box visual
features with multiplicative embedding

Our second branch focuses on learning question-related de-
tection features by attending the detection-box features with
joint input feature representation. Similar to the ﬁrst branch,
we fuse the question representation, the whole-image fea-
tures, and the detection-box features for learning the at-
tention weights for detection boxes. Unlike previous detec-
tion based attention methods (Shih, Singh, and Hoiem 2016;
Li and Jia 2016), our proposed attention mechanism in-
tegrates whole-image features for better understanding the
overall image contents. The structure for learning the joint
feature representation is shown in Figure 3(b).

Similar to the joint representation for free-form image re-
gion attention, given the question embedding Q ∈ Rk, the
image representation R ∈ R2048×14×14, the detection rep-
resentation D ∈ R4097×19, we transform each representa-
tion to a common semantic space, and then obtain the 1200-
dimensional joint feature representation C2 as

D2 = tanh(Wd2 D + bd2),

R2 =

· 1(tanh(Wr2 R + br2 )T ),

1
196

Q2 = tanh(Wq2 Q + bq2 ),
C2 = Norm2( ˜Q2 ◦ ˜R2 ◦ D2),

(11)

(12)

(13)

(14)

where Wd2, Wr2, Wq2 , bd1 , br1, bq2 are the learnable pa-
rameters for linear transformation. The transformed ques-
tion feature Q2 and whole-image feature R2 are replicated
across the number dimension to match the dimension of the
transformed detection features D2 ∈ R1200×19 before cal-
culating the joint representation C2 following Eq. (14).

The ﬁnal attended detection representation v2 over all de-

tection boxes is deﬁned as

a2 = softmax(Wc2C2 + bc2 ),

v2 =

a2(i)D2(i),

19
(cid:88)

i

(15)

(16)

where Wc2 ∈ R19×1200 and bc2 ∈ R19 are the learnable
parameters of learning the attention weights a2 for the de-
tection boxes.

3.4 Learning for answer prediction
Similar to existing VQA approaches (Antol et al. 2015; Lu et
al. 2016; Fukui et al. 2016), we model VQA as a multi-class
classiﬁcation problem. Given the attended whole-image fea-
ture v1, attended detection feature v2 with the input ques-
tion feature Q, question-image joint encodings are obtained
by element-wise multiplication of the transformed question
features and the attended features from both branch,

hr = v1 ◦ tanh(WhrQ + bhr),
hd = v2 ◦ tanh(WhdQ + bhd),

(17)
(18)

where Whr, Whd, bhr, bhd are the learnable parameters for
transforming the input question feature Q. The reason why
we choose different question transformation parameters for
the two branches is that the attended visual features from the
two different branches captures different information from
the input image. One is from attended free-form region fea-
tures and is able to capture the global context and attributes
of the scene. The another is from attended detection features
and is able to extract information about foreground objects.
After merging question-image encodings from the two
branches via addition, a linear classiﬁer is trained for ﬁnal
answer prediction,

pans = softmax(Wp(hr + hd) + bp),

(19)

where Wp and bp are the classiﬁer parameters, and pans rep-
resents the probability of the ﬁnal answer prediction.

4 Experiments

4.1 Datasets and evaluation metrics
We evaluate the proposed model on two public datasets, the
VQA (Antol et al. 2015) and COCO-QA (Ren, Kiros, and
Zemel 2015) datasets.

VQA dataset (Antol et al. 2015) is based on Microsoft
COCO image data (Lin et al. 2014). The dataset con-
sists of 248,349 training questions, 121,512 validation ques-
tions and 244,302 testing questions, generated on a total of
123,287 images. There are three types of questions includ-
ing yes/no, number and other. For each question, 10 free-
response answers are provided. We take the top 2000 most
frequent answers as the candidate outputs for learning the
classiﬁcation model similar to (Kim et al. 2016), which cov-
ers 90.45% answers in the training and validation sets.

COCO-QA dataset (Ren, Kiros, and Zemel 2015) is cre-
ated based on the caption annotations of Microsoft COCO
dataset (Lin et al. 2014). There are 78,736 training samples
and 38,948 testing samples, respectively. It contains four

types of questions, object, number, color and location. All
of the answers are single-words and considered as the valid
answers, namely 430 answers, which are used for the possi-
ble answer classiﬁcation.

Since we formulate VQA as a classiﬁcation task, we use
the accuracy metric to measure the performances of dif-
ferent models on both two datasets. In particular, for the
VQA dataset, a predicted answer is regarded as correct if it
matches more than three in ten ground truth answers. WUPS
calculates the similarity between two words based on their
common subsequence in the taxonomy tree. In addition, Wu-
Palmer similarity (WUPS) (Wu and Palmer 1994) is also
reported for the COCO-QA dataset. Same as the previous
work (Lu et al. 2016; Li and Jia 2016), we report the WUPS
scores with the thresholds of 0.9 and 0.0.

Implementation details

4.2
For encoding questions, the length of questions is ﬁxed to 26
and each word embedding is a vector of size 620. The hid-
den state of GRU is set to 2400. Given the question, image
and detection representations, the joint feature embedding of
these inputs h is set as 1200. Following (Kim et al. 2017),
we take two glimpses of each attention map, that is to say,
for each branch, another set of attention weights a1 is trained
for whole-image features R1 in the ﬁrst branch and another
set of a2 for detection features D2 in the second branch. The
two sets of attended features are concatenated in each branch
as the ﬁnal feature.

We implement our model with the Torch library. The RM-
SProp method is used for training our network with an initial
learning rate of 3×10−4, a momentum of 0.99 and a weight-
decay of 10−8. The batch size is set to 300, and is trained for
250,000 iterations. The validation process is performed ev-
ery 10,000 iterations with early stopping when validation ac-
curacy stops improving for more than 5 validations. Dropout
is applied after every linear transformation and gradient clip-
ping techniques are used for regularization in GRU training.
Multi-GPU parallel technology is adopted to accelerate the
training process.

4.3 Comparison with state-of-the-arts
Table 1 shows results on the VQA test set for both open-
ended and multiple-choice tasks by our proposed approach
and compared methods. The approaches shown in Table 1
are trained on the train+val split of the VQA dataset and
evaluated on the test split, where the test-dev set is normally
used for validation and the test-std set for standard testing.
The models in the ﬁrst part of Table 1 are based on sim-
ple joint question-image feature embedding methods. Mod-
els in the second part of Table 1 employ detection-based
attention mechanism and models in the third part use free-
form region-based attention mechanism. We only compare
results of the single models since most approaches do not
adopt the model ensemble strategy. We can see that our ﬁ-
nal model (denoted as Dual-MFA, where MFA stands for
Multiplicative Feature Attention) improves the state-of-the-
art MLB approach (Kim et al. 2017) from 65.07% to 66.09%
for the open-ended task, and from 68.89% to 69.97% for the
multiple-choice task on the test-std set. Speciﬁcally, in the

MC

All

57.57
61.97
62.69
64.18
-

62.43
65.43

-
-
66.33
66.07
67.80
-
68.89

-
69.97

36.80
42.62
42.24
46.10
48.33

-
-

43.48
46.42
49.41
51.95
53.20
-
54.77

-
56.89

59.01
56.34

59.18
59.01
58.93

59.82
58.35
58.53

55.57
53.99
54.89

59.18
57.40
58.16

59.07
59.82

Validation

Method

All

Y/N

Num. Other

All

Y/N

Num. Other

Test-dev

Open-Ended

Test-std

Open-Ended

LSTM Q+I (Antol et al. 2015)
iBOWING (Zhou et al. 2015)
DPPnet (Noh, Hongsuck Seo, and Han 2016)
FDA (Ilievski, Yan, and Feng 2016)
DMN+ (Xiong, Merity, and Socher 2016)

Region Sel. (Shih, Singh, and Hoiem 2016)
QRU (Li and Jia 2016)

SMem (Xu and Saenko 2016)
SAN (Yang et al. 2016)
MRN (Kim et al. 2016)
HieCoAtt (Lu et al. 2016)
VQA-Machine (Wang et al. 2017)
MCB (Fukui et al. 2016)
MLB (Kim et al. 2017)

Dual-MLB (our baseline)
Dual-MFA (ours)

53.74
55.72
57.22
59.24
60.30

-
60.72

57.99
58.70
61.68
61.80
63.10
64.70
64.89

65.12
66.01

78.94
76.55
80.71
81.14
80.50

-
82.29

80.87
79.30
82.28
79.70
81.50
82.50
84.13

83.32
83.59

35.24
35.03
37.24
36.16
36.80

-
37.02

37.32
36.60
38.82
38.70
38.40
37.60
37.85

39.96
40.18

36.42
42.62
41.69
45.77
48.30

-
47.67

43.12
46.10
49.25
51.70
53.00
55.60
54.57

55.26
56.84

MC

All

57.17
61.68
62.48
64.01
-

62.44
65.43

-
-
66.15
65.80
67.70
69.10
-

69.55
70.04

54.06
55.89
57.36
59.54
60.36

-
60.76

58.24
58.85
61.84
62.06
63.30
-
65.07

-
66.09

79.01
76.76
80.28
81.34
80.43

-
-

80.80
79.11
82.39
79.95
81.40
-
84.02

-
83.37

35.55
34.98
36.92
35.67
36.82

-
-

37.53
36.41
38.23
38.22
38.20
-
37.90

-
40.39

Table 1: Evaluation results by our proposed method and compared methods on the VQA dataset.

question types of number and other, our proposed approach
brings 2.49% and 2.12% improvements on the test-std set.
QRU (Li and Jia 2016) is the state-of-the-art detection-based
attention method, and our model signiﬁcantly outperforms it
by 5.33% on the test-std set. We also compare our approach
with a baseline model Dual-MLB, which consists of two at-
tention branches based on the MLB tensor fusion module.
The baseline model fuses the question and free-form image
region embedding by MLB in one branch, and fuses ques-
tion and image detection embedding by MLB in another
branch. The baseline Dual-MLB also outperforms MLB by
0.23% on the test-dev set, which shows that our improve-
ments are not only from the integration of the two attention
mechanisms, but are also caused by effective joint feature
embedding of question, image, and detection features.

Table 3 compares our approach with the state-of-the-art
approaches on the COCO-QA test set. Our ﬁnal model Dual-
MFA improves the state-of-the-art HieCoAtt (Lu et al. 2016)
from 65.40% to 66.49%. In particular, our model achieves
an improvement of 2.99% for the question type color. Sim-
ilar to the results on the VQA dataset, our model signif-
icantly outperforms the state-of-the-art detection-based at-
tention method QRU by 3.49%.

4.4 Ablation study

In this section, we conduct ablation experiments to study ef-
fectiveness of individual component designs in our model.
Table 2 shows the results of baseline models replacing dif-
ferent components in our model, which are trained on the
VQA training set and tested on the validation set. Following
other compared approaches, the test set is not used in the
study due to online submission restrictions. Speciﬁcally, we
compare different multi-modal feature embedding designs
and investigate the roles of the two attention mechanisms.

Method

MFA-MUL*
MFA-ADD

MFA-Norm*
MFA w/o Norm
MFA-Power

Dual-MFA-ADD*
Dual-MFA-MUL
Dual-MFA-CAT

MFA-D*
QRU-D (Li and Jia 2016)
MLB-D (Kim et al. 2017)

MFA-R*
MLB-R (Kim et al. 2017)
MUTAN (Ben-Younes et al. 2017)

Dual-MLB
Dual-MFA* (full model)

Table 2: Ablation study on the VQA dataset, where “*” de-
notes our model’s design.

The ﬁrst part of Table 2 shows different element-wise op-
erations used for joint embedding of three input features.
Element-wise multiplication (denoted as MFA-MUL) in Eq.
(8) performs better than element-wise addition (denoted as
MFA-ADD) by 3.67%. The second part of Table 2 shows L2
normalization (denoted as MFA-Norm) in joint feature em-
bedding works better than the model with unsigned power
operation of 1/3 (denoted as MFA-Power) and the model
without normalization (denoted as MFA w/o Norm). For
fusing outputs hr and hd from the two attention branches,
element-wise addition in Eq. (19) achieves better perfor-

Method

All

Obj.

Num. Color

Loc. WUPS0.9 WUPS0.0

2VIS+BLSTM (Ren, Kiros, and Zemel 2015)
IMG-CNN (Ma, Lu, and Li 2016)
DDPnet (Noh, Hongsuck Seo, and Han 2016)
SAN (Yang et al. 2016)
QRU (Li and Jia 2016)
HieCoAtt (Lu et al. 2016)

55.09
58.40
61.16
61.60
62.50
65.40

58.17
-
-
65.40
65.06
68.00

44.79
-
-
48.60
46.90
51.00

49.53
-
-
57.90
60.50
62.90

47.34
-
-
54.00
56.99
58.80

Dual-MFA (ours)

66.49

68.86

51.32

65.89

58.92

65.34
68.50
70.84
71.60
72.58
75.10

76.15

88.64
89.67
90.61
90.90
91.62
92.00

92.29

Table 3: Evaluation results by our proposed method and compared methods on the COCO QA dataset.

Q: What sport is this?
A: tennis

Q: What is the color of the surfboard?
A: white

Q: Is it a sunny day?
A: yes

Q: How many giraffes are there?
A: 5

Q: What does the red circle sign mean?
A: no parking

(a)

(b)

(c)

(d)

(e)

Figure 4: Visualization examples on the VQA test-dev set. (First row) input images. (Second row) free-form region based
attention maps. (Third row) detection-based attention maps.

mance than element-wise multiplication (denoted as Dual-
MFA-MUL) and concatenation of two vectors (denoted as
Dual-MFA-CAT).

The third and fourth parts of Table 2 compare our multi-
modal multiplicative feature embedding for a single atten-
tion mechanism (MFA-R or MFA-D) with detection-based
attention baseline models (QRU-D and MLB-D) and region-
based attention baseline models (MUTAN and MLB-R),
which replace our multiplicative feature embedding with
QRU (Li and Jia 2016) or MLB (Kim et al. 2017) respec-
tively. The MUTAN (Ben-Younes et al. 2017) approach
is also used for comparison. Results show that our mod-
els with a single attention branch outperform all com-
pared detection-based and region-based baselines. Finally,
we compare our ﬁnal model Dual-MFA with the base-
line model Dual-MLB. Our Dual-MFA model beneﬁts from
its effective multi-modal multiplicative feature embedding
scheme and achieves an improvement of 0.75%.

The parameter size of our ﬁnal full model is 82.6M, while
the best region-based model (MFA-R) is 61.7M, and the
best detection-based model (MFA-D) is 56.8M. As the pa-
rameters in the language model and the classiﬁer are shared

by two attention branches, parameters of our full model in-
crease in a minor degree.

4.5 Qualitative evaluation
We visualize some co-attention maps generated by our
model in Figure 4 and present ﬁve examples from the
VQA test set. Figures 4(a) and (b) show that our model at-
tends to the corresponding image regions with two attention
branches, which leads to correct answers with higher conﬁ-
dence. There are also cases where only one attention branch
is able to attend the correct image region to obtain the cor-
rect answer. In Figure 4(c), the free-form region based at-
tention map is able to attend the blue sky and dry ground,
while the detection-based attention map fails as there are no
pre-given detection boxes covering these image regions. In
Figure 4(d), the detection-based attention map has higher
weights on all ﬁve giraffes to generate the correct answer,
while the free-form attention map attends incorrect regions.
A failure case is also presented in Figure 4(e). The model
fails to generate the correct answer as the classiﬁcation label
“no turning right” does not exit in the training set despite
attending the correct image region.

5 Conclusion
In this paper, we propose a novel deep neural network
with co-attention mechanism for visual question answering.
The deep model contains two branches for visual attention
which aim to select the free-form image regions and detec-
tion boxes most related to the input question. We generate
visual attention weights by a novel multiplicative embed-
ding scheme, which fuses the question, whole-image and
detection-box features effectively. Ablation study demon-
strates the effectiveness of individual components of our
proposed model. Experimental results on two large VQA
datasets show that our proposed model outperforms state-
of-the-art approaches.

Acknowledgement
This work was supported in part by National Natural
Science Foundation of China under Grant No. 61532010
and No. 61702190, National Basic Research Program of
China (973 Program) under Grant No. 2014CB340505,
in part by SenseTime Group
and SHMEC (16CG24),
Limited, in part by the General Research Fund sponsored
by the Research Grants Council of Hong Kong (Nos.
CUHK14205615,
CUHK14206114,
CUHK14213616,
CUHK14239816,
CUHK14203015,
CUHK419412,
CUHK14207814), in part by the Hong Kong Innovation and
Technology Support Programme Grant ITS/121/15FX. We
also thank Tong Xiao, Kun Wang and Kai Kang for helpful
discussions.

References
Antol, S.; Agrawal, A.; Lu, J.; Mitchell, M.; Batra, D.;
Lawrence Zitnick, C.; and Parikh, D. 2015. Vqa: Visual ques-
tion answering. In ICCV.
Ben-Younes, H.; Cad`ene, R.; Thome, N.; and Cord, M. 2017.
Mutan: Multimodal tucker fusion for visual question answer-
ing. In ICCV.
Cho, K.; van Merri¨enboer, B.; G¨ulc¸ehre, C¸ .; Bahdanau, D.;
Bougares, F.; Schwenk, H.; and Bengio, Y. 2014. Learning
phrase representations using rnn encoder–decoder for statisti-
cal machine translation. In EMNLP.
Fukui, A.; Park, D. H.; Yang, D.; Rohrbach, A.; Darrell, T.; and
Rohrbach, M. 2016. Multimodal compact bilinear pooling for
visual question answering and visual grounding. In EMNLP.
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual
learning for image recognition. In CVPR.
Ilievski, I.; Yan, S.; and Feng, J. 2016. A focused dynamic
attention model for visual question answering. arXiv preprint
arXiv:1604.01485.
Karpathy, A., and Fei-Fei, L. 2015. Deep visual-semantic align-
ments for generating image descriptions. In CVPR.
Kim, J.-H.; Lee, S.-W.; Kwak, D.; Heo, M.-O.; Kim, J.; Ha, J.-
W.; and Zhang, B.-T. 2016. Multimodal residual learning for
visual qa. In NIPS.
Kim, J.-H.; On, K. W.; Lim, W.; Kim, J.; Ha, J.-W.; and Zhang,
B.-T. 2017. Hadamard product for low-rank bilinear pooling.
In ICLR.

Kiros, R.; Zhu, Y.; Salakhutdinov, R. R.; Zemel, R.; Urtasun,
R.; Torralba, A.; and Fidler, S. 2015. Skip-thought vectors. In
NIPS.
Li, R., and Jia, J. 2016. Visual question answering with question
representation update (qru). In NIPS.
Li, S.; Xiao, T.; Li, H.; Zhou, B.; Yue, D.; and Wang, X. 2017a.
Person search with natural language description. In CVPR.
Li, Y.; Duan, N.; Zhou, B.; Chu, X.; Ouyang, W.; and Wang,
X. 2017b. Visual question generation as dual task of visual
question answering. arXiv preprint arXiv:1709.07192.
Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-
manan, D.; Doll´ar, P.; and Zitnick, C. L. 2014. Microsoft coco:
Common objects in context. In ECCV.
Lu, J.; Yang, J.; Batra, D.; and Parikh, D. 2016. Hierarchical
question-image co-attention for visual question answering. In
NIPS.
Ma, L.; Lu, Z.; and Li, H. 2016. Learning to answer questions
from image using convolutional neural network. In AAAI.
Malinowski, M.; Rohrbach, M.; and Fritz, M. 2015. Ask your
neurons: A neural-based approach to answering questions about
images. In ICCV.
Mostafazadeh, N.; Misra, I.; Devlin, J.; Mitchell, M.; He, X.;
and Vanderwende, L. 2016. Generating natural questions about
an image. In ACL.
Noh, H.; Hongsuck Seo, P.; and Han, B. 2016. Image question
answering using convolutional neural network with dynamic
parameter prediction. In CVPR.
Ren, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster r-cnn: To-
wards real-time object detection with region proposal networks.
In NIPS.
Ren, M.; Kiros, R.; and Zemel, R. 2015. Exploring models and
data for image question answering. In NIPS.
Shih, K. J.; Singh, S.; and Hoiem, D. 2016. Where to look:
Focus regions for visual question answering. In CVPR.
Wang, P.; Wu, Q.; Shen, C.; and Hengel, A. v. d. 2017. The
vqa-machine: Learning how to use existing vision algorithms
to answer new questions. In CVPR.
Wu, Z., and Palmer, M. 1994. Verbs semantics and lexical
selection. In ACL.
Xie, L.; Shen, J.; and Zhu, L. 2016. Online cross-modal hashing
for web image retrieval. In AAAI.
Xiong, C.; Merity, S.; and Socher, R. 2016. Dynamic memory
networks for visual and textual question answering. In ICML.
Xu, H., and Saenko, K. 2016. Ask, attend and answer: Ex-
ploring question-guided spatial attention for visual question an-
swering. In ECCV.
Yang, Z.; He, X.; Gao, J.; Deng, L.; and Smola, A. 2016.
Stacked attention networks for image question answering. In
CVPR.
Ye, Y.; Zhao, Z.; Li, Y.; Chen, L.; Xiao, J.; and Zhuang, Y.
2017. Video question answering via attribute-augmented at-
tention network learning. In ACM Multimedia.
Zhou, B.; Tian, Y.; Sukhbaatar, S.; Szlam, A.; and Fergus, R.
2015. Simple baseline for visual question answering. arXiv
preprint arXiv:1512.02167.
Zitnick, C. L., and Doll´ar, P. 2014. Edge boxes: Locating object
proposals from edges. In ECCV.

Co-attending Free-form Regions and Detections with Multi-modal Multiplicative
Feature Embedding for Visual Question Answering

Pan Lu† Hongsheng Li‡∗ Wei Zhang(cid:92)

Jianyong Wang† Xiaogang Wang‡∗

† Tsinghua National Laboratory for Information Science and Technology (TNList)
Department of Computer Science, Tsinghua University
‡ Department of Electronic Engineering, The Chinese University of Hong Kong
(cid:92) Shanghai Key Laboratory of Trustworthy Computing, East China Normal University

{lupantech, zhangwei.thu2011}@gmail.com, {hsli, xgwang}@ee.cuhk.edu.hk,

jianyong@mail.tsinghua.edu.cn

7
1
0
2
 
c
e
D
 
2
1
 
 
]

V
C
.
s
c
[
 
 
2
v
4
9
7
6
0
.
1
1
7
1
:
v
i
X
r
a

Abstract

Recently, the Visual Question Answering (VQA) task has
gained increasing attention in artiﬁcial intelligence. Exist-
ing VQA methods mainly adopt the visual attention mech-
anism to associate the input question with corresponding
image regions for effective question answering. The free-
form region based and the detection-based visual attention
mechanisms are mostly investigated, with the former ones
attending free-form image regions and the latter ones at-
tending pre-speciﬁed detection-box regions. We argue that
the two attention mechanisms are able to provide comple-
mentary information and should be effectively integrated
to better solve the VQA problem. In this paper, we pro-
pose a novel deep neural network for VQA that integrates
both attention mechanisms. Our proposed framework effec-
tively fuses features from free-form image regions, detec-
tion boxes, and question representations via a multi-modal
multiplicative feature embedding scheme to jointly attend
question-related free-form image regions and detection boxes
for more accurate question answering. The proposed method
is extensively evaluated on two publicly available datasets,
COCO-QA and VQA, and outperforms state-of-the-art ap-
proaches. Source code is available at https://github.
com/lupantech/dual-mfa-vqa.

1

Introduction

In recent years, multi-modal learning for language and vi-
sion has gained much attention in artiﬁcial intelligence.
Great progress has been achieved for different tasks in-
cluding image captioning (Karpathy and Fei-Fei 2015), vi-
sual question generation (Mostafazadeh et al. 2016; Li et
al. 2017b), video question answering (Ye et al. 2017) and
text-to-image retrieval (Xie, Shen, and Zhu 2016; Li et al.
2017a). The Visual Question Answering (VQA) (Antol et
al. 2015) task has recently emerged as a more challenging
task. The algorithms are required to answer natural language
questions about a given image’s contents. Compared with
the conventional visual-language tasks such as image cap-
tioning and text-to-image retrieval, the VQA task requires
the algorithms to have a better understanding on both the
input image and question in order to infer the answer.

∗Co-corresponding authors.

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Figure 1: Co-attending free-form regions and detection
boxes based on the question, the whole image, and detec-
tion boxes for better utilizing complementary information to
solve the VQA task.

State-of-the-art VQA approaches utilize visual attention
mechanism to relate the question to meaningful image re-
gions for accurate question answering. Most visual attention
mechanisms in VQA can be categorized into free-form re-
gion based methods (Lu et al. 2016; Fukui et al. 2016) and
detection-based methods (Li and Jia 2016; Shih, Singh, and
Hoiem 2016). For the free-form region based methods, the
question features learned by a Long Short-Term Memory
(LSTM) network and the image features learned by a Con-
volutional Neural Network (CNN) are fused by either ad-
dictive, multiplicative or concatenation operations at every
image spatial location. The free-form attention map is ob-
tained by applying a softmax non-linearity operation across
the fused feature map. Since there is no restriction on the ob-
tained attention map, the free-form attention region is able
to attend both global visual context and speciﬁc foreground
objects for inferring answers. However, since there is no re-
striction, the free-form attentive regions might focus on par-
tial objects or irrelevant context sometimes. For instance, for
an question, “What animal do you see?”, a free-form re-
gion attention map might mistakenly focus on only a part
of the foreground cat and generate an answer of “dog”. On
the other hand, for the detection-based attention methods,
the attention mechanism is utilized for relating the question
to pre-speciﬁed detection boxes (e.g., Edge boxes (Zitnick

and Doll´ar 2014)). Instead of applying the softmax operation
over all image spatial locations, the operation is calculated
over all detection boxes. Therefore, the attended regions are
restricted to pre-speciﬁed detection-box regions and such
question-related regions could be more effective for answer-
ing questions about foreground objects. However, such re-
strictions also cause challenges for other types of questions.
For instance, for the question “How is the weather today?”,
there might not exist a detection box in the sky, resulting in
a failure in answering this question.

For better understanding the question and image con-
tents and their relations, a good VQA algorithm needs to
identify global scene attributes, locate objects, identify ob-
ject attributes, quantity and categories to make accurate in-
ference. We argue that the above mentioned two types of
attended mechanisms provide complementary information
and should be effectively integrated in a uniﬁed framework
to take advantages of both the attended free-form regions
and attended detection regions. Take the above mentioned
two questions as examples, the question about animal could
be more effectively answered with detection-based attention
maps, while the question about the weather can be better an-
swered with the free-form region based attention maps.

In this paper, we propose a novel dual-branch deep neu-
ral network for solving the VQA problem that combines
free-form region based and detection-based attention mech-
anisms (see Figure 1). The overall framework consists of two
attention branches, each of which associates the question
with the most relevant free-form regions or with the most
relevant detection regions in the input image. For obtaining
more question-related attention weights for both types of re-
gions, we propose to learn the joint feature representation
of the input question, the whole image, and the detection
boxes with a multiplicative feature embedding scheme. Such
a multiplicative scheme does not share parameters between
the two branches and is shown to result in more robust an-
swering performance than existing methods.

The main contributions of our work can be summarized

as twofold.
• We propose a novel dual-branch deep neural network
that effectively integrates the free-form region based and
detection-based attention mechanisms in a uniﬁed frame-
work;

• In order to better fuse features from different modalities,
a novel multiplicative feature embedding scheme is pro-
posed to learn joint feature representations from the ques-
tion, the whole image, and the detection boxes.

2 Related work
State-of-the-art VQA algorithms are mainly based on deep
neural networks for learning visual-question features and for
predicting the ﬁnal answers. Due to the establishment of the
VQA dataset and the online evaluation platform by (Antol et
al. 2015), there is an increasing number of VQA algorithms
proposed every year.

Multi-modal feature embedding for VQA. The existing
joint feature embedding methods for VQA typically com-
bine the visual and question features learned by deep neural

networks and then solve the task as a multi-class classiﬁca-
tion problem. (Zhou et al. 2015) proposed a simple baseline,
which learns image features with CNN and question features
from LSTM, and concatenated these two features to predict
the answer. Instead of using LSTM for learning question
representations, (Noh, Hongsuck Seo, and Han 2016) used
GRU (Cho et al. 2014) and (Ma, Lu, and Li 2016) trained
CNN for question embedding. Different from the above
mentioned methods addressing the VQA task as a classiﬁca-
tion problem, the work by (Malinowski, Rohrbach, and Fritz
2015) fed both image CNN features and question represen-
tations into an LSTM to generate the answer by sequence-to-
sequence learning. There are also high-order approaches for
multi-modal feature embedding. MCB (Fukui et al. 2016)
and MLB (Kim et al. 2017) both designed bilinear pool-
ing approaches to learn multi-modal feature embedding for
VQA. In (Ben-Younes et al. 2017), a generalized multi-
modal pooling framework is proposed, which shows that
MCB and MLB are its special cases. Inspired by ResNet
(He et al. 2016), (Kim et al. 2016) proposed element-wise
multiplication for the joint residual mappings.

Attention mechanism for VQA. A large quantity of re-
cent works focused on incorporating attention mechanisms
for solving the VQA task. In (Xu and Saenko 2016), the at-
tention weights over different image regions are calculated
based on the semantic similarity between the question and
image regions, and the updated question features are ob-
tained as the weighted sum of the different regions. (Yang et
al. 2016) proposed a multi-stage attention framework, which
stacks the attention modules to search question-related im-
age regions by iterative feature fusion. The attention mech-
anism is not limited to image modality, (Lu et al. 2016) pro-
posed a co-attention mechanism that simultaneously attends
both the question and image with joint visual-question fea-
ture representations.

Since some of the questions are related to objects in the
images, object detection results are explored to replace the
visual features obtained from the whole-image region. (Shih,
Singh, and Hoiem 2016) utilized the attention mechanism to
generate visual features from 100 detection boxes for the
VQA task. Similarly, (Li and Jia 2016) proposed a frame-
work that updates the joint visual-question feature embed-
ding by iteratively attending the top detection boxes.

However, all the attention-based methods focus on one
type of image regions for question-image association (i.e.,
either free-form image regions or detection boxes), and each
of them have limitations on solving certain types of ques-
tions. In contrast, in order to better utilize the complemen-
tary information from both types of image regions, our pro-
posed approach effectively integrates both attention mecha-
nisms in a uniﬁed framework.

3 Method
The overall structure of our proposed deep neural network is
illustrated in Figure 2, which takes the question, the whole
image, and the detection boxes as inputs, and learns to as-
sociate questions to free-form image regions and detection
boxes simultaneously to infer the answer. Our proposed
model for VQA consists of two co-attention branches for

Figure 2: Illustration of the overall network structure for solving the VQA task. The network has two attention branches with
the proposed multiplicative featucre embedding scheme, where one branch attends free-form image regions and another branch
attends detection boxes for encoding question-related visual features.

free-form image regions and for detection boxes, respec-
tively. Before calculating the attention weights for each type
of regions, the joint feature representations of the question,
the whole-image visual features, and the detection box vi-
sual features are obtained via a multiplicative approach. In
this way, the attention weights for each type of regions are
learned from all the three inputs. Importantly, the joint multi-
modal feature embedding has different parameters for each
branch, which leads to better answering accuracy. The re-
sulting visual features from the two branches are fused with
the question representation and then added to obtain the ﬁnal
question-image embedding. A multi-class linear classiﬁer is
adopted for obtaining the ﬁnal predicted answer.

The input feature encoding will be introduced in Section
3.1. Section 3.2 introduces the branch with visual attention
mechanism for associating free-form image regions to the
input questions with our multiplicative feature embedding
scheme, and Section 3.3 describes the another branch with
detection based attention mechanism. The ﬁnal answer pre-
diction will be introduced in Section 3.4.

3.1

Input feature encoding

Encoding whole-image features. We utilize an ImageNet
pre-trained ResNet-152 (He et al. 2016) for learning im-
age visual features. The input image to ResNet is resized to
448 × 448 and the 2048 × 14 × 14 output feature map of the
last convolution layer is used to encode the whole-image vi-
sual features R ∈ R2048×14×14, where 14 × 14 is its spatial
size and 2048 is the number of visual feature channels.

Encoding detection-box features. We adopt the Faster-
RCNN (Ren et al. 2015) framework to obtain object detec-
tion boxes in the input image. For all the object proposals
and their associated detection scores generated by Faster-
RCNN, the non-maximum suppression with an intersection
over union (IoU) 0.3 is applied and the top-ranked 19 detec-
tion boxes are chosen as the detection-box features for our
overall framework. The 4096-dimensional visual features of
Faster-RCNN’s fc7 layer concatenated with boxes’ detec-
tion scores are utilized to encode the visual feature of each

detection box. Let D = [f1, · · · , f19] ∈ R4097×19 denote
the visual features of the top-ranked 19 detection boxes.

Encoding question features. The Gated Recurrent Unit
(GRU) (Cho et al. 2014) is adopted to encode the question
features, which show its effectiveness in recent VQA meth-
ods (Lu et al. 2016; Kim et al. 2017). A GRU cell consists
of an update gate z and a reset gate r. Given a question
q = [q1, ..., qT ], where qt is the one-hot vector of at po-
sition t, and T is the length of the question. We convert
each word qt into a feature vector via a linear transforma-
tion xt = Weqt. At each time step, the word feature vector
qt is sequentially fed into the GRU to encode the input ques-
tion. At each step, the GRU updates the update gate zt and
reset gate rt and outputs a hidden state ht. The GRU update
process operates as

(1)
(2)

(3)

zt = σ(Wzxt + Uzht−1 + bz),
rt = σ(Wrxt + Urht−1 + br),
˜ht = tanh(Whxt + Uh(rt ◦ ht−1) + bh),
ht = zt ◦ ht−1 + (1 − zt) ◦ ˜ht,

(4)
where σ represents the sigmoid activation function. The
weight matrices W , U and bias vector b are learnable pa-
rameters of the GRU.

We take the ﬁnal hidden state hT as the question embed-
ding, i.e., Q = hT ∈ Rk, where k denotes the embedding
length. Following (Li and Jia 2016; Kim et al. 2017), we uti-
lize the pre-trained skip-thought model (Kiros et al. 2015) to
initialize the embedding matrix We in our question language
model. Since the skip-thought model is previously trained on
large text corpus, we are able to transfer external language-
based knowledge to our VQA task by ﬁne-tuning the GRU
from the initial point.

3.2 Learning to attend free-form region visual
features with multiplicative embedding
Our proposed framework has two branches, one for attend-
ing question-related free-form image regions to learn whole-
image features and the other for attending detection-box re-
gions to learn question-related detection features. For each

attention branch, it takes question feature Q, whole-image
feature R and detection-box feature D as inputs, and outputs
the question-attended visual features v1 and v2 for question
answering.

Our free-form region attention branch tries to associate
the input question to relevant regions of the input image.
There is no restriction to the attended regions, which are of
free-form and are able to capture the global visual context
and attributes of the image. Unlike existing VQA methods
that fuse question and image features via simple concatena-
tion (Shih, Singh, and Hoiem 2016) or addition (Lu et al.
2016) to guide the calculation of the attention weights, each
of our attention branch fuses all three types of input features
via a multiplicative feature embedding scheme for utilizing
full information from the inputs. The network structure for
attending visual features with our multiplicative feature em-
bedding scheme is show in Figure 3(a).

Given the question embedding Q ∈ Rk, the whole-image
representation R ∈ R2048×14×14, and the detection repre-
sentation D ∈ R4097×19, we ﬁrst embed them to a 1200-
dimensional common space via the following equations,

R1 = tanh(Wr1R + br1),

D1 =

· 1(tanh(Wd1D + bd1)T ),

1
19

(5)

(6)

Q1 = tanh(Wq1 Q + bq1 ),

(7)
where Wr1 ∈ R1200×2408, Wd1 ∈ R1200×4097, Wq1 ∈
R1200×k are the learnable weight parameters, br1 , bd1 , bq1
∈ R1200 are the bias parameters, 1 ∈ R19 represents an all-
1 vector, and tanh is the hyperbolic tangent function. For
learning the attention weights for the whole-image feature
R, the transformed detection features are averaged across
all detection boxes following Eq. (6) before feature fusion.
After mapping all input features into the 1200-dimensional
common space, the detection feature D1 ∈ R1200 and ques-
tion feature Q1 ∈ R1200, are spatially replicated to a 14×14
grid to form ˜D1 and ˜R1, which match the spatial size of the
whole-image feature R1 ∈ R1200×14×14.

The joint feature representation C1 of the three inputs is
obtained by element-wise multiplication (Hadamard prod-
uct) of ˜Q1, R1 and ˜D1, and followed by a L2 normalization
to constrain the magnitude of the representation,

C1 = Norm2( ˜Q1 ◦ R1 ◦ ˜D1),
(8)
where ◦ indicates element-wise multiplication. The free-
form attention map is then obtained by convolving the joint
feature representation C1 with a 1 × 1 convolution followed
by a softmax operation over the 14 × 14 grid,
a1 = softmax(Wc1 ∗ C1 + bc1),
(9)
where Wc1 ∈ R1200×1×1 and bc1 ∈ R1200 are the learnable
convolution kernel parameters. The attended whole-image
feature over all spatial locations can be calculated by

v1 =

a1(i)R1(i),

(10)

14×14
(cid:88)

i

which can represent the attended whole-image visual feature
that are most related to the input question.

Figure 3: Learning to attend visual features with multi-
modal multiplicative feature embedding for (a) free-form
image regions and for (b) detection boxes.

3.3 Learning to attend detection-box visual
features with multiplicative embedding

Our second branch focuses on learning question-related de-
tection features by attending the detection-box features with
joint input feature representation. Similar to the ﬁrst branch,
we fuse the question representation, the whole-image fea-
tures, and the detection-box features for learning the at-
tention weights for detection boxes. Unlike previous detec-
tion based attention methods (Shih, Singh, and Hoiem 2016;
Li and Jia 2016), our proposed attention mechanism in-
tegrates whole-image features for better understanding the
overall image contents. The structure for learning the joint
feature representation is shown in Figure 3(b).

Similar to the joint representation for free-form image re-
gion attention, given the question embedding Q ∈ Rk, the
image representation R ∈ R2048×14×14, the detection rep-
resentation D ∈ R4097×19, we transform each representa-
tion to a common semantic space, and then obtain the 1200-
dimensional joint feature representation C2 as

D2 = tanh(Wd2 D + bd2),

R2 =

· 1(tanh(Wr2 R + br2 )T ),

1
196

Q2 = tanh(Wq2 Q + bq2 ),
C2 = Norm2( ˜Q2 ◦ ˜R2 ◦ D2),

(11)

(12)

(13)

(14)

where Wd2, Wr2, Wq2 , bd1 , br1, bq2 are the learnable pa-
rameters for linear transformation. The transformed ques-
tion feature Q2 and whole-image feature R2 are replicated
across the number dimension to match the dimension of the
transformed detection features D2 ∈ R1200×19 before cal-
culating the joint representation C2 following Eq. (14).

The ﬁnal attended detection representation v2 over all de-

tection boxes is deﬁned as

a2 = softmax(Wc2C2 + bc2 ),

v2 =

a2(i)D2(i),

19
(cid:88)

i

(15)

(16)

where Wc2 ∈ R19×1200 and bc2 ∈ R19 are the learnable
parameters of learning the attention weights a2 for the de-
tection boxes.

3.4 Learning for answer prediction
Similar to existing VQA approaches (Antol et al. 2015; Lu et
al. 2016; Fukui et al. 2016), we model VQA as a multi-class
classiﬁcation problem. Given the attended whole-image fea-
ture v1, attended detection feature v2 with the input ques-
tion feature Q, question-image joint encodings are obtained
by element-wise multiplication of the transformed question
features and the attended features from both branch,

hr = v1 ◦ tanh(WhrQ + bhr),
hd = v2 ◦ tanh(WhdQ + bhd),

(17)
(18)

where Whr, Whd, bhr, bhd are the learnable parameters for
transforming the input question feature Q. The reason why
we choose different question transformation parameters for
the two branches is that the attended visual features from the
two different branches captures different information from
the input image. One is from attended free-form region fea-
tures and is able to capture the global context and attributes
of the scene. The another is from attended detection features
and is able to extract information about foreground objects.
After merging question-image encodings from the two
branches via addition, a linear classiﬁer is trained for ﬁnal
answer prediction,

pans = softmax(Wp(hr + hd) + bp),

(19)

where Wp and bp are the classiﬁer parameters, and pans rep-
resents the probability of the ﬁnal answer prediction.

4 Experiments

4.1 Datasets and evaluation metrics
We evaluate the proposed model on two public datasets, the
VQA (Antol et al. 2015) and COCO-QA (Ren, Kiros, and
Zemel 2015) datasets.

VQA dataset (Antol et al. 2015) is based on Microsoft
COCO image data (Lin et al. 2014). The dataset con-
sists of 248,349 training questions, 121,512 validation ques-
tions and 244,302 testing questions, generated on a total of
123,287 images. There are three types of questions includ-
ing yes/no, number and other. For each question, 10 free-
response answers are provided. We take the top 2000 most
frequent answers as the candidate outputs for learning the
classiﬁcation model similar to (Kim et al. 2016), which cov-
ers 90.45% answers in the training and validation sets.

COCO-QA dataset (Ren, Kiros, and Zemel 2015) is cre-
ated based on the caption annotations of Microsoft COCO
dataset (Lin et al. 2014). There are 78,736 training samples
and 38,948 testing samples, respectively. It contains four

types of questions, object, number, color and location. All
of the answers are single-words and considered as the valid
answers, namely 430 answers, which are used for the possi-
ble answer classiﬁcation.

Since we formulate VQA as a classiﬁcation task, we use
the accuracy metric to measure the performances of dif-
ferent models on both two datasets. In particular, for the
VQA dataset, a predicted answer is regarded as correct if it
matches more than three in ten ground truth answers. WUPS
calculates the similarity between two words based on their
common subsequence in the taxonomy tree. In addition, Wu-
Palmer similarity (WUPS) (Wu and Palmer 1994) is also
reported for the COCO-QA dataset. Same as the previous
work (Lu et al. 2016; Li and Jia 2016), we report the WUPS
scores with the thresholds of 0.9 and 0.0.

Implementation details

4.2
For encoding questions, the length of questions is ﬁxed to 26
and each word embedding is a vector of size 620. The hid-
den state of GRU is set to 2400. Given the question, image
and detection representations, the joint feature embedding of
these inputs h is set as 1200. Following (Kim et al. 2017),
we take two glimpses of each attention map, that is to say,
for each branch, another set of attention weights a1 is trained
for whole-image features R1 in the ﬁrst branch and another
set of a2 for detection features D2 in the second branch. The
two sets of attended features are concatenated in each branch
as the ﬁnal feature.

We implement our model with the Torch library. The RM-
SProp method is used for training our network with an initial
learning rate of 3×10−4, a momentum of 0.99 and a weight-
decay of 10−8. The batch size is set to 300, and is trained for
250,000 iterations. The validation process is performed ev-
ery 10,000 iterations with early stopping when validation ac-
curacy stops improving for more than 5 validations. Dropout
is applied after every linear transformation and gradient clip-
ping techniques are used for regularization in GRU training.
Multi-GPU parallel technology is adopted to accelerate the
training process.

4.3 Comparison with state-of-the-arts
Table 1 shows results on the VQA test set for both open-
ended and multiple-choice tasks by our proposed approach
and compared methods. The approaches shown in Table 1
are trained on the train+val split of the VQA dataset and
evaluated on the test split, where the test-dev set is normally
used for validation and the test-std set for standard testing.
The models in the ﬁrst part of Table 1 are based on sim-
ple joint question-image feature embedding methods. Mod-
els in the second part of Table 1 employ detection-based
attention mechanism and models in the third part use free-
form region-based attention mechanism. We only compare
results of the single models since most approaches do not
adopt the model ensemble strategy. We can see that our ﬁ-
nal model (denoted as Dual-MFA, where MFA stands for
Multiplicative Feature Attention) improves the state-of-the-
art MLB approach (Kim et al. 2017) from 65.07% to 66.09%
for the open-ended task, and from 68.89% to 69.97% for the
multiple-choice task on the test-std set. Speciﬁcally, in the

MC

All

57.57
61.97
62.69
64.18
-

62.43
65.43

-
-
66.33
66.07
67.80
-
68.89

-
69.97

36.80
42.62
42.24
46.10
48.33

-
-

43.48
46.42
49.41
51.95
53.20
-
54.77

-
56.89

59.01
56.34

59.18
59.01
58.93

59.82
58.35
58.53

55.57
53.99
54.89

59.18
57.40
58.16

59.07
59.82

Validation

Method

All

Y/N

Num. Other

All

Y/N

Num. Other

Test-dev

Open-Ended

Test-std

Open-Ended

LSTM Q+I (Antol et al. 2015)
iBOWING (Zhou et al. 2015)
DPPnet (Noh, Hongsuck Seo, and Han 2016)
FDA (Ilievski, Yan, and Feng 2016)
DMN+ (Xiong, Merity, and Socher 2016)

Region Sel. (Shih, Singh, and Hoiem 2016)
QRU (Li and Jia 2016)

SMem (Xu and Saenko 2016)
SAN (Yang et al. 2016)
MRN (Kim et al. 2016)
HieCoAtt (Lu et al. 2016)
VQA-Machine (Wang et al. 2017)
MCB (Fukui et al. 2016)
MLB (Kim et al. 2017)

Dual-MLB (our baseline)
Dual-MFA (ours)

53.74
55.72
57.22
59.24
60.30

-
60.72

57.99
58.70
61.68
61.80
63.10
64.70
64.89

65.12
66.01

78.94
76.55
80.71
81.14
80.50

-
82.29

80.87
79.30
82.28
79.70
81.50
82.50
84.13

83.32
83.59

35.24
35.03
37.24
36.16
36.80

-
37.02

37.32
36.60
38.82
38.70
38.40
37.60
37.85

39.96
40.18

36.42
42.62
41.69
45.77
48.30

-
47.67

43.12
46.10
49.25
51.70
53.00
55.60
54.57

55.26
56.84

MC

All

57.17
61.68
62.48
64.01
-

62.44
65.43

-
-
66.15
65.80
67.70
69.10
-

69.55
70.04

54.06
55.89
57.36
59.54
60.36

-
60.76

58.24
58.85
61.84
62.06
63.30
-
65.07

-
66.09

79.01
76.76
80.28
81.34
80.43

-
-

80.80
79.11
82.39
79.95
81.40
-
84.02

-
83.37

35.55
34.98
36.92
35.67
36.82

-
-

37.53
36.41
38.23
38.22
38.20
-
37.90

-
40.39

Table 1: Evaluation results by our proposed method and compared methods on the VQA dataset.

question types of number and other, our proposed approach
brings 2.49% and 2.12% improvements on the test-std set.
QRU (Li and Jia 2016) is the state-of-the-art detection-based
attention method, and our model signiﬁcantly outperforms it
by 5.33% on the test-std set. We also compare our approach
with a baseline model Dual-MLB, which consists of two at-
tention branches based on the MLB tensor fusion module.
The baseline model fuses the question and free-form image
region embedding by MLB in one branch, and fuses ques-
tion and image detection embedding by MLB in another
branch. The baseline Dual-MLB also outperforms MLB by
0.23% on the test-dev set, which shows that our improve-
ments are not only from the integration of the two attention
mechanisms, but are also caused by effective joint feature
embedding of question, image, and detection features.

Table 3 compares our approach with the state-of-the-art
approaches on the COCO-QA test set. Our ﬁnal model Dual-
MFA improves the state-of-the-art HieCoAtt (Lu et al. 2016)
from 65.40% to 66.49%. In particular, our model achieves
an improvement of 2.99% for the question type color. Sim-
ilar to the results on the VQA dataset, our model signif-
icantly outperforms the state-of-the-art detection-based at-
tention method QRU by 3.49%.

4.4 Ablation study

In this section, we conduct ablation experiments to study ef-
fectiveness of individual component designs in our model.
Table 2 shows the results of baseline models replacing dif-
ferent components in our model, which are trained on the
VQA training set and tested on the validation set. Following
other compared approaches, the test set is not used in the
study due to online submission restrictions. Speciﬁcally, we
compare different multi-modal feature embedding designs
and investigate the roles of the two attention mechanisms.

Method

MFA-MUL*
MFA-ADD

MFA-Norm*
MFA w/o Norm
MFA-Power

Dual-MFA-ADD*
Dual-MFA-MUL
Dual-MFA-CAT

MFA-D*
QRU-D (Li and Jia 2016)
MLB-D (Kim et al. 2017)

MFA-R*
MLB-R (Kim et al. 2017)
MUTAN (Ben-Younes et al. 2017)

Dual-MLB
Dual-MFA* (full model)

Table 2: Ablation study on the VQA dataset, where “*” de-
notes our model’s design.

The ﬁrst part of Table 2 shows different element-wise op-
erations used for joint embedding of three input features.
Element-wise multiplication (denoted as MFA-MUL) in Eq.
(8) performs better than element-wise addition (denoted as
MFA-ADD) by 3.67%. The second part of Table 2 shows L2
normalization (denoted as MFA-Norm) in joint feature em-
bedding works better than the model with unsigned power
operation of 1/3 (denoted as MFA-Power) and the model
without normalization (denoted as MFA w/o Norm). For
fusing outputs hr and hd from the two attention branches,
element-wise addition in Eq. (19) achieves better perfor-

Method

All

Obj.

Num. Color

Loc. WUPS0.9 WUPS0.0

2VIS+BLSTM (Ren, Kiros, and Zemel 2015)
IMG-CNN (Ma, Lu, and Li 2016)
DDPnet (Noh, Hongsuck Seo, and Han 2016)
SAN (Yang et al. 2016)
QRU (Li and Jia 2016)
HieCoAtt (Lu et al. 2016)

55.09
58.40
61.16
61.60
62.50
65.40

58.17
-
-
65.40
65.06
68.00

44.79
-
-
48.60
46.90
51.00

49.53
-
-
57.90
60.50
62.90

47.34
-
-
54.00
56.99
58.80

Dual-MFA (ours)

66.49

68.86

51.32

65.89

58.92

65.34
68.50
70.84
71.60
72.58
75.10

76.15

88.64
89.67
90.61
90.90
91.62
92.00

92.29

Table 3: Evaluation results by our proposed method and compared methods on the COCO QA dataset.

Q: What sport is this?
A: tennis

Q: What is the color of the surfboard?
A: white

Q: Is it a sunny day?
A: yes

Q: How many giraffes are there?
A: 5

Q: What does the red circle sign mean?
A: no parking

(a)

(b)

(c)

(d)

(e)

Figure 4: Visualization examples on the VQA test-dev set. (First row) input images. (Second row) free-form region based
attention maps. (Third row) detection-based attention maps.

mance than element-wise multiplication (denoted as Dual-
MFA-MUL) and concatenation of two vectors (denoted as
Dual-MFA-CAT).

The third and fourth parts of Table 2 compare our multi-
modal multiplicative feature embedding for a single atten-
tion mechanism (MFA-R or MFA-D) with detection-based
attention baseline models (QRU-D and MLB-D) and region-
based attention baseline models (MUTAN and MLB-R),
which replace our multiplicative feature embedding with
QRU (Li and Jia 2016) or MLB (Kim et al. 2017) respec-
tively. The MUTAN (Ben-Younes et al. 2017) approach
is also used for comparison. Results show that our mod-
els with a single attention branch outperform all com-
pared detection-based and region-based baselines. Finally,
we compare our ﬁnal model Dual-MFA with the base-
line model Dual-MLB. Our Dual-MFA model beneﬁts from
its effective multi-modal multiplicative feature embedding
scheme and achieves an improvement of 0.75%.

The parameter size of our ﬁnal full model is 82.6M, while
the best region-based model (MFA-R) is 61.7M, and the
best detection-based model (MFA-D) is 56.8M. As the pa-
rameters in the language model and the classiﬁer are shared

by two attention branches, parameters of our full model in-
crease in a minor degree.

4.5 Qualitative evaluation
We visualize some co-attention maps generated by our
model in Figure 4 and present ﬁve examples from the
VQA test set. Figures 4(a) and (b) show that our model at-
tends to the corresponding image regions with two attention
branches, which leads to correct answers with higher conﬁ-
dence. There are also cases where only one attention branch
is able to attend the correct image region to obtain the cor-
rect answer. In Figure 4(c), the free-form region based at-
tention map is able to attend the blue sky and dry ground,
while the detection-based attention map fails as there are no
pre-given detection boxes covering these image regions. In
Figure 4(d), the detection-based attention map has higher
weights on all ﬁve giraffes to generate the correct answer,
while the free-form attention map attends incorrect regions.
A failure case is also presented in Figure 4(e). The model
fails to generate the correct answer as the classiﬁcation label
“no turning right” does not exit in the training set despite
attending the correct image region.

5 Conclusion
In this paper, we propose a novel deep neural network
with co-attention mechanism for visual question answering.
The deep model contains two branches for visual attention
which aim to select the free-form image regions and detec-
tion boxes most related to the input question. We generate
visual attention weights by a novel multiplicative embed-
ding scheme, which fuses the question, whole-image and
detection-box features effectively. Ablation study demon-
strates the effectiveness of individual components of our
proposed model. Experimental results on two large VQA
datasets show that our proposed model outperforms state-
of-the-art approaches.

Acknowledgement
This work was supported in part by National Natural
Science Foundation of China under Grant No. 61532010
and No. 61702190, National Basic Research Program of
China (973 Program) under Grant No. 2014CB340505,
in part by SenseTime Group
and SHMEC (16CG24),
Limited, in part by the General Research Fund sponsored
by the Research Grants Council of Hong Kong (Nos.
CUHK14205615,
CUHK14206114,
CUHK14213616,
CUHK14239816,
CUHK14203015,
CUHK419412,
CUHK14207814), in part by the Hong Kong Innovation and
Technology Support Programme Grant ITS/121/15FX. We
also thank Tong Xiao, Kun Wang and Kai Kang for helpful
discussions.

References
Antol, S.; Agrawal, A.; Lu, J.; Mitchell, M.; Batra, D.;
Lawrence Zitnick, C.; and Parikh, D. 2015. Vqa: Visual ques-
tion answering. In ICCV.
Ben-Younes, H.; Cad`ene, R.; Thome, N.; and Cord, M. 2017.
Mutan: Multimodal tucker fusion for visual question answer-
ing. In ICCV.
Cho, K.; van Merri¨enboer, B.; G¨ulc¸ehre, C¸ .; Bahdanau, D.;
Bougares, F.; Schwenk, H.; and Bengio, Y. 2014. Learning
phrase representations using rnn encoder–decoder for statisti-
cal machine translation. In EMNLP.
Fukui, A.; Park, D. H.; Yang, D.; Rohrbach, A.; Darrell, T.; and
Rohrbach, M. 2016. Multimodal compact bilinear pooling for
visual question answering and visual grounding. In EMNLP.
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual
learning for image recognition. In CVPR.
Ilievski, I.; Yan, S.; and Feng, J. 2016. A focused dynamic
attention model for visual question answering. arXiv preprint
arXiv:1604.01485.
Karpathy, A., and Fei-Fei, L. 2015. Deep visual-semantic align-
ments for generating image descriptions. In CVPR.
Kim, J.-H.; Lee, S.-W.; Kwak, D.; Heo, M.-O.; Kim, J.; Ha, J.-
W.; and Zhang, B.-T. 2016. Multimodal residual learning for
visual qa. In NIPS.
Kim, J.-H.; On, K. W.; Lim, W.; Kim, J.; Ha, J.-W.; and Zhang,
B.-T. 2017. Hadamard product for low-rank bilinear pooling.
In ICLR.

Kiros, R.; Zhu, Y.; Salakhutdinov, R. R.; Zemel, R.; Urtasun,
R.; Torralba, A.; and Fidler, S. 2015. Skip-thought vectors. In
NIPS.
Li, R., and Jia, J. 2016. Visual question answering with question
representation update (qru). In NIPS.
Li, S.; Xiao, T.; Li, H.; Zhou, B.; Yue, D.; and Wang, X. 2017a.
Person search with natural language description. In CVPR.
Li, Y.; Duan, N.; Zhou, B.; Chu, X.; Ouyang, W.; and Wang,
X. 2017b. Visual question generation as dual task of visual
question answering. arXiv preprint arXiv:1709.07192.
Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-
manan, D.; Doll´ar, P.; and Zitnick, C. L. 2014. Microsoft coco:
Common objects in context. In ECCV.
Lu, J.; Yang, J.; Batra, D.; and Parikh, D. 2016. Hierarchical
question-image co-attention for visual question answering. In
NIPS.
Ma, L.; Lu, Z.; and Li, H. 2016. Learning to answer questions
from image using convolutional neural network. In AAAI.
Malinowski, M.; Rohrbach, M.; and Fritz, M. 2015. Ask your
neurons: A neural-based approach to answering questions about
images. In ICCV.
Mostafazadeh, N.; Misra, I.; Devlin, J.; Mitchell, M.; He, X.;
and Vanderwende, L. 2016. Generating natural questions about
an image. In ACL.
Noh, H.; Hongsuck Seo, P.; and Han, B. 2016. Image question
answering using convolutional neural network with dynamic
parameter prediction. In CVPR.
Ren, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster r-cnn: To-
wards real-time object detection with region proposal networks.
In NIPS.
Ren, M.; Kiros, R.; and Zemel, R. 2015. Exploring models and
data for image question answering. In NIPS.
Shih, K. J.; Singh, S.; and Hoiem, D. 2016. Where to look:
Focus regions for visual question answering. In CVPR.
Wang, P.; Wu, Q.; Shen, C.; and Hengel, A. v. d. 2017. The
vqa-machine: Learning how to use existing vision algorithms
to answer new questions. In CVPR.
Wu, Z., and Palmer, M. 1994. Verbs semantics and lexical
selection. In ACL.
Xie, L.; Shen, J.; and Zhu, L. 2016. Online cross-modal hashing
for web image retrieval. In AAAI.
Xiong, C.; Merity, S.; and Socher, R. 2016. Dynamic memory
networks for visual and textual question answering. In ICML.
Xu, H., and Saenko, K. 2016. Ask, attend and answer: Ex-
ploring question-guided spatial attention for visual question an-
swering. In ECCV.
Yang, Z.; He, X.; Gao, J.; Deng, L.; and Smola, A. 2016.
Stacked attention networks for image question answering. In
CVPR.
Ye, Y.; Zhao, Z.; Li, Y.; Chen, L.; Xiao, J.; and Zhuang, Y.
2017. Video question answering via attribute-augmented at-
tention network learning. In ACM Multimedia.
Zhou, B.; Tian, Y.; Sukhbaatar, S.; Szlam, A.; and Fergus, R.
2015. Simple baseline for visual question answering. arXiv
preprint arXiv:1512.02167.
Zitnick, C. L., and Doll´ar, P. 2014. Edge boxes: Locating object
proposals from edges. In ECCV.

Co-attending Free-form Regions and Detections with Multi-modal Multiplicative
Feature Embedding for Visual Question Answering

Pan Lu† Hongsheng Li‡∗ Wei Zhang(cid:92)

Jianyong Wang† Xiaogang Wang‡∗

† Tsinghua National Laboratory for Information Science and Technology (TNList)
Department of Computer Science, Tsinghua University
‡ Department of Electronic Engineering, The Chinese University of Hong Kong
(cid:92) Shanghai Key Laboratory of Trustworthy Computing, East China Normal University

{lupantech, zhangwei.thu2011}@gmail.com, {hsli, xgwang}@ee.cuhk.edu.hk,

jianyong@mail.tsinghua.edu.cn

7
1
0
2
 
c
e
D
 
2
1
 
 
]

V
C
.
s
c
[
 
 
2
v
4
9
7
6
0
.
1
1
7
1
:
v
i
X
r
a

Abstract

Recently, the Visual Question Answering (VQA) task has
gained increasing attention in artiﬁcial intelligence. Exist-
ing VQA methods mainly adopt the visual attention mech-
anism to associate the input question with corresponding
image regions for effective question answering. The free-
form region based and the detection-based visual attention
mechanisms are mostly investigated, with the former ones
attending free-form image regions and the latter ones at-
tending pre-speciﬁed detection-box regions. We argue that
the two attention mechanisms are able to provide comple-
mentary information and should be effectively integrated
to better solve the VQA problem. In this paper, we pro-
pose a novel deep neural network for VQA that integrates
both attention mechanisms. Our proposed framework effec-
tively fuses features from free-form image regions, detec-
tion boxes, and question representations via a multi-modal
multiplicative feature embedding scheme to jointly attend
question-related free-form image regions and detection boxes
for more accurate question answering. The proposed method
is extensively evaluated on two publicly available datasets,
COCO-QA and VQA, and outperforms state-of-the-art ap-
proaches. Source code is available at https://github.
com/lupantech/dual-mfa-vqa.

1

Introduction

In recent years, multi-modal learning for language and vi-
sion has gained much attention in artiﬁcial intelligence.
Great progress has been achieved for different tasks in-
cluding image captioning (Karpathy and Fei-Fei 2015), vi-
sual question generation (Mostafazadeh et al. 2016; Li et
al. 2017b), video question answering (Ye et al. 2017) and
text-to-image retrieval (Xie, Shen, and Zhu 2016; Li et al.
2017a). The Visual Question Answering (VQA) (Antol et
al. 2015) task has recently emerged as a more challenging
task. The algorithms are required to answer natural language
questions about a given image’s contents. Compared with
the conventional visual-language tasks such as image cap-
tioning and text-to-image retrieval, the VQA task requires
the algorithms to have a better understanding on both the
input image and question in order to infer the answer.

∗Co-corresponding authors.

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Figure 1: Co-attending free-form regions and detection
boxes based on the question, the whole image, and detec-
tion boxes for better utilizing complementary information to
solve the VQA task.

State-of-the-art VQA approaches utilize visual attention
mechanism to relate the question to meaningful image re-
gions for accurate question answering. Most visual attention
mechanisms in VQA can be categorized into free-form re-
gion based methods (Lu et al. 2016; Fukui et al. 2016) and
detection-based methods (Li and Jia 2016; Shih, Singh, and
Hoiem 2016). For the free-form region based methods, the
question features learned by a Long Short-Term Memory
(LSTM) network and the image features learned by a Con-
volutional Neural Network (CNN) are fused by either ad-
dictive, multiplicative or concatenation operations at every
image spatial location. The free-form attention map is ob-
tained by applying a softmax non-linearity operation across
the fused feature map. Since there is no restriction on the ob-
tained attention map, the free-form attention region is able
to attend both global visual context and speciﬁc foreground
objects for inferring answers. However, since there is no re-
striction, the free-form attentive regions might focus on par-
tial objects or irrelevant context sometimes. For instance, for
an question, “What animal do you see?”, a free-form re-
gion attention map might mistakenly focus on only a part
of the foreground cat and generate an answer of “dog”. On
the other hand, for the detection-based attention methods,
the attention mechanism is utilized for relating the question
to pre-speciﬁed detection boxes (e.g., Edge boxes (Zitnick

and Doll´ar 2014)). Instead of applying the softmax operation
over all image spatial locations, the operation is calculated
over all detection boxes. Therefore, the attended regions are
restricted to pre-speciﬁed detection-box regions and such
question-related regions could be more effective for answer-
ing questions about foreground objects. However, such re-
strictions also cause challenges for other types of questions.
For instance, for the question “How is the weather today?”,
there might not exist a detection box in the sky, resulting in
a failure in answering this question.

For better understanding the question and image con-
tents and their relations, a good VQA algorithm needs to
identify global scene attributes, locate objects, identify ob-
ject attributes, quantity and categories to make accurate in-
ference. We argue that the above mentioned two types of
attended mechanisms provide complementary information
and should be effectively integrated in a uniﬁed framework
to take advantages of both the attended free-form regions
and attended detection regions. Take the above mentioned
two questions as examples, the question about animal could
be more effectively answered with detection-based attention
maps, while the question about the weather can be better an-
swered with the free-form region based attention maps.

In this paper, we propose a novel dual-branch deep neu-
ral network for solving the VQA problem that combines
free-form region based and detection-based attention mech-
anisms (see Figure 1). The overall framework consists of two
attention branches, each of which associates the question
with the most relevant free-form regions or with the most
relevant detection regions in the input image. For obtaining
more question-related attention weights for both types of re-
gions, we propose to learn the joint feature representation
of the input question, the whole image, and the detection
boxes with a multiplicative feature embedding scheme. Such
a multiplicative scheme does not share parameters between
the two branches and is shown to result in more robust an-
swering performance than existing methods.

The main contributions of our work can be summarized

as twofold.
• We propose a novel dual-branch deep neural network
that effectively integrates the free-form region based and
detection-based attention mechanisms in a uniﬁed frame-
work;

• In order to better fuse features from different modalities,
a novel multiplicative feature embedding scheme is pro-
posed to learn joint feature representations from the ques-
tion, the whole image, and the detection boxes.

2 Related work
State-of-the-art VQA algorithms are mainly based on deep
neural networks for learning visual-question features and for
predicting the ﬁnal answers. Due to the establishment of the
VQA dataset and the online evaluation platform by (Antol et
al. 2015), there is an increasing number of VQA algorithms
proposed every year.

Multi-modal feature embedding for VQA. The existing
joint feature embedding methods for VQA typically com-
bine the visual and question features learned by deep neural

networks and then solve the task as a multi-class classiﬁca-
tion problem. (Zhou et al. 2015) proposed a simple baseline,
which learns image features with CNN and question features
from LSTM, and concatenated these two features to predict
the answer. Instead of using LSTM for learning question
representations, (Noh, Hongsuck Seo, and Han 2016) used
GRU (Cho et al. 2014) and (Ma, Lu, and Li 2016) trained
CNN for question embedding. Different from the above
mentioned methods addressing the VQA task as a classiﬁca-
tion problem, the work by (Malinowski, Rohrbach, and Fritz
2015) fed both image CNN features and question represen-
tations into an LSTM to generate the answer by sequence-to-
sequence learning. There are also high-order approaches for
multi-modal feature embedding. MCB (Fukui et al. 2016)
and MLB (Kim et al. 2017) both designed bilinear pool-
ing approaches to learn multi-modal feature embedding for
VQA. In (Ben-Younes et al. 2017), a generalized multi-
modal pooling framework is proposed, which shows that
MCB and MLB are its special cases. Inspired by ResNet
(He et al. 2016), (Kim et al. 2016) proposed element-wise
multiplication for the joint residual mappings.

Attention mechanism for VQA. A large quantity of re-
cent works focused on incorporating attention mechanisms
for solving the VQA task. In (Xu and Saenko 2016), the at-
tention weights over different image regions are calculated
based on the semantic similarity between the question and
image regions, and the updated question features are ob-
tained as the weighted sum of the different regions. (Yang et
al. 2016) proposed a multi-stage attention framework, which
stacks the attention modules to search question-related im-
age regions by iterative feature fusion. The attention mech-
anism is not limited to image modality, (Lu et al. 2016) pro-
posed a co-attention mechanism that simultaneously attends
both the question and image with joint visual-question fea-
ture representations.

Since some of the questions are related to objects in the
images, object detection results are explored to replace the
visual features obtained from the whole-image region. (Shih,
Singh, and Hoiem 2016) utilized the attention mechanism to
generate visual features from 100 detection boxes for the
VQA task. Similarly, (Li and Jia 2016) proposed a frame-
work that updates the joint visual-question feature embed-
ding by iteratively attending the top detection boxes.

However, all the attention-based methods focus on one
type of image regions for question-image association (i.e.,
either free-form image regions or detection boxes), and each
of them have limitations on solving certain types of ques-
tions. In contrast, in order to better utilize the complemen-
tary information from both types of image regions, our pro-
posed approach effectively integrates both attention mecha-
nisms in a uniﬁed framework.

3 Method
The overall structure of our proposed deep neural network is
illustrated in Figure 2, which takes the question, the whole
image, and the detection boxes as inputs, and learns to as-
sociate questions to free-form image regions and detection
boxes simultaneously to infer the answer. Our proposed
model for VQA consists of two co-attention branches for

Figure 2: Illustration of the overall network structure for solving the VQA task. The network has two attention branches with
the proposed multiplicative featucre embedding scheme, where one branch attends free-form image regions and another branch
attends detection boxes for encoding question-related visual features.

free-form image regions and for detection boxes, respec-
tively. Before calculating the attention weights for each type
of regions, the joint feature representations of the question,
the whole-image visual features, and the detection box vi-
sual features are obtained via a multiplicative approach. In
this way, the attention weights for each type of regions are
learned from all the three inputs. Importantly, the joint multi-
modal feature embedding has different parameters for each
branch, which leads to better answering accuracy. The re-
sulting visual features from the two branches are fused with
the question representation and then added to obtain the ﬁnal
question-image embedding. A multi-class linear classiﬁer is
adopted for obtaining the ﬁnal predicted answer.

The input feature encoding will be introduced in Section
3.1. Section 3.2 introduces the branch with visual attention
mechanism for associating free-form image regions to the
input questions with our multiplicative feature embedding
scheme, and Section 3.3 describes the another branch with
detection based attention mechanism. The ﬁnal answer pre-
diction will be introduced in Section 3.4.

3.1

Input feature encoding

Encoding whole-image features. We utilize an ImageNet
pre-trained ResNet-152 (He et al. 2016) for learning im-
age visual features. The input image to ResNet is resized to
448 × 448 and the 2048 × 14 × 14 output feature map of the
last convolution layer is used to encode the whole-image vi-
sual features R ∈ R2048×14×14, where 14 × 14 is its spatial
size and 2048 is the number of visual feature channels.

Encoding detection-box features. We adopt the Faster-
RCNN (Ren et al. 2015) framework to obtain object detec-
tion boxes in the input image. For all the object proposals
and their associated detection scores generated by Faster-
RCNN, the non-maximum suppression with an intersection
over union (IoU) 0.3 is applied and the top-ranked 19 detec-
tion boxes are chosen as the detection-box features for our
overall framework. The 4096-dimensional visual features of
Faster-RCNN’s fc7 layer concatenated with boxes’ detec-
tion scores are utilized to encode the visual feature of each

detection box. Let D = [f1, · · · , f19] ∈ R4097×19 denote
the visual features of the top-ranked 19 detection boxes.

Encoding question features. The Gated Recurrent Unit
(GRU) (Cho et al. 2014) is adopted to encode the question
features, which show its effectiveness in recent VQA meth-
ods (Lu et al. 2016; Kim et al. 2017). A GRU cell consists
of an update gate z and a reset gate r. Given a question
q = [q1, ..., qT ], where qt is the one-hot vector of at po-
sition t, and T is the length of the question. We convert
each word qt into a feature vector via a linear transforma-
tion xt = Weqt. At each time step, the word feature vector
qt is sequentially fed into the GRU to encode the input ques-
tion. At each step, the GRU updates the update gate zt and
reset gate rt and outputs a hidden state ht. The GRU update
process operates as

(1)
(2)

(3)

zt = σ(Wzxt + Uzht−1 + bz),
rt = σ(Wrxt + Urht−1 + br),
˜ht = tanh(Whxt + Uh(rt ◦ ht−1) + bh),
ht = zt ◦ ht−1 + (1 − zt) ◦ ˜ht,

(4)
where σ represents the sigmoid activation function. The
weight matrices W , U and bias vector b are learnable pa-
rameters of the GRU.

We take the ﬁnal hidden state hT as the question embed-
ding, i.e., Q = hT ∈ Rk, where k denotes the embedding
length. Following (Li and Jia 2016; Kim et al. 2017), we uti-
lize the pre-trained skip-thought model (Kiros et al. 2015) to
initialize the embedding matrix We in our question language
model. Since the skip-thought model is previously trained on
large text corpus, we are able to transfer external language-
based knowledge to our VQA task by ﬁne-tuning the GRU
from the initial point.

3.2 Learning to attend free-form region visual
features with multiplicative embedding
Our proposed framework has two branches, one for attend-
ing question-related free-form image regions to learn whole-
image features and the other for attending detection-box re-
gions to learn question-related detection features. For each

attention branch, it takes question feature Q, whole-image
feature R and detection-box feature D as inputs, and outputs
the question-attended visual features v1 and v2 for question
answering.

Our free-form region attention branch tries to associate
the input question to relevant regions of the input image.
There is no restriction to the attended regions, which are of
free-form and are able to capture the global visual context
and attributes of the image. Unlike existing VQA methods
that fuse question and image features via simple concatena-
tion (Shih, Singh, and Hoiem 2016) or addition (Lu et al.
2016) to guide the calculation of the attention weights, each
of our attention branch fuses all three types of input features
via a multiplicative feature embedding scheme for utilizing
full information from the inputs. The network structure for
attending visual features with our multiplicative feature em-
bedding scheme is show in Figure 3(a).

Given the question embedding Q ∈ Rk, the whole-image
representation R ∈ R2048×14×14, and the detection repre-
sentation D ∈ R4097×19, we ﬁrst embed them to a 1200-
dimensional common space via the following equations,

R1 = tanh(Wr1R + br1),

D1 =

· 1(tanh(Wd1D + bd1)T ),

1
19

(5)

(6)

Q1 = tanh(Wq1 Q + bq1 ),

(7)
where Wr1 ∈ R1200×2408, Wd1 ∈ R1200×4097, Wq1 ∈
R1200×k are the learnable weight parameters, br1 , bd1 , bq1
∈ R1200 are the bias parameters, 1 ∈ R19 represents an all-
1 vector, and tanh is the hyperbolic tangent function. For
learning the attention weights for the whole-image feature
R, the transformed detection features are averaged across
all detection boxes following Eq. (6) before feature fusion.
After mapping all input features into the 1200-dimensional
common space, the detection feature D1 ∈ R1200 and ques-
tion feature Q1 ∈ R1200, are spatially replicated to a 14×14
grid to form ˜D1 and ˜R1, which match the spatial size of the
whole-image feature R1 ∈ R1200×14×14.

The joint feature representation C1 of the three inputs is
obtained by element-wise multiplication (Hadamard prod-
uct) of ˜Q1, R1 and ˜D1, and followed by a L2 normalization
to constrain the magnitude of the representation,

C1 = Norm2( ˜Q1 ◦ R1 ◦ ˜D1),
(8)
where ◦ indicates element-wise multiplication. The free-
form attention map is then obtained by convolving the joint
feature representation C1 with a 1 × 1 convolution followed
by a softmax operation over the 14 × 14 grid,
a1 = softmax(Wc1 ∗ C1 + bc1),
(9)
where Wc1 ∈ R1200×1×1 and bc1 ∈ R1200 are the learnable
convolution kernel parameters. The attended whole-image
feature over all spatial locations can be calculated by

v1 =

a1(i)R1(i),

(10)

14×14
(cid:88)

i

which can represent the attended whole-image visual feature
that are most related to the input question.

Figure 3: Learning to attend visual features with multi-
modal multiplicative feature embedding for (a) free-form
image regions and for (b) detection boxes.

3.3 Learning to attend detection-box visual
features with multiplicative embedding

Our second branch focuses on learning question-related de-
tection features by attending the detection-box features with
joint input feature representation. Similar to the ﬁrst branch,
we fuse the question representation, the whole-image fea-
tures, and the detection-box features for learning the at-
tention weights for detection boxes. Unlike previous detec-
tion based attention methods (Shih, Singh, and Hoiem 2016;
Li and Jia 2016), our proposed attention mechanism in-
tegrates whole-image features for better understanding the
overall image contents. The structure for learning the joint
feature representation is shown in Figure 3(b).

Similar to the joint representation for free-form image re-
gion attention, given the question embedding Q ∈ Rk, the
image representation R ∈ R2048×14×14, the detection rep-
resentation D ∈ R4097×19, we transform each representa-
tion to a common semantic space, and then obtain the 1200-
dimensional joint feature representation C2 as

D2 = tanh(Wd2 D + bd2),

R2 =

· 1(tanh(Wr2 R + br2 )T ),

1
196

Q2 = tanh(Wq2 Q + bq2 ),
C2 = Norm2( ˜Q2 ◦ ˜R2 ◦ D2),

(11)

(12)

(13)

(14)

where Wd2, Wr2, Wq2 , bd1 , br1, bq2 are the learnable pa-
rameters for linear transformation. The transformed ques-
tion feature Q2 and whole-image feature R2 are replicated
across the number dimension to match the dimension of the
transformed detection features D2 ∈ R1200×19 before cal-
culating the joint representation C2 following Eq. (14).

The ﬁnal attended detection representation v2 over all de-

tection boxes is deﬁned as

a2 = softmax(Wc2C2 + bc2 ),

v2 =

a2(i)D2(i),

19
(cid:88)

i

(15)

(16)

where Wc2 ∈ R19×1200 and bc2 ∈ R19 are the learnable
parameters of learning the attention weights a2 for the de-
tection boxes.

3.4 Learning for answer prediction
Similar to existing VQA approaches (Antol et al. 2015; Lu et
al. 2016; Fukui et al. 2016), we model VQA as a multi-class
classiﬁcation problem. Given the attended whole-image fea-
ture v1, attended detection feature v2 with the input ques-
tion feature Q, question-image joint encodings are obtained
by element-wise multiplication of the transformed question
features and the attended features from both branch,

hr = v1 ◦ tanh(WhrQ + bhr),
hd = v2 ◦ tanh(WhdQ + bhd),

(17)
(18)

where Whr, Whd, bhr, bhd are the learnable parameters for
transforming the input question feature Q. The reason why
we choose different question transformation parameters for
the two branches is that the attended visual features from the
two different branches captures different information from
the input image. One is from attended free-form region fea-
tures and is able to capture the global context and attributes
of the scene. The another is from attended detection features
and is able to extract information about foreground objects.
After merging question-image encodings from the two
branches via addition, a linear classiﬁer is trained for ﬁnal
answer prediction,

pans = softmax(Wp(hr + hd) + bp),

(19)

where Wp and bp are the classiﬁer parameters, and pans rep-
resents the probability of the ﬁnal answer prediction.

4 Experiments

4.1 Datasets and evaluation metrics
We evaluate the proposed model on two public datasets, the
VQA (Antol et al. 2015) and COCO-QA (Ren, Kiros, and
Zemel 2015) datasets.

VQA dataset (Antol et al. 2015) is based on Microsoft
COCO image data (Lin et al. 2014). The dataset con-
sists of 248,349 training questions, 121,512 validation ques-
tions and 244,302 testing questions, generated on a total of
123,287 images. There are three types of questions includ-
ing yes/no, number and other. For each question, 10 free-
response answers are provided. We take the top 2000 most
frequent answers as the candidate outputs for learning the
classiﬁcation model similar to (Kim et al. 2016), which cov-
ers 90.45% answers in the training and validation sets.

COCO-QA dataset (Ren, Kiros, and Zemel 2015) is cre-
ated based on the caption annotations of Microsoft COCO
dataset (Lin et al. 2014). There are 78,736 training samples
and 38,948 testing samples, respectively. It contains four

types of questions, object, number, color and location. All
of the answers are single-words and considered as the valid
answers, namely 430 answers, which are used for the possi-
ble answer classiﬁcation.

Since we formulate VQA as a classiﬁcation task, we use
the accuracy metric to measure the performances of dif-
ferent models on both two datasets. In particular, for the
VQA dataset, a predicted answer is regarded as correct if it
matches more than three in ten ground truth answers. WUPS
calculates the similarity between two words based on their
common subsequence in the taxonomy tree. In addition, Wu-
Palmer similarity (WUPS) (Wu and Palmer 1994) is also
reported for the COCO-QA dataset. Same as the previous
work (Lu et al. 2016; Li and Jia 2016), we report the WUPS
scores with the thresholds of 0.9 and 0.0.

Implementation details

4.2
For encoding questions, the length of questions is ﬁxed to 26
and each word embedding is a vector of size 620. The hid-
den state of GRU is set to 2400. Given the question, image
and detection representations, the joint feature embedding of
these inputs h is set as 1200. Following (Kim et al. 2017),
we take two glimpses of each attention map, that is to say,
for each branch, another set of attention weights a1 is trained
for whole-image features R1 in the ﬁrst branch and another
set of a2 for detection features D2 in the second branch. The
two sets of attended features are concatenated in each branch
as the ﬁnal feature.

We implement our model with the Torch library. The RM-
SProp method is used for training our network with an initial
learning rate of 3×10−4, a momentum of 0.99 and a weight-
decay of 10−8. The batch size is set to 300, and is trained for
250,000 iterations. The validation process is performed ev-
ery 10,000 iterations with early stopping when validation ac-
curacy stops improving for more than 5 validations. Dropout
is applied after every linear transformation and gradient clip-
ping techniques are used for regularization in GRU training.
Multi-GPU parallel technology is adopted to accelerate the
training process.

4.3 Comparison with state-of-the-arts
Table 1 shows results on the VQA test set for both open-
ended and multiple-choice tasks by our proposed approach
and compared methods. The approaches shown in Table 1
are trained on the train+val split of the VQA dataset and
evaluated on the test split, where the test-dev set is normally
used for validation and the test-std set for standard testing.
The models in the ﬁrst part of Table 1 are based on sim-
ple joint question-image feature embedding methods. Mod-
els in the second part of Table 1 employ detection-based
attention mechanism and models in the third part use free-
form region-based attention mechanism. We only compare
results of the single models since most approaches do not
adopt the model ensemble strategy. We can see that our ﬁ-
nal model (denoted as Dual-MFA, where MFA stands for
Multiplicative Feature Attention) improves the state-of-the-
art MLB approach (Kim et al. 2017) from 65.07% to 66.09%
for the open-ended task, and from 68.89% to 69.97% for the
multiple-choice task on the test-std set. Speciﬁcally, in the

MC

All

57.57
61.97
62.69
64.18
-

62.43
65.43

-
-
66.33
66.07
67.80
-
68.89

-
69.97

36.80
42.62
42.24
46.10
48.33

-
-

43.48
46.42
49.41
51.95
53.20
-
54.77

-
56.89

59.01
56.34

59.18
59.01
58.93

59.82
58.35
58.53

55.57
53.99
54.89

59.18
57.40
58.16

59.07
59.82

Validation

Method

All

Y/N

Num. Other

All

Y/N

Num. Other

Test-dev

Open-Ended

Test-std

Open-Ended

LSTM Q+I (Antol et al. 2015)
iBOWING (Zhou et al. 2015)
DPPnet (Noh, Hongsuck Seo, and Han 2016)
FDA (Ilievski, Yan, and Feng 2016)
DMN+ (Xiong, Merity, and Socher 2016)

Region Sel. (Shih, Singh, and Hoiem 2016)
QRU (Li and Jia 2016)

SMem (Xu and Saenko 2016)
SAN (Yang et al. 2016)
MRN (Kim et al. 2016)
HieCoAtt (Lu et al. 2016)
VQA-Machine (Wang et al. 2017)
MCB (Fukui et al. 2016)
MLB (Kim et al. 2017)

Dual-MLB (our baseline)
Dual-MFA (ours)

53.74
55.72
57.22
59.24
60.30

-
60.72

57.99
58.70
61.68
61.80
63.10
64.70
64.89

65.12
66.01

78.94
76.55
80.71
81.14
80.50

-
82.29

80.87
79.30
82.28
79.70
81.50
82.50
84.13

83.32
83.59

35.24
35.03
37.24
36.16
36.80

-
37.02

37.32
36.60
38.82
38.70
38.40
37.60
37.85

39.96
40.18

36.42
42.62
41.69
45.77
48.30

-
47.67

43.12
46.10
49.25
51.70
53.00
55.60
54.57

55.26
56.84

MC

All

57.17
61.68
62.48
64.01
-

62.44
65.43

-
-
66.15
65.80
67.70
69.10
-

69.55
70.04

54.06
55.89
57.36
59.54
60.36

-
60.76

58.24
58.85
61.84
62.06
63.30
-
65.07

-
66.09

79.01
76.76
80.28
81.34
80.43

-
-

80.80
79.11
82.39
79.95
81.40
-
84.02

-
83.37

35.55
34.98
36.92
35.67
36.82

-
-

37.53
36.41
38.23
38.22
38.20
-
37.90

-
40.39

Table 1: Evaluation results by our proposed method and compared methods on the VQA dataset.

question types of number and other, our proposed approach
brings 2.49% and 2.12% improvements on the test-std set.
QRU (Li and Jia 2016) is the state-of-the-art detection-based
attention method, and our model signiﬁcantly outperforms it
by 5.33% on the test-std set. We also compare our approach
with a baseline model Dual-MLB, which consists of two at-
tention branches based on the MLB tensor fusion module.
The baseline model fuses the question and free-form image
region embedding by MLB in one branch, and fuses ques-
tion and image detection embedding by MLB in another
branch. The baseline Dual-MLB also outperforms MLB by
0.23% on the test-dev set, which shows that our improve-
ments are not only from the integration of the two attention
mechanisms, but are also caused by effective joint feature
embedding of question, image, and detection features.

Table 3 compares our approach with the state-of-the-art
approaches on the COCO-QA test set. Our ﬁnal model Dual-
MFA improves the state-of-the-art HieCoAtt (Lu et al. 2016)
from 65.40% to 66.49%. In particular, our model achieves
an improvement of 2.99% for the question type color. Sim-
ilar to the results on the VQA dataset, our model signif-
icantly outperforms the state-of-the-art detection-based at-
tention method QRU by 3.49%.

4.4 Ablation study

In this section, we conduct ablation experiments to study ef-
fectiveness of individual component designs in our model.
Table 2 shows the results of baseline models replacing dif-
ferent components in our model, which are trained on the
VQA training set and tested on the validation set. Following
other compared approaches, the test set is not used in the
study due to online submission restrictions. Speciﬁcally, we
compare different multi-modal feature embedding designs
and investigate the roles of the two attention mechanisms.

Method

MFA-MUL*
MFA-ADD

MFA-Norm*
MFA w/o Norm
MFA-Power

Dual-MFA-ADD*
Dual-MFA-MUL
Dual-MFA-CAT

MFA-D*
QRU-D (Li and Jia 2016)
MLB-D (Kim et al. 2017)

MFA-R*
MLB-R (Kim et al. 2017)
MUTAN (Ben-Younes et al. 2017)

Dual-MLB
Dual-MFA* (full model)

Table 2: Ablation study on the VQA dataset, where “*” de-
notes our model’s design.

The ﬁrst part of Table 2 shows different element-wise op-
erations used for joint embedding of three input features.
Element-wise multiplication (denoted as MFA-MUL) in Eq.
(8) performs better than element-wise addition (denoted as
MFA-ADD) by 3.67%. The second part of Table 2 shows L2
normalization (denoted as MFA-Norm) in joint feature em-
bedding works better than the model with unsigned power
operation of 1/3 (denoted as MFA-Power) and the model
without normalization (denoted as MFA w/o Norm). For
fusing outputs hr and hd from the two attention branches,
element-wise addition in Eq. (19) achieves better perfor-

Method

All

Obj.

Num. Color

Loc. WUPS0.9 WUPS0.0

2VIS+BLSTM (Ren, Kiros, and Zemel 2015)
IMG-CNN (Ma, Lu, and Li 2016)
DDPnet (Noh, Hongsuck Seo, and Han 2016)
SAN (Yang et al. 2016)
QRU (Li and Jia 2016)
HieCoAtt (Lu et al. 2016)

55.09
58.40
61.16
61.60
62.50
65.40

58.17
-
-
65.40
65.06
68.00

44.79
-
-
48.60
46.90
51.00

49.53
-
-
57.90
60.50
62.90

47.34
-
-
54.00
56.99
58.80

Dual-MFA (ours)

66.49

68.86

51.32

65.89

58.92

65.34
68.50
70.84
71.60
72.58
75.10

76.15

88.64
89.67
90.61
90.90
91.62
92.00

92.29

Table 3: Evaluation results by our proposed method and compared methods on the COCO QA dataset.

Q: What sport is this?
A: tennis

Q: What is the color of the surfboard?
A: white

Q: Is it a sunny day?
A: yes

Q: How many giraffes are there?
A: 5

Q: What does the red circle sign mean?
A: no parking

(a)

(b)

(c)

(d)

(e)

Figure 4: Visualization examples on the VQA test-dev set. (First row) input images. (Second row) free-form region based
attention maps. (Third row) detection-based attention maps.

mance than element-wise multiplication (denoted as Dual-
MFA-MUL) and concatenation of two vectors (denoted as
Dual-MFA-CAT).

The third and fourth parts of Table 2 compare our multi-
modal multiplicative feature embedding for a single atten-
tion mechanism (MFA-R or MFA-D) with detection-based
attention baseline models (QRU-D and MLB-D) and region-
based attention baseline models (MUTAN and MLB-R),
which replace our multiplicative feature embedding with
QRU (Li and Jia 2016) or MLB (Kim et al. 2017) respec-
tively. The MUTAN (Ben-Younes et al. 2017) approach
is also used for comparison. Results show that our mod-
els with a single attention branch outperform all com-
pared detection-based and region-based baselines. Finally,
we compare our ﬁnal model Dual-MFA with the base-
line model Dual-MLB. Our Dual-MFA model beneﬁts from
its effective multi-modal multiplicative feature embedding
scheme and achieves an improvement of 0.75%.

The parameter size of our ﬁnal full model is 82.6M, while
the best region-based model (MFA-R) is 61.7M, and the
best detection-based model (MFA-D) is 56.8M. As the pa-
rameters in the language model and the classiﬁer are shared

by two attention branches, parameters of our full model in-
crease in a minor degree.

4.5 Qualitative evaluation
We visualize some co-attention maps generated by our
model in Figure 4 and present ﬁve examples from the
VQA test set. Figures 4(a) and (b) show that our model at-
tends to the corresponding image regions with two attention
branches, which leads to correct answers with higher conﬁ-
dence. There are also cases where only one attention branch
is able to attend the correct image region to obtain the cor-
rect answer. In Figure 4(c), the free-form region based at-
tention map is able to attend the blue sky and dry ground,
while the detection-based attention map fails as there are no
pre-given detection boxes covering these image regions. In
Figure 4(d), the detection-based attention map has higher
weights on all ﬁve giraffes to generate the correct answer,
while the free-form attention map attends incorrect regions.
A failure case is also presented in Figure 4(e). The model
fails to generate the correct answer as the classiﬁcation label
“no turning right” does not exit in the training set despite
attending the correct image region.

5 Conclusion
In this paper, we propose a novel deep neural network
with co-attention mechanism for visual question answering.
The deep model contains two branches for visual attention
which aim to select the free-form image regions and detec-
tion boxes most related to the input question. We generate
visual attention weights by a novel multiplicative embed-
ding scheme, which fuses the question, whole-image and
detection-box features effectively. Ablation study demon-
strates the effectiveness of individual components of our
proposed model. Experimental results on two large VQA
datasets show that our proposed model outperforms state-
of-the-art approaches.

Acknowledgement
This work was supported in part by National Natural
Science Foundation of China under Grant No. 61532010
and No. 61702190, National Basic Research Program of
China (973 Program) under Grant No. 2014CB340505,
in part by SenseTime Group
and SHMEC (16CG24),
Limited, in part by the General Research Fund sponsored
by the Research Grants Council of Hong Kong (Nos.
CUHK14205615,
CUHK14206114,
CUHK14213616,
CUHK14239816,
CUHK14203015,
CUHK419412,
CUHK14207814), in part by the Hong Kong Innovation and
Technology Support Programme Grant ITS/121/15FX. We
also thank Tong Xiao, Kun Wang and Kai Kang for helpful
discussions.

References
Antol, S.; Agrawal, A.; Lu, J.; Mitchell, M.; Batra, D.;
Lawrence Zitnick, C.; and Parikh, D. 2015. Vqa: Visual ques-
tion answering. In ICCV.
Ben-Younes, H.; Cad`ene, R.; Thome, N.; and Cord, M. 2017.
Mutan: Multimodal tucker fusion for visual question answer-
ing. In ICCV.
Cho, K.; van Merri¨enboer, B.; G¨ulc¸ehre, C¸ .; Bahdanau, D.;
Bougares, F.; Schwenk, H.; and Bengio, Y. 2014. Learning
phrase representations using rnn encoder–decoder for statisti-
cal machine translation. In EMNLP.
Fukui, A.; Park, D. H.; Yang, D.; Rohrbach, A.; Darrell, T.; and
Rohrbach, M. 2016. Multimodal compact bilinear pooling for
visual question answering and visual grounding. In EMNLP.
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual
learning for image recognition. In CVPR.
Ilievski, I.; Yan, S.; and Feng, J. 2016. A focused dynamic
attention model for visual question answering. arXiv preprint
arXiv:1604.01485.
Karpathy, A., and Fei-Fei, L. 2015. Deep visual-semantic align-
ments for generating image descriptions. In CVPR.
Kim, J.-H.; Lee, S.-W.; Kwak, D.; Heo, M.-O.; Kim, J.; Ha, J.-
W.; and Zhang, B.-T. 2016. Multimodal residual learning for
visual qa. In NIPS.
Kim, J.-H.; On, K. W.; Lim, W.; Kim, J.; Ha, J.-W.; and Zhang,
B.-T. 2017. Hadamard product for low-rank bilinear pooling.
In ICLR.

Kiros, R.; Zhu, Y.; Salakhutdinov, R. R.; Zemel, R.; Urtasun,
R.; Torralba, A.; and Fidler, S. 2015. Skip-thought vectors. In
NIPS.
Li, R., and Jia, J. 2016. Visual question answering with question
representation update (qru). In NIPS.
Li, S.; Xiao, T.; Li, H.; Zhou, B.; Yue, D.; and Wang, X. 2017a.
Person search with natural language description. In CVPR.
Li, Y.; Duan, N.; Zhou, B.; Chu, X.; Ouyang, W.; and Wang,
X. 2017b. Visual question generation as dual task of visual
question answering. arXiv preprint arXiv:1709.07192.
Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-
manan, D.; Doll´ar, P.; and Zitnick, C. L. 2014. Microsoft coco:
Common objects in context. In ECCV.
Lu, J.; Yang, J.; Batra, D.; and Parikh, D. 2016. Hierarchical
question-image co-attention for visual question answering. In
NIPS.
Ma, L.; Lu, Z.; and Li, H. 2016. Learning to answer questions
from image using convolutional neural network. In AAAI.
Malinowski, M.; Rohrbach, M.; and Fritz, M. 2015. Ask your
neurons: A neural-based approach to answering questions about
images. In ICCV.
Mostafazadeh, N.; Misra, I.; Devlin, J.; Mitchell, M.; He, X.;
and Vanderwende, L. 2016. Generating natural questions about
an image. In ACL.
Noh, H.; Hongsuck Seo, P.; and Han, B. 2016. Image question
answering using convolutional neural network with dynamic
parameter prediction. In CVPR.
Ren, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster r-cnn: To-
wards real-time object detection with region proposal networks.
In NIPS.
Ren, M.; Kiros, R.; and Zemel, R. 2015. Exploring models and
data for image question answering. In NIPS.
Shih, K. J.; Singh, S.; and Hoiem, D. 2016. Where to look:
Focus regions for visual question answering. In CVPR.
Wang, P.; Wu, Q.; Shen, C.; and Hengel, A. v. d. 2017. The
vqa-machine: Learning how to use existing vision algorithms
to answer new questions. In CVPR.
Wu, Z., and Palmer, M. 1994. Verbs semantics and lexical
selection. In ACL.
Xie, L.; Shen, J.; and Zhu, L. 2016. Online cross-modal hashing
for web image retrieval. In AAAI.
Xiong, C.; Merity, S.; and Socher, R. 2016. Dynamic memory
networks for visual and textual question answering. In ICML.
Xu, H., and Saenko, K. 2016. Ask, attend and answer: Ex-
ploring question-guided spatial attention for visual question an-
swering. In ECCV.
Yang, Z.; He, X.; Gao, J.; Deng, L.; and Smola, A. 2016.
Stacked attention networks for image question answering. In
CVPR.
Ye, Y.; Zhao, Z.; Li, Y.; Chen, L.; Xiao, J.; and Zhuang, Y.
2017. Video question answering via attribute-augmented at-
tention network learning. In ACM Multimedia.
Zhou, B.; Tian, Y.; Sukhbaatar, S.; Szlam, A.; and Fergus, R.
2015. Simple baseline for visual question answering. arXiv
preprint arXiv:1512.02167.
Zitnick, C. L., and Doll´ar, P. 2014. Edge boxes: Locating object
proposals from edges. In ECCV.

Co-attending Free-form Regions and Detections with Multi-modal Multiplicative
Feature Embedding for Visual Question Answering

Pan Lu† Hongsheng Li‡∗ Wei Zhang(cid:92)

Jianyong Wang† Xiaogang Wang‡∗

† Tsinghua National Laboratory for Information Science and Technology (TNList)
Department of Computer Science, Tsinghua University
‡ Department of Electronic Engineering, The Chinese University of Hong Kong
(cid:92) Shanghai Key Laboratory of Trustworthy Computing, East China Normal University

{lupantech, zhangwei.thu2011}@gmail.com, {hsli, xgwang}@ee.cuhk.edu.hk,

jianyong@mail.tsinghua.edu.cn

7
1
0
2
 
c
e
D
 
2
1
 
 
]

V
C
.
s
c
[
 
 
2
v
4
9
7
6
0
.
1
1
7
1
:
v
i
X
r
a

Abstract

Recently, the Visual Question Answering (VQA) task has
gained increasing attention in artiﬁcial intelligence. Exist-
ing VQA methods mainly adopt the visual attention mech-
anism to associate the input question with corresponding
image regions for effective question answering. The free-
form region based and the detection-based visual attention
mechanisms are mostly investigated, with the former ones
attending free-form image regions and the latter ones at-
tending pre-speciﬁed detection-box regions. We argue that
the two attention mechanisms are able to provide comple-
mentary information and should be effectively integrated
to better solve the VQA problem. In this paper, we pro-
pose a novel deep neural network for VQA that integrates
both attention mechanisms. Our proposed framework effec-
tively fuses features from free-form image regions, detec-
tion boxes, and question representations via a multi-modal
multiplicative feature embedding scheme to jointly attend
question-related free-form image regions and detection boxes
for more accurate question answering. The proposed method
is extensively evaluated on two publicly available datasets,
COCO-QA and VQA, and outperforms state-of-the-art ap-
proaches. Source code is available at https://github.
com/lupantech/dual-mfa-vqa.

1

Introduction

In recent years, multi-modal learning for language and vi-
sion has gained much attention in artiﬁcial intelligence.
Great progress has been achieved for different tasks in-
cluding image captioning (Karpathy and Fei-Fei 2015), vi-
sual question generation (Mostafazadeh et al. 2016; Li et
al. 2017b), video question answering (Ye et al. 2017) and
text-to-image retrieval (Xie, Shen, and Zhu 2016; Li et al.
2017a). The Visual Question Answering (VQA) (Antol et
al. 2015) task has recently emerged as a more challenging
task. The algorithms are required to answer natural language
questions about a given image’s contents. Compared with
the conventional visual-language tasks such as image cap-
tioning and text-to-image retrieval, the VQA task requires
the algorithms to have a better understanding on both the
input image and question in order to infer the answer.

∗Co-corresponding authors.

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Figure 1: Co-attending free-form regions and detection
boxes based on the question, the whole image, and detec-
tion boxes for better utilizing complementary information to
solve the VQA task.

State-of-the-art VQA approaches utilize visual attention
mechanism to relate the question to meaningful image re-
gions for accurate question answering. Most visual attention
mechanisms in VQA can be categorized into free-form re-
gion based methods (Lu et al. 2016; Fukui et al. 2016) and
detection-based methods (Li and Jia 2016; Shih, Singh, and
Hoiem 2016). For the free-form region based methods, the
question features learned by a Long Short-Term Memory
(LSTM) network and the image features learned by a Con-
volutional Neural Network (CNN) are fused by either ad-
dictive, multiplicative or concatenation operations at every
image spatial location. The free-form attention map is ob-
tained by applying a softmax non-linearity operation across
the fused feature map. Since there is no restriction on the ob-
tained attention map, the free-form attention region is able
to attend both global visual context and speciﬁc foreground
objects for inferring answers. However, since there is no re-
striction, the free-form attentive regions might focus on par-
tial objects or irrelevant context sometimes. For instance, for
an question, “What animal do you see?”, a free-form re-
gion attention map might mistakenly focus on only a part
of the foreground cat and generate an answer of “dog”. On
the other hand, for the detection-based attention methods,
the attention mechanism is utilized for relating the question
to pre-speciﬁed detection boxes (e.g., Edge boxes (Zitnick

and Doll´ar 2014)). Instead of applying the softmax operation
over all image spatial locations, the operation is calculated
over all detection boxes. Therefore, the attended regions are
restricted to pre-speciﬁed detection-box regions and such
question-related regions could be more effective for answer-
ing questions about foreground objects. However, such re-
strictions also cause challenges for other types of questions.
For instance, for the question “How is the weather today?”,
there might not exist a detection box in the sky, resulting in
a failure in answering this question.

For better understanding the question and image con-
tents and their relations, a good VQA algorithm needs to
identify global scene attributes, locate objects, identify ob-
ject attributes, quantity and categories to make accurate in-
ference. We argue that the above mentioned two types of
attended mechanisms provide complementary information
and should be effectively integrated in a uniﬁed framework
to take advantages of both the attended free-form regions
and attended detection regions. Take the above mentioned
two questions as examples, the question about animal could
be more effectively answered with detection-based attention
maps, while the question about the weather can be better an-
swered with the free-form region based attention maps.

In this paper, we propose a novel dual-branch deep neu-
ral network for solving the VQA problem that combines
free-form region based and detection-based attention mech-
anisms (see Figure 1). The overall framework consists of two
attention branches, each of which associates the question
with the most relevant free-form regions or with the most
relevant detection regions in the input image. For obtaining
more question-related attention weights for both types of re-
gions, we propose to learn the joint feature representation
of the input question, the whole image, and the detection
boxes with a multiplicative feature embedding scheme. Such
a multiplicative scheme does not share parameters between
the two branches and is shown to result in more robust an-
swering performance than existing methods.

The main contributions of our work can be summarized

as twofold.
• We propose a novel dual-branch deep neural network
that effectively integrates the free-form region based and
detection-based attention mechanisms in a uniﬁed frame-
work;

• In order to better fuse features from different modalities,
a novel multiplicative feature embedding scheme is pro-
posed to learn joint feature representations from the ques-
tion, the whole image, and the detection boxes.

2 Related work
State-of-the-art VQA algorithms are mainly based on deep
neural networks for learning visual-question features and for
predicting the ﬁnal answers. Due to the establishment of the
VQA dataset and the online evaluation platform by (Antol et
al. 2015), there is an increasing number of VQA algorithms
proposed every year.

Multi-modal feature embedding for VQA. The existing
joint feature embedding methods for VQA typically com-
bine the visual and question features learned by deep neural

networks and then solve the task as a multi-class classiﬁca-
tion problem. (Zhou et al. 2015) proposed a simple baseline,
which learns image features with CNN and question features
from LSTM, and concatenated these two features to predict
the answer. Instead of using LSTM for learning question
representations, (Noh, Hongsuck Seo, and Han 2016) used
GRU (Cho et al. 2014) and (Ma, Lu, and Li 2016) trained
CNN for question embedding. Different from the above
mentioned methods addressing the VQA task as a classiﬁca-
tion problem, the work by (Malinowski, Rohrbach, and Fritz
2015) fed both image CNN features and question represen-
tations into an LSTM to generate the answer by sequence-to-
sequence learning. There are also high-order approaches for
multi-modal feature embedding. MCB (Fukui et al. 2016)
and MLB (Kim et al. 2017) both designed bilinear pool-
ing approaches to learn multi-modal feature embedding for
VQA. In (Ben-Younes et al. 2017), a generalized multi-
modal pooling framework is proposed, which shows that
MCB and MLB are its special cases. Inspired by ResNet
(He et al. 2016), (Kim et al. 2016) proposed element-wise
multiplication for the joint residual mappings.

Attention mechanism for VQA. A large quantity of re-
cent works focused on incorporating attention mechanisms
for solving the VQA task. In (Xu and Saenko 2016), the at-
tention weights over different image regions are calculated
based on the semantic similarity between the question and
image regions, and the updated question features are ob-
tained as the weighted sum of the different regions. (Yang et
al. 2016) proposed a multi-stage attention framework, which
stacks the attention modules to search question-related im-
age regions by iterative feature fusion. The attention mech-
anism is not limited to image modality, (Lu et al. 2016) pro-
posed a co-attention mechanism that simultaneously attends
both the question and image with joint visual-question fea-
ture representations.

Since some of the questions are related to objects in the
images, object detection results are explored to replace the
visual features obtained from the whole-image region. (Shih,
Singh, and Hoiem 2016) utilized the attention mechanism to
generate visual features from 100 detection boxes for the
VQA task. Similarly, (Li and Jia 2016) proposed a frame-
work that updates the joint visual-question feature embed-
ding by iteratively attending the top detection boxes.

However, all the attention-based methods focus on one
type of image regions for question-image association (i.e.,
either free-form image regions or detection boxes), and each
of them have limitations on solving certain types of ques-
tions. In contrast, in order to better utilize the complemen-
tary information from both types of image regions, our pro-
posed approach effectively integrates both attention mecha-
nisms in a uniﬁed framework.

3 Method
The overall structure of our proposed deep neural network is
illustrated in Figure 2, which takes the question, the whole
image, and the detection boxes as inputs, and learns to as-
sociate questions to free-form image regions and detection
boxes simultaneously to infer the answer. Our proposed
model for VQA consists of two co-attention branches for

Figure 2: Illustration of the overall network structure for solving the VQA task. The network has two attention branches with
the proposed multiplicative featucre embedding scheme, where one branch attends free-form image regions and another branch
attends detection boxes for encoding question-related visual features.

free-form image regions and for detection boxes, respec-
tively. Before calculating the attention weights for each type
of regions, the joint feature representations of the question,
the whole-image visual features, and the detection box vi-
sual features are obtained via a multiplicative approach. In
this way, the attention weights for each type of regions are
learned from all the three inputs. Importantly, the joint multi-
modal feature embedding has different parameters for each
branch, which leads to better answering accuracy. The re-
sulting visual features from the two branches are fused with
the question representation and then added to obtain the ﬁnal
question-image embedding. A multi-class linear classiﬁer is
adopted for obtaining the ﬁnal predicted answer.

The input feature encoding will be introduced in Section
3.1. Section 3.2 introduces the branch with visual attention
mechanism for associating free-form image regions to the
input questions with our multiplicative feature embedding
scheme, and Section 3.3 describes the another branch with
detection based attention mechanism. The ﬁnal answer pre-
diction will be introduced in Section 3.4.

3.1

Input feature encoding

Encoding whole-image features. We utilize an ImageNet
pre-trained ResNet-152 (He et al. 2016) for learning im-
age visual features. The input image to ResNet is resized to
448 × 448 and the 2048 × 14 × 14 output feature map of the
last convolution layer is used to encode the whole-image vi-
sual features R ∈ R2048×14×14, where 14 × 14 is its spatial
size and 2048 is the number of visual feature channels.

Encoding detection-box features. We adopt the Faster-
RCNN (Ren et al. 2015) framework to obtain object detec-
tion boxes in the input image. For all the object proposals
and their associated detection scores generated by Faster-
RCNN, the non-maximum suppression with an intersection
over union (IoU) 0.3 is applied and the top-ranked 19 detec-
tion boxes are chosen as the detection-box features for our
overall framework. The 4096-dimensional visual features of
Faster-RCNN’s fc7 layer concatenated with boxes’ detec-
tion scores are utilized to encode the visual feature of each

detection box. Let D = [f1, · · · , f19] ∈ R4097×19 denote
the visual features of the top-ranked 19 detection boxes.

Encoding question features. The Gated Recurrent Unit
(GRU) (Cho et al. 2014) is adopted to encode the question
features, which show its effectiveness in recent VQA meth-
ods (Lu et al. 2016; Kim et al. 2017). A GRU cell consists
of an update gate z and a reset gate r. Given a question
q = [q1, ..., qT ], where qt is the one-hot vector of at po-
sition t, and T is the length of the question. We convert
each word qt into a feature vector via a linear transforma-
tion xt = Weqt. At each time step, the word feature vector
qt is sequentially fed into the GRU to encode the input ques-
tion. At each step, the GRU updates the update gate zt and
reset gate rt and outputs a hidden state ht. The GRU update
process operates as

(1)
(2)

(3)

zt = σ(Wzxt + Uzht−1 + bz),
rt = σ(Wrxt + Urht−1 + br),
˜ht = tanh(Whxt + Uh(rt ◦ ht−1) + bh),
ht = zt ◦ ht−1 + (1 − zt) ◦ ˜ht,

(4)
where σ represents the sigmoid activation function. The
weight matrices W , U and bias vector b are learnable pa-
rameters of the GRU.

We take the ﬁnal hidden state hT as the question embed-
ding, i.e., Q = hT ∈ Rk, where k denotes the embedding
length. Following (Li and Jia 2016; Kim et al. 2017), we uti-
lize the pre-trained skip-thought model (Kiros et al. 2015) to
initialize the embedding matrix We in our question language
model. Since the skip-thought model is previously trained on
large text corpus, we are able to transfer external language-
based knowledge to our VQA task by ﬁne-tuning the GRU
from the initial point.

3.2 Learning to attend free-form region visual
features with multiplicative embedding
Our proposed framework has two branches, one for attend-
ing question-related free-form image regions to learn whole-
image features and the other for attending detection-box re-
gions to learn question-related detection features. For each

attention branch, it takes question feature Q, whole-image
feature R and detection-box feature D as inputs, and outputs
the question-attended visual features v1 and v2 for question
answering.

Our free-form region attention branch tries to associate
the input question to relevant regions of the input image.
There is no restriction to the attended regions, which are of
free-form and are able to capture the global visual context
and attributes of the image. Unlike existing VQA methods
that fuse question and image features via simple concatena-
tion (Shih, Singh, and Hoiem 2016) or addition (Lu et al.
2016) to guide the calculation of the attention weights, each
of our attention branch fuses all three types of input features
via a multiplicative feature embedding scheme for utilizing
full information from the inputs. The network structure for
attending visual features with our multiplicative feature em-
bedding scheme is show in Figure 3(a).

Given the question embedding Q ∈ Rk, the whole-image
representation R ∈ R2048×14×14, and the detection repre-
sentation D ∈ R4097×19, we ﬁrst embed them to a 1200-
dimensional common space via the following equations,

R1 = tanh(Wr1R + br1),

D1 =

· 1(tanh(Wd1D + bd1)T ),

1
19

(5)

(6)

Q1 = tanh(Wq1 Q + bq1 ),

(7)
where Wr1 ∈ R1200×2408, Wd1 ∈ R1200×4097, Wq1 ∈
R1200×k are the learnable weight parameters, br1 , bd1 , bq1
∈ R1200 are the bias parameters, 1 ∈ R19 represents an all-
1 vector, and tanh is the hyperbolic tangent function. For
learning the attention weights for the whole-image feature
R, the transformed detection features are averaged across
all detection boxes following Eq. (6) before feature fusion.
After mapping all input features into the 1200-dimensional
common space, the detection feature D1 ∈ R1200 and ques-
tion feature Q1 ∈ R1200, are spatially replicated to a 14×14
grid to form ˜D1 and ˜R1, which match the spatial size of the
whole-image feature R1 ∈ R1200×14×14.

The joint feature representation C1 of the three inputs is
obtained by element-wise multiplication (Hadamard prod-
uct) of ˜Q1, R1 and ˜D1, and followed by a L2 normalization
to constrain the magnitude of the representation,

C1 = Norm2( ˜Q1 ◦ R1 ◦ ˜D1),
(8)
where ◦ indicates element-wise multiplication. The free-
form attention map is then obtained by convolving the joint
feature representation C1 with a 1 × 1 convolution followed
by a softmax operation over the 14 × 14 grid,
a1 = softmax(Wc1 ∗ C1 + bc1),
(9)
where Wc1 ∈ R1200×1×1 and bc1 ∈ R1200 are the learnable
convolution kernel parameters. The attended whole-image
feature over all spatial locations can be calculated by

v1 =

a1(i)R1(i),

(10)

14×14
(cid:88)

i

which can represent the attended whole-image visual feature
that are most related to the input question.

Figure 3: Learning to attend visual features with multi-
modal multiplicative feature embedding for (a) free-form
image regions and for (b) detection boxes.

3.3 Learning to attend detection-box visual
features with multiplicative embedding

Our second branch focuses on learning question-related de-
tection features by attending the detection-box features with
joint input feature representation. Similar to the ﬁrst branch,
we fuse the question representation, the whole-image fea-
tures, and the detection-box features for learning the at-
tention weights for detection boxes. Unlike previous detec-
tion based attention methods (Shih, Singh, and Hoiem 2016;
Li and Jia 2016), our proposed attention mechanism in-
tegrates whole-image features for better understanding the
overall image contents. The structure for learning the joint
feature representation is shown in Figure 3(b).

Similar to the joint representation for free-form image re-
gion attention, given the question embedding Q ∈ Rk, the
image representation R ∈ R2048×14×14, the detection rep-
resentation D ∈ R4097×19, we transform each representa-
tion to a common semantic space, and then obtain the 1200-
dimensional joint feature representation C2 as

D2 = tanh(Wd2 D + bd2),

R2 =

· 1(tanh(Wr2 R + br2 )T ),

1
196

Q2 = tanh(Wq2 Q + bq2 ),
C2 = Norm2( ˜Q2 ◦ ˜R2 ◦ D2),

(11)

(12)

(13)

(14)

where Wd2, Wr2, Wq2 , bd1 , br1, bq2 are the learnable pa-
rameters for linear transformation. The transformed ques-
tion feature Q2 and whole-image feature R2 are replicated
across the number dimension to match the dimension of the
transformed detection features D2 ∈ R1200×19 before cal-
culating the joint representation C2 following Eq. (14).

The ﬁnal attended detection representation v2 over all de-

tection boxes is deﬁned as

a2 = softmax(Wc2C2 + bc2 ),

v2 =

a2(i)D2(i),

19
(cid:88)

i

(15)

(16)

where Wc2 ∈ R19×1200 and bc2 ∈ R19 are the learnable
parameters of learning the attention weights a2 for the de-
tection boxes.

3.4 Learning for answer prediction
Similar to existing VQA approaches (Antol et al. 2015; Lu et
al. 2016; Fukui et al. 2016), we model VQA as a multi-class
classiﬁcation problem. Given the attended whole-image fea-
ture v1, attended detection feature v2 with the input ques-
tion feature Q, question-image joint encodings are obtained
by element-wise multiplication of the transformed question
features and the attended features from both branch,

hr = v1 ◦ tanh(WhrQ + bhr),
hd = v2 ◦ tanh(WhdQ + bhd),

(17)
(18)

where Whr, Whd, bhr, bhd are the learnable parameters for
transforming the input question feature Q. The reason why
we choose different question transformation parameters for
the two branches is that the attended visual features from the
two different branches captures different information from
the input image. One is from attended free-form region fea-
tures and is able to capture the global context and attributes
of the scene. The another is from attended detection features
and is able to extract information about foreground objects.
After merging question-image encodings from the two
branches via addition, a linear classiﬁer is trained for ﬁnal
answer prediction,

pans = softmax(Wp(hr + hd) + bp),

(19)

where Wp and bp are the classiﬁer parameters, and pans rep-
resents the probability of the ﬁnal answer prediction.

4 Experiments

4.1 Datasets and evaluation metrics
We evaluate the proposed model on two public datasets, the
VQA (Antol et al. 2015) and COCO-QA (Ren, Kiros, and
Zemel 2015) datasets.

VQA dataset (Antol et al. 2015) is based on Microsoft
COCO image data (Lin et al. 2014). The dataset con-
sists of 248,349 training questions, 121,512 validation ques-
tions and 244,302 testing questions, generated on a total of
123,287 images. There are three types of questions includ-
ing yes/no, number and other. For each question, 10 free-
response answers are provided. We take the top 2000 most
frequent answers as the candidate outputs for learning the
classiﬁcation model similar to (Kim et al. 2016), which cov-
ers 90.45% answers in the training and validation sets.

COCO-QA dataset (Ren, Kiros, and Zemel 2015) is cre-
ated based on the caption annotations of Microsoft COCO
dataset (Lin et al. 2014). There are 78,736 training samples
and 38,948 testing samples, respectively. It contains four

types of questions, object, number, color and location. All
of the answers are single-words and considered as the valid
answers, namely 430 answers, which are used for the possi-
ble answer classiﬁcation.

Since we formulate VQA as a classiﬁcation task, we use
the accuracy metric to measure the performances of dif-
ferent models on both two datasets. In particular, for the
VQA dataset, a predicted answer is regarded as correct if it
matches more than three in ten ground truth answers. WUPS
calculates the similarity between two words based on their
common subsequence in the taxonomy tree. In addition, Wu-
Palmer similarity (WUPS) (Wu and Palmer 1994) is also
reported for the COCO-QA dataset. Same as the previous
work (Lu et al. 2016; Li and Jia 2016), we report the WUPS
scores with the thresholds of 0.9 and 0.0.

Implementation details

4.2
For encoding questions, the length of questions is ﬁxed to 26
and each word embedding is a vector of size 620. The hid-
den state of GRU is set to 2400. Given the question, image
and detection representations, the joint feature embedding of
these inputs h is set as 1200. Following (Kim et al. 2017),
we take two glimpses of each attention map, that is to say,
for each branch, another set of attention weights a1 is trained
for whole-image features R1 in the ﬁrst branch and another
set of a2 for detection features D2 in the second branch. The
two sets of attended features are concatenated in each branch
as the ﬁnal feature.

We implement our model with the Torch library. The RM-
SProp method is used for training our network with an initial
learning rate of 3×10−4, a momentum of 0.99 and a weight-
decay of 10−8. The batch size is set to 300, and is trained for
250,000 iterations. The validation process is performed ev-
ery 10,000 iterations with early stopping when validation ac-
curacy stops improving for more than 5 validations. Dropout
is applied after every linear transformation and gradient clip-
ping techniques are used for regularization in GRU training.
Multi-GPU parallel technology is adopted to accelerate the
training process.

4.3 Comparison with state-of-the-arts
Table 1 shows results on the VQA test set for both open-
ended and multiple-choice tasks by our proposed approach
and compared methods. The approaches shown in Table 1
are trained on the train+val split of the VQA dataset and
evaluated on the test split, where the test-dev set is normally
used for validation and the test-std set for standard testing.
The models in the ﬁrst part of Table 1 are based on sim-
ple joint question-image feature embedding methods. Mod-
els in the second part of Table 1 employ detection-based
attention mechanism and models in the third part use free-
form region-based attention mechanism. We only compare
results of the single models since most approaches do not
adopt the model ensemble strategy. We can see that our ﬁ-
nal model (denoted as Dual-MFA, where MFA stands for
Multiplicative Feature Attention) improves the state-of-the-
art MLB approach (Kim et al. 2017) from 65.07% to 66.09%
for the open-ended task, and from 68.89% to 69.97% for the
multiple-choice task on the test-std set. Speciﬁcally, in the

MC

All

57.57
61.97
62.69
64.18
-

62.43
65.43

-
-
66.33
66.07
67.80
-
68.89

-
69.97

36.80
42.62
42.24
46.10
48.33

-
-

43.48
46.42
49.41
51.95
53.20
-
54.77

-
56.89

59.01
56.34

59.18
59.01
58.93

59.82
58.35
58.53

55.57
53.99
54.89

59.18
57.40
58.16

59.07
59.82

Validation

Method

All

Y/N

Num. Other

All

Y/N

Num. Other

Test-dev

Open-Ended

Test-std

Open-Ended

LSTM Q+I (Antol et al. 2015)
iBOWING (Zhou et al. 2015)
DPPnet (Noh, Hongsuck Seo, and Han 2016)
FDA (Ilievski, Yan, and Feng 2016)
DMN+ (Xiong, Merity, and Socher 2016)

Region Sel. (Shih, Singh, and Hoiem 2016)
QRU (Li and Jia 2016)

SMem (Xu and Saenko 2016)
SAN (Yang et al. 2016)
MRN (Kim et al. 2016)
HieCoAtt (Lu et al. 2016)
VQA-Machine (Wang et al. 2017)
MCB (Fukui et al. 2016)
MLB (Kim et al. 2017)

Dual-MLB (our baseline)
Dual-MFA (ours)

53.74
55.72
57.22
59.24
60.30

-
60.72

57.99
58.70
61.68
61.80
63.10
64.70
64.89

65.12
66.01

78.94
76.55
80.71
81.14
80.50

-
82.29

80.87
79.30
82.28
79.70
81.50
82.50
84.13

83.32
83.59

35.24
35.03
37.24
36.16
36.80

-
37.02

37.32
36.60
38.82
38.70
38.40
37.60
37.85

39.96
40.18

36.42
42.62
41.69
45.77
48.30

-
47.67

43.12
46.10
49.25
51.70
53.00
55.60
54.57

55.26
56.84

MC

All

57.17
61.68
62.48
64.01
-

62.44
65.43

-
-
66.15
65.80
67.70
69.10
-

69.55
70.04

54.06
55.89
57.36
59.54
60.36

-
60.76

58.24
58.85
61.84
62.06
63.30
-
65.07

-
66.09

79.01
76.76
80.28
81.34
80.43

-
-

80.80
79.11
82.39
79.95
81.40
-
84.02

-
83.37

35.55
34.98
36.92
35.67
36.82

-
-

37.53
36.41
38.23
38.22
38.20
-
37.90

-
40.39

Table 1: Evaluation results by our proposed method and compared methods on the VQA dataset.

question types of number and other, our proposed approach
brings 2.49% and 2.12% improvements on the test-std set.
QRU (Li and Jia 2016) is the state-of-the-art detection-based
attention method, and our model signiﬁcantly outperforms it
by 5.33% on the test-std set. We also compare our approach
with a baseline model Dual-MLB, which consists of two at-
tention branches based on the MLB tensor fusion module.
The baseline model fuses the question and free-form image
region embedding by MLB in one branch, and fuses ques-
tion and image detection embedding by MLB in another
branch. The baseline Dual-MLB also outperforms MLB by
0.23% on the test-dev set, which shows that our improve-
ments are not only from the integration of the two attention
mechanisms, but are also caused by effective joint feature
embedding of question, image, and detection features.

Table 3 compares our approach with the state-of-the-art
approaches on the COCO-QA test set. Our ﬁnal model Dual-
MFA improves the state-of-the-art HieCoAtt (Lu et al. 2016)
from 65.40% to 66.49%. In particular, our model achieves
an improvement of 2.99% for the question type color. Sim-
ilar to the results on the VQA dataset, our model signif-
icantly outperforms the state-of-the-art detection-based at-
tention method QRU by 3.49%.

4.4 Ablation study

In this section, we conduct ablation experiments to study ef-
fectiveness of individual component designs in our model.
Table 2 shows the results of baseline models replacing dif-
ferent components in our model, which are trained on the
VQA training set and tested on the validation set. Following
other compared approaches, the test set is not used in the
study due to online submission restrictions. Speciﬁcally, we
compare different multi-modal feature embedding designs
and investigate the roles of the two attention mechanisms.

Method

MFA-MUL*
MFA-ADD

MFA-Norm*
MFA w/o Norm
MFA-Power

Dual-MFA-ADD*
Dual-MFA-MUL
Dual-MFA-CAT

MFA-D*
QRU-D (Li and Jia 2016)
MLB-D (Kim et al. 2017)

MFA-R*
MLB-R (Kim et al. 2017)
MUTAN (Ben-Younes et al. 2017)

Dual-MLB
Dual-MFA* (full model)

Table 2: Ablation study on the VQA dataset, where “*” de-
notes our model’s design.

The ﬁrst part of Table 2 shows different element-wise op-
erations used for joint embedding of three input features.
Element-wise multiplication (denoted as MFA-MUL) in Eq.
(8) performs better than element-wise addition (denoted as
MFA-ADD) by 3.67%. The second part of Table 2 shows L2
normalization (denoted as MFA-Norm) in joint feature em-
bedding works better than the model with unsigned power
operation of 1/3 (denoted as MFA-Power) and the model
without normalization (denoted as MFA w/o Norm). For
fusing outputs hr and hd from the two attention branches,
element-wise addition in Eq. (19) achieves better perfor-

Method

All

Obj.

Num. Color

Loc. WUPS0.9 WUPS0.0

2VIS+BLSTM (Ren, Kiros, and Zemel 2015)
IMG-CNN (Ma, Lu, and Li 2016)
DDPnet (Noh, Hongsuck Seo, and Han 2016)
SAN (Yang et al. 2016)
QRU (Li and Jia 2016)
HieCoAtt (Lu et al. 2016)

55.09
58.40
61.16
61.60
62.50
65.40

58.17
-
-
65.40
65.06
68.00

44.79
-
-
48.60
46.90
51.00

49.53
-
-
57.90
60.50
62.90

47.34
-
-
54.00
56.99
58.80

Dual-MFA (ours)

66.49

68.86

51.32

65.89

58.92

65.34
68.50
70.84
71.60
72.58
75.10

76.15

88.64
89.67
90.61
90.90
91.62
92.00

92.29

Table 3: Evaluation results by our proposed method and compared methods on the COCO QA dataset.

Q: What sport is this?
A: tennis

Q: What is the color of the surfboard?
A: white

Q: Is it a sunny day?
A: yes

Q: How many giraffes are there?
A: 5

Q: What does the red circle sign mean?
A: no parking

(a)

(b)

(c)

(d)

(e)

Figure 4: Visualization examples on the VQA test-dev set. (First row) input images. (Second row) free-form region based
attention maps. (Third row) detection-based attention maps.

mance than element-wise multiplication (denoted as Dual-
MFA-MUL) and concatenation of two vectors (denoted as
Dual-MFA-CAT).

The third and fourth parts of Table 2 compare our multi-
modal multiplicative feature embedding for a single atten-
tion mechanism (MFA-R or MFA-D) with detection-based
attention baseline models (QRU-D and MLB-D) and region-
based attention baseline models (MUTAN and MLB-R),
which replace our multiplicative feature embedding with
QRU (Li and Jia 2016) or MLB (Kim et al. 2017) respec-
tively. The MUTAN (Ben-Younes et al. 2017) approach
is also used for comparison. Results show that our mod-
els with a single attention branch outperform all com-
pared detection-based and region-based baselines. Finally,
we compare our ﬁnal model Dual-MFA with the base-
line model Dual-MLB. Our Dual-MFA model beneﬁts from
its effective multi-modal multiplicative feature embedding
scheme and achieves an improvement of 0.75%.

The parameter size of our ﬁnal full model is 82.6M, while
the best region-based model (MFA-R) is 61.7M, and the
best detection-based model (MFA-D) is 56.8M. As the pa-
rameters in the language model and the classiﬁer are shared

by two attention branches, parameters of our full model in-
crease in a minor degree.

4.5 Qualitative evaluation
We visualize some co-attention maps generated by our
model in Figure 4 and present ﬁve examples from the
VQA test set. Figures 4(a) and (b) show that our model at-
tends to the corresponding image regions with two attention
branches, which leads to correct answers with higher conﬁ-
dence. There are also cases where only one attention branch
is able to attend the correct image region to obtain the cor-
rect answer. In Figure 4(c), the free-form region based at-
tention map is able to attend the blue sky and dry ground,
while the detection-based attention map fails as there are no
pre-given detection boxes covering these image regions. In
Figure 4(d), the detection-based attention map has higher
weights on all ﬁve giraffes to generate the correct answer,
while the free-form attention map attends incorrect regions.
A failure case is also presented in Figure 4(e). The model
fails to generate the correct answer as the classiﬁcation label
“no turning right” does not exit in the training set despite
attending the correct image region.

5 Conclusion
In this paper, we propose a novel deep neural network
with co-attention mechanism for visual question answering.
The deep model contains two branches for visual attention
which aim to select the free-form image regions and detec-
tion boxes most related to the input question. We generate
visual attention weights by a novel multiplicative embed-
ding scheme, which fuses the question, whole-image and
detection-box features effectively. Ablation study demon-
strates the effectiveness of individual components of our
proposed model. Experimental results on two large VQA
datasets show that our proposed model outperforms state-
of-the-art approaches.

Acknowledgement
This work was supported in part by National Natural
Science Foundation of China under Grant No. 61532010
and No. 61702190, National Basic Research Program of
China (973 Program) under Grant No. 2014CB340505,
in part by SenseTime Group
and SHMEC (16CG24),
Limited, in part by the General Research Fund sponsored
by the Research Grants Council of Hong Kong (Nos.
CUHK14205615,
CUHK14206114,
CUHK14213616,
CUHK14239816,
CUHK14203015,
CUHK419412,
CUHK14207814), in part by the Hong Kong Innovation and
Technology Support Programme Grant ITS/121/15FX. We
also thank Tong Xiao, Kun Wang and Kai Kang for helpful
discussions.

References
Antol, S.; Agrawal, A.; Lu, J.; Mitchell, M.; Batra, D.;
Lawrence Zitnick, C.; and Parikh, D. 2015. Vqa: Visual ques-
tion answering. In ICCV.
Ben-Younes, H.; Cad`ene, R.; Thome, N.; and Cord, M. 2017.
Mutan: Multimodal tucker fusion for visual question answer-
ing. In ICCV.
Cho, K.; van Merri¨enboer, B.; G¨ulc¸ehre, C¸ .; Bahdanau, D.;
Bougares, F.; Schwenk, H.; and Bengio, Y. 2014. Learning
phrase representations using rnn encoder–decoder for statisti-
cal machine translation. In EMNLP.
Fukui, A.; Park, D. H.; Yang, D.; Rohrbach, A.; Darrell, T.; and
Rohrbach, M. 2016. Multimodal compact bilinear pooling for
visual question answering and visual grounding. In EMNLP.
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual
learning for image recognition. In CVPR.
Ilievski, I.; Yan, S.; and Feng, J. 2016. A focused dynamic
attention model for visual question answering. arXiv preprint
arXiv:1604.01485.
Karpathy, A., and Fei-Fei, L. 2015. Deep visual-semantic align-
ments for generating image descriptions. In CVPR.
Kim, J.-H.; Lee, S.-W.; Kwak, D.; Heo, M.-O.; Kim, J.; Ha, J.-
W.; and Zhang, B.-T. 2016. Multimodal residual learning for
visual qa. In NIPS.
Kim, J.-H.; On, K. W.; Lim, W.; Kim, J.; Ha, J.-W.; and Zhang,
B.-T. 2017. Hadamard product for low-rank bilinear pooling.
In ICLR.

Kiros, R.; Zhu, Y.; Salakhutdinov, R. R.; Zemel, R.; Urtasun,
R.; Torralba, A.; and Fidler, S. 2015. Skip-thought vectors. In
NIPS.
Li, R., and Jia, J. 2016. Visual question answering with question
representation update (qru). In NIPS.
Li, S.; Xiao, T.; Li, H.; Zhou, B.; Yue, D.; and Wang, X. 2017a.
Person search with natural language description. In CVPR.
Li, Y.; Duan, N.; Zhou, B.; Chu, X.; Ouyang, W.; and Wang,
X. 2017b. Visual question generation as dual task of visual
question answering. arXiv preprint arXiv:1709.07192.
Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-
manan, D.; Doll´ar, P.; and Zitnick, C. L. 2014. Microsoft coco:
Common objects in context. In ECCV.
Lu, J.; Yang, J.; Batra, D.; and Parikh, D. 2016. Hierarchical
question-image co-attention for visual question answering. In
NIPS.
Ma, L.; Lu, Z.; and Li, H. 2016. Learning to answer questions
from image using convolutional neural network. In AAAI.
Malinowski, M.; Rohrbach, M.; and Fritz, M. 2015. Ask your
neurons: A neural-based approach to answering questions about
images. In ICCV.
Mostafazadeh, N.; Misra, I.; Devlin, J.; Mitchell, M.; He, X.;
and Vanderwende, L. 2016. Generating natural questions about
an image. In ACL.
Noh, H.; Hongsuck Seo, P.; and Han, B. 2016. Image question
answering using convolutional neural network with dynamic
parameter prediction. In CVPR.
Ren, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster r-cnn: To-
wards real-time object detection with region proposal networks.
In NIPS.
Ren, M.; Kiros, R.; and Zemel, R. 2015. Exploring models and
data for image question answering. In NIPS.
Shih, K. J.; Singh, S.; and Hoiem, D. 2016. Where to look:
Focus regions for visual question answering. In CVPR.
Wang, P.; Wu, Q.; Shen, C.; and Hengel, A. v. d. 2017. The
vqa-machine: Learning how to use existing vision algorithms
to answer new questions. In CVPR.
Wu, Z., and Palmer, M. 1994. Verbs semantics and lexical
selection. In ACL.
Xie, L.; Shen, J.; and Zhu, L. 2016. Online cross-modal hashing
for web image retrieval. In AAAI.
Xiong, C.; Merity, S.; and Socher, R. 2016. Dynamic memory
networks for visual and textual question answering. In ICML.
Xu, H., and Saenko, K. 2016. Ask, attend and answer: Ex-
ploring question-guided spatial attention for visual question an-
swering. In ECCV.
Yang, Z.; He, X.; Gao, J.; Deng, L.; and Smola, A. 2016.
Stacked attention networks for image question answering. In
CVPR.
Ye, Y.; Zhao, Z.; Li, Y.; Chen, L.; Xiao, J.; and Zhuang, Y.
2017. Video question answering via attribute-augmented at-
tention network learning. In ACM Multimedia.
Zhou, B.; Tian, Y.; Sukhbaatar, S.; Szlam, A.; and Fergus, R.
2015. Simple baseline for visual question answering. arXiv
preprint arXiv:1512.02167.
Zitnick, C. L., and Doll´ar, P. 2014. Edge boxes: Locating object
proposals from edges. In ECCV.

