Geometric Pose Affordance: 3D Human Pose with Scene Constraints

Zhe Wang, Liyan Chen, Shaurya Rathore, Daeyun Shin, Charless Fowlkes
Dept. of Computer Science, UC Irvine
{zwang15,liyanc,rathores,daeyuns,fowlkes}@uci.edu

9
1
0
2
 
y
a
M
 
9
1
 
 
]

V
C
.
s
c
[
 
 
1
v
8
1
7
7
0
.
5
0
9
1
:
v
i
X
r
a

Abstract

Full 3D estimation of human pose from a single image
remains a challenging task despite many recent advances.
In this paper, we explore the hypothesis that strong prior
information about scene geometry can be used to improve
pose estimation accuracy. To tackle this question empiri-
cally, we have assembled a novel Geometric Pose Affor-
dance dataset1, consisting of multi-view imagery of people
interacting with a variety of rich 3D environments. We uti-
lized a commercial motion capture system to collect gold-
standard estimates of pose and construct accurate geomet-
ric 3D CAD models of the scene itself.

To inject prior knowledge of scene constraints into ex-
isting frameworks for pose estimation from images, we in-
troduce a novel, view-based representation of scene geom-
etry, a multi-layer depth map, which employs multi-hit ray
tracing to concisely encode multiple surface entry and exit
points along each camera view ray direction. We propose
two different mechanisms for integrating multi-layer depth
information pose estimation: input as encoded ray features
used in lifting 2D pose to full 3D, and secondly as a dif-
ferentiable loss that encourages learned models to favor
geometrically consistent pose estimates. We show experi-
mentally that these techniques can improve the accuracy of
3D pose estimates, particularly in the presence of occlusion
and complex scene geometry.

1. Introduction

Accurate estimation of human pose in 3D from image
data would enable a wide range of interesting applications
in emerging ﬁelds such as virtual and augmented reality,
humanoid robotics, and monitoring mobility and fall pre-
vention in aging populations. Interestingly, many such ap-
plications are set in relatively controlled environments (e.g.,
the home) where large parts of the scene geometry are rela-
tively static (e.g., walls, doors, heavy furniture). We are in-
terested in the following question, “Can strong knowledge

Figure 1. Sample images from our training set with geometry af-
forded pose: stepping on the stairs, sitting on the tables and touch-
ing boxes.

of the scene geometry improve our estimates of human pose
from images?”.

Consider the images in Figure 1. Intuitively, if we know
the 3D locations of surfaces in the scene, this should con-
strain our estimates of pose. Hands and feet should not in-
terpenetrate scene surfaces, and if we see someone sitting
on a surface of known height we should have a good esti-
mate of where their hips are even if large parts of the body
are occluded. This general notion of scene affordance 2 has
been explored as a tool for understanding functional and ge-
ometric properties of a scene [11, 9, 36, 19]. However, the
focus of such work has largely been on using estimated hu-
man pose to infer scene geometry and function.

Surprisingly, there has been little demonstration of how
scene knowledge can improve pose estimation. Traditional
3D pose datasets have focused on people freely perform-
ing actions in large open spaces and have focused on kine-
matic and dynamic constraints which are scene agnostic.
Recent examples include [23] which proposes temporal ﬁl-
ter and skeleton ﬁtting with pictorial structure constraints,
[40] which utilizes constraint on bone-length ratios for 2D

1The

dataset
https://wangzheallen.github.io/GPA.html

is

available

online

here:

2”the meaning or value of a thing consists of what it affords.” -JJ Gibson

(1979)

1

and 3D estimation with weak supervision, and [2] explores
to use of joint angle priors. We expect one reason that
scene constraints have not been explored is lack of large-
scale datasets of 3D pose in rich environments. Methods
have been developed on datasets like Human3.6M [16] and
MPI-INF-3DHP [22], which do not include interesting geo-
metric scene constraints (at most one chair or sofa) and are
free from scene occlusion. Recent efforts have expanded to
more precise 3D pose for in-the-wild environments [33] but
lack ground-truth scene descriptions.

Instead of tackling human pose estimation in isolation,
we argue that systems should take into account available
information about constraints imposed by complex envi-
ronments. A complete solution must ultimately tackle two
problems: (i) estimating the geometry and free space of the
environment (even when much of that free space is occluded
from view), (ii) integrating this information into pose es-
timation process. Tools for building 3D models of static
environments are well developed and estimation of novel
scene geometry from single-view imagery has also shown
rapid progress. Thus, we focus on the second aspect under
the assumption that high-quality geometric information is
available as an input.

The question of how to represent geometry and incorpo-
rate the (hard) constraints it imposes into current learning-
based approaches to modeling human pose is an open prob-
lem. There are several candidates for representing scene
geometry: voxel representations of occupancy [25] are
straightforward but demand signiﬁcant memory and com-
putation to achieve reasonable resolution; Point cloud [4]
representations provide more compact representations of
surfaces by sampling but lack topological information about
which locations in a scene constitute free space. Instead,
we propose to utilize multi-layer depth maps [29] which
provide a compact and nearly complete representation of
geometric constraints that can be readily queried to verify
pose-scene consistency.

We develop several approaches to utilize information
contained in this representation. Since multi-layer depth is a
view-centered representation of geometry, it can be readily
incorporated as an additional input feature. We leverage es-
timates of 2D pose either as a heatmap or regressed coordi-
nate and query the multi-layer depth map directly to extract
features encoding local constraints on the z-coordinates of
joints which can be used to predict geometry-aware 3D joint
locations. Additionally, we introduce a differentiable loss
that can encourage a model trained with such features to re-
spect hard constraints imposed by scene geometry. We per-
form an extensive evaluation of our multi-layer depth map
models on a range of scenes of varying complexity and oc-
clusion. We provide both qualitative and quantitative evalu-
ation on real data demonstrating that our pipeline improves
upon scene-agnostic state-of-the-art methods for 3D pose

estimation.

To summarize, our main contributions are:

• We collect and curate a unique, large-scale 3D human
pose estimation dataset with rich ground-truth scene
geometry and a wide variety of pose-scene interactions (see
e.g. Figure 1)
• We propose a novel representation of scene geometry
constraints: multi-layer depth map, and explore multiple
ways to incorporate geometric constraints into contempo-
rary learning-based methods for predicting 3D human pose.
• We experimentally demonstrate the effectiveness of inte-
grating geometric constraints relative to two state-of-the-art
scene-agnostic pose estimation methods.

2. Related Work

Motion capture for ground-truth 3D pose The work of
[30] offered one of the ﬁrst large-scale 3D human pose esti-
mation dataset with synchronized images and ground-truth
3D keypoint locations. [16] scales their dataset to 3.6 mil-
lion images covering a range of subjects and actions along
with depth images an 3D scans of the human subjects. To
overcome the limitations of marker-based data collection,
several marker-less approaches have also been used. [31]
and [22] propose marker-less capturing system (based on
inertial measurement sensors and recording in green screen
studio with more cameras) and also capture 3D human pose
estimation in the wild. [17] creates a panoptic studio and
capture poses with 10 pre-calibrated RGBD cameras. [41]
also explores motion capture both indoor and outdoor us-
ing a Drone. However, since these datasets focus on pose
and action recognition, they involve relatively simple en-
vironments with weak geometric constraints. In contrast,
our dataset not only provides the 3D human joints ground
truth but also rich geometry, offering a promising test-bed
for the research in 3D human pose estimation with rich ge-
ometric affordance. We provide more details of comparison
between recent 3d human pose estimation dataset in Table
1.

Modeling of affordance The term “affordance” was
coined by J Gibson [10] in 1979 to capture the notion that
meaning and relevance of many objects in the environment
are largely deﬁned in relation the ways in which an indi-
vidual can functionally interact with them. For computer
vision, this suggests scenarios in which the natural labels
for some types of visual content may not be object cate-
gories or geometry but what human interactions they afford.
[11] presents a human-centric paradigm for scene under-
standing by modeling the physical interactions between the
[9] rely on pose estimation method to extract func-
two.
tional and geometric constraints about the scene and use the
constraint to improve estimates of 3D scene geometry. [36]

Dataset
HumanEva [30]
Human36M [16]
MPI-INF-3DHP [22]
Total Capture [31]
Surreal [32]
ski-pose PTZ camera [27]
3DPW [33]
GPA

Frames
80k
3.6M
3k
1.9M
6M
10k
51k
0.7M boxes, chairs, stairs

Scenes
ground plane
chairs
chairs, sofa
ground plane
ground plane
ski facility
in the wild

Year
2010
2014
2017
2017
2017
2018
2018
2019

Property
1st synchronized 3d pose and image dataset
with human scan and MR test set
indoor and outdoor, markerless
IMU(inertial measurement unit) and 2d matte
Mosh+ SMPL body, depth, body parts.etc
skiing
IMUs and phone captured videos
affordance learning and full scene geometry

Table 1. Comparison of existing dataset for training and evaluating 3d human pose estimation. While other datasets mainly focus on the
way to capture more rich information from human: both markers and IMUs. Our dataset focuses on providing full geometry mesh and
designing different action subset for evaluation of human scene interaction.

collects a large-scale sitcom dataset with not only scenes
but also the same scenes with humans. Leveraging state-
of-the-art pose estimation and generative model [18], they
can tell what kind of pose can each sitcom scene affords.
[19] build a fully automatic 3D pose synthesizer to pre-
dict semantically plausible and physically feasible human
poses within a given scene. [24] applies an energy-based
model on synthetic videos to improve both scene and hu-
man motion mapping. Rather than labeling image content
based on poses we observe, our approach is instead estimat-
ing scene affordance directly from physical principles and
leverage those in constraining estimates of human pose and
interactions with the scene.

Our work is also closely related to earlier work on scene
context for object detection.
[15, 14] used estimates of
ground-plane geometry to reason about location and scales
of objects in an image. More recent work such as [35, 6, 21]
use more extensive 3D models of scenes as context to im-
prove object detection performance in outdoor scenes. Our
work on human pose differs in that humans are articulated
objects which make incorporating such constraints more
complicated as the resulting predictions should simulta-
neously satisfy both scene-geometric and kinematic con-
straints of the body.

Constraints in 3D human pose estimation Estimating
3D human pose from monocular image or video is an
ill-posed problem which can beneﬁt from prior constraints
on prediction. [39, 12] utilize ground-plane as prior to get
better 3d human pose.
[7] models kinematics, symmetry
and motor control using an RNN when predicting 3D
human joints directly from 2D key point.
[38] proposes
an anthropometrically adversarial network as a regularizer.
[34, 42] constructs a graphical model encoding priors
to ﬁt to 3D pose reconstruction.
[28, 5] ﬁrst builds a
large 3D human pose set and treats it as a matching or
classiﬁcation problem. [2, 27] explores joint constraint in
3D and geometric consistency from multi-view images.
[40] improve the performance by adding constant bone
ratio constraint. To the best of our knowledge, our work is

the ﬁrst to exploit strong geometric constraint provided by
the scene to improve 3D human pose estimation.

3. Geometric Pose Affordance Dataset

To collect a rich dataset for studying interaction of scene
geometry and human pose, we designed a set of 3 action
scripts performed by 13 subjects, each of which takes place
in one of 8 scene arrangements. In this section, we describe
the dataset components and the collection process.

3.1. Human Poses and Subjects

The three groups of designed poses each put empha-
sis on semantic actions, mechanical dynamics of skeletons,
and pose-scene interactions. Thus, we name them by Ac-
tion, Motion, and Interaction Set respectively. The seman-
tic actions of Action Set are constructed from a subset of
Human3.6M [16], namely, Direction, Discussion, Writing,
Greeting, Phoning, Photo, Posing and Walk Dog to provide
a connection for comparisons between our dataset and the
de facto standard benchmark. Motion Set includes poses
with more dynamic range of motion, such as running, side-
to-side jumping, rotating, jumping over, and improvised
Interaction Set mainly consists of
poses from subjects.
close interactions between poses and object boundaries to
provide ground truth for modeling affordance in 3D. There
are three main poses in this group: Sitting, Touching, Stand-
ing on, corresponding to typical affordance relations Walk-
able, Reachable, Sittable [9, 11]. The 13 subjects include 9
males and 4 female with roughly the same age and medium
variations in heights approximately from 155cm to 190cm,
giving comparable subject diversity to Human3.6M.

3.2. Image Recording and Motion Capture

As shown in Figure 2.a, there are two types of camera,
RGBD and RGB, with 5 distinct locations in the captur-
ing studio. All 5 cameras have a steady 30fps frame rate
but their time stamps are only partially synchronized, and
thus require an additional stage in processing that we de-

Figure 2. a: Motion capture setup. We utilized multiple computers for capture, one acquires 3 KinectV2 data streams and the other one
controls 2 HD cameras and the VICON mocap system. b: sample image of a human interacting with scene geometry. c: the corresponding
3D Maya model scene mesh with acquired human skeleton for visualization. d-f: corresponding ﬁrst three layers of multi-layer depth map
representation of scene geometry. d corresponds to a traditional depth map, recording the depth of the ﬁrst surface of scene geometry from
the same camera view of b. e is when the multi-hit ray leaves the ﬁrst layer of objects (e.g. the backside of the boxes). f is the depth when
the multi-hit view ray hits a third surface (e.g., ﬂoor behind the box).

scribe below. The color sensor of 5 cameras have the same
1920x1080 resolution and the depth sensor of 3 kinect v2
cameras has a resolution at 640x480. Coexisting with the
image capturing system, the motion capture system is a
standard VICON [1] system with 28 pre-calibrated cameras
covering the capture space to accurately estimate the 3D co-
ordinates of IR-reﬂective tracking markers that we attach to
the surface of subjects and subjects. This studio layout and
recorded data is further illustrated in Figure 2.

3.3. Geometry Layouts and Arrangements

Unlike previous works that only focus on human poses
without many objects present [16, 22], we introduce a va-
riety of scene geometries with arrangements of 9 cuboid
boxes in the scene. We capture the scene from 5 distinct
viewpoints and two types of cameras, three of which in-
clude a depth sensor (KinectV2). The resulting images ex-
hibit substantially more occlusion of subjects than existing
datasets (as illustrated in Figure 1). We capture 1 or 2 sub-
jects for each scene geometry and have a total of 8 scene
geometries (see Appendix).

To record static scene geometry, we measure physical di-
mension of all the objects as well as scan with a Kinect sen-
sor. We utilize additional motion-capture markers attached
to the corners and center face of each surface so that we can
easily align geometric models of the boxes with the global

coordinate system of the motion capture system. We also
use the location of these markers when visible in the 5 cap-
ture cameras in order to estimate extrinsic camera parame-
ters in the same global coordinate system. This allowed us
to quickly create geometric models of the scene which are
well aligned to all calibrated camera views and the motion
capture data.

3.4. Geometry Representation

As mentioned previously, the scene geometry is repre-
sented as multi-layer depth maps computed by casting rays
from a camera viewpoint and recording the depth of inter-
sections with the scene mesh models. Mesh models are
constructed with modeling software (Maya) with assistance
from physical measurements.
Multi-layer depth map: Multi-layer depth maps are de-
ﬁned as a map of ray entry and exit depths through all sur-
faces in a scene (as illustrated in Figure 2 d − f ). Un-
like standard depth-maps which only encode the geome-
try of visible surfaces in a scene (sometimes referred to
as 2.5D), multi-layer depth provides a nearly3 complete,
view-based description of scene geometry. The multi-layer
depth representation can be computed by performing multi-
hit ray tracing from the camera. Speciﬁcally, the multi-hit
ray tracing function g sends a ray from the camera cen-

3Surfaces tangent to a camera view ray are not represented

Figure 3. Overview of model architecture: we use ResNet-50 as our backbone to extract features from a human centered cropped image. The
feature map is used to predict 2D joint location heatmaps and is also concatenated with encoded multi-layer depth map. The concatenated
feature is used to regress the depth (z-coordinate) of each joint. The model is trained with a loss on joint location (joint regression loss) and
scene affordance (geometric consistency loss). Argmax function is applied to the 2d joint heatmap to get the x,y location. The geometric
consistency loss is described in more detail in Fig 4 (a) and Section 4.2.

ter towards a point on the image plane that corresponds to
the pixel at (x, y) and outputs distance values g(x, y) =
{t1, t2, t3, ..., tk} where k is the total number of polygon
intersections at that pixel. Given a unit ray direction r and
camera viewing direction v, the depth value at layer i is
Di(x, y) = tir · v if i <= k and Di(x, y) = ∅ if i > k. In
our case, the number of multi-layer depth maps is set to 15
which sufﬁces to cover all scenes in our dataset.

3.5. Data Processing Pipeline

The whole data processing pipeline includes validating
motion capture pose estimates, camera calibration, joint
temporal alignment of all data sources, and camera calibra-
tion. Unlike datasets such as [16] which have few occlusion,
many markers attached to the human body are occluded in
the scene during our capture sessions. We spent 4 month on
pre-processing with help of 6 annotators in total. There are
three stages of generating ground truth joints from recorded
(a) recognizing and labeling recorded
VICON sessions:
markers in each frame to 53 candidate labels which included
three passes to minimize errors; (b) applying selective tem-
poral interpolation for missing markers based on annota-
tors’ judgement. (c) removing clips with too few tracked
markers. After the annotation pipeline, we compile record-
ings and annotations into 61 sessions recorded at 120fps by
the VICON software. To temporally align these compiled
ground-truth pose streams to image capture streams, (a)
we ﬁrst ask annotators to manually correspond 10-20 pose
frames to image frames. (b) Then we estimate the scaling
and offset parameters with RANSAC [8], linear regressing
to convert all timestamps to a global timeline.

After we paired pose data and image frames, we gen-
erate the dataset by selecting non-redundant frames, esti-
mating camera poses and rendering multi-layer depth maps

at the calibrated camera positions. We use an adaptive ap-
proach to sampling frames. We consider frames with suf-
ﬁcient difference from adjacent ones as interesting frames.
Here, the measure of difference between two skeletons is
deﬁned as the 75th percentile of L2 distances between cor-
responding joints (34 pairs per skeleton pair), since we want
to both keep skeletons where few body parts moved sig-
niﬁcantly and be robust to the cases where skeletons have
jerky movements caused by noise. With the measure of dif-
ference deﬁned, we threshold the frames by choosing the
change threshold as the 55th percentile, retaining 45% of
total frames.

The RGB camera calibration is done manually using an-
notators’ markings of corresponding image coordinates of
visible markers (whose global 3D coordinates are known)
and estimating camera parameters from those correspon-
dences. With estimated camera parameters, speciﬁcally
nonlinear ones, we correct the radial and lens distortions
of the image so that they can be treated as projections from
ideal pinhole cameras in later steps. Also, we performed vi-
sual inspection on all clips to check that the estimated cam-
era parameters yield correct projects 3D markers to their
corresponding locations in the image.

The last component of our dataset is the geometry of
recorded scenes, which is represented as multi-layer depth
maps. As mentioned before, we construct the scene geom-
etry with 3D modeling software, Maya in this case. The
scene geometry models are then rendered into multi-layer
depth maps with given camera parameters from last step.
We performed visual inspections to ensure the edges of ob-
jects in renderings overlay with images.

4. Geometry-aware Pose Estimation

We now introduce two approaches for incorporating ge-
ometric affordance in CNN-based pose estimation, building
on the baseline architecture of [40]. Given an image I of a
human subject, we aim to estimate the 3D human pose rep-
resented by a set of 3D joint coordinates of the human skele-
ton, P ∈ RJ×3 where J is the number of joints. We follow
the convention of representing each 3D coordinate in the lo-
cal camera coordinate system associated with I, namely, the
ﬁrst two coordinates are given by image pixel coordinates
and the third coordinate is the joint depth in metric coordi-
nates, e.g., millimeters. We use PXY and PZ respectively
as short-hand notations for the components of P .

4.1. Pose Estimation Baseline Model

We adopt the state-of-the-art ResNet-based network in
[37] as our 2D pose estimation module. The network out-
put is a set of low-resolution heat-maps ˆS ∈ RH×W ×J ,
where each map ˆS[:, :, j] can be interpreted as a probability
distribution over the j-th joint location. At test time, the 2D
prediction ˆPXY is given by the most probable (arg max)
locations in S. This heat-map representation is convenient
as it can be easily combined (concatenate or sum) with the
other deep layer feature maps. To train this module, the loss
function is

(cid:96)2D( ˆS|P ) = (cid:107) ˆS − G(PXY )(cid:107)2

(1)

where G(P ) is a target distribution created from ground-
truth P by placing a Gaussian with σ = 3 at each joint
location.

To predict the depth of each joint, we follow the ap-
proach of [40], which combines the 2D joint heatmap and
the intermediate feature representations in the 2D pose
module as input to a joint depth regression module. These
shared visual features provide additional cue for recovering
full 3D pose. We train with a smooth (cid:96)1 loss [26] given by:

(cid:96)1smooth( ˆP |P ) =

(cid:40) 1

2 (cid:107) ˆPZ − PZ(cid:107)2
(cid:107) ˆPZ − PZ(cid:107) − 1
2

(cid:107) ˆPZ − PZ(cid:107) ≤ 1
o.w.

(2)

Alternate baseline As a second baseline model we uti-
lize the model of [20], which detects 2D joint locations and
then trains a multi-layer perceptron to regress the 3D co-
ordinates P from the vector of 2D coordinates PXY . We
use the ResNet model [37] to detect the 2d locations key
points and also consider an upper-bound based on lifting
the ground-truth 2d joint locations to 3d.

4.2. Geometry Consistency Loss and Encoding

To inject knowledge of scene geometry we consider two
approaches, geometric consistency loss which incorporates

geometry during training and geometric encoding which
makes scene geometry available as an input feature at test
time.

Geometric consistency loss: We design a geometric con-
sistency loss (GCL) which speciﬁcally penalizes errors in
pose estimation that violate scene geometry constraints.
The intuition is illustrated in Fig. 4. For a joint at 2D loca-
tion (x, y), the estimated depth z should lie within one of a
disjoint set of intervals deﬁned by the multi-depth values at
that location.

To penalize a joint prediction P j = (x, y, z) that falls
inside a region bounded by front-back surfaces with depths
Di(x, y) and Di+1(x, y) we deﬁne a loss that increases lin-
early with the penetration distance inside the surface:

(cid:96)G(i)( ˆP j|D) = min(max(0, ˆP j

Z − Di( ˆP j

max(0, Di+1( ˆP j

XY )),
XY ) − ˆP j

Z))

(3)

Our complete geometric consistency loss penalizes predic-
tions which place any joint inside the occupied scene geom-
etry

(cid:96)G( ˆP |D) =

(cid:88)

j

max
i∈{0,2,4,...}

(cid:96)G(i)( ˆP j|D)

(4)

Assuming {Di} is piecewise smooth, this loss is differen-
tiable almost everywhere and hence amenable to optimiza-
tion with SGD. The gradient of the loss “pushes” joint lo-
cation predictions for a given example to the surface of oc-
cupied volumes in the scene.

Encoding local scene geometry: When scene geometry
is available at test time (e.g., ﬁxed cameras pointed at a
known scene), it is reasonable to provide the model with an
encoding of the scene geometry. Our view-centered multi-
depth representation of scene geometry can be naturally in-
cluded as an additional feature channel in a CNN. We con-
sidered two different encodings of multi-layer depth. (1) We
crop the multi-layer depth map to the input frame, re-sample
to the same resolution as the 2D heatmap using nearest-
neighbor interpolation, and offset by the depth of the skele-
ton root joint. (2) Instead of feeding the transformed multi-
depth values, we also consider a volumetric encoding of the
scene geometry by sampling 64 depths centered around the
root joint using a range based on the largest residual depth
between the root and any other joint seen during training
(approx. +/ − 1m). For each (x, y) location and depth,
we evaluate the geometric consistency loss (cid:96)G at that point.
This resulting encoding is of size H × W × 64 and en-
codes the local volume occupancy around the pose estimate.
In our experiments we found that the simple and efﬁcient
multi-layer depth encoding (1), performed the same or bet-
ter than volumetric encoding (2) and utilized it exclusively
for the results reported here.

Full
Action
Motion
Interaction
Cross Subject (CS)
Cross Action (CA)
Occlusion
Close2Geometry

82,378
44,102
22,916
15,360
58,882
23,496
7,707
1,727

Table 2. Numbers of frames in each test subset. We evaluate per-
formance on different subsets of the test data split by the scripted
behavior (Action/Motion/Interaction), subjects that were excluded
from the training data (cross-subject) and novel actions (cross-
action). Finally, we evaluate on a subset with signiﬁcant occlu-
sion (Occlusion) and a subset where many joints were near scene
geometry (Close2Geometry).

82k images for held-out test evaluation. In addition to the
three subsets – Action, Motion, and Interaction – that are
inherited from the global split of the dataset based on script
contents, we also report performance on 4 other subsets of
the test data: cross-subject (CS), cross-action (CA), occlu-
sion, close-to-geometry. Despite non-orthogonal splits for
these four test subset, trait subsets give us ﬁner character-
izations of model performances in various scenarios: (a)
CS subset includes clips from held-out subjects to evalu-
ate generalization ability on unseen subjects; (b) CA sub-
set includes clips of held-out actions from same subjects
from the training set; (c) occlusion subset includes frames
with signiﬁcant occlusions (at least 10 out of 34 joints are
occluded by objects); (d) close2geometry subset includes
frames where subjects are close to objects (i.e. at least 8
joints have distance less than 175 mm to the nearest sur-
face). Statistics of these testing subsets are shown in Table
2

Following [40], we also use the MPII [3] dataset, a large
scale in-the-wild human pose dataset for training the 2D
pose module.
It contains 25k training images and 2,957
validation images.

For the alternative baseline model we experiment on
[20], we use the MPII pre-trained ResNet [37] to detect the
2d human key point. We also evaluate performance when
using the ground truth 2d human pose, which serve as an
upper-bound for lifting [20] based method.

Implementation details: We take a crop around the
skeleton from the original 1920 × 1080 image and isotrop-
ically resize them to 256 × 256, so that projected skeletons
have roughly the same size. Ground truth 2D joint loca-
tion are adjusted accordingly. Following [40], the ground
truth depth coordinates are normalized to [0, 1]. The back-
bone for all models is ResNet-50 [13] (50-layer ResNet),
2d heat map/depth map spatial size is 64 × 64 with one out-
put channel per joint. Models are implemented in Pytorch

Figure 4. (a) is the illustration of the geometry consistency loss
as a function of depth along a speciﬁc camera ray correspond-
ing to a predicted 2D joint location.
In (b) the green line indi-
cates the ray corresponding to the 2D location of the right foot.
Our multi-depth encoding of the scene geometry stores the depth
to each surface intersection along this ray (i.e., the depth val-
ues Z0, Z1, Z2, Z3, Z4). Valid poses must satisfy the constraint
that the joint depth falls in one of the intervals: ZJ < Z0 or
Z1 < ZJ < Z2 or Z3 < ZJ < Z4. The geometric consistency
loss pushes the prediction ZJ towards the closest valid conﬁgura-
tion, ZJ = Z2 .

Injection of scene geometry for alternate method: To
inject the scene geometry into the second model [20], we
use 2D joint locations as the query input to get the ex-
act multi-depth values, and use values as encoded input or
GCL.

4.3. Overall Training

Finally we combine the losses in Eq. 1, 2, and 4, the

overall loss for each training image is
(cid:96)( ˆP , ˆS|P, D) = (cid:96)2D( ˆS|P ) + (cid:96)1smooth( ˆP |P ) + (cid:96)geom( ˆP |P, D)

We follow [40] and adopt a stage-wise training approach:
Stage 1 initializes the 2D pose module using 2D annotated
images (i.e., MPII dataset); Stage 2 trains the 3D pose es-
timation module, including the depth regression module as
well as the 2D pose estimation module; Stage 3 training
enables geometry-aware components (encoding input, geo-
metric consistency loss) in addition to the modules in stage
2.

5. Experiments

Dataset: Our GeometricPoseAffordance (GPA) dataset
has 304,892 images along with scene geometry. We utilize

Action Set Motion Set

Interaction Set Occlusion

MPJPE
Baseline[40]
ResNet-E
ResNet-C
ResNet-F
PCK3D
Baseline[40]
ResNet-E
ResNet-C
ResNet-F

97.2
95.8
96.6
95.1

81.4
81.8
81.6
82.0

99.6
97.0
97.9
96.5

80.7
81.5
81.6
82.0

89.7
87.5
88.3
87.4

85.2
86.0
85.7
86.1

120.5
116.1
117.9
115.1

72.2
73.9
73.7
74.2

CS
99.4
98.1
98.8
97.8
CS
81.3
81.7
81.5
82.0

CA
89.2
85.8
86.7
85.6
CA
83.6
84.7
84.5
84.8

Close2Geometry
118.1
113.2
116.3
111.5
Close2Geometry
71.4
73.7
72.1
74.7

Full
96.5
94.6
95.4
94.1
Full
81.9
82.5
82.3
82.9

Action Set Motion Set

Interaction Set Occlusion

Table 3. MPJPE (in mm) and PCK3D on each subset and the full test set. ResNet-E is the base model with encoded geometry input,
ResNet-C is with geometric consistency loss (GCL). ResNet-F is our full model with both geometry input and GCL loss. CS stands for
cross-subject test while CA stands for cross-action test.

with RMSprop as the optimizer. For the [20] based method
we use the same process as above to detect 2d human joint
and following the original paper by subtracting mean and
dividing the variance for both 2d input and 3d ground truth.
Evaluation metrics: Following standard protocols de-
ﬁned in [22, 16], we consider two evaluation metrics for
experiments: MPJPE (mean per joint position error) and
the 3D PCK (percent correctly localized keypoints) with a
distance threshold of 150 mm. In computing the evaluation
metrics, root-joint-relative joint locations are evaluated ac-
cording to the standard evaluation protocol.

Ablative study: To demonstrate the performance of
each component, we evaluate four variants of each model:
the baseline[40] / SIM-P / SIM-G [20]; ResNet-E / SIM-
P-E / SIM-G-E, the model with encoded scene geometry
input; ResNet-C / SIM-P-C / SIM-G-C, the model with en-
coded geometry consistency loss (GCL); ResNet-F / SIM-
P-F / SIM-G-F , our full model with both encoded geome-
try priors and GCL.

5.1. Result analysis

Analysis of baseline performance: Table 3 and 4 com-
pares the proposed approaches against the baseline. For
ResNet based method [40] the baseline model achieves a
MPJPE 96.5 mm, while the same method gives 64.9 mm on
Human3.6M; For lifting-based method [20], baseline per-
forms a MPJPE 68.2 mm with ground truth on GPA, while
on Human36M it is 45.5 mm. This demonstrates the dif-
ﬁculty of our dataset and the necessity of including richer
geometric constraints on understanding 3d human pose.
The motion, occlusion and close2geometry subsets prove
to be the most challenging as they involve large numbers of
frames where subjects interact with the scene geometry.

Effectiveness of proposed methods: ResNet-E / SIM-
P-E / SIM-G-E and ResNet-C / SIM-P-C / SIM-G-C models
add geometric context as an input to the network or penalize
predictions that violate constraints during training. We can
see both methods yield improvement on all the test subsets.
Not surprisingly, using geometry information during both

training and test gives the best results, allowing the network
to be geometry-aware and training with a loss that encour-
ages predictions to respect those geometric constraints. We
can see from table 3 that our full model, ResNet-F decreases
the MPJPE by 2.1mm over the full test set. Among 4 sub-
sets, the most signiﬁcant improvement comes in the oc-
clusion and close2geometry subsets. Our geometry-aware
method decreases MPJPE in occlusion and close2geometry
set by 5.4mm / 6.6mm and increase the PCK3d about 2%
/ 3%. Table 4 shows corresponding results for the SIM
model. We can see the full model decreases the MPJPE for
both predicted (SIM-P) and ground-truth based input (SIM-
G) by 3 mm / 3.6 mm and improves 1.2% and 1.1% in terms
of PCK3d. For subset of occlusion / close2geometry, they
are decreased in terms of MPJPE by 8.6mm / 7 mm. And
4.2% / 2.8% in terms of PCK3d.

Errors by joint type: We partition the 16 human joints
into the limb joints which are more likely to be interacting
with scene geometry (out group) and the torso and hips (in
group). The performance on these two subsets of joints is il-
lustrated in Table 5, which veriﬁes our assumption that limb
joint estimation (wrist,elbow,knees,ankles) beneﬁts more
from incorporating geometric scene affordance. One inter-
esting conclusion we draw from table 5 of ResNet based
method: good MPJPE does not guarantee good PCK3d.
In group has better MPJPE than out group, however, the
PCK3d is not comparable to out group.

Quantitative results: We show qualitative examples
and its interaction with geometry in Fig 13. These exam-
ples show that ResNet-F has better accuracy in both xy lo-
calization and depth prediction and even resolves ambiguity
under heavy occlusions.

6. Conclusion and Future Work

In this work, we introduce the ﬁrst large scale 3D hu-
man pose estimate dataset with rich geometric pose affor-
dance constraints. We propose multi-layer depth as a con-
cise representation of scene geometry and explore two ef-

Figure 5. MPJPE of predictions of ResNet-F and the baseline. Data points are from the close2geometry test set. We list the example
performances by an increasing order of baseline MPJPE (red) with corresponding ResNet-F performances (GCL + encoding, in blue).
Among those examples, We also show 3 qualitative results, from left to right: (a) case shows ResNet-F improve over the baseline with
respect to the depth prediction. (b,c) cases show ResNet-F improves over the baseline in all x, y, z axes. Furthermore, (b) case demonstrates
ResNet-F can even resolve ambiguity under heavy occlusions with the aid of geometry information. We show the image with the estimated
2D pose(after cropping), 1st layer of multi-layer depth map and whether the joint is occluded or not. legend: hollow circles: occluded
joints; solid dots: non-occluded joints; dotted lines: partially/completely occluded body parts; solid lines: non-occluded body parts.

Action Set Motion Set

Interaction Set Occlusion

MPJPE
SIM-P [20]
SIM-P-E
SIM-P-C
SIM-P-F
SIM-G [20]
SIM-G-E
SIM-G-C
SIM-G-F
PCK3D
SIM-P [20]
SIM-P+E
SIM-P+C
SIM-P+F
SIM-G [20]
SIM-G+E
SIM-G+C
SIM-G+F

91.8
89.4
91.6
88.4
70.7
66.5
67.0
66.6

85.0
85.9
85.1
86.3
92.4
93.7
93.8
93.8

93.4
92.6
93.0
91.8
66.7
64.8
65.3
65.0

84.4
84.7
84.6
85.0
93.7
94.3
94.2
94.3

86.0
82.6
86.2
82.1
63.3
59.0
59.1
58.2

86.9
88.4
86.7
88.5
94.5
95.8
95.6
95.9

117.8
112.3
118.2
111.8
67.2
63.3
65.0
63.7

74.8
76.4
74.6
76.7
93.6
94.5
94.2
94.4

CS
92.8
92.1
92.5
91.0
70.4
67.3
67.4
67.3
CS
84.9
85.2
85.1
85.7
92.7
93.8
93.9
93.9

CA
87.1
81.4
87.2
81.1
62.7
57.9
59.4
57.8
CA
85.9
88.0
85.7
88.1
94.2
95.4
95.2
95.5

Close2Geometry
112.8
105.2
112.0
104.2
79.8
74.2
73.3
72.8
Close2Geometry
75.4
79.4
76.1
79.6
89.3
91.9
92.1
92.1

Full
91.2
89.1
90.9
88.2
68.2
64.6
65.1
64.6
Full
85.2
86.0
85.3
86.4
93.2
94.3
94.2
94.3

Action Set Motion Set

Interaction Set Occlusion

Table 4. MPJPE (in mm) and PCK3D on each subset and the full test set with SIM-P [20]. The 2d prediction is based on the 2d prediction
from MPII pre-trained ResNet (50 layers), while SIM-G [20] is based on 2d ground truth as input.

fective ways to incorporate geometric constraints into train-
ing in an end-to-end fashion. There are many other alterna-
tives for representing geometric scene constraints. We hope
the availability of this dataset will inspire future work on
geometry-aware feature design and affordance learning for
3D human pose estimation.

Acknowledgement

This project is supported by NSF grants IIS-1813785,
IIS1618806, IIS-1253538, CNS-1730158 and a hardware
donation from NVIDIA. Zhe Wang personally thanks Shu
Kong and Minhaeng Lee for helpful discussion, John Craw-
ford and Fabio Paolizzo for providing support on the motion
capture studio, and all the UCI friends who contribute to the
dataset collection.

MPJPE
Baseline[40]
ResNet-E
ResNet-C
ResNet-F
PCK3D
Baseline[40]
ResNet-E
ResNet-C
ResNet-F
MPJPE
SIM-P [20]
SIM-P-E
SIM-P-C
SIM-P-F
PCK3D
SIM-P [20]
SIM-P-E
SIM-P-C
SIM-P-F
MPJPE
SIM-G [20]
SIM-G-E
SIM-G-C
SIM-G-F
PCK3D
SIM-G [20]
SIM-G-E
SIM-G-C
SIM-G-F

In Group Out Group

In Group Out Group

In Group Out Group

In Group Out Group

In Group Out Group

In Group Out Group

104.5
100.8
102.1
99.8

81.9
83.5
83.1
84.0

116.3
113.7
116.0
112.6

76.4
77.6
76.5
78.1

85.0
81.4
81.4
81.1

88.9
90.6
90.6
90.7

87.2
87.1
87.2
87.0

82.0
82.1
82.0
82.1

62.4
60.9
62.4
60.3

95.3
95.6
95.3
95.8

49.1
45.5
46.4
45.7

98.0
98.5
98.4
98.5

Full
96.5
94.6
95.4
94.1
Full
81.9
82.5
82.3
82.9
Full
91.2
89.1
90.9
88.2
Full
85.2
86.0
85.3
86.4
Full
68.2
64.6
65.1
64.6
Full
93.2
94.3
94.2
94.3

Table 5. Performance grouped by joints, with out group joints:
rightfoot, rightleg, leftleg, leftfoot, righthand, rightforearm, left-
forearm,lefthand, and in group joints: rightupleg, leftupleg, hips,
spine1, head, neck, rightarm, leftarm. We can see that PCK3D
and MPJPE are both important evaluation criteria which performs
differently between in groups and out groups (in groups perform
good on MPJPE while out groups perform good on PCK3d for
ResNet [40] based method).

References

[1] Vicon blade. http://www.vicon.com. 4
[2] I. Akhter and M. J. Black. Pose-conditioned joint angle lim-
In CVPR, 2015. 2,

its for 3d human pose reconstruction.
3

[3] M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele. 2d
human pose estimation: New benchmark and state of the art
analysis. In CVPR, 2014. 7, 11

[4] K.-C. Chan, C.-K. Koh, and C. S. G. Lee. A 3d-point-cloud

feature for human-pose estimation. In ICRA, 2013. 2

[5] C.-H. Chen and D. Ramanan. 3d human pose estimation =

2d pose estimation + matching. In CVPR, 2017. 3

[6] R. D´ıaz, M. Lee, J. Schubert, and C. C. Fowlkes. Lifting gis
maps into strong geometric context for scene understanding.
In WACV, 2016. 3

[7] H.-S. Fang, Y. Xu, W. Wang, X. Liu, and S.-C. Zhu. Learning
pose grammar to encode human body conﬁguration for 3d
pose estimation. In AAAI, 2018. 3

[8] M. A. Fischler and R. C. Bolles. Random sample consen-
sus: a paradigm for model ﬁtting with applications to image
analysis and automated cartography. Communications of the
ACM, 1981. 5

[9] D. F. Fouhey, V. Delaitre, A. Gupta, A. A. Efros, I. Laptev,
and J. Sivic. People watching: Human actions as a cue for
single-view geometry. In ECCV, 2012. 1, 2, 3

[10] J. Gibson. The ecological approach to visual perception. In

Boston: Houghton Mifﬂin, 1979. 2

[11] A. Gupta, S. Satkin, A. A. Efros, and M. Hebert. From 3d
scene geometry to human workspace. In CVPR, 2011. 1, 2,
3

[12] N. Hasler, B. Rosenhahn, a. J. G. Thorsten Thormahle1, and
Michael Wand, and H.-P. Seidel. Markerless motion capture
with unsynchronized moving cameras. CVPR, 2009. 3
[13] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. CVPR, 2016. 7

[14] D. Hoiem, A. Efros, and M. Hebert. Putting objects in per-

spective. CVPR, 2006. 3

[15] D. Hoiem, A. A. Efros, and M. Hebert. Geometric context

from a single image. ICCV, 2005. 3

[16] C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu. Hu-
man3.6m: Large scale datasets and predictive methods for
3d human sensing in natural environments. PAMI, 2014. 2,
3, 4, 5, 8, 11

[17] H. Joo, T. Simon, and Y. Sheikh. Total capture: A 3d de-
formation model for tracking faces, hands, and bodies.
In
CVPR, 2016. 2

[18] D. P. Kingma and M. Welling. Auto-encoding variational

bayes. In Arxiv, 2013. 3

[19] X. Li, S. Liu, K. Kim, X. Wang, M.-H. Yang, and J. Kautz.
Putting humans in a scene: Learning affordance in 3d indoor
environments. In CVPR, 2019. 1, 3

[20] J. Martinez, R. Hossain, J. Romero, and J. J. Little. A sim-
ple yet effective baseline for 3d human pose estimation. In
ICCV, 2017. 6, 7, 8, 9, 10, 11, 14, 15, 18, 21

[21] K. Matzen and N. Snavely. Nyc3dcars: A dataset of 3d ve-

hicles in geographic context. In ICCV, 2013. 3

[22] D. Mehta, H. Rhodin, D. Casas, P. Fua, O. Sotnychenko,
W. Xu, and C. Theobalt. Monocular 3d human pose esti-
mation in the wild using improved cnn supervision. In 3DV,
2017. 2, 3, 4, 8

[23] D. Mehta, S. Sridhar, O. Sotnychenko, H. Rhodin,
M. Shaﬁei, H.-P. Seidel, W. Xu, D. Casas, and C. Theobalt.
Vnect: Real-time 3d human pose estimation with a single
rgb camera. 2017. 1

[24] A. Monszpart, P. Guerrero, D. Ceylan, E. Yumer, and N. J.
imapper: Interaction-guided joint scene and human

Mitra.
motion mapping from monocular videos. In arxiv, 2018. 3

[25] G. Pavlakos, X. Zhou, K. G. Derpanis, and K. Daniilidis.
Coarse-to-ﬁne volumetric prediction for single-image 3D hu-
man pose. In CVPR, 2017. 2

[26] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: To-
wards real-time object detection with region proposal net-
works. NIPS, 2015. 6

[27] H. Rhodin, M. Salzmann, and P. Fua.

Unsupervised
geometry-aware representation for 3d human pose estima-
tion. In ECCV, 2018. 3

[28] G. Rogez, P. Weinzaepfel, and C. Schmid. Lcr-net++: Multi-
person 2d and 3d pose detection in natural images. PAMI,
2019. 3

[29] D. Shin, Z. Ren, E. Sudderth, and C. Fowlkes. Multi-layer
depth and epipolar feature transformers for 3d scene recon-
struction. In ArXiv, 2019. 2

[30] L. Sigal, A. O. Balan, and M. J. Black. Humaneva: Syn-
chronizedvideo and motion capture dataset and baseline al-
gorithm forevaluation of articulated human motion. In IJCV,
2010. 2, 3

[31] M. Trumble, A. Gilbert, C. Malleson, A. Hilton, and J. Col-
lomosse. Total capture: 3d human pose estimation fusing
video and inertial sensors. In BMVC, 2017. 2, 3

[32] G. Varol, J. Romero, X. Martin, N. Mahmood, M. J. Black,
I. Laptev, and C. Schmid. Learning from synthetic humans.
In CVPR, 2017. 3

[33] T. von Marcard, R. Henschel, M. J. Black, B. Rosenhahn,
and G. Pons-Moll. Recovering accurate 3d human pose in
the wild using imus and a moving camera. ECCV, 2018. 2,
3

[34] C. Wang, Y. Wang, Z. Lin, and A. L. Yuille. Robust 3d hu-
man pose estimation from single images or video sequences.
In PAMI, 2018. 3

[35] S. Wang, S. Fidler, and R. Urtasun. Holistic 3d scene under-
standing from a single geo-tagged image. In CVPR, 2015.
3

[36] X. Wang, R. Girdhar, and A. Gupta. Binge watching: Scaling
affordance learning from sitcoms. In CVPR, 2017. 1, 2
[37] B. Xiao, H. Wu, and Y. Wei. Simple baselines for human

pose estimation and tracking. In ECCV, 2018. 6, 7

[38] W. Yang, W. Ouyang, X. Wang, J. Ren, H. Li, and X. Wang.
3d human pose estimation in the wild by adversarial learning.
In CVPR, 2018. 3

[39] A. Zanﬁr, E. Marinoiu, and C. Sminchisescu. Monocular
3d pose and shape estimation of multiple people in natural
scenes. CVPR, 2018. 3

[40] X. Zhou, Q. Huang, X. Sun, X. Xue, and Y. Wei. Towards
3d human pose estimation in the wild: A weakly-supervised
approach. In ICCV, 2017. 1, 3, 6, 7, 8, 10, 11, 13, 18
[41] X. Zhou, S. Liu, G. Pavlakos, V. Kumar, and K. Daniilidis.
Human motion capture using a drone. In ICRA, 2018. 2
[42] X. Zhou, M. Zhu, G. Pavlakos, S. Leonardos, K. G. Derpa-
nis, and K. Daniilidis. Monocap: Monocular human motion
capture using a cnn coupled with a geometric prior. PAMI,
2018. 3

Appendix

A. Dataset Statistics and Visualizations

Figure 6 shows sample images for different script seg-
ments when capturing: action/motion/interaction subset.
We provide the illustration of all camera views from one
scene in Figure 7, with corresponding multi-layer depth
maps in Figure 8. In addition, all 8 scenes in our dataset
are summarized in Figure 9 with corresponding multi-layer
depth map in Figure 10. Lastly, we illustrate all the scene
geometry meshes in Figure 11. Figure 12 summarizes
statistics on the number of occluded joints as well as the
distribution of which multi-depth layer is closest to a joint.
There are sample images of each subset included in Fig-
ure 6 to show their different emphasis on pose, geome-
try, and interactions: action set focuses on the mimicking
actions from Human3.6M [16] and probing how state-of-
the-art algorithm performs on these actions with geometry
added on. The motion set focuses on dynamic actions in
which the subjects move quickly and take on poses which
are not statically stable; The interaction set provides the test
bed for studying affordance learning between human pose
and environment geometry.

In addition, we attach a demo video 4 showing the detail
of full resolution video, cropped video with ground truth
joints/markers overlay. We also show the subject id (here
we use anonymous), take name, camera name, video time
id, mocap time id, bone length (which is constant overtime),
velocity, number of valid markers, invisible joints, and in-
visible markers (there are 53 markers and 34 joints for VI-
CON system). The video is sampled from 10 clips from
’Action’, ’Motion’ set with the same subject.

B. Performance Breakdown and Qualitative

Evaluations

We provide more detailed quantitative performance com-
parisons between baseline [40, 20] model prediction, and
full model prediction for each joint on our GPA dataset in
Table 6, 7 and 8 (MPJPE) and Table 9, 10 and 11 (PCK3D).
For the joint deﬁnition, we use joints from the human
skeleton rig ﬁt by the VICON motion capture system. To
make it consistent with pre-deﬁned set like MPII [3]. We
have our: 0: rightfoot, 1: rightleg, 2: rightupleg, 3: leftu-
pleg, 4:leftleg, 5: leftfoot, 6: hips (root joint) , 7: spine1,
8: head, 9:site (neck), 10: righthand, 11: rightforearm, 12:
rightarm, 13: leftarm, 14: leftforearm, 15: lefthand, corre-
sponding to MPII: 0 - r ankle, 1 - r knee, 2 - r hip, 3 - l hip,
4 - l knee, 5 - l ankle, 6 - pelvis, 7 - thorax, 8 - upper neck,
9 - head top, 10 - r wrist, 11 - r elbow, 12 - r shoulder, 13 - l
shoulder, 14 - l elbow, 15 - l wrist.

4 Video Link: https://youtu.be/ZRnCBySt2fk

Figure 6. Sample images from different subset (the same across capturing to evaluation): action, motion, interaction. For the image in
action set we can see the human action: Direction, Discussion, Posing, Walk Dog and Greeting. From the images in the interaction set
we can see Touching, Standing, Climbing Stairs, Sitting. From the images in the motion set we can see side-to-side jumping, running,
jumping over, improvising poses from subjects. Designing different subsets makes it possible to pose research on different concepts: 3d
pose estimation, affordance learning, stability and action recognition.

To give qualitative evaluations, we include visualization
of model prediction with scene geometry in Figure 13 and
Figure 14. The scene geometry is represented by the 1st
layer of multi-layer depth map and the ground truth pose
is included along with the predictions. The visualization
and comparison is here to show how baseline models fail in
some difﬁcult scenarios while geometry priors effectively
help our model to resolve to physically sound pose conﬁg-
urations.

Figure 7. Illustration of 5 camera views of the same static scene without subjects performing actions. The Cam0, Cam1, Cam2 are from
Kinect V2 cameras while Cam3 and Cam4 are from the other 2 HD cameras.

Baseline [40] ResNet-E ResNet-C ResNet-F

Method
rightfoot
rightleg
rightupleg
leftupleg
leftleg
leftfoot
spine1
head
neck
righthand
rightforearm
rightarm
leftarm
leftforearm
lefthand

117.8
84.6
30.4
31.0
83.6
118.4
188.8
79.3
121.4
120.1
101.4
81.3
81.9
96.5
113.8

109.9
82.2
29.3
30.1
80.9
111.4
188.1
80.0
117.9
115.7
99.8
80.7
81.3
95.4
111.1

111.9
82.9
29.7
30.3
82.5
114.0
188.6
80.4
120.9
117.1
100.7
80.9
81.7
95.9
112.0

108.1
81.0
29.0
30.0
80.0
110.2
187.3
80.0
117.6
114.7
99.7
80.7
81.2
94.9
109.8

Table 6. MPJPE (in mm, the lower the better) by each joint we use for evaluation.

Figure 8. The same 5 cam views as in Figure 7 and with the ﬁrst 3 layers of corresponding multi-layer depth map (for visualization clarity,
we plot 1/depth).

SIM-P-C SIM-P-F

Method
rightfoot
rightleg
rightupleg
leftupleg
leftleg
leftfoot
spine1
head
neck
righthand
rightforearm
rightarm
leftarm
leftforearm
lefthand

SIM-P [20]
127.5
91.8
22.5
22.9
88.9
131.0
63.5
70.4
102.0
140.1
108.5
75.9
79.7
104.1
138.5

SIM-P-E
123.4
88.1
20.9
21.6
85.6
126.3
63.0
69.6
100.2
138.3
105.3
75.2
76.0
103.3
139.3

126.6
91.7
22.4
22.9
89.3
130.3
63.5
70.6
102.2
139.8
107.8
76.5
78.6
104.1
138.6

122.1
87.2
20.7
21.4
84.9
125.8
62.4
69.0
98.8
135.7
103.6
74.2
75.4
102.9
138.3

Table 7. MPJPE (in mm, the lower the better) by each joint we use for evaluation.

Figure 9. Illustration of all the 8 scenes we capture from Cam2 view. The different scene object layout can be viewed from this ﬁgure.

SIM-G-C SIM-G-F

Method
rightfoot
rightleg
rightupleg
leftupleg
leftleg
leftfoot
spine1
head
neck
righthand
rightforearm
rightarm
leftarm
leftforearm
lefthand

SIM-G [20]
86.6
64.6
17.1
17.3
64.1
88.9
48.3
55.1
86.1
102.5
81.4
58.5
61.0
84.5
107.1

SIM-G-E
83.4
61.5
15.1
15.6
61.2
87.2
44.7
51.6
81.3
96.5
74.4
54.4
55.5
81.6
105.6

83.1
61.2
15.5
15.8
60.7
85.2
45.5
52.6
83.2
95.6
74.8
54.9
57.6
84.1
106.4

83.2
61.3
15.5
15.8
60.6
86.2
44.9
51.7
81.3
96.1
75.1
54.1
56.7
81.8
104.8

Table 8. MPJPE (in mm, the lower the better) by each joint we use for evaluation.

Figure 10. All of our scenes from Cam2 view and the corresponding ﬁrst 3 layers of multi-layer depth map which encodes the entry and
exits of the objects. Here the depth value is displayed as disparity (1/depth) for visualization purposes.

Figure 11. Illustration of all the 8 scene meshes, visualized in MeshLab.

Figure 12. Left: Percentile of number of joints occluded in training set and testing set. Right: Percentile of number of joints closest to
each layer of multi-layer depth map.

Table 9. PCK3D (the higher the better, with threshold of 150 mm) by each joint we use for evaluation.

Baseline [40] ResNet-E ResNet-C ResNet-F

Method
rightfoot
rightleg
rightupleg
leftupleg
leftleg
leftfoot
spine1
head
neck
righthand
rightforearm
rightarm
leftarm
leftforearm
lefthand

77.3
90.0
100.0
100.0
89.5
76.8
20.1
94.7
68.7
74.1
83.8
93.7
92.9
86.0
77.5

80.7
90.7
100.0
100.0
90.3
79.8
20.7
94.7
71.5
75.8
84.6
94.1
93.0
86.7
78.7

Method
rightfoot
rightleg
rightupleg
leftupleg
leftleg
leftfoot
spine1
head
neck
righthand
rightforearm
rightarm
leftarm
leftforearm
lefthand

SIM-P [20]
71.2
87.1
100.0
100.0
88.2
69.9
100.0
96.3
85.8
65.0
80.4
95.1
92.6
82.2
67.0

SIM-P-E
73.0
89.0
100.0
100.0
90.2
72.2
100.0
96.2
87.0
66.3
81.9
95.0
93.8
82.3
66.0

80.3
90.6
100.0
100.0
89.8
79.0
20.5
94.5
69.6
75.5
84.4
94.0
93.1
86.5
78.4

71.8
87.2
100.0
100.0
88.0
70.2
100.0
96.3
85.7
65.5
80.7
95.2
92.7
81.9
66.6

81.5
91.3
100.0
100.0
90.6
80.5
21.0
94.7
71.6
76.5
85.1
94.2
93.2
87.0
79.0

73.6
89.2
100.0
100.0
90.6
72.2
100.0
96.3
87.2
67.2
82.9
95.4
94.1
82.9
66.5

SIM-P-C SIM-P-F

Table 10. PCK3d (in mm, the lower the better) by each joint we use for evaluation.

Figure 13. Visualization for the input images, with the overlay of ground truth pose in the same view(GT)(blue corresponds right human
skeletons while red represents left human skeletons), column 2-4 is the ﬁrst 3 layer of multi-layer depth map. Column 5 is the baseline
prediction overlay with the 1st layer multi-layer depth map while column 6 is our ResNet-F prediction. We can view from the ﬁgures
that geometry input and constraint always help the model give better prediction. The red rectangles highlight where baseline model give
prediction violating geometry or not as good as the full model.

Figure 14. More visualization for the cases that ResNet-F model prediction get more reasonable pose prediction than baseline methods
with the help of multi-layer depth map.

SIM-G-C SIM-G-F

Method
rightfoot
rightleg
rightupleg
leftupleg
leftleg
leftfoot
spine1
head
neck
righthand
rightforearm
rightarm
leftarm
leftforearm
lefthand

SIM-G [20]
89.9
95.1
100.0
100.0
95.5
89.4
100.0
99.0
91.9
81.2
91.1
98.4
97.3
89.3
79.8

SIM-G-E
91.2
96.4
100.0
100.0
96.9
90.4
100.0
99.3
93.4
84.5
93.4
98.9
98.4
91.2
80.8

91.4
96.4
100.0
100.0
96.9
90.7
100.0
99.2
93.3
84.8
93.3
98.8
98.0
90.3
80.8

91.3
96.5
100.0
100.0
97.0
90.6
100.0
99.2
93.5
84.3
93.3
98.8
98.2
91.2
81.2

Table 11. PCK3d (in mm, the lower the better) by each joint we use for evaluation.

