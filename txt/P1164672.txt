JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2019

1

Variable Rate Deep Image Compression with
Modulated Autoencoder

Fei Yang, Luis Herranz, Joost van de Weijer, Jos A. Iglesias Guitin, Antonio Lpez, Mikhail Mozerov, Member,
IEEE

9
1
0
2
 
c
e
D
 
1
1
 
 
]

V

I
.
s
s
e
e
[
 
 
1
v
6
2
5
5
0
.
2
1
9
1
:
v
i
X
r
a

Abstract—Variable rate is a requirement

for ﬂexible and
adaptable image and video compression. However, deep image
compression methods are optimized for a single ﬁxed rate-
distortion tradeoff. While this can be addressed by training
multiple models for different tradeoffs, the memory requirements
increase proportionally to the number of models. Scaling the
bottleneck representation of a shared autoencoder can provide
variable rate compression with a single shared autoencoder.
However, the R-D performance using this simple mechanism
degrades in low bitrates, and also shrinks the effective range of
bit rates. Addressing these limitations, we formulate the problem
of variable rate-distortion optimization for deep image com-
pression, and propose modulated autoencoders (MAEs), where
the representations of a shared autoencoder are adapted to the
speciﬁc rate-distortion tradeoff via a modulation network. Jointly
training this modulated autoencoder and modulation network
provides an effective way to navigate the R-D operational curve.
Our experiments show that the proposed method can achieve
almost the same R-D performance of independent models with
signiﬁcantly fewer parameters.

Index Terms—Deep image compression, variable rate, autoen-

coder, modulated autoencoder

I. INTRODUCTION

I MAGE compression is a fundamental and well-studied

problem in image processing and computer vision [2], [3],
[4]. The goal is to design binary representations (i.e. bitstream)
with minimal entropy [5] that minimize the number of bits
required to represent an image (i.e. rate) at a given level of
ﬁdelity (i.e. distortion) [6]. In many communication scenarios
the network or storage device may impose a constraint on the
maximum bitrate, which requires the image encoder to adapt
to a given bitrate budget. In other scenarios that constrain
may even change dynamically over time (e.g. video). In all
these cases, a rate control mechanism is required, and it is
available in most traditional image and video compression
codecs. In general, reducing the rate causes an increase in
the distortion (i.e. rate-distortion tradeoff). This mechanism is
typically based on scaling the latent representation prior to
quantization to obtain ﬁner or coarser quantization, and then
inverting the scaling at the decoder side (see Fig. 1a).

Recent studies show that deep image compression (DIC)
achieves comparable or even better results than classical image
compression techniques [7], [8], [9], [10], [1], [11], [12], [13],
[14], [15]. In this paradigm, the parameters of the encoder

F. Yang, L. Herranz, J. van de Weijer, J. A. Iglesias Guitin, A. Lpez and
M. Mozerov are with the Computer Vision Center of Department Informatics,
Universitat Autnoma de Barcelona, Barcelona, Spain, 08193.

F.Yang, is also with Key Laboratory of Information Fusion Technology,

Northwestern Polytechnical University, China

E-mail: fyang@cvc.uab.es

(a)

(b)

(c)

Fig. 1: Image compression and rate-distortion control: (a)
JPEG transform, (b) pre-trained autoencoder with bottleneck
scaling [1], and (c) our proposed modulated autoencoder
with joint training. Entropy coding/decoding is omitted for
simplicity and red frames emphasize trainable blocks.

and decoder are learned from certain image data by jointly
minimizing rate and distortion at a particular rate-distortion
tradeoff (instead of engineered by experts). However, variable
bitrate requires an independent model for every R-D tradeoff.
This is an obvious limitation, since it requires storing each
model separately, resulting in large memory requirement.

Addressing this limitation, Theis et al. [1] use a single
autoencoder whose bottleneck representation is scaled before
quantization depending on the target rate (see Fig. 1b). How-
ever, this approach only considers the importance of differ-
ent channels from the bottleneck representation of learned
autoencoders under R-D tradeoff constraint. In addition, the
autoencoder is optimized for a single speciﬁc R-D tradeoff
(typically high rate). These two aspects lead to a drop in
performance for low rates and a narrow effective range of
bit rates.

Addressing the limitations of multiple independent models
and bottleneck scaling, we formulate the problem of variable

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2019

rate-distortion optimization for DIC, and propose the modu-
lated autoencoder (MAE) framework, where the representa-
tions of a shared autoencoder at different layers are adapted
to a speciﬁc rate-distortion tradeoff via a modulating network.
The modulating network is conditioned on the target R-D
tradeoff, and synchronized with the actual tradeoff optimized
to learn the parameters of the autoencoder and the modulating
network. MAEs can achieve almost the same operational R-
D points of independent models with much fewer overall
parameters (i.e. just the shared autoencoder plus the small
overhead of the modulating network). Multi-layer modulation
does not suffer from the main limitations of bottleneck scaling,
namely, drop in performance for low rates, and shrinkage of
the effective range of rates.

II. BACKGROUND

Almost all lossy image and video compression approaches
follow the transform coding paradigm [16]. The basic structure
is a transform z = f (x) that takes an input image x ∈ RN
and obtains a transformed representation z, followed by a
quantizer q = Q (z) where q ∈ ZD is discrete-valued
vector. The decoder reverses the quantization (i.e. dequantizer
ˆz = Q−1 (q)) and the transform (i.e.
inverse transform)
image ˆx ∈ RN .
as ˆx = g (ˆz) reconstructing the output
Before the transmission (or storage), the discrete-valued vector
q is binarized and serialized into a bitstream b. Entropy
coding [17] is used to exploit the statistical redundancy in
that bitstream and reduce its length.

In deep image compression [1], [7], [8], the handcrafted
analysis and synthesis transforms are replaced by the encoder
z = f (x; θ) and decoder ˆx = g (ˆz; φ) of a convolutional
autoencoder, parametrized by θ and φ. The fundamental dif-
ference is that the transforms are not designed but learned
from training data.

The model is typically trained end-to-end minimizing the

following optimization problem

arg minθ,φR (b) + λD (x, ˆx)

(1)

where R (b) measures the rate of the bitstream b and D (ˆx, x)
represents a distortion metric between x and ˆx, and the
Lagrange multiplier λ controls the tradeoff between rate and
distortion, i.e. R-D tradeoff. Note that λ is a ﬁxed constant in
this case. The problem is solved using gradient descent and
backpropagation [18].

To make the model differentiable, which is required to apply
backpropagation, during training the quantizer is replaced by
a differentiable proxy function [1], [7], [8]. Similarly, entropy
coding is invertible, but it is necessary to compute the length
of the bitstream b. This is usually approximated by the entropy
of the distribution of quantized vector, R (b) ≈ H [Pq], which
is a lower bound of the actual bitstream length.

In this paper, we will use scalar quantization by (element-
wise) rounding to the nearest neighboor, i.e. q = (cid:98)z(cid:99), which
will be replaced by additive uniform noise as proxy during
(cid:1). There
training [8], i.e. ˜z = z + ∆z, with ∆z ∼ U (cid:0)− 1
is no de-quantization in the decoder, and the reconstructed
representation is simply ˆz = q. To estimate the entropy we

2 , 1

2

2

(2)

(3)

(4)

will use the entropy model described in [8] to approximate Pq
by p˜z (˜z). Finally, we will use mean squared error (MSE) as a
distortion metric. With these particular choices, (1) becomes

argmin
θ,φ

R (˜z; θ) + λD (x, ˆx; θ, φ)

with

R (˜z; θ) = Ex∼px,∆z∼U [− log2 p˜z (˜z)]

D (x, ˆx; θ, φ) = Ex∼px,∆z∼U

(cid:104)

(cid:107)x − ˆx(cid:107)2(cid:105)

III. MULTI-RATE DEEP IMAGE COMPRESSION WITH
MODULATED AUTOENCODERS

A. Problem deﬁnition

We are interested in deep image compression models that
can operate satisfactorily on different R-D tradeoffs, and
adapt to the speciﬁc R-D tradeoff when required. Note that
(Eq. 2) optimizes rate and distortion for a ﬁxed tradeoff λ.
We extend that formulation to multiple R-D tradeoffs (i.e.
λ ∈ Λ = {λ1, . . . , λM }) as the multi-rate-distortion problem

[R (˜z; θ, λ) + λD (x, ˆx; θ, φ, λ)]

(5)

argmin
θ,φ

(cid:88)

λ∈Λ

with

R (˜z; θ, λ) = Ex∼px,∆z∼U [− log2 p˜z (˜z)]

D (x, ˆx; θ, φ, λ) = Ex∼px,∆z∼U

(cid:104)

(cid:107)x − ˆx(cid:107)2(cid:105)

(6)

(7)

Here we omitted the explicit dependency on λ of the features
and (implicitely) the encoder and decoder), i.e. ˜z = ˜z (λ) =
f (x; θ, λ)) and ˆx = ˆx (λ) = g (˜z (λ); φ, λ). In the following
we may also omit the explicit dependency for conciseness.
Note also that this formulation can be easily extended to a con-
tinuous range of tradeoffs. Note also that these optimization
problems assume that all R-D operational points are equally
important. It could be possible to integrate an importance
function I (λ) to further give more importance to certain R-D
operational points if the application requires that. We assume
uniform importance (continuous or discrete) for simplicity.

B. Bottleneck scaling

A possible way to make the encoder and decoder aware of λ
is by simply scaling the latent representation in the bottleneck
before quantization (implicitly scaling the quantization bin),
and then inverting the scaling in the decoder. In that case,
q = Q (z (cid:12) s (λ)) and ˆx (λ) = g (ˆz (λ) (cid:12) (1/s (λ)); φ),
where s (λ) is the scaling factor speciﬁc for the tradeoff
λ. Conventional codecs use predeﬁned tables for s (λ) (the
descaling is often implicitly subsumed in the dequantization,
e.g. JPEG). Instead [1] learns them while keeping the encoder
and decoder ﬁxed, optimized for a particular R-D tradeoff (see
Fig. 1(b)).

We observe several limitations in this approach [1]: (1)
scaling only the bottleneck feature is not ﬂexible enough to
adapt to a large range of R-D tradeoffs, (2) using the inverse of
the scaling factor in the decoder may also limit the ﬂexibility
of the adaptation mechanism, (3) optimizing the parameters

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2019

3

Fig. 2: Modulated autoencoder (MAE) architecture, combining modulating networks and shared autoencoder.

of the autoencoder only for a single R-D tradeoff leads to
suboptimal parameters for other distant tradeoffs, (4) training
the autoencoder and the scaling factors separately may also be
limiting. In order to overcome this limitations we propose the
modulated autoencoder (MAE) framework.

C. Modulated autoencoders

Variable rate is achieved in MAEs by modulating the
internal representations in the encoder and the decoder (see
Fig. 2). Given a set of internal representations in the encoder
Z = {z1, . . . , zK} and in the decoder U = {u1, . . . , uL},
they are replaced by the corresponding modulated and de-
modulated versions Z(cid:48) = {z1 (cid:12) m1 (λ) , . . . , zK (cid:12) mK (λ)}
and U(cid:48) = {u1 (cid:12) d1 (λ) , . . . , uL (cid:12) dL (λ)}, where m (λ) =
(m1 (λ) , . . . , mK (λ)) and d (λ) = (d1 (λ) , . . . , dL (λ)) are
the modulating and demodulating functions.

Our MAE architecture extends the deep image compression
architecture proposed in [8] which combines convolutional
layers and GDN/IGDN layers [19]. In our experiments we
choose to modulate the outputs of the convolutional layers in
the encoder and decoder, i.e. Z and U, respectively.

The modulating function m (λ) for the encoder is learned
by a modulating network as m (λ) = m (λ; ϑ) and the
demodulating function d (λ) by the demodulating network
as d (λ) = d (λ; ϕ). As a result, the encoder has learnable
parameters {θ, ϑ} and the decoder {φ, ϕ}.

Finally, the optimization problem for the MAE is

argmin
θ,φ,ϑ,ϕ

(cid:88)

λ∈Λ

[R (˜z; θ, ϑ, λ) + λD (x, ˆx; θ, φ, ϑ, ϕ, λ)]

(8)

which extends equation (5) with the modulating/demodulating
networks and their corresponding parameters. All parameters
are learned jointly using gradient descent and backpropagation.
This mechanism is more ﬂexible than bottleneck scaling
since it allows multi-level modulation, decouples encoder and
decoder scaling and allows effective joint training of both
therefore optimizing
autoencoder and modulating network,
jointly to all R-D tradeoffs of interest.

D. Modulating and demodulating networks

The modulating network is a perceptron with two fully
connected layers and ReLU [20] and exponential nonlinearities
(see Fig. 2). The exponential nonlinearity guarantees positive
outputs which we found beneﬁcial in training. The output is
directly m (λ) = (m1 (λ) , . . . , mK (λ)). A small ﬁrst hidden
layer allows learning a meaningful nonlinear function between
tradeoffs and modulation vectors, which is more ﬂexible than
simple scaling factors and allows more expressive interpolation
between tradeoffs. In practice, we use normalized tradeoffs as
ˆλk = λk/maxλ∈Λ (λ). The demodulating network follows a
similar architecture.

IV. EXPERIMENTS

A. Experimental setup

We evaluated MAE on the CLIC dataset1, with 1,633 train
images (containing images of both “professional” and “mo-
bile” sets) and 44 test images (from the “professional” set).
Our implementation is based on the autoencoder architecture
of [8], which is augmented with modulation mechanisms and
modulating networks (two fully connected layers, with 50 and
192 units, respectively) for all the convolutional layers. We use
MSE as distortion metric. The model is trained with crops of
size 240 × 240 using Adam with a minibatch size of 8 and
initial learning rates of 0.0004 and 0.002 for MAE and the
entropy model, respectively. After 400k iterations, the learning
rates are halved for another 150k iterations. We also tested
MAEs with scale hyperpriors, as described in [21]. In our
experiments, we consider seven and four R-D tradeoffs for the
models without and with hyperprior, respectively. We consider
two baselines:

Independent models. Each R-D operational point is ob-
tained by training a new model with a different R-D tradeoff
λ in (2), requiring each model to be stored separately. This
provides the optimal R-D performance, but also requires more
memory to store all the models for different R-D tradeoffs.

1https://www.compression.cc

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2019

4

(a) Measured with PSNR.

Fig. 4: Modulated feature maps: (a) reconstructed images for
high (λ = 4096) and low (λ = 64) bitrates (ﬁrst row),
and the corresponding feature maps for two channels of the
bottleneck feature before quantization (second and third rows),
(b) original image for comparison, and (c) element-wise ratio
between the feature maps at the two different tradeoffs.

TABLE I: Model size (millions of parameters)

Architecture

w/o hyperprior [8]

w/ hyperprior [21]
(seven R-D tradeoffs) (four R-D tradeoffs)

Independent models
Bottleneck scaling [1]
Modulated AE (ours)

28.02 M
4.00 M
4.06 M

40.53 M
-
10.27 M

the image in Fig. 4b and the reconstructions for high and low
bitrates (see Fig. 4a). We show two of the 192 channels in
the bottleneck feature before quantization (see Fig. 4a), and
observe that the maps for the two bitrates are similar but the
range is higher for λ = 4096, so the quantization will be
ﬁner. This is also what we would expect in bottleneck scaling.
However, a closer look highlights the difference between both
methods. We also compute the element-wise ratio between
the bottleneck features at λ = 4096 and λ = 64, and show
the ratio image for the same channels of the example (see
Fig. 4c). We can see that the MAE learns to perform a more
complex adaptation of the features beyond simple channel-
wise bottleneck scaling since different parts of the image
have different ratios (the ratio image would be constant in
bottleneck scaling), which allows MAE to allocate bits more
freely when optimizing for different R-D tradeoffs, especially
for low bit-rate.

V. CONCLUSION

In this work, we introduce the modulated autoencoder,
a novel variable rate deep image compression framework,
based on multi-layer feature modulation and joint learning
of autoencoder parameters. MAEs can realize variable rate
image compression with a single model, while keeping the
performance close to the upper bound of independent models
that require signiﬁcantly more memory. We show that MAE
outperforms bottleneck scaling [1], especially for low bit-rates.

(b) Measured with MS-SSIM (dB).

Fig. 3: Rate-distortion curves of different methods on test
dataset.

Bottleneck scaling [1]. The autoencoder is optimized for
the highest R-D tradeoff in the range. Then the autoencoder
is frozen and the scaling parameters are learned for the other
tradeoffs.

B. Results

Fig. 3 shows the R-D operational curves for the proposed
MAE and the two baselines, for both PSNR and MS-SSIM.
We can see that the best R-D performance is obtained by using
independent models. Hyperprior models also have superior
R-D performance. Bottleneck scaling is satisfactory for high
bitrates, closer to the optimal R-D operational point of the
autoencoder, but degrades for lower bitrates. Interestingly,
bottleneck scaling cannot achieve as low bitrates as inde-
pendent models since the autoencoder is optimized for high
bitrate. This can be observed in the R-D curve as a narrower
range of bitrates. The proposed MAEs can achieve an R-
D performance very close to the corresponding independent
models, demonstrating that multi-layer modulation with joint
training is a more powerful mechanism to achieve effective
variable rate compression.

The main advantage of bottleneck scaling and MAEs is
that the autoencoder is shared, which results in much fewer
parameters than independent models, which depend on the
number of R-D tradeoffs (see Table. I). Both methods have a
small overhead due to the modulating networks or the scaling
factors (which is smaller in bottleneck scaling).

In order to illustrate the differences between the bottleneck
scaling and MAE rate adaptation mechanisms, we consider

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2019

5

REFERENCES

[1] L. Theis, W. Shi, A. Cunningham, and F. Husz´ar, “Lossy image compres-
sion with compressive autoencoders,” arXiv preprint arXiv:1703.00395,
2017.

[2] A. S. Lewis and G. Knowles, “Image compression using the 2-d wavelet
transform,” IEEE Transactions on image Processing, vol. 1, no. 2, pp.
244–250, 1992.

[3] J. D. Villasenor, B. Belzer, and J. Liao, “Wavelet ﬁlter evaluation for
image compression,” IEEE Transactions on image processing, vol. 4,
no. 8, pp. 1053–1060, 1995.

[4] D. Taubman and M. Marcellin, JPEG2000 image compression fun-
damentals, standards and practice: image compression fundamentals,
standards and practice.
Springer Science & Business Media, 2012,
vol. 642.

[5] C. E. Shannon, “A mathematical theory of communication,” Bell system

technical journal, vol. 27, no. 3, pp. 379–423, 1948.
[6] A. K. Jain, Fundamentals of digital image processing.

Englewood

Cliffs, NJ: Prentice Hall,, 1989.

[7] G. Toderici, S. M. O’Malley, S. J. Hwang, D. Vincent, D. Minnen,
S. Baluja, M. Covell, and R. Sukthankar, “Variable rate image compres-
sion with recurrent neural networks,” arXiv preprint arXiv:1511.06085,
2015.

[8] J. Ball´e, V. Laparra, and E. P. Simoncelli, “End-to-end optimized image

compression,” arXiv preprint arXiv:1611.01704, 2016.

[9] K. Gregor, F. Besse, D. J. Rezende, I. Danihelka, and D. Wierstra,
“Towards conceptual compression,” in Advances In Neural Information
Processing Systems, 2016, pp. 3549–3557.

[10] G. Toderici, D. Vincent, N. Johnston, S. Jin Hwang, D. Minnen, J. Shor,
and M. Covell, “Full resolution image compression with recurrent neural
networks,” in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2017, pp. 5306–5314.

[11] N. Johnston, D. Vincent, D. Minnen, M. Covell, S. Singh, T. Chinen,
S. Jin Hwang, J. Shor, and G. Toderici, “Improved lossy image compres-
sion with priming and spatially adaptive bit rates for recurrent networks,”
in Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2018, pp. 4385–4393.

[12] D. Liu, H. Ma, Z. Xiong, and F. Wu, “Cnn-based dct-like transform
for image compression,” in International Conference on Multimedia
Modeling. Springer, 2018, pp. 61–72.

[13] F. Mentzer, E. Agustsson, M. Tschannen, R. Timofte, and L. Van Gool,
“Conditional probability models for deep image compression,” in Pro-
the IEEE Conference on Computer Vision and Pattern
ceedings of
Recognition, 2018, pp. 4394–4402.

[14] D. Minnen, J. Ball´e, and G. D. Toderici, “Joint autoregressive and
hierarchical priors for learned image compression,” in Advances in
Neural Information Processing Systems, 2018, pp. 10 771–10 780.
[15] M. Li, W. Zuo, S. Gu, D. Zhao, and D. Zhang, “Learning convolutional
networks for content-weighted image compression,” in Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition,
2018, pp. 3214–3223.

[16] V. K. Goyal, “Theoretical foundations of transform coding,” IEEE Signal

Processing Magazine, vol. 18, no. 5, pp. 9–21, 2001.

[17] P. A. Wintz, “Transform picture coding,” Proceedings of the IEEE,

vol. 60, no. 7, pp. 809–820, 1972.

[18] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning internal
representations by error propagation,” California Univ San Diego La
Jolla Inst for Cognitive Science, Tech. Rep., 1985.

[19] J. Ball´e, V. Laparra, and E. P. Simoncelli, “Density modeling of
images using a generalized normalization transformation,” arXiv preprint
arXiv:1511.06281, 2015.

[20] V. Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted boltz-
mann machines,” in Proceedings of the 27th international conference on
machine learning (ICML-10), 2010, pp. 807–814.

[21] J. Ball´e, D. Minnen, S. Singh, S. J. Hwang, and N. Johnston, “Vari-
image compression with a scale hyperprior,” arXiv preprint

ational
arXiv:1802.01436, 2018.

