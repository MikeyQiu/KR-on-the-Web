Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks

Yang He1,2, Guoliang Kang2, Xuanyi Dong2, Yanwei Fu3∗, Yi Yang1,2∗
1SUSTech-UTS Joint Centre of CIS, Southern University of Science and Technology
2CAI, University of Technology Sydney
3The School of Data Science, Fudan University
{yang.he-1, guoliang.kang, xuanyi.dong}@student.uts.edu.au,
yanweifu@fudan.edu.cn, yi.yang@uts.edu.au

8
1
0
2
 
g
u
A
 
1
2
 
 
]

V
C
.
s
c
[
 
 
1
v
6
6
8
6
0
.
8
0
8
1
:
v
i
X
r
a

Abstract

This paper proposed a Soft Filter Pruning (SFP)
method to accelerate the inference procedure of
deep Convolutional Neural Networks (CNNs).
Speciﬁcally, the proposed SFP enables the pruned
ﬁlters to be updated when training the model af-
ter pruning. SFP has two advantages over previ-
ous works: (1) Larger model capacity. Updat-
ing previously pruned ﬁlters provides our approach
with larger optimization space than ﬁxing the ﬁl-
ters to zero. Therefore, the network trained by our
method has a larger model capacity to learn from
the training data. (2) Less dependence on the pre-
trained model. Large capacity enables SFP to train
from scratch and prune the model simultaneously.
In contrast, previous ﬁlter pruning methods should
be conducted on the basis of the pre-trained model
to guarantee their performance. Empirically, SFP
from scratch outperforms the previous ﬁlter prun-
ing methods. Moreover, our approach has been
demonstrated effective for many advanced CNN ar-
chitectures. Notably, on ILSCRC-2012, SFP re-
duces more than 42% FLOPs on ResNet-101 with
even 0.2% top-5 accuracy improvement, which has
advanced the state-of-the-art. Code is publicly
available on GitHub: https://github.com/he-y/soft-
ﬁlter-pruning

1 Introduction

The superior performance of deep CNNs usually comes from
the deeper and wider architectures, which cause the pro-
hibitively expensive computation cost. Even if we use more
efﬁcient architectures, such as residual connection [He et al.,
2016a] or inception module [Szegedy et al., 2015], it is still
difﬁcult in deploying the state-of-the-art CNN models on mo-
bile devices. For example, ResNet-152 has 60.2 million pa-
rameters with 231MB storage spaces; besides, it also needs
more than 380MB memory footprint and six seconds (11.3
billion ﬂoat point operations, FLOPs) to process a single im-
age on CPU. The storage, memory, and computation of this

∗Corrsponding Author

Figure 1: Hard Filter Pruning v.s. Soft Filter Pruning. We mark
the pruned ﬁlter as the green dashed box. For the hard ﬁlter pruning,
the pruned ﬁlters are always ﬁxed during the whole training proce-
dure. Therefore, the model capacity is reduced and thus harms the
performance because the dashed blue box is useless during train-
ing. On the contrary, our SFP allows the pruned ﬁlters to be updated
during the training procedure.
In this way, the model capacity is
recovered from the pruned model, and thus leads a better accuracy.

cumbersome model signiﬁcantly exceed the computing limi-
tation of current mobile devices. Therefore, it is essential to
maintain the small size of the deep CNN models which has
relatively low computational cost but high accuracy in real-
world applications.

Recent efforts have been made either on directly deleting
weight values of ﬁlters [Han et al., 2015b] (i.e., weight prun-
ing) or totally discarding some ﬁlters (i.e., ﬁlter pruning) [Li
et al., 2017; He et al., 2017; Luo et al., 2017]. However, the
weight pruning may result in the unstructured sparsity of ﬁl-
ters, which may still be less efﬁcient in saving the memory
usage and computational cost, since the unstructured model
cannot leverage the existing high-efﬁciency BLAS libraries.
In contrast, the ﬁlter pruning enables the model with struc-
tured sparsity and more efﬁcient memory usage than weight
pruning, and thus takes full advantage of BLAS libraries to
achieve a more realistic acceleration. Therefore, the ﬁlter
pruning is more advocated in accelerating the networks.

Nevertheless, most of the previous works on ﬁlter pruning

still suffer from the problems of (1) the model capacity re-
duction and (2) the dependence on pre-trained model. Specif-
ically, as shown in Fig. 1, most previous works conduct the
“hard ﬁlter pruning”, which directly delete the pruned ﬁlters.
The discarded ﬁlters will reduce the model capacity of origi-
nal models, and thus inevitably harm the performance. More-
over, to maintain a reasonable performance with respect to the
full models, previous works [Li et al., 2017; He et al., 2017;
Luo et al., 2017] always ﬁne-tuned the hard pruned model
after pruning the ﬁlters of a pre-trained model, which how-
ever has low training efﬁciency and often requires much more
training time than the traditional training schema.

To address the above mentioned two problems, we propose
a novel Soft Filter Pruning (SFP) approach. The SFP dynam-
ically prunes the ﬁlters in a soft manner. Particularly, before
ﬁrst training epoch, the ﬁlters of almost all layers with small
(cid:96)2-norm are selected and set to zero. Then the training data
is used to update the pruned model. Before the next training
epoch, our SFP will prune a new set of ﬁlters of small (cid:96)2-
norm. These training process is continued until converged.
Finally, some ﬁlters will be selected and pruned without fur-
ther updating. The SFP algorithm enables the compressed
network to have a larger model capacity, and thus achieve a
higher accuracy than others.

Contributions. We highlight three contributions: (1) We
propose SFP to allow the pruned ﬁlters to be updated dur-
ing the training procedure. This soft manner can dramatically
maintain the model capacity and thus achieves the superior
performance. (2) Our acceleration approach can train a model
from scratch and achieve better performance compared to the
state-of-the-art.
In this way, the ﬁne-tuning procedure and
the overall training time is saved. Moreover, using the pre-
trained model can further enhance the performance of our ap-
proach to advance the state-of-the-art in model acceleration.
(3) The extensive experiment on two benchmark datasets
demonstrates the effectiveness and efﬁciency of our SFP. We
accelerate ResNet-110 by two times with about 4% relative
accuracy improvement on CIFAR-10, and also achieve state-
of-the-art results on ILSVRC-2012.

2 Related Works

Most previous works on accelerating CNNs can be roughly
divided into three categories, namely, matrix decomposition,
low-precision weights, and pruning.
In particular, the ma-
trix decomposition of deep CNN tensors is approximated by
the product of two low-rank matrices [Jaderberg et al., 2014;
Zhang et al., 2016; Tai et al., 2016]. This can save the
computational cost. Some works [Zhu et al., 2017; Zhou
et al., 2017] focus on compressing the CNNs by using low-
precision weights. Pruning-based approaches aim to remove
the unnecessary connections of the neural network [Han et
al., 2015b; Li et al., 2017]. Essentially, the work of this pa-
per is based on the idea of pruning techniques; and the ap-
proaches of matrix decomposition and low-precision weights
are orthogonal but potentially useful here – it may be still
worth simplifying the weight matrix after pruning ﬁlters,
which would be taken as future work.

Weight Pruning. Many recent works [Han et al., 2015b;

Han et al., 2015a; Guo et al., 2016] pruning weights of
neural network resulting in small models. For example,
[Han et al., 2015b] proposed an iterative weight pruning
method by discarding the small weights whose values are
[Guo et al., 2016] proposed the dy-
below the threshold.
namic network surgery to reduce the training iteration while
maintaining a good prediction accuracy. [Wen et al., 2016;
Lebedev and Lempitsky, 2016] leveraged the sparsity prop-
erty of feature maps or weight parameters to accelerate the
CNN models. A special case of weight pruning is neuron
pruning. However, pruning weights always leads to unstruc-
tured models, so the model cannot leverage the existing efﬁ-
cient BLAS libraries in practice. Therefore, it is difﬁcult for
weight pruning to achieve realistic speedup.

Filter Pruning. Concurrently with our work, some ﬁl-
ter pruning strategies [Li et al., 2017; Liu et al., 2017;
He et al., 2017; Luo et al., 2017] have been explored. Prun-
ing the ﬁlters leads to the removal of the corresponding fea-
ture maps. This not only reduces the storage usage on de-
vices but also decreases the memory footprint consumption
to accelerate the inference. [Li et al., 2017] uses (cid:96)1-norm to
select unimportant ﬁlters and explores the sensitivity of lay-
ers for ﬁlter pruning. [Liu et al., 2017] introduces (cid:96)1 regu-
larization on the scaling factors in batch normalization (BN)
layers as a penalty term, and prune channel with small scal-
ing factors in BN layers. [Molchanov et al., 2017] proposes
a Taylor expansion based pruning criterion to approximate
the change in the cost function induced by pruning. [Luo et
al., 2017] adopts the statistics information from next layer to
guide the importance evaluation of ﬁlters. [He et al., 2017]
proposes a LASSO-based channel selection strategy, and a
least square reconstruction algorithm to prune ﬁlers. How-
ever, for all these ﬁlter pruning methods, the representative
capacity of neural network after pruning is seriously affected
by smaller optimization space.

Discussion. To the best of our knowledge, there is only one
approach that uses the soft manner to prune weights [Guo et
al., 2016]. We would like to highlight our advantages com-
pared to this approach as below: (1) Our SPF focuses on
the ﬁlter pruning, but they focus on the weight pruning. As
discussed above, weight pruning approaches lack the practi-
cal implementations to achieve the realistic acceleration. (2)
[Guo et al., 2016] paid more attention to the model com-
pression, whereas our approach can achieve both compres-
sion and acceleration of the model. (3) Extensive experiments
have been conducted to validate the effectiveness of our pro-
posed approach both on large-scale datasets and the state-of-
the-art CNN models. In contrast, [Guo et al., 2016] only had
the experiments on Alexnet which is more redundant the ad-
vanced models, such as ResNet.

3 Methodology
3.1 Preliminaries
We will formally introduce the symbol and annotations in this
section. The deep CNN network can be parameterized by
{W(i) ∈ RNi+1×Ni×K×K, 1 ≤ i ≤ L} W(i) denotes a
matrix of connection weights in the i-th layer. Ni denotes the
number of input channels for the i-th convolution layer. L

denotes the number of layers. The shapes of input tensor U
and output tensor V are Ni × Hi × Wi and Ni+1 × Hi+1 ×
Wi+1, respectively. The convolutional operation of the i-th
layer can be written as:

Vi,j = Fi,j ∗ U for 1 ≤ j ≤ Ni+1,
(1)
where Fi,j ∈ RNi×K×K represents the j-th ﬁlter of the i-th
layer. W(i) consists of {Fi,j, 1 ≤ j ≤ Ni+1}. The Vi,j
represents the j-th output feature map of the i-th layer.

Pruning ﬁlters can remove the output feature maps.
In
this way, the computational cost of the neural network will
Let us assume the pruning rate of
reduce remarkably.
SFP is Pi for the i-th layer. The number of ﬁlters of
this layer will be reduced from Ni+1 to Ni+1(1 − Pi),
thereby the size of the output tensor Vi,j can be reduced to
Ni+1(1 − Pi) × Hi+1 × Wi+1. As the output tensor of i-th
layer is the input tensor of i + 1-th layer, we can reduce the
input size of i-th layer to achieve a higher acceleration ratio.

3.2 Soft Filter Pruning (SFP)
Most of previous ﬁlter pruning works [Li et al., 2017; Liu et
al., 2017; He et al., 2017; Luo et al., 2017] compressed the
deep CNNs in a hard manner. We call them as the hard ﬁlter
pruning. Typically, these algorithms ﬁrstly prune ﬁlters of a
single layer of a pre-trained model and ﬁne-tune the pruned
model to complement the degrade of the performance. Then
they prune the next layer and ﬁne-tune the model again until
the last layer of the model is pruned. However, once ﬁlters are
pruned, these approaches will not update these ﬁlters again.
Therefore, the model capacity is drastically reduced due to
the removed ﬁlters; and such a hard pruning manner affects
the performance of the compressed models negatively.

As summarized in Alg. 1, the proposed SFP algorithm can
dynamically remove the ﬁlters in a soft manner. Speciﬁcally,
the key is to keep updating the pruned ﬁlters in the train-
ing stage. Such an updating manner brings several bene-
ﬁts. It not only keeps the model capacity of the compressed
deep CNN models as the original models, but also avoids the
greedy layer by layer pruning procedure and enable pruning
almost all layers at the same time. More speciﬁcally, our
approach can prune a model either in the process of train-
ing from scratch, or a pre-trained model.
In each training
epoch, the full model is optimized and trained on the training
data. After each epoch, the (cid:96)2-norm of all ﬁlters are com-
puted for each weighted layer and used as the criterion of our
ﬁlter selection strategy. Then we will prune the selected ﬁl-
ters by setting the corresponding ﬁlter weights as zero, which
is followed by next training epoch. Finally, the original deep
CNNs are pruned into a compact and efﬁcient model. The
details of SFP is illustratively explained in Alg. 1, which can
be divided into the following four steps.

Filter selection. We use the (cid:96)p-norm to evaluate the impor-
tance of each ﬁlter as Eq. (2). In general, the convolutional
results of the ﬁlter with the smaller (cid:96)p-norm lead to relatively
lower activation values; and thus have a less numerical im-
pact on the ﬁnal prediction of deep CNN models.
In term
of this understanding, such ﬁlters of small (cid:96)p-norm will be
given high priority of being pruned than those of higher (cid:96)p-
norm. Particularly, we use a pruning rate Pi to select Ni+1Pi

Algorithm 1 Algorithm Description of SFP
Input: training data: X, pruning rate: Pi

the model with parameters W = {W(i), 0 ≤ i ≤ L}.
Initialize the model parameter W
for epoch = 1; epoch ≤ epochmax; epoch + + do
Update the model parameter W based on X
for i = 1; i ≤ L; i + + do

Calculate the (cid:96)2-norm for each ﬁlter (cid:107)Fi,j(cid:107)2, 1 ≤

j ≤ Ni+1

end for

Zeroize Ni+1Pi ﬁlters by (cid:96)2-norm ﬁlter selection

end for
Obtain the compact model with parameters W∗ from W

Output: The compact model and its parameters W∗

unimportant ﬁlters for the i-th weighted layer. In other words,
the lowest Ni+1Pi ﬁlters are selected, e.g., the blue ﬁlters in
Fig. 2. In practice, (cid:96)2-norm is used based on the empirical
analysis.

(cid:118)
(cid:117)
(cid:117)
(cid:107)Fi,j(cid:107)p = p
(cid:116)

Ni(cid:88)

K
(cid:88)

K
(cid:88)

|Fi,j(n, k1, k2)|p,

(2)

n=1

k1=1

k2=1
Filter Pruning. We set the value of selected Ni+1Pi ﬁlters
to zero (see the ﬁlter pruning step in Fig. 2). This can tem-
porarily eliminate their contribution to the network output.
Nevertheless, in the following training stage, we still allow
these selected ﬁlters to be updated, in order to keep the repre-
sentative capacity and the high performance of the model.

In the ﬁlter pruning step, we simply prune all the weighted
layers at the same time.
In this way, we can prune each
ﬁlter in parallel, which would cost negligible computation
time.
In contrast, the previous ﬁlter pruning methods al-
ways conduct layer by layer greedy pruning. After prun-
ing ﬁlters of one single layer, existing methods always re-
quire training to converge the network [Luo et al., 2017;
He et al., 2017]. This procedure cost much extra computa-
tion time, especially when the depth increases. Moreover, we
use the same pruning rate for all weighted layers. Therefore,
we need only one hyper-parameter Pi = P to balance the
acceleration and accuracy. This can avoid the inconvenient
hyper-parameter search or the complicated sensitivity anal-
ysis [Li et al., 2017]. As we allow the pruned ﬁlters to be
updated, the model has a large model capacity and becomes
more ﬂexible and thus can well balance the contribution of
each ﬁlter to the ﬁnal prediction.

Reconstruction. After the pruning step, we train the net-
work for one epoch to reconstruct the pruned ﬁlters. As
shown in Fig. 2, the pruned ﬁlters are updated to non-zero by
back-propagation. In this way, SFP allows the pruned model
to have the same capacity as the original model during train-
ing. In contrast, hard ﬁlter pruning decreases the number of
feature maps. The reduction of feature maps would dramat-
ically reduce the model capacity, and further harm the per-
formance. Previous pruning methods usually require a pre-
trained model and then ﬁne-tune it. However, as we inte-
grate the pruning step into the normal training schema, our
approach can train the model from scratch. Therefore, the

Figure 2: Overview of SFP. At the end of each training epoch, we prune the ﬁlters based on their importance evaluations. The ﬁlters are ranked
by their (cid:96)p-norms (purple rectangles) and the small ones (blue circles) are selected to be pruned. After ﬁlter pruning, the model undergoes a
reconstruction process where pruned ﬁlters are capable of being reconstructed (i.e., updated from zeros) by the forward-backward process.
(a): ﬁlter instantiations before pruning. (b): ﬁlter instantiations after pruning. (c): ﬁlter instantiations after reconstruction.

ﬁne-tuning stage is no longer necessary for SFP. As we will
show in experiments, the network trained from scratch by
SFP can obtain the competitive results with the one trained
from a well-trained model by others. By leveraging the pre-
trained model, SFP obtains a much higher performance and
advances the state-of-the-art.

Obtaining Compact Model. SFP iterates over the ﬁlter
selection, ﬁlter pruning and reconstruction steps. After the
model gets converged, we can obtain a sparse model contain-
ing many “zero ﬁlters”. One “zero ﬁlter” corresponds to one
feature map. The features maps, corresponding to those “zero
ﬁlters”, will always be zero during the inference procedure.
There will be no inﬂuence to remove these ﬁlters as well as
the corresponding feature maps. Speciﬁcally, for the prun-
ing rate Pi in the i-th layer, only Ni+1(1 − Pi) ﬁlters are
non-zero and have an effect on the ﬁnal prediction. Consider
pruning the previous layer, the input channel of i-th layer is
changed from Ni to Ni(1 − Pi−1). We can thus re-build
the i-th layer into a smaller one. Finally, a compact model
{W∗(i) ∈ RNi+1(1−Pi)×Ni(1−Pi−1)×K×K} is obtained.

3.3 Computation Complexity Analysis
Theoretical speedup analysis. Suppose the ﬁlter pruning
rate of the ith layer is Pi, which means the Ni+1 × Pi ﬁl-
ters are set to zero and pruned from the layer, and the other
Ni+1 × (1 − Pi) ﬁlters remain unchanged, and suppose the
size of the input and output feature map of ith layer is Hi×Wi
and Hi+1 × Wi+1. Then after ﬁlter pruning, the dimension
of useful output feature map of the ith layer decreases from
Ni+1 × Hi+1 × Wi+1 to Ni+1(1 − Pi) × Hi+1 × Wi+1.
Note that the output of ith layer is the input of (i + 1) th
layer. And we further prunes the (i + 1)th layer with a ﬁl-
ter pruning rate Pi+1, then the calculation of (i + 1)th layer
is decrease from Ni+2 × Ni+1 × k2 × Hi+2 × Wi+2 to
Ni+2(1 − Pi+1) × Ni+1(1 − Pi) × k2 × Hi+2 × Wi+2. In
other words, a proportion of 1 − (1 − Pi+1) × (1 − Pi) of the
original calculation is reduced, which will make the neural
network inference much faster.

Realistic speedup analysis. In theoretical speedup anal-
ysis, other operations such as batch normalization (BN) and
pooling are negligible comparing to convolution operations.
Therefore, we consider the FLOPs of convolution operations

for computation complexity comparison, which is commonly
used in previous work [Li et al., 2017; Luo et al., 2017].
However, reduced FLOPs cannot bring the same level of re-
alistic speedup because non-tensor layers (e.g., BN and pool-
ing layers) also need the inference time on GPU [Luo et al.,
2017]. In addition, the limitation of IO delay, buffer switch
and efﬁciency of BLAS libraries also lead to the wide gap be-
tween theoretical and realistic speedup ratio. We compare the
theoretical and realistic speedup in Section 4.3.

4 Evaluation and Results

4.1 Benchmark Datasets and Experimental Setting

Our method is evaluated on two benchmarks: CIFAR-
10 [Krizhevsky and Hinton, 2009] and ILSVRC-2012 [Rus-
sakovsky et al., 2015]. The CIFAR-10 dataset contains
50,000 training images and 10,000 testing images, which are
categorized into 10 classes.
ILSVRC-2012 is a large-scale
dataset containing 1.28 million training images and 50k val-
idation images of 1,000 classes. Following the common set-
ting in [Luo et al., 2017; He et al., 2017; Dong et al., 2017a],
we focus on pruning the challenging ResNet model in this
paper. SFP should also be effective on different computer
vision tasks, such as [Kang et al., 2017; Ren et al., 2015;
Dong et al., 2018; Shen et al., 2018b; Yang et al., 2010;
Shen et al., 2018a; Dong et al., 2017b], and we will explore
this in future.

In the CIFAR-10 experiments, we use the default parame-
ter setting as [He et al., 2016b] and follow the training sched-
ule in [Zagoruyko and Komodakis, 2016]. On ILSVRC-2012,
we follow the same parameter settings as [He et al., 2016a;
He et al., 2016b]. We use the same data argumentation strate-
gies with PyTorch ofﬁcial examples [Paszke et al., 2017].

We conduct our SFP operation at the end of every training
epoch. For pruning a scratch model, we use the normal train-
ing schedule. For pruning a pre-trained model, we reduce the
learning rate by 10 compared to the schedule for the scratch
model. We run each experiment three times and report the
“mean ± std”. We compare the performance with other state-
of-the-art acceleration algorithms, e.g., [Dong et al., 2017a;
Li et al., 2017; He et al., 2017; Luo et al., 2017].

Depth

Fine-tune? Baseline Accu. (%) Accelerated Accu. (%) Accu. Drop (%) FLOPs Pruned FLOPs(%)

20

32

56

110

Method
[Dong et al., 2017a]
Ours(10%)
Ours(20%)
Ours(30%)
[Dong et al., 2017a]
Ours(10%)
Ours(20%)
Ours(30%)
[Li et al., 2017]
[Li et al., 2017]
[He et al., 2017]
[He et al., 2017]
Ours(10%)
Ours(20%)
Ours(30%)
Ours(30%)
Ours(40%)
Ours(40%)
[Li et al., 2017]
[Li et al., 2017]
[Dong et al., 2017a]
Ours(10%)
Ours(20%)
Ours(30%)
Ours(30%)

N
N
N
N
N
N
N
N
N
Y
N
Y
N
N
N
Y
N
Y
N
Y
N
N
N
N
Y

91.53
92.20 ± 0.18
92.20 ± 0.18
92.20 ± 0.18
92.33
92.63 ± 0.70
92.63 ± 0.70
92.63 ± 0.70
93.04
93.04
92.80
92.80
93.59 ± 0.58
93.59 ± 0.58
93.59 ± 0.58
93.59 ± 0.58
93.59 ± 0.58
93.59 ± 0.58
93.53
93.53
93.63
93.68 ± 0.32
93.68 ± 0.32
93.68 ± 0.32
93.68 ± 0.32

91.43
92.24 ± 0.33
91.20 ± 0.30
90.83 ± 0.31
90.74
93.22 ± 0.09
90.63 ± 0.37
90.08 ± 0.08
91.31
93.06
90.90
91.80
93.89 ± 0.19
93.47 ± 0.24
93.10 ± 0.20
93.78 ± 0.22
92.26 ± 0.31
93.35 ± 0.31
92.94
93.30
93.44
93.83 ± 0.19
93.93 ± 0.41
93.38 ± 0.30
93.86 ± 0.21

0.10
-0.04
1.00
1.37
1.59
-0.59
0.00
0.55
1.75
-0.02
1.90
1.00
-0.30
0.12
0.49
-0.19
1.33
0.24
0.61
0.20
0.19
-0.15
-0.25
0.30
-0.18

3.20E7
3.44E7
2.87E7
2.43E7
4.70E7
5.86E7
4.90E7
4.03E7
9.09E7
9.09E7
-
-
1.070E8
8.98E7
7.40E7
7.40E7
5.94E7
5.94E7
1.55E8
1.55E8
-
2.16E8
1.82E8
1.50E8
1.50E8

20.3
15.2
29.3
42.2
31.2
14.9
28.8
41.5
27.6
27.6
50.0
50.0
14.7
28.4
41.1
41.1
52.6
52.6
38.6
38.6
34.2
14.6
28.2
40.8
40.8

Table 1: Comparison of pruning ResNet on CIFAR-10. In “Fine-tune?” column, “Y” and “N” indicate whether to use the pre-trained model
as initialization or not, respectively. The “Accu. Drop” is the accuracy of the pruned model minus that of the baseline model, so negative
number means the accelerated model has a higher accuracy than the baseline model. A smaller number of ”Accu. Drop” is better.

4.2 ResNet on CIFAR-10
Settings. For CIFAR-10 dataset, we test our SFP on ResNet-
20, 32, 56 and 110. We use several different pruning rates,
and also analyze the difference between using the pre-trained
model and from scratch.

Results. Tab. 1 shows the results. Our SFP could achieve
a better performance than the other state-of-the-art hard ﬁlter
pruning methods. For example, [Li et al., 2017] use the hard
pruning method to accelerate ResNet-110 by 38.6% speedup
ratio with 0.61% accuracy drop when without ﬁne-tuning.
When using pre-trained model and ﬁne-tuning, the accuracy
drop becomes 0.20%. However, we can accelerate the infer-
ence of ResNet-110 to 40.8% speed-up with only 0.30% ac-
curacy drop without ﬁne-tuning. When using the pre-trained
model, we can even outperform the original model by 0.18%
with about more than 40% FLOPs reduced.

These results validate the effectiveness of SFP, which can
produce a more compressed model with comparable perfor-
mance to the original model.

4.3 ResNet on ILSVRC-2012
Settings. For ILSVRC-2012 dataset, we test our SFP on
ResNet-18, 34, 50 and 101; and we use the same pruning rate
30% for all the models. All the convolutional layer of ResNet
are pruned with the same pruning rate at the same time. (We
do not prune the projection shortcuts for simpliﬁcation, which
only need negligible time and do not affect the overall cost.)
Results. Tab. 2 shows that SFP outperforms other state-
of-the-art methods.
For ResNet-34, SFP without ﬁne-
tuning achieves more inference speedup to the hard pruning

method [Luo et al., 2017], but the accuracy of our pruned
model exceeds their model by 2.57%. Moreover, for prun-
ing a pre-trained ResNet-101, SFP reduces more than 40%
FLOPs of the model with even 0.2% top-5 accuracy in-
crease, which is the state-of-the-art result.
In contrast, the
performance degradation is inevitable for hard ﬁlter pruning
method. Maintained model capacity of SFP is the main rea-
son for the superior performance. In addition, the non-greedy
all-layer pruning method may have a better performance than
the locally optimal solution obtained from previous greedy
pruning method, which seems to be another reason. Occa-
sionally, large performance degradation happens for the pre-
trained model (e.g., 14.01% top-1 accuracy drop for ResNet-
50). This will be explored in our future work.

To test the realistic speedup ratio, we measure the forward
time of the pruned models on one GTX1080 GPU with a
batch size of 64 (shown in Tab. 3). The gap between theo-
retical and realistic model may come from and the limitation
of IO delay, buffer switch and efﬁciency of BLAS libraries.

4.4 Ablation Study
We conducted extensive ablation studies to further analyze
each component of SFP.

Filter Selection Criteria. The magnitude based criteria
such as (cid:96)p-norm are widely used to ﬁlter selection because
computational resources cost is small [Li et al., 2017]. We
compare the (cid:96)2-norm and (cid:96)1-norm. For (cid:96)1-norm criteria,
the accuracy of the model under pruning rate 10%, 20%,
30% are 93.68±0.60%, 93.68±0.76% and 93.34±0.12%,
respectively. While for (cid:96)2-norm criteria,
the accuracy

Depth

Method

Fine-
tune?

18

34

50

101

Ours(30%)

[Dong et al., 2017a] N
N
[Dong et al., 2017a] N
Y
N
Y
Y
N
Y
N
Y

[Li et al., 2017]
Ours(30%)
[He et al., 2017]
[Luo et al., 2017]
Ours(30%)
Ours(30%)
Ours(30%)
Ours(30%)

Top-1 Accu.
Baseline(%)
69.98
70.28
73.42
73.23
73.92
-
72.88
76.15
76.15
77.37
77.37

Top-1 Accu.
Accelerated(%)
66.33
67.10
72.99
72.17
71.83
-
72.04
74.61
62.14
77.03
77.51

Top-5 Accu.
Baseline(%)
89.24
89.63
91.36
-
91.62
92.20
91.14
92.87
92.87
93.56
93.56

Top-5 Accu.
Accelerated(%)
86.94
87.78
91.19
-
90.33
90.80
90.67
92.06
84.60
93.46
93.71

Top-1 Accu.
Drop(%)
3.65
3.18
0.43
1.06
2.09
-
0.84
1.54
14.01
0.34
-0.14

Top-5 Accu.
Drop(%)
2.30
1.85
0.17
-
1.29
1.40
0.47
0.81
8.27
0.10
-0.20

Pruned
FLOPs(%)
34.6
41.8
24.8
24.2
41.1
50.0
36.7
41.8
41.8
42.2
42.2

Table 2: Comparison of pruning ResNet on ImageNet. “Fine-tune?” and ”Accu. Drop” have the same meaning with Tab. 1.

Model

ResNet-18
ResNet-34
ResNet-50
ResNet-101

Baseline
time (ms)
37.10
63.97
135.01
219.71

Pruned
time (ms)
26.97
45.14
94.66
148.64

Realistic
Speed-up(%)
27.4
29.4
29.8
32.3

Theoretical
Speed-up(%)
41.8
41.1
41.8
42.2

Table 3: Comparison on the theoretical and realistic speedup. We
only count the time consumption of the forward procedure.

(a) Different Pruning Rates

(b) Different SFP Intervals

Figure 3: Accuracy of ResNet-110 on CIFAR-10 regarding differ-
ent hyper-parameters. (Solid line and shadow denotes the mean and
standard deviation of three experiment, respectively.)

are 93.89±0.19%, 93.93±0.41% and 93.38±0.30%, respec-
tively. The performance of (cid:96)2-norm criteria is slightly better
than that of (cid:96)1-norm criteria. The result of (cid:96)2-norm is dom-
inated by the largest element, while the result of (cid:96)1-norm is
also largely affected by other small elements. Therefore, ﬁl-
ters with some large weights would be preserved by the (cid:96)2-
norm criteria. So the corresponding discriminative features
are kept so the performance of the pruned model is better.

Varying pruning rates. To comprehensively understand
SFP, we test the accuracy of different pruning rates for
ResNet-110, shown in Fig. 3(a). As the pruning rate in-
creases, the accuracy of the pruned model ﬁrst rises above the
baseline model and then drops approximately linearly. For
the pruning rate between 0% and about 23%, the accuracy of
the accelerated model is higher than the baseline model. This
shows that our SFP has a regularization effect on the neural
network because SFP reduces the over-ﬁtting of the model.

Sensitivity of SFP interval. By default, we conduct our
SFP operation at the end of every training epoch. However,

different SFP intervals may lead to different performance; so
we explore the sensitivity of SFP interval. We use the ResNet-
110 under pruning rate 30% as a baseline, and change the SFP
interval from one epoch to ten epochs, as shown in Fig. 3(b).
It is shown that the model accuracy has no large ﬂuctuation
along with the different SFP intervals. Moreover, the model
accuracy of most (80%) intervals surpasses the accuracy of
one epoch interval. Therefore, we can even achieve a better
performance if we ﬁne-tune this parameter.

Selection of pruned layers. Previous works always prune
a portion of the layers of the network. Besides, different lay-
ers always have different pruning rates. For example, [Li et
al., 2017] only prunes insensitive layers, [Luo et al., 2017]
skips the last layer of every block of the ResNet, and [Luo
et al., 2017] prunes more aggressive for shallower layers and
prune less for deep layers. Similarly, we compare the perfor-
mance of pruning ﬁrst and second layer of all basic blocks
of ResNet-110. We set the pruning rate as 30%. The model
with all the ﬁrst layers of blocks pruned has an accuracy of
93.96 ± 0.13%, while that with the second layers of blocks
pruned has an accuracy of 93.38 ± 0.44%. Therefore, differ-
ent layers have different sensitivity for SFP, and careful selec-
tion of pruned layers would potentially lead to performance
improvement, although more hyper-parameters are needed.

5 Conclusion and Future Work

In this paper, we propose a soft ﬁlter pruning (SFP) approach
to accelerate the deep CNNs. During the training procedure,
SFP allows the pruned ﬁlters to be updated. This soft manner
can maintain the model capacity and thus achieve the supe-
rior performance. Remarkably, SFP can achieve the competi-
tive performance compared to the state-of-the-art without the
pre-trained model. Moreover, by leveraging the pre-trained
model, SFP achieves a better result and advances the state-
of-the-art. Furthermore, SFP can be combined with other
acceleration algorithms, e.g., matrix decomposition and low-
precision weights, to further improve the performance.

Acknowledgments

Yi Yang is the recipient of a Google Faculty Research Award.
We acknowledge the Data to Decisions CRC (D2D CRC), the
Cooperative Research Centres Programme and ARC’s DE-

CRA (project DE170101415) for funding this research. We
thank Amazon for the AWS Cloud Credits.

References
[Dong et al., 2017a] Xuanyi Dong, Junshi Huang, Yi Yang,
and Shuicheng Yan. More is less: A more complicated
network with less inference complexity. In CVPR, 2017.
[Dong et al., 2017b] Xuanyi Dong, Deyu Meng, Fan Ma,
and Yi Yang. A dual-network progressive approach to
weakly supervised object detection. In ACM Multimedia,
2017.

[Dong et al., 2018] Xuanyi Dong, Shoou-I Yu, Xinshuo
Weng, Shih-En Wei, Yi Yang, and Yaser Sheikh.
Supervision-by-Registration: An unsupervised approach
to improve the precision of facial landmark detectors. In
CVPR, 2018.

[Guo et al., 2016] Yiwen Guo, Anbang Yao, and Yurong
Chen. Dynamic network surgery for efﬁcient DNNs. In
NIPS, 2016.

[Han et al., 2015a] Song Han, Huizi Mao, and William J
Dally. Deep compression: Compressing deep neural net-
works with pruning, trained quantization and huffman cod-
ing. In ICLR, 2015.

[Han et al., 2015b] Song Han, Jeff Pool, John Tran, and
William Dally. Learning both weights and connections for
efﬁcient neural network. In NIPS, 2015.

[He et al., 2016a] Kaiming He, Xiangyu Zhang, Shaoqing
Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, 2016.

[He et al., 2016b] Kaiming He, Xiangyu Zhang, Shaoqing
Identity mappings in deep residual

Ren, and Jian Sun.
networks. In ECCV, 2016.

[He et al., 2017] Yihui He, Xiangyu Zhang, and Jian Sun.
Channel pruning for accelerating very deep neural net-
works. In ICCV, 2017.

[Jaderberg et al., 2014] Max Jaderberg, Andrea Vedaldi, and
Andrew Zisserman. Speeding up convolutional neural net-
works with low rank expansions. In BMVC, 2014.

[Kang et al., 2017] Guoliang Kang, Jun Li, and Dacheng
Tao. Shakeout: A new approach to regularized deep neural
network training. IEEE T-PAMI, 2017.

[Krizhevsky and Hinton, 2009] Alex Krizhevsky and Geof-
frey Hinton. Learning multiple layers of features from tiny
images. 2009.

[Lebedev and Lempitsky, 2016] Vadim Lebedev and Victor
Lempitsky. Fast ConvNets using group-wise brain dam-
age. In CVPR, 2016.

[Li et al., 2017] Hao Li, Asim Kadav,

Igor Durdanovic,
Hanan Samet, and Hans Peter Graf. Pruning ﬁlters for
efﬁcient ConvNets. In ICLR, 2017.

[Luo et al., 2017] Jian-Hao Luo, Jianxin Wu, and Weiyao
Lin. ThiNet: A ﬁlter level pruning method for deep neural
network compression. In ICCV, 2017.

[Molchanov et al., 2017] Pavlo Molchanov, Stephen Tyree,
Tero Karras, Timo Aila, and Jan Kautz. Pruning convolu-
tional neural networks for resource efﬁcient transfer learn-
ing. In ICLR, 2017.

[Paszke et al., 2017] Adam Paszke, Sam Gross, Soumith
Chintala, Gregory Chanan, Edward Yang, Zachary De-
Vito, Zeming Lin, Alban Desmaison, Luca Antiga, and
In
Adam Lerer. Automatic differentiation in pytorch.
NIPS-W, 2017.

[Ren et al., 2015] Shaoqing Ren, Kaiming He, Ross Gir-
shick, and Jian Sun. Faster r-cnn: Towards real-time object
detection with region proposal networks. In NIPS, 2015.

[Russakovsky et al., 2015] Olga Russakovsky,

Jia Deng,
Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael
Bernstein, et al. ImageNet large scale visual recognition
challenge. IJCV, 2015.

[Shen et al., 2018a] Tao Shen, Tianyi Zhou, Guodong Long,
Jing Jiang, Shirui Pan, and Chengqi Zhang. Disan: Di-
rectional self-attention network for rnn/cnn-free language
understanding. In AAAI, 2018.

[Shen et al., 2018b] Tao Shen, Tianyi Zhou, Guodong Long,
Jing Jiang, and Chengqi Zhang. Bi-directional block self-
attention for fast and memory-efﬁcient sequence model-
ing. In ICLR, 2018.

[Szegedy et al., 2015] Christian Szegedy, Wei Liu, Yangqing
Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov,
Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabi-
novich. Going deeper with convolutions. In CVPR, 2015.
[Tai et al., 2016] Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang
Wang, et al. Convolutional neural networks with low-rank
regularization. In ICLR, 2016.

[Wen et al., 2016] Wei Wen, Chunpeng Wu, Yandan Wang,
Yiran Chen, and Hai Li. Learning structured sparsity in
deep neural networks. In NIPS, 2016.

[Yang et al., 2010] Yi Yang, Dong Xu, Feiping Nie,
Shuicheng Yan, and Yueting Zhuang. Image clustering us-
ing local discriminant models and global integration. IEEE
T-IP, 2010.

[Zagoruyko and Komodakis, 2016] Sergey Zagoruyko and
In BMVC,

Nikos Komodakis. Wide residual networks.
2016.

[Zhang et al., 2016] Xiangyu Zhang, Jianhua Zou, Kaiming
He, and Jian Sun. Accelerating very deep convolutional
networks for classiﬁcation and detection. IEEE T-PAMI,
2016.

[Liu et al., 2017] Zhuang Liu, Jianguo Li, Zhiqiang Shen,
Gao Huang, Shoumeng Yan, and Changshui Zhang.
Learning efﬁcient convolutional networks through net-
work slimming. In ICCV, 2017.

[Zhou et al., 2017] Aojun Zhou, Anbang Yao, Yiwen Guo,
Lin Xu, and Yurong Chen. Incremental network quantiza-
tion: Towards lossless cnns with low-precision weights. In
ICLR, 2017.

[Zhu et al., 2017] Chenzhuo Zhu, Song Han, Huizi Mao, and
William J Dally. Trained ternary quantization. In ICLR,
2017.

Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks

Yang He1,2, Guoliang Kang2, Xuanyi Dong2, Yanwei Fu3∗, Yi Yang1,2∗
1SUSTech-UTS Joint Centre of CIS, Southern University of Science and Technology
2CAI, University of Technology Sydney
3The School of Data Science, Fudan University
{yang.he-1, guoliang.kang, xuanyi.dong}@student.uts.edu.au,
yanweifu@fudan.edu.cn, yi.yang@uts.edu.au

8
1
0
2
 
g
u
A
 
1
2
 
 
]

V
C
.
s
c
[
 
 
1
v
6
6
8
6
0
.
8
0
8
1
:
v
i
X
r
a

Abstract

This paper proposed a Soft Filter Pruning (SFP)
method to accelerate the inference procedure of
deep Convolutional Neural Networks (CNNs).
Speciﬁcally, the proposed SFP enables the pruned
ﬁlters to be updated when training the model af-
ter pruning. SFP has two advantages over previ-
ous works: (1) Larger model capacity. Updat-
ing previously pruned ﬁlters provides our approach
with larger optimization space than ﬁxing the ﬁl-
ters to zero. Therefore, the network trained by our
method has a larger model capacity to learn from
the training data. (2) Less dependence on the pre-
trained model. Large capacity enables SFP to train
from scratch and prune the model simultaneously.
In contrast, previous ﬁlter pruning methods should
be conducted on the basis of the pre-trained model
to guarantee their performance. Empirically, SFP
from scratch outperforms the previous ﬁlter prun-
ing methods. Moreover, our approach has been
demonstrated effective for many advanced CNN ar-
chitectures. Notably, on ILSCRC-2012, SFP re-
duces more than 42% FLOPs on ResNet-101 with
even 0.2% top-5 accuracy improvement, which has
advanced the state-of-the-art. Code is publicly
available on GitHub: https://github.com/he-y/soft-
ﬁlter-pruning

1 Introduction

The superior performance of deep CNNs usually comes from
the deeper and wider architectures, which cause the pro-
hibitively expensive computation cost. Even if we use more
efﬁcient architectures, such as residual connection [He et al.,
2016a] or inception module [Szegedy et al., 2015], it is still
difﬁcult in deploying the state-of-the-art CNN models on mo-
bile devices. For example, ResNet-152 has 60.2 million pa-
rameters with 231MB storage spaces; besides, it also needs
more than 380MB memory footprint and six seconds (11.3
billion ﬂoat point operations, FLOPs) to process a single im-
age on CPU. The storage, memory, and computation of this

∗Corrsponding Author

Figure 1: Hard Filter Pruning v.s. Soft Filter Pruning. We mark
the pruned ﬁlter as the green dashed box. For the hard ﬁlter pruning,
the pruned ﬁlters are always ﬁxed during the whole training proce-
dure. Therefore, the model capacity is reduced and thus harms the
performance because the dashed blue box is useless during train-
ing. On the contrary, our SFP allows the pruned ﬁlters to be updated
during the training procedure.
In this way, the model capacity is
recovered from the pruned model, and thus leads a better accuracy.

cumbersome model signiﬁcantly exceed the computing limi-
tation of current mobile devices. Therefore, it is essential to
maintain the small size of the deep CNN models which has
relatively low computational cost but high accuracy in real-
world applications.

Recent efforts have been made either on directly deleting
weight values of ﬁlters [Han et al., 2015b] (i.e., weight prun-
ing) or totally discarding some ﬁlters (i.e., ﬁlter pruning) [Li
et al., 2017; He et al., 2017; Luo et al., 2017]. However, the
weight pruning may result in the unstructured sparsity of ﬁl-
ters, which may still be less efﬁcient in saving the memory
usage and computational cost, since the unstructured model
cannot leverage the existing high-efﬁciency BLAS libraries.
In contrast, the ﬁlter pruning enables the model with struc-
tured sparsity and more efﬁcient memory usage than weight
pruning, and thus takes full advantage of BLAS libraries to
achieve a more realistic acceleration. Therefore, the ﬁlter
pruning is more advocated in accelerating the networks.

Nevertheless, most of the previous works on ﬁlter pruning

still suffer from the problems of (1) the model capacity re-
duction and (2) the dependence on pre-trained model. Specif-
ically, as shown in Fig. 1, most previous works conduct the
“hard ﬁlter pruning”, which directly delete the pruned ﬁlters.
The discarded ﬁlters will reduce the model capacity of origi-
nal models, and thus inevitably harm the performance. More-
over, to maintain a reasonable performance with respect to the
full models, previous works [Li et al., 2017; He et al., 2017;
Luo et al., 2017] always ﬁne-tuned the hard pruned model
after pruning the ﬁlters of a pre-trained model, which how-
ever has low training efﬁciency and often requires much more
training time than the traditional training schema.

To address the above mentioned two problems, we propose
a novel Soft Filter Pruning (SFP) approach. The SFP dynam-
ically prunes the ﬁlters in a soft manner. Particularly, before
ﬁrst training epoch, the ﬁlters of almost all layers with small
(cid:96)2-norm are selected and set to zero. Then the training data
is used to update the pruned model. Before the next training
epoch, our SFP will prune a new set of ﬁlters of small (cid:96)2-
norm. These training process is continued until converged.
Finally, some ﬁlters will be selected and pruned without fur-
ther updating. The SFP algorithm enables the compressed
network to have a larger model capacity, and thus achieve a
higher accuracy than others.

Contributions. We highlight three contributions: (1) We
propose SFP to allow the pruned ﬁlters to be updated dur-
ing the training procedure. This soft manner can dramatically
maintain the model capacity and thus achieves the superior
performance. (2) Our acceleration approach can train a model
from scratch and achieve better performance compared to the
state-of-the-art.
In this way, the ﬁne-tuning procedure and
the overall training time is saved. Moreover, using the pre-
trained model can further enhance the performance of our ap-
proach to advance the state-of-the-art in model acceleration.
(3) The extensive experiment on two benchmark datasets
demonstrates the effectiveness and efﬁciency of our SFP. We
accelerate ResNet-110 by two times with about 4% relative
accuracy improvement on CIFAR-10, and also achieve state-
of-the-art results on ILSVRC-2012.

2 Related Works

Most previous works on accelerating CNNs can be roughly
divided into three categories, namely, matrix decomposition,
low-precision weights, and pruning.
In particular, the ma-
trix decomposition of deep CNN tensors is approximated by
the product of two low-rank matrices [Jaderberg et al., 2014;
Zhang et al., 2016; Tai et al., 2016]. This can save the
computational cost. Some works [Zhu et al., 2017; Zhou
et al., 2017] focus on compressing the CNNs by using low-
precision weights. Pruning-based approaches aim to remove
the unnecessary connections of the neural network [Han et
al., 2015b; Li et al., 2017]. Essentially, the work of this pa-
per is based on the idea of pruning techniques; and the ap-
proaches of matrix decomposition and low-precision weights
are orthogonal but potentially useful here – it may be still
worth simplifying the weight matrix after pruning ﬁlters,
which would be taken as future work.

Weight Pruning. Many recent works [Han et al., 2015b;

Han et al., 2015a; Guo et al., 2016] pruning weights of
neural network resulting in small models. For example,
[Han et al., 2015b] proposed an iterative weight pruning
method by discarding the small weights whose values are
[Guo et al., 2016] proposed the dy-
below the threshold.
namic network surgery to reduce the training iteration while
maintaining a good prediction accuracy. [Wen et al., 2016;
Lebedev and Lempitsky, 2016] leveraged the sparsity prop-
erty of feature maps or weight parameters to accelerate the
CNN models. A special case of weight pruning is neuron
pruning. However, pruning weights always leads to unstruc-
tured models, so the model cannot leverage the existing efﬁ-
cient BLAS libraries in practice. Therefore, it is difﬁcult for
weight pruning to achieve realistic speedup.

Filter Pruning. Concurrently with our work, some ﬁl-
ter pruning strategies [Li et al., 2017; Liu et al., 2017;
He et al., 2017; Luo et al., 2017] have been explored. Prun-
ing the ﬁlters leads to the removal of the corresponding fea-
ture maps. This not only reduces the storage usage on de-
vices but also decreases the memory footprint consumption
to accelerate the inference. [Li et al., 2017] uses (cid:96)1-norm to
select unimportant ﬁlters and explores the sensitivity of lay-
ers for ﬁlter pruning. [Liu et al., 2017] introduces (cid:96)1 regu-
larization on the scaling factors in batch normalization (BN)
layers as a penalty term, and prune channel with small scal-
ing factors in BN layers. [Molchanov et al., 2017] proposes
a Taylor expansion based pruning criterion to approximate
the change in the cost function induced by pruning. [Luo et
al., 2017] adopts the statistics information from next layer to
guide the importance evaluation of ﬁlters. [He et al., 2017]
proposes a LASSO-based channel selection strategy, and a
least square reconstruction algorithm to prune ﬁlers. How-
ever, for all these ﬁlter pruning methods, the representative
capacity of neural network after pruning is seriously affected
by smaller optimization space.

Discussion. To the best of our knowledge, there is only one
approach that uses the soft manner to prune weights [Guo et
al., 2016]. We would like to highlight our advantages com-
pared to this approach as below: (1) Our SPF focuses on
the ﬁlter pruning, but they focus on the weight pruning. As
discussed above, weight pruning approaches lack the practi-
cal implementations to achieve the realistic acceleration. (2)
[Guo et al., 2016] paid more attention to the model com-
pression, whereas our approach can achieve both compres-
sion and acceleration of the model. (3) Extensive experiments
have been conducted to validate the effectiveness of our pro-
posed approach both on large-scale datasets and the state-of-
the-art CNN models. In contrast, [Guo et al., 2016] only had
the experiments on Alexnet which is more redundant the ad-
vanced models, such as ResNet.

3 Methodology
3.1 Preliminaries
We will formally introduce the symbol and annotations in this
section. The deep CNN network can be parameterized by
{W(i) ∈ RNi+1×Ni×K×K, 1 ≤ i ≤ L} W(i) denotes a
matrix of connection weights in the i-th layer. Ni denotes the
number of input channels for the i-th convolution layer. L

denotes the number of layers. The shapes of input tensor U
and output tensor V are Ni × Hi × Wi and Ni+1 × Hi+1 ×
Wi+1, respectively. The convolutional operation of the i-th
layer can be written as:

Vi,j = Fi,j ∗ U for 1 ≤ j ≤ Ni+1,
(1)
where Fi,j ∈ RNi×K×K represents the j-th ﬁlter of the i-th
layer. W(i) consists of {Fi,j, 1 ≤ j ≤ Ni+1}. The Vi,j
represents the j-th output feature map of the i-th layer.

Pruning ﬁlters can remove the output feature maps.
In
this way, the computational cost of the neural network will
Let us assume the pruning rate of
reduce remarkably.
SFP is Pi for the i-th layer. The number of ﬁlters of
this layer will be reduced from Ni+1 to Ni+1(1 − Pi),
thereby the size of the output tensor Vi,j can be reduced to
Ni+1(1 − Pi) × Hi+1 × Wi+1. As the output tensor of i-th
layer is the input tensor of i + 1-th layer, we can reduce the
input size of i-th layer to achieve a higher acceleration ratio.

3.2 Soft Filter Pruning (SFP)
Most of previous ﬁlter pruning works [Li et al., 2017; Liu et
al., 2017; He et al., 2017; Luo et al., 2017] compressed the
deep CNNs in a hard manner. We call them as the hard ﬁlter
pruning. Typically, these algorithms ﬁrstly prune ﬁlters of a
single layer of a pre-trained model and ﬁne-tune the pruned
model to complement the degrade of the performance. Then
they prune the next layer and ﬁne-tune the model again until
the last layer of the model is pruned. However, once ﬁlters are
pruned, these approaches will not update these ﬁlters again.
Therefore, the model capacity is drastically reduced due to
the removed ﬁlters; and such a hard pruning manner affects
the performance of the compressed models negatively.

As summarized in Alg. 1, the proposed SFP algorithm can
dynamically remove the ﬁlters in a soft manner. Speciﬁcally,
the key is to keep updating the pruned ﬁlters in the train-
ing stage. Such an updating manner brings several bene-
ﬁts. It not only keeps the model capacity of the compressed
deep CNN models as the original models, but also avoids the
greedy layer by layer pruning procedure and enable pruning
almost all layers at the same time. More speciﬁcally, our
approach can prune a model either in the process of train-
ing from scratch, or a pre-trained model.
In each training
epoch, the full model is optimized and trained on the training
data. After each epoch, the (cid:96)2-norm of all ﬁlters are com-
puted for each weighted layer and used as the criterion of our
ﬁlter selection strategy. Then we will prune the selected ﬁl-
ters by setting the corresponding ﬁlter weights as zero, which
is followed by next training epoch. Finally, the original deep
CNNs are pruned into a compact and efﬁcient model. The
details of SFP is illustratively explained in Alg. 1, which can
be divided into the following four steps.

Filter selection. We use the (cid:96)p-norm to evaluate the impor-
tance of each ﬁlter as Eq. (2). In general, the convolutional
results of the ﬁlter with the smaller (cid:96)p-norm lead to relatively
lower activation values; and thus have a less numerical im-
pact on the ﬁnal prediction of deep CNN models.
In term
of this understanding, such ﬁlters of small (cid:96)p-norm will be
given high priority of being pruned than those of higher (cid:96)p-
norm. Particularly, we use a pruning rate Pi to select Ni+1Pi

Algorithm 1 Algorithm Description of SFP
Input: training data: X, pruning rate: Pi

the model with parameters W = {W(i), 0 ≤ i ≤ L}.
Initialize the model parameter W
for epoch = 1; epoch ≤ epochmax; epoch + + do
Update the model parameter W based on X
for i = 1; i ≤ L; i + + do

Calculate the (cid:96)2-norm for each ﬁlter (cid:107)Fi,j(cid:107)2, 1 ≤

j ≤ Ni+1

end for

Zeroize Ni+1Pi ﬁlters by (cid:96)2-norm ﬁlter selection

end for
Obtain the compact model with parameters W∗ from W

Output: The compact model and its parameters W∗

unimportant ﬁlters for the i-th weighted layer. In other words,
the lowest Ni+1Pi ﬁlters are selected, e.g., the blue ﬁlters in
Fig. 2. In practice, (cid:96)2-norm is used based on the empirical
analysis.

(cid:118)
(cid:117)
(cid:117)
(cid:107)Fi,j(cid:107)p = p
(cid:116)

Ni(cid:88)

K
(cid:88)

K
(cid:88)

|Fi,j(n, k1, k2)|p,

(2)

n=1

k1=1

k2=1
Filter Pruning. We set the value of selected Ni+1Pi ﬁlters
to zero (see the ﬁlter pruning step in Fig. 2). This can tem-
porarily eliminate their contribution to the network output.
Nevertheless, in the following training stage, we still allow
these selected ﬁlters to be updated, in order to keep the repre-
sentative capacity and the high performance of the model.

In the ﬁlter pruning step, we simply prune all the weighted
layers at the same time.
In this way, we can prune each
ﬁlter in parallel, which would cost negligible computation
time.
In contrast, the previous ﬁlter pruning methods al-
ways conduct layer by layer greedy pruning. After prun-
ing ﬁlters of one single layer, existing methods always re-
quire training to converge the network [Luo et al., 2017;
He et al., 2017]. This procedure cost much extra computa-
tion time, especially when the depth increases. Moreover, we
use the same pruning rate for all weighted layers. Therefore,
we need only one hyper-parameter Pi = P to balance the
acceleration and accuracy. This can avoid the inconvenient
hyper-parameter search or the complicated sensitivity anal-
ysis [Li et al., 2017]. As we allow the pruned ﬁlters to be
updated, the model has a large model capacity and becomes
more ﬂexible and thus can well balance the contribution of
each ﬁlter to the ﬁnal prediction.

Reconstruction. After the pruning step, we train the net-
work for one epoch to reconstruct the pruned ﬁlters. As
shown in Fig. 2, the pruned ﬁlters are updated to non-zero by
back-propagation. In this way, SFP allows the pruned model
to have the same capacity as the original model during train-
ing. In contrast, hard ﬁlter pruning decreases the number of
feature maps. The reduction of feature maps would dramat-
ically reduce the model capacity, and further harm the per-
formance. Previous pruning methods usually require a pre-
trained model and then ﬁne-tune it. However, as we inte-
grate the pruning step into the normal training schema, our
approach can train the model from scratch. Therefore, the

Figure 2: Overview of SFP. At the end of each training epoch, we prune the ﬁlters based on their importance evaluations. The ﬁlters are ranked
by their (cid:96)p-norms (purple rectangles) and the small ones (blue circles) are selected to be pruned. After ﬁlter pruning, the model undergoes a
reconstruction process where pruned ﬁlters are capable of being reconstructed (i.e., updated from zeros) by the forward-backward process.
(a): ﬁlter instantiations before pruning. (b): ﬁlter instantiations after pruning. (c): ﬁlter instantiations after reconstruction.

ﬁne-tuning stage is no longer necessary for SFP. As we will
show in experiments, the network trained from scratch by
SFP can obtain the competitive results with the one trained
from a well-trained model by others. By leveraging the pre-
trained model, SFP obtains a much higher performance and
advances the state-of-the-art.

Obtaining Compact Model. SFP iterates over the ﬁlter
selection, ﬁlter pruning and reconstruction steps. After the
model gets converged, we can obtain a sparse model contain-
ing many “zero ﬁlters”. One “zero ﬁlter” corresponds to one
feature map. The features maps, corresponding to those “zero
ﬁlters”, will always be zero during the inference procedure.
There will be no inﬂuence to remove these ﬁlters as well as
the corresponding feature maps. Speciﬁcally, for the prun-
ing rate Pi in the i-th layer, only Ni+1(1 − Pi) ﬁlters are
non-zero and have an effect on the ﬁnal prediction. Consider
pruning the previous layer, the input channel of i-th layer is
changed from Ni to Ni(1 − Pi−1). We can thus re-build
the i-th layer into a smaller one. Finally, a compact model
{W∗(i) ∈ RNi+1(1−Pi)×Ni(1−Pi−1)×K×K} is obtained.

3.3 Computation Complexity Analysis
Theoretical speedup analysis. Suppose the ﬁlter pruning
rate of the ith layer is Pi, which means the Ni+1 × Pi ﬁl-
ters are set to zero and pruned from the layer, and the other
Ni+1 × (1 − Pi) ﬁlters remain unchanged, and suppose the
size of the input and output feature map of ith layer is Hi×Wi
and Hi+1 × Wi+1. Then after ﬁlter pruning, the dimension
of useful output feature map of the ith layer decreases from
Ni+1 × Hi+1 × Wi+1 to Ni+1(1 − Pi) × Hi+1 × Wi+1.
Note that the output of ith layer is the input of (i + 1) th
layer. And we further prunes the (i + 1)th layer with a ﬁl-
ter pruning rate Pi+1, then the calculation of (i + 1)th layer
is decrease from Ni+2 × Ni+1 × k2 × Hi+2 × Wi+2 to
Ni+2(1 − Pi+1) × Ni+1(1 − Pi) × k2 × Hi+2 × Wi+2. In
other words, a proportion of 1 − (1 − Pi+1) × (1 − Pi) of the
original calculation is reduced, which will make the neural
network inference much faster.

Realistic speedup analysis. In theoretical speedup anal-
ysis, other operations such as batch normalization (BN) and
pooling are negligible comparing to convolution operations.
Therefore, we consider the FLOPs of convolution operations

for computation complexity comparison, which is commonly
used in previous work [Li et al., 2017; Luo et al., 2017].
However, reduced FLOPs cannot bring the same level of re-
alistic speedup because non-tensor layers (e.g., BN and pool-
ing layers) also need the inference time on GPU [Luo et al.,
2017]. In addition, the limitation of IO delay, buffer switch
and efﬁciency of BLAS libraries also lead to the wide gap be-
tween theoretical and realistic speedup ratio. We compare the
theoretical and realistic speedup in Section 4.3.

4 Evaluation and Results

4.1 Benchmark Datasets and Experimental Setting

Our method is evaluated on two benchmarks: CIFAR-
10 [Krizhevsky and Hinton, 2009] and ILSVRC-2012 [Rus-
sakovsky et al., 2015]. The CIFAR-10 dataset contains
50,000 training images and 10,000 testing images, which are
categorized into 10 classes.
ILSVRC-2012 is a large-scale
dataset containing 1.28 million training images and 50k val-
idation images of 1,000 classes. Following the common set-
ting in [Luo et al., 2017; He et al., 2017; Dong et al., 2017a],
we focus on pruning the challenging ResNet model in this
paper. SFP should also be effective on different computer
vision tasks, such as [Kang et al., 2017; Ren et al., 2015;
Dong et al., 2018; Shen et al., 2018b; Yang et al., 2010;
Shen et al., 2018a; Dong et al., 2017b], and we will explore
this in future.

In the CIFAR-10 experiments, we use the default parame-
ter setting as [He et al., 2016b] and follow the training sched-
ule in [Zagoruyko and Komodakis, 2016]. On ILSVRC-2012,
we follow the same parameter settings as [He et al., 2016a;
He et al., 2016b]. We use the same data argumentation strate-
gies with PyTorch ofﬁcial examples [Paszke et al., 2017].

We conduct our SFP operation at the end of every training
epoch. For pruning a scratch model, we use the normal train-
ing schedule. For pruning a pre-trained model, we reduce the
learning rate by 10 compared to the schedule for the scratch
model. We run each experiment three times and report the
“mean ± std”. We compare the performance with other state-
of-the-art acceleration algorithms, e.g., [Dong et al., 2017a;
Li et al., 2017; He et al., 2017; Luo et al., 2017].

Depth

Fine-tune? Baseline Accu. (%) Accelerated Accu. (%) Accu. Drop (%) FLOPs Pruned FLOPs(%)

20

32

56

110

Method
[Dong et al., 2017a]
Ours(10%)
Ours(20%)
Ours(30%)
[Dong et al., 2017a]
Ours(10%)
Ours(20%)
Ours(30%)
[Li et al., 2017]
[Li et al., 2017]
[He et al., 2017]
[He et al., 2017]
Ours(10%)
Ours(20%)
Ours(30%)
Ours(30%)
Ours(40%)
Ours(40%)
[Li et al., 2017]
[Li et al., 2017]
[Dong et al., 2017a]
Ours(10%)
Ours(20%)
Ours(30%)
Ours(30%)

N
N
N
N
N
N
N
N
N
Y
N
Y
N
N
N
Y
N
Y
N
Y
N
N
N
N
Y

91.53
92.20 ± 0.18
92.20 ± 0.18
92.20 ± 0.18
92.33
92.63 ± 0.70
92.63 ± 0.70
92.63 ± 0.70
93.04
93.04
92.80
92.80
93.59 ± 0.58
93.59 ± 0.58
93.59 ± 0.58
93.59 ± 0.58
93.59 ± 0.58
93.59 ± 0.58
93.53
93.53
93.63
93.68 ± 0.32
93.68 ± 0.32
93.68 ± 0.32
93.68 ± 0.32

91.43
92.24 ± 0.33
91.20 ± 0.30
90.83 ± 0.31
90.74
93.22 ± 0.09
90.63 ± 0.37
90.08 ± 0.08
91.31
93.06
90.90
91.80
93.89 ± 0.19
93.47 ± 0.24
93.10 ± 0.20
93.78 ± 0.22
92.26 ± 0.31
93.35 ± 0.31
92.94
93.30
93.44
93.83 ± 0.19
93.93 ± 0.41
93.38 ± 0.30
93.86 ± 0.21

0.10
-0.04
1.00
1.37
1.59
-0.59
0.00
0.55
1.75
-0.02
1.90
1.00
-0.30
0.12
0.49
-0.19
1.33
0.24
0.61
0.20
0.19
-0.15
-0.25
0.30
-0.18

3.20E7
3.44E7
2.87E7
2.43E7
4.70E7
5.86E7
4.90E7
4.03E7
9.09E7
9.09E7
-
-
1.070E8
8.98E7
7.40E7
7.40E7
5.94E7
5.94E7
1.55E8
1.55E8
-
2.16E8
1.82E8
1.50E8
1.50E8

20.3
15.2
29.3
42.2
31.2
14.9
28.8
41.5
27.6
27.6
50.0
50.0
14.7
28.4
41.1
41.1
52.6
52.6
38.6
38.6
34.2
14.6
28.2
40.8
40.8

Table 1: Comparison of pruning ResNet on CIFAR-10. In “Fine-tune?” column, “Y” and “N” indicate whether to use the pre-trained model
as initialization or not, respectively. The “Accu. Drop” is the accuracy of the pruned model minus that of the baseline model, so negative
number means the accelerated model has a higher accuracy than the baseline model. A smaller number of ”Accu. Drop” is better.

4.2 ResNet on CIFAR-10
Settings. For CIFAR-10 dataset, we test our SFP on ResNet-
20, 32, 56 and 110. We use several different pruning rates,
and also analyze the difference between using the pre-trained
model and from scratch.

Results. Tab. 1 shows the results. Our SFP could achieve
a better performance than the other state-of-the-art hard ﬁlter
pruning methods. For example, [Li et al., 2017] use the hard
pruning method to accelerate ResNet-110 by 38.6% speedup
ratio with 0.61% accuracy drop when without ﬁne-tuning.
When using pre-trained model and ﬁne-tuning, the accuracy
drop becomes 0.20%. However, we can accelerate the infer-
ence of ResNet-110 to 40.8% speed-up with only 0.30% ac-
curacy drop without ﬁne-tuning. When using the pre-trained
model, we can even outperform the original model by 0.18%
with about more than 40% FLOPs reduced.

These results validate the effectiveness of SFP, which can
produce a more compressed model with comparable perfor-
mance to the original model.

4.3 ResNet on ILSVRC-2012
Settings. For ILSVRC-2012 dataset, we test our SFP on
ResNet-18, 34, 50 and 101; and we use the same pruning rate
30% for all the models. All the convolutional layer of ResNet
are pruned with the same pruning rate at the same time. (We
do not prune the projection shortcuts for simpliﬁcation, which
only need negligible time and do not affect the overall cost.)
Results. Tab. 2 shows that SFP outperforms other state-
of-the-art methods.
For ResNet-34, SFP without ﬁne-
tuning achieves more inference speedup to the hard pruning

method [Luo et al., 2017], but the accuracy of our pruned
model exceeds their model by 2.57%. Moreover, for prun-
ing a pre-trained ResNet-101, SFP reduces more than 40%
FLOPs of the model with even 0.2% top-5 accuracy in-
crease, which is the state-of-the-art result.
In contrast, the
performance degradation is inevitable for hard ﬁlter pruning
method. Maintained model capacity of SFP is the main rea-
son for the superior performance. In addition, the non-greedy
all-layer pruning method may have a better performance than
the locally optimal solution obtained from previous greedy
pruning method, which seems to be another reason. Occa-
sionally, large performance degradation happens for the pre-
trained model (e.g., 14.01% top-1 accuracy drop for ResNet-
50). This will be explored in our future work.

To test the realistic speedup ratio, we measure the forward
time of the pruned models on one GTX1080 GPU with a
batch size of 64 (shown in Tab. 3). The gap between theo-
retical and realistic model may come from and the limitation
of IO delay, buffer switch and efﬁciency of BLAS libraries.

4.4 Ablation Study
We conducted extensive ablation studies to further analyze
each component of SFP.

Filter Selection Criteria. The magnitude based criteria
such as (cid:96)p-norm are widely used to ﬁlter selection because
computational resources cost is small [Li et al., 2017]. We
compare the (cid:96)2-norm and (cid:96)1-norm. For (cid:96)1-norm criteria,
the accuracy of the model under pruning rate 10%, 20%,
30% are 93.68±0.60%, 93.68±0.76% and 93.34±0.12%,
respectively. While for (cid:96)2-norm criteria,
the accuracy

Depth

Method

Fine-
tune?

18

34

50

101

Ours(30%)

[Dong et al., 2017a] N
N
[Dong et al., 2017a] N
Y
N
Y
Y
N
Y
N
Y

[Li et al., 2017]
Ours(30%)
[He et al., 2017]
[Luo et al., 2017]
Ours(30%)
Ours(30%)
Ours(30%)
Ours(30%)

Top-1 Accu.
Baseline(%)
69.98
70.28
73.42
73.23
73.92
-
72.88
76.15
76.15
77.37
77.37

Top-1 Accu.
Accelerated(%)
66.33
67.10
72.99
72.17
71.83
-
72.04
74.61
62.14
77.03
77.51

Top-5 Accu.
Baseline(%)
89.24
89.63
91.36
-
91.62
92.20
91.14
92.87
92.87
93.56
93.56

Top-5 Accu.
Accelerated(%)
86.94
87.78
91.19
-
90.33
90.80
90.67
92.06
84.60
93.46
93.71

Top-1 Accu.
Drop(%)
3.65
3.18
0.43
1.06
2.09
-
0.84
1.54
14.01
0.34
-0.14

Top-5 Accu.
Drop(%)
2.30
1.85
0.17
-
1.29
1.40
0.47
0.81
8.27
0.10
-0.20

Pruned
FLOPs(%)
34.6
41.8
24.8
24.2
41.1
50.0
36.7
41.8
41.8
42.2
42.2

Table 2: Comparison of pruning ResNet on ImageNet. “Fine-tune?” and ”Accu. Drop” have the same meaning with Tab. 1.

Model

ResNet-18
ResNet-34
ResNet-50
ResNet-101

Baseline
time (ms)
37.10
63.97
135.01
219.71

Pruned
time (ms)
26.97
45.14
94.66
148.64

Realistic
Speed-up(%)
27.4
29.4
29.8
32.3

Theoretical
Speed-up(%)
41.8
41.1
41.8
42.2

Table 3: Comparison on the theoretical and realistic speedup. We
only count the time consumption of the forward procedure.

(a) Different Pruning Rates

(b) Different SFP Intervals

Figure 3: Accuracy of ResNet-110 on CIFAR-10 regarding differ-
ent hyper-parameters. (Solid line and shadow denotes the mean and
standard deviation of three experiment, respectively.)

are 93.89±0.19%, 93.93±0.41% and 93.38±0.30%, respec-
tively. The performance of (cid:96)2-norm criteria is slightly better
than that of (cid:96)1-norm criteria. The result of (cid:96)2-norm is dom-
inated by the largest element, while the result of (cid:96)1-norm is
also largely affected by other small elements. Therefore, ﬁl-
ters with some large weights would be preserved by the (cid:96)2-
norm criteria. So the corresponding discriminative features
are kept so the performance of the pruned model is better.

Varying pruning rates. To comprehensively understand
SFP, we test the accuracy of different pruning rates for
ResNet-110, shown in Fig. 3(a). As the pruning rate in-
creases, the accuracy of the pruned model ﬁrst rises above the
baseline model and then drops approximately linearly. For
the pruning rate between 0% and about 23%, the accuracy of
the accelerated model is higher than the baseline model. This
shows that our SFP has a regularization effect on the neural
network because SFP reduces the over-ﬁtting of the model.

Sensitivity of SFP interval. By default, we conduct our
SFP operation at the end of every training epoch. However,

different SFP intervals may lead to different performance; so
we explore the sensitivity of SFP interval. We use the ResNet-
110 under pruning rate 30% as a baseline, and change the SFP
interval from one epoch to ten epochs, as shown in Fig. 3(b).
It is shown that the model accuracy has no large ﬂuctuation
along with the different SFP intervals. Moreover, the model
accuracy of most (80%) intervals surpasses the accuracy of
one epoch interval. Therefore, we can even achieve a better
performance if we ﬁne-tune this parameter.

Selection of pruned layers. Previous works always prune
a portion of the layers of the network. Besides, different lay-
ers always have different pruning rates. For example, [Li et
al., 2017] only prunes insensitive layers, [Luo et al., 2017]
skips the last layer of every block of the ResNet, and [Luo
et al., 2017] prunes more aggressive for shallower layers and
prune less for deep layers. Similarly, we compare the perfor-
mance of pruning ﬁrst and second layer of all basic blocks
of ResNet-110. We set the pruning rate as 30%. The model
with all the ﬁrst layers of blocks pruned has an accuracy of
93.96 ± 0.13%, while that with the second layers of blocks
pruned has an accuracy of 93.38 ± 0.44%. Therefore, differ-
ent layers have different sensitivity for SFP, and careful selec-
tion of pruned layers would potentially lead to performance
improvement, although more hyper-parameters are needed.

5 Conclusion and Future Work

In this paper, we propose a soft ﬁlter pruning (SFP) approach
to accelerate the deep CNNs. During the training procedure,
SFP allows the pruned ﬁlters to be updated. This soft manner
can maintain the model capacity and thus achieve the supe-
rior performance. Remarkably, SFP can achieve the competi-
tive performance compared to the state-of-the-art without the
pre-trained model. Moreover, by leveraging the pre-trained
model, SFP achieves a better result and advances the state-
of-the-art. Furthermore, SFP can be combined with other
acceleration algorithms, e.g., matrix decomposition and low-
precision weights, to further improve the performance.

Acknowledgments

Yi Yang is the recipient of a Google Faculty Research Award.
We acknowledge the Data to Decisions CRC (D2D CRC), the
Cooperative Research Centres Programme and ARC’s DE-

CRA (project DE170101415) for funding this research. We
thank Amazon for the AWS Cloud Credits.

References
[Dong et al., 2017a] Xuanyi Dong, Junshi Huang, Yi Yang,
and Shuicheng Yan. More is less: A more complicated
network with less inference complexity. In CVPR, 2017.
[Dong et al., 2017b] Xuanyi Dong, Deyu Meng, Fan Ma,
and Yi Yang. A dual-network progressive approach to
weakly supervised object detection. In ACM Multimedia,
2017.

[Dong et al., 2018] Xuanyi Dong, Shoou-I Yu, Xinshuo
Weng, Shih-En Wei, Yi Yang, and Yaser Sheikh.
Supervision-by-Registration: An unsupervised approach
to improve the precision of facial landmark detectors. In
CVPR, 2018.

[Guo et al., 2016] Yiwen Guo, Anbang Yao, and Yurong
Chen. Dynamic network surgery for efﬁcient DNNs. In
NIPS, 2016.

[Han et al., 2015a] Song Han, Huizi Mao, and William J
Dally. Deep compression: Compressing deep neural net-
works with pruning, trained quantization and huffman cod-
ing. In ICLR, 2015.

[Han et al., 2015b] Song Han, Jeff Pool, John Tran, and
William Dally. Learning both weights and connections for
efﬁcient neural network. In NIPS, 2015.

[He et al., 2016a] Kaiming He, Xiangyu Zhang, Shaoqing
Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, 2016.

[He et al., 2016b] Kaiming He, Xiangyu Zhang, Shaoqing
Identity mappings in deep residual

Ren, and Jian Sun.
networks. In ECCV, 2016.

[He et al., 2017] Yihui He, Xiangyu Zhang, and Jian Sun.
Channel pruning for accelerating very deep neural net-
works. In ICCV, 2017.

[Jaderberg et al., 2014] Max Jaderberg, Andrea Vedaldi, and
Andrew Zisserman. Speeding up convolutional neural net-
works with low rank expansions. In BMVC, 2014.

[Kang et al., 2017] Guoliang Kang, Jun Li, and Dacheng
Tao. Shakeout: A new approach to regularized deep neural
network training. IEEE T-PAMI, 2017.

[Krizhevsky and Hinton, 2009] Alex Krizhevsky and Geof-
frey Hinton. Learning multiple layers of features from tiny
images. 2009.

[Lebedev and Lempitsky, 2016] Vadim Lebedev and Victor
Lempitsky. Fast ConvNets using group-wise brain dam-
age. In CVPR, 2016.

[Li et al., 2017] Hao Li, Asim Kadav,

Igor Durdanovic,
Hanan Samet, and Hans Peter Graf. Pruning ﬁlters for
efﬁcient ConvNets. In ICLR, 2017.

[Luo et al., 2017] Jian-Hao Luo, Jianxin Wu, and Weiyao
Lin. ThiNet: A ﬁlter level pruning method for deep neural
network compression. In ICCV, 2017.

[Molchanov et al., 2017] Pavlo Molchanov, Stephen Tyree,
Tero Karras, Timo Aila, and Jan Kautz. Pruning convolu-
tional neural networks for resource efﬁcient transfer learn-
ing. In ICLR, 2017.

[Paszke et al., 2017] Adam Paszke, Sam Gross, Soumith
Chintala, Gregory Chanan, Edward Yang, Zachary De-
Vito, Zeming Lin, Alban Desmaison, Luca Antiga, and
In
Adam Lerer. Automatic differentiation in pytorch.
NIPS-W, 2017.

[Ren et al., 2015] Shaoqing Ren, Kaiming He, Ross Gir-
shick, and Jian Sun. Faster r-cnn: Towards real-time object
detection with region proposal networks. In NIPS, 2015.

[Russakovsky et al., 2015] Olga Russakovsky,

Jia Deng,
Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael
Bernstein, et al. ImageNet large scale visual recognition
challenge. IJCV, 2015.

[Shen et al., 2018a] Tao Shen, Tianyi Zhou, Guodong Long,
Jing Jiang, Shirui Pan, and Chengqi Zhang. Disan: Di-
rectional self-attention network for rnn/cnn-free language
understanding. In AAAI, 2018.

[Shen et al., 2018b] Tao Shen, Tianyi Zhou, Guodong Long,
Jing Jiang, and Chengqi Zhang. Bi-directional block self-
attention for fast and memory-efﬁcient sequence model-
ing. In ICLR, 2018.

[Szegedy et al., 2015] Christian Szegedy, Wei Liu, Yangqing
Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov,
Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabi-
novich. Going deeper with convolutions. In CVPR, 2015.
[Tai et al., 2016] Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang
Wang, et al. Convolutional neural networks with low-rank
regularization. In ICLR, 2016.

[Wen et al., 2016] Wei Wen, Chunpeng Wu, Yandan Wang,
Yiran Chen, and Hai Li. Learning structured sparsity in
deep neural networks. In NIPS, 2016.

[Yang et al., 2010] Yi Yang, Dong Xu, Feiping Nie,
Shuicheng Yan, and Yueting Zhuang. Image clustering us-
ing local discriminant models and global integration. IEEE
T-IP, 2010.

[Zagoruyko and Komodakis, 2016] Sergey Zagoruyko and
In BMVC,

Nikos Komodakis. Wide residual networks.
2016.

[Zhang et al., 2016] Xiangyu Zhang, Jianhua Zou, Kaiming
He, and Jian Sun. Accelerating very deep convolutional
networks for classiﬁcation and detection. IEEE T-PAMI,
2016.

[Liu et al., 2017] Zhuang Liu, Jianguo Li, Zhiqiang Shen,
Gao Huang, Shoumeng Yan, and Changshui Zhang.
Learning efﬁcient convolutional networks through net-
work slimming. In ICCV, 2017.

[Zhou et al., 2017] Aojun Zhou, Anbang Yao, Yiwen Guo,
Lin Xu, and Yurong Chen. Incremental network quantiza-
tion: Towards lossless cnns with low-precision weights. In
ICLR, 2017.

[Zhu et al., 2017] Chenzhuo Zhu, Song Han, Huizi Mao, and
William J Dally. Trained ternary quantization. In ICLR,
2017.

Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks

Yang He1,2, Guoliang Kang2, Xuanyi Dong2, Yanwei Fu3∗, Yi Yang1,2∗
1SUSTech-UTS Joint Centre of CIS, Southern University of Science and Technology
2CAI, University of Technology Sydney
3The School of Data Science, Fudan University
{yang.he-1, guoliang.kang, xuanyi.dong}@student.uts.edu.au,
yanweifu@fudan.edu.cn, yi.yang@uts.edu.au

8
1
0
2
 
g
u
A
 
1
2
 
 
]

V
C
.
s
c
[
 
 
1
v
6
6
8
6
0
.
8
0
8
1
:
v
i
X
r
a

Abstract

This paper proposed a Soft Filter Pruning (SFP)
method to accelerate the inference procedure of
deep Convolutional Neural Networks (CNNs).
Speciﬁcally, the proposed SFP enables the pruned
ﬁlters to be updated when training the model af-
ter pruning. SFP has two advantages over previ-
ous works: (1) Larger model capacity. Updat-
ing previously pruned ﬁlters provides our approach
with larger optimization space than ﬁxing the ﬁl-
ters to zero. Therefore, the network trained by our
method has a larger model capacity to learn from
the training data. (2) Less dependence on the pre-
trained model. Large capacity enables SFP to train
from scratch and prune the model simultaneously.
In contrast, previous ﬁlter pruning methods should
be conducted on the basis of the pre-trained model
to guarantee their performance. Empirically, SFP
from scratch outperforms the previous ﬁlter prun-
ing methods. Moreover, our approach has been
demonstrated effective for many advanced CNN ar-
chitectures. Notably, on ILSCRC-2012, SFP re-
duces more than 42% FLOPs on ResNet-101 with
even 0.2% top-5 accuracy improvement, which has
advanced the state-of-the-art. Code is publicly
available on GitHub: https://github.com/he-y/soft-
ﬁlter-pruning

1 Introduction

The superior performance of deep CNNs usually comes from
the deeper and wider architectures, which cause the pro-
hibitively expensive computation cost. Even if we use more
efﬁcient architectures, such as residual connection [He et al.,
2016a] or inception module [Szegedy et al., 2015], it is still
difﬁcult in deploying the state-of-the-art CNN models on mo-
bile devices. For example, ResNet-152 has 60.2 million pa-
rameters with 231MB storage spaces; besides, it also needs
more than 380MB memory footprint and six seconds (11.3
billion ﬂoat point operations, FLOPs) to process a single im-
age on CPU. The storage, memory, and computation of this

∗Corrsponding Author

Figure 1: Hard Filter Pruning v.s. Soft Filter Pruning. We mark
the pruned ﬁlter as the green dashed box. For the hard ﬁlter pruning,
the pruned ﬁlters are always ﬁxed during the whole training proce-
dure. Therefore, the model capacity is reduced and thus harms the
performance because the dashed blue box is useless during train-
ing. On the contrary, our SFP allows the pruned ﬁlters to be updated
during the training procedure.
In this way, the model capacity is
recovered from the pruned model, and thus leads a better accuracy.

cumbersome model signiﬁcantly exceed the computing limi-
tation of current mobile devices. Therefore, it is essential to
maintain the small size of the deep CNN models which has
relatively low computational cost but high accuracy in real-
world applications.

Recent efforts have been made either on directly deleting
weight values of ﬁlters [Han et al., 2015b] (i.e., weight prun-
ing) or totally discarding some ﬁlters (i.e., ﬁlter pruning) [Li
et al., 2017; He et al., 2017; Luo et al., 2017]. However, the
weight pruning may result in the unstructured sparsity of ﬁl-
ters, which may still be less efﬁcient in saving the memory
usage and computational cost, since the unstructured model
cannot leverage the existing high-efﬁciency BLAS libraries.
In contrast, the ﬁlter pruning enables the model with struc-
tured sparsity and more efﬁcient memory usage than weight
pruning, and thus takes full advantage of BLAS libraries to
achieve a more realistic acceleration. Therefore, the ﬁlter
pruning is more advocated in accelerating the networks.

Nevertheless, most of the previous works on ﬁlter pruning

still suffer from the problems of (1) the model capacity re-
duction and (2) the dependence on pre-trained model. Specif-
ically, as shown in Fig. 1, most previous works conduct the
“hard ﬁlter pruning”, which directly delete the pruned ﬁlters.
The discarded ﬁlters will reduce the model capacity of origi-
nal models, and thus inevitably harm the performance. More-
over, to maintain a reasonable performance with respect to the
full models, previous works [Li et al., 2017; He et al., 2017;
Luo et al., 2017] always ﬁne-tuned the hard pruned model
after pruning the ﬁlters of a pre-trained model, which how-
ever has low training efﬁciency and often requires much more
training time than the traditional training schema.

To address the above mentioned two problems, we propose
a novel Soft Filter Pruning (SFP) approach. The SFP dynam-
ically prunes the ﬁlters in a soft manner. Particularly, before
ﬁrst training epoch, the ﬁlters of almost all layers with small
(cid:96)2-norm are selected and set to zero. Then the training data
is used to update the pruned model. Before the next training
epoch, our SFP will prune a new set of ﬁlters of small (cid:96)2-
norm. These training process is continued until converged.
Finally, some ﬁlters will be selected and pruned without fur-
ther updating. The SFP algorithm enables the compressed
network to have a larger model capacity, and thus achieve a
higher accuracy than others.

Contributions. We highlight three contributions: (1) We
propose SFP to allow the pruned ﬁlters to be updated dur-
ing the training procedure. This soft manner can dramatically
maintain the model capacity and thus achieves the superior
performance. (2) Our acceleration approach can train a model
from scratch and achieve better performance compared to the
state-of-the-art.
In this way, the ﬁne-tuning procedure and
the overall training time is saved. Moreover, using the pre-
trained model can further enhance the performance of our ap-
proach to advance the state-of-the-art in model acceleration.
(3) The extensive experiment on two benchmark datasets
demonstrates the effectiveness and efﬁciency of our SFP. We
accelerate ResNet-110 by two times with about 4% relative
accuracy improvement on CIFAR-10, and also achieve state-
of-the-art results on ILSVRC-2012.

2 Related Works

Most previous works on accelerating CNNs can be roughly
divided into three categories, namely, matrix decomposition,
low-precision weights, and pruning.
In particular, the ma-
trix decomposition of deep CNN tensors is approximated by
the product of two low-rank matrices [Jaderberg et al., 2014;
Zhang et al., 2016; Tai et al., 2016]. This can save the
computational cost. Some works [Zhu et al., 2017; Zhou
et al., 2017] focus on compressing the CNNs by using low-
precision weights. Pruning-based approaches aim to remove
the unnecessary connections of the neural network [Han et
al., 2015b; Li et al., 2017]. Essentially, the work of this pa-
per is based on the idea of pruning techniques; and the ap-
proaches of matrix decomposition and low-precision weights
are orthogonal but potentially useful here – it may be still
worth simplifying the weight matrix after pruning ﬁlters,
which would be taken as future work.

Weight Pruning. Many recent works [Han et al., 2015b;

Han et al., 2015a; Guo et al., 2016] pruning weights of
neural network resulting in small models. For example,
[Han et al., 2015b] proposed an iterative weight pruning
method by discarding the small weights whose values are
[Guo et al., 2016] proposed the dy-
below the threshold.
namic network surgery to reduce the training iteration while
maintaining a good prediction accuracy. [Wen et al., 2016;
Lebedev and Lempitsky, 2016] leveraged the sparsity prop-
erty of feature maps or weight parameters to accelerate the
CNN models. A special case of weight pruning is neuron
pruning. However, pruning weights always leads to unstruc-
tured models, so the model cannot leverage the existing efﬁ-
cient BLAS libraries in practice. Therefore, it is difﬁcult for
weight pruning to achieve realistic speedup.

Filter Pruning. Concurrently with our work, some ﬁl-
ter pruning strategies [Li et al., 2017; Liu et al., 2017;
He et al., 2017; Luo et al., 2017] have been explored. Prun-
ing the ﬁlters leads to the removal of the corresponding fea-
ture maps. This not only reduces the storage usage on de-
vices but also decreases the memory footprint consumption
to accelerate the inference. [Li et al., 2017] uses (cid:96)1-norm to
select unimportant ﬁlters and explores the sensitivity of lay-
ers for ﬁlter pruning. [Liu et al., 2017] introduces (cid:96)1 regu-
larization on the scaling factors in batch normalization (BN)
layers as a penalty term, and prune channel with small scal-
ing factors in BN layers. [Molchanov et al., 2017] proposes
a Taylor expansion based pruning criterion to approximate
the change in the cost function induced by pruning. [Luo et
al., 2017] adopts the statistics information from next layer to
guide the importance evaluation of ﬁlters. [He et al., 2017]
proposes a LASSO-based channel selection strategy, and a
least square reconstruction algorithm to prune ﬁlers. How-
ever, for all these ﬁlter pruning methods, the representative
capacity of neural network after pruning is seriously affected
by smaller optimization space.

Discussion. To the best of our knowledge, there is only one
approach that uses the soft manner to prune weights [Guo et
al., 2016]. We would like to highlight our advantages com-
pared to this approach as below: (1) Our SPF focuses on
the ﬁlter pruning, but they focus on the weight pruning. As
discussed above, weight pruning approaches lack the practi-
cal implementations to achieve the realistic acceleration. (2)
[Guo et al., 2016] paid more attention to the model com-
pression, whereas our approach can achieve both compres-
sion and acceleration of the model. (3) Extensive experiments
have been conducted to validate the effectiveness of our pro-
posed approach both on large-scale datasets and the state-of-
the-art CNN models. In contrast, [Guo et al., 2016] only had
the experiments on Alexnet which is more redundant the ad-
vanced models, such as ResNet.

3 Methodology
3.1 Preliminaries
We will formally introduce the symbol and annotations in this
section. The deep CNN network can be parameterized by
{W(i) ∈ RNi+1×Ni×K×K, 1 ≤ i ≤ L} W(i) denotes a
matrix of connection weights in the i-th layer. Ni denotes the
number of input channels for the i-th convolution layer. L

denotes the number of layers. The shapes of input tensor U
and output tensor V are Ni × Hi × Wi and Ni+1 × Hi+1 ×
Wi+1, respectively. The convolutional operation of the i-th
layer can be written as:

Vi,j = Fi,j ∗ U for 1 ≤ j ≤ Ni+1,
(1)
where Fi,j ∈ RNi×K×K represents the j-th ﬁlter of the i-th
layer. W(i) consists of {Fi,j, 1 ≤ j ≤ Ni+1}. The Vi,j
represents the j-th output feature map of the i-th layer.

Pruning ﬁlters can remove the output feature maps.
In
this way, the computational cost of the neural network will
Let us assume the pruning rate of
reduce remarkably.
SFP is Pi for the i-th layer. The number of ﬁlters of
this layer will be reduced from Ni+1 to Ni+1(1 − Pi),
thereby the size of the output tensor Vi,j can be reduced to
Ni+1(1 − Pi) × Hi+1 × Wi+1. As the output tensor of i-th
layer is the input tensor of i + 1-th layer, we can reduce the
input size of i-th layer to achieve a higher acceleration ratio.

3.2 Soft Filter Pruning (SFP)
Most of previous ﬁlter pruning works [Li et al., 2017; Liu et
al., 2017; He et al., 2017; Luo et al., 2017] compressed the
deep CNNs in a hard manner. We call them as the hard ﬁlter
pruning. Typically, these algorithms ﬁrstly prune ﬁlters of a
single layer of a pre-trained model and ﬁne-tune the pruned
model to complement the degrade of the performance. Then
they prune the next layer and ﬁne-tune the model again until
the last layer of the model is pruned. However, once ﬁlters are
pruned, these approaches will not update these ﬁlters again.
Therefore, the model capacity is drastically reduced due to
the removed ﬁlters; and such a hard pruning manner affects
the performance of the compressed models negatively.

As summarized in Alg. 1, the proposed SFP algorithm can
dynamically remove the ﬁlters in a soft manner. Speciﬁcally,
the key is to keep updating the pruned ﬁlters in the train-
ing stage. Such an updating manner brings several bene-
ﬁts. It not only keeps the model capacity of the compressed
deep CNN models as the original models, but also avoids the
greedy layer by layer pruning procedure and enable pruning
almost all layers at the same time. More speciﬁcally, our
approach can prune a model either in the process of train-
ing from scratch, or a pre-trained model.
In each training
epoch, the full model is optimized and trained on the training
data. After each epoch, the (cid:96)2-norm of all ﬁlters are com-
puted for each weighted layer and used as the criterion of our
ﬁlter selection strategy. Then we will prune the selected ﬁl-
ters by setting the corresponding ﬁlter weights as zero, which
is followed by next training epoch. Finally, the original deep
CNNs are pruned into a compact and efﬁcient model. The
details of SFP is illustratively explained in Alg. 1, which can
be divided into the following four steps.

Filter selection. We use the (cid:96)p-norm to evaluate the impor-
tance of each ﬁlter as Eq. (2). In general, the convolutional
results of the ﬁlter with the smaller (cid:96)p-norm lead to relatively
lower activation values; and thus have a less numerical im-
pact on the ﬁnal prediction of deep CNN models.
In term
of this understanding, such ﬁlters of small (cid:96)p-norm will be
given high priority of being pruned than those of higher (cid:96)p-
norm. Particularly, we use a pruning rate Pi to select Ni+1Pi

Algorithm 1 Algorithm Description of SFP
Input: training data: X, pruning rate: Pi

the model with parameters W = {W(i), 0 ≤ i ≤ L}.
Initialize the model parameter W
for epoch = 1; epoch ≤ epochmax; epoch + + do
Update the model parameter W based on X
for i = 1; i ≤ L; i + + do

Calculate the (cid:96)2-norm for each ﬁlter (cid:107)Fi,j(cid:107)2, 1 ≤

j ≤ Ni+1

end for

Zeroize Ni+1Pi ﬁlters by (cid:96)2-norm ﬁlter selection

end for
Obtain the compact model with parameters W∗ from W

Output: The compact model and its parameters W∗

unimportant ﬁlters for the i-th weighted layer. In other words,
the lowest Ni+1Pi ﬁlters are selected, e.g., the blue ﬁlters in
Fig. 2. In practice, (cid:96)2-norm is used based on the empirical
analysis.

(cid:118)
(cid:117)
(cid:117)
(cid:107)Fi,j(cid:107)p = p
(cid:116)

Ni(cid:88)

K
(cid:88)

K
(cid:88)

|Fi,j(n, k1, k2)|p,

(2)

n=1

k1=1

k2=1
Filter Pruning. We set the value of selected Ni+1Pi ﬁlters
to zero (see the ﬁlter pruning step in Fig. 2). This can tem-
porarily eliminate their contribution to the network output.
Nevertheless, in the following training stage, we still allow
these selected ﬁlters to be updated, in order to keep the repre-
sentative capacity and the high performance of the model.

In the ﬁlter pruning step, we simply prune all the weighted
layers at the same time.
In this way, we can prune each
ﬁlter in parallel, which would cost negligible computation
time.
In contrast, the previous ﬁlter pruning methods al-
ways conduct layer by layer greedy pruning. After prun-
ing ﬁlters of one single layer, existing methods always re-
quire training to converge the network [Luo et al., 2017;
He et al., 2017]. This procedure cost much extra computa-
tion time, especially when the depth increases. Moreover, we
use the same pruning rate for all weighted layers. Therefore,
we need only one hyper-parameter Pi = P to balance the
acceleration and accuracy. This can avoid the inconvenient
hyper-parameter search or the complicated sensitivity anal-
ysis [Li et al., 2017]. As we allow the pruned ﬁlters to be
updated, the model has a large model capacity and becomes
more ﬂexible and thus can well balance the contribution of
each ﬁlter to the ﬁnal prediction.

Reconstruction. After the pruning step, we train the net-
work for one epoch to reconstruct the pruned ﬁlters. As
shown in Fig. 2, the pruned ﬁlters are updated to non-zero by
back-propagation. In this way, SFP allows the pruned model
to have the same capacity as the original model during train-
ing. In contrast, hard ﬁlter pruning decreases the number of
feature maps. The reduction of feature maps would dramat-
ically reduce the model capacity, and further harm the per-
formance. Previous pruning methods usually require a pre-
trained model and then ﬁne-tune it. However, as we inte-
grate the pruning step into the normal training schema, our
approach can train the model from scratch. Therefore, the

Figure 2: Overview of SFP. At the end of each training epoch, we prune the ﬁlters based on their importance evaluations. The ﬁlters are ranked
by their (cid:96)p-norms (purple rectangles) and the small ones (blue circles) are selected to be pruned. After ﬁlter pruning, the model undergoes a
reconstruction process where pruned ﬁlters are capable of being reconstructed (i.e., updated from zeros) by the forward-backward process.
(a): ﬁlter instantiations before pruning. (b): ﬁlter instantiations after pruning. (c): ﬁlter instantiations after reconstruction.

ﬁne-tuning stage is no longer necessary for SFP. As we will
show in experiments, the network trained from scratch by
SFP can obtain the competitive results with the one trained
from a well-trained model by others. By leveraging the pre-
trained model, SFP obtains a much higher performance and
advances the state-of-the-art.

Obtaining Compact Model. SFP iterates over the ﬁlter
selection, ﬁlter pruning and reconstruction steps. After the
model gets converged, we can obtain a sparse model contain-
ing many “zero ﬁlters”. One “zero ﬁlter” corresponds to one
feature map. The features maps, corresponding to those “zero
ﬁlters”, will always be zero during the inference procedure.
There will be no inﬂuence to remove these ﬁlters as well as
the corresponding feature maps. Speciﬁcally, for the prun-
ing rate Pi in the i-th layer, only Ni+1(1 − Pi) ﬁlters are
non-zero and have an effect on the ﬁnal prediction. Consider
pruning the previous layer, the input channel of i-th layer is
changed from Ni to Ni(1 − Pi−1). We can thus re-build
the i-th layer into a smaller one. Finally, a compact model
{W∗(i) ∈ RNi+1(1−Pi)×Ni(1−Pi−1)×K×K} is obtained.

3.3 Computation Complexity Analysis
Theoretical speedup analysis. Suppose the ﬁlter pruning
rate of the ith layer is Pi, which means the Ni+1 × Pi ﬁl-
ters are set to zero and pruned from the layer, and the other
Ni+1 × (1 − Pi) ﬁlters remain unchanged, and suppose the
size of the input and output feature map of ith layer is Hi×Wi
and Hi+1 × Wi+1. Then after ﬁlter pruning, the dimension
of useful output feature map of the ith layer decreases from
Ni+1 × Hi+1 × Wi+1 to Ni+1(1 − Pi) × Hi+1 × Wi+1.
Note that the output of ith layer is the input of (i + 1) th
layer. And we further prunes the (i + 1)th layer with a ﬁl-
ter pruning rate Pi+1, then the calculation of (i + 1)th layer
is decrease from Ni+2 × Ni+1 × k2 × Hi+2 × Wi+2 to
Ni+2(1 − Pi+1) × Ni+1(1 − Pi) × k2 × Hi+2 × Wi+2. In
other words, a proportion of 1 − (1 − Pi+1) × (1 − Pi) of the
original calculation is reduced, which will make the neural
network inference much faster.

Realistic speedup analysis. In theoretical speedup anal-
ysis, other operations such as batch normalization (BN) and
pooling are negligible comparing to convolution operations.
Therefore, we consider the FLOPs of convolution operations

for computation complexity comparison, which is commonly
used in previous work [Li et al., 2017; Luo et al., 2017].
However, reduced FLOPs cannot bring the same level of re-
alistic speedup because non-tensor layers (e.g., BN and pool-
ing layers) also need the inference time on GPU [Luo et al.,
2017]. In addition, the limitation of IO delay, buffer switch
and efﬁciency of BLAS libraries also lead to the wide gap be-
tween theoretical and realistic speedup ratio. We compare the
theoretical and realistic speedup in Section 4.3.

4 Evaluation and Results

4.1 Benchmark Datasets and Experimental Setting

Our method is evaluated on two benchmarks: CIFAR-
10 [Krizhevsky and Hinton, 2009] and ILSVRC-2012 [Rus-
sakovsky et al., 2015]. The CIFAR-10 dataset contains
50,000 training images and 10,000 testing images, which are
categorized into 10 classes.
ILSVRC-2012 is a large-scale
dataset containing 1.28 million training images and 50k val-
idation images of 1,000 classes. Following the common set-
ting in [Luo et al., 2017; He et al., 2017; Dong et al., 2017a],
we focus on pruning the challenging ResNet model in this
paper. SFP should also be effective on different computer
vision tasks, such as [Kang et al., 2017; Ren et al., 2015;
Dong et al., 2018; Shen et al., 2018b; Yang et al., 2010;
Shen et al., 2018a; Dong et al., 2017b], and we will explore
this in future.

In the CIFAR-10 experiments, we use the default parame-
ter setting as [He et al., 2016b] and follow the training sched-
ule in [Zagoruyko and Komodakis, 2016]. On ILSVRC-2012,
we follow the same parameter settings as [He et al., 2016a;
He et al., 2016b]. We use the same data argumentation strate-
gies with PyTorch ofﬁcial examples [Paszke et al., 2017].

We conduct our SFP operation at the end of every training
epoch. For pruning a scratch model, we use the normal train-
ing schedule. For pruning a pre-trained model, we reduce the
learning rate by 10 compared to the schedule for the scratch
model. We run each experiment three times and report the
“mean ± std”. We compare the performance with other state-
of-the-art acceleration algorithms, e.g., [Dong et al., 2017a;
Li et al., 2017; He et al., 2017; Luo et al., 2017].

Depth

Fine-tune? Baseline Accu. (%) Accelerated Accu. (%) Accu. Drop (%) FLOPs Pruned FLOPs(%)

20

32

56

110

Method
[Dong et al., 2017a]
Ours(10%)
Ours(20%)
Ours(30%)
[Dong et al., 2017a]
Ours(10%)
Ours(20%)
Ours(30%)
[Li et al., 2017]
[Li et al., 2017]
[He et al., 2017]
[He et al., 2017]
Ours(10%)
Ours(20%)
Ours(30%)
Ours(30%)
Ours(40%)
Ours(40%)
[Li et al., 2017]
[Li et al., 2017]
[Dong et al., 2017a]
Ours(10%)
Ours(20%)
Ours(30%)
Ours(30%)

N
N
N
N
N
N
N
N
N
Y
N
Y
N
N
N
Y
N
Y
N
Y
N
N
N
N
Y

91.53
92.20 ± 0.18
92.20 ± 0.18
92.20 ± 0.18
92.33
92.63 ± 0.70
92.63 ± 0.70
92.63 ± 0.70
93.04
93.04
92.80
92.80
93.59 ± 0.58
93.59 ± 0.58
93.59 ± 0.58
93.59 ± 0.58
93.59 ± 0.58
93.59 ± 0.58
93.53
93.53
93.63
93.68 ± 0.32
93.68 ± 0.32
93.68 ± 0.32
93.68 ± 0.32

91.43
92.24 ± 0.33
91.20 ± 0.30
90.83 ± 0.31
90.74
93.22 ± 0.09
90.63 ± 0.37
90.08 ± 0.08
91.31
93.06
90.90
91.80
93.89 ± 0.19
93.47 ± 0.24
93.10 ± 0.20
93.78 ± 0.22
92.26 ± 0.31
93.35 ± 0.31
92.94
93.30
93.44
93.83 ± 0.19
93.93 ± 0.41
93.38 ± 0.30
93.86 ± 0.21

0.10
-0.04
1.00
1.37
1.59
-0.59
0.00
0.55
1.75
-0.02
1.90
1.00
-0.30
0.12
0.49
-0.19
1.33
0.24
0.61
0.20
0.19
-0.15
-0.25
0.30
-0.18

3.20E7
3.44E7
2.87E7
2.43E7
4.70E7
5.86E7
4.90E7
4.03E7
9.09E7
9.09E7
-
-
1.070E8
8.98E7
7.40E7
7.40E7
5.94E7
5.94E7
1.55E8
1.55E8
-
2.16E8
1.82E8
1.50E8
1.50E8

20.3
15.2
29.3
42.2
31.2
14.9
28.8
41.5
27.6
27.6
50.0
50.0
14.7
28.4
41.1
41.1
52.6
52.6
38.6
38.6
34.2
14.6
28.2
40.8
40.8

Table 1: Comparison of pruning ResNet on CIFAR-10. In “Fine-tune?” column, “Y” and “N” indicate whether to use the pre-trained model
as initialization or not, respectively. The “Accu. Drop” is the accuracy of the pruned model minus that of the baseline model, so negative
number means the accelerated model has a higher accuracy than the baseline model. A smaller number of ”Accu. Drop” is better.

4.2 ResNet on CIFAR-10
Settings. For CIFAR-10 dataset, we test our SFP on ResNet-
20, 32, 56 and 110. We use several different pruning rates,
and also analyze the difference between using the pre-trained
model and from scratch.

Results. Tab. 1 shows the results. Our SFP could achieve
a better performance than the other state-of-the-art hard ﬁlter
pruning methods. For example, [Li et al., 2017] use the hard
pruning method to accelerate ResNet-110 by 38.6% speedup
ratio with 0.61% accuracy drop when without ﬁne-tuning.
When using pre-trained model and ﬁne-tuning, the accuracy
drop becomes 0.20%. However, we can accelerate the infer-
ence of ResNet-110 to 40.8% speed-up with only 0.30% ac-
curacy drop without ﬁne-tuning. When using the pre-trained
model, we can even outperform the original model by 0.18%
with about more than 40% FLOPs reduced.

These results validate the effectiveness of SFP, which can
produce a more compressed model with comparable perfor-
mance to the original model.

4.3 ResNet on ILSVRC-2012
Settings. For ILSVRC-2012 dataset, we test our SFP on
ResNet-18, 34, 50 and 101; and we use the same pruning rate
30% for all the models. All the convolutional layer of ResNet
are pruned with the same pruning rate at the same time. (We
do not prune the projection shortcuts for simpliﬁcation, which
only need negligible time and do not affect the overall cost.)
Results. Tab. 2 shows that SFP outperforms other state-
of-the-art methods.
For ResNet-34, SFP without ﬁne-
tuning achieves more inference speedup to the hard pruning

method [Luo et al., 2017], but the accuracy of our pruned
model exceeds their model by 2.57%. Moreover, for prun-
ing a pre-trained ResNet-101, SFP reduces more than 40%
FLOPs of the model with even 0.2% top-5 accuracy in-
crease, which is the state-of-the-art result.
In contrast, the
performance degradation is inevitable for hard ﬁlter pruning
method. Maintained model capacity of SFP is the main rea-
son for the superior performance. In addition, the non-greedy
all-layer pruning method may have a better performance than
the locally optimal solution obtained from previous greedy
pruning method, which seems to be another reason. Occa-
sionally, large performance degradation happens for the pre-
trained model (e.g., 14.01% top-1 accuracy drop for ResNet-
50). This will be explored in our future work.

To test the realistic speedup ratio, we measure the forward
time of the pruned models on one GTX1080 GPU with a
batch size of 64 (shown in Tab. 3). The gap between theo-
retical and realistic model may come from and the limitation
of IO delay, buffer switch and efﬁciency of BLAS libraries.

4.4 Ablation Study
We conducted extensive ablation studies to further analyze
each component of SFP.

Filter Selection Criteria. The magnitude based criteria
such as (cid:96)p-norm are widely used to ﬁlter selection because
computational resources cost is small [Li et al., 2017]. We
compare the (cid:96)2-norm and (cid:96)1-norm. For (cid:96)1-norm criteria,
the accuracy of the model under pruning rate 10%, 20%,
30% are 93.68±0.60%, 93.68±0.76% and 93.34±0.12%,
respectively. While for (cid:96)2-norm criteria,
the accuracy

Depth

Method

Fine-
tune?

18

34

50

101

Ours(30%)

[Dong et al., 2017a] N
N
[Dong et al., 2017a] N
Y
N
Y
Y
N
Y
N
Y

[Li et al., 2017]
Ours(30%)
[He et al., 2017]
[Luo et al., 2017]
Ours(30%)
Ours(30%)
Ours(30%)
Ours(30%)

Top-1 Accu.
Baseline(%)
69.98
70.28
73.42
73.23
73.92
-
72.88
76.15
76.15
77.37
77.37

Top-1 Accu.
Accelerated(%)
66.33
67.10
72.99
72.17
71.83
-
72.04
74.61
62.14
77.03
77.51

Top-5 Accu.
Baseline(%)
89.24
89.63
91.36
-
91.62
92.20
91.14
92.87
92.87
93.56
93.56

Top-5 Accu.
Accelerated(%)
86.94
87.78
91.19
-
90.33
90.80
90.67
92.06
84.60
93.46
93.71

Top-1 Accu.
Drop(%)
3.65
3.18
0.43
1.06
2.09
-
0.84
1.54
14.01
0.34
-0.14

Top-5 Accu.
Drop(%)
2.30
1.85
0.17
-
1.29
1.40
0.47
0.81
8.27
0.10
-0.20

Pruned
FLOPs(%)
34.6
41.8
24.8
24.2
41.1
50.0
36.7
41.8
41.8
42.2
42.2

Table 2: Comparison of pruning ResNet on ImageNet. “Fine-tune?” and ”Accu. Drop” have the same meaning with Tab. 1.

Model

ResNet-18
ResNet-34
ResNet-50
ResNet-101

Baseline
time (ms)
37.10
63.97
135.01
219.71

Pruned
time (ms)
26.97
45.14
94.66
148.64

Realistic
Speed-up(%)
27.4
29.4
29.8
32.3

Theoretical
Speed-up(%)
41.8
41.1
41.8
42.2

Table 3: Comparison on the theoretical and realistic speedup. We
only count the time consumption of the forward procedure.

(a) Different Pruning Rates

(b) Different SFP Intervals

Figure 3: Accuracy of ResNet-110 on CIFAR-10 regarding differ-
ent hyper-parameters. (Solid line and shadow denotes the mean and
standard deviation of three experiment, respectively.)

are 93.89±0.19%, 93.93±0.41% and 93.38±0.30%, respec-
tively. The performance of (cid:96)2-norm criteria is slightly better
than that of (cid:96)1-norm criteria. The result of (cid:96)2-norm is dom-
inated by the largest element, while the result of (cid:96)1-norm is
also largely affected by other small elements. Therefore, ﬁl-
ters with some large weights would be preserved by the (cid:96)2-
norm criteria. So the corresponding discriminative features
are kept so the performance of the pruned model is better.

Varying pruning rates. To comprehensively understand
SFP, we test the accuracy of different pruning rates for
ResNet-110, shown in Fig. 3(a). As the pruning rate in-
creases, the accuracy of the pruned model ﬁrst rises above the
baseline model and then drops approximately linearly. For
the pruning rate between 0% and about 23%, the accuracy of
the accelerated model is higher than the baseline model. This
shows that our SFP has a regularization effect on the neural
network because SFP reduces the over-ﬁtting of the model.

Sensitivity of SFP interval. By default, we conduct our
SFP operation at the end of every training epoch. However,

different SFP intervals may lead to different performance; so
we explore the sensitivity of SFP interval. We use the ResNet-
110 under pruning rate 30% as a baseline, and change the SFP
interval from one epoch to ten epochs, as shown in Fig. 3(b).
It is shown that the model accuracy has no large ﬂuctuation
along with the different SFP intervals. Moreover, the model
accuracy of most (80%) intervals surpasses the accuracy of
one epoch interval. Therefore, we can even achieve a better
performance if we ﬁne-tune this parameter.

Selection of pruned layers. Previous works always prune
a portion of the layers of the network. Besides, different lay-
ers always have different pruning rates. For example, [Li et
al., 2017] only prunes insensitive layers, [Luo et al., 2017]
skips the last layer of every block of the ResNet, and [Luo
et al., 2017] prunes more aggressive for shallower layers and
prune less for deep layers. Similarly, we compare the perfor-
mance of pruning ﬁrst and second layer of all basic blocks
of ResNet-110. We set the pruning rate as 30%. The model
with all the ﬁrst layers of blocks pruned has an accuracy of
93.96 ± 0.13%, while that with the second layers of blocks
pruned has an accuracy of 93.38 ± 0.44%. Therefore, differ-
ent layers have different sensitivity for SFP, and careful selec-
tion of pruned layers would potentially lead to performance
improvement, although more hyper-parameters are needed.

5 Conclusion and Future Work

In this paper, we propose a soft ﬁlter pruning (SFP) approach
to accelerate the deep CNNs. During the training procedure,
SFP allows the pruned ﬁlters to be updated. This soft manner
can maintain the model capacity and thus achieve the supe-
rior performance. Remarkably, SFP can achieve the competi-
tive performance compared to the state-of-the-art without the
pre-trained model. Moreover, by leveraging the pre-trained
model, SFP achieves a better result and advances the state-
of-the-art. Furthermore, SFP can be combined with other
acceleration algorithms, e.g., matrix decomposition and low-
precision weights, to further improve the performance.

Acknowledgments

Yi Yang is the recipient of a Google Faculty Research Award.
We acknowledge the Data to Decisions CRC (D2D CRC), the
Cooperative Research Centres Programme and ARC’s DE-

CRA (project DE170101415) for funding this research. We
thank Amazon for the AWS Cloud Credits.

References
[Dong et al., 2017a] Xuanyi Dong, Junshi Huang, Yi Yang,
and Shuicheng Yan. More is less: A more complicated
network with less inference complexity. In CVPR, 2017.
[Dong et al., 2017b] Xuanyi Dong, Deyu Meng, Fan Ma,
and Yi Yang. A dual-network progressive approach to
weakly supervised object detection. In ACM Multimedia,
2017.

[Dong et al., 2018] Xuanyi Dong, Shoou-I Yu, Xinshuo
Weng, Shih-En Wei, Yi Yang, and Yaser Sheikh.
Supervision-by-Registration: An unsupervised approach
to improve the precision of facial landmark detectors. In
CVPR, 2018.

[Guo et al., 2016] Yiwen Guo, Anbang Yao, and Yurong
Chen. Dynamic network surgery for efﬁcient DNNs. In
NIPS, 2016.

[Han et al., 2015a] Song Han, Huizi Mao, and William J
Dally. Deep compression: Compressing deep neural net-
works with pruning, trained quantization and huffman cod-
ing. In ICLR, 2015.

[Han et al., 2015b] Song Han, Jeff Pool, John Tran, and
William Dally. Learning both weights and connections for
efﬁcient neural network. In NIPS, 2015.

[He et al., 2016a] Kaiming He, Xiangyu Zhang, Shaoqing
Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, 2016.

[He et al., 2016b] Kaiming He, Xiangyu Zhang, Shaoqing
Identity mappings in deep residual

Ren, and Jian Sun.
networks. In ECCV, 2016.

[He et al., 2017] Yihui He, Xiangyu Zhang, and Jian Sun.
Channel pruning for accelerating very deep neural net-
works. In ICCV, 2017.

[Jaderberg et al., 2014] Max Jaderberg, Andrea Vedaldi, and
Andrew Zisserman. Speeding up convolutional neural net-
works with low rank expansions. In BMVC, 2014.

[Kang et al., 2017] Guoliang Kang, Jun Li, and Dacheng
Tao. Shakeout: A new approach to regularized deep neural
network training. IEEE T-PAMI, 2017.

[Krizhevsky and Hinton, 2009] Alex Krizhevsky and Geof-
frey Hinton. Learning multiple layers of features from tiny
images. 2009.

[Lebedev and Lempitsky, 2016] Vadim Lebedev and Victor
Lempitsky. Fast ConvNets using group-wise brain dam-
age. In CVPR, 2016.

[Li et al., 2017] Hao Li, Asim Kadav,

Igor Durdanovic,
Hanan Samet, and Hans Peter Graf. Pruning ﬁlters for
efﬁcient ConvNets. In ICLR, 2017.

[Luo et al., 2017] Jian-Hao Luo, Jianxin Wu, and Weiyao
Lin. ThiNet: A ﬁlter level pruning method for deep neural
network compression. In ICCV, 2017.

[Molchanov et al., 2017] Pavlo Molchanov, Stephen Tyree,
Tero Karras, Timo Aila, and Jan Kautz. Pruning convolu-
tional neural networks for resource efﬁcient transfer learn-
ing. In ICLR, 2017.

[Paszke et al., 2017] Adam Paszke, Sam Gross, Soumith
Chintala, Gregory Chanan, Edward Yang, Zachary De-
Vito, Zeming Lin, Alban Desmaison, Luca Antiga, and
In
Adam Lerer. Automatic differentiation in pytorch.
NIPS-W, 2017.

[Ren et al., 2015] Shaoqing Ren, Kaiming He, Ross Gir-
shick, and Jian Sun. Faster r-cnn: Towards real-time object
detection with region proposal networks. In NIPS, 2015.

[Russakovsky et al., 2015] Olga Russakovsky,

Jia Deng,
Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael
Bernstein, et al. ImageNet large scale visual recognition
challenge. IJCV, 2015.

[Shen et al., 2018a] Tao Shen, Tianyi Zhou, Guodong Long,
Jing Jiang, Shirui Pan, and Chengqi Zhang. Disan: Di-
rectional self-attention network for rnn/cnn-free language
understanding. In AAAI, 2018.

[Shen et al., 2018b] Tao Shen, Tianyi Zhou, Guodong Long,
Jing Jiang, and Chengqi Zhang. Bi-directional block self-
attention for fast and memory-efﬁcient sequence model-
ing. In ICLR, 2018.

[Szegedy et al., 2015] Christian Szegedy, Wei Liu, Yangqing
Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov,
Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabi-
novich. Going deeper with convolutions. In CVPR, 2015.
[Tai et al., 2016] Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang
Wang, et al. Convolutional neural networks with low-rank
regularization. In ICLR, 2016.

[Wen et al., 2016] Wei Wen, Chunpeng Wu, Yandan Wang,
Yiran Chen, and Hai Li. Learning structured sparsity in
deep neural networks. In NIPS, 2016.

[Yang et al., 2010] Yi Yang, Dong Xu, Feiping Nie,
Shuicheng Yan, and Yueting Zhuang. Image clustering us-
ing local discriminant models and global integration. IEEE
T-IP, 2010.

[Zagoruyko and Komodakis, 2016] Sergey Zagoruyko and
In BMVC,

Nikos Komodakis. Wide residual networks.
2016.

[Zhang et al., 2016] Xiangyu Zhang, Jianhua Zou, Kaiming
He, and Jian Sun. Accelerating very deep convolutional
networks for classiﬁcation and detection. IEEE T-PAMI,
2016.

[Liu et al., 2017] Zhuang Liu, Jianguo Li, Zhiqiang Shen,
Gao Huang, Shoumeng Yan, and Changshui Zhang.
Learning efﬁcient convolutional networks through net-
work slimming. In ICCV, 2017.

[Zhou et al., 2017] Aojun Zhou, Anbang Yao, Yiwen Guo,
Lin Xu, and Yurong Chen. Incremental network quantiza-
tion: Towards lossless cnns with low-precision weights. In
ICLR, 2017.

[Zhu et al., 2017] Chenzhuo Zhu, Song Han, Huizi Mao, and
William J Dally. Trained ternary quantization. In ICLR,
2017.

