Optical Flow Guided Feature: A Fast and Robust Motion Representation for
Video Action Recognition

Shuyang Sun1,2, Zhanghui Kuang2, Lu Sheng3, Wanli Ouyang1, Wei Zhang2

1The University of Sydney 2SenseTime Research 3The Chinese University of Hong Kong
{wayne.zhang kuangzhanghui}@sensetime.com
{shuyang.sun wanli.ouyang}@sydney.edu.au

lsheng@ee.cuhk.edu.hk

8
1
0
2
 
l
u
J
 
7
 
 
]

V
C
.
s
c
[
 
 
2
v
2
5
1
1
1
.
1
1
7
1
:
v
i
X
r
a

Abstract

Motion representation plays a vital role in human action
recognition in videos. In this study, we introduce a novel
compact motion representation for video action recogni-
tion, named Optical Flow guided Feature (OFF), which en-
ables the network to distill temporal information through
a fast and robust approach. The OFF is derived from the
deﬁnition of optical ﬂow and is orthogonal to the optical
ﬂow. The derivation also provides theoretical support for
using the difference between two frames. By directly cal-
culating pixel-wise spatio-temporal gradients of the deep
feature maps, the OFF could be embedded in any existing
CNN based video action recognition framework with only a
slight additional cost. It enables the CNN to extract spatio-
temporal information, especially the temporal information
between frames simultaneously. This simple but powerful
idea is validated by experimental results. The network with
OFF fed only by RGB inputs achieves a competitive accu-
racy of 93.3% on UCF-101, which is comparable with the
result obtained by two streams (RGB and optical ﬂow), but
is 15 times faster in speed. Experimental results also show
that OFF is complementary to other motion modalities such
as optical ﬂow. When the proposed method is plugged into
the state-of-the-art video action recognition framework, it
has 96.0% and 74.2% accuracy on UCF-101 and HMDB-
51 respectively. The code for this project is available at:
https://github.com/kevin-ssy/Optical-Flow-Guided-Feature

1. Introduction

Video action recognition has received longstanding at-
tentions in the community of computer vision for decades.
It aims at automatically recognizing human action from
video sequences. Since CNNs have achieved great suc-
cesses in image classiﬁcation and other related tasks [20,
30, 34, 15, 49, 51, 25], lots of CNN based methods have

Figure 1. The Optical Flow guided Feature (OFF). Left column:
input frames. Middle two columns: standard deep features before
applying OFF onto two frames. Right column: temporal differ-
ence in OFF. The colors red and cyan are used respectively for
positive and negative values. The feature difference between two
frames is valid and comprehensive in representing motion infor-
mation. Best seen in color and zoomed in.

been proposed by considering video action recognition as a
classiﬁcation task [5, 43, 23, 50, 11, 10, 9, 41, 42, 33, 29].
Compared to the image classiﬁcation methods, temporal in-
formation is the key ingredient of video action recognition.
Optical ﬂow is found to be a useful motion representa-
tion in video action recognition, including the Two-Stream-
based [29, 43] and 3D convolution-based methods [5].
However, extracting dense optical ﬂows is still inefﬁcient. It
costs over 90% of the whole run-time in a two-stream based
pipeline both at training and testing phases. Moreover, 3D
convolutions on RGB input can also capture temporal infor-
mation, but the RGB-based 3D CNN still does not perform
on par with its two-stream version. Other motion descrip-
tors, e.g., 3DHOG [19], improved Dense Trajectory [40],
and motion vector [50], are either inefﬁcient or not so ef-
fective as optical ﬂow.

How to design/use motion representation that is both fast

1

and robust? To this end, the required computation should
be economical and the representation should be sufﬁciently
guided by the motion information. Taking the above re-
quirements into consideration, we propose the Optical Flow
guided Feature (OFF), which is fast to compute and can
comprehensively represent motion dynamics in a video clip.
In this paper, we deﬁne a new feature representation from
the orthogonal space of optical ﬂow on the feature level
[16]. Such deﬁnition brings the guidance from optical ﬂow
here to the representation, therefore, we name it as the Op-
tical Flow guided Feature (OFF). The feature consists of
spatial gradients of feature maps in horizontal and vertical
directions, and temporal gradients obtained from the differ-
ence between feature maps from different frames. Since all
the operations in OFF are differentiable, the whole process
is end-to-end trainable when OFF is plugged into one CNN
architecture. Actually the OFF unit only consists of pixel-
wise operators on CNN features. These operators are fast
to apply, and enable the network with RGB input to capture
spatial and temporal information simultaneously.

One vital component in OFF is the difference between
features from different images/segments. As shown in
Fig. 1, the difference between the features from two im-
ages provides representative motion information that can be
conveniently employed by CNNs. The negative values in
the difference image depict the locations where the body
parts/objects disappear, while the positive values represent
where they emerge. This pattern of disappearing at one lo-
cation and emerging at another location can be easily treated
as a speciﬁc motion pattern and captured by later CNN lay-
ers. The temporal difference could be further combined
with the spatial gradients such that the constituted OFF is
guided by the optical ﬂow on feature level according to our
derivation in later section. Moreover, calculation of the mo-
tion dynamics at the feature level is faster and also more
robust because 1) it enables the spatial and temporal net-
works with the capability of weight sharing and 2) deeply
learned features convey more semantic and discriminative
representations with reliable elimination of local and back-
ground noises in the raw frames.

Our work has two main contributions.
First, OFF is a fast and robust motion representation.
OFF is fast to enable over 200 frames per second with only
RGB as the input and is derived from and guided by the
optical ﬂow. Taking only RGB from videos, experimental
results show that the CNN with OFF is close in performance
when compared with the state-of-the-art optical ﬂow based
algorithms. The CNNs with OFF can achieve 93.3% on
UCF-101 with only RGB as the input, which is currently
state-of-the-art among the RGB-based action recognition
methods. When plugging OFF in the state-of-the-art action
recognition method [43] in a Two-Stream manner (RGB +
Optical Flow), the performance of our algorithm could re-

sult in 96.0% on UCF-101 and 74.2% on HMDB-51.

Second, an OFF equipped network can be trained in
an end-to-end fashion.
In this way, the spatial and mo-
tion representations can be jointly learned through a sin-
gle network. This property is friendly for video tasks on
large-scale datasets, as it may not require the network to
pre-compute and store motion modalities for training. Be-
sides, the OFF can be used between images/segments in a
video clip both on image level and feature level.

The rest of this paper is organized as follows. Section
2 introduces recent methods that are related to our work.
Section 3 illustrates the deﬁnition of OFF and details our
proposed method. Section 4 explains our implementation
method in CNN. Our experimental results is summarized in
section 5, with concluding remarks in conclusion Section 6.

2. Related Work

Traditional methods extracted hand-craft local visual
features such as 3DHOG [19], Motion Boundary His-
tograms (MBH) [8],
improved Dense Trajectory (iDT)
[40, 39] and then encoded them into sparse or compact fea-
ture vectors which were fed into classiﬁers [27, 26]. Deeply
learned features were then found to perform better than
hand-crafted features for action recognition [29, 41].

As a signiﬁcant breakthrough in action recognition, Two-
Stream based frameworks used the deep CNN to learn from
the hand-craft motion features like optical ﬂow and iDT
[29, 41, 50, 43, 9, 47, 5, 35, 11, 12]. These attempts have
achieved remarkable progress in improving the recognition
accuracy, but still rely on the pre-computed optical ﬂow or
iDT, which constrains the speed of the whole framework.

In order to obtain the motion modality in a fast way, re-
cent works used optical ﬂow only at the training stage [23],
or proposed motion vector as the simpliﬁed version of opti-
cal ﬂow [50]. These attempts have produced degraded opti-
cal ﬂow results and still did not perform on par with the ap-
proaches using traditional optical ﬂow as the input stream.
Many approaches learn to capture the motion informa-
tion directly from input frames using 3D CNN [35, 37, 5,
36, 9, 38]. Boosted by the temporal convolution and pool-
ing operations, 3D CNN could distill the temporal informa-
tion between consecutive frames without segmenting them
into short snippets. Compared with the learning of ﬁlters to
capture motion information, our OFF is a principled repre-
sentation mathematically derived from the optical ﬂow. 3D
CNN, constrained by network design, training sample, and
parameter regularization like weight decay, may not be able
to learn good motion representation like OFF. Therefore,
current state-of-the-art 3D CNN based algorithms still rely
on traditional optical ﬂow to help the networks to capture
motion patterns. In comparison, our OFF 1) well captures
the motion patterns so that RGB stream with OFF performs
on par with two stream methods, and 2) is also complemen-

2

It assumes that for any point that moves from (x, y) at frame
t to (x + ∆x, y + ∆y) at frame t + ∆t, its brightness keeps
unchanged over time. When we apply this constraint at the
feature level, we have

f (I; w)(x, y, t) = f (I; w)(x + ∆x, y + ∆y, t + ∆t), (2)

where f is a mapping function for extracting features from
the image I. w denotes the parameters in the mapping func-
tion. The mapping function f can be any differentiable
function.
In this paper, we employ trainable CNNs con-
sisted of stacks of convolution , ReLU, and pooling opera-
tions. According to the deﬁnition of optical ﬂow, we assume
that p = (x, y, t) and obtain the equation as follows:

∂f (I; w)(p)
∂x

∆x +

∂f (I; w)(p)
∂y

∆y +

∂f (I; w)(p)
∂t

∆t = 0.

(3)

By dividing ∆t in both sides of Equation 3, we obtain

∂f (I; w)(p)
∂x

vx +

∂f (I; w)(p)
∂y

vy +

∂f (I; w)(p)
∂t

= 0, (4)

where p = (x, y, t), and (vx, vy) denotes the two di-
mensional velocity of feature point at p.
and
∂f (I;w)(p)
are the spatial gradients of ∂f (I; w)(p) in x and
∂y

∂f (I;w)(p)
∂x

y axes respectively. ∂f (I;w)
time axis.

∂t

is the temporal gradient along

As a special case, when f (I; w)(p) = I(p),

then
f (I; w)(p) simply represents pixel at p. In this special case,
(vx, vy) are called optical ﬂow. Optical ﬂow is obtained
by solving an optimization problem with the constraint in
Equation 4 for each p [1, 4, 2]. Here in this case, the term
∂f (I;w)(p)
represents the difference between RGB frames.
∂t
Previous works have shown that the temporal difference be-
tween frames is useful in video related tasks [43], however,
there is no theoretical evidence to help explain why this sim-
ple idea works that well. Here, we can ﬁnd its correlation to
spatial features and optical ﬂow.

, ∂f (I;w)(p)
∂y

We generalize the representation of optical ﬂow from
pixel I(p) to feature f (I; w)(p).
In this general case,
[vx, vy] are called the feature ﬂow. We can see from Equa-
tion 4 that (cid:126)F (I; w)(p) = [ ∂f (I;w)(p)
, ∂f (I;w)(p)
]
∂t
∂x
is orthogonal to the vector [vx, vy, 1] containing feature-
level optical ﬂow. (cid:126)F (I; w)(p) changes as the feature-level
optical ﬂow changes. Therefore, (cid:126)F (I; w)(p) is guided by
the feature-level optical ﬂow. We call (cid:126)F (I; w)(p) as Optical
Flow guided Feature (OFF). The OFF (cid:126)F (I; w)(p) encodes
the spatial-temporal information orthogonally and comple-
mentarily to the feature-level optical ﬂow (vx, vy). In the
next section, detailed implementation of OFF and its usage
for action recognition are introduced.

Figure 2. Network architecture overview. The feature genera-
tion sub-network extracts feature for each frame sampled from the
video. Based on the features from two adjacent frames extracted
by the feature generation sub-networks, a OFF sub-network is ap-
plied to generate the OFF for further classiﬁcation. The scores
from all sub-networks are fused to get the ﬁnal result.

tary to other motion representations like optical ﬂow.

To capture long-term temporal information from videos,
one intuitive approach is to introduce the Long Short-Term
Memory (LSTM) module as an encoder to encode the re-
lationship between the sequence-illustrating deep features
[47, 32, 28]. LSTM can still be applied on the OFF. There-
fore, our OFF is complementary to these methods.

Concurrent with our work, another state-of-the-art
method applies a strategy called ranked pool [13] that gen-
erates a fast video-level descriptor, namely, the dynamic im-
ages [3]. However, the very nature in design and implemen-
tation between the dynamic images and ours are different.
The dynamic images are designed to summarize a series of
frames while our method is designed to capture the motion
information related to optical ﬂow.

3. Optical Flow Guided Feature

Our proposed OFF is inspired by the famous brightness
constant constraint deﬁned by traditional optical ﬂow [16].
It is formulated as follows:

I(x, y, t) = I(x + ∆x, y + ∆y, t + ∆t),

(1)

where I(x, y, t) denotes the pixel at the location (x, y) of a
frame at time t. For frames t and (t + ∆t), ∆x and ∆y are
the spatial pixel displacement in x and y axes respectively.

3

Figure 3. Network architecture overview for two segments. The inputs are two segments in blue and green colors that are separately fed
into the feature generation sub-network to obtain basic features. In our experiment, the backbone for each feature generation sub-network
is the BN-Inception [34]. Here K represents the largest side length of the square feature map selected to undergo the OFF sub-network for
obtaining the OFF features. The OFF sub-network consists of several OFF units, and several residual blocks [15] are connected between
OFF units from different levels of resolution. These residual blocks constitute a ResNet-20 when seen as a whole. The scores obtained by
different sub-networks are supervised independently. Detailed structure of the OFF unit is shown in Figure 4.

4. Using Optical Flow Guided Feature in Con-

volutional Neural Network

4.1. Network Architecture

Network Architecture Overview. Figure 2 shows an
overview of the whole network architecture. The network
consists of three sub-networks for different purposes: fea-
ture generation sub-network, OFF sub-network and classi-
ﬁcation sub-network. The feature generation sub-network
generates basic features using common CNN structures. In
the OFF sub-network, the OFF features are extracted using
the features from the feature generation sub-network, and
then several residual blocks are stacked for obtaining the
reﬁned features. The features from the previous two sub-
networks are then used by the classiﬁcation sub-network
for obtaining the action recognition results. The Figure 3
exhibits the more detailed network structure with the inputs
of two segments. As shown in Figure 3, we extract features
from multiple layers on a speciﬁc level with the same res-
olution by concatenating them together and feed them into
one OFF unit. The whole network has 3 OFF units with
different scales. The details about the structure of each sub-
network is discussed as follows.

Feature Generation Sub-network. The basic features
f (I) (equivalent to the representation f (I; w) in previous
section) are extracted from the input image using several
convolutional layers with Rectiﬁed Linear Unit (ReLU) for
non-linear function and max-pooling for down-sampling.
We select BN-Inception [34] as the network structure to ex-
tract feature maps. The feature generation sub-network can
be replaced by any other network architecture.

OFF Sub-network. The OFF sub-network consists of
several OFF units. Different units use basic features f (I)
from different depths. As shown in Figure 4, an OFF unit
contains an OFF layer to generate the OFF. Each OFF layer
contains a 1×1 convolutional layer for each piece of feature,
and a set of operators including sobel and element-wise sub-
traction for OFF generation. After the OFF is obtained, the
OFF unit will concatenate them together with features from
the lower level, then the combined features will be output to
the following residual blocks.

The OFF layer is responsible for generating the OFF
from the basic features f (I). Figure 4 shows the detailed
implementation the OFF layer. According to Equation 3,
the OFF should consist of both spatial and temporal gradi-
ent of the feature. Denote f (I, c) as the cth channel of the

4

basic feature f (I). Denote Fx and Fy as the OFF for gra-
dients of x and y directions respectively, which correspond
to spatial gradients. We apply the Sobel operator for spatial
gradient generation as follows:

Fx =










−1 0
−1 0
−1 0


1
1
 ∗ f (I, c)
1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

c = 0 . . . , Nc − 1

(5)






Fy =










1
1
1
0
0
0
−1 −1 −1

 ∗ f (I, c)

(cid:12)
(cid:12)
c = 0, . . . , Nc − 1
(cid:12)
(cid:12)







(6)
where ∗ denotes a convolution operation, and the constant
Nc indicates the number of channels of the feature f (I).
Denote Ft as the OFF for gradients at the temporal direc-
tions. Temporal gradient is obtained by element-wise sub-
traction as follows:

Ft = {ft(I, c) − ft−∆t(I, c)|c = 0, . . . , Nc − 1}

(7)

With the features Fx, Fy, and Ft obtained above, we
concatenate them together with the features from the lower
level as the output of the OFF layer. We use a 1 × 1 convo-
lutional layer before the sobel and subtraction operations
to reduce the number of channels.
In our experiments,
the channel dimension is reduced to 128 regardless of how
many the input channels are. Then the feature is fed into
the OFF unit to calculate the OFF we deﬁned in previous
section. After the OFF is obtained, several residual blocks
designed in [15] are connected between the OFF units at
different levels of resolution as reﬁnement. The dimension-
ality of OFF is further reduced in the residual block adjacent
to the OFF unit for saving computation and the number of
parameters. The residual blocks on different levels of reso-
lution ﬁnally constitute a ResNet-20. Note that there is no
Batch Normalization [17] operation applied in our residual
network in order to avoid the over-ﬁtting problem.

The OFF unit can be applied for CNN layers on different
levels. The inputs of one OFF unit include the basic deep
features from two segments, and the feature from the OFF
unit on the previous feature level if it exists. In this way, the
OFF at the previous semantic level can be used for reﬁning
the OFF at the current semantic level.

Classiﬁcation Sub-network. The classiﬁcation sub-
network takes features from different sources and uses mul-
tiple inner-product classiﬁers to obtain multiple classiﬁca-
tion scores. The classiﬁcation scores of all sampled frames
are then combine by averaging for each feature generation
sub-network, or OFF sub-network. The OFF at a seman-
tic level can be used to produce a classiﬁcation score at the
training stage, which is learned using its corresponding loss.
Such strategy has been proved to be useful in many tasks
[34, 45, 22]. In the testing phase, scores from different sub-
networks could be assembled for better performance.

Figure 4. Detailed architecture of OFF unit. A 1x1 convolution
layer is connected to the input basic feature for dimension reduc-
tion. After that, we utilize the Sobel operator and element-wise
subtraction to calculate the spatial and temporal gradients respec-
tively. The combination of gradients constitutes the OFF, and the
sobel operator, subtracting operator and the 1 × 1 convolution lay-
ers before them constitute a OFF layer.

4.2. Network Training

Action recognition is treated as a multi-class classiﬁca-
tion problem. Followed by the settings in TSN, as there are
multiple classiﬁcation scores produced by each segment, we
need to fuse them all in each sub-network separately to gen-
erate a video-level score for loss calculation. Here, for the
OFF sub-networks, the features produced by the output of
OFF sub-network for the tth segment on level l is denoted
by Ft,l. The classiﬁcation score for segment t on the level
l using Ft,l is denoted by Gt,l. The aggregated video-level
score at level l is denoted by Gl. The video-level action
classiﬁcation score Gl is obtained by:

Gl = G(G0,l, . . . , G1,l, . . . , GNt−1−1,l),

(8)

where Nt denotes the number of frames for extracting fea-
tures. The aggregation function denoted by G is used for
summarizing the scores predicted from different segments
along time. Following the investigations in TSN, G is imple-
mented by average pooling for better performance [43]. As
for the feature generation sub-network, the above equations
are also applicable. While as we do not need intermediate
supervision for feature generation sub-network, the feature
Ft,l at level l for segment t is simply equivalent to the ﬁnal
feature output of the sub-network.

To update the parameters of the whole network, the loss
is set to be the standard categorical cross-entropy loss. As
the sub-network for each feature level is supervised inde-
pendently, a loss function is used for each level as:

Ll(y, Gl) = −

yc(Gl,c − log

eGl,j ).

(9)

C
(cid:88)

c=1

C
(cid:88)

j=1

5

where C is the number of action categories, Gl,c is the es-
timated score for class c from the features at level l, and yc
represents the ground-truth class label. By using this loss
function we can optimize the network parameters through
back-propagation. Detailed implementation of training is
described as follows.

Two-stage Training Strategy. Training of the whole
network consists of two stages. The ﬁrst stage indeed is
to apply existing approaches, e.g. TSN [43], to train the
feature generation sub-network. At the second stage, we
train the OFF and classiﬁcation sub-network with all the
weights in feature generation sub-network frozen. The
weights of OFF sub-network and classiﬁcation sub-network
are learned from scratch. The whole network could be fur-
ther ﬁne-tuned in an end-to-end manner, however, we do not
ﬁnd signiﬁcant gain in this stage. To simplify the training
process, we only train the network using the ﬁrst two stages.

Intermediate Supervision during Training. Interme-
diate supervision has been proven to be practical training
strategy in many other computer vision tasks [22, 45, 46,
24, 6]. As the OFF sub-networks are fed by intermediate
inputs, here we add the intermediate supervision on each
level to get better OFFs on each level of resolution.

Reducing the Memory Cost. As our framework con-
sists of several sub-networks, it costs more memory than the
original TSN framework, which extracts and stores motion
frames before training CNNs, and trains several networks
In order to reduce the computational and
independently.
memorial cost, we sample less frames in the training phase
than in the testing phase, and still obtain satisfactory results.

However, the time duration between segments may be
varied if we sample different number of segments between
training and testing. According to our deﬁnition in equation
3, only when the denotation ∆t is a ﬁxed constant, the equa-
tion 4 could be derived from the equation 3. If we sample
different frames between training and testing, the time in-
terval ∆t may be inconsistent, which makes our deﬁnition
to be invalid and inﬂuences the ﬁnal performance. In order
to keep time interval consistent between training and test-
ing, we design the sampling scheme carefully. Therefore,
during training, we sample frames from a video as follows:

Let α be the number of frames sampled for training, and
β be the number for testing. In training phase, a video with
length L, L >= β would be divided into β segments. Each
segment has length (cid:98)L/β(cid:99). We randomly select p from
0, 1, . . . , L − 1 − (α − 1) ∗ (cid:98)L/β(cid:99), where p is treated as a
frame seed. Then the whole training set is constructed as
{p, p + (cid:98)L/β(cid:99), ..., p + (α − 1) ∗ (cid:98)L/β(cid:99)}, which has interval
(cid:98)L/β(cid:99). In testing phase, we sample the images using the
same interval (cid:98)L/β(cid:99) as that in the training phase.

4.3. Network Testing

As there are multiple classiﬁcation scores produced by
different sub-networks, we need to fuse them all in test-
ing phase for better performance. In this study, we assem-
ble scores from the feature generation sub-network and the
last level of OFF sub-network by a simple summing opera-
tion. We select to test our model based on a state-of-the-art
framework TSN [43]. The testing setting under the TSN
framework is illustrated as follows:

Testing under TSN Framework. In the testing stage of
TSN, 25 segments are sampled from RGB, RGB difference,
and optical ﬂow. However, the number of frames in each
segment is different among these modalities. We use the
original settings adopted by TSN to sample 1, 5, 5 frames
per segment for RGB, RGB difference, and optical ﬂow re-
spectively. The input of our network is 25 segments, where
the tth segment is treated as the Frame t in Figure 3. In
this case, the features extracted by a separate branch of our
feature generation sub-network is for a segment instead of
a frame when using TSN. Other settings are kept to be the
same as those in TSN.

5. Experiments and Evaluations

In this section, datasets and implementation details used
in experiments will be ﬁrst introduced. Then we will ex-
plore the OFF and compare it with other modalities un-
der current state-of-the-art frameworks. Moreover, as our
method can be extended to other modalities such as RGB
difference and optical ﬂow, we will show how such a sim-
ple operation could improve the performance for input with
different modalities. Finally, we will discuss the meaning
and difference between the OFF and other motion modali-
ties such as optical ﬂow and RGB difference.

5.1. Datasets and Implementation Details

Evaluation Datasets. The experimental results are eval-
uated on two popular video action datasets, UCF-101 [31]
and HMDB-51 [21]. The UCF-101 dataset has 13320
videos and is divided into 101 classes, while the HMDB-
51 contains 6766 videos and 51 classes. Our experiments
follow the ofﬁcially offered scheme which divides a dataset
into 3 training and testing splits and ﬁnally calculating the
average accuracy over all 3 splits. We prepare the optical
ﬂow between frames before training by directly using the
OpenCV implemented algorithm [48].

Implementation Details. We train our model with 4
NVIDIA TITAN X GPU, under the implementation on
Caffe [18] and OpenMPI. We ﬁrst train the feature gener-
ation sub-networks using the same strategy provided in the
corresponding method [43]. Then at the second stage, we
train the OFF sub-networks from scratch with all param-
eters in the feature generation sub-networks frozen. The
mini-batch stochastic gradient descent algorithm is adopted

6

Method
TSN(RGB) [43]
TSN(RGB+RGB Diff) [43]
TSN(Flow) [43]
TSN(RGB+Flow) [43]
RGB+EMV-CNN [50]
MDI+RGB [3]
Two-Stream I3D
(RGB+Flow) [5]
RGB+OFF(RGB)+
RGB Diff+OFF(RGB Diff)

Speed (fps)
680
340
14
14
390
<131

<14

206

Acc.
85.5%
91.0%
87.9%
94.0%
86.4%
76.9%

93.4%

93.3%

Table 1. Experimental results of accuracy and efﬁciency for differ-
ent real-time video action recognition methods on UCF-101 over
three splits. Here the notation Flow represents the motion modal-
ity Optical Flow. Note that our OFF based algorithm could achieve
the state-of-the-art performance among real-time algorithms.

(cid:88)

(cid:88)

(cid:88)
(cid:88)

Acc.

Flow

RGB

(cid:88)
(cid:88)
(cid:88)

RGB
Diff

OFF
(Flow)

OFF
(RGB)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

OFF
(RGB Diff)

Speed
(fps)
680 85.5%
450 90.0%
340 90.7%
257 92.0%
206 93.0%
93.5%
14
95.1%
14
95.5%
14
Table 2. Experimental results for different modalities using the
OFF on UCF-101 Split1. Here Flow denotes the optical ﬂow.
OFF(*) denotes the use of OFF for the input *. For example,
OFF(RGB) denotes the use of OFF for RGB input. The speed
here illustrates the time cost for network forward. The results for
RGB and RGB + Flow are from [43]. The OFF(RGB) provides a
strong 4.5% improvement when fusing with RGB.

(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

here to learn the network parameters. When the feature gen-
eration sub-networks are fed by RGB frames, the whole
training procedure for OFF sub-network takes 20000 iter-
ations to converge with the learning rate initialized at 0.02
and decreased to its 0.1 using multi-step policy at the itera-
tion 10000, 15000 and 18000. When input changes to tem-
poral modality like optical ﬂow, the learning rate is initial-
ized at 0.05, and other policies are kept the same with what
have been proposed in RGB. The batch size is set to 128 and
all the training strategies described in previous sections are
applied. When evaluating on UCF-101 and HMDB-51, we
add dropout modules on spatial stream of OFF. There is no
difference on training parameters for different modalities.
However, when the input is RGB difference or optical ﬂow,
it would cost more time in both training and testing stages
as more frames are read into the network.

5.2. Experimental Investigations on OFF.

In this section, we will investigate the performance of
OFF under the TSN framework. The analysis for the per-
formance of single and multiple modalities, and the per-
formance comparison between the state-of-the-art will be
shown. All the results for OFF based networks are trained
with the same network backbone and strategies illustrated
in previous sections for fair comparison.

Efﬁciency Evaluation.

In this experiment, we evalu-
ate the efﬁciency between the OFF based method and other
state-of-the-art methods. The experimental results for ef-
ﬁciency and accuracy for different algorithms are summa-
rized in Table 1. OFF(RGB) denotes our use of OFF for
the network with RGB input, in this case, the OFF is ac-
quired from spatial deep features. As one special case, the
denotation RGB Diff represents the OFF calculated directly
from consecutive RGB frames on the input level instead of
on the feature level. After applying the OFF calculation

to RGB frames, the processed inputs could be fed into the
feature generation sub-network and the generated feature
maps could be again used to calculate their corresponding
OFF features on the feature level. The other methods we
compared here includes TSN [43] with different inputs, mo-
tion vector based RGB+EMV-CNN [50], dynamic image
based CNN [3] and current state-of-the-art 3D-CNN with
two stream [5]. From the Table 1, by applying the OFF to
the spatial features and the RGB inputs, we can achieve a
competitive accuracy 93.3% with only RGB inputs on the
UCF-101 over three splits, which is even comparable with
some Two-Stream based methods such as [5, 43]. Besides,
our methods is still very efﬁcient under this kind of set-
tings. The whole network could run over 200 fps, while
other methods listed here are either inefﬁcient or not so ef-
fective as the Two-Stream based approaches.

Effectiveness Evaluation. In this part, we try to investi-
gate the robustness of OFF when applying to different kinds
of input. According to the deﬁnition in equation 4, we can
replace the image I from RGB image to optical ﬂow or
RGB difference image to extract OFF on feature level for
further experiments. Based on the scores predicted by dif-
ferent modalities, we can further improve the classiﬁcation
performance by fusing them together [29, 9, 43, 50]. We
carry out the experimental results with various score fusing
schemes on UCF-101 split 1, and summarize them in Table
2. Table 2 shows the results when different kinds of modal-
ities are introduced as the network input. From each block
separated by a horizon line, we can ﬁnd that the OFF is
complementary to other kinds of modalities, e.g. RGB and
optical ﬂow, and could get a remarkable gain every time the
OFF is introduced. Besides, interestingly, the OFF is still
working when the input modality is already describing the
motion information. This phenomenon indicates that the
acceleration information between frames might also make

7

RGB Hyp-Net + RGB OFF(RGB) + RGB
85.5%

86.0%

90.0%

Acc.

Table 3. Experimental results of accuracy for hypercolumn net-
work and the comparison with OFF on UCF-101 Split1. The de-
notation ”Hyp-Net” indicates the output of hypercolumn network.

a difference in describing the temporal patterns.

Comparison with the Hypercolumns CNN. As our net-
work extracts intermediate deep features from a pre-trained
CNN, such hypercolumn based network structure may lead
to additional gain on speciﬁc datasets [14]. Experiment and
analysis are conducted to investigate whether the OFF is
playing a key role for the improvement. The network archi-
tecture and all training strategies for the hypercolumn CNN
are the same as that in OFF except for the removal of OFF
unit, in other words, the hypercolumn network here is con-
structed as the same structure of OFF sub-network without
OFF unit. In this case, the features from feature generation
sub-networks are directly fed into the OFF sub-networks
without the calculation of OFF.

From the experimental results shown in Table 3, it is
clear that, despite the hypercolumn network could get a
slight 0.5% improvement on UCF-101 split 1, its ﬁnal ac-
curacy is still apparently less than the one obtained by
OFF(RGB). Therefore, a conclusion could be drawn that
it is the OFF calculation rather than the hypercolumn struc-
ture that plays the key role in achieving the signiﬁcant gain.
Comparison with the State-of-the-art. Above all, af-
ter the exploration and analysis of the OFF, we show our
ﬁnal result. As what has been done in TSN, we also assem-
ble the classiﬁcation scores obtained by different kinds of
modalities. We sum the scores produced by each modality
together, and get the ﬁnal version output in Table 4. All the
results are evaluated in the UCF-101 and HMDB-51 over
3 splits. Our results are obtained by assembling the scores
from RGB, OFF(RGB), optical ﬂow and their correspond-
ing version of OFF(optical ﬂow) together. When we add
one more score from OFF(RGB Diff), a slight 0.3% gain
is obtained compared to the version without it, and ﬁnally
results in 96.0% on UCF-101 and 74.2% on HMDB-51.
Note that we do not introduce improved Dense Trajectories
(iDT)[40] into our network as the input. The components of
inputs we need to prepare in advance for our ﬁnal version
result only consist of RGB and optical ﬂow.

We compare our result with both the traditional ap-
proaches and deep learning based approaches. We obtain
2.0%/5.7% gain compared with the baseline Two-Stream
TSN [43] on UCF-101 [31] and HMDB-51 [21] respec-
tively. Note that the ﬁnal version TSN takes 3 modalities
(RGB, Optical Flow and iDT) as network input. The other
compared methods listed in Table 4 include iDT [40], Two-
Stream ConvNet [29], Two-Stream + LSTM [47], Tempo-
ral Deep-convolutional Descriptors (TDD) [41], Long-term

Method

iDT [40]
Two-Stream [29]
Two-Stream TSN [43]
Three-Stream TSN [43]
Two-Stream+LSTM [47]
TDD+iDT [41]
LTC+iDT [37]
KVMDF [52]
STP [44]
STMN+iDT [12]
ST-VLMPF+iDT [7]
L2STM [32]
Two-Stream I3D [5]
Two-Stream I3D
(with Kinetics 300k) [5]
Ours

UCF-101
86.4%
88.0%
94.0%
94.2%
88.6%
91.5%
91.7%
93.1%
94.6%
94.9%
94.3%
93.6%
93.4%

98.0%

96.0%

HMDB-51
61.7%
59.4%
68.5%
69.4%
-%
65.9%
64.8%
63.3%
68.9%
72.2%
73.1%
66.2%
66.4%

80.7%

74.2%

Table 4. Performance comparison to the state-of-the-art methods
on UCF-101 and HMDB-51 over 3 splits.

Temporal Convolutions (LTC) [37], Key Volume Mining
Deep Framework (KVMDF) [52], and also the current state-
of-the-art methods such as Spatio-Temporal Pyramid (STP)
[44], Saptio-Temporal Multiplier Network (STMN) [12],
Spatio-Temporal Vector [7], Lattice LSTM (L2STM) [32],
and I3D [5]. The method I3D could achieve spectacular per-
formance (98.0% on UCF-101, 80.7% on HMDB-51, over
3 splits) when proposing a new large dataset Kinetics for
pre-train. While without the pre-training, the method I3D
could achieve 93.4% on UCF-101 Split1. From the com-
parison with all the listed methods, we conclude that our
OFF based method allow for state-of-the-art performance
in video action recognition.

6. Conclusion

In this paper, we have presented Optical Flow guided
Feature (OFF), a novel motion representation derived from
and guided by the optical ﬂow. OFF is both fast and ro-
bust. By plugging the OFF into CNN framework, the result
with only RGB as input on UCF-101 is even comparable
to the result obtained by Two-Stream (RGB+Optical Flow)
approaches, and at the same time, the OFF plugged network
is still very efﬁcient with the speed over 200 frames per sec-
ond. Besides, it has been proven that the OFF is still com-
plementary to other motion representations like optical ﬂow.
Based on this representation, we proposed an new CNN ar-
chitecture for video action recognition. This architecture
outperforms many other state-of-the-art video action recog-
nition methods on two popular video datasets UCF-101 and
HMDB-51, and could be used to accelerate the speed of the
video based tasks.
In future works, we will validate our
method on other video based tasks and datasets.

8

References

[1] J. L. Barron, D. J. Fleet, and S. S. Beauchemin. Performance

of optical ﬂow techniques. IJCV, 12(1):43–77, 1994.

[2] J. Bigun, G. H. Granlund, and J. Wiklund. Multidimensional
orientation estimation with applications to texture analysis
and optical ﬂow. T-PAMI, 13(8):775–790, 1991.

[3] H. Bilen, B. Fernando, E. Gavves, A. Vedaldi, and S. Gould.
Dynamic image networks for action recognition. In CVPR,
pages 3034–3042, 2016.

[4] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert. High ac-
curacy optical ﬂow estimation based on a theory for warping.
In ECCV, pages 25–36. Springer, 2004.

[5] J. Carreira and A. Zisserman. Quo Vadis, Action Recogni-
tion? A New Model and the Kinetics Dataset. arXiv preprint
arXiv:1705.07750, 2017.

[6] X. Chu, W. Yang, W. Ouyang, C. Ma, A. L. Yuille, and
X. Wang. Multi-context attention for human pose estima-
tion. In CVPR, July 2017.

[7] I. Cosmin Duta, B. Ionescu, K. Aizawa, and N. Sebe. Spatio-
Temporal Vector of Locally Max Pooled Features for Action
Recognition in Videos. In CVPR, pages 3097–3106, 2017.
[8] N. Dalal, B. Triggs, C. Schmid, N. Dalal, B. Triggs,
C. Schmid, H. Detection, and U. Oriented. Human Detec-
tion Using Oriented Histograms of Flow and Appearance. In
ECCV, pages 428–441, 2006.

[9] A. Diba, A. M. Pazandeh, and L. Van Gool. Efﬁcient two-
stream motion and appearance 3d cnns for video classiﬁca-
tion. arXiv preprint arXiv:1608.08851, 2016.

[10] A. Diba, V. Sharma, and L. Van Gool. Deep temporal linear
encoding networks. arXiv preprint arXiv:1611.06678, 2016.
[11] C. Feichtenhofer, A. Pinz, and R. Wildes. Spatiotemporal
In NIPS,

residual networks for video action recognition.
pages 3468–3476, 2016.

[12] C. Feichtenhofer, A. Pinz, and R. P. Wildes. Spatiotemporal
multiplier networks for video action recognition. In CVPR,
2017.

[13] B. Fernando, E. Gavves, J. Oramas, A. Ghodrati, and
T. Tuytelaars. Rank pooling for action recognition. T-PAMI,
39(4):773–787, 2017.

[14] B. Hariharan, P. Arbel´aez, R. Girshick, and J. Malik. Hyper-
columns for object segmentation and ﬁne-grained localiza-
tion. In CVPR, pages 447–456, 2015.

[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In CVPR, pages 770–778, 2016.
[16] B. G. Horn, Berthold K.P.; Schunck. Determining Optical

Flow. Artiﬁcial Intelligence, 17:185–203, 1981.

[17] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, pages 448–456, 2015.

[18] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
Architecture for Fast Feature Embedding. arXiv preprint
arXiv:1408.5093, 2014.

[19] A. Klaser, M. Marszalek, and C. Schmid. A Spatio-Temporal
Descriptor Based on 3D-Gradients. In BMVC, pages 275–1,
2008.

[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, pages 1097–1105, 2012.

Imagenet
In

[21] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre.
HMDB: A large video database for human motion recogni-
tion. In ICCV, pages 2556–2563, 2011.

[22] A. Newell, K. Yang, and J. Deng. Stacked Hourglass Net-
In ECCV, pages 483–

works for Human Pose Estimation.
499. Springer, 2016.

[23] J. Y.-H. Ng, J. Choi, J. Neumann, and L. S. Davis. Ac-
tionFlowNet: Learning Motion Representation for Action
Recognition. arXiv preprint arXiv:1612.03052, 2016.
[24] W. Ouyang, K. Wang, X. Zhu, and X. Wang. Chained cas-
cade network for object detection. In ICCV, Oct 2017.
[25] W. Ouyang, X. Zeng, and X. Wang. Learning mutual visibil-
ity relationship for pedestrian detection with a deep model.
IJCV, 120(1):14–27, 2016.

[26] X. Peng, L. Wang, X. Wang, and Y. Qiao. Bag of Visual
Words and Fusion Methods for Action Recognition: Com-
prehensive Study and Good Practice. Computer Vision and
Image Understanding, 150:109–125, 2016.

[27] P. Scovanner, S. Ali, and M. Shah. A 3-dimensional sift
In
descriptor and its application to action recognition.
ACM’MM, pages 357–360, 2007.

[28] Y. Shi, Y. Tian, Y. Wang, W. Zeng, and T. Huang. Learn-
ing long-term dependencies for action recognition with a
In CVPR, pages 716–
biologically-inspired deep network.
725, 2017.

[29] K. Simonyan and A. Zisserman. Two-stream convolutional
In NIPS, pages

networks for action recognition in videos.
568–576, 2014.

[30] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014.

[31] K. Soomro, A. R. Zamir, and M. Shah. UCF101: A Dataset
of 101 Human Actions Classes From Videos in The Wild.
arXiv preprint arXiv:1212.0402, 2012.

[32] L. Sun, K. Jia, K. Chen, D. Y. Yeung, B. E. Shi, and
S. Savarese. Lattice Long Short-Term Memory for Hu-
man Action Recognition. arXiv preprint arXiv:1708.03958,
2017.

[33] L. Sun, K. Jia, D.-Y. Yeung, and B. E. Shi. Human action
recognition using factorized spatio-temporal convolutional
networks. In ICCV, pages 4597–4605, 2015.

[34] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
In CVPR, pages 1–9,
Going Deeper with Convolutions.
2015.

[35] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri.
Learning spatiotemporal features with 3d convolutional net-
works. In ICCV, pages 4489–4497, 2015.

[36] D. Tran, J. Ray, Z. Shou, S.-F. Chang, and M. Paluri. Con-
vNet Architecture Search for Spatiotemporal Feature Learn-
ing. arXiv preprint arXiv:1708.05038, 2017.

[37] G. Varol, I. Laptev, and C. Schmid. Long-term tempo-
arXiv preprint

ral convolutions for action recognition.
arXiv:1604.04494, 2016.

9

[38] G. Varol, I. Laptev, and C. Schmid. Long-term temporal

convolutions for action recognition. T-PAMI, 2017.

[39] H. Wang, A. Klaser, C. Schmid, and C.-L. Liu. Action recog-
In CVPR, pages 3169–3176.

nition by dense trajectories.
IEEE, 2011.

[40] H. Wang and C. Schmid. Action Recognition with Improved

Trajectories. In ICCV, pages 3551–3558, 2013.

[41] L. Wang, Y. Qiao, and X. Tang. Action recognition with
trajectory-pooled deep-convolutional descriptors. In ICCV,
pages 4305–4314, 2015.

[42] L. Wang, Y. Xiong, Z. Wang, and Y. Qiao. Towards good
practices for very deep two-stream convnets. arXiv preprint
arXiv:1507.02159, 2015.

[43] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and
L. Van Gool. Temporal segment networks: Towards good
practices for deep action recognition. ECCV, pages 20–36,
2016.

[44] Y. Wang, M. Long, J. Wang, and P. S. Yu. Spatiotemporal
Pyramid Network for Video Action Recognition. In CVPR,
pages 1529–1538, 2017.

[45] S.-E. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh. Con-
In CVPR, pages 4724–4732,

volutional pose machines.
2016.

[46] W. Yang, S. Li, W. Ouyang, H. Li, and X. Wang. Learning
feature pyramids for human pose estimation. In ICCV, Oct
2017.

[47] J. Yue-Hei Ng, M. Hausknecht, S. Vijayanarasimhan,
O. Vinyals, R. Monga, and G. Toderici. Beyond short snip-
pets: Deep networks for video classiﬁcation. In CVPR, pages
4694–4702, 2015.

[48] C. Zach, T. Pock, and H. Bischof. A duality based approach
for realtime TV-L1 optical ﬂow. Pattern Recognition, pages
214–223, 2007.

[49] X. Zeng, W. Ouyang, J. Yan, H. Li, T. Xiao, K. Wang, Y. Liu,
Y. Zhou, B. Yang, Z. Wang, et al. Crafting gbd-net for object
detection. T-PAMI, 2017.

[50] B. Zhang, L. Wang, Z. Wang, Y. Qiao, and H. Wang. Real-
time action recognition with enhanced motion vector CNNs.
In CVPR, pages 2718–2726, 2016.

[51] H. Zhao, M. Tian, S. Sun, J. Shao, J. Yan, S. Yi, X. Wang,
and X. Tang. Spindle net: Person re-identiﬁcation with hu-
man body region guided feature decomposition and fusion.
In CVPR, July 2017.

[52] W. Zhu, J. Hu, G. Sun, X. Cao, and Y. Qiao. A key volume
In CVPR,

mining deep framework for action recognition.
pages 1991–1999, 2016.

10

Optical Flow Guided Feature: A Fast and Robust Motion Representation for
Video Action Recognition

Shuyang Sun1,2, Zhanghui Kuang2, Lu Sheng3, Wanli Ouyang1, Wei Zhang2

1The University of Sydney 2SenseTime Research 3The Chinese University of Hong Kong
{wayne.zhang kuangzhanghui}@sensetime.com
{shuyang.sun wanli.ouyang}@sydney.edu.au

lsheng@ee.cuhk.edu.hk

8
1
0
2
 
l
u
J
 
7
 
 
]

V
C
.
s
c
[
 
 
2
v
2
5
1
1
1
.
1
1
7
1
:
v
i
X
r
a

Abstract

Motion representation plays a vital role in human action
recognition in videos. In this study, we introduce a novel
compact motion representation for video action recogni-
tion, named Optical Flow guided Feature (OFF), which en-
ables the network to distill temporal information through
a fast and robust approach. The OFF is derived from the
deﬁnition of optical ﬂow and is orthogonal to the optical
ﬂow. The derivation also provides theoretical support for
using the difference between two frames. By directly cal-
culating pixel-wise spatio-temporal gradients of the deep
feature maps, the OFF could be embedded in any existing
CNN based video action recognition framework with only a
slight additional cost. It enables the CNN to extract spatio-
temporal information, especially the temporal information
between frames simultaneously. This simple but powerful
idea is validated by experimental results. The network with
OFF fed only by RGB inputs achieves a competitive accu-
racy of 93.3% on UCF-101, which is comparable with the
result obtained by two streams (RGB and optical ﬂow), but
is 15 times faster in speed. Experimental results also show
that OFF is complementary to other motion modalities such
as optical ﬂow. When the proposed method is plugged into
the state-of-the-art video action recognition framework, it
has 96.0% and 74.2% accuracy on UCF-101 and HMDB-
51 respectively. The code for this project is available at:
https://github.com/kevin-ssy/Optical-Flow-Guided-Feature

1. Introduction

Video action recognition has received longstanding at-
tentions in the community of computer vision for decades.
It aims at automatically recognizing human action from
video sequences. Since CNNs have achieved great suc-
cesses in image classiﬁcation and other related tasks [20,
30, 34, 15, 49, 51, 25], lots of CNN based methods have

Figure 1. The Optical Flow guided Feature (OFF). Left column:
input frames. Middle two columns: standard deep features before
applying OFF onto two frames. Right column: temporal differ-
ence in OFF. The colors red and cyan are used respectively for
positive and negative values. The feature difference between two
frames is valid and comprehensive in representing motion infor-
mation. Best seen in color and zoomed in.

been proposed by considering video action recognition as a
classiﬁcation task [5, 43, 23, 50, 11, 10, 9, 41, 42, 33, 29].
Compared to the image classiﬁcation methods, temporal in-
formation is the key ingredient of video action recognition.
Optical ﬂow is found to be a useful motion representa-
tion in video action recognition, including the Two-Stream-
based [29, 43] and 3D convolution-based methods [5].
However, extracting dense optical ﬂows is still inefﬁcient. It
costs over 90% of the whole run-time in a two-stream based
pipeline both at training and testing phases. Moreover, 3D
convolutions on RGB input can also capture temporal infor-
mation, but the RGB-based 3D CNN still does not perform
on par with its two-stream version. Other motion descrip-
tors, e.g., 3DHOG [19], improved Dense Trajectory [40],
and motion vector [50], are either inefﬁcient or not so ef-
fective as optical ﬂow.

How to design/use motion representation that is both fast

1

and robust? To this end, the required computation should
be economical and the representation should be sufﬁciently
guided by the motion information. Taking the above re-
quirements into consideration, we propose the Optical Flow
guided Feature (OFF), which is fast to compute and can
comprehensively represent motion dynamics in a video clip.
In this paper, we deﬁne a new feature representation from
the orthogonal space of optical ﬂow on the feature level
[16]. Such deﬁnition brings the guidance from optical ﬂow
here to the representation, therefore, we name it as the Op-
tical Flow guided Feature (OFF). The feature consists of
spatial gradients of feature maps in horizontal and vertical
directions, and temporal gradients obtained from the differ-
ence between feature maps from different frames. Since all
the operations in OFF are differentiable, the whole process
is end-to-end trainable when OFF is plugged into one CNN
architecture. Actually the OFF unit only consists of pixel-
wise operators on CNN features. These operators are fast
to apply, and enable the network with RGB input to capture
spatial and temporal information simultaneously.

One vital component in OFF is the difference between
features from different images/segments. As shown in
Fig. 1, the difference between the features from two im-
ages provides representative motion information that can be
conveniently employed by CNNs. The negative values in
the difference image depict the locations where the body
parts/objects disappear, while the positive values represent
where they emerge. This pattern of disappearing at one lo-
cation and emerging at another location can be easily treated
as a speciﬁc motion pattern and captured by later CNN lay-
ers. The temporal difference could be further combined
with the spatial gradients such that the constituted OFF is
guided by the optical ﬂow on feature level according to our
derivation in later section. Moreover, calculation of the mo-
tion dynamics at the feature level is faster and also more
robust because 1) it enables the spatial and temporal net-
works with the capability of weight sharing and 2) deeply
learned features convey more semantic and discriminative
representations with reliable elimination of local and back-
ground noises in the raw frames.

Our work has two main contributions.
First, OFF is a fast and robust motion representation.
OFF is fast to enable over 200 frames per second with only
RGB as the input and is derived from and guided by the
optical ﬂow. Taking only RGB from videos, experimental
results show that the CNN with OFF is close in performance
when compared with the state-of-the-art optical ﬂow based
algorithms. The CNNs with OFF can achieve 93.3% on
UCF-101 with only RGB as the input, which is currently
state-of-the-art among the RGB-based action recognition
methods. When plugging OFF in the state-of-the-art action
recognition method [43] in a Two-Stream manner (RGB +
Optical Flow), the performance of our algorithm could re-

sult in 96.0% on UCF-101 and 74.2% on HMDB-51.

Second, an OFF equipped network can be trained in
an end-to-end fashion.
In this way, the spatial and mo-
tion representations can be jointly learned through a sin-
gle network. This property is friendly for video tasks on
large-scale datasets, as it may not require the network to
pre-compute and store motion modalities for training. Be-
sides, the OFF can be used between images/segments in a
video clip both on image level and feature level.

The rest of this paper is organized as follows. Section
2 introduces recent methods that are related to our work.
Section 3 illustrates the deﬁnition of OFF and details our
proposed method. Section 4 explains our implementation
method in CNN. Our experimental results is summarized in
section 5, with concluding remarks in conclusion Section 6.

2. Related Work

Traditional methods extracted hand-craft local visual
features such as 3DHOG [19], Motion Boundary His-
tograms (MBH) [8],
improved Dense Trajectory (iDT)
[40, 39] and then encoded them into sparse or compact fea-
ture vectors which were fed into classiﬁers [27, 26]. Deeply
learned features were then found to perform better than
hand-crafted features for action recognition [29, 41].

As a signiﬁcant breakthrough in action recognition, Two-
Stream based frameworks used the deep CNN to learn from
the hand-craft motion features like optical ﬂow and iDT
[29, 41, 50, 43, 9, 47, 5, 35, 11, 12]. These attempts have
achieved remarkable progress in improving the recognition
accuracy, but still rely on the pre-computed optical ﬂow or
iDT, which constrains the speed of the whole framework.

In order to obtain the motion modality in a fast way, re-
cent works used optical ﬂow only at the training stage [23],
or proposed motion vector as the simpliﬁed version of opti-
cal ﬂow [50]. These attempts have produced degraded opti-
cal ﬂow results and still did not perform on par with the ap-
proaches using traditional optical ﬂow as the input stream.
Many approaches learn to capture the motion informa-
tion directly from input frames using 3D CNN [35, 37, 5,
36, 9, 38]. Boosted by the temporal convolution and pool-
ing operations, 3D CNN could distill the temporal informa-
tion between consecutive frames without segmenting them
into short snippets. Compared with the learning of ﬁlters to
capture motion information, our OFF is a principled repre-
sentation mathematically derived from the optical ﬂow. 3D
CNN, constrained by network design, training sample, and
parameter regularization like weight decay, may not be able
to learn good motion representation like OFF. Therefore,
current state-of-the-art 3D CNN based algorithms still rely
on traditional optical ﬂow to help the networks to capture
motion patterns. In comparison, our OFF 1) well captures
the motion patterns so that RGB stream with OFF performs
on par with two stream methods, and 2) is also complemen-

2

It assumes that for any point that moves from (x, y) at frame
t to (x + ∆x, y + ∆y) at frame t + ∆t, its brightness keeps
unchanged over time. When we apply this constraint at the
feature level, we have

f (I; w)(x, y, t) = f (I; w)(x + ∆x, y + ∆y, t + ∆t), (2)

where f is a mapping function for extracting features from
the image I. w denotes the parameters in the mapping func-
tion. The mapping function f can be any differentiable
function.
In this paper, we employ trainable CNNs con-
sisted of stacks of convolution , ReLU, and pooling opera-
tions. According to the deﬁnition of optical ﬂow, we assume
that p = (x, y, t) and obtain the equation as follows:

∂f (I; w)(p)
∂x

∆x +

∂f (I; w)(p)
∂y

∆y +

∂f (I; w)(p)
∂t

∆t = 0.

(3)

By dividing ∆t in both sides of Equation 3, we obtain

∂f (I; w)(p)
∂x

vx +

∂f (I; w)(p)
∂y

vy +

∂f (I; w)(p)
∂t

= 0, (4)

where p = (x, y, t), and (vx, vy) denotes the two di-
mensional velocity of feature point at p.
and
∂f (I;w)(p)
are the spatial gradients of ∂f (I; w)(p) in x and
∂y

∂f (I;w)(p)
∂x

y axes respectively. ∂f (I;w)
time axis.

∂t

is the temporal gradient along

As a special case, when f (I; w)(p) = I(p),

then
f (I; w)(p) simply represents pixel at p. In this special case,
(vx, vy) are called optical ﬂow. Optical ﬂow is obtained
by solving an optimization problem with the constraint in
Equation 4 for each p [1, 4, 2]. Here in this case, the term
∂f (I;w)(p)
represents the difference between RGB frames.
∂t
Previous works have shown that the temporal difference be-
tween frames is useful in video related tasks [43], however,
there is no theoretical evidence to help explain why this sim-
ple idea works that well. Here, we can ﬁnd its correlation to
spatial features and optical ﬂow.

, ∂f (I;w)(p)
∂y

We generalize the representation of optical ﬂow from
pixel I(p) to feature f (I; w)(p).
In this general case,
[vx, vy] are called the feature ﬂow. We can see from Equa-
tion 4 that (cid:126)F (I; w)(p) = [ ∂f (I;w)(p)
, ∂f (I;w)(p)
]
∂t
∂x
is orthogonal to the vector [vx, vy, 1] containing feature-
level optical ﬂow. (cid:126)F (I; w)(p) changes as the feature-level
optical ﬂow changes. Therefore, (cid:126)F (I; w)(p) is guided by
the feature-level optical ﬂow. We call (cid:126)F (I; w)(p) as Optical
Flow guided Feature (OFF). The OFF (cid:126)F (I; w)(p) encodes
the spatial-temporal information orthogonally and comple-
mentarily to the feature-level optical ﬂow (vx, vy). In the
next section, detailed implementation of OFF and its usage
for action recognition are introduced.

Figure 2. Network architecture overview. The feature genera-
tion sub-network extracts feature for each frame sampled from the
video. Based on the features from two adjacent frames extracted
by the feature generation sub-networks, a OFF sub-network is ap-
plied to generate the OFF for further classiﬁcation. The scores
from all sub-networks are fused to get the ﬁnal result.

tary to other motion representations like optical ﬂow.

To capture long-term temporal information from videos,
one intuitive approach is to introduce the Long Short-Term
Memory (LSTM) module as an encoder to encode the re-
lationship between the sequence-illustrating deep features
[47, 32, 28]. LSTM can still be applied on the OFF. There-
fore, our OFF is complementary to these methods.

Concurrent with our work, another state-of-the-art
method applies a strategy called ranked pool [13] that gen-
erates a fast video-level descriptor, namely, the dynamic im-
ages [3]. However, the very nature in design and implemen-
tation between the dynamic images and ours are different.
The dynamic images are designed to summarize a series of
frames while our method is designed to capture the motion
information related to optical ﬂow.

3. Optical Flow Guided Feature

Our proposed OFF is inspired by the famous brightness
constant constraint deﬁned by traditional optical ﬂow [16].
It is formulated as follows:

I(x, y, t) = I(x + ∆x, y + ∆y, t + ∆t),

(1)

where I(x, y, t) denotes the pixel at the location (x, y) of a
frame at time t. For frames t and (t + ∆t), ∆x and ∆y are
the spatial pixel displacement in x and y axes respectively.

3

Figure 3. Network architecture overview for two segments. The inputs are two segments in blue and green colors that are separately fed
into the feature generation sub-network to obtain basic features. In our experiment, the backbone for each feature generation sub-network
is the BN-Inception [34]. Here K represents the largest side length of the square feature map selected to undergo the OFF sub-network for
obtaining the OFF features. The OFF sub-network consists of several OFF units, and several residual blocks [15] are connected between
OFF units from different levels of resolution. These residual blocks constitute a ResNet-20 when seen as a whole. The scores obtained by
different sub-networks are supervised independently. Detailed structure of the OFF unit is shown in Figure 4.

4. Using Optical Flow Guided Feature in Con-

volutional Neural Network

4.1. Network Architecture

Network Architecture Overview. Figure 2 shows an
overview of the whole network architecture. The network
consists of three sub-networks for different purposes: fea-
ture generation sub-network, OFF sub-network and classi-
ﬁcation sub-network. The feature generation sub-network
generates basic features using common CNN structures. In
the OFF sub-network, the OFF features are extracted using
the features from the feature generation sub-network, and
then several residual blocks are stacked for obtaining the
reﬁned features. The features from the previous two sub-
networks are then used by the classiﬁcation sub-network
for obtaining the action recognition results. The Figure 3
exhibits the more detailed network structure with the inputs
of two segments. As shown in Figure 3, we extract features
from multiple layers on a speciﬁc level with the same res-
olution by concatenating them together and feed them into
one OFF unit. The whole network has 3 OFF units with
different scales. The details about the structure of each sub-
network is discussed as follows.

Feature Generation Sub-network. The basic features
f (I) (equivalent to the representation f (I; w) in previous
section) are extracted from the input image using several
convolutional layers with Rectiﬁed Linear Unit (ReLU) for
non-linear function and max-pooling for down-sampling.
We select BN-Inception [34] as the network structure to ex-
tract feature maps. The feature generation sub-network can
be replaced by any other network architecture.

OFF Sub-network. The OFF sub-network consists of
several OFF units. Different units use basic features f (I)
from different depths. As shown in Figure 4, an OFF unit
contains an OFF layer to generate the OFF. Each OFF layer
contains a 1×1 convolutional layer for each piece of feature,
and a set of operators including sobel and element-wise sub-
traction for OFF generation. After the OFF is obtained, the
OFF unit will concatenate them together with features from
the lower level, then the combined features will be output to
the following residual blocks.

The OFF layer is responsible for generating the OFF
from the basic features f (I). Figure 4 shows the detailed
implementation the OFF layer. According to Equation 3,
the OFF should consist of both spatial and temporal gradi-
ent of the feature. Denote f (I, c) as the cth channel of the

4

basic feature f (I). Denote Fx and Fy as the OFF for gra-
dients of x and y directions respectively, which correspond
to spatial gradients. We apply the Sobel operator for spatial
gradient generation as follows:

Fx =










−1 0
−1 0
−1 0


1
1
 ∗ f (I, c)
1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

c = 0 . . . , Nc − 1

(5)






Fy =










1
1
1
0
0
0
−1 −1 −1

 ∗ f (I, c)

(cid:12)
(cid:12)
c = 0, . . . , Nc − 1
(cid:12)
(cid:12)







(6)
where ∗ denotes a convolution operation, and the constant
Nc indicates the number of channels of the feature f (I).
Denote Ft as the OFF for gradients at the temporal direc-
tions. Temporal gradient is obtained by element-wise sub-
traction as follows:

Ft = {ft(I, c) − ft−∆t(I, c)|c = 0, . . . , Nc − 1}

(7)

With the features Fx, Fy, and Ft obtained above, we
concatenate them together with the features from the lower
level as the output of the OFF layer. We use a 1 × 1 convo-
lutional layer before the sobel and subtraction operations
to reduce the number of channels.
In our experiments,
the channel dimension is reduced to 128 regardless of how
many the input channels are. Then the feature is fed into
the OFF unit to calculate the OFF we deﬁned in previous
section. After the OFF is obtained, several residual blocks
designed in [15] are connected between the OFF units at
different levels of resolution as reﬁnement. The dimension-
ality of OFF is further reduced in the residual block adjacent
to the OFF unit for saving computation and the number of
parameters. The residual blocks on different levels of reso-
lution ﬁnally constitute a ResNet-20. Note that there is no
Batch Normalization [17] operation applied in our residual
network in order to avoid the over-ﬁtting problem.

The OFF unit can be applied for CNN layers on different
levels. The inputs of one OFF unit include the basic deep
features from two segments, and the feature from the OFF
unit on the previous feature level if it exists. In this way, the
OFF at the previous semantic level can be used for reﬁning
the OFF at the current semantic level.

Classiﬁcation Sub-network. The classiﬁcation sub-
network takes features from different sources and uses mul-
tiple inner-product classiﬁers to obtain multiple classiﬁca-
tion scores. The classiﬁcation scores of all sampled frames
are then combine by averaging for each feature generation
sub-network, or OFF sub-network. The OFF at a seman-
tic level can be used to produce a classiﬁcation score at the
training stage, which is learned using its corresponding loss.
Such strategy has been proved to be useful in many tasks
[34, 45, 22]. In the testing phase, scores from different sub-
networks could be assembled for better performance.

Figure 4. Detailed architecture of OFF unit. A 1x1 convolution
layer is connected to the input basic feature for dimension reduc-
tion. After that, we utilize the Sobel operator and element-wise
subtraction to calculate the spatial and temporal gradients respec-
tively. The combination of gradients constitutes the OFF, and the
sobel operator, subtracting operator and the 1 × 1 convolution lay-
ers before them constitute a OFF layer.

4.2. Network Training

Action recognition is treated as a multi-class classiﬁca-
tion problem. Followed by the settings in TSN, as there are
multiple classiﬁcation scores produced by each segment, we
need to fuse them all in each sub-network separately to gen-
erate a video-level score for loss calculation. Here, for the
OFF sub-networks, the features produced by the output of
OFF sub-network for the tth segment on level l is denoted
by Ft,l. The classiﬁcation score for segment t on the level
l using Ft,l is denoted by Gt,l. The aggregated video-level
score at level l is denoted by Gl. The video-level action
classiﬁcation score Gl is obtained by:

Gl = G(G0,l, . . . , G1,l, . . . , GNt−1−1,l),

(8)

where Nt denotes the number of frames for extracting fea-
tures. The aggregation function denoted by G is used for
summarizing the scores predicted from different segments
along time. Following the investigations in TSN, G is imple-
mented by average pooling for better performance [43]. As
for the feature generation sub-network, the above equations
are also applicable. While as we do not need intermediate
supervision for feature generation sub-network, the feature
Ft,l at level l for segment t is simply equivalent to the ﬁnal
feature output of the sub-network.

To update the parameters of the whole network, the loss
is set to be the standard categorical cross-entropy loss. As
the sub-network for each feature level is supervised inde-
pendently, a loss function is used for each level as:

Ll(y, Gl) = −

yc(Gl,c − log

eGl,j ).

(9)

C
(cid:88)

c=1

C
(cid:88)

j=1

5

where C is the number of action categories, Gl,c is the es-
timated score for class c from the features at level l, and yc
represents the ground-truth class label. By using this loss
function we can optimize the network parameters through
back-propagation. Detailed implementation of training is
described as follows.

Two-stage Training Strategy. Training of the whole
network consists of two stages. The ﬁrst stage indeed is
to apply existing approaches, e.g. TSN [43], to train the
feature generation sub-network. At the second stage, we
train the OFF and classiﬁcation sub-network with all the
weights in feature generation sub-network frozen. The
weights of OFF sub-network and classiﬁcation sub-network
are learned from scratch. The whole network could be fur-
ther ﬁne-tuned in an end-to-end manner, however, we do not
ﬁnd signiﬁcant gain in this stage. To simplify the training
process, we only train the network using the ﬁrst two stages.

Intermediate Supervision during Training. Interme-
diate supervision has been proven to be practical training
strategy in many other computer vision tasks [22, 45, 46,
24, 6]. As the OFF sub-networks are fed by intermediate
inputs, here we add the intermediate supervision on each
level to get better OFFs on each level of resolution.

Reducing the Memory Cost. As our framework con-
sists of several sub-networks, it costs more memory than the
original TSN framework, which extracts and stores motion
frames before training CNNs, and trains several networks
In order to reduce the computational and
independently.
memorial cost, we sample less frames in the training phase
than in the testing phase, and still obtain satisfactory results.

However, the time duration between segments may be
varied if we sample different number of segments between
training and testing. According to our deﬁnition in equation
3, only when the denotation ∆t is a ﬁxed constant, the equa-
tion 4 could be derived from the equation 3. If we sample
different frames between training and testing, the time in-
terval ∆t may be inconsistent, which makes our deﬁnition
to be invalid and inﬂuences the ﬁnal performance. In order
to keep time interval consistent between training and test-
ing, we design the sampling scheme carefully. Therefore,
during training, we sample frames from a video as follows:

Let α be the number of frames sampled for training, and
β be the number for testing. In training phase, a video with
length L, L >= β would be divided into β segments. Each
segment has length (cid:98)L/β(cid:99). We randomly select p from
0, 1, . . . , L − 1 − (α − 1) ∗ (cid:98)L/β(cid:99), where p is treated as a
frame seed. Then the whole training set is constructed as
{p, p + (cid:98)L/β(cid:99), ..., p + (α − 1) ∗ (cid:98)L/β(cid:99)}, which has interval
(cid:98)L/β(cid:99). In testing phase, we sample the images using the
same interval (cid:98)L/β(cid:99) as that in the training phase.

4.3. Network Testing

As there are multiple classiﬁcation scores produced by
different sub-networks, we need to fuse them all in test-
ing phase for better performance. In this study, we assem-
ble scores from the feature generation sub-network and the
last level of OFF sub-network by a simple summing opera-
tion. We select to test our model based on a state-of-the-art
framework TSN [43]. The testing setting under the TSN
framework is illustrated as follows:

Testing under TSN Framework. In the testing stage of
TSN, 25 segments are sampled from RGB, RGB difference,
and optical ﬂow. However, the number of frames in each
segment is different among these modalities. We use the
original settings adopted by TSN to sample 1, 5, 5 frames
per segment for RGB, RGB difference, and optical ﬂow re-
spectively. The input of our network is 25 segments, where
the tth segment is treated as the Frame t in Figure 3. In
this case, the features extracted by a separate branch of our
feature generation sub-network is for a segment instead of
a frame when using TSN. Other settings are kept to be the
same as those in TSN.

5. Experiments and Evaluations

In this section, datasets and implementation details used
in experiments will be ﬁrst introduced. Then we will ex-
plore the OFF and compare it with other modalities un-
der current state-of-the-art frameworks. Moreover, as our
method can be extended to other modalities such as RGB
difference and optical ﬂow, we will show how such a sim-
ple operation could improve the performance for input with
different modalities. Finally, we will discuss the meaning
and difference between the OFF and other motion modali-
ties such as optical ﬂow and RGB difference.

5.1. Datasets and Implementation Details

Evaluation Datasets. The experimental results are eval-
uated on two popular video action datasets, UCF-101 [31]
and HMDB-51 [21]. The UCF-101 dataset has 13320
videos and is divided into 101 classes, while the HMDB-
51 contains 6766 videos and 51 classes. Our experiments
follow the ofﬁcially offered scheme which divides a dataset
into 3 training and testing splits and ﬁnally calculating the
average accuracy over all 3 splits. We prepare the optical
ﬂow between frames before training by directly using the
OpenCV implemented algorithm [48].

Implementation Details. We train our model with 4
NVIDIA TITAN X GPU, under the implementation on
Caffe [18] and OpenMPI. We ﬁrst train the feature gener-
ation sub-networks using the same strategy provided in the
corresponding method [43]. Then at the second stage, we
train the OFF sub-networks from scratch with all param-
eters in the feature generation sub-networks frozen. The
mini-batch stochastic gradient descent algorithm is adopted

6

Method
TSN(RGB) [43]
TSN(RGB+RGB Diff) [43]
TSN(Flow) [43]
TSN(RGB+Flow) [43]
RGB+EMV-CNN [50]
MDI+RGB [3]
Two-Stream I3D
(RGB+Flow) [5]
RGB+OFF(RGB)+
RGB Diff+OFF(RGB Diff)

Speed (fps)
680
340
14
14
390
<131

<14

206

Acc.
85.5%
91.0%
87.9%
94.0%
86.4%
76.9%

93.4%

93.3%

Table 1. Experimental results of accuracy and efﬁciency for differ-
ent real-time video action recognition methods on UCF-101 over
three splits. Here the notation Flow represents the motion modal-
ity Optical Flow. Note that our OFF based algorithm could achieve
the state-of-the-art performance among real-time algorithms.

(cid:88)

(cid:88)

(cid:88)
(cid:88)

Acc.

Flow

RGB

(cid:88)
(cid:88)
(cid:88)

RGB
Diff

OFF
(Flow)

OFF
(RGB)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

OFF
(RGB Diff)

Speed
(fps)
680 85.5%
450 90.0%
340 90.7%
257 92.0%
206 93.0%
93.5%
14
95.1%
14
95.5%
14
Table 2. Experimental results for different modalities using the
OFF on UCF-101 Split1. Here Flow denotes the optical ﬂow.
OFF(*) denotes the use of OFF for the input *. For example,
OFF(RGB) denotes the use of OFF for RGB input. The speed
here illustrates the time cost for network forward. The results for
RGB and RGB + Flow are from [43]. The OFF(RGB) provides a
strong 4.5% improvement when fusing with RGB.

(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

here to learn the network parameters. When the feature gen-
eration sub-networks are fed by RGB frames, the whole
training procedure for OFF sub-network takes 20000 iter-
ations to converge with the learning rate initialized at 0.02
and decreased to its 0.1 using multi-step policy at the itera-
tion 10000, 15000 and 18000. When input changes to tem-
poral modality like optical ﬂow, the learning rate is initial-
ized at 0.05, and other policies are kept the same with what
have been proposed in RGB. The batch size is set to 128 and
all the training strategies described in previous sections are
applied. When evaluating on UCF-101 and HMDB-51, we
add dropout modules on spatial stream of OFF. There is no
difference on training parameters for different modalities.
However, when the input is RGB difference or optical ﬂow,
it would cost more time in both training and testing stages
as more frames are read into the network.

5.2. Experimental Investigations on OFF.

In this section, we will investigate the performance of
OFF under the TSN framework. The analysis for the per-
formance of single and multiple modalities, and the per-
formance comparison between the state-of-the-art will be
shown. All the results for OFF based networks are trained
with the same network backbone and strategies illustrated
in previous sections for fair comparison.

Efﬁciency Evaluation.

In this experiment, we evalu-
ate the efﬁciency between the OFF based method and other
state-of-the-art methods. The experimental results for ef-
ﬁciency and accuracy for different algorithms are summa-
rized in Table 1. OFF(RGB) denotes our use of OFF for
the network with RGB input, in this case, the OFF is ac-
quired from spatial deep features. As one special case, the
denotation RGB Diff represents the OFF calculated directly
from consecutive RGB frames on the input level instead of
on the feature level. After applying the OFF calculation

to RGB frames, the processed inputs could be fed into the
feature generation sub-network and the generated feature
maps could be again used to calculate their corresponding
OFF features on the feature level. The other methods we
compared here includes TSN [43] with different inputs, mo-
tion vector based RGB+EMV-CNN [50], dynamic image
based CNN [3] and current state-of-the-art 3D-CNN with
two stream [5]. From the Table 1, by applying the OFF to
the spatial features and the RGB inputs, we can achieve a
competitive accuracy 93.3% with only RGB inputs on the
UCF-101 over three splits, which is even comparable with
some Two-Stream based methods such as [5, 43]. Besides,
our methods is still very efﬁcient under this kind of set-
tings. The whole network could run over 200 fps, while
other methods listed here are either inefﬁcient or not so ef-
fective as the Two-Stream based approaches.

Effectiveness Evaluation. In this part, we try to investi-
gate the robustness of OFF when applying to different kinds
of input. According to the deﬁnition in equation 4, we can
replace the image I from RGB image to optical ﬂow or
RGB difference image to extract OFF on feature level for
further experiments. Based on the scores predicted by dif-
ferent modalities, we can further improve the classiﬁcation
performance by fusing them together [29, 9, 43, 50]. We
carry out the experimental results with various score fusing
schemes on UCF-101 split 1, and summarize them in Table
2. Table 2 shows the results when different kinds of modal-
ities are introduced as the network input. From each block
separated by a horizon line, we can ﬁnd that the OFF is
complementary to other kinds of modalities, e.g. RGB and
optical ﬂow, and could get a remarkable gain every time the
OFF is introduced. Besides, interestingly, the OFF is still
working when the input modality is already describing the
motion information. This phenomenon indicates that the
acceleration information between frames might also make

7

RGB Hyp-Net + RGB OFF(RGB) + RGB
85.5%

86.0%

90.0%

Acc.

Table 3. Experimental results of accuracy for hypercolumn net-
work and the comparison with OFF on UCF-101 Split1. The de-
notation ”Hyp-Net” indicates the output of hypercolumn network.

a difference in describing the temporal patterns.

Comparison with the Hypercolumns CNN. As our net-
work extracts intermediate deep features from a pre-trained
CNN, such hypercolumn based network structure may lead
to additional gain on speciﬁc datasets [14]. Experiment and
analysis are conducted to investigate whether the OFF is
playing a key role for the improvement. The network archi-
tecture and all training strategies for the hypercolumn CNN
are the same as that in OFF except for the removal of OFF
unit, in other words, the hypercolumn network here is con-
structed as the same structure of OFF sub-network without
OFF unit. In this case, the features from feature generation
sub-networks are directly fed into the OFF sub-networks
without the calculation of OFF.

From the experimental results shown in Table 3, it is
clear that, despite the hypercolumn network could get a
slight 0.5% improvement on UCF-101 split 1, its ﬁnal ac-
curacy is still apparently less than the one obtained by
OFF(RGB). Therefore, a conclusion could be drawn that
it is the OFF calculation rather than the hypercolumn struc-
ture that plays the key role in achieving the signiﬁcant gain.
Comparison with the State-of-the-art. Above all, af-
ter the exploration and analysis of the OFF, we show our
ﬁnal result. As what has been done in TSN, we also assem-
ble the classiﬁcation scores obtained by different kinds of
modalities. We sum the scores produced by each modality
together, and get the ﬁnal version output in Table 4. All the
results are evaluated in the UCF-101 and HMDB-51 over
3 splits. Our results are obtained by assembling the scores
from RGB, OFF(RGB), optical ﬂow and their correspond-
ing version of OFF(optical ﬂow) together. When we add
one more score from OFF(RGB Diff), a slight 0.3% gain
is obtained compared to the version without it, and ﬁnally
results in 96.0% on UCF-101 and 74.2% on HMDB-51.
Note that we do not introduce improved Dense Trajectories
(iDT)[40] into our network as the input. The components of
inputs we need to prepare in advance for our ﬁnal version
result only consist of RGB and optical ﬂow.

We compare our result with both the traditional ap-
proaches and deep learning based approaches. We obtain
2.0%/5.7% gain compared with the baseline Two-Stream
TSN [43] on UCF-101 [31] and HMDB-51 [21] respec-
tively. Note that the ﬁnal version TSN takes 3 modalities
(RGB, Optical Flow and iDT) as network input. The other
compared methods listed in Table 4 include iDT [40], Two-
Stream ConvNet [29], Two-Stream + LSTM [47], Tempo-
ral Deep-convolutional Descriptors (TDD) [41], Long-term

Method

iDT [40]
Two-Stream [29]
Two-Stream TSN [43]
Three-Stream TSN [43]
Two-Stream+LSTM [47]
TDD+iDT [41]
LTC+iDT [37]
KVMDF [52]
STP [44]
STMN+iDT [12]
ST-VLMPF+iDT [7]
L2STM [32]
Two-Stream I3D [5]
Two-Stream I3D
(with Kinetics 300k) [5]
Ours

UCF-101
86.4%
88.0%
94.0%
94.2%
88.6%
91.5%
91.7%
93.1%
94.6%
94.9%
94.3%
93.6%
93.4%

98.0%

96.0%

HMDB-51
61.7%
59.4%
68.5%
69.4%
-%
65.9%
64.8%
63.3%
68.9%
72.2%
73.1%
66.2%
66.4%

80.7%

74.2%

Table 4. Performance comparison to the state-of-the-art methods
on UCF-101 and HMDB-51 over 3 splits.

Temporal Convolutions (LTC) [37], Key Volume Mining
Deep Framework (KVMDF) [52], and also the current state-
of-the-art methods such as Spatio-Temporal Pyramid (STP)
[44], Saptio-Temporal Multiplier Network (STMN) [12],
Spatio-Temporal Vector [7], Lattice LSTM (L2STM) [32],
and I3D [5]. The method I3D could achieve spectacular per-
formance (98.0% on UCF-101, 80.7% on HMDB-51, over
3 splits) when proposing a new large dataset Kinetics for
pre-train. While without the pre-training, the method I3D
could achieve 93.4% on UCF-101 Split1. From the com-
parison with all the listed methods, we conclude that our
OFF based method allow for state-of-the-art performance
in video action recognition.

6. Conclusion

In this paper, we have presented Optical Flow guided
Feature (OFF), a novel motion representation derived from
and guided by the optical ﬂow. OFF is both fast and ro-
bust. By plugging the OFF into CNN framework, the result
with only RGB as input on UCF-101 is even comparable
to the result obtained by Two-Stream (RGB+Optical Flow)
approaches, and at the same time, the OFF plugged network
is still very efﬁcient with the speed over 200 frames per sec-
ond. Besides, it has been proven that the OFF is still com-
plementary to other motion representations like optical ﬂow.
Based on this representation, we proposed an new CNN ar-
chitecture for video action recognition. This architecture
outperforms many other state-of-the-art video action recog-
nition methods on two popular video datasets UCF-101 and
HMDB-51, and could be used to accelerate the speed of the
video based tasks.
In future works, we will validate our
method on other video based tasks and datasets.

8

References

[1] J. L. Barron, D. J. Fleet, and S. S. Beauchemin. Performance

of optical ﬂow techniques. IJCV, 12(1):43–77, 1994.

[2] J. Bigun, G. H. Granlund, and J. Wiklund. Multidimensional
orientation estimation with applications to texture analysis
and optical ﬂow. T-PAMI, 13(8):775–790, 1991.

[3] H. Bilen, B. Fernando, E. Gavves, A. Vedaldi, and S. Gould.
Dynamic image networks for action recognition. In CVPR,
pages 3034–3042, 2016.

[4] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert. High ac-
curacy optical ﬂow estimation based on a theory for warping.
In ECCV, pages 25–36. Springer, 2004.

[5] J. Carreira and A. Zisserman. Quo Vadis, Action Recogni-
tion? A New Model and the Kinetics Dataset. arXiv preprint
arXiv:1705.07750, 2017.

[6] X. Chu, W. Yang, W. Ouyang, C. Ma, A. L. Yuille, and
X. Wang. Multi-context attention for human pose estima-
tion. In CVPR, July 2017.

[7] I. Cosmin Duta, B. Ionescu, K. Aizawa, and N. Sebe. Spatio-
Temporal Vector of Locally Max Pooled Features for Action
Recognition in Videos. In CVPR, pages 3097–3106, 2017.
[8] N. Dalal, B. Triggs, C. Schmid, N. Dalal, B. Triggs,
C. Schmid, H. Detection, and U. Oriented. Human Detec-
tion Using Oriented Histograms of Flow and Appearance. In
ECCV, pages 428–441, 2006.

[9] A. Diba, A. M. Pazandeh, and L. Van Gool. Efﬁcient two-
stream motion and appearance 3d cnns for video classiﬁca-
tion. arXiv preprint arXiv:1608.08851, 2016.

[10] A. Diba, V. Sharma, and L. Van Gool. Deep temporal linear
encoding networks. arXiv preprint arXiv:1611.06678, 2016.
[11] C. Feichtenhofer, A. Pinz, and R. Wildes. Spatiotemporal
In NIPS,

residual networks for video action recognition.
pages 3468–3476, 2016.

[12] C. Feichtenhofer, A. Pinz, and R. P. Wildes. Spatiotemporal
multiplier networks for video action recognition. In CVPR,
2017.

[13] B. Fernando, E. Gavves, J. Oramas, A. Ghodrati, and
T. Tuytelaars. Rank pooling for action recognition. T-PAMI,
39(4):773–787, 2017.

[14] B. Hariharan, P. Arbel´aez, R. Girshick, and J. Malik. Hyper-
columns for object segmentation and ﬁne-grained localiza-
tion. In CVPR, pages 447–456, 2015.

[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In CVPR, pages 770–778, 2016.
[16] B. G. Horn, Berthold K.P.; Schunck. Determining Optical

Flow. Artiﬁcial Intelligence, 17:185–203, 1981.

[17] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, pages 448–456, 2015.

[18] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
Architecture for Fast Feature Embedding. arXiv preprint
arXiv:1408.5093, 2014.

[19] A. Klaser, M. Marszalek, and C. Schmid. A Spatio-Temporal
Descriptor Based on 3D-Gradients. In BMVC, pages 275–1,
2008.

[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, pages 1097–1105, 2012.

Imagenet
In

[21] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre.
HMDB: A large video database for human motion recogni-
tion. In ICCV, pages 2556–2563, 2011.

[22] A. Newell, K. Yang, and J. Deng. Stacked Hourglass Net-
In ECCV, pages 483–

works for Human Pose Estimation.
499. Springer, 2016.

[23] J. Y.-H. Ng, J. Choi, J. Neumann, and L. S. Davis. Ac-
tionFlowNet: Learning Motion Representation for Action
Recognition. arXiv preprint arXiv:1612.03052, 2016.
[24] W. Ouyang, K. Wang, X. Zhu, and X. Wang. Chained cas-
cade network for object detection. In ICCV, Oct 2017.
[25] W. Ouyang, X. Zeng, and X. Wang. Learning mutual visibil-
ity relationship for pedestrian detection with a deep model.
IJCV, 120(1):14–27, 2016.

[26] X. Peng, L. Wang, X. Wang, and Y. Qiao. Bag of Visual
Words and Fusion Methods for Action Recognition: Com-
prehensive Study and Good Practice. Computer Vision and
Image Understanding, 150:109–125, 2016.

[27] P. Scovanner, S. Ali, and M. Shah. A 3-dimensional sift
In
descriptor and its application to action recognition.
ACM’MM, pages 357–360, 2007.

[28] Y. Shi, Y. Tian, Y. Wang, W. Zeng, and T. Huang. Learn-
ing long-term dependencies for action recognition with a
In CVPR, pages 716–
biologically-inspired deep network.
725, 2017.

[29] K. Simonyan and A. Zisserman. Two-stream convolutional
In NIPS, pages

networks for action recognition in videos.
568–576, 2014.

[30] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014.

[31] K. Soomro, A. R. Zamir, and M. Shah. UCF101: A Dataset
of 101 Human Actions Classes From Videos in The Wild.
arXiv preprint arXiv:1212.0402, 2012.

[32] L. Sun, K. Jia, K. Chen, D. Y. Yeung, B. E. Shi, and
S. Savarese. Lattice Long Short-Term Memory for Hu-
man Action Recognition. arXiv preprint arXiv:1708.03958,
2017.

[33] L. Sun, K. Jia, D.-Y. Yeung, and B. E. Shi. Human action
recognition using factorized spatio-temporal convolutional
networks. In ICCV, pages 4597–4605, 2015.

[34] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
In CVPR, pages 1–9,
Going Deeper with Convolutions.
2015.

[35] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri.
Learning spatiotemporal features with 3d convolutional net-
works. In ICCV, pages 4489–4497, 2015.

[36] D. Tran, J. Ray, Z. Shou, S.-F. Chang, and M. Paluri. Con-
vNet Architecture Search for Spatiotemporal Feature Learn-
ing. arXiv preprint arXiv:1708.05038, 2017.

[37] G. Varol, I. Laptev, and C. Schmid. Long-term tempo-
arXiv preprint

ral convolutions for action recognition.
arXiv:1604.04494, 2016.

9

[38] G. Varol, I. Laptev, and C. Schmid. Long-term temporal

convolutions for action recognition. T-PAMI, 2017.

[39] H. Wang, A. Klaser, C. Schmid, and C.-L. Liu. Action recog-
In CVPR, pages 3169–3176.

nition by dense trajectories.
IEEE, 2011.

[40] H. Wang and C. Schmid. Action Recognition with Improved

Trajectories. In ICCV, pages 3551–3558, 2013.

[41] L. Wang, Y. Qiao, and X. Tang. Action recognition with
trajectory-pooled deep-convolutional descriptors. In ICCV,
pages 4305–4314, 2015.

[42] L. Wang, Y. Xiong, Z. Wang, and Y. Qiao. Towards good
practices for very deep two-stream convnets. arXiv preprint
arXiv:1507.02159, 2015.

[43] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and
L. Van Gool. Temporal segment networks: Towards good
practices for deep action recognition. ECCV, pages 20–36,
2016.

[44] Y. Wang, M. Long, J. Wang, and P. S. Yu. Spatiotemporal
Pyramid Network for Video Action Recognition. In CVPR,
pages 1529–1538, 2017.

[45] S.-E. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh. Con-
In CVPR, pages 4724–4732,

volutional pose machines.
2016.

[46] W. Yang, S. Li, W. Ouyang, H. Li, and X. Wang. Learning
feature pyramids for human pose estimation. In ICCV, Oct
2017.

[47] J. Yue-Hei Ng, M. Hausknecht, S. Vijayanarasimhan,
O. Vinyals, R. Monga, and G. Toderici. Beyond short snip-
pets: Deep networks for video classiﬁcation. In CVPR, pages
4694–4702, 2015.

[48] C. Zach, T. Pock, and H. Bischof. A duality based approach
for realtime TV-L1 optical ﬂow. Pattern Recognition, pages
214–223, 2007.

[49] X. Zeng, W. Ouyang, J. Yan, H. Li, T. Xiao, K. Wang, Y. Liu,
Y. Zhou, B. Yang, Z. Wang, et al. Crafting gbd-net for object
detection. T-PAMI, 2017.

[50] B. Zhang, L. Wang, Z. Wang, Y. Qiao, and H. Wang. Real-
time action recognition with enhanced motion vector CNNs.
In CVPR, pages 2718–2726, 2016.

[51] H. Zhao, M. Tian, S. Sun, J. Shao, J. Yan, S. Yi, X. Wang,
and X. Tang. Spindle net: Person re-identiﬁcation with hu-
man body region guided feature decomposition and fusion.
In CVPR, July 2017.

[52] W. Zhu, J. Hu, G. Sun, X. Cao, and Y. Qiao. A key volume
In CVPR,

mining deep framework for action recognition.
pages 1991–1999, 2016.

10

