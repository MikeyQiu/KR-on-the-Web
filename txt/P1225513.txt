Interpreting Neural Networks With Nearest Neighbors

Eric Wallace∗, Shi Feng∗, Jordan Boyd-Graber
University of Maryland
{ewallac2,shifeng,jbg}@umiacs.umd.edu

8
1
0
2
 
v
o
N
 
7
 
 
]
L
C
.
s
c
[
 
 
2
v
7
4
8
2
0
.
9
0
8
1
:
v
i
X
r
a

Abstract

Local model interpretation methods explain
individual predictions by assigning an impor-
tance value to each input feature. This value is
often determined by measuring the change in
conﬁdence when a feature is removed. How-
ever, the conﬁdence of neural networks is not a
robust measure of model uncertainty. This is-
sue makes reliably judging the importance of
the input features difﬁcult. We address this by
changing the test-time behavior of neural net-
works using Deep k-Nearest Neighbors. With-
out harming text classiﬁcation accuracy, this
algorithm provides a more robust uncertainty
metric which we use to generate feature im-
portance values. The resulting interpretations
better align with human perception than base-
line methods. Finally, we use our interpreta-
tion method to analyze model predictions on
dataset annotation artifacts.

1

Introduction

The growing use of neural networks in sensitive
domains such as medicine, ﬁnance, and security
raises concerns about human trust in these ma-
chine learning systems. A central question is test-
time interpretability: how can humans understand
the reasoning behind model predictions?

A common way to interpret neural network pre-
dictions is to identify the most important input fea-
tures. For instance, a saliency map that highlights
important pixels in an image (Sundararajan et al.,
2017) or words in a sentence (Li et al., 2016).
Given a test prediction, the importance of each
input feature is the change in model conﬁdence
when that feature is removed.

However, neural network conﬁdence is not a
proper measure of model uncertainty (Guo et al.,
2017). This issue is emphasized when models
make highly conﬁdent predictions on inputs that

are completely void of information, for example,
images of pure noise (Goodfellow et al., 2015)
or meaningless text snippets (Feng et al., 2018).
Consequently, a model’s conﬁdence may not prop-
erly reﬂect whether discriminative input features
are present. This issue makes it difﬁcult to re-
liably judge the importance of each input fea-
ture using common conﬁdence-based interpreta-
tion methods (Feng et al., 2018).

To address this, we apply Deep k-Nearest
Neighbors (DKNN)
(Papernot and McDaniel,
2018) to neural models for text classiﬁcation.
Concretely, predictions are no longer made with a
softmax classiﬁer, but using the labels of the train-
ing examples whose representations are most sim-
ilar to the test example (Section 3). This provides
an alternative metric for model uncertainty, con-
formity, which measures how much support a test
prediction has by comparing its hidden represen-
tations to the training data. This representation-
based uncertainty measurement can be used in
combination with existing interpretation methods,
such as leave-one-out (Li et al., 2016), to better
identify important input features.

We combine DKNN with CNN and LSTM
models on six NLP text classiﬁcation tasks, includ-
ing sentiment analysis and textual entailment, with
no loss in classiﬁcation accuracy (Section 4). We
compare interpretations generated using DKNN
conformity to baseline interpretation methods,
ﬁnding DKNN interpretations rarely assign im-
portance to extraneous words that do not align
with human perception (Section 5). Finally, we
generate interpretations using DKNN conformity
for a dataset with known artifacts (SNLI), helping
to indicate whether a model has learned superﬁcial
patterns. We open source the code for DKNN and
our results.1

∗(cid:63)Equal contribution

1https://github.com/Eric-Wallace/deep-knn

2

Interpretation Through Feature
Attribution

Feature attribution methods explain a test predic-
tion by assigning an importance value to each in-
put feature (typically pixels or words).

In the case of text classiﬁcation, we have an in-
put sequence of n words x = (cid:104)w1, w2, . . . wn(cid:105),
represented as one-hot vectors. The word se-
quence is then converted to a sequence of word
embeddings e = (cid:104)v1, v2, . . . vn(cid:105). A classiﬁer
f outputs a probability distribution over classes.
The class with the highest probability is selected
as the prediction y, with its probability serving as
the model conﬁdence. To create an interpretation,
each input word is assigned an importance value,
| x, y), which indicates the word’s contri-
g(wi
bution to the prediction. A saliency map (or heat
map) visually highlights the words in a sentence.

2.1 Leave-one-out Attribution

A simple way to deﬁne the importance g is via
leave-one-out (Li et al., 2016):
individually re-
move a word from the input and see how the con-
ﬁdence changes. The importance of word wi is the
decrease in conﬁdence2 when word i is removed:

g(wi | x, y) = f (y | x) − f (y | x−i),

(1)

where x−i is the input sequence with the ith word
removed and f (y | x) is the model conﬁdence for
class y. This can be repeated for all words in the
input. Under this deﬁnition, the sign of the impor-
tance value is opposite the sign of the conﬁdence
change: if a word’s removal causes a decrease in
the conﬁdence, it gets a positive importance value.
We refer to this interpretation method as Conﬁ-
dence leave-one-out in our experiments.

2.2 Gradient-Based Feature Attribution

In the case of neural networks, the model f (x) as
a function of word wi is a highly non-linear, dif-
ferentiable function. Rather than leaving one word
out at a time, we can simulate a word’s removal by
approximating f with a function that is linear in wi
through the ﬁrst-order Taylor expansion. The im-
portance of wi is computed as the derivative of f
with respect to the one-hot vector:

∂f
∂wi

=

∂f
∂vi

∂vi
∂wi

=

∂f
∂vi

· vi

(2)

2equivalently the change in class score or cross entropy

loss

Thus, a word’s importance is the dot product be-
tween the gradient of the class prediction with re-
spect to the embedding and the word embedding
itself. This gradient approximation simulates the
change in conﬁdence when an input word is re-
moved and has been used in various interpreta-
tion methods for NLP (Arras et al., 2016; Ebrahimi
et al., 2017). We refer to this interpretation ap-
proach as Gradient in our experiments.

2.3

Interpretation Method Failures

Interpreting neural networks can have unexpected
negative results. Ghorbani et al. (2017) and Kin-
dermans et al. (2017) show how a lack of model
robustness and stability can cause egregious in-
terpretation failures in computer vision settings.
Feng et al. (2018) extend this to NLP and draw con-
nections between interpretation failures and adver-
sarial examples (Szegedy et al., 2014). To counter-
act this, new interpretation methods alone are not
enough—models must be improved. For instance,
Feng et al. (2018) argue that interpretation meth-
ods should not rely on prediction conﬁdence as it
does not reﬂect a model’s uncertainty.

Following this, we improve interpretations by
replacing the softmax conﬁdence with a more ro-
bust uncertainty estimate using DKNN (Papernot
and McDaniel, 2018). This algorithm maintains
the accuracy of standard image classiﬁcation mod-
els while providing a better uncertainty metric ca-
pable of defending against adversarial examples.

3 Deep k-Nearest Neighbors for

Sequential Inputs

This section describes Deep k-Nearest Neighbors,
its application to sequential inputs, and how we
use it to determine word importance values.

3.1 Deep k-Nearest Neighbors

Papernot and McDaniel (2018) propose Deep k-
Nearest Neighbors (DKNN), a modiﬁcation to the
test-time behavior of neural networks.

After training completes, the DKNN algorithm
passes every training example through the model
and saves each of the layer’s representations. This
creates a new dataset, whose features are the rep-
resentations and whose labels are the model pre-
dictions. Test-time predictions are made by pass-
ing an example through the model and performing
k-nearest neighbors classiﬁcation on the resulting
representations. This modiﬁcation does not de-

For our purposes,

grade the accuracy of image classiﬁers on several
standard datasets (Papernot and McDaniel, 2018).
the beneﬁt of DKNN is
the algorithm’s uncertainty metric, the conformity
score. This score is the percentage of nearest
neighbors belonging to the predicted class. Con-
formity follows from the framework of conformal
prediction (Shafer and Vovk, 2008) and estimates
how much the training data supports a classiﬁca-
tion decision.

The conformity score uses the representations
at each neural network layer, and therefore, a pre-
diction only receives high conformity if it largely
agrees with the training data at all representation
levels. This mechanism defends against adversar-
ial examples (Szegedy et al., 2014), as it is difﬁ-
cult to construct a perturbation which changes the
neighbors at every layer. Consequently, confor-
mity is a better uncertainty metric for both regu-
lar examples and out-of-domain examples such as
noisy or adversarial inputs, making it suitable for
interpreting models.

3.2 Handling Sequences

The DKNN algorithm requires ﬁxed-size vector
representations. To reach a ﬁxed-size representa-
tion for text classiﬁcation, we take either the ﬁnal
hidden state of a recurrent neural network or use
max pooling across time (Collobert and Weston,
2008). We consider deep architectures of these
two forms, using each of the layers’ representa-
tions as the features for DKNN.

3.3 Conformity leave-one-out

Using conformity, we generate interpretations
through a modiﬁed version of leave-one-out (Li
et al., 2016). After removing a word, rather than
observing the drop in conﬁdence, we instead mea-
sure the drop in conformity. Formally, we modify
classiﬁer f in Equation 1 to output probabilities
based on conformity. We refer to this method as
conformity leave-one-out.

4 DKNN Maintains Classiﬁcation

Accuracy

Interpretability should not come at
the cost
of performance—before investigating how inter-
pretable DKNN is, we ﬁrst evaluate its accuracy.
We experiment with six text classiﬁcation tasks
and two models, verifying that DKNN achieves
accuracy comparable to regular classiﬁers.

4.1 Datasets and Models

We consider six common text classiﬁcation tasks:
binary sentiment analysis using Stanford Senti-
ment Treebank (Socher et al., 2013, SST) and Cus-
tomer Reviews (Hu and Liu, 2004, CR), topic clas-
siﬁcation using TREC (Li and Roth, 2002), opin-
ion polarity (Wiebe et al., 2005, MPQA), and sub-
jectivity/objectivity (Pang and Lee, 2004, SUBJ).
Additionally, we consider natural language infer-
ence with SNLI (Bowman et al., 2015). We exper-
iment with BILSTM and CNN models.

CNN Our CNN architecture resembles Kim
(2014). We use convolutional ﬁlters of size three,
four, and ﬁve, with max-pooling over time (Col-
lobert and Weston, 2008). The ﬁlters are followed
by three fully-connected layers. We ﬁne-tune
GLOVE embeddings (Pennington et al., 2014) of
each word. For DKNN, we use the activations
from the convolution layer and the three fully-
connected layers.

BILSTM Our architecture uses a bidirectional
LSTM (Graves and Schmidhuber, 2005), with the
ﬁnal hidden state forming the ﬁxed-size represen-
tation. We use three LSTM layers, followed by
two fully-connected layers. We ﬁne-tune GLOVE
embeddings of each word. For DKNN, we use the
ﬁnal activations of the three recurrent layers and
the two fully-connected layers.

SNLI Classiﬁer Unlike the other tasks which
consist of a single input sentence, SNLI has two
inputs, a premise and hypothesis.
Following
Conneau et al. (2017), we use the same model
to encode the two inputs, generating representa-
tions u for the premise and v for the hypoth-
esis. We concatenate these two representations
along with their dot-product and element-wise ab-
solute difference, arriving at a ﬁnal representation
[u; v; u ∗ v; |u − v|]. This vector passes through
two fully-connected layers for classiﬁcation. For
DKNN, we use the activations of the two fully-
connected layers.

Nearest Neighbor Search For accurate inter-
pretations, we trade efﬁciency for accuracy and
replace locally sensitive hashing (Gionis et al.,
1999) used by Papernot and McDaniel (2018) with
a k-d tree (Bentley, 1975). We use k = 75 nearest
neighbors at each layer. The empirical results are
robust to the choice of k.

4.2 Classiﬁcation Results

DKNN achieves comparable accuracy on the ﬁve
classiﬁcation tasks (Table 1). On SNLI, the BIL-
STM achieves an accuracy of 81.2% with a soft-
max classiﬁer and 81.0% with DKNN.

5 DKNN is Interpretable

Following past work (Li et al., 2016; Murdoch
et al., 2018), we focus on the SST dataset for
generating interpretations. Due to the lack of
standard interpretation evaluation metrics (Doshi-
Velez and Kim, 2017), we use qualitative evalu-
ations (Smilkov et al., 2017; Sundararajan et al.,
2017; Li et al., 2016), performing quantitative ex-
periments where possible to examine the distinc-
tion between the interpretation methods.

5.1

Interpretation Analysis

We compare our method (Conformity leave-one-
out) against two baselines:
leave-one-out using
regular conﬁdence (Conﬁdence leave-one-out, see
Section 2.1) and the gradient with respect to the in-
put (Gradient, see Section 2.2). To create saliency
maps, we normalize each word’s importance by
dividing it by the total importance of the words
in the sentence. We display unknown words in an-
gle brackets <>. Table 2 shows SST interpretation
examples for the BILSTM model and further ex-
amples are shown on a supplementary website.3

Conformity leave-one-out assigns concentrated
importance values to a small number of input
words.
In contrast, the baseline methods assign
non-zero importance values to numerous words,
many of which are irrelevant. For instance, in all
three examples of Table 2, both baselines highlight
almost half of the input, including words such as
“ﬁction” and “clash”. We suspect model conﬁ-
dence is oversensitive to these unimportant input
changes, causing the baseline interpretations to
highlight unimportant words. On the other hand,
the conformity score better separates word impor-
tance, generating clearer interpretations.

The tendency for conﬁdence-based approaches
to assign importance to many words holds for the
entire test set. We compute the average number
of highlighted words using a threshold of 0.05 (a
normalized importance value corresponding to a
light blue or light red highlight). Out of the av-
erage 20.23 words in SST test set, gradient high-

3https://sites.google.com/view/

language-dknn/

lights 5.32 words, conﬁdence leave-one-out high-
lights 5.79 words, and conformity leave-one-out
highlights 3.65 words.

The second,

and related, observation for
conﬁdence-based approaches is a bias towards se-
lecting word importance based on a word’s inher-
ent sentiment, rather than its meaning in context.
For example, see “clash”, “terribly”, and “unfaith-
ful” in Table 2. The removal of these words causes
a small change in the model conﬁdence. When us-
ing DKNN, the conformity score indicates that the
model’s uncertainty has not risen without these in-
put words and leave-one-out does not assign them
any importance.

We characterize our interpretation method as
signiﬁcantly higher precision, but slightly lower
recall than conﬁdence-based methods. Confor-
mity leave-one-out rarely assigns high importance
to words that do not align with human perception
of sentiment. However, there are cases when our
method does not assign signiﬁcant importance to
any word. This occurs when the input has a high
redundancy. For example, a positive movie re-
view that describes the sentiment in four distinct
ways. In these cases, leaving out a single senti-
ment word has little effect on the conformity as the
model’s representation remains supported by the
other redundant features. Conﬁdence-based inter-
pretations, which interpret models using the linear
units that produce class scores, achieve higher re-
call by responding to every change in the input for
a certain direction but may have lower precision.

In the second example of Table 2, the word “ter-
ribly” is assigned a negative importance value, dis-
regarding its positive meaning in context. To ex-
amine if this is a stand-alone example or a more
general pattern of uninterpretable behavior, we
calculate the importance value of the word “ter-
ribly” in other positive examples. For each occur-
rence of the word “great” in positive validation ex-
amples, we paraphrase it to “awesome”, “wonder-
ful”, or “impressive”, and add the word “terribly”
in front of it. This process yields 66 examples.
For each of these examples, we compute the im-
portance value of each input word and rank them
from most negative to most positive (the most neg-
ative word has a rank of 1). We compare the av-
erage ranking of “terribly” from the three meth-
ods: 7.9 from conformity leave-one-out, 1.68 from
conﬁdence leave-one-out, and 1.1 from gradient.
The baseline methods consistently rank “terribly”

SST

CR

TREC MPQA SUBJ

LSTM
86.7
LSTM DKNN 86.6
85.7
CNN
85.8
CNN DKNN

82.7
82.5
83.3
83.4

91.5
91.3
92.8
92.4

88.9
88.6
89.1
88.7

94.8
94.9
93.5
93.1

Table 1: Replacing a neural network’s softmax classiﬁer with DKNN maintains classiﬁcation accuracy
on standard text classiﬁcation tasks.

Method

Saliency Map

Conformity
Conﬁdence
Gradient

an intelligent ﬁction about learning through cultural clash.
an intelligent ﬁction about learning through cultural clash.
an intelligent ﬁction about learning through cultural clash.

Conformity <Schweiger> is talented and terribly charismatic.
Conﬁdence <Schweiger> is talented and terribly charismatic.
<Schweiger> is talented and terribly charismatic.
Gradient

Conformity Diane Lane shines in unfaithful.
Conﬁdence Diane Lane shines in unfaithful.
Gradient
Diane Lane shines in unfaithful.

Color Legend Positive Impact Negative Impact

Table 2: Comparison of interpretation approaches on SST test examples for the BILSTM model. Blue
indicates positive impact and red indicates negative impact. Our method (Conformity leave-one-out) has
higher precision, rarely assigning importance to extraneous words such as “clash” or “ﬁction”.

as the most negative word, ignoring its meaning in
context. This echoes our suspicion: DKNN gener-
ates interpretations with higher precision because
conformity is robust to irrelevant input changes.

5.2 Analyzing Dataset Annotation Artifacts

We use conformity leave-one-out to interpret a
model trained on SNLI, a dataset known to contain
annotation artifacts. We demonstrate that our in-
terpretation method can help identify when mod-
els exploit dataset biases.

Recent studies (Gururangan et al., 2018; Po-
liak et al., 2018) identify annotation artifacts in
SNLI. Superﬁcial patterns exist in the input which
strongly correlate with certain labels, making it
possible for models to “game” the task: obtain
high accuracy without true understanding. For in-
stance, the hypothesis of an entailment example
is often a general paraphrase of the premise, us-
ing words such as “outside” instead of “playing
in a park”. Contradiction examples often contain
negation words or non-action verbs like “sleep-
ing”. Models trained solely on the hypothesis can

learn these patterns and reach accuracies consider-
ably higher than the majority baseline.

These studies indicate that the SNLI task can be
gamed. We look to conﬁrm that some artifacts are
indeed exploited by normally trained models that
use full input pairs. We create saliency maps for
examples in the validation set using conformity
leave-one-out. Table 3 shows samples and more
can be found on the supplementary website. We
use blue highlights to indicate words which posi-
tively support the model’s predicted class, and the
color red to indicate words that support a differ-
ent class. The ﬁrst example is a randomly sam-
pled baseline, showing how the words “swims”
and “pool” support the model’s prediction of con-
tradiction. The other examples are selected be-
cause they contain terms identiﬁed as artifacts. In
the second example, conformity leave-one-out as-
signs extremely high word importance to “sleep-
ing”, disregarding the other words necessary to
predict contradiction (i.e., the neutral class is still
possible if “pets” is replaced with “people”). In
the ﬁnal two hypotheses, the interpretation method

diagnoses the model failure, assigning high impor-
tance to “wearing”, rather than focusing positively
on the shirt color.

dence predictions produced on inputs that are ad-
versarial (Szegedy et al., 2014) or contain solely
noise (Goodfellow et al., 2015).

To explore this further, we analyze the hypothe-
ses in each SNLI class which contain a top ﬁve ar-
tifact identiﬁed by Gururangan et al. (2018). For
each of these examples, we compute the impor-
tance value for each input word using both conﬁ-
dence and conformity leave-one-out. We then rank
the words from most important for the prediction
to least important (a score of 1 indicates highest
importance) and report the average rank for the
artifacts in Table 4. We sort the words by their
Pointwise Mutual Information with the correct la-
bel as provided by Gururangan et al. (2018). The
it is the
word “nobody” particularly stands out:
most important input word every time it appears
in a contradiction example.

For most of the artifacts, conformity leave-one-
out assigns them a high importance, often rank-
ing the artifacts as the most important input word.
Conﬁdence leave-one-out correlates less strongly
with the known artifacts, frequently ranking them
as low as the ﬁfth or sixth most important word.
Given the high correlation between conformity
leave-one-out and the manually identiﬁed arti-
facts, this interpretation method may serve as a
technique to identify undesirable biases a model
has learned.

6 Discussion and Related Work

We connect the improvements made by confor-
mity leave-one-out to model conﬁdence issues,
compare alternative interpretation improvements,
and discuss further features of DKNN.

6.1

Issues in Neural Network Conﬁdence

Many existing feature attribution methods rely on
estimates of model uncertainty: both input gra-
dient and conﬁdence leave-one-out rely on pre-
diction conﬁdence, our method relies on DKNN
conformity.
Interpretation quality is thus deter-
mined by reliable uncertainty estimation. For in-
stance, past work shows relying on neural network
conﬁdence can lead to unreasonable interpreta-
tions (Kindermans et al., 2017; Ghorbani et al.,
2017; Feng et al., 2018).
Independent of in-
terpretability, Guo et al. (2017) show that neu-
ral network conﬁdence is unreasonably high: on
held-out examples, it far exceeds empirical accu-
racy. This is further exempliﬁed by the high conﬁ-

6.2 Conﬁdence Calibration is Insufﬁcient

We attribute one interpretation failure to neural
network conﬁdence issues. Guo et al. (2017) study
overconﬁdence and propose a calibration proce-
dure using Platt scaling, which adjusts the temper-
ature parameter of the softmax function to align
conﬁdence with accuracy on a held-out dataset.
However, this is not input dependent—the conﬁ-
dence is lower for both full-length examples and
ones with words left out. Hence, selecting inﬂuen-
tial words will remain difﬁcult.

To verify this, we create an interpretation base-
line using temperature scaling. The results cor-
roborate the intuition: calibrating the conﬁdence
of leave-one-out does not improve interpretations.
Qualitatively, the calibrated interpretation results
remain comparable to conﬁdence leave-one-out.
Furthermore, calibrating the DKNN conformity
score as in Papernot and McDaniel (2018) did not
improve interpretability compared to the uncali-
brated conformity score.

6.3 Alternative Interpretation Improvements

Recent work improves interpretation methods
through other means. Smilkov et al. (2017) and
Sundararajan et al. (2017) both aggregate gradi-
ent values over multiple backpropagation passes to
eliminate local noise or satisfy interpretation ax-
ioms. This work does not address model conﬁ-
dence and is orthogonal to our DKNN approach.

6.4

Interpretation Through Data Selection

Retrieval-Augmented Convolutional Neural Net-
works (Zhao and Cho, 2018) are similar
to
DKNN: they augment model predictions with an
information retrieval system that searches over
network activations from the training data.

Retrieval-Augmented models and DKNN can
both select inﬂuential training examples for a test
prediction. In particular, the training data activa-
tions which are closest to the test point’s activa-
tions are inﬂuential according to the model. These
training examples can provide interpretations as
a form of analogy (Caruana et al., 1999), an in-
tuitive explanation for both machine learning ex-
perts and non-experts (Klein, 1989; Kim et al.,
2014; Koh and Liang, 2017; Wallace and Boyd-
Graber, 2018). However, unlike in computer vi-

Prediction

Input

Saliency Map

Contradiction

Premise

Contradiction

Hypothesis

Premise

Hypothesis

Premise

Entailment
Entailment

Hypothesis
Hypothesis

a young boy reaches for and touches the propeller of a vintage
aircraft.
a young boy swims in his pool.

a brown dog and a black dog in the edge of the ocean with a
wave under them boats are on the water in the background.
the pets are sleeping on the grass.

man in a blue shirt standing in front of a structure painted with
geometric designs.
a man is wearing a blue shirt.
a man is wearing a black shirt.

Color Legend Positive Impact Negative Impact

Table 3: Interpretations generated with conformity leave-one-out align with annotation biases identiﬁed
in SNLI. In the second example, the model puts emphasis on the word “sleeping”, disregarding other
words that could indicate the Neutral class. The ﬁnal example diagnoses a model’s incorrect Entailment
prediction (shown in red). Green highlights indicate words that support the classiﬁcation decision made
(shown in parenthesis), pink highlights indicate words that support a different class.

Label

Artifact

Conformity Conﬁdence

Entailment

Neutral

Contradiction

outdoors
least
instrument
outside
animal

tall
ﬁrst
competition
sad
favorite

nobody
sleeping
no
tv
cat

2.93
2.22
3.57
4.08
2.00

1.09
2.14
2.33
1.39
1.69

1.00
1.64
2.53
1.92
1.42

3.26
4.41
4.47
4.80
4.73

2.61
2.99
5.56
1.79
3.89

1.00
2.34
5.74
3.74
3.62

Table 4: The top SNLI artifacts identiﬁed by Guru-
rangan et al. (2018) are shown on the left. For each
word, we compute the average importance rank
over the validation set using either Conformity or
Conﬁdence leave-one-out. A score of 1 indicates
that a word is always ranked as the most impor-
tant word in the input. Conformity leave-one-out
assigns higher importance to artifacts, suggesting
it better diagnoses model biases.

sion where training data selection using DKNN
yielded interpretable examples (Papernot and Mc-
Daniel, 2018), our experiments did not ﬁnd human
interpretable data points for SST or SNLI.

6.5 Trust in Model Predictions

Model conﬁdence is important for real-world ap-
plications: it signals how much one should trust
a neural network’s predictions. Unfortunately,
users may be misled when a model outputs highly
conﬁdent predictions on rubbish examples (Good-
fellow et al., 2015; Nguyen et al., 2015) or ad-
versarial examples (Szegedy et al., 2014). Re-
cent work decides when to trust a neural network
model (Ribeiro et al., 2016; Doshi-Velez and Kim,
2017; Jiang et al., 2018). For instance, analyzing
local linear model approximations (Ribeiro et al.,
2016) or ﬂagging rare network activations us-
ing kernel density estimation (Jiang et al., 2018).
The DKNN conformity score is a trust metric
that helps defend against image adversarial exam-
ples (Papernot and McDaniel, 2018). Future work
should study if this robustness extends to interpre-
tations.

7 Future Work and Conclusion

A robust estimate of model uncertainty is critical
to determine feature importance. The DKNN con-
formity score is one such uncertainty metric which
leads to higher precision interpretations. Al-

though DKNN is only a test-time improvement—
the model is still trained using maximum like-
lihood. Combining nearest neighbor and max-
imum likelihood objectives during training may
further improve model accuracy and interpretabil-
ity. Moreover, other uncertainty estimators do
not require test-time modiﬁcations. For example,
modeling p(x) and p(y | x) using Bayesian Neu-
ral Networks (Gal et al., 2016).

Similar

to other NLP interpretation meth-
ods (Sundararajan et al., 2017; Li et al., 2016),
conformity leave-one-out works when a model’s
representation has a ﬁxed size. For other NLP
tasks, such as structured prediction (e.g., trans-
lation and parsing) or span prediction (e.g., ex-
tractive summarization and reading comprehen-
sion), models output a variable number of pre-
dictions and our interpretation approach will not
sufﬁce. Developing interpretation techniques for
these types of models is a necessary area for fu-
ture work.

We apply DKNN to neural models for text
classiﬁcation. This provides a better estimate of
model uncertainty—conformity—which we com-
bine with leave-one-out. This overcomes issues
stemming from neural network conﬁdence, lead-
ing to higher precision interpretations. Most inter-
estingly, our interpretations are supported by the
training data, providing insights into the represen-
tations learned by a model.

Acknowledgments

subcontract

supported under

to
Feng was
Raytheon BBN Technologies by DARPA award
JBG is supported by NSF
HR0011-15-C-0113.
Grant
Any opinions, ﬁndings,
IIS1652666.
conclusions, or recommendations expressed here
are those of the authors and do not necessarily
reﬂect the view of the sponsor. The authors would
like to thank the members of the CLIP lab at
the University of Maryland and the anonymous
reviewers for their feedback.

References

Leila Arras, Franziska Horn, Gr´egoire Montavon,
Klaus-Robert M¨uller, and Wojciech Samek. 2016.
Explaining predictions of non-linear classiﬁers in
NLP. In Workshop on Representation Learning for
NLP.

Jon Louis Bentley. 1975. Multidimensional binary
search trees used for associative searching. Com-
munications of the ACM .

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In EMNLP.

Rich Caruana, Hooshang Kangarloo, John David N.
Dionisio, Usha S. Sinha, and David B. Johnson.
1999. Case-based explanation of non-case-based
learning methods. Proceedings of AMIA Symposium
.

Ronan Collobert and Jason Weston. 2008. A uniﬁed
architecture for natural language processing: Deep
neural networks with multitask learning. In ICML.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In EMNLP.

Finale Doshi-Velez and Been Kim. 2017. Towards a
rigorous science of interpretable machine learning.
arXiv preprint arXiv: 1702.08608 .

Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing
Dou. 2017. HotFlip: White-box adversarial exam-
ples for text classiﬁcation. In ACL.

Shi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer,
Pedro Rodriguez, and Jordan Boyd-Graber. 2018.
Pathologies of neural models make interpretations
difﬁcult. In EMNLP.

Yarin Gal, Yutian Chen, Roger Frigola, S. Gu, Alex
Kendall, Yingzhen Li, Rowan McAllister, Carl Ras-
mussen, Ilya Sutskever, Gabriel Synnaeve, Nilesh
Tripuraneni, Richard Turner, Oriol Vinyals, Adrian
Weller, Mark van der Wilk, and Yan Wu. 2016. Un-
certainty in Deep Learning. Ph.D. thesis, University
of Oxford.

Amirata Ghorbani, Abubakar Abid, and James Y. Zou.
Interpretation of neural networks is fragile.

2017.
arXiv preprint arXiv: 1710.10547 .

Aristides Gionis, Piotr Indyk, and Rajeev Motwani.
Similarity search in high dimensions via

1999.
hashing. In VLDB.

Ian J. Goodfellow, Jonathon Shlens, and Christian
Szegedy. 2015. Explaining and harnessing adver-
sarial examples. In ICLR.

Alex Graves and J¨urgen Schmidhuber. 2005. Frame-
wise phoneme classiﬁcation with bidirectional
LSTM and other neural network architectures. Neu-
ral networks : the ofﬁcial journal of the Interna-
tional Neural Network Society .

Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Wein-
berger. 2017. On calibration of modern neural net-
works. In ICML.

Suchin Gururangan, Swabha Swayamdipta, Omer
Levy, Roy Schwartz, Samuel R. Bowman, and
Noah A. Smith. 2018. Annotation artifacts in nat-
ural language inference data. In NAACL.

Marco T´ulio Ribeiro, Sameer Singh, and Carlos
Guestrin. 2016. Why should i trust you?”: Explain-
ing the predictions of any classiﬁer. In KDD.

Glenn Shafer and Vladimir Vovk. 2008. A tutorial on

Minqing Hu and Bing Liu. 2004. Mining and summa-

conformal prediction. JMLR .

rizing customer reviews. In KDD.

Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda B.
Vi´egas, and Martin Wattenberg. 2017. SmoothGrad:
arXiv preprint
removing noise by adding noise.
arXiv: 1706.03825 .

Richard Socher, A. V. Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In EMNLP.

Mukund Sundararajan, Ankur Taly, and Qiqi Yan.
2017. Axiomatic attribution for deep networks. In
ICML.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and
Rob Fergus. 2014.
Intriguing properties of neural
networks. In ICLR.

Eric Wallace and Jordan Boyd-Graber. 2018. Trick me
if you can: Adversarial writing of trivia challenge
questions. In Proceedings of ACL 2018 Student Re-
search Workshop.

Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. In LREC.

Jake Zhao and Kyunghyun Cho. 2018. Retrieval-
augmented convolutional neural networks for im-
proved robustness against adversarial examples.
arXiv preprint arXiv: 1802.09502 .

Heinrich Jiang, Been Kim, and Maya R. Gupta. 2018.
To trust or not to trust a classiﬁer. arXiv preprint
arXiv: 1805.11783 .

Been Kim, Cynthia Rudin, and Julie A. Shah. 2014.
The bayesian case model: A generative approach for
casebased reasoning and prototype classiﬁcation. In
NIPS.

Yoon Kim. 2014. Convolutional neural networks for

sentence classiﬁcation. EMNLP .

Pieter-Jan Kindermans, Sara Hooker, Julius Ade-
bayo, Maximilian Alber, Kristof T. Sch¨utt, Sven
D¨ahne, Dumitru Erhan, and Been Kim. 2017. The
(un)reliability of saliency methods. arXiv preprint
arXiv: 1711.00867 .

Gary A. Klein. 1989. Do decision biases explain too
In Proceedings of the Human Factors and

much.
Ergonomics Society.

Pang Wei Koh and Percy Liang. 2017. Understand-
ing black-box predictions via inﬂuence functions. In
ICML.

Jiwei Li, Will Monroe, and Daniel Jurafsky. 2016. Un-
derstanding neural networks through representation
erasure. arXiv preprint arXiv: 1612.08220 .

Xin Li and Dan Roth. 2002. Learning question classi-

ﬁers. In COLT.

W. James Murdoch, Peter J. Liu, and Bin Yu. 2018. Be-
yond word importance: Contextual decomposition
to extract interactions from lstms. In ICLR.

Anh Mai Nguyen, Jason Yosinski, and Jeff Clune.
2015. Deep neural networks are easily fooled: High
conﬁdence predictions for unrecognizable images.
In CVPR.

Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In ACL.

Nicolas Papernot and Patrick D. McDaniel. 2018.
Deep k-nearest neighbors: Towards conﬁdent, inter-
pretable and robust deep learning. arXiv preprint
arXiv: 1803.04765 .

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global vectors for
word representation. In EMNLP.

Adam Poliak, Jason Naradowsky, Aparajita Haldar,
Rachel Rudinger, and Benjamin Van Durme. 2018.
Hypothesis only baselines in natural language infer-
ence. In *SEM@NAACL-HLT.

