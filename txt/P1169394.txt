Short Text Hashing Improved by Integrating
Multi-Granularity Topics and Tags

Jiaming Xu, Bo Xu, Guanhua Tian, Jun Zhao,
Fangyuan Wang, and Hongwei Hao

Institute of Automation, Chinese Academy of Sciences. 100190, Beijing, P.R. China
{jiaming.xu,boxu,guanhua.tian,fangyuan.wang,hongwei.hao}@ia.ac.cn,
{jzhao}@nlpr.ia.ac.cn

Abstract. Due to computational and storage eﬃciencies of compact bi-
nary codes, hashing has been widely used for large-scale similarity search.
Unfortunately, many existing hashing methods based on observed key-
word features are not eﬀective for short texts due to the sparseness and
shortness. Recently, some researchers try to utilize latent topics of certain
granularity to preserve semantic similarity in hash codes beyond key-
word matching. However, topics of certain granularity are not adequate
to represent the intrinsic semantic information. In this paper, we present
a novel uniﬁed approach for short text Hashing using Multi-granularity
Topics and Tags, dubbed HMTT. In particular, we propose a selection
method to choose the optimal multi-granularity topics depending on the
type of dataset, and design two distinct hashing strategies to incorporate
multi-granularity topics. We also propose a simple and eﬀective method
to exploit tags to enhance the similarity of related texts. We carry out
extensive experiments on one short text dataset as well as on one normal
text dataset. The results demonstrate that our approach is eﬀective and
signiﬁcantly outperforms baselines on several evaluation metrics.

Keywords: Similarity Search, Hashing, Topic Features, Short Text.

1

Introduction

With the explosion of social media, numerous short texts become available in
a variety of genres, e.g. tweets, instant messages, questions in Question and
Answer (Q&A) websites and online advertisements [6]. In order to conduct
fast similarity search in those massive datasets, hashing, which tries to learn
similarity-preserving binary codes for document representation, has been widely
used to accelerate similarity search. Unfortunately, many existing hashing meth-
ods based on keyword feature space usually fail to fully preserve the semantic
similarity of short texts due to the sparseness of the original feature space. For
example, there are three short texts as follows:

d1: “Rafael Nadal missed the Australian Open”;
d2: “Roger Federer won Grand Slam title”;
d3: “Tiger Woods broke numerous golf records”.

5
1
0
2
 
r
a

M
 
0
1
 
 
]

R

I
.
s
c
[
 
 
1
v
1
0
8
2
0
.
3
0
5
1
:
v
i
X
r
a

Obviously, the hashing methods based on keyword space cannot see the simi-
larity among d1, d2 and d3. In recent years, some researchers seek to address the
challenge by latent semantic approach. For example, Wang et al. [12] preserve
the semantic similarity of documents in hash codes by ﬁtting the topic distri-
butions, and Xu et al. [14] directly treat the latent topic features as tokens to
represent one document for hashing learning. However, topics of certain gran-
ularity are not adequate to represent the intrinsic semantic information [4]. As
we know, diﬀerent topic models with pre-deﬁned number of topics can extract
diﬀerent semantic level topics. For example, the topic model with a large num-
ber of topics can extract more ﬁne grained topic features, such as “Tennis Open
Progress” for d1 and d2, and “Golf Star News” for d3, but fail to construct the
semantic relevance of d3 with the other texts, and the topic model with a few
topics can extract more coarse grained semantic features, such as “Sport” and
“Star” for d1, d2 and d3, but lack distinguishing information and cannot learn
the hashing function eﬀectively, As a reasonable assumption, multi-granularity
topics are more suitable to preserve semantic similarity and learn hashing func-
tion for short text hashing.

On the other hand, tags are not fully utilized in many hashing methods. Actu-
ally, in various real-world applications, documents are often associated with mul-
tiple tags, which provide useful knowledge in learning eﬀective hash codes [12].
For instance, in Q&A websites, each question has category labels or related tags
assigned by its questioner. Another example is microblog, some tweets are labeled
by their authors with hashtags in the form of “#keyword”. Thus, we should fully
exploit the information contained in tags to strengthen the semantic relationship
of related texts for hashing learning.

Based on the above observations, this paper proposes a uniﬁed short text
Hashing using Multi-granularity Topics and Tags, referred as HMTT for simplic-
ity. In HMTT, two diﬀerent ways are introduced to incorporate multi-granularity
topics and tag information for improving short text hashing.

The main contributions of this paper are three-fold: Firstly, a novel uniﬁed
short text hashing is proposed. To our best knowledge, this is the ﬁrst time of
incorporating multi-granularity topics and tags into a uniﬁed hashing approach,
and experiments are conducted to verify our assumption that short text hash-
ing can be improved by integrating multi-granularity topics and tags. Secondly,
the optimal multi-granularity topics can be selected automatically, i.e., to ex-
tract eﬀective latent topic features for hashing learning. The experimental results
indicate the optimal multi-granularity topics can achieve better performances,
compared with other multi-granularity topics. Finally, two strategies to incorpo-
rate multi-granularity topics for short text hashing are designed and compared
through extensive experimental evaluations and analyses.

2 Related Work

Hash-based methods can be mainly divided into two categories. One category
is data-oblivious hashing. As the most popular hashing technique, Locality-

(cid:8)(cid:23)(cid:2)(cid:5)(cid:3)(cid:5)(cid:3)(cid:26)(cid:12)(cid:8)(cid:7)(cid:24)(cid:6)(cid:15)

(cid:5)

(cid:12)(cid:19)(cid:18)(cid:10)(cid:20)(cid:7)(cid:14)(cid:4)(cid:21)(cid:6)(cid:14)(cid:6)(cid:17)(cid:18)(cid:10)(cid:22)(cid:9)

(cid:8)(cid:9)(cid:10)(cid:5)(cid:11)(cid:12) (cid:7)(cid:2)(cid:6)(cid:25)(cid:23)(cid:7)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:1)(cid:1)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:4)(cid:2)(cid:6)(cid:7)
(cid:8)(cid:9)(cid:10)(cid:5)(cid:11)(cid:12)(cid:13)(cid:9)(cid:4)(cid:7)(cid:14)(cid:15)(cid:12)(cid:16)(cid:17)(cid:16)

(cid:18)(cid:10)(cid:6)(cid:5)(cid:19)(cid:2)(cid:14)
(cid:8)(cid:9)(cid:10)(cid:5)(cid:11)(cid:12)(cid:13)(cid:9)(cid:4)(cid:7)(cid:14)(cid:15)(cid:12)(cid:16)(cid:13)(cid:16)

(cid:8)(cid:2)(cid:26)(cid:15)

(cid:2)

(cid:8)(cid:9)(cid:10)(cid:5)(cid:11)(cid:12) (cid:7)(cid:2)(cid:6)(cid:25)(cid:23)(cid:7)(cid:12)(cid:12)(cid:12)(cid:12)
!(cid:1)

(cid:31)(cid:31)(cid:31)

(cid:31)(cid:31)(cid:31)

(cid:8)(cid:9)(cid:10)(cid:5)(cid:11)(cid:12) (cid:7)(cid:2)(cid:6)(cid:25)(cid:23)(cid:7)(cid:12)(cid:12)(cid:12)(cid:12)
!(cid:1)

(cid:8)(cid:9)(cid:10)(cid:5)(cid:11)(cid:12) (cid:7)(cid:2)(cid:6)(cid:25)(cid:23)(cid:7)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:1)(cid:1)

(cid:1)(cid:2)(cid:3)(cid:3)(cid:4)
(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:9)(cid:11)

 !!! 

!! ! 

!!   

    !

 ! ! 

  ! !

(cid:12)(cid:13)(cid:13)(cid:14)(cid:10)(cid:9)(cid:6)
(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:9)(cid:11)

(cid:20)(cid:2)(cid:15)(cid:21)(cid:12)(cid:1)(cid:9)(cid:4)(cid:7)(cid:15)(cid:12)(cid:22)(cid:9)(cid:23)(cid:12)(cid:8)(cid:7)(cid:24)(cid:6)(cid:15)(cid:12)(cid:1)(cid:9)(cid:23)(cid:10)(cid:25)(cid:15)

(cid:3)

(cid:1)(cid:7)(cid:23)(cid:24)(cid:4)(cid:25)(cid:26)(cid:9)(cid:17)(cid:18)(cid:10)(cid:22)(cid:9)

(cid:2)

(cid:29)(cid:7)(cid:6)(cid:23)(cid:5)(cid:7)(cid:30)(cid:2)(cid:14)

  ! !

(cid:12)(cid:9)(cid:14)(cid:10)(cid:9)(cid:6)
(cid:15)(cid:8)(cid:6)(cid:16)(cid:10)(cid:17)(cid:18)(cid:10)(cid:9)(cid:11)

(cid:20)(cid:2)(cid:15)(cid:21)(cid:12)(cid:1)(cid:9)(cid:4)(cid:7)(cid:12)(cid:22)(cid:9)(cid:23)(cid:12)(cid:27)(cid:25)(cid:7)(cid:23)(cid:28)(cid:12)(cid:8)(cid:7)(cid:24)(cid:6)

(cid:3)(cid:4)

(cid:27)(cid:25)(cid:7)(cid:23)(cid:28)(cid:12)(cid:8)(cid:7)(cid:24)(cid:6)

(cid:3)(cid:6)

Fig. 1. The proposed approach HMTT for short text hashing

Sensitive Hashing (LSH) [1] based on random projection has been widely used
for similarity search. However, since they are not aware of data distribution,
those methods may lead to generate quite ineﬃcient hash codes in practice [16].
Recently, more researchers focus attention on the other category, data-aware
hashing, For example, the Spectral Hashing (SpH) [13] generates compact bi-
nary codes by forcing the balanced and uncorrelated constraints into the learned
codes. Self-Taught Hashing (STH) [18] and Two Step Hashing (TSH) [9] decom-
pose the learning procedure into two steps: generating binary code and learning
hash function, and a supervised version of STH is proposed in [16] denoted
as STHs. However, the previous hashing methods, directly working in keyword
feature space, usually fail to fully preserve semantic similarity. More recently,
Wang et al. [12] proposed a Semantic Hashing using Tags and Topic Modeling
(SHTTM). However, the limitations of SHTTM are that: Although the topic
distributions are used to preserve the content similarity to generate hash codes,
they do not utilize the topics to improve hashing function learning; Even the
number of topics must keep consistent with dimensions of hash code, that this
assumption is too strict to capture the optimal semantic features for diﬀerent
types of datasets.

3 Algorithm Description

A uniﬁed short text hashing approach HMTT is depicted in Fig. 1. Given a
dataset of n training texts denoted as: X = {x1, x2, ..., xn} ∈ Rd×n, where d is
the dimensionality of the keyword feature. Denote their tags as: t = {t1, t2, ..., tn} ∈
{0, 1}q×n, where q is the total number of possible tags associated with each
text. A tag with label 1 means a text is associated with a certain tag/category,
while a tag with label 0 means a missing tag or the text is not associated
with that tag/category. The goal of HMTT is to obtain optimal binary codes
Y = {y1, y2, ..., yn}T ∈ {−1, 1}n×l, and a hashing function f : Rd → {−1, 1}l,
which embeds the query text xq to its binary vector representation yq with l
bits. To achieve the similarity-preserving property, we require the similar texts
to have similar binary codes in Hamming space. We ﬁrst select the optimal
topic models from the candidate topic models, and extract the multi-granularity

Algorithm 1 The Optimal Topics Selection
Input: n training texts X = {x1, x2, ..., xn} with tags t = {t1, t2, ..., tn}, N candidate

Find nn+(ˆx) and nn−(ˆx);
for i ← 1 to N do

topic sets T = {T1, T2, ..., TN } and a speciﬁed number M .
Output: The optimal topic sets O, and the weight vector µ.
1: Sample a sub-set ˆX with tags ˆt; Initialize µ ← 0, and O ← ∅;
2: for each text ˆx ∈ ˆX do
3:
4:
5:
6:
7: end for
8: while size(O) < M do
9:
10: end while
11: return O and µ;

Update µ(Ti) by Eq. 1;

end for

T (p) = arg maxTi∈Tµ(Ti); Update O = O ∪ {T (p)}, T = T − {T (p)};

topic features {θ1, θ2, ..., θM }. Then the binary codes and hash functions can be
learned by integrating multi-granularity topic features and tags. In the second
phase which is online, the query text is represented by binary code mapped from
the derived hash function, and then the approximate nearest neighbor search is
accomplished in Hamming space. All pairs of hash code found within a certain
Hamming distance of each other are semantic similar texts.

The main challenges of the idea are that: (1). How to select the optimal
topic models; (2). How to utilize the tag information eﬃciently; and (3). How
to integrate the multi-granularity topics to preserve semantic similarity. The
proposed approach HMTT will be described in detail in the following sections.

3.1 Estimate and Select the Optimal Topics

In this work, we straightforwardly obtain a set of candidate topics by pre-deﬁning
several diﬀerent topic numbers of Latent Dirichlet Allocation (LDA) [3]. After
training the topic models, we can draw multi-granularity topic features, cor-
responding as distributions over the topics, from the candidate topic models.

In order to select the optimal topic models, we should utilize the tag in-
formation to evaluate the quality of topics. Inspired by [4,7], the selection of
optimal topic model sets depends on their capability in helping discriminate
short texts without sharing any common tags. We denote N diﬀerent sets of
topics as T = {T1, T2, ..., TN }. For each entry Ti, the probability topics distri-
butions over documents are denoted as θ = p(z|x). The weight vector is µ =
{µ(T1), µ(T2), ..., µ(TN )}, where µ(Ti) is the weight indicating the importance of
topic set. The purpose is to select the optimal topic sets O = {T1, T2, ..., TM }.
In [4], Chen et al. evaluate the quality of topics based on two aspects: discrim-
ination and complementarity of the multi-granularity topics. However, how to
balance those two aspects is a tricky problem and the latter aspect, comple-
mentarity, is easy to introduce noises for preserving similarity. Thus, we propose

a simple and eﬀective method directly based on the key idea of Relief [7] as
follows: Firstly, a sub-set ˆX = {ˆx1, ˆx2, ..., ˆxm} with tags ˆt = {ˆt1, ˆt2, ..., ˆtm} is
sampled from training dataset, and we ﬁnd two groups of k nearest neighbors
for each text ˆxi: one group is from the texts sharing any common tags (denoted
as nn+(ˆx)), and the other from the texts not sharing any common tags (denoted
as nn−(ˆx)). Then the weight is updated as follows:

µ(Ti) = µ(Ti) +

k

k

DKL(Ti(x),Ti(nn

−

j (x)))

DKL(Ti(x),Ti(nn+

p (x)))

k

(1)

j=1
P

−

k

p=1
P

where, DKL is the symmetric Kullback-Leibler (KL) divergence:

DKL(Ti(x), Ti(nn−

j (x))) = 1
2

(p(zk|x) · log(

zk∈Ti
P
+p(zk|nn−

j (x)) · log(

)

p(zk|x)
−

p(zk|nn

p(zk|nn

p(zk|x)

j (x))
j (x))

−

)),

so is the value of DKL(Ti(x), Ti(nn+
p (x))). After updating the weight vector, we
directly select the optimal topic sets O according to the top-M weight values.
In summary, the optimal topics selection procedure is depicted in Algorithm 1.

3.2 Content Similarity and Tags Preservation

In hashing problem, one key component is how to deﬁne the aﬃnity matrix S.
Diverse approaches can be applied to construct the similarity matrix. In this
paper, we choose cosine function as an example and use the local similarity
structure of all text pairs to reconstruct the similarity function as follows:

xT
i

xj

Sij =

(

cij ·

kxik·kxj k , if xi ∈ NNk(xj) or vice versa
0,

otherwise

(2)

where NNk(x) represents the set of k-nearest-neighbors of x, and cij is an
conﬁdence coeﬃcient. If two documents xi and xj share any common tag, we
set cij a higher value a. In reverse, the cij is given a lower value b if two documents
xi and xj are not related. The parameters a and b satisfy 1 ≥ a ≥ b > 0. For
a particular dataset, the more trustworthy the tags are, the greater diﬀerence
between a and b we set. In our experiments, we set a = 1 and b = 0.1.

3.3 Learning to Hash with Multi-Level Topics

Below, from diﬀerent perspectives, we propose two strategies to integrate multi-
granularity topics for improving short text hashing.

Feature-Level Fusion In order to integrate multi-granularity topics, we here
adopt a simple but powerful way to combine observed features and latent features
for short text, similar as [10] and [4], and create a high dimensional vector Ω as:

Ω = [ˆµ1θ1, ˆµ2θ2, ..., ˆµM θM ],

(3)

Algorithm 2 Feature-Level Fusion Procedure
Input: A set of n training texts X with tags t, M optimal topic models O associated

with their weight vector ˆµ.

Output: The optimal hash codes Y and the hash function: l linear SVM classiﬁers.
1: Extract M topic feature sets {θ1, θ2, ..., θM } from the optimal topic models O;
2: Produce the new feature Ω by Eq. 3 and construct conﬁdence matrix S by Eq. 2;
3: Obtain the l-dimensional vectors ˜Y by optimizing Eq. 5;
4: Generate Y by thresholding ˜Y to the median vector m = median( ˜Y);
5: Train l linear SVM classiﬁers by the learned codes Y;
6: return Hash codes Y and l linear SVM;

where, {θ1, θ2, ..., θM } are the optimal topic features, and

ˆµi = µi(Ti)/minTk∈O(µk(Tk)).

(4)

We can straightforwardly construct the similarity matrix S by Eq. 2 with the
new features Ω of training texts. Similar as Two-Step Hashing (TSH) [9], we see
the binary code generation and hash function learning process as two separate
steps. As a special example, Laplacian aﬃnity loss and linear SVM are chosen
to solve our problem. In ﬁrst step, the training hash codes procedure can be
formulated as following optimization:

n

Sij kyi − yj k2
F

min
Y
s.t.Y ∈ {−1, 1}n×l, YT 1 = 0, YT Y = I

i,j=1
P

where Sij is the pairwise similarity between documents xi and xj , yi is the hash
code for xi, and k·kF is the Frobenius norm. To satisfy the similarity preserva-
tion, we seeks to minimize the quantity, because it incurs a heavy penalty if two
similar documents are mapped far away. The problem is relaxed by discarding
Y ∈ {−1, 1}n×l, the optimal l-dimensional real-valued vector ˜Y can be obtained
by solving Laplacian Eigenmaps problem [2]. Then, ˜Y can be converted into bi-
nary codes Y via the media vector m = median( ˜Y). In hash function learning
step, thinking of each bit y(p)
i ∈ {+1, −1} in the binary code as a binary class
label for that text, we can train l linear SVM classiﬁers f (x) = sgn(WT x) to
predict the l-bit binary code for any query document xq. Algorithm 2 shows the
procedure of this strategy.

Decision-Level Fusion From another perspective, we can treat the optimal
multi-granularity topic feature sets {θ1, θ2, ..., θM } extracted from short texts as
multi-view features. In our situation, there are M -view features: {θ1, θ2, ..., θM }.
We take a linear sum of those M -view similarities as follows:

(5)

(6)

M

n

k=1
X

i,j=1
X

S(k)
ij kyi − yjk2

F

Algorithm 3 Decision-Level Fusion Procedure
Input: A set of n training texts X with tags t, M optimal topic models O and

trade-oﬀ parameters, C1 and C2.

Output: The optimal hash codes Y and a set of linear hash function matrices ˜W.
1: Extract M topic feature sets {θ1, θ2, ..., θM } from the optimal topic models O;
2: Construct a series of conﬁdence matrices {S(1), S(2), ..., S(M )} by Eq. 2 for M fea-

ture sets: {θ1, θ2, ..., θM };

3: Obtain the l-dimensional vectors ˜Y and ˜W by optimizing Eq. 7;
4: Generate Y by thresholding ˜Y to the median vector m = median( ˜Y);
5: return Hash codes Y and hash function matrix set ˜W;

M

k=1
P

where, S(k)
constructed as Eq. 2 is the aﬃnity matrix deﬁned on the k-th view
ij
features. By introducing a diagonal n × n matrix D(k) whose entries are given

by D

(k)

ii =

n

j=1 S(k)

ij , Eq. 6 can be rewritten as tr(YT

(D(k) − S(k))Y) =

tr(YT

P
L(k)Y), where L(k) is the Laplacian matrix deﬁned on the k-th view

M

k=1
P

features. By introducing Composite Hashing with Multiple Information Sources
(CHMIS) [15], as a representative of Multiple View Hashing (MVH), we can
simultaneously learn the hash codes Y of the training texts X as well as a set
X(k) to infer the hash code for query
of linear hash functions
text xq. The overall objective function is given as follows:

T
M
k=1 αk(W(k))

P

M

M

C1tr(YT

˜L(k)Y) + C2
min
Y,W,α
s.t.Y ∈ {−1, 1}n×k, YT 1 = 0, YT Y = I,αT 1= 1,α ≥ 0

αk(W(k))X(k)

k=1
P

k=1
P

Y −

(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)

M

+

W(k)

k=1
P

(cid:13)
(cid:13)

2
F

(cid:13)
(cid:13)

(7)
where, C1 and C2 are trade-oﬀ parameters, tr(·) is the matrix trace function,
α = [α1, α2, ..., αM ] is a combination coeﬃcient vector to balance the out-
puts from each view features, and a series of linear hash function matrices:
˜W = {α1W(1), α2W(2), ..., αMW(M)}. In order to solve this hard optimization
problem, we ﬁrst relax the discrete constraints Y ∈ {−1, 1}n×l, and iteratively
optimize one variable with the other two ﬁxed. More detailed optimization pro-
cedures of this method can be found in [15]. Diﬀerent from the former strategy,
we do not need to pre-allocate the weight value of each view features, because
that the combination coeﬃcient vector α = [α1, α2, ..., αM ] learned iteratively in
the process of optimization can balance the outputs of each view features, and
the procedure of this strategy is shown in Algorithm 3.

3.4 Complexity Analysis

The training processes including binary code learning and hash function training
are always conducted oﬀ-line. Thus, our focus of eﬃciency is on the prediction
process. This process of generating hash code for a query text only involves some

Gibbs sampling iterations to extract multi-granularity topics {θ1, θ2, ..., θM }
and dot products in hash function y = sgn(WT x), which can be done in
O(r ˜Ks + l ˜K). Here, r is the number of Gibbs sampling iterations for topic
inference, ˜K is the sum of multi-granularity topic numbers {K1, K2, ..., KM }, l
is the dimensionality of hash code and s denotes the sparsity of the observed
keyword features. The values of the parameters above can be regarded as quite
small constants. For example, r = 20, ˜K ≈ 100, l ≤ 64 and the average number
of sparsity per document s is no more than 100 in our experimental datasets.
We can see the major time complexity is the Gibbs sampling for topic inference.
In recent works, lots of studies focus to accelerate the topic inference. For exam-
ple, in Biterm Topic Model (BTM), [5] gives a simplicity and eﬃcient method
without Gibbs sampling iterations and the time complexity for topic inference
can be reduced to O(Kb), where b is the number of biterms in a query text.

4 Experiment and Analysis

4.1 Dataset and Experimental Settings

We carried out extensive experiments on two publicly available real-world text
datasets: one is typical short text dataset, Search Snippets1, and another is nor-
mal text dataset, 20Newsgroups2.

The Search Snippets dataset collected by Phan [10] was selected from the
results of web search transaction using predeﬁned phrases of 8 diﬀerent domains.
We further ﬁlter the stop words and stem the texts. 20139 distinct words, 10059
training texts and 2279 test texts are left, and the average text length is 17.1.

The 20Newsgroups corpus was collected by Lang [8]. We use the popu-
lar ‘bydate‘ version which contains 20 categories, 26214 distinct words, 11314
training texts and 7532 test texts, and the average text length is 136.7.

For these datasets, we denote the category labels as tags. For Search Snippets,
we use a large-scale corpus [10] crawled from Wikipedia to estimate the topic
models, and the original keyword features are directly used for learning the can-
didate topic models for 20Newsgroups due to the suﬃcient keyword features. In
order to evaluate our method’s performance, we compute standard retrieval per-
formance measures: recall and precision, by using each document in the test set
as a query to retrieve documents in the training set within a speciﬁed Hamming
distance. For the original keyword feature space cannot well reﬂect the semantic
similarity of documents, even worse for short text, we simply test if the two doc-
uments share any common tag to decide whether a semantic similar text. This
methodology is used in SH [11], STH [18], CHMIS [15] and SHTTM [12].

Five alternative hashing methods compared with our proposed approach are
STHs [16], STH [18], LCH [17], LSI [11] and SpH [13]. The results of all baseline
methods are obtained by the open-source implementation provided on their cor-
responding author’s homepage. In order to distinguish the proposed two strate-

1 http://jwebpro.sourceforge.net/data-web-snippets.tar.gz
2 http://people.csail.mit.edu/jrennie/20Newsgroups/

SearchSnippets

20Newsgroups

1

0.8

i

i

n
o
s
c
e
r
P

0.6

0.4

0.2

0

 
0

 

HMTT−Fea
HMTT−Dec
STHs−Tag
STHs
STH
LCH
LSI
SpH

1

0.8

i

i

n
o
s
c
e
r
P

0.6

0.4

0.2

0

 
0

 

HMTT−Fea
HMTT−Dec
STHs−Tag
STHs
STH
LCH
LSI
SpH

0.2

0.4

0.6

0.8

1

0.2

0.4

0.6

0.8

1

Recall

(a) SearchSnippets

Recall

(b) 20Newsgroups

Fig. 2. Precision-Recall curves of retrieved examples within Hamming radius 3 on two
datasets with diﬀerent hashing bits (4:4:64 bits).

gies in our approach, the feature level fusion method is denoted as HMTT-Fea,
and the decision level fusion method is named as HMTT-Dec3.

In our experiments, the candidate topic sets T = {T 10, T 30, T 50, T 70,
T 90, T 120, T 150} and the number of the optimal topic sets is ﬁxed to 3. The
parameters C1 and C2 in Eq. 7 are tuned from {0.1, 1, 10, 100}. The number of
nearest neighbors is ﬁxed to 25 when constructing the graph Laplacians in our
approach, as well as in the baseline methods, STHs and STH. We evaluate the
performance of diﬀerent methods by varying the number of hashing bits from
4 to 64. For LDA, we used the open-source implementation GibbsLDA4, and
the hyper-parameters are tuned as α = 0.5, β = 0.01, 1000 iterations of Gibbs
sampling for learning, and 20 iterations for topic inference. The results reported
are the average over 5 runs.

4.2 Results and Analysis

We sample 100 texts for each category with tags information randomly from
training dataset and set k in Eq. 1 to 10 to evaluate the quality of topic sets
by Algorithm 1. As the number of optimal topic sets is ﬁxed to 3, we get the
optimal topic sets O = {T 10, T 30, T 50} for both two datasets coincidentally,
and the weight vectors ˆµ = {3.44, 1.7, 1} for Search Snippets and ˆµ = {1.31,
1.22, 1} for 20Newsgroups. It is noteworthy that the weight values of the topic
sets are aﬀected by both the type of dataset and the settings of LDA. Below, a
series of experiments are conducted to answer the questions: (1). How does the
proposed approach HMTT compare with other baseline methods; (2). Whether
the optimal multi-granularity topics can outperform single-granularity topics
and other multi-granularity topics; (3). Which approach of the two strategies to
integrate multi-granularity topics can achieve a better performance.

3 https://github.com/jacoxu/short-text-hashing-HMTT,

http://www.CICLing.org/2015/data/148

4 http://jgibblda.sourceforge.net/

Table 1. Mean precision (mP) of the top 200 examples and the retrieved examples
within Hamming radius 3 on SearchSnippets with 8 and 16 hashing bits. e.g. 10-30-
50* means that the proposed methods incorporate the optimal multi-granularity topics,
and 10-30-50W1 means that hashing method uses the multi-granularity topic sets {T 10,
T 30, T 50} while ﬁxing the balance values to 1:1:1.

—
Methods

mP@Top 200

mP@Hamming Radius 3

HMTT-Fea

HMTT-Dec

HMTT-Fea

HMTT-Dec

10-30-50*
10-70-90
30-90-150
10-30
10-50
30-50

Code Length 8 bits 16 bits 8 bits 16 bits 8 bits 16 bits 8 bits 16 bits
0.826 0.782 0.411 0.802 0.403 0.778
0.754
0.797
0.757
0.801
0.744
0.776
0.740
0.772
0.764
0.805
0.774
0.822
0.576
0.639
0.692
0.728
0.707
0.731
0.704
0.728
0.692
0.735
0.683
0.707
0.660
0.675

0.829
0.819
0.802
0.810
0.813
0.806
10-30-50W1 0.811
0.627
0.792
0.782
0.771
0.757
0.730
0.740

0.799
0.800
0.787
0.789
0.788
0.796
0.780
0.624
0.764
0.758
0.755
0.733
0.705
0.727

0.328
0.382
0.374
0.334
0.369
0.398
0.296
0.335
0.320
0.318
0.332
0.309
0.304

0.762
0.755
0.757
0.752
0.777
0.778
0.602
0.708
0.723
0.720
0.708
0.700
0.674

0.375
0.393
0.382
0.383
0.393
0.368
0.316
0.377
0.360
0.365
0.363
0.366
0.370

0.789
0.777
0.776
0.790
0.779
0.761
0.610
0.757
0.730
0.747
0.736
0.714
0.729

10
30
50
70
90
120
150

Compared with the existing hashing methods: In this section, we de-
sign an improved version of STHs, denoted as STHs-Tag, by replacing the orig-
inal construction of similarity matrix with the proposed method described in
Section 3.2. We remove 60 percent tags randomly from the training dataset to
verify the robustness for HMTT-Fea, HMTT-Dec, STHs and STHs-Tag. The
precision-recall curves for retrieved examples are reported in Fig. 2. From these
comparison results, we can see that HMTT-Fea and HMTT-Dec signiﬁcantly
outperform other baseline methods on Search Snippets as shown in Fig. 2 (a).
For 20Newsgroups, HMTT-Dec performs close results with STHs-Tag in Fig. 2
(b). The reasons to explain this problem are that: Firstly, 20Newsgroups as a nor-
mal dataset has suﬃcient original features to learn hash codes so that STHs-Tag
based on keyword features works well. Secondly, we directly learn the topic mod-
els of 20Newsgroups from the training dataset that result in some restrictions.
Furthermore, STHs get a worse performance than STHs-Tag on two datasets.
Because STHs uses a complete supervised approach which only utilizes the pair-
wise similarity of the documents with common tags, that method cannot well
deal with the situations that tags are missing or incomplete. In our approach, we
extract the optimal multi-granularity topics depending on the type of dataset to
learn hash codes and hashing function, and the tags are just utilized to adjust
the similarity, which has stronger robustness. In the following experiment sets,
we keep the all tags to improve the performance of hashing learning.

Compared with single-granularity and other multi-granularity topic
sets: Here, the hashing performances of the optimal multi-granularity topics are

compared with single-granularity and other multi-granularity topics. We further
evaluate the balance values of the multi-granularity topics by ﬁxing them to 1. In
particular, we keep the parameters ˆµi in Eq. 3 and αi in Eq. 7 to 1 for HMTT-
Fea and HMTT-Dec respectively. The quantitative results on Search Snippets
are reported in Table 1. From the results, we can see that the performances
of multi-granularity topics signiﬁcantly outperform single-granularity topics and
the optimal multi-granularity topics achieve a better performance in most situ-
ations. We also observe similar results on 20Newsgroups. But due to the limit of
space, we select to present the results on the typical short texts dataset Search
Snippets.

Compared between the proposed two strategies: Finally, we mainly
discuss the performances between the proposed two strategies, HMTT-Fea and
HMTT-Dec. In HMTT-Fea, we directly concatenate the multi-granularity topics
to produce one feature vector and decompose the hashing learning problem into
two separate stages. In HMTT-Dec, the multi-granularity topics extracted from
the text content are treated as multi-view features, and we simultaneously learn
the hash codes as well as hash function. From the results in Table 1, we can see
that the performances of HMTT-Fea surpass HMTT-Dec on several evaluation
metrics. Obviously, the former strategy is more simple and eﬀective for short
text hashing in our approach. In summary, no matter in HMTT-Fea or HMTT-
Dec, the experimental results indicate that short text hashing can be improved
by integrating multi-granularity topics.

5 Discussions and Conclusions

Short text hashing is a challenging problem due to the sparseness of text rep-
resentation. In order to address this challenge, tags and latent topics should be
fully and properly utilized to improve hashing learning. Furthermore, it is better
to estimate the topic models from an external large-scale corpus and the opti-
mal topics should be selected depending on the type of dataset. This paper uses
a simple and eﬀective selection methods based on symmetric KL-divergence of
topic distributions, we think that there are many other selection methods worthy
of being explored further. Another key issue worthy of research is how to inte-
grate the multi-granularity topics eﬀectively. In this paper, we propose a novel
uniﬁed hashing approach for short text retrieval. In particular, the optimal multi-
granularity topics are chosen depending on the type of dataset. We then use the
optimal multi-granularity topics to learn hash codes and hashing function on two
distinct ways, meanwhile, tags are utilized to enhance the semantic similarity
of related texts. Extensive experiments demonstrate that the proposed method
can perform better than the competitive methods on two public datasets.

Acknowledgments

This work is supported by the National Natural Science Foundation of China
under Grant No. 61203281 and No. 61303172.

References

1. Andoni, A., Indyk, P.: Near-optimal hashing algorithms for approximate nearest
neighbor in high dimensions. In: Foundations of Computer Science, 2006. FOCS’06.
47th Annual IEEE Symposium on. pp. 459–468. IEEE (2006)

2. Belkin, M., Niyogi, P.: Laplacian eigenmaps for dimensionality reduction and data

representation. Neural computation 15(6), 1373–1396 (2003)

3. Blei, D.M., Ng, A.Y., Jordan, M.I.: Latent dirichlet allocation. the Journal of

Machine Learning Research 3, 993–1022 (2003)

4. Chen, M., Jin, X., Shen, D.: Short text classiﬁcation improved by learning multi-
granularity topics. In: Proceedings of the 22nd international joint conference on
Artiﬁcial Intelligence. pp. 1776–1781. AAAI Press (2011)

5. Cheng, X., Lan, Y., Guo, J., Yan, X.: Btm: Topic modeling over short texts. IEEE

Transactions on Knowledge and Data Engineering p. 1 (2014)

6. Jin, O., Liu, N.N., Zhao, K., Yu, Y., Yang, Q.: Transferring topical knowledge from
auxiliary long texts for short text clustering. In: CIKM. pp. 775–784. ACM (2011)
7. Kononenko, I.: Estimating attributes: analysis and extensions of relief. In: Machine

Learning: ECML-94. pp. 171–182. Springer (1994)

8. Lang, K.: Newsweeder: Learning to ﬁlter netnews. In: In Proceedings of the Twelfth

International Conference on Machine Learning. Citeseer (1995)

9. Lin, G., Shen, C., Suter, D., Hengel, A.v.d.: A general two-step approach to
learning-based hashing. In: Computer Vision (ICCV), 2013 IEEE International
Conference on. pp. 2552–2559. IEEE (2013)

10. Phan, X.H., Nguyen, L.M., Horiguchi, S.: Learning to classify short and sparse
text & web with hidden topics from large-scale data collections. In: Proceedings of
the 17th international conference on World Wide Web. pp. 91–100. ACM (2008)

11. Salakhutdinov, R., Hinton, G.: Semantic hashing. International Journal of Approx-

imate Reasoning 50(7), 969–978 (2009)

12. Wang, Q., Zhang, D., Si, L.: Semantic hashing using tags and topic modeling. In:
Proceedings of the 36th international ACM SIGIR conference on Research and
development in information retrieval. pp. 213–222. ACM (2013)

13. Weiss, Y., Torralba, A., Fergus, R.: Spectral hashing. In: Advances in neural in-

formation processing systems. pp. 1753–1760 (2009)

14. Xu, J., Liu, P., Wu, G., Sun, Z., Xu, B., Hao, H.: A fast matching method based on
semantic similarity for short texts. In: Natural Language Processing and Chinese
Computing, pp. 299–309. Springer (2013)

15. Zhang, D., Wang, F., Si, L.: Composite hashing with multiple information sources.
In: Proceedings of the 34th international ACM SIGIR conference on Research and
development in Information Retrieval. pp. 225–234. ACM (2011)

16. Zhang, D., Wang, J., Cai, D., Lu, J.: Extensions to self-taught hashing: Kernelisa-

tion and supervision. practice 29, 38 (2010)

17. Zhang, D., Wang, J., Cai, D., Lu, J.: Laplacian co-hashing of terms and documents.

In: Advances in Information Retrieval, pp. 577–580. Springer (2010)

18. Zhang, D., Wang, J., Cai, D., Lu, J.: Self-taught hashing for fast similarity search.
In: Proceedings of the 33rd international ACM SIGIR conference on Research and
development in information retrieval. pp. 18–25. ACM (2010)

