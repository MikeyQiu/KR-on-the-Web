8
1
0
2
 
t
c
O
 
6
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
3
8
7
1
1
.
5
0
8
1
:
v
i
X
r
a

To Trust Or Not To Trust A Classiﬁer

Heinrich Jiang∗
Google Research
heinrichj@google.com

Been Kim
Google Brain
beenkim@google.com

Melody Y. Guan†
Stanford University
mguan@stanford.edu

Maya Gupta
Google Research
mayagupta@google.com

Abstract

Knowing when a classiﬁer’s prediction can be trusted is useful in many applications
and critical for safely using AI. While the bulk of the effort in machine learning
research has been towards improving classiﬁer performance, understanding when
a classiﬁer’s predictions should and should not be trusted has received far less
attention. The standard approach is to use the classiﬁer’s discriminant or conﬁdence
score; however, we show there exists an alternative that is more effective in many
situations. We propose a new score, called the trust score, which measures the
agreement between the classiﬁer and a modiﬁed nearest-neighbor classiﬁer on
the testing example. We show empirically that high (low) trust scores produce
surprisingly high precision at identifying correctly (incorrectly) classiﬁed examples,
consistently outperforming the classiﬁer’s conﬁdence score as well as many other
baselines. Further, under some mild distributional assumptions, we show that if the
trust score for an example is high (low), the classiﬁer will likely agree (disagree)
with the Bayes-optimal classiﬁer. Our guarantees consist of non-asymptotic rates
of statistical consistency under various nonparametric settings and build on recent
developments in topological data analysis.

1

Introduction

Machine learning (ML) is a powerful and widely-used tool for making potentially important decisions,
from product recommendations to medical diagnosis. However, despite ML’s impressive performance,
it makes mistakes, with some more costly than others. As such, ML trust and safety is an important
theme [54, 36, 1]. While improving overall accuracy is an important goal that the bulk of the effort in
ML community has been focused on, it may not be enough: we need to also better understand the
strengths and limitations of ML techniques.

This work focuses on one such challenge: knowing whether a classiﬁer’s prediction for a test example
can be trusted or not. Such trust scores have practical applications. They can be directly shown to
users to help them gauge whether they should trust the AI system. This is crucial when a model’s
prediction inﬂuences important decisions such as a medical diagnosis, but can also be helpful even
in low-stakes scenarios such as movie recommendations. Trust scores can be used to override the
classiﬁer and send the decision to a human operator, or to prioritize decisions that human operators
should be making. Trust scores are also useful for monitoring classiﬁers to detect distribution shifts
that may mean the classiﬁer is no longer as useful as it was when deployed.

∗All authors contributed equally.
†Work done while intern at Google Research.
An open-source implementation of Trust Scores can be found here: https://github.com/google/TrustScore

32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.

A standard approach to deciding whether to trust a classiﬁer’s decision is to use the classiﬁers’ own
reported conﬁdence or score, e.g. probabilities from the softmax layer of a neural network, distance
to the separating hyperplane in support vector classiﬁcation, mean class probabilities for the trees in
a random forest. While using a model’s own implied conﬁdences appears reasonable, it has been
shown that the raw conﬁdence values from a classiﬁer are poorly calibrated [24, 32]. Worse yet,
even if the scores are calibrated, the ranking of the scores itself may not be reliable. In other words,
a higher conﬁdence score from the model does not necessarily imply higher probability that the
classiﬁer is correct, as shown in [45, 22, 39]. A classiﬁer may simply not be the best judge of its own
trustworthiness.

In this paper, we use a set of labeled examples (e.g. training data or validation data) to help determine
a classiﬁer’s trustworthiness for a particular testing example. First, we propose a simple procedure
that reduces the training data to a high density set for each class. Then we deﬁne the trust score—the
ratio between the distance from the testing sample to the nearest class different from the predicted
class and the distance to the predicted class—to determine whether to trust that classiﬁer prediction.

Theoretically, we show that high/low trust scores correspond to high probability of agree-
ment/disagreement with the Bayes-optimal classiﬁer. We show ﬁnite-sample estimation rates when
the data is full-dimension and supported on or near a low-dimensional manifold. Interestingly, we
attain bounds that depend only on the lower manifold dimension and independent of the ambient
dimension without any changes to the procedure or knowledge of the manifold. To our knowledge,
these results are new and may be of independent interest.

Experimentally, we found that the trust score better identiﬁes correctly-classiﬁed points for low and
medium-dimension feature spaces than the model itself. However, high-dimensional feature spaces
were more challenging, and we demonstrate that the trust score’s utility depends on the vector space
used to compute the trust score differences.

2 Related Work

One related line of work is that of conﬁdence calibration, which transforms classiﬁer outputs into
values that can be interpreted as probabilities, e.g. [44, 58, 40, 24]. In recent work, [32] explore the
structured prediction setting, and [33] obtain conﬁdence estimates by using ensembles of networks.
These calibration techniques typically only use the the model’s reported score (and the softmax
layer in the case of a neural network) for calibration, which notably preserves the rankings of the
classiﬁer scores. Similarly, [26] considered using the softmax probabilities for the related problem of
identifying misclassiﬁcations and mislabeled points.

Recent work explored estimating uncertainty for Bayesian neural networks and returning a distribution
over the outputs [20, 30]. The proposed trust score does not change the network structure (nor does
it assume any structure) and gives a single score, rather than a distribution over outputs as the
representation of uncertainty.

The problem of classiﬁcation with a reject option or learning with abstention [3, 57, 9, 23, 8, 27, 10]
is a highly related framework where the classiﬁer is allowed to abstain from making a prediction at
a certain cost. Typically such methods jointly learn the classiﬁer and the rejection function. Note
that the interplay between classiﬁcation rate and reject rate is studied in many various forms e.g.
[7, 13, 19, 48, 52, 18, 34, 14, 56, 51]. Our paper assumes an already trained and possibly black-box
classiﬁer and learns the conﬁdence scores separately, but we do not explicitly learn the appropriate
rejection thresholds.

Whether to trust a classiﬁer also arises in the setting where one has access to a sequence of classiﬁers,
but there is some cost to evaluating each classiﬁer, and the goal is to decide after evaluating each
classiﬁer in the sequence if one should trust the current classiﬁer decision enough to stop, rather than
evaluating more classiﬁers in the sequence (e.g. [55, 43, 16]). Those conﬁdence decisions are usually
based on whether the current classiﬁer score will match the classiﬁcation of the full sequence.

Experimentally we ﬁnd that the vector space used to compute the distances in the trust score matters,
and that computing trust scores on more-processed layers of a deep model generally works better.
This observation is similar to the work of Papernot and McDaniel [42], who use k-NN regression on
the intermediate representations of the network which they showed enhances robustness to adversarial
attacks and leads to better calibrated uncertainty estimations.

2

Our work builds on recent results in topological data analysis. Our method to ﬁlter low-density
points estimates a particular density level-set given a parameter α, which aims at ﬁnding the level-
set that contains 1 − α fraction of the probability mass. Level-set estimation has a long history
[25, 15, 53, 50, 46, 28]. However such works assume knowledge of the density level, which is
difﬁcult to determine in practice. We provide rates for Algorithm 1 in estimating the appropriate
level-set corresponding to α without knowledge of the level. The proxy α offers a more intuitive
parameter compared to the density value, is used for level-set estimation. Our analysis is also done
under various settings including when the data lies near a lower dimensional manifold and we provide
rates that depend only on the lower dimension.

3 Algorithm: The Trust Score

Our approach proceeds in two steps outlined in Algorithm 1 and 2. We ﬁrst pre-process the training
data, as described in Algorithm 1, to ﬁnd the α-high-density-set of each class, which is deﬁned as
the training samples within that class after ﬁltering out α-fraction of the samples with lowest density
(which may be outliers):
Deﬁnition 1 (α-high-density-set). Let 0 ≤ α < 1 and f be a continuous density function with
compact support X ⊆ RD. Then deﬁne Hα(f ), the α-high-density-set of f , to be the λα-level set of
f , deﬁned as {x ∈ X : f (x) ≥ λα} where λα := inf (cid:8)λ ≥ 0 : (cid:82)

X 1 [f (x) ≤ λ] f (x)dx ≥ α(cid:9).

In order to approximate the α-high-density-set, Algorithm 1 ﬁlters the α-fraction of the sample points
with lowest empirical density, based on k-nearest neighbors. This data ﬁltering step is independent of
the given classiﬁer h.

Then, the second step: given a testing sample, we deﬁne its trust score to be the ratio between the
distance from the testing sample to the α-high-density-set of the nearest class different from the
predicted class, and the distance from the test sample to the α-high-density-set of the class predicted
by h, as detailed in Algorithm 2. The intuition is that if the classiﬁer h predicts a label that is
considerably farther than the closest label, then this is a warning that the classiﬁer may be making a
mistake.

Our procedure can thus be viewed as a comparison to a modiﬁed nearest-neighbor classiﬁer, where
the modiﬁcation lies in the initial ﬁltering of points not in the α-high-density-set for each class.
Remark 1. The distances can be computed with respect to any representation of the data. For exam-
ple, the raw inputs, an unsupervised embedding of the space, or the activations of the intermediate
representations of the classiﬁer. Moreover, the nearest-neighbor distance can be replaced by other
distance measures, such as k-nearest neighbors or distance to a centroid.

Algorithm 1 Estimating α-high-density-set
Parameters: α (density threshold), k.
Inputs: Sample points X := {x1, .., xn} drawn from f .
Deﬁne k-NN radius rk(x) := inf{r > 0 : |B(x, r) ∩ X| ≥ k} and let ε := inf{r > 0 : |{x ∈ X :
rk(x) > r}| ≤ α · n}.
return (cid:99)Hα(f ) := {x ∈ X : rk(x) ≤ ε}.

Algorithm 2 Trust Score

Parameters: α (density threshold), k.
Input: Classiﬁer h : X → Y. Training data (x1, y1), ..., (xn, yn). Test example x.
For each (cid:96) ∈ Y, let (cid:99)Hα(f(cid:96)) be the output of Algorithm 1 with parameters α, k and sample points
{xj : 1 ≤ j ≤ n, yj = (cid:96)}. Then, return the trust score, deﬁned as:

ξ(h, x) := d

x, (cid:99)Hα(f

/d

x, (cid:99)Hα(fh(x))

(cid:17)

,

(cid:16)

(cid:16)

(cid:16)

(cid:17)
(cid:101)h(x))
(cid:17)

where (cid:101)h(x) = argminl∈Y,l(cid:54)=h(x) d

x, (cid:99)Hα(fl)

.

The method has two hyperparameters: k (the number of neighbors, such as in k-NN) and α (fraction
of data to ﬁlter) to compute the empirical densities. We show in theory that k can lie in a wide range

3

and still give us the desired consistency guarantees. Throughout our experiments, we ﬁx k = 10, and
use cross-validation to select α as it is data-dependent.
Remark 2. We observed that the procedure was not very sensitive to the choice of k and α. As will
be shown in the experimental section, for efﬁciency on larger datasets, we skipped the initial ﬁltering
step of Algorithm 1 (leading to a hyperparameter-free procedure) and obtained reasonable results.
This initial ﬁltering step can also be replaced by other strategies. One such example is ﬁltering
examples whose labels have high disagreement amongst its neighbors, which is implemented in the
open-source code release but not experimented with here.

4 Theoretical Analysis

In this section, we provide theoretical guarantees for Algorithms 1 and 2. Due to space constraints,
all the proofs are deferred to the Appendix. To simplify the main text, we state our results treating δ,
the conﬁdence level, as a constant. The dependence on δ in the rates is made explicit in the Appendix.

We show that Algorithm 1 is a statistically consistent estimator of the α-high-density-level set with
ﬁnite-sample estimation rates. We analyze Algorithm 1 in three different settings: when the data lies
on (i) a full-dimensional RD; (ii) an unknown lower dimensional submanifold embedded in RD; and
(iii) an unknown lower dimensional submanifold with full-dimensional noise.
For setting (i), where the data lies in RD, the estimation rate has a dependence on the dimension D,
which may be unattractive in high-dimensional situations: this is known as the curse of dimensionality,
suffered by density-based procedures in general. However, when the data has low intrinsic dimension
in (ii), it turns out that, remarkably, without any changes to the procedure, the estimation rate depends
on the lower dimension d and is independent of the ambient dimension D. However, in realistic
situations, the data may not lie exactly on a lower-dimensional manifold, but near one. This reﬂects
the setting of (iii), where the data essentially lies on a manifold but has general full-dimensional noise
so the data is overall full-dimensional. Interestingly, we show that we still obtain estimation rates
depending only on the manifold dimension and independent of the ambient dimension; moreover, we
do not require knowledge of the manifold nor its dimension to attain these rates.

We then analyze Algorithm 2, and establish the culminating result of Theorem 4: for labeled data
distributions with well-behaved class margins, when the trust score is large, the classiﬁer likely agrees
with the Bayes-optimal classiﬁer, and when the trust score is small, the classiﬁer likely disagrees with
the Bayes-optimal classiﬁer. If it turns out that even the Bayes-optimal classiﬁer has high-error in
a certain region, then any classiﬁer will have difﬁculties in that region. Thus, Theorem 4 does not
guarantee that the trust score can predict misclassiﬁcation, but rather that it can predict when the
classiﬁer is making an unreasonable decision.

4.1 Analysis of Algorithm 1

We require the following regularity assumptions on the boundaries of Hα(f ), which are standard
in analyses of level-set estimation [50]. Assumption 1.1 ensures that the density around Hα(f ) has
both smoothness and curvature. The upper bound gives smoothness, which is important to ensure
that our density estimators are accurate for our analysis (we only require this smoothness near the
boundaries and not globally). The lower bound ensures curvature: this ensures that Hα(f ) is salient
enough to be estimated. Assumption 1.2 ensures that Hα(f ) does not get arbitrarily thin anywhere.
Assumption 1 (α-high-density-set regularity). Let β > 0. There exists ˇCβ, ˆCβ, β, rc, r0, ρ > 0 s.t.

1. ˇCβ · d(x, Hα(f ))β ≤ |λα − f (x)| ≤ ˆCβ · d(x, Hα(f ))β for all x ∈ ∂Hα(f ) + B(0, rc).

2. For all 0 < r < r0 and x ∈ Hα(f ), we have Vol(B(x, r)) ≥ ρ · rD.

where ∂A denotes the boundary of a set A, d(x, A) := inf x(cid:48)∈A ||x−x(cid:48)||, B(x, r) := {x(cid:48) : |x−x(cid:48)| ≤
r} and A + B(0, r) := {x : d(x, A) ≤ r}.

Our statistical guarantees are under the Hausdorff metric, which ensures a uniform guarantee over
our estimator: it is a stronger notion of consistency than other common metrics [46, 47].
Deﬁnition 2 (Hausdorff distance). dH (A, B) := max{supx∈A d(x, B), supx∈B d(x, A)}.

4

We now give the following result for Algorithm 1. It says that as long as our density function satisﬁes
the regularity assumptions stated earlier, and the parameter k lies within a certain range, then we can
bound the Hausdorff distance between what Algorithm 1 recovers and Hα(f ), the true α-high-density
set, from an i.i.d. sample drawn from f of size n. Then, as n goes to ∞, and k grows as a function of
n, the quantity goes to 0.
Theorem 1 (Algorithm 1 guarantees). Let 0 < δ < 1 and suppose that f is continuous and has
compact support X ⊆ RD and satisﬁes Assumption 1. There exists constants Cl, Cu, C > 0
depending on f and δ such that the following holds with probability at least 1 − δ. Suppose that k
satisﬁes Cl · log n ≤ k ≤ Cu · (log n)D(2β+D) · n2β/(2β+D). Then we have

dH (Hα(f ), (cid:99)Hα(f )) ≤ C ·

(cid:16)

n−1/2D + log(n)1/2β · k−1/2β(cid:17)

.

Remark 3. The condition on k can be simpliﬁed by ignoring log factors: log n (cid:46) k (cid:46) n2β/(2β+D),
which is a wide range. Setting k to its allowed upper bound, we obtain our consistency guarantee of

dH (Hα(f ), (cid:99)Hα(f )) (cid:46) max{n−1/2D, n−1/(2β+D)}.
The ﬁrst term is due to the error from estimating the appropriate level given α (i.e. identifying the
level λα) and the second term corresponds to the error for recovering the level set given knowledge
of the level. The latter term matches the lower bound for level-set estimation up to log factors [53].

4.2 Analysis of Algorithm 1 on Manifolds

One of the disadvantages of Theorem 1 is that the estimation errors have a dependence on D, the
dimension of the data, which may be highly undesirable in high-dimensional settings. We next
improve these rates when the data has a lower intrinsic dimension. Interestingly, we are able to show
rates that depend only on the intrinsic dimension of the data, without explicit knowledge of that
dimension nor any changes to the procedure. As common to related work in the manifold setting, we
make the following regularity assumptions which are standard among works in manifold learning
(e.g. [41, 21, 2]).
Assumption 2 (Manifold Regularity). M is a d-dimensional smooth compact Riemannian manifold
without boundary embedded in compact subset X ⊆ RD with bounded volume. M has ﬁnite
condition number 1/τ , which controls the curvature and prevents self-intersection.
Theorem 2 (Manifold analogue of Theorem 1). Let 0 < δ < 1. Suppose that density function f
is continuous and supported on M and Assumptions 1 and 2 hold. Suppose also that there exists
λ0 > 0 such that f (x) ≥ λ0 for all x ∈ M . Then, there exists constants Cl, Cu, C > 0 depending
on f and δ such that the following holds with probability at least 1 − δ. Suppose that k satisﬁes
Cl · log n ≤ k ≤ Cu · (log n)d(2β(cid:48)+d) · n2β(cid:48)/(2β(cid:48)+d). where β(cid:48) := max{1, β}. Then we have
n−1/2d + log(n)1/2β · k−1/2β(cid:17)
(cid:16)

dH (Hα(f ), (cid:99)Hα(f )) ≤ C ·

.

Remark 4. Setting k to its allowed upper bound, we obtain (ignoring log factors),

dH (Hα(f ), (cid:99)Hα(f )) (cid:46) max{n−1/2d, n−1/(2 max{1,β}+d)}.
The ﬁrst term can be compared to that of the previous result where D is replaced with d. The second
term is the error for recovering the level set on manifolds, which matches recent rates [28].

4.3 Analysis of Algorithm 1 on Manifolds with Full Dimensional Noise

In realistic settings, the data may not lie exactly on a low-dimensional manifold, but near one. We
next present a result where the data is distributed along a manifold with additional full-dimensional
noise. We make mild assumptions on the noise distribution. Thus, in this situation, the data has
intrinsic dimension equal to the ambient dimension. Interestingly, we are still able to show that the
rates only depend on the dimension of the manifold and not the dimension of the entire data.
Theorem 3. Let 0 < η < α < 1 and 0 < δ < 1. Suppose that distribution F is a weighted
mixture (1 − η) · FM + η · FE where FM is a distribution with continous density fM supported on a
d-dimensional manifold M satisfying Assumption 2 and FE is a (noise) distribution with continuous
density fE with compact support over RD with d < D. Suppose also that there exists λ0 > 0 such

5

(cid:101)α(fM ) (where (cid:101)α := α−η
1−η ) satisﬁes Assumption 1 for density
that fM (x) ≥ λ0 for all x ∈ M and H
fM . Let (cid:98)Hα be the output of Algorithm 1 on a sample X of size n drawn i.i.d. from F. Then, there
exists constants Cl, Cu, C > 0 depending on fM , fE, η, M and δ such that the following holds with
probability at least 1 − δ. Suppose that k satisﬁes Cl · log n ≤ k ≤ Cu · (log n)d(2β(cid:48)+d) · n2β(cid:48)/(2β(cid:48)+d),
where β(cid:48) := max{1, β}. Then we have

dH (H

(cid:101)α(fM ), (cid:99)Hα) ≤ C ·

(cid:16)

n−1/2d + log(n)1/2β · k−1/2β(cid:17)

.

The above result is compelling because it shows why our methods can work, even in high-dimensions,
despite the curse of dimensionality of non-parametric methods. In typical real-world data, even if
the data lies in a high-dimensional space, there may be far fewer degrees of freedom. Thus, our
theoretical results suggest that when this is true, then our methods will enjoy far better convergence
rates – even when the data overall has full intrinsic dimension due to factors such as noise.

4.4 Analysis of Algorithm 2: the Trust Score

We now provide a guarantee about the trust score, making the same assumptions as in Theorem 3 for
each of the label distributions. We additionally assume that the class distributions are well-behaved
in the following sense: that high-density-regions for each of the classes satisfy the property that for
any point x ∈ X , if the ratio of the distance to one class’s high-density-region to that of another is
smaller by some margin γ, then it is more likely that x’s label corresponds to the former class.
Theorem 4. Let 0 < η < α < 1. Let us have labeled data (x1, y1), ..., (xn, yn) drawn from
distribution D, which is a joint distribution over X × Y where Y are the labels, |Y| < ∞, and
X ⊆ RD is compact. Suppose for each (cid:96) ∈ Y, the conditional distribution for label (cid:96) satisﬁes the
conditions of Theorem 3 for some manifold and noise level η. Let fM,(cid:96) be the density of the portion of
1−η and
the conditional distribution for label (cid:96) supported on M . Deﬁne M(cid:96) := H
let (cid:15)n be the maximum Hausdorff error from estimating M(cid:96) over each (cid:96) ∈ Y in Theorem 3. Assume
that min(cid:96)∈Y PD(y = (cid:96)) > 0 to ensure we have samples from each label.
Suppose also that for each x ∈ X , if d(x, Mi)/d(x, Mj) < 1 − γ then P(y = i|x) > P(y = j|x)
for i, j ∈ Y. That is, if we are closer to Mi than Mj by a ratio of less than 1 − γ, then the
point is more likely to be from class i. Let h∗ be the Bayes-optimal classiﬁer, deﬁned by h∗(x) :=
P(y = (cid:96)|x). Then the trust score ξ of Algorithm 2 satisﬁes the following with high
argmax(cid:96)∈Y
probability uniformly over all x ∈ X and all classiﬁers h : X → Y simultaneously for n sufﬁciently
large depending on D:

(cid:101)α(f(cid:96)), where (cid:101)α := α−η

ξ(h, x) < 1 − γ −

(cid:15)n
d(x, Mh(x)) + (cid:15)n

(cid:32) d(x, M

(cid:101)h(x))
d(x, Mh(x))

1
ξ(h, x)

< 1 − γ −

(cid:15)n
(cid:101)h(x)) + (cid:15)n

d(x, M

d(x, Mh(x))
(cid:101)h(x))
d(x, M

·

·

(cid:32)

(cid:33)

(cid:33)

+ 1

⇒ h(x) (cid:54)= h∗(x),

+ 1

⇒ h(x) = h∗(x).

5 Experiments

In this section, we empirically test whether trust scores can both detect examples that are incorrectly
classiﬁed with high precision and be used as a signal to determine which examples are likely correctly
classiﬁed. We perform this evaluation across (i) different datasets (Sections 5.1 and 5.3), (ii) different
families of classiﬁers (neural network, random forest and logistic regression) (Section 5.1), (iii)
classiﬁers with varying accuracy on the same task (Section 5.2) and (iv) different representations of
the data e.g. input data or activations of various intermediate layers in neural network (Section 5.3).

First, we test if testing examples with high trust score corresponds to examples in which the model is
correct ("identifying trustworthy examples"). Each method produces a numeric score for each testing
example. For each method, we bin the data points by percentile value of the score (i.e. 100 bins).
Given a recall percentile level (i.e. the x-axis on our plots), we take the performance of the classiﬁer
on the bins above the percentile level as the precision (i.e. the y-axis). Then, we take the negative of
each signal and test if low trust score corresponds to the model being wrong ("identifying suspicious

6

Figure 1: Two example datasets and models. For predicting correctness (top row) the vertical dotted
black line indicates error level of the trained classiﬁer. For predicting incorrectness (bottom) the
vertical black dotted line is the accuracy rate of the classiﬁer. For detecting trustworthy, for each
percentile level, we take the test examples whose trust score was above that percentile level and plot
the percentage of those test points that were correctly classiﬁed by the classiﬁer, and do the same
model conﬁdence and 1-nn ratio. For detecting suspicious, we take the negative of each signal and
plot the precision of identifying incorrectly classiﬁed examples. Shown are average of 20 runs with
shaded standard error band. The trust score consistently attains a higher precision for each given
percentile of classiﬁer decision-rejection. Furthermore, the trust score generally shows increasing
precision as the percentile level increases, but surprisingly, many of the comparison baselines do not.
See the Appendix for the full results.

examples"). Here the y-axis is the misclassiﬁcation rate and the x-axis corresponds to decreasing
trust score or model conﬁdence.

In both cases, the higher the precision vs percentile curve, the better the method. The vertical black
dotted lines in the plots represent the omniscient ideal. For identifying trustworthy examples it is the
error rate of the classiﬁer and for identifying suspicious examples" it is the accuracy rate.

The baseline we use in Section is the model’s own conﬁdence score, which is similar to the approach
of [26]. While calibrating the classiﬁers’ conﬁdence scores (i.e. transforming them into probability
estimates of correctness) is an important related work [24, 44], such techniques typically do not
change the rankings of the score, at least in the binary case. Since we evaluate the trust score on
its precision at a given recall percentile level, we are interested in the relative ranking of the scores
rather than their absolute values. Thus, we do not compare against calibration techniques. There
are surprisingly few methods aimed at identifying correctly or incorrectly classiﬁed examples with
precision at a recall percentile level as noted in [26].

Choosing Hyperparameters: The two hyperparameters for the trust score are α and k. Throughout
the experiments, we ﬁx k = 10 and choose α using cross-validation over (negative) powers of 2 on
the training set. The metric for cross-validation was optimal performance on detecting suspicious
examples at the percentile corresponding to the classiﬁer’s accuracy. The bulk of the computational
cost for the trust-score is in k-nearest neighbor computations for training and 1-nearest neighbor
searches for evaluation. To speed things up for the larger datasets MNIST, SVHN, CIFAR-10
and CIFAR-100, we skipped the initial ﬁltering step of Algorithm 1 altogether and reduced the
intermediate layers down to 20 dimensions using PCA before being processed by the trust score
which showed similar performance. We note that any approximation method (such as approximate
instead of exact nearest neighbors) could have been used instead.

5.1 Performance on Benchmark UCI Datasets

In this section, we show performance on ﬁve benchmark UCI datasets [17], each for three kinds
of classiﬁers (neural network, random forest and logistic regression). Due to space, we only show

7

Figure 2: We show the performance of trust score on the Digits dataset for a neural network as we
increase the accuracy. As we go from left to right, we train the network with more iterations (each
with batch size 50) thus increasing the accuracy indicated by the dotted vertical lines. While the trust
score still performs better than model conﬁdence, the amount of improvement diminishes.

two data sets and two models in Figure 1. The rest can be found in the Appendix. For each method
and dataset, we evaluated with multiple runs. For each run we took a random stratiﬁed split of the
dataset into two halves. One portion was used for training the trust score and the other was used for
evaluation and the standard error is shown in addition to the average precision across the runs at each
percentile level. The results show that our method consistently has a higher precision vs percentile
curve than the rest of the methods across the datasets and models. This suggests the trust score
considerably improves upon known methods as a signal for identifying trustworthy and suspicious
testing examples for low-dimensional data.

In addition to the model’s own conﬁdence score, we try one additional baseline, which we call the
nearest neighbor ratio (1-nn ratio). It is the ratio between the 1-nearest neighbor distance to the
closest and second closest class, which can be viewed as an analogue to the trust score without
knowledge of the classiﬁer’s hard prediction.

5.2 Performance as Model Accuracy Varies

In Figure 2, we show how the performance of trust score changes as the accuracy of the classiﬁer
changes (averaged over 20 runs for each condition). We observe that as the accuracy of the model
increases, while the trust score still performs better than model conﬁdence, the amount of improvement
diminishes. This suggests that as the model improves, the information trust score can provide in
addition to the model conﬁdence decreases. However, as we show in Section 5.3, the trust score
can still have added value even when the classiﬁer is known to be of high performance on some
benchmark larger-scale datasets.

5.3 Performance on MNIST, SVHN, CIFAR-10 and CIFAR-100 Datasets

The MNIST handwritten digit dataset [35] consists of 60,000 28×28-pixel training images and
10,000 testing images in 10 classes. The SVHN dataset [38] consists of 73,257 32×32-pixel colour
training images and 26,032 testing images and also has 10 classes. The CIFAR-10 and CIFAR-100
datasets [31] both consist of 60,000 32×32-pixel colour images, with 50,000 training images and
10,000 test images. The CIFAR-10 and CIFAR-100 datasets are split evenly between 10 classes and
100 classes respectively.

8

(a) MNIST

(b) SVHN

(c) CIFAR-10

(d) MNIST

(e) SVHN

(f) CIFAR-10

Figure 3: Trust score results using convolutional neural networks on MNIST, SVHN, and CIFAR-10
datasets. Top row is detecting trustworthy; bottom row is detecting suspicious. Full chart with
CIFAR-100 (which was essentially a negative result) is shown in the Appendix.

We used a pretrained VGG-16 [49] architecture with adaptation to the CIFAR datasets based on [37].
The CIFAR-10 VGG-16 network achieves a test accuracy of 93.56% while the CIFAR-100 network
achieves a test accuracy of 70.48%. We used pretrained, smaller CNNs for MNIST and SVHN. The
MNIST network achieves a test accuracy of 99.07% and the SVHN network achieves a test accuracy
of 95.45%. All architectures were implemented in Keras [6].

One simple generalization of our method is to use intermediate layers of a neural network as an
input instead of the raw x. Many prior work suggests that a neural network may learn different
representations of x at each layer. As input to the trust score, we tried using 1) the logit layer, 2) the
preceding fully connected layer with ReLU activation, 3) this fully connected layer, which has 128
dimensions in the MNIST network and 512 dimensions in the other networks, reduced down to 20
dimensions from applying PCA.

The trust score results on various layers are shown in Figure 3. They suggest that for high dimensional
datasets, the trust score may only provide little or no improvement over the model conﬁdence at
detecting trustworthy and suspicious examples. All plots were made using α = 0; using cross-
validation to select a different α did not improve trust score performance. We also did not see much
difference from using different layers.
Conclusion:
In this paper, we provide the trust score: a new, simple, and effective way to judge if one should trust
the prediction from a classiﬁer. The trust score provides information about the relative positions of
the datapoints, which may be lost in common approaches such as the model conﬁdence when the
model is trained using SGD. We show high-probability non-asymptotic statistical guarantees that
high (low) trust scores correspond to agreement (disagreement) with the Bayes-optimal classiﬁer
under various nonparametric settings, which build on recent results in topological data analysis. Our
empirical results across many datasets, classiﬁers, and representations of the data show that our
method consistently outperforms the classiﬁer’s own reported conﬁdence in identifying trustworthy
and suspicious examples in low to mid dimensional datasets. The theoretical and empirical results
suggest that this approach may have important practical implications in low to mid dimension
settings.

https://github.com/geifmany/cifar-vgg
https://github.com/EN10/KerasMNIST
https://github.com/tohinz/SVHN-Classiﬁer

9

References

[1] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul F Christiano, John Schulman, and Dan Mané.
Concrete problems in AI safety. CoRR, abs/1606.06565, 2016. URL http://arxiv.org/
abs/1606.06565.

[2] Sivaraman Balakrishnan, Srivatsan Narayanan, Alessandro Rinaldo, Aarti Singh, and Larry
Wasserman. Cluster trees on manifolds. In Advances in Neural Information Processing Systems,
pages 2679–2687, 2013.

[3] Peter L Bartlett and Marten H Wegkamp. Classiﬁcation with a reject option using a hinge loss.

Journal of Machine Learning Research, 9(Aug):1823–1840, 2008.

[4] Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for the cluster tree.

In

Advances in Neural Information Processing Systems, pages 343–351, 2010.

[5] Frédéric Chazal. An upper bound for the volume of geodesic balls in submanifolds of Euclidean
spaces. https://geometrica.saclay.inria.fr/team/Fred.Chazal/BallVolumeJan2013.pdf, 2013.

[6] François Chollet et al. Keras. 2015.

[7] C Chow. On optimum recognition error and reject tradeoff. IEEE Transactions on Information

Theory, 16(1):41–46, 1970.

[8] Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri. Boosting with abstention. In Advances in

Neural Information Processing Systems, pages 1660–1668, 2016.

[9] Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri. Learning with rejection. In International

Conference on Algorithmic Learning Theory, pages 67–82. Springer, 2016.

[10] Corinna Cortes, Giulia DeSalvo, Claudio Gentile, Mehryar Mohri, and Scott Yang. Online

learning with abstention. arXiv preprint arXiv:1703.03478, 2017.

[11] Sanjoy Dasgupta and Samory Kpotufe. Optimal rates for k-NN density and mode estimation.

In Advances in Neural Information Processing Systems, pages 2555–2563, 2014.

[12] Luc Devroye, Laszlo Gyorﬁ, Adam Krzyzak, and Gábor Lugosi. On the strong universal
consistency of nearest neighbor regression function estimates. The Annals of Statistics, pages
1371–1385, 1994.

[13] Bernard Dubuisson and Mylene Masson. A statistical decision rule with incomplete knowledge

about classes. Pattern Recognition, 26(1):155–165, 1993.

[14] Ran El-Yaniv and Yair Wiener. On the foundations of noise-free selective classiﬁcation. Journal

of Machine Learning Research, 11(May):1605–1641, 2010.

[15] Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, et al. A density-based algorithm

for discovering clusters in large spatial databases with noise. In Kdd, pages 226–231, 1996.

[16] Wei Fan, Fang Chu, Haixun Wang, and Philip S. Yu. Pruning and dynamic scheduling of

cost-sensitive ensembles. AAAI, 2002.

[17] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The Elements of Statistical Learning.

Springer, 2001.

[18] Giorgio Fumera and Fabio Roli. Support vector machines with embedded reject option. In

Pattern Recognition with Support Vector Machines, pages 68–82. Springer, 2002.

[19] Giorgio Fumera, Fabio Roli, and Giorgio Giacinto. Multiple reject thresholds for improving
classiﬁcation reliability. In Joint IAPR International Workshops on Statistical Techniques in
Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR), pages
863–871. Springer, 2000.

[20] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing
model uncertainty in deep learning. In International Conference on Machine Learning, pages
1050–1059, 2016.

10

[21] Christopher Genovese, Marco Perone-Paciﬁco, Isabella Verdinelli, and Larry Wasserman.
Minimax manifold estimation. Journal of Machine Learning Research, 13(May):1263–1291,
2012.

[22] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversar-

ial examples. arXiv preprint arXiv:1412.6572, 2014.

[23] Yves Grandvalet, Alain Rakotomamonjy, Joseph Keshet, and Stéphane Canu. Support vector
machines with a reject option. In Advances in Neural Information Processing Systems, pages
537–544, 2009.

[24] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural

networks. arXiv preprint arXiv:1706.04599, 2017.

[25] John A Hartigan. Clustering algorithms. 1975.

[26] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassiﬁed and out-of-distribution

examples in neural networks. arXiv preprint arXiv:1610.02136, 2016.

[27] Radu Herbei and Marten H Wegkamp. Classiﬁcation with reject option. Canadian Journal of

Statistics, 34(4):709–721, 2006.

[28] Heinrich Jiang. Density level set estimation on manifolds with DBSCAN. In International

Conference on Machine Learning, pages 1684–1693, 2017.

[29] Heinrich Jiang. Uniform convergence rates for kernel density estimation. In International

Conference on Machine Learning, pages 1694–1703, 2017.

[30] Alex Kendall and Yarin Gal. What uncertainties do we need in Bayesian deep learning for
computer vision? In Advances in Neural Information Processing Systems, pages 5580–5590,
2017.

[31] Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.

[32] Volodymyr Kuleshov and Percy S Liang. Calibrated structured prediction. In Advances in

Neural Information Processing Systems, pages 3474–3482, 2015.

[33] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable
predictive uncertainty estimation using deep ensembles. In Advances in Neural Information
Processing Systems, pages 6405–6416, 2017.

[34] Thomas CW Landgrebe, David MJ Tax, Pavel Paclík, and Robert PW Duin. The interaction
between classiﬁcation and reject performance for distance-based reject-option classiﬁers. Pattern
Recognition Letters, 27(8):908–917, 2006.

[35] Yann LeCun. The MNIST database of handwritten digits. http://yann. lecun. com/exdb/mnist/,

1998.

factors, 46(1):50–80, 2004.

[36] John D Lee and Katrina A See. Trust in automation: Designing for appropriate reliance. Human

[37] Shuying Liu and Weihong Deng. Very deep convolutional neural network based image classiﬁca-
tion using small training sample size. 2015 3rd IAPR Asian Conference on Pattern Recognition
(ACPR), pages 730–734, 2015.

[38] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng.

Reading digits in natural images with unsupervised feature learning. 2011.

[39] Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High
conﬁdence predictions for unrecognizable images. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 427–436, 2015.

[40] Alexandru Niculescu-Mizil and Rich Caruana. Predicting good probabilities with supervised
learning. In Proceedings of the 22nd International Conference on Machine Learning, pages
625–632. ACM, 2005.

11

[41] Partha Niyogi, Stephen Smale, and Shmuel Weinberger. Finding the homology of submanifolds
with high conﬁdence from random samples. Discrete & Computational Geometry, 39(1-3):
419–441, 2008.

[42] Nicolas Papernot and Patrick McDaniel. Deep k-nearest neighbors: Towards conﬁdent, inter-

pretable and robust deep learning. arXiv preprint arXiv:1803.04765, 2018.

[43] Nathan Parrish, Hyrum S. Anderson, Maya R. Gupta, and Dun Yu Hsaio. Classifying with
conﬁdence from incomplete information. Journal of Machine Learning Research, 14(December):
3561–3589, 2013.

[44] John Platt. Probabilistic outputs for support vector machines and comparisons to regularized

likelihood methods. Advances in Large Margin Classiﬁers, 10(3):61–74, 1999.

[45] Foster J Provost, Tom Fawcett, and Ron Kohavi. The case against accuracy estimation for

comparing induction algorithms. In ICML, volume 98, pages 445–453, 1998.

[46] Philippe Rigollet, Régis Vert, et al. Optimal rates for plug-in estimators of density level sets.

Bernoulli, 15(4):1154–1178, 2009.

Statistics, 38(5):2678–2722, 2010.

[47] Alessandro Rinaldo and Larry Wasserman. Generalized density clustering. The Annals of

[48] Carla M Santos-Pereira and Ana M Pires. On optimal reject rules and ROC curves. Pattern

Recognition Letters, 26(7):943–952, 2005.

[49] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale

image recognition. arXiv preprint arXiv:1409.1556, 2014.

[50] Aarti Singh, Clayton Scott, Robert Nowak, et al. Adaptive Hausdorff estimation of density level

sets. The Annals of Statistics, 37(5B):2760–2782, 2009.

[51] David MJ Tax and Robert PW Duin. Growing a multi-class classiﬁer with a reject option.

Pattern Recognition Letters, 29(10):1565–1570, 2008.

[52] Francesco Tortorella. An optimal reject rule for binary classiﬁers. In Joint IAPR International
Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic
Pattern Recognition (SSPR), pages 611–620. Springer, 2000.

[53] Alexandre B Tsybakov et al. On nonparametric estimation of density level sets. The Annals of

Statistics, 25(3):948–969, 1997.

[54] Kush R Varshney and Homa Alemzadeh. On the safety of machine learning: Cyber-physical

systems, decision sciences, and data products. Big data, 5(3):246–255, 2017.

[55] Joseph Wang, Kirill Trapeznikov, and Venkatesh Saligrama. Efﬁcient learning by directed
acyclic graph for resource constrained prediction. Advances in Neural Information Processing
Systems (NIPS), 2015.

[56] Yair Wiener and Ran El-Yaniv. Agnostic selective classiﬁcation.

In Advances in Neural

Information Processing Systems, pages 1665–1673, 2011.

[57] Ming Yuan and Marten Wegkamp. Classiﬁcation methods with reject option based on convex

risk minimization. Journal of Machine Learning Research, 11(Jan):111–130, 2010.

[58] Bianca Zadrozny and Charles Elkan. Transforming classiﬁer scores into accurate multiclass
probability estimates. In Proceedings of the Eighth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages 694–699. ACM, 2002.

12

Appendix

A Supporting results for Theorem 1 Proof

We need the following result giving guarantees on the empirical balls.
Lemma 1 (Uniform convergence of balls [4]). Let F be the distribution corresponding to f and
Fn be the empirical distribution corresponding to the sample X. Pick 0 < δ < 1. Assume that
k ≥ D log n. Then with probability at least 1 − δ, for every ball B ⊂ RD we have

F(B) ≥ Cδ,n

⇒ Fn(B) > 0

F(B) ≥

+ Cδ,n

⇒ Fn(B) ≥

√

D log n
n

√

k
n
√
k
n

k
n
k
n

F(B) ≤

− Cδ,n

⇒ Fn(B) <

,

√

k
n
k
n

D log n

where Cδ,n = 16 log(2/δ)
Remark 5. For the rest of the paper, many results are qualiﬁed to hold with probability at least 1 − δ.
This is precisely the event in which Lemma 1 holds.
Remark 6. If δ = 1/n, then Cδ,n = O((log n)3/2).

To analyze Algorithm 1, we use the k-NN density estimator[12], deﬁned below.
Deﬁnition 3. Deﬁne the k-NN radius of x ∈ RD as

rk(x) := inf{r > 0 : |X ∩ B(x, r)| ≥ k},

Deﬁnition 4 (k-NN Density Estimator).

where vD is the volume of a unit ball in RD.

fk(x) :=

k
n · vD · rk(x)D ,

We will use bounds on the k-NN density estimator from [11], which are repeated here.

Deﬁne the following one-sided modulus of continuity which characterizes how much the density
increases locally:

ˆr((cid:15), x) := sup

r :

f (x(cid:48)) − f (x) ≤ (cid:15)

.

sup
x(cid:48)∈B(x,r)

δ,n. Then with probability at least 1 − δ, the

Lemma 2 (Lemma 3 of [11]). Suppose that k ≥ 4C 2
following holds for all x ∈ RD and (cid:15) > 0.
(cid:18)

(cid:19)

provided k satisﬁes vD · ˆr(x, (cid:15))D · (f (x) + (cid:15)) ≥ k

fk(x) <

1 + 2

(f (x) + (cid:15)),

Cδ,n√
k

n + Cδ,n

√
k
n .

Analogously, deﬁne the following which characterizes how much the density decreases locally:

(cid:40)

(cid:40)

(cid:41)

(cid:41)

ˇr((cid:15), x) := sup

r :

f (x) − f (x(cid:48)) ≤ (cid:15)

.

sup
x(cid:48)∈B(x,r)

δ,n. Then with probability at least 1 − δ, the

Lemma 3 (Lemma 4 of [11]). Suppose that k ≥ 4C 2
following holds for all x ∈ RD and (cid:15) > 0.
(cid:18)

(cid:19)

fk(x) ≥

1 − 2

(f (x) − (cid:15)),

provided k satisﬁes vD · ˇr(x, (cid:15))D · (f (x) − (cid:15)) ≥ k

n − Cδ,n

√
k
n .

Cδ,n√
k

13

B Proof of Theorem 1

In this section, we assume the conditions of Theorem 1. We ﬁrst show that λα, that is the density
level corresponding to the α-high-density-set, is smooth in α.
Lemma 4. There exists constants C1, r1 > 0 depending on f such that the following holds for all
0 < (cid:15) < r1 such that

0 < λα − λα−(cid:15) ≤ C1(cid:15)β/D and 0 < λα+(cid:15) − λα ≤ C1(cid:15)β/D.

Proof. We have
(cid:90)

X

(cid:15) =

1[λα−(cid:15) < f (x) ≤ λα] · f (x)dx ≥ λα−(cid:15)

1[λα−(cid:15) < f (x) ≤ λα]dx,

(cid:90)

X

where the ﬁrst equality holds by deﬁnition. Choosing (cid:15) sufﬁciently small such that Assumption 1
holds, we have

λα−(cid:15)

1[λα−(cid:15) < f (x) ≤ λα]dx

(cid:90)

X

≥ λα−(cid:15) · Vol

Hα(x) + B

(cid:16)(cid:16)

(cid:16)

0, ((λα−(cid:15) − λα)/ (cid:98)Cβ)1/β(cid:17)(cid:17)

(cid:17)

\Hα(f )

≥ λα−(cid:15) · C (cid:48)((λα−(cid:15) − λα)/ (cid:98)Cβ)D/β,

where the last inequality holds for some constant C (cid:48) depending on f and Vol is the volume w.r.t. to
the Lebesgue measure in RD. It then follows that

λα−(cid:15) − λα ≤ (cid:98)Cβ

(cid:18)

(cid:15)
λα−(cid:15) · C (cid:48)

(cid:19)β/D

,

and the result for the ﬁrst part follows by taking C1 ≤ (cid:98)Cβ · (λα−r1 · C (cid:48))−β/D and r1 < α. Showing
that 0 < λα+(cid:15) − λα ≤ C1(cid:15)β/D can be done analogously and is omitted here.

The next result gets a handle on the density level corresponding to α returned by Algorithm 1.
Lemma 5. Let 0 < δ < 1. Let (cid:98)ε be the ε setting chosen by Algorithm 1. Deﬁne

(cid:99)λα :=

k

vD · n · (cid:98)εD .

Then, with probability at least 1 − δ, we have there exist constant C1 > 0 depending on f such that
for n sufﬁciently large depending on f , we have

|(cid:98)λα − λα| ≤ C1

(cid:32)(cid:114)





log(1/δ)
n

(cid:33)β/D

+

log(1/δ)
√
k

√

log n



 .

Proof. Let ˜α > 0. Then, we have that if x ∼ f , then P(x ∈ H ˜α(f )) = 1 − ˜α. Thus, the probability
that a sample point falls in H ˜α(f ) is a Bernoulli random variable with probability 1 − ˜α. Hence, by
Hoeffding’s inequality, we have that there exist constant C (cid:48) > 0 such that

(cid:32)

P

1 − (cid:101)α − C (cid:48)

(cid:114)

log(1/δ)
n

≤

|H ˜α(f ) ∩ X|
n

≤ 1 − (cid:101)α + C (cid:48)

(cid:114)

(cid:33)

log(1/δ)
n

≥ 1 − δ/4.

Then it follows that choosing αU := α + C (cid:48)

Similarly, choosing αL = α − C (cid:48)

gives us

P

(cid:18) |HαU (f ) ∩ X|
n
(cid:113) log(1/δ)
n

P

(cid:18) |HαL(f ) ∩ X|
n

(cid:113) log(1/δ)
n

we get

(cid:19)

≤ 1 − α

≥ 1 − δ/4.

(cid:19)

≥ 1 − α

≥ 1 − δ/4.

14

Next, deﬁne

H upper
α

(f ) := {x ∈ X : fk(x) ≥ λα − (cid:15)} ,

where (cid:15) > 0 will be chosen later in order for (cid:99)Hα(f ) ⊆ H upper
(f ). By Lemma 4, there exists
C2, r1 > 0 depending on f such that for (cid:98)ε < r1 (which holds for n sufﬁciently large depending on
(cid:19)β/D
≤ λαL . As such, it sufﬁces to choose (cid:15) such
f by Lemma 1), we have λα − C2

(cid:18)(cid:113) log(1/δ)

α

n

that for all x ∈ X such that if f (x) ≥ λα − C2

then fk(x) ≥ λα − (cid:15). This is

because {x ∈ X : fk(x) ≥ λα − (cid:15)} would contain HαL (f ) ∩ X, which we showed earlier contains

(cid:18)(cid:113) log(1/δ)

(cid:19)β/D

n

at least 1 − α fraction of the samples. Deﬁne (cid:15)0 such that (cid:15) = C2

+ (cid:15)0 We have by

(cid:18)(cid:113) log(1/δ)

(cid:19)β/D

n

Assumption 1,

ˇr(x, (cid:15)0) ≥

(cid:15)0 + C2







1
ˆCβ

(cid:32)(cid:114)

(cid:33)β/D




log(1/δ)
n



1/β





(cid:32)(cid:114)

−



C2

1
ˇCβ

log(1/δ)
n



1/β

(cid:33)β/D




.

Then, there exists constant C (cid:48)(cid:48) > 0 sufﬁciently large depending on f such that if

(cid:15)0 ≥ C (cid:48)(cid:48)

(cid:32)(cid:114)





log(1/δ)
n

(cid:33)β/D

+

log(1/δ)
√
k

√

log n





then the conditions in Lemma 3 are satisﬁed for n sufﬁciently large. Thus, we have for all x ∈ X

with f (x) ≥ α − C2

(cid:18)(cid:113) log(1/δ)

(cid:19)β/D

n

, then fk(x) ≥ α − (cid:15). Hence, (cid:99)Hα(f ) ⊆ H upper

α

(f ).

We now do the same in the other direction. Deﬁne

H lower
α

(f ) := {x ∈ X : fk(x) ≥ λα + (cid:15)} ,

where (cid:15) will be chosen such that H lower

α

(f ) ⊆ (cid:99)Hα(f ). By Lemma 4, it sufﬁces to show that if

fk(x) ≥ λα + (cid:15) then f (x) ≥ λα + C2

. This direction follows a similar argument

(cid:18)(cid:113) log(1/δ)

(cid:19)β/D

n

Thus, there exists a constant C1 > 0 depending on f such that for n sufﬁciently large depending on
f , we have:

|(cid:98)λα − λα| ≤ C1

(cid:32)(cid:114)





log(1/δ)
n

(cid:33)β/D

+

log(1/δ)
√
k

√

log n



 ,

as the previous.

as desired.

The next result bounds (cid:99)Hα(f ) between two level sets of f .
Lemma 6. Let 0 < δ < 1. There exists constant C1 > 0 depending on f such that the following
holds with probability at least 1 − δ for n sufﬁciently large depending on f . Deﬁne
√





(cid:33)β/D

(cid:32)(cid:114)

H U

α (f ) :=

x ∈ X : f (x) ≥ λα − C1







log(1/δ)
n

log(1/δ)
n

(cid:32)(cid:114)

(cid:33)β/D

+

log(1/δ)
√
k

log n

√

log n

+

log(1/δ)
√
k
















.

H L

α (f ) :=

x ∈ X : f (x) ≥ λα + C1










Then,

H L

α (f ) ∩ X ⊆ (cid:99)Hα(f ) ⊆ H U

α (f ) ∩ X.

15

Proof. To simplify notation, let us deﬁne the following:

K(n, k, δ) :=

(cid:32)(cid:114)





log(1/δ)
n

(cid:33)β/D

+

log(1/δ)
√
k

√

log n



 .

By Lemma 5, there exists C2 > 0 such that deﬁning

(cid:100)H U

α (f ) := {x ∈ X : fk(x) ≥ λα − C2 · K(n, k, δ)}

(cid:100)H L

α (f ) := {x ∈ X : fk(x) ≥ λα + C2 · K(n, k, δ)} ,

then we have

α (f ).
It sufﬁces to show that there exists a constant C1 > 0 such that

α (f ) ⊆ (cid:99)Hα(f ) ⊆ (cid:100)H U

(cid:100)H L

H L

α (f ) ∩ X ⊆ (cid:100)H L

α (f ) and (cid:100)H U

α (f ) ⊆ H U

α (f ) ∩ X.

α (f ) ∩ X ⊆ (cid:100)H L

We start by showing H L
α(f ). To do this, show that for any x ∈ X satisfying
f (x) ≥ λα + C1 · K(n, k, δ) + (cid:15) satisﬁes fk(x) ≥ λα + C1 · K(n, k, δ), where (cid:15) > 0 will be chosen
later. By a similar argument as in the proof of Lemma 5, we can choose (cid:15) ≥ C (cid:48) · K(n, k, δ) for some
constant C (cid:48) > 0 and the desired result holds for n sufﬁciently large. Similarly, there exists C (cid:48)(cid:48) > 0
such that fk(x) ≤ λα − (C1 + C (cid:48)(cid:48)) · K(n, k, δ) implies that f (x) ≤ λα − C1 · K(n, k, δ). The result
follows by taking C2 = C1 + max{C (cid:48), C (cid:48)(cid:48)}.

We are now ready to prove Theorem 5, a more general version of Theorem 1 which makes the
dependence on δ explicit. Note that if δ = 1/n, then log(1/δ) = log(n).
Theorem 5. [Extends Theorem 1] Let 0 < δ < 1 and suppose that f is continuous and has compact
support X ⊆ RD and satisﬁes Assumption 1. There exists constants Cl, Cu, C > 0 depending on f
such that the following holds with probability at least 1 − δ. Suppose that k satisﬁes

Cl · log(1/δ)2 · log n ≤ k ≤ Cu · log(1/δ)2D/(2β+D) · (log n)D(2β+D) · n2β/(2β+D),

then we have

dH (Hα(f ), (cid:99)Hα(f )) ≤ C ·

(cid:16)

log(1/δ)1/2D · n−1/2D + log(1/δ)1/β · log(n)1/2β · k−1/2β(cid:17)

.

Proof of Theorem 5. Again, to simplify notation, let us deﬁne the following:

K(n, k, δ) :=

(cid:32)(cid:114)





log(1/δ)
n

(cid:33)β/D

+

log(1/δ)
√
k

√

log n



 .

There are two directions to show for the Hausdorff distance result. That (i) maxx∈(cid:100)Hα(f ) d(x, Hα(f ))
is bounded, that is none of the high-density points recovered by Algorithm 1 are far from the true
high-density region; and (ii) that supx∈Hα(f ) d(x, (cid:99)Hα(f )) is bounded, that Algorithm 1 recovers a
good covering of the entire high-density region.

We ﬁrst show (i). By Lemma 6, we have that there exists C1 > 0 such that

H U

α (f ) := {x ∈ X : f (x) ≥ λα − C1K(n, k, δ)}

contains (cid:99)Hα(f ). Thus,

max
x∈(cid:100)Hα(f )

d(x, Hα(f )) ≤ sup
x∈H U

α (f )

d(x, Hα(f )) ≤

C1 · K(n, k, δ) ·

(cid:18)

(cid:19)1/β

,

1
ˇCβ

where the second inequality holds by Assumption 1. Now for the other direction, we have by triangle
inequality that

sup
x∈Hα(f )

d(x, (cid:99)Hα(f )) ≤ sup

d(x, H L

α (f )) + sup
x∈H L

α (f )

x∈Hα(f )

d(x, (cid:99)Hα(f )).

16

The ﬁrst term can be bounded by using Assumption 1:

sup
x∈Hα(f )

d(x, H L

α (f )) ≤

C1 · K(n, k, δ) ·

(cid:18)

(cid:19)1/β

.

1
ˇCβ

Now for the second term, we see that by Lemma 6, (cid:99)Hα(f ) contains all of the sample points of H L
Thus, we have

α (f ).

sup
x∈H L

α (f )

d(x, (cid:99)Hα(f )) ≤ sup
x∈H L

α (f )

d(x, H L

α (f ) ∩ X).

By Assumption 1, for r < r0, and x ∈ H L

distribution corresponding to f . Choosing r ≥
Fn(B(x, r)) > 0 where Fn is the distribution of X and thus, we have

α (f ) we have F(B(x, r)) ≥ ρrD, where F is the
gives us that by Lemma 1 that

(cid:17)1/D

√

(cid:16) Cδ,n
ρ

D log n
n

sup
x∈H L

α (f )

d(x, H L

α (f ) ∩ X) ≤

√

(cid:18) Cδ,n
ρ

D log n
n

(cid:19)1/D

,

which is dominated by the error contributed by the other error and the result follows.

C Supporting results for Theorem 2 Proof

In this section, we note that we will reuse some notation from the last section for the manifold case.
Lemma 7 (Manifold version of uniform convergence of empirical Euclidean balls (Lemma 7 of [2])).
Let F be the true distribution and Fn be the empirical distribution w.r.t. sample X. Let N be a
minimal ﬁxed set such that each point in M is at most distance 1/n from some point in N . There
exists a universal constant C0 such that the following holds with probability at least 1 − δ. For all
x ∈ X ∪ N ,

F(B) ≥ Cδ,n

⇒ Fn(B) > 0

F(B) ≥

+ Cδ,n

⇒ Fn(B) ≥

√

d log n
n

√

k
n
√
k
n

k
n
k
n

F(B) ≤

− Cδ,n

⇒ Fn(B) <

,

√

k
n
k
n

fk(x) :=

k
n · vd · rk(x)d .

where Cδ,n = C0 log(2/δ)
Deﬁnition 5 (k-NN Density Estimator on Manifold).

d log n, Fn is the empirical distribution, and k ≥ Cδ,n.

Lemma 8 (Manifold version of fk upper bound [28]). Deﬁne the following which charaterizes how
much the density increases locally in M :

(cid:40)

(cid:41)

f (x(cid:48)) − f (x) ≤ (cid:15)

.

ˆr((cid:15), x) := sup

r :

sup
x(cid:48)∈B(x,r)∩M

Fix λ0 > 0 and δ > 0 and suppose that k ≥ C 2
that if

δ,n. Then there exists constant C1 ≡ C1(λ0, d, τ ) such

then the following holds with probability at least 1 − δ uniformly in (cid:15) > 0 and x ∈ X with
f (x) + (cid:15) ≥ λ0:

k ≤ C1 · C 2d/(2+d)

δ,n

· n2/(2+d),

(cid:18)

fk(x) <

1 + 3 ·

· (f (x) + (cid:15)),

(cid:19)

Cδ,n√
k

n − Cδ,n

√
k
n .

17

provided k satisﬁes vd · ˆr((cid:15), x)d · (f (x) + (cid:15)) ≥ k

Lemma 9 (Manifold version of fk lower bound [28]). Deﬁne the following which charaterizes how
much the density decreases locally in M :

(cid:40)

(cid:41)

f (x) − f (x(cid:48)) ≤ (cid:15)

.

ˇr((cid:15), x) := sup

r :

sup
x(cid:48)∈B(x,r)∩M

Fix λ0 > 0 and 0 < δ < 1 and suppose k ≥ Cδ,n. Then there exists constant C2 ≡ C2(λ0, d, τ )
such that if

then with probability at least 1 − δ, the following holds uniformly for all (cid:15) > 0 and x ∈ X with
f (x) − (cid:15) ≥ λ0:

k ≤ C2 · C 2d/(4+d)

δ,n

· n4/(4+d),

provided k satisﬁes vd · ˇr((cid:15), x)d · (f (x) − (cid:15)) ≥ 4
3

n + Cδ,n

√

k
n

(cid:17)

.

fk(x) ≥

1 − 3 ·

· (f (x) − (cid:15)),

(cid:18)

(cid:19)

Cδ,n√
k
(cid:16) k

D Proof of Theorem 2

The proof essentially follows the same structure as the full-dimensional case, with the primary
difference in the density estimation bounds.
Lemma 10 (Manifold Version of Lemma 4). There exists constants C1, r1 > 0 depending on f such
that the following holds for all 0 < (cid:15) < r1 such that

0 < λα − λα−(cid:15) ≤ C1(cid:15)β/d and 0 < λα+(cid:15) − λα ≤ C1(cid:15)β/d.

Proof. The proof follows the same structure as the proof of Lemma 4, with the difference being the
change in dimension, and is omitted here.

Lemma 11 (Manifold Version of Lemma 5). Let 0 < δ < 1. Let (cid:98)ε be the ε setting chosen by
Algorithm 1 after the binary search procedure. Deﬁne
k

(cid:99)λα :=

vD · n · (cid:98)εd .

Then, with probability at least 1 − δ, we have there exist constant C1 > 0 depending on f and M
such that for n sufﬁciently large depending on f and M , we have

|(cid:98)λα − λα| ≤ C1

(cid:32)(cid:114)





log(1/δ)
n

(cid:33)β/d

+

log(1/δ)
√
k

√

log n



 .

Proof. The proof is essentially the same as that of Lemma 5. The only difference is that instead
of applying the full-dimensional versions of the uniform k-NN density estimate bounds (Lemma 2
and 3), we instead apply the manifold analogues (Lemma 8 and 9). Asides from constant fac-
tors, the major difference is in the allowable range for k. In the full-dimensional case, we only
need k (cid:46) n2β/(2β+d) for the density estimation bounds to hold. However, here we require
k (cid:46) min{n2/(2+d), n2β/(2β+d)} = n2 max{1,β}/(2β(cid:48)+d).

Lemma 12 (Manifold Version of Lemma 5). Let 0 < δ < 1. There exists constant C1 > 0 depending
on f and M such that the following holds with probability at least 1 − δ for n sufﬁciently large
depending on f and M . Deﬁne

H U

α (f ) :=

x ∈ X : f (x) ≥ λα − C1

H L

α (f ) :=

x ∈ X : f (x) ≥ λα + C1










(cid:32)(cid:114)

(cid:33)β/d









log(1/δ)
n

log(1/δ)
n

(cid:32)(cid:114)

(cid:33)β/d

+

log(1/δ)
√
k

+

log(1/δ)
√
k

√

log n

√

log n


















.

Then,

H L

α (f ) ∩ X ⊆ (cid:99)Hα(f ) ⊆ H U

α (f ) ∩ X.

18

Proof. Same comment as the proof for Lemma 11.

Theorem 6. [Extends Theorem 2] Let 0 < δ < 1. Suppose that density function f is continuous
and supported on M and Assumptions 1 and 2 hold. Suppose also that there exists λ0 > 0 such that
f (x) ≥ λ0 for all x ∈ M . Then, there exists constants Cl, Cu, C > 0 depending on f such that the
following holds with probability at least 1 − δ. Suppose that k satisﬁes,

Cl · log(1/δ)2 · log n ≤ k ≤ Cu · log(1/δ)2d/(2β(cid:48)+d) · (log n)d(2β(cid:48)+d) · n2β(cid:48)/(2β(cid:48)+d)

where β(cid:48) := max{1, β}. Then we have

dH (Hα(f ), (cid:99)Hα(f )) ≤ C ·

(cid:16)

log(1/δ)1/2d · n−1/2d + log(1/δ)1/β · log(n)1/2β · k−1/2β(cid:17)

.

Proof of Theorem 6. Proof is the same as the full-dimensional case given the contributed Lemmas of
this section and is omitted here.

E Supporting Results for Theorem 3 Proof

Next, we need the following on the volume of the intersection of the Euclidean ball and M ; this is
required to get a handle on the true mass of the ball under FM in later arguments. The upper and
lower bounds follow from [5] and Lemma 5.3 of [41]. The proof can be found e.g. in [28].
Lemma 13 (Ball Volume). If 0 < r < min{τ /4d, 1/τ }, and x ∈ M then

vdrd(1 − τ 2r2) ≤ vold(B(x, r) ∩ M ) ≤ vdrd(1 + 4dr/τ ),
where vd is the volume of a unit ball in Rd and vold is the volume w.r.t. the uniform measure on M .

The next is a bound uniform convergence of balls:
Lemma 14 (Lemma 3 of [29]). Let B be the set of all balls in RD, F is some distribution and Fn is
an empirical distribution. With probability at least 1 − δ, the following holds uniformly for every
B ∈ B and γ ≥ 0:

F(B) ≥ γ ⇒ Fn(B) ≥ γ − βn
F(B) ≤ γ ⇒ Fn(B) ≤ γ + βn

√

√

γ − β2
n,
γ + β2
n,

where βn = 8d log(1/δ)(cid:112)log n/n.

F Proof of Theorem 3

The ﬁrst result says that within the manifold, the vast majority of the probability mass is attributed to
the manifold distributions.
Lemma 15. There exists C1, r1 > 0 depending on FM , FE, M such that the following holds
uniformly over x ∈ M and 0 < r < r1.

Proof. Let x ∈ M and r > 0. We have

FM (B(x, r)) ≥ λ0 · vold(B(x, r) ∩ M ) ≥ vdrd(1 − τ 2r2) · λ0,
where the second inequality holds by Lemma 13 for r sufﬁciently small. On the other hand, we have

Thus, we have there exists C1 > 0 depending on fM , M , and fE such that

FE(B(x, r))
FM (B(x, r))

≤ C1 · rD−d.

FE(B(x, r)) ≤ ||fE||∞vDrD.

FE(B(x, r))
FM (B(x, r))

≤ C1 · rD−d,

19

as desired.

We next show that points far away from H
Algorithm 1.
Lemma 16. There exists ω0 > 0 such that for any 0 < ω < ω0 and n sufﬁciently large depending
on FM , FE, M and ω, with probability at least 1 − δ, Algorithm 1 will not select any points outside
of H

(cid:101)α(fM ) do not get selected as high-density points by

(cid:101)α−ω(fM ).

Proof. By Assumption 1, we can choose ω sufﬁciently small so that for the density fM , |λ
(cid:101)α−ω −
(cid:101)α| ≤ ˇCβ ·(rc/3)β. Then, at the ((cid:101)α−ω)-density level, we will be within the area where the regularity
λ
assumptions hold.
Next, by Hoeffding’s inequality, we have that there exist constant C (cid:48) > 0 such that for ¯α > 0:

(cid:32)

P

1 − ¯α − C (cid:48)

(cid:114)

log(1/δ)
n

≤

|H ¯α−η
1−η

(fM ) ∩ X|

n

≤ 1 − ¯α + C (cid:48)

≥ 1 − δ/3.

(cid:114)

(cid:33)

log(1/δ)
n

Choosing ¯α = α − C (cid:48)

, then it follows that with probability at least 1 − δ/3,

(cid:113) log(1/δ)
n

H0 := H

√

(cid:101)α−C(cid:48)

log(1/δ)/(

n·(1−η))

√

(fM )

satisﬁes |H0 ∩ X| > (1 − α) · n. Next let

Hω := H

(cid:101)α−ω(fM ).

Let r be the value of ε used by Algorithm 1. Now, it sufﬁces to show that for n sufﬁciently large
depending on fM :

max
x∈X\Hω

Fn(B(x, r)) < min
x∈H0

Fn(B(x, r)),

where Fn is the empirical distribution. This is because Algorithm 1 ﬁlters out sample points whose
ε-ball has less than k sample points for its ﬁnal ε value, which is the value which allows it to ﬁlter
α-fraction of the points.

By Lemma 15, it sufﬁces to show that

max
x∈X\Hω

FM,n(B(x, r)) (cid:0)1 + C1rD−d(cid:1) < min
x∈H0

FM,n(B(x, r)) (cid:0)1 − C1rD−d(cid:1) ,

where FM,n(A) denote the fraction of samples drawn from FM which lie in A w.r.t. our entire
sample X.

Then, by Lemma 14, it will be enough to show that

max
x∈X\Hω

< min
x∈H0

(FM (B(x, r)) + βn

(FM (B(x, r)) − βn

(cid:112)FM (B(x, r)) + β2
(cid:112)FM (B(x, r)) − β2

n) (cid:0)1 + C1rD−d(cid:1)
n) (cid:0)1 − C1rD−d(cid:1) ,

where βn = 8d log(1/δ)(cid:112)log n/n.
To bound the LHS, we have by Lemma 13

max
x∈X\Hω

(FM (B(x, r)) + βn · (cid:112)FM (B(x, r)) + β2

n) (cid:0)1 + C1rD−d(cid:1)

≤ max

x∈X\Hω

inf
x(cid:48)∈B(x,r)

fM (x(cid:48))

(cid:26)(cid:18)

(cid:26)(cid:18)

(cid:19)

(cid:19)

≤ max

x∈X\Hω

inf
x(cid:48)∈B(x,r)

fM (x(cid:48))

(1 + C2βn + C3r)

≤ (λ

(cid:101)α−ω + ι(fM , r)) (1 + C2βn + C3r) ,

(cid:27)

(cid:16)

·

1 + βn/(cid:112)||fM ||∞ + β2

n/||fM ||∞

(cid:17) (cid:0)1 + C1rD−d(cid:1) (1 + 4dr/τ )

(cid:27)

for some C2, C3 > 0 and ι
:=
supx,x(cid:48)∈M :|x−x(cid:48)|≤r |fM (x) − fM (x(cid:48))| (i.e. fM is uniformly continuous since it is continuous over a
compact support, so ι(fM , r) → 0 as r → 0).

is the modulus of continuity,

is ι(fM , r)

that

20

Similarly, for the RHS, we can show for some constants C4, C5 that

(FM (B(x, r)) − βn

(cid:112)FM (B(x, r)) − β2

n) (cid:0)1 − C1rD−d(cid:1)

log(1/δ)/(

n(1−η))

√

− ι(fM , r)) (1 − C4βn − C5r) .

min
x∈H0
≥ (λ

√

(cid:101)α−C(cid:48)

The result follows since r → 0 as n → ∞ (since by Lemma 1, r is a k-NN radius so r (cid:46) (k/n)1/D →
0 given the conditions on k of Theorem 3) and the fact that λ
n for n
sufﬁciently large. As desired.

(cid:101)α−ω < λ

log(1/δ)/

(cid:101)α−C(cid:48)

√

√

Lemma 17 (Bounding density estimators w.r.t to entire sample vs w.r.t. samples on manifold). For
x ∈ RD, deﬁne the following:

rk(x) := inf{(cid:15) > 0 : |B(x, (cid:15)) ∩ X| ≥ k}
(cid:101)rk(x) := inf{(cid:15) > 0 : |B(x, (cid:15)) ∩ X ∩ M | ≥ k}
where the former is simply the k-NN radius we’ve been using thus far and the latter is the k-NN
radius if we were to restrict the samples to only those that came from M . Then likewise, deﬁne the
analogous density estimators:

fk(x) :=

n · vd · rk(x)d and (cid:101)fk(x) :=

k

k

n · vd · (cid:101)rk(x)d ,

where again, the former is the usual k-NN density estimator on manifolds. Then, there exists C1 such
that the following holds with high probability.

|fk(x) − (cid:101)fk(x)| ≤ C1 · (k/n)D/d−1.

sup
x∈M

Proof. By Lemma 14, there exists C2 > 0 depending on F and FM such that

Fn(B(x, rk(x)) = k ⇒ |F(B(x, rk(x)) − k| ≤ C2βn,

and

FM,n(B(x, (cid:101)rk(x)) = k ⇒ |FM (B(x, (cid:101)rk(x)) − k| ≤ C2βn,
where FM,n is the empirical distribution w.r.t. X ∩ M . Next, by Lemma 15, we have for some
constant C3 > 0:

F(B(x, (cid:101)rk(x)) ≤ FM (B(x, (cid:101)rk(x))(1 + C3 (cid:101)rk(x)D−d)

≤ (k + C2βn)(1 + C3 · (cid:101)rk(x)D−d)
≤ (k + C2βn)(1 + C4 · (k/n)D/d−1)
≤ k · (1 + C5(k/n)D/d−1),

where the second last inequality holds for some constant C4 > 0 by Lemma 7 and C5 > 0 is some
constant depending on F and FM and M . Then it follows that for some constant C6 > 0, we have

F(B(x, (cid:101)rk(x))
F(B(x, rk(x))

≤ 1 + C6(k/n)D/d−1.

In the other direction, we trivially have (cid:101)rk(x) ≥ rk(x), so

1 ≤

F(B(x, (cid:101)rk(x))
F(B(x, rk(x))

≤ 1 + C6(k/n)D/d−1.

The result follows.

Theorem 7. [Extends Theorem 3] Let 0 < η < α < 1 and 0 < δ < 1. Suppose that distribution F
is a weighted mixture (1 − η) · FM + η · FE where FM is a distribution with continous density fM
supported on a d-dimensional manifold M satisfying Assumption 2 and FE is a (noise) distribution
with continuous density fE with compact support over RD with d < D. Suppose also that there exists
λ0 > 0 such that fM (x) ≥ λ0 for all x ∈ M and H
1−η ) satisﬁes Assumption 1
for density fM . Let (cid:98)Hα be the output of Algorithm 1 on a sample X of size n drawn i.i.d. from F.

(cid:101)α(fM ) (where (cid:101)α := α−η

21

Then, there exists constants Cl, Cu, C > 0 depending on fM , fE, η, M such that the following holds
with probability at least 1 − δ. Suppose that k satisﬁes

Cl · log(1/δ)2 · log n ≤ k ≤ Cu · log(1/δ)2d/(2β(cid:48)+d) · (log n)d(2β(cid:48)+d) · n2β(cid:48)/(2β(cid:48)+d)

where β(cid:48) := max{1, β}. Then we have

dH (H

(cid:101)α(fM ), (cid:99)Hα) ≤ C ·

(cid:16)

log(1/δ)1/2d · n−1/2d + log(1/δ)1/β · log(n)1/2β · k−1/2β(cid:17)

.

Proof of Theorem 7. The proof follows in a similar way as that of Theorem 6, except with the
complexity of having added full-dimensional noise. We will only highlight the difference and provide
a sketch of the proof here.

Lemma 16 and 17 give us a handle on the additional complexity when having separate noise
distribution, compared to the earlier manifold setting of Theorem 2.

Lemma 16 guarantees that the points in (cid:99)Hα lie in the inside of M with margin. In particular, that
means the noise points are ﬁltered out by the algorithm and thus, we are reduced to reasoning about
the (cid:101)α-high-density-set of fM .
Then, Lemma 17 ensures that the k-NN density estimator used for our analysis for the entire sample
X is actually quite close to the k-NN density estimator with respect to M ∩ X within M . In other
words, we can use the k-NN density estimator to estimate fM without knowing which samples
of X were in M . Lemma 17 shows that the additional error in density estimation we obtain is
≈ (k/n)D/d−1 (cid:46) (k/n)1/d (cid:46) (log n)/
k, where the ﬁrst inequality holds since D > d and the
latter holds from the conditions on k. It turns out that this error term can be absorbed as a constant in
the previous result of Theorem 6.

√

G Proof of Theorem 4

Proof of Theorem 4. For the ﬁrst inequality, we have

ξ(h, x) ≥

d(x, M

(cid:101)h(x)) − (cid:15)n
d(x, Mh(x)) + (cid:15)n

=

d(x, M

(cid:101)h(x))
d(x, Mh(x))

−

(cid:15)n
d(x, Mh(x)) + (cid:15)n

(cid:32) d(x, M

(cid:101)h(x))
d(x, Mh(x))

·

(cid:33)

+ 1

,

where the ﬁrst inequality holds by Theorem 3. This, along with the condition on γ and ε(h, x) fromo
the theorem statement, implies that

d(x, M

(cid:101)h(x))
d(x, Mh(x))

< 1 − γ,

which implies that h(x) (cid:54)= h∗(x). For the second inequality, we have

1
ξ(h, x)

≥

d(x, Mh(x)) − (cid:15)n
(cid:101)h(x)) + (cid:15)n
d(x, M

=

d(x, Mh(x))
(cid:101)h(x))
d(x, M

−

(cid:15)n
(cid:101)h(x)) + (cid:15)n

·

d(x, Mh(x))
(cid:101)h(x))
d(x, M

d(x, M

(cid:32)

(cid:33)

+ 1

,

where the ﬁrst inequality holds by Theorem 3. Thus, if the condition of the theorem statement holds,
then

d(x, Mh(x))
(cid:101)h(x))
d(x, M

< 1 − γ ⇒

< 1 − γ

d(x, Mh(x))
d(x, Mc)

for all c (cid:54)= h(x), which implies that h(x) = h∗(x)

22

H Additional UCI Experiments

H.1 When to trust: Precision for correct predictions by percentile

Figure 4: UCI data sets and precision on correctness

23

H.2 When to not trust: Precision for misclassiﬁcation predictions by percentile

Figure 5: UCI data sets and precision on incorrectness

H.3 High dimensional Datasets

24

(a) MNIST

(b) MNIST

(c) SVHN

(d) SVHN

(e) CIFAR-10

(f) CIFAR-10

(g) CIFAR-100

(h) CIFAR-100

Figure 6: Trust score results using convolutional neural networks on MNIST, SVHN, CIFAR-10, and
CIFAR-100 datasets. Left column is detecting trustworthy; right column is detecting suspicious.

25

8
1
0
2
 
t
c
O
 
6
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
3
8
7
1
1
.
5
0
8
1
:
v
i
X
r
a

To Trust Or Not To Trust A Classiﬁer

Heinrich Jiang∗
Google Research
heinrichj@google.com

Been Kim
Google Brain
beenkim@google.com

Melody Y. Guan†
Stanford University
mguan@stanford.edu

Maya Gupta
Google Research
mayagupta@google.com

Abstract

Knowing when a classiﬁer’s prediction can be trusted is useful in many applications
and critical for safely using AI. While the bulk of the effort in machine learning
research has been towards improving classiﬁer performance, understanding when
a classiﬁer’s predictions should and should not be trusted has received far less
attention. The standard approach is to use the classiﬁer’s discriminant or conﬁdence
score; however, we show there exists an alternative that is more effective in many
situations. We propose a new score, called the trust score, which measures the
agreement between the classiﬁer and a modiﬁed nearest-neighbor classiﬁer on
the testing example. We show empirically that high (low) trust scores produce
surprisingly high precision at identifying correctly (incorrectly) classiﬁed examples,
consistently outperforming the classiﬁer’s conﬁdence score as well as many other
baselines. Further, under some mild distributional assumptions, we show that if the
trust score for an example is high (low), the classiﬁer will likely agree (disagree)
with the Bayes-optimal classiﬁer. Our guarantees consist of non-asymptotic rates
of statistical consistency under various nonparametric settings and build on recent
developments in topological data analysis.

1

Introduction

Machine learning (ML) is a powerful and widely-used tool for making potentially important decisions,
from product recommendations to medical diagnosis. However, despite ML’s impressive performance,
it makes mistakes, with some more costly than others. As such, ML trust and safety is an important
theme [54, 36, 1]. While improving overall accuracy is an important goal that the bulk of the effort in
ML community has been focused on, it may not be enough: we need to also better understand the
strengths and limitations of ML techniques.

This work focuses on one such challenge: knowing whether a classiﬁer’s prediction for a test example
can be trusted or not. Such trust scores have practical applications. They can be directly shown to
users to help them gauge whether they should trust the AI system. This is crucial when a model’s
prediction inﬂuences important decisions such as a medical diagnosis, but can also be helpful even
in low-stakes scenarios such as movie recommendations. Trust scores can be used to override the
classiﬁer and send the decision to a human operator, or to prioritize decisions that human operators
should be making. Trust scores are also useful for monitoring classiﬁers to detect distribution shifts
that may mean the classiﬁer is no longer as useful as it was when deployed.

∗All authors contributed equally.
†Work done while intern at Google Research.
An open-source implementation of Trust Scores can be found here: https://github.com/google/TrustScore

32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.

A standard approach to deciding whether to trust a classiﬁer’s decision is to use the classiﬁers’ own
reported conﬁdence or score, e.g. probabilities from the softmax layer of a neural network, distance
to the separating hyperplane in support vector classiﬁcation, mean class probabilities for the trees in
a random forest. While using a model’s own implied conﬁdences appears reasonable, it has been
shown that the raw conﬁdence values from a classiﬁer are poorly calibrated [24, 32]. Worse yet,
even if the scores are calibrated, the ranking of the scores itself may not be reliable. In other words,
a higher conﬁdence score from the model does not necessarily imply higher probability that the
classiﬁer is correct, as shown in [45, 22, 39]. A classiﬁer may simply not be the best judge of its own
trustworthiness.

In this paper, we use a set of labeled examples (e.g. training data or validation data) to help determine
a classiﬁer’s trustworthiness for a particular testing example. First, we propose a simple procedure
that reduces the training data to a high density set for each class. Then we deﬁne the trust score—the
ratio between the distance from the testing sample to the nearest class different from the predicted
class and the distance to the predicted class—to determine whether to trust that classiﬁer prediction.

Theoretically, we show that high/low trust scores correspond to high probability of agree-
ment/disagreement with the Bayes-optimal classiﬁer. We show ﬁnite-sample estimation rates when
the data is full-dimension and supported on or near a low-dimensional manifold. Interestingly, we
attain bounds that depend only on the lower manifold dimension and independent of the ambient
dimension without any changes to the procedure or knowledge of the manifold. To our knowledge,
these results are new and may be of independent interest.

Experimentally, we found that the trust score better identiﬁes correctly-classiﬁed points for low and
medium-dimension feature spaces than the model itself. However, high-dimensional feature spaces
were more challenging, and we demonstrate that the trust score’s utility depends on the vector space
used to compute the trust score differences.

2 Related Work

One related line of work is that of conﬁdence calibration, which transforms classiﬁer outputs into
values that can be interpreted as probabilities, e.g. [44, 58, 40, 24]. In recent work, [32] explore the
structured prediction setting, and [33] obtain conﬁdence estimates by using ensembles of networks.
These calibration techniques typically only use the the model’s reported score (and the softmax
layer in the case of a neural network) for calibration, which notably preserves the rankings of the
classiﬁer scores. Similarly, [26] considered using the softmax probabilities for the related problem of
identifying misclassiﬁcations and mislabeled points.

Recent work explored estimating uncertainty for Bayesian neural networks and returning a distribution
over the outputs [20, 30]. The proposed trust score does not change the network structure (nor does
it assume any structure) and gives a single score, rather than a distribution over outputs as the
representation of uncertainty.

The problem of classiﬁcation with a reject option or learning with abstention [3, 57, 9, 23, 8, 27, 10]
is a highly related framework where the classiﬁer is allowed to abstain from making a prediction at
a certain cost. Typically such methods jointly learn the classiﬁer and the rejection function. Note
that the interplay between classiﬁcation rate and reject rate is studied in many various forms e.g.
[7, 13, 19, 48, 52, 18, 34, 14, 56, 51]. Our paper assumes an already trained and possibly black-box
classiﬁer and learns the conﬁdence scores separately, but we do not explicitly learn the appropriate
rejection thresholds.

Whether to trust a classiﬁer also arises in the setting where one has access to a sequence of classiﬁers,
but there is some cost to evaluating each classiﬁer, and the goal is to decide after evaluating each
classiﬁer in the sequence if one should trust the current classiﬁer decision enough to stop, rather than
evaluating more classiﬁers in the sequence (e.g. [55, 43, 16]). Those conﬁdence decisions are usually
based on whether the current classiﬁer score will match the classiﬁcation of the full sequence.

Experimentally we ﬁnd that the vector space used to compute the distances in the trust score matters,
and that computing trust scores on more-processed layers of a deep model generally works better.
This observation is similar to the work of Papernot and McDaniel [42], who use k-NN regression on
the intermediate representations of the network which they showed enhances robustness to adversarial
attacks and leads to better calibrated uncertainty estimations.

2

Our work builds on recent results in topological data analysis. Our method to ﬁlter low-density
points estimates a particular density level-set given a parameter α, which aims at ﬁnding the level-
set that contains 1 − α fraction of the probability mass. Level-set estimation has a long history
[25, 15, 53, 50, 46, 28]. However such works assume knowledge of the density level, which is
difﬁcult to determine in practice. We provide rates for Algorithm 1 in estimating the appropriate
level-set corresponding to α without knowledge of the level. The proxy α offers a more intuitive
parameter compared to the density value, is used for level-set estimation. Our analysis is also done
under various settings including when the data lies near a lower dimensional manifold and we provide
rates that depend only on the lower dimension.

3 Algorithm: The Trust Score

Our approach proceeds in two steps outlined in Algorithm 1 and 2. We ﬁrst pre-process the training
data, as described in Algorithm 1, to ﬁnd the α-high-density-set of each class, which is deﬁned as
the training samples within that class after ﬁltering out α-fraction of the samples with lowest density
(which may be outliers):
Deﬁnition 1 (α-high-density-set). Let 0 ≤ α < 1 and f be a continuous density function with
compact support X ⊆ RD. Then deﬁne Hα(f ), the α-high-density-set of f , to be the λα-level set of
f , deﬁned as {x ∈ X : f (x) ≥ λα} where λα := inf (cid:8)λ ≥ 0 : (cid:82)

X 1 [f (x) ≤ λ] f (x)dx ≥ α(cid:9).

In order to approximate the α-high-density-set, Algorithm 1 ﬁlters the α-fraction of the sample points
with lowest empirical density, based on k-nearest neighbors. This data ﬁltering step is independent of
the given classiﬁer h.

Then, the second step: given a testing sample, we deﬁne its trust score to be the ratio between the
distance from the testing sample to the α-high-density-set of the nearest class different from the
predicted class, and the distance from the test sample to the α-high-density-set of the class predicted
by h, as detailed in Algorithm 2. The intuition is that if the classiﬁer h predicts a label that is
considerably farther than the closest label, then this is a warning that the classiﬁer may be making a
mistake.

Our procedure can thus be viewed as a comparison to a modiﬁed nearest-neighbor classiﬁer, where
the modiﬁcation lies in the initial ﬁltering of points not in the α-high-density-set for each class.
Remark 1. The distances can be computed with respect to any representation of the data. For exam-
ple, the raw inputs, an unsupervised embedding of the space, or the activations of the intermediate
representations of the classiﬁer. Moreover, the nearest-neighbor distance can be replaced by other
distance measures, such as k-nearest neighbors or distance to a centroid.

Algorithm 1 Estimating α-high-density-set
Parameters: α (density threshold), k.
Inputs: Sample points X := {x1, .., xn} drawn from f .
Deﬁne k-NN radius rk(x) := inf{r > 0 : |B(x, r) ∩ X| ≥ k} and let ε := inf{r > 0 : |{x ∈ X :
rk(x) > r}| ≤ α · n}.
return (cid:99)Hα(f ) := {x ∈ X : rk(x) ≤ ε}.

Algorithm 2 Trust Score

Parameters: α (density threshold), k.
Input: Classiﬁer h : X → Y. Training data (x1, y1), ..., (xn, yn). Test example x.
For each (cid:96) ∈ Y, let (cid:99)Hα(f(cid:96)) be the output of Algorithm 1 with parameters α, k and sample points
{xj : 1 ≤ j ≤ n, yj = (cid:96)}. Then, return the trust score, deﬁned as:

ξ(h, x) := d

x, (cid:99)Hα(f

/d

x, (cid:99)Hα(fh(x))

(cid:17)

,

(cid:16)

(cid:16)

(cid:16)

(cid:17)
(cid:101)h(x))
(cid:17)

where (cid:101)h(x) = argminl∈Y,l(cid:54)=h(x) d

x, (cid:99)Hα(fl)

.

The method has two hyperparameters: k (the number of neighbors, such as in k-NN) and α (fraction
of data to ﬁlter) to compute the empirical densities. We show in theory that k can lie in a wide range

3

and still give us the desired consistency guarantees. Throughout our experiments, we ﬁx k = 10, and
use cross-validation to select α as it is data-dependent.
Remark 2. We observed that the procedure was not very sensitive to the choice of k and α. As will
be shown in the experimental section, for efﬁciency on larger datasets, we skipped the initial ﬁltering
step of Algorithm 1 (leading to a hyperparameter-free procedure) and obtained reasonable results.
This initial ﬁltering step can also be replaced by other strategies. One such example is ﬁltering
examples whose labels have high disagreement amongst its neighbors, which is implemented in the
open-source code release but not experimented with here.

4 Theoretical Analysis

In this section, we provide theoretical guarantees for Algorithms 1 and 2. Due to space constraints,
all the proofs are deferred to the Appendix. To simplify the main text, we state our results treating δ,
the conﬁdence level, as a constant. The dependence on δ in the rates is made explicit in the Appendix.

We show that Algorithm 1 is a statistically consistent estimator of the α-high-density-level set with
ﬁnite-sample estimation rates. We analyze Algorithm 1 in three different settings: when the data lies
on (i) a full-dimensional RD; (ii) an unknown lower dimensional submanifold embedded in RD; and
(iii) an unknown lower dimensional submanifold with full-dimensional noise.
For setting (i), where the data lies in RD, the estimation rate has a dependence on the dimension D,
which may be unattractive in high-dimensional situations: this is known as the curse of dimensionality,
suffered by density-based procedures in general. However, when the data has low intrinsic dimension
in (ii), it turns out that, remarkably, without any changes to the procedure, the estimation rate depends
on the lower dimension d and is independent of the ambient dimension D. However, in realistic
situations, the data may not lie exactly on a lower-dimensional manifold, but near one. This reﬂects
the setting of (iii), where the data essentially lies on a manifold but has general full-dimensional noise
so the data is overall full-dimensional. Interestingly, we show that we still obtain estimation rates
depending only on the manifold dimension and independent of the ambient dimension; moreover, we
do not require knowledge of the manifold nor its dimension to attain these rates.

We then analyze Algorithm 2, and establish the culminating result of Theorem 4: for labeled data
distributions with well-behaved class margins, when the trust score is large, the classiﬁer likely agrees
with the Bayes-optimal classiﬁer, and when the trust score is small, the classiﬁer likely disagrees with
the Bayes-optimal classiﬁer. If it turns out that even the Bayes-optimal classiﬁer has high-error in
a certain region, then any classiﬁer will have difﬁculties in that region. Thus, Theorem 4 does not
guarantee that the trust score can predict misclassiﬁcation, but rather that it can predict when the
classiﬁer is making an unreasonable decision.

4.1 Analysis of Algorithm 1

We require the following regularity assumptions on the boundaries of Hα(f ), which are standard
in analyses of level-set estimation [50]. Assumption 1.1 ensures that the density around Hα(f ) has
both smoothness and curvature. The upper bound gives smoothness, which is important to ensure
that our density estimators are accurate for our analysis (we only require this smoothness near the
boundaries and not globally). The lower bound ensures curvature: this ensures that Hα(f ) is salient
enough to be estimated. Assumption 1.2 ensures that Hα(f ) does not get arbitrarily thin anywhere.
Assumption 1 (α-high-density-set regularity). Let β > 0. There exists ˇCβ, ˆCβ, β, rc, r0, ρ > 0 s.t.

1. ˇCβ · d(x, Hα(f ))β ≤ |λα − f (x)| ≤ ˆCβ · d(x, Hα(f ))β for all x ∈ ∂Hα(f ) + B(0, rc).

2. For all 0 < r < r0 and x ∈ Hα(f ), we have Vol(B(x, r)) ≥ ρ · rD.

where ∂A denotes the boundary of a set A, d(x, A) := inf x(cid:48)∈A ||x−x(cid:48)||, B(x, r) := {x(cid:48) : |x−x(cid:48)| ≤
r} and A + B(0, r) := {x : d(x, A) ≤ r}.

Our statistical guarantees are under the Hausdorff metric, which ensures a uniform guarantee over
our estimator: it is a stronger notion of consistency than other common metrics [46, 47].
Deﬁnition 2 (Hausdorff distance). dH (A, B) := max{supx∈A d(x, B), supx∈B d(x, A)}.

4

We now give the following result for Algorithm 1. It says that as long as our density function satisﬁes
the regularity assumptions stated earlier, and the parameter k lies within a certain range, then we can
bound the Hausdorff distance between what Algorithm 1 recovers and Hα(f ), the true α-high-density
set, from an i.i.d. sample drawn from f of size n. Then, as n goes to ∞, and k grows as a function of
n, the quantity goes to 0.
Theorem 1 (Algorithm 1 guarantees). Let 0 < δ < 1 and suppose that f is continuous and has
compact support X ⊆ RD and satisﬁes Assumption 1. There exists constants Cl, Cu, C > 0
depending on f and δ such that the following holds with probability at least 1 − δ. Suppose that k
satisﬁes Cl · log n ≤ k ≤ Cu · (log n)D(2β+D) · n2β/(2β+D). Then we have

dH (Hα(f ), (cid:99)Hα(f )) ≤ C ·

(cid:16)

n−1/2D + log(n)1/2β · k−1/2β(cid:17)

.

Remark 3. The condition on k can be simpliﬁed by ignoring log factors: log n (cid:46) k (cid:46) n2β/(2β+D),
which is a wide range. Setting k to its allowed upper bound, we obtain our consistency guarantee of

dH (Hα(f ), (cid:99)Hα(f )) (cid:46) max{n−1/2D, n−1/(2β+D)}.
The ﬁrst term is due to the error from estimating the appropriate level given α (i.e. identifying the
level λα) and the second term corresponds to the error for recovering the level set given knowledge
of the level. The latter term matches the lower bound for level-set estimation up to log factors [53].

4.2 Analysis of Algorithm 1 on Manifolds

One of the disadvantages of Theorem 1 is that the estimation errors have a dependence on D, the
dimension of the data, which may be highly undesirable in high-dimensional settings. We next
improve these rates when the data has a lower intrinsic dimension. Interestingly, we are able to show
rates that depend only on the intrinsic dimension of the data, without explicit knowledge of that
dimension nor any changes to the procedure. As common to related work in the manifold setting, we
make the following regularity assumptions which are standard among works in manifold learning
(e.g. [41, 21, 2]).
Assumption 2 (Manifold Regularity). M is a d-dimensional smooth compact Riemannian manifold
without boundary embedded in compact subset X ⊆ RD with bounded volume. M has ﬁnite
condition number 1/τ , which controls the curvature and prevents self-intersection.
Theorem 2 (Manifold analogue of Theorem 1). Let 0 < δ < 1. Suppose that density function f
is continuous and supported on M and Assumptions 1 and 2 hold. Suppose also that there exists
λ0 > 0 such that f (x) ≥ λ0 for all x ∈ M . Then, there exists constants Cl, Cu, C > 0 depending
on f and δ such that the following holds with probability at least 1 − δ. Suppose that k satisﬁes
Cl · log n ≤ k ≤ Cu · (log n)d(2β(cid:48)+d) · n2β(cid:48)/(2β(cid:48)+d). where β(cid:48) := max{1, β}. Then we have
n−1/2d + log(n)1/2β · k−1/2β(cid:17)
(cid:16)

dH (Hα(f ), (cid:99)Hα(f )) ≤ C ·

.

Remark 4. Setting k to its allowed upper bound, we obtain (ignoring log factors),

dH (Hα(f ), (cid:99)Hα(f )) (cid:46) max{n−1/2d, n−1/(2 max{1,β}+d)}.
The ﬁrst term can be compared to that of the previous result where D is replaced with d. The second
term is the error for recovering the level set on manifolds, which matches recent rates [28].

4.3 Analysis of Algorithm 1 on Manifolds with Full Dimensional Noise

In realistic settings, the data may not lie exactly on a low-dimensional manifold, but near one. We
next present a result where the data is distributed along a manifold with additional full-dimensional
noise. We make mild assumptions on the noise distribution. Thus, in this situation, the data has
intrinsic dimension equal to the ambient dimension. Interestingly, we are still able to show that the
rates only depend on the dimension of the manifold and not the dimension of the entire data.
Theorem 3. Let 0 < η < α < 1 and 0 < δ < 1. Suppose that distribution F is a weighted
mixture (1 − η) · FM + η · FE where FM is a distribution with continous density fM supported on a
d-dimensional manifold M satisfying Assumption 2 and FE is a (noise) distribution with continuous
density fE with compact support over RD with d < D. Suppose also that there exists λ0 > 0 such

5

(cid:101)α(fM ) (where (cid:101)α := α−η
1−η ) satisﬁes Assumption 1 for density
that fM (x) ≥ λ0 for all x ∈ M and H
fM . Let (cid:98)Hα be the output of Algorithm 1 on a sample X of size n drawn i.i.d. from F. Then, there
exists constants Cl, Cu, C > 0 depending on fM , fE, η, M and δ such that the following holds with
probability at least 1 − δ. Suppose that k satisﬁes Cl · log n ≤ k ≤ Cu · (log n)d(2β(cid:48)+d) · n2β(cid:48)/(2β(cid:48)+d),
where β(cid:48) := max{1, β}. Then we have

dH (H

(cid:101)α(fM ), (cid:99)Hα) ≤ C ·

(cid:16)

n−1/2d + log(n)1/2β · k−1/2β(cid:17)

.

The above result is compelling because it shows why our methods can work, even in high-dimensions,
despite the curse of dimensionality of non-parametric methods. In typical real-world data, even if
the data lies in a high-dimensional space, there may be far fewer degrees of freedom. Thus, our
theoretical results suggest that when this is true, then our methods will enjoy far better convergence
rates – even when the data overall has full intrinsic dimension due to factors such as noise.

4.4 Analysis of Algorithm 2: the Trust Score

We now provide a guarantee about the trust score, making the same assumptions as in Theorem 3 for
each of the label distributions. We additionally assume that the class distributions are well-behaved
in the following sense: that high-density-regions for each of the classes satisfy the property that for
any point x ∈ X , if the ratio of the distance to one class’s high-density-region to that of another is
smaller by some margin γ, then it is more likely that x’s label corresponds to the former class.
Theorem 4. Let 0 < η < α < 1. Let us have labeled data (x1, y1), ..., (xn, yn) drawn from
distribution D, which is a joint distribution over X × Y where Y are the labels, |Y| < ∞, and
X ⊆ RD is compact. Suppose for each (cid:96) ∈ Y, the conditional distribution for label (cid:96) satisﬁes the
conditions of Theorem 3 for some manifold and noise level η. Let fM,(cid:96) be the density of the portion of
1−η and
the conditional distribution for label (cid:96) supported on M . Deﬁne M(cid:96) := H
let (cid:15)n be the maximum Hausdorff error from estimating M(cid:96) over each (cid:96) ∈ Y in Theorem 3. Assume
that min(cid:96)∈Y PD(y = (cid:96)) > 0 to ensure we have samples from each label.
Suppose also that for each x ∈ X , if d(x, Mi)/d(x, Mj) < 1 − γ then P(y = i|x) > P(y = j|x)
for i, j ∈ Y. That is, if we are closer to Mi than Mj by a ratio of less than 1 − γ, then the
point is more likely to be from class i. Let h∗ be the Bayes-optimal classiﬁer, deﬁned by h∗(x) :=
P(y = (cid:96)|x). Then the trust score ξ of Algorithm 2 satisﬁes the following with high
argmax(cid:96)∈Y
probability uniformly over all x ∈ X and all classiﬁers h : X → Y simultaneously for n sufﬁciently
large depending on D:

(cid:101)α(f(cid:96)), where (cid:101)α := α−η

ξ(h, x) < 1 − γ −

(cid:15)n
d(x, Mh(x)) + (cid:15)n

(cid:32) d(x, M

(cid:101)h(x))
d(x, Mh(x))

1
ξ(h, x)

< 1 − γ −

(cid:15)n
(cid:101)h(x)) + (cid:15)n

d(x, M

d(x, Mh(x))
(cid:101)h(x))
d(x, M

·

·

(cid:32)

(cid:33)

(cid:33)

+ 1

⇒ h(x) (cid:54)= h∗(x),

+ 1

⇒ h(x) = h∗(x).

5 Experiments

In this section, we empirically test whether trust scores can both detect examples that are incorrectly
classiﬁed with high precision and be used as a signal to determine which examples are likely correctly
classiﬁed. We perform this evaluation across (i) different datasets (Sections 5.1 and 5.3), (ii) different
families of classiﬁers (neural network, random forest and logistic regression) (Section 5.1), (iii)
classiﬁers with varying accuracy on the same task (Section 5.2) and (iv) different representations of
the data e.g. input data or activations of various intermediate layers in neural network (Section 5.3).

First, we test if testing examples with high trust score corresponds to examples in which the model is
correct ("identifying trustworthy examples"). Each method produces a numeric score for each testing
example. For each method, we bin the data points by percentile value of the score (i.e. 100 bins).
Given a recall percentile level (i.e. the x-axis on our plots), we take the performance of the classiﬁer
on the bins above the percentile level as the precision (i.e. the y-axis). Then, we take the negative of
each signal and test if low trust score corresponds to the model being wrong ("identifying suspicious

6

Figure 1: Two example datasets and models. For predicting correctness (top row) the vertical dotted
black line indicates error level of the trained classiﬁer. For predicting incorrectness (bottom) the
vertical black dotted line is the accuracy rate of the classiﬁer. For detecting trustworthy, for each
percentile level, we take the test examples whose trust score was above that percentile level and plot
the percentage of those test points that were correctly classiﬁed by the classiﬁer, and do the same
model conﬁdence and 1-nn ratio. For detecting suspicious, we take the negative of each signal and
plot the precision of identifying incorrectly classiﬁed examples. Shown are average of 20 runs with
shaded standard error band. The trust score consistently attains a higher precision for each given
percentile of classiﬁer decision-rejection. Furthermore, the trust score generally shows increasing
precision as the percentile level increases, but surprisingly, many of the comparison baselines do not.
See the Appendix for the full results.

examples"). Here the y-axis is the misclassiﬁcation rate and the x-axis corresponds to decreasing
trust score or model conﬁdence.

In both cases, the higher the precision vs percentile curve, the better the method. The vertical black
dotted lines in the plots represent the omniscient ideal. For identifying trustworthy examples it is the
error rate of the classiﬁer and for identifying suspicious examples" it is the accuracy rate.

The baseline we use in Section is the model’s own conﬁdence score, which is similar to the approach
of [26]. While calibrating the classiﬁers’ conﬁdence scores (i.e. transforming them into probability
estimates of correctness) is an important related work [24, 44], such techniques typically do not
change the rankings of the score, at least in the binary case. Since we evaluate the trust score on
its precision at a given recall percentile level, we are interested in the relative ranking of the scores
rather than their absolute values. Thus, we do not compare against calibration techniques. There
are surprisingly few methods aimed at identifying correctly or incorrectly classiﬁed examples with
precision at a recall percentile level as noted in [26].

Choosing Hyperparameters: The two hyperparameters for the trust score are α and k. Throughout
the experiments, we ﬁx k = 10 and choose α using cross-validation over (negative) powers of 2 on
the training set. The metric for cross-validation was optimal performance on detecting suspicious
examples at the percentile corresponding to the classiﬁer’s accuracy. The bulk of the computational
cost for the trust-score is in k-nearest neighbor computations for training and 1-nearest neighbor
searches for evaluation. To speed things up for the larger datasets MNIST, SVHN, CIFAR-10
and CIFAR-100, we skipped the initial ﬁltering step of Algorithm 1 altogether and reduced the
intermediate layers down to 20 dimensions using PCA before being processed by the trust score
which showed similar performance. We note that any approximation method (such as approximate
instead of exact nearest neighbors) could have been used instead.

5.1 Performance on Benchmark UCI Datasets

In this section, we show performance on ﬁve benchmark UCI datasets [17], each for three kinds
of classiﬁers (neural network, random forest and logistic regression). Due to space, we only show

7

Figure 2: We show the performance of trust score on the Digits dataset for a neural network as we
increase the accuracy. As we go from left to right, we train the network with more iterations (each
with batch size 50) thus increasing the accuracy indicated by the dotted vertical lines. While the trust
score still performs better than model conﬁdence, the amount of improvement diminishes.

two data sets and two models in Figure 1. The rest can be found in the Appendix. For each method
and dataset, we evaluated with multiple runs. For each run we took a random stratiﬁed split of the
dataset into two halves. One portion was used for training the trust score and the other was used for
evaluation and the standard error is shown in addition to the average precision across the runs at each
percentile level. The results show that our method consistently has a higher precision vs percentile
curve than the rest of the methods across the datasets and models. This suggests the trust score
considerably improves upon known methods as a signal for identifying trustworthy and suspicious
testing examples for low-dimensional data.

In addition to the model’s own conﬁdence score, we try one additional baseline, which we call the
nearest neighbor ratio (1-nn ratio). It is the ratio between the 1-nearest neighbor distance to the
closest and second closest class, which can be viewed as an analogue to the trust score without
knowledge of the classiﬁer’s hard prediction.

5.2 Performance as Model Accuracy Varies

In Figure 2, we show how the performance of trust score changes as the accuracy of the classiﬁer
changes (averaged over 20 runs for each condition). We observe that as the accuracy of the model
increases, while the trust score still performs better than model conﬁdence, the amount of improvement
diminishes. This suggests that as the model improves, the information trust score can provide in
addition to the model conﬁdence decreases. However, as we show in Section 5.3, the trust score
can still have added value even when the classiﬁer is known to be of high performance on some
benchmark larger-scale datasets.

5.3 Performance on MNIST, SVHN, CIFAR-10 and CIFAR-100 Datasets

The MNIST handwritten digit dataset [35] consists of 60,000 28×28-pixel training images and
10,000 testing images in 10 classes. The SVHN dataset [38] consists of 73,257 32×32-pixel colour
training images and 26,032 testing images and also has 10 classes. The CIFAR-10 and CIFAR-100
datasets [31] both consist of 60,000 32×32-pixel colour images, with 50,000 training images and
10,000 test images. The CIFAR-10 and CIFAR-100 datasets are split evenly between 10 classes and
100 classes respectively.

8

(a) MNIST

(b) SVHN

(c) CIFAR-10

(d) MNIST

(e) SVHN

(f) CIFAR-10

Figure 3: Trust score results using convolutional neural networks on MNIST, SVHN, and CIFAR-10
datasets. Top row is detecting trustworthy; bottom row is detecting suspicious. Full chart with
CIFAR-100 (which was essentially a negative result) is shown in the Appendix.

We used a pretrained VGG-16 [49] architecture with adaptation to the CIFAR datasets based on [37].
The CIFAR-10 VGG-16 network achieves a test accuracy of 93.56% while the CIFAR-100 network
achieves a test accuracy of 70.48%. We used pretrained, smaller CNNs for MNIST and SVHN. The
MNIST network achieves a test accuracy of 99.07% and the SVHN network achieves a test accuracy
of 95.45%. All architectures were implemented in Keras [6].

One simple generalization of our method is to use intermediate layers of a neural network as an
input instead of the raw x. Many prior work suggests that a neural network may learn different
representations of x at each layer. As input to the trust score, we tried using 1) the logit layer, 2) the
preceding fully connected layer with ReLU activation, 3) this fully connected layer, which has 128
dimensions in the MNIST network and 512 dimensions in the other networks, reduced down to 20
dimensions from applying PCA.

The trust score results on various layers are shown in Figure 3. They suggest that for high dimensional
datasets, the trust score may only provide little or no improvement over the model conﬁdence at
detecting trustworthy and suspicious examples. All plots were made using α = 0; using cross-
validation to select a different α did not improve trust score performance. We also did not see much
difference from using different layers.
Conclusion:
In this paper, we provide the trust score: a new, simple, and effective way to judge if one should trust
the prediction from a classiﬁer. The trust score provides information about the relative positions of
the datapoints, which may be lost in common approaches such as the model conﬁdence when the
model is trained using SGD. We show high-probability non-asymptotic statistical guarantees that
high (low) trust scores correspond to agreement (disagreement) with the Bayes-optimal classiﬁer
under various nonparametric settings, which build on recent results in topological data analysis. Our
empirical results across many datasets, classiﬁers, and representations of the data show that our
method consistently outperforms the classiﬁer’s own reported conﬁdence in identifying trustworthy
and suspicious examples in low to mid dimensional datasets. The theoretical and empirical results
suggest that this approach may have important practical implications in low to mid dimension
settings.

https://github.com/geifmany/cifar-vgg
https://github.com/EN10/KerasMNIST
https://github.com/tohinz/SVHN-Classiﬁer

9

References

[1] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul F Christiano, John Schulman, and Dan Mané.
Concrete problems in AI safety. CoRR, abs/1606.06565, 2016. URL http://arxiv.org/
abs/1606.06565.

[2] Sivaraman Balakrishnan, Srivatsan Narayanan, Alessandro Rinaldo, Aarti Singh, and Larry
Wasserman. Cluster trees on manifolds. In Advances in Neural Information Processing Systems,
pages 2679–2687, 2013.

[3] Peter L Bartlett and Marten H Wegkamp. Classiﬁcation with a reject option using a hinge loss.

Journal of Machine Learning Research, 9(Aug):1823–1840, 2008.

[4] Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for the cluster tree.

In

Advances in Neural Information Processing Systems, pages 343–351, 2010.

[5] Frédéric Chazal. An upper bound for the volume of geodesic balls in submanifolds of Euclidean
spaces. https://geometrica.saclay.inria.fr/team/Fred.Chazal/BallVolumeJan2013.pdf, 2013.

[6] François Chollet et al. Keras. 2015.

[7] C Chow. On optimum recognition error and reject tradeoff. IEEE Transactions on Information

Theory, 16(1):41–46, 1970.

[8] Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri. Boosting with abstention. In Advances in

Neural Information Processing Systems, pages 1660–1668, 2016.

[9] Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri. Learning with rejection. In International

Conference on Algorithmic Learning Theory, pages 67–82. Springer, 2016.

[10] Corinna Cortes, Giulia DeSalvo, Claudio Gentile, Mehryar Mohri, and Scott Yang. Online

learning with abstention. arXiv preprint arXiv:1703.03478, 2017.

[11] Sanjoy Dasgupta and Samory Kpotufe. Optimal rates for k-NN density and mode estimation.

In Advances in Neural Information Processing Systems, pages 2555–2563, 2014.

[12] Luc Devroye, Laszlo Gyorﬁ, Adam Krzyzak, and Gábor Lugosi. On the strong universal
consistency of nearest neighbor regression function estimates. The Annals of Statistics, pages
1371–1385, 1994.

[13] Bernard Dubuisson and Mylene Masson. A statistical decision rule with incomplete knowledge

about classes. Pattern Recognition, 26(1):155–165, 1993.

[14] Ran El-Yaniv and Yair Wiener. On the foundations of noise-free selective classiﬁcation. Journal

of Machine Learning Research, 11(May):1605–1641, 2010.

[15] Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, et al. A density-based algorithm

for discovering clusters in large spatial databases with noise. In Kdd, pages 226–231, 1996.

[16] Wei Fan, Fang Chu, Haixun Wang, and Philip S. Yu. Pruning and dynamic scheduling of

cost-sensitive ensembles. AAAI, 2002.

[17] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The Elements of Statistical Learning.

Springer, 2001.

[18] Giorgio Fumera and Fabio Roli. Support vector machines with embedded reject option. In

Pattern Recognition with Support Vector Machines, pages 68–82. Springer, 2002.

[19] Giorgio Fumera, Fabio Roli, and Giorgio Giacinto. Multiple reject thresholds for improving
classiﬁcation reliability. In Joint IAPR International Workshops on Statistical Techniques in
Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR), pages
863–871. Springer, 2000.

[20] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing
model uncertainty in deep learning. In International Conference on Machine Learning, pages
1050–1059, 2016.

10

[21] Christopher Genovese, Marco Perone-Paciﬁco, Isabella Verdinelli, and Larry Wasserman.
Minimax manifold estimation. Journal of Machine Learning Research, 13(May):1263–1291,
2012.

[22] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversar-

ial examples. arXiv preprint arXiv:1412.6572, 2014.

[23] Yves Grandvalet, Alain Rakotomamonjy, Joseph Keshet, and Stéphane Canu. Support vector
machines with a reject option. In Advances in Neural Information Processing Systems, pages
537–544, 2009.

[24] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural

networks. arXiv preprint arXiv:1706.04599, 2017.

[25] John A Hartigan. Clustering algorithms. 1975.

[26] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassiﬁed and out-of-distribution

examples in neural networks. arXiv preprint arXiv:1610.02136, 2016.

[27] Radu Herbei and Marten H Wegkamp. Classiﬁcation with reject option. Canadian Journal of

Statistics, 34(4):709–721, 2006.

[28] Heinrich Jiang. Density level set estimation on manifolds with DBSCAN. In International

Conference on Machine Learning, pages 1684–1693, 2017.

[29] Heinrich Jiang. Uniform convergence rates for kernel density estimation. In International

Conference on Machine Learning, pages 1694–1703, 2017.

[30] Alex Kendall and Yarin Gal. What uncertainties do we need in Bayesian deep learning for
computer vision? In Advances in Neural Information Processing Systems, pages 5580–5590,
2017.

[31] Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.

[32] Volodymyr Kuleshov and Percy S Liang. Calibrated structured prediction. In Advances in

Neural Information Processing Systems, pages 3474–3482, 2015.

[33] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable
predictive uncertainty estimation using deep ensembles. In Advances in Neural Information
Processing Systems, pages 6405–6416, 2017.

[34] Thomas CW Landgrebe, David MJ Tax, Pavel Paclík, and Robert PW Duin. The interaction
between classiﬁcation and reject performance for distance-based reject-option classiﬁers. Pattern
Recognition Letters, 27(8):908–917, 2006.

[35] Yann LeCun. The MNIST database of handwritten digits. http://yann. lecun. com/exdb/mnist/,

1998.

factors, 46(1):50–80, 2004.

[36] John D Lee and Katrina A See. Trust in automation: Designing for appropriate reliance. Human

[37] Shuying Liu and Weihong Deng. Very deep convolutional neural network based image classiﬁca-
tion using small training sample size. 2015 3rd IAPR Asian Conference on Pattern Recognition
(ACPR), pages 730–734, 2015.

[38] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng.

Reading digits in natural images with unsupervised feature learning. 2011.

[39] Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High
conﬁdence predictions for unrecognizable images. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 427–436, 2015.

[40] Alexandru Niculescu-Mizil and Rich Caruana. Predicting good probabilities with supervised
learning. In Proceedings of the 22nd International Conference on Machine Learning, pages
625–632. ACM, 2005.

11

[41] Partha Niyogi, Stephen Smale, and Shmuel Weinberger. Finding the homology of submanifolds
with high conﬁdence from random samples. Discrete & Computational Geometry, 39(1-3):
419–441, 2008.

[42] Nicolas Papernot and Patrick McDaniel. Deep k-nearest neighbors: Towards conﬁdent, inter-

pretable and robust deep learning. arXiv preprint arXiv:1803.04765, 2018.

[43] Nathan Parrish, Hyrum S. Anderson, Maya R. Gupta, and Dun Yu Hsaio. Classifying with
conﬁdence from incomplete information. Journal of Machine Learning Research, 14(December):
3561–3589, 2013.

[44] John Platt. Probabilistic outputs for support vector machines and comparisons to regularized

likelihood methods. Advances in Large Margin Classiﬁers, 10(3):61–74, 1999.

[45] Foster J Provost, Tom Fawcett, and Ron Kohavi. The case against accuracy estimation for

comparing induction algorithms. In ICML, volume 98, pages 445–453, 1998.

[46] Philippe Rigollet, Régis Vert, et al. Optimal rates for plug-in estimators of density level sets.

Bernoulli, 15(4):1154–1178, 2009.

Statistics, 38(5):2678–2722, 2010.

[47] Alessandro Rinaldo and Larry Wasserman. Generalized density clustering. The Annals of

[48] Carla M Santos-Pereira and Ana M Pires. On optimal reject rules and ROC curves. Pattern

Recognition Letters, 26(7):943–952, 2005.

[49] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale

image recognition. arXiv preprint arXiv:1409.1556, 2014.

[50] Aarti Singh, Clayton Scott, Robert Nowak, et al. Adaptive Hausdorff estimation of density level

sets. The Annals of Statistics, 37(5B):2760–2782, 2009.

[51] David MJ Tax and Robert PW Duin. Growing a multi-class classiﬁer with a reject option.

Pattern Recognition Letters, 29(10):1565–1570, 2008.

[52] Francesco Tortorella. An optimal reject rule for binary classiﬁers. In Joint IAPR International
Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic
Pattern Recognition (SSPR), pages 611–620. Springer, 2000.

[53] Alexandre B Tsybakov et al. On nonparametric estimation of density level sets. The Annals of

Statistics, 25(3):948–969, 1997.

[54] Kush R Varshney and Homa Alemzadeh. On the safety of machine learning: Cyber-physical

systems, decision sciences, and data products. Big data, 5(3):246–255, 2017.

[55] Joseph Wang, Kirill Trapeznikov, and Venkatesh Saligrama. Efﬁcient learning by directed
acyclic graph for resource constrained prediction. Advances in Neural Information Processing
Systems (NIPS), 2015.

[56] Yair Wiener and Ran El-Yaniv. Agnostic selective classiﬁcation.

In Advances in Neural

Information Processing Systems, pages 1665–1673, 2011.

[57] Ming Yuan and Marten Wegkamp. Classiﬁcation methods with reject option based on convex

risk minimization. Journal of Machine Learning Research, 11(Jan):111–130, 2010.

[58] Bianca Zadrozny and Charles Elkan. Transforming classiﬁer scores into accurate multiclass
probability estimates. In Proceedings of the Eighth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages 694–699. ACM, 2002.

12

Appendix

A Supporting results for Theorem 1 Proof

We need the following result giving guarantees on the empirical balls.
Lemma 1 (Uniform convergence of balls [4]). Let F be the distribution corresponding to f and
Fn be the empirical distribution corresponding to the sample X. Pick 0 < δ < 1. Assume that
k ≥ D log n. Then with probability at least 1 − δ, for every ball B ⊂ RD we have

F(B) ≥ Cδ,n

⇒ Fn(B) > 0

F(B) ≥

+ Cδ,n

⇒ Fn(B) ≥

√

D log n
n

√

k
n
√
k
n

k
n
k
n

F(B) ≤

− Cδ,n

⇒ Fn(B) <

,

√

k
n
k
n

D log n

where Cδ,n = 16 log(2/δ)
Remark 5. For the rest of the paper, many results are qualiﬁed to hold with probability at least 1 − δ.
This is precisely the event in which Lemma 1 holds.
Remark 6. If δ = 1/n, then Cδ,n = O((log n)3/2).

To analyze Algorithm 1, we use the k-NN density estimator[12], deﬁned below.
Deﬁnition 3. Deﬁne the k-NN radius of x ∈ RD as

rk(x) := inf{r > 0 : |X ∩ B(x, r)| ≥ k},

Deﬁnition 4 (k-NN Density Estimator).

where vD is the volume of a unit ball in RD.

fk(x) :=

k
n · vD · rk(x)D ,

We will use bounds on the k-NN density estimator from [11], which are repeated here.

Deﬁne the following one-sided modulus of continuity which characterizes how much the density
increases locally:

ˆr((cid:15), x) := sup

r :

f (x(cid:48)) − f (x) ≤ (cid:15)

.

sup
x(cid:48)∈B(x,r)

δ,n. Then with probability at least 1 − δ, the

Lemma 2 (Lemma 3 of [11]). Suppose that k ≥ 4C 2
following holds for all x ∈ RD and (cid:15) > 0.
(cid:18)

(cid:19)

provided k satisﬁes vD · ˆr(x, (cid:15))D · (f (x) + (cid:15)) ≥ k

fk(x) <

1 + 2

(f (x) + (cid:15)),

Cδ,n√
k

n + Cδ,n

√
k
n .

Analogously, deﬁne the following which characterizes how much the density decreases locally:

(cid:41)

(cid:41)

(cid:40)

(cid:40)

ˇr((cid:15), x) := sup

r :

f (x) − f (x(cid:48)) ≤ (cid:15)

.

sup
x(cid:48)∈B(x,r)

δ,n. Then with probability at least 1 − δ, the

Lemma 3 (Lemma 4 of [11]). Suppose that k ≥ 4C 2
following holds for all x ∈ RD and (cid:15) > 0.
(cid:18)

(cid:19)

fk(x) ≥

1 − 2

(f (x) − (cid:15)),

provided k satisﬁes vD · ˇr(x, (cid:15))D · (f (x) − (cid:15)) ≥ k

n − Cδ,n

√
k
n .

Cδ,n√
k

13

B Proof of Theorem 1

In this section, we assume the conditions of Theorem 1. We ﬁrst show that λα, that is the density
level corresponding to the α-high-density-set, is smooth in α.
Lemma 4. There exists constants C1, r1 > 0 depending on f such that the following holds for all
0 < (cid:15) < r1 such that

0 < λα − λα−(cid:15) ≤ C1(cid:15)β/D and 0 < λα+(cid:15) − λα ≤ C1(cid:15)β/D.

Proof. We have
(cid:90)

X

(cid:15) =

1[λα−(cid:15) < f (x) ≤ λα] · f (x)dx ≥ λα−(cid:15)

1[λα−(cid:15) < f (x) ≤ λα]dx,

(cid:90)

X

where the ﬁrst equality holds by deﬁnition. Choosing (cid:15) sufﬁciently small such that Assumption 1
holds, we have

λα−(cid:15)

1[λα−(cid:15) < f (x) ≤ λα]dx

(cid:90)

X

≥ λα−(cid:15) · Vol

Hα(x) + B

(cid:16)(cid:16)

(cid:16)

0, ((λα−(cid:15) − λα)/ (cid:98)Cβ)1/β(cid:17)(cid:17)

(cid:17)

\Hα(f )

≥ λα−(cid:15) · C (cid:48)((λα−(cid:15) − λα)/ (cid:98)Cβ)D/β,

where the last inequality holds for some constant C (cid:48) depending on f and Vol is the volume w.r.t. to
the Lebesgue measure in RD. It then follows that

λα−(cid:15) − λα ≤ (cid:98)Cβ

(cid:18)

(cid:15)
λα−(cid:15) · C (cid:48)

(cid:19)β/D

,

and the result for the ﬁrst part follows by taking C1 ≤ (cid:98)Cβ · (λα−r1 · C (cid:48))−β/D and r1 < α. Showing
that 0 < λα+(cid:15) − λα ≤ C1(cid:15)β/D can be done analogously and is omitted here.

The next result gets a handle on the density level corresponding to α returned by Algorithm 1.
Lemma 5. Let 0 < δ < 1. Let (cid:98)ε be the ε setting chosen by Algorithm 1. Deﬁne

(cid:99)λα :=

k

vD · n · (cid:98)εD .

Then, with probability at least 1 − δ, we have there exist constant C1 > 0 depending on f such that
for n sufﬁciently large depending on f , we have

|(cid:98)λα − λα| ≤ C1

(cid:32)(cid:114)





log(1/δ)
n

(cid:33)β/D

+

log(1/δ)
√
k

√

log n



 .

Proof. Let ˜α > 0. Then, we have that if x ∼ f , then P(x ∈ H ˜α(f )) = 1 − ˜α. Thus, the probability
that a sample point falls in H ˜α(f ) is a Bernoulli random variable with probability 1 − ˜α. Hence, by
Hoeffding’s inequality, we have that there exist constant C (cid:48) > 0 such that

(cid:32)

P

1 − (cid:101)α − C (cid:48)

(cid:114)

log(1/δ)
n

≤

|H ˜α(f ) ∩ X|
n

≤ 1 − (cid:101)α + C (cid:48)

(cid:114)

(cid:33)

log(1/δ)
n

≥ 1 − δ/4.

Then it follows that choosing αU := α + C (cid:48)

Similarly, choosing αL = α − C (cid:48)

gives us

P

(cid:18) |HαU (f ) ∩ X|
n
(cid:113) log(1/δ)
n

P

(cid:18) |HαL(f ) ∩ X|
n

(cid:113) log(1/δ)
n

we get

(cid:19)

≤ 1 − α

≥ 1 − δ/4.

(cid:19)

≥ 1 − α

≥ 1 − δ/4.

14

Next, deﬁne

H upper
α

(f ) := {x ∈ X : fk(x) ≥ λα − (cid:15)} ,

where (cid:15) > 0 will be chosen later in order for (cid:99)Hα(f ) ⊆ H upper
(f ). By Lemma 4, there exists
C2, r1 > 0 depending on f such that for (cid:98)ε < r1 (which holds for n sufﬁciently large depending on
(cid:19)β/D
≤ λαL . As such, it sufﬁces to choose (cid:15) such
f by Lemma 1), we have λα − C2

(cid:18)(cid:113) log(1/δ)

α

n

that for all x ∈ X such that if f (x) ≥ λα − C2

then fk(x) ≥ λα − (cid:15). This is

because {x ∈ X : fk(x) ≥ λα − (cid:15)} would contain HαL (f ) ∩ X, which we showed earlier contains

(cid:18)(cid:113) log(1/δ)

(cid:19)β/D

n

at least 1 − α fraction of the samples. Deﬁne (cid:15)0 such that (cid:15) = C2

+ (cid:15)0 We have by

(cid:18)(cid:113) log(1/δ)

(cid:19)β/D

n

Assumption 1,

ˇr(x, (cid:15)0) ≥

(cid:15)0 + C2







1
ˆCβ

(cid:32)(cid:114)

(cid:33)β/D




log(1/δ)
n



1/β





(cid:32)(cid:114)

−



C2

1
ˇCβ

log(1/δ)
n



1/β

(cid:33)β/D




.

Then, there exists constant C (cid:48)(cid:48) > 0 sufﬁciently large depending on f such that if

(cid:15)0 ≥ C (cid:48)(cid:48)

(cid:32)(cid:114)





log(1/δ)
n

(cid:33)β/D

+

log(1/δ)
√
k

√

log n





then the conditions in Lemma 3 are satisﬁed for n sufﬁciently large. Thus, we have for all x ∈ X

with f (x) ≥ α − C2

(cid:18)(cid:113) log(1/δ)

(cid:19)β/D

n

, then fk(x) ≥ α − (cid:15). Hence, (cid:99)Hα(f ) ⊆ H upper

α

(f ).

We now do the same in the other direction. Deﬁne

H lower
α

(f ) := {x ∈ X : fk(x) ≥ λα + (cid:15)} ,

where (cid:15) will be chosen such that H lower

α

(f ) ⊆ (cid:99)Hα(f ). By Lemma 4, it sufﬁces to show that if

fk(x) ≥ λα + (cid:15) then f (x) ≥ λα + C2

. This direction follows a similar argument

(cid:18)(cid:113) log(1/δ)

(cid:19)β/D

n

Thus, there exists a constant C1 > 0 depending on f such that for n sufﬁciently large depending on
f , we have:

|(cid:98)λα − λα| ≤ C1

(cid:32)(cid:114)





log(1/δ)
n

(cid:33)β/D

+

log(1/δ)
√
k

√

log n



 ,

as the previous.

as desired.

The next result bounds (cid:99)Hα(f ) between two level sets of f .
Lemma 6. Let 0 < δ < 1. There exists constant C1 > 0 depending on f such that the following
holds with probability at least 1 − δ for n sufﬁciently large depending on f . Deﬁne
√





(cid:33)β/D

(cid:32)(cid:114)

H U

α (f ) :=

x ∈ X : f (x) ≥ λα − C1







log(1/δ)
n

log(1/δ)
n

(cid:32)(cid:114)

(cid:33)β/D

+

log(1/δ)
√
k

log n

√

log n

+

log(1/δ)
√
k
















.

H L

α (f ) :=

x ∈ X : f (x) ≥ λα + C1










Then,

H L

α (f ) ∩ X ⊆ (cid:99)Hα(f ) ⊆ H U

α (f ) ∩ X.

15

Proof. To simplify notation, let us deﬁne the following:

K(n, k, δ) :=

(cid:32)(cid:114)





log(1/δ)
n

(cid:33)β/D

+

log(1/δ)
√
k

√

log n



 .

By Lemma 5, there exists C2 > 0 such that deﬁning

(cid:100)H U

α (f ) := {x ∈ X : fk(x) ≥ λα − C2 · K(n, k, δ)}

(cid:100)H L

α (f ) := {x ∈ X : fk(x) ≥ λα + C2 · K(n, k, δ)} ,

then we have

α (f ).
It sufﬁces to show that there exists a constant C1 > 0 such that

α (f ) ⊆ (cid:99)Hα(f ) ⊆ (cid:100)H U

(cid:100)H L

H L

α (f ) ∩ X ⊆ (cid:100)H L

α (f ) and (cid:100)H U

α (f ) ⊆ H U

α (f ) ∩ X.

α (f ) ∩ X ⊆ (cid:100)H L

We start by showing H L
α(f ). To do this, show that for any x ∈ X satisfying
f (x) ≥ λα + C1 · K(n, k, δ) + (cid:15) satisﬁes fk(x) ≥ λα + C1 · K(n, k, δ), where (cid:15) > 0 will be chosen
later. By a similar argument as in the proof of Lemma 5, we can choose (cid:15) ≥ C (cid:48) · K(n, k, δ) for some
constant C (cid:48) > 0 and the desired result holds for n sufﬁciently large. Similarly, there exists C (cid:48)(cid:48) > 0
such that fk(x) ≤ λα − (C1 + C (cid:48)(cid:48)) · K(n, k, δ) implies that f (x) ≤ λα − C1 · K(n, k, δ). The result
follows by taking C2 = C1 + max{C (cid:48), C (cid:48)(cid:48)}.

We are now ready to prove Theorem 5, a more general version of Theorem 1 which makes the
dependence on δ explicit. Note that if δ = 1/n, then log(1/δ) = log(n).
Theorem 5. [Extends Theorem 1] Let 0 < δ < 1 and suppose that f is continuous and has compact
support X ⊆ RD and satisﬁes Assumption 1. There exists constants Cl, Cu, C > 0 depending on f
such that the following holds with probability at least 1 − δ. Suppose that k satisﬁes

Cl · log(1/δ)2 · log n ≤ k ≤ Cu · log(1/δ)2D/(2β+D) · (log n)D(2β+D) · n2β/(2β+D),

then we have

dH (Hα(f ), (cid:99)Hα(f )) ≤ C ·

(cid:16)

log(1/δ)1/2D · n−1/2D + log(1/δ)1/β · log(n)1/2β · k−1/2β(cid:17)

.

Proof of Theorem 5. Again, to simplify notation, let us deﬁne the following:

K(n, k, δ) :=

(cid:32)(cid:114)





log(1/δ)
n

(cid:33)β/D

+

log(1/δ)
√
k

√

log n



 .

There are two directions to show for the Hausdorff distance result. That (i) maxx∈(cid:100)Hα(f ) d(x, Hα(f ))
is bounded, that is none of the high-density points recovered by Algorithm 1 are far from the true
high-density region; and (ii) that supx∈Hα(f ) d(x, (cid:99)Hα(f )) is bounded, that Algorithm 1 recovers a
good covering of the entire high-density region.

We ﬁrst show (i). By Lemma 6, we have that there exists C1 > 0 such that

H U

α (f ) := {x ∈ X : f (x) ≥ λα − C1K(n, k, δ)}

contains (cid:99)Hα(f ). Thus,

max
x∈(cid:100)Hα(f )

d(x, Hα(f )) ≤ sup
x∈H U

α (f )

d(x, Hα(f )) ≤

C1 · K(n, k, δ) ·

(cid:18)

(cid:19)1/β

,

1
ˇCβ

where the second inequality holds by Assumption 1. Now for the other direction, we have by triangle
inequality that

sup
x∈Hα(f )

d(x, (cid:99)Hα(f )) ≤ sup

d(x, H L

α (f )) + sup
x∈H L

α (f )

x∈Hα(f )

d(x, (cid:99)Hα(f )).

16

The ﬁrst term can be bounded by using Assumption 1:

sup
x∈Hα(f )

d(x, H L

α (f )) ≤

C1 · K(n, k, δ) ·

(cid:18)

(cid:19)1/β

.

1
ˇCβ

Now for the second term, we see that by Lemma 6, (cid:99)Hα(f ) contains all of the sample points of H L
Thus, we have

α (f ).

sup
x∈H L

α (f )

d(x, (cid:99)Hα(f )) ≤ sup
x∈H L

α (f )

d(x, H L

α (f ) ∩ X).

By Assumption 1, for r < r0, and x ∈ H L

distribution corresponding to f . Choosing r ≥
Fn(B(x, r)) > 0 where Fn is the distribution of X and thus, we have

α (f ) we have F(B(x, r)) ≥ ρrD, where F is the
gives us that by Lemma 1 that

(cid:17)1/D

√

(cid:16) Cδ,n
ρ

D log n
n

sup
x∈H L

α (f )

d(x, H L

α (f ) ∩ X) ≤

√

(cid:18) Cδ,n
ρ

D log n
n

(cid:19)1/D

,

which is dominated by the error contributed by the other error and the result follows.

C Supporting results for Theorem 2 Proof

In this section, we note that we will reuse some notation from the last section for the manifold case.
Lemma 7 (Manifold version of uniform convergence of empirical Euclidean balls (Lemma 7 of [2])).
Let F be the true distribution and Fn be the empirical distribution w.r.t. sample X. Let N be a
minimal ﬁxed set such that each point in M is at most distance 1/n from some point in N . There
exists a universal constant C0 such that the following holds with probability at least 1 − δ. For all
x ∈ X ∪ N ,

F(B) ≥ Cδ,n

⇒ Fn(B) > 0

F(B) ≥

+ Cδ,n

⇒ Fn(B) ≥

√

d log n
n

√

k
n
√
k
n

k
n
k
n

F(B) ≤

− Cδ,n

⇒ Fn(B) <

,

√

k
n
k
n

fk(x) :=

k
n · vd · rk(x)d .

where Cδ,n = C0 log(2/δ)
Deﬁnition 5 (k-NN Density Estimator on Manifold).

d log n, Fn is the empirical distribution, and k ≥ Cδ,n.

Lemma 8 (Manifold version of fk upper bound [28]). Deﬁne the following which charaterizes how
much the density increases locally in M :

(cid:40)

(cid:41)

f (x(cid:48)) − f (x) ≤ (cid:15)

.

ˆr((cid:15), x) := sup

r :

sup
x(cid:48)∈B(x,r)∩M

Fix λ0 > 0 and δ > 0 and suppose that k ≥ C 2
that if

δ,n. Then there exists constant C1 ≡ C1(λ0, d, τ ) such

then the following holds with probability at least 1 − δ uniformly in (cid:15) > 0 and x ∈ X with
f (x) + (cid:15) ≥ λ0:

k ≤ C1 · C 2d/(2+d)

δ,n

· n2/(2+d),

(cid:18)

fk(x) <

1 + 3 ·

· (f (x) + (cid:15)),

(cid:19)

Cδ,n√
k

n − Cδ,n

√
k
n .

17

provided k satisﬁes vd · ˆr((cid:15), x)d · (f (x) + (cid:15)) ≥ k

Lemma 9 (Manifold version of fk lower bound [28]). Deﬁne the following which charaterizes how
much the density decreases locally in M :

(cid:40)

(cid:41)

f (x) − f (x(cid:48)) ≤ (cid:15)

.

ˇr((cid:15), x) := sup

r :

sup
x(cid:48)∈B(x,r)∩M

Fix λ0 > 0 and 0 < δ < 1 and suppose k ≥ Cδ,n. Then there exists constant C2 ≡ C2(λ0, d, τ )
such that if

then with probability at least 1 − δ, the following holds uniformly for all (cid:15) > 0 and x ∈ X with
f (x) − (cid:15) ≥ λ0:

k ≤ C2 · C 2d/(4+d)

δ,n

· n4/(4+d),

provided k satisﬁes vd · ˇr((cid:15), x)d · (f (x) − (cid:15)) ≥ 4
3

n + Cδ,n

√

k
n

(cid:17)

.

fk(x) ≥

1 − 3 ·

· (f (x) − (cid:15)),

(cid:18)

(cid:19)

Cδ,n√
k
(cid:16) k

D Proof of Theorem 2

The proof essentially follows the same structure as the full-dimensional case, with the primary
difference in the density estimation bounds.
Lemma 10 (Manifold Version of Lemma 4). There exists constants C1, r1 > 0 depending on f such
that the following holds for all 0 < (cid:15) < r1 such that

0 < λα − λα−(cid:15) ≤ C1(cid:15)β/d and 0 < λα+(cid:15) − λα ≤ C1(cid:15)β/d.

Proof. The proof follows the same structure as the proof of Lemma 4, with the difference being the
change in dimension, and is omitted here.

Lemma 11 (Manifold Version of Lemma 5). Let 0 < δ < 1. Let (cid:98)ε be the ε setting chosen by
Algorithm 1 after the binary search procedure. Deﬁne
k

(cid:99)λα :=

vD · n · (cid:98)εd .

Then, with probability at least 1 − δ, we have there exist constant C1 > 0 depending on f and M
such that for n sufﬁciently large depending on f and M , we have

|(cid:98)λα − λα| ≤ C1

(cid:32)(cid:114)





log(1/δ)
n

(cid:33)β/d

+

log(1/δ)
√
k

√

log n



 .

Proof. The proof is essentially the same as that of Lemma 5. The only difference is that instead
of applying the full-dimensional versions of the uniform k-NN density estimate bounds (Lemma 2
and 3), we instead apply the manifold analogues (Lemma 8 and 9). Asides from constant fac-
tors, the major difference is in the allowable range for k. In the full-dimensional case, we only
need k (cid:46) n2β/(2β+d) for the density estimation bounds to hold. However, here we require
k (cid:46) min{n2/(2+d), n2β/(2β+d)} = n2 max{1,β}/(2β(cid:48)+d).

Lemma 12 (Manifold Version of Lemma 5). Let 0 < δ < 1. There exists constant C1 > 0 depending
on f and M such that the following holds with probability at least 1 − δ for n sufﬁciently large
depending on f and M . Deﬁne

H U

α (f ) :=

x ∈ X : f (x) ≥ λα − C1

H L

α (f ) :=

x ∈ X : f (x) ≥ λα + C1










(cid:32)(cid:114)

(cid:33)β/d









log(1/δ)
n

log(1/δ)
n

(cid:32)(cid:114)

(cid:33)β/d

+

log(1/δ)
√
k

+

log(1/δ)
√
k

√

log n

√

log n


















.

Then,

H L

α (f ) ∩ X ⊆ (cid:99)Hα(f ) ⊆ H U

α (f ) ∩ X.

18

Proof. Same comment as the proof for Lemma 11.

Theorem 6. [Extends Theorem 2] Let 0 < δ < 1. Suppose that density function f is continuous
and supported on M and Assumptions 1 and 2 hold. Suppose also that there exists λ0 > 0 such that
f (x) ≥ λ0 for all x ∈ M . Then, there exists constants Cl, Cu, C > 0 depending on f such that the
following holds with probability at least 1 − δ. Suppose that k satisﬁes,

Cl · log(1/δ)2 · log n ≤ k ≤ Cu · log(1/δ)2d/(2β(cid:48)+d) · (log n)d(2β(cid:48)+d) · n2β(cid:48)/(2β(cid:48)+d)

where β(cid:48) := max{1, β}. Then we have

dH (Hα(f ), (cid:99)Hα(f )) ≤ C ·

(cid:16)

log(1/δ)1/2d · n−1/2d + log(1/δ)1/β · log(n)1/2β · k−1/2β(cid:17)

.

Proof of Theorem 6. Proof is the same as the full-dimensional case given the contributed Lemmas of
this section and is omitted here.

E Supporting Results for Theorem 3 Proof

Next, we need the following on the volume of the intersection of the Euclidean ball and M ; this is
required to get a handle on the true mass of the ball under FM in later arguments. The upper and
lower bounds follow from [5] and Lemma 5.3 of [41]. The proof can be found e.g. in [28].
Lemma 13 (Ball Volume). If 0 < r < min{τ /4d, 1/τ }, and x ∈ M then

vdrd(1 − τ 2r2) ≤ vold(B(x, r) ∩ M ) ≤ vdrd(1 + 4dr/τ ),
where vd is the volume of a unit ball in Rd and vold is the volume w.r.t. the uniform measure on M .

The next is a bound uniform convergence of balls:
Lemma 14 (Lemma 3 of [29]). Let B be the set of all balls in RD, F is some distribution and Fn is
an empirical distribution. With probability at least 1 − δ, the following holds uniformly for every
B ∈ B and γ ≥ 0:

F(B) ≥ γ ⇒ Fn(B) ≥ γ − βn
F(B) ≤ γ ⇒ Fn(B) ≤ γ + βn

√

√

γ − β2
n,
γ + β2
n,

where βn = 8d log(1/δ)(cid:112)log n/n.

F Proof of Theorem 3

The ﬁrst result says that within the manifold, the vast majority of the probability mass is attributed to
the manifold distributions.
Lemma 15. There exists C1, r1 > 0 depending on FM , FE, M such that the following holds
uniformly over x ∈ M and 0 < r < r1.

Proof. Let x ∈ M and r > 0. We have

FM (B(x, r)) ≥ λ0 · vold(B(x, r) ∩ M ) ≥ vdrd(1 − τ 2r2) · λ0,
where the second inequality holds by Lemma 13 for r sufﬁciently small. On the other hand, we have

Thus, we have there exists C1 > 0 depending on fM , M , and fE such that

FE(B(x, r))
FM (B(x, r))

≤ C1 · rD−d.

FE(B(x, r)) ≤ ||fE||∞vDrD.

FE(B(x, r))
FM (B(x, r))

≤ C1 · rD−d,

19

as desired.

We next show that points far away from H
Algorithm 1.
Lemma 16. There exists ω0 > 0 such that for any 0 < ω < ω0 and n sufﬁciently large depending
on FM , FE, M and ω, with probability at least 1 − δ, Algorithm 1 will not select any points outside
of H

(cid:101)α(fM ) do not get selected as high-density points by

(cid:101)α−ω(fM ).

Proof. By Assumption 1, we can choose ω sufﬁciently small so that for the density fM , |λ
(cid:101)α−ω −
(cid:101)α| ≤ ˇCβ ·(rc/3)β. Then, at the ((cid:101)α−ω)-density level, we will be within the area where the regularity
λ
assumptions hold.
Next, by Hoeffding’s inequality, we have that there exist constant C (cid:48) > 0 such that for ¯α > 0:

(cid:32)

P

1 − ¯α − C (cid:48)

(cid:114)

log(1/δ)
n

≤

|H ¯α−η
1−η

(fM ) ∩ X|

n

≤ 1 − ¯α + C (cid:48)

≥ 1 − δ/3.

(cid:114)

(cid:33)

log(1/δ)
n

Choosing ¯α = α − C (cid:48)

, then it follows that with probability at least 1 − δ/3,

(cid:113) log(1/δ)
n

H0 := H

√

(cid:101)α−C(cid:48)

log(1/δ)/(

n·(1−η))

√

(fM )

satisﬁes |H0 ∩ X| > (1 − α) · n. Next let

Hω := H

(cid:101)α−ω(fM ).

Let r be the value of ε used by Algorithm 1. Now, it sufﬁces to show that for n sufﬁciently large
depending on fM :

max
x∈X\Hω

Fn(B(x, r)) < min
x∈H0

Fn(B(x, r)),

where Fn is the empirical distribution. This is because Algorithm 1 ﬁlters out sample points whose
ε-ball has less than k sample points for its ﬁnal ε value, which is the value which allows it to ﬁlter
α-fraction of the points.

By Lemma 15, it sufﬁces to show that

max
x∈X\Hω

FM,n(B(x, r)) (cid:0)1 + C1rD−d(cid:1) < min
x∈H0

FM,n(B(x, r)) (cid:0)1 − C1rD−d(cid:1) ,

where FM,n(A) denote the fraction of samples drawn from FM which lie in A w.r.t. our entire
sample X.

Then, by Lemma 14, it will be enough to show that

max
x∈X\Hω

< min
x∈H0

(FM (B(x, r)) + βn

(FM (B(x, r)) − βn

(cid:112)FM (B(x, r)) + β2
(cid:112)FM (B(x, r)) − β2

n) (cid:0)1 + C1rD−d(cid:1)
n) (cid:0)1 − C1rD−d(cid:1) ,

where βn = 8d log(1/δ)(cid:112)log n/n.
To bound the LHS, we have by Lemma 13

max
x∈X\Hω

(FM (B(x, r)) + βn · (cid:112)FM (B(x, r)) + β2

n) (cid:0)1 + C1rD−d(cid:1)

≤ max

x∈X\Hω

inf
x(cid:48)∈B(x,r)

fM (x(cid:48))

(cid:26)(cid:18)

(cid:26)(cid:18)

(cid:19)

(cid:19)

≤ max

x∈X\Hω

inf
x(cid:48)∈B(x,r)

fM (x(cid:48))

(1 + C2βn + C3r)

≤ (λ

(cid:101)α−ω + ι(fM , r)) (1 + C2βn + C3r) ,

(cid:27)

(cid:16)

·

1 + βn/(cid:112)||fM ||∞ + β2

n/||fM ||∞

(cid:17) (cid:0)1 + C1rD−d(cid:1) (1 + 4dr/τ )

(cid:27)

for some C2, C3 > 0 and ι
:=
supx,x(cid:48)∈M :|x−x(cid:48)|≤r |fM (x) − fM (x(cid:48))| (i.e. fM is uniformly continuous since it is continuous over a
compact support, so ι(fM , r) → 0 as r → 0).

is the modulus of continuity,

is ι(fM , r)

that

20

Similarly, for the RHS, we can show for some constants C4, C5 that

(FM (B(x, r)) − βn

(cid:112)FM (B(x, r)) − β2

n) (cid:0)1 − C1rD−d(cid:1)

log(1/δ)/(

n(1−η))

√

− ι(fM , r)) (1 − C4βn − C5r) .

min
x∈H0
≥ (λ

√

(cid:101)α−C(cid:48)

The result follows since r → 0 as n → ∞ (since by Lemma 1, r is a k-NN radius so r (cid:46) (k/n)1/D →
0 given the conditions on k of Theorem 3) and the fact that λ
n for n
sufﬁciently large. As desired.

(cid:101)α−ω < λ

log(1/δ)/

(cid:101)α−C(cid:48)

√

√

Lemma 17 (Bounding density estimators w.r.t to entire sample vs w.r.t. samples on manifold). For
x ∈ RD, deﬁne the following:

rk(x) := inf{(cid:15) > 0 : |B(x, (cid:15)) ∩ X| ≥ k}
(cid:101)rk(x) := inf{(cid:15) > 0 : |B(x, (cid:15)) ∩ X ∩ M | ≥ k}
where the former is simply the k-NN radius we’ve been using thus far and the latter is the k-NN
radius if we were to restrict the samples to only those that came from M . Then likewise, deﬁne the
analogous density estimators:

fk(x) :=

n · vd · rk(x)d and (cid:101)fk(x) :=

k

k

n · vd · (cid:101)rk(x)d ,

where again, the former is the usual k-NN density estimator on manifolds. Then, there exists C1 such
that the following holds with high probability.

|fk(x) − (cid:101)fk(x)| ≤ C1 · (k/n)D/d−1.

sup
x∈M

Proof. By Lemma 14, there exists C2 > 0 depending on F and FM such that

Fn(B(x, rk(x)) = k ⇒ |F(B(x, rk(x)) − k| ≤ C2βn,

and

FM,n(B(x, (cid:101)rk(x)) = k ⇒ |FM (B(x, (cid:101)rk(x)) − k| ≤ C2βn,
where FM,n is the empirical distribution w.r.t. X ∩ M . Next, by Lemma 15, we have for some
constant C3 > 0:

F(B(x, (cid:101)rk(x)) ≤ FM (B(x, (cid:101)rk(x))(1 + C3 (cid:101)rk(x)D−d)

≤ (k + C2βn)(1 + C3 · (cid:101)rk(x)D−d)
≤ (k + C2βn)(1 + C4 · (k/n)D/d−1)
≤ k · (1 + C5(k/n)D/d−1),

where the second last inequality holds for some constant C4 > 0 by Lemma 7 and C5 > 0 is some
constant depending on F and FM and M . Then it follows that for some constant C6 > 0, we have

F(B(x, (cid:101)rk(x))
F(B(x, rk(x))

≤ 1 + C6(k/n)D/d−1.

In the other direction, we trivially have (cid:101)rk(x) ≥ rk(x), so

1 ≤

F(B(x, (cid:101)rk(x))
F(B(x, rk(x))

≤ 1 + C6(k/n)D/d−1.

The result follows.

Theorem 7. [Extends Theorem 3] Let 0 < η < α < 1 and 0 < δ < 1. Suppose that distribution F
is a weighted mixture (1 − η) · FM + η · FE where FM is a distribution with continous density fM
supported on a d-dimensional manifold M satisfying Assumption 2 and FE is a (noise) distribution
with continuous density fE with compact support over RD with d < D. Suppose also that there exists
λ0 > 0 such that fM (x) ≥ λ0 for all x ∈ M and H
1−η ) satisﬁes Assumption 1
for density fM . Let (cid:98)Hα be the output of Algorithm 1 on a sample X of size n drawn i.i.d. from F.

(cid:101)α(fM ) (where (cid:101)α := α−η

21

Then, there exists constants Cl, Cu, C > 0 depending on fM , fE, η, M such that the following holds
with probability at least 1 − δ. Suppose that k satisﬁes

Cl · log(1/δ)2 · log n ≤ k ≤ Cu · log(1/δ)2d/(2β(cid:48)+d) · (log n)d(2β(cid:48)+d) · n2β(cid:48)/(2β(cid:48)+d)

where β(cid:48) := max{1, β}. Then we have

dH (H

(cid:101)α(fM ), (cid:99)Hα) ≤ C ·

(cid:16)

log(1/δ)1/2d · n−1/2d + log(1/δ)1/β · log(n)1/2β · k−1/2β(cid:17)

.

Proof of Theorem 7. The proof follows in a similar way as that of Theorem 6, except with the
complexity of having added full-dimensional noise. We will only highlight the difference and provide
a sketch of the proof here.

Lemma 16 and 17 give us a handle on the additional complexity when having separate noise
distribution, compared to the earlier manifold setting of Theorem 2.

Lemma 16 guarantees that the points in (cid:99)Hα lie in the inside of M with margin. In particular, that
means the noise points are ﬁltered out by the algorithm and thus, we are reduced to reasoning about
the (cid:101)α-high-density-set of fM .
Then, Lemma 17 ensures that the k-NN density estimator used for our analysis for the entire sample
X is actually quite close to the k-NN density estimator with respect to M ∩ X within M . In other
words, we can use the k-NN density estimator to estimate fM without knowing which samples
of X were in M . Lemma 17 shows that the additional error in density estimation we obtain is
≈ (k/n)D/d−1 (cid:46) (k/n)1/d (cid:46) (log n)/
k, where the ﬁrst inequality holds since D > d and the
latter holds from the conditions on k. It turns out that this error term can be absorbed as a constant in
the previous result of Theorem 6.

√

G Proof of Theorem 4

Proof of Theorem 4. For the ﬁrst inequality, we have

ξ(h, x) ≥

d(x, M

(cid:101)h(x)) − (cid:15)n
d(x, Mh(x)) + (cid:15)n

=

d(x, M

(cid:101)h(x))
d(x, Mh(x))

−

(cid:15)n
d(x, Mh(x)) + (cid:15)n

(cid:32) d(x, M

(cid:101)h(x))
d(x, Mh(x))

·

(cid:33)

+ 1

,

where the ﬁrst inequality holds by Theorem 3. This, along with the condition on γ and ε(h, x) fromo
the theorem statement, implies that

d(x, M

(cid:101)h(x))
d(x, Mh(x))

< 1 − γ,

which implies that h(x) (cid:54)= h∗(x). For the second inequality, we have

1
ξ(h, x)

≥

d(x, Mh(x)) − (cid:15)n
(cid:101)h(x)) + (cid:15)n
d(x, M

=

d(x, Mh(x))
(cid:101)h(x))
d(x, M

−

(cid:15)n
(cid:101)h(x)) + (cid:15)n

·

d(x, Mh(x))
(cid:101)h(x))
d(x, M

d(x, M

(cid:32)

(cid:33)

+ 1

,

where the ﬁrst inequality holds by Theorem 3. Thus, if the condition of the theorem statement holds,
then

d(x, Mh(x))
(cid:101)h(x))
d(x, M

< 1 − γ ⇒

< 1 − γ

d(x, Mh(x))
d(x, Mc)

for all c (cid:54)= h(x), which implies that h(x) = h∗(x)

22

H Additional UCI Experiments

H.1 When to trust: Precision for correct predictions by percentile

Figure 4: UCI data sets and precision on correctness

23

H.2 When to not trust: Precision for misclassiﬁcation predictions by percentile

Figure 5: UCI data sets and precision on incorrectness

H.3 High dimensional Datasets

24

(a) MNIST

(b) MNIST

(c) SVHN

(d) SVHN

(e) CIFAR-10

(f) CIFAR-10

(g) CIFAR-100

(h) CIFAR-100

Figure 6: Trust score results using convolutional neural networks on MNIST, SVHN, CIFAR-10, and
CIFAR-100 datasets. Left column is detecting trustworthy; right column is detecting suspicious.

25

