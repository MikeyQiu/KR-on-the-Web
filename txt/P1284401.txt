RaspiReader: Open Source Fingerprint Reader

Joshua J. Engelsma, Kai Cao, and Anil K. Jain, Life Fellow, IEEE

1

Abstract—We open source an easy to assemble, spoof resistant, high resolution, optical ﬁngerprint reader, called RaspiReader, using
ubiquitous components. By using our open source STL ﬁles and software, RaspiReader can be built in under one hour for only US
$175. As such, RaspiReader provides the ﬁngerprint research community a seamless and simple method for quickly prototyping new
ideas involving ﬁngerprint reader hardware. In particular, we posit that this open source ﬁngerprint reader will facilitate the exploration
of novel ﬁngerprint spoof detection techniques involving both hardware and software. We demonstrate one such spoof detection
technique by specially customizing RaspiReader with two cameras for ﬁngerprint image acquisition. One camera provides high
contrast, frustrated total internal reﬂection (FTIR) ﬁngerprint images, and the other outputs direct images of the ﬁnger in contact with
the platen. Using both of these image streams, we extract complementary information which, when fused together and used for spoof
detection, results in marked performance improvement over previous methods relying only on grayscale FTIR images provided by
COTS optical readers. Finally, ﬁngerprint matching experiments between images acquired from the FTIR output of RaspiReader and
images acquired from a COTS reader verify the interoperability of the RaspiReader with existing COTS optical readers.

Index Terms—Raspberry Pi, Frustrated Total Internal Reﬂection (FTIR), Open Source Fingerprint Readers, Presentation Attack
Detection, Spoof Detection, Interoperability

(cid:70)

7
1
0
2
 
c
e
D
 
6
2
 
 
]

V
C
.
s
c
[
 
 
1
v
2
9
3
9
0
.
2
1
7
1
:
v
i
X
r
a

1 INTRODUCTION

O NE of the major challenges facing biometric technology

today is the growing threat of presentation attacks1 [2].
The most common type of presentation attack (referred to
as spooﬁng) occurs when a hacker intentionally assumes
the identity of unsuspecting individuals, called victims here,
through stealing their ﬁngerprints, fabricating spoofs with
the stolen ﬁngerprints, and maliciously attacking ﬁngerprint
recognition systems with the spoofs into identifying the
hacker as the victim2 [3], [4], [5], [6].

The need to prevent spoof attacks is becoming increas-
ingly urgent due to the monumental costs and loss of
user privacy associated with spoofed systems. Consider
for example India’s Aadhaar program which (i) provides
beneﬁts and services to an ever growing population of
over 1.2 billion residents through ﬁngerprint and/or iris
recognition [7], [8] and (ii) facilitates electronic ﬁnancial
transactions through the Uniﬁed Payments Interface (UPI)
[9]. Failure to detect spoof attacks in the Aadhaar system
could cause the disruption of a commerce system affecting
untold numbers of people. Also consider the United States
Ofﬁce of Biometric Identity Management (US OBIM) which
is responsible for supporting the Department of Homeland
Security (DHS) with biometric identiﬁcation services specif-
ically aimed at preventing people who pose security risks to
the United States from entering the country [10]. Failure to
detect spoofs on systems deployed by OBIM could result in

•

J. J. Engelsma, K. Cao and A. K. Jain are with the Department of Computer
Science and Engineering, Michigan State University, East Lansing, MI,
48824
E-mail: {engelsm7, kaicao, jain}@cse.msu.edu

1. In ISO standard IEC 30107-1:2016(E), presentation attacks are de-
ﬁned as the “presentation to the biometric data capture subsystem with the
goal of interfering with the operation of the biometric system” [1].

2. Presentation attacks can also occur when (i) two individuals are in
collusion or (ii) an individual obfuscates his or her own ﬁngerprints to
avoid recognition [3]. However, in this paper our speciﬁc aim is to stop
ﬁngerprint spooﬁng presentation attacks.

Fig. 1. Prototype of RaspiReader: two ﬁngerprint images (b, (i)) and
(b, (ii)) of the input ﬁnger (a) are captured. The raw direct image (b,
(i)) and the raw, high contrast FTIR image (b, (ii)) both contain useful
information for spoof detection. Following the use of (b, (ii)) for spoof
detection, image calibration and processing are performed on the raw
FTIR image to output a high quality, 500 ppi ﬁngerprint for matching (b,
(iii)). The dimensions of the RaspiReader shown in (a) are 100 mm x
100 mm x 105 mm (about the size of a 4 inch cube).

a deadly terrorist attack3. Finally, almost all of us are actively
carrying ﬁngerprint recognition systems embedded within
our personal smart devices. Failure to detect spoof attacks
on smartphones [12] could compromise emails, banking
information, social media content, personal photos and a
plethora of other conﬁdential information.

In an effort to mitigate the costs associated with spoof
attacks, a number of spoof detection techniques involving

3. In 2012, a journalist successfully demonstrated that the Hong

Kong-China border control system could be easily spoofed [11].

2

Fig. 2. Fingerprint images acquired using the RaspiReader. Images in (a) were collected from a live ﬁnger. Images in (b) were collected from a
spoof ﬁnger. Using features extracted from both raw image outputs ((i), direct) and ((ii), FTIR) of the RaspiReader, our spoof detectors are better
able to discriminate between live ﬁngers and spoof ﬁngers. The raw FTIR image output of the RaspiReader (ii) can be post processed (after spoof
detection) to output images suitable for ﬁngerprint matching. Images in (c) were acquired from the same live ﬁnger (a) and spoof ﬁnger (b) on a
commercial off-the-shelf (COTS) 500 ppi optical reader. The close similarity between the two images in (c) qualitatively illustrates why current spoof
detectors are limited by the low information content, processed ﬁngerprint images (c, (iii)) output by COTS readers.

both hardware and software have been proposed in the
literature. Special hardware embedded in ﬁngerprint read-
ers4 enables capture of features such as heartbeat, thermal
output, blood ﬂow, odor, and sub-dermal ﬁnger character-
istics useful for distinguishing a live ﬁnger from a spoof
[3], [13], [15], [16], [17], [18], [19], [20], [21]. Spoof detection
methods in software are based on extracting textural [22],
[23], [24], [25], [26], anatomical [27], and physiological [28],
[29] features from processed5 ﬁngerprint images which are
used in conjunction with a classiﬁer such as Support Vec-
tor Machines (SVM). Alternatively, a Convolutional Neural
Network (CNN) can be trained to distinguish a live ﬁnger
from a spoof [30], [31], [32].

While existing hardware and software spoof detection
schemes provide a reasonable starting point for solving the
spoof detection problem, current solutions have a plethora
of shortcomings. As noted in [19], [20], [21] most hardware
based approaches can be easily bypassed by developing
very thin spoofs (Fig. 3 (a)), since heartbeat, thermal output,
and blood ﬂow can still be read from the live human skin be-
hind the thin spoof. Additionally, some of the characteristics
(such as odor and heartbeat) acquired by the hardware vary
tremendously amongst different human subjects, making it
very difﬁcult to build an adequate model representative of
all live subjects [19], [20].

Current spoof detection software solutions have their
own limitations. Although the LivDet 2015 competition

4. Several ﬁngerprint vendors have developed hardware spoof de-
tection solutions by employing multispectral imaging, infrared imaging
(useful for sub-dermal ﬁnger analysis), and pulse capture to distinguish
live ﬁngers from spoof ﬁngers [13], [14].

5. Raw ﬁngerprint images are “processed” (such as RGB to grayscale
conversion, contrast enhancement, and scaling) by COTS readers to
boost matching performance. However, useful spoof detection informa-
tion (such as color and/or minute textural abberations) is lost during
this processing.

reported state-of-the-art spoof detection software to have an
average accuracy of 95.51% [33], the spoof detection perfor-
mance at desired operating points such as False Detect Rate
(FDR) of 0.1% was not reported, and very limited evaluation
was performed to determine the effects of testing spoof
detectors with spoofs fabricated from materials not seen
during training (cross-material evaluation). In the limited
cross material evaluation that was performed, the rate of
spoofs correctly classiﬁed as spoofs was shown to drop from
96.57% to 94.20% [33]. While this slight drop in accuracy
seems promising, without knowing the performance at ﬁeld
conditions, namely False Detect Rate (FDR)6 of 0.1% on a
larger collection of unknown materials, the reported levels
of total accuracy should be accepted with caution. Chugh
et al. [32] pushed state-of-the-art ﬁngerprint spoof detec-
tion performance on the LivDet 2015 dataset from 95.51%
average accuracy to 98.61% average accuracy using a CNN
trained on patches around minutiae points, but they also
demonstrated that performance at strict operating points
dropped signiﬁcantly in some experiments. For example,
Chugh et al. reported an average accuracy on the LivDet
2011 dataset of 97.41%, however, at a FDR of 1.0%, the
TDR was only 90.32%, indicating that current state-of-the-
art spoof detection systems leave room for improvement at
desired operating points. Finally, several other studies have
reported up to a three-fold increase in error when testing
spoof detectors on unknown material types [34], [35], [36].

Because of the less than desired performance of spoof
detection software to adapt to spoofs fabricated from unseen
materials, studies in [37], [38], and [39] developed open-
set recognition classiﬁers to better detect spoofs fabricated
with novel material types. However, while these classiﬁers

6. The required operating point for the ODIN program supporting

this research is FDR = 0.2%

3

Fig. 3. Example spoof ﬁngers and live ﬁngers in our database. (a) Spoof ﬁngers and (b) live ﬁngers used to acquire both spoof ﬁngerprint impressions
and live ﬁngerprint impressions for conducting the experiments reported in this paper. The spoofs in (a) and the live ﬁngers in (b) are not in 1-to-1
correspondence.

are able to generalize to spoofs made with new materials
better than closed-set recognition algorithms, their overall
accuracy (approx. 85% - 90%) still does not meet the desired
performance for ﬁeld deployments.

tection) (Fig. 2 (i)). Both images of the RaspiReader visually
differentiate between live ﬁngers and spoof ﬁngers much
more than the processed ﬁngerprint images output by COTS
ﬁngerprint readers (Fig. 2 (c)).

Given the limitations of state-of-the-art ﬁngerprint spoof
detection (both in hardware and software), it is evident that
much work remains to be done in developing robust and
generalizable spoof detection solutions. We posit that one of
the biggest limitations facing the most successful spoof de-
tection solutions to date (such as use of textural features [36]
and CNNs [30], [31], [32]), is the processed COTS ﬁngerprint
reader images used to train spoof detectors. In particular,
because COTS ﬁngerprint readers output ﬁngerprint images
which have undergone a number of image processing oper-
ations (in an effort to achieve high matching performance),
they are not optimal for ﬁngerprint spoof detection, since
valuable information such as color and textural aberrations
is lost during the image processing operations. By removing
color and minute textural details from the raw ﬁngerprint
images, spoof ﬁngerprint impressions and live ﬁngerprint
impressions (acquired on COTS optical readers) appear very
similar (Fig. 2 (c)), even when the physical live/spoof ﬁngers
used to collect the respective ﬁngerprint impressions appear
very different (Fig. 3).

This limitation inherent to many existing spoof detec-
tion solutions motivated us to develop a custom, optical
ﬁngerprint reader, called RaspiReader, with the capability
to output 2 raw images (from 2 different cameras) for spoof
detection. By mounting two cameras at appropriate angles
to a glass prism (Fig. 4), one camera is able to capture high
contrast FTIR ﬁngerprint images (useful for both ﬁngerprint
spoof detection and ﬁngerprint matching) (Fig. 2 (ii)), while
the other camera captures direct images of the ﬁnger skin
in contact with the platen (useful for ﬁngerprint spoof de-

RaspiReader’s two camera approach is similar to that
which was prescribed by Rowe et al. in [13], [17] where
both an FTIR image and a direct view image were acquired
using different wavelength LEDs, however, the commercial
products developed around the ideas in [13], [17] act as a
proprietary black box outputting only a single processed
composite image of a collection of raw image frames cap-
tured under various wavelengths. As such, ﬁngerprint re-
searchers cannot implement new spoof detection schemes
on the individual raw frames captured by the reader. Fur-
thermore, unlike the patented ideas in [13], RaspiReader is
built with ubiquitous components and open source software
packages, enabling ﬁngerprint researchers to very easily
prototype their own RaspiReader, further customize it with
new spoof detection hardware, and gain direct access to
the raw images captured by the reader. In short, the low
cost ($175 USD) and easy to implement (1 hour build time)
RaspiReader is a truly unique concept which we posit will
push the boundaries of state-of-the-art ﬁngerprint spoof
detection, by facilitating spoof detection schemes which use
both hardware and software.

Experiments demonstrate that by utilizing the two cam-
eras of RaspiReader, we are able to signiﬁcantly boost the
performance of state-of-the-art spoof detectors previously
trained on COTS grayscale images (both on known-material
and cross-material testing scenarios). In particular, because
both image outputs of the RaspiReader are raw and contain
useful color information, we can extract discriminative and
complementary information from each of the image outputs.
By fusing this complementary information (at a feature

4

This is particularly true of ﬁngerprint recognition systems.
As shown in the NIST FpVTE 2013 [40] results, the single
most important factor responsible for degrading ﬁngerprint
recognition performance is the ﬁngerprint image quality.
However, most ﬁngerprint researchers have no control over
the quality of the ﬁngerprint images being used to develop
ﬁngerprint recognition algorithms since they must rely on
blackbox COTS ﬁngerprint readers. RaspiReader changes
this by providing ﬁngerprint matching algorithm designers
an easy method for prototyping their own ﬁngerprint reader
and optimizing ﬁngerprint image quality and ﬁngerprint
matching algorithms jointly in an effort to further improve
ﬁngerprint recognition performance.

In summary, our work on RaspiReader removes the mys-
tery of designing and understanding the internals of a ﬁn-
gerprint reader. Using the open-source fabrication process
of this ﬁngerprint reader, any ﬁngerprint algorithm designer
can quickly and affordably construct his or her own reader
with the capabilities (spoof detection and matching image
quality) necessary to meet their application requirements.
More concisely, the contributions of this research are:

• An open source, easy to assemble, cost effective ﬁn-
gerprint reader, called RaspiReader, capable of pro-
ducing ﬁngerprint images useful for spoof detection
and that are of high quality and resolution (1,500 ppi
- 3,300 ppi native resolution) for ﬁngerprint match-
ing. The custom RaspiReader can be easily modiﬁed
to facilitate spoof detection and ﬁngerprint matching
studies.

• A customized ﬁngerprint reader with two cameras
for image acquisition rather than a single camera.
Use of two cameras enables robust ﬁngerprint spoof
detection, since we can extract features from two
complementary, information rich images instead of
processed grayscale images output by traditional
COTS optical ﬁngerprint readers.

• A signiﬁcant boost in spoof detection performance
(both known-material and seven cross-material test-
ing scenarios) using current state-of-the-art software
based spoof detection methods in conjunction with
RaspiReader images as opposed to COTS optical
grayscale images. Spoofs of seven materials were
used in both known-material and cross-material test-
ing scenarios.

• Demonstrated matching

interoperability

of
RaspiReader with a COTS optical ﬁngerprint reader.
Since RaspiReader is shown to be interoperable with
COTS readers, it could immediately be deployed
in the real world since interoperability makes the
device compatible with legacy ﬁngerprint databases.

2 RASPIREADER CONSTRUCTION AND CALIBRA-
TION

In this section, the construction of the RaspiReader us-
ing ubiquitous, off-the-shelf components (Table 1) is ex-
plained. In particular, the main steps involved in construct-
ing RaspiReader consist of (i) properly mounting cameras
(angle and position) with respect to a glass prism, (ii)
fabricating a plastic case to house the hardware components,

Fig. 4. Schematic illustrating RaspiReader functionality. Incoming white
light from three LEDs enters the prism. Camera 2 receives light rays
reﬂected from the ﬁngerprint ridges only (light rays are not reﬂected back
from the ﬁngerprint valleys due to total internal reﬂection (TIR)). This
image from Camera 2, with high contrast between ridges and valleys
can be used for both spoof detection and ﬁngerprint matching. Camera 1
receives light rays reﬂected from both the ridges and valleys. This image
from Camera 1 provides complementary information for spoof detection.

level or score level) the performance of spoof detectors is
signiﬁcantly higher than when features are extracted from
COTS grayscale images.

Finally, by calibrating and processing the FTIR im-
age output of the RaspiReader (post spoof detection), we
demonstrate that RaspiReader is not only interoperable with
existing COTS optical readers but is also capable of achiev-
ing state-of-the-art ﬁngerprint matching accuracy. Note that
interoperability with existing COTS readers is absolutely
vital in any new hardware based spoof detection solution
as it makes the spoof resistant device compatible (in terms
of matching) with legacy ﬁngerprint databases7. Further-
more, by making the RaspiReader compatible with existing
COTS readers, we further extend the utility of RaspiReader
beyond spoof detection. In particular, RaspiReader is not
only useful for providing direct access to multiple raw
images for spoof detection; it also provides researchers in
ﬁngerprint matching the easy ability to ﬁne tune (resolution
and processing) the images being output by the ﬁngerprint
reader. In any imaging system, the recognition performance
depends on the quality of the image output by the sensor.

7. Interoperability with existing COTS readers is a strict requirement

of the IARPA ODIN program supporting this research [2].

Table 1: Primary Components Used to Construct RaspiReader. Total Cost is $175.20 (as of December 12, 2017)

Component Image

Name and Description

Quantity

Cost (USD)1

Raspberry Pi 3B: A single board computer (SBC) with 1.2 GHz 64-bit

quad-core CPU, 1 GB RAM, MicroSDHC storage, and Broadcom

$38.27

VideoCore IV Graphic card

5

$13.49

$49.99

$0.10

$5.16

$54.50

1

2

1

3

3

1

Raspberry Pi Camera Module V1: A 5.0 megapixel, 30 frames per

second, ﬁxed focal length camera

Multi-Camera Adapter: Splits Raspberry Pi camera slot into two

slots, enabling connection of two cameras

LEDs: white light, 5 mm, 1 watt

Resistors: 1 kΩ

Right Angle Prism:2 25 mm leg, 35.4 mm hypotenuse

1 All items except the glass prism were purchased for the listed prices on Amazon.com
2 The glass prism was purchased from ThorLabs [41].

(iii) assembling the cameras and hardware within the plastic
case, and (iv) writing software to capture ﬁngerprint images
with the assembled hardware. Each of these steps is de-
scribed in more detail in the following subsections. Finally,
we provide the steps for calibrating and processing the raw
FTIR ﬁngerprint images of the RaspiReader for ﬁngerprint
matching.

2.1 Camera Placement

The most important step in constructing RaspiReader is
the placement (angle and position) of the two cameras
capturing ﬁngerprint images. In particular, to collect an
FTIR image of a ﬁngerprint, a camera needs to be mounted
at an angle greater than the critical angle, and to collect a
direct view image, a camera needs to be mounted an an
angle less than the critical angle (both with respect to the
platen). Here, the critical angle is deﬁned as the angle at
which total internal reﬂection occurs when light passes from
a medium with an index of refraction n1 to another medium
with index of refraction n2 (Eq. 1):

to the prism, the ﬁngerprint image resolution (pixels per
inch) is increased. However, if the cameras are too close
to the platen, only part of the ﬁngerprint image is within
the ﬁeld of view (FOV). In constructing RaspiReader, we
wanted to maximize the ﬁngerprint image resolution, while
still capturing the entire ﬁngerprint image within the FOV.
We experimentally determined that at a distance of 23 mm
from the prism, the cameras would capture the entire ﬁnger-
print area. At closer distances, part of the ﬁngerprint image
would start to be outside the FOV. As a ﬁnal step in camera
placement, the focal length of the Raspicams (cameras used
in RaspiReader) must be increased so that the camera will
focus on the nearby glass prism (the default focus-length
of the Raspicams is 1 meter; much greater than the 23 mm
distant prism). By default, the Raspicams have a ﬁxed-focal
length of 3.6 mm. However, by rotating the Raspicam lens
652.5◦ counterclockwise (for the FTIR imaging camera) and
405◦ counterclockwise (for the direct imaging camera), the
focal length can be slightly increased to bring the nearby
ﬁngerprint images into focus.

θc = arcsin(

n2
n1

)

2.2 Case Fabrication

(1)

In the case of ﬁngerprint sensing, the ﬁrst medium is
glass which has an index of refraction n1 = 1.5, and the
second medium is air which has an index of refraction of
n2 = 1.0 leading to a critical angle (θc) of 41.8◦. Therefore,
as shown in (Fig. 4), we mount the direct view camera
(camera1) at an angle of θ1 = 10◦ and we mount the FTIR
camera (camera2) at an angle of θ2 = 45◦.

With respect to the position of each camera lens to
the glass prism, there is a tradeoff between resolution and
ﬁngerprint area to consider. As the camera is moved closer

After determining the angle and position of both cameras,
an outer casing (Fig. 5) accommodating these positions
is electronically modeled using Meshlab [42] and subse-
quently 3D printed on a high resolution 3D printer (Strata-
sys Objet350 Connex)8. To make the fabrication process
easily reproducible, the camera mounts and light source
mounts are modeled in place on the front part of the
ﬁngerprint reader case (Fig. 6). As such, one only needs to
3D print the open-source STL ﬁles and clip the LEDs and

8. We are currently investigating alternative case manufacturing

methods such as CNC milling.

Raspicams to their respective mounts (Fig. 6) in order to
quickly build their own RaspiReader replica.

Fig. 5. Electronic CAD model of the RaspiReader case. The dimensions
here were provided to a 3D printer for fabricating the prototype.

Fig. 6. Inside view of the RaspiReader case. The camera and LED
mounts are positioned at the necessary angles and distance to the glass
prism, making the reproduction of RaspiReader as simple as 3D printing
the open-sourced STL ﬁles.

2.3 Image Acquisition Hardware and Software

The backbone of the RaspiReader is the popular Raspberry
Pi 3B single board computer, which enables easy interfacing
with GPIO pins (for controlling LEDs) and image acqui-
sition (with its standard camera and camera connection
port). Because the Raspberry Pi only has a single camera
connection port, a camera port multiplexer is used to enable
the use of multiple cameras on a single Pi [43]. Using the
Raspberry Pi GPIO pins, the code available in [43], and the
camera multiplexer, one can easily extend the Raspberry Pi
to use multiple cameras.

6

After assembling the camera port multiplexer to the Pi
(with two Raspicams), wiring 3 LEDs to the Raspberry Pi
GPIO pins, and attaching the Raspicams and LEDs to the 3D
printed casing mounts (Fig. 6), open source python libraries
[43] can be used to illuminate the glass prism and subse-
quently acquire two images (Fig. 2 (a)) from the ﬁngerprint
reader (one raw FTIR ﬁngerprint image and another raw
direct ﬁngerprint image).

2.4 Fingerprint Image Processing

In order for the RaspiReader to be used for spoof detec-
tion, it must also demonstrate the ability to output high
quality ﬁngerprint images suitable for ﬁngerprint matching.
As previously mentioned, the RaspiReader performs spoof
detection on non-processed, raw ﬁngerprint images. While
these raw images are shown to provide discriminatory infor-
mation for spoof detection, they need to be made compatible
with processed images output by other COTS ﬁngerprint
readers. Therefore, after spoof detection, the RaspiReader
performs image processing operations on the raw high
contrast, FTIR image frames in order to output high ﬁdelity
images compatible with COTS optical ﬁngerprint readers.

Let a raw (unprocessed) FTIR ﬁngerprint image from
the RaspiReader be denoted as F T IRraw. This raw image
F T IRraw is ﬁrst converted from the RGB color space to
grayscale (F T IRgray) (Fig. 9 (a)). Then, in order to fur-
ther contrast the ridges from the valleys of the ﬁngerprint,
histogram equalization is performed on F T IRgray (Fig. 9
(b)). Finally, F T IRgray is negated so that the ridges of the
ﬁngerprint image are dark, and the background of the image
is white (as are ﬁngerprint images acquired from COTS
readers) (Fig. 9 (c)).

Following the aforementioned image processing tech-
niques, the RaspiReader FTIR ﬁngerprint images are further
processed by performing a perspective transformation (to
frontalize the ﬁngerprint to the image plane) and scaling to
500 ppi (Figs. 9 (d), (f)).

A perspective transformation is performed using Equa-

tion 2,







x(cid:48)
y(cid:48)
1

 =





c
b
a
f
e
d
g h 1













x
y
1

1
λ

(2)

where x and y are the source coordinates, x(cid:48) and y(cid:48) are
the transformed coordinates, (a, b, c, d, e, f, g, h) is the set of
transformation parameters, and λ = gx + hy + 1 is a scale
parameter. In this work, we image a 2D printed checker-
board pattern to deﬁne source and destination coordinate
pairs such that the transformation parameters could be
estimated (Fig. 7). Once the perspective transformation has
been completed, the RaspiReader image is downsampled
(by averaging neighborhood pixels) to 500 ppi (Fig. 9 (f)).
Note that the native resolution of the RaspiReader images
was acquired using a 2D printed checkerboard calibration
pattern (Fig. 7 (b)) and ranges from approx. 1594 ppi to 2480
ppi in the x-axis (Fig. 8 (a)) and 2463 ppi to 3320 ppi in the
y-axis (Fig. 8 (b)). While the high resolution images captured
by the RaspiReader 5 Megapixel cameras far exceed the
resolution of COTS ﬁngerprint readers (providing added
minute textural details for distinguishing live ﬁngers from

spoof ﬁngers), we observed that the focus of the native
images captured by RaspiReader does deteriorate on the left
and right edges (Fig. 7 (b)). We are currently investigating
methods for properly focusing the lens on the entire FOV,
so that minute textural details are not lost on the edges of
the RaspiReader images.

7

(a)

(b)

Fig. 7. Acquiring Image Transformation Parameters. A 2D printed
checkerboard pattern (a) is imaged by the RaspiReader (b). Corre-
sponding points between the frontalized checkerboard pattern (a) and
the distorted checkerboard pattern (b) are deﬁned so that perspective
transformation parameters can be estimated to map (b) into (c). These
transformation parameters are subsequently used to frontalize ﬁnger-
print images acquired by RaspiReader for the purpose of ﬁngerprint
matching. The checkerboard imaged in (b) is also used to acquire the
native resolution of RaspiReader in order to scale matching images to
500 ppi in both the x and y axis as shown in (c).

Upon completion of this entire ﬁngerprint reader assem-
bly and image processing procedure, the RaspiReader is
fully functional and ready for use in both spoof detection
and subsequent ﬁngerprint matching.

3 LIVE AND SPOOF FINGERPRINT DATABASE
CONSTRUCTION

To test the utility of the RaspiReader for spoof detection
and its interoperability for ﬁngerprint matching, a database
of live and spoof ﬁngerprint impressions was collected for
performing experiments. This database is constructed as
follows.

Fig. 8. Native resolution (ppi) in (a) x-axis and (b) y-axis over the
raw FTIR RaspiReader image. As is normal, native resolution changes
across the image because the right side of the image is closer to the
camera than the left side.

Using 7 different materials (Fig. 3 (a)), 66 spoofs were
fabricated9. Then, for each of these spoofs, 10 impressions
were captured at varying orientations and pressure on both
the RaspiReader (Rpi) and a COTS 500 ppi optical FTIR

9. Our spoofs were shipped to us by Precise Biometrics [44], a
company specializing in evaluating spoof detection capability and that
also has close ties to the LivDet dataset authors. As such, our spoofs
are of high quality and are similar to the spoofs used in the LivDet
competition.

8

Fig. 9. Processing a RaspiReader raw FTIR ﬁngerprint image into a 500 ppi ﬁngerprint image compatible for matching with existing COTS ﬁngerprint
readers. (a) The RGB raw FTIR image is ﬁrst converted to grayscale. (b) Histogram equalization is performed on the grayscale FTIR image to
enhance the contrast between the ﬁngerprint ridges and valleys. (c) The ﬁngerprint is negated so that the ridges appear dark, and the valleys
appear white. (d), (f) Calibration (estimated using the checkerboard calibration pattern in (e)) is applied to frontalize the ﬁngerprint image to the
image plane and down sample (by averaging neighborhood pixels) to 500 ppi in both the x and y directions.

ﬁngerprint reader (COT SA). The summary of this data
collection is enumerated in Table 2.

To collect a sufﬁcient variety of live ﬁnger data, we
enlisted 15 human subjects with different skin colors (Fig.
3 (b)). Each of these subjects gave 5 ﬁnger impressions (at
different orientations and pressures) from all 10 of their
10. A summary
ﬁngers on both the RaspiReader and COT SA
of this data collection is enumerated in Table 3.

In addition to the images of live ﬁnger impressions and
spoof ﬁnger impressions we collected for conducting spoof
detection experiments, we also veriﬁed that for spoofs with
optical properties too far from that of live ﬁnger skin (Fig.
10), images would not be captured by the RaspiReader.
These “failure to capture” spoofs are therefore ﬁltered out as
attacks before any software based spoof detection methods
need to be performed.

Table 2: Summary of Spoof1 Fingerprints Collected

RPi Direct
Images

RPi FTIR
Images

Material

Ecoﬂex
Wood Glue
Monster Liquid
Latex
Liquid Latex
Body Paint
Gelatin
Silver Coated
Ecoﬂex
Crayola Model
Magic
Total

Number
of
Spoofs2
10
10

10

10

10

10

6

66

100
100

100

100

100

100

60

660

COT SA
FTIR
Images
100
100

100

100

100

100

60

660

100
100

100

100

100

100

60

660

1 The spoof materials used to fabricate these spoofs were in accordance
with the approved materials by the IARPA ODIN project [2].
2 The spoofs are all of unique ﬁngerprint patterns.

Table 3: Summary of Live Finger Data Collected

Number of
Subjects

Number of
Fingers

RPi Direct
Images

RPi FTIR
Images

15

150

750

750

COT SA
FTIR
Images
750

10. Acquiring a ﬁngerprint on RaspiReader involves the same user
interactions that a COTS optical reader does. A user simply places
their ﬁnger on a glass prism. Then, LEDs illuminate the ﬁnger surface
and images are captured from both cameras over a time period of 1.5
seconds (Fig. 1). The only difference in the acquisition process between
a COTS reader and RaspiReader is that RaspiReader acquires two
complementary images of the ﬁnger in contact with the glass platen
from two separately mounted cameras.

Fig. 10. Failure to Capture. Several spoofs are unable to be imaged by
the RaspiReader due to their dissimilarity in color. In particular, because
spoofs in (a) and (b) are black, all light rays will be absorbed preventing
light rays from reﬂecting back to the FTIR imaging sensor. In (c), the
dark blue color again prevents enough light from reﬂecting back to the
camera. (a) and (b) are both ecoﬂex spoofs coated with two different
conductive coatings. (c) is a blue crayola model magic spoof attack.

4 SPOOF DETECTION EXPERIMENTS AND RE-
SULTS

Given the database of live and spoof ﬁngerprint images
collected on both COT SA, and the prototype RaspiReader,
a number of spoof detection experiments are conducted
to demonstrate the superiority of the raw images from
the RaspiReader for training spoof detectors in comparison
to the grayscale images output by COTS optical readers.
In particular, we (i) take several successful spoof detec-
tion techniques from the literature, (ii) train and test the

spoof detectors on COT SA images, (iii) train and test the
spoof detectors on RaspiReader images, and (iv) compare
the results to show the signiﬁcant boost in performance
when RaspiReader images are used to train spoof detectors
rather than COT SA images. In addition, experiments are
conducted to demonstrate that ﬁngerprint images from the
RaspiReader are compatible for matching with ﬁngerprint
images acquired from COT SA.

4.1 Spoof Detection Methods

To thoroughly demonstrate the value RaspiReader images
provide in training spoof detectors, we select two different
spoof detection methods, namely, (i) textural features (LBP
[45]) in conjunction with a linear Support Vector Machine
(SVM) and (ii) a Convolutional Neural Network (CNN).
Textural features were chosen because of their popularity
and their demonstrated superior spoof detection perfor-
mance in comparison to other “hand-crafted features” such
as anatomical or physiological features in the literature [36].
CNNs were chosen as a second spoof detection method in
our experiments given that they are currently state-of-the-
art on the publicly available LivDet datasets. [30], [31], [32],
[45]. The details of the experiments performed with both of
these spoof detection methods are provided in the following
subsections.

4.1.1 LBP Features From COT SA Images
We begin our experiments using grayscale processed ﬁnger-
print images acquired from COT SA (Fig. 2 (c)). From these
images, we extract the very prevalent grayscale and rotation
invariant local binary patterns (LBP) [45]. LBP features are
extracted by constructing a histogram of bit string values de-
termined by thresholding pixels in the local neighborhoods
around each pixel in the image. Since image texture can be
observed at different spatial resolutions, parameters R and
P are speciﬁed in LBP construction to indicate the length
(in pixels) of the neighborhood radius used for selecting
pixels and also the number of neighbors to consider in
a local neighborhood. Previous studies have shown that
more than 90% of fundamental textures in an image can
belong to a small subset of binary patterns called “uniform”
textures (local binary patterns containing two or fewer 0/1
bit transitions) [45]. Therefore, in line with previous studies
using local binary patterns for ﬁngerprint spoof detection,
we also employ the use of uniform local binary patterns.

More formally, let LBP (P, R) be the uniform local bi-
nary pattern histogram constructed by binning the local
binary patterns for each pixel in an image according to
the well known LBP operation [45] with parameters P and
R. In our experiments, we extract LBP (8, 1), LBP (16, 2),
and LBP (24, 3) in order to capture textures at different
spatial resolutions. These histograms (each having P + 2
bins) are individually normalized and concatenated into a
single feature vector X of dimension 54.

For classiﬁcation of these features, we employ a binary
linear SVM. As known in the art, an initially “hard margin”
SVM can be “softened” by a parameter C to enable better
generalization of the classiﬁer to the testing dataset. In our
case, we use ﬁve-fold cross validation to select the value of
10−4
C (from the list of
) such that

... 104

(cid:2)10−5

105(cid:3)

9

the best performance is achieved in different folds. In our
experiments, the best classiﬁcation results were achieved
with C = 102.

4.1.2 CLBP Features From RaspiReader Images
In this experiment, we make use of the information rich
images from the RaspiReader (Figs. 2 (a, b)) for spoof detec-
tion. As with Experiment 1, we again pursue the use of LBP
textural features. However, since the raw images from the
RaspiReader contain color information, rather than using
the traditional grayscale LBP features, we employ the use
of color local binary patterns (CLBP). Previous works have
shown the efﬁcacy of CLBP for both face recognition and
face spoof detection [46], [47]. However, because ﬁngerprint
images from COTS ﬁngerprint readers are grayscale, CLBP
features have, to our knowledge, not been investigated for
use in ﬁngerprint spoof detection until now.

Unlike traditional grayscale LBP patterns, color local
binary patterns (CLBP) encode discriminative spatiochro-
matic textures from across multiple spectral channels [46]. In
other words, CLBP extracts textures across all the different
image bands in a given input image. More formally, given
an input image I with K spectral channels, let the set of
all spectral channels for I be deﬁned as S = {S1, ..., SK}.
Then, the CLBP feature vector X of dimension 486 can be
extracted from I using Algorithm 1. Note that in Algorithm
1, LBP (Si, Sj, P, R) returns a normalized histogram of
local binary patterns using Si as the image channel that
the center (thresholding) pixels are selected from, and Sj
as the image channel from which the neighborhood pixels
are selected from in the same computation of LBP as per-
formed in Experiment 1. Also note that in Algorithm 1, (cid:107)
indicates vector concatenation. Finally, in our experiments,
we preprocess the RaspiReader input image I prior to CLBP
extraction by (i) downsampling (FTIR images from 1450 x
1944 to 108 x 145 and direct view images from 1290 x 1944
to 96 x 145), and (ii) converting to the HSV color space (Fig.
11)11.

Algorithm 1 Extraction of Color Local Binary Patterns

X ← [ ]
for i ← 1, K do

for j ← 1, K do

end for

end for
return X

X ← X(cid:107)LBP (Si, Sj, 8, 1)(cid:107)

LBP (Si, Sj, 16, 2)(cid:107)LBP (Si, Sj, 24, 3)

As in Experiment 1, a binary linear SVM with a pa-
rameter of C = 102 is trained with these features and
subsequently used for classiﬁcation. We again choose the
parameter C using 5-fold cross validation and a selec-
... 104
. Since RaspiReader
tion list of
outputs two color images (one raw FTIR image and one
direct view image), we perform multiple experiments us-
ing the proposed CLBP features in conjunction with the

(cid:2)10−5

105(cid:3)

10−4

11. Other color spaces were experimented with, but HSV consistently
provided the highest performance. This is likely because HSV separates
the luminance and chrominance components in an image, allowing
extraction of features on more complementary image channels.

SVM. In particular, we (i) extract CLBP features from the
RaspiReader raw FTIR images to train/test a SVM, (ii)
extract CLBP features from RaspiReader direct view images
to train/test a SVM, and (iii) fuse CLBP features from both
image outputs to train and test a SVM. We also attempted
fusing CLBP features from the RaspiReader raw images
with grayscale LBP features from RaspiReader processed
FTIR images, but found no signiﬁcant performance gains
under this last fusion scheme.

Fig. 11. RGB to HSV conversion. (a) A live direct view image from
RaspiReader is converted to the HSV color space. (b) An ecoﬂex spoof
attack imaged by the direct view camera of RaspiReader is converted to
the HSV color space. Experimental results demonstrate a performance
boost when preprocessing the RaspiReader images by converting from
the RGB to HSV color space.

4.1.3 MobileNet

In addition to performing experiments involving “hand-
crafted” textural features, we also perform experiments
where the features are directly learned and classiﬁed by a
Convolutional Neural Network (CNN). In choosing a CNN
architecture, we carefully considered both the size and com-
putational overhead, since in future works, we will optimize
the architecture to directly run on the RaspiReader’s Rasp-
berry Pi Processor. The need for a “low over-head” architec-
ture prompted us to select MobileNet [48]. MobileNet has

10

been shown to perform very closely (within 1 % accuracy)
to popular CNN models (VGG and Inception v3) on the
ImageNet and Stanford Dogs datasets while being 32 times
smaller than VGG, 7 times smaller than Inception v3, 27
times less computationally expensive than VGG, and 8 times
less computationally expensive than Inception v3.

In our experiments, we employ the Tensorﬂow Slim
implementation of MobileNet12. MobileNet is comprised
of 28 convolutional layers, and in our case, a ﬁnal 2 class
softmax layer for classiﬁcation of live or spoof. In all of our
experiments involving MobileNet, the RMSProp optimizer
was used for training the network along with a batch size
of 32, and an adaptive (exponential decay) learning rate. To
increase the generalization ability of the networks, we em-
ploy various data augmentation methods such as brightness
adjustment, random cropping, and horizontal and vertical
reﬂections.

Using the aforementioned MobileNet architecture and
hyper-parameters, we train/test
the network with (i)
COT SA grayscale ﬁngerprint images, (ii) RaspiReader raw
FTIR images, (iii) RaspiReader direct view images, and
(iv) RaspiReader processed FTIR images. Additionally, we
perform experiments in which we fuse the score outputs of
MobileNet models trained on the different image outputs
from RaspiReader to take advantage of the complementary
information within the different RaspiReader image out-
puts. When training and testing MobileNet with COT SA
images or RaspiReader processed FTIR images, the three
input channels of the network are each fed with the same
down sampled (357 x 392 to 224 x 224) grayscale COT SA
image or (290 x 267 to 224 x 224) RaspiReader processed
FTIR image. When training the network with RaspiReader
raw images, we again down sample the images (1450 x
1944 to 224 x 224 for raw FTIR and 1290 x 1944 to 224
x 224 for direct image), however, in this case, each of the
three color channels are fed as input to the three input
channels of the network. More speciﬁcally, we ﬁrst convert
the RaspiReader image to HSV (given our earlier ﬁndings
of superior performance in this color space), and then feed
each channel H, S, and V into the network’s input channels.

4.2 Spoof Detection Results

Using the spoof detection schemas previously described, we
train and test classiﬁers under two main scenarios. In the
ﬁrst scenario, we train the classiﬁer on a subset of spoof
images from every type in the dataset (Table 2). During
testing, spoof images from the same spoof types seen during
training will be passed to the spoof detector for classiﬁca-
tion. We hereafter refer to this training and testing scenario
as a “known-material” scenario. In the second scenario,
we train the classiﬁer with images from all of the spoof
types in the dataset except one (i.e. the spoof impressions
from one type of spoof are withheld). Then, during test-
ing the impressions of the withheld spoof type are used
for testing. In the literature, this type of spoof detection
evaluation is referred to as a “cross-material” scenario. In
the following experimental results, we demonstrate that the
RaspiReader images signiﬁcantly boost the spoof detection

12. https://github.com/tensorﬂow/models/tree/master/research/

slim

performance in both the known-material evaluations and
the cross-material evaluations.

Table 5: MobileNet and Known Testing Materials

Method

TDR @ FDR = 1.0%
µ ± σ1

Detection Time (msecs)

4.2.1 Known-Material Scenarios

The ﬁrst known-material results are reported in accor-
dance with spoof detection methods 1 and 2. That is, we
extract textural features from both COT SA images and
RaspiReader images respectively, train and test linear SVMs,
and ﬁnally, compare the results (Table 4). In all of our
known-material scenario experiments, we report the aver-
age spoof detection performance and standard deviation
over 5-folds. That is, for spoof data, we select 80% of the
spoof impressions from each spoof material for training
(each fold) and use the remaining 20% for testing. For live
ﬁnger data, we select the ﬁnger impressions of 12 subjects
each fold (600 total images) for training, and use the live
ﬁnger impressions of the remaining 3 subjects for testing.

Table 4: Textural Features and Known Testing Materials

TDR @ FDR = 1.0%
µ ± σ1

Detection Time (msecs)

Method

COT SA
+ LBP
Rpi raw FTIR
+ CLBP
Rpi Direct
+ CLBP
Rpi Fusion
+ CLBP2

75.9% ± 30.8

91.5% ± 11.0

98.10% ± 1.9

97.7% ± 3.0

236

243

243

486

1 These results are reported over 5-folds.
2 Rpi Fusion + CLBP is a feature level fusion (concatenation) of
CLBP features extracted from both Rpi raw FTIR images and Rpi
Direct Images, respectively.

From the results of Table 4, one can observe that both
image outputs of the RaspiReader contain far more discrim-
inative information for spoof detection than the processed
grayscale images output by COT SA. In particular, spoof
detection performance is signiﬁcantly higher when extract-
ing textural (CLBP) features from the RaspiReader images,
than when extracting textural features (LBP) from COT SA
images. While in these ﬁrst results, the fusion of features
from both RaspiReader image outputs actually hurts the
classiﬁcation performance slightly (compared to extracting
features only from the direct view images), in subsequent
experiments, we will demonstrate that different feature
extraction and classiﬁcation techniques can better utilize
the multiple outputs of RaspiReader in a complementary
manner to instead boost the classiﬁcation performance.

The second known-material results are reported in ac-
cordance with spoof detection scheme 3. More speciﬁcally,
the results are reported (over 5-folds) when MobileNet is
trained and tested with (i) COT SA images, (ii) RaspiReader
processed FTIR images, (iii) RaspiReader raw FTIR images,
and (iv) RaspiReader direct images. In addition, we report
the results when fusing the score outputs of multiple Mo-
bileNet models trained on the different image outputs of
RaspiReader (Table 5).

The results of Table 5 show that both the raw image
outputs of RaspiReader and the processed image output
of RaspiReader contain more discriminative information
for spoof detection than the processed images output by
COT SA. The MobileNet models trained on RaspiReader
images always outperform the MobileNet model trained on

11

22

22

22

22

45

67

COT SA
+ MobileNet
Rpi processed FTIR
+ MobileNet
Rpi raw FTIR
+ MobileNet
Rpi Direct
+ MobileNet
Rpi Fusion 2
+ MobileNet2
Rpi Fusion 3
+ MobileNet3

91.9% ± 8.0

94.5% ± 3.7

95.1% ± 5.6

95.3% ± 3.5

98.4% ± 2.3

98.9% ± 1.5

1 These results are reported over 5-folds.
2 Rpi Fusion 2 + MobileNet is a score level fusion (averaging) of a
MobileNet model trained on Rpi raw FTIR images and a MobileNet model
trained on Rpi Direct Images.
3 Rpi Fusion 3 + MobileNet is a score level fusion (averaging) of separate
MobileNet models trained on Rpi raw FTIR images, Rpi Direct Images,
and on Rpi processed FTIR images.

COT SA grayscale images both in average spoof detection
performance and stability (signiﬁcantly lower s.d.). What
is further interesting about the results of Table 5 is that the
features extracted by MobileNet from each RaspiReader out-
put are quite complementary, demonstrated by the fact that
spoof detection performance is improved when fusing the
scores of MobileNet models trained on each RaspiReader
image output. So, while CLBP features outperform Mo-
bileNet on the RaspiReader direct images, the fused Mo-
bileNet classiﬁers outperform the fused CLBP classiﬁer.

4.2.2 Cross-Material Scenarios

The cross-material results use the same spoof detection
schemas as enumerated in the known-material results with a
primary difference being the training and testing data splits
provided to the various classiﬁers. In all the cross-material
scenarios, spoof impressions of six materials are partitioned
to the classiﬁer for training, and the spoof impressions of
one “unseen” material are kept aside for testing. In this
manner the generalization capability of the spoof detector
to novel spoof types is thoroughly assessed. For live ﬁnger
data, we randomly select the ﬁnger impressions of two
subjects (100 total images) for testing, and use the live ﬁnger
impressions of the remaining thirteen subjects for training.
Since there are seven different spoof materials in our train-
ing set (Table 2), we conduct seven different cross-material
experiments for each spoof detection schema (where one of
the seven spoof types is left aside for testing). The cross
material results when using textural features in conjunction
with SVMs is reported in Table 6. The cross-material results
when using MobileNet extracted features is reported in
Table 7. Note, we only report the best textural fusion and
CNN fusion methods in the cross-material results. The other
non-fusion based methods were experimented with, but did
not provide as high of performance in the cross-material
scenarios.

The key ﬁndings of the cross-material experiments as re-
vealed in Tables 6 and 7 are as follows. First, in both textural
based spoof detection methods and CNN based spoof detec-
tion methods, the raw images output by RaspiReader pro-
vide more discriminative information than COTS grayscale

Table 6: Textural Features and Cross-Material Testing1

Testing Material

Rpi Fusion + CLBP2

Crayola Model
Magic
Ecoﬂex
Silver Coated
Ecoﬂex
Gelatin
Liquid Latex
Body Paint
Monster Liquid
Latex
Wood Glue

COT SA
+ LBP

91.7%

66.0%

88.0%

62.0%

84.0%

68.0%

100.0%

98.3%

77.0%

100.0%

87.0%

100.0%

98.0%

81.0%

1 TDR @ FDR = 1.0% is reported
2 Rpi Fusion + CLBP is a feature level fusion (concatenta-
tion) of CLBP features extracted from both Rpi raw FTIR
images and Rpi Direct Images, respectively.

Table 7: MobileNet and Cross-Material Testing1

COT SA
+ MobileNet

Rpi
Fusion 2 +
MobileNet2

Rpi
Fusion 3 +
MobileNet3

Testing Material

Crayola Model
Magic
Ecoﬂex
Silver Coated
Ecoﬂex
Gelatin
Liquid Latex
Body Paint
Monster Liquid
Latex
Wood Glue

50.0%

100.0%

77.0%

88.0%

97.0%

86.0%

100.0%

8.0%

100.0%

100.0%

100.0%

100.0%

56.0%

100.0%

100.0%

100.0%

100.0%

100.0%

96.0%

96.0%

94.0%
1 TDR @ FDR = 1.0% is reported
2 Rpi Fusion 2 + MobileNet is a score level fusion (max) of
separate MobileNet models trained on Rpi raw FTIR images and
on Rpi Direct Images, respectively.
3 Rpi Fusion 3 + MobileNet is a score level fusion (max) of
separate MobileNet models trained on Rpi raw FTIR images, Rpi
Direct Images, and on Rpi processed FTIR images.

ﬁngerprint images. This enables much higher spoof detec-
tion performance on spoofs fabricated from materials not
seen by the classiﬁer during training, a major ﬂaw in many
existing spoof detection methods relying on only COTS
grayscale images.

The one case of poor cross-material performance (when
using RaspiReader images) came when the testing material
withheld was ecoﬂex (Table 7). This can be explained by
ecoﬂex being a very transparent spoof, enabling much of the
live ﬁnger color behind the spoof to seep through. As such,
when the MobileNet models were trained on the other non-
transparent spoofs and tested on the transparent ecoﬂex, the
performance dropped considerably. However, we also no-
ticed that the best cross-material performance (when using
COT SA images) came when the testing material withheld
was ecoﬂex. The most plausible explanation for this is that
the MobileNet model trained on the COT SA images must
focus on textural features rather than color. As such, the
transparent property of ecoﬂex did not affect the classiﬁer
trained on the grayscale images. This prompted us to train a
third model on the RaspiReader processed FTIR images (i.e.
the raw FTIR images were converted to grayscale and con-
trast enhanced). We then fused the score of this third model
with the two MobileNet models trained on the RaspiReader
raw FTIR and direct images respectively. The ﬁnal product

12

was a three CNN model system which performed much
better on the ecoﬂex testing scenario (48% improvement).
While the ecoﬂex testing scenario is still low, in a real world
setting, this limitation is easily solved by including one
transparent spoof in the training set (evidenced by the fact
that in the known-material experiments, ecoﬂex could be
differentiated from live ﬁngers with high accuracy).

5 INTEROPERABILITY OF RASPIREADER
In addition to demonstrating the usefulness of
the
RaspiReader images for ﬁngerprint spoof detection, we also
demonstrate that by processing the RaspiReader FTIR im-
ages, we can output images which are compatible for match-
ing with images from COTS ﬁngerprint readers. Previously,
we discussed how to process and transform a RaspiReader
raw FTIR image into an image suitable for matching. In
this experiment, we evaluate the matching performance (of
11,175 imposter pairs and 6,750 genuine pairs) when using
(i) the RaspiReader processed images as both the enrollment
and probe images, (ii) the COT SA images as both the
enrollment and probe images, and (iii) the COT SA images
as the enrollment images and the RaspiReader processed
images as the probe images. The results for these matching
experiments are listed in Table 8.

Enrollment Reader
COT SA
RaspiReader
COT SA

Table 8: Fingerprint Matching Results1
Probe Reader
COT SA
RaspiReader
RaspiReader
1 We use the Innovatrics ﬁngerprint SDK which is shown to
have high accuracy in the NIST FpVTE evaluation [40].

98.62%
99.21%
95.56%

TAR @ FAR = 0.1%

From these results, we make two observations. First, the
best performance is achieved for native comparisons, where
the enrolled and search (probe) images are produced by
the same capture device. RaspiReader’s native performance
is slightly better than that of COT SA. This indicates that
the RaspiReader is capable of outputting images which
are compatible with state of the art ﬁngerprint matchers.
Second, we note that the performance does drop slightly
when conducting the interoperability experiment (COT SA
is used for enrollment images and RaspiReader is used for
probe images). However, the matching performance is still
quite high considering the stringent operating point (FAR =
0.1%). Furthermore, studies have shown that when different
ﬁngerprint readers are used for enrollment and subsequent
veriﬁcation or identiﬁcation, the matching performance in-
deed drops [49], [50], [51]. Finally, we are currently investi-
gating other approaches for processing and downsampling
RaspiReader images to reduce some of the drop in cross-
reader performance.

6 COMPUTATIONAL RESOURCES
All image preprocessing, LBP and CLBP feature extractions,
and SVM classiﬁcations were performed with a single CPU
core on a Macbook Pro running a 2.9 GHz Intel Core i5 pro-
cessor. MobileNet training and classiﬁcation was performed
on a single Nvidia GTX Titan GPU. The total time from
image capture to spoof detection with our best MobileNet
model (RpiFusion3) is approximately 3.067 seconds (1.5

seconds for image capture, 1.5 seconds to transmit data to
GPU, and 67 milliseconds for classiﬁcation). In the future,
we will port all spoof detection and ﬁngerprint matching
onto the RaspiReader creating a completely portable and
secure “ﬁngerprint match on box”.

7 CONCLUSIONS
We have open sourced13, the design and assembly of a cus-
tom ﬁngerprint reader, called RaspiReader, with Raspberry
Pi and other ubiquitous components. This ﬁngerprint reader
is both low cost (US $175) and easy to assemble, enabling
other researchers to easily and seamlessly develop their
own novel ﬁngerprint spoof detection solutions which use
both hardware and software. By customizing RaspiReader
with two cameras for ﬁngerprint image acquisition rather
than the customary one, we were able to extract discrim-
inative information from both raw images which, when
fused together, enabled us to achieve higher spoof detection
performance (in both known-material and cross-material
testing scenarios) compared to when features were extracted
from COTS grayscale images. Finally, by processing the raw
FTIR images of the RaspiReader, we were able to output
ﬁngerprint images compatible for matching with COTS op-
tical ﬁngerprint readers demonstrating the interoperability
of RaspiReader.

In our ongoing work, we plan to integrate special-
ized hardware into RaspiReader such as Optical Coher-
ence Tomography (OCT) for sub-dermal imagery, IR cam-
eras for vein detection, or microscopes for capturing ex-
tremely high resolution images of the ﬁngerprint. Because
the RaspiReader uses ubiquitous components running open
source software, RaspiReader enables future integration of
these additional hardware components. In addition to the
integration of specialized hardware, we also plan to use
the raw, information rich images from the RaspiReader
to pursue one-class classiﬁcation schemes for ﬁngerprint
spoof detection. In particular, we posit that the RaspiReader
images will assist us in modeling the class of live ﬁngerprint
images, such that spoofs of all material types can be easily
rejected. Finally, we will make RaspiReader a self contained
ﬁngerprint recognition system (similar to “match on card
[52]”), so that ﬁngerprint image acquisition, spoof detection,
feature extraction, and matching can all be accomplished
inside RaspiReader. This will provide an entire, portable,
secure “ﬁngerprint match in a box” on an approximately 4
inch cube.

ACKNOWLEDGMENT

This research was supported by grant no. 70NANB17H027
from the NIST Measurement Science program and by the
Ofﬁce of the Director of National Intelligence (ODNI), Intel-
ligence Advanced Research Projects Activity (IARPA), via
IARPA R&D Contract No. 2017 - 17020200004. The views
and conclusions contained herein are those of the authors
and should not be interpreted as necessarily representing
the ofﬁcial policies, either expressed or implied, of ODNI,
IARPA, or the U.S. Government. The U.S. Government is

13. https://github.com/engelsjo/RaspiReader

authorized to reproduce and distribute reprints for govern-
mental purposes notwithstanding any copyright annotation
therein.

13

REFERENCES

[1]

[2]

[5]

“International Standards Organization, ISO/IEC 30107-1:2016, In-
formation Technology Biometric Presentation Attack Detection
Part 1: Framework.” https://www.iso.org/standard/53227.html,,
2016.
“IARPA ODIN program.” https://www.iarpa.gov/index.php/
research-programs/odin/odin-baa.

[3] E. Marasco and A. Ross, “A survey on antispooﬁng schemes
for ﬁngerprint recognition systems,” ACM Comput. Surv., vol. 47,
pp. 28:1–28:36, Nov. 2014.

[4] D. Maltoni, D. Maio, A. K. Jain, and S. Prabhakar, Handbook of

Fingerprint Recognition. Springer, 2nd ed., 2009.
S. Yoon, J. Feng, and A. K. Jain, “Altered ﬁngerprints: Analysis
and detection,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 34, no. 3, pp. 451–464, 2012.

[7]

[6] T. Matsumoto, H. Matsumoto, K. Yamada, and S. Hoshino, “Im-
pact of artiﬁcial gummy ﬁngers on ﬁngerprint systems,” in Pro-
ceedings of SPIE, vol. 4677, pp. 275–289, 2002.
“Unique Identiﬁcation Authority of India, dashboard summary.”
https://portal.uidai.gov.in/uidwebportal/dashboard.do.
“UIDAI
http://www.stqc.gov.in/sites/upload ﬁles/stqc/ﬁles/
UIDAI-Biometric-Device-Speciﬁcations-Authentication-14-05-2012
0.pdf.
“UPI United Payments
documents/UPI Procedural Guidelines.pdf.

Interface.” http://www.npci.org.in/

(authentication).”

speciﬁcations

biometric

device

[9]

[8]

[10] “Ofﬁce of Biometric Identity Management.” https://www.dhs.

gov/obim.

Hong

Device

Kong
Spoofed.”

[11] “Report
rics
report-hong-kong-china-border-biometrics-device-spoofed.
[12] K. Cao and A. K. Jain, “Hacking mobile phones using 2d printed
ﬁngerprints,” tech. rep., MSU Technical report, MSU-CSE-16-2,
2016.

Biomet-
http://cw.com.hk/news/

Border

China

[13] K. Nixon, V. Aimale, and R. Rowe, “Spoof detection schemes,”

Handbook of Biometrics, pp. 403–423, 2008.

[14] “Goodix live ﬁnger detection.” https://ﬁndbiometrics.com/

goodix-zte-biometric-sensor-3103187/.

[15] P. D. Lapsley, J. A. Lee, D. F. Pare Jr, and N. Hoffman, “Anti-fraud
biometric scanner that accurately detects blood ﬂow,” Apr. 7 1998.
US Patent 5,737,439.

[16] D. Baldisserra, A. Franco, D. Maio, and D. Maltoni, “Fake ﬁn-
gerprint detection by odor analysis,” in International Conference on
Biometrics, pp. 265–272, Springer, 2006.

[17] R. K. Rowe, “Multispectral imaging biometrics,” Dec. 2 2008. US

Patent 7,460,696.

[18] A. Shiratsuki, E. Sano, M. Shikai, T. Nakashima, T. Takashima,
M. Ohmi, and M. Haruna, “Novel optical ﬁngerprint sensor uti-
lizing optical characteristics of skin tissue under ﬁngerprints,” in
Biomedical Optics 2005, pp. 80–87, International Society for Optics
and Photonics, 2005.

[19] M. Sepasian, C. Mares, and W. Balachandran, “Liveness and
spooﬁng in ﬁngerprint identiﬁcation: Issues and challenges,” in
Proceedings of the 4th WSEAS International Conference on Computer
Engineering and Applications, CEA’10, pp. 150–158, 2009.

[20] T. Van der Putte and J. Keuning, “Biometrical ﬁngerprint recog-
nition: dont get your ﬁngers burned,” in Smart Card Research and
Advanced Applications, pp. 289–303, Springer, 2000.

[21] S. A. Schuckers, “Spooﬁng and anti-spooﬁng measures,” Informa-

tion Security technical report, vol. 7, no. 4, pp. 56–62, 2002.

[22] S. B. Nikam and S. Agarwal, “Local binary pattern and wavelet
based spoof ﬁngerprint detection,” Int. J. Biometrics, vol. 1, pp. 141–
159, Aug. 2008.

[23] L. Ghiani, G. L. Marcialis, and F. Roli, “Fingerprint liveness de-
tection by local phase quantization,” in Pattern Recognition (ICPR),
2012 21st International Conference on, pp. 537–540, IEEE, 2012.
[24] L. Ghiani, A. Hadid, G. L. Marcialis, and F. Roli, “Fingerprint
liveness detection using binarized statistical image features,” in
Biometrics: Theory, Applications and Systems (BTAS), 2013 IEEE Sixth
International Conference on, pp. 1–6, IEEE, 2013.

14

[47] Z. Boulkenafet, J. Komulainen, and A. Hadid, “Face spooﬁng
detection using colour texture analysis,” IEEE Trans. Information
Forensics and Security, vol. 11, no. 8, pp. 1818–1830, 2016.

[48] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efﬁcient
convolutional neural networks for mobile vision applications,”
arXiv preprint arXiv:1704.04861, 2017.

[49] A. Ross and A. Jain, “Biometric sensor interoperability: A case
study in ﬁngerprints,” in ECCV Workshop BioAW, pp. 134–145,
Springer, 2004.

[50] X. Jia, X. Yang, Y. Zang, N. Zhang, and J. Tian, “A cross-device
matching ﬁngerprint database from multi-type sensors,” in Pattern
Recognition (ICPR), 2012 21st International Conference on, pp. 3001–
3004, IEEE, 2012.

[51] J. J. Engelsma, S. S. Arora, A. K. Jain, and N. G. Paulter Jr,
“Universal 3d wearable ﬁngerprint targets: Advancing ﬁngerprint
reader evaluations,” arXiv preprint arXiv:1705.07972, 2017.

[52] “SecureID

match-on-card
https://www.secureidnews.com/news-item/
tech-101-match-on-card-biometrics/.

News:

biometrics.”

Joshua J. Engelsma graduated magna cum
laude with a B.S. degree in computer science
from Grand Valley State University, Allendale,
Michigan, in 2016. He is currently working to-
wards a PhD degree in the Department of
Computer Science and Engineering at Michigan
State University, East Lansing, Michigan. His
research interests include pattern recognition,
computer vision, and image processing with ap-
plications in biometrics.

Kai Cao received the Ph.D. degree from the
Key Laboratory of Complex Systems and Intelli-
gence Science, Institute of Automation, Chinese
Academy of Sciences, Beijing, China, in 2010.
He is currently a Post Doctoral Fellow in the De-
partment of Computer Science & Engineering,
Michigan State University. He was afﬁliated with
Xidian University as an Associate Professor. His
research interests include biometric recognition,
image processing and machine learning.

Anil K. Jain is a University distinguished pro-
fessor in the Department of Computer Science
and Engineering at Michigan State University.
He was the editor-in-chief of the IEEE Trans.
Pattern Analysis and Machine Intelligence and
a member of the United States Defense Science
Board. He has received Fulbright, Guggenheim,
Alexander von Humboldt, and IAPR King Sun
the National
Fu awards. He is a member of
Academy of Engineering and foreign fellow of
the Indian National Academy of Engineering.

[25] D. Gragnaniello, G. Poggi, C. Sansone, and L. Verdoliva, “Finger-
print liveness detection based on weber local image descriptor,”
in Biometric Measurements and Systems for Security and Medical
Applications (BIOMS), 2013 IEEE Workshop on, pp. 46–50, IEEE,
2013.

[26] D. Gragnaniello, G. Poggi, C. Sansone, and L. Verdoliva, “Local
contrast phase descriptor for ﬁngerprint liveness detection,” Pat-
tern Recognition, vol. 48, no. 4, pp. 1050–1058, 2015.

[27] L. Ghiani, P. Denti, and G. L. Marcialis, “Experimental results on
ﬁngerprint liveness detection,” in Proceedings of the 7th International
Conference on Articulated Motion and Deformable Objects, AMDO’12,
pp. 210–218, Springer-Verlag, 2012.

[28] A. Abhyankar and S. Schuckers, “Integrating a wavelet based
perspiration liveness check with ﬁngerprint recognition,” Pattern
Recogn., vol. 42, pp. 452–464, Mar. 2009.

[29] E. Marasco and C. Sansone, “Combining perspiration-and
morphology-based static features for ﬁngerprint liveness detec-
tion,” Pattern Recognition Letters, vol. 33, no. 9, pp. 1148–1156, 2012.
[30] D. Menotti, G. Chiachia, A. da Silva Pinto, W. R. Schwartz,
H. Pedrini, A. X. Falc˜ao, and A. Rocha, “Deep representations
for iris, face, and ﬁngerprint spooﬁng detection,” IEEE Trans.
Information Forensics and Security, vol. 10, no. 4, pp. 864–879, 2015.
[31] R. Nogueira, R. Lotufo, and R. Machado, “Fingerprint liveness
detection using convolutional neural networks,” IEEE Trans. Infor-
mation Forensics and Security, vol. 11, no. 6, pp. 1206–1213, 2016.

[32] T. Chugh, K. Cao, and A. Jain, “Fingerprint spoof detection using
minutiae-based local patches,” in 2017 IEEE International Joint
Conference on Biometrics (IJCB), IEEE, 2017.

[33] V. Mura, L. Ghiani, G. L. Marcialis, F. Roli, D. A. Yambay, and
S. A. C. Schuckers, “Livdet 2015 ﬁngerprint liveness detection
competition 2015.,” in BTAS, pp. 1–6, IEEE, 2015.

[34] E. Marasco and C. Sansone, “On the robustness of ﬁngerprint live-
ness detection algorithms against new materials used for spoof-
ing,” in Proc. Int. Conf. Bio-Inspired Syst. Signal Process., pp. 553–
558, 2011.

[35] B. Tan, A. Lewicke, D. Yambay, and S. Schuckers, “The effect of
environmental conditions and novel spooﬁng methods on ﬁn-
gerprint anti-spooﬁng algorithms,” IEEE Information Forensics and
Security, WIFS, 2010.

[36] D. Yambay, L. Ghiani, P. Denti, G. L. Marcialis, F. Roli, and
S. A. C. Schuckers, “Livdet 2011 - ﬁngerprint liveness detection
competition 2011.,” in Proc. International Conf. Biometrics, pp. 208–
215, IEEE, 2012.

[37] A. Rattani, W. J. Scheirer, and A. Ross, “Open set ﬁngerprint
spoof detection across novel fabrication materials,” IEEE Trans.
on Information Forensics and Security, vol. 10, no. 11, pp. 2447–2460,
2015.

[38] A. Rattani and A. Ross, “Automatic adaptation of ﬁngerprint live-
ness detector to new spoof materials,” in 2014 IEEE International
Joint Conference on Biometrics (IJCB), pp. 1–8, IEEE, 2014.

[39] Y. Ding and A. Ross, “An ensemble of one-class svms for ﬁn-
gerprint spoof detection across different fabrication materials,” in
IEEE International Workshop on Information Forensics and Security,
WIFS 2016, Abu Dhabi, December 4-7, 2016, pp. 1–6, 2016.

[40] C. I. Watson, G. Fiumara, E. Tabassi, S. Cheng, P. Flanagan,
and W. Salamon, “Fingerprint vendor technology evaluation,
nist interagency/internal report 8034: 2015,” available at https:
//dx.doi.org/10.6028/NIST.IR.8034.

[41] “ThorLabs.”

https://www.thorlabs.com/thorproduct.cfm?

partnumber=PS911.

[42] P. Cignoni, M. Corsini, and G. Ranzuglia, “Meshlab: an open-
source 3d mesh processing system,” ERCIM News, pp. 45–46, April
2008.

[43] “Multi camera adapter module for raspberry pi.” https://www.
arducam.com/multi-camera-adapter-module-raspberry-pi. Ac-
cessed: 2017-4-15.

[44] “Precise

Biometrics.”

https://precisebiometrics.

com/products/ﬁngerprint-spoof-liveness-detection/
sensor-vulnerability-analysis/.

[45] T. Ojala, M. Pietik¨ainen, and T. M¨aenp¨a¨a, “Multiresolution gray-
scale and rotation invariant texture classiﬁcation with local binary
patterns,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 24, pp. 971–
987, July 2002.

[46] J. Y. Choi, K. N. Plataniotis, and Y. M. Ro, “Using colour local
binary pattern features for face recognition,” in Proceedings of the
International Conference on Image Processing, ICIP 2010, September
26-29, Hong Kong, pp. 4541–4544, 2010.

RaspiReader: Open Source Fingerprint Reader

Joshua J. Engelsma, Kai Cao, and Anil K. Jain, Life Fellow, IEEE

1

Abstract—We open source an easy to assemble, spoof resistant, high resolution, optical ﬁngerprint reader, called RaspiReader, using
ubiquitous components. By using our open source STL ﬁles and software, RaspiReader can be built in under one hour for only US
$175. As such, RaspiReader provides the ﬁngerprint research community a seamless and simple method for quickly prototyping new
ideas involving ﬁngerprint reader hardware. In particular, we posit that this open source ﬁngerprint reader will facilitate the exploration
of novel ﬁngerprint spoof detection techniques involving both hardware and software. We demonstrate one such spoof detection
technique by specially customizing RaspiReader with two cameras for ﬁngerprint image acquisition. One camera provides high
contrast, frustrated total internal reﬂection (FTIR) ﬁngerprint images, and the other outputs direct images of the ﬁnger in contact with
the platen. Using both of these image streams, we extract complementary information which, when fused together and used for spoof
detection, results in marked performance improvement over previous methods relying only on grayscale FTIR images provided by
COTS optical readers. Finally, ﬁngerprint matching experiments between images acquired from the FTIR output of RaspiReader and
images acquired from a COTS reader verify the interoperability of the RaspiReader with existing COTS optical readers.

Index Terms—Raspberry Pi, Frustrated Total Internal Reﬂection (FTIR), Open Source Fingerprint Readers, Presentation Attack
Detection, Spoof Detection, Interoperability

(cid:70)

7
1
0
2
 
c
e
D
 
6
2
 
 
]

V
C
.
s
c
[
 
 
1
v
2
9
3
9
0
.
2
1
7
1
:
v
i
X
r
a

1 INTRODUCTION

O NE of the major challenges facing biometric technology

today is the growing threat of presentation attacks1 [2].
The most common type of presentation attack (referred to
as spooﬁng) occurs when a hacker intentionally assumes
the identity of unsuspecting individuals, called victims here,
through stealing their ﬁngerprints, fabricating spoofs with
the stolen ﬁngerprints, and maliciously attacking ﬁngerprint
recognition systems with the spoofs into identifying the
hacker as the victim2 [3], [4], [5], [6].

The need to prevent spoof attacks is becoming increas-
ingly urgent due to the monumental costs and loss of
user privacy associated with spoofed systems. Consider
for example India’s Aadhaar program which (i) provides
beneﬁts and services to an ever growing population of
over 1.2 billion residents through ﬁngerprint and/or iris
recognition [7], [8] and (ii) facilitates electronic ﬁnancial
transactions through the Uniﬁed Payments Interface (UPI)
[9]. Failure to detect spoof attacks in the Aadhaar system
could cause the disruption of a commerce system affecting
untold numbers of people. Also consider the United States
Ofﬁce of Biometric Identity Management (US OBIM) which
is responsible for supporting the Department of Homeland
Security (DHS) with biometric identiﬁcation services specif-
ically aimed at preventing people who pose security risks to
the United States from entering the country [10]. Failure to
detect spoofs on systems deployed by OBIM could result in

•

J. J. Engelsma, K. Cao and A. K. Jain are with the Department of Computer
Science and Engineering, Michigan State University, East Lansing, MI,
48824
E-mail: {engelsm7, kaicao, jain}@cse.msu.edu

1. In ISO standard IEC 30107-1:2016(E), presentation attacks are de-
ﬁned as the “presentation to the biometric data capture subsystem with the
goal of interfering with the operation of the biometric system” [1].

2. Presentation attacks can also occur when (i) two individuals are in
collusion or (ii) an individual obfuscates his or her own ﬁngerprints to
avoid recognition [3]. However, in this paper our speciﬁc aim is to stop
ﬁngerprint spooﬁng presentation attacks.

Fig. 1. Prototype of RaspiReader: two ﬁngerprint images (b, (i)) and
(b, (ii)) of the input ﬁnger (a) are captured. The raw direct image (b,
(i)) and the raw, high contrast FTIR image (b, (ii)) both contain useful
information for spoof detection. Following the use of (b, (ii)) for spoof
detection, image calibration and processing are performed on the raw
FTIR image to output a high quality, 500 ppi ﬁngerprint for matching (b,
(iii)). The dimensions of the RaspiReader shown in (a) are 100 mm x
100 mm x 105 mm (about the size of a 4 inch cube).

a deadly terrorist attack3. Finally, almost all of us are actively
carrying ﬁngerprint recognition systems embedded within
our personal smart devices. Failure to detect spoof attacks
on smartphones [12] could compromise emails, banking
information, social media content, personal photos and a
plethora of other conﬁdential information.

In an effort to mitigate the costs associated with spoof
attacks, a number of spoof detection techniques involving

3. In 2012, a journalist successfully demonstrated that the Hong

Kong-China border control system could be easily spoofed [11].

2

Fig. 2. Fingerprint images acquired using the RaspiReader. Images in (a) were collected from a live ﬁnger. Images in (b) were collected from a
spoof ﬁnger. Using features extracted from both raw image outputs ((i), direct) and ((ii), FTIR) of the RaspiReader, our spoof detectors are better
able to discriminate between live ﬁngers and spoof ﬁngers. The raw FTIR image output of the RaspiReader (ii) can be post processed (after spoof
detection) to output images suitable for ﬁngerprint matching. Images in (c) were acquired from the same live ﬁnger (a) and spoof ﬁnger (b) on a
commercial off-the-shelf (COTS) 500 ppi optical reader. The close similarity between the two images in (c) qualitatively illustrates why current spoof
detectors are limited by the low information content, processed ﬁngerprint images (c, (iii)) output by COTS readers.

both hardware and software have been proposed in the
literature. Special hardware embedded in ﬁngerprint read-
ers4 enables capture of features such as heartbeat, thermal
output, blood ﬂow, odor, and sub-dermal ﬁnger character-
istics useful for distinguishing a live ﬁnger from a spoof
[3], [13], [15], [16], [17], [18], [19], [20], [21]. Spoof detection
methods in software are based on extracting textural [22],
[23], [24], [25], [26], anatomical [27], and physiological [28],
[29] features from processed5 ﬁngerprint images which are
used in conjunction with a classiﬁer such as Support Vec-
tor Machines (SVM). Alternatively, a Convolutional Neural
Network (CNN) can be trained to distinguish a live ﬁnger
from a spoof [30], [31], [32].

While existing hardware and software spoof detection
schemes provide a reasonable starting point for solving the
spoof detection problem, current solutions have a plethora
of shortcomings. As noted in [19], [20], [21] most hardware
based approaches can be easily bypassed by developing
very thin spoofs (Fig. 3 (a)), since heartbeat, thermal output,
and blood ﬂow can still be read from the live human skin be-
hind the thin spoof. Additionally, some of the characteristics
(such as odor and heartbeat) acquired by the hardware vary
tremendously amongst different human subjects, making it
very difﬁcult to build an adequate model representative of
all live subjects [19], [20].

Current spoof detection software solutions have their
own limitations. Although the LivDet 2015 competition

4. Several ﬁngerprint vendors have developed hardware spoof de-
tection solutions by employing multispectral imaging, infrared imaging
(useful for sub-dermal ﬁnger analysis), and pulse capture to distinguish
live ﬁngers from spoof ﬁngers [13], [14].

5. Raw ﬁngerprint images are “processed” (such as RGB to grayscale
conversion, contrast enhancement, and scaling) by COTS readers to
boost matching performance. However, useful spoof detection informa-
tion (such as color and/or minute textural abberations) is lost during
this processing.

reported state-of-the-art spoof detection software to have an
average accuracy of 95.51% [33], the spoof detection perfor-
mance at desired operating points such as False Detect Rate
(FDR) of 0.1% was not reported, and very limited evaluation
was performed to determine the effects of testing spoof
detectors with spoofs fabricated from materials not seen
during training (cross-material evaluation). In the limited
cross material evaluation that was performed, the rate of
spoofs correctly classiﬁed as spoofs was shown to drop from
96.57% to 94.20% [33]. While this slight drop in accuracy
seems promising, without knowing the performance at ﬁeld
conditions, namely False Detect Rate (FDR)6 of 0.1% on a
larger collection of unknown materials, the reported levels
of total accuracy should be accepted with caution. Chugh
et al. [32] pushed state-of-the-art ﬁngerprint spoof detec-
tion performance on the LivDet 2015 dataset from 95.51%
average accuracy to 98.61% average accuracy using a CNN
trained on patches around minutiae points, but they also
demonstrated that performance at strict operating points
dropped signiﬁcantly in some experiments. For example,
Chugh et al. reported an average accuracy on the LivDet
2011 dataset of 97.41%, however, at a FDR of 1.0%, the
TDR was only 90.32%, indicating that current state-of-the-
art spoof detection systems leave room for improvement at
desired operating points. Finally, several other studies have
reported up to a three-fold increase in error when testing
spoof detectors on unknown material types [34], [35], [36].

Because of the less than desired performance of spoof
detection software to adapt to spoofs fabricated from unseen
materials, studies in [37], [38], and [39] developed open-
set recognition classiﬁers to better detect spoofs fabricated
with novel material types. However, while these classiﬁers

6. The required operating point for the ODIN program supporting

this research is FDR = 0.2%

3

Fig. 3. Example spoof ﬁngers and live ﬁngers in our database. (a) Spoof ﬁngers and (b) live ﬁngers used to acquire both spoof ﬁngerprint impressions
and live ﬁngerprint impressions for conducting the experiments reported in this paper. The spoofs in (a) and the live ﬁngers in (b) are not in 1-to-1
correspondence.

are able to generalize to spoofs made with new materials
better than closed-set recognition algorithms, their overall
accuracy (approx. 85% - 90%) still does not meet the desired
performance for ﬁeld deployments.

tection) (Fig. 2 (i)). Both images of the RaspiReader visually
differentiate between live ﬁngers and spoof ﬁngers much
more than the processed ﬁngerprint images output by COTS
ﬁngerprint readers (Fig. 2 (c)).

Given the limitations of state-of-the-art ﬁngerprint spoof
detection (both in hardware and software), it is evident that
much work remains to be done in developing robust and
generalizable spoof detection solutions. We posit that one of
the biggest limitations facing the most successful spoof de-
tection solutions to date (such as use of textural features [36]
and CNNs [30], [31], [32]), is the processed COTS ﬁngerprint
reader images used to train spoof detectors. In particular,
because COTS ﬁngerprint readers output ﬁngerprint images
which have undergone a number of image processing oper-
ations (in an effort to achieve high matching performance),
they are not optimal for ﬁngerprint spoof detection, since
valuable information such as color and textural aberrations
is lost during the image processing operations. By removing
color and minute textural details from the raw ﬁngerprint
images, spoof ﬁngerprint impressions and live ﬁngerprint
impressions (acquired on COTS optical readers) appear very
similar (Fig. 2 (c)), even when the physical live/spoof ﬁngers
used to collect the respective ﬁngerprint impressions appear
very different (Fig. 3).

This limitation inherent to many existing spoof detec-
tion solutions motivated us to develop a custom, optical
ﬁngerprint reader, called RaspiReader, with the capability
to output 2 raw images (from 2 different cameras) for spoof
detection. By mounting two cameras at appropriate angles
to a glass prism (Fig. 4), one camera is able to capture high
contrast FTIR ﬁngerprint images (useful for both ﬁngerprint
spoof detection and ﬁngerprint matching) (Fig. 2 (ii)), while
the other camera captures direct images of the ﬁnger skin
in contact with the platen (useful for ﬁngerprint spoof de-

RaspiReader’s two camera approach is similar to that
which was prescribed by Rowe et al. in [13], [17] where
both an FTIR image and a direct view image were acquired
using different wavelength LEDs, however, the commercial
products developed around the ideas in [13], [17] act as a
proprietary black box outputting only a single processed
composite image of a collection of raw image frames cap-
tured under various wavelengths. As such, ﬁngerprint re-
searchers cannot implement new spoof detection schemes
on the individual raw frames captured by the reader. Fur-
thermore, unlike the patented ideas in [13], RaspiReader is
built with ubiquitous components and open source software
packages, enabling ﬁngerprint researchers to very easily
prototype their own RaspiReader, further customize it with
new spoof detection hardware, and gain direct access to
the raw images captured by the reader. In short, the low
cost ($175 USD) and easy to implement (1 hour build time)
RaspiReader is a truly unique concept which we posit will
push the boundaries of state-of-the-art ﬁngerprint spoof
detection, by facilitating spoof detection schemes which use
both hardware and software.

Experiments demonstrate that by utilizing the two cam-
eras of RaspiReader, we are able to signiﬁcantly boost the
performance of state-of-the-art spoof detectors previously
trained on COTS grayscale images (both on known-material
and cross-material testing scenarios). In particular, because
both image outputs of the RaspiReader are raw and contain
useful color information, we can extract discriminative and
complementary information from each of the image outputs.
By fusing this complementary information (at a feature

4

This is particularly true of ﬁngerprint recognition systems.
As shown in the NIST FpVTE 2013 [40] results, the single
most important factor responsible for degrading ﬁngerprint
recognition performance is the ﬁngerprint image quality.
However, most ﬁngerprint researchers have no control over
the quality of the ﬁngerprint images being used to develop
ﬁngerprint recognition algorithms since they must rely on
blackbox COTS ﬁngerprint readers. RaspiReader changes
this by providing ﬁngerprint matching algorithm designers
an easy method for prototyping their own ﬁngerprint reader
and optimizing ﬁngerprint image quality and ﬁngerprint
matching algorithms jointly in an effort to further improve
ﬁngerprint recognition performance.

In summary, our work on RaspiReader removes the mys-
tery of designing and understanding the internals of a ﬁn-
gerprint reader. Using the open-source fabrication process
of this ﬁngerprint reader, any ﬁngerprint algorithm designer
can quickly and affordably construct his or her own reader
with the capabilities (spoof detection and matching image
quality) necessary to meet their application requirements.
More concisely, the contributions of this research are:

• An open source, easy to assemble, cost effective ﬁn-
gerprint reader, called RaspiReader, capable of pro-
ducing ﬁngerprint images useful for spoof detection
and that are of high quality and resolution (1,500 ppi
- 3,300 ppi native resolution) for ﬁngerprint match-
ing. The custom RaspiReader can be easily modiﬁed
to facilitate spoof detection and ﬁngerprint matching
studies.

• A customized ﬁngerprint reader with two cameras
for image acquisition rather than a single camera.
Use of two cameras enables robust ﬁngerprint spoof
detection, since we can extract features from two
complementary, information rich images instead of
processed grayscale images output by traditional
COTS optical ﬁngerprint readers.

• A signiﬁcant boost in spoof detection performance
(both known-material and seven cross-material test-
ing scenarios) using current state-of-the-art software
based spoof detection methods in conjunction with
RaspiReader images as opposed to COTS optical
grayscale images. Spoofs of seven materials were
used in both known-material and cross-material test-
ing scenarios.

• Demonstrated matching

interoperability

of
RaspiReader with a COTS optical ﬁngerprint reader.
Since RaspiReader is shown to be interoperable with
COTS readers, it could immediately be deployed
in the real world since interoperability makes the
device compatible with legacy ﬁngerprint databases.

2 RASPIREADER CONSTRUCTION AND CALIBRA-
TION

In this section, the construction of the RaspiReader us-
ing ubiquitous, off-the-shelf components (Table 1) is ex-
plained. In particular, the main steps involved in construct-
ing RaspiReader consist of (i) properly mounting cameras
(angle and position) with respect to a glass prism, (ii)
fabricating a plastic case to house the hardware components,

Fig. 4. Schematic illustrating RaspiReader functionality. Incoming white
light from three LEDs enters the prism. Camera 2 receives light rays
reﬂected from the ﬁngerprint ridges only (light rays are not reﬂected back
from the ﬁngerprint valleys due to total internal reﬂection (TIR)). This
image from Camera 2, with high contrast between ridges and valleys
can be used for both spoof detection and ﬁngerprint matching. Camera 1
receives light rays reﬂected from both the ridges and valleys. This image
from Camera 1 provides complementary information for spoof detection.

level or score level) the performance of spoof detectors is
signiﬁcantly higher than when features are extracted from
COTS grayscale images.

Finally, by calibrating and processing the FTIR im-
age output of the RaspiReader (post spoof detection), we
demonstrate that RaspiReader is not only interoperable with
existing COTS optical readers but is also capable of achiev-
ing state-of-the-art ﬁngerprint matching accuracy. Note that
interoperability with existing COTS readers is absolutely
vital in any new hardware based spoof detection solution
as it makes the spoof resistant device compatible (in terms
of matching) with legacy ﬁngerprint databases7. Further-
more, by making the RaspiReader compatible with existing
COTS readers, we further extend the utility of RaspiReader
beyond spoof detection. In particular, RaspiReader is not
only useful for providing direct access to multiple raw
images for spoof detection; it also provides researchers in
ﬁngerprint matching the easy ability to ﬁne tune (resolution
and processing) the images being output by the ﬁngerprint
reader. In any imaging system, the recognition performance
depends on the quality of the image output by the sensor.

7. Interoperability with existing COTS readers is a strict requirement

of the IARPA ODIN program supporting this research [2].

Table 1: Primary Components Used to Construct RaspiReader. Total Cost is $175.20 (as of December 12, 2017)

Component Image

Name and Description

Quantity

Cost (USD)1

Raspberry Pi 3B: A single board computer (SBC) with 1.2 GHz 64-bit

quad-core CPU, 1 GB RAM, MicroSDHC storage, and Broadcom

$38.27

VideoCore IV Graphic card

5

$13.49

$49.99

$0.10

$5.16

$54.50

1

2

1

3

3

1

Raspberry Pi Camera Module V1: A 5.0 megapixel, 30 frames per

second, ﬁxed focal length camera

Multi-Camera Adapter: Splits Raspberry Pi camera slot into two

slots, enabling connection of two cameras

LEDs: white light, 5 mm, 1 watt

Resistors: 1 kΩ

Right Angle Prism:2 25 mm leg, 35.4 mm hypotenuse

1 All items except the glass prism were purchased for the listed prices on Amazon.com
2 The glass prism was purchased from ThorLabs [41].

(iii) assembling the cameras and hardware within the plastic
case, and (iv) writing software to capture ﬁngerprint images
with the assembled hardware. Each of these steps is de-
scribed in more detail in the following subsections. Finally,
we provide the steps for calibrating and processing the raw
FTIR ﬁngerprint images of the RaspiReader for ﬁngerprint
matching.

2.1 Camera Placement

The most important step in constructing RaspiReader is
the placement (angle and position) of the two cameras
capturing ﬁngerprint images. In particular, to collect an
FTIR image of a ﬁngerprint, a camera needs to be mounted
at an angle greater than the critical angle, and to collect a
direct view image, a camera needs to be mounted an an
angle less than the critical angle (both with respect to the
platen). Here, the critical angle is deﬁned as the angle at
which total internal reﬂection occurs when light passes from
a medium with an index of refraction n1 to another medium
with index of refraction n2 (Eq. 1):

to the prism, the ﬁngerprint image resolution (pixels per
inch) is increased. However, if the cameras are too close
to the platen, only part of the ﬁngerprint image is within
the ﬁeld of view (FOV). In constructing RaspiReader, we
wanted to maximize the ﬁngerprint image resolution, while
still capturing the entire ﬁngerprint image within the FOV.
We experimentally determined that at a distance of 23 mm
from the prism, the cameras would capture the entire ﬁnger-
print area. At closer distances, part of the ﬁngerprint image
would start to be outside the FOV. As a ﬁnal step in camera
placement, the focal length of the Raspicams (cameras used
in RaspiReader) must be increased so that the camera will
focus on the nearby glass prism (the default focus-length
of the Raspicams is 1 meter; much greater than the 23 mm
distant prism). By default, the Raspicams have a ﬁxed-focal
length of 3.6 mm. However, by rotating the Raspicam lens
652.5◦ counterclockwise (for the FTIR imaging camera) and
405◦ counterclockwise (for the direct imaging camera), the
focal length can be slightly increased to bring the nearby
ﬁngerprint images into focus.

θc = arcsin(

n2
n1

)

2.2 Case Fabrication

(1)

In the case of ﬁngerprint sensing, the ﬁrst medium is
glass which has an index of refraction n1 = 1.5, and the
second medium is air which has an index of refraction of
n2 = 1.0 leading to a critical angle (θc) of 41.8◦. Therefore,
as shown in (Fig. 4), we mount the direct view camera
(camera1) at an angle of θ1 = 10◦ and we mount the FTIR
camera (camera2) at an angle of θ2 = 45◦.

With respect to the position of each camera lens to
the glass prism, there is a tradeoff between resolution and
ﬁngerprint area to consider. As the camera is moved closer

After determining the angle and position of both cameras,
an outer casing (Fig. 5) accommodating these positions
is electronically modeled using Meshlab [42] and subse-
quently 3D printed on a high resolution 3D printer (Strata-
sys Objet350 Connex)8. To make the fabrication process
easily reproducible, the camera mounts and light source
mounts are modeled in place on the front part of the
ﬁngerprint reader case (Fig. 6). As such, one only needs to
3D print the open-source STL ﬁles and clip the LEDs and

8. We are currently investigating alternative case manufacturing

methods such as CNC milling.

Raspicams to their respective mounts (Fig. 6) in order to
quickly build their own RaspiReader replica.

Fig. 5. Electronic CAD model of the RaspiReader case. The dimensions
here were provided to a 3D printer for fabricating the prototype.

Fig. 6. Inside view of the RaspiReader case. The camera and LED
mounts are positioned at the necessary angles and distance to the glass
prism, making the reproduction of RaspiReader as simple as 3D printing
the open-sourced STL ﬁles.

2.3 Image Acquisition Hardware and Software

The backbone of the RaspiReader is the popular Raspberry
Pi 3B single board computer, which enables easy interfacing
with GPIO pins (for controlling LEDs) and image acqui-
sition (with its standard camera and camera connection
port). Because the Raspberry Pi only has a single camera
connection port, a camera port multiplexer is used to enable
the use of multiple cameras on a single Pi [43]. Using the
Raspberry Pi GPIO pins, the code available in [43], and the
camera multiplexer, one can easily extend the Raspberry Pi
to use multiple cameras.

6

After assembling the camera port multiplexer to the Pi
(with two Raspicams), wiring 3 LEDs to the Raspberry Pi
GPIO pins, and attaching the Raspicams and LEDs to the 3D
printed casing mounts (Fig. 6), open source python libraries
[43] can be used to illuminate the glass prism and subse-
quently acquire two images (Fig. 2 (a)) from the ﬁngerprint
reader (one raw FTIR ﬁngerprint image and another raw
direct ﬁngerprint image).

2.4 Fingerprint Image Processing

In order for the RaspiReader to be used for spoof detec-
tion, it must also demonstrate the ability to output high
quality ﬁngerprint images suitable for ﬁngerprint matching.
As previously mentioned, the RaspiReader performs spoof
detection on non-processed, raw ﬁngerprint images. While
these raw images are shown to provide discriminatory infor-
mation for spoof detection, they need to be made compatible
with processed images output by other COTS ﬁngerprint
readers. Therefore, after spoof detection, the RaspiReader
performs image processing operations on the raw high
contrast, FTIR image frames in order to output high ﬁdelity
images compatible with COTS optical ﬁngerprint readers.

Let a raw (unprocessed) FTIR ﬁngerprint image from
the RaspiReader be denoted as F T IRraw. This raw image
F T IRraw is ﬁrst converted from the RGB color space to
grayscale (F T IRgray) (Fig. 9 (a)). Then, in order to fur-
ther contrast the ridges from the valleys of the ﬁngerprint,
histogram equalization is performed on F T IRgray (Fig. 9
(b)). Finally, F T IRgray is negated so that the ridges of the
ﬁngerprint image are dark, and the background of the image
is white (as are ﬁngerprint images acquired from COTS
readers) (Fig. 9 (c)).

Following the aforementioned image processing tech-
niques, the RaspiReader FTIR ﬁngerprint images are further
processed by performing a perspective transformation (to
frontalize the ﬁngerprint to the image plane) and scaling to
500 ppi (Figs. 9 (d), (f)).

A perspective transformation is performed using Equa-

tion 2,







x(cid:48)
y(cid:48)
1

 =





c
b
a
f
e
d
g h 1













x
y
1

1
λ

(2)

where x and y are the source coordinates, x(cid:48) and y(cid:48) are
the transformed coordinates, (a, b, c, d, e, f, g, h) is the set of
transformation parameters, and λ = gx + hy + 1 is a scale
parameter. In this work, we image a 2D printed checker-
board pattern to deﬁne source and destination coordinate
pairs such that the transformation parameters could be
estimated (Fig. 7). Once the perspective transformation has
been completed, the RaspiReader image is downsampled
(by averaging neighborhood pixels) to 500 ppi (Fig. 9 (f)).
Note that the native resolution of the RaspiReader images
was acquired using a 2D printed checkerboard calibration
pattern (Fig. 7 (b)) and ranges from approx. 1594 ppi to 2480
ppi in the x-axis (Fig. 8 (a)) and 2463 ppi to 3320 ppi in the
y-axis (Fig. 8 (b)). While the high resolution images captured
by the RaspiReader 5 Megapixel cameras far exceed the
resolution of COTS ﬁngerprint readers (providing added
minute textural details for distinguishing live ﬁngers from

spoof ﬁngers), we observed that the focus of the native
images captured by RaspiReader does deteriorate on the left
and right edges (Fig. 7 (b)). We are currently investigating
methods for properly focusing the lens on the entire FOV,
so that minute textural details are not lost on the edges of
the RaspiReader images.

7

(a)

(b)

Fig. 7. Acquiring Image Transformation Parameters. A 2D printed
checkerboard pattern (a) is imaged by the RaspiReader (b). Corre-
sponding points between the frontalized checkerboard pattern (a) and
the distorted checkerboard pattern (b) are deﬁned so that perspective
transformation parameters can be estimated to map (b) into (c). These
transformation parameters are subsequently used to frontalize ﬁnger-
print images acquired by RaspiReader for the purpose of ﬁngerprint
matching. The checkerboard imaged in (b) is also used to acquire the
native resolution of RaspiReader in order to scale matching images to
500 ppi in both the x and y axis as shown in (c).

Upon completion of this entire ﬁngerprint reader assem-
bly and image processing procedure, the RaspiReader is
fully functional and ready for use in both spoof detection
and subsequent ﬁngerprint matching.

3 LIVE AND SPOOF FINGERPRINT DATABASE
CONSTRUCTION

To test the utility of the RaspiReader for spoof detection
and its interoperability for ﬁngerprint matching, a database
of live and spoof ﬁngerprint impressions was collected for
performing experiments. This database is constructed as
follows.

Fig. 8. Native resolution (ppi) in (a) x-axis and (b) y-axis over the
raw FTIR RaspiReader image. As is normal, native resolution changes
across the image because the right side of the image is closer to the
camera than the left side.

Using 7 different materials (Fig. 3 (a)), 66 spoofs were
fabricated9. Then, for each of these spoofs, 10 impressions
were captured at varying orientations and pressure on both
the RaspiReader (Rpi) and a COTS 500 ppi optical FTIR

9. Our spoofs were shipped to us by Precise Biometrics [44], a
company specializing in evaluating spoof detection capability and that
also has close ties to the LivDet dataset authors. As such, our spoofs
are of high quality and are similar to the spoofs used in the LivDet
competition.

8

Fig. 9. Processing a RaspiReader raw FTIR ﬁngerprint image into a 500 ppi ﬁngerprint image compatible for matching with existing COTS ﬁngerprint
readers. (a) The RGB raw FTIR image is ﬁrst converted to grayscale. (b) Histogram equalization is performed on the grayscale FTIR image to
enhance the contrast between the ﬁngerprint ridges and valleys. (c) The ﬁngerprint is negated so that the ridges appear dark, and the valleys
appear white. (d), (f) Calibration (estimated using the checkerboard calibration pattern in (e)) is applied to frontalize the ﬁngerprint image to the
image plane and down sample (by averaging neighborhood pixels) to 500 ppi in both the x and y directions.

ﬁngerprint reader (COT SA). The summary of this data
collection is enumerated in Table 2.

To collect a sufﬁcient variety of live ﬁnger data, we
enlisted 15 human subjects with different skin colors (Fig.
3 (b)). Each of these subjects gave 5 ﬁnger impressions (at
different orientations and pressures) from all 10 of their
10. A summary
ﬁngers on both the RaspiReader and COT SA
of this data collection is enumerated in Table 3.

In addition to the images of live ﬁnger impressions and
spoof ﬁnger impressions we collected for conducting spoof
detection experiments, we also veriﬁed that for spoofs with
optical properties too far from that of live ﬁnger skin (Fig.
10), images would not be captured by the RaspiReader.
These “failure to capture” spoofs are therefore ﬁltered out as
attacks before any software based spoof detection methods
need to be performed.

Table 2: Summary of Spoof1 Fingerprints Collected

RPi Direct
Images

RPi FTIR
Images

Material

Ecoﬂex
Wood Glue
Monster Liquid
Latex
Liquid Latex
Body Paint
Gelatin
Silver Coated
Ecoﬂex
Crayola Model
Magic
Total

Number
of
Spoofs2
10
10

10

10

10

10

6

66

100
100

100

100

100

100

60

660

COT SA
FTIR
Images
100
100

100

100

100

100

60

660

100
100

100

100

100

100

60

660

1 The spoof materials used to fabricate these spoofs were in accordance
with the approved materials by the IARPA ODIN project [2].
2 The spoofs are all of unique ﬁngerprint patterns.

Table 3: Summary of Live Finger Data Collected

Number of
Subjects

Number of
Fingers

RPi Direct
Images

RPi FTIR
Images

15

150

750

750

COT SA
FTIR
Images
750

10. Acquiring a ﬁngerprint on RaspiReader involves the same user
interactions that a COTS optical reader does. A user simply places
their ﬁnger on a glass prism. Then, LEDs illuminate the ﬁnger surface
and images are captured from both cameras over a time period of 1.5
seconds (Fig. 1). The only difference in the acquisition process between
a COTS reader and RaspiReader is that RaspiReader acquires two
complementary images of the ﬁnger in contact with the glass platen
from two separately mounted cameras.

Fig. 10. Failure to Capture. Several spoofs are unable to be imaged by
the RaspiReader due to their dissimilarity in color. In particular, because
spoofs in (a) and (b) are black, all light rays will be absorbed preventing
light rays from reﬂecting back to the FTIR imaging sensor. In (c), the
dark blue color again prevents enough light from reﬂecting back to the
camera. (a) and (b) are both ecoﬂex spoofs coated with two different
conductive coatings. (c) is a blue crayola model magic spoof attack.

4 SPOOF DETECTION EXPERIMENTS AND RE-
SULTS

Given the database of live and spoof ﬁngerprint images
collected on both COT SA, and the prototype RaspiReader,
a number of spoof detection experiments are conducted
to demonstrate the superiority of the raw images from
the RaspiReader for training spoof detectors in comparison
to the grayscale images output by COTS optical readers.
In particular, we (i) take several successful spoof detec-
tion techniques from the literature, (ii) train and test the

spoof detectors on COT SA images, (iii) train and test the
spoof detectors on RaspiReader images, and (iv) compare
the results to show the signiﬁcant boost in performance
when RaspiReader images are used to train spoof detectors
rather than COT SA images. In addition, experiments are
conducted to demonstrate that ﬁngerprint images from the
RaspiReader are compatible for matching with ﬁngerprint
images acquired from COT SA.

4.1 Spoof Detection Methods

To thoroughly demonstrate the value RaspiReader images
provide in training spoof detectors, we select two different
spoof detection methods, namely, (i) textural features (LBP
[45]) in conjunction with a linear Support Vector Machine
(SVM) and (ii) a Convolutional Neural Network (CNN).
Textural features were chosen because of their popularity
and their demonstrated superior spoof detection perfor-
mance in comparison to other “hand-crafted features” such
as anatomical or physiological features in the literature [36].
CNNs were chosen as a second spoof detection method in
our experiments given that they are currently state-of-the-
art on the publicly available LivDet datasets. [30], [31], [32],
[45]. The details of the experiments performed with both of
these spoof detection methods are provided in the following
subsections.

4.1.1 LBP Features From COT SA Images
We begin our experiments using grayscale processed ﬁnger-
print images acquired from COT SA (Fig. 2 (c)). From these
images, we extract the very prevalent grayscale and rotation
invariant local binary patterns (LBP) [45]. LBP features are
extracted by constructing a histogram of bit string values de-
termined by thresholding pixels in the local neighborhoods
around each pixel in the image. Since image texture can be
observed at different spatial resolutions, parameters R and
P are speciﬁed in LBP construction to indicate the length
(in pixels) of the neighborhood radius used for selecting
pixels and also the number of neighbors to consider in
a local neighborhood. Previous studies have shown that
more than 90% of fundamental textures in an image can
belong to a small subset of binary patterns called “uniform”
textures (local binary patterns containing two or fewer 0/1
bit transitions) [45]. Therefore, in line with previous studies
using local binary patterns for ﬁngerprint spoof detection,
we also employ the use of uniform local binary patterns.

More formally, let LBP (P, R) be the uniform local bi-
nary pattern histogram constructed by binning the local
binary patterns for each pixel in an image according to
the well known LBP operation [45] with parameters P and
R. In our experiments, we extract LBP (8, 1), LBP (16, 2),
and LBP (24, 3) in order to capture textures at different
spatial resolutions. These histograms (each having P + 2
bins) are individually normalized and concatenated into a
single feature vector X of dimension 54.

For classiﬁcation of these features, we employ a binary
linear SVM. As known in the art, an initially “hard margin”
SVM can be “softened” by a parameter C to enable better
generalization of the classiﬁer to the testing dataset. In our
case, we use ﬁve-fold cross validation to select the value of
10−4
C (from the list of
) such that

... 104

(cid:2)10−5

105(cid:3)

9

the best performance is achieved in different folds. In our
experiments, the best classiﬁcation results were achieved
with C = 102.

4.1.2 CLBP Features From RaspiReader Images
In this experiment, we make use of the information rich
images from the RaspiReader (Figs. 2 (a, b)) for spoof detec-
tion. As with Experiment 1, we again pursue the use of LBP
textural features. However, since the raw images from the
RaspiReader contain color information, rather than using
the traditional grayscale LBP features, we employ the use
of color local binary patterns (CLBP). Previous works have
shown the efﬁcacy of CLBP for both face recognition and
face spoof detection [46], [47]. However, because ﬁngerprint
images from COTS ﬁngerprint readers are grayscale, CLBP
features have, to our knowledge, not been investigated for
use in ﬁngerprint spoof detection until now.

Unlike traditional grayscale LBP patterns, color local
binary patterns (CLBP) encode discriminative spatiochro-
matic textures from across multiple spectral channels [46]. In
other words, CLBP extracts textures across all the different
image bands in a given input image. More formally, given
an input image I with K spectral channels, let the set of
all spectral channels for I be deﬁned as S = {S1, ..., SK}.
Then, the CLBP feature vector X of dimension 486 can be
extracted from I using Algorithm 1. Note that in Algorithm
1, LBP (Si, Sj, P, R) returns a normalized histogram of
local binary patterns using Si as the image channel that
the center (thresholding) pixels are selected from, and Sj
as the image channel from which the neighborhood pixels
are selected from in the same computation of LBP as per-
formed in Experiment 1. Also note that in Algorithm 1, (cid:107)
indicates vector concatenation. Finally, in our experiments,
we preprocess the RaspiReader input image I prior to CLBP
extraction by (i) downsampling (FTIR images from 1450 x
1944 to 108 x 145 and direct view images from 1290 x 1944
to 96 x 145), and (ii) converting to the HSV color space (Fig.
11)11.

Algorithm 1 Extraction of Color Local Binary Patterns

X ← [ ]
for i ← 1, K do

for j ← 1, K do

end for

end for
return X

X ← X(cid:107)LBP (Si, Sj, 8, 1)(cid:107)

LBP (Si, Sj, 16, 2)(cid:107)LBP (Si, Sj, 24, 3)

As in Experiment 1, a binary linear SVM with a pa-
rameter of C = 102 is trained with these features and
subsequently used for classiﬁcation. We again choose the
parameter C using 5-fold cross validation and a selec-
... 104
. Since RaspiReader
tion list of
outputs two color images (one raw FTIR image and one
direct view image), we perform multiple experiments us-
ing the proposed CLBP features in conjunction with the

(cid:2)10−5

105(cid:3)

10−4

11. Other color spaces were experimented with, but HSV consistently
provided the highest performance. This is likely because HSV separates
the luminance and chrominance components in an image, allowing
extraction of features on more complementary image channels.

SVM. In particular, we (i) extract CLBP features from the
RaspiReader raw FTIR images to train/test a SVM, (ii)
extract CLBP features from RaspiReader direct view images
to train/test a SVM, and (iii) fuse CLBP features from both
image outputs to train and test a SVM. We also attempted
fusing CLBP features from the RaspiReader raw images
with grayscale LBP features from RaspiReader processed
FTIR images, but found no signiﬁcant performance gains
under this last fusion scheme.

Fig. 11. RGB to HSV conversion. (a) A live direct view image from
RaspiReader is converted to the HSV color space. (b) An ecoﬂex spoof
attack imaged by the direct view camera of RaspiReader is converted to
the HSV color space. Experimental results demonstrate a performance
boost when preprocessing the RaspiReader images by converting from
the RGB to HSV color space.

4.1.3 MobileNet

In addition to performing experiments involving “hand-
crafted” textural features, we also perform experiments
where the features are directly learned and classiﬁed by a
Convolutional Neural Network (CNN). In choosing a CNN
architecture, we carefully considered both the size and com-
putational overhead, since in future works, we will optimize
the architecture to directly run on the RaspiReader’s Rasp-
berry Pi Processor. The need for a “low over-head” architec-
ture prompted us to select MobileNet [48]. MobileNet has

10

been shown to perform very closely (within 1 % accuracy)
to popular CNN models (VGG and Inception v3) on the
ImageNet and Stanford Dogs datasets while being 32 times
smaller than VGG, 7 times smaller than Inception v3, 27
times less computationally expensive than VGG, and 8 times
less computationally expensive than Inception v3.

In our experiments, we employ the Tensorﬂow Slim
implementation of MobileNet12. MobileNet is comprised
of 28 convolutional layers, and in our case, a ﬁnal 2 class
softmax layer for classiﬁcation of live or spoof. In all of our
experiments involving MobileNet, the RMSProp optimizer
was used for training the network along with a batch size
of 32, and an adaptive (exponential decay) learning rate. To
increase the generalization ability of the networks, we em-
ploy various data augmentation methods such as brightness
adjustment, random cropping, and horizontal and vertical
reﬂections.

Using the aforementioned MobileNet architecture and
hyper-parameters, we train/test
the network with (i)
COT SA grayscale ﬁngerprint images, (ii) RaspiReader raw
FTIR images, (iii) RaspiReader direct view images, and
(iv) RaspiReader processed FTIR images. Additionally, we
perform experiments in which we fuse the score outputs of
MobileNet models trained on the different image outputs
from RaspiReader to take advantage of the complementary
information within the different RaspiReader image out-
puts. When training and testing MobileNet with COT SA
images or RaspiReader processed FTIR images, the three
input channels of the network are each fed with the same
down sampled (357 x 392 to 224 x 224) grayscale COT SA
image or (290 x 267 to 224 x 224) RaspiReader processed
FTIR image. When training the network with RaspiReader
raw images, we again down sample the images (1450 x
1944 to 224 x 224 for raw FTIR and 1290 x 1944 to 224
x 224 for direct image), however, in this case, each of the
three color channels are fed as input to the three input
channels of the network. More speciﬁcally, we ﬁrst convert
the RaspiReader image to HSV (given our earlier ﬁndings
of superior performance in this color space), and then feed
each channel H, S, and V into the network’s input channels.

4.2 Spoof Detection Results

Using the spoof detection schemas previously described, we
train and test classiﬁers under two main scenarios. In the
ﬁrst scenario, we train the classiﬁer on a subset of spoof
images from every type in the dataset (Table 2). During
testing, spoof images from the same spoof types seen during
training will be passed to the spoof detector for classiﬁca-
tion. We hereafter refer to this training and testing scenario
as a “known-material” scenario. In the second scenario,
we train the classiﬁer with images from all of the spoof
types in the dataset except one (i.e. the spoof impressions
from one type of spoof are withheld). Then, during test-
ing the impressions of the withheld spoof type are used
for testing. In the literature, this type of spoof detection
evaluation is referred to as a “cross-material” scenario. In
the following experimental results, we demonstrate that the
RaspiReader images signiﬁcantly boost the spoof detection

12. https://github.com/tensorﬂow/models/tree/master/research/

slim

performance in both the known-material evaluations and
the cross-material evaluations.

Table 5: MobileNet and Known Testing Materials

Method

TDR @ FDR = 1.0%
µ ± σ1

Detection Time (msecs)

4.2.1 Known-Material Scenarios

The ﬁrst known-material results are reported in accor-
dance with spoof detection methods 1 and 2. That is, we
extract textural features from both COT SA images and
RaspiReader images respectively, train and test linear SVMs,
and ﬁnally, compare the results (Table 4). In all of our
known-material scenario experiments, we report the aver-
age spoof detection performance and standard deviation
over 5-folds. That is, for spoof data, we select 80% of the
spoof impressions from each spoof material for training
(each fold) and use the remaining 20% for testing. For live
ﬁnger data, we select the ﬁnger impressions of 12 subjects
each fold (600 total images) for training, and use the live
ﬁnger impressions of the remaining 3 subjects for testing.

Table 4: Textural Features and Known Testing Materials

TDR @ FDR = 1.0%
µ ± σ1

Detection Time (msecs)

Method

COT SA
+ LBP
Rpi raw FTIR
+ CLBP
Rpi Direct
+ CLBP
Rpi Fusion
+ CLBP2

75.9% ± 30.8

91.5% ± 11.0

98.10% ± 1.9

97.7% ± 3.0

236

243

243

486

1 These results are reported over 5-folds.
2 Rpi Fusion + CLBP is a feature level fusion (concatenation) of
CLBP features extracted from both Rpi raw FTIR images and Rpi
Direct Images, respectively.

From the results of Table 4, one can observe that both
image outputs of the RaspiReader contain far more discrim-
inative information for spoof detection than the processed
grayscale images output by COT SA. In particular, spoof
detection performance is signiﬁcantly higher when extract-
ing textural (CLBP) features from the RaspiReader images,
than when extracting textural features (LBP) from COT SA
images. While in these ﬁrst results, the fusion of features
from both RaspiReader image outputs actually hurts the
classiﬁcation performance slightly (compared to extracting
features only from the direct view images), in subsequent
experiments, we will demonstrate that different feature
extraction and classiﬁcation techniques can better utilize
the multiple outputs of RaspiReader in a complementary
manner to instead boost the classiﬁcation performance.

The second known-material results are reported in ac-
cordance with spoof detection scheme 3. More speciﬁcally,
the results are reported (over 5-folds) when MobileNet is
trained and tested with (i) COT SA images, (ii) RaspiReader
processed FTIR images, (iii) RaspiReader raw FTIR images,
and (iv) RaspiReader direct images. In addition, we report
the results when fusing the score outputs of multiple Mo-
bileNet models trained on the different image outputs of
RaspiReader (Table 5).

The results of Table 5 show that both the raw image
outputs of RaspiReader and the processed image output
of RaspiReader contain more discriminative information
for spoof detection than the processed images output by
COT SA. The MobileNet models trained on RaspiReader
images always outperform the MobileNet model trained on

11

22

22

22

22

45

67

COT SA
+ MobileNet
Rpi processed FTIR
+ MobileNet
Rpi raw FTIR
+ MobileNet
Rpi Direct
+ MobileNet
Rpi Fusion 2
+ MobileNet2
Rpi Fusion 3
+ MobileNet3

91.9% ± 8.0

94.5% ± 3.7

95.1% ± 5.6

95.3% ± 3.5

98.4% ± 2.3

98.9% ± 1.5

1 These results are reported over 5-folds.
2 Rpi Fusion 2 + MobileNet is a score level fusion (averaging) of a
MobileNet model trained on Rpi raw FTIR images and a MobileNet model
trained on Rpi Direct Images.
3 Rpi Fusion 3 + MobileNet is a score level fusion (averaging) of separate
MobileNet models trained on Rpi raw FTIR images, Rpi Direct Images,
and on Rpi processed FTIR images.

COT SA grayscale images both in average spoof detection
performance and stability (signiﬁcantly lower s.d.). What
is further interesting about the results of Table 5 is that the
features extracted by MobileNet from each RaspiReader out-
put are quite complementary, demonstrated by the fact that
spoof detection performance is improved when fusing the
scores of MobileNet models trained on each RaspiReader
image output. So, while CLBP features outperform Mo-
bileNet on the RaspiReader direct images, the fused Mo-
bileNet classiﬁers outperform the fused CLBP classiﬁer.

4.2.2 Cross-Material Scenarios

The cross-material results use the same spoof detection
schemas as enumerated in the known-material results with a
primary difference being the training and testing data splits
provided to the various classiﬁers. In all the cross-material
scenarios, spoof impressions of six materials are partitioned
to the classiﬁer for training, and the spoof impressions of
one “unseen” material are kept aside for testing. In this
manner the generalization capability of the spoof detector
to novel spoof types is thoroughly assessed. For live ﬁnger
data, we randomly select the ﬁnger impressions of two
subjects (100 total images) for testing, and use the live ﬁnger
impressions of the remaining thirteen subjects for training.
Since there are seven different spoof materials in our train-
ing set (Table 2), we conduct seven different cross-material
experiments for each spoof detection schema (where one of
the seven spoof types is left aside for testing). The cross
material results when using textural features in conjunction
with SVMs is reported in Table 6. The cross-material results
when using MobileNet extracted features is reported in
Table 7. Note, we only report the best textural fusion and
CNN fusion methods in the cross-material results. The other
non-fusion based methods were experimented with, but did
not provide as high of performance in the cross-material
scenarios.

The key ﬁndings of the cross-material experiments as re-
vealed in Tables 6 and 7 are as follows. First, in both textural
based spoof detection methods and CNN based spoof detec-
tion methods, the raw images output by RaspiReader pro-
vide more discriminative information than COTS grayscale

Table 6: Textural Features and Cross-Material Testing1

Testing Material

Rpi Fusion + CLBP2

Crayola Model
Magic
Ecoﬂex
Silver Coated
Ecoﬂex
Gelatin
Liquid Latex
Body Paint
Monster Liquid
Latex
Wood Glue

COT SA
+ LBP

91.7%

66.0%

88.0%

62.0%

84.0%

68.0%

100.0%

98.3%

77.0%

100.0%

87.0%

100.0%

98.0%

81.0%

1 TDR @ FDR = 1.0% is reported
2 Rpi Fusion + CLBP is a feature level fusion (concatenta-
tion) of CLBP features extracted from both Rpi raw FTIR
images and Rpi Direct Images, respectively.

Table 7: MobileNet and Cross-Material Testing1

COT SA
+ MobileNet

Rpi
Fusion 2 +
MobileNet2

Rpi
Fusion 3 +
MobileNet3

Testing Material

Crayola Model
Magic
Ecoﬂex
Silver Coated
Ecoﬂex
Gelatin
Liquid Latex
Body Paint
Monster Liquid
Latex
Wood Glue

50.0%

100.0%

77.0%

88.0%

97.0%

86.0%

100.0%

8.0%

100.0%

100.0%

100.0%

100.0%

56.0%

100.0%

100.0%

100.0%

100.0%

100.0%

96.0%

96.0%

94.0%
1 TDR @ FDR = 1.0% is reported
2 Rpi Fusion 2 + MobileNet is a score level fusion (max) of
separate MobileNet models trained on Rpi raw FTIR images and
on Rpi Direct Images, respectively.
3 Rpi Fusion 3 + MobileNet is a score level fusion (max) of
separate MobileNet models trained on Rpi raw FTIR images, Rpi
Direct Images, and on Rpi processed FTIR images.

ﬁngerprint images. This enables much higher spoof detec-
tion performance on spoofs fabricated from materials not
seen by the classiﬁer during training, a major ﬂaw in many
existing spoof detection methods relying on only COTS
grayscale images.

The one case of poor cross-material performance (when
using RaspiReader images) came when the testing material
withheld was ecoﬂex (Table 7). This can be explained by
ecoﬂex being a very transparent spoof, enabling much of the
live ﬁnger color behind the spoof to seep through. As such,
when the MobileNet models were trained on the other non-
transparent spoofs and tested on the transparent ecoﬂex, the
performance dropped considerably. However, we also no-
ticed that the best cross-material performance (when using
COT SA images) came when the testing material withheld
was ecoﬂex. The most plausible explanation for this is that
the MobileNet model trained on the COT SA images must
focus on textural features rather than color. As such, the
transparent property of ecoﬂex did not affect the classiﬁer
trained on the grayscale images. This prompted us to train a
third model on the RaspiReader processed FTIR images (i.e.
the raw FTIR images were converted to grayscale and con-
trast enhanced). We then fused the score of this third model
with the two MobileNet models trained on the RaspiReader
raw FTIR and direct images respectively. The ﬁnal product

12

was a three CNN model system which performed much
better on the ecoﬂex testing scenario (48% improvement).
While the ecoﬂex testing scenario is still low, in a real world
setting, this limitation is easily solved by including one
transparent spoof in the training set (evidenced by the fact
that in the known-material experiments, ecoﬂex could be
differentiated from live ﬁngers with high accuracy).

5 INTEROPERABILITY OF RASPIREADER
In addition to demonstrating the usefulness of
the
RaspiReader images for ﬁngerprint spoof detection, we also
demonstrate that by processing the RaspiReader FTIR im-
ages, we can output images which are compatible for match-
ing with images from COTS ﬁngerprint readers. Previously,
we discussed how to process and transform a RaspiReader
raw FTIR image into an image suitable for matching. In
this experiment, we evaluate the matching performance (of
11,175 imposter pairs and 6,750 genuine pairs) when using
(i) the RaspiReader processed images as both the enrollment
and probe images, (ii) the COT SA images as both the
enrollment and probe images, and (iii) the COT SA images
as the enrollment images and the RaspiReader processed
images as the probe images. The results for these matching
experiments are listed in Table 8.

Enrollment Reader
COT SA
RaspiReader
COT SA

Table 8: Fingerprint Matching Results1
Probe Reader
COT SA
RaspiReader
RaspiReader
1 We use the Innovatrics ﬁngerprint SDK which is shown to
have high accuracy in the NIST FpVTE evaluation [40].

98.62%
99.21%
95.56%

TAR @ FAR = 0.1%

From these results, we make two observations. First, the
best performance is achieved for native comparisons, where
the enrolled and search (probe) images are produced by
the same capture device. RaspiReader’s native performance
is slightly better than that of COT SA. This indicates that
the RaspiReader is capable of outputting images which
are compatible with state of the art ﬁngerprint matchers.
Second, we note that the performance does drop slightly
when conducting the interoperability experiment (COT SA
is used for enrollment images and RaspiReader is used for
probe images). However, the matching performance is still
quite high considering the stringent operating point (FAR =
0.1%). Furthermore, studies have shown that when different
ﬁngerprint readers are used for enrollment and subsequent
veriﬁcation or identiﬁcation, the matching performance in-
deed drops [49], [50], [51]. Finally, we are currently investi-
gating other approaches for processing and downsampling
RaspiReader images to reduce some of the drop in cross-
reader performance.

6 COMPUTATIONAL RESOURCES
All image preprocessing, LBP and CLBP feature extractions,
and SVM classiﬁcations were performed with a single CPU
core on a Macbook Pro running a 2.9 GHz Intel Core i5 pro-
cessor. MobileNet training and classiﬁcation was performed
on a single Nvidia GTX Titan GPU. The total time from
image capture to spoof detection with our best MobileNet
model (RpiFusion3) is approximately 3.067 seconds (1.5

seconds for image capture, 1.5 seconds to transmit data to
GPU, and 67 milliseconds for classiﬁcation). In the future,
we will port all spoof detection and ﬁngerprint matching
onto the RaspiReader creating a completely portable and
secure “ﬁngerprint match on box”.

7 CONCLUSIONS
We have open sourced13, the design and assembly of a cus-
tom ﬁngerprint reader, called RaspiReader, with Raspberry
Pi and other ubiquitous components. This ﬁngerprint reader
is both low cost (US $175) and easy to assemble, enabling
other researchers to easily and seamlessly develop their
own novel ﬁngerprint spoof detection solutions which use
both hardware and software. By customizing RaspiReader
with two cameras for ﬁngerprint image acquisition rather
than the customary one, we were able to extract discrim-
inative information from both raw images which, when
fused together, enabled us to achieve higher spoof detection
performance (in both known-material and cross-material
testing scenarios) compared to when features were extracted
from COTS grayscale images. Finally, by processing the raw
FTIR images of the RaspiReader, we were able to output
ﬁngerprint images compatible for matching with COTS op-
tical ﬁngerprint readers demonstrating the interoperability
of RaspiReader.

In our ongoing work, we plan to integrate special-
ized hardware into RaspiReader such as Optical Coher-
ence Tomography (OCT) for sub-dermal imagery, IR cam-
eras for vein detection, or microscopes for capturing ex-
tremely high resolution images of the ﬁngerprint. Because
the RaspiReader uses ubiquitous components running open
source software, RaspiReader enables future integration of
these additional hardware components. In addition to the
integration of specialized hardware, we also plan to use
the raw, information rich images from the RaspiReader
to pursue one-class classiﬁcation schemes for ﬁngerprint
spoof detection. In particular, we posit that the RaspiReader
images will assist us in modeling the class of live ﬁngerprint
images, such that spoofs of all material types can be easily
rejected. Finally, we will make RaspiReader a self contained
ﬁngerprint recognition system (similar to “match on card
[52]”), so that ﬁngerprint image acquisition, spoof detection,
feature extraction, and matching can all be accomplished
inside RaspiReader. This will provide an entire, portable,
secure “ﬁngerprint match in a box” on an approximately 4
inch cube.

ACKNOWLEDGMENT

This research was supported by grant no. 70NANB17H027
from the NIST Measurement Science program and by the
Ofﬁce of the Director of National Intelligence (ODNI), Intel-
ligence Advanced Research Projects Activity (IARPA), via
IARPA R&D Contract No. 2017 - 17020200004. The views
and conclusions contained herein are those of the authors
and should not be interpreted as necessarily representing
the ofﬁcial policies, either expressed or implied, of ODNI,
IARPA, or the U.S. Government. The U.S. Government is

13. https://github.com/engelsjo/RaspiReader

authorized to reproduce and distribute reprints for govern-
mental purposes notwithstanding any copyright annotation
therein.

13

REFERENCES

[1]

[2]

[5]

“International Standards Organization, ISO/IEC 30107-1:2016, In-
formation Technology Biometric Presentation Attack Detection
Part 1: Framework.” https://www.iso.org/standard/53227.html,,
2016.
“IARPA ODIN program.” https://www.iarpa.gov/index.php/
research-programs/odin/odin-baa.

[3] E. Marasco and A. Ross, “A survey on antispooﬁng schemes
for ﬁngerprint recognition systems,” ACM Comput. Surv., vol. 47,
pp. 28:1–28:36, Nov. 2014.

[4] D. Maltoni, D. Maio, A. K. Jain, and S. Prabhakar, Handbook of

Fingerprint Recognition. Springer, 2nd ed., 2009.
S. Yoon, J. Feng, and A. K. Jain, “Altered ﬁngerprints: Analysis
and detection,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 34, no. 3, pp. 451–464, 2012.

[7]

[6] T. Matsumoto, H. Matsumoto, K. Yamada, and S. Hoshino, “Im-
pact of artiﬁcial gummy ﬁngers on ﬁngerprint systems,” in Pro-
ceedings of SPIE, vol. 4677, pp. 275–289, 2002.
“Unique Identiﬁcation Authority of India, dashboard summary.”
https://portal.uidai.gov.in/uidwebportal/dashboard.do.
“UIDAI
http://www.stqc.gov.in/sites/upload ﬁles/stqc/ﬁles/
UIDAI-Biometric-Device-Speciﬁcations-Authentication-14-05-2012
0.pdf.
“UPI United Payments
documents/UPI Procedural Guidelines.pdf.

Interface.” http://www.npci.org.in/

(authentication).”

speciﬁcations

biometric

device

[9]

[8]

[10] “Ofﬁce of Biometric Identity Management.” https://www.dhs.

gov/obim.

Hong

Device

Kong
Spoofed.”

[11] “Report
rics
report-hong-kong-china-border-biometrics-device-spoofed.
[12] K. Cao and A. K. Jain, “Hacking mobile phones using 2d printed
ﬁngerprints,” tech. rep., MSU Technical report, MSU-CSE-16-2,
2016.

Biomet-
http://cw.com.hk/news/

Border

China

[13] K. Nixon, V. Aimale, and R. Rowe, “Spoof detection schemes,”

Handbook of Biometrics, pp. 403–423, 2008.

[14] “Goodix live ﬁnger detection.” https://ﬁndbiometrics.com/

goodix-zte-biometric-sensor-3103187/.

[15] P. D. Lapsley, J. A. Lee, D. F. Pare Jr, and N. Hoffman, “Anti-fraud
biometric scanner that accurately detects blood ﬂow,” Apr. 7 1998.
US Patent 5,737,439.

[16] D. Baldisserra, A. Franco, D. Maio, and D. Maltoni, “Fake ﬁn-
gerprint detection by odor analysis,” in International Conference on
Biometrics, pp. 265–272, Springer, 2006.

[17] R. K. Rowe, “Multispectral imaging biometrics,” Dec. 2 2008. US

Patent 7,460,696.

[18] A. Shiratsuki, E. Sano, M. Shikai, T. Nakashima, T. Takashima,
M. Ohmi, and M. Haruna, “Novel optical ﬁngerprint sensor uti-
lizing optical characteristics of skin tissue under ﬁngerprints,” in
Biomedical Optics 2005, pp. 80–87, International Society for Optics
and Photonics, 2005.

[19] M. Sepasian, C. Mares, and W. Balachandran, “Liveness and
spooﬁng in ﬁngerprint identiﬁcation: Issues and challenges,” in
Proceedings of the 4th WSEAS International Conference on Computer
Engineering and Applications, CEA’10, pp. 150–158, 2009.

[20] T. Van der Putte and J. Keuning, “Biometrical ﬁngerprint recog-
nition: dont get your ﬁngers burned,” in Smart Card Research and
Advanced Applications, pp. 289–303, Springer, 2000.

[21] S. A. Schuckers, “Spooﬁng and anti-spooﬁng measures,” Informa-

tion Security technical report, vol. 7, no. 4, pp. 56–62, 2002.

[22] S. B. Nikam and S. Agarwal, “Local binary pattern and wavelet
based spoof ﬁngerprint detection,” Int. J. Biometrics, vol. 1, pp. 141–
159, Aug. 2008.

[23] L. Ghiani, G. L. Marcialis, and F. Roli, “Fingerprint liveness de-
tection by local phase quantization,” in Pattern Recognition (ICPR),
2012 21st International Conference on, pp. 537–540, IEEE, 2012.
[24] L. Ghiani, A. Hadid, G. L. Marcialis, and F. Roli, “Fingerprint
liveness detection using binarized statistical image features,” in
Biometrics: Theory, Applications and Systems (BTAS), 2013 IEEE Sixth
International Conference on, pp. 1–6, IEEE, 2013.

14

[47] Z. Boulkenafet, J. Komulainen, and A. Hadid, “Face spooﬁng
detection using colour texture analysis,” IEEE Trans. Information
Forensics and Security, vol. 11, no. 8, pp. 1818–1830, 2016.

[48] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efﬁcient
convolutional neural networks for mobile vision applications,”
arXiv preprint arXiv:1704.04861, 2017.

[49] A. Ross and A. Jain, “Biometric sensor interoperability: A case
study in ﬁngerprints,” in ECCV Workshop BioAW, pp. 134–145,
Springer, 2004.

[50] X. Jia, X. Yang, Y. Zang, N. Zhang, and J. Tian, “A cross-device
matching ﬁngerprint database from multi-type sensors,” in Pattern
Recognition (ICPR), 2012 21st International Conference on, pp. 3001–
3004, IEEE, 2012.

[51] J. J. Engelsma, S. S. Arora, A. K. Jain, and N. G. Paulter Jr,
“Universal 3d wearable ﬁngerprint targets: Advancing ﬁngerprint
reader evaluations,” arXiv preprint arXiv:1705.07972, 2017.

[52] “SecureID

match-on-card
https://www.secureidnews.com/news-item/
tech-101-match-on-card-biometrics/.

News:

biometrics.”

Joshua J. Engelsma graduated magna cum
laude with a B.S. degree in computer science
from Grand Valley State University, Allendale,
Michigan, in 2016. He is currently working to-
wards a PhD degree in the Department of
Computer Science and Engineering at Michigan
State University, East Lansing, Michigan. His
research interests include pattern recognition,
computer vision, and image processing with ap-
plications in biometrics.

Kai Cao received the Ph.D. degree from the
Key Laboratory of Complex Systems and Intelli-
gence Science, Institute of Automation, Chinese
Academy of Sciences, Beijing, China, in 2010.
He is currently a Post Doctoral Fellow in the De-
partment of Computer Science & Engineering,
Michigan State University. He was afﬁliated with
Xidian University as an Associate Professor. His
research interests include biometric recognition,
image processing and machine learning.

Anil K. Jain is a University distinguished pro-
fessor in the Department of Computer Science
and Engineering at Michigan State University.
He was the editor-in-chief of the IEEE Trans.
Pattern Analysis and Machine Intelligence and
a member of the United States Defense Science
Board. He has received Fulbright, Guggenheim,
Alexander von Humboldt, and IAPR King Sun
the National
Fu awards. He is a member of
Academy of Engineering and foreign fellow of
the Indian National Academy of Engineering.

[25] D. Gragnaniello, G. Poggi, C. Sansone, and L. Verdoliva, “Finger-
print liveness detection based on weber local image descriptor,”
in Biometric Measurements and Systems for Security and Medical
Applications (BIOMS), 2013 IEEE Workshop on, pp. 46–50, IEEE,
2013.

[26] D. Gragnaniello, G. Poggi, C. Sansone, and L. Verdoliva, “Local
contrast phase descriptor for ﬁngerprint liveness detection,” Pat-
tern Recognition, vol. 48, no. 4, pp. 1050–1058, 2015.

[27] L. Ghiani, P. Denti, and G. L. Marcialis, “Experimental results on
ﬁngerprint liveness detection,” in Proceedings of the 7th International
Conference on Articulated Motion and Deformable Objects, AMDO’12,
pp. 210–218, Springer-Verlag, 2012.

[28] A. Abhyankar and S. Schuckers, “Integrating a wavelet based
perspiration liveness check with ﬁngerprint recognition,” Pattern
Recogn., vol. 42, pp. 452–464, Mar. 2009.

[29] E. Marasco and C. Sansone, “Combining perspiration-and
morphology-based static features for ﬁngerprint liveness detec-
tion,” Pattern Recognition Letters, vol. 33, no. 9, pp. 1148–1156, 2012.
[30] D. Menotti, G. Chiachia, A. da Silva Pinto, W. R. Schwartz,
H. Pedrini, A. X. Falc˜ao, and A. Rocha, “Deep representations
for iris, face, and ﬁngerprint spooﬁng detection,” IEEE Trans.
Information Forensics and Security, vol. 10, no. 4, pp. 864–879, 2015.
[31] R. Nogueira, R. Lotufo, and R. Machado, “Fingerprint liveness
detection using convolutional neural networks,” IEEE Trans. Infor-
mation Forensics and Security, vol. 11, no. 6, pp. 1206–1213, 2016.

[32] T. Chugh, K. Cao, and A. Jain, “Fingerprint spoof detection using
minutiae-based local patches,” in 2017 IEEE International Joint
Conference on Biometrics (IJCB), IEEE, 2017.

[33] V. Mura, L. Ghiani, G. L. Marcialis, F. Roli, D. A. Yambay, and
S. A. C. Schuckers, “Livdet 2015 ﬁngerprint liveness detection
competition 2015.,” in BTAS, pp. 1–6, IEEE, 2015.

[34] E. Marasco and C. Sansone, “On the robustness of ﬁngerprint live-
ness detection algorithms against new materials used for spoof-
ing,” in Proc. Int. Conf. Bio-Inspired Syst. Signal Process., pp. 553–
558, 2011.

[35] B. Tan, A. Lewicke, D. Yambay, and S. Schuckers, “The effect of
environmental conditions and novel spooﬁng methods on ﬁn-
gerprint anti-spooﬁng algorithms,” IEEE Information Forensics and
Security, WIFS, 2010.

[36] D. Yambay, L. Ghiani, P. Denti, G. L. Marcialis, F. Roli, and
S. A. C. Schuckers, “Livdet 2011 - ﬁngerprint liveness detection
competition 2011.,” in Proc. International Conf. Biometrics, pp. 208–
215, IEEE, 2012.

[37] A. Rattani, W. J. Scheirer, and A. Ross, “Open set ﬁngerprint
spoof detection across novel fabrication materials,” IEEE Trans.
on Information Forensics and Security, vol. 10, no. 11, pp. 2447–2460,
2015.

[38] A. Rattani and A. Ross, “Automatic adaptation of ﬁngerprint live-
ness detector to new spoof materials,” in 2014 IEEE International
Joint Conference on Biometrics (IJCB), pp. 1–8, IEEE, 2014.

[39] Y. Ding and A. Ross, “An ensemble of one-class svms for ﬁn-
gerprint spoof detection across different fabrication materials,” in
IEEE International Workshop on Information Forensics and Security,
WIFS 2016, Abu Dhabi, December 4-7, 2016, pp. 1–6, 2016.

[40] C. I. Watson, G. Fiumara, E. Tabassi, S. Cheng, P. Flanagan,
and W. Salamon, “Fingerprint vendor technology evaluation,
nist interagency/internal report 8034: 2015,” available at https:
//dx.doi.org/10.6028/NIST.IR.8034.

[41] “ThorLabs.”

https://www.thorlabs.com/thorproduct.cfm?

partnumber=PS911.

[42] P. Cignoni, M. Corsini, and G. Ranzuglia, “Meshlab: an open-
source 3d mesh processing system,” ERCIM News, pp. 45–46, April
2008.

[43] “Multi camera adapter module for raspberry pi.” https://www.
arducam.com/multi-camera-adapter-module-raspberry-pi. Ac-
cessed: 2017-4-15.

[44] “Precise

Biometrics.”

https://precisebiometrics.

com/products/ﬁngerprint-spoof-liveness-detection/
sensor-vulnerability-analysis/.

[45] T. Ojala, M. Pietik¨ainen, and T. M¨aenp¨a¨a, “Multiresolution gray-
scale and rotation invariant texture classiﬁcation with local binary
patterns,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 24, pp. 971–
987, July 2002.

[46] J. Y. Choi, K. N. Plataniotis, and Y. M. Ro, “Using colour local
binary pattern features for face recognition,” in Proceedings of the
International Conference on Image Processing, ICIP 2010, September
26-29, Hong Kong, pp. 4541–4544, 2010.

