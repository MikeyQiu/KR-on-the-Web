OctNet: Learning Deep 3D Representations at High Resolutions

Gernot Riegler1

Ali Osman Ulusoy2
1Institute for Computer Graphics and Vision, Graz University of Technology
2Autonomous Vision Group, MPI for Intelligent Systems T¨ubingen
3Computer Vision and Geometry Group, ETH Z¨urich

Andreas Geiger2,3

riegler@icg.tugraz.at

{osman.ulusoy,andreas.geiger}@tue.mpg.de

7
1
0
2
 
r
p
A
 
0
1
 
 
]

V
C
.
s
c
[
 
 
4
v
9
0
0
5
0
.
1
1
6
1
:
v
i
X
r
a

Abstract

We present OctNet, a representation for deep learning
with sparse 3D data. In contrast to existing models, our rep-
resentation enables 3D convolutional networks which are
both deep and high resolution. Towards this goal, we ex-
ploit the sparsity in the input data to hierarchically parti-
tion the space using a set of unbalanced octrees where each
leaf node stores a pooled feature representation. This al-
lows to focus memory allocation and computation to the
relevant dense regions and enables deeper networks without
compromising resolution. We demonstrate the utility of our
OctNet representation by analyzing the impact of resolution
on several 3D tasks including 3D object classiﬁcation, ori-
entation estimation and point cloud labeling.

1. Introduction

Over the last several years, convolutional networks have
lead to substantial performance gains in many areas of com-
puter vision. In most of these cases, the input to the network
is of two-dimensional nature, e.g., in image classiﬁcation
[19], object detection [36] or semantic segmentation [14].
However, recent advances in 3D reconstruction [34] and
graphics [22] allow capturing and modeling large amounts
of 3D data. At the same time, large 3D repositories such as
ModelNet [48], ShapeNet [6] or 3D Warehouse1 as well as
databases of 3D object scans [7] are becoming increasingly
available. These factors have motivated the development of
convolutional networks that operate on 3D data.

Most existing 3D network architectures [8,30,35,48] re-
place the 2D pixel array by its 3D analogue, i.e., a dense and
regular 3D voxel grid, and process this grid using 3D con-
volution and pooling operations. However, for dense 3D
data, computational and memory requirements grow cubi-
cally with the resolution. Consequently, existing 3D net-
works are limited to low 3D resolutions, typically in the
order of 303 voxels. To fully exploit the rich and detailed

1https://3dwarehouse.sketchup.com

t
e
N
v
n
o
C
D
3

e
s
n
e
D

t
e
N
v
n
o
C
D
3

e
s
n
e
D

t
e
N
t
c
O

1

(a) Layer 1: 323

(b) Layer 2: 163

(c) Layer 3: 83

Figure 1: Motivation.
For illustration purposes, we
trained a dense convolutional network to classify 3D shapes
from [48]. Given a voxelized bed as input, we show the
maximum response across all feature maps at intermediate
layers (a-c) of the network before pooling. Higher activa-
tions are indicated with darker colors. Voxels with zero
activation are not displayed. The ﬁrst row visualizes the
responses in 3D while the second row shows a 2D slice.
Note how voxels close to the object contour respond more
strongly than voxels further away. We exploit the sparsity
in our data by allocating memory and computations using a
space partitioning data structure (bottom row).

geometry of our 3D world, however, much higher resolution
networks are required.

In this work, we build on the observation that 3D data is
often sparse in nature, e.g., point clouds, or meshes, result-
ing in wasted computations when applying 3D convolutions

na¨ıvely. We illustrate this in Fig. 1 for a 3D classiﬁcation
example. Given the 3D meshes of [48] we voxelize the input
at a resolution of 643 and train a simple 3D convolutional
network to minimize a classiﬁcation loss. We depict the
maximum of the responses across all feature maps at dif-
ferent layers of the network. It is easy to observe that high
activations occur only near the object boundaries.

Motivated by this observation, we propose OctNet, a 3D
convolutional network that exploits this sparsity property.
Our OctNet hierarchically partitions the 3D space into a set
of unbalanced octrees [32]. Each octree splits the 3D space
according to the density of the data. More speciﬁcally, we
recursively split octree nodes that contain a data point in its
domain, i.e., 3D points, or mesh triangles, stopping at the
ﬁnest resolution of the tree. Therefore, leaf nodes vary in
size, e.g., an empty leaf node may comprise up to 83 = 512
voxels for a tree of depth 3 and each leaf node in the octree
stores a pooled summary of all feature activations of the
voxel it comprises. The convolutional network operations
are directly deﬁned on the structure of these trees. There-
fore, our network dynamically focuses computational and
memory resources, depending on the 3D structure of the in-
put. This leads to a signiﬁcant reduction in computational
and memory requirements which allows for deep learning
at high resolutions. Importantly, we also show how essen-
tial network operations (convolution, pooling or unpooling)
can be efﬁciently implemented on this new data structure.

We demonstrate the utility of the proposed OctNet on
three different problems involving three-dimensional data:
3D classiﬁcation, 3D orientation estimation of unknown
object instances and semantic segmentation of 3D point
clouds. In particular, we show that the proposed OctNet en-
ables signiﬁcant higher input resolutions compared to dense
inputs due to its lower memory consumption, while achiev-
ing identical performance compared to the equivalent dense
network at lower resolutions. At the same time we gain sig-
niﬁcant speed-ups at resolutions of 1283 and above. Using
our OctNet, we investigate the impact of high resolution in-
puts wrt. accuracy on the three tasks and demonstrate that
higher resolutions are particularly beneﬁcial for orientation
estimation and semantic point cloud labeling. Our code is
available from the project website2.

2. Related Work

While 2D convolutional networks have proven very suc-
cessful in extracting information from images [12, 14, 19,
36, 42, 43, 47, 49, 50], there exists comparably little work
on processing three-dimensional data. In this Section, we
review existing work on dense and sparse models.

Dense Models: Wu et al. [48] trained a deep belief network
on shapes discretized to a 303 voxel grid for object classi-

2https://github.com/griegler/octnet

ﬁcation, shape completion and next best view prediction.
Maturana et al. [30] proposed VoxNet, a feed-forward con-
volutional network for classifying 323 voxel volumes from
RGB-D data. In follow-up work, Sedaghat et al. [1] showed
that introducing an auxiliary orientation loss increases clas-
siﬁcation performance over the original VoxNet. Similar
models have also been exploited for semantic point cloud
labeling [21] and scene context has been integrated in [52].
Recently, generative models [37] and auto-encoders [5,
40] have demonstrated impressive performance in learning
low-dimensional object representations from collections of
low-resolution (323) 3D shapes.
Interestingly, these low-
dimensional representations can be directly inferred from a
single image [15] or a sequence of images [8].

Due to computational and memory limitations, all afore-
mentioned methods are only able to process and generate
shapes at a very coarse resolution, typically in the order of
303 voxels. Besides, when high-resolution outputs are de-
sired, e.g., for labeling 3D point clouds, inefﬁcient sliding-
window techniques with a limited receptive ﬁeld must be
adopted [21]. Increasing the resolution na¨ıvely [33, 41, 53]
reduces the depth of the networks and hence their expres-
siveness. In contrast, the proposed OctNets allow for train-
ing deep architectures at signiﬁcant higher resolutions.

Sparse Models: There exist only few network architec-
tures which explicitly exploit sparsity in the data. As these
networks do not require exhaustive dense convolutions they
have the potential of handling higher resolutions.

Engelcke et al. [10] proposed to calculate convolutions
at sparse input locations by pushing values to their target
locations. This has the potential to reduce the number of
convolutions but does not reduce the amount of memory re-
quired. Consequently, their work considers only very shal-
low networks with up to three layers.

A similar approach is presented in [16, 17] where sparse
convolutions are reduced to matrix operations. Unfortu-
nately, the model only allows for 2
2 convolutions and
results in indexing and copy overhead which prevents pro-
cessing volumes of larger resolution (the maximum reso-
lution considered in [16, 17] is 803 voxels). Besides, each
layer decreases sparsity and thus increases the number of
operations, even at a single resolution. In contrast, the num-
ber of operations remains constant in our model.

×

Li et al. [28] proposed ﬁeld probing networks which
sample 3D data at sparse points before feeding them into
fully connected layers. While this reduces memory and
computation, it does not allow for exploiting the distributed
computational power of convolutional networks as ﬁeld
probing layers can not be stacked, convolved or pooled.

Jampani et al. [23] introduced bilateral convolution lay-
ers (BCL) which map sparse inputs into permutohedral
space where learnt convolutional ﬁlters are applied. Their
work is related to ours with respect to efﬁciently exploiting

the sparsity in the input data. However, in contrast to BCL
our method is speciﬁcally targeted at 3D convolutional net-
works and can be immediately dropped in as a replacement
in existing network architectures.

3. Octree Networks

To decrease the memory footprint of convolutional net-
works operating on sparse 3D data, we propose an adaptive
space partitioning scheme which focuses computations on
the relevant regions. As mathematical operations of deep
networks, especially convolutional networks, are best un-
derstood on regular grids, we restrict our attention to data
structures on 3D voxel grids. One of the most popular space
partitioning structures on voxel grids are octrees [31] which
have been widely adopted due to their ﬂexible and hier-
archical structure. Areas of application include depth fu-
sion [24], image rendering [27] and 3D reconstruction [45].
In this paper, we propose 3D convolutional networks on oc-
trees to learn representations from high resolution 3D data.
An octree partitions the 3D space by recursively subdi-
viding it into octants. By subdividing only the cells which
contain relevant information (e.g., cells crossing a surface
boundary or cells containing one or more 3D points) storage
can be allocated adaptively. Densely populated regions are
modeled with high accuracy (i.e., using small cells) while
empty regions are summarized by large cells in the octree.
Unfortunately, vanilla octree implementations [31] have
several drawbacks that hamper its application in deep net-
works. While octrees reduce the memory footprint of the
3D representation, most versions do not allow for efﬁcient
access to the underlying data. In particular, octrees are typi-
cally implemented using pointers, where each node contains
a pointer to its children. Accessing an arbitrary element (or
the neighbor of an element) in the octree requires a traversal
starting from the root until the desired cell is reached. Thus,
the number of memory accesses is equal to the depth of the
tree. This becomes increasingly costly for deep, i.e., high-
resolution, octrees. Convolutional network operations such
as convolution or pooling require frequent access to neigh-
boring elements. It is thus critical to utilize an octree design
that allows for fast data access.

We tackle these challenges by leveraging a hybrid grid-
octree data structure which we describe in Section 3.1. In
Section 3.2, we show how 3D convolution and pooling oper-
ations can be implemented efﬁciently on this data structure.

3.1. Hybrid Grid-Octree Data Structure

The above mentioned problems with the vanilla octree
data structure increase with the octree depth. Instead of rep-
resenting the entire high resolution 3D input with a single
unbalanced octree, we leverage a hybrid grid-octree struc-
ture similar to the one proposed by Miller et al. [32]. The
key idea is to restrict the maximal depth of an octree to a

Figure 2: Hybrid Grid-Octree Data Structure. This ex-
ample illustrates a hybrid grid-octree consisting of 8 shal-
low octrees indicated by different colors. Using 2 shallow
octrees in each dimension with a maximum depth of 3 leads
to a total resolution of 163 voxels.

(a) Shallow Octree

(b) Bit-Representation

Figure 3: Bit Representation. Shallow octrees can be
efﬁciently encoded using bit-strings. Here, the bit-string
1 01010000 00000000 01010000 00000000 01010000 0...
deﬁnes the octree in (a). The corresponding tree is shown
in (b). The color of the voxels corresponds to the split level.

small number, e.g., three, and place several such shallow
octrees along a regular grid (Fig. 2). While this data struc-
ture may not be as memory efﬁcient as the standard octree,
signiﬁcant compression ratios can still be achieved. For in-
stance, a single shallow octree that does not contain input
data stores only a single vector, instead of 83 = 512 vectors
for all voxels at the ﬁnest resolution at depth 3.

An additional beneﬁt of a collection of shallow octrees
is that their structure can be encoded very efﬁciently us-
ing a bit string representation which further lowers access
time and allows for efﬁcient GPGPU implementations [32].
Given a shallow octree of depth 3, we use 73 bit to repre-
sent the complete tree. The ﬁrst bit with index 0 indicates,
if the root node is split, or not. Further, bits 1 to 8 indicate
if one of the child nodes is subdivided and bits 9 to 72 de-
note splits of the grandchildren, see Fig. 3. A tree depth of
3 gives a good trade-off between memory consumption and
computational efﬁciency. Increasing the octree depth results
in an exponential growth in the required bits to store the tree
structure and further increases the cell traversal time.

Using this bit-representation, a single voxel in the shal-

low octree is fully characterised by its bit index. This index
determines the depth of the voxel in the octree and therefore
also the voxel size. Instead of using pointers to the parent
and child nodes, simple arithmetic can be used to retrieve
the corresponding indices of a voxel with bit index i:

(cid:22) i

,

(cid:23)

1

−
8
i + 1 .

pa(i) =

ch(i) = 8

·

(1)

(2)

In contrast to [32], we associate a data container (for storing
features vectors) with all leaf nodes of each shallow tree.
We allocate the data of a shallow octree in a contiguous data
array. The offset associated with a particular voxel in this
array can be computed as follows:

data idx(i) = 8

bit(j) + 1

bit(j)

pa(i)−1
(cid:88)

j=0

(cid:124)

(cid:123)(cid:122)
#nodes above i

+ mod (i
−
(cid:123)(cid:122)
offset

(cid:124)

1, 8)
(cid:125)

i−1
(cid:88)

−

(cid:125)

.

j=0
(cid:124)
(cid:123)(cid:122)
(cid:125)
#split nodes pre i

(3)

Here, mod denotes the modulo operator and bit returns the
tree bit-string value at i. See supp. document for an exam-
ple. Both sum operations can be efﬁciently implemented
using bit counting intrinsics (popcnt). The data arrays of
all shallow octrees are concatenated into a single contiguous
data array during training and testing to reduce I/O latency.

3.2. Network Operations

Given the hybrid grid-octree data structure introduced in
the previous Section, we now discuss the efﬁcient imple-
mentation of network operations on this data structure. We
will focus on the most common operations in convolutional
networks [14, 19, 36]: convolution, pooling and unpooling.
Note that point-wise operations, like activation functions,
do not differ in their implementation as they are indepen-
dent of the data structure.

×

H

Let us ﬁrst introduce the notation which will be used
throughout this Section. Ti,j,k denotes the value of a 3D
tensor T at location (i, j, k). Now assume a hybrid grid-
octree structure with D
W unbalanced shallow oc-
×
trees of maximum depth 3. Let O[i, j, k] denote the value
of the smallest cell in this structure which comprises the
voxel (i, j, k). Note that in contrast to the tensor notation,
O[i1, j1, k1] and O[i2, j2, k2] with i1
=
k2 may refer to the same voxel in the hybrid grid-octree,
depending on the size of the voxels. We obtain the index
of the shallow octree in the grid via (
) and the
local index of the voxel at the ﬁnest resolution in that octree
by (mod (i, 8), mod (j, 8), mod (k, 8)).

= j2

= i2

k
8 (cid:99)

j
8 (cid:99)

i
8 (cid:99)

k1

j1

∨

∨

(cid:98)

(cid:98)

(cid:98)

,

,

(a) Standard Convolution

(b) Efﬁcient Convolution

Figure 4: Convolution. This ﬁgure illustrates the convolu-
tion of a 33 kernel (red) with a 83 grid-octree cell (black).
Only 2 of the 3 dimensions are shown. A na¨ıve implemen-
tation evaluates the kernel at every location (i, j, k) within
a grid-octree cell as shown in (a). This results in
14k mul-
In contrast, (b) depicts our
tiplications for this example.
efﬁcient implementation of the same operation which re-
3k multiplications. As all 83 voxels inside the
quires only
grid-octree cell are the same value, the convolution kernel
inside the cell needs to be evaluated only once. Voxels at the
cell boundary need to integrate information from neighbor-
ing cells. This can be efﬁciently implemented by summing
truncated kernels. See our supp. document for details.

∼

∼

Given this notation, the mapping from a grid-octree O to

a tensor T with compatible dimensions is given by

oc2ten : Ti,j,k = O[i, j, k] .

Similarly, the reverse mapping is given by

ten2oc : O[i, j, k] = pool voxels
(¯i,¯j,¯k)∈Ω[i,j,k]

(T¯i,¯j,¯k) ,

(4)

(5)

·

where pool voxels (
) is a pooling function (e.g., average-
or max-pooling) which pools all voxels in T over the small-
est grid-octree cell comprising location (i, j, k), denoted by
Ω[i, j, k]. This pooling is necessary as a single voxel in O
can cover up to 83 = 512 elements of T , depending on its
size
Remark: With the two functions deﬁned above, we could
wrap any network operation f deﬁned on 3D tensors via

Ω[i, j, k]
|

|

.

g(O) = ten2oc(f (oc2ten(O))) .

(6)

However, this would require a costly conversion from the
memory efﬁcient grid-octrees to a regular 3D tensor and
back. Besides, storing a dense tensor in memory limits the
maximal resolution. We therefore deﬁne our network oper-
ations directly on the hybrid grid-octree data structure.

Convolution The convolution operation is the most im-
portant, but also the most computational expensive opera-
tion in deep convolutional networks. For a single feature

(a) Input

(b) Output

(a) Input

(b) Output

Figure 5: Pooling. The 23 pooling operation on the grid-
octree structure combines 8 neighbouring shallow octrees
(a) into one shallow octree (b). The size of each voxel is
halved and copied to the new shallow octree structure. Vox-
els on the ﬁnest resolution are pooled. Different shallow
octrees are depicted in different colors.

Figure 6: Unpooling. The 23 unpooling operation trans-
forms a single shallow octree of depth d as shown in (a)
into 8 shallow octrees of depth d
1, illustrated in (b). For
each node at depth zero one shallow octree is spawned. All
other voxels double in size. Different shallow octrees are
depicted in different colors.

−

map, convolving a 3D tensor T with a 3D convolution ker-
nel W

RL×M ×N can be written as

∈

T out
i,j,k =

L−1
(cid:88)

M −1
(cid:88)

N −1
(cid:88)

l=0

m=0

n=0

Wl,m,n

T in
ˆi,ˆj,ˆk

,

·

(7)

l +

n +
. Similarly, the convolutions on the grid-octree data

M/2
(cid:99)
(cid:98)

L/2
(cid:99)

m +

−

−

−

(cid:98)

, ˆj = j

, ˆk = k

with ˆi = i
N/2
(cid:98)
(cid:99)
structure are deﬁned as

Oout[i, j, k] = pool voxels
(¯i,¯j,¯k)∈Ω[i,j,k]

(T¯i,¯j,¯k)

(8)

Ti,j,k =

L−1
(cid:88)

M −1
(cid:88)

N −1
(cid:88)

l=0

m=0

n=0

Wl,m,n

Oin[ˆi, ˆj, ˆk] .

·

While this calculation yields the same result as the tensor
convolution in Eq. (7) with the oc2ten, ten2oc wrapper, we
are now able to deﬁne a computationally more efﬁcient con-
volution operator. Our key observation is that for small con-
volution kernels and large voxels, Ti,j,k is constant within
a small margin of the voxel due to its constant support
Oin[ˆi, ˆj, ˆk]. Thus, we only need to compute the convolu-
tion within the voxel once, followed by convolution along
the surface of the voxel where the support changes due to
adjacent voxels taking different values (Fig. 4). This mini-
mizes the number of calculations by a factor of 4 for voxels
of size 83, see supp. material for a detailed derivation. At
the same time, it enables a better caching mechanism.

Pooling Another important operation in deep convolu-
tional networks is pooling. Pooling reduces the spatial res-
olution of the input tensor and aggregates higher-level in-
formation for further processing, thereby increasing the re-
ceptive ﬁeld and capturing context. For instance, strided
23 max-pooling divides the input tensor T in into 23 non-
overlapping regions and computes the maximum value

within each region. Formally, we have

T out
i,j,k = max

l,m,n∈[0,1]

(cid:0)T in

2i+l,2j+m,2k+n

(cid:1) ,

(9)

∈

∈

where T in

R2D×2H×2W and T out

RD×H×W .

To implement pooling on the grid-octree data structure
we reduce the number of shallow octrees. For an input grid-
octree Oin with 2D
2W shallow octrees, the output
×
Oout contains D
W shallow octrees. Each voxel
of Oin is halved in size and copied one level deeper in the
shallow octree. Voxels at depth 3 in Oin are pooled. This
can be formalized as

×
H

2H

×

×

(cid:40)

Oout[i, j, k] =

Oin[2i, 2j, 2k]
P

if vxd(2i, 2j, 2k) < 3
else

P = max

l,m,n∈[0,1]

(Oin[2i + l, 2j + m, 2k + n]) ,

(10)

where vxd(
) computes the depth of the indexed voxel in
·
the shallow octree. A visual example is depicted in Fig. 5.

Unpooling For several tasks such as semantic segmenta-
tion, the desired network output is of the same size as the
network input. While pooling is crucial to increase the re-
ceptive ﬁeld size of the network and capture context, it loses
spatial resolution. To increase the resolution of the net-
work, U-shaped network architectures have become pop-
ular [2, 53] which encode information using pooling oper-
ations and increase the resolution in a decoder part using
unpooling or deconvolution layers [51], possibly in com-
bination with skip-connections [9, 19] to increase preci-
sion. The simplest unpooling strategy uses nearest neigh-
bour interpolation and can be formalized on dense input
R2D×2H×2W tensors
T in
∈
as follows:

RD×H×W and output T out

∈

i,j,k = T in
T out

(cid:98)i/2(cid:99),(cid:98)j/2(cid:99),(cid:98)k/2(cid:99) .

(11)

Again, we can deﬁne the analogous operation on the hybrid
grid-octree data structure by

,

,

(cid:98)

k/2

(12)

] .
(cid:99)

i/2
(cid:99)

j/2
(cid:99)
(cid:98)

Oout[i, j, k] = Oin[
(cid:98)
This operation also changes the data structure: The number
of shallow octrees increases by a factor of 8, as each node
at depth 0 spawns a new shallow octree. All other nodes
double their size. Thus, after this operation the tree depth is
decreased. See Fig. 6 for a visual example of this operation.
Remark: To capture ﬁne details, voxels can be split again
at the ﬁnest resolution according to the original octree of
the corresponding pooling layer. This allows us to take full
advantage of skip connections. We follow this approach in
our semantic 3D point cloud labeling experiments.

4. Experimental Evaluation

In this Section we leverage our OctNet representation to
investigate the impact of input resolution on three different
3D tasks: 3D shape classiﬁcation, 3D orientation estimation
and semantic segmentation of 3D point clouds. To isolate
the effect of resolution from other factors we consider sim-
ple network architectures. Orthogonal techniques like data
augmentation, joint 2D/3D modeling or ensemble learning
are likely to further improve the performance of our models.

Implementation We implemented the grid-octree data
structure, all layers including the necessary forward and
backward functions, as well as utility methods to create the
data structure from point clouds and meshes, as a stand-
alone C++/CUDA library. This allows the usage of our code
within all existing deep learning frameworks. For our ex-
perimental evaluation we used the Torch3 framework.

4.1. 3D Classiﬁcation

We use the popular ModelNet10 dataset [48] for the 3D
shape classiﬁcation task. The dataset contains 10 shape cat-
egories and consists of 3991 3D shapes for training and 908
3D shapes for testing. Each shape is provided as a trian-
gular mesh, oriented in a canonical pose. We convert the
triangle meshes to dense respective grid-octree occupancy
grids, where a voxel is set to 1 if it intersects the mesh. We
P )3 voxels,
scale each mesh to ﬁt into a 3D grid of (N
where N is the number of voxels in each dimension of the
input grid and P = 2 is a padding parameter.

−

We ﬁrst study the inﬂuence of the input resolution on
memory usage, runtime and classiﬁcation accuracy. To-
wards this goal, we create a series of networks of different
input resolution from 83 to 2563 voxels. Each network con-
sists of several blocks which reduce resolution by half until
we reach a resolution of 83. Each block comprises two con-
volutional layers (33 ﬁlters, stride 1) and one max-pooling

3http://torch.ch/

(a) Memory

(b) Runtime

(c) Accuracy

(d) Accuracy

Figure 7: Results on ModelNet10 Classiﬁcation Task.

layer (23 ﬁlters, stride 2). The number of feature maps in the
ﬁrst block is 8 and increases by 6 with every block. After the
last block we add a fully-connected layer with 512 units and
a ﬁnal output layer with 10 units. Each convolutional layer
and the ﬁrst fully-connected layer are followed by a recti-
ﬁed linear unit [26] as activation function and the weights
are initialized as described in [18]. We use the standard
cross-entropy loss for training and train all networks for 20
epochs with a batch size of 32 using Adam [25]. The initial
learning rate is set to 0.001 and we decrease the learning
rate by a factor of 10 after 15 epochs.

Overall, we consider three different types of networks:
the original VoxNet architecture of Maturana et al. [30]
which operates on a ﬁxed 323 voxel grid, the proposed Oct-
Net and a dense version of it which we denote “DenseNet”
in the following. While performance gains can be ob-
tained using orthogonal approaches such as network ensem-
bles [5] or a combination of 3D and 2D convolutional net-
works [20,42], in this paper we deliberately focus on “pure”
3D convolutional network approaches to isolate the effect of
resolution from other inﬂuencing factors.

Fig. 7 shows our results. First, we compare the mem-
ory consumption and run-time of our OctNet wrt. the dense
baseline approach, see Fig. 7a and 7b. Importantly, OctNets
require signiﬁcantly less memory and run-time for high in-
put resolutions compared to dense input grids. Using a
batch size of 32 samples, our OctNet easily ﬁts in a modern
GPU’s memory (12GB) for an input resolution of 2563. In
contrast, the corresponding dense model ﬁts into the mem-
643. A more detailed analysis of
ory only for resolutions
the memory consumption wrt. the sparsity in the data is pro-
vided in the supp. document. OctNets also run faster than
their dense counterparts for resolutions >643. For resolu-
643, OctNets run slightly slower due to the overhead
tions
incurred by the grid-octree representation and processing.

≤

≤

3
8

3
6
1

3
2
3

3
4
6

83

323

Figure 9: Confusion Matrices on ModelNet10.

Bathtub

Bed

Dresser

N. Stand

(a) Mean Angular Error

(b) Mean Angular Error

Figure 8: Voxelized 3D Shapes from ModelNet10.

Figure 10: Orientation Estimation on ModelNet10.

Leveraging our OctNets, we now compare the impact
of input resolution with respect to classiﬁcation accuracy.
Fig. 7c shows the results of different OctNet architectures
where we keep the number of convolutional layers per block
ﬁxed to 1, 2 and 3. Fig. 7d shows a comparison of accuracy
with respect to DenseNet and VoxNet when keeping the ca-
pacity of the model, i.e., the number of parameters, constant
by removing max-pooling layers from the beginning of the
network. We ﬁrst note that despite its pooled representation,
OctNet performs on par with its dense equivalent. This con-
ﬁrms our initial intuition (Fig. 1) that sparse data allows for
allocating resources adaptively without loss in performance.
Furthermore, both models outperform the shallower VoxNet
architecture, indicating the importance of network depth.

Regarding classiﬁcation accuracy we observed improve-
ments for lower resolutions but diminishing returns beyond
an input resolution of 323 voxels. Taking a closer look at
the confusion matrices in Fig. 9, we observe that higher in-
put resolution helps for some classes, e.g., bathtub, while
others remain ambiguous independently of the resolution,
e.g., dresser vs. night stand. We visualize this lack of dis-
criminative power by showing voxelized representations of
3D shapes from the ModelNet10 database Fig. 8. While
bathtubs look similar to beds (or sofas, tables) at low reso-
lution they can be successfully distinguished at higher reso-
lutions. However, a certain ambiguity between dresser and
night stand remains.

4.2. 3D Orientation Estimation

In this Section, we investigate the importance of input
resolution on 3D orientation estimation. Most existing ap-
proaches to 3D pose estimation [3, 4, 39, 44, 46] assume that
the true 3D shape of the object instance is known. To assess

the generalization ability of 3D convolutional networks, we
consider a slightly different setup where only the object cat-
egory is known. After training a model on a hold-out set of
3D shapes from a single category, we test the ability of the
model to predict the 3D orientation of unseen 3D shapes
from the same category.

±

−

2
(cid:105)

q1, q2

More concretely, given an instance of an object category
with unknown pose, the goal is to estimate the rotation with
respect to the canonical pose. We utilize the 3D shapes from
the chair class of the ModelNet10 dataset and rotate them
15◦ around each axis. We use the same
randomly between
network architectures and training protocol as in the classi-
ﬁcation experiment, except that the networks regress orien-
tations. We use unit quaternions to represent 3D rotations
and train our networks with an Euclidean loss. For small an-
gles, this loss is a good approximation to the rotation angle
φ = arccos(2
(cid:104)

1) between quaternions q1, q2.

Fig. 10 shows our results using the same naming con-
vention as in the previous Section. We observe that ﬁne
details are more important compared to the classiﬁcation
task. For the OctNet 1-3 architectures we observe a steady
increase in performance, while for networks with constant
capacity across resolutions (Fig. 10b), performance levels
beyond 1283 voxels input resolution. Qualitative results of
the latter experiment are shown in Fig. 11. Each row shows
10 different predictions for two randomly selected chair
instance over several input resolutions, ranging from 163
to 1283. Darker colors indicate larger errors which occur
more frequently at lower resolutions. In contrast, predic-
tions at higher network resolutions cluster around the true
pose. Note that learning a dense 3D representation at a res-
olution of 1283 voxels or beyond would not be feasible.

Riemenschneider et al. [38]
Martinovic et al. [29]
Gadde et al. [13]

OctNet 643
OctNet 1283
OctNet 2563

Average Overall

IoU

-
-
68.5

60.0
65.3
73.6

-
-
78.6

73.6
76.1
81.5

42.3
52.2
54.4

45.6
50.4
59.2

Table 1: Semantic Segmentation on RueMonge2014.

(a) Voxelized Input

(b) Voxel Estimates

(c) Estimated Point Cloud (d) Ground Truth Point Cloud
Figure 12: OctNet 2563 Facade Labeling Results.

TP

the test set are assigned the label of the corresponding grid-
octree voxels. As evaluation measures we use overall pixel
TP+FN over all 3D points, average class accu-
accuracy
racy, and intersection over union
TP+FN+FP over all classes.
Here, FP, FN and TP denote false positives, false negatives
and true positives, respectively.

TP

Our results clearly show that increasing the input reso-
lution is essential to obtain state-of-the-art results, as ﬁner
details vanish at coarser resolutions. Qualitative results for
one facade are provided in Fig. 12. Further results are pro-
vided in the supp. document.

5. Conclusion and Future Work

We presented OctNet, a novel 3D representation which
makes deep learning with high-resolution inputs tractable.
We analyzed the importance of high resolution inputs on
several 3D learning tasks, such as object categorization,
pose estimation and semantic segmentation. Our exper-
iments revealed that for ModelNet10 classiﬁcation low-
resolution networks prove sufﬁcient while high input (and
output) resolution matters for 3D orientation estimation and
3D point cloud labeling. We believe that as the community
moves from low resolution object datasets such as Model-
Net10 to high resolution large scale 3D data, OctNet will
enable further improvements. One particularly promising
avenue for future research is in learning representations for
multi-view 3D reconstruction where the ability to process
high resolution voxelized shapes is of crucial importance.

163

323

643

1283

Figure 11: Orientation Estimation on ModelNet10. This
ﬁgure illustrates 10 rotation estimates for 3 chair instances
while varying the input resolution from 163 to 1283. Darker
colors indicate larger deviations from the ground truth.

4.3. 3D Semantic Segmentation

In this Section, we evaluate the proposed OctNets on the
problem of labeling 3D point cloud with semantic informa-
tion. We use the RueMonge2014 dataset [38] that provides
a colored 3D point cloud of several Haussmanian style fa-
cades, comprising
1 million 3D points in total. The labels
are window, wall, balcony, door, roof, sky and shop.

∼

For this task, we train a U-shaped network [2, 53] on
three different input resolutions, 643, 1283 and 2563, where
the voxel size was selected such that the height of all build-
ings ﬁts into the input volume. We ﬁrst map the point cloud
into the grid-octree structure. For all leaf nodes which con-
tain more than one point, we average the input features and
calculate the majority vote of the ground truth labels for
training. As features we use the binary voxel occupancy, the
RGB color, the normal vector and the height above ground.
Due to the small number of training samples, we augment
the data for this task by applying small rotations.

Our network architecture comprises an encoder and a de-
coder part. The encoder part consists of four blocks which
comprise 2 convolution layers (33 ﬁlters, stride 1) followed
by one max-pooling layer each. The decoder consists of
four blocks which comprise 2 convolutions (33 ﬁlters, stride
1) followed by a guided unpooling layer as discussed in the
previous Section. Additionally, after each unpooling step all
features from the last layer of the encoder at the same res-
olution are concatenated to provide high-resolution details.
All networks are trained with a per voxel cross entropy loss
using Adam [25] and a learning rate of 0.0001.

Table 1 compares the proposed OctNet to several state
of the art approaches on the facade labeling task following
the extended evaluation protocol of [13]. The 3D points of

References

[1] N. S. Alvar, M. Zolfaghari, and T. Brox. Orientation-boosted
voxel nets for 3d object recognition. arXiv.org, 1604.03351,
2016. 2

[2] V. Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A
deep convolutional encoder-decoder architecture for image
segmentation. arXiv.org, 1511.00561, 2015. 5, 8, 14

[3] E. Brachmann, A. Krull, F. Michel, S. Gumhold, J. Shotton,
and C. Rother. Learning 6d object pose estimation using
In Proc. of the European Conf. on
3d object coordinates.
Computer Vision (ECCV), 2014. 7

[4] E. Brachmann, F. Michel, A. Krull, M. Y. Yang, S. Gumhold,
and C. Rother. Uncertainty-driven 6d pose estimation of ob-
In Proc. IEEE
jects and scenes from a single rgb image.
Conf. on Computer Vision and Pattern Recognition (CVPR),
2016. 7

[5] A. Brock, T. Lim, J. M. Ritchie, and N. Weston. Generative
and discriminative voxel modeling with convolutional neural
networks. arXiv.org, 1608.04236, 2016. 2, 6

[6] A. X. Chang, T. A. Funkhouser, L. J. Guibas, P. Hanrahan,
Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su,
J. Xiao, L. Yi, and F. Yu. Shapenet: An information-rich 3d
model repository. arXiv.org, 1512.03012, 2015. 1

[7] S. Choi, Q. Zhou, S. Miller, and V. Koltun. A large dataset

of object scans. arXiv.org, 1602.02481, 2016. 1

[8] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese. 3d-
r2n2: A uniﬁed approach for single and multi-view 3d object
reconstruction. In Proc. of the European Conf. on Computer
Vision (ECCV), 2016. 1, 2

[9] A. Dosovitskiy, P. Fischer, E. Ilg, P. Haeusser, C. Hazirbas,
V. Golkov, P. v.d. Smagt, D. Cremers, and T. Brox. Flownet:
Learning optical ﬂow with convolutional networks. In Proc.
of the IEEE International Conf. on Computer Vision (ICCV),
2015. 5

[10] M. Engelcke, D. Rao, D. Z. Wang, C. H. Tong, and I. Posner.
Vote3deep: Fast object detection in 3d point clouds using ef-
ﬁcient convolutional neural networks. arXiv.org, 609.06666,
2016. 2

[11] G. Fanelli, J. Gall, and L. Van Gool. Real time head pose
In Proc. IEEE
estimation with random regression forests.
Conf. on Computer Vision and Pattern Recognition (CVPR),
2011. 13

[12] J. Flynn, I. Neulander, J. Philbin, and N. Snavely. Deep-
stereo: Learning to predict new views from the world’s im-
agery. arXiv.org, 1506.06825, 2015. 2

[13] R. Gadde, V. Jampani, R. Marlet, and P. V. Gehler. Ef-
ﬁcient 2d and 3d facade segmentation using auto-context.
arXiv.org, 1606.06437, 2016. 8

[14] G. Ghiasi and C. C. Fowlkes. Laplacian pyramid reconstruc-
tion and reﬁnement for semantic segmentation. In Proc. of
the European Conf. on Computer Vision (ECCV), 2016. 1, 2,
4

[15] R. Girdhar, D. F. Fouhey, M. Rodriguez, and A. Gupta.
Learning a predictable and generative vector representation
In Proc. of the European Conf. on Computer
for objects.
Vision (ECCV), 2016. 2

[16] B. Graham. Spatially-sparse convolutional neural networks.

arXiv.org, 2014. 2

[17] B. Graham. Sparse 3d convolutional neural networks.

In
Proc. of the British Machine Vision Conf. (BMVC), 2015. 2
[18] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation. In Proc. of the IEEE International Conf. on
Computer Vision (ICCV), 2015. 6

[19] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
In Proc. IEEE Conf. on Computer

for image recognition.
Vision and Pattern Recognition (CVPR), 2016. 1, 2, 4, 5
[20] V. Hegde and R. Zadeh. Fusionnet: 3d object classiﬁcation
using multiple data representations. arXiv.org, 1607.05695,
2016. 6

[21] J. Huang and S. You. Point cloud labeling using 3d convolu-
tional neural network. In Proc. of the International Conf. on
Pattern Recognition (ICPR), 2016. 2

[22] Q. Huang, H. Wang, and V. Koltun. Single-view reconstruc-
In
tion via joint analysis of image and shape collections.
ACM Trans. on Graphics (SIGGRAPH), 2015. 1

[23] V. Jampani, M. Kiefel, and P. V. Gehler. Learning sparse high
dimensional ﬁlters: Image ﬁltering, dense crfs and bilateral
neural networks. In Proc. IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), 2016. 2

[24] W. Kehl, T. Holl, F. Tombari, S. Ilic, and N. Navab. An
octree-based approach towards efﬁcient variational range
data fusion. arXiv.org, 1608.07411, 2016. 3

[25] D. P. Kingma and J. Ba. Adam: A method for stochastic
optimization. In Proc. of the International Conf. on Learning
Representations (ICLR), 2015. 6, 8

[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in Neural Information Processing Systems (NIPS),
2012. 6

[27] S. Laine and T. Karras. Efﬁcient sparse voxel octrees. IEEE
Trans. on Visualization and Computer Graphics (VCG),
17(8):1048–1059, 2011. 3

[28] Y. Li, S. Pirk, H. Su, C. R. Qi, and L. J. Guibas. FPNN: ﬁeld
probing neural networks for 3d data. arXiv.org, 1605.06240,
2016. 2

[29] A. Martinovi´c, J. Knopp, H. Riemenschneider, and L. Van
Gool. 3d all the way: Semantic segmentation of urban scenes
from start to end in 3d. In Proc. IEEE Conf. on Computer
Vision and Pattern Recognition (CVPR), 2015. 8

[30] D. Maturana and S. Scherer. Voxnet: A 3d convolutional
In Proc.
neural network for real-time object recognition.
IEEE International Conf. on Intelligent Robots and Systems
(IROS), 2015. 1, 2, 6

[31] D. Meagher. Geometric modeling using octree encod-
ing. Computer Graphics and Image Processing (CGIP),
19(1):85, 1982. 3

[32] A. Miller, V. Jain, and J. L. Mundy. Real-time rendering
In Proc. of
and dynamic updating of 3-d volumetric data.
the Workshop on General Purpose Processing on Graphics
Processing Units (GPGPU), page 8, 2011. 2, 3, 4

[33] F. Milletari, N. Navab, and S. Ahmadi. V-net: Fully convo-
lutional neural networks for volumetric medical image seg-
mentation. arXiv.org, 1606.04797, 2016. 2

[49] J. Xie, R. B. Girshick, and A. Farhadi. Deep3d: Fully au-
tomatic 2d-to-3d video conversion with deep convolutional
In Proc. of the European Conf. on Com-
neural networks.
puter Vision (ECCV), 2016. 2

[50] A. R. Zamir, T. Wekel, P. Agrawal, C. Wei, J. Malik, and
S. Savarese. Generic 3d representation via pose estimation
and matching. In Proc. of the European Conf. on Computer
Vision (ECCV), 2016. 2

[51] M. D. Zeiler, G. W. Taylor, and R. Fergus. Adaptive decon-
volutional networks for mid and high level feature learning.
In Proc. of the IEEE International Conf. on Computer Vision
(ICCV), 2011. 5

[52] Y. Zhang, M. Bai, P. Kohli, S. Izadi, and J. Xiao. Deepcon-
text: Context-encoding neural pathways for 3d holistic scene
understanding. arXiv.org, 1603.04922, 2016. 2
¨Ozg¨un Cicek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and
O. Ronneberger. 3d u-net: Learning dense volumetric seg-
mentation from sparse annotation. arXiv.org, 1606.06650,
2016. 2, 5, 8, 14

[53]

[34] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux,
D. Kim, A. J. Davison, P. Kohli, J. Shotton, S. Hodges, and
A. Fitzgibbon. Kinectfusion: Real-time dense surface map-
ping and tracking. In Proc. of the International Symposium
on Mixed and Augmented Reality (ISMAR), 2011. 1

[35] C. R. Qi, H. Su, M. Nießner, A. Dai, M. Yan, and L. Guibas.
Volumetric and multi-view cnns for object classiﬁcation on
3d data. In Proc. IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR), 2016. 1

[36] S. Ren, K. He, R. B. Girshick, and J. Sun. Faster R-CNN:
towards real-time object detection with region proposal net-
works. In Advances in Neural Information Processing Sys-
tems (NIPS), 2015. 1, 2, 4

[37] D. J. Rezende, S. M. A. Eslami, S. Mohamed, P. Battaglia,
M. Jaderberg, and N. Heess. Unsupervised learning of 3d
structure from images. arXiv.org, 1607.00662, 2016. 2
[38] H. Riemenschneider, A. B´odis-Szomor´u, J. Weissenberg,
and L. V. Gool. Learning where to classify in multi-view
semantic segmentation. In Proc. of the European Conf. on
Computer Vision (ECCV), 2014. 8, 14

[39] R. Rios-Cabrera and T. Tuytelaars. Discriminatively trained
templates for 3d object detection: A real-time scalable ap-
proach. In Proc. of the IEEE International Conf. on Com-
puter Vision (ICCV), pages 2048–2055, 2013. 7

[40] A. Sharma, O. Grau, and M. Fritz. Vconv-dae: Deep vol-
arXiv.org,

umetric shape learning without object labels.
1604.03755, 2016. 2

[41] S. Song and J. Xiao. Deep sliding shapes for amodal 3d
object detection in RGB-D images. arXiv.org, 1511.02300,
2015. 2

[42] H. Su, S. Maji, E. Kalogerakis, and E. G. Learned-Miller.
Multi-view convolutional neural networks for 3d shape
In Proc. of the IEEE International Conf. on
recognition.
Computer Vision (ICCV), 2015. 2, 6

[43] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Multi-view
3d models from single images with a convolutional network.
In Proc. of the European Conf. on Computer Vision (ECCV),
2016. 2

[44] A. Tejani, D. Tang, R. Kouskouridas, and T. Kim. Latent-
class hough forests for 3d object detection and pose estima-
In Proc. of the European Conf. on Computer Vision
tion.
(ECCV), 2014. 7

[45] A. O. Ulusoy, A. Geiger, and M. J. Black. Towards prob-
abilistic volumetric reconstruction using ray potentials.
In
Proc. of the International Conf. on 3D Vision (3DV), 2015. 3
[46] P. Wohlhart and V. Lepetit. Learning descriptors for object
recognition and 3d pose estimation. In Proc. IEEE Conf. on
Computer Vision and Pattern Recognition (CVPR), 2015. 7
[47] J. Wu, T. Xue, J. J. Lim, Y. Tian, J. B. Tenenbaum, A. Tor-
ralba, and W. T. Freeman. Single image 3d interpreter net-
work. In Proc. of the European Conf. on Computer Vision
(ECCV), 2016. 2

[48] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and
J. Xiao. 3d shapenets: A deep representation for volumetric
shapes. In Proc. IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR), 2015. 1, 2, 6, 12

A. Appendix

A.1. Introduction

A.2. Data Index

In the following supplemental material we present details regarding our operations on the hybrid grid-octree data structure,
additional experimental results and all details on the used network architectures. In Section A.2 we provide details on the
data index used to efﬁciently access data in the grid-octree data structure. Section A.3 yields more insights into the efﬁcient
implementation of the convolution operation on octrees. We present further quantitative and qualitative results in Section A.4
and in Section A.8 we specify all details of the network architectures used in our experiments.

An important implementation detail of the hybrid grid-octree data structure is the memory alignment for fast data access.
We store all data associated with the leaf nodes of a shallow octree in a compact contiguous array. Thus, we need a fast way
to compute the offset in this data array for any given voxel. In the main text we presented the following equation:

(13)

(14)

(15)

data idx(i) = 8

bit(j) + 1

bit(j)

pa(i)−1
(cid:88)

j=0

(cid:124)

(cid:123)(cid:122)
#nodes above i

i−1
(cid:88)

−

(cid:125)

j=0
(cid:123)(cid:122)
(cid:125)
(cid:124)
#split nodes pre i

+ mod (i
−
(cid:123)(cid:122)
offset

(cid:124)

.

1, 8)
(cid:125)

As explained in the main text the whole octree structure is stored as a bit-string and a voxel is uniquely identiﬁed by the bit
index i, i.e., the index within the bit string. The data is aligned breadth-ﬁrst and only the leaf nodes have data associated.
Consequently, the ﬁrst part of the equation above counts the number of split and leaf nodes up to the voxel with bit index i.
The second term subtracts the number of split nodes before the particular voxel as data is only associated with leaf nodes.
Finally, we need to get the offset within the voxel’s neighborhood. This is done by the last term of the equation.

Let us illustrate this with a simple example: For ease of visualization we will consider a quadtree. Hence, each voxel can

be split into 4 instead of 8 children. The equation for the offset changes to

data idx4(i) = 4

bit(j) + 1

bit(j)

pa4(i)−1
(cid:88)

i−1
(cid:88)

j=0

(cid:124)

(cid:123)(cid:122)
#nodes above i

−

(cid:125)

j=0
(cid:125)
(cid:123)(cid:122)
(cid:124)
#split nodes pre i

+ mod (i
−
(cid:123)(cid:122)
offset

(cid:124)

,

1, 4)
(cid:125)

with

pa4(i) =

(cid:22) i

(cid:23)

1

.

−
4

Now consider the following bit string for instance: 1 0101 0000 1001 0000 0100. According to our deﬁnition, this bit string
corresponds to the tree structure visualized in Fig. 13a and 13b, where s indicates a split node and v a leaf node with associated
data. In Fig. 13c we show the bit indices for all nodes. Note that the leaf nodes at depth 3 do not need to be stored in the bit
string as this information is implicit. Finally, the data index for all leaf nodes is visualized in Fig. 13d. Now we can verify
equation (14) using a simple example. Assume the bit index 51: The parent bit index is given by equation (15) as 12. To
compute the data index we ﬁrst count the number of nodes before 49 as it is the ﬁrst node within its siblings (ﬁrst term of
equation), which is 17. Next, we count the number of split nodes up to 49 (second term of equation), which is 6. Finally, we
look up the position 51 within its siblings (last term of equation), which is 2. Combining those three terms yields the data
index 17

6 + 2 = 13.

−

A.3. Efﬁcient Convolution

In the main text we discussed that the convolution for larger octree cells and small convolution kernels can be efﬁ-
ciently implemented. A na¨ıve implementation applies the convolution kernel at every location (i, j, k) comprised by the cell
Ω[i, j, k]. Therefore, for an octree cell of size 83 and a convolution kernel kernel of 33 this would require 83
33 = 13, 824
multiplications. However, we can implement this calculation much more efﬁciently as depicted in Fig. 14. We observe that
the value inside the cell of size 83 is constant. Thus, we only need to evaluate the convolution once inside this cell and
multiply the result with the size of the cell 83, see Fig. 14a. Additionally, we only need to evaluate a truncated versions of the

·

(a) Bit-String

(b) Split and Leaf Nodes

(c) Bit Index

(d) Data Index

Figure 13: Data Index.

(a) Constant

(b) Corners

(c) Edges

(d) Faces

Figure 14: Efﬁcient Convolution.

kernel on the corners, edges and faces of the voxel, see Fig. 14b-d. This implementation is more efﬁcient, as we need only
27 multiplications for the constant part, 8
15 multiplications for the edges, and
19 multiplications for the corners, 12
9 multiplications for the faces of the voxel. In total this yields 3203 multiplications, or 23.17% of the multiplications
6
required by the na¨ıve implementation.

62

6

·

·

·

·

·

In this Section we show additional quantitative and qualitative results for 3D shape classiﬁcation, 3D orientation estimation

A.4. Additional Results

and semantic 3D point labeling.

A.5. 3D Classiﬁcation

In the main text of our work we analyzed the runtime and memory consumption of OctNet compared with the equivalent
dense networks on ModelNet10 [48]. Additionally, we demonstrated that without further data augmentation, ensemble
learning, or more sophisticated architectures the accuracy saturates at an input resolution of about 163, when keeping the
number of network parameters ﬁxed across all resolutions. In this Section we show the same experiment on ModelNet40 [48].
The results are summarized in Fig. 15. In contrast to ModelNet10, we see an increase in accuracy up to an input resolution
of 323. Beyond this resolution the classiﬁcation performance does not further improve. Note that the only form of data
augmentation we used in this experiment was rotation around the up-vector as the 3D models in this dataset vary in pose. We
conclude that object classiﬁcation on the ModelNet40 dataset is more challenging than on the ModelNet10 dataset, but both
datasets are relatively easy in the sense that details do not matter as much as in the datasets used for our other experiments.

We further use this experiment to investigate the question at which level of sparsity OctNet becomes useful. To answer
this, we visualize the memory consumption of a dense representation vs. our data structure at different resolutions and
occupancies by sampling 500 shapes from the ModelNet40 dataset (see Fig. 16). It can be observed that even at very low
resolutions (83), our data structure is superior compared to the dense representation, even up to an occupancy level of 50%. As
the voxel resolution increases, the occupancy levels (x-axis) decreases since the data becomes sparser. Our OctNet exploits
this sparsity to achieve a signiﬁcant reduction in memory consumption.

A.6. 3D Orientation Estimation

To demonstrate that OctNet can handle input resolutions larger than 2563 we added results for the 3D orientation estimation
experiment with an input resolution of 5123. The results are presented in Fig. 17. As for the orientation experiment in the
main paper, we can observe the trend that performance increases with increasing input resolution.

(a) Accuracy

(b) Accuracy

(c) Confusion Matrix 83

(d) Confusion Matrix 323

Figure 15: ModelNet40 results.

83

163

323

643

1283

2563

Figure 16: Memory consumption vs. occupancy.

We evaluated our OctNet also on the Biwi Kinect Head Pose Database [11] as an additional experiment on 3D pose
estimation. The dataset consists of 24 sequences of 20 individuals sitting in front of a Kinect depth sensor. For each
frame the head center and the head pose in terms of its 3D rotation is annotated. We split the dataset into a training set
of 18 individuals for training and 2 individuals for testing and project the depth map to 3D points with the given camera
parameters. We then create the hybrid grid-octree structure from the 3D points that belong to the head (In this experiment
we are only interested in 3D orientation estimation, as the head can be reliable detected in the color images). As in the
previous experiment we parameterize the orientation with unit quaternions and train our OctNet using the same settings as
in the previous 3D orientation estimation experiment. Fig. 18 shows the quantitative results over varying input resolutions.
We see a reasonable improvement of accuracy from 83 up to 643. Beyond this input resolution the octree resolution becomes
ﬁner than the resolution of the 3D point cloud. Thus, further improvements can not be expected. In Fig. 19 we show some
qualitative results.

(a) 83

(b) 163

(c) 323

(d) 643

(e) 1283

Figure 19: Qualitative Results for Head Pose Estimation.

Figure 17: Additional Chair Orientation Results.

Figure 18: Head Pose Results.

A.7. 3D Semantic Segmentation

In this Section we present additional qualitative results of the semantic 3D point labeling task in Fig. 20 to 22. In these
visualizations we show the color part of the voxelized input, the result of the labeling in the voxel representation, and the
result back-projected to the 3D point cloud for different houses in the test set of [38].

A.8. Network Architecture Details

In this Section we detail the network architectures used throughout our experimental evaluations. We use the following no-
tation for brevity: conv(x, y) denotes a 33 convolutional layer with x input feature maps and y output feature maps. Similarly,
maxpool(f ) is a max-pooling operation that decreases dimensionality by a factor of f along each axis. All convolutional and
fully-connected layers, except the very last one, are followed by a ReLU as activation function.

In the ﬁrst two experiments, 3D classiﬁcation and 3D orientation estimation, we show two different classes of architectures.
In the ﬁrst one we keep the number of convolution layers per block ﬁxed and add blocks depending on the input resolution
of the network. We call those networks OctNet1, OctNet2, and OctNet3, depending on the number of convolution layers per
block. Therefore, the number of parameters increases along with the input resolution. The detailed architectures are depicted
in Table 2, 3, and 4 for the classiﬁcation task and in Table 6, 7, and 8 for the orientation estimation tasks, respectively. Second,
we trained network architectures where we keep the number of parameters ﬁxed, independently of the input resolution. The
detailed network architectures for those experiments are presented in Table 5 and 9.

Finally, for semantic 3D point labeling, we use the U-Net type architecture [2, 53] shown in Table 10. We use a concate-

) to combine the outputs from the decoder and encoder parts of the networks to preserve details.

nation layer concat(
·

,

·

3
4
6

3
8
2
1

3
6
5
2

3
4
6

3
8
2
1

3
6
5
2

(a) Voxelized Input

(b) Voxel Estimates

(c) Estimated Point Cloud

(d) Ground Truth Point Cloud

Figure 20: Facade Labeling Results. Zoom in for details.

(a) Voxelized Input

(b) Voxel Estimates

(c) Estimated Point Cloud

(d) Ground Truth Point Cloud

Figure 21: Facade Labeling Results. Zoom in for details.

3
4
6

3
8
2
1

3
6
5
2

(a) Voxelized Input

(b) Voxel Estimates

(c) Estimated Point Cloud

(d) Ground Truth Point Cloud

Figure 22: Facade Labeling Results. Zoom in for details.

83
conv(1, 8)

163
conv(1, 8)
maxpool(2)
conv(8, 16)

323
conv(1, 8)
maxpool(2)
conv(8, 16)
maxpool(2)
conv(16, 24)

643
conv(1, 8)
maxpool(2)
conv(8, 16)
maxpool(2)
conv(16, 24)
maxpool(2)
conv(24, 32)

1283
conv(1, 8)
maxpool(2)
conv(8, 16)
maxpool(2)
conv(16, 24)
maxpool(2)
conv(24, 32)
maxpool(2)
conv(32, 40)

2563
conv(1, 8)
maxpool(2)
conv(8, 16)
maxpool(2)
conv(16, 24)
maxpool(2)
conv(24, 32)
maxpool(2)
conv(32, 40)
maxpool(2)
conv(40, 48)

Dropout(0.5)
fully-connected(1024)
fully-connected(10)
SoftMax

Table 2: Network Architectures ModelNet10 Classiﬁcation: OctNet1

Table 3: Network Architectures ModelNet10 Classiﬁcation: OctNet2

83
conv(1, 8)
conv(8, 8)

163
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)

83
conv(1, 8)
conv(8, 8)
conv(8, 8)

163
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)

1283
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
maxpool(2)
conv(32, 40)
conv(40, 40)

2563
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
maxpool(2)
conv(32, 40)
conv(40, 40)
maxpool(2)
conv(40, 48)
conv(48, 48)

1283
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
conv(32, 32)
maxpool(2)
conv(32, 40)
conv(40, 40)
conv(40, 40)

2563
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
conv(32, 32)
maxpool(2)
conv(32, 40)
conv(40, 40)
conv(40, 40)
maxpool(2)
conv(40, 48)
conv(48, 48)
conv(48, 48)

323
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)

643
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)

Dropout(0.5)
fully-connected(1024)
fully-connected(10)
SoftMax

323
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
conv(24, 24)

643
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
conv(32, 32)

Dropout(0.5)
fully-connected(1024)
fully-connected(10)
SoftMax

Table 4: Network Architectures ModelNet10 Classiﬁcation: OctNet3.

83
conv(1, 8)
conv(8, 14)

163
conv(1, 8)
conv(8, 14)

323
conv(1, 8)
conv(8, 14)

643
conv(1, 8)
conv(8, 14)

1283
conv(1, 8)
conv(8, 14)

conv(14, 14)
conv(14, 20)

conv(14, 14)
conv(14, 20)

conv(14, 14)
conv(14, 20)

conv(14, 14)
conv(14, 20)

conv(20, 20)
conv(20, 26)

conv(20, 20)
conv(20, 26)

conv(20, 20)
conv(20, 26)

conv(26, 26)
conv(26, 32)

conv(26, 26)
conv(26, 32)

conv(32, 32)
conv(32, 32)

conv(32, 32)
conv(32, 32)
maxpool(2)

conv(26, 26)
conv(26, 32)
maxpool(2)
conv(32, 32)
conv(32, 32)
maxpool(2)

conv(20, 20)
conv(20, 26)
maxpool(2)
conv(26, 26)
conv(26, 32)
maxpool(2)
conv(32, 32)
conv(32, 32)
maxpool(2)

conv(14, 14)
conv(14, 20)
maxpool(2)
conv(20, 20)
conv(20, 26)
maxpool(2)
conv(26, 26)
conv(26, 32)
maxpool(2)
conv(32, 32)
conv(32, 32)
maxpool(2)

2563
conv(1, 8)
conv(8, 14)
maxpool(2)
conv(14, 14)
conv(14, 20)
maxpool(2)
conv(20, 20)
conv(20, 26)
maxpool(2)
conv(26, 26)
conv(26, 32)
maxpool(2)
conv(32, 32)
conv(32, 32)
maxpool(2)

Dropout(0.5)
fully-connected(512)
fully-connected(10)
SoftMax

Table 5: Network Architectures ModelNet10 Classiﬁcation.

83
conv(1, 8)

163
conv(1, 8)
maxpool(2)
conv(8, 16)

323
conv(1, 8)
maxpool(2)
conv(8, 16)
maxpool(2)
conv(16, 24)

643
conv(1, 8)
maxpool(2)
conv(8, 16)
maxpool(2)
conv(16, 24)
maxpool(2)
conv(24, 32)

1283
conv(1, 8)
maxpool(2)
conv(8, 16)
maxpool(2)
conv(16, 24)
maxpool(2)
conv(24, 32)
maxpool(2)
conv(32, 40)

2563
conv(1, 8)
maxpool(2)
conv(8, 16)
maxpool(2)
conv(16, 24)
maxpool(2)
conv(24, 32)
maxpool(2)
conv(32, 40)
maxpool(2)
conv(40, 48)

Dropout(0.5)
fully-connected(1024)
fully-connected(4)
Normalize

Table 6: Network Architectures Orientation Estimation: OctNet1

Table 7: Network Architectures Orientation Estimation: OctNet2

83
conv(1, 8)
conv(8, 8)

163
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)

83
conv(1, 8)
conv(8, 8)
conv(8, 8)

163
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)

1283
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
maxpool(2)
conv(32, 40)
conv(40, 40)

2563
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
maxpool(2)
conv(32, 40)
conv(40, 40)
maxpool(2)
conv(40, 48)
conv(48, 48)

1283
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
conv(32, 32)
maxpool(2)
conv(32, 40)
conv(40, 40)
conv(40, 40)

2563
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
conv(32, 32)
maxpool(2)
conv(32, 40)
conv(40, 40)
conv(40, 40)
maxpool(2)
conv(40, 48)
conv(48, 48)
conv(48, 48)

323
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)

643
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)

Dropout(0.5)
fully-connected(1024)
fully-connected(4)
Normalize

323
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
conv(24, 24)

643
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
conv(32, 32)

Dropout(0.5)
fully-connected(1024)
fully-connected(4)
Normalize

Table 8: Network Architectures Orientation Estimation: OctNet3.

83
conv(1, 8)
conv(8, 14)

163
conv(1, 8)
conv(8, 14)

323
conv(1, 8)
conv(8, 14)

643
conv(1, 8)
conv(8, 14)

1283
conv(1, 8)
conv(8, 14)

conv(14, 14)
conv(14, 20)

conv(14, 14)
conv(14, 20)

conv(14, 14)
conv(14, 20)

conv(14, 14)
conv(14, 20)

conv(20, 20)
conv(20, 26)

conv(20, 20)
conv(20, 26)

conv(20, 20)
conv(20, 26)

conv(26, 26)
conv(26, 32)

conv(26, 26)
conv(26, 32)

conv(32, 32)
conv(32, 32)

conv(32, 32)
conv(32, 32)
maxpool(2)

conv(26, 26)
conv(26, 32)
maxpool(2)
conv(32, 32)
conv(32, 32)
maxpool(2)

conv(20, 20)
conv(20, 26)
maxpool(2)
conv(26, 26)
conv(26, 32)
maxpool(2)
conv(32, 32)
conv(32, 32)
maxpool(2)

conv(14, 14)
conv(14, 20)
maxpool(2)
conv(20, 20)
conv(20, 26)
maxpool(2)
conv(26, 26)
conv(26, 32)
maxpool(2)
conv(32, 32)
conv(32, 32)
maxpool(2)

2563
conv(1, 8)
conv(8, 14)
maxpool(2)
conv(14, 14)
conv(14, 20)
maxpool(2)
conv(20, 20)
conv(20, 26)
maxpool(2)
conv(26, 26)
conv(26, 32)
maxpool(2)
conv(32, 32)
conv(32, 32)
maxpool(2)

Dropout(0.5)
fully-connected(512)
fully-connected(4)
Normalize

Table 9: Network Architectures Orientation Estimation.

Output name

Enc1

Enc2

Enc3

Enc4

Dec4

Dec3

Dec2

Dec1

Output

Operation
conv(8, 8)
conv(8, 16)
maxpool(2)
conv(16, 16)
conv(16, 32)
maxpool(2)
conv(32, 32)
conv(32, 64)
maxpool(2)
conv(64, 64)
conv(64, 128)
maxpool(2)
conv(128, 128)
conv(128, 128)
conv(128, 128)
unpool(2)
concat(Enc4,Dec4)
conv(256, 128)
conv(128, 64)
unpool(2)
concat(Enc3,Dec3)
conv(128, 64)
conv(64, 32)
unpool(2)
concat(Enc2,Dec2)
conv(64, 32)
conv(32, 16)
unpool(2)
concat(Enc1,Dec1)
conv(32, 32)
conv(32, 8)
SoftMax

Table 10: Network Architecture Semantic 3D Point Cloud Labeling.

OctNet: Learning Deep 3D Representations at High Resolutions

Gernot Riegler1

Ali Osman Ulusoy2
1Institute for Computer Graphics and Vision, Graz University of Technology
2Autonomous Vision Group, MPI for Intelligent Systems T¨ubingen
3Computer Vision and Geometry Group, ETH Z¨urich

Andreas Geiger2,3

riegler@icg.tugraz.at

{osman.ulusoy,andreas.geiger}@tue.mpg.de

7
1
0
2
 
r
p
A
 
0
1
 
 
]

V
C
.
s
c
[
 
 
4
v
9
0
0
5
0
.
1
1
6
1
:
v
i
X
r
a

Abstract

We present OctNet, a representation for deep learning
with sparse 3D data. In contrast to existing models, our rep-
resentation enables 3D convolutional networks which are
both deep and high resolution. Towards this goal, we ex-
ploit the sparsity in the input data to hierarchically parti-
tion the space using a set of unbalanced octrees where each
leaf node stores a pooled feature representation. This al-
lows to focus memory allocation and computation to the
relevant dense regions and enables deeper networks without
compromising resolution. We demonstrate the utility of our
OctNet representation by analyzing the impact of resolution
on several 3D tasks including 3D object classiﬁcation, ori-
entation estimation and point cloud labeling.

1. Introduction

Over the last several years, convolutional networks have
lead to substantial performance gains in many areas of com-
puter vision. In most of these cases, the input to the network
is of two-dimensional nature, e.g., in image classiﬁcation
[19], object detection [36] or semantic segmentation [14].
However, recent advances in 3D reconstruction [34] and
graphics [22] allow capturing and modeling large amounts
of 3D data. At the same time, large 3D repositories such as
ModelNet [48], ShapeNet [6] or 3D Warehouse1 as well as
databases of 3D object scans [7] are becoming increasingly
available. These factors have motivated the development of
convolutional networks that operate on 3D data.

Most existing 3D network architectures [8,30,35,48] re-
place the 2D pixel array by its 3D analogue, i.e., a dense and
regular 3D voxel grid, and process this grid using 3D con-
volution and pooling operations. However, for dense 3D
data, computational and memory requirements grow cubi-
cally with the resolution. Consequently, existing 3D net-
works are limited to low 3D resolutions, typically in the
order of 303 voxels. To fully exploit the rich and detailed

1https://3dwarehouse.sketchup.com

t
e
N
v
n
o
C
D
3

e
s
n
e
D

t
e
N
v
n
o
C
D
3

e
s
n
e
D

t
e
N
t
c
O

1

(a) Layer 1: 323

(b) Layer 2: 163

(c) Layer 3: 83

Figure 1: Motivation.
For illustration purposes, we
trained a dense convolutional network to classify 3D shapes
from [48]. Given a voxelized bed as input, we show the
maximum response across all feature maps at intermediate
layers (a-c) of the network before pooling. Higher activa-
tions are indicated with darker colors. Voxels with zero
activation are not displayed. The ﬁrst row visualizes the
responses in 3D while the second row shows a 2D slice.
Note how voxels close to the object contour respond more
strongly than voxels further away. We exploit the sparsity
in our data by allocating memory and computations using a
space partitioning data structure (bottom row).

geometry of our 3D world, however, much higher resolution
networks are required.

In this work, we build on the observation that 3D data is
often sparse in nature, e.g., point clouds, or meshes, result-
ing in wasted computations when applying 3D convolutions

na¨ıvely. We illustrate this in Fig. 1 for a 3D classiﬁcation
example. Given the 3D meshes of [48] we voxelize the input
at a resolution of 643 and train a simple 3D convolutional
network to minimize a classiﬁcation loss. We depict the
maximum of the responses across all feature maps at dif-
ferent layers of the network. It is easy to observe that high
activations occur only near the object boundaries.

Motivated by this observation, we propose OctNet, a 3D
convolutional network that exploits this sparsity property.
Our OctNet hierarchically partitions the 3D space into a set
of unbalanced octrees [32]. Each octree splits the 3D space
according to the density of the data. More speciﬁcally, we
recursively split octree nodes that contain a data point in its
domain, i.e., 3D points, or mesh triangles, stopping at the
ﬁnest resolution of the tree. Therefore, leaf nodes vary in
size, e.g., an empty leaf node may comprise up to 83 = 512
voxels for a tree of depth 3 and each leaf node in the octree
stores a pooled summary of all feature activations of the
voxel it comprises. The convolutional network operations
are directly deﬁned on the structure of these trees. There-
fore, our network dynamically focuses computational and
memory resources, depending on the 3D structure of the in-
put. This leads to a signiﬁcant reduction in computational
and memory requirements which allows for deep learning
at high resolutions. Importantly, we also show how essen-
tial network operations (convolution, pooling or unpooling)
can be efﬁciently implemented on this new data structure.

We demonstrate the utility of the proposed OctNet on
three different problems involving three-dimensional data:
3D classiﬁcation, 3D orientation estimation of unknown
object instances and semantic segmentation of 3D point
clouds. In particular, we show that the proposed OctNet en-
ables signiﬁcant higher input resolutions compared to dense
inputs due to its lower memory consumption, while achiev-
ing identical performance compared to the equivalent dense
network at lower resolutions. At the same time we gain sig-
niﬁcant speed-ups at resolutions of 1283 and above. Using
our OctNet, we investigate the impact of high resolution in-
puts wrt. accuracy on the three tasks and demonstrate that
higher resolutions are particularly beneﬁcial for orientation
estimation and semantic point cloud labeling. Our code is
available from the project website2.

2. Related Work

While 2D convolutional networks have proven very suc-
cessful in extracting information from images [12, 14, 19,
36, 42, 43, 47, 49, 50], there exists comparably little work
on processing three-dimensional data. In this Section, we
review existing work on dense and sparse models.

Dense Models: Wu et al. [48] trained a deep belief network
on shapes discretized to a 303 voxel grid for object classi-

2https://github.com/griegler/octnet

ﬁcation, shape completion and next best view prediction.
Maturana et al. [30] proposed VoxNet, a feed-forward con-
volutional network for classifying 323 voxel volumes from
RGB-D data. In follow-up work, Sedaghat et al. [1] showed
that introducing an auxiliary orientation loss increases clas-
siﬁcation performance over the original VoxNet. Similar
models have also been exploited for semantic point cloud
labeling [21] and scene context has been integrated in [52].
Recently, generative models [37] and auto-encoders [5,
40] have demonstrated impressive performance in learning
low-dimensional object representations from collections of
low-resolution (323) 3D shapes.
Interestingly, these low-
dimensional representations can be directly inferred from a
single image [15] or a sequence of images [8].

Due to computational and memory limitations, all afore-
mentioned methods are only able to process and generate
shapes at a very coarse resolution, typically in the order of
303 voxels. Besides, when high-resolution outputs are de-
sired, e.g., for labeling 3D point clouds, inefﬁcient sliding-
window techniques with a limited receptive ﬁeld must be
adopted [21]. Increasing the resolution na¨ıvely [33, 41, 53]
reduces the depth of the networks and hence their expres-
siveness. In contrast, the proposed OctNets allow for train-
ing deep architectures at signiﬁcant higher resolutions.

Sparse Models: There exist only few network architec-
tures which explicitly exploit sparsity in the data. As these
networks do not require exhaustive dense convolutions they
have the potential of handling higher resolutions.

Engelcke et al. [10] proposed to calculate convolutions
at sparse input locations by pushing values to their target
locations. This has the potential to reduce the number of
convolutions but does not reduce the amount of memory re-
quired. Consequently, their work considers only very shal-
low networks with up to three layers.

A similar approach is presented in [16, 17] where sparse
convolutions are reduced to matrix operations. Unfortu-
nately, the model only allows for 2
2 convolutions and
results in indexing and copy overhead which prevents pro-
cessing volumes of larger resolution (the maximum reso-
lution considered in [16, 17] is 803 voxels). Besides, each
layer decreases sparsity and thus increases the number of
operations, even at a single resolution. In contrast, the num-
ber of operations remains constant in our model.

×

Li et al. [28] proposed ﬁeld probing networks which
sample 3D data at sparse points before feeding them into
fully connected layers. While this reduces memory and
computation, it does not allow for exploiting the distributed
computational power of convolutional networks as ﬁeld
probing layers can not be stacked, convolved or pooled.

Jampani et al. [23] introduced bilateral convolution lay-
ers (BCL) which map sparse inputs into permutohedral
space where learnt convolutional ﬁlters are applied. Their
work is related to ours with respect to efﬁciently exploiting

the sparsity in the input data. However, in contrast to BCL
our method is speciﬁcally targeted at 3D convolutional net-
works and can be immediately dropped in as a replacement
in existing network architectures.

3. Octree Networks

To decrease the memory footprint of convolutional net-
works operating on sparse 3D data, we propose an adaptive
space partitioning scheme which focuses computations on
the relevant regions. As mathematical operations of deep
networks, especially convolutional networks, are best un-
derstood on regular grids, we restrict our attention to data
structures on 3D voxel grids. One of the most popular space
partitioning structures on voxel grids are octrees [31] which
have been widely adopted due to their ﬂexible and hier-
archical structure. Areas of application include depth fu-
sion [24], image rendering [27] and 3D reconstruction [45].
In this paper, we propose 3D convolutional networks on oc-
trees to learn representations from high resolution 3D data.
An octree partitions the 3D space by recursively subdi-
viding it into octants. By subdividing only the cells which
contain relevant information (e.g., cells crossing a surface
boundary or cells containing one or more 3D points) storage
can be allocated adaptively. Densely populated regions are
modeled with high accuracy (i.e., using small cells) while
empty regions are summarized by large cells in the octree.
Unfortunately, vanilla octree implementations [31] have
several drawbacks that hamper its application in deep net-
works. While octrees reduce the memory footprint of the
3D representation, most versions do not allow for efﬁcient
access to the underlying data. In particular, octrees are typi-
cally implemented using pointers, where each node contains
a pointer to its children. Accessing an arbitrary element (or
the neighbor of an element) in the octree requires a traversal
starting from the root until the desired cell is reached. Thus,
the number of memory accesses is equal to the depth of the
tree. This becomes increasingly costly for deep, i.e., high-
resolution, octrees. Convolutional network operations such
as convolution or pooling require frequent access to neigh-
boring elements. It is thus critical to utilize an octree design
that allows for fast data access.

We tackle these challenges by leveraging a hybrid grid-
octree data structure which we describe in Section 3.1. In
Section 3.2, we show how 3D convolution and pooling oper-
ations can be implemented efﬁciently on this data structure.

3.1. Hybrid Grid-Octree Data Structure

The above mentioned problems with the vanilla octree
data structure increase with the octree depth. Instead of rep-
resenting the entire high resolution 3D input with a single
unbalanced octree, we leverage a hybrid grid-octree struc-
ture similar to the one proposed by Miller et al. [32]. The
key idea is to restrict the maximal depth of an octree to a

Figure 2: Hybrid Grid-Octree Data Structure. This ex-
ample illustrates a hybrid grid-octree consisting of 8 shal-
low octrees indicated by different colors. Using 2 shallow
octrees in each dimension with a maximum depth of 3 leads
to a total resolution of 163 voxels.

(a) Shallow Octree

(b) Bit-Representation

Figure 3: Bit Representation. Shallow octrees can be
efﬁciently encoded using bit-strings. Here, the bit-string
1 01010000 00000000 01010000 00000000 01010000 0...
deﬁnes the octree in (a). The corresponding tree is shown
in (b). The color of the voxels corresponds to the split level.

small number, e.g., three, and place several such shallow
octrees along a regular grid (Fig. 2). While this data struc-
ture may not be as memory efﬁcient as the standard octree,
signiﬁcant compression ratios can still be achieved. For in-
stance, a single shallow octree that does not contain input
data stores only a single vector, instead of 83 = 512 vectors
for all voxels at the ﬁnest resolution at depth 3.

An additional beneﬁt of a collection of shallow octrees
is that their structure can be encoded very efﬁciently us-
ing a bit string representation which further lowers access
time and allows for efﬁcient GPGPU implementations [32].
Given a shallow octree of depth 3, we use 73 bit to repre-
sent the complete tree. The ﬁrst bit with index 0 indicates,
if the root node is split, or not. Further, bits 1 to 8 indicate
if one of the child nodes is subdivided and bits 9 to 72 de-
note splits of the grandchildren, see Fig. 3. A tree depth of
3 gives a good trade-off between memory consumption and
computational efﬁciency. Increasing the octree depth results
in an exponential growth in the required bits to store the tree
structure and further increases the cell traversal time.

Using this bit-representation, a single voxel in the shal-

low octree is fully characterised by its bit index. This index
determines the depth of the voxel in the octree and therefore
also the voxel size. Instead of using pointers to the parent
and child nodes, simple arithmetic can be used to retrieve
the corresponding indices of a voxel with bit index i:

(cid:22) i

,

(cid:23)

1

−
8
i + 1 .

pa(i) =

ch(i) = 8

·

(1)

(2)

In contrast to [32], we associate a data container (for storing
features vectors) with all leaf nodes of each shallow tree.
We allocate the data of a shallow octree in a contiguous data
array. The offset associated with a particular voxel in this
array can be computed as follows:

data idx(i) = 8

bit(j) + 1

bit(j)

pa(i)−1
(cid:88)

j=0

(cid:124)

(cid:123)(cid:122)
#nodes above i

+ mod (i
−
(cid:123)(cid:122)
offset

(cid:124)

1, 8)
(cid:125)

i−1
(cid:88)

−

(cid:125)

.

j=0
(cid:124)
(cid:123)(cid:122)
(cid:125)
#split nodes pre i

(3)

Here, mod denotes the modulo operator and bit returns the
tree bit-string value at i. See supp. document for an exam-
ple. Both sum operations can be efﬁciently implemented
using bit counting intrinsics (popcnt). The data arrays of
all shallow octrees are concatenated into a single contiguous
data array during training and testing to reduce I/O latency.

3.2. Network Operations

Given the hybrid grid-octree data structure introduced in
the previous Section, we now discuss the efﬁcient imple-
mentation of network operations on this data structure. We
will focus on the most common operations in convolutional
networks [14, 19, 36]: convolution, pooling and unpooling.
Note that point-wise operations, like activation functions,
do not differ in their implementation as they are indepen-
dent of the data structure.

×

H

Let us ﬁrst introduce the notation which will be used
throughout this Section. Ti,j,k denotes the value of a 3D
tensor T at location (i, j, k). Now assume a hybrid grid-
octree structure with D
W unbalanced shallow oc-
×
trees of maximum depth 3. Let O[i, j, k] denote the value
of the smallest cell in this structure which comprises the
voxel (i, j, k). Note that in contrast to the tensor notation,
O[i1, j1, k1] and O[i2, j2, k2] with i1
=
k2 may refer to the same voxel in the hybrid grid-octree,
depending on the size of the voxels. We obtain the index
of the shallow octree in the grid via (
) and the
local index of the voxel at the ﬁnest resolution in that octree
by (mod (i, 8), mod (j, 8), mod (k, 8)).

= j2

= i2

k
8 (cid:99)

j
8 (cid:99)

i
8 (cid:99)

k1

j1

∨

∨

(cid:98)

(cid:98)

(cid:98)

,

,

(a) Standard Convolution

(b) Efﬁcient Convolution

Figure 4: Convolution. This ﬁgure illustrates the convolu-
tion of a 33 kernel (red) with a 83 grid-octree cell (black).
Only 2 of the 3 dimensions are shown. A na¨ıve implemen-
tation evaluates the kernel at every location (i, j, k) within
a grid-octree cell as shown in (a). This results in
14k mul-
In contrast, (b) depicts our
tiplications for this example.
efﬁcient implementation of the same operation which re-
3k multiplications. As all 83 voxels inside the
quires only
grid-octree cell are the same value, the convolution kernel
inside the cell needs to be evaluated only once. Voxels at the
cell boundary need to integrate information from neighbor-
ing cells. This can be efﬁciently implemented by summing
truncated kernels. See our supp. document for details.

∼

∼

Given this notation, the mapping from a grid-octree O to

a tensor T with compatible dimensions is given by

oc2ten : Ti,j,k = O[i, j, k] .

Similarly, the reverse mapping is given by

ten2oc : O[i, j, k] = pool voxels
(¯i,¯j,¯k)∈Ω[i,j,k]

(T¯i,¯j,¯k) ,

(4)

(5)

·

where pool voxels (
) is a pooling function (e.g., average-
or max-pooling) which pools all voxels in T over the small-
est grid-octree cell comprising location (i, j, k), denoted by
Ω[i, j, k]. This pooling is necessary as a single voxel in O
can cover up to 83 = 512 elements of T , depending on its
size
Remark: With the two functions deﬁned above, we could
wrap any network operation f deﬁned on 3D tensors via

Ω[i, j, k]
|

|

.

g(O) = ten2oc(f (oc2ten(O))) .

(6)

However, this would require a costly conversion from the
memory efﬁcient grid-octrees to a regular 3D tensor and
back. Besides, storing a dense tensor in memory limits the
maximal resolution. We therefore deﬁne our network oper-
ations directly on the hybrid grid-octree data structure.

Convolution The convolution operation is the most im-
portant, but also the most computational expensive opera-
tion in deep convolutional networks. For a single feature

(a) Input

(b) Output

(a) Input

(b) Output

Figure 5: Pooling. The 23 pooling operation on the grid-
octree structure combines 8 neighbouring shallow octrees
(a) into one shallow octree (b). The size of each voxel is
halved and copied to the new shallow octree structure. Vox-
els on the ﬁnest resolution are pooled. Different shallow
octrees are depicted in different colors.

Figure 6: Unpooling. The 23 unpooling operation trans-
forms a single shallow octree of depth d as shown in (a)
into 8 shallow octrees of depth d
1, illustrated in (b). For
each node at depth zero one shallow octree is spawned. All
other voxels double in size. Different shallow octrees are
depicted in different colors.

−

map, convolving a 3D tensor T with a 3D convolution ker-
nel W

RL×M ×N can be written as

∈

T out
i,j,k =

L−1
(cid:88)

M −1
(cid:88)

N −1
(cid:88)

l=0

m=0

n=0

Wl,m,n

T in
ˆi,ˆj,ˆk

,

·

(7)

l +

n +
. Similarly, the convolutions on the grid-octree data

M/2
(cid:99)
(cid:98)

L/2
(cid:99)

m +

−

−

−

(cid:98)

, ˆj = j

, ˆk = k

with ˆi = i
N/2
(cid:98)
(cid:99)
structure are deﬁned as

Oout[i, j, k] = pool voxels
(¯i,¯j,¯k)∈Ω[i,j,k]

(T¯i,¯j,¯k)

(8)

Ti,j,k =

L−1
(cid:88)

M −1
(cid:88)

N −1
(cid:88)

l=0

m=0

n=0

Wl,m,n

Oin[ˆi, ˆj, ˆk] .

·

While this calculation yields the same result as the tensor
convolution in Eq. (7) with the oc2ten, ten2oc wrapper, we
are now able to deﬁne a computationally more efﬁcient con-
volution operator. Our key observation is that for small con-
volution kernels and large voxels, Ti,j,k is constant within
a small margin of the voxel due to its constant support
Oin[ˆi, ˆj, ˆk]. Thus, we only need to compute the convolu-
tion within the voxel once, followed by convolution along
the surface of the voxel where the support changes due to
adjacent voxels taking different values (Fig. 4). This mini-
mizes the number of calculations by a factor of 4 for voxels
of size 83, see supp. material for a detailed derivation. At
the same time, it enables a better caching mechanism.

Pooling Another important operation in deep convolu-
tional networks is pooling. Pooling reduces the spatial res-
olution of the input tensor and aggregates higher-level in-
formation for further processing, thereby increasing the re-
ceptive ﬁeld and capturing context. For instance, strided
23 max-pooling divides the input tensor T in into 23 non-
overlapping regions and computes the maximum value

within each region. Formally, we have

T out
i,j,k = max

l,m,n∈[0,1]

(cid:0)T in

2i+l,2j+m,2k+n

(cid:1) ,

(9)

∈

∈

where T in

R2D×2H×2W and T out

RD×H×W .

To implement pooling on the grid-octree data structure
we reduce the number of shallow octrees. For an input grid-
octree Oin with 2D
2W shallow octrees, the output
×
Oout contains D
W shallow octrees. Each voxel
of Oin is halved in size and copied one level deeper in the
shallow octree. Voxels at depth 3 in Oin are pooled. This
can be formalized as

×
H

2H

×

×

(cid:40)

Oout[i, j, k] =

Oin[2i, 2j, 2k]
P

if vxd(2i, 2j, 2k) < 3
else

P = max

l,m,n∈[0,1]

(Oin[2i + l, 2j + m, 2k + n]) ,

(10)

where vxd(
) computes the depth of the indexed voxel in
·
the shallow octree. A visual example is depicted in Fig. 5.

Unpooling For several tasks such as semantic segmenta-
tion, the desired network output is of the same size as the
network input. While pooling is crucial to increase the re-
ceptive ﬁeld size of the network and capture context, it loses
spatial resolution. To increase the resolution of the net-
work, U-shaped network architectures have become pop-
ular [2, 53] which encode information using pooling oper-
ations and increase the resolution in a decoder part using
unpooling or deconvolution layers [51], possibly in com-
bination with skip-connections [9, 19] to increase preci-
sion. The simplest unpooling strategy uses nearest neigh-
bour interpolation and can be formalized on dense input
R2D×2H×2W tensors
T in
∈
as follows:

RD×H×W and output T out

∈

i,j,k = T in
T out

(cid:98)i/2(cid:99),(cid:98)j/2(cid:99),(cid:98)k/2(cid:99) .

(11)

Again, we can deﬁne the analogous operation on the hybrid
grid-octree data structure by

,

,

(cid:98)

k/2

(12)

] .
(cid:99)

i/2
(cid:99)

j/2
(cid:99)
(cid:98)

Oout[i, j, k] = Oin[
(cid:98)
This operation also changes the data structure: The number
of shallow octrees increases by a factor of 8, as each node
at depth 0 spawns a new shallow octree. All other nodes
double their size. Thus, after this operation the tree depth is
decreased. See Fig. 6 for a visual example of this operation.
Remark: To capture ﬁne details, voxels can be split again
at the ﬁnest resolution according to the original octree of
the corresponding pooling layer. This allows us to take full
advantage of skip connections. We follow this approach in
our semantic 3D point cloud labeling experiments.

4. Experimental Evaluation

In this Section we leverage our OctNet representation to
investigate the impact of input resolution on three different
3D tasks: 3D shape classiﬁcation, 3D orientation estimation
and semantic segmentation of 3D point clouds. To isolate
the effect of resolution from other factors we consider sim-
ple network architectures. Orthogonal techniques like data
augmentation, joint 2D/3D modeling or ensemble learning
are likely to further improve the performance of our models.

Implementation We implemented the grid-octree data
structure, all layers including the necessary forward and
backward functions, as well as utility methods to create the
data structure from point clouds and meshes, as a stand-
alone C++/CUDA library. This allows the usage of our code
within all existing deep learning frameworks. For our ex-
perimental evaluation we used the Torch3 framework.

4.1. 3D Classiﬁcation

We use the popular ModelNet10 dataset [48] for the 3D
shape classiﬁcation task. The dataset contains 10 shape cat-
egories and consists of 3991 3D shapes for training and 908
3D shapes for testing. Each shape is provided as a trian-
gular mesh, oriented in a canonical pose. We convert the
triangle meshes to dense respective grid-octree occupancy
grids, where a voxel is set to 1 if it intersects the mesh. We
P )3 voxels,
scale each mesh to ﬁt into a 3D grid of (N
where N is the number of voxels in each dimension of the
input grid and P = 2 is a padding parameter.

−

We ﬁrst study the inﬂuence of the input resolution on
memory usage, runtime and classiﬁcation accuracy. To-
wards this goal, we create a series of networks of different
input resolution from 83 to 2563 voxels. Each network con-
sists of several blocks which reduce resolution by half until
we reach a resolution of 83. Each block comprises two con-
volutional layers (33 ﬁlters, stride 1) and one max-pooling

3http://torch.ch/

(a) Memory

(b) Runtime

(c) Accuracy

(d) Accuracy

Figure 7: Results on ModelNet10 Classiﬁcation Task.

layer (23 ﬁlters, stride 2). The number of feature maps in the
ﬁrst block is 8 and increases by 6 with every block. After the
last block we add a fully-connected layer with 512 units and
a ﬁnal output layer with 10 units. Each convolutional layer
and the ﬁrst fully-connected layer are followed by a recti-
ﬁed linear unit [26] as activation function and the weights
are initialized as described in [18]. We use the standard
cross-entropy loss for training and train all networks for 20
epochs with a batch size of 32 using Adam [25]. The initial
learning rate is set to 0.001 and we decrease the learning
rate by a factor of 10 after 15 epochs.

Overall, we consider three different types of networks:
the original VoxNet architecture of Maturana et al. [30]
which operates on a ﬁxed 323 voxel grid, the proposed Oct-
Net and a dense version of it which we denote “DenseNet”
in the following. While performance gains can be ob-
tained using orthogonal approaches such as network ensem-
bles [5] or a combination of 3D and 2D convolutional net-
works [20,42], in this paper we deliberately focus on “pure”
3D convolutional network approaches to isolate the effect of
resolution from other inﬂuencing factors.

Fig. 7 shows our results. First, we compare the mem-
ory consumption and run-time of our OctNet wrt. the dense
baseline approach, see Fig. 7a and 7b. Importantly, OctNets
require signiﬁcantly less memory and run-time for high in-
put resolutions compared to dense input grids. Using a
batch size of 32 samples, our OctNet easily ﬁts in a modern
GPU’s memory (12GB) for an input resolution of 2563. In
contrast, the corresponding dense model ﬁts into the mem-
643. A more detailed analysis of
ory only for resolutions
the memory consumption wrt. the sparsity in the data is pro-
vided in the supp. document. OctNets also run faster than
their dense counterparts for resolutions >643. For resolu-
643, OctNets run slightly slower due to the overhead
tions
incurred by the grid-octree representation and processing.

≤

≤

3
8

3
6
1

3
2
3

3
4
6

83

323

Figure 9: Confusion Matrices on ModelNet10.

Bathtub

Bed

Dresser

N. Stand

(a) Mean Angular Error

(b) Mean Angular Error

Figure 8: Voxelized 3D Shapes from ModelNet10.

Figure 10: Orientation Estimation on ModelNet10.

Leveraging our OctNets, we now compare the impact
of input resolution with respect to classiﬁcation accuracy.
Fig. 7c shows the results of different OctNet architectures
where we keep the number of convolutional layers per block
ﬁxed to 1, 2 and 3. Fig. 7d shows a comparison of accuracy
with respect to DenseNet and VoxNet when keeping the ca-
pacity of the model, i.e., the number of parameters, constant
by removing max-pooling layers from the beginning of the
network. We ﬁrst note that despite its pooled representation,
OctNet performs on par with its dense equivalent. This con-
ﬁrms our initial intuition (Fig. 1) that sparse data allows for
allocating resources adaptively without loss in performance.
Furthermore, both models outperform the shallower VoxNet
architecture, indicating the importance of network depth.

Regarding classiﬁcation accuracy we observed improve-
ments for lower resolutions but diminishing returns beyond
an input resolution of 323 voxels. Taking a closer look at
the confusion matrices in Fig. 9, we observe that higher in-
put resolution helps for some classes, e.g., bathtub, while
others remain ambiguous independently of the resolution,
e.g., dresser vs. night stand. We visualize this lack of dis-
criminative power by showing voxelized representations of
3D shapes from the ModelNet10 database Fig. 8. While
bathtubs look similar to beds (or sofas, tables) at low reso-
lution they can be successfully distinguished at higher reso-
lutions. However, a certain ambiguity between dresser and
night stand remains.

4.2. 3D Orientation Estimation

In this Section, we investigate the importance of input
resolution on 3D orientation estimation. Most existing ap-
proaches to 3D pose estimation [3, 4, 39, 44, 46] assume that
the true 3D shape of the object instance is known. To assess

the generalization ability of 3D convolutional networks, we
consider a slightly different setup where only the object cat-
egory is known. After training a model on a hold-out set of
3D shapes from a single category, we test the ability of the
model to predict the 3D orientation of unseen 3D shapes
from the same category.

−

±

2
(cid:105)

q1, q2

More concretely, given an instance of an object category
with unknown pose, the goal is to estimate the rotation with
respect to the canonical pose. We utilize the 3D shapes from
the chair class of the ModelNet10 dataset and rotate them
15◦ around each axis. We use the same
randomly between
network architectures and training protocol as in the classi-
ﬁcation experiment, except that the networks regress orien-
tations. We use unit quaternions to represent 3D rotations
and train our networks with an Euclidean loss. For small an-
gles, this loss is a good approximation to the rotation angle
φ = arccos(2
(cid:104)

1) between quaternions q1, q2.

Fig. 10 shows our results using the same naming con-
vention as in the previous Section. We observe that ﬁne
details are more important compared to the classiﬁcation
task. For the OctNet 1-3 architectures we observe a steady
increase in performance, while for networks with constant
capacity across resolutions (Fig. 10b), performance levels
beyond 1283 voxels input resolution. Qualitative results of
the latter experiment are shown in Fig. 11. Each row shows
10 different predictions for two randomly selected chair
instance over several input resolutions, ranging from 163
to 1283. Darker colors indicate larger errors which occur
more frequently at lower resolutions. In contrast, predic-
tions at higher network resolutions cluster around the true
pose. Note that learning a dense 3D representation at a res-
olution of 1283 voxels or beyond would not be feasible.

Riemenschneider et al. [38]
Martinovic et al. [29]
Gadde et al. [13]

OctNet 643
OctNet 1283
OctNet 2563

Average Overall

IoU

-
-
68.5

60.0
65.3
73.6

-
-
78.6

73.6
76.1
81.5

42.3
52.2
54.4

45.6
50.4
59.2

Table 1: Semantic Segmentation on RueMonge2014.

(a) Voxelized Input

(b) Voxel Estimates

(c) Estimated Point Cloud (d) Ground Truth Point Cloud
Figure 12: OctNet 2563 Facade Labeling Results.

TP

the test set are assigned the label of the corresponding grid-
octree voxels. As evaluation measures we use overall pixel
TP+FN over all 3D points, average class accu-
accuracy
racy, and intersection over union
TP+FN+FP over all classes.
Here, FP, FN and TP denote false positives, false negatives
and true positives, respectively.

TP

Our results clearly show that increasing the input reso-
lution is essential to obtain state-of-the-art results, as ﬁner
details vanish at coarser resolutions. Qualitative results for
one facade are provided in Fig. 12. Further results are pro-
vided in the supp. document.

5. Conclusion and Future Work

We presented OctNet, a novel 3D representation which
makes deep learning with high-resolution inputs tractable.
We analyzed the importance of high resolution inputs on
several 3D learning tasks, such as object categorization,
pose estimation and semantic segmentation. Our exper-
iments revealed that for ModelNet10 classiﬁcation low-
resolution networks prove sufﬁcient while high input (and
output) resolution matters for 3D orientation estimation and
3D point cloud labeling. We believe that as the community
moves from low resolution object datasets such as Model-
Net10 to high resolution large scale 3D data, OctNet will
enable further improvements. One particularly promising
avenue for future research is in learning representations for
multi-view 3D reconstruction where the ability to process
high resolution voxelized shapes is of crucial importance.

163

323

643

1283

Figure 11: Orientation Estimation on ModelNet10. This
ﬁgure illustrates 10 rotation estimates for 3 chair instances
while varying the input resolution from 163 to 1283. Darker
colors indicate larger deviations from the ground truth.

4.3. 3D Semantic Segmentation

In this Section, we evaluate the proposed OctNets on the
problem of labeling 3D point cloud with semantic informa-
tion. We use the RueMonge2014 dataset [38] that provides
a colored 3D point cloud of several Haussmanian style fa-
cades, comprising
1 million 3D points in total. The labels
are window, wall, balcony, door, roof, sky and shop.

∼

For this task, we train a U-shaped network [2, 53] on
three different input resolutions, 643, 1283 and 2563, where
the voxel size was selected such that the height of all build-
ings ﬁts into the input volume. We ﬁrst map the point cloud
into the grid-octree structure. For all leaf nodes which con-
tain more than one point, we average the input features and
calculate the majority vote of the ground truth labels for
training. As features we use the binary voxel occupancy, the
RGB color, the normal vector and the height above ground.
Due to the small number of training samples, we augment
the data for this task by applying small rotations.

Our network architecture comprises an encoder and a de-
coder part. The encoder part consists of four blocks which
comprise 2 convolution layers (33 ﬁlters, stride 1) followed
by one max-pooling layer each. The decoder consists of
four blocks which comprise 2 convolutions (33 ﬁlters, stride
1) followed by a guided unpooling layer as discussed in the
previous Section. Additionally, after each unpooling step all
features from the last layer of the encoder at the same res-
olution are concatenated to provide high-resolution details.
All networks are trained with a per voxel cross entropy loss
using Adam [25] and a learning rate of 0.0001.

Table 1 compares the proposed OctNet to several state
of the art approaches on the facade labeling task following
the extended evaluation protocol of [13]. The 3D points of

References

[1] N. S. Alvar, M. Zolfaghari, and T. Brox. Orientation-boosted
voxel nets for 3d object recognition. arXiv.org, 1604.03351,
2016. 2

[2] V. Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A
deep convolutional encoder-decoder architecture for image
segmentation. arXiv.org, 1511.00561, 2015. 5, 8, 14

[3] E. Brachmann, A. Krull, F. Michel, S. Gumhold, J. Shotton,
and C. Rother. Learning 6d object pose estimation using
In Proc. of the European Conf. on
3d object coordinates.
Computer Vision (ECCV), 2014. 7

[4] E. Brachmann, F. Michel, A. Krull, M. Y. Yang, S. Gumhold,
and C. Rother. Uncertainty-driven 6d pose estimation of ob-
In Proc. IEEE
jects and scenes from a single rgb image.
Conf. on Computer Vision and Pattern Recognition (CVPR),
2016. 7

[5] A. Brock, T. Lim, J. M. Ritchie, and N. Weston. Generative
and discriminative voxel modeling with convolutional neural
networks. arXiv.org, 1608.04236, 2016. 2, 6

[6] A. X. Chang, T. A. Funkhouser, L. J. Guibas, P. Hanrahan,
Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su,
J. Xiao, L. Yi, and F. Yu. Shapenet: An information-rich 3d
model repository. arXiv.org, 1512.03012, 2015. 1

[7] S. Choi, Q. Zhou, S. Miller, and V. Koltun. A large dataset

of object scans. arXiv.org, 1602.02481, 2016. 1

[8] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese. 3d-
r2n2: A uniﬁed approach for single and multi-view 3d object
reconstruction. In Proc. of the European Conf. on Computer
Vision (ECCV), 2016. 1, 2

[9] A. Dosovitskiy, P. Fischer, E. Ilg, P. Haeusser, C. Hazirbas,
V. Golkov, P. v.d. Smagt, D. Cremers, and T. Brox. Flownet:
Learning optical ﬂow with convolutional networks. In Proc.
of the IEEE International Conf. on Computer Vision (ICCV),
2015. 5

[10] M. Engelcke, D. Rao, D. Z. Wang, C. H. Tong, and I. Posner.
Vote3deep: Fast object detection in 3d point clouds using ef-
ﬁcient convolutional neural networks. arXiv.org, 609.06666,
2016. 2

[11] G. Fanelli, J. Gall, and L. Van Gool. Real time head pose
In Proc. IEEE
estimation with random regression forests.
Conf. on Computer Vision and Pattern Recognition (CVPR),
2011. 13

[12] J. Flynn, I. Neulander, J. Philbin, and N. Snavely. Deep-
stereo: Learning to predict new views from the world’s im-
agery. arXiv.org, 1506.06825, 2015. 2

[13] R. Gadde, V. Jampani, R. Marlet, and P. V. Gehler. Ef-
ﬁcient 2d and 3d facade segmentation using auto-context.
arXiv.org, 1606.06437, 2016. 8

[14] G. Ghiasi and C. C. Fowlkes. Laplacian pyramid reconstruc-
tion and reﬁnement for semantic segmentation. In Proc. of
the European Conf. on Computer Vision (ECCV), 2016. 1, 2,
4

[15] R. Girdhar, D. F. Fouhey, M. Rodriguez, and A. Gupta.
Learning a predictable and generative vector representation
In Proc. of the European Conf. on Computer
for objects.
Vision (ECCV), 2016. 2

[16] B. Graham. Spatially-sparse convolutional neural networks.

arXiv.org, 2014. 2

[17] B. Graham. Sparse 3d convolutional neural networks.

In
Proc. of the British Machine Vision Conf. (BMVC), 2015. 2
[18] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation. In Proc. of the IEEE International Conf. on
Computer Vision (ICCV), 2015. 6

[19] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
In Proc. IEEE Conf. on Computer

for image recognition.
Vision and Pattern Recognition (CVPR), 2016. 1, 2, 4, 5
[20] V. Hegde and R. Zadeh. Fusionnet: 3d object classiﬁcation
using multiple data representations. arXiv.org, 1607.05695,
2016. 6

[21] J. Huang and S. You. Point cloud labeling using 3d convolu-
tional neural network. In Proc. of the International Conf. on
Pattern Recognition (ICPR), 2016. 2

[22] Q. Huang, H. Wang, and V. Koltun. Single-view reconstruc-
In
tion via joint analysis of image and shape collections.
ACM Trans. on Graphics (SIGGRAPH), 2015. 1

[23] V. Jampani, M. Kiefel, and P. V. Gehler. Learning sparse high
dimensional ﬁlters: Image ﬁltering, dense crfs and bilateral
neural networks. In Proc. IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), 2016. 2

[24] W. Kehl, T. Holl, F. Tombari, S. Ilic, and N. Navab. An
octree-based approach towards efﬁcient variational range
data fusion. arXiv.org, 1608.07411, 2016. 3

[25] D. P. Kingma and J. Ba. Adam: A method for stochastic
optimization. In Proc. of the International Conf. on Learning
Representations (ICLR), 2015. 6, 8

[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in Neural Information Processing Systems (NIPS),
2012. 6

[27] S. Laine and T. Karras. Efﬁcient sparse voxel octrees. IEEE
Trans. on Visualization and Computer Graphics (VCG),
17(8):1048–1059, 2011. 3

[28] Y. Li, S. Pirk, H. Su, C. R. Qi, and L. J. Guibas. FPNN: ﬁeld
probing neural networks for 3d data. arXiv.org, 1605.06240,
2016. 2

[29] A. Martinovi´c, J. Knopp, H. Riemenschneider, and L. Van
Gool. 3d all the way: Semantic segmentation of urban scenes
from start to end in 3d. In Proc. IEEE Conf. on Computer
Vision and Pattern Recognition (CVPR), 2015. 8

[30] D. Maturana and S. Scherer. Voxnet: A 3d convolutional
In Proc.
neural network for real-time object recognition.
IEEE International Conf. on Intelligent Robots and Systems
(IROS), 2015. 1, 2, 6

[31] D. Meagher. Geometric modeling using octree encod-
ing. Computer Graphics and Image Processing (CGIP),
19(1):85, 1982. 3

[32] A. Miller, V. Jain, and J. L. Mundy. Real-time rendering
In Proc. of
and dynamic updating of 3-d volumetric data.
the Workshop on General Purpose Processing on Graphics
Processing Units (GPGPU), page 8, 2011. 2, 3, 4

[33] F. Milletari, N. Navab, and S. Ahmadi. V-net: Fully convo-
lutional neural networks for volumetric medical image seg-
mentation. arXiv.org, 1606.04797, 2016. 2

[49] J. Xie, R. B. Girshick, and A. Farhadi. Deep3d: Fully au-
tomatic 2d-to-3d video conversion with deep convolutional
In Proc. of the European Conf. on Com-
neural networks.
puter Vision (ECCV), 2016. 2

[50] A. R. Zamir, T. Wekel, P. Agrawal, C. Wei, J. Malik, and
S. Savarese. Generic 3d representation via pose estimation
and matching. In Proc. of the European Conf. on Computer
Vision (ECCV), 2016. 2

[51] M. D. Zeiler, G. W. Taylor, and R. Fergus. Adaptive decon-
volutional networks for mid and high level feature learning.
In Proc. of the IEEE International Conf. on Computer Vision
(ICCV), 2011. 5

[52] Y. Zhang, M. Bai, P. Kohli, S. Izadi, and J. Xiao. Deepcon-
text: Context-encoding neural pathways for 3d holistic scene
understanding. arXiv.org, 1603.04922, 2016. 2
¨Ozg¨un Cicek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and
O. Ronneberger. 3d u-net: Learning dense volumetric seg-
mentation from sparse annotation. arXiv.org, 1606.06650,
2016. 2, 5, 8, 14

[53]

[34] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux,
D. Kim, A. J. Davison, P. Kohli, J. Shotton, S. Hodges, and
A. Fitzgibbon. Kinectfusion: Real-time dense surface map-
ping and tracking. In Proc. of the International Symposium
on Mixed and Augmented Reality (ISMAR), 2011. 1

[35] C. R. Qi, H. Su, M. Nießner, A. Dai, M. Yan, and L. Guibas.
Volumetric and multi-view cnns for object classiﬁcation on
3d data. In Proc. IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR), 2016. 1

[36] S. Ren, K. He, R. B. Girshick, and J. Sun. Faster R-CNN:
towards real-time object detection with region proposal net-
works. In Advances in Neural Information Processing Sys-
tems (NIPS), 2015. 1, 2, 4

[37] D. J. Rezende, S. M. A. Eslami, S. Mohamed, P. Battaglia,
M. Jaderberg, and N. Heess. Unsupervised learning of 3d
structure from images. arXiv.org, 1607.00662, 2016. 2
[38] H. Riemenschneider, A. B´odis-Szomor´u, J. Weissenberg,
and L. V. Gool. Learning where to classify in multi-view
semantic segmentation. In Proc. of the European Conf. on
Computer Vision (ECCV), 2014. 8, 14

[39] R. Rios-Cabrera and T. Tuytelaars. Discriminatively trained
templates for 3d object detection: A real-time scalable ap-
proach. In Proc. of the IEEE International Conf. on Com-
puter Vision (ICCV), pages 2048–2055, 2013. 7

[40] A. Sharma, O. Grau, and M. Fritz. Vconv-dae: Deep vol-
arXiv.org,

umetric shape learning without object labels.
1604.03755, 2016. 2

[41] S. Song and J. Xiao. Deep sliding shapes for amodal 3d
object detection in RGB-D images. arXiv.org, 1511.02300,
2015. 2

[42] H. Su, S. Maji, E. Kalogerakis, and E. G. Learned-Miller.
Multi-view convolutional neural networks for 3d shape
In Proc. of the IEEE International Conf. on
recognition.
Computer Vision (ICCV), 2015. 2, 6

[43] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Multi-view
3d models from single images with a convolutional network.
In Proc. of the European Conf. on Computer Vision (ECCV),
2016. 2

[44] A. Tejani, D. Tang, R. Kouskouridas, and T. Kim. Latent-
class hough forests for 3d object detection and pose estima-
In Proc. of the European Conf. on Computer Vision
tion.
(ECCV), 2014. 7

[45] A. O. Ulusoy, A. Geiger, and M. J. Black. Towards prob-
abilistic volumetric reconstruction using ray potentials.
In
Proc. of the International Conf. on 3D Vision (3DV), 2015. 3
[46] P. Wohlhart and V. Lepetit. Learning descriptors for object
recognition and 3d pose estimation. In Proc. IEEE Conf. on
Computer Vision and Pattern Recognition (CVPR), 2015. 7
[47] J. Wu, T. Xue, J. J. Lim, Y. Tian, J. B. Tenenbaum, A. Tor-
ralba, and W. T. Freeman. Single image 3d interpreter net-
work. In Proc. of the European Conf. on Computer Vision
(ECCV), 2016. 2

[48] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and
J. Xiao. 3d shapenets: A deep representation for volumetric
shapes. In Proc. IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR), 2015. 1, 2, 6, 12

A. Appendix

A.1. Introduction

A.2. Data Index

In the following supplemental material we present details regarding our operations on the hybrid grid-octree data structure,
additional experimental results and all details on the used network architectures. In Section A.2 we provide details on the
data index used to efﬁciently access data in the grid-octree data structure. Section A.3 yields more insights into the efﬁcient
implementation of the convolution operation on octrees. We present further quantitative and qualitative results in Section A.4
and in Section A.8 we specify all details of the network architectures used in our experiments.

An important implementation detail of the hybrid grid-octree data structure is the memory alignment for fast data access.
We store all data associated with the leaf nodes of a shallow octree in a compact contiguous array. Thus, we need a fast way
to compute the offset in this data array for any given voxel. In the main text we presented the following equation:

(13)

(14)

(15)

data idx(i) = 8

bit(j) + 1

bit(j)

pa(i)−1
(cid:88)

j=0

(cid:124)

(cid:123)(cid:122)
#nodes above i

i−1
(cid:88)

−

(cid:125)

j=0
(cid:123)(cid:122)
(cid:125)
(cid:124)
#split nodes pre i

+ mod (i
−
(cid:123)(cid:122)
offset

(cid:124)

.

1, 8)
(cid:125)

As explained in the main text the whole octree structure is stored as a bit-string and a voxel is uniquely identiﬁed by the bit
index i, i.e., the index within the bit string. The data is aligned breadth-ﬁrst and only the leaf nodes have data associated.
Consequently, the ﬁrst part of the equation above counts the number of split and leaf nodes up to the voxel with bit index i.
The second term subtracts the number of split nodes before the particular voxel as data is only associated with leaf nodes.
Finally, we need to get the offset within the voxel’s neighborhood. This is done by the last term of the equation.

Let us illustrate this with a simple example: For ease of visualization we will consider a quadtree. Hence, each voxel can

be split into 4 instead of 8 children. The equation for the offset changes to

data idx4(i) = 4

bit(j) + 1

bit(j)

pa4(i)−1
(cid:88)

i−1
(cid:88)

j=0

(cid:124)

(cid:123)(cid:122)
#nodes above i

−

(cid:125)

j=0
(cid:125)
(cid:123)(cid:122)
(cid:124)
#split nodes pre i

+ mod (i
−
(cid:123)(cid:122)
offset

(cid:124)

,

1, 4)
(cid:125)

with

pa4(i) =

(cid:22) i

(cid:23)

1

.

−
4

Now consider the following bit string for instance: 1 0101 0000 1001 0000 0100. According to our deﬁnition, this bit string
corresponds to the tree structure visualized in Fig. 13a and 13b, where s indicates a split node and v a leaf node with associated
data. In Fig. 13c we show the bit indices for all nodes. Note that the leaf nodes at depth 3 do not need to be stored in the bit
string as this information is implicit. Finally, the data index for all leaf nodes is visualized in Fig. 13d. Now we can verify
equation (14) using a simple example. Assume the bit index 51: The parent bit index is given by equation (15) as 12. To
compute the data index we ﬁrst count the number of nodes before 49 as it is the ﬁrst node within its siblings (ﬁrst term of
equation), which is 17. Next, we count the number of split nodes up to 49 (second term of equation), which is 6. Finally, we
look up the position 51 within its siblings (last term of equation), which is 2. Combining those three terms yields the data
index 17

6 + 2 = 13.

−

A.3. Efﬁcient Convolution

In the main text we discussed that the convolution for larger octree cells and small convolution kernels can be efﬁ-
ciently implemented. A na¨ıve implementation applies the convolution kernel at every location (i, j, k) comprised by the cell
Ω[i, j, k]. Therefore, for an octree cell of size 83 and a convolution kernel kernel of 33 this would require 83
33 = 13, 824
multiplications. However, we can implement this calculation much more efﬁciently as depicted in Fig. 14. We observe that
the value inside the cell of size 83 is constant. Thus, we only need to evaluate the convolution once inside this cell and
multiply the result with the size of the cell 83, see Fig. 14a. Additionally, we only need to evaluate a truncated versions of the

·

(a) Bit-String

(b) Split and Leaf Nodes

(c) Bit Index

(d) Data Index

Figure 13: Data Index.

(a) Constant

(b) Corners

(c) Edges

(d) Faces

Figure 14: Efﬁcient Convolution.

kernel on the corners, edges and faces of the voxel, see Fig. 14b-d. This implementation is more efﬁcient, as we need only
27 multiplications for the constant part, 8
15 multiplications for the edges, and
19 multiplications for the corners, 12
9 multiplications for the faces of the voxel. In total this yields 3203 multiplications, or 23.17% of the multiplications
6
required by the na¨ıve implementation.

62

6

·

·

·

·

·

In this Section we show additional quantitative and qualitative results for 3D shape classiﬁcation, 3D orientation estimation

A.4. Additional Results

and semantic 3D point labeling.

A.5. 3D Classiﬁcation

In the main text of our work we analyzed the runtime and memory consumption of OctNet compared with the equivalent
dense networks on ModelNet10 [48]. Additionally, we demonstrated that without further data augmentation, ensemble
learning, or more sophisticated architectures the accuracy saturates at an input resolution of about 163, when keeping the
number of network parameters ﬁxed across all resolutions. In this Section we show the same experiment on ModelNet40 [48].
The results are summarized in Fig. 15. In contrast to ModelNet10, we see an increase in accuracy up to an input resolution
of 323. Beyond this resolution the classiﬁcation performance does not further improve. Note that the only form of data
augmentation we used in this experiment was rotation around the up-vector as the 3D models in this dataset vary in pose. We
conclude that object classiﬁcation on the ModelNet40 dataset is more challenging than on the ModelNet10 dataset, but both
datasets are relatively easy in the sense that details do not matter as much as in the datasets used for our other experiments.

We further use this experiment to investigate the question at which level of sparsity OctNet becomes useful. To answer
this, we visualize the memory consumption of a dense representation vs. our data structure at different resolutions and
occupancies by sampling 500 shapes from the ModelNet40 dataset (see Fig. 16). It can be observed that even at very low
resolutions (83), our data structure is superior compared to the dense representation, even up to an occupancy level of 50%. As
the voxel resolution increases, the occupancy levels (x-axis) decreases since the data becomes sparser. Our OctNet exploits
this sparsity to achieve a signiﬁcant reduction in memory consumption.

A.6. 3D Orientation Estimation

To demonstrate that OctNet can handle input resolutions larger than 2563 we added results for the 3D orientation estimation
experiment with an input resolution of 5123. The results are presented in Fig. 17. As for the orientation experiment in the
main paper, we can observe the trend that performance increases with increasing input resolution.

(a) Accuracy

(b) Accuracy

(c) Confusion Matrix 83

(d) Confusion Matrix 323

Figure 15: ModelNet40 results.

83

163

323

643

1283

2563

Figure 16: Memory consumption vs. occupancy.

We evaluated our OctNet also on the Biwi Kinect Head Pose Database [11] as an additional experiment on 3D pose
estimation. The dataset consists of 24 sequences of 20 individuals sitting in front of a Kinect depth sensor. For each
frame the head center and the head pose in terms of its 3D rotation is annotated. We split the dataset into a training set
of 18 individuals for training and 2 individuals for testing and project the depth map to 3D points with the given camera
parameters. We then create the hybrid grid-octree structure from the 3D points that belong to the head (In this experiment
we are only interested in 3D orientation estimation, as the head can be reliable detected in the color images). As in the
previous experiment we parameterize the orientation with unit quaternions and train our OctNet using the same settings as
in the previous 3D orientation estimation experiment. Fig. 18 shows the quantitative results over varying input resolutions.
We see a reasonable improvement of accuracy from 83 up to 643. Beyond this input resolution the octree resolution becomes
ﬁner than the resolution of the 3D point cloud. Thus, further improvements can not be expected. In Fig. 19 we show some
qualitative results.

(a) 83

(b) 163

(c) 323

(d) 643

(e) 1283

Figure 19: Qualitative Results for Head Pose Estimation.

Figure 17: Additional Chair Orientation Results.

Figure 18: Head Pose Results.

A.7. 3D Semantic Segmentation

In this Section we present additional qualitative results of the semantic 3D point labeling task in Fig. 20 to 22. In these
visualizations we show the color part of the voxelized input, the result of the labeling in the voxel representation, and the
result back-projected to the 3D point cloud for different houses in the test set of [38].

A.8. Network Architecture Details

In this Section we detail the network architectures used throughout our experimental evaluations. We use the following no-
tation for brevity: conv(x, y) denotes a 33 convolutional layer with x input feature maps and y output feature maps. Similarly,
maxpool(f ) is a max-pooling operation that decreases dimensionality by a factor of f along each axis. All convolutional and
fully-connected layers, except the very last one, are followed by a ReLU as activation function.

In the ﬁrst two experiments, 3D classiﬁcation and 3D orientation estimation, we show two different classes of architectures.
In the ﬁrst one we keep the number of convolution layers per block ﬁxed and add blocks depending on the input resolution
of the network. We call those networks OctNet1, OctNet2, and OctNet3, depending on the number of convolution layers per
block. Therefore, the number of parameters increases along with the input resolution. The detailed architectures are depicted
in Table 2, 3, and 4 for the classiﬁcation task and in Table 6, 7, and 8 for the orientation estimation tasks, respectively. Second,
we trained network architectures where we keep the number of parameters ﬁxed, independently of the input resolution. The
detailed network architectures for those experiments are presented in Table 5 and 9.

Finally, for semantic 3D point labeling, we use the U-Net type architecture [2, 53] shown in Table 10. We use a concate-

) to combine the outputs from the decoder and encoder parts of the networks to preserve details.

nation layer concat(
·

,

·

3
4
6

3
8
2
1

3
6
5
2

3
4
6

3
8
2
1

3
6
5
2

(a) Voxelized Input

(b) Voxel Estimates

(c) Estimated Point Cloud

(d) Ground Truth Point Cloud

Figure 20: Facade Labeling Results. Zoom in for details.

(a) Voxelized Input

(b) Voxel Estimates

(c) Estimated Point Cloud

(d) Ground Truth Point Cloud

Figure 21: Facade Labeling Results. Zoom in for details.

3
4
6

3
8
2
1

3
6
5
2

(a) Voxelized Input

(b) Voxel Estimates

(c) Estimated Point Cloud

(d) Ground Truth Point Cloud

Figure 22: Facade Labeling Results. Zoom in for details.

83
conv(1, 8)

163
conv(1, 8)
maxpool(2)
conv(8, 16)

323
conv(1, 8)
maxpool(2)
conv(8, 16)
maxpool(2)
conv(16, 24)

643
conv(1, 8)
maxpool(2)
conv(8, 16)
maxpool(2)
conv(16, 24)
maxpool(2)
conv(24, 32)

1283
conv(1, 8)
maxpool(2)
conv(8, 16)
maxpool(2)
conv(16, 24)
maxpool(2)
conv(24, 32)
maxpool(2)
conv(32, 40)

2563
conv(1, 8)
maxpool(2)
conv(8, 16)
maxpool(2)
conv(16, 24)
maxpool(2)
conv(24, 32)
maxpool(2)
conv(32, 40)
maxpool(2)
conv(40, 48)

Dropout(0.5)
fully-connected(1024)
fully-connected(10)
SoftMax

Table 2: Network Architectures ModelNet10 Classiﬁcation: OctNet1

Table 3: Network Architectures ModelNet10 Classiﬁcation: OctNet2

83
conv(1, 8)
conv(8, 8)

163
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)

83
conv(1, 8)
conv(8, 8)
conv(8, 8)

163
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)

1283
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
maxpool(2)
conv(32, 40)
conv(40, 40)

2563
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
maxpool(2)
conv(32, 40)
conv(40, 40)
maxpool(2)
conv(40, 48)
conv(48, 48)

1283
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
conv(32, 32)
maxpool(2)
conv(32, 40)
conv(40, 40)
conv(40, 40)

2563
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
conv(32, 32)
maxpool(2)
conv(32, 40)
conv(40, 40)
conv(40, 40)
maxpool(2)
conv(40, 48)
conv(48, 48)
conv(48, 48)

323
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)

643
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)

Dropout(0.5)
fully-connected(1024)
fully-connected(10)
SoftMax

323
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
conv(24, 24)

643
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
conv(32, 32)

Dropout(0.5)
fully-connected(1024)
fully-connected(10)
SoftMax

Table 4: Network Architectures ModelNet10 Classiﬁcation: OctNet3.

83
conv(1, 8)
conv(8, 14)

163
conv(1, 8)
conv(8, 14)

323
conv(1, 8)
conv(8, 14)

643
conv(1, 8)
conv(8, 14)

1283
conv(1, 8)
conv(8, 14)

conv(14, 14)
conv(14, 20)

conv(14, 14)
conv(14, 20)

conv(14, 14)
conv(14, 20)

conv(14, 14)
conv(14, 20)

conv(20, 20)
conv(20, 26)

conv(20, 20)
conv(20, 26)

conv(20, 20)
conv(20, 26)

conv(26, 26)
conv(26, 32)

conv(26, 26)
conv(26, 32)

conv(32, 32)
conv(32, 32)

conv(32, 32)
conv(32, 32)
maxpool(2)

conv(26, 26)
conv(26, 32)
maxpool(2)
conv(32, 32)
conv(32, 32)
maxpool(2)

conv(20, 20)
conv(20, 26)
maxpool(2)
conv(26, 26)
conv(26, 32)
maxpool(2)
conv(32, 32)
conv(32, 32)
maxpool(2)

conv(14, 14)
conv(14, 20)
maxpool(2)
conv(20, 20)
conv(20, 26)
maxpool(2)
conv(26, 26)
conv(26, 32)
maxpool(2)
conv(32, 32)
conv(32, 32)
maxpool(2)

2563
conv(1, 8)
conv(8, 14)
maxpool(2)
conv(14, 14)
conv(14, 20)
maxpool(2)
conv(20, 20)
conv(20, 26)
maxpool(2)
conv(26, 26)
conv(26, 32)
maxpool(2)
conv(32, 32)
conv(32, 32)
maxpool(2)

Dropout(0.5)
fully-connected(512)
fully-connected(10)
SoftMax

Table 5: Network Architectures ModelNet10 Classiﬁcation.

83
conv(1, 8)

163
conv(1, 8)
maxpool(2)
conv(8, 16)

323
conv(1, 8)
maxpool(2)
conv(8, 16)
maxpool(2)
conv(16, 24)

643
conv(1, 8)
maxpool(2)
conv(8, 16)
maxpool(2)
conv(16, 24)
maxpool(2)
conv(24, 32)

1283
conv(1, 8)
maxpool(2)
conv(8, 16)
maxpool(2)
conv(16, 24)
maxpool(2)
conv(24, 32)
maxpool(2)
conv(32, 40)

2563
conv(1, 8)
maxpool(2)
conv(8, 16)
maxpool(2)
conv(16, 24)
maxpool(2)
conv(24, 32)
maxpool(2)
conv(32, 40)
maxpool(2)
conv(40, 48)

Dropout(0.5)
fully-connected(1024)
fully-connected(4)
Normalize

Table 6: Network Architectures Orientation Estimation: OctNet1

Table 7: Network Architectures Orientation Estimation: OctNet2

83
conv(1, 8)
conv(8, 8)

163
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)

83
conv(1, 8)
conv(8, 8)
conv(8, 8)

163
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)

1283
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
maxpool(2)
conv(32, 40)
conv(40, 40)

2563
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
maxpool(2)
conv(32, 40)
conv(40, 40)
maxpool(2)
conv(40, 48)
conv(48, 48)

1283
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
conv(32, 32)
maxpool(2)
conv(32, 40)
conv(40, 40)
conv(40, 40)

2563
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
conv(32, 32)
maxpool(2)
conv(32, 40)
conv(40, 40)
conv(40, 40)
maxpool(2)
conv(40, 48)
conv(48, 48)
conv(48, 48)

323
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)

643
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)

Dropout(0.5)
fully-connected(1024)
fully-connected(4)
Normalize

323
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
conv(24, 24)

643
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
conv(32, 32)

Dropout(0.5)
fully-connected(1024)
fully-connected(4)
Normalize

Table 8: Network Architectures Orientation Estimation: OctNet3.

83
conv(1, 8)
conv(8, 14)

163
conv(1, 8)
conv(8, 14)

323
conv(1, 8)
conv(8, 14)

643
conv(1, 8)
conv(8, 14)

1283
conv(1, 8)
conv(8, 14)

conv(14, 14)
conv(14, 20)

conv(14, 14)
conv(14, 20)

conv(14, 14)
conv(14, 20)

conv(14, 14)
conv(14, 20)

conv(20, 20)
conv(20, 26)

conv(20, 20)
conv(20, 26)

conv(20, 20)
conv(20, 26)

conv(26, 26)
conv(26, 32)

conv(26, 26)
conv(26, 32)

conv(32, 32)
conv(32, 32)

conv(32, 32)
conv(32, 32)
maxpool(2)

conv(26, 26)
conv(26, 32)
maxpool(2)
conv(32, 32)
conv(32, 32)
maxpool(2)

conv(20, 20)
conv(20, 26)
maxpool(2)
conv(26, 26)
conv(26, 32)
maxpool(2)
conv(32, 32)
conv(32, 32)
maxpool(2)

conv(14, 14)
conv(14, 20)
maxpool(2)
conv(20, 20)
conv(20, 26)
maxpool(2)
conv(26, 26)
conv(26, 32)
maxpool(2)
conv(32, 32)
conv(32, 32)
maxpool(2)

2563
conv(1, 8)
conv(8, 14)
maxpool(2)
conv(14, 14)
conv(14, 20)
maxpool(2)
conv(20, 20)
conv(20, 26)
maxpool(2)
conv(26, 26)
conv(26, 32)
maxpool(2)
conv(32, 32)
conv(32, 32)
maxpool(2)

Dropout(0.5)
fully-connected(512)
fully-connected(4)
Normalize

Table 9: Network Architectures Orientation Estimation.

Output name

Enc1

Enc2

Enc3

Enc4

Dec4

Dec3

Dec2

Dec1

Output

Operation
conv(8, 8)
conv(8, 16)
maxpool(2)
conv(16, 16)
conv(16, 32)
maxpool(2)
conv(32, 32)
conv(32, 64)
maxpool(2)
conv(64, 64)
conv(64, 128)
maxpool(2)
conv(128, 128)
conv(128, 128)
conv(128, 128)
unpool(2)
concat(Enc4,Dec4)
conv(256, 128)
conv(128, 64)
unpool(2)
concat(Enc3,Dec3)
conv(128, 64)
conv(64, 32)
unpool(2)
concat(Enc2,Dec2)
conv(64, 32)
conv(32, 16)
unpool(2)
concat(Enc1,Dec1)
conv(32, 32)
conv(32, 8)
SoftMax

Table 10: Network Architecture Semantic 3D Point Cloud Labeling.

OctNet: Learning Deep 3D Representations at High Resolutions

Gernot Riegler1

Ali Osman Ulusoy2
1Institute for Computer Graphics and Vision, Graz University of Technology
2Autonomous Vision Group, MPI for Intelligent Systems T¨ubingen
3Computer Vision and Geometry Group, ETH Z¨urich

Andreas Geiger2,3

riegler@icg.tugraz.at

{osman.ulusoy,andreas.geiger}@tue.mpg.de

7
1
0
2
 
r
p
A
 
0
1
 
 
]

V
C
.
s
c
[
 
 
4
v
9
0
0
5
0
.
1
1
6
1
:
v
i
X
r
a

Abstract

We present OctNet, a representation for deep learning
with sparse 3D data. In contrast to existing models, our rep-
resentation enables 3D convolutional networks which are
both deep and high resolution. Towards this goal, we ex-
ploit the sparsity in the input data to hierarchically parti-
tion the space using a set of unbalanced octrees where each
leaf node stores a pooled feature representation. This al-
lows to focus memory allocation and computation to the
relevant dense regions and enables deeper networks without
compromising resolution. We demonstrate the utility of our
OctNet representation by analyzing the impact of resolution
on several 3D tasks including 3D object classiﬁcation, ori-
entation estimation and point cloud labeling.

1. Introduction

Over the last several years, convolutional networks have
lead to substantial performance gains in many areas of com-
puter vision. In most of these cases, the input to the network
is of two-dimensional nature, e.g., in image classiﬁcation
[19], object detection [36] or semantic segmentation [14].
However, recent advances in 3D reconstruction [34] and
graphics [22] allow capturing and modeling large amounts
of 3D data. At the same time, large 3D repositories such as
ModelNet [48], ShapeNet [6] or 3D Warehouse1 as well as
databases of 3D object scans [7] are becoming increasingly
available. These factors have motivated the development of
convolutional networks that operate on 3D data.

Most existing 3D network architectures [8,30,35,48] re-
place the 2D pixel array by its 3D analogue, i.e., a dense and
regular 3D voxel grid, and process this grid using 3D con-
volution and pooling operations. However, for dense 3D
data, computational and memory requirements grow cubi-
cally with the resolution. Consequently, existing 3D net-
works are limited to low 3D resolutions, typically in the
order of 303 voxels. To fully exploit the rich and detailed

1https://3dwarehouse.sketchup.com

t
e
N
v
n
o
C
D
3

e
s
n
e
D

t
e
N
v
n
o
C
D
3

e
s
n
e
D

t
e
N
t
c
O

1

(a) Layer 1: 323

(b) Layer 2: 163

(c) Layer 3: 83

Figure 1: Motivation.
For illustration purposes, we
trained a dense convolutional network to classify 3D shapes
from [48]. Given a voxelized bed as input, we show the
maximum response across all feature maps at intermediate
layers (a-c) of the network before pooling. Higher activa-
tions are indicated with darker colors. Voxels with zero
activation are not displayed. The ﬁrst row visualizes the
responses in 3D while the second row shows a 2D slice.
Note how voxels close to the object contour respond more
strongly than voxels further away. We exploit the sparsity
in our data by allocating memory and computations using a
space partitioning data structure (bottom row).

geometry of our 3D world, however, much higher resolution
networks are required.

In this work, we build on the observation that 3D data is
often sparse in nature, e.g., point clouds, or meshes, result-
ing in wasted computations when applying 3D convolutions

na¨ıvely. We illustrate this in Fig. 1 for a 3D classiﬁcation
example. Given the 3D meshes of [48] we voxelize the input
at a resolution of 643 and train a simple 3D convolutional
network to minimize a classiﬁcation loss. We depict the
maximum of the responses across all feature maps at dif-
ferent layers of the network. It is easy to observe that high
activations occur only near the object boundaries.

Motivated by this observation, we propose OctNet, a 3D
convolutional network that exploits this sparsity property.
Our OctNet hierarchically partitions the 3D space into a set
of unbalanced octrees [32]. Each octree splits the 3D space
according to the density of the data. More speciﬁcally, we
recursively split octree nodes that contain a data point in its
domain, i.e., 3D points, or mesh triangles, stopping at the
ﬁnest resolution of the tree. Therefore, leaf nodes vary in
size, e.g., an empty leaf node may comprise up to 83 = 512
voxels for a tree of depth 3 and each leaf node in the octree
stores a pooled summary of all feature activations of the
voxel it comprises. The convolutional network operations
are directly deﬁned on the structure of these trees. There-
fore, our network dynamically focuses computational and
memory resources, depending on the 3D structure of the in-
put. This leads to a signiﬁcant reduction in computational
and memory requirements which allows for deep learning
at high resolutions. Importantly, we also show how essen-
tial network operations (convolution, pooling or unpooling)
can be efﬁciently implemented on this new data structure.

We demonstrate the utility of the proposed OctNet on
three different problems involving three-dimensional data:
3D classiﬁcation, 3D orientation estimation of unknown
object instances and semantic segmentation of 3D point
clouds. In particular, we show that the proposed OctNet en-
ables signiﬁcant higher input resolutions compared to dense
inputs due to its lower memory consumption, while achiev-
ing identical performance compared to the equivalent dense
network at lower resolutions. At the same time we gain sig-
niﬁcant speed-ups at resolutions of 1283 and above. Using
our OctNet, we investigate the impact of high resolution in-
puts wrt. accuracy on the three tasks and demonstrate that
higher resolutions are particularly beneﬁcial for orientation
estimation and semantic point cloud labeling. Our code is
available from the project website2.

2. Related Work

While 2D convolutional networks have proven very suc-
cessful in extracting information from images [12, 14, 19,
36, 42, 43, 47, 49, 50], there exists comparably little work
on processing three-dimensional data. In this Section, we
review existing work on dense and sparse models.

Dense Models: Wu et al. [48] trained a deep belief network
on shapes discretized to a 303 voxel grid for object classi-

2https://github.com/griegler/octnet

ﬁcation, shape completion and next best view prediction.
Maturana et al. [30] proposed VoxNet, a feed-forward con-
volutional network for classifying 323 voxel volumes from
RGB-D data. In follow-up work, Sedaghat et al. [1] showed
that introducing an auxiliary orientation loss increases clas-
siﬁcation performance over the original VoxNet. Similar
models have also been exploited for semantic point cloud
labeling [21] and scene context has been integrated in [52].
Recently, generative models [37] and auto-encoders [5,
40] have demonstrated impressive performance in learning
low-dimensional object representations from collections of
low-resolution (323) 3D shapes.
Interestingly, these low-
dimensional representations can be directly inferred from a
single image [15] or a sequence of images [8].

Due to computational and memory limitations, all afore-
mentioned methods are only able to process and generate
shapes at a very coarse resolution, typically in the order of
303 voxels. Besides, when high-resolution outputs are de-
sired, e.g., for labeling 3D point clouds, inefﬁcient sliding-
window techniques with a limited receptive ﬁeld must be
adopted [21]. Increasing the resolution na¨ıvely [33, 41, 53]
reduces the depth of the networks and hence their expres-
siveness. In contrast, the proposed OctNets allow for train-
ing deep architectures at signiﬁcant higher resolutions.

Sparse Models: There exist only few network architec-
tures which explicitly exploit sparsity in the data. As these
networks do not require exhaustive dense convolutions they
have the potential of handling higher resolutions.

Engelcke et al. [10] proposed to calculate convolutions
at sparse input locations by pushing values to their target
locations. This has the potential to reduce the number of
convolutions but does not reduce the amount of memory re-
quired. Consequently, their work considers only very shal-
low networks with up to three layers.

A similar approach is presented in [16, 17] where sparse
convolutions are reduced to matrix operations. Unfortu-
nately, the model only allows for 2
2 convolutions and
results in indexing and copy overhead which prevents pro-
cessing volumes of larger resolution (the maximum reso-
lution considered in [16, 17] is 803 voxels). Besides, each
layer decreases sparsity and thus increases the number of
operations, even at a single resolution. In contrast, the num-
ber of operations remains constant in our model.

×

Li et al. [28] proposed ﬁeld probing networks which
sample 3D data at sparse points before feeding them into
fully connected layers. While this reduces memory and
computation, it does not allow for exploiting the distributed
computational power of convolutional networks as ﬁeld
probing layers can not be stacked, convolved or pooled.

Jampani et al. [23] introduced bilateral convolution lay-
ers (BCL) which map sparse inputs into permutohedral
space where learnt convolutional ﬁlters are applied. Their
work is related to ours with respect to efﬁciently exploiting

the sparsity in the input data. However, in contrast to BCL
our method is speciﬁcally targeted at 3D convolutional net-
works and can be immediately dropped in as a replacement
in existing network architectures.

3. Octree Networks

To decrease the memory footprint of convolutional net-
works operating on sparse 3D data, we propose an adaptive
space partitioning scheme which focuses computations on
the relevant regions. As mathematical operations of deep
networks, especially convolutional networks, are best un-
derstood on regular grids, we restrict our attention to data
structures on 3D voxel grids. One of the most popular space
partitioning structures on voxel grids are octrees [31] which
have been widely adopted due to their ﬂexible and hier-
archical structure. Areas of application include depth fu-
sion [24], image rendering [27] and 3D reconstruction [45].
In this paper, we propose 3D convolutional networks on oc-
trees to learn representations from high resolution 3D data.
An octree partitions the 3D space by recursively subdi-
viding it into octants. By subdividing only the cells which
contain relevant information (e.g., cells crossing a surface
boundary or cells containing one or more 3D points) storage
can be allocated adaptively. Densely populated regions are
modeled with high accuracy (i.e., using small cells) while
empty regions are summarized by large cells in the octree.
Unfortunately, vanilla octree implementations [31] have
several drawbacks that hamper its application in deep net-
works. While octrees reduce the memory footprint of the
3D representation, most versions do not allow for efﬁcient
access to the underlying data. In particular, octrees are typi-
cally implemented using pointers, where each node contains
a pointer to its children. Accessing an arbitrary element (or
the neighbor of an element) in the octree requires a traversal
starting from the root until the desired cell is reached. Thus,
the number of memory accesses is equal to the depth of the
tree. This becomes increasingly costly for deep, i.e., high-
resolution, octrees. Convolutional network operations such
as convolution or pooling require frequent access to neigh-
boring elements. It is thus critical to utilize an octree design
that allows for fast data access.

We tackle these challenges by leveraging a hybrid grid-
octree data structure which we describe in Section 3.1. In
Section 3.2, we show how 3D convolution and pooling oper-
ations can be implemented efﬁciently on this data structure.

3.1. Hybrid Grid-Octree Data Structure

The above mentioned problems with the vanilla octree
data structure increase with the octree depth. Instead of rep-
resenting the entire high resolution 3D input with a single
unbalanced octree, we leverage a hybrid grid-octree struc-
ture similar to the one proposed by Miller et al. [32]. The
key idea is to restrict the maximal depth of an octree to a

Figure 2: Hybrid Grid-Octree Data Structure. This ex-
ample illustrates a hybrid grid-octree consisting of 8 shal-
low octrees indicated by different colors. Using 2 shallow
octrees in each dimension with a maximum depth of 3 leads
to a total resolution of 163 voxels.

(a) Shallow Octree

(b) Bit-Representation

Figure 3: Bit Representation. Shallow octrees can be
efﬁciently encoded using bit-strings. Here, the bit-string
1 01010000 00000000 01010000 00000000 01010000 0...
deﬁnes the octree in (a). The corresponding tree is shown
in (b). The color of the voxels corresponds to the split level.

small number, e.g., three, and place several such shallow
octrees along a regular grid (Fig. 2). While this data struc-
ture may not be as memory efﬁcient as the standard octree,
signiﬁcant compression ratios can still be achieved. For in-
stance, a single shallow octree that does not contain input
data stores only a single vector, instead of 83 = 512 vectors
for all voxels at the ﬁnest resolution at depth 3.

An additional beneﬁt of a collection of shallow octrees
is that their structure can be encoded very efﬁciently us-
ing a bit string representation which further lowers access
time and allows for efﬁcient GPGPU implementations [32].
Given a shallow octree of depth 3, we use 73 bit to repre-
sent the complete tree. The ﬁrst bit with index 0 indicates,
if the root node is split, or not. Further, bits 1 to 8 indicate
if one of the child nodes is subdivided and bits 9 to 72 de-
note splits of the grandchildren, see Fig. 3. A tree depth of
3 gives a good trade-off between memory consumption and
computational efﬁciency. Increasing the octree depth results
in an exponential growth in the required bits to store the tree
structure and further increases the cell traversal time.

Using this bit-representation, a single voxel in the shal-

low octree is fully characterised by its bit index. This index
determines the depth of the voxel in the octree and therefore
also the voxel size. Instead of using pointers to the parent
and child nodes, simple arithmetic can be used to retrieve
the corresponding indices of a voxel with bit index i:

(cid:22) i

,

(cid:23)

1

−
8
i + 1 .

pa(i) =

ch(i) = 8

·

(1)

(2)

In contrast to [32], we associate a data container (for storing
features vectors) with all leaf nodes of each shallow tree.
We allocate the data of a shallow octree in a contiguous data
array. The offset associated with a particular voxel in this
array can be computed as follows:

data idx(i) = 8

bit(j) + 1

bit(j)

pa(i)−1
(cid:88)

j=0

(cid:124)

(cid:123)(cid:122)
#nodes above i

+ mod (i
−
(cid:123)(cid:122)
offset

(cid:124)

1, 8)
(cid:125)

i−1
(cid:88)

−

(cid:125)

.

j=0
(cid:124)
(cid:123)(cid:122)
(cid:125)
#split nodes pre i

(3)

Here, mod denotes the modulo operator and bit returns the
tree bit-string value at i. See supp. document for an exam-
ple. Both sum operations can be efﬁciently implemented
using bit counting intrinsics (popcnt). The data arrays of
all shallow octrees are concatenated into a single contiguous
data array during training and testing to reduce I/O latency.

3.2. Network Operations

Given the hybrid grid-octree data structure introduced in
the previous Section, we now discuss the efﬁcient imple-
mentation of network operations on this data structure. We
will focus on the most common operations in convolutional
networks [14, 19, 36]: convolution, pooling and unpooling.
Note that point-wise operations, like activation functions,
do not differ in their implementation as they are indepen-
dent of the data structure.

×

H

Let us ﬁrst introduce the notation which will be used
throughout this Section. Ti,j,k denotes the value of a 3D
tensor T at location (i, j, k). Now assume a hybrid grid-
octree structure with D
W unbalanced shallow oc-
×
trees of maximum depth 3. Let O[i, j, k] denote the value
of the smallest cell in this structure which comprises the
voxel (i, j, k). Note that in contrast to the tensor notation,
O[i1, j1, k1] and O[i2, j2, k2] with i1
=
k2 may refer to the same voxel in the hybrid grid-octree,
depending on the size of the voxels. We obtain the index
of the shallow octree in the grid via (
) and the
local index of the voxel at the ﬁnest resolution in that octree
by (mod (i, 8), mod (j, 8), mod (k, 8)).

= j2

= i2

k
8 (cid:99)

j
8 (cid:99)

i
8 (cid:99)

k1

j1

∨

∨

(cid:98)

(cid:98)

(cid:98)

,

,

(a) Standard Convolution

(b) Efﬁcient Convolution

Figure 4: Convolution. This ﬁgure illustrates the convolu-
tion of a 33 kernel (red) with a 83 grid-octree cell (black).
Only 2 of the 3 dimensions are shown. A na¨ıve implemen-
tation evaluates the kernel at every location (i, j, k) within
a grid-octree cell as shown in (a). This results in
14k mul-
In contrast, (b) depicts our
tiplications for this example.
efﬁcient implementation of the same operation which re-
3k multiplications. As all 83 voxels inside the
quires only
grid-octree cell are the same value, the convolution kernel
inside the cell needs to be evaluated only once. Voxels at the
cell boundary need to integrate information from neighbor-
ing cells. This can be efﬁciently implemented by summing
truncated kernels. See our supp. document for details.

∼

∼

Given this notation, the mapping from a grid-octree O to

a tensor T with compatible dimensions is given by

oc2ten : Ti,j,k = O[i, j, k] .

Similarly, the reverse mapping is given by

ten2oc : O[i, j, k] = pool voxels
(¯i,¯j,¯k)∈Ω[i,j,k]

(T¯i,¯j,¯k) ,

(4)

(5)

·

where pool voxels (
) is a pooling function (e.g., average-
or max-pooling) which pools all voxels in T over the small-
est grid-octree cell comprising location (i, j, k), denoted by
Ω[i, j, k]. This pooling is necessary as a single voxel in O
can cover up to 83 = 512 elements of T , depending on its
size
Remark: With the two functions deﬁned above, we could
wrap any network operation f deﬁned on 3D tensors via

Ω[i, j, k]
|

|

.

g(O) = ten2oc(f (oc2ten(O))) .

(6)

However, this would require a costly conversion from the
memory efﬁcient grid-octrees to a regular 3D tensor and
back. Besides, storing a dense tensor in memory limits the
maximal resolution. We therefore deﬁne our network oper-
ations directly on the hybrid grid-octree data structure.

Convolution The convolution operation is the most im-
portant, but also the most computational expensive opera-
tion in deep convolutional networks. For a single feature

(a) Input

(b) Output

(a) Input

(b) Output

Figure 5: Pooling. The 23 pooling operation on the grid-
octree structure combines 8 neighbouring shallow octrees
(a) into one shallow octree (b). The size of each voxel is
halved and copied to the new shallow octree structure. Vox-
els on the ﬁnest resolution are pooled. Different shallow
octrees are depicted in different colors.

Figure 6: Unpooling. The 23 unpooling operation trans-
forms a single shallow octree of depth d as shown in (a)
into 8 shallow octrees of depth d
1, illustrated in (b). For
each node at depth zero one shallow octree is spawned. All
other voxels double in size. Different shallow octrees are
depicted in different colors.

−

map, convolving a 3D tensor T with a 3D convolution ker-
nel W

RL×M ×N can be written as

∈

T out
i,j,k =

L−1
(cid:88)

M −1
(cid:88)

N −1
(cid:88)

l=0

m=0

n=0

Wl,m,n

T in
ˆi,ˆj,ˆk

,

·

(7)

n +
. Similarly, the convolutions on the grid-octree data

M/2
(cid:99)
(cid:98)

m +

−

−

−

(cid:98)

, ˆj = j

, ˆk = k

l +

with ˆi = i
N/2
(cid:98)
(cid:99)
structure are deﬁned as

L/2
(cid:99)

Oout[i, j, k] = pool voxels
(¯i,¯j,¯k)∈Ω[i,j,k]

(T¯i,¯j,¯k)

(8)

Ti,j,k =

L−1
(cid:88)

M −1
(cid:88)

N −1
(cid:88)

l=0

m=0

n=0

Wl,m,n

Oin[ˆi, ˆj, ˆk] .

·

While this calculation yields the same result as the tensor
convolution in Eq. (7) with the oc2ten, ten2oc wrapper, we
are now able to deﬁne a computationally more efﬁcient con-
volution operator. Our key observation is that for small con-
volution kernels and large voxels, Ti,j,k is constant within
a small margin of the voxel due to its constant support
Oin[ˆi, ˆj, ˆk]. Thus, we only need to compute the convolu-
tion within the voxel once, followed by convolution along
the surface of the voxel where the support changes due to
adjacent voxels taking different values (Fig. 4). This mini-
mizes the number of calculations by a factor of 4 for voxels
of size 83, see supp. material for a detailed derivation. At
the same time, it enables a better caching mechanism.

Pooling Another important operation in deep convolu-
tional networks is pooling. Pooling reduces the spatial res-
olution of the input tensor and aggregates higher-level in-
formation for further processing, thereby increasing the re-
ceptive ﬁeld and capturing context. For instance, strided
23 max-pooling divides the input tensor T in into 23 non-
overlapping regions and computes the maximum value

within each region. Formally, we have

T out
i,j,k = max

l,m,n∈[0,1]

(cid:0)T in

2i+l,2j+m,2k+n

(cid:1) ,

(9)

∈

∈

where T in

R2D×2H×2W and T out

RD×H×W .

To implement pooling on the grid-octree data structure
we reduce the number of shallow octrees. For an input grid-
octree Oin with 2D
2W shallow octrees, the output
×
Oout contains D
W shallow octrees. Each voxel
of Oin is halved in size and copied one level deeper in the
shallow octree. Voxels at depth 3 in Oin are pooled. This
can be formalized as

×
H

2H

×

×

(cid:40)

Oout[i, j, k] =

Oin[2i, 2j, 2k]
P

if vxd(2i, 2j, 2k) < 3
else

P = max

l,m,n∈[0,1]

(Oin[2i + l, 2j + m, 2k + n]) ,

(10)

where vxd(
) computes the depth of the indexed voxel in
·
the shallow octree. A visual example is depicted in Fig. 5.

Unpooling For several tasks such as semantic segmenta-
tion, the desired network output is of the same size as the
network input. While pooling is crucial to increase the re-
ceptive ﬁeld size of the network and capture context, it loses
spatial resolution. To increase the resolution of the net-
work, U-shaped network architectures have become pop-
ular [2, 53] which encode information using pooling oper-
ations and increase the resolution in a decoder part using
unpooling or deconvolution layers [51], possibly in com-
bination with skip-connections [9, 19] to increase preci-
sion. The simplest unpooling strategy uses nearest neigh-
bour interpolation and can be formalized on dense input
R2D×2H×2W tensors
T in
∈
as follows:

RD×H×W and output T out

∈

i,j,k = T in
T out

(cid:98)i/2(cid:99),(cid:98)j/2(cid:99),(cid:98)k/2(cid:99) .

(11)

Again, we can deﬁne the analogous operation on the hybrid
grid-octree data structure by

,

,

(cid:98)

k/2

(12)

] .
(cid:99)

i/2
(cid:99)

j/2
(cid:99)
(cid:98)

Oout[i, j, k] = Oin[
(cid:98)
This operation also changes the data structure: The number
of shallow octrees increases by a factor of 8, as each node
at depth 0 spawns a new shallow octree. All other nodes
double their size. Thus, after this operation the tree depth is
decreased. See Fig. 6 for a visual example of this operation.
Remark: To capture ﬁne details, voxels can be split again
at the ﬁnest resolution according to the original octree of
the corresponding pooling layer. This allows us to take full
advantage of skip connections. We follow this approach in
our semantic 3D point cloud labeling experiments.

4. Experimental Evaluation

In this Section we leverage our OctNet representation to
investigate the impact of input resolution on three different
3D tasks: 3D shape classiﬁcation, 3D orientation estimation
and semantic segmentation of 3D point clouds. To isolate
the effect of resolution from other factors we consider sim-
ple network architectures. Orthogonal techniques like data
augmentation, joint 2D/3D modeling or ensemble learning
are likely to further improve the performance of our models.

Implementation We implemented the grid-octree data
structure, all layers including the necessary forward and
backward functions, as well as utility methods to create the
data structure from point clouds and meshes, as a stand-
alone C++/CUDA library. This allows the usage of our code
within all existing deep learning frameworks. For our ex-
perimental evaluation we used the Torch3 framework.

4.1. 3D Classiﬁcation

We use the popular ModelNet10 dataset [48] for the 3D
shape classiﬁcation task. The dataset contains 10 shape cat-
egories and consists of 3991 3D shapes for training and 908
3D shapes for testing. Each shape is provided as a trian-
gular mesh, oriented in a canonical pose. We convert the
triangle meshes to dense respective grid-octree occupancy
grids, where a voxel is set to 1 if it intersects the mesh. We
P )3 voxels,
scale each mesh to ﬁt into a 3D grid of (N
where N is the number of voxels in each dimension of the
input grid and P = 2 is a padding parameter.

−

We ﬁrst study the inﬂuence of the input resolution on
memory usage, runtime and classiﬁcation accuracy. To-
wards this goal, we create a series of networks of different
input resolution from 83 to 2563 voxels. Each network con-
sists of several blocks which reduce resolution by half until
we reach a resolution of 83. Each block comprises two con-
volutional layers (33 ﬁlters, stride 1) and one max-pooling

3http://torch.ch/

(a) Memory

(b) Runtime

(c) Accuracy

(d) Accuracy

Figure 7: Results on ModelNet10 Classiﬁcation Task.

layer (23 ﬁlters, stride 2). The number of feature maps in the
ﬁrst block is 8 and increases by 6 with every block. After the
last block we add a fully-connected layer with 512 units and
a ﬁnal output layer with 10 units. Each convolutional layer
and the ﬁrst fully-connected layer are followed by a recti-
ﬁed linear unit [26] as activation function and the weights
are initialized as described in [18]. We use the standard
cross-entropy loss for training and train all networks for 20
epochs with a batch size of 32 using Adam [25]. The initial
learning rate is set to 0.001 and we decrease the learning
rate by a factor of 10 after 15 epochs.

Overall, we consider three different types of networks:
the original VoxNet architecture of Maturana et al. [30]
which operates on a ﬁxed 323 voxel grid, the proposed Oct-
Net and a dense version of it which we denote “DenseNet”
in the following. While performance gains can be ob-
tained using orthogonal approaches such as network ensem-
bles [5] or a combination of 3D and 2D convolutional net-
works [20,42], in this paper we deliberately focus on “pure”
3D convolutional network approaches to isolate the effect of
resolution from other inﬂuencing factors.

Fig. 7 shows our results. First, we compare the mem-
ory consumption and run-time of our OctNet wrt. the dense
baseline approach, see Fig. 7a and 7b. Importantly, OctNets
require signiﬁcantly less memory and run-time for high in-
put resolutions compared to dense input grids. Using a
batch size of 32 samples, our OctNet easily ﬁts in a modern
GPU’s memory (12GB) for an input resolution of 2563. In
contrast, the corresponding dense model ﬁts into the mem-
643. A more detailed analysis of
ory only for resolutions
the memory consumption wrt. the sparsity in the data is pro-
vided in the supp. document. OctNets also run faster than
their dense counterparts for resolutions >643. For resolu-
643, OctNets run slightly slower due to the overhead
tions
incurred by the grid-octree representation and processing.

≤

≤

3
8

3
6
1

3
2
3

3
4
6

83

323

Figure 9: Confusion Matrices on ModelNet10.

Bathtub

Bed

Dresser

N. Stand

(a) Mean Angular Error

(b) Mean Angular Error

Figure 8: Voxelized 3D Shapes from ModelNet10.

Figure 10: Orientation Estimation on ModelNet10.

Leveraging our OctNets, we now compare the impact
of input resolution with respect to classiﬁcation accuracy.
Fig. 7c shows the results of different OctNet architectures
where we keep the number of convolutional layers per block
ﬁxed to 1, 2 and 3. Fig. 7d shows a comparison of accuracy
with respect to DenseNet and VoxNet when keeping the ca-
pacity of the model, i.e., the number of parameters, constant
by removing max-pooling layers from the beginning of the
network. We ﬁrst note that despite its pooled representation,
OctNet performs on par with its dense equivalent. This con-
ﬁrms our initial intuition (Fig. 1) that sparse data allows for
allocating resources adaptively without loss in performance.
Furthermore, both models outperform the shallower VoxNet
architecture, indicating the importance of network depth.

Regarding classiﬁcation accuracy we observed improve-
ments for lower resolutions but diminishing returns beyond
an input resolution of 323 voxels. Taking a closer look at
the confusion matrices in Fig. 9, we observe that higher in-
put resolution helps for some classes, e.g., bathtub, while
others remain ambiguous independently of the resolution,
e.g., dresser vs. night stand. We visualize this lack of dis-
criminative power by showing voxelized representations of
3D shapes from the ModelNet10 database Fig. 8. While
bathtubs look similar to beds (or sofas, tables) at low reso-
lution they can be successfully distinguished at higher reso-
lutions. However, a certain ambiguity between dresser and
night stand remains.

4.2. 3D Orientation Estimation

In this Section, we investigate the importance of input
resolution on 3D orientation estimation. Most existing ap-
proaches to 3D pose estimation [3, 4, 39, 44, 46] assume that
the true 3D shape of the object instance is known. To assess

the generalization ability of 3D convolutional networks, we
consider a slightly different setup where only the object cat-
egory is known. After training a model on a hold-out set of
3D shapes from a single category, we test the ability of the
model to predict the 3D orientation of unseen 3D shapes
from the same category.

−

±

2
(cid:105)

q1, q2

More concretely, given an instance of an object category
with unknown pose, the goal is to estimate the rotation with
respect to the canonical pose. We utilize the 3D shapes from
the chair class of the ModelNet10 dataset and rotate them
15◦ around each axis. We use the same
randomly between
network architectures and training protocol as in the classi-
ﬁcation experiment, except that the networks regress orien-
tations. We use unit quaternions to represent 3D rotations
and train our networks with an Euclidean loss. For small an-
gles, this loss is a good approximation to the rotation angle
φ = arccos(2
(cid:104)

1) between quaternions q1, q2.

Fig. 10 shows our results using the same naming con-
vention as in the previous Section. We observe that ﬁne
details are more important compared to the classiﬁcation
task. For the OctNet 1-3 architectures we observe a steady
increase in performance, while for networks with constant
capacity across resolutions (Fig. 10b), performance levels
beyond 1283 voxels input resolution. Qualitative results of
the latter experiment are shown in Fig. 11. Each row shows
10 different predictions for two randomly selected chair
instance over several input resolutions, ranging from 163
to 1283. Darker colors indicate larger errors which occur
more frequently at lower resolutions. In contrast, predic-
tions at higher network resolutions cluster around the true
pose. Note that learning a dense 3D representation at a res-
olution of 1283 voxels or beyond would not be feasible.

Riemenschneider et al. [38]
Martinovic et al. [29]
Gadde et al. [13]

OctNet 643
OctNet 1283
OctNet 2563

Average Overall

IoU

-
-
68.5

60.0
65.3
73.6

-
-
78.6

73.6
76.1
81.5

42.3
52.2
54.4

45.6
50.4
59.2

Table 1: Semantic Segmentation on RueMonge2014.

(a) Voxelized Input

(b) Voxel Estimates

(c) Estimated Point Cloud (d) Ground Truth Point Cloud
Figure 12: OctNet 2563 Facade Labeling Results.

TP

the test set are assigned the label of the corresponding grid-
octree voxels. As evaluation measures we use overall pixel
TP+FN over all 3D points, average class accu-
accuracy
racy, and intersection over union
TP+FN+FP over all classes.
Here, FP, FN and TP denote false positives, false negatives
and true positives, respectively.

TP

Our results clearly show that increasing the input reso-
lution is essential to obtain state-of-the-art results, as ﬁner
details vanish at coarser resolutions. Qualitative results for
one facade are provided in Fig. 12. Further results are pro-
vided in the supp. document.

5. Conclusion and Future Work

We presented OctNet, a novel 3D representation which
makes deep learning with high-resolution inputs tractable.
We analyzed the importance of high resolution inputs on
several 3D learning tasks, such as object categorization,
pose estimation and semantic segmentation. Our exper-
iments revealed that for ModelNet10 classiﬁcation low-
resolution networks prove sufﬁcient while high input (and
output) resolution matters for 3D orientation estimation and
3D point cloud labeling. We believe that as the community
moves from low resolution object datasets such as Model-
Net10 to high resolution large scale 3D data, OctNet will
enable further improvements. One particularly promising
avenue for future research is in learning representations for
multi-view 3D reconstruction where the ability to process
high resolution voxelized shapes is of crucial importance.

163

323

643

1283

Figure 11: Orientation Estimation on ModelNet10. This
ﬁgure illustrates 10 rotation estimates for 3 chair instances
while varying the input resolution from 163 to 1283. Darker
colors indicate larger deviations from the ground truth.

4.3. 3D Semantic Segmentation

In this Section, we evaluate the proposed OctNets on the
problem of labeling 3D point cloud with semantic informa-
tion. We use the RueMonge2014 dataset [38] that provides
a colored 3D point cloud of several Haussmanian style fa-
cades, comprising
1 million 3D points in total. The labels
are window, wall, balcony, door, roof, sky and shop.

∼

For this task, we train a U-shaped network [2, 53] on
three different input resolutions, 643, 1283 and 2563, where
the voxel size was selected such that the height of all build-
ings ﬁts into the input volume. We ﬁrst map the point cloud
into the grid-octree structure. For all leaf nodes which con-
tain more than one point, we average the input features and
calculate the majority vote of the ground truth labels for
training. As features we use the binary voxel occupancy, the
RGB color, the normal vector and the height above ground.
Due to the small number of training samples, we augment
the data for this task by applying small rotations.

Our network architecture comprises an encoder and a de-
coder part. The encoder part consists of four blocks which
comprise 2 convolution layers (33 ﬁlters, stride 1) followed
by one max-pooling layer each. The decoder consists of
four blocks which comprise 2 convolutions (33 ﬁlters, stride
1) followed by a guided unpooling layer as discussed in the
previous Section. Additionally, after each unpooling step all
features from the last layer of the encoder at the same res-
olution are concatenated to provide high-resolution details.
All networks are trained with a per voxel cross entropy loss
using Adam [25] and a learning rate of 0.0001.

Table 1 compares the proposed OctNet to several state
of the art approaches on the facade labeling task following
the extended evaluation protocol of [13]. The 3D points of

References

[1] N. S. Alvar, M. Zolfaghari, and T. Brox. Orientation-boosted
voxel nets for 3d object recognition. arXiv.org, 1604.03351,
2016. 2

[2] V. Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A
deep convolutional encoder-decoder architecture for image
segmentation. arXiv.org, 1511.00561, 2015. 5, 8, 14

[3] E. Brachmann, A. Krull, F. Michel, S. Gumhold, J. Shotton,
and C. Rother. Learning 6d object pose estimation using
In Proc. of the European Conf. on
3d object coordinates.
Computer Vision (ECCV), 2014. 7

[4] E. Brachmann, F. Michel, A. Krull, M. Y. Yang, S. Gumhold,
and C. Rother. Uncertainty-driven 6d pose estimation of ob-
In Proc. IEEE
jects and scenes from a single rgb image.
Conf. on Computer Vision and Pattern Recognition (CVPR),
2016. 7

[5] A. Brock, T. Lim, J. M. Ritchie, and N. Weston. Generative
and discriminative voxel modeling with convolutional neural
networks. arXiv.org, 1608.04236, 2016. 2, 6

[6] A. X. Chang, T. A. Funkhouser, L. J. Guibas, P. Hanrahan,
Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su,
J. Xiao, L. Yi, and F. Yu. Shapenet: An information-rich 3d
model repository. arXiv.org, 1512.03012, 2015. 1

[7] S. Choi, Q. Zhou, S. Miller, and V. Koltun. A large dataset

of object scans. arXiv.org, 1602.02481, 2016. 1

[8] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese. 3d-
r2n2: A uniﬁed approach for single and multi-view 3d object
reconstruction. In Proc. of the European Conf. on Computer
Vision (ECCV), 2016. 1, 2

[9] A. Dosovitskiy, P. Fischer, E. Ilg, P. Haeusser, C. Hazirbas,
V. Golkov, P. v.d. Smagt, D. Cremers, and T. Brox. Flownet:
Learning optical ﬂow with convolutional networks. In Proc.
of the IEEE International Conf. on Computer Vision (ICCV),
2015. 5

[10] M. Engelcke, D. Rao, D. Z. Wang, C. H. Tong, and I. Posner.
Vote3deep: Fast object detection in 3d point clouds using ef-
ﬁcient convolutional neural networks. arXiv.org, 609.06666,
2016. 2

[11] G. Fanelli, J. Gall, and L. Van Gool. Real time head pose
In Proc. IEEE
estimation with random regression forests.
Conf. on Computer Vision and Pattern Recognition (CVPR),
2011. 13

[12] J. Flynn, I. Neulander, J. Philbin, and N. Snavely. Deep-
stereo: Learning to predict new views from the world’s im-
agery. arXiv.org, 1506.06825, 2015. 2

[13] R. Gadde, V. Jampani, R. Marlet, and P. V. Gehler. Ef-
ﬁcient 2d and 3d facade segmentation using auto-context.
arXiv.org, 1606.06437, 2016. 8

[14] G. Ghiasi and C. C. Fowlkes. Laplacian pyramid reconstruc-
tion and reﬁnement for semantic segmentation. In Proc. of
the European Conf. on Computer Vision (ECCV), 2016. 1, 2,
4

[15] R. Girdhar, D. F. Fouhey, M. Rodriguez, and A. Gupta.
Learning a predictable and generative vector representation
In Proc. of the European Conf. on Computer
for objects.
Vision (ECCV), 2016. 2

[16] B. Graham. Spatially-sparse convolutional neural networks.

arXiv.org, 2014. 2

[17] B. Graham. Sparse 3d convolutional neural networks.

In
Proc. of the British Machine Vision Conf. (BMVC), 2015. 2
[18] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation. In Proc. of the IEEE International Conf. on
Computer Vision (ICCV), 2015. 6

[19] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
In Proc. IEEE Conf. on Computer

for image recognition.
Vision and Pattern Recognition (CVPR), 2016. 1, 2, 4, 5
[20] V. Hegde and R. Zadeh. Fusionnet: 3d object classiﬁcation
using multiple data representations. arXiv.org, 1607.05695,
2016. 6

[21] J. Huang and S. You. Point cloud labeling using 3d convolu-
tional neural network. In Proc. of the International Conf. on
Pattern Recognition (ICPR), 2016. 2

[22] Q. Huang, H. Wang, and V. Koltun. Single-view reconstruc-
In
tion via joint analysis of image and shape collections.
ACM Trans. on Graphics (SIGGRAPH), 2015. 1

[23] V. Jampani, M. Kiefel, and P. V. Gehler. Learning sparse high
dimensional ﬁlters: Image ﬁltering, dense crfs and bilateral
neural networks. In Proc. IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), 2016. 2

[24] W. Kehl, T. Holl, F. Tombari, S. Ilic, and N. Navab. An
octree-based approach towards efﬁcient variational range
data fusion. arXiv.org, 1608.07411, 2016. 3

[25] D. P. Kingma and J. Ba. Adam: A method for stochastic
optimization. In Proc. of the International Conf. on Learning
Representations (ICLR), 2015. 6, 8

[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in Neural Information Processing Systems (NIPS),
2012. 6

[27] S. Laine and T. Karras. Efﬁcient sparse voxel octrees. IEEE
Trans. on Visualization and Computer Graphics (VCG),
17(8):1048–1059, 2011. 3

[28] Y. Li, S. Pirk, H. Su, C. R. Qi, and L. J. Guibas. FPNN: ﬁeld
probing neural networks for 3d data. arXiv.org, 1605.06240,
2016. 2

[29] A. Martinovi´c, J. Knopp, H. Riemenschneider, and L. Van
Gool. 3d all the way: Semantic segmentation of urban scenes
from start to end in 3d. In Proc. IEEE Conf. on Computer
Vision and Pattern Recognition (CVPR), 2015. 8

[30] D. Maturana and S. Scherer. Voxnet: A 3d convolutional
In Proc.
neural network for real-time object recognition.
IEEE International Conf. on Intelligent Robots and Systems
(IROS), 2015. 1, 2, 6

[31] D. Meagher. Geometric modeling using octree encod-
ing. Computer Graphics and Image Processing (CGIP),
19(1):85, 1982. 3

[32] A. Miller, V. Jain, and J. L. Mundy. Real-time rendering
In Proc. of
and dynamic updating of 3-d volumetric data.
the Workshop on General Purpose Processing on Graphics
Processing Units (GPGPU), page 8, 2011. 2, 3, 4

[33] F. Milletari, N. Navab, and S. Ahmadi. V-net: Fully convo-
lutional neural networks for volumetric medical image seg-
mentation. arXiv.org, 1606.04797, 2016. 2

[49] J. Xie, R. B. Girshick, and A. Farhadi. Deep3d: Fully au-
tomatic 2d-to-3d video conversion with deep convolutional
In Proc. of the European Conf. on Com-
neural networks.
puter Vision (ECCV), 2016. 2

[50] A. R. Zamir, T. Wekel, P. Agrawal, C. Wei, J. Malik, and
S. Savarese. Generic 3d representation via pose estimation
and matching. In Proc. of the European Conf. on Computer
Vision (ECCV), 2016. 2

[51] M. D. Zeiler, G. W. Taylor, and R. Fergus. Adaptive decon-
volutional networks for mid and high level feature learning.
In Proc. of the IEEE International Conf. on Computer Vision
(ICCV), 2011. 5

[52] Y. Zhang, M. Bai, P. Kohli, S. Izadi, and J. Xiao. Deepcon-
text: Context-encoding neural pathways for 3d holistic scene
understanding. arXiv.org, 1603.04922, 2016. 2
¨Ozg¨un Cicek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and
O. Ronneberger. 3d u-net: Learning dense volumetric seg-
mentation from sparse annotation. arXiv.org, 1606.06650,
2016. 2, 5, 8, 14

[53]

[34] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux,
D. Kim, A. J. Davison, P. Kohli, J. Shotton, S. Hodges, and
A. Fitzgibbon. Kinectfusion: Real-time dense surface map-
ping and tracking. In Proc. of the International Symposium
on Mixed and Augmented Reality (ISMAR), 2011. 1

[35] C. R. Qi, H. Su, M. Nießner, A. Dai, M. Yan, and L. Guibas.
Volumetric and multi-view cnns for object classiﬁcation on
3d data. In Proc. IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR), 2016. 1

[36] S. Ren, K. He, R. B. Girshick, and J. Sun. Faster R-CNN:
towards real-time object detection with region proposal net-
works. In Advances in Neural Information Processing Sys-
tems (NIPS), 2015. 1, 2, 4

[37] D. J. Rezende, S. M. A. Eslami, S. Mohamed, P. Battaglia,
M. Jaderberg, and N. Heess. Unsupervised learning of 3d
structure from images. arXiv.org, 1607.00662, 2016. 2
[38] H. Riemenschneider, A. B´odis-Szomor´u, J. Weissenberg,
and L. V. Gool. Learning where to classify in multi-view
semantic segmentation. In Proc. of the European Conf. on
Computer Vision (ECCV), 2014. 8, 14

[39] R. Rios-Cabrera and T. Tuytelaars. Discriminatively trained
templates for 3d object detection: A real-time scalable ap-
proach. In Proc. of the IEEE International Conf. on Com-
puter Vision (ICCV), pages 2048–2055, 2013. 7

[40] A. Sharma, O. Grau, and M. Fritz. Vconv-dae: Deep vol-
arXiv.org,

umetric shape learning without object labels.
1604.03755, 2016. 2

[41] S. Song and J. Xiao. Deep sliding shapes for amodal 3d
object detection in RGB-D images. arXiv.org, 1511.02300,
2015. 2

[42] H. Su, S. Maji, E. Kalogerakis, and E. G. Learned-Miller.
Multi-view convolutional neural networks for 3d shape
In Proc. of the IEEE International Conf. on
recognition.
Computer Vision (ICCV), 2015. 2, 6

[43] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Multi-view
3d models from single images with a convolutional network.
In Proc. of the European Conf. on Computer Vision (ECCV),
2016. 2

[44] A. Tejani, D. Tang, R. Kouskouridas, and T. Kim. Latent-
class hough forests for 3d object detection and pose estima-
In Proc. of the European Conf. on Computer Vision
tion.
(ECCV), 2014. 7

[45] A. O. Ulusoy, A. Geiger, and M. J. Black. Towards prob-
abilistic volumetric reconstruction using ray potentials.
In
Proc. of the International Conf. on 3D Vision (3DV), 2015. 3
[46] P. Wohlhart and V. Lepetit. Learning descriptors for object
recognition and 3d pose estimation. In Proc. IEEE Conf. on
Computer Vision and Pattern Recognition (CVPR), 2015. 7
[47] J. Wu, T. Xue, J. J. Lim, Y. Tian, J. B. Tenenbaum, A. Tor-
ralba, and W. T. Freeman. Single image 3d interpreter net-
work. In Proc. of the European Conf. on Computer Vision
(ECCV), 2016. 2

[48] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and
J. Xiao. 3d shapenets: A deep representation for volumetric
shapes. In Proc. IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR), 2015. 1, 2, 6, 12

A. Appendix

A.1. Introduction

A.2. Data Index

In the following supplemental material we present details regarding our operations on the hybrid grid-octree data structure,
additional experimental results and all details on the used network architectures. In Section A.2 we provide details on the
data index used to efﬁciently access data in the grid-octree data structure. Section A.3 yields more insights into the efﬁcient
implementation of the convolution operation on octrees. We present further quantitative and qualitative results in Section A.4
and in Section A.8 we specify all details of the network architectures used in our experiments.

An important implementation detail of the hybrid grid-octree data structure is the memory alignment for fast data access.
We store all data associated with the leaf nodes of a shallow octree in a compact contiguous array. Thus, we need a fast way
to compute the offset in this data array for any given voxel. In the main text we presented the following equation:

(13)

(14)

(15)

data idx(i) = 8

bit(j) + 1

bit(j)

pa(i)−1
(cid:88)

j=0

(cid:124)

(cid:123)(cid:122)
#nodes above i

i−1
(cid:88)

−

(cid:125)

j=0
(cid:123)(cid:122)
(cid:125)
(cid:124)
#split nodes pre i

+ mod (i
−
(cid:123)(cid:122)
offset

(cid:124)

.

1, 8)
(cid:125)

As explained in the main text the whole octree structure is stored as a bit-string and a voxel is uniquely identiﬁed by the bit
index i, i.e., the index within the bit string. The data is aligned breadth-ﬁrst and only the leaf nodes have data associated.
Consequently, the ﬁrst part of the equation above counts the number of split and leaf nodes up to the voxel with bit index i.
The second term subtracts the number of split nodes before the particular voxel as data is only associated with leaf nodes.
Finally, we need to get the offset within the voxel’s neighborhood. This is done by the last term of the equation.

Let us illustrate this with a simple example: For ease of visualization we will consider a quadtree. Hence, each voxel can

be split into 4 instead of 8 children. The equation for the offset changes to

data idx4(i) = 4

bit(j) + 1

bit(j)

pa4(i)−1
(cid:88)

i−1
(cid:88)

j=0

(cid:124)

(cid:123)(cid:122)
#nodes above i

−

(cid:125)

j=0
(cid:125)
(cid:123)(cid:122)
(cid:124)
#split nodes pre i

+ mod (i
−
(cid:123)(cid:122)
offset

(cid:124)

,

1, 4)
(cid:125)

with

pa4(i) =

(cid:22) i

(cid:23)

1

.

−
4

Now consider the following bit string for instance: 1 0101 0000 1001 0000 0100. According to our deﬁnition, this bit string
corresponds to the tree structure visualized in Fig. 13a and 13b, where s indicates a split node and v a leaf node with associated
data. In Fig. 13c we show the bit indices for all nodes. Note that the leaf nodes at depth 3 do not need to be stored in the bit
string as this information is implicit. Finally, the data index for all leaf nodes is visualized in Fig. 13d. Now we can verify
equation (14) using a simple example. Assume the bit index 51: The parent bit index is given by equation (15) as 12. To
compute the data index we ﬁrst count the number of nodes before 49 as it is the ﬁrst node within its siblings (ﬁrst term of
equation), which is 17. Next, we count the number of split nodes up to 49 (second term of equation), which is 6. Finally, we
look up the position 51 within its siblings (last term of equation), which is 2. Combining those three terms yields the data
index 17

6 + 2 = 13.

−

A.3. Efﬁcient Convolution

In the main text we discussed that the convolution for larger octree cells and small convolution kernels can be efﬁ-
ciently implemented. A na¨ıve implementation applies the convolution kernel at every location (i, j, k) comprised by the cell
Ω[i, j, k]. Therefore, for an octree cell of size 83 and a convolution kernel kernel of 33 this would require 83
33 = 13, 824
multiplications. However, we can implement this calculation much more efﬁciently as depicted in Fig. 14. We observe that
the value inside the cell of size 83 is constant. Thus, we only need to evaluate the convolution once inside this cell and
multiply the result with the size of the cell 83, see Fig. 14a. Additionally, we only need to evaluate a truncated versions of the

·

(a) Bit-String

(b) Split and Leaf Nodes

(c) Bit Index

(d) Data Index

Figure 13: Data Index.

(a) Constant

(b) Corners

(c) Edges

(d) Faces

Figure 14: Efﬁcient Convolution.

kernel on the corners, edges and faces of the voxel, see Fig. 14b-d. This implementation is more efﬁcient, as we need only
27 multiplications for the constant part, 8
15 multiplications for the edges, and
19 multiplications for the corners, 12
9 multiplications for the faces of the voxel. In total this yields 3203 multiplications, or 23.17% of the multiplications
6
required by the na¨ıve implementation.

62

6

·

·

·

·

·

In this Section we show additional quantitative and qualitative results for 3D shape classiﬁcation, 3D orientation estimation

A.4. Additional Results

and semantic 3D point labeling.

A.5. 3D Classiﬁcation

In the main text of our work we analyzed the runtime and memory consumption of OctNet compared with the equivalent
dense networks on ModelNet10 [48]. Additionally, we demonstrated that without further data augmentation, ensemble
learning, or more sophisticated architectures the accuracy saturates at an input resolution of about 163, when keeping the
number of network parameters ﬁxed across all resolutions. In this Section we show the same experiment on ModelNet40 [48].
The results are summarized in Fig. 15. In contrast to ModelNet10, we see an increase in accuracy up to an input resolution
of 323. Beyond this resolution the classiﬁcation performance does not further improve. Note that the only form of data
augmentation we used in this experiment was rotation around the up-vector as the 3D models in this dataset vary in pose. We
conclude that object classiﬁcation on the ModelNet40 dataset is more challenging than on the ModelNet10 dataset, but both
datasets are relatively easy in the sense that details do not matter as much as in the datasets used for our other experiments.

We further use this experiment to investigate the question at which level of sparsity OctNet becomes useful. To answer
this, we visualize the memory consumption of a dense representation vs. our data structure at different resolutions and
occupancies by sampling 500 shapes from the ModelNet40 dataset (see Fig. 16). It can be observed that even at very low
resolutions (83), our data structure is superior compared to the dense representation, even up to an occupancy level of 50%. As
the voxel resolution increases, the occupancy levels (x-axis) decreases since the data becomes sparser. Our OctNet exploits
this sparsity to achieve a signiﬁcant reduction in memory consumption.

A.6. 3D Orientation Estimation

To demonstrate that OctNet can handle input resolutions larger than 2563 we added results for the 3D orientation estimation
experiment with an input resolution of 5123. The results are presented in Fig. 17. As for the orientation experiment in the
main paper, we can observe the trend that performance increases with increasing input resolution.

(a) Accuracy

(b) Accuracy

(c) Confusion Matrix 83

(d) Confusion Matrix 323

Figure 15: ModelNet40 results.

83

163

323

643

1283

2563

Figure 16: Memory consumption vs. occupancy.

We evaluated our OctNet also on the Biwi Kinect Head Pose Database [11] as an additional experiment on 3D pose
estimation. The dataset consists of 24 sequences of 20 individuals sitting in front of a Kinect depth sensor. For each
frame the head center and the head pose in terms of its 3D rotation is annotated. We split the dataset into a training set
of 18 individuals for training and 2 individuals for testing and project the depth map to 3D points with the given camera
parameters. We then create the hybrid grid-octree structure from the 3D points that belong to the head (In this experiment
we are only interested in 3D orientation estimation, as the head can be reliable detected in the color images). As in the
previous experiment we parameterize the orientation with unit quaternions and train our OctNet using the same settings as
in the previous 3D orientation estimation experiment. Fig. 18 shows the quantitative results over varying input resolutions.
We see a reasonable improvement of accuracy from 83 up to 643. Beyond this input resolution the octree resolution becomes
ﬁner than the resolution of the 3D point cloud. Thus, further improvements can not be expected. In Fig. 19 we show some
qualitative results.

(a) 83

(b) 163

(c) 323

(d) 643

(e) 1283

Figure 19: Qualitative Results for Head Pose Estimation.

Figure 17: Additional Chair Orientation Results.

Figure 18: Head Pose Results.

A.7. 3D Semantic Segmentation

In this Section we present additional qualitative results of the semantic 3D point labeling task in Fig. 20 to 22. In these
visualizations we show the color part of the voxelized input, the result of the labeling in the voxel representation, and the
result back-projected to the 3D point cloud for different houses in the test set of [38].

A.8. Network Architecture Details

In this Section we detail the network architectures used throughout our experimental evaluations. We use the following no-
tation for brevity: conv(x, y) denotes a 33 convolutional layer with x input feature maps and y output feature maps. Similarly,
maxpool(f ) is a max-pooling operation that decreases dimensionality by a factor of f along each axis. All convolutional and
fully-connected layers, except the very last one, are followed by a ReLU as activation function.

In the ﬁrst two experiments, 3D classiﬁcation and 3D orientation estimation, we show two different classes of architectures.
In the ﬁrst one we keep the number of convolution layers per block ﬁxed and add blocks depending on the input resolution
of the network. We call those networks OctNet1, OctNet2, and OctNet3, depending on the number of convolution layers per
block. Therefore, the number of parameters increases along with the input resolution. The detailed architectures are depicted
in Table 2, 3, and 4 for the classiﬁcation task and in Table 6, 7, and 8 for the orientation estimation tasks, respectively. Second,
we trained network architectures where we keep the number of parameters ﬁxed, independently of the input resolution. The
detailed network architectures for those experiments are presented in Table 5 and 9.

Finally, for semantic 3D point labeling, we use the U-Net type architecture [2, 53] shown in Table 10. We use a concate-

) to combine the outputs from the decoder and encoder parts of the networks to preserve details.

nation layer concat(
·

,

·

3
4
6

3
8
2
1

3
6
5
2

3
4
6

3
8
2
1

3
6
5
2

(a) Voxelized Input

(b) Voxel Estimates

(c) Estimated Point Cloud

(d) Ground Truth Point Cloud

Figure 20: Facade Labeling Results. Zoom in for details.

(a) Voxelized Input

(b) Voxel Estimates

(c) Estimated Point Cloud

(d) Ground Truth Point Cloud

Figure 21: Facade Labeling Results. Zoom in for details.

3
4
6

3
8
2
1

3
6
5
2

(a) Voxelized Input

(b) Voxel Estimates

(c) Estimated Point Cloud

(d) Ground Truth Point Cloud

Figure 22: Facade Labeling Results. Zoom in for details.

83
conv(1, 8)

163
conv(1, 8)
maxpool(2)
conv(8, 16)

323
conv(1, 8)
maxpool(2)
conv(8, 16)
maxpool(2)
conv(16, 24)

643
conv(1, 8)
maxpool(2)
conv(8, 16)
maxpool(2)
conv(16, 24)
maxpool(2)
conv(24, 32)

1283
conv(1, 8)
maxpool(2)
conv(8, 16)
maxpool(2)
conv(16, 24)
maxpool(2)
conv(24, 32)
maxpool(2)
conv(32, 40)

2563
conv(1, 8)
maxpool(2)
conv(8, 16)
maxpool(2)
conv(16, 24)
maxpool(2)
conv(24, 32)
maxpool(2)
conv(32, 40)
maxpool(2)
conv(40, 48)

Dropout(0.5)
fully-connected(1024)
fully-connected(10)
SoftMax

Table 2: Network Architectures ModelNet10 Classiﬁcation: OctNet1

Table 3: Network Architectures ModelNet10 Classiﬁcation: OctNet2

83
conv(1, 8)
conv(8, 8)

163
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)

83
conv(1, 8)
conv(8, 8)
conv(8, 8)

163
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)

1283
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
maxpool(2)
conv(32, 40)
conv(40, 40)

2563
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
maxpool(2)
conv(32, 40)
conv(40, 40)
maxpool(2)
conv(40, 48)
conv(48, 48)

1283
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
conv(32, 32)
maxpool(2)
conv(32, 40)
conv(40, 40)
conv(40, 40)

2563
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
conv(32, 32)
maxpool(2)
conv(32, 40)
conv(40, 40)
conv(40, 40)
maxpool(2)
conv(40, 48)
conv(48, 48)
conv(48, 48)

323
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)

643
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)

Dropout(0.5)
fully-connected(1024)
fully-connected(10)
SoftMax

323
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
conv(24, 24)

643
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
conv(32, 32)

Dropout(0.5)
fully-connected(1024)
fully-connected(10)
SoftMax

Table 4: Network Architectures ModelNet10 Classiﬁcation: OctNet3.

83
conv(1, 8)
conv(8, 14)

163
conv(1, 8)
conv(8, 14)

323
conv(1, 8)
conv(8, 14)

643
conv(1, 8)
conv(8, 14)

1283
conv(1, 8)
conv(8, 14)

conv(14, 14)
conv(14, 20)

conv(14, 14)
conv(14, 20)

conv(14, 14)
conv(14, 20)

conv(14, 14)
conv(14, 20)

conv(20, 20)
conv(20, 26)

conv(20, 20)
conv(20, 26)

conv(20, 20)
conv(20, 26)

conv(26, 26)
conv(26, 32)

conv(26, 26)
conv(26, 32)

conv(32, 32)
conv(32, 32)

conv(32, 32)
conv(32, 32)
maxpool(2)

conv(26, 26)
conv(26, 32)
maxpool(2)
conv(32, 32)
conv(32, 32)
maxpool(2)

conv(20, 20)
conv(20, 26)
maxpool(2)
conv(26, 26)
conv(26, 32)
maxpool(2)
conv(32, 32)
conv(32, 32)
maxpool(2)

conv(14, 14)
conv(14, 20)
maxpool(2)
conv(20, 20)
conv(20, 26)
maxpool(2)
conv(26, 26)
conv(26, 32)
maxpool(2)
conv(32, 32)
conv(32, 32)
maxpool(2)

2563
conv(1, 8)
conv(8, 14)
maxpool(2)
conv(14, 14)
conv(14, 20)
maxpool(2)
conv(20, 20)
conv(20, 26)
maxpool(2)
conv(26, 26)
conv(26, 32)
maxpool(2)
conv(32, 32)
conv(32, 32)
maxpool(2)

Dropout(0.5)
fully-connected(512)
fully-connected(10)
SoftMax

Table 5: Network Architectures ModelNet10 Classiﬁcation.

83
conv(1, 8)

163
conv(1, 8)
maxpool(2)
conv(8, 16)

323
conv(1, 8)
maxpool(2)
conv(8, 16)
maxpool(2)
conv(16, 24)

643
conv(1, 8)
maxpool(2)
conv(8, 16)
maxpool(2)
conv(16, 24)
maxpool(2)
conv(24, 32)

1283
conv(1, 8)
maxpool(2)
conv(8, 16)
maxpool(2)
conv(16, 24)
maxpool(2)
conv(24, 32)
maxpool(2)
conv(32, 40)

2563
conv(1, 8)
maxpool(2)
conv(8, 16)
maxpool(2)
conv(16, 24)
maxpool(2)
conv(24, 32)
maxpool(2)
conv(32, 40)
maxpool(2)
conv(40, 48)

Dropout(0.5)
fully-connected(1024)
fully-connected(4)
Normalize

Table 6: Network Architectures Orientation Estimation: OctNet1

Table 7: Network Architectures Orientation Estimation: OctNet2

83
conv(1, 8)
conv(8, 8)

163
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)

83
conv(1, 8)
conv(8, 8)
conv(8, 8)

163
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)

1283
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
maxpool(2)
conv(32, 40)
conv(40, 40)

2563
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
maxpool(2)
conv(32, 40)
conv(40, 40)
maxpool(2)
conv(40, 48)
conv(48, 48)

1283
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
conv(32, 32)
maxpool(2)
conv(32, 40)
conv(40, 40)
conv(40, 40)

2563
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
conv(32, 32)
maxpool(2)
conv(32, 40)
conv(40, 40)
conv(40, 40)
maxpool(2)
conv(40, 48)
conv(48, 48)
conv(48, 48)

323
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)

643
conv(1, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)

Dropout(0.5)
fully-connected(1024)
fully-connected(4)
Normalize

323
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
conv(24, 24)

643
conv(1, 8)
conv(8, 8)
conv(8, 8)
maxpool(2)
conv(8, 16)
conv(16, 16)
conv(16, 16)
maxpool(2)
conv(16, 24)
conv(24, 24)
conv(24, 24)
maxpool(2)
conv(24, 32)
conv(32, 32)
conv(32, 32)

Dropout(0.5)
fully-connected(1024)
fully-connected(4)
Normalize

Table 8: Network Architectures Orientation Estimation: OctNet3.

83
conv(1, 8)
conv(8, 14)

163
conv(1, 8)
conv(8, 14)

323
conv(1, 8)
conv(8, 14)

643
conv(1, 8)
conv(8, 14)

1283
conv(1, 8)
conv(8, 14)

conv(14, 14)
conv(14, 20)

conv(14, 14)
conv(14, 20)

conv(14, 14)
conv(14, 20)

conv(14, 14)
conv(14, 20)

conv(20, 20)
conv(20, 26)

conv(20, 20)
conv(20, 26)

conv(20, 20)
conv(20, 26)

conv(26, 26)
conv(26, 32)

conv(26, 26)
conv(26, 32)

conv(32, 32)
conv(32, 32)

conv(32, 32)
conv(32, 32)
maxpool(2)

conv(26, 26)
conv(26, 32)
maxpool(2)
conv(32, 32)
conv(32, 32)
maxpool(2)

conv(20, 20)
conv(20, 26)
maxpool(2)
conv(26, 26)
conv(26, 32)
maxpool(2)
conv(32, 32)
conv(32, 32)
maxpool(2)

conv(14, 14)
conv(14, 20)
maxpool(2)
conv(20, 20)
conv(20, 26)
maxpool(2)
conv(26, 26)
conv(26, 32)
maxpool(2)
conv(32, 32)
conv(32, 32)
maxpool(2)

2563
conv(1, 8)
conv(8, 14)
maxpool(2)
conv(14, 14)
conv(14, 20)
maxpool(2)
conv(20, 20)
conv(20, 26)
maxpool(2)
conv(26, 26)
conv(26, 32)
maxpool(2)
conv(32, 32)
conv(32, 32)
maxpool(2)

Dropout(0.5)
fully-connected(512)
fully-connected(4)
Normalize

Table 9: Network Architectures Orientation Estimation.

Output name

Enc1

Enc2

Enc3

Enc4

Dec4

Dec3

Dec2

Dec1

Output

Operation
conv(8, 8)
conv(8, 16)
maxpool(2)
conv(16, 16)
conv(16, 32)
maxpool(2)
conv(32, 32)
conv(32, 64)
maxpool(2)
conv(64, 64)
conv(64, 128)
maxpool(2)
conv(128, 128)
conv(128, 128)
conv(128, 128)
unpool(2)
concat(Enc4,Dec4)
conv(256, 128)
conv(128, 64)
unpool(2)
concat(Enc3,Dec3)
conv(128, 64)
conv(64, 32)
unpool(2)
concat(Enc2,Dec2)
conv(64, 32)
conv(32, 16)
unpool(2)
concat(Enc1,Dec1)
conv(32, 32)
conv(32, 8)
SoftMax

Table 10: Network Architecture Semantic 3D Point Cloud Labeling.

