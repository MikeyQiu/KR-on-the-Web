9
1
0
2
 
n
a
J
 
8
 
 
]

G
L
.
s
c
[
 
 
1
v
0
3
2
2
0
.
1
0
9
1
:
v
i
X
r
a

Soft-Bayes: Prod for Mixtures of Experts with Log-Loss

Laurent Orseau1

Tor Lattimore1

Shane Legg1

{lorseau,lattimore,legg}@google.com
DeepMind, London, UK

Abstract

We consider prediction with expert advice under the log-loss with the goal of deriving
eﬃcient and robust algorithms. We argue that existing algorithms such as exponentiated
gradient, online gradient descent and online Newton step do not adequately satisfy
both requirements. Our main contribution is an analysis of the Prod algorithm that is
robust to any data sequence and runs in linear time relative to the number of experts
in each round. Despite the unbounded nature of the log-loss, we derive a bound that
is independent of the largest loss and of the largest gradient, and depends only on the
number of experts and the time horizon. Furthermore we give a Bayesian interpretation
of Prod and adapt the algorithm to derive a tracking regret.

1 Introduction

Sequence prediction is a simple problem at the core of many machine learning problems:
Given a sequence of past observations, what is the probability of the next one? We ap-
proach this problem using the prediction with expert advice framework with logarithmic
loss. The log-loss is arguably the most fundamental choice because of its connections to
probability (minimising the log-loss is maximising the likelihood of the observed data), se-
quential betting, information theory and compression (minimising the log-loss corresponds
to maximising the compression ratio when compressing using arithmetic coding).

Setup Let X be a countable alphabet and D be the set of probability distributions over
X . We consider a game over T rounds, where in each round t the learner ﬁrst receives the
predictions of N experts (pi
t ∈ D. The learner then chooses their own prediction
Mt ∈ D and the next symbol xt ∈ X is revealed. The learner then suﬀers an instantaneous
loss at round t of

i=1 with pi

t)N

(cid:96)t(Mt) = − log (Mt(xt))

and the cycles continue up to round T , which may or may not be known in advance. The
learner would like to combine the advice of the experts so as to make the loss as small as
possible, which corresponds to good prediction/compression. We make no assumptions on
the data source; in particular x1:T = x1, x2, . . . , xT need not be independent and identically
distributed, and may even be generated by an adversary. The standard approach in this
setting is to analyse the regret of the predictor M relative to some interesting class of

1

competitors. Here we focus on predicting well relative to a ﬁxed convex combination of
experts. Let W be the (N − 1)-dimensional probability simplex: ∀w ∈ W : (cid:80)N
i=1 wi = 1
and ∀i ∈ [N ] : wi ∈ [0, 1], where wi is the ith component of the vector w. Deﬁne the regret
(also called redundancy for the log-loss) relative to a ∈ W by

RT (a) =

(cid:96)t(Mt) − (cid:96)t

(cid:34)

T
(cid:88)

t=1

(cid:32) N
(cid:88)

(cid:33)(cid:35)

aipi
t

.

i=1

(1)

This deﬁnition of the regret is more demanding than the usual notion of competing with
the best single expert in hindsight, which corresponds to competing with ‘Dirac’ experts
a ∈ {e1, e2, . . . , eN } with ei the standard basis vectors. In particular, deﬁning the regret as
in Eq. (1) forces the learner to exploit the ‘wisdom of the crowd’ by combining the experts
predictions, rather than focusing on the single best expert in hindsight. This is often crucial
because it often happens that no single expert predicts well over all time periods, and the
nature of the log-loss means that even a single round of poor prediction can lead to a large
instantaneous loss.

This framework has attracted signiﬁcant attention over the last three decades, mainly
due to its applications to compression and portfolio optimisation; see for example Kalai and
Vempala [2002] and references within. Our objective is to design algorithms that are (a)
linear-time in the number of experts N , (b) robust to the existence of incompetent experts
and (c) recover the tracking guarantees of ﬁxed share [Herbster and Warmuth, 1998]. A
variety of approaches are now known for this problem, but none satisfy all of (a), (b) and
(c) above. The minimax optimal and Bayesian solutions are known to achieve logarithmic
regret [Cover, 1991], but there are currently no linear-time implementations and it seems
unlikely that one exists. The main computational challenge for the Bayesian approach is
evaluating the normalisation integral. This issue was addressed by Kalai and Vempala [2002]
via a polynomial-time sampling approach, but unfortunately the solution far from linear in
N and is not suitable for practical implementations when N is large. More recently Hazan
et al. [2007] proposed the online Newton step, a pseudo second-order algorithm that also
achieves logarithmic regret, but depends on computing a generalised projection for which
the best-known running time is O(N 2) per step [Luo et al., 2016]. Even in the idealised
case that the projection could be computed in O(1), the algorithm depends on maintaining
and updating a covariance matrix and so O(N 2) running time is unavoidable without some
form of dimensionality reduction that weakens the regret guarantees (see Luo et al., 2016).
All of the algorithms mentioned until now enjoy optimal logarithmic regret. The expo-
nentiated gradient (EG) algorithm by Helmbold et al. [1998] has a regret bound that looks
like

RT (a) ≤ C

log N ,

(cid:114)

T
2

(2)

pi
t(xt)
Mt(xt) ≤ maxt

1

where C = maxt,i
Mt(xt) . The EG algorithm runs in O(N ) time per round,
but unfortunately its regret depends on C, which may be so large that it can make Eq. (2)
vacuous (Section 3). Even worse, the dependence on C is not an artifact of the analysis,
but rather a failing of the EG algorithm, which becomes unstable when experts transition
from predicting badly to predicting well. Another algorithm with near-linear running time

2

is online gradient descent (OGD) by Zinkevich [2003] (and applied to this setting by Veness
et al. [2012a]), which runs in O(N log(N )) time using the fast simplex projection by Duchi
et al. [2008]. The regret of this algorithm also depends on the size of the maximum gradient
of the loss, however, which leads to bound of the same order as Eq. (2). Note that EG is
equivalent to using mirror descent with neg-entropy regularisation, which makes the projec-
tion to the simplex nothing more than normalisation. We will return brieﬂy to alternative
regularisation choices for mirror descent in the discussion.

We revisit the Prod algorithm by Cesa-Bianchi et al. [2007], which runs in O(N ) time per
step and was originally designed for obtaining second-order bounds when competing with
the best expert rather than the mixture. The stability of Prod has not gone unnoticed, with
recent work by Gaillard et al. [2014] and Sani et al. [2014] also exploiting its advantages
over exponential weighting. Since the log-loss is unbounded, it is not immediately suitable
for use in Prod, but conveniently the linearised loss is semi-bounded.
In this sense our
algorithm is to Prod what exponentiated gradient is to exponential weighting.

Contributions Our main contribution is an analysis of Prod when competing against a
mixture in the log-loss setting (Sections 4 and 5). By tuning the learning rate we are able
to show two regret bounds:

(cid:16)(cid:112)T C log N

(cid:17)

(cid:16)(cid:112)N T log N

(cid:17)

(3)

RT = O

RT = O

(4)
where C is deﬁned as above. The ﬁrst bound (Section 4) eliminates all dependence on the
arbitrarily large C at the price of a square-root dependence on the dimension N . The second
bound (Section 5) retains a dependence on C, but moves it inside the square root relative
to the EG algorithm. We also prove self-conﬁdent bounds (Section 5) and analyse a truly
online version of Prod that does not need prior knowledge of the horizon and simultaneously
achieves a tracking guarantee (Section 6). To complement the upper bounds we prove lower
bounds for EG and OGD showing that in the worst case they suﬀer nearly linear regret
(Section 3). Moreover, we give two Bayesian interpretations of Prod as a ‘slowed down’
Bayesian predictor or a mixture of ‘partially sleeping’ experts (Section 2).

,

Notation For a natural number n let [n] = {1, 2, . . . , n} and ei be the standard basis
vectors (the dimension will always be clear from context). The number of experts is denoted
by N ≥ 1 and the time horizon is T ≥ 1. Let D be the set of probability distributions
on the countable alphabet X , and W be the (N − 1)-dimensional probability simplex so
that for a ∈ W we have ai ≥ 0 for all i and (cid:80)N
i=1 ai = 1. For a, w ∈ W we deﬁne
RE(a(cid:107)b) = (cid:80)N
i:ai>0 ai ln(ai/bi) to be the relative entropy between a and b. All of the
following analysis only relies on pi
t through pi
t(xt), which is the probability that expert i
assigned to the actual observation xt in round t. For this reason we abbreviate pi
t(xt).
We also use pi
s. Similarly we abbreviate the prediction Mt = Mt(xt) and
M1:t = (cid:81)t
s=1 Ms. We usually reserve w, wt ∈ W to denote the weights over experts used by
an algorithm and a ∈ W for a ﬁxed competitor. Given a ∈ W, we let At = (cid:80)n
t be
the prediction of the mixture of experts over a and A1:t = (cid:81)t
s=1 As. The indicator function
is denoted by [[test]] ∈ {0, 1} and equals 1 if the boolean test is true.

1:t = (cid:81)t

i=1 aipi

t = pi

s=1 pi

3

2 The Soft-Bayes algorithm

As discussed in the introduction, the Prod algorithm was originally designed for prediction
with expert advice when competing with the best expert in hindsight. Let w1 ∈ W be the
initial (prior) weights, which we always take to be uniform w1 = 1
N unless otherwise stated.
Then in each round t the Prod algorithm predicts using a mixture over the experts (5) and
updates its weights using a multiplicative update rule (6):

Mt =

wi

tpi
t .

(5)

N
(cid:88)

i=1

wi

t+1 =

wi
j=1 wj

t(1 − ¯η(cid:96)i
t)
t (1 − ¯η(cid:96)j
t )

,

(cid:80)N

(6)

where (cid:96)i
t = (cid:96)t(ei) is the loss suﬀered by expert i in round t and ¯η ∈ (0, 1) is the learning
rate. The algorithm only makes sense if (cid:96)i
t ≤ 1, which is usually assumed. Recall our loss
function is (cid:96)t : W → R is given by (cid:96)t(w) = − log Mt = − log (cid:80)N
t, which is convex
but arbitrarily large. The key idea is to predict using Eq. (5), but replace the loss with
the linearised loss, ∇(cid:96)t(wt)(cid:62)w. A simple calculation shows that ∇(cid:96)t(wt)i = −pi
t/Mt ≤ 0.
Therefore the linearised losses are semi-bounded. If we instantiate Prod, but replace the
loss of each expert with the linearised loss, then the resulting algorithm predicts like Eq. (5)
and updates its weights by

i=1 wi

tpi

wi

t+1 =

(cid:16)

wi
t

(cid:17)

1 + ¯η pi
t
Mt
(cid:16)
1 + ¯η pj
t
Mt

t

(cid:80)N

j=1 wj

(cid:17) =

(cid:16)

wi
t

1 + ¯η pi
t
Mt

(cid:17)

1 + ¯η

(cid:18)

= wi
t

1 − η + η

(cid:19)

,

pi
t
Mt

(7)

where η = ¯η/(1 + ¯η) so that ¯η = η/(1 − η) and the second equality follows from the
deﬁnition of Mt = (cid:80)
t = 1. Notice that the computations of
the prediction Eq. (5) and weight update Eq. (7) are both linear in the number of experts
N . For reference, the EG and OGD algorithms also predicts like Eq. (5), but update their
weights by

t and the fact that (cid:80)

j wj

i wi

t pj

EG: wi

t+1 =

wi

t exp
j=1 wj

(cid:16)

(cid:17)

η pi
t
Mt
(cid:16)
η pj
t
Mt

t exp

(cid:17)

(cid:80)N

OGD: wi

t+1 = Π

wt + η

(8)

(cid:18)

(cid:19)

pt
Mt

,

i

where Π is the projection onto the simplex with respect to the Euclidean norm. A careful
examination of the EG update leads to a worrying observation: If wi
t is close to zero and pi
t
is close to one, then pi
t+1
to drop to nearly zero for all j (cid:54)= i. This makes EG unstable when the gradients are large.
The OGD update can be even worse because the projection has the potential to concentrate
the weights on a Dirac after which the regret can be inﬁnite! In contrast, Prod behaves
more conservatively since1

t/Mt can be extremely large, causing wi

t+1 to be close to one and wj

wi

t+1 = wi
t

1 − η + η

= (1 − η)wi

t + η

≤ (1 − η)wi

t + η ,

(cid:18)

(cid:19)

pi
t
Mt

wi
tpi
t
Mt

1The second equality suggests that Prod and Soft-Bayes are closely related to exponential smoothing for

probability estimation [Mattern, 2016].

4

where the inequality follows from the dominance property that Mt ≥ wi
t for all i and t.
This means that even in the most extreme scenarios, the weight increases by at most η,
which is usually tuned to be approximately T −1/2.

tpi

Bayesian interpretation We now give two Bayesian interpretations of this algorithm.
The ﬁrst is to note that if η = 1, then

wi

t+1 =

wi
tpi
t
Mt

and

M1:T =

wi

1pi

1:T .

N
(cid:88)

i=1

t is the posterior of the Bayesian mixture over sources (pi)i with prior w1.
In this case wi
While its regret relative to a single expert is at most RT (ei) ≤ log N , the algorithm does
not compete with convex combinations of experts. From a Bayesian perspective, there is
no reason to believe that it should because the convex combinations lies outside the class of
the learner. On the other hand, if the learning η is chosen to be close to zero, then the Prod
update has the eﬀect of ‘slowing down Bayes’ to ensure it does not concentrate too fast
on a single promising expert. The multiplicative/additive nature of the update also means
that experts can make big mistakes while losing at most (1 − η) of their current weight.
In contrast, a ‘slow’ update derived from the exponential weights algorithm that looks like
wi

t/Mt)) still reduces the weight of an expert to zero if pi

t exp(η log(pi

t+1 ∝ wi

t = 0.

The second interpretation comes from the sleeping expert framework [Freund et al.,
1997] that allows experts to ‘fall asleep’ and abstain from predicting in some rounds. If
the weights are normalised appropriately, then this is equivalent to assuming the sleeping
experts defer their vote to the wakeful, which for them is equivalent to predicting like the
mixture Mt [Chernov and Vovk, 2009]. We consider a smooth version of this idea, where
an expert can be ‘sleepy’ and predict partially like the mixture. From a given expert pi, we
build the meta-expert ˜pi such that for all time steps t:

t := (1 − η)Mt + ηpi
˜pi
t

and thus

˜pi
1:T =

((1 − η)Mt + ηpi

t) ,

T
(cid:89)

t=1

where η ∈ (0, 1] and Mt is now deﬁned as a mixture of the meta-experts ˜pi:

M1:T =

wi

1 ˜pi

1:T

N
(cid:88)

i=1

(9)

Note that since both p and M are predictors the convex combination of these is also a
predictor. Such self-referential constructions have been noted in the past, for example by
Koolen et al. [2012]. The main point is that the meta-experts are not normal predictors
because they depend on the learner M . Nevertheless, they are useful for analysis and
intuition. With this view of M we note that all the usual properties of Bayesian predictors
hold. In particular:

• The posterior weight of the ith meta-expert is

wi

t+1 = wi
1

= wi
t

= wi
t

1 − η + η

(cid:18)

(cid:19)

.

pi
t
Mt

˜pi
1:t
M1:t

˜pi
t
Mt

5

T
(cid:88)

t=1
T
(cid:88)

t=1

T
(cid:88)

t=1

• The Bayes prediction over the class of meta-experts is Mt =

• The weights are properly normalised:

N
(cid:88)

wi

t = 1 for all t ∈ [T ].

N
(cid:88)

i=1

wi

t ˜pi
t .

i=1
Based on these observations we call the algorithm deﬁned by the prediction in Eq. (5) and
updates in Eq. (7), or equivalently by Eq. (9), the Soft-Bayes algorithm.

Regret relative to a single expert We start the theoretical results with a simple bound
on the regret relative to a single expert.

Theorem 1. For the Soft-Bayes algorithm, RT (ei) = ln pi

1:T
M1:T

≤ 1

η ln 1
wi
1

for all i.

Proof. Using dominance and the deﬁnition of concavity applied to the function ln(·):

∀i ∈ [N ] : ln M1:T ≥ ln wi

1 ˜pi

1:T = ln wi

1 +

ln((1 − η)Mt + ηpi
t)

≥ ln wi

1 + (1 − η)

ln Mt + η

ln pi

t = ln wi

1 + (1 − η) ln M1:T + η ln pi

1:T .

Therefore η ln M1:T ≥ η ln pi

1:T + ln wi

1, and the proof is completed by rearrangement.

If the goal is to compete with the best expert only, then setting η = 1 is optimal,
which makes Prod equivalent to the standard Bayesian algorithm over (ei). As an aside, in
Appendix F we present a simple setup where we recover several well-known algorithms by
using Soft-Bayes with speciﬁc simple learning rates.

3 Failure of EG and OGD

As remarked in the introduction, the EG and OGD algorithms can become unstable when
the gradients are uncontrolled. Here we demonstrate this with a carefully crafted example
that best illustrates the issue.

Theorem 2. If N = 2 and w1 = (1/2, 1/2) ∈ W is the uniform prior, then for any
learning rate η > 0 there exists a sequence of predictors such that the regret of EG and
OGD is Ω(T 1−ε) for all ε ∈ (0, 1).

The proof is given in Appendix D and depends on a simple example where the experts
predict Dirac measures with disjoint support (they always disagree). In the ﬁrst T /2 rounds
the ﬁrst expert is always wrong and for the next T /2 rounds the correctness of the experts
alternates. If the learning rate is suﬃciently large, then the weights of the EG algorithm
oscillate wildly, which leads to a super-exponential regret. The only way to avoid this
calamity is to choose a learning rate so small that EG barely learns at all, in which case it
suﬀers near-linear regret. For OGD the regret can even be inﬁnite in this example. A naive
attempt to ﬁx the EG algorithm is to replace the experts with ‘meta-experts’ pδi deﬁned

6

:= (1 − δ)pi

t + δ/|X |. This ensures that Mt = (cid:80)

by pδi
t ≥ δ/|X | ≈ 1/c for all t. While
t
this does prevent super-exponential regret, it does not solve the problem. Compared to a
mixture of the base experts pi, the mixture of the meta-experts pδi can suﬀer a regret of T δ.
Hence, considering the bound in Eq. (2), the optimal balance is for δ ≈ T −1/4 leading to a
bound of O(T 3/4), which is much worse than the O(T 1/2) regret that we prove for Prod. A
similar correction is possible for OGD, but does not seem worthwhile in light of the above
discussion.

i wi

tpδi

4 Regret against a mixture

Recall that for any a ∈ W, the mixture predictor is At = (cid:80)N
t = maxj pj
Let M∗ := {i | ∃t ∈ [T ] : pi
any other expert at least once, and let m = |M∗| ≤ N .

i=1 aipi
t=1 At.
t } be the set of experts that predict at least as well as

t and A1:T = (cid:81)T

Theorem 3. For any η ∈ (0, 1), the regret of the Soft-Bayes algorithm M is bounded by:

RT (a) = ln

≤

ln N + ¯ηmT + m ln

+ ln N ,

where ¯η :=

A1:T
M1:T

1
¯η

N
m

η
1 − η

.

The learning rate ¯η is optimised by ¯η =

(cid:113) ln N

T m and for this choice

RT (a) = ln

≤ 2

T m ln N + m ln

+ ln N

N
m

A1:T
M1:T

√

√

and RT (a)

≤ 2

T N ln N + ln N .

To prove this theorem we need the following lemma (proof in Appendix B).

Lemma 4. Let η ∈ (0, 1), a ∈ W, and q ∈ [0, ∞)N ,

ln

aiqi ≤

ai ln (1 − η + ηqi) + max

ln

1 +

(cid:18)

(cid:19)

qi

.

η
1 − η

i

N
(cid:88)

i=1

1
η

N
(cid:88)

i=1

Proof. (Theorem 3) By Lemma 4 and using wi

t+1 = wi

t(1 − η)

(cid:16)

1 + η
1−η

pi
t
Mt

(cid:17)

, for any t ∈ [T ]:

ln

At
Mt

= ln

=

1
η

N
(cid:88)

i=1

(cid:88)

i

ai pi
t
Mt

≤

1
η

ai ln

wi
t+1
wi
t

(cid:18)

(cid:88)

ai ln

1 − η + η

i

+ max

ln

i

(cid:19)

(cid:18) wi
t+1
wi
t

− ln(1 − η) .

(cid:19)

pi
t
Mt

+ max
i≤N

ln

(cid:18)

1 +

(cid:19)

η
1 − η

pi
t
Mt

The ﬁrst term telescopes when summed over time, but more eﬀort is required to control
the second since the index i of maxi can change with time. The idea is to introduce all the
missing ln wi
t terms for all i that can be the maxi at some step t, that is all i ∈ M∗.
This comes at a cost of ln(1 − η) for each of them since wi

t+1/wi

t+1/wi

t ≥ 1 − η:

ln

At
Mt

≤

1
η

N
(cid:88)

i=1

ai ln

wi
t+1
wi
t

− m ln(1 − η) +

(10)

(cid:88)

ln

i∈M∗

wi
t+1
wi
t

.

7

We can now telescope the series over time, also using − ln(1 − η) ≤ η

1−η = ¯η (Lemma 13),

ln

A1:T
M1:T

=

ln

≤

T
(cid:88)

t=1

At
Mt

1
η

N
(cid:88)

i=1

ai ln

wi

T +1
wi
1

+ ¯ηmT +

(cid:88)

ln

i∈M∗

wi

.

T +1
wi
1

Using wi
larly (cid:80)

1 = 1/N it holds that (cid:80)
i∈M∗ ln(wi
T +1/wi

i ai ln(wi
T +1/wi
1) is maximised when wi

1) is maximised when wi
T +1 = 1/m. Therefore

T +1 = ai and simi-

≤

RE(a(cid:107)w1) + ¯ηmT + m ln

≤

ln N + ¯ηmT + m ln

(11)

N
m

1
η

N
m

ln

A1:T
M1:T

1
η
1
¯η

=

ln N + ¯ηmT + m ln

+ ln N .

N
m

If we replace M∗ with all the N experts in Eq. (10), we obtain m = N giving the second
bound.

5 Self-conﬁdent bounds

Self-conﬁdent bounds were introduced by Auer and Gentile [2000] in online prediction to
build algorithms that can perform better when the sequence is easy, instead of considering
that all sequences are worst cases. These bounds depend on the loss of the competitor. For
Prod, Gaillard et al. [2014] derived second order self-conﬁdent bounds that depend on the
excess losses, that is, the diﬀerence between the instantaneous loss of the learner and that
of one of the experts. We provide bounds of a similar ﬂavour.

Theorem 5. Consider the Soft-Bayes algorithm with learning rate η ∈ (0, 1) and ¯η = η
Then

1−η .

RT (a) = ln

≤

ln N + ¯η max
i≤N

A1:T
M1:T

1
¯η

1
¯η

− 1

+ ln N

(cid:19)2

(cid:19)2

T
(cid:88)

t=1

(cid:18) pi
t
Mt
(cid:18) pi
t
Mt

i≤N,t≤T

(cid:16) pi
t
Mt

(cid:17)2

≤

ln N + ¯ηT max

− 1

+ ln N .

(12)

(13)

− 1

, then Equation (13) is opti-

The proof is in Appendix C. Let C2 := maxi,t
√

(cid:113) ln N
T C2

− 1

T C2 ln p+ln N = maxi,t 2

leading to a regret bound of 2

mized for ¯η :=
ln N . Furthermore, since the learning rate is monotonically decreasing, this bound is suit-
able for an online learning rate. The case for Eq. (12) is more complicated: as noted by
Cesa-Bianchi et al. [2007], Gaillard et al. [2014] for Prod and others elsewhere this sort
of bound depends on the best expert in hindsight and thus does not lead to a monotone
decreasing learning rate in general. Gaillard et al. [2014] nicely circumvent this issue by
using one learning rate per expert, at the cost of only a multiplicative O(ln ln T ) factor in
the loss. We perform a similar transformation in Appendix E. As discussed in Section 3,
since pi
t/Mt − 1)2 can be poor. For
this reason we show that with a small additional cost the quadratic term can be replaced
with a linear term.

t/Mt can be large, the quadratic dependence on maxi,t(pi

(cid:16) pi
t
Mt

(cid:17) √

T ln N +

8

Theorem 6. Consider the Soft-Bayes algorithm with learning rate η ∈ (0, 1) and let C1 =
(cid:80)T

(cid:17)

t=1 maxi≤N

− 1

. Then

(cid:16) pi
t
Mt

RT (a) = ln

≤ min

C1,

ln N +

C1 + η2T

.

A1:T
M1:T

(cid:26)

1
η

η
2

(cid:27)

The proof is in Appendix C. If the learning rate is chosen to be η =

then the

(cid:113) 2 log N
C1

theorem shows that

RT (a) ≤ min

C1,

2C1 ln N +

(cid:26)

(cid:112)

2T ln N
C1

(cid:27)

.

In another case where for example At = (cid:80)

t and
− 1 becomes close to 0 and then C1 << T ; However for this case learning will likely still

Observe that if a single expert i is always the best predictor, Mt becomes close to pi
pi
t
Mt
be slower than with second-order self-conﬁdent bounds.
1
m pi

t, that is, at worst the best expert
alternates uniformly between a subset M∗ of m = |M∗| experts, Mt cannot become close to
all the pi
T ) (omitting
other dependencies); Furthermore, since this means that M learns, Mt should become close
to (cid:80)
mT ln N ), hence possibly
providing good guarantees against the best subset of the experts. By contrast, a second-
order self-conﬁdent bound would only provide a guarantee of O(m

t at the same time and then C1 = O(T ), which still makes RT (a) ≤ O(

t and thus C1 ≈ mT so we should have RT (a) ≤ O(

T ln N ).

1
m pi

i∈M∗

i∈M∗

√

√

√

Moreover, upper bounding C1 with T maxi,t

in Theorem 6 and then optimizing η as

above gives the result of Eq. (4) for C = maxi,t

Also note that C1 is monotonically increasing with T and thus the learning rate can be

pi
t
Mt
pi
t
Mt

.

updated online.

6 Online bounds

√

T , the weights can decay as wi

We now provide a fully ‘online’ algorithm that does not require advance knowledge of the
time horizon T or the number of ‘sometimes optimal experts’ m in advance. Surprisingly,
√
t
we could not simply replace the learning rating η with a time-varying version ηt ≈ 1/
and adapt the proofs in a straightforward manner. The reason seems to be that with a
T ) after
1 exp(−t/
ﬁxed rate of η := 1/
t steps, whereas for a time-varying learning rate ηt := 1/
t, the weights can decay as fast
as wi
T ). An easy solution is
1
to use the doubling trick [Cesa-Bianchi et al., 1997], which was the approach taken in the
analysis of the original Prod algorithm [Cesa-Bianchi et al., 2007]. Another option is to use
an exponential rescaling with renormalization, which was used for ML-Prod [Gaillard et al.,
2014]. We show instead a diﬀerent online correction of the update rule based on a special
form of the ﬁxed-share rule [Herbster and Warmuth, 1998]. When 0 < ηt+1 ≤ ηt ≤ 1, we

t) (consider for example t =

s=1(1 − 1/

1(1 − 1/
√

T )t ≈ wi

1 exp(−

t) ≈ wi

(cid:81)t

√

√

√

√

√

9

(14)

(15)

(16)

use the following online correction term applied to the update rule:2
(cid:19)

(cid:19)

(cid:18)

(cid:18)

wi

t+1 := wi
t
(cid:124)

1 − ηt + ηt

+

1 −

pi
t
Mt

ηt+1
ηt

(cid:123)(cid:122)
update

(cid:125)

(cid:124)

(cid:123)(cid:122)
online correction

ηt+1
ηt

wi
1

.

(cid:125)

Lemma 7. The online update rule of Eq. (14) has the following properties:

wt+1 ∈ W

(cid:18)

wi

t ≥ wi
1

1 −

(cid:19)

ηt
ηt−1

(normalized)

(restarting)

(cid:18)

(cid:19)

ln

+ ln

≤ ln

0 ≤ ln

pi
t
Mt

wi
t+1
wi
t
wi
t+1
wi
t

1 − ηt + ηt

(telescoping)

ηt
ηt+1
ηt
ηt+1
1
ηt
The restarting property ensures that the weights are never too small, enabling the
mixture to ‘restart’ the learning process and oﬀer tracking guarantees. The loss injection
property will be useful in the theorems to force some series to telescope by ‘injecting’ some
additional loss as was done oﬄine in the proof of Theorem 3.

ηt
1 − ηt
wi
t
wi
1

(1/η-telescoping)

(loss injection)

wi
t+1
wi
1

1 − ηt + ηt

1
ηt+1

pi
t
Mt

1
ηt

+ ln

(17)

(18)

(19)

ln

ln

ln

≤

−

+

(cid:19)

(cid:18)

Proof. Equation (15) follows from the fact that the Soft-Bayes update rule keeps the weights
normalized, and so does the ﬁxed-share rule. Starting from Eq. (14), Eq. (16) follows from
dropping the l.h.s. of the +. Equation (17) follows from dropping the r.h.s. of the +,
dividing by wi
t, taking the log then rearranging. Equation (18) follows from Eq. (17) by
taking pi
1−x (Lemma 13) and rearranging. For Eq. (19),
dividing by wi

t = 0 and from − ln(1 − x) ≤ x

we have:

ln

(cid:18)

= ln

1, taking β = ηt+1
ηt
(cid:18)
wi
wi
t+1
t
wi
wi
1
1
(cid:18)
wi
t
wi
1

≥ β ln

β

1 − ηt + ηt

+ (1 − β)

(cid:19)

(cid:19)

pi
t
Mt
(cid:19)
pi
t
Mt

1 − ηt + ηt

(cid:18)

=

ηt+1
ηt

ln

wi
t
wi
1

1 − ηt + ηt

(cid:19)

,

pi
t
Mt

where we used Jensen’s inequality, with β ∈ [0, 1] since ηt+1 ≥ ηt as required in Eq. (14).
Dividing by ηt+1 and rearranging gives the result.

We will not provide formal results for the self-conﬁdent learner but we make the following

observation.

Remark 8. Using the online correction rule of Eq. (14) with the self-conﬁdent learning rate
of Theorem 6 gives
(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)

(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)1 −

k=1(maxi

≈ 1 −

maxi

(cid:80)t−1

− 1)

=

=

.

pi
t
Mt
k=1(maxi

− 1
pi
k
Mk

2 (cid:80)t

− 1)

maxi

pi
t
Mt
k=1(maxi

− 1
pi
k
Mk

(cid:80)t

− 1)

(cid:80)t

k=1(maxi

− 1)

ηt+1
ηt

pi
k
Mk
pi
k
Mk

2Note that this online correction can also be used with EG.

10

t
Mt

t. On a step t where the mixture makes a bad prediction, pit

≤ 1/wit
Let it := arg maxi pi
t
is large so the weight wit
is small. Considering the restarting property of Eq. (16), this
t
t (in particular) receives a boost 1 − ηt+1
means that the weight wit
toward its prior, hence
ηt
helping the mixture coping with experts that suddenly become good predictors after a long
period of bad predictions—except that this may be one time step too late. Indeed, for the
self-conﬁdent learner the ratio ηt+1
can be close to 1 when the mixture predicts well, which
ηt
means that the weights of bad predictors may still decrease exponentially fast—potentially
resulting in large instantaneous losses if they become good predictors later. To prevent this,
(cid:113) t
we advise replacing ηt+1
in Eq. (14), which ensures that the weights
ηt
t+1
do not decrease faster than O(1/t), while still retaining the quicker restarting property of
the self-conﬁdent learning rate.

(cid:110) ηt+1
ηt

with min

(cid:111)

,

The following generic bound will be used for the various proofs.

Lemma 9. For any sequence of monotone decreasing learning rates ηt ∈ (0, 1), when using
the update rule of Eq. (14), the regret of the mixture M of the N experts with prior weights
w1 compared to the best ﬁxed combination A with weights a is bounded by:

ln

A1:T
M1:T

≤

1
ηT +1

RE(a(cid:107)w1) + ln

η1
ηT +1

+

T
(cid:88)

t=1

ηt
1 − ηt

+

T
(cid:88)

t=1

max
i≤N

ln

wi
t+1
wi
t

.

Proof. Starting from Lemma 4 and similarly to the proof of Theorem 3 we have:

ln

At
Mt

(cid:18)

(cid:18)

≤

=

N
(cid:88)

i=1
N
(cid:88)

i=1

ai 1
ηt

ai 1
ηt

ln

1 − ηt + ηt

+ max

ln

1 +

(cid:19)

(cid:19)

pi
t
Mt

pi
t
Mt

(cid:18)

(cid:18)

(cid:19)

ηt
1 − ηt

pi
t
Mt

(cid:19)

pi
t
Mt

i

i

ln

1 − ηt + ηt

+ max

ln

1 − ηt + ηt

− ln(1 − ηt) .

(20)

Using Lemma 13, − ln(1 − ηt) ≤ ηt
along with the 1/η-telescoping property of Eq. (19)
1−ηt
on the term in the sum and the telescoping property of Eq. (17) on the term in the max
gives:

ln

At
Mt

≤

N
(cid:88)

i=1

ai

(cid:18) 1
ηt+1

ln

wi
t+1
wi
1

−

ln

1
ηt

(cid:19)

wi
t
wi
1

+ max

ln

i

wi
t+1
wi
t

+ ln

ηt
ηt+1

+

ηt
1 − ηt

.

Summing ln At
Mt
the second one) leads to:

over t and telescoping the ﬁrst term (cid:80)

i ai(·) and the third term (but not

ln

A1:T
M1:T

≤

ai ln

N
(cid:88)

i=1

wi

T +1
wi
1

+

T
(cid:88)

t=1

max
i

ln

wi
t+1
wi
t

+ ln

η1
ηT +1

+

T
(cid:88)

t=1

ηt
1 − ηt

.

Finally, taking the worst case wi

T +1 = ai and rearranging gives the result.

We are now ready to show the online bounds. First we track only the time step t.

11

(cid:113) ln N
Theorem 10. When using the update rule in Eq. (14) with the learning rate ηt :=
2N t ,
we have the following regret bound against the best ﬁxed convex combination A of the experts:

ln

A1:T
M1:T

≤ 2(cid:112)2(T + 1)N ln N + ( 1

2 N + ln N ) ln(T + 1) + ln N.

Proof. We start from Lemma 9, and work on the (cid:80)
t maxi term. As in the proof of Theo-
rem 3, the main idea is to make this term telescope by injecting some additional positive
terms. This is done by using the loss injection property of Eq. (18) repeatedly (starting at
t = 1) on all the N − 1 experts i that are not already in the sum, leading to:

T
(cid:88)

t=1

ln

At
Mt

≤

1
ηT +1

RE(a(cid:107)w1)
(cid:124)
(cid:123)(cid:122)
(cid:125)
≤ln N

+N ln

+ N

η1
ηT +1

T
(cid:88)

t=1

ηt
1 − ηt
(cid:124) (cid:123)(cid:122) (cid:125)
η2
t
=ηt+
1−ηt

+

ln

N
(cid:88)

i=1
(cid:124)

wi

T +1
wi
1

(cid:123)(cid:122)
≤0

(cid:125)

≤

1
ηT +1

ln N + N ln

+ N

ηt + N

η1
ηT +1

T
(cid:88)

t=1

T
(cid:88)

t=1

η2
t
1 − ηt

.

Taking ηt :=

2N t , and since 1 − ηt ≥ 1

2 , we obtain:

(cid:113) ln N

T
(cid:88)

t=1

ln

At
Mt

≤ (cid:112)2(T + 1)N ln N + N ln

≤ 2(cid:112)2(T + 1)N ln N + ( 1

(cid:114)

T
(cid:88)

√

T + 1 +

N ln N
2

1
√
t
t=1
(cid:124) (cid:123)(cid:122) (cid:125)
√
T +1
≤2
2 N + ln N ) ln(T + 1) + ln N.

+ ln N

T
(cid:88)

1
t
t=1
(cid:124) (cid:123)(cid:122) (cid:125)
≤1+ln T

This regret is only a factor

√
oﬄine bound using η = (cid:112)ln(N )/(N T ), which is better than the
that would be obtained via the doubling trick.

2 ≈ 1.41 worse on the leading term than the corresponding
2 − 1) ≈ 3.41 factor

2/(

√

√

6.1 Sparse expert set: Tracking M∗
t

√

√

(cid:80)

i∈M∗

t ηt ≈ m

We would like to have an online bound of the order O(
T m ln N ) as in Theorem 3 where
m is the number of ‘good’ experts. Interestingly, setting naively ηt = (cid:112)ln N/(2tmt) works,
but not for the naive reasons. Indeed, merely adapting the proof of Theorem 3 leads to
(cid:80)
T ln N regret in the worst case where only one expert is good for
T − m + 1 steps (mt = 1), and on the m − 1 last steps the other experts are the best
predictors. Let Ti the ﬁrst time step at which expert i is the best expert,3 that is Ti :=
min{t : pi
t := {i : Ti < t} be the set of experts that have been the best
expert at least once (strictly) before time step t, let mt := max{1, |M∗
T +1
and m := mT +1.

t |}, M∗ := M∗

t = maxj pj

t }. Let M∗

3Breaking ties can be done most favourably by picking as the best expert one that was already counted

as such, to avoid introducing new ‘good’ experts.

12

(cid:113) ln N
Theorem 11. When using the update rule in Eq. (14) with the learning rate ηt :=
2mtt ,
we have the following regret bound against the best ﬁxed convex combination A of the experts:

ln

A1:T
M1:T

≤ 2(cid:112)2m(T + 1) ln N + (m + ln N ) ln T + m ln

N
m

+ 1.2m +

(cid:113) 1

2 ln N (1 + ln m) + 3.5 ln N.
√

The proof is in Appendix C. Again, we only get a

2 factor on the leading term compared
to the oﬄine version. One drawback of this algorithm is that any expert that is the best
one even only once will be counted in m. It may be desirable to forget about experts that
have not been best for a long time and thus decrease m. This is left as an open problem.

6.2 Shifting regret

In this subsection we show that the online version of Prod can compete with the best
sequence of convex combinations of experts. Let 1 ≤ K ≤ T and A be a sequence of
constant competitors A1, A2 . . . AK. Each competitor Ak starts at step Tk and ends at step
Tk(cid:48) = Tk+1 − 1. Thus, assuming TK(cid:48) = T , we have A1:T = A1
TK :T . Each
competitor Ak has associated weights ai

k that remain ﬁxed on the interval Tk : Tk(cid:48).

1:T1(cid:48) A2

. . . AK

T2:T2

With the learning rate of Theorem 10, we readily obtain a shifting regret bounded in
T N ln N ), but by tuning the learning rate and still without prior knowledge of

√

O(K ln(T )
K it can be reduced to O((K + ln T )

√

T N ln N ):

Theorem 12. When using the learning rate ηt =
Eq. (14), for T ≥ 2 the K-shifting regret is bounded by:

(cid:113) ln N

2N t ln(t + 3) with the update rule in

ln

A1:T
M1:T

≤ (cid:112)2(T + 1)N ln N

ln(T + 3) + K

(cid:18)

(cid:18) 2
ln N

+

1
ln T

(cid:19)(cid:19)

+

5
4

ln N
N

(1 + ln T )3 +

ln(T + 1) .

N
2

If K were known in advance, then the learning rate can be tuned so that the regret is
O((cid:112)T KN ln(N T )). In the absence of this knowledge one can still have the K inside the
square root by competing with several learning rates as discussed in the conclusion. The
proof is in Appendix C.

7 Conclusion

We have shown new regret guarantees for the Prod algorithm when competing against a
T N ln N ),
convex combination of the experts.
which unlike EG does not depend on the (possibly unbounded) largest gradient. The online
version of the algorithm uses a special form of the ﬁxed-share rule, which simultaneously
makes the algorithm truly online, computable in O(N ) steps per round and also enjoys
strong shifting regret guarantees. A short discussion and some open questions follow.

In particular, we proved that RT = O(

√

13

Alternative approaches As discussed in Section 3, the EG algorithm fails when it en-
counters large gradients. Since these only occur when the weights are close to zero, one
might try an approach based on follow-the-regularised-leader or mirror descent [Hazan,
2016, for an overview]. The natural choice of regulariser is R(w) = − (cid:80)
i log wi, which after
a long calculation can be shown to eliminate the dependence on the largest gradient. The
regret guarantee is slightly worse than for Prod, as it has a logarithmic dependence on T
rather than N so that RT = O((cid:112)N T log(T /N )). Furthermore, the algorithm does not
immediately have tracking guarantees and the projection step involves a line-search, which
naively requires a computation time of O(N log(T )) per round.

Mixtures of learning rates Many of the results in the previous sections have depended
on a speciﬁc ‘optimal’ choice of learning rate that allows Prod or its online variant to adapt
to speciﬁc kinds of structure in the data. The downside of ﬁxing a single learning rate is
that if the structure of interest is not present, then the algorithm may perform badly. In
many cases it is possible to derive a clever scheme for adapting the learning rate online
to achieve the best of several worlds. An alternative is to exploit the special property of
the log-loss and to simply create a meta-agent that mixes over a discrete set of predictors.
For example, in the oﬄine case one can predict using the Bayesian mixture over a set of
Soft-Bayes predictors with learning rates (ηi)K
i=0 where ηi = 2−i and K = log2(T ). If the
uniform prior is used, then the Bayesian mixture will suﬀer an additional regret of only
ln(K) = ln log2(T ) relative to the best soft-Bayes in the class. Provided that the optimal
learning rate lies in [1/T, 1], then this mixture will compete with a learning rate that is at
most a factor of 2 oﬀ for which the penalty is just a factor of 2 at worst. The additional
regret is small enough to be insigniﬁcant. More concerning is that the computation cost
becomes O(KN ) per round. In practice, however, this procedure is easily parallelised and
often leads to signiﬁcant improvement. Additionally, at almost no additional computation
cost we can build a switching mixture [Herbster and Warmuth, 1998, Veness et al., 2012b]
between the individual experts (full Bayes, η = 1) and a Soft-Bayes mixture with rate
(cid:112)ln N/(2N t), to enjoy logarithmic loss (ln(N T ), the cost of switching) against segments
of the sequence where a single expert is the best one, and revert to
T loss for segments
where we need to compete against a combination of the experts. The computation time is
still in O(N ) per round.

√

Computationally eﬃcient logarithmic regret The holy grail would be an O(N )-time
algorithm with logarithmic regret and no dependence on the largest observed gradients of
the (linearised) loss. A reasonable conjecture is that this is not possible, a proof of which
would be quite remarkable. The most eﬃcient algorithm with logarithmic regret is online
Newton step for which the best known computation time is O(N 2), which is prohibitively
large for n (cid:38) 104. Furthermore, the online Newton step suﬀers from the same catastrophic
failures as EG when poorly performing predictors suddenly become good. This limitation
can be overcome via additional regularisation as for mirror descent above, but naively this
pushes the computation cost to at least O(N 4) because the projection step becomes more
complex.

14

Diﬀerent frameworks Another interesting direction is to extend the analysis beyond
the log-loss and the linear mixing. Regarding losses, the most natural ﬁrst step might be
to examine the exp-concave case. Alternatively one could generalise the linear mixture to
(say) a geometric mixture and see if Prod or similar can be applied to this practical setting
[Mattern, 2016].

Acknowledgements We would like to thank the following people for their help: Marc
Bellemare, Guillaume Desjardins, Marc Lanctot, Andrew Lefranq, Jan Leike, R´emi Munos,
Georg Ostrovski, Bernardo Avila Pires, David Saxton, Joel Veness.

15

A Technical results

Lemma 13.

Proof.

∀x < 1 : − ln(1 − x) ≤

x
1 − x

.

− ln(1 − x) = ln

(cid:18) 1

(cid:19)

1 − x

(cid:18)

= ln

1 +

(cid:19)

x
1 − x

≤

x
1 − x

where the inequality follows from ln(1 + x) ≤ x.

Lemma 14 (Love, 1980).

∀x ≥ 0 : ln(1 + x) ≥

(cid:19)−1

(cid:18) 1
x

+

1
2

=

x
1 + x/2

=

2x
2 + x

.

Proof.

Let f (x) := (2 + x) ln(1 + x) − 2x

then f (cid:48)(x) = ln(1 + x) +

− 2 = ln(1 + x) +

and f (cid:48)(cid:48)(x) =

1
1 + x

−

2 + x
1 + x
1
(1 + x)2 =

x
(1 + x)2 .

1
1 + x

− 1

For all x ≥ 0, since f (cid:48)(cid:48)(x) ≥ 0, f is convex, and since f (0) = 0 and f (cid:48)(0) = 0 then f (x) ≥ 0,
which proves the result.

Lemma 15.

∀x ≥ 0 :

ln(1 + x) ≤ x −

x2/2
1 + x

.

Proof. Let f (x) := x − x2/2

1+x − ln(1 + x). Then
x2/2
x
(1 + x)2 −
1 + x
Since f (0) = 0, and f (cid:48)(x) > 0 ∀x > 0, f is positive monotone increasing, which proves the
result.

x2/2
(1 + x)2 .

f (cid:48)(x) = 1 −

1
1 + x

+

=

Corollary 16.

Proof. Using Lemma 15 and 1

∀x ∈ (0, 1

ln

1
2 ] :
x
1−x = 1 + x
1−x :

1
1 − x

− 1 ≤ x/2 + x2.

ln

1
1 − x

(cid:18)

= ln

1 +

(cid:19)

x
1 − x

≤

x
1 − x

− 1
2

x2
(1−x)2
1 + x
1−x

=

x
1 − x

− 1
2

x2
1 − x

− 1
2

x2
1 − x

= x +

= x +

x2
1 − x
x2/2
1 − x

.

16

Hence

1
x

ln

1
1 − x

x/2
1 − x

− 1 ≤

= x/2 +

≤ x/2 + x2

x2/2
1 − x

where the last inequality holds if x ≤ 1
2 .

Lemma 17.

∀x ≥ 0, ∀η ∈ (0, 1) :

(x − 1) ≤

ln(1 − η + ηx) +

1
η

η
1 − η

(x − 1)2.

Proof. Using log(1 + x) ≥ x

1+x :

1
η

ln(1 − η + ηx) =

ln(1 + η(x − 1)) ≥

= (x − 1) −

x − 1
1 + η(x − 1)

η(x − 1)2
1 + η(x − 1)

1
η

≥ (x − 1) −

η(x − 1)2
1 − η

where the last inequality holds with x ≥ 0. Rearranging gives the result.

Lemma 18. For all t = 1, 2, 3, . . .:

(cid:32)

− ln

1 −

ln(t + 3)
ln(t + 2)

(cid:114) t − 1
t

(cid:33)

≤ ln(t) + 1.6.

Proof. By exhaustive search, the result holds for all t ∈ [1..30]. Now consider t ≥ 30 for the
rest of the proof. Observe that

ln(t + 3)
ln(t + 2)

= 1 +

ln(1 + 1/(t + 2))
ln(t + 2)

≤ 1 +

1
(t + 2) ln(t + 2)

.

Therefore:

(cid:32)

− ln

1 −

(cid:33)

(cid:114) t − 1
t

ln(t + 3)
ln(t + 2)

t(1 − (cid:112)1 − 1/t

(cid:124)
(cid:125)
(cid:123)(cid:122)
≤1−1/(2t)
(cid:19)

(cid:18)

1
2 −

1
ln(t + 2)

= ln(t) − ln

(cid:18)

(cid:18)

≤ ln(t) − ln

t −

1 +

(cid:19)

(cid:19)
(cid:112)t(t − 1)

1
(t + 2) ln(t + 2)


) −

(cid:112)t(t − 1)
(t + 2) ln(t + 2)




≤ ln(t) − ln

≤ ln(t) − ln

≤ ln(t) + 1.6 .

(cid:18)

1
2 −

1
ln(30 + 2)

(cid:19)

17

B Reverse Jensens’ inequalities

Lemma 19. Let η ∈ (0, 1

2 ], a ∈ W, and ∀i ∈ [N ] : qi ≥ 0 then:

ln

aiqi ≤

N
(cid:88)

i=1

N
(cid:88)

i=1

ai

1
η

ln (1 − η + ηqi) + max
i≤N

(qi − 1) + η2.

η
2

Proof. Let lnη(q) := 1

η ln(1 − η + ηq), and let ¯η := η

1−η . By concavity of lnη, for q ∈ [0, Q]:

lnη(q) ≥ lnη(0) +

(lnη(Q) − lnη(0))

q
Q

=

ln(1 − η) +

(1 − η)

1 +

−

ln(1 − η)

(cid:18)

(cid:19)(cid:21)

η
1 − η

Q

1
η

(cid:19)

1
η
1
η

(cid:20)

ln

(cid:18) 1
η

q
Q
ln(1 + ¯ηQ)
ηQ

=

ln(1 − η) + q

=: g(q).

Now since d
Therefore:

dq (ln q −g(q)) = 1

q − ln(1+¯ηQ)

ηQ

, the maximum of ln q −g(q) is found at ˆq = ηQ

ln(1+¯ηQ) .

ln q − g(q) ≤ ln ˆq − g(ˆq) ≤ ln

−

ln(1 − η) − 1.

(21)

ηQ
ln(1 + ¯ηQ)

1
η

Using Lemma 14, ln(1 + ¯ηQ) ≥ ¯ηQ

1+¯ηQ/2 =

1−η+ηQ/2 and thus:

ηQ

ln q − g(q) ≤ ln(1 − η + ηQ/2) −

ln(1 − η) − 1

= ln(1 + η(Q/2 − 1)) −

ln(1 − η) − 1.

1
η

1
η

Hence, using Corollary 16 with η ≤ 1

2 , together with ln(1 + x) ≤ x:

ln q − g(q) ≤

(Q − 1) + η2.

Finally, by linearity of g we have g((cid:80)

i aiqi) = (cid:80)

(cid:88)

ln

aiqi ≤

(cid:88)

aig(qi) +

(Q − 1) + η2 ≤

i

i

i aig(qi) and thus since lnη(q) ≥ g(q):
η
2

(Q − 1) + η2.

ai lnη(qi) +

(cid:88)

i

Substituting lnη and Q by their deﬁnitions ﬁnishes the proof.

Proof. (Lemma 4) The beginning of the proof matches that of Lemma 19, and thus we start
from Eq. (21). Since ln(1 + ¯ηQ) ≥ ¯ηQ

1+¯ηQ = ηQ

1−η+ηQ :

η
2

η
2

ln ˆq − g(ˆq) ≤ ln(1 − η + ηQ) −

ln(1 − η) − 1

(cid:18)

≤ ln

1 +

(cid:19)

Q

−

η
1 − η

− 1

ln(1 − η) − 1

1
η

(cid:18) 1
η

(cid:124)

(cid:123)(cid:122)
= 1−η
η

(cid:19)

(cid:125)

18

and since from Lemma 13 − ln(1 − η) ≤ η

1−η = ¯η:

ln ˆq − g(ˆq) ≤ ln(1 + ¯ηQ).

By linearity of g, we have g((cid:80)

i aiqi) = (cid:80)

i aig(qi) and thus:

(cid:88)

ln

aiqi ≤

(cid:88)

i

i

(cid:88)

i

aig(qi) + ln(1 + ¯ηQ) ≤

ai lnη(qi) + ln(1 + ¯ηQ)

where the last inequality follows from lnη(q) ≥ g(q).

C Proofs for the main results

Proof. (Theorem 5) Using ln(x) ≤ x − 1 followed by Lemma 17 and the deﬁnition of the
update rule Eq. (7) leads to:

T
(cid:88)

t=1

ln

At
Mt

(cid:88)

≤

(cid:18) At
Mt

(cid:19)

(cid:88)

N
(cid:88)

− 1

=

ai

(cid:18) pi
t
Mt

(cid:19)

− 1

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:32)

(cid:32)

1
η

1
η

ai

ai

i

i

t

i=1

(cid:18)

ln

1 − η + η

(cid:19)

+

pi
t
Mt

η
1 − η

(cid:18) pi
t
Mt

− 1

(cid:19)2(cid:33)

ln

wi
t+1
wi
t

+

η
1 − η

(cid:18) pi
t
Mt

(cid:19)2(cid:33)

− 1

(cid:88)

ai ln

i

wi

T +1
wi
1

+

η
1 − η

≤

ln N +

η
1 − η

max
i

(cid:88)

t

(cid:88)

ai (cid:88)

i
(cid:18) pi
t
Mt

t
(cid:19)2

− 1

(cid:18) pi
t
Mt

(cid:19)2

− 1

=

1 − η
η

ln N +

η
1 − η

max
i

(cid:18) pi
t
Mt

(cid:88)

t

(cid:19)2

− 1

+ ln N .

≤

=

=

t

t

t

1
η

1
η

The result is completed by substituting the deﬁnitions.

Proof. (Theorem 6) For the ﬁrst entry in the minimum we use the fact that log x ≤ x − 1:

T
(cid:88)

t=1

ln

At
Mt

(cid:88)

≤

t

(cid:18) At
M

(cid:19)

(cid:88)

N
(cid:88)

− 1

=

t

i=1

ai

(cid:18) pi
t
Mt

(cid:19)

− 1

≤

(cid:88)

t

max
i

(cid:18) pi
t
Mt

(cid:19)

− 1

.

For the other terms we use Lemma 19 to obtain:

ln

At
Mt

= ln

(cid:32)

(cid:88)

(cid:33)

ai pi
t
Mt

i

ai 1
η

(cid:88)

=

i

ln

wi
t+1
wi
t

+

(cid:88)

≤

(cid:18)

ln

ai 1
η
(cid:18) pi
t
Mt

max
i

i

η
2

(cid:19)

− 1

+ η2 .

19

1 − η + η

(cid:19)

pi
t
Mt

+

η
2

(cid:18) pi
t
Mt

max
i

(cid:19)

− 1

+ η2

Therefore

T
(cid:88)

t=1

ln

At
Mt

≤

ln N +

1
η

η
2

T
(cid:88)

t=1

(cid:18) pi
t
Mt

max
i

(cid:19)

− 1

+ η2T.

Proof. (Theorem 11) As for the proof of Theorem 10 we start from Lemma 9:

ln

A1:T
M1:T

≤

1
ηT +1

RE(a(cid:107)w1) + ln

η1
ηT +1

+

T
(cid:88)

t=1

ηt
1 − ηt

+

T
(cid:88)

t=1

max
i≤N

ln

wi
t+1
wi
t

√

but instead of complementing the missing terms for each expert from t = 1 to T (which
would lead to O(m
T ) if all Ti ≈ T ) we complement using the loss injection property of
Eq. (18) only for expert i from Ti to T and rely on the restarting property of Eq. (16) to
start with a high enough weight wi
Ti

≈ 1/t. Telescoping the series, we obtain:

ln

A1:T
M1:T

≤

1
ηT +1

ln N +



ln

(cid:88)

i∈M∗

wi
T +1
wi
Ti

+ ln

+

ηTi
ηT

T
(cid:88)

t=Ti

ηt
1 − ηt



 .

Using the restarting property of Eq. (16):

(cid:88)

−

i∈M∗

(cid:18)

ln

1 −

(cid:19)

ηTi
ηTi−1

(cid:88)

ln

i∈M∗

wi
T +1
wi
Ti

(cid:88)

≤

i∈M∗

wi

T +1

(cid:16)

1 −

wi
1

(cid:17) =

(cid:88)

(cid:34)

ln

wi

T +1
wi
1

(cid:18)

− ln

1 −

(cid:19)(cid:35)

ηTi
ηTi−1

≤ m ln

−

ln

1 −

(cid:88)

i∈M∗
(cid:32)

(cid:115)

= −

ln

1 −

ηTi
ηTi−1
(cid:18)

i∈M∗
(cid:19)

ηTi
ηTi−1

,

(cid:33)

mTi−1(Ti − 1)
mTiTi
(cid:33)

ln

N
m

(cid:88)

i∈M∗

(cid:88)

i∈M∗

(cid:88)

i∈M∗

≤ −

ln

1 −

= −

ln

1 −

1 −

(cid:32)

(cid:114) Ti − 1
Ti

(cid:88)

i∈M∗

(cid:18)

(cid:114)

(cid:19)

1
Ti

≤

ln (2Ti) = m ln 2 +

ln Ti

(cid:88)

i∈M∗

√

where we used
include the expert i, which will be added at Ti + 1:

1 − x ≤ 1 − x/2. For the next term, remember that M∗
Ti

does not yet

(cid:88)

i∈M∗

ln

ηTi
ηT

(cid:88)

ln

≤

i∈M∗

(cid:115)

mT
mTiTi

= 1

2 (m ln m − ln((m − 1)!) + 1

2

(cid:88)

i∈M∗

ln

T
Ti

≤ 1

2 (m + ln m) +

ln T − 1
2

(cid:88)

i∈M∗

ln Ti

m
2

20

where we used ln((m − 1)!) = ln(m!) − ln m and ln(m!) ≥ m ln(m) − m. For the next term,
we have:

(cid:88)

T
(cid:88)

ηt
1 − ηt

=

(cid:88)

T
(cid:88)

(cid:16)

ηt +

i∈M∗

t=Ti

i∈M∗

t=Ti

(cid:17)

=

T
(cid:88)

t=1

η2
t
1 − ηt
(cid:124) (cid:123)(cid:122) (cid:125)
≤2η2
t

mt+1(ηt + 2η2
t )

=

T
(cid:88)

t=1

mt(ηt + 2η2

t ) +

(cid:0)ηTi + 2η2
Ti

(cid:1) .

(cid:88)

i∈M∗

For the rightmost term, we take the worst case of largest learning rates, that is for mt = t
up to t = m:

(cid:114)

m
(cid:88)

t=1

T
(cid:88)

(cid:114)

(cid:88)

i∈M∗
T
(cid:88)

t=1

ηTi + 2η2

Ti ≤

ln N
2t2 +

ln N
t2 ≤

(cid:113) 1

2 ln N (1 + ln m) + 2 ln N ,

mt(ηt + 2η2

t ) =

mt ln N
2t

m ln N
2t
≤ (cid:112)2m(T + 1) ln N + (1 + ln T ) ln N.

+ ln N

1
t

t=1

t=1

≤

(cid:114)

T
(cid:88)

+ ln N

1
t

Putting it all together we have:

ln

A1:T
M1:T

≤ 2(cid:112)2m(T + 1) ln N +

(cid:16) m
2

(cid:17)

+ ln N

ln T + 1
2

(cid:88)

i∈M∗

ln Ti + m ln

N
m

2 m + 1

+ m ln 2 + 1

2 ln m +
≤ 2(cid:112)2m(T + 1) ln N + (m + ln N ) ln T + m ln

2 ln N (1 + ln m) + 3 ln N

(cid:113) 1

N
m

(cid:113) 1

+ 1.2m +

2 ln N (1 + ln m) + 3.5 ln N

which concludes the proof.

Proof. (Theorem 12) The beginning of the proof is similar to that of Theorem 10, using
ﬁrst Lemma 4:

ln

A1:T
M1:T

=

ln

K
(cid:88)

k=1

K
(cid:88)

Ak
Tk:Tk(cid:48)
MTk:Tk(cid:48)
(cid:34) N
(cid:88)

Tk(cid:48)
(cid:88)

k=1

t=Tk

i=1

≤

=

T
(cid:88)

t=1
(cid:124)

(cid:18)

ai
k

1
ηt

(cid:19)(cid:35)

pi
t
Mt

ln

1 − ηt + ηt

+ max

ln

1 + ¯ηt

(cid:18)

i

(cid:18)

(cid:123)(cid:122)
(B)

ln

1 − ηt + ηt

(cid:19)

pi
t
Mt

(cid:19)

.

pi
t
Mt

(cid:125)

max
i

ln

1 + ¯ηt

(cid:18)

(cid:123)(cid:122)
(A)

+

pi
t
Mt

(cid:19)

(cid:125)

K
(cid:88)

Tk(cid:48)
(cid:88)

N
(cid:88)

k=1

i=1

t=Tk
(cid:124)

ai
k

1
ηt

21

For (A), we ﬁrst apply the same transformation as in Eq. (20), then we repeatedly use the
loss injection property of Eq. (18) and the telescoping property of Eq. (17) as in the proof
of Theorem 10. It can be shown that maxN ≥2,t≥1 ηt ≤ 3/5, hence:

(A) ≤ N ln

+ N

η1
ηT +1

T
(cid:88)

(cid:18)

t=1

ηt +

(cid:19)

η2
t
1 − ηt

≤ N ln

√

ln(4)

T + 1

ln(T + 4)
√

(cid:113) 1

+

2 N ln N ln(T + 3)

T
(cid:88)

1
√
t

+

5
2

ln N
2N

(ln(T + 3))2

T
(cid:88)

t=1

1
t

√

t=1
5
4

ln N
N

( ln(T + 3)
(cid:123)(cid:122)
(cid:125)
≤1+ln T,∀T ≥2

(cid:124)

)2(1 + ln T )

≤

ln(T + 1) +

2N ln N ln(T + 3)

T +

≤

ln(T + 1) +

2T N ln N ln(T + 3) +

√

5
4

ln N
N

(1 + ln T )3

N
2

N
2

For (B), using the 1/η-telescoping property of Eq. (19) and then telescoping the series gives:

(B) ≤

(cid:32)

N
(cid:88)

i=1

ai
k

1
ηTk+1

ln

wi

Tk+1
wi
1

−

ln

1
ηTk

wi
Tk
wi
1

(cid:33)

.

Now, with the restarting property of Eq. (16),

ln

≥ ln

1 −

together with

wi
Tk
wi
1

(cid:16)

(cid:17)

ηTk
ηTk −1

Lemma 18, and also with (cid:80)N

i=1 ai

k ln

≤ ln N we have:

wi

Tk+1
wi
1

1
ηTk+1

(cid:114)

≤

1
ηTk


(B) ≤

ln N −

ln

1 −

ln N +

(ln(T ) + 1.6)

(cid:18)

(cid:19)

≤

ηTk
ηTk−1

1
ηT +1

1
ηT +1

2N (T + 1)
ln N


1 +

ln N
ln(T + 4)


 ≤ (cid:112)2(T + 1)N ln N

+ 1.6/ ln(T + 4)
(cid:125)

(cid:124)

(cid:18) 2
ln N

+

1
ln T

(cid:19)

,

(cid:123)(cid:122)
≤1
(cid:18) 2
ln N

(cid:19)(cid:19)

+

1
ln T

ln

A1:T
M1:T

≤ (cid:112)2(T + 1)N ln N

ln(T + 3) + K

(cid:18)

+

5
4

ln N
N

(1 + ln T )3 +

ln(T + 1)

N
2

which was to be proven.

D Failure of EG and OGD

Proof. (Theorem 2) We start by proving the claim for EG. The theorem will follow by
considering two examples. For the ﬁrst, let pa
t = 1 for all t. Then according to
the EG update rule in Eq. (8):

t = 0 and pb

wa

t ∝ wa
1

and

wb

t ∝ wb

1 exp

(22)

(cid:32)
η

(cid:33)

.

t
(cid:88)

s=1

1
wb
s

22

It is easy to see that wb

s ∈ [1/2, 1], which means that

wb

t =

(cid:16)

exp

1 + exp

η (cid:80)t
(cid:16)

s=1
η (cid:80)t

(cid:17)

1
wb
s

s=1

1
wb
s

(cid:17) ≤

exp(2ηt)
1 + exp(2ηt)

.

The best mixture in hindsight in this case assigns all mass to the second expert and suﬀers
no loss. Therefore the regret

RT =

T
(cid:88)

t=1

ln

(cid:19)

(cid:18) 1
wb
t

≥

min{1/η,T }
(cid:88)

t=1

ln

(cid:18) 1 + exp(2)
exp(2)

(cid:19)

1
10

≥

min

, T

,

(cid:27)

(cid:26) 1
η

which proves the result for small learning rates η ≤ T ε−1. From now on suppose that T is
reasonably large and η ≥ T ε−1. Now we consider a diﬀerent sequence of expert predictions.
Let the ﬁrst expert be a poor predictor for the ﬁrst T /2 rounds, and subsequently let the
prediction quality of each expert alternate. Formally,

pa
t =






0 if t ≤ T /2
1 if t > T /2 and t is even
0 otherwise .

pb
t =


1

0

1

if t ≤ T /2
if t > T /2 and t is even
otherwise .

Notice that this is the same sequence as considered in the ﬁrst part until t = T /2. Therefore
by Eq. (22) for t = T /2 + 1 we have





wa

T /2+1 =

1 + exp

η





−1





T /2
(cid:88)

s=1

1
wb
s

≤ exp(−ηT /2) .

This means that in a single round the predictor suﬀers loss − log exp(−ηT /2) = ηT /2, which
may already be quite large. But there are still many rounds to go and things do not get
better. In round T /2 + 2 we have

wb

T /2+2 =

wb

T /2+1

T /2+1 + wa
wb
T /2+1 exp
exp (−η exp(ηT /2)))
exp(−ηT /2)

≤

(cid:16)

η/wa

T /2+1

(cid:17) ≤

≤ exp(−ηT /2)

(cid:16)

exp

−η/wa

T /2+1

(cid:17)

wa

T /2+1

(23)

and so on. Therefore the loss of the EG algorithm over the ﬁnal T /2 rounds is at least
ηT b/4 = Ω(T 1+ε). For this sequence the loss of the best mixture in hindsight is O(T ) and
hence the regret of EG is at least Ω(T 1+ε), which completes the proof for EG. Moving to
gradient descent. We use a similar example where in rounds t < T the ﬁrst expert has
t = 0 and the second has pb
pa

t = 1. An easy calculation shows that
(cid:27)

(cid:26)

≥ max

(cid:110)

1, wb

t +

(cid:111)

.

η
2

wb

t+1 = max

1, wb

t +

η
2wb
t

23

Therefore since wb
T = 1. In this case let pa
which leads to inﬁnite regret. On the other hand if T ≤ 1 + 1/η, then let pa
so that the minimum loss in hindsight vanishes. Therefore the regret of OGD is at least

1 = 1/2, if T > 1 + 1/η, then wb

T = 1 and pb
T = 0 and pb

T = 0,
T = 1

RT =

T
(cid:88)

t=1

ln

(cid:18) 1
wb
t

(cid:19)

T /2
(cid:88)

≥

ln

(cid:18)

(cid:19)

1
1/2 + ηt

t=1
t ≥ 1/2 so that max{1, wb

= Ω(T ) ,

where we used the fact that wb

t + η/(2wb

t )} ≤ max{1, wb

t + η}.

Remark 20. Notice that the second inequality in Eq. (23) is rather loose when η is large. In
fact for learning rates η = O(T ε−1) one can show the regret of EG grows super-exponentially.

E A multi-learning-rate Soft-Bayes

ML-Prod [Gaillard et al., 2014] was designed for [0,1] losses for linear optimization. It uses
one learning rate per expert to be able to track the best of them with a loss that does not
depend on the other experts. Interestingly, using the gradient trick on Prod with excess
losses also leads to the same update as Soft-Bayes since the linearized loss of the algorithm
is (cid:80)
t/Mt) = −1. We provide similar results as for ML-
Prod, but with the online correction rule. First, using ﬁxed learning rates ηi, we deﬁne the
mixture as:

t∂(− ln Mt)/∂wi

t = (cid:80)

i≤N wi

t(−pi

i wi

N
(cid:88)

T
(cid:89)

M1:T :=

wi
1

((1 − ηi)Mt + ηipi
t)

i=1
M1:t
M<t

=

t=1
(cid:80)
i wi
(cid:80)
i wi

tηipi
t
tηi
t=1((1 − ηi)Mt + ηipi
t)
M1:T

(cid:81)T

Mt =

wi

t+1 := wi
1

= wi
t

(cid:18)
1 − ηi + ηi pi
t
Mt

(cid:19)

i wi

t[(1 − ηi)Mt + ηipi

where the equation for Mt follows from Mt = (cid:80)
t] and algebra. Observe
that for now the weights are kept normalized. For time-dependent learning rates, we apply
the online correction rule Eq. (14) to the update of the weights, which gives the following:
(cid:80)
i wi
tpi
tηi
t
(cid:80)
tηi
i wi
t
pi
t
Mt

M1:t
M<t
(cid:18)

t+1 := wi
t

t + ηi
t

1 − ηi

Mt :=

(cid:19) ηi
t+1
ηi
t

ηi
t+1
ηi
t

wi
1 .

(25)

(24)

1 −

wi

=

+

(cid:18)

(cid:19)

All the properties of Lemma 7 still hold, just with additional indices on the learning rates.
Furthermore, the weights always remain positive since pi

t ≥ 0.

Lemma 21. For the algorithm deﬁned by Eqs. (24) and (25), with sequences ηi
t ∈ (0, 1) of
monotonically decreasing learning rates, we have the following regret guarantee against the
best ﬁxed convex combination of the experts A in hindsight such that At := (cid:80)N

i=1 aipi
t:

ln

A1:T
M1:T

≤

(cid:34)

N
(cid:88)

i=1

ai

1

¯ηi
T +1

ln

wi

T +1
wi
1

+

T
(cid:88)

t=1

¯ηi
t

(cid:18) pi
t
Mt

(cid:19)2

− 1

+ ln

(cid:35)

,

wi

T +1
wi
1

with

¯ηi
t :=

ηi
t
1 − ηi
t

.

24

Proof. Using ln(x) ≤ x−1, then Lemma 17 on the second line, the 1/η-telescoping property
of Eq. (19) on the third line, and telescoping on the fourth line we have:

T
(cid:88)

t=1

ln

At
Mt

(cid:88)

≤

(cid:18) At
Mt

(cid:19)

(cid:88)

N
(cid:88)

− 1

=

ai

(cid:18) pi
t
Mt

t

i=1

(cid:19)

− 1

=

(cid:88)

ai (cid:88)

(cid:19)

− 1

(cid:18) pi
t
Mt
(cid:19)2(cid:35)

− 1

(cid:19)2(cid:35)

− 1

i

t

(cid:18) pi
t
Mt
(cid:18) pi
t
Mt

(cid:88)

ai (cid:88)

≤

(cid:88)

ai (cid:88)

≤

(cid:18)

(cid:34)

ln

1
ηi
t
(cid:34)(cid:18) 1
ηi
t+1

1 − ηi

t + ηi
t

ln

wi
t+1
wi
1

−

1

ηi
T +1

ln

wi

T +1
wi
1

+

T
(cid:88)

t=1

t

i

i

i

i

t

t
(cid:34)

(cid:34)

(cid:88)

ai

=

(cid:88)

ai

=

(cid:19)

+

pi
t
Mt

ηi
t
1 − ηi
t

wi
t
wi
1

ln

1
ηi
t
(cid:18) pi
t
Mt

¯ηi
t

(cid:19)

+ ¯ηi
t

(cid:19)2(cid:35)

− 1

T +1

1 − ηi
ηi
T +1

ln

wi

T +1
wi
1

+

T
(cid:88)

t=1

¯ηi
t

(cid:18) pi
t
Mt

(cid:19)2

− 1

+ ln

(cid:35)

,

wi

T +1
wi
1

which with (1 − ηi

T +1)/ηi

T +1 = ¯ηi

T +1 proves the claim.

Unfortunately, applying the correction rule with a diﬀerent ratio for each i means that
the weights may not be normalized anymore, so we cannot use wi
T +1 ≤ 1, but it can be
shown that the correction rule still ensures that they do not grow faster than O(ln ln T ) if
the ratios pi

t/Mt are bounded, as for ML-Prod.

More precisely, it can be shown that wi

T +1 is bounded above by O((cid:80)

i ln(1/ηi

T +1)): From
t ≥ 0), and 1 − 1/x ≤ ln x

t+1/ηi

t ≤ 1 together with ηi

t < 1 (ensuring 1 − ηi

Eq. (25), using ηi
we can deduce

(cid:32)

(cid:18)

wi

t+1 ≤

wi
t

1 − ηi

t + ηi
t

(cid:19)

pi
t
Mt

(cid:19)(cid:19)ηi
t+1
(cid:19)
ηi
t

(cid:19)

+ wi

1 ln

(cid:33)

ηi
t
ηi
t+1

(cid:88)

i

≤

≤

≤

(cid:88)

i
(cid:88)

i
(cid:88)

i
(cid:88)

i

i

wi

t +

wi

1 ln

(cid:88)

i
(cid:88)

i
(cid:88)

wi

1 +

wi

1 ln

wi

1 +

wi

1 ln

i

(cid:32)

ηi
t
ηi
t+1
ηi
1
ηi
t+1
¯ηi
1
¯ηi
t+1
(cid:33)

,

,

¯ηi
1

¯ηi
T +1

25

∀j ∈ [N ] : 0 ≤ wj

T +1 ≤

(cid:88)

wi
1

1 + ln

where we used (cid:80)
t on the third line.
Finally, let V i

i wi

tηi
t

pi
t
Mt

= (cid:80)

i wi

tηi

t from Eq. (24) on the second line and telescoping over

k := (cid:80)

t≤k

(cid:16) pi
t
Mt

(cid:17)2

− 1

. Then, from Lemma 21, setting ¯ηi

t :=

(cid:114)

ln(N )/2
ln N +V i

t−1

can be shown to lead to an upper bound on the regret of



(cid:20)

O



(cid:18)

ln N + ln

1 + max

ln(1 + V j
T )

j

(cid:115)

(cid:19)(cid:21)





V i
T
ln N

1 +

simultaneously for all i.

F Sequence prediction with experts with disjoint supports

In this section we consider that the experts have disjoint supports, that is, for any observa-
tion exactly one expert predicts it with positive probability:4 ∀t ∈ [1..T ], |{i ∈ [1..N ]|pi
t >
0}| = 1. This happens in particular if the experts are designed so that each expert i predicts
the symbol of index i, that is ∀t, i : pi

t(xt = i) = 1 when considering that X = [1..N ].

In this setting, we show that the Soft-Bayes rule with a learning rate ηt of

1
t+c recovers
exactly some well-known density estimators such as Laplace’s rule of succession and the
minmax-optimal KT estimator.

Let ˆit be the index of the model that places positive probability for the current obser-
ˆit
ˆit
t p
t = w
t .

vation xt at time t. Then Mt = (cid:80)

tpi

i wi

Let ni

t := (cid:80)t

s=1[[ˆis = i]] be the number of times up to t where the expert i is correct.

Then we have the following property.

Theorem 22. If the experts have disjoint supports and uniform prior wi
a learning rate ηt := 1
t+c makes the mixture predict

1 = 1/N , then using

∀t : Mt+1(xt+1 = i) =

t + c
ni
N
t + c

pi
t+1.

Proof. We proceed by induction on the weights:

ˆit
p
t
Mt

w

ˆit
ˆit
t+1 = w
t (1 − ηt + ηt
t+1 = wi
t+1 = wi

t(1 − ηt)
t(1 − ηt) + ηt[[ˆit = i]]

∀i (cid:54)= ˆit : wi
that is ∀i : wi

) = wi

t(1 − ηt) + ηt

4But a single expert can still place positive probability over several observations, as long as there is no

overlap with any other expert.

26

Now with ηt = 1

t+c , observe5 that

ηt
1−ηt

= 1

t−1+c = ηt−1. Then

wi

t+1 = wi

1
ηt

wi

t+1 =

wi

1(1 − ηt) + ηt[[ˆit = i]]
1 − ηt
1 + [[ˆit = i]]
ηt
1
ηt−1

t + [[ˆit = i]]

wi

=

=

t
(cid:88)

s=1

1 +

wi

1
η0
c
N
t + c
ni
N
t + c

+ ni
t,

1
t + c

wi

t+1 =

hence wi

t+1 =

[[ˆis = i]]

(by induction)

which with Mt+1 = wi

t+1pi

t+1 for i = ˆit+1 proves the claim.

In particular, for experts such that pi

t ∈ {0, 1} and compared to the best constant convex

combination of the experts in hindsight (still with disjoint support),

• setting c = 1 recovers Perks’ estimator [Perks, 1947, Hutter, 2013], with a regret of
O(m ln T ) where m is the number of experts that make at least one good prediction,

• setting c = N recovers Laplace’s rule of succession, with a regret of O(N ln T
• setting c = N/2 recovers the KT estimator [Krichevsky and Troﬁmov, 1981], with a

N ),

regret of O( N

2 ln T ).

See [Hutter, 2013] for more details and comparison of these estimators.

Indeed Perks’ estimator with learning rate ηt = 1

We can draw an interesting parallel between these estimators and the diﬀerent learning
rates for competing against a ﬁxed combination of the experts (with non-disjoint supports).
t+1 is a sparse estimator: It pays a cost
pays

of log t each time a symbol is seen for the ﬁrst time, just like a learning rate of ηt = 1√
t
a cost of
estimator with a learning rate of
be introduced, similarly to a learning rate of

t for convex combinations when an expert is good for the ﬁrst time. The KT
t+N/2 minimizes the worst case where all symbols must
for convex combinations.

1√

√

1

tN

References

Adam Kalai and Santosh Vempala. Eﬃcient algorithms for universal portfolios. Journal of

Machine Learning Research, 3(Nov):423–440, 2002.

Mark Herbster and Manfred K. Warmuth. Tracking the best expert. Machine Learning, 32

(2):151–178, August 1998.

Thomas M Cover. Universal portfolios. Mathematical ﬁnance, 1(1):1–29, 1991.

Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online

convex optimization. Machine Learning, 69(2):169–192, 2007.

5Interestingly, this property exists only for this type of learning rate, and not for example for ηt ∝ 1√
t

.

27

Haipeng Luo, Alekh Agarwal, Nicol`o Cesa-Bianchi, and John Langford. Eﬃcient second
order online learning by sketching. In Advances in Neural Information Processing Systems,
pages 902–910, 2016.

David P Helmbold, Robert E Schapire, Yoram Singer, and Manfred K Warmuth. On-line
portfolio selection using multiplicative updates. Mathematical Finance, 8(4):325–347,
1998.

Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent.
In Proceedings of the Twentieth International Conference on Machine Learning, pages
928–935, 2003.

Joel Veness, Peter Sunehag, and Marcus Hutter. On ensemble techniques for AIXI approx-
imation. In International Conference on Artiﬁcial General Intelligence, pages 341–351.
Springer, 2012a.

John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Eﬃcient projections
onto the l 1-ball for learning in high dimensions. In Proceedings of the 25th international
conference on machine learning, pages 272–279. ACM, 2008.

Nicol`o Cesa-Bianchi, Yishay Mansour, and Gilles Stoltz. Improved second-order bounds for

prediction with expert advice. Machine Learning, 66(2-3):321–352, 2007.

Pierre Gaillard, Gilles Stoltz, and Tim van Erven. A second-order bound with excess losses.

Journal of Machine Learning Research, W&CP: COLT, 35:176–196, 2014.

Amir Sani, Gergely Neu, and Alessandro Lazaric. Exploiting easy data in online optimiza-

tion. In Advances in Neural Information Processing Systems, pages 810–818, 2014.

Christopher Mattern. On Statistical Data Compression. PhD thesis, Technische Universit¨at

Ilmenau, Fakult¨at f¨ur Informatik und Automatisierung, Feb 2016.

Yoav Freund, Robert E Schapire, Yoram Singer, and Manfred K Warmuth. Using and
In Proceedings of the twenty-ninth annual ACM

combining predictors that specialize.
symposium on Theory of computing, pages 334–343. ACM, 1997.

Alexey Chernov and Vladimir Vovk. Prediction with expert evaluators’ advice. In Algo-
rithmic Learning Theory, volume 5809 of Lecture Notes in Artiﬁcial Intelligence, pages
8–22. Springer, 2009.

Wouter M. Koolen, Dmitry Adamskiy, and Manfred K. Warmuth. Putting bayes to sleep.

In Advances in Neural Information Processing Systems, pages 135–143, 2012.

Peter Auer and Claudio Gentile. Adaptive and self-conﬁdent on-line learning algorithms.

In Proceedings of COLT’00, pages 107–117, 2000.

Nicol`o Cesa-Bianchi, Yoav Freund, David Haussler, David P. Helmbold, Robert E. Schapire,
and Manfred K. Warmuth. How to use expert advice. J. ACM, 44(3):427–485, May 1997.

Elad Hazan.

Introduction to online convex optimization. Foundations and Trends R(cid:13) in

Optimization, 2(3-4):157–325, 2016.

28

Joel Veness, Kee Siong Ng, Marcus Hutter, and Michael Bowling. Context tree switching.

In Data Compression Conference (DCC), 2012, pages 327–336. IEEE, 2012b.

E. R. Love. 64.4 Some logarithm inequalities. The Mathematical Gazette, 64(427):55–57,

1980.

Wilfred Perks. Some observations on inverse probability including a new indiﬀerence rule.

Journal of the Institute of Actuaries, 73(2):285–334, 1947.

Marcus Hutter. Sparse adaptive Dirichlet-multinomial-like processes. Journal of Machine

Learning Research, W&CP: COLT, 30:432–459, 2013.

R Krichevsky and V Troﬁmov. The performance of universal encoding. IEEE Transactions

on Information Theory, 27(2):199–207, 1981.

29

