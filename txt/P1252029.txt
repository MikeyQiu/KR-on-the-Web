Deep Pyramidal Residual Networks

Dongyoon Han∗
EE, KAIST
dyhan@kaist.ac.kr

Jiwhan Kim∗
EE, KAIST
jhkim89@kaist.ac.kr

Junmo Kim
EE, KAIST
junmo.kim@kaist.ac.kr

7
1
0
2
 
p
e
S
 
6
 
 
]

V
C
.
s
c
[
 
 
4
v
5
1
9
2
0
.
0
1
6
1
:
v
i
X
r
a

Abstract

Deep convolutional neural networks (DCNNs) have
shown remarkable performance in image classiﬁcation
tasks in recent years. Generally, deep neural network ar-
chitectures are stacks consisting of a large number of con-
volutional layers, and they perform downsampling along
the spatial dimension via pooling to reduce memory us-
age. Concurrently, the feature map dimension (i.e., the num-
ber of channels) is sharply increased at downsampling lo-
cations, which is essential to ensure effective performance
because it increases the diversity of high-level attributes.
This also applies to residual networks and is very closely
In this research, instead of
related to their performance.
sharply increasing the feature map dimension at units that
perform downsampling, we gradually increase the feature
map dimension at all units to involve as many locations as
possible. This design, which is discussed in depth together
with our new insights, has proven to be an effective means
of improving generalization ability. Furthermore, we pro-
pose a novel residual unit capable of further improving the
classiﬁcation accuracy with our new network architecture.
Experiments on benchmark CIFAR-10, CIFAR-100, and Im-
ageNet datasets have shown that our network architecture
has superior generalization ability compared to the original
residual networks.

Code is available at https://github.com/jhkim89/PyramidNet

1. Introduction

The emergence of deep convolutional neural networks
(DCNNs) has greatly contributed to advancements in solv-
ing complex tasks [13, 23, 2, 3, 19] in computer vision
with signiﬁcantly improved performance. Since the pro-
posal of LeNet [16], which introduced the use of deep neu-
ral network architectures for computer vision tasks, the ad-
vanced architecture AlexNet [13] was selected as the win-
ner of the 2012 ImageNet competition [22] by a large mar-
gin over traditional methods. Subsequently, ZF-net [35],

∗These two authors contributed equally.

VGG [25], GoogleNet [31], Residual Networks [7, 8], and
Inception Residual Networks [30] were successively pro-
posed to demonstrate advances in network architectures.
In particular, Residual Networks (ResNets) [7, 8] leverage
the concept of shortcut connections [29] inside a proposed
residual unit for residual learning, to make it possible to
train much deeper network architectures. Deeper network
architectures are known for their superior performance, and
these network architectures commonly have deeply stacked
convolutional ﬁlters with nonlinearity [25, 31].

With respect to feature map dimension, the conventional
method of stacking several convolutional ﬁlters is to in-
crease the dimension while decreasing the size of feature
maps by increasing the strides of the ﬁlters or poolings.
This is the widely adopted method of controlling the size
of feature maps, because extracting the diversiﬁed high-
level attributes with the increased feature map dimension
is very effective for classiﬁcation tasks. Architectures such
as those of AlexNet [13] and VGG [25] utilize this method
of increasing the feature map dimension to construct their
network architectures. The most successful deep neural net-
work, ResNets [7, 8], which was introduced by He et al. [7],
also follows this approach for ﬁlter stacking.

According to the research of Veit et al. [33], ResNets
are considered to behave as ensembles of relatively shallow
networks. These researchers showed that the deletion of an
individual residual unit from ResNets, i.e., such that only a
shortcut connection remains, does not signiﬁcantly affect
the overall performance, proving that deleting a residual
unit is equivalent to deleting some shallow networks in the
ensemble networks. Contrary to this, deleting a single layer
in plain network architectures such as a VGG-network [25]
damages the network by causing additional severe errors.

However, in the case of ResNets, it was found that delet-
ing the building blocks in a residual unit with downsam-
pling, where the feature map dimension is doubled, still in-
creases the classiﬁcation error by a signiﬁcant margin. In-
terestingly, when the residual net is trained using a stochas-
tic depth [10], it was found that deleting the blocks with
downsampling does not degrade the classiﬁcation perfor-
mance, as shown in Figure 8 in [33]. One may think that

1

Figure 1. Schematic illustration of (a) basic residual units [7], (b) bottleneck residual units [7], (c) wide residual units [34], (d) our pyramidal
residual units, and (e) our pyramidal bottleneck residual units.

this phenomenon is related to the overall improvement in
the classiﬁcation performance enabled by stochastic depth.
Motivated by the ensemble interpretation of residual net-
works in Veit et al. [33] and the results with stochastic
depth [10], we devised another method to handle the phe-
nomenon associated with deleting the downsampling unit.
In the proposed method, the feature map dimensions are in-
creased at all layers to distribute the burden concentrated at
locations of residual units affected by downsampling, such
that it is equally distributed across all units. It was found
that using the proposed new network architecture, deleting
the units with downsampling does not degrade the perfor-
mance signiﬁcantly. In our paper, we refer to this network
architecture as a deep “pyramidal” network and a “pyrami-
dal” residual network with a residual-type network archi-
tecture. This reﬂects the fact that the shape of the network
architecture can be compared to that of a pyramid. That is,
the number of channels gradually increases as a function
of the depth at which the layer occurs, which is similar to a
pyramid structure of which the shape gradually widens from
the top downwards. This structure is illustrated in compar-
ison to other network architectures in Figure 1. The key
contributions are summarized as follows:

• A deep pyramidal residual network (PyramidNet) is in-
troduced. The key idea is to concentrate on the feature
map dimension by increasing it gradually instead of by
increasing it sharply at each residual unit with down-
sampling. In addition, our network architecture works
as a mixture of both plain and residual networks by
using zero-padded identity-mapping shortcut connec-
tions when increasing the feature map dimension.

• A novel residual unit is also proposed, which can fur-
ther improve the performance of ResNet-based archi-
tectures (compared with state-of-the-art network archi-
tectures).

The remainder of this paper is organized as follows. Sec-
tion 2 presents our PyramidNets and introduces a novel

residual unit that can further improve ResNet. Section 3
closely analyzes our PyramidNets via several discussions.
Section 4 presents experimental results and comparisons
with several state-of-the-art deep network architectures.
Section 5 concludes our paper with suggestions for future
works.

2. Network Architecture

In this section, we introduce the network architectures of
our PyramidNets. The major difference between Pyramid-
Nets and other network architectures is that the dimension
of channels gradually increases, instead of maintaining the
dimension until a residual unit with downsampling appears.
A schematic illustration is shown in Figure 1 (d) to facilitate
understanding of our network architecture.

2.1. Feature Map Dimension Conﬁguration

Most deep CNN architectures [7, 8, 13, 25, 31, 35] uti-
lize an approach whereby feature map dimensions are in-
creased by a large margin when the size of the feature map
decreases, and feature map dimensions are not increased
until they encounter a layer with downsampling. In the case
of the original ResNet for CIFAR datasets [12], the number
of feature map dimensions Dk of the k-th residual unit that
belongs to the n-th group can be described as follows:

Dk =

(cid:40)

16,
16 · 2n(k)−2,

if n(k) = 1,
if n(k) ≥ 2,

(1)

in which n(k) ∈ {1, 2, 3, 4} denotes the index of the group
to which the k-th residual unit belongs. The residual units
that belong to the same group have an equal feature map
size, and the n-th group contains Nn residual units. In the
ﬁrst group, there is only one convolutional layer that con-
verts an RGB image into multiple feature maps. For the
n-th group, after Nn residual units have passed, the feature
size is downsampled by half and the number of dimensions
is doubled. We propose a method of increasing the feature

Group
conv 1

conv 2

Output size
32×32

32×32

conv 3

16×16

conv 4

avg pool

8×8

1×1

Building Block
[3 × 3, 16]

(cid:20) 3 × 3, (cid:98)16 + α(k − 1)/N (cid:99)
3 × 3, (cid:98)16 + α(k − 1)/N (cid:99)
(cid:20) 3 × 3, (cid:98)16 + α(k − 1)/N (cid:99)
3 × 3, (cid:98)16 + α(k − 1)/N (cid:99)
(cid:20) 3 × 3, (cid:98)16 + α(k − 1)/N (cid:99)
3 × 3, (cid:98)16 + α(k − 1)/N (cid:99)

(cid:21)

(cid:21)

(cid:21)

× N2

× N3

× N4

[8 × 8, 16 + α]

Table 1. Structure of our PyramidNet for benchmarking with
CIFAR-10 and CIFAR-100 datasets. α denotes the widening fac-
tor, and Nn signiﬁes the number of blocks in a group. Downsam-
pling is performed at conv3 1 and conv4 1 with a stride of 2.

Figure 6, the layers can be stacked in various manners to
construct a single building block. We found the building
block shown in Figure 6 (d) to be the most promising, and
therefore we included this structure as building block in our
PyramidNets. The discussion of this matter is continued in
the following section.

In terms of shortcut connections, many researchers ei-
ther use those based on identity mapping, or those employ-
ing convolution-based projection. However, as the feature
map dimension of PyramidNet is increased at every unit,
we can only consider two options: zero-padded identity-
mapping shortcuts, and projection shortcuts conducted by
1×1 convolutions. However, as mentioned in the work of
He et al. [8], the 1×1 convolutional shortcut produces a
poor result when there are too many residual units, i.e., this
shortcut is unsuitable for very deep network architectures.
Therefore, we select zero-padded identity-mapping short-
cuts for all residual units. Further discussions about the
zero-padded shortcut are provided in the following section.

3. Discussions

In this section, we present an in-depth study of the ar-
chitecture of our PyramidNet, together with the proposed
novel residual units. The experiments we include here sup-
port the study and conﬁrm that insights obtained from our
network architecture can further improve the performance
of existing ResNet-based architectures.

3.1. Effect of PyramidNet

According to the work of Veit et al. [33], ResNets can be
viewed as ensembles of relatively shallow networks, sup-
ported by the observation that deleting an individual build-
ing block in a residual unit of ResNets incurs minor classi-
ﬁcation loss, whereas removing layers from plain networks
such as VGG [25] severely reduces the classiﬁcation rate.
However, in both original and pre-activation ResNets [7, 8],
another noteworthy aspect is that deleting the units with
downsampling (and doubling the feature dimension) still
degrades performance by a large margin [33]. Meanwhile,

(a)

(b)

(c)

Figure 2. Visual illustrations of (a) additive PyramidNet, (b) mul-
tiplicative PyramidNet, and (c) a comparison of (a) and (b).

map dimension as follows:
(cid:40)

Dk =

16,
(cid:98)Dk−1 + α/N (cid:99),

if k = 1,
if 2 ≤ k ≤ N + 1,

(2)

in which N denotes the total number of residual units, de-
ﬁned as N = (cid:80)4
n=2 Nn. The dimension is increased by a
step factor of α/N , and the output dimension of the ﬁnal
unit of each group becomes 16 + (n − 1)α/3 with same
number of residual units in each group. The details of our
network architecture are presented in Table 1.

The above equations are based on an addition-based
widening step factor α for increasing dimensions. However,
of course, multiplication-based widening (i.e., the process
of multiplying by a factor to increase the channel dimen-
sion geometrically) presents another possibility for creating
a pyramid-like structure. Then, eq.(2) can be transformed
as follows:

(cid:40)

Dk =

16,
(cid:98)Dk−1 · α 1

N (cid:99),

if k = 1,
if 2 ≤ k ≤ N + 1.

(3)

The main difference between additive and multiplicative
PyramidNets is that the feature map dimension of an ad-
ditive network gradually increases linearly, whereas the di-
mension of a multiplicative network increases geometri-
cally. That is, the dimension slowly increases in input-side
layers and sharply increases in output-side layers. This pro-
cess is similar to that of the original deep network architec-
tures such as VGG [25] and ResNet [7]. The visual illustra-
tions of additive and multiplicative PyramidNets are shown
in Figure 2. In this paper, we compare the performance of
both of these dimension-increasing approaches by compar-
ing an additive PyramidNet (eq. (2)) and a multiplicative
PyramidNet (eq. (3)) in section 4.

2.2. Building Block

The building block (i.e., the convolutional ﬁlter stacks
with ReLUs and BN layers) in a residual unit is the core
of ResNet-based architectures. It is obvious that in order
to maximize the capability of the network architecture, de-
signing a good building block is essential. As shown in

Figure 3. Performance comparison between the pre-activation
ResNet [8] and our PyramidNet, using CIFAR datasets. Dashed
and solid lines denote the training loss and test error, respectively.

when a stochastic depth [10] is applied, this phenomenon
is not observed, and the performance is also improved, ac-
cording to the experiment of Veit et al. [33]. The objective
of our PyramidNet is to resolve this phenomenon differ-
ently, by attempting to gradually increase the feature map
dimension instead of doubling it at one of the residual units
and to evenly distribute the burden of increasing the feature
maps. We observed that our PyramidNet indeed resolves
this phenomenon and at the same time improves overall per-
formance. We further analyze the effect of our PyramidNet
by comparing it against the pre-activation ResNet, with the
following experimental results. First, we compare the train-
ing and test error curves of our PyramidNet with those of
the pre-activation ResNet [8] in Figure 3. The standard pre-
activation ResNet with 110 layers is used for comparison.
For our PyramidNet, we used a depth of 110 layers with a
widening factor of α = 48; it had the same number of pa-
rameters (1.7M) as the pre-activation ResNet to allow for a
fair comparison. The results indicate that our PyramidNet
has superior test accuracy, thereby conﬁrming its greater
ability to generalize compared to existing deep networks.

Second, we verify the ensemble effect of our Pyramid-
Nets by evaluating the performance after deleting individ-
ual units, similar to the experiment of Veit et al. [33]. The
results are shown in Figure 4. As mentioned by Veit et
al. [33], removing individual units only causes a slight
performance loss, compared with a plain network such as
the VGG [25]. However, in the case of the pre-activation
ResNet, removing the blocks subjected to downsampling
tends to affect the classiﬁcation accuracy by a relatively
large margin, whereas this does not occur with our Pyra-
midNets. Furthermore, the mean average error differences
between the baseline result and the result obtained when
individual units were deleted from both the pre-activation
ResNet and our PyramidNet were 0.72% and 0.54%, re-

Figure 4. Test error curves to study the extent to which residual
units contribute to the performance in different network architec-
tures by deleting their individual units. The dashed and solid lines
denote the test errors that occur when no units are deleted, and
when an individual unit is deleted, respectively. Bold vertical lines
denote the location of residual units through downsampling.

spectively. This result shows that the ensemble effect of
our PyramidNet becomes stronger than the original ResNet,
such that generalization ability is improved.

3.2. Zero-padded Shortcut Connection

ResNets and pre-activation ResNets [7, 8] were stud-
ied several types of shortcuts, such as an identity-mapping
shortcut or projection shortcut. The experimental results
in [8] showed that the identity-mapping shortcut is a much
more appropriate choice than other shortcuts. Because an
identity-mapping shortcut does not have parameters, it has a
lower possibility of overﬁtting compared to the other types
of shortcuts; this ensures improved generalization ability.
Moreover, it can purely pass through the gradient accord-
ing to the identity mapping, and therefore it provides more
stability in the training stage.

In the case of our PyramidNet, identity mapping alone
cannot be used for a shortcut because the feature map di-
mension differs among individual residual units. Therefore,
only a zero-padded shortcut or projection shortcut can be
used for all the residual units. However, as discussed in [8],
a projection shortcut can hamper information propagation
and lead to optimization problems, especially for very deep
networks. On the other hand, we found that the zero-padded
shortcut does not lead to the overﬁtting problem because no
additional parameters exist, and surprisingly, it shows sig-
niﬁcant generalization ability compared to other shortcuts.
We now examine the effect of the zero-padded identity-
mapping shortcut on the k-th residual unit that belongs to
the n-th group with the reshaped vector xl
k of the l-th fea-
ture map:

(cid:40)

xl

k =

F(k,l)(xl
F(k,l)(xl

k−1) + xl
k−1),

k−1,

if 1 ≤ l ≤ Dk−1
if Dk−1 < l ≤ Dk

(4)

where F(k,l)(·) denotes the l-th residual function of the k-
th residual unit and Dk represents the pre-deﬁned channel

Shortcut Types
(a) Identity mapping with projection shortcut
(b) Projection with zero-padded shortcut
(c) Only projection shortcut
(d) Identity mapping with zero-padded shortcut

CIFAR-10
5.03
6.84
6.98
4.70

CIFAR-100
23.48
31.29
31.62
22.77

Table 2. Top-1 errors (%) on CIFAR datasets using our Pyramid-
Net with several combinations of shortcut connections.

tions and the number of ReLUs. This could be discussed
with original ResNets [7], for which it was shown that
the performance increases as the network becomes deeper;
however, if the depth exceeds 1,000 layers, overﬁtting still
occurs and the result is less accurate than that generated by
shallower ResNets.

First, we note that using ReLUs after the addition of

residual units adversely affects performance:

k = ReLU (F(k ,l)(xl
xl

k −1 ) + xl

k −1 ),

(5)

where the ReLUs seem to have the function of ﬁltering non-
negative elements. Gross and Wilber [5] found that simply
removing ReLUs from the original ResNet [7] after each
addition with the shortcut connection leads to small perfor-
mance improvements. This could be understood by consid-
ering that, after addition, ReLUs provide non-negative in-
put to the subsequent residual units, and therefore the short-
cut connection is always non-negative and the convolutional
layers would take responsibility for producing negative out-
put before addition; this may decrease the overall capability
of the network architecture as analyzed in [8]. The pre-
activation ResNets proposed by He et al. [8] also overcame
this issue with pre-activated residual units that place BN
layers and ReLUs before (instead of after) the convolutional
layers:

k = F(k,l)(xl
xl

k−1) + xl

k−1,

(6)

where ReLUs are removed after addition to create an iden-
tity path. Consequently, the overall performance has in-
creased by a large margin without overﬁtting, even at depths
exceeding 1,000 layers. Furthermore, Shen et al. [24] pro-
posed a weighted residual network architecture, which lo-
cates a ReLU inside a residual unit (instead of locating
ReLU after addition) to create an identity path, and showed
that this structure also does not overﬁt even at depths of
more than 1,000 layers.

Second, we found that the use of a large number of Re-
LUs in the blocks of each residual unit may negatively af-
fect performance. Removing the ﬁrst ReLU in the blocks
of each residual unit, as shown in Figure 6 (b) and (d), was
found to enhance performance compared with the blocks
shown in Figure 6 (a) and (c). Experimentally, we found
that removal of the ﬁrst ReLU in the stack is preferable and
that the other ReLU should remain to ensure nonlinearity.
Removing the second ReLU in Figure 6 (a) changes the

(a)

(b)

Figure 5. Structure of residual unit (a) with zero-padded identity-
mapping shortcut, (b) unraveled view of (a) showing that the zero-
padded identity-mapping shortcut constitutes a mixture of a resid-
ual network with a shortcut connection and a plain network.

dimensions of the k-th residual unit. From eq.(4), zero-
padded elements of the identity-mapping shortcut for in-
creasing dimension let xl
k contain the outputs of both resid-
ual networks and plain networks. Therefore, we could con-
jecture that each zero-padded identity-mapping shortcut can
provide a mixture of the residual network and plain net-
work, as shown in Figure 5. Furthermore, our Pyramid-
Net increases the channel dimension at every residual unit,
and the mixture effect of the residual network and plain net-
work increases markedly. Figure 4 supports the conclusion
that the test error of PyramidNet does not oscillate as much
as that of the pre-activation ResNet. Finally, we investigate
several types of shortcuts including proposed zero-padded
identity-mapping shortcut in Table 2.

3.3. A New Building Block

To maximize the capability of the network, it is natural
to ask the following question: “Can we design a better
building block by altering the stacked elements inside
the building block in more principled way?”. The ﬁrst
building block types were proposed in the original paper on
ResNets [7], and another type of building block was subse-
quently proposed in the paper on pre-activation ResNets [8],
to answer the question. Moreover, pre-activation ResNets
attempted to solve the backward gradient ﬂowing problem
[8] by redesigning residual modules; this proved to be suc-
cessful in trials. However, although the pre-activation resid-
ual unit was discovered with empirically improved perfor-
mance, further investigation over the possible combinations
is not yet performed, leaving a potential room for improve-
ment. We next attempt to answer the question from two
points of view by considering Rectiﬁed Linear Units (Re-
LUs) [20] and Batch Normalization (BN) [11] layers.

3.3.1 ReLUs in a Building Block

Including ReLUs [20] in the building blocks of residual
units is essential for nonlinearity; however, we found empir-
ically that the performance can vary depending on the loca-

(a)

(b)

(c)

(d)

Figure 6. Various types of basic and bottleneck residual units. “BatchNorm” denotes a Batch Normalization (BN) layer. (a) original
pre-activation ResNets [8], (b) pre-activation ResNets removing the ﬁrst ReLU, (c) pre-activation ResNets with a BN layer after the ﬁnal
convolutional layer, and (d) pre-activation ResNets removing the ﬁst ReLU with a BN layer after the ﬁnal convolutional layer.

blocks to BN-ReLU-conv-BN-conv, and it is clear that, in
these blocks, the convolutional layers are successively lo-
cated without ReLUs to weaken their representation pow-
ers of each other. However, when we remove the ﬁrst
ReLU, the blocks are changed to BN-conv-BN-ReLU-conv,
in which case the two convolutional layers are separated by
the second ReLU, thereby guaranteeing nonlinearity. The
results in Table 3 conﬁrm that removing the ﬁrst ReLU as
in (b) and (d) in Figure 6, enhances the performance. Con-
sequently, provided that an appropriate number of ReLUs
are used to guarantee the nonlinearity of the feature space
manifold, the remaining ReLUs could be removed to im-
prove network performance.

3.3.2 BN Layers in a Building Block

The main role of a BN layer is to normalize the activations
for fast convergence and to improve performance. The ex-
perimental results of the four structures provided in Table 3
show that the BN layer can be used to maximize the capabil-
ity of a single residual unit. A BN layer conducts an afﬁne
transformation with the following equation:

y = γx + β,

(7)

where γ and β are learned for every activation in feature
maps. We experimentally found that the learned γ and β
could closely approximate 0. This implies that if the learned
γ and β are both close to 0, then the corresponding acti-
vation is considered not to be useful. Weighted ResNets
[24], in which the learnable weights occur at the end of
their building blocks, are also similarly learned to determine
whether the corresponding residual unit is useful. Thus, the
BN layers at the end of each residual unit are a generalized
version including [24] to enable decisions to be made as to
whether each residual unit is helpful. Therefore, the degrees

ResNet Architecture
(a) Pre-activation [8]
(b) Removing the ﬁrst ReLU
(c) BN after the ﬁnal conv
(d) (b) + (c)
PyramidNet Architecture
(a) Pre-activation [8]
(b) Removing the ﬁrst ReLU
(c) BN after the ﬁnal conv
(d) (b) + (c)
PyramidNet (bottleneck) Architecture
(a) Pre-activation [8]
(b) Removing the ﬁrst ReLU
(c) BN after the ﬁnal conv
(d) (b) + (c)

CIFAR-10
5.82
5.31
5.74
5.29
CIFAR-10
5.15
4.81
4.96
4.62
CIFAR-10
4.61
4.45
4.56
4.26

CIFAR-100
25.06
24.55
24.54
23.74
CIFAR-100
24.40
23.43
23.89
23.31
CIFAR-100
21.10
20.40
20.44
20.32

Table 3. Top-1 errors (%) on CIFAR datasets for several build-
ing block combinations of ReLUs and BN layers shown in Fig-
ure 6 (a)–(d), using ResNet [8] (with original feature map dimen-
sion conﬁguration) and our PyramidNet.

of freedom obtained by involving γ and β from the BN lay-
ers could improve the capability of the network architecture.
The results in Table 3 support the conclusion that adding a
BN layer at the end of each building block, as in type (c)
and (d) in Figure 6, improves the performance. Note that
the aforementioned network removing the ﬁrst ReLU is also
improved by adding a BN layer after the ﬁnal convolutional
layer. Furthermore, the results in Table 3 show that both
PyramidNet and a new building block improve the perfor-
mance signiﬁcantly.

4. Experimental Results

We evaluate and compare the performance of our algo-
rithm with that of existing algorithms [7, 8, 18, 24, 34] using
representative benchmark datasets: CIFAR-10 and CIFAR-
100 [12]. CIFAR-10 and CIFAR-100 each contain 32×32-
pixel color images, consists of 50,000 training images and
10,000 testing images. But in case of CIFAR-10, it includes

# of Params Output Feat. Dim. Depth

Network
NiN [18]
All-CNN [27]
DSN [17]
FitNet [21]
Highway [29]
Fractional Max-pooling [4]
ELU [29]
ResNet [7]
ResNet [7]
ResNet [7]
Pre-activation ResNet [8]
Pre-activation ResNet [8]
Stochastic Depth [10]
Stochastic Depth [10]
FractalNet [14]
SwapOut v2 (width×4) [26]
Wide ResNet (width×4) [34]
Wide ResNet (width×10) [34]
Weighted ResNet [24]
DenseNet (k = 24) [9]
DenseNet-BC (k = 40) [9]
PyramidNet (α = 48)
PyramidNet (α = 84)
PyramidNet (α = 270)
PyramidNet (bottleneck, α = 270)
PyramidNet (bottleneck, α = 240)
PyramidNet (bottleneck, α = 220)
PyramidNet (bottleneck, α = 200)

-
-
-
-
-
-
-
1.7M
10.2M
19.4M
1.7M
10.2M
1.7M
10.2M
38.6M
7.4M
8.7M
36.5M
19.1M
27.2M
25.6M
1.7M
3.8M
28.3M
27.0M
26.6M
26.8M
26.0M

-
-
-
-
-
-
-
64
64
64
64
64
64
64
1,024
256
256
640
64
2,352
2,190
64
100
286
1,144
1,024
944
864

Training Mem.
-
-
-
-
-
-
-
547MB
2,921MB
2,069MB
841MB
2,921MB
547MB
2,069MB
-
-
775MB
1,383MB
-
4,381MB
7,247MB
655MB
781MB
1,437MB
4,169MB
4,451MB
4,767MB
5,005MB

CIFAR-10
8.81
7.25
7.97
8.39
7.72
4.50
6.55
6.43
-
7.93
5.46
4.62
5.23
4.91
4.60
4.76
4.97
4.17
5.10
3.74
3.46
4.58±0.06
4.26±0.23
3.73±0.04
3.48±0.20
3.44±0.11
3.40±0.07
3.31±0.08

CIFAR-100
35.68
33.71
34.57
35.04
32.39
27.62
24.28
25.16
27.82
-
24.33
22.71
24.58
-
23.73
22.72
22.89
20.50
-
19.25
17.18
23.12±0.04
20.66±0.40
18.25±0.10
17.01±0.39
16.51±0.13
16.37±0.29
16.35±0.24

-
-
-
-
-
-
-
110
1001
1202
164
1001
110
1202
21
32
40
28
1192
100
190
110
110
110
164
200
236
272

Table 4. Top-1 error rates (%) on CIFAR datasets. All the results of PyramidNets are produced with additive PyramidNets, and α denotes
the widening factor. “Output Feat. Dim.” denotes the feature dimension of just before the last softmax classiﬁer. The best results are
highlighted in red.

10 classes, and CIFAR-100 includes 100 classes. The stan-
dard data augmentation, horizontal ﬂipping, and translation
by 4 pixels are adopted in our experiments, following the
common practice [18]. The results achieved by Pyramid-
Nets are based on the proposed residual unit: placing a BN
layer after the ﬁnal convolutional layer, and removing the
ﬁrst ReLU as in Figure 6 (d). Our code is built on Torch
open source deep learning framework [1].

4.1. Training Settings

Our PyramidNets are trained using backpropagation [15]
by Stochastic Gradient Descent (SGD) with Nesterov mo-
mentum for 300 epochs on CIFAR-10 and CIFAR-100
datasets. The initial learning rate is set to 0.1 for CIFAR-10
and 0.5 for CIFAR-100, and is decayed by a factor of 0.1 at
150 and 225 epochs, respectively. The ﬁlter parameters are
initialized by “msra” [6]. We use a weight decay of 0.0001,
a dampening of 0, a momentum of 0.9, and a batch size of
128.

4.2. Performance Evaluation

In our work, we mainly use the top-1 error rate for evalu-
ating our network architecture. Additive PyramidNets with

Figure 7. Comparison of test error curves with error bars of ad-
ditive PyramidNet and multiplicative PyramidNet on CIFAR-10
(left) and CIFAR-100 (right) datasets, according to the different
number of parameters.

both basic and pyramidal bottleneck residual units are used.
The error rates are provided in Table 4 for ours and the state-
of-the-art models. The experimental results show that our
network has superior generalization ability, in terms of the
number of parameters, showing the best results compared
with other models.

Figure 7 compares additive and multiplicative Pyramid-
Nets using CIFAR datasets. When the number of param-
eters is low, both additive and multiplicative PyramidNets
show similar performance, because these two network ar-

# of Params Output Feat. Dim.

Network
ResNet-152 [7]
Pre-ResNet-152† [8]
Pre-ResNet-200† [8]
WRN-50-2-bottleneck [34]
PyramidNet-200 (α = 300)
PyramidNet-200 (α = 300)∗
PyramidNet-200 (α = 450)∗
ResNet-200 [7]
Pre-ResNet-200 [8]
Inception-v3 [32]
Inception-ResNet-v1 [30]
Inception-v4 [30]
Inception-ResNet-v2 [30]
PyramidNet-200 (α = 300)
PyramidNet-200 (α = 300)∗
PyramidNet-200 (α = 450)∗

60.0M
60.0M
64.5M
68.9M
62.1M
62.1M
116.4M
64.5M
64.5M
-
-
-
-
62.1M
62.1M
116.4M

2,048
2,048
2,048
2,048
1,456
1,456
2,056
2,048
2,048
2,048
1,792
1,536
1,792
1,456
1,456
2,056

Augmentation
scale
scale+asp ratio
scale+asp ratio
scale+asp ratio
scale+asp ratio
scale+asp ratio
scale+asp ratio
scale
scale+asp ratio
scale+asp ratio
scale+asp ratio
scale+asp ratio
scale+asp ratio
scale+asp ratio
scale+asp ratio
scale+asp ratio

Train Crop
224×224
224×224
224×224
224×224
224×224
224×224
224×224
224×224
224×224
299×299
299×299
299×299
299×299
224×224
224×224
224×224

Test Crop
224×224
224×224
224×224
224×224
224×224
224×224
224×224
320×320
320×320
299×299
299×299
299×299
299×299
320×320
320×320
320×320

Top-1
23.0
22.2
21.7
21.9
20.5
20.5
20.1
21.8
20.1
21.2
21.3
20.0
19.9
19.6
19.5
19.2

Top-5
6.7
6.2
5.8
6.0
5.3
5.4
5.4
6.0
4.8
5.6
5.5
5.0
4.9
4.8
4.8
4.7

Table 5. Comparisons of single-model, single-crop error (%) on the ILSVRC 2012 validation set. All the results of PyramidNets are
produced with additive PyramidNets. “asp ratio” means the aspect ratio applied for data augmention, and “Output feat. dim.” denotes the
feature dimension of just after the last global pooling layer. ∗ denotes the models which applied dropout method, and † denotes the results
obtained from https://github.com/facebook/fb.resnet.torch.

chitectures do not have signiﬁcant structural differences.
As the number of parameters increases, they start to show
a more marked difference in terms of the feature map di-
mension conﬁguration. Because the feature map dimension
increases linearly in the case of additive PyramidNets, the
feature map dimensions of the input-side layers tend to be
larger, and those of the output-side layers tend to be smaller,
compared with multiplicative PyramidNets as illustrated in
Figure 2.

Previous works [7, 25] typically set multiplicative scal-
ing of feature map dimension for downsampling modules,
which is implemented to give a larger degree of freedom to
the classiﬁcation part by increasing the feature map dimen-
sion of the output-side layers. However, for our Pyramid-
Net, the results in Figure 7 implies that increasing the model
capacity of the input-side layers would lead to a better per-
formance improvement than using a conventional way of
multiplicative scaling of feature map dimension.

We also note that, although the use of regularization
methods such as dropout [28] or stochastic depth [10] could
further improve the performance of our model, we did not
involve those methods to ensure a fair comparison with
other models.

We train our models for 120 epochs with a batch size of
128, and the initial learning rate is set to 0.05, divided by 10
at 60, 90 and 105 epochs. We use the same weight decay,
momentum, and initialization settings as those of CIFAR
datasets. We train our model by using a standard data aug-
mentation with scale jittering and aspect ratio as suggested
in Szegedy et al. [31]. Table 5 shows the results of our
PyramidNets in ImageNet dataset compared with the state-
of-the-art models. The experimental results show that our
PyramidNet with α = 300 has a top-1 error rate of 20.5%,
which is 1.2% lower than the pre-activation ResNet-200 [8]
which has a similar number of parameters but higher out-
put feature dimension than our model. We also notice that
increasing α with an appropriate regularization method can
further improve the performance.

For comparison with the Inception-ResNet [30] that uses
a testing crop with 299 × 299 size, we test our model on a
320 × 320 crop, by the same reason with the work of He et
al. [8]. Our PyramidNet with α = 300 shows a top-1 error
rate of 19.6%, which outperforms both the pre-activation
ResNet [8] and the Inception-ResNet-v2 [30] models.

5. Conclusion

4.3. ImageNet

1,000-class ImageNet dataset [22] used for ILSVRC
contains more than one million training images and 50,000
validation images. We use additive PyramidNets with the
pyramidal bottleneck residual units, deleting the ﬁrst ReLU
and adding a BN layer at the last layer as described in Sec-
tion 3.3 and shown in Figure 6 (d) for further performance
improvement.

The main idea of the novel deep network architecture
described in this paper involves increasing the feature map
dimension gradually, in order to construct so-called Pyra-
midNets along with the concept of ResNets. We also devel-
oped a novel residual unit, which includes a new building
block for a residual unit with a zero-padded shortcut; this
design leads to signiﬁcantly improved generalization abil-
ity. In tests using CIFAR-10, CIFAR-100, and ImageNet-
1k datasets, our PyramidNets outperform all previous state-

of-the-art deep network architectures. Furthermore, the in-
sights in this paper could be utilized by any network archi-
tecture, to improve their capacity for better performance.
In future work, we will develop methods of optimizing pa-
rameters such as feature map dimensions in more principled
ways with proper cost functions that give insight into the na-
ture of residual networks.
Acknowledgements: This work was supported by the ICT
R&D program of MSIP/IITP, 2016-0-00563, Research on
Adaptive Machine Learning Technology Development for
Intelligent Autonomous Digital Companion.

References

[1] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A
matlab-like environment for machine learning. In BigLearn,
NIPS Workshop, 2011. 7

[2] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,
E. Tzeng, and T. Darrell. Decaf: A deep convolutional acti-
vation feature for generic visual recognition. In ICML, 2014.
1

[3] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
segmentation. In CVPR, 2014. 1

[4] B. Graham.

Fractional max-pooling.

arXiv preprint

arXiv:1412.6071, 2014. 7

[5] S. Gross and M. Wilber. Training and investigating residual

nets. 2016. http://torch.ch/blog/2016/02/04/resnets.html. 5

[6] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation. In ICCV, 2015. 7

[7] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 1, 2, 3, 4, 5, 6, 7, 8

[8] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in
deep residual networks. In ECCV, 2016. 1, 2, 3, 4, 5, 6, 7, 8
[9] G. Huang, Z. Liu, and K. Q. Weinberger. Densely connected
convolutional networks. arXiv preprint arXiv:1608.06993,
2016. 7

[10] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Weinberger. Deep
networks with stochastic depth. In ECCV, 2016. 1, 2, 4, 7, 8
[11] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, 2015. 5

[12] A. Krizhevsky. Learning multiple layers of features from

tiny images. In Tech Report, 2009. 2, 6

[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

ImageNet
Classiﬁcation with Deep Convolutional Neural Networks. In
NIPS, 2012. 1, 2

[14] G. Larsson, M. Maire, and G. Shakhnarovich. Fractalnet:
Ultra-deep neural networks without residuals. arXiv preprint
arXiv:1605.07648, 2016. 7

[15] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.
Howard, W. Hubbard, and L. D. Jackel. Backpropagation
applied to handwritten zip code recognition. Neural compu-
tation, 1(4):541–551, 1989. 7

[16] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278–2324, 1998. 1

[17] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-

supervised nets. In AISTATS, 2015. 7

[18] M. Lin, Q. Chen, and S. Yan. Network in network. In ICLR,

2014. 6, 7

[19] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In CVPR, 2015. 1
[20] V. Nair and G. E. Hinton. Rectiﬁed linear units improve re-

stricted boltzmann machines. In ICML, 2010. 5

[21] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta,
and Y. Bengio. Fitnets: Hints for thin deep nets. In ICLR,
2015. 7

[22] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge. International Journal of Computer
Vision, 115(3):211–252, 2015. 1, 8

[23] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,
and Y. LeCun. Overfeat: Integrated recognition, localization
and detection using convolutional networks. In ICLR, 2014.
1

[24] F. Shen and G. Zeng. Weighted residuals for very deep net-
works. arXiv preprint arXiv:1605.08831, 2016. 5, 6, 7
[25] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
1, 2, 3, 4, 8

[26] S. Singh, D. Hoiem, and D. Forsyth. Swapout: Learning an

ensemble of deep architectures. In NIPS, 2016. 7

[27] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Ried-
miller. Striving for simplicity: The all convolutional net. In
ICLR Workshop, 2015. 7

[28] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov. Dropout: A simple way to prevent neu-
ral networks from overﬁtting. Journal of Machine Learning
Research, 15:1929–1958, 2014. 8

[29] R. K. Srivastava, K. Greff, and J. Schmidhuber. Training

very deep networks. In NIPS, 2015. 1, 7
[30] C. Szegedy, S. Ioffe, and V. Vanhoucke.

Inception-v4,
inception-resnet and the impact of residual connections on
learning. In ICLR Workshop, 2016. 1, 8

[31] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015. 1, 2, 8
[32] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
Rethinking the inception architecture for computer vision. In
CVPR, 2016. 8

[33] A. Veit, M. Wilber, and S. Belongie. Residual networks be-
have like ensembles of relatively shallow networks. In NIPS,
2016. 1, 2, 3, 4

[34] S. Zagoruyko and N. Komodakis. Wide residual networks.

In BMVC, 2016. 2, 6, 7, 8

[35] M. D. Zeiler and R. Fergus. Visualizing and understanding

convolutional networks. In ECCV, 2014. 1, 2

Deep Pyramidal Residual Networks

Dongyoon Han∗
EE, KAIST
dyhan@kaist.ac.kr

Jiwhan Kim∗
EE, KAIST
jhkim89@kaist.ac.kr

Junmo Kim
EE, KAIST
junmo.kim@kaist.ac.kr

7
1
0
2
 
p
e
S
 
6
 
 
]

V
C
.
s
c
[
 
 
4
v
5
1
9
2
0
.
0
1
6
1
:
v
i
X
r
a

Abstract

Deep convolutional neural networks (DCNNs) have
shown remarkable performance in image classiﬁcation
tasks in recent years. Generally, deep neural network ar-
chitectures are stacks consisting of a large number of con-
volutional layers, and they perform downsampling along
the spatial dimension via pooling to reduce memory us-
age. Concurrently, the feature map dimension (i.e., the num-
ber of channels) is sharply increased at downsampling lo-
cations, which is essential to ensure effective performance
because it increases the diversity of high-level attributes.
This also applies to residual networks and is very closely
In this research, instead of
related to their performance.
sharply increasing the feature map dimension at units that
perform downsampling, we gradually increase the feature
map dimension at all units to involve as many locations as
possible. This design, which is discussed in depth together
with our new insights, has proven to be an effective means
of improving generalization ability. Furthermore, we pro-
pose a novel residual unit capable of further improving the
classiﬁcation accuracy with our new network architecture.
Experiments on benchmark CIFAR-10, CIFAR-100, and Im-
ageNet datasets have shown that our network architecture
has superior generalization ability compared to the original
residual networks.

Code is available at https://github.com/jhkim89/PyramidNet

1. Introduction

The emergence of deep convolutional neural networks
(DCNNs) has greatly contributed to advancements in solv-
ing complex tasks [13, 23, 2, 3, 19] in computer vision
with signiﬁcantly improved performance. Since the pro-
posal of LeNet [16], which introduced the use of deep neu-
ral network architectures for computer vision tasks, the ad-
vanced architecture AlexNet [13] was selected as the win-
ner of the 2012 ImageNet competition [22] by a large mar-
gin over traditional methods. Subsequently, ZF-net [35],

∗These two authors contributed equally.

VGG [25], GoogleNet [31], Residual Networks [7, 8], and
Inception Residual Networks [30] were successively pro-
posed to demonstrate advances in network architectures.
In particular, Residual Networks (ResNets) [7, 8] leverage
the concept of shortcut connections [29] inside a proposed
residual unit for residual learning, to make it possible to
train much deeper network architectures. Deeper network
architectures are known for their superior performance, and
these network architectures commonly have deeply stacked
convolutional ﬁlters with nonlinearity [25, 31].

With respect to feature map dimension, the conventional
method of stacking several convolutional ﬁlters is to in-
crease the dimension while decreasing the size of feature
maps by increasing the strides of the ﬁlters or poolings.
This is the widely adopted method of controlling the size
of feature maps, because extracting the diversiﬁed high-
level attributes with the increased feature map dimension
is very effective for classiﬁcation tasks. Architectures such
as those of AlexNet [13] and VGG [25] utilize this method
of increasing the feature map dimension to construct their
network architectures. The most successful deep neural net-
work, ResNets [7, 8], which was introduced by He et al. [7],
also follows this approach for ﬁlter stacking.

According to the research of Veit et al. [33], ResNets
are considered to behave as ensembles of relatively shallow
networks. These researchers showed that the deletion of an
individual residual unit from ResNets, i.e., such that only a
shortcut connection remains, does not signiﬁcantly affect
the overall performance, proving that deleting a residual
unit is equivalent to deleting some shallow networks in the
ensemble networks. Contrary to this, deleting a single layer
in plain network architectures such as a VGG-network [25]
damages the network by causing additional severe errors.

However, in the case of ResNets, it was found that delet-
ing the building blocks in a residual unit with downsam-
pling, where the feature map dimension is doubled, still in-
creases the classiﬁcation error by a signiﬁcant margin. In-
terestingly, when the residual net is trained using a stochas-
tic depth [10], it was found that deleting the blocks with
downsampling does not degrade the classiﬁcation perfor-
mance, as shown in Figure 8 in [33]. One may think that

1

Figure 1. Schematic illustration of (a) basic residual units [7], (b) bottleneck residual units [7], (c) wide residual units [34], (d) our pyramidal
residual units, and (e) our pyramidal bottleneck residual units.

this phenomenon is related to the overall improvement in
the classiﬁcation performance enabled by stochastic depth.
Motivated by the ensemble interpretation of residual net-
works in Veit et al. [33] and the results with stochastic
depth [10], we devised another method to handle the phe-
nomenon associated with deleting the downsampling unit.
In the proposed method, the feature map dimensions are in-
creased at all layers to distribute the burden concentrated at
locations of residual units affected by downsampling, such
that it is equally distributed across all units. It was found
that using the proposed new network architecture, deleting
the units with downsampling does not degrade the perfor-
mance signiﬁcantly. In our paper, we refer to this network
architecture as a deep “pyramidal” network and a “pyrami-
dal” residual network with a residual-type network archi-
tecture. This reﬂects the fact that the shape of the network
architecture can be compared to that of a pyramid. That is,
the number of channels gradually increases as a function
of the depth at which the layer occurs, which is similar to a
pyramid structure of which the shape gradually widens from
the top downwards. This structure is illustrated in compar-
ison to other network architectures in Figure 1. The key
contributions are summarized as follows:

• A deep pyramidal residual network (PyramidNet) is in-
troduced. The key idea is to concentrate on the feature
map dimension by increasing it gradually instead of by
increasing it sharply at each residual unit with down-
sampling. In addition, our network architecture works
as a mixture of both plain and residual networks by
using zero-padded identity-mapping shortcut connec-
tions when increasing the feature map dimension.

• A novel residual unit is also proposed, which can fur-
ther improve the performance of ResNet-based archi-
tectures (compared with state-of-the-art network archi-
tectures).

The remainder of this paper is organized as follows. Sec-
tion 2 presents our PyramidNets and introduces a novel

residual unit that can further improve ResNet. Section 3
closely analyzes our PyramidNets via several discussions.
Section 4 presents experimental results and comparisons
with several state-of-the-art deep network architectures.
Section 5 concludes our paper with suggestions for future
works.

2. Network Architecture

In this section, we introduce the network architectures of
our PyramidNets. The major difference between Pyramid-
Nets and other network architectures is that the dimension
of channels gradually increases, instead of maintaining the
dimension until a residual unit with downsampling appears.
A schematic illustration is shown in Figure 1 (d) to facilitate
understanding of our network architecture.

2.1. Feature Map Dimension Conﬁguration

Most deep CNN architectures [7, 8, 13, 25, 31, 35] uti-
lize an approach whereby feature map dimensions are in-
creased by a large margin when the size of the feature map
decreases, and feature map dimensions are not increased
until they encounter a layer with downsampling. In the case
of the original ResNet for CIFAR datasets [12], the number
of feature map dimensions Dk of the k-th residual unit that
belongs to the n-th group can be described as follows:

Dk =

(cid:40)

16,
16 · 2n(k)−2,

if n(k) = 1,
if n(k) ≥ 2,

(1)

in which n(k) ∈ {1, 2, 3, 4} denotes the index of the group
to which the k-th residual unit belongs. The residual units
that belong to the same group have an equal feature map
size, and the n-th group contains Nn residual units. In the
ﬁrst group, there is only one convolutional layer that con-
verts an RGB image into multiple feature maps. For the
n-th group, after Nn residual units have passed, the feature
size is downsampled by half and the number of dimensions
is doubled. We propose a method of increasing the feature

Group
conv 1

conv 2

Output size
32×32

32×32

conv 3

16×16

conv 4

avg pool

8×8

1×1

Building Block
[3 × 3, 16]

(cid:20) 3 × 3, (cid:98)16 + α(k − 1)/N (cid:99)
3 × 3, (cid:98)16 + α(k − 1)/N (cid:99)
(cid:20) 3 × 3, (cid:98)16 + α(k − 1)/N (cid:99)
3 × 3, (cid:98)16 + α(k − 1)/N (cid:99)
(cid:20) 3 × 3, (cid:98)16 + α(k − 1)/N (cid:99)
3 × 3, (cid:98)16 + α(k − 1)/N (cid:99)

(cid:21)

(cid:21)

(cid:21)

× N2

× N3

× N4

[8 × 8, 16 + α]

Table 1. Structure of our PyramidNet for benchmarking with
CIFAR-10 and CIFAR-100 datasets. α denotes the widening fac-
tor, and Nn signiﬁes the number of blocks in a group. Downsam-
pling is performed at conv3 1 and conv4 1 with a stride of 2.

Figure 6, the layers can be stacked in various manners to
construct a single building block. We found the building
block shown in Figure 6 (d) to be the most promising, and
therefore we included this structure as building block in our
PyramidNets. The discussion of this matter is continued in
the following section.

In terms of shortcut connections, many researchers ei-
ther use those based on identity mapping, or those employ-
ing convolution-based projection. However, as the feature
map dimension of PyramidNet is increased at every unit,
we can only consider two options: zero-padded identity-
mapping shortcuts, and projection shortcuts conducted by
1×1 convolutions. However, as mentioned in the work of
He et al. [8], the 1×1 convolutional shortcut produces a
poor result when there are too many residual units, i.e., this
shortcut is unsuitable for very deep network architectures.
Therefore, we select zero-padded identity-mapping short-
cuts for all residual units. Further discussions about the
zero-padded shortcut are provided in the following section.

3. Discussions

In this section, we present an in-depth study of the ar-
chitecture of our PyramidNet, together with the proposed
novel residual units. The experiments we include here sup-
port the study and conﬁrm that insights obtained from our
network architecture can further improve the performance
of existing ResNet-based architectures.

3.1. Effect of PyramidNet

According to the work of Veit et al. [33], ResNets can be
viewed as ensembles of relatively shallow networks, sup-
ported by the observation that deleting an individual build-
ing block in a residual unit of ResNets incurs minor classi-
ﬁcation loss, whereas removing layers from plain networks
such as VGG [25] severely reduces the classiﬁcation rate.
However, in both original and pre-activation ResNets [7, 8],
another noteworthy aspect is that deleting the units with
downsampling (and doubling the feature dimension) still
degrades performance by a large margin [33]. Meanwhile,

(a)

(b)

(c)

Figure 2. Visual illustrations of (a) additive PyramidNet, (b) mul-
tiplicative PyramidNet, and (c) a comparison of (a) and (b).

map dimension as follows:
(cid:40)

Dk =

16,
(cid:98)Dk−1 + α/N (cid:99),

if k = 1,
if 2 ≤ k ≤ N + 1,

(2)

in which N denotes the total number of residual units, de-
ﬁned as N = (cid:80)4
n=2 Nn. The dimension is increased by a
step factor of α/N , and the output dimension of the ﬁnal
unit of each group becomes 16 + (n − 1)α/3 with same
number of residual units in each group. The details of our
network architecture are presented in Table 1.

The above equations are based on an addition-based
widening step factor α for increasing dimensions. However,
of course, multiplication-based widening (i.e., the process
of multiplying by a factor to increase the channel dimen-
sion geometrically) presents another possibility for creating
a pyramid-like structure. Then, eq.(2) can be transformed
as follows:

(cid:40)

Dk =

16,
(cid:98)Dk−1 · α 1

N (cid:99),

if k = 1,
if 2 ≤ k ≤ N + 1.

(3)

The main difference between additive and multiplicative
PyramidNets is that the feature map dimension of an ad-
ditive network gradually increases linearly, whereas the di-
mension of a multiplicative network increases geometri-
cally. That is, the dimension slowly increases in input-side
layers and sharply increases in output-side layers. This pro-
cess is similar to that of the original deep network architec-
tures such as VGG [25] and ResNet [7]. The visual illustra-
tions of additive and multiplicative PyramidNets are shown
in Figure 2. In this paper, we compare the performance of
both of these dimension-increasing approaches by compar-
ing an additive PyramidNet (eq. (2)) and a multiplicative
PyramidNet (eq. (3)) in section 4.

2.2. Building Block

The building block (i.e., the convolutional ﬁlter stacks
with ReLUs and BN layers) in a residual unit is the core
of ResNet-based architectures. It is obvious that in order
to maximize the capability of the network architecture, de-
signing a good building block is essential. As shown in

Figure 3. Performance comparison between the pre-activation
ResNet [8] and our PyramidNet, using CIFAR datasets. Dashed
and solid lines denote the training loss and test error, respectively.

when a stochastic depth [10] is applied, this phenomenon
is not observed, and the performance is also improved, ac-
cording to the experiment of Veit et al. [33]. The objective
of our PyramidNet is to resolve this phenomenon differ-
ently, by attempting to gradually increase the feature map
dimension instead of doubling it at one of the residual units
and to evenly distribute the burden of increasing the feature
maps. We observed that our PyramidNet indeed resolves
this phenomenon and at the same time improves overall per-
formance. We further analyze the effect of our PyramidNet
by comparing it against the pre-activation ResNet, with the
following experimental results. First, we compare the train-
ing and test error curves of our PyramidNet with those of
the pre-activation ResNet [8] in Figure 3. The standard pre-
activation ResNet with 110 layers is used for comparison.
For our PyramidNet, we used a depth of 110 layers with a
widening factor of α = 48; it had the same number of pa-
rameters (1.7M) as the pre-activation ResNet to allow for a
fair comparison. The results indicate that our PyramidNet
has superior test accuracy, thereby conﬁrming its greater
ability to generalize compared to existing deep networks.

Second, we verify the ensemble effect of our Pyramid-
Nets by evaluating the performance after deleting individ-
ual units, similar to the experiment of Veit et al. [33]. The
results are shown in Figure 4. As mentioned by Veit et
al. [33], removing individual units only causes a slight
performance loss, compared with a plain network such as
the VGG [25]. However, in the case of the pre-activation
ResNet, removing the blocks subjected to downsampling
tends to affect the classiﬁcation accuracy by a relatively
large margin, whereas this does not occur with our Pyra-
midNets. Furthermore, the mean average error differences
between the baseline result and the result obtained when
individual units were deleted from both the pre-activation
ResNet and our PyramidNet were 0.72% and 0.54%, re-

Figure 4. Test error curves to study the extent to which residual
units contribute to the performance in different network architec-
tures by deleting their individual units. The dashed and solid lines
denote the test errors that occur when no units are deleted, and
when an individual unit is deleted, respectively. Bold vertical lines
denote the location of residual units through downsampling.

spectively. This result shows that the ensemble effect of
our PyramidNet becomes stronger than the original ResNet,
such that generalization ability is improved.

3.2. Zero-padded Shortcut Connection

ResNets and pre-activation ResNets [7, 8] were stud-
ied several types of shortcuts, such as an identity-mapping
shortcut or projection shortcut. The experimental results
in [8] showed that the identity-mapping shortcut is a much
more appropriate choice than other shortcuts. Because an
identity-mapping shortcut does not have parameters, it has a
lower possibility of overﬁtting compared to the other types
of shortcuts; this ensures improved generalization ability.
Moreover, it can purely pass through the gradient accord-
ing to the identity mapping, and therefore it provides more
stability in the training stage.

In the case of our PyramidNet, identity mapping alone
cannot be used for a shortcut because the feature map di-
mension differs among individual residual units. Therefore,
only a zero-padded shortcut or projection shortcut can be
used for all the residual units. However, as discussed in [8],
a projection shortcut can hamper information propagation
and lead to optimization problems, especially for very deep
networks. On the other hand, we found that the zero-padded
shortcut does not lead to the overﬁtting problem because no
additional parameters exist, and surprisingly, it shows sig-
niﬁcant generalization ability compared to other shortcuts.
We now examine the effect of the zero-padded identity-
mapping shortcut on the k-th residual unit that belongs to
the n-th group with the reshaped vector xl
k of the l-th fea-
ture map:

(cid:40)

xl

k =

F(k,l)(xl
F(k,l)(xl

k−1) + xl
k−1),

k−1,

if 1 ≤ l ≤ Dk−1
if Dk−1 < l ≤ Dk

(4)

where F(k,l)(·) denotes the l-th residual function of the k-
th residual unit and Dk represents the pre-deﬁned channel

Shortcut Types
(a) Identity mapping with projection shortcut
(b) Projection with zero-padded shortcut
(c) Only projection shortcut
(d) Identity mapping with zero-padded shortcut

CIFAR-10
5.03
6.84
6.98
4.70

CIFAR-100
23.48
31.29
31.62
22.77

Table 2. Top-1 errors (%) on CIFAR datasets using our Pyramid-
Net with several combinations of shortcut connections.

tions and the number of ReLUs. This could be discussed
with original ResNets [7], for which it was shown that
the performance increases as the network becomes deeper;
however, if the depth exceeds 1,000 layers, overﬁtting still
occurs and the result is less accurate than that generated by
shallower ResNets.

First, we note that using ReLUs after the addition of

residual units adversely affects performance:

k = ReLU (F(k ,l)(xl
xl

k −1 ) + xl

k −1 ),

(5)

where the ReLUs seem to have the function of ﬁltering non-
negative elements. Gross and Wilber [5] found that simply
removing ReLUs from the original ResNet [7] after each
addition with the shortcut connection leads to small perfor-
mance improvements. This could be understood by consid-
ering that, after addition, ReLUs provide non-negative in-
put to the subsequent residual units, and therefore the short-
cut connection is always non-negative and the convolutional
layers would take responsibility for producing negative out-
put before addition; this may decrease the overall capability
of the network architecture as analyzed in [8]. The pre-
activation ResNets proposed by He et al. [8] also overcame
this issue with pre-activated residual units that place BN
layers and ReLUs before (instead of after) the convolutional
layers:

k = F(k,l)(xl
xl

k−1) + xl

k−1,

(6)

where ReLUs are removed after addition to create an iden-
tity path. Consequently, the overall performance has in-
creased by a large margin without overﬁtting, even at depths
exceeding 1,000 layers. Furthermore, Shen et al. [24] pro-
posed a weighted residual network architecture, which lo-
cates a ReLU inside a residual unit (instead of locating
ReLU after addition) to create an identity path, and showed
that this structure also does not overﬁt even at depths of
more than 1,000 layers.

Second, we found that the use of a large number of Re-
LUs in the blocks of each residual unit may negatively af-
fect performance. Removing the ﬁrst ReLU in the blocks
of each residual unit, as shown in Figure 6 (b) and (d), was
found to enhance performance compared with the blocks
shown in Figure 6 (a) and (c). Experimentally, we found
that removal of the ﬁrst ReLU in the stack is preferable and
that the other ReLU should remain to ensure nonlinearity.
Removing the second ReLU in Figure 6 (a) changes the

(a)

(b)

Figure 5. Structure of residual unit (a) with zero-padded identity-
mapping shortcut, (b) unraveled view of (a) showing that the zero-
padded identity-mapping shortcut constitutes a mixture of a resid-
ual network with a shortcut connection and a plain network.

dimensions of the k-th residual unit. From eq.(4), zero-
padded elements of the identity-mapping shortcut for in-
creasing dimension let xl
k contain the outputs of both resid-
ual networks and plain networks. Therefore, we could con-
jecture that each zero-padded identity-mapping shortcut can
provide a mixture of the residual network and plain net-
work, as shown in Figure 5. Furthermore, our Pyramid-
Net increases the channel dimension at every residual unit,
and the mixture effect of the residual network and plain net-
work increases markedly. Figure 4 supports the conclusion
that the test error of PyramidNet does not oscillate as much
as that of the pre-activation ResNet. Finally, we investigate
several types of shortcuts including proposed zero-padded
identity-mapping shortcut in Table 2.

3.3. A New Building Block

To maximize the capability of the network, it is natural
to ask the following question: “Can we design a better
building block by altering the stacked elements inside
the building block in more principled way?”. The ﬁrst
building block types were proposed in the original paper on
ResNets [7], and another type of building block was subse-
quently proposed in the paper on pre-activation ResNets [8],
to answer the question. Moreover, pre-activation ResNets
attempted to solve the backward gradient ﬂowing problem
[8] by redesigning residual modules; this proved to be suc-
cessful in trials. However, although the pre-activation resid-
ual unit was discovered with empirically improved perfor-
mance, further investigation over the possible combinations
is not yet performed, leaving a potential room for improve-
ment. We next attempt to answer the question from two
points of view by considering Rectiﬁed Linear Units (Re-
LUs) [20] and Batch Normalization (BN) [11] layers.

3.3.1 ReLUs in a Building Block

Including ReLUs [20] in the building blocks of residual
units is essential for nonlinearity; however, we found empir-
ically that the performance can vary depending on the loca-

(a)

(b)

(c)

(d)

Figure 6. Various types of basic and bottleneck residual units. “BatchNorm” denotes a Batch Normalization (BN) layer. (a) original
pre-activation ResNets [8], (b) pre-activation ResNets removing the ﬁrst ReLU, (c) pre-activation ResNets with a BN layer after the ﬁnal
convolutional layer, and (d) pre-activation ResNets removing the ﬁst ReLU with a BN layer after the ﬁnal convolutional layer.

blocks to BN-ReLU-conv-BN-conv, and it is clear that, in
these blocks, the convolutional layers are successively lo-
cated without ReLUs to weaken their representation pow-
ers of each other. However, when we remove the ﬁrst
ReLU, the blocks are changed to BN-conv-BN-ReLU-conv,
in which case the two convolutional layers are separated by
the second ReLU, thereby guaranteeing nonlinearity. The
results in Table 3 conﬁrm that removing the ﬁrst ReLU as
in (b) and (d) in Figure 6, enhances the performance. Con-
sequently, provided that an appropriate number of ReLUs
are used to guarantee the nonlinearity of the feature space
manifold, the remaining ReLUs could be removed to im-
prove network performance.

3.3.2 BN Layers in a Building Block

The main role of a BN layer is to normalize the activations
for fast convergence and to improve performance. The ex-
perimental results of the four structures provided in Table 3
show that the BN layer can be used to maximize the capabil-
ity of a single residual unit. A BN layer conducts an afﬁne
transformation with the following equation:

y = γx + β,

(7)

where γ and β are learned for every activation in feature
maps. We experimentally found that the learned γ and β
could closely approximate 0. This implies that if the learned
γ and β are both close to 0, then the corresponding acti-
vation is considered not to be useful. Weighted ResNets
[24], in which the learnable weights occur at the end of
their building blocks, are also similarly learned to determine
whether the corresponding residual unit is useful. Thus, the
BN layers at the end of each residual unit are a generalized
version including [24] to enable decisions to be made as to
whether each residual unit is helpful. Therefore, the degrees

ResNet Architecture
(a) Pre-activation [8]
(b) Removing the ﬁrst ReLU
(c) BN after the ﬁnal conv
(d) (b) + (c)
PyramidNet Architecture
(a) Pre-activation [8]
(b) Removing the ﬁrst ReLU
(c) BN after the ﬁnal conv
(d) (b) + (c)
PyramidNet (bottleneck) Architecture
(a) Pre-activation [8]
(b) Removing the ﬁrst ReLU
(c) BN after the ﬁnal conv
(d) (b) + (c)

CIFAR-10
5.82
5.31
5.74
5.29
CIFAR-10
5.15
4.81
4.96
4.62
CIFAR-10
4.61
4.45
4.56
4.26

CIFAR-100
25.06
24.55
24.54
23.74
CIFAR-100
24.40
23.43
23.89
23.31
CIFAR-100
21.10
20.40
20.44
20.32

Table 3. Top-1 errors (%) on CIFAR datasets for several build-
ing block combinations of ReLUs and BN layers shown in Fig-
ure 6 (a)–(d), using ResNet [8] (with original feature map dimen-
sion conﬁguration) and our PyramidNet.

of freedom obtained by involving γ and β from the BN lay-
ers could improve the capability of the network architecture.
The results in Table 3 support the conclusion that adding a
BN layer at the end of each building block, as in type (c)
and (d) in Figure 6, improves the performance. Note that
the aforementioned network removing the ﬁrst ReLU is also
improved by adding a BN layer after the ﬁnal convolutional
layer. Furthermore, the results in Table 3 show that both
PyramidNet and a new building block improve the perfor-
mance signiﬁcantly.

4. Experimental Results

We evaluate and compare the performance of our algo-
rithm with that of existing algorithms [7, 8, 18, 24, 34] using
representative benchmark datasets: CIFAR-10 and CIFAR-
100 [12]. CIFAR-10 and CIFAR-100 each contain 32×32-
pixel color images, consists of 50,000 training images and
10,000 testing images. But in case of CIFAR-10, it includes

# of Params Output Feat. Dim. Depth

Network
NiN [18]
All-CNN [27]
DSN [17]
FitNet [21]
Highway [29]
Fractional Max-pooling [4]
ELU [29]
ResNet [7]
ResNet [7]
ResNet [7]
Pre-activation ResNet [8]
Pre-activation ResNet [8]
Stochastic Depth [10]
Stochastic Depth [10]
FractalNet [14]
SwapOut v2 (width×4) [26]
Wide ResNet (width×4) [34]
Wide ResNet (width×10) [34]
Weighted ResNet [24]
DenseNet (k = 24) [9]
DenseNet-BC (k = 40) [9]
PyramidNet (α = 48)
PyramidNet (α = 84)
PyramidNet (α = 270)
PyramidNet (bottleneck, α = 270)
PyramidNet (bottleneck, α = 240)
PyramidNet (bottleneck, α = 220)
PyramidNet (bottleneck, α = 200)

-
-
-
-
-
-
-
1.7M
10.2M
19.4M
1.7M
10.2M
1.7M
10.2M
38.6M
7.4M
8.7M
36.5M
19.1M
27.2M
25.6M
1.7M
3.8M
28.3M
27.0M
26.6M
26.8M
26.0M

-
-
-
-
-
-
-
64
64
64
64
64
64
64
1,024
256
256
640
64
2,352
2,190
64
100
286
1,144
1,024
944
864

Training Mem.
-
-
-
-
-
-
-
547MB
2,921MB
2,069MB
841MB
2,921MB
547MB
2,069MB
-
-
775MB
1,383MB
-
4,381MB
7,247MB
655MB
781MB
1,437MB
4,169MB
4,451MB
4,767MB
5,005MB

CIFAR-10
8.81
7.25
7.97
8.39
7.72
4.50
6.55
6.43
-
7.93
5.46
4.62
5.23
4.91
4.60
4.76
4.97
4.17
5.10
3.74
3.46
4.58±0.06
4.26±0.23
3.73±0.04
3.48±0.20
3.44±0.11
3.40±0.07
3.31±0.08

CIFAR-100
35.68
33.71
34.57
35.04
32.39
27.62
24.28
25.16
27.82
-
24.33
22.71
24.58
-
23.73
22.72
22.89
20.50
-
19.25
17.18
23.12±0.04
20.66±0.40
18.25±0.10
17.01±0.39
16.51±0.13
16.37±0.29
16.35±0.24

-
-
-
-
-
-
-
110
1001
1202
164
1001
110
1202
21
32
40
28
1192
100
190
110
110
110
164
200
236
272

Table 4. Top-1 error rates (%) on CIFAR datasets. All the results of PyramidNets are produced with additive PyramidNets, and α denotes
the widening factor. “Output Feat. Dim.” denotes the feature dimension of just before the last softmax classiﬁer. The best results are
highlighted in red.

10 classes, and CIFAR-100 includes 100 classes. The stan-
dard data augmentation, horizontal ﬂipping, and translation
by 4 pixels are adopted in our experiments, following the
common practice [18]. The results achieved by Pyramid-
Nets are based on the proposed residual unit: placing a BN
layer after the ﬁnal convolutional layer, and removing the
ﬁrst ReLU as in Figure 6 (d). Our code is built on Torch
open source deep learning framework [1].

4.1. Training Settings

Our PyramidNets are trained using backpropagation [15]
by Stochastic Gradient Descent (SGD) with Nesterov mo-
mentum for 300 epochs on CIFAR-10 and CIFAR-100
datasets. The initial learning rate is set to 0.1 for CIFAR-10
and 0.5 for CIFAR-100, and is decayed by a factor of 0.1 at
150 and 225 epochs, respectively. The ﬁlter parameters are
initialized by “msra” [6]. We use a weight decay of 0.0001,
a dampening of 0, a momentum of 0.9, and a batch size of
128.

4.2. Performance Evaluation

In our work, we mainly use the top-1 error rate for evalu-
ating our network architecture. Additive PyramidNets with

Figure 7. Comparison of test error curves with error bars of ad-
ditive PyramidNet and multiplicative PyramidNet on CIFAR-10
(left) and CIFAR-100 (right) datasets, according to the different
number of parameters.

both basic and pyramidal bottleneck residual units are used.
The error rates are provided in Table 4 for ours and the state-
of-the-art models. The experimental results show that our
network has superior generalization ability, in terms of the
number of parameters, showing the best results compared
with other models.

Figure 7 compares additive and multiplicative Pyramid-
Nets using CIFAR datasets. When the number of param-
eters is low, both additive and multiplicative PyramidNets
show similar performance, because these two network ar-

# of Params Output Feat. Dim.

Network
ResNet-152 [7]
Pre-ResNet-152† [8]
Pre-ResNet-200† [8]
WRN-50-2-bottleneck [34]
PyramidNet-200 (α = 300)
PyramidNet-200 (α = 300)∗
PyramidNet-200 (α = 450)∗
ResNet-200 [7]
Pre-ResNet-200 [8]
Inception-v3 [32]
Inception-ResNet-v1 [30]
Inception-v4 [30]
Inception-ResNet-v2 [30]
PyramidNet-200 (α = 300)
PyramidNet-200 (α = 300)∗
PyramidNet-200 (α = 450)∗

60.0M
60.0M
64.5M
68.9M
62.1M
62.1M
116.4M
64.5M
64.5M
-
-
-
-
62.1M
62.1M
116.4M

2,048
2,048
2,048
2,048
1,456
1,456
2,056
2,048
2,048
2,048
1,792
1,536
1,792
1,456
1,456
2,056

Augmentation
scale
scale+asp ratio
scale+asp ratio
scale+asp ratio
scale+asp ratio
scale+asp ratio
scale+asp ratio
scale
scale+asp ratio
scale+asp ratio
scale+asp ratio
scale+asp ratio
scale+asp ratio
scale+asp ratio
scale+asp ratio
scale+asp ratio

Train Crop
224×224
224×224
224×224
224×224
224×224
224×224
224×224
224×224
224×224
299×299
299×299
299×299
299×299
224×224
224×224
224×224

Test Crop
224×224
224×224
224×224
224×224
224×224
224×224
224×224
320×320
320×320
299×299
299×299
299×299
299×299
320×320
320×320
320×320

Top-1
23.0
22.2
21.7
21.9
20.5
20.5
20.1
21.8
20.1
21.2
21.3
20.0
19.9
19.6
19.5
19.2

Top-5
6.7
6.2
5.8
6.0
5.3
5.4
5.4
6.0
4.8
5.6
5.5
5.0
4.9
4.8
4.8
4.7

Table 5. Comparisons of single-model, single-crop error (%) on the ILSVRC 2012 validation set. All the results of PyramidNets are
produced with additive PyramidNets. “asp ratio” means the aspect ratio applied for data augmention, and “Output feat. dim.” denotes the
feature dimension of just after the last global pooling layer. ∗ denotes the models which applied dropout method, and † denotes the results
obtained from https://github.com/facebook/fb.resnet.torch.

chitectures do not have signiﬁcant structural differences.
As the number of parameters increases, they start to show
a more marked difference in terms of the feature map di-
mension conﬁguration. Because the feature map dimension
increases linearly in the case of additive PyramidNets, the
feature map dimensions of the input-side layers tend to be
larger, and those of the output-side layers tend to be smaller,
compared with multiplicative PyramidNets as illustrated in
Figure 2.

Previous works [7, 25] typically set multiplicative scal-
ing of feature map dimension for downsampling modules,
which is implemented to give a larger degree of freedom to
the classiﬁcation part by increasing the feature map dimen-
sion of the output-side layers. However, for our Pyramid-
Net, the results in Figure 7 implies that increasing the model
capacity of the input-side layers would lead to a better per-
formance improvement than using a conventional way of
multiplicative scaling of feature map dimension.

We also note that, although the use of regularization
methods such as dropout [28] or stochastic depth [10] could
further improve the performance of our model, we did not
involve those methods to ensure a fair comparison with
other models.

We train our models for 120 epochs with a batch size of
128, and the initial learning rate is set to 0.05, divided by 10
at 60, 90 and 105 epochs. We use the same weight decay,
momentum, and initialization settings as those of CIFAR
datasets. We train our model by using a standard data aug-
mentation with scale jittering and aspect ratio as suggested
in Szegedy et al. [31]. Table 5 shows the results of our
PyramidNets in ImageNet dataset compared with the state-
of-the-art models. The experimental results show that our
PyramidNet with α = 300 has a top-1 error rate of 20.5%,
which is 1.2% lower than the pre-activation ResNet-200 [8]
which has a similar number of parameters but higher out-
put feature dimension than our model. We also notice that
increasing α with an appropriate regularization method can
further improve the performance.

For comparison with the Inception-ResNet [30] that uses
a testing crop with 299 × 299 size, we test our model on a
320 × 320 crop, by the same reason with the work of He et
al. [8]. Our PyramidNet with α = 300 shows a top-1 error
rate of 19.6%, which outperforms both the pre-activation
ResNet [8] and the Inception-ResNet-v2 [30] models.

5. Conclusion

4.3. ImageNet

1,000-class ImageNet dataset [22] used for ILSVRC
contains more than one million training images and 50,000
validation images. We use additive PyramidNets with the
pyramidal bottleneck residual units, deleting the ﬁrst ReLU
and adding a BN layer at the last layer as described in Sec-
tion 3.3 and shown in Figure 6 (d) for further performance
improvement.

The main idea of the novel deep network architecture
described in this paper involves increasing the feature map
dimension gradually, in order to construct so-called Pyra-
midNets along with the concept of ResNets. We also devel-
oped a novel residual unit, which includes a new building
block for a residual unit with a zero-padded shortcut; this
design leads to signiﬁcantly improved generalization abil-
ity. In tests using CIFAR-10, CIFAR-100, and ImageNet-
1k datasets, our PyramidNets outperform all previous state-

of-the-art deep network architectures. Furthermore, the in-
sights in this paper could be utilized by any network archi-
tecture, to improve their capacity for better performance.
In future work, we will develop methods of optimizing pa-
rameters such as feature map dimensions in more principled
ways with proper cost functions that give insight into the na-
ture of residual networks.
Acknowledgements: This work was supported by the ICT
R&D program of MSIP/IITP, 2016-0-00563, Research on
Adaptive Machine Learning Technology Development for
Intelligent Autonomous Digital Companion.

References

[1] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A
matlab-like environment for machine learning. In BigLearn,
NIPS Workshop, 2011. 7

[2] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,
E. Tzeng, and T. Darrell. Decaf: A deep convolutional acti-
vation feature for generic visual recognition. In ICML, 2014.
1

[3] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
segmentation. In CVPR, 2014. 1

[4] B. Graham.

Fractional max-pooling.

arXiv preprint

arXiv:1412.6071, 2014. 7

[5] S. Gross and M. Wilber. Training and investigating residual

nets. 2016. http://torch.ch/blog/2016/02/04/resnets.html. 5

[6] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation. In ICCV, 2015. 7

[7] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 1, 2, 3, 4, 5, 6, 7, 8

[8] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in
deep residual networks. In ECCV, 2016. 1, 2, 3, 4, 5, 6, 7, 8
[9] G. Huang, Z. Liu, and K. Q. Weinberger. Densely connected
convolutional networks. arXiv preprint arXiv:1608.06993,
2016. 7

[10] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Weinberger. Deep
networks with stochastic depth. In ECCV, 2016. 1, 2, 4, 7, 8
[11] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, 2015. 5

[12] A. Krizhevsky. Learning multiple layers of features from

tiny images. In Tech Report, 2009. 2, 6

[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

ImageNet
Classiﬁcation with Deep Convolutional Neural Networks. In
NIPS, 2012. 1, 2

[14] G. Larsson, M. Maire, and G. Shakhnarovich. Fractalnet:
Ultra-deep neural networks without residuals. arXiv preprint
arXiv:1605.07648, 2016. 7

[15] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.
Howard, W. Hubbard, and L. D. Jackel. Backpropagation
applied to handwritten zip code recognition. Neural compu-
tation, 1(4):541–551, 1989. 7

[16] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278–2324, 1998. 1

[17] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-

supervised nets. In AISTATS, 2015. 7

[18] M. Lin, Q. Chen, and S. Yan. Network in network. In ICLR,

2014. 6, 7

[19] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In CVPR, 2015. 1
[20] V. Nair and G. E. Hinton. Rectiﬁed linear units improve re-

stricted boltzmann machines. In ICML, 2010. 5

[21] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta,
and Y. Bengio. Fitnets: Hints for thin deep nets. In ICLR,
2015. 7

[22] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge. International Journal of Computer
Vision, 115(3):211–252, 2015. 1, 8

[23] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,
and Y. LeCun. Overfeat: Integrated recognition, localization
and detection using convolutional networks. In ICLR, 2014.
1

[24] F. Shen and G. Zeng. Weighted residuals for very deep net-
works. arXiv preprint arXiv:1605.08831, 2016. 5, 6, 7
[25] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
1, 2, 3, 4, 8

[26] S. Singh, D. Hoiem, and D. Forsyth. Swapout: Learning an

ensemble of deep architectures. In NIPS, 2016. 7

[27] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Ried-
miller. Striving for simplicity: The all convolutional net. In
ICLR Workshop, 2015. 7

[28] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov. Dropout: A simple way to prevent neu-
ral networks from overﬁtting. Journal of Machine Learning
Research, 15:1929–1958, 2014. 8

[29] R. K. Srivastava, K. Greff, and J. Schmidhuber. Training

very deep networks. In NIPS, 2015. 1, 7
[30] C. Szegedy, S. Ioffe, and V. Vanhoucke.

Inception-v4,
inception-resnet and the impact of residual connections on
learning. In ICLR Workshop, 2016. 1, 8

[31] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015. 1, 2, 8
[32] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
Rethinking the inception architecture for computer vision. In
CVPR, 2016. 8

[33] A. Veit, M. Wilber, and S. Belongie. Residual networks be-
have like ensembles of relatively shallow networks. In NIPS,
2016. 1, 2, 3, 4

[34] S. Zagoruyko and N. Komodakis. Wide residual networks.

In BMVC, 2016. 2, 6, 7, 8

[35] M. D. Zeiler and R. Fergus. Visualizing and understanding

convolutional networks. In ECCV, 2014. 1, 2

