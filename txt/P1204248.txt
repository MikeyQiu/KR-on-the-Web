6
1
0
2
 
g
u
A
 
0
1
 
 
]

V
C
.
s
c
[
 
 
3
v
2
7
2
0
0
.
8
0
6
1
:
v
i
X
r
a

Modeling Context in Referring Expressions

Licheng Yu, Patrick Poirson, Shan Yang, Alexander C. Berg, Tamara L. Berg

Department of Computer Science,
University of North Carolina at Chapel Hill
{licheng,poirson,alexyang,aberg,tlberg}@cs.unc.edu

Abstract. Humans refer to objects in their environments all the time,
especially in dialogue with other people. We explore generating and com-
prehending natural language referring expressions for objects in images.
In particular, we focus on incorporating better measures of visual con-
text into referring expression models and ﬁnd that visual comparison to
other objects within an image helps improve performance signiﬁcantly.
We also develop methods to tie the language generation process together,
so that we generate expressions for all objects of a particular category
jointly. Evaluation on three recent datasets - RefCOCO, RefCOCO+,
and RefCOCOg1, shows the advantages of our methods for both refer-
ring expression generation and comprehension.

Keywords: language, language and vision, generation, referring expres-
sion generation

1 Introduction

In this paper, we look at the dual-tasks of generating and comprehending nat-
ural language expressions referring to particular objects within an image. Re-
ferring to objects is a natural and common experience. For example, one often
uses referring expressions in everyday speech to indicate a particular person or
object to a co-observer, e.g., “the man in the red hat” or “the book on the ta-
ble”. Computational models to generate and comprehend such expressions would
have applicability to human-computer interactions, especially for agents such as
robots, interacting with humans in the physical world.

Successful models will have to connect both recognition of visual attributes of
objects and eﬀective natural language generation to compose useful expressions
for dialogue. A broader version of this latter goal was considered in 1975 by
Paul Grice who introduced maxims describing cooperative conversation between
people [11]. These maxims, called the Gricean Maxims, describe a set of rational
principles for natural language dialogue interactions. The 4 maxims are: quality
(try to be truthful), quantity (make your contribution as informative as you can,
giving as much information as is needed but no more), relevance (be relevant
and pertinent to the discussion), and manner (be as clear, brief, and orderly as
possible, avoiding obscurity and ambiguity).

1 Datasets and toolbox can be downloaded from https://github.com/lichengunc/refer

2

Licheng et al.

Fig. 1. Example referring expressions for the giraﬀe outlined in green from three re-
ferring expression datasets (described in Sec 4).

For the purpose of referring to objects in complex real world scenes these
maxims suggest that a well formed expression should be informative, succinct,
and unambiguous. The last point is especially necessary for referring to objects
in the real world since we often ﬁnd multiple objects of a particular category
situated together in a scene. For example, consider the image in Fig. 1 which
contains three giraﬀes. We should not refer to the target (outlined in green) as
“the spotted giraﬀe” since all of the giraﬀes are spotted and this would create
an ambiguous reference. More reasonably we should refer to the target as “the
giraﬀe with lowered head” to diﬀerentiate this giraﬀe from the other two.

The task of referring expression generation (REG) has been studied since the
1970s [40,22,30,7], with most work focused on studying particular aspects of the
problem in some relatively constrained datasets. Recent approaches have pushed
this work toword more realistic scenarios. Kazemzadeh et al [19] introduced the
ﬁrst large-scale dataset of referring expressions for objects in real-world natural
images, collected in a two-player game. This dataset was originally collected on
top of the 20,000 image ImageCleft dataset, but has recently been extended
to images from the MSCOCO collection. We make use of the RefCOCO and
RefCOCO+ datasets in our work along with another recently collected referring
expression dataset, released by Google, denoted in our paper as RefCOCOg [26].
The most relevant work to ours is Mao et al [26] which introduced the ﬁrst
deep learning approach to REG. In this model, the authors use a Convolutional
Neural Network (CNN) [36] model pre-trained on ImageNet [34] to extract visual
features from a bounding box around the target object and from the entire image.
They use these features plus 5 features encoding the target object location and
size as input to a Long Short-term Memory (LSTM) [10] network that generates
expressions. Additionally, they apply the same model to the inverse problem
of referring expression comprehension where the input is a natural language
expression and the goal is to localize the referred object in the image.

Similar to these recent methods, we also take a deep learning approach to
referring expression generation and comprehension. However, while they use a
generic model for object context – CNN features for the entire image containing

Modeling Context in Referring Expressions

3

the target object – we take a more focused approach to encode object com-
parisons. These object comparisons are critical for producing an unambiguous
referring expression since one must consider visual characteristics of similar ob-
jects during generation in order to select the most distinct aspects for description.
This mimics the process that a human would use to compose a good referring ex-
pression for an object, e.g. look at the object, look at other relevant objects, and
generate an expression that could be used by a co-observer to unambiguously
pick out the target object.

In addition, for the referring expression generation task, we introduce a
method to tie the language generation process together for all depicted objects
of the same type. This helps generate a good set of expressions such that the
expressions diﬀerentiate between objects but are also complementary. For exam-
ple, we never want to generate the exact same expression for two objects in an
image. Alternatively, if we call one object “the red ball” then we may desire the
expression for the other object to follow the same generation pattern, i.e., “the
blue ball”. Our experimental evaluations show that these visual and linguistic
comparisons improve performance over previous state of the art.

In the rest of our paper, we ﬁrst describe related work (Sec 2). We then
describe our improvements to models for referring expression generation and
comprehension (Sec 3), describe 3 referring expression datasets (Sec 4), and
perform experimental evaluations on several model variations (Sec 5). Finally
we present our conclusions (Sec 6).

2 Related Work

Referring expressions are closely related to the more general problem of modeling
the connection between images and descriptive language. In recent years, this has
been studied in the image captioning task [6,37,13,31,23]. There, the aim is to
condition the generation of language on the visual information from an image.
The wide range of aspects of an image that could be described, and the variety
of words that could be chosen for a particular description complicate studying
image captioning. Our study of referring expressions is partially motivated by
focusing on description for a speciﬁc, and more easily evaluated, communication
goal. Although our task is somewhat diﬀerent, we borrow machinery from state of
the art caption generation [3,39,27,5,18,21,41] using LSTM to generate captions
based on CNN features computed on an input image. Three recent approaches for
referring expression generation [26] and comprehension [14,33] also take a deep
learning approach. However, we add visual object comparisons and tie together
language generation for multiple objects.

Referring expression generation has been studied for many years [40,22,30]

in linguistics and natural language processing. These works were limited by data
collection and insuﬃcient computer vision algorithms. Together Amazon Me-
chanical Turk and CNNs have somewhat mitigated these limitations, allowing
us to revisit these ideas on large-scale datasets. We still use such work to motivate
the architecture of our pipeline. For instance, Mitchell and Jordan et al [30,16]

4

Licheng et al.

Fig. 2. Framework: We extract VGG-fc7 and location features for each object of the
same type, then compute visual diﬀerences. These features and diﬀerences are then fed
into LSTM. For sentence generation, the LSTMs are tied together, incorporating the
hidden output diﬀerence as additional information for predicting words.

show the importance of using attributes, Funakoshi et al [8] show the importance
of relative relations between objects in the same perceptual group, and Kelleher
et al [20] show the importance of spatial relationships. These provide motivation
for our modeling choices: when considering a referring expression for an object,
the model takes into account the relative spatial location of other objects of the
same type and visual comparisons to objects in the same perceptual group.

The REG datasets of the past were sometimes limited to using computer
generated images [38], or relatively small collections of natural objects [29,28,7].
Recently, a large-scale referring expression dataset was collected by Kazemzadeh
et al [19] featuring natural objects in the real world. Since then, another three
REG datasets based on the object labels in MSCOCO have been collected [19,26].
The availability of large-scale referring expression datasets allows us to train
deep learning models. Additionally, our analysis of these datasets motivates our
incorporation of visual comparisons between same-type objects, and the need to
tie together choices for referring expression generation between objects.

3 Models

We implement several model variations for referring expression generation and
comprehension. The ﬁrst set of models are recent state of the art deep learn-
ing approaches from Mao et al [26]. We use these as our baselines (Sec 3.1).
Next, we investigate incorporating better visual context features into the mod-
els (Sec 3.2). Finally, we explore methods to jointly produce an entire set of
referring expressions for all depicted objects of the same category (Sec 3.3).

3.1 Baselines

For comparison, we implement both the baseline and strong model of Mao et
al [26]. Both models utilize a pre-trained CNN network to model the target

Modeling Context in Referring Expressions

5

object and its context within the image, and then use a LSTM for generation.
In particular, object and context are modeled as features from a CNN trained to
recognize 1,000 object categories [36] from ImageNet [34]. Speciﬁcally, the visual
representation is composed of:

– Target object representation, oi. The object is modeled as features extracted
from the VGG-fc7 layer by forwarding its bounding box through the network.
– Global context representation, gi. Context is modeled as features extracted

from the VGG-fc7 layer for the entire image.

– Location/size representation, li, for the target object. Location and size are
modeled as a 5-d vector encoding the x and y locations of the top left and
bottom right corners of the target object bounding box, as well as the bound-
ing box size with respect to the image, i.e., li = [ xtl

W , ytl

H , xbr

W , ybr

H , w·h

W ·H ].

Language generation is handled by a long short-term memory network (LSTM)
[10] where inputs are the above visual features and the network is trained to
generate natural language referring expressions. In Mao et al’s baseline [26],
the model uses maximum likelihood training and outputs the most likely refer-
ring expression given the target object, context, and location/size features. In
addition, they also propose a stronger model that uses maximum mutual infor-
mation (MMI) training to consider whether a listener would interpret a referring
expression unambiguously. They impose this by penalizing the model if a gen-
erated referring expression could also be generated by some other object within
the image. We implement both their original model and MMI model in our ex-
periments. We subsequently refer to these two models as Baseline and MMI,
respectively.

3.2 Visual Comparison

Previous works [2,30] have shown that objects in an image, of the same type as
the target object, are most important for inﬂuencing what attributes people use
to describe the target. One drawback of considering a general feature over the
entire image to encode context (as in the baseline models) is that it may not
speciﬁcally focus on visual comparisons to the most relevant objects – the other
objects of the same object category within the image.

In this paper, we propose a more explicit encoding of the visual diﬀerence
between objects of the same category within an image. This helps for gener-
ating referring expressions which best discriminate the target object from the
surrounding objects. For example, in an image with three cars, two blue and
one red, visual appearance comparisons could help generate “the red car” as an
expression for the latter object.

Given the referred object and its surrounding objects, we compute two types
of features for visual comparison. The ﬁrst type encodes the similarities and
diﬀerences in visual appearance between the target object and other objects of
the same cateogry depicted in the image. Inspired by Sadeghi et al [35], we
compute the diﬀerence in visual CNN features as our representation of relative

6

Licheng et al.

appearance. Because there may be many surrounding objects of the same type
in the image, and not every object will provide useful information about how
to describe the target object, we need to ﬁrst select which objects to compare
and aggregate their visual diﬀerences. In Section 5, we experiment with selecting
diﬀerent subsets of comparison objects: objects of the same category, objects of
diﬀerent category, or all other depicted objects. For each selected comparison
object, we compute the appearance diﬀerence as the subtraction of the target
object and comparison object CNN representations. We experiment with three
diﬀerent strategies for computing an aggregate vector to represent the visual
diﬀerence between the target object and the surrounding objects: minimum,
maximum, and average over each feature dimension. In our experiments, pooling
the average diﬀerence between the target object and surrounding objects seems
to work best. Therefore, we use this pooling in all experiments.

– Visual appearance diﬀerence representation, δvi = 1
n

oi−oj
(cid:107)oi−oj (cid:107) , where n
is the number of objects chosen for comparisons and we use average pooling
to aggregate the diﬀerences.

j(cid:54)=i

(cid:80)

The second type of comparison feature encodes the relative location and size
diﬀerences between the target object and surrounding objects of the same ob-
ject category. People often use comparative size or location terms in referring
expressions, e.g. “the second giraﬀe from the left” or “the smaller monkey” [38].
To address the dynamic number of nearby objects, we choose up to ﬁve com-
parison objects of the same category as the target object, sorted by distance to
the target. When fewer than ﬁve objects of the same category are depicted, this
25-d vector (5-d x 5 surrounding objects) is padded with zeros.

– Location diﬀerence representation, δli, where each 5-d diﬀerence is computed
, [(cid:52)ybr]ij
hi

as δlij = [ [(cid:52)xtl]ij

, [(cid:52)xbr]ij
wi

, [(cid:52)ytl]ij
hi

, wj hj
wihi

wi

].

In summary, our ﬁnal visual representation for a target object is:

ri = Wm[oi, gi, li, δvi, δli] + bm

(1)

where oi, gi, li are the target object, global context, and location/size features
from the baseline model, δvi and δli encodes visual appearance diﬀerence and
location diﬀerence. Wm and bm project the concatenation of the ﬁve types of
features to be the ﬁnal representation.

3.3 Joint Language Generation

For the referring expression generation task, rather than generating sentences
for each object in an image separately [15][26], we consider tying the generation
process together into a single task to jointly generate expressions for all objects
of the same object category depicted in an image. This makes sense intuitively
– when a person attempts to generate a referring expression for an object in an
image they inherently compose that expression while keeping in mind expressions

Modeling Context in Referring Expressions

7

for the other objects in the picture. This can be observed in the fact that the
expressions people generate for objects in an image tend to share similar patterns
of expression. If you say “the man on the left” for one object then you tend to
say “the man on the right” for the other object. We would like our algorithms to
mimic these behaviors. Additionally, the algorithm should also be able to push
generated expressions away from each other to create less ambiguous references.
For example, if we use the word “red” to describe one object, then we probably
shouldn’t use the same word to describe another object.

To model this joint generation process, we model generation using an LSTM
model where in addition to the usual connections between time steps within
an expression we also add connections between expressions for diﬀerent objects.
This architecture is illustrated in Fig 2.

Speciﬁcally, we use LSTM to generate multiple referring expressions, {ri},

given depicted objects of the same type, {oj}.

P (R|O) =

P (ri|oi, {oj(cid:54)=i}, {rj(cid:54)=i}),

P (wit|wit−1 , ..., wi1, vi, {hjt,j(cid:54)=i})

(2)

(cid:89)

i
(cid:89)

=

(cid:89)

i

t

where wit are words at time t, vi visual representations, and hjt is the hidden
output of j-th object at time step t that encodes the visual and sentence infor-
mation for the j-th object. As visual comparison, we aggregate the diﬀerence of
hit −hjt
(cid:107)hit −hjt (cid:107) .
hidden outputs to push away ambiguous information. hdifit
There, n is the the number of other objects of the same type. The hidden diﬀer-
ence is jointly embedded with the target object’s hidden output, and forwarded
to the softmax layer for predicting the word.

= 1
n

(cid:80)

j(cid:54)=i

P (wit|wit−1, ..., wi1, vi, {hjt,j(cid:54)=i}) = softmax(Wh[hit, hdifit

] + bh)

(3)

4 Data

We make use of 3 referring expression datasets in our work, all collected on top
of the Microsoft COCO image collection [24]. One dataset, RefCOCOg [26] is
collected in a non-interactive setting, while the other two datasets, RefCOCO
and RefCOCO+, are collected interactively in a two-player game [19]. In the
following, we describe each dataset and provide some analysis of their similarities
and diﬀerences, and then discuss splits of the datasets used in our experiments .

4.1 Datasets & Analysis

Images for each dataset were selected to contain multiple objects of the same cat-
egory (object categories depicted cover the 80 common objects from MSCOCO
with ground-truth segmentation). These images provide useful cases for referring
expression generation since the referrer needs to compose a referring expression
that uniquely singles out one object from other relevant objects.

8

Licheng et al.

RefCOCOg: This dataset was collected on Amazon Mechanical Turk in a
non-interactive setting. One set of workers were asked to write natural language
referring expressions for objects in MSCOCO images then another set of workers
were asked to click on the indicated object given a referring expression. If the click
overlapped with the correct object then the referring expression was considered
valid and added to the dataset. If not, another referring expression was collected
for the object. This dataset consists of 85,474 referring expressions for 54,822
objects in 26,711 images. Images were selected to contain between 2 and 4 objects
of the same object category.

RefCOCO & RefCOCO+: These datasets were collected using the Refer-
itGame [19]. In this two-player game, the ﬁrst player is shown an image with a
segmented target object and asked to write a natural language expression refer-
ring to the target object. The second player is shown only the image and the
referring expression and asked to click on the corresponding object. If the players
do their job correctly, they receive points and swap roles. If not, they are pre-
sented with a new object and image for description. Images in these collections
were selected to contain two or more objects of the same object category. In the
RefCOCO dataset, no restrictions are placed on the type of language used in
the referring expressions while in the RefCOCO+ dataset players are disallowed
from using location words in their referring expressions by adding “taboo” words
to the ReferItGame. This dataset was collected to obtain a referring expression
dataset focsed on purely appearance based description, e.g., “the man in the
yellow polka-dotted shirt” rather than “the second man from the left”, which
tend to be more interesting from a computer vision based perspective and are in-
dependent of viewer perspective. RefCOCO consists of 142,209 refer expressions
for 50,000 objects in 19,994 images, and RefCOCO+ has 141,564 expressions for
49,856 objects in 19,992 images.

Dataset Comparisons: As shown in Fig. 1, the languages used in RefCOCO
and RefCOCO+ datasets tend to be more concise and less ﬂowery than the
languages used in the RefCOCOg. RefCOCO expressions have an average length
of 3.61 while RefCOCO+ have an average length of 3.53, and RefCOCOg contain
an average of 8.43 words. This is most likely due to the diﬀerences in collection
strategy. RefCOCO and RefCOCO+ were collected in a game scenario where
players are trying to eﬃciently provide enough information to indicate the correct
object to the other player. RefCOCOg was collected in independent rounds of
Mechanical Turk without any interactive time constraints and therefore tend to
provide more complex expressions, often entire sentences rather than phrases.

In addition, RefCOCO and RefCOCO+ do not limit the number of objects
of the same type to 4 and thus contain some images with many objects of the
same type. Both RefCOCO and RefCOCO+ contain an average of 3.9 same-
type objects per image, while RefCOCOg contains an average of 1.63 same-
type objects per image. The large number of same-type objects per image in
RefCOCO and RefCOCO+ suggests that incorporating visual comparisons to
same-type objecs will be useful.

Modeling Context in Referring Expressions

9

Dataset Splits: There are two types of splits of the data into train/test

sets: a per-object split and a people-vs-objects split.

The ﬁrst type is per-object split. In this split, the dataset is divided by
randomly partitioning objects into training and testing sets. This means that
each object will only appear either in training or testing set, but that one object
from an image may appear in the training set while another object from the
same image may appear in the test set. We use this split for RefCOCOg since
same division was used in the previous state-of-the-art approach [26].

The second type is people-vs-objects splits. One thing we observe from
analyzing the datasets is that about half of the referred objects are people.
Therefore, we create a split for RefCOCO and RefCOCO+ datasets that eval-
uates images containing multiple people (testA) vs images containing multiple
instances of all other objects (testB). In this split all objects from an image will
appear either in the training or testing sets, but not both. This split creates a
more meaningfully separated division between training and testing, allowing us
to evaluate the usefulness of context more fairly.

5 Experiments

We ﬁrst perform some experiments to analyze the use of context in referring
expressions (Sec 5.1). Given these ﬁndings, we then perform experiments eval-
uating the usefulness of our proposed visual and language innovations on the
comprehension (Sec 5.2) and generation tasks (Sec 5.3).

In experiments for the referring expression comprehension task, we use the
same evaluation as Mao et al [26], namely we ﬁrst predict the region referred
by the given expression, then we compute the intersection over union (IOU)
ratio between the true and predicted bounding box. If the IOU is larger than
0.5 we count it as a true positive. Otherwise, we count it as a false positive. We
average this score over all images. For the referring expression generation task
we use automatic evaluation metrics, BLEU, ROUGE, and METEOR developed
for evaluating machine translation results, commonly used to evaluate language
generation results [41,18,5,27,39,23]. We further perform human evaluations, and
propose a new metric evaluating the duplicate rate of generated expressions. For
both tasks, we compare our models with “Baseline” and “MMI” [26]. Speciﬁcally,

RefCOCO

RefCOCO+

no context

Test A Test B Test A Test B
63.91% 66.31% 50.09% 45.05%
global context 63.15% 64.21% 48.73% 42.13%
65.57% 67.13% 50.38% 44.89%
66.14% 68.07% 50.25% 45.40%
66.68% 68.56% 50.34% 45.48%

scale 2
scale 3
scale 4

Table 1. Expression Comprehension accuracies on RefCOCO and RefCOCO+ of the
Baseline model with diﬀerenct context source. Scale n indicates the size of the cropped
window centered by the target object.

10

Licheng et al.

we denote “visdif” as our visual comparison model, and “tie” as the LSTM tying
model. We also perform an ablation study, evaluating the combinations.

5.1 Analysis Experiments

Context Representation As previously discussed, we suggest that the ap-
proaches proposed in recent referring expression works [26,14] make use of rel-
atively weak contextual information, by only considering a single global image
context for all objects. To verify this intuition, we implemented both the baseline
and strong MMI models from Mao et al [26], and compare the results for referring
expression comprehension task with and without global context on RefCOCO
and Refcoco+ in Table 1. Surprisingly we ﬁnd that the global context does not
improve the performance of the model. In fact, adding context even decreases
performance slightly. This may be due to the fact that the global context for
each object in an image would be the same, introducing some ambiguity into the
referring expression comprehension task. Given these ﬁndings, we implemented
a simple modiﬁcation to the global context, computing the same visual repre-
sentation, but on a somewhat scaled window centered around the target object.
We found this to improve performance, suggesting room for improving the visual
context feature. This motivate our development of a better context feature.

Visual Comparison For our visual comparison model, there could be several
choices regarding which objects from the image should be compared to the target
object. We experiment with three sets of reference objects on RefCOCO and
RefCOCO+ datasets: a) objects of the same-category in the image, b) objects
of diﬀerent-category in the image, and c) all objects appeared in the image. We
use our “visdif” model for this experiment. The results are shown in Figure 3.
It is clear to see the visual comparisons to the same-category objects are most
useful for referring expression comprehension task. This is more like mimicing
how human refer object – we tend to point out the diﬀerence between the target
object with the other same-category objects within the same image.

Fig. 3. Comprehension accuracies on RefCOCO and RefCOCO+ datasets. We compare
the performance of “visdif” model without visual comparison, and visual comparison
between diﬀerent-category objects, between all objects, and between same-type objects.

Modeling Context in Referring Expressions

11

5.2 Referring Expression Comprehension

We evaluate performance on the referring expression comprehension task on Re-
fCOCO, RefCOCO+ and RefCOCOg datasets. For RefCOCO and RefCOCO+,
we evaluate on the two subsets of people (testA) and all other objects (testB).
For RefCOCOg, we evaluate on the per-object split as previous work [26]. Since
the authors haven’t released their testing set, we show the performance on their
validation set only, using the optimized hyper-parameters on RefCOCO. Ta-
ble 2 shows the comprehension accuracies. We observe that our implementation
of Mao et al [26] achieves comparable performance to the numbers reported in
their paper. We also ﬁnd that adding visual comparison features to the Baseline
model improves performance across all datasets and splits. Similar improvements
are also observed on top of the MMI model.

In order to make a fully automatic referring system, we also train a Fast-
RCNN [9] detector and build our system on top of the detections. We train
Fast-RCNN on the validation portion only as the RefCOCO and RefCOCO+
are collected using MSCOCO training data. For RefCOCOg, we use the detec-
tion results provided by [26], which were trained uisng Multibox [4]. Results on
shown in the bottom half of Table 2. Although all comprehension accuracies
drop due to imperfect detections, the improvements of our models over Baseline
and MMI are still observed. One weakness of our automatic system is that it
highly depends on detection performance, especially for general objects (testB).
However, considering our detector was trained on MSCOCO validation only, we
believe such weakness may be alleviated with more training data and stronger
detection techniques, e.g., [12][25][32][17][1], etc.

We show some automatic comprehension examples of RefCOCO, RefCOCO+
and RefCOCOg in Fig 4, where top three rows show correct comprehensions (ob-
ject correctly localized) and bottom three rows show incorrect comprehensions
(wrong object localized).

Baseline[26]
visdif
MMI[26]
visdif+MMI

RefCOCO

RefCOCO+

RefCOCOg
Test A Test B Test A Test B Validation
55.16%
63.15% 64.21% 48.73% 42.13%
59.25%
67.57% 71.19% 52.44% 47.51%
62.14%
71.72% 71.09% 58.42% 51.23%
73.98% 76.59% 59.17% 55.62% 64.02%

40.75%
Baseline(det)[26] 58.32% 48.48% 46.86% 34.04%
41.85%
62.50% 50.80% 50.10% 37.48%
visdif(det)
MMI(det)[26]
45.85%
64.90% 54.51% 54.03% 42.81%
visdif+MMI(det) 67.64% 55.16% 55.81% 43.43% 46.86%

Table 2. Referring Expression comprehension results on the RefCOCO, RefCOCO+,
and RefCOCOg datasets. Rows of “method(det)” are the results of automatic system
built on Fast-RCNN [9] and Multibox [4] detections.

12

Licheng et al.

5.3 Referring Expression Generation

For the referring expression generation task, we evaluate the usefulness of our
visual comparison features as well as our joint language generation model. These
serve to tie the generation process together so that the model considers other
objects of the same type both visually and linguistically during generation. On
the visual side, comparisons are used to judge similarity of the target object
to other objects of the same type in terms of appearance, size and location.
On the language side, the joint LSTM model serves to both diﬀerentiate and
mimic language patterns in the referring expressions for the entire set of depicted
objects. Fig 5 shows some comparison between our model with other methods.
Our full results are shown in Table 3. We ﬁnd that incorporating our visual
comparison features into the Baseline model improves generation quality (com-
pare row “Baseline” to row “visdif”). It also improves the performance of MMI
model (compare row “MMI” to row “visdif+MMI”). We also observe that tying
the language generation together across all objects consistently improves the per-
formance (compare the bottom three “+tie” rows with the above). Especially for
method “visdif+tie”, it achieves the highest score under almost every measure-
ment. We do not perform language tying on RefCOCOg since here some objects
from an image may appear in training while others may appear in testing.

Baseline [26]
MMI [26]
visdif
visdif+MMI
Baseline+tie
visdif+tie
visdif+MMI+tie

Baseline [26]
MMI [26]
visdif
visdif+MMI
Baseline+tie
visdif+tie
visdif+MMI+tie

Bleu 1
0.477
0.478
0.505
0.494
0.490
0.510
0.506

Bleu 1
0.391
0.370
0.407
0.386
0.392
0.409
0.393

RefCOCO

Test A

Bleu 2 Rouge Meteor
0.173
0.413
0.290
0.175
0.418
0.295
0.184
0.441
0.322
0.185
0.441
0.307
0.181
0.431
0.308
0.189
0.446
0.318
0.188
0.445
0.312
RefCOCO+

Test A

Bleu 2 Rouge Meteor
0.140
0.356
0.218
0.136
0.346
0.203
0.145
0.363
0.235
0.142
0.360
0.221
0.143
0.361
0.219
0.150
0.372
0.232
0.142
0.360
0.220
RefCOCOg

Bleu 1
0.553
0.547
0.583
0.578
0.561
0.593
0.579

Bleu 1
0.331
0.324
0.339
0.327
0.336
0.340
0.327

Test B

Bleu 2 Rouge Meteor
0.228
0.499
0.343
0.228
0.497
0.341
0.245
0.530
0.382
0.247
0.531
0.375
0.234
0.505
0.352
0.249
0.533
0.386
0.246
0.525
0.370

Test B

Bleu 2 Rouge Meteor
0.135
0.322
0.174
0.133
0.320
0.167
0.145
0.325
0.177
0.135
0.325
0.172
0.140
0.325
0.177
0.143
0.328
0.178
0.137
0.321
0.175

validation
Bleu 1 Bleu 2 Rouge Meteor
0.149
Baseline [26] 0.437 0.273 0.363
0.428 0.263 0.354
0.144
0.442 0.277 0.370 0.151
0.145

visdif+MMI 0.430 0.262 0.356

MMI [26]
visdif

Table 3. Referring Expression Generation Results: Bleu, Rouge, Meteor evaluations
for RefCOCO, RefCOCO+ and RefCOCOg.

We observe in Table 3 that models incoporating “+MMI” are worse than
without “+MMI” under the automatic scoring metrics. To verify whether these
metrics really reﬂect performance, we performed human evaluations on the ex-
pression generation task. Three Turkers were asked to click on the referred object

Modeling Context in Referring Expressions

13

RefCOCO

RefCOCO+

Test A Test B Test A Test B
62.42% 64.99% 49.18% 42.03%
65.76% 68.25% 49.84% 45.38%
68.27% 74.92% 55.20% 43.65%
70.25% 75.47% 53.56% 47.58%
64.51% 68.34% 52.06% 43.53%
71.40% 76.14% 57.17% 47.92%
visdif+MMI+tie 70.01% 76.31% 55.64% 48.04%
Table 4. Human Evaluations on referring expression generation.

Baseline [26]
MMI
visdif
visdif+MMI
Baseline+tie
visdif+tie

Baseline [26]
MMI
visdif
visdif+MMI
Baseline+tie
visdif+tie
visdif+MMI+tie

RefCOCO

RefCOCO+

Test A Test B Test A Test B
15.60% 16.40% 28.67% 46.27%
11.60% 11.73% 21.07% 26.40%
8.80% 19.60% 31.07%
9.20%
5.07%
6.13% 12.13% 16.00%
11.20% 14.93% 22.00% 32.13%
4.27% 5.33% 11.73% 16.27%
6.53% 4.53% 10.13% 13.33%

Table 5. Fraction of images for which the algorithm generates the same referring
expression for multiple objects. Smaller is better.

given the image and the generated expression. If more than two clicked on the
true target object, we consider this expression to be correct. Table 4 shows the
human evaluation results, indicating that models with “+MMI” are consistently
higher performance. We also ﬁnd “+tie” methods perform the best, indicating
that tying language together is able to produce less ambiguous referring expres-
sions. Some referring expression generation examples using diﬀerent methods
are shown in Fig 5. Besides, we show more examples of tied generations using
“visdif+MMI+tie” model in Fig 6.

Finally, we introduce another evaluation metric which measures the fraction
of images for which an algorithm produces the same generated referring expres-
sion for multiple objects within the image. Obviously, a good referring expression
generator should never produce the same expressions for two objects within the
same image. Thus we would like this number to be as small as possible. The
evaluation results under such metric are shown in Table 5. We ﬁnd “+MMI”
produces smaller number of duplicated expressions on both RefCOCO and Re-
fCOCO+, while “+tie” helps generating even more diﬀerent expressions. Our
combined model “visdif+MMI+tie” performs the best under this metric.

6 Conclusion
In this paper, we have developed a new model for incorporating detailed context
into referring expression models. With this visual comparison based context we
have improved performance over previous state of the art for referring expression
generation and comprehension. In addition, for the referring expression gener-
ation task, we explore methods for joint generation over all relevant objects.
Experiments verify that this joint generation improves results over previous at-
tempts to reduce ambiguity during generation.

14

Licheng et al.

Fig. 4. Referring expression comprehension on RefCOCO and RefCOCO+ using “vis-
dif” based on detections. The blue and red bounding boxes are correct and incorrect
comprehension respectively, while the green boxes indicate the ground-truth regions.

Modeling Context in Referring Expressions

15

Fig. 5. Referring expression generation on RefCOCO and RefCOCO+ by diﬀerent
methods.

16

Licheng et al.

Fig. 6. Joint referring expression generation using our full model of “visdif+MMI+tie”.

Modeling Context in Referring Expressions

17

Acknowledgements: We thank Junhua Mao, Dequan Wang and Varun K.
Nagaraja for helpful discussions. This research is supported by NSF Grants
#1444234, 1445409, 1405822, and Microsoft.

References

1. Bell, S., Zitnick, C.L., Bala, K., Girshick, R.: Inside-outside net: Detecting objects

in context with skip pooling and recurrent neural networks (2015)

2. Brown-Schmidt, S., Tanenhaus, M.K.: Watching the eyes when talking about size:
An investigation of message formulation and utterance planning. Journal of Mem-
ory and Language 54(4), 592–609 (2006)

3. Donahue, J., Anne Hendricks, L., Guadarrama, S., Rohrbach, M., Venugopalan,
S., Saenko, K., Darrell, T.: Long-term recurrent convolutional networks for visual
recognition and description. In: CVPR (2015)

4. Erhan, D., Szegedy, C., Toshev, A., Anguelov, D.: Scalable object detection using

deep neural networks. In: CVPR (2014)

5. Fang, H., Gupta, S., Iandola, F., Srivastava, R.K., Deng, L., Doll´ar, P., Gao, J.,
He, X., Mitchell, M., Platt, J.C., et al.: From captions to visual concepts and back.
In: CVPR (2015)

6. Farhadi, A., Hejrati, M., Sadeghi, M.A., Young, P., Rashtchian, C., Hockenmaier,
J., Forsyth, D.: Every picture tells a story: Generating sentences from images. In:
ECCV (2010)

7. FitzGerald, N., Artzi, Y., Zettlemoyer, L.S.: Learning distributions over logical

forms for referring expression generation. In: EMNLP. pp. 1914–1925 (2013)

8. Funakoshi, K., Watanabe, S., Kuriyama, N., Tokunaga, T.: Generating referring
expressions using perceptual groups. In: Natural Language Generation, pp. 51–60.
Springer (2004)

9. Girshick, R.: Fast r-cnn. In: Proceedings of the IEEE International Conference on

Computer Vision. pp. 1440–1448 (2015)

10. Greﬀ, K., Srivastava, R.K., Koutn´ık, J., Steunebrink, B.R., Schmidhuber, J.: Lstm:

A search space odyssey. arXiv preprint arXiv:1503.04069 (2015)

11. Grice, H.P.: Logic and conversation. In: Cole, P., Morgan, J.L. (eds.) Syntax and
Semantics: Vol. 3: Speech Acts, pp. 41–58. Academic Press, San Diego, CA (1975)
12. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition

(2015)

13. Hodosh, M., Young, P., Hockenmaier, J.: Framing image description as a rank-
ing task: Data, models and evaluation metrics. Journal of Artiﬁcial Intelligence
Research (2013)

14. Hu, R., Xu, H., Rohrbach, M., Feng, J., Saenko, K., Darrell, T.: Natural language

object retrieval. In: CVPR (2016)

15. Johnson, J., Karpathy, A., Fei-Fei, L.: Densecap: Fully convolutional localization

networks for dense captioning. arXiv preprint arXiv:1511.07571 (2015)

16. Jordan, P., Walker, M.: Learning attribute selections for non-pronominal expres-

17. Jun Zhu, X.C., Yuille, A.: Deepm: A deep part-based model for object detection

18. Karpathy, A., Fei-Fei, L.: Deep visual-semantic alignments for generating image

sions. In: ACL (2000)

and semantic part localization (2015)

descriptions. In: CVPR (2015)

18

Licheng et al.

19. Kazemzadeh, S., Ordonez, V., Matten, M., Berg, T.L.: Referitgame: Referring to

objects in photographs of natural scenes. In: EMNLP. pp. 787–798 (2014)

20. Kelleher, J.D., Kruijﬀ, G.J.M.: Incremental generation of spatial referring expres-

sions in situated dialog. In: ACL (2006)

21. Kiros, R., Salakhutdinov, R., Zemel, R.S.: Unifying visual-semantic embeddings

with multimodal neural language models. TACL (2015)

22. Krahmer, E., Van Deemter, K.: Computational generation of referring expressions:

A survey. Computational Linguistics 38(1), 173–218 (2012)

23. Kulkarni, G., Premraj, V., Ordonez, V., Dhar, S., Li, S., Choi, Y., Berg, A.C., Berg,
T.: Babytalk: Understanding and generating simple image descriptions. Pattern
Analysis and Machine Intelligence, IEEE Transactions on (2013)

24. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV (2014)
25. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S.: Ssd: Single shot multibox

detector (2015)

26. Mao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A., Murphy, K.: Generation

and comprehension of unambiguous object descriptions. In: CVPR (2016)

27. Mao, J., Xu, W., Yang, Y., Wang, J., Huang, Z., Yuille, A.: Deep captioning with

multimodal recurrent neural networks (m-rnn). ICLR (2015)

28. Mitchell, M., van Deemter, K., Reiter, E.: Natural reference to objects in a vi-
sual domain. In: Proceedings of the 6th international natural language generation
conference. pp. 95–104. Association for Computational Linguistics (2010)

29. Mitchell, M., Reiter, E., van Deemter, K.: Typicality and object reference. Cogni-

tive Science (CogSci) (2013)

30. Mitchell, M., Van Deemter, K., Reiter, E.: Generating expressions that refer to

visible objects. In: HLT-NAACL. pp. 1174–1184 (2013)

31. Ordonez, V., Kulkarni, G., Berg, T.L.: Im2text: Describing images using 1 million
captioned photographs. In: Advances in Neural Information Processing Systems
(2011)

32. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object de-

tection with region proposal networks. In: NIPS (2015)

33. Rohrbach, A., Rohrbach, M., Hu, R., Darrell, T., Schiele, B.: Grounding of textual
phrases in images by reconstruction. arXiv preprint arXiv:1511.03745 (2015)
34. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., et al.: Imagenet large scale visual recog-
nition challenge. International Journal of Computer Vision 115(3), 211–252 (2015)
35. Sadeghi, F., Zitnick, C.L., Farhadi, A.: Visalogy: Answering visual analogy ques-

tions. In: NIPS (2015)

36. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale

image recognition. arXiv preprint arXiv:1409.1556 (2014)

37. Socher, R., Karpathy, A., Le, Q.V., Manning, C.D., Ng, A.Y.: Grounded compo-
sitional semantics for ﬁnding and describing images with sentences. Transactions
of the Association for Computational Linguistics (2014)

38. Viethen, J., Dale, R.: The use of spatial relations in referring expression gener-
ation. In: Proceedings of the Fifth International Natural Language Generation
Conference. pp. 59–67. Association for Computational Linguistics (2008)

39. Vinyals, O., Toshev, A., Bengio, S., Erhan, D.: Show and tell: A neural image

caption generator. In: CVPR (2015)

40. Winograd, T.: Understanding natural language. Cognitive psychology 3(1), 1–191

(1972)

Modeling Context in Referring Expressions

19

41. Xu, K., Ba, J., Kiros, R., Courville, A., Salakhutdinov, R., Zemel, R., Bengio,
Y.: Show, attend and tell: Neural image caption generation with visual attention.
ICML (2015)

6
1
0
2
 
g
u
A
 
0
1
 
 
]

V
C
.
s
c
[
 
 
3
v
2
7
2
0
0
.
8
0
6
1
:
v
i
X
r
a

Modeling Context in Referring Expressions

Licheng Yu, Patrick Poirson, Shan Yang, Alexander C. Berg, Tamara L. Berg

Department of Computer Science,
University of North Carolina at Chapel Hill
{licheng,poirson,alexyang,aberg,tlberg}@cs.unc.edu

Abstract. Humans refer to objects in their environments all the time,
especially in dialogue with other people. We explore generating and com-
prehending natural language referring expressions for objects in images.
In particular, we focus on incorporating better measures of visual con-
text into referring expression models and ﬁnd that visual comparison to
other objects within an image helps improve performance signiﬁcantly.
We also develop methods to tie the language generation process together,
so that we generate expressions for all objects of a particular category
jointly. Evaluation on three recent datasets - RefCOCO, RefCOCO+,
and RefCOCOg1, shows the advantages of our methods for both refer-
ring expression generation and comprehension.

Keywords: language, language and vision, generation, referring expres-
sion generation

1 Introduction

In this paper, we look at the dual-tasks of generating and comprehending nat-
ural language expressions referring to particular objects within an image. Re-
ferring to objects is a natural and common experience. For example, one often
uses referring expressions in everyday speech to indicate a particular person or
object to a co-observer, e.g., “the man in the red hat” or “the book on the ta-
ble”. Computational models to generate and comprehend such expressions would
have applicability to human-computer interactions, especially for agents such as
robots, interacting with humans in the physical world.

Successful models will have to connect both recognition of visual attributes of
objects and eﬀective natural language generation to compose useful expressions
for dialogue. A broader version of this latter goal was considered in 1975 by
Paul Grice who introduced maxims describing cooperative conversation between
people [11]. These maxims, called the Gricean Maxims, describe a set of rational
principles for natural language dialogue interactions. The 4 maxims are: quality
(try to be truthful), quantity (make your contribution as informative as you can,
giving as much information as is needed but no more), relevance (be relevant
and pertinent to the discussion), and manner (be as clear, brief, and orderly as
possible, avoiding obscurity and ambiguity).

1 Datasets and toolbox can be downloaded from https://github.com/lichengunc/refer

2

Licheng et al.

Fig. 1. Example referring expressions for the giraﬀe outlined in green from three re-
ferring expression datasets (described in Sec 4).

For the purpose of referring to objects in complex real world scenes these
maxims suggest that a well formed expression should be informative, succinct,
and unambiguous. The last point is especially necessary for referring to objects
in the real world since we often ﬁnd multiple objects of a particular category
situated together in a scene. For example, consider the image in Fig. 1 which
contains three giraﬀes. We should not refer to the target (outlined in green) as
“the spotted giraﬀe” since all of the giraﬀes are spotted and this would create
an ambiguous reference. More reasonably we should refer to the target as “the
giraﬀe with lowered head” to diﬀerentiate this giraﬀe from the other two.

The task of referring expression generation (REG) has been studied since the
1970s [40,22,30,7], with most work focused on studying particular aspects of the
problem in some relatively constrained datasets. Recent approaches have pushed
this work toword more realistic scenarios. Kazemzadeh et al [19] introduced the
ﬁrst large-scale dataset of referring expressions for objects in real-world natural
images, collected in a two-player game. This dataset was originally collected on
top of the 20,000 image ImageCleft dataset, but has recently been extended
to images from the MSCOCO collection. We make use of the RefCOCO and
RefCOCO+ datasets in our work along with another recently collected referring
expression dataset, released by Google, denoted in our paper as RefCOCOg [26].
The most relevant work to ours is Mao et al [26] which introduced the ﬁrst
deep learning approach to REG. In this model, the authors use a Convolutional
Neural Network (CNN) [36] model pre-trained on ImageNet [34] to extract visual
features from a bounding box around the target object and from the entire image.
They use these features plus 5 features encoding the target object location and
size as input to a Long Short-term Memory (LSTM) [10] network that generates
expressions. Additionally, they apply the same model to the inverse problem
of referring expression comprehension where the input is a natural language
expression and the goal is to localize the referred object in the image.

Similar to these recent methods, we also take a deep learning approach to
referring expression generation and comprehension. However, while they use a
generic model for object context – CNN features for the entire image containing

Modeling Context in Referring Expressions

3

the target object – we take a more focused approach to encode object com-
parisons. These object comparisons are critical for producing an unambiguous
referring expression since one must consider visual characteristics of similar ob-
jects during generation in order to select the most distinct aspects for description.
This mimics the process that a human would use to compose a good referring ex-
pression for an object, e.g. look at the object, look at other relevant objects, and
generate an expression that could be used by a co-observer to unambiguously
pick out the target object.

In addition, for the referring expression generation task, we introduce a
method to tie the language generation process together for all depicted objects
of the same type. This helps generate a good set of expressions such that the
expressions diﬀerentiate between objects but are also complementary. For exam-
ple, we never want to generate the exact same expression for two objects in an
image. Alternatively, if we call one object “the red ball” then we may desire the
expression for the other object to follow the same generation pattern, i.e., “the
blue ball”. Our experimental evaluations show that these visual and linguistic
comparisons improve performance over previous state of the art.

In the rest of our paper, we ﬁrst describe related work (Sec 2). We then
describe our improvements to models for referring expression generation and
comprehension (Sec 3), describe 3 referring expression datasets (Sec 4), and
perform experimental evaluations on several model variations (Sec 5). Finally
we present our conclusions (Sec 6).

2 Related Work

Referring expressions are closely related to the more general problem of modeling
the connection between images and descriptive language. In recent years, this has
been studied in the image captioning task [6,37,13,31,23]. There, the aim is to
condition the generation of language on the visual information from an image.
The wide range of aspects of an image that could be described, and the variety
of words that could be chosen for a particular description complicate studying
image captioning. Our study of referring expressions is partially motivated by
focusing on description for a speciﬁc, and more easily evaluated, communication
goal. Although our task is somewhat diﬀerent, we borrow machinery from state of
the art caption generation [3,39,27,5,18,21,41] using LSTM to generate captions
based on CNN features computed on an input image. Three recent approaches for
referring expression generation [26] and comprehension [14,33] also take a deep
learning approach. However, we add visual object comparisons and tie together
language generation for multiple objects.

Referring expression generation has been studied for many years [40,22,30]

in linguistics and natural language processing. These works were limited by data
collection and insuﬃcient computer vision algorithms. Together Amazon Me-
chanical Turk and CNNs have somewhat mitigated these limitations, allowing
us to revisit these ideas on large-scale datasets. We still use such work to motivate
the architecture of our pipeline. For instance, Mitchell and Jordan et al [30,16]

4

Licheng et al.

Fig. 2. Framework: We extract VGG-fc7 and location features for each object of the
same type, then compute visual diﬀerences. These features and diﬀerences are then fed
into LSTM. For sentence generation, the LSTMs are tied together, incorporating the
hidden output diﬀerence as additional information for predicting words.

show the importance of using attributes, Funakoshi et al [8] show the importance
of relative relations between objects in the same perceptual group, and Kelleher
et al [20] show the importance of spatial relationships. These provide motivation
for our modeling choices: when considering a referring expression for an object,
the model takes into account the relative spatial location of other objects of the
same type and visual comparisons to objects in the same perceptual group.

The REG datasets of the past were sometimes limited to using computer
generated images [38], or relatively small collections of natural objects [29,28,7].
Recently, a large-scale referring expression dataset was collected by Kazemzadeh
et al [19] featuring natural objects in the real world. Since then, another three
REG datasets based on the object labels in MSCOCO have been collected [19,26].
The availability of large-scale referring expression datasets allows us to train
deep learning models. Additionally, our analysis of these datasets motivates our
incorporation of visual comparisons between same-type objects, and the need to
tie together choices for referring expression generation between objects.

3 Models

We implement several model variations for referring expression generation and
comprehension. The ﬁrst set of models are recent state of the art deep learn-
ing approaches from Mao et al [26]. We use these as our baselines (Sec 3.1).
Next, we investigate incorporating better visual context features into the mod-
els (Sec 3.2). Finally, we explore methods to jointly produce an entire set of
referring expressions for all depicted objects of the same category (Sec 3.3).

3.1 Baselines

For comparison, we implement both the baseline and strong model of Mao et
al [26]. Both models utilize a pre-trained CNN network to model the target

Modeling Context in Referring Expressions

5

object and its context within the image, and then use a LSTM for generation.
In particular, object and context are modeled as features from a CNN trained to
recognize 1,000 object categories [36] from ImageNet [34]. Speciﬁcally, the visual
representation is composed of:

– Target object representation, oi. The object is modeled as features extracted
from the VGG-fc7 layer by forwarding its bounding box through the network.
– Global context representation, gi. Context is modeled as features extracted

from the VGG-fc7 layer for the entire image.

– Location/size representation, li, for the target object. Location and size are
modeled as a 5-d vector encoding the x and y locations of the top left and
bottom right corners of the target object bounding box, as well as the bound-
ing box size with respect to the image, i.e., li = [ xtl

W , ytl

H , xbr

W , ybr

H , w·h

W ·H ].

Language generation is handled by a long short-term memory network (LSTM)
[10] where inputs are the above visual features and the network is trained to
generate natural language referring expressions. In Mao et al’s baseline [26],
the model uses maximum likelihood training and outputs the most likely refer-
ring expression given the target object, context, and location/size features. In
addition, they also propose a stronger model that uses maximum mutual infor-
mation (MMI) training to consider whether a listener would interpret a referring
expression unambiguously. They impose this by penalizing the model if a gen-
erated referring expression could also be generated by some other object within
the image. We implement both their original model and MMI model in our ex-
periments. We subsequently refer to these two models as Baseline and MMI,
respectively.

3.2 Visual Comparison

Previous works [2,30] have shown that objects in an image, of the same type as
the target object, are most important for inﬂuencing what attributes people use
to describe the target. One drawback of considering a general feature over the
entire image to encode context (as in the baseline models) is that it may not
speciﬁcally focus on visual comparisons to the most relevant objects – the other
objects of the same object category within the image.

In this paper, we propose a more explicit encoding of the visual diﬀerence
between objects of the same category within an image. This helps for gener-
ating referring expressions which best discriminate the target object from the
surrounding objects. For example, in an image with three cars, two blue and
one red, visual appearance comparisons could help generate “the red car” as an
expression for the latter object.

Given the referred object and its surrounding objects, we compute two types
of features for visual comparison. The ﬁrst type encodes the similarities and
diﬀerences in visual appearance between the target object and other objects of
the same cateogry depicted in the image. Inspired by Sadeghi et al [35], we
compute the diﬀerence in visual CNN features as our representation of relative

6

Licheng et al.

appearance. Because there may be many surrounding objects of the same type
in the image, and not every object will provide useful information about how
to describe the target object, we need to ﬁrst select which objects to compare
and aggregate their visual diﬀerences. In Section 5, we experiment with selecting
diﬀerent subsets of comparison objects: objects of the same category, objects of
diﬀerent category, or all other depicted objects. For each selected comparison
object, we compute the appearance diﬀerence as the subtraction of the target
object and comparison object CNN representations. We experiment with three
diﬀerent strategies for computing an aggregate vector to represent the visual
diﬀerence between the target object and the surrounding objects: minimum,
maximum, and average over each feature dimension. In our experiments, pooling
the average diﬀerence between the target object and surrounding objects seems
to work best. Therefore, we use this pooling in all experiments.

– Visual appearance diﬀerence representation, δvi = 1
n

oi−oj
(cid:107)oi−oj (cid:107) , where n
is the number of objects chosen for comparisons and we use average pooling
to aggregate the diﬀerences.

j(cid:54)=i

(cid:80)

The second type of comparison feature encodes the relative location and size
diﬀerences between the target object and surrounding objects of the same ob-
ject category. People often use comparative size or location terms in referring
expressions, e.g. “the second giraﬀe from the left” or “the smaller monkey” [38].
To address the dynamic number of nearby objects, we choose up to ﬁve com-
parison objects of the same category as the target object, sorted by distance to
the target. When fewer than ﬁve objects of the same category are depicted, this
25-d vector (5-d x 5 surrounding objects) is padded with zeros.

– Location diﬀerence representation, δli, where each 5-d diﬀerence is computed
, [(cid:52)ybr]ij
hi

as δlij = [ [(cid:52)xtl]ij

, [(cid:52)xbr]ij
wi

, [(cid:52)ytl]ij
hi

, wj hj
wihi

wi

].

In summary, our ﬁnal visual representation for a target object is:

ri = Wm[oi, gi, li, δvi, δli] + bm

(1)

where oi, gi, li are the target object, global context, and location/size features
from the baseline model, δvi and δli encodes visual appearance diﬀerence and
location diﬀerence. Wm and bm project the concatenation of the ﬁve types of
features to be the ﬁnal representation.

3.3 Joint Language Generation

For the referring expression generation task, rather than generating sentences
for each object in an image separately [15][26], we consider tying the generation
process together into a single task to jointly generate expressions for all objects
of the same object category depicted in an image. This makes sense intuitively
– when a person attempts to generate a referring expression for an object in an
image they inherently compose that expression while keeping in mind expressions

Modeling Context in Referring Expressions

7

for the other objects in the picture. This can be observed in the fact that the
expressions people generate for objects in an image tend to share similar patterns
of expression. If you say “the man on the left” for one object then you tend to
say “the man on the right” for the other object. We would like our algorithms to
mimic these behaviors. Additionally, the algorithm should also be able to push
generated expressions away from each other to create less ambiguous references.
For example, if we use the word “red” to describe one object, then we probably
shouldn’t use the same word to describe another object.

To model this joint generation process, we model generation using an LSTM
model where in addition to the usual connections between time steps within
an expression we also add connections between expressions for diﬀerent objects.
This architecture is illustrated in Fig 2.

Speciﬁcally, we use LSTM to generate multiple referring expressions, {ri},

given depicted objects of the same type, {oj}.

P (R|O) =

P (ri|oi, {oj(cid:54)=i}, {rj(cid:54)=i}),

P (wit|wit−1 , ..., wi1, vi, {hjt,j(cid:54)=i})

(2)

(cid:89)

i
(cid:89)

=

(cid:89)

i

t

where wit are words at time t, vi visual representations, and hjt is the hidden
output of j-th object at time step t that encodes the visual and sentence infor-
mation for the j-th object. As visual comparison, we aggregate the diﬀerence of
hit −hjt
(cid:107)hit −hjt (cid:107) .
hidden outputs to push away ambiguous information. hdifit
There, n is the the number of other objects of the same type. The hidden diﬀer-
ence is jointly embedded with the target object’s hidden output, and forwarded
to the softmax layer for predicting the word.

= 1
n

(cid:80)

j(cid:54)=i

P (wit|wit−1, ..., wi1, vi, {hjt,j(cid:54)=i}) = softmax(Wh[hit, hdifit

] + bh)

(3)

4 Data

We make use of 3 referring expression datasets in our work, all collected on top
of the Microsoft COCO image collection [24]. One dataset, RefCOCOg [26] is
collected in a non-interactive setting, while the other two datasets, RefCOCO
and RefCOCO+, are collected interactively in a two-player game [19]. In the
following, we describe each dataset and provide some analysis of their similarities
and diﬀerences, and then discuss splits of the datasets used in our experiments .

4.1 Datasets & Analysis

Images for each dataset were selected to contain multiple objects of the same cat-
egory (object categories depicted cover the 80 common objects from MSCOCO
with ground-truth segmentation). These images provide useful cases for referring
expression generation since the referrer needs to compose a referring expression
that uniquely singles out one object from other relevant objects.

8

Licheng et al.

RefCOCOg: This dataset was collected on Amazon Mechanical Turk in a
non-interactive setting. One set of workers were asked to write natural language
referring expressions for objects in MSCOCO images then another set of workers
were asked to click on the indicated object given a referring expression. If the click
overlapped with the correct object then the referring expression was considered
valid and added to the dataset. If not, another referring expression was collected
for the object. This dataset consists of 85,474 referring expressions for 54,822
objects in 26,711 images. Images were selected to contain between 2 and 4 objects
of the same object category.

RefCOCO & RefCOCO+: These datasets were collected using the Refer-
itGame [19]. In this two-player game, the ﬁrst player is shown an image with a
segmented target object and asked to write a natural language expression refer-
ring to the target object. The second player is shown only the image and the
referring expression and asked to click on the corresponding object. If the players
do their job correctly, they receive points and swap roles. If not, they are pre-
sented with a new object and image for description. Images in these collections
were selected to contain two or more objects of the same object category. In the
RefCOCO dataset, no restrictions are placed on the type of language used in
the referring expressions while in the RefCOCO+ dataset players are disallowed
from using location words in their referring expressions by adding “taboo” words
to the ReferItGame. This dataset was collected to obtain a referring expression
dataset focsed on purely appearance based description, e.g., “the man in the
yellow polka-dotted shirt” rather than “the second man from the left”, which
tend to be more interesting from a computer vision based perspective and are in-
dependent of viewer perspective. RefCOCO consists of 142,209 refer expressions
for 50,000 objects in 19,994 images, and RefCOCO+ has 141,564 expressions for
49,856 objects in 19,992 images.

Dataset Comparisons: As shown in Fig. 1, the languages used in RefCOCO
and RefCOCO+ datasets tend to be more concise and less ﬂowery than the
languages used in the RefCOCOg. RefCOCO expressions have an average length
of 3.61 while RefCOCO+ have an average length of 3.53, and RefCOCOg contain
an average of 8.43 words. This is most likely due to the diﬀerences in collection
strategy. RefCOCO and RefCOCO+ were collected in a game scenario where
players are trying to eﬃciently provide enough information to indicate the correct
object to the other player. RefCOCOg was collected in independent rounds of
Mechanical Turk without any interactive time constraints and therefore tend to
provide more complex expressions, often entire sentences rather than phrases.

In addition, RefCOCO and RefCOCO+ do not limit the number of objects
of the same type to 4 and thus contain some images with many objects of the
same type. Both RefCOCO and RefCOCO+ contain an average of 3.9 same-
type objects per image, while RefCOCOg contains an average of 1.63 same-
type objects per image. The large number of same-type objects per image in
RefCOCO and RefCOCO+ suggests that incorporating visual comparisons to
same-type objecs will be useful.

Modeling Context in Referring Expressions

9

Dataset Splits: There are two types of splits of the data into train/test

sets: a per-object split and a people-vs-objects split.

The ﬁrst type is per-object split. In this split, the dataset is divided by
randomly partitioning objects into training and testing sets. This means that
each object will only appear either in training or testing set, but that one object
from an image may appear in the training set while another object from the
same image may appear in the test set. We use this split for RefCOCOg since
same division was used in the previous state-of-the-art approach [26].

The second type is people-vs-objects splits. One thing we observe from
analyzing the datasets is that about half of the referred objects are people.
Therefore, we create a split for RefCOCO and RefCOCO+ datasets that eval-
uates images containing multiple people (testA) vs images containing multiple
instances of all other objects (testB). In this split all objects from an image will
appear either in the training or testing sets, but not both. This split creates a
more meaningfully separated division between training and testing, allowing us
to evaluate the usefulness of context more fairly.

5 Experiments

We ﬁrst perform some experiments to analyze the use of context in referring
expressions (Sec 5.1). Given these ﬁndings, we then perform experiments eval-
uating the usefulness of our proposed visual and language innovations on the
comprehension (Sec 5.2) and generation tasks (Sec 5.3).

In experiments for the referring expression comprehension task, we use the
same evaluation as Mao et al [26], namely we ﬁrst predict the region referred
by the given expression, then we compute the intersection over union (IOU)
ratio between the true and predicted bounding box. If the IOU is larger than
0.5 we count it as a true positive. Otherwise, we count it as a false positive. We
average this score over all images. For the referring expression generation task
we use automatic evaluation metrics, BLEU, ROUGE, and METEOR developed
for evaluating machine translation results, commonly used to evaluate language
generation results [41,18,5,27,39,23]. We further perform human evaluations, and
propose a new metric evaluating the duplicate rate of generated expressions. For
both tasks, we compare our models with “Baseline” and “MMI” [26]. Speciﬁcally,

RefCOCO

RefCOCO+

no context

Test A Test B Test A Test B
63.91% 66.31% 50.09% 45.05%
global context 63.15% 64.21% 48.73% 42.13%
65.57% 67.13% 50.38% 44.89%
66.14% 68.07% 50.25% 45.40%
66.68% 68.56% 50.34% 45.48%

scale 2
scale 3
scale 4

Table 1. Expression Comprehension accuracies on RefCOCO and RefCOCO+ of the
Baseline model with diﬀerenct context source. Scale n indicates the size of the cropped
window centered by the target object.

10

Licheng et al.

we denote “visdif” as our visual comparison model, and “tie” as the LSTM tying
model. We also perform an ablation study, evaluating the combinations.

5.1 Analysis Experiments

Context Representation As previously discussed, we suggest that the ap-
proaches proposed in recent referring expression works [26,14] make use of rel-
atively weak contextual information, by only considering a single global image
context for all objects. To verify this intuition, we implemented both the baseline
and strong MMI models from Mao et al [26], and compare the results for referring
expression comprehension task with and without global context on RefCOCO
and Refcoco+ in Table 1. Surprisingly we ﬁnd that the global context does not
improve the performance of the model. In fact, adding context even decreases
performance slightly. This may be due to the fact that the global context for
each object in an image would be the same, introducing some ambiguity into the
referring expression comprehension task. Given these ﬁndings, we implemented
a simple modiﬁcation to the global context, computing the same visual repre-
sentation, but on a somewhat scaled window centered around the target object.
We found this to improve performance, suggesting room for improving the visual
context feature. This motivate our development of a better context feature.

Visual Comparison For our visual comparison model, there could be several
choices regarding which objects from the image should be compared to the target
object. We experiment with three sets of reference objects on RefCOCO and
RefCOCO+ datasets: a) objects of the same-category in the image, b) objects
of diﬀerent-category in the image, and c) all objects appeared in the image. We
use our “visdif” model for this experiment. The results are shown in Figure 3.
It is clear to see the visual comparisons to the same-category objects are most
useful for referring expression comprehension task. This is more like mimicing
how human refer object – we tend to point out the diﬀerence between the target
object with the other same-category objects within the same image.

Fig. 3. Comprehension accuracies on RefCOCO and RefCOCO+ datasets. We compare
the performance of “visdif” model without visual comparison, and visual comparison
between diﬀerent-category objects, between all objects, and between same-type objects.

Modeling Context in Referring Expressions

11

5.2 Referring Expression Comprehension

We evaluate performance on the referring expression comprehension task on Re-
fCOCO, RefCOCO+ and RefCOCOg datasets. For RefCOCO and RefCOCO+,
we evaluate on the two subsets of people (testA) and all other objects (testB).
For RefCOCOg, we evaluate on the per-object split as previous work [26]. Since
the authors haven’t released their testing set, we show the performance on their
validation set only, using the optimized hyper-parameters on RefCOCO. Ta-
ble 2 shows the comprehension accuracies. We observe that our implementation
of Mao et al [26] achieves comparable performance to the numbers reported in
their paper. We also ﬁnd that adding visual comparison features to the Baseline
model improves performance across all datasets and splits. Similar improvements
are also observed on top of the MMI model.

In order to make a fully automatic referring system, we also train a Fast-
RCNN [9] detector and build our system on top of the detections. We train
Fast-RCNN on the validation portion only as the RefCOCO and RefCOCO+
are collected using MSCOCO training data. For RefCOCOg, we use the detec-
tion results provided by [26], which were trained uisng Multibox [4]. Results on
shown in the bottom half of Table 2. Although all comprehension accuracies
drop due to imperfect detections, the improvements of our models over Baseline
and MMI are still observed. One weakness of our automatic system is that it
highly depends on detection performance, especially for general objects (testB).
However, considering our detector was trained on MSCOCO validation only, we
believe such weakness may be alleviated with more training data and stronger
detection techniques, e.g., [12][25][32][17][1], etc.

We show some automatic comprehension examples of RefCOCO, RefCOCO+
and RefCOCOg in Fig 4, where top three rows show correct comprehensions (ob-
ject correctly localized) and bottom three rows show incorrect comprehensions
(wrong object localized).

Baseline[26]
visdif
MMI[26]
visdif+MMI

RefCOCO

RefCOCO+

RefCOCOg
Test A Test B Test A Test B Validation
55.16%
63.15% 64.21% 48.73% 42.13%
59.25%
67.57% 71.19% 52.44% 47.51%
62.14%
71.72% 71.09% 58.42% 51.23%
73.98% 76.59% 59.17% 55.62% 64.02%

40.75%
Baseline(det)[26] 58.32% 48.48% 46.86% 34.04%
41.85%
62.50% 50.80% 50.10% 37.48%
visdif(det)
MMI(det)[26]
45.85%
64.90% 54.51% 54.03% 42.81%
visdif+MMI(det) 67.64% 55.16% 55.81% 43.43% 46.86%

Table 2. Referring Expression comprehension results on the RefCOCO, RefCOCO+,
and RefCOCOg datasets. Rows of “method(det)” are the results of automatic system
built on Fast-RCNN [9] and Multibox [4] detections.

12

Licheng et al.

5.3 Referring Expression Generation

For the referring expression generation task, we evaluate the usefulness of our
visual comparison features as well as our joint language generation model. These
serve to tie the generation process together so that the model considers other
objects of the same type both visually and linguistically during generation. On
the visual side, comparisons are used to judge similarity of the target object
to other objects of the same type in terms of appearance, size and location.
On the language side, the joint LSTM model serves to both diﬀerentiate and
mimic language patterns in the referring expressions for the entire set of depicted
objects. Fig 5 shows some comparison between our model with other methods.
Our full results are shown in Table 3. We ﬁnd that incorporating our visual
comparison features into the Baseline model improves generation quality (com-
pare row “Baseline” to row “visdif”). It also improves the performance of MMI
model (compare row “MMI” to row “visdif+MMI”). We also observe that tying
the language generation together across all objects consistently improves the per-
formance (compare the bottom three “+tie” rows with the above). Especially for
method “visdif+tie”, it achieves the highest score under almost every measure-
ment. We do not perform language tying on RefCOCOg since here some objects
from an image may appear in training while others may appear in testing.

Baseline [26]
MMI [26]
visdif
visdif+MMI
Baseline+tie
visdif+tie
visdif+MMI+tie

Baseline [26]
MMI [26]
visdif
visdif+MMI
Baseline+tie
visdif+tie
visdif+MMI+tie

Bleu 1
0.477
0.478
0.505
0.494
0.490
0.510
0.506

Bleu 1
0.391
0.370
0.407
0.386
0.392
0.409
0.393

RefCOCO

Test A

Bleu 2 Rouge Meteor
0.173
0.413
0.290
0.175
0.418
0.295
0.184
0.441
0.322
0.185
0.441
0.307
0.181
0.431
0.308
0.189
0.446
0.318
0.188
0.445
0.312
RefCOCO+

Test A

Bleu 2 Rouge Meteor
0.140
0.356
0.218
0.136
0.346
0.203
0.145
0.363
0.235
0.142
0.360
0.221
0.143
0.361
0.219
0.150
0.372
0.232
0.142
0.360
0.220
RefCOCOg

Bleu 1
0.553
0.547
0.583
0.578
0.561
0.593
0.579

Bleu 1
0.331
0.324
0.339
0.327
0.336
0.340
0.327

Test B

Bleu 2 Rouge Meteor
0.228
0.499
0.343
0.228
0.497
0.341
0.245
0.530
0.382
0.247
0.531
0.375
0.234
0.505
0.352
0.249
0.533
0.386
0.246
0.525
0.370

Test B

Bleu 2 Rouge Meteor
0.135
0.322
0.174
0.133
0.320
0.167
0.145
0.325
0.177
0.135
0.325
0.172
0.140
0.325
0.177
0.143
0.328
0.178
0.137
0.321
0.175

validation
Bleu 1 Bleu 2 Rouge Meteor
0.149
Baseline [26] 0.437 0.273 0.363
0.428 0.263 0.354
0.144
0.442 0.277 0.370 0.151
0.145

visdif+MMI 0.430 0.262 0.356

MMI [26]
visdif

Table 3. Referring Expression Generation Results: Bleu, Rouge, Meteor evaluations
for RefCOCO, RefCOCO+ and RefCOCOg.

We observe in Table 3 that models incoporating “+MMI” are worse than
without “+MMI” under the automatic scoring metrics. To verify whether these
metrics really reﬂect performance, we performed human evaluations on the ex-
pression generation task. Three Turkers were asked to click on the referred object

Modeling Context in Referring Expressions

13

RefCOCO

RefCOCO+

Test A Test B Test A Test B
62.42% 64.99% 49.18% 42.03%
65.76% 68.25% 49.84% 45.38%
68.27% 74.92% 55.20% 43.65%
70.25% 75.47% 53.56% 47.58%
64.51% 68.34% 52.06% 43.53%
71.40% 76.14% 57.17% 47.92%
visdif+MMI+tie 70.01% 76.31% 55.64% 48.04%
Table 4. Human Evaluations on referring expression generation.

Baseline [26]
MMI
visdif
visdif+MMI
Baseline+tie
visdif+tie

Baseline [26]
MMI
visdif
visdif+MMI
Baseline+tie
visdif+tie
visdif+MMI+tie

RefCOCO

RefCOCO+

Test A Test B Test A Test B
15.60% 16.40% 28.67% 46.27%
11.60% 11.73% 21.07% 26.40%
8.80% 19.60% 31.07%
9.20%
5.07%
6.13% 12.13% 16.00%
11.20% 14.93% 22.00% 32.13%
4.27% 5.33% 11.73% 16.27%
6.53% 4.53% 10.13% 13.33%

Table 5. Fraction of images for which the algorithm generates the same referring
expression for multiple objects. Smaller is better.

given the image and the generated expression. If more than two clicked on the
true target object, we consider this expression to be correct. Table 4 shows the
human evaluation results, indicating that models with “+MMI” are consistently
higher performance. We also ﬁnd “+tie” methods perform the best, indicating
that tying language together is able to produce less ambiguous referring expres-
sions. Some referring expression generation examples using diﬀerent methods
are shown in Fig 5. Besides, we show more examples of tied generations using
“visdif+MMI+tie” model in Fig 6.

Finally, we introduce another evaluation metric which measures the fraction
of images for which an algorithm produces the same generated referring expres-
sion for multiple objects within the image. Obviously, a good referring expression
generator should never produce the same expressions for two objects within the
same image. Thus we would like this number to be as small as possible. The
evaluation results under such metric are shown in Table 5. We ﬁnd “+MMI”
produces smaller number of duplicated expressions on both RefCOCO and Re-
fCOCO+, while “+tie” helps generating even more diﬀerent expressions. Our
combined model “visdif+MMI+tie” performs the best under this metric.

6 Conclusion
In this paper, we have developed a new model for incorporating detailed context
into referring expression models. With this visual comparison based context we
have improved performance over previous state of the art for referring expression
generation and comprehension. In addition, for the referring expression gener-
ation task, we explore methods for joint generation over all relevant objects.
Experiments verify that this joint generation improves results over previous at-
tempts to reduce ambiguity during generation.

14

Licheng et al.

Fig. 4. Referring expression comprehension on RefCOCO and RefCOCO+ using “vis-
dif” based on detections. The blue and red bounding boxes are correct and incorrect
comprehension respectively, while the green boxes indicate the ground-truth regions.

Modeling Context in Referring Expressions

15

Fig. 5. Referring expression generation on RefCOCO and RefCOCO+ by diﬀerent
methods.

16

Licheng et al.

Fig. 6. Joint referring expression generation using our full model of “visdif+MMI+tie”.

Modeling Context in Referring Expressions

17

Acknowledgements: We thank Junhua Mao, Dequan Wang and Varun K.
Nagaraja for helpful discussions. This research is supported by NSF Grants
#1444234, 1445409, 1405822, and Microsoft.

References

1. Bell, S., Zitnick, C.L., Bala, K., Girshick, R.: Inside-outside net: Detecting objects

in context with skip pooling and recurrent neural networks (2015)

2. Brown-Schmidt, S., Tanenhaus, M.K.: Watching the eyes when talking about size:
An investigation of message formulation and utterance planning. Journal of Mem-
ory and Language 54(4), 592–609 (2006)

3. Donahue, J., Anne Hendricks, L., Guadarrama, S., Rohrbach, M., Venugopalan,
S., Saenko, K., Darrell, T.: Long-term recurrent convolutional networks for visual
recognition and description. In: CVPR (2015)

4. Erhan, D., Szegedy, C., Toshev, A., Anguelov, D.: Scalable object detection using

deep neural networks. In: CVPR (2014)

5. Fang, H., Gupta, S., Iandola, F., Srivastava, R.K., Deng, L., Doll´ar, P., Gao, J.,
He, X., Mitchell, M., Platt, J.C., et al.: From captions to visual concepts and back.
In: CVPR (2015)

6. Farhadi, A., Hejrati, M., Sadeghi, M.A., Young, P., Rashtchian, C., Hockenmaier,
J., Forsyth, D.: Every picture tells a story: Generating sentences from images. In:
ECCV (2010)

7. FitzGerald, N., Artzi, Y., Zettlemoyer, L.S.: Learning distributions over logical

forms for referring expression generation. In: EMNLP. pp. 1914–1925 (2013)

8. Funakoshi, K., Watanabe, S., Kuriyama, N., Tokunaga, T.: Generating referring
expressions using perceptual groups. In: Natural Language Generation, pp. 51–60.
Springer (2004)

9. Girshick, R.: Fast r-cnn. In: Proceedings of the IEEE International Conference on

Computer Vision. pp. 1440–1448 (2015)

10. Greﬀ, K., Srivastava, R.K., Koutn´ık, J., Steunebrink, B.R., Schmidhuber, J.: Lstm:

A search space odyssey. arXiv preprint arXiv:1503.04069 (2015)

11. Grice, H.P.: Logic and conversation. In: Cole, P., Morgan, J.L. (eds.) Syntax and
Semantics: Vol. 3: Speech Acts, pp. 41–58. Academic Press, San Diego, CA (1975)
12. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition

(2015)

13. Hodosh, M., Young, P., Hockenmaier, J.: Framing image description as a rank-
ing task: Data, models and evaluation metrics. Journal of Artiﬁcial Intelligence
Research (2013)

14. Hu, R., Xu, H., Rohrbach, M., Feng, J., Saenko, K., Darrell, T.: Natural language

object retrieval. In: CVPR (2016)

15. Johnson, J., Karpathy, A., Fei-Fei, L.: Densecap: Fully convolutional localization

networks for dense captioning. arXiv preprint arXiv:1511.07571 (2015)

16. Jordan, P., Walker, M.: Learning attribute selections for non-pronominal expres-

17. Jun Zhu, X.C., Yuille, A.: Deepm: A deep part-based model for object detection

18. Karpathy, A., Fei-Fei, L.: Deep visual-semantic alignments for generating image

sions. In: ACL (2000)

and semantic part localization (2015)

descriptions. In: CVPR (2015)

18

Licheng et al.

19. Kazemzadeh, S., Ordonez, V., Matten, M., Berg, T.L.: Referitgame: Referring to

objects in photographs of natural scenes. In: EMNLP. pp. 787–798 (2014)

20. Kelleher, J.D., Kruijﬀ, G.J.M.: Incremental generation of spatial referring expres-

sions in situated dialog. In: ACL (2006)

21. Kiros, R., Salakhutdinov, R., Zemel, R.S.: Unifying visual-semantic embeddings

with multimodal neural language models. TACL (2015)

22. Krahmer, E., Van Deemter, K.: Computational generation of referring expressions:

A survey. Computational Linguistics 38(1), 173–218 (2012)

23. Kulkarni, G., Premraj, V., Ordonez, V., Dhar, S., Li, S., Choi, Y., Berg, A.C., Berg,
T.: Babytalk: Understanding and generating simple image descriptions. Pattern
Analysis and Machine Intelligence, IEEE Transactions on (2013)

24. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV (2014)
25. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S.: Ssd: Single shot multibox

detector (2015)

26. Mao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A., Murphy, K.: Generation

and comprehension of unambiguous object descriptions. In: CVPR (2016)

27. Mao, J., Xu, W., Yang, Y., Wang, J., Huang, Z., Yuille, A.: Deep captioning with

multimodal recurrent neural networks (m-rnn). ICLR (2015)

28. Mitchell, M., van Deemter, K., Reiter, E.: Natural reference to objects in a vi-
sual domain. In: Proceedings of the 6th international natural language generation
conference. pp. 95–104. Association for Computational Linguistics (2010)

29. Mitchell, M., Reiter, E., van Deemter, K.: Typicality and object reference. Cogni-

tive Science (CogSci) (2013)

30. Mitchell, M., Van Deemter, K., Reiter, E.: Generating expressions that refer to

visible objects. In: HLT-NAACL. pp. 1174–1184 (2013)

31. Ordonez, V., Kulkarni, G., Berg, T.L.: Im2text: Describing images using 1 million
captioned photographs. In: Advances in Neural Information Processing Systems
(2011)

32. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object de-

tection with region proposal networks. In: NIPS (2015)

33. Rohrbach, A., Rohrbach, M., Hu, R., Darrell, T., Schiele, B.: Grounding of textual
phrases in images by reconstruction. arXiv preprint arXiv:1511.03745 (2015)
34. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., et al.: Imagenet large scale visual recog-
nition challenge. International Journal of Computer Vision 115(3), 211–252 (2015)
35. Sadeghi, F., Zitnick, C.L., Farhadi, A.: Visalogy: Answering visual analogy ques-

tions. In: NIPS (2015)

36. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale

image recognition. arXiv preprint arXiv:1409.1556 (2014)

37. Socher, R., Karpathy, A., Le, Q.V., Manning, C.D., Ng, A.Y.: Grounded compo-
sitional semantics for ﬁnding and describing images with sentences. Transactions
of the Association for Computational Linguistics (2014)

38. Viethen, J., Dale, R.: The use of spatial relations in referring expression gener-
ation. In: Proceedings of the Fifth International Natural Language Generation
Conference. pp. 59–67. Association for Computational Linguistics (2008)

39. Vinyals, O., Toshev, A., Bengio, S., Erhan, D.: Show and tell: A neural image

caption generator. In: CVPR (2015)

40. Winograd, T.: Understanding natural language. Cognitive psychology 3(1), 1–191

(1972)

Modeling Context in Referring Expressions

19

41. Xu, K., Ba, J., Kiros, R., Courville, A., Salakhutdinov, R., Zemel, R., Bengio,
Y.: Show, attend and tell: Neural image caption generation with visual attention.
ICML (2015)

