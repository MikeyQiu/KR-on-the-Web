9
1
0
2
 
g
u
A
 
4
 
 
]

V
C
.
s
c
[
 
 
2
v
9
4
6
2
1
.
1
1
8
1
:
v
i
X
r
a
1

ZHAI, WU: CLASSIFICATION IS A STRONG BASELINE FOR DEEP METRIC LEARNING

1

Classiﬁcation is a Strong Baseline for Deep
Metric Learning

Andrew Zhai*
andrew@pinterest.com

Hao-Yu Wu*
rexwu@pinterest.com

Pinterest, Inc.
San Francisco, US

Abstract

Deep metric learning aims to learn a function mapping image pixels to embedding
feature vectors that model the similarity between images. Two major applications of
metric learning are content-based image retrieval and face veriﬁcation. For the retrieval
tasks, the majority of current state-of-the-art (SOTA) approaches are triplet-based non-
parametric training. For the face veriﬁcation tasks, however, recent SOTA approaches
have adopted classiﬁcation-based parametric training. In this paper, we look into the ef-
fectiveness of classiﬁcation based approaches on image retrieval datasets. We evaluate
on several standard retrieval datasets such as CAR-196, CUB-200-2011, Stanford Online
Product, and In-Shop datasets for image retrieval and clustering, and establish that our
classiﬁcation-based approach is competitive across different feature dimensions and base
feature networks. We further provide insights into the performance effects of subsam-
pling classes for scalable classiﬁcation-based training, and the effects of binarization,
enabling efﬁcient storage and computation for practical applications.

Introduction

Metric learning, also known as learning image embeddings, is a core problem of a variety of
applications including face recognition [13, 16, 22, 23], ﬁne-grained retrieval [17, 18, 27],
clustering [31], and visual search [7, 11, 32, 33]. The goal of metric learning is that the
learned embedding generalize well to novel instances during test time, an open-set setup in
machine learning. This goal aligns well with practical applications in which the deployed
machine learning system is required to handle large amount of unseen data.

Standard deep neural network metric learning methods train image embeddings through
the local relationships between images in the form of pairs [1, 2] or triplets [6, 16]. A core
challenge with metric learning is sampling informative samples for training. As described
in [5, 16, 27], negatives that are too hard can destabilize training, while negatives that are too
easy result in triplets that have near zero loss leading to slow convergence.

Recent methods such as [5, 17, 18, 27] focus on addressing this sampling problem, many
of which utilize the relationships of all images within the batch to form informative triplets.
These methods typically require a large batch size so that informative triplets can be selected

c(cid:13) 2019. The copyright of this document resides with its authors.
It may be distributed unchanged freely in print or electronic forms.

*These two authors contributed equally to this work

2

ZHAI, WU: CLASSIFICATION IS A STRONG BASELINE FOR DEEP METRIC LEARNING

Figure 1: Architecture overview for training high dimensional binary embedding

within the batch. In practice, batch size is constrained by the hardware memory size. There-
fore, as the dataset size grows, one still faces the challenge of the diminishing probability of
a randomly sampled batch containing any informative triplet.

To alleviate the challenge of sampling informative triplets, another school of deep met-
ric learning approaches propose to use classiﬁcation-based training [13, 14, 22, 23, 29]. In
contrast to triplet-based approaches, the classiﬁcation-based approach can be viewed as ap-
proximating each class using a proxy [14], and uses all proxies to provide global context
for each training iteration. Though classiﬁcation-based metric learning simplify training by
removing sampling, they have scalability limitations as in the extreme classiﬁcation [18]
problem, where it is considered impractical to extend to much larger number of classes dur-
ing training.

While generic deep metric learning approaches have been pushing SOTA through better
negative sampling and ensembles of triplet-based approaches, face veriﬁcation has in parallel
seen SOTA results through classiﬁcation-based loss functions. We set out to investigate if
classiﬁcation-based training can perform similarly well for general image retrieval tasks.

Our major contributions are as follows: 1) we establish that classiﬁcation is a strong
baseline for deep metric learning across different datasets, base feature networks and em-
bedding dimensions, 2) we provide insights into the performance effects of binarization
and subsampling classes for scalable extreme classiﬁcation-based training 3) we propose
a classiﬁcation-based approach to learn high-dimensional binary embeddings that achieve
state-of-the-art retrieval performance with the same memory footprint as 64 dimensional
ﬂoat embeddings across the suite of retrieval tasks.

2 Related Works

Metric Learning Losses Metric learning approaches aim to learn a good embedding space
such that the similarity between samples are preserved as distance between embedding vec-
tors of the samples. The metric learning losses, such as contrastive loss[2] and triplet loss[6],
are formulated to minimize intra-class distances and maximize inter-class distances. Recent
approaches in metric learning design the loss function to consider the relationships of all the
samples within the training batch[9, 17, 18, 21, 27], and achieve state-of-the-art performance

ZHAI, WU: CLASSIFICATION IS A STRONG BASELINE FOR DEEP METRIC LEARNING

3

in image retrieval datasets[12, 18, 25].

Training Sampling Sampling informative training samples plays an important role in
metric learning as also suggested in [16, 27]. Semi-hard sampling in conjunction with triplet-
loss [16] has been widely adopted for many tasks. Distanced-weighted sampling [27] sug-
gests that with a balanced mix of difﬁculties during training, the image retrieval performance
can be further improved. Hierarchical Triplet Loss [24] proposed that by merging similar
classes dynamically during training into a hierarchical tree, more informative samples can
be drawn from such a structure and the loss also provides global context for training.

Classiﬁcation Losses for Metric Learning Classiﬁcation losses are widely adopted in
face veriﬁcation applications [13, 22, 23] and achieve state-of-the-art performance. The the-
oretical link between classiﬁcation and metric learning is shown in [14], and image retrieval
tasks have some success adopting classiﬁcation loss [14, 29]. Though classiﬁcation-based
training alleviates the need for carefully choosing sampling method, the parametric nature
may cause issue in ﬁne-grained open-set recognition setting [28].

Ensembling Ensembling embeddings has been the most recent focus to further improve
image retrieval performance. The ensembled embeddings are trained via boosting [15], at-
tending diverse spatial locations [26], or partitioning the training classes [29]. However, such
ensembled embeddings trade off image retrieval performance with higher dimensions. They
also introduce additional hyperparameters into the system.

3 Method

We study recent model architectures and triplet-based and classiﬁcation losses in Section 4.
Beyond standard classiﬁcation training [4], we describe below techniques we used to achieve
SOTA on the retrieval tasks including L2 normalization of embedding to optimize for cosine
similarity, Layer Normalization of ﬁnal pooled feature layer, and class balanced sampling of
minibatch. An overview of our approach is shown in Figure 1.

3.1 Normalized Softmax Loss

When training the classiﬁcation network for metric learning, we remove the bias term in
the last linear layer and add an L2 normalization module to the inputs and weights before
softmax loss to optimize for cosine similarity. Temperature scaling is used for exaggerating
the difference among classes and boosting the gradients as also used in [13, 23, 28]. We
follow the same derivation and notations as in [14]. For embedding x of input image with
class label y, the loss with temperature σ can be expressed with the weight of class y py
among all possible classes set Z:

Lnorm = − log

(cid:18) exp(xT py/σ ))

(cid:19)

∑z∈Z exp(xT pz/σ ))

(1)

Normalized softmax loss ﬁts into the proxy paradigm when we view the class weight as
proxy and choose the distance metric as cosine distance function. A more general form of
classiﬁcation loss, Large Margin Cosine Loss (LMCL), has been proposed for face veriﬁca-
tion [23] with an additional margin hyperparameter. We also evaluate the effect of LMCL in
conjunction with our proposed techniques on image retrieval tasks in Section 4.6.

4

ZHAI, WU: CLASSIFICATION IS A STRONG BASELINE FOR DEEP METRIC LEARNING

3.2 Layer Normalization

The layer normalization without afﬁne parameters[10] is added immediately after the ﬁnal
pooling layer of the feature model (e.g. GoogleNet [20]’s pool5 layer) to normalize the
feature dimension of our embeddings to have a distribution of values centered at zero. This
allows us to easily binarize embeddings via thresholding at zero. We also show empirically
through ablation experiments in Section 4.4.1 that layer normalization helps the network
better initialize new parameters and reach better optima.

3.3 Class Balanced Sampling

Based on the proxy explanation of using classiﬁcation loss for metric learning, the loss is
bounded by the worst approximated examples within the class [14]. A simple way to alleviate
this is by including multiple examples per class when constructing the training mini batch.
For each training batch, we ﬁrst sample C classes, and sample S samples within each class.
The effect of class balanced sampling is studied through ablation experiment in Section 4.4.2
and is a common approach to retrieval tasks [17, 27].

4 Experiments

We follow the same evaluation protocol commonly used in image retrieval tasks with the
standard train/test split on four datasets: CARS-196 [12], CUB-200-2011 [25], Stanford On-
line Products (SOP) [18], and In-shop Clothes Retrieval [34]. We compare our method using
Recall@K to measure retrieval quality. To compute Recall@K, we use cosine similarity to
retrieve the top K images from the test set, excluding the query image itself.

We ﬁrst investigate how our embeddings trained with normalized softmax loss compare
against embeddings trained with existing metric learning losses using the same featurizer and
embedding dimension. We then study in detail how the dimensionality of our embeddings
(Section 4.3) affects its performance and the relative performance between ﬂoat and binary
embeddings. Ablation studies on different design choices of our approach on CUB-200-
2011 [25] (Section 4.4) are provided as empirical justiﬁcations. Next, we investigate how our
embeddings are affected by class subsampling in Section 4.5, addressing the key scalability
concern of softmax loss where training complexity is linear with the number of classes.
Finally in Section 4.6, we show that our method outperforms state-of-the-art methods on
several retrieval datasets.

4.1

Implementation

All experiments were executed using PyTorch and a Tesla V100 graphic card. We compare
our method with common architectures used in metric learning including GoogleNet [20],
GoogleNet with Batch Normalization [8], and ResNet50 [4]. We initialize our networks
with pre-trained ImageNet ILSVRC-2012 [3] weights. We add a randomly initialized fully
connected layer to the pool5 features of each architecture to learn embeddings of varying di-
mensionality. To simplify the sensitivity to the initialization of the fully connected layer, we
add a Layer Normalization [10] without additional parameters between the pool5 and fully
connected layer (See Section 4.4.1 for the ablation study). We L2 normalize the embedding
and class weights before Softmax and use a temperature of 0.05 consistently.

ZHAI, WU: CLASSIFICATION IS A STRONG BASELINE FOR DEEP METRIC LEARNING

5

Figure 2: Recall@1 for CARS-196 (left) and CUB-200-2011 (right) across varying em-
bedding dimensions. Softmax based embeddings improve performance when increasing di-
mensionality. The performance gap between ﬂoat and binary embeddings converge when
increasing dimensionality, showing that when given enough representational freedom, Soft-
max learns bit like features.

Unless otherwise stated, we ﬁrst train for 1 epoch, updating only new parameters for
better initialization. We then optimize all parameters for 30 epochs with a batch size of 75
with learning rate 0.01 with exception to CUB200 where we use learning rate of 0.001 due
to overﬁtting. We construct our batch by sampling 25 examples per class for Cars196 and
CUB200 and 5 examples per class for SOP and InShop (as only ≈ 5 images per class in
dataset). We use SGD with momentum of 0.9, weight decay of 1e-4, and gamma of 0.1
to reduce learning rate at epoch 15. During training, we apply horizontal mirroring and
random crops from 256x256 images; during testing we center crop from the 256x256 im-
age. Following [9] [14], we crop to 227x227 for GoogleNet, otherwise 224x224. Complete
implementation details can be found in our source code repository.1

4.2 Loss Function Comparisons

We compare our normalized softmax loss against existing metric learning losses. To focus
on contributions from the loss functions, we leave comparisons against methods that en-
semble models [29, 30], modify the feature extractor architecture [26], or propose complex
activation paths between the featurizer and ﬁnal embedding [15] for Section 4.6.

We present Recall@K and NMI results on three standard retrieval datasets in Table 1,
Table 2, and Table 3, comparing against reported performance of methods trained with
model architectures of GoogleNet, GoogleNet with Batch Normalization (BNInception), and
ResNet50 respectively. For GoogleNet with Stanford Online Products only, we saw around
a 1% Recall@1 improvement by training all parameters from start with 10x learning rate on
new parameters when compared with models trained with our standard ﬁnetuning procedure.

As shown, our approach compares very strongly against existing baselines. When ﬁxing
dimensionality to 512, we see that the performance improvements of our softmax embed-
dings across architectures mirror classiﬁcation performance on ImageNet ILSVRC-2012.
We hope our results help disentangle performance improvements of existing metric learning
methods due to advancements in methodology versus changes of base feature models.

6

ZHAI, WU: CLASSIFICATION IS A STRONG BASELINE FOR DEEP METRIC LEARNING

SOP

Recall@K
Contras.128 [18]
Lift. Struc128 [18]
Lift. Struc512 [18]
Hist Loss512 [21]
Bin. Dev512 [21]
Npairs512 [17]
Npairs64 [17]
Angular512 [9]
NormSoftmax512

2
32.3
60.3
-
-
-
-
79.7
81.4
84.7
Table 1: Recall@K and NMI across standard retrieval tasks. All methods are trained using
GoogleNet for a fair comparison.

100 NMI
73.8
-
91.3
92.2
92.3
93.0
-
93.5
93.1

NMI
-
55.0
-
-
-
-
64.0
63.2
64.5

NMI
-
55.6
-
-
-
-
60.4
61.1
62.8

10
58.2
-
79.8
81.7
82.3
83.8
-
85.0
84.5

1
42.0
-
62.1
63.9
65.5
67.7
-
70.9
69.0

8
58.9
81.5
-
-
-
-
91.6
92.1
94.2

2
37.7
58.9
-
61.9
64.4
-
63.3
66.3
67.0

8
62.3
80.2
-
82.4
83.9
-
83.2
83.9
85.4

1
21.7
49.0
-
-
-
-
71.1
71.4
75.2

1
26.4
47.2
-
50.3
52.8
-
51.0
54.7
55.3

-
-
-
-
-
88.1
-
88.6
88.2

CARS-196
4
46.1
72.1
-
-
-
-
86.5
87.5
90.4

CUB-200
4
49.8
70.2
-
72.6
74.7
-
74.3
76.0
77.6

Recall@K
Clustering64 [19]
Proxy NCA64[14]
HTL512 [24]
NormSoftmax512

2
70.6
82.4
88.0
88.9
Table 2: Recall@K and NMI across standard retrieval tasks. All methods are trained using
BNInception for a fair comparison

100 NMI
89.5
93.2
90.6
-
-
94.8
95.0
89.8

NMI
59.2
59.5
-
66.2

NMI
59.0
64.9
-
70.5

10
83.7
-
88.3
88.1

1
67.0
73.7
74.8
73.8

8
81.9
72.4
86.5
88.4

2
61.4
61.9
68.8
72.0

8
87.8
88.7
95.7
96.0

1
48.2
49.2
57.1
59.6

1
58.1
73.2
81.4
81.7

SOP

SOP

Recall@K
Margin128 [27]
NormSoftmax128
NormSoftmax512

2
86.5
88.7
90.4
Table 3: Recall@K and NMI across standard retrieval tasks. All methods are trained using
ResNet50 for a fair comparison

100 NMI
90.7
93.8
90.4
95.2
91.0
96.2

NMI
69.0
66.9
69.7

NMI
69.1
72.9
74.0

2
74.4
69.6
73.9

1
63.6
56.5
61.3

8
95.1
96.3
96.9

10
86.2
88.7
90.6

8
90.0
87.6
90.0

1
72.7
75.2
78.2

1
79.6
81.6
84.2

CARS-196
4
80.3
86.4
92.7
93.4

CARS-196
4
91.9
93.4
94.4

CUB-200
4
71.8
67.9
78.7
81.2

CUB-200
4
83.1
79.9
83.5

4.3 Embedding Dimensionality

We study the effects of dimensionality on our classiﬁcation-based embeddings by varying
only the embedding dimension while keeping all other optimization parameters ﬁxed. We
have consistently observed that dimensionality is directly related to retrieval performance.
Two examples of this across different datasets (CARS-196 and CUB-200-2011) and model
architectures (ResNet50 and GoogleNet) are shown in Figure 2. Interestingly, this contra-
dicts to reported behaviors for previous non-parametric metric learning methods [9, 18, 19],
showing that dimensionality does not signiﬁcantly affect retrieval performance. This differ-
ence is seen clearly when comparing R@1 across dimensionality for CUB-200-2011 with
GoogleNet in Figure 2 with the same dataset and model combination from [18].

Higher dimensional embeddings lead to an increase in retrieval performance. Lower di-
mensional embeddings however are preferred for scalability to reduce storage and distance
computation costs especially in large scale applications such as visual search [11]. We ob-
serve however that as we increase dimensionality of our embeddings, the optimizer does not
fully utilize the higher dimensional metric space. Instead, we see that feature dimensions
start relying less on the magnitude of each feature dimension and instead rely on the sign
value. In Figure 2, we see for both datasets that the Recall@1 performance of binary fea-
tures (thresholding the feature value at zero) converges with the performance of the ﬂoat
embeddings. This is a consistent result we see across datasets and model architectures. We
show that training high dimensional embeddings and binarizing leads to the best trade-off of

1Code: https://github.com/azgo14/classiﬁcation_metric_learning.git

ZHAI, WU: CLASSIFICATION IS A STRONG BASELINE FOR DEEP METRIC LEARNING

7

Figure 3: Loss and R@1 trends for training CUB-200 ResNet50 with and without Layer
Normalization. Layer Normalization helps initialize learning, leading to better training con-
vergence and R@1 performance.

S
C
R@1

-
-
59.5

1
75
59.6

3
25
60.0

12
6
60.8

25
3
61.3

37
2
59.6

75
1
40.9

Table 4: ResNet50 Recall@1 on CUB-200-2011 dataset across varying samples per class for
batch size of 75. (S) Samples per class in batch. (C) Distinct classes in batch. First column
shows no class balancing in batch

performance and scalability as described in Section 4.6.

4.4 Ablation Studies

In this set of experiments we report the effect on Recall@1 with different design choices in
our approach on CUB-200-2011. We train the ResNet50 with embedding dimension of 512
variant as in Table 3. We ﬁx all hyperparameters besides the one of interest.

4.4.1 Layer Normalization

We utilize Layer Normalization [10] without parameters to standardize activation values
after pooling to help the initialization of our training. With 100 classes in CUB-200-2011,
we expect that a random classiﬁer would have a loss value of roughly − ln( 1
100 ) = 4.6. As
shown in Table 3, this occurs when training with Layer Normalization, but not without. We
have found incorporating Layer Normalization in our training allows us to be robust against
poor weight initialization of new parameters across model architectures, alleviating the need
for careful weight initialization.

4.4.2 Class Balanced Sampling

As seen in Table 4, class balanced sampling is beneﬁcial over random sequential iteration
over the dataset. Generally we’ve observed that performance improves when we have more
samples per class until too few distinct classes exist in the minibatch where the bias of
separating few distinct classes may introduce too much noise to the optimization, resulting

8

ZHAI, WU: CLASSIFICATION IS A STRONG BASELINE FOR DEEP METRIC LEARNING

Figure 4: Recall@K for SOP with ResNet50 across class sampling ratios. We see that with
sampling only 10% of classes per iteration (∼1K classes), we converge to a R@1 that is less
than 1% absolute drop in performance from using all classes.

in lower performance. For SOP and InShop, we simply sample roughly how many images
exist per class on average (≈ 5).

4.5 Subsampling for Classiﬁcation Scalability

To show that classiﬁcation-based training is scalable, we apply the subsampling methodol-
ogy to the Stanford Online Products dataset, which has the largest number of classes among
datasets we tested. By subsampling, for each training minibatch the network only need to
classify among a subset of all classes (randomly sampled subset which includes all classes
within the training minibatch). We present our ﬁndings in Figure 4, showing that with only
10% of the classes available during the forward pass of training, we can reach a R@1 per-
formance comparable to using all classes (1% drop in performance). When using 1% of
classes, we reach a R@1 of 75.7. When using 0.1% of classes, we reach a R@1 of 72.0. As
we can see, subsampling classes during training is an effective method of scaling softmax
embedding training with little drop in performance.

4.6 Comparison against State of the Art

Finally we compare our best performing softmax embedding model against state-of-the-art
metric learning approaches on Stanford Online Products, In-Shop, CARS-196, and CUB-
200-2011. We reproduced the state-of-the-art method in face veriﬁcation literature, Large
Maring Cosine Loss (LMCL) [23], and trained two variants of the networks: one with their
recommended network architecture of 512 dimensional embeddings, and one with our mod-
iﬁcations of 2048 dimensional embeddings.

As shown in Table 5 and Table 6, our 2048 dimensional ResNet50 embedding outper-
forms previous approaches. Considering the higher dimensionality of our embeddings, we
also show that our 2048 binary embedding, sharing the same memory footprint of a 64 di-
mensional ﬂoat embedding, similarly outperforms state-of-the-art baselines. These binary
features were obtained by thresholding the ﬂoat embedding features at zero as in Figure 2.
Considering the scalability and performance of our binary softmax features along the sim-
plicity of our approach, we believe softmax embeddings should be a strong baseline for
future metric learning work.

ZHAI, WU: CLASSIFICATION IS A STRONG BASELINE FOR DEEP METRIC LEARNING

9

Net.

Recall@K
Contrastive128 [18]
G
Lifted Struct512 [18]
G
Clustering64 [19]
B
Npairs512 [17]
G
HDC384 [30]
G
Angular Loss512 [9]
G
Margin128 [27]
R50
Proxy NCA64[14]
B
A-BIER512 [15]
G
HTL512 [24]
B
HTL128 [24]
B
ABE-8512 [26]
G†
DREML9216 [29]
R18
LMCL512 [23]
R50
LMCL∗2048 [23]
R50
NormSoftmax1024
B
NormSoftmax128
R50
NormSoftmax512
R50
NormSoftmax2048
R50
NormSoftmax2048bits R50

SOP
10
58.2
79.8
83.7
83.8
84.4
85.0
86.2
-
86.9
88.3
-
88.4
-
76.5
89.7
88.3
88.7
90.6
91.5
90.9

1
42.0
62.1
67.0
67.7
69.5
70.9
72.7
73.7
74.2
74.8
-
76.3
-
60.9
77.4
74.7
75.2
78.2
79.5
78.2

100
73.8
91.3
-
93.0
92.8
93.5
93.8
-
94.0
94.8
-
94.8
-
87.0
95.3
95.2
95.2
96.2
96.7
96.4

1
-
-
-
-
62.1
-
-
-
83.1
-
-
87.3
78.4
73.3
89.8
86.6
86.6
88.6
89.4
88.8

In-Shop
20
10
-
-
-
-
-
-
-
-
89.0
84.9
-
-
-
-
-
-
96.9
95.1
-
-
94.3
80.9
97.9
96.7
95.8
93.7
93.1
90.5
97.8
98.6
98.0
97.0
97.8
96.8
98.4
97.5
98.7
97.8
97.8
98.5

Net.

Recall@K
Contrastive128 [18]
G
Lifted Struct128 [18]
G
Clustering64 [19]
B
Npairs64 [17]
G
Angular Loss512 [9]
G
Proxy NCA64 [14]
B
HDC384 [30]
G
Margin128 [27]
R50
HTL512 [24]
B
A-BIER512 [15]
G
ABE-8512 [26]
G†
DREML576 [29]
R18
LMCL512 [23]
R50
LMCL∗2048 [23]
R50
NormSoftMax1024
B
NormSoftmax128
R50
NormSoftmax512
R50
NormSoftmax2048
R50
NormSoftmax2048bits R50

CARS-196
4
2
46.1
32.3
72.1
60.3
80.3
70.6
86.5
79.7
87.5
81.4
86.4
82.4
89.5
83.2
91.9
86.5
92.7
88.0
93.2
89.0
94.0
90.5
95.0
91.7
87.4
81.7
95.7
93.1
96.2
93.2
93.4
88.7
94.4
90.4
96.4
94.1
96.4
93.7

1
21.7
49.0
58.1
71.1
71.4
73.2
73.7
79.6
81.4
82.0
85.2
86.0
73.9
88.3
87.9
81.6
84.2
89.3
88.7

8
58.9
81.5
87.8
91.6
92.1
88.7
93.8
95.1
95.7
96.1
96.1
97.2
91.5
97.4
98.1
96.3
96.9
98.0
98.0

1
26.4
47.2
48.2
51.0
54.7
49.2
53.6
63.6
57.1
57.5
60.6
63.9
58.7
61.2
62.2
56.5
61.3
65.3
63.3

CUB-200
4
2
49.8
37.7
70.2
58.9
71.8
61.4
74.3
63.3
76.0
66.3
67.9
61.9
77.0
65.7
83.1
74.4
78.7
68.8
78.3
68.7
79.8
71.5
83.1
75.0
79.9
70.3
80.4
71.4
82.7
73.9
79.9
69.6
83.5
73.9
85.4
76.7
84.3
75.2

30
-
-
-
-
91.2
-
-
-
97.5
-
95.8
98.2
96.7
94.3
98.8
98.5
98.3
98.8
99.0
98.8

8
62.3
80.2
81.9
83.2
83.9
72.4
85.6
90.0
86.5
86.2
87.4
89.7
86.9
87.4
89.4
87.6
90.0
91.8
91.0

Table 5: Recall@K on Stanford Online Products (SOP) and In-Shop. R - ResNet, G -
GoogleNet, B - BNInception, † refers to refers to additional attention parameters, LMCL∗ is
our method trained with the Loss

Table 6: Recall@K on CARS-196 and CUB-200-2011. R - ResNet, G - GoogleNet, B -
BNInception, † refers to additional attention parameters, LMCL∗ is our method trained with
the Loss

10 ZHAI, WU: CLASSIFICATION IS A STRONG BASELINE FOR DEEP METRIC LEARNING

5 Conclusion

In this paper, we show that classiﬁcation-based metric learning approaches can achieve state-
of-the-art not only in face veriﬁcation but general image retrieval tasks. In the metric learn-
ing community, a diverse set of base networks for training embedding of different sizes are
compared to one another. In our work, we conducted comparisons through extensive exper-
imentation, and establish that normalized softmax loss is a strong baseline in a wide variety
of settings. We investigate common critiques of classiﬁcation training and high dimension-
ality for metric learning through subsampling, making our approach viable even for tasks
with a very large number of classes and binarization, allowing us to achieve SOTA perfor-
mance with the same memory footprint as 64 dimensional ﬂoat embeddings across the suite
of retrieval tasks. We believe these practical beneﬁts are valuable for large scale deep metric
learning and real world applications.

References

[1] Sean Bell and Kavita Bala. Learning visual similarity for product design with convo-

lutional neural networks. ACM Trans. on Graphics (SIGGRAPH), 34(4), 2015.

[2] Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning a similarity metric discrimina-
tively, with application to face veriﬁcation. In 2005 IEEE Computer Society Conference
on Computer Vision and Pattern Recognition (CVPR’05), volume 1, pages 539–546.
IEEE, 2005.

[3] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale

Hierarchical Image Database. In CVPR09, 2009.

[4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for

image recognition. arXiv preprint arXiv:1512.03385, 2015.

[5] Alexander Hermans, Lucas Beyer, and Bastian Leibe. In Defense of the Triplet Loss
for Person Re-Identiﬁcation. Technical report. URL https://arxiv.org/pdf/
1703.07737.pdf.

[6] Elad Hoffer and Nir Ailon. Deep metric learning using triplet network. CoRR,

abs/1412.6622, 2014. URL http://arxiv.org/abs/1412.6622.

[7] Houdong Hu, Yan Wang, Linjun Yang, Pavel Komlev, Li Huang, Xi (Stephen) Chen,
Jiapei Huang, Ye Wu, Meenaz Merchant, and Arun Sacheti. Web-scale respon-
In Proceedings of the 24th ACM SIGKDD International
sive visual search at bing.
Conference on Knowledge Discovery & Data Mining, KDD 2018, London, UK, Au-
gust 19-23, 2018, pages 359–367, 2018.
doi: 10.1145/3219819.3219843. URL
http://doi.acm.org/10.1145/3219819.3219843.

[8] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network
training by reducing internal covariate shift. CoRR, abs/1502.03167, 2015. URL
http://arxiv.org/abs/1502.03167.

[9] Shilei Wen Xiao Liu Yuanqing Lin Jian Wang, Feng Zhou. Deep metric learning with

angular loss. In International Conference on Computer Vision, 2017.

ZHAI, WU: CLASSIFICATION IS A STRONG BASELINE FOR DEEP METRIC LEARNING 11

[10] Geoffrey E. Hinton Jimmy Lei Ba, Jamie Ryan Kiros. Layer normalization. arXiv

preprint arXiv:1607.06450, 2016.

[11] Y. Jing, D. Liu, D. Kislyuk, A. Zhai, J. Xu, and J. Donahue. Visual search at pinterest.
In Proceedings of the International Conference on Knowledge Discovery and Data
Mining (SIGKDD).

[12] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for
ﬁne-grained categorization. In 4th International IEEE Workshop on 3D Representation
and Recognition (3dRR-13), Sydney, Australia, 2013.

[13] Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song.

Sphereface: Deep hypersphere embedding for face recognition. CVPR’17.

[14] Yair Movshovitz-Attias, Alexander Toshev, Thomas K. Leung, Sergey Ioffe, and
Saurabh Singh. No fuss distance metric learning using proxies. CoRR, abs/1703.07464,
2017. URL http://arxiv.org/abs/1703.07464.

[15] M. Opitz, G. Waltner, H. Possegger, and H. Bischof. Deep Metric Learning with BIER:

Boosting Independent Embeddings Robustly. arXiv:cs/1801.04815, 2018.

[16] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A uniﬁed embed-
ding for face recognition and clustering. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2015.

[17] Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In
D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in
Neural Information Processing Systems 29, pages 1857–1865. Curran Associates, Inc.,
2016.

[18] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning
via lifted structured feature embedding. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2016.

[19] Hyun Oh Song, Stefanie Jegelka, Vivek Rathod, and Kevin Murphy. Deep metric
In Computer Vision and Pattern Recognition (CVPR),

learning via facility location.
2017.

[20] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper
with convolutions. arXiv preprint arXiv:1409.4842, 2014.

[21] Evgeniya Ustinova and Victor Lempitsky. Learning deep embeddings with histogram

loss. In Neural Information Processing Systems, 2016.

[22] Feng Wang, Jian Cheng, Weiyang Liu, and Haijun Liu. Additive margin softmax for

face veriﬁcation. IEEE Signal Processing Letters, 2018.

[23] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Zhifeng Li, Dihong Gong, Jingchao
Zhou, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition. CVPR
’18.

12 ZHAI, WU: CLASSIFICATION IS A STRONG BASELINE FOR DEEP METRIC LEARNING

[24] Dengke Dong Weifeng Ge, Weilin Huang and Matthew R. Scott. Deep metric learning

with hierarchical triplet loss. In ECCV, 2018.

[25] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona.
Caltech-UCSD Birds 200. Technical Report CNS-TR-2010-001, California Institute
of Technology, 2010.

[26] Kunal Chawla Jungmin Lee Keunjoo Kwon Wonsik Kim, Bhavya Goyal. Attention-

based ensemble for deep metric learning. In ECCV, 2018.

[27] Chao-Yuan Wu, R. Manmatha, Alexander J. Smola, and Philipp Krähenbühl. Sampling

matters in deep embedding learning. CoRR, abs/1706.07567, 2017.

[28] Zhirong Wu, Alexei A. Efros, and Stella X. Yu. Improving generalization via scalable
neighborhood component analysis. In Computer Vision - ECCV 2018 - 15th European
Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part VII, pages
712–728, 2018.

[29] Hong Xuan, Richard Souvenir, and Robert Pless. Deep randomized ensembles for

metric learning. In ECCV, 2018.

[30] Yuhui Yuan, Kuiyuan Yang, and Chao Zhang. Hard-aware deeply cascaded embedding.

arXiv preprint arXiv:1611.05720, 2016.

[31] Jingbin Wang David Tsai Chuck Rosenberg Yushi Jing, Henry Rowley and Michele
Covell. Google image swirl: a large-scale content-based image visualization system.
In Proceedings of the 21st International Conference on World Wide Web, 2012.

[32] Andrew Zhai, Dmitry Kislyuk, Yushi Jing, Michael Feng, Eric Tzeng, Jeff Don-
ahue, Yue Li Du, and Trevor Darrell. Visual discovery at pinterest. arXiv preprint
arXiv:1702.04680, 2017.

[33] Yanhao Zhang, Pan Pan, Yun Zheng, Kang Zhao, Yingya Zhang, Xiaofeng Ren, and
Rong Jin. Visual search at alibaba. In Proceedings of the 24th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery & Data Mining, KDD 2018, London, UK,
August 19-23, 2018, pages 993–1001, 2018. doi: 10.1145/3219819.3219820. URL
http://doi.acm.org/10.1145/3219819.3219820.

[34] Shi Qiu Xiaogang Wang Ziwei Liu, Ping Luo and Xiaoou Tang. Deepfashion: Pow-
ering robust clothes recognition and retrieval with rich annotations. In Proceedings of
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.

