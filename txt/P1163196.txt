6
1
0
2
 
l
u
J
 
9
2
 
 
]

V
C
.
s
c
[
 
 
2
v
4
1
1
9
0
.
3
0
6
1
:
v
i
X
r
a

LIFT: Learned Invariant Feature Transform

Kwang Moo Yi∗,1, Eduard Trulls∗,1, Vincent Lepetit2, Pascal Fua1

1Computer Vision Laboratory, Ecole Polytechnique F´ed´erale de Lausanne (EPFL)
2Institute for Computer Graphics and Vision, Graz University of Technology
{kwang.yi, eduard.trulls, pascal.fua}@epfl.ch, lepetit@icg.tugraz.at

Abstract. We introduce a novel Deep Network architecture that imple-
ments the full feature point handling pipeline, that is, detection, orienta-
tion estimation, and feature description. While previous works have suc-
cessfully tackled each one of these problems individually, we show how to
learn to do all three in a uniﬁed manner while preserving end-to-end dif-
ferentiability. We then demonstrate that our Deep pipeline outperforms
state-of-the-art methods on a number of benchmark datasets, without
the need of retraining.

Keywords: Local Features, Feature Descriptors, Deep Learning

1 Introduction

Local features play a key role in many Computer Vision applications. Find-
ing and matching them across images has been the subject of vast amounts of
research. Until recently, the best techniques relied on carefully hand-crafted fea-
tures [1,2,3,4,5]. Over the past few years, as in many areas of Computer Vision,
methods based in Machine Learning, and more speciﬁcally Deep Learning, have
started to outperform these traditional methods [6,7,8,9,10].

These new algorithms, however, address only a single step in the complete
processing chain, which includes detecting the features, computing their orienta-
tion, and extracting robust representations that allow us to match them across
images. In this paper we introduce a novel Deep architecture that performs all
three steps together. We demonstrate that it achieves better overall performance
than the state-of-the-art methods, in large part because it allows these individual
steps to be optimized to perform well in conjunction with each other.

Our architecture, which we refer to as LIFT for Learned Invariant Feature
Transform, is depicted by Fig. 1. It consists of three components that feed into
each other: the Detector, the Orientation Estimator, and the Descriptor. Each
one is based on Convolutional Neural Networks (CNNs), and patterned after
recent ones [6,9,10] that have been shown to perform these individual functions
well. To mesh them together we use Spatial Transformers [11] to rectify the

∗ First two authors contributed equally.
This work was supported in part by the EU FP7 project MAGELLAN under grant
number ICT-FP7-611526.

2

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Fig. 1. Our integrated feature extraction pipeline. Our pipeline consists of three major
components: the Detector, the Orientation Estimator, and the Descriptor. They are
tied together with diﬀerentiable operations to preserve end-to-end diﬀerentiability.1

image patches given the output of the Detector and the Orientation Estimator.
We also replace the traditional approaches to non-local maximum suppression
(NMS) by the soft argmax function [12]. This allows us to preserve end-to-end
diﬀerentiability, and results in a full network that can still be trained with back-
propagation, which is not the case of any other architecture we know of.

Also, we show how to learn such a pipeline in an eﬀective manner. To this
end, we build a Siamese network and train it using the feature points produced
by a Structure-from-Motion (SfM) algorithm that we ran on images of a scene
captured under diﬀerent viewpoints and lighting conditions, to learn its weights.
We formulate this training problem on image patches extracted at diﬀerent scales
to make the optimization tractable. In practice, we found it impossible to train
the full architecture from scratch, because the individual components try to op-
timize for diﬀerent objectives. Instead, we introduce a problem-speciﬁc learning
approach to overcome this problem. It involves training the Descriptor ﬁrst,
which is then used to train the Orientation Estimator, and ﬁnally the Detector,
based on the already learned Descriptor and Orientation Estimator, diﬀerenti-
ating through the entire network. At test time, we decouple the Detector, which
runs over the whole image in scale space, from the Orientation Estimator and
Descriptor, which process only the keypoints.

In the next section we brieﬂy discuss earlier approaches. We then present our
approach in detail and show that it outperforms many state-of-the-art methods.

2 Related work

The amount of literature relating to local features is immense, but it always
revolves about ﬁnding feature points, computing their orientation, and matching
them. In this section, we will therefore discuss these three elements separately.

2.1 Feature Point Detectors

Research on feature point detection has focused mostly on ﬁnding distinctive lo-
cations whose scale and rotation can be reliably estimated. Early works [13,14]

1 Figures are best viewed in color.

LIFT: Learned Invariant Feature Transform

3

used ﬁrst-order approximations of the image signal to ﬁnd corner points in im-
ages. FAST [15] used Machine Learning techniques but only to speed up the
process of ﬁnding corners. Other than corner points, SIFT [1] detect blobs in
scale-space; SURF [2] use Haar ﬁlters to speed up the process; Maximally Sta-
ble Extremal Regions (MSER) [16] detect regions; [17] detect aﬃne regions.
SFOP [18] use junctions and blobs, and Edge Foci [19] use edges for robustness
to illumination changes. More recently, feature points based on more sophisti-
cated and carefully designed ﬁlter responses [5,20] have also been proposed to
further enhance the performance of feature point detectors.

In contrast to these approaches that focus on better engineering, and follow-
ing the early attempts in learning detectors [21,22], [6] showed that a detector
could be learned to deliver signiﬁcantly better performance than the state-of-
the-art. In this work, piecewise-linear convolutional ﬁlters are learned to robustly
detect feature points in spite of lighting and seasonal changes. Unfortunately, this
was done only for a single scale and from a dataset without viewpoint changes.
We therefore took our inspiration from it but had to extend it substantially to
incorporate it into our pipeline.

2.2 Orientation Estimation

Despite the fact that it plays a critical role in matching feature points, the
problem of estimating a discriminative orientation has received noticeably less
attention than detection or feature description. As a result, the method intro-
duced by SIFT [1] remains the de facto standard up to small improvements, such
as the fact that it can be sped-up by using the intensity centroid, as in ORB [4].
A departure from this can be found in a recent paper [9] that introduced a
Deep Learning-based approach to predicting stable orientations. This resulted
in signiﬁcant gains over the state-of-the-art. We incorporate this architecture
into our pipeline and show how to train it using our problem-speciﬁc training
strategy, given our learned descriptors.

2.3 Feature Descriptors

Feature descriptors are designed to provide discriminative representations of
salient image patches, while being robust to transformations such as viewpoint
or illumination changes. The ﬁeld reached maturity with the introduction of
SIFT [1], which is computed from local histograms of gradient orientations, and
SURF [2], which uses integral image representations to speed up the computa-
tion. Along similar lines, DAISY [3] relies on convolved maps of oriented gra-
dients to approximate the histograms, which yields large computational gains
when extracting dense descriptors.

Even though they have been extremely successful, these hand-crafted de-
scriptors can now be outperformed by newer ones that have been learned. These
range from unsupervised hashing to supervised learning techniques based on
linear discriminant analysis [23,24], genetic algorithm [25], and convex optimiza-
tion [26]. An even more recent trend is to extract features directly from raw image

4

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Fig. 2. Our Siamese training architecture with four branches, which takes as input
a quadruplet of patches: Patches P1 and P2 (blue) correspond to diﬀerent views of
the same physical point, and are used as positive examples to train the Descriptor;
P3 (green) shows a diﬀerent 3D point, which serves as a negative example for the
Descriptor; and P4 (red) contains no distinctive feature points and is only used as a
negative example to train the Detector. Given a patch P, the Detector, the softargmax,
and the Spatial Transformer layer Crop provide all together a smaller patch p inside P.
p is then fed to the Orientation Estimator, which along with the Spatial Transformer
layer Rot, provides the rotated patch pθ that is processed by the Descriptor to obtain
the ﬁnal description vector d.

patches with CNNs trained on large volumes of data. For example, MatchNet [7]
trained a Siamese CNN for feature representation, followed by a fully-connected
network to learn the comparison metric. DeepCompare [8] showed that a net-
work that focuses on the center of the image can increase performance. The
approach of [27] relied on a similar architecture to obtain state-of-the-art results
for narrow-baseline stereo. In [10], hard negative mining was used to learn com-
pact descriptors that use on the Euclidean distance to measure similarity. The
algorithm of [28] relied on sample triplets to mine hard negatives.

In this work, we rely on the architecture of [10] because the corresponding
descriptors are trained and compared with the Euclidean distance, which has a
wider range of applicability than descriptors that require a learned metric.

3 Method

In this section, we ﬁrst formulate the entire feature detection and description
pipeline in terms of the Siamese architecture depicted by Fig. 2. Next, we discuss
the type of data we need to train our networks and how to collect it. We then
describe the training procedure in detail.

3.1 Problem formulation

We use image patches as input, rather than full images. This makes the learning
scalable without loss of information, as most image regions do not contain key-
points. The patches are extracted from the keypoints used by a SfM pipeline, as
will be discussed in Section 3.2. We take them to be small enough that we can

LIFT: Learned Invariant Feature Transform

5

assume they contain only one dominant local feature at the given scale, which
reduces the learning process to ﬁnding the most distinctive point in the patch.
To train our network we create the four-branch Siamese architecture pictured
in Fig. 2. Each branch contains three distinct CNNs, a Detector, an Orientation
Estimator, and a Descriptor. For training purposes, we use quadruplets of image
patches. Each one includes two image patches P1 and P2, that correspond to
diﬀerent views of the same 3D point, one image patch P3, that contains the pro-
jection of a diﬀerent 3D point, and one image patch P4 that does not contain any
distinctive feature point. During training, the i-th patch Pi of each quadruplet
will go through the i-th branch.

To achieve end-to-end diﬀerentiability, the components of each branch are

connected as follows:

1. Given an input image patch P, the Detector provides a score map S.
2. We perform a soft argmax [12] on the score map S and return the location

x of a single potential feature point.

3. We extract a smaller patch p centered on x with the Spatial Transformer
layer Crop (Fig. 2). This serves as the input to the Orientation Estimator.

4. The Orientation Estimator predicts a patch orientation θ.
5. We rotate p according to this orientation using a second Spatial Transformer

layer, labeled as Rot in Fig. 2, to produce pθ.

6. pθ is fed to the Descriptor network, which computes a feature vector d.

Note that the Spatial Transformer layers are used only to manipulate the
image patches while preserving diﬀerentiability. They are not learned modules.
Also, both the location x proposed by the Detector and the orientation θ for
the patch proposal are treated implicitly, meaning that we let the entire network
discover distinctive locations and stable orientations while learning.

Since our network consists of components with diﬀerent purposes, learning
the weights is non-trivial. Our early attempts at training the network as a whole
from scratch were unsuccessful. We therefore designed a problem-speciﬁc learn-
ing approach that involves learning ﬁrst the Descriptor, then the Orientation
Estimator given the learned descriptor, and ﬁnally the Detector, conditioned on
the other two. This allows us to tune the Orientation Estimator for the Descrip-
tor, and the Detector for the other two components.

We will elaborate on this learning strategy in Secs. 3.3 (Descriptor), 3.4 (Ori-
entation Estimator), and 3.5 (Detector), that is, in the order they are learned.

3.2 Creating the Training Dataset

There are datasets that can be used to train feature descriptors [24] and orien-
tation estimators [9]. However it is not so clear how to train a keypoint detec-
tor, and the vast majority of techniques still rely on hand-crafted features. The
TILDE detector [6] is an exception, but the training dataset does not exhibit
any viewpoint changes.

To achieve invariance we need images that capture views of the same scene
under diﬀerent illumination conditions and seen from diﬀerent perspectives. We

6

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Fig. 3. Sample images and patches from Piccadilly (left) and Roman-Forum (right).
Keypoints that survive the SfM pipeline are drawn in blue, and the rest in red.

thus turned to photo-tourism image sets. We used the collections from Piccadilly
Circus in London and the Roman Forum in Rome from [29] to reconstruct the
3D using VisualSFM [30], which relies of SIFT features. Piccadilly contains 3384
images, and the reconstruction has 59k unique points with an average of 6.5 ob-
servations for each. Roman-Forum contains 1658 images and 51k unique points,
with an average of 5.2 observations for each. Fig. 3 shows some examples.

We split the data into training and validation sets, discarding views of train-
ing points on the validation set and vice-versa. To build the positive training
samples we consider only the feature points that survive the SfM reconstruction
process. To extract patches that do not contain any distinctive feature point,
as required by our training method, we randomly sample image regions that
contain no SIFT features, including those that were not used by SfM.

We extract grayscale training patches according to the scale σ of the point,
for both feature and non-feature point image regions. Patches P are extracted
from a 24σ × 24σ support region at these locations, and standardized into S × S
pixels where S = 128. The smaller patches p and pθ that serve as input to the
Orientation Estimator and the Descriptor, are cropped and rotated versions of
these patches, each having size s×s, where s = 64. The smaller patches eﬀectively
correspond to the SIFT descriptor support region size of 12σ. To avoid biasing
the data, we apply uniform random perturbations to the patch location with a
range of 20% (4.8σ). Finally, we normalize the patches with the grayscale mean
and standard deviation of the entire training set.

3.3 Descriptor

Learning feature descriptors from raw image patches has been extensively re-
searched during the past year [7,8,10,27,28,31], with multiple works reporting
impressive results on patch retrieval, narrow baseline stereo, and matching non-
rigid deformations. Here we rely on the relatively simple networks of [10], with
three convolutional layers followed by hyperbolic tangent units, l2 pooling [32]
and local subtractive normalization, as they do not require learning a metric.

The Descriptor can be formalized simply as

d = hρ(pθ) ,

(1)

LIFT: Learned Invariant Feature Transform

7

where h(.) denotes the Descriptor CNN, ρ its parameters, and pθ is the rotated
patch from the Orientation Estimator. When training the Descriptor, we do not
yet have the Detector and the Orientation Estimator trained. We therefore use
the image locations and orientations of the feature points used by the SfM to
generate image patches pθ.

We train the Descriptor by minimizing the sum of the loss for pairs of cor-
(cid:1) and the loss for pairs of non-corresponding patches
(cid:1) is deﬁned as the hinge embedding loss of the

responding patches (cid:0)p1
(cid:0)p1
θ , pl
θ
Euclidean distance between their description vectors. We write

(cid:1). The loss for pair (cid:0)pk

θ, p2
θ

θ, p3
θ

Ldesc(pk

θ , pl

θ) =

(cid:40)(cid:13)

(cid:1) − hρ
(cid:0)pk
(cid:13)hρ
max (cid:0)0, C − (cid:13)

(cid:0)pl
(cid:13)hρ

θ

θ

(cid:1)(cid:13)
(cid:13)2
(cid:1) − hρ
(cid:0)pk

θ

(cid:0)pl

θ

(cid:1)(cid:13)
(cid:13)2

(cid:1)

for positive pairs, and
for negative pairs ,

(2)
where positive and negative samples are pairs of patches that do or do not
correspond to the same physical 3D points, (cid:107)·(cid:107)2 is the Euclidean distance, and
C = 4 is the margin for embedding.

We use hard mining during training, which was shown in [10] to be critical for
descriptor performance. Following this methodology, we forward Kf sample pairs
and use only the Kb pairs with the highest training loss for back-propagation,
where r = Kf /Kb ≥ 1 is the ‘mining ratio’. In [10] the network was pre-trained
without mining and then ﬁne-tuned with r = 8. Here, we use an increasing
mining scheme where we start with r = 1 and double the mining ratio every
5000 batches. We use balanced batches with 128 positive pairs and 128 negative
pairs, mining each separately.

3.4 Orientation Estimator

Our Orientation Estimator is inspired by that of [9]. However, this speciﬁc one
requires pre-computations of description vectors for multiple orientations to com-
pute numerically the Jacobian of the method parameters with respect to orien-
tations. This is a critical limitation for us because we treat the output of the
detector component implicitly throughout the pipeline and it is thus not possible
to pre-compute the description vectors.

We therefore propose to use Spatial Transformers [11] instead to learn the
orientations. Given a patch p from the region proposed by the detector, the
Orientation Estimator predicts an orientation

θ = gφ(p) ,

(3)

where g denotes the Orientation Estimator CNN, and φ its parameters.

Together with the location x from the Detector and P the original image
patch, θ is then used by the second Spatial Transformer Layer Rot(.) to provide
a patch pθ = Rot (P, x, θ), which is the rotated version of patch p.

We train the Orientation Estimator to provide the orientations that minimize
the distances between description vectors for diﬀerent views of the same 3D
points. We use the already trained Descriptor to compute the description vectors,

8

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

and as the Detector is still not trained, we use the image locations from SfM.
More formally, we minimize the loss for pairs of corresponding patches, deﬁned
as the Euclidean distance between their description vectors

Lorientation(P1, x1, P2, x2) = (cid:13)

(cid:13)hρ(G(P1, x1)) − hρ(G(P2, x2))(cid:13)

(cid:13)2 ,

(4)

where G(P, x) is the patch centered on x after orientation correction: G(P, x) =
Rot(P, x, gφ(Crop(P, x))). This complex notation is necessary to properly han-
dle the cropping of the image patches. Recall that pairs (P1, P2) comprise image
patches containing the projections of the same 3D point, and locations x1 and
x2 denote the reprojections of these 3D points. As in [9], we do not use pairs
that correspond to diﬀerent physical points whose orientations are not related.

3.5 Detector

The Detector takes an image patch as input, and returns a score map. We imple-
ment it as a convolution layer followed by piecewise linear activation functions,
as in TILDE [6]. More precisely, the score map S for patch P is computed as:

S = fµ(P) =

(Wmn ∗ P + bmn) ,

(5)

N
(cid:88)

n

δn

M
max
m

where fµ(P) denotes the Detector itself with parameters µ, δn is +1 if n is odd
and −1 otherwise, µ is made of the ﬁlters Wmn and biases bmn of the convolution
layer to learn, ∗ denotes the convolution operation, and N and M are hyper-
parameters controlling the complexity of the piecewise linear activation function.
The main diﬀerence with TILDE lies in the way we train this layer. To let
S have maxima in places other than a ﬁxed location retrieved by SfM, we treat
this location implicitly, as a latent variable. Our method can potentially discover
points that are more reliable and easier to learn, whereas [6] cannot. Incidentally,
in our early experiments, we noticed that it was harmful to force the Detector
to optimize directly for SfM locations.

From the score map S, we obtain the location x of a feature point as

x = softargmax (S) ,

where softargmax is a function which computes the Center of Mass with the
weights being the output of a standard softmax function [12]. We write

softargmax (S) =

(cid:80)

y exp(βS(y))y
y exp(βS(y))

(cid:80)

,

(6)

(7)

where y are locations in S, and β = 10 is a hyper-parameter controlling the
smoothness of the softargmax. This softargmax function acts as a diﬀerentiable
version of non-maximum suppression. x is given to the ﬁrst Spatial Trans-
former Layer Crop(.) together with the patch P to extract a smaller patch
p = Crop (P, x) used as input to the Orientation Estimator.

LIFT: Learned Invariant Feature Transform

9

As the Orientation Estimator and the Descriptor have been learned by this
point, we can train the Detector given the full pipeline. To optimize over the
parameters µ, we minimize the distances between description vectors for the
pairs of patches that correspond to the same physical points, while maximizing
the classiﬁcation score for patches not corresponding to the same physical points.
More exactly, given training quadruplets (cid:0)P1, P2, P3, P4(cid:1), where P1 and P2
correspond to the same physical point, P1 and P3 correspond to diﬀerent SfM
points, and P4 to a non-feature point location, we minimize the sum of their
loss functions

Ldetector(P1, P2, P3, P4) = γLclass(P1, P2, P3, P4) + Lpair(P1, P2) ,

(8)

where γ is a hyper-parameter balancing the two terms in this summation

Lclass(P1, P2, P3, P4) =

αi max (cid:0)0, (cid:0)1 − softmax (cid:0)fµ

(cid:0)Pi(cid:1)(cid:1) yi

(cid:1)(cid:1)2

,

(9)

4
(cid:88)

i=1

with yi = −1 and αi = 3/6 if i = 4, and yi = +1 and αi = 1/6 otherwise to
balance the positives and negatives. softmax is the log-mean-exponential softmax
function. We write

Lpair(P1, P2) = (cid:107) hρ(G(P1, softargmax(fµ(P1)))) −

hρ(G(P2, softargmax(fµ(P2))))

(cid:107)2 .

(10)

Note that the locations of the detected feature points x appear only implicitly
and are discovered during training. Furthermore, all three components are tied
in with the Detector learning. As with the Descriptor we use a hard mining
strategy, in this case with a ﬁxed mining ratio of r = 4.

In practice, as the Descriptor already learns some invariance, it can be hard
for the Detector to ﬁnd new points to learn implicitly. To let the Detector start
with an idea of the regions it should ﬁnd, we ﬁrst constrain the patch proposals
p = Crop(P, softargmax(fµ(P))) that correspond to the same physical points
to overlap. We then continue training the Detector without this constraint.

Speciﬁcally, when pre-training the Detector, we replace Lpair in Eq. (8) with
˜Lpair, where ˜Lpair is equal to 0 when the patch proposals overlap exactly, and
increases with the distance between them otherwise. We therefore write

˜Lpair(P1, P2) = 1 −

p1 ∩ p2
p1 ∪ p2 +

max (cid:0)0, (cid:13)

(cid:13)x1 − x2(cid:13)
(cid:112)p1 ∪ p2

(cid:13)1 − 2s(cid:1)

,

(11)

where xj = softargmax(fµ(Pj)), pj = Crop(Pj, xj), (cid:107)·(cid:107)1 is the l1 norm. Recall
that s = 64 pixels is the width and height of the patch proposals.

3.6 Runtime pipeline

The pipeline used at run-time is shown in Fig. 4. As our method is trained on
patches, simply applying it over the image would require the network to be tested

10

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Fig. 4. An overview of our runtime architecture. As the Orientation Estimator and the
Descriptor only require evaluation at local maxima, we decouple the Detector and run it
in scale space with traditional NMS to obtain proposals for the two other components.

with a sliding window scheme over the whole image. In practice, this would be
too expensive. Fortunately, as the Orientation Estimator and the Descriptor only
need to be run at local maxima, we can simply decouple the detector from the
rest to apply it to the full image, and replace the softargmax function by NMS,
as outlined in red in Fig. 4. We then apply the Orientation Estimator and the
Descriptor only to the patches centered on local maxima.

More exactly, we apply the Detector independently to the image at diﬀerent
resolutions to obtain score maps in scale space. We then apply a traditional NMS
scheme similar to that of [1] to detect feature point locations.

4 Experimental validation

In this section, we ﬁrst present the datasets and metrics we used. We then present
qualitative results, followed by a thorough quantitative comparison against a
number of state-of-the-art baselines, which we consistently outperform.

Finally, to better understand what elements of our approach most contribute
to this result, we study the importance of the pre-training of the Detector com-
ponent, discussed in Section 3.5, and analyze the performance gains attributable
to each component.

4.1 Dataset and Experimental Setup

We evaluate our pipeline on three standard datasets:

– The Strecha dataset [33], which contains 19 images of two scenes seen from

increasingly diﬀerent viewpoints.

– The DTU dataset [34], which contains 60 sequences of objects with diﬀerent
viewpoints and illumination settings. We use this dataset to evaluate our
method under viewpoint changes.

– The Webcam dataset [6], which contains 710 images of 6 scenes with strong
illumination changes but seen from the same viewpoint. We use this dataset
to evaluate our method under natural illumination changes.

LIFT: Learned Invariant Feature Transform

11

For Strecha and DTU we use the provided ground truth to establish corre-
spondences across viewpoints. We use a maximum of 1000 keypoints per image,
and follow the standard evaluation protocol of [35] on the common viewpoint
region. This lets us evaluate the following metrics.

– Repeatability (Rep.): Repeatability of feature points, expressed as a ratio.
This metric captures the performance of the feature point detector by report-
ing the ratio of keypoints that are found consistently in the shared region.
– Nearest Neighbor mean Average Precision (NN mAP): Area Under Curve
(AUC) of the Precision-Recall curve, using the Nearest Neighbor match-
ing strategy. This metric captures how discriminating the descriptor is by
evaluating it at multiple descriptor distance thresholds.

– Matching Score (M. Score): The ratio of ground truth correspondences that
can be recovered by the whole pipeline over the number of features proposed
by the pipeline in the shared viewpoint region. This metric measures the
overall performance of the pipeline.

We compare our method on the three datasets to the following combination of
feature point detectors and descriptors, as reported by the authors of the corre-
sponding papers: SIFT [1], SURF [2], KAZE [36], ORB [4], Daisy [37] with SIFT
detector, sGLOH [38] with Harris-aﬃne detector [39], MROGH [40] with Harris-
aﬃne detector, LIOP [41] with Harris-aﬃne detector, BiCE [42] with Edge Foci
detector [19], BRISK [43], FREAK [44] with BRISK detector, VGG [26] with
SIFT detector, DeepDesc [10] with SIFT detector, PN-Net [28] with SIFT detec-
tor, and MatchNet [7] with SIFT detector. We also consider SIFT with Hessian-
Aﬃne keypoints [17]. For the learned descriptors VGG, DeepDesc, PN-Net and
MatchNet we use SIFT keypoints because they are trained using a dataset cre-
ated with Diﬀerence-of-Gaussians, which is essentially the same as SIFT. In the
case of Daisy, which was not developed for a speciﬁc detector, we also use SIFT
keypoints. To make our results reproducible, we provide additional implementa-
tion details for LIFT and the baselines in the supplementary material.2

4.2 Qualitative Examples

Fig. 5 shows image matching results with 500 feature points, for both SIFT
and our LIFT pipeline trained with Piccadilly. As expected, LIFT returns more
correct correspondences across the two images. One thing to note is that the two
DTU scenes in the bottom two rows are completely diﬀerent from the photo-
tourism datasets we used for training. Given that the two datasets are very
diﬀerent, this shows good generalization properties.

4.3 Quantitative Evaluation of the Full Pipeline

Fig. 6 shows the average matching score for all three datasets, and Table 1
provides the exact numbers for the two LIFT variants. LIFT (pic) is trained

2 Source and models will be available at https://github.com/cvlab-epfl/LIFT.

12

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Fig. 5. Qualitative local feature matching examples of left: SIFT and right: our
method LIFT. Correct matches recovered by each method are shown in green lines and
the descriptor support regions with red circles. Top row: Herz-Jesu-P8 of Strecha,
second row: Frankfurt of Webcam, third row: Scene 7 of DTU and bottom row:
Scene 19 of DTU. Note that the images are very diﬀerent from one another.

with Piccadilly and LIFT (rf) with Roman-Forum. Both of our learned models
signiﬁcantly outperform the state-of-the-art on Strecha and DTU and achieve
state-of-the-art on Webcam. Note that KAZE, which is the best performing
competitor on Webcam, performs poorly on the other two datasets. As discussed
above, Piccadilly and Roman-Forum are very diﬀerent from the datasets used
for testing. This underlines the strong generalization capability of our approach,
which is not always in evidence with learning-based methods.

Interestingly, on DTU, SIFT is still the best performing method among the
competitors, even compared to methods that rely on Deep Learning, such as
DeepDesc and PN-Net. Also, the gap between SIFT and the learning-based
VGG, DeepDesc, and PN-Net is not large for the Strecha dataset.

These results show that although a component may outperform another
method when evaluated individually, they may fail to deliver their full potential
when integrated into the full pipeline, which is what really matters. In other
words, it is important to learn the components together, as we do, and to con-
sider the whole pipeline when evaluating feature point detectors and descriptors.

4.4 Performance of Individual Components

Fine-tuning the Detector. Recall that we pre-train the detector and then
ﬁnalize the training with the Orientation Estimator and the Descriptor, as dis-

LIFT: Learned Invariant Feature Transform

13

Table 1. Average matching score for all baselines.

SIFT SIFT-HesAﬀ SURF

ORB

Daisy

sGLOH MROGH LIOP

BiCE

Strecha
DTU

.283
.272
Webcam .128

Strecha
DTU

.208
.193
Webcam .118

BRISK FREAK

VGG MatchNet DeepDesc PN-Net KAZE LIFT-pic LIFT-rf

.314
.274
.164

.183
.186
.116

.208
.244
.117

.300
.271
.118

.157
.127
.120

.223
.198
.101

.272
.262
.120

.298
.257
.116

.207
.187
.113

.300
.267
.114

.239
.223
.125

.250
.213
.195

.211
.189
.086

.270
.242
.166

.374
.317
.196

.369
.308
.202

Fig. 6. Average matching score for all baselines.

cussed in Section 3.5. It is therefore interesting to see the eﬀect of this ﬁnalizing
stage. In Table 2 we evaluate the entire pipeline with the pre-trained Detector
and the ﬁnal Detector. As the pair-wise loss term ˜Lpair of Eq. (11) is designed
to emulate the behavior of an ideal descriptor, the pre-trained Detector already
performs well. However, the full training pushes the performance slightly higher.
A closer look at Table 2 reveals that gains are larger overall for Piccadilly
than for Roman-Forum. This is probably due to the fact that Roman-Forum
does not have many non-feature point regions. In fact, the network started to
over-ﬁt quickly after a few iterations on this dataset. The same happened when
we further tried to ﬁne-tune the full pipeline as a whole, suggesting that our
learning strategy is already providing a good global solution.

Performance of individual components. To understand the inﬂuence of
each component on the overall performance, we exchange them with their SIFT
counterparts, for both LIFT (pic) and LIFT (rf), on Strecha. We report the
results in Table 3. In short, each time we exchange to SIFT, we decrease per-
formance, thus showing that each element of the pipeline plays and important
role. Our Detector gives higher repeatability for both models. Having better ori-
entations also helps whichever detector or descriptor is being used, and also the
Deep Descriptors perform better than SIFT.

One thing to note is that our Detector is not only better in terms of repeata-
bility, but generally better in terms of both the NN mAP, which captures the

14

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Table 2. Results on Strecha for both LIFT models trained on Piccadilly and Roman-
Forum, with the pre-trained and fully-trained Detector.

Trained on Piccadilly

Trained on Roman-Forum

Pre-trained
Fully-trained

Rep.
.436
.446

M.Score
.367
.374

Rep.
.447
.447

M.Score
.368
.369

Table 3. Results on Strecha for both LIFT models trained on Piccadilly and Roman-
Forum, interchanging our components with their SIFT counterparts.

Det.

Ori.

Desc. Rep. NN mAP M.Score Rep. NN mAP M.Score

Trained on Piccadilly

Trained on Roman-Forum

.428

.428

SIFT SIFT SIFT

SIFT Ours

SIFT

SIFT SIFT Ours

SIFT Ours Ours

Ours

SIFT SIFT

Ours Ours

SIFT

.446

Ours

SIFT Ours

.517

.671

.568

.685

.540

.644

.629

.282

.341

.290

.344

.325

.372

.339

.447

.517

.662

.581

.688

.545

.630

.644

.282

.338

.295

.342

.319

.360

.337

Ours Ours Ours

.446

.686

.374

.447

.683

.369

descriptor performance, and in terms of matching score, which evaluates the full
pipeline. This shows that our Detector learns to ﬁnd not only points that can be
found often but also points that can be matched easily, indicating that training
the pipeline as a whole is important for optimal performance.

5 Conclusion

We have introduced a novel Deep Network architecture that combines the three
components of standard pipelines for local feature detection and description
into a single diﬀerentiable network. We used Spatial Transformers together with
the softargmax function to mesh them together into a uniﬁed network that
can be trained end-to-end with back-propagation. While this makes learning
the network from scratch theoretically possible, it is not practical. We therefore
proposed an eﬀective strategy to train it.

Our experimental results demonstrate that our integrated approach outper-
forms the state-of-the-art. To further improve performance, we will look into
strategies that allow us to take advantage even more eﬀectively of our ability to
train the network as a whole. In particular, we will look into using hard negative
mining strategies over the whole image [45] instead of relying on pre-extracted
patches. This has the potential of producing more discriminative ﬁlters and,
consequently, better descriptors.

LIFT: Learned Invariant Feature Transform

15

References

(2004)

1. Lowe, D.: Distinctive Image Features from Scale-Invariant Keypoints. IJCV 20(2)

2. Bay, H., Ess, A., Tuytelaars, T., Van Gool, L.: SURF: Speeded Up Robust Features.

3. Tola, E., Lepetit, V., Fua, P.: A Fast Local Descriptor for Dense Matching. In:

CVIU 10(3) (2008) 346–359

CVPR. (2008)

4. Rublee, E., Rabaud, V., Konolidge, K., Bradski, G.: ORB: An Eﬃcient Alternative

to SIFT or SURF. In: ICCV. (2011)

5. Mainali, P., Lafruit, G., Tack, K., Van Gool, L., Lauwereins, R.: Derivative-Based
Scale Invariant Image Feature Detector with Error Resilience. TIP 23(5) (2014)
2380–2391

6. Verdie, Y., Yi, K.M., Fua, P., Lepetit, V.: TILDE: A Temporally Invariant Learned

DEtector. In: CVPR. (2015)

7. Han, X., Leung, T., Jia, Y., Sukthankar, R., Berg, A.C.: MatchNet: Unifying

Feature and Metric Learning for Patch-Based Matching. In: CVPR. (2015)

8. Zagoruyko, S., Komodakis, N.: Learning to Compare Image Patches via Convolu-

tional Neural Networks. In: CVPR. (2015)

9. Yi, K., Verdie, Y., Lepetit, V., Fua, P.: Learning to Assign Orientations to Feature

Points. In: CVPR. (2016)

10. Simo-Serra, E., Trulls, E., Ferraz, L., Kokkinos, I., Fua, P., Moreno-Noguer, F.:
Discriminative Learning of Deep Convolutional Feature Point Descriptors.
In:
ICCV. (2015)

11. Jaderberg, M., Simonyan, K., Zisserman, A., Kavukcuoglu, K.: Spatial Transformer

Networks. In: NIPS. (2015)

12. Chapelle, O., Wu, M.: Gradient Descent Optimization of Smoothed Information

Retrieval Metrics. Information Retrieval 13(3) (2009) 216–235

13. Harris, C., Stephens, M.: A Combined Corner and Edge Detector.

In: Fourth

Alvey Vision Conference. (1988)

14. Moravec, H.: Obstacle Avoidance and Navigation in the Real World by a Seeing
In: tech. report CMU-RI-TR-80-03, Robotics Institute, Carnegie

Robot Rover.
Mellon University, Stanford University. (September 1980)

15. Rosten, E., Drummond, T.: Machine Learning for High-Speed Corner Detection.

In: ECCV. (2006)

16. Matas, J., Chum, O., Martin, U., Pajdla, T.: Robust Wide Baseline Stereo from
Maximally Stable Extremal Regions. In: BMVC. (September 2002) 384–393
17. Mikolajczyk, K., Schmid, C.: An Aﬃne Invariant Interest Point Detector.

In:

ECCV. (2002) 128–142

18. F¨orstner, W., Dickscheid, T., Schindler, F.: Detecting Interpretable and Accurate

Scale-Invariant Keypoints. In: ICCV. (September 2009)

19. Zitnick, C., Ramnath, K.: Edge Foci Interest Points. In: ICCV. (2011)
20. Mainali, P., Lafruit, G., Yang, Q., Geelen, B., Van Gool, L., Lauwereins, R.: SIFER:
Scale-Invariant Feature Detector with Error Resilience. IJCV 104(2) (2013) 172–
197

21. Sochman, J., Matas, J.: Learning a Fast Emulator of a Binary Decision Process.

22. Trujillo, L., Olague, G.: Using Evolution to Learn How to Perform Interest Point

In: ACCV. (2007) 236–245

Detection. In: ICPR. (2006) 211–214

16

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

23. Strecha, C., Bronstein, A., Bronstein, M., Fua, P.: LDAHash: Improved Matching

with Smaller Descriptors. PAMI 34(1) (January 2012)

24. Winder, S., Brown, M.: Learning Local Image Descriptors. In: CVPR. (June 2007)
25. Perez, C., Olague, G.: Genetic Programming As Strategy for Learning Image

Descriptor Operators. Intelligent Data Analysis 17 (2013) 561–583

26. Simonyan, K., Vedaldi, A., Zisserman, A.: Learning Local Feature Descriptors

Using Convex Optimisation. PAMI (2014)

27. Zbontar, J., LeCun, Y.: Computing the Stereo Matching Cost with a Convolutional

Neural Network. In: CVPR. (2015)

28. Balntas, V., Johns, E., Tang, L., Mikolajczyk, K.: PN-Net: Conjoined Triple Deep

Network for Learning Local Image Descriptors. In: arXiv Preprint. (2016)

29. Wilson, K., Snavely, N.: Robust Global Translations with 1DSfM.

In: ECCV.

(2014)

30. Wu, C.: Towards Linear-Time Incremental Structure from Motion. In: 3DV. (2013)
31. Paulin, M., Douze, M., Harchaoui, Z., Mairal, J., Perronnin, F., Schmid, C.: Local
Convolutional Features with Unsupervised Training for Image Retrieval. In: ICCV.
(2015)

32. Sermanet, P., Chintala, S., LeCun, Y.: Convolutional Neural Networks Applied to

House Numbers Digit Classiﬁcation. In: ICPR. (2012)

33. Strecha, C., Hansen, W., Van Gool, L., Fua, P., Thoennessen, U.: On Benchmark-
ing Camera Calibration and Multi-View Stereo for High Resolution Imagery. In:
CVPR. (2008)

34. Aanaes, H., Dahl, A.L., Pedersen, K.S.: Interesting Interest Points. IJCV 97 (2012)

35. Mikolajczyk, K., Schmid, C.: A Performance Evaluation of Local Descriptors. In:

36. Alcantarilla, P., Fern´andez, P., Bartoli, A., Davidson, A.J.: KAZE Features. In:

18–35

CVPR. (June 2003) 257–263

ECCV. (2012)

37. Tola, E., Lepetit, V., Fua, P.: Daisy: An Eﬃcient Dense Descriptor Applied to

Wide Baseline Stereo. PAMI 32(5) (2010) 815–830

38. Bellavia, F., Tegolo, D.: Improving Sift-Based Descriptors Stability to Rotations.

39. Mikolajczyk, K., Schmid, C.: Scale and Aﬃne Invariant Interest Point Detectors.

In: ICPR. (2010)

IJCV 60 (2004) 63–86

40. Fan, B., Wu, F., Hu, Z.: Aggregating Gradient Distributions into Intensity Orders:

A Novel Local Image Descriptor. In: CVPR. (2011)

41. Wang, Z., Fan, B., Wu, F.: Local Intensity Order Pattern for Feature Description.

In: ICCV. (2011)

42. Zitnick, C.: Binary Coherent Edge Descriptors. In: ECCV. (2010)
43. Leutenegger, S., Chli, M., Siegwart, R.: BRISK: Binary Robust Invariant Scalable

44. Alahi, A., Ortiz, R., Vandergheynst, P.: FREAK: Fast Retina Keypoint. In: CVPR.

Keypoints. In: ICCV. (2011)

(2012)

45. Felzenszwalb, P., Girshick, R., McAllester, D., Ramanan, D.: Object Detection

with Discriminatively Trained Part Based Models. PAMI (2010)

6
1
0
2
 
l
u
J
 
9
2
 
 
]

V
C
.
s
c
[
 
 
2
v
4
1
1
9
0
.
3
0
6
1
:
v
i
X
r
a

LIFT: Learned Invariant Feature Transform

Kwang Moo Yi∗,1, Eduard Trulls∗,1, Vincent Lepetit2, Pascal Fua1

1Computer Vision Laboratory, Ecole Polytechnique F´ed´erale de Lausanne (EPFL)
2Institute for Computer Graphics and Vision, Graz University of Technology
{kwang.yi, eduard.trulls, pascal.fua}@epfl.ch, lepetit@icg.tugraz.at

Abstract. We introduce a novel Deep Network architecture that imple-
ments the full feature point handling pipeline, that is, detection, orienta-
tion estimation, and feature description. While previous works have suc-
cessfully tackled each one of these problems individually, we show how to
learn to do all three in a uniﬁed manner while preserving end-to-end dif-
ferentiability. We then demonstrate that our Deep pipeline outperforms
state-of-the-art methods on a number of benchmark datasets, without
the need of retraining.

Keywords: Local Features, Feature Descriptors, Deep Learning

1 Introduction

Local features play a key role in many Computer Vision applications. Find-
ing and matching them across images has been the subject of vast amounts of
research. Until recently, the best techniques relied on carefully hand-crafted fea-
tures [1,2,3,4,5]. Over the past few years, as in many areas of Computer Vision,
methods based in Machine Learning, and more speciﬁcally Deep Learning, have
started to outperform these traditional methods [6,7,8,9,10].

These new algorithms, however, address only a single step in the complete
processing chain, which includes detecting the features, computing their orienta-
tion, and extracting robust representations that allow us to match them across
images. In this paper we introduce a novel Deep architecture that performs all
three steps together. We demonstrate that it achieves better overall performance
than the state-of-the-art methods, in large part because it allows these individual
steps to be optimized to perform well in conjunction with each other.

Our architecture, which we refer to as LIFT for Learned Invariant Feature
Transform, is depicted by Fig. 1. It consists of three components that feed into
each other: the Detector, the Orientation Estimator, and the Descriptor. Each
one is based on Convolutional Neural Networks (CNNs), and patterned after
recent ones [6,9,10] that have been shown to perform these individual functions
well. To mesh them together we use Spatial Transformers [11] to rectify the

∗ First two authors contributed equally.
This work was supported in part by the EU FP7 project MAGELLAN under grant
number ICT-FP7-611526.

2

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Fig. 1. Our integrated feature extraction pipeline. Our pipeline consists of three major
components: the Detector, the Orientation Estimator, and the Descriptor. They are
tied together with diﬀerentiable operations to preserve end-to-end diﬀerentiability.1

image patches given the output of the Detector and the Orientation Estimator.
We also replace the traditional approaches to non-local maximum suppression
(NMS) by the soft argmax function [12]. This allows us to preserve end-to-end
diﬀerentiability, and results in a full network that can still be trained with back-
propagation, which is not the case of any other architecture we know of.

Also, we show how to learn such a pipeline in an eﬀective manner. To this
end, we build a Siamese network and train it using the feature points produced
by a Structure-from-Motion (SfM) algorithm that we ran on images of a scene
captured under diﬀerent viewpoints and lighting conditions, to learn its weights.
We formulate this training problem on image patches extracted at diﬀerent scales
to make the optimization tractable. In practice, we found it impossible to train
the full architecture from scratch, because the individual components try to op-
timize for diﬀerent objectives. Instead, we introduce a problem-speciﬁc learning
approach to overcome this problem. It involves training the Descriptor ﬁrst,
which is then used to train the Orientation Estimator, and ﬁnally the Detector,
based on the already learned Descriptor and Orientation Estimator, diﬀerenti-
ating through the entire network. At test time, we decouple the Detector, which
runs over the whole image in scale space, from the Orientation Estimator and
Descriptor, which process only the keypoints.

In the next section we brieﬂy discuss earlier approaches. We then present our
approach in detail and show that it outperforms many state-of-the-art methods.

2 Related work

The amount of literature relating to local features is immense, but it always
revolves about ﬁnding feature points, computing their orientation, and matching
them. In this section, we will therefore discuss these three elements separately.

2.1 Feature Point Detectors

Research on feature point detection has focused mostly on ﬁnding distinctive lo-
cations whose scale and rotation can be reliably estimated. Early works [13,14]

1 Figures are best viewed in color.

LIFT: Learned Invariant Feature Transform

3

used ﬁrst-order approximations of the image signal to ﬁnd corner points in im-
ages. FAST [15] used Machine Learning techniques but only to speed up the
process of ﬁnding corners. Other than corner points, SIFT [1] detect blobs in
scale-space; SURF [2] use Haar ﬁlters to speed up the process; Maximally Sta-
ble Extremal Regions (MSER) [16] detect regions; [17] detect aﬃne regions.
SFOP [18] use junctions and blobs, and Edge Foci [19] use edges for robustness
to illumination changes. More recently, feature points based on more sophisti-
cated and carefully designed ﬁlter responses [5,20] have also been proposed to
further enhance the performance of feature point detectors.

In contrast to these approaches that focus on better engineering, and follow-
ing the early attempts in learning detectors [21,22], [6] showed that a detector
could be learned to deliver signiﬁcantly better performance than the state-of-
the-art. In this work, piecewise-linear convolutional ﬁlters are learned to robustly
detect feature points in spite of lighting and seasonal changes. Unfortunately, this
was done only for a single scale and from a dataset without viewpoint changes.
We therefore took our inspiration from it but had to extend it substantially to
incorporate it into our pipeline.

2.2 Orientation Estimation

Despite the fact that it plays a critical role in matching feature points, the
problem of estimating a discriminative orientation has received noticeably less
attention than detection or feature description. As a result, the method intro-
duced by SIFT [1] remains the de facto standard up to small improvements, such
as the fact that it can be sped-up by using the intensity centroid, as in ORB [4].
A departure from this can be found in a recent paper [9] that introduced a
Deep Learning-based approach to predicting stable orientations. This resulted
in signiﬁcant gains over the state-of-the-art. We incorporate this architecture
into our pipeline and show how to train it using our problem-speciﬁc training
strategy, given our learned descriptors.

2.3 Feature Descriptors

Feature descriptors are designed to provide discriminative representations of
salient image patches, while being robust to transformations such as viewpoint
or illumination changes. The ﬁeld reached maturity with the introduction of
SIFT [1], which is computed from local histograms of gradient orientations, and
SURF [2], which uses integral image representations to speed up the computa-
tion. Along similar lines, DAISY [3] relies on convolved maps of oriented gra-
dients to approximate the histograms, which yields large computational gains
when extracting dense descriptors.

Even though they have been extremely successful, these hand-crafted de-
scriptors can now be outperformed by newer ones that have been learned. These
range from unsupervised hashing to supervised learning techniques based on
linear discriminant analysis [23,24], genetic algorithm [25], and convex optimiza-
tion [26]. An even more recent trend is to extract features directly from raw image

4

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Fig. 2. Our Siamese training architecture with four branches, which takes as input
a quadruplet of patches: Patches P1 and P2 (blue) correspond to diﬀerent views of
the same physical point, and are used as positive examples to train the Descriptor;
P3 (green) shows a diﬀerent 3D point, which serves as a negative example for the
Descriptor; and P4 (red) contains no distinctive feature points and is only used as a
negative example to train the Detector. Given a patch P, the Detector, the softargmax,
and the Spatial Transformer layer Crop provide all together a smaller patch p inside P.
p is then fed to the Orientation Estimator, which along with the Spatial Transformer
layer Rot, provides the rotated patch pθ that is processed by the Descriptor to obtain
the ﬁnal description vector d.

patches with CNNs trained on large volumes of data. For example, MatchNet [7]
trained a Siamese CNN for feature representation, followed by a fully-connected
network to learn the comparison metric. DeepCompare [8] showed that a net-
work that focuses on the center of the image can increase performance. The
approach of [27] relied on a similar architecture to obtain state-of-the-art results
for narrow-baseline stereo. In [10], hard negative mining was used to learn com-
pact descriptors that use on the Euclidean distance to measure similarity. The
algorithm of [28] relied on sample triplets to mine hard negatives.

In this work, we rely on the architecture of [10] because the corresponding
descriptors are trained and compared with the Euclidean distance, which has a
wider range of applicability than descriptors that require a learned metric.

3 Method

In this section, we ﬁrst formulate the entire feature detection and description
pipeline in terms of the Siamese architecture depicted by Fig. 2. Next, we discuss
the type of data we need to train our networks and how to collect it. We then
describe the training procedure in detail.

3.1 Problem formulation

We use image patches as input, rather than full images. This makes the learning
scalable without loss of information, as most image regions do not contain key-
points. The patches are extracted from the keypoints used by a SfM pipeline, as
will be discussed in Section 3.2. We take them to be small enough that we can

LIFT: Learned Invariant Feature Transform

5

assume they contain only one dominant local feature at the given scale, which
reduces the learning process to ﬁnding the most distinctive point in the patch.
To train our network we create the four-branch Siamese architecture pictured
in Fig. 2. Each branch contains three distinct CNNs, a Detector, an Orientation
Estimator, and a Descriptor. For training purposes, we use quadruplets of image
patches. Each one includes two image patches P1 and P2, that correspond to
diﬀerent views of the same 3D point, one image patch P3, that contains the pro-
jection of a diﬀerent 3D point, and one image patch P4 that does not contain any
distinctive feature point. During training, the i-th patch Pi of each quadruplet
will go through the i-th branch.

To achieve end-to-end diﬀerentiability, the components of each branch are

connected as follows:

1. Given an input image patch P, the Detector provides a score map S.
2. We perform a soft argmax [12] on the score map S and return the location

x of a single potential feature point.

3. We extract a smaller patch p centered on x with the Spatial Transformer
layer Crop (Fig. 2). This serves as the input to the Orientation Estimator.

4. The Orientation Estimator predicts a patch orientation θ.
5. We rotate p according to this orientation using a second Spatial Transformer

layer, labeled as Rot in Fig. 2, to produce pθ.

6. pθ is fed to the Descriptor network, which computes a feature vector d.

Note that the Spatial Transformer layers are used only to manipulate the
image patches while preserving diﬀerentiability. They are not learned modules.
Also, both the location x proposed by the Detector and the orientation θ for
the patch proposal are treated implicitly, meaning that we let the entire network
discover distinctive locations and stable orientations while learning.

Since our network consists of components with diﬀerent purposes, learning
the weights is non-trivial. Our early attempts at training the network as a whole
from scratch were unsuccessful. We therefore designed a problem-speciﬁc learn-
ing approach that involves learning ﬁrst the Descriptor, then the Orientation
Estimator given the learned descriptor, and ﬁnally the Detector, conditioned on
the other two. This allows us to tune the Orientation Estimator for the Descrip-
tor, and the Detector for the other two components.

We will elaborate on this learning strategy in Secs. 3.3 (Descriptor), 3.4 (Ori-
entation Estimator), and 3.5 (Detector), that is, in the order they are learned.

3.2 Creating the Training Dataset

There are datasets that can be used to train feature descriptors [24] and orien-
tation estimators [9]. However it is not so clear how to train a keypoint detec-
tor, and the vast majority of techniques still rely on hand-crafted features. The
TILDE detector [6] is an exception, but the training dataset does not exhibit
any viewpoint changes.

To achieve invariance we need images that capture views of the same scene
under diﬀerent illumination conditions and seen from diﬀerent perspectives. We

6

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Fig. 3. Sample images and patches from Piccadilly (left) and Roman-Forum (right).
Keypoints that survive the SfM pipeline are drawn in blue, and the rest in red.

thus turned to photo-tourism image sets. We used the collections from Piccadilly
Circus in London and the Roman Forum in Rome from [29] to reconstruct the
3D using VisualSFM [30], which relies of SIFT features. Piccadilly contains 3384
images, and the reconstruction has 59k unique points with an average of 6.5 ob-
servations for each. Roman-Forum contains 1658 images and 51k unique points,
with an average of 5.2 observations for each. Fig. 3 shows some examples.

We split the data into training and validation sets, discarding views of train-
ing points on the validation set and vice-versa. To build the positive training
samples we consider only the feature points that survive the SfM reconstruction
process. To extract patches that do not contain any distinctive feature point,
as required by our training method, we randomly sample image regions that
contain no SIFT features, including those that were not used by SfM.

We extract grayscale training patches according to the scale σ of the point,
for both feature and non-feature point image regions. Patches P are extracted
from a 24σ × 24σ support region at these locations, and standardized into S × S
pixels where S = 128. The smaller patches p and pθ that serve as input to the
Orientation Estimator and the Descriptor, are cropped and rotated versions of
these patches, each having size s×s, where s = 64. The smaller patches eﬀectively
correspond to the SIFT descriptor support region size of 12σ. To avoid biasing
the data, we apply uniform random perturbations to the patch location with a
range of 20% (4.8σ). Finally, we normalize the patches with the grayscale mean
and standard deviation of the entire training set.

3.3 Descriptor

Learning feature descriptors from raw image patches has been extensively re-
searched during the past year [7,8,10,27,28,31], with multiple works reporting
impressive results on patch retrieval, narrow baseline stereo, and matching non-
rigid deformations. Here we rely on the relatively simple networks of [10], with
three convolutional layers followed by hyperbolic tangent units, l2 pooling [32]
and local subtractive normalization, as they do not require learning a metric.

The Descriptor can be formalized simply as

d = hρ(pθ) ,

(1)

LIFT: Learned Invariant Feature Transform

7

where h(.) denotes the Descriptor CNN, ρ its parameters, and pθ is the rotated
patch from the Orientation Estimator. When training the Descriptor, we do not
yet have the Detector and the Orientation Estimator trained. We therefore use
the image locations and orientations of the feature points used by the SfM to
generate image patches pθ.

We train the Descriptor by minimizing the sum of the loss for pairs of cor-
(cid:1) and the loss for pairs of non-corresponding patches
(cid:1) is deﬁned as the hinge embedding loss of the

responding patches (cid:0)p1
(cid:0)p1
θ , pl
θ
Euclidean distance between their description vectors. We write

(cid:1). The loss for pair (cid:0)pk

θ, p2
θ

θ, p3
θ

Ldesc(pk

θ , pl

θ) =

(cid:40)(cid:13)

(cid:1) − hρ
(cid:0)pk
(cid:13)hρ
max (cid:0)0, C − (cid:13)

(cid:0)pl
(cid:13)hρ

θ

θ

(cid:1)(cid:13)
(cid:13)2
(cid:1) − hρ
(cid:0)pk

θ

(cid:0)pl

θ

(cid:1)(cid:13)
(cid:13)2

(cid:1)

for positive pairs, and
for negative pairs ,

(2)
where positive and negative samples are pairs of patches that do or do not
correspond to the same physical 3D points, (cid:107)·(cid:107)2 is the Euclidean distance, and
C = 4 is the margin for embedding.

We use hard mining during training, which was shown in [10] to be critical for
descriptor performance. Following this methodology, we forward Kf sample pairs
and use only the Kb pairs with the highest training loss for back-propagation,
where r = Kf /Kb ≥ 1 is the ‘mining ratio’. In [10] the network was pre-trained
without mining and then ﬁne-tuned with r = 8. Here, we use an increasing
mining scheme where we start with r = 1 and double the mining ratio every
5000 batches. We use balanced batches with 128 positive pairs and 128 negative
pairs, mining each separately.

3.4 Orientation Estimator

Our Orientation Estimator is inspired by that of [9]. However, this speciﬁc one
requires pre-computations of description vectors for multiple orientations to com-
pute numerically the Jacobian of the method parameters with respect to orien-
tations. This is a critical limitation for us because we treat the output of the
detector component implicitly throughout the pipeline and it is thus not possible
to pre-compute the description vectors.

We therefore propose to use Spatial Transformers [11] instead to learn the
orientations. Given a patch p from the region proposed by the detector, the
Orientation Estimator predicts an orientation

θ = gφ(p) ,

(3)

where g denotes the Orientation Estimator CNN, and φ its parameters.

Together with the location x from the Detector and P the original image
patch, θ is then used by the second Spatial Transformer Layer Rot(.) to provide
a patch pθ = Rot (P, x, θ), which is the rotated version of patch p.

We train the Orientation Estimator to provide the orientations that minimize
the distances between description vectors for diﬀerent views of the same 3D
points. We use the already trained Descriptor to compute the description vectors,

8

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

and as the Detector is still not trained, we use the image locations from SfM.
More formally, we minimize the loss for pairs of corresponding patches, deﬁned
as the Euclidean distance between their description vectors

Lorientation(P1, x1, P2, x2) = (cid:13)

(cid:13)hρ(G(P1, x1)) − hρ(G(P2, x2))(cid:13)

(cid:13)2 ,

(4)

where G(P, x) is the patch centered on x after orientation correction: G(P, x) =
Rot(P, x, gφ(Crop(P, x))). This complex notation is necessary to properly han-
dle the cropping of the image patches. Recall that pairs (P1, P2) comprise image
patches containing the projections of the same 3D point, and locations x1 and
x2 denote the reprojections of these 3D points. As in [9], we do not use pairs
that correspond to diﬀerent physical points whose orientations are not related.

3.5 Detector

The Detector takes an image patch as input, and returns a score map. We imple-
ment it as a convolution layer followed by piecewise linear activation functions,
as in TILDE [6]. More precisely, the score map S for patch P is computed as:

S = fµ(P) =

(Wmn ∗ P + bmn) ,

(5)

N
(cid:88)

n

δn

M
max
m

where fµ(P) denotes the Detector itself with parameters µ, δn is +1 if n is odd
and −1 otherwise, µ is made of the ﬁlters Wmn and biases bmn of the convolution
layer to learn, ∗ denotes the convolution operation, and N and M are hyper-
parameters controlling the complexity of the piecewise linear activation function.
The main diﬀerence with TILDE lies in the way we train this layer. To let
S have maxima in places other than a ﬁxed location retrieved by SfM, we treat
this location implicitly, as a latent variable. Our method can potentially discover
points that are more reliable and easier to learn, whereas [6] cannot. Incidentally,
in our early experiments, we noticed that it was harmful to force the Detector
to optimize directly for SfM locations.

From the score map S, we obtain the location x of a feature point as

x = softargmax (S) ,

where softargmax is a function which computes the Center of Mass with the
weights being the output of a standard softmax function [12]. We write

softargmax (S) =

(cid:80)

y exp(βS(y))y
y exp(βS(y))

(cid:80)

,

(6)

(7)

where y are locations in S, and β = 10 is a hyper-parameter controlling the
smoothness of the softargmax. This softargmax function acts as a diﬀerentiable
version of non-maximum suppression. x is given to the ﬁrst Spatial Trans-
former Layer Crop(.) together with the patch P to extract a smaller patch
p = Crop (P, x) used as input to the Orientation Estimator.

LIFT: Learned Invariant Feature Transform

9

As the Orientation Estimator and the Descriptor have been learned by this
point, we can train the Detector given the full pipeline. To optimize over the
parameters µ, we minimize the distances between description vectors for the
pairs of patches that correspond to the same physical points, while maximizing
the classiﬁcation score for patches not corresponding to the same physical points.
More exactly, given training quadruplets (cid:0)P1, P2, P3, P4(cid:1), where P1 and P2
correspond to the same physical point, P1 and P3 correspond to diﬀerent SfM
points, and P4 to a non-feature point location, we minimize the sum of their
loss functions

Ldetector(P1, P2, P3, P4) = γLclass(P1, P2, P3, P4) + Lpair(P1, P2) ,

(8)

where γ is a hyper-parameter balancing the two terms in this summation

Lclass(P1, P2, P3, P4) =

αi max (cid:0)0, (cid:0)1 − softmax (cid:0)fµ

(cid:0)Pi(cid:1)(cid:1) yi

(cid:1)(cid:1)2

,

(9)

4
(cid:88)

i=1

with yi = −1 and αi = 3/6 if i = 4, and yi = +1 and αi = 1/6 otherwise to
balance the positives and negatives. softmax is the log-mean-exponential softmax
function. We write

Lpair(P1, P2) = (cid:107) hρ(G(P1, softargmax(fµ(P1)))) −

hρ(G(P2, softargmax(fµ(P2))))

(cid:107)2 .

(10)

Note that the locations of the detected feature points x appear only implicitly
and are discovered during training. Furthermore, all three components are tied
in with the Detector learning. As with the Descriptor we use a hard mining
strategy, in this case with a ﬁxed mining ratio of r = 4.

In practice, as the Descriptor already learns some invariance, it can be hard
for the Detector to ﬁnd new points to learn implicitly. To let the Detector start
with an idea of the regions it should ﬁnd, we ﬁrst constrain the patch proposals
p = Crop(P, softargmax(fµ(P))) that correspond to the same physical points
to overlap. We then continue training the Detector without this constraint.

Speciﬁcally, when pre-training the Detector, we replace Lpair in Eq. (8) with
˜Lpair, where ˜Lpair is equal to 0 when the patch proposals overlap exactly, and
increases with the distance between them otherwise. We therefore write

˜Lpair(P1, P2) = 1 −

p1 ∩ p2
p1 ∪ p2 +

max (cid:0)0, (cid:13)

(cid:13)x1 − x2(cid:13)
(cid:112)p1 ∪ p2

(cid:13)1 − 2s(cid:1)

,

(11)

where xj = softargmax(fµ(Pj)), pj = Crop(Pj, xj), (cid:107)·(cid:107)1 is the l1 norm. Recall
that s = 64 pixels is the width and height of the patch proposals.

3.6 Runtime pipeline

The pipeline used at run-time is shown in Fig. 4. As our method is trained on
patches, simply applying it over the image would require the network to be tested

10

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Fig. 4. An overview of our runtime architecture. As the Orientation Estimator and the
Descriptor only require evaluation at local maxima, we decouple the Detector and run it
in scale space with traditional NMS to obtain proposals for the two other components.

with a sliding window scheme over the whole image. In practice, this would be
too expensive. Fortunately, as the Orientation Estimator and the Descriptor only
need to be run at local maxima, we can simply decouple the detector from the
rest to apply it to the full image, and replace the softargmax function by NMS,
as outlined in red in Fig. 4. We then apply the Orientation Estimator and the
Descriptor only to the patches centered on local maxima.

More exactly, we apply the Detector independently to the image at diﬀerent
resolutions to obtain score maps in scale space. We then apply a traditional NMS
scheme similar to that of [1] to detect feature point locations.

4 Experimental validation

In this section, we ﬁrst present the datasets and metrics we used. We then present
qualitative results, followed by a thorough quantitative comparison against a
number of state-of-the-art baselines, which we consistently outperform.

Finally, to better understand what elements of our approach most contribute
to this result, we study the importance of the pre-training of the Detector com-
ponent, discussed in Section 3.5, and analyze the performance gains attributable
to each component.

4.1 Dataset and Experimental Setup

We evaluate our pipeline on three standard datasets:

– The Strecha dataset [33], which contains 19 images of two scenes seen from

increasingly diﬀerent viewpoints.

– The DTU dataset [34], which contains 60 sequences of objects with diﬀerent
viewpoints and illumination settings. We use this dataset to evaluate our
method under viewpoint changes.

– The Webcam dataset [6], which contains 710 images of 6 scenes with strong
illumination changes but seen from the same viewpoint. We use this dataset
to evaluate our method under natural illumination changes.

LIFT: Learned Invariant Feature Transform

11

For Strecha and DTU we use the provided ground truth to establish corre-
spondences across viewpoints. We use a maximum of 1000 keypoints per image,
and follow the standard evaluation protocol of [35] on the common viewpoint
region. This lets us evaluate the following metrics.

– Repeatability (Rep.): Repeatability of feature points, expressed as a ratio.
This metric captures the performance of the feature point detector by report-
ing the ratio of keypoints that are found consistently in the shared region.
– Nearest Neighbor mean Average Precision (NN mAP): Area Under Curve
(AUC) of the Precision-Recall curve, using the Nearest Neighbor match-
ing strategy. This metric captures how discriminating the descriptor is by
evaluating it at multiple descriptor distance thresholds.

– Matching Score (M. Score): The ratio of ground truth correspondences that
can be recovered by the whole pipeline over the number of features proposed
by the pipeline in the shared viewpoint region. This metric measures the
overall performance of the pipeline.

We compare our method on the three datasets to the following combination of
feature point detectors and descriptors, as reported by the authors of the corre-
sponding papers: SIFT [1], SURF [2], KAZE [36], ORB [4], Daisy [37] with SIFT
detector, sGLOH [38] with Harris-aﬃne detector [39], MROGH [40] with Harris-
aﬃne detector, LIOP [41] with Harris-aﬃne detector, BiCE [42] with Edge Foci
detector [19], BRISK [43], FREAK [44] with BRISK detector, VGG [26] with
SIFT detector, DeepDesc [10] with SIFT detector, PN-Net [28] with SIFT detec-
tor, and MatchNet [7] with SIFT detector. We also consider SIFT with Hessian-
Aﬃne keypoints [17]. For the learned descriptors VGG, DeepDesc, PN-Net and
MatchNet we use SIFT keypoints because they are trained using a dataset cre-
ated with Diﬀerence-of-Gaussians, which is essentially the same as SIFT. In the
case of Daisy, which was not developed for a speciﬁc detector, we also use SIFT
keypoints. To make our results reproducible, we provide additional implementa-
tion details for LIFT and the baselines in the supplementary material.2

4.2 Qualitative Examples

Fig. 5 shows image matching results with 500 feature points, for both SIFT
and our LIFT pipeline trained with Piccadilly. As expected, LIFT returns more
correct correspondences across the two images. One thing to note is that the two
DTU scenes in the bottom two rows are completely diﬀerent from the photo-
tourism datasets we used for training. Given that the two datasets are very
diﬀerent, this shows good generalization properties.

4.3 Quantitative Evaluation of the Full Pipeline

Fig. 6 shows the average matching score for all three datasets, and Table 1
provides the exact numbers for the two LIFT variants. LIFT (pic) is trained

2 Source and models will be available at https://github.com/cvlab-epfl/LIFT.

12

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Fig. 5. Qualitative local feature matching examples of left: SIFT and right: our
method LIFT. Correct matches recovered by each method are shown in green lines and
the descriptor support regions with red circles. Top row: Herz-Jesu-P8 of Strecha,
second row: Frankfurt of Webcam, third row: Scene 7 of DTU and bottom row:
Scene 19 of DTU. Note that the images are very diﬀerent from one another.

with Piccadilly and LIFT (rf) with Roman-Forum. Both of our learned models
signiﬁcantly outperform the state-of-the-art on Strecha and DTU and achieve
state-of-the-art on Webcam. Note that KAZE, which is the best performing
competitor on Webcam, performs poorly on the other two datasets. As discussed
above, Piccadilly and Roman-Forum are very diﬀerent from the datasets used
for testing. This underlines the strong generalization capability of our approach,
which is not always in evidence with learning-based methods.

Interestingly, on DTU, SIFT is still the best performing method among the
competitors, even compared to methods that rely on Deep Learning, such as
DeepDesc and PN-Net. Also, the gap between SIFT and the learning-based
VGG, DeepDesc, and PN-Net is not large for the Strecha dataset.

These results show that although a component may outperform another
method when evaluated individually, they may fail to deliver their full potential
when integrated into the full pipeline, which is what really matters. In other
words, it is important to learn the components together, as we do, and to con-
sider the whole pipeline when evaluating feature point detectors and descriptors.

4.4 Performance of Individual Components

Fine-tuning the Detector. Recall that we pre-train the detector and then
ﬁnalize the training with the Orientation Estimator and the Descriptor, as dis-

LIFT: Learned Invariant Feature Transform

13

Table 1. Average matching score for all baselines.

SIFT SIFT-HesAﬀ SURF

ORB

Daisy

sGLOH MROGH LIOP

BiCE

Strecha
DTU

.283
.272
Webcam .128

Strecha
DTU

.208
.193
Webcam .118

BRISK FREAK

VGG MatchNet DeepDesc PN-Net KAZE LIFT-pic LIFT-rf

.314
.274
.164

.183
.186
.116

.208
.244
.117

.300
.271
.118

.157
.127
.120

.223
.198
.101

.272
.262
.120

.298
.257
.116

.207
.187
.113

.300
.267
.114

.239
.223
.125

.250
.213
.195

.211
.189
.086

.270
.242
.166

.374
.317
.196

.369
.308
.202

Fig. 6. Average matching score for all baselines.

cussed in Section 3.5. It is therefore interesting to see the eﬀect of this ﬁnalizing
stage. In Table 2 we evaluate the entire pipeline with the pre-trained Detector
and the ﬁnal Detector. As the pair-wise loss term ˜Lpair of Eq. (11) is designed
to emulate the behavior of an ideal descriptor, the pre-trained Detector already
performs well. However, the full training pushes the performance slightly higher.
A closer look at Table 2 reveals that gains are larger overall for Piccadilly
than for Roman-Forum. This is probably due to the fact that Roman-Forum
does not have many non-feature point regions. In fact, the network started to
over-ﬁt quickly after a few iterations on this dataset. The same happened when
we further tried to ﬁne-tune the full pipeline as a whole, suggesting that our
learning strategy is already providing a good global solution.

Performance of individual components. To understand the inﬂuence of
each component on the overall performance, we exchange them with their SIFT
counterparts, for both LIFT (pic) and LIFT (rf), on Strecha. We report the
results in Table 3. In short, each time we exchange to SIFT, we decrease per-
formance, thus showing that each element of the pipeline plays and important
role. Our Detector gives higher repeatability for both models. Having better ori-
entations also helps whichever detector or descriptor is being used, and also the
Deep Descriptors perform better than SIFT.

One thing to note is that our Detector is not only better in terms of repeata-
bility, but generally better in terms of both the NN mAP, which captures the

14

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Table 2. Results on Strecha for both LIFT models trained on Piccadilly and Roman-
Forum, with the pre-trained and fully-trained Detector.

Trained on Piccadilly

Trained on Roman-Forum

Pre-trained
Fully-trained

Rep.
.436
.446

M.Score
.367
.374

Rep.
.447
.447

M.Score
.368
.369

Table 3. Results on Strecha for both LIFT models trained on Piccadilly and Roman-
Forum, interchanging our components with their SIFT counterparts.

Det.

Ori.

Desc. Rep. NN mAP M.Score Rep. NN mAP M.Score

Trained on Piccadilly

Trained on Roman-Forum

.428

.428

SIFT SIFT SIFT

SIFT Ours

SIFT

SIFT SIFT Ours

SIFT Ours Ours

Ours

SIFT SIFT

Ours Ours

SIFT

.446

Ours

SIFT Ours

.517

.671

.568

.685

.540

.644

.629

.282

.341

.290

.344

.325

.372

.339

.447

.517

.662

.581

.688

.545

.630

.644

.282

.338

.295

.342

.319

.360

.337

Ours Ours Ours

.446

.686

.374

.447

.683

.369

descriptor performance, and in terms of matching score, which evaluates the full
pipeline. This shows that our Detector learns to ﬁnd not only points that can be
found often but also points that can be matched easily, indicating that training
the pipeline as a whole is important for optimal performance.

5 Conclusion

We have introduced a novel Deep Network architecture that combines the three
components of standard pipelines for local feature detection and description
into a single diﬀerentiable network. We used Spatial Transformers together with
the softargmax function to mesh them together into a uniﬁed network that
can be trained end-to-end with back-propagation. While this makes learning
the network from scratch theoretically possible, it is not practical. We therefore
proposed an eﬀective strategy to train it.

Our experimental results demonstrate that our integrated approach outper-
forms the state-of-the-art. To further improve performance, we will look into
strategies that allow us to take advantage even more eﬀectively of our ability to
train the network as a whole. In particular, we will look into using hard negative
mining strategies over the whole image [45] instead of relying on pre-extracted
patches. This has the potential of producing more discriminative ﬁlters and,
consequently, better descriptors.

LIFT: Learned Invariant Feature Transform

15

References

(2004)

1. Lowe, D.: Distinctive Image Features from Scale-Invariant Keypoints. IJCV 20(2)

2. Bay, H., Ess, A., Tuytelaars, T., Van Gool, L.: SURF: Speeded Up Robust Features.

3. Tola, E., Lepetit, V., Fua, P.: A Fast Local Descriptor for Dense Matching. In:

CVIU 10(3) (2008) 346–359

CVPR. (2008)

4. Rublee, E., Rabaud, V., Konolidge, K., Bradski, G.: ORB: An Eﬃcient Alternative

to SIFT or SURF. In: ICCV. (2011)

5. Mainali, P., Lafruit, G., Tack, K., Van Gool, L., Lauwereins, R.: Derivative-Based
Scale Invariant Image Feature Detector with Error Resilience. TIP 23(5) (2014)
2380–2391

6. Verdie, Y., Yi, K.M., Fua, P., Lepetit, V.: TILDE: A Temporally Invariant Learned

DEtector. In: CVPR. (2015)

7. Han, X., Leung, T., Jia, Y., Sukthankar, R., Berg, A.C.: MatchNet: Unifying

Feature and Metric Learning for Patch-Based Matching. In: CVPR. (2015)

8. Zagoruyko, S., Komodakis, N.: Learning to Compare Image Patches via Convolu-

tional Neural Networks. In: CVPR. (2015)

9. Yi, K., Verdie, Y., Lepetit, V., Fua, P.: Learning to Assign Orientations to Feature

Points. In: CVPR. (2016)

10. Simo-Serra, E., Trulls, E., Ferraz, L., Kokkinos, I., Fua, P., Moreno-Noguer, F.:
Discriminative Learning of Deep Convolutional Feature Point Descriptors.
In:
ICCV. (2015)

11. Jaderberg, M., Simonyan, K., Zisserman, A., Kavukcuoglu, K.: Spatial Transformer

Networks. In: NIPS. (2015)

12. Chapelle, O., Wu, M.: Gradient Descent Optimization of Smoothed Information

Retrieval Metrics. Information Retrieval 13(3) (2009) 216–235

13. Harris, C., Stephens, M.: A Combined Corner and Edge Detector.

In: Fourth

Alvey Vision Conference. (1988)

14. Moravec, H.: Obstacle Avoidance and Navigation in the Real World by a Seeing
In: tech. report CMU-RI-TR-80-03, Robotics Institute, Carnegie

Robot Rover.
Mellon University, Stanford University. (September 1980)

15. Rosten, E., Drummond, T.: Machine Learning for High-Speed Corner Detection.

In: ECCV. (2006)

16. Matas, J., Chum, O., Martin, U., Pajdla, T.: Robust Wide Baseline Stereo from
Maximally Stable Extremal Regions. In: BMVC. (September 2002) 384–393
17. Mikolajczyk, K., Schmid, C.: An Aﬃne Invariant Interest Point Detector.

In:

ECCV. (2002) 128–142

18. F¨orstner, W., Dickscheid, T., Schindler, F.: Detecting Interpretable and Accurate

Scale-Invariant Keypoints. In: ICCV. (September 2009)

19. Zitnick, C., Ramnath, K.: Edge Foci Interest Points. In: ICCV. (2011)
20. Mainali, P., Lafruit, G., Yang, Q., Geelen, B., Van Gool, L., Lauwereins, R.: SIFER:
Scale-Invariant Feature Detector with Error Resilience. IJCV 104(2) (2013) 172–
197

21. Sochman, J., Matas, J.: Learning a Fast Emulator of a Binary Decision Process.

22. Trujillo, L., Olague, G.: Using Evolution to Learn How to Perform Interest Point

In: ACCV. (2007) 236–245

Detection. In: ICPR. (2006) 211–214

16

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

23. Strecha, C., Bronstein, A., Bronstein, M., Fua, P.: LDAHash: Improved Matching

with Smaller Descriptors. PAMI 34(1) (January 2012)

24. Winder, S., Brown, M.: Learning Local Image Descriptors. In: CVPR. (June 2007)
25. Perez, C., Olague, G.: Genetic Programming As Strategy for Learning Image

Descriptor Operators. Intelligent Data Analysis 17 (2013) 561–583

26. Simonyan, K., Vedaldi, A., Zisserman, A.: Learning Local Feature Descriptors

Using Convex Optimisation. PAMI (2014)

27. Zbontar, J., LeCun, Y.: Computing the Stereo Matching Cost with a Convolutional

Neural Network. In: CVPR. (2015)

28. Balntas, V., Johns, E., Tang, L., Mikolajczyk, K.: PN-Net: Conjoined Triple Deep

Network for Learning Local Image Descriptors. In: arXiv Preprint. (2016)

29. Wilson, K., Snavely, N.: Robust Global Translations with 1DSfM.

In: ECCV.

(2014)

30. Wu, C.: Towards Linear-Time Incremental Structure from Motion. In: 3DV. (2013)
31. Paulin, M., Douze, M., Harchaoui, Z., Mairal, J., Perronnin, F., Schmid, C.: Local
Convolutional Features with Unsupervised Training for Image Retrieval. In: ICCV.
(2015)

32. Sermanet, P., Chintala, S., LeCun, Y.: Convolutional Neural Networks Applied to

House Numbers Digit Classiﬁcation. In: ICPR. (2012)

33. Strecha, C., Hansen, W., Van Gool, L., Fua, P., Thoennessen, U.: On Benchmark-
ing Camera Calibration and Multi-View Stereo for High Resolution Imagery. In:
CVPR. (2008)

34. Aanaes, H., Dahl, A.L., Pedersen, K.S.: Interesting Interest Points. IJCV 97 (2012)

35. Mikolajczyk, K., Schmid, C.: A Performance Evaluation of Local Descriptors. In:

36. Alcantarilla, P., Fern´andez, P., Bartoli, A., Davidson, A.J.: KAZE Features. In:

18–35

CVPR. (June 2003) 257–263

ECCV. (2012)

37. Tola, E., Lepetit, V., Fua, P.: Daisy: An Eﬃcient Dense Descriptor Applied to

Wide Baseline Stereo. PAMI 32(5) (2010) 815–830

38. Bellavia, F., Tegolo, D.: Improving Sift-Based Descriptors Stability to Rotations.

39. Mikolajczyk, K., Schmid, C.: Scale and Aﬃne Invariant Interest Point Detectors.

In: ICPR. (2010)

IJCV 60 (2004) 63–86

40. Fan, B., Wu, F., Hu, Z.: Aggregating Gradient Distributions into Intensity Orders:

A Novel Local Image Descriptor. In: CVPR. (2011)

41. Wang, Z., Fan, B., Wu, F.: Local Intensity Order Pattern for Feature Description.

In: ICCV. (2011)

42. Zitnick, C.: Binary Coherent Edge Descriptors. In: ECCV. (2010)
43. Leutenegger, S., Chli, M., Siegwart, R.: BRISK: Binary Robust Invariant Scalable

44. Alahi, A., Ortiz, R., Vandergheynst, P.: FREAK: Fast Retina Keypoint. In: CVPR.

Keypoints. In: ICCV. (2011)

(2012)

45. Felzenszwalb, P., Girshick, R., McAllester, D., Ramanan, D.: Object Detection

with Discriminatively Trained Part Based Models. PAMI (2010)

6
1
0
2
 
l
u
J
 
9
2
 
 
]

V
C
.
s
c
[
 
 
2
v
4
1
1
9
0
.
3
0
6
1
:
v
i
X
r
a

LIFT: Learned Invariant Feature Transform

Kwang Moo Yi∗,1, Eduard Trulls∗,1, Vincent Lepetit2, Pascal Fua1

1Computer Vision Laboratory, Ecole Polytechnique F´ed´erale de Lausanne (EPFL)
2Institute for Computer Graphics and Vision, Graz University of Technology
{kwang.yi, eduard.trulls, pascal.fua}@epfl.ch, lepetit@icg.tugraz.at

Abstract. We introduce a novel Deep Network architecture that imple-
ments the full feature point handling pipeline, that is, detection, orienta-
tion estimation, and feature description. While previous works have suc-
cessfully tackled each one of these problems individually, we show how to
learn to do all three in a uniﬁed manner while preserving end-to-end dif-
ferentiability. We then demonstrate that our Deep pipeline outperforms
state-of-the-art methods on a number of benchmark datasets, without
the need of retraining.

Keywords: Local Features, Feature Descriptors, Deep Learning

1 Introduction

Local features play a key role in many Computer Vision applications. Find-
ing and matching them across images has been the subject of vast amounts of
research. Until recently, the best techniques relied on carefully hand-crafted fea-
tures [1,2,3,4,5]. Over the past few years, as in many areas of Computer Vision,
methods based in Machine Learning, and more speciﬁcally Deep Learning, have
started to outperform these traditional methods [6,7,8,9,10].

These new algorithms, however, address only a single step in the complete
processing chain, which includes detecting the features, computing their orienta-
tion, and extracting robust representations that allow us to match them across
images. In this paper we introduce a novel Deep architecture that performs all
three steps together. We demonstrate that it achieves better overall performance
than the state-of-the-art methods, in large part because it allows these individual
steps to be optimized to perform well in conjunction with each other.

Our architecture, which we refer to as LIFT for Learned Invariant Feature
Transform, is depicted by Fig. 1. It consists of three components that feed into
each other: the Detector, the Orientation Estimator, and the Descriptor. Each
one is based on Convolutional Neural Networks (CNNs), and patterned after
recent ones [6,9,10] that have been shown to perform these individual functions
well. To mesh them together we use Spatial Transformers [11] to rectify the

∗ First two authors contributed equally.
This work was supported in part by the EU FP7 project MAGELLAN under grant
number ICT-FP7-611526.

2

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Fig. 1. Our integrated feature extraction pipeline. Our pipeline consists of three major
components: the Detector, the Orientation Estimator, and the Descriptor. They are
tied together with diﬀerentiable operations to preserve end-to-end diﬀerentiability.1

image patches given the output of the Detector and the Orientation Estimator.
We also replace the traditional approaches to non-local maximum suppression
(NMS) by the soft argmax function [12]. This allows us to preserve end-to-end
diﬀerentiability, and results in a full network that can still be trained with back-
propagation, which is not the case of any other architecture we know of.

Also, we show how to learn such a pipeline in an eﬀective manner. To this
end, we build a Siamese network and train it using the feature points produced
by a Structure-from-Motion (SfM) algorithm that we ran on images of a scene
captured under diﬀerent viewpoints and lighting conditions, to learn its weights.
We formulate this training problem on image patches extracted at diﬀerent scales
to make the optimization tractable. In practice, we found it impossible to train
the full architecture from scratch, because the individual components try to op-
timize for diﬀerent objectives. Instead, we introduce a problem-speciﬁc learning
approach to overcome this problem. It involves training the Descriptor ﬁrst,
which is then used to train the Orientation Estimator, and ﬁnally the Detector,
based on the already learned Descriptor and Orientation Estimator, diﬀerenti-
ating through the entire network. At test time, we decouple the Detector, which
runs over the whole image in scale space, from the Orientation Estimator and
Descriptor, which process only the keypoints.

In the next section we brieﬂy discuss earlier approaches. We then present our
approach in detail and show that it outperforms many state-of-the-art methods.

2 Related work

The amount of literature relating to local features is immense, but it always
revolves about ﬁnding feature points, computing their orientation, and matching
them. In this section, we will therefore discuss these three elements separately.

2.1 Feature Point Detectors

Research on feature point detection has focused mostly on ﬁnding distinctive lo-
cations whose scale and rotation can be reliably estimated. Early works [13,14]

1 Figures are best viewed in color.

LIFT: Learned Invariant Feature Transform

3

used ﬁrst-order approximations of the image signal to ﬁnd corner points in im-
ages. FAST [15] used Machine Learning techniques but only to speed up the
process of ﬁnding corners. Other than corner points, SIFT [1] detect blobs in
scale-space; SURF [2] use Haar ﬁlters to speed up the process; Maximally Sta-
ble Extremal Regions (MSER) [16] detect regions; [17] detect aﬃne regions.
SFOP [18] use junctions and blobs, and Edge Foci [19] use edges for robustness
to illumination changes. More recently, feature points based on more sophisti-
cated and carefully designed ﬁlter responses [5,20] have also been proposed to
further enhance the performance of feature point detectors.

In contrast to these approaches that focus on better engineering, and follow-
ing the early attempts in learning detectors [21,22], [6] showed that a detector
could be learned to deliver signiﬁcantly better performance than the state-of-
the-art. In this work, piecewise-linear convolutional ﬁlters are learned to robustly
detect feature points in spite of lighting and seasonal changes. Unfortunately, this
was done only for a single scale and from a dataset without viewpoint changes.
We therefore took our inspiration from it but had to extend it substantially to
incorporate it into our pipeline.

2.2 Orientation Estimation

Despite the fact that it plays a critical role in matching feature points, the
problem of estimating a discriminative orientation has received noticeably less
attention than detection or feature description. As a result, the method intro-
duced by SIFT [1] remains the de facto standard up to small improvements, such
as the fact that it can be sped-up by using the intensity centroid, as in ORB [4].
A departure from this can be found in a recent paper [9] that introduced a
Deep Learning-based approach to predicting stable orientations. This resulted
in signiﬁcant gains over the state-of-the-art. We incorporate this architecture
into our pipeline and show how to train it using our problem-speciﬁc training
strategy, given our learned descriptors.

2.3 Feature Descriptors

Feature descriptors are designed to provide discriminative representations of
salient image patches, while being robust to transformations such as viewpoint
or illumination changes. The ﬁeld reached maturity with the introduction of
SIFT [1], which is computed from local histograms of gradient orientations, and
SURF [2], which uses integral image representations to speed up the computa-
tion. Along similar lines, DAISY [3] relies on convolved maps of oriented gra-
dients to approximate the histograms, which yields large computational gains
when extracting dense descriptors.

Even though they have been extremely successful, these hand-crafted de-
scriptors can now be outperformed by newer ones that have been learned. These
range from unsupervised hashing to supervised learning techniques based on
linear discriminant analysis [23,24], genetic algorithm [25], and convex optimiza-
tion [26]. An even more recent trend is to extract features directly from raw image

4

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Fig. 2. Our Siamese training architecture with four branches, which takes as input
a quadruplet of patches: Patches P1 and P2 (blue) correspond to diﬀerent views of
the same physical point, and are used as positive examples to train the Descriptor;
P3 (green) shows a diﬀerent 3D point, which serves as a negative example for the
Descriptor; and P4 (red) contains no distinctive feature points and is only used as a
negative example to train the Detector. Given a patch P, the Detector, the softargmax,
and the Spatial Transformer layer Crop provide all together a smaller patch p inside P.
p is then fed to the Orientation Estimator, which along with the Spatial Transformer
layer Rot, provides the rotated patch pθ that is processed by the Descriptor to obtain
the ﬁnal description vector d.

patches with CNNs trained on large volumes of data. For example, MatchNet [7]
trained a Siamese CNN for feature representation, followed by a fully-connected
network to learn the comparison metric. DeepCompare [8] showed that a net-
work that focuses on the center of the image can increase performance. The
approach of [27] relied on a similar architecture to obtain state-of-the-art results
for narrow-baseline stereo. In [10], hard negative mining was used to learn com-
pact descriptors that use on the Euclidean distance to measure similarity. The
algorithm of [28] relied on sample triplets to mine hard negatives.

In this work, we rely on the architecture of [10] because the corresponding
descriptors are trained and compared with the Euclidean distance, which has a
wider range of applicability than descriptors that require a learned metric.

3 Method

In this section, we ﬁrst formulate the entire feature detection and description
pipeline in terms of the Siamese architecture depicted by Fig. 2. Next, we discuss
the type of data we need to train our networks and how to collect it. We then
describe the training procedure in detail.

3.1 Problem formulation

We use image patches as input, rather than full images. This makes the learning
scalable without loss of information, as most image regions do not contain key-
points. The patches are extracted from the keypoints used by a SfM pipeline, as
will be discussed in Section 3.2. We take them to be small enough that we can

LIFT: Learned Invariant Feature Transform

5

assume they contain only one dominant local feature at the given scale, which
reduces the learning process to ﬁnding the most distinctive point in the patch.
To train our network we create the four-branch Siamese architecture pictured
in Fig. 2. Each branch contains three distinct CNNs, a Detector, an Orientation
Estimator, and a Descriptor. For training purposes, we use quadruplets of image
patches. Each one includes two image patches P1 and P2, that correspond to
diﬀerent views of the same 3D point, one image patch P3, that contains the pro-
jection of a diﬀerent 3D point, and one image patch P4 that does not contain any
distinctive feature point. During training, the i-th patch Pi of each quadruplet
will go through the i-th branch.

To achieve end-to-end diﬀerentiability, the components of each branch are

connected as follows:

1. Given an input image patch P, the Detector provides a score map S.
2. We perform a soft argmax [12] on the score map S and return the location

x of a single potential feature point.

3. We extract a smaller patch p centered on x with the Spatial Transformer
layer Crop (Fig. 2). This serves as the input to the Orientation Estimator.

4. The Orientation Estimator predicts a patch orientation θ.
5. We rotate p according to this orientation using a second Spatial Transformer

layer, labeled as Rot in Fig. 2, to produce pθ.

6. pθ is fed to the Descriptor network, which computes a feature vector d.

Note that the Spatial Transformer layers are used only to manipulate the
image patches while preserving diﬀerentiability. They are not learned modules.
Also, both the location x proposed by the Detector and the orientation θ for
the patch proposal are treated implicitly, meaning that we let the entire network
discover distinctive locations and stable orientations while learning.

Since our network consists of components with diﬀerent purposes, learning
the weights is non-trivial. Our early attempts at training the network as a whole
from scratch were unsuccessful. We therefore designed a problem-speciﬁc learn-
ing approach that involves learning ﬁrst the Descriptor, then the Orientation
Estimator given the learned descriptor, and ﬁnally the Detector, conditioned on
the other two. This allows us to tune the Orientation Estimator for the Descrip-
tor, and the Detector for the other two components.

We will elaborate on this learning strategy in Secs. 3.3 (Descriptor), 3.4 (Ori-
entation Estimator), and 3.5 (Detector), that is, in the order they are learned.

3.2 Creating the Training Dataset

There are datasets that can be used to train feature descriptors [24] and orien-
tation estimators [9]. However it is not so clear how to train a keypoint detec-
tor, and the vast majority of techniques still rely on hand-crafted features. The
TILDE detector [6] is an exception, but the training dataset does not exhibit
any viewpoint changes.

To achieve invariance we need images that capture views of the same scene
under diﬀerent illumination conditions and seen from diﬀerent perspectives. We

6

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Fig. 3. Sample images and patches from Piccadilly (left) and Roman-Forum (right).
Keypoints that survive the SfM pipeline are drawn in blue, and the rest in red.

thus turned to photo-tourism image sets. We used the collections from Piccadilly
Circus in London and the Roman Forum in Rome from [29] to reconstruct the
3D using VisualSFM [30], which relies of SIFT features. Piccadilly contains 3384
images, and the reconstruction has 59k unique points with an average of 6.5 ob-
servations for each. Roman-Forum contains 1658 images and 51k unique points,
with an average of 5.2 observations for each. Fig. 3 shows some examples.

We split the data into training and validation sets, discarding views of train-
ing points on the validation set and vice-versa. To build the positive training
samples we consider only the feature points that survive the SfM reconstruction
process. To extract patches that do not contain any distinctive feature point,
as required by our training method, we randomly sample image regions that
contain no SIFT features, including those that were not used by SfM.

We extract grayscale training patches according to the scale σ of the point,
for both feature and non-feature point image regions. Patches P are extracted
from a 24σ × 24σ support region at these locations, and standardized into S × S
pixels where S = 128. The smaller patches p and pθ that serve as input to the
Orientation Estimator and the Descriptor, are cropped and rotated versions of
these patches, each having size s×s, where s = 64. The smaller patches eﬀectively
correspond to the SIFT descriptor support region size of 12σ. To avoid biasing
the data, we apply uniform random perturbations to the patch location with a
range of 20% (4.8σ). Finally, we normalize the patches with the grayscale mean
and standard deviation of the entire training set.

3.3 Descriptor

Learning feature descriptors from raw image patches has been extensively re-
searched during the past year [7,8,10,27,28,31], with multiple works reporting
impressive results on patch retrieval, narrow baseline stereo, and matching non-
rigid deformations. Here we rely on the relatively simple networks of [10], with
three convolutional layers followed by hyperbolic tangent units, l2 pooling [32]
and local subtractive normalization, as they do not require learning a metric.

The Descriptor can be formalized simply as

d = hρ(pθ) ,

(1)

LIFT: Learned Invariant Feature Transform

7

where h(.) denotes the Descriptor CNN, ρ its parameters, and pθ is the rotated
patch from the Orientation Estimator. When training the Descriptor, we do not
yet have the Detector and the Orientation Estimator trained. We therefore use
the image locations and orientations of the feature points used by the SfM to
generate image patches pθ.

We train the Descriptor by minimizing the sum of the loss for pairs of cor-
(cid:1) and the loss for pairs of non-corresponding patches
(cid:1) is deﬁned as the hinge embedding loss of the

responding patches (cid:0)p1
(cid:0)p1
θ , pl
θ
Euclidean distance between their description vectors. We write

(cid:1). The loss for pair (cid:0)pk

θ, p2
θ

θ, p3
θ

Ldesc(pk

θ , pl

θ) =

(cid:40)(cid:13)

(cid:1) − hρ
(cid:0)pk
(cid:13)hρ
max (cid:0)0, C − (cid:13)

(cid:0)pl
(cid:13)hρ

θ

θ

(cid:1)(cid:13)
(cid:13)2
(cid:1) − hρ
(cid:0)pk

θ

(cid:0)pl

θ

(cid:1)(cid:13)
(cid:13)2

(cid:1)

for positive pairs, and
for negative pairs ,

(2)
where positive and negative samples are pairs of patches that do or do not
correspond to the same physical 3D points, (cid:107)·(cid:107)2 is the Euclidean distance, and
C = 4 is the margin for embedding.

We use hard mining during training, which was shown in [10] to be critical for
descriptor performance. Following this methodology, we forward Kf sample pairs
and use only the Kb pairs with the highest training loss for back-propagation,
where r = Kf /Kb ≥ 1 is the ‘mining ratio’. In [10] the network was pre-trained
without mining and then ﬁne-tuned with r = 8. Here, we use an increasing
mining scheme where we start with r = 1 and double the mining ratio every
5000 batches. We use balanced batches with 128 positive pairs and 128 negative
pairs, mining each separately.

3.4 Orientation Estimator

Our Orientation Estimator is inspired by that of [9]. However, this speciﬁc one
requires pre-computations of description vectors for multiple orientations to com-
pute numerically the Jacobian of the method parameters with respect to orien-
tations. This is a critical limitation for us because we treat the output of the
detector component implicitly throughout the pipeline and it is thus not possible
to pre-compute the description vectors.

We therefore propose to use Spatial Transformers [11] instead to learn the
orientations. Given a patch p from the region proposed by the detector, the
Orientation Estimator predicts an orientation

θ = gφ(p) ,

(3)

where g denotes the Orientation Estimator CNN, and φ its parameters.

Together with the location x from the Detector and P the original image
patch, θ is then used by the second Spatial Transformer Layer Rot(.) to provide
a patch pθ = Rot (P, x, θ), which is the rotated version of patch p.

We train the Orientation Estimator to provide the orientations that minimize
the distances between description vectors for diﬀerent views of the same 3D
points. We use the already trained Descriptor to compute the description vectors,

8

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

and as the Detector is still not trained, we use the image locations from SfM.
More formally, we minimize the loss for pairs of corresponding patches, deﬁned
as the Euclidean distance between their description vectors

Lorientation(P1, x1, P2, x2) = (cid:13)

(cid:13)hρ(G(P1, x1)) − hρ(G(P2, x2))(cid:13)

(cid:13)2 ,

(4)

where G(P, x) is the patch centered on x after orientation correction: G(P, x) =
Rot(P, x, gφ(Crop(P, x))). This complex notation is necessary to properly han-
dle the cropping of the image patches. Recall that pairs (P1, P2) comprise image
patches containing the projections of the same 3D point, and locations x1 and
x2 denote the reprojections of these 3D points. As in [9], we do not use pairs
that correspond to diﬀerent physical points whose orientations are not related.

3.5 Detector

The Detector takes an image patch as input, and returns a score map. We imple-
ment it as a convolution layer followed by piecewise linear activation functions,
as in TILDE [6]. More precisely, the score map S for patch P is computed as:

S = fµ(P) =

(Wmn ∗ P + bmn) ,

(5)

N
(cid:88)

n

δn

M
max
m

where fµ(P) denotes the Detector itself with parameters µ, δn is +1 if n is odd
and −1 otherwise, µ is made of the ﬁlters Wmn and biases bmn of the convolution
layer to learn, ∗ denotes the convolution operation, and N and M are hyper-
parameters controlling the complexity of the piecewise linear activation function.
The main diﬀerence with TILDE lies in the way we train this layer. To let
S have maxima in places other than a ﬁxed location retrieved by SfM, we treat
this location implicitly, as a latent variable. Our method can potentially discover
points that are more reliable and easier to learn, whereas [6] cannot. Incidentally,
in our early experiments, we noticed that it was harmful to force the Detector
to optimize directly for SfM locations.

From the score map S, we obtain the location x of a feature point as

x = softargmax (S) ,

where softargmax is a function which computes the Center of Mass with the
weights being the output of a standard softmax function [12]. We write

softargmax (S) =

(cid:80)

y exp(βS(y))y
y exp(βS(y))

(cid:80)

,

(6)

(7)

where y are locations in S, and β = 10 is a hyper-parameter controlling the
smoothness of the softargmax. This softargmax function acts as a diﬀerentiable
version of non-maximum suppression. x is given to the ﬁrst Spatial Trans-
former Layer Crop(.) together with the patch P to extract a smaller patch
p = Crop (P, x) used as input to the Orientation Estimator.

LIFT: Learned Invariant Feature Transform

9

As the Orientation Estimator and the Descriptor have been learned by this
point, we can train the Detector given the full pipeline. To optimize over the
parameters µ, we minimize the distances between description vectors for the
pairs of patches that correspond to the same physical points, while maximizing
the classiﬁcation score for patches not corresponding to the same physical points.
More exactly, given training quadruplets (cid:0)P1, P2, P3, P4(cid:1), where P1 and P2
correspond to the same physical point, P1 and P3 correspond to diﬀerent SfM
points, and P4 to a non-feature point location, we minimize the sum of their
loss functions

Ldetector(P1, P2, P3, P4) = γLclass(P1, P2, P3, P4) + Lpair(P1, P2) ,

(8)

where γ is a hyper-parameter balancing the two terms in this summation

Lclass(P1, P2, P3, P4) =

αi max (cid:0)0, (cid:0)1 − softmax (cid:0)fµ

(cid:0)Pi(cid:1)(cid:1) yi

(cid:1)(cid:1)2

,

(9)

4
(cid:88)

i=1

with yi = −1 and αi = 3/6 if i = 4, and yi = +1 and αi = 1/6 otherwise to
balance the positives and negatives. softmax is the log-mean-exponential softmax
function. We write

Lpair(P1, P2) = (cid:107) hρ(G(P1, softargmax(fµ(P1)))) −

hρ(G(P2, softargmax(fµ(P2))))

(cid:107)2 .

(10)

Note that the locations of the detected feature points x appear only implicitly
and are discovered during training. Furthermore, all three components are tied
in with the Detector learning. As with the Descriptor we use a hard mining
strategy, in this case with a ﬁxed mining ratio of r = 4.

In practice, as the Descriptor already learns some invariance, it can be hard
for the Detector to ﬁnd new points to learn implicitly. To let the Detector start
with an idea of the regions it should ﬁnd, we ﬁrst constrain the patch proposals
p = Crop(P, softargmax(fµ(P))) that correspond to the same physical points
to overlap. We then continue training the Detector without this constraint.

Speciﬁcally, when pre-training the Detector, we replace Lpair in Eq. (8) with
˜Lpair, where ˜Lpair is equal to 0 when the patch proposals overlap exactly, and
increases with the distance between them otherwise. We therefore write

˜Lpair(P1, P2) = 1 −

p1 ∩ p2
p1 ∪ p2 +

max (cid:0)0, (cid:13)

(cid:13)x1 − x2(cid:13)
(cid:112)p1 ∪ p2

(cid:13)1 − 2s(cid:1)

,

(11)

where xj = softargmax(fµ(Pj)), pj = Crop(Pj, xj), (cid:107)·(cid:107)1 is the l1 norm. Recall
that s = 64 pixels is the width and height of the patch proposals.

3.6 Runtime pipeline

The pipeline used at run-time is shown in Fig. 4. As our method is trained on
patches, simply applying it over the image would require the network to be tested

10

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Fig. 4. An overview of our runtime architecture. As the Orientation Estimator and the
Descriptor only require evaluation at local maxima, we decouple the Detector and run it
in scale space with traditional NMS to obtain proposals for the two other components.

with a sliding window scheme over the whole image. In practice, this would be
too expensive. Fortunately, as the Orientation Estimator and the Descriptor only
need to be run at local maxima, we can simply decouple the detector from the
rest to apply it to the full image, and replace the softargmax function by NMS,
as outlined in red in Fig. 4. We then apply the Orientation Estimator and the
Descriptor only to the patches centered on local maxima.

More exactly, we apply the Detector independently to the image at diﬀerent
resolutions to obtain score maps in scale space. We then apply a traditional NMS
scheme similar to that of [1] to detect feature point locations.

4 Experimental validation

In this section, we ﬁrst present the datasets and metrics we used. We then present
qualitative results, followed by a thorough quantitative comparison against a
number of state-of-the-art baselines, which we consistently outperform.

Finally, to better understand what elements of our approach most contribute
to this result, we study the importance of the pre-training of the Detector com-
ponent, discussed in Section 3.5, and analyze the performance gains attributable
to each component.

4.1 Dataset and Experimental Setup

We evaluate our pipeline on three standard datasets:

– The Strecha dataset [33], which contains 19 images of two scenes seen from

increasingly diﬀerent viewpoints.

– The DTU dataset [34], which contains 60 sequences of objects with diﬀerent
viewpoints and illumination settings. We use this dataset to evaluate our
method under viewpoint changes.

– The Webcam dataset [6], which contains 710 images of 6 scenes with strong
illumination changes but seen from the same viewpoint. We use this dataset
to evaluate our method under natural illumination changes.

LIFT: Learned Invariant Feature Transform

11

For Strecha and DTU we use the provided ground truth to establish corre-
spondences across viewpoints. We use a maximum of 1000 keypoints per image,
and follow the standard evaluation protocol of [35] on the common viewpoint
region. This lets us evaluate the following metrics.

– Repeatability (Rep.): Repeatability of feature points, expressed as a ratio.
This metric captures the performance of the feature point detector by report-
ing the ratio of keypoints that are found consistently in the shared region.
– Nearest Neighbor mean Average Precision (NN mAP): Area Under Curve
(AUC) of the Precision-Recall curve, using the Nearest Neighbor match-
ing strategy. This metric captures how discriminating the descriptor is by
evaluating it at multiple descriptor distance thresholds.

– Matching Score (M. Score): The ratio of ground truth correspondences that
can be recovered by the whole pipeline over the number of features proposed
by the pipeline in the shared viewpoint region. This metric measures the
overall performance of the pipeline.

We compare our method on the three datasets to the following combination of
feature point detectors and descriptors, as reported by the authors of the corre-
sponding papers: SIFT [1], SURF [2], KAZE [36], ORB [4], Daisy [37] with SIFT
detector, sGLOH [38] with Harris-aﬃne detector [39], MROGH [40] with Harris-
aﬃne detector, LIOP [41] with Harris-aﬃne detector, BiCE [42] with Edge Foci
detector [19], BRISK [43], FREAK [44] with BRISK detector, VGG [26] with
SIFT detector, DeepDesc [10] with SIFT detector, PN-Net [28] with SIFT detec-
tor, and MatchNet [7] with SIFT detector. We also consider SIFT with Hessian-
Aﬃne keypoints [17]. For the learned descriptors VGG, DeepDesc, PN-Net and
MatchNet we use SIFT keypoints because they are trained using a dataset cre-
ated with Diﬀerence-of-Gaussians, which is essentially the same as SIFT. In the
case of Daisy, which was not developed for a speciﬁc detector, we also use SIFT
keypoints. To make our results reproducible, we provide additional implementa-
tion details for LIFT and the baselines in the supplementary material.2

4.2 Qualitative Examples

Fig. 5 shows image matching results with 500 feature points, for both SIFT
and our LIFT pipeline trained with Piccadilly. As expected, LIFT returns more
correct correspondences across the two images. One thing to note is that the two
DTU scenes in the bottom two rows are completely diﬀerent from the photo-
tourism datasets we used for training. Given that the two datasets are very
diﬀerent, this shows good generalization properties.

4.3 Quantitative Evaluation of the Full Pipeline

Fig. 6 shows the average matching score for all three datasets, and Table 1
provides the exact numbers for the two LIFT variants. LIFT (pic) is trained

2 Source and models will be available at https://github.com/cvlab-epfl/LIFT.

12

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Fig. 5. Qualitative local feature matching examples of left: SIFT and right: our
method LIFT. Correct matches recovered by each method are shown in green lines and
the descriptor support regions with red circles. Top row: Herz-Jesu-P8 of Strecha,
second row: Frankfurt of Webcam, third row: Scene 7 of DTU and bottom row:
Scene 19 of DTU. Note that the images are very diﬀerent from one another.

with Piccadilly and LIFT (rf) with Roman-Forum. Both of our learned models
signiﬁcantly outperform the state-of-the-art on Strecha and DTU and achieve
state-of-the-art on Webcam. Note that KAZE, which is the best performing
competitor on Webcam, performs poorly on the other two datasets. As discussed
above, Piccadilly and Roman-Forum are very diﬀerent from the datasets used
for testing. This underlines the strong generalization capability of our approach,
which is not always in evidence with learning-based methods.

Interestingly, on DTU, SIFT is still the best performing method among the
competitors, even compared to methods that rely on Deep Learning, such as
DeepDesc and PN-Net. Also, the gap between SIFT and the learning-based
VGG, DeepDesc, and PN-Net is not large for the Strecha dataset.

These results show that although a component may outperform another
method when evaluated individually, they may fail to deliver their full potential
when integrated into the full pipeline, which is what really matters. In other
words, it is important to learn the components together, as we do, and to con-
sider the whole pipeline when evaluating feature point detectors and descriptors.

4.4 Performance of Individual Components

Fine-tuning the Detector. Recall that we pre-train the detector and then
ﬁnalize the training with the Orientation Estimator and the Descriptor, as dis-

LIFT: Learned Invariant Feature Transform

13

Table 1. Average matching score for all baselines.

SIFT SIFT-HesAﬀ SURF

ORB

Daisy

sGLOH MROGH LIOP

BiCE

Strecha
DTU

.283
.272
Webcam .128

Strecha
DTU

.208
.193
Webcam .118

BRISK FREAK

VGG MatchNet DeepDesc PN-Net KAZE LIFT-pic LIFT-rf

.314
.274
.164

.183
.186
.116

.208
.244
.117

.300
.271
.118

.157
.127
.120

.223
.198
.101

.272
.262
.120

.298
.257
.116

.207
.187
.113

.300
.267
.114

.239
.223
.125

.250
.213
.195

.211
.189
.086

.270
.242
.166

.374
.317
.196

.369
.308
.202

Fig. 6. Average matching score for all baselines.

cussed in Section 3.5. It is therefore interesting to see the eﬀect of this ﬁnalizing
stage. In Table 2 we evaluate the entire pipeline with the pre-trained Detector
and the ﬁnal Detector. As the pair-wise loss term ˜Lpair of Eq. (11) is designed
to emulate the behavior of an ideal descriptor, the pre-trained Detector already
performs well. However, the full training pushes the performance slightly higher.
A closer look at Table 2 reveals that gains are larger overall for Piccadilly
than for Roman-Forum. This is probably due to the fact that Roman-Forum
does not have many non-feature point regions. In fact, the network started to
over-ﬁt quickly after a few iterations on this dataset. The same happened when
we further tried to ﬁne-tune the full pipeline as a whole, suggesting that our
learning strategy is already providing a good global solution.

Performance of individual components. To understand the inﬂuence of
each component on the overall performance, we exchange them with their SIFT
counterparts, for both LIFT (pic) and LIFT (rf), on Strecha. We report the
results in Table 3. In short, each time we exchange to SIFT, we decrease per-
formance, thus showing that each element of the pipeline plays and important
role. Our Detector gives higher repeatability for both models. Having better ori-
entations also helps whichever detector or descriptor is being used, and also the
Deep Descriptors perform better than SIFT.

One thing to note is that our Detector is not only better in terms of repeata-
bility, but generally better in terms of both the NN mAP, which captures the

14

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Table 2. Results on Strecha for both LIFT models trained on Piccadilly and Roman-
Forum, with the pre-trained and fully-trained Detector.

Trained on Piccadilly

Trained on Roman-Forum

Pre-trained
Fully-trained

Rep.
.436
.446

M.Score
.367
.374

Rep.
.447
.447

M.Score
.368
.369

Table 3. Results on Strecha for both LIFT models trained on Piccadilly and Roman-
Forum, interchanging our components with their SIFT counterparts.

Det.

Ori.

Desc. Rep. NN mAP M.Score Rep. NN mAP M.Score

Trained on Piccadilly

Trained on Roman-Forum

.428

.428

SIFT SIFT SIFT

SIFT Ours

SIFT

SIFT SIFT Ours

SIFT Ours Ours

Ours

SIFT SIFT

Ours Ours

SIFT

.446

Ours

SIFT Ours

.517

.671

.568

.685

.540

.644

.629

.282

.341

.290

.344

.325

.372

.339

.447

.517

.662

.581

.688

.545

.630

.644

.282

.338

.295

.342

.319

.360

.337

Ours Ours Ours

.446

.686

.374

.447

.683

.369

descriptor performance, and in terms of matching score, which evaluates the full
pipeline. This shows that our Detector learns to ﬁnd not only points that can be
found often but also points that can be matched easily, indicating that training
the pipeline as a whole is important for optimal performance.

5 Conclusion

We have introduced a novel Deep Network architecture that combines the three
components of standard pipelines for local feature detection and description
into a single diﬀerentiable network. We used Spatial Transformers together with
the softargmax function to mesh them together into a uniﬁed network that
can be trained end-to-end with back-propagation. While this makes learning
the network from scratch theoretically possible, it is not practical. We therefore
proposed an eﬀective strategy to train it.

Our experimental results demonstrate that our integrated approach outper-
forms the state-of-the-art. To further improve performance, we will look into
strategies that allow us to take advantage even more eﬀectively of our ability to
train the network as a whole. In particular, we will look into using hard negative
mining strategies over the whole image [45] instead of relying on pre-extracted
patches. This has the potential of producing more discriminative ﬁlters and,
consequently, better descriptors.

LIFT: Learned Invariant Feature Transform

15

References

(2004)

1. Lowe, D.: Distinctive Image Features from Scale-Invariant Keypoints. IJCV 20(2)

2. Bay, H., Ess, A., Tuytelaars, T., Van Gool, L.: SURF: Speeded Up Robust Features.

3. Tola, E., Lepetit, V., Fua, P.: A Fast Local Descriptor for Dense Matching. In:

CVIU 10(3) (2008) 346–359

CVPR. (2008)

4. Rublee, E., Rabaud, V., Konolidge, K., Bradski, G.: ORB: An Eﬃcient Alternative

to SIFT or SURF. In: ICCV. (2011)

5. Mainali, P., Lafruit, G., Tack, K., Van Gool, L., Lauwereins, R.: Derivative-Based
Scale Invariant Image Feature Detector with Error Resilience. TIP 23(5) (2014)
2380–2391

6. Verdie, Y., Yi, K.M., Fua, P., Lepetit, V.: TILDE: A Temporally Invariant Learned

DEtector. In: CVPR. (2015)

7. Han, X., Leung, T., Jia, Y., Sukthankar, R., Berg, A.C.: MatchNet: Unifying

Feature and Metric Learning for Patch-Based Matching. In: CVPR. (2015)

8. Zagoruyko, S., Komodakis, N.: Learning to Compare Image Patches via Convolu-

tional Neural Networks. In: CVPR. (2015)

9. Yi, K., Verdie, Y., Lepetit, V., Fua, P.: Learning to Assign Orientations to Feature

Points. In: CVPR. (2016)

10. Simo-Serra, E., Trulls, E., Ferraz, L., Kokkinos, I., Fua, P., Moreno-Noguer, F.:
Discriminative Learning of Deep Convolutional Feature Point Descriptors.
In:
ICCV. (2015)

11. Jaderberg, M., Simonyan, K., Zisserman, A., Kavukcuoglu, K.: Spatial Transformer

Networks. In: NIPS. (2015)

12. Chapelle, O., Wu, M.: Gradient Descent Optimization of Smoothed Information

Retrieval Metrics. Information Retrieval 13(3) (2009) 216–235

13. Harris, C., Stephens, M.: A Combined Corner and Edge Detector.

In: Fourth

Alvey Vision Conference. (1988)

14. Moravec, H.: Obstacle Avoidance and Navigation in the Real World by a Seeing
In: tech. report CMU-RI-TR-80-03, Robotics Institute, Carnegie

Robot Rover.
Mellon University, Stanford University. (September 1980)

15. Rosten, E., Drummond, T.: Machine Learning for High-Speed Corner Detection.

In: ECCV. (2006)

16. Matas, J., Chum, O., Martin, U., Pajdla, T.: Robust Wide Baseline Stereo from
Maximally Stable Extremal Regions. In: BMVC. (September 2002) 384–393
17. Mikolajczyk, K., Schmid, C.: An Aﬃne Invariant Interest Point Detector.

In:

ECCV. (2002) 128–142

18. F¨orstner, W., Dickscheid, T., Schindler, F.: Detecting Interpretable and Accurate

Scale-Invariant Keypoints. In: ICCV. (September 2009)

19. Zitnick, C., Ramnath, K.: Edge Foci Interest Points. In: ICCV. (2011)
20. Mainali, P., Lafruit, G., Yang, Q., Geelen, B., Van Gool, L., Lauwereins, R.: SIFER:
Scale-Invariant Feature Detector with Error Resilience. IJCV 104(2) (2013) 172–
197

21. Sochman, J., Matas, J.: Learning a Fast Emulator of a Binary Decision Process.

22. Trujillo, L., Olague, G.: Using Evolution to Learn How to Perform Interest Point

In: ACCV. (2007) 236–245

Detection. In: ICPR. (2006) 211–214

16

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

23. Strecha, C., Bronstein, A., Bronstein, M., Fua, P.: LDAHash: Improved Matching

with Smaller Descriptors. PAMI 34(1) (January 2012)

24. Winder, S., Brown, M.: Learning Local Image Descriptors. In: CVPR. (June 2007)
25. Perez, C., Olague, G.: Genetic Programming As Strategy for Learning Image

Descriptor Operators. Intelligent Data Analysis 17 (2013) 561–583

26. Simonyan, K., Vedaldi, A., Zisserman, A.: Learning Local Feature Descriptors

Using Convex Optimisation. PAMI (2014)

27. Zbontar, J., LeCun, Y.: Computing the Stereo Matching Cost with a Convolutional

Neural Network. In: CVPR. (2015)

28. Balntas, V., Johns, E., Tang, L., Mikolajczyk, K.: PN-Net: Conjoined Triple Deep

Network for Learning Local Image Descriptors. In: arXiv Preprint. (2016)

29. Wilson, K., Snavely, N.: Robust Global Translations with 1DSfM.

In: ECCV.

(2014)

30. Wu, C.: Towards Linear-Time Incremental Structure from Motion. In: 3DV. (2013)
31. Paulin, M., Douze, M., Harchaoui, Z., Mairal, J., Perronnin, F., Schmid, C.: Local
Convolutional Features with Unsupervised Training for Image Retrieval. In: ICCV.
(2015)

32. Sermanet, P., Chintala, S., LeCun, Y.: Convolutional Neural Networks Applied to

House Numbers Digit Classiﬁcation. In: ICPR. (2012)

33. Strecha, C., Hansen, W., Van Gool, L., Fua, P., Thoennessen, U.: On Benchmark-
ing Camera Calibration and Multi-View Stereo for High Resolution Imagery. In:
CVPR. (2008)

34. Aanaes, H., Dahl, A.L., Pedersen, K.S.: Interesting Interest Points. IJCV 97 (2012)

35. Mikolajczyk, K., Schmid, C.: A Performance Evaluation of Local Descriptors. In:

36. Alcantarilla, P., Fern´andez, P., Bartoli, A., Davidson, A.J.: KAZE Features. In:

18–35

CVPR. (June 2003) 257–263

ECCV. (2012)

37. Tola, E., Lepetit, V., Fua, P.: Daisy: An Eﬃcient Dense Descriptor Applied to

Wide Baseline Stereo. PAMI 32(5) (2010) 815–830

38. Bellavia, F., Tegolo, D.: Improving Sift-Based Descriptors Stability to Rotations.

39. Mikolajczyk, K., Schmid, C.: Scale and Aﬃne Invariant Interest Point Detectors.

In: ICPR. (2010)

IJCV 60 (2004) 63–86

40. Fan, B., Wu, F., Hu, Z.: Aggregating Gradient Distributions into Intensity Orders:

A Novel Local Image Descriptor. In: CVPR. (2011)

41. Wang, Z., Fan, B., Wu, F.: Local Intensity Order Pattern for Feature Description.

In: ICCV. (2011)

42. Zitnick, C.: Binary Coherent Edge Descriptors. In: ECCV. (2010)
43. Leutenegger, S., Chli, M., Siegwart, R.: BRISK: Binary Robust Invariant Scalable

44. Alahi, A., Ortiz, R., Vandergheynst, P.: FREAK: Fast Retina Keypoint. In: CVPR.

Keypoints. In: ICCV. (2011)

(2012)

45. Felzenszwalb, P., Girshick, R., McAllester, D., Ramanan, D.: Object Detection

with Discriminatively Trained Part Based Models. PAMI (2010)

6
1
0
2
 
l
u
J
 
9
2
 
 
]

V
C
.
s
c
[
 
 
2
v
4
1
1
9
0
.
3
0
6
1
:
v
i
X
r
a

LIFT: Learned Invariant Feature Transform

Kwang Moo Yi∗,1, Eduard Trulls∗,1, Vincent Lepetit2, Pascal Fua1

1Computer Vision Laboratory, Ecole Polytechnique F´ed´erale de Lausanne (EPFL)
2Institute for Computer Graphics and Vision, Graz University of Technology
{kwang.yi, eduard.trulls, pascal.fua}@epfl.ch, lepetit@icg.tugraz.at

Abstract. We introduce a novel Deep Network architecture that imple-
ments the full feature point handling pipeline, that is, detection, orienta-
tion estimation, and feature description. While previous works have suc-
cessfully tackled each one of these problems individually, we show how to
learn to do all three in a uniﬁed manner while preserving end-to-end dif-
ferentiability. We then demonstrate that our Deep pipeline outperforms
state-of-the-art methods on a number of benchmark datasets, without
the need of retraining.

Keywords: Local Features, Feature Descriptors, Deep Learning

1 Introduction

Local features play a key role in many Computer Vision applications. Find-
ing and matching them across images has been the subject of vast amounts of
research. Until recently, the best techniques relied on carefully hand-crafted fea-
tures [1,2,3,4,5]. Over the past few years, as in many areas of Computer Vision,
methods based in Machine Learning, and more speciﬁcally Deep Learning, have
started to outperform these traditional methods [6,7,8,9,10].

These new algorithms, however, address only a single step in the complete
processing chain, which includes detecting the features, computing their orienta-
tion, and extracting robust representations that allow us to match them across
images. In this paper we introduce a novel Deep architecture that performs all
three steps together. We demonstrate that it achieves better overall performance
than the state-of-the-art methods, in large part because it allows these individual
steps to be optimized to perform well in conjunction with each other.

Our architecture, which we refer to as LIFT for Learned Invariant Feature
Transform, is depicted by Fig. 1. It consists of three components that feed into
each other: the Detector, the Orientation Estimator, and the Descriptor. Each
one is based on Convolutional Neural Networks (CNNs), and patterned after
recent ones [6,9,10] that have been shown to perform these individual functions
well. To mesh them together we use Spatial Transformers [11] to rectify the

∗ First two authors contributed equally.
This work was supported in part by the EU FP7 project MAGELLAN under grant
number ICT-FP7-611526.

2

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Fig. 1. Our integrated feature extraction pipeline. Our pipeline consists of three major
components: the Detector, the Orientation Estimator, and the Descriptor. They are
tied together with diﬀerentiable operations to preserve end-to-end diﬀerentiability.1

image patches given the output of the Detector and the Orientation Estimator.
We also replace the traditional approaches to non-local maximum suppression
(NMS) by the soft argmax function [12]. This allows us to preserve end-to-end
diﬀerentiability, and results in a full network that can still be trained with back-
propagation, which is not the case of any other architecture we know of.

Also, we show how to learn such a pipeline in an eﬀective manner. To this
end, we build a Siamese network and train it using the feature points produced
by a Structure-from-Motion (SfM) algorithm that we ran on images of a scene
captured under diﬀerent viewpoints and lighting conditions, to learn its weights.
We formulate this training problem on image patches extracted at diﬀerent scales
to make the optimization tractable. In practice, we found it impossible to train
the full architecture from scratch, because the individual components try to op-
timize for diﬀerent objectives. Instead, we introduce a problem-speciﬁc learning
approach to overcome this problem. It involves training the Descriptor ﬁrst,
which is then used to train the Orientation Estimator, and ﬁnally the Detector,
based on the already learned Descriptor and Orientation Estimator, diﬀerenti-
ating through the entire network. At test time, we decouple the Detector, which
runs over the whole image in scale space, from the Orientation Estimator and
Descriptor, which process only the keypoints.

In the next section we brieﬂy discuss earlier approaches. We then present our
approach in detail and show that it outperforms many state-of-the-art methods.

2 Related work

The amount of literature relating to local features is immense, but it always
revolves about ﬁnding feature points, computing their orientation, and matching
them. In this section, we will therefore discuss these three elements separately.

2.1 Feature Point Detectors

Research on feature point detection has focused mostly on ﬁnding distinctive lo-
cations whose scale and rotation can be reliably estimated. Early works [13,14]

1 Figures are best viewed in color.

LIFT: Learned Invariant Feature Transform

3

used ﬁrst-order approximations of the image signal to ﬁnd corner points in im-
ages. FAST [15] used Machine Learning techniques but only to speed up the
process of ﬁnding corners. Other than corner points, SIFT [1] detect blobs in
scale-space; SURF [2] use Haar ﬁlters to speed up the process; Maximally Sta-
ble Extremal Regions (MSER) [16] detect regions; [17] detect aﬃne regions.
SFOP [18] use junctions and blobs, and Edge Foci [19] use edges for robustness
to illumination changes. More recently, feature points based on more sophisti-
cated and carefully designed ﬁlter responses [5,20] have also been proposed to
further enhance the performance of feature point detectors.

In contrast to these approaches that focus on better engineering, and follow-
ing the early attempts in learning detectors [21,22], [6] showed that a detector
could be learned to deliver signiﬁcantly better performance than the state-of-
the-art. In this work, piecewise-linear convolutional ﬁlters are learned to robustly
detect feature points in spite of lighting and seasonal changes. Unfortunately, this
was done only for a single scale and from a dataset without viewpoint changes.
We therefore took our inspiration from it but had to extend it substantially to
incorporate it into our pipeline.

2.2 Orientation Estimation

Despite the fact that it plays a critical role in matching feature points, the
problem of estimating a discriminative orientation has received noticeably less
attention than detection or feature description. As a result, the method intro-
duced by SIFT [1] remains the de facto standard up to small improvements, such
as the fact that it can be sped-up by using the intensity centroid, as in ORB [4].
A departure from this can be found in a recent paper [9] that introduced a
Deep Learning-based approach to predicting stable orientations. This resulted
in signiﬁcant gains over the state-of-the-art. We incorporate this architecture
into our pipeline and show how to train it using our problem-speciﬁc training
strategy, given our learned descriptors.

2.3 Feature Descriptors

Feature descriptors are designed to provide discriminative representations of
salient image patches, while being robust to transformations such as viewpoint
or illumination changes. The ﬁeld reached maturity with the introduction of
SIFT [1], which is computed from local histograms of gradient orientations, and
SURF [2], which uses integral image representations to speed up the computa-
tion. Along similar lines, DAISY [3] relies on convolved maps of oriented gra-
dients to approximate the histograms, which yields large computational gains
when extracting dense descriptors.

Even though they have been extremely successful, these hand-crafted de-
scriptors can now be outperformed by newer ones that have been learned. These
range from unsupervised hashing to supervised learning techniques based on
linear discriminant analysis [23,24], genetic algorithm [25], and convex optimiza-
tion [26]. An even more recent trend is to extract features directly from raw image

4

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Fig. 2. Our Siamese training architecture with four branches, which takes as input
a quadruplet of patches: Patches P1 and P2 (blue) correspond to diﬀerent views of
the same physical point, and are used as positive examples to train the Descriptor;
P3 (green) shows a diﬀerent 3D point, which serves as a negative example for the
Descriptor; and P4 (red) contains no distinctive feature points and is only used as a
negative example to train the Detector. Given a patch P, the Detector, the softargmax,
and the Spatial Transformer layer Crop provide all together a smaller patch p inside P.
p is then fed to the Orientation Estimator, which along with the Spatial Transformer
layer Rot, provides the rotated patch pθ that is processed by the Descriptor to obtain
the ﬁnal description vector d.

patches with CNNs trained on large volumes of data. For example, MatchNet [7]
trained a Siamese CNN for feature representation, followed by a fully-connected
network to learn the comparison metric. DeepCompare [8] showed that a net-
work that focuses on the center of the image can increase performance. The
approach of [27] relied on a similar architecture to obtain state-of-the-art results
for narrow-baseline stereo. In [10], hard negative mining was used to learn com-
pact descriptors that use on the Euclidean distance to measure similarity. The
algorithm of [28] relied on sample triplets to mine hard negatives.

In this work, we rely on the architecture of [10] because the corresponding
descriptors are trained and compared with the Euclidean distance, which has a
wider range of applicability than descriptors that require a learned metric.

3 Method

In this section, we ﬁrst formulate the entire feature detection and description
pipeline in terms of the Siamese architecture depicted by Fig. 2. Next, we discuss
the type of data we need to train our networks and how to collect it. We then
describe the training procedure in detail.

3.1 Problem formulation

We use image patches as input, rather than full images. This makes the learning
scalable without loss of information, as most image regions do not contain key-
points. The patches are extracted from the keypoints used by a SfM pipeline, as
will be discussed in Section 3.2. We take them to be small enough that we can

LIFT: Learned Invariant Feature Transform

5

assume they contain only one dominant local feature at the given scale, which
reduces the learning process to ﬁnding the most distinctive point in the patch.
To train our network we create the four-branch Siamese architecture pictured
in Fig. 2. Each branch contains three distinct CNNs, a Detector, an Orientation
Estimator, and a Descriptor. For training purposes, we use quadruplets of image
patches. Each one includes two image patches P1 and P2, that correspond to
diﬀerent views of the same 3D point, one image patch P3, that contains the pro-
jection of a diﬀerent 3D point, and one image patch P4 that does not contain any
distinctive feature point. During training, the i-th patch Pi of each quadruplet
will go through the i-th branch.

To achieve end-to-end diﬀerentiability, the components of each branch are

connected as follows:

1. Given an input image patch P, the Detector provides a score map S.
2. We perform a soft argmax [12] on the score map S and return the location

x of a single potential feature point.

3. We extract a smaller patch p centered on x with the Spatial Transformer
layer Crop (Fig. 2). This serves as the input to the Orientation Estimator.

4. The Orientation Estimator predicts a patch orientation θ.
5. We rotate p according to this orientation using a second Spatial Transformer

layer, labeled as Rot in Fig. 2, to produce pθ.

6. pθ is fed to the Descriptor network, which computes a feature vector d.

Note that the Spatial Transformer layers are used only to manipulate the
image patches while preserving diﬀerentiability. They are not learned modules.
Also, both the location x proposed by the Detector and the orientation θ for
the patch proposal are treated implicitly, meaning that we let the entire network
discover distinctive locations and stable orientations while learning.

Since our network consists of components with diﬀerent purposes, learning
the weights is non-trivial. Our early attempts at training the network as a whole
from scratch were unsuccessful. We therefore designed a problem-speciﬁc learn-
ing approach that involves learning ﬁrst the Descriptor, then the Orientation
Estimator given the learned descriptor, and ﬁnally the Detector, conditioned on
the other two. This allows us to tune the Orientation Estimator for the Descrip-
tor, and the Detector for the other two components.

We will elaborate on this learning strategy in Secs. 3.3 (Descriptor), 3.4 (Ori-
entation Estimator), and 3.5 (Detector), that is, in the order they are learned.

3.2 Creating the Training Dataset

There are datasets that can be used to train feature descriptors [24] and orien-
tation estimators [9]. However it is not so clear how to train a keypoint detec-
tor, and the vast majority of techniques still rely on hand-crafted features. The
TILDE detector [6] is an exception, but the training dataset does not exhibit
any viewpoint changes.

To achieve invariance we need images that capture views of the same scene
under diﬀerent illumination conditions and seen from diﬀerent perspectives. We

6

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Fig. 3. Sample images and patches from Piccadilly (left) and Roman-Forum (right).
Keypoints that survive the SfM pipeline are drawn in blue, and the rest in red.

thus turned to photo-tourism image sets. We used the collections from Piccadilly
Circus in London and the Roman Forum in Rome from [29] to reconstruct the
3D using VisualSFM [30], which relies of SIFT features. Piccadilly contains 3384
images, and the reconstruction has 59k unique points with an average of 6.5 ob-
servations for each. Roman-Forum contains 1658 images and 51k unique points,
with an average of 5.2 observations for each. Fig. 3 shows some examples.

We split the data into training and validation sets, discarding views of train-
ing points on the validation set and vice-versa. To build the positive training
samples we consider only the feature points that survive the SfM reconstruction
process. To extract patches that do not contain any distinctive feature point,
as required by our training method, we randomly sample image regions that
contain no SIFT features, including those that were not used by SfM.

We extract grayscale training patches according to the scale σ of the point,
for both feature and non-feature point image regions. Patches P are extracted
from a 24σ × 24σ support region at these locations, and standardized into S × S
pixels where S = 128. The smaller patches p and pθ that serve as input to the
Orientation Estimator and the Descriptor, are cropped and rotated versions of
these patches, each having size s×s, where s = 64. The smaller patches eﬀectively
correspond to the SIFT descriptor support region size of 12σ. To avoid biasing
the data, we apply uniform random perturbations to the patch location with a
range of 20% (4.8σ). Finally, we normalize the patches with the grayscale mean
and standard deviation of the entire training set.

3.3 Descriptor

Learning feature descriptors from raw image patches has been extensively re-
searched during the past year [7,8,10,27,28,31], with multiple works reporting
impressive results on patch retrieval, narrow baseline stereo, and matching non-
rigid deformations. Here we rely on the relatively simple networks of [10], with
three convolutional layers followed by hyperbolic tangent units, l2 pooling [32]
and local subtractive normalization, as they do not require learning a metric.

The Descriptor can be formalized simply as

d = hρ(pθ) ,

(1)

LIFT: Learned Invariant Feature Transform

7

where h(.) denotes the Descriptor CNN, ρ its parameters, and pθ is the rotated
patch from the Orientation Estimator. When training the Descriptor, we do not
yet have the Detector and the Orientation Estimator trained. We therefore use
the image locations and orientations of the feature points used by the SfM to
generate image patches pθ.

We train the Descriptor by minimizing the sum of the loss for pairs of cor-
(cid:1) and the loss for pairs of non-corresponding patches
(cid:1) is deﬁned as the hinge embedding loss of the

responding patches (cid:0)p1
(cid:0)p1
θ , pl
θ
Euclidean distance between their description vectors. We write

(cid:1). The loss for pair (cid:0)pk

θ, p2
θ

θ, p3
θ

Ldesc(pk

θ , pl

θ) =

(cid:40)(cid:13)

(cid:1) − hρ
(cid:0)pk
(cid:13)hρ
max (cid:0)0, C − (cid:13)

(cid:0)pl
(cid:13)hρ

θ

θ

(cid:1)(cid:13)
(cid:13)2
(cid:1) − hρ
(cid:0)pk

θ

(cid:0)pl

θ

(cid:1)(cid:13)
(cid:13)2

(cid:1)

for positive pairs, and
for negative pairs ,

(2)
where positive and negative samples are pairs of patches that do or do not
correspond to the same physical 3D points, (cid:107)·(cid:107)2 is the Euclidean distance, and
C = 4 is the margin for embedding.

We use hard mining during training, which was shown in [10] to be critical for
descriptor performance. Following this methodology, we forward Kf sample pairs
and use only the Kb pairs with the highest training loss for back-propagation,
where r = Kf /Kb ≥ 1 is the ‘mining ratio’. In [10] the network was pre-trained
without mining and then ﬁne-tuned with r = 8. Here, we use an increasing
mining scheme where we start with r = 1 and double the mining ratio every
5000 batches. We use balanced batches with 128 positive pairs and 128 negative
pairs, mining each separately.

3.4 Orientation Estimator

Our Orientation Estimator is inspired by that of [9]. However, this speciﬁc one
requires pre-computations of description vectors for multiple orientations to com-
pute numerically the Jacobian of the method parameters with respect to orien-
tations. This is a critical limitation for us because we treat the output of the
detector component implicitly throughout the pipeline and it is thus not possible
to pre-compute the description vectors.

We therefore propose to use Spatial Transformers [11] instead to learn the
orientations. Given a patch p from the region proposed by the detector, the
Orientation Estimator predicts an orientation

θ = gφ(p) ,

(3)

where g denotes the Orientation Estimator CNN, and φ its parameters.

Together with the location x from the Detector and P the original image
patch, θ is then used by the second Spatial Transformer Layer Rot(.) to provide
a patch pθ = Rot (P, x, θ), which is the rotated version of patch p.

We train the Orientation Estimator to provide the orientations that minimize
the distances between description vectors for diﬀerent views of the same 3D
points. We use the already trained Descriptor to compute the description vectors,

8

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

and as the Detector is still not trained, we use the image locations from SfM.
More formally, we minimize the loss for pairs of corresponding patches, deﬁned
as the Euclidean distance between their description vectors

Lorientation(P1, x1, P2, x2) = (cid:13)

(cid:13)hρ(G(P1, x1)) − hρ(G(P2, x2))(cid:13)

(cid:13)2 ,

(4)

where G(P, x) is the patch centered on x after orientation correction: G(P, x) =
Rot(P, x, gφ(Crop(P, x))). This complex notation is necessary to properly han-
dle the cropping of the image patches. Recall that pairs (P1, P2) comprise image
patches containing the projections of the same 3D point, and locations x1 and
x2 denote the reprojections of these 3D points. As in [9], we do not use pairs
that correspond to diﬀerent physical points whose orientations are not related.

3.5 Detector

The Detector takes an image patch as input, and returns a score map. We imple-
ment it as a convolution layer followed by piecewise linear activation functions,
as in TILDE [6]. More precisely, the score map S for patch P is computed as:

S = fµ(P) =

(Wmn ∗ P + bmn) ,

(5)

N
(cid:88)

n

δn

M
max
m

where fµ(P) denotes the Detector itself with parameters µ, δn is +1 if n is odd
and −1 otherwise, µ is made of the ﬁlters Wmn and biases bmn of the convolution
layer to learn, ∗ denotes the convolution operation, and N and M are hyper-
parameters controlling the complexity of the piecewise linear activation function.
The main diﬀerence with TILDE lies in the way we train this layer. To let
S have maxima in places other than a ﬁxed location retrieved by SfM, we treat
this location implicitly, as a latent variable. Our method can potentially discover
points that are more reliable and easier to learn, whereas [6] cannot. Incidentally,
in our early experiments, we noticed that it was harmful to force the Detector
to optimize directly for SfM locations.

From the score map S, we obtain the location x of a feature point as

x = softargmax (S) ,

where softargmax is a function which computes the Center of Mass with the
weights being the output of a standard softmax function [12]. We write

softargmax (S) =

(cid:80)

y exp(βS(y))y
y exp(βS(y))

(cid:80)

,

(6)

(7)

where y are locations in S, and β = 10 is a hyper-parameter controlling the
smoothness of the softargmax. This softargmax function acts as a diﬀerentiable
version of non-maximum suppression. x is given to the ﬁrst Spatial Trans-
former Layer Crop(.) together with the patch P to extract a smaller patch
p = Crop (P, x) used as input to the Orientation Estimator.

LIFT: Learned Invariant Feature Transform

9

As the Orientation Estimator and the Descriptor have been learned by this
point, we can train the Detector given the full pipeline. To optimize over the
parameters µ, we minimize the distances between description vectors for the
pairs of patches that correspond to the same physical points, while maximizing
the classiﬁcation score for patches not corresponding to the same physical points.
More exactly, given training quadruplets (cid:0)P1, P2, P3, P4(cid:1), where P1 and P2
correspond to the same physical point, P1 and P3 correspond to diﬀerent SfM
points, and P4 to a non-feature point location, we minimize the sum of their
loss functions

Ldetector(P1, P2, P3, P4) = γLclass(P1, P2, P3, P4) + Lpair(P1, P2) ,

(8)

where γ is a hyper-parameter balancing the two terms in this summation

Lclass(P1, P2, P3, P4) =

αi max (cid:0)0, (cid:0)1 − softmax (cid:0)fµ

(cid:0)Pi(cid:1)(cid:1) yi

(cid:1)(cid:1)2

,

(9)

4
(cid:88)

i=1

with yi = −1 and αi = 3/6 if i = 4, and yi = +1 and αi = 1/6 otherwise to
balance the positives and negatives. softmax is the log-mean-exponential softmax
function. We write

Lpair(P1, P2) = (cid:107) hρ(G(P1, softargmax(fµ(P1)))) −

hρ(G(P2, softargmax(fµ(P2))))

(cid:107)2 .

(10)

Note that the locations of the detected feature points x appear only implicitly
and are discovered during training. Furthermore, all three components are tied
in with the Detector learning. As with the Descriptor we use a hard mining
strategy, in this case with a ﬁxed mining ratio of r = 4.

In practice, as the Descriptor already learns some invariance, it can be hard
for the Detector to ﬁnd new points to learn implicitly. To let the Detector start
with an idea of the regions it should ﬁnd, we ﬁrst constrain the patch proposals
p = Crop(P, softargmax(fµ(P))) that correspond to the same physical points
to overlap. We then continue training the Detector without this constraint.

Speciﬁcally, when pre-training the Detector, we replace Lpair in Eq. (8) with
˜Lpair, where ˜Lpair is equal to 0 when the patch proposals overlap exactly, and
increases with the distance between them otherwise. We therefore write

˜Lpair(P1, P2) = 1 −

p1 ∩ p2
p1 ∪ p2 +

max (cid:0)0, (cid:13)

(cid:13)x1 − x2(cid:13)
(cid:112)p1 ∪ p2

(cid:13)1 − 2s(cid:1)

,

(11)

where xj = softargmax(fµ(Pj)), pj = Crop(Pj, xj), (cid:107)·(cid:107)1 is the l1 norm. Recall
that s = 64 pixels is the width and height of the patch proposals.

3.6 Runtime pipeline

The pipeline used at run-time is shown in Fig. 4. As our method is trained on
patches, simply applying it over the image would require the network to be tested

10

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Fig. 4. An overview of our runtime architecture. As the Orientation Estimator and the
Descriptor only require evaluation at local maxima, we decouple the Detector and run it
in scale space with traditional NMS to obtain proposals for the two other components.

with a sliding window scheme over the whole image. In practice, this would be
too expensive. Fortunately, as the Orientation Estimator and the Descriptor only
need to be run at local maxima, we can simply decouple the detector from the
rest to apply it to the full image, and replace the softargmax function by NMS,
as outlined in red in Fig. 4. We then apply the Orientation Estimator and the
Descriptor only to the patches centered on local maxima.

More exactly, we apply the Detector independently to the image at diﬀerent
resolutions to obtain score maps in scale space. We then apply a traditional NMS
scheme similar to that of [1] to detect feature point locations.

4 Experimental validation

In this section, we ﬁrst present the datasets and metrics we used. We then present
qualitative results, followed by a thorough quantitative comparison against a
number of state-of-the-art baselines, which we consistently outperform.

Finally, to better understand what elements of our approach most contribute
to this result, we study the importance of the pre-training of the Detector com-
ponent, discussed in Section 3.5, and analyze the performance gains attributable
to each component.

4.1 Dataset and Experimental Setup

We evaluate our pipeline on three standard datasets:

– The Strecha dataset [33], which contains 19 images of two scenes seen from

increasingly diﬀerent viewpoints.

– The DTU dataset [34], which contains 60 sequences of objects with diﬀerent
viewpoints and illumination settings. We use this dataset to evaluate our
method under viewpoint changes.

– The Webcam dataset [6], which contains 710 images of 6 scenes with strong
illumination changes but seen from the same viewpoint. We use this dataset
to evaluate our method under natural illumination changes.

LIFT: Learned Invariant Feature Transform

11

For Strecha and DTU we use the provided ground truth to establish corre-
spondences across viewpoints. We use a maximum of 1000 keypoints per image,
and follow the standard evaluation protocol of [35] on the common viewpoint
region. This lets us evaluate the following metrics.

– Repeatability (Rep.): Repeatability of feature points, expressed as a ratio.
This metric captures the performance of the feature point detector by report-
ing the ratio of keypoints that are found consistently in the shared region.
– Nearest Neighbor mean Average Precision (NN mAP): Area Under Curve
(AUC) of the Precision-Recall curve, using the Nearest Neighbor match-
ing strategy. This metric captures how discriminating the descriptor is by
evaluating it at multiple descriptor distance thresholds.

– Matching Score (M. Score): The ratio of ground truth correspondences that
can be recovered by the whole pipeline over the number of features proposed
by the pipeline in the shared viewpoint region. This metric measures the
overall performance of the pipeline.

We compare our method on the three datasets to the following combination of
feature point detectors and descriptors, as reported by the authors of the corre-
sponding papers: SIFT [1], SURF [2], KAZE [36], ORB [4], Daisy [37] with SIFT
detector, sGLOH [38] with Harris-aﬃne detector [39], MROGH [40] with Harris-
aﬃne detector, LIOP [41] with Harris-aﬃne detector, BiCE [42] with Edge Foci
detector [19], BRISK [43], FREAK [44] with BRISK detector, VGG [26] with
SIFT detector, DeepDesc [10] with SIFT detector, PN-Net [28] with SIFT detec-
tor, and MatchNet [7] with SIFT detector. We also consider SIFT with Hessian-
Aﬃne keypoints [17]. For the learned descriptors VGG, DeepDesc, PN-Net and
MatchNet we use SIFT keypoints because they are trained using a dataset cre-
ated with Diﬀerence-of-Gaussians, which is essentially the same as SIFT. In the
case of Daisy, which was not developed for a speciﬁc detector, we also use SIFT
keypoints. To make our results reproducible, we provide additional implementa-
tion details for LIFT and the baselines in the supplementary material.2

4.2 Qualitative Examples

Fig. 5 shows image matching results with 500 feature points, for both SIFT
and our LIFT pipeline trained with Piccadilly. As expected, LIFT returns more
correct correspondences across the two images. One thing to note is that the two
DTU scenes in the bottom two rows are completely diﬀerent from the photo-
tourism datasets we used for training. Given that the two datasets are very
diﬀerent, this shows good generalization properties.

4.3 Quantitative Evaluation of the Full Pipeline

Fig. 6 shows the average matching score for all three datasets, and Table 1
provides the exact numbers for the two LIFT variants. LIFT (pic) is trained

2 Source and models will be available at https://github.com/cvlab-epfl/LIFT.

12

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Fig. 5. Qualitative local feature matching examples of left: SIFT and right: our
method LIFT. Correct matches recovered by each method are shown in green lines and
the descriptor support regions with red circles. Top row: Herz-Jesu-P8 of Strecha,
second row: Frankfurt of Webcam, third row: Scene 7 of DTU and bottom row:
Scene 19 of DTU. Note that the images are very diﬀerent from one another.

with Piccadilly and LIFT (rf) with Roman-Forum. Both of our learned models
signiﬁcantly outperform the state-of-the-art on Strecha and DTU and achieve
state-of-the-art on Webcam. Note that KAZE, which is the best performing
competitor on Webcam, performs poorly on the other two datasets. As discussed
above, Piccadilly and Roman-Forum are very diﬀerent from the datasets used
for testing. This underlines the strong generalization capability of our approach,
which is not always in evidence with learning-based methods.

Interestingly, on DTU, SIFT is still the best performing method among the
competitors, even compared to methods that rely on Deep Learning, such as
DeepDesc and PN-Net. Also, the gap between SIFT and the learning-based
VGG, DeepDesc, and PN-Net is not large for the Strecha dataset.

These results show that although a component may outperform another
method when evaluated individually, they may fail to deliver their full potential
when integrated into the full pipeline, which is what really matters. In other
words, it is important to learn the components together, as we do, and to con-
sider the whole pipeline when evaluating feature point detectors and descriptors.

4.4 Performance of Individual Components

Fine-tuning the Detector. Recall that we pre-train the detector and then
ﬁnalize the training with the Orientation Estimator and the Descriptor, as dis-

LIFT: Learned Invariant Feature Transform

13

Table 1. Average matching score for all baselines.

SIFT SIFT-HesAﬀ SURF

ORB

Daisy

sGLOH MROGH LIOP

BiCE

Strecha
DTU

.283
.272
Webcam .128

Strecha
DTU

.208
.193
Webcam .118

BRISK FREAK

VGG MatchNet DeepDesc PN-Net KAZE LIFT-pic LIFT-rf

.314
.274
.164

.183
.186
.116

.208
.244
.117

.300
.271
.118

.157
.127
.120

.223
.198
.101

.272
.262
.120

.298
.257
.116

.207
.187
.113

.300
.267
.114

.239
.223
.125

.250
.213
.195

.211
.189
.086

.270
.242
.166

.374
.317
.196

.369
.308
.202

Fig. 6. Average matching score for all baselines.

cussed in Section 3.5. It is therefore interesting to see the eﬀect of this ﬁnalizing
stage. In Table 2 we evaluate the entire pipeline with the pre-trained Detector
and the ﬁnal Detector. As the pair-wise loss term ˜Lpair of Eq. (11) is designed
to emulate the behavior of an ideal descriptor, the pre-trained Detector already
performs well. However, the full training pushes the performance slightly higher.
A closer look at Table 2 reveals that gains are larger overall for Piccadilly
than for Roman-Forum. This is probably due to the fact that Roman-Forum
does not have many non-feature point regions. In fact, the network started to
over-ﬁt quickly after a few iterations on this dataset. The same happened when
we further tried to ﬁne-tune the full pipeline as a whole, suggesting that our
learning strategy is already providing a good global solution.

Performance of individual components. To understand the inﬂuence of
each component on the overall performance, we exchange them with their SIFT
counterparts, for both LIFT (pic) and LIFT (rf), on Strecha. We report the
results in Table 3. In short, each time we exchange to SIFT, we decrease per-
formance, thus showing that each element of the pipeline plays and important
role. Our Detector gives higher repeatability for both models. Having better ori-
entations also helps whichever detector or descriptor is being used, and also the
Deep Descriptors perform better than SIFT.

One thing to note is that our Detector is not only better in terms of repeata-
bility, but generally better in terms of both the NN mAP, which captures the

14

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Table 2. Results on Strecha for both LIFT models trained on Piccadilly and Roman-
Forum, with the pre-trained and fully-trained Detector.

Trained on Piccadilly

Trained on Roman-Forum

Pre-trained
Fully-trained

Rep.
.436
.446

M.Score
.367
.374

Rep.
.447
.447

M.Score
.368
.369

Table 3. Results on Strecha for both LIFT models trained on Piccadilly and Roman-
Forum, interchanging our components with their SIFT counterparts.

Det.

Ori.

Desc. Rep. NN mAP M.Score Rep. NN mAP M.Score

Trained on Piccadilly

Trained on Roman-Forum

.428

.428

SIFT SIFT SIFT

SIFT Ours

SIFT

SIFT SIFT Ours

SIFT Ours Ours

Ours

SIFT SIFT

Ours Ours

SIFT

.446

Ours

SIFT Ours

.517

.671

.568

.685

.540

.644

.629

.282

.341

.290

.344

.325

.372

.339

.447

.517

.662

.581

.688

.545

.630

.644

.282

.338

.295

.342

.319

.360

.337

Ours Ours Ours

.446

.686

.374

.447

.683

.369

descriptor performance, and in terms of matching score, which evaluates the full
pipeline. This shows that our Detector learns to ﬁnd not only points that can be
found often but also points that can be matched easily, indicating that training
the pipeline as a whole is important for optimal performance.

5 Conclusion

We have introduced a novel Deep Network architecture that combines the three
components of standard pipelines for local feature detection and description
into a single diﬀerentiable network. We used Spatial Transformers together with
the softargmax function to mesh them together into a uniﬁed network that
can be trained end-to-end with back-propagation. While this makes learning
the network from scratch theoretically possible, it is not practical. We therefore
proposed an eﬀective strategy to train it.

Our experimental results demonstrate that our integrated approach outper-
forms the state-of-the-art. To further improve performance, we will look into
strategies that allow us to take advantage even more eﬀectively of our ability to
train the network as a whole. In particular, we will look into using hard negative
mining strategies over the whole image [45] instead of relying on pre-extracted
patches. This has the potential of producing more discriminative ﬁlters and,
consequently, better descriptors.

LIFT: Learned Invariant Feature Transform

15

References

(2004)

1. Lowe, D.: Distinctive Image Features from Scale-Invariant Keypoints. IJCV 20(2)

2. Bay, H., Ess, A., Tuytelaars, T., Van Gool, L.: SURF: Speeded Up Robust Features.

3. Tola, E., Lepetit, V., Fua, P.: A Fast Local Descriptor for Dense Matching. In:

CVIU 10(3) (2008) 346–359

CVPR. (2008)

4. Rublee, E., Rabaud, V., Konolidge, K., Bradski, G.: ORB: An Eﬃcient Alternative

to SIFT or SURF. In: ICCV. (2011)

5. Mainali, P., Lafruit, G., Tack, K., Van Gool, L., Lauwereins, R.: Derivative-Based
Scale Invariant Image Feature Detector with Error Resilience. TIP 23(5) (2014)
2380–2391

6. Verdie, Y., Yi, K.M., Fua, P., Lepetit, V.: TILDE: A Temporally Invariant Learned

DEtector. In: CVPR. (2015)

7. Han, X., Leung, T., Jia, Y., Sukthankar, R., Berg, A.C.: MatchNet: Unifying

Feature and Metric Learning for Patch-Based Matching. In: CVPR. (2015)

8. Zagoruyko, S., Komodakis, N.: Learning to Compare Image Patches via Convolu-

tional Neural Networks. In: CVPR. (2015)

9. Yi, K., Verdie, Y., Lepetit, V., Fua, P.: Learning to Assign Orientations to Feature

Points. In: CVPR. (2016)

10. Simo-Serra, E., Trulls, E., Ferraz, L., Kokkinos, I., Fua, P., Moreno-Noguer, F.:
Discriminative Learning of Deep Convolutional Feature Point Descriptors.
In:
ICCV. (2015)

11. Jaderberg, M., Simonyan, K., Zisserman, A., Kavukcuoglu, K.: Spatial Transformer

Networks. In: NIPS. (2015)

12. Chapelle, O., Wu, M.: Gradient Descent Optimization of Smoothed Information

Retrieval Metrics. Information Retrieval 13(3) (2009) 216–235

13. Harris, C., Stephens, M.: A Combined Corner and Edge Detector.

In: Fourth

Alvey Vision Conference. (1988)

14. Moravec, H.: Obstacle Avoidance and Navigation in the Real World by a Seeing
In: tech. report CMU-RI-TR-80-03, Robotics Institute, Carnegie

Robot Rover.
Mellon University, Stanford University. (September 1980)

15. Rosten, E., Drummond, T.: Machine Learning for High-Speed Corner Detection.

In: ECCV. (2006)

16. Matas, J., Chum, O., Martin, U., Pajdla, T.: Robust Wide Baseline Stereo from
Maximally Stable Extremal Regions. In: BMVC. (September 2002) 384–393
17. Mikolajczyk, K., Schmid, C.: An Aﬃne Invariant Interest Point Detector.

In:

ECCV. (2002) 128–142

18. F¨orstner, W., Dickscheid, T., Schindler, F.: Detecting Interpretable and Accurate

Scale-Invariant Keypoints. In: ICCV. (September 2009)

19. Zitnick, C., Ramnath, K.: Edge Foci Interest Points. In: ICCV. (2011)
20. Mainali, P., Lafruit, G., Yang, Q., Geelen, B., Van Gool, L., Lauwereins, R.: SIFER:
Scale-Invariant Feature Detector with Error Resilience. IJCV 104(2) (2013) 172–
197

21. Sochman, J., Matas, J.: Learning a Fast Emulator of a Binary Decision Process.

22. Trujillo, L., Olague, G.: Using Evolution to Learn How to Perform Interest Point

In: ACCV. (2007) 236–245

Detection. In: ICPR. (2006) 211–214

16

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

23. Strecha, C., Bronstein, A., Bronstein, M., Fua, P.: LDAHash: Improved Matching

with Smaller Descriptors. PAMI 34(1) (January 2012)

24. Winder, S., Brown, M.: Learning Local Image Descriptors. In: CVPR. (June 2007)
25. Perez, C., Olague, G.: Genetic Programming As Strategy for Learning Image

Descriptor Operators. Intelligent Data Analysis 17 (2013) 561–583

26. Simonyan, K., Vedaldi, A., Zisserman, A.: Learning Local Feature Descriptors

Using Convex Optimisation. PAMI (2014)

27. Zbontar, J., LeCun, Y.: Computing the Stereo Matching Cost with a Convolutional

Neural Network. In: CVPR. (2015)

28. Balntas, V., Johns, E., Tang, L., Mikolajczyk, K.: PN-Net: Conjoined Triple Deep

Network for Learning Local Image Descriptors. In: arXiv Preprint. (2016)

29. Wilson, K., Snavely, N.: Robust Global Translations with 1DSfM.

In: ECCV.

(2014)

30. Wu, C.: Towards Linear-Time Incremental Structure from Motion. In: 3DV. (2013)
31. Paulin, M., Douze, M., Harchaoui, Z., Mairal, J., Perronnin, F., Schmid, C.: Local
Convolutional Features with Unsupervised Training for Image Retrieval. In: ICCV.
(2015)

32. Sermanet, P., Chintala, S., LeCun, Y.: Convolutional Neural Networks Applied to

House Numbers Digit Classiﬁcation. In: ICPR. (2012)

33. Strecha, C., Hansen, W., Van Gool, L., Fua, P., Thoennessen, U.: On Benchmark-
ing Camera Calibration and Multi-View Stereo for High Resolution Imagery. In:
CVPR. (2008)

34. Aanaes, H., Dahl, A.L., Pedersen, K.S.: Interesting Interest Points. IJCV 97 (2012)

35. Mikolajczyk, K., Schmid, C.: A Performance Evaluation of Local Descriptors. In:

36. Alcantarilla, P., Fern´andez, P., Bartoli, A., Davidson, A.J.: KAZE Features. In:

18–35

CVPR. (June 2003) 257–263

ECCV. (2012)

37. Tola, E., Lepetit, V., Fua, P.: Daisy: An Eﬃcient Dense Descriptor Applied to

Wide Baseline Stereo. PAMI 32(5) (2010) 815–830

38. Bellavia, F., Tegolo, D.: Improving Sift-Based Descriptors Stability to Rotations.

39. Mikolajczyk, K., Schmid, C.: Scale and Aﬃne Invariant Interest Point Detectors.

In: ICPR. (2010)

IJCV 60 (2004) 63–86

40. Fan, B., Wu, F., Hu, Z.: Aggregating Gradient Distributions into Intensity Orders:

A Novel Local Image Descriptor. In: CVPR. (2011)

41. Wang, Z., Fan, B., Wu, F.: Local Intensity Order Pattern for Feature Description.

In: ICCV. (2011)

42. Zitnick, C.: Binary Coherent Edge Descriptors. In: ECCV. (2010)
43. Leutenegger, S., Chli, M., Siegwart, R.: BRISK: Binary Robust Invariant Scalable

44. Alahi, A., Ortiz, R., Vandergheynst, P.: FREAK: Fast Retina Keypoint. In: CVPR.

Keypoints. In: ICCV. (2011)

(2012)

45. Felzenszwalb, P., Girshick, R., McAllester, D., Ramanan, D.: Object Detection

with Discriminatively Trained Part Based Models. PAMI (2010)

6
1
0
2
 
l
u
J
 
9
2
 
 
]

V
C
.
s
c
[
 
 
2
v
4
1
1
9
0
.
3
0
6
1
:
v
i
X
r
a

LIFT: Learned Invariant Feature Transform

Kwang Moo Yi∗,1, Eduard Trulls∗,1, Vincent Lepetit2, Pascal Fua1

1Computer Vision Laboratory, Ecole Polytechnique F´ed´erale de Lausanne (EPFL)
2Institute for Computer Graphics and Vision, Graz University of Technology
{kwang.yi, eduard.trulls, pascal.fua}@epfl.ch, lepetit@icg.tugraz.at

Abstract. We introduce a novel Deep Network architecture that imple-
ments the full feature point handling pipeline, that is, detection, orienta-
tion estimation, and feature description. While previous works have suc-
cessfully tackled each one of these problems individually, we show how to
learn to do all three in a uniﬁed manner while preserving end-to-end dif-
ferentiability. We then demonstrate that our Deep pipeline outperforms
state-of-the-art methods on a number of benchmark datasets, without
the need of retraining.

Keywords: Local Features, Feature Descriptors, Deep Learning

1 Introduction

Local features play a key role in many Computer Vision applications. Find-
ing and matching them across images has been the subject of vast amounts of
research. Until recently, the best techniques relied on carefully hand-crafted fea-
tures [1,2,3,4,5]. Over the past few years, as in many areas of Computer Vision,
methods based in Machine Learning, and more speciﬁcally Deep Learning, have
started to outperform these traditional methods [6,7,8,9,10].

These new algorithms, however, address only a single step in the complete
processing chain, which includes detecting the features, computing their orienta-
tion, and extracting robust representations that allow us to match them across
images. In this paper we introduce a novel Deep architecture that performs all
three steps together. We demonstrate that it achieves better overall performance
than the state-of-the-art methods, in large part because it allows these individual
steps to be optimized to perform well in conjunction with each other.

Our architecture, which we refer to as LIFT for Learned Invariant Feature
Transform, is depicted by Fig. 1. It consists of three components that feed into
each other: the Detector, the Orientation Estimator, and the Descriptor. Each
one is based on Convolutional Neural Networks (CNNs), and patterned after
recent ones [6,9,10] that have been shown to perform these individual functions
well. To mesh them together we use Spatial Transformers [11] to rectify the

∗ First two authors contributed equally.
This work was supported in part by the EU FP7 project MAGELLAN under grant
number ICT-FP7-611526.

2

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Fig. 1. Our integrated feature extraction pipeline. Our pipeline consists of three major
components: the Detector, the Orientation Estimator, and the Descriptor. They are
tied together with diﬀerentiable operations to preserve end-to-end diﬀerentiability.1

image patches given the output of the Detector and the Orientation Estimator.
We also replace the traditional approaches to non-local maximum suppression
(NMS) by the soft argmax function [12]. This allows us to preserve end-to-end
diﬀerentiability, and results in a full network that can still be trained with back-
propagation, which is not the case of any other architecture we know of.

Also, we show how to learn such a pipeline in an eﬀective manner. To this
end, we build a Siamese network and train it using the feature points produced
by a Structure-from-Motion (SfM) algorithm that we ran on images of a scene
captured under diﬀerent viewpoints and lighting conditions, to learn its weights.
We formulate this training problem on image patches extracted at diﬀerent scales
to make the optimization tractable. In practice, we found it impossible to train
the full architecture from scratch, because the individual components try to op-
timize for diﬀerent objectives. Instead, we introduce a problem-speciﬁc learning
approach to overcome this problem. It involves training the Descriptor ﬁrst,
which is then used to train the Orientation Estimator, and ﬁnally the Detector,
based on the already learned Descriptor and Orientation Estimator, diﬀerenti-
ating through the entire network. At test time, we decouple the Detector, which
runs over the whole image in scale space, from the Orientation Estimator and
Descriptor, which process only the keypoints.

In the next section we brieﬂy discuss earlier approaches. We then present our
approach in detail and show that it outperforms many state-of-the-art methods.

2 Related work

The amount of literature relating to local features is immense, but it always
revolves about ﬁnding feature points, computing their orientation, and matching
them. In this section, we will therefore discuss these three elements separately.

2.1 Feature Point Detectors

Research on feature point detection has focused mostly on ﬁnding distinctive lo-
cations whose scale and rotation can be reliably estimated. Early works [13,14]

1 Figures are best viewed in color.

LIFT: Learned Invariant Feature Transform

3

used ﬁrst-order approximations of the image signal to ﬁnd corner points in im-
ages. FAST [15] used Machine Learning techniques but only to speed up the
process of ﬁnding corners. Other than corner points, SIFT [1] detect blobs in
scale-space; SURF [2] use Haar ﬁlters to speed up the process; Maximally Sta-
ble Extremal Regions (MSER) [16] detect regions; [17] detect aﬃne regions.
SFOP [18] use junctions and blobs, and Edge Foci [19] use edges for robustness
to illumination changes. More recently, feature points based on more sophisti-
cated and carefully designed ﬁlter responses [5,20] have also been proposed to
further enhance the performance of feature point detectors.

In contrast to these approaches that focus on better engineering, and follow-
ing the early attempts in learning detectors [21,22], [6] showed that a detector
could be learned to deliver signiﬁcantly better performance than the state-of-
the-art. In this work, piecewise-linear convolutional ﬁlters are learned to robustly
detect feature points in spite of lighting and seasonal changes. Unfortunately, this
was done only for a single scale and from a dataset without viewpoint changes.
We therefore took our inspiration from it but had to extend it substantially to
incorporate it into our pipeline.

2.2 Orientation Estimation

Despite the fact that it plays a critical role in matching feature points, the
problem of estimating a discriminative orientation has received noticeably less
attention than detection or feature description. As a result, the method intro-
duced by SIFT [1] remains the de facto standard up to small improvements, such
as the fact that it can be sped-up by using the intensity centroid, as in ORB [4].
A departure from this can be found in a recent paper [9] that introduced a
Deep Learning-based approach to predicting stable orientations. This resulted
in signiﬁcant gains over the state-of-the-art. We incorporate this architecture
into our pipeline and show how to train it using our problem-speciﬁc training
strategy, given our learned descriptors.

2.3 Feature Descriptors

Feature descriptors are designed to provide discriminative representations of
salient image patches, while being robust to transformations such as viewpoint
or illumination changes. The ﬁeld reached maturity with the introduction of
SIFT [1], which is computed from local histograms of gradient orientations, and
SURF [2], which uses integral image representations to speed up the computa-
tion. Along similar lines, DAISY [3] relies on convolved maps of oriented gra-
dients to approximate the histograms, which yields large computational gains
when extracting dense descriptors.

Even though they have been extremely successful, these hand-crafted de-
scriptors can now be outperformed by newer ones that have been learned. These
range from unsupervised hashing to supervised learning techniques based on
linear discriminant analysis [23,24], genetic algorithm [25], and convex optimiza-
tion [26]. An even more recent trend is to extract features directly from raw image

4

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Fig. 2. Our Siamese training architecture with four branches, which takes as input
a quadruplet of patches: Patches P1 and P2 (blue) correspond to diﬀerent views of
the same physical point, and are used as positive examples to train the Descriptor;
P3 (green) shows a diﬀerent 3D point, which serves as a negative example for the
Descriptor; and P4 (red) contains no distinctive feature points and is only used as a
negative example to train the Detector. Given a patch P, the Detector, the softargmax,
and the Spatial Transformer layer Crop provide all together a smaller patch p inside P.
p is then fed to the Orientation Estimator, which along with the Spatial Transformer
layer Rot, provides the rotated patch pθ that is processed by the Descriptor to obtain
the ﬁnal description vector d.

patches with CNNs trained on large volumes of data. For example, MatchNet [7]
trained a Siamese CNN for feature representation, followed by a fully-connected
network to learn the comparison metric. DeepCompare [8] showed that a net-
work that focuses on the center of the image can increase performance. The
approach of [27] relied on a similar architecture to obtain state-of-the-art results
for narrow-baseline stereo. In [10], hard negative mining was used to learn com-
pact descriptors that use on the Euclidean distance to measure similarity. The
algorithm of [28] relied on sample triplets to mine hard negatives.

In this work, we rely on the architecture of [10] because the corresponding
descriptors are trained and compared with the Euclidean distance, which has a
wider range of applicability than descriptors that require a learned metric.

3 Method

In this section, we ﬁrst formulate the entire feature detection and description
pipeline in terms of the Siamese architecture depicted by Fig. 2. Next, we discuss
the type of data we need to train our networks and how to collect it. We then
describe the training procedure in detail.

3.1 Problem formulation

We use image patches as input, rather than full images. This makes the learning
scalable without loss of information, as most image regions do not contain key-
points. The patches are extracted from the keypoints used by a SfM pipeline, as
will be discussed in Section 3.2. We take them to be small enough that we can

LIFT: Learned Invariant Feature Transform

5

assume they contain only one dominant local feature at the given scale, which
reduces the learning process to ﬁnding the most distinctive point in the patch.
To train our network we create the four-branch Siamese architecture pictured
in Fig. 2. Each branch contains three distinct CNNs, a Detector, an Orientation
Estimator, and a Descriptor. For training purposes, we use quadruplets of image
patches. Each one includes two image patches P1 and P2, that correspond to
diﬀerent views of the same 3D point, one image patch P3, that contains the pro-
jection of a diﬀerent 3D point, and one image patch P4 that does not contain any
distinctive feature point. During training, the i-th patch Pi of each quadruplet
will go through the i-th branch.

To achieve end-to-end diﬀerentiability, the components of each branch are

connected as follows:

1. Given an input image patch P, the Detector provides a score map S.
2. We perform a soft argmax [12] on the score map S and return the location

x of a single potential feature point.

3. We extract a smaller patch p centered on x with the Spatial Transformer
layer Crop (Fig. 2). This serves as the input to the Orientation Estimator.

4. The Orientation Estimator predicts a patch orientation θ.
5. We rotate p according to this orientation using a second Spatial Transformer

layer, labeled as Rot in Fig. 2, to produce pθ.

6. pθ is fed to the Descriptor network, which computes a feature vector d.

Note that the Spatial Transformer layers are used only to manipulate the
image patches while preserving diﬀerentiability. They are not learned modules.
Also, both the location x proposed by the Detector and the orientation θ for
the patch proposal are treated implicitly, meaning that we let the entire network
discover distinctive locations and stable orientations while learning.

Since our network consists of components with diﬀerent purposes, learning
the weights is non-trivial. Our early attempts at training the network as a whole
from scratch were unsuccessful. We therefore designed a problem-speciﬁc learn-
ing approach that involves learning ﬁrst the Descriptor, then the Orientation
Estimator given the learned descriptor, and ﬁnally the Detector, conditioned on
the other two. This allows us to tune the Orientation Estimator for the Descrip-
tor, and the Detector for the other two components.

We will elaborate on this learning strategy in Secs. 3.3 (Descriptor), 3.4 (Ori-
entation Estimator), and 3.5 (Detector), that is, in the order they are learned.

3.2 Creating the Training Dataset

There are datasets that can be used to train feature descriptors [24] and orien-
tation estimators [9]. However it is not so clear how to train a keypoint detec-
tor, and the vast majority of techniques still rely on hand-crafted features. The
TILDE detector [6] is an exception, but the training dataset does not exhibit
any viewpoint changes.

To achieve invariance we need images that capture views of the same scene
under diﬀerent illumination conditions and seen from diﬀerent perspectives. We

6

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Fig. 3. Sample images and patches from Piccadilly (left) and Roman-Forum (right).
Keypoints that survive the SfM pipeline are drawn in blue, and the rest in red.

thus turned to photo-tourism image sets. We used the collections from Piccadilly
Circus in London and the Roman Forum in Rome from [29] to reconstruct the
3D using VisualSFM [30], which relies of SIFT features. Piccadilly contains 3384
images, and the reconstruction has 59k unique points with an average of 6.5 ob-
servations for each. Roman-Forum contains 1658 images and 51k unique points,
with an average of 5.2 observations for each. Fig. 3 shows some examples.

We split the data into training and validation sets, discarding views of train-
ing points on the validation set and vice-versa. To build the positive training
samples we consider only the feature points that survive the SfM reconstruction
process. To extract patches that do not contain any distinctive feature point,
as required by our training method, we randomly sample image regions that
contain no SIFT features, including those that were not used by SfM.

We extract grayscale training patches according to the scale σ of the point,
for both feature and non-feature point image regions. Patches P are extracted
from a 24σ × 24σ support region at these locations, and standardized into S × S
pixels where S = 128. The smaller patches p and pθ that serve as input to the
Orientation Estimator and the Descriptor, are cropped and rotated versions of
these patches, each having size s×s, where s = 64. The smaller patches eﬀectively
correspond to the SIFT descriptor support region size of 12σ. To avoid biasing
the data, we apply uniform random perturbations to the patch location with a
range of 20% (4.8σ). Finally, we normalize the patches with the grayscale mean
and standard deviation of the entire training set.

3.3 Descriptor

Learning feature descriptors from raw image patches has been extensively re-
searched during the past year [7,8,10,27,28,31], with multiple works reporting
impressive results on patch retrieval, narrow baseline stereo, and matching non-
rigid deformations. Here we rely on the relatively simple networks of [10], with
three convolutional layers followed by hyperbolic tangent units, l2 pooling [32]
and local subtractive normalization, as they do not require learning a metric.

The Descriptor can be formalized simply as

d = hρ(pθ) ,

(1)

LIFT: Learned Invariant Feature Transform

7

where h(.) denotes the Descriptor CNN, ρ its parameters, and pθ is the rotated
patch from the Orientation Estimator. When training the Descriptor, we do not
yet have the Detector and the Orientation Estimator trained. We therefore use
the image locations and orientations of the feature points used by the SfM to
generate image patches pθ.

We train the Descriptor by minimizing the sum of the loss for pairs of cor-
(cid:1) and the loss for pairs of non-corresponding patches
(cid:1) is deﬁned as the hinge embedding loss of the

responding patches (cid:0)p1
(cid:0)p1
θ , pl
θ
Euclidean distance between their description vectors. We write

(cid:1). The loss for pair (cid:0)pk

θ, p2
θ

θ, p3
θ

Ldesc(pk

θ , pl

θ) =

(cid:40)(cid:13)

(cid:1) − hρ
(cid:0)pk
(cid:13)hρ
max (cid:0)0, C − (cid:13)

(cid:0)pl
(cid:13)hρ

θ

θ

(cid:1)(cid:13)
(cid:13)2
(cid:1) − hρ
(cid:0)pk

θ

(cid:0)pl

θ

(cid:1)(cid:13)
(cid:13)2

(cid:1)

for positive pairs, and
for negative pairs ,

(2)
where positive and negative samples are pairs of patches that do or do not
correspond to the same physical 3D points, (cid:107)·(cid:107)2 is the Euclidean distance, and
C = 4 is the margin for embedding.

We use hard mining during training, which was shown in [10] to be critical for
descriptor performance. Following this methodology, we forward Kf sample pairs
and use only the Kb pairs with the highest training loss for back-propagation,
where r = Kf /Kb ≥ 1 is the ‘mining ratio’. In [10] the network was pre-trained
without mining and then ﬁne-tuned with r = 8. Here, we use an increasing
mining scheme where we start with r = 1 and double the mining ratio every
5000 batches. We use balanced batches with 128 positive pairs and 128 negative
pairs, mining each separately.

3.4 Orientation Estimator

Our Orientation Estimator is inspired by that of [9]. However, this speciﬁc one
requires pre-computations of description vectors for multiple orientations to com-
pute numerically the Jacobian of the method parameters with respect to orien-
tations. This is a critical limitation for us because we treat the output of the
detector component implicitly throughout the pipeline and it is thus not possible
to pre-compute the description vectors.

We therefore propose to use Spatial Transformers [11] instead to learn the
orientations. Given a patch p from the region proposed by the detector, the
Orientation Estimator predicts an orientation

θ = gφ(p) ,

(3)

where g denotes the Orientation Estimator CNN, and φ its parameters.

Together with the location x from the Detector and P the original image
patch, θ is then used by the second Spatial Transformer Layer Rot(.) to provide
a patch pθ = Rot (P, x, θ), which is the rotated version of patch p.

We train the Orientation Estimator to provide the orientations that minimize
the distances between description vectors for diﬀerent views of the same 3D
points. We use the already trained Descriptor to compute the description vectors,

8

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

and as the Detector is still not trained, we use the image locations from SfM.
More formally, we minimize the loss for pairs of corresponding patches, deﬁned
as the Euclidean distance between their description vectors

Lorientation(P1, x1, P2, x2) = (cid:13)

(cid:13)hρ(G(P1, x1)) − hρ(G(P2, x2))(cid:13)

(cid:13)2 ,

(4)

where G(P, x) is the patch centered on x after orientation correction: G(P, x) =
Rot(P, x, gφ(Crop(P, x))). This complex notation is necessary to properly han-
dle the cropping of the image patches. Recall that pairs (P1, P2) comprise image
patches containing the projections of the same 3D point, and locations x1 and
x2 denote the reprojections of these 3D points. As in [9], we do not use pairs
that correspond to diﬀerent physical points whose orientations are not related.

3.5 Detector

The Detector takes an image patch as input, and returns a score map. We imple-
ment it as a convolution layer followed by piecewise linear activation functions,
as in TILDE [6]. More precisely, the score map S for patch P is computed as:

S = fµ(P) =

(Wmn ∗ P + bmn) ,

(5)

N
(cid:88)

n

δn

M
max
m

where fµ(P) denotes the Detector itself with parameters µ, δn is +1 if n is odd
and −1 otherwise, µ is made of the ﬁlters Wmn and biases bmn of the convolution
layer to learn, ∗ denotes the convolution operation, and N and M are hyper-
parameters controlling the complexity of the piecewise linear activation function.
The main diﬀerence with TILDE lies in the way we train this layer. To let
S have maxima in places other than a ﬁxed location retrieved by SfM, we treat
this location implicitly, as a latent variable. Our method can potentially discover
points that are more reliable and easier to learn, whereas [6] cannot. Incidentally,
in our early experiments, we noticed that it was harmful to force the Detector
to optimize directly for SfM locations.

From the score map S, we obtain the location x of a feature point as

x = softargmax (S) ,

where softargmax is a function which computes the Center of Mass with the
weights being the output of a standard softmax function [12]. We write

softargmax (S) =

(cid:80)

y exp(βS(y))y
y exp(βS(y))

(cid:80)

,

(6)

(7)

where y are locations in S, and β = 10 is a hyper-parameter controlling the
smoothness of the softargmax. This softargmax function acts as a diﬀerentiable
version of non-maximum suppression. x is given to the ﬁrst Spatial Trans-
former Layer Crop(.) together with the patch P to extract a smaller patch
p = Crop (P, x) used as input to the Orientation Estimator.

LIFT: Learned Invariant Feature Transform

9

As the Orientation Estimator and the Descriptor have been learned by this
point, we can train the Detector given the full pipeline. To optimize over the
parameters µ, we minimize the distances between description vectors for the
pairs of patches that correspond to the same physical points, while maximizing
the classiﬁcation score for patches not corresponding to the same physical points.
More exactly, given training quadruplets (cid:0)P1, P2, P3, P4(cid:1), where P1 and P2
correspond to the same physical point, P1 and P3 correspond to diﬀerent SfM
points, and P4 to a non-feature point location, we minimize the sum of their
loss functions

Ldetector(P1, P2, P3, P4) = γLclass(P1, P2, P3, P4) + Lpair(P1, P2) ,

(8)

where γ is a hyper-parameter balancing the two terms in this summation

Lclass(P1, P2, P3, P4) =

αi max (cid:0)0, (cid:0)1 − softmax (cid:0)fµ

(cid:0)Pi(cid:1)(cid:1) yi

(cid:1)(cid:1)2

,

(9)

4
(cid:88)

i=1

with yi = −1 and αi = 3/6 if i = 4, and yi = +1 and αi = 1/6 otherwise to
balance the positives and negatives. softmax is the log-mean-exponential softmax
function. We write

Lpair(P1, P2) = (cid:107) hρ(G(P1, softargmax(fµ(P1)))) −

hρ(G(P2, softargmax(fµ(P2))))

(cid:107)2 .

(10)

Note that the locations of the detected feature points x appear only implicitly
and are discovered during training. Furthermore, all three components are tied
in with the Detector learning. As with the Descriptor we use a hard mining
strategy, in this case with a ﬁxed mining ratio of r = 4.

In practice, as the Descriptor already learns some invariance, it can be hard
for the Detector to ﬁnd new points to learn implicitly. To let the Detector start
with an idea of the regions it should ﬁnd, we ﬁrst constrain the patch proposals
p = Crop(P, softargmax(fµ(P))) that correspond to the same physical points
to overlap. We then continue training the Detector without this constraint.

Speciﬁcally, when pre-training the Detector, we replace Lpair in Eq. (8) with
˜Lpair, where ˜Lpair is equal to 0 when the patch proposals overlap exactly, and
increases with the distance between them otherwise. We therefore write

˜Lpair(P1, P2) = 1 −

p1 ∩ p2
p1 ∪ p2 +

max (cid:0)0, (cid:13)

(cid:13)x1 − x2(cid:13)
(cid:112)p1 ∪ p2

(cid:13)1 − 2s(cid:1)

,

(11)

where xj = softargmax(fµ(Pj)), pj = Crop(Pj, xj), (cid:107)·(cid:107)1 is the l1 norm. Recall
that s = 64 pixels is the width and height of the patch proposals.

3.6 Runtime pipeline

The pipeline used at run-time is shown in Fig. 4. As our method is trained on
patches, simply applying it over the image would require the network to be tested

10

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Fig. 4. An overview of our runtime architecture. As the Orientation Estimator and the
Descriptor only require evaluation at local maxima, we decouple the Detector and run it
in scale space with traditional NMS to obtain proposals for the two other components.

with a sliding window scheme over the whole image. In practice, this would be
too expensive. Fortunately, as the Orientation Estimator and the Descriptor only
need to be run at local maxima, we can simply decouple the detector from the
rest to apply it to the full image, and replace the softargmax function by NMS,
as outlined in red in Fig. 4. We then apply the Orientation Estimator and the
Descriptor only to the patches centered on local maxima.

More exactly, we apply the Detector independently to the image at diﬀerent
resolutions to obtain score maps in scale space. We then apply a traditional NMS
scheme similar to that of [1] to detect feature point locations.

4 Experimental validation

In this section, we ﬁrst present the datasets and metrics we used. We then present
qualitative results, followed by a thorough quantitative comparison against a
number of state-of-the-art baselines, which we consistently outperform.

Finally, to better understand what elements of our approach most contribute
to this result, we study the importance of the pre-training of the Detector com-
ponent, discussed in Section 3.5, and analyze the performance gains attributable
to each component.

4.1 Dataset and Experimental Setup

We evaluate our pipeline on three standard datasets:

– The Strecha dataset [33], which contains 19 images of two scenes seen from

increasingly diﬀerent viewpoints.

– The DTU dataset [34], which contains 60 sequences of objects with diﬀerent
viewpoints and illumination settings. We use this dataset to evaluate our
method under viewpoint changes.

– The Webcam dataset [6], which contains 710 images of 6 scenes with strong
illumination changes but seen from the same viewpoint. We use this dataset
to evaluate our method under natural illumination changes.

LIFT: Learned Invariant Feature Transform

11

For Strecha and DTU we use the provided ground truth to establish corre-
spondences across viewpoints. We use a maximum of 1000 keypoints per image,
and follow the standard evaluation protocol of [35] on the common viewpoint
region. This lets us evaluate the following metrics.

– Repeatability (Rep.): Repeatability of feature points, expressed as a ratio.
This metric captures the performance of the feature point detector by report-
ing the ratio of keypoints that are found consistently in the shared region.
– Nearest Neighbor mean Average Precision (NN mAP): Area Under Curve
(AUC) of the Precision-Recall curve, using the Nearest Neighbor match-
ing strategy. This metric captures how discriminating the descriptor is by
evaluating it at multiple descriptor distance thresholds.

– Matching Score (M. Score): The ratio of ground truth correspondences that
can be recovered by the whole pipeline over the number of features proposed
by the pipeline in the shared viewpoint region. This metric measures the
overall performance of the pipeline.

We compare our method on the three datasets to the following combination of
feature point detectors and descriptors, as reported by the authors of the corre-
sponding papers: SIFT [1], SURF [2], KAZE [36], ORB [4], Daisy [37] with SIFT
detector, sGLOH [38] with Harris-aﬃne detector [39], MROGH [40] with Harris-
aﬃne detector, LIOP [41] with Harris-aﬃne detector, BiCE [42] with Edge Foci
detector [19], BRISK [43], FREAK [44] with BRISK detector, VGG [26] with
SIFT detector, DeepDesc [10] with SIFT detector, PN-Net [28] with SIFT detec-
tor, and MatchNet [7] with SIFT detector. We also consider SIFT with Hessian-
Aﬃne keypoints [17]. For the learned descriptors VGG, DeepDesc, PN-Net and
MatchNet we use SIFT keypoints because they are trained using a dataset cre-
ated with Diﬀerence-of-Gaussians, which is essentially the same as SIFT. In the
case of Daisy, which was not developed for a speciﬁc detector, we also use SIFT
keypoints. To make our results reproducible, we provide additional implementa-
tion details for LIFT and the baselines in the supplementary material.2

4.2 Qualitative Examples

Fig. 5 shows image matching results with 500 feature points, for both SIFT
and our LIFT pipeline trained with Piccadilly. As expected, LIFT returns more
correct correspondences across the two images. One thing to note is that the two
DTU scenes in the bottom two rows are completely diﬀerent from the photo-
tourism datasets we used for training. Given that the two datasets are very
diﬀerent, this shows good generalization properties.

4.3 Quantitative Evaluation of the Full Pipeline

Fig. 6 shows the average matching score for all three datasets, and Table 1
provides the exact numbers for the two LIFT variants. LIFT (pic) is trained

2 Source and models will be available at https://github.com/cvlab-epfl/LIFT.

12

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Fig. 5. Qualitative local feature matching examples of left: SIFT and right: our
method LIFT. Correct matches recovered by each method are shown in green lines and
the descriptor support regions with red circles. Top row: Herz-Jesu-P8 of Strecha,
second row: Frankfurt of Webcam, third row: Scene 7 of DTU and bottom row:
Scene 19 of DTU. Note that the images are very diﬀerent from one another.

with Piccadilly and LIFT (rf) with Roman-Forum. Both of our learned models
signiﬁcantly outperform the state-of-the-art on Strecha and DTU and achieve
state-of-the-art on Webcam. Note that KAZE, which is the best performing
competitor on Webcam, performs poorly on the other two datasets. As discussed
above, Piccadilly and Roman-Forum are very diﬀerent from the datasets used
for testing. This underlines the strong generalization capability of our approach,
which is not always in evidence with learning-based methods.

Interestingly, on DTU, SIFT is still the best performing method among the
competitors, even compared to methods that rely on Deep Learning, such as
DeepDesc and PN-Net. Also, the gap between SIFT and the learning-based
VGG, DeepDesc, and PN-Net is not large for the Strecha dataset.

These results show that although a component may outperform another
method when evaluated individually, they may fail to deliver their full potential
when integrated into the full pipeline, which is what really matters. In other
words, it is important to learn the components together, as we do, and to con-
sider the whole pipeline when evaluating feature point detectors and descriptors.

4.4 Performance of Individual Components

Fine-tuning the Detector. Recall that we pre-train the detector and then
ﬁnalize the training with the Orientation Estimator and the Descriptor, as dis-

LIFT: Learned Invariant Feature Transform

13

Table 1. Average matching score for all baselines.

SIFT SIFT-HesAﬀ SURF

ORB

Daisy

sGLOH MROGH LIOP

BiCE

Strecha
DTU

.283
.272
Webcam .128

Strecha
DTU

.208
.193
Webcam .118

BRISK FREAK

VGG MatchNet DeepDesc PN-Net KAZE LIFT-pic LIFT-rf

.314
.274
.164

.183
.186
.116

.208
.244
.117

.300
.271
.118

.157
.127
.120

.223
.198
.101

.272
.262
.120

.298
.257
.116

.207
.187
.113

.300
.267
.114

.239
.223
.125

.250
.213
.195

.211
.189
.086

.270
.242
.166

.374
.317
.196

.369
.308
.202

Fig. 6. Average matching score for all baselines.

cussed in Section 3.5. It is therefore interesting to see the eﬀect of this ﬁnalizing
stage. In Table 2 we evaluate the entire pipeline with the pre-trained Detector
and the ﬁnal Detector. As the pair-wise loss term ˜Lpair of Eq. (11) is designed
to emulate the behavior of an ideal descriptor, the pre-trained Detector already
performs well. However, the full training pushes the performance slightly higher.
A closer look at Table 2 reveals that gains are larger overall for Piccadilly
than for Roman-Forum. This is probably due to the fact that Roman-Forum
does not have many non-feature point regions. In fact, the network started to
over-ﬁt quickly after a few iterations on this dataset. The same happened when
we further tried to ﬁne-tune the full pipeline as a whole, suggesting that our
learning strategy is already providing a good global solution.

Performance of individual components. To understand the inﬂuence of
each component on the overall performance, we exchange them with their SIFT
counterparts, for both LIFT (pic) and LIFT (rf), on Strecha. We report the
results in Table 3. In short, each time we exchange to SIFT, we decrease per-
formance, thus showing that each element of the pipeline plays and important
role. Our Detector gives higher repeatability for both models. Having better ori-
entations also helps whichever detector or descriptor is being used, and also the
Deep Descriptors perform better than SIFT.

One thing to note is that our Detector is not only better in terms of repeata-
bility, but generally better in terms of both the NN mAP, which captures the

14

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

Table 2. Results on Strecha for both LIFT models trained on Piccadilly and Roman-
Forum, with the pre-trained and fully-trained Detector.

Trained on Piccadilly

Trained on Roman-Forum

Pre-trained
Fully-trained

Rep.
.436
.446

M.Score
.367
.374

Rep.
.447
.447

M.Score
.368
.369

Table 3. Results on Strecha for both LIFT models trained on Piccadilly and Roman-
Forum, interchanging our components with their SIFT counterparts.

Det.

Ori.

Desc. Rep. NN mAP M.Score Rep. NN mAP M.Score

Trained on Piccadilly

Trained on Roman-Forum

.428

.428

SIFT SIFT SIFT

SIFT Ours

SIFT

SIFT SIFT Ours

SIFT Ours Ours

Ours

SIFT SIFT

Ours Ours

SIFT

.446

Ours

SIFT Ours

.517

.671

.568

.685

.540

.644

.629

.282

.341

.290

.344

.325

.372

.339

.447

.517

.662

.581

.688

.545

.630

.644

.282

.338

.295

.342

.319

.360

.337

Ours Ours Ours

.446

.686

.374

.447

.683

.369

descriptor performance, and in terms of matching score, which evaluates the full
pipeline. This shows that our Detector learns to ﬁnd not only points that can be
found often but also points that can be matched easily, indicating that training
the pipeline as a whole is important for optimal performance.

5 Conclusion

We have introduced a novel Deep Network architecture that combines the three
components of standard pipelines for local feature detection and description
into a single diﬀerentiable network. We used Spatial Transformers together with
the softargmax function to mesh them together into a uniﬁed network that
can be trained end-to-end with back-propagation. While this makes learning
the network from scratch theoretically possible, it is not practical. We therefore
proposed an eﬀective strategy to train it.

Our experimental results demonstrate that our integrated approach outper-
forms the state-of-the-art. To further improve performance, we will look into
strategies that allow us to take advantage even more eﬀectively of our ability to
train the network as a whole. In particular, we will look into using hard negative
mining strategies over the whole image [45] instead of relying on pre-extracted
patches. This has the potential of producing more discriminative ﬁlters and,
consequently, better descriptors.

LIFT: Learned Invariant Feature Transform

15

References

(2004)

1. Lowe, D.: Distinctive Image Features from Scale-Invariant Keypoints. IJCV 20(2)

2. Bay, H., Ess, A., Tuytelaars, T., Van Gool, L.: SURF: Speeded Up Robust Features.

3. Tola, E., Lepetit, V., Fua, P.: A Fast Local Descriptor for Dense Matching. In:

CVIU 10(3) (2008) 346–359

CVPR. (2008)

4. Rublee, E., Rabaud, V., Konolidge, K., Bradski, G.: ORB: An Eﬃcient Alternative

to SIFT or SURF. In: ICCV. (2011)

5. Mainali, P., Lafruit, G., Tack, K., Van Gool, L., Lauwereins, R.: Derivative-Based
Scale Invariant Image Feature Detector with Error Resilience. TIP 23(5) (2014)
2380–2391

6. Verdie, Y., Yi, K.M., Fua, P., Lepetit, V.: TILDE: A Temporally Invariant Learned

DEtector. In: CVPR. (2015)

7. Han, X., Leung, T., Jia, Y., Sukthankar, R., Berg, A.C.: MatchNet: Unifying

Feature and Metric Learning for Patch-Based Matching. In: CVPR. (2015)

8. Zagoruyko, S., Komodakis, N.: Learning to Compare Image Patches via Convolu-

tional Neural Networks. In: CVPR. (2015)

9. Yi, K., Verdie, Y., Lepetit, V., Fua, P.: Learning to Assign Orientations to Feature

Points. In: CVPR. (2016)

10. Simo-Serra, E., Trulls, E., Ferraz, L., Kokkinos, I., Fua, P., Moreno-Noguer, F.:
Discriminative Learning of Deep Convolutional Feature Point Descriptors.
In:
ICCV. (2015)

11. Jaderberg, M., Simonyan, K., Zisserman, A., Kavukcuoglu, K.: Spatial Transformer

Networks. In: NIPS. (2015)

12. Chapelle, O., Wu, M.: Gradient Descent Optimization of Smoothed Information

Retrieval Metrics. Information Retrieval 13(3) (2009) 216–235

13. Harris, C., Stephens, M.: A Combined Corner and Edge Detector.

In: Fourth

Alvey Vision Conference. (1988)

14. Moravec, H.: Obstacle Avoidance and Navigation in the Real World by a Seeing
In: tech. report CMU-RI-TR-80-03, Robotics Institute, Carnegie

Robot Rover.
Mellon University, Stanford University. (September 1980)

15. Rosten, E., Drummond, T.: Machine Learning for High-Speed Corner Detection.

In: ECCV. (2006)

16. Matas, J., Chum, O., Martin, U., Pajdla, T.: Robust Wide Baseline Stereo from
Maximally Stable Extremal Regions. In: BMVC. (September 2002) 384–393
17. Mikolajczyk, K., Schmid, C.: An Aﬃne Invariant Interest Point Detector.

In:

ECCV. (2002) 128–142

18. F¨orstner, W., Dickscheid, T., Schindler, F.: Detecting Interpretable and Accurate

Scale-Invariant Keypoints. In: ICCV. (September 2009)

19. Zitnick, C., Ramnath, K.: Edge Foci Interest Points. In: ICCV. (2011)
20. Mainali, P., Lafruit, G., Yang, Q., Geelen, B., Van Gool, L., Lauwereins, R.: SIFER:
Scale-Invariant Feature Detector with Error Resilience. IJCV 104(2) (2013) 172–
197

21. Sochman, J., Matas, J.: Learning a Fast Emulator of a Binary Decision Process.

22. Trujillo, L., Olague, G.: Using Evolution to Learn How to Perform Interest Point

In: ACCV. (2007) 236–245

Detection. In: ICPR. (2006) 211–214

16

K. M. Yi, E. Trulls, V. Lepetit, P. Fua

23. Strecha, C., Bronstein, A., Bronstein, M., Fua, P.: LDAHash: Improved Matching

with Smaller Descriptors. PAMI 34(1) (January 2012)

24. Winder, S., Brown, M.: Learning Local Image Descriptors. In: CVPR. (June 2007)
25. Perez, C., Olague, G.: Genetic Programming As Strategy for Learning Image

Descriptor Operators. Intelligent Data Analysis 17 (2013) 561–583

26. Simonyan, K., Vedaldi, A., Zisserman, A.: Learning Local Feature Descriptors

Using Convex Optimisation. PAMI (2014)

27. Zbontar, J., LeCun, Y.: Computing the Stereo Matching Cost with a Convolutional

Neural Network. In: CVPR. (2015)

28. Balntas, V., Johns, E., Tang, L., Mikolajczyk, K.: PN-Net: Conjoined Triple Deep

Network for Learning Local Image Descriptors. In: arXiv Preprint. (2016)

29. Wilson, K., Snavely, N.: Robust Global Translations with 1DSfM.

In: ECCV.

(2014)

30. Wu, C.: Towards Linear-Time Incremental Structure from Motion. In: 3DV. (2013)
31. Paulin, M., Douze, M., Harchaoui, Z., Mairal, J., Perronnin, F., Schmid, C.: Local
Convolutional Features with Unsupervised Training for Image Retrieval. In: ICCV.
(2015)

32. Sermanet, P., Chintala, S., LeCun, Y.: Convolutional Neural Networks Applied to

House Numbers Digit Classiﬁcation. In: ICPR. (2012)

33. Strecha, C., Hansen, W., Van Gool, L., Fua, P., Thoennessen, U.: On Benchmark-
ing Camera Calibration and Multi-View Stereo for High Resolution Imagery. In:
CVPR. (2008)

34. Aanaes, H., Dahl, A.L., Pedersen, K.S.: Interesting Interest Points. IJCV 97 (2012)

35. Mikolajczyk, K., Schmid, C.: A Performance Evaluation of Local Descriptors. In:

36. Alcantarilla, P., Fern´andez, P., Bartoli, A., Davidson, A.J.: KAZE Features. In:

18–35

CVPR. (June 2003) 257–263

ECCV. (2012)

37. Tola, E., Lepetit, V., Fua, P.: Daisy: An Eﬃcient Dense Descriptor Applied to

Wide Baseline Stereo. PAMI 32(5) (2010) 815–830

38. Bellavia, F., Tegolo, D.: Improving Sift-Based Descriptors Stability to Rotations.

39. Mikolajczyk, K., Schmid, C.: Scale and Aﬃne Invariant Interest Point Detectors.

In: ICPR. (2010)

IJCV 60 (2004) 63–86

40. Fan, B., Wu, F., Hu, Z.: Aggregating Gradient Distributions into Intensity Orders:

A Novel Local Image Descriptor. In: CVPR. (2011)

41. Wang, Z., Fan, B., Wu, F.: Local Intensity Order Pattern for Feature Description.

In: ICCV. (2011)

42. Zitnick, C.: Binary Coherent Edge Descriptors. In: ECCV. (2010)
43. Leutenegger, S., Chli, M., Siegwart, R.: BRISK: Binary Robust Invariant Scalable

44. Alahi, A., Ortiz, R., Vandergheynst, P.: FREAK: Fast Retina Keypoint. In: CVPR.

Keypoints. In: ICCV. (2011)

(2012)

45. Felzenszwalb, P., Girshick, R., McAllester, D., Ramanan, D.: Object Detection

with Discriminatively Trained Part Based Models. PAMI (2010)

