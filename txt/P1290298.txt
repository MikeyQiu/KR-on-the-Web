0
2
0
2
 
b
e
F
 
9
1
 
 
]

G
L
.
s
c
[
 
 
2
v
3
3
8
5
0
.
2
1
9
1
:
v
i
X
r
a

SPEECH-DRIVEN FACIAL ANIMATION USING POLYNOMIAL FUSION OF FEATURES

Triantafyllos Kefalas(cid:63)

Konstantinos Vougioukas(cid:63)
Jean Kossaiﬁ(cid:63)‡

Maja Pantic(cid:63)‡

Yannis Panagakis†

Stavros Petridis(cid:63)‡

(cid:63) Department of Computing, Imperial College London, UK
† Department of Informatics and Telecommunications, University of Athens, Greece
‡ Samsung AI Centre, Cambridge, UK

ABSTRACT
Speech-driven facial animation involves using a speech sig-
nal to generate realistic videos of talking faces. Recent deep
learning approaches to facial synthesis rely on extracting
low-dimensional representations and concatenating them,
followed by a decoding step of the concatenated vector. This
accounts for only ﬁrst-order interactions of the features and
ignores higher-order interactions. In this paper we propose
a polynomial fusion layer that models the joint representa-
tion of the encodings by a higher-order polynomial, with the
parameters modelled by a tensor decomposition. We demon-
strate the suitability of this approach through experiments on
generated videos evaluated on a range of metrics on video
quality, audiovisual synchronisation and generation of blinks.

Index Terms— multiview learning, tensor factorization,

deep learning, GAN, audiovisual learning

1. INTRODUCTION

The problem of speech-driven facial animation entails gen-
erating realistic videos of talking faces based on a speech
input [1, 2, 3, 4, 5]. Facial animation is an important part
of computer generated imagery (CGI) applications, such as
video games and virtual assistants [5, 2]. Considered one of
the most challenging problems in computer graphics, facial
animation involves the coordinated movement of hundreds of
muscles [5] and expresses a wide range of emotions, in ad-
dition to the underlying speech. Furthermore, generating re-
alistic videos requires synchronisation of lip movements with
speech as well as natural facial expressions such as blinks [2].
CGI traditionally employs face capture methods for facial
synthesis [2], which are costly and require manual labor. To
alleviate this, recent research has focused on automatic gen-
eration of video with machine learning [1, 2, 3, 4, 5]. MoCo-
GAN [4] models motion and content as separate latent spaces,
which are learned in an unsupervised way using both image
and video discriminators. The Speech2Vid model [3] gener-
ates a video of a target face which is lip synced with the audio
signal. This model uses CNNs on Mel-Frequency Cepstral
Coefﬁcients (MFCCs) of the audio input as well as a frame

deblurring network to preserve high-frequency visual content.
Vougioukas et al. [2] propose a deep architecture for speech-
driven facial synthesis that takes into account facial expres-
sion dynamics in the course of speech. This is achieved by
including a temporal noise input signal as well as by using
multiple discriminators to control for frame quality, audio-
visual synchronisation and overall video quality.
The aforementioned methods extract

low-dimensional
representations from the inputs and combine them with a
simple concatenation operation. Concatenating multiple in-
puts to produce a joint representation has several limitations,
such as accounting for only the ﬁrst-order interactions among
inputs and excluding higher-order interactions. In addition,
the statistics of the concatenated vector can be dominated
by a subset of the representations following any arbitrary
scaling of the latter. Furthermore, we often wish to capture
invariances in the data to perform a given task and a concate-
nated joint representation may not reﬂect this. For example,
in speaker identiﬁcation applications it has been observed
that concatenating audio and visual features makes the joint
representation sensitive to large facial movements [6].

In this work, we abstract away the concatenation opera-
tion into a new polynomial fusion layer that models the inter-
actions between the low-dimensional representations of the
multiple views (visual and audio). We propose modelling the
joint representation as a higher-order polynomial of the in-
puts, whose parameters are naturally represented by tensors.
To alleviate the exponential space complexity of tensors, we
model the parameters of the polynomial using tensor factor-
izations [7]. Previous work on polynomial fusion of features
[8] modelled within-feature interactions from a single view,
whereas our method focuses on between-feature interactions
from multiple views. To our knowledge, this is the ﬁrst work
in the literature that uses tensor factorizations and concepts
from multi-view learning [9] [10] to compute a joint repre-
sentation for a generative task.

1.1. Notation and preliminaries

The notation of [7] is adopted in this paper. The sets of
integers and real numbers are denoted by Z and R respec-

tively. Matrices and vectors are denoted by bold uppercase
(e.g. X ∈ RI1×I2 ) and lower case letters (e.g. x ∈ RI1)
respectively. Tensors are higher-dimensional generalisations
of vectors (ﬁrst-order tensors) and matrices (second-order
tensors) denoted by bold calligraphic capital letters (e.g. X ).
The order of a tensor is the number of indices required to ad-
dress its elements. A real-valued tensor X ∈ RI1×I2×...×IN
of order N is deﬁned over the tensor product of N vector
spaces, where In ∈ Z for n = 1, 2, ...N .

A set of N real matrices (vectors) of varying dimensional-
(cid:1).
ity is denoted by {X(n) ∈ RIn×Jn }N
An order-N tensor has rank-1 when it is expressed as the
outer product of N vectors, i.e. X = x(1) ◦ x(2) ◦ ... ◦ x(N )
where {x(n) ∈ RIn}N

(cid:0) {x(n) ∈ RIn }N

n=1

n=1

n=1.

k=1
k(cid:54)=n

The mode-n unfolding matrix of tensor X ∈ RI1×I2×...×IN
reorders the elements of the tensor into a matrix X(n) ∈
RIn× ¯In where ¯In = (cid:81)N
Ik. The mode-n vector product
of a tensor X ∈ RI1×I2×...×IN and a vector x ∈ RIn re-
sults to an order N − 1 tensor and is denoted by X ×n x ∈
RI1×I2×...×In−1×In+1×...×IN . The Kronecker product and
Khatri-Rao product, deﬁned in [7] are denoted by ⊗ and (cid:12)
respectively. Finally, the Frobenius norm is denoted by ||·||F .
The reader is referred to [7] for a detailed survey of tensors
and tensor notation.

2. METHODOLOGY

2.1. Pipeline overview

The pipeline follows an encoder-decoder structure, inspired
by [2], and is illustrated in Fig. 1. The three temporal en-
coders extract representations of the speaker identity, the au-
dio segment and spontaneous facial expressions. The poly-
nomial fusion layer combines the three encodings which are
then fed into the generator produce the frames.

The identity encoder takes as input a single face image
and passes it through a 6-layer CNN, where each layer has
strided 2D convolutions, batch normalization and ReLU ac-
tivation functions. The audio encoder receives a 0.2s audio
signal and passes it through a network of 1D convolutions
with batch normalization and ReLU activation functions, fol-
lowed by a 1-layer GRU. The noise encoder receives a gaus-
sian noise input which is passed through a 1-layer GRU. The
purpose of the noise encoder is to introduce some natural
variability in the face generation such as blinks and eyebrow
movements.

The polynomial fusion layer generates a joint representa-
tion of the three encodings which is then fed into the U-Net
based [11] generator, along with skip connections from the in-
termediate layers of the identity encoder, to produce a frame
from a video sequence.

Furthermore, we employ three CNN-based disciminators
during training to control for frame quality, plausibility of the
video (frame sequence) and audiovisual synchronisation.

2.2. Polynomial fusion layer

Let za ∈ Ra, zd ∈ Rd and zn ∈ Rn be the encoder out-
puts (encodings) for the audio, identity and noise respectively.
The polynomial fusion layer is a function that takes the audio
and identity encodings as input and returns a vector ˜z ∈ Rm,
which is the joint representation:

˜z = f (za, zd)

where m is a hyperparameter.

We propose modelling the interactions of the audio and

identity encodings using a higher-order polynomial:

˜z = b + W[a]za + W[d]zd + W [a,d] ×2 za ×3 zd

(1)

where b ∈ Rm is the global bias, W[a] ∈ Rm×a, W[d] ∈
Rm×d are the ﬁrst-order interaction matrices for audio and
identity respectively and W [a,d] ∈ Rm×a×d is the tensor of
second-order interactions between audio and identity. Finally,
we concatenate the noise embedding to the joint representa-
tion to obtain the input for the generator:

z = [˜z, zn]

(2)

where z ∈ Rc, c = m + n.

However, the number of parameters in (1) is exponential
with respect to the number of input embeddings. To reduce
the complexity, it is common practice to assume a low-rank
structure of the parameters which can be captured using ten-
sor factorizations [12]. We propose several ways to do this,
as follows:

Coupled matrix-tensor factorizations:
In this approach
we model the dependence between the ﬁrst-order and second-
order interactions by the sharing of column and/or row spaces.
By allowing the ﬁrst-order and second-order interactions to
share their column space (column mode) we can factorize (1)
as follows:

W[a] = UV[a]T
W[d] = UV[d]T
(4)
where U ∈ Rm×k describes the shared column space, V[a] ∈
Ra×k, V[d] ∈ Rd×k describe the audio-speciﬁc and identity
speciﬁc row spaces respectively.

(3)

Furthermore, we perform the Canonical Polyadic (CP)

[13] decomposition on W [a,d] deﬁned as:

W[a,d]

(1) = B(1)(B(3) (cid:12) B(2))T
where B(1) ∈ Rm×k, B(2) ∈ Ra×k, B(3) ∈ Rd×k are the
factor matrices. We also apply the constraint B(1) := U to
enable the sharing of the column space.

(5)

We can perform further parameter sharing by assuming
that the mode-2 and mode-3 factor matrices are equivalent to

the matrices describing the row spaces, i.e.: B(2) := V[a],
B(3) := V[d] (6b)

2.3. Objective function

Joint factorization of interactions:
In this approach, we
collect the parameters in (1) into a single parameter tensor
to perform joint factorization of all-order interactions. Let
W ∈ Rm×(a+1)×(d+1) be the single parameter tensor de-
ﬁned as follows: W:,a+1,d+1 := b, W:,1:a,d+1 := W[a],
W:,a+1,1:d := W[d], W:,1:a,1:d := W [a,d], where : indicates
all elements of a mode of the tensor and 1 : x indicates all
elements from the ﬁrst to the xth element of that mode.
Using W we then compute the joint representation:

˜z = W ×2 φ(za) ×3 φ(zd)

(6)

where φ(x) := [x, 1] concatenates a 1 to the end of the in-
put vector. The last step is required for the computation of
interactions of all orders as per [9].

We investigate the Canonical Polyadic (CP) [13] and
Tucker decompositions [14] respectively. The CP decompo-
sition for this model is deﬁned as follows:

W(1) = A(1)(A(3) (cid:12) A(2))T

(7)

where {A(n)}3
n=1 are the factor matrices where A(1) ∈
Rm×k, A(2) ∈ R(a+1)×k, A(3) ∈ R(d+1)×k, and k is the
tensor rank.

The Tucker decomposition can be expressed in tensor no-

tation as follows:

W = G ×1 U(1) ×2 U(2) ×3 U(3)

(8)

where G ∈ Rk1×k2×k3 is the Tucker core, {U(n)}3
n=1 are the
factor matrices where U(1) ∈ Rm×k1, U(2) ∈ R(a+1)×k2,
U(3) ∈ R(d+1)×k3 , and (k1, k2, k3) is the multilinear rank.

Let G denote the parameters of the polynomial fusion layer
and the generator. We assume that the encoders are pre-
trained and ﬁxed. Let D = {Df rame, Dseq, Dsync} denote
the parameters of the three discriminators (frame, video se-
quence, synchronisation). The adversarial losses for each
discriminator is are the following:

Lf rame
adv

= Ex∼Pd [log Df rame

(cid:0)S(x), x1
Ez∼Pz [log(1 − Df rame

(cid:1)]+

(cid:0)S(G(z)), x1

(cid:1)]

(9)

Lseq
adv = Ex∼Pd [log Dseq

(cid:0)x, a(cid:1)]+
Ez∼Pz [log(1 − Dseq

(cid:0)S(G(z)), a(cid:1)]

(10)

Lsync
adv = Ex∼Pd [log Dsync

(cid:1)]+

(cid:0)pin

Ex∼Pd

1
2
Ez∼Pz [log(1 − Dsync

+

1
2

(cid:2) log 1−Dsync(pout)(cid:3)

(cid:0)Ssnip(pf )(cid:1)]

(11)

where x is a sample video, S(x) is a sampling function, a is
the corresponding audio, {pin, pout} are in and out of sync
pairs of the ground truth video and pf is a generated video
and its corresponding audio. We also assign a corresponding
weight to each of the above adversarial losses to obtain the
overall adversarial loss:
Ladv := λf rameLf rame

adv + λsyncLsync
adv .

adv + λseqLseq

Furthermore, we include an l1-norm reconstruction loss,
LL1 on the bottom half of the frame to encourage the correct
mouth movements. Finally, we add a Frobenius norm penality
term, Ω applied to the parameters of the fusion layer.
The ﬁnal objective function is then the following:

min
G

max
D

Ladv + λ1LL1 + λ2Ω

(12)

where λ1, λ2, λf rame, λseq and λsync are hyperparameters.

3. EXPERIMENTAL EVALUATION

We conducted experiments on the GRID [15], CREMA-D
[16] and TCD TIMIT [17] datasets. GRID contains 33 speak-
ers uttering 1000 short structured sentences. CREMA-D con-
tains data from 91 actors uttering 12 sentences acted out for
different emotions. TCD TIMIT contains data from 59 speak-
ers uttering a selection of approximately 100 sentences from
the TIMIT [18] dataset.

We use a 50%-20%-30% training, validation and test set
split on the speakers of GRID, and a 70%-15%-15% datset
split for CREMA-D. For TCD TIMIT we use the recom-
mended split, excluding some test set speakers to be used as
validation.

3.1. Training protocol

We ran experiments on the above datasets using the following
methodologies for the polynomial fusion layer:

Fig. 1. Pipeline overview

Dataset

Method

Metrics

PSNR SSIM CPBD

AV Offset AV conﬁdence

blinks/sec

blink duration (sec.)

GRID

TCD-TIMIT

CREMA-D

PF-Tucker
PF-CP
PF-CMF
PF-CMF-SR
Vougioukas et al, 2019
Speech2Vid

PF-Tucker
PF-CP
PF-CMF
PF-CMF-SR
Vougioukas et al, 2019
Speech2Vid

PF-Tucker
PF-CP
PF-CMF
PF-CMF-SR
Vougioukas et al, 2019
Speech2Vid

27.360
27.619
27.529
27.608
27.100
22.662

23.958
24.403
24.095
23.871
24.243
20.305

23.272
23.502
23.277
23.457
23.565
22.190

0.831
0.863
0.836
0.833
0.818
0.720

0.722
0.737
0.723
0.719
0.730
0.658

0.691
0.698
0.689
0.697
0.700
0.700

0.260
0.259
0.268
0.253
0.268
0.255

0.283
0.304
0.286
0.285
0.308
0.211

0.221
0.233
0.208
0.214
0.216
0.217

ACD
1.11 x 10−4
1.18 x 10−4
8.63 x 10-5
9.74 x 10−5
1.47 x 10−4
1.48 x 10−4
2.69 x 10−4
2.04 x 10−4
2.65 x 10−4
2.94 x 10−4
1.76 x 10-4
1.81 x 10−4
1.96 x 10−4
1.69 x 10−4
1.73 x 10−4
1.63 x 10−4
1.40 x 10-4
1.73 x 10−4

1
1
1
1
1
1

1
1
1
1
1
1

2
2
2
2
2
1

7.1
7.3
7.4
7.3
7.4
5.3

5.5
5.6
5.5
5.2
5.5
4.6

5.1
5.1
5.1
5.3
5.5
4.7

0.37
0.32
0.33
0.37
0.45
0.00

0.13
0.11
0.10
0.07
0.19
0.00

0.25
0.17
0.38
0.18
0.25
0.00

0.38
0.36
0.36
0.32
0.36
0.00

0.31
0.28
0.30
0.31
0.33
0.00

0.39
0.37
0.35
0.32
0.26
0.00

Table 1. Evaluation summary of the proposed method in comparison with Vougioukas et al., 2019 and Speech2Vid

• Tucker decomposition as per (8) (PF-Tucker)
• CP decomposition as per (7) (PF-CP)
• Coupled matrix-tensor factorization as per (3), (4), (5)

(PF-CMF)

• Coupled matrix-tensor factorization with sharing of
row space matrices as per (3), (4), (5), (6b) (PF-CMF-
SR)

We set a = 256, d = 128, n = 10 and trained on video
sequences of 3 seconds with frame size 128 × 96 as per [2].
For all models, c = a + d + n = 394, implying m = 384,
given that c = m + n by construction. For the Tucker de-
composition above, we set (k1, k2, k3) = (192, 128, 64), cor-
responding to half the dimensionality along each mode. We
set k = d = 128 for all remaining methodologies.

For the three encoders, we used the pre-trained weights
from the experiments of Vougioukas et al. [2] and these re-
mained ﬁxed during training. We performed Gaussian initial-
ization on the parameters of the polynomial fusion layer, the
generator and the discriminators. We set the hyperparameters
λ1 = 600, λf rame = 1, λseq = 0.2, λsync = 0.8 as in [2] and
λ2 = 100. The models were trained with Adam [19] using
learning rates of 1 × 10−4 for G and Df rame and 1 × 10−5
for Dseq and Dsync. Training was carried out until, by in-
spection, there was no noticeable improvement in the video
quality on the validation set for 5 epochs.

We also compare our models with Vougioukas et al.
[2] which uses concatenation as its fusion layer, and the
Speech2Vid model [3], which is trained on the Voxceleb [20]
and LRW [21] datasets.

We evaluate the reconstruction quality of the videos the
results using the peak signal to noise ratio (PSNR) and struc-
tural similarity index (SSIM). In addition, we measure the

sharpness of the frames using the cumulative probability blur
detection measure [22]. We perform face veriﬁcation on the
videos using the average content distance (ACD) metric using
OpenFace [23]. For PSNR and SSIM, higher values indicate
better quality whereas for ACD a lower value indicates a bet-
ter capturing of the identity. Furthermore, we evaluate the au-
diovisual synchronisation of the videos using SyncNet [24],
where lower AV offset (measured in frames) indicates better
synchronisation and higher AV conﬁdence indicates stronger
audiovisual correlation. For reference we also evaluate the
blinks in generated videos using the blink detector in [2]. The
results are shown on Table 3.

From the experiments above, we observe that our model
outperforms the Speech2Vid model and is comparable to the
state-of-the-art of Vougioukas et al, 2019. In particular, our
model is able to generate natural blinks at rates comparable to
real-life values (estimated at 0.28 blinks/sec [25]).

4. CONCLUSION

In this paper we present a polynomial fusion layer that pro-
duces a joint low-dimensional representation of encodings of
audio and visual identity using a higher-order polynomial.
The parameters of the polynomial are modelled by a low-
rank tensor decomposition. We have investigated this method
for the task of speech-driven facial animation, where the joint
representation is fed into a generator to produce a sequence of
talking faces. We demonstrate the comparability of the pro-
posed method with the state-of-the art on several metrics from
experiments on three audiovisual datasets.

5. REFERENCES

[1] K. Vougioukas, S. Petridis, and M. Pantic, “End-to-end
speech-driven facial animation with temporal gans,” in
British Machine Vision Conference. Newcastle, Septem-
ber 2018.

[2] K. Vougioukas, S. Petridis, and M. Pantic, “Realistic
speech-driven facial animation with gans,” International
Journal of Computer Vision, 2019.

[3] A. Jamaludin, Joon Son Chung, and Andrew Zisserman,
“You said that?: Synthesising talking faces from audio,”
International Journal of Computer Vision, Feb 2019.

[4] S. Tulyakov, M. Liu, X. Yang, and J. Kautz, “Moco-
gan: Decomposing motion and content for video gen-
eration,” in The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.

[5] Y. Cao, W. Tien, P. Faloutsos, and F. Pighin, “Expressive
speech-driven facial animation,” ACM Trans. Graph.,
vol. 24, no. 4, pp. 1283–1302, Oct. 2005.

[6] X. Liu, J. Geng, H. Ling, and Y. Cheung, “Attention
guided deep audio-face fusion for efﬁcient speaker nam-
ing,” Pattern Recognition, vol. 88, 12 2018.

[7] T. Kolda and B. Bader, “Tensor decompositions and
applications,” SIAM Rev., vol. 51, no. 3, pp. 455–500,
Aug. 2009.

[8] Grigorios Chrysos, Stylianos Moschoglou, Yannis Pana-
gakis, and Stefanos Zafeiriou, “Polygan: High-order
polynomial generators,” 2019.

[9] B. Cao, H. Zhou, G. Li, and P. Yu, “Multi-view ma-
chines,” in Proceedings of the Ninth ACM International
Conference on Web Search and Data Mining, New York,
NY, USA, 2016, WSDM ’16, pp. 427–436, ACM.

[10] B. Romera-Paredes, M. Aung, N. Bianchi-Berthouze,
and M. Pontil, “Multilinear multitask learning,” in Pro-
ceedings of the 30th International Conference on Inter-
national Conference on Machine Learning - Volume 28.
2013, ICML’13, pp. III–1444–III–1452, JMLR.org.

[11] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convo-
lutional networks for biomedical image segmentation,”
in Medical Image Computing and Computer-Assisted
Intervention – MICCAI 2015, Cham, 2015, pp. 234–
241, Springer International Publishing.

[12] A. Cichocki, N. Lee, I. Oseledets, A. Phan, Q. Zhao, and
D. Mandic, “Low-rank tensor networks for dimension-
ality reduction and large-scale optimization problems:
Perspectives and challenges part 1,” 2016.

[13] F. Hitchcock, “The expression of a tensor or a polyadic
Journal of Mathematics and

as a sum of products,”
Physics, vol. 6, no. 1-4, pp. 164–189, 1927.

[14] L. Tucker, “Some mathematical notes on three-mode
factor analysis,” Psychometrika, vol. 31, no. 3, pp. 279–
311, 1966.

[15] M. Cooke, J. Barker, S. Cunningham, and X. Shao, “An
audio-visual corpus for speech perception and automatic
speech recognition (l),” The Journal of the Acoustical
Society of America, vol. 120, pp. 2421–4, 12 2006.

[16] H. Cao, D. G. Cooper, M. K. Keutmann, R. C. Gur,
A. Nenkova, and R. Verma, “Crema-d: Crowd-sourced
emotional multimodal actors dataset,” IEEE Transac-
tions on Affective Computing, vol. 5, no. 4, pp. 377–390,
Oct 2014.

[17] N. Harte and E. Gillen, “Tcd-timit: An audio-visual
corpus of continuous speech,” IEEE Transactions on
Multimedia, vol. 17, no. 5, pp. 603–615, May 2015.

[18] J. Garofolo, Lori Lamel, W. Fisher, Jonathan Fiscus,
D. Pallett, N. Dahlgren, and V. Zue, “Timit acoustic-
phonetic continuous speech corpus,” Linguistic Data
Consortium, 11 1992.

[19] D. Kingma and J. Ba, “Adam: A method for stochas-
tic optimization,” International Conference on Learning
Representations, 12 2014.

[20] A. Nagrani, J. S. Chung, and A. Zisserman, “Voxceleb:
a large-scale speaker identiﬁcation dataset,” in INTER-
SPEECH, 2017.

[21] J. S. Chung and A. Zisserman, “Lip reading in the wild,”

in Asian Conference on Computer Vision, 2016.

[22] C. Vondrick, H. Pirsiavash, and A. Torralba,

“Gen-
erating videos with scene dynamics,” in Advances in
Neural Information Processing Systems 29, D. D. Lee,
M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett,
Eds., pp. 613–621. Curran Associates, Inc., 2016.

[23] Brandon A., Bartosz L., and Mahadev S., “Openface:
A general-purpose face recognition library with mobile
applications,” 2016.

[24] J. S. Chung and A. Zisserman, “Out of time: automated
lip sync in the wild,” in Workshop on Multi-view Lip-
reading, ACCV, 2016.

[25] A.R. Bentivoglio, S. Bressman, E. Cassetta, D. Carretta,
P. Tonali, and A. Albanese, “Analysis of blink rate pat-
terns in normal subjects.,” Movement disorders : ofﬁcial
journal of the Movement Disorder Society, vol. 12 6, pp.
1028–34, 1997.

