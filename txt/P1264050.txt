9
1
0
2
 
n
u
J
 
7
1
 
 
]
L
C
.
s
c
[
 
 
1
v
0
8
2
7
0
.
6
0
9
1
:
v
i
X
r
a

Natural Language Engineering 1 (1): 000–000. Printed in the United Kingdom

1

c(cid:13) 1998 Cambridge University Press

A Structured Distributional Model
of Sentence Meaning and Processing

E. C h e r s o n i , T h e H o n g K o n g P o l y t e c h n i c U n i v e r s i t y

E. S a n t u s , M a s s a c h u s e t t s I n s t i t u t e o f T e c h n o l o g y

L. P a n n i t t o , U n i v e r s i t y o f P i s a

A. L e n c i , U n i v e r s i t y o f P i s a

P. B l a c h e , A i x - M a r s e i l l e U n i v e r s i t y

C. - R. H u a n g , T h e H o n g K o n g P o l y t e c h n i c U n i v e r s i t y

( Received )

Abstract

Most compositional distributional semantic models represent sentence meaning with a
single vector. In this paper, we propose a Structured Distributional Model (SDM) that
combines word embeddings with formal semantics and is based on the assumption that
sentences represent events and situations. The semantic representation of a sentence is
a formal structure derived from Discourse Representation Theory and containing distri-
butional vectors. This structure is dynamically and incrementally built by integrating
knowledge about events and their typical participants, as they are activated by lexical
items. Event knowledge is modeled as a graph extracted from parsed corpora and en-
coding roles and relationships between participants that are represented as distributional
vectors. SDM is grounded on extensive psycholinguistic research showing that general-
ized knowledge about events stored in semantic memory plays a key role in sentence
comprehension. We evaluate SDM on two recently introduced compositionality datasets,
and our results show that combining a simple compositional model with event knowledge
constantly improves performances, even with diﬀerent types of word embeddings.

1 Sentence Meaning in Vector Spaces

While for decades sentence meaning has been represented in terms of complex for-
mal structures, the most recent trend in computational semantics is to model seman-
tic representations with dense distributional vectors (aka embeddings). As a matter
of fact, distributional semantics has become one of the most inﬂuential approaches
to lexical meaning, because of the important theoretical and computational advan-
tages of representing words with continuous vectors, such as automatically learning
lexical representations from natural language corpora and multimodal data, assess-
ing semantic similarity in terms of the distance between the vectors, and dealing
with the inherently gradient and fuzzy nature of meaning (Erk 2012, Lenci 2018a).

2

Chersoni et al.

Over the years, intense research has tried to address the question of how to
project the strengths of vector models of meaning beyond word level, to phrases
and sentences. The mainstream approach in distributional semantics assumes the
representation of sentence meaning to be a vector, exactly like lexical items. Early
approaches simply used pointwise vector operations (such as addition or multipli-
cation) to combine word vectors to form phrase or sentence vectors (Mitchell and
Lapata 2010), and in several tasks they still represent a non-trivial baseline to beat
(Rimell et al. 2016). More recent contributions can be essentially divided into two
separate trends. The former attempts to model ‘Fregean compositionality’ in vector
space, and aimes at ﬁnding progressively more sophisticated compositional opera-
tions to derive sentence representations from the vectors of the words composing
them (Baroni et al. 2013, Paperno et al. 2014). In the latter trend, dense vectors
for sentences are learned as a whole, in a similar way to neural word embeddings
(Mikolov et al. 2013, Levy and Goldberg 2014): for example, the encoder-decoder
models of works like Kiros et al. (2015) and Hill et al. (2016) are trained to predict,
given a sentence vector, the vectors of the surrounding sentences.

Representing sentences with vectors appears to be unrivaled from the applica-
tive point of view, and has indeed important advantages such as the possibility of
measuring similarity between sentences with their embeddings, as is customary at
the lexical level, which is then exploited in tasks like automatic paraphrasing and
captioning, question-answering, etc. Recently, probing tasks have been proposed to
test what kind of syntactic and semantic information is encoded in sentence embed-
dings (Ettinger et al. 2016, Adi et al. 2017, Conneau et al. 2018, Zhu et al. 2018). In
particular, Zhu et al. (2018) show that current models are not able to discriminate
between diﬀerent syntactic realization of semantic roles, and fail to recognize that
Lilly loves Imogen is more similar to its passive counterpart than to Imogen loves
Lilly. Moreover, it is diﬃcult to recover information about the component words
from sentence embeddings (Adi et al. 2017, Conneau et al. 2018). The semantic rep-
resentations built with tensor product in the question-answering system by Palangi
et al. (2018) have been claimed to be grammatically interpretable as well. However,
the complexity of the semantic information brought by sentences and the diﬃculty
to interpret the embeddings raise doubts about the general theoretical and empirical
validity of the “sentence-meaning-as-vector” approach.

In this paper, we propose a Structured Distributional Model (SDM) of sen-
tence meaning that combines word embeddings with formal semantics and is based
on the assumption that sentences represent events and situations. These are re-
garded as inherently complex semantic objects, involving multiple entities that
interact with diﬀerent roles (e.g., agents, patients, locations etc.). The semantic
representation of a sentence is a formal structure inspired by Discourse Repre-
sentation Theory (DRT) (Kamp 2013) and containing distributional vectors. This
structure is dynamically and incrementally built by integrating knowledge about
events and their typical participants, as they are activated by lexical items. Event
knowledge is modeled as a graph extracted from parsed corpora and encoding roles
and relationships between participants that are represented as distributional vec-
tors. The semantic representations of SDM retain the advantages of embeddings

A Structured Distributional Model

3

(e.g., learnability, gradability, etc.), but also contain directly interpretable formal
structures, diﬀerently from classical vector-based approaches.

SDM is grounded on extensive psycholinguistic research showing that generalized
knowledge about events stored in semantic memory plays a key role in sentence
comprehension (McRae and Matsuki 2009). On the other hand, it is also close
to recent attempts to look for a “division of labour” between formal and vector
semantics, representing sentences with logical forms enriched with distributional
representations of lexical items (Beltagy et al. 2016, Boleda and Herbelot 2016,
McNally 2017). Like SDM, McNally and Boleda (2017) propose to introduce em-
beddings within DRT semantic representations. At the same time, diﬀerently from
these other approaches, SDM consists of formal structures that integrate word em-
beddings with a distributional representation of activated event knowledge, which
is then dynamically integrated during semantic composition.

The contribution of this paper is twofold. First, we introduce SDM as a
cognitively-inspired distributional model of sentence meaning based on a struc-
tured formalization of semantic representations and contextual event knowledge
(Section 2). Secondly, we show that the event knowledge used by SDM in the con-
struction of sentence meaning representations leads to improvements over other
state-of-the-art models in compositionality tasks. In Section 3, SDM is tested on
two diﬀerent benchmarks: the ﬁrst is RELPRON (Rimell et al. 2016), a popular
dataset for the similarity estimation between compositional distributional represen-
tations; the second is DTFit (Vassallo et al. 2018), a dataset created to model an
important aspect of sentence meaning, that is the typicality of the described event
or situation, which has been shown to have important processing consequences for
language comprehension.

2 Dynamic Composition with Embeddings and Event Knowledge

SDM rests on the assumption that natural language comprehension involves the
dynamic construction of semantic representations, as mental characterization of the
events or situations described in sentences. We use the term ‘dynamic’ in the sense
of dynamic semantic frameworks like DRT, to refer to a bidirectional relationship
between linguistic meaning and context (see also Heim 1983):

The meaning of an expression depends on the context in which it is used, and its content is
itself deﬁned as a context-change potential, which aﬀects and determines the interpretation
of the following expressions.

The content of an expression E used in a context C depends on C, but – once
the content has been determined – it will contribute to update C to a new context
C (cid:48), which will help ﬁxing the content of the next expression. Similarly to DRT,
SDM integrates word embeddings in a dynamic process to construct the semantic
representations of sentences. Contextual knowledge is represented in distributional
terms and aﬀects the interpretation of following expressions, which in turn cue new
information that updates the current context.1

1 An early work on a distributional model of lexical expectations in context is Washtell

4

Chersoni et al.

Context is a highly multifaceted notion that includes several types of factors
guiding and inﬂuencing language comprehension: information about the commu-
nicative settings, preceding discourse, general presuppositions and knowledge about
the world, etc. In DRT, Kamp (2016) has introduced the notion of articulated
context to model diﬀerent sources of contextual information that intervene in the
dynamic construction of semantic representations. In this paper, we focus on the
contribution of a speciﬁc type of contextual information, which we refer to as Gen-
eralized Event Knowledge (gek). This is knowledge about events and situations
that we have experienced under diﬀerent modalities, including the linguistic in-
put (McRae and Matsuki 2009), and is generalized because it contains information
about prototypical event structures.

In linguistics, the Generative Lexicon theory (Pustejovsky 1995) argues that the
lexical entries of nouns also contain information about events that are crucial to
deﬁne their meaning (e.g., read for book ). Psycholinguistic studies in the last two
decades have brought extensive evidence that the array of event knowledge activated
during sentence processing is extremely rich: verbs (e.g. arrest) activate expecta-
tions about typical arguments (e.g. cop, thief ) and vice versa (McRae et al. 1998,
Ferretti et al. 2001, McRae et al. 2005), and similarly nouns activate other nouns
typically co-occurring as participants in the same events (key, door ) (Hare et al.
2009). The inﬂuence of argument structure relations on how words are neurally
processed is also an important ﬁeld of study in cognitive neuroscience (Thompson
and Meltzer-Asscher 2014, Meltzer-Asscher et al. 2015, Williams et al. 2017).

Stored event knowledge has relevant processing consequences. Neurocognitive
research showed that the brain is constantly engaged in making predictions to an-
ticipate future events (Bar 2009, Clark 2013). Language comprehension, in turn,
has been characterized as a largely predictive process (Kuperberg and Jaeger 2015).
Predictions are memory-based, and experiences about events and their participants
are used to generate expectations about the upcoming linguistic input, thereby min-
imizing the processing eﬀort (Elman 2014, McRae and Matsuki 2009). For instance,
argument combinations that are more ‘coherent’ with the event scenarios activated
by the previous words are read faster in self-paced reading tasks and elicited smaller
N400 amplitudes in ERP experiments (Bicknell et al. 2010, Matsuki et al. 2011,
Paczynski and Kuperberg 2012, Metusalem et al. 2012).2

Elman (2009; 2014) has proposed a general interpretation of these experimental
results in the light of the Words-as-Cues framework. According to this theory, words
are arranged in the mental lexicon as a sort of network of mutual expectations, and
listeners rely on pre-stored representations of events and common situations to try

(2010), but its focus was more on word sense disambiguation than on representing
sentence meaning.

2 Event-related potentials are the electrophysiological response of the brain to a stimulus.
In the sentence processing literature, the ERPs are recorded for each stimulus word and
the N400, one of the most studied ones, is a negative-going deﬂection appearing 400ms
after the presentation of the word. A common interpretation of the N400 assumes that
the wave amplitude is proportional to the diﬃculty of semantic uniﬁcation (Baggio and
Hagoort 2011).

A Structured Distributional Model

5

to identify the one that a speaker is more likely to communicate. As new input words
are processed, they are quickly integrated in a data structure containing a dynamic
representation of the sentence content, until some events are recognized as the ‘best
candidates’ for explaining the cues (i.e., the words) observed in the linguistic input.
It is important to stress that, in such a view, the meaning of complex units such
as phrases and sentences is not always built by composing lexical meanings, as the
representation of typical events might be already stored and retrieved as a whole
in semantic memory. Participants often occurring together become active when the
representation of one of them is activated (see also Bar et al., 2007 on the relation
between associative processing and predictions).

SDM aims at integrating the core aspects of dynamic formal semantics and the
evidence on the role of event knowledge for language processing into a general model
for compositional semantic representations that relies on two major assumptions:

• lexical items are represented as embeddings within a network of relations
encoding knowledge about events and typical participants, which corresponds
to what we have termed above gek;

• the semantic representation (sr) of a sentence (or even larger stretches of
linguistic input, such as discourse) is a formal structure that dynamically
combines the information cued by lexical items.

Like in Chersoni et al. (2017), the model is inspired by Memory, Uniﬁcation and
Control (MUC), proposed by Hagoort (Hagoort 2013; 2016) as a general model
for the neurobiology of language. MUC incorporates three main functional com-
ponents: i.) Memory corresponds to knowledge stored in long-term memory; ii.)
Uniﬁcation refers to the process of combining the units stored in Memory to create
larger structures, with contributions from the context; and iii.) Control is responsi-
ble for relating language to joint action and social interaction. Similarly, our model
distinguishes between a component storing event knowledge, in the form of a Dis-
tributional Event Graph (deg, Section 2.1), and a meaning composition
function that integrates information activated from lexical items and incremen-
tally builds the sr (Section 2.2).

2.1 The Distributional Event Graph

The Distributional Event Graph represents the event knowledge stored in long-term
memory with information extracted from parsed corpora. We assume a very broad
notion of event, as an n-ary relation between entities. Accordingly, an event can be a
complex situation involving multiple participants, such as The student reads a book
in the library, but also the association between an entity and a property expressed by
the noun phrase heavy book. This notion of event corresponds to what psychologist
call situation knowledge or thematic associations (Binder 2016). As McRae and
Matsuki (2009) argue, gek is acquired from both sensorimotor experience (e.g.,
watching or playing football matches) and linguistic experience (e.g., reading about
football matches). deg can thus be regarded as a model of the gek derived from
the linguistic input.

6

Chersoni et al.

Fig. 1. Reduced version of the parsing for the sentence The student is reading the
book about Shakespeare in the university library. Three events are identiﬁed, each
represented with a dotted box.

Events are extracted from parsed sentences, using syntactic relations as an ap-
proximation of deeper semantic roles (e.g., the subject relation for the agent, the
direct object relation for the patient, etc.). In the present paper, we use depen-
dency parses, as it is customary in distributional semantics, but nothing in SDM
hinges on the choice of the syntactic representation. Given a verb or a noun head,
all its syntactic dependents are grouped together.3 More schematic events are also
generated by abstracting from one or more event participants for every recorded
instance. Since we expect each participant to be able to trigger the event and con-
sequently any of the other participants, a relation can be created and added to the
graph from every subset of each group extracted from a sentence (cf. Figure 1).

The resulting deg structure is a weighted hypergraph, as it contains weighted
relations holding between nodes pairs, and a labeled multigraph, since the edges are
labeled in order to represent speciﬁc syntactic relations. The weights σ are derived
from co-occurrence statistics and measure the association strengths between event
nodes. They are intended as salience scores that identify the most prototypical
events associated with an entity (e.g., the typical actions performed by a student).
Crucially, the graph nodes are represented as word embeddings. Thus, given a
lexical cue w, the information in deg can be activated along two dimensions during
processing (cf. Table 1):

1. by retrieving the most similar nodes to w (the paradigmatic neighbors), on
the basis of their cosine similarity between their vectors and the vector of w;
2. by retrieving the closest associates of w (the syntagmatic neighbors), using

the edge weights.

Figure 2 shows a toy example of deg. The little boxes with circles in them represent
the embedding associated with each node. Edges are labeled with syntactic relations

3 The extracted graphs are similar to the syntactic joint contexts for verb representation

that were proposed by Chersoni et al. (2016).

A Structured Distributional Model

7

Fig. 2. Toy sample of deg showing several instances of events, each represented
by a sequence of co-indexed e. The σ are the event salience weights.

Paradigmatic Neighbors

Syntagmatic Neighbors

essay, story, novel, author, biography

publish, write, read, child, series

Table 1. The ﬁve nearest paradigmatic and syntagmatic neighbors for the lexical
item book, extracted from deg.

(as a surface approximation of event roles) and weighted with salience scores σ.
Each event is a set of co-indexed edges. For example, e2 corresponds to the event of
students reading books in libraries, while e1 represents a schematic event of students
performing some generic action on books (e.g., reading, consulting, studying, etc.).

2.2 The Meaning Composition Function

We assume that during sentence comprehension lexical items activate fragments
of event knowledge stored in deg (like in Elman’s Words-as-Cues model), which
are then dynamically integrated in a semantic representation sr. This is a formal
structure directly inspired by DRT and consisting of three diﬀerent yet interacting
information tiers:

1. universe (U) - this tier, which we do not discuss further in the present paper,
includes the entities mentioned in the sentence (corresponding to the discourse
referents in DRT). They are typically introduced by noun phrases and provide
the targets of anaphoric links;

2. linguistic conditions (lc) - a context-independent tier of meaning that accumu-
lates the embeddings associated with the lexical items. This corresponds to the

8

Chersoni et al.

Fig. 3. Sample sr for the sentence The student drinks the coﬀee. The sentence
activates typical locations and times in which the event could take place.

conditions that in DRT content words add to the discourse referents. The crucial
diﬀerence is that now such conditions are embeddings;

3. active context (ac) - similarly to the notion of articulated context in Kamp
(2016), this component consists of several types of contextual information avail-
able during sentence processing or activated by lexical items (e.g., information
from the current communication setting, general world knowledge, etc.). More
speciﬁcally, we assume that ac contains the embeddings activated from deg
by the single lexemes (or by other contextual elements) and integrated into a
semantically coherent structure contributing to the sentence interpretation.

Figure 3 shows an example of sr built from the sentence The student drinks the
coﬀee (ignoring the speciﬁc contribution of determiners and tense). The universe U
contains the discourse referents introduced by the noun phrases, while lc includes
the embeddings of the lexical items in the sentence, each linked to the relevant
−−−−−→
referent (e.g.,
student : u means that the embedding introduced by student is linked
to the discourse referent u). ac consists of the embeddings activated from deg and
ranked by their salience with respect to the current content in the sr. The elements
in ac are grouped by their syntactic relation in deg, which again we regard here
just as a surface approximation of their semantic role (e.g., the items listed under
“obl:loc” are a set of possible locations of the event expressed by the sentence). ac
makes it possible to enrich the semantic content of the sentence with contextual
information, predict other elements of the event, and generate expectations about
incoming input. For instance, given the ac in Figure 3, we can predict that the
student is most likely to be drinking a coﬀee at the cafeteria and that he/she is
drinking it for breakfast or in the morning. The ranking of each element in ac
depends on two factors: i.) its degree of activation by the lexical items, ii.) its
overall coherence with respect to the information already available in the ac.

A Structured Distributional Model

9

Fig. 4. On the left, the sr for The student. On the right, the embedding and deg
portion activated by the verb drink.

A crucial feature of each sr is that lc and ac are also represented with vectors
that are incrementally updated with the information activated by lexical items. Let
sri−1 be the semantic representation built for the linguistic input w1, . . . , wi−1.
When we process a new pair (cid:104)wi, ri(cid:105) with a lexeme wi and syntactic role ri:

1. lc in sri−1 is updated with the embedding −→wi;
2. ac in sri−1 is updated with the embeddings of the syntagmatic neighbors of

wi extracted from deg.

Figures 4 and 5 exemplify the update of the sr for the subject The student with the
information activated by the verb drink. The update process is deﬁned as follows:
−→
LC obtained from the linear combination
of the embeddings of the words contained in the sentence. Therefore, when
(cid:104)wi, ri(cid:105) is processed, the embedding −→wi is simply added to

1. lc is represented with the vector

−→
LC;4

2. for each syntactic role ri, ac contains a set of ranked lists (one for each pro-
cessed pair) of embeddings corresponding to the most likely words expected
to ﬁll that role. For instance, the ac for the fragment The student in Figure
4 contains a list of the embeddings of the most expected direct objects asso-
ciated with student, a list of the embeddings of the most expected locations,
etc. Each list of expected role ﬁllers is itself represented with the weighted
−−→
dobj) of their k most prominent items (with k a model
centroid vector (e.g.,

4 At the same time, the embedding is linked either to a new discourse referent added to

U, or to an already available one.

10

Chersoni et al.

Fig. 5. The original semantic representation sr for The student . . . is updated with
the information activated by the verb, producing the sr for The student drinks . . .
The new event knowledge is re-ranked with respect to the previous content of ac.

−−→
book and

−−→
dobj centroid in the ac in
hyperparameter). For instance, setting k = 2, the
−−−−−−→
ﬁgure 4 is built just from
research; less salient elements (the gray
areas in Figures 3, 4 and 5) are kept in the list of likely direct objects, but at
this stage do not contribute to the centroid representing the expected ﬁllers
for that role. ac is then updated with the deg fragment activated by the new
lexeme wi (e.g., the verb drink ):

• the event knowledge activated by wi for a given role ri is ranked according
to cosine similarity with the vector −→ri available in ac: in our example, the
−−−−→
cof f ee, etc.) are
direct objects activated by the verb drink (e.g.,
ranked according to their cosine similarity to the

−−→
beer,
−−→
dobj vector of the ac;

• the ranking process works also in the opposite direction: the newly re-
trieved information is used to update the centroids in ac. For example,
the direct objects activated by the verb drink are aggregated into centroids
and the corresponding weighted lists in ac are re-ranked according to the
cosine similarity with the new centroids, in order to maximize the semantic
−−−−−−→
coherence of the representation. At this point,
research, which
−−→
beer in the drinking context, are down-
are not as salient as
graded in the ranked list and are therefore less likely to become part of
the

−−→
dobj centroid at the next step.

−−−−→
cof f ee and

−−→
book and

The newly retrieved information is now added to the ac: as shown in Figure
5, once the pair (cid:104)drink, root(cid:105) has been fully processed, the ac contains two
ranked lists for the dobj role and two ranked lists for the obl:loc role, the top k

A Structured Distributional Model

11

elements of each list will be part of the centroid for their relation in the next
−→
step. Finally, the whole ac is represented with the centroid vector
AC built
−→
out of the role vectors −→r1 , . . . , −→rn available in ac. The vector
AC encodes the
integrated event knowledge activated by the linguistic input.

As an example of gek re-ranking, assume that after processing the subject noun
phrase The student, the ac of the corresponding sr predicts that the most expected
verbs are read, study, drink, etc., the most expected associated direct objects are
book, research, beer, etc., and the most expected locations are library, cafeteria,
university, etc. (Figure 4). When the main verb drink is processed, the correspond-
ing role list is removed by the ac, because that syntactic slot is now overtly ﬁlled
by this lexeme, whose embedding is then added to the lc. The verb drink cues
its own event knowledge, for instance that the most typical objects of drinking are
tea, coﬀee, beer, etc., and the most typical locations are cafeteria, pub, bar, etc.
The information cued by drink is re-ranked to promote those items that are most
compatible and coherent with the current content of ac (i.e., direct objects and
locations that are likely to interact with students). Analogously, the information in
the ac is re-ranked to make it more compatible with the gek cued by drink (e.g.,
the salience of book and research gets decreased, because they are not similar to
the typical direct objects and locations of drink ). The output of the sr update is
shown in Figure 5, whose ac now contains the gek associated with an event of
drinking by a student.

A crucial feature of sr is that it is a much richer representation than the bare
linguistic input: the overtly realized arguments in fact activate a broader array of
roles than the ones actually appearing in the sentence. As an example of how these
unexpressed arguments contribute to the semantic representation of the event, con-
sider a situation in which three diﬀerent sentences are represented by means of ac,
namely The student writes the thesis, The headmaster writes the review and The
teacher writes the assignment. Although teacher could be judged as closer to head-
master than to student, and thesis as closer to assignment than to review, taking
into account also the typical locations (e.g., a library for the ﬁrst two sentences,
a classroom for the last one) and writing supports (e.g., a laptop in the ﬁrst two
cases, a blackboard in the last one) would lead to the ﬁrst two events being judged
as the most similar ones.

In the case of unexpected continuations, the ac will be updated with the new in-
formation, though in this case the re-ranking process would probably not change the
gek prominence. Consider the case of an input fragment like The student plows...:
student activates event knowledge as it is shown in Figure 3, but the verb does
not belong to the set of expected events given student. The verb triggers diﬀerent
direct objects from those already in the ac (e.g., typical objects of plow such as
furrow, ﬁeld, etc.). Since the similarity of their centroid with the elements of the
direct object list in the ac will be very low, the relative ordering of the ranked list
will roughly stay the same, and direct objects pertaining to the plowing situation
will coexist with direct objects triggered by student. Depending on the continuation

12

Chersoni et al.

Fig. 6. Image from Rimell et al. (2016), showing the terminology for terms and
properties in RELPRON: subject relative clause top, object relative clause
bottom.

of the sentence, then, the elements triggered by plow might gain centrality in the
representation or remain peripheral.

It is worth noting that the incremental process of the sr update is consistent
with the main principles of formal dynamic semantic frameworks like DRT. As
we said above, dynamics semantics assumes the meaning of an expression to be a
context-change potential that aﬀects the interpretation of the following expressions.
Similarly, in our distributional model of sentence representation the ac in sri−1
aﬀects the interpretation of the incoming input wi, via the gek re-ranking process.5

3 Experiments

3.1 Datasets and Tasks

Our goal is to test SDM in compositionality-related tasks, with a particular focus on
the contribution of event knowledge. For the present study, we selected two diﬀerent
datasets: the development set of the RELPRON dataset (Rimell et al. 2016)6 and
the DTFit dataset (Vassallo et al. 2018).

RELPRON consists of 518 target-property pairs, where the target is a noun
labeled with a syntactic function (either subject or direct object) and the property
is a subject or object relative clause providing the deﬁnition of the target (Figure 6).
Given a model, we produce a compositional representation for each of the properties.
In each deﬁnition, the verb, the head noun and the argument are composed to
obtain a representation of the property. Following the original evaluation in Rimell
et al. (2016), we tested six diﬀerent combinations for each composition model: the
verb only, the argument only, the head noun and the verb, the head noun and the
argument, the verb and the argument and all three of them. For each target, the
518 composed vectors are ranked according to their cosine similarity to the target.
Like Rimell et al. (2016), we use Mean Average Precision (henceforth MAP) to

5 For a more comprehensive analysis of the relationship between distributional semantics

and dynamics semantics, see Lenci (2018b).

6 We used the development set of RELPRON in order to compare our results with those

published by Rimell et al. (2016).

A Structured Distributional Model

13

evaluate our models on RELPRON. Formally, MAP is deﬁned as

where N is the number of terms in RELPRON, and AP (t) is the Average Precision
for term t, deﬁned as:

(1)

(2)

M AP =

AP (ti)

1
N

N
(cid:88)

i=1

AP (t) =

P rec(k) × rel(k)

1
Pt

M
(cid:88)

k=1

where Pt is the number of correct properties for term t in the dataset, M is the
total number of properties in the dataset, P rec(k) is the precision at rank k, and
rel(k) is a function equal to one if the property at rank k is a correct property for
t, and zero otherwise. Intuitively, AP (t) will be 1 if, for the term t, all the correct
properties associated to the term are ranked in the top positions, and the value
becomes lower when the correct items are ranked farther from the head of the list.
Our second evaluation dataset, DTFit, has been introduced with the goal of
building a new gold standard for the thematic ﬁt estimation task (Vassallo et al.
2018). Thematic ﬁt is a psycholinguistic notion similar to selectional preferences,
the main diﬀerence being that the latter involve the satisfaction of constraints
on discrete semantic features of the arguments, while thematic ﬁt is a continuous
value expressing the degree of compatibility between an argument and a semantic
role (McRae et al. 1998). Distributional models for thematic ﬁt estimation have
been proposed by several authors (Erk 2007, Baroni and Lenci 2010, Erk et al.
2010, Lenci 2011, Sayeed et al. 2015, Greenberg et al. 2015, Santus et al. 2017,
Tilk et al. 2016, Hong et al. 2018). While thematic ﬁt datasets typically include
human-elicited typicality scores for argument-ﬁller pairs taken in isolation, DTFit
includes tuples of arguments of diﬀerent length, so that the typicality value of
an argument depends on its interaction with the other arguments in the tuple.
This makes it possible to model the dynamic aspect of argument typicality, since
the expectations on an argument are dynamically updated as the other roles in the
sentence are ﬁlled. The argument combinations in DTFit describe events associated
with crowdsourced scores ranging from 1 (very atypical) to 7 (very typical). The
dataset items are grouped into typical and atypical pairs that diﬀer only for one
argument, and divided into three subsets:

• 795 triplets, each diﬀering only for the Patient role:

— sergeant N assign V mission N (typical)
— sergeant N assign V homework N (atypical)

• 300 quadruples, each diﬀering only for the Location role:

— policeman N check V bag N airport N (typical)
— policeman N check V bag N kitchen N (atypical)

• 200 quadruples, each diﬀering only for the Instrument role:

— painter N decorate V wall N brush N (typical)
— painter N decorate V wall N scalpel N (atypical)

14

Chersoni et al.

However, the Instrument subset of DTFit was excluded from our current evaluation.
After applying the threshold of 5 for storing events in the deg (cf. Section 3.2.3),
we found that the SDM coverage on this subset was too low.

For each tuple in the DTFit dataset, the task for our models is to predict the
upcoming argument on the basis of the previous ones. Given a model, we build
a compositional vector representation for each dataset item by excluding the last
argument in the tuple, and then we measured the cosine similarity between the
resulting vector and the argument vector. Models are evaluated in terms of the
Spearman correlation between the similarity scores and the human ratings.

As suggested by the experimental results of Bicknell et al. (2010) and Matsuki
et al. (2011), the typicality of the described events has important processing con-
sequences: atypical events lead to longer reading times and stronger N400 compo-
nents, while typical ones are easier to process thanks to the contribution of gek.
Thus, the task of modeling typicality judgements can be seen as closely related to
modeling semantic processing complexity.

3.2 Models Settings

In this study, we compare the performance of SDM with three baselines. The simple
additive model formulated in Mitchell and Lapata (2010), a smoothed additive
model, and a multi-layer Long-Short-Term-Memory (LSTM) neural language model
trained against one-hot targets (Zaremba et al. 2014).

The additive models (Mitchell and Lapata 2010) have been evaluated on diﬀerent
types of word embeddings. We compared their performances with SDM.7 Despite
their simplicity, previous evaluation studies on several benchmarks showed that such
models can be diﬃcult to beat, even for sophisticated compositionality frameworks
(Rimell et al. 2016, Arora et al. 2017, Tian et al. 2017).

The embeddings we used in our tests are the word2vec models by Mikolov et al.
(2013), that is the Skip-Gram with Negative Sampling (SG) and the Continuous-
Bag-of-Words (CBOW), and the C-Phrase model by Kruszewski et al. (2015).
The latter model incorporates information about syntactic constituents, as the prin-
ciples of the model training are i.) to group the words together according to the
syntactic structure of the sentences and ii.) to optimize simultaneously the context
predictions at diﬀerent levels of the syntactic hierarchy (e.g., given the training
sentence A sad dog is howling in the park, the context prediction will be optimized
for dog, a dog, a sad dog etc., that is for all the words that form a syntactic con-
stituent). The performance of C-Phrase is particularly useful to assess the beneﬁts
of using vectors that encode directly structural/syntactic information.

We used the same corpora both for training the embeddings and for extracting
the syntactic relations for deg. The training data come from the concatenation
of three dependency-parsed corpora: the BNC (Leech 1992), the Ukwac (Baroni
et al. 2009) and a 2018 dump of the English Wikipedia, for a combined size of

7 We also tested pointwise multiplicative models, but in our tasks the performances were

extremely low, so they were omitted.

(3)

(4)

A Structured Distributional Model

15

approximately 4 billion tokens. The corpora were parsed with Stanford CoreNLP
(Manning et al. 2014). The hyperparameters of the embeddings were the following
for all models: 400 dimensions, a context window of size 10, 10 negative samples,
100 as the minimum word frequency.8

3.2.1 Simple Additive Models

−→
Our additive models, corresponding to a sr consisting of the
LC component only,
represent the meaning of a sentence sent by summing the embeddings of its words:

−−→
sent =

(cid:88)

(cid:126)w

w∈sent

The similarity with the targets is measured with the cosine between the target
vector and the sentence vector.

3.2.2 Smoothed Additive Models

These models are a smoothed version of the additive baseline, in which the ﬁnal
representation is simply the sum of the vectors of the words in the sentence, plus the
top k = 5 nearest neighbor of each word in the sentence.9 Therefore, the meaning
of a sentence sent is obtained by:

−−→
sent =

(cid:88)



 (cid:126)w +

(cid:88)



(cid:126)x



w∈sent

x∈N5(w)

where Nk(w) is the set of the k nearest neighbors of w. Compared to the gek
models, the smoothed additive baseline modiﬁes the sentence vector by adding the
vectors of related words. Thus, it represents a useful comparison term for under-
standing the actual added value of the structural aspects of SDM.10

3.2.3 The Structured Distributional Models

The SDM introduced in Section 2 consists of a full sr including the linguistic
−→
−→
conditions vector
AC. In this section, we detail
LC and the event knowledge vector
the hyperparameter setting for the actual implementation of the model.

Distributional Event Graph We included in the graph only events with a mini-
mum frequency of 5 in the training corpora. The edges of the graph were weighted

8 We tested diﬀerent values for the dimension hyperparameter, and we noticed that vec-
tors with higher dimensionality lead to constant improvements on the thematic ﬁt
datasets. The best results were obtained with 400 dimensions.

9 We have experimented with k = 2, 5, 10 and, although the scores do not signiﬁcantly

diﬀer, this baseline model reports slightly better scores for k = 5.

10 We would like to thank one of the anonymous reviewers for the suggestion.

16

Chersoni et al.

with Smoothed LMI. Given a triple composed by the words w1 and w2, and a syn-
tactic relation s linking them, we computed its weight by using a smoothed version
of the Local Mutual Information (Evert 2004):

(5)

LM Iα(w1, w2, s) = f (w1, w2, s) ∗ log(

P (w1, w2, s)
P (w1) ∗ Pα(w2) ∗ P (s)

)

where the smoothed probabilities are deﬁned as follows:

(6)

Pα(x) =

f (x)α
x f (x)α

(cid:80)

This type of smoothing, with α = 0.75, was chosen to mitigate the bias of MI
statistical association measures towards rare events (Levy et al. 2015). While this
formula only involves pairs (as only pairs were employed in the experiments), it is
easily extensible to more complex tuples of elements.

Re-ranking settings For each word in the dataset items, the top 50 associated
words were retrieved from deg. Both for the re-ranking phase and for the construc-
tion of the ﬁnal representation, the event knowledge vectors (i.e., the role vectors
−→
−→r and the ac vector
AC) are built from the top 20 elements of each weighted list.
As detailed in Section 2.2, the ranking process in SDM can be performed in the
forward direction and in the backward direction at the same time (i.e., the ac can
be used to re-rank newly retrieved information and vice versa, respectively), but
for simplicity we only implemented the forward ranking.

Scoring As in SDM the similarity computations with the target words involves
two separate vectors, we combined the similarity scores with addition. Thus, given
a target word in a sentence sent, the score for SDM will be computed as:

(7)

score(target, sent) = cos(

−−−−→
target,

−→
LC(sent)) + cos(

−−−−→
target,

−→
AC(sent))

In all settings, we assume the model to be aware of the syntactic parse of the
test items. In DTFit, word order fully determines the syntactic constituents, as the
sentences are always in the subject verb object [location-obl—instrument-obl] order.
In RELPRON, on the other hand, the item contains information about the relation
that is being tested: in the subject relative clauses, the properties always show the
verb followed by the argument (e.g., telescope: device that detects planets), while in
the object relative clauses the properties always present the opposite situation (e.g.,
telescope: device that observatory has). In the present experiments, we did not use
−→
AC, and we restricted the
the predictions on non-expressed arguments to compute

A Structured Distributional Model

17

evaluation to the representation of the target argument. For example, in the DTFit
Patients set,

−→
AC(sent) only contains the

−−→
dobj centroid.

3.2.4 LSTM Neural Language Model.

We also compared the additive vector baselines and SDM with an LSTM neural
network, taking as input word2Vec embeddings. For every task, we trained the
LSTM on syntactically-labeled tuples (extracted from the same training corpora
used for the other models), with the objective of predicting the relevant target. In
DTFit, for example, for the Location task, in the tuple student learn history library,
the network is trained to predict the argument library given the tuple student learn
history. Similarly, in RELPRON, for the tuple engineer patent design, the LSTM is
trained to predict engineer in the subject task and design in the object task, given
patent design and engineer patent respectively.

In both DTFit and RELPRON, for each input tuple, we took the top N network
predictions (we tested with N = 3, 5, 10, and we always obtained the best results
with N = 10), we averaged their respective word embeddings, and we used the
vector cosine between the resulting vector and the embedding of the target reported
in the gold standard.

The LSTM is composed by: i.) an input layer of the same size of the word2Vec
embeddings (400 dimensions, with dropout=0.1); ii.) a single-layer monodirectional
LSTM with l hidden layers (where l = 2 when predicting Patients and l = 3 when
predicting Locations) of the same size of the embeddings; iii.) a linear layer (again
with dropout= 0.1) of the same size of the embeddings, which takes in input the
average of the hidden layers of the LSTM; iv.) and ﬁnally a softmax layer that
projects the ﬁller probability distribution over the vocabulary.

4 Results and Discussion

4.1 RELPRON

Given the targets and the composed vectors of all the deﬁnitions in RELPRON,
we assessed the cosine similarity of each pair and computed the Mean Average
Precision scores shown in Table 2. First of all, the Skip-Gram based models always
turn out to be the best performing ones, with rare exceptions, closely followed by
the C-Phrase ones. The scores of the additive models are slightly inferior, but very
close to those reported by Rimell et al. (2016), while the LSTM model lags behind
vector addition, improving only when the parameter N is increased. Results seem
to conﬁrm the original ﬁndings: even with very complex models (in that case, the
Lexical Function Model by Paperno et al. 2014), it is diﬃcult to outperform simple
vector addition in compositionality tasks.

Interestingly, SDM shows a constant improvement over the simple vector addition
equivalents (Table 2), with the only exception of the composition of the verb and the
argument. All the results for the headN oun+verb+arg composition are, to the best
of our knowledge, the best scores reported so far on the dataset. Unfortunately, given

18

Chersoni et al.

Word combination

R&al.

SG CBOW C-Phrase

Additive

Smoothed

verb
arg
head noun+verb
head noun+arg
verb+arg
head noun+verb+arg

verb
arg
head noun+verb
head noun+arg
verb+arg
head noun+verb+arg

SDM

verb
arg
head noun+verb
head noun+arg
verb + arg
head noun + verb + arg

0.18
0.35
0.26
0.45
0.40
0.50

-
-
-
-
-
-

-

-
-
-
-
-
-

0.16
0.33
0.26
0.44
0.43
0.50

0.15
0.35
0.24
0.45
0.41
0.49

0.21
0.38
0.27
0.50
0.41
0.54

0.16
0.32
0.25
0.46
0.36
0.47

0.16
0.33
0.23
0.46
0.36
0.46

0.20
0.36
0.28
0.50
0.36
0.52

LSTM

LSTM 10

0.10

0.32

0.13
0.37
0.21
0.45
0.41
0.47

0.14
0.40
0.22
0.49
0.41
0.47

-

0.19
0.41
0.26
0.50
0.41
0.54

Table 2. Results for the Vector Addition Baseline, Smoothed Vector Addition Base-
line, LSTM and the Structured Distributional Model (SDM) on the RELPRON
development set (Mean Average Precision scores). Rows refer to the diﬀerent word
combinations tested in Rimell et al. 2016 (R&al.).

the relatively small size of RELPRON, the improvement of the gek models fails to
reach signiﬁcance (p > 0.1 for all comparisons between a basic additive model and
its respective augmentation with deg, p−values computed with the Wilcoxon rank
sum test). Compared to SDM, the Smoothed Vector Addition baseline seems to
be way less consistent (Table 2): for some combinations and for some vector types,
adding the nearest neighbors is detrimental. We take these results as supporting the
added value of the structured event knowledge and the sr update process in SDM,
over the simple enrichment of vector addition with nearest neighbors. Finally, we
can notice that the Skip-Gram vectors have again an edge over the competitors,
even over the syntactically-informed C-Phrase vectors.

A Structured Distributional Model

19

Dataset

SG CBOW C-Phrase

Additive

Smoothed

LSTM

SDM

Patients
Locations

0.63
0.74

Patients
Locations

0.58
0.74

Patients
Locations

Patients
Locations

ns
0.58

0.65
0.75

0.52
0.70

0.51
0.71

0.42
0.60

0.60
0.74

0.58
0.76

-
-

0.62**
0.74

0.66 *
0.76

Table 3. Results for the Vector Addition Baseline, Smoothed Vector Addition Base-
line, LSTM and the Structured Distributional Model (SDM) on the Patients and
Locations subsets of DTFit. The scores are expressed in terms of Spearman cor-
relation with the gold standard ratings. The LSTM scores refer to the best con-
ﬁguration, with N = 10 and vectors of size 400. The statistical signiﬁcance of
the improvements over the additive baseline is reported as follows: * p < 0.05, **
p < 0.01 (p-values computed with Fisher’s r-to-z transformation, one-tailed test).
ns = non signiﬁcant correlation.

4.2 DTFit

At a ﬁrst glance, the results on DTFit follow a similar pattern (Table 3): The
three embedding types perform similarly, although in this case the CBOW vectors
perform much worse than the others in the Patients dataset. LSTM also largely
lags behind all the additive models, showing that thematic ﬁt modeling is not a
trivial task for language models, and that more complex neural architectures are
required in order to obtain state-of-the-art results (Tilk et al. 2016).11

The results for SDM again show that including the deg information leads to
improvements in the performances (Table 3). While on the Locations the diﬀerence
is only marginal, also due to the smaller number of test items, two models out of
three showed signiﬁcantly higher correlations than their respective additive base-
lines. The increase is particularly noticeable for the CBOW vectors that, in their
augmented version, manage to ﬁll the gap with the other models and to achieve a
competitive performance. However, it should also be noticed that there is a striking
diﬀerence between the two subsets of DTFit: while on patients the advantage of
the gek models on both the baselines is clear, on locations the results are almost
indistinguishable from those of the smoothed additive baseline, which simply adds

11 It should also be noticed that our LSTM baseline has been trained on simple syntactic
dependencies, while state-of-the-art neural models rely simultaneously on dependencies
and semantic role labels (Tilk et al. 2016, Hong et al. 2018).

20

Chersoni et al.

the nearest neighbours to the vectors of the words in the sentence. This complies
with previous studies on thematic ﬁt modeling with dependency-based distribu-
tional models (Sayeed et al. 2015, Santus et al. 2017). Because of the ambiguous
nature of the prepositions used to identify potential locations, the role vectors used
by SDM can be very noisy. Moreover, since most locative complements are optional
adjuncts, it is likely that the event knowledge extracted from corpora contain a
much smaller number of locations. Therefore, the structural information about lo-
cations in deg is probably less reliable and does not provide any clear advantage
compared to additive models.

Concerning the comparison between the diﬀerent types of embeddings, Skip-
Gram still retains an advantage over C-Phrase in its basic version, while it is out-
performed when the latter vectors are used in SDM. However, the diﬀerences are
clearly minimal, suggesting that the structured knowledge encoded in the C-Phrase
embeddings is not a plus for the thematic ﬁt task. Concerning this point, it must
be mentioned that most of the current models for thematic ﬁt estimation rely on
vectors relying either on syntactic information (Baroni and Lenci 2010, Greenberg
et al. 2015, Santus et al. 2017, Chersoni et al. 2017) or semantic roles (Sayeed et al.
2015, Tilk et al. 2016). On the other hand, our results comply with studies like
Lapesa and Evert (2017), who reported comparable performance for bag-of-words
and dependency-based models on several semantic modeling tasks, thus questioning
whether the injection of linguistic structure in the word vectors is actually worth its
processing cost. However, this is the ﬁrst time that such a comparison is carried out
on the basis of the DTFit dataset, while previous studies proposed slightly diﬀer-
ent versions of the task and evaluated their systems on diﬀerent benchmarks.12 A
more extensive and in-depth study is required in order to formulate more conclusive
arguments on this issue.

Another constant ﬁnding of previous studies on thematic ﬁt modeling was that
high-dimensional, count-based vector representations perform generally better than
dense word embeddings, to the point that Sayeed et al. (2016) stressed the sensi-
tivity of this task to linguistic detail and to the interpretability of the vector space.
Therefore, we tested whether vector dimensionality had an impact on task perfor-
mance (Table 4). Although the observed diﬀerences are generally small, we noticed
that higher-dimensional vectors are generally better in the DTFit evaluation and,
in one case, the diﬀerences reach a marginal signiﬁcance (i.e., the diﬀerence between
the 100-dimensional and the 400-dimensional basic Skip-Gram model is marginally
signiﬁcant at p < 0.1). This point will also deserve future investigation, but it
seems plausible that for this task embeddings beneﬁt from higher dimensionality
for encoding more information, as it has been suggested by Sayeed and colleagues.
However, these advantages do not seem to be related to the injection of linguistic
structure directly in the embeddings (i.e., not to the direct use of syntactic contexts

12 In datasets such as McRae et al. (1998) and Pad´o (2007), the verb-ﬁller compatibility is
modeled without taking into account the inﬂuence of the other ﬁllers. On the other hand,
studies on the composition and update of argument expectations generally propose
evaluations in terms of classiﬁcation tasks (Lenci 2011, Chersoni et al. 2017) instead of
assessing directly the correlation with human judgements.

A Structured Distributional Model

21

Dimensions Additive SDM

100
200
300
400

0.58
0.58
0.60
0.64

0.63
0.63
0.64
0.65

Table 4. Spearman correlations of the Vector Addition Baseline and the
Structured Distributional Model (SDM) based on Skip-Gram on the DTFit
patients subset.

Model

Additive Smoothed SDM ∆

CBOW
SG
C-Phrase

0.18
0.29
0.30

0.18
0.24
0.29

+ 0.12 *
0.30
+ 0.04
0.33
0.37 + 0.07

Table 5. Comparison of the performance of the Vector Addition Baseline, Smoothed
Vector Addition Baseline, and the Structured Distributional Model (SDM) on the
typical items of DTFit Patients (Spearman correlations). ∆ reports the SDM im-
provements over the basic additive models. Signiﬁcance is noted with the following
notation: * p < 0.05.

for training the vectors), as bag-of-words models perform similarly to - if not better
than - a syntactic-based model like C-Phrase. We leave to future research a sys-
tematic comparison with sparse count-based models to assess whether interpretable
dimensions are advantageous for modeling context-sensitive thematic ﬁt.

4.3 Error Analysis

One of our basic assumptions about gek is that semantic memory stores representa-
tions of typical events and their participants. Therefore, we expect that integrating
gek into our models might lead to an improvement especially on the typical items
of the DTFit dataset. A quick test with the correlations revealed that this is ac-
tually the case (Table 5): all models showed increased Spearman correlations on
the tuples in the typical condition (and in the larger Patients subset of DTFit,
the increase is signiﬁcant at p < 0.05 for the CBOW model), while they remain
unchanged or even decrease for the tuples in the atypical conditions. Notice that
this is true only for SDM, which is enriched with gek. On the other hand, the
simple addition of the nearest neighbors never leads to improvements, as proved
by the low correlation scores of the smoothed additive baseline. As new and larger
datasets for compositionality tasks are currently under construction (Vassallo et al.
2018), it will be interesting to assess the consistency of these results.

22

Chersoni et al.

SDM

Subset

SG

CBOW C-Phrase

sbj

obj

sbj

obj

sbj

obj

head noun+verb
head noun+arg
verb+arg
head noun+verb+arg

0.29
0.54
0.45
0.56

0.31
0.57
0.47
0.61

0.32
0.54
0.40
0.58

0.32
0.56
0.43
0.57

0.29
0.56
0.47
0.60

0.28
0.57
0.47
0.58

Table 6. Comparison of the Structured Distributional Model (SDM) performance
(MAP) on the subject and object relative clauses in RELPRON.

Turning to the RELPRON dataset, we noticed that the diﬀerence between subject
and object relative clauses is particularly relevant for SDM, which generally shows
better performances on the latter. Table 6 summarizes the scores component on the
two subsets. While relying on syntactic dependencies, SDM also processes properties
in linear order: the verb+arg model, therefore, works diﬀerently when applied to
subject clauses than to object clauses. In the subject case, in fact, the verb is found
ﬁrst, and then its expectations are used to re-rank the object ones. In the object
case, on the other hand, things proceed the opposite way: at ﬁrst the subject is
found, and then its expectations are used to re-rank the verb ones. Therefore, the
event knowledge triggered by the verb seems not only less informative than the one
triggered by the argument, but it is often detrimental to the composition process.

5 Conclusion

In this contribution, we introduced a Structured Distributional Model (SDM) that
represents sentence meaning with formal structures derived from DRT and including
embeddings enriched with event knowledge. This is modeled with a Distributional
Event Graph that represents events and their prototypical participants with distri-
butional vectors linked in a network of syntagmatic relations extracted from parsed
corpora. The compositional construction of sentence meaning in SDM is directly
inspired by the principles of dynamic semantics. Word embeddings are integrated
in a dynamic process to construct the semantic representations of sentences: con-
textual event knowledge aﬀects the interpretation of following expressions, which
cue new information that updates the current context.

Current methods for representing sentence meaning generally lack information
about typical events and situation, while SDM rests on the assumption that such
information can lead to better compositional representations and to an increased
capacity of modeling typicality, which is one striking capacity of the human pro-
cessing system. This corresponds to the hypothesis by Baggio and Hagoort (2011)
that semantic compositionality actually results from a balance between storage and
computation: on the one hand, language speakers rely on a wide amount of stored
events and scenarios for common, familiar situations; on the other hand, a compo-

A Structured Distributional Model

23

sitional mechanism is needed to account for our understanding of new and unheard
sentences. Processing complexity, as revealed by eﬀects such as the reduced ampli-
tude of the N400 component in ERP experiments, is inversely proportional to the
typicality of the described events and situations: The more they are typical, the
more they will be coherent with already-stored representations.

We evaluated SDM on two tasks, namely a classical similarity estimation tasks
on the target-deﬁnition pairs of the RELPRON dataset (Rimell et al. 2016) and
a thematic ﬁt modeling task on the event tuples of the DTFit dataset (Vassallo
et al. 2018). Our results still proved that additive models are quite eﬃcient for
compositionality tasks, and that integrating the event information activated by
lexical items improves the performance on both the evaluation datasets. Particularly
interesting for our evaluation was the performance on the DTFit dataset, since
this dataset has been especially created with the purpose of testing computational
models on their capacity to account for human typicality judgments about event
participants. The reported scores on the latter dataset showed that not only SDM
improves over simple and smoothed additive models, but also that the increase in
correlation concerns the dataset items rated as most typical by human subjects,
fulﬁlling our initial predictions.

Diﬀerently from other distributional semantic models tested on the thematic ﬁt
task, ‘structure’ is now externally encoded in a graph, whose nodes are embeddings,
and not directly in the dimension of the embeddings themselves. The fact that the
best performing word embeddings in our framework are the Skip-Gram ones is
somewhat surprising, and against the ﬁnding of previous literature in which bag-of-
words models were always described as struggling on this task (Baroni et al. 2014,
Sayeed et al. 2016). Given our results, we also suggested that the dimensionality
of the embeddings could be an important factor, much more than the choice of
training them on syntactic contexts.

References

Adi, Y., E. Kermany, Y. Belinkov, O. Lavi, and Y. Goldberg 2017. Fine-grained Analysis
In Proceedings of ICLR,

of Sentence Embeddings Using Auxiliary Prediction Tasks.
Toulon, France.

Arora, S., Y. Liang, and T. Ma 2017. A Simple But Tough-to-beat Baseline For Sentence

Embeddings. In Proceedings of ICLR, Toulon, France.

Baggio, G. and P. Hagoort 2011. The Balance between Memory and Uniﬁcation in Se-
mantics: A Dynamic Account of the N400. Language and Cognitive Processes 26 (9),
1338–1367.

Bar, M. 2009. The Proactive Brain. Philosophical Transactions of the Royal Society

Bar, M., E. Aminoﬀ, M. Mason, and M. Fenske 2007. The Units of Thought. Hippocam-

B 364 (March), 1235–1243.

pus 17 (6), 420–428.

Baroni, M., R. Bernardi, and R. Zamparelli 2013. Frege in Space: A Program for Compo-

sitional Distributional Semantics. Linguistic Issues in Language Technologies 9.

Baroni, M., S. Bernardini, A. Ferraresi, and E. Zanchetta 2009. The WaCky Wide Web:
A Collection of Very Large Linguistically Processed Web-Crawled Corpora. Language
Resources and Evaluation 43 (3), 209–226.

24

Chersoni et al.

Baroni, M., G. Dinu, and G. Kruszewski 2014. Don’t Count, Predict! A Systematic Com-
parison of Context-Counting vs. Context-Predicting Semantic Vectors. In Proceedings
of ACL, Baltimore, MD, USA, pp. 238–247.

Baroni, M. and A. Lenci 2010. Distributional Memory: A General Framework for Corpus-

based Semantics. Computational Linguistics 36 (4), 673–721.

Beltagy, I., S. Roller, P. Cheng, K. Erk, and R. J. Mooney 2016. Representing Meaning
with a Combination of Logical and Distributional Models. Computational Linguis-
tics 42 (4), 763–808.

Bicknell, K., J. L. Elman, M. Hare, K. McRae, and M. Kutas 2010. Eﬀects of Event
Knowledge in Processing Verbal Arguments. Journal of Memory and Language 63 (4),
489–505.

Binder, J. R. 2016.

In Defense of Abstract Conceptual Representations. Psychonomic

Bulletin & Review 23, 1096–1108.

Boleda, G. and A. Herbelot 2016. Formal Distributional Semantics: Introduction to the

Special Issue. Computational Linguistics 42 (4), 619–635.

Chersoni, E., A. Lenci, and P. Blache 2017. Logical Metonymy in a Distributional Model
of Sentence Comprehension. In Proceedings of *SEM, Vancouver, Canada, pp. 168–177.
Chersoni, E., E. Santus, P. Blache, and A. Lenci 2017. Is Structure Necessary for Mod-
eling Argument Expectations in Distributional Semantics? In Proceedings of IWCS,
Montpellier, France.

Chersoni, E., E. Santus, A. Lenci, P. Blache, and C.-R. Huang 2016. Representing Verbs
In Proceedings of EMNLP,

with Rich Contexts: An Evaluation on Verb Similarity.
Austin, TX, USA, pp. 1967—-1972.

Clark, A. 2013. Whatever Next? Predictive Brains, Situated Agents, and the Future of

Cognitive Science. Behavioral and Brain Sciences 36 (3), 1–73.

Conneau, A., G. Kruszewski, G. Lample, L. Barrault, and M. Baroni 2018. What you can
cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties.
In Proceedings of ACL, Melbourne, Australia, pp. 2126–2136.

Elman, J. L. 2009. On the Meaning of Words and Dinosaur Bones: Lexical Knowledge

Without a Lexicon. Cognitive Science 33 (4), 547–582.

Elman, J. L. 2014. Systematicity in the Lexicon: On Having Your Cake and Eating It Too.
In P. Calvo and J. Symons (Eds.), The Architecture of Cognition: Rethinking Fodor and
Pylyshyn’s Systematicity Challenge, pp. 115–145. Cambridge, MA: The MIT Press.
Erk, K. 2007. A Simple, Similarity-Based Model for Selectional Preferences. In Proceedings

of ACL, Prague, Czech Republic, pp. 216–223.

Erk, K. 2012. Vector Space Models of Word Meaning and Phrase Meaning: A Survey.

Linguistics and Language Compass 6 (10), 635–653.

Erk, K., S. Pad´o, and U. Pad´o 2010. A Flexible, Corpus-Driven Model of Regular and

Inverse Selectional Preferences. Computational Linguistics 36, 723–763.

Ettinger, A., A. Elgohary, and P. Resnik 2016. Probing for Semantic Evidence of Com-
position by Means of Simple Classiﬁcation Tasks. In Proceedings of the ACL Workshop
on Evaluating Vector-Space Representations for NLP, Berlin, Germany, pp. 134–139.
Evert, S. 2004. The Statistics of Word Cooccurrences Word Pairs and Collocations. Ph.

D. thesis, University of Stuttgart.

Ferretti, T. R., K. McRae, and A. Hatherell 2001. Integrating Verbs, Situation Schemas,
and Thematic Role Concepts . Journal of Memory and Language 44 (4), 516 – 547.

Greenberg, C., A. B. Sayeed, and V. Demberg 2015.

Improving Unsupervised Vector-
space Thematic Fit Evaluation via Role-ﬁller Prototype Clustering. In Proceedings of
NAACL-HLT, Denver, CO, USA, pp. 21–31.

Hagoort, P. 2013. MUC (Memory, Uniﬁcation, Control) and Beyond. Frontiers in Psy-

chology 4 (JUL), 1–13.

Hagoort, P. 2016. MUC (Memory, Uniﬁcation, Control): A Model on the Neurobiology of
Language beyond Single Word Processing. In G. Hickok and S. Small (Eds.), Neurobi-
ology of Language, Volume 28, pp. 339–347. Amsterdam: Elsevier.

A Structured Distributional Model

25

Hare, M., M. Jones, C. Thomson, S. Kelly, and K. McRae 2009. Activating Event Knowl-

edge. Cognition 111 (2), 151–167.

Heim, I. 1983. File Change Semantics and the Familiarity Theory of Deﬁniteness.

In
R. B¨auerle, C. Schwarze, and A. von Stechow (eds.) (Eds.), Meaning, Use, and Inter-
pretation of Language, pp. 164–189. Berlin: De Gruyter.

Hill, F., K. Cho, and A. Korhonen 2016. Learning Distributed Representations of Sentences
from Unlabelled Data. In Proceedings of NAACL-HLT, San Diego, CA, USA, pp. 1367–
–1377.

Hong, X., A. Sayeed, and V. Demberg 2018. Learning Distributed Event Representations
In Proceedings of *SEM, New Orleans, LA, USA, pp.

with a Multi-Task Approach.
11–21.

Kamp, H. 2013. Meaning and the Dynamics of Interpretation: Selected papers by Hans

Kamp. Leiden-Boston: Brill.

Kamp, H. 2016. Entity Representations and Articulated Contexts: An Exploration of the

Semantics and Pragmatics of Deﬁnite Noun Phrases. Unpublished manuscript.

Kiros, R., Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba, and S. Fidler
2015. Skip-Thought Vectors. In Advances in NIPS, Montr´eal, Canada, pp. 3294–3302.
Kruszewski, G., A. Lazaridou, M. Baroni, et al. 2015. Jointly Optimizing Word Repre-
sentations for Lexical and Sentential Tasks with the C-Phrase Model. In Proceedings of
ACL, Beijing, China, pp. 971–981.

Kuperberg, G. R. and T. F. Jaeger 2015. What do we mean by prediction in language

comprehension? Language Cognition & Neuroscience 3798, 1–70.

Lapesa, G. and S. Evert 2017. Large-Scale Evaluation of Dependency-Based DSMs: Are

They Worth the Eﬀort? In Proceedings of EACL, Valencia, Spain, pp. 394–400.

Leech, G. N. 1992. 100 Million Words of English: The British National Corpus (BNC).
Lenci, A. 2011. Composing and Updating Verb Argument Expectations: A Distributional
In Proceedings of the ACL Workshop on Cognitive Modeling and

Semantic Model.
Computational Linguistics, Portland, OR, USA, pp. 58–66.

Lenci, A. 2018a. Distributional Models of Word Meaning. Annual Review of Linguistics 4,

151–171.

Lenci, A. 2018b. Dynamic Distributional Semantics. Unpublished manuscript.
Levy, O. and Y. Goldberg 2014. Neural Word Embedding as Implicit Matrix Factorization.

In Advances in NIPS, Montr´eal, Canada, pp. 2177–2185.

Levy, O., Y. Goldberg, and I. Dagan 2015.

Improving Distributional Similarity with
Lessons Learned from Word Embeddings. Transactions of the Association for Compu-
tational Linguistics 3, 211–225.

Manning, C., M. Surdeanu, J. Bauer, J. Finkel, S. Bethard, and D. McClosky 2014. The
Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of ACL (Sys-
tem Demonstrations), Baltimore, MD, USA, pp. 55–60.

Matsuki, K., T. Chow, M. Hare, J. L. Elman, C. Scheepers, and K. McRae 2011. Event-
Based Plausibility Immediately Inﬂuences Online Language Comprehension. Journal
of Experimental Psychology: Learning, Memory, and Cognition 37 (4), 913.

McNally, L. 2017. Kinds, descriptions of kinds, concepts, and distributions. In K. Balogh
and W. Petersen (Eds.), Bridging Formal and Conceptual Semantics. Selected Papers
of BRIDGE-14, pp. 39–61. D¨usseldorf: DUP.

McNally, L. and G. Boleda 2017. Conceptual vs. Referential Aﬀordance in Concept Com-
position. In J. A. Hampton and Y. Winter (Eds.), Compositionality and Concepts in
Linguistics and Psychology, pp. 245–267. Berlin: Springer.

McRae, K., M. Hare, J. L. Elman, and T. Ferretti 2005. A Basis for Generating Expectan-

cies for Verbs from Nouns. Memory & Cognition 33 (7), 1174–1184.

McRae, K. and K. Matsuki 2009. People Use their Knowledge of Common Events to
Understand Language, and Do So as Quickly as Possible. Language and Linguistics
Compass 3 (6), 1417–1429.

26

Chersoni et al.

McRae, K., M. J. Spivey-Knowlton, and M. K. Tanenhaus 1998. Modeling the Inﬂuence
of Thematic Fit (and Other Constraints) in Online Sentence Comprehension. Journal
of Memory and Language 38 (3), 283–312.

Meltzer-Asscher, A., J. E. Mack, E. Barbieri, and C. K. Thompson 2015. How the
Brain Processes Diﬀerent Dimensions of Argument Structure Complexity: Evidence
from fMRI. Brain and Language 142, 65–75.

Metusalem, R., M. Kutas, T. P. Urbach, M. Hare, K. McRae, and J. L. Elman 2012. Gen-
eralized Event Knowledge Activation During Online Sentence Comprehension. Journal
of Memory and Language 66 (4), 545–567.

Mikolov, T., I. Sutskever, K. Chen, G. S. Corrado, and J. Dean 2013. Distributed Rep-
resentations of Words and Phrases and their Compositionality. In Advances in NIPS,
Lake Tahoe, NV, USA, pp. 3111–3119.

Mitchell, J. and M. Lapata 2010. Composition in Distributional Models of Semantics.

Cognitive Science 34 (8), 1388–1429.

Paczynski, M. and G. R. Kuperberg 2012. Multiple Inﬂuences of Semantic Memory on
Sentence Processing: Distinct Eﬀects of Semantic Relatedness on Violations of Real-
World Event/State Knowledge and Animacy Selection Restrictions. Journal of Memory
and Language 67 (4), 426–448.

Pad´o, U. 2007. The Integration of Syntax and Semantic Plausibility in a Wide-coverage

Model of Human Sentence Processing. Ph. D. thesis, University of Stuttgart.

Palangi, H., P. Smolensky, X. He, and L. Deng 2018. Question-Answering with
Grammatically-Interpretable Representations. In Proceedings of AAAI, New Orleans,
LA, USA, pp. 5350–5357.

Paperno, D., N. T. Pham, and M. Baroni 2014. A Practical and Linguistically-Motivated
Approach to Compositional Distributional Semantics. In Proceedings of ACL, Volume 1,
Baltimore, MD, USA, pp. 90–99.

Pustejovsky, J. 1995. The Generative Lexicon. Cambridge, MA: MIT Press.
Rimell, L., J. Maillard, T. Polajnar, and S. Clark 2016. RELPRON: A Relative Clause
Evaluation Data Set for Compositional Distributional Semantics. Computational Lin-
guistics 42 (4), 661–701.

Santus, E., E. Chersoni, A. Lenci, and P. Blache 2017. Measuring Thematic Fit with
Distributional Feature Overlap. In Proceedings of EMNLP, Copenhagen, Denmark, pp.
648—-658.

Sayeed, A., V. Demberg, and P. Shkadzko 2015. An Exploration of Semantic Features in an
Unsupervised Thematic Fit Evaluation Framework. Italian Journal of Linguistics 1 (1),
25–40.

Sayeed, A., C. Greenberg, and V. Demberg 2016. Thematic Fit Evaluation: an Aspect
of Selectional Preferences. In Proceedings of the ACL Workshop for Evaluating Vector
Space Representations for NLP, Berlin, Germany, pp. 99–105.

Thompson, C. K. and A. Meltzer-Asscher 2014. Neurocognitive Mechanisms of Verb Argu-
ment Structure Processing. In A. Bachrach, I. Roy, and L. Stockall (Eds.), Structuring
the Argument: Multidisciplinary Research on Verb Argument Structure, pp. 141–168.
Amsterdam: John Benjamins.

Tian, R., N. Okazaki, and K. Inui 2017. The Mechanism of Additive Composition. Machine

Learning 106 (7), 1083–1130.

Tilk, O., V. Demberg, A. B. Sayeed, D. Klakow, and S. Thater 2016. Event Participant
Modelling with Neural Networks. In Proceedings of EMNLP, Austin, TX, USA, pp.
171–182.

Vassallo, P., E. Chersoni, E. Santus, A. Lenci, and P. Blache 2018. Event Knowledge in
Sentence Processing: A New Dataset for the Evaluation of Argument Typicality. In Pro-
ceedings of the LREC Workshop on Linguistic and Neurocognitive Resources (LiNCR),
Miyazaki, Japan, pp. 1–7.

A Structured Distributional Model

27

Washtell, J. 2010. Expectation Vectors: A Semiotics Inspired Approach to Geometric
Lexical-Semantic Representation. In Proceedings of the ACL Workshop on GEometrical
Models of Natural Language Semantics, Uppsala, Sweden, pp. 45–50.

Williams, A., S. Reddigari, and L. Pylkk¨anen 2017. Early Sensitivity of Left Perisylvian

Cortex to Relationality in Nouns and Verbs. Neuropsychologia 100, 131–143.

Zarcone, A., J. Utt, and S. Pad´o 2012. Modeling Covert Event Retrieval in Logical
Metonymy: Probabilistic and Distributional Accounts. In Proceedings of the NAACL
Workshop on Cognitive Modeling and Computational Linguistics, Montr´eal, Canada,
pp. 70–79.

Zaremba, W., I. Sutskever, and O. Vinyals 2014. Recurrent Neural Network Regulariza-

tion. arXiv preprint arXiv:1409.2329 .

Zhu, X., T. Li, and G. de Melo 2018. Exploring Semantic Properties of Sentence Embed-

dings. In Proceedings of ACL, Melbourne, Australia, pp. 632–637.

