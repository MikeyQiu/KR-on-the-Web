0
2
0
2
 
r
p
A
 
7
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
0
1
9
7
0
.
1
1
7
1
:
v
i
X
r
a

Domain Generalization by Marginal Transfer Learning

Gilles Blanchard
Institut f¨ur Mathematik
Universit¨at Potsdam

Aniket Anand Deshmukh
Microsoft AI & Research
¨Urun Dogan
Microsoft AI & Research

Gyemin Lee
Dept. Electronic and IT Media Engineering
Seoul National University of Science and Technology

Clayton Scott
Electrical and Computer Engineering
University of Michigan

blanchard@math.uni-potsdam.de

aniketde@umich.edu

urundogan@gmail.com

gyemin@seoultech.ac.kr

clayscot@umich.edu

Abstract
In the problem of domain generalization (DG), there are labeled training data sets from
several related prediction problems, and the goal is to make accurate predictions on future
unlabeled data sets that are not known to the learner. This problem arises in several ap-
plications where data distributions ﬂuctuate because of environmental, technical, or other
sources of variation. We introduce a formal framework for DG, and argue that it can be
viewed as a kind of supervised learning problem by augmenting the original feature space
with the marginal distribution of feature vectors. While our framework has several con-
nections to conventional analysis of supervised learning algorithms, several unique aspects
of DG require new methods of analysis.

This work lays the learning theoretic foundations of domain generalization, building on
our earlier conference paper where the problem of DG was introduced (Blanchard et al.,
2011). We present two formal models of data generation, corresponding notions of risk, and
distribution-free generalization error analysis. By focusing our attention on kernel meth-
ods, we also provide more quantitative results and a universally consistent algorithm. An
eﬃcient implementation is provided for this algorithm, which is experimentally compared
to a pooling strategy on one synthetic and three real-world data sets.
Keywords: Domain Generalization, Generalization Error Bounds, Kernel Methods, Uni-
versal Consistency, Kernel Approximation

1. Introduction

Domain generalization (DG) is a machine learning problem where the learner has access to
labeled training data sets from several related prediction problems, and must generalize to
a future prediction problem for which no labeled data are available. In more detail, there
are N labeled training data sets Si = (Xij, Yij)1≤j≤ni, i = 1, . . . , N , that describe similar
but possibly distinct prediction tasks. The objective is to learn a rule that takes as input

1

a previously unseen unlabeled test data set X T
for these or possibly other unlabeled points from the associated learning task.

1 , . . . , X T
nT

, and accurately predicts outcomes

DG arises in several applications. One prominent example is precision medicine, where
a common objective is to design a patient-speciﬁc classiﬁer (e.g., of health status) based
on clinical measurements, such as an electrocardiogram or electroencephalogram. In such
measurements, patient-to-patient variation is common, arising from biological variations
between patients, or technical or environmental factors inﬂuencing data acquisition. Be-
cause of patient-to-patient variation, a classiﬁer that is trained on data from one patient
may not be well matched to another patient. In this context, domain generalization enables
the transfer of knowledge from historical patients (for whom labeled data are available) to a
new patient without the need to acquire training labels for that patient. A detailed example
in the context of ﬂow cytometry is given below.

We view domain generalization as a conventional supervised learning problem where
the original feature space is augmented to include the marginal distribution generating the
features. We refer to this reframing of DG as “marginal transfer learning,” because it reﬂects
the fact that in DG, information about the test task must be drawn from that tasks’ marginal
feature distribution. Leveraging this perspective, we formulate two statistical frameworks
for analyzing DG. The ﬁrst framework allows the observations within each data set to
have arbitrary dependency structure, and makes connections to the literature on Campbell
measures and structured prediction. The second framework is a special case of the ﬁrst,
assuming the data points are drawn iid within each task, and allows for a more reﬁned risk
analysis.

We further develop a distribution-free kernel machine that employs a kernel on the
aforementioned augmented feature space. Our methodology is shown to yield a universally
consistent learning procedure under both statistical frameworks, meaning that the domain
generalization risk tends to the best possible value as the relevant sample sizes tend inﬁnity,
with no assumptions on the data generating distributions. Although DG may be viewed as
a conventional supervised learning problem on an augmented feature space, the analysis is
nontrivial owing to unique aspects of the sampling plans and risks. We oﬀer a computation-
ally eﬃcient and freely available1 implementation of our algorithm, and present a thorough
experimental study validating the proposed approach on one synthetic and three real-world
data sets, including comparisons to a simple pooling approach.

To our knowledge, the problem of domain generalization was ﬁrst proposed and studied
by our earlier conference publication (Blanchard et al., 2011) which this work extends
in several ways.
It adds (1) a new statistical framework, the agnostic generative model
described below; (2) generalization error and consistency results for the new statistical
model; (3) an extensive literature review; (4) an extension to the regression setting in
both theory and experiments; (5) a more general statistical analysis, in particular, we no
longer assume a bounded loss, and therefore accommodate common convex losses such as
the hinge and logistic losses; (6) extensive experiments (the conference paper considered a
single small dataset); (7) a scalable implementation based on a novel extension of random
Fourier features; and (8) error analysis for the random Fourier features approximation.

1. https://github.com/aniketde/DomainGeneralizationMarginal

2

2. Motivating Application: Automatic Gating of Flow Cytometry Data

Flow cytometry is a high-throughput measurement platform that is an important clinical
tool for the diagnosis of blood-related pathologies. This technology allows for quantitative
analysis of individual cells from a given cell population, derived for example from a blood
sample from a patient. We may think of a ﬂow cytometry data set as a set of d-dimensional
attribute vectors (Xj)1≤j≤n, where n is the number of cells analyzed, and d is the number
of attributes recorded per cell. These attributes pertain to various physical and chemical
properties of the cell. Thus, a ﬂow cytometry data set may be viewed as a random sample
from a patient-speciﬁc distribution.

Now suppose a pathologist needs to analyze a new (test) patient with data (X T

j )1≤j≤nT .
Before proceeding, the pathologist ﬁrst needs the data set to be “puriﬁed” so that only cells
of a certain type are present. For example, lymphocytes are known to be relevant for the
diagnosis of leukemia, whereas non-lymphocytes may potentially confound the analysis. In
other words, it is necessary to determine the label Y T
j ∈ {−1, 1} associated to each cell,
where Y T

j = 1 indicates that the j-th cell is of the desired type.

In clinical practice this is accomplished through a manual process known as “gating.”
The data are visualized through a sequence of two-dimensional scatter plots, where at each
stage a line segment or polygon is manually drawn to eliminate a portion of the unwanted
cells. Because of the variability in ﬂow cytometry data, this process is diﬃcult to quantify
in terms of a small subset of simple rules. Instead, it requires domain-speciﬁc knowledge
and iterative reﬁnement. Modern clinical laboratories routinely see dozens of cases per day,
so it would be desirable to automate this process.

Since clinical laboratories maintain historical databases, we can assume access to a
number (N ) of historical (training) patients that have already been expert-gated. Because
of biological and technical variations in ﬂow cytometry data, the distributions P (i)
XY of
the historical patients will vary. To illustrate the ﬂow cytometry gating problem, we use
the NDD data set from the FlowCap-I challenge.2 For example, Fig. 1 shows exemplary
two-dimensional scatter plots for two diﬀerent patients – see caption for details. Despite
diﬀerences in the two distributions, there are also general trends that hold for all patients.
Virtually every cell type of interest has a known tendency (e.g., high or low) for most
measured attributes. Therefore, it is reasonable to assume that there is an underlying
distribution (on distributions) governing ﬂow cytometry data sets, that produces roughly
similar distributions thereby making possible the automation of the gating process.

3. Formal Setting and General Results

In this section we formally deﬁne domain generalization via two possible data generation
models together with associated notions of risk. We also provide a basic generalization error
bound for the ﬁrst of these data generation models.

Let X denote the observation space (assumed to be a Radon space) and Y ⊆ R the
output space. Let PX and PX ×Y denote the set of probability distributions on X and X ×Y,
respectively. The spaces PX and PX ×Y are endowed with the topology of weak convergence
and the associated Borel σ-algebras. The symbol ⊗ indicates a product measure.

2. We will revisit this data set in Section 8.5 where details are given.

3

Figure 1: Two-dimensional projections of multi-dimensional ﬂow cytometry data. Each row
corresponds to a single patient, and each column to a particular two-dimensional projection.
The distribution of cells diﬀers from patient to patient. The colors indicate the results of
gating, where a particular type of cell, marked dark (blue), is separated from all other cells,
marked bright (red). Labels were manually selected by a domain expert.

The disintegration theorem for joint probability distributions (see for instance Kallen-
berg, 2002, Theorem 6.4) tells us that (under suitable regularity properties, satisﬁed if X is
a Radon space) any element PXY ∈ PX ×Y can be written as a Markov semi-direct product
PXY = PX • PY |X , with PX ∈ PX , PY |X ∈ PY |X , where PY |X is the space of conditional
probability distributions of Y given X, also called Markov transition kernels from X to Y.
This speciﬁcally means that

E(X,Y )∼PXY [h(X, Y )] =

h(x, y)PY |X (dy|X = x)

PX (dx),

(1)

(cid:90) (cid:18)(cid:90)

(cid:19)

for any integrable function h : X × Y → R. Following common terminology in the statistical
learning literature, we will also call PY |X the posterior distribution (of Y given X).

We assume that N training samples Si = (Xij, Yij)1≤j≤ni, i = 1, . . . , N , are observed.
To allow for possibly unequal sample sizes ni, it is convenient to formally identify each
sample Si with its associated empirical distribution (cid:98)P (i)
j=1 δ(Xij ,Yij ) ∈ PX ×Y .
We assume that the ordering of the observations inside a given sample Si is arbitrary
and does not contain any relevant information. We also denote by (cid:98)P (i)
j=1 δXij ∈
PX the ith training sample without labels. Similarly, a test sample is denoted by ST =
(X T

j )1≤j≤nT , and the empirical distribution of the unlabeled data by (cid:98)P T
X .

XY = 1
ni

X = 1
ni

j , Y T

(cid:80)ni

(cid:80)ni

3.1 Data Generation Models

We propose two data generation models. The ﬁrst is most general, and includes the second
as a special case.

4

Assumption 1 (AGM) There exists a distribution PS on PX ×Y such that S1, . . . , SN are
i.i.d. realizations from PS.

We call this the agnostic generative model. This is a quite general model in which samples are
assumed to be identically distributed and independent of each other, but nothing particular
is assumed about the generation mechanism for observations inside a given sample, nor for
the (random) sample size.

We also introduce a more speciﬁc generative mechanism, where observations (Xij, Yij)
XY , a latent unobserved random distribu-

inside the sample Si are themselves i.i.d. from P (i)
tion, as follows.

Assumption 2 (2SGM) There exists a distribution µ on PX ×Y and a distribution ν on
N, such that (P (1)
XY , nN ) are i.i.d. realizations from µ ⊗ ν, and conditional to
(P (i)
XY , ni) the sample Si is made of ni i.i.d. realizations of (X, Y ) following the distribution
P (i)
XY .

XY , n1), . . . , (P (N )

This model, called the 2-stage generative model, is a subcase of (AGM): since the
(P (i)
It has been considered in the distinct but
XY , ni) are i.i.d., the samples Si also are.
related context of “learning to learn” (Baxter, 2000; see also a more detailed discussion
below, Section 4.2). Many of our results will hold for the agnostic generative model, but
the two-stage generative model allows for additional developments, as will be discussed
further below. This model was the one studied in our conference paper (Blanchard et al.,
2011).

3.2 Decision Functions and Augmented Feature Space

In domain generalization, the learner’s goal is to infer from the training data a general
rule that takes an arbitrary, previously unseen, unlabeled dataset corresponding to a new
prediction task, and produces a classiﬁer for that prediction task that could be applied to
any x (possibly outside the unlabeled data set). In other words, the learner should out-
put a mapping g : PX → (X → R). Equivalently, the learner should output a function
f : PX × X → R, where the two notations are related via g(PX )(x) = f (PX , x). In the
latter viewpoint, f may be viewed as a standard decision function on the “augmented”
or “extended” feature space PX × X , which facilitates connections to standard supervised
learning. We refer to this view of DG as marginal transfer learning, because the informa-
tion that facilitates generalization to a new task is conveyed entirely through the marginal
distribution. In the next two subsections, we present two deﬁnitions of the risk of a decision
function f , one associated to each of the two data generation models.

3.3 Risk and Generalization Error Bound under the Agnostic Generative

Model

Now consider a test sample ST = (X T
j )1≤j≤nT , whose labels are not observed by the
learner. If (cid:96) : R×Y (cid:55)→ R+ is a loss function for a single prediction, and predictions of a ﬁxed
decision function f on the test sample are given by (cid:98)Y T
j ), then the empirical

j = f ( (cid:98)P T

j , Y T

X , X T

5

average loss incurred on the test sample is

L(ST , f ) :=

(cid:96)( (cid:98)Y T

j , Y T

j ) .

1
nT

nT(cid:88)

j=1

Based on this, we deﬁne the risk of a decision function as the average of the above quantity
when test samples are drawn according to the same mechanism as the training samples:

E(f ) := E

ST ∼PS

(cid:2)L(ST , f )(cid:3) = E

ST ∼PS

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:96)(f ( (cid:98)P T

X , X T

(cid:35)
i ), Y T
i )

.

In a similar way, we deﬁne the empirical risk of a decision function as its average prediction
error over the training samples:

(cid:98)E(f, N ) :=

L(Si, f ) =

(cid:96)(f ( (cid:98)P (i)

X , Xij), Yij).

(2)

1
N

N
(cid:88)

i=1

1
N

N
(cid:88)

ni(cid:88)

1
ni

i=1

j=1

Remark 3 It is possible to understand the above setting as a particular instance of a struc-
tured output learning problem (Tsochantaridis et al., 2005; Bakır et al., 2007), in which
the input variable X ∗ is (cid:98)P T
X , and the “structured output” Y ∗ is the collection of labels
(Y T
i )1≤i≤nT (matched to their respective input points). As is generally the case for struc-
tured output learning, the nature of the problem and the “structure” of the outputs is very
much encoded in the particular form of the loss function. In our setting the loss function is
additive over the labels forming the collection Y ∗, and we will exploit this particular form
for our method and analysis.

Remark 4 The risk E(f ) deﬁned above can be described in the following way: consider the
random variable ξ := ( (cid:98)PXY ; (X, Y )) obtained by ﬁrst drawing (cid:98)PXY according to PS, then,
conditional to this, drawing (X, Y ) according to (cid:98)PXY . The risk is then the expectation of a
certain function of ξ (namely Ff (ξ) = (cid:96)(f ( (cid:98)PX , X), Y )). In probability theory literature, the
distribution of the variable ξ is known as the Campbell measure associated to the distribution
PS over the measure space PX ×Y ; this object is in particular of fundamental use in point
process theory (see, e.g., Daley and Vere-Jones (2008), Section 13.1). We will denote
it by C(PS) here. This intriguing connection suggests that more elaborate tools of point
process literature may ﬁnd their use to analyze DG when various classical point processes
are considered for the generating distribution. The Campbell measure will also appear in
the Rademacher analysis below.

The next result establishes an analogue of classical Rademacher analysis under the

agnostic generative model.

Theorem 5 (Uniform estimation error control under (AGM)) Let F be a class of
decision functions PX × X → R. Assume the following boundedness condition holds:

sup
f ∈F

sup
PX ∈PX

sup
(x,y)∈X ×Y

(cid:96)(f (PX , x), y) ≤ B(cid:96).

(3)

6

Under (AGM), if S1, . . . , SN are i.i.d. realizations from PS, then with probability at least
1 − δ with respect to the draws of the training samples:

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:98)E(f, N ) − E(f )
(cid:12)

sup
f ∈F

≤

E

2
N

( (cid:98)P (i)

XY ;(Xi,Yi))∼C(PS )⊗N

E(εi)1≤i≤N

(cid:34)

sup
f ∈F

(cid:12)
N
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)

i=1

εi(cid:96)(f ( (cid:98)P (i)

(cid:35)

(cid:12)
(cid:12)
(cid:12)
X , Xi), Yi)
(cid:12)
(cid:12)

(cid:114)

+ B(cid:96)

log(δ−1)
2N

,

(4)

where (εi)1≤i≤N are i.i.d. Rademacher variables, independent from ( (cid:98)P (i)

XY , (Xi, Yi))1≤i≤N .

(cid:12)
(cid:12)
(cid:12)
(cid:12)
Proof Since the (Si)1≤i≤N are i.i.d., supf ∈F
(cid:12) takes the form of a uniform
(cid:12) (cid:98)E(f, N ) − E(f )
deviation between average and expected loss over the function class F. We can therefore
apply standard analysis (Azuma-McDiarmid inequality followed by Rademacher complexity
analysis for a nonnegative bounded loss; see, e.g., Koltchinskii, 2001; Bartlett and Mendel-
son, 2002, Theorem 8) to obtain that with probability at least 1 − δ with respect to the
draw of the training samples (Si)1≤i≤N :

(cid:12)
(cid:12)
(cid:12) (cid:98)E(f, N ) − E(f )

(cid:12)
(cid:12)
(cid:12) ≤

2
N

sup
f ∈F

E(Si)1≤i≤N

E(εi)1≤i≤N

(cid:34)

sup
f ∈F

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

N
(cid:88)

i=1

(cid:35)

(cid:12)
(cid:12)
(cid:12)
εiL(Si, f )
(cid:12)
(cid:12)

(cid:114)

+ B(cid:96)

log(δ−1)
2N

,

where (εi)1≤i≤N are i.i.d. Rademacher variables, independent of (Si)1≤i≤N .

We may write

L(Si, f ) =

1
ni

ni(cid:88)

j=1

(cid:96)(f ( (cid:98)P (i)

X , Xij), Yij) = E

(X,Y )∼ (cid:98)P (i)
XY

(cid:104)

(cid:96)(f ( (cid:98)P (i)

(cid:105)
X , X), Y )

;

thus, we have

E(Si)1≤i≤N

E(εi)1≤i≤N

εiL(Si, f )

(cid:34)

sup
f ∈F

(cid:12)
N
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)

i=1
(cid:34)

(cid:12)
(cid:35)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= E

( (cid:98)P (i)

XY )1≤i≤N

E(εi)1≤i≤N

(cid:12)
N
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)

i=1

sup
f ∈F

εiE

(Xi,Yi)∼ (cid:98)P (i)
XY

(cid:104)
(cid:96)(f ( (cid:98)P (i)

(cid:105)
X , Xi), Yi)

(cid:12)
(cid:35)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ E

E

( (cid:98)P (i)

XY )1≤i≤N

(X1,Y1)∼ (cid:98)P (1)

XY ,...,(XN ,YN )∼ (cid:98)P (N )

XY

E(εi)1≤i≤N

(cid:34)

sup
f ∈F

(cid:12)
N
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)

i=1

εi(cid:96)(f ( (cid:98)P (i)

(cid:35)

(cid:12)
(cid:12)
(cid:12)
X , Xi), Yi)
(cid:12)
(cid:12)

.

In the above inequality, the inner expectation on the (Xi, Yi) is pulled outwards by Jensen’s
inequality and convexity of the supremum.

To obtain the announced estimate, notice that the above expectation is the same as the

expectation w.r.t. to the N -fold Campbell measure C(PS) (see Remark 4).

Remark 6 The main term in the theorem is just the conventional Rademacher complexity
for the augmented feature space PX × X endowed with the Campbell measure C(PS). It
could also be thought of as the Rademacher complexity for the meta-distribution PS.

7

3.4 Risk under the 2-stage generative model

While we will state more results holding under (AGM) below, one advantage of the more
speciﬁc (2SGM) is to allow us to study the eﬀect of the sample sizes ni. In particular, for
the purpose of reducing computational complexity, we can analyze the eﬀect of subsampling
observations inside a given sample. Such an analysis would not be possible under (AGM).
For the test sample, parallel to the training data generating mechanism, under (2SGM)
XY , nT ) is drawn according to µ⊗ν, and conditional to this the test sample

we assume that (P T
ST is drawn from nT i.i.d. realizations of P T

Observe that under (2SGM), the distribution P (i)

XY is not observed and is therefore a
latent variable at training time as well as at test time. Still, by the law of large numbers,
if nT becomes large, (cid:98)P T
X (in the sense of weak convergence). This
motivates the introduction of the following risk which assumes access to an inﬁnite test
sample, and thus the true marginal P T
X :
E

X will converge to P T

(cid:2)(cid:96)(f (P T

X , X T ), Y T )(cid:3) .

E ∞(f ) := E

(X T ,Y T )∼P T

(5)

XY .

P T

XY ∼µ

XY

This quantity only makes sense under (2SGM). The following proposition makes the above
motivating observation more precise. First, under (2SGM), introduce the following risk
conditional to a ﬁnite test sample size nT :

E(f |nT ) = E

E

P T

XY ∼µ

(X T

i ,Y T

i )1≤i≤nT

∼(P T

XY )⊗nT

(cid:96)(f ( (cid:98)P T

X , X T

i ), Y T
i )

.

(6)

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:35)

Proposition 7 Assume (cid:96) is a bounded, L-Lipschitz loss function and f : PX × X → R is
a ﬁxed decision function which is continuous with respect to both its arguments (recalling
PX is endowed with the weak convergence topology). Then it holds under (2GSM):

lim
nT →∞

E(f |nT ) = E ∞(f ).

Remark 8 This result provides one setting where the risk E ∞ is clearly motivated as the
goal of asymptotic analysis when nT → ∞. Although Proposition 7 is not used elsewhere in
this work, a more quantitative version of this result is stated below for kernels (see Theorem
15), where convergence holds uniformly and the assumption of a bounded loss is dropped.

To gain more insight into the risk E ∞, recalling the standard decomposition (1) of
PXY into the marginal PX and the posterior PY |X , we observe that we can apply the
disintegration theorem not only to any PXY , but also to µ, and thus decompose it into two
parts, µX which generates the marginal distribution PX , and µY |X which, conditioned on
PX , generates the posterior PY |X . (More precise notation might be µPX instead of µX and
µPY |X |PX instead of µY |X , but this is rather cumbersome.) Denote (cid:101)X = (PX , X). We then
have, using Fubini’s theorem,

E ∞(f ) = EPX ∼µX

EPY |X ∼µY |X

EX∼PX

EY ∼PY |X

= EPX ∼µX

EPY |X ∼µY |X

EY ∼PY |X

EX∼PX
(cid:104)

= E

( (cid:101)X,Y )∼Qµ

(cid:96)(f ( (cid:101)X), Y )

.

(cid:105)

(cid:104)
(cid:96)(f ( (cid:101)X), Y )
(cid:104)
(cid:96)(f ( (cid:101)X), Y )

(cid:105)

(cid:105)

8

Here Qµ is the distribution that generates (cid:101)X by ﬁrst drawing PX according to µX , and then
drawing X according to PX ; similarly, Y is generated, conditioned on (cid:101)X, by ﬁrst drawing
PY |X according to µY |X , and then drawing Y from PY |X . (The distribution of (cid:101)X again
takes the form of a Campbell measure, see Remark 4.)

From the previous expression, we see that the risk E ∞ is like a standard supervised
learning risk based on ( (cid:101)X, Y ) ∼ Qµ. Thus, we can deduce properties that are known
to hold for supervised learning risks. For example, in the binary classiﬁcation setting, if
the loss is the 0/1 loss, then f ∗( (cid:101)X) = 2˜η( (cid:101)X) − 1 is an optimal predictor, where ˜η( (cid:101)X) =
E

(cid:2)1{Y =1}

(cid:3), and

Y ∼Qµ

Y | (cid:101)X

E ∞(f ) − E ∞(f ∗) = E

(cid:101)X∼Qµ
(cid:101)X

(cid:105)
(cid:104)
1{sign(f ( (cid:101)X))(cid:54)=sign(f ∗( (cid:101)X))}|2˜η( (cid:101)X) − 1|

.

Furthermore, consistency in the sense of E ∞ with respect to a general loss (cid:96) (thought of as
a surrogate) will imply consistency for the 0/1 loss, provided (cid:96) is classiﬁcation calibrated
(Bartlett et al., 2006).

X . There is, however, a condition where for µ-almost all test distributions P T

For a given loss (cid:96), the optimal or Bayes E ∞-risk in DG is in general larger than the
expected Bayes risk under the (random) test sample generating distribution P T
XY , because
it is typically not possible to fully determine the Bayes-optimal predictor from only the
marginal P T
XY ,
the decision function f ∗(P T
X , .) (where f ∗ is a global minimizer of (5)) coincides with an
optimal Bayes decision function for P T
XY . This condition is simply that the posterior PY |X
is (µ-almost surely) a function of PX (in other terms: that with the notation introduced
above, µY |X (PX ) is a Dirac measure for µ-almost all PX ). Although we will not be assuming
this condition throughout the paper under (2SGM), observe that it is implicitly assumed
in the motivating application presented in Section 2, where an expert labels the data points
by just looking at their marginal distribution.

Lemma 9 For a ﬁxed distribution PXY , and a decision function g : X → R, let us denote
R(g, PXY ) = E(X,Y )∼PXY [(cid:96)(g(X), Y )] and

R∗(PXY ) := min
g:X →R

R(g, PXY ) = min
g:X →R

E(X,Y )∼PXY [(cid:96)(g(X), Y )]

the corresponding optimal (Bayes) risk for the loss function (cid:96) under data distribution PXY .
Then under (2SGM):

E ∞(f ∗) ≥ EPXY ∼µ [R∗(PXY )] ,

where f ∗ : PX × X → R is a minimizer of the idealized DG risk E ∞ deﬁned in (5).

Furthermore, if µ is a distribution on PX ×Y such that µ-a.s. it holds PY |X = F (PX )

for some deterministic mapping F , then for µ-almost all PXY :

and

R(f ∗(PX , .), PXY ) = R∗(PXY )

E ∞(f ∗) = EPXY ∼µ [R∗(PXY )] .

9

Proof For any f : PX × X → R, one has for all PXY : R(f (PX , .), PXY ) ≥ R∗(PXY ).
Taking expectation with respect to PXY establishes the ﬁrst claim. Now for any ﬁxed
PX ∈ PX , consider PXY := PX • F (PX ) and g∗(PX ) a Bayes decision function for this joint
distribution. Pose f (PX , x) := g∗(PX )(x). Then f coincides for µ-almost all PXY with a
Bayes decision function for PXY , achieving equality in the above inequality. The second
equality follows by taking expectation over PXY ∼ µ.

Under (2SGM), we will establish that our proposed learning algorithm is E ∞-consistent,
provided the average sample size grows to inﬁnity as well as the total number of samples.
Thus, the above result provides a condition on µ under which it is possible to asymptotically
attain the Bayes risk on any test distribution although no labels from this test distribution
are observed.

More generally, and speaking informally, if µ is such that PY |X is close to being a function
of PX in some sense, we can expect the Bayes E ∞ risk for domain generalization to be close
to the expected Bayes risk for a random test distribution. We reiterate, however, that we
make no assumptions on µ in this work so that the two quantities may be far apart. In
the worst case, the posterior may be independent of the marginal, in which case a method
for domain generalization will do no better than the na¨ıve pooling strategy. For further
discussion, see the comparison of domain adaptation and domain generalization in the next
section.

4. Related Work

Since at least the 1990s, machine learning researchers have investigated the possibility of
solving one learning problem by leveraging data from one or more related problems. In this
section, we provide an overview of such problems and their relation to domain generalization,
while also reviewing prior work on DG.

Two critical terms are domain and task. Use of these terms is not consistent throughout
the literature, but at a minimum, the domain of a learning problem describes the input
(feature) space X and marginal distribution of X, while the task describes the output space
Y and the conditional distribution of Y given X (also called posterior). In many settings,
however, the sets X and Y are the same for all learning problems, and the terms “domain”
and “task” are used interchangeably to refer to a joint distribution PXY on X × Y. This is
the perspective adopted in this work, as well as in much of the work on multi-task learning,
domain adaptation (DA), and domain generalization.

Multi-task learning is similar to DG, except only the training tasks are of interest,
and the goal is to leverage the similarity among distributions to improve the learning of
individual predictors for each task (Caruana, 1997; Evgeniou et al., 2005; Yang et al., 2009).
In contrast, in DG, we are concerned with generalization to a new task.

Domain adaptation refers to the setting in which there is a speciﬁc target task and one
or more source tasks. The goal is to design a predictor for the target task, for which there
are typically few to no labeled training examples, by leveraging labeled training data from
the source task(s). DA is reviewed below, and contrasted with DG.

10

4.1 Domain Generalization vs. Domain Adaptation

Formulations of domain adaptation may take several forms, depending on the number of
sources and whether there are any labeled examples from the target to supplement the
unlabeled examples. In multi-source, unsupervised domain adaptation, the learner is pre-
sented with labeled training data from several source distributions, and unlabeled data from
a target marginal distribution (see Zhang et al. (2015) and references therein). Thus, the
available data are the same as in domain generalization, and algorithms for one of these
problems may be applied to the other. In this section we illuminate the diﬀerence between
DA and DG.

In all forms of DA, the goal is to attain optimal performance with respect to the joint
distribution of the target domain. For example, if the performance measure is a risk, the goal
is to attain the Bayes risk for the target domain. To achieve this goal, it is necessary to make
assumptions about how the source and target distributions are related (Quionero-Candela
et al., 2009). For example, several works adopt the covariate shift assumption, which
requires the source and target domains to have the same posterior, allowing the marginals
to diﬀer arbitrarily (Zadrozny, 2004; Huang et al., 2007; Cortes et al., 2008; Sugiyama
et al., 2008; Bickel et al., 2009; Kanamori et al., 2009; Yu and Szepesvari, 2012; Ben-David
and Urner, 2012). Another common assumption is target shift, which stipulates that the
source and target have the same class-conditional distributions, allowing the prior class
probability to change (Hall, 1981; Titterington, 1983; Latinne et al., 2001; Storkey, 2009;
Du Plessis and Sugiyama, 2012; Sanderson and Scott, 2014; Azizzadenesheli et al., 2019).
Mansour et al. (2009b); Zhang et al. (2015) assume that the target posterior is a weighted
combination of source posteriors, while Zhang et al. (2013); Gong et al. (2016) extend target
shift by also allowing the class-conditional distributions to undergo a location-scale shift,
and Tasche (2017) assumes the ratio of class-conditional distributions is unchanged. Work
on classiﬁcation with label noise assumes the source data are obtained from the target
distribution but the labels have been corrupted in either a label-dependent (Blanchard
et al., 2016; Natarajan et al., 2018; van Rooyen and Williamson, 2018) or feature-dependent
(Menon et al., 2018; Cannings et al., 2018; Scott, 2019) way. Finally, there are several works
that assume the existence of a predictor that achieves good performance on both source and
target domains (Ben-David et al., 2007, 2010; Blitzer et al., 2008; Mansour et al., 2009a;
Cortes et al., 2015; Germain et al., 2016).

The key diﬀerence between DG and DA may be found in the performance measures
optimized. In DG, the goal is to design a single predictor f (PX , x) that can apply to any
future task, and risk is assessed with respect to the draw of both a new task, and (under
2SGM) a new data point from that task. This is in contrast to DA, where the target
distribution is typically considered ﬁxed, and the goal is to design a predictor f (x) where, in
assessing the risk, the only randomness is in the draw of a new sample from the target task.
This diﬀerence in performance measures for DG and DA has an interesting consequence
for analysis. As we will show, it is possible to attain optimal risk (asymptotically) in DG
without making any distributional assumptions like those described above for DA. Of course,
this optimal risk is typically larger than the Bayes risk for any particular target domain
(see Lemma 9). An interesting question for future research is whether it is possible to

11

close or eliminate this gap (between DG and expected DA risks) by imposing distributional
assumptions like those for DA.

Another diﬀerence between DA and DG lies in whether the learning algorithm must be
rerun for each new test data set. Most unsupervised DA methods employ the unlabeled
target data for training and thus, when a new unlabeled target data set is presented, the
learning algorithm must be rerun. In contrast, most existing DG methods do not assume
access to the unlabeled test data at learning time, and are capable of making predictions
as new unlabeled data sets arrive without any further training.

4.2 Domain Generalization vs. Learning to Learn

In the problem of learning to learn (LTL, Thrun, 1996), which has also been called bias
learning, meta-learning, and (typically in an online setting) lifelong learning, there are la-
beled datasets for several tasks, as in DG. There is also a given family of learning algorithms,
and the objective is to design a meta-learner that selects the learning algorithm that will
perform best on future tasks. The learning theoretic study of LTL traces to the work of
Baxter (2000), who was the ﬁrst to propose a distribution on tasks, which he calls an “en-
vironment,” and which coincides with our µ. Given this setting, the performance of the
learning algorithm selected by a meta-learner is obtained by drawing a new task at random,
drawing a labeled training dataset from that task, running the selected algorithm, drawing
a test point, and evaluating the expected loss, where the expectation is with respect to all
sources of randomness (new task, training data from new task, test point from new task).

Baxter analyzes learning algorithms given by usual empirical risk minimization over a
hypothesis (prediction function) class, and the goal of the meta-learner is then to select a
hypothesis class from a family of such classes. He shows that it is possible to ﬁnd a good
trade-oﬀ between the complexity of a hypothesis class and its approximation capabilities
for tasks sampled from µ, in an average sense. In particular, the information gained by
ﬁnding a well-adapted hypothesis class can lead to signiﬁcantly improved sample eﬃciency
when learning a new task. See Maurer (2009) for a discussion of the performance measure
studied by Baxter (2000), which is slightly diﬀerent from the one described above.

Later work on LTL establishes similar results that quantify the ability of a meta-learner
to transfer knowledge to a new task. These meta-learners all optimize a particular structure
that deﬁnes a learning algorithm, such as a feature representation (Maurer, 2009; Maurer
et al., 2016; Denevi et al., 2018a), a prior on predictors in a PAC-Bayesian setting (Pentina
and Lampert, 2014), a dictionary (Maurer et al., 2013), the bias of a regularizer (Denevi
et al., 2018b), and a pretrained neural network (Finn et al., 2017). It is also worth not-
ing that some algorithms on multi-task learning extract structures that characterize an
environment and can be applied to LTL.

Although DG and LTL both involve generalization to a new task, they are clearly
diﬀerent problems because LTL assumes access to labeled data from the new task, whereas
DG only sees unlabeled data and requires no additional learning. In LTL, the learner can
achieve the Bayes risk for the new task, the only issue is the sample complexity. DG is
thus a more challenging problem, but also potentially more useful since in many transfer
learning settings, labeled data for the new task are unavailable.

12

4.3 Prior Work on Domain Generalization

To our knowledge, the ﬁrst paper to consider domain generalization (as formulated in Sec-
tion 3.2) was our earlier conference paper (Blanchard et al., 2011). The term “domain
generalization” was coined by Muandet et al. (2013), who study the same setting and build
upon our work by extracting features that facilitate DG. Carbonell et al. (2013) study an
active learning variant of DG in the realizable setting, and directly learn the task sampling
distribution.

Other methods for DG were studied by Khosla et al. (2012); Xu et al. (2014); Grubinger
et al. (2015); Ghifary et al. (2015); Gan et al. (2016); Ghifary et al. (2017); Motiian et al.
(2017); Li et al. (2017, 2018a,b,c,d); Balaji et al. (2018); Ding and Fu (2018); Shankar
et al. (2018); Hu et al. (2019); Dou et al. (2019); Carlucci et al. (2019); Wang et al. (2019);
Akuzawa et al. (2019). Many of these methods learn a common feature space for all tasks.
Such methods are complementary to the method that we study. Indeed, our kernel-based
learning algorithm may be applied after having learned a feature representation by another
method, as was done by Muandet et al. (2013). Since our interest is primarily theoretical,
we restrict our experimental comparison to another algorithm that also operates directly on
the original input space, namely, a simple pooling algorithm that lumps all training tasks
into a single data set and trains a single support vector machine.

5. Learning Algorithm

In this section, we introduce a concrete algorithm to tackle the learning problem exposed in
Section 3, using an approach based on kernels. The function k : Ω×Ω → R is called a kernel
on Ω if the matrix (k(xi, xj))1≤i,j≤n is symmetric and positive semi-deﬁnite for all positive
integers n and all x1, . . . , xn ∈ Ω. It is well known that every kernel k on Ω is associated to
a space of functions f : Ω → R called the reproducing kernel Hilbert space (RKHS) Hk with
kernel k. One way to envision Hk is as follows. Deﬁne Φ(x) := k(·, x), which is called the
canonical feature map associated with k. Then the span of {Φ(x) : x ∈ Ω}, endowed with
the inner product (cid:104)Φ(x), Φ(x(cid:48))(cid:105) = k(x, x(cid:48)), is dense in Hk. We also recall the reproducing
property, which states that (cid:104)f, Φ(x)(cid:105) = f (x) for all f ∈ Hk and x ∈ Ω.

For later use, we introduce the notion of a universal kernel. A kernel k on a compact
metric space Ω is said to be universal when its RKHS is dense in C(Ω), the set of continuous
functions on Ω, with respect to the supremum norm. Universal kernels are important for
establishing universal consistency of many learning algorithms. See Steinwart and Christ-
mann (2008) for background on kernels and reproducing kernel Hilbert spaces.

Several well-known learning algorithms, such as support vector machines and kernel
ridge regression, may be viewed as minimizers of a norm-regularized empirical risk over
the RKHS of a kernel. A similar development has also been made for multi-task learning
Inspired by this framework, we consider a general kernel-based
(Evgeniou et al., 2005).
algorithm as follows.

Consider the loss function (cid:96) : R × Y → R+. Let k be a kernel on PX × X , and let Hk
be the associated RKHS. For the sample Si, recall that (cid:98)P (i)
j=1 δXij denotes the
corresponding empirical X distribution. Also consider the extended input space PX × X
X , Xij). Note that (cid:98)P (i)
and the extended data (cid:101)Xij = ( (cid:98)P (i)
X plays a role analogous to the task

X = 1
ni

(cid:80)ni

13

index in multi-task learning. Now deﬁne

(cid:98)fλ = arg min

f ∈Hk

1
N

N
(cid:88)

ni(cid:88)

1
ni

i=1

j=1

(cid:96)(f ( (cid:101)Xij), Yij) + λ (cid:107)f (cid:107)2 .

Algorithms for solving (7) will be discussed in Section 7.

5.1 Specifying the Kernels

In the rest of the paper we will consider a kernel k on PX × X of the product form

k((P1, x1), (P2, x2)) = kP (P1, P2)kX (x1, x2),

where kP is a kernel on PX and kX a kernel on X .

Furthermore, we will consider kernels on PX of a particular form. Let k(cid:48)

X denote a
kernel on X (which might be diﬀerent from kX ) that is measurable and bounded. We deﬁne
the kernel mean embedding Ψ : PX → Hk(cid:48)

:

PX (cid:55)→ Ψ(PX ) :=

k(cid:48)
X (x, ·)dPX (x).

X

(cid:90)

X

This mapping has been studied in the framework of “characteristic kernels” (Gretton et al.,
2007a), and it has been proved that universality of k(cid:48)
X implies injectivity of Ψ (Gretton
et al., 2007b; Sriperumbudur et al., 2010).

Note that the mapping Ψ is linear. Therefore, if we consider the kernel kP (PX , P (cid:48)

(cid:104)Ψ(PX ), Ψ(P (cid:48)
reason, we introduce yet another kernel K on Hk(cid:48)

X ) =
X )(cid:105), it is a linear kernel on PX and cannot be a universal kernel. For this
and consider the kernel on PX given by

X

kP (PX , P (cid:48)

X ) = K (cid:0)Ψ(PX ), Ψ(P (cid:48)

X )(cid:1) .

Note that particular kernels inspired by the ﬁnite dimensional case are of the form
K(v, v(cid:48)) = F ((cid:13)

(cid:13)v − v(cid:48)(cid:13)
(cid:13)),

or

K(v, v(cid:48)) = G((cid:10)v, v(cid:48)(cid:11)),

where F, G are real functions of a real variable such that they deﬁne a kernel. For exam-
ple, F (t) = exp(−t2/(2σ2)) yields a Gaussian-like kernel, while G(t) = (1 + t)d yields a
polynomial-like kernel. Kernels of the above form on the space of probability distributions
over a compact space X have been introduced and studied in Christmann and Steinwart
(2010). Below we apply their results to deduce that k is a universal kernel for certain choices
of kX , k(cid:48)

X , and K.

5.2 Relation to Other Kernel Methods

By choosing k diﬀerently, one can recover other existing kernel methods.
consider the class of kernels of the same product form as above, but where

In particular,

(7)

(8)

(9)

(10)

(11)

(12)

kP (PX , P (cid:48)

X ) =

(cid:26) 1 PX = P (cid:48)
X
τ PX (cid:54)= P (cid:48)
X

14

If τ = 0, the algorithm (7) corresponds to training N kernel machines f ( (cid:98)P (i)
X , ·) using kernel
kX (e.g., support vector machines in the case of the hinge loss) on each training data set,
independently of the others (note that this does not oﬀer any generalization ability to a new
data set). If τ = 1, we have a “pooling” strategy that, in the case of equal sample sizes ni,
is equivalent to pooling all training data sets together in a single data set, and running a
conventional supervised learning algorithm with kernel kX (i.e., this corresponds to trying
to ﬁnd a single “one-ﬁts-all” prediction function which does not depend on the marginal).
In the intermediate case 0 < τ < 1, the resulting kernel is a “multi-task kernel,” and the
algorithm recovers a multitask learning algorithm like that of Evgeniou et al. (2005). We
compare to the pooling strategy below in our experiments. We also examined the multi-
task kernel with τ < 1, but found that, as far as generalization to a new unlabeled task is
concerned, it was always outperformed by pooling, and so those results are not reported.
This ﬁts the observation that the choice τ = 0 does not provide any generalization to a
new task, while τ = 1 at least oﬀers some form of generalization, if only by ﬁtting the same
predictor to all data sets.

In the special case where all labels Yij are the same value for a given task, and kX is
taken to be the constant kernel, the problem we consider reduces to “distributional” classi-
ﬁcation or regression, which is essentially standard supervised learning where a distribution
(observed through a sample) plays the role of the feature vector. Many of our analysis
techniques specialize to this setting.

6. Learning Theoretic Study

This section presents generalization error and consistency analysis for the proposed kernel
method under the agnostic and 2-stage generative models. Although the regularized esti-
mation formula (7) deﬁning (cid:98)fλ is standard, the generalization error analysis is not, owing
to the particular sampling structures and risks under (AGM) and (2SGM).

6.1 Universal Consistency under the Agnostic Generative Model

We will consider the following assumptions on the loss function and kernels:

(LB) The loss function (cid:96) : R × Y → R+ is L(cid:96)-Lipschitz in its ﬁrst variable and satisﬁes

(K-Bounded) The kernels kX , k(cid:48)

X and K are bounded respectively by constants B2

k, B2

k(cid:48) ≥

B0 := supy∈Y (cid:96)(0, y) < ∞.

1, and B2
K .

The condition B0 < ∞ always holds for classiﬁcation, as well as certain regression
settings. The boundedness assumptions are clearly satisﬁed for Gaussian kernels, and can
be enforced by normalizing the kernel (discussed further below).

We begin with a generalization error bound that establishes uniform estimation error
control over functions belonging to a ball of Hk . We then discuss universal kernels, and
ﬁnally deduce universal consistency of the algorithm.

Let Bk(r) denote the closed ball of radius r, centered at the origin, in the RKHS of the
kernel k. We start with the following simple result allowing us to bound the loss on a RKHS
ball.

15

Lemma 10 Suppose k is a kernel on a set Ω, bounded by B2. Let (cid:96) : R × Y → [0, ∞) be a
loss satisfying (LB). Then for any R > 0 and f ∈ Bk(R), and any z ∈ Ω and y ∈ Y,

(cid:12)(cid:96)(f (z), y)(cid:12)
(cid:12)

(cid:12) ≤ B0 + L(cid:96)RB

(13)

Proof By the Lipschitz continuity of (cid:96), the reproducing property, and Cauchy-Schwarz,
we have

(cid:12)(cid:96)(f (z), y)(cid:12)
(cid:12)

(cid:12)(cid:96)(f (z), y) − (cid:96)(0, y)(cid:12)
(cid:12)

(cid:12) ≤ (cid:96)(0, y) + (cid:12)
≤ B0 + L(cid:96)|f (z) − 0|
(cid:12)(cid:104)f, k(z, ·)(cid:105)(cid:12)
(cid:12)
= B0 + L(cid:96)
(cid:12)
≤ B0 + L(cid:96)(cid:107)f (cid:107)Hk B
≤ B0 + L(cid:96)RB.

The expression in (13) serves to replace the boundedness assumption (3) in Theorem 5.

We now state the following, which is a specialization of Theorem 5 to the kernel setting.

Theorem 11 (Uniform estimation error control over RKHS balls) Assume (LB)
and (K-Bounded) hold, and data generation follows (AGM). Then for any R > 0, with
probability at least 1 − δ (with respect to the draws of the samples Si, i = 1, . . . , N )

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ≤ (B0 + L(cid:96)RBKBk)
(cid:12) (cid:98)E(f, N ) − E(f )

((cid:112)log δ−1 + 2)
√
N

.

sup
f ∈Bk(R)

(14)

Proof This is a direct consequence of Theorem 5 and of Lemma 10, the kernel k on
PX × X being bounded by B2
K. As noted there, the main term in the upper bound (4)
is a standard Rademacher complexity on the augmented input space P × X , endowed with
the Campbell measure C(PS).

kB2

In the kernel learning context, we can bound the Rademacher complexity term using
a standard bound for the Rademacher complexity of a Lipschitz loss function on the ball
of radius R of Hk (Koltchinskii, 2001; Bartlett and Mendelson, 2002, e.g., Theorems 8, 12
and Lemma 22 there), using again the bound B2

K on the kernel k, giving the conclusion.

kB2

Next, we turn our attention to universal kernels (see Section 5 for the deﬁnition). A
relevant notion for our purposes is that of a normalized kernel. If k is a kernel on Ω, then

k∗(x, x(cid:48)) :=

k(x, x(cid:48))
(cid:112)k(x, x)k(x(cid:48), x(cid:48))

is the associated normalized kernel. If a kernel is universal, then so is its associated normal-
ized kernel. For example, the exponential kernel k(x, x(cid:48)) = exp(κ(cid:104)x, x(cid:48)(cid:105)Rd), κ > 0, can be
shown to be universal on Rd through a Taylor series argument. Consequently, the Gaussian
kernel

kσ(x, x(cid:48)) :=

exp( 1

σ2 (cid:104)x, x(cid:48)(cid:105))
2σ2 (cid:107)x(cid:107)2) exp( 1

exp( 1

2σ2 (cid:107)x(cid:48)(cid:107)2)

16

is universal, being the normalized kernel associated with the exponential kernel with κ =
1/σ2. See Steinwart and Christmann (2008) for additional details and discussion.
To establish that k is universal on PX × X , the following lemma is useful.

Lemma 12 Let Ω, Ω(cid:48) be two compact spaces and k, k(cid:48) be kernels on Ω, Ω(cid:48), respectively. If
k, k(cid:48) are both universal, then the product kernel

k((x, x(cid:48)), (y, y(cid:48))) := k(x, y)k(cid:48)(x(cid:48), y(cid:48))

is universal on Ω × Ω(cid:48).

Several examples of universal kernels are known on Euclidean space. For our purposes,
we also need universal kernels on PX . Fortunately, this was studied by Christmann and
Steinwart (2010). Some additional assumptions on the kernels and feature space are re-
quired:

(K-Univ) kX , k(cid:48)

X , K, and X satisfy the following:

• X is a compact metric space

• kX is universal on X
• k(cid:48)

X is continuous and universal on X

• K is universal on any compact subset of Hk(cid:48)

.

X

Adapting the results of Christmann and Steinwart (2010), we have the following.

Theorem 13 (Universal kernel) Assume condition (K-Univ) holds. Then, for kP de-
ﬁned as in (10), the product kernel k in (8) is universal on PX × X .

Furthermore, the assumption on K is fulﬁlled if K is of the form (12), where G is an
analytical function with positive Taylor series coeﬃcients, or if K is the normalized kernel
associated to such a kernel.

Proof By Lemma 12, it suﬃces to show PX is a compact metric space, and that kP (PX , P (cid:48)
X )
is universal on PX . The former statement follows from Theorem 6.4 of Parthasarathy
(1967), where the metric is the Prohorov metric. We will deduce the latter statement from
Theorem 2.2 of Christmann and Steinwart (2010). The statement of Theorem 2.2 there
is apparently restricted to kernels of the form (12), but the proof actually only uses that
the kernel K is universal on any compact set of Hk(cid:48)
. To apply Theorem 2.2, it remains
is a separable Hilbert space, and that Ψ is injective and continuous.
to show that Hk(cid:48)
Injectivity of Ψ is equivalent to k(cid:48)
X being a characteristic kernel, and follows from the
assumed universality of k(cid:48)
X (Sriperumbudur et al., 2010). The continuity of k(cid:48)
X implies
(Steinwart and Christmann (2008), Lemma 4.33) as well as continuity of
separability of Hk(cid:48)
Ψ (Christmann and Steinwart (2010), Lemma 2.3 and preceding discussion). Now Theorem
2.2 of Christmann and Steinwart (2010) may be applied, and the results follows.

X

X

X

The fact that kernels of the form (12), where G is analytic with positive Taylor coeﬃ-
was established in the proof of Theorem

cients, are universal on any compact set of Hk(cid:48)
2.2 of the same work (Christmann and Steinwart, 2010).

X

17

As an example, suppose that X is a compact subset of Rd. Let kX and k(cid:48)

X be Gaussian
kernels on X . Taking G(t) = exp(t), it follows that K(PX , P (cid:48)
)
is universal on PX . By similar reasoning as in the ﬁnite dimensional case, the Gaussian-like
kernel K(PX , P (cid:48)
) is also universal on PX . Thus the
product kernel is universal on PX × X .

X ) = exp((cid:104)Ψ(PX ), Ψ(P (cid:48)

2σ2 (cid:107)Ψ(PX ) − Ψ(P (cid:48)

X ) = exp(− 1

X )(cid:105)Hk(cid:48)

X )(cid:107)2

Hk(cid:48)
X

X

From Theorems 11 and 13, we may deduce universal consistency of the learning algo-

rithm.

Corollary 14 (Universal consistency) Assume that conditions (LB), (K-Bounded)
and (K-Univ) are satisﬁed. Let λ = λ(N ) be a sequence such that as N → ∞: λ(N ) → 0
and λ(N )N/ log N → ∞. Then

E( (cid:98)fλ(N )) → inf

f :PX ×X →R

E(f ) a.s., as N → ∞.

The proof of the corollary relies on the bound established in Theorem 11, the universality
of k established in Theorem 13, and otherwise relatively standard arguments.

One notable feature of this result is that we have established consistency where only N
is required to diverge. In particular, the training sample sizes ni may remain bounded. In
the next subsection, we consider the role of the ni under the 2-stage generative model.

6.2 Role of the Individual Sample Sizes under the 2-Stage Generative Model

In this section, we are concerned with the role of the individual sample sizes (ni)1≤i≤N .
More precisely, in some applications the number of training points per task is large, which
can give rise to a high computational burden at the learning stage (and also for storing the
learned model in computer memory). We investigate to which extent reducing the number of
training points per task (by random subsampling) in order to reduce computational burden
can be done without suﬀering a signiﬁcant loss in statistical performance. For this we need
a more speciﬁc model for the generating model of points in each task, and we therefore
assume here that the (2SGM), introduced in Section 3.1, holds.

We will consider the following additional assumption.

(K-H¨older) The canonical feature map ΦK : Hk(cid:48)

→ HK associated to K satisﬁes a H¨older

X

condition of order α ∈ (0, 1] with constant LK, on Bk(cid:48)

(Bk(cid:48)) :

X

∀v, w ∈ Bk(cid:48)

(Bk(cid:48)) :

X

(cid:107)ΦK(v) − ΦK(w)(cid:107) ≤ LK (cid:107)v − w(cid:107)α .

(15)

Suﬃcient conditions for (15) are described in Section A.4. As an example, the condition is
shown to hold with α = 1 when K is the Gaussian-like kernel on Hk(cid:48)

.

Since we are interested in the inﬂuence of the number of training points per task, it
is helpful to introduce notations for the (2SGM) risks that are conditioned on a ﬁxed
task PXY . Thus, we introduce the following notation, in analogy to (5)–(6) introduced in

X

18

Section 3.4, for risk at sample size n, and risk at inﬁnite sample size, conditional to PXY :

E(f |PXY , n) := E

ST ∼(PXY )⊗n

(cid:96)(f ( (cid:98)PX , Xi), Yi)

;

(cid:34)

1
n

n
(cid:88)

i=1

(cid:35)

E ∞(f |PXY ) := E(X,Y )∼PXY [(cid:96)(f (PX , X), Y )] .

(16)

(17)

The following proposition gives an upper bound on the discrepancy between these risks.
It can be seen as a quantitative version of Proposition 7 in the kernel setting, which is
furthermore uniform over an RKHS ball.

If the
Theorem 15 Assume conditions (LB), (K-Bounded), and (K-H¨older) hold.
sample S = (Xj, Yj)1≤j≤n is made of n i.i.d. realizations from PXY , with PXY and n ﬁxed,
then for any R > 0, with probability at least 1 − δ:

sup
f ∈Bk(R)

|L(S, f ) − E ∞(f |PXY )| ≤ (B0 + 3L(cid:96)RBk(Bα

k(cid:48)LK + BK))

(cid:19)− α

2

(cid:18) log(3δ−1)
n

.

(18)

Averaging over the draw of S, again with PXY and n ﬁxed, it holds for any R > 0:

sup
f ∈Bk(R)

|E(f |PXY , n) − E ∞(f |PXY )| ≤ 2L(cid:96)RBkLKBα

k(cid:48)n−α/2.

(19)

As a consequence, for the unconditional risks when (PXY , n) is drawn from µ ⊗ ν under
(2GSM), for any R > 0:

|E(f ) − E ∞(f )| ≤ 2RL(cid:96)BkLKBα

k(cid:48)E

(cid:104)
n− α

2

(cid:105)

.

(20)

sup
f ∈Bk(R)

The above results are useful in a number of ways. First, under (2SGM), we can consider
the goal of asymptotically achieving the optimal risk inf f E ∞(f ), where we recall that E ∞(f )
is the expected loss of a decision function f over a random test task P T
XY in the case where
P T
X would be perfectly observed (this can be thought of as observing as an inﬁnite sample
from the marginal). Equation (20) bounds the risk under (2SGM) in terms of the risk
under (AGM), for which we have already established consistency. Thus, consistency under
(2SGM) will be possible if the number of examples ni per training task also grows together
with the number of training tasks N . The following result formalizes this intuition.

XY , . . . , P (N )

Corollary 16 Assume (LB), (K-Bounded), and (K-H¨older) hold, and let n be ﬁxed.
If P (1)
from µ, and the corresponding samples S1, . . . , SN are
i.i.d. all of size n from their respective distributions, then for any R > 0, with probability
at least 1 − δ with respect to the draws of the training tasks and training samples

XY are drawn i.i.d.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
N

N
(cid:88)

i=1

(cid:12)
(cid:12)
L(Si, f ) − E ∞(f )
(cid:12)
(cid:12)
(cid:12)

sup
f ∈Bk(R)

≤ (B0 + L(cid:96)RBKBk)

+ 2RL(cid:96)BkLKBα

k(cid:48)n− α
2 .

(21)

((cid:112)log δ−1 + 2)
√
N

19

Furthermore, if assumption (K-Univ) is satisﬁed and, as the number of training tasks
N → ∞, the sample size n = n(N ) → ∞, and the regularization parameter λ(N ) is such
that λ(N ) → 0 and λ(N ) min(N, nα) → ∞, then

E( (cid:98)fλ(N )) → inf

f :PX ×X →R

E ∞(f ) in probability, as N → ∞.

Proof The setting is that of the (2GSM) model, where the sample size is ﬁxed at n (i.e.
the sample size distribution ν is the Dirac δn). This is a particular case of (AGM), so we
can apply Theorem 11 and combine with (20) (wherein the test sample is also of size n) to
get the announced bound. The consistency statement follows the same argument as in the
proof of Corollary 14, with E(f ) replaced by E ∞(f ), and ε(N ) there replaced by the RHS
in (21). We consider only consistency in probability this time, since a.s. consistency does
not appear of relevance in a setting where the individual training task sample size grows
with the number of tasks.

Remark 17 Our conference paper (Blanchard et al., 2011) also established a generalization
error bound and consistency for E ∞ under (2SGM). That bound had a diﬀerent form for
two main reasons. First, it assumed the loss to be bounded, whereas the present analysis
avoids that assumption via Lemma 10. Second, that analysis did not leverage a connection
to (AGM), which led to a log N in the second term. This required the two sample sizes to
be coupled asymptotically to achieve consistency. In the present analysis, the two sample
sizes N and n may diverge at arbitrary rates.

Remark 18 It is possible to obtain a result similar to Corollary 16 when the training task
sample sizes (ni)1≤i≤N are unequal and possibly arbitrary. In this case we would follow a
slightly diﬀerent argument, using (18) for all training tasks together with a union bound,
and applying Theorem 11 to the idealized situation with an inﬁnite number of samples per
training task. This way, the order n− α
. We eschew
an exact statement for brevity.

2 is replaced by log(N )N −1 (cid:80)N

i=1 n

− α
2
i

Coming back to our initial motivation of possibly reducing computational burden by
subsampling, using (21) we can compare under (2SGM) the two settings where we have
the same task generating distribution µ but diﬀerent individual training task sample sizes
n versus n(cid:48) < n. Under the (2SGM) model, the setting n(cid:48) < n can be obtained by simple
random subsampling of the original data. We see that the statistical risk bound in (21) is
unchanged up to a small factor if n(cid:48) ≥ min(N α, n). Assuming α = 1 to simplify, in the case
where the original sample sizes n are much larger than the number of training tasks N , this
suggests that we can subsample to n(cid:48) ≈ N without taking a signiﬁcant hit to generalization
performance. This applies equally well to subsampling the tasks used for prediction or
testing. The most precise statement in this regard is (18), since it bounds the deviations of
the observed prediction loss for a ﬁxed task PXY and i.i.d. sample from that task.

The minimal subsampling size n(cid:48) can be interpreted as an optimal eﬃciency/accuracy
tradeoﬀ, since it reduces computational complexity as much as possible without sacriﬁcing
statistical accuracy. Similar considerations appear in the context of distribution regres-
sion (Szab´o et al., 2016, Remark 6). In that reference, a sharp analysis giving rise to fast

20

convergence rates is presented, resulting in a more involved optimal balance between N and
n. In the present work, we have focused on slow rates based on a uniform control of the
estimation error over RKHS balls; we leave for future work sharper convergence bounds (un-
der additional regularity conditions), which would also give rise to more reﬁned balancing
conditions between n and N .

7. Implementation

Implementation3 of the algorithm in (7) relies on techniques that are similar to those used
for other kernel methods, but with some variations. The ﬁrst subsection illustrates how,
for the case of hinge loss, the optimization problem corresponds to a certain cost-sensitive
support vector machine. Subsequent subsections focus on more scalable implementations
based on approximate feature mappings.

7.1 Representer Theorem and Hinge loss

For a particular loss (cid:96), existing algorithms for optimizing an empirical risk based on that
loss can be adapted to the setting of marginal transfer learning. We now illustrate this
idea for the case of the hinge loss, (cid:96)(t, y) = max(0, 1 − yt). To make the presentation more
concise, we will employ the extended feature representation (cid:101)Xij = ( (cid:98)P (i)
X , Xij), and we will
also “vectorize” the indices (i, j) so as to employ a single index on these variables and on
the labels. Thus the training data are ( (cid:101)Xi, Yi)1≤i≤M , where M = (cid:80)N
i=1 ni, and we seek a
solution to

min
f ∈Hk

M
(cid:88)

i=1

ci max(0, 1 − Yif ( (cid:101)Xi)) +

1
2

(cid:107)f (cid:107)2 .

Here ci = 1
, where m is the smallest positive integer such that i ≤ n1 + · · · + nm. By
the representer theorem (Steinwart and Christmann, 2008), the solution of (7) has the form

λN nm

for real numbers ri. Plugging this expression into the objective function of (7), and intro-
ducing the auxiliary variables ξi, we have the quadratic program

(cid:98)fλ =

rik( (cid:101)Xi, ·)

M
(cid:88)

i=1

min
r,ξ

1
2

rT Kr +

ciξi

M
(cid:88)

i=1

M
(cid:88)

j=1
ξi ≥ 0, ∀i,

s.t. Yi

rjk( (cid:101)Xi, (cid:101)Xj) ≥ 1 − ξi, ∀i

21

3. Software available at https://github.com/aniketde/DomainGeneralizationMarginal

where K := (k( (cid:101)Xi, (cid:101)Xj))1≤i,j≤M . Using Lagrange multiplier theory, the dual quadratic
program is

max
α

−

1
2

M
(cid:88)

i,j=1

s.t. 0 ≤ αi ≤ ci ∀i,

αiαjYiYjk( (cid:101)Xi, (cid:101)Xj) +

αi

M
(cid:88)

i=1

and the optimal function is

(cid:98)fλ =

αiYik( (cid:101)Xi, ·).

M
(cid:88)

i=1

This is equivalent to the dual of a cost-sensitive support vector machine, without oﬀset,
where the costs are given by ci. Therefore we can learn the weights αi using any existing
software package for SVMs that accepts example-dependent costs and a user-speciﬁed kernel
matrix, and allows for no oﬀset. Returning to the original notation, the ﬁnal predictor given
a test X-sample ST has the form

(cid:98)fλ( (cid:98)P T

X , x) =

αijYijk(( (cid:98)P (i)

X , Xij), ( (cid:98)P T

X , x))

N
(cid:88)

ni(cid:88)

i=1

j=1

where the αij are nonnegative. Like the SVM, the solution is often sparse, meaning most
αij are zero.

Finally, we remark on the computation of kP ( (cid:98)PX , (cid:98)P (cid:48)

(12), the calculation of kP may be reduced to computations of the form
If (cid:98)PX and (cid:98)P (cid:48)
then

X are empirical distributions based on the samples X1, . . . , Xn and X (cid:48)

X ). When K has the form of (11) or
(cid:69)
(cid:68)
Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)
.
X )
1, . . . , X (cid:48)

n(cid:48),

(cid:68)
Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

X )

(cid:69)

=

k(cid:48)
X (Xi, ·),

X (X (cid:48)
k(cid:48)

j, ·)

(cid:43)

(cid:42)

1
n

n
(cid:88)

i=1

1
n(cid:48)

n(cid:48)
(cid:88)

j=1

=

1
nn(cid:48)

n
(cid:88)

n(cid:48)
(cid:88)

i=1

j=1

X (Xi, X (cid:48)
k(cid:48)

j).

Note that when k(cid:48)
a smoothing kernel density estimate for PX .

X is a (normalized) Gaussian kernel, Ψ( (cid:98)PX ) coincides (as a function) with

7.2 Approximate Feature Mapping for Scalable Implementation

Assuming ni = n, for all i, the computational complexity of a nonlinear SVM solver (in our
context) is between O(N 2n2) and O(N 3n3) (Joachims, 1999; Chang and Lin, 2011). Thus,
standard nonlinear SVM solvers may be insuﬃcient when N or n are very large.

One approach to scaling up kernel methods is to employ approximate feature mappings
together with linear solvers. This is based on the idea that kernel methods are solving for
a linear predictor after ﬁrst nonlinearly transforming the data. Since this nonlinear trans-
formation can have an extremely high- or even inﬁnite-dimensional output, classical kernel

22

methods avoid computing it explicitly. However, if the feature mapping can be approxi-
mated by a ﬁnite dimensional transformation with a relatively low-dimensional output, one
can directly solve for the linear predictor, which can be accomplished in O(N n) time (Hsieh
et al., 2008).

In particular, given a kernel k, the goal is to ﬁnd an approximate feature mapping z(˜x)
such that k(˜x, ˜x(cid:48)) ≈ z(˜x)T z(˜x(cid:48)). Given such a mapping z, one then applies an eﬃcient linear
solver, such as Liblinear (Fan et al., 2008), to the training data (z( ˜Xij), Yij)ij to obtain a
weight vector w. The ﬁnal prediction on a test point ˜x is then wT z(˜x). As described in
the previous subsection, the linear solver may need to be tweaked, as in the case of unequal
sample sizes ni, but this is usually straightforward.

Recently, such low-dimensional approximate future mappings z(x) have been developed
for several kernels. We examine two such techniques in the context of marginal transfer
learning, the Nystr¨om approximation (Williams and Seeger, 2001; Drineas and Mahoney,
2005) and random Fourier features. The Nystr¨om approximation applies to any kernel
method, and therefore extends to the marginal transfer setting without additional work.
On the other hand, we give a novel extension of random Fourier features to the marginal
transfer learning setting (for the case of all Gaussian kernels), together with performance
analysis. Our approach is similar to the one in Jitkrittum et al. (2015) which proposes a
two-stage approximation for the mean embedding. Note that Jitkrittum et al. (2015) does
not give an error bound.

7.2.1 Random Fourier Features

The approximation of Rahimi and Recht (2007) is based on Bochner’s theorem, which
characterizes shift invariant kernels.

Theorem 19 A continuous kernel k(x, y) = k(x − y) on Rd is positive deﬁnite iﬀ k(x − y)
is the Fourier transform of a ﬁnite positive measure p(w), i.e.,

k(x − y) =

p(w)ejwT (x−y)dw .

(22)

If a shift invariant kernel k(x − y) is properly scaled then Theorem 19 guarantees that

p(w) in (22) is a proper probability distribution.

(cid:90)

Rd

23

Random Fourier features (RFFs) approximate the integral in (22) using samples drawn

from p(w). If w1, w2, ..., wL are i.i.d. draws from p(w),

k(x − y) =

p(w)ejwT (x−y)dw

p(w) cos(wT x − wT y)dw

cos(wT

i x − wT

i y)

(cid:90)

(cid:90)

Rd

Rd

L
(cid:88)

i=1
L
(cid:88)

i=1
L
(cid:88)

1
L

1
L

1
L

=

≈

=

=

cos(wT

i x) cos(wT

i y) + sin(wT

i x) sin(wT

i y)

[cos(wT

i x), sin(wT

i x)]T [cos(wT

i y), sin(wT

i y)]

i=1
= zw(x)T zw(y) ,

(23)

[cos(wT

1 x), sin(wT

L x), sin(wT

L x)] ∈ R2L is an approximate
where zw(x) = 1√
1 x), ..., cos(wT
L
In the following, we extend the RFF
nonlinear feature mapping of dimensionality 2L.
methodology to the kernel ¯k on the extended feature space PX × X . Let X1, . . . , Xn1
and X (cid:48)
X respectively, and let (cid:98)PX and (cid:98)P (cid:48)
X
denote the corresponding empirical distributions. Given x, x(cid:48) ∈ X , denote ˜x = ( (cid:98)PX , x)
and ˜x(cid:48) = ( (cid:98)P (cid:48)
X , x(cid:48)). The goal is to ﬁnd an approximate feature mapping ¯z(˜x) such that
¯k(˜x, ˜x(cid:48)) ≈ ¯z(˜x)T ¯z(˜x(cid:48)). Recall that

realizations of PX and P (cid:48)

n2 be i.i.d.

1, . . . , X (cid:48)

¯k(˜x, ˜x(cid:48)) = kP ( (cid:98)PX , (cid:98)P (cid:48)

X )kX (x, x(cid:48));

speciﬁcally, we consider kX and k(cid:48)
kP to have the Gaussian-like form

X to be Gaussian kernels and the kernel on distributions

kP ( (cid:98)PX , (cid:98)P (cid:48)

X ) = exp

(cid:107)Ψ( (cid:98)PX ) − Ψ( (cid:98)P (cid:48)

X )(cid:107)2

(cid:26) 1
2σ2
P

(cid:27)

.

Hk(cid:48)
X

As noted earlier in this section, the calculation of kP ( (cid:98)PX , (cid:98)P (cid:48)
of

X ) reduces to the computation

(cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

X )(cid:105) =

X (Xi, X (cid:48)
k(cid:48)

j).

(24)

1
n1n2

n1(cid:88)

n2(cid:88)

i=1

j=1

24

We use Theorem 19 to approximate k(cid:48)

X and thus (cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

X )(cid:105). Let w1, w2, ..., wL be

i.i.d. draws from p(cid:48)(w), the inverse Fourier transform of k(cid:48)

X . Then we have:

(cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

X )(cid:105) =

X (Xi, X (cid:48)
k(cid:48)
j)

1
n1n2

n1(cid:88)

n2(cid:88)

i=1

j=1

≈

=

=

1
Ln1n2

1
Ln1n2

1
Ln1n2

L
(cid:88)

n1(cid:88)

n2(cid:88)

l=1

i=1

j=1

L
(cid:88)

n1(cid:88)

n2(cid:88)

l=1

i=1

j=1

L
(cid:88)
{

n1(cid:88)

l=1

i=1
= ZP ( (cid:98)PX )T ZP ( (cid:98)P (cid:48)

X ),

cos(wT

l Xi − wT

l X (cid:48)
j)

[cos(wT

l Xi) cos(wT

l X (cid:48)

j) + sin(wT

l Xi) sin(wT

l X (cid:48)

j)]

[cos(wT

l Xi), sin(wT

l Xi)]T

[cos(wT

l X (cid:48)

j), sin(wT

l X (cid:48)

j)]}

n2(cid:88)

j=1

where

ZP ( (cid:98)PX ) =

cos(wT

1 Xi), sin(wT

1 Xi), ..., cos(wT

L Xi), sin(wT

L Xi)

(25)

(cid:105)
,

n1(cid:88)

(cid:104)

1
√

n1

L

i=1

and ZP ( (cid:98)P (cid:48)
let z(cid:48)
1
n1

X ) is deﬁned analogously with n1 replaced by n2. For the proof of Theorem 20,
X , which satisﬁes ZP ( (cid:98)PX ) =

X denote the approximate feature map corresponding to k(cid:48)
(cid:80)n1
i=1 z(cid:48)

X (Xi).

Note that the lengths of the vectors ZP ( (cid:98)PX ) and ZP ( (cid:98)P (cid:48)

X ) are 2L. To approximate ¯k we

may write

¯k(˜x, ˜x(cid:48)) ≈ exp

−(cid:107)x − x(cid:48)(cid:107)2
Rd
2σ2
X
P (cid:107)x − x(cid:48)(cid:107)2

Rd)

R2L + σ2

· exp

−(cid:107)ZP ( (cid:98)PX ) − ZP ( (cid:98)P (cid:48)

X )(cid:107)2

R2L

−(σ2

2σ2
P
X (cid:107)ZP ( (cid:98)PX ) − ZP ( (cid:98)P (cid:48)
2σ2

X )(cid:107)2
P σ2
X
X )(cid:107)2
−((cid:107)σX ZP ( (cid:98)PX ) − σX ZP ( (cid:98)P (cid:48)
P σ2
2σ2
X

R2L + (cid:107)σP x − σP x(cid:48)(cid:107)2

Rd)

−(cid:107)(σX ZP ( (cid:98)PX ), σP x) − (σX ZP ( (cid:98)P (cid:48)

X ), σP x(cid:48))(cid:107)2

R2L+d

.

2σ2

P σ2
X

= exp

= exp

= exp

(26)

This is also a Gaussian kernel, now on R2L+d. Again by applying Theorem 19, we have

¯k( (cid:98)PX , X), ( (cid:98)P (cid:48)

X , X (cid:48))) ≈

p(v)ejvT ((σX ZP (PX ),σP X)−(σX ZP (P (cid:48)

X ),σP X (cid:48)))dv.

(cid:90)

R2L+d

Let v1, v2, ..., vq be drawn i.i.d.
kernel with bandwidth σP σX . Let u = (σX ZP ( (cid:98)PX ), σP x) and u(cid:48) = (σX ZP ( (cid:98)P (cid:48)

from p(v), the inverse Fourier transform of the Gaussian
X ), σP x(cid:48)).

25

Then

where

¯k(˜x, ˜x(cid:48)) ≈

cos(vT

q (u − u(cid:48)))

Q
(cid:88)

1
Q

q=1
= ¯z(˜x)T ¯z(˜x(cid:48)),

¯z(˜x) =

[cos(vT

1 u), sin(vT

1 u), ..., cos(vT

Qu), sin(vT

Qu)] ∈ R2Q

(27)

1
√
Q

and ¯z(˜x(cid:48)) is deﬁned similarly.

This completes the construction of the approximate feature map. The following result,
which uses Hoeﬀding’s inequality and generalizes a result of Rahimi and Recht (2007), says
that the approximation achieves any desired approximation error with very high probability
as L, Q → ∞.

Theorem 20 Let L be the number of random features to approximate the kernel on distri-
butions and Q be the number of features to approximate the ﬁnal product kernel. For any
(cid:15)l > 0, (cid:15)q > 0, ˜x = ( (cid:98)PX , x), ˜x(cid:48) = ( (cid:98)P (cid:48)

X , x(cid:48)),

P (|¯k(˜x, ˜x(cid:48)) − ¯z(˜x)T ¯z(˜x(cid:48))| ≥ (cid:15)l + (cid:15)q) ≤ 2 exp

(cid:16)

−

(cid:17)

Q(cid:15)2
q
2

+ 6n1n2 exp

−

(28)

(cid:16)

(cid:17)

,

L(cid:15)2
2

where (cid:15) = σ2
n1 and n2 are the sizes of the empirical distributions (cid:98)PX and (cid:98)P (cid:48)

2 log(1 + (cid:15)l), σP is the bandwidth parameter of the Gaussian-like kernel kP , and

X , respectively.

P

The above results holds for ﬁxed ˜x and ˜x(cid:48). Following again Rahimi and Recht (2007),
one can use an (cid:15)-net argument to prove a stronger statement for every pair of points in the
input space simultaneously. They show

Lemma 21 Let M be a compact subset of Rd with diameter r = diam(M) and let D be the
number of random Fourier features used. Then for the mapping deﬁned in (23), we have

(cid:16)

P

sup
x,y∈M

(cid:17)
|zw(x)T zw(y) − k(x − y)| ≥ (cid:15)

(cid:17)2

≤ 28(cid:16) σr
(cid:15)

exp

(cid:16) −D(cid:15)2
2(d + 2)

(cid:17)

,

where σ = E[wT w] is the second moment of the Fourier transform of k.

Our RFF approximation of ¯k is grounded on Gaussian RFF approximations on Euclidean
spaces, and thus, the following result holds by invoking Lemma 21, and otherwise following
the argument of Theorem 20.

Theorem 22 Using the same notations as in Theorem 20 and Lemma 21,

(cid:16)

P

sup
x,x(cid:48)∈M

|¯k(˜x, ˜x(cid:48)) − ¯z(˜x)T ¯z(˜x(cid:48))| ≥ (cid:15)l + (cid:15)q

(cid:17)

(cid:17)2

≤ 28(cid:16) σ(cid:48)
X r
(cid:15)q

exp

(cid:17)

(cid:16) −Q(cid:15)2
q
2(d + 2)

+ 293n1n2

(cid:16) σP σX r
(cid:15)l
X in Eqn. (24) and σP and σX are the widths of kernels

(cid:16) −L(cid:15)2
l

2(d + 2)

(29)

exp

(cid:17)2

(cid:17)

where σ(cid:48)
kP and kX respectively.

X is the width of kernel k(cid:48)

26

There are recent developments that give faster rates for approximation quality of random
Fourier features and could potentially be combined with our analysis (Sriperumbudur and
Szab´o, 2015; Sutherland and Schneider, 2015). For example, approximation quality for the
kernel mean map is discussed in Sutherland and Schneider (2015), and these ideas could be
extended to Theorem 22 by combining with the two-stage approach presented in this paper.
We also note that our analysis of random Fourier features is separate from our analysis of
the kernel learning algorithm. We have not presented a generalization error bound for the
learning algorithm using random Fourier features (Rudi and Rosasco, 2017).

7.2.2 Nystr¨om Approximation

Like random Fourier features, the Nystr¨om approximation is a technique to approximate
kernel matrices (Williams and Seeger, 2001; Drineas and Mahoney, 2005). Unlike random
Fourier features, for the Nystr¨om approximation, the feature maps are data-dependent.
Also, in the last subsection, all kernels were assumed to be shift invariant. With the
Nystr¨om approximation there is no such assumption.

For a general kernel k, the goal is to ﬁnd a feature mapping z : Rd → RL, where L > d,
such that k(x, x(cid:48)) ≈ z(x)T z(x(cid:48)). Let r be the target rank of the ﬁnal approximated kernel
matrix, and m be the number of selected columns of the original kernel matrix. In general
r ≤ m (cid:28) n.

Given data points x1, . . . , xn, the Nystr¨om method approximates the kernel matrix by
m without replacement from the original sample,
j)]n×m,

ﬁrst sampling m data points x(cid:48)
and then constructing a low rank matrix by (cid:98)Kr = Kb (cid:98)K−1KT
and (cid:98)K = [k(x(cid:48)

j)]m×m. Hence, the ﬁnal approximate feature mapping is

b , where Kb = [k(xi, x(cid:48)

2, ..., x(cid:48)

1, x(cid:48)

i, x(cid:48)

zn(x) = (cid:98)D− 1

2 (cid:98)V T [k(x, x(cid:48)

1), ..., k(x, x(cid:48)

m)],

(30)

where (cid:98)D is the eigenvalue matrix of (cid:98)K and (cid:98)V is the corresponding eigenvector matrix.

The Nystr¨om approximation holds for any positive deﬁnite kernel, but random Fourier
features can be used only for shift invariant kernels. On the other hand, random Fourier
features are very easy to implement and the Nystr¨om method has additional time complexity
due to an eigenvalue decomposition. Moreover, the Nystr¨om method is useful only when
the kernel matrix has low rank. For additional comparison of various kernel approximation
approaches we refer the reader to Le et al. (2013).
In our experiments, we use random
Fourier features when all kernels are Gaussian and the Nystr¨om method otherwise.

8. Experiments

This section empirically compares our marginal transfer learning method with pooling.4
One implementation of the pooling algorithm was mentioned in Section 5.2, where kP is
taken to be a constant kernel. Another implementation is to put all the training data sets
together and train a single conventional kernel method. The only diﬀerence between the two
implementations is that in the former, weights of 1/ni are used for examples from training
task i. In almost all of our experiments below, the various training tasks have the same

4. Software available at https://github.com/aniketde/DomainGeneralizationMarginal

27

sample sizes, in which case the two implementations coincide. The only exception is the
fourth experiment when we use all training data, in which case we use the second of the
two implementations mentioned above.

We consider three classiﬁcation problems (Y = {−1, 1}), for which the hinge loss is
employed, and one regression problem (Y ⊂ R), where the (cid:15)-insensitive loss is employed.
Thus, the algorithms implemented are natural extensions of support vector classiﬁcation
and regression to domain generalization. Performance of a learning strategy is assessed by
holding out several data sets ST
, learning a decision function (cid:98)f on the remaining
data sets, and reporting the average empirical risk 1
i , (cid:98)f ). In some cases, this
NT
value is again averaged over several randomized versions of the experiment.

1 , . . . , ST
NT

i=1 L(ST

(cid:80)NT

8.1 Model Selection

T x2 and Gaussian kernels kσ(x1, x2) = exp (cid:0) − ||x1−x2||2

The various experiments use diﬀerent combinations of kernels. In all experiments, linear
(cid:1) were used.
kernels k(x1, x2) = x1
The bandwidth σ of each Gaussian kernel and the regularization parameter λ of the
machines were selected by grid search. For model selection, ﬁve-fold cross-validation was
used. In order to stabilize the cross-validation procedure, it was repeated 5 times over in-
dependent random splits into folds (Kohavi et al., 1995). Thus, candidate parameter values
were evaluated on the 5 × 5 validation sets and the conﬁguration yielding the best average
performance was selected. If any of the chosen hyper-parameters was at the grid boundary,
the grid was extended accordingly, i.e., the same grid size has been used, however, the center
of grid has been assigned to the previously selected point. The grid used for kernels was
σ ∈ (cid:0)10−2, 104(cid:1) with logarithmic spacing, and the grid used for the regularization parameter
was λ ∈ (cid:0)10−1, 101(cid:1) with logarithmic spacing.

2σ2

8.2 Synthetic Data Experiment

To illustrate the proposed method, a synthetic problem was constructed. The synthetic
data generation algorithm is given in Algorithm 1. In brief, for each classiﬁcation task, the
data are uniformly supported on an ellipse, with the major axis determining the labels, and
the rotation of the major axis randomly generated in a 90 degree range for each task. One
random realization of this synthetic data is shown in Figure 2. This synthetic data set is
an ideal candidate for marginal transfer learning, because the Bayes classiﬁer for a task is
uniquely determined by the marginal distribution of the features, i.e. Lemma 9 applies (and
the optimal error inf f E ∞(f ) is zero). On the other hand, observe that the expectation of
each X distribution is the same regardless of the task and thus does not provide any relevant
information, so that taking into account at least second order information is needed to
perform domain generalization.

To analyse the eﬀects of number of examples per task (n) and number of tasks (N ), we
constructed 12 synthetic data sets by taking combinations N × n where N ∈ {16, 64, 256}
and n ∈ {8, 16, 32, 256}. For each synthetic data set, the test set contains 10 tasks and each
task contains one million data points. All kernels are taken to be Gaussian, and the random
Fourier features speedup is used. The results are shown in Figure 3 and Tables 1 and 2
(see appendix). The marginal transfer learning (MTL) method signiﬁcantly outperforms
the baseline pooling method. Furthermore, the performance of MTL improves as N and n

28

increase, as expected. The pooling method, however, does no better than random guessing
regardless of N and n.

In the remaining experiments, the marginal distribution does not perfectly characterize
the optimal decision function, but still provides some information to oﬀer improvements
over pooling.

Algorithm 1: Synthetic Data Generation

input : N : Number of tasks, n: Number of training examples per task
output: Realization of synthetic data set for N tasks
for i = 1 to N do

• sample rotation αi uniformly in

(cid:104) π
4
• Take an ellipse whose major axis is aligned with the horizontal axis, and

3π
4

(cid:105)

;

,

rotate it by an angle of αi about its center;

• Sample n points Xij, j = 1, . . . , n uniformly at random from the rotated

ellipse;

• Label the points according to their position with respect to the major axis i.e.
the points that are on the right of the major axis are considered as class 1 and
the points on the left of the major axis are considered as class −1.

end

8.3 Parkinson’s Disease Telemonitoring

We test our method in the regression setting using the Parkinson’s disease telemonitoring
data set, which is composed of a range of biomedical voice measurements using a telemon-
itoring device from 42 people with early-stage Parkinson’s. The recordings were automat-
ically captured in the patients’ homes. The aim is to predict the clinician’s Parkinson’s
disease symptom score for each recording on the uniﬁed Parkinson’s disease rating scale
(UPDRS) (Tsanas et al., 2010). Thus we are in a regression setting, and employ the (cid:15)-
insensitive loss from support vector regression. All kernels are taken to be Gaussian, and
the random Fourier features speedup is used.

There are around 200 recordings per patient. We randomly select 7 test users and then
vary the number of training users N from 10 to 35 in steps of 5, and we also vary the
number of training examples n per user from 20 to 100. We repeat this process several
times to get the average errors which are shown in Fig 4 and Tables 3 and 4 (see appendix).
The marginal transfer learning method clearly outperforms pooling, especially as N and n
increase.

8.4 Satellite Classiﬁcation

Microsatellites are increasingly deployed in space missions for a variety of scientiﬁc and
technological purposes. Because of randomness in the launch process, the orbit of a mi-
crosatellite is random, and must be determined after the launch. One recently proposed

29

(a)

(b)

(c)

(d)

Figure 2: Plots of synthetic data sets (red and blue points represent negative and positive
classes) for diﬀerent settings: (a) Random realization of a single task with 256 training
examples per task. Plots (b), (c) and(d) are random realizations of synthetic data with 256
training examples for 16, 64 and 256 tasks.

approach is to estimate the orbit of a satellite based on radiofrequency (RF) signals as mea-
sured in a ground sensor network. However, microsatellites are often launched in bunches,
and for this approach to be successful, it is necessary to associate each RF measurement
vector with a particular satellite. Furthermore, the ground antennae are not able to decode
unique identiﬁer signals transmitted by the microsatellites, because (a) of constraints on
the satellite/ground antennae links, including transmission power, atmospheric attenuation,
scattering, and thermal noise, and (b) ground antennae must have low gain and low direc-
tional speciﬁcity owing to uncertainty in satellite position and dynamics. To address this
problem, recent work has proposed to apply our marginal transfer learning methodology
(Sharma and Cutler, 2015).

As a concrete instance of this problem, suppose two microsatellites are launched to-
gether. Each launch is a random phenomenon and may be viewed as a task in our frame-
work. For each launch i, training data (Xij, Yij), j = 1, . . . , ni, are generated using a highly
realistic simulation model, where Xij is a feature vector of RF measurements across a par-
ticular sensor network and at a particular time, and Yij is a binary label identifying which
of the two microsatellites produced a given measurement. By applying our methodology,
we can classify unlabeled measurements X T
from a new launch with high accuracy. Given
j
these labels, orbits can subsequently be estimated using the observed RF measurements.

30

Figure 3: Synthetic data set: Classiﬁcation error rates for proposed method and diﬀerence
with baseline for diﬀerent experimental settings, i.e., number of examples per task and
number of tasks.

Figure 4: Parkinson’s disease telemonitoring data set: Root mean square error rates for pro-
posed method and diﬀerence with baseline for diﬀerent experimental settings, i.e., number
of examples per task and number of tasks.

31

We thank Srinagesh Sharma and James Cutler for providing us with their simulated data,
and refer the reader to their paper for more details on the application (Sharma and Cutler,
2015).

To demonstrate this idea, we analyzed the data from Sharma and Cutler (2015) for
T = 50 launches, viewing up to 40 as training data and 10 as testing. We use Gaussian
kernels and the RFF kernel approximation technique to speed up the algorithm. Results
are shown in Fig 5 (tables given in the appendix). As expected, the error for the proposed
method is much lower than for pooling, especially as N and n increase.

Figure 5: Satellite data set: Classiﬁcation error rates for proposed method and diﬀerence
with baseline for diﬀerent experimental settings, i.e., number of examples per task and
number of tasks.

8.5 Flow Cytometry Experiments

We demonstrate the proposed methodology for the ﬂow cytometry auto-gating problem,
described in Sec. 2. The pooling approach has been previously investigated in this context
by Toedling et al. (2006). We used a data set that is a part of the FlowCAP Challenges
where the ground truth labels have been supplied by human experts (Aghaeepour et al.,
2013). We used the so-called “Normal Donors” data set. The data set contains 8 diﬀerent
classes and 30 subjects. Only two classes (0 and 2) have consistent class ratios, so we have
restricted our attention to these two.

The corresponding ﬂow cytometry data sets have sample sizes ranging from 18,641 to
59,411, and the proportion of class 0 in each data set ranges from 25.59 to 38.44%. We
randomly selected 10 tasks to serve as the test tasks. These tasks were removed from the
pool of eligible training tasks. We varied the number of training tasks from 5 to 20 with an
additive step size of 5, and the number of training examples per task from 1024 to 16384

32

with a multiplicative step size of 2. We repeated this process 10 times to get the average
classiﬁcation errors which are shown in Fig. 6 and Tables 7 and 8 (see appendix). The
kernel kP was Gaussian, and the other two were linear. The Nystr¨om approximation was
used to achieve an eﬃcient implementation.

For nearly all settings the proposed method has a smaller error rate than the baseline.
Furthermore, for the marginal transfer learning method, when one ﬁxes the number of
training examples and increases the number of tasks then the classiﬁcation error rate drops.
On the other hand, we observe on Table 7 that the number n of training points per
task hardly aﬀects the ﬁnal performance when n ≥ 103. This is in contrast with the
previous experimental examples (synthetic, Parkinson’s disease telemonitoring, and satellite
classiﬁcation), for which increasing n led to better performance, but where the values of
n remained somewhat modest (n ≤ 256). This is qualitatively in line with the theoretical
results under (2SGM) in Section 6.2 (see in particular the concluding discussion there),
suggesting that the inﬂuence of increasing n on the performance should eventually taper
oﬀ, in particular if n (cid:29) N .

Figure 6: Flow Cytometry Data set: Percentage Classiﬁcation error rates for proposed
method and diﬀerence with baseline for diﬀerent experimental settings, i.e., number of
examples per task and number of tasks.

9. Discussion

Our approach to domain generalization relies on the extended input pattern (cid:101)X = (PX , X).
Thus, we study the natural algorithm of minimizing a regularized empirical loss over a
reproducing kernel Hilbert space associated with the extended input domain PX × X . We
also establish universal consistency under two sampling plans. To achieve this, we present

33

novel generalization error analyses, and construct a universal kernel on PX × X . A detailed
implementation based on novel approximate feature mappings is also presented.

On one synthetic and three real-world data sets, the marginal transfer learning approach
consistently outperforms a pooling baseline. On some data sets, however, the diﬀerence
between the two methods is small. This is because the utility of transfer learning varies
from one DG problem to another. As an extreme example, if all of the task are the same,
then pooling should do just as well as our method.

Several future directions exist. From an application perspective, the need for adaptive
classiﬁers arises in many applications, especially in biomedical applications involving biolog-
ical and/or technical variation in patient data. Examples include brain computer interfaces
and patient monitoring. For example, when electrocardiograms are used to continuously
monitor cardiac patients, it is desirable to classify each heartbeat as irregular or not. Given
the extraordinary amount of data involved, automation of this process is essential. How-
ever, irregularities in a test patient’s heartbeat will diﬀer from irregularities of historical
patients, hence the need to adapt to the test distribution (Wiens, 2010).

From a theoretical and methodological perspective, several questions are of interest. We
would like to specify conditions on the meta-distributions PS or µ under which the DG risk
is close to the expected Bayes risk of the test distribution (beyond the simple condition
discussed in Lemma 9). We would also like to develop fast learning rates under suitable dis-
tributional assumptions. Furthermore, given the close connections with supervised learning,
many common variants of supervised learning can also be investigated in the DG context,
including multiclass classiﬁcation, class probability estimation, and robustness to various
forms of noise.

We can also ask how the methodology and analysis can be extended to the context
where a small number of labels are available for the test distribution (additionally to a
larger number of unlabeled data from the same distribution); this situation appears to be
common in practice, and can be seen as intermediary between the DG and learning to learn
(LTL, see Section 4.2) settings (one could dub it “semi-supervised domain generalization”).
In this setting, two approaches appear promising to take advantage of the labeled data. The
simplest one is to use the same optimization problem (7), where we include additionally
the labeled examples of the test distribution. However, if several test samples are to be
treated in succession, and we want to avoid a full, resource-consuming re-training using
all the training samples each time, an interesting alternative is the following:
learn once
a function f0(PX , x) using the available training samples via (7); then, given a partially
labeled test sample, learn a decision function on this sample only via the usual kernel (kX )
norm regularized empirical loss minimization method, but replace the usual regularizer
term (cid:107)f (cid:107)2
X , .) ∈ HkX ). In this sense, the marginal-
adaptive decision function learned from the training samples would serve as a “prior” or
“informed guess” for learning on the test data. This can be also interpreted as learning
an adequate complexity penalty to improve learning on new samples, thus connecting to
the general principles of LTL (see Section 4.2). An interesting diﬀerence with underlying
existing LTL approaches is that those tend to adapt the hypothesis class or the“shape” of the
regularization penalty to the problem at hand, while the approach delineated above would

(cid:13)
(cid:13)f − f0( (cid:98)P T
(cid:13)

(note that f0( (cid:98)P T

H by

X , .)

(cid:13)
2
(cid:13)
(cid:13)

H

34

modify the “origin” of the penalty, using the marginal distribution information. These two
principles could also be combined.

35

Appendix A. Proofs, Technical Details, and Experimental Details

This section contains technical details for the proofs of the announced results.

A.1 Proof of Proposition 7

XY be a ﬁxed probability distribution on X × R, and ε > 0 a ﬁxed number. Since X
X (the
XY ), is inner regular, so that there exists a compact set K ⊂ X such that

Let P T
is a Radon space, by deﬁnition any Borel probability measure on it, in particular P T
X-marginal of P T
P T
X (Kc) ≤ ε.

(cid:12)f (u, v) − f (P T

For all x ∈ K, by the assumed continuity of the decision function f at point (P T

X , x) there
exists an open neighborhood Ux×Vx ⊂ PX ×X of this point such that (cid:12)
X , x)(cid:12)
(cid:12) ≤
ε for all (u, v) ∈ Ux × Vx. Since the family (Vx)x∈K is an open covering of the compact
K, there exists a ﬁnite subfamily (Vxi)i∈I covering K. Denoting U0 := (cid:84)
i∈I Uxi which is
an open neighborhood of P T
X in PX , it therefore holds for any P ∈ U0 and uniformly over
X , x)(cid:12)
x ∈ K that (cid:12)
(cid:12)f (P T
(cid:12)f (P, x) − f (P T
(cid:12) ≤ 2ε, where
i0 ∈ I is such that x ∈ Vxi0
.
Denote ST = (X T
i , Y T
(cid:98)P T
X ∈ U0

XY , and A the
X in probability,
event
so that P [Ac] ≤ ε holds for nT large enough. We have (denoting B a bound on the loss
function):

from P T
X weakly converges to P T

i )1≤i≤nT a sample of size nT drawn i.i.d.

. By the law of large numbers, (cid:98)P T

(cid:12) ≤ |f (P, x) − f (P, xi0)| + (cid:12)

X , x) − f (P, xi0)(cid:12)

(cid:111)

(cid:110)

EST

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:96)(f ( (cid:98)P T

X , X T

(cid:35)
i ), Y T
i )

≤ Bε + EST

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:96)(f ( (cid:98)P T

X , X T

i ), Y T

i )1{X T

i ∈K}

(cid:35)

(cid:35)

(cid:35)

1
nT

nT(cid:88)

i=1

1
nT

nT(cid:88)

i=1

≤ B(ε + P [Ac]) + EST

1{ (cid:98)P T

X ∈U0}

(cid:96)(f ( (cid:98)P T

X , X T

i ), Y T

i )1{X T

i ∈K}

≤ 2Bε + 2Lε + EST

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:96)(f (P T

X , X T

(cid:35)
i ), Y T
i )

≤ 2(B + L)ε + E

(X T ,Y T )∼P T

XY

(cid:2)(cid:96)(f (P T

X , X T ), Y T )(cid:3) .

Conversely,

EST

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:35)

(cid:96)(f ( (cid:98)P T

X , X T

i ), Y T
i )

≥ EST

1{ (cid:98)P T

X ∈U0}

(cid:96)(f ( (cid:98)P T

X , X T

i ), Y T

i )1{X T

i ∈K}

≥ EST

1
nT

nT(cid:88)

i=1

(cid:96)(f (P T

X , X T

(cid:35)
i ), Y T
i )

− 2Bε − 2Lε

≥ E

(X T ,Y T )∼P T

XY

(cid:2)(cid:96)(f (P T

X , X T ), Y T )(cid:3) − 2(B + L)ε.

Since the above inequalities hold for any ε > 0 provided nT is large enough, this yields that
for any ﬁxed P T

XY , we have

E

lim
nT →∞

ST ∼(P T

XY )⊗nT

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:96)(f ( (cid:98)P T

X , X T

(cid:35)
i ), Y T
i )

= E

(X T ,Y T )∼P T

XY

(cid:2)(cid:96)(f (P T

X , X T ), Y T )(cid:3) .

(cid:34)

(cid:34)

(cid:34)

36

Finally, since the above right-hand side is bounded by B, applying dominated convergence
to integrate over P T

XY ∼ µ yields the desired conclusion.

A.2 Proof of Corollary 14

Proof Denote E ∗ = inf f :PX ×X →R E(f ). Let ε > 0. Since k is a universal kernel on PX × X
and (cid:96) is Lipschitz, there exists f0 ∈ Hk such that E(f0) ≤ E ∗+ ε
2 (Steinwart and Christmann,
2008).

By comparing the objective function in (7) at the minimizer (cid:98)fλ and at the null function,
using assumption (LB) we deduce that we must have (cid:107) (cid:98)fλ(cid:107) ≤ (cid:112)B0/λ. Applying Theorem 11
for R = Rλ = (cid:112)B0/λ, and δ = 1/N 2, gives that with probability at least 1 − 1/N 2,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ≤ ε(N ) := (B0 + L(cid:96)BKBk
(cid:12) (cid:98)E(f, N ) − E(f )

(cid:112)B0/λ)

sup
f ∈Bk(R)

√
(

log N + 2)
N

√

.

Let N be large enough so that (cid:107)f0(cid:107) ≤ Rλ. We can now deduce that with probability at

least 1 − 1/N 2,

E( (cid:98)fλ) ≤ (cid:98)E( (cid:98)fλ, N ) + ε(N )

= (cid:98)E( (cid:98)fλ, N ) + λ(cid:107) (cid:98)fλ(cid:107)2 − λ(cid:107) (cid:98)fλ(cid:107)2 + ε(N )
≤ (cid:98)E(f0, N ) + λ(cid:107)f0(cid:107)2 − λ(cid:107) (cid:98)fλ(cid:107)2 + ε(N )
≤ (cid:98)E(f0, N ) + λ(cid:107)f0(cid:107)2 + ε(N )
≤ E(f0) + λ(cid:107)f0(cid:107)2 + 2ε(N )
≤ E ∗ +

+ λ(cid:107)f0(cid:107)2 + 2ε(N ).

ε
2

The last two terms become less than ε
growth of λ = λ(N ). This establishes that for any ε > 0, there exists N0 such that

2 for N suﬃciently large by the assumptions on the

(cid:88)

N ≥N0

Pr(E( (cid:98)fλ) ≥ E ∗ + ε) ≤

(cid:88)

N ≥N0

1
N 2 < ∞,

and so the result follows by the Borel-Cantelli lemma.

37

A.3 Proof of Theorem 15

We control the diﬀerence between the training loss and the conditional risk at inﬁnite sample
size via the following decomposition:

|L(S, f ) − E ∞(f |PXY )| = sup

sup
f ∈Bk(R)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:96)(f ( (cid:98)PX , Xi), Yi) − E ∞(f |PXY )
(cid:12)
(cid:12)
(cid:12)

f ∈Bk(R)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

f ∈Bk(R)

≤ sup

1
n
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
=: (I) + (II).

+ sup

f ∈Bk(R)

n
(cid:88)

(cid:16)

i=1

1
n

n
(cid:88)

i=1

(cid:96)(f ( (cid:98)PX , Xi), Yi) − (cid:96)(f (PX , Xi), Yi)

(cid:12)
(cid:12)
(cid:96)(f (PX , Xi), Yi) − E ∞(f |PXY )
(cid:12)
(cid:12)
(cid:12)

(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(31)

A.3.1 Control of term (I)

Using the assumption that the loss (cid:96) is L(cid:96)-Lipschitz in its ﬁrst coordinate, we can bound
the ﬁrst term as follows:

(I) ≤ L(cid:96)

sup
f ∈Bk(R)

1
n

n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ≤ L(cid:96)
(cid:12)f ( (cid:98)PX , Xi) − f (PX , Xi)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)f ( (cid:98)PX , ·) − f (PX , ·)
(cid:13)∞

. (32)

sup
f ∈Bk(R)

This can now be controlled using the ﬁrst part of the following result:

Lemma 23 Assume (K-Bounded) holds. Let PX be an arbitrary distribution on X and
(cid:98)PX denote an empirical distribution on X based on an iid sample of size n from PX . Then
with probability at least 1 − δ over the draw of this sample, it holds that

(cid:13)
(cid:13)
(cid:13)f ( (cid:98)PX , ·) − f (PX , ·)

(cid:13)
(cid:13)
(cid:13)∞

≤ 3RBkLKBα
k(cid:48)

(cid:18) log 2δ−1
n

(cid:19) α

2

.

sup
f ∈Bk(R)

(33)

In expectation, it holds

(cid:34)

E

sup
f ∈Bk(R)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)f ( (cid:98)PX , ·) − f (PX , ·)
(cid:13)∞

(cid:35)

≤ 2RBkLKBα

k(cid:48)n−α/2.

(34)

Proof Let X1, . . . , Xn denote the n-sample from PX . Let us denote by Φ(cid:48)
X the canonical
feature mapping x (cid:55)→ k(cid:48)
X (x)(cid:107) ≤ Bk(cid:48),
and so, as a consequence of Hoeﬀding’s inequality in a Hilbert space (see, e.g., Pinelis and
Sakhanenko, 1985), it holds with probability at least 1 − δ:

. We have for all x ∈ X , (cid:107)Φ(cid:48)

X (x, ·) from X into Hk(cid:48)

X

(cid:13)
(cid:13)
(cid:13)Ψ(PX ) − Ψ( (cid:98)PX )

(cid:13)
(cid:13)
(cid:13) =

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

Φ(cid:48)

X (Xi) − EX∼PX

(cid:2)Φ(cid:48)

X (X)(cid:3)

≤ 3Bk(cid:48)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:114)

log 2δ−1
n

.

(35)

38

Furthermore, using the reproducing property of the kernel k, we have for any x ∈ X and
f ∈ Bk(R):

|f ( (cid:98)PX , x) − f (PX , x)| =

(cid:68)

(cid:12)
(cid:12)
(cid:12)

k(( (cid:98)PX , x), ·) − k((PX , x), ·), f

(cid:69)(cid:12)
(cid:12)
(cid:12)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)k(( (cid:98)PX , x), ·) − k((PX , x), ·)
(cid:13)
K(Ψ(PX ), Ψ(PX ))

(cid:16)

1
2

≤ RkX (x, x)

≤ (cid:107)f (cid:107)

+ K(Ψ( (cid:98)PX ), Ψ( (cid:98)PX )) − 2K(Ψ(PX ), Ψ( (cid:98)PX ))
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)ΦK(Ψ(PX )) − ΦK(Ψ( (cid:98)PX ))
(cid:13)

≤ RBk

(cid:17) 1
2

≤ RBkLK

(cid:13)
(cid:13)
α
(cid:13)
(cid:13)
(cid:13)Ψ(PX ) − Ψ( (cid:98)PX )
(cid:13)

,

where in the last step we have used property (K-H¨older) together with the fact that for
all P ∈ PX , (cid:107)Ψ(P )(cid:107) ≤ (cid:82)
(Bk(cid:48)). Combining
with (35) gives (33).

X (x, ·)(cid:107) dPX (x) ≤ Bk(cid:48), so that Ψ(P ) ∈ Bk(cid:48)

X (cid:107)k(cid:48)

X

For the bound in expectation, we use the inequality above, and can bound further (using

Jensen’s inequality, since α ≤ 1)

E

(cid:13)
(cid:104)(cid:13)
(cid:13)
(cid:13)
(cid:13)Ψ(PX ) − Ψ( (cid:98)PX )
(cid:13)

α(cid:105)

≤ E

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Ψ(PX ) − Ψ( (cid:98)PX )
(cid:13)

2(cid:21)α/2

E (cid:2)(cid:10)Φ(cid:48)

X (Xi) − E (cid:2)Φ(cid:48)

X (X)(cid:3) , Φ(cid:48)

X (Xj) − E (cid:2)Φ(cid:48)

X (X)(cid:3)(cid:11)(cid:3)





α/2

E

(cid:104)(cid:13)
(cid:13)Φ(cid:48)

X (Xi) − E (cid:2)Φ(cid:48)

X (X)(cid:3)(cid:13)
(cid:13)

(cid:33)α/2

2(cid:105)



=



(cid:32)

=

≤

1
n2

1
n2

n
(cid:88)

i,j=1

n
(cid:88)

i=1

(cid:19) α

2

(cid:18) 4B2
k(cid:48)
n

,

which yields (34) in combination with the above.

A.3.2 Control of term (II)

Term (II) takes the form of a uniform deviation over a RKHS ball of an empirical loss
for the data ( (cid:101)Xi, Yi), where (cid:101)Xi := (PX , Xi). Since PX is ﬁxed (in contrast with term (I)
where (cid:98)PX depended on the whole sample), these data are i.i.d. Similar to the proofs of
Theorems 5 and 11, we can therefore apply again standard Rademacher analysis, this time
at the level of one speciﬁc task (Azuma-McDiarmid inequality followed by Rademacher
complexity analysis for a Lipschitz, bounded loss over a RKHS ball; see Koltchinskii, 2001;
Bartlett and Mendelson, 2002, Theorems 8, 12 and Lemma 22 there). The kernel k is
bounded by B2
K by assumption (K-Bounded); by Lemma 10 and assumption (LB), the

kB2

39

loss is bounded by B0 + L(cid:96)RBkBK, and is L(cid:96)-Lipschitz. Therefore, with probability at least
1 − δ we get

(II) ≤ (B0 + 3L(cid:96)RBkBK) min

(cid:32)(cid:114)

(cid:33)

log(δ−1)
2n

, 1

(cid:32)(cid:18) log(δ−1)

(cid:19) α

2

2n

(cid:33)

, 1

.

≤ (B0 + 3L(cid:96)RBkBK) min

(36)

Observe that we can cap the second factor at 1 since (II) is upper bounded by the bound
on the loss in all cases; the second inequality then uses α ≤ 1. Combining with a union
bound the probabilistic controls (32), (33) of term (I) and (36) of (II) yields (18).

To establish the bound (19) we use a similar argument. We use the decomposition

|E(f |PXY , n) − E ∞(f |PXY )|

sup
f ∈Bk(R)

≤ sup

f ∈Bk(R)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ sup

f ∈Bk(R)
=: (I (cid:48)) + (II (cid:48)).

ESn∼(PXY )⊗n

(cid:12)
(cid:12)
ES∼(PXY )⊗n
(cid:12)
(cid:12)
(cid:12)

(cid:34)

(cid:34)

1
n

1
n

n
(cid:88)

(cid:16)

i=1
n
(cid:88)

i=1

(cid:96)(f ( (cid:98)PX , X T

i ), Yi) − (cid:96)(f (PX , Xi), Yi)

(cid:0)(cid:96)(f (PX , X T

i ), Yi)(cid:1)

(cid:35)

(cid:12)
(cid:12)
− E(X,Y )∼PXY [(cid:96)(f (PX , X), Y )]
(cid:12)
(cid:12)
(cid:12)

(cid:17)

(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

for any ﬁxed f ∈ Bk(R) and PXY , the
It is easily seen that the second term vanishes:
diﬀerence of the expectations is zero. For the ﬁrst term, using Lipschitzness of the loss,
then (34), we obtain

(I (cid:48)) ≤ L(cid:96)E

(cid:34)

sup
f ∈Bk(R)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)f ( (cid:98)PX , .) − f (PX , .)
(cid:13)∞

(cid:35)

≤ 2L(cid:96)RBkLKBα

k(cid:48)n−α/2,

yielding (19). The bound (20) is obtained as a direct consequence by taking expectation
over PXY ∼ µ and using Jensen’s inequality to pull out the absolute value.

A.4 Regularity conditions for the kernel on distributions

We investigate suﬃcient conditions on the kernel K to ensure the regularity condition (K-
H¨older) (15). Roughly speaking, the regularity of the feature mapping of a reproducing
kernel is “one half” of the regularity of the kernel in each of its variables. The next result
considers the situation where K is itself simply a H¨older continuous function of its variables.

Lemma 24 Let α ∈ (0, 1
constant L2

2 ]. Assume that the kernel K is H¨older continuous of order 2α and

K/2 in each of its two variables on Bk(cid:48)

X

(Bk(cid:48)). Then (K-H¨older) is satisﬁed.

Proof For any v, w ∈ Bk(cid:48)

(Bk(cid:48)):

X

(cid:107)ΦK(v) − ΦK(w)(cid:107) = (K(v, v) + K(w, w) − 2K(v, w))

1

2 ≤ LK (cid:107)v − w(cid:107)α .

40

The above type of regularity only leads to a H¨older feature mapping of order at most 1
2
(when the kernel function is Lipschitz continuous in each variable). Since this order plays
an important role in the rate of convergence of the upper bound in the main error control
theorem, it is desirable to study conditions ensuring more regularity, in particular a feature
mapping which has at least Lipschitz continuity. For this, we consider the following stronger
condition, namely that the kernel function is twice diﬀerentiable in a speciﬁc sense:

Lemma 25 Assume that, for any u, v ∈ Bk(cid:48)
, the
function hu,v,e : (λ, µ) ∈ R2 (cid:55)→ K(u + λe, v + µe) admits a mixed partial derivative ∂1∂2hu,v,e
at the point (λ, µ) = (0, 0) which is bounded in absolute value by a constant C2
K independent
of (u, v, e). Then (15) is satisﬁed with α = 1 and LK = CK, that is, the canonical feature
mapping of K is Lipschitz continuous on Bk(cid:48)

(Bk(cid:48)) and unit norm vector e of Hk(cid:48)

(Bk(cid:48)).

X

X

X

Proof The argument is along the same lines as Steinwart and Christmann (2008), Lemma
4.34. Observe that, since hu,v,e(λ + λ(cid:48), µ + µ(cid:48)) = hu+λe,v+µe,e(λ(cid:48), µ(cid:48)), the function hu,v,e
actually admits a uniformly bounded mixed partial derivative in any point (λ, µ) ∈ R2 such
(Bk(cid:48)) . Let us denote ∆1hu,v,e(λ, µ) := hu,v,e(λ, µ) − hu,v,e(0, µ) .
that (u + λe, v + µe) ∈ Bk(cid:48)
(Bk(cid:48)) , u (cid:54)= v , let us set λ := (cid:107)v − u(cid:107) and the unit vector e := λ−1(v − u);
For any u, v ∈ Bk(cid:48)
we have

X

X

(cid:107)ΦK(u) − ΦK(v)(cid:107)2 = K(u, u) + K(u + λe, u + λe) − K(u, u + λe) − K(u + λe, u)

= ∆1hu,v,e(λ, λ) − ∆1hu,v,e(λ, 0)
= λ∂2∆1hu,v,e(λ, λ(cid:48)) ,

where we have used the mean value theorem, yielding existence of λ(cid:48) ∈ [0, λ] such that the
last equality holds. Furthermore,

∂2∆1hu,v,e(λ, λ(cid:48)) = ∂2hu,v,e(λ, λ(cid:48)) − ∂2hu,v,e(0, λ(cid:48))

= λ∂1∂2hu,v,e(λ(cid:48)(cid:48), λ(cid:48)) ,

using again the mean value theorem, yielding existence of λ(cid:48)(cid:48) ∈ [0, λ] in the last equality.
Finally, we get

(cid:107)ΦK(u) − ΦK(v)(cid:107)2 = λ2∂1∂2hu,v,e(λ(cid:48), λ(cid:48)(cid:48)) ≤ C2

K (cid:107)v − u(cid:107)2 .

Lemma 26 Assume that the kernel K takes the form of either (a) K(u, v) = g((cid:107)u − v(cid:107)2)
or (b) K(u, v) = g((cid:104)u, v(cid:105)) , where g is a twice diﬀerentiable real function of a real variable
k(cid:48)] in case (b). Assume (cid:107)g(cid:48)(cid:107)∞ ≤ C1 and
deﬁned on [0, 4B2
(cid:107)g(cid:48)(cid:48)(cid:107)∞ ≤ C2. Then K satisﬁes the assumption of Lemma 25 with CK := 2C1 + 16C2B2
k(cid:48) in
case (a), and CK := C1 + C2B2

k(cid:48)] in case (a), and on [−B2

k(cid:48), B2

k(cid:48) for case (b).

41

Proof In case (a), we have hu,v,e(λ, µ) = g((cid:107)u − v + (λ − µ)e(cid:107)2). It follows

|∂1∂2hu,v,e(0, 0)| =

(cid:12)
(cid:12)

(cid:12)−2g(cid:48)((cid:107)u − v(cid:107)2) (cid:107)e(cid:107)2 − 4g(cid:48)(cid:48)((cid:107)u − v(cid:107)2) (cid:104)u − v, e(cid:105)2(cid:12)
≤ 2C1 + 16C2B2

(cid:12)
(cid:12)

k(cid:48) .

In case (b), we have hu,v,e(λ, µ) = g((cid:104)u + λe, v + µe(cid:105)). It follows

|∂1∂2hu,v,e(0, 0)| =

(cid:12)
(cid:12)g(cid:48)((cid:104)u, v(cid:105)) (cid:107)e(cid:107)2 + g(cid:48)(cid:48)((cid:104)u, v(cid:105)) (cid:104)u, e(cid:105) (cid:104)v, e(cid:105)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

≤ C1 + C2B2

k(cid:48) .

A.5 Proof of Lemma 12

Proof Let H, H(cid:48) the RKHS associated to k, k(cid:48) with the associated feature mappings Φ, Φ(cid:48).
Then it can be checked that (x, x(cid:48)) ∈ X × X (cid:48)
(cid:55)→ Φ(x) ⊗ Φ(cid:48)(x(cid:48)) is a feature mapping for
k into the Hilbert space H ⊗ H(cid:48). Using (Steinwart and Christmann, 2008), Th. 4.21,
we deduce that the RKHS H of k contains precisely all functions of the form (x, x(cid:48)) ∈
X × X (cid:48)
(cid:55)→ Fw(x, x(cid:48)) = (cid:104)w, Φ(x) ⊗ Φ(x(cid:48))(cid:105), where w ranges over H ⊗ H(cid:48). Taking w of the
form w = g ⊗ g(cid:48), g ∈ H, g ∈ H(cid:48), we deduce that H contains in particular all functions of
the form f (x, x(cid:48)) = g(x)g(x(cid:48)), and further

(cid:101)H := span (cid:8)(x, x(cid:48)) ∈ X × X (cid:48) (cid:55)→ g(x)g(x(cid:48)); g ∈ H, g(cid:48) ∈ H(cid:48)(cid:9) ⊂ H.

Denote C(X ), C(X (cid:48)), C(X × X (cid:48)) the set of real-valued continuous functions on the respective
spaces. Let

C(X ) ⊗ C(X (cid:48)) := span (cid:8)(x, x(cid:48)) ∈ X × X (cid:48) (cid:55)→ f (x)f (cid:48)(x(cid:48)); f ∈ C(X ), f (cid:48) ∈ C(X (cid:48))(cid:9) .

Let G(x, x(cid:48)) be an arbitrary element of C(X ) ⊗ C(X (cid:48)), G(x, x(cid:48)) = (cid:80)k
gi ∈ C(X ), g(cid:48)
exist fi ∈ H, f (cid:48)
F (x, x(cid:48)) := (cid:80)k

i(x(cid:48)) with
i ∈ C(X (cid:48)) for i = 1, . . . , k. For ε > 0, by universality of k and k(cid:48), there
i(cid:107)∞ ≤ ε for i = 1, . . . , k. Let

i ∈ H(cid:48) so that (cid:107)fi − gi(cid:107)∞ ≤ ε, (cid:107)f (cid:48)
i(x(cid:48)) ∈ (cid:101)H. We have

i=1 λigi(x)g(cid:48)

i=1 λifi(x)f (cid:48)

i − g(cid:48)

(cid:13)F (x, x(cid:48)) − G(x, x(cid:48))(cid:13)
(cid:13)

(cid:13)∞ ≤

λi(gi(x)g(cid:48)

i(x) − fi(x)f (cid:48)

(cid:13)
(cid:13)
(cid:13)
i(x))
(cid:13)
(cid:13)∞

=

λi

(fi(x) − gi(x))(g(cid:48)

i(x(cid:48)) − f (cid:48)

i(x(cid:48)))

(cid:104)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k
(cid:88)

i=1

k
(cid:88)

i=1

+ gi(x)(g(cid:48)

i(x) − f (cid:48)

i(x(cid:48))) + (gi(x) − fi(x))g(cid:48)

(cid:105)
i(x(cid:48))

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤ ε

k
(cid:88)

i=1

|λi| (ε + (cid:107)gi(cid:107)∞ + (cid:13)
(cid:13)g(cid:48)
i

(cid:13)
(cid:13)∞) .

42

This establishes that (cid:101)H is dense in C(X ) ⊗ C(X (cid:48)) for the supremum norm. It can be easily
checked that C(X ) ⊗ C(X (cid:48)) is an algebra of functions which does not vanish and separates
points on X × X (cid:48). By the Stone-Weierstrass theorem, it is therefore dense in C(X × X (cid:48)) for
the supremum norm. We deduce that (cid:101)H (and thus also H) is dense in C(X × X (cid:48)), so that
k is universal.

A.6 Proof of Theorem 20

Proof Observe:

and denote:

(cid:26) −1
2σ2
P

(cid:26) −1
2σ2
P

¯k(˜x, ˜x(cid:48)) = exp

(cid:107)Ψ( (cid:98)PX ) − Ψ( (cid:98)P (cid:48)

X )(cid:107)2

exp

(cid:27)

(cid:27)

(cid:107)x − x(cid:48)(cid:107)2

,

(cid:26) −1
2σ2
X

˜k(˜x, ˜x(cid:48)) = exp

(cid:107)ZP ( (cid:98)PX ) − ZP ( (cid:98)P (cid:48)

X )(cid:107)2

exp

(cid:27)

(cid:27)

(cid:107)x − x(cid:48)(cid:107)2

,

(cid:26) −1
2σ2
X

We omit the arguments of ¯k, ˜k for brevity. Let kq be the ﬁnal approximation (kq =
¯z(˜x)T ¯z(˜x(cid:48))) and then we have

|¯k − kq| = |¯k − ˜k + ˜k − kq| ≤ |¯k − ˜k| + |˜k − kq|.

From Eqn. (37) it follows that,

P (|¯k − kq| ≥ (cid:15)l + (cid:15)q) ≤ P (|¯k − ˜k| ≥ (cid:15)l) + P (|˜k − kq| ≥ (cid:15)q).

By a direct application of Hoeﬀding’s inequality,

P (|˜k − kq| ≥ (cid:15)q) ≤ 2 exp(−

Q(cid:15)2
q
2

).

(37)

(38)

(39)

Recall that (cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)
again by Hoeﬀding

X )(cid:105) = 1
n1n2

(cid:80)n1
i=1

(cid:80)n2

j=1 k(cid:48)

X (Xi, X (cid:48)

j). For a pair Xi, X (cid:48)

j, we have

P (|z(cid:48)

X (Xi)T z(cid:48)

X (X (cid:48)

j) − k(cid:48)

X (Xi, X (cid:48)

j)| ≥ (cid:15)) ≤ 2 exp(−

Let Ωij be the event |z(cid:48)
bound we have

X (Xi)T z(cid:48)

X (X (cid:48)

j) − k(cid:48)

X (Xi, X (cid:48)

j)| ≥ (cid:15), for particular i, j. Using the union

P (Ω11 ∪ Ω12 ∪ . . . ∪ Ωn1n2) ≤ 2n1n2 exp(−

This implies

P (|ZP ( (cid:98)PX )T ZP ( (cid:98)P (cid:48)

X ) − (cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

X )(cid:105)| ≥ (cid:15)) ≤ 2n1n2 exp(−

(40)

L(cid:15)2
2

).

43

L(cid:15)2
2

).

L(cid:15)2
2

)

Therefore,

(cid:12)
¯k − ˜k
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) =

(cid:34)

≤

exp

(cid:26) −1
2σ2
X

exp

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:26) −1
2σ2
P
(cid:26) −1
2σ2
P

(cid:27) (cid:34)

(cid:107)x − x(cid:48)(cid:107)2

exp

(cid:107)Ψ( (cid:98)PX ) − Ψ( (cid:98)P (cid:48)

X )(cid:107)2

(cid:27)

− exp

(cid:107)ZP ( (cid:98)PX ) − ZP ( (cid:98)P (cid:48)

X )(cid:107)2

(cid:107)Ψ( (cid:98)PX ) − Ψ( (cid:98)P (cid:48)

X )(cid:107)2

− exp

(cid:107)ZP ( (cid:98)PX ) − ZP ( (cid:98)P (cid:48)

X )(cid:107)2

(cid:27) (cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:26) −1
2σ2
P
(cid:27) (cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:27)

(cid:27) (cid:34)

(cid:26) −1
2σ2
P

(cid:16)

(cid:110) −1
2σ2
P

=

exp

(cid:107)Ψ( (cid:98)PX ) − Ψ( (cid:98)P (cid:48)

X )(cid:107)2

1 − exp

(cid:107)ZP ( (cid:98)PX ) − ZP ( (cid:98)P (cid:48)

X )(cid:107)2

(cid:26) −1
2σ2
P

− (cid:107)Ψ( (cid:98)PX ) − Ψ( (cid:98)P (cid:48)

(cid:35)(cid:12)
(cid:12)
X )(cid:107)2(cid:17)(cid:111)
(cid:12)
(cid:12)
(cid:12)

(cid:34)

≤

1 − exp

(cid:16)

(cid:26) −1
2σ2
P

(cid:40)

(cid:16)

−1
2σ2
P

(cid:40)

(cid:16)

1
2σ2
P

(cid:107)ZP ( (cid:98)PX ) − ZP ( (cid:98)P (cid:48)

X )(cid:107)2 − (cid:107)Ψ( (cid:98)PX ) − Ψ( (cid:98)P (cid:48)

=

1 − exp

ZP ( (cid:98)PX )T ZP ( (cid:98)PX ) − (cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)PX )(cid:105) + ZP ( (cid:98)P (cid:48)

X )T ZP ( (cid:98)P (cid:48)

X )

− (cid:104)Ψ( (cid:98)P (cid:48)

X ), Ψ( (cid:98)P (cid:48)

X )(cid:105) − 2(cid:0)ZP ( (cid:98)PX )T ZP ( (cid:98)P (cid:48)

X ) − (cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

≤

1 − exp

|ZP ( (cid:98)PX )T ZP ( (cid:98)PX ) − (cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)PX )(cid:105)| + |ZP ( (cid:98)P (cid:48)

X )T ZP ( (cid:98)P (cid:48)

X )

− (cid:104)Ψ( (cid:98)P (cid:48)

X ), Ψ( (cid:98)P (cid:48)

X )(cid:105)| + 2|(cid:0)ZP ( (cid:98)PX )T ZP ( (cid:98)P (cid:48)

X ) − (cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

X )(cid:107)2(cid:17)(cid:27) (cid:35)(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

X )(cid:105)(cid:1)(cid:17)

(cid:41)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:17)
X )(cid:105)(cid:1)|

(cid:41)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

The result now follows by applying the bound of Eqn. (40) to each of the three terms in
the exponent of the preceding expression, together with the stated formula for (cid:15) in terms
of (cid:15)(cid:96).

A.7 Proof of Theorem 22

Proof The proof is very similar to the proof of Theorem 20. We use Lemma 21 to replace
bound (39) with:

(cid:18)

P

sup
x,x(cid:48)∈M

(cid:19)

|˜k − kq| ≥ (cid:15)q

≤ 28

(cid:19)2

(cid:18) σ(cid:48)
X r
(cid:15)q

exp

(cid:18) −Q(cid:15)2
q
2(d + 2)

(cid:19)
.

(41)

44

Similarly, Eqn. (40) is replaced by

(cid:18)

P

sup
x,x(cid:48)∈M

(cid:12)
(cid:12)ZP ( (cid:98)PX )T ZP ( (cid:98)P (cid:48)

X ) − (cid:10)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

X )(cid:11)(cid:12)

(cid:12) ≥ (cid:15)

(cid:19)

≤ 29n1n2

(cid:19)2

(cid:18) σP σX r
(cid:15)l

exp

(cid:18) −L(cid:15)2
l

2(d + 2)

(cid:19)
.

(42)

The remainder of the proof now proceeds as in the previous proof.

A.8 Results in Tabular Format

Table 1: Average Classiﬁcation Error of Marginal Transfer Learning on Synthetic Data set

Table 2: Average Classiﬁcation Error of Pooling on Synthetic Data set

k
s
a
T
r
e
p

s
e
l
p
m
a
x
E

k
s
a
T
r
e
p

s
e
l

p
m
a
x
E

Tasks

16

64

256

8

16

32

36.01

33.08

31.69

31.55

31.03

30.96

30.44

29.31

23.87

256

23.78

7.22

1.27

Tasks

16

64

256

8

16

32

49.14

49.11

50.04

49.89

50.04

49.68

50.32

50.21

49.61

256

50.01

50.43

49.93

45

Table 3: RMSE of Marginal Transfer Learning on Parkinson’s Disease Data set

Tasks
20

10

15

25

30

35

13.78

12.37

11.93

10.74

10.08

11.17

14.18

11.89

11.51

10.90

10.55

10.18

14.95

13.29

12.00

10.21

10.59

9.52

13.27

11.66

11.79

12.89

11.27

11.17

13.15

11.70

13.81

10.12

9.16

9.91

9.28

9.03

9.03

8.01

9.34

9.10

9.01

8.44

8.16

7.30

7.14

10.50

10.05

8.69

7.62

7.88

7.01

7.5

12.16

13.03

11.98

9.59

9.16

9.18

8.48

9.85

8.80

9.74

9.52

100

12.69

20

24

28

34

41

49

58

70

84

20

24

28

34

41

49

58

70

84

k
s
a
T
r
e
p

s
e
l
p
m
a
x
E

k
s
a
T
r
e
p

s
e
l
p
m
a
x
E

Table 4: RMSE of Pooling on Parkinson’s Disease Data set

Tasks
20

10

15

25

30

35

13.64

11.93

11.95

11.06

11.91

12.08

13.80

11.83

11.70

11.98

11.68

11.48

13.78

11.70

11.72

11.18

11.58

11.73

13.71

12.20

12.04

11.17

11.67

11.92

13.69

11.73

12.08

11.28

11.55

12.59

13.75

11.85

11.79

11.17

11.34

11.82

13.70

11.89

12.06

11.06

11.82

11.65

13.54

11.86

12.14

11.21

11.40

11.96

13.55

11.98

12.03

11.25

11.54

12.22

100

13.53

11.85

11.92

11.12

11.96

11.84

46

Table 5: Average Classiﬁcation Error of Marginal Transfer Learning on Satellite Data set

Tasks
10

20

30

40

8.62

7.61

8.25

7.17

6.21

5.90

5.85

5.43

6.61

5.33

5.37

5.35

5.61

5.19

4.71

4.70

all training data

5.36

4.91

3.86

4.08

k
s
a
T
r
e
p

s
e
l
p
m
a
x
E

k
s
a
T
r
e
p

s
e
l
p
m
a
x
E

5

15

30

45

5

15

30

45

Table 6: Average Classiﬁcation Error of Pooling on Satellite Data set

Tasks
10

20

30

40

8.13

7.54

7.94

6.96

6.55

5.81

5.79

5.57

6.06

5.36

5.56

5.31

5.58

5.12

5.30

4.99

all training data

5.37

4.98

5.32

5.14

Table 7: Average Classiﬁcation Error of Marginal Transfer Learning on Flow Cytometry
Data set

Tasks
10

15

20

9.03

9.03

8.70

5

9

9.12

9.56

9.07

8.62

8.96

8.91

9.01

8.66

9.18

9.20

9.04

8.74

k
s
a
T
r
e
p

s
e
l
p
m
a
x
E

1024

2048

4096

8192

16384

9.05

9.08

9.04

8.63

47

Table 8: Average Classiﬁcation Error of Pooling on Flow Cytometry Data set

Tasks
10

5

15

20

1024

2048

4096

8192

9.41

9.48

9.32

9.52

9.92

9.57

9.45

9.54

9.72

9.56

9.36

9.40

9.43

9.53

9.38

9.50

16384

9.42

9.56

9.40

9.33

k
s
a
T
r
e
p

s
e
l
p
m
a
x
E

C. Scott and A. Deshmukh where supported in part by NSF Grants No. 1422157, 1217880,
and 1047871. G. Blanchard acknowledges support by the DFG via Research Unit 1735
Structural Inference in Statistics.

Acknowledgments

References

N. Aghaeepour, G. Finak, H. Hoos, T. R. Mosmann, R. Brinkman, R. Gottardo, R. H.
Scheuermann, FlowCAP Consortium, DREAM Consortium, et al. Critical assessment
of automated ﬂow cytometry data analysis techniques. Nature methods, 10(3):228–238,
2013.

Kei Akuzawa, Yusuke Iwasawa, and Yutaka Matsuo. Adversarial invariant feature learning

with accuracy constraint for domain generalization. ArXiv, abs/1904.12543, 2019.

Kamyar Azizzadenesheli, Anqi Liu, Fanny Yang, and Animashree Anandkumar. Regularized
learning for domain adaptation under label shifts. In International Conference on Learn-
ing Representations, 2019. URL https://openreview.net/forum?id=rJl0r3R9KX.

G¨okhan Bakır, Thomas Hofmann, Bernhard Sch¨olkopf, Alexander J Smola, and Ben Taskar.

Predicting structured data. MIT press, 2007.

Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa.

Metareg: To-
In S. Bengio, H. Wal-
wards domain generalization using meta-regularization.
ed-
and R. Garnett,
lach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
998–
itors, Advances
1008. Curran Associates,
URL http://papers.nips.cc/paper/
7378-metareg-towards-domain-generalization-using-meta-regularization.pdf.

Information Processing Systems

in Neural

pages

2018.

Inc.,

31,

P. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and

structural results. Journal of Machine Learning Research, 3:463–482, 2002.

P. Bartlett, M. Jordan, and J. McAuliﬀe. Convexity, classiﬁcation, and risk bounds. J.

Amer. Stat. Assoc., 101(473):138–156, 2006.

48

J. Baxter. A model of inductive bias learning. Journal of Artiﬁcial Intelligence Research,

12:149–198, 2000.

Shai Ben-David and Ruth Urner. On the hardness of domain adaptation and the utility
of unlabeled target samples. In Nader H. Bshouty, Gilles Stoltz, Nicolas Vayatis, and
Thomas Zeugmann, editors, Algorithmic Learning Theory, pages 139–153, 2012.

Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of repre-
sentations for domain adaptation. In B. Sch¨olkopf, J. C. Platt, and T. Hoﬀman, editors,
Advances in Neural Information Processing Systems 19, pages 137–144. 2007.

Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jen-
nifer Wortman Vaughan. A theory of learning from diﬀerent domains. Machine Learning,
79:151–175, 2010.

S. Bickel, M. Br¨uckner, and T. Scheﬀer. Discriminative learning under covariate shift. J.

Machine Learning Research, pages 2137–2155, 2009.

G. Blanchard, G. Lee, and C. Scott. Generalizing from several related classiﬁcation tasks
to a new unlabeled sample. In J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira,
and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 24,
pages 2178–2186. 2011.

G. Blanchard, M. Flaska, G. Handy, S. Pozzi, and C. Scott. Classiﬁcation with asymmetric
label noise: Consistency and maximal denoising. Electronic Journal of Statistics, 10:
2780–2824, 2016.

John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman.
Learning bounds for domain adaptation. In J. C. Platt, D. Koller, Y. Singer, and S. T.
Roweis, editors, Advances in Neural Information Processing Systems 20, pages 129–136.
2008.

Timothy I. Cannings, Yingying Fan, and Richard J. Samworth. Classiﬁcation with imperfect

training labels. Technical Report arXiv:1805.11505, 2018.

J. Carbonell, S. Hanneke, and L. Yang. A theory of transfer learning with applications to

active learning. Machine Learning, 90(2):161–189, 2013.

Fabio Maria Carlucci, Antonio D’Innocente, Silvia Bucci, Barbara Caputo, and Tatiana
Tommasi. Domain generalization by solving jigsaw puzzles. 2019 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pages 2224–2233, 2019.

R. Caruana. Multitask learning. Machine Learning, 28:41–75, 1997.

C. Chang and C. Lin. Libsvm: A library for support vector machines. ACM Transactions

on Intelligent Systems and Technology (TIST), 2(3):27, 2011.

A. Christmann and I. Steinwart. Universal kernels on non-standard input spaces. In J. Laf-
ferty, C. K. I. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, editors, Advances in
Neural Information Processing Systems 23, pages 406–414, 2010.

49

Corinna Cortes, Mehryar Mohri, Michael Riley, and Afshin Rostamizadeh. Sample selection

bias correction theory. In Algorithmic Learning Theory, pages 38–53, 2008.

Corinna Cortes, Mehryar Mohri, and Andr´es Mu˜noz Medina. Adaptation algorithm and
theory based on generalized discrepancy.
In Proceedings of the 21th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, KDD ’15, pages
169–178, 2015.

Daryl J. Daley and David Vere-Jones. An introduction to the theory of point processes,

volume II: general theory and structure. Springer, 2008.

G Denevi, Carlo Ciliberto, D Stamos, and Massimiliano Pontil. Incremental learning-to-
learn with statistical guarantees. In Proc. Uncertainty in Artiﬁcial Intelligence, 2018a.

Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil. Learning to
learn around a common mean. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing
Systems 31, pages 10169–10179. 2018b.

Zhengming Ding and Yun Fu. Deep domain generalization with structured low-rank con-

straint. IEEE Transactions on Image Processing, 27:304–313, 2018.

Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain gen-
eralization via model-agnostic learning of semantic features. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d’ Alch´e-Buc, E. Fox, and R. Garnett, editors, Advances in Neural
Information Processing Systems 32, pages 6450–6461. 2019.

P. Drineas and M. W. Mahoney. On the Nystr¨om method for approximating a gram matrix
for improved kernel-based learning. The Journal of Machine Learning Research, 6:2153–
2175, 2005.

M. C. Du Plessis and M. Sugiyama. Semi-supervised learning of class balance under class-
prior change by distribution matching. In J. Langford and J. Pineau, editors, Proc. 29th
Int. Conf. on Machine Learning, pages 823–830, 2012.

T. Evgeniou, C. A. Michelli, and M. Pontil. Learning multiple tasks with kernel methods.

J. Machine Learning Research, pages 615–637, 2005.

R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. Liblinear: A library for large linear

classiﬁcation. The Journal of Machine Learning Research, 9:1871–1874, 2008.

Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast
adaptation of deep networks. In Doina Precup and Yee Whye Teh, editors, Proceedings
of the 34th International Conference on Machine Learning, volume 70 of Proceedings of
Machine Learning Research, pages 1126–1135, 2017.

Chuang Gan, Tianbao Yang, and Boqing Gong. Learning attributes equals multi-source
domain generalization. In The IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), June 2016.

50

Pascal Germain, Amaury Habrard, Fran¸cois Laviolette, and Emilie Morvant. A new pac-
bayesian perspective on domain adaptation. In ICML, volume 48 of JMLR Workshop
and Conference Proceedings, pages 859–868, 2016.

M. Ghifary, D. Balduzzi, B. Kleijn, and M. Zhang. Scatter component analysis: A uniﬁed
framework for domain adaptation and domain generalization. IEEE Trans. Patt. Anal.
Mach. Intell., 39(7):1411–1430, 2017.

Muhammad Ghifary, W. Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain
generalization for object recognition with multi-task autoencoders. In Proceedings of the
2015 IEEE International Conference on Computer Vision (ICCV), page 25512559, 2015.

Mingming Gong, Kun Zhang, Tongliang Liu, Dacheng Tao, Clark Glymour, and Bernhard
Sch¨olkopf. Domain adaptation with conditional transferable components. In International
conference on machine learning, pages 2839–2848, 2016.

A. Gretton, K. Borgwardt, M. Rasch, B. Sch¨olkopf, and A. Smola. A kernel approach to
comparing distributions. In R. Holte and A. Howe, editors, Proceedings of the 22nd AAAI
Conference on Artiﬁcial Intelligence, pages 1637–1641, 2007a.

A. Gretton, K. Borgwardt, M. Rasch, B. Sch¨olkopf, and A. Smola. A kernel method for
the two-sample-problem. In B. Sch¨olkopf, J. Platt, and T. Hoﬀman, editors, Advances in
Neural Information Processing Systems 19, pages 513–520, 2007b.

T. Grubinger, A. Birlutiu, H. Sch¨oner, T. Natschl¨ager, and T. Heskes. Domain generaliza-
tion based on transfer component analysis. In I. Rojas, G. Joya, and A. Catala, editors,
Advances in Computational Intelligence. IWANN 2015, volume 9094 of Lecture Notes in
Computer Science, pages 325–334. Springer, 2015.

P. Hall. On the non-parametric estimation of mixture proportions. Journal of the Royal

Statistical Society, 43(2):147–156, 1981.

C. Hsieh, K. Chang, C. Lin, S. S. Keerthi, and S. Sundararajan. A dual coordinate descent
method for large-scale linear svm. In Proceedings of the 25th international conference on
Machine learning, pages 408–415. ACM, 2008.

Shoubo Hu, Kun Zhang, Zhitang Chen, and Laiwan Chan. Domain generalization via mul-
tidomain discriminant analysis. In Amir Globerson and Ricardo Silva, editors, Proceedings
of the Thirty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence, UAI 2019, Tel
Aviv, Israel, July 22-25, 2019, 2019.

Jiayuan Huang, Alexander J. Smola, Arthur Gretton, Karsten M. Borgwardt, and Bernhard
Scholkopf. Correcting sample selection bias by unlabeled data.
In Proceedings of the
19th International Conference on Neural Information Processing Systems, pages 601–608,
2007.

Wittawat Jitkrittum, Arthur Gretton, Nicolas Heess, SM Eslami, Balaji Lakshminarayanan,
Dino Sejdinovic, and Zolt´an Szab´o. Kernel-based just-in-time learning for passing expec-
tation propagation messages. In Proceedings of the Thirty-First Conference on Uncer-
tainty in Artiﬁcial Intelligence, pages 405–414. AUAI Press, 2015.

51

T. Joachims. Making large scale svm learning practical. Technical report, Universit¨at

Dortmund, 1999.

O. Kallenberg. Foundations of Modern Probability. Springer, 2002.

Takafumi Kanamori, Shohei Hido, and Masashi Sugiyama. A least-squares approach to

direct importance estimation. J. Mach. Learn. Res., 10:1391–1445, 2009.

Aditya Khosla, Tinghui Zhou, Tomasz Malisiewicz, Alexei A. Efros, and Antonio Torralba.
Undoing the damage of dataset bias. In Proceedings of the 12th European Conference on
Computer Vision - Volume Part I, page 158171, 2012.

Ron Kohavi et al. A study of cross-validation and bootstrap for accuracy estimation and

model selection. In IJCAI, volume 14, pages 1137–1145, 1995.

V. Koltchinskii. Rademacher penalties and structural risk minimization. IEEE Transactions

on Information Theory, 47(5):1902 – 1914, 2001.

P. Latinne, M. Saerens, and C. Decaestecker. Adjusting the outputs of a classiﬁer to new
a priori probabilities may signiﬁcantly improve classiﬁcation accuracy: Evidence from a
multi-class problem in remote sensing. In C. Sammut and A. H. Hoﬀmann, editors, Proc.
18th Int. Conf. on Machine Learning, pages 298–305, 2001.

Quoc Le, Tam´as Sarl´os, and Alex Smola. Fastfood: approximating kernel expansions in
In Proceedings of the 30th International Conference on International

loglinear time.
Conference on Machine Learning-Volume 28, pages III–244. JMLR. org, 2013.

Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and
artier domain generalization. In Proceedings of the IEEE International Conference on
Computer Vision, pages 5542–5550, 2017.

Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Learning to generalize:
Meta-learning for domain generalization. In Thirty-Second AAAI Conference on Artiﬁcial
Intelligence, 2018a.

H. Li, S. J. Pan, S. Wang, and A. C. Kot. Domain generalization with adversarial feature
learning. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, pages
5400–5409, 2018b.

Ya Li, Mingming Gong, Xinmei Tian, Tongliang Liu, and Dacheng Tao. Domain general-
ization via conditional invariant representations. In Thirty-Second AAAI Conference on
Artiﬁcial Intelligence, 2018c.

Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng
Tao. Deep domain generalization via conditional invariant adversarial networks. In Pro-
ceedings of the European Conference on Computer Vision (ECCV), pages 624–639, 2018d.

Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning
In COLT 2009 - The 22nd Conference on Learning Theory,

bounds and algorithms.
2009a.

52

Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation with
In Advances in neural information processing systems, pages 1041–

A. Maurer. Transfer bounds for linear feature learning. Machine Learning, 75(3):327–350,

multiple sources.
1048, 2009b.

2009.

A. Maurer, M. Pontil, and B. Romera-Paredes. Sparse coding for multitask and transfer
learning.
In Sanjoy Dasgupta and David McAllester, editors, Proceedings of the 30th
International Conference on Machine Learning, volume 28 of Proceedings of Machine
Learning Research, pages 343–351, 2013.

Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The beneﬁt of

multitask representation learning. J. Mach. Learn. Res., 17(1):2853–2884, 2016.

Aditya Krishna Menon, Brendan van Rooyen, and Nagarajan Natarajan. Learning from
binary labels with instance-dependent noise. Machine Learning, 107:1561–1595, 2018.

Saeid Motiian, Marco Piccirilli, Donald A Adjeroh, and Gianfranco Doretto. Uniﬁed deep
supervised domain adaptation and generalization. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision, pages 5715–5725, 2017.

K. Muandet, D. Balduzzi, and B. Sch¨olkopf. Domain generalization via invariant feature
representation. In Proceedings of the 30th International Conference on International Con-
ference on Machine Learning (ICML’13), volume 28 of Proceedings of Machine Learning
Research, pages I–10–I–18, 2013.

Nagarajan Natarajan, Inderjit S. Dhillon, Pradeep Ravikumar, and Ambuj Tewari. Cost-
sensitive learning with noisy labels. Journal of Machine Learning Research, 18(155):1–33,
2018. URL http://jmlr.org/papers/v18/15-226.html.

K. R. Parthasarathy. Probability Measures on Metric Spaces. Academic Press, 1967.

A. Pentina and C. Lampert. A pac-bayesian bound for lifelong learning. In Eric P. Xing
and Tony Jebara, editors, Proceedings of the 31st International Conference on Machine
Learning, volume 32 of Proceedings of Machine Learning Research, pages 991–999, 2014.

I.F. Pinelis and A.I. Sakhanenko. Remarks on inequalities for probabilities of large devia-

tions. Theory Probab. Appl., 30(1):143–148, 1985.

J. Quionero-Candela, M. Sugiyama, A. Schwaighofer, and N. D. Lawrence. Dataset Shift in

Machine Learning. The MIT Press, 2009.

A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in

neural information processing systems, pages 1177–1184, 2007.

Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random
features. In Advances in Neural Information Processing Systems, pages 3215–3225, 2017.

53

T. Sanderson and C. Scott. Class proportion estimation with application to multiclass
In Proceedings of the 17th International Conference on Artiﬁcial

anomaly rejection.
Intelligence and Statistics (AISTATS), 2014.

Clayton Scott. A generalized neyman-pearson criterion for optimal domain adaptation. In
Aur´elien Garivier and Satyen Kale, editors, Proceedings of the 30th International Con-
ference on Algorithmic Learning Theory, volume 98 of Proceedings of Machine Learning
Research, pages 738–761, 2019.

Shiv Shankar, Vihari Piratla, Soumen Chakrabarti, Siddhartha Chaudhuri, Preethi Jyothi,
In In-
and Sunita Sarawagi. Generalizing across domains via cross-gradient training.
ternational Conference on Learning Representations, 2018. URL https://openreview.
net/forum?id=r1Dx7fbCW.

S. Sharma and J. W. Cutler. Robust orbit determination and classiﬁcation: A learning

theoretic approach. Interplanetary Network Progress Report, 203:1, 2015.

B. Sriperumbudur, A. Gretton, K. Fukumizu, B. Sch¨olkopf, and G. Lanckriet. Hilbert space
embeddings and metrics on probability measures. Journal of Machine Learning Research,
11:1517–1561, 2010.

Bharath Sriperumbudur and Zolt´an Szab´o. Optimal rates for random fourier features. In

Advances in Neural Information Processing Systems, pages 1144–1152, 2015.

I. Steinwart and A. Christmann. Support Vector Machines. Springer, 2008.

Amos J Storkey. When training and test sets are diﬀerent: characterising learning transfer.

In In Dataset Shift in Machine Learning, pages 3–28. MIT Press, 2009.

Masashi Sugiyama, Taiji Suzuki, Shinichi Nakajima, Hisashi Kashima, Paul von B¨unau, and
Motoaki Kawanabe. Direct importance estimation for covariate shift adaptation. Annals
of the Institute of Statistical Mathematics, 60:699–746, 2008.

Dougal J. Sutherland and Jeﬀ Schneider. On the error of random Fourier features. In Pro-
ceedings of the Thirty-First Conference on Uncertainty in Artiﬁcial Intelligence, UAI’15,
pages 862–871. AUAI Press, 2015.

Zolt´an Szab´o, Bharath K Sriperumbudur, Barnab´as P´oczos, and Arthur Gretton. Learning
theory for distribution regression. The Journal of Machine Learning Research, 17(1):
5272–5311, 2016.

Dirk Tasche. Fisher consistency for prior probability shift. Journal of Machine Learning

Research, 18:1–32, 2017.

S. Thrun. Is learning the n-th thing any easier than learning the ﬁrst? Advances in Neural

Information Processing Systems, pages 640–646, 1996.

D. M. Titterington. Minimum distance non-parametric estimation of mixture proportions.

Journal of the Royal Statistical Society, 45(1):37–46, 1983.

54

J. Toedling, P. Rhein, R. Ratei, L. Karawajew, and R. Spang. Automated in-silico detection
of cell populations in ﬂow cytometry readouts and its application to leukemia disease
monitoring. BMC Bioinformatics, 7:282, 2006.

A. Tsanas, M. A. Little, P. E. McSharry, and L. O. Ramig. Accurate telemonitoring
IEEE transactions on

of parkinson’s disease progression by noninvasive speech tests.
Biomedical Engineering, 57(4):884–893, 2010.

Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun. Large
margin methods for structured and interdependent output variables. Journal of machine
learning research, 6(Sep):1453–1484, 2005.

Brendan van Rooyen and Robert C. Williamson. A theory of learning with corrupted labels.

Journal of Machine Learning Research, 18(228):1–50, 2018.

Haohan Wang, Zexue He, Zachary C. Lipton, and Eric P. Xing. Learning robust represen-
tations by projecting superﬁcial statistics out. In International Conference on Learning
Representations, 2019. URL https://openreview.net/forum?id=rJEjjoR9K7.

J. Wiens. Machine Learning for Patient-Adaptive Ectopic Beat Classication. Masters Thesis,
Department of Electrical Engineering and Computer Science, Massachusetts Institute of
Technology, 2010.

C. Williams and M. Seeger. Using the Nystr¨om method to speed up kernel machines. In
Proceedings of the 14th Annual Conference on Neural Information Processing Systems,
number EPFL-CONF-161322, pages 682–688, 2001.

Zheng Xu, Wen Li, Li Niu, and Dong Xu. Exploiting low-rank structure from latent domains
for domain generalization. In European Conference on Computer Vision, pages 628–643.
Springer, 2014.

Xiaolin Yang, Seyoung Kim, and Eric P Xing. Heterogeneous multitask learning with joint
sparsity constraints. In Advances in neural information processing systems, pages 2151–
2159, 2009.

Yao-Liang Yu and Csaba Szepesvari. Analysis of kernel mean matching under covariate
shift. In Proceedings of the 29th International Conference on Machine Learning, pages
607–614, 2012.

Bianca Zadrozny. Learning and evaluating classiﬁers under sample selection bias. In Pro-

ceedings of the Twenty-ﬁrst International Conference on Machine Learning, 2004.

Kun Zhang, Bernhard Sch¨olkopf, Krikamol Muandet, and Zhikun Wang. Domain adapta-
tion under target and conditional shift. In International Conference on Machine Learning,
pages 819–827, 2013.

Kun Zhang, Mingming Gong, and Bernhard Scholkopf. Multi-source domain adaptation:
A causal view. In Proceedings of the Twenty-Ninth AAAI Conference on Artiﬁcial Intel-
ligence, AAAI’15, pages 3150–3157. AAAI Press, 2015.

55

0
2
0
2
 
r
p
A
 
7
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
0
1
9
7
0
.
1
1
7
1
:
v
i
X
r
a

Domain Generalization by Marginal Transfer Learning

Gilles Blanchard
Institut f¨ur Mathematik
Universit¨at Potsdam

Aniket Anand Deshmukh
Microsoft AI & Research
¨Urun Dogan
Microsoft AI & Research

Gyemin Lee
Dept. Electronic and IT Media Engineering
Seoul National University of Science and Technology

Clayton Scott
Electrical and Computer Engineering
University of Michigan

blanchard@math.uni-potsdam.de

aniketde@umich.edu

urundogan@gmail.com

gyemin@seoultech.ac.kr

clayscot@umich.edu

Abstract
In the problem of domain generalization (DG), there are labeled training data sets from
several related prediction problems, and the goal is to make accurate predictions on future
unlabeled data sets that are not known to the learner. This problem arises in several ap-
plications where data distributions ﬂuctuate because of environmental, technical, or other
sources of variation. We introduce a formal framework for DG, and argue that it can be
viewed as a kind of supervised learning problem by augmenting the original feature space
with the marginal distribution of feature vectors. While our framework has several con-
nections to conventional analysis of supervised learning algorithms, several unique aspects
of DG require new methods of analysis.

This work lays the learning theoretic foundations of domain generalization, building on
our earlier conference paper where the problem of DG was introduced (Blanchard et al.,
2011). We present two formal models of data generation, corresponding notions of risk, and
distribution-free generalization error analysis. By focusing our attention on kernel meth-
ods, we also provide more quantitative results and a universally consistent algorithm. An
eﬃcient implementation is provided for this algorithm, which is experimentally compared
to a pooling strategy on one synthetic and three real-world data sets.
Keywords: Domain Generalization, Generalization Error Bounds, Kernel Methods, Uni-
versal Consistency, Kernel Approximation

1. Introduction

Domain generalization (DG) is a machine learning problem where the learner has access to
labeled training data sets from several related prediction problems, and must generalize to
a future prediction problem for which no labeled data are available. In more detail, there
are N labeled training data sets Si = (Xij, Yij)1≤j≤ni, i = 1, . . . , N , that describe similar
but possibly distinct prediction tasks. The objective is to learn a rule that takes as input

1

a previously unseen unlabeled test data set X T
for these or possibly other unlabeled points from the associated learning task.

1 , . . . , X T
nT

, and accurately predicts outcomes

DG arises in several applications. One prominent example is precision medicine, where
a common objective is to design a patient-speciﬁc classiﬁer (e.g., of health status) based
on clinical measurements, such as an electrocardiogram or electroencephalogram. In such
measurements, patient-to-patient variation is common, arising from biological variations
between patients, or technical or environmental factors inﬂuencing data acquisition. Be-
cause of patient-to-patient variation, a classiﬁer that is trained on data from one patient
may not be well matched to another patient. In this context, domain generalization enables
the transfer of knowledge from historical patients (for whom labeled data are available) to a
new patient without the need to acquire training labels for that patient. A detailed example
in the context of ﬂow cytometry is given below.

We view domain generalization as a conventional supervised learning problem where
the original feature space is augmented to include the marginal distribution generating the
features. We refer to this reframing of DG as “marginal transfer learning,” because it reﬂects
the fact that in DG, information about the test task must be drawn from that tasks’ marginal
feature distribution. Leveraging this perspective, we formulate two statistical frameworks
for analyzing DG. The ﬁrst framework allows the observations within each data set to
have arbitrary dependency structure, and makes connections to the literature on Campbell
measures and structured prediction. The second framework is a special case of the ﬁrst,
assuming the data points are drawn iid within each task, and allows for a more reﬁned risk
analysis.

We further develop a distribution-free kernel machine that employs a kernel on the
aforementioned augmented feature space. Our methodology is shown to yield a universally
consistent learning procedure under both statistical frameworks, meaning that the domain
generalization risk tends to the best possible value as the relevant sample sizes tend inﬁnity,
with no assumptions on the data generating distributions. Although DG may be viewed as
a conventional supervised learning problem on an augmented feature space, the analysis is
nontrivial owing to unique aspects of the sampling plans and risks. We oﬀer a computation-
ally eﬃcient and freely available1 implementation of our algorithm, and present a thorough
experimental study validating the proposed approach on one synthetic and three real-world
data sets, including comparisons to a simple pooling approach.

To our knowledge, the problem of domain generalization was ﬁrst proposed and studied
by our earlier conference publication (Blanchard et al., 2011) which this work extends
in several ways.
It adds (1) a new statistical framework, the agnostic generative model
described below; (2) generalization error and consistency results for the new statistical
model; (3) an extensive literature review; (4) an extension to the regression setting in
both theory and experiments; (5) a more general statistical analysis, in particular, we no
longer assume a bounded loss, and therefore accommodate common convex losses such as
the hinge and logistic losses; (6) extensive experiments (the conference paper considered a
single small dataset); (7) a scalable implementation based on a novel extension of random
Fourier features; and (8) error analysis for the random Fourier features approximation.

1. https://github.com/aniketde/DomainGeneralizationMarginal

2

2. Motivating Application: Automatic Gating of Flow Cytometry Data

Flow cytometry is a high-throughput measurement platform that is an important clinical
tool for the diagnosis of blood-related pathologies. This technology allows for quantitative
analysis of individual cells from a given cell population, derived for example from a blood
sample from a patient. We may think of a ﬂow cytometry data set as a set of d-dimensional
attribute vectors (Xj)1≤j≤n, where n is the number of cells analyzed, and d is the number
of attributes recorded per cell. These attributes pertain to various physical and chemical
properties of the cell. Thus, a ﬂow cytometry data set may be viewed as a random sample
from a patient-speciﬁc distribution.

Now suppose a pathologist needs to analyze a new (test) patient with data (X T

j )1≤j≤nT .
Before proceeding, the pathologist ﬁrst needs the data set to be “puriﬁed” so that only cells
of a certain type are present. For example, lymphocytes are known to be relevant for the
diagnosis of leukemia, whereas non-lymphocytes may potentially confound the analysis. In
other words, it is necessary to determine the label Y T
j ∈ {−1, 1} associated to each cell,
where Y T

j = 1 indicates that the j-th cell is of the desired type.

In clinical practice this is accomplished through a manual process known as “gating.”
The data are visualized through a sequence of two-dimensional scatter plots, where at each
stage a line segment or polygon is manually drawn to eliminate a portion of the unwanted
cells. Because of the variability in ﬂow cytometry data, this process is diﬃcult to quantify
in terms of a small subset of simple rules. Instead, it requires domain-speciﬁc knowledge
and iterative reﬁnement. Modern clinical laboratories routinely see dozens of cases per day,
so it would be desirable to automate this process.

Since clinical laboratories maintain historical databases, we can assume access to a
number (N ) of historical (training) patients that have already been expert-gated. Because
of biological and technical variations in ﬂow cytometry data, the distributions P (i)
XY of
the historical patients will vary. To illustrate the ﬂow cytometry gating problem, we use
the NDD data set from the FlowCap-I challenge.2 For example, Fig. 1 shows exemplary
two-dimensional scatter plots for two diﬀerent patients – see caption for details. Despite
diﬀerences in the two distributions, there are also general trends that hold for all patients.
Virtually every cell type of interest has a known tendency (e.g., high or low) for most
measured attributes. Therefore, it is reasonable to assume that there is an underlying
distribution (on distributions) governing ﬂow cytometry data sets, that produces roughly
similar distributions thereby making possible the automation of the gating process.

3. Formal Setting and General Results

In this section we formally deﬁne domain generalization via two possible data generation
models together with associated notions of risk. We also provide a basic generalization error
bound for the ﬁrst of these data generation models.

Let X denote the observation space (assumed to be a Radon space) and Y ⊆ R the
output space. Let PX and PX ×Y denote the set of probability distributions on X and X ×Y,
respectively. The spaces PX and PX ×Y are endowed with the topology of weak convergence
and the associated Borel σ-algebras. The symbol ⊗ indicates a product measure.

2. We will revisit this data set in Section 8.5 where details are given.

3

Figure 1: Two-dimensional projections of multi-dimensional ﬂow cytometry data. Each row
corresponds to a single patient, and each column to a particular two-dimensional projection.
The distribution of cells diﬀers from patient to patient. The colors indicate the results of
gating, where a particular type of cell, marked dark (blue), is separated from all other cells,
marked bright (red). Labels were manually selected by a domain expert.

The disintegration theorem for joint probability distributions (see for instance Kallen-
berg, 2002, Theorem 6.4) tells us that (under suitable regularity properties, satisﬁed if X is
a Radon space) any element PXY ∈ PX ×Y can be written as a Markov semi-direct product
PXY = PX • PY |X , with PX ∈ PX , PY |X ∈ PY |X , where PY |X is the space of conditional
probability distributions of Y given X, also called Markov transition kernels from X to Y.
This speciﬁcally means that

E(X,Y )∼PXY [h(X, Y )] =

h(x, y)PY |X (dy|X = x)

PX (dx),

(1)

(cid:90) (cid:18)(cid:90)

(cid:19)

for any integrable function h : X × Y → R. Following common terminology in the statistical
learning literature, we will also call PY |X the posterior distribution (of Y given X).

We assume that N training samples Si = (Xij, Yij)1≤j≤ni, i = 1, . . . , N , are observed.
To allow for possibly unequal sample sizes ni, it is convenient to formally identify each
sample Si with its associated empirical distribution (cid:98)P (i)
j=1 δ(Xij ,Yij ) ∈ PX ×Y .
We assume that the ordering of the observations inside a given sample Si is arbitrary
and does not contain any relevant information. We also denote by (cid:98)P (i)
j=1 δXij ∈
PX the ith training sample without labels. Similarly, a test sample is denoted by ST =
(X T

j )1≤j≤nT , and the empirical distribution of the unlabeled data by (cid:98)P T
X .

XY = 1
ni

X = 1
ni

j , Y T

(cid:80)ni

(cid:80)ni

3.1 Data Generation Models

We propose two data generation models. The ﬁrst is most general, and includes the second
as a special case.

4

Assumption 1 (AGM) There exists a distribution PS on PX ×Y such that S1, . . . , SN are
i.i.d. realizations from PS.

We call this the agnostic generative model. This is a quite general model in which samples are
assumed to be identically distributed and independent of each other, but nothing particular
is assumed about the generation mechanism for observations inside a given sample, nor for
the (random) sample size.

We also introduce a more speciﬁc generative mechanism, where observations (Xij, Yij)
XY , a latent unobserved random distribu-

inside the sample Si are themselves i.i.d. from P (i)
tion, as follows.

Assumption 2 (2SGM) There exists a distribution µ on PX ×Y and a distribution ν on
N, such that (P (1)
XY , nN ) are i.i.d. realizations from µ ⊗ ν, and conditional to
(P (i)
XY , ni) the sample Si is made of ni i.i.d. realizations of (X, Y ) following the distribution
P (i)
XY .

XY , n1), . . . , (P (N )

This model, called the 2-stage generative model, is a subcase of (AGM): since the
(P (i)
It has been considered in the distinct but
XY , ni) are i.i.d., the samples Si also are.
related context of “learning to learn” (Baxter, 2000; see also a more detailed discussion
below, Section 4.2). Many of our results will hold for the agnostic generative model, but
the two-stage generative model allows for additional developments, as will be discussed
further below. This model was the one studied in our conference paper (Blanchard et al.,
2011).

3.2 Decision Functions and Augmented Feature Space

In domain generalization, the learner’s goal is to infer from the training data a general
rule that takes an arbitrary, previously unseen, unlabeled dataset corresponding to a new
prediction task, and produces a classiﬁer for that prediction task that could be applied to
any x (possibly outside the unlabeled data set). In other words, the learner should out-
put a mapping g : PX → (X → R). Equivalently, the learner should output a function
f : PX × X → R, where the two notations are related via g(PX )(x) = f (PX , x). In the
latter viewpoint, f may be viewed as a standard decision function on the “augmented”
or “extended” feature space PX × X , which facilitates connections to standard supervised
learning. We refer to this view of DG as marginal transfer learning, because the informa-
tion that facilitates generalization to a new task is conveyed entirely through the marginal
distribution. In the next two subsections, we present two deﬁnitions of the risk of a decision
function f , one associated to each of the two data generation models.

3.3 Risk and Generalization Error Bound under the Agnostic Generative

Model

Now consider a test sample ST = (X T
j )1≤j≤nT , whose labels are not observed by the
learner. If (cid:96) : R×Y (cid:55)→ R+ is a loss function for a single prediction, and predictions of a ﬁxed
decision function f on the test sample are given by (cid:98)Y T
j ), then the empirical

j = f ( (cid:98)P T

j , Y T

X , X T

5

average loss incurred on the test sample is

L(ST , f ) :=

(cid:96)( (cid:98)Y T

j , Y T

j ) .

1
nT

nT(cid:88)

j=1

Based on this, we deﬁne the risk of a decision function as the average of the above quantity
when test samples are drawn according to the same mechanism as the training samples:

E(f ) := E

ST ∼PS

(cid:2)L(ST , f )(cid:3) = E

ST ∼PS

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:96)(f ( (cid:98)P T

X , X T

(cid:35)
i ), Y T
i )

.

In a similar way, we deﬁne the empirical risk of a decision function as its average prediction
error over the training samples:

(cid:98)E(f, N ) :=

L(Si, f ) =

(cid:96)(f ( (cid:98)P (i)

X , Xij), Yij).

(2)

1
N

N
(cid:88)

i=1

1
N

N
(cid:88)

ni(cid:88)

1
ni

i=1

j=1

Remark 3 It is possible to understand the above setting as a particular instance of a struc-
tured output learning problem (Tsochantaridis et al., 2005; Bakır et al., 2007), in which
the input variable X ∗ is (cid:98)P T
X , and the “structured output” Y ∗ is the collection of labels
(Y T
i )1≤i≤nT (matched to their respective input points). As is generally the case for struc-
tured output learning, the nature of the problem and the “structure” of the outputs is very
much encoded in the particular form of the loss function. In our setting the loss function is
additive over the labels forming the collection Y ∗, and we will exploit this particular form
for our method and analysis.

Remark 4 The risk E(f ) deﬁned above can be described in the following way: consider the
random variable ξ := ( (cid:98)PXY ; (X, Y )) obtained by ﬁrst drawing (cid:98)PXY according to PS, then,
conditional to this, drawing (X, Y ) according to (cid:98)PXY . The risk is then the expectation of a
certain function of ξ (namely Ff (ξ) = (cid:96)(f ( (cid:98)PX , X), Y )). In probability theory literature, the
distribution of the variable ξ is known as the Campbell measure associated to the distribution
PS over the measure space PX ×Y ; this object is in particular of fundamental use in point
process theory (see, e.g., Daley and Vere-Jones (2008), Section 13.1). We will denote
it by C(PS) here. This intriguing connection suggests that more elaborate tools of point
process literature may ﬁnd their use to analyze DG when various classical point processes
are considered for the generating distribution. The Campbell measure will also appear in
the Rademacher analysis below.

The next result establishes an analogue of classical Rademacher analysis under the

agnostic generative model.

Theorem 5 (Uniform estimation error control under (AGM)) Let F be a class of
decision functions PX × X → R. Assume the following boundedness condition holds:

sup
f ∈F

sup
PX ∈PX

sup
(x,y)∈X ×Y

(cid:96)(f (PX , x), y) ≤ B(cid:96).

(3)

6

Under (AGM), if S1, . . . , SN are i.i.d. realizations from PS, then with probability at least
1 − δ with respect to the draws of the training samples:

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:98)E(f, N ) − E(f )
(cid:12)

sup
f ∈F

≤

E

2
N

( (cid:98)P (i)

XY ;(Xi,Yi))∼C(PS )⊗N

E(εi)1≤i≤N

(cid:34)

sup
f ∈F

(cid:12)
N
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)

i=1

εi(cid:96)(f ( (cid:98)P (i)

(cid:35)

(cid:12)
(cid:12)
(cid:12)
X , Xi), Yi)
(cid:12)
(cid:12)

(cid:114)

+ B(cid:96)

log(δ−1)
2N

,

(4)

where (εi)1≤i≤N are i.i.d. Rademacher variables, independent from ( (cid:98)P (i)

XY , (Xi, Yi))1≤i≤N .

(cid:12)
(cid:12)
(cid:12)
(cid:12)
Proof Since the (Si)1≤i≤N are i.i.d., supf ∈F
(cid:12) takes the form of a uniform
(cid:12) (cid:98)E(f, N ) − E(f )
deviation between average and expected loss over the function class F. We can therefore
apply standard analysis (Azuma-McDiarmid inequality followed by Rademacher complexity
analysis for a nonnegative bounded loss; see, e.g., Koltchinskii, 2001; Bartlett and Mendel-
son, 2002, Theorem 8) to obtain that with probability at least 1 − δ with respect to the
draw of the training samples (Si)1≤i≤N :

(cid:12)
(cid:12)
(cid:12) (cid:98)E(f, N ) − E(f )

(cid:12)
(cid:12)
(cid:12) ≤

2
N

sup
f ∈F

E(Si)1≤i≤N

E(εi)1≤i≤N

(cid:34)

sup
f ∈F

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

N
(cid:88)

i=1

(cid:35)

(cid:12)
(cid:12)
(cid:12)
εiL(Si, f )
(cid:12)
(cid:12)

(cid:114)

+ B(cid:96)

log(δ−1)
2N

,

where (εi)1≤i≤N are i.i.d. Rademacher variables, independent of (Si)1≤i≤N .

We may write

L(Si, f ) =

1
ni

ni(cid:88)

j=1

(cid:96)(f ( (cid:98)P (i)

X , Xij), Yij) = E

(X,Y )∼ (cid:98)P (i)
XY

(cid:104)

(cid:96)(f ( (cid:98)P (i)

(cid:105)
X , X), Y )

;

thus, we have

E(Si)1≤i≤N

E(εi)1≤i≤N

εiL(Si, f )

(cid:34)

sup
f ∈F

(cid:12)
N
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)

i=1
(cid:34)

(cid:12)
(cid:35)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= E

( (cid:98)P (i)

XY )1≤i≤N

E(εi)1≤i≤N

(cid:12)
N
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)

i=1

sup
f ∈F

εiE

(Xi,Yi)∼ (cid:98)P (i)
XY

(cid:104)
(cid:96)(f ( (cid:98)P (i)

(cid:105)
X , Xi), Yi)

(cid:12)
(cid:35)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ E

E

( (cid:98)P (i)

XY )1≤i≤N

(X1,Y1)∼ (cid:98)P (1)

XY ,...,(XN ,YN )∼ (cid:98)P (N )

XY

E(εi)1≤i≤N

(cid:34)

sup
f ∈F

(cid:12)
N
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)

i=1

εi(cid:96)(f ( (cid:98)P (i)

(cid:35)

(cid:12)
(cid:12)
(cid:12)
X , Xi), Yi)
(cid:12)
(cid:12)

.

In the above inequality, the inner expectation on the (Xi, Yi) is pulled outwards by Jensen’s
inequality and convexity of the supremum.

To obtain the announced estimate, notice that the above expectation is the same as the

expectation w.r.t. to the N -fold Campbell measure C(PS) (see Remark 4).

Remark 6 The main term in the theorem is just the conventional Rademacher complexity
for the augmented feature space PX × X endowed with the Campbell measure C(PS). It
could also be thought of as the Rademacher complexity for the meta-distribution PS.

7

3.4 Risk under the 2-stage generative model

While we will state more results holding under (AGM) below, one advantage of the more
speciﬁc (2SGM) is to allow us to study the eﬀect of the sample sizes ni. In particular, for
the purpose of reducing computational complexity, we can analyze the eﬀect of subsampling
observations inside a given sample. Such an analysis would not be possible under (AGM).
For the test sample, parallel to the training data generating mechanism, under (2SGM)
XY , nT ) is drawn according to µ⊗ν, and conditional to this the test sample

we assume that (P T
ST is drawn from nT i.i.d. realizations of P T

Observe that under (2SGM), the distribution P (i)

XY is not observed and is therefore a
latent variable at training time as well as at test time. Still, by the law of large numbers,
if nT becomes large, (cid:98)P T
X (in the sense of weak convergence). This
motivates the introduction of the following risk which assumes access to an inﬁnite test
sample, and thus the true marginal P T
X :
E

X will converge to P T

(cid:2)(cid:96)(f (P T

X , X T ), Y T )(cid:3) .

E ∞(f ) := E

(X T ,Y T )∼P T

(5)

XY .

P T

XY ∼µ

XY

This quantity only makes sense under (2SGM). The following proposition makes the above
motivating observation more precise. First, under (2SGM), introduce the following risk
conditional to a ﬁnite test sample size nT :

E(f |nT ) = E

E

P T

XY ∼µ

(X T

i ,Y T

i )1≤i≤nT

∼(P T

XY )⊗nT

(cid:96)(f ( (cid:98)P T

X , X T

i ), Y T
i )

.

(6)

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:35)

Proposition 7 Assume (cid:96) is a bounded, L-Lipschitz loss function and f : PX × X → R is
a ﬁxed decision function which is continuous with respect to both its arguments (recalling
PX is endowed with the weak convergence topology). Then it holds under (2GSM):

lim
nT →∞

E(f |nT ) = E ∞(f ).

Remark 8 This result provides one setting where the risk E ∞ is clearly motivated as the
goal of asymptotic analysis when nT → ∞. Although Proposition 7 is not used elsewhere in
this work, a more quantitative version of this result is stated below for kernels (see Theorem
15), where convergence holds uniformly and the assumption of a bounded loss is dropped.

To gain more insight into the risk E ∞, recalling the standard decomposition (1) of
PXY into the marginal PX and the posterior PY |X , we observe that we can apply the
disintegration theorem not only to any PXY , but also to µ, and thus decompose it into two
parts, µX which generates the marginal distribution PX , and µY |X which, conditioned on
PX , generates the posterior PY |X . (More precise notation might be µPX instead of µX and
µPY |X |PX instead of µY |X , but this is rather cumbersome.) Denote (cid:101)X = (PX , X). We then
have, using Fubini’s theorem,

E ∞(f ) = EPX ∼µX

EPY |X ∼µY |X

EX∼PX

EY ∼PY |X

= EPX ∼µX

EPY |X ∼µY |X

EY ∼PY |X

EX∼PX
(cid:104)

= E

( (cid:101)X,Y )∼Qµ

(cid:96)(f ( (cid:101)X), Y )

.

(cid:105)

(cid:104)
(cid:96)(f ( (cid:101)X), Y )
(cid:104)
(cid:96)(f ( (cid:101)X), Y )

(cid:105)

(cid:105)

8

Here Qµ is the distribution that generates (cid:101)X by ﬁrst drawing PX according to µX , and then
drawing X according to PX ; similarly, Y is generated, conditioned on (cid:101)X, by ﬁrst drawing
PY |X according to µY |X , and then drawing Y from PY |X . (The distribution of (cid:101)X again
takes the form of a Campbell measure, see Remark 4.)

From the previous expression, we see that the risk E ∞ is like a standard supervised
learning risk based on ( (cid:101)X, Y ) ∼ Qµ. Thus, we can deduce properties that are known
to hold for supervised learning risks. For example, in the binary classiﬁcation setting, if
the loss is the 0/1 loss, then f ∗( (cid:101)X) = 2˜η( (cid:101)X) − 1 is an optimal predictor, where ˜η( (cid:101)X) =
E

(cid:2)1{Y =1}

(cid:3), and

Y ∼Qµ

Y | (cid:101)X

E ∞(f ) − E ∞(f ∗) = E

(cid:101)X∼Qµ
(cid:101)X

(cid:105)
(cid:104)
1{sign(f ( (cid:101)X))(cid:54)=sign(f ∗( (cid:101)X))}|2˜η( (cid:101)X) − 1|

.

Furthermore, consistency in the sense of E ∞ with respect to a general loss (cid:96) (thought of as
a surrogate) will imply consistency for the 0/1 loss, provided (cid:96) is classiﬁcation calibrated
(Bartlett et al., 2006).

X . There is, however, a condition where for µ-almost all test distributions P T

For a given loss (cid:96), the optimal or Bayes E ∞-risk in DG is in general larger than the
expected Bayes risk under the (random) test sample generating distribution P T
XY , because
it is typically not possible to fully determine the Bayes-optimal predictor from only the
marginal P T
XY ,
the decision function f ∗(P T
X , .) (where f ∗ is a global minimizer of (5)) coincides with an
optimal Bayes decision function for P T
XY . This condition is simply that the posterior PY |X
is (µ-almost surely) a function of PX (in other terms: that with the notation introduced
above, µY |X (PX ) is a Dirac measure for µ-almost all PX ). Although we will not be assuming
this condition throughout the paper under (2SGM), observe that it is implicitly assumed
in the motivating application presented in Section 2, where an expert labels the data points
by just looking at their marginal distribution.

Lemma 9 For a ﬁxed distribution PXY , and a decision function g : X → R, let us denote
R(g, PXY ) = E(X,Y )∼PXY [(cid:96)(g(X), Y )] and

R∗(PXY ) := min
g:X →R

R(g, PXY ) = min
g:X →R

E(X,Y )∼PXY [(cid:96)(g(X), Y )]

the corresponding optimal (Bayes) risk for the loss function (cid:96) under data distribution PXY .
Then under (2SGM):

E ∞(f ∗) ≥ EPXY ∼µ [R∗(PXY )] ,

where f ∗ : PX × X → R is a minimizer of the idealized DG risk E ∞ deﬁned in (5).

Furthermore, if µ is a distribution on PX ×Y such that µ-a.s. it holds PY |X = F (PX )

for some deterministic mapping F , then for µ-almost all PXY :

and

R(f ∗(PX , .), PXY ) = R∗(PXY )

E ∞(f ∗) = EPXY ∼µ [R∗(PXY )] .

9

Proof For any f : PX × X → R, one has for all PXY : R(f (PX , .), PXY ) ≥ R∗(PXY ).
Taking expectation with respect to PXY establishes the ﬁrst claim. Now for any ﬁxed
PX ∈ PX , consider PXY := PX • F (PX ) and g∗(PX ) a Bayes decision function for this joint
distribution. Pose f (PX , x) := g∗(PX )(x). Then f coincides for µ-almost all PXY with a
Bayes decision function for PXY , achieving equality in the above inequality. The second
equality follows by taking expectation over PXY ∼ µ.

Under (2SGM), we will establish that our proposed learning algorithm is E ∞-consistent,
provided the average sample size grows to inﬁnity as well as the total number of samples.
Thus, the above result provides a condition on µ under which it is possible to asymptotically
attain the Bayes risk on any test distribution although no labels from this test distribution
are observed.

More generally, and speaking informally, if µ is such that PY |X is close to being a function
of PX in some sense, we can expect the Bayes E ∞ risk for domain generalization to be close
to the expected Bayes risk for a random test distribution. We reiterate, however, that we
make no assumptions on µ in this work so that the two quantities may be far apart. In
the worst case, the posterior may be independent of the marginal, in which case a method
for domain generalization will do no better than the na¨ıve pooling strategy. For further
discussion, see the comparison of domain adaptation and domain generalization in the next
section.

4. Related Work

Since at least the 1990s, machine learning researchers have investigated the possibility of
solving one learning problem by leveraging data from one or more related problems. In this
section, we provide an overview of such problems and their relation to domain generalization,
while also reviewing prior work on DG.

Two critical terms are domain and task. Use of these terms is not consistent throughout
the literature, but at a minimum, the domain of a learning problem describes the input
(feature) space X and marginal distribution of X, while the task describes the output space
Y and the conditional distribution of Y given X (also called posterior). In many settings,
however, the sets X and Y are the same for all learning problems, and the terms “domain”
and “task” are used interchangeably to refer to a joint distribution PXY on X × Y. This is
the perspective adopted in this work, as well as in much of the work on multi-task learning,
domain adaptation (DA), and domain generalization.

Multi-task learning is similar to DG, except only the training tasks are of interest,
and the goal is to leverage the similarity among distributions to improve the learning of
individual predictors for each task (Caruana, 1997; Evgeniou et al., 2005; Yang et al., 2009).
In contrast, in DG, we are concerned with generalization to a new task.

Domain adaptation refers to the setting in which there is a speciﬁc target task and one
or more source tasks. The goal is to design a predictor for the target task, for which there
are typically few to no labeled training examples, by leveraging labeled training data from
the source task(s). DA is reviewed below, and contrasted with DG.

10

4.1 Domain Generalization vs. Domain Adaptation

Formulations of domain adaptation may take several forms, depending on the number of
sources and whether there are any labeled examples from the target to supplement the
unlabeled examples. In multi-source, unsupervised domain adaptation, the learner is pre-
sented with labeled training data from several source distributions, and unlabeled data from
a target marginal distribution (see Zhang et al. (2015) and references therein). Thus, the
available data are the same as in domain generalization, and algorithms for one of these
problems may be applied to the other. In this section we illuminate the diﬀerence between
DA and DG.

In all forms of DA, the goal is to attain optimal performance with respect to the joint
distribution of the target domain. For example, if the performance measure is a risk, the goal
is to attain the Bayes risk for the target domain. To achieve this goal, it is necessary to make
assumptions about how the source and target distributions are related (Quionero-Candela
et al., 2009). For example, several works adopt the covariate shift assumption, which
requires the source and target domains to have the same posterior, allowing the marginals
to diﬀer arbitrarily (Zadrozny, 2004; Huang et al., 2007; Cortes et al., 2008; Sugiyama
et al., 2008; Bickel et al., 2009; Kanamori et al., 2009; Yu and Szepesvari, 2012; Ben-David
and Urner, 2012). Another common assumption is target shift, which stipulates that the
source and target have the same class-conditional distributions, allowing the prior class
probability to change (Hall, 1981; Titterington, 1983; Latinne et al., 2001; Storkey, 2009;
Du Plessis and Sugiyama, 2012; Sanderson and Scott, 2014; Azizzadenesheli et al., 2019).
Mansour et al. (2009b); Zhang et al. (2015) assume that the target posterior is a weighted
combination of source posteriors, while Zhang et al. (2013); Gong et al. (2016) extend target
shift by also allowing the class-conditional distributions to undergo a location-scale shift,
and Tasche (2017) assumes the ratio of class-conditional distributions is unchanged. Work
on classiﬁcation with label noise assumes the source data are obtained from the target
distribution but the labels have been corrupted in either a label-dependent (Blanchard
et al., 2016; Natarajan et al., 2018; van Rooyen and Williamson, 2018) or feature-dependent
(Menon et al., 2018; Cannings et al., 2018; Scott, 2019) way. Finally, there are several works
that assume the existence of a predictor that achieves good performance on both source and
target domains (Ben-David et al., 2007, 2010; Blitzer et al., 2008; Mansour et al., 2009a;
Cortes et al., 2015; Germain et al., 2016).

The key diﬀerence between DG and DA may be found in the performance measures
optimized. In DG, the goal is to design a single predictor f (PX , x) that can apply to any
future task, and risk is assessed with respect to the draw of both a new task, and (under
2SGM) a new data point from that task. This is in contrast to DA, where the target
distribution is typically considered ﬁxed, and the goal is to design a predictor f (x) where, in
assessing the risk, the only randomness is in the draw of a new sample from the target task.
This diﬀerence in performance measures for DG and DA has an interesting consequence
for analysis. As we will show, it is possible to attain optimal risk (asymptotically) in DG
without making any distributional assumptions like those described above for DA. Of course,
this optimal risk is typically larger than the Bayes risk for any particular target domain
(see Lemma 9). An interesting question for future research is whether it is possible to

11

close or eliminate this gap (between DG and expected DA risks) by imposing distributional
assumptions like those for DA.

Another diﬀerence between DA and DG lies in whether the learning algorithm must be
rerun for each new test data set. Most unsupervised DA methods employ the unlabeled
target data for training and thus, when a new unlabeled target data set is presented, the
learning algorithm must be rerun. In contrast, most existing DG methods do not assume
access to the unlabeled test data at learning time, and are capable of making predictions
as new unlabeled data sets arrive without any further training.

4.2 Domain Generalization vs. Learning to Learn

In the problem of learning to learn (LTL, Thrun, 1996), which has also been called bias
learning, meta-learning, and (typically in an online setting) lifelong learning, there are la-
beled datasets for several tasks, as in DG. There is also a given family of learning algorithms,
and the objective is to design a meta-learner that selects the learning algorithm that will
perform best on future tasks. The learning theoretic study of LTL traces to the work of
Baxter (2000), who was the ﬁrst to propose a distribution on tasks, which he calls an “en-
vironment,” and which coincides with our µ. Given this setting, the performance of the
learning algorithm selected by a meta-learner is obtained by drawing a new task at random,
drawing a labeled training dataset from that task, running the selected algorithm, drawing
a test point, and evaluating the expected loss, where the expectation is with respect to all
sources of randomness (new task, training data from new task, test point from new task).

Baxter analyzes learning algorithms given by usual empirical risk minimization over a
hypothesis (prediction function) class, and the goal of the meta-learner is then to select a
hypothesis class from a family of such classes. He shows that it is possible to ﬁnd a good
trade-oﬀ between the complexity of a hypothesis class and its approximation capabilities
for tasks sampled from µ, in an average sense. In particular, the information gained by
ﬁnding a well-adapted hypothesis class can lead to signiﬁcantly improved sample eﬃciency
when learning a new task. See Maurer (2009) for a discussion of the performance measure
studied by Baxter (2000), which is slightly diﬀerent from the one described above.

Later work on LTL establishes similar results that quantify the ability of a meta-learner
to transfer knowledge to a new task. These meta-learners all optimize a particular structure
that deﬁnes a learning algorithm, such as a feature representation (Maurer, 2009; Maurer
et al., 2016; Denevi et al., 2018a), a prior on predictors in a PAC-Bayesian setting (Pentina
and Lampert, 2014), a dictionary (Maurer et al., 2013), the bias of a regularizer (Denevi
et al., 2018b), and a pretrained neural network (Finn et al., 2017). It is also worth not-
ing that some algorithms on multi-task learning extract structures that characterize an
environment and can be applied to LTL.

Although DG and LTL both involve generalization to a new task, they are clearly
diﬀerent problems because LTL assumes access to labeled data from the new task, whereas
DG only sees unlabeled data and requires no additional learning. In LTL, the learner can
achieve the Bayes risk for the new task, the only issue is the sample complexity. DG is
thus a more challenging problem, but also potentially more useful since in many transfer
learning settings, labeled data for the new task are unavailable.

12

4.3 Prior Work on Domain Generalization

To our knowledge, the ﬁrst paper to consider domain generalization (as formulated in Sec-
tion 3.2) was our earlier conference paper (Blanchard et al., 2011). The term “domain
generalization” was coined by Muandet et al. (2013), who study the same setting and build
upon our work by extracting features that facilitate DG. Carbonell et al. (2013) study an
active learning variant of DG in the realizable setting, and directly learn the task sampling
distribution.

Other methods for DG were studied by Khosla et al. (2012); Xu et al. (2014); Grubinger
et al. (2015); Ghifary et al. (2015); Gan et al. (2016); Ghifary et al. (2017); Motiian et al.
(2017); Li et al. (2017, 2018a,b,c,d); Balaji et al. (2018); Ding and Fu (2018); Shankar
et al. (2018); Hu et al. (2019); Dou et al. (2019); Carlucci et al. (2019); Wang et al. (2019);
Akuzawa et al. (2019). Many of these methods learn a common feature space for all tasks.
Such methods are complementary to the method that we study. Indeed, our kernel-based
learning algorithm may be applied after having learned a feature representation by another
method, as was done by Muandet et al. (2013). Since our interest is primarily theoretical,
we restrict our experimental comparison to another algorithm that also operates directly on
the original input space, namely, a simple pooling algorithm that lumps all training tasks
into a single data set and trains a single support vector machine.

5. Learning Algorithm

In this section, we introduce a concrete algorithm to tackle the learning problem exposed in
Section 3, using an approach based on kernels. The function k : Ω×Ω → R is called a kernel
on Ω if the matrix (k(xi, xj))1≤i,j≤n is symmetric and positive semi-deﬁnite for all positive
integers n and all x1, . . . , xn ∈ Ω. It is well known that every kernel k on Ω is associated to
a space of functions f : Ω → R called the reproducing kernel Hilbert space (RKHS) Hk with
kernel k. One way to envision Hk is as follows. Deﬁne Φ(x) := k(·, x), which is called the
canonical feature map associated with k. Then the span of {Φ(x) : x ∈ Ω}, endowed with
the inner product (cid:104)Φ(x), Φ(x(cid:48))(cid:105) = k(x, x(cid:48)), is dense in Hk. We also recall the reproducing
property, which states that (cid:104)f, Φ(x)(cid:105) = f (x) for all f ∈ Hk and x ∈ Ω.

For later use, we introduce the notion of a universal kernel. A kernel k on a compact
metric space Ω is said to be universal when its RKHS is dense in C(Ω), the set of continuous
functions on Ω, with respect to the supremum norm. Universal kernels are important for
establishing universal consistency of many learning algorithms. See Steinwart and Christ-
mann (2008) for background on kernels and reproducing kernel Hilbert spaces.

Several well-known learning algorithms, such as support vector machines and kernel
ridge regression, may be viewed as minimizers of a norm-regularized empirical risk over
the RKHS of a kernel. A similar development has also been made for multi-task learning
Inspired by this framework, we consider a general kernel-based
(Evgeniou et al., 2005).
algorithm as follows.

Consider the loss function (cid:96) : R × Y → R+. Let k be a kernel on PX × X , and let Hk
be the associated RKHS. For the sample Si, recall that (cid:98)P (i)
j=1 δXij denotes the
corresponding empirical X distribution. Also consider the extended input space PX × X
X , Xij). Note that (cid:98)P (i)
and the extended data (cid:101)Xij = ( (cid:98)P (i)
X plays a role analogous to the task

X = 1
ni

(cid:80)ni

13

index in multi-task learning. Now deﬁne

(cid:98)fλ = arg min

f ∈Hk

1
N

N
(cid:88)

ni(cid:88)

1
ni

i=1

j=1

(cid:96)(f ( (cid:101)Xij), Yij) + λ (cid:107)f (cid:107)2 .

Algorithms for solving (7) will be discussed in Section 7.

5.1 Specifying the Kernels

In the rest of the paper we will consider a kernel k on PX × X of the product form

k((P1, x1), (P2, x2)) = kP (P1, P2)kX (x1, x2),

where kP is a kernel on PX and kX a kernel on X .

Furthermore, we will consider kernels on PX of a particular form. Let k(cid:48)

X denote a
kernel on X (which might be diﬀerent from kX ) that is measurable and bounded. We deﬁne
the kernel mean embedding Ψ : PX → Hk(cid:48)

:

PX (cid:55)→ Ψ(PX ) :=

k(cid:48)
X (x, ·)dPX (x).

X

(cid:90)

X

This mapping has been studied in the framework of “characteristic kernels” (Gretton et al.,
2007a), and it has been proved that universality of k(cid:48)
X implies injectivity of Ψ (Gretton
et al., 2007b; Sriperumbudur et al., 2010).

Note that the mapping Ψ is linear. Therefore, if we consider the kernel kP (PX , P (cid:48)

(cid:104)Ψ(PX ), Ψ(P (cid:48)
reason, we introduce yet another kernel K on Hk(cid:48)

X ) =
X )(cid:105), it is a linear kernel on PX and cannot be a universal kernel. For this
and consider the kernel on PX given by

X

kP (PX , P (cid:48)

X ) = K (cid:0)Ψ(PX ), Ψ(P (cid:48)

X )(cid:1) .

Note that particular kernels inspired by the ﬁnite dimensional case are of the form
K(v, v(cid:48)) = F ((cid:13)

(cid:13)v − v(cid:48)(cid:13)
(cid:13)),

or

K(v, v(cid:48)) = G((cid:10)v, v(cid:48)(cid:11)),

where F, G are real functions of a real variable such that they deﬁne a kernel. For exam-
ple, F (t) = exp(−t2/(2σ2)) yields a Gaussian-like kernel, while G(t) = (1 + t)d yields a
polynomial-like kernel. Kernels of the above form on the space of probability distributions
over a compact space X have been introduced and studied in Christmann and Steinwart
(2010). Below we apply their results to deduce that k is a universal kernel for certain choices
of kX , k(cid:48)

X , and K.

5.2 Relation to Other Kernel Methods

By choosing k diﬀerently, one can recover other existing kernel methods.
consider the class of kernels of the same product form as above, but where

In particular,

(7)

(8)

(9)

(10)

(11)

(12)

kP (PX , P (cid:48)

X ) =

(cid:26) 1 PX = P (cid:48)
X
τ PX (cid:54)= P (cid:48)
X

14

If τ = 0, the algorithm (7) corresponds to training N kernel machines f ( (cid:98)P (i)
X , ·) using kernel
kX (e.g., support vector machines in the case of the hinge loss) on each training data set,
independently of the others (note that this does not oﬀer any generalization ability to a new
data set). If τ = 1, we have a “pooling” strategy that, in the case of equal sample sizes ni,
is equivalent to pooling all training data sets together in a single data set, and running a
conventional supervised learning algorithm with kernel kX (i.e., this corresponds to trying
to ﬁnd a single “one-ﬁts-all” prediction function which does not depend on the marginal).
In the intermediate case 0 < τ < 1, the resulting kernel is a “multi-task kernel,” and the
algorithm recovers a multitask learning algorithm like that of Evgeniou et al. (2005). We
compare to the pooling strategy below in our experiments. We also examined the multi-
task kernel with τ < 1, but found that, as far as generalization to a new unlabeled task is
concerned, it was always outperformed by pooling, and so those results are not reported.
This ﬁts the observation that the choice τ = 0 does not provide any generalization to a
new task, while τ = 1 at least oﬀers some form of generalization, if only by ﬁtting the same
predictor to all data sets.

In the special case where all labels Yij are the same value for a given task, and kX is
taken to be the constant kernel, the problem we consider reduces to “distributional” classi-
ﬁcation or regression, which is essentially standard supervised learning where a distribution
(observed through a sample) plays the role of the feature vector. Many of our analysis
techniques specialize to this setting.

6. Learning Theoretic Study

This section presents generalization error and consistency analysis for the proposed kernel
method under the agnostic and 2-stage generative models. Although the regularized esti-
mation formula (7) deﬁning (cid:98)fλ is standard, the generalization error analysis is not, owing
to the particular sampling structures and risks under (AGM) and (2SGM).

6.1 Universal Consistency under the Agnostic Generative Model

We will consider the following assumptions on the loss function and kernels:

(LB) The loss function (cid:96) : R × Y → R+ is L(cid:96)-Lipschitz in its ﬁrst variable and satisﬁes

(K-Bounded) The kernels kX , k(cid:48)

X and K are bounded respectively by constants B2

k, B2

k(cid:48) ≥

B0 := supy∈Y (cid:96)(0, y) < ∞.

1, and B2
K .

The condition B0 < ∞ always holds for classiﬁcation, as well as certain regression
settings. The boundedness assumptions are clearly satisﬁed for Gaussian kernels, and can
be enforced by normalizing the kernel (discussed further below).

We begin with a generalization error bound that establishes uniform estimation error
control over functions belonging to a ball of Hk . We then discuss universal kernels, and
ﬁnally deduce universal consistency of the algorithm.

Let Bk(r) denote the closed ball of radius r, centered at the origin, in the RKHS of the
kernel k. We start with the following simple result allowing us to bound the loss on a RKHS
ball.

15

Lemma 10 Suppose k is a kernel on a set Ω, bounded by B2. Let (cid:96) : R × Y → [0, ∞) be a
loss satisfying (LB). Then for any R > 0 and f ∈ Bk(R), and any z ∈ Ω and y ∈ Y,

(cid:12)(cid:96)(f (z), y)(cid:12)
(cid:12)

(cid:12) ≤ B0 + L(cid:96)RB

(13)

Proof By the Lipschitz continuity of (cid:96), the reproducing property, and Cauchy-Schwarz,
we have

(cid:12)(cid:96)(f (z), y)(cid:12)
(cid:12)

(cid:12)(cid:96)(f (z), y) − (cid:96)(0, y)(cid:12)
(cid:12)

(cid:12) ≤ (cid:96)(0, y) + (cid:12)
≤ B0 + L(cid:96)|f (z) − 0|
(cid:12)(cid:104)f, k(z, ·)(cid:105)(cid:12)
(cid:12)
= B0 + L(cid:96)
(cid:12)
≤ B0 + L(cid:96)(cid:107)f (cid:107)Hk B
≤ B0 + L(cid:96)RB.

The expression in (13) serves to replace the boundedness assumption (3) in Theorem 5.

We now state the following, which is a specialization of Theorem 5 to the kernel setting.

Theorem 11 (Uniform estimation error control over RKHS balls) Assume (LB)
and (K-Bounded) hold, and data generation follows (AGM). Then for any R > 0, with
probability at least 1 − δ (with respect to the draws of the samples Si, i = 1, . . . , N )

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ≤ (B0 + L(cid:96)RBKBk)
(cid:12) (cid:98)E(f, N ) − E(f )

((cid:112)log δ−1 + 2)
√
N

.

sup
f ∈Bk(R)

(14)

Proof This is a direct consequence of Theorem 5 and of Lemma 10, the kernel k on
PX × X being bounded by B2
K. As noted there, the main term in the upper bound (4)
is a standard Rademacher complexity on the augmented input space P × X , endowed with
the Campbell measure C(PS).

kB2

In the kernel learning context, we can bound the Rademacher complexity term using
a standard bound for the Rademacher complexity of a Lipschitz loss function on the ball
of radius R of Hk (Koltchinskii, 2001; Bartlett and Mendelson, 2002, e.g., Theorems 8, 12
and Lemma 22 there), using again the bound B2

K on the kernel k, giving the conclusion.

kB2

Next, we turn our attention to universal kernels (see Section 5 for the deﬁnition). A
relevant notion for our purposes is that of a normalized kernel. If k is a kernel on Ω, then

k∗(x, x(cid:48)) :=

k(x, x(cid:48))
(cid:112)k(x, x)k(x(cid:48), x(cid:48))

is the associated normalized kernel. If a kernel is universal, then so is its associated normal-
ized kernel. For example, the exponential kernel k(x, x(cid:48)) = exp(κ(cid:104)x, x(cid:48)(cid:105)Rd), κ > 0, can be
shown to be universal on Rd through a Taylor series argument. Consequently, the Gaussian
kernel

kσ(x, x(cid:48)) :=

exp( 1

σ2 (cid:104)x, x(cid:48)(cid:105))
2σ2 (cid:107)x(cid:107)2) exp( 1

exp( 1

2σ2 (cid:107)x(cid:48)(cid:107)2)

16

is universal, being the normalized kernel associated with the exponential kernel with κ =
1/σ2. See Steinwart and Christmann (2008) for additional details and discussion.
To establish that k is universal on PX × X , the following lemma is useful.

Lemma 12 Let Ω, Ω(cid:48) be two compact spaces and k, k(cid:48) be kernels on Ω, Ω(cid:48), respectively. If
k, k(cid:48) are both universal, then the product kernel

k((x, x(cid:48)), (y, y(cid:48))) := k(x, y)k(cid:48)(x(cid:48), y(cid:48))

is universal on Ω × Ω(cid:48).

Several examples of universal kernels are known on Euclidean space. For our purposes,
we also need universal kernels on PX . Fortunately, this was studied by Christmann and
Steinwart (2010). Some additional assumptions on the kernels and feature space are re-
quired:

(K-Univ) kX , k(cid:48)

X , K, and X satisfy the following:

• X is a compact metric space

• kX is universal on X
• k(cid:48)

X is continuous and universal on X

• K is universal on any compact subset of Hk(cid:48)

.

X

Adapting the results of Christmann and Steinwart (2010), we have the following.

Theorem 13 (Universal kernel) Assume condition (K-Univ) holds. Then, for kP de-
ﬁned as in (10), the product kernel k in (8) is universal on PX × X .

Furthermore, the assumption on K is fulﬁlled if K is of the form (12), where G is an
analytical function with positive Taylor series coeﬃcients, or if K is the normalized kernel
associated to such a kernel.

Proof By Lemma 12, it suﬃces to show PX is a compact metric space, and that kP (PX , P (cid:48)
X )
is universal on PX . The former statement follows from Theorem 6.4 of Parthasarathy
(1967), where the metric is the Prohorov metric. We will deduce the latter statement from
Theorem 2.2 of Christmann and Steinwart (2010). The statement of Theorem 2.2 there
is apparently restricted to kernels of the form (12), but the proof actually only uses that
the kernel K is universal on any compact set of Hk(cid:48)
. To apply Theorem 2.2, it remains
is a separable Hilbert space, and that Ψ is injective and continuous.
to show that Hk(cid:48)
Injectivity of Ψ is equivalent to k(cid:48)
X being a characteristic kernel, and follows from the
assumed universality of k(cid:48)
X (Sriperumbudur et al., 2010). The continuity of k(cid:48)
X implies
(Steinwart and Christmann (2008), Lemma 4.33) as well as continuity of
separability of Hk(cid:48)
Ψ (Christmann and Steinwart (2010), Lemma 2.3 and preceding discussion). Now Theorem
2.2 of Christmann and Steinwart (2010) may be applied, and the results follows.

X

X

X

The fact that kernels of the form (12), where G is analytic with positive Taylor coeﬃ-
was established in the proof of Theorem

cients, are universal on any compact set of Hk(cid:48)
2.2 of the same work (Christmann and Steinwart, 2010).

X

17

As an example, suppose that X is a compact subset of Rd. Let kX and k(cid:48)

X be Gaussian
kernels on X . Taking G(t) = exp(t), it follows that K(PX , P (cid:48)
)
is universal on PX . By similar reasoning as in the ﬁnite dimensional case, the Gaussian-like
kernel K(PX , P (cid:48)
) is also universal on PX . Thus the
product kernel is universal on PX × X .

X ) = exp((cid:104)Ψ(PX ), Ψ(P (cid:48)

2σ2 (cid:107)Ψ(PX ) − Ψ(P (cid:48)

X ) = exp(− 1

X )(cid:105)Hk(cid:48)

X )(cid:107)2

Hk(cid:48)
X

X

From Theorems 11 and 13, we may deduce universal consistency of the learning algo-

rithm.

Corollary 14 (Universal consistency) Assume that conditions (LB), (K-Bounded)
and (K-Univ) are satisﬁed. Let λ = λ(N ) be a sequence such that as N → ∞: λ(N ) → 0
and λ(N )N/ log N → ∞. Then

E( (cid:98)fλ(N )) → inf

f :PX ×X →R

E(f ) a.s., as N → ∞.

The proof of the corollary relies on the bound established in Theorem 11, the universality
of k established in Theorem 13, and otherwise relatively standard arguments.

One notable feature of this result is that we have established consistency where only N
is required to diverge. In particular, the training sample sizes ni may remain bounded. In
the next subsection, we consider the role of the ni under the 2-stage generative model.

6.2 Role of the Individual Sample Sizes under the 2-Stage Generative Model

In this section, we are concerned with the role of the individual sample sizes (ni)1≤i≤N .
More precisely, in some applications the number of training points per task is large, which
can give rise to a high computational burden at the learning stage (and also for storing the
learned model in computer memory). We investigate to which extent reducing the number of
training points per task (by random subsampling) in order to reduce computational burden
can be done without suﬀering a signiﬁcant loss in statistical performance. For this we need
a more speciﬁc model for the generating model of points in each task, and we therefore
assume here that the (2SGM), introduced in Section 3.1, holds.

We will consider the following additional assumption.

(K-H¨older) The canonical feature map ΦK : Hk(cid:48)

→ HK associated to K satisﬁes a H¨older

X

condition of order α ∈ (0, 1] with constant LK, on Bk(cid:48)

(Bk(cid:48)) :

X

∀v, w ∈ Bk(cid:48)

(Bk(cid:48)) :

X

(cid:107)ΦK(v) − ΦK(w)(cid:107) ≤ LK (cid:107)v − w(cid:107)α .

(15)

Suﬃcient conditions for (15) are described in Section A.4. As an example, the condition is
shown to hold with α = 1 when K is the Gaussian-like kernel on Hk(cid:48)

.

Since we are interested in the inﬂuence of the number of training points per task, it
is helpful to introduce notations for the (2SGM) risks that are conditioned on a ﬁxed
task PXY . Thus, we introduce the following notation, in analogy to (5)–(6) introduced in

X

18

Section 3.4, for risk at sample size n, and risk at inﬁnite sample size, conditional to PXY :

E(f |PXY , n) := E

ST ∼(PXY )⊗n

(cid:96)(f ( (cid:98)PX , Xi), Yi)

;

(cid:34)

1
n

n
(cid:88)

i=1

(cid:35)

E ∞(f |PXY ) := E(X,Y )∼PXY [(cid:96)(f (PX , X), Y )] .

(16)

(17)

The following proposition gives an upper bound on the discrepancy between these risks.
It can be seen as a quantitative version of Proposition 7 in the kernel setting, which is
furthermore uniform over an RKHS ball.

If the
Theorem 15 Assume conditions (LB), (K-Bounded), and (K-H¨older) hold.
sample S = (Xj, Yj)1≤j≤n is made of n i.i.d. realizations from PXY , with PXY and n ﬁxed,
then for any R > 0, with probability at least 1 − δ:

sup
f ∈Bk(R)

|L(S, f ) − E ∞(f |PXY )| ≤ (B0 + 3L(cid:96)RBk(Bα

k(cid:48)LK + BK))

(cid:19)− α

2

(cid:18) log(3δ−1)
n

.

(18)

Averaging over the draw of S, again with PXY and n ﬁxed, it holds for any R > 0:

sup
f ∈Bk(R)

|E(f |PXY , n) − E ∞(f |PXY )| ≤ 2L(cid:96)RBkLKBα

k(cid:48)n−α/2.

(19)

As a consequence, for the unconditional risks when (PXY , n) is drawn from µ ⊗ ν under
(2GSM), for any R > 0:

|E(f ) − E ∞(f )| ≤ 2RL(cid:96)BkLKBα

k(cid:48)E

(cid:104)
n− α

2

(cid:105)

.

(20)

sup
f ∈Bk(R)

The above results are useful in a number of ways. First, under (2SGM), we can consider
the goal of asymptotically achieving the optimal risk inf f E ∞(f ), where we recall that E ∞(f )
is the expected loss of a decision function f over a random test task P T
XY in the case where
P T
X would be perfectly observed (this can be thought of as observing as an inﬁnite sample
from the marginal). Equation (20) bounds the risk under (2SGM) in terms of the risk
under (AGM), for which we have already established consistency. Thus, consistency under
(2SGM) will be possible if the number of examples ni per training task also grows together
with the number of training tasks N . The following result formalizes this intuition.

XY , . . . , P (N )

Corollary 16 Assume (LB), (K-Bounded), and (K-H¨older) hold, and let n be ﬁxed.
If P (1)
from µ, and the corresponding samples S1, . . . , SN are
i.i.d. all of size n from their respective distributions, then for any R > 0, with probability
at least 1 − δ with respect to the draws of the training tasks and training samples

XY are drawn i.i.d.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
N

N
(cid:88)

i=1

(cid:12)
(cid:12)
L(Si, f ) − E ∞(f )
(cid:12)
(cid:12)
(cid:12)

sup
f ∈Bk(R)

≤ (B0 + L(cid:96)RBKBk)

+ 2RL(cid:96)BkLKBα

k(cid:48)n− α
2 .

(21)

((cid:112)log δ−1 + 2)
√
N

19

Furthermore, if assumption (K-Univ) is satisﬁed and, as the number of training tasks
N → ∞, the sample size n = n(N ) → ∞, and the regularization parameter λ(N ) is such
that λ(N ) → 0 and λ(N ) min(N, nα) → ∞, then

E( (cid:98)fλ(N )) → inf

f :PX ×X →R

E ∞(f ) in probability, as N → ∞.

Proof The setting is that of the (2GSM) model, where the sample size is ﬁxed at n (i.e.
the sample size distribution ν is the Dirac δn). This is a particular case of (AGM), so we
can apply Theorem 11 and combine with (20) (wherein the test sample is also of size n) to
get the announced bound. The consistency statement follows the same argument as in the
proof of Corollary 14, with E(f ) replaced by E ∞(f ), and ε(N ) there replaced by the RHS
in (21). We consider only consistency in probability this time, since a.s. consistency does
not appear of relevance in a setting where the individual training task sample size grows
with the number of tasks.

Remark 17 Our conference paper (Blanchard et al., 2011) also established a generalization
error bound and consistency for E ∞ under (2SGM). That bound had a diﬀerent form for
two main reasons. First, it assumed the loss to be bounded, whereas the present analysis
avoids that assumption via Lemma 10. Second, that analysis did not leverage a connection
to (AGM), which led to a log N in the second term. This required the two sample sizes to
be coupled asymptotically to achieve consistency. In the present analysis, the two sample
sizes N and n may diverge at arbitrary rates.

Remark 18 It is possible to obtain a result similar to Corollary 16 when the training task
sample sizes (ni)1≤i≤N are unequal and possibly arbitrary. In this case we would follow a
slightly diﬀerent argument, using (18) for all training tasks together with a union bound,
and applying Theorem 11 to the idealized situation with an inﬁnite number of samples per
training task. This way, the order n− α
. We eschew
an exact statement for brevity.

2 is replaced by log(N )N −1 (cid:80)N

i=1 n

− α
2
i

Coming back to our initial motivation of possibly reducing computational burden by
subsampling, using (21) we can compare under (2SGM) the two settings where we have
the same task generating distribution µ but diﬀerent individual training task sample sizes
n versus n(cid:48) < n. Under the (2SGM) model, the setting n(cid:48) < n can be obtained by simple
random subsampling of the original data. We see that the statistical risk bound in (21) is
unchanged up to a small factor if n(cid:48) ≥ min(N α, n). Assuming α = 1 to simplify, in the case
where the original sample sizes n are much larger than the number of training tasks N , this
suggests that we can subsample to n(cid:48) ≈ N without taking a signiﬁcant hit to generalization
performance. This applies equally well to subsampling the tasks used for prediction or
testing. The most precise statement in this regard is (18), since it bounds the deviations of
the observed prediction loss for a ﬁxed task PXY and i.i.d. sample from that task.

The minimal subsampling size n(cid:48) can be interpreted as an optimal eﬃciency/accuracy
tradeoﬀ, since it reduces computational complexity as much as possible without sacriﬁcing
statistical accuracy. Similar considerations appear in the context of distribution regres-
sion (Szab´o et al., 2016, Remark 6). In that reference, a sharp analysis giving rise to fast

20

convergence rates is presented, resulting in a more involved optimal balance between N and
n. In the present work, we have focused on slow rates based on a uniform control of the
estimation error over RKHS balls; we leave for future work sharper convergence bounds (un-
der additional regularity conditions), which would also give rise to more reﬁned balancing
conditions between n and N .

7. Implementation

Implementation3 of the algorithm in (7) relies on techniques that are similar to those used
for other kernel methods, but with some variations. The ﬁrst subsection illustrates how,
for the case of hinge loss, the optimization problem corresponds to a certain cost-sensitive
support vector machine. Subsequent subsections focus on more scalable implementations
based on approximate feature mappings.

7.1 Representer Theorem and Hinge loss

For a particular loss (cid:96), existing algorithms for optimizing an empirical risk based on that
loss can be adapted to the setting of marginal transfer learning. We now illustrate this
idea for the case of the hinge loss, (cid:96)(t, y) = max(0, 1 − yt). To make the presentation more
concise, we will employ the extended feature representation (cid:101)Xij = ( (cid:98)P (i)
X , Xij), and we will
also “vectorize” the indices (i, j) so as to employ a single index on these variables and on
the labels. Thus the training data are ( (cid:101)Xi, Yi)1≤i≤M , where M = (cid:80)N
i=1 ni, and we seek a
solution to

min
f ∈Hk

M
(cid:88)

i=1

ci max(0, 1 − Yif ( (cid:101)Xi)) +

1
2

(cid:107)f (cid:107)2 .

Here ci = 1
, where m is the smallest positive integer such that i ≤ n1 + · · · + nm. By
the representer theorem (Steinwart and Christmann, 2008), the solution of (7) has the form

λN nm

for real numbers ri. Plugging this expression into the objective function of (7), and intro-
ducing the auxiliary variables ξi, we have the quadratic program

(cid:98)fλ =

rik( (cid:101)Xi, ·)

M
(cid:88)

i=1

min
r,ξ

1
2

rT Kr +

ciξi

M
(cid:88)

i=1

M
(cid:88)

j=1
ξi ≥ 0, ∀i,

s.t. Yi

rjk( (cid:101)Xi, (cid:101)Xj) ≥ 1 − ξi, ∀i

21

3. Software available at https://github.com/aniketde/DomainGeneralizationMarginal

where K := (k( (cid:101)Xi, (cid:101)Xj))1≤i,j≤M . Using Lagrange multiplier theory, the dual quadratic
program is

max
α

−

1
2

M
(cid:88)

i,j=1

s.t. 0 ≤ αi ≤ ci ∀i,

αiαjYiYjk( (cid:101)Xi, (cid:101)Xj) +

αi

M
(cid:88)

i=1

and the optimal function is

(cid:98)fλ =

αiYik( (cid:101)Xi, ·).

M
(cid:88)

i=1

This is equivalent to the dual of a cost-sensitive support vector machine, without oﬀset,
where the costs are given by ci. Therefore we can learn the weights αi using any existing
software package for SVMs that accepts example-dependent costs and a user-speciﬁed kernel
matrix, and allows for no oﬀset. Returning to the original notation, the ﬁnal predictor given
a test X-sample ST has the form

(cid:98)fλ( (cid:98)P T

X , x) =

αijYijk(( (cid:98)P (i)

X , Xij), ( (cid:98)P T

X , x))

N
(cid:88)

ni(cid:88)

i=1

j=1

where the αij are nonnegative. Like the SVM, the solution is often sparse, meaning most
αij are zero.

Finally, we remark on the computation of kP ( (cid:98)PX , (cid:98)P (cid:48)

(12), the calculation of kP may be reduced to computations of the form
If (cid:98)PX and (cid:98)P (cid:48)
then

X are empirical distributions based on the samples X1, . . . , Xn and X (cid:48)

X ). When K has the form of (11) or
(cid:69)
(cid:68)
Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)
.
X )
1, . . . , X (cid:48)

n(cid:48),

(cid:68)
Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

X )

(cid:69)

=

k(cid:48)
X (Xi, ·),

X (X (cid:48)
k(cid:48)

j, ·)

(cid:43)

(cid:42)

1
n

n
(cid:88)

i=1

1
n(cid:48)

n(cid:48)
(cid:88)

j=1

=

1
nn(cid:48)

n
(cid:88)

n(cid:48)
(cid:88)

i=1

j=1

X (Xi, X (cid:48)
k(cid:48)

j).

Note that when k(cid:48)
a smoothing kernel density estimate for PX .

X is a (normalized) Gaussian kernel, Ψ( (cid:98)PX ) coincides (as a function) with

7.2 Approximate Feature Mapping for Scalable Implementation

Assuming ni = n, for all i, the computational complexity of a nonlinear SVM solver (in our
context) is between O(N 2n2) and O(N 3n3) (Joachims, 1999; Chang and Lin, 2011). Thus,
standard nonlinear SVM solvers may be insuﬃcient when N or n are very large.

One approach to scaling up kernel methods is to employ approximate feature mappings
together with linear solvers. This is based on the idea that kernel methods are solving for
a linear predictor after ﬁrst nonlinearly transforming the data. Since this nonlinear trans-
formation can have an extremely high- or even inﬁnite-dimensional output, classical kernel

22

methods avoid computing it explicitly. However, if the feature mapping can be approxi-
mated by a ﬁnite dimensional transformation with a relatively low-dimensional output, one
can directly solve for the linear predictor, which can be accomplished in O(N n) time (Hsieh
et al., 2008).

In particular, given a kernel k, the goal is to ﬁnd an approximate feature mapping z(˜x)
such that k(˜x, ˜x(cid:48)) ≈ z(˜x)T z(˜x(cid:48)). Given such a mapping z, one then applies an eﬃcient linear
solver, such as Liblinear (Fan et al., 2008), to the training data (z( ˜Xij), Yij)ij to obtain a
weight vector w. The ﬁnal prediction on a test point ˜x is then wT z(˜x). As described in
the previous subsection, the linear solver may need to be tweaked, as in the case of unequal
sample sizes ni, but this is usually straightforward.

Recently, such low-dimensional approximate future mappings z(x) have been developed
for several kernels. We examine two such techniques in the context of marginal transfer
learning, the Nystr¨om approximation (Williams and Seeger, 2001; Drineas and Mahoney,
2005) and random Fourier features. The Nystr¨om approximation applies to any kernel
method, and therefore extends to the marginal transfer setting without additional work.
On the other hand, we give a novel extension of random Fourier features to the marginal
transfer learning setting (for the case of all Gaussian kernels), together with performance
analysis. Our approach is similar to the one in Jitkrittum et al. (2015) which proposes a
two-stage approximation for the mean embedding. Note that Jitkrittum et al. (2015) does
not give an error bound.

7.2.1 Random Fourier Features

The approximation of Rahimi and Recht (2007) is based on Bochner’s theorem, which
characterizes shift invariant kernels.

Theorem 19 A continuous kernel k(x, y) = k(x − y) on Rd is positive deﬁnite iﬀ k(x − y)
is the Fourier transform of a ﬁnite positive measure p(w), i.e.,

k(x − y) =

p(w)ejwT (x−y)dw .

(22)

If a shift invariant kernel k(x − y) is properly scaled then Theorem 19 guarantees that

p(w) in (22) is a proper probability distribution.

(cid:90)

Rd

23

Random Fourier features (RFFs) approximate the integral in (22) using samples drawn

from p(w). If w1, w2, ..., wL are i.i.d. draws from p(w),

k(x − y) =

p(w)ejwT (x−y)dw

p(w) cos(wT x − wT y)dw

cos(wT

i x − wT

i y)

(cid:90)

(cid:90)

Rd

Rd

L
(cid:88)

i=1
L
(cid:88)

i=1
L
(cid:88)

1
L

1
L

1
L

=

≈

=

=

cos(wT

i x) cos(wT

i y) + sin(wT

i x) sin(wT

i y)

[cos(wT

i x), sin(wT

i x)]T [cos(wT

i y), sin(wT

i y)]

i=1
= zw(x)T zw(y) ,

(23)

[cos(wT

1 x), sin(wT

L x), sin(wT

L x)] ∈ R2L is an approximate
where zw(x) = 1√
1 x), ..., cos(wT
L
In the following, we extend the RFF
nonlinear feature mapping of dimensionality 2L.
methodology to the kernel ¯k on the extended feature space PX × X . Let X1, . . . , Xn1
and X (cid:48)
X respectively, and let (cid:98)PX and (cid:98)P (cid:48)
X
denote the corresponding empirical distributions. Given x, x(cid:48) ∈ X , denote ˜x = ( (cid:98)PX , x)
and ˜x(cid:48) = ( (cid:98)P (cid:48)
X , x(cid:48)). The goal is to ﬁnd an approximate feature mapping ¯z(˜x) such that
¯k(˜x, ˜x(cid:48)) ≈ ¯z(˜x)T ¯z(˜x(cid:48)). Recall that

realizations of PX and P (cid:48)

n2 be i.i.d.

1, . . . , X (cid:48)

¯k(˜x, ˜x(cid:48)) = kP ( (cid:98)PX , (cid:98)P (cid:48)

X )kX (x, x(cid:48));

speciﬁcally, we consider kX and k(cid:48)
kP to have the Gaussian-like form

X to be Gaussian kernels and the kernel on distributions

kP ( (cid:98)PX , (cid:98)P (cid:48)

X ) = exp

(cid:107)Ψ( (cid:98)PX ) − Ψ( (cid:98)P (cid:48)

X )(cid:107)2

(cid:26) 1
2σ2
P

(cid:27)

.

Hk(cid:48)
X

As noted earlier in this section, the calculation of kP ( (cid:98)PX , (cid:98)P (cid:48)
of

X ) reduces to the computation

(cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

X )(cid:105) =

X (Xi, X (cid:48)
k(cid:48)

j).

(24)

1
n1n2

n1(cid:88)

n2(cid:88)

i=1

j=1

24

We use Theorem 19 to approximate k(cid:48)

X and thus (cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

X )(cid:105). Let w1, w2, ..., wL be

i.i.d. draws from p(cid:48)(w), the inverse Fourier transform of k(cid:48)

X . Then we have:

(cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

X )(cid:105) =

X (Xi, X (cid:48)
k(cid:48)
j)

1
n1n2

n1(cid:88)

n2(cid:88)

i=1

j=1

≈

=

=

1
Ln1n2

1
Ln1n2

1
Ln1n2

L
(cid:88)

n1(cid:88)

n2(cid:88)

l=1

i=1

j=1

L
(cid:88)

n1(cid:88)

n2(cid:88)

l=1

i=1

j=1

L
(cid:88)
{

n1(cid:88)

l=1

i=1
= ZP ( (cid:98)PX )T ZP ( (cid:98)P (cid:48)

X ),

cos(wT

l Xi − wT

l X (cid:48)
j)

[cos(wT

l Xi) cos(wT

l X (cid:48)

j) + sin(wT

l Xi) sin(wT

l X (cid:48)

j)]

[cos(wT

l Xi), sin(wT

l Xi)]T

[cos(wT

l X (cid:48)

j), sin(wT

l X (cid:48)

j)]}

n2(cid:88)

j=1

where

ZP ( (cid:98)PX ) =

cos(wT

1 Xi), sin(wT

1 Xi), ..., cos(wT

L Xi), sin(wT

L Xi)

(25)

(cid:105)
,

n1(cid:88)

(cid:104)

1
√

n1

L

i=1

and ZP ( (cid:98)P (cid:48)
let z(cid:48)
1
n1

X ) is deﬁned analogously with n1 replaced by n2. For the proof of Theorem 20,
X , which satisﬁes ZP ( (cid:98)PX ) =

X denote the approximate feature map corresponding to k(cid:48)
(cid:80)n1
i=1 z(cid:48)

X (Xi).

Note that the lengths of the vectors ZP ( (cid:98)PX ) and ZP ( (cid:98)P (cid:48)

X ) are 2L. To approximate ¯k we

may write

¯k(˜x, ˜x(cid:48)) ≈ exp

−(cid:107)x − x(cid:48)(cid:107)2
Rd
2σ2
X
P (cid:107)x − x(cid:48)(cid:107)2

Rd)

R2L + σ2

· exp

−(cid:107)ZP ( (cid:98)PX ) − ZP ( (cid:98)P (cid:48)

X )(cid:107)2

R2L

−(σ2

2σ2
P
X (cid:107)ZP ( (cid:98)PX ) − ZP ( (cid:98)P (cid:48)
2σ2

X )(cid:107)2
P σ2
X
X )(cid:107)2
−((cid:107)σX ZP ( (cid:98)PX ) − σX ZP ( (cid:98)P (cid:48)
P σ2
2σ2
X

R2L + (cid:107)σP x − σP x(cid:48)(cid:107)2

Rd)

−(cid:107)(σX ZP ( (cid:98)PX ), σP x) − (σX ZP ( (cid:98)P (cid:48)

X ), σP x(cid:48))(cid:107)2

R2L+d

.

2σ2

P σ2
X

= exp

= exp

= exp

(26)

This is also a Gaussian kernel, now on R2L+d. Again by applying Theorem 19, we have

¯k( (cid:98)PX , X), ( (cid:98)P (cid:48)

X , X (cid:48))) ≈

p(v)ejvT ((σX ZP (PX ),σP X)−(σX ZP (P (cid:48)

X ),σP X (cid:48)))dv.

(cid:90)

R2L+d

Let v1, v2, ..., vq be drawn i.i.d.
kernel with bandwidth σP σX . Let u = (σX ZP ( (cid:98)PX ), σP x) and u(cid:48) = (σX ZP ( (cid:98)P (cid:48)

from p(v), the inverse Fourier transform of the Gaussian
X ), σP x(cid:48)).

25

Then

where

¯k(˜x, ˜x(cid:48)) ≈

cos(vT

q (u − u(cid:48)))

Q
(cid:88)

1
Q

q=1
= ¯z(˜x)T ¯z(˜x(cid:48)),

¯z(˜x) =

[cos(vT

1 u), sin(vT

1 u), ..., cos(vT

Qu), sin(vT

Qu)] ∈ R2Q

(27)

1
√
Q

and ¯z(˜x(cid:48)) is deﬁned similarly.

This completes the construction of the approximate feature map. The following result,
which uses Hoeﬀding’s inequality and generalizes a result of Rahimi and Recht (2007), says
that the approximation achieves any desired approximation error with very high probability
as L, Q → ∞.

Theorem 20 Let L be the number of random features to approximate the kernel on distri-
butions and Q be the number of features to approximate the ﬁnal product kernel. For any
(cid:15)l > 0, (cid:15)q > 0, ˜x = ( (cid:98)PX , x), ˜x(cid:48) = ( (cid:98)P (cid:48)

X , x(cid:48)),

P (|¯k(˜x, ˜x(cid:48)) − ¯z(˜x)T ¯z(˜x(cid:48))| ≥ (cid:15)l + (cid:15)q) ≤ 2 exp

(cid:16)

−

(cid:17)

Q(cid:15)2
q
2

+ 6n1n2 exp

−

(28)

(cid:16)

(cid:17)

,

L(cid:15)2
2

where (cid:15) = σ2
n1 and n2 are the sizes of the empirical distributions (cid:98)PX and (cid:98)P (cid:48)

2 log(1 + (cid:15)l), σP is the bandwidth parameter of the Gaussian-like kernel kP , and

X , respectively.

P

The above results holds for ﬁxed ˜x and ˜x(cid:48). Following again Rahimi and Recht (2007),
one can use an (cid:15)-net argument to prove a stronger statement for every pair of points in the
input space simultaneously. They show

Lemma 21 Let M be a compact subset of Rd with diameter r = diam(M) and let D be the
number of random Fourier features used. Then for the mapping deﬁned in (23), we have

(cid:16)

P

sup
x,y∈M

(cid:17)
|zw(x)T zw(y) − k(x − y)| ≥ (cid:15)

(cid:17)2

≤ 28(cid:16) σr
(cid:15)

exp

(cid:16) −D(cid:15)2
2(d + 2)

(cid:17)

,

where σ = E[wT w] is the second moment of the Fourier transform of k.

Our RFF approximation of ¯k is grounded on Gaussian RFF approximations on Euclidean
spaces, and thus, the following result holds by invoking Lemma 21, and otherwise following
the argument of Theorem 20.

Theorem 22 Using the same notations as in Theorem 20 and Lemma 21,

(cid:16)

P

sup
x,x(cid:48)∈M

|¯k(˜x, ˜x(cid:48)) − ¯z(˜x)T ¯z(˜x(cid:48))| ≥ (cid:15)l + (cid:15)q

(cid:17)

(cid:17)2

≤ 28(cid:16) σ(cid:48)
X r
(cid:15)q

exp

(cid:17)

(cid:16) −Q(cid:15)2
q
2(d + 2)

+ 293n1n2

(cid:16) σP σX r
(cid:15)l
X in Eqn. (24) and σP and σX are the widths of kernels

(cid:16) −L(cid:15)2
l

2(d + 2)

(29)

exp

(cid:17)2

(cid:17)

where σ(cid:48)
kP and kX respectively.

X is the width of kernel k(cid:48)

26

There are recent developments that give faster rates for approximation quality of random
Fourier features and could potentially be combined with our analysis (Sriperumbudur and
Szab´o, 2015; Sutherland and Schneider, 2015). For example, approximation quality for the
kernel mean map is discussed in Sutherland and Schneider (2015), and these ideas could be
extended to Theorem 22 by combining with the two-stage approach presented in this paper.
We also note that our analysis of random Fourier features is separate from our analysis of
the kernel learning algorithm. We have not presented a generalization error bound for the
learning algorithm using random Fourier features (Rudi and Rosasco, 2017).

7.2.2 Nystr¨om Approximation

Like random Fourier features, the Nystr¨om approximation is a technique to approximate
kernel matrices (Williams and Seeger, 2001; Drineas and Mahoney, 2005). Unlike random
Fourier features, for the Nystr¨om approximation, the feature maps are data-dependent.
Also, in the last subsection, all kernels were assumed to be shift invariant. With the
Nystr¨om approximation there is no such assumption.

For a general kernel k, the goal is to ﬁnd a feature mapping z : Rd → RL, where L > d,
such that k(x, x(cid:48)) ≈ z(x)T z(x(cid:48)). Let r be the target rank of the ﬁnal approximated kernel
matrix, and m be the number of selected columns of the original kernel matrix. In general
r ≤ m (cid:28) n.

Given data points x1, . . . , xn, the Nystr¨om method approximates the kernel matrix by
m without replacement from the original sample,
j)]n×m,

ﬁrst sampling m data points x(cid:48)
and then constructing a low rank matrix by (cid:98)Kr = Kb (cid:98)K−1KT
and (cid:98)K = [k(x(cid:48)

j)]m×m. Hence, the ﬁnal approximate feature mapping is

b , where Kb = [k(xi, x(cid:48)

2, ..., x(cid:48)

1, x(cid:48)

i, x(cid:48)

zn(x) = (cid:98)D− 1

2 (cid:98)V T [k(x, x(cid:48)

1), ..., k(x, x(cid:48)

m)],

(30)

where (cid:98)D is the eigenvalue matrix of (cid:98)K and (cid:98)V is the corresponding eigenvector matrix.

The Nystr¨om approximation holds for any positive deﬁnite kernel, but random Fourier
features can be used only for shift invariant kernels. On the other hand, random Fourier
features are very easy to implement and the Nystr¨om method has additional time complexity
due to an eigenvalue decomposition. Moreover, the Nystr¨om method is useful only when
the kernel matrix has low rank. For additional comparison of various kernel approximation
approaches we refer the reader to Le et al. (2013).
In our experiments, we use random
Fourier features when all kernels are Gaussian and the Nystr¨om method otherwise.

8. Experiments

This section empirically compares our marginal transfer learning method with pooling.4
One implementation of the pooling algorithm was mentioned in Section 5.2, where kP is
taken to be a constant kernel. Another implementation is to put all the training data sets
together and train a single conventional kernel method. The only diﬀerence between the two
implementations is that in the former, weights of 1/ni are used for examples from training
task i. In almost all of our experiments below, the various training tasks have the same

4. Software available at https://github.com/aniketde/DomainGeneralizationMarginal

27

sample sizes, in which case the two implementations coincide. The only exception is the
fourth experiment when we use all training data, in which case we use the second of the
two implementations mentioned above.

We consider three classiﬁcation problems (Y = {−1, 1}), for which the hinge loss is
employed, and one regression problem (Y ⊂ R), where the (cid:15)-insensitive loss is employed.
Thus, the algorithms implemented are natural extensions of support vector classiﬁcation
and regression to domain generalization. Performance of a learning strategy is assessed by
holding out several data sets ST
, learning a decision function (cid:98)f on the remaining
data sets, and reporting the average empirical risk 1
i , (cid:98)f ). In some cases, this
NT
value is again averaged over several randomized versions of the experiment.

1 , . . . , ST
NT

i=1 L(ST

(cid:80)NT

8.1 Model Selection

T x2 and Gaussian kernels kσ(x1, x2) = exp (cid:0) − ||x1−x2||2

The various experiments use diﬀerent combinations of kernels. In all experiments, linear
(cid:1) were used.
kernels k(x1, x2) = x1
The bandwidth σ of each Gaussian kernel and the regularization parameter λ of the
machines were selected by grid search. For model selection, ﬁve-fold cross-validation was
used. In order to stabilize the cross-validation procedure, it was repeated 5 times over in-
dependent random splits into folds (Kohavi et al., 1995). Thus, candidate parameter values
were evaluated on the 5 × 5 validation sets and the conﬁguration yielding the best average
performance was selected. If any of the chosen hyper-parameters was at the grid boundary,
the grid was extended accordingly, i.e., the same grid size has been used, however, the center
of grid has been assigned to the previously selected point. The grid used for kernels was
σ ∈ (cid:0)10−2, 104(cid:1) with logarithmic spacing, and the grid used for the regularization parameter
was λ ∈ (cid:0)10−1, 101(cid:1) with logarithmic spacing.

2σ2

8.2 Synthetic Data Experiment

To illustrate the proposed method, a synthetic problem was constructed. The synthetic
data generation algorithm is given in Algorithm 1. In brief, for each classiﬁcation task, the
data are uniformly supported on an ellipse, with the major axis determining the labels, and
the rotation of the major axis randomly generated in a 90 degree range for each task. One
random realization of this synthetic data is shown in Figure 2. This synthetic data set is
an ideal candidate for marginal transfer learning, because the Bayes classiﬁer for a task is
uniquely determined by the marginal distribution of the features, i.e. Lemma 9 applies (and
the optimal error inf f E ∞(f ) is zero). On the other hand, observe that the expectation of
each X distribution is the same regardless of the task and thus does not provide any relevant
information, so that taking into account at least second order information is needed to
perform domain generalization.

To analyse the eﬀects of number of examples per task (n) and number of tasks (N ), we
constructed 12 synthetic data sets by taking combinations N × n where N ∈ {16, 64, 256}
and n ∈ {8, 16, 32, 256}. For each synthetic data set, the test set contains 10 tasks and each
task contains one million data points. All kernels are taken to be Gaussian, and the random
Fourier features speedup is used. The results are shown in Figure 3 and Tables 1 and 2
(see appendix). The marginal transfer learning (MTL) method signiﬁcantly outperforms
the baseline pooling method. Furthermore, the performance of MTL improves as N and n

28

increase, as expected. The pooling method, however, does no better than random guessing
regardless of N and n.

In the remaining experiments, the marginal distribution does not perfectly characterize
the optimal decision function, but still provides some information to oﬀer improvements
over pooling.

Algorithm 1: Synthetic Data Generation

input : N : Number of tasks, n: Number of training examples per task
output: Realization of synthetic data set for N tasks
for i = 1 to N do

• sample rotation αi uniformly in

(cid:104) π
4
• Take an ellipse whose major axis is aligned with the horizontal axis, and

3π
4

(cid:105)

,

;

rotate it by an angle of αi about its center;

• Sample n points Xij, j = 1, . . . , n uniformly at random from the rotated

ellipse;

• Label the points according to their position with respect to the major axis i.e.
the points that are on the right of the major axis are considered as class 1 and
the points on the left of the major axis are considered as class −1.

end

8.3 Parkinson’s Disease Telemonitoring

We test our method in the regression setting using the Parkinson’s disease telemonitoring
data set, which is composed of a range of biomedical voice measurements using a telemon-
itoring device from 42 people with early-stage Parkinson’s. The recordings were automat-
ically captured in the patients’ homes. The aim is to predict the clinician’s Parkinson’s
disease symptom score for each recording on the uniﬁed Parkinson’s disease rating scale
(UPDRS) (Tsanas et al., 2010). Thus we are in a regression setting, and employ the (cid:15)-
insensitive loss from support vector regression. All kernels are taken to be Gaussian, and
the random Fourier features speedup is used.

There are around 200 recordings per patient. We randomly select 7 test users and then
vary the number of training users N from 10 to 35 in steps of 5, and we also vary the
number of training examples n per user from 20 to 100. We repeat this process several
times to get the average errors which are shown in Fig 4 and Tables 3 and 4 (see appendix).
The marginal transfer learning method clearly outperforms pooling, especially as N and n
increase.

8.4 Satellite Classiﬁcation

Microsatellites are increasingly deployed in space missions for a variety of scientiﬁc and
technological purposes. Because of randomness in the launch process, the orbit of a mi-
crosatellite is random, and must be determined after the launch. One recently proposed

29

(a)

(b)

(c)

(d)

Figure 2: Plots of synthetic data sets (red and blue points represent negative and positive
classes) for diﬀerent settings: (a) Random realization of a single task with 256 training
examples per task. Plots (b), (c) and(d) are random realizations of synthetic data with 256
training examples for 16, 64 and 256 tasks.

approach is to estimate the orbit of a satellite based on radiofrequency (RF) signals as mea-
sured in a ground sensor network. However, microsatellites are often launched in bunches,
and for this approach to be successful, it is necessary to associate each RF measurement
vector with a particular satellite. Furthermore, the ground antennae are not able to decode
unique identiﬁer signals transmitted by the microsatellites, because (a) of constraints on
the satellite/ground antennae links, including transmission power, atmospheric attenuation,
scattering, and thermal noise, and (b) ground antennae must have low gain and low direc-
tional speciﬁcity owing to uncertainty in satellite position and dynamics. To address this
problem, recent work has proposed to apply our marginal transfer learning methodology
(Sharma and Cutler, 2015).

As a concrete instance of this problem, suppose two microsatellites are launched to-
gether. Each launch is a random phenomenon and may be viewed as a task in our frame-
work. For each launch i, training data (Xij, Yij), j = 1, . . . , ni, are generated using a highly
realistic simulation model, where Xij is a feature vector of RF measurements across a par-
ticular sensor network and at a particular time, and Yij is a binary label identifying which
of the two microsatellites produced a given measurement. By applying our methodology,
we can classify unlabeled measurements X T
from a new launch with high accuracy. Given
j
these labels, orbits can subsequently be estimated using the observed RF measurements.

30

Figure 3: Synthetic data set: Classiﬁcation error rates for proposed method and diﬀerence
with baseline for diﬀerent experimental settings, i.e., number of examples per task and
number of tasks.

Figure 4: Parkinson’s disease telemonitoring data set: Root mean square error rates for pro-
posed method and diﬀerence with baseline for diﬀerent experimental settings, i.e., number
of examples per task and number of tasks.

31

We thank Srinagesh Sharma and James Cutler for providing us with their simulated data,
and refer the reader to their paper for more details on the application (Sharma and Cutler,
2015).

To demonstrate this idea, we analyzed the data from Sharma and Cutler (2015) for
T = 50 launches, viewing up to 40 as training data and 10 as testing. We use Gaussian
kernels and the RFF kernel approximation technique to speed up the algorithm. Results
are shown in Fig 5 (tables given in the appendix). As expected, the error for the proposed
method is much lower than for pooling, especially as N and n increase.

Figure 5: Satellite data set: Classiﬁcation error rates for proposed method and diﬀerence
with baseline for diﬀerent experimental settings, i.e., number of examples per task and
number of tasks.

8.5 Flow Cytometry Experiments

We demonstrate the proposed methodology for the ﬂow cytometry auto-gating problem,
described in Sec. 2. The pooling approach has been previously investigated in this context
by Toedling et al. (2006). We used a data set that is a part of the FlowCAP Challenges
where the ground truth labels have been supplied by human experts (Aghaeepour et al.,
2013). We used the so-called “Normal Donors” data set. The data set contains 8 diﬀerent
classes and 30 subjects. Only two classes (0 and 2) have consistent class ratios, so we have
restricted our attention to these two.

The corresponding ﬂow cytometry data sets have sample sizes ranging from 18,641 to
59,411, and the proportion of class 0 in each data set ranges from 25.59 to 38.44%. We
randomly selected 10 tasks to serve as the test tasks. These tasks were removed from the
pool of eligible training tasks. We varied the number of training tasks from 5 to 20 with an
additive step size of 5, and the number of training examples per task from 1024 to 16384

32

with a multiplicative step size of 2. We repeated this process 10 times to get the average
classiﬁcation errors which are shown in Fig. 6 and Tables 7 and 8 (see appendix). The
kernel kP was Gaussian, and the other two were linear. The Nystr¨om approximation was
used to achieve an eﬃcient implementation.

For nearly all settings the proposed method has a smaller error rate than the baseline.
Furthermore, for the marginal transfer learning method, when one ﬁxes the number of
training examples and increases the number of tasks then the classiﬁcation error rate drops.
On the other hand, we observe on Table 7 that the number n of training points per
task hardly aﬀects the ﬁnal performance when n ≥ 103. This is in contrast with the
previous experimental examples (synthetic, Parkinson’s disease telemonitoring, and satellite
classiﬁcation), for which increasing n led to better performance, but where the values of
n remained somewhat modest (n ≤ 256). This is qualitatively in line with the theoretical
results under (2SGM) in Section 6.2 (see in particular the concluding discussion there),
suggesting that the inﬂuence of increasing n on the performance should eventually taper
oﬀ, in particular if n (cid:29) N .

Figure 6: Flow Cytometry Data set: Percentage Classiﬁcation error rates for proposed
method and diﬀerence with baseline for diﬀerent experimental settings, i.e., number of
examples per task and number of tasks.

9. Discussion

Our approach to domain generalization relies on the extended input pattern (cid:101)X = (PX , X).
Thus, we study the natural algorithm of minimizing a regularized empirical loss over a
reproducing kernel Hilbert space associated with the extended input domain PX × X . We
also establish universal consistency under two sampling plans. To achieve this, we present

33

novel generalization error analyses, and construct a universal kernel on PX × X . A detailed
implementation based on novel approximate feature mappings is also presented.

On one synthetic and three real-world data sets, the marginal transfer learning approach
consistently outperforms a pooling baseline. On some data sets, however, the diﬀerence
between the two methods is small. This is because the utility of transfer learning varies
from one DG problem to another. As an extreme example, if all of the task are the same,
then pooling should do just as well as our method.

Several future directions exist. From an application perspective, the need for adaptive
classiﬁers arises in many applications, especially in biomedical applications involving biolog-
ical and/or technical variation in patient data. Examples include brain computer interfaces
and patient monitoring. For example, when electrocardiograms are used to continuously
monitor cardiac patients, it is desirable to classify each heartbeat as irregular or not. Given
the extraordinary amount of data involved, automation of this process is essential. How-
ever, irregularities in a test patient’s heartbeat will diﬀer from irregularities of historical
patients, hence the need to adapt to the test distribution (Wiens, 2010).

From a theoretical and methodological perspective, several questions are of interest. We
would like to specify conditions on the meta-distributions PS or µ under which the DG risk
is close to the expected Bayes risk of the test distribution (beyond the simple condition
discussed in Lemma 9). We would also like to develop fast learning rates under suitable dis-
tributional assumptions. Furthermore, given the close connections with supervised learning,
many common variants of supervised learning can also be investigated in the DG context,
including multiclass classiﬁcation, class probability estimation, and robustness to various
forms of noise.

We can also ask how the methodology and analysis can be extended to the context
where a small number of labels are available for the test distribution (additionally to a
larger number of unlabeled data from the same distribution); this situation appears to be
common in practice, and can be seen as intermediary between the DG and learning to learn
(LTL, see Section 4.2) settings (one could dub it “semi-supervised domain generalization”).
In this setting, two approaches appear promising to take advantage of the labeled data. The
simplest one is to use the same optimization problem (7), where we include additionally
the labeled examples of the test distribution. However, if several test samples are to be
treated in succession, and we want to avoid a full, resource-consuming re-training using
all the training samples each time, an interesting alternative is the following:
learn once
a function f0(PX , x) using the available training samples via (7); then, given a partially
labeled test sample, learn a decision function on this sample only via the usual kernel (kX )
norm regularized empirical loss minimization method, but replace the usual regularizer
term (cid:107)f (cid:107)2
X , .) ∈ HkX ). In this sense, the marginal-
adaptive decision function learned from the training samples would serve as a “prior” or
“informed guess” for learning on the test data. This can be also interpreted as learning
an adequate complexity penalty to improve learning on new samples, thus connecting to
the general principles of LTL (see Section 4.2). An interesting diﬀerence with underlying
existing LTL approaches is that those tend to adapt the hypothesis class or the“shape” of the
regularization penalty to the problem at hand, while the approach delineated above would

(cid:13)
(cid:13)f − f0( (cid:98)P T
(cid:13)

(note that f0( (cid:98)P T

H by

X , .)

(cid:13)
2
(cid:13)
(cid:13)

H

34

modify the “origin” of the penalty, using the marginal distribution information. These two
principles could also be combined.

35

Appendix A. Proofs, Technical Details, and Experimental Details

This section contains technical details for the proofs of the announced results.

A.1 Proof of Proposition 7

XY be a ﬁxed probability distribution on X × R, and ε > 0 a ﬁxed number. Since X
X (the
XY ), is inner regular, so that there exists a compact set K ⊂ X such that

Let P T
is a Radon space, by deﬁnition any Borel probability measure on it, in particular P T
X-marginal of P T
P T
X (Kc) ≤ ε.

(cid:12)f (u, v) − f (P T

For all x ∈ K, by the assumed continuity of the decision function f at point (P T

X , x) there
exists an open neighborhood Ux×Vx ⊂ PX ×X of this point such that (cid:12)
X , x)(cid:12)
(cid:12) ≤
ε for all (u, v) ∈ Ux × Vx. Since the family (Vx)x∈K is an open covering of the compact
K, there exists a ﬁnite subfamily (Vxi)i∈I covering K. Denoting U0 := (cid:84)
i∈I Uxi which is
an open neighborhood of P T
X in PX , it therefore holds for any P ∈ U0 and uniformly over
X , x)(cid:12)
x ∈ K that (cid:12)
(cid:12)f (P T
(cid:12)f (P, x) − f (P T
(cid:12) ≤ 2ε, where
i0 ∈ I is such that x ∈ Vxi0
.
Denote ST = (X T
i , Y T
(cid:98)P T
X ∈ U0

XY , and A the
X in probability,
event
so that P [Ac] ≤ ε holds for nT large enough. We have (denoting B a bound on the loss
function):

from P T
X weakly converges to P T

i )1≤i≤nT a sample of size nT drawn i.i.d.

. By the law of large numbers, (cid:98)P T

(cid:12) ≤ |f (P, x) − f (P, xi0)| + (cid:12)

X , x) − f (P, xi0)(cid:12)

(cid:111)

(cid:110)

EST

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:96)(f ( (cid:98)P T

X , X T

(cid:35)
i ), Y T
i )

≤ Bε + EST

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:96)(f ( (cid:98)P T

X , X T

i ), Y T

i )1{X T

i ∈K}

(cid:35)

(cid:35)

(cid:35)

1
nT

nT(cid:88)

i=1

1
nT

nT(cid:88)

i=1

≤ B(ε + P [Ac]) + EST

1{ (cid:98)P T

X ∈U0}

(cid:96)(f ( (cid:98)P T

X , X T

i ), Y T

i )1{X T

i ∈K}

≤ 2Bε + 2Lε + EST

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:96)(f (P T

X , X T

(cid:35)
i ), Y T
i )

≤ 2(B + L)ε + E

(X T ,Y T )∼P T

XY

(cid:2)(cid:96)(f (P T

X , X T ), Y T )(cid:3) .

Conversely,

EST

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:35)

(cid:96)(f ( (cid:98)P T

X , X T

i ), Y T
i )

≥ EST

1{ (cid:98)P T

X ∈U0}

(cid:96)(f ( (cid:98)P T

X , X T

i ), Y T

i )1{X T

i ∈K}

≥ EST

1
nT

nT(cid:88)

i=1

(cid:96)(f (P T

X , X T

(cid:35)
i ), Y T
i )

− 2Bε − 2Lε

≥ E

(X T ,Y T )∼P T

XY

(cid:2)(cid:96)(f (P T

X , X T ), Y T )(cid:3) − 2(B + L)ε.

Since the above inequalities hold for any ε > 0 provided nT is large enough, this yields that
for any ﬁxed P T

XY , we have

E

lim
nT →∞

ST ∼(P T

XY )⊗nT

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:96)(f ( (cid:98)P T

X , X T

(cid:35)
i ), Y T
i )

= E

(X T ,Y T )∼P T

XY

(cid:2)(cid:96)(f (P T

X , X T ), Y T )(cid:3) .

(cid:34)

(cid:34)

(cid:34)

36

Finally, since the above right-hand side is bounded by B, applying dominated convergence
to integrate over P T

XY ∼ µ yields the desired conclusion.

A.2 Proof of Corollary 14

Proof Denote E ∗ = inf f :PX ×X →R E(f ). Let ε > 0. Since k is a universal kernel on PX × X
and (cid:96) is Lipschitz, there exists f0 ∈ Hk such that E(f0) ≤ E ∗+ ε
2 (Steinwart and Christmann,
2008).

By comparing the objective function in (7) at the minimizer (cid:98)fλ and at the null function,
using assumption (LB) we deduce that we must have (cid:107) (cid:98)fλ(cid:107) ≤ (cid:112)B0/λ. Applying Theorem 11
for R = Rλ = (cid:112)B0/λ, and δ = 1/N 2, gives that with probability at least 1 − 1/N 2,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ≤ ε(N ) := (B0 + L(cid:96)BKBk
(cid:12) (cid:98)E(f, N ) − E(f )

(cid:112)B0/λ)

sup
f ∈Bk(R)

√
(

log N + 2)
N

√

.

Let N be large enough so that (cid:107)f0(cid:107) ≤ Rλ. We can now deduce that with probability at

least 1 − 1/N 2,

E( (cid:98)fλ) ≤ (cid:98)E( (cid:98)fλ, N ) + ε(N )

= (cid:98)E( (cid:98)fλ, N ) + λ(cid:107) (cid:98)fλ(cid:107)2 − λ(cid:107) (cid:98)fλ(cid:107)2 + ε(N )
≤ (cid:98)E(f0, N ) + λ(cid:107)f0(cid:107)2 − λ(cid:107) (cid:98)fλ(cid:107)2 + ε(N )
≤ (cid:98)E(f0, N ) + λ(cid:107)f0(cid:107)2 + ε(N )
≤ E(f0) + λ(cid:107)f0(cid:107)2 + 2ε(N )
≤ E ∗ +

+ λ(cid:107)f0(cid:107)2 + 2ε(N ).

ε
2

The last two terms become less than ε
growth of λ = λ(N ). This establishes that for any ε > 0, there exists N0 such that

2 for N suﬃciently large by the assumptions on the

(cid:88)

N ≥N0

Pr(E( (cid:98)fλ) ≥ E ∗ + ε) ≤

(cid:88)

N ≥N0

1
N 2 < ∞,

and so the result follows by the Borel-Cantelli lemma.

37

A.3 Proof of Theorem 15

We control the diﬀerence between the training loss and the conditional risk at inﬁnite sample
size via the following decomposition:

|L(S, f ) − E ∞(f |PXY )| = sup

sup
f ∈Bk(R)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:96)(f ( (cid:98)PX , Xi), Yi) − E ∞(f |PXY )
(cid:12)
(cid:12)
(cid:12)

f ∈Bk(R)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

f ∈Bk(R)

≤ sup

1
n
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
=: (I) + (II).

+ sup

f ∈Bk(R)

n
(cid:88)

(cid:16)

i=1

1
n

n
(cid:88)

i=1

(cid:96)(f ( (cid:98)PX , Xi), Yi) − (cid:96)(f (PX , Xi), Yi)

(cid:12)
(cid:12)
(cid:96)(f (PX , Xi), Yi) − E ∞(f |PXY )
(cid:12)
(cid:12)
(cid:12)

(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(31)

A.3.1 Control of term (I)

Using the assumption that the loss (cid:96) is L(cid:96)-Lipschitz in its ﬁrst coordinate, we can bound
the ﬁrst term as follows:

(I) ≤ L(cid:96)

sup
f ∈Bk(R)

1
n

n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ≤ L(cid:96)
(cid:12)f ( (cid:98)PX , Xi) − f (PX , Xi)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)f ( (cid:98)PX , ·) − f (PX , ·)
(cid:13)∞

. (32)

sup
f ∈Bk(R)

This can now be controlled using the ﬁrst part of the following result:

Lemma 23 Assume (K-Bounded) holds. Let PX be an arbitrary distribution on X and
(cid:98)PX denote an empirical distribution on X based on an iid sample of size n from PX . Then
with probability at least 1 − δ over the draw of this sample, it holds that

(cid:13)
(cid:13)
(cid:13)f ( (cid:98)PX , ·) − f (PX , ·)

(cid:13)
(cid:13)
(cid:13)∞

≤ 3RBkLKBα
k(cid:48)

(cid:18) log 2δ−1
n

(cid:19) α

2

.

sup
f ∈Bk(R)

(33)

In expectation, it holds

(cid:34)

E

sup
f ∈Bk(R)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)f ( (cid:98)PX , ·) − f (PX , ·)
(cid:13)∞

(cid:35)

≤ 2RBkLKBα

k(cid:48)n−α/2.

(34)

Proof Let X1, . . . , Xn denote the n-sample from PX . Let us denote by Φ(cid:48)
X the canonical
feature mapping x (cid:55)→ k(cid:48)
X (x)(cid:107) ≤ Bk(cid:48),
and so, as a consequence of Hoeﬀding’s inequality in a Hilbert space (see, e.g., Pinelis and
Sakhanenko, 1985), it holds with probability at least 1 − δ:

. We have for all x ∈ X , (cid:107)Φ(cid:48)

X (x, ·) from X into Hk(cid:48)

X

(cid:13)
(cid:13)
(cid:13)Ψ(PX ) − Ψ( (cid:98)PX )

(cid:13)
(cid:13)
(cid:13) =

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

Φ(cid:48)

X (Xi) − EX∼PX

(cid:2)Φ(cid:48)

X (X)(cid:3)

≤ 3Bk(cid:48)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:114)

log 2δ−1
n

.

(35)

38

Furthermore, using the reproducing property of the kernel k, we have for any x ∈ X and
f ∈ Bk(R):

|f ( (cid:98)PX , x) − f (PX , x)| =

(cid:68)

(cid:12)
(cid:12)
(cid:12)

k(( (cid:98)PX , x), ·) − k((PX , x), ·), f

(cid:69)(cid:12)
(cid:12)
(cid:12)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)k(( (cid:98)PX , x), ·) − k((PX , x), ·)
(cid:13)
K(Ψ(PX ), Ψ(PX ))

(cid:16)

1
2

≤ RkX (x, x)

≤ (cid:107)f (cid:107)

+ K(Ψ( (cid:98)PX ), Ψ( (cid:98)PX )) − 2K(Ψ(PX ), Ψ( (cid:98)PX ))
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)ΦK(Ψ(PX )) − ΦK(Ψ( (cid:98)PX ))
(cid:13)

≤ RBk

(cid:17) 1
2

≤ RBkLK

(cid:13)
(cid:13)
α
(cid:13)
(cid:13)
(cid:13)Ψ(PX ) − Ψ( (cid:98)PX )
(cid:13)

,

where in the last step we have used property (K-H¨older) together with the fact that for
all P ∈ PX , (cid:107)Ψ(P )(cid:107) ≤ (cid:82)
(Bk(cid:48)). Combining
with (35) gives (33).

X (x, ·)(cid:107) dPX (x) ≤ Bk(cid:48), so that Ψ(P ) ∈ Bk(cid:48)

X (cid:107)k(cid:48)

X

For the bound in expectation, we use the inequality above, and can bound further (using

Jensen’s inequality, since α ≤ 1)

E

(cid:13)
(cid:104)(cid:13)
(cid:13)
(cid:13)
(cid:13)Ψ(PX ) − Ψ( (cid:98)PX )
(cid:13)

α(cid:105)

≤ E

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Ψ(PX ) − Ψ( (cid:98)PX )
(cid:13)

2(cid:21)α/2

E (cid:2)(cid:10)Φ(cid:48)

X (Xi) − E (cid:2)Φ(cid:48)

X (X)(cid:3) , Φ(cid:48)

X (Xj) − E (cid:2)Φ(cid:48)

X (X)(cid:3)(cid:11)(cid:3)





α/2

E

(cid:104)(cid:13)
(cid:13)Φ(cid:48)

X (Xi) − E (cid:2)Φ(cid:48)

X (X)(cid:3)(cid:13)
(cid:13)

(cid:33)α/2

2(cid:105)



=



(cid:32)

=

≤

1
n2

1
n2

n
(cid:88)

i,j=1

n
(cid:88)

i=1

(cid:19) α

2

(cid:18) 4B2
k(cid:48)
n

,

which yields (34) in combination with the above.

A.3.2 Control of term (II)

Term (II) takes the form of a uniform deviation over a RKHS ball of an empirical loss
for the data ( (cid:101)Xi, Yi), where (cid:101)Xi := (PX , Xi). Since PX is ﬁxed (in contrast with term (I)
where (cid:98)PX depended on the whole sample), these data are i.i.d. Similar to the proofs of
Theorems 5 and 11, we can therefore apply again standard Rademacher analysis, this time
at the level of one speciﬁc task (Azuma-McDiarmid inequality followed by Rademacher
complexity analysis for a Lipschitz, bounded loss over a RKHS ball; see Koltchinskii, 2001;
Bartlett and Mendelson, 2002, Theorems 8, 12 and Lemma 22 there). The kernel k is
bounded by B2
K by assumption (K-Bounded); by Lemma 10 and assumption (LB), the

kB2

39

loss is bounded by B0 + L(cid:96)RBkBK, and is L(cid:96)-Lipschitz. Therefore, with probability at least
1 − δ we get

(II) ≤ (B0 + 3L(cid:96)RBkBK) min

(cid:32)(cid:114)

(cid:33)

log(δ−1)
2n

, 1

(cid:32)(cid:18) log(δ−1)

(cid:19) α

2

2n

(cid:33)

, 1

.

≤ (B0 + 3L(cid:96)RBkBK) min

(36)

Observe that we can cap the second factor at 1 since (II) is upper bounded by the bound
on the loss in all cases; the second inequality then uses α ≤ 1. Combining with a union
bound the probabilistic controls (32), (33) of term (I) and (36) of (II) yields (18).

To establish the bound (19) we use a similar argument. We use the decomposition

|E(f |PXY , n) − E ∞(f |PXY )|

sup
f ∈Bk(R)

≤ sup

f ∈Bk(R)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ sup

f ∈Bk(R)
=: (I (cid:48)) + (II (cid:48)).

ESn∼(PXY )⊗n

(cid:12)
(cid:12)
ES∼(PXY )⊗n
(cid:12)
(cid:12)
(cid:12)

(cid:34)

(cid:34)

1
n

1
n

n
(cid:88)

(cid:16)

i=1
n
(cid:88)

i=1

(cid:96)(f ( (cid:98)PX , X T

i ), Yi) − (cid:96)(f (PX , Xi), Yi)

(cid:0)(cid:96)(f (PX , X T

i ), Yi)(cid:1)

(cid:35)

(cid:12)
(cid:12)
− E(X,Y )∼PXY [(cid:96)(f (PX , X), Y )]
(cid:12)
(cid:12)
(cid:12)

(cid:17)

(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

for any ﬁxed f ∈ Bk(R) and PXY , the
It is easily seen that the second term vanishes:
diﬀerence of the expectations is zero. For the ﬁrst term, using Lipschitzness of the loss,
then (34), we obtain

(I (cid:48)) ≤ L(cid:96)E

(cid:34)

sup
f ∈Bk(R)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)f ( (cid:98)PX , .) − f (PX , .)
(cid:13)∞

(cid:35)

≤ 2L(cid:96)RBkLKBα

k(cid:48)n−α/2,

yielding (19). The bound (20) is obtained as a direct consequence by taking expectation
over PXY ∼ µ and using Jensen’s inequality to pull out the absolute value.

A.4 Regularity conditions for the kernel on distributions

We investigate suﬃcient conditions on the kernel K to ensure the regularity condition (K-
H¨older) (15). Roughly speaking, the regularity of the feature mapping of a reproducing
kernel is “one half” of the regularity of the kernel in each of its variables. The next result
considers the situation where K is itself simply a H¨older continuous function of its variables.

Lemma 24 Let α ∈ (0, 1
constant L2

2 ]. Assume that the kernel K is H¨older continuous of order 2α and

K/2 in each of its two variables on Bk(cid:48)

X

(Bk(cid:48)). Then (K-H¨older) is satisﬁed.

Proof For any v, w ∈ Bk(cid:48)

(Bk(cid:48)):

X

(cid:107)ΦK(v) − ΦK(w)(cid:107) = (K(v, v) + K(w, w) − 2K(v, w))

1

2 ≤ LK (cid:107)v − w(cid:107)α .

40

The above type of regularity only leads to a H¨older feature mapping of order at most 1
2
(when the kernel function is Lipschitz continuous in each variable). Since this order plays
an important role in the rate of convergence of the upper bound in the main error control
theorem, it is desirable to study conditions ensuring more regularity, in particular a feature
mapping which has at least Lipschitz continuity. For this, we consider the following stronger
condition, namely that the kernel function is twice diﬀerentiable in a speciﬁc sense:

Lemma 25 Assume that, for any u, v ∈ Bk(cid:48)
, the
function hu,v,e : (λ, µ) ∈ R2 (cid:55)→ K(u + λe, v + µe) admits a mixed partial derivative ∂1∂2hu,v,e
at the point (λ, µ) = (0, 0) which is bounded in absolute value by a constant C2
K independent
of (u, v, e). Then (15) is satisﬁed with α = 1 and LK = CK, that is, the canonical feature
mapping of K is Lipschitz continuous on Bk(cid:48)

(Bk(cid:48)) and unit norm vector e of Hk(cid:48)

(Bk(cid:48)).

X

X

X

Proof The argument is along the same lines as Steinwart and Christmann (2008), Lemma
4.34. Observe that, since hu,v,e(λ + λ(cid:48), µ + µ(cid:48)) = hu+λe,v+µe,e(λ(cid:48), µ(cid:48)), the function hu,v,e
actually admits a uniformly bounded mixed partial derivative in any point (λ, µ) ∈ R2 such
(Bk(cid:48)) . Let us denote ∆1hu,v,e(λ, µ) := hu,v,e(λ, µ) − hu,v,e(0, µ) .
that (u + λe, v + µe) ∈ Bk(cid:48)
(Bk(cid:48)) , u (cid:54)= v , let us set λ := (cid:107)v − u(cid:107) and the unit vector e := λ−1(v − u);
For any u, v ∈ Bk(cid:48)
we have

X

X

(cid:107)ΦK(u) − ΦK(v)(cid:107)2 = K(u, u) + K(u + λe, u + λe) − K(u, u + λe) − K(u + λe, u)

= ∆1hu,v,e(λ, λ) − ∆1hu,v,e(λ, 0)
= λ∂2∆1hu,v,e(λ, λ(cid:48)) ,

where we have used the mean value theorem, yielding existence of λ(cid:48) ∈ [0, λ] such that the
last equality holds. Furthermore,

∂2∆1hu,v,e(λ, λ(cid:48)) = ∂2hu,v,e(λ, λ(cid:48)) − ∂2hu,v,e(0, λ(cid:48))

= λ∂1∂2hu,v,e(λ(cid:48)(cid:48), λ(cid:48)) ,

using again the mean value theorem, yielding existence of λ(cid:48)(cid:48) ∈ [0, λ] in the last equality.
Finally, we get

(cid:107)ΦK(u) − ΦK(v)(cid:107)2 = λ2∂1∂2hu,v,e(λ(cid:48), λ(cid:48)(cid:48)) ≤ C2

K (cid:107)v − u(cid:107)2 .

Lemma 26 Assume that the kernel K takes the form of either (a) K(u, v) = g((cid:107)u − v(cid:107)2)
or (b) K(u, v) = g((cid:104)u, v(cid:105)) , where g is a twice diﬀerentiable real function of a real variable
k(cid:48)] in case (b). Assume (cid:107)g(cid:48)(cid:107)∞ ≤ C1 and
deﬁned on [0, 4B2
(cid:107)g(cid:48)(cid:48)(cid:107)∞ ≤ C2. Then K satisﬁes the assumption of Lemma 25 with CK := 2C1 + 16C2B2
k(cid:48) in
case (a), and CK := C1 + C2B2

k(cid:48)] in case (a), and on [−B2

k(cid:48), B2

k(cid:48) for case (b).

41

Proof In case (a), we have hu,v,e(λ, µ) = g((cid:107)u − v + (λ − µ)e(cid:107)2). It follows

|∂1∂2hu,v,e(0, 0)| =

(cid:12)
(cid:12)

(cid:12)−2g(cid:48)((cid:107)u − v(cid:107)2) (cid:107)e(cid:107)2 − 4g(cid:48)(cid:48)((cid:107)u − v(cid:107)2) (cid:104)u − v, e(cid:105)2(cid:12)
≤ 2C1 + 16C2B2

(cid:12)
(cid:12)

k(cid:48) .

In case (b), we have hu,v,e(λ, µ) = g((cid:104)u + λe, v + µe(cid:105)). It follows

|∂1∂2hu,v,e(0, 0)| =

(cid:12)
(cid:12)g(cid:48)((cid:104)u, v(cid:105)) (cid:107)e(cid:107)2 + g(cid:48)(cid:48)((cid:104)u, v(cid:105)) (cid:104)u, e(cid:105) (cid:104)v, e(cid:105)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

≤ C1 + C2B2

k(cid:48) .

A.5 Proof of Lemma 12

Proof Let H, H(cid:48) the RKHS associated to k, k(cid:48) with the associated feature mappings Φ, Φ(cid:48).
Then it can be checked that (x, x(cid:48)) ∈ X × X (cid:48)
(cid:55)→ Φ(x) ⊗ Φ(cid:48)(x(cid:48)) is a feature mapping for
k into the Hilbert space H ⊗ H(cid:48). Using (Steinwart and Christmann, 2008), Th. 4.21,
we deduce that the RKHS H of k contains precisely all functions of the form (x, x(cid:48)) ∈
X × X (cid:48)
(cid:55)→ Fw(x, x(cid:48)) = (cid:104)w, Φ(x) ⊗ Φ(x(cid:48))(cid:105), where w ranges over H ⊗ H(cid:48). Taking w of the
form w = g ⊗ g(cid:48), g ∈ H, g ∈ H(cid:48), we deduce that H contains in particular all functions of
the form f (x, x(cid:48)) = g(x)g(x(cid:48)), and further

(cid:101)H := span (cid:8)(x, x(cid:48)) ∈ X × X (cid:48) (cid:55)→ g(x)g(x(cid:48)); g ∈ H, g(cid:48) ∈ H(cid:48)(cid:9) ⊂ H.

Denote C(X ), C(X (cid:48)), C(X × X (cid:48)) the set of real-valued continuous functions on the respective
spaces. Let

C(X ) ⊗ C(X (cid:48)) := span (cid:8)(x, x(cid:48)) ∈ X × X (cid:48) (cid:55)→ f (x)f (cid:48)(x(cid:48)); f ∈ C(X ), f (cid:48) ∈ C(X (cid:48))(cid:9) .

Let G(x, x(cid:48)) be an arbitrary element of C(X ) ⊗ C(X (cid:48)), G(x, x(cid:48)) = (cid:80)k
gi ∈ C(X ), g(cid:48)
exist fi ∈ H, f (cid:48)
F (x, x(cid:48)) := (cid:80)k

i(x(cid:48)) with
i ∈ C(X (cid:48)) for i = 1, . . . , k. For ε > 0, by universality of k and k(cid:48), there
i(cid:107)∞ ≤ ε for i = 1, . . . , k. Let

i ∈ H(cid:48) so that (cid:107)fi − gi(cid:107)∞ ≤ ε, (cid:107)f (cid:48)
i(x(cid:48)) ∈ (cid:101)H. We have

i=1 λigi(x)g(cid:48)

i=1 λifi(x)f (cid:48)

i − g(cid:48)

(cid:13)F (x, x(cid:48)) − G(x, x(cid:48))(cid:13)
(cid:13)

(cid:13)∞ ≤

λi(gi(x)g(cid:48)

i(x) − fi(x)f (cid:48)

(cid:13)
(cid:13)
(cid:13)
i(x))
(cid:13)
(cid:13)∞

=

λi

(fi(x) − gi(x))(g(cid:48)

i(x(cid:48)) − f (cid:48)

i(x(cid:48)))

(cid:104)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k
(cid:88)

i=1

k
(cid:88)

i=1

+ gi(x)(g(cid:48)

i(x) − f (cid:48)

i(x(cid:48))) + (gi(x) − fi(x))g(cid:48)

(cid:105)
i(x(cid:48))

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤ ε

k
(cid:88)

i=1

|λi| (ε + (cid:107)gi(cid:107)∞ + (cid:13)
(cid:13)g(cid:48)
i

(cid:13)
(cid:13)∞) .

42

This establishes that (cid:101)H is dense in C(X ) ⊗ C(X (cid:48)) for the supremum norm. It can be easily
checked that C(X ) ⊗ C(X (cid:48)) is an algebra of functions which does not vanish and separates
points on X × X (cid:48). By the Stone-Weierstrass theorem, it is therefore dense in C(X × X (cid:48)) for
the supremum norm. We deduce that (cid:101)H (and thus also H) is dense in C(X × X (cid:48)), so that
k is universal.

A.6 Proof of Theorem 20

Proof Observe:

and denote:

(cid:26) −1
2σ2
P

(cid:26) −1
2σ2
P

¯k(˜x, ˜x(cid:48)) = exp

(cid:107)Ψ( (cid:98)PX ) − Ψ( (cid:98)P (cid:48)

X )(cid:107)2

exp

(cid:27)

(cid:27)

(cid:107)x − x(cid:48)(cid:107)2

,

(cid:26) −1
2σ2
X

˜k(˜x, ˜x(cid:48)) = exp

(cid:107)ZP ( (cid:98)PX ) − ZP ( (cid:98)P (cid:48)

X )(cid:107)2

exp

(cid:27)

(cid:27)

(cid:107)x − x(cid:48)(cid:107)2

,

(cid:26) −1
2σ2
X

We omit the arguments of ¯k, ˜k for brevity. Let kq be the ﬁnal approximation (kq =
¯z(˜x)T ¯z(˜x(cid:48))) and then we have

|¯k − kq| = |¯k − ˜k + ˜k − kq| ≤ |¯k − ˜k| + |˜k − kq|.

From Eqn. (37) it follows that,

P (|¯k − kq| ≥ (cid:15)l + (cid:15)q) ≤ P (|¯k − ˜k| ≥ (cid:15)l) + P (|˜k − kq| ≥ (cid:15)q).

By a direct application of Hoeﬀding’s inequality,

P (|˜k − kq| ≥ (cid:15)q) ≤ 2 exp(−

Q(cid:15)2
q
2

).

(37)

(38)

(39)

Recall that (cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)
again by Hoeﬀding

X )(cid:105) = 1
n1n2

(cid:80)n1
i=1

(cid:80)n2

j=1 k(cid:48)

X (Xi, X (cid:48)

j). For a pair Xi, X (cid:48)

j, we have

P (|z(cid:48)

X (Xi)T z(cid:48)

X (X (cid:48)

j) − k(cid:48)

X (Xi, X (cid:48)

j)| ≥ (cid:15)) ≤ 2 exp(−

Let Ωij be the event |z(cid:48)
bound we have

X (Xi)T z(cid:48)

X (X (cid:48)

j) − k(cid:48)

X (Xi, X (cid:48)

j)| ≥ (cid:15), for particular i, j. Using the union

P (Ω11 ∪ Ω12 ∪ . . . ∪ Ωn1n2) ≤ 2n1n2 exp(−

This implies

P (|ZP ( (cid:98)PX )T ZP ( (cid:98)P (cid:48)

X ) − (cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

X )(cid:105)| ≥ (cid:15)) ≤ 2n1n2 exp(−

(40)

L(cid:15)2
2

).

43

L(cid:15)2
2

).

L(cid:15)2
2

)

Therefore,

(cid:12)
¯k − ˜k
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) =

(cid:34)

≤

exp

(cid:26) −1
2σ2
X

exp

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:26) −1
2σ2
P
(cid:26) −1
2σ2
P

(cid:27) (cid:34)

(cid:107)x − x(cid:48)(cid:107)2

exp

(cid:107)Ψ( (cid:98)PX ) − Ψ( (cid:98)P (cid:48)

X )(cid:107)2

(cid:27)

− exp

(cid:107)ZP ( (cid:98)PX ) − ZP ( (cid:98)P (cid:48)

X )(cid:107)2

(cid:107)Ψ( (cid:98)PX ) − Ψ( (cid:98)P (cid:48)

X )(cid:107)2

− exp

(cid:107)ZP ( (cid:98)PX ) − ZP ( (cid:98)P (cid:48)

X )(cid:107)2

(cid:27) (cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:26) −1
2σ2
P
(cid:27) (cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:27)

(cid:27) (cid:34)

(cid:26) −1
2σ2
P

(cid:16)

(cid:110) −1
2σ2
P

=

exp

(cid:107)Ψ( (cid:98)PX ) − Ψ( (cid:98)P (cid:48)

X )(cid:107)2

1 − exp

(cid:107)ZP ( (cid:98)PX ) − ZP ( (cid:98)P (cid:48)

X )(cid:107)2

(cid:26) −1
2σ2
P

− (cid:107)Ψ( (cid:98)PX ) − Ψ( (cid:98)P (cid:48)

(cid:35)(cid:12)
(cid:12)
X )(cid:107)2(cid:17)(cid:111)
(cid:12)
(cid:12)
(cid:12)

(cid:34)

≤

1 − exp

(cid:16)

(cid:26) −1
2σ2
P

(cid:40)

(cid:16)

−1
2σ2
P

(cid:40)

(cid:16)

1
2σ2
P

(cid:107)ZP ( (cid:98)PX ) − ZP ( (cid:98)P (cid:48)

X )(cid:107)2 − (cid:107)Ψ( (cid:98)PX ) − Ψ( (cid:98)P (cid:48)

=

1 − exp

ZP ( (cid:98)PX )T ZP ( (cid:98)PX ) − (cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)PX )(cid:105) + ZP ( (cid:98)P (cid:48)

X )T ZP ( (cid:98)P (cid:48)

X )

− (cid:104)Ψ( (cid:98)P (cid:48)

X ), Ψ( (cid:98)P (cid:48)

X )(cid:105) − 2(cid:0)ZP ( (cid:98)PX )T ZP ( (cid:98)P (cid:48)

X ) − (cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

≤

1 − exp

|ZP ( (cid:98)PX )T ZP ( (cid:98)PX ) − (cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)PX )(cid:105)| + |ZP ( (cid:98)P (cid:48)

X )T ZP ( (cid:98)P (cid:48)

X )

− (cid:104)Ψ( (cid:98)P (cid:48)

X ), Ψ( (cid:98)P (cid:48)

X )(cid:105)| + 2|(cid:0)ZP ( (cid:98)PX )T ZP ( (cid:98)P (cid:48)

X ) − (cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

X )(cid:107)2(cid:17)(cid:27) (cid:35)(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

X )(cid:105)(cid:1)(cid:17)

(cid:41)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:17)
X )(cid:105)(cid:1)|

(cid:41)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

The result now follows by applying the bound of Eqn. (40) to each of the three terms in
the exponent of the preceding expression, together with the stated formula for (cid:15) in terms
of (cid:15)(cid:96).

A.7 Proof of Theorem 22

Proof The proof is very similar to the proof of Theorem 20. We use Lemma 21 to replace
bound (39) with:

(cid:18)

P

sup
x,x(cid:48)∈M

(cid:19)

|˜k − kq| ≥ (cid:15)q

≤ 28

(cid:19)2

(cid:18) σ(cid:48)
X r
(cid:15)q

exp

(cid:18) −Q(cid:15)2
q
2(d + 2)

(cid:19)
.

(41)

44

Similarly, Eqn. (40) is replaced by

(cid:18)

P

sup
x,x(cid:48)∈M

(cid:12)
(cid:12)ZP ( (cid:98)PX )T ZP ( (cid:98)P (cid:48)

X ) − (cid:10)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

X )(cid:11)(cid:12)

(cid:12) ≥ (cid:15)

(cid:19)

≤ 29n1n2

(cid:19)2

(cid:18) σP σX r
(cid:15)l

exp

(cid:18) −L(cid:15)2
l

2(d + 2)

(cid:19)
.

(42)

The remainder of the proof now proceeds as in the previous proof.

A.8 Results in Tabular Format

Table 1: Average Classiﬁcation Error of Marginal Transfer Learning on Synthetic Data set

Table 2: Average Classiﬁcation Error of Pooling on Synthetic Data set

k
s
a
T
r
e
p

s
e
l
p
m
a
x
E

k
s
a
T
r
e
p

s
e
l

p
m
a
x
E

Tasks

16

64

256

8

16

32

36.01

33.08

31.69

31.55

31.03

30.96

30.44

29.31

23.87

256

23.78

7.22

1.27

Tasks

16

64

256

8

16

32

49.14

49.11

50.04

49.89

50.04

49.68

50.32

50.21

49.61

256

50.01

50.43

49.93

45

Table 3: RMSE of Marginal Transfer Learning on Parkinson’s Disease Data set

Tasks
20

10

15

25

30

35

13.78

12.37

11.93

10.74

10.08

11.17

14.18

11.89

11.51

10.90

10.55

10.18

14.95

13.29

12.00

10.21

10.59

9.52

13.27

11.66

11.79

12.89

11.27

11.17

13.15

11.70

13.81

10.12

9.16

9.91

9.28

9.03

9.03

8.01

9.34

9.10

9.01

8.44

8.16

7.30

7.14

10.50

10.05

8.69

7.62

7.88

7.01

7.5

12.16

13.03

11.98

9.59

9.16

9.18

8.48

9.85

8.80

9.74

9.52

100

12.69

20

24

28

34

41

49

58

70

84

20

24

28

34

41

49

58

70

84

k
s
a
T
r
e
p

s
e
l
p
m
a
x
E

k
s
a
T
r
e
p

s
e
l
p
m
a
x
E

Table 4: RMSE of Pooling on Parkinson’s Disease Data set

Tasks
20

10

15

25

30

35

13.64

11.93

11.95

11.06

11.91

12.08

13.80

11.83

11.70

11.98

11.68

11.48

13.78

11.70

11.72

11.18

11.58

11.73

13.71

12.20

12.04

11.17

11.67

11.92

13.69

11.73

12.08

11.28

11.55

12.59

13.75

11.85

11.79

11.17

11.34

11.82

13.70

11.89

12.06

11.06

11.82

11.65

13.54

11.86

12.14

11.21

11.40

11.96

13.55

11.98

12.03

11.25

11.54

12.22

100

13.53

11.85

11.92

11.12

11.96

11.84

46

Table 5: Average Classiﬁcation Error of Marginal Transfer Learning on Satellite Data set

Tasks
10

20

30

40

8.62

7.61

8.25

7.17

6.21

5.90

5.85

5.43

6.61

5.33

5.37

5.35

5.61

5.19

4.71

4.70

all training data

5.36

4.91

3.86

4.08

k
s
a
T
r
e
p

s
e
l
p
m
a
x
E

k
s
a
T
r
e
p

s
e
l
p
m
a
x
E

5

15

30

45

5

15

30

45

Table 6: Average Classiﬁcation Error of Pooling on Satellite Data set

Tasks
10

20

30

40

8.13

7.54

7.94

6.96

6.55

5.81

5.79

5.57

6.06

5.36

5.56

5.31

5.58

5.12

5.30

4.99

all training data

5.37

4.98

5.32

5.14

Table 7: Average Classiﬁcation Error of Marginal Transfer Learning on Flow Cytometry
Data set

Tasks
10

15

20

9.03

9.03

8.70

5

9

9.12

9.56

9.07

8.62

8.96

8.91

9.01

8.66

9.18

9.20

9.04

8.74

k
s
a
T
r
e
p

s
e
l
p
m
a
x
E

1024

2048

4096

8192

16384

9.05

9.08

9.04

8.63

47

Table 8: Average Classiﬁcation Error of Pooling on Flow Cytometry Data set

Tasks
10

5

15

20

1024

2048

4096

8192

9.41

9.48

9.32

9.52

9.92

9.57

9.45

9.54

9.72

9.56

9.36

9.40

9.43

9.53

9.38

9.50

16384

9.42

9.56

9.40

9.33

k
s
a
T
r
e
p

s
e
l
p
m
a
x
E

C. Scott and A. Deshmukh where supported in part by NSF Grants No. 1422157, 1217880,
and 1047871. G. Blanchard acknowledges support by the DFG via Research Unit 1735
Structural Inference in Statistics.

Acknowledgments

References

N. Aghaeepour, G. Finak, H. Hoos, T. R. Mosmann, R. Brinkman, R. Gottardo, R. H.
Scheuermann, FlowCAP Consortium, DREAM Consortium, et al. Critical assessment
of automated ﬂow cytometry data analysis techniques. Nature methods, 10(3):228–238,
2013.

Kei Akuzawa, Yusuke Iwasawa, and Yutaka Matsuo. Adversarial invariant feature learning

with accuracy constraint for domain generalization. ArXiv, abs/1904.12543, 2019.

Kamyar Azizzadenesheli, Anqi Liu, Fanny Yang, and Animashree Anandkumar. Regularized
learning for domain adaptation under label shifts. In International Conference on Learn-
ing Representations, 2019. URL https://openreview.net/forum?id=rJl0r3R9KX.

G¨okhan Bakır, Thomas Hofmann, Bernhard Sch¨olkopf, Alexander J Smola, and Ben Taskar.

Predicting structured data. MIT press, 2007.

Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa.

Metareg: To-
In S. Bengio, H. Wal-
wards domain generalization using meta-regularization.
ed-
and R. Garnett,
lach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
998–
itors, Advances
1008. Curran Associates,
URL http://papers.nips.cc/paper/
7378-metareg-towards-domain-generalization-using-meta-regularization.pdf.

Information Processing Systems

in Neural

pages

2018.

Inc.,

31,

P. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and

structural results. Journal of Machine Learning Research, 3:463–482, 2002.

P. Bartlett, M. Jordan, and J. McAuliﬀe. Convexity, classiﬁcation, and risk bounds. J.

Amer. Stat. Assoc., 101(473):138–156, 2006.

48

J. Baxter. A model of inductive bias learning. Journal of Artiﬁcial Intelligence Research,

12:149–198, 2000.

Shai Ben-David and Ruth Urner. On the hardness of domain adaptation and the utility
of unlabeled target samples. In Nader H. Bshouty, Gilles Stoltz, Nicolas Vayatis, and
Thomas Zeugmann, editors, Algorithmic Learning Theory, pages 139–153, 2012.

Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of repre-
sentations for domain adaptation. In B. Sch¨olkopf, J. C. Platt, and T. Hoﬀman, editors,
Advances in Neural Information Processing Systems 19, pages 137–144. 2007.

Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jen-
nifer Wortman Vaughan. A theory of learning from diﬀerent domains. Machine Learning,
79:151–175, 2010.

S. Bickel, M. Br¨uckner, and T. Scheﬀer. Discriminative learning under covariate shift. J.

Machine Learning Research, pages 2137–2155, 2009.

G. Blanchard, G. Lee, and C. Scott. Generalizing from several related classiﬁcation tasks
to a new unlabeled sample. In J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira,
and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 24,
pages 2178–2186. 2011.

G. Blanchard, M. Flaska, G. Handy, S. Pozzi, and C. Scott. Classiﬁcation with asymmetric
label noise: Consistency and maximal denoising. Electronic Journal of Statistics, 10:
2780–2824, 2016.

John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman.
Learning bounds for domain adaptation. In J. C. Platt, D. Koller, Y. Singer, and S. T.
Roweis, editors, Advances in Neural Information Processing Systems 20, pages 129–136.
2008.

Timothy I. Cannings, Yingying Fan, and Richard J. Samworth. Classiﬁcation with imperfect

training labels. Technical Report arXiv:1805.11505, 2018.

J. Carbonell, S. Hanneke, and L. Yang. A theory of transfer learning with applications to

active learning. Machine Learning, 90(2):161–189, 2013.

Fabio Maria Carlucci, Antonio D’Innocente, Silvia Bucci, Barbara Caputo, and Tatiana
Tommasi. Domain generalization by solving jigsaw puzzles. 2019 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pages 2224–2233, 2019.

R. Caruana. Multitask learning. Machine Learning, 28:41–75, 1997.

C. Chang and C. Lin. Libsvm: A library for support vector machines. ACM Transactions

on Intelligent Systems and Technology (TIST), 2(3):27, 2011.

A. Christmann and I. Steinwart. Universal kernels on non-standard input spaces. In J. Laf-
ferty, C. K. I. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, editors, Advances in
Neural Information Processing Systems 23, pages 406–414, 2010.

49

Corinna Cortes, Mehryar Mohri, Michael Riley, and Afshin Rostamizadeh. Sample selection

bias correction theory. In Algorithmic Learning Theory, pages 38–53, 2008.

Corinna Cortes, Mehryar Mohri, and Andr´es Mu˜noz Medina. Adaptation algorithm and
theory based on generalized discrepancy.
In Proceedings of the 21th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, KDD ’15, pages
169–178, 2015.

Daryl J. Daley and David Vere-Jones. An introduction to the theory of point processes,

volume II: general theory and structure. Springer, 2008.

G Denevi, Carlo Ciliberto, D Stamos, and Massimiliano Pontil. Incremental learning-to-
learn with statistical guarantees. In Proc. Uncertainty in Artiﬁcial Intelligence, 2018a.

Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil. Learning to
learn around a common mean. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing
Systems 31, pages 10169–10179. 2018b.

Zhengming Ding and Yun Fu. Deep domain generalization with structured low-rank con-

straint. IEEE Transactions on Image Processing, 27:304–313, 2018.

Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain gen-
eralization via model-agnostic learning of semantic features. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d’ Alch´e-Buc, E. Fox, and R. Garnett, editors, Advances in Neural
Information Processing Systems 32, pages 6450–6461. 2019.

P. Drineas and M. W. Mahoney. On the Nystr¨om method for approximating a gram matrix
for improved kernel-based learning. The Journal of Machine Learning Research, 6:2153–
2175, 2005.

M. C. Du Plessis and M. Sugiyama. Semi-supervised learning of class balance under class-
prior change by distribution matching. In J. Langford and J. Pineau, editors, Proc. 29th
Int. Conf. on Machine Learning, pages 823–830, 2012.

T. Evgeniou, C. A. Michelli, and M. Pontil. Learning multiple tasks with kernel methods.

J. Machine Learning Research, pages 615–637, 2005.

R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. Liblinear: A library for large linear

classiﬁcation. The Journal of Machine Learning Research, 9:1871–1874, 2008.

Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast
adaptation of deep networks. In Doina Precup and Yee Whye Teh, editors, Proceedings
of the 34th International Conference on Machine Learning, volume 70 of Proceedings of
Machine Learning Research, pages 1126–1135, 2017.

Chuang Gan, Tianbao Yang, and Boqing Gong. Learning attributes equals multi-source
domain generalization. In The IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), June 2016.

50

Pascal Germain, Amaury Habrard, Fran¸cois Laviolette, and Emilie Morvant. A new pac-
bayesian perspective on domain adaptation. In ICML, volume 48 of JMLR Workshop
and Conference Proceedings, pages 859–868, 2016.

M. Ghifary, D. Balduzzi, B. Kleijn, and M. Zhang. Scatter component analysis: A uniﬁed
framework for domain adaptation and domain generalization. IEEE Trans. Patt. Anal.
Mach. Intell., 39(7):1411–1430, 2017.

Muhammad Ghifary, W. Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain
generalization for object recognition with multi-task autoencoders. In Proceedings of the
2015 IEEE International Conference on Computer Vision (ICCV), page 25512559, 2015.

Mingming Gong, Kun Zhang, Tongliang Liu, Dacheng Tao, Clark Glymour, and Bernhard
Sch¨olkopf. Domain adaptation with conditional transferable components. In International
conference on machine learning, pages 2839–2848, 2016.

A. Gretton, K. Borgwardt, M. Rasch, B. Sch¨olkopf, and A. Smola. A kernel approach to
comparing distributions. In R. Holte and A. Howe, editors, Proceedings of the 22nd AAAI
Conference on Artiﬁcial Intelligence, pages 1637–1641, 2007a.

A. Gretton, K. Borgwardt, M. Rasch, B. Sch¨olkopf, and A. Smola. A kernel method for
the two-sample-problem. In B. Sch¨olkopf, J. Platt, and T. Hoﬀman, editors, Advances in
Neural Information Processing Systems 19, pages 513–520, 2007b.

T. Grubinger, A. Birlutiu, H. Sch¨oner, T. Natschl¨ager, and T. Heskes. Domain generaliza-
tion based on transfer component analysis. In I. Rojas, G. Joya, and A. Catala, editors,
Advances in Computational Intelligence. IWANN 2015, volume 9094 of Lecture Notes in
Computer Science, pages 325–334. Springer, 2015.

P. Hall. On the non-parametric estimation of mixture proportions. Journal of the Royal

Statistical Society, 43(2):147–156, 1981.

C. Hsieh, K. Chang, C. Lin, S. S. Keerthi, and S. Sundararajan. A dual coordinate descent
method for large-scale linear svm. In Proceedings of the 25th international conference on
Machine learning, pages 408–415. ACM, 2008.

Shoubo Hu, Kun Zhang, Zhitang Chen, and Laiwan Chan. Domain generalization via mul-
tidomain discriminant analysis. In Amir Globerson and Ricardo Silva, editors, Proceedings
of the Thirty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence, UAI 2019, Tel
Aviv, Israel, July 22-25, 2019, 2019.

Jiayuan Huang, Alexander J. Smola, Arthur Gretton, Karsten M. Borgwardt, and Bernhard
Scholkopf. Correcting sample selection bias by unlabeled data.
In Proceedings of the
19th International Conference on Neural Information Processing Systems, pages 601–608,
2007.

Wittawat Jitkrittum, Arthur Gretton, Nicolas Heess, SM Eslami, Balaji Lakshminarayanan,
Dino Sejdinovic, and Zolt´an Szab´o. Kernel-based just-in-time learning for passing expec-
tation propagation messages. In Proceedings of the Thirty-First Conference on Uncer-
tainty in Artiﬁcial Intelligence, pages 405–414. AUAI Press, 2015.

51

T. Joachims. Making large scale svm learning practical. Technical report, Universit¨at

Dortmund, 1999.

O. Kallenberg. Foundations of Modern Probability. Springer, 2002.

Takafumi Kanamori, Shohei Hido, and Masashi Sugiyama. A least-squares approach to

direct importance estimation. J. Mach. Learn. Res., 10:1391–1445, 2009.

Aditya Khosla, Tinghui Zhou, Tomasz Malisiewicz, Alexei A. Efros, and Antonio Torralba.
Undoing the damage of dataset bias. In Proceedings of the 12th European Conference on
Computer Vision - Volume Part I, page 158171, 2012.

Ron Kohavi et al. A study of cross-validation and bootstrap for accuracy estimation and

model selection. In IJCAI, volume 14, pages 1137–1145, 1995.

V. Koltchinskii. Rademacher penalties and structural risk minimization. IEEE Transactions

on Information Theory, 47(5):1902 – 1914, 2001.

P. Latinne, M. Saerens, and C. Decaestecker. Adjusting the outputs of a classiﬁer to new
a priori probabilities may signiﬁcantly improve classiﬁcation accuracy: Evidence from a
multi-class problem in remote sensing. In C. Sammut and A. H. Hoﬀmann, editors, Proc.
18th Int. Conf. on Machine Learning, pages 298–305, 2001.

Quoc Le, Tam´as Sarl´os, and Alex Smola. Fastfood: approximating kernel expansions in
In Proceedings of the 30th International Conference on International

loglinear time.
Conference on Machine Learning-Volume 28, pages III–244. JMLR. org, 2013.

Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and
artier domain generalization. In Proceedings of the IEEE International Conference on
Computer Vision, pages 5542–5550, 2017.

Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Learning to generalize:
Meta-learning for domain generalization. In Thirty-Second AAAI Conference on Artiﬁcial
Intelligence, 2018a.

H. Li, S. J. Pan, S. Wang, and A. C. Kot. Domain generalization with adversarial feature
learning. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, pages
5400–5409, 2018b.

Ya Li, Mingming Gong, Xinmei Tian, Tongliang Liu, and Dacheng Tao. Domain general-
ization via conditional invariant representations. In Thirty-Second AAAI Conference on
Artiﬁcial Intelligence, 2018c.

Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng
Tao. Deep domain generalization via conditional invariant adversarial networks. In Pro-
ceedings of the European Conference on Computer Vision (ECCV), pages 624–639, 2018d.

Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning
In COLT 2009 - The 22nd Conference on Learning Theory,

bounds and algorithms.
2009a.

52

Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation with
In Advances in neural information processing systems, pages 1041–

A. Maurer. Transfer bounds for linear feature learning. Machine Learning, 75(3):327–350,

multiple sources.
1048, 2009b.

2009.

A. Maurer, M. Pontil, and B. Romera-Paredes. Sparse coding for multitask and transfer
learning.
In Sanjoy Dasgupta and David McAllester, editors, Proceedings of the 30th
International Conference on Machine Learning, volume 28 of Proceedings of Machine
Learning Research, pages 343–351, 2013.

Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The beneﬁt of

multitask representation learning. J. Mach. Learn. Res., 17(1):2853–2884, 2016.

Aditya Krishna Menon, Brendan van Rooyen, and Nagarajan Natarajan. Learning from
binary labels with instance-dependent noise. Machine Learning, 107:1561–1595, 2018.

Saeid Motiian, Marco Piccirilli, Donald A Adjeroh, and Gianfranco Doretto. Uniﬁed deep
supervised domain adaptation and generalization. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision, pages 5715–5725, 2017.

K. Muandet, D. Balduzzi, and B. Sch¨olkopf. Domain generalization via invariant feature
representation. In Proceedings of the 30th International Conference on International Con-
ference on Machine Learning (ICML’13), volume 28 of Proceedings of Machine Learning
Research, pages I–10–I–18, 2013.

Nagarajan Natarajan, Inderjit S. Dhillon, Pradeep Ravikumar, and Ambuj Tewari. Cost-
sensitive learning with noisy labels. Journal of Machine Learning Research, 18(155):1–33,
2018. URL http://jmlr.org/papers/v18/15-226.html.

K. R. Parthasarathy. Probability Measures on Metric Spaces. Academic Press, 1967.

A. Pentina and C. Lampert. A pac-bayesian bound for lifelong learning. In Eric P. Xing
and Tony Jebara, editors, Proceedings of the 31st International Conference on Machine
Learning, volume 32 of Proceedings of Machine Learning Research, pages 991–999, 2014.

I.F. Pinelis and A.I. Sakhanenko. Remarks on inequalities for probabilities of large devia-

tions. Theory Probab. Appl., 30(1):143–148, 1985.

J. Quionero-Candela, M. Sugiyama, A. Schwaighofer, and N. D. Lawrence. Dataset Shift in

Machine Learning. The MIT Press, 2009.

A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in

neural information processing systems, pages 1177–1184, 2007.

Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random
features. In Advances in Neural Information Processing Systems, pages 3215–3225, 2017.

53

T. Sanderson and C. Scott. Class proportion estimation with application to multiclass
In Proceedings of the 17th International Conference on Artiﬁcial

anomaly rejection.
Intelligence and Statistics (AISTATS), 2014.

Clayton Scott. A generalized neyman-pearson criterion for optimal domain adaptation. In
Aur´elien Garivier and Satyen Kale, editors, Proceedings of the 30th International Con-
ference on Algorithmic Learning Theory, volume 98 of Proceedings of Machine Learning
Research, pages 738–761, 2019.

Shiv Shankar, Vihari Piratla, Soumen Chakrabarti, Siddhartha Chaudhuri, Preethi Jyothi,
In In-
and Sunita Sarawagi. Generalizing across domains via cross-gradient training.
ternational Conference on Learning Representations, 2018. URL https://openreview.
net/forum?id=r1Dx7fbCW.

S. Sharma and J. W. Cutler. Robust orbit determination and classiﬁcation: A learning

theoretic approach. Interplanetary Network Progress Report, 203:1, 2015.

B. Sriperumbudur, A. Gretton, K. Fukumizu, B. Sch¨olkopf, and G. Lanckriet. Hilbert space
embeddings and metrics on probability measures. Journal of Machine Learning Research,
11:1517–1561, 2010.

Bharath Sriperumbudur and Zolt´an Szab´o. Optimal rates for random fourier features. In

Advances in Neural Information Processing Systems, pages 1144–1152, 2015.

I. Steinwart and A. Christmann. Support Vector Machines. Springer, 2008.

Amos J Storkey. When training and test sets are diﬀerent: characterising learning transfer.

In In Dataset Shift in Machine Learning, pages 3–28. MIT Press, 2009.

Masashi Sugiyama, Taiji Suzuki, Shinichi Nakajima, Hisashi Kashima, Paul von B¨unau, and
Motoaki Kawanabe. Direct importance estimation for covariate shift adaptation. Annals
of the Institute of Statistical Mathematics, 60:699–746, 2008.

Dougal J. Sutherland and Jeﬀ Schneider. On the error of random Fourier features. In Pro-
ceedings of the Thirty-First Conference on Uncertainty in Artiﬁcial Intelligence, UAI’15,
pages 862–871. AUAI Press, 2015.

Zolt´an Szab´o, Bharath K Sriperumbudur, Barnab´as P´oczos, and Arthur Gretton. Learning
theory for distribution regression. The Journal of Machine Learning Research, 17(1):
5272–5311, 2016.

Dirk Tasche. Fisher consistency for prior probability shift. Journal of Machine Learning

Research, 18:1–32, 2017.

S. Thrun. Is learning the n-th thing any easier than learning the ﬁrst? Advances in Neural

Information Processing Systems, pages 640–646, 1996.

D. M. Titterington. Minimum distance non-parametric estimation of mixture proportions.

Journal of the Royal Statistical Society, 45(1):37–46, 1983.

54

J. Toedling, P. Rhein, R. Ratei, L. Karawajew, and R. Spang. Automated in-silico detection
of cell populations in ﬂow cytometry readouts and its application to leukemia disease
monitoring. BMC Bioinformatics, 7:282, 2006.

A. Tsanas, M. A. Little, P. E. McSharry, and L. O. Ramig. Accurate telemonitoring
IEEE transactions on

of parkinson’s disease progression by noninvasive speech tests.
Biomedical Engineering, 57(4):884–893, 2010.

Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun. Large
margin methods for structured and interdependent output variables. Journal of machine
learning research, 6(Sep):1453–1484, 2005.

Brendan van Rooyen and Robert C. Williamson. A theory of learning with corrupted labels.

Journal of Machine Learning Research, 18(228):1–50, 2018.

Haohan Wang, Zexue He, Zachary C. Lipton, and Eric P. Xing. Learning robust represen-
tations by projecting superﬁcial statistics out. In International Conference on Learning
Representations, 2019. URL https://openreview.net/forum?id=rJEjjoR9K7.

J. Wiens. Machine Learning for Patient-Adaptive Ectopic Beat Classication. Masters Thesis,
Department of Electrical Engineering and Computer Science, Massachusetts Institute of
Technology, 2010.

C. Williams and M. Seeger. Using the Nystr¨om method to speed up kernel machines. In
Proceedings of the 14th Annual Conference on Neural Information Processing Systems,
number EPFL-CONF-161322, pages 682–688, 2001.

Zheng Xu, Wen Li, Li Niu, and Dong Xu. Exploiting low-rank structure from latent domains
for domain generalization. In European Conference on Computer Vision, pages 628–643.
Springer, 2014.

Xiaolin Yang, Seyoung Kim, and Eric P Xing. Heterogeneous multitask learning with joint
sparsity constraints. In Advances in neural information processing systems, pages 2151–
2159, 2009.

Yao-Liang Yu and Csaba Szepesvari. Analysis of kernel mean matching under covariate
shift. In Proceedings of the 29th International Conference on Machine Learning, pages
607–614, 2012.

Bianca Zadrozny. Learning and evaluating classiﬁers under sample selection bias. In Pro-

ceedings of the Twenty-ﬁrst International Conference on Machine Learning, 2004.

Kun Zhang, Bernhard Sch¨olkopf, Krikamol Muandet, and Zhikun Wang. Domain adapta-
tion under target and conditional shift. In International Conference on Machine Learning,
pages 819–827, 2013.

Kun Zhang, Mingming Gong, and Bernhard Scholkopf. Multi-source domain adaptation:
A causal view. In Proceedings of the Twenty-Ninth AAAI Conference on Artiﬁcial Intel-
ligence, AAAI’15, pages 3150–3157. AAAI Press, 2015.

55

0
2
0
2
 
r
p
A
 
7
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
0
1
9
7
0
.
1
1
7
1
:
v
i
X
r
a

Domain Generalization by Marginal Transfer Learning

Gilles Blanchard
Institut f¨ur Mathematik
Universit¨at Potsdam

Aniket Anand Deshmukh
Microsoft AI & Research
¨Urun Dogan
Microsoft AI & Research

Gyemin Lee
Dept. Electronic and IT Media Engineering
Seoul National University of Science and Technology

Clayton Scott
Electrical and Computer Engineering
University of Michigan

blanchard@math.uni-potsdam.de

aniketde@umich.edu

urundogan@gmail.com

gyemin@seoultech.ac.kr

clayscot@umich.edu

Abstract
In the problem of domain generalization (DG), there are labeled training data sets from
several related prediction problems, and the goal is to make accurate predictions on future
unlabeled data sets that are not known to the learner. This problem arises in several ap-
plications where data distributions ﬂuctuate because of environmental, technical, or other
sources of variation. We introduce a formal framework for DG, and argue that it can be
viewed as a kind of supervised learning problem by augmenting the original feature space
with the marginal distribution of feature vectors. While our framework has several con-
nections to conventional analysis of supervised learning algorithms, several unique aspects
of DG require new methods of analysis.

This work lays the learning theoretic foundations of domain generalization, building on
our earlier conference paper where the problem of DG was introduced (Blanchard et al.,
2011). We present two formal models of data generation, corresponding notions of risk, and
distribution-free generalization error analysis. By focusing our attention on kernel meth-
ods, we also provide more quantitative results and a universally consistent algorithm. An
eﬃcient implementation is provided for this algorithm, which is experimentally compared
to a pooling strategy on one synthetic and three real-world data sets.
Keywords: Domain Generalization, Generalization Error Bounds, Kernel Methods, Uni-
versal Consistency, Kernel Approximation

1. Introduction

Domain generalization (DG) is a machine learning problem where the learner has access to
labeled training data sets from several related prediction problems, and must generalize to
a future prediction problem for which no labeled data are available. In more detail, there
are N labeled training data sets Si = (Xij, Yij)1≤j≤ni, i = 1, . . . , N , that describe similar
but possibly distinct prediction tasks. The objective is to learn a rule that takes as input

1

a previously unseen unlabeled test data set X T
for these or possibly other unlabeled points from the associated learning task.

1 , . . . , X T
nT

, and accurately predicts outcomes

DG arises in several applications. One prominent example is precision medicine, where
a common objective is to design a patient-speciﬁc classiﬁer (e.g., of health status) based
on clinical measurements, such as an electrocardiogram or electroencephalogram. In such
measurements, patient-to-patient variation is common, arising from biological variations
between patients, or technical or environmental factors inﬂuencing data acquisition. Be-
cause of patient-to-patient variation, a classiﬁer that is trained on data from one patient
may not be well matched to another patient. In this context, domain generalization enables
the transfer of knowledge from historical patients (for whom labeled data are available) to a
new patient without the need to acquire training labels for that patient. A detailed example
in the context of ﬂow cytometry is given below.

We view domain generalization as a conventional supervised learning problem where
the original feature space is augmented to include the marginal distribution generating the
features. We refer to this reframing of DG as “marginal transfer learning,” because it reﬂects
the fact that in DG, information about the test task must be drawn from that tasks’ marginal
feature distribution. Leveraging this perspective, we formulate two statistical frameworks
for analyzing DG. The ﬁrst framework allows the observations within each data set to
have arbitrary dependency structure, and makes connections to the literature on Campbell
measures and structured prediction. The second framework is a special case of the ﬁrst,
assuming the data points are drawn iid within each task, and allows for a more reﬁned risk
analysis.

We further develop a distribution-free kernel machine that employs a kernel on the
aforementioned augmented feature space. Our methodology is shown to yield a universally
consistent learning procedure under both statistical frameworks, meaning that the domain
generalization risk tends to the best possible value as the relevant sample sizes tend inﬁnity,
with no assumptions on the data generating distributions. Although DG may be viewed as
a conventional supervised learning problem on an augmented feature space, the analysis is
nontrivial owing to unique aspects of the sampling plans and risks. We oﬀer a computation-
ally eﬃcient and freely available1 implementation of our algorithm, and present a thorough
experimental study validating the proposed approach on one synthetic and three real-world
data sets, including comparisons to a simple pooling approach.

To our knowledge, the problem of domain generalization was ﬁrst proposed and studied
by our earlier conference publication (Blanchard et al., 2011) which this work extends
in several ways.
It adds (1) a new statistical framework, the agnostic generative model
described below; (2) generalization error and consistency results for the new statistical
model; (3) an extensive literature review; (4) an extension to the regression setting in
both theory and experiments; (5) a more general statistical analysis, in particular, we no
longer assume a bounded loss, and therefore accommodate common convex losses such as
the hinge and logistic losses; (6) extensive experiments (the conference paper considered a
single small dataset); (7) a scalable implementation based on a novel extension of random
Fourier features; and (8) error analysis for the random Fourier features approximation.

1. https://github.com/aniketde/DomainGeneralizationMarginal

2

2. Motivating Application: Automatic Gating of Flow Cytometry Data

Flow cytometry is a high-throughput measurement platform that is an important clinical
tool for the diagnosis of blood-related pathologies. This technology allows for quantitative
analysis of individual cells from a given cell population, derived for example from a blood
sample from a patient. We may think of a ﬂow cytometry data set as a set of d-dimensional
attribute vectors (Xj)1≤j≤n, where n is the number of cells analyzed, and d is the number
of attributes recorded per cell. These attributes pertain to various physical and chemical
properties of the cell. Thus, a ﬂow cytometry data set may be viewed as a random sample
from a patient-speciﬁc distribution.

Now suppose a pathologist needs to analyze a new (test) patient with data (X T

j )1≤j≤nT .
Before proceeding, the pathologist ﬁrst needs the data set to be “puriﬁed” so that only cells
of a certain type are present. For example, lymphocytes are known to be relevant for the
diagnosis of leukemia, whereas non-lymphocytes may potentially confound the analysis. In
other words, it is necessary to determine the label Y T
j ∈ {−1, 1} associated to each cell,
where Y T

j = 1 indicates that the j-th cell is of the desired type.

In clinical practice this is accomplished through a manual process known as “gating.”
The data are visualized through a sequence of two-dimensional scatter plots, where at each
stage a line segment or polygon is manually drawn to eliminate a portion of the unwanted
cells. Because of the variability in ﬂow cytometry data, this process is diﬃcult to quantify
in terms of a small subset of simple rules. Instead, it requires domain-speciﬁc knowledge
and iterative reﬁnement. Modern clinical laboratories routinely see dozens of cases per day,
so it would be desirable to automate this process.

Since clinical laboratories maintain historical databases, we can assume access to a
number (N ) of historical (training) patients that have already been expert-gated. Because
of biological and technical variations in ﬂow cytometry data, the distributions P (i)
XY of
the historical patients will vary. To illustrate the ﬂow cytometry gating problem, we use
the NDD data set from the FlowCap-I challenge.2 For example, Fig. 1 shows exemplary
two-dimensional scatter plots for two diﬀerent patients – see caption for details. Despite
diﬀerences in the two distributions, there are also general trends that hold for all patients.
Virtually every cell type of interest has a known tendency (e.g., high or low) for most
measured attributes. Therefore, it is reasonable to assume that there is an underlying
distribution (on distributions) governing ﬂow cytometry data sets, that produces roughly
similar distributions thereby making possible the automation of the gating process.

3. Formal Setting and General Results

In this section we formally deﬁne domain generalization via two possible data generation
models together with associated notions of risk. We also provide a basic generalization error
bound for the ﬁrst of these data generation models.

Let X denote the observation space (assumed to be a Radon space) and Y ⊆ R the
output space. Let PX and PX ×Y denote the set of probability distributions on X and X ×Y,
respectively. The spaces PX and PX ×Y are endowed with the topology of weak convergence
and the associated Borel σ-algebras. The symbol ⊗ indicates a product measure.

2. We will revisit this data set in Section 8.5 where details are given.

3

Figure 1: Two-dimensional projections of multi-dimensional ﬂow cytometry data. Each row
corresponds to a single patient, and each column to a particular two-dimensional projection.
The distribution of cells diﬀers from patient to patient. The colors indicate the results of
gating, where a particular type of cell, marked dark (blue), is separated from all other cells,
marked bright (red). Labels were manually selected by a domain expert.

The disintegration theorem for joint probability distributions (see for instance Kallen-
berg, 2002, Theorem 6.4) tells us that (under suitable regularity properties, satisﬁed if X is
a Radon space) any element PXY ∈ PX ×Y can be written as a Markov semi-direct product
PXY = PX • PY |X , with PX ∈ PX , PY |X ∈ PY |X , where PY |X is the space of conditional
probability distributions of Y given X, also called Markov transition kernels from X to Y.
This speciﬁcally means that

E(X,Y )∼PXY [h(X, Y )] =

h(x, y)PY |X (dy|X = x)

PX (dx),

(1)

(cid:90) (cid:18)(cid:90)

(cid:19)

for any integrable function h : X × Y → R. Following common terminology in the statistical
learning literature, we will also call PY |X the posterior distribution (of Y given X).

We assume that N training samples Si = (Xij, Yij)1≤j≤ni, i = 1, . . . , N , are observed.
To allow for possibly unequal sample sizes ni, it is convenient to formally identify each
sample Si with its associated empirical distribution (cid:98)P (i)
j=1 δ(Xij ,Yij ) ∈ PX ×Y .
We assume that the ordering of the observations inside a given sample Si is arbitrary
and does not contain any relevant information. We also denote by (cid:98)P (i)
j=1 δXij ∈
PX the ith training sample without labels. Similarly, a test sample is denoted by ST =
(X T

j )1≤j≤nT , and the empirical distribution of the unlabeled data by (cid:98)P T
X .

XY = 1
ni

X = 1
ni

j , Y T

(cid:80)ni

(cid:80)ni

3.1 Data Generation Models

We propose two data generation models. The ﬁrst is most general, and includes the second
as a special case.

4

Assumption 1 (AGM) There exists a distribution PS on PX ×Y such that S1, . . . , SN are
i.i.d. realizations from PS.

We call this the agnostic generative model. This is a quite general model in which samples are
assumed to be identically distributed and independent of each other, but nothing particular
is assumed about the generation mechanism for observations inside a given sample, nor for
the (random) sample size.

We also introduce a more speciﬁc generative mechanism, where observations (Xij, Yij)
XY , a latent unobserved random distribu-

inside the sample Si are themselves i.i.d. from P (i)
tion, as follows.

Assumption 2 (2SGM) There exists a distribution µ on PX ×Y and a distribution ν on
N, such that (P (1)
XY , nN ) are i.i.d. realizations from µ ⊗ ν, and conditional to
(P (i)
XY , ni) the sample Si is made of ni i.i.d. realizations of (X, Y ) following the distribution
P (i)
XY .

XY , n1), . . . , (P (N )

This model, called the 2-stage generative model, is a subcase of (AGM): since the
(P (i)
It has been considered in the distinct but
XY , ni) are i.i.d., the samples Si also are.
related context of “learning to learn” (Baxter, 2000; see also a more detailed discussion
below, Section 4.2). Many of our results will hold for the agnostic generative model, but
the two-stage generative model allows for additional developments, as will be discussed
further below. This model was the one studied in our conference paper (Blanchard et al.,
2011).

3.2 Decision Functions and Augmented Feature Space

In domain generalization, the learner’s goal is to infer from the training data a general
rule that takes an arbitrary, previously unseen, unlabeled dataset corresponding to a new
prediction task, and produces a classiﬁer for that prediction task that could be applied to
any x (possibly outside the unlabeled data set). In other words, the learner should out-
put a mapping g : PX → (X → R). Equivalently, the learner should output a function
f : PX × X → R, where the two notations are related via g(PX )(x) = f (PX , x). In the
latter viewpoint, f may be viewed as a standard decision function on the “augmented”
or “extended” feature space PX × X , which facilitates connections to standard supervised
learning. We refer to this view of DG as marginal transfer learning, because the informa-
tion that facilitates generalization to a new task is conveyed entirely through the marginal
distribution. In the next two subsections, we present two deﬁnitions of the risk of a decision
function f , one associated to each of the two data generation models.

3.3 Risk and Generalization Error Bound under the Agnostic Generative

Model

Now consider a test sample ST = (X T
j )1≤j≤nT , whose labels are not observed by the
learner. If (cid:96) : R×Y (cid:55)→ R+ is a loss function for a single prediction, and predictions of a ﬁxed
decision function f on the test sample are given by (cid:98)Y T
j ), then the empirical

j = f ( (cid:98)P T

j , Y T

X , X T

5

average loss incurred on the test sample is

L(ST , f ) :=

(cid:96)( (cid:98)Y T

j , Y T

j ) .

1
nT

nT(cid:88)

j=1

Based on this, we deﬁne the risk of a decision function as the average of the above quantity
when test samples are drawn according to the same mechanism as the training samples:

E(f ) := E

ST ∼PS

(cid:2)L(ST , f )(cid:3) = E

ST ∼PS

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:96)(f ( (cid:98)P T

X , X T

(cid:35)
i ), Y T
i )

.

In a similar way, we deﬁne the empirical risk of a decision function as its average prediction
error over the training samples:

(cid:98)E(f, N ) :=

L(Si, f ) =

(cid:96)(f ( (cid:98)P (i)

X , Xij), Yij).

(2)

1
N

N
(cid:88)

i=1

1
N

N
(cid:88)

ni(cid:88)

1
ni

i=1

j=1

Remark 3 It is possible to understand the above setting as a particular instance of a struc-
tured output learning problem (Tsochantaridis et al., 2005; Bakır et al., 2007), in which
the input variable X ∗ is (cid:98)P T
X , and the “structured output” Y ∗ is the collection of labels
(Y T
i )1≤i≤nT (matched to their respective input points). As is generally the case for struc-
tured output learning, the nature of the problem and the “structure” of the outputs is very
much encoded in the particular form of the loss function. In our setting the loss function is
additive over the labels forming the collection Y ∗, and we will exploit this particular form
for our method and analysis.

Remark 4 The risk E(f ) deﬁned above can be described in the following way: consider the
random variable ξ := ( (cid:98)PXY ; (X, Y )) obtained by ﬁrst drawing (cid:98)PXY according to PS, then,
conditional to this, drawing (X, Y ) according to (cid:98)PXY . The risk is then the expectation of a
certain function of ξ (namely Ff (ξ) = (cid:96)(f ( (cid:98)PX , X), Y )). In probability theory literature, the
distribution of the variable ξ is known as the Campbell measure associated to the distribution
PS over the measure space PX ×Y ; this object is in particular of fundamental use in point
process theory (see, e.g., Daley and Vere-Jones (2008), Section 13.1). We will denote
it by C(PS) here. This intriguing connection suggests that more elaborate tools of point
process literature may ﬁnd their use to analyze DG when various classical point processes
are considered for the generating distribution. The Campbell measure will also appear in
the Rademacher analysis below.

The next result establishes an analogue of classical Rademacher analysis under the

agnostic generative model.

Theorem 5 (Uniform estimation error control under (AGM)) Let F be a class of
decision functions PX × X → R. Assume the following boundedness condition holds:

sup
f ∈F

sup
PX ∈PX

sup
(x,y)∈X ×Y

(cid:96)(f (PX , x), y) ≤ B(cid:96).

(3)

6

Under (AGM), if S1, . . . , SN are i.i.d. realizations from PS, then with probability at least
1 − δ with respect to the draws of the training samples:

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:98)E(f, N ) − E(f )
(cid:12)

sup
f ∈F

≤

E

2
N

( (cid:98)P (i)

XY ;(Xi,Yi))∼C(PS )⊗N

E(εi)1≤i≤N

(cid:34)

sup
f ∈F

(cid:12)
N
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)

i=1

εi(cid:96)(f ( (cid:98)P (i)

(cid:35)

(cid:12)
(cid:12)
(cid:12)
X , Xi), Yi)
(cid:12)
(cid:12)

(cid:114)

+ B(cid:96)

log(δ−1)
2N

,

(4)

where (εi)1≤i≤N are i.i.d. Rademacher variables, independent from ( (cid:98)P (i)

XY , (Xi, Yi))1≤i≤N .

(cid:12)
(cid:12)
(cid:12)
(cid:12)
Proof Since the (Si)1≤i≤N are i.i.d., supf ∈F
(cid:12) takes the form of a uniform
(cid:12) (cid:98)E(f, N ) − E(f )
deviation between average and expected loss over the function class F. We can therefore
apply standard analysis (Azuma-McDiarmid inequality followed by Rademacher complexity
analysis for a nonnegative bounded loss; see, e.g., Koltchinskii, 2001; Bartlett and Mendel-
son, 2002, Theorem 8) to obtain that with probability at least 1 − δ with respect to the
draw of the training samples (Si)1≤i≤N :

(cid:12)
(cid:12)
(cid:12) (cid:98)E(f, N ) − E(f )

(cid:12)
(cid:12)
(cid:12) ≤

2
N

sup
f ∈F

E(Si)1≤i≤N

E(εi)1≤i≤N

(cid:34)

sup
f ∈F

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

N
(cid:88)

i=1

(cid:35)

(cid:12)
(cid:12)
(cid:12)
εiL(Si, f )
(cid:12)
(cid:12)

(cid:114)

+ B(cid:96)

log(δ−1)
2N

,

where (εi)1≤i≤N are i.i.d. Rademacher variables, independent of (Si)1≤i≤N .

We may write

L(Si, f ) =

1
ni

ni(cid:88)

j=1

(cid:96)(f ( (cid:98)P (i)

X , Xij), Yij) = E

(X,Y )∼ (cid:98)P (i)
XY

(cid:104)

(cid:96)(f ( (cid:98)P (i)

(cid:105)
X , X), Y )

;

thus, we have

E(Si)1≤i≤N

E(εi)1≤i≤N

εiL(Si, f )

(cid:34)

sup
f ∈F

(cid:12)
N
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)

i=1
(cid:34)

(cid:12)
(cid:35)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= E

( (cid:98)P (i)

XY )1≤i≤N

E(εi)1≤i≤N

(cid:12)
N
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)

i=1

sup
f ∈F

εiE

(Xi,Yi)∼ (cid:98)P (i)
XY

(cid:104)
(cid:96)(f ( (cid:98)P (i)

(cid:105)
X , Xi), Yi)

(cid:12)
(cid:35)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ E

E

( (cid:98)P (i)

XY )1≤i≤N

(X1,Y1)∼ (cid:98)P (1)

XY ,...,(XN ,YN )∼ (cid:98)P (N )

XY

E(εi)1≤i≤N

(cid:34)

sup
f ∈F

(cid:12)
N
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)

i=1

εi(cid:96)(f ( (cid:98)P (i)

(cid:35)

(cid:12)
(cid:12)
(cid:12)
X , Xi), Yi)
(cid:12)
(cid:12)

.

In the above inequality, the inner expectation on the (Xi, Yi) is pulled outwards by Jensen’s
inequality and convexity of the supremum.

To obtain the announced estimate, notice that the above expectation is the same as the

expectation w.r.t. to the N -fold Campbell measure C(PS) (see Remark 4).

Remark 6 The main term in the theorem is just the conventional Rademacher complexity
for the augmented feature space PX × X endowed with the Campbell measure C(PS). It
could also be thought of as the Rademacher complexity for the meta-distribution PS.

7

3.4 Risk under the 2-stage generative model

While we will state more results holding under (AGM) below, one advantage of the more
speciﬁc (2SGM) is to allow us to study the eﬀect of the sample sizes ni. In particular, for
the purpose of reducing computational complexity, we can analyze the eﬀect of subsampling
observations inside a given sample. Such an analysis would not be possible under (AGM).
For the test sample, parallel to the training data generating mechanism, under (2SGM)
XY , nT ) is drawn according to µ⊗ν, and conditional to this the test sample

we assume that (P T
ST is drawn from nT i.i.d. realizations of P T

Observe that under (2SGM), the distribution P (i)

XY is not observed and is therefore a
latent variable at training time as well as at test time. Still, by the law of large numbers,
if nT becomes large, (cid:98)P T
X (in the sense of weak convergence). This
motivates the introduction of the following risk which assumes access to an inﬁnite test
sample, and thus the true marginal P T
X :
E

X will converge to P T

(cid:2)(cid:96)(f (P T

X , X T ), Y T )(cid:3) .

E ∞(f ) := E

(X T ,Y T )∼P T

(5)

XY .

P T

XY ∼µ

XY

This quantity only makes sense under (2SGM). The following proposition makes the above
motivating observation more precise. First, under (2SGM), introduce the following risk
conditional to a ﬁnite test sample size nT :

E(f |nT ) = E

E

P T

XY ∼µ

(X T

i ,Y T

i )1≤i≤nT

∼(P T

XY )⊗nT

(cid:96)(f ( (cid:98)P T

X , X T

i ), Y T
i )

.

(6)

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:35)

Proposition 7 Assume (cid:96) is a bounded, L-Lipschitz loss function and f : PX × X → R is
a ﬁxed decision function which is continuous with respect to both its arguments (recalling
PX is endowed with the weak convergence topology). Then it holds under (2GSM):

lim
nT →∞

E(f |nT ) = E ∞(f ).

Remark 8 This result provides one setting where the risk E ∞ is clearly motivated as the
goal of asymptotic analysis when nT → ∞. Although Proposition 7 is not used elsewhere in
this work, a more quantitative version of this result is stated below for kernels (see Theorem
15), where convergence holds uniformly and the assumption of a bounded loss is dropped.

To gain more insight into the risk E ∞, recalling the standard decomposition (1) of
PXY into the marginal PX and the posterior PY |X , we observe that we can apply the
disintegration theorem not only to any PXY , but also to µ, and thus decompose it into two
parts, µX which generates the marginal distribution PX , and µY |X which, conditioned on
PX , generates the posterior PY |X . (More precise notation might be µPX instead of µX and
µPY |X |PX instead of µY |X , but this is rather cumbersome.) Denote (cid:101)X = (PX , X). We then
have, using Fubini’s theorem,

E ∞(f ) = EPX ∼µX

EPY |X ∼µY |X

EX∼PX

EY ∼PY |X

= EPX ∼µX

EPY |X ∼µY |X

EY ∼PY |X

EX∼PX
(cid:104)

= E

( (cid:101)X,Y )∼Qµ

(cid:96)(f ( (cid:101)X), Y )

.

(cid:105)

(cid:104)
(cid:96)(f ( (cid:101)X), Y )
(cid:104)
(cid:96)(f ( (cid:101)X), Y )

(cid:105)

(cid:105)

8

Here Qµ is the distribution that generates (cid:101)X by ﬁrst drawing PX according to µX , and then
drawing X according to PX ; similarly, Y is generated, conditioned on (cid:101)X, by ﬁrst drawing
PY |X according to µY |X , and then drawing Y from PY |X . (The distribution of (cid:101)X again
takes the form of a Campbell measure, see Remark 4.)

From the previous expression, we see that the risk E ∞ is like a standard supervised
learning risk based on ( (cid:101)X, Y ) ∼ Qµ. Thus, we can deduce properties that are known
to hold for supervised learning risks. For example, in the binary classiﬁcation setting, if
the loss is the 0/1 loss, then f ∗( (cid:101)X) = 2˜η( (cid:101)X) − 1 is an optimal predictor, where ˜η( (cid:101)X) =
E

(cid:2)1{Y =1}

(cid:3), and

Y ∼Qµ

Y | (cid:101)X

E ∞(f ) − E ∞(f ∗) = E

(cid:101)X∼Qµ
(cid:101)X

(cid:105)
(cid:104)
1{sign(f ( (cid:101)X))(cid:54)=sign(f ∗( (cid:101)X))}|2˜η( (cid:101)X) − 1|

.

Furthermore, consistency in the sense of E ∞ with respect to a general loss (cid:96) (thought of as
a surrogate) will imply consistency for the 0/1 loss, provided (cid:96) is classiﬁcation calibrated
(Bartlett et al., 2006).

X . There is, however, a condition where for µ-almost all test distributions P T

For a given loss (cid:96), the optimal or Bayes E ∞-risk in DG is in general larger than the
expected Bayes risk under the (random) test sample generating distribution P T
XY , because
it is typically not possible to fully determine the Bayes-optimal predictor from only the
marginal P T
XY ,
the decision function f ∗(P T
X , .) (where f ∗ is a global minimizer of (5)) coincides with an
optimal Bayes decision function for P T
XY . This condition is simply that the posterior PY |X
is (µ-almost surely) a function of PX (in other terms: that with the notation introduced
above, µY |X (PX ) is a Dirac measure for µ-almost all PX ). Although we will not be assuming
this condition throughout the paper under (2SGM), observe that it is implicitly assumed
in the motivating application presented in Section 2, where an expert labels the data points
by just looking at their marginal distribution.

Lemma 9 For a ﬁxed distribution PXY , and a decision function g : X → R, let us denote
R(g, PXY ) = E(X,Y )∼PXY [(cid:96)(g(X), Y )] and

R∗(PXY ) := min
g:X →R

R(g, PXY ) = min
g:X →R

E(X,Y )∼PXY [(cid:96)(g(X), Y )]

the corresponding optimal (Bayes) risk for the loss function (cid:96) under data distribution PXY .
Then under (2SGM):

E ∞(f ∗) ≥ EPXY ∼µ [R∗(PXY )] ,

where f ∗ : PX × X → R is a minimizer of the idealized DG risk E ∞ deﬁned in (5).

Furthermore, if µ is a distribution on PX ×Y such that µ-a.s. it holds PY |X = F (PX )

for some deterministic mapping F , then for µ-almost all PXY :

and

R(f ∗(PX , .), PXY ) = R∗(PXY )

E ∞(f ∗) = EPXY ∼µ [R∗(PXY )] .

9

Proof For any f : PX × X → R, one has for all PXY : R(f (PX , .), PXY ) ≥ R∗(PXY ).
Taking expectation with respect to PXY establishes the ﬁrst claim. Now for any ﬁxed
PX ∈ PX , consider PXY := PX • F (PX ) and g∗(PX ) a Bayes decision function for this joint
distribution. Pose f (PX , x) := g∗(PX )(x). Then f coincides for µ-almost all PXY with a
Bayes decision function for PXY , achieving equality in the above inequality. The second
equality follows by taking expectation over PXY ∼ µ.

Under (2SGM), we will establish that our proposed learning algorithm is E ∞-consistent,
provided the average sample size grows to inﬁnity as well as the total number of samples.
Thus, the above result provides a condition on µ under which it is possible to asymptotically
attain the Bayes risk on any test distribution although no labels from this test distribution
are observed.

More generally, and speaking informally, if µ is such that PY |X is close to being a function
of PX in some sense, we can expect the Bayes E ∞ risk for domain generalization to be close
to the expected Bayes risk for a random test distribution. We reiterate, however, that we
make no assumptions on µ in this work so that the two quantities may be far apart. In
the worst case, the posterior may be independent of the marginal, in which case a method
for domain generalization will do no better than the na¨ıve pooling strategy. For further
discussion, see the comparison of domain adaptation and domain generalization in the next
section.

4. Related Work

Since at least the 1990s, machine learning researchers have investigated the possibility of
solving one learning problem by leveraging data from one or more related problems. In this
section, we provide an overview of such problems and their relation to domain generalization,
while also reviewing prior work on DG.

Two critical terms are domain and task. Use of these terms is not consistent throughout
the literature, but at a minimum, the domain of a learning problem describes the input
(feature) space X and marginal distribution of X, while the task describes the output space
Y and the conditional distribution of Y given X (also called posterior). In many settings,
however, the sets X and Y are the same for all learning problems, and the terms “domain”
and “task” are used interchangeably to refer to a joint distribution PXY on X × Y. This is
the perspective adopted in this work, as well as in much of the work on multi-task learning,
domain adaptation (DA), and domain generalization.

Multi-task learning is similar to DG, except only the training tasks are of interest,
and the goal is to leverage the similarity among distributions to improve the learning of
individual predictors for each task (Caruana, 1997; Evgeniou et al., 2005; Yang et al., 2009).
In contrast, in DG, we are concerned with generalization to a new task.

Domain adaptation refers to the setting in which there is a speciﬁc target task and one
or more source tasks. The goal is to design a predictor for the target task, for which there
are typically few to no labeled training examples, by leveraging labeled training data from
the source task(s). DA is reviewed below, and contrasted with DG.

10

4.1 Domain Generalization vs. Domain Adaptation

Formulations of domain adaptation may take several forms, depending on the number of
sources and whether there are any labeled examples from the target to supplement the
unlabeled examples. In multi-source, unsupervised domain adaptation, the learner is pre-
sented with labeled training data from several source distributions, and unlabeled data from
a target marginal distribution (see Zhang et al. (2015) and references therein). Thus, the
available data are the same as in domain generalization, and algorithms for one of these
problems may be applied to the other. In this section we illuminate the diﬀerence between
DA and DG.

In all forms of DA, the goal is to attain optimal performance with respect to the joint
distribution of the target domain. For example, if the performance measure is a risk, the goal
is to attain the Bayes risk for the target domain. To achieve this goal, it is necessary to make
assumptions about how the source and target distributions are related (Quionero-Candela
et al., 2009). For example, several works adopt the covariate shift assumption, which
requires the source and target domains to have the same posterior, allowing the marginals
to diﬀer arbitrarily (Zadrozny, 2004; Huang et al., 2007; Cortes et al., 2008; Sugiyama
et al., 2008; Bickel et al., 2009; Kanamori et al., 2009; Yu and Szepesvari, 2012; Ben-David
and Urner, 2012). Another common assumption is target shift, which stipulates that the
source and target have the same class-conditional distributions, allowing the prior class
probability to change (Hall, 1981; Titterington, 1983; Latinne et al., 2001; Storkey, 2009;
Du Plessis and Sugiyama, 2012; Sanderson and Scott, 2014; Azizzadenesheli et al., 2019).
Mansour et al. (2009b); Zhang et al. (2015) assume that the target posterior is a weighted
combination of source posteriors, while Zhang et al. (2013); Gong et al. (2016) extend target
shift by also allowing the class-conditional distributions to undergo a location-scale shift,
and Tasche (2017) assumes the ratio of class-conditional distributions is unchanged. Work
on classiﬁcation with label noise assumes the source data are obtained from the target
distribution but the labels have been corrupted in either a label-dependent (Blanchard
et al., 2016; Natarajan et al., 2018; van Rooyen and Williamson, 2018) or feature-dependent
(Menon et al., 2018; Cannings et al., 2018; Scott, 2019) way. Finally, there are several works
that assume the existence of a predictor that achieves good performance on both source and
target domains (Ben-David et al., 2007, 2010; Blitzer et al., 2008; Mansour et al., 2009a;
Cortes et al., 2015; Germain et al., 2016).

The key diﬀerence between DG and DA may be found in the performance measures
optimized. In DG, the goal is to design a single predictor f (PX , x) that can apply to any
future task, and risk is assessed with respect to the draw of both a new task, and (under
2SGM) a new data point from that task. This is in contrast to DA, where the target
distribution is typically considered ﬁxed, and the goal is to design a predictor f (x) where, in
assessing the risk, the only randomness is in the draw of a new sample from the target task.
This diﬀerence in performance measures for DG and DA has an interesting consequence
for analysis. As we will show, it is possible to attain optimal risk (asymptotically) in DG
without making any distributional assumptions like those described above for DA. Of course,
this optimal risk is typically larger than the Bayes risk for any particular target domain
(see Lemma 9). An interesting question for future research is whether it is possible to

11

close or eliminate this gap (between DG and expected DA risks) by imposing distributional
assumptions like those for DA.

Another diﬀerence between DA and DG lies in whether the learning algorithm must be
rerun for each new test data set. Most unsupervised DA methods employ the unlabeled
target data for training and thus, when a new unlabeled target data set is presented, the
learning algorithm must be rerun. In contrast, most existing DG methods do not assume
access to the unlabeled test data at learning time, and are capable of making predictions
as new unlabeled data sets arrive without any further training.

4.2 Domain Generalization vs. Learning to Learn

In the problem of learning to learn (LTL, Thrun, 1996), which has also been called bias
learning, meta-learning, and (typically in an online setting) lifelong learning, there are la-
beled datasets for several tasks, as in DG. There is also a given family of learning algorithms,
and the objective is to design a meta-learner that selects the learning algorithm that will
perform best on future tasks. The learning theoretic study of LTL traces to the work of
Baxter (2000), who was the ﬁrst to propose a distribution on tasks, which he calls an “en-
vironment,” and which coincides with our µ. Given this setting, the performance of the
learning algorithm selected by a meta-learner is obtained by drawing a new task at random,
drawing a labeled training dataset from that task, running the selected algorithm, drawing
a test point, and evaluating the expected loss, where the expectation is with respect to all
sources of randomness (new task, training data from new task, test point from new task).

Baxter analyzes learning algorithms given by usual empirical risk minimization over a
hypothesis (prediction function) class, and the goal of the meta-learner is then to select a
hypothesis class from a family of such classes. He shows that it is possible to ﬁnd a good
trade-oﬀ between the complexity of a hypothesis class and its approximation capabilities
for tasks sampled from µ, in an average sense. In particular, the information gained by
ﬁnding a well-adapted hypothesis class can lead to signiﬁcantly improved sample eﬃciency
when learning a new task. See Maurer (2009) for a discussion of the performance measure
studied by Baxter (2000), which is slightly diﬀerent from the one described above.

Later work on LTL establishes similar results that quantify the ability of a meta-learner
to transfer knowledge to a new task. These meta-learners all optimize a particular structure
that deﬁnes a learning algorithm, such as a feature representation (Maurer, 2009; Maurer
et al., 2016; Denevi et al., 2018a), a prior on predictors in a PAC-Bayesian setting (Pentina
and Lampert, 2014), a dictionary (Maurer et al., 2013), the bias of a regularizer (Denevi
et al., 2018b), and a pretrained neural network (Finn et al., 2017). It is also worth not-
ing that some algorithms on multi-task learning extract structures that characterize an
environment and can be applied to LTL.

Although DG and LTL both involve generalization to a new task, they are clearly
diﬀerent problems because LTL assumes access to labeled data from the new task, whereas
DG only sees unlabeled data and requires no additional learning. In LTL, the learner can
achieve the Bayes risk for the new task, the only issue is the sample complexity. DG is
thus a more challenging problem, but also potentially more useful since in many transfer
learning settings, labeled data for the new task are unavailable.

12

4.3 Prior Work on Domain Generalization

To our knowledge, the ﬁrst paper to consider domain generalization (as formulated in Sec-
tion 3.2) was our earlier conference paper (Blanchard et al., 2011). The term “domain
generalization” was coined by Muandet et al. (2013), who study the same setting and build
upon our work by extracting features that facilitate DG. Carbonell et al. (2013) study an
active learning variant of DG in the realizable setting, and directly learn the task sampling
distribution.

Other methods for DG were studied by Khosla et al. (2012); Xu et al. (2014); Grubinger
et al. (2015); Ghifary et al. (2015); Gan et al. (2016); Ghifary et al. (2017); Motiian et al.
(2017); Li et al. (2017, 2018a,b,c,d); Balaji et al. (2018); Ding and Fu (2018); Shankar
et al. (2018); Hu et al. (2019); Dou et al. (2019); Carlucci et al. (2019); Wang et al. (2019);
Akuzawa et al. (2019). Many of these methods learn a common feature space for all tasks.
Such methods are complementary to the method that we study. Indeed, our kernel-based
learning algorithm may be applied after having learned a feature representation by another
method, as was done by Muandet et al. (2013). Since our interest is primarily theoretical,
we restrict our experimental comparison to another algorithm that also operates directly on
the original input space, namely, a simple pooling algorithm that lumps all training tasks
into a single data set and trains a single support vector machine.

5. Learning Algorithm

In this section, we introduce a concrete algorithm to tackle the learning problem exposed in
Section 3, using an approach based on kernels. The function k : Ω×Ω → R is called a kernel
on Ω if the matrix (k(xi, xj))1≤i,j≤n is symmetric and positive semi-deﬁnite for all positive
integers n and all x1, . . . , xn ∈ Ω. It is well known that every kernel k on Ω is associated to
a space of functions f : Ω → R called the reproducing kernel Hilbert space (RKHS) Hk with
kernel k. One way to envision Hk is as follows. Deﬁne Φ(x) := k(·, x), which is called the
canonical feature map associated with k. Then the span of {Φ(x) : x ∈ Ω}, endowed with
the inner product (cid:104)Φ(x), Φ(x(cid:48))(cid:105) = k(x, x(cid:48)), is dense in Hk. We also recall the reproducing
property, which states that (cid:104)f, Φ(x)(cid:105) = f (x) for all f ∈ Hk and x ∈ Ω.

For later use, we introduce the notion of a universal kernel. A kernel k on a compact
metric space Ω is said to be universal when its RKHS is dense in C(Ω), the set of continuous
functions on Ω, with respect to the supremum norm. Universal kernels are important for
establishing universal consistency of many learning algorithms. See Steinwart and Christ-
mann (2008) for background on kernels and reproducing kernel Hilbert spaces.

Several well-known learning algorithms, such as support vector machines and kernel
ridge regression, may be viewed as minimizers of a norm-regularized empirical risk over
the RKHS of a kernel. A similar development has also been made for multi-task learning
Inspired by this framework, we consider a general kernel-based
(Evgeniou et al., 2005).
algorithm as follows.

Consider the loss function (cid:96) : R × Y → R+. Let k be a kernel on PX × X , and let Hk
be the associated RKHS. For the sample Si, recall that (cid:98)P (i)
j=1 δXij denotes the
corresponding empirical X distribution. Also consider the extended input space PX × X
X , Xij). Note that (cid:98)P (i)
and the extended data (cid:101)Xij = ( (cid:98)P (i)
X plays a role analogous to the task

X = 1
ni

(cid:80)ni

13

index in multi-task learning. Now deﬁne

(cid:98)fλ = arg min

f ∈Hk

1
N

N
(cid:88)

ni(cid:88)

1
ni

i=1

j=1

(cid:96)(f ( (cid:101)Xij), Yij) + λ (cid:107)f (cid:107)2 .

Algorithms for solving (7) will be discussed in Section 7.

5.1 Specifying the Kernels

In the rest of the paper we will consider a kernel k on PX × X of the product form

k((P1, x1), (P2, x2)) = kP (P1, P2)kX (x1, x2),

where kP is a kernel on PX and kX a kernel on X .

Furthermore, we will consider kernels on PX of a particular form. Let k(cid:48)

X denote a
kernel on X (which might be diﬀerent from kX ) that is measurable and bounded. We deﬁne
the kernel mean embedding Ψ : PX → Hk(cid:48)

:

PX (cid:55)→ Ψ(PX ) :=

k(cid:48)
X (x, ·)dPX (x).

X

(cid:90)

X

This mapping has been studied in the framework of “characteristic kernels” (Gretton et al.,
2007a), and it has been proved that universality of k(cid:48)
X implies injectivity of Ψ (Gretton
et al., 2007b; Sriperumbudur et al., 2010).

Note that the mapping Ψ is linear. Therefore, if we consider the kernel kP (PX , P (cid:48)

(cid:104)Ψ(PX ), Ψ(P (cid:48)
reason, we introduce yet another kernel K on Hk(cid:48)

X ) =
X )(cid:105), it is a linear kernel on PX and cannot be a universal kernel. For this
and consider the kernel on PX given by

X

kP (PX , P (cid:48)

X ) = K (cid:0)Ψ(PX ), Ψ(P (cid:48)

X )(cid:1) .

Note that particular kernels inspired by the ﬁnite dimensional case are of the form
K(v, v(cid:48)) = F ((cid:13)

(cid:13)v − v(cid:48)(cid:13)
(cid:13)),

or

K(v, v(cid:48)) = G((cid:10)v, v(cid:48)(cid:11)),

where F, G are real functions of a real variable such that they deﬁne a kernel. For exam-
ple, F (t) = exp(−t2/(2σ2)) yields a Gaussian-like kernel, while G(t) = (1 + t)d yields a
polynomial-like kernel. Kernels of the above form on the space of probability distributions
over a compact space X have been introduced and studied in Christmann and Steinwart
(2010). Below we apply their results to deduce that k is a universal kernel for certain choices
of kX , k(cid:48)

X , and K.

5.2 Relation to Other Kernel Methods

By choosing k diﬀerently, one can recover other existing kernel methods.
consider the class of kernels of the same product form as above, but where

In particular,

(7)

(8)

(9)

(10)

(11)

(12)

kP (PX , P (cid:48)

X ) =

(cid:26) 1 PX = P (cid:48)
X
τ PX (cid:54)= P (cid:48)
X

14

If τ = 0, the algorithm (7) corresponds to training N kernel machines f ( (cid:98)P (i)
X , ·) using kernel
kX (e.g., support vector machines in the case of the hinge loss) on each training data set,
independently of the others (note that this does not oﬀer any generalization ability to a new
data set). If τ = 1, we have a “pooling” strategy that, in the case of equal sample sizes ni,
is equivalent to pooling all training data sets together in a single data set, and running a
conventional supervised learning algorithm with kernel kX (i.e., this corresponds to trying
to ﬁnd a single “one-ﬁts-all” prediction function which does not depend on the marginal).
In the intermediate case 0 < τ < 1, the resulting kernel is a “multi-task kernel,” and the
algorithm recovers a multitask learning algorithm like that of Evgeniou et al. (2005). We
compare to the pooling strategy below in our experiments. We also examined the multi-
task kernel with τ < 1, but found that, as far as generalization to a new unlabeled task is
concerned, it was always outperformed by pooling, and so those results are not reported.
This ﬁts the observation that the choice τ = 0 does not provide any generalization to a
new task, while τ = 1 at least oﬀers some form of generalization, if only by ﬁtting the same
predictor to all data sets.

In the special case where all labels Yij are the same value for a given task, and kX is
taken to be the constant kernel, the problem we consider reduces to “distributional” classi-
ﬁcation or regression, which is essentially standard supervised learning where a distribution
(observed through a sample) plays the role of the feature vector. Many of our analysis
techniques specialize to this setting.

6. Learning Theoretic Study

This section presents generalization error and consistency analysis for the proposed kernel
method under the agnostic and 2-stage generative models. Although the regularized esti-
mation formula (7) deﬁning (cid:98)fλ is standard, the generalization error analysis is not, owing
to the particular sampling structures and risks under (AGM) and (2SGM).

6.1 Universal Consistency under the Agnostic Generative Model

We will consider the following assumptions on the loss function and kernels:

(LB) The loss function (cid:96) : R × Y → R+ is L(cid:96)-Lipschitz in its ﬁrst variable and satisﬁes

(K-Bounded) The kernels kX , k(cid:48)

X and K are bounded respectively by constants B2

k, B2

k(cid:48) ≥

B0 := supy∈Y (cid:96)(0, y) < ∞.

1, and B2
K .

The condition B0 < ∞ always holds for classiﬁcation, as well as certain regression
settings. The boundedness assumptions are clearly satisﬁed for Gaussian kernels, and can
be enforced by normalizing the kernel (discussed further below).

We begin with a generalization error bound that establishes uniform estimation error
control over functions belonging to a ball of Hk . We then discuss universal kernels, and
ﬁnally deduce universal consistency of the algorithm.

Let Bk(r) denote the closed ball of radius r, centered at the origin, in the RKHS of the
kernel k. We start with the following simple result allowing us to bound the loss on a RKHS
ball.

15

Lemma 10 Suppose k is a kernel on a set Ω, bounded by B2. Let (cid:96) : R × Y → [0, ∞) be a
loss satisfying (LB). Then for any R > 0 and f ∈ Bk(R), and any z ∈ Ω and y ∈ Y,

(cid:12)(cid:96)(f (z), y)(cid:12)
(cid:12)

(cid:12) ≤ B0 + L(cid:96)RB

(13)

Proof By the Lipschitz continuity of (cid:96), the reproducing property, and Cauchy-Schwarz,
we have

(cid:12)(cid:96)(f (z), y)(cid:12)
(cid:12)

(cid:12)(cid:96)(f (z), y) − (cid:96)(0, y)(cid:12)
(cid:12)

(cid:12) ≤ (cid:96)(0, y) + (cid:12)
≤ B0 + L(cid:96)|f (z) − 0|
(cid:12)(cid:104)f, k(z, ·)(cid:105)(cid:12)
(cid:12)
= B0 + L(cid:96)
(cid:12)
≤ B0 + L(cid:96)(cid:107)f (cid:107)Hk B
≤ B0 + L(cid:96)RB.

The expression in (13) serves to replace the boundedness assumption (3) in Theorem 5.

We now state the following, which is a specialization of Theorem 5 to the kernel setting.

Theorem 11 (Uniform estimation error control over RKHS balls) Assume (LB)
and (K-Bounded) hold, and data generation follows (AGM). Then for any R > 0, with
probability at least 1 − δ (with respect to the draws of the samples Si, i = 1, . . . , N )

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ≤ (B0 + L(cid:96)RBKBk)
(cid:12) (cid:98)E(f, N ) − E(f )

((cid:112)log δ−1 + 2)
√
N

.

sup
f ∈Bk(R)

(14)

Proof This is a direct consequence of Theorem 5 and of Lemma 10, the kernel k on
PX × X being bounded by B2
K. As noted there, the main term in the upper bound (4)
is a standard Rademacher complexity on the augmented input space P × X , endowed with
the Campbell measure C(PS).

kB2

In the kernel learning context, we can bound the Rademacher complexity term using
a standard bound for the Rademacher complexity of a Lipschitz loss function on the ball
of radius R of Hk (Koltchinskii, 2001; Bartlett and Mendelson, 2002, e.g., Theorems 8, 12
and Lemma 22 there), using again the bound B2

K on the kernel k, giving the conclusion.

kB2

Next, we turn our attention to universal kernels (see Section 5 for the deﬁnition). A
relevant notion for our purposes is that of a normalized kernel. If k is a kernel on Ω, then

k∗(x, x(cid:48)) :=

k(x, x(cid:48))
(cid:112)k(x, x)k(x(cid:48), x(cid:48))

is the associated normalized kernel. If a kernel is universal, then so is its associated normal-
ized kernel. For example, the exponential kernel k(x, x(cid:48)) = exp(κ(cid:104)x, x(cid:48)(cid:105)Rd), κ > 0, can be
shown to be universal on Rd through a Taylor series argument. Consequently, the Gaussian
kernel

kσ(x, x(cid:48)) :=

exp( 1

σ2 (cid:104)x, x(cid:48)(cid:105))
2σ2 (cid:107)x(cid:107)2) exp( 1

exp( 1

2σ2 (cid:107)x(cid:48)(cid:107)2)

16

is universal, being the normalized kernel associated with the exponential kernel with κ =
1/σ2. See Steinwart and Christmann (2008) for additional details and discussion.
To establish that k is universal on PX × X , the following lemma is useful.

Lemma 12 Let Ω, Ω(cid:48) be two compact spaces and k, k(cid:48) be kernels on Ω, Ω(cid:48), respectively. If
k, k(cid:48) are both universal, then the product kernel

k((x, x(cid:48)), (y, y(cid:48))) := k(x, y)k(cid:48)(x(cid:48), y(cid:48))

is universal on Ω × Ω(cid:48).

Several examples of universal kernels are known on Euclidean space. For our purposes,
we also need universal kernels on PX . Fortunately, this was studied by Christmann and
Steinwart (2010). Some additional assumptions on the kernels and feature space are re-
quired:

(K-Univ) kX , k(cid:48)

X , K, and X satisfy the following:

• X is a compact metric space

• kX is universal on X
• k(cid:48)

X is continuous and universal on X

• K is universal on any compact subset of Hk(cid:48)

.

X

Adapting the results of Christmann and Steinwart (2010), we have the following.

Theorem 13 (Universal kernel) Assume condition (K-Univ) holds. Then, for kP de-
ﬁned as in (10), the product kernel k in (8) is universal on PX × X .

Furthermore, the assumption on K is fulﬁlled if K is of the form (12), where G is an
analytical function with positive Taylor series coeﬃcients, or if K is the normalized kernel
associated to such a kernel.

Proof By Lemma 12, it suﬃces to show PX is a compact metric space, and that kP (PX , P (cid:48)
X )
is universal on PX . The former statement follows from Theorem 6.4 of Parthasarathy
(1967), where the metric is the Prohorov metric. We will deduce the latter statement from
Theorem 2.2 of Christmann and Steinwart (2010). The statement of Theorem 2.2 there
is apparently restricted to kernels of the form (12), but the proof actually only uses that
the kernel K is universal on any compact set of Hk(cid:48)
. To apply Theorem 2.2, it remains
is a separable Hilbert space, and that Ψ is injective and continuous.
to show that Hk(cid:48)
Injectivity of Ψ is equivalent to k(cid:48)
X being a characteristic kernel, and follows from the
assumed universality of k(cid:48)
X (Sriperumbudur et al., 2010). The continuity of k(cid:48)
X implies
(Steinwart and Christmann (2008), Lemma 4.33) as well as continuity of
separability of Hk(cid:48)
Ψ (Christmann and Steinwart (2010), Lemma 2.3 and preceding discussion). Now Theorem
2.2 of Christmann and Steinwart (2010) may be applied, and the results follows.

X

X

X

The fact that kernels of the form (12), where G is analytic with positive Taylor coeﬃ-
was established in the proof of Theorem

cients, are universal on any compact set of Hk(cid:48)
2.2 of the same work (Christmann and Steinwart, 2010).

X

17

As an example, suppose that X is a compact subset of Rd. Let kX and k(cid:48)

X be Gaussian
kernels on X . Taking G(t) = exp(t), it follows that K(PX , P (cid:48)
)
is universal on PX . By similar reasoning as in the ﬁnite dimensional case, the Gaussian-like
kernel K(PX , P (cid:48)
) is also universal on PX . Thus the
product kernel is universal on PX × X .

X ) = exp((cid:104)Ψ(PX ), Ψ(P (cid:48)

2σ2 (cid:107)Ψ(PX ) − Ψ(P (cid:48)

X ) = exp(− 1

X )(cid:105)Hk(cid:48)

X )(cid:107)2

Hk(cid:48)
X

X

From Theorems 11 and 13, we may deduce universal consistency of the learning algo-

rithm.

Corollary 14 (Universal consistency) Assume that conditions (LB), (K-Bounded)
and (K-Univ) are satisﬁed. Let λ = λ(N ) be a sequence such that as N → ∞: λ(N ) → 0
and λ(N )N/ log N → ∞. Then

E( (cid:98)fλ(N )) → inf

f :PX ×X →R

E(f ) a.s., as N → ∞.

The proof of the corollary relies on the bound established in Theorem 11, the universality
of k established in Theorem 13, and otherwise relatively standard arguments.

One notable feature of this result is that we have established consistency where only N
is required to diverge. In particular, the training sample sizes ni may remain bounded. In
the next subsection, we consider the role of the ni under the 2-stage generative model.

6.2 Role of the Individual Sample Sizes under the 2-Stage Generative Model

In this section, we are concerned with the role of the individual sample sizes (ni)1≤i≤N .
More precisely, in some applications the number of training points per task is large, which
can give rise to a high computational burden at the learning stage (and also for storing the
learned model in computer memory). We investigate to which extent reducing the number of
training points per task (by random subsampling) in order to reduce computational burden
can be done without suﬀering a signiﬁcant loss in statistical performance. For this we need
a more speciﬁc model for the generating model of points in each task, and we therefore
assume here that the (2SGM), introduced in Section 3.1, holds.

We will consider the following additional assumption.

(K-H¨older) The canonical feature map ΦK : Hk(cid:48)

→ HK associated to K satisﬁes a H¨older

X

condition of order α ∈ (0, 1] with constant LK, on Bk(cid:48)

(Bk(cid:48)) :

X

∀v, w ∈ Bk(cid:48)

(Bk(cid:48)) :

X

(cid:107)ΦK(v) − ΦK(w)(cid:107) ≤ LK (cid:107)v − w(cid:107)α .

(15)

Suﬃcient conditions for (15) are described in Section A.4. As an example, the condition is
shown to hold with α = 1 when K is the Gaussian-like kernel on Hk(cid:48)

.

Since we are interested in the inﬂuence of the number of training points per task, it
is helpful to introduce notations for the (2SGM) risks that are conditioned on a ﬁxed
task PXY . Thus, we introduce the following notation, in analogy to (5)–(6) introduced in

X

18

Section 3.4, for risk at sample size n, and risk at inﬁnite sample size, conditional to PXY :

E(f |PXY , n) := E

ST ∼(PXY )⊗n

(cid:96)(f ( (cid:98)PX , Xi), Yi)

;

(cid:34)

1
n

n
(cid:88)

i=1

(cid:35)

E ∞(f |PXY ) := E(X,Y )∼PXY [(cid:96)(f (PX , X), Y )] .

(16)

(17)

The following proposition gives an upper bound on the discrepancy between these risks.
It can be seen as a quantitative version of Proposition 7 in the kernel setting, which is
furthermore uniform over an RKHS ball.

If the
Theorem 15 Assume conditions (LB), (K-Bounded), and (K-H¨older) hold.
sample S = (Xj, Yj)1≤j≤n is made of n i.i.d. realizations from PXY , with PXY and n ﬁxed,
then for any R > 0, with probability at least 1 − δ:

sup
f ∈Bk(R)

|L(S, f ) − E ∞(f |PXY )| ≤ (B0 + 3L(cid:96)RBk(Bα

k(cid:48)LK + BK))

(cid:19)− α

2

(cid:18) log(3δ−1)
n

.

(18)

Averaging over the draw of S, again with PXY and n ﬁxed, it holds for any R > 0:

sup
f ∈Bk(R)

|E(f |PXY , n) − E ∞(f |PXY )| ≤ 2L(cid:96)RBkLKBα

k(cid:48)n−α/2.

(19)

As a consequence, for the unconditional risks when (PXY , n) is drawn from µ ⊗ ν under
(2GSM), for any R > 0:

|E(f ) − E ∞(f )| ≤ 2RL(cid:96)BkLKBα

k(cid:48)E

(cid:104)
n− α

2

(cid:105)

.

(20)

sup
f ∈Bk(R)

The above results are useful in a number of ways. First, under (2SGM), we can consider
the goal of asymptotically achieving the optimal risk inf f E ∞(f ), where we recall that E ∞(f )
is the expected loss of a decision function f over a random test task P T
XY in the case where
P T
X would be perfectly observed (this can be thought of as observing as an inﬁnite sample
from the marginal). Equation (20) bounds the risk under (2SGM) in terms of the risk
under (AGM), for which we have already established consistency. Thus, consistency under
(2SGM) will be possible if the number of examples ni per training task also grows together
with the number of training tasks N . The following result formalizes this intuition.

XY , . . . , P (N )

Corollary 16 Assume (LB), (K-Bounded), and (K-H¨older) hold, and let n be ﬁxed.
If P (1)
from µ, and the corresponding samples S1, . . . , SN are
i.i.d. all of size n from their respective distributions, then for any R > 0, with probability
at least 1 − δ with respect to the draws of the training tasks and training samples

XY are drawn i.i.d.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
N

N
(cid:88)

i=1

(cid:12)
(cid:12)
L(Si, f ) − E ∞(f )
(cid:12)
(cid:12)
(cid:12)

sup
f ∈Bk(R)

≤ (B0 + L(cid:96)RBKBk)

+ 2RL(cid:96)BkLKBα

k(cid:48)n− α
2 .

(21)

((cid:112)log δ−1 + 2)
√
N

19

Furthermore, if assumption (K-Univ) is satisﬁed and, as the number of training tasks
N → ∞, the sample size n = n(N ) → ∞, and the regularization parameter λ(N ) is such
that λ(N ) → 0 and λ(N ) min(N, nα) → ∞, then

E( (cid:98)fλ(N )) → inf

f :PX ×X →R

E ∞(f ) in probability, as N → ∞.

Proof The setting is that of the (2GSM) model, where the sample size is ﬁxed at n (i.e.
the sample size distribution ν is the Dirac δn). This is a particular case of (AGM), so we
can apply Theorem 11 and combine with (20) (wherein the test sample is also of size n) to
get the announced bound. The consistency statement follows the same argument as in the
proof of Corollary 14, with E(f ) replaced by E ∞(f ), and ε(N ) there replaced by the RHS
in (21). We consider only consistency in probability this time, since a.s. consistency does
not appear of relevance in a setting where the individual training task sample size grows
with the number of tasks.

Remark 17 Our conference paper (Blanchard et al., 2011) also established a generalization
error bound and consistency for E ∞ under (2SGM). That bound had a diﬀerent form for
two main reasons. First, it assumed the loss to be bounded, whereas the present analysis
avoids that assumption via Lemma 10. Second, that analysis did not leverage a connection
to (AGM), which led to a log N in the second term. This required the two sample sizes to
be coupled asymptotically to achieve consistency. In the present analysis, the two sample
sizes N and n may diverge at arbitrary rates.

Remark 18 It is possible to obtain a result similar to Corollary 16 when the training task
sample sizes (ni)1≤i≤N are unequal and possibly arbitrary. In this case we would follow a
slightly diﬀerent argument, using (18) for all training tasks together with a union bound,
and applying Theorem 11 to the idealized situation with an inﬁnite number of samples per
training task. This way, the order n− α
. We eschew
an exact statement for brevity.

2 is replaced by log(N )N −1 (cid:80)N

i=1 n

− α
2
i

Coming back to our initial motivation of possibly reducing computational burden by
subsampling, using (21) we can compare under (2SGM) the two settings where we have
the same task generating distribution µ but diﬀerent individual training task sample sizes
n versus n(cid:48) < n. Under the (2SGM) model, the setting n(cid:48) < n can be obtained by simple
random subsampling of the original data. We see that the statistical risk bound in (21) is
unchanged up to a small factor if n(cid:48) ≥ min(N α, n). Assuming α = 1 to simplify, in the case
where the original sample sizes n are much larger than the number of training tasks N , this
suggests that we can subsample to n(cid:48) ≈ N without taking a signiﬁcant hit to generalization
performance. This applies equally well to subsampling the tasks used for prediction or
testing. The most precise statement in this regard is (18), since it bounds the deviations of
the observed prediction loss for a ﬁxed task PXY and i.i.d. sample from that task.

The minimal subsampling size n(cid:48) can be interpreted as an optimal eﬃciency/accuracy
tradeoﬀ, since it reduces computational complexity as much as possible without sacriﬁcing
statistical accuracy. Similar considerations appear in the context of distribution regres-
sion (Szab´o et al., 2016, Remark 6). In that reference, a sharp analysis giving rise to fast

20

convergence rates is presented, resulting in a more involved optimal balance between N and
n. In the present work, we have focused on slow rates based on a uniform control of the
estimation error over RKHS balls; we leave for future work sharper convergence bounds (un-
der additional regularity conditions), which would also give rise to more reﬁned balancing
conditions between n and N .

7. Implementation

Implementation3 of the algorithm in (7) relies on techniques that are similar to those used
for other kernel methods, but with some variations. The ﬁrst subsection illustrates how,
for the case of hinge loss, the optimization problem corresponds to a certain cost-sensitive
support vector machine. Subsequent subsections focus on more scalable implementations
based on approximate feature mappings.

7.1 Representer Theorem and Hinge loss

For a particular loss (cid:96), existing algorithms for optimizing an empirical risk based on that
loss can be adapted to the setting of marginal transfer learning. We now illustrate this
idea for the case of the hinge loss, (cid:96)(t, y) = max(0, 1 − yt). To make the presentation more
concise, we will employ the extended feature representation (cid:101)Xij = ( (cid:98)P (i)
X , Xij), and we will
also “vectorize” the indices (i, j) so as to employ a single index on these variables and on
the labels. Thus the training data are ( (cid:101)Xi, Yi)1≤i≤M , where M = (cid:80)N
i=1 ni, and we seek a
solution to

min
f ∈Hk

M
(cid:88)

i=1

ci max(0, 1 − Yif ( (cid:101)Xi)) +

1
2

(cid:107)f (cid:107)2 .

Here ci = 1
, where m is the smallest positive integer such that i ≤ n1 + · · · + nm. By
the representer theorem (Steinwart and Christmann, 2008), the solution of (7) has the form

λN nm

for real numbers ri. Plugging this expression into the objective function of (7), and intro-
ducing the auxiliary variables ξi, we have the quadratic program

(cid:98)fλ =

rik( (cid:101)Xi, ·)

M
(cid:88)

i=1

min
r,ξ

1
2

rT Kr +

ciξi

M
(cid:88)

i=1

M
(cid:88)

j=1
ξi ≥ 0, ∀i,

s.t. Yi

rjk( (cid:101)Xi, (cid:101)Xj) ≥ 1 − ξi, ∀i

21

3. Software available at https://github.com/aniketde/DomainGeneralizationMarginal

where K := (k( (cid:101)Xi, (cid:101)Xj))1≤i,j≤M . Using Lagrange multiplier theory, the dual quadratic
program is

max
α

−

1
2

M
(cid:88)

i,j=1

s.t. 0 ≤ αi ≤ ci ∀i,

αiαjYiYjk( (cid:101)Xi, (cid:101)Xj) +

αi

M
(cid:88)

i=1

and the optimal function is

(cid:98)fλ =

αiYik( (cid:101)Xi, ·).

M
(cid:88)

i=1

This is equivalent to the dual of a cost-sensitive support vector machine, without oﬀset,
where the costs are given by ci. Therefore we can learn the weights αi using any existing
software package for SVMs that accepts example-dependent costs and a user-speciﬁed kernel
matrix, and allows for no oﬀset. Returning to the original notation, the ﬁnal predictor given
a test X-sample ST has the form

(cid:98)fλ( (cid:98)P T

X , x) =

αijYijk(( (cid:98)P (i)

X , Xij), ( (cid:98)P T

X , x))

N
(cid:88)

ni(cid:88)

i=1

j=1

where the αij are nonnegative. Like the SVM, the solution is often sparse, meaning most
αij are zero.

Finally, we remark on the computation of kP ( (cid:98)PX , (cid:98)P (cid:48)

(12), the calculation of kP may be reduced to computations of the form
If (cid:98)PX and (cid:98)P (cid:48)
then

X are empirical distributions based on the samples X1, . . . , Xn and X (cid:48)

X ). When K has the form of (11) or
(cid:69)
(cid:68)
Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)
.
X )
1, . . . , X (cid:48)

n(cid:48),

(cid:68)
Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

X )

(cid:69)

=

k(cid:48)
X (Xi, ·),

X (X (cid:48)
k(cid:48)

j, ·)

(cid:43)

(cid:42)

1
n

n
(cid:88)

i=1

1
n(cid:48)

n(cid:48)
(cid:88)

j=1

=

1
nn(cid:48)

n
(cid:88)

n(cid:48)
(cid:88)

i=1

j=1

X (Xi, X (cid:48)
k(cid:48)

j).

Note that when k(cid:48)
a smoothing kernel density estimate for PX .

X is a (normalized) Gaussian kernel, Ψ( (cid:98)PX ) coincides (as a function) with

7.2 Approximate Feature Mapping for Scalable Implementation

Assuming ni = n, for all i, the computational complexity of a nonlinear SVM solver (in our
context) is between O(N 2n2) and O(N 3n3) (Joachims, 1999; Chang and Lin, 2011). Thus,
standard nonlinear SVM solvers may be insuﬃcient when N or n are very large.

One approach to scaling up kernel methods is to employ approximate feature mappings
together with linear solvers. This is based on the idea that kernel methods are solving for
a linear predictor after ﬁrst nonlinearly transforming the data. Since this nonlinear trans-
formation can have an extremely high- or even inﬁnite-dimensional output, classical kernel

22

methods avoid computing it explicitly. However, if the feature mapping can be approxi-
mated by a ﬁnite dimensional transformation with a relatively low-dimensional output, one
can directly solve for the linear predictor, which can be accomplished in O(N n) time (Hsieh
et al., 2008).

In particular, given a kernel k, the goal is to ﬁnd an approximate feature mapping z(˜x)
such that k(˜x, ˜x(cid:48)) ≈ z(˜x)T z(˜x(cid:48)). Given such a mapping z, one then applies an eﬃcient linear
solver, such as Liblinear (Fan et al., 2008), to the training data (z( ˜Xij), Yij)ij to obtain a
weight vector w. The ﬁnal prediction on a test point ˜x is then wT z(˜x). As described in
the previous subsection, the linear solver may need to be tweaked, as in the case of unequal
sample sizes ni, but this is usually straightforward.

Recently, such low-dimensional approximate future mappings z(x) have been developed
for several kernels. We examine two such techniques in the context of marginal transfer
learning, the Nystr¨om approximation (Williams and Seeger, 2001; Drineas and Mahoney,
2005) and random Fourier features. The Nystr¨om approximation applies to any kernel
method, and therefore extends to the marginal transfer setting without additional work.
On the other hand, we give a novel extension of random Fourier features to the marginal
transfer learning setting (for the case of all Gaussian kernels), together with performance
analysis. Our approach is similar to the one in Jitkrittum et al. (2015) which proposes a
two-stage approximation for the mean embedding. Note that Jitkrittum et al. (2015) does
not give an error bound.

7.2.1 Random Fourier Features

The approximation of Rahimi and Recht (2007) is based on Bochner’s theorem, which
characterizes shift invariant kernels.

Theorem 19 A continuous kernel k(x, y) = k(x − y) on Rd is positive deﬁnite iﬀ k(x − y)
is the Fourier transform of a ﬁnite positive measure p(w), i.e.,

k(x − y) =

p(w)ejwT (x−y)dw .

(22)

If a shift invariant kernel k(x − y) is properly scaled then Theorem 19 guarantees that

p(w) in (22) is a proper probability distribution.

(cid:90)

Rd

23

Random Fourier features (RFFs) approximate the integral in (22) using samples drawn

from p(w). If w1, w2, ..., wL are i.i.d. draws from p(w),

k(x − y) =

p(w)ejwT (x−y)dw

p(w) cos(wT x − wT y)dw

cos(wT

i x − wT

i y)

(cid:90)

(cid:90)

Rd

Rd

L
(cid:88)

i=1
L
(cid:88)

i=1
L
(cid:88)

1
L

1
L

1
L

=

≈

=

=

cos(wT

i x) cos(wT

i y) + sin(wT

i x) sin(wT

i y)

[cos(wT

i x), sin(wT

i x)]T [cos(wT

i y), sin(wT

i y)]

i=1
= zw(x)T zw(y) ,

(23)

[cos(wT

1 x), sin(wT

L x), sin(wT

L x)] ∈ R2L is an approximate
where zw(x) = 1√
1 x), ..., cos(wT
L
In the following, we extend the RFF
nonlinear feature mapping of dimensionality 2L.
methodology to the kernel ¯k on the extended feature space PX × X . Let X1, . . . , Xn1
and X (cid:48)
X respectively, and let (cid:98)PX and (cid:98)P (cid:48)
X
denote the corresponding empirical distributions. Given x, x(cid:48) ∈ X , denote ˜x = ( (cid:98)PX , x)
and ˜x(cid:48) = ( (cid:98)P (cid:48)
X , x(cid:48)). The goal is to ﬁnd an approximate feature mapping ¯z(˜x) such that
¯k(˜x, ˜x(cid:48)) ≈ ¯z(˜x)T ¯z(˜x(cid:48)). Recall that

realizations of PX and P (cid:48)

n2 be i.i.d.

1, . . . , X (cid:48)

¯k(˜x, ˜x(cid:48)) = kP ( (cid:98)PX , (cid:98)P (cid:48)

X )kX (x, x(cid:48));

speciﬁcally, we consider kX and k(cid:48)
kP to have the Gaussian-like form

X to be Gaussian kernels and the kernel on distributions

kP ( (cid:98)PX , (cid:98)P (cid:48)

X ) = exp

(cid:107)Ψ( (cid:98)PX ) − Ψ( (cid:98)P (cid:48)

X )(cid:107)2

(cid:26) 1
2σ2
P

(cid:27)

.

Hk(cid:48)
X

As noted earlier in this section, the calculation of kP ( (cid:98)PX , (cid:98)P (cid:48)
of

X ) reduces to the computation

(cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

X )(cid:105) =

X (Xi, X (cid:48)
k(cid:48)

j).

(24)

1
n1n2

n1(cid:88)

n2(cid:88)

i=1

j=1

24

We use Theorem 19 to approximate k(cid:48)

X and thus (cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

X )(cid:105). Let w1, w2, ..., wL be

i.i.d. draws from p(cid:48)(w), the inverse Fourier transform of k(cid:48)

X . Then we have:

(cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

X )(cid:105) =

X (Xi, X (cid:48)
k(cid:48)
j)

1
n1n2

n1(cid:88)

n2(cid:88)

i=1

j=1

≈

=

=

1
Ln1n2

1
Ln1n2

1
Ln1n2

L
(cid:88)

n1(cid:88)

n2(cid:88)

l=1

i=1

j=1

L
(cid:88)

n1(cid:88)

n2(cid:88)

l=1

i=1

j=1

L
(cid:88)
{

n1(cid:88)

l=1

i=1
= ZP ( (cid:98)PX )T ZP ( (cid:98)P (cid:48)

X ),

cos(wT

l Xi − wT

l X (cid:48)
j)

[cos(wT

l Xi) cos(wT

l X (cid:48)

j) + sin(wT

l Xi) sin(wT

l X (cid:48)

j)]

[cos(wT

l Xi), sin(wT

l Xi)]T

[cos(wT

l X (cid:48)

j), sin(wT

l X (cid:48)

j)]}

n2(cid:88)

j=1

where

ZP ( (cid:98)PX ) =

cos(wT

1 Xi), sin(wT

1 Xi), ..., cos(wT

L Xi), sin(wT

L Xi)

(25)

(cid:105)
,

n1(cid:88)

(cid:104)

1
√

n1

L

i=1

and ZP ( (cid:98)P (cid:48)
let z(cid:48)
1
n1

X ) is deﬁned analogously with n1 replaced by n2. For the proof of Theorem 20,
X , which satisﬁes ZP ( (cid:98)PX ) =

X denote the approximate feature map corresponding to k(cid:48)
(cid:80)n1
i=1 z(cid:48)

X (Xi).

Note that the lengths of the vectors ZP ( (cid:98)PX ) and ZP ( (cid:98)P (cid:48)

X ) are 2L. To approximate ¯k we

may write

¯k(˜x, ˜x(cid:48)) ≈ exp

−(cid:107)x − x(cid:48)(cid:107)2
Rd
2σ2
X
P (cid:107)x − x(cid:48)(cid:107)2

Rd)

R2L + σ2

· exp

−(cid:107)ZP ( (cid:98)PX ) − ZP ( (cid:98)P (cid:48)

X )(cid:107)2

R2L

−(σ2

2σ2
P
X (cid:107)ZP ( (cid:98)PX ) − ZP ( (cid:98)P (cid:48)
2σ2

X )(cid:107)2
P σ2
X
X )(cid:107)2
−((cid:107)σX ZP ( (cid:98)PX ) − σX ZP ( (cid:98)P (cid:48)
P σ2
2σ2
X

R2L + (cid:107)σP x − σP x(cid:48)(cid:107)2

Rd)

−(cid:107)(σX ZP ( (cid:98)PX ), σP x) − (σX ZP ( (cid:98)P (cid:48)

X ), σP x(cid:48))(cid:107)2

R2L+d

.

2σ2

P σ2
X

= exp

= exp

= exp

(26)

This is also a Gaussian kernel, now on R2L+d. Again by applying Theorem 19, we have

¯k( (cid:98)PX , X), ( (cid:98)P (cid:48)

X , X (cid:48))) ≈

p(v)ejvT ((σX ZP (PX ),σP X)−(σX ZP (P (cid:48)

X ),σP X (cid:48)))dv.

(cid:90)

R2L+d

Let v1, v2, ..., vq be drawn i.i.d.
kernel with bandwidth σP σX . Let u = (σX ZP ( (cid:98)PX ), σP x) and u(cid:48) = (σX ZP ( (cid:98)P (cid:48)

from p(v), the inverse Fourier transform of the Gaussian
X ), σP x(cid:48)).

25

Then

where

¯k(˜x, ˜x(cid:48)) ≈

cos(vT

q (u − u(cid:48)))

Q
(cid:88)

1
Q

q=1
= ¯z(˜x)T ¯z(˜x(cid:48)),

¯z(˜x) =

[cos(vT

1 u), sin(vT

1 u), ..., cos(vT

Qu), sin(vT

Qu)] ∈ R2Q

(27)

1
√
Q

and ¯z(˜x(cid:48)) is deﬁned similarly.

This completes the construction of the approximate feature map. The following result,
which uses Hoeﬀding’s inequality and generalizes a result of Rahimi and Recht (2007), says
that the approximation achieves any desired approximation error with very high probability
as L, Q → ∞.

Theorem 20 Let L be the number of random features to approximate the kernel on distri-
butions and Q be the number of features to approximate the ﬁnal product kernel. For any
(cid:15)l > 0, (cid:15)q > 0, ˜x = ( (cid:98)PX , x), ˜x(cid:48) = ( (cid:98)P (cid:48)

X , x(cid:48)),

P (|¯k(˜x, ˜x(cid:48)) − ¯z(˜x)T ¯z(˜x(cid:48))| ≥ (cid:15)l + (cid:15)q) ≤ 2 exp

(cid:16)

−

(cid:17)

Q(cid:15)2
q
2

+ 6n1n2 exp

−

(28)

(cid:16)

(cid:17)

,

L(cid:15)2
2

where (cid:15) = σ2
n1 and n2 are the sizes of the empirical distributions (cid:98)PX and (cid:98)P (cid:48)

2 log(1 + (cid:15)l), σP is the bandwidth parameter of the Gaussian-like kernel kP , and

X , respectively.

P

The above results holds for ﬁxed ˜x and ˜x(cid:48). Following again Rahimi and Recht (2007),
one can use an (cid:15)-net argument to prove a stronger statement for every pair of points in the
input space simultaneously. They show

Lemma 21 Let M be a compact subset of Rd with diameter r = diam(M) and let D be the
number of random Fourier features used. Then for the mapping deﬁned in (23), we have

(cid:16)

P

sup
x,y∈M

(cid:17)
|zw(x)T zw(y) − k(x − y)| ≥ (cid:15)

(cid:17)2

≤ 28(cid:16) σr
(cid:15)

exp

(cid:16) −D(cid:15)2
2(d + 2)

(cid:17)

,

where σ = E[wT w] is the second moment of the Fourier transform of k.

Our RFF approximation of ¯k is grounded on Gaussian RFF approximations on Euclidean
spaces, and thus, the following result holds by invoking Lemma 21, and otherwise following
the argument of Theorem 20.

Theorem 22 Using the same notations as in Theorem 20 and Lemma 21,

(cid:16)

P

sup
x,x(cid:48)∈M

|¯k(˜x, ˜x(cid:48)) − ¯z(˜x)T ¯z(˜x(cid:48))| ≥ (cid:15)l + (cid:15)q

(cid:17)

(cid:17)2

≤ 28(cid:16) σ(cid:48)
X r
(cid:15)q

exp

(cid:17)

(cid:16) −Q(cid:15)2
q
2(d + 2)

+ 293n1n2

(cid:16) σP σX r
(cid:15)l
X in Eqn. (24) and σP and σX are the widths of kernels

(cid:16) −L(cid:15)2
l

2(d + 2)

(29)

exp

(cid:17)2

(cid:17)

where σ(cid:48)
kP and kX respectively.

X is the width of kernel k(cid:48)

26

There are recent developments that give faster rates for approximation quality of random
Fourier features and could potentially be combined with our analysis (Sriperumbudur and
Szab´o, 2015; Sutherland and Schneider, 2015). For example, approximation quality for the
kernel mean map is discussed in Sutherland and Schneider (2015), and these ideas could be
extended to Theorem 22 by combining with the two-stage approach presented in this paper.
We also note that our analysis of random Fourier features is separate from our analysis of
the kernel learning algorithm. We have not presented a generalization error bound for the
learning algorithm using random Fourier features (Rudi and Rosasco, 2017).

7.2.2 Nystr¨om Approximation

Like random Fourier features, the Nystr¨om approximation is a technique to approximate
kernel matrices (Williams and Seeger, 2001; Drineas and Mahoney, 2005). Unlike random
Fourier features, for the Nystr¨om approximation, the feature maps are data-dependent.
Also, in the last subsection, all kernels were assumed to be shift invariant. With the
Nystr¨om approximation there is no such assumption.

For a general kernel k, the goal is to ﬁnd a feature mapping z : Rd → RL, where L > d,
such that k(x, x(cid:48)) ≈ z(x)T z(x(cid:48)). Let r be the target rank of the ﬁnal approximated kernel
matrix, and m be the number of selected columns of the original kernel matrix. In general
r ≤ m (cid:28) n.

Given data points x1, . . . , xn, the Nystr¨om method approximates the kernel matrix by
m without replacement from the original sample,
j)]n×m,

ﬁrst sampling m data points x(cid:48)
and then constructing a low rank matrix by (cid:98)Kr = Kb (cid:98)K−1KT
and (cid:98)K = [k(x(cid:48)

j)]m×m. Hence, the ﬁnal approximate feature mapping is

b , where Kb = [k(xi, x(cid:48)

2, ..., x(cid:48)

1, x(cid:48)

i, x(cid:48)

zn(x) = (cid:98)D− 1

2 (cid:98)V T [k(x, x(cid:48)

1), ..., k(x, x(cid:48)

m)],

(30)

where (cid:98)D is the eigenvalue matrix of (cid:98)K and (cid:98)V is the corresponding eigenvector matrix.

The Nystr¨om approximation holds for any positive deﬁnite kernel, but random Fourier
features can be used only for shift invariant kernels. On the other hand, random Fourier
features are very easy to implement and the Nystr¨om method has additional time complexity
due to an eigenvalue decomposition. Moreover, the Nystr¨om method is useful only when
the kernel matrix has low rank. For additional comparison of various kernel approximation
approaches we refer the reader to Le et al. (2013).
In our experiments, we use random
Fourier features when all kernels are Gaussian and the Nystr¨om method otherwise.

8. Experiments

This section empirically compares our marginal transfer learning method with pooling.4
One implementation of the pooling algorithm was mentioned in Section 5.2, where kP is
taken to be a constant kernel. Another implementation is to put all the training data sets
together and train a single conventional kernel method. The only diﬀerence between the two
implementations is that in the former, weights of 1/ni are used for examples from training
task i. In almost all of our experiments below, the various training tasks have the same

4. Software available at https://github.com/aniketde/DomainGeneralizationMarginal

27

sample sizes, in which case the two implementations coincide. The only exception is the
fourth experiment when we use all training data, in which case we use the second of the
two implementations mentioned above.

We consider three classiﬁcation problems (Y = {−1, 1}), for which the hinge loss is
employed, and one regression problem (Y ⊂ R), where the (cid:15)-insensitive loss is employed.
Thus, the algorithms implemented are natural extensions of support vector classiﬁcation
and regression to domain generalization. Performance of a learning strategy is assessed by
holding out several data sets ST
, learning a decision function (cid:98)f on the remaining
data sets, and reporting the average empirical risk 1
i , (cid:98)f ). In some cases, this
NT
value is again averaged over several randomized versions of the experiment.

1 , . . . , ST
NT

i=1 L(ST

(cid:80)NT

8.1 Model Selection

T x2 and Gaussian kernels kσ(x1, x2) = exp (cid:0) − ||x1−x2||2

The various experiments use diﬀerent combinations of kernels. In all experiments, linear
(cid:1) were used.
kernels k(x1, x2) = x1
The bandwidth σ of each Gaussian kernel and the regularization parameter λ of the
machines were selected by grid search. For model selection, ﬁve-fold cross-validation was
used. In order to stabilize the cross-validation procedure, it was repeated 5 times over in-
dependent random splits into folds (Kohavi et al., 1995). Thus, candidate parameter values
were evaluated on the 5 × 5 validation sets and the conﬁguration yielding the best average
performance was selected. If any of the chosen hyper-parameters was at the grid boundary,
the grid was extended accordingly, i.e., the same grid size has been used, however, the center
of grid has been assigned to the previously selected point. The grid used for kernels was
σ ∈ (cid:0)10−2, 104(cid:1) with logarithmic spacing, and the grid used for the regularization parameter
was λ ∈ (cid:0)10−1, 101(cid:1) with logarithmic spacing.

2σ2

8.2 Synthetic Data Experiment

To illustrate the proposed method, a synthetic problem was constructed. The synthetic
data generation algorithm is given in Algorithm 1. In brief, for each classiﬁcation task, the
data are uniformly supported on an ellipse, with the major axis determining the labels, and
the rotation of the major axis randomly generated in a 90 degree range for each task. One
random realization of this synthetic data is shown in Figure 2. This synthetic data set is
an ideal candidate for marginal transfer learning, because the Bayes classiﬁer for a task is
uniquely determined by the marginal distribution of the features, i.e. Lemma 9 applies (and
the optimal error inf f E ∞(f ) is zero). On the other hand, observe that the expectation of
each X distribution is the same regardless of the task and thus does not provide any relevant
information, so that taking into account at least second order information is needed to
perform domain generalization.

To analyse the eﬀects of number of examples per task (n) and number of tasks (N ), we
constructed 12 synthetic data sets by taking combinations N × n where N ∈ {16, 64, 256}
and n ∈ {8, 16, 32, 256}. For each synthetic data set, the test set contains 10 tasks and each
task contains one million data points. All kernels are taken to be Gaussian, and the random
Fourier features speedup is used. The results are shown in Figure 3 and Tables 1 and 2
(see appendix). The marginal transfer learning (MTL) method signiﬁcantly outperforms
the baseline pooling method. Furthermore, the performance of MTL improves as N and n

28

increase, as expected. The pooling method, however, does no better than random guessing
regardless of N and n.

In the remaining experiments, the marginal distribution does not perfectly characterize
the optimal decision function, but still provides some information to oﬀer improvements
over pooling.

Algorithm 1: Synthetic Data Generation

input : N : Number of tasks, n: Number of training examples per task
output: Realization of synthetic data set for N tasks
for i = 1 to N do

• sample rotation αi uniformly in

(cid:104) π
4
• Take an ellipse whose major axis is aligned with the horizontal axis, and

3π
4

(cid:105)

;

,

rotate it by an angle of αi about its center;

• Sample n points Xij, j = 1, . . . , n uniformly at random from the rotated

ellipse;

• Label the points according to their position with respect to the major axis i.e.
the points that are on the right of the major axis are considered as class 1 and
the points on the left of the major axis are considered as class −1.

end

8.3 Parkinson’s Disease Telemonitoring

We test our method in the regression setting using the Parkinson’s disease telemonitoring
data set, which is composed of a range of biomedical voice measurements using a telemon-
itoring device from 42 people with early-stage Parkinson’s. The recordings were automat-
ically captured in the patients’ homes. The aim is to predict the clinician’s Parkinson’s
disease symptom score for each recording on the uniﬁed Parkinson’s disease rating scale
(UPDRS) (Tsanas et al., 2010). Thus we are in a regression setting, and employ the (cid:15)-
insensitive loss from support vector regression. All kernels are taken to be Gaussian, and
the random Fourier features speedup is used.

There are around 200 recordings per patient. We randomly select 7 test users and then
vary the number of training users N from 10 to 35 in steps of 5, and we also vary the
number of training examples n per user from 20 to 100. We repeat this process several
times to get the average errors which are shown in Fig 4 and Tables 3 and 4 (see appendix).
The marginal transfer learning method clearly outperforms pooling, especially as N and n
increase.

8.4 Satellite Classiﬁcation

Microsatellites are increasingly deployed in space missions for a variety of scientiﬁc and
technological purposes. Because of randomness in the launch process, the orbit of a mi-
crosatellite is random, and must be determined after the launch. One recently proposed

29

(a)

(b)

(c)

(d)

Figure 2: Plots of synthetic data sets (red and blue points represent negative and positive
classes) for diﬀerent settings: (a) Random realization of a single task with 256 training
examples per task. Plots (b), (c) and(d) are random realizations of synthetic data with 256
training examples for 16, 64 and 256 tasks.

approach is to estimate the orbit of a satellite based on radiofrequency (RF) signals as mea-
sured in a ground sensor network. However, microsatellites are often launched in bunches,
and for this approach to be successful, it is necessary to associate each RF measurement
vector with a particular satellite. Furthermore, the ground antennae are not able to decode
unique identiﬁer signals transmitted by the microsatellites, because (a) of constraints on
the satellite/ground antennae links, including transmission power, atmospheric attenuation,
scattering, and thermal noise, and (b) ground antennae must have low gain and low direc-
tional speciﬁcity owing to uncertainty in satellite position and dynamics. To address this
problem, recent work has proposed to apply our marginal transfer learning methodology
(Sharma and Cutler, 2015).

As a concrete instance of this problem, suppose two microsatellites are launched to-
gether. Each launch is a random phenomenon and may be viewed as a task in our frame-
work. For each launch i, training data (Xij, Yij), j = 1, . . . , ni, are generated using a highly
realistic simulation model, where Xij is a feature vector of RF measurements across a par-
ticular sensor network and at a particular time, and Yij is a binary label identifying which
of the two microsatellites produced a given measurement. By applying our methodology,
we can classify unlabeled measurements X T
from a new launch with high accuracy. Given
j
these labels, orbits can subsequently be estimated using the observed RF measurements.

30

Figure 3: Synthetic data set: Classiﬁcation error rates for proposed method and diﬀerence
with baseline for diﬀerent experimental settings, i.e., number of examples per task and
number of tasks.

Figure 4: Parkinson’s disease telemonitoring data set: Root mean square error rates for pro-
posed method and diﬀerence with baseline for diﬀerent experimental settings, i.e., number
of examples per task and number of tasks.

31

We thank Srinagesh Sharma and James Cutler for providing us with their simulated data,
and refer the reader to their paper for more details on the application (Sharma and Cutler,
2015).

To demonstrate this idea, we analyzed the data from Sharma and Cutler (2015) for
T = 50 launches, viewing up to 40 as training data and 10 as testing. We use Gaussian
kernels and the RFF kernel approximation technique to speed up the algorithm. Results
are shown in Fig 5 (tables given in the appendix). As expected, the error for the proposed
method is much lower than for pooling, especially as N and n increase.

Figure 5: Satellite data set: Classiﬁcation error rates for proposed method and diﬀerence
with baseline for diﬀerent experimental settings, i.e., number of examples per task and
number of tasks.

8.5 Flow Cytometry Experiments

We demonstrate the proposed methodology for the ﬂow cytometry auto-gating problem,
described in Sec. 2. The pooling approach has been previously investigated in this context
by Toedling et al. (2006). We used a data set that is a part of the FlowCAP Challenges
where the ground truth labels have been supplied by human experts (Aghaeepour et al.,
2013). We used the so-called “Normal Donors” data set. The data set contains 8 diﬀerent
classes and 30 subjects. Only two classes (0 and 2) have consistent class ratios, so we have
restricted our attention to these two.

The corresponding ﬂow cytometry data sets have sample sizes ranging from 18,641 to
59,411, and the proportion of class 0 in each data set ranges from 25.59 to 38.44%. We
randomly selected 10 tasks to serve as the test tasks. These tasks were removed from the
pool of eligible training tasks. We varied the number of training tasks from 5 to 20 with an
additive step size of 5, and the number of training examples per task from 1024 to 16384

32

with a multiplicative step size of 2. We repeated this process 10 times to get the average
classiﬁcation errors which are shown in Fig. 6 and Tables 7 and 8 (see appendix). The
kernel kP was Gaussian, and the other two were linear. The Nystr¨om approximation was
used to achieve an eﬃcient implementation.

For nearly all settings the proposed method has a smaller error rate than the baseline.
Furthermore, for the marginal transfer learning method, when one ﬁxes the number of
training examples and increases the number of tasks then the classiﬁcation error rate drops.
On the other hand, we observe on Table 7 that the number n of training points per
task hardly aﬀects the ﬁnal performance when n ≥ 103. This is in contrast with the
previous experimental examples (synthetic, Parkinson’s disease telemonitoring, and satellite
classiﬁcation), for which increasing n led to better performance, but where the values of
n remained somewhat modest (n ≤ 256). This is qualitatively in line with the theoretical
results under (2SGM) in Section 6.2 (see in particular the concluding discussion there),
suggesting that the inﬂuence of increasing n on the performance should eventually taper
oﬀ, in particular if n (cid:29) N .

Figure 6: Flow Cytometry Data set: Percentage Classiﬁcation error rates for proposed
method and diﬀerence with baseline for diﬀerent experimental settings, i.e., number of
examples per task and number of tasks.

9. Discussion

Our approach to domain generalization relies on the extended input pattern (cid:101)X = (PX , X).
Thus, we study the natural algorithm of minimizing a regularized empirical loss over a
reproducing kernel Hilbert space associated with the extended input domain PX × X . We
also establish universal consistency under two sampling plans. To achieve this, we present

33

novel generalization error analyses, and construct a universal kernel on PX × X . A detailed
implementation based on novel approximate feature mappings is also presented.

On one synthetic and three real-world data sets, the marginal transfer learning approach
consistently outperforms a pooling baseline. On some data sets, however, the diﬀerence
between the two methods is small. This is because the utility of transfer learning varies
from one DG problem to another. As an extreme example, if all of the task are the same,
then pooling should do just as well as our method.

Several future directions exist. From an application perspective, the need for adaptive
classiﬁers arises in many applications, especially in biomedical applications involving biolog-
ical and/or technical variation in patient data. Examples include brain computer interfaces
and patient monitoring. For example, when electrocardiograms are used to continuously
monitor cardiac patients, it is desirable to classify each heartbeat as irregular or not. Given
the extraordinary amount of data involved, automation of this process is essential. How-
ever, irregularities in a test patient’s heartbeat will diﬀer from irregularities of historical
patients, hence the need to adapt to the test distribution (Wiens, 2010).

From a theoretical and methodological perspective, several questions are of interest. We
would like to specify conditions on the meta-distributions PS or µ under which the DG risk
is close to the expected Bayes risk of the test distribution (beyond the simple condition
discussed in Lemma 9). We would also like to develop fast learning rates under suitable dis-
tributional assumptions. Furthermore, given the close connections with supervised learning,
many common variants of supervised learning can also be investigated in the DG context,
including multiclass classiﬁcation, class probability estimation, and robustness to various
forms of noise.

We can also ask how the methodology and analysis can be extended to the context
where a small number of labels are available for the test distribution (additionally to a
larger number of unlabeled data from the same distribution); this situation appears to be
common in practice, and can be seen as intermediary between the DG and learning to learn
(LTL, see Section 4.2) settings (one could dub it “semi-supervised domain generalization”).
In this setting, two approaches appear promising to take advantage of the labeled data. The
simplest one is to use the same optimization problem (7), where we include additionally
the labeled examples of the test distribution. However, if several test samples are to be
treated in succession, and we want to avoid a full, resource-consuming re-training using
all the training samples each time, an interesting alternative is the following:
learn once
a function f0(PX , x) using the available training samples via (7); then, given a partially
labeled test sample, learn a decision function on this sample only via the usual kernel (kX )
norm regularized empirical loss minimization method, but replace the usual regularizer
term (cid:107)f (cid:107)2
X , .) ∈ HkX ). In this sense, the marginal-
adaptive decision function learned from the training samples would serve as a “prior” or
“informed guess” for learning on the test data. This can be also interpreted as learning
an adequate complexity penalty to improve learning on new samples, thus connecting to
the general principles of LTL (see Section 4.2). An interesting diﬀerence with underlying
existing LTL approaches is that those tend to adapt the hypothesis class or the“shape” of the
regularization penalty to the problem at hand, while the approach delineated above would

(cid:13)
(cid:13)f − f0( (cid:98)P T
(cid:13)

(note that f0( (cid:98)P T

H by

X , .)

(cid:13)
2
(cid:13)
(cid:13)

H

34

modify the “origin” of the penalty, using the marginal distribution information. These two
principles could also be combined.

35

Appendix A. Proofs, Technical Details, and Experimental Details

This section contains technical details for the proofs of the announced results.

A.1 Proof of Proposition 7

XY be a ﬁxed probability distribution on X × R, and ε > 0 a ﬁxed number. Since X
X (the
XY ), is inner regular, so that there exists a compact set K ⊂ X such that

Let P T
is a Radon space, by deﬁnition any Borel probability measure on it, in particular P T
X-marginal of P T
P T
X (Kc) ≤ ε.

(cid:12)f (u, v) − f (P T

For all x ∈ K, by the assumed continuity of the decision function f at point (P T

X , x) there
exists an open neighborhood Ux×Vx ⊂ PX ×X of this point such that (cid:12)
X , x)(cid:12)
(cid:12) ≤
ε for all (u, v) ∈ Ux × Vx. Since the family (Vx)x∈K is an open covering of the compact
K, there exists a ﬁnite subfamily (Vxi)i∈I covering K. Denoting U0 := (cid:84)
i∈I Uxi which is
an open neighborhood of P T
X in PX , it therefore holds for any P ∈ U0 and uniformly over
X , x)(cid:12)
x ∈ K that (cid:12)
(cid:12)f (P T
(cid:12)f (P, x) − f (P T
(cid:12) ≤ 2ε, where
i0 ∈ I is such that x ∈ Vxi0
.
Denote ST = (X T
i , Y T
(cid:98)P T
X ∈ U0

XY , and A the
X in probability,
event
so that P [Ac] ≤ ε holds for nT large enough. We have (denoting B a bound on the loss
function):

from P T
X weakly converges to P T

i )1≤i≤nT a sample of size nT drawn i.i.d.

. By the law of large numbers, (cid:98)P T

(cid:12) ≤ |f (P, x) − f (P, xi0)| + (cid:12)

X , x) − f (P, xi0)(cid:12)

(cid:111)

(cid:110)

EST

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:96)(f ( (cid:98)P T

X , X T

(cid:35)
i ), Y T
i )

≤ Bε + EST

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:96)(f ( (cid:98)P T

X , X T

i ), Y T

i )1{X T

i ∈K}

(cid:35)

(cid:35)

(cid:35)

1
nT

nT(cid:88)

i=1

1
nT

nT(cid:88)

i=1

≤ B(ε + P [Ac]) + EST

1{ (cid:98)P T

X ∈U0}

(cid:96)(f ( (cid:98)P T

X , X T

i ), Y T

i )1{X T

i ∈K}

≤ 2Bε + 2Lε + EST

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:96)(f (P T

X , X T

(cid:35)
i ), Y T
i )

≤ 2(B + L)ε + E

(X T ,Y T )∼P T

XY

(cid:2)(cid:96)(f (P T

X , X T ), Y T )(cid:3) .

Conversely,

EST

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:35)

(cid:96)(f ( (cid:98)P T

X , X T

i ), Y T
i )

≥ EST

1{ (cid:98)P T

X ∈U0}

(cid:96)(f ( (cid:98)P T

X , X T

i ), Y T

i )1{X T

i ∈K}

≥ EST

1
nT

nT(cid:88)

i=1

(cid:96)(f (P T

X , X T

(cid:35)
i ), Y T
i )

− 2Bε − 2Lε

≥ E

(X T ,Y T )∼P T

XY

(cid:2)(cid:96)(f (P T

X , X T ), Y T )(cid:3) − 2(B + L)ε.

Since the above inequalities hold for any ε > 0 provided nT is large enough, this yields that
for any ﬁxed P T

XY , we have

E

lim
nT →∞

ST ∼(P T

XY )⊗nT

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:96)(f ( (cid:98)P T

X , X T

(cid:35)
i ), Y T
i )

= E

(X T ,Y T )∼P T

XY

(cid:2)(cid:96)(f (P T

X , X T ), Y T )(cid:3) .

(cid:34)

(cid:34)

(cid:34)

36

Finally, since the above right-hand side is bounded by B, applying dominated convergence
to integrate over P T

XY ∼ µ yields the desired conclusion.

A.2 Proof of Corollary 14

Proof Denote E ∗ = inf f :PX ×X →R E(f ). Let ε > 0. Since k is a universal kernel on PX × X
and (cid:96) is Lipschitz, there exists f0 ∈ Hk such that E(f0) ≤ E ∗+ ε
2 (Steinwart and Christmann,
2008).

By comparing the objective function in (7) at the minimizer (cid:98)fλ and at the null function,
using assumption (LB) we deduce that we must have (cid:107) (cid:98)fλ(cid:107) ≤ (cid:112)B0/λ. Applying Theorem 11
for R = Rλ = (cid:112)B0/λ, and δ = 1/N 2, gives that with probability at least 1 − 1/N 2,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ≤ ε(N ) := (B0 + L(cid:96)BKBk
(cid:12) (cid:98)E(f, N ) − E(f )

(cid:112)B0/λ)

sup
f ∈Bk(R)

√
(

log N + 2)
N

√

.

Let N be large enough so that (cid:107)f0(cid:107) ≤ Rλ. We can now deduce that with probability at

least 1 − 1/N 2,

E( (cid:98)fλ) ≤ (cid:98)E( (cid:98)fλ, N ) + ε(N )

= (cid:98)E( (cid:98)fλ, N ) + λ(cid:107) (cid:98)fλ(cid:107)2 − λ(cid:107) (cid:98)fλ(cid:107)2 + ε(N )
≤ (cid:98)E(f0, N ) + λ(cid:107)f0(cid:107)2 − λ(cid:107) (cid:98)fλ(cid:107)2 + ε(N )
≤ (cid:98)E(f0, N ) + λ(cid:107)f0(cid:107)2 + ε(N )
≤ E(f0) + λ(cid:107)f0(cid:107)2 + 2ε(N )
≤ E ∗ +

+ λ(cid:107)f0(cid:107)2 + 2ε(N ).

ε
2

The last two terms become less than ε
growth of λ = λ(N ). This establishes that for any ε > 0, there exists N0 such that

2 for N suﬃciently large by the assumptions on the

(cid:88)

N ≥N0

Pr(E( (cid:98)fλ) ≥ E ∗ + ε) ≤

(cid:88)

N ≥N0

1
N 2 < ∞,

and so the result follows by the Borel-Cantelli lemma.

37

A.3 Proof of Theorem 15

We control the diﬀerence between the training loss and the conditional risk at inﬁnite sample
size via the following decomposition:

|L(S, f ) − E ∞(f |PXY )| = sup

sup
f ∈Bk(R)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:96)(f ( (cid:98)PX , Xi), Yi) − E ∞(f |PXY )
(cid:12)
(cid:12)
(cid:12)

f ∈Bk(R)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

f ∈Bk(R)

≤ sup

1
n
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
=: (I) + (II).

+ sup

f ∈Bk(R)

n
(cid:88)

(cid:16)

i=1

1
n

n
(cid:88)

i=1

(cid:96)(f ( (cid:98)PX , Xi), Yi) − (cid:96)(f (PX , Xi), Yi)

(cid:12)
(cid:12)
(cid:96)(f (PX , Xi), Yi) − E ∞(f |PXY )
(cid:12)
(cid:12)
(cid:12)

(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(31)

A.3.1 Control of term (I)

Using the assumption that the loss (cid:96) is L(cid:96)-Lipschitz in its ﬁrst coordinate, we can bound
the ﬁrst term as follows:

(I) ≤ L(cid:96)

sup
f ∈Bk(R)

1
n

n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ≤ L(cid:96)
(cid:12)f ( (cid:98)PX , Xi) − f (PX , Xi)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)f ( (cid:98)PX , ·) − f (PX , ·)
(cid:13)∞

. (32)

sup
f ∈Bk(R)

This can now be controlled using the ﬁrst part of the following result:

Lemma 23 Assume (K-Bounded) holds. Let PX be an arbitrary distribution on X and
(cid:98)PX denote an empirical distribution on X based on an iid sample of size n from PX . Then
with probability at least 1 − δ over the draw of this sample, it holds that

(cid:13)
(cid:13)
(cid:13)f ( (cid:98)PX , ·) − f (PX , ·)

(cid:13)
(cid:13)
(cid:13)∞

≤ 3RBkLKBα
k(cid:48)

(cid:18) log 2δ−1
n

(cid:19) α

2

.

sup
f ∈Bk(R)

(33)

In expectation, it holds

(cid:34)

E

sup
f ∈Bk(R)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)f ( (cid:98)PX , ·) − f (PX , ·)
(cid:13)∞

(cid:35)

≤ 2RBkLKBα

k(cid:48)n−α/2.

(34)

Proof Let X1, . . . , Xn denote the n-sample from PX . Let us denote by Φ(cid:48)
X the canonical
feature mapping x (cid:55)→ k(cid:48)
X (x)(cid:107) ≤ Bk(cid:48),
and so, as a consequence of Hoeﬀding’s inequality in a Hilbert space (see, e.g., Pinelis and
Sakhanenko, 1985), it holds with probability at least 1 − δ:

. We have for all x ∈ X , (cid:107)Φ(cid:48)

X (x, ·) from X into Hk(cid:48)

X

(cid:13)
(cid:13)
(cid:13)Ψ(PX ) − Ψ( (cid:98)PX )

(cid:13)
(cid:13)
(cid:13) =

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

Φ(cid:48)

X (Xi) − EX∼PX

(cid:2)Φ(cid:48)

X (X)(cid:3)

≤ 3Bk(cid:48)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:114)

log 2δ−1
n

.

(35)

38

Furthermore, using the reproducing property of the kernel k, we have for any x ∈ X and
f ∈ Bk(R):

|f ( (cid:98)PX , x) − f (PX , x)| =

(cid:68)

(cid:12)
(cid:12)
(cid:12)

k(( (cid:98)PX , x), ·) − k((PX , x), ·), f

(cid:69)(cid:12)
(cid:12)
(cid:12)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)k(( (cid:98)PX , x), ·) − k((PX , x), ·)
(cid:13)
K(Ψ(PX ), Ψ(PX ))

(cid:16)

1
2

≤ RkX (x, x)

≤ (cid:107)f (cid:107)

+ K(Ψ( (cid:98)PX ), Ψ( (cid:98)PX )) − 2K(Ψ(PX ), Ψ( (cid:98)PX ))
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)ΦK(Ψ(PX )) − ΦK(Ψ( (cid:98)PX ))
(cid:13)

≤ RBk

(cid:17) 1
2

≤ RBkLK

(cid:13)
(cid:13)
α
(cid:13)
(cid:13)
(cid:13)Ψ(PX ) − Ψ( (cid:98)PX )
(cid:13)

,

where in the last step we have used property (K-H¨older) together with the fact that for
all P ∈ PX , (cid:107)Ψ(P )(cid:107) ≤ (cid:82)
(Bk(cid:48)). Combining
with (35) gives (33).

X (x, ·)(cid:107) dPX (x) ≤ Bk(cid:48), so that Ψ(P ) ∈ Bk(cid:48)

X (cid:107)k(cid:48)

X

For the bound in expectation, we use the inequality above, and can bound further (using

Jensen’s inequality, since α ≤ 1)

E

(cid:13)
(cid:104)(cid:13)
(cid:13)
(cid:13)
(cid:13)Ψ(PX ) − Ψ( (cid:98)PX )
(cid:13)

α(cid:105)

≤ E

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Ψ(PX ) − Ψ( (cid:98)PX )
(cid:13)

2(cid:21)α/2

E (cid:2)(cid:10)Φ(cid:48)

X (Xi) − E (cid:2)Φ(cid:48)

X (X)(cid:3) , Φ(cid:48)

X (Xj) − E (cid:2)Φ(cid:48)

X (X)(cid:3)(cid:11)(cid:3)





α/2

E

(cid:104)(cid:13)
(cid:13)Φ(cid:48)

X (Xi) − E (cid:2)Φ(cid:48)

X (X)(cid:3)(cid:13)
(cid:13)

(cid:33)α/2

2(cid:105)



=



(cid:32)

=

≤

1
n2

1
n2

n
(cid:88)

i,j=1

n
(cid:88)

i=1

(cid:19) α

2

(cid:18) 4B2
k(cid:48)
n

,

which yields (34) in combination with the above.

A.3.2 Control of term (II)

Term (II) takes the form of a uniform deviation over a RKHS ball of an empirical loss
for the data ( (cid:101)Xi, Yi), where (cid:101)Xi := (PX , Xi). Since PX is ﬁxed (in contrast with term (I)
where (cid:98)PX depended on the whole sample), these data are i.i.d. Similar to the proofs of
Theorems 5 and 11, we can therefore apply again standard Rademacher analysis, this time
at the level of one speciﬁc task (Azuma-McDiarmid inequality followed by Rademacher
complexity analysis for a Lipschitz, bounded loss over a RKHS ball; see Koltchinskii, 2001;
Bartlett and Mendelson, 2002, Theorems 8, 12 and Lemma 22 there). The kernel k is
bounded by B2
K by assumption (K-Bounded); by Lemma 10 and assumption (LB), the

kB2

39

loss is bounded by B0 + L(cid:96)RBkBK, and is L(cid:96)-Lipschitz. Therefore, with probability at least
1 − δ we get

(II) ≤ (B0 + 3L(cid:96)RBkBK) min

(cid:32)(cid:114)

(cid:33)

log(δ−1)
2n

, 1

(cid:32)(cid:18) log(δ−1)

(cid:19) α

2

2n

(cid:33)

, 1

.

≤ (B0 + 3L(cid:96)RBkBK) min

(36)

Observe that we can cap the second factor at 1 since (II) is upper bounded by the bound
on the loss in all cases; the second inequality then uses α ≤ 1. Combining with a union
bound the probabilistic controls (32), (33) of term (I) and (36) of (II) yields (18).

To establish the bound (19) we use a similar argument. We use the decomposition

|E(f |PXY , n) − E ∞(f |PXY )|

sup
f ∈Bk(R)

≤ sup

f ∈Bk(R)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ sup

f ∈Bk(R)
=: (I (cid:48)) + (II (cid:48)).

ESn∼(PXY )⊗n

(cid:12)
(cid:12)
ES∼(PXY )⊗n
(cid:12)
(cid:12)
(cid:12)

(cid:34)

(cid:34)

1
n

1
n

n
(cid:88)

(cid:16)

i=1
n
(cid:88)

i=1

(cid:96)(f ( (cid:98)PX , X T

i ), Yi) − (cid:96)(f (PX , Xi), Yi)

(cid:0)(cid:96)(f (PX , X T

i ), Yi)(cid:1)

(cid:35)

(cid:12)
(cid:12)
− E(X,Y )∼PXY [(cid:96)(f (PX , X), Y )]
(cid:12)
(cid:12)
(cid:12)

(cid:17)

(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

for any ﬁxed f ∈ Bk(R) and PXY , the
It is easily seen that the second term vanishes:
diﬀerence of the expectations is zero. For the ﬁrst term, using Lipschitzness of the loss,
then (34), we obtain

(I (cid:48)) ≤ L(cid:96)E

(cid:34)

sup
f ∈Bk(R)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)f ( (cid:98)PX , .) − f (PX , .)
(cid:13)∞

(cid:35)

≤ 2L(cid:96)RBkLKBα

k(cid:48)n−α/2,

yielding (19). The bound (20) is obtained as a direct consequence by taking expectation
over PXY ∼ µ and using Jensen’s inequality to pull out the absolute value.

A.4 Regularity conditions for the kernel on distributions

We investigate suﬃcient conditions on the kernel K to ensure the regularity condition (K-
H¨older) (15). Roughly speaking, the regularity of the feature mapping of a reproducing
kernel is “one half” of the regularity of the kernel in each of its variables. The next result
considers the situation where K is itself simply a H¨older continuous function of its variables.

Lemma 24 Let α ∈ (0, 1
constant L2

2 ]. Assume that the kernel K is H¨older continuous of order 2α and

K/2 in each of its two variables on Bk(cid:48)

X

(Bk(cid:48)). Then (K-H¨older) is satisﬁed.

Proof For any v, w ∈ Bk(cid:48)

(Bk(cid:48)):

X

(cid:107)ΦK(v) − ΦK(w)(cid:107) = (K(v, v) + K(w, w) − 2K(v, w))

1

2 ≤ LK (cid:107)v − w(cid:107)α .

40

The above type of regularity only leads to a H¨older feature mapping of order at most 1
2
(when the kernel function is Lipschitz continuous in each variable). Since this order plays
an important role in the rate of convergence of the upper bound in the main error control
theorem, it is desirable to study conditions ensuring more regularity, in particular a feature
mapping which has at least Lipschitz continuity. For this, we consider the following stronger
condition, namely that the kernel function is twice diﬀerentiable in a speciﬁc sense:

Lemma 25 Assume that, for any u, v ∈ Bk(cid:48)
, the
function hu,v,e : (λ, µ) ∈ R2 (cid:55)→ K(u + λe, v + µe) admits a mixed partial derivative ∂1∂2hu,v,e
at the point (λ, µ) = (0, 0) which is bounded in absolute value by a constant C2
K independent
of (u, v, e). Then (15) is satisﬁed with α = 1 and LK = CK, that is, the canonical feature
mapping of K is Lipschitz continuous on Bk(cid:48)

(Bk(cid:48)) and unit norm vector e of Hk(cid:48)

(Bk(cid:48)).

X

X

X

Proof The argument is along the same lines as Steinwart and Christmann (2008), Lemma
4.34. Observe that, since hu,v,e(λ + λ(cid:48), µ + µ(cid:48)) = hu+λe,v+µe,e(λ(cid:48), µ(cid:48)), the function hu,v,e
actually admits a uniformly bounded mixed partial derivative in any point (λ, µ) ∈ R2 such
(Bk(cid:48)) . Let us denote ∆1hu,v,e(λ, µ) := hu,v,e(λ, µ) − hu,v,e(0, µ) .
that (u + λe, v + µe) ∈ Bk(cid:48)
(Bk(cid:48)) , u (cid:54)= v , let us set λ := (cid:107)v − u(cid:107) and the unit vector e := λ−1(v − u);
For any u, v ∈ Bk(cid:48)
we have

X

X

(cid:107)ΦK(u) − ΦK(v)(cid:107)2 = K(u, u) + K(u + λe, u + λe) − K(u, u + λe) − K(u + λe, u)

= ∆1hu,v,e(λ, λ) − ∆1hu,v,e(λ, 0)
= λ∂2∆1hu,v,e(λ, λ(cid:48)) ,

where we have used the mean value theorem, yielding existence of λ(cid:48) ∈ [0, λ] such that the
last equality holds. Furthermore,

∂2∆1hu,v,e(λ, λ(cid:48)) = ∂2hu,v,e(λ, λ(cid:48)) − ∂2hu,v,e(0, λ(cid:48))

= λ∂1∂2hu,v,e(λ(cid:48)(cid:48), λ(cid:48)) ,

using again the mean value theorem, yielding existence of λ(cid:48)(cid:48) ∈ [0, λ] in the last equality.
Finally, we get

(cid:107)ΦK(u) − ΦK(v)(cid:107)2 = λ2∂1∂2hu,v,e(λ(cid:48), λ(cid:48)(cid:48)) ≤ C2

K (cid:107)v − u(cid:107)2 .

Lemma 26 Assume that the kernel K takes the form of either (a) K(u, v) = g((cid:107)u − v(cid:107)2)
or (b) K(u, v) = g((cid:104)u, v(cid:105)) , where g is a twice diﬀerentiable real function of a real variable
k(cid:48)] in case (b). Assume (cid:107)g(cid:48)(cid:107)∞ ≤ C1 and
deﬁned on [0, 4B2
(cid:107)g(cid:48)(cid:48)(cid:107)∞ ≤ C2. Then K satisﬁes the assumption of Lemma 25 with CK := 2C1 + 16C2B2
k(cid:48) in
case (a), and CK := C1 + C2B2

k(cid:48)] in case (a), and on [−B2

k(cid:48), B2

k(cid:48) for case (b).

41

Proof In case (a), we have hu,v,e(λ, µ) = g((cid:107)u − v + (λ − µ)e(cid:107)2). It follows

|∂1∂2hu,v,e(0, 0)| =

(cid:12)
(cid:12)

(cid:12)−2g(cid:48)((cid:107)u − v(cid:107)2) (cid:107)e(cid:107)2 − 4g(cid:48)(cid:48)((cid:107)u − v(cid:107)2) (cid:104)u − v, e(cid:105)2(cid:12)
≤ 2C1 + 16C2B2

(cid:12)
(cid:12)

k(cid:48) .

In case (b), we have hu,v,e(λ, µ) = g((cid:104)u + λe, v + µe(cid:105)). It follows

|∂1∂2hu,v,e(0, 0)| =

(cid:12)
(cid:12)g(cid:48)((cid:104)u, v(cid:105)) (cid:107)e(cid:107)2 + g(cid:48)(cid:48)((cid:104)u, v(cid:105)) (cid:104)u, e(cid:105) (cid:104)v, e(cid:105)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

≤ C1 + C2B2

k(cid:48) .

A.5 Proof of Lemma 12

Proof Let H, H(cid:48) the RKHS associated to k, k(cid:48) with the associated feature mappings Φ, Φ(cid:48).
Then it can be checked that (x, x(cid:48)) ∈ X × X (cid:48)
(cid:55)→ Φ(x) ⊗ Φ(cid:48)(x(cid:48)) is a feature mapping for
k into the Hilbert space H ⊗ H(cid:48). Using (Steinwart and Christmann, 2008), Th. 4.21,
we deduce that the RKHS H of k contains precisely all functions of the form (x, x(cid:48)) ∈
X × X (cid:48)
(cid:55)→ Fw(x, x(cid:48)) = (cid:104)w, Φ(x) ⊗ Φ(x(cid:48))(cid:105), where w ranges over H ⊗ H(cid:48). Taking w of the
form w = g ⊗ g(cid:48), g ∈ H, g ∈ H(cid:48), we deduce that H contains in particular all functions of
the form f (x, x(cid:48)) = g(x)g(x(cid:48)), and further

(cid:101)H := span (cid:8)(x, x(cid:48)) ∈ X × X (cid:48) (cid:55)→ g(x)g(x(cid:48)); g ∈ H, g(cid:48) ∈ H(cid:48)(cid:9) ⊂ H.

Denote C(X ), C(X (cid:48)), C(X × X (cid:48)) the set of real-valued continuous functions on the respective
spaces. Let

C(X ) ⊗ C(X (cid:48)) := span (cid:8)(x, x(cid:48)) ∈ X × X (cid:48) (cid:55)→ f (x)f (cid:48)(x(cid:48)); f ∈ C(X ), f (cid:48) ∈ C(X (cid:48))(cid:9) .

Let G(x, x(cid:48)) be an arbitrary element of C(X ) ⊗ C(X (cid:48)), G(x, x(cid:48)) = (cid:80)k
gi ∈ C(X ), g(cid:48)
exist fi ∈ H, f (cid:48)
F (x, x(cid:48)) := (cid:80)k

i(x(cid:48)) with
i ∈ C(X (cid:48)) for i = 1, . . . , k. For ε > 0, by universality of k and k(cid:48), there
i(cid:107)∞ ≤ ε for i = 1, . . . , k. Let

i ∈ H(cid:48) so that (cid:107)fi − gi(cid:107)∞ ≤ ε, (cid:107)f (cid:48)
i(x(cid:48)) ∈ (cid:101)H. We have

i=1 λigi(x)g(cid:48)

i=1 λifi(x)f (cid:48)

i − g(cid:48)

(cid:13)F (x, x(cid:48)) − G(x, x(cid:48))(cid:13)
(cid:13)

(cid:13)∞ ≤

λi(gi(x)g(cid:48)

i(x) − fi(x)f (cid:48)

(cid:13)
(cid:13)
(cid:13)
i(x))
(cid:13)
(cid:13)∞

=

λi

(fi(x) − gi(x))(g(cid:48)

i(x(cid:48)) − f (cid:48)

i(x(cid:48)))

(cid:104)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k
(cid:88)

i=1

k
(cid:88)

i=1

+ gi(x)(g(cid:48)

i(x) − f (cid:48)

i(x(cid:48))) + (gi(x) − fi(x))g(cid:48)

(cid:105)
i(x(cid:48))

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤ ε

k
(cid:88)

i=1

|λi| (ε + (cid:107)gi(cid:107)∞ + (cid:13)
(cid:13)g(cid:48)
i

(cid:13)
(cid:13)∞) .

42

This establishes that (cid:101)H is dense in C(X ) ⊗ C(X (cid:48)) for the supremum norm. It can be easily
checked that C(X ) ⊗ C(X (cid:48)) is an algebra of functions which does not vanish and separates
points on X × X (cid:48). By the Stone-Weierstrass theorem, it is therefore dense in C(X × X (cid:48)) for
the supremum norm. We deduce that (cid:101)H (and thus also H) is dense in C(X × X (cid:48)), so that
k is universal.

A.6 Proof of Theorem 20

Proof Observe:

and denote:

(cid:26) −1
2σ2
P

(cid:26) −1
2σ2
P

¯k(˜x, ˜x(cid:48)) = exp

(cid:107)Ψ( (cid:98)PX ) − Ψ( (cid:98)P (cid:48)

X )(cid:107)2

exp

(cid:27)

(cid:27)

(cid:107)x − x(cid:48)(cid:107)2

,

(cid:26) −1
2σ2
X

˜k(˜x, ˜x(cid:48)) = exp

(cid:107)ZP ( (cid:98)PX ) − ZP ( (cid:98)P (cid:48)

X )(cid:107)2

exp

(cid:27)

(cid:27)

(cid:107)x − x(cid:48)(cid:107)2

,

(cid:26) −1
2σ2
X

We omit the arguments of ¯k, ˜k for brevity. Let kq be the ﬁnal approximation (kq =
¯z(˜x)T ¯z(˜x(cid:48))) and then we have

|¯k − kq| = |¯k − ˜k + ˜k − kq| ≤ |¯k − ˜k| + |˜k − kq|.

From Eqn. (37) it follows that,

P (|¯k − kq| ≥ (cid:15)l + (cid:15)q) ≤ P (|¯k − ˜k| ≥ (cid:15)l) + P (|˜k − kq| ≥ (cid:15)q).

By a direct application of Hoeﬀding’s inequality,

P (|˜k − kq| ≥ (cid:15)q) ≤ 2 exp(−

Q(cid:15)2
q
2

).

(37)

(38)

(39)

Recall that (cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)
again by Hoeﬀding

X )(cid:105) = 1
n1n2

(cid:80)n1
i=1

(cid:80)n2

j=1 k(cid:48)

X (Xi, X (cid:48)

j). For a pair Xi, X (cid:48)

j, we have

P (|z(cid:48)

X (Xi)T z(cid:48)

X (X (cid:48)

j) − k(cid:48)

X (Xi, X (cid:48)

j)| ≥ (cid:15)) ≤ 2 exp(−

Let Ωij be the event |z(cid:48)
bound we have

X (Xi)T z(cid:48)

X (X (cid:48)

j) − k(cid:48)

X (Xi, X (cid:48)

j)| ≥ (cid:15), for particular i, j. Using the union

P (Ω11 ∪ Ω12 ∪ . . . ∪ Ωn1n2) ≤ 2n1n2 exp(−

This implies

P (|ZP ( (cid:98)PX )T ZP ( (cid:98)P (cid:48)

X ) − (cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

X )(cid:105)| ≥ (cid:15)) ≤ 2n1n2 exp(−

(40)

L(cid:15)2
2

).

43

L(cid:15)2
2

).

L(cid:15)2
2

)

Therefore,

(cid:12)
¯k − ˜k
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) =

(cid:34)

≤

exp

(cid:26) −1
2σ2
X

exp

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:26) −1
2σ2
P
(cid:26) −1
2σ2
P

(cid:27) (cid:34)

(cid:107)x − x(cid:48)(cid:107)2

exp

(cid:107)Ψ( (cid:98)PX ) − Ψ( (cid:98)P (cid:48)

X )(cid:107)2

(cid:27)

− exp

(cid:107)ZP ( (cid:98)PX ) − ZP ( (cid:98)P (cid:48)

X )(cid:107)2

(cid:107)Ψ( (cid:98)PX ) − Ψ( (cid:98)P (cid:48)

X )(cid:107)2

− exp

(cid:107)ZP ( (cid:98)PX ) − ZP ( (cid:98)P (cid:48)

X )(cid:107)2

(cid:27) (cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:26) −1
2σ2
P
(cid:27) (cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:27)

(cid:27) (cid:34)

(cid:26) −1
2σ2
P

(cid:16)

(cid:110) −1
2σ2
P

=

exp

(cid:107)Ψ( (cid:98)PX ) − Ψ( (cid:98)P (cid:48)

X )(cid:107)2

1 − exp

(cid:107)ZP ( (cid:98)PX ) − ZP ( (cid:98)P (cid:48)

X )(cid:107)2

(cid:26) −1
2σ2
P

− (cid:107)Ψ( (cid:98)PX ) − Ψ( (cid:98)P (cid:48)

(cid:35)(cid:12)
(cid:12)
X )(cid:107)2(cid:17)(cid:111)
(cid:12)
(cid:12)
(cid:12)

(cid:34)

≤

1 − exp

(cid:16)

(cid:26) −1
2σ2
P

(cid:40)

(cid:16)

−1
2σ2
P

(cid:40)

(cid:16)

1
2σ2
P

(cid:107)ZP ( (cid:98)PX ) − ZP ( (cid:98)P (cid:48)

X )(cid:107)2 − (cid:107)Ψ( (cid:98)PX ) − Ψ( (cid:98)P (cid:48)

=

1 − exp

ZP ( (cid:98)PX )T ZP ( (cid:98)PX ) − (cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)PX )(cid:105) + ZP ( (cid:98)P (cid:48)

X )T ZP ( (cid:98)P (cid:48)

X )

− (cid:104)Ψ( (cid:98)P (cid:48)

X ), Ψ( (cid:98)P (cid:48)

X )(cid:105) − 2(cid:0)ZP ( (cid:98)PX )T ZP ( (cid:98)P (cid:48)

X ) − (cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

≤

1 − exp

|ZP ( (cid:98)PX )T ZP ( (cid:98)PX ) − (cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)PX )(cid:105)| + |ZP ( (cid:98)P (cid:48)

X )T ZP ( (cid:98)P (cid:48)

X )

− (cid:104)Ψ( (cid:98)P (cid:48)

X ), Ψ( (cid:98)P (cid:48)

X )(cid:105)| + 2|(cid:0)ZP ( (cid:98)PX )T ZP ( (cid:98)P (cid:48)

X ) − (cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

X )(cid:107)2(cid:17)(cid:27) (cid:35)(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

X )(cid:105)(cid:1)(cid:17)

(cid:41)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:17)
X )(cid:105)(cid:1)|

(cid:41)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

The result now follows by applying the bound of Eqn. (40) to each of the three terms in
the exponent of the preceding expression, together with the stated formula for (cid:15) in terms
of (cid:15)(cid:96).

A.7 Proof of Theorem 22

Proof The proof is very similar to the proof of Theorem 20. We use Lemma 21 to replace
bound (39) with:

(cid:18)

P

sup
x,x(cid:48)∈M

(cid:19)

|˜k − kq| ≥ (cid:15)q

≤ 28

(cid:19)2

(cid:18) σ(cid:48)
X r
(cid:15)q

exp

(cid:18) −Q(cid:15)2
q
2(d + 2)

(cid:19)
.

(41)

44

Similarly, Eqn. (40) is replaced by

(cid:18)

P

sup
x,x(cid:48)∈M

(cid:12)
(cid:12)ZP ( (cid:98)PX )T ZP ( (cid:98)P (cid:48)

X ) − (cid:10)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

X )(cid:11)(cid:12)

(cid:12) ≥ (cid:15)

(cid:19)

≤ 29n1n2

(cid:19)2

(cid:18) σP σX r
(cid:15)l

exp

(cid:18) −L(cid:15)2
l

2(d + 2)

(cid:19)
.

(42)

The remainder of the proof now proceeds as in the previous proof.

A.8 Results in Tabular Format

Table 1: Average Classiﬁcation Error of Marginal Transfer Learning on Synthetic Data set

Table 2: Average Classiﬁcation Error of Pooling on Synthetic Data set

k
s
a
T
r
e
p

s
e
l
p
m
a
x
E

k
s
a
T
r
e
p

s
e
l

p
m
a
x
E

Tasks

16

64

256

8

16

32

36.01

33.08

31.69

31.55

31.03

30.96

30.44

29.31

23.87

256

23.78

7.22

1.27

Tasks

16

64

256

8

16

32

49.14

49.11

50.04

49.89

50.04

49.68

50.32

50.21

49.61

256

50.01

50.43

49.93

45

Table 3: RMSE of Marginal Transfer Learning on Parkinson’s Disease Data set

Tasks
20

10

15

25

30

35

13.78

12.37

11.93

10.74

10.08

11.17

14.18

11.89

11.51

10.90

10.55

10.18

14.95

13.29

12.00

10.21

10.59

9.52

13.27

11.66

11.79

12.89

11.27

11.17

13.15

11.70

13.81

10.12

9.16

9.91

9.28

9.03

9.03

8.01

9.34

9.10

9.01

8.44

8.16

7.30

7.14

10.50

10.05

8.69

7.62

7.88

7.01

7.5

12.16

13.03

11.98

9.59

9.16

9.18

8.48

9.85

8.80

9.74

9.52

100

12.69

20

24

28

34

41

49

58

70

84

20

24

28

34

41

49

58

70

84

k
s
a
T
r
e
p

s
e
l
p
m
a
x
E

k
s
a
T
r
e
p

s
e
l
p
m
a
x
E

Table 4: RMSE of Pooling on Parkinson’s Disease Data set

Tasks
20

10

15

25

30

35

13.64

11.93

11.95

11.06

11.91

12.08

13.80

11.83

11.70

11.98

11.68

11.48

13.78

11.70

11.72

11.18

11.58

11.73

13.71

12.20

12.04

11.17

11.67

11.92

13.69

11.73

12.08

11.28

11.55

12.59

13.75

11.85

11.79

11.17

11.34

11.82

13.70

11.89

12.06

11.06

11.82

11.65

13.54

11.86

12.14

11.21

11.40

11.96

13.55

11.98

12.03

11.25

11.54

12.22

100

13.53

11.85

11.92

11.12

11.96

11.84

46

Table 5: Average Classiﬁcation Error of Marginal Transfer Learning on Satellite Data set

Tasks
10

20

30

40

8.62

7.61

8.25

7.17

6.21

5.90

5.85

5.43

6.61

5.33

5.37

5.35

5.61

5.19

4.71

4.70

all training data

5.36

4.91

3.86

4.08

k
s
a
T
r
e
p

s
e
l
p
m
a
x
E

k
s
a
T
r
e
p

s
e
l
p
m
a
x
E

5

15

30

45

5

15

30

45

Table 6: Average Classiﬁcation Error of Pooling on Satellite Data set

Tasks
10

20

30

40

8.13

7.54

7.94

6.96

6.55

5.81

5.79

5.57

6.06

5.36

5.56

5.31

5.58

5.12

5.30

4.99

all training data

5.37

4.98

5.32

5.14

Table 7: Average Classiﬁcation Error of Marginal Transfer Learning on Flow Cytometry
Data set

Tasks
10

15

20

9.03

9.03

8.70

5

9

9.12

9.56

9.07

8.62

8.96

8.91

9.01

8.66

9.18

9.20

9.04

8.74

k
s
a
T
r
e
p

s
e
l
p
m
a
x
E

1024

2048

4096

8192

16384

9.05

9.08

9.04

8.63

47

Table 8: Average Classiﬁcation Error of Pooling on Flow Cytometry Data set

Tasks
10

5

15

20

1024

2048

4096

8192

9.41

9.48

9.32

9.52

9.92

9.57

9.45

9.54

9.72

9.56

9.36

9.40

9.43

9.53

9.38

9.50

16384

9.42

9.56

9.40

9.33

k
s
a
T
r
e
p

s
e
l
p
m
a
x
E

C. Scott and A. Deshmukh where supported in part by NSF Grants No. 1422157, 1217880,
and 1047871. G. Blanchard acknowledges support by the DFG via Research Unit 1735
Structural Inference in Statistics.

Acknowledgments

References

N. Aghaeepour, G. Finak, H. Hoos, T. R. Mosmann, R. Brinkman, R. Gottardo, R. H.
Scheuermann, FlowCAP Consortium, DREAM Consortium, et al. Critical assessment
of automated ﬂow cytometry data analysis techniques. Nature methods, 10(3):228–238,
2013.

Kei Akuzawa, Yusuke Iwasawa, and Yutaka Matsuo. Adversarial invariant feature learning

with accuracy constraint for domain generalization. ArXiv, abs/1904.12543, 2019.

Kamyar Azizzadenesheli, Anqi Liu, Fanny Yang, and Animashree Anandkumar. Regularized
learning for domain adaptation under label shifts. In International Conference on Learn-
ing Representations, 2019. URL https://openreview.net/forum?id=rJl0r3R9KX.

G¨okhan Bakır, Thomas Hofmann, Bernhard Sch¨olkopf, Alexander J Smola, and Ben Taskar.

Predicting structured data. MIT press, 2007.

Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa.

Metareg: To-
In S. Bengio, H. Wal-
wards domain generalization using meta-regularization.
ed-
and R. Garnett,
lach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
998–
itors, Advances
1008. Curran Associates,
URL http://papers.nips.cc/paper/
7378-metareg-towards-domain-generalization-using-meta-regularization.pdf.

Information Processing Systems

in Neural

pages

2018.

Inc.,

31,

P. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and

structural results. Journal of Machine Learning Research, 3:463–482, 2002.

P. Bartlett, M. Jordan, and J. McAuliﬀe. Convexity, classiﬁcation, and risk bounds. J.

Amer. Stat. Assoc., 101(473):138–156, 2006.

48

J. Baxter. A model of inductive bias learning. Journal of Artiﬁcial Intelligence Research,

12:149–198, 2000.

Shai Ben-David and Ruth Urner. On the hardness of domain adaptation and the utility
of unlabeled target samples. In Nader H. Bshouty, Gilles Stoltz, Nicolas Vayatis, and
Thomas Zeugmann, editors, Algorithmic Learning Theory, pages 139–153, 2012.

Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of repre-
sentations for domain adaptation. In B. Sch¨olkopf, J. C. Platt, and T. Hoﬀman, editors,
Advances in Neural Information Processing Systems 19, pages 137–144. 2007.

Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jen-
nifer Wortman Vaughan. A theory of learning from diﬀerent domains. Machine Learning,
79:151–175, 2010.

S. Bickel, M. Br¨uckner, and T. Scheﬀer. Discriminative learning under covariate shift. J.

Machine Learning Research, pages 2137–2155, 2009.

G. Blanchard, G. Lee, and C. Scott. Generalizing from several related classiﬁcation tasks
to a new unlabeled sample. In J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira,
and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 24,
pages 2178–2186. 2011.

G. Blanchard, M. Flaska, G. Handy, S. Pozzi, and C. Scott. Classiﬁcation with asymmetric
label noise: Consistency and maximal denoising. Electronic Journal of Statistics, 10:
2780–2824, 2016.

John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman.
Learning bounds for domain adaptation. In J. C. Platt, D. Koller, Y. Singer, and S. T.
Roweis, editors, Advances in Neural Information Processing Systems 20, pages 129–136.
2008.

Timothy I. Cannings, Yingying Fan, and Richard J. Samworth. Classiﬁcation with imperfect

training labels. Technical Report arXiv:1805.11505, 2018.

J. Carbonell, S. Hanneke, and L. Yang. A theory of transfer learning with applications to

active learning. Machine Learning, 90(2):161–189, 2013.

Fabio Maria Carlucci, Antonio D’Innocente, Silvia Bucci, Barbara Caputo, and Tatiana
Tommasi. Domain generalization by solving jigsaw puzzles. 2019 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pages 2224–2233, 2019.

R. Caruana. Multitask learning. Machine Learning, 28:41–75, 1997.

C. Chang and C. Lin. Libsvm: A library for support vector machines. ACM Transactions

on Intelligent Systems and Technology (TIST), 2(3):27, 2011.

A. Christmann and I. Steinwart. Universal kernels on non-standard input spaces. In J. Laf-
ferty, C. K. I. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, editors, Advances in
Neural Information Processing Systems 23, pages 406–414, 2010.

49

Corinna Cortes, Mehryar Mohri, Michael Riley, and Afshin Rostamizadeh. Sample selection

bias correction theory. In Algorithmic Learning Theory, pages 38–53, 2008.

Corinna Cortes, Mehryar Mohri, and Andr´es Mu˜noz Medina. Adaptation algorithm and
theory based on generalized discrepancy.
In Proceedings of the 21th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, KDD ’15, pages
169–178, 2015.

Daryl J. Daley and David Vere-Jones. An introduction to the theory of point processes,

volume II: general theory and structure. Springer, 2008.

G Denevi, Carlo Ciliberto, D Stamos, and Massimiliano Pontil. Incremental learning-to-
learn with statistical guarantees. In Proc. Uncertainty in Artiﬁcial Intelligence, 2018a.

Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil. Learning to
learn around a common mean. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing
Systems 31, pages 10169–10179. 2018b.

Zhengming Ding and Yun Fu. Deep domain generalization with structured low-rank con-

straint. IEEE Transactions on Image Processing, 27:304–313, 2018.

Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain gen-
eralization via model-agnostic learning of semantic features. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d’ Alch´e-Buc, E. Fox, and R. Garnett, editors, Advances in Neural
Information Processing Systems 32, pages 6450–6461. 2019.

P. Drineas and M. W. Mahoney. On the Nystr¨om method for approximating a gram matrix
for improved kernel-based learning. The Journal of Machine Learning Research, 6:2153–
2175, 2005.

M. C. Du Plessis and M. Sugiyama. Semi-supervised learning of class balance under class-
prior change by distribution matching. In J. Langford and J. Pineau, editors, Proc. 29th
Int. Conf. on Machine Learning, pages 823–830, 2012.

T. Evgeniou, C. A. Michelli, and M. Pontil. Learning multiple tasks with kernel methods.

J. Machine Learning Research, pages 615–637, 2005.

R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. Liblinear: A library for large linear

classiﬁcation. The Journal of Machine Learning Research, 9:1871–1874, 2008.

Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast
adaptation of deep networks. In Doina Precup and Yee Whye Teh, editors, Proceedings
of the 34th International Conference on Machine Learning, volume 70 of Proceedings of
Machine Learning Research, pages 1126–1135, 2017.

Chuang Gan, Tianbao Yang, and Boqing Gong. Learning attributes equals multi-source
domain generalization. In The IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), June 2016.

50

Pascal Germain, Amaury Habrard, Fran¸cois Laviolette, and Emilie Morvant. A new pac-
bayesian perspective on domain adaptation. In ICML, volume 48 of JMLR Workshop
and Conference Proceedings, pages 859–868, 2016.

M. Ghifary, D. Balduzzi, B. Kleijn, and M. Zhang. Scatter component analysis: A uniﬁed
framework for domain adaptation and domain generalization. IEEE Trans. Patt. Anal.
Mach. Intell., 39(7):1411–1430, 2017.

Muhammad Ghifary, W. Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain
generalization for object recognition with multi-task autoencoders. In Proceedings of the
2015 IEEE International Conference on Computer Vision (ICCV), page 25512559, 2015.

Mingming Gong, Kun Zhang, Tongliang Liu, Dacheng Tao, Clark Glymour, and Bernhard
Sch¨olkopf. Domain adaptation with conditional transferable components. In International
conference on machine learning, pages 2839–2848, 2016.

A. Gretton, K. Borgwardt, M. Rasch, B. Sch¨olkopf, and A. Smola. A kernel approach to
comparing distributions. In R. Holte and A. Howe, editors, Proceedings of the 22nd AAAI
Conference on Artiﬁcial Intelligence, pages 1637–1641, 2007a.

A. Gretton, K. Borgwardt, M. Rasch, B. Sch¨olkopf, and A. Smola. A kernel method for
the two-sample-problem. In B. Sch¨olkopf, J. Platt, and T. Hoﬀman, editors, Advances in
Neural Information Processing Systems 19, pages 513–520, 2007b.

T. Grubinger, A. Birlutiu, H. Sch¨oner, T. Natschl¨ager, and T. Heskes. Domain generaliza-
tion based on transfer component analysis. In I. Rojas, G. Joya, and A. Catala, editors,
Advances in Computational Intelligence. IWANN 2015, volume 9094 of Lecture Notes in
Computer Science, pages 325–334. Springer, 2015.

P. Hall. On the non-parametric estimation of mixture proportions. Journal of the Royal

Statistical Society, 43(2):147–156, 1981.

C. Hsieh, K. Chang, C. Lin, S. S. Keerthi, and S. Sundararajan. A dual coordinate descent
method for large-scale linear svm. In Proceedings of the 25th international conference on
Machine learning, pages 408–415. ACM, 2008.

Shoubo Hu, Kun Zhang, Zhitang Chen, and Laiwan Chan. Domain generalization via mul-
tidomain discriminant analysis. In Amir Globerson and Ricardo Silva, editors, Proceedings
of the Thirty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence, UAI 2019, Tel
Aviv, Israel, July 22-25, 2019, 2019.

Jiayuan Huang, Alexander J. Smola, Arthur Gretton, Karsten M. Borgwardt, and Bernhard
Scholkopf. Correcting sample selection bias by unlabeled data.
In Proceedings of the
19th International Conference on Neural Information Processing Systems, pages 601–608,
2007.

Wittawat Jitkrittum, Arthur Gretton, Nicolas Heess, SM Eslami, Balaji Lakshminarayanan,
Dino Sejdinovic, and Zolt´an Szab´o. Kernel-based just-in-time learning for passing expec-
tation propagation messages. In Proceedings of the Thirty-First Conference on Uncer-
tainty in Artiﬁcial Intelligence, pages 405–414. AUAI Press, 2015.

51

T. Joachims. Making large scale svm learning practical. Technical report, Universit¨at

Dortmund, 1999.

O. Kallenberg. Foundations of Modern Probability. Springer, 2002.

Takafumi Kanamori, Shohei Hido, and Masashi Sugiyama. A least-squares approach to

direct importance estimation. J. Mach. Learn. Res., 10:1391–1445, 2009.

Aditya Khosla, Tinghui Zhou, Tomasz Malisiewicz, Alexei A. Efros, and Antonio Torralba.
Undoing the damage of dataset bias. In Proceedings of the 12th European Conference on
Computer Vision - Volume Part I, page 158171, 2012.

Ron Kohavi et al. A study of cross-validation and bootstrap for accuracy estimation and

model selection. In IJCAI, volume 14, pages 1137–1145, 1995.

V. Koltchinskii. Rademacher penalties and structural risk minimization. IEEE Transactions

on Information Theory, 47(5):1902 – 1914, 2001.

P. Latinne, M. Saerens, and C. Decaestecker. Adjusting the outputs of a classiﬁer to new
a priori probabilities may signiﬁcantly improve classiﬁcation accuracy: Evidence from a
multi-class problem in remote sensing. In C. Sammut and A. H. Hoﬀmann, editors, Proc.
18th Int. Conf. on Machine Learning, pages 298–305, 2001.

Quoc Le, Tam´as Sarl´os, and Alex Smola. Fastfood: approximating kernel expansions in
In Proceedings of the 30th International Conference on International

loglinear time.
Conference on Machine Learning-Volume 28, pages III–244. JMLR. org, 2013.

Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and
artier domain generalization. In Proceedings of the IEEE International Conference on
Computer Vision, pages 5542–5550, 2017.

Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Learning to generalize:
Meta-learning for domain generalization. In Thirty-Second AAAI Conference on Artiﬁcial
Intelligence, 2018a.

H. Li, S. J. Pan, S. Wang, and A. C. Kot. Domain generalization with adversarial feature
learning. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, pages
5400–5409, 2018b.

Ya Li, Mingming Gong, Xinmei Tian, Tongliang Liu, and Dacheng Tao. Domain general-
ization via conditional invariant representations. In Thirty-Second AAAI Conference on
Artiﬁcial Intelligence, 2018c.

Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng
Tao. Deep domain generalization via conditional invariant adversarial networks. In Pro-
ceedings of the European Conference on Computer Vision (ECCV), pages 624–639, 2018d.

Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning
In COLT 2009 - The 22nd Conference on Learning Theory,

bounds and algorithms.
2009a.

52

Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation with
In Advances in neural information processing systems, pages 1041–

A. Maurer. Transfer bounds for linear feature learning. Machine Learning, 75(3):327–350,

multiple sources.
1048, 2009b.

2009.

A. Maurer, M. Pontil, and B. Romera-Paredes. Sparse coding for multitask and transfer
learning.
In Sanjoy Dasgupta and David McAllester, editors, Proceedings of the 30th
International Conference on Machine Learning, volume 28 of Proceedings of Machine
Learning Research, pages 343–351, 2013.

Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The beneﬁt of

multitask representation learning. J. Mach. Learn. Res., 17(1):2853–2884, 2016.

Aditya Krishna Menon, Brendan van Rooyen, and Nagarajan Natarajan. Learning from
binary labels with instance-dependent noise. Machine Learning, 107:1561–1595, 2018.

Saeid Motiian, Marco Piccirilli, Donald A Adjeroh, and Gianfranco Doretto. Uniﬁed deep
supervised domain adaptation and generalization. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision, pages 5715–5725, 2017.

K. Muandet, D. Balduzzi, and B. Sch¨olkopf. Domain generalization via invariant feature
representation. In Proceedings of the 30th International Conference on International Con-
ference on Machine Learning (ICML’13), volume 28 of Proceedings of Machine Learning
Research, pages I–10–I–18, 2013.

Nagarajan Natarajan, Inderjit S. Dhillon, Pradeep Ravikumar, and Ambuj Tewari. Cost-
sensitive learning with noisy labels. Journal of Machine Learning Research, 18(155):1–33,
2018. URL http://jmlr.org/papers/v18/15-226.html.

K. R. Parthasarathy. Probability Measures on Metric Spaces. Academic Press, 1967.

A. Pentina and C. Lampert. A pac-bayesian bound for lifelong learning. In Eric P. Xing
and Tony Jebara, editors, Proceedings of the 31st International Conference on Machine
Learning, volume 32 of Proceedings of Machine Learning Research, pages 991–999, 2014.

I.F. Pinelis and A.I. Sakhanenko. Remarks on inequalities for probabilities of large devia-

tions. Theory Probab. Appl., 30(1):143–148, 1985.

J. Quionero-Candela, M. Sugiyama, A. Schwaighofer, and N. D. Lawrence. Dataset Shift in

Machine Learning. The MIT Press, 2009.

A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in

neural information processing systems, pages 1177–1184, 2007.

Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random
features. In Advances in Neural Information Processing Systems, pages 3215–3225, 2017.

53

T. Sanderson and C. Scott. Class proportion estimation with application to multiclass
In Proceedings of the 17th International Conference on Artiﬁcial

anomaly rejection.
Intelligence and Statistics (AISTATS), 2014.

Clayton Scott. A generalized neyman-pearson criterion for optimal domain adaptation. In
Aur´elien Garivier and Satyen Kale, editors, Proceedings of the 30th International Con-
ference on Algorithmic Learning Theory, volume 98 of Proceedings of Machine Learning
Research, pages 738–761, 2019.

Shiv Shankar, Vihari Piratla, Soumen Chakrabarti, Siddhartha Chaudhuri, Preethi Jyothi,
In In-
and Sunita Sarawagi. Generalizing across domains via cross-gradient training.
ternational Conference on Learning Representations, 2018. URL https://openreview.
net/forum?id=r1Dx7fbCW.

S. Sharma and J. W. Cutler. Robust orbit determination and classiﬁcation: A learning

theoretic approach. Interplanetary Network Progress Report, 203:1, 2015.

B. Sriperumbudur, A. Gretton, K. Fukumizu, B. Sch¨olkopf, and G. Lanckriet. Hilbert space
embeddings and metrics on probability measures. Journal of Machine Learning Research,
11:1517–1561, 2010.

Bharath Sriperumbudur and Zolt´an Szab´o. Optimal rates for random fourier features. In

Advances in Neural Information Processing Systems, pages 1144–1152, 2015.

I. Steinwart and A. Christmann. Support Vector Machines. Springer, 2008.

Amos J Storkey. When training and test sets are diﬀerent: characterising learning transfer.

In In Dataset Shift in Machine Learning, pages 3–28. MIT Press, 2009.

Masashi Sugiyama, Taiji Suzuki, Shinichi Nakajima, Hisashi Kashima, Paul von B¨unau, and
Motoaki Kawanabe. Direct importance estimation for covariate shift adaptation. Annals
of the Institute of Statistical Mathematics, 60:699–746, 2008.

Dougal J. Sutherland and Jeﬀ Schneider. On the error of random Fourier features. In Pro-
ceedings of the Thirty-First Conference on Uncertainty in Artiﬁcial Intelligence, UAI’15,
pages 862–871. AUAI Press, 2015.

Zolt´an Szab´o, Bharath K Sriperumbudur, Barnab´as P´oczos, and Arthur Gretton. Learning
theory for distribution regression. The Journal of Machine Learning Research, 17(1):
5272–5311, 2016.

Dirk Tasche. Fisher consistency for prior probability shift. Journal of Machine Learning

Research, 18:1–32, 2017.

S. Thrun. Is learning the n-th thing any easier than learning the ﬁrst? Advances in Neural

Information Processing Systems, pages 640–646, 1996.

D. M. Titterington. Minimum distance non-parametric estimation of mixture proportions.

Journal of the Royal Statistical Society, 45(1):37–46, 1983.

54

J. Toedling, P. Rhein, R. Ratei, L. Karawajew, and R. Spang. Automated in-silico detection
of cell populations in ﬂow cytometry readouts and its application to leukemia disease
monitoring. BMC Bioinformatics, 7:282, 2006.

A. Tsanas, M. A. Little, P. E. McSharry, and L. O. Ramig. Accurate telemonitoring
IEEE transactions on

of parkinson’s disease progression by noninvasive speech tests.
Biomedical Engineering, 57(4):884–893, 2010.

Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun. Large
margin methods for structured and interdependent output variables. Journal of machine
learning research, 6(Sep):1453–1484, 2005.

Brendan van Rooyen and Robert C. Williamson. A theory of learning with corrupted labels.

Journal of Machine Learning Research, 18(228):1–50, 2018.

Haohan Wang, Zexue He, Zachary C. Lipton, and Eric P. Xing. Learning robust represen-
tations by projecting superﬁcial statistics out. In International Conference on Learning
Representations, 2019. URL https://openreview.net/forum?id=rJEjjoR9K7.

J. Wiens. Machine Learning for Patient-Adaptive Ectopic Beat Classication. Masters Thesis,
Department of Electrical Engineering and Computer Science, Massachusetts Institute of
Technology, 2010.

C. Williams and M. Seeger. Using the Nystr¨om method to speed up kernel machines. In
Proceedings of the 14th Annual Conference on Neural Information Processing Systems,
number EPFL-CONF-161322, pages 682–688, 2001.

Zheng Xu, Wen Li, Li Niu, and Dong Xu. Exploiting low-rank structure from latent domains
for domain generalization. In European Conference on Computer Vision, pages 628–643.
Springer, 2014.

Xiaolin Yang, Seyoung Kim, and Eric P Xing. Heterogeneous multitask learning with joint
sparsity constraints. In Advances in neural information processing systems, pages 2151–
2159, 2009.

Yao-Liang Yu and Csaba Szepesvari. Analysis of kernel mean matching under covariate
shift. In Proceedings of the 29th International Conference on Machine Learning, pages
607–614, 2012.

Bianca Zadrozny. Learning and evaluating classiﬁers under sample selection bias. In Pro-

ceedings of the Twenty-ﬁrst International Conference on Machine Learning, 2004.

Kun Zhang, Bernhard Sch¨olkopf, Krikamol Muandet, and Zhikun Wang. Domain adapta-
tion under target and conditional shift. In International Conference on Machine Learning,
pages 819–827, 2013.

Kun Zhang, Mingming Gong, and Bernhard Scholkopf. Multi-source domain adaptation:
A causal view. In Proceedings of the Twenty-Ninth AAAI Conference on Artiﬁcial Intel-
ligence, AAAI’15, pages 3150–3157. AAAI Press, 2015.

55

0
2
0
2
 
r
p
A
 
7
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
0
1
9
7
0
.
1
1
7
1
:
v
i
X
r
a

Domain Generalization by Marginal Transfer Learning

Gilles Blanchard
Institut f¨ur Mathematik
Universit¨at Potsdam

Aniket Anand Deshmukh
Microsoft AI & Research
¨Urun Dogan
Microsoft AI & Research

Gyemin Lee
Dept. Electronic and IT Media Engineering
Seoul National University of Science and Technology

Clayton Scott
Electrical and Computer Engineering
University of Michigan

blanchard@math.uni-potsdam.de

aniketde@umich.edu

urundogan@gmail.com

gyemin@seoultech.ac.kr

clayscot@umich.edu

Abstract
In the problem of domain generalization (DG), there are labeled training data sets from
several related prediction problems, and the goal is to make accurate predictions on future
unlabeled data sets that are not known to the learner. This problem arises in several ap-
plications where data distributions ﬂuctuate because of environmental, technical, or other
sources of variation. We introduce a formal framework for DG, and argue that it can be
viewed as a kind of supervised learning problem by augmenting the original feature space
with the marginal distribution of feature vectors. While our framework has several con-
nections to conventional analysis of supervised learning algorithms, several unique aspects
of DG require new methods of analysis.

This work lays the learning theoretic foundations of domain generalization, building on
our earlier conference paper where the problem of DG was introduced (Blanchard et al.,
2011). We present two formal models of data generation, corresponding notions of risk, and
distribution-free generalization error analysis. By focusing our attention on kernel meth-
ods, we also provide more quantitative results and a universally consistent algorithm. An
eﬃcient implementation is provided for this algorithm, which is experimentally compared
to a pooling strategy on one synthetic and three real-world data sets.
Keywords: Domain Generalization, Generalization Error Bounds, Kernel Methods, Uni-
versal Consistency, Kernel Approximation

1. Introduction

Domain generalization (DG) is a machine learning problem where the learner has access to
labeled training data sets from several related prediction problems, and must generalize to
a future prediction problem for which no labeled data are available. In more detail, there
are N labeled training data sets Si = (Xij, Yij)1≤j≤ni, i = 1, . . . , N , that describe similar
but possibly distinct prediction tasks. The objective is to learn a rule that takes as input

1

a previously unseen unlabeled test data set X T
for these or possibly other unlabeled points from the associated learning task.

1 , . . . , X T
nT

, and accurately predicts outcomes

DG arises in several applications. One prominent example is precision medicine, where
a common objective is to design a patient-speciﬁc classiﬁer (e.g., of health status) based
on clinical measurements, such as an electrocardiogram or electroencephalogram. In such
measurements, patient-to-patient variation is common, arising from biological variations
between patients, or technical or environmental factors inﬂuencing data acquisition. Be-
cause of patient-to-patient variation, a classiﬁer that is trained on data from one patient
may not be well matched to another patient. In this context, domain generalization enables
the transfer of knowledge from historical patients (for whom labeled data are available) to a
new patient without the need to acquire training labels for that patient. A detailed example
in the context of ﬂow cytometry is given below.

We view domain generalization as a conventional supervised learning problem where
the original feature space is augmented to include the marginal distribution generating the
features. We refer to this reframing of DG as “marginal transfer learning,” because it reﬂects
the fact that in DG, information about the test task must be drawn from that tasks’ marginal
feature distribution. Leveraging this perspective, we formulate two statistical frameworks
for analyzing DG. The ﬁrst framework allows the observations within each data set to
have arbitrary dependency structure, and makes connections to the literature on Campbell
measures and structured prediction. The second framework is a special case of the ﬁrst,
assuming the data points are drawn iid within each task, and allows for a more reﬁned risk
analysis.

We further develop a distribution-free kernel machine that employs a kernel on the
aforementioned augmented feature space. Our methodology is shown to yield a universally
consistent learning procedure under both statistical frameworks, meaning that the domain
generalization risk tends to the best possible value as the relevant sample sizes tend inﬁnity,
with no assumptions on the data generating distributions. Although DG may be viewed as
a conventional supervised learning problem on an augmented feature space, the analysis is
nontrivial owing to unique aspects of the sampling plans and risks. We oﬀer a computation-
ally eﬃcient and freely available1 implementation of our algorithm, and present a thorough
experimental study validating the proposed approach on one synthetic and three real-world
data sets, including comparisons to a simple pooling approach.

To our knowledge, the problem of domain generalization was ﬁrst proposed and studied
by our earlier conference publication (Blanchard et al., 2011) which this work extends
in several ways.
It adds (1) a new statistical framework, the agnostic generative model
described below; (2) generalization error and consistency results for the new statistical
model; (3) an extensive literature review; (4) an extension to the regression setting in
both theory and experiments; (5) a more general statistical analysis, in particular, we no
longer assume a bounded loss, and therefore accommodate common convex losses such as
the hinge and logistic losses; (6) extensive experiments (the conference paper considered a
single small dataset); (7) a scalable implementation based on a novel extension of random
Fourier features; and (8) error analysis for the random Fourier features approximation.

1. https://github.com/aniketde/DomainGeneralizationMarginal

2

2. Motivating Application: Automatic Gating of Flow Cytometry Data

Flow cytometry is a high-throughput measurement platform that is an important clinical
tool for the diagnosis of blood-related pathologies. This technology allows for quantitative
analysis of individual cells from a given cell population, derived for example from a blood
sample from a patient. We may think of a ﬂow cytometry data set as a set of d-dimensional
attribute vectors (Xj)1≤j≤n, where n is the number of cells analyzed, and d is the number
of attributes recorded per cell. These attributes pertain to various physical and chemical
properties of the cell. Thus, a ﬂow cytometry data set may be viewed as a random sample
from a patient-speciﬁc distribution.

Now suppose a pathologist needs to analyze a new (test) patient with data (X T

j )1≤j≤nT .
Before proceeding, the pathologist ﬁrst needs the data set to be “puriﬁed” so that only cells
of a certain type are present. For example, lymphocytes are known to be relevant for the
diagnosis of leukemia, whereas non-lymphocytes may potentially confound the analysis. In
other words, it is necessary to determine the label Y T
j ∈ {−1, 1} associated to each cell,
where Y T

j = 1 indicates that the j-th cell is of the desired type.

In clinical practice this is accomplished through a manual process known as “gating.”
The data are visualized through a sequence of two-dimensional scatter plots, where at each
stage a line segment or polygon is manually drawn to eliminate a portion of the unwanted
cells. Because of the variability in ﬂow cytometry data, this process is diﬃcult to quantify
in terms of a small subset of simple rules. Instead, it requires domain-speciﬁc knowledge
and iterative reﬁnement. Modern clinical laboratories routinely see dozens of cases per day,
so it would be desirable to automate this process.

Since clinical laboratories maintain historical databases, we can assume access to a
number (N ) of historical (training) patients that have already been expert-gated. Because
of biological and technical variations in ﬂow cytometry data, the distributions P (i)
XY of
the historical patients will vary. To illustrate the ﬂow cytometry gating problem, we use
the NDD data set from the FlowCap-I challenge.2 For example, Fig. 1 shows exemplary
two-dimensional scatter plots for two diﬀerent patients – see caption for details. Despite
diﬀerences in the two distributions, there are also general trends that hold for all patients.
Virtually every cell type of interest has a known tendency (e.g., high or low) for most
measured attributes. Therefore, it is reasonable to assume that there is an underlying
distribution (on distributions) governing ﬂow cytometry data sets, that produces roughly
similar distributions thereby making possible the automation of the gating process.

3. Formal Setting and General Results

In this section we formally deﬁne domain generalization via two possible data generation
models together with associated notions of risk. We also provide a basic generalization error
bound for the ﬁrst of these data generation models.

Let X denote the observation space (assumed to be a Radon space) and Y ⊆ R the
output space. Let PX and PX ×Y denote the set of probability distributions on X and X ×Y,
respectively. The spaces PX and PX ×Y are endowed with the topology of weak convergence
and the associated Borel σ-algebras. The symbol ⊗ indicates a product measure.

2. We will revisit this data set in Section 8.5 where details are given.

3

Figure 1: Two-dimensional projections of multi-dimensional ﬂow cytometry data. Each row
corresponds to a single patient, and each column to a particular two-dimensional projection.
The distribution of cells diﬀers from patient to patient. The colors indicate the results of
gating, where a particular type of cell, marked dark (blue), is separated from all other cells,
marked bright (red). Labels were manually selected by a domain expert.

The disintegration theorem for joint probability distributions (see for instance Kallen-
berg, 2002, Theorem 6.4) tells us that (under suitable regularity properties, satisﬁed if X is
a Radon space) any element PXY ∈ PX ×Y can be written as a Markov semi-direct product
PXY = PX • PY |X , with PX ∈ PX , PY |X ∈ PY |X , where PY |X is the space of conditional
probability distributions of Y given X, also called Markov transition kernels from X to Y.
This speciﬁcally means that

E(X,Y )∼PXY [h(X, Y )] =

h(x, y)PY |X (dy|X = x)

PX (dx),

(1)

(cid:90) (cid:18)(cid:90)

(cid:19)

for any integrable function h : X × Y → R. Following common terminology in the statistical
learning literature, we will also call PY |X the posterior distribution (of Y given X).

We assume that N training samples Si = (Xij, Yij)1≤j≤ni, i = 1, . . . , N , are observed.
To allow for possibly unequal sample sizes ni, it is convenient to formally identify each
sample Si with its associated empirical distribution (cid:98)P (i)
j=1 δ(Xij ,Yij ) ∈ PX ×Y .
We assume that the ordering of the observations inside a given sample Si is arbitrary
and does not contain any relevant information. We also denote by (cid:98)P (i)
j=1 δXij ∈
PX the ith training sample without labels. Similarly, a test sample is denoted by ST =
(X T

j )1≤j≤nT , and the empirical distribution of the unlabeled data by (cid:98)P T
X .

XY = 1
ni

X = 1
ni

j , Y T

(cid:80)ni

(cid:80)ni

3.1 Data Generation Models

We propose two data generation models. The ﬁrst is most general, and includes the second
as a special case.

4

Assumption 1 (AGM) There exists a distribution PS on PX ×Y such that S1, . . . , SN are
i.i.d. realizations from PS.

We call this the agnostic generative model. This is a quite general model in which samples are
assumed to be identically distributed and independent of each other, but nothing particular
is assumed about the generation mechanism for observations inside a given sample, nor for
the (random) sample size.

We also introduce a more speciﬁc generative mechanism, where observations (Xij, Yij)
XY , a latent unobserved random distribu-

inside the sample Si are themselves i.i.d. from P (i)
tion, as follows.

Assumption 2 (2SGM) There exists a distribution µ on PX ×Y and a distribution ν on
N, such that (P (1)
XY , nN ) are i.i.d. realizations from µ ⊗ ν, and conditional to
(P (i)
XY , ni) the sample Si is made of ni i.i.d. realizations of (X, Y ) following the distribution
P (i)
XY .

XY , n1), . . . , (P (N )

This model, called the 2-stage generative model, is a subcase of (AGM): since the
(P (i)
It has been considered in the distinct but
XY , ni) are i.i.d., the samples Si also are.
related context of “learning to learn” (Baxter, 2000; see also a more detailed discussion
below, Section 4.2). Many of our results will hold for the agnostic generative model, but
the two-stage generative model allows for additional developments, as will be discussed
further below. This model was the one studied in our conference paper (Blanchard et al.,
2011).

3.2 Decision Functions and Augmented Feature Space

In domain generalization, the learner’s goal is to infer from the training data a general
rule that takes an arbitrary, previously unseen, unlabeled dataset corresponding to a new
prediction task, and produces a classiﬁer for that prediction task that could be applied to
any x (possibly outside the unlabeled data set). In other words, the learner should out-
put a mapping g : PX → (X → R). Equivalently, the learner should output a function
f : PX × X → R, where the two notations are related via g(PX )(x) = f (PX , x). In the
latter viewpoint, f may be viewed as a standard decision function on the “augmented”
or “extended” feature space PX × X , which facilitates connections to standard supervised
learning. We refer to this view of DG as marginal transfer learning, because the informa-
tion that facilitates generalization to a new task is conveyed entirely through the marginal
distribution. In the next two subsections, we present two deﬁnitions of the risk of a decision
function f , one associated to each of the two data generation models.

3.3 Risk and Generalization Error Bound under the Agnostic Generative

Model

Now consider a test sample ST = (X T
j )1≤j≤nT , whose labels are not observed by the
learner. If (cid:96) : R×Y (cid:55)→ R+ is a loss function for a single prediction, and predictions of a ﬁxed
decision function f on the test sample are given by (cid:98)Y T
j ), then the empirical

j = f ( (cid:98)P T

j , Y T

X , X T

5

average loss incurred on the test sample is

L(ST , f ) :=

(cid:96)( (cid:98)Y T

j , Y T

j ) .

1
nT

nT(cid:88)

j=1

Based on this, we deﬁne the risk of a decision function as the average of the above quantity
when test samples are drawn according to the same mechanism as the training samples:

E(f ) := E

ST ∼PS

(cid:2)L(ST , f )(cid:3) = E

ST ∼PS

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:96)(f ( (cid:98)P T

X , X T

(cid:35)
i ), Y T
i )

.

In a similar way, we deﬁne the empirical risk of a decision function as its average prediction
error over the training samples:

(cid:98)E(f, N ) :=

L(Si, f ) =

(cid:96)(f ( (cid:98)P (i)

X , Xij), Yij).

(2)

1
N

N
(cid:88)

i=1

1
N

N
(cid:88)

ni(cid:88)

1
ni

i=1

j=1

Remark 3 It is possible to understand the above setting as a particular instance of a struc-
tured output learning problem (Tsochantaridis et al., 2005; Bakır et al., 2007), in which
the input variable X ∗ is (cid:98)P T
X , and the “structured output” Y ∗ is the collection of labels
(Y T
i )1≤i≤nT (matched to their respective input points). As is generally the case for struc-
tured output learning, the nature of the problem and the “structure” of the outputs is very
much encoded in the particular form of the loss function. In our setting the loss function is
additive over the labels forming the collection Y ∗, and we will exploit this particular form
for our method and analysis.

Remark 4 The risk E(f ) deﬁned above can be described in the following way: consider the
random variable ξ := ( (cid:98)PXY ; (X, Y )) obtained by ﬁrst drawing (cid:98)PXY according to PS, then,
conditional to this, drawing (X, Y ) according to (cid:98)PXY . The risk is then the expectation of a
certain function of ξ (namely Ff (ξ) = (cid:96)(f ( (cid:98)PX , X), Y )). In probability theory literature, the
distribution of the variable ξ is known as the Campbell measure associated to the distribution
PS over the measure space PX ×Y ; this object is in particular of fundamental use in point
process theory (see, e.g., Daley and Vere-Jones (2008), Section 13.1). We will denote
it by C(PS) here. This intriguing connection suggests that more elaborate tools of point
process literature may ﬁnd their use to analyze DG when various classical point processes
are considered for the generating distribution. The Campbell measure will also appear in
the Rademacher analysis below.

The next result establishes an analogue of classical Rademacher analysis under the

agnostic generative model.

Theorem 5 (Uniform estimation error control under (AGM)) Let F be a class of
decision functions PX × X → R. Assume the following boundedness condition holds:

sup
f ∈F

sup
PX ∈PX

sup
(x,y)∈X ×Y

(cid:96)(f (PX , x), y) ≤ B(cid:96).

(3)

6

Under (AGM), if S1, . . . , SN are i.i.d. realizations from PS, then with probability at least
1 − δ with respect to the draws of the training samples:

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:98)E(f, N ) − E(f )
(cid:12)

sup
f ∈F

≤

E

2
N

( (cid:98)P (i)

XY ;(Xi,Yi))∼C(PS )⊗N

E(εi)1≤i≤N

(cid:34)

sup
f ∈F

(cid:12)
N
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)

i=1

εi(cid:96)(f ( (cid:98)P (i)

(cid:35)

(cid:12)
(cid:12)
(cid:12)
X , Xi), Yi)
(cid:12)
(cid:12)

(cid:114)

+ B(cid:96)

log(δ−1)
2N

,

(4)

where (εi)1≤i≤N are i.i.d. Rademacher variables, independent from ( (cid:98)P (i)

XY , (Xi, Yi))1≤i≤N .

(cid:12)
(cid:12)
(cid:12)
(cid:12)
Proof Since the (Si)1≤i≤N are i.i.d., supf ∈F
(cid:12) takes the form of a uniform
(cid:12) (cid:98)E(f, N ) − E(f )
deviation between average and expected loss over the function class F. We can therefore
apply standard analysis (Azuma-McDiarmid inequality followed by Rademacher complexity
analysis for a nonnegative bounded loss; see, e.g., Koltchinskii, 2001; Bartlett and Mendel-
son, 2002, Theorem 8) to obtain that with probability at least 1 − δ with respect to the
draw of the training samples (Si)1≤i≤N :

(cid:12)
(cid:12)
(cid:12) (cid:98)E(f, N ) − E(f )

(cid:12)
(cid:12)
(cid:12) ≤

2
N

sup
f ∈F

E(Si)1≤i≤N

E(εi)1≤i≤N

(cid:34)

sup
f ∈F

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

N
(cid:88)

i=1

(cid:35)

(cid:12)
(cid:12)
(cid:12)
εiL(Si, f )
(cid:12)
(cid:12)

(cid:114)

+ B(cid:96)

log(δ−1)
2N

,

where (εi)1≤i≤N are i.i.d. Rademacher variables, independent of (Si)1≤i≤N .

We may write

L(Si, f ) =

1
ni

ni(cid:88)

j=1

(cid:96)(f ( (cid:98)P (i)

X , Xij), Yij) = E

(X,Y )∼ (cid:98)P (i)
XY

(cid:104)

(cid:96)(f ( (cid:98)P (i)

(cid:105)
X , X), Y )

;

thus, we have

E(Si)1≤i≤N

E(εi)1≤i≤N

εiL(Si, f )

(cid:34)

sup
f ∈F

(cid:12)
N
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)

i=1
(cid:34)

(cid:12)
(cid:35)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= E

( (cid:98)P (i)

XY )1≤i≤N

E(εi)1≤i≤N

(cid:12)
N
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)

i=1

sup
f ∈F

εiE

(Xi,Yi)∼ (cid:98)P (i)
XY

(cid:104)
(cid:96)(f ( (cid:98)P (i)

(cid:105)
X , Xi), Yi)

(cid:12)
(cid:35)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ E

E

( (cid:98)P (i)

XY )1≤i≤N

(X1,Y1)∼ (cid:98)P (1)

XY ,...,(XN ,YN )∼ (cid:98)P (N )

XY

E(εi)1≤i≤N

(cid:34)

sup
f ∈F

(cid:12)
N
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)

i=1

εi(cid:96)(f ( (cid:98)P (i)

(cid:35)

(cid:12)
(cid:12)
(cid:12)
X , Xi), Yi)
(cid:12)
(cid:12)

.

In the above inequality, the inner expectation on the (Xi, Yi) is pulled outwards by Jensen’s
inequality and convexity of the supremum.

To obtain the announced estimate, notice that the above expectation is the same as the

expectation w.r.t. to the N -fold Campbell measure C(PS) (see Remark 4).

Remark 6 The main term in the theorem is just the conventional Rademacher complexity
for the augmented feature space PX × X endowed with the Campbell measure C(PS). It
could also be thought of as the Rademacher complexity for the meta-distribution PS.

7

3.4 Risk under the 2-stage generative model

While we will state more results holding under (AGM) below, one advantage of the more
speciﬁc (2SGM) is to allow us to study the eﬀect of the sample sizes ni. In particular, for
the purpose of reducing computational complexity, we can analyze the eﬀect of subsampling
observations inside a given sample. Such an analysis would not be possible under (AGM).
For the test sample, parallel to the training data generating mechanism, under (2SGM)
XY , nT ) is drawn according to µ⊗ν, and conditional to this the test sample

we assume that (P T
ST is drawn from nT i.i.d. realizations of P T

Observe that under (2SGM), the distribution P (i)

XY is not observed and is therefore a
latent variable at training time as well as at test time. Still, by the law of large numbers,
if nT becomes large, (cid:98)P T
X (in the sense of weak convergence). This
motivates the introduction of the following risk which assumes access to an inﬁnite test
sample, and thus the true marginal P T
X :
E

X will converge to P T

(cid:2)(cid:96)(f (P T

X , X T ), Y T )(cid:3) .

E ∞(f ) := E

(X T ,Y T )∼P T

(5)

XY .

P T

XY ∼µ

XY

This quantity only makes sense under (2SGM). The following proposition makes the above
motivating observation more precise. First, under (2SGM), introduce the following risk
conditional to a ﬁnite test sample size nT :

E(f |nT ) = E

E

P T

XY ∼µ

(X T

i ,Y T

i )1≤i≤nT

∼(P T

XY )⊗nT

(cid:96)(f ( (cid:98)P T

X , X T

i ), Y T
i )

.

(6)

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:35)

Proposition 7 Assume (cid:96) is a bounded, L-Lipschitz loss function and f : PX × X → R is
a ﬁxed decision function which is continuous with respect to both its arguments (recalling
PX is endowed with the weak convergence topology). Then it holds under (2GSM):

lim
nT →∞

E(f |nT ) = E ∞(f ).

Remark 8 This result provides one setting where the risk E ∞ is clearly motivated as the
goal of asymptotic analysis when nT → ∞. Although Proposition 7 is not used elsewhere in
this work, a more quantitative version of this result is stated below for kernels (see Theorem
15), where convergence holds uniformly and the assumption of a bounded loss is dropped.

To gain more insight into the risk E ∞, recalling the standard decomposition (1) of
PXY into the marginal PX and the posterior PY |X , we observe that we can apply the
disintegration theorem not only to any PXY , but also to µ, and thus decompose it into two
parts, µX which generates the marginal distribution PX , and µY |X which, conditioned on
PX , generates the posterior PY |X . (More precise notation might be µPX instead of µX and
µPY |X |PX instead of µY |X , but this is rather cumbersome.) Denote (cid:101)X = (PX , X). We then
have, using Fubini’s theorem,

E ∞(f ) = EPX ∼µX

EPY |X ∼µY |X

EX∼PX

EY ∼PY |X

= EPX ∼µX

EPY |X ∼µY |X

EY ∼PY |X

EX∼PX
(cid:104)

= E

( (cid:101)X,Y )∼Qµ

(cid:96)(f ( (cid:101)X), Y )

.

(cid:105)

(cid:104)
(cid:96)(f ( (cid:101)X), Y )
(cid:104)
(cid:96)(f ( (cid:101)X), Y )

(cid:105)

(cid:105)

8

Here Qµ is the distribution that generates (cid:101)X by ﬁrst drawing PX according to µX , and then
drawing X according to PX ; similarly, Y is generated, conditioned on (cid:101)X, by ﬁrst drawing
PY |X according to µY |X , and then drawing Y from PY |X . (The distribution of (cid:101)X again
takes the form of a Campbell measure, see Remark 4.)

From the previous expression, we see that the risk E ∞ is like a standard supervised
learning risk based on ( (cid:101)X, Y ) ∼ Qµ. Thus, we can deduce properties that are known
to hold for supervised learning risks. For example, in the binary classiﬁcation setting, if
the loss is the 0/1 loss, then f ∗( (cid:101)X) = 2˜η( (cid:101)X) − 1 is an optimal predictor, where ˜η( (cid:101)X) =
E

(cid:2)1{Y =1}

(cid:3), and

Y ∼Qµ

Y | (cid:101)X

E ∞(f ) − E ∞(f ∗) = E

(cid:101)X∼Qµ
(cid:101)X

(cid:105)
(cid:104)
1{sign(f ( (cid:101)X))(cid:54)=sign(f ∗( (cid:101)X))}|2˜η( (cid:101)X) − 1|

.

Furthermore, consistency in the sense of E ∞ with respect to a general loss (cid:96) (thought of as
a surrogate) will imply consistency for the 0/1 loss, provided (cid:96) is classiﬁcation calibrated
(Bartlett et al., 2006).

X . There is, however, a condition where for µ-almost all test distributions P T

For a given loss (cid:96), the optimal or Bayes E ∞-risk in DG is in general larger than the
expected Bayes risk under the (random) test sample generating distribution P T
XY , because
it is typically not possible to fully determine the Bayes-optimal predictor from only the
marginal P T
XY ,
the decision function f ∗(P T
X , .) (where f ∗ is a global minimizer of (5)) coincides with an
optimal Bayes decision function for P T
XY . This condition is simply that the posterior PY |X
is (µ-almost surely) a function of PX (in other terms: that with the notation introduced
above, µY |X (PX ) is a Dirac measure for µ-almost all PX ). Although we will not be assuming
this condition throughout the paper under (2SGM), observe that it is implicitly assumed
in the motivating application presented in Section 2, where an expert labels the data points
by just looking at their marginal distribution.

Lemma 9 For a ﬁxed distribution PXY , and a decision function g : X → R, let us denote
R(g, PXY ) = E(X,Y )∼PXY [(cid:96)(g(X), Y )] and

R∗(PXY ) := min
g:X →R

R(g, PXY ) = min
g:X →R

E(X,Y )∼PXY [(cid:96)(g(X), Y )]

the corresponding optimal (Bayes) risk for the loss function (cid:96) under data distribution PXY .
Then under (2SGM):

E ∞(f ∗) ≥ EPXY ∼µ [R∗(PXY )] ,

where f ∗ : PX × X → R is a minimizer of the idealized DG risk E ∞ deﬁned in (5).

Furthermore, if µ is a distribution on PX ×Y such that µ-a.s. it holds PY |X = F (PX )

for some deterministic mapping F , then for µ-almost all PXY :

and

R(f ∗(PX , .), PXY ) = R∗(PXY )

E ∞(f ∗) = EPXY ∼µ [R∗(PXY )] .

9

Proof For any f : PX × X → R, one has for all PXY : R(f (PX , .), PXY ) ≥ R∗(PXY ).
Taking expectation with respect to PXY establishes the ﬁrst claim. Now for any ﬁxed
PX ∈ PX , consider PXY := PX • F (PX ) and g∗(PX ) a Bayes decision function for this joint
distribution. Pose f (PX , x) := g∗(PX )(x). Then f coincides for µ-almost all PXY with a
Bayes decision function for PXY , achieving equality in the above inequality. The second
equality follows by taking expectation over PXY ∼ µ.

Under (2SGM), we will establish that our proposed learning algorithm is E ∞-consistent,
provided the average sample size grows to inﬁnity as well as the total number of samples.
Thus, the above result provides a condition on µ under which it is possible to asymptotically
attain the Bayes risk on any test distribution although no labels from this test distribution
are observed.

More generally, and speaking informally, if µ is such that PY |X is close to being a function
of PX in some sense, we can expect the Bayes E ∞ risk for domain generalization to be close
to the expected Bayes risk for a random test distribution. We reiterate, however, that we
make no assumptions on µ in this work so that the two quantities may be far apart. In
the worst case, the posterior may be independent of the marginal, in which case a method
for domain generalization will do no better than the na¨ıve pooling strategy. For further
discussion, see the comparison of domain adaptation and domain generalization in the next
section.

4. Related Work

Since at least the 1990s, machine learning researchers have investigated the possibility of
solving one learning problem by leveraging data from one or more related problems. In this
section, we provide an overview of such problems and their relation to domain generalization,
while also reviewing prior work on DG.

Two critical terms are domain and task. Use of these terms is not consistent throughout
the literature, but at a minimum, the domain of a learning problem describes the input
(feature) space X and marginal distribution of X, while the task describes the output space
Y and the conditional distribution of Y given X (also called posterior). In many settings,
however, the sets X and Y are the same for all learning problems, and the terms “domain”
and “task” are used interchangeably to refer to a joint distribution PXY on X × Y. This is
the perspective adopted in this work, as well as in much of the work on multi-task learning,
domain adaptation (DA), and domain generalization.

Multi-task learning is similar to DG, except only the training tasks are of interest,
and the goal is to leverage the similarity among distributions to improve the learning of
individual predictors for each task (Caruana, 1997; Evgeniou et al., 2005; Yang et al., 2009).
In contrast, in DG, we are concerned with generalization to a new task.

Domain adaptation refers to the setting in which there is a speciﬁc target task and one
or more source tasks. The goal is to design a predictor for the target task, for which there
are typically few to no labeled training examples, by leveraging labeled training data from
the source task(s). DA is reviewed below, and contrasted with DG.

10

4.1 Domain Generalization vs. Domain Adaptation

Formulations of domain adaptation may take several forms, depending on the number of
sources and whether there are any labeled examples from the target to supplement the
unlabeled examples. In multi-source, unsupervised domain adaptation, the learner is pre-
sented with labeled training data from several source distributions, and unlabeled data from
a target marginal distribution (see Zhang et al. (2015) and references therein). Thus, the
available data are the same as in domain generalization, and algorithms for one of these
problems may be applied to the other. In this section we illuminate the diﬀerence between
DA and DG.

In all forms of DA, the goal is to attain optimal performance with respect to the joint
distribution of the target domain. For example, if the performance measure is a risk, the goal
is to attain the Bayes risk for the target domain. To achieve this goal, it is necessary to make
assumptions about how the source and target distributions are related (Quionero-Candela
et al., 2009). For example, several works adopt the covariate shift assumption, which
requires the source and target domains to have the same posterior, allowing the marginals
to diﬀer arbitrarily (Zadrozny, 2004; Huang et al., 2007; Cortes et al., 2008; Sugiyama
et al., 2008; Bickel et al., 2009; Kanamori et al., 2009; Yu and Szepesvari, 2012; Ben-David
and Urner, 2012). Another common assumption is target shift, which stipulates that the
source and target have the same class-conditional distributions, allowing the prior class
probability to change (Hall, 1981; Titterington, 1983; Latinne et al., 2001; Storkey, 2009;
Du Plessis and Sugiyama, 2012; Sanderson and Scott, 2014; Azizzadenesheli et al., 2019).
Mansour et al. (2009b); Zhang et al. (2015) assume that the target posterior is a weighted
combination of source posteriors, while Zhang et al. (2013); Gong et al. (2016) extend target
shift by also allowing the class-conditional distributions to undergo a location-scale shift,
and Tasche (2017) assumes the ratio of class-conditional distributions is unchanged. Work
on classiﬁcation with label noise assumes the source data are obtained from the target
distribution but the labels have been corrupted in either a label-dependent (Blanchard
et al., 2016; Natarajan et al., 2018; van Rooyen and Williamson, 2018) or feature-dependent
(Menon et al., 2018; Cannings et al., 2018; Scott, 2019) way. Finally, there are several works
that assume the existence of a predictor that achieves good performance on both source and
target domains (Ben-David et al., 2007, 2010; Blitzer et al., 2008; Mansour et al., 2009a;
Cortes et al., 2015; Germain et al., 2016).

The key diﬀerence between DG and DA may be found in the performance measures
optimized. In DG, the goal is to design a single predictor f (PX , x) that can apply to any
future task, and risk is assessed with respect to the draw of both a new task, and (under
2SGM) a new data point from that task. This is in contrast to DA, where the target
distribution is typically considered ﬁxed, and the goal is to design a predictor f (x) where, in
assessing the risk, the only randomness is in the draw of a new sample from the target task.
This diﬀerence in performance measures for DG and DA has an interesting consequence
for analysis. As we will show, it is possible to attain optimal risk (asymptotically) in DG
without making any distributional assumptions like those described above for DA. Of course,
this optimal risk is typically larger than the Bayes risk for any particular target domain
(see Lemma 9). An interesting question for future research is whether it is possible to

11

close or eliminate this gap (between DG and expected DA risks) by imposing distributional
assumptions like those for DA.

Another diﬀerence between DA and DG lies in whether the learning algorithm must be
rerun for each new test data set. Most unsupervised DA methods employ the unlabeled
target data for training and thus, when a new unlabeled target data set is presented, the
learning algorithm must be rerun. In contrast, most existing DG methods do not assume
access to the unlabeled test data at learning time, and are capable of making predictions
as new unlabeled data sets arrive without any further training.

4.2 Domain Generalization vs. Learning to Learn

In the problem of learning to learn (LTL, Thrun, 1996), which has also been called bias
learning, meta-learning, and (typically in an online setting) lifelong learning, there are la-
beled datasets for several tasks, as in DG. There is also a given family of learning algorithms,
and the objective is to design a meta-learner that selects the learning algorithm that will
perform best on future tasks. The learning theoretic study of LTL traces to the work of
Baxter (2000), who was the ﬁrst to propose a distribution on tasks, which he calls an “en-
vironment,” and which coincides with our µ. Given this setting, the performance of the
learning algorithm selected by a meta-learner is obtained by drawing a new task at random,
drawing a labeled training dataset from that task, running the selected algorithm, drawing
a test point, and evaluating the expected loss, where the expectation is with respect to all
sources of randomness (new task, training data from new task, test point from new task).

Baxter analyzes learning algorithms given by usual empirical risk minimization over a
hypothesis (prediction function) class, and the goal of the meta-learner is then to select a
hypothesis class from a family of such classes. He shows that it is possible to ﬁnd a good
trade-oﬀ between the complexity of a hypothesis class and its approximation capabilities
for tasks sampled from µ, in an average sense. In particular, the information gained by
ﬁnding a well-adapted hypothesis class can lead to signiﬁcantly improved sample eﬃciency
when learning a new task. See Maurer (2009) for a discussion of the performance measure
studied by Baxter (2000), which is slightly diﬀerent from the one described above.

Later work on LTL establishes similar results that quantify the ability of a meta-learner
to transfer knowledge to a new task. These meta-learners all optimize a particular structure
that deﬁnes a learning algorithm, such as a feature representation (Maurer, 2009; Maurer
et al., 2016; Denevi et al., 2018a), a prior on predictors in a PAC-Bayesian setting (Pentina
and Lampert, 2014), a dictionary (Maurer et al., 2013), the bias of a regularizer (Denevi
et al., 2018b), and a pretrained neural network (Finn et al., 2017). It is also worth not-
ing that some algorithms on multi-task learning extract structures that characterize an
environment and can be applied to LTL.

Although DG and LTL both involve generalization to a new task, they are clearly
diﬀerent problems because LTL assumes access to labeled data from the new task, whereas
DG only sees unlabeled data and requires no additional learning. In LTL, the learner can
achieve the Bayes risk for the new task, the only issue is the sample complexity. DG is
thus a more challenging problem, but also potentially more useful since in many transfer
learning settings, labeled data for the new task are unavailable.

12

4.3 Prior Work on Domain Generalization

To our knowledge, the ﬁrst paper to consider domain generalization (as formulated in Sec-
tion 3.2) was our earlier conference paper (Blanchard et al., 2011). The term “domain
generalization” was coined by Muandet et al. (2013), who study the same setting and build
upon our work by extracting features that facilitate DG. Carbonell et al. (2013) study an
active learning variant of DG in the realizable setting, and directly learn the task sampling
distribution.

Other methods for DG were studied by Khosla et al. (2012); Xu et al. (2014); Grubinger
et al. (2015); Ghifary et al. (2015); Gan et al. (2016); Ghifary et al. (2017); Motiian et al.
(2017); Li et al. (2017, 2018a,b,c,d); Balaji et al. (2018); Ding and Fu (2018); Shankar
et al. (2018); Hu et al. (2019); Dou et al. (2019); Carlucci et al. (2019); Wang et al. (2019);
Akuzawa et al. (2019). Many of these methods learn a common feature space for all tasks.
Such methods are complementary to the method that we study. Indeed, our kernel-based
learning algorithm may be applied after having learned a feature representation by another
method, as was done by Muandet et al. (2013). Since our interest is primarily theoretical,
we restrict our experimental comparison to another algorithm that also operates directly on
the original input space, namely, a simple pooling algorithm that lumps all training tasks
into a single data set and trains a single support vector machine.

5. Learning Algorithm

In this section, we introduce a concrete algorithm to tackle the learning problem exposed in
Section 3, using an approach based on kernels. The function k : Ω×Ω → R is called a kernel
on Ω if the matrix (k(xi, xj))1≤i,j≤n is symmetric and positive semi-deﬁnite for all positive
integers n and all x1, . . . , xn ∈ Ω. It is well known that every kernel k on Ω is associated to
a space of functions f : Ω → R called the reproducing kernel Hilbert space (RKHS) Hk with
kernel k. One way to envision Hk is as follows. Deﬁne Φ(x) := k(·, x), which is called the
canonical feature map associated with k. Then the span of {Φ(x) : x ∈ Ω}, endowed with
the inner product (cid:104)Φ(x), Φ(x(cid:48))(cid:105) = k(x, x(cid:48)), is dense in Hk. We also recall the reproducing
property, which states that (cid:104)f, Φ(x)(cid:105) = f (x) for all f ∈ Hk and x ∈ Ω.

For later use, we introduce the notion of a universal kernel. A kernel k on a compact
metric space Ω is said to be universal when its RKHS is dense in C(Ω), the set of continuous
functions on Ω, with respect to the supremum norm. Universal kernels are important for
establishing universal consistency of many learning algorithms. See Steinwart and Christ-
mann (2008) for background on kernels and reproducing kernel Hilbert spaces.

Several well-known learning algorithms, such as support vector machines and kernel
ridge regression, may be viewed as minimizers of a norm-regularized empirical risk over
the RKHS of a kernel. A similar development has also been made for multi-task learning
Inspired by this framework, we consider a general kernel-based
(Evgeniou et al., 2005).
algorithm as follows.

Consider the loss function (cid:96) : R × Y → R+. Let k be a kernel on PX × X , and let Hk
be the associated RKHS. For the sample Si, recall that (cid:98)P (i)
j=1 δXij denotes the
corresponding empirical X distribution. Also consider the extended input space PX × X
X , Xij). Note that (cid:98)P (i)
and the extended data (cid:101)Xij = ( (cid:98)P (i)
X plays a role analogous to the task

X = 1
ni

(cid:80)ni

13

index in multi-task learning. Now deﬁne

(cid:98)fλ = arg min

f ∈Hk

1
N

N
(cid:88)

ni(cid:88)

1
ni

i=1

j=1

(cid:96)(f ( (cid:101)Xij), Yij) + λ (cid:107)f (cid:107)2 .

Algorithms for solving (7) will be discussed in Section 7.

5.1 Specifying the Kernels

In the rest of the paper we will consider a kernel k on PX × X of the product form

k((P1, x1), (P2, x2)) = kP (P1, P2)kX (x1, x2),

where kP is a kernel on PX and kX a kernel on X .

Furthermore, we will consider kernels on PX of a particular form. Let k(cid:48)

X denote a
kernel on X (which might be diﬀerent from kX ) that is measurable and bounded. We deﬁne
the kernel mean embedding Ψ : PX → Hk(cid:48)

:

PX (cid:55)→ Ψ(PX ) :=

k(cid:48)
X (x, ·)dPX (x).

X

(cid:90)

X

This mapping has been studied in the framework of “characteristic kernels” (Gretton et al.,
2007a), and it has been proved that universality of k(cid:48)
X implies injectivity of Ψ (Gretton
et al., 2007b; Sriperumbudur et al., 2010).

Note that the mapping Ψ is linear. Therefore, if we consider the kernel kP (PX , P (cid:48)

(cid:104)Ψ(PX ), Ψ(P (cid:48)
reason, we introduce yet another kernel K on Hk(cid:48)

X ) =
X )(cid:105), it is a linear kernel on PX and cannot be a universal kernel. For this
and consider the kernel on PX given by

X

kP (PX , P (cid:48)

X ) = K (cid:0)Ψ(PX ), Ψ(P (cid:48)

X )(cid:1) .

Note that particular kernels inspired by the ﬁnite dimensional case are of the form
K(v, v(cid:48)) = F ((cid:13)

(cid:13)v − v(cid:48)(cid:13)
(cid:13)),

or

K(v, v(cid:48)) = G((cid:10)v, v(cid:48)(cid:11)),

where F, G are real functions of a real variable such that they deﬁne a kernel. For exam-
ple, F (t) = exp(−t2/(2σ2)) yields a Gaussian-like kernel, while G(t) = (1 + t)d yields a
polynomial-like kernel. Kernels of the above form on the space of probability distributions
over a compact space X have been introduced and studied in Christmann and Steinwart
(2010). Below we apply their results to deduce that k is a universal kernel for certain choices
of kX , k(cid:48)

X , and K.

5.2 Relation to Other Kernel Methods

By choosing k diﬀerently, one can recover other existing kernel methods.
consider the class of kernels of the same product form as above, but where

In particular,

(7)

(8)

(9)

(10)

(11)

(12)

kP (PX , P (cid:48)

X ) =

(cid:26) 1 PX = P (cid:48)
X
τ PX (cid:54)= P (cid:48)
X

14

If τ = 0, the algorithm (7) corresponds to training N kernel machines f ( (cid:98)P (i)
X , ·) using kernel
kX (e.g., support vector machines in the case of the hinge loss) on each training data set,
independently of the others (note that this does not oﬀer any generalization ability to a new
data set). If τ = 1, we have a “pooling” strategy that, in the case of equal sample sizes ni,
is equivalent to pooling all training data sets together in a single data set, and running a
conventional supervised learning algorithm with kernel kX (i.e., this corresponds to trying
to ﬁnd a single “one-ﬁts-all” prediction function which does not depend on the marginal).
In the intermediate case 0 < τ < 1, the resulting kernel is a “multi-task kernel,” and the
algorithm recovers a multitask learning algorithm like that of Evgeniou et al. (2005). We
compare to the pooling strategy below in our experiments. We also examined the multi-
task kernel with τ < 1, but found that, as far as generalization to a new unlabeled task is
concerned, it was always outperformed by pooling, and so those results are not reported.
This ﬁts the observation that the choice τ = 0 does not provide any generalization to a
new task, while τ = 1 at least oﬀers some form of generalization, if only by ﬁtting the same
predictor to all data sets.

In the special case where all labels Yij are the same value for a given task, and kX is
taken to be the constant kernel, the problem we consider reduces to “distributional” classi-
ﬁcation or regression, which is essentially standard supervised learning where a distribution
(observed through a sample) plays the role of the feature vector. Many of our analysis
techniques specialize to this setting.

6. Learning Theoretic Study

This section presents generalization error and consistency analysis for the proposed kernel
method under the agnostic and 2-stage generative models. Although the regularized esti-
mation formula (7) deﬁning (cid:98)fλ is standard, the generalization error analysis is not, owing
to the particular sampling structures and risks under (AGM) and (2SGM).

6.1 Universal Consistency under the Agnostic Generative Model

We will consider the following assumptions on the loss function and kernels:

(LB) The loss function (cid:96) : R × Y → R+ is L(cid:96)-Lipschitz in its ﬁrst variable and satisﬁes

(K-Bounded) The kernels kX , k(cid:48)

X and K are bounded respectively by constants B2

k, B2

k(cid:48) ≥

B0 := supy∈Y (cid:96)(0, y) < ∞.

1, and B2
K .

The condition B0 < ∞ always holds for classiﬁcation, as well as certain regression
settings. The boundedness assumptions are clearly satisﬁed for Gaussian kernels, and can
be enforced by normalizing the kernel (discussed further below).

We begin with a generalization error bound that establishes uniform estimation error
control over functions belonging to a ball of Hk . We then discuss universal kernels, and
ﬁnally deduce universal consistency of the algorithm.

Let Bk(r) denote the closed ball of radius r, centered at the origin, in the RKHS of the
kernel k. We start with the following simple result allowing us to bound the loss on a RKHS
ball.

15

Lemma 10 Suppose k is a kernel on a set Ω, bounded by B2. Let (cid:96) : R × Y → [0, ∞) be a
loss satisfying (LB). Then for any R > 0 and f ∈ Bk(R), and any z ∈ Ω and y ∈ Y,

(cid:12)(cid:96)(f (z), y)(cid:12)
(cid:12)

(cid:12) ≤ B0 + L(cid:96)RB

(13)

Proof By the Lipschitz continuity of (cid:96), the reproducing property, and Cauchy-Schwarz,
we have

(cid:12)(cid:96)(f (z), y)(cid:12)
(cid:12)

(cid:12)(cid:96)(f (z), y) − (cid:96)(0, y)(cid:12)
(cid:12)

(cid:12) ≤ (cid:96)(0, y) + (cid:12)
≤ B0 + L(cid:96)|f (z) − 0|
(cid:12)(cid:104)f, k(z, ·)(cid:105)(cid:12)
(cid:12)
= B0 + L(cid:96)
(cid:12)
≤ B0 + L(cid:96)(cid:107)f (cid:107)Hk B
≤ B0 + L(cid:96)RB.

The expression in (13) serves to replace the boundedness assumption (3) in Theorem 5.

We now state the following, which is a specialization of Theorem 5 to the kernel setting.

Theorem 11 (Uniform estimation error control over RKHS balls) Assume (LB)
and (K-Bounded) hold, and data generation follows (AGM). Then for any R > 0, with
probability at least 1 − δ (with respect to the draws of the samples Si, i = 1, . . . , N )

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ≤ (B0 + L(cid:96)RBKBk)
(cid:12) (cid:98)E(f, N ) − E(f )

((cid:112)log δ−1 + 2)
√
N

.

sup
f ∈Bk(R)

(14)

Proof This is a direct consequence of Theorem 5 and of Lemma 10, the kernel k on
PX × X being bounded by B2
K. As noted there, the main term in the upper bound (4)
is a standard Rademacher complexity on the augmented input space P × X , endowed with
the Campbell measure C(PS).

kB2

In the kernel learning context, we can bound the Rademacher complexity term using
a standard bound for the Rademacher complexity of a Lipschitz loss function on the ball
of radius R of Hk (Koltchinskii, 2001; Bartlett and Mendelson, 2002, e.g., Theorems 8, 12
and Lemma 22 there), using again the bound B2

K on the kernel k, giving the conclusion.

kB2

Next, we turn our attention to universal kernels (see Section 5 for the deﬁnition). A
relevant notion for our purposes is that of a normalized kernel. If k is a kernel on Ω, then

k∗(x, x(cid:48)) :=

k(x, x(cid:48))
(cid:112)k(x, x)k(x(cid:48), x(cid:48))

is the associated normalized kernel. If a kernel is universal, then so is its associated normal-
ized kernel. For example, the exponential kernel k(x, x(cid:48)) = exp(κ(cid:104)x, x(cid:48)(cid:105)Rd), κ > 0, can be
shown to be universal on Rd through a Taylor series argument. Consequently, the Gaussian
kernel

kσ(x, x(cid:48)) :=

exp( 1

σ2 (cid:104)x, x(cid:48)(cid:105))
2σ2 (cid:107)x(cid:107)2) exp( 1

exp( 1

2σ2 (cid:107)x(cid:48)(cid:107)2)

16

is universal, being the normalized kernel associated with the exponential kernel with κ =
1/σ2. See Steinwart and Christmann (2008) for additional details and discussion.
To establish that k is universal on PX × X , the following lemma is useful.

Lemma 12 Let Ω, Ω(cid:48) be two compact spaces and k, k(cid:48) be kernels on Ω, Ω(cid:48), respectively. If
k, k(cid:48) are both universal, then the product kernel

k((x, x(cid:48)), (y, y(cid:48))) := k(x, y)k(cid:48)(x(cid:48), y(cid:48))

is universal on Ω × Ω(cid:48).

Several examples of universal kernels are known on Euclidean space. For our purposes,
we also need universal kernels on PX . Fortunately, this was studied by Christmann and
Steinwart (2010). Some additional assumptions on the kernels and feature space are re-
quired:

(K-Univ) kX , k(cid:48)

X , K, and X satisfy the following:

• X is a compact metric space

• kX is universal on X
• k(cid:48)

X is continuous and universal on X

• K is universal on any compact subset of Hk(cid:48)

.

X

Adapting the results of Christmann and Steinwart (2010), we have the following.

Theorem 13 (Universal kernel) Assume condition (K-Univ) holds. Then, for kP de-
ﬁned as in (10), the product kernel k in (8) is universal on PX × X .

Furthermore, the assumption on K is fulﬁlled if K is of the form (12), where G is an
analytical function with positive Taylor series coeﬃcients, or if K is the normalized kernel
associated to such a kernel.

Proof By Lemma 12, it suﬃces to show PX is a compact metric space, and that kP (PX , P (cid:48)
X )
is universal on PX . The former statement follows from Theorem 6.4 of Parthasarathy
(1967), where the metric is the Prohorov metric. We will deduce the latter statement from
Theorem 2.2 of Christmann and Steinwart (2010). The statement of Theorem 2.2 there
is apparently restricted to kernels of the form (12), but the proof actually only uses that
the kernel K is universal on any compact set of Hk(cid:48)
. To apply Theorem 2.2, it remains
is a separable Hilbert space, and that Ψ is injective and continuous.
to show that Hk(cid:48)
Injectivity of Ψ is equivalent to k(cid:48)
X being a characteristic kernel, and follows from the
assumed universality of k(cid:48)
X (Sriperumbudur et al., 2010). The continuity of k(cid:48)
X implies
(Steinwart and Christmann (2008), Lemma 4.33) as well as continuity of
separability of Hk(cid:48)
Ψ (Christmann and Steinwart (2010), Lemma 2.3 and preceding discussion). Now Theorem
2.2 of Christmann and Steinwart (2010) may be applied, and the results follows.

X

X

X

The fact that kernels of the form (12), where G is analytic with positive Taylor coeﬃ-
was established in the proof of Theorem

cients, are universal on any compact set of Hk(cid:48)
2.2 of the same work (Christmann and Steinwart, 2010).

X

17

As an example, suppose that X is a compact subset of Rd. Let kX and k(cid:48)

X be Gaussian
kernels on X . Taking G(t) = exp(t), it follows that K(PX , P (cid:48)
)
is universal on PX . By similar reasoning as in the ﬁnite dimensional case, the Gaussian-like
kernel K(PX , P (cid:48)
) is also universal on PX . Thus the
product kernel is universal on PX × X .

X ) = exp((cid:104)Ψ(PX ), Ψ(P (cid:48)

2σ2 (cid:107)Ψ(PX ) − Ψ(P (cid:48)

X ) = exp(− 1

X )(cid:105)Hk(cid:48)

X )(cid:107)2

Hk(cid:48)
X

X

From Theorems 11 and 13, we may deduce universal consistency of the learning algo-

rithm.

Corollary 14 (Universal consistency) Assume that conditions (LB), (K-Bounded)
and (K-Univ) are satisﬁed. Let λ = λ(N ) be a sequence such that as N → ∞: λ(N ) → 0
and λ(N )N/ log N → ∞. Then

E( (cid:98)fλ(N )) → inf

f :PX ×X →R

E(f ) a.s., as N → ∞.

The proof of the corollary relies on the bound established in Theorem 11, the universality
of k established in Theorem 13, and otherwise relatively standard arguments.

One notable feature of this result is that we have established consistency where only N
is required to diverge. In particular, the training sample sizes ni may remain bounded. In
the next subsection, we consider the role of the ni under the 2-stage generative model.

6.2 Role of the Individual Sample Sizes under the 2-Stage Generative Model

In this section, we are concerned with the role of the individual sample sizes (ni)1≤i≤N .
More precisely, in some applications the number of training points per task is large, which
can give rise to a high computational burden at the learning stage (and also for storing the
learned model in computer memory). We investigate to which extent reducing the number of
training points per task (by random subsampling) in order to reduce computational burden
can be done without suﬀering a signiﬁcant loss in statistical performance. For this we need
a more speciﬁc model for the generating model of points in each task, and we therefore
assume here that the (2SGM), introduced in Section 3.1, holds.

We will consider the following additional assumption.

(K-H¨older) The canonical feature map ΦK : Hk(cid:48)

→ HK associated to K satisﬁes a H¨older

X

condition of order α ∈ (0, 1] with constant LK, on Bk(cid:48)

(Bk(cid:48)) :

X

∀v, w ∈ Bk(cid:48)

(Bk(cid:48)) :

X

(cid:107)ΦK(v) − ΦK(w)(cid:107) ≤ LK (cid:107)v − w(cid:107)α .

(15)

Suﬃcient conditions for (15) are described in Section A.4. As an example, the condition is
shown to hold with α = 1 when K is the Gaussian-like kernel on Hk(cid:48)

.

Since we are interested in the inﬂuence of the number of training points per task, it
is helpful to introduce notations for the (2SGM) risks that are conditioned on a ﬁxed
task PXY . Thus, we introduce the following notation, in analogy to (5)–(6) introduced in

X

18

Section 3.4, for risk at sample size n, and risk at inﬁnite sample size, conditional to PXY :

E(f |PXY , n) := E

ST ∼(PXY )⊗n

(cid:96)(f ( (cid:98)PX , Xi), Yi)

;

(cid:34)

1
n

n
(cid:88)

i=1

(cid:35)

E ∞(f |PXY ) := E(X,Y )∼PXY [(cid:96)(f (PX , X), Y )] .

(16)

(17)

The following proposition gives an upper bound on the discrepancy between these risks.
It can be seen as a quantitative version of Proposition 7 in the kernel setting, which is
furthermore uniform over an RKHS ball.

If the
Theorem 15 Assume conditions (LB), (K-Bounded), and (K-H¨older) hold.
sample S = (Xj, Yj)1≤j≤n is made of n i.i.d. realizations from PXY , with PXY and n ﬁxed,
then for any R > 0, with probability at least 1 − δ:

sup
f ∈Bk(R)

|L(S, f ) − E ∞(f |PXY )| ≤ (B0 + 3L(cid:96)RBk(Bα

k(cid:48)LK + BK))

(cid:19)− α

2

(cid:18) log(3δ−1)
n

.

(18)

Averaging over the draw of S, again with PXY and n ﬁxed, it holds for any R > 0:

sup
f ∈Bk(R)

|E(f |PXY , n) − E ∞(f |PXY )| ≤ 2L(cid:96)RBkLKBα

k(cid:48)n−α/2.

(19)

As a consequence, for the unconditional risks when (PXY , n) is drawn from µ ⊗ ν under
(2GSM), for any R > 0:

|E(f ) − E ∞(f )| ≤ 2RL(cid:96)BkLKBα

k(cid:48)E

(cid:104)
n− α

2

(cid:105)

.

(20)

sup
f ∈Bk(R)

The above results are useful in a number of ways. First, under (2SGM), we can consider
the goal of asymptotically achieving the optimal risk inf f E ∞(f ), where we recall that E ∞(f )
is the expected loss of a decision function f over a random test task P T
XY in the case where
P T
X would be perfectly observed (this can be thought of as observing as an inﬁnite sample
from the marginal). Equation (20) bounds the risk under (2SGM) in terms of the risk
under (AGM), for which we have already established consistency. Thus, consistency under
(2SGM) will be possible if the number of examples ni per training task also grows together
with the number of training tasks N . The following result formalizes this intuition.

XY , . . . , P (N )

Corollary 16 Assume (LB), (K-Bounded), and (K-H¨older) hold, and let n be ﬁxed.
If P (1)
from µ, and the corresponding samples S1, . . . , SN are
i.i.d. all of size n from their respective distributions, then for any R > 0, with probability
at least 1 − δ with respect to the draws of the training tasks and training samples

XY are drawn i.i.d.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
N

N
(cid:88)

i=1

(cid:12)
(cid:12)
L(Si, f ) − E ∞(f )
(cid:12)
(cid:12)
(cid:12)

sup
f ∈Bk(R)

≤ (B0 + L(cid:96)RBKBk)

+ 2RL(cid:96)BkLKBα

k(cid:48)n− α
2 .

(21)

((cid:112)log δ−1 + 2)
√
N

19

Furthermore, if assumption (K-Univ) is satisﬁed and, as the number of training tasks
N → ∞, the sample size n = n(N ) → ∞, and the regularization parameter λ(N ) is such
that λ(N ) → 0 and λ(N ) min(N, nα) → ∞, then

E( (cid:98)fλ(N )) → inf

f :PX ×X →R

E ∞(f ) in probability, as N → ∞.

Proof The setting is that of the (2GSM) model, where the sample size is ﬁxed at n (i.e.
the sample size distribution ν is the Dirac δn). This is a particular case of (AGM), so we
can apply Theorem 11 and combine with (20) (wherein the test sample is also of size n) to
get the announced bound. The consistency statement follows the same argument as in the
proof of Corollary 14, with E(f ) replaced by E ∞(f ), and ε(N ) there replaced by the RHS
in (21). We consider only consistency in probability this time, since a.s. consistency does
not appear of relevance in a setting where the individual training task sample size grows
with the number of tasks.

Remark 17 Our conference paper (Blanchard et al., 2011) also established a generalization
error bound and consistency for E ∞ under (2SGM). That bound had a diﬀerent form for
two main reasons. First, it assumed the loss to be bounded, whereas the present analysis
avoids that assumption via Lemma 10. Second, that analysis did not leverage a connection
to (AGM), which led to a log N in the second term. This required the two sample sizes to
be coupled asymptotically to achieve consistency. In the present analysis, the two sample
sizes N and n may diverge at arbitrary rates.

Remark 18 It is possible to obtain a result similar to Corollary 16 when the training task
sample sizes (ni)1≤i≤N are unequal and possibly arbitrary. In this case we would follow a
slightly diﬀerent argument, using (18) for all training tasks together with a union bound,
and applying Theorem 11 to the idealized situation with an inﬁnite number of samples per
training task. This way, the order n− α
. We eschew
an exact statement for brevity.

2 is replaced by log(N )N −1 (cid:80)N

i=1 n

− α
2
i

Coming back to our initial motivation of possibly reducing computational burden by
subsampling, using (21) we can compare under (2SGM) the two settings where we have
the same task generating distribution µ but diﬀerent individual training task sample sizes
n versus n(cid:48) < n. Under the (2SGM) model, the setting n(cid:48) < n can be obtained by simple
random subsampling of the original data. We see that the statistical risk bound in (21) is
unchanged up to a small factor if n(cid:48) ≥ min(N α, n). Assuming α = 1 to simplify, in the case
where the original sample sizes n are much larger than the number of training tasks N , this
suggests that we can subsample to n(cid:48) ≈ N without taking a signiﬁcant hit to generalization
performance. This applies equally well to subsampling the tasks used for prediction or
testing. The most precise statement in this regard is (18), since it bounds the deviations of
the observed prediction loss for a ﬁxed task PXY and i.i.d. sample from that task.

The minimal subsampling size n(cid:48) can be interpreted as an optimal eﬃciency/accuracy
tradeoﬀ, since it reduces computational complexity as much as possible without sacriﬁcing
statistical accuracy. Similar considerations appear in the context of distribution regres-
sion (Szab´o et al., 2016, Remark 6). In that reference, a sharp analysis giving rise to fast

20

convergence rates is presented, resulting in a more involved optimal balance between N and
n. In the present work, we have focused on slow rates based on a uniform control of the
estimation error over RKHS balls; we leave for future work sharper convergence bounds (un-
der additional regularity conditions), which would also give rise to more reﬁned balancing
conditions between n and N .

7. Implementation

Implementation3 of the algorithm in (7) relies on techniques that are similar to those used
for other kernel methods, but with some variations. The ﬁrst subsection illustrates how,
for the case of hinge loss, the optimization problem corresponds to a certain cost-sensitive
support vector machine. Subsequent subsections focus on more scalable implementations
based on approximate feature mappings.

7.1 Representer Theorem and Hinge loss

For a particular loss (cid:96), existing algorithms for optimizing an empirical risk based on that
loss can be adapted to the setting of marginal transfer learning. We now illustrate this
idea for the case of the hinge loss, (cid:96)(t, y) = max(0, 1 − yt). To make the presentation more
concise, we will employ the extended feature representation (cid:101)Xij = ( (cid:98)P (i)
X , Xij), and we will
also “vectorize” the indices (i, j) so as to employ a single index on these variables and on
the labels. Thus the training data are ( (cid:101)Xi, Yi)1≤i≤M , where M = (cid:80)N
i=1 ni, and we seek a
solution to

min
f ∈Hk

M
(cid:88)

i=1

ci max(0, 1 − Yif ( (cid:101)Xi)) +

1
2

(cid:107)f (cid:107)2 .

Here ci = 1
, where m is the smallest positive integer such that i ≤ n1 + · · · + nm. By
the representer theorem (Steinwart and Christmann, 2008), the solution of (7) has the form

λN nm

for real numbers ri. Plugging this expression into the objective function of (7), and intro-
ducing the auxiliary variables ξi, we have the quadratic program

(cid:98)fλ =

rik( (cid:101)Xi, ·)

M
(cid:88)

i=1

min
r,ξ

1
2

rT Kr +

ciξi

M
(cid:88)

i=1

M
(cid:88)

j=1
ξi ≥ 0, ∀i,

s.t. Yi

rjk( (cid:101)Xi, (cid:101)Xj) ≥ 1 − ξi, ∀i

21

3. Software available at https://github.com/aniketde/DomainGeneralizationMarginal

where K := (k( (cid:101)Xi, (cid:101)Xj))1≤i,j≤M . Using Lagrange multiplier theory, the dual quadratic
program is

max
α

−

1
2

M
(cid:88)

i,j=1

s.t. 0 ≤ αi ≤ ci ∀i,

αiαjYiYjk( (cid:101)Xi, (cid:101)Xj) +

αi

M
(cid:88)

i=1

and the optimal function is

(cid:98)fλ =

αiYik( (cid:101)Xi, ·).

M
(cid:88)

i=1

This is equivalent to the dual of a cost-sensitive support vector machine, without oﬀset,
where the costs are given by ci. Therefore we can learn the weights αi using any existing
software package for SVMs that accepts example-dependent costs and a user-speciﬁed kernel
matrix, and allows for no oﬀset. Returning to the original notation, the ﬁnal predictor given
a test X-sample ST has the form

(cid:98)fλ( (cid:98)P T

X , x) =

αijYijk(( (cid:98)P (i)

X , Xij), ( (cid:98)P T

X , x))

N
(cid:88)

ni(cid:88)

i=1

j=1

where the αij are nonnegative. Like the SVM, the solution is often sparse, meaning most
αij are zero.

Finally, we remark on the computation of kP ( (cid:98)PX , (cid:98)P (cid:48)

(12), the calculation of kP may be reduced to computations of the form
If (cid:98)PX and (cid:98)P (cid:48)
then

X are empirical distributions based on the samples X1, . . . , Xn and X (cid:48)

X ). When K has the form of (11) or
(cid:69)
(cid:68)
Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)
.
X )
1, . . . , X (cid:48)

n(cid:48),

(cid:68)
Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

X )

(cid:69)

=

k(cid:48)
X (Xi, ·),

X (X (cid:48)
k(cid:48)

j, ·)

(cid:43)

(cid:42)

1
n

n
(cid:88)

i=1

1
n(cid:48)

n(cid:48)
(cid:88)

j=1

=

1
nn(cid:48)

n
(cid:88)

n(cid:48)
(cid:88)

i=1

j=1

X (Xi, X (cid:48)
k(cid:48)

j).

Note that when k(cid:48)
a smoothing kernel density estimate for PX .

X is a (normalized) Gaussian kernel, Ψ( (cid:98)PX ) coincides (as a function) with

7.2 Approximate Feature Mapping for Scalable Implementation

Assuming ni = n, for all i, the computational complexity of a nonlinear SVM solver (in our
context) is between O(N 2n2) and O(N 3n3) (Joachims, 1999; Chang and Lin, 2011). Thus,
standard nonlinear SVM solvers may be insuﬃcient when N or n are very large.

One approach to scaling up kernel methods is to employ approximate feature mappings
together with linear solvers. This is based on the idea that kernel methods are solving for
a linear predictor after ﬁrst nonlinearly transforming the data. Since this nonlinear trans-
formation can have an extremely high- or even inﬁnite-dimensional output, classical kernel

22

methods avoid computing it explicitly. However, if the feature mapping can be approxi-
mated by a ﬁnite dimensional transformation with a relatively low-dimensional output, one
can directly solve for the linear predictor, which can be accomplished in O(N n) time (Hsieh
et al., 2008).

In particular, given a kernel k, the goal is to ﬁnd an approximate feature mapping z(˜x)
such that k(˜x, ˜x(cid:48)) ≈ z(˜x)T z(˜x(cid:48)). Given such a mapping z, one then applies an eﬃcient linear
solver, such as Liblinear (Fan et al., 2008), to the training data (z( ˜Xij), Yij)ij to obtain a
weight vector w. The ﬁnal prediction on a test point ˜x is then wT z(˜x). As described in
the previous subsection, the linear solver may need to be tweaked, as in the case of unequal
sample sizes ni, but this is usually straightforward.

Recently, such low-dimensional approximate future mappings z(x) have been developed
for several kernels. We examine two such techniques in the context of marginal transfer
learning, the Nystr¨om approximation (Williams and Seeger, 2001; Drineas and Mahoney,
2005) and random Fourier features. The Nystr¨om approximation applies to any kernel
method, and therefore extends to the marginal transfer setting without additional work.
On the other hand, we give a novel extension of random Fourier features to the marginal
transfer learning setting (for the case of all Gaussian kernels), together with performance
analysis. Our approach is similar to the one in Jitkrittum et al. (2015) which proposes a
two-stage approximation for the mean embedding. Note that Jitkrittum et al. (2015) does
not give an error bound.

7.2.1 Random Fourier Features

The approximation of Rahimi and Recht (2007) is based on Bochner’s theorem, which
characterizes shift invariant kernels.

Theorem 19 A continuous kernel k(x, y) = k(x − y) on Rd is positive deﬁnite iﬀ k(x − y)
is the Fourier transform of a ﬁnite positive measure p(w), i.e.,

k(x − y) =

p(w)ejwT (x−y)dw .

(22)

If a shift invariant kernel k(x − y) is properly scaled then Theorem 19 guarantees that

p(w) in (22) is a proper probability distribution.

(cid:90)

Rd

23

Random Fourier features (RFFs) approximate the integral in (22) using samples drawn

from p(w). If w1, w2, ..., wL are i.i.d. draws from p(w),

k(x − y) =

p(w)ejwT (x−y)dw

p(w) cos(wT x − wT y)dw

cos(wT

i x − wT

i y)

(cid:90)

(cid:90)

Rd

Rd

L
(cid:88)

i=1
L
(cid:88)

i=1
L
(cid:88)

1
L

1
L

1
L

=

≈

=

=

cos(wT

i x) cos(wT

i y) + sin(wT

i x) sin(wT

i y)

[cos(wT

i x), sin(wT

i x)]T [cos(wT

i y), sin(wT

i y)]

i=1
= zw(x)T zw(y) ,

(23)

[cos(wT

1 x), sin(wT

L x), sin(wT

L x)] ∈ R2L is an approximate
where zw(x) = 1√
1 x), ..., cos(wT
L
In the following, we extend the RFF
nonlinear feature mapping of dimensionality 2L.
methodology to the kernel ¯k on the extended feature space PX × X . Let X1, . . . , Xn1
and X (cid:48)
X respectively, and let (cid:98)PX and (cid:98)P (cid:48)
X
denote the corresponding empirical distributions. Given x, x(cid:48) ∈ X , denote ˜x = ( (cid:98)PX , x)
and ˜x(cid:48) = ( (cid:98)P (cid:48)
X , x(cid:48)). The goal is to ﬁnd an approximate feature mapping ¯z(˜x) such that
¯k(˜x, ˜x(cid:48)) ≈ ¯z(˜x)T ¯z(˜x(cid:48)). Recall that

realizations of PX and P (cid:48)

n2 be i.i.d.

1, . . . , X (cid:48)

¯k(˜x, ˜x(cid:48)) = kP ( (cid:98)PX , (cid:98)P (cid:48)

X )kX (x, x(cid:48));

speciﬁcally, we consider kX and k(cid:48)
kP to have the Gaussian-like form

X to be Gaussian kernels and the kernel on distributions

kP ( (cid:98)PX , (cid:98)P (cid:48)

X ) = exp

(cid:107)Ψ( (cid:98)PX ) − Ψ( (cid:98)P (cid:48)

X )(cid:107)2

(cid:26) 1
2σ2
P

(cid:27)

.

Hk(cid:48)
X

As noted earlier in this section, the calculation of kP ( (cid:98)PX , (cid:98)P (cid:48)
of

X ) reduces to the computation

(cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

X )(cid:105) =

X (Xi, X (cid:48)
k(cid:48)

j).

(24)

1
n1n2

n1(cid:88)

n2(cid:88)

i=1

j=1

24

We use Theorem 19 to approximate k(cid:48)

X and thus (cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

X )(cid:105). Let w1, w2, ..., wL be

i.i.d. draws from p(cid:48)(w), the inverse Fourier transform of k(cid:48)

X . Then we have:

(cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

X )(cid:105) =

X (Xi, X (cid:48)
k(cid:48)
j)

1
n1n2

n1(cid:88)

n2(cid:88)

i=1

j=1

≈

=

=

1
Ln1n2

1
Ln1n2

1
Ln1n2

L
(cid:88)

n1(cid:88)

n2(cid:88)

l=1

i=1

j=1

L
(cid:88)

n1(cid:88)

n2(cid:88)

l=1

i=1

j=1

L
(cid:88)
{

n1(cid:88)

l=1

i=1
= ZP ( (cid:98)PX )T ZP ( (cid:98)P (cid:48)

X ),

cos(wT

l Xi − wT

l X (cid:48)
j)

[cos(wT

l Xi) cos(wT

l X (cid:48)

j) + sin(wT

l Xi) sin(wT

l X (cid:48)

j)]

[cos(wT

l Xi), sin(wT

l Xi)]T

[cos(wT

l X (cid:48)

j), sin(wT

l X (cid:48)

j)]}

n2(cid:88)

j=1

where

ZP ( (cid:98)PX ) =

cos(wT

1 Xi), sin(wT

1 Xi), ..., cos(wT

L Xi), sin(wT

L Xi)

(25)

(cid:105)
,

n1(cid:88)

(cid:104)

1
√

n1

L

i=1

and ZP ( (cid:98)P (cid:48)
let z(cid:48)
1
n1

X ) is deﬁned analogously with n1 replaced by n2. For the proof of Theorem 20,
X , which satisﬁes ZP ( (cid:98)PX ) =

X denote the approximate feature map corresponding to k(cid:48)
(cid:80)n1
i=1 z(cid:48)

X (Xi).

Note that the lengths of the vectors ZP ( (cid:98)PX ) and ZP ( (cid:98)P (cid:48)

X ) are 2L. To approximate ¯k we

may write

¯k(˜x, ˜x(cid:48)) ≈ exp

−(cid:107)x − x(cid:48)(cid:107)2
Rd
2σ2
X
P (cid:107)x − x(cid:48)(cid:107)2

Rd)

R2L + σ2

· exp

−(cid:107)ZP ( (cid:98)PX ) − ZP ( (cid:98)P (cid:48)

X )(cid:107)2

R2L

−(σ2

2σ2
P
X (cid:107)ZP ( (cid:98)PX ) − ZP ( (cid:98)P (cid:48)
2σ2

X )(cid:107)2
P σ2
X
X )(cid:107)2
−((cid:107)σX ZP ( (cid:98)PX ) − σX ZP ( (cid:98)P (cid:48)
P σ2
2σ2
X

R2L + (cid:107)σP x − σP x(cid:48)(cid:107)2

Rd)

−(cid:107)(σX ZP ( (cid:98)PX ), σP x) − (σX ZP ( (cid:98)P (cid:48)

X ), σP x(cid:48))(cid:107)2

R2L+d

.

2σ2

P σ2
X

= exp

= exp

= exp

(26)

This is also a Gaussian kernel, now on R2L+d. Again by applying Theorem 19, we have

¯k( (cid:98)PX , X), ( (cid:98)P (cid:48)

X , X (cid:48))) ≈

p(v)ejvT ((σX ZP (PX ),σP X)−(σX ZP (P (cid:48)

X ),σP X (cid:48)))dv.

(cid:90)

R2L+d

Let v1, v2, ..., vq be drawn i.i.d.
kernel with bandwidth σP σX . Let u = (σX ZP ( (cid:98)PX ), σP x) and u(cid:48) = (σX ZP ( (cid:98)P (cid:48)

from p(v), the inverse Fourier transform of the Gaussian
X ), σP x(cid:48)).

25

Then

where

¯k(˜x, ˜x(cid:48)) ≈

cos(vT

q (u − u(cid:48)))

Q
(cid:88)

1
Q

q=1
= ¯z(˜x)T ¯z(˜x(cid:48)),

¯z(˜x) =

[cos(vT

1 u), sin(vT

1 u), ..., cos(vT

Qu), sin(vT

Qu)] ∈ R2Q

(27)

1
√
Q

and ¯z(˜x(cid:48)) is deﬁned similarly.

This completes the construction of the approximate feature map. The following result,
which uses Hoeﬀding’s inequality and generalizes a result of Rahimi and Recht (2007), says
that the approximation achieves any desired approximation error with very high probability
as L, Q → ∞.

Theorem 20 Let L be the number of random features to approximate the kernel on distri-
butions and Q be the number of features to approximate the ﬁnal product kernel. For any
(cid:15)l > 0, (cid:15)q > 0, ˜x = ( (cid:98)PX , x), ˜x(cid:48) = ( (cid:98)P (cid:48)

X , x(cid:48)),

P (|¯k(˜x, ˜x(cid:48)) − ¯z(˜x)T ¯z(˜x(cid:48))| ≥ (cid:15)l + (cid:15)q) ≤ 2 exp

(cid:16)

−

(cid:17)

Q(cid:15)2
q
2

+ 6n1n2 exp

−

(28)

(cid:16)

(cid:17)

,

L(cid:15)2
2

where (cid:15) = σ2
n1 and n2 are the sizes of the empirical distributions (cid:98)PX and (cid:98)P (cid:48)

2 log(1 + (cid:15)l), σP is the bandwidth parameter of the Gaussian-like kernel kP , and

X , respectively.

P

The above results holds for ﬁxed ˜x and ˜x(cid:48). Following again Rahimi and Recht (2007),
one can use an (cid:15)-net argument to prove a stronger statement for every pair of points in the
input space simultaneously. They show

Lemma 21 Let M be a compact subset of Rd with diameter r = diam(M) and let D be the
number of random Fourier features used. Then for the mapping deﬁned in (23), we have

(cid:16)

P

sup
x,y∈M

(cid:17)
|zw(x)T zw(y) − k(x − y)| ≥ (cid:15)

(cid:17)2

≤ 28(cid:16) σr
(cid:15)

exp

(cid:16) −D(cid:15)2
2(d + 2)

(cid:17)

,

where σ = E[wT w] is the second moment of the Fourier transform of k.

Our RFF approximation of ¯k is grounded on Gaussian RFF approximations on Euclidean
spaces, and thus, the following result holds by invoking Lemma 21, and otherwise following
the argument of Theorem 20.

Theorem 22 Using the same notations as in Theorem 20 and Lemma 21,

(cid:16)

P

sup
x,x(cid:48)∈M

|¯k(˜x, ˜x(cid:48)) − ¯z(˜x)T ¯z(˜x(cid:48))| ≥ (cid:15)l + (cid:15)q

(cid:17)

(cid:17)2

≤ 28(cid:16) σ(cid:48)
X r
(cid:15)q

exp

(cid:17)

(cid:16) −Q(cid:15)2
q
2(d + 2)

+ 293n1n2

(cid:16) σP σX r
(cid:15)l
X in Eqn. (24) and σP and σX are the widths of kernels

(cid:16) −L(cid:15)2
l

2(d + 2)

(29)

exp

(cid:17)2

(cid:17)

where σ(cid:48)
kP and kX respectively.

X is the width of kernel k(cid:48)

26

There are recent developments that give faster rates for approximation quality of random
Fourier features and could potentially be combined with our analysis (Sriperumbudur and
Szab´o, 2015; Sutherland and Schneider, 2015). For example, approximation quality for the
kernel mean map is discussed in Sutherland and Schneider (2015), and these ideas could be
extended to Theorem 22 by combining with the two-stage approach presented in this paper.
We also note that our analysis of random Fourier features is separate from our analysis of
the kernel learning algorithm. We have not presented a generalization error bound for the
learning algorithm using random Fourier features (Rudi and Rosasco, 2017).

7.2.2 Nystr¨om Approximation

Like random Fourier features, the Nystr¨om approximation is a technique to approximate
kernel matrices (Williams and Seeger, 2001; Drineas and Mahoney, 2005). Unlike random
Fourier features, for the Nystr¨om approximation, the feature maps are data-dependent.
Also, in the last subsection, all kernels were assumed to be shift invariant. With the
Nystr¨om approximation there is no such assumption.

For a general kernel k, the goal is to ﬁnd a feature mapping z : Rd → RL, where L > d,
such that k(x, x(cid:48)) ≈ z(x)T z(x(cid:48)). Let r be the target rank of the ﬁnal approximated kernel
matrix, and m be the number of selected columns of the original kernel matrix. In general
r ≤ m (cid:28) n.

Given data points x1, . . . , xn, the Nystr¨om method approximates the kernel matrix by
m without replacement from the original sample,
j)]n×m,

ﬁrst sampling m data points x(cid:48)
and then constructing a low rank matrix by (cid:98)Kr = Kb (cid:98)K−1KT
and (cid:98)K = [k(x(cid:48)

j)]m×m. Hence, the ﬁnal approximate feature mapping is

b , where Kb = [k(xi, x(cid:48)

2, ..., x(cid:48)

1, x(cid:48)

i, x(cid:48)

zn(x) = (cid:98)D− 1

2 (cid:98)V T [k(x, x(cid:48)

1), ..., k(x, x(cid:48)

m)],

(30)

where (cid:98)D is the eigenvalue matrix of (cid:98)K and (cid:98)V is the corresponding eigenvector matrix.

The Nystr¨om approximation holds for any positive deﬁnite kernel, but random Fourier
features can be used only for shift invariant kernels. On the other hand, random Fourier
features are very easy to implement and the Nystr¨om method has additional time complexity
due to an eigenvalue decomposition. Moreover, the Nystr¨om method is useful only when
the kernel matrix has low rank. For additional comparison of various kernel approximation
approaches we refer the reader to Le et al. (2013).
In our experiments, we use random
Fourier features when all kernels are Gaussian and the Nystr¨om method otherwise.

8. Experiments

This section empirically compares our marginal transfer learning method with pooling.4
One implementation of the pooling algorithm was mentioned in Section 5.2, where kP is
taken to be a constant kernel. Another implementation is to put all the training data sets
together and train a single conventional kernel method. The only diﬀerence between the two
implementations is that in the former, weights of 1/ni are used for examples from training
task i. In almost all of our experiments below, the various training tasks have the same

4. Software available at https://github.com/aniketde/DomainGeneralizationMarginal

27

sample sizes, in which case the two implementations coincide. The only exception is the
fourth experiment when we use all training data, in which case we use the second of the
two implementations mentioned above.

We consider three classiﬁcation problems (Y = {−1, 1}), for which the hinge loss is
employed, and one regression problem (Y ⊂ R), where the (cid:15)-insensitive loss is employed.
Thus, the algorithms implemented are natural extensions of support vector classiﬁcation
and regression to domain generalization. Performance of a learning strategy is assessed by
holding out several data sets ST
, learning a decision function (cid:98)f on the remaining
data sets, and reporting the average empirical risk 1
i , (cid:98)f ). In some cases, this
NT
value is again averaged over several randomized versions of the experiment.

1 , . . . , ST
NT

i=1 L(ST

(cid:80)NT

8.1 Model Selection

T x2 and Gaussian kernels kσ(x1, x2) = exp (cid:0) − ||x1−x2||2

The various experiments use diﬀerent combinations of kernels. In all experiments, linear
(cid:1) were used.
kernels k(x1, x2) = x1
The bandwidth σ of each Gaussian kernel and the regularization parameter λ of the
machines were selected by grid search. For model selection, ﬁve-fold cross-validation was
used. In order to stabilize the cross-validation procedure, it was repeated 5 times over in-
dependent random splits into folds (Kohavi et al., 1995). Thus, candidate parameter values
were evaluated on the 5 × 5 validation sets and the conﬁguration yielding the best average
performance was selected. If any of the chosen hyper-parameters was at the grid boundary,
the grid was extended accordingly, i.e., the same grid size has been used, however, the center
of grid has been assigned to the previously selected point. The grid used for kernels was
σ ∈ (cid:0)10−2, 104(cid:1) with logarithmic spacing, and the grid used for the regularization parameter
was λ ∈ (cid:0)10−1, 101(cid:1) with logarithmic spacing.

2σ2

8.2 Synthetic Data Experiment

To illustrate the proposed method, a synthetic problem was constructed. The synthetic
data generation algorithm is given in Algorithm 1. In brief, for each classiﬁcation task, the
data are uniformly supported on an ellipse, with the major axis determining the labels, and
the rotation of the major axis randomly generated in a 90 degree range for each task. One
random realization of this synthetic data is shown in Figure 2. This synthetic data set is
an ideal candidate for marginal transfer learning, because the Bayes classiﬁer for a task is
uniquely determined by the marginal distribution of the features, i.e. Lemma 9 applies (and
the optimal error inf f E ∞(f ) is zero). On the other hand, observe that the expectation of
each X distribution is the same regardless of the task and thus does not provide any relevant
information, so that taking into account at least second order information is needed to
perform domain generalization.

To analyse the eﬀects of number of examples per task (n) and number of tasks (N ), we
constructed 12 synthetic data sets by taking combinations N × n where N ∈ {16, 64, 256}
and n ∈ {8, 16, 32, 256}. For each synthetic data set, the test set contains 10 tasks and each
task contains one million data points. All kernels are taken to be Gaussian, and the random
Fourier features speedup is used. The results are shown in Figure 3 and Tables 1 and 2
(see appendix). The marginal transfer learning (MTL) method signiﬁcantly outperforms
the baseline pooling method. Furthermore, the performance of MTL improves as N and n

28

increase, as expected. The pooling method, however, does no better than random guessing
regardless of N and n.

In the remaining experiments, the marginal distribution does not perfectly characterize
the optimal decision function, but still provides some information to oﬀer improvements
over pooling.

Algorithm 1: Synthetic Data Generation

input : N : Number of tasks, n: Number of training examples per task
output: Realization of synthetic data set for N tasks
for i = 1 to N do

• sample rotation αi uniformly in

(cid:104) π
4
• Take an ellipse whose major axis is aligned with the horizontal axis, and

3π
4

(cid:105)

;

,

rotate it by an angle of αi about its center;

• Sample n points Xij, j = 1, . . . , n uniformly at random from the rotated

ellipse;

• Label the points according to their position with respect to the major axis i.e.
the points that are on the right of the major axis are considered as class 1 and
the points on the left of the major axis are considered as class −1.

end

8.3 Parkinson’s Disease Telemonitoring

We test our method in the regression setting using the Parkinson’s disease telemonitoring
data set, which is composed of a range of biomedical voice measurements using a telemon-
itoring device from 42 people with early-stage Parkinson’s. The recordings were automat-
ically captured in the patients’ homes. The aim is to predict the clinician’s Parkinson’s
disease symptom score for each recording on the uniﬁed Parkinson’s disease rating scale
(UPDRS) (Tsanas et al., 2010). Thus we are in a regression setting, and employ the (cid:15)-
insensitive loss from support vector regression. All kernels are taken to be Gaussian, and
the random Fourier features speedup is used.

There are around 200 recordings per patient. We randomly select 7 test users and then
vary the number of training users N from 10 to 35 in steps of 5, and we also vary the
number of training examples n per user from 20 to 100. We repeat this process several
times to get the average errors which are shown in Fig 4 and Tables 3 and 4 (see appendix).
The marginal transfer learning method clearly outperforms pooling, especially as N and n
increase.

8.4 Satellite Classiﬁcation

Microsatellites are increasingly deployed in space missions for a variety of scientiﬁc and
technological purposes. Because of randomness in the launch process, the orbit of a mi-
crosatellite is random, and must be determined after the launch. One recently proposed

29

(a)

(b)

(c)

(d)

Figure 2: Plots of synthetic data sets (red and blue points represent negative and positive
classes) for diﬀerent settings: (a) Random realization of a single task with 256 training
examples per task. Plots (b), (c) and(d) are random realizations of synthetic data with 256
training examples for 16, 64 and 256 tasks.

approach is to estimate the orbit of a satellite based on radiofrequency (RF) signals as mea-
sured in a ground sensor network. However, microsatellites are often launched in bunches,
and for this approach to be successful, it is necessary to associate each RF measurement
vector with a particular satellite. Furthermore, the ground antennae are not able to decode
unique identiﬁer signals transmitted by the microsatellites, because (a) of constraints on
the satellite/ground antennae links, including transmission power, atmospheric attenuation,
scattering, and thermal noise, and (b) ground antennae must have low gain and low direc-
tional speciﬁcity owing to uncertainty in satellite position and dynamics. To address this
problem, recent work has proposed to apply our marginal transfer learning methodology
(Sharma and Cutler, 2015).

As a concrete instance of this problem, suppose two microsatellites are launched to-
gether. Each launch is a random phenomenon and may be viewed as a task in our frame-
work. For each launch i, training data (Xij, Yij), j = 1, . . . , ni, are generated using a highly
realistic simulation model, where Xij is a feature vector of RF measurements across a par-
ticular sensor network and at a particular time, and Yij is a binary label identifying which
of the two microsatellites produced a given measurement. By applying our methodology,
we can classify unlabeled measurements X T
from a new launch with high accuracy. Given
j
these labels, orbits can subsequently be estimated using the observed RF measurements.

30

Figure 3: Synthetic data set: Classiﬁcation error rates for proposed method and diﬀerence
with baseline for diﬀerent experimental settings, i.e., number of examples per task and
number of tasks.

Figure 4: Parkinson’s disease telemonitoring data set: Root mean square error rates for pro-
posed method and diﬀerence with baseline for diﬀerent experimental settings, i.e., number
of examples per task and number of tasks.

31

We thank Srinagesh Sharma and James Cutler for providing us with their simulated data,
and refer the reader to their paper for more details on the application (Sharma and Cutler,
2015).

To demonstrate this idea, we analyzed the data from Sharma and Cutler (2015) for
T = 50 launches, viewing up to 40 as training data and 10 as testing. We use Gaussian
kernels and the RFF kernel approximation technique to speed up the algorithm. Results
are shown in Fig 5 (tables given in the appendix). As expected, the error for the proposed
method is much lower than for pooling, especially as N and n increase.

Figure 5: Satellite data set: Classiﬁcation error rates for proposed method and diﬀerence
with baseline for diﬀerent experimental settings, i.e., number of examples per task and
number of tasks.

8.5 Flow Cytometry Experiments

We demonstrate the proposed methodology for the ﬂow cytometry auto-gating problem,
described in Sec. 2. The pooling approach has been previously investigated in this context
by Toedling et al. (2006). We used a data set that is a part of the FlowCAP Challenges
where the ground truth labels have been supplied by human experts (Aghaeepour et al.,
2013). We used the so-called “Normal Donors” data set. The data set contains 8 diﬀerent
classes and 30 subjects. Only two classes (0 and 2) have consistent class ratios, so we have
restricted our attention to these two.

The corresponding ﬂow cytometry data sets have sample sizes ranging from 18,641 to
59,411, and the proportion of class 0 in each data set ranges from 25.59 to 38.44%. We
randomly selected 10 tasks to serve as the test tasks. These tasks were removed from the
pool of eligible training tasks. We varied the number of training tasks from 5 to 20 with an
additive step size of 5, and the number of training examples per task from 1024 to 16384

32

with a multiplicative step size of 2. We repeated this process 10 times to get the average
classiﬁcation errors which are shown in Fig. 6 and Tables 7 and 8 (see appendix). The
kernel kP was Gaussian, and the other two were linear. The Nystr¨om approximation was
used to achieve an eﬃcient implementation.

For nearly all settings the proposed method has a smaller error rate than the baseline.
Furthermore, for the marginal transfer learning method, when one ﬁxes the number of
training examples and increases the number of tasks then the classiﬁcation error rate drops.
On the other hand, we observe on Table 7 that the number n of training points per
task hardly aﬀects the ﬁnal performance when n ≥ 103. This is in contrast with the
previous experimental examples (synthetic, Parkinson’s disease telemonitoring, and satellite
classiﬁcation), for which increasing n led to better performance, but where the values of
n remained somewhat modest (n ≤ 256). This is qualitatively in line with the theoretical
results under (2SGM) in Section 6.2 (see in particular the concluding discussion there),
suggesting that the inﬂuence of increasing n on the performance should eventually taper
oﬀ, in particular if n (cid:29) N .

Figure 6: Flow Cytometry Data set: Percentage Classiﬁcation error rates for proposed
method and diﬀerence with baseline for diﬀerent experimental settings, i.e., number of
examples per task and number of tasks.

9. Discussion

Our approach to domain generalization relies on the extended input pattern (cid:101)X = (PX , X).
Thus, we study the natural algorithm of minimizing a regularized empirical loss over a
reproducing kernel Hilbert space associated with the extended input domain PX × X . We
also establish universal consistency under two sampling plans. To achieve this, we present

33

novel generalization error analyses, and construct a universal kernel on PX × X . A detailed
implementation based on novel approximate feature mappings is also presented.

On one synthetic and three real-world data sets, the marginal transfer learning approach
consistently outperforms a pooling baseline. On some data sets, however, the diﬀerence
between the two methods is small. This is because the utility of transfer learning varies
from one DG problem to another. As an extreme example, if all of the task are the same,
then pooling should do just as well as our method.

Several future directions exist. From an application perspective, the need for adaptive
classiﬁers arises in many applications, especially in biomedical applications involving biolog-
ical and/or technical variation in patient data. Examples include brain computer interfaces
and patient monitoring. For example, when electrocardiograms are used to continuously
monitor cardiac patients, it is desirable to classify each heartbeat as irregular or not. Given
the extraordinary amount of data involved, automation of this process is essential. How-
ever, irregularities in a test patient’s heartbeat will diﬀer from irregularities of historical
patients, hence the need to adapt to the test distribution (Wiens, 2010).

From a theoretical and methodological perspective, several questions are of interest. We
would like to specify conditions on the meta-distributions PS or µ under which the DG risk
is close to the expected Bayes risk of the test distribution (beyond the simple condition
discussed in Lemma 9). We would also like to develop fast learning rates under suitable dis-
tributional assumptions. Furthermore, given the close connections with supervised learning,
many common variants of supervised learning can also be investigated in the DG context,
including multiclass classiﬁcation, class probability estimation, and robustness to various
forms of noise.

We can also ask how the methodology and analysis can be extended to the context
where a small number of labels are available for the test distribution (additionally to a
larger number of unlabeled data from the same distribution); this situation appears to be
common in practice, and can be seen as intermediary between the DG and learning to learn
(LTL, see Section 4.2) settings (one could dub it “semi-supervised domain generalization”).
In this setting, two approaches appear promising to take advantage of the labeled data. The
simplest one is to use the same optimization problem (7), where we include additionally
the labeled examples of the test distribution. However, if several test samples are to be
treated in succession, and we want to avoid a full, resource-consuming re-training using
all the training samples each time, an interesting alternative is the following:
learn once
a function f0(PX , x) using the available training samples via (7); then, given a partially
labeled test sample, learn a decision function on this sample only via the usual kernel (kX )
norm regularized empirical loss minimization method, but replace the usual regularizer
term (cid:107)f (cid:107)2
X , .) ∈ HkX ). In this sense, the marginal-
adaptive decision function learned from the training samples would serve as a “prior” or
“informed guess” for learning on the test data. This can be also interpreted as learning
an adequate complexity penalty to improve learning on new samples, thus connecting to
the general principles of LTL (see Section 4.2). An interesting diﬀerence with underlying
existing LTL approaches is that those tend to adapt the hypothesis class or the“shape” of the
regularization penalty to the problem at hand, while the approach delineated above would

(cid:13)
(cid:13)f − f0( (cid:98)P T
(cid:13)

(note that f0( (cid:98)P T

H by

X , .)

(cid:13)
2
(cid:13)
(cid:13)

H

34

modify the “origin” of the penalty, using the marginal distribution information. These two
principles could also be combined.

35

Appendix A. Proofs, Technical Details, and Experimental Details

This section contains technical details for the proofs of the announced results.

A.1 Proof of Proposition 7

XY be a ﬁxed probability distribution on X × R, and ε > 0 a ﬁxed number. Since X
X (the
XY ), is inner regular, so that there exists a compact set K ⊂ X such that

Let P T
is a Radon space, by deﬁnition any Borel probability measure on it, in particular P T
X-marginal of P T
P T
X (Kc) ≤ ε.

(cid:12)f (u, v) − f (P T

For all x ∈ K, by the assumed continuity of the decision function f at point (P T

X , x) there
exists an open neighborhood Ux×Vx ⊂ PX ×X of this point such that (cid:12)
X , x)(cid:12)
(cid:12) ≤
ε for all (u, v) ∈ Ux × Vx. Since the family (Vx)x∈K is an open covering of the compact
K, there exists a ﬁnite subfamily (Vxi)i∈I covering K. Denoting U0 := (cid:84)
i∈I Uxi which is
an open neighborhood of P T
X in PX , it therefore holds for any P ∈ U0 and uniformly over
X , x)(cid:12)
x ∈ K that (cid:12)
(cid:12)f (P T
(cid:12)f (P, x) − f (P T
(cid:12) ≤ 2ε, where
i0 ∈ I is such that x ∈ Vxi0
.
Denote ST = (X T
i , Y T
(cid:98)P T
X ∈ U0

XY , and A the
X in probability,
event
so that P [Ac] ≤ ε holds for nT large enough. We have (denoting B a bound on the loss
function):

from P T
X weakly converges to P T

i )1≤i≤nT a sample of size nT drawn i.i.d.

. By the law of large numbers, (cid:98)P T

(cid:12) ≤ |f (P, x) − f (P, xi0)| + (cid:12)

X , x) − f (P, xi0)(cid:12)

(cid:111)

(cid:110)

EST

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:96)(f ( (cid:98)P T

X , X T

(cid:35)
i ), Y T
i )

≤ Bε + EST

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:96)(f ( (cid:98)P T

X , X T

i ), Y T

i )1{X T

i ∈K}

(cid:35)

(cid:35)

(cid:35)

1
nT

nT(cid:88)

i=1

1
nT

nT(cid:88)

i=1

≤ B(ε + P [Ac]) + EST

1{ (cid:98)P T

X ∈U0}

(cid:96)(f ( (cid:98)P T

X , X T

i ), Y T

i )1{X T

i ∈K}

≤ 2Bε + 2Lε + EST

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:96)(f (P T

X , X T

(cid:35)
i ), Y T
i )

≤ 2(B + L)ε + E

(X T ,Y T )∼P T

XY

(cid:2)(cid:96)(f (P T

X , X T ), Y T )(cid:3) .

Conversely,

EST

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:35)

(cid:96)(f ( (cid:98)P T

X , X T

i ), Y T
i )

≥ EST

1{ (cid:98)P T

X ∈U0}

(cid:96)(f ( (cid:98)P T

X , X T

i ), Y T

i )1{X T

i ∈K}

≥ EST

1
nT

nT(cid:88)

i=1

(cid:96)(f (P T

X , X T

(cid:35)
i ), Y T
i )

− 2Bε − 2Lε

≥ E

(X T ,Y T )∼P T

XY

(cid:2)(cid:96)(f (P T

X , X T ), Y T )(cid:3) − 2(B + L)ε.

Since the above inequalities hold for any ε > 0 provided nT is large enough, this yields that
for any ﬁxed P T

XY , we have

E

lim
nT →∞

ST ∼(P T

XY )⊗nT

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:96)(f ( (cid:98)P T

X , X T

(cid:35)
i ), Y T
i )

= E

(X T ,Y T )∼P T

XY

(cid:2)(cid:96)(f (P T

X , X T ), Y T )(cid:3) .

(cid:34)

(cid:34)

(cid:34)

36

Finally, since the above right-hand side is bounded by B, applying dominated convergence
to integrate over P T

XY ∼ µ yields the desired conclusion.

A.2 Proof of Corollary 14

Proof Denote E ∗ = inf f :PX ×X →R E(f ). Let ε > 0. Since k is a universal kernel on PX × X
and (cid:96) is Lipschitz, there exists f0 ∈ Hk such that E(f0) ≤ E ∗+ ε
2 (Steinwart and Christmann,
2008).

By comparing the objective function in (7) at the minimizer (cid:98)fλ and at the null function,
using assumption (LB) we deduce that we must have (cid:107) (cid:98)fλ(cid:107) ≤ (cid:112)B0/λ. Applying Theorem 11
for R = Rλ = (cid:112)B0/λ, and δ = 1/N 2, gives that with probability at least 1 − 1/N 2,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ≤ ε(N ) := (B0 + L(cid:96)BKBk
(cid:12) (cid:98)E(f, N ) − E(f )

(cid:112)B0/λ)

sup
f ∈Bk(R)

√
(

log N + 2)
N

√

.

Let N be large enough so that (cid:107)f0(cid:107) ≤ Rλ. We can now deduce that with probability at

least 1 − 1/N 2,

E( (cid:98)fλ) ≤ (cid:98)E( (cid:98)fλ, N ) + ε(N )

= (cid:98)E( (cid:98)fλ, N ) + λ(cid:107) (cid:98)fλ(cid:107)2 − λ(cid:107) (cid:98)fλ(cid:107)2 + ε(N )
≤ (cid:98)E(f0, N ) + λ(cid:107)f0(cid:107)2 − λ(cid:107) (cid:98)fλ(cid:107)2 + ε(N )
≤ (cid:98)E(f0, N ) + λ(cid:107)f0(cid:107)2 + ε(N )
≤ E(f0) + λ(cid:107)f0(cid:107)2 + 2ε(N )
≤ E ∗ +

+ λ(cid:107)f0(cid:107)2 + 2ε(N ).

ε
2

The last two terms become less than ε
growth of λ = λ(N ). This establishes that for any ε > 0, there exists N0 such that

2 for N suﬃciently large by the assumptions on the

(cid:88)

N ≥N0

Pr(E( (cid:98)fλ) ≥ E ∗ + ε) ≤

(cid:88)

N ≥N0

1
N 2 < ∞,

and so the result follows by the Borel-Cantelli lemma.

37

A.3 Proof of Theorem 15

We control the diﬀerence between the training loss and the conditional risk at inﬁnite sample
size via the following decomposition:

|L(S, f ) − E ∞(f |PXY )| = sup

sup
f ∈Bk(R)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:96)(f ( (cid:98)PX , Xi), Yi) − E ∞(f |PXY )
(cid:12)
(cid:12)
(cid:12)

f ∈Bk(R)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

f ∈Bk(R)

≤ sup

1
n
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
=: (I) + (II).

+ sup

f ∈Bk(R)

n
(cid:88)

(cid:16)

i=1

1
n

n
(cid:88)

i=1

(cid:96)(f ( (cid:98)PX , Xi), Yi) − (cid:96)(f (PX , Xi), Yi)

(cid:12)
(cid:12)
(cid:96)(f (PX , Xi), Yi) − E ∞(f |PXY )
(cid:12)
(cid:12)
(cid:12)

(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(31)

A.3.1 Control of term (I)

Using the assumption that the loss (cid:96) is L(cid:96)-Lipschitz in its ﬁrst coordinate, we can bound
the ﬁrst term as follows:

(I) ≤ L(cid:96)

sup
f ∈Bk(R)

1
n

n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ≤ L(cid:96)
(cid:12)f ( (cid:98)PX , Xi) − f (PX , Xi)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)f ( (cid:98)PX , ·) − f (PX , ·)
(cid:13)∞

. (32)

sup
f ∈Bk(R)

This can now be controlled using the ﬁrst part of the following result:

Lemma 23 Assume (K-Bounded) holds. Let PX be an arbitrary distribution on X and
(cid:98)PX denote an empirical distribution on X based on an iid sample of size n from PX . Then
with probability at least 1 − δ over the draw of this sample, it holds that

(cid:13)
(cid:13)
(cid:13)f ( (cid:98)PX , ·) − f (PX , ·)

(cid:13)
(cid:13)
(cid:13)∞

≤ 3RBkLKBα
k(cid:48)

(cid:18) log 2δ−1
n

(cid:19) α

2

.

sup
f ∈Bk(R)

(33)

In expectation, it holds

(cid:34)

E

sup
f ∈Bk(R)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)f ( (cid:98)PX , ·) − f (PX , ·)
(cid:13)∞

(cid:35)

≤ 2RBkLKBα

k(cid:48)n−α/2.

(34)

Proof Let X1, . . . , Xn denote the n-sample from PX . Let us denote by Φ(cid:48)
X the canonical
feature mapping x (cid:55)→ k(cid:48)
X (x)(cid:107) ≤ Bk(cid:48),
and so, as a consequence of Hoeﬀding’s inequality in a Hilbert space (see, e.g., Pinelis and
Sakhanenko, 1985), it holds with probability at least 1 − δ:

. We have for all x ∈ X , (cid:107)Φ(cid:48)

X (x, ·) from X into Hk(cid:48)

X

(cid:13)
(cid:13)
(cid:13)Ψ(PX ) − Ψ( (cid:98)PX )

(cid:13)
(cid:13)
(cid:13) =

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

Φ(cid:48)

X (Xi) − EX∼PX

(cid:2)Φ(cid:48)

X (X)(cid:3)

≤ 3Bk(cid:48)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:114)

log 2δ−1
n

.

(35)

38

Furthermore, using the reproducing property of the kernel k, we have for any x ∈ X and
f ∈ Bk(R):

|f ( (cid:98)PX , x) − f (PX , x)| =

(cid:68)

(cid:12)
(cid:12)
(cid:12)

k(( (cid:98)PX , x), ·) − k((PX , x), ·), f

(cid:69)(cid:12)
(cid:12)
(cid:12)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)k(( (cid:98)PX , x), ·) − k((PX , x), ·)
(cid:13)
K(Ψ(PX ), Ψ(PX ))

(cid:16)

1
2

≤ RkX (x, x)

≤ (cid:107)f (cid:107)

+ K(Ψ( (cid:98)PX ), Ψ( (cid:98)PX )) − 2K(Ψ(PX ), Ψ( (cid:98)PX ))
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)ΦK(Ψ(PX )) − ΦK(Ψ( (cid:98)PX ))
(cid:13)

≤ RBk

(cid:17) 1
2

≤ RBkLK

(cid:13)
(cid:13)
α
(cid:13)
(cid:13)
(cid:13)Ψ(PX ) − Ψ( (cid:98)PX )
(cid:13)

,

where in the last step we have used property (K-H¨older) together with the fact that for
all P ∈ PX , (cid:107)Ψ(P )(cid:107) ≤ (cid:82)
(Bk(cid:48)). Combining
with (35) gives (33).

X (x, ·)(cid:107) dPX (x) ≤ Bk(cid:48), so that Ψ(P ) ∈ Bk(cid:48)

X (cid:107)k(cid:48)

X

For the bound in expectation, we use the inequality above, and can bound further (using

Jensen’s inequality, since α ≤ 1)

E

(cid:13)
(cid:104)(cid:13)
(cid:13)
(cid:13)
(cid:13)Ψ(PX ) − Ψ( (cid:98)PX )
(cid:13)

α(cid:105)

≤ E

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Ψ(PX ) − Ψ( (cid:98)PX )
(cid:13)

2(cid:21)α/2

E (cid:2)(cid:10)Φ(cid:48)

X (Xi) − E (cid:2)Φ(cid:48)

X (X)(cid:3) , Φ(cid:48)

X (Xj) − E (cid:2)Φ(cid:48)

X (X)(cid:3)(cid:11)(cid:3)





α/2

E

(cid:104)(cid:13)
(cid:13)Φ(cid:48)

X (Xi) − E (cid:2)Φ(cid:48)

X (X)(cid:3)(cid:13)
(cid:13)

(cid:33)α/2

2(cid:105)



=



(cid:32)

=

≤

1
n2

1
n2

n
(cid:88)

i,j=1

n
(cid:88)

i=1

(cid:19) α

2

(cid:18) 4B2
k(cid:48)
n

,

which yields (34) in combination with the above.

A.3.2 Control of term (II)

Term (II) takes the form of a uniform deviation over a RKHS ball of an empirical loss
for the data ( (cid:101)Xi, Yi), where (cid:101)Xi := (PX , Xi). Since PX is ﬁxed (in contrast with term (I)
where (cid:98)PX depended on the whole sample), these data are i.i.d. Similar to the proofs of
Theorems 5 and 11, we can therefore apply again standard Rademacher analysis, this time
at the level of one speciﬁc task (Azuma-McDiarmid inequality followed by Rademacher
complexity analysis for a Lipschitz, bounded loss over a RKHS ball; see Koltchinskii, 2001;
Bartlett and Mendelson, 2002, Theorems 8, 12 and Lemma 22 there). The kernel k is
bounded by B2
K by assumption (K-Bounded); by Lemma 10 and assumption (LB), the

kB2

39

loss is bounded by B0 + L(cid:96)RBkBK, and is L(cid:96)-Lipschitz. Therefore, with probability at least
1 − δ we get

(II) ≤ (B0 + 3L(cid:96)RBkBK) min

(cid:32)(cid:114)

(cid:33)

log(δ−1)
2n

, 1

(cid:32)(cid:18) log(δ−1)

(cid:19) α

2

2n

(cid:33)

, 1

.

≤ (B0 + 3L(cid:96)RBkBK) min

(36)

Observe that we can cap the second factor at 1 since (II) is upper bounded by the bound
on the loss in all cases; the second inequality then uses α ≤ 1. Combining with a union
bound the probabilistic controls (32), (33) of term (I) and (36) of (II) yields (18).

To establish the bound (19) we use a similar argument. We use the decomposition

|E(f |PXY , n) − E ∞(f |PXY )|

sup
f ∈Bk(R)

≤ sup

f ∈Bk(R)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ sup

f ∈Bk(R)
=: (I (cid:48)) + (II (cid:48)).

ESn∼(PXY )⊗n

(cid:12)
(cid:12)
ES∼(PXY )⊗n
(cid:12)
(cid:12)
(cid:12)

(cid:34)

(cid:34)

1
n

1
n

n
(cid:88)

(cid:16)

i=1
n
(cid:88)

i=1

(cid:96)(f ( (cid:98)PX , X T

i ), Yi) − (cid:96)(f (PX , Xi), Yi)

(cid:0)(cid:96)(f (PX , X T

i ), Yi)(cid:1)

(cid:35)

(cid:12)
(cid:12)
− E(X,Y )∼PXY [(cid:96)(f (PX , X), Y )]
(cid:12)
(cid:12)
(cid:12)

(cid:17)

(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

for any ﬁxed f ∈ Bk(R) and PXY , the
It is easily seen that the second term vanishes:
diﬀerence of the expectations is zero. For the ﬁrst term, using Lipschitzness of the loss,
then (34), we obtain

(I (cid:48)) ≤ L(cid:96)E

(cid:34)

sup
f ∈Bk(R)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)f ( (cid:98)PX , .) − f (PX , .)
(cid:13)∞

(cid:35)

≤ 2L(cid:96)RBkLKBα

k(cid:48)n−α/2,

yielding (19). The bound (20) is obtained as a direct consequence by taking expectation
over PXY ∼ µ and using Jensen’s inequality to pull out the absolute value.

A.4 Regularity conditions for the kernel on distributions

We investigate suﬃcient conditions on the kernel K to ensure the regularity condition (K-
H¨older) (15). Roughly speaking, the regularity of the feature mapping of a reproducing
kernel is “one half” of the regularity of the kernel in each of its variables. The next result
considers the situation where K is itself simply a H¨older continuous function of its variables.

Lemma 24 Let α ∈ (0, 1
constant L2

2 ]. Assume that the kernel K is H¨older continuous of order 2α and

K/2 in each of its two variables on Bk(cid:48)

X

(Bk(cid:48)). Then (K-H¨older) is satisﬁed.

Proof For any v, w ∈ Bk(cid:48)

(Bk(cid:48)):

X

(cid:107)ΦK(v) − ΦK(w)(cid:107) = (K(v, v) + K(w, w) − 2K(v, w))

1

2 ≤ LK (cid:107)v − w(cid:107)α .

40

The above type of regularity only leads to a H¨older feature mapping of order at most 1
2
(when the kernel function is Lipschitz continuous in each variable). Since this order plays
an important role in the rate of convergence of the upper bound in the main error control
theorem, it is desirable to study conditions ensuring more regularity, in particular a feature
mapping which has at least Lipschitz continuity. For this, we consider the following stronger
condition, namely that the kernel function is twice diﬀerentiable in a speciﬁc sense:

Lemma 25 Assume that, for any u, v ∈ Bk(cid:48)
, the
function hu,v,e : (λ, µ) ∈ R2 (cid:55)→ K(u + λe, v + µe) admits a mixed partial derivative ∂1∂2hu,v,e
at the point (λ, µ) = (0, 0) which is bounded in absolute value by a constant C2
K independent
of (u, v, e). Then (15) is satisﬁed with α = 1 and LK = CK, that is, the canonical feature
mapping of K is Lipschitz continuous on Bk(cid:48)

(Bk(cid:48)) and unit norm vector e of Hk(cid:48)

(Bk(cid:48)).

X

X

X

Proof The argument is along the same lines as Steinwart and Christmann (2008), Lemma
4.34. Observe that, since hu,v,e(λ + λ(cid:48), µ + µ(cid:48)) = hu+λe,v+µe,e(λ(cid:48), µ(cid:48)), the function hu,v,e
actually admits a uniformly bounded mixed partial derivative in any point (λ, µ) ∈ R2 such
(Bk(cid:48)) . Let us denote ∆1hu,v,e(λ, µ) := hu,v,e(λ, µ) − hu,v,e(0, µ) .
that (u + λe, v + µe) ∈ Bk(cid:48)
(Bk(cid:48)) , u (cid:54)= v , let us set λ := (cid:107)v − u(cid:107) and the unit vector e := λ−1(v − u);
For any u, v ∈ Bk(cid:48)
we have

X

X

(cid:107)ΦK(u) − ΦK(v)(cid:107)2 = K(u, u) + K(u + λe, u + λe) − K(u, u + λe) − K(u + λe, u)

= ∆1hu,v,e(λ, λ) − ∆1hu,v,e(λ, 0)
= λ∂2∆1hu,v,e(λ, λ(cid:48)) ,

where we have used the mean value theorem, yielding existence of λ(cid:48) ∈ [0, λ] such that the
last equality holds. Furthermore,

∂2∆1hu,v,e(λ, λ(cid:48)) = ∂2hu,v,e(λ, λ(cid:48)) − ∂2hu,v,e(0, λ(cid:48))

= λ∂1∂2hu,v,e(λ(cid:48)(cid:48), λ(cid:48)) ,

using again the mean value theorem, yielding existence of λ(cid:48)(cid:48) ∈ [0, λ] in the last equality.
Finally, we get

(cid:107)ΦK(u) − ΦK(v)(cid:107)2 = λ2∂1∂2hu,v,e(λ(cid:48), λ(cid:48)(cid:48)) ≤ C2

K (cid:107)v − u(cid:107)2 .

Lemma 26 Assume that the kernel K takes the form of either (a) K(u, v) = g((cid:107)u − v(cid:107)2)
or (b) K(u, v) = g((cid:104)u, v(cid:105)) , where g is a twice diﬀerentiable real function of a real variable
k(cid:48)] in case (b). Assume (cid:107)g(cid:48)(cid:107)∞ ≤ C1 and
deﬁned on [0, 4B2
(cid:107)g(cid:48)(cid:48)(cid:107)∞ ≤ C2. Then K satisﬁes the assumption of Lemma 25 with CK := 2C1 + 16C2B2
k(cid:48) in
case (a), and CK := C1 + C2B2

k(cid:48)] in case (a), and on [−B2

k(cid:48), B2

k(cid:48) for case (b).

41

Proof In case (a), we have hu,v,e(λ, µ) = g((cid:107)u − v + (λ − µ)e(cid:107)2). It follows

|∂1∂2hu,v,e(0, 0)| =

(cid:12)
(cid:12)

(cid:12)−2g(cid:48)((cid:107)u − v(cid:107)2) (cid:107)e(cid:107)2 − 4g(cid:48)(cid:48)((cid:107)u − v(cid:107)2) (cid:104)u − v, e(cid:105)2(cid:12)
≤ 2C1 + 16C2B2

(cid:12)
(cid:12)

k(cid:48) .

In case (b), we have hu,v,e(λ, µ) = g((cid:104)u + λe, v + µe(cid:105)). It follows

|∂1∂2hu,v,e(0, 0)| =

(cid:12)
(cid:12)g(cid:48)((cid:104)u, v(cid:105)) (cid:107)e(cid:107)2 + g(cid:48)(cid:48)((cid:104)u, v(cid:105)) (cid:104)u, e(cid:105) (cid:104)v, e(cid:105)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

≤ C1 + C2B2

k(cid:48) .

A.5 Proof of Lemma 12

Proof Let H, H(cid:48) the RKHS associated to k, k(cid:48) with the associated feature mappings Φ, Φ(cid:48).
Then it can be checked that (x, x(cid:48)) ∈ X × X (cid:48)
(cid:55)→ Φ(x) ⊗ Φ(cid:48)(x(cid:48)) is a feature mapping for
k into the Hilbert space H ⊗ H(cid:48). Using (Steinwart and Christmann, 2008), Th. 4.21,
we deduce that the RKHS H of k contains precisely all functions of the form (x, x(cid:48)) ∈
X × X (cid:48)
(cid:55)→ Fw(x, x(cid:48)) = (cid:104)w, Φ(x) ⊗ Φ(x(cid:48))(cid:105), where w ranges over H ⊗ H(cid:48). Taking w of the
form w = g ⊗ g(cid:48), g ∈ H, g ∈ H(cid:48), we deduce that H contains in particular all functions of
the form f (x, x(cid:48)) = g(x)g(x(cid:48)), and further

(cid:101)H := span (cid:8)(x, x(cid:48)) ∈ X × X (cid:48) (cid:55)→ g(x)g(x(cid:48)); g ∈ H, g(cid:48) ∈ H(cid:48)(cid:9) ⊂ H.

Denote C(X ), C(X (cid:48)), C(X × X (cid:48)) the set of real-valued continuous functions on the respective
spaces. Let

C(X ) ⊗ C(X (cid:48)) := span (cid:8)(x, x(cid:48)) ∈ X × X (cid:48) (cid:55)→ f (x)f (cid:48)(x(cid:48)); f ∈ C(X ), f (cid:48) ∈ C(X (cid:48))(cid:9) .

Let G(x, x(cid:48)) be an arbitrary element of C(X ) ⊗ C(X (cid:48)), G(x, x(cid:48)) = (cid:80)k
gi ∈ C(X ), g(cid:48)
exist fi ∈ H, f (cid:48)
F (x, x(cid:48)) := (cid:80)k

i(x(cid:48)) with
i ∈ C(X (cid:48)) for i = 1, . . . , k. For ε > 0, by universality of k and k(cid:48), there
i(cid:107)∞ ≤ ε for i = 1, . . . , k. Let

i ∈ H(cid:48) so that (cid:107)fi − gi(cid:107)∞ ≤ ε, (cid:107)f (cid:48)
i(x(cid:48)) ∈ (cid:101)H. We have

i=1 λigi(x)g(cid:48)

i=1 λifi(x)f (cid:48)

i − g(cid:48)

(cid:13)F (x, x(cid:48)) − G(x, x(cid:48))(cid:13)
(cid:13)

(cid:13)∞ ≤

λi(gi(x)g(cid:48)

i(x) − fi(x)f (cid:48)

(cid:13)
(cid:13)
(cid:13)
i(x))
(cid:13)
(cid:13)∞

=

λi

(fi(x) − gi(x))(g(cid:48)

i(x(cid:48)) − f (cid:48)

i(x(cid:48)))

(cid:104)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k
(cid:88)

i=1

k
(cid:88)

i=1

+ gi(x)(g(cid:48)

i(x) − f (cid:48)

i(x(cid:48))) + (gi(x) − fi(x))g(cid:48)

(cid:105)
i(x(cid:48))

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤ ε

k
(cid:88)

i=1

|λi| (ε + (cid:107)gi(cid:107)∞ + (cid:13)
(cid:13)g(cid:48)
i

(cid:13)
(cid:13)∞) .

42

This establishes that (cid:101)H is dense in C(X ) ⊗ C(X (cid:48)) for the supremum norm. It can be easily
checked that C(X ) ⊗ C(X (cid:48)) is an algebra of functions which does not vanish and separates
points on X × X (cid:48). By the Stone-Weierstrass theorem, it is therefore dense in C(X × X (cid:48)) for
the supremum norm. We deduce that (cid:101)H (and thus also H) is dense in C(X × X (cid:48)), so that
k is universal.

A.6 Proof of Theorem 20

Proof Observe:

and denote:

(cid:26) −1
2σ2
P

(cid:26) −1
2σ2
P

¯k(˜x, ˜x(cid:48)) = exp

(cid:107)Ψ( (cid:98)PX ) − Ψ( (cid:98)P (cid:48)

X )(cid:107)2

exp

(cid:27)

(cid:27)

(cid:107)x − x(cid:48)(cid:107)2

,

(cid:26) −1
2σ2
X

˜k(˜x, ˜x(cid:48)) = exp

(cid:107)ZP ( (cid:98)PX ) − ZP ( (cid:98)P (cid:48)

X )(cid:107)2

exp

(cid:27)

(cid:27)

(cid:107)x − x(cid:48)(cid:107)2

,

(cid:26) −1
2σ2
X

We omit the arguments of ¯k, ˜k for brevity. Let kq be the ﬁnal approximation (kq =
¯z(˜x)T ¯z(˜x(cid:48))) and then we have

|¯k − kq| = |¯k − ˜k + ˜k − kq| ≤ |¯k − ˜k| + |˜k − kq|.

From Eqn. (37) it follows that,

P (|¯k − kq| ≥ (cid:15)l + (cid:15)q) ≤ P (|¯k − ˜k| ≥ (cid:15)l) + P (|˜k − kq| ≥ (cid:15)q).

By a direct application of Hoeﬀding’s inequality,

P (|˜k − kq| ≥ (cid:15)q) ≤ 2 exp(−

Q(cid:15)2
q
2

).

(37)

(38)

(39)

Recall that (cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)
again by Hoeﬀding

X )(cid:105) = 1
n1n2

(cid:80)n1
i=1

(cid:80)n2

j=1 k(cid:48)

X (Xi, X (cid:48)

j). For a pair Xi, X (cid:48)

j, we have

P (|z(cid:48)

X (Xi)T z(cid:48)

X (X (cid:48)

j) − k(cid:48)

X (Xi, X (cid:48)

j)| ≥ (cid:15)) ≤ 2 exp(−

Let Ωij be the event |z(cid:48)
bound we have

X (Xi)T z(cid:48)

X (X (cid:48)

j) − k(cid:48)

X (Xi, X (cid:48)

j)| ≥ (cid:15), for particular i, j. Using the union

P (Ω11 ∪ Ω12 ∪ . . . ∪ Ωn1n2) ≤ 2n1n2 exp(−

This implies

P (|ZP ( (cid:98)PX )T ZP ( (cid:98)P (cid:48)

X ) − (cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

X )(cid:105)| ≥ (cid:15)) ≤ 2n1n2 exp(−

(40)

L(cid:15)2
2

).

43

L(cid:15)2
2

).

L(cid:15)2
2

)

Therefore,

(cid:12)
¯k − ˜k
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) =

(cid:34)

≤

exp

(cid:26) −1
2σ2
X

exp

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:26) −1
2σ2
P
(cid:26) −1
2σ2
P

(cid:27) (cid:34)

(cid:107)x − x(cid:48)(cid:107)2

exp

(cid:107)Ψ( (cid:98)PX ) − Ψ( (cid:98)P (cid:48)

X )(cid:107)2

(cid:27)

− exp

(cid:107)ZP ( (cid:98)PX ) − ZP ( (cid:98)P (cid:48)

X )(cid:107)2

(cid:107)Ψ( (cid:98)PX ) − Ψ( (cid:98)P (cid:48)

X )(cid:107)2

− exp

(cid:107)ZP ( (cid:98)PX ) − ZP ( (cid:98)P (cid:48)

X )(cid:107)2

(cid:27) (cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:26) −1
2σ2
P
(cid:27) (cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:27)

(cid:27) (cid:34)

(cid:26) −1
2σ2
P

(cid:16)

(cid:110) −1
2σ2
P

=

exp

(cid:107)Ψ( (cid:98)PX ) − Ψ( (cid:98)P (cid:48)

X )(cid:107)2

1 − exp

(cid:107)ZP ( (cid:98)PX ) − ZP ( (cid:98)P (cid:48)

X )(cid:107)2

(cid:26) −1
2σ2
P

− (cid:107)Ψ( (cid:98)PX ) − Ψ( (cid:98)P (cid:48)

(cid:35)(cid:12)
(cid:12)
X )(cid:107)2(cid:17)(cid:111)
(cid:12)
(cid:12)
(cid:12)

(cid:34)

≤

1 − exp

(cid:16)

(cid:26) −1
2σ2
P

(cid:40)

(cid:16)

−1
2σ2
P

(cid:40)

(cid:16)

1
2σ2
P

(cid:107)ZP ( (cid:98)PX ) − ZP ( (cid:98)P (cid:48)

X )(cid:107)2 − (cid:107)Ψ( (cid:98)PX ) − Ψ( (cid:98)P (cid:48)

=

1 − exp

ZP ( (cid:98)PX )T ZP ( (cid:98)PX ) − (cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)PX )(cid:105) + ZP ( (cid:98)P (cid:48)

X )T ZP ( (cid:98)P (cid:48)

X )

− (cid:104)Ψ( (cid:98)P (cid:48)

X ), Ψ( (cid:98)P (cid:48)

X )(cid:105) − 2(cid:0)ZP ( (cid:98)PX )T ZP ( (cid:98)P (cid:48)

X ) − (cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

≤

1 − exp

|ZP ( (cid:98)PX )T ZP ( (cid:98)PX ) − (cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)PX )(cid:105)| + |ZP ( (cid:98)P (cid:48)

X )T ZP ( (cid:98)P (cid:48)

X )

− (cid:104)Ψ( (cid:98)P (cid:48)

X ), Ψ( (cid:98)P (cid:48)

X )(cid:105)| + 2|(cid:0)ZP ( (cid:98)PX )T ZP ( (cid:98)P (cid:48)

X ) − (cid:104)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

X )(cid:107)2(cid:17)(cid:27) (cid:35)(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

X )(cid:105)(cid:1)(cid:17)

(cid:41)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:17)
X )(cid:105)(cid:1)|

(cid:41)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

The result now follows by applying the bound of Eqn. (40) to each of the three terms in
the exponent of the preceding expression, together with the stated formula for (cid:15) in terms
of (cid:15)(cid:96).

A.7 Proof of Theorem 22

Proof The proof is very similar to the proof of Theorem 20. We use Lemma 21 to replace
bound (39) with:

(cid:18)

P

sup
x,x(cid:48)∈M

(cid:19)

|˜k − kq| ≥ (cid:15)q

≤ 28

(cid:19)2

(cid:18) σ(cid:48)
X r
(cid:15)q

exp

(cid:18) −Q(cid:15)2
q
2(d + 2)

(cid:19)
.

(41)

44

Similarly, Eqn. (40) is replaced by

(cid:18)

P

sup
x,x(cid:48)∈M

(cid:12)
(cid:12)ZP ( (cid:98)PX )T ZP ( (cid:98)P (cid:48)

X ) − (cid:10)Ψ( (cid:98)PX ), Ψ( (cid:98)P (cid:48)

X )(cid:11)(cid:12)

(cid:12) ≥ (cid:15)

(cid:19)

≤ 29n1n2

(cid:19)2

(cid:18) σP σX r
(cid:15)l

exp

(cid:18) −L(cid:15)2
l

2(d + 2)

(cid:19)
.

(42)

The remainder of the proof now proceeds as in the previous proof.

A.8 Results in Tabular Format

Table 1: Average Classiﬁcation Error of Marginal Transfer Learning on Synthetic Data set

Table 2: Average Classiﬁcation Error of Pooling on Synthetic Data set

k
s
a
T
r
e
p

s
e
l
p
m
a
x
E

k
s
a
T
r
e
p

s
e
l

p
m
a
x
E

Tasks

16

64

256

8

16

32

36.01

33.08

31.69

31.55

31.03

30.96

30.44

29.31

23.87

256

23.78

7.22

1.27

Tasks

16

64

256

8

16

32

49.14

49.11

50.04

49.89

50.04

49.68

50.32

50.21

49.61

256

50.01

50.43

49.93

45

Table 3: RMSE of Marginal Transfer Learning on Parkinson’s Disease Data set

Tasks
20

10

15

25

30

35

13.78

12.37

11.93

10.74

10.08

11.17

14.18

11.89

11.51

10.90

10.55

10.18

14.95

13.29

12.00

10.21

10.59

9.52

13.27

11.66

11.79

12.89

11.27

11.17

13.15

11.70

13.81

10.12

9.16

9.91

9.28

9.03

9.03

8.01

9.34

9.10

9.01

8.44

8.16

7.30

7.14

10.50

10.05

8.69

7.62

7.88

7.01

7.5

12.16

13.03

11.98

9.59

9.16

9.18

8.48

9.85

8.80

9.74

9.52

100

12.69

20

24

28

34

41

49

58

70

84

20

24

28

34

41

49

58

70

84

k
s
a
T
r
e
p

s
e
l
p
m
a
x
E

k
s
a
T
r
e
p

s
e
l
p
m
a
x
E

Table 4: RMSE of Pooling on Parkinson’s Disease Data set

Tasks
20

10

15

25

30

35

13.64

11.93

11.95

11.06

11.91

12.08

13.80

11.83

11.70

11.98

11.68

11.48

13.78

11.70

11.72

11.18

11.58

11.73

13.71

12.20

12.04

11.17

11.67

11.92

13.69

11.73

12.08

11.28

11.55

12.59

13.75

11.85

11.79

11.17

11.34

11.82

13.70

11.89

12.06

11.06

11.82

11.65

13.54

11.86

12.14

11.21

11.40

11.96

13.55

11.98

12.03

11.25

11.54

12.22

100

13.53

11.85

11.92

11.12

11.96

11.84

46

Table 5: Average Classiﬁcation Error of Marginal Transfer Learning on Satellite Data set

Tasks
10

20

30

40

8.62

7.61

8.25

7.17

6.21

5.90

5.85

5.43

6.61

5.33

5.37

5.35

5.61

5.19

4.71

4.70

all training data

5.36

4.91

3.86

4.08

k
s
a
T
r
e
p

s
e
l
p
m
a
x
E

k
s
a
T
r
e
p

s
e
l
p
m
a
x
E

5

15

30

45

5

15

30

45

Table 6: Average Classiﬁcation Error of Pooling on Satellite Data set

Tasks
10

20

30

40

8.13

7.54

7.94

6.96

6.55

5.81

5.79

5.57

6.06

5.36

5.56

5.31

5.58

5.12

5.30

4.99

all training data

5.37

4.98

5.32

5.14

Table 7: Average Classiﬁcation Error of Marginal Transfer Learning on Flow Cytometry
Data set

Tasks
10

15

20

9.03

9.03

8.70

5

9

9.12

9.56

9.07

8.62

8.96

8.91

9.01

8.66

9.18

9.20

9.04

8.74

k
s
a
T
r
e
p

s
e
l
p
m
a
x
E

1024

2048

4096

8192

16384

9.05

9.08

9.04

8.63

47

Table 8: Average Classiﬁcation Error of Pooling on Flow Cytometry Data set

Tasks
10

5

15

20

1024

2048

4096

8192

9.41

9.48

9.32

9.52

9.92

9.57

9.45

9.54

9.72

9.56

9.36

9.40

9.43

9.53

9.38

9.50

16384

9.42

9.56

9.40

9.33

k
s
a
T
r
e
p

s
e
l
p
m
a
x
E

C. Scott and A. Deshmukh where supported in part by NSF Grants No. 1422157, 1217880,
and 1047871. G. Blanchard acknowledges support by the DFG via Research Unit 1735
Structural Inference in Statistics.

Acknowledgments

References

N. Aghaeepour, G. Finak, H. Hoos, T. R. Mosmann, R. Brinkman, R. Gottardo, R. H.
Scheuermann, FlowCAP Consortium, DREAM Consortium, et al. Critical assessment
of automated ﬂow cytometry data analysis techniques. Nature methods, 10(3):228–238,
2013.

Kei Akuzawa, Yusuke Iwasawa, and Yutaka Matsuo. Adversarial invariant feature learning

with accuracy constraint for domain generalization. ArXiv, abs/1904.12543, 2019.

Kamyar Azizzadenesheli, Anqi Liu, Fanny Yang, and Animashree Anandkumar. Regularized
learning for domain adaptation under label shifts. In International Conference on Learn-
ing Representations, 2019. URL https://openreview.net/forum?id=rJl0r3R9KX.

G¨okhan Bakır, Thomas Hofmann, Bernhard Sch¨olkopf, Alexander J Smola, and Ben Taskar.

Predicting structured data. MIT press, 2007.

Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa.

Metareg: To-
In S. Bengio, H. Wal-
wards domain generalization using meta-regularization.
ed-
and R. Garnett,
lach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
998–
itors, Advances
1008. Curran Associates,
URL http://papers.nips.cc/paper/
7378-metareg-towards-domain-generalization-using-meta-regularization.pdf.

Information Processing Systems

in Neural

pages

2018.

Inc.,

31,

P. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and

structural results. Journal of Machine Learning Research, 3:463–482, 2002.

P. Bartlett, M. Jordan, and J. McAuliﬀe. Convexity, classiﬁcation, and risk bounds. J.

Amer. Stat. Assoc., 101(473):138–156, 2006.

48

J. Baxter. A model of inductive bias learning. Journal of Artiﬁcial Intelligence Research,

12:149–198, 2000.

Shai Ben-David and Ruth Urner. On the hardness of domain adaptation and the utility
of unlabeled target samples. In Nader H. Bshouty, Gilles Stoltz, Nicolas Vayatis, and
Thomas Zeugmann, editors, Algorithmic Learning Theory, pages 139–153, 2012.

Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of repre-
sentations for domain adaptation. In B. Sch¨olkopf, J. C. Platt, and T. Hoﬀman, editors,
Advances in Neural Information Processing Systems 19, pages 137–144. 2007.

Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jen-
nifer Wortman Vaughan. A theory of learning from diﬀerent domains. Machine Learning,
79:151–175, 2010.

S. Bickel, M. Br¨uckner, and T. Scheﬀer. Discriminative learning under covariate shift. J.

Machine Learning Research, pages 2137–2155, 2009.

G. Blanchard, G. Lee, and C. Scott. Generalizing from several related classiﬁcation tasks
to a new unlabeled sample. In J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira,
and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 24,
pages 2178–2186. 2011.

G. Blanchard, M. Flaska, G. Handy, S. Pozzi, and C. Scott. Classiﬁcation with asymmetric
label noise: Consistency and maximal denoising. Electronic Journal of Statistics, 10:
2780–2824, 2016.

John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman.
Learning bounds for domain adaptation. In J. C. Platt, D. Koller, Y. Singer, and S. T.
Roweis, editors, Advances in Neural Information Processing Systems 20, pages 129–136.
2008.

Timothy I. Cannings, Yingying Fan, and Richard J. Samworth. Classiﬁcation with imperfect

training labels. Technical Report arXiv:1805.11505, 2018.

J. Carbonell, S. Hanneke, and L. Yang. A theory of transfer learning with applications to

active learning. Machine Learning, 90(2):161–189, 2013.

Fabio Maria Carlucci, Antonio D’Innocente, Silvia Bucci, Barbara Caputo, and Tatiana
Tommasi. Domain generalization by solving jigsaw puzzles. 2019 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pages 2224–2233, 2019.

R. Caruana. Multitask learning. Machine Learning, 28:41–75, 1997.

C. Chang and C. Lin. Libsvm: A library for support vector machines. ACM Transactions

on Intelligent Systems and Technology (TIST), 2(3):27, 2011.

A. Christmann and I. Steinwart. Universal kernels on non-standard input spaces. In J. Laf-
ferty, C. K. I. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, editors, Advances in
Neural Information Processing Systems 23, pages 406–414, 2010.

49

Corinna Cortes, Mehryar Mohri, Michael Riley, and Afshin Rostamizadeh. Sample selection

bias correction theory. In Algorithmic Learning Theory, pages 38–53, 2008.

Corinna Cortes, Mehryar Mohri, and Andr´es Mu˜noz Medina. Adaptation algorithm and
theory based on generalized discrepancy.
In Proceedings of the 21th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, KDD ’15, pages
169–178, 2015.

Daryl J. Daley and David Vere-Jones. An introduction to the theory of point processes,

volume II: general theory and structure. Springer, 2008.

G Denevi, Carlo Ciliberto, D Stamos, and Massimiliano Pontil. Incremental learning-to-
learn with statistical guarantees. In Proc. Uncertainty in Artiﬁcial Intelligence, 2018a.

Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil. Learning to
learn around a common mean. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing
Systems 31, pages 10169–10179. 2018b.

Zhengming Ding and Yun Fu. Deep domain generalization with structured low-rank con-

straint. IEEE Transactions on Image Processing, 27:304–313, 2018.

Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain gen-
eralization via model-agnostic learning of semantic features. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d’ Alch´e-Buc, E. Fox, and R. Garnett, editors, Advances in Neural
Information Processing Systems 32, pages 6450–6461. 2019.

P. Drineas and M. W. Mahoney. On the Nystr¨om method for approximating a gram matrix
for improved kernel-based learning. The Journal of Machine Learning Research, 6:2153–
2175, 2005.

M. C. Du Plessis and M. Sugiyama. Semi-supervised learning of class balance under class-
prior change by distribution matching. In J. Langford and J. Pineau, editors, Proc. 29th
Int. Conf. on Machine Learning, pages 823–830, 2012.

T. Evgeniou, C. A. Michelli, and M. Pontil. Learning multiple tasks with kernel methods.

J. Machine Learning Research, pages 615–637, 2005.

R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. Liblinear: A library for large linear

classiﬁcation. The Journal of Machine Learning Research, 9:1871–1874, 2008.

Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast
adaptation of deep networks. In Doina Precup and Yee Whye Teh, editors, Proceedings
of the 34th International Conference on Machine Learning, volume 70 of Proceedings of
Machine Learning Research, pages 1126–1135, 2017.

Chuang Gan, Tianbao Yang, and Boqing Gong. Learning attributes equals multi-source
domain generalization. In The IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), June 2016.

50

Pascal Germain, Amaury Habrard, Fran¸cois Laviolette, and Emilie Morvant. A new pac-
bayesian perspective on domain adaptation. In ICML, volume 48 of JMLR Workshop
and Conference Proceedings, pages 859–868, 2016.

M. Ghifary, D. Balduzzi, B. Kleijn, and M. Zhang. Scatter component analysis: A uniﬁed
framework for domain adaptation and domain generalization. IEEE Trans. Patt. Anal.
Mach. Intell., 39(7):1411–1430, 2017.

Muhammad Ghifary, W. Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain
generalization for object recognition with multi-task autoencoders. In Proceedings of the
2015 IEEE International Conference on Computer Vision (ICCV), page 25512559, 2015.

Mingming Gong, Kun Zhang, Tongliang Liu, Dacheng Tao, Clark Glymour, and Bernhard
Sch¨olkopf. Domain adaptation with conditional transferable components. In International
conference on machine learning, pages 2839–2848, 2016.

A. Gretton, K. Borgwardt, M. Rasch, B. Sch¨olkopf, and A. Smola. A kernel approach to
comparing distributions. In R. Holte and A. Howe, editors, Proceedings of the 22nd AAAI
Conference on Artiﬁcial Intelligence, pages 1637–1641, 2007a.

A. Gretton, K. Borgwardt, M. Rasch, B. Sch¨olkopf, and A. Smola. A kernel method for
the two-sample-problem. In B. Sch¨olkopf, J. Platt, and T. Hoﬀman, editors, Advances in
Neural Information Processing Systems 19, pages 513–520, 2007b.

T. Grubinger, A. Birlutiu, H. Sch¨oner, T. Natschl¨ager, and T. Heskes. Domain generaliza-
tion based on transfer component analysis. In I. Rojas, G. Joya, and A. Catala, editors,
Advances in Computational Intelligence. IWANN 2015, volume 9094 of Lecture Notes in
Computer Science, pages 325–334. Springer, 2015.

P. Hall. On the non-parametric estimation of mixture proportions. Journal of the Royal

Statistical Society, 43(2):147–156, 1981.

C. Hsieh, K. Chang, C. Lin, S. S. Keerthi, and S. Sundararajan. A dual coordinate descent
method for large-scale linear svm. In Proceedings of the 25th international conference on
Machine learning, pages 408–415. ACM, 2008.

Shoubo Hu, Kun Zhang, Zhitang Chen, and Laiwan Chan. Domain generalization via mul-
tidomain discriminant analysis. In Amir Globerson and Ricardo Silva, editors, Proceedings
of the Thirty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence, UAI 2019, Tel
Aviv, Israel, July 22-25, 2019, 2019.

Jiayuan Huang, Alexander J. Smola, Arthur Gretton, Karsten M. Borgwardt, and Bernhard
Scholkopf. Correcting sample selection bias by unlabeled data.
In Proceedings of the
19th International Conference on Neural Information Processing Systems, pages 601–608,
2007.

Wittawat Jitkrittum, Arthur Gretton, Nicolas Heess, SM Eslami, Balaji Lakshminarayanan,
Dino Sejdinovic, and Zolt´an Szab´o. Kernel-based just-in-time learning for passing expec-
tation propagation messages. In Proceedings of the Thirty-First Conference on Uncer-
tainty in Artiﬁcial Intelligence, pages 405–414. AUAI Press, 2015.

51

T. Joachims. Making large scale svm learning practical. Technical report, Universit¨at

Dortmund, 1999.

O. Kallenberg. Foundations of Modern Probability. Springer, 2002.

Takafumi Kanamori, Shohei Hido, and Masashi Sugiyama. A least-squares approach to

direct importance estimation. J. Mach. Learn. Res., 10:1391–1445, 2009.

Aditya Khosla, Tinghui Zhou, Tomasz Malisiewicz, Alexei A. Efros, and Antonio Torralba.
Undoing the damage of dataset bias. In Proceedings of the 12th European Conference on
Computer Vision - Volume Part I, page 158171, 2012.

Ron Kohavi et al. A study of cross-validation and bootstrap for accuracy estimation and

model selection. In IJCAI, volume 14, pages 1137–1145, 1995.

V. Koltchinskii. Rademacher penalties and structural risk minimization. IEEE Transactions

on Information Theory, 47(5):1902 – 1914, 2001.

P. Latinne, M. Saerens, and C. Decaestecker. Adjusting the outputs of a classiﬁer to new
a priori probabilities may signiﬁcantly improve classiﬁcation accuracy: Evidence from a
multi-class problem in remote sensing. In C. Sammut and A. H. Hoﬀmann, editors, Proc.
18th Int. Conf. on Machine Learning, pages 298–305, 2001.

Quoc Le, Tam´as Sarl´os, and Alex Smola. Fastfood: approximating kernel expansions in
In Proceedings of the 30th International Conference on International

loglinear time.
Conference on Machine Learning-Volume 28, pages III–244. JMLR. org, 2013.

Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and
artier domain generalization. In Proceedings of the IEEE International Conference on
Computer Vision, pages 5542–5550, 2017.

Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Learning to generalize:
Meta-learning for domain generalization. In Thirty-Second AAAI Conference on Artiﬁcial
Intelligence, 2018a.

H. Li, S. J. Pan, S. Wang, and A. C. Kot. Domain generalization with adversarial feature
learning. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, pages
5400–5409, 2018b.

Ya Li, Mingming Gong, Xinmei Tian, Tongliang Liu, and Dacheng Tao. Domain general-
ization via conditional invariant representations. In Thirty-Second AAAI Conference on
Artiﬁcial Intelligence, 2018c.

Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng
Tao. Deep domain generalization via conditional invariant adversarial networks. In Pro-
ceedings of the European Conference on Computer Vision (ECCV), pages 624–639, 2018d.

Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning
In COLT 2009 - The 22nd Conference on Learning Theory,

bounds and algorithms.
2009a.

52

Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation with
In Advances in neural information processing systems, pages 1041–

A. Maurer. Transfer bounds for linear feature learning. Machine Learning, 75(3):327–350,

multiple sources.
1048, 2009b.

2009.

A. Maurer, M. Pontil, and B. Romera-Paredes. Sparse coding for multitask and transfer
learning.
In Sanjoy Dasgupta and David McAllester, editors, Proceedings of the 30th
International Conference on Machine Learning, volume 28 of Proceedings of Machine
Learning Research, pages 343–351, 2013.

Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The beneﬁt of

multitask representation learning. J. Mach. Learn. Res., 17(1):2853–2884, 2016.

Aditya Krishna Menon, Brendan van Rooyen, and Nagarajan Natarajan. Learning from
binary labels with instance-dependent noise. Machine Learning, 107:1561–1595, 2018.

Saeid Motiian, Marco Piccirilli, Donald A Adjeroh, and Gianfranco Doretto. Uniﬁed deep
supervised domain adaptation and generalization. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision, pages 5715–5725, 2017.

K. Muandet, D. Balduzzi, and B. Sch¨olkopf. Domain generalization via invariant feature
representation. In Proceedings of the 30th International Conference on International Con-
ference on Machine Learning (ICML’13), volume 28 of Proceedings of Machine Learning
Research, pages I–10–I–18, 2013.

Nagarajan Natarajan, Inderjit S. Dhillon, Pradeep Ravikumar, and Ambuj Tewari. Cost-
sensitive learning with noisy labels. Journal of Machine Learning Research, 18(155):1–33,
2018. URL http://jmlr.org/papers/v18/15-226.html.

K. R. Parthasarathy. Probability Measures on Metric Spaces. Academic Press, 1967.

A. Pentina and C. Lampert. A pac-bayesian bound for lifelong learning. In Eric P. Xing
and Tony Jebara, editors, Proceedings of the 31st International Conference on Machine
Learning, volume 32 of Proceedings of Machine Learning Research, pages 991–999, 2014.

I.F. Pinelis and A.I. Sakhanenko. Remarks on inequalities for probabilities of large devia-

tions. Theory Probab. Appl., 30(1):143–148, 1985.

J. Quionero-Candela, M. Sugiyama, A. Schwaighofer, and N. D. Lawrence. Dataset Shift in

Machine Learning. The MIT Press, 2009.

A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in

neural information processing systems, pages 1177–1184, 2007.

Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random
features. In Advances in Neural Information Processing Systems, pages 3215–3225, 2017.

53

T. Sanderson and C. Scott. Class proportion estimation with application to multiclass
In Proceedings of the 17th International Conference on Artiﬁcial

anomaly rejection.
Intelligence and Statistics (AISTATS), 2014.

Clayton Scott. A generalized neyman-pearson criterion for optimal domain adaptation. In
Aur´elien Garivier and Satyen Kale, editors, Proceedings of the 30th International Con-
ference on Algorithmic Learning Theory, volume 98 of Proceedings of Machine Learning
Research, pages 738–761, 2019.

Shiv Shankar, Vihari Piratla, Soumen Chakrabarti, Siddhartha Chaudhuri, Preethi Jyothi,
In In-
and Sunita Sarawagi. Generalizing across domains via cross-gradient training.
ternational Conference on Learning Representations, 2018. URL https://openreview.
net/forum?id=r1Dx7fbCW.

S. Sharma and J. W. Cutler. Robust orbit determination and classiﬁcation: A learning

theoretic approach. Interplanetary Network Progress Report, 203:1, 2015.

B. Sriperumbudur, A. Gretton, K. Fukumizu, B. Sch¨olkopf, and G. Lanckriet. Hilbert space
embeddings and metrics on probability measures. Journal of Machine Learning Research,
11:1517–1561, 2010.

Bharath Sriperumbudur and Zolt´an Szab´o. Optimal rates for random fourier features. In

Advances in Neural Information Processing Systems, pages 1144–1152, 2015.

I. Steinwart and A. Christmann. Support Vector Machines. Springer, 2008.

Amos J Storkey. When training and test sets are diﬀerent: characterising learning transfer.

In In Dataset Shift in Machine Learning, pages 3–28. MIT Press, 2009.

Masashi Sugiyama, Taiji Suzuki, Shinichi Nakajima, Hisashi Kashima, Paul von B¨unau, and
Motoaki Kawanabe. Direct importance estimation for covariate shift adaptation. Annals
of the Institute of Statistical Mathematics, 60:699–746, 2008.

Dougal J. Sutherland and Jeﬀ Schneider. On the error of random Fourier features. In Pro-
ceedings of the Thirty-First Conference on Uncertainty in Artiﬁcial Intelligence, UAI’15,
pages 862–871. AUAI Press, 2015.

Zolt´an Szab´o, Bharath K Sriperumbudur, Barnab´as P´oczos, and Arthur Gretton. Learning
theory for distribution regression. The Journal of Machine Learning Research, 17(1):
5272–5311, 2016.

Dirk Tasche. Fisher consistency for prior probability shift. Journal of Machine Learning

Research, 18:1–32, 2017.

S. Thrun. Is learning the n-th thing any easier than learning the ﬁrst? Advances in Neural

Information Processing Systems, pages 640–646, 1996.

D. M. Titterington. Minimum distance non-parametric estimation of mixture proportions.

Journal of the Royal Statistical Society, 45(1):37–46, 1983.

54

J. Toedling, P. Rhein, R. Ratei, L. Karawajew, and R. Spang. Automated in-silico detection
of cell populations in ﬂow cytometry readouts and its application to leukemia disease
monitoring. BMC Bioinformatics, 7:282, 2006.

A. Tsanas, M. A. Little, P. E. McSharry, and L. O. Ramig. Accurate telemonitoring
IEEE transactions on

of parkinson’s disease progression by noninvasive speech tests.
Biomedical Engineering, 57(4):884–893, 2010.

Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun. Large
margin methods for structured and interdependent output variables. Journal of machine
learning research, 6(Sep):1453–1484, 2005.

Brendan van Rooyen and Robert C. Williamson. A theory of learning with corrupted labels.

Journal of Machine Learning Research, 18(228):1–50, 2018.

Haohan Wang, Zexue He, Zachary C. Lipton, and Eric P. Xing. Learning robust represen-
tations by projecting superﬁcial statistics out. In International Conference on Learning
Representations, 2019. URL https://openreview.net/forum?id=rJEjjoR9K7.

J. Wiens. Machine Learning for Patient-Adaptive Ectopic Beat Classication. Masters Thesis,
Department of Electrical Engineering and Computer Science, Massachusetts Institute of
Technology, 2010.

C. Williams and M. Seeger. Using the Nystr¨om method to speed up kernel machines. In
Proceedings of the 14th Annual Conference on Neural Information Processing Systems,
number EPFL-CONF-161322, pages 682–688, 2001.

Zheng Xu, Wen Li, Li Niu, and Dong Xu. Exploiting low-rank structure from latent domains
for domain generalization. In European Conference on Computer Vision, pages 628–643.
Springer, 2014.

Xiaolin Yang, Seyoung Kim, and Eric P Xing. Heterogeneous multitask learning with joint
sparsity constraints. In Advances in neural information processing systems, pages 2151–
2159, 2009.

Yao-Liang Yu and Csaba Szepesvari. Analysis of kernel mean matching under covariate
shift. In Proceedings of the 29th International Conference on Machine Learning, pages
607–614, 2012.

Bianca Zadrozny. Learning and evaluating classiﬁers under sample selection bias. In Pro-

ceedings of the Twenty-ﬁrst International Conference on Machine Learning, 2004.

Kun Zhang, Bernhard Sch¨olkopf, Krikamol Muandet, and Zhikun Wang. Domain adapta-
tion under target and conditional shift. In International Conference on Machine Learning,
pages 819–827, 2013.

Kun Zhang, Mingming Gong, and Bernhard Scholkopf. Multi-source domain adaptation:
A causal view. In Proceedings of the Twenty-Ninth AAAI Conference on Artiﬁcial Intel-
ligence, AAAI’15, pages 3150–3157. AAAI Press, 2015.

55

