PolyU CBS-CFA at the FinSBD task:
Sentence Boundary Detection of Financial Data with Domain
Knowledge Enhancement and Bilingual Training

Mingyu Wan1∗ , Rong Xiang2 , Emmanuele Chersoni1 , Natalia Klyueva1 , Kathleen
Ahrens3 , Bin Miao4 , David Broadstock4 , Jian Kang4 , Amos Yung3 and Chu-Ren
Huang1
1Department of Chinese and Bilingual Studies, The Hong Kong Polytechnic University
2Department of Computing, The Hong Kong Polytechnic University
3Department of English, The Hong Kong Polytechnic University
4School of Accounting and Finance, The Hong Kong Polytechnic University
{mingyu.wan, kathleen.ahrens, bin.miao, david.broadstock, jian.kang, amos.yung,
churen.huang}@polyu.edu.hk, {xiangrong0302, emmanuelechersoni, natalka.kljueva}@gmail.com

Abstract

Sentence Boundary Detection is a basic require-
ment in Natural Language Processing and re-
mains a challenge to language processing for
speciﬁc purposes especially with noisy source
documents. In this paper, we deal with the pro-
cessing of scanned ﬁnancial prospectuses with
a feature-oriented and knowledge-enriched ap-
proach. Feature engineering and knowledge
enrichment are conducted with the participa-
tion of domain experts and for the detection
of sentence boundaries in both English and
French. Two versions of the detection sys-
tem are implemented with a Random Forest
Classiﬁer and a Neural Network. We engi-
neer a fused feature set of punctuation, digi-
tal number, capitalization, acronym, letter and
POS tag for model ﬁtting. For knowledge en-
hancement, we implement a rule-based valida-
tion by extracting a keyword dictionary from
the out-of-vocabulary sequences in FinSBD’s
datasets. Bilingual training on both English
and French training sets are conducted to en-
sure the multilingual robustness of the sys-
tem and to extend the relatively small training
data. Without using any extra data, our sys-
tem achieves fair results on both tracks in the
shared task. Our results (English1: F1-Mean
= 0.835; French: F1-Mean = 0.86) as well as a
post-task quick improvement with self-adaptive
knowledge enhancement based on testing data
demonstrate the eﬀectiveness and robustness
of bilingual training with multi-feature min-
ing and knowledge enhancement for domain-
speciﬁc SBD task.

∗Contact Author
1This is the adapted result as illustrated in Section 5.

1 Introduction
Sentence Boundary Detection (SBD), which aims at de-
tecting/disambiguating sentence boundaries of texts, is
a fundamental step in many Natural Language Process-
ing (NLP) applications. It should be carried out before
other critical components of NLP, e.g. part-of-speech
(POS) tagging, syntactic-semantic-discourse parsing, in-
formation extraction or machine translation. Existing
SBD approaches have shown promising results for lan-
guages that have dependable orthographic conventions
to mark beginning and ending of sentences, such as in
English and many Europeans languages. Relevant re-
cent work include (e.g. [Riley, 1989; Reynar and Ad-
wait, 1997; Mikheev, 2002; Palmer and Hearst, 1997;
Read, 2012]). However, previous work in SBD mainly
dealt with well-formed and clean data such as articles
from the Brown corpus [Hearst, 1994] or Wall Street
Journal [Palmer and Hearst, 1997].

SBD remains challenging in two scenarios. The ﬁrst
involves documents encoded in non-text formats, such
as Adobe PDF format, or other image formats. They
provide the exact layout of a human readable docu-
ment on a wide range of machines. However, texts con-
verted from PDF documents by OCR software are usu-
ally noisy and potentially with the loss of substantial
formatting features. This chaos leads to diﬃculties in
SBD, and so far has been under-researched. The second
involves languages whose orthography does not mark
sentence boundary conventionally. For instance, [Huang
and Chen, 2017] shows that using the period punctu-
ation will lead to signiﬁcant divergence from sentence
boundaries. What they proposed are followed by [Hou
et al., 2019] in their Menzerath-Altamann based power
relations between a clause and its constituent words (in-
stead of between sentences and words). In this current
paper, we deal with the ﬁrst challenge.

There are a number of issues to be addressed when ap-
plying SBD to ﬁnancial documents. Unlike passages of

Proceedings of the First Workshop on Financial Technology and Natural Language Processing 
(FinNLP@IJCAI 2019), pages 122-129, Macao, China, August 12, 2019.

formal texts, ﬁnancial documents are often heavily pop-
ulated with rich tables of data—sometimes stretching
over multiple pages—and ﬁgures, titles, dates and key-
words of various types. The presence of such non-textual
information is admittedly not unique to ﬁnancial docu-
ments, but it should be noted that many ﬁnancial docu-
ments also do not come in clean/easily machine readable
structures. Detecting sentence boundaries on the basis
of periods/stops may also be less straightforward, for
example the presence of company tickers in a document
may introduce some diﬃculties in cleanly identifying sen-
tence boundaries, especially if appended with exchange,
for example the full ticker for China Light and Power
(CLP) company listed in Hong Kong can be written as
‘0002.hk’. Additionally, sentences may contain various
ﬁnancial terms/acronyms, including company name ab-
breviations, that may impact the syntactic structure and
hence generate sentence confusion.

As such ﬁnancial documents present a range of chal-
lenges that result in the need to use a hybrid of lan-
guage processing tools in combination with knowledge
enrichment speciﬁc to the ﬁnancial domain. With such
endeavors, we can expect chances of achieving SBD with
reasonable levels of accuracy.

In the following sections, we will review some related
work in Section 2, describe the features and methodology
in Section 3, show and discuss the results in Section 4
and ﬁnally conclude this work in Section 5.

2 Related Work
Sentence Boundary Detection is a fundamental issue in
Natural Language Processing, which can be viewed as
a classiﬁcation issue. Current studies normally tackle
the problem as the identiﬁcation of the truthful sen-
tence ending markers among the ambiguous ones. The
history of SBD development witnessed machine learn-
ing as the earliest attempts (e.g. [Riley, 1989; Reynar
and Adwait, 1997; Palmer and Hearst, 1997]), with rule-
based systems coming afterwards (e.g. [Mikheev, 2002;
Mikheev, 2000]). There has been some work occasionally
using unsupervised techniques (e.g. [Kiss and Strunk,
2006]).

Early attempts have already shown promising results
of SBD, but all with well-formed data. For example,
since Riley [1989]’s very ﬁrst work of SBD, a 99.8% ac-
curacy was reported by investigating only a single punc-
tuation mark, i.e. period, with the use of Decision Tree
classiﬁer trained on 25 million words of newswire texts.
Hearst [1994] achieved a 1.5% error rate by using Feed-
forward Neural Network of POS features trained on the
Brown corpus. Later on, Palmer and Hearst [1997] de-
veloped SATZ, a system that used features of contex-
tual POS distribution and words-as-vectors of the target
punctuations via NN and DT with training on the 30 mil-
lion WSJ corpus. Their work hit the record of the stat-
of-the-arts of SBD with error rates of 1.1% for NN and
1% for DT. Synchronically, Reynar and Adwait [1997]
adopted supervised Maximum Entropy learning with two

sets of features: in-domain ﬁnancial uses, e.g. honoriﬁcs
(Mr., Dr., etc.)
and corporate designations (Corp.);
domain-independent abbreviations, as well as ‘!’, ‘.’ or
‘?’ as potential boundaries. This work, however, was
slightly inferior in performance with accuracies of 98.8%
and 97.9% respectively for the domain-dependent sys-
tem and accuracies of 98.0% and 97.5% respectively on
the portable system.

Modern machine learning techniques provide us with
a series of statistical models focusing on data patterns,
nonlinear features and forecast accuracy. With the
breakthrough of computing technology, the nonlinear
methods became feasible in 1980s, as represented by
Breiman et al. [1984]’s work with tree-based and re-
gression models. Since then, an increasing number of
tree-based models, both supervised and unsupervised,
were developed and promptly emerged, such as Random
Forests, Boosting Trees, etc. Prior to traditional classi-
ﬁers, Neural Network methods were introduced to SBD
by McCulloch and Pitts [1943]. From 1980s, the Neural
Network, incorporating the Bayesian Neural Network,
was resurged by the upgrades of computing technology as
well as the appearance of back-propagation algorithms.
Unlike tree-based methods, NN methods present smooth
functions of parameters, which facilitate the develop-
ment of Bayesian inference.

Complementary to machine learning, Mikheev [2000;
2002] employed rule-based systems for SBD, and re-
ported error rates of 0.31% and 0.20% with training on
WSJ and the Brown corpus respectively. Recently, Read
et al. [2012] and Griﬃs et al. [2016] both adopted sev-
eral state-of-the-art NLP toolkits for SBD with mixed
datasets of varied formality and speciﬁcity. Both works
showed that the existing toolkits for SBD in speciﬁc do-
mains are worse without resistance to domain-transfer
or formality change.

To approach the above-mentioned problem of SBD
in recent decade, we need to enforce renewed eﬀorts of
further shaping the NLP tools as well as addressing to
domain-speciﬁc and informality issues, with aim of re-
freshing a new record in the SBD history.

In this paper, we propose a feature-oriented and
knowledge-enriched approach to detecting the begin-
nings and endings of ﬁnancial data in both English and
French by using a Random Forest Classiﬁer (RFC) and a
Neural Network (NN). In addition to feature engineering,
model ﬁtting and parameter tuning, we conduct post-
classiﬁcation knowledge enhancement with a rule-based
keyword validation on the predictions by automatically
identifying and extracting the out-of-boundary word se-
quences from the FinSBD datasets [Ait Azzi et al., ]. In
addition, the main body of noise in the datasets provides
us with a useful resource as a by-product of the task for
rectifying the ambiguous boundaries.

3 Features and Methodology
In this section, we describe the features and methodol-
ogy of this work with a pipeline of feature engineering,

classiﬁcation and aspect knowledge enrichment (post-
classiﬁcation validation).

3.1 Feature Engineering
Feature engineering plays an important role in machine-
learning, involving the selection of a subset/fused set of
informative and discriminative features with dual pur-
poses of dimension reduction and classiﬁcation lever-
age [Garla and Brandt, 2012].
In general classiﬁca-
tion tasks, features typically include bags of characters,
words, n-grams and/or concepts in a text corpus, which,
however, causes high dimensionality of feature space in
lowering classiﬁcation eﬃciency.

Feature selection is necessary when feature space is
overloaded or in redundancy. Algorithms of term fre-
quency, chi-square, information gain, mutual informa-
tion or relevance score are usually adopted in automatic
feature selection (e.g. [Lee and Lee, 2006; Chen et al.,
2009]); domain knowledge is also helpful to guide the
feature engineering process. In this sentence segmenta-
tion task, we utilize a semi-automatic selection method
that both considers high frequency words and keyword
knowledge, as shown in Sections 3.1 and 3.3 below.
By a close observation and comparison of

the
scanned documents with the gold boundaries in the
datasets [Ait Azzi et al., ], we introspect the key sections
of erroneous predictions both manually and statistically.
This leads to the inclusion of the following sets of salient
features for ﬁtting the classiﬁers.

• Two sets of punctuation: Punctuation serves as
important cues for SBD and has been proved as the
most useful feature in SBD. In addition to using pe-
riod as the baseline feature set, we also include a
set of special punctuation-symbols that are preva-
lent in ﬁnancial data, such as the dollar signs, math
operators and copyright symbols, as listed below:

– PUNC SET1 = [‘.’]
– PUNC SET2 = [‘?’, ‘!’, ‘;’, ‘%’, ‘-’, ‘/’, ‘”’, ‘\’,
‘)’, ‘(’, ‘∗’, ‘(cid:0)’, ‘<’, ‘≥’, ‘>’, ‘≤’, ‘•’, ‘e’, ‘$’,
‘£’, “‘’, ‘©’, ‘®’]

By adding the second punctuation set in the at-
tribute table, we got 1% F1 improvement for the
validation sets of both languages.

• Initially capitalized words: As suggested in
the samples of gold boundaries, most BEGINs are
marked with initially capitalized words (e.g. “Dis-
tribution BEGIN”) or the ENDs are largely pre-
ceding such words (e.g. “. Enter END The BEGIN
sales”). Although it introduces some confusing in-
formation for the classiﬁers, such as the keywords
in titles, tables, ﬁgures, etc., the inclusion of this
type of feature on average improves 2% F1 score
of validation for both languages. In order to asso-
ciate such feature with both BEGINs and ENDs, we
use a feature array of three dimension in the pre-,
current, post- positions to maximally represent its
discriminativeness in predicting boundaries.

• Acronyms or Abbreviations: Acronyms or ab-
breviations are also salient features for marking
boundaries as indicated in both the existing lit-
erature and the FinSBD datasets. For example,
“UBS BEGIN” “co” or “kiid” show that acronyms
tend to co-occur (all capitalized words) or not occur
(all lower cased words) with boundaries. As such,
we construct a three-dimension attribute array of
storing the Boolean value of all-word-capitalization
in the pre-, current, post- positions. This feature
set also improves around 1% F1 in the validation
set for both languages.

• Digital numbers: Digital numbers is a common
property of ﬁnancial data which causes confusion
for disambiguating e.g. decimal points from end-
ings, as in “10.3”. To identify both the left and
right context of the target period, we also construct
a three-dimension attribute array for representing
such cases. This feature set helps improve 2% F1
for the French validation set and 1% F1 for the En-
glish validation set.

• Letters or Roman numbers: As another salient
feature in ﬁnancial data, letters (e.g. A-Z, a-z al-
phabetical letters) or Romance numbers (I, II, . . . ,
XII ) are highly suggestive of non-boundary tokens.
Therefore it also serves as a useful feature for ex-
cluding the wrong boundaries. A tri-gram feature
array is also constructed to represent such informa-
tion in the pre-, current, post- positions, which helps
around 1% F1 improvement of both validation sets.

• POS tags: Despite the fact that sentence segmen-
tation occurs prior to part-of-speech tagging in NLP
processing, the pos information of individual tokens
can, in turn, indicate the phrasal structure of a word
sequence which may provide useful cues to the iden-
tiﬁcation of verbal sentences or alternatively, the
non-clausal noun phrases (keywords). By including
the three-dimension POS feature (the UPOS tag set
by UDPipe2) in our experiment, our system is fur-
ther optimized with 3% F1 increase for both valida-
tion sets.

• Enter (‘\n’): Enter (‘\n’) seems a universal fea-
ture for any type of document. But after a close look
into the converted pdf documents in the FinSBD
datasets, we found that Enters (‘\n’) is strongly as-
sociated with the conversion errors caused by the
pdf scanning. With the inclusion of such feature
in a three-dimension array, we further improve the
system with 1% F1 for both validation sets.

For maximizing the discriminative power of the above
features, we construct a fused feature set of 24 dimen-
sions to ﬁt the machine learning models and get an op-
timized performance (English: F1-Mean = 0.87; French:
F1-Mean = 0.85) in the validation sets.

2http://ufal.mﬀ.cuni.cz/udpipe

3.2 Classiﬁcation Models
Ensemble Learning of Random Forest
Random Forest Classiﬁer (RFC) is a tree-based ensemble
classiﬁer. It combines the decision of multiple Decision
Tree (DT) classiﬁers where each classiﬁer is generated
using a random vector independently sampled from the
input vector, and each tree casts a unit vote for the most
popular class to classify an input vector [Breiman, 1999].
The ensemble RFC is generally more accurate than all
the individual classiﬁers as it makes use of many naive
classiﬁers that randomly use a subset of the vector, thus
it is more robust to overﬁtting in comparison to tradi-
tional decision trees. As such, it is our ﬁrst choice of
classiﬁer in this study. The RFC classiﬁer is imported
from the sklearn package3 where a random forest is taken
as a meta estimator (n estimators is 10 by default) that
ﬁts a number of decision tree classiﬁers on various sub-
samples of the dataset and uses averaging to improve the
predictive accuracy and control over-ﬁtting.

The design of a decision tree required appropriate at-
In
tribute selection measure and a pruning method.
our experiment, we use randomly selected features at
each node to grow a tree with an optimized setting of
min samples split as 8, max features as “log2” and ran-
dom state as 10. In addition, we set oob score true to
use out-of-bag samples to estimate the generalization ac-
curacy.

The above setting of parameters of RFC work out the
best performance in our estimation on the validation
sets.

Neural Network
Resembling the biological neural networks, artiﬁcial
Neural Networks (NN) approaches were proposed and
led to great improvements in a number of NLP tasks.
An artiﬁcial NN is usually composed of many simple
processors (neurons) that are interconnected, operate in
several layers and learn from input of examples. Con-
sidering the similar characteristics of ﬁnancial data, we
implemented a NN-based approach as a complementary
work to the RFC model.

In the validation, we trained the Multi-layer Neural
Network using Tensorﬂow4 following several runs of pa-
rameter optimization. The optimization was done with
bilingual training on both English and French training
sets and testing on the English validation set. The op-
timal setup for training the model was a network of one
input layer (density 300) and 1 hidden layer(density 100)
with the relu activation function, and one output layer
(3 categories) with the softmax activation function. As a
loss function we used categorical crossentropy, and Adam
as an optimizer. The batch size was set to 32, and the
number of epochs was 5.

The input units were the feature vectors as described
in Section 3.1, but certain features were excluded for the

3https://scikit-learn.org/stable/modules/generated/

sklearn.ensemble.RandomForestClassiﬁer.html

4https://www.tensorﬂow.org/api docs/python/tf/nn

Neural Network run of the French trial for reasons of
optimization.

3.3 Domain Knowledge Enhancement
Knowledge enrichment has been proven to be a useful
guidance for the post-processing of confused classiﬁca-
tion by statistical models [Ghosh and NAG, 2002]. In
this task, OCR conversion of ﬁnancial documents intro-
duced large chunks of out-liars, such as the titles, dates,
tables, ﬁgures, etc., which fail to fall into the traditional
category of sentence boundary. These non-textual sec-
tions, as ﬁnely segmented with the “Enter” marks, cause
erroneous predictions of boundaries. To solve this prob-
lem, we implemented a post-processing procedure to cor-
rect false positive predictions, as realized with the follow-
ing two algorithms.

Keyword Extraction
Following the above-mentioned principle, we constructed
a keyword dictionary with Algorithm 1. A broader def-
including out-of-
inition of keyword is adopted here,
boundary words, symbols and phrases. We utilize the
well segmented training and validation sets [Ait Azzi et
al., ] for resource construction. Intuitively, if any word
sequence locates between an “END” and the next “BE-
GIN”, we regard it as a potential out-lier and construct
keywords with further segmentation marked by “Enter”.
The ﬁnal keyword dictionary with respect to both lan-
guages is then constructed, containing elements of key-
value (keyword-frequency) pairs.

Algorithm 1 Keyword Extraction
Input: dataset
Output: keyword dict

else

continue

if “BEGIN” in nxtword then

1: for i in len(dataset) do
curword = dataset[i].
2:
nxtword = dataset[i+1].
3:
if “END” in curword then
4:
5:
6:
7:
8:
9:
10:
end if
11:
12: end for
13: reﬁne keyword dict with length threshold.
14: reﬁne keyword dict with frequency threshold.
15: return keyword dict

add keyword to keyword dict.
update frequency in keyword dict.

end if

To further control the quality of extracted keywords,
we introduced length threshold and frequency threshold
to ﬁlter those patterns that are too short or rarely oc-
curring. As as result, a keyword dictionary of 16,501
keyword-frequency pairs is generated for the rule-based
validation, as well as providing a ﬁnancial resource for
potential use in future IR inquiries.

Rule-based Validation
With the keyword dictionary generated, we used a rule-
based approach for correcting the potentially wrong
boundaries that are not in the keyword list, as illustrated
in Algorithm 2.

Algorithm 2 Rule-based validation
Input: dataset, raw pred, keyword dict
Output: updated prediction

if word sequence match keyword then

for i in len(dataset)-len(keyword) do

1: for keyword in keyword dict.key do
2:
3:
4:
5:
6:
7: end for
8: return raw pred

end if
end for

update raw pred with NO BOUNDARY

As shown in Algorithm 2, each keyword in the dic-
tionary is used as a rule. Every word sequence that
matches a keyword in the dictionary shall be forced to
the “NO BOUNDARY” class. This process will be exe-
cuted iteratively until all the predictions are validated.
As to be shown in the following section, the experimental
results on the validation sets and the ﬁnal results on the
test sets of both languages have consistently veriﬁed the
usefulness of knowledge enhancement in domain-speciﬁc
classiﬁcations.

4 Results
In this section, we show our classiﬁcation results with
the following four aspects of comparisons.

4.1 Classiﬁers
This section focuses on the comparison of the classiﬁ-
cation performance of the two classiﬁers, i.e. RFC and
NN, with the same experimental setting. A fused fea-
ture set is used and bilingual training is conducted. The
classiﬁcation results on both the validation (Dev) set and
the test set of the two languages are shown in Figure 1
below:

As it is easy to see in Figure 1, RFC shows superior
performance with 1% or 4 % F1 gain compared to NN
for both the validation tasks (Dev en and Dev fr) and
the testing of the French track (Test fr). This is highly
suggestive that the ensemble random forest is more ﬁtted
to the selected feature set in this work, while NN seems
to demonstrate no advantage of wininning traditional
classiﬁers despite having the same salient feature set in
this task.

However, what is contradictory to our estimation is:
NN outperforms RFC with 3.5% F1 discrepancy in the
English track (Test en), whereas the results of our vali-
dation on the English development set is opposite (RFC:
0.875 vs. NN: 0.86). Another obvious observation is:
both classiﬁers’ performance on the English test set

Figure 1: Classiﬁcation Results of NN vs. RFC

drops signiﬁcantly with a slip of 4-10% F1. By review-
ing the released test set with gold labels, we found that a
large number of new acronyms and code series are intro-
duced. This causes a systematic deterioration of both
classiﬁers, but NN presents a higher robustness to the
feature surprise.

The unexpected performance of RFC in the English
test set can be attributed to an over-ﬁtting problem and
hence draws our attention to look for a semi-supervised
or un-supervised mechanism in complementing the fea-
ture mismatch between the validation set and the test
set, which shall lead to a more stable result in similar
tasks.

4.2 Bilingual Training
This section focuses on the comparison of the classiﬁca-
tion performance of bilingual vs. monolingual training
with the same experimental setting. A fused feature set
and the RFC classiﬁer is adopted. As what we submit-
ted for the contest are all based on bilingual training,
the current comparison is based on the validation (Dev)
set only. The classiﬁcation results on the validation set
of the two languages are shown in Figure 2 below:

Figure 2: Classiﬁcation Results of bi- vs. mono-lingual train-
ing

As shown in Figure 2, bilingual training brings a con-
sistent beneﬁt to the classiﬁcation performance with 1%

F1 improvement for English and 2% F1 improvement
for French. This impact can be counted as huge for any
kinds of competition. By referring to this set of valida-
tion results, we ﬁnally conduct bilingual training for all
the runs of submission.

4.3 Features
This section focuses on the comparison of the feature dis-
criminativeness in the SBD classiﬁcation task by adding
the individual feature set separately in each implemen-
tation. The basic experimental setting is the same, in-
cluding using the RFC classiﬁer and bilingual training.
As the submissions are all based on the full feature set,
the current comparison is implemented on the validation
(Dev) set only. The classiﬁcation results on the valida-
tion set of English are shown in Table 1 below:

Features\F1
Punc1
+Punc2
+Cap
+Acro
+Dig
+Lett
+POS
+Enter/All

BSa ESb Mean

0.70
0.71
0.72
0.73
0.73
0.74
0.78
0.80

0.82
0.83
0.86
0.87
0.89
0.90
0.92
0.92

0.76
0.77
0.79
0.80
0.81
0.82
0.85
0.86

δ(%)c
baseline
1 ↑
2 ↑
1 ↑
1 ↑
1 ↑
3 ↑
1 ↑

a Beginning boundaries
b Ending boundaries
c F1 improvement in percentage

Table 1: Performance of feature mining in the English Dev
set with RFC

In Table 1, we can see that by using the period punc-
tuation as the baseline feature set, the classiﬁcation per-
formance is already decent, with 0.76 F1-Mean. And
the ‘ES’ prediction is apparently more accurate than the
‘BS’ prediction, which is intuitively reasonable as a pe-
riod usually marks an end of a sentence.

By adding the other feature set one by one, as men-
tioned in Section 3.1, the performance consistently in-
creases with 1-3% F1 improvement. Some features help
on both ‘BS’ and ‘ES’, such as ‘Punc2’, ‘Cap’, ‘Acro’,
‘Lett’, ‘POS’; some help only on ‘BS’, such as ‘Enter’;
and some help only on ‘ES’, such as ‘Dig’. Among all the
feature sets, POS shows the greatest improvement to the
identiﬁcation of both ‘BS’ and ‘ES’, which implies the
usefulness of incorporating certain syntactic information
into sentence detection.

4.4 Keyword Validation
section focuses on the comparison of post-
This
classiﬁcation validation vs. non-validation of keyword
knowledge to demonstrate the eﬀectiveness of knowl-
edge enhancement. The basic experimental setting is
the same, including the fused feature set, the RFC clas-
siﬁer and bilingual training. As we submitted 2 runs of
both languages with the RFC method for the contest,

the current comparison is based on both the validation
(Dev) set and the test set. The corresponding classiﬁca-
tion results are shown in Table 2 below.

Dev

Test

F1

BS
ES
Mean
BS
ES
Mean

NOa YESb δ(%)
0
0.83
0.83
1 ↑
0.87
0.86
0.5 ↑
0.85
0.845
3 ↑
0.84
0.81
0.88
0.88
0
1.5 ↑
0.86
0.845

a Without keyword validation
b With keyword validation

Table 2: Performance of RFC in the French Dev and Test
sets in terms of keyword validation

Informative knowledge can be a very useful guidance
to correcting the confused predictions of statistical mod-
els, as evidenced in this experiment. We implemented
the keyword extraction and validation procedure, as
shown in Section 3.3, to post-process the predictions of
the RFC model with the aim of rectifying certain wrong
labels caused by confusion of the out-of-boundary key-
words.

The results in Table 2 indicate that keyword valida-
tion is indeed successful for both validation and testing.
Notably, the improvement of predicting ‘BS’ is signiﬁ-
cant (3%↑), which leads to a 1.5% F1-Mean gain for our
system in the ﬁnal contest, and this result is comparable
to the top teams. The success of keyword validation in
our experiment suggests that by using adequate domain
knowledge in NLP tasks, we could optimize the classi-
ﬁcation performance in an eﬃcient way. Moreover, the
domain knowledge itself serves a valuable resource for
text processing and information extraction of the spe-
ciﬁc domain.

5 Knowledge Adaptation to the Test

Sets and the Final Results

This section aims to ﬁll in the gap of our mistake in miss-
ing the implementation of the knowledge enhancement
procedure on the test sets, which causes an unexpected
low result of the English trial for RFC.

In order to remedy the above mistake, we simply run
the script of the same procedure in Section 3.3 by includ-
ing the keywords of test sets in the knowledge dictionary
so as to cover the additional domain speciﬁc words that
are not included/recoverable in the training data5. The
corresponding results and ranks of our system are shown
in Table 3 below.

As Table 3 shows, the results of RFC for the French
trial are consistent among the three types of valida-
tion and our team achieves a stable rank (No.
8)
for the English trial,
in the competition. However,

5https://github.com/ClaraWan629/FinSBD RFC r1

Test Set

F1-Mean Rank

Dev en rfc1
Test en rfc1
Test en rfc1 adapted

Dev fr rfc1
Test fr rfc1
Test fr rfc1 adapted

0.875
0.78∗
0.835?
0.85
0.86∗
0.86?

—
16(cid:5)
10(cid:62)
—
8(cid:5)
8(cid:62)

∗ Result without adaptation to the test set
(cid:5) Rank without adaptation to the test set
? Result with adaptation to the test set
(cid:62) Rank with adaptation to the test set

Table 3: Performance of RFC w.r.t. knowledge adaptation

there is a 9.5% F1-Mean gap between Dev en rfc1 and
Test en rfc1, which is surprisingly diﬀerent from our es-
timation. As mentioned above, we conduct a keyword
adaption step on the test set and obtained a more rea-
sonable result of the English trial with a rank of 10, as
highlighted in Table 3. This adaptation step necessar-
ily proves that our method of feature engineering and
knowledge enhancement is eﬀective and robust and it is
important to resolve the over-ﬁtting problem by apply-
ing it to the test sets.

6 Conclusion
In this work, we demonstrate the eﬃciency and robust-
ness of combining feature engineering, bilingual training
and knowledge-enriched approaches to the detection of
sentence boundaries for noisy data in Financial NLP. We
ﬁrst conduct document introspection and error analysis
in mining salient features for ﬁtting the models and the
ﬁnal features include a 24-dimension array of punctu-
ation, digital number, capitalization, acronym, letters,
Enter, and POS tags. We then tune the classiﬁers on
parameters of min samples split and max features for
RFC and batch size, epochs for NN with optimized per-
formance on Dev sets. We also implement rule-based
validation of keyword knowledge extracted from the
out-of-vocabulary word sequences in FinSBD’s datasets.
Lastly, we train on both English and French datasets
to make predictions with maximal training data. The
results of the four aspects of comparisons suggest the
following ﬁndings: 1) NN does not showing signiﬁcant
advantage over traditional classiﬁers as RFC is better
ﬁtted to the selected feature set in this work; 2) NN
performs better in terms of new features not originally
selected; 3) The signiﬁcant improvement of using POS
information of a three-dimension sequence in the task
shows that syntactic information may be helpful for sen-
tence detection; 4) Informative knowledge enhancement
shows a double beneﬁt for both the correction of mis-
classiﬁcation of statistical models and resource construc-
tion in domain-speciﬁc NLP tasks.
It is important to
note that our unexpectedly lower result of RFC for the
English trial is found to be caused by the mistake of not

implementing knowledge enhancement on the test set.
After we conduct a knowledge adaptation to the test set,
the outcome achieved is close to our estimation (English:
0.835%; French: 0.86%) and within reasonable range of
the best results. Although our result is not currently the
best, our system is designed to be highly adaptive with
minimal training data for a new language and/or a novel
domain. We hope to conduct additional studies to verify
the eﬀectiveness of this feature design.

Acknowledgments
This work is partially supported by the fund of the GRF
grant (RGC Ref No. 15608618).

References
[Ait Azzi et al., ] Abderrahim Ait Azzi,

Houda
Bouamor, and Sira Ferradans.
The FinSBD-
2019 Shared Task: Sentence boundary detection in
PDF Noisy text in the Financial Domain.

[Breiman et al., 1984] Breiman, Friedman, Olshen, and
Stone. Classiﬁcation and regression trees. Wadsworth
Int. Group, 37(15):237–251, 1984.

[Breiman, 1999] L. Breiman. Random forests—random

features. Technical Report 567, 1999.

[Chen et al., 2009] Chen, Huang, Tian, and Qu. Fea-
ture selection for text classiﬁcation with na¨ıve bayes.
Expert Systems with Applications, 36(3):5432–5435,
2009.

[Garla and Brandt, 2012] Vijay N. Garla and Cynthia
Brandt. Ontology-guided feature engineering for clin-
ical text classiﬁcation. Journal of biomedical infor-
matics, 45(5):992–998, 2012.
[Ghosh and NAG, 2002] Joydeep

and
Arindam C. NAG. Knowledge enhancement and
reuse with radial basis function networks. In Proceed-
ings of the 2002 International Joint Conference on
Neural Networks. IJCNN’02 (Cat. No. 02CH37290),
pages 1322–1327. IEEE, 2002.

Ghosh

[Griﬃs et al., 2016] Griﬃs, Shivade, Lussier E. Fosler,
and A. M. Lai. A quantitative and qualitative evalu-
ation of sentence boundary detection for the clinical
domain. In AMIA Summits on Translational Science
Proceedings, page 88, 2016.

[Hearst, 1994] Marti A Hearst. Multi-paragraph seg-
In Proceedings of the
mentation of expository text.
32nd annual meeting on Association for Computa-
tional Linguistics, pages 9–16. Association for Com-
putational Linguistics, 1994.

[Hou et al., 2019] Renkui Hou, Chu-Ren Huang, and
Hongchao Liu. A study on chinese register character-
istics based on regression analysis and text clustering.
Corpus Linguistics and Linguistic Theory, 15(1):1–37,
2019.

[Huang and Chen, 2017] Chu-Ren Huang and Keh-
Jiann Chen. Sinica treebank. In Nancy Ide and James

Pustejovsky (Eds.), Handbook of Linguistic Annota-
tion, pages 641–657. Dordrecht: Springer, 2017.

[Kiss and Strunk, 2006] Tiber Kiss and Jan Strunk. Un-
supervised multilingual sentence boundary detection.
Computational linguistics, 32(4):485–525, 2006.

[Lee and Lee, 2006] Changki Lee and Gary G. Lee. In-
formation gain and divergence-based feature selection
for machine learning-based text categorization.
In-
formation processing & management, 42(1):155–165,
2006.

[McCulloch and Pitts, 1943] W. S. McCulloch and
W. Pitts. A logical calculus of the ideas immanent
in nervous activity. The bulletin of mathematical
biophysics, 5(4):115–133, 1943.
[Mikheev, 2000] Andrei Mikheev.

Tagging sentence
boundaries. In Proceedings of the 1st North American
chapter of the Association for Computational Linguis-
tics conference, pages 264–271. Association for Com-
putational Linguistics, 2000.

[Mikheev, 2002] Andrei Mikheev. Periods, capitalized
words, etc. Computational Linguistics, 28(3):289–318,
2002.

[Palmer and Hearst, 1997] David D.

and
Marti A. Hearst. Adaptive multilingual sentence
boundary disambiguation. Computational linguistics,
23(2):241–267, April–June 1997.

Palmer

[Read, 2012] Jonathon Read. Sentence boundary detec-
tion: A long solved problem? In Proceedings of COL-
ING 2012: Posters, pages 985–994, Mumbai, India,
2012. Association for Computational Linguistics.
[Reynar and Adwait, 1997] Jeﬀrey C. Reynar and Rat-
naparkhi Adwait. A maximum entropy approach to
identifying sentence boundaries. In Proceedings of the
ﬁfth conference on Applied natural language process-
ing, pages 16–19. Association for Computational Lin-
guistics, March 1997.

[Riley, 1989] Michael D. Riley. Some applications of
tree-based modelling to speech and language. In Pro-
ceedings of DARPA ,5’peech and Language Technology
Workshop, pages 339–352, Cape Cod, Massachusetts,
1989.

