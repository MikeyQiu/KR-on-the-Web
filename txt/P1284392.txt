Online SSVEP-based BCI using Riemannian geometry
Emmanuel Kalunga, Sylvain Chevallier, Quentin Barthélemy, Karim Djouani,

Eric Monacelli, Yskandar Hamam

To cite this version:

Emmanuel Kalunga, Sylvain Chevallier, Quentin Barthélemy, Karim Djouani, Eric Monacelli, et al..
Online SSVEP-based BCI using Riemannian geometry. Neurocomputing, Elsevier, 2016, 191, pp.55-
68. ￿10.1016/j.neucom.2016.01.007￿. ￿hal-01351623￿

HAL Id: hal-01351623

https://hal.archives-ouvertes.fr/hal-01351623

Submitted on 4 Aug 2016

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Neurocomputing 191 (2016) 55–68

Contents lists available at ScienceDirect

Neurocomputing

journal homepage: www.elsevier.com/locate/neucom

Online SSVEP-based BCI using Riemannian geometry

Emmanuel K. Kalunga a,b, Sylvain Chevallier b,n, Quentin Barthélemy c, Karim Djouani a,
Eric Monacelli b, Yskandar Hamam a

a Department of Electrical Engineering and the French South African Institute of Technology, Tshwane University of Technology, Pretoria 0001, South Africa
b Laboratoire d'Ingénierie des Systèmes de Versailles, Université de Versailles Saint-Quentin, 78140 Velizy, France
c Mensia Technologies, S.A. ICM, Hôpital de la Pitié-Salpêtrière, 75013 Paris, France

a r t i c l e i n f o

a b s t r a c t

Article history:
Received 10 April 2015
Received in revised form
17 December 2015
Accepted 8 January 2016
Communicated by S. Fiori
Available online 17 February 2016

Keywords:
Riemannian geometry
Online
Asynchronous
Brain–Computer Interfaces
Steady State Visually Evoked Potentials

Challenges for the next generation of Brain Computer Interfaces (BCI) are to mitigate the common
sources of variability (electronic, electrical, biological) and to develop online and adaptive systems fol-
lowing the evolution of the subject's brain waves. Studying electroencephalographic (EEG) signals from
their associated covariance matrices allows the construction of a representation which is invariant to
extrinsic perturbations. As covariance matrices should be estimated, this paper ﬁrst presents a thorough
study of all estimators conducted on real EEG recording. Working in Euclidean space with covariance
matrices is known to be error-prone, one might take advantage of algorithmic advances in Riemannian
geometry and matrix manifold to implement methods for Symmetric Positive-Deﬁnite (SPD) matrices.
Nonetheless, existing classiﬁcation algorithms in Riemannian spaces are designed for ofﬂine analysis. We
propose a novel algorithm for online and asynchronous processing of brain signals, borrowing principles
from semi-unsupervised approaches and following a dynamic stopping scheme to provide a prediction
as soon as possible. The assessment is conducted on real EEG recording: this is the ﬁrst study on Steady-
State Visually Evoked Potential (SSVEP) experimentations to exploit online classiﬁcation based on Rie-
mannian geometry. The proposed online algorithm is evaluated and compared with state-of-the-art
SSVEP methods, which are based on Canonical Correlation Analysis (CCA). It is shown to improve both
the classiﬁcation accuracy and the information transfer rate in the online and asynchronous setup.

& 2016 Elsevier B.V. All rights reserved.

1.

Introduction

Human–machine interactions without relying on muscular
capabilities is possible with Brain–Computer Interfaces (BCI) [1]
They are the focus of a large scientiﬁc interest [2–4], especially
those based on electroencephalography (EEG) [5]. From a large
literature based on the BCI competition datasets [6–8], one can
identify the two most challenging BCI problems: on the one hand,
the inter-individual variability plagues the models and leads to
BCI-inefﬁciency effect [9–11], on the other hand, the intra-
individual changes calls for the development of online algo-
rithms and adaptive systems following the evolution of the sub-
ject's brain waves [12–14]. To alleviate these variations, several
signal processing and machine learning techniques have been
proposed, such as ﬁltering, regularization or clustering [15,16]
candidate”
without
the
methodology.

emergence of

an obvious

“best

A common vision is shared by all the most successful approa-
ches to reduce signal variabilities: they are applied on covariance

n Corresponding author.

http://dx.doi.org/10.1016/j.neucom.2016.01.007
0925-2312/& 2016 Elsevier B.V. All rights reserved.

matrices instead of working in the input signal space. Common
Spatial Pattern (CSP) [17–19], which is the most known pre-
processing technique in 2-class BCI, try to maximize the covar-
iance of one class while minimizing the covariance of the other.
Similarly, Principal Components Analysis (PCA) [6,7], also applied
for spatial ﬁltering in BCI, is based on the estimation of covariance
matrices. Canonical Correlation Analysis (CCA) is another example
of a technique relying on covariance estimates successfully applied
on EEG for spatial ﬁltering [15,20]. Covariance matrices are also
found in classiﬁers such as the Linear Discriminant Analysis (LDA),
which is largely used in BCI. In all cases, they are handled as ele-
ments of an Euclidean space. However, being Symmetric and
Positive-Deﬁnite (SPD), covariance matrices lie on a subset of the
Euclidean space, with reduced dimensionality and speciﬁc prop-
erties, the Riemannian manifold. Considering covariance matrices
in their original space would reduce the search area for an opti-
mization problem [21,22]. As Riemannian manifolds inherently
deﬁne a metric, the distance between SPD matrices takes into
account the space where they lie on; approximating it to an
Euclidean space introduces inaccuracies and results in ill-
conditioned matrices.

56

E.K. Kalunga et al. / Neurocomputing 191 (2016) 55–68

Recently, studies have been done to consider covariance
matrices obtained from multichannel brain signals in their original
space [23–25]. Covariance matrices are the input features of the
BCI system and the classiﬁer algorithms rely on Riemannian metric
for partitioning the feature space. The authors propose building
speciﬁc covariance matrices in order to emphasize the spatial and
frequential information of the multichannel brain signals [25]. The
outcome of this approach is a simple processing tool chain, which
achieves state-of-the-art classiﬁcation performances.

This paper introduces an online version of the minimum dis-
tance to Riemannian mean (MDRM) algorithm [23], with an
application to Steady-State Visually Evoked Potential (SSVEP) sig-
nals. In SSVEP, the subjects concentrate on stimuli blinking at ﬁxed
frequencies. Depending on the focus of their attention, brain waves
will arise with the same phase and frequency as the stimulus
chosen by the subject. The signals are recorded in an application of
assistive robotics,1 with a shared control scheme relying on an
SSVEP-based BCI and a 3D touchless interface based on IR-sensors
to operate an arm exoskeleton [26]. The long term objective is to
equip a home environment with assistive technologies, including
BCI, as proposed in [27,28]. In this context, it is important to
design an online system, i.e. that adapt continuously to the user's
brain signals, and asynchronous, i.e. that could be activated “on
demand”.

Our online implementation2 is similar to the unsupervised or
semi-unsupervised learning scheme proposed in [29,30]; that has
the potential of shortening (or even removing) the calibration
phase. We apply a similar approach to the dynamic stopping cri-
terion used in [31] to increase the speed of the BCI system. This
approach allows to dynamically determine the trial length and
ensure robustness in classiﬁcation results. Our MDRM approach
outperforms state-of-the-art algorithms in the ofﬂine setup.
Moreover, these state-of-the-art algorithms, that are based on
CCA, are inherently limited as they could not handle resting state.
They must rely on an external command to be turn on or off, and
are thus only suitable to lab environment.

When working with covariance matrices, a crucial point is to
correctly estimate the covariance when the number of samples is
small or heavily corrupted by noise. Several approaches have been
proposed to build the covariance matrices, relying on normal-
ization or regularization of the sample covariances. To assess the
quality of the covariance matrices obtained from EEG samples, a
comparative study of these estimators is conducted.

Hence, the contributions of this works are:

(cid:1) a comprehensive review of the literature on Riemannian geo-

metry applied to EEG and time-series,

(cid:1) a thorough analysis of the covariance estimators and their

impact on tools derived from information geometry,

(cid:1) ﬁrst online application of a Riemannian classiﬁcation algorithm

on SSVEP-based BCI,

(cid:1) introduction of a novel algorithm for online and asynchronous
BCI, including a resting state class, yielding better performance
than state-of-the-art SSVEP algorithms. No phase synchroniza-
tion is required for the SSVEP.

The paper is divided as follows: Section 2 reviews the state of
the art in SSVEP-based BCI and the applications of Riemannian
geometry in machine learning for BCI. Section 3 presents concepts
of Riemannian geometry relevant to this work and estimators of
covariance. In Section 4, the proposed classiﬁcation algorithm for

1 This dataset is freely available from https://github.com/sylvchev/dataset-

2 The open source code is available on https://github.com/emmanuelkalunga/

ssvep-exoskeleton.

Online-SSVEP.

online SSVEP is introduced and the experimental results are pre-
sented in Section 5 for ofﬂine and online setups as well as without
and with a resting state class.

2. State of the art

2.1. Steady-state visually evoked potential

Sensory evoked potentials often oppose Event Related Potential
(ERP) and Steady-State Response (SSR) [32]. This distinction ori-
ginates from the idea that the SSR may be generated by neural
oscillations elicited by the repeated stimulations [33] whereas the
ERP is the transient response to an event occurring at sufﬁciently
long time interval to allow the system to return to its initial state
[34]. We will focus on the visual SSR, called SSVEP and its appli-
cation to BCI.

The SSVEP-based BCI is often employed as a dependent BCI
[35], that is, some residual muscular capabilities are required to
move the eye toward the blinking stimulus as opposed to inde-
pendent BCI, such as Motor Imagery (MI), where the commu-
nication does not rely on any motor capability. It has been shown
that SSVEP could be used as an independent BCI [36,37] as the
brain oscillations are strongly related to the focus of attention.
Using covert attention, i.e. shifting the focus of attention without
moving the eyes, subjects can generate different SSVEP responses.
BCI have highly variable subject-speciﬁc performances. 20–30%
of the subjects cannot operate correctly brain interfaces. This
phenomenon is referred to as BCI illiteracy [9–11]. It affects SSVEP-
based BCI and it is correlated with age and gender, male subjects
being more afﬂicted than female ones [38]. Ofﬂine BCI, that is
approaches where the learning algorithms are trained on a large
dataset of subject's EEG recording, are also afﬂicted which indicate
that a source of variability at the subject level is not handled
correctly by the existing approaches. BCI illiteracy is also afﬂicting
online approaches, where the algorithms are adapted to the sub-
ject's EEG as the experiment goes by.

Visual stimulus plays a crucial role, affecting the BCI perfor-
mance, and should be designed carefully. An in-depth review of
the literature [39] shows that LED stimuli provide better results
than those obtained on computer screen. A cognitive study [40]
indicates that any stimulation between 2 and 50 Hz induces visible
oscillations in the visual cortex. Another study shows that a peak
in signal to noise ratio is visible at around 15 Hz [41]. Common
values employed in SSVEP studies are between 12 and 25 Hz, as
they induce oscillations with higher amplitudes [39]. One should
note that safety of the subject should be taken into account as
some frequency ranges of the stimulation train could trigger epi-
leptic seizure [42].

The phase of the stimulation signal can also be modulated,
enhancing the BCI performance by boosting the Information
Transfer Rate (ITR) [43,44]. An important constraint in that case is
that the experimental setup requires a synchronization between
the display and the recording system, to ensure the correct esti-
mation of the stimulus' phase. Better alternatives are available
when considering systems with such constraints: code-modulated
VEP (c-VEP) has yield the highest ITR in BCI [45,46]. In c-VEP, the
sole difference is that the stimulus ﬂickering is based on pseu-
dorandom sequences instead of the ﬁxed frequencies of SSVEP. All
these successful approaches in SSVEP and c-VEP rely on CCA. Given
two sets of signals, CCA aims at ﬁnding the projection space that
maximizes their cross-covariance while jointly minimizing their
covariance [20,15,44]. The common methodology is to ﬁnd the
canonical space between the multichannel EEG trial on the one
hand and reference signals, usually sine and cosine of target fre-
quencies and harmonics, on the other hand.

E.K. Kalunga et al. / Neurocomputing 191 (2016) 55–68

57

This study is part of our efforts to conceive a smart and adapted
environment for people with disabilities, with a standing point
similar to [27,28]. In our case, the device generating ﬂickering
stimulus should not be connected to the EEG processing system, to
allow, for example, the design of “smart switches” distributed in
the home environment in further studies. Hence, we could not rely
on phase-dependent
or phase-
modulated SSVEP.

such as

setups,

c-VEP

Another requirement of our system is to be self-paced, a
property also called asynchronous, to provide the user with the
ability to use the system “on demand”, i.e. when needed. As
pointed out by [47], it is thus necessary to provide a “no-control”
state to cope with situation where the user does not want to
produce any command. Several methods could be considered, such
as including a reject threshold in the system, as in [47], or directly
provide the system with a reject class, see for example [48]. This
“no-control” state or “resting” state is not always included in the
existing studies, see for example [27,20,44]. The high ITR obtained
with these systems are thus conﬁned to the lab environment and
could not be directly applied to realistic assistive scenarios.

2.2. Riemannian geometry in BCI

tools

Information geometry provides useful

for various
machine learning and optimization problems. In machine learning,
SPD matrices have been used in various applications where fea-
tures and data are only considered in the Euclidean space. Indeed,
covariance matrices lie in the space of SPD matrices which is a
subset of the Euclidean space when considered with the scalar
product. But the same space of SPD matrices, endowed with a
differential structure, induces a Riemannian manifold.

Riemannian geometry can improve machine learning algo-
rithms, taking explicitly into consideration the underlying struc-
ture of the considered space. Three kinds of approaches in the
literature use the geometry of data in machine learning. The ﬁrst
one relies on the mapping of the Riemannian manifold onto an
Euclidean vector space. One such mapping, called logarithmic
mapping, exists between the manifold and its tangent space,
which is an Euclidean space, and has been used in classiﬁcation
task for BCI [24]. Some kernels have been applied successfully to
this end: Stein kernel, Log-Euclidean kernels as well as their nor-
malized versions [49]. The main idea is to map the input data to a
high dimensional feature space, providing a rich and hopefully
linearly separable representation. The so-called kernel trick is to
provide a kernel function, which computes an inner product in the
feature space directly from points lying in the input space, deﬁning
a Reproducing Kernel Hilbert Space (RKHS). The family of kernels
deﬁned on the Riemannian manifold allows the implementation of
extensions of all kernel-based methods, such as SVM, kernel-PCA
or kernel k-means [50]. Apart from the kernel approaches, once
the data are mapped onto a vector space, any machine learning
algorithm working in Euclidean space, such as LDA, could be
applied [23].

A second kind of machine learning approach exploits the
underlying geometry of the data. Instead of mapping the data to
an Euclidean space, either a tangent space or an RKHS, the algo-
rithms are adapted to Riemannian space. For instance, sparse
coding algorithm has been adapted to Riemannian manifold, using
the geodesic distance to estimate the data point and its sparse
estimate [51]. Similarly nonlinear dimensionality reduction tech-
niques have been adapted to Riemannian manifold, such as
Laplacian Eigenmaps (LE), Locally Linear Embedding (LLE), and
Hessian LLE. This adaptation was used to cluster data using their
probability density functions (pdf) [52] or covariance matrices [53]
as features. Another example is the adaptation of interpolation and
ﬁltering of data to Riemannian space performed in [54], where an

afﬁne-invariant Riemannian metric is also proposed to offer a
geodesically complete manifold i.e. a manifold with no edge and
no singular point that can be reached in ﬁnite time.

In the last kind of approach, instead of adapting existing algo-
rithm from Euclidean to Riemannian geometry, new algorithms
are developed directly for Riemannian manifolds. The minimum
distance to Riemannian mean (MDRM) relies on a Riemannian
metric to implement a multi-class classiﬁer and have been applied
on EEG. New EEG trials are assigned to the class whose average
covariance matrix is the closest to the trial covariance matrix [23].
The MDRM classiﬁcation can be preceded by a ﬁltering of covar-
iance matrices, like in [55] where covariance matrices are ﬁltered
with LDA component in the tangent space, then brought back to
the Riemannian space for classiﬁcation with MDRM. Another
example is the Riemannian Potato [56], an unsupervised and
adaptive artifact detection method, providing an online adaptive
EEG ﬁltering (i.e. outliers removal). Incoming signals are rejected if
their covariance matrix lies beyond a predeﬁned z-score, com-
puted from a sliding window. With the same objective of achiev-
ing robustness to noise that affects covariance matrices, Rie-
mannian geometry is used to solve divergence functions of pdfs
[57]. This allows to reformulate the CSP as the maximization of the
divergence between the distributions of data from two different
classes corresponding to two cognitive states [58,59]. Using the
beta divergence the obtained CSP is robust to outliers in sample
covariance matrices and this algorithm is successfully applied to
EEG ﬁltering for BCI. Riemannian metrics are also used for EEG
channel selection [60] and the selection of the most discriminatory
spatial ﬁlters in CSP [61].

In MI experiment, the subject is asked to imagine a movement
(usually hand, feet or tongue), generating Event-Related Syn-
chronization and Desynchronization (ERD/ERS) in pre-motor brain
area. Riemannian BCI is well suited for MI experiment as the
spatial
information linked with synchronization is directly
embedded in covariance matrices obtained from multichannel
recordings. However, for BCI that rely on Evoked Potential such as
SSVEP or Event Related Potential (ERP), as P300, both frequential
and temporal
information are needed; the spatial covariance
matrix does not contain these information. To apply Riemannian
geometry to SSVEP and ERP, the sample covariance matrices can
be deﬁned from a rearrangement of the recorded data. The rear-
rangement is done such that the temporal or frequency informa-
tion are captured [25]. With similar motivations, [62,63] deﬁned a
new Riemannian distance between SPD matrices that would take
into account a weighting factor on matrices. They use this new
distance as a dissimilarity between weighted matrices of power
spectral density to classify EEG into different sleep state by k-
nearest neighbors.

3. Covariance matrices and their geometry

This section presents some formal deﬁnitions for the informa-
tion geometry concepts used in this paper. The link with the
covariance matrices is explicated in Section 3.2, along with the
covariance estimators proposed in the literature.

3.1. Riemannian manifold

An m-dimensional manifold M is a Hausdorff space for which
every point has a neighborhood that is homeomorphic to an open
subset of Rm [64]. When a tangent space is deﬁned at each point,
M is called a differential manifold. A geodesic γ is the shortest
smooth curve between two points, Σ1 and Σ2. The tangent space
T ΣM at point Σ is the vector space spanned by the tangent vectors
of all geodesics on M passing through Σ. A Riemannian manifold is

58

E.K. Kalunga et al. / Neurocomputing 191 (2016) 55–68

a manifold endowed with an inner product deﬁned on the tangent
space, which varies smoothly from point to point.

For the rest of this paper, we will restrict to the analysis of the
manifold MC of the C (cid:3) C symmetric positive deﬁnite matrices,
deﬁned as:
MC ¼ Σ A RC(cid:3)C : Σ ¼ Σ > and x > Σx4 0; 8 x A RC ⧹0

n

o

:

The tangent space T ΣMC is identiﬁed to the Euclidean space of
symmetric matrices:
SC ¼ Θ A RC(cid:3)C : Θ ¼ Θ >

n

o

:

The dimension of the manifold MC , and its tangent space T ΣMC , is
m ¼ CðC þ 1Þ=2.

The mapping from a point Θi of the tangent space to the
manifold is called the exponential mapping ExpΣ ðΘiÞ: T ΣMC-
MC and is deﬁned as:
2ExpðΣ (cid:4) 1
ExpΣ ðΘiÞ ¼ Σ 1

2ΘiΣ (cid:4) 1

2ÞΣ 1
2:

ð1Þ

ð2Þ

2ΣiΣ (cid:4) 1

2Log ðΣ (cid:4) 1

Its inverse mapping, from the manifold to the tangent space is the
logarithmic mapping Log Σ ðΣiÞ: MC-TΣMC and is deﬁned as:
2ÞΣ 1
Log Σ ðΣiÞ ¼ Σ 1
2:
Expð(cid:5)Þ and Log ð(cid:5)Þ are the matrix exponential and matrix logarithm,
respectively. The computation of these operators is straightfor-
ward for SPD matrices of MC . They are obtained from their
eigenvalue decomposition (EVD):
Σ ¼ U diagðλ1; …; λC ÞU > ;
ExpðΣÞ ¼ U diagðlog ðλ1Þ; …; log ðλC ÞÞU > ;
Log ðΣÞ ¼ U diagðexpðλ1Þ; …; expðλC ÞÞU > ;
where λ1; …; λC are the eigenvalues and U the matrix of eigen-
vectors of Σ. As any SPD matrix can be diagonalized with strictly
positive eigenvalues, Log ð(cid:5)Þ is always deﬁned. Similarly the square
root Σ 1

2 is obtained as:

Σ 1

2 ¼ U diagðλ1

; …; λ1

2

2
1

C ÞU > ;

and is unique. The same goes for Σ (cid:4) 1
2.

The tangent vector of the geodesic γðtÞ between Σ1 and Σ2,
(cid:2)(cid:2)(cid:2)!
where γð0Þ ¼ Σ1 and γð1Þ ¼ Σ2 is deﬁned as v ¼ Σ1Σ2
¼ Log
Σ1ðΣ2Þ. A Riemannian distance between Σ1 and Σ2 can thus be
deﬁned as [65]:

δðΣ1; Σ 2Þ ¼ J Log ðΣ (cid:4) 1

Σ2Þ J F ¼

1

log 2λc

;

#

1=2

"

XC

c ¼ 1

where λc, c ¼ 1; …; C, are the eigenvalues of Σ (cid:4) 1
Σ2. From Eq. (3),
the geometric mean of I points Σi on the manifold, i ¼ 1; …; I, can
be deﬁned as the point that minimizes the sum of squared dis-
tances to all Σi:

1

μðΣ1; …; ΣIÞ ¼ arg min
Σ A MC

XI

i ¼ 1

δ2ðΣi; ΣÞ:

ð3Þ

ð4Þ

This mean has no closed form, and can be computed iteratively
[66].

3.2. Covariance matrix estimation

Let xn A RC, n ¼ 1; …; N, denotes a sample of a multichannel EEG
trial recorded on C electrodes. N is the trial length. Let X A RC(cid:3)N be
the EEG trial such as X ¼ ½x1; …; xN(cid:6). Under the hypothesis that all
N samples xn are randomly drawn from a distribution, it follows
that x is a variable of random vectors and its expected vector is
ω ¼ Efxg [67]. The covariance matrix of the random variable x is
deﬁned by Σ ¼ Efðx (cid:4)ωÞðx (cid:4)ωÞ > g and is unknown,
thus an

^Σ should be computed. The choice of the appropriate
estimate
estimator is crucial to verify that the obtained covariance matrices
fulﬁll the following properties: they should be accurate, SPD, and
well-conditioned. The last property requires that
the ratio
between the maximum and minimum singular value is not too
large. Moreover, to ensure the computational stability of the
algorithm, the estimator should provide full-rank matrices, and its
inversion should not amplify estimation errors.

3.2.1. Sample covariance matrix estimator

The most usual estimator is the empirical sample covariance

matrix (SCM), deﬁned as:

^Σ scm ¼

1
N (cid:4) 1

XN

n ¼ 1

ðxn (cid:4) xÞðxn (cid:4) xÞ > ¼

X IN (cid:4)

1N1 >
N

X > ;

ð5Þ

(cid:3)

(cid:4)

1
N (cid:4) 1

1
N

P

N
where x A RC is the sample mean vector x ¼ 1
n ¼ 1 xn. In the
N
matrix notation, IN is the N (cid:3) N identity matrix and 1N is the
vector ½1; …; 1(cid:6). The SCM is often normalized [67] as:

^Σ nscm ¼

C
N

XN

n ¼ 1

ðxn (cid:4) xÞðxn (cid:4) xÞ >
σ2
xn

;

ð6Þ

time ndeﬁned as σ2
with the inter-channel variance at
xn
ðxn (cid:4) xÞ > ðxn (cid:4) xÞ. Other normalization techniques could be used.

¼

This estimation is fast and computationally simple. However
when C (cid:7) N, the SCM is not a good estimator of the true covar-
iance. In the case C 4 N, the SCM is not even full rank.

3.2.2. Shrinkage covariance matrix estimators

To overcome the shortcomings of SCM, the shrinkage estima-
tors have been developed as a weighted combination of the SCM
and a target covariance matrix, which is often chosen to be close to
the identity matrix, i.e. resulting from almost independent vari-
ables of unit variance.
^Σ
shrink ¼ κΓ þ ð1 (cid:4)κÞ
ð7Þ
where 0 rκ o 1. This estimator provides a regularized covariance
^Σ scm for small sample size, that is
that outperforms the empirical
C (cid:7) N. The shrinkage estimator has the same eigenvectors as the
SCM, but the extreme eigenvalues are modiﬁed i.e. the estimator is
shrunk or elongated toward the average.

^Σ scm;

The different shrinkage estimators differ in their deﬁnition of

the target covariance matrix Γ. Ledoit and Wolf [68] (
^Σ scmÞ. Blankertz
in Fig. 1) have proposed Γ ¼ vIC , with v ¼ Trð
. Schäfer
[69] (
^Σ shrink_schaf Þ proposes several ways of deﬁning Γdepending on the
ð
observed

shrink_blank) deﬁnes Γalso as vIC but with v ¼ Trð

^Σ scm [70].

^Σ scmÞ
C

shrink_ledoit

^Σ

^Σ

Fig. 1. Comparison of covariance estimators in terms of classiﬁcation accuracy
obtained with MDRM with increasing EEG trial length. For each trial length, the
average accuracy across all subjects and across all replications is shown. Bars
indicate the error of the mean, i.e. standard deviation divided by the square root of
n(cid:4) 1, n ¼ number of samples.

E.K. Kalunga et al. / Neurocomputing 191 (2016) 55–68

59

3.2.3. Fixed point covariance matrix estimator

The Fixed Point Covariance Matrix [71] is based on the max-
imum likelihood estimator ^ℓ which is a solution to the following
equation:

 

XN

^Σ fp ¼ ^ℓ ¼

C
N

ðxn (cid:4) xÞðxn (cid:4) xÞ >

n ¼ 1

ðxn (cid:4) xÞ > ^ℓ (cid:4) 1

ðxn (cid:4) xÞ

!

:

ð8Þ

As there is no closed form expression to Eq. (8), it can be written as
a function of ^ℓ : gð ^ℓÞ ¼
, where
, which is a solution to Eq. (8). Using ^ℓ0≔ ^Σ nscm as the
gð ^ℓ n
initial value of ^ℓ, it is solved recursively as ^ℓt ⟶
t-1

^Σ fp. g admits a single ﬁxed point ^ℓn

Þ ¼ ^ℓ n

^ℓ n

.

4. Online adaptation of the Riemannian classiﬁer

Concerning Riemannian classiﬁcation of SSVEP, the ofﬂine
methodology is explained in [25]. In this paper, we propose an
online classiﬁer for SSVEP, composed of an ofﬂine training phase
and an online and asynchronous test phase. This analysis is per-
formed for each subject independently.

4.1. Ofﬂine Riemannian classiﬁcation

The proposed classiﬁer relies on the Minimum Distance to
Riemannian Mean (MDRM) introduced in [55] and extended in
[25,72] for possible ofﬂine applications on SSVEP signals. Let us
consider an experimental SSVEP setup with F stimulus blinking at
F different frequencies. It is a multiclass classiﬁcation with K ¼ F
þ 1 classes: one class per stimulus and one resting state class. The
covariance matrices are estimated from a modiﬁed version of the
input signal X:

X A RC(cid:3)N-

7
5 A RFC(cid:3)N;

3

2

6
4

Xfreq1
⋮

XfreqF

is the input signal X band-pass ﬁltered around fre-
where Xfreqf
^Σ
quency freqf , f ¼ 1; …; F. Thus the resulting covariance matrix
belongs to MFC . Henceforth, all EEG signals will be considered as
ﬁltered and modiﬁed by Eq. (9).

From I labelled training trials Xif

gI
i ¼ 1 recorded per subject, K
centers of class ΣðkÞ
μ are estimated using Algorithm 1. When an
unlabelled test trial Y is given, it is classiﬁed as belonging to the
class whose center ΣðkÞ
is the closest to the trial's covariance
μ
matrix (Algorithm 2, step 2).

Algorithm 1. Ofﬁne estimation of Riemannian centers of classes.

Inputs: Xi A RFC(cid:3)N, for i ¼ 1; …; I, a set of labelled trials.
Inputs: I ðkÞ, a set of indices of trials belonging to class k.
Output: ΣðkÞ

μ , k ¼ 1; …; K, centers of classes.
^Σ i of Xi

Compute covariance matrices
for k ¼1 to K do

^Σ i : i A I ðkÞÞ, Eq. (4)

1:
2:
3: ΣðkÞ
end
4:
return ΣðkÞ
μ

μ ¼ μð

5:

Algorithm 2. Minimum distance to Riemannian mean.

μ , K centers of classes from Algorithm 1.

Inputs: ΣðkÞ
Input: Y A RFC(cid:3)N, an unlabelled test trial.
Output: k

, the predicted label of Y.

n

1:

2:

Compute covariance matrix

n

k

¼ arg minkδð

^Σ ; ΣðkÞ
μ Þ

^Σ of Y

3:

return k

n

4.2. Curve-based online classiﬁcation

In ofﬂine synchronous BCI paradigm, cue onset are used as
reference for the localization of a brain response, e.g. an evoked
potential. Nonetheless most of the BCI applications are online and
asynchronous; cue onsets are not known, thus designing online
version of BCI algorithms is not a trivial task. The approach
introduced here identiﬁes a period (i.e. time interval) in the online
EEG χ A RFC(cid:3)N , where N is the number of recorded samples,
associated with a high probability (above threshold) of observing
an SSVEP at a speciﬁc frequency, as illustrated in Algorithm 3.

Algorithm 3. Curve-based online classiﬁcation.

Inputs: hyper-parameters w; Δn; D; and ϑ.
Inputs: ΣðkÞ

μ , k ¼ 1; …; K, centers of classes from Algorithm 1

(ofﬂine training).

Inputs: Online EEG recording χðnÞ.
~
kðnÞ, online predicted class.
Output:
1:
2:
3:
4:
5:

d ¼ 1
for n ¼ w to N step Δn
Epoch Xd, Eq. (10), and classify it with Algorithm 2
if dZ D

Find the most recurrent class in K ¼ k

n
j A J ðdÞ:

k ¼ arg maxk

ρðkÞ, Eq. (11)

6:

7:

8:

if ρðkÞ 4ϑ
~δ
Compute
o 0

~δ

if

k , Eq. (12)

k
return

~
k ¼ k

ð9Þ

9:
10:
11:
12:
13:
14: end

end
end

end
d ¼ d þ 1

(cid:5)

To locate this interval, we focus on the last D recorded EEG
(cid:6)
overlapping epochs Xj A RFC(cid:3)w
j A J ðdÞ, with the set of indices
J ðdÞ ¼ d (cid:4) D þ 1; …; d (cid:4) 1; d; where d is the index of the current
epoch Xd in the online recording χðnÞ. Epochs have size w, and the
interval between two consecutive epochs is Δn, with w 4Δn:
Xd ¼ χðn (cid:4) w; …; nÞ:
ð10Þ
To obtain the ﬁrst D epochs Xj A J ðdÞ, at least w þ ðD (cid:4) 1ÞΔn samples
of χ should be recorded (step 4).
The classiﬁcation outputs k

n
j A J ðdÞ obtained in step 3 by applying
Algorithm 2 on Xj A J ðdÞ are stored in a vector K, which always
contains the latest D classiﬁcation outputs. The class that occurs
the most in K (step 5), with an occurrence probability ρðkÞ above a
deﬁned threshold ϑ, is considered to be the class, denoted k, of the
ongoing EEG recording χðnÞ. The vector ρ is deﬁned as:

n
#fk
j A J ðdÞ ¼ kg

ρðkÞ ¼

D

;

for k ¼ 1; …; K;

ð11Þ

ρðkÞ; then ρðkÞ is compared to the threshold ϑ.
with k ¼ arg maxk
If ϑ is not reached within the last D epochs, the classiﬁcation
output is held back, and the sliding process continues until ϑ is
reached. In the last D epochs, once a class k has been identiﬁed, a
curve direction criterion is introduced to enforce the robustness of
the result. For class k to be validated, this criterion requires that
the direction taken by the displacement of covariance matrices

60

E.K. Kalunga et al. / Neurocomputing 191 (2016) 55–68

j A J ðdÞ be toward the center of class ΣðkÞ

^Σ
k , the sum of
gradients (i.e. differentials) of the curve made by distances from
^Σ j A J ðdÞ to ΣðkÞ
Δδ
X
Δj

μ should be negative (step 8):

ðj (cid:4) 1Þ o 0 with

μ . Hence

ðjÞ (cid:4)δ

Xd

ðjÞ

¼

¼

~δ

δ

k

k

k

k

~δ

j A J ðdÞ

j ¼ d (cid:4) D þ 2

In this work, we propose a slightly different application of the
Riemannian potato where the outliers are removed per class.
are deﬁned
Hence
n
Σk

for K class, K Riemannian potatoes

. Since Riemannian distances to geometric mean

μ; μk; σk

o

K

do not have a Gaussian distribution, we make use of the geometric
mean for μ, the geometric standard deviation for σ and the geo-
metric z-score. They are deﬁned as follows [72]:

k ¼ 1

δ

k

ðjÞ ¼

P

δð
K
k ¼ 1

^Σ j; ΣðkÞ
μ Þ
^Σ j; ΣðkÞ
δð
μ Þ

:

The occurrence criterion is inspired by the dynamic stopping of
[31]; there is no ﬁxed trial length for classiﬁcation. The occurrence
criterion ensures that the detected user intention is unaffected by
any short time disturbances due to noise or subject's inattention,
as presented in Algorithm 3. This approach offers a good com-
promise to obtain robust results within a short and ﬂexible time.
The curve direction criterion solves both the problems of
latency in the EEG synchronization and of the delays inserted by
the EEG epochs processing. Indeed, some EEG epochs gather sig-
nals from different classes and might be wrongfully classiﬁed if the
decision is solely based on the distance with the center of the
class. This situation and the effect of the curve direction criterion
are well shown in Section 5.4. Ensuring that the covariance
matrices are displaced toward the center of the detected class
provides a guarantee that it matches with the current EEG state.
Inversely, if the direction of the curve is moving away from the
center of the detected class, it might indicate that there has been a
change in the EEG state that has not been detected.

Algorithm 3 has 4 hyperparameters: w; Δn; D, and ϑ. The values
of w; D, and ϑ are set through cross validation and are given in
Section 5.4. Although a large window size w is expected to
increase the classiﬁcation accuracy, it increases the response time,
thus reducing the time resolution, and extends the overlap
between different EEG states. The step size Δn should be set to a
minimum value to allow a maximum number of overlapping
epochs (D) within a short time. However, it should be large enough
to avoid too many calculations within a time interval with small or
inexistent changes in EEG states. If the number of epoch D is too
small, the classiﬁcation will be sensitive to non-intentional and
abrupt changes in the EEG. A too large D will
increase the
momentum and reinforce the inﬂuence of the past EEG signals. It
should also be mentioned that both the occurrence and the curve
direction criteria cannot have a signiﬁcant impact if the value of D
is too small. The probability threshold parameter ϑ acts like a
rejection parameter: high ϑ values correspond to a high
rejection rate.

4.3. Outliers removal with Riemannian potato

Outliers in the training data might affect the Riemannian mean
of classes in the MDRM classiﬁcation scheme. To alleviate this
effect, an approach called the Riemannian potato, introduced in
[56], is exploited. In this approach, all trials are represented by
their covariance matrices Σi. A reference covariance matrix is
estimated, e.g. Riemannian mean of all trials Σμ. The Riemannian
distances δi between each Σi and Σμ are computed. Any trial that
lies too far, i.e. beyond a certain threshold, from the reference
matrix Σμ in terms of Riemannian distance is rejected. In [56], the
distance z-score thresholding is deﬁned as:

zðδiÞ ¼

δi (cid:4)μ

σ 4 zth

ð13Þ

where μ and σ are respectively the mean and standard deviation of
(cid:5) (cid:6)
i ¼ 1. In other words, any trial Σi whose z-score zðδiÞ
distances δi
I
is larger than the threshold zth ¼ 2:5 is rejected.

ð12Þ

μk ¼ exp

lnðδk
i Þ

!

X

 

1
I
s 

(cid:8)

X

i
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
(cid:9)
(cid:8)
1
2
ln δk
I

=μk

(cid:9)

i

!

σk ¼ exp

(cid:8)
ln δk

i
=μk
lnðσkÞ

i

(cid:9)

:

zðδk

i Þ ¼

ð14Þ

Through cross-validation, the z-score threshold is set to zth ¼ 2:2.
Moreover, outliers are removed iteratively. Each time outliers
are rejected, a new center of class is computed and used as
reference for the next iteration. Iterations continue until con-
vergence, i.e. no more outlier found.

5. Experimental validation

Covariance matrix estimators, Algorithms 2 and 3 are applied
to SSVEP signals for ofﬂine and online analysis. This section pre-
sents the analysis and results obtained.

5.1. Data description

The signals are recorded from 12 subjects during an SSVEP
experiment. EEG are measured on C ¼ 8 channels: OZ, O1, O2, POZ,
PO3, PO4, PO7, and PO8. The ground and the reference electrodes
were placed respectively on FZ and the right hear mastoid
respectively. The acquisition rate is T s ¼ 256 Hz on a gTec MobiLab
Amp (gTec, Graz, Austria). The subjects are presented with F ¼ 3
visual target stimuli blinking respectively at freq ¼ 13 Hz; 17 Hz
and 21 Hz. It is a K ¼ 4 classes BCI setup made of the F ¼ 3 stimulus
classes and one resting class (no-SSVEP). In a session, which lasts
5 min, 32 trials are recorded: 8 for each visual stimulus and 8 for
the resting class. The number of sessions recorded per subject
varies from 2 to 5. Thus the longest EEG recorded for a single
subject is 25 min or 160 trials. The trial length is 6 s, that is N ¼ 6 (cid:3)
T s ¼ 1536 samples. Since data are rearranged as detailed in (9),
trials X A RFC(cid:3)N, where FC ¼ 24 corresponding to 8 channel times
3 stimulus frequencies. For each subject, a test set is made of 32
trials whereas the remaining trials (which might vary from 32 to
128) make up for the training set.

5.2. Covariance estimators comparison

In this section, the effectiveness of covariance matrix estima-
tors is evaluated for SSVEP signals. The evaluation is done in terms
of classiﬁcation accuracy and integrated discrimination improve-
ment (IDI), obtained by each estimator (see Section 3.2) with
respect to SCM estimator while using the ofﬂine MDRM classiﬁer.
The different conditioning of covariance matrices are also
investigated.

A bootstrapping with 1000 replications is performed to assess
the performances of each estimator. Estimators are compared on
10 trial lengths t A f0:5; 1:0; …; 5:0g s, as these are known to affect
the estimators performance. Here N A f128; 256; …; 1280g is com-
puted as N ¼ t (cid:3) T s.

Fig. 1 shows the classiﬁcation accuracies of each estimator
computed across all subjects. Even if the error bars show an

important inter-subject variability, the increase in the accuracy can
be attributed to the fact that the relevant patterns in EEG accu-
mulate with the trial length, producing better estimation of the
covariance matrices. This is known to be particularly true for the
SCM estimator and it could be seen in Fig. 1. It appears that
shrinkage estimators (especially Ledoit and Schäfer) are less
affected by the reduction of epoch sizes than the other estimators.
This is a direct consequence of the regularization between the
sample covariance matrices and the targeted (expected) covar-
iance matrix of independent variables.

For computational purposes, it is important to look at the
matrix conditioning. Fig. 2 shows the ratio C between the largest
and smallest eigenvalues: in well-conditioned matrices, C is small.
Shrinkage estimators offer better conditioned matrices whereas
the SCM, NSCM, and Fixed Point matrices are ill-conditioned
below 2 s of trial length, and may result in singular matrices.

In Fig. 2b, the Integrated Discrimination Improvement (IDI), as
deﬁned in [73], is computed for the different estimators and trial
lengths. The SCM is used as a reference for improvement, as this is
the most popular estimator in the literature. Negative IDI means a
deterioration in the method discrimination ability. It is clear that
shrinkage estimators increase the discrimination power of the
classiﬁer. However, despite being more complex than the SCM, the

E.K. Kalunga et al. / Neurocomputing 191 (2016) 55–68

61

NSCM and the Fixed Point estimators decrease the discrimination
ability of classiﬁers. From Figs. 1 and 2b, it is apparent that the
difference in performance between the SCM and shrinkage esti-
mators reduces as the trial length increases. The simplicity of the
SCM plays a favorable role: it is an attractive method for longer
trials. The p-values under the hypothesis that
there is no
improvement (i.e. IDI ¼0) from one estimator to another are all
inferior to 10 (cid:4) 47, (po 10 (cid:4) 3 indicating a statistically signiﬁcant
discriminatory improvement); hence the improvement is sig-
niﬁcant. It should be noted that the estimation of covariance
matrices is a trade-off between the quality of the estimate and the
computation time required; this should be considered for real-
time processing.

5.3. Effect of outliers on center estimations

Outliers can affect the ofﬂine training of the K centers of class
ΣðkÞ
μ by Algorithm 1, which is crucial for the evaluation phase and
online application. Fig. 3 shows representations of training cov-
ariance matrices Σi in the tangent space ðΘiÞ, projected at the
mean of all training trials, for the subjects with the lowest (Fig. 3a
and b) and the highest (Fig. 3c and d) BCI performance. To obtain
this visualization, the ﬁrst two principal components of a PCA
(cid:5) (cid:6)
applied on Θi
I
i ¼ 1 are selected. In Fig. 3b and d, the Riemannian
potato presented in Section 4.3 is applied; outliers in each class are
removed. The interest of using a Riemannian potato is well seen in
Fig. 3a and b. In Fig. 3a, the outliers are so distant from the rest of
the class matrices that the center of class is stretched away.
Applying a Riemannian potato removes the outliers, and the
center of class is better estimated (Fig. 3b).

When training trials are not noisy, their covariance matrices are
compact around their Riemannian mean. In this case the removal
of outliers by the Riemannian potato does not inﬂuence, at least
not signiﬁcantly, the Riemannian mean. This is the case in Fig. 3c
and d. Thus, applying the Riemannian potato is crucial for noisy
data and will have a limited effect on clean data. The impact of the
Riemannian potato on the classiﬁcation accuracy is discussed in
Section 5.4.

5.4. Classiﬁcation results and analysis

In this section, the performance of the proposed method is
presented. First, the performance of the MDRM approach in an
ofﬂine setup is analyzed, then the results of the online algorithm
are presented. In the ofﬂine analysis, the relevance of identifying
the latency between cue onset and SSVEP response is shown. The
results of the MDRM approach are compared to two state-of-the-
art methods [20,44]. The online evaluation is divided into two
parts: in the ﬁrst one the algorithm discriminates between K ¼ F
¼ 3 SSVEP classes (i.e. 13, 17 and 21 Hz) and in the second one is
applied on K ¼ 4 classes,
i.e. the F ¼ 3 SSVEP class and the
resting class.

5.4.1. Ofﬂine analysis

A close inspection of the ﬁltered signals shows that almost all
signals are synchronized with the trial frequency 2 s after cue
onset τ0 ¼ 0, as shown in Fig. 4. This delay is mainly due to pro-
tocol design and user speciﬁc cognitive processes. The protocol is
aimed to provide an asynchronous setup close to real application.
The user are not required to look at a ﬁxation point or to directly
gaze toward the target, as in [74,44], during inter-trial periods.
This is a tentative explanation for the higher delay observed in our
study and it is consistent with literature observations [75,76]. In
fact, before τ0 þ 2 s, for some users the signal could still be syn-
frequencies. An important
chronized with the previous trial

Fig. 2. (a) Covariance matrices condition expressed as the ratio C between largest
and smallest eigenvalues for the different covariance estimators. The comparison is
done for increasing EEG trial length. (b) Integrated discrimination improvement
brought to the classiﬁcation task by various estimators along varying trail length.
The indicated IDI values are multiplied by 102. ^Σ scm is used as a baseline.

62

E.K. Kalunga et al. / Neurocomputing 191 (2016) 55–68

Fig. 3. Scatter plot of covariance matrices for all trials mapped on the tangent space. The distance between each trial covariance matrix Σi and its Riemannian mean class ΣðkÞ
μ
is shown as connection line. The black/thick star represents the Riemannian mean of all trials. Matrices of resting class, 13Hz class, 21Hz class, and 17Hz class are represented
with starts in black, grey, dark grey, and light grey respectively. Subject with lowest BCI performance, (3a) before and (3b) after Riemannian potato ﬁltering. Subject with
highest BCI performance, (3c) before and (3d) after Riemannian potato ﬁltering.

Fig. 4. Signal amplitude at each stimulus frequency, showing synchronization of EEG with respect to time (seconds). The raw signal of the trial measured on Oz is band
ﬁltered using a Butterworth of order 8 at each stimulus frequency and the resulting signals are shown in blue (light grey), green (grey), and red (dark grey) for the same
signal ﬁltered respectively at 13, 17, and 21 Hz. The cue onset τ0 at time 0 on the x-axis is shown with a vertical discontinued line. 4 trials are shown, one for each class.
Signals are from the subjects with the highest (4a) and with the lowest BCI performance (4b). (For interpretation of the references to color in this ﬁgure caption, the reader is
referred to the web version of this paper.)

increase in average classiﬁcation accuracy (almost 10%) could be
obtained by taking the trial from 2 s after cue onset. It is therefore
crucial to consider the latency between the cue onset of trial and
the actual synchronization of SSVEP at stimulus frequency. Thus in

the ofﬂine synchronous processing, the conﬁdent window for
classiﬁcation is set 2 s after the cue onset ðτ0 þ 2Þ.

Table 1 shows the ofﬂine classiﬁcation accuracies for each
subject obtained by the application of the MDRM as described in

E.K. Kalunga et al. / Neurocomputing 191 (2016) 55–68

63

Table 1
Ofﬂine performance in terms of accuracy and ITR. Five methods are compared: (1) CCA approach introduced by Lin et al. [20], (2) CCA approach introduced by Nakanishi et al.
[44], (3) MDRM described in Section 4.1 (Algorithm 2), (4) MDRM where processed epochs are taken 2 s from the beginning of the trial, and (5) MDRM-Potato, where outliers
are removed using the Riemannian potato approach described in Section 4.3.

Ofﬂine algorithms

Lin et al. [20]

Nakanishi et al. [44]

MDRM ðτ0Þ

MDRM

MDRM-Potato

acc (%)

itr (bpm)

acc (%)

itr (bpm)

acc (%)

itr (bpm)

acc (%)

itr (bpm)

acc (%)

itr (bpm)

S1
S2
S3
S4
S5
S6
S7
S8
S9
S10
S11
S12

91.7
45.8
100.0
97.9
83.3
77.1
98.6
97.9
91.7
80.2
89.6
95.8

16.3
0.7
23.8
21.3
11.5
8.7
22.0
21.3
16.3
10.0
15.0
19.4

84.7
47.9
93.0
96.6
82.2
76.2
96.7
65.5
77.9
76.9
82.7
93.8

12.2
1.0
17.2
20.0
11.0
8.3
20.1
4.7
9.0
8.6
11.2
17.8

67.6
66.0
90.2
78.3
76.0
72.2
90.0
90.4
64.0
79.2
54.8
82.3

3.5
3.2
10.3
6.1
5.5
4.5
10.2
10.3
2.8
6.4
1.4
7.4

84.7
79.4
99.3
89.7
89.5
87.2
99.8
99.7
85.8
93.1
78.2
98.6

12.2
9.7
22.7
15.0
14.9
13.6
23.5
23.2
12.8
17.3
9.2
22.0

84.5
79.3
99.3
89.7
89.4
87.2
99.8
99.7
85.7
93.0
78.2
98.6

12.1
9.6
22.7
15.0
14.9
13.6
23.4
23.2
12.7
17.2
9.1
22.0

Mean

87.57 15.1

15.5 76.8

81.27 14.1

11.8 7 6.0

75.97 11.4

6.0 73.1

90.47 7.8

16.37 5.3

90.4 7 7.8

16.37 5.3

Fig. 5. Evaluation of the online algorithm parameters. 5a shows the decrease of the average classiﬁcation error over all subjects during the successive epochs after the
beginning of the trial. 5b is an example taken from the subject with the best performance showing how the probability of the actual class varies with epoch position from
beginning of trial. The groundtruth class probability is represented with a thick-and-star line, while other classes probability lines are thin-and-diamond. 5c shows the
variation of the average classiﬁcation error for different probability threshold ð0r ϑ o 1Þ and its inﬂuence on the classiﬁer output (Algorithm 3 step 6). 5d shows how the
average online performance varies with respect to the epoch size (w). It shows both the classiﬁcation accuracy (left y-axis, black curve) and the ITR (right y-axis, grey curve).
In 5a, 5c, and 5d, the bars represent the error of the mean i.e. standard deviation divided by the square root of n (cid:4) 1, n ¼ number of samples.

Algorithm 2, with the epochs taken at τ0 þ 2. Column MDRM ðτ0Þ
shows the results obtained when the epochs are taken from cue
onset. The Riemannian potato technique presented in Section 4.3

was applied for outliers removal (MDRM-Potato). The performance
of the MDRM approach is compared to two CCA-based state-of-
the-art methods proposed by Lin et al. [20] and Nakanishi et al.

64

E.K. Kalunga et al. / Neurocomputing 191 (2016) 55–68

[44] respectively. In the implementation of these methods, the
epochs are also taken from τ0 þ 2.

The MDRM approach outperforms both CCA-based method
with an average classiﬁcation accuracy of 90.4 77.8% and ITR of
16.37 5.3 bits/min. Lin et al. rank second with 87.57 15.1% and
15:5 7 6:8 bits=min. The method proposed by Nakanishi et al.,
which could be expected to achieve better results as reported in
[44], only ranks third. This is mainly due to the fact that this
method requires information on the phase of the stimuli. In fact,
Nakanishi et al. use the average of all training trials belonging to a
unique class as reference signal in the CCA. When SSVEP trials
belonging to a unique trial are not in-phase, which is the case in
the current work, averaging them will cancel the signal.

Within the MDRM approach, it is shown that taking into account
the latency between the cue onset and the SSVEP response sig-
niﬁcantly increases the classiﬁcation performances: accuracy and ITR
rises from 75.9711.4% and 6.073.1 bits/min to 90.477.8% and
16:3 7 5:3 bits=min. In turn removing outliers with the Riemannian
potato does not bring signiﬁcant change. This could be attributed to
the fact that the recording have been conducted in controlled envir-
onment, with small or little external noise.

5.4.2. Online analysis without resting class

In an online asynchronous experiment, there is no cue onset,
and the delay before SSVEP synchronization might differ from one
trial to another and from one subject to another. To locate the trust
EEG region for the classiﬁcation, D and ϑ are set respectively to
5 and 0.7 through cross-validation. The performance of this online
setup are analyzed and Fig. 5 shows the results. From the analysis
shown in Fig. 5d, the epoch size is set to w ¼ 2:6 s. The step size is
set to Δn ¼ 0:2 s, that is a new epoch is classiﬁed every 0.2 s.

In Fig. 5a, the classiﬁcation error is plotted against the epoch
index. It shows that the error decreases as epochs move from the
beginning of the trial. The error increases in the last epochs of the
trial, corresponding to the end of the SSVEP task. Fig. 5b details the
evolution of the probability for each class as epochs index
increases. It appears clearly that the class of the EEG trial (thick-
and-star line) has the largest probability only a few epochs after
the beginning of the trial. Moreover, one can see that this is an
increasing trend over the whole trial. Thus by setting an appro-
priate probability threshold ϑ, the actual class can be identiﬁed
with enough conﬁdence. Fig. 5c shows the inﬂuence of the prob-
ability threshold ϑ on the classiﬁcation error. The error is reduced
when the probability threshold ϑ is increased. Fig. 5d shows how

Fig. 6. Covariance matrices trajectory during a 4-class SSVEP online recording. The
circles represent class centers. The triangles mark the beginning in the experiment
of a new trial whose class is indicated by the triangle's color. Matrices of resting
class, 13Hz class, 21Hz class, and 17Hz class are represented in black, grey, dark
grey, and light grey respectively. 6a shows the ﬁrst 7 trials. The ﬁrst 3 trials are from
the resting class, the remaining are respectively class 13 Hz, 17 Hz, and 21 Hz. 6b
shows the entire recording. Data are taken from the subject with the highest BCI
performance.

Table 2
Classiﬁcation performances (accuracy in %, delay before valid and conﬁdent classiﬁcation in seconds, and ITR in bits/min) achieved using the online algorithm. The ﬁrst
column indicates the subjects. The following three columns show the results obtained without the curve direction criterion (Algorithm 3 up to 6): by stopping at step 6, k is
taken to be the valid class. The next three columns contain the results of the complete online algorithm. The last three columns report the results obtain when outliers are
removed in the training phase using the Riemannian potato technique described in Section 4.3.

Online ðρðkÞ 4ϑÞ

Online (full Algorithm 3)

Online-Potato

acc (%)

delay (s)

itr (bpm)

acc (%)

delay (s)

itr (bpm)

acc (%)

delay (s)

itr (bpm)

S1
S2
S3
S4
S5
S6
S7
S8
S9
S10
S11
S12

68.8
64.6
81.2
83.3
72.9
66.7
93.1
87.5
60.4
64.6
54.2
52.5

0.8
0.7
0.7
0.8
0.7
0.7
0.7
0.6
0.7
0.7
0.7
0.7

26.3
21.6
54.3
53.2
37.1
24.5
89.6
76.2
15.7
21.5
9.9
8.0

77.1
77.1
95.8
91.7
83.3
72.9
98.6
100.0
77.1
87.5
87.5
99.2

1.1
1.2
1.0
1.0
1.0
1.1
0.9
0.9
1.2
1.1
1.3
1.2

27.9
26.8
73.0
58.6
42.5
24.3
87.0
95.9
27.6
45.3
38.9
71.7

77.1
77.1
95.8
95.8
83.3
72.9
98.6
100.0
77.1
87.5
87.5
99.2

1.1
1.2
1.0
1.0
1.0
1.1
0.9
0.9
1.2
1.1
1.3
1.2

27.9
26.8
73.0
69.2
42.5
24.3
86.8
95.9
27.6
45.3
38.9
71.8

Mean

70.8713

0.77 0.0

36.57 26.3

87.37 9.8

1.1 7 0.1

51.6 725.1

87.77 10

1.17 0.1

52.57 25.5

E.K. Kalunga et al. / Neurocomputing 191 (2016) 55–68

65

Table 3
This table summarizes the performance achieved with the online algorithm with resting class identiﬁcation, as in Table 2.

Online ðρðkÞ 4ϑÞ

Online (full Algorithm3)

Online-Potato

acc (%)

delay (s)

itr (bpm)

acc (%)

delay (s)

itr (bpm)

acc (%)

delay (s)

itr (bpm)

S1
S2
S3
S4
S5
S6
S7
S8
S9
S10
S11
S12

67.2
78.1
89.1
75.0
71.9
87.5
84.4
85.9
67.2
62.5
59.4
69.4

0.7
0.7
0.8
0.7
0.7
0.8
0.7
0.8
0.7
0.7
0.8
0.7

37.6
59.0
85.2
52.2
46.7
80.2
76.3
76.4
37.2
30.3
23.5
44.8

71.4
75.0
89.1
75.0
70.3
87.3
85.4
89.1
75.0
69.5
68.8
93.8

1.1
1.0
1.0
0.9
1.1
1.1
1.0
1.0
1.0
1.0
1.1
1.0

32.4
39.2
67.6
42.9
31.0
58.7
62.5
68.1
39.6
32.0
29.1
79.4

71.4
75.0
89.1
75.0
70.3
87.3
88.5
89.1
76.6
69.5
68.8
93.8

1.1
1.0
1.0
0.9
1.1
1.1
1.0
1.0
1.1
1.0
1.1
1.0

32.4
39.2
67.6
43.4
31.0
58.7
69.1
68.1
40.3
32.0
29.1
79.9

Mean

74.87 10.2

0.77 0.0

54.1 721.0

79.17 9.1

1.0 70.1

48.6 717.6

79.57 9.3

1.07 0.1

49.27 18.2

Fig. 7. (a) Confusion matrix for K¼4 classes with Online-Potato. (b): ROC curve indicating the inﬂuence of the ϑ parameter.

the average online performance varies with respect to the epoch
size (w). Both the classiﬁcation accuracy and the ITR are shown.
With short w values, the epoch size does not capture enough
feature for a correct classiﬁcation, and with long w, the epoch
loses temporal resolution. The ITR increases with the classiﬁcation
rate but drops sensibly after a peak value.

The observation of Fig. 6 provides a visualization of the prin-
ciple guiding the online implementation of Eq. (12). This ﬁgure
shows the trajectory on the tangent space taken by covariance
matrices during a 4-class SSVEP experiment, and how they are
classiﬁed epoch by epoch. It can be seen (encircled in Fig. 6a) that
a change in the SSVEP stimulus might not be detected instanta-
neously by the classiﬁer. The trials are erroneously attributed with
conﬁdence to the previous class. The proposed online algorithm,
described in Algorithm 3, mitigates this issue and increases the
classiﬁcation accuracy as shown in Table 2. The “Online ðρðkÞ 4ϑÞ”
column shows the results of the online algorithm without the
curve direction criterion (i.e., without steps 6–11), and “Online
(full Algorithm 3)” shows the improvement brought by this cri-
terion. The performances are in terms of average classiﬁcation
accuracy (acc (%)), average time taken into the trial before classi-
ﬁcation (delay (s)), and the ITR (itr (bits/min)).

The curve direction criterion increases the rejection of epochs that
could be wrongly classiﬁed, it thus signiﬁcantly increases the classiﬁ-
cation accuracy of the online algorithm (70.8713% to 87.379.8%),
while increasing the delay (0.7–1.1 s) before classiﬁcation. When

compared to the state-of-the-art ofﬂine MDRM, the online curve-
based classiﬁcation yields better results in terms of ITR as the delay
before classiﬁcation is much shorter in the latter than the trial length
used in the former; classiﬁcation outputs are reached faster with the
online algorithm. Moreover, the online algorithm can be applied in
both synchronous and asynchronous paradigms, whereas the ofﬂine
algorithms are limited to synchronous paradigms which provide
strongly limited user interaction.

Last, the impact of the Riemannian potato is analyzed. A
bootstrapping with 50 replications was performed on the ofﬂine
data to assess the effect of applying the Riemannian potato. The
results show that for most subjects the results are unchanged
when the Riemannian potato is applied: due to the fact that data
are recorded in a controlled environment, most of them are thus
clean. It does however improve the results of few subjects. It was
then applied in the training phase of the online application, and a
similar observation is made. We can conclude that the Riemannian
potato can be used as a safety guard to ensure that the Riemannian
mean used in the MDRM classiﬁcation scheme is not affected by
outliers, especially for BCI used in less controlled environment.

5.4.3. Online analysis with resting class

Using the MDRM approach it is possible to identify the resting
class. In fact, covariance matrices of signal recorded during resting
periods can be characterized with their own Riemannian mean. As
such, they can be identiﬁed as any other class using the MDRM

66

E.K. Kalunga et al. / Neurocomputing 191 (2016) 55–68

approach. The state-of-the-art methods, Lin et al. [20] and Naka-
nishi et al. [44], are both based on CCA where a reference signal is
needed. These methods do not handle resting class, since there is
no reference signal for them. In this section, the performance of
the proposed approach including the identiﬁcation of the resting
class is presented. Table 3 summarizes the classiﬁer performance
in the same format as Table 2, in terms of classiﬁcation accuracy,
delay before valid classiﬁcation and ITR. Like in Table 2, the best
performance is achieved by the complete online algorithm pre-
ceded with outliers removal with the Riemannian potatoes (i.e.
Online-Potato). The identiﬁcation of the resting class induces a
drop of the overall classiﬁcation accuracy by 8.2%, and a drop of
ITR from 52.5 725.5 to 49.2718.2.

The effect of the resting class is seen with more details in Fig. 7.
Fig. 7a shows the classiﬁcation confusion matrix. There are few mis-
classiﬁcations between SSVEP classes compared to the misclassiﬁca-
tions between the resting class and any SSVEP class: the largest per-
centages are located in the ﬁrst row and the ﬁrst column, apart from
the diagonal block. Fig. 7b displays a ROC curve showing how the
classiﬁer performs in discriminating each class versus the others
depending on the value of the ϑ parameter. On this ROC curve, the
performance of the Online(cid:4)Potato algorithm are indicated in terms of
False Positive Rate (FPR) and True Positive Rate (TPR).

Conﬁrming the observation from the confusion matrix, the ROC
curve indicates that the resting is the most prone to false positive.
Despite the drop in performance, the identiﬁcation of resting class
is crucial for online BCI setup, allowing the subject to use the
system at his own pace.

6. Conclusion

This work investigated the efﬁciency of Riemannian geometry
when dealing with covariance matrices as classiﬁcation features. A
novel algorithm based on MDRM, enhanced by class probability
and the curve direction in the space of covariance EEG signals, was
introduced and applied on an SSVEP classiﬁcation task for a 4-class
brain computer interface. Existing covariance matrix estimators
were investigated and their robustness was assessed on multi-
channel SSVEP signals to ensure that the obtained matrices are
accurate estimates of data covariance, are well conditioned, and
verify the positive-deﬁniteness property. The Schäfer shrinkage
estimator was found to be the best as it yielded the highest clas-
siﬁcation accuracy with the MDRM algorithm.

The MDRM approach is ﬁrst analyzed in a ofﬂine classiﬁcation
setup. To prevent the effect of noisy signals on the MDRM
approach, outliers in the training set of are removed using a
modiﬁed version of the Riemannian potato. This approach is
compared to two CCA-based state-of-the-art methods. The results
show that ofﬂine MDRM achieves better classiﬁcation perfor-
mances than any of the CCA-based methods.

In the online setup, the proposed online algorithm enhances
the stability of the BCI system, balancing between classiﬁcation
speed and prediction accuracy. The evaluation of the classiﬁcation
conﬁdence over several epochs mitigates the short term pertur-
bations in the experimental conditions and the attentional varia-
tions of the subject. The curve direction overcomes the mis-
classiﬁcation of EEG trials that are still synchronized with past
stimuli frequencies at classiﬁcation time.

Unlike the CCA-based state-of-the-art methods considered in this
work, the proposed online algorithm is capable of identifying the
resting periods during an online EEG recording. These resting periods
are considered as an additional class in the classiﬁcation task.

All these contributions help to pave the way towards BCI used

in non-controlled, assistive environment.

Acknowledgement

The authors would like to thank Louis Mayaud from Mensia
Technologies for his contribution in discussions that led to the
completion of this work.

References

[1] J.R. Wolpaw, N. Birbaumer, D.J. McFarland, G. Pfurtscheller, T.M. Vaughan,
Brain–computer interfaces for communication and control, Clin. Neurophysiol.
113 (6) (2002) 767–791.

[2] J.J. Vidal, Toward direct brain–computer communication, Annu. Rev. Biophys.

Bioeng. 2 (1) (1973) 157–180.

[3] J.D. Bayliss, D.H. Ballard, Single trial P3 epoch recognition in a virtual envir-

onment, Neurocomputing 32–33 (2000) 637–642.

[4] W. Tu, S. Sun, A subject transfer framework for EEG classiﬁcation, Neuro-

computing 82 (2012) 109–116.

[5] E. Niedermeyer, F. Lopes da Silva, Electroencephalography: Basic Principles,
Clinical Applications, and Related Fields, 5th Ed., Lippincott Williams &
Wilkins, 2004.

[6] B. Blankertz, K.-R.R. Müller, G. Curio, T.M. Vaughan, G. Schalk, J.R. Wolpaw,
A. Schlögl, C. Neuper, G. Pfurtscheller, T. Hinterberger, M. Schröder,
N. Birbaumer, The BCI competition 2003: progress and perspectives in
detection and discrimination of EEG single trials, IEEE Trans. Biomed. Eng. 51
(6) (2004) 1044–1051.

[7] B. Blankertz, K.R. Muller, D.J. Krusienski, G. Schalk, J.R. Wolpaw, A. Schlogl,
G. Pfurtscheller, J. Millan, M. Schroder, N. Birbaumer, The BCI competition III:
validating alternative approaches to actual BCI problems, IEEE Trans. Neural
Syst. Rehabil. Eng. 14 (2) (2006) 153–159.

[8] M. Tangermann, K.-R. Müller, A. Aertsen, N. Birbaumer, C. Braun, C. Brunner,
R. Leeb, C. Mehring, K.J. Miller, G. Mueller-Putz, G. Nolte, G. Pfurtscheller,
H. Preissl, G. Schalk, A. Schlögl, C. Vidaurre, S. Waldert, B. Blankertz, Review of
the BCI Competition IV, Front. Neurosci. 6 (55). http://dx.doi.org/10.3389/fnins.
2012.00055.

[9] T. Dickhaus, C. Sannelli, K.-R. Müller, G. Curio, B. Blankertz, Predicting BCI

performance to study BCI illiteracy, BMC Neurosci. 10 (Suppl 1) (2009) 1–2.

[10] B.Z. Allison, C. Neuper, Could anyone use a BCI? in: D.S. Nijholt (Eds.), Brain–
Computer Interfaces, Human–Computer Interaction Series, Springer, London,
2010, pp. 35–54 (Chapter 3).

[11] C. Vidaurre, B. Blankertz, Towards a cure for BCI illiteracy, Brain Topogr. 23 (2)

(2010) 194–198.

[12] B. Obermaier, C. Guger, C. Neuper, G. Pfurtscheller, Hidden Markov models for
online classiﬁcation of single trial EEG data, Pattern Recognit. Lett. 22 (12)
(2001) 1299–1309.

[13] A. Lenhardt, M. Kaper, H. Ritter, An adaptive P300-based online brain com-
puter interface, IEEE Trans. Neural Syst. Rehabil. Eng. 16 (2) (2008) 121–130.
[14] L.F. Nicolas-Alonso, R. Corralejo, J. Gomez-Pilar, D. Álvarez, R. Hornero, Adaptive
semi-supervised classiﬁcation to reduce intersession non-stationarity in multi-
class motor imagery-based brain–computer interfaces, Neurocomputing 2016.
http://dx.doi.org/10.1016/j.neucom.2015.02.005.

[15] E. Kalunga, K. Djouani, Y. Hamam, S. Chevallier, E. Monacelli, SSVEP
enhancement based on Canonical Correlation Analysis to improve bci per-
formances, in: IEEE, Africon 2013, pp. 1–5.

[16] H. Lu, H.-L. Eng, C. Guan, K. Plataniotis, A. Venetsanopoulos, Regularized
common spatial pattern with aggregation for EEG classiﬁcation in small-
sample setting, IEEE Trans. Biomed. Eng. 57 (12) (2010) 2936–2946.

[17] B. Blankertz, R. Tomioka, S. Lemm, M. Kawanabe, K.R. Muller, Optimizing
spatial ﬁlters for robust EEG single-trial analysis, IEEE Signal Process. Mag. 25
(1) (2008) 41–56.

[18] F. Lotte, C. Guan, Regularizing common spatial patterns to improve BCI
designs: uniﬁed theory and new algorithms, IEEE Trans. Biomed. Eng. 58 (2)
(2011) 355–362.

[19] Y. Yang, S. Chevallier, J. Wiart, I. Bloch, Automatic selection of the number of
spatial ﬁlters for motor-imagery BCI, in: European Symposium on Artiﬁcial
Neural Networks (ESANN), 2012, pp. 109–114.

[20] Z. Lin, C. Zhang, W. Wu, X. Gao, Frequency recognition based on canonical
correlation analysis for SSVEP-based BCIs, IEEE Trans. Biomed. Eng. 53 (12)
(2006) 2610–2614.

[21] P.-A. Absil, R. Mahony, R. Sepulchre, Optimization Algorithms on Matrix

Manifolds, Princeton University Press, Princeton, 2009.

[22] R. Bhatia, Positive Deﬁnite Matrices, Princeton University Press, Princeton,

2009.

[23] A. Barachant, S. Bonnet, M. Congedo, C. Jutten, Multiclass brain–computer
interface classiﬁcation by Riemannian geometry, IEEE Trans. Biomed. Eng. 59
(4) (2012) 920–928.

[24] A. Barachant, S. Bonnet, M. Congedo, C. Jutten, Classiﬁcation of covariance
for BCI applications, Neuro-

matrices using a Riemannian-based kernel
computing 112 (2013) 172–178.

[25] M. Congedo, A. Barachant, A. Andreev, A new generation of brain–computer
interface based on Riemannian geometry, arXiv preprint arXiv:1310.8115.
[26] E.K. Kalunga, S. Chevallier, O. Rabreau, E. Monacelli, Hybrid interface: inte-
in multimodal human–machine interfaces,
IEEE/ASME

grating BCI

in:

E.K. Kalunga et al. / Neurocomputing 191 (2016) 55–68

67

International Conference on Advanced Intelligent Mechatronics (AIM), 2014,
pp. 530–535.

[55] A. Barachant, S. Bonnet, M. Congedo, C. Jutten, Riemannian geometry applied
in: Latent Variable Analysis and Signal Separation,

to BCI classiﬁcation,
Springer, 2010, pp. 629–636.

[27] X. Gao, D. Xu, M. Cheng, S. Gao, A BCI-based environmental controller for the
motion-disabled, IEEE Trans. Neural Syst. Rehabil. Eng. 11 (2) (2003) 137–140.
[28] G. Edlinger, C. Holzner, C. Guger, A hybrid Brain–Computer interface for smart
home control, in: J.A. Jacko (Ed.), Human–Computer Interaction. Interaction
Techniques and Environments, Lecture Notes in Computer Science, vol. 6762,
Springer, Berlin, Heidelberg, 2011, pp. 417–426.

[29] R.C. Panicker, S. Puthusserypady, Y. Sun, Adaptation in P300 brain–computer
interfaces: a two-classiﬁer cotraining approach, IEEE Trans. Biomed. Eng. 57
(12) (2010) 2927–2935.

[30] F. Schettini, F. Aloise, P. Aricò, S. Salinari, D. Mattia, F. Cincotti, Self-calibration
algorithm in an asynchronous P300-based brain–computer interface, J. Neural
Eng. 11 (3) (2014) 035004.

[31] H. Verschore, P.-J. Kindermans, D. Verstraeten, B. Schrauwen, Dynamic stop-
ping improves the speed and accuracy of a P300 speller, in: Artiﬁcial Neural
Networks and Machine Learning–ICANN 2012, Toronto, Canada, Springer,
2012, pp. 661–668.

[32] D. Regan, Comparison of transient and steady-state methods, Ann. N.Y. Acad.

Sci. 388 (1) (1982) 45–71.

[33] T. Takahashi, K.H. Chiappa, Activation methods, Electroencephalography. in:
Basic Principles, Clinical Applications, and Related Fields, 5th ed., Lippincott
Williams & Wilkins, 2004, pp. 241–262.

[34] E. Niedermeyer, F.L. da Silva, Electroencephalography: Basic Principles, Clinical

Applications, and Related Fields, 5th ed., 2004.

[35] J. Wolpaw, N. Birbaumer, D.J. McFarland, G. Pfurtscheller, T.M. Vaughan, Brain–
computer interfaces for communication and control, Clin. Neurophysiol. 113
(6) (2002) 767–791.

[36] S.T. Morgan, J.C. Hansen, S.A. Hillyard, Selective attention to stimulus location
modulates the steady-state visual evoked potential, Proc. Natl. Acad. Sci. USA
93 (10) (1996) 4770–4774.

[37] M.M. Müller, S. Andersen, N.J. Trujillo, P. Valdés-Sosa, P. Malinowski, S.A. Hillyard,
Feature-selective attention enhances color signals in early visual areas of the
human brain, Proc. Natl. Acad. Sci. USA 103 (38) (2006) 14250–14254.

[38] B. Allison, T. Lüth, D. Valbuena, A. Teymourian, I. Volosyak, A. Gräser, BCI
demographics: how many (and what kinds of) people can use an SSVEP BCI?,
IEEE Trans. Neural Syst. Rehabil. Eng. 18 (2) (2010) 107–116.

[39] D. Zhu, J. Bieger, G.G. Molina, R.M. Aarts, A survey of stimulation methods used
in SSVEP-based BCIs, Intell. Neurosci. (2010), http://dx.doi.org/10.1155/2010/
702357.

[40] C.S. Herrmann, Human EEG responses to 1100 hz ﬂicker: resonance phe-
nomena in visual cortex and their potential correlation to cognitive phe-
nomena, Exp. Brain Res. 137 (2001) 346–353.

[41] M.A. Pastor, J. Artieda, J. Arbizu, M. Valencia, J.C. Masdeu, Human cerebral
activation during steady-state visual-evoked responses, J. Neurosci. 23 (37)
(2003) 11621–11627.

[42] R.S. Fisher, G. Harding, G. Erba, G.L. Barkley, A. Wilkins, Photic- and pattern-
induced seizures: a review for the epilepsy foundation of america working
group, Epilepsia 46 (9) (2005) 1426–1441.

[43] J. Pan, X. Gao, F. Duan, Z. Yan, S. Gao, Enhancing the classiﬁcation accuracy of
steady-state visual evoked potential-based brain–computer interfaces using
phase constrained canonical correlation analysis, J. Neural Eng. 8 (3) (2011)
036027. /http://iopscience.iop.org/article/10.1088/1741-2560/8/3/036027/metaS.
[44] M. Nakanishi, Y. Wang, Y.-T. Wang, Y. Mitsukura, T.-P. Jung, A high-speed brain
speller using steady-state visual evoked potentials, Int. J. Neural Syst. 24 (06)
(2014) 1450019.

[45] M. Spüler, W. Rosenstiel, M. Bogdan, Online adaptation of a c-VEP brain–
computer interface (BCI) based on error-related potentials and unsupervised
learning, PLoS ONE 7 (12) (2012) e51077. /http://journals.plos.org/plosone/
article?id=10.1371/journal.pone.0051077S.

[46] G. Bin, X. Gao, Y. Wang, Y. Li, B. Hong, S. Gao, A high-speed BCI based on code
modulation VEP, J. Neural Eng. 8 (2) (2011) 025015. /http://iopscience.iop.
org/article/10.1088/1741-2560/8/2/025015/metaS.

[47] H. Cecotti, A self-paced and calibration-less SSVEP-based brain–computer
interface speller, IEEE Trans. Neural Syst. Rehabil. Eng. 18 (2) (2010) 127–133.
[48] S. Parini, L. Maggi, A.C. Turconi, G. Andreoni, A robust and self-paced BCI
system based on a four class SSVEP paradigm: algorithms and protocols for a
high-transfer-rate direct brain communication, Intell. Neurosci. (2009), http:
//dx.doi.org/10.1155/2009/864564.

[49] F. Yger, A review of kernels on covariance matrices for BCI applications, in:
IEEE International Workshop on Machine Learning for Signal Processing
(MLSP), 2013, pp. 1–6.

[50] S. Jayasumana, R. Hartley, M. Salzmann, H. Li, M. Harandi, Kernel methods on the
Riemannian manifold of symmetric positive deﬁnite matrices, in: IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR), 2013, pp. 73–80.
[51] Y. Xie, J. Ho, B. Vemuri, On a nonlinear generalization of sparse coding and
dictionary learning, in: International Conference on Machine Learning (ICML),
2013, p. 1480.

[52] A. Goh, R. Vidal, Unsupervised Riemannian clustering of probability density
in: Machine Learning and Knowledge Discovery in Databases,

functions,
Springer, 2008, pp. 377–392.

[53] A. Goh, R. Vidal, Clustering and dimensionality reduction on Riemannian
manifolds, in: IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2008, pp. 1–7.

[54] X. Pennec, P. Fillard, N. Ayache, A Riemannian framework for tensor com-

puting, Int. J. Comput. Vis. 66 (1) (2006) 41–66.

[56] A. Barachant, A. Andreev, M. Congedo, et al., The Riemannian potato: an auto-
matic and adaptive artifact detection method for online experiments using
Riemannian geometry, in: Proceedings of TOBI Workshop IV, 2013, pp. 19–20.
[57] S.-I. Amari, α-divergence is unique, belonging to both f-divergence and
Bregman divergence classes, IEEE Trans. Inf. Theory 55 (11) (2009) 4925–4931.
[58] W. Samek, D. Blythe, K.-R. Müller, M. Kawanabe, Robust spatial ﬁltering with
in: Advances in NeuralInformation Processing Systems

beta divergence,
(NIPS), 2013, pp. 1007–1015.

[59] W. Samek, K.-R. Muller, Information geometry meets BCI spatial ﬁltering using
divergences, in: International Winter Workshop on Brain–Computer Interface,
Seoul, Korea, IEEE, 2014, pp. 1–4.

[60] A. Barachant, S. Bonnet, Channel selection procedure using Riemannian dis-
tance for BCI applications, in: International IEEE/EMBS Conference on Neural
Engineering (NER), 2011, pp. 348–351.

[61] A. Barachant, S. Bonnet, M. Congedo, C. Jutten, Common spatial pattern
revisited by Riemannian geometry, in: IEEE International Workshop on Mul-
timedia Signal Processing (MMSP), 2010, Saint Malo, France, pp. 472–476.
[62] Y. Li, K.M. Wong, H. De Bruin, EEG signal classiﬁcation based on a Riemannian
distance measure, in: Science and Technology for Humanity (TIC-STH), 2009
IEEE Toronto International Conference, IEEE, 2009, Toronto, Canada pp. 268–
273.

[63] Y. Li, K. Wong, H. De Bruin, Electroencephalogram signals classiﬁcation for
sleepstate decision: a Riemannian geometry approach, IET Signal Process. 6
(4) (2012) 288–299.

[64] J. Jost, Riemannian Geometry and Geometric Analysis, vol. 62011, Springer,

New-York, NY, USA, 2011.

[65] M. Moakher, A differential geometric approach to the geometric mean of
symmetric positive-deﬁnite matrices, SIAM J. Matrix Anal. Appl. 26 (3) (2005)
735–747.

[66] P.T. Fletcher, C. Lu, S.M. Pizer, S. Joshi, Principal geodesic analysis for the study
Imag. 23 (8) (2004)

IEEE Trans. Med.

of nonlinear statistics of shape,
995–1005.

[67] K. Fukunaga, Introduction to Statistical Pattern Recognition, Academic Press, San

Diego, CA, USA, 1990.

[68] O. Ledoit, M. Wolf, A well-conditioned estimator for large-dimensional cov-

ariance matrices, J. Multivar. Anal. 88 (2) (2004) 365–411.

[69] B. Blankertz, S. Lemm, M. Treder, S. Haufe, K.-R. Müller, Single-trial analysis
and classiﬁcation of ERP components: a tutorial, NeuroImage 56 (2) (2011)
814–825.

[70] J. Schäfer, K. Strimmer, A shrinkage approach to large-scale covariance matrix
estimation and implications for functional genomics, Stat. Appl. Genet. Mol.
Biol. 4 (1). http://dx.doi.org/10.2202/1544-6115.1175.

[71] F. Pascal, P. Forster, J.P. Ovarlez, P. Arzabal, Theoretical analysis of an improved
covariance matrix estimator in non-gaussian noise, in: IEEE International
Conference on Acoustics, Speech, and Signal Processing (ICASSP), vol. 4, 2005.
[72] M. Congedo, EEG source analysis, Habilitation á diriger des recherches, Uni-

versité de Grenoble, October 2013.

[73] M.J. Pencina, R.B. D'Agostino, R.S. Vasan, Evaluating the added predictive
ability of a new marker: from area under the ROC curve to reclassiﬁcation and
beyond, Stat. Med. 27 (2) (2008) 157–172.

[74] Y. Kimura, T. Tanaka, H. Higashi, N. Morikawa, SSVEP-based brain–computer
interfaces using FSK-modulated visual stimuli, IEEE Trans. Biomed. Eng. 60
(10) (2013) 2831–2838.

[75] F.-B. Vialatte, M. Maurice, J. Dauwels, A. Cichocki, Steady-state visually evoked
potentials: focus on essential paradigms and future perspectives, Prog. Neu-
robiol. 90 (4) (2010) 418–438.

[76] H. Bakardjian, T. Tanaka, A. Cichocki, Optimization of SSVEP brain responses
with application to eight-command brain–computer interface, Neurosci. Lett.
469 (1) (2010) 34–38.

Emmanuel K. Kalunga is a Ph.D. candidate, in his sec-
ond year, in the joint doctorate program between the
Tshwane University of Technology (South Africa) and
Université de Versailles Saint-Quentin, France. On
completion of his Baccalaureus Technologiae (Cum
Laude) in 2010 at the Tshwane University, Emmanuel
was awarded the Mandela Rhodes Scholarship and
pursued a double Masters degree (Mtech/Msc)
in
Control, Image and Signal processing at the same uni-
versity on the topic “Development of brain computer
interface (BCI) based intention detection approach for
persons with limited neuro-muscular control”. He
graduated Cum Laude in September 2013 after spend-
ing a year of internship at the Laboratoire d'ingénierie des systémes de Versailles
(France). Since January 2014, Emmanuel is enrolled for his PhD on the topic of
“Pattern recognition techniques for implicit brain computer interfaces”, under the
supervision of Karim Djouani,
and
Yskandar Hamam.

Sylvain Chevallier,

Eric Monacelli,

Eric Monacelli is an HDR Associate Professor at Ver-
sailles University (UVSQ), working in the LISV labora-
tory. He is working on the development of analysis
methods and experimental devices adapted to the
assistance of speciﬁc end users. His research projects
incorporate issues of man–machine interface, robotics,
assistive
intelligence
and
systems.

technologies

ambient

(AUB)

Yskandar Hamam graduated as a Bachelor of the
American University of Beirut
in 1966. He
obtained his M.Sc. in 1970 and Ph.D. in 1972 from the
University of Manchester Institute of Science and
Technology. He also obtained his “Diplôme d'Habilita-
tion á Diriger des Recherches” (equivalent to D.Sc.)
from the “Université des Sciences et Technologies de
Lille” in 1998. He conducted research activities and
lectured in England, Brazil, Lebanon, Belgium and
France. He was the head of the Control department and
dean of faculty at ESIEE, France. He was an active
member in modelling and simulation societies and was
the president of EUROSIM. He was the Scientiﬁc
Director of the French South African Institute of Technology (F'SATI) at TUT in South
Africa from 2007 to 2012. He is currently professor at the Department of Electrical
Engineering of TUT. He has authored/co-authored about 300 papers in archival
journals and conference proceedings as well as book contributions. He is a senior
member of the IEEE.

68

E.K. Kalunga et al. / Neurocomputing 191 (2016) 55–68

Sylvain Chevallier obtained his Ph.D. in computer sci-
ence at the LIMSICNRS, with a speciality in cognitive
science as he was interested in visual attention process
for robotics. He spent 2 years in the neurocybernetic
team of ETIS-CNRS where he worked on biologically
inspired decision processes and visual perception. He
joined afterwards INRIA Saclay to study deep neural
network and brain computer interface. He pursued this
post-doctoral research theme within Telecom ParisTech
laboratory. In 2011, he was hired as an assistant pro-
fessor in Université de Versailles to work on rehabili-
tation robotics. He is interested in neural communica-
tion and brain-like bioinspired methods for assistive
technologies. He is focusing on brain–computer interfaces using shared control and
passive approaches.

Quentin Barthélemy received the Engineering degree
and the M.Res. in signal and images analysis and pro-
cessing (with distinction)
from Grenoble Institut
National Polytechnique (Grenoble INP), France, both in
2009; and the Ph.D. degree in signal processing from
Grenoble University and CEA-LIST (Alternative Energies
and Atomic Energy Commission), France, in 2013. His
Ph.D. dissertation deals with sparse representations for
multivariate signals,
including invariances as shift,
rotation and afﬁne transformation. He joined Mensia
Technologies at the Institut du Cerveau et de la Moelle
Epiniére, Paris, France, in 2013, to develop signal pro-
cessing and machine learning methods for real-time
EEG analysis. His research interests are sparse representation, time-frequency
analysis, source separation and Riemannian geometry.

Karim Djouani is a professor, scientist and technical
group supervisor of soft computing, telecommunica-
tion, networking systems and Robotics. Since January
2011 he is Full professor at University Paris Est-Creteil
(UPEC), France and Tshwane University of Technology,
Pretoria, South Africa. From July 2008 to December
2010, he was seconded by the French Ministry of
Higher Education to the French South African Institute
of Technology (F'SATI) at Tshwane University of Tech-
nology (TUT), Pretoria, South Africa. He was also
national and European projects manager at the LISSI
Lab.His current works focus on the development of
novel and highly efﬁcient algorithms for reasoning
systems with uncertainty as well as optimization, for distributed systems, net-
worked control systems, wireless ad-hoc network, wireless and mobile commu-
nication, and wireless sensors networks as well as Robotics. He has authored/co-
authored over 150 articles in archival journals and conference proceedings as well
as ﬁve chapters in edited books.

Online SSVEP-based BCI using Riemannian geometry
Emmanuel Kalunga, Sylvain Chevallier, Quentin Barthélemy, Karim Djouani,

Eric Monacelli, Yskandar Hamam

To cite this version:

Emmanuel Kalunga, Sylvain Chevallier, Quentin Barthélemy, Karim Djouani, Eric Monacelli, et al..
Online SSVEP-based BCI using Riemannian geometry. Neurocomputing, Elsevier, 2016, 191, pp.55-
68. ￿10.1016/j.neucom.2016.01.007￿. ￿hal-01351623￿

HAL Id: hal-01351623

https://hal.archives-ouvertes.fr/hal-01351623

Submitted on 4 Aug 2016

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Neurocomputing 191 (2016) 55–68

Contents lists available at ScienceDirect

Neurocomputing

journal homepage: www.elsevier.com/locate/neucom

Online SSVEP-based BCI using Riemannian geometry

Emmanuel K. Kalunga a,b, Sylvain Chevallier b,n, Quentin Barthélemy c, Karim Djouani a,
Eric Monacelli b, Yskandar Hamam a

a Department of Electrical Engineering and the French South African Institute of Technology, Tshwane University of Technology, Pretoria 0001, South Africa
b Laboratoire d'Ingénierie des Systèmes de Versailles, Université de Versailles Saint-Quentin, 78140 Velizy, France
c Mensia Technologies, S.A. ICM, Hôpital de la Pitié-Salpêtrière, 75013 Paris, France

a r t i c l e i n f o

a b s t r a c t

Article history:
Received 10 April 2015
Received in revised form
17 December 2015
Accepted 8 January 2016
Communicated by S. Fiori
Available online 17 February 2016

Keywords:
Riemannian geometry
Online
Asynchronous
Brain–Computer Interfaces
Steady State Visually Evoked Potentials

Challenges for the next generation of Brain Computer Interfaces (BCI) are to mitigate the common
sources of variability (electronic, electrical, biological) and to develop online and adaptive systems fol-
lowing the evolution of the subject's brain waves. Studying electroencephalographic (EEG) signals from
their associated covariance matrices allows the construction of a representation which is invariant to
extrinsic perturbations. As covariance matrices should be estimated, this paper ﬁrst presents a thorough
study of all estimators conducted on real EEG recording. Working in Euclidean space with covariance
matrices is known to be error-prone, one might take advantage of algorithmic advances in Riemannian
geometry and matrix manifold to implement methods for Symmetric Positive-Deﬁnite (SPD) matrices.
Nonetheless, existing classiﬁcation algorithms in Riemannian spaces are designed for ofﬂine analysis. We
propose a novel algorithm for online and asynchronous processing of brain signals, borrowing principles
from semi-unsupervised approaches and following a dynamic stopping scheme to provide a prediction
as soon as possible. The assessment is conducted on real EEG recording: this is the ﬁrst study on Steady-
State Visually Evoked Potential (SSVEP) experimentations to exploit online classiﬁcation based on Rie-
mannian geometry. The proposed online algorithm is evaluated and compared with state-of-the-art
SSVEP methods, which are based on Canonical Correlation Analysis (CCA). It is shown to improve both
the classiﬁcation accuracy and the information transfer rate in the online and asynchronous setup.

& 2016 Elsevier B.V. All rights reserved.

1.

Introduction

Human–machine interactions without relying on muscular
capabilities is possible with Brain–Computer Interfaces (BCI) [1]
They are the focus of a large scientiﬁc interest [2–4], especially
those based on electroencephalography (EEG) [5]. From a large
literature based on the BCI competition datasets [6–8], one can
identify the two most challenging BCI problems: on the one hand,
the inter-individual variability plagues the models and leads to
BCI-inefﬁciency effect [9–11], on the other hand, the intra-
individual changes calls for the development of online algo-
rithms and adaptive systems following the evolution of the sub-
ject's brain waves [12–14]. To alleviate these variations, several
signal processing and machine learning techniques have been
proposed, such as ﬁltering, regularization or clustering [15,16]
candidate”
without
the
methodology.

emergence of

an obvious

“best

A common vision is shared by all the most successful approa-
ches to reduce signal variabilities: they are applied on covariance

n Corresponding author.

http://dx.doi.org/10.1016/j.neucom.2016.01.007
0925-2312/& 2016 Elsevier B.V. All rights reserved.

matrices instead of working in the input signal space. Common
Spatial Pattern (CSP) [17–19], which is the most known pre-
processing technique in 2-class BCI, try to maximize the covar-
iance of one class while minimizing the covariance of the other.
Similarly, Principal Components Analysis (PCA) [6,7], also applied
for spatial ﬁltering in BCI, is based on the estimation of covariance
matrices. Canonical Correlation Analysis (CCA) is another example
of a technique relying on covariance estimates successfully applied
on EEG for spatial ﬁltering [15,20]. Covariance matrices are also
found in classiﬁers such as the Linear Discriminant Analysis (LDA),
which is largely used in BCI. In all cases, they are handled as ele-
ments of an Euclidean space. However, being Symmetric and
Positive-Deﬁnite (SPD), covariance matrices lie on a subset of the
Euclidean space, with reduced dimensionality and speciﬁc prop-
erties, the Riemannian manifold. Considering covariance matrices
in their original space would reduce the search area for an opti-
mization problem [21,22]. As Riemannian manifolds inherently
deﬁne a metric, the distance between SPD matrices takes into
account the space where they lie on; approximating it to an
Euclidean space introduces inaccuracies and results in ill-
conditioned matrices.

56

E.K. Kalunga et al. / Neurocomputing 191 (2016) 55–68

Recently, studies have been done to consider covariance
matrices obtained from multichannel brain signals in their original
space [23–25]. Covariance matrices are the input features of the
BCI system and the classiﬁer algorithms rely on Riemannian metric
for partitioning the feature space. The authors propose building
speciﬁc covariance matrices in order to emphasize the spatial and
frequential information of the multichannel brain signals [25]. The
outcome of this approach is a simple processing tool chain, which
achieves state-of-the-art classiﬁcation performances.

This paper introduces an online version of the minimum dis-
tance to Riemannian mean (MDRM) algorithm [23], with an
application to Steady-State Visually Evoked Potential (SSVEP) sig-
nals. In SSVEP, the subjects concentrate on stimuli blinking at ﬁxed
frequencies. Depending on the focus of their attention, brain waves
will arise with the same phase and frequency as the stimulus
chosen by the subject. The signals are recorded in an application of
assistive robotics,1 with a shared control scheme relying on an
SSVEP-based BCI and a 3D touchless interface based on IR-sensors
to operate an arm exoskeleton [26]. The long term objective is to
equip a home environment with assistive technologies, including
BCI, as proposed in [27,28]. In this context, it is important to
design an online system, i.e. that adapt continuously to the user's
brain signals, and asynchronous, i.e. that could be activated “on
demand”.

Our online implementation2 is similar to the unsupervised or
semi-unsupervised learning scheme proposed in [29,30]; that has
the potential of shortening (or even removing) the calibration
phase. We apply a similar approach to the dynamic stopping cri-
terion used in [31] to increase the speed of the BCI system. This
approach allows to dynamically determine the trial length and
ensure robustness in classiﬁcation results. Our MDRM approach
outperforms state-of-the-art algorithms in the ofﬂine setup.
Moreover, these state-of-the-art algorithms, that are based on
CCA, are inherently limited as they could not handle resting state.
They must rely on an external command to be turn on or off, and
are thus only suitable to lab environment.

When working with covariance matrices, a crucial point is to
correctly estimate the covariance when the number of samples is
small or heavily corrupted by noise. Several approaches have been
proposed to build the covariance matrices, relying on normal-
ization or regularization of the sample covariances. To assess the
quality of the covariance matrices obtained from EEG samples, a
comparative study of these estimators is conducted.

Hence, the contributions of this works are:

(cid:1) a comprehensive review of the literature on Riemannian geo-

metry applied to EEG and time-series,

(cid:1) a thorough analysis of the covariance estimators and their

impact on tools derived from information geometry,

(cid:1) ﬁrst online application of a Riemannian classiﬁcation algorithm

on SSVEP-based BCI,

(cid:1) introduction of a novel algorithm for online and asynchronous
BCI, including a resting state class, yielding better performance
than state-of-the-art SSVEP algorithms. No phase synchroniza-
tion is required for the SSVEP.

The paper is divided as follows: Section 2 reviews the state of
the art in SSVEP-based BCI and the applications of Riemannian
geometry in machine learning for BCI. Section 3 presents concepts
of Riemannian geometry relevant to this work and estimators of
covariance. In Section 4, the proposed classiﬁcation algorithm for

1 This dataset is freely available from https://github.com/sylvchev/dataset-

2 The open source code is available on https://github.com/emmanuelkalunga/

ssvep-exoskeleton.

Online-SSVEP.

online SSVEP is introduced and the experimental results are pre-
sented in Section 5 for ofﬂine and online setups as well as without
and with a resting state class.

2. State of the art

2.1. Steady-state visually evoked potential

Sensory evoked potentials often oppose Event Related Potential
(ERP) and Steady-State Response (SSR) [32]. This distinction ori-
ginates from the idea that the SSR may be generated by neural
oscillations elicited by the repeated stimulations [33] whereas the
ERP is the transient response to an event occurring at sufﬁciently
long time interval to allow the system to return to its initial state
[34]. We will focus on the visual SSR, called SSVEP and its appli-
cation to BCI.

The SSVEP-based BCI is often employed as a dependent BCI
[35], that is, some residual muscular capabilities are required to
move the eye toward the blinking stimulus as opposed to inde-
pendent BCI, such as Motor Imagery (MI), where the commu-
nication does not rely on any motor capability. It has been shown
that SSVEP could be used as an independent BCI [36,37] as the
brain oscillations are strongly related to the focus of attention.
Using covert attention, i.e. shifting the focus of attention without
moving the eyes, subjects can generate different SSVEP responses.
BCI have highly variable subject-speciﬁc performances. 20–30%
of the subjects cannot operate correctly brain interfaces. This
phenomenon is referred to as BCI illiteracy [9–11]. It affects SSVEP-
based BCI and it is correlated with age and gender, male subjects
being more afﬂicted than female ones [38]. Ofﬂine BCI, that is
approaches where the learning algorithms are trained on a large
dataset of subject's EEG recording, are also afﬂicted which indicate
that a source of variability at the subject level is not handled
correctly by the existing approaches. BCI illiteracy is also afﬂicting
online approaches, where the algorithms are adapted to the sub-
ject's EEG as the experiment goes by.

Visual stimulus plays a crucial role, affecting the BCI perfor-
mance, and should be designed carefully. An in-depth review of
the literature [39] shows that LED stimuli provide better results
than those obtained on computer screen. A cognitive study [40]
indicates that any stimulation between 2 and 50 Hz induces visible
oscillations in the visual cortex. Another study shows that a peak
in signal to noise ratio is visible at around 15 Hz [41]. Common
values employed in SSVEP studies are between 12 and 25 Hz, as
they induce oscillations with higher amplitudes [39]. One should
note that safety of the subject should be taken into account as
some frequency ranges of the stimulation train could trigger epi-
leptic seizure [42].

The phase of the stimulation signal can also be modulated,
enhancing the BCI performance by boosting the Information
Transfer Rate (ITR) [43,44]. An important constraint in that case is
that the experimental setup requires a synchronization between
the display and the recording system, to ensure the correct esti-
mation of the stimulus' phase. Better alternatives are available
when considering systems with such constraints: code-modulated
VEP (c-VEP) has yield the highest ITR in BCI [45,46]. In c-VEP, the
sole difference is that the stimulus ﬂickering is based on pseu-
dorandom sequences instead of the ﬁxed frequencies of SSVEP. All
these successful approaches in SSVEP and c-VEP rely on CCA. Given
two sets of signals, CCA aims at ﬁnding the projection space that
maximizes their cross-covariance while jointly minimizing their
covariance [20,15,44]. The common methodology is to ﬁnd the
canonical space between the multichannel EEG trial on the one
hand and reference signals, usually sine and cosine of target fre-
quencies and harmonics, on the other hand.

E.K. Kalunga et al. / Neurocomputing 191 (2016) 55–68

57

This study is part of our efforts to conceive a smart and adapted
environment for people with disabilities, with a standing point
similar to [27,28]. In our case, the device generating ﬂickering
stimulus should not be connected to the EEG processing system, to
allow, for example, the design of “smart switches” distributed in
the home environment in further studies. Hence, we could not rely
on phase-dependent
or phase-
modulated SSVEP.

such as

setups,

c-VEP

Another requirement of our system is to be self-paced, a
property also called asynchronous, to provide the user with the
ability to use the system “on demand”, i.e. when needed. As
pointed out by [47], it is thus necessary to provide a “no-control”
state to cope with situation where the user does not want to
produce any command. Several methods could be considered, such
as including a reject threshold in the system, as in [47], or directly
provide the system with a reject class, see for example [48]. This
“no-control” state or “resting” state is not always included in the
existing studies, see for example [27,20,44]. The high ITR obtained
with these systems are thus conﬁned to the lab environment and
could not be directly applied to realistic assistive scenarios.

2.2. Riemannian geometry in BCI

tools

Information geometry provides useful

for various
machine learning and optimization problems. In machine learning,
SPD matrices have been used in various applications where fea-
tures and data are only considered in the Euclidean space. Indeed,
covariance matrices lie in the space of SPD matrices which is a
subset of the Euclidean space when considered with the scalar
product. But the same space of SPD matrices, endowed with a
differential structure, induces a Riemannian manifold.

Riemannian geometry can improve machine learning algo-
rithms, taking explicitly into consideration the underlying struc-
ture of the considered space. Three kinds of approaches in the
literature use the geometry of data in machine learning. The ﬁrst
one relies on the mapping of the Riemannian manifold onto an
Euclidean vector space. One such mapping, called logarithmic
mapping, exists between the manifold and its tangent space,
which is an Euclidean space, and has been used in classiﬁcation
task for BCI [24]. Some kernels have been applied successfully to
this end: Stein kernel, Log-Euclidean kernels as well as their nor-
malized versions [49]. The main idea is to map the input data to a
high dimensional feature space, providing a rich and hopefully
linearly separable representation. The so-called kernel trick is to
provide a kernel function, which computes an inner product in the
feature space directly from points lying in the input space, deﬁning
a Reproducing Kernel Hilbert Space (RKHS). The family of kernels
deﬁned on the Riemannian manifold allows the implementation of
extensions of all kernel-based methods, such as SVM, kernel-PCA
or kernel k-means [50]. Apart from the kernel approaches, once
the data are mapped onto a vector space, any machine learning
algorithm working in Euclidean space, such as LDA, could be
applied [23].

A second kind of machine learning approach exploits the
underlying geometry of the data. Instead of mapping the data to
an Euclidean space, either a tangent space or an RKHS, the algo-
rithms are adapted to Riemannian space. For instance, sparse
coding algorithm has been adapted to Riemannian manifold, using
the geodesic distance to estimate the data point and its sparse
estimate [51]. Similarly nonlinear dimensionality reduction tech-
niques have been adapted to Riemannian manifold, such as
Laplacian Eigenmaps (LE), Locally Linear Embedding (LLE), and
Hessian LLE. This adaptation was used to cluster data using their
probability density functions (pdf) [52] or covariance matrices [53]
as features. Another example is the adaptation of interpolation and
ﬁltering of data to Riemannian space performed in [54], where an

afﬁne-invariant Riemannian metric is also proposed to offer a
geodesically complete manifold i.e. a manifold with no edge and
no singular point that can be reached in ﬁnite time.

In the last kind of approach, instead of adapting existing algo-
rithm from Euclidean to Riemannian geometry, new algorithms
are developed directly for Riemannian manifolds. The minimum
distance to Riemannian mean (MDRM) relies on a Riemannian
metric to implement a multi-class classiﬁer and have been applied
on EEG. New EEG trials are assigned to the class whose average
covariance matrix is the closest to the trial covariance matrix [23].
The MDRM classiﬁcation can be preceded by a ﬁltering of covar-
iance matrices, like in [55] where covariance matrices are ﬁltered
with LDA component in the tangent space, then brought back to
the Riemannian space for classiﬁcation with MDRM. Another
example is the Riemannian Potato [56], an unsupervised and
adaptive artifact detection method, providing an online adaptive
EEG ﬁltering (i.e. outliers removal). Incoming signals are rejected if
their covariance matrix lies beyond a predeﬁned z-score, com-
puted from a sliding window. With the same objective of achiev-
ing robustness to noise that affects covariance matrices, Rie-
mannian geometry is used to solve divergence functions of pdfs
[57]. This allows to reformulate the CSP as the maximization of the
divergence between the distributions of data from two different
classes corresponding to two cognitive states [58,59]. Using the
beta divergence the obtained CSP is robust to outliers in sample
covariance matrices and this algorithm is successfully applied to
EEG ﬁltering for BCI. Riemannian metrics are also used for EEG
channel selection [60] and the selection of the most discriminatory
spatial ﬁlters in CSP [61].

In MI experiment, the subject is asked to imagine a movement
(usually hand, feet or tongue), generating Event-Related Syn-
chronization and Desynchronization (ERD/ERS) in pre-motor brain
area. Riemannian BCI is well suited for MI experiment as the
spatial
information linked with synchronization is directly
embedded in covariance matrices obtained from multichannel
recordings. However, for BCI that rely on Evoked Potential such as
SSVEP or Event Related Potential (ERP), as P300, both frequential
and temporal
information are needed; the spatial covariance
matrix does not contain these information. To apply Riemannian
geometry to SSVEP and ERP, the sample covariance matrices can
be deﬁned from a rearrangement of the recorded data. The rear-
rangement is done such that the temporal or frequency informa-
tion are captured [25]. With similar motivations, [62,63] deﬁned a
new Riemannian distance between SPD matrices that would take
into account a weighting factor on matrices. They use this new
distance as a dissimilarity between weighted matrices of power
spectral density to classify EEG into different sleep state by k-
nearest neighbors.

3. Covariance matrices and their geometry

This section presents some formal deﬁnitions for the informa-
tion geometry concepts used in this paper. The link with the
covariance matrices is explicated in Section 3.2, along with the
covariance estimators proposed in the literature.

3.1. Riemannian manifold

An m-dimensional manifold M is a Hausdorff space for which
every point has a neighborhood that is homeomorphic to an open
subset of Rm [64]. When a tangent space is deﬁned at each point,
M is called a differential manifold. A geodesic γ is the shortest
smooth curve between two points, Σ1 and Σ2. The tangent space
T ΣM at point Σ is the vector space spanned by the tangent vectors
of all geodesics on M passing through Σ. A Riemannian manifold is

58

E.K. Kalunga et al. / Neurocomputing 191 (2016) 55–68

a manifold endowed with an inner product deﬁned on the tangent
space, which varies smoothly from point to point.

For the rest of this paper, we will restrict to the analysis of the
manifold MC of the C (cid:3) C symmetric positive deﬁnite matrices,
deﬁned as:
MC ¼ Σ A RC(cid:3)C : Σ ¼ Σ > and x > Σx4 0; 8 x A RC ⧹0

n

o

:

The tangent space T ΣMC is identiﬁed to the Euclidean space of
symmetric matrices:
SC ¼ Θ A RC(cid:3)C : Θ ¼ Θ >

n

o

:

The dimension of the manifold MC , and its tangent space T ΣMC , is
m ¼ CðC þ 1Þ=2.

The mapping from a point Θi of the tangent space to the
manifold is called the exponential mapping ExpΣ ðΘiÞ: T ΣMC-
MC and is deﬁned as:
2ExpðΣ (cid:4) 1
ExpΣ ðΘiÞ ¼ Σ 1

2ΘiΣ (cid:4) 1

2ÞΣ 1
2:

ð1Þ

ð2Þ

2ΣiΣ (cid:4) 1

2Log ðΣ (cid:4) 1

Its inverse mapping, from the manifold to the tangent space is the
logarithmic mapping Log Σ ðΣiÞ: MC-TΣMC and is deﬁned as:
2ÞΣ 1
Log Σ ðΣiÞ ¼ Σ 1
2:
Expð(cid:5)Þ and Log ð(cid:5)Þ are the matrix exponential and matrix logarithm,
respectively. The computation of these operators is straightfor-
ward for SPD matrices of MC . They are obtained from their
eigenvalue decomposition (EVD):
Σ ¼ U diagðλ1; …; λC ÞU > ;
ExpðΣÞ ¼ U diagðlog ðλ1Þ; …; log ðλC ÞÞU > ;
Log ðΣÞ ¼ U diagðexpðλ1Þ; …; expðλC ÞÞU > ;
where λ1; …; λC are the eigenvalues and U the matrix of eigen-
vectors of Σ. As any SPD matrix can be diagonalized with strictly
positive eigenvalues, Log ð(cid:5)Þ is always deﬁned. Similarly the square
root Σ 1

2 is obtained as:

Σ 1

2 ¼ U diagðλ1

; …; λ1

2

2
1

C ÞU > ;

and is unique. The same goes for Σ (cid:4) 1
2.

The tangent vector of the geodesic γðtÞ between Σ1 and Σ2,
(cid:2)(cid:2)(cid:2)!
where γð0Þ ¼ Σ1 and γð1Þ ¼ Σ2 is deﬁned as v ¼ Σ1Σ2
¼ Log
Σ1ðΣ2Þ. A Riemannian distance between Σ1 and Σ2 can thus be
deﬁned as [65]:

δðΣ1; Σ 2Þ ¼ J Log ðΣ (cid:4) 1

Σ2Þ J F ¼

1

log 2λc

;

#

1=2

"

XC

c ¼ 1

where λc, c ¼ 1; …; C, are the eigenvalues of Σ (cid:4) 1
Σ2. From Eq. (3),
the geometric mean of I points Σi on the manifold, i ¼ 1; …; I, can
be deﬁned as the point that minimizes the sum of squared dis-
tances to all Σi:

1

μðΣ1; …; ΣIÞ ¼ arg min
Σ A MC

XI

i ¼ 1

δ2ðΣi; ΣÞ:

ð3Þ

ð4Þ

This mean has no closed form, and can be computed iteratively
[66].

3.2. Covariance matrix estimation

Let xn A RC, n ¼ 1; …; N, denotes a sample of a multichannel EEG
trial recorded on C electrodes. N is the trial length. Let X A RC(cid:3)N be
the EEG trial such as X ¼ ½x1; …; xN(cid:6). Under the hypothesis that all
N samples xn are randomly drawn from a distribution, it follows
that x is a variable of random vectors and its expected vector is
ω ¼ Efxg [67]. The covariance matrix of the random variable x is
deﬁned by Σ ¼ Efðx (cid:4)ωÞðx (cid:4)ωÞ > g and is unknown,
thus an

^Σ should be computed. The choice of the appropriate
estimate
estimator is crucial to verify that the obtained covariance matrices
fulﬁll the following properties: they should be accurate, SPD, and
well-conditioned. The last property requires that
the ratio
between the maximum and minimum singular value is not too
large. Moreover, to ensure the computational stability of the
algorithm, the estimator should provide full-rank matrices, and its
inversion should not amplify estimation errors.

3.2.1. Sample covariance matrix estimator

The most usual estimator is the empirical sample covariance

matrix (SCM), deﬁned as:

^Σ scm ¼

1
N (cid:4) 1

XN

n ¼ 1

ðxn (cid:4) xÞðxn (cid:4) xÞ > ¼

X IN (cid:4)

1N1 >
N

X > ;

ð5Þ

(cid:3)

(cid:4)

1
N (cid:4) 1

1
N

P

N
where x A RC is the sample mean vector x ¼ 1
n ¼ 1 xn. In the
N
matrix notation, IN is the N (cid:3) N identity matrix and 1N is the
vector ½1; …; 1(cid:6). The SCM is often normalized [67] as:

^Σ nscm ¼

C
N

XN

n ¼ 1

ðxn (cid:4) xÞðxn (cid:4) xÞ >
σ2
xn

;

ð6Þ

time ndeﬁned as σ2
with the inter-channel variance at
xn
ðxn (cid:4) xÞ > ðxn (cid:4) xÞ. Other normalization techniques could be used.

¼

This estimation is fast and computationally simple. However
when C (cid:7) N, the SCM is not a good estimator of the true covar-
iance. In the case C 4 N, the SCM is not even full rank.

3.2.2. Shrinkage covariance matrix estimators

To overcome the shortcomings of SCM, the shrinkage estima-
tors have been developed as a weighted combination of the SCM
and a target covariance matrix, which is often chosen to be close to
the identity matrix, i.e. resulting from almost independent vari-
ables of unit variance.
^Σ
shrink ¼ κΓ þ ð1 (cid:4)κÞ
ð7Þ
where 0 rκ o 1. This estimator provides a regularized covariance
^Σ scm for small sample size, that is
that outperforms the empirical
C (cid:7) N. The shrinkage estimator has the same eigenvectors as the
SCM, but the extreme eigenvalues are modiﬁed i.e. the estimator is
shrunk or elongated toward the average.

^Σ scm;

The different shrinkage estimators differ in their deﬁnition of

the target covariance matrix Γ. Ledoit and Wolf [68] (
^Σ scmÞ. Blankertz
in Fig. 1) have proposed Γ ¼ vIC , with v ¼ Trð
. Schäfer
[69] (
^Σ shrink_schaf Þ proposes several ways of deﬁning Γdepending on the
ð
observed

shrink_blank) deﬁnes Γalso as vIC but with v ¼ Trð

^Σ scm [70].

^Σ scmÞ
C

shrink_ledoit

^Σ

^Σ

Fig. 1. Comparison of covariance estimators in terms of classiﬁcation accuracy
obtained with MDRM with increasing EEG trial length. For each trial length, the
average accuracy across all subjects and across all replications is shown. Bars
indicate the error of the mean, i.e. standard deviation divided by the square root of
n(cid:4) 1, n ¼ number of samples.

E.K. Kalunga et al. / Neurocomputing 191 (2016) 55–68

59

3.2.3. Fixed point covariance matrix estimator

The Fixed Point Covariance Matrix [71] is based on the max-
imum likelihood estimator ^ℓ which is a solution to the following
equation:

 

XN

^Σ fp ¼ ^ℓ ¼

C
N

ðxn (cid:4) xÞðxn (cid:4) xÞ >

n ¼ 1

ðxn (cid:4) xÞ > ^ℓ (cid:4) 1

ðxn (cid:4) xÞ

!

:

ð8Þ

As there is no closed form expression to Eq. (8), it can be written as
a function of ^ℓ : gð ^ℓÞ ¼
, where
, which is a solution to Eq. (8). Using ^ℓ0≔ ^Σ nscm as the
gð ^ℓ n
initial value of ^ℓ, it is solved recursively as ^ℓt ⟶
t-1

^Σ fp. g admits a single ﬁxed point ^ℓn

Þ ¼ ^ℓ n

^ℓ n

.

4. Online adaptation of the Riemannian classiﬁer

Concerning Riemannian classiﬁcation of SSVEP, the ofﬂine
methodology is explained in [25]. In this paper, we propose an
online classiﬁer for SSVEP, composed of an ofﬂine training phase
and an online and asynchronous test phase. This analysis is per-
formed for each subject independently.

4.1. Ofﬂine Riemannian classiﬁcation

The proposed classiﬁer relies on the Minimum Distance to
Riemannian Mean (MDRM) introduced in [55] and extended in
[25,72] for possible ofﬂine applications on SSVEP signals. Let us
consider an experimental SSVEP setup with F stimulus blinking at
F different frequencies. It is a multiclass classiﬁcation with K ¼ F
þ 1 classes: one class per stimulus and one resting state class. The
covariance matrices are estimated from a modiﬁed version of the
input signal X:

X A RC(cid:3)N-

7
5 A RFC(cid:3)N;

3

2

6
4

Xfreq1
⋮

XfreqF

is the input signal X band-pass ﬁltered around fre-
where Xfreqf
^Σ
quency freqf , f ¼ 1; …; F. Thus the resulting covariance matrix
belongs to MFC . Henceforth, all EEG signals will be considered as
ﬁltered and modiﬁed by Eq. (9).

From I labelled training trials Xif

gI
i ¼ 1 recorded per subject, K
centers of class ΣðkÞ
μ are estimated using Algorithm 1. When an
unlabelled test trial Y is given, it is classiﬁed as belonging to the
class whose center ΣðkÞ
is the closest to the trial's covariance
μ
matrix (Algorithm 2, step 2).

Algorithm 1. Ofﬁne estimation of Riemannian centers of classes.

Inputs: Xi A RFC(cid:3)N, for i ¼ 1; …; I, a set of labelled trials.
Inputs: I ðkÞ, a set of indices of trials belonging to class k.
Output: ΣðkÞ

μ , k ¼ 1; …; K, centers of classes.
^Σ i of Xi

Compute covariance matrices
for k ¼1 to K do

^Σ i : i A I ðkÞÞ, Eq. (4)

1:
2:
3: ΣðkÞ
end
4:
return ΣðkÞ
μ

μ ¼ μð

5:

Algorithm 2. Minimum distance to Riemannian mean.

μ , K centers of classes from Algorithm 1.

Inputs: ΣðkÞ
Input: Y A RFC(cid:3)N, an unlabelled test trial.
Output: k

, the predicted label of Y.

n

1:

2:

Compute covariance matrix

n

k

¼ arg minkδð

^Σ ; ΣðkÞ
μ Þ

^Σ of Y

3:

return k

n

4.2. Curve-based online classiﬁcation

In ofﬂine synchronous BCI paradigm, cue onset are used as
reference for the localization of a brain response, e.g. an evoked
potential. Nonetheless most of the BCI applications are online and
asynchronous; cue onsets are not known, thus designing online
version of BCI algorithms is not a trivial task. The approach
introduced here identiﬁes a period (i.e. time interval) in the online
EEG χ A RFC(cid:3)N , where N is the number of recorded samples,
associated with a high probability (above threshold) of observing
an SSVEP at a speciﬁc frequency, as illustrated in Algorithm 3.

Algorithm 3. Curve-based online classiﬁcation.

Inputs: hyper-parameters w; Δn; D; and ϑ.
Inputs: ΣðkÞ

μ , k ¼ 1; …; K, centers of classes from Algorithm 1

(ofﬂine training).

Inputs: Online EEG recording χðnÞ.
~
kðnÞ, online predicted class.
Output:
1:
2:
3:
4:
5:

d ¼ 1
for n ¼ w to N step Δn
Epoch Xd, Eq. (10), and classify it with Algorithm 2
if dZ D

Find the most recurrent class in K ¼ k

n
j A J ðdÞ:

k ¼ arg maxk

ρðkÞ, Eq. (11)

6:

7:

8:

if ρðkÞ 4ϑ
~δ
Compute
o 0

~δ

if

k , Eq. (12)

k
return

~
k ¼ k

ð9Þ

9:
10:
11:
12:
13:
14: end

end
end

end
d ¼ d þ 1

(cid:5)

To locate this interval, we focus on the last D recorded EEG
(cid:6)
overlapping epochs Xj A RFC(cid:3)w
j A J ðdÞ, with the set of indices
J ðdÞ ¼ d (cid:4) D þ 1; …; d (cid:4) 1; d; where d is the index of the current
epoch Xd in the online recording χðnÞ. Epochs have size w, and the
interval between two consecutive epochs is Δn, with w 4Δn:
Xd ¼ χðn (cid:4) w; …; nÞ:
ð10Þ
To obtain the ﬁrst D epochs Xj A J ðdÞ, at least w þ ðD (cid:4) 1ÞΔn samples
of χ should be recorded (step 4).
The classiﬁcation outputs k

n
j A J ðdÞ obtained in step 3 by applying
Algorithm 2 on Xj A J ðdÞ are stored in a vector K, which always
contains the latest D classiﬁcation outputs. The class that occurs
the most in K (step 5), with an occurrence probability ρðkÞ above a
deﬁned threshold ϑ, is considered to be the class, denoted k, of the
ongoing EEG recording χðnÞ. The vector ρ is deﬁned as:

n
#fk
j A J ðdÞ ¼ kg

ρðkÞ ¼

D

;

for k ¼ 1; …; K;

ð11Þ

ρðkÞ; then ρðkÞ is compared to the threshold ϑ.
with k ¼ arg maxk
If ϑ is not reached within the last D epochs, the classiﬁcation
output is held back, and the sliding process continues until ϑ is
reached. In the last D epochs, once a class k has been identiﬁed, a
curve direction criterion is introduced to enforce the robustness of
the result. For class k to be validated, this criterion requires that
the direction taken by the displacement of covariance matrices

60

E.K. Kalunga et al. / Neurocomputing 191 (2016) 55–68

j A J ðdÞ be toward the center of class ΣðkÞ

^Σ
k , the sum of
gradients (i.e. differentials) of the curve made by distances from
^Σ j A J ðdÞ to ΣðkÞ
Δδ
X
Δj

μ should be negative (step 8):

ðj (cid:4) 1Þ o 0 with

μ . Hence

ðjÞ (cid:4)δ

Xd

ðjÞ

¼

¼

~δ

δ

k

k

k

k

~δ

j A J ðdÞ

j ¼ d (cid:4) D þ 2

In this work, we propose a slightly different application of the
Riemannian potato where the outliers are removed per class.
are deﬁned
Hence
n
Σk

for K class, K Riemannian potatoes

. Since Riemannian distances to geometric mean

μ; μk; σk

o

K

do not have a Gaussian distribution, we make use of the geometric
mean for μ, the geometric standard deviation for σ and the geo-
metric z-score. They are deﬁned as follows [72]:

k ¼ 1

δ

k

ðjÞ ¼

P

δð
K
k ¼ 1

^Σ j; ΣðkÞ
μ Þ
^Σ j; ΣðkÞ
δð
μ Þ

:

The occurrence criterion is inspired by the dynamic stopping of
[31]; there is no ﬁxed trial length for classiﬁcation. The occurrence
criterion ensures that the detected user intention is unaffected by
any short time disturbances due to noise or subject's inattention,
as presented in Algorithm 3. This approach offers a good com-
promise to obtain robust results within a short and ﬂexible time.
The curve direction criterion solves both the problems of
latency in the EEG synchronization and of the delays inserted by
the EEG epochs processing. Indeed, some EEG epochs gather sig-
nals from different classes and might be wrongfully classiﬁed if the
decision is solely based on the distance with the center of the
class. This situation and the effect of the curve direction criterion
are well shown in Section 5.4. Ensuring that the covariance
matrices are displaced toward the center of the detected class
provides a guarantee that it matches with the current EEG state.
Inversely, if the direction of the curve is moving away from the
center of the detected class, it might indicate that there has been a
change in the EEG state that has not been detected.

Algorithm 3 has 4 hyperparameters: w; Δn; D, and ϑ. The values
of w; D, and ϑ are set through cross validation and are given in
Section 5.4. Although a large window size w is expected to
increase the classiﬁcation accuracy, it increases the response time,
thus reducing the time resolution, and extends the overlap
between different EEG states. The step size Δn should be set to a
minimum value to allow a maximum number of overlapping
epochs (D) within a short time. However, it should be large enough
to avoid too many calculations within a time interval with small or
inexistent changes in EEG states. If the number of epoch D is too
small, the classiﬁcation will be sensitive to non-intentional and
abrupt changes in the EEG. A too large D will
increase the
momentum and reinforce the inﬂuence of the past EEG signals. It
should also be mentioned that both the occurrence and the curve
direction criteria cannot have a signiﬁcant impact if the value of D
is too small. The probability threshold parameter ϑ acts like a
rejection parameter: high ϑ values correspond to a high
rejection rate.

4.3. Outliers removal with Riemannian potato

Outliers in the training data might affect the Riemannian mean
of classes in the MDRM classiﬁcation scheme. To alleviate this
effect, an approach called the Riemannian potato, introduced in
[56], is exploited. In this approach, all trials are represented by
their covariance matrices Σi. A reference covariance matrix is
estimated, e.g. Riemannian mean of all trials Σμ. The Riemannian
distances δi between each Σi and Σμ are computed. Any trial that
lies too far, i.e. beyond a certain threshold, from the reference
matrix Σμ in terms of Riemannian distance is rejected. In [56], the
distance z-score thresholding is deﬁned as:

zðδiÞ ¼

δi (cid:4)μ

σ 4 zth

ð13Þ

where μ and σ are respectively the mean and standard deviation of
(cid:5) (cid:6)
i ¼ 1. In other words, any trial Σi whose z-score zðδiÞ
distances δi
I
is larger than the threshold zth ¼ 2:5 is rejected.

ð12Þ

μk ¼ exp

lnðδk
i Þ

!

X

 

1
I
s 

(cid:8)

X

i
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
(cid:9)
(cid:8)
1
2
ln δk
I

=μk

(cid:9)

i

!

σk ¼ exp

(cid:8)
ln δk

i
=μk
lnðσkÞ

i

(cid:9)

:

zðδk

i Þ ¼

ð14Þ

Through cross-validation, the z-score threshold is set to zth ¼ 2:2.
Moreover, outliers are removed iteratively. Each time outliers
are rejected, a new center of class is computed and used as
reference for the next iteration. Iterations continue until con-
vergence, i.e. no more outlier found.

5. Experimental validation

Covariance matrix estimators, Algorithms 2 and 3 are applied
to SSVEP signals for ofﬂine and online analysis. This section pre-
sents the analysis and results obtained.

5.1. Data description

The signals are recorded from 12 subjects during an SSVEP
experiment. EEG are measured on C ¼ 8 channels: OZ, O1, O2, POZ,
PO3, PO4, PO7, and PO8. The ground and the reference electrodes
were placed respectively on FZ and the right hear mastoid
respectively. The acquisition rate is T s ¼ 256 Hz on a gTec MobiLab
Amp (gTec, Graz, Austria). The subjects are presented with F ¼ 3
visual target stimuli blinking respectively at freq ¼ 13 Hz; 17 Hz
and 21 Hz. It is a K ¼ 4 classes BCI setup made of the F ¼ 3 stimulus
classes and one resting class (no-SSVEP). In a session, which lasts
5 min, 32 trials are recorded: 8 for each visual stimulus and 8 for
the resting class. The number of sessions recorded per subject
varies from 2 to 5. Thus the longest EEG recorded for a single
subject is 25 min or 160 trials. The trial length is 6 s, that is N ¼ 6 (cid:3)
T s ¼ 1536 samples. Since data are rearranged as detailed in (9),
trials X A RFC(cid:3)N, where FC ¼ 24 corresponding to 8 channel times
3 stimulus frequencies. For each subject, a test set is made of 32
trials whereas the remaining trials (which might vary from 32 to
128) make up for the training set.

5.2. Covariance estimators comparison

In this section, the effectiveness of covariance matrix estima-
tors is evaluated for SSVEP signals. The evaluation is done in terms
of classiﬁcation accuracy and integrated discrimination improve-
ment (IDI), obtained by each estimator (see Section 3.2) with
respect to SCM estimator while using the ofﬂine MDRM classiﬁer.
The different conditioning of covariance matrices are also
investigated.

A bootstrapping with 1000 replications is performed to assess
the performances of each estimator. Estimators are compared on
10 trial lengths t A f0:5; 1:0; …; 5:0g s, as these are known to affect
the estimators performance. Here N A f128; 256; …; 1280g is com-
puted as N ¼ t (cid:3) T s.

Fig. 1 shows the classiﬁcation accuracies of each estimator
computed across all subjects. Even if the error bars show an

important inter-subject variability, the increase in the accuracy can
be attributed to the fact that the relevant patterns in EEG accu-
mulate with the trial length, producing better estimation of the
covariance matrices. This is known to be particularly true for the
SCM estimator and it could be seen in Fig. 1. It appears that
shrinkage estimators (especially Ledoit and Schäfer) are less
affected by the reduction of epoch sizes than the other estimators.
This is a direct consequence of the regularization between the
sample covariance matrices and the targeted (expected) covar-
iance matrix of independent variables.

For computational purposes, it is important to look at the
matrix conditioning. Fig. 2 shows the ratio C between the largest
and smallest eigenvalues: in well-conditioned matrices, C is small.
Shrinkage estimators offer better conditioned matrices whereas
the SCM, NSCM, and Fixed Point matrices are ill-conditioned
below 2 s of trial length, and may result in singular matrices.

In Fig. 2b, the Integrated Discrimination Improvement (IDI), as
deﬁned in [73], is computed for the different estimators and trial
lengths. The SCM is used as a reference for improvement, as this is
the most popular estimator in the literature. Negative IDI means a
deterioration in the method discrimination ability. It is clear that
shrinkage estimators increase the discrimination power of the
classiﬁer. However, despite being more complex than the SCM, the

E.K. Kalunga et al. / Neurocomputing 191 (2016) 55–68

61

NSCM and the Fixed Point estimators decrease the discrimination
ability of classiﬁers. From Figs. 1 and 2b, it is apparent that the
difference in performance between the SCM and shrinkage esti-
mators reduces as the trial length increases. The simplicity of the
SCM plays a favorable role: it is an attractive method for longer
trials. The p-values under the hypothesis that
there is no
improvement (i.e. IDI ¼0) from one estimator to another are all
inferior to 10 (cid:4) 47, (po 10 (cid:4) 3 indicating a statistically signiﬁcant
discriminatory improvement); hence the improvement is sig-
niﬁcant. It should be noted that the estimation of covariance
matrices is a trade-off between the quality of the estimate and the
computation time required; this should be considered for real-
time processing.

5.3. Effect of outliers on center estimations

Outliers can affect the ofﬂine training of the K centers of class
ΣðkÞ
μ by Algorithm 1, which is crucial for the evaluation phase and
online application. Fig. 3 shows representations of training cov-
ariance matrices Σi in the tangent space ðΘiÞ, projected at the
mean of all training trials, for the subjects with the lowest (Fig. 3a
and b) and the highest (Fig. 3c and d) BCI performance. To obtain
this visualization, the ﬁrst two principal components of a PCA
(cid:5) (cid:6)
applied on Θi
I
i ¼ 1 are selected. In Fig. 3b and d, the Riemannian
potato presented in Section 4.3 is applied; outliers in each class are
removed. The interest of using a Riemannian potato is well seen in
Fig. 3a and b. In Fig. 3a, the outliers are so distant from the rest of
the class matrices that the center of class is stretched away.
Applying a Riemannian potato removes the outliers, and the
center of class is better estimated (Fig. 3b).

When training trials are not noisy, their covariance matrices are
compact around their Riemannian mean. In this case the removal
of outliers by the Riemannian potato does not inﬂuence, at least
not signiﬁcantly, the Riemannian mean. This is the case in Fig. 3c
and d. Thus, applying the Riemannian potato is crucial for noisy
data and will have a limited effect on clean data. The impact of the
Riemannian potato on the classiﬁcation accuracy is discussed in
Section 5.4.

5.4. Classiﬁcation results and analysis

In this section, the performance of the proposed method is
presented. First, the performance of the MDRM approach in an
ofﬂine setup is analyzed, then the results of the online algorithm
are presented. In the ofﬂine analysis, the relevance of identifying
the latency between cue onset and SSVEP response is shown. The
results of the MDRM approach are compared to two state-of-the-
art methods [20,44]. The online evaluation is divided into two
parts: in the ﬁrst one the algorithm discriminates between K ¼ F
¼ 3 SSVEP classes (i.e. 13, 17 and 21 Hz) and in the second one is
applied on K ¼ 4 classes,
i.e. the F ¼ 3 SSVEP class and the
resting class.

5.4.1. Ofﬂine analysis

A close inspection of the ﬁltered signals shows that almost all
signals are synchronized with the trial frequency 2 s after cue
onset τ0 ¼ 0, as shown in Fig. 4. This delay is mainly due to pro-
tocol design and user speciﬁc cognitive processes. The protocol is
aimed to provide an asynchronous setup close to real application.
The user are not required to look at a ﬁxation point or to directly
gaze toward the target, as in [74,44], during inter-trial periods.
This is a tentative explanation for the higher delay observed in our
study and it is consistent with literature observations [75,76]. In
fact, before τ0 þ 2 s, for some users the signal could still be syn-
frequencies. An important
chronized with the previous trial

Fig. 2. (a) Covariance matrices condition expressed as the ratio C between largest
and smallest eigenvalues for the different covariance estimators. The comparison is
done for increasing EEG trial length. (b) Integrated discrimination improvement
brought to the classiﬁcation task by various estimators along varying trail length.
The indicated IDI values are multiplied by 102. ^Σ scm is used as a baseline.

62

E.K. Kalunga et al. / Neurocomputing 191 (2016) 55–68

Fig. 3. Scatter plot of covariance matrices for all trials mapped on the tangent space. The distance between each trial covariance matrix Σi and its Riemannian mean class ΣðkÞ
μ
is shown as connection line. The black/thick star represents the Riemannian mean of all trials. Matrices of resting class, 13Hz class, 21Hz class, and 17Hz class are represented
with starts in black, grey, dark grey, and light grey respectively. Subject with lowest BCI performance, (3a) before and (3b) after Riemannian potato ﬁltering. Subject with
highest BCI performance, (3c) before and (3d) after Riemannian potato ﬁltering.

Fig. 4. Signal amplitude at each stimulus frequency, showing synchronization of EEG with respect to time (seconds). The raw signal of the trial measured on Oz is band
ﬁltered using a Butterworth of order 8 at each stimulus frequency and the resulting signals are shown in blue (light grey), green (grey), and red (dark grey) for the same
signal ﬁltered respectively at 13, 17, and 21 Hz. The cue onset τ0 at time 0 on the x-axis is shown with a vertical discontinued line. 4 trials are shown, one for each class.
Signals are from the subjects with the highest (4a) and with the lowest BCI performance (4b). (For interpretation of the references to color in this ﬁgure caption, the reader is
referred to the web version of this paper.)

increase in average classiﬁcation accuracy (almost 10%) could be
obtained by taking the trial from 2 s after cue onset. It is therefore
crucial to consider the latency between the cue onset of trial and
the actual synchronization of SSVEP at stimulus frequency. Thus in

the ofﬂine synchronous processing, the conﬁdent window for
classiﬁcation is set 2 s after the cue onset ðτ0 þ 2Þ.

Table 1 shows the ofﬂine classiﬁcation accuracies for each
subject obtained by the application of the MDRM as described in

E.K. Kalunga et al. / Neurocomputing 191 (2016) 55–68

63

Table 1
Ofﬂine performance in terms of accuracy and ITR. Five methods are compared: (1) CCA approach introduced by Lin et al. [20], (2) CCA approach introduced by Nakanishi et al.
[44], (3) MDRM described in Section 4.1 (Algorithm 2), (4) MDRM where processed epochs are taken 2 s from the beginning of the trial, and (5) MDRM-Potato, where outliers
are removed using the Riemannian potato approach described in Section 4.3.

Ofﬂine algorithms

Lin et al. [20]

Nakanishi et al. [44]

MDRM ðτ0Þ

MDRM

MDRM-Potato

acc (%)

itr (bpm)

acc (%)

itr (bpm)

acc (%)

itr (bpm)

acc (%)

itr (bpm)

acc (%)

itr (bpm)

S1
S2
S3
S4
S5
S6
S7
S8
S9
S10
S11
S12

91.7
45.8
100.0
97.9
83.3
77.1
98.6
97.9
91.7
80.2
89.6
95.8

16.3
0.7
23.8
21.3
11.5
8.7
22.0
21.3
16.3
10.0
15.0
19.4

84.7
47.9
93.0
96.6
82.2
76.2
96.7
65.5
77.9
76.9
82.7
93.8

12.2
1.0
17.2
20.0
11.0
8.3
20.1
4.7
9.0
8.6
11.2
17.8

67.6
66.0
90.2
78.3
76.0
72.2
90.0
90.4
64.0
79.2
54.8
82.3

3.5
3.2
10.3
6.1
5.5
4.5
10.2
10.3
2.8
6.4
1.4
7.4

84.7
79.4
99.3
89.7
89.5
87.2
99.8
99.7
85.8
93.1
78.2
98.6

12.2
9.7
22.7
15.0
14.9
13.6
23.5
23.2
12.8
17.3
9.2
22.0

84.5
79.3
99.3
89.7
89.4
87.2
99.8
99.7
85.7
93.0
78.2
98.6

12.1
9.6
22.7
15.0
14.9
13.6
23.4
23.2
12.7
17.2
9.1
22.0

Mean

87.57 15.1

15.5 76.8

81.27 14.1

11.8 7 6.0

75.97 11.4

6.0 73.1

90.47 7.8

16.37 5.3

90.4 7 7.8

16.37 5.3

Fig. 5. Evaluation of the online algorithm parameters. 5a shows the decrease of the average classiﬁcation error over all subjects during the successive epochs after the
beginning of the trial. 5b is an example taken from the subject with the best performance showing how the probability of the actual class varies with epoch position from
beginning of trial. The groundtruth class probability is represented with a thick-and-star line, while other classes probability lines are thin-and-diamond. 5c shows the
variation of the average classiﬁcation error for different probability threshold ð0r ϑ o 1Þ and its inﬂuence on the classiﬁer output (Algorithm 3 step 6). 5d shows how the
average online performance varies with respect to the epoch size (w). It shows both the classiﬁcation accuracy (left y-axis, black curve) and the ITR (right y-axis, grey curve).
In 5a, 5c, and 5d, the bars represent the error of the mean i.e. standard deviation divided by the square root of n (cid:4) 1, n ¼ number of samples.

Algorithm 2, with the epochs taken at τ0 þ 2. Column MDRM ðτ0Þ
shows the results obtained when the epochs are taken from cue
onset. The Riemannian potato technique presented in Section 4.3

was applied for outliers removal (MDRM-Potato). The performance
of the MDRM approach is compared to two CCA-based state-of-
the-art methods proposed by Lin et al. [20] and Nakanishi et al.

64

E.K. Kalunga et al. / Neurocomputing 191 (2016) 55–68

[44] respectively. In the implementation of these methods, the
epochs are also taken from τ0 þ 2.

The MDRM approach outperforms both CCA-based method
with an average classiﬁcation accuracy of 90.4 77.8% and ITR of
16.37 5.3 bits/min. Lin et al. rank second with 87.57 15.1% and
15:5 7 6:8 bits=min. The method proposed by Nakanishi et al.,
which could be expected to achieve better results as reported in
[44], only ranks third. This is mainly due to the fact that this
method requires information on the phase of the stimuli. In fact,
Nakanishi et al. use the average of all training trials belonging to a
unique class as reference signal in the CCA. When SSVEP trials
belonging to a unique trial are not in-phase, which is the case in
the current work, averaging them will cancel the signal.

Within the MDRM approach, it is shown that taking into account
the latency between the cue onset and the SSVEP response sig-
niﬁcantly increases the classiﬁcation performances: accuracy and ITR
rises from 75.9711.4% and 6.073.1 bits/min to 90.477.8% and
16:3 7 5:3 bits=min. In turn removing outliers with the Riemannian
potato does not bring signiﬁcant change. This could be attributed to
the fact that the recording have been conducted in controlled envir-
onment, with small or little external noise.

5.4.2. Online analysis without resting class

In an online asynchronous experiment, there is no cue onset,
and the delay before SSVEP synchronization might differ from one
trial to another and from one subject to another. To locate the trust
EEG region for the classiﬁcation, D and ϑ are set respectively to
5 and 0.7 through cross-validation. The performance of this online
setup are analyzed and Fig. 5 shows the results. From the analysis
shown in Fig. 5d, the epoch size is set to w ¼ 2:6 s. The step size is
set to Δn ¼ 0:2 s, that is a new epoch is classiﬁed every 0.2 s.

In Fig. 5a, the classiﬁcation error is plotted against the epoch
index. It shows that the error decreases as epochs move from the
beginning of the trial. The error increases in the last epochs of the
trial, corresponding to the end of the SSVEP task. Fig. 5b details the
evolution of the probability for each class as epochs index
increases. It appears clearly that the class of the EEG trial (thick-
and-star line) has the largest probability only a few epochs after
the beginning of the trial. Moreover, one can see that this is an
increasing trend over the whole trial. Thus by setting an appro-
priate probability threshold ϑ, the actual class can be identiﬁed
with enough conﬁdence. Fig. 5c shows the inﬂuence of the prob-
ability threshold ϑ on the classiﬁcation error. The error is reduced
when the probability threshold ϑ is increased. Fig. 5d shows how

Fig. 6. Covariance matrices trajectory during a 4-class SSVEP online recording. The
circles represent class centers. The triangles mark the beginning in the experiment
of a new trial whose class is indicated by the triangle's color. Matrices of resting
class, 13Hz class, 21Hz class, and 17Hz class are represented in black, grey, dark
grey, and light grey respectively. 6a shows the ﬁrst 7 trials. The ﬁrst 3 trials are from
the resting class, the remaining are respectively class 13 Hz, 17 Hz, and 21 Hz. 6b
shows the entire recording. Data are taken from the subject with the highest BCI
performance.

Table 2
Classiﬁcation performances (accuracy in %, delay before valid and conﬁdent classiﬁcation in seconds, and ITR in bits/min) achieved using the online algorithm. The ﬁrst
column indicates the subjects. The following three columns show the results obtained without the curve direction criterion (Algorithm 3 up to 6): by stopping at step 6, k is
taken to be the valid class. The next three columns contain the results of the complete online algorithm. The last three columns report the results obtain when outliers are
removed in the training phase using the Riemannian potato technique described in Section 4.3.

Online ðρðkÞ 4ϑÞ

Online (full Algorithm 3)

Online-Potato

acc (%)

delay (s)

itr (bpm)

acc (%)

delay (s)

itr (bpm)

acc (%)

delay (s)

itr (bpm)

S1
S2
S3
S4
S5
S6
S7
S8
S9
S10
S11
S12

68.8
64.6
81.2
83.3
72.9
66.7
93.1
87.5
60.4
64.6
54.2
52.5

0.8
0.7
0.7
0.8
0.7
0.7
0.7
0.6
0.7
0.7
0.7
0.7

26.3
21.6
54.3
53.2
37.1
24.5
89.6
76.2
15.7
21.5
9.9
8.0

77.1
77.1
95.8
91.7
83.3
72.9
98.6
100.0
77.1
87.5
87.5
99.2

1.1
1.2
1.0
1.0
1.0
1.1
0.9
0.9
1.2
1.1
1.3
1.2

27.9
26.8
73.0
58.6
42.5
24.3
87.0
95.9
27.6
45.3
38.9
71.7

77.1
77.1
95.8
95.8
83.3
72.9
98.6
100.0
77.1
87.5
87.5
99.2

1.1
1.2
1.0
1.0
1.0
1.1
0.9
0.9
1.2
1.1
1.3
1.2

27.9
26.8
73.0
69.2
42.5
24.3
86.8
95.9
27.6
45.3
38.9
71.8

Mean

70.8713

0.77 0.0

36.57 26.3

87.37 9.8

1.1 7 0.1

51.6 725.1

87.77 10

1.17 0.1

52.57 25.5

E.K. Kalunga et al. / Neurocomputing 191 (2016) 55–68

65

Table 3
This table summarizes the performance achieved with the online algorithm with resting class identiﬁcation, as in Table 2.

Online ðρðkÞ 4ϑÞ

Online (full Algorithm3)

Online-Potato

acc (%)

delay (s)

itr (bpm)

acc (%)

delay (s)

itr (bpm)

acc (%)

delay (s)

itr (bpm)

S1
S2
S3
S4
S5
S6
S7
S8
S9
S10
S11
S12

67.2
78.1
89.1
75.0
71.9
87.5
84.4
85.9
67.2
62.5
59.4
69.4

0.7
0.7
0.8
0.7
0.7
0.8
0.7
0.8
0.7
0.7
0.8
0.7

37.6
59.0
85.2
52.2
46.7
80.2
76.3
76.4
37.2
30.3
23.5
44.8

71.4
75.0
89.1
75.0
70.3
87.3
85.4
89.1
75.0
69.5
68.8
93.8

1.1
1.0
1.0
0.9
1.1
1.1
1.0
1.0
1.0
1.0
1.1
1.0

32.4
39.2
67.6
42.9
31.0
58.7
62.5
68.1
39.6
32.0
29.1
79.4

71.4
75.0
89.1
75.0
70.3
87.3
88.5
89.1
76.6
69.5
68.8
93.8

1.1
1.0
1.0
0.9
1.1
1.1
1.0
1.0
1.1
1.0
1.1
1.0

32.4
39.2
67.6
43.4
31.0
58.7
69.1
68.1
40.3
32.0
29.1
79.9

Mean

74.87 10.2

0.77 0.0

54.1 721.0

79.17 9.1

1.0 70.1

48.6 717.6

79.57 9.3

1.07 0.1

49.27 18.2

Fig. 7. (a) Confusion matrix for K¼4 classes with Online-Potato. (b): ROC curve indicating the inﬂuence of the ϑ parameter.

the average online performance varies with respect to the epoch
size (w). Both the classiﬁcation accuracy and the ITR are shown.
With short w values, the epoch size does not capture enough
feature for a correct classiﬁcation, and with long w, the epoch
loses temporal resolution. The ITR increases with the classiﬁcation
rate but drops sensibly after a peak value.

The observation of Fig. 6 provides a visualization of the prin-
ciple guiding the online implementation of Eq. (12). This ﬁgure
shows the trajectory on the tangent space taken by covariance
matrices during a 4-class SSVEP experiment, and how they are
classiﬁed epoch by epoch. It can be seen (encircled in Fig. 6a) that
a change in the SSVEP stimulus might not be detected instanta-
neously by the classiﬁer. The trials are erroneously attributed with
conﬁdence to the previous class. The proposed online algorithm,
described in Algorithm 3, mitigates this issue and increases the
classiﬁcation accuracy as shown in Table 2. The “Online ðρðkÞ 4ϑÞ”
column shows the results of the online algorithm without the
curve direction criterion (i.e., without steps 6–11), and “Online
(full Algorithm 3)” shows the improvement brought by this cri-
terion. The performances are in terms of average classiﬁcation
accuracy (acc (%)), average time taken into the trial before classi-
ﬁcation (delay (s)), and the ITR (itr (bits/min)).

The curve direction criterion increases the rejection of epochs that
could be wrongly classiﬁed, it thus signiﬁcantly increases the classiﬁ-
cation accuracy of the online algorithm (70.8713% to 87.379.8%),
while increasing the delay (0.7–1.1 s) before classiﬁcation. When

compared to the state-of-the-art ofﬂine MDRM, the online curve-
based classiﬁcation yields better results in terms of ITR as the delay
before classiﬁcation is much shorter in the latter than the trial length
used in the former; classiﬁcation outputs are reached faster with the
online algorithm. Moreover, the online algorithm can be applied in
both synchronous and asynchronous paradigms, whereas the ofﬂine
algorithms are limited to synchronous paradigms which provide
strongly limited user interaction.

Last, the impact of the Riemannian potato is analyzed. A
bootstrapping with 50 replications was performed on the ofﬂine
data to assess the effect of applying the Riemannian potato. The
results show that for most subjects the results are unchanged
when the Riemannian potato is applied: due to the fact that data
are recorded in a controlled environment, most of them are thus
clean. It does however improve the results of few subjects. It was
then applied in the training phase of the online application, and a
similar observation is made. We can conclude that the Riemannian
potato can be used as a safety guard to ensure that the Riemannian
mean used in the MDRM classiﬁcation scheme is not affected by
outliers, especially for BCI used in less controlled environment.

5.4.3. Online analysis with resting class

Using the MDRM approach it is possible to identify the resting
class. In fact, covariance matrices of signal recorded during resting
periods can be characterized with their own Riemannian mean. As
such, they can be identiﬁed as any other class using the MDRM

66

E.K. Kalunga et al. / Neurocomputing 191 (2016) 55–68

approach. The state-of-the-art methods, Lin et al. [20] and Naka-
nishi et al. [44], are both based on CCA where a reference signal is
needed. These methods do not handle resting class, since there is
no reference signal for them. In this section, the performance of
the proposed approach including the identiﬁcation of the resting
class is presented. Table 3 summarizes the classiﬁer performance
in the same format as Table 2, in terms of classiﬁcation accuracy,
delay before valid classiﬁcation and ITR. Like in Table 2, the best
performance is achieved by the complete online algorithm pre-
ceded with outliers removal with the Riemannian potatoes (i.e.
Online-Potato). The identiﬁcation of the resting class induces a
drop of the overall classiﬁcation accuracy by 8.2%, and a drop of
ITR from 52.5 725.5 to 49.2718.2.

The effect of the resting class is seen with more details in Fig. 7.
Fig. 7a shows the classiﬁcation confusion matrix. There are few mis-
classiﬁcations between SSVEP classes compared to the misclassiﬁca-
tions between the resting class and any SSVEP class: the largest per-
centages are located in the ﬁrst row and the ﬁrst column, apart from
the diagonal block. Fig. 7b displays a ROC curve showing how the
classiﬁer performs in discriminating each class versus the others
depending on the value of the ϑ parameter. On this ROC curve, the
performance of the Online(cid:4)Potato algorithm are indicated in terms of
False Positive Rate (FPR) and True Positive Rate (TPR).

Conﬁrming the observation from the confusion matrix, the ROC
curve indicates that the resting is the most prone to false positive.
Despite the drop in performance, the identiﬁcation of resting class
is crucial for online BCI setup, allowing the subject to use the
system at his own pace.

6. Conclusion

This work investigated the efﬁciency of Riemannian geometry
when dealing with covariance matrices as classiﬁcation features. A
novel algorithm based on MDRM, enhanced by class probability
and the curve direction in the space of covariance EEG signals, was
introduced and applied on an SSVEP classiﬁcation task for a 4-class
brain computer interface. Existing covariance matrix estimators
were investigated and their robustness was assessed on multi-
channel SSVEP signals to ensure that the obtained matrices are
accurate estimates of data covariance, are well conditioned, and
verify the positive-deﬁniteness property. The Schäfer shrinkage
estimator was found to be the best as it yielded the highest clas-
siﬁcation accuracy with the MDRM algorithm.

The MDRM approach is ﬁrst analyzed in a ofﬂine classiﬁcation
setup. To prevent the effect of noisy signals on the MDRM
approach, outliers in the training set of are removed using a
modiﬁed version of the Riemannian potato. This approach is
compared to two CCA-based state-of-the-art methods. The results
show that ofﬂine MDRM achieves better classiﬁcation perfor-
mances than any of the CCA-based methods.

In the online setup, the proposed online algorithm enhances
the stability of the BCI system, balancing between classiﬁcation
speed and prediction accuracy. The evaluation of the classiﬁcation
conﬁdence over several epochs mitigates the short term pertur-
bations in the experimental conditions and the attentional varia-
tions of the subject. The curve direction overcomes the mis-
classiﬁcation of EEG trials that are still synchronized with past
stimuli frequencies at classiﬁcation time.

Unlike the CCA-based state-of-the-art methods considered in this
work, the proposed online algorithm is capable of identifying the
resting periods during an online EEG recording. These resting periods
are considered as an additional class in the classiﬁcation task.

All these contributions help to pave the way towards BCI used

in non-controlled, assistive environment.

Acknowledgement

The authors would like to thank Louis Mayaud from Mensia
Technologies for his contribution in discussions that led to the
completion of this work.

References

[1] J.R. Wolpaw, N. Birbaumer, D.J. McFarland, G. Pfurtscheller, T.M. Vaughan,
Brain–computer interfaces for communication and control, Clin. Neurophysiol.
113 (6) (2002) 767–791.

[2] J.J. Vidal, Toward direct brain–computer communication, Annu. Rev. Biophys.

Bioeng. 2 (1) (1973) 157–180.

[3] J.D. Bayliss, D.H. Ballard, Single trial P3 epoch recognition in a virtual envir-

onment, Neurocomputing 32–33 (2000) 637–642.

[4] W. Tu, S. Sun, A subject transfer framework for EEG classiﬁcation, Neuro-

computing 82 (2012) 109–116.

[5] E. Niedermeyer, F. Lopes da Silva, Electroencephalography: Basic Principles,
Clinical Applications, and Related Fields, 5th Ed., Lippincott Williams &
Wilkins, 2004.

[6] B. Blankertz, K.-R.R. Müller, G. Curio, T.M. Vaughan, G. Schalk, J.R. Wolpaw,
A. Schlögl, C. Neuper, G. Pfurtscheller, T. Hinterberger, M. Schröder,
N. Birbaumer, The BCI competition 2003: progress and perspectives in
detection and discrimination of EEG single trials, IEEE Trans. Biomed. Eng. 51
(6) (2004) 1044–1051.

[7] B. Blankertz, K.R. Muller, D.J. Krusienski, G. Schalk, J.R. Wolpaw, A. Schlogl,
G. Pfurtscheller, J. Millan, M. Schroder, N. Birbaumer, The BCI competition III:
validating alternative approaches to actual BCI problems, IEEE Trans. Neural
Syst. Rehabil. Eng. 14 (2) (2006) 153–159.

[8] M. Tangermann, K.-R. Müller, A. Aertsen, N. Birbaumer, C. Braun, C. Brunner,
R. Leeb, C. Mehring, K.J. Miller, G. Mueller-Putz, G. Nolte, G. Pfurtscheller,
H. Preissl, G. Schalk, A. Schlögl, C. Vidaurre, S. Waldert, B. Blankertz, Review of
the BCI Competition IV, Front. Neurosci. 6 (55). http://dx.doi.org/10.3389/fnins.
2012.00055.

[9] T. Dickhaus, C. Sannelli, K.-R. Müller, G. Curio, B. Blankertz, Predicting BCI

performance to study BCI illiteracy, BMC Neurosci. 10 (Suppl 1) (2009) 1–2.

[10] B.Z. Allison, C. Neuper, Could anyone use a BCI? in: D.S. Nijholt (Eds.), Brain–
Computer Interfaces, Human–Computer Interaction Series, Springer, London,
2010, pp. 35–54 (Chapter 3).

[11] C. Vidaurre, B. Blankertz, Towards a cure for BCI illiteracy, Brain Topogr. 23 (2)

(2010) 194–198.

[12] B. Obermaier, C. Guger, C. Neuper, G. Pfurtscheller, Hidden Markov models for
online classiﬁcation of single trial EEG data, Pattern Recognit. Lett. 22 (12)
(2001) 1299–1309.

[13] A. Lenhardt, M. Kaper, H. Ritter, An adaptive P300-based online brain com-
puter interface, IEEE Trans. Neural Syst. Rehabil. Eng. 16 (2) (2008) 121–130.
[14] L.F. Nicolas-Alonso, R. Corralejo, J. Gomez-Pilar, D. Álvarez, R. Hornero, Adaptive
semi-supervised classiﬁcation to reduce intersession non-stationarity in multi-
class motor imagery-based brain–computer interfaces, Neurocomputing 2016.
http://dx.doi.org/10.1016/j.neucom.2015.02.005.

[15] E. Kalunga, K. Djouani, Y. Hamam, S. Chevallier, E. Monacelli, SSVEP
enhancement based on Canonical Correlation Analysis to improve bci per-
formances, in: IEEE, Africon 2013, pp. 1–5.

[16] H. Lu, H.-L. Eng, C. Guan, K. Plataniotis, A. Venetsanopoulos, Regularized
common spatial pattern with aggregation for EEG classiﬁcation in small-
sample setting, IEEE Trans. Biomed. Eng. 57 (12) (2010) 2936–2946.

[17] B. Blankertz, R. Tomioka, S. Lemm, M. Kawanabe, K.R. Muller, Optimizing
spatial ﬁlters for robust EEG single-trial analysis, IEEE Signal Process. Mag. 25
(1) (2008) 41–56.

[18] F. Lotte, C. Guan, Regularizing common spatial patterns to improve BCI
designs: uniﬁed theory and new algorithms, IEEE Trans. Biomed. Eng. 58 (2)
(2011) 355–362.

[19] Y. Yang, S. Chevallier, J. Wiart, I. Bloch, Automatic selection of the number of
spatial ﬁlters for motor-imagery BCI, in: European Symposium on Artiﬁcial
Neural Networks (ESANN), 2012, pp. 109–114.

[20] Z. Lin, C. Zhang, W. Wu, X. Gao, Frequency recognition based on canonical
correlation analysis for SSVEP-based BCIs, IEEE Trans. Biomed. Eng. 53 (12)
(2006) 2610–2614.

[21] P.-A. Absil, R. Mahony, R. Sepulchre, Optimization Algorithms on Matrix

Manifolds, Princeton University Press, Princeton, 2009.

[22] R. Bhatia, Positive Deﬁnite Matrices, Princeton University Press, Princeton,

2009.

[23] A. Barachant, S. Bonnet, M. Congedo, C. Jutten, Multiclass brain–computer
interface classiﬁcation by Riemannian geometry, IEEE Trans. Biomed. Eng. 59
(4) (2012) 920–928.

[24] A. Barachant, S. Bonnet, M. Congedo, C. Jutten, Classiﬁcation of covariance
for BCI applications, Neuro-

matrices using a Riemannian-based kernel
computing 112 (2013) 172–178.

[25] M. Congedo, A. Barachant, A. Andreev, A new generation of brain–computer
interface based on Riemannian geometry, arXiv preprint arXiv:1310.8115.
[26] E.K. Kalunga, S. Chevallier, O. Rabreau, E. Monacelli, Hybrid interface: inte-
in multimodal human–machine interfaces,
IEEE/ASME

grating BCI

in:

E.K. Kalunga et al. / Neurocomputing 191 (2016) 55–68

67

International Conference on Advanced Intelligent Mechatronics (AIM), 2014,
pp. 530–535.

[55] A. Barachant, S. Bonnet, M. Congedo, C. Jutten, Riemannian geometry applied
in: Latent Variable Analysis and Signal Separation,

to BCI classiﬁcation,
Springer, 2010, pp. 629–636.

[27] X. Gao, D. Xu, M. Cheng, S. Gao, A BCI-based environmental controller for the
motion-disabled, IEEE Trans. Neural Syst. Rehabil. Eng. 11 (2) (2003) 137–140.
[28] G. Edlinger, C. Holzner, C. Guger, A hybrid Brain–Computer interface for smart
home control, in: J.A. Jacko (Ed.), Human–Computer Interaction. Interaction
Techniques and Environments, Lecture Notes in Computer Science, vol. 6762,
Springer, Berlin, Heidelberg, 2011, pp. 417–426.

[29] R.C. Panicker, S. Puthusserypady, Y. Sun, Adaptation in P300 brain–computer
interfaces: a two-classiﬁer cotraining approach, IEEE Trans. Biomed. Eng. 57
(12) (2010) 2927–2935.

[30] F. Schettini, F. Aloise, P. Aricò, S. Salinari, D. Mattia, F. Cincotti, Self-calibration
algorithm in an asynchronous P300-based brain–computer interface, J. Neural
Eng. 11 (3) (2014) 035004.

[31] H. Verschore, P.-J. Kindermans, D. Verstraeten, B. Schrauwen, Dynamic stop-
ping improves the speed and accuracy of a P300 speller, in: Artiﬁcial Neural
Networks and Machine Learning–ICANN 2012, Toronto, Canada, Springer,
2012, pp. 661–668.

[32] D. Regan, Comparison of transient and steady-state methods, Ann. N.Y. Acad.

Sci. 388 (1) (1982) 45–71.

[33] T. Takahashi, K.H. Chiappa, Activation methods, Electroencephalography. in:
Basic Principles, Clinical Applications, and Related Fields, 5th ed., Lippincott
Williams & Wilkins, 2004, pp. 241–262.

[34] E. Niedermeyer, F.L. da Silva, Electroencephalography: Basic Principles, Clinical

Applications, and Related Fields, 5th ed., 2004.

[35] J. Wolpaw, N. Birbaumer, D.J. McFarland, G. Pfurtscheller, T.M. Vaughan, Brain–
computer interfaces for communication and control, Clin. Neurophysiol. 113
(6) (2002) 767–791.

[36] S.T. Morgan, J.C. Hansen, S.A. Hillyard, Selective attention to stimulus location
modulates the steady-state visual evoked potential, Proc. Natl. Acad. Sci. USA
93 (10) (1996) 4770–4774.

[37] M.M. Müller, S. Andersen, N.J. Trujillo, P. Valdés-Sosa, P. Malinowski, S.A. Hillyard,
Feature-selective attention enhances color signals in early visual areas of the
human brain, Proc. Natl. Acad. Sci. USA 103 (38) (2006) 14250–14254.

[38] B. Allison, T. Lüth, D. Valbuena, A. Teymourian, I. Volosyak, A. Gräser, BCI
demographics: how many (and what kinds of) people can use an SSVEP BCI?,
IEEE Trans. Neural Syst. Rehabil. Eng. 18 (2) (2010) 107–116.

[39] D. Zhu, J. Bieger, G.G. Molina, R.M. Aarts, A survey of stimulation methods used
in SSVEP-based BCIs, Intell. Neurosci. (2010), http://dx.doi.org/10.1155/2010/
702357.

[40] C.S. Herrmann, Human EEG responses to 1100 hz ﬂicker: resonance phe-
nomena in visual cortex and their potential correlation to cognitive phe-
nomena, Exp. Brain Res. 137 (2001) 346–353.

[41] M.A. Pastor, J. Artieda, J. Arbizu, M. Valencia, J.C. Masdeu, Human cerebral
activation during steady-state visual-evoked responses, J. Neurosci. 23 (37)
(2003) 11621–11627.

[42] R.S. Fisher, G. Harding, G. Erba, G.L. Barkley, A. Wilkins, Photic- and pattern-
induced seizures: a review for the epilepsy foundation of america working
group, Epilepsia 46 (9) (2005) 1426–1441.

[43] J. Pan, X. Gao, F. Duan, Z. Yan, S. Gao, Enhancing the classiﬁcation accuracy of
steady-state visual evoked potential-based brain–computer interfaces using
phase constrained canonical correlation analysis, J. Neural Eng. 8 (3) (2011)
036027. /http://iopscience.iop.org/article/10.1088/1741-2560/8/3/036027/metaS.
[44] M. Nakanishi, Y. Wang, Y.-T. Wang, Y. Mitsukura, T.-P. Jung, A high-speed brain
speller using steady-state visual evoked potentials, Int. J. Neural Syst. 24 (06)
(2014) 1450019.

[45] M. Spüler, W. Rosenstiel, M. Bogdan, Online adaptation of a c-VEP brain–
computer interface (BCI) based on error-related potentials and unsupervised
learning, PLoS ONE 7 (12) (2012) e51077. /http://journals.plos.org/plosone/
article?id=10.1371/journal.pone.0051077S.

[46] G. Bin, X. Gao, Y. Wang, Y. Li, B. Hong, S. Gao, A high-speed BCI based on code
modulation VEP, J. Neural Eng. 8 (2) (2011) 025015. /http://iopscience.iop.
org/article/10.1088/1741-2560/8/2/025015/metaS.

[47] H. Cecotti, A self-paced and calibration-less SSVEP-based brain–computer
interface speller, IEEE Trans. Neural Syst. Rehabil. Eng. 18 (2) (2010) 127–133.
[48] S. Parini, L. Maggi, A.C. Turconi, G. Andreoni, A robust and self-paced BCI
system based on a four class SSVEP paradigm: algorithms and protocols for a
high-transfer-rate direct brain communication, Intell. Neurosci. (2009), http:
//dx.doi.org/10.1155/2009/864564.

[49] F. Yger, A review of kernels on covariance matrices for BCI applications, in:
IEEE International Workshop on Machine Learning for Signal Processing
(MLSP), 2013, pp. 1–6.

[50] S. Jayasumana, R. Hartley, M. Salzmann, H. Li, M. Harandi, Kernel methods on the
Riemannian manifold of symmetric positive deﬁnite matrices, in: IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR), 2013, pp. 73–80.
[51] Y. Xie, J. Ho, B. Vemuri, On a nonlinear generalization of sparse coding and
dictionary learning, in: International Conference on Machine Learning (ICML),
2013, p. 1480.

[52] A. Goh, R. Vidal, Unsupervised Riemannian clustering of probability density
in: Machine Learning and Knowledge Discovery in Databases,

functions,
Springer, 2008, pp. 377–392.

[53] A. Goh, R. Vidal, Clustering and dimensionality reduction on Riemannian
manifolds, in: IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2008, pp. 1–7.

[54] X. Pennec, P. Fillard, N. Ayache, A Riemannian framework for tensor com-

puting, Int. J. Comput. Vis. 66 (1) (2006) 41–66.

[56] A. Barachant, A. Andreev, M. Congedo, et al., The Riemannian potato: an auto-
matic and adaptive artifact detection method for online experiments using
Riemannian geometry, in: Proceedings of TOBI Workshop IV, 2013, pp. 19–20.
[57] S.-I. Amari, α-divergence is unique, belonging to both f-divergence and
Bregman divergence classes, IEEE Trans. Inf. Theory 55 (11) (2009) 4925–4931.
[58] W. Samek, D. Blythe, K.-R. Müller, M. Kawanabe, Robust spatial ﬁltering with
in: Advances in NeuralInformation Processing Systems

beta divergence,
(NIPS), 2013, pp. 1007–1015.

[59] W. Samek, K.-R. Muller, Information geometry meets BCI spatial ﬁltering using
divergences, in: International Winter Workshop on Brain–Computer Interface,
Seoul, Korea, IEEE, 2014, pp. 1–4.

[60] A. Barachant, S. Bonnet, Channel selection procedure using Riemannian dis-
tance for BCI applications, in: International IEEE/EMBS Conference on Neural
Engineering (NER), 2011, pp. 348–351.

[61] A. Barachant, S. Bonnet, M. Congedo, C. Jutten, Common spatial pattern
revisited by Riemannian geometry, in: IEEE International Workshop on Mul-
timedia Signal Processing (MMSP), 2010, Saint Malo, France, pp. 472–476.
[62] Y. Li, K.M. Wong, H. De Bruin, EEG signal classiﬁcation based on a Riemannian
distance measure, in: Science and Technology for Humanity (TIC-STH), 2009
IEEE Toronto International Conference, IEEE, 2009, Toronto, Canada pp. 268–
273.

[63] Y. Li, K. Wong, H. De Bruin, Electroencephalogram signals classiﬁcation for
sleepstate decision: a Riemannian geometry approach, IET Signal Process. 6
(4) (2012) 288–299.

[64] J. Jost, Riemannian Geometry and Geometric Analysis, vol. 62011, Springer,

New-York, NY, USA, 2011.

[65] M. Moakher, A differential geometric approach to the geometric mean of
symmetric positive-deﬁnite matrices, SIAM J. Matrix Anal. Appl. 26 (3) (2005)
735–747.

[66] P.T. Fletcher, C. Lu, S.M. Pizer, S. Joshi, Principal geodesic analysis for the study
Imag. 23 (8) (2004)

IEEE Trans. Med.

of nonlinear statistics of shape,
995–1005.

[67] K. Fukunaga, Introduction to Statistical Pattern Recognition, Academic Press, San

Diego, CA, USA, 1990.

[68] O. Ledoit, M. Wolf, A well-conditioned estimator for large-dimensional cov-

ariance matrices, J. Multivar. Anal. 88 (2) (2004) 365–411.

[69] B. Blankertz, S. Lemm, M. Treder, S. Haufe, K.-R. Müller, Single-trial analysis
and classiﬁcation of ERP components: a tutorial, NeuroImage 56 (2) (2011)
814–825.

[70] J. Schäfer, K. Strimmer, A shrinkage approach to large-scale covariance matrix
estimation and implications for functional genomics, Stat. Appl. Genet. Mol.
Biol. 4 (1). http://dx.doi.org/10.2202/1544-6115.1175.

[71] F. Pascal, P. Forster, J.P. Ovarlez, P. Arzabal, Theoretical analysis of an improved
covariance matrix estimator in non-gaussian noise, in: IEEE International
Conference on Acoustics, Speech, and Signal Processing (ICASSP), vol. 4, 2005.
[72] M. Congedo, EEG source analysis, Habilitation á diriger des recherches, Uni-

versité de Grenoble, October 2013.

[73] M.J. Pencina, R.B. D'Agostino, R.S. Vasan, Evaluating the added predictive
ability of a new marker: from area under the ROC curve to reclassiﬁcation and
beyond, Stat. Med. 27 (2) (2008) 157–172.

[74] Y. Kimura, T. Tanaka, H. Higashi, N. Morikawa, SSVEP-based brain–computer
interfaces using FSK-modulated visual stimuli, IEEE Trans. Biomed. Eng. 60
(10) (2013) 2831–2838.

[75] F.-B. Vialatte, M. Maurice, J. Dauwels, A. Cichocki, Steady-state visually evoked
potentials: focus on essential paradigms and future perspectives, Prog. Neu-
robiol. 90 (4) (2010) 418–438.

[76] H. Bakardjian, T. Tanaka, A. Cichocki, Optimization of SSVEP brain responses
with application to eight-command brain–computer interface, Neurosci. Lett.
469 (1) (2010) 34–38.

Emmanuel K. Kalunga is a Ph.D. candidate, in his sec-
ond year, in the joint doctorate program between the
Tshwane University of Technology (South Africa) and
Université de Versailles Saint-Quentin, France. On
completion of his Baccalaureus Technologiae (Cum
Laude) in 2010 at the Tshwane University, Emmanuel
was awarded the Mandela Rhodes Scholarship and
pursued a double Masters degree (Mtech/Msc)
in
Control, Image and Signal processing at the same uni-
versity on the topic “Development of brain computer
interface (BCI) based intention detection approach for
persons with limited neuro-muscular control”. He
graduated Cum Laude in September 2013 after spend-
ing a year of internship at the Laboratoire d'ingénierie des systémes de Versailles
(France). Since January 2014, Emmanuel is enrolled for his PhD on the topic of
“Pattern recognition techniques for implicit brain computer interfaces”, under the
supervision of Karim Djouani,
and
Yskandar Hamam.

Sylvain Chevallier,

Eric Monacelli,

Eric Monacelli is an HDR Associate Professor at Ver-
sailles University (UVSQ), working in the LISV labora-
tory. He is working on the development of analysis
methods and experimental devices adapted to the
assistance of speciﬁc end users. His research projects
incorporate issues of man–machine interface, robotics,
assistive
intelligence
and
systems.

technologies

ambient

(AUB)

Yskandar Hamam graduated as a Bachelor of the
American University of Beirut
in 1966. He
obtained his M.Sc. in 1970 and Ph.D. in 1972 from the
University of Manchester Institute of Science and
Technology. He also obtained his “Diplôme d'Habilita-
tion á Diriger des Recherches” (equivalent to D.Sc.)
from the “Université des Sciences et Technologies de
Lille” in 1998. He conducted research activities and
lectured in England, Brazil, Lebanon, Belgium and
France. He was the head of the Control department and
dean of faculty at ESIEE, France. He was an active
member in modelling and simulation societies and was
the president of EUROSIM. He was the Scientiﬁc
Director of the French South African Institute of Technology (F'SATI) at TUT in South
Africa from 2007 to 2012. He is currently professor at the Department of Electrical
Engineering of TUT. He has authored/co-authored about 300 papers in archival
journals and conference proceedings as well as book contributions. He is a senior
member of the IEEE.

68

E.K. Kalunga et al. / Neurocomputing 191 (2016) 55–68

Sylvain Chevallier obtained his Ph.D. in computer sci-
ence at the LIMSICNRS, with a speciality in cognitive
science as he was interested in visual attention process
for robotics. He spent 2 years in the neurocybernetic
team of ETIS-CNRS where he worked on biologically
inspired decision processes and visual perception. He
joined afterwards INRIA Saclay to study deep neural
network and brain computer interface. He pursued this
post-doctoral research theme within Telecom ParisTech
laboratory. In 2011, he was hired as an assistant pro-
fessor in Université de Versailles to work on rehabili-
tation robotics. He is interested in neural communica-
tion and brain-like bioinspired methods for assistive
technologies. He is focusing on brain–computer interfaces using shared control and
passive approaches.

Quentin Barthélemy received the Engineering degree
and the M.Res. in signal and images analysis and pro-
cessing (with distinction)
from Grenoble Institut
National Polytechnique (Grenoble INP), France, both in
2009; and the Ph.D. degree in signal processing from
Grenoble University and CEA-LIST (Alternative Energies
and Atomic Energy Commission), France, in 2013. His
Ph.D. dissertation deals with sparse representations for
multivariate signals,
including invariances as shift,
rotation and afﬁne transformation. He joined Mensia
Technologies at the Institut du Cerveau et de la Moelle
Epiniére, Paris, France, in 2013, to develop signal pro-
cessing and machine learning methods for real-time
EEG analysis. His research interests are sparse representation, time-frequency
analysis, source separation and Riemannian geometry.

Karim Djouani is a professor, scientist and technical
group supervisor of soft computing, telecommunica-
tion, networking systems and Robotics. Since January
2011 he is Full professor at University Paris Est-Creteil
(UPEC), France and Tshwane University of Technology,
Pretoria, South Africa. From July 2008 to December
2010, he was seconded by the French Ministry of
Higher Education to the French South African Institute
of Technology (F'SATI) at Tshwane University of Tech-
nology (TUT), Pretoria, South Africa. He was also
national and European projects manager at the LISSI
Lab.His current works focus on the development of
novel and highly efﬁcient algorithms for reasoning
systems with uncertainty as well as optimization, for distributed systems, net-
worked control systems, wireless ad-hoc network, wireless and mobile commu-
nication, and wireless sensors networks as well as Robotics. He has authored/co-
authored over 150 articles in archival journals and conference proceedings as well
as ﬁve chapters in edited books.

