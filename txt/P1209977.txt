Hybrid Recommender System based on Autoencoders

Florian Strub
ﬂorian.strub@inria.fr

J´er´emie Mary
jeremie.mary@univ-lille3.fr

Romaric Gaudel
romaric.gaudel@univ-lille3.fr

Univ. Lille, CNRS, Centrale Lille, Inria

Univ. Lille, CNRS, Centrale Lille, Inria

Univ. Lille, CNRS, Centrale Lille, Inria

7
1
0
2
 
c
e
D
 
9
2
 
 
]

G
L
.
s
c
[
 
 
3
v
9
5
6
7
0
.
6
0
6
1
:
v
i
X
r
a

Abstract

Proﬁcient Recommender Systems heavily rely on
Matrix Factorization (MF) techniques. MF aims
at reconstructing a matrix of ratings from an in-
complete and noisy initial matrix; this prediction is
then used to build the actual recommendation. Si-
multaneously, Neural Networks (NN) met tremen-
dous successes in the last decades but few attempts
have been made to perform recommendation with
In this paper, we gather the best
autoencoders.
practice from the literature to achieve this goal.
We ﬁrst highlight the link between these autoen-
coder based approaches and MF. Then, we reﬁne
the training approach of autoencoders to handle in-
complete data. Second, we design an end-to-end
system which handles external information. Fi-
nally, we empirically evaluate these approaches on
the MovieLens and Douban dataset.

1 Introduction

Recommendation systems advise users on which items
(movies, music, books etc.) they are more likely to be inter-
ested in. A good recommendation system may dramatically
increase the number of sales of a ﬁrm or retain customers.
For instance, 80% of movies watched on Netﬂix come from
the recommender system of the company [Gomez-Uribe and
Hunt, 2015]. Collaborative Filtering (CF) aims at recom-
mending an item to a user by predicting how a user would
rate this item. To do so, the feedback of one user on some
items is combined with the feedback of all other users on all
items to predict a new rating. For instance, if someone rated
a few books, CF objective is to estimate the ratings he would
have given to thousands of other books by using the ratings of
all the other readers. The most successful approach in CF is to
factorize an incomplete matrix of ratings [Koren et al., 2009;
Zhou et al., 2008]. This approach simultaneously learns a
representation of users and items that encapsulate the taste,
genre, writing styles, etc. A well-known limit of this ap-
proach is the cold start setting: how to recommend an item to
a user when few rating exists for either the user or the item?
While Deep Learning methods start to be used for several
scenarios in recommendation system that goes from dialogue

systems [Wen et al., 2016] to temporal user-item interac-
tions [Dai et al., 2016] through heterogeneous classiﬁcation
applied to the recommendation setting [Cheng et al., 2016],
few attempts have been done to use Neural Networks (NN) in
CF. Deep Learning key successes have mainly been on fully
observable data [LeCun et al., 2015] while CF relies on in-
put with missing values. This constraint has received less at-
tention and remains a challenging problem for NN. Handling
missing values for CF has ﬁrst been tackled by Salakhutdi-
nov et al. [Salakhutdinov et al., 2007] for the Netﬂix chal-
lenge by using Restricted Boltzmann Machines. More recent
works train autoencoders to perform CF tasks on explicit data
[Sedhain et al., 2015; Strub et al., 2016] and implicit data
[Zheng et al., 2016]. Other approaches rely on recurrent net-
works [Wang et al., 2016]. Although those methods report
excellent results, they ignore the cold start scenario and pro-
vide little insights.

The key contribution of this paper is to collect the best
practices of autoencoder approaches in order to standardize
the use of autoencoders for CF tasks. We ﬁrst highlight
autoencoders perform a factorization of the matrix of rat-
ings. Then we contribute to an end-to-end autoencoder based
approach in two points: (i) we introduce a proper training
loss/process of autoencoders on incomplete data, (ii) we in-
tegrate the side information to autoencoders to alleviate the
cold start problem. Compared to previous attempts in that
direction [Salakhutdinov et al., 2007; Sedhain et al., 2015;
Wu et al., 2016], our framework integrates both ratings and
side information into a unique network. This joint model
leads to a scalable and robust approach which beats state-
of-the-art results in CF. Reusable source code is provided in
Lua/Torch to reproduce the results.

The paper is organized as follows. First, Sec. 2 ﬁxes the
setting and highlights the link between autoencoders and ma-
trix factorization in the context of CF. Then, Sec. 3 describes
our model. Finally, Sec. 4 details several experimental results
from our approach.

2 State of the art

2.1 Matrix Factorization based Collaborative

Filtering

One of the most successful approach of CF consists of com-
pleting the matrix of ratings through Matrix Factorization

(MF) [Koren et al., 2009]. Given N users and M items, we
denote rij the rating given by the ith user for the jth item.
It entails an incomplete matrix of ratings R ∈ RN ×M . MF
aims at ﬁnding a rank k matrix (cid:98)R ∈ RN ×M which matches
known values of R and predicts unknown ones. Typically,
(cid:98)R = UVT with U ∈ RN ×k, V ∈ RM ×k and (U ,V) the
solution of

arg min
U,V

(cid:88)

(i,j)∈K(R)

(rij − uT

i vj)2 + λ((cid:107)ui(cid:107)2

F + (cid:107)vj(cid:107)2

F ),

where K(R) is the set of indices of known ratings of R, (ui,
vj) are column-vectors of the low rank rows of (U, V) and
(cid:107).(cid:107)F is the Frobenius norm. In the following, ri,. and r.,j will
respectively be the i-th row and j-th column of R.

2.2 Autoencoder based Collaborative Filtering
Recently, autoencoders have been used to handle CF prob-
lems [Sedhain et al., 2015; Strub et al., 2016]. Autoencoders
are NN popularized by Kramer [Kramer, 1991].They are un-
supervised networks where the output of the network aims
at reconstructing the input. The network is trained by back-
propagating the squared error loss on the output. More specif-
ically, when the network limits itself to one hidden layer, its
output is

nn(x) def= σ(W2σ(W1x + b1) + b2),
with x ∈ RN the input, W1 ∈ Rk×N and W2 ∈ RN ×k the
weight matrices, b1 ∈ Rk and b2 ∈ RN the bias vectors, and
σ(.) a non-linear transfer function.

In the context of CF, the autoencoder is fed with incom-
plete rows ri,. (resp. columns r.,j) of R [Sedhain et al., 2015;
Strub et al., 2016]. It then outputs a vector ˆri,. (resp. ˆr.,j)
which predict the missing entries. Note that these approaches
perform a non-linear low-rank approximation of R. Using
MF notations and assuming that the network works on rows
ri,. of R, we recover a predicted vector ˆri,. of the form
ˆri,. = σ (Vui):

ˆri,. = nn(ri,.) = σ








[W2 IN ]
(cid:124)
(cid:123)(cid:122)
(cid:125)
V

(cid:20) σ(W1ri,. + b1)
b2
(cid:123)(cid:122)
ui

(cid:124)

(cid:21)

(cid:125)








.

The activation of the hidden units of the autoencoder ui itera-
tively builds the low rank matrix U. Besides, the ﬁnal matrix
of weights corresponds to the low rank matrix V. In the end,
the output of the autoencoder performs a non linear matrix
factorization ˆR = σ (cid:0)UVT (cid:1). Identically, it is possible to it-
erate over the columns r.,j of R to compute ˆr.,j and get the
ﬁnal matrix ˆR.

vectors. The task with missing data is all the more difﬁcult as
the missing data have an impact on both input and target vec-
tors. Miranda et al. [Miranda et al., 2012] study the impact
of missing values on autoencoders with industrial data. Yet,
they only have 5% of missing values in her dataset, whereas
CF tasks usually have more than 95% of missing values.

Autorec [Sedhain et al., 2015] handles the missing values
by associating one autoencoder per sample, whose input size
matches the number of known values. The corresponding
weight matrices are shared among the autoencoders. Even
if this approach is intellectually appealing, it faces techni-
cal limitations. First, sharing weights among networks pre-
vent efﬁcient computations (especially on GPU). Secondly, it
prevents gradient optimization methods such as momentum,
weight decay, etc. from being applied to missing data. In the
rest of the paper, we introduce a new method based on a sin-
gle autoencoder to fully regain the beneﬁts of NN techniques.
Although this architecture is mathematically equivalent to a
mixture of autoencoders [Sedhain et al., 2015], it turns out to
be a more ﬂexible framework.

CF systems also face the cold start problem. The main
solution is to supplant the lack of ratings by integrating side
information. Some approaches [Burke, 2002] mix CF with a
second system based only on side information. Recent work
tends to incorporate the side information into the matrix com-
pletion by modifying the training error [Adams et al., 2010;
Chen et al., 2012; Rendle, 2010; Porteous et al., 2010]. In
this line of research, some papers incorporate NN embedding
on side information. For instance, [Wang et al., 2014] re-
spectively auto-encode bag-of-words from movie plots, [Li et
al., 2015] auto-encode heterogeneous side information from
users and items. Finally, [Wang and Wang, 2014] uses con-
volutional networks on music samples. From the best of our
knowledge, training a NN from end to end on both ratings
and heterogeneous side information has never been done. In
Sec. 3.2, we build such NN by integrating side information
into our CF system.

3 End-to-End Collaborative Filtering with

Autoencoders

Our approach builds upon an autoencoder to predict full vec-
tors ˆri,./ˆr.,j from incomplete vectors ri,./r.,j. As in [Salakhut-
dinov and Mnih, 2008; Sedhain et al., 2015; Strub et al.,
2016], we deﬁne two types of autoencoders: U-CFN is de-
ﬁned as ˆri,. = nn(ri,.) and predicts the missing ratings given
by the users; I-CFN is deﬁned as ˆr.,j = nn(r.,j) and pre-
dicts the missing ratings given to the items. First, we design
a training process to predict missing ratings from incomplete
vectors. Secondly, we extend CF techniques using side infor-
mation to autoencoders to improve predictions for users/items
with few ratings.

2.3 Challenges
While the link between MF and autoencoders is straightfor-
ward, there is no guarantee that autoencoders can success-
fully perform matrix completion from incomplete data. In-
deed, most of the prominent results with autoencoders such
as word2vec [Mikolov et al., 2013] only deal with complete

3.1 Handling Incomplete Data
MC tasks introduce two major difﬁculties for autoencoders.
The training process must handle incomplete input/target vec-
tors. The other challenge is to predict missing ratings as op-
posed to the initial goal of autoencoders to rebuild the initial
input. We handle those obstacles by two means.

Figure 1: Training steps for autoencoders with incomplete data. The input is drawn from the matrix of ratings, unknown values
are turned to zero, some ratings are masked (corruption) and a dense estimate is obtained. Before backpropagation, unknown
ratings are turned to zero error, prediction errors are reweighted by α and reconstruction errors are reweighted by β.

First, we inhibit input and output nodes corresponding to
missing data. For input nodes, the inhibition is obtained by
setting their value to zero. To inhibit the back-propagated un-
known errors, we use an empirical loss that disregards the loss
of unknown values. No error is back-propagated for missing
values, while the error is back-propagated for actual zero val-
ues.

Second, we shake the training data and design a speciﬁc
loss function to enforce reconstruction of missing values. In-
deed, basic autoencoders loss function only aims at recon-
structing the initial input. Such approach misses the point
that CF goal is to predict missing ratings. Worse, there is
little interest to reconstruct already known ratings. To in-
crease the generalization properties of autoencoders, we ap-
ply the Denoising AutoEncoder (DAE) approach [Vincent et
al., 2008] to the CF task we handle. The DAE approach cor-
rupts the input vectors and lets the network denoise the out-
puts. The corresponding loss function reﬂects that objective
and is based on two main hyperparameters: α and β. They
balance whether the network would focus on denoising the
input (α) or reconstructing the input (β). In our context, the
data are corrupted by masking a small fraction of the known
rating. This corruption simulates missing values in the train-
ing process to train the autoencoders to predict them. We also
set α > β to emphasize the prediction of missing ratings over
the reconstruction of known ratings. In overall, the training
loss with regularization is:

Lα,β(x, ˜x) = α

(cid:88)

(nn(˜x)j − xj)2

 +



j∈K(x)∩C(˜x)



β



(nn(˜x)j − xj)2


 + λ(cid:107)W(cid:107)2
F ,

(1)

j∈K(x)\C(˜x)





(cid:88)

where ˜x is the corrupted input, C contains the indices of cor-
rupted elements in ˜x, K(x) contains the indices of known val-
ues of x, W is the ﬂatten vector of weights of the network and
λ is the regularization parameter. Note that the regularization
is applied to the full matrix of weights as opposed to [Sed-
hain et al., 2015]. The overall training process is described in
Fig. 1.

Integrating Side Information

3.2
CF only relies on the feedback of the users regarding a set
of items. However, adding more information may help to in-
crease the prediction accuracy and robustness. Furthermore,
CF suffers from the cold start problem: when little informa-
tion is available on an item, it will greatly lower the prediction
accuracy. We now integrate side information to CFN to alle-
viate the cold start scenario.

The simplest approach to integrate side information to MF
techniques is to append a user/item bias to the rating predic-
tion [Koren et al., 2009]: ˆrij = uT
i vj + bu,i + bv,j + b(cid:48),
where bu,i, bv,j, b(cid:48) are respectively the user, item, and global
bias. These biases are computed with hand-crafted engineer-
ing or CF technique [Chen et al., 2012; Porteous et al., 2010;
Rendle, 2010]. For instance, side information can directly be
concatenated to the feature vector ui/vj of rank k. Therefore,
the estimated rating is computed by:

ˆrij = {ui, xi} ⊗ {vj, yj}
def=uT
[1:k],iv[1:k],j + xT
(cid:124)

i v[k+1:k+P ],j
(cid:123)(cid:122)
(cid:125)
bu,i

+ uT
(cid:124)

,

[k+1:k+Q],iyj
(cid:125)
(cid:123)(cid:122)
bv,j

(2)

(3)

where xi ∈ RP and yj ∈ RQ encodes user/item side infor-
mation.

Unfortunately, those methods cannot be directly applied to
NNs as autoencoders optimize U and V independently. How-
ever, it is possible to retrieve similar equations by (i) append-
ing side information to incomplete input vectors, (ii) inject-
ing side information to every layer of the autoencoders as in
Fig. 2. By example, with U-CFN we get the following output:

(cid:122)
σ(W(cid:48)
nn({ri,., xi}) = σ(V(cid:48) {

1{ri,., xi} + b1), xi} + b2)

u(cid:48)
i
(cid:125)(cid:124)

= σ(V(cid:48)

[1:k]u(cid:48)

i + V(cid:48)
(cid:124)

[k+1:k+P ]xi
(cid:123)(cid:122)
(cid:125)
bu,i

(cid:123)

+

),

b2
(cid:124)(cid:123)(cid:122)(cid:125)
[bv,1...bv,M ]T
(4)

where V(cid:48) ∈ R(N ×k+P ) is a weight matrix, V(cid:48)
[1:k] ∈
RN ×k, V(cid:48)
[k+1:k+P ] ∈ RN ×P are respectively the submatri-
ces of V(cid:48) that contain the columns from 1 to k and k + 1 to
k +P . Injecting side information to the last hidden layer as in
Eq. 4 enables to partially retrieve the error function of classic

Figure 2: Side information is wired to all the neuron.

hybrid systems described in Eq. 2. Secondly, injecting side
information to the other intermediate hidden layers enhance
the internal representation. Finally, appending side informa-
tion to the input vectors supplants the absence of input data
when no rating is available. The autoencoder ﬁts CF stan-
dards while being trained end-to-end by backpropagation.

4 Experiments
In this section, we empirically evaluate CFN on two major
CF datasets: MovieLens and Douban. We ﬁrst describe the
experimental settings before introducing the benchmark mod-
els. Finally, we provide an extensive analysis of the results.

4.1 Experimental Setting
Experiments are conducted on MovieLens and Douban
datasets as they are among the biggest CF dataset freely avail-
able at the time of writing. Besides, MovieLens contains
side information on the items and it is a widespread bench-
mark while Douban provides side information on the users.
The MovieLens-1M, MovieLens-10M and MovieLens-20M
respectively provide 1/10/20 million discrete ratings from
6/72/138 thousands users on 4/10/27 thousands movies. Side
information for MovieLens-1M is the age, sex and gender of
the user and the movie category (action, thriller etc.). Side in-
formation for MovieLens-10/20M is a matrix of tags T where
Tij is the occurrence of the jth tag for the ith movie and the
movie category. No side information is provided for users.
The Douban dataset [Ma et al., 2011] provides 17 million
discrete ratings from 129 thousand users on 58 thousands
movies. Side information is the bi-directional user/friend
relations for the user. The user/friend relations are treated
like the matrix of tags from MovieLens. No side informa-
tion is provided for items. Finally, we perform 90%-10%
train-test sets as reported in [Lee et al., 2013; Li et al., 2016;
Zheng et al., 2016].

Preprocessing For each dataset, we ﬁrst centered the rat-
ings to zero-mean by row (resp. by col) for U-CFN (resp
I-CFN): denoting bi the mean of the ith user and bj the mean
of the jth item, U-CFN and I-CFN respectively learn from
runbiased
= rij − bj. Then all the
ij
ratings are linearly rescaled from -1 to 1 to ﬁt the output range
of the autoencoder transfer functions. Theses operations are
ﬁnally reversed while evaluating the ﬁnal matrix.

= rij − bi and runbiased

ij

of the matrix of tags/friendships. To do so, we use a low
rank matrix factorization in our experiments. For a dif-
ferent nature of data as texts or pictures (which are not
available in our dataset), CFN can directly learn this em-
bedding as [Wang and Wang, 2014; Wang et al., 2014;
Li et al., 2015]. Formally, we use the left part of a matrix
factorization of the tag matrix T. From T = PDQT with D
the diagonal matrix of eigenvalues sorted in descending order,
the movie tags are represented by Y = PJ×K(cid:48)D0.5
K(cid:48)×K(cid:48) with
K (cid:48) the number of kept eigenvectors. Binary representation
such as the movie category is concatenated to Y.

Error Function The algorithms are compared based on
their respective Root Mean Square Error (RMSE) on test data.
Denoting Rtest the matrix of test ratings and (cid:98)R the full ma-
trix returned by the learning algorithm, the RMSE is:

L( (cid:98)R, Rtest) =

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
|K(Rtest)|

(cid:88)

(rtest

ij − ˆrij)2,

(i,j)∈K(Rtest)

(5)
where |K(Rtest)| is the number of ratings in the testing
dataset. Note that in the case of autoencoders (cid:98)R is computed
by feeding the network with training data. As such, ˆrij stands
for nn(ˆrtrain

)j for U-CFN, and nn(ˆrtrain

)i for I-CFN.

.,j

i,.

Training Settings We train a one-hidden layer autoen-
coders with hyperbolic tangent transfer functions. The layers
have 600 hidden neurons. Weights are randomly initialized
with a uniform law Wij ∼ U [−1/
n]. The latent
dimension of the low rank matrix of tags/friendships is set to
50. Hyperparamenters were are ﬁne-tuned by a genetic algo-
rithm and the ﬁnal learning rate, learning decay and weight
decay are respectively set to 0.7, 0.3 and 0.5. α, β and mask-
ing ratio are set to 1, 0.5 and 0.25.

n, 1/

√

√

Source code
In order to ensure easy reproducibility and
reuse the experimental results, we provide the code in an out-
of-the-box tutorial in Torch. 1.

4.2 Benchmark Models
We benchmark CFN with ﬁve matrix completion algorithms:
• ALS-WR (Alternating Least Squares with Weighted-λ-
Regularization) [Zhou et al., 2008] solves the low-rank
MF problem by alternatively ﬁxing U and V and solving
the resulting linear regression problem. Experiments are
run with the Apache Mahout Software 2 with a rank of
200;

• SVDFeature [Chen et al., 2012] learns a feature-based
MF : side information is used to predict the bias term
and to reweight the matrix factorization. We use a rank
of 64 and tune other hyperparameters by random search;
• BPMF (Bayesian Probabilistic Matrix Factorization) in-
fers the matrix decomposition after a statistical model.

Side Information As the dimensionality of the side in-
formation is huge, we ﬁrst perform a dimension reduction

1 https://github.com/fstrub95/Autoencoders_cf
2 http://mahout.apache.org/

Algorithms
BPMF
ALS-WR
SVDFeature
LLORMA
I-Autorec
U-CFN
U-CFN++
I-CFN
I-CFN++

MovieLens-1M MovieLens-10M MovieLens-20M
0.8123 ± 3.5e-4
0.8213 ± 6.5e-4
0.8705 ± 4.3e-3
0.7746 ± 2.7e-4
0.7830 ± 1.9e-4
0.8433 ± 1.8e-3
0.7852 ± 5.4e-4
0.7907 ± 8.4e-4
0.8631 ± 2.5e-3
0.7843 ± 3.2e-4
0.7949 ± 2.3e-4
0.8371 ± 2.4e-3
0.7742 ± 4.4e-4
0.7831 ± 2.4e-4
0.8305 ± 2.8e-3
0.7856 ± 1.4e-4
0.7954 ± 7.4e-4
0.8574 ± 2.4e-3
0.8572 ± 1.6e-3
N/A
N/A
0.7663 ± 2.9e-4
0.7767 ± 5.4e-4
0.8321 ± 2.5e-3
0.7652 ± 2.3e-4
0.7754 ± 6.3e-4
0.8316 ± 1.9e-3

Douban
0.7133 ± 3.0e-4
0.7010 ± 3.2e-4
*
0.6968 ± 2.7e-4
0.6945 ± 3.1e-4
0.7049 ± 2.2e-4
0.7050 ± 1.2e-4
0.6911 ± 3.2e-4
N/A

Table 1: RMSE on MovieLens-10M (90%/10%). The ++ sufﬁx denotes when side information is added to CFN.

As a bayesian algorithm, the performances can be im-
proved by the ﬁne tuning of priors over the parameters
Here, we use the recommendations of [Salakhutdinov
and Mnih, 2008] for priors and rank (set to 10).

• LLORMA estimates the rating matrix as a weighted
sum of low-rank matrices. Experiments are run with the
Prea API3. We use a rank of 20, 30 anchor points which
entail a global pseudo-rank of 600. Other hyperparame-
ters are picked as recommended in [Lee et al., 2013];
• I-Autorec [Sedhain et al., 2015] trains one autoencoder
per item, sharing the weights between the different au-
toencoders. We use 600 hidden neurons with the training
hyperparameters recommended by the author.

In every scenario, we select the highest possible rank which
does not lead to overﬁtting despite a strong regularization.
For instance, increasing the rank of BPMF does not sig-
niﬁcantly increase the ﬁnal RMSE, idem for SVDFeature.
Similar benchmarks exist in the literature [Lee et al., 2013;
Li et al., 2016; Zheng et al., 2016].

4.3 General Results
Comparison to state-of-the-art Tab. 1 summarizes the
RMSE on MovieLens and Douban datasets. Conﬁdence in-
tervals correspond to a 95% range.
I-CFNs have excellent
performance for every dataset we run. It is competitive com-
pared to the state-of-the-art CF algorithms and outperforms
them for MovieLens-10M. To the best of our knowledge, the
best result published for MovieLens-10M (without side infor-
mation) has a ﬁnal RMSE of 0.7682 [Li et al., 2016]. Note
that I-CFN outperforms U-CFN as shown in Fig. 5. It sug-
gests that the structure of the items is stronger than the one
on the users i.e. it is easier to guess tastes based on movies
you liked than to ﬁnd users similar to you. Yet, this behavior
could be different on another dataset. Finally, I-CFN out-
performs Autorec on big dataset while both methods rely on
autoencoder. In addition to the DAE loss, CFN beneﬁts from
regularizing missing ratings during the training as described
in Eq. 1. Thus, uncommon ratings are more regularized and
they turn out to be less overﬁtted.

Impact of side information At ﬁrst sight at Tab. 1, the use
of side information has a limited impact on the RMSE. This
statement has to be mitigated: as the repartition of known

3 http://prea.gatech.edu/

Figure 3: Impact of the denoising loss in the training pro-
cess. α = 1 is kept constant while varying the reconstruction
hyperparameter β and the masking ratio .

entries in the dataset is not uniform, the estimates are bi-
ased towards users and items with a lot of ratings. For the-
ses users and movies, the dataset already contains a lot of
information, thus having some extra information will have a
marginal effect. Users and items with few ratings should ben-
eﬁt more from some side information but the estimation bias
hides them. In order to exhibit the utility of side information,
we report in Tab. 2 the RMSE conditionally to the number of
missing values for items. As expected, the fewer number of
ratings for an item, the more important is the side informa-
tion. This is very desirable for a real system: the effective use
of side information to the new items is crucial to deal with the
ﬂow of new products. A more careful analysis of the RMSE
improvement in this setting shows that the improvement is
uniformly distributed over the users whatever their number of
ratings. This corresponds to the fact that the available side
information is only about items. Finally, we train I-CFN on
MovieLens-10M (90%/10%) with either the movie genre or
the matrix of tags. Individually picked, side information in-
creases the global RMSE by 0.10% while concatenating them
increases the ﬁnal score by 0.14%. Therefore, I-CFN handles
the heterogeneity of side information.

Impact of the loss The DAE loss positively impacts the
ﬁnal RMSE on big dataset as highlighted in Fig. 3 when
carefully balanced. Surprisingly, the autoencoder has already
good generalization properties while only focusing on the re-
construction criterion (no masking). More importantly, the
reconstruction criterion cannot be discarded (β = 0) to learn
an efﬁcient representation.

Figure 4: RMSE vs training set ratio for MovieLens-10M. Training hyperpa-
rameters are kept constant across dataset. CFN and I-Autorec are very robust
to a change in the density while SVDFeature must be reﬁned each time.

Figure 5: RMSE by epoch for CFN for
MovieLens-10M (90%/10%).

Interval
0.0-0.2
0.2-0.4
0.4-0.6
0.6-0.8
0.8-1.0
Full

Interval
0.0-0.2
0.2-0.4
0.4-0.6
0.6-0.8
0.8-1.0
Full

I-CFN I-CFN++ %Improv.
1.0030
0.9188
0.8748
0.8473
0.7976
0.8075

0.9938
0.9084
0.8669
0.8420
0.7964
0.8055

0.96
1.15
0.91
0.63
0.15
0.25

(a) MovieLens-10M (50%/50%)

I-CFN I-CFN++ %Improv.
0.9539
0.8815
0.8487
0.8139
0.7674
0.7767

0.9444
0.8730
0.8408
0.8110
0.7669
0.7756

1.01
0.96
0.95
0.35
0.06
0.14

(b) MovieLens-10M (90%/10%)

Table 2: RMSE computed by clusters of items sorted by the
number of ratings on MovieLens-10M (90%/10%). For in-
stance, the ﬁrst cluster contains the 20% of items with the
lowest number of ratings.

Impact of the non-linearity We removed the non-linearity
from I-CFN to study its relative impact. For fairness, we kept
α, β, the masking ratio and the number of hidden neurons
constant. We ﬁne-tune the learning rates and weight decay.
For MovieLens-10M, we obtain a ﬁnal RMSE of 0.8151 ±
1.4e-3 which is far worse than classic non-linear I-CFN.

Impact of the training ratio CFN remains very robust to
a variation of data density as shown in Fig. 4. It is all the
more impressive that hyperparameters are ﬁrst optimized for
movieLens-10M (90%/10%). Cold-start and warm-start sce-
nario are also far more well-handled by NNs than more clas-
sic CF algorithms.

4.4 CFN Tractability
One major problem faced by CF algorithms is scalability as
CF datasets contain hundred of thousands of users and items.

Model Dataset

# Param

U-CFN

I-CFN

MLens-1M
MLens-10M
MLens-20M
MLens-1M
MLens-10M
MLens-20M

Time
5M 7m17s
15M 34m51s
38M 59m35s
8M 2m03s
100M 18m34s
194M 34m45s

Memory
262MiB
543MiB
1,044Mib
250MiB
1,532MiB
2,905MiB

Table 3: Training time and memory footprint for a 2-layers
CFN on GTX 980 for 20 epochs.

Recent advances in GPU computation managed to reduce the
training time of NNs by several orders of magnitude. CFN
(and ALS-WR) fully beneﬁts from those advances and is
trained within a few minutes as shown in Tab. 3. On the other
side, CF gradient based methods such as SVDFeature [Chen
et al., 2012] cannot be easily parallelized or used on GPU as
they are mainly iterative. Similarly, Autorec [Sedhain et al.,
2015] suffers from synchronization and memory fetching la-
tencies because of the shared weights among autoencoders.
Furthermore, CFN has the key advantage to provide excellent
performance while being able to reﬁne its prediction on the
ﬂy for new ratings. Thus, U-CFN/I-CFN does not need to be
retrained for new items/users.

5 Conclusion
In this paper, we highlight the connections between autoen-
coders and matrix factorization for matrix completion. We
pack some modern training techniques - as well than some
code - able to defeat state of the art methods while remaining
scalable. Moreover, we propose a systematic way to integrate
side information without the need to combine two separate
systems. To some extent, this work extends the construction
of embeddings by neural networks to the collaborative ﬁlter-
ing setting. A natural follow-up is to work with deeper ar-
chitectures using batch normalization [Sergey and Szegedy,
2015], adaptive gradient methods (such as ADAM [Kingma
and Ba, 2014]) or residual networks [He et al., 2015]. Other
extensions could use recurrent networks to grasp the sequen-
tial aspect of the collection of ratings.

Acknowledgements
The authors would like to acknowledge the stimulating en-
vironment provided by SequeL research group, Inria and
CRIStAL. This work was supported by French Ministry
of Higher Education and Research, by CPER Nord-Pas de
Calais / FEDER DATA Advanced data science and technolo-
gies 2015-2020, the Projet CHIST-ERA IGLU and by FUI
Herm`es. Experiments presented in this paper were carried
out using Grid’5000 testbed, hosted by Inria and supported by
CNRS, RENATER and several Universities as well as other
organizations.

References
[Adams et al., 2010] R. P. Adams, G. E. Dahland, and I. Murray.
Incorporating side information in probabilistic matrix factoriza-
tion with gaussian processes. In Proc. of UAI, 2010.

[Burke, 2002] R. Burke. Hybrid recommender systems: Survey
and experiments. User modeling and user-adapted interaction,
12(4):331–370, 2002.

[Chen et al., 2012] T. Chen, W. Zhang, Q. Lu, K. Chen, Z. Zheng,
and Y. Yong. Svdfeature: a toolkit for feature-based collaborative
ﬁltering. JMLR, 13(1):3619–3622, 2012.

[Cheng et al., 2016] H. Cheng, L. Koc, J. Harmsen, T. Shaked,
T. Chandra, H. Aradhye, G. Anderson, G. Corrado, W. Chai,
M. Ispir, et al. Wide & deep learning for recommender systems.
In Recsys DLRS workshop, 2016.

[Dai et al., 2016] H. Dai, Y. Wang, R. Trivedi, and L. Song. Recur-
rent Coevolutionary Feature Embedding Processes for Recom-
mendation. 2016.

[Gomez-Uribe and Hunt, 2015] C. Gomez-Uribe and N. Hunt. The
netﬂix recommender system: Algorithms, business value, and in-
novation. ACM Trans. Manage. Inf. Syst., 6(4):13:1–13:19, 2015.

[He et al., 2015] K. He, X. Zhang, S. Ren, and J. Sun. Deep resid-
ual learning for image recognition. In Proc. of CVPR, 2015.

[Kingma and Ba, 2014] Diederik Kingma and Jimmy Ba. Adam:
arXiv preprint
stochastic optimization.

A method for
arXiv:1412.6980, 2014.

[Koren et al., 2009] Y. Koren, R. Bell, and C. Volinsky. Matrix
factorization techniques for recommender systems. Computer,
42(8):30–37, 2009.

[Kramer, 1991] M. A. Kramer. Nonlinear principal component
analysis using autoassociative neural networks. AIChE journal,
37(2):233–243, 1991.

[LeCun et al., 2015] Y. LeCun, Y. Bengio, and G. Hinton. Deep

learning. Nature, 521(7553), 2015.

[Lee et al., 2013] J. Lee, S. Kim, G. Lebanon, and Y. Singerm. Lo-
cal low-rank matrix approximation. In Proc. of ICML, 2013.

[Li et al., 2015] S. Li, J. Kawale, and Y. Fu. Deep collaborative
In Proc. of

ﬁltering via marginalized denoising auto-encoder.
CIKM, 2015.

[Li et al., 2016] D. Li, C. Chen, Q. Lv, J. Yan, L. Shang, and S. Chu.
Low-rank matrix approximation with stability. In Proc. of ICML,
2016.

[Ma et al., 2011] H. Ma, D. Zhou, C. Liu, M. R. Lyu, and I. King.
In Proc. of

Recommender systems with social regularization.
WSDM, 2011.

[Mikolov et al., 2013] T. Mikolov, I. Sutskever, K. Chen, G. Cor-
rado, and J. Dean. Distributed representations of words and
phrases and their compositionality. In Proc. of NIPS, 2013.
[Miranda et al., 2012] V. Miranda, J. Krstulovic, H. Keko, C. Mor-
eira, and J. Pereira. Reconstructing Missing Data in State Estima-
tion With Autoencoders. IEEE Transactions on Power Systems,
27(2):604–611, 2012.

[Porteous et al., 2010] I. Porteous, A. U. Asuncion,

and
M. Welling. Bayesian matrix factorization with side infor-
In Proc. of AAAI,
mation and dirichlet process mixtures.
2010.

[Rendle, 2010] S. Rendle. Factorization machines.

In Proc. of

ICDM, 2010.

[Salakhutdinov and Mnih, 2008] R. Salakhutdinov and A. Mnih.
Bayesian probabilistic matrix factorization using markov chain
monte carlo. In Proc. of ICML, 2008.

[Salakhutdinov et al., 2007] R. Salakhutdinov, A. Mnih,

and
G. Hinton. Restricted boltzmann machines for collaborative ﬁl-
tering. In Proc. of ICML. ACM, 2007.

[Sedhain et al., 2015] S. Sedhain, A. K. Menon, S. Sanner, and
L. Xie. Autorec: Autoencoders meet collaborative ﬁltering. In
Proc. of WWW, 2015.

[Sergey and Szegedy, 2015] I. Sergey and C. Szegedy. Batch nor-
malization: Accelerating deep network training by reducing in-
ternal covariate shift. In Proc. of ICML, 2015.

[Strub et al., 2016] F. Strub, R. Gaudel, and J. Mary. Hybrid rec-
ommender system based on autoencoders. In Recsys DLRS work-
shop, 2016.

[Vincent et al., 2008] P. Vincent, H. Larochelle, Y. Bengio Yoshua,
and P. Manzagol. Extracting and composing robust features with
denoising autoencoders. In Proc. of ICML, 2008.

[Wang and Wang, 2014] X. Wang and Y. Wang. Improving content-
based and hybrid music recommendation using deep learning. In
Proc. of the ACM Int. Conf. on Multimedia, 2014.

[Wang et al., 2014] H. Wang, N. Wang, and D.-Y. Yeung. Collabo-
rative deep learning for recommender systems. In Proc. of KDD,
2014.

[Wang et al., 2016] H. Wang, X. Shi, and D. Yeung. Collaborative
Recurrent Autoencoder: Recommend while Learning to Fill in
the Blanks. In Proc of NIPS, 2016.

[Wen et al., 2016] T. Wen, D. Vandyke, N. Mrksic, M. Gasic,
L. Rojas-Barahona, P. Su, S. Ultes, and S. Young. A network-
based end-to-end trainable task-oriented dialogue system. arXiv
preprint arXiv:1604.04562, 2016.

[Wu et al., 2016] Y. Wu, C. DuBois, A. Zheng, and M. Ester. Col-
laborative denoising auto-encoders for top-n recommender sys-
tems. In Proc. of WSDM, 2016.

[Zheng et al., 2016] Y. Zheng, C. Liu, B. Tang, and H. Zhou. Neu-
ral autoregressive collaborative ﬁltering for implicit feedback. In
Proc. of ICML, 2016.

[Zhou et al., 2008] Y. Zhou, D. Wilkinson, R. Schreiber, and
R. Pan. Large-scale parallel collaborative ﬁltering for the netﬂix
prize. In Proc. of AAIM. Springer, 2008.

Hybrid Recommender System based on Autoencoders

Florian Strub
ﬂorian.strub@inria.fr

J´er´emie Mary
jeremie.mary@univ-lille3.fr

Romaric Gaudel
romaric.gaudel@univ-lille3.fr

Univ. Lille, CNRS, Centrale Lille, Inria

Univ. Lille, CNRS, Centrale Lille, Inria

Univ. Lille, CNRS, Centrale Lille, Inria

7
1
0
2
 
c
e
D
 
9
2
 
 
]

G
L
.
s
c
[
 
 
3
v
9
5
6
7
0
.
6
0
6
1
:
v
i
X
r
a

Abstract

Proﬁcient Recommender Systems heavily rely on
Matrix Factorization (MF) techniques. MF aims
at reconstructing a matrix of ratings from an in-
complete and noisy initial matrix; this prediction is
then used to build the actual recommendation. Si-
multaneously, Neural Networks (NN) met tremen-
dous successes in the last decades but few attempts
have been made to perform recommendation with
In this paper, we gather the best
autoencoders.
practice from the literature to achieve this goal.
We ﬁrst highlight the link between these autoen-
coder based approaches and MF. Then, we reﬁne
the training approach of autoencoders to handle in-
complete data. Second, we design an end-to-end
system which handles external information. Fi-
nally, we empirically evaluate these approaches on
the MovieLens and Douban dataset.

1 Introduction

Recommendation systems advise users on which items
(movies, music, books etc.) they are more likely to be inter-
ested in. A good recommendation system may dramatically
increase the number of sales of a ﬁrm or retain customers.
For instance, 80% of movies watched on Netﬂix come from
the recommender system of the company [Gomez-Uribe and
Hunt, 2015]. Collaborative Filtering (CF) aims at recom-
mending an item to a user by predicting how a user would
rate this item. To do so, the feedback of one user on some
items is combined with the feedback of all other users on all
items to predict a new rating. For instance, if someone rated
a few books, CF objective is to estimate the ratings he would
have given to thousands of other books by using the ratings of
all the other readers. The most successful approach in CF is to
factorize an incomplete matrix of ratings [Koren et al., 2009;
Zhou et al., 2008]. This approach simultaneously learns a
representation of users and items that encapsulate the taste,
genre, writing styles, etc. A well-known limit of this ap-
proach is the cold start setting: how to recommend an item to
a user when few rating exists for either the user or the item?
While Deep Learning methods start to be used for several
scenarios in recommendation system that goes from dialogue

systems [Wen et al., 2016] to temporal user-item interac-
tions [Dai et al., 2016] through heterogeneous classiﬁcation
applied to the recommendation setting [Cheng et al., 2016],
few attempts have been done to use Neural Networks (NN) in
CF. Deep Learning key successes have mainly been on fully
observable data [LeCun et al., 2015] while CF relies on in-
put with missing values. This constraint has received less at-
tention and remains a challenging problem for NN. Handling
missing values for CF has ﬁrst been tackled by Salakhutdi-
nov et al. [Salakhutdinov et al., 2007] for the Netﬂix chal-
lenge by using Restricted Boltzmann Machines. More recent
works train autoencoders to perform CF tasks on explicit data
[Sedhain et al., 2015; Strub et al., 2016] and implicit data
[Zheng et al., 2016]. Other approaches rely on recurrent net-
works [Wang et al., 2016]. Although those methods report
excellent results, they ignore the cold start scenario and pro-
vide little insights.

The key contribution of this paper is to collect the best
practices of autoencoder approaches in order to standardize
the use of autoencoders for CF tasks. We ﬁrst highlight
autoencoders perform a factorization of the matrix of rat-
ings. Then we contribute to an end-to-end autoencoder based
approach in two points: (i) we introduce a proper training
loss/process of autoencoders on incomplete data, (ii) we in-
tegrate the side information to autoencoders to alleviate the
cold start problem. Compared to previous attempts in that
direction [Salakhutdinov et al., 2007; Sedhain et al., 2015;
Wu et al., 2016], our framework integrates both ratings and
side information into a unique network. This joint model
leads to a scalable and robust approach which beats state-
of-the-art results in CF. Reusable source code is provided in
Lua/Torch to reproduce the results.

The paper is organized as follows. First, Sec. 2 ﬁxes the
setting and highlights the link between autoencoders and ma-
trix factorization in the context of CF. Then, Sec. 3 describes
our model. Finally, Sec. 4 details several experimental results
from our approach.

2 State of the art

2.1 Matrix Factorization based Collaborative

Filtering

One of the most successful approach of CF consists of com-
pleting the matrix of ratings through Matrix Factorization

(MF) [Koren et al., 2009]. Given N users and M items, we
denote rij the rating given by the ith user for the jth item.
It entails an incomplete matrix of ratings R ∈ RN ×M . MF
aims at ﬁnding a rank k matrix (cid:98)R ∈ RN ×M which matches
known values of R and predicts unknown ones. Typically,
(cid:98)R = UVT with U ∈ RN ×k, V ∈ RM ×k and (U ,V) the
solution of

arg min
U,V

(cid:88)

(i,j)∈K(R)

(rij − uT

i vj)2 + λ((cid:107)ui(cid:107)2

F + (cid:107)vj(cid:107)2

F ),

where K(R) is the set of indices of known ratings of R, (ui,
vj) are column-vectors of the low rank rows of (U, V) and
(cid:107).(cid:107)F is the Frobenius norm. In the following, ri,. and r.,j will
respectively be the i-th row and j-th column of R.

2.2 Autoencoder based Collaborative Filtering
Recently, autoencoders have been used to handle CF prob-
lems [Sedhain et al., 2015; Strub et al., 2016]. Autoencoders
are NN popularized by Kramer [Kramer, 1991].They are un-
supervised networks where the output of the network aims
at reconstructing the input. The network is trained by back-
propagating the squared error loss on the output. More specif-
ically, when the network limits itself to one hidden layer, its
output is

nn(x) def= σ(W2σ(W1x + b1) + b2),
with x ∈ RN the input, W1 ∈ Rk×N and W2 ∈ RN ×k the
weight matrices, b1 ∈ Rk and b2 ∈ RN the bias vectors, and
σ(.) a non-linear transfer function.

In the context of CF, the autoencoder is fed with incom-
plete rows ri,. (resp. columns r.,j) of R [Sedhain et al., 2015;
Strub et al., 2016]. It then outputs a vector ˆri,. (resp. ˆr.,j)
which predict the missing entries. Note that these approaches
perform a non-linear low-rank approximation of R. Using
MF notations and assuming that the network works on rows
ri,. of R, we recover a predicted vector ˆri,. of the form
ˆri,. = σ (Vui):

ˆri,. = nn(ri,.) = σ








[W2 IN ]
(cid:124)
(cid:123)(cid:122)
(cid:125)
V

(cid:20) σ(W1ri,. + b1)
b2
(cid:123)(cid:122)
ui

(cid:124)

(cid:21)

(cid:125)








.

The activation of the hidden units of the autoencoder ui itera-
tively builds the low rank matrix U. Besides, the ﬁnal matrix
of weights corresponds to the low rank matrix V. In the end,
the output of the autoencoder performs a non linear matrix
factorization ˆR = σ (cid:0)UVT (cid:1). Identically, it is possible to it-
erate over the columns r.,j of R to compute ˆr.,j and get the
ﬁnal matrix ˆR.

vectors. The task with missing data is all the more difﬁcult as
the missing data have an impact on both input and target vec-
tors. Miranda et al. [Miranda et al., 2012] study the impact
of missing values on autoencoders with industrial data. Yet,
they only have 5% of missing values in her dataset, whereas
CF tasks usually have more than 95% of missing values.

Autorec [Sedhain et al., 2015] handles the missing values
by associating one autoencoder per sample, whose input size
matches the number of known values. The corresponding
weight matrices are shared among the autoencoders. Even
if this approach is intellectually appealing, it faces techni-
cal limitations. First, sharing weights among networks pre-
vent efﬁcient computations (especially on GPU). Secondly, it
prevents gradient optimization methods such as momentum,
weight decay, etc. from being applied to missing data. In the
rest of the paper, we introduce a new method based on a sin-
gle autoencoder to fully regain the beneﬁts of NN techniques.
Although this architecture is mathematically equivalent to a
mixture of autoencoders [Sedhain et al., 2015], it turns out to
be a more ﬂexible framework.

CF systems also face the cold start problem. The main
solution is to supplant the lack of ratings by integrating side
information. Some approaches [Burke, 2002] mix CF with a
second system based only on side information. Recent work
tends to incorporate the side information into the matrix com-
pletion by modifying the training error [Adams et al., 2010;
Chen et al., 2012; Rendle, 2010; Porteous et al., 2010]. In
this line of research, some papers incorporate NN embedding
on side information. For instance, [Wang et al., 2014] re-
spectively auto-encode bag-of-words from movie plots, [Li et
al., 2015] auto-encode heterogeneous side information from
users and items. Finally, [Wang and Wang, 2014] uses con-
volutional networks on music samples. From the best of our
knowledge, training a NN from end to end on both ratings
and heterogeneous side information has never been done. In
Sec. 3.2, we build such NN by integrating side information
into our CF system.

3 End-to-End Collaborative Filtering with

Autoencoders

Our approach builds upon an autoencoder to predict full vec-
tors ˆri,./ˆr.,j from incomplete vectors ri,./r.,j. As in [Salakhut-
dinov and Mnih, 2008; Sedhain et al., 2015; Strub et al.,
2016], we deﬁne two types of autoencoders: U-CFN is de-
ﬁned as ˆri,. = nn(ri,.) and predicts the missing ratings given
by the users; I-CFN is deﬁned as ˆr.,j = nn(r.,j) and pre-
dicts the missing ratings given to the items. First, we design
a training process to predict missing ratings from incomplete
vectors. Secondly, we extend CF techniques using side infor-
mation to autoencoders to improve predictions for users/items
with few ratings.

2.3 Challenges
While the link between MF and autoencoders is straightfor-
ward, there is no guarantee that autoencoders can success-
fully perform matrix completion from incomplete data. In-
deed, most of the prominent results with autoencoders such
as word2vec [Mikolov et al., 2013] only deal with complete

3.1 Handling Incomplete Data
MC tasks introduce two major difﬁculties for autoencoders.
The training process must handle incomplete input/target vec-
tors. The other challenge is to predict missing ratings as op-
posed to the initial goal of autoencoders to rebuild the initial
input. We handle those obstacles by two means.

Figure 1: Training steps for autoencoders with incomplete data. The input is drawn from the matrix of ratings, unknown values
are turned to zero, some ratings are masked (corruption) and a dense estimate is obtained. Before backpropagation, unknown
ratings are turned to zero error, prediction errors are reweighted by α and reconstruction errors are reweighted by β.

First, we inhibit input and output nodes corresponding to
missing data. For input nodes, the inhibition is obtained by
setting their value to zero. To inhibit the back-propagated un-
known errors, we use an empirical loss that disregards the loss
of unknown values. No error is back-propagated for missing
values, while the error is back-propagated for actual zero val-
ues.

Second, we shake the training data and design a speciﬁc
loss function to enforce reconstruction of missing values. In-
deed, basic autoencoders loss function only aims at recon-
structing the initial input. Such approach misses the point
that CF goal is to predict missing ratings. Worse, there is
little interest to reconstruct already known ratings. To in-
crease the generalization properties of autoencoders, we ap-
ply the Denoising AutoEncoder (DAE) approach [Vincent et
al., 2008] to the CF task we handle. The DAE approach cor-
rupts the input vectors and lets the network denoise the out-
puts. The corresponding loss function reﬂects that objective
and is based on two main hyperparameters: α and β. They
balance whether the network would focus on denoising the
input (α) or reconstructing the input (β). In our context, the
data are corrupted by masking a small fraction of the known
rating. This corruption simulates missing values in the train-
ing process to train the autoencoders to predict them. We also
set α > β to emphasize the prediction of missing ratings over
the reconstruction of known ratings. In overall, the training
loss with regularization is:

Lα,β(x, ˜x) = α

(cid:88)

(nn(˜x)j − xj)2

 +



j∈K(x)∩C(˜x)



β



(nn(˜x)j − xj)2


 + λ(cid:107)W(cid:107)2
F ,

(1)

j∈K(x)\C(˜x)





(cid:88)

where ˜x is the corrupted input, C contains the indices of cor-
rupted elements in ˜x, K(x) contains the indices of known val-
ues of x, W is the ﬂatten vector of weights of the network and
λ is the regularization parameter. Note that the regularization
is applied to the full matrix of weights as opposed to [Sed-
hain et al., 2015]. The overall training process is described in
Fig. 1.

Integrating Side Information

3.2
CF only relies on the feedback of the users regarding a set
of items. However, adding more information may help to in-
crease the prediction accuracy and robustness. Furthermore,
CF suffers from the cold start problem: when little informa-
tion is available on an item, it will greatly lower the prediction
accuracy. We now integrate side information to CFN to alle-
viate the cold start scenario.

The simplest approach to integrate side information to MF
techniques is to append a user/item bias to the rating predic-
tion [Koren et al., 2009]: ˆrij = uT
i vj + bu,i + bv,j + b(cid:48),
where bu,i, bv,j, b(cid:48) are respectively the user, item, and global
bias. These biases are computed with hand-crafted engineer-
ing or CF technique [Chen et al., 2012; Porteous et al., 2010;
Rendle, 2010]. For instance, side information can directly be
concatenated to the feature vector ui/vj of rank k. Therefore,
the estimated rating is computed by:

ˆrij = {ui, xi} ⊗ {vj, yj}
def=uT
[1:k],iv[1:k],j + xT
(cid:124)

i v[k+1:k+P ],j
(cid:123)(cid:122)
(cid:125)
bu,i

+ uT
(cid:124)

,

[k+1:k+Q],iyj
(cid:125)
(cid:123)(cid:122)
bv,j

(2)

(3)

where xi ∈ RP and yj ∈ RQ encodes user/item side infor-
mation.

Unfortunately, those methods cannot be directly applied to
NNs as autoencoders optimize U and V independently. How-
ever, it is possible to retrieve similar equations by (i) append-
ing side information to incomplete input vectors, (ii) inject-
ing side information to every layer of the autoencoders as in
Fig. 2. By example, with U-CFN we get the following output:

(cid:122)
σ(W(cid:48)
nn({ri,., xi}) = σ(V(cid:48) {

1{ri,., xi} + b1), xi} + b2)

u(cid:48)
i
(cid:125)(cid:124)

= σ(V(cid:48)

[1:k]u(cid:48)

i + V(cid:48)
(cid:124)

[k+1:k+P ]xi
(cid:123)(cid:122)
(cid:125)
bu,i

(cid:123)

+

),

b2
(cid:124)(cid:123)(cid:122)(cid:125)
[bv,1...bv,M ]T
(4)

where V(cid:48) ∈ R(N ×k+P ) is a weight matrix, V(cid:48)
[1:k] ∈
RN ×k, V(cid:48)
[k+1:k+P ] ∈ RN ×P are respectively the submatri-
ces of V(cid:48) that contain the columns from 1 to k and k + 1 to
k +P . Injecting side information to the last hidden layer as in
Eq. 4 enables to partially retrieve the error function of classic

Figure 2: Side information is wired to all the neuron.

hybrid systems described in Eq. 2. Secondly, injecting side
information to the other intermediate hidden layers enhance
the internal representation. Finally, appending side informa-
tion to the input vectors supplants the absence of input data
when no rating is available. The autoencoder ﬁts CF stan-
dards while being trained end-to-end by backpropagation.

4 Experiments
In this section, we empirically evaluate CFN on two major
CF datasets: MovieLens and Douban. We ﬁrst describe the
experimental settings before introducing the benchmark mod-
els. Finally, we provide an extensive analysis of the results.

4.1 Experimental Setting
Experiments are conducted on MovieLens and Douban
datasets as they are among the biggest CF dataset freely avail-
able at the time of writing. Besides, MovieLens contains
side information on the items and it is a widespread bench-
mark while Douban provides side information on the users.
The MovieLens-1M, MovieLens-10M and MovieLens-20M
respectively provide 1/10/20 million discrete ratings from
6/72/138 thousands users on 4/10/27 thousands movies. Side
information for MovieLens-1M is the age, sex and gender of
the user and the movie category (action, thriller etc.). Side in-
formation for MovieLens-10/20M is a matrix of tags T where
Tij is the occurrence of the jth tag for the ith movie and the
movie category. No side information is provided for users.
The Douban dataset [Ma et al., 2011] provides 17 million
discrete ratings from 129 thousand users on 58 thousands
movies. Side information is the bi-directional user/friend
relations for the user. The user/friend relations are treated
like the matrix of tags from MovieLens. No side informa-
tion is provided for items. Finally, we perform 90%-10%
train-test sets as reported in [Lee et al., 2013; Li et al., 2016;
Zheng et al., 2016].

Preprocessing For each dataset, we ﬁrst centered the rat-
ings to zero-mean by row (resp. by col) for U-CFN (resp
I-CFN): denoting bi the mean of the ith user and bj the mean
of the jth item, U-CFN and I-CFN respectively learn from
runbiased
= rij − bj. Then all the
ij
ratings are linearly rescaled from -1 to 1 to ﬁt the output range
of the autoencoder transfer functions. Theses operations are
ﬁnally reversed while evaluating the ﬁnal matrix.

= rij − bi and runbiased

ij

of the matrix of tags/friendships. To do so, we use a low
rank matrix factorization in our experiments. For a dif-
ferent nature of data as texts or pictures (which are not
available in our dataset), CFN can directly learn this em-
bedding as [Wang and Wang, 2014; Wang et al., 2014;
Li et al., 2015]. Formally, we use the left part of a matrix
factorization of the tag matrix T. From T = PDQT with D
the diagonal matrix of eigenvalues sorted in descending order,
the movie tags are represented by Y = PJ×K(cid:48)D0.5
K(cid:48)×K(cid:48) with
K (cid:48) the number of kept eigenvectors. Binary representation
such as the movie category is concatenated to Y.

Error Function The algorithms are compared based on
their respective Root Mean Square Error (RMSE) on test data.
Denoting Rtest the matrix of test ratings and (cid:98)R the full ma-
trix returned by the learning algorithm, the RMSE is:

L( (cid:98)R, Rtest) =

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
|K(Rtest)|

(cid:88)

(rtest

ij − ˆrij)2,

(i,j)∈K(Rtest)

(5)
where |K(Rtest)| is the number of ratings in the testing
dataset. Note that in the case of autoencoders (cid:98)R is computed
by feeding the network with training data. As such, ˆrij stands
for nn(ˆrtrain

)j for U-CFN, and nn(ˆrtrain

)i for I-CFN.

.,j

i,.

Training Settings We train a one-hidden layer autoen-
coders with hyperbolic tangent transfer functions. The layers
have 600 hidden neurons. Weights are randomly initialized
with a uniform law Wij ∼ U [−1/
n]. The latent
dimension of the low rank matrix of tags/friendships is set to
50. Hyperparamenters were are ﬁne-tuned by a genetic algo-
rithm and the ﬁnal learning rate, learning decay and weight
decay are respectively set to 0.7, 0.3 and 0.5. α, β and mask-
ing ratio are set to 1, 0.5 and 0.25.

n, 1/

√

√

Source code
In order to ensure easy reproducibility and
reuse the experimental results, we provide the code in an out-
of-the-box tutorial in Torch. 1.

4.2 Benchmark Models
We benchmark CFN with ﬁve matrix completion algorithms:
• ALS-WR (Alternating Least Squares with Weighted-λ-
Regularization) [Zhou et al., 2008] solves the low-rank
MF problem by alternatively ﬁxing U and V and solving
the resulting linear regression problem. Experiments are
run with the Apache Mahout Software 2 with a rank of
200;

• SVDFeature [Chen et al., 2012] learns a feature-based
MF : side information is used to predict the bias term
and to reweight the matrix factorization. We use a rank
of 64 and tune other hyperparameters by random search;
• BPMF (Bayesian Probabilistic Matrix Factorization) in-
fers the matrix decomposition after a statistical model.

Side Information As the dimensionality of the side in-
formation is huge, we ﬁrst perform a dimension reduction

1 https://github.com/fstrub95/Autoencoders_cf
2 http://mahout.apache.org/

Algorithms
BPMF
ALS-WR
SVDFeature
LLORMA
I-Autorec
U-CFN
U-CFN++
I-CFN
I-CFN++

MovieLens-1M MovieLens-10M MovieLens-20M
0.8123 ± 3.5e-4
0.8213 ± 6.5e-4
0.8705 ± 4.3e-3
0.7746 ± 2.7e-4
0.7830 ± 1.9e-4
0.8433 ± 1.8e-3
0.7852 ± 5.4e-4
0.7907 ± 8.4e-4
0.8631 ± 2.5e-3
0.7843 ± 3.2e-4
0.7949 ± 2.3e-4
0.8371 ± 2.4e-3
0.7742 ± 4.4e-4
0.7831 ± 2.4e-4
0.8305 ± 2.8e-3
0.7856 ± 1.4e-4
0.7954 ± 7.4e-4
0.8574 ± 2.4e-3
0.8572 ± 1.6e-3
N/A
N/A
0.7663 ± 2.9e-4
0.7767 ± 5.4e-4
0.8321 ± 2.5e-3
0.7652 ± 2.3e-4
0.7754 ± 6.3e-4
0.8316 ± 1.9e-3

Douban
0.7133 ± 3.0e-4
0.7010 ± 3.2e-4
*
0.6968 ± 2.7e-4
0.6945 ± 3.1e-4
0.7049 ± 2.2e-4
0.7050 ± 1.2e-4
0.6911 ± 3.2e-4
N/A

Table 1: RMSE on MovieLens-10M (90%/10%). The ++ sufﬁx denotes when side information is added to CFN.

As a bayesian algorithm, the performances can be im-
proved by the ﬁne tuning of priors over the parameters
Here, we use the recommendations of [Salakhutdinov
and Mnih, 2008] for priors and rank (set to 10).

• LLORMA estimates the rating matrix as a weighted
sum of low-rank matrices. Experiments are run with the
Prea API3. We use a rank of 20, 30 anchor points which
entail a global pseudo-rank of 600. Other hyperparame-
ters are picked as recommended in [Lee et al., 2013];
• I-Autorec [Sedhain et al., 2015] trains one autoencoder
per item, sharing the weights between the different au-
toencoders. We use 600 hidden neurons with the training
hyperparameters recommended by the author.

In every scenario, we select the highest possible rank which
does not lead to overﬁtting despite a strong regularization.
For instance, increasing the rank of BPMF does not sig-
niﬁcantly increase the ﬁnal RMSE, idem for SVDFeature.
Similar benchmarks exist in the literature [Lee et al., 2013;
Li et al., 2016; Zheng et al., 2016].

4.3 General Results
Comparison to state-of-the-art Tab. 1 summarizes the
RMSE on MovieLens and Douban datasets. Conﬁdence in-
tervals correspond to a 95% range.
I-CFNs have excellent
performance for every dataset we run. It is competitive com-
pared to the state-of-the-art CF algorithms and outperforms
them for MovieLens-10M. To the best of our knowledge, the
best result published for MovieLens-10M (without side infor-
mation) has a ﬁnal RMSE of 0.7682 [Li et al., 2016]. Note
that I-CFN outperforms U-CFN as shown in Fig. 5. It sug-
gests that the structure of the items is stronger than the one
on the users i.e. it is easier to guess tastes based on movies
you liked than to ﬁnd users similar to you. Yet, this behavior
could be different on another dataset. Finally, I-CFN out-
performs Autorec on big dataset while both methods rely on
autoencoder. In addition to the DAE loss, CFN beneﬁts from
regularizing missing ratings during the training as described
in Eq. 1. Thus, uncommon ratings are more regularized and
they turn out to be less overﬁtted.

Impact of side information At ﬁrst sight at Tab. 1, the use
of side information has a limited impact on the RMSE. This
statement has to be mitigated: as the repartition of known

3 http://prea.gatech.edu/

Figure 3: Impact of the denoising loss in the training pro-
cess. α = 1 is kept constant while varying the reconstruction
hyperparameter β and the masking ratio .

entries in the dataset is not uniform, the estimates are bi-
ased towards users and items with a lot of ratings. For the-
ses users and movies, the dataset already contains a lot of
information, thus having some extra information will have a
marginal effect. Users and items with few ratings should ben-
eﬁt more from some side information but the estimation bias
hides them. In order to exhibit the utility of side information,
we report in Tab. 2 the RMSE conditionally to the number of
missing values for items. As expected, the fewer number of
ratings for an item, the more important is the side informa-
tion. This is very desirable for a real system: the effective use
of side information to the new items is crucial to deal with the
ﬂow of new products. A more careful analysis of the RMSE
improvement in this setting shows that the improvement is
uniformly distributed over the users whatever their number of
ratings. This corresponds to the fact that the available side
information is only about items. Finally, we train I-CFN on
MovieLens-10M (90%/10%) with either the movie genre or
the matrix of tags. Individually picked, side information in-
creases the global RMSE by 0.10% while concatenating them
increases the ﬁnal score by 0.14%. Therefore, I-CFN handles
the heterogeneity of side information.

Impact of the loss The DAE loss positively impacts the
ﬁnal RMSE on big dataset as highlighted in Fig. 3 when
carefully balanced. Surprisingly, the autoencoder has already
good generalization properties while only focusing on the re-
construction criterion (no masking). More importantly, the
reconstruction criterion cannot be discarded (β = 0) to learn
an efﬁcient representation.

Figure 4: RMSE vs training set ratio for MovieLens-10M. Training hyperpa-
rameters are kept constant across dataset. CFN and I-Autorec are very robust
to a change in the density while SVDFeature must be reﬁned each time.

Figure 5: RMSE by epoch for CFN for
MovieLens-10M (90%/10%).

Interval
0.0-0.2
0.2-0.4
0.4-0.6
0.6-0.8
0.8-1.0
Full

Interval
0.0-0.2
0.2-0.4
0.4-0.6
0.6-0.8
0.8-1.0
Full

I-CFN I-CFN++ %Improv.
1.0030
0.9188
0.8748
0.8473
0.7976
0.8075

0.9938
0.9084
0.8669
0.8420
0.7964
0.8055

0.96
1.15
0.91
0.63
0.15
0.25

(a) MovieLens-10M (50%/50%)

I-CFN I-CFN++ %Improv.
0.9539
0.8815
0.8487
0.8139
0.7674
0.7767

0.9444
0.8730
0.8408
0.8110
0.7669
0.7756

1.01
0.96
0.95
0.35
0.06
0.14

(b) MovieLens-10M (90%/10%)

Table 2: RMSE computed by clusters of items sorted by the
number of ratings on MovieLens-10M (90%/10%). For in-
stance, the ﬁrst cluster contains the 20% of items with the
lowest number of ratings.

Impact of the non-linearity We removed the non-linearity
from I-CFN to study its relative impact. For fairness, we kept
α, β, the masking ratio and the number of hidden neurons
constant. We ﬁne-tune the learning rates and weight decay.
For MovieLens-10M, we obtain a ﬁnal RMSE of 0.8151 ±
1.4e-3 which is far worse than classic non-linear I-CFN.

Impact of the training ratio CFN remains very robust to
a variation of data density as shown in Fig. 4. It is all the
more impressive that hyperparameters are ﬁrst optimized for
movieLens-10M (90%/10%). Cold-start and warm-start sce-
nario are also far more well-handled by NNs than more clas-
sic CF algorithms.

4.4 CFN Tractability
One major problem faced by CF algorithms is scalability as
CF datasets contain hundred of thousands of users and items.

Model Dataset

# Param

U-CFN

I-CFN

MLens-1M
MLens-10M
MLens-20M
MLens-1M
MLens-10M
MLens-20M

Time
5M 7m17s
15M 34m51s
38M 59m35s
8M 2m03s
100M 18m34s
194M 34m45s

Memory
262MiB
543MiB
1,044Mib
250MiB
1,532MiB
2,905MiB

Table 3: Training time and memory footprint for a 2-layers
CFN on GTX 980 for 20 epochs.

Recent advances in GPU computation managed to reduce the
training time of NNs by several orders of magnitude. CFN
(and ALS-WR) fully beneﬁts from those advances and is
trained within a few minutes as shown in Tab. 3. On the other
side, CF gradient based methods such as SVDFeature [Chen
et al., 2012] cannot be easily parallelized or used on GPU as
they are mainly iterative. Similarly, Autorec [Sedhain et al.,
2015] suffers from synchronization and memory fetching la-
tencies because of the shared weights among autoencoders.
Furthermore, CFN has the key advantage to provide excellent
performance while being able to reﬁne its prediction on the
ﬂy for new ratings. Thus, U-CFN/I-CFN does not need to be
retrained for new items/users.

5 Conclusion
In this paper, we highlight the connections between autoen-
coders and matrix factorization for matrix completion. We
pack some modern training techniques - as well than some
code - able to defeat state of the art methods while remaining
scalable. Moreover, we propose a systematic way to integrate
side information without the need to combine two separate
systems. To some extent, this work extends the construction
of embeddings by neural networks to the collaborative ﬁlter-
ing setting. A natural follow-up is to work with deeper ar-
chitectures using batch normalization [Sergey and Szegedy,
2015], adaptive gradient methods (such as ADAM [Kingma
and Ba, 2014]) or residual networks [He et al., 2015]. Other
extensions could use recurrent networks to grasp the sequen-
tial aspect of the collection of ratings.

Acknowledgements
The authors would like to acknowledge the stimulating en-
vironment provided by SequeL research group, Inria and
CRIStAL. This work was supported by French Ministry
of Higher Education and Research, by CPER Nord-Pas de
Calais / FEDER DATA Advanced data science and technolo-
gies 2015-2020, the Projet CHIST-ERA IGLU and by FUI
Herm`es. Experiments presented in this paper were carried
out using Grid’5000 testbed, hosted by Inria and supported by
CNRS, RENATER and several Universities as well as other
organizations.

References
[Adams et al., 2010] R. P. Adams, G. E. Dahland, and I. Murray.
Incorporating side information in probabilistic matrix factoriza-
tion with gaussian processes. In Proc. of UAI, 2010.

[Burke, 2002] R. Burke. Hybrid recommender systems: Survey
and experiments. User modeling and user-adapted interaction,
12(4):331–370, 2002.

[Chen et al., 2012] T. Chen, W. Zhang, Q. Lu, K. Chen, Z. Zheng,
and Y. Yong. Svdfeature: a toolkit for feature-based collaborative
ﬁltering. JMLR, 13(1):3619–3622, 2012.

[Cheng et al., 2016] H. Cheng, L. Koc, J. Harmsen, T. Shaked,
T. Chandra, H. Aradhye, G. Anderson, G. Corrado, W. Chai,
M. Ispir, et al. Wide & deep learning for recommender systems.
In Recsys DLRS workshop, 2016.

[Dai et al., 2016] H. Dai, Y. Wang, R. Trivedi, and L. Song. Recur-
rent Coevolutionary Feature Embedding Processes for Recom-
mendation. 2016.

[Gomez-Uribe and Hunt, 2015] C. Gomez-Uribe and N. Hunt. The
netﬂix recommender system: Algorithms, business value, and in-
novation. ACM Trans. Manage. Inf. Syst., 6(4):13:1–13:19, 2015.

[He et al., 2015] K. He, X. Zhang, S. Ren, and J. Sun. Deep resid-
ual learning for image recognition. In Proc. of CVPR, 2015.

[Kingma and Ba, 2014] Diederik Kingma and Jimmy Ba. Adam:
arXiv preprint
stochastic optimization.

A method for
arXiv:1412.6980, 2014.

[Koren et al., 2009] Y. Koren, R. Bell, and C. Volinsky. Matrix
factorization techniques for recommender systems. Computer,
42(8):30–37, 2009.

[Kramer, 1991] M. A. Kramer. Nonlinear principal component
analysis using autoassociative neural networks. AIChE journal,
37(2):233–243, 1991.

[LeCun et al., 2015] Y. LeCun, Y. Bengio, and G. Hinton. Deep

learning. Nature, 521(7553), 2015.

[Lee et al., 2013] J. Lee, S. Kim, G. Lebanon, and Y. Singerm. Lo-
cal low-rank matrix approximation. In Proc. of ICML, 2013.

[Li et al., 2015] S. Li, J. Kawale, and Y. Fu. Deep collaborative
In Proc. of

ﬁltering via marginalized denoising auto-encoder.
CIKM, 2015.

[Li et al., 2016] D. Li, C. Chen, Q. Lv, J. Yan, L. Shang, and S. Chu.
Low-rank matrix approximation with stability. In Proc. of ICML,
2016.

[Ma et al., 2011] H. Ma, D. Zhou, C. Liu, M. R. Lyu, and I. King.
In Proc. of

Recommender systems with social regularization.
WSDM, 2011.

[Mikolov et al., 2013] T. Mikolov, I. Sutskever, K. Chen, G. Cor-
rado, and J. Dean. Distributed representations of words and
phrases and their compositionality. In Proc. of NIPS, 2013.
[Miranda et al., 2012] V. Miranda, J. Krstulovic, H. Keko, C. Mor-
eira, and J. Pereira. Reconstructing Missing Data in State Estima-
tion With Autoencoders. IEEE Transactions on Power Systems,
27(2):604–611, 2012.

[Porteous et al., 2010] I. Porteous, A. U. Asuncion,

and
M. Welling. Bayesian matrix factorization with side infor-
In Proc. of AAAI,
mation and dirichlet process mixtures.
2010.

[Rendle, 2010] S. Rendle. Factorization machines.

In Proc. of

ICDM, 2010.

[Salakhutdinov and Mnih, 2008] R. Salakhutdinov and A. Mnih.
Bayesian probabilistic matrix factorization using markov chain
monte carlo. In Proc. of ICML, 2008.

[Salakhutdinov et al., 2007] R. Salakhutdinov, A. Mnih,

and
G. Hinton. Restricted boltzmann machines for collaborative ﬁl-
tering. In Proc. of ICML. ACM, 2007.

[Sedhain et al., 2015] S. Sedhain, A. K. Menon, S. Sanner, and
L. Xie. Autorec: Autoencoders meet collaborative ﬁltering. In
Proc. of WWW, 2015.

[Sergey and Szegedy, 2015] I. Sergey and C. Szegedy. Batch nor-
malization: Accelerating deep network training by reducing in-
ternal covariate shift. In Proc. of ICML, 2015.

[Strub et al., 2016] F. Strub, R. Gaudel, and J. Mary. Hybrid rec-
ommender system based on autoencoders. In Recsys DLRS work-
shop, 2016.

[Vincent et al., 2008] P. Vincent, H. Larochelle, Y. Bengio Yoshua,
and P. Manzagol. Extracting and composing robust features with
denoising autoencoders. In Proc. of ICML, 2008.

[Wang and Wang, 2014] X. Wang and Y. Wang. Improving content-
based and hybrid music recommendation using deep learning. In
Proc. of the ACM Int. Conf. on Multimedia, 2014.

[Wang et al., 2014] H. Wang, N. Wang, and D.-Y. Yeung. Collabo-
rative deep learning for recommender systems. In Proc. of KDD,
2014.

[Wang et al., 2016] H. Wang, X. Shi, and D. Yeung. Collaborative
Recurrent Autoencoder: Recommend while Learning to Fill in
the Blanks. In Proc of NIPS, 2016.

[Wen et al., 2016] T. Wen, D. Vandyke, N. Mrksic, M. Gasic,
L. Rojas-Barahona, P. Su, S. Ultes, and S. Young. A network-
based end-to-end trainable task-oriented dialogue system. arXiv
preprint arXiv:1604.04562, 2016.

[Wu et al., 2016] Y. Wu, C. DuBois, A. Zheng, and M. Ester. Col-
laborative denoising auto-encoders for top-n recommender sys-
tems. In Proc. of WSDM, 2016.

[Zheng et al., 2016] Y. Zheng, C. Liu, B. Tang, and H. Zhou. Neu-
ral autoregressive collaborative ﬁltering for implicit feedback. In
Proc. of ICML, 2016.

[Zhou et al., 2008] Y. Zhou, D. Wilkinson, R. Schreiber, and
R. Pan. Large-scale parallel collaborative ﬁltering for the netﬂix
prize. In Proc. of AAIM. Springer, 2008.

