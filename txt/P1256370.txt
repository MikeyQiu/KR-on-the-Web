Learning Steerable Filters for Rotation Equivariant CNNs

Maurice Weiler1,2

Fred A. Hamprecht2

Martin Storath2

1AMLab / QUVA Lab, University of Amsterdam

2HCI/IWR, University of Heidelberg

m.weiler@uva.nl

{fred.hamprecht, martin.storath}@iwr.uni-heidelberg.de

8
1
0
2
 
r
a

M
 
9
1
 
 
]

G
L
.
s
c
[
 
 
3
v
9
8
2
7
0
.
1
1
7
1
:
v
i
X
r
a

Abstract

In many machine learning tasks it is desirable that a
model’s prediction transforms in an equivariant way under
transformations of its input. Convolutional neural networks
(CNNs) implement translational equivariance by construc-
tion; for other transformations, however, they are com-
pelled to learn the proper mapping. In this work, we de-
velop Steerable Filter CNNs (SFCNNs) which achieve joint
equivariance under translations and rotations by design.
The proposed architecture employs steerable ﬁlters to ef-
ﬁciently compute orientation dependent responses for many
orientations without suffering interpolation artifacts from
ﬁlter rotation. We utilize group convolutions which guar-
antee an equivariant mapping. In addition, we generalize
He’s weight initialization scheme to ﬁlters which are de-
ﬁned as a linear combination of a system of atomic ﬁlters.
Numerical experiments show a substantial enhancement of
the sample complexity with a growing number of sampled
ﬁlter orientations and conﬁrm that the network generalizes
learned patterns over orientations. The proposed approach
achieves state-of-the-art on the rotated MNIST benchmark
and on the ISBI 2012 2D EM segmentation challenge.

1. Introduction

Convolutional neural networks are extremely successful
predictive models when the input data has spatial structure.
One principal reason is that the convolution operation ex-
hibits translational equivariance so that feature extraction is
independent of the spatial position. For many types of im-
ages it is desirable to make feature extraction orientation
independent as well. Typical examples are biomedical mi-
croscopy images or astronomical data which do not show
a prevailing global orientation. Consequently, the output
of a network processing such data should be equivariant
w.r.t. the orientation of its input – if the input is rotated,
the output should transform accordingly. Even when there
is a predominant direction in an image as a whole, the low
level features in the ﬁrst layers such as edges usually appear
in all orientations; see e.g. the ﬁlterbanks visualized in [1].

In both cases, conventional CNNs are compelled to learn
rotated versions of the same ﬁlter, introducing redundant
degrees of freedom and increasing the risk of overﬁtting.

1.1. Contribution

We propose a rotation-equivariant CNN architecture
which shares weights over ﬁlter orientations to improve
generalization and to reduce sample complexity. A key
property of our network is that its ﬁlters are learned such
that they are steerable. This approach avoids interpolation
artifacts which can be severe at the small length scale of
typical ﬁlter kernels. We accomplish the steerability of the
learned ﬁlters by representing them as linear combinations
of a ﬁxed system of atomic steerable ﬁlters.

In all intermediate layers of the network, we utilize group
convolutions to ensure an equivariant mapping of feature
maps. Group-convolutional networks were proposed by Co-
hen and Welling [2] who considered four ﬁlter orientations.
An advantage of our construction is that we can achieve an
arbitrary angular resolution w.r.t. the sampled ﬁlter orien-
tations. Indeed, our experiments show that results improve
signiﬁcantly when using more than four orientations.

An important practical aspect of CNNs is a proper
weight initialization. Since the weights to be learned serve
as expansion coefﬁcients for the steerable function space,
common weight initialization schemes need to be adapted.
Here, we generalize the results found by Glorot and Ben-
gio [3] and He et al. [4] to networks which learn ﬁlters as a
composition of (not necessarily steerable) atomic ﬁlters.

Our network achieves state-of-the-art results on two
important rotation-equivariant/invariant recognition tasks:
(i) The proposed approach is the ﬁrst to obtain an accuracy
higher than 99% on the rotated MNIST dataset, which is
the standard benchmark for rotation-invariant classiﬁcation.
(ii) A processing pipeline based on the proposed SFCNN
layers ranks among the top three entries in the ISBI 2012
electron microscopy segmentation challenge [5].

Figure 1 gives an overview over the key concepts utilized

in Steerable Filter CNNs.

1

Figure 1: Key concepts of the proposed Steerable Filter CNN: The ﬁlters are parameterized in a steerable function space with shared weights over ﬁlter
orientations. Exact ﬁlter rotations are achieved by a phase manipulation of the expansion coefﬁcients wq. All layers are designed to be jointly translation
and rotation equivariant. The weights wq serve as expansion coefﬁcients of a ﬁxed ﬁlter basis {ψq}q rather than pixel values. Therefore, we adapt He’s
weight initialization scheme to this more general case which implies to normalize the basis ﬁlter energies.

2. Equivariance properties of CNNs

Equivariance is the property of a function to commute
with the actions of a symmetry group acting on its domain
and codomain. Formally, given a transformation group G,
a function f : X → Y is said to be equivariant if

f (cid:0)ϕX

g (x)(cid:1) = ϕY

g (f (x))

∀g ∈ G, x ∈ X,

where ϕ(·)
g denotes a group action in the corresponding
space. A special case of equivariance is invariance for
which ϕY

g = id.

In many machine learning tasks a set of transformations
is known a-priori under which the prediction should trans-
form in an equivariant way. Including such knowledge di-
rectly into the model can greatly facilitate learning by free-
ing up model capacity for other factors of variation. As an
example consider a segmentation problem where the goal is
to learn a mapping from an image space I to label images
in L, which we formalize by a ground truth segmentation
map S : I → L. The learning process involves ﬁtting a
model M : I → L to approximate the ground truth. For
segmentation tasks, however, translations of the input im-
age I ∈ I should typically lead to a translated segmentation
map. Speciﬁcally, one has

S (TdI) = TdS(I) ∀d ∈ R2, I ∈ I,

(1)

where Td is an action of the translation group T = (R2, +)
which shifts the image or segmentation by d ∈ R2. The
group action partitions the image space in equivalence
classes T.I = {TdI | d ∈ R2} which are known as group
orbits and comprise all images that are related by the action.
Note that the translation equivariance (1) of the ground truth

segmentation function implies a mapping of whole orbits in
I to orbits in L. It is therefore possible to reformulate the
ground truth as (cid:101)S : I/T → L/T , where (·)/T denotes the
quotient space resulting from collapsing equivalent images
Instead of ﬁtting an un-
in an orbit to a single element.
restricted model M to S it is advantageous to incorporate
the transformation behavior into the model by construction.
The crucial consequence is that this reduces the hypothesis
space to models (cid:102)M : I/T → L/T.

CNN layers, which transform feature maps ζ by convolv-
ing them with ﬁlters Ψ, are by construction equivariant un-
der translations, that is, (Tdζ) ∗ Ψ = Td (ζ ∗ Ψ) . Therefore,
their hypothesis space is restricted to (cid:102)M . 1 As consequence,
patterns learned at one speciﬁc location evoke the same re-
sponse at each other location which leads to reduced sample
complexity and enhanced generalization.

Besides translations, there are often further transforma-
tions like rotations, mirroring or dilations under which the
model should be equivariant. Enforcing equivariance under
an extended transformation group G leads to an enhanced
generalization over larger orbits G.I and reduces the hy-
pothesis space further to (cid:102)M : I/G → L/G.

3. Steerable Filter CNNs

Here, we develop Steerable Filter CNNs (SFCNNs)
which achieve equivariance under joint translations and dis-
crete rotations. The key concept leading to translation
equivariance of CNNs is translational weight sharing. We

1In practice one often uses strided pooling layers which make the pre-
diction more robust to local deformations but reduce the equivariance to a
subgroup determined by the stride.

3.1. Parametrization of the steerable ﬁlters

ρθψk(x) = e−ikθψk(x).

(4)

extend the transformation group under which our networks’
layers are equivariant by additionally sharing weights over
ﬁlter orientations. This implies to perform convolutions
with several rotated versions of each ﬁlter. The rotational
weight sharing leads to an improved sample complexity and
to an enhanced generalization over orbits consisting of im-
ages connected by translations and discrete rotations.

In the following sections we introduce our parametriza-
tion of steerable ﬁlters, propose the network design in terms
of these ﬁlters and derive a weight initialization scheme
adapted to the ﬁlter parametrization. For the formal deriva-
tions we assume the images, feature maps and ﬁlters to be
deﬁned on the continuous domain R2. The effects resulting
from a discretized implementation are investigated in the
experimental section.

3

2

1

0

At the heart of convolutional neural networks lies the
concept of learning ﬁlter kernels. Our construction de-
mands for ﬁlters whose responses can be computed accu-
rately and economically for several ﬁlter orientations. Si-
multaneously the ﬁlters should not be restricted in their ex-
pressive power, i.e.
in the patterns to be learned. All of
these requirements are met by learning linear combinations
of a system of steerable ﬁlters. Here we describe a suitable
construction of steerable ﬁlters for learning in CNNs.

A ﬁlter Ψ is rotationally steerable in the sense of Hel-
Or and Teo [6], when its rotation by an arbitrary angle θ
can be expressed in a function space spanned by a ﬁxed
set of atomic basis functions {ψq}Q
q=1. This deﬁnition in-
cludes the classical formulation of steerability by Freeman
and Adelson [7] as a speciﬁc choice of basis. Formally, a
steerable ﬁlter Ψ : R2 → R satisﬁes

ρθΨ(x) =

κq(θ)ψq(x),

(2)

(cid:88)Q

q=1

for all angles θ ∈ (−π, π] and for angular expansion co-
efﬁcient functions κq. Here ρθ denotes both the rotation
operator deﬁned by ρθΨ(x) = Ψ(ρ−θx) when acting on a
function as well as a counterclockwise rotation by the an-
gle θ when acting on a coordinate vector. As pointed out
by Freeman and Adelson [7], the rotation by steerability
is analytic and exact even for signals sampled on a grid.
In contrast to rotations by interpolation the approach does
not suffer from interpolation artifacts. An important practi-
cal consequence of steerability is that the response of each
orientation can be synthesized from the atomic responses
f ∗ ψq; that is, (f ∗ ρθΨ) (x) = (cid:80)Q
q=1 κq(θ) (f ∗ ψq) (x).
A basis of a steerable function space which is particu-
larly easy to handle is given by circular harmonics; see e.g.
[8, 9]. They are deﬁned by a sinusoidal angular part multi-
plied with a radial function τ : R+ → R, i.e.

ψk(r, φ) = τ (r) eikφ,

(3)

j \ k

0
Re Re

1

2

3

4

Im Re

Im Re

Im Re

Im

· · ·

· · ·

Figure 2: Illustration of the circular harmonics ψjk(r, φ) = τj (r) eikφ
sampled on a 9 × 9 grid. Each row shows a different radial part j, the
angular frequencies are arranged in the columns. For larger scales there
are higher frequency ﬁlters not shown here.

where (r, φ) denote polar coordinates of x = (x1, x2) and
k ∈ Z is the angular frequency. By construction, ψk can be
rotated by multiplication with a complex exponential,

In our network, we utilize a system of circular harmonics
ψjk with j = 1, . . . , J, and k = 0, . . . , Kj where the addi-
tional index j controls the radial part of ψjk = τj(r) eikφ.
Figure 2 shows the real and imaginary parts of the atoms
used in the experiments where we chose Gaussian radial
parts τj(r) = exp(−(r − µj)2/2σ2) with µj = j. The
maximum angular frequencies Kj are limited to the point
where aliasing effects occur. We found this system to be
convenient for learning as the ﬁlters are approximately or-
thogonal and radially localized.

The learned ﬁlters are then deﬁned as linear combina-

tions of the elementary ﬁlters, that is,

˜Ψ(x) =

(cid:88)J

(cid:88)Kj
k=0

j=1

wjkψjk(x),

(5)

with weights wjk ∈ C. The complex phase of the weights
allows rotating the atomic ﬁlters with respect to each other.
Such a composed ﬁlter can subsequently be steered as a
whole by phase manipulation of the atoms via

ρθ ˜Ψ(x) =

(cid:88)J

(cid:88)Kj
k=0

j=1

wjke−ikθψjk(x).

(6)

We select a single orientation by taking their real part

Ψ(x) = Re ˜Ψ(x)

(7)

and let ρθΨ = Re ρθ ˜Ψ.

3.2. Equivariant network architecture

The basic building blocks of the proposed SFCNN are three
equivariant layer types which we introduce in this section.

Input layer: The ﬁrst layer l = 1 of our network ingests
an image with C channels Ic : R2 → R, c = 1, . . . C
and convolves these with ˆC rotated ﬁlters ρθΨ(1)
ˆcc , where
Ψ(1)
: R2 → R, ˆc = 1, . . . , ˆC, are ﬁlter channels of the
ˆcc

form (7). This results in pre-nonlinearity features

y(1)
ˆc (x, θ) =

(cid:88)C

(cid:16)

Ic ∗ ρθΨ(1)
ˆcc

(cid:17)

(x)

(cid:18)

(cid:88)C

=

c=1

Ic ∗ Re

c=1
(cid:88)J

(cid:88)Kj
k=0

j=1

(8)

(cid:19)

wˆccjke−ikθψjk

(x)

= Re

(cid:88)C

(cid:88)J

c=1

j=1

(cid:88)Kj
k=0

wˆccjke−ikθ (Ic ∗ ψjk) (x),

where the ﬁlters are rotated by in total Λ equidistant ori-
entations θ ∈ Θ = {0, . . . , 2π Λ−1
In this setting the
rotational weight sharing is reﬂected by the phase manipu-
lation of the weights wˆccjk which themselves are indepen-
dent of the angle θ. A higher resolution in orientations can
be achieved by simply expanding the tensor containing the
phase-factors.

Λ }.

As usual, after the convolution step a bias β(1)

is added
and a nonlinearity σ is applied, so that we end up with the
ﬁrst layer’s feature map given by

ˆc

ζ (1)
ˆc

(x, θ) = σ

ˆc (x, θ) + β(1)
y(1)

ˆc

(cid:16)

(cid:17)

.

Note that the resulting representation ζ (1)
depends on a spa-
tial location x and an orientation angle θ, i.e. on the trans-
formation group applied to the ﬁlters.

ˆc

Group-convolutional layers: To process the resulting fea-
ture maps further we utilize group convolutions which
naturally generalize spatial convolutions from translations
to more general transformation groups. Given a feature
: G → R and a ﬁlter Ψ : G → R liv-
map ζ
ing on a group G, their group convolution is deﬁned by
(ζ (cid:126) Ψ)(g) = (cid:82)
G ζ(h)Ψ(h−1g) dλ(h), where we use the
symbol (cid:126) to distinguish group convolutions from the spa-
tial convolution operator ∗, and λ denotes a Haar mea-
sure. The resulting feature map is again a function on
the group. In analogy to spatial convolutions, group con-
volutions are equivariant under the group operation, i.e.
(ϕh(ζ) (cid:126) Ψ) (g) = ϕh (ζ (cid:126) Ψ) (g),
∀h, g ∈ G, where
ϕh is given by ϕhζ(g) = ζ(h−1g). For a deeper discussion
of group convolutions in neural networks we refer to [2].

The feature maps calculated by the input layer are func-
tions on the semidirect product group R2 (cid:111) Θ ≤ SE(2).
Keeping the parameterization by (x, θ), the group convolu-
tions with summation over input channels can be explicitly
instantiated as

y(l)
ˆc (x, θ) =
(cid:88)C

(cid:88)

(cid:88)C

(cid:16)

c=1

ζ (l−1)
c

(cid:126) Ψ(l)
ˆcc

(cid:17)

(x, θ)

(9)

ζ (l−1)
c

(u, φ)Ψ(l)
ˆcc

(cid:0)(u, φ)−1(x, θ)(cid:1) du

c=1

φ∈Θ

R2

(cid:88)C

(cid:88)

c=1

φ∈Θ

(cid:88)C

(cid:88)

c=1

φ∈Θ

ζ (l−1)
c

(·, φ) ∗ ρφΨ(l)

(cid:17)
ˆcc (·, θ − φ)

(x)

ζ (l−1)
c

(·, φ) ∗ RφΨ(l)

(cid:17)
ˆcc (·, θ)

(x).

(cid:90)

(cid:16)

(cid:16)

=

=

=

Here the multiplication with the inverse group element,
(u, φ)−1(x, θ) = (ρ−φ(x − u), θ − φ), was evaluated by
switching to a representation of the group. We further in-
troduced the action Rφ deﬁned by

RφΨ(x, θ) := ρφΨ(x, θ − φ)

which transforms functions on the group by rotating them
spatially and shifting their orientation components cycli-
cally. The above equation reveals that the group convolution
can be decomposed into a spatial convolution, rotation and
linear combination. In analogy to the ﬁrst layer we make
use of the steerable ﬁlters which on the group are deﬁned
(cid:80)Kj
by Ψ(l)
k=0 wˆccjkθψjk(x). Note that
the additional orientation dimension is reﬂected by an ad-
ditional index of the weight tensor. Inserting the steerable
ﬁlters in (9) we obtain the pre-nonlinearity feature maps of
the group-convolutional layers

ˆcc (x, θ) = Re (cid:80)J

j=1

y(l)
ˆc (x, θ)

C
(cid:88)
ζ (l−1)
c

(cid:88)

=

c=1

φ∈Θ

= Re

C
(cid:88)

(cid:88)

(cid:88)

c=1

φ∈Θ

j,k

(10)


(·, φ) ∗ Re

wˆccjk,θ−φe−ikφψjk

(x)

(cid:88)

j,k

wˆccjk,θ−φe−ikφ (cid:16)

ζ (l−1)
c

(·, φ) ∗ ψjk

(x).

(cid:17)

As before, a bias β(l)
ˆc
is applied, ζ (l)

ˆc (x, θ) = σ(y(l)

is added and the activation function σ
ˆc (x, θ) + β(l)
ˆc ).

By the linearity of the steerability and the convolution,
one can implement the layers either by a direct convolu-
tion with linearly combined ﬁlters, or by linearly combin-
ing the responses of the atomic ﬁlters. We implemented
both approaches and found that in typical operation regimes
the ﬁrst option is faster since the kernels to be linearly
combined have a smaller spatial extent than the atomic re-
sponses of the second option.

Output layer: After the last group-convolutional layer we
extract the information of interest for the speciﬁc task. For
rotation-invariant classiﬁcation we pool globally over both
the orientation dimension and the remaining spatial resolu-
tion. A pooling over orientations is also done for rotation-
equivariant segmentation where spatial dimensions remain
and the output rotates according to the rotation of the net-
work’s input. If the orientation itself is of interest it could
be kept as extra feature.

Equivariance: Each individual layer L(·) of the network
is equivariant under joint translations and rotations in the
group R2 (cid:111) Θ : Rotating the input image leads to a trans-
formation Lin(ρφI) = RφLin(I) of the ﬁrst layer’s fea-
ture maps. The subsequent group-convolutional layers then
transform like Lgconv(Rφζ) = RφLgconv(ζ). When using
orientation pooling in the output layer the resulting feature

Figure 3: Top: Basic structure of a typical SFCNN for rotation-equivariant segmentation. For clarity, we display only a single group-convolutional layer and
a single feature channel and omit pooling and normalization layers. Rotated Greek letters represent the spatial orientations of the ﬁlters and the feature maps.
Orientation components are abbreviated as subscript, i.e. Ψλ = Ψ(·, θλ). Filters in the same color share their weights as they are connected by rotations.
The weight sharing of the ﬁlters on the group is prescribed by the group convolution (9). After the last group-convolutional layer we pool over orientations
to obtain predictions which are invariant under rotations of local patches in the ﬁeld of view. Bottom: Visualization of the layerwise rotation-equivariance.
Applying a rotation ρφ to the input image results in a joint spatial rotation operation and cyclic shift over orientation indices Rφ of the feature maps ζ(l).
This transformation behavior can be understood intuitively when paying attention to the relative orientation of each layer’s input and ﬁlters.

maps are rotated: Lout(Rφζ) = ρφLout(ζ). Overall, this im-
plies the equivariance of a whole network,
(cid:0)Lout ◦ Ld
(cid:0)Lout ◦ Ld

gconv ◦ Lin(I)(cid:1) ,
where d is the number of group-convolutional layers. The
layers’ equivariance is formally proven in appendix A.

(cid:1) (ρφI) = ρφ

gconv ◦ Lin

The top part of Figure 3 visualizes the building blocks
of a typical SFCNN for rotation-equivariant segmentation.
An overview over the transformation behavior of the feature
maps under rotation of the input is given in the bottom part.
The spatial rotation and cyclic shift over orientation chan-
nels Rφ of the feature maps on the group can be understood
intuitively when paying attention to the relative orientation
of each layer’s input and ﬁlters.

Compared to a conventional CNN which independently
learns ﬁlters in Λ orientations in a rotation-invariant recog-
nition task, a corresponding SFCNN consumes Λ times less
parameters to extract the same representation.

SFCNN incur a small computational overhead for build-
ing the ﬁlter kernels from the circular harmonics basis
which we found to be negligible. The computational cost
of SFCNNs is therefore equivalent to that of a conventional
CNN when the effective number of channels coincide, i.e.
when ICNN = ΛISFCNN.

3.3. Generalizing He’s weight initialization scheme

An important practical aspect of training deep networks
is an appropriate initialization of their weights. When the

weights’ variances are chosen too high or low, the sig-
nals propagating through the network are ampliﬁed or sup-
pressed exponentially with depth. Glorot and Bengio [3]
and He et al. [4] investigated this issue and came up with
initialization schemes which are accepted as a standard for
random weight initialization. In contrast to [3] and [4] our
ﬁlters are not parameterized in a pixel basis but as a linear
combination of a system of atomic ﬁlters with weights serv-
ing as expansion coefﬁcients. To be speciﬁc, we consider
ﬁlters Ψˆccx = (cid:80)Q
q=1 wˆccqψqx which are built from Q, not
necessarily steerable, real valued atomic ﬁlters which map
C input channels to ˆC output channels. This assumption is
more general than that of the aforementioned works since
they only consider the pixel basis ψDirac
qx = δq,x, i.e. atomic
ﬁlters which are zero everywhere but at one pixel.

Most of the further assumptions are identical to those
in [4]: We assume the activations and gradients to be
i.i.d. and to be independent from the weights. Further, the
weights themselves are initialized to be mutually indepen-
dent and have zero mean. An important difference is that
we do not restrict the weights to be identically distributed
because of the inherent asymmetry of the different atomic
ﬁlters. All biases are initialized to be zero and the nonlin-
earities are chosen to be ReLUs. These assumptions lead to
the initialization conditions

Var [wq] =

or Var [wq] =

2
CQ (cid:107)ψq(cid:107)2
2

2
ˆCQ (cid:107)ψq(cid:107)2
2

for the forward or backward pass, respectively. A detailed
derivation is given in appendix B.

(cid:13)
2
(cid:13)
2

(cid:13)ψDirac
qx

, for ψDirac

qx = δq,x with (cid:13)

As discussed in [4], the difference between both initial-
izations cancels out for intermediate layers. Note that our
results include those of He et al. [4], that is, Var [wq] = 2
nin
or Var [wq] = 2
= 1.
nout
We further want to point out that the learned ﬁlters are
combined of products wqψq which implies that the factors
(cid:107)ψq(cid:107)2
2 counterbalance different energies of the basis ﬁlters.
A convenient way to initialize the network is hence to nor-
malize all ﬁlters to unit norm and subsequently initialize the
CQ or Var [wq] = 2
weights uniformly by Var [wq] = 2
.
ˆCQ
In our group-convolutional layers the ﬁlters additionally
comprise orientation channels. From the perspective of
weight initialization these have the same effect as conven-
tional channels, therefore we propose to normalize their
weights variance with an additional factor of Λ. We em-
phasize that using normalization layers like batch normal-
ization does not obviate the need for a proper weight initial-
ization. This is because such layers scale activations as a
whole while our initialization conditions indicates that the
relative scale of the summands contributing to each activa-
tion needs to be adapted. Further details, in particular on
initializing weights of complex-valued ﬁlters, are given in
the appendix.

4. Prior and related work

A priori knowledge about transformation-invariance of
images can be exploited in manifold ways. A commonly
utilized technique is data augmentation, see e.g. [10]. The
basic idea is to enrich the training set by transformed sam-
ples. Augmenting datasets allows to train larger models and
is easily applicable without modifying the network architec-
tures. When the augmenting transformations form a group
G the additional images I ⊆ I lie on the orbit G.I.
In
contrast to equivariant models the hypothesis space is not
restricted to the quotient space I/G under the utilized sym-
metry group but the equivariance needs to be learned ex-
plicitly by the network. This demands for a high learning
capacity which makes the network prone to overﬁtting.

Recent work focuses on incorporating equivariance to
various transformations directly into the network’s architec-
ture. Invariance to speciﬁc transformations can be achieved
by applying them to the input and subsequently pooling
their responses [11, 12, 13]. In [14] the regions in symme-
try space to pool over are learned to become invariant only
to nuisance deformations. Another approach is to resample
the input and apply standard convolutions. Henriques and
Vedaldi [15] achieve equivariance w.r.t. Abelian symmetry
groups by ﬁxing a sampling grid according to the symme-
try while in [16, 17] the network itself estimates the grid.
In [18] transformations are dealt with by convolving with

ﬁlters which are steered by a subnetwork.

In particular, there has recently been a considerable in-
terest in rotation-equivariant CNNs. The work [19] intro-
duces four operations which are easily included into exist-
ing networks and enrich both the batch- and feature dimen-
sion with transformed versions of their content. In [2], the
feature maps resulting from transformed ﬁlters are treated
as functions of the corresponding symmetry-group which
allows to use group-convolutional layers. As their compu-
tational cost is coupled to the size of the group, Cohen and
Welling [20] propose to alternatively use steerable repre-
sentations as composition of elementary feature types. Be-
sides translations and rotations, the aforementioned works
they operate on the dihe-
also incorporate reﬂections, i.e.
dral group. Their current limitation is the restriction to rota-
tions by the angle π
2 , thus to four orientations. In [21], sev-
eral rotated versions of the same image are sent through a
conventional CNN. The resulting features are subsequently
pooled over the orientation dimension. The approach can
be easily extended to other transformations. On the down-
side, the equivariance is only w.r.t. global transformations.
Marcos et al. [22] perform convolutions with rotated ver-
sions of a each ﬁlter in a shallow network followed by a
global pooling over orientations. These ideas were extended
to networks which additionally propagate the orientation of
the maximum response [23]. In both approaches the ﬁlter
rotation is based on bicubic interpolation, allowing for ﬁne
resolutions with respect to the orientation but causing in-
terpolation artifacts. Worrall et al. [24] achieve continuous
resolution in orientations by working with complex valued
steerable ﬁlters and feature maps. However, this requires
the angular frequencies of the feature maps to be kept dis-
entangled. Rotation-equivariant feature extraction can also
be achieved by using group-convolutional scattering trans-
forms [25]. A fundamental difference to our work is that
the ﬁlter banks are ﬁxed rather than learned.

5. Experimental results

We evaluate the proposed SFCNNs on two datasets ex-
hibiting rotational symmetries. On the rotated MNIST
dataset we ﬁrst investigate speciﬁc network properties like
the accuracy’s dependence on the number of sampled ori-
entations and the generalization of learned patterns over
orientations. With the insights gained in these experi-
ments we benchmark the model and the proposed initial-
ization scheme. To evaluate the segmentation capabilities
of SFCNNs on real world data we run a further experiment
on the ISBI 2012 EM segmentation challenge.

5.1. Rotated MNIST

In our ﬁrst experiments we investigate the equivari-
ance properties of the proposed network architecture on the
rotated MNIST dataset (mnist-rot) which is the standard

Figure 4: Left: Test error versus number of sampled ﬁlter orientations for different training subsets from mnist-rot. Shaded regions highlight the standard
deviations over several runs. The accuracy improves signiﬁcantly with increasing angular resolution until it saturates at around 12 to 16 orientations. Right:
Rotational generalization capabilities of a conventional CNN and a SFCNN with Λ = 16 using different data augmentation strategies. In this experiment
the training set consists of unrotated MNIST digits while the test set for each angle contains the remaining digits, rotated to the corresponding angle.

benchmark for rotation-equivariant models. The dataset
contains the handwritten digits of the classical MNIST
dataset, rotated to random orientations in [0, 2π). It is split
in 12000 training and 50000 test images; model selection
is done by training on 10000 images and validating on the
2000 remaining samples in the training set.

For our initial experiments we utilize the classiﬁcation-
SFCNN given in Table 2 in the appendix as baseline. It con-
sists of one steerable input layer which maps the input im-
ages to the group, ﬁve following group-convolutional layers
and three fully connected layers. After every two steerable
ﬁlter layers we perform a spatial 2 × 2 max-pooling. The
orientation dimension and the remaining spatial dimensions
are pooled out globally after the last convolutional layer.
Details on the further training setup are given in appendix C.

Sampled orientations: The number of sampled orienta-
tions Λ is a parameter speciﬁc to our network, so we ﬁrst
explore its inﬂuence on the test accuracy. We are further in-
terested in the network’s sample complexity, i.e. the depen-
dence on the size of the training set. The accuracies result-
ing when varying these parameters are reported in Figure 4
(left). As expected, the test error and its standard devia-
tion decrease with the size of the training data set. We ob-
serve that the accuracy improves signiﬁcantly when increas-
ing the number of orientations until it saturates at around
12 to 16 angles. Up to this point, the gain of adding more
sampled orientations is considerable. For example, in al-
most all cases, increasing the angular resolution from 2 to
4 sampled orientations provides a higher gain in accuracy
than sticking with 2 orientations and doubling the number
of training samples. We want to emphasize that the possibil-
ity of SFCNNs to go beyond the four sampled orientations
of [19, 2, 20] leads to a signiﬁcant gain in accuracy. Note
that the case Λ = 1 corresponds, up to the different ﬁlter
parameterization, to conventional CNNs.

Rotational generalization: In order to test how well the
networks generalize learned patterns over orientations we

4 or π

conduct an experiment where we train them on unrotated
digits and record their accuracy over the orientation of ro-
tated digits. Speciﬁcally, we take the the ﬁrst 12000 samples
of the conventional MNIST dataset to train a SFCNN with
Λ = 16 as well as a conventional CNN of comparable size
using either no augmentation, augmentation by rotations
which are multiples of either π
2 or augmentation by
rotations which are densely sampled from [0, 2π). As test
set we take the remaining 58000 samples and record the test
errors’ dependence on the orientation of this dataset. To ob-
tain a fair comparison between the networks we experiment
with conventional CNNs with the same number of parame-
ters or the same number of channels like the SFCNN. Since
both show the same behavior we only report the accuracies
of the network with the same number of channels which
performs slightly better. The results are plotted in Figure 4
(right). One can see that, lacking rotational equivariance,
the conventional CNN does not generalize well over orien-
tations. When using rotational augmentation the error re-
duces considerably on average, it grows, however, for small
angles in a neighborhood of zero. This is the case because
the network needs to learn to detect the augmented samples
additionally which demands an increased learning capac-
ity. The SFCNN on the other hand generalizes quite well
over orientations even without augmentation. In continuous
space we would expect the test error curve to be 2π
Λ -periodic
because of the rotational equivariance. The deviations from
this behavior can be attributed to the sampling effects of us-
ing digitized images. As to be expected for Λ = 16 orienta-
tions, the accuracy is not inﬂuenced by augmentation with
π
2 -rotations since the additional samples lie on the group
orbit on which the network is invariant. In contrast to con-
ventional CNNs, SFCNNs do not show an increased error
for small angles in a neighborhood of zero when using aug-
mentation. This indicates that the cost of learning rotated
versions of each digit is negligible thanks to the approxi-
mate rotation equivariance. An augmentation by rotations
which are multiples of π
4 or by continuous rotations give

Method
IAL MC/LMC
CASIA MIRA
Ours
Quan et al. [26]
Beier et al. [27]
Drozdzal et al. [28]

V Info

V Rand
0.98792 0.99183
0.99072
0.98788
0.99144
0.98680
0.99130
0.98365
0.98845
0.98224
0.98816
0.98058

Figure 5: Experimental results on the ISBI 2012 challenge. The shown patches are cropped from slice 30 of the training data set which we used for
validation. Left: Raw EM image. Mid-left: Binary membrane ground truth segmentation. Mid-right: Probability map predicted by the proposed network.
Right: Top 6 of more than 100 entries of the leaderboard, accessed on November 13, 2017. Higher values mean better accuracy.

Method

Ours – CoeffInit, train time augmentation
Ours – CoeffInit
Ours – HeInit
Marcos et al. [23] – test time augmentation
Marcos et al. [23]
Laptev et al. [21]
Worrall et al. [24]
Cohen and Welling [2] - G-CNN
Schmidt and Roth [29]
Sohn and Lee [11]
Cohen and Welling [2] - conventional CNN
Larochelle et al. [30]

Test Error (%)

0.714 ± 0.022
0.880 ± 0.029
0.957 ± 0.025
1.01
1.09
1.2
1.69
2.28 ± 0.0004
4.0
4.2
5.03 ± 0.0020

10.4 ± 0.27

Table 1: Test errors on the rotated MNIST dataset. We distinguish He
initialization (HeInit) from the proposed initialization scheme (CoeffInit).

very similar results. Both seem to act as a regularization
preventing the ﬁlters to overﬁt on the pixel grid.

We conclude that SFCNNs outperform the rotational

generalization of CNNs for all levels of augmentation.

Benchmarking: Based on the insights from the above ex-
periments we ﬁx the number of sampled orientations to
Λ = 16 and tune the network further to the slightly larger
architecture given in Table 3 in the appendix. The results
are reported in Table 1. Using the SFCNN with He’s weight
initialization and no data augmentation, we obtain a test er-
ror of 0.957% which already exceeds the previous state-of-
the-art. The proposed initialization scheme, adapted to ﬁlter
coefﬁcients, signiﬁcantly improves the test error to 0.880%.
When additionally augmenting the dataset with continuous
rotations during training time the error decreases further to
0.714%. To summarize, our approach reduces the best pre-
viously published error by a factor of 29%.

5.2. ISBI 2012 2D EM segmentation challenge

In a second experiment we evaluate the performance of
our model on the ISBI 2012 electron microscopy segmen-
tation challenge [5]. The goal of the challenge is to predict
the locations of the cell boundaries in the Drosophila ven-
tral nerve cord from EM images which is a key step for
investigating the connectome of the brain. The dataset con-
sists of 30 train and test slices of size 512 × 512 px with a
binary segmentation ground truth provided for the training
set. Figure 5 shows an exemplary raw EM image with the
corresponding ground truth segmentation mask and our net-
work’s prediction. An important property of the dataset is
that the images have no preferred orientation which makes

it suitable for evaluating rotation-equivariant networks.

We build on an established pipeline introduced in [27]
where a crucial step is the boundary prediction via a con-
ventional CNN. In the present experiment, we replaced their
network by a SFCNN with a U-net design [31]. The net-
work architecture is visualized in Figure 6 in the appendix.
As loss function we chose a pixel wise binary cross en-
tropy loss. The dataset was augmented by random elastic
deformations, ﬂips and rotations by multiples of π
2 during
train time. In the experiment on rotational generalization
we found that augmenting samples by transformations in a
subgroup under which the network is equivariant does not
have any effect. We therefore sampled Λ = 17 orientations
which is mutually prime with the 4 augmented orientations.
This way the augmented images do not fall into a subgroup
w.r.t. which the network is invariant.

Segmentation predictions are evaluated by the challenge
hosters and ranked w.r.t. the foreground-restricted Rand
score V Rand and the information score V Info; for an expla-
nation of these metrics see [5]. The current leaderboard in
Figure 5 (right) shows that our approach yields top-tier re-
sults. In particular, it improves upon the results of [27].

6. Conclusion

We have developed a rotation-equivariant CNN whose
ﬁlters are learned such that they are steerable. Layer-
wise equivariance is obtained by using group convolutions.
He’s weight initialization scheme is extended to general
ﬁlter bases which empirically leads to an increased accu-
racy. Our network allows sampling an arbitrary number
of ﬁlter orientations which improves the performance un-
til a saturation is reached. We conﬁrmed experimentally
that SFCNNs generalize learned patterns over orientations
and therefore achieve a lower sampling complexity than
CNNs in rotation-equivariant recognition tasks. The pro-
posed SFCNNs achieve state-of-the-art results on rotated
MNIST and the ISBI 2012 2D EM segmentation challenge.

Acknowledgement. We would like to thank T. Beier,
C. Pape, N. Rahaman and I. Arganda-Carreras for their
technical support and U. K¨othe and T. Cohen for valuable
discussions. This work was partially supported by the Ger-
man Research Foundation (DFG grant STO1126/2-1).

Appendix

A. Equivariance properties

In this section we prove the equivariance of the individ-
ual layers of Steerable Filter CNNs under rotations by the
sampled orientations in Θ, assuming signals on a continu-
ous domain R2. Translational equivariance follows directly
from either the utilization of spatial convolutions or from
the independence of the operation on the spatial position.

A.1. Input layer

The ﬁrst layer maps an image I : R2 → R to a fea-
ture map ζ (1) : R2 (cid:111) Θ → R by ﬁrst convolving it with
multiple rotated versions ρθΨ of a ﬁlter Ψ : R2 → R and
subsequently adding a bias β and applying a nonlinearity
σ. Both steps are equivariant under rotations of the image
by angles α ∈ Θ. This means that ραI(x) is mapped to
Rαζ (1)(x, θ) = ραζ (1)(x, θ − α) where Rα is the group
action on functions on the group. To see that the ﬁrst step
performs an equivariant mapping, simply insert a rotated
image,

(ραI ∗ ρθΨ) (x) =

I(ρ−αu) Ψ(ρ−θ(x − u)) du ,

and substitute ˜u := ρ−αu. Since the transformation is or-
thogonal we have (cid:12)

(cid:1)(cid:12)
(cid:12) = 1 and hence:

(ραI ∗ ρθΨ) (x) =

I(˜u) Ψ(ρ−(θ−α)(ρ−αx − ˜u)) d˜u

(cid:90)

R2

(cid:12)det (cid:0) ∂ ˜u
(cid:90)

∂u

R2

= ρα (I ∗ ρθ−αΨ) (x)
= ραy(1)(x, θ − α)
= Rαy(1)(x, θ) .

The mutual transformation behavior is visualized in the fol-
lowing commutative diagram:

I(x)

ρα

ραI(x)

∗ρθΨ

y(1)(x, θ)

Rα

∗ρθΨ

ραy(1)(x, θ − α)

Adding a bias β to each feature map channel and applying a
nonlinearity σ does not interfere with translational- or rota-
tional equivariance since both operations do neither depend
on the spatial position nor orientation channel:

y(l)(x, θ)

Rα

σ( · + β)

ζ (l)(x, θ)

Rα

ραy(l)(x, θ − α)

ραζ (l)(x, θ − α)

σ( · + β)

A.2. Group-convolutional layers

Given feature maps ζ (l)(x, θ), the group-convolutional
layers perform an equivariant mapping of Rαζ (l)(x, θ) to
Rαζ (l+1)(x, θ) under the group action R. The step of
adding the bias and applying the activation function is
equivariant by the same argument as in the ﬁrst layer. What
is left to show is the equivariance (cid:0)Rαζ (l) (cid:126) Ψ(cid:1) (x, θ) =
(cid:0)ζ (l) (cid:126) Ψ(cid:1) (x, θ) = Rαy(l)(x, θ) of the group convo-
Rα
lution. Inserting a transformed feature map and writing the
group convolution out explicitly yields:

Rαζ (l) (cid:126) Ψ

(x, θ)

(cid:17)

(cid:16)

(cid:90)

(cid:88)

=

R2

φ∈Θ

ζ (l)(ρ−αu, φ − α) Ψ(ρ−φ(x − u), θ − φ) du .

Again, we substitute ˜u := ρ−αu with (cid:12)
(cid:1)(cid:12)
(cid:12) = 1. Fur-
thermore, we let ˜φ := φ − α under which the sum is invari-
ant thanks to the cyclic structure of the subgroup Θ, and we
obtain

(cid:12)det (cid:0) ∂ ˜u

∂u

ζ (l)(˜u, ˜φ) Ψ(ρ− ˜φ(ρ−αx − ˜u), (θ − α) − ˜φ) d˜u

Rαζ (l) (cid:126) Ψ

(x, θ)

(cid:17)

(cid:16)

(cid:90)

R2
(cid:16)

=

(cid:88)

˜φ∈Θ

=

ζ (l) (cid:126) Ψ

(ρ−αx, θ − α)

(cid:17)

=ραy(l+1)(x, θ − α)
=Rαy(l+1)(x, θ) .

This proves the equivariance of the intermediate layers.
Again, the relations are illustrated in a commutative dia-
gram:

ζ (l)(x, θ)

Rα

y(l+1)(x, θ)

Rα

ραζ (l)(x, θ − α)

ραy(l+1)(x, θ − α)

(cid:126)Ψ

(cid:126)Ψ

A.3. Orientation max-pooling layer

For

rotation-invariant segmentation or classiﬁcation
we max-pool over orientations after
the last group-
convolutional layer. The pooling step is itself equivariant
and results in a rotated version of its output:

max
θ

Rαζ (l)(x, θ) = max
θ
(cid:18)

ραζ (l)(x, θ − α)

= ρα

= ρα

(cid:18)

max
θ

max
θ

ζ (l)(x, θ − α)

(cid:19)

(cid:19)

ζ (l)(x, θ)

.

The rotation operator commutes with the maximum over
orientation channels because it acts on spatial coordinates
only. We again visualize the transformation behavior by a
commutative diagram:

ζ (l)(x, θ)

Rα

maxθ ζ (l)(x, θ)

ρα

ραζ (l)(x, θ − α)

ρα maxθ ζ (l)(x, θ)

max
θ

max
θ

In the case of classiﬁcation the remaining spatial structure
is pooled out such that the output is invariant under trans-
formations of the input.

Instead of the maximum pooling which we applied in
our experiments, one could also utilize average pooling lay-
ers. The equivariance of average pooling can be derived in
analogy to the derivation for maximum pooling.

B. Derivation of the generalized He weight ini-

tialization scheme

In this section we give the derivation of the generalized
weight initialization scheme whose results are stated in the
main paper. For completeness we recall the assumptions
going into the following calculations. We consider the acti-
vation of a single neuron in layer l,

ˆcx = max(0, y(l)
ζ (l)
ˆcx ),

(11)

where rectiﬁed linear units were chosen as nonlinearities.
The pre-nonlinearity activations are given by the convolu-
tion with ﬁlters Ψ and summing over the input channels:

For convenience we shifted the addition of the bias to the
pre-nonlinearity activations. The ﬁlters are deﬁned by

Ψ(l)

ˆccx =

w(l)

ˆccqψqx ,

Q
(cid:88)

q=1

that is, they are built from Q real valued atomic ﬁlters ψq.
We keep the discussion general by not restricting the atomic
ﬁlters to be steerable. In analogy to Glorot and Bengio [3]
and He et al. [4] we assume the activations and gradients to
be i.i.d. and to be independent from the weights. We let the
weights themselves be mutually independent and have zero
mean but do not restrict them to be identically distributed
because of the inherent asymmetry coming from the differ-
ent atomic ﬁlters. Furthermore we initialize all biases to be
zero.

B.1. Backpropagation

In order to prevent vanishing or exploding gradients of
the loss E due to inappropriate initialization we demand
their variance Var
to be constant across all layers.
It follows from (11) and (12) that the gradient with respect
to the activation ζ (l)
c0x0 of a particular neuron in layer l is
given by

(cid:104) ∂E
∂ζ(l)

(cid:105)

∂y(l+1)
ˆcx
∂ζ (l)
c0x0

∂E
∂ζ (l)
c0x0

=

=

(cid:88)

ˆc,x

(cid:88)

ˆc,x

∂E
∂y(l+1)
ˆcx
∂E
∂ζ (l+1)
ˆcx

I

y(l+1)
ˆcx >0

(cid:88)

q

w(l+1)

ˆcc0q ψq,x−x0,

(13)

where the indicator function I stems from the derivative of
the rectiﬁed linear unit. Like He et al. [4] we assume the
factors occurring in (13) to be statistically independent. Ob-
serving that E (cid:2)w(l)(cid:3) and therefore also E
vanish,
and without loss of generality setting x0 = 0 this leads to

(cid:104) ∂E
∂ζ(l)

(cid:105)

Var

(cid:34)

(cid:35)

∂E
∂ζ (l)
c0x0



(cid:32)

= E



(cid:88)

(cid:88)

(cid:88)

=

ˆc,ˆc(cid:48)

x,x(cid:48)

q,q(cid:48)

(cid:34)

E

∂E
∂ζ (l+1)
ˆcx

(cid:33)2


(cid:35)

(cid:104)
I

E

∂E
∂ζ (l)
c0x0

∂E
∂ζ (l+1)
ˆc(cid:48)x(cid:48)
(cid:104)

· E
(cid:33)2

I

y(l+1)
ˆcx >0

y(l+1)
ˆc(cid:48) x(cid:48) >0

(cid:105)

ˆcc0q w(l+1)
w(l+1)
ˆc(cid:48)c0q(cid:48)

ψq,xψq(cid:48),x(cid:48)

(cid:105)

(cid:105)

(cid:20)(cid:16)

· E

w(l+1)
ˆcc0q

(cid:17)2(cid:21)

ψ2

q,x

(cid:88)

(cid:88)

(cid:88)

=

ˆc

x

q



(cid:32)

E



∂E
∂ζ (l+1)
ˆcx

(cid:104)

 E

I
y(l+1)
ˆcx >0

(cid:88)

(cid:16)

y(l)
ˆcx =

ζ (l−1)
c

∗ Ψ(l)
ˆcc

+ β(l)
ˆc

(cid:17)

x

=

c
(cid:88)

(cid:88)

c

x(cid:48)

c,x−x(cid:48)Ψ(l)
ζ (l−1)

ˆccx(cid:48) + β(l)
ˆc .

(12)

(cid:88)

(cid:88)

(cid:88)

=

ˆc

x

q

(cid:35)

(cid:34)

1
2

Var

∂E
∂ζ (l+1)
ˆcx

Var

(cid:104)
w(l+1)
ˆcc0q

(cid:105)

ψ2

q,x.

The factor 1

2 in the last line originates from the sym-
metric distribution of y(l) in conjunction with the indicator
function. Using the fact that the weights’ variances are ini-
tialized to only depend on q and the assumption of iden-
tically distributed gradients, both can be pulled out of the
sums:

Var

= Var

(cid:21)

(cid:20) ∂E
∂ζ (l)
(cid:20) ∂E
∂ζ (l+1)

(cid:21) ˆC
2

q

(cid:88)

(cid:104)

Var

w(l+1)
q

(cid:105)

(cid:107)ψq(cid:107)2
2 .

It seems reasonable to assign the contribution to the overall
variance equally to the Q summands. Demanding the gra-
dients’ variances to be constant over layers then leads to the
initialization condition

Var [wq] =

2
ˆCQ (cid:107)ψq(cid:107)2
2

.

B.2. Forward pass

The calculation for the forward pass is similar to the case
of backpropagation but considers the variance Var (cid:2)y(l)(cid:3) of
pre-nonlinearity activations instead of gradients. As an ex-
act calculation depends on the expectation value E (cid:2)ζ (l−1)(cid:3),
which is not known, we approximate the result by exploit-
ing the central limit theorem. To this end, we note that
the pre-nonlinearity activations (12) are summed up from
C (cid:80)
q | supp ψq| independent terms of ﬁnite variance which
is a relatively large number in typical networks. This al-
lows to approximate the variance by the asymptotic result
implied by the central limit theorem:

Var

(cid:105)

(cid:104)

y(l)
ˆcx
(cid:34)

= Var

(cid:88)

(cid:88)

(cid:88)

c,x−x(cid:48)w(l)
ζ (l−1)

ˆccqψqx(cid:48)

c
(cid:88)

x(cid:48)
(cid:88)

q

(cid:104)

(CLT)
≈

(cid:88)

Var

c,x−x(cid:48)w(l)
ζ (l−1)

ˆccqψqx(cid:48)

(cid:35)

(cid:105)

c

x(cid:48)

q

(cid:88)

(cid:88)

(cid:88)

c

x(cid:48)

q

=

(cid:20)(cid:16)

E

ζ (l−1)
c,x−x(cid:48)

(cid:17)2(cid:21)

(cid:20)(cid:16)

E

w(l)
ˆccq

(cid:17)2(cid:21)

ψ2

qx(cid:48) .

In the last step we made use of the independence of
the weights from the previous layer’s feature maps and
E[w] = 0. The symmetric distribution of weights leads
to a symmetric distribution of pre-nonlinearity activations
which in conjunction with ReLU nonlinearities implies
E[ζ 2] = 1
2 Var[y]. To see this, note that the symmetry of the
distribution of pre-nonlinearity activation leads on the one

hand to

Var[y] = E[y2]

(cid:90)

=

˜y2 py(˜y) d˜y

R
(cid:90)

R+

= 2

˜y2 py(˜y) d˜y

and on the other hand to

E[ζ 2] =

(cid:90)

(cid:90)

R

R

(cid:90)

=

=

˜ζ 2 pζ(˜ζ) d˜ζ
(cid:18) 1
2

˜ζ 2

˜ζ 2 py(˜ζ) d˜ζ ,

R+

δ(0) + Θ(˜ζ)py(˜ζ)

d˜ζ

(cid:19)

where δ denotes the delta distribution and Θ is the Heav-
iside step function. As before, we drop all indices which
the random variables are independent from to compute the
sums. This leads to

y(l)(cid:105)
(cid:104)

Var

≈ Var

(cid:104)

y(l−1)(cid:105) C
2

(cid:88)

Var

(cid:105)

(cid:104)

w(l)
q

(cid:107)ψq(cid:107)2
2 ,

q

which in turn suggests a weight initialization according to

Var [wq] =

2
CQ (cid:107)ψq(cid:107)2
2

to ensure that the activations’ variances are not ampliﬁed.

B.3. Normalization of complex atomic ﬁlters

The results derived above suggest

to initialize the

weights of each layer uniformly by

Var [wq] =

or Var [wq] =

2
CQ (cid:107)ψq(cid:107)2
2

2
ˆCQ (cid:107)ψq(cid:107)2
2

after normalizing the atomic ﬁlters to (cid:107)ψq(cid:107)2 = 1. An ad-
ditional complication arises in our network construction
where steerability is only preserved when the relative ampli-
tude of the ﬁlters’ real and imaginary parts is not changed.
While for circular harmonics both parts have equal norms in
continuous space, this is not necessarily true for their sam-
pled versions which rules out an independent normalization
of the real and imaginary parts. As a steerability consistent
way of normalizing circular harmonics, we propose to ade-
quately normalize their complex modulus. The proper scale
follows from (cid:107)ψ(cid:107)2
2 for ψ ∈ C to
be (cid:107)ψ(cid:107)2 = 1 for DC ﬁlters whose imaginary part vanishes
and (cid:107)ψ(cid:107)2 =

2 = (cid:107)Re [ψ](cid:107)2

2 + (cid:107)Im [ψ](cid:107)2

2 for non-DC ﬁlters.

√

Operation

Filter Size

Feature Channels

Operation

Filter Size

Feature Channels

7 × 7
5 × 5
2 × 2

5 × 5
5 × 5
2 × 2

5 × 5
5 × 5

Steerable input layer
Steerable group convolution
Spatial max pooling

Steerable group convolution
Steerable group convolution
Spatial max pooling

Steerable group convolution
Steerable group convolution
Global spatial pooling
Global orientation pooling

Fully connected
Fully connected
Fully connected + Softmax

9 × 9
7 × 7
2 × 2

7 × 7
7 × 7
2 × 2

7 × 7
5 × 5

16
24

32
32

48
64

64
64
10

Steerable input layer
Steerable group convolution
Spatial max pooling

Steerable group convolution
Steerable group convolution
Spatial max pooling

Steerable group convolution
Steerable group convolution
Global spatial pooling
Global orientation pooling

Fully connected
Fully connected
Fully connected + Softmax

24
32

36
36

64
96

96
96
10

Table 2: Architecture of the SFCNN used in the initial experiments on the
resolution of sampled orientations and the rotational generalization.

Table 3: Architecture of the SFCNN used with Λ = 16 sampled orienta-
tions in the ﬁnal benchmarking experiments on rotated MNIST.

C. Details on the experimental setup

Here we give further details on the network architectures

and the training setup of our experiments.

C.1. Rotated MNIST

For our initial experiments on the dependence on sam-
pled orientations and the networks’ rotational generaliza-
tion capabilities we utilize the architecture given in Table 2
as baseline. Based on the results of these experiment we ﬁx
the number of sampled orientations to Λ = 16 and tune the
network architecture further. We achieve the best bench-
mark results using the slightly larger network given in Ta-
ble 3. In particular, we found that increasing the size of the
ﬁlter masks improved the results. Both architectures con-
sist of one steerable input layer which maps the input im-
ages to the group, ﬁve following group convolutional layers
and three fully connected layers. After every two steerable
ﬁlter layers we perform a spatial 2 × 2 max-pooling. The
orientation dimension and the remaining spatial dimensions
are pooled out globally after the last convolutional layer.
We normalize the activations by adding batch normaliza-
tion layers [32] after each convolutional and fully connected
layer. The batch normalization on the group does not inter-
fere with the equivariance when the responses are normal-
ized by averaging over both spatial and orientation dimen-
sions.

The number of feature channels stated in the tables refers
to the number of learned ﬁlters ˆC of the corresponding
layer. As these ﬁlters are themselves applied with respect to
Λ orientations we end up with ˆCΛ responses; e.g. 24 · 16 =
384 effective responses in the ﬁrst layer of the smaller net-
work. Note that the extraction of this comparatively large
number of responses without overﬁtting is possible because
the rotational weight sharing leads to an increased parame-

ter utilization (in the sense of Cohen and Welling [20]) by a
factor Λ.

All networks are trained for 40 epochs using the Adam
optimizer [33] with standard parameters. The initial learn-
ing rate is set to 0.015 and is decayed exponentially with
a rate of 0.8 per epoch starting from epoch 15. We regu-
larize the weights with an elastic net penalty with hyperpa-
rameters λL1 = λL2 which are set to 10−7 and 10−8 for
the convolutional and fully connected layers respectively.
Dropout [34] is used only in the fully connected layers with
a dropping probability of p = 0.3.

C.2. ISBI 2012 EM segmentation challenge

The network architecture used to segment the mem-
branes from raw EM images of neural tissue for the ISBI
EM segmentation challenge is visualized in Figure 6. In-
spired by the U-Net [31] it is build as a symmetric encoder-
decoder network with additional skip-connections between
stages of the same resolution. This allows to extract seman-
tic information from a large ﬁeld of view while at the same
time preserving precise spatial localization. Further, we
adopt two modiﬁcations from [26]: we do not concatenate
the skipped feature maps but add it to the decoder features
upsampled from the previous stage, and we use intermediate
residual blocks (here of depth 1). On the highest resolution
level we learn ˆC = 12 ﬁlters, applied in Λ = 17 orienta-
tions which corresponds to ˆCΛ = 204 effective channels.
The number of ﬁlters is doubled when going to the second
and third level and is afterwards kept constant since we did
not observe further gains in performance when adding more
channels. All group-convolutional layers utilize kernels of
size 7 × 7 pixels while the input layer applies 11 × 11 pixel
kernels.

As input, we feed the network cropped regions of 256 ×

Figure 6: Network architecture used to predict the membrane probability map for the ISBI 2012 EM segmentation challenge. The topology is inspired by
the U-Net [31] and FusionNet [26] but uses the proposed steerable group-convolution layers with Λ = 17 orientations. To mitigate boundary artifacts we
feed reﬂect-padded images into the network.

256 pixels which are padded to 320 × 320 pixels by re-
ﬂecting a region of 32 pixels around the borders to alleviate
boundary artifacts. The padded regions are augmented by
random elastic deformations, reﬂections and rotations by
multiples of π
2 . After the decoder we max-pool over ori-
entations to obtain locally invariant features and crop out
256 × 256 pixels centrally. Two subsequent 1 × 1 convo-
lution layers map these features pixel-wise to the desired
probability map.

The network is optimized by minimizing the spatially av-
eraged binary cross-entropy loss between predictions and
the ground truth segmentation masks using the ADAM op-
timizer. As on the rotated MNIST dataset we regularize
the convolutional weights with an elastic net penalty with
hyperparameters λL1 = λL2 set to 10−7 and 10−8 for the
steerable and 1 × 1 convolution layers respectively. Here
we chose a dropout probability of p = 0.4 both in the steer-
able as well as in the 1 × 1 convolution layers. The learning
rate is decayed exponentially by a factor of 0.85 per epoch
starting from an initial rate of 5 · 10−2.

References

[1] M. Zeiler and R. Fergus, “Visualizing and understanding
convolutional networks,” in European Conference on Com-
puter Vision (ECCV). Springer, 2014, pp. 818–833.

[2] T. Cohen and M. Welling, “Group equivariant convolutional
networks,” in International Conference on Machine Learn-
ing (ICML), 2016.

[3] X. Glorot and Y. Bengio, “Understanding the difﬁculty of
training deep feedforward neural networks,” in International
Conference on Artiﬁcial Intelligence and Statistics (AIS-
TATS), 2010.

[4] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation,” in IEEE International Conference on Com-
puter Vision (ICCV), 2015, pp. 1026–1034.

[5] I. Arganda-Carreras, S. C. Turaga, D. R. Berger, D. Cires¸an,
A. Giusti, L. M. Gambardella, J. Schmidhuber, D. Laptev,
S. Dwivedi, J. M. Buhmann et al., “Crowdsourcing the cre-
ation of image segmentation algorithms for connectomics,”
Frontiers in neuroanatomy, vol. 9, 2015.

[6] Y. Hel-Or and P. C. Teo, “Canonical decomposition of steer-
able functions,” Journal of Mathematical Imaging and Vi-
sion, vol. 9, no. 1, pp. 83–95, 1998.

[7] W. Freeman and E. Adelson, “The design and use of steer-
able ﬁlters,” IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, vol. 13, no. 9, pp. 891–906, 1991.

[14] R. Gens and P. Domingos, “Deep symmetry networks,” in
Advances in Neural Information Processing Systems (NIPS).
Curran Associates, Inc., 2014, pp. 2537–2545.

[15] J. F. Henriques and A. Vedaldi, “Warped convolutions: Ef-
ﬁcient invariance to spatial transformations,” in Proceedings
of the 34th International Conference on Machine Learning,
vol. 70, 2017, pp. 1461–1469.

[16] M.

Jaderberg, K. Simonyan, A. Zisserman,

and
in
K. Kavukcuoglu,
Advances in Neural Information Processing Systems (NIPS),
2015, pp. 2017–2025.

transformer networks,”

“Spatial

[17] C.-H. Lin and S. Lucey, “Inverse compositional spatial trans-
former networks,” IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2017.

[18] J.-H. Jacobsen, B. de Brabandere, and A. W. Smeulders,
“Dynamic steerable blocks in deep residual networks,” arXiv
preprint arXiv:1706.00598, 2017.

[19] S. Dieleman, J. De Fauw, and K. Kavukcuoglu, “Exploiting
cyclic symmetry in convolutional neural networks,” in Inter-
national Conference on Machine Learning (ICML), 2016.

[20] T. Cohen and M. Welling, “Steerable CNNs,” in Inter-
national Conference on Learning Representations (ICLR),
2017.

[21] D. Laptev, N. Savinov, J. Buhmann, and M. Pollefeys,
“TI-POOLING:
transformation-invariant pooling for fea-
ture learning in Convolutional Neural Networks,” in IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2016, pp. 289–297.

[8] Y.-N. Hsu and H. Arsenault, “Optical pattern recognition us-
ing circular harmonic expansion,” Applied Optics, vol. 21,
no. 22, pp. 4016–4019, 1982.

[22] D. Marcos, M. Volpi, and D. Tuia, “Learning rotation invari-
ant convolutional ﬁlters for texture classiﬁcation,” in Inter-
national Conference on Pattern Recognition (ICPR), 2016.

[9] J. Rosen and J. Shamir, “Circular harmonic phase ﬁlters for
efﬁcient rotation-invariant pattern recognition,” Applied Op-
tics, vol. 27, no. 14, pp. 2895–2899, 1988.

[10] A. Krizhevsky, I. Sutskever, and G. Hinton, “Imagenet clas-
siﬁcation with deep convolutional neural networks,” in Ad-
vances in Neural Information Processing Systems (NIPS),
2012, pp. 1097–1105.

[11] K. Sohn and H. Lee, “Learning invariant representations with
local transformations,” in International Conference on Ma-
chine Learning (ICML), 2012, pp. 1311–1318.

[12] A. Kanazawa, A. Sharma, and D. Jacobs, “Locally
scale-invariant convolutional neural networks,” Preprint
arXiv:1412.5104, 2014.

[13] C. Zhang, S. Voinea, G. Evangelopoulos, L. Rosasco,
and T. Poggio, “Discriminative template learning in group-
convolutional networks for invariant speech representations,”
in Annual Conference of the International Speech Communi-
cation Association, 2015.

[23] D. Marcos, M. Volpi, N. Komodakis, and D. Tuia, “Rotation
equivariant vector ﬁeld networks,” arXiv:1612.09346, 2016.

[24] D. Worrall, S. Garbin, D. Turmukhambetov, and G. Brostow,
“Harmonic networks: Deep translation and rotation equiv-
ariance,” Preprint arXiv:1612.04642, 2016.

[25] L. Sifre and S. Mallat, “Rotation, scaling and deforma-
tion invariant scattering for texture discrimination,” in IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2013, pp. 1233–1240.

[26] T. M. Quan, D. G. Hilderbrand, and W.-K. Jeong, “Fu-
sionnet: A deep fully residual convolutional neural network
for image segmentation in connectomics,” arXiv preprint
arXiv:1612.05360, 2016.

[27] T. Beier, C. Pape, N. Rahaman, T. Prange, S. Berg, D. D.
Bock, A. Cardona, G. W. Knott, S. M. Plaza, L. K. Schef-
fer, U. Koethe, A. Kreshuk, and F. A. Hamprecht, “Multi-
cut brings automated neurite segmentation closer to human
performance,” Nature Methods, vol. 14, no. 2, pp. 101–102,
2017.

[28] M. Drozdzal, E. Vorontsov, G. Chartrand, S. Kadoury, and
C. Pal, “The importance of skip connections in biomedical
image segmentation,” in International Workshop on Large-
Scale Annotation of Biomedical Data and Expert Label Syn-
thesis. Springer, 2016, pp. 179–187.

[29] U. Schmidt and S. Roth, “Learning rotation-aware features:
From invariant priors to equivariant descriptors,” in IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2012, pp. 2050–2057.

[30] H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and
Y. Bengio, “An empirical evaluation of deep architectures on
problems with many factors of variation,” in International
Conference on Machine Learning (ICML), 2007, pp. 473–
480.

[31] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolu-
tional networks for biomedical image segmentation,” in In-
ternational Conference on Medical Image Computing and
Computer-Assisted Intervention. Springer, 2015, pp. 234–
241.

[32] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating
deep network training by reducing internal covariate shift,”
in International Conference on Machine Learning (ICML),
2015, pp. 448–456.

[33] D. Kingma and J. Ba, “Adam: A method for stochastic op-
timization,” in International Conference on Learning Repre-
sentations (ICLR), 2015.

[34] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov, “Dropout: a simple way to prevent neu-
ral networks from overﬁtting,” Journal of Machine Learning
Research, vol. 15, no. 1, pp. 1929–1958, 2014.

