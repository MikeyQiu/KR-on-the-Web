0
2
0
2
 
n
a
J
 
6
1
 
 
]
L
C
.
s
c
[
 
 
4
v
1
1
5
1
1
.
8
0
9
1
:
v
i
X
r
a

DCMN+: Dual Co-Matching Network for Multi-choice Reading Comprehension

Shuailiang Zhang,1,2,3 Hai Zhao,1,2,3∗ Yuwei Wu,1,2,3 Zhuosheng Zhang,1,2,3
Xi Zhou,4 Xiang Zhou4
1Department of Computer Science and Engineering, Shanghai Jiao Tong University
2Key Laboratory of Shanghai Education Commission for Intelligent Interaction
and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China
3MoE Key Lab of Artiﬁcial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China
4CloudWalk Technology, Shanghai, China
zsl123@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn,
{will8821,zhangzs}@sjtu.edu.cn, {zhouxi,zhouxiang}@cloudwalk.cn

Abstract

Multi-choice reading comprehension is a challenging task
to select an answer from a set of candidate options when
given passage and question. Previous approaches usually only
calculate question-aware passage representation and ignore
passage-aware question representation when modeling the re-
lationship between passage and question, which cannot effec-
tively capture the relationship between passage and question.
In this work, we propose dual co-matching network (DCMN)
which models the relationship among passage, question and
answer options bidirectionally. Besides, inspired by how hu-
mans solve multi-choice questions, we integrate two read-
ing strategies into our model: (i) passage sentence selec-
tion that ﬁnds the most salient supporting sentences to an-
swer the question, (ii) answer option interaction that encodes
the comparison information between answer options. DCMN
equipped with the two strategies (DCMN+) obtains state-of-
the-art results on ﬁve multi-choice reading comprehension
datasets from different domains: RACE, SemEval-2018 Task
11, ROCStories, COIN, MCTest.

Introduction
Machine reading comprehension (MRC) is a fundamental
and long-standing goal of natural language understanding
which aims to teach machine to answer question automati-
cally according to given passage (Hermann et al. 2015; Ra-
jpurkar et al. 2016; Nguyen et al. 2016; Zhang et al. 2018).
In this paper, we focus on multi-choice MRC tasks such as
RACE (Lai et al. 2017) which requests to choose the right
option from a set of candidate answers according to given
passage and question. Different from MRC datasets such as
SQuAD (Rajpurkar et al. 2016) and NewsQA (Trischler et
al. 2017) where the expected answer is usually in the form
of a short span from the given passage, answer in multi-
choice MRC is non-extractive and may not appear in the
original passage, which allows rich types of questions such

∗ Corresponding author. This paper was partially supported
by National Key Research and Development Program of China
(No. 2017YFB0304100), Key Projects of National Natural Science
Foundation of China (U1836222 and 61733011).
Copyright c(cid:13) 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Passage: Runners in a relay race pass a stick in one
direction. However, merchants passed silk, gold, fruit,
and glass along the Silk Road in more than one direc-
tion. They earned their living by traveling the famous
Silk Road. ... The Silk Road was made up of many
routes, not one smooth path. They passed through
what are now 18 countries. The routes crossed moun-
tains and deserts and had many dangers of hot sun,
deep snow and even battles...
Question: The Silk Road became less important be-
cause .

A. it was made up of different routes
B. silk trading became less popular
C. sea travel provided easier routes
D. people needed fewer foreign goods

Table 1: An example passage with related question and op-
tions from RACE dataset. The ground-truth answer and the
evidence sentences in the passage are in bold.

as commonsense reasoning and passage summarization, as
illustrated by the example in Table 1.

Pre-trained language models such as BERT (Devlin et al.
2019) and XLNet (Yang et al. 2019) have achieved signiﬁ-
cant improvement on various MRC tasks. Recent works on
MRC may be put into two categories, training more pow-
erful language models or exploring effective applying pat-
tern of the language models to solve speciﬁc task. There
is no doubt that training a better language model is es-
sential and indeed extremely helpful (Devlin et al. 2019;
Yang et al. 2019) but at the same time it is time-consuming
and resource-demanding to impart massive amounts of gen-
eral knowledge from external corpora into a deep language
model via pre-training (Sun et al. 2019; Zhang et al. 2019b).
For example, training a 24-layer transformer (Devlin et al.
2019) requires 64 TPUs for 4 days. So from the prac-
tical viewpoint, given limited computing resources and a
well pre-trained model, can we improve the machine read-
ing comprehension during ﬁne-tuning instead of via expen-
sive full pre-training? This work starts from this viewpoint
and focuses on exploring effective applying pattern of lan-

Figure 1: The framework of our model. P-Passage, Q-Question, O-Option.

guage models instead of presenting better language models
to furthermore enhance state-of-the-art multi-choice MRC.
We will show the way to use a strong pre-trained language
model may still have a heavy impact on MRC performance
no matter how strong the language model itself is.

To well handle multi-choice MRC problem, an effective
solution has to carefully model the relationship among the
triplet of three sequences, passage (P), question (Q) and an-
swer candidate options (A) with a matching module to deter-
mine the answer. However, previous unidirectional match-
ing strategies usually calculate question-aware passage rep-
resentation and ignore passage-aware question representa-
tion when modeling the relationship between passage and
question (Wang et al. 2018b; Tang, Cai, and Zhuo 2019;
Chen et al. 2019).

To alleviate such an obvious defect in modeling the {P,
Q, A} triplet from existing work, we propose dual co-
matching network (DCMN) which bidirectionally incorpo-
rates all the pairwise relationships among the {P, Q, A}
triplet. In detail, we model the passage-question, passage-
option and question-option pairwise relationship simulta-
neously and bidirectionally for each triplet, exploiting the
gated mechanism to fuse the representations from two direc-
tions. Besides, we integrate two reading strategies which hu-
mans usually use into the model. One is passage sentence se-
lection that helps extract salient evidence sentences from the
given passage, and then matches evidence sentences with an-
swer options. The other is answer option interaction that en-
codes comparison information into each option. The overall
framework is shown in Figure 1. The output of pre-trained
language model (i.e. BERT (Devlin et al. 2019) and XLNet
(Yang et al. 2019)) is used as the contextualized encoding.
After passage sentence selection and answer option inter-
action, bidirectional matching representations are built for
every pairwise relationship among the {P, Q, A} triplet.

Our model achieves new state-of-the-art results on the
multi-choice MRC benchmark challenge RACE (Lai et al.
2017). We further conduct experiments on four representa-
tive multi-choice MRC datasets from different domains (i.e.,
ROCStories (Mostafazadeh et al. 2016), SemEval-2018 Task
11 (Ostermann et al. 2018), MCTest (Richardson, Burges,
and Renshaw 2013), COIN Shared Task 1 (Ostermann et al.

2018)) and achieve the absolute improvement of 4.9% and
2.8% in average accuracy from directly ﬁne-tuned BERT
and XLNet, respectively, which indicates our method has
a heavy impact on the MRC performance no matter how
strong the pre-trained language model itself is.

Our Proposed Model
The illustration of our model is shown in Figure 1. The ma-
jor components of the model are Contextualized Encoding,
Passage Sentence Selection, Answer Option Interaction and
Bidirectional Matching. We will discuss each component in
detail.

Task Deﬁnition
For the task of multi-choice reading comprehension, the ma-
chine is given a passage (P), a question (Q), and a set of an-
swer candidate options (A) to select the correct answer from
the candidates, where P = {p1, p2, ..., pn} is the passage
composed of n sentences, A = {A1, A2, ..., Am} is the
option set with m answer candidates.

Contextualized Encoding
In this work, pre-trained language models are used as the
encoder of our model which encodes each token in passage
and question into a ﬁxed-length vector. Given an encoder,
the passage, the question, and the answer options are en-
coded as follows:

Hp = Encode(P), Hq = Encode(Q)

Ha =Encode(A)

(1)

where Encode(·) returns the last layer output by the en-
coder, which can be well pre-trained language models such
as BERT (Devlin et al. 2019) and XLNet (Yang et al.
2019), as using transformer as the contextualized encoder
has shown to be very powerful in language representation
(Zhang et al. 2019a; Zhou and Zhao ; Luo, Xiao, and Zhao
2019; Xiao et al. 2019). Hp ∈ R|P |×l, Hq ∈ R|Q|×l, and
Ha ∈ R|A|×l are sequence representation of passage, ques-
tion and answer option, respectively. |P |, |Q|, |A| are the
sequence length, respectively. l is the dimension of the hid-
den state.

N
sent
1

% on
RACE
65

% on
COIN
76

Passage

2

22

10

>=3

13

14

Soon after, the snack came out. I then opened the chips and
started to enjoy them, before enjoying the soda. I had a great
little snack...
She lived in the house across the street that had 9 people and
3 dogs and another cat living in it. She didn’t seem very happy
there, especially with a 2 year old that chased her and grabbed
her. The other people in the house agreed...
When I was hungry last week for a little snack and a soda, I
went to the closest vending machine. I felt that it was a little
overpriced, but being as though I needed something...

Question

What else did the person en-
joy?

What did the 2 year old’s
mom own?

What’s the main idea of this
passage?

Table 2: Analysis of the sentences in passage required to answer questions on RACE and COIN. 50 examples from each dataset
are sampled randomly. N sent indicates the number of sentences required to answer the question. The evidence sentences in the
passage are in emphasis and the correct answer is with bold.

Passage Sentence Selection

quences, which can be calculated as follows:

Existing multi-choice MRC models learn the passage rep-
resentation with all the sentences in one-shot, which is in-
efﬁcient and counter-intuitive. To explore how many sen-
tences are necessarily required to answer the question, we
randomly extract 50 examples from the development set of
RACE and COIN, as shown in Table 2. Among all exam-
ples, 87% questions on RACE and 86% on COIN can be
answered within two evidence sentences. From this obser-
vation, the model should be extremely beneﬁcial if focusing
on a few key evidence sentences.

To select the evidence sentences from the passage P =
{p1, p2, .., pi, .., pn}, this module scores each sentence pi
with respect to the question Q and answer option A in par-
allel. The top K scored sentences will be selected. This
module shares the encoder with the whole model. For each
{pi, Q, A} triplet, Hpi ∈ R|pi|×l, Hq, and Ha are all rep-
resentations offered by the encoder. Here we introduce two
methods to compute the score of the triplet based on the rep-
resentations.

• Cosine score: The model computes word-by-word cosine
similarity between the sentence and question-option se-
quence pair.

Dpa = Cosine(Ha, Hpi) ∈ R|A|×|pi|
Dpq = Cosine(Hq, Hpi ) ∈ R|Q|×|pi|
¯Dpa = M axP ooling(Dpa) ∈ R|A|
¯Dpq = M axP ooling(Dpq) ∈ R|Q|
¯Dpa
k

¯Dpq
k

(cid:80)|A|
k=1
|A|

+

(cid:80)|Q|
k=1
|Q|

score =

(2)

where Dpa, Dpa are the distance matrices and Dpa
ij is the
cosine similarity between the i-th word in the candidate
option and the j-th word in the passage sentence.

α = Sof tM ax(HqW1) ∈ R|Q|×l
q = αT Hq ∈ Rl
¯Pj = Hpi
pq
ˆP

j W2q ∈ Rl, j ∈ [1, |pi|]
= M ax(¯P1 ¯P2, ..., ¯P|pi|) ∈ Rl

(3)

where W1, W2 ∈ Rl×l are learnable parameters, ˆP
is
the bilinear similarity vector between the passage sen-
tence and question. Similarly, the vector ˆP
between the
passage sentence and answer can be calculated with the
same procedure. The ﬁnal score can be computed as fol-
lows:

pa

pq

score = W T
3

pq
ˆP

+ W T
4

pa
ˆP

(4)

where W3, W4 ∈ Rl are learnable parameters.
After scoring each sentence, top K scored sentences are
selected and concatenated together as an updated passage Ps
to replace original full passage. So the new sequence triplet
is {Ps, Q, A} and the new passage is represented as Hps.

Answer Option Interaction
Human solving multi-choice problem may seek help from
comparing all answer options. For example, one option has
to be picked up not because it is the most likely correct, but
all the others are impossibly correct. Inspired by such hu-
man experience, we introduce the comparison information
among answer options so that each option is not independent
of the other. Here we build bilinear representations between
any two options. Gated mechanism (Srivastava, Greff, and
Schmidhuber 2015) is used to fuse interaction representa-
tion into the original answer option representations.

The encoder encodes each answer option Ai as Hai. Then
the comparison vector between option Ai and Aj can be
computed as follows:

G = Sof tM ax(HaiW5Haj T ) ∈ R|Ai|×|Aj |

Hai,j = ReLU (GHaj ) ∈ R|Ai|×l

(5)

• Bilinear score: Inspired by (Min et al. 2018a), we com-
pute the bilinear weighted distance between two se-

where W5 ∈ Rl×l is one learnable parameter, G is the bi-
linear interaction matrix between Ai and Aj, Hai,j is the

interaction representation. Then gated mechanism is used to
fuse interaction representation into the original answer op-
tion representations as follows:

ˆH
¯Hai = ˆH

ai = [{Hai,j }j(cid:54)=i] ∈ R|Ai|×(m−1)l
aiW6 ∈ R|Ai|×l
g = σ( ¯HaiW7 + HaiW8 + b)
Hoi = g ∗ Hai + (1 − g) ∗ ¯Hai

(6)

where W7, W8 ∈ Rl×l and W6 ∈ R(m−1)l×l are learnable
ai is the concatenation of all the interaction
parameters, ˆH
representations. g ∈ R|Ai|×l is a reset gate which balances
the inﬂuence of ¯Hai and Hai, and Hoi is the ﬁnal option rep-
resentation of Ai encoded with the interaction information.
At last, we denote O = {Ho1, Ho2 , ..., Hom} as the ﬁnal
answer option representation set fused with comparison in-
formation across answer options.

Bidirectional Matching
The triplet changes from {P, Q, A} to {Ps, Q, O} with
passage sentence selection and answer option interaction.
To fully model the relationship in the {Ps, Q, O} triplet,
bidirectional matching is built to get all pairwise representa-
tions among the triplet, including passage-answer, passage-
question and question-answer representation. Here shows
how to model the relationship between question-answer se-
quence pair as an example and it is the same for the other
two pairs.

Bidirectional matching representation between the ques-
tion Hq and answer option Ho can be calculated as follows:

Gqo = Sof tM ax(HqW9HoT )
Goq = Sof tM ax(HoW10Hq T )
Eq = GqoHo, Eo = GoqHq
Sq = ReLU (EqW11)
So = ReLU (EoW12)

where W9, W10, W11, W12 ∈ Rl×l are learnable pa-
rameters. Gqo ∈ R|Q|×|O| and Goq ∈ R|O|×|Q| are the
weight matrices between question and answer option. Eq ∈
R|Q|×l, Eo ∈ R|A|×l represent option-aware question repre-
sentation and question-aware option representation, respec-
tively. The ﬁnal representation of question-answer pair is
calculated as follows:

Sq o = M axP ooling(Sq)
So q = M axP ooling(So)

g = σ(Sq oW13 + So qW14 + b)

Mq o = g ∗ So q + (1 − g) ∗ So q

(8)

where W13, W14 ∈ Rl×l and b ∈ Rl are three learnable pa-
rameters. After a row-wise max pooling operation, we get
the aggregation representation Mq ∈ Rl and Mo ∈ Rl.
g ∈ Rl is a reset gate. Mq o ∈ Rl is the ﬁnal bidirec-
tional matching representation of the question-answer se-
quence pair.

Passage-question and passage-option sequence matching
representation Mp q, Mp o ∈ Rl can be calculated in the
same procedure from Eq.(7) to Eq.(8). The framework of
this module is shown in Figure 1.

Objective Function
With the built matching representations Mp q, Mp o, Mq o
for three sequence pairs, we concatenate them as the ﬁnal
representation C ∈ R3l for each passage-question-option
triplet. We denote the representation Ci for each {Ps, Q, Oi}
triplet. If Ak is the correct option, then the objective function
can be computed as follows:

C = [Mp q; Mp o; Mq o]

L(Ak|P, Q) = −log

exp(V T Ck)
j=1 exp(V T Cj)

(cid:80)m

(9)

where V ∈ R3l is a learnable parameter and m is the number
of answer options.

Experiments

Dataset
We evaluate our model on ﬁve multi-choice MRC datasets
from different domains. Statistics of these datasets are de-
tailed in Table 3. Accuracy is calculated as acc = N +/N ,
where N + and N are the number of correct predictions
and the total number of questions. Some details about these
datasets are shown as follows:

• RACE (Lai et al. 2017): RACE consists of two sub-
sets: RACE-M and RACE-H respectively corresponding
to middle school and high school difﬁculty levels, which
is recognized as one of the largest and most difﬁcult
datasets in multi-choice reading comprehension.

(7)

• SemEval-2018 Task11 (Ostermann et al. 2018): Multi-
choice questions should be answered based on narrative
texts about everyday activities.

• ROCStories (Mostafazadeh et al. 2016): This dataset
contains 98,162 ﬁve-sentence coherent stories in the train-
ing dataset, 1,871 four-sentence story contexts along with
a right ending and a wrong ending in the development and
test datasets, respectively.

• MCTest (Richardson, Burges, and Renshaw 2013): This
task requires machines to answer questions about ﬁctional

Domain
general
narrative text

Task
RACE
SemEval
ROCStories stories
stories
MCTest
everyday scenarios
COIN

#o
4
2
2
4
2

#q

#p
27,933 97,687
13,939
2,119
3472
3472
2,640
660
5,102

Table 3: Statistics of multi-choice machine reading compre-
hension datasets. #o is the average number of candidate op-
tions for each question. #p and #q are the number of docu-
ments and questions in the dataset.

∗

Model
HAF (Zhu et al. 2018)
MRU (Tay, Tuan, and Hui 2018)
HCM (Wang et al. 2018b)
MMN (Tang, Cai, and Zhuo 2019)
GPT (Radford 2018)
RSM (Sun et al. 2019)
OCN (Ran et al. 2019)
XLNet (Yang et al. 2019)
BERTbase
BERTlarge
XLNetlarge
Our Models
BERTbase
BERTlarge
BERTlarge
XLNetlarge
Human Performance
Turkers
Ceiling

∗ + DCMN
∗ + DCMN
∗ + DCMN + PSS + AOI
∗ + DCMN + PSS + AOI

∗

∗

RACE-M/H RACE
45.0/46.4
57.7/47.4
55.8/48.2
61.1/52.2
62.9/57.4
69.2/61.5
76.7/69.6
85.5/80.2
71.1/62.3
76.6/70.1
83.7/78.6

46.0
50.4
50.4
54.7
59.0
63.8
71.7
81.8
65.0
72.0
80.1

73.2/64.2
79.2/72.1
79.3/74.4
86.5/81.3

85.1/69.4
95.4/94.2

67.0
74.1
75.8
82.8

73.3
94.5

Table 4: Experiment results on RACE test set. All the re-
sults are from single models. PSS: Passage Sentence Selec-
tion; AOI: Answer Option Interaction. ∗ indicates our imple-
mentation.

stories, directly tackling the high-level goal of open-
domain machine comprehension.

• COIN Task 1 (Ostermann et al. 2018): The data for the
task is short narrations about everyday scenarios with
multiple-choice questions.

Implementation Details
Our model is evaluated based on the pre-trained language
model BERT (Devlin et al. 2019) and XLNet (Yang et al.
2019) which both have small and large versions. The ba-
sic version BERTbase has 12-layer transformer blocks, 768
hidden-size, and 12 self-attention heads, totally 110M pa-
rameters. The large version BERTlarge has 24-layer trans-
former blocks, 1024 hidden-size, and 16 self-attention
heads, totally 340M parameters. Two versions of XLNet
have the similar sizes as BERT.

In our experiments, the max input sequence length is set
to 512. A dropout rate of 0.1 is applied to every BERT layer.
We optimize the model using BertAdam (Devlin et al. 2019)
optimizer with a learning rate 2e-5. We train for 10 epochs
with batch size 8 using eight 1080Ti GPUs when BERTlarge
and XLNetlarge are used as the encoder. Batch size is set to
16 when using BERTbase and XLNetbase as the encoder1.

Evaluation and Ablation Study on RACE
Table 4 reports the experimental results on RACE and its two
subtasks: RACE-M and RACE-H. In the table, Turkers is
the performance of Amazon Turkers on a randomly sampled
subset of the RACE test set and Ceiling is the percentage of
the unambiguous questions with a correct answer in a subset

1Our code is at https://github.com/Qzsl123/dcmn.

base encoder
+ DCMN
+ DCMN + P SS
+ DCMN + P OI
+ DCMN + ALL (DCMN+)

BERTbase
64.6
66.0 (+1.4)
66.6 (+2.0)
66.8 (+2.2)
67.4 (+2.8)

BERTlarge
71.8
73.8 (+2.0)
74.6 (+2.8)
74.4 (+2.6)
75.4 (+3.6)

XLNetlarge
80.1
81.5 (+1.4)
82.1 (+2.0)
82.2 (+2.1)
82.6 (+2.5)

Table 5: Ablation study on RACE dev set. PSS: Passage Sen-
tence Selection. AOI: Answer Option Interaction. DCMN+:
DCMN + PSS + AOI.

of the test set. Here we give the results of directly ﬁne-tuned
BERTbase, BERTlarge and XLNetlarge on RACE and get the
accuracy of 65.0%, 72.0% and 80.1%, respectively. Because
of the limited computing resources, the largest batch size can
only be set to 8 in our experiments which leads to 1.7% de-
crease (80.1% vs. 81.8%) on XLNet compared to the result
reported in (Yang et al. 2019)2.

The comparison indicates that our proposed method ob-
tains signiﬁcant
improvement over pre-trained language
models (75.8% vs. 72.0% on BERTlarge and 82.8% vs. 80.1%
on XLNetlarge) and achieves the state-of-the-art result on
RACE.

In Table 5, we focus on the contribution of main compo-
nents (DCMN, passage sentence selection and answer op-
tion interaction) in our model. From the results, the bidirec-
tional matching strategy (DCMN) gives the main contribu-
tion and achieves further improvement by integrating with
the two reading strategies. Finally, we have the best perfor-
mance by combining all components.

Evaluation on Other Multi-choice Datasets

The results on four other multi-choice MRC challenges are
shown in Table 6. When adapting our method to the non-
conventional MRC dataset ROCStories which requires to
choose the correct ending to a four-sentence incomplete
story from two answer options (Mostafazadeh et al. 2016),
the question context is left empty as no explicit questions
are provided. Passage sentence selection is not used in this
dataset because there are only four sentences as the passage.
Since the test set of COIN is not publicly available, we re-
port the performance of the model on its development set.

As shown in Table 6, we achieve state-of-the-art (SOTA)
results on all datasets and obtain 3.1% absolute improve-
ment in average accuracy over the previous average SOTA
(88.9% vs. 85.8%) by using BERT as encoder and 4.8%
(90.6% vs. 85.8%) by using XLNet as encoder. To further
investigate the contribution of our model, we also report
the results of directly ﬁne-tuned BERT/XLNet on the target
datasets. From the comparison, we can see that our model
obtains 4.9% and 2.8% absolute improvement in average ac-
curacy over the baseline of directly ﬁne-tuned BERT (88.9%
vs. 84.0%) and XLNet (90.6% vs. 87.8%), respectively.
These results indicate our proposed model has a heavy im-
pact on the performance no matter how strong the adopted
pre-trained language model itself is.

2The implementation is very close to the result 80.3% in (Yang

et al. 2019) when using batch size 8 on RACE.

Task
SemEval Task 11
ROCStories
MCTest-MC160
MCTest-MC500
COIN Task 1
Average

Previous STOA
(Sun et al. 2019)
(Li, Ding, and Liu 2019)
(Sun et al. 2019)
(Sun et al. 2019)
(Devlin et al. 2019)

BERT DCMN BERT XLNet DCMN XLNet
90.5
90.8
73.8
80.4
84.3
84.0

91.8 (+1.3)
92.4 (+1.6)
85.0 (+11.2)
86.5 (+6.1)
88.8 (+4.5)
88.9 (+4.9)

93.4 (+1.4)
95.8 (+2.0)
86.2 (+5.6)
86.6 (+3.2)
91.1 (+2.0)
90.6 (+2.8)

92.0
93.8
80.6
83.4
89.1
87.8

89.5
91.8
81.7
82.0
84.2
85.8

Table 6: Results on the test set of SemEval Task 11, ROCStories, MCTest and the development set of COIN Task 1. The test
set of COIN is not public. DCMN BERT: BERT + DCMN + PSS + AOI. Previous SOTA: previous state-of-the-art model. All
the results are from single models.

Model
BERTbase
+ Unidirectional
[SP O; SP Q; SO Q]
[SP O; SQ P ; SO Q]
[SP Q; SP O] (HCM)
[SP O; SP Q; SQ O] (HAF)
[SQ O; SO Q; SP Q; SP O] (MMN)
+ Bidirectional
[M P Q; M P O]
[M P Q; M P O; M Q O] (DCMN)

65.0
65.2
64.4
64.2
63.2

66.4
67.1

RACE Model
64.6

RACE

Model

RACE

[SP Q; SQ O]
[SP O; SQ O]
[SP O; SO Q]
[SP O; SQ P ; SQ O]

63.4
63.6
64.2
64.4

[SP Q; SO Q]
[SQ P ; SO Q]
[SP Q; SO P ]
[SQ P ; SQ O]

64.5
65.2
64.7
64.3

[M P O; M Q O]

66.0

[M P Q; M Q O]

65.5

Table 7: Performance comparison with different combination methods on the RACE dev set. (HCM) (Wang et al. 2018b),
(HAF) (Zhu et al. 2018), (MMN) (Tang, Cai, and Zhuo 2019) are previous methods. We use BERTbase as our encoder here. [; ]
indicates the concatenation operation. SP O and M P O are the unidirectional and bidirectional representation referred in Eq. 8.

Comparison with Unidirectional Methods

Here we focus on whether the bidirectional matching
method works better than previous unidirectional methods.
In Table 7, we enumerate all the combinations of unidirec-
tional matching strategies3 which only use passage-aware
question representation SQ P or question-aware passage
representation SP Q when modeling the relationship be-
tween the passage and question. Specially, we roughly sum-
marize the matching methods in previous work (i.e. HCM,
HAF, MMN) using our model notations which meet their
general ideas except some calculation details.

From the comparison, we observe that previous matching
strategies (HCM 64.4%, HAF 64.2%, MMN 63.2%) fail to
give further performance improvement over the strong en-
coder (64.6%). In contrast, all bidirectional combinations
work better than the encoder. All three pairwise matching
representations (M P Q, M P O, M Q O) are necessary and
by concatenating them together, we achieve the highest per-
formance (67.1%).

Results with Different Settings in PSS

Table 8 shows the performance comparison with different
scoring methods, and we observe that both methods have
their advantages and disadvantages. Cosine score method

3Here we omit the combinations with SO P because we ﬁnd the

combinations with SP O works better than SO P .

Top K
RACE-cos
RACE-bi
COIN-cos
COIN-bi

1
58.4
59.5
81.0
81.7

2
60.1
60.5
82.0
82.0

3
63.3
63.4
83.5
82.6

4
65.8
66.8
83.0
82.8

5
66.5
66.4
82.5
82.4

6
66
66.2
82.4
82.2

Table 8: Results on RACE and COIN dev set with cosine
and bilinear score in PSS. We use BERTbase as encoder here.

works better on COIN dataset (83.5% vs. 82.8%) and bilin-
ear score works better on RACE dataset (66.8% vs. 66.5%).
Figure 2 shows the results of passage sentence selection
(Pss) on COIN and RACE dev set with different numbers of
selected sentences (Top K). The results without Pss module
are also shown in the ﬁgure (RACE-w and COIN-w). We
observe that Pss mechanism consistently shows a positive
impact on both datasets when more than four sentences are
selected compared to the model without Pss (RACE-w and
COIN-w). The highest performance is achieved when top
3 sentences are selected on COIN and top 5 sentences on
RACE where the main reason is that the questions in RACE
are designed by human experts and require more complex
reasoning.

Why Previous Methods Break Down?
As shown in Table 7, applying previous models to a strong
BERT encoder fails to give performance increase over di-

Figure 2: Results of sentence selection on dev sets of
RACE and COIN when selecting different numbers of sen-
tences (Top K). We use BERTbase as encoder and cosine
score method here. RACE/COIN-w indicates the results on
RACE/COIN without passage sentence selection module.

rectly ﬁne-tuned BERT. The contrast is clear that our pro-
posed model achieves more than 3.8% absolute increase
over the BERT baseline. We summarize the reasons result-
ing in such contrast as follows: (i) the unidirectional repre-
sentations cannot well capture the relationship between two
sequences, (ii) previous methods use elementwise subtrac-
tion and multiplication to fuse Eq and Ho in Eq. 7 (i.e.,
[Eq (cid:9)Ho; Eq ⊗Ho]) which is shown suboptimal as such pro-
cessing breaks the symmetry of equation. Symmetric repre-
sentations from both directions show essentially helpful for
bidirectional architecture.

Evaluation on Different Types of Questions
Inspired by (Sun et al. 2019), we further analyze the perfor-
mance of the main components on different question types.
Questions are roughly divided into ﬁve categories: detail,
inference, main, attitude and vocabulary (Lai et al. 2017;
Qian and Schedl 2004). We annotate all the instances of the
RACE development set. As shown in Figure 3, all the com-
binations of components work better than the BERT base-
line in most question types. Bidirectional matching strategy
(DCMN) consistently improves the results across all cate-
gories. DCMN+PSS works best on the inference and attitude
categories which indicates PSS module may effectively im-
prove the reasoning ability of the model. DCMN+AOI works
better than DCMN on detail and main categories which indi-
cates that the model achieves better distinguish ability with
answer option interaction information.

Related Work
Neural network based methods have been applied to sev-
eral natural language processing tasks, especially to MRC
(Zhang et al. 2019c; Zhang, Huang, and Zhao 2018).

The task of selecting sentences to answer the question
has been studied across several question-answering (QA)
datasets, by modeling the relevance between a sentence
and the question (Min et al. 2018b; Wang et al. 2019;
Choi et al. 2017; Raiman and Miller 2017; Wang et al.

Figure 3: Results on different question types, tested on the
RACE dev set. BERTlarge is used as encoder here. OI: An-
swer Option Interaction. SS: Passage Sentence Selection.

2018a). (Wang et al. 2019) apply distant supervision to gen-
erate imperfect labels and then use them to train a neural
evidence extractor. (Min et al. 2018b) propose a simple sen-
tence selector to select the minimal set of sentences then
feed into the QA model. They are different from our work
in that (i) we select the sentences by modeling the rele-
vance among sentence-question-option triplet, not sentence-
question pair. (ii) Our model uses the output of language
model as the sentence embedding and computes the rele-
vance score using these sentence vectors directly, without
the need of manually deﬁned labels. (iii) We achieve a gen-
erally positive impact by selecting sentences while previous
sentence selection methods usually bring performance de-
crease in most cases.

Most recent works attempting to integrate answer option
interaction information focus on building attention mech-
anism at word-level (Ran et al. 2019; Zhu et al. 2018;
Pujari and Goldwasser 2019) whose performance increase
is very limited. Our answer option interaction module is dif-
ferent from previous works in that: (i) we encode the com-
parison information by modeling the bilinear representation
among the options at sentence-level which is similar to mod-
eling passage-question sequence relationship, other than at-
tention mechanism. (ii) We use gated mechanism to fuse the
comparison information into the original answer option rep-
resentations.

Conclusion

This paper proposes dual co-matching network integrated
with two reading strategies (passage sentence selection and
answer option interaction) to enhance multi-choice machine
reading comprehension. In terms of strong pre-trained lan-
guage models such as BERT and XLNet as encoder, our pro-
posed method achieves state-of-the-art results on ﬁve repre-
sentative multi-choice MRC datasets including RACE. The
experiment results consistently indicate the general effec-
tiveness and applicability of our model.

References
Chen, Z.; Cui, Y.; Ma, W.; and Wang, S. 2019. Convolu-
tional Spatial Attention Model for Reading Comprehension
with Multiple-Choice Questions. In AAAI 2019.
Choi, E.; Hewlett, D.; Uszkoreit, J.; Polosukhin, I.; Lacoste,
A.; and Berant, J. 2017. Coarse-to-Fine Question Answering
for Long Documents. In ACL 2017, 209–220.
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.
BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding. In NAACL 2019, 4171–4186.
Hermann, K. M.; Kocisky, T.; Grefenstette, E.; Espeholt, L.;
Kay, W.; Suleyman, M.; and Blunsom, P. 2015. Teaching
Machines to Read and Comprehend. In NIPS 2015.
Lai, G.; Xie, Q.; Liu, H.; Yang, Y.; and Hovy, E. 2017.
RACE: Large-scale ReAding Comprehension Dataset From
Examinations. In EMNLP 2017, 785–794.
Li, Z.; Ding, X.; and Liu, T. 2019. Story Ending Prediction
by Transferable BERT. In IJCAI-19, 1800–1806.
Luo, Y.; Xiao, F.; and Zhao, H. 2019. Hierarchical Contex-
tualized Representation for Named Entity Recognition. In
AAAI 2020.
Min, S.; Zhong, V.; Socher, R.; and Xiong, C. 2018a. Ef-
ﬁcient and Robust Question Answering from Minimal Con-
text over Documents. In ACL 2018, 1725–1735.
Min, S.; Zhong, V.; Socher, R.; and Xiong, C. 2018b. Ef-
ﬁcient and Robust Question Answering from Minimal Con-
text over Documents. In ACL 2018, 1725–1735.
Mostafazadeh, N.; Chambers, N.; He, X.; Parikh, D.; Ba-
tra, D.; Vanderwende, L.; Kohli, P.; and Allen, J. 2016. A
Corpus and Cloze Evaluation for Deeper Understanding of
Commonsense Stories. In NAACL 2016, 839–849.
Nguyen, T.; Rosenberg, M.; Song, X.; Gao, J.; Tiwary, S.;
Majumder, R.; and Deng, L. 2016. MS MARCO: A Hu-
man Generated MAchine Reading COmprehension Dataset.
CoRR abs/1611.09268.
Ostermann, S.; Roth, M.; Modi, A.; Thater, S.; and Pinkal,
M. 2018. SemEval-2018 Task 11: Machine Comprehension
In Proceedings of The
Using Commonsense Knowledge.
12th International Workshop on Semantic Evaluation.
Pujari, R., and Goldwasser, D.
2019. Using Natural
Language Relations between Answer Choices for Machine
Comprehension. In NAACL-HLT 2019, 4010–4015.
Qian, D., and Schedl, M. 2004. Evaluation of an In-Depth
Vocabulary Knowledge Measure for Assessing Reading Per-
formance. Language Testing - LANG TEST 21:28–52.
Radford, A. 2018. Improving Language Understanding by
Generative Pre-Training. In OpenAI preprint.
Raiman, J., and Miller, J.
Reader. In EMNLP 2017, 1059–1069.
Rajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.
SQuAD: 100,000+ Questions for Machine Comprehension
of Text. In EMNLP 2016, 2383–2392.
Ran, Q.; Li, P.; Hu, W.; and Zhou, J. 2019. Option Compar-
ison Network for Multiple-choice Reading Comprehension.
CoRR abs/1903.03033.

2017. Globally Normalized

Richardson, M.; Burges, C. J.; and Renshaw, E.
2013.
MCTest: A Challenge Dataset for the Open-Domain Ma-
chine Comprehension of Text. In EMNLP 2013, 193–203.
Srivastava, R. K.; Greff, K.; and Schmidhuber, J. 2015.
Highway networks. In ICML 2015.
Sun, K.; Yu, D.; Yu, D.; and Cardie, C. 2019.
Improv-
ing Machine Reading Comprehension with General Reading
Strategies. In NAACL 2019.
Tang, M.; Cai, J.; and Zhuo, H. H. 2019. Multi-Matching
Network for Multiple Choice Reading Comprehension. In
AAAI 2019.
Tay, Y.; Tuan, L. A.; and Hui, S. C. 2018. Multi-range Rea-
soning for Machine Comprehension. CoRR abs/1803.09074.
Trischler, A.; Wang, T.; Yuan, X.; Harris, J.; Sordoni, A.;
Bachman, P.; and Suleman, K. 2017. NewsQA: A machine
comprehension dataset. In Proceedings of the 2nd Workshop
on Representation Learning for NLP, 191–200.
Wang, L.; Sun, M.; Zhao, W.; Shen, K.; and Liu, J. 2018a.
Yuanfudao at SemEval-2018 Task 11: Three-way Atten-
tion and Relational Knowledge for Commonsense Machine
Comprehension. In Proceedings of The 12th International
Workshop on Semantic Evaluation, 758–762.
Wang, S.; Yu, M.; Jiang, J.; and Chang, S. 2018b. A Co-
Matching Model for Multi-choice Reading Comprehension.
In ACL 2018, 746–751.
Wang, H.; Yu, D.; Sun, K.; Chen, J.; Yu, D.; Roth, D.; and
McAllester, D. A. 2019. Evidence Sentence Extraction for
Machine Reading Comprehension. CoRR abs/1902.08852.
Xiao, F.; Li, J.; Zhao, H.; Wang, R.; and Chen, K. 2019.
Lattice-Based Transformer Encoder for Neural Machine
Translation. In ACL 2019, 3090–3097.
Yang, Z.; Dai, Z.; Yang, Y.; Carbonell, J. G.; Salakhutdinov,
R.; and Le, Q. V. 2019. XLNet: Generalized Autoregressive
Pretraining for Language Understanding. In NIPS 2019.
Zhang, Z.; Huang, Y.; Zhu, P.; and Zhao, H. 2018. Effective
Character-augmented Word Embedding for Machine Read-
ing Comprehension. In NLPCC 2018.
Zhang, Z.; Wu, Y.; Zhao, H.; Li, Z.; Zhang, S.; Zhou, X.;
and Zhou, X. 2019a. Semantics-aware BERT for Language
Understanding. In AAAI 2020.
Zhang, Z.; Wu, Y.; Zhou, J.; Duan, S.; Zhao, H.; and Wang,
R. 2019b. SG-Net: Syntax-Guided Machine Reading Com-
prehension. In AAAI 2020.
Zhang, Z.; Zhao, H.; Ling, K.; Li, J.; Li, Z.; and He, S.
2019c. Effective Subword Segmentation for Text Compre-
hension. In IEEE/ACM Transactions on Audio, Speech, and
Language Processing, 2019.
Zhang, Z.; Huang, Y.; and Zhao, H.
2018. Subword-
augmented Embedding for Cloze Reading Comprehension.
In COLING 2018, 1802–1814.
Zhou, J., and Zhao, H. Head-Driven Phrase Structure Gram-
mar Parsing on Penn Treebank. In ACL 2019, 2396–2408.
Zhu, H.; Wei, F.; Qin, B.; and Liu, T. 2018. Hierarchical At-
tention Flow for Multiple-choice Reading Comprehension.
In AAAI 2018.

