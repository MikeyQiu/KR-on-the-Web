7
1
0
2
 
r
a

M
 
8
 
 
]
L
C
.
s
c
[
 
 
3
v
0
4
1
8
0
.
6
0
6
1
:
v
i
X
r
a

STransE: a novel embedding model of entities and relationships
in knowledge bases∗

Dat Quoc Nguyen1, Kairit Sirts1, Lizhen Qu2 and Mark Johnson1

1 Department of Computing, Macquarie University, Sydney, Australia
dat.nguyen@students.mq.edu.au, {kairit.sirts, mark.johnson}@mq.edu.au
2 NICTA, ACT 2601, Australia
lizhen.qu@nicta.com.au

Abstract

Knowledge bases of real-world facts about
entities and their relationships are useful re-
sources for a variety of natural language pro-
cessing tasks. However, because knowledge
bases are typically incomplete, it is useful to
be able to perform link prediction or knowl-
edge base completion, i.e., predict whether
a relationship not in the knowledge base is
likely to be true. This paper combines insights
from several previous link prediction models
into a new embedding model STransE that
represents each entity as a low-dimensional
vector, and each relation by two matrices and
a translation vector. STransE is a simple com-
bination of the SE and TransE models, but it
obtains better link prediction performance on
two benchmark datasets than previous embed-
ding models. Thus, STransE can serve as a
new baseline for the more complex models in
the link prediction task.

1

Introduction

Knowledge bases (KBs), such as WordNet (Fell-
baum, 1998), YAGO (Suchanek et al., 2007), Free-
base (Bollacker et al., 2008) and DBpedia (Lehmann
et al., 2015), represent relationships between entities
as triples (head entity, relation, tail entity). Even
very large knowledge bases are still far from com-
plete (Socher et al., 2013; West et al., 2014). Link
prediction or knowledge base completion systems
(Nickel et al., 2016a) predict which triples not in
a knowledge base are likely to be true (Taskar et

∗ A revised version of our NAACL-HLT 2016 paper with

additional experimental results and latest related work.

al., 2004; Bordes et al., 2011). A variety of differ-
ent kinds of information is potentially useful here,
including information extracted from external cor-
pora (Riedel et al., 2013; Wang et al., 2014a) and
the other relationships that hold between the enti-
ties (Angeli and Manning, 2013; Zhao et al., 2015).
For example, Toutanova et al. (2015) used informa-
tion from the external ClueWeb-12 corpus to signif-
icantly enhance performance.

While integrating a wide variety of information
sources can produce excellent results (Das et al.,
2017), there are several reasons for studying sim-
pler models that directly optimize a score function
for the triples in a knowledge base, such as the
one presented here. First, additional information
sources might not be available, e.g., for knowledge
bases for specialized domains. Second, models that
don’t exploit external resources are simpler and thus
typically much faster to train than the more com-
plex models using additional information. Third,
the more complex models that exploit external in-
formation are typically extensions of these simpler
models, and are often initialized with parameters es-
timated by such simpler models, so improvements to
the simpler models should yield corresponding im-
provements to the more complex models as well.

Embedding models for KB completion associate
entities and/or relations with dense feature vectors
or matrices. Such models obtain state-of-the-art per-
formance (Nickel et al., 2011; Bordes et al., 2011;
Bordes et al., 2012; Bordes et al., 2013; Socher et
al., 2013; Wang et al., 2014b; Guu et al., 2015) and
generalize to large KBs (Krompaß et al., 2015). Ta-
ble 1 summarizes a number of prominent embedding

Model

SE

Unstructured

TransE

DISTMULT

NTN

TransH

TransD

TransR

TranSparse

Our STransE

Opt.

Score function fr(h, t)
(cid:107)Wr,1h − Wr,2t(cid:107)(cid:96)1/2 ; Wr,1, Wr,2 ∈ Rk×k
(cid:107)h − t(cid:107)(cid:96)1/2
(cid:107)h + r − t(cid:107)(cid:96)1/2 ; r ∈ Rk
h(cid:62)Wrt ; Wr is a diagonal matrix ∈ Rk×k
r tanh(h(cid:62)Mrt + Wr,1h + Wr,2t + br) ; ur, br ∈ Rd; Mr ∈ Rk×k×d; Wr,1, Wr,2 ∈ Rd×k
u(cid:62)
(cid:107)(I − rpr(cid:62)
(cid:107)(I + rph(cid:62)
(cid:107)Wrh + r − Wrt(cid:107)(cid:96)1/2 ; Wr ∈ Rd×k ; r ∈ Rd
r ∈ Rd×k; θh
(cid:107)Wh
r(θt
(cid:107)Wr,1h + r − Wr,2t(cid:107)(cid:96)1/2 ; Wr,1, Wr,2 ∈ Rk×k; r ∈ Rk

p )h + r − (I − rpr(cid:62)
p )h + r − (I + rpt(cid:62)

r ∈ R ; r ∈ Rd

r)t(cid:107)(cid:96)1/2 ; Wh

r )h + r − Wt

r , Wt

r (θh

r , θt

p )t(cid:107)(cid:96)1/2 ; rp, r ∈ Rk ; I: Identity matrix size k × k
p )t(cid:107)(cid:96)1/2 ; rp, r ∈ Rd ; hp, tp ∈ Rk ; I: Identity matrix size d × k AdaDelta

SGD

AdaGrad

L-BFGS

SGD

SGD

SGD

SGD

SGD

SGD

Table 1: The score functions fr(h, t) and the optimization methods (Opt.) of several prominent embedding models
for KB completion. In all of these the entities h and t are represented by vectors h and t ∈ Rk respectively.

models for KB completion.

Let (h, r, t) represent a triple. In all of the models
discussed here, the head entity h and the tail entity
t are represented by vectors h and t ∈ Rk respec-
tively. The Unstructured model (Bordes et al., 2012)
assumes that h ≈ t. As the Unstructured model
does not take the relationship r into account, it can-
not distinguish different relation types. The Struc-
tured Embedding (SE) model (Bordes et al., 2011)
extends the unstructured model by assuming that h
and t are similar only in a relation-dependent sub-
space. It represents each relation r with two matri-
ces Wr,1 and Wr,2 ∈ Rk×k, which are chosen so
that Wr,1h ≈ Wr,2t. The TransE model (Bordes et
al., 2013) is inspired by models such as Word2Vec
(Mikolov et al., 2013) where relationships between
words often correspond to translations in latent fea-
ture space. The TransE model represents each rela-
tion r by a translation vector r ∈ Rk, which is cho-
sen so that h + r ≈ t.

The primary contribution of this paper is that
two very simple relation-prediction models, SE and
TransE, can be combined into a single model, which
we call STransE.1 Speciﬁcally, we use relation-
speciﬁc matrices Wr,1 and Wr,2 as in the SE model
to identify the relation-dependent aspects of both h
and t, and use a vector r as in the TransE model
to describe the relationship between h and t in this
subspace. Speciﬁcally, our new KB completion
model STransE chooses Wr,1, Wr,2 and r so that

Wr,1h + r ≈ Wr,2t. That is, a TransE-style rela-
tionship holds in some relation-dependent subspace,
and crucially, this subspace may involve very dif-
ferent projections of the head h and tail t. So Wr,1
and Wr,2 can highlight, suppress, or even change the
sign of, relation-speciﬁc attributes of h and t. For
example, for the “purchases” relationship, certain
attributes of individuals h (e.g., age, gender, mari-
tal status) are presumably strongly correlated with
very different attributes of objects t (e.g., sports car,
washing machine and the like).

As we show below, STransE performs better than
the SE and TransE models and other state-of-the-art
link prediction models on two standard link predic-
tion datasets WN18 and FB15k, so it can serve as
a new baseline for KB completion. We expect that
the STransE will also be able to serve as the basis
for extended models that exploit a wider variety of
information sources, just as TransE does.

2 Our approach

Let E denote the set of entities and R the set of re-
lation types. For each triple (h, r, t), where h, t ∈ E
and r ∈ R, the STransE model deﬁnes a score func-
tion fr(h, t) of its implausibility. Our goal is to
choose f such that the score fr(h, t) of a plausi-
ble triple (h, r, t) is smaller than the score fr(cid:48)(h(cid:48), t(cid:48))
of an implausible triple (h(cid:48), r(cid:48), t(cid:48)). We deﬁne the
STransE score function f as follows:

fr(h, t) = (cid:107)Wr,1h + r − Wr,2t(cid:107)(cid:96)1/2

1Source code: https://github.com/datquocnguyen/STransE

using either the (cid:96)1 or the (cid:96)2-norm (the choice is made

using validation data; in our experiments we found
that the (cid:96)1 norm gave slightly better results). To
learn the vectors and matrices we minimize the fol-
lowing margin-based objective function:

L =

(cid:88)

[γ + fr(h, t) − fr(h(cid:48), t(cid:48))]+

(h,r,t)∈G

(h(cid:48),r,t(cid:48))∈G(cid:48)

(h,r,t)

where [x]+ = max(0, x), γ is the margin hyper-
parameter, G is the training set consisting of correct
(h,r,t) = {(h(cid:48), r, t) | h(cid:48) ∈ E, (h(cid:48), r, t) /∈
triples, and G(cid:48)
G} ∪ {(h, r, t(cid:48)) | t(cid:48) ∈ E, (h, r, t(cid:48)) /∈ G} is the set
of incorrect triples generated by corrupting a correct
triple (h, r, t) ∈ G.

We use Stochastic Gradient Descent (SGD) to
minimize L, and impose the following constraints
during training: (cid:107)h(cid:107)2 (cid:54) 1, (cid:107)r(cid:107)2 (cid:54) 1, (cid:107)t(cid:107)2 (cid:54) 1,
(cid:107)Wr,1h(cid:107)2 (cid:54) 1 and (cid:107)Wr,2t(cid:107)2 (cid:54) 1.

3 Related work

Table 1 summarizes related embedding models for
link prediction and KB completion. The models
differ in the score functions fr(h, t) and the algo-
rithms used to optimize the margin-based objective
function, e.g., SGD, AdaGrad (Duchi et al., 2011),
AdaDelta (Zeiler, 2012) and L-BFGS (Liu and No-
cedal, 1989).

DISTMULT (Yang et al., 2015) is based on a
Bilinear model (Nickel et al., 2011; Bordes et al.,
2012; Jenatton et al., 2012) where each relation is
represented by a diagonal rather than a full matrix.
The neural tensor network (NTN) model (Socher et
al., 2013) uses a bilinear tensor operator to represent
each relation while ProjE (Shi and Weninger, 2017)
could be viewed as a simpliﬁed version of NTN
with diagonal matrices. Similar quadratic forms
are used to model entities and relations in KG2E
(He et al., 2015), ComplEx (Trouillon et al., 2016),
TATEC (Garc´ıa-Dur´an et al., 2016) and RSTE (Tay
et al., 2017).
In addition, HolE (Nickel et al.,
2016b) uses circular correlation—a compositional
operator—which could be interpreted as a compres-
sion of the tensor product.

The TransH model (Wang et al., 2014b) asso-
ciates each relation with a relation-speciﬁc hyper-
plane and uses a projection vector to project en-
tity vectors onto that hyperplane. TransD (Ji et al.,

2015) and TransR/CTransR (Lin et al., 2015b) ex-
tend the TransH model using two projection vec-
tors and a matrix to project entity vectors into a
relation-speciﬁc space, respectively. TransD learns
a relation-role speciﬁc mapping just as STransE, but
represents this mapping by projection vectors rather
than full matrices, as in STransE. The lppTransD
model (Yoon et al., 2016) extends TransD to ad-
ditionally use two projection vectors for represent-
ing each relation. In fact, our STransE model and
TranSparse (Ji et al., 2016) can be viewed as direct
extensions of the TransR model, where head and tail
entities are associated with their own projection ma-
trices, rather than using the same matrix for both, as
in TransR and CTransR.

Recently, several authors have shown that relation
paths between entities in KBs provide richer infor-
mation and improve the relationship prediction (Lin
et al., 2015a; Garc´ıa-Dur´an et al., 2015; Guu et al.,
2015; Wang et al., 2016; Feng et al., 2016; Liu et al.,
2016; Niepert, 2016; Wei et al., 2016; Toutanova et
al., 2016; Nguyen et al., 2016). In addition, Nickel
et al. (2016a) reviews other approaches for learning
from KBs and multi-relational data.

4 Experiments

For link prediction evaluation, we conduct experi-
ments and compare the performance of our STransE
model with published results on the benchmark
WN18 and FB15k datasets (Bordes et al., 2013). In-
formation about these datasets is given in Table 2.

#R
18
1,345

#E
40,943
14,951

Dataset
WN18
FB15k

#Train
141,442
483,142

#Valid #Test
5,000
5,000
59,071
50,000
Table 2: Statistics of the experimental datasets used in
this study (and previous works). #E is the number of
entities, #R is the number of relation types, and #Train,
#Valid and #Test are the numbers of triples in the training,
validation and test sets, respectively.

4.1 Task and evaluation protocol

The link prediction task (Bordes et al., 2011; Bordes
et al., 2012; Bordes et al., 2013) predicts the head or
tail entity given the relation type and the other entity,
i.e. predicting h given (?, r, t) or predicting t given
(h, r, ?) where ? denotes the missing element. The

Raw

Filtered

WN18

FB15k

Method

SE (Bordes et al., 2011)
Unstructured (Bordes et al., 2012)
TransE (Bordes et al., 2013)
TransH (Wang et al., 2014b)
TransR (Lin et al., 2015b)
CTransR (Lin et al., 2015b)
KG2E (He et al., 2015)
TransD (Ji et al., 2015)
lppTransD (Yoon et al., 2016)
TranSparse (Ji et al., 2016)
TATEC (Garc´ıa-Dur´an et al., 2016)
NTN (Socher et al., 2013)
DISTMULT (Yang et al., 2015)
HolE (Nickel et al., 2016b)
Our STransE
RTransE (Garc´ıa-Dur´an et al., 2015)
PTransE (Lin et al., 2015a)
GAKE (Feng et al., 2016)
Gaifman (Niepert, 2016)
Hiri (Liu et al., 2016)
NLFeat (Toutanova and Chen, 2015)
TEKE H (Wang and Li, 2016)
SSP (Xiao et al., 2017)

WN18
H10 MRR MR
273
68.5
1074
35.3
243
75.4
212
73.0
198
79.8
199
79.4
174
80.2
194
79.6
195
80.5
187
80.1
-
-
-
-
-
-
-
-
80.9
219
-
-
207
-
228
-
-
-
-
-
-
-
212
80.3
163
81.2
Table 3: Link prediction results. MR, H10 and MRR denote evaluation metrics of mean rank, Hits@10 (in %) and
mean reciprocal rank, respectively. “NLFeat” abbreviates Node+LinkFeat. The results for NTN (Socher et al., 2013)
listed in this table are taken from Yang et al. (2015) since NTN was originally evaluated on different datasets.

FB15k
H10 MRR MR H10 MRR MR H10 MRR
-
28.8
-
4.5
-
34.9
-
45.7
-
48.2
-
48.4
-
48.9
-
53.4
-
53.0
53.5
-
-
-
0.53
-
0.83
-
0.938
-
0.657
51.6
-
-
-
51.4
-
44.5
-
-
0.691
-
0.940
-
-
51.2
57.2
-

-
-
-
-
-
-
-
-
-
-
-
-
-
0.232
0.252
-
-
-
-
-
-
-
-

-
-
-
-
-
-
-
-
-
-
-
0.25
0.35
0.524
0.543
-
-
-
-
0.603
0.822
-
-

-
-
-
-
-
-
-
-
-
-
-
-
-
0.616
0.469
-
-
-
-
-
-
-
-

MR
1011
315
263
401
238
231
342
224
283
223
-
-
-
-
217
-
-
-
-
-
-
127
168

39.8
6.3
47.1
64.4
68.7
70.2
74.0
77.3
78.7
79.5
76.7
41.4
57.7
73.9
79.7
76.2
84.6
64.8
84.2
70.3
87.0
73.0
79.0

80.5
38.2
89.2
86.7
92.0
92.3
92.8
92.2
94.3
93.2
-
66.1
94.2
94.9
93.4
-
-
-
93.9
90.8
94.3
92.9
93.2

162
979
125
87
77
75
59
91
78
82
58
-
-
-
69
50
58
119
75
-
-
108
82

985
304
251
303
225
218
331
212
270
211
-
-
-
-
206
-
-
-
352
-
-
114
156

results are evaluated using the ranking induced by
the score function fr(h, t) on test triples.

For each test triple (h, r, t), we corrupted it by re-
placing either h or t by each of the possible entities
in turn, and then rank these candidates in ascend-
ing order of their implausibility value computed by
the score function. This is called as the “Raw” set-
ting protocol. For the “Filtered” setting protocol de-
scribed in Bordes et al. (2013), we removed any cor-
rupted triples that appear in the knowledge base, to
avoid cases where a correct corrupted triple might
be ranked higher than the test triple. The “Filtered”
setting thus provides a clearer view on the ranking
performance. Following Bordes et al. (2013), we re-
port the mean rank and the Hits@10 (i.e., the pro-
portion of test triples in which the target entity was
ranked in the top 10 predictions) for each model. In
addition, we report the mean reciprocal rank, which
is commonly used in information retrieval. In both
“Raw” and “Filtered” settings, lower mean rank,
higher mean reciprocal rank or higher Hits@10 in-
dicates better link prediction performance.

Following TransR (Lin et al., 2015b), TransD (Ji
et al., 2015), RTransE (Garc´ıa-Dur´an et al., 2015),
PTransE (Lin et al., 2015a), TATEC (Garc´ıa-Dur´an
et al., 2016) and TranSparse (Ji et al., 2016), we used
the entity and relation vectors produced by TransE
(Bordes et al., 2013) to initialize the entity and re-
lation vectors in STransE, and we initialized the re-
lation matrices with identity matrices. We applied
the “Bernoulli” trick used also in previous work for
generating head or tail entities when sampling incor-
rect triples (Wang et al., 2014b; Lin et al., 2015b; He
et al., 2015; Ji et al., 2015; Lin et al., 2015a; Yoon
et al., 2016; Ji et al., 2016). We ran SGD for 2,000
epochs to estimate the model parameters. Following
Bordes et al. (2013) we used a grid search on vali-
dation set to choose either the l1 or l2 norm in the
score function f , as well as to set the SGD learning
rate λ ∈ {0.0001, 0.0005, 0.001, 0.005, 0.01}, the
margin hyper-parameter γ ∈ {1, 3, 5} and the vector
size k ∈ {50, 100}. The lowest ﬁltered mean rank
on the validation set was obtained when using the
l1 norm in f on both WN18 and FB15k, and when

λ = 0.0005, γ = 5, and k = 50 for WN18, and
λ = 0.0001, γ = 1, and k = 100 for FB15k.

4.2 Main results

Table 3 compares the link prediction results of our
STransE model with results reported in prior work,
using the same experimental setup. The ﬁrst 15 rows
report the performance of the models that do not
exploit information about alternative paths between
head and tail entities. The next 5 rows report results
of the models that exploit information about relation
paths. The last 3 rows present results for the models
which make use of textual mentions derived from a
large external corpus.

It is clear that the models with the additional ex-
ternal corpus information obtained best results. In
future work we plan to extend the STransE model
to incorporate such additional information. Table 3
also shows that the models employing path infor-
mation generally achieve better results than mod-
els that do not use such information.
In terms of
models not exploiting path information or exter-
nal information, the STransE model produces the
highest ﬁltered mean rank on WN18 and the high-
est ﬁltered Hits@10 and mean reciprocal rank on
FB15k. Compared to the closely related models SE,
TransE, TransR, CTransR, TransD and TranSparse,
our STransE model does better than these models on
both WN18 and FB15k.

Following Bordes et al. (2013), Table 4 analyzes
Hits@10 results on FB15k with respect to the re-
lation categories deﬁned as follows: for each rela-
tion type r, we computed the averaged number ah of
heads h for a pair (r, t) and the averaged number at
of tails t for a pair (h, r). If ah < 1.5 and at < 1.5,
then r is labeled 1-1. If ah ≥ 1.5 and at < 1.5, then
r is labeled M-1. If ah < 1.5 and at ≥ 1.5, then r is
labeled as 1-M. If ah ≥ 1.5 and at ≥ 1.5, then r is
labeled as M-M. 1.4%, 8.9%, 14.6% and 75.1% of
the test triples belong to a relation type classiﬁed as
1-1, 1-M, M-1 and M-M, respectively.

Table 4 shows that in comparison to prior mod-
els not using path information, STransE obtains the
second highest Hits@10 result for M-M relation cat-
egory at (80.1% + 83.1%)/2 = 81.6% which is
0.5% smaller than the Hits@10 result of TranSparse
for M-M. However, STransE obtains 2.5% higher
Hits@10 result than TranSparse for M-1. In addi-

Predicting head h

Predicting tail t

Method

1.9

6.6

6.1

34.3 4.2

1-1 1-M M-1 M-M 1-1 1-M M-1 M-M
35.6 62.6 17.2 37.5 34.9 14.6 68.3 41.3
SE
34.5 2.5
6.6
Unstr.
43.7 65.7 18.2 47.2 43.7 19.7 66.7 50.0
TransE
TransH
66.8 87.6 28.7 64.5 65.5 39.8 83.3 67.2
78.8 89.2 34.1 69.2 79.2 37.4 90.4 72.1
TransR
81.5 89.0 34.7 71.2 80.8 38.6 90.1 73.8
CTransR
92.3 94.6 66.0 69.6 92.6 67.9 94.4 73.4
KG2E
TATEC
79.3 93.2 42.3 77.2 78.5 51.5 92.7 80.7
86.1 95.5 39.8 78.5 85.4 50.6 94.4 81.2
TransD
lppTransD 86.0 94.2 54.4 82.2 79.7 43.2 95.3 79.7
TranSparse 86.8 95.5 44.3 80.9 86.6 56.6 94.4 83.3
82.8 94.2 50.4 80.1 82.4 56.9 93.4 83.1
STransE

Table 4: Hits@10 (in %) by the relation category on
FB15k. “Unstr.” abbreviates Unstructured.

tion, STransE also performs better than TransD for
1-M and M-1 relation categories. We believe the
improved performance of the STransE model is due
to its use of full matrices, rather than just projection
vectors as in TransD. This permits STransE to model
diverse and complex relation categories (such as 1-
M, M-1 and especially M-M) better than TransD
and other similiar models. However, STransE is not
as good as TransD for the 1-1 relations. Perhaps the
extra parameters in STransE hurt performance in this
case (note that 1-1 relations are relatively rare, so
STransE does better overall).

5 Conclusion and future work

This paper presented a new embedding model for
link prediction and KB completion. Our STransE
combines insights from several simpler embed-
ding models, speciﬁcally the Structured Embedding
model (Bordes et al., 2011) and the TransE model
(Bordes et al., 2013), by using a low-dimensional
vector and two projection matrices to represent each
relation. STransE, while being conceptually sim-
ple, produces highly competitive results on standard
link prediction evaluations, and scores better than
the embedding-based models it builds on. Thus it
is a suitable candidate for serving as future baseline
for more complex models in the link prediction task.
In future work we plan to extend STransE to ex-
ploit relation path information in knowledge bases,
in a manner similar to Lin et al. (2015a), Guu et al.
(2015) or Nguyen et al. (2016).

Acknowledgments

This research was supported by a Google award
through the Natural Language Understanding Fo-
the Australian Re-
cused Program, and under
search Council’s Discovery Projects funding scheme
(project number DP160102156).

NICTA is funded by the Australian Government
through the Department of Communications and the
Australian Research Council through the ICT Centre
of Excellence Program. The ﬁrst author is supported
by an International Postgraduate Research Scholar-
ship and a NICTA NRPA Top-Up Scholarship.

References

[Angeli and Manning2013] Gabor Angeli and Christo-
pher Manning. 2013. Philosophers are Mortal: In-
ferring the Truth of Unseen Facts. In Proceedings of
the Seventeenth Conference on Computational Natural
Language Learning, pages 133–142.

[Bollacker et al.2008] Kurt Bollacker, Colin Evans,
Praveen Paritosh, Tim Sturge, and Jamie Taylor.
2008. Freebase: A Collaboratively Created Graph
Database for Structuring Human Knowledge.
In
Proceedings of
the 2008 ACM SIGMOD Interna-
tional Conference on Management of Data, pages
1247–1250.

[Bordes et al.2011] Antoine Bordes, Jason Weston, Ro-
nan Collobert, and Yoshua Bengio. 2011. Learning
Structured Embeddings of Knowledge Bases. In Pro-
ceedings of the Twenty-Fifth AAAI Conference on Ar-
tiﬁcial Intelligence, pages 301–306.

[Bordes et al.2012] Antoine Bordes, Xavier Glorot, Ja-
son Weston, and Yoshua Bengio. 2012. A Semantic
Matching Energy Function for Learning with Multi-
relational Data. Machine Learning, 94(2):233–259.
[Bordes et al.2013] Antoine Bordes, Nicolas Usunier,
Alberto Garcia-Duran, Jason Weston, and Oksana
Yakhnenko. 2013. Translating Embeddings for Mod-
eling Multi-relational Data. In Advances in Neural In-
formation Processing Systems 26, pages 2787–2795.
[Das et al.2017] Rajarshi Das, Arvind Neelakantan,
David Belanger, and Andrew McCallum.
2017.
Chains of reasoning over entities, relations, and text
In Proceedings of
using recurrent neural networks.
the 15th Conference of the European Chapter of the
Association for Computational Linguistics.

[Duchi et al.2011] John Duchi, Elad Hazan, and Yoram
Singer. 2011. Adaptive Subgradient Methods for On-
line Learning and Stochastic Optimization. The Jour-
nal of Machine Learning Research, 12:2121–2159.

[Fellbaum1998] Christiane D. Fellbaum. 1998. WordNet:

An Electronic Lexical Database. MIT Press.

[Feng et al.2016] Jun Feng, Minlie Huang, Yang Yang,
and xiaoyan zhu. 2016. GAKE: Graph Aware Knowl-
In Proceedings of COLING 2016,
edge Embedding.
the 26th International Conference on Computational
Linguistics: Technical Papers, pages 641–651.

[Garc´ıa-Dur´an et al.2015] Alberto Garc´ıa-Dur´an, An-
toine Bordes, and Nicolas Usunier. 2015. Composing
Relationships with Translations. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, pages 286–290.

[Garc´ıa-Dur´an et al.2016] Alberto Garc´ıa-Dur´an, An-
toine Bordes, Nicolas Usunier, and Yves Grandvalet.
2016.
Combining Two and Three-Way Embed-
ding Models for Link Prediction in Knowledge
Journal of Artiﬁcial Intelligence Research,
Bases.
55:715–742.

[Guu et al.2015] Kelvin Guu, John Miller, and Percy
Liang. 2015. Traversing Knowledge Graphs in Vec-
tor Space. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing,
pages 318–327.

[He et al.2015] Shizhu He, Kang Liu, Guoliang Ji, and
Jun Zhao. 2015. Learning to Represent Knowledge
Graphs with Gaussian Embedding. In Proceedings of
the 24th ACM International on Conference on Infor-
mation and Knowledge Management, pages 623–632.
Jenatton, Nicolas L.
Roux, Antoine Bordes, and Guillaume R Obozinski.
2012. A latent factor model for highly multi-relational
data. In Advances in Neural Information Processing
Systems 25, pages 3167–3175.

[Jenatton et al.2012] Rodolphe

[Ji et al.2015] Guoliang Ji, Shizhu He, Liheng Xu, Kang
Liu, and Jun Zhao. 2015. Knowledge Graph Embed-
ding via Dynamic Mapping Matrix. In Proceedings of
the 53rd Annual Meeting of the Association for Com-
putational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Volume
1: Long Papers), pages 687–696.

[Ji et al.2016] Guoliang Ji, Kang Liu, Shizhu He, and Jun
2016. Knowledge Graph Completion with
Zhao.
In Proceedings of
Adaptive Sparse Transfer Matrix.
the Thirtieth AAAI Conference on Artiﬁcial Intelli-
gence, pages 985–991.

[Krompaß et al.2015] Denis Krompaß, Stephan Baier,
and Volker Tresp. 2015. Type-Constrained Represen-
tation Learning in Knowledge Graphs. In Proceedings
of the 14th International Semantic Web Conference,
pages 640–655.

[Lehmann et al.2015] Jens Lehmann, Robert Isele, Max
Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N.
Mendes, Sebastian Hellmann, Mohamed Morsey,

Patrick van Kleef, S¨oren Auer, and Christian Bizer.
2015. DBpedia - A Large-scale, Multilingual Knowl-
edge Base Extracted from Wikipedia. Semantic Web,
6(2):167–195.

[Lin et al.2015a] Yankai Lin, Zhiyuan Liu, Huanbo Luan,
Maosong Sun, Siwei Rao, and Song Liu. 2015a. Mod-
eling Relation Paths for Representation Learning of
Knowledge Bases. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 705–714.

[Lin et al.2015b] Yankai Lin, Zhiyuan Liu, Maosong Sun,
Yang Liu, and Xuan Zhu. 2015b. Learning Entity
and Relation Embeddings for Knowledge Graph Com-
In Proceedings of the Twenty-Ninth AAAI
pletion.
Conference on Artiﬁcial Intelligence Learning, pages
2181–2187.

[Liu and Nocedal1989] D. C. Liu and J. Nocedal. 1989.
On the Limited Memory BFGS Method for Large
Scale Optimization. Mathematical Programming,
45(3):503–528.

[Liu et al.2016] Qiao Liu, Liuyi Jiang, Minghao Han, Yao
Liu, and Zhiguang Qin. 2016. Hierarchical Random
Walk Inference in Knowledge Graphs. In Proceedings
of the 39th International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 445–454.

[Mikolov et al.2013] Tomas Mikolov, Wen-tau Yih, and
Geoffrey Zweig.
2013. Linguistic Regularities in
Continuous Space Word Representations. In Proceed-
ings of the 2013 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 746–751.
[Nguyen et al.2016] Dat Quoc Nguyen, Kairit Sirts,
Lizhen Qu, and Mark Johnson. 2016. Neighborhood
Mixture Model for Knowledge Base Completion. In
Proceedings of The 20th SIGNLL Conference on Com-
putational Natural Language Learning, pages 40–50.
[Nickel et al.2011] Maximilian Nickel, Volker Tresp, and
Hans-Peter Kriegel. 2011. A Three-Way Model for
Collective Learning on Multi-Relational Data. In Pro-
ceedings of the 28th International Conference on Ma-
chine Learning, pages 809–816.

[Nickel et al.2016a] Maximilian Nickel, Kevin Murphy,
Volker Tresp, and Evgeniy Gabrilovich. 2016a. A Re-
view of Relational Machine Learning for Knowledge
Graphs. Proceedings of the IEEE, 104(1):11–33.

[Nickel et al.2016b] Maximilian

Lorenzo
Holo-
In
the Thirtieth AAAI Conference on

Rosasco, and Tomaso Poggio.
graphic embeddings of knowledge graphs.
Proceedings of
Artiﬁcial Intelligence, pages 1955–1961.

Nickel,

2016b.

[Riedel et al.2013] Sebastian Riedel, Limin Yao, Andrew
McCallum, and Benjamin M. Marlin. 2013. Rela-
tion Extraction with Matrix Factorization and Univer-
sal Schemas. In Proceedings of the 2013 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 74–84.

[Shi and Weninger2017] Baoxu Shi and Tim Weninger.
2017. ProjE: Embedding Projection for Knowledge
Graph Completion. In Proceedings of the 31st AAAI
Conference on Artiﬁcial Intelligence.

[Socher et al.2013] Richard Socher, Danqi Chen, Christo-
pher D Manning, and Andrew Ng. 2013. Reason-
ing With Neural Tensor Networks for Knowledge Base
Completion. In Advances in Neural Information Pro-
cessing Systems 26, pages 926–934.

[Suchanek et al.2007] Fabian M. Suchanek, Gjergji Kas-
neci, and Gerhard Weikum. 2007. YAGO: A Core
of Semantic Knowledge. In Proceedings of the 16th
International Conference on World Wide Web, pages
697–706.

[Taskar et al.2004] Ben Taskar, Ming fai Wong, Pieter
Abbeel, and Daphne Koller. 2004. Link Prediction in
In Advances in Neural Information
Relational Data.
Processing Systems 16, pages 659–666.

[Tay et al.2017] Yi Tay, Anh Tuan Luu, Siu Cheung Hui,
and Falk Brauer. 2017. Random Semantic Tensor
Ensemble for Scalable Knowledge Graph Link Predic-
tion. In Proceedings of the Tenth ACM International
Conference on Web Search and Data Mining, pages
751–760.

[Toutanova and Chen2015] Kristina Toutanova and Danqi
Chen. 2015. Observed Versus Latent Features for
Knowledge Base and Text Inference. In Proceedings
of the 3rd Workshop on Continuous Vector Space Mod-
els and their Compositionality, pages 57–66.

[Toutanova et al.2015] Kristina Toutanova, Danqi Chen,
Patrick Pantel, Hoifung Poon, Pallavi Choudhury, and
Michael Gamon. 2015. Representing Text for Joint
In Pro-
Embedding of Text and Knowledge Bases.
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1499–
1509.

[Toutanova et al.2016] Kristina Toutanova, Victoria Lin,
Wen-tau Yih, Hoifung Poon, and Chris Quirk. 2016.
Compositional Learning of Embeddings for Relation
Paths in Knowledge Base and Text. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers), pages
1434–1444.

[Niepert2016] Mathias Niepert.

2016. Discriminative
Gaifman Models. In Advances in Neural Information
Processing Systems 29, pages 3405–3413.

[Trouillon et al.2016] Th´eo Trouillon, Johannes Welbl,
´Eric Gaussier, and Guillaume
Sebastian Riedel,
Bouchard. 2016. Complex Embeddings for Simple

Link Prediction. In Proceedings of the 33nd Interna-
tional Conference on Machine Learning, pages 2071–
2080.

[Zeiler2012] Matthew D. Zeiler. 2012. ADADELTA:
CoRR,

An Adaptive Learning Rate Method.
abs/1212.5701.

[Zhao et al.2015] Yu Zhao, Sheng Gao, Patrick Gallinari,
and Jun Guo. 2015. Knowledge Base Completion
by Learning Pairwise-Interaction Differentiated Em-
beddings. Data Mining and Knowledge Discovery,
29(5):1486–1504.

[Wang and Li2016] Zhigang Wang and Juan-Zi Li. 2016.
Text-Enhanced Representation Learning for Knowl-
In Proceedings of the Twenty-Fifth In-
edge Graph.
ternational Joint Conference on Artiﬁcial Intelligence,
pages 1293–1299.

[Wang et al.2014a] Zhen Wang, Jianwen Zhang, Jianlin
Feng, and Zheng Chen. 2014a. Knowledge Graph
In Proceedings of the
and Text Jointly Embedding.
2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1591–1601.
[Wang et al.2014b] Zhen Wang, Jianwen Zhang, Jianlin
Feng, and Zheng Chen. 2014b. Knowledge Graph
In Pro-
Embedding by Translating on Hyperplanes.
ceedings of the Twenty-Eighth AAAI Conference on
Artiﬁcial Intelligence, pages 1112–1119.

[Wang et al.2016] Quan Wang, Jing Liu, Yuanfei Luo,
Bin Wang, and Chin-Yew Lin. 2016. Knowledge Base
In Proceed-
Completion via Coupled Path Ranking.
ings of the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 1308–1318.

[Wei et al.2016] Zhuoyu Wei, Jun Zhao, and Kang Liu.
2016. Mining Inference Formulas by Goal-Directed
Random Walks. In Proceedings of the 2016 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1379–1388.

[West et al.2014] Robert West, Evgeniy Gabrilovich,
Kevin Murphy, Shaohua Sun, Rahul Gupta, and
Dekang Lin. 2014. Knowledge Base Completion via
Search-based Question Answering. In Proceedings of
the 23rd International Conference on World Wide Web,
pages 515–526.

[Xiao et al.2017] Han Xiao, Minlie Huang, and Xiaoyan
Zhu. 2017. SSP: semantic space projection for knowl-
edge graph embedding with text descriptions. In Pro-
ceedings of the 31st AAAI Conference on Artiﬁcial In-
telligence.

[Yang et al.2015] Bishan Yang, Wen-tau Yih, Xiaodong
He, Jianfeng Gao, and Li Deng. 2015. Embedding
Entities and Relations for Learning and Inference in
Knowledge Bases. In Proceedings of the International
Conference on Learning Representations.

[Yoon et al.2016] Hee-Geun Yoon, Hyun-Je Song, Seong-
Bae Park, and Se-Young Park. 2016. A Translation-
Based Knowledge Graph Embedding Preserving Log-
ical Property of Relations. In Proceedings of the 2016
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 907–916.

7
1
0
2
 
r
a

M
 
8
 
 
]
L
C
.
s
c
[
 
 
3
v
0
4
1
8
0
.
6
0
6
1
:
v
i
X
r
a

STransE: a novel embedding model of entities and relationships
in knowledge bases∗

Dat Quoc Nguyen1, Kairit Sirts1, Lizhen Qu2 and Mark Johnson1

1 Department of Computing, Macquarie University, Sydney, Australia
dat.nguyen@students.mq.edu.au, {kairit.sirts, mark.johnson}@mq.edu.au
2 NICTA, ACT 2601, Australia
lizhen.qu@nicta.com.au

Abstract

Knowledge bases of real-world facts about
entities and their relationships are useful re-
sources for a variety of natural language pro-
cessing tasks. However, because knowledge
bases are typically incomplete, it is useful to
be able to perform link prediction or knowl-
edge base completion, i.e., predict whether
a relationship not in the knowledge base is
likely to be true. This paper combines insights
from several previous link prediction models
into a new embedding model STransE that
represents each entity as a low-dimensional
vector, and each relation by two matrices and
a translation vector. STransE is a simple com-
bination of the SE and TransE models, but it
obtains better link prediction performance on
two benchmark datasets than previous embed-
ding models. Thus, STransE can serve as a
new baseline for the more complex models in
the link prediction task.

1

Introduction

Knowledge bases (KBs), such as WordNet (Fell-
baum, 1998), YAGO (Suchanek et al., 2007), Free-
base (Bollacker et al., 2008) and DBpedia (Lehmann
et al., 2015), represent relationships between entities
as triples (head entity, relation, tail entity). Even
very large knowledge bases are still far from com-
plete (Socher et al., 2013; West et al., 2014). Link
prediction or knowledge base completion systems
(Nickel et al., 2016a) predict which triples not in
a knowledge base are likely to be true (Taskar et

∗ A revised version of our NAACL-HLT 2016 paper with

additional experimental results and latest related work.

al., 2004; Bordes et al., 2011). A variety of differ-
ent kinds of information is potentially useful here,
including information extracted from external cor-
pora (Riedel et al., 2013; Wang et al., 2014a) and
the other relationships that hold between the enti-
ties (Angeli and Manning, 2013; Zhao et al., 2015).
For example, Toutanova et al. (2015) used informa-
tion from the external ClueWeb-12 corpus to signif-
icantly enhance performance.

While integrating a wide variety of information
sources can produce excellent results (Das et al.,
2017), there are several reasons for studying sim-
pler models that directly optimize a score function
for the triples in a knowledge base, such as the
one presented here. First, additional information
sources might not be available, e.g., for knowledge
bases for specialized domains. Second, models that
don’t exploit external resources are simpler and thus
typically much faster to train than the more com-
plex models using additional information. Third,
the more complex models that exploit external in-
formation are typically extensions of these simpler
models, and are often initialized with parameters es-
timated by such simpler models, so improvements to
the simpler models should yield corresponding im-
provements to the more complex models as well.

Embedding models for KB completion associate
entities and/or relations with dense feature vectors
or matrices. Such models obtain state-of-the-art per-
formance (Nickel et al., 2011; Bordes et al., 2011;
Bordes et al., 2012; Bordes et al., 2013; Socher et
al., 2013; Wang et al., 2014b; Guu et al., 2015) and
generalize to large KBs (Krompaß et al., 2015). Ta-
ble 1 summarizes a number of prominent embedding

Model

SE

Unstructured

TransE

DISTMULT

NTN

TransH

TransD

TransR

TranSparse

Our STransE

Opt.

Score function fr(h, t)
(cid:107)Wr,1h − Wr,2t(cid:107)(cid:96)1/2 ; Wr,1, Wr,2 ∈ Rk×k
(cid:107)h − t(cid:107)(cid:96)1/2
(cid:107)h + r − t(cid:107)(cid:96)1/2 ; r ∈ Rk
h(cid:62)Wrt ; Wr is a diagonal matrix ∈ Rk×k
r tanh(h(cid:62)Mrt + Wr,1h + Wr,2t + br) ; ur, br ∈ Rd; Mr ∈ Rk×k×d; Wr,1, Wr,2 ∈ Rd×k
u(cid:62)
(cid:107)(I − rpr(cid:62)
(cid:107)(I + rph(cid:62)
(cid:107)Wrh + r − Wrt(cid:107)(cid:96)1/2 ; Wr ∈ Rd×k ; r ∈ Rd
r ∈ Rd×k; θh
(cid:107)Wh
r(θt
(cid:107)Wr,1h + r − Wr,2t(cid:107)(cid:96)1/2 ; Wr,1, Wr,2 ∈ Rk×k; r ∈ Rk

p )h + r − (I − rpr(cid:62)
p )h + r − (I + rpt(cid:62)

r ∈ R ; r ∈ Rd

r)t(cid:107)(cid:96)1/2 ; Wh

r )h + r − Wt

r , Wt

r (θh

r , θt

p )t(cid:107)(cid:96)1/2 ; rp, r ∈ Rk ; I: Identity matrix size k × k
p )t(cid:107)(cid:96)1/2 ; rp, r ∈ Rd ; hp, tp ∈ Rk ; I: Identity matrix size d × k AdaDelta

SGD

AdaGrad

L-BFGS

SGD

SGD

SGD

SGD

SGD

SGD

Table 1: The score functions fr(h, t) and the optimization methods (Opt.) of several prominent embedding models
for KB completion. In all of these the entities h and t are represented by vectors h and t ∈ Rk respectively.

models for KB completion.

Let (h, r, t) represent a triple. In all of the models
discussed here, the head entity h and the tail entity
t are represented by vectors h and t ∈ Rk respec-
tively. The Unstructured model (Bordes et al., 2012)
assumes that h ≈ t. As the Unstructured model
does not take the relationship r into account, it can-
not distinguish different relation types. The Struc-
tured Embedding (SE) model (Bordes et al., 2011)
extends the unstructured model by assuming that h
and t are similar only in a relation-dependent sub-
space. It represents each relation r with two matri-
ces Wr,1 and Wr,2 ∈ Rk×k, which are chosen so
that Wr,1h ≈ Wr,2t. The TransE model (Bordes et
al., 2013) is inspired by models such as Word2Vec
(Mikolov et al., 2013) where relationships between
words often correspond to translations in latent fea-
ture space. The TransE model represents each rela-
tion r by a translation vector r ∈ Rk, which is cho-
sen so that h + r ≈ t.

The primary contribution of this paper is that
two very simple relation-prediction models, SE and
TransE, can be combined into a single model, which
we call STransE.1 Speciﬁcally, we use relation-
speciﬁc matrices Wr,1 and Wr,2 as in the SE model
to identify the relation-dependent aspects of both h
and t, and use a vector r as in the TransE model
to describe the relationship between h and t in this
subspace. Speciﬁcally, our new KB completion
model STransE chooses Wr,1, Wr,2 and r so that

Wr,1h + r ≈ Wr,2t. That is, a TransE-style rela-
tionship holds in some relation-dependent subspace,
and crucially, this subspace may involve very dif-
ferent projections of the head h and tail t. So Wr,1
and Wr,2 can highlight, suppress, or even change the
sign of, relation-speciﬁc attributes of h and t. For
example, for the “purchases” relationship, certain
attributes of individuals h (e.g., age, gender, mari-
tal status) are presumably strongly correlated with
very different attributes of objects t (e.g., sports car,
washing machine and the like).

As we show below, STransE performs better than
the SE and TransE models and other state-of-the-art
link prediction models on two standard link predic-
tion datasets WN18 and FB15k, so it can serve as
a new baseline for KB completion. We expect that
the STransE will also be able to serve as the basis
for extended models that exploit a wider variety of
information sources, just as TransE does.

2 Our approach

Let E denote the set of entities and R the set of re-
lation types. For each triple (h, r, t), where h, t ∈ E
and r ∈ R, the STransE model deﬁnes a score func-
tion fr(h, t) of its implausibility. Our goal is to
choose f such that the score fr(h, t) of a plausi-
ble triple (h, r, t) is smaller than the score fr(cid:48)(h(cid:48), t(cid:48))
of an implausible triple (h(cid:48), r(cid:48), t(cid:48)). We deﬁne the
STransE score function f as follows:

fr(h, t) = (cid:107)Wr,1h + r − Wr,2t(cid:107)(cid:96)1/2

1Source code: https://github.com/datquocnguyen/STransE

using either the (cid:96)1 or the (cid:96)2-norm (the choice is made

using validation data; in our experiments we found
that the (cid:96)1 norm gave slightly better results). To
learn the vectors and matrices we minimize the fol-
lowing margin-based objective function:

L =

(cid:88)

[γ + fr(h, t) − fr(h(cid:48), t(cid:48))]+

(h,r,t)∈G

(h(cid:48),r,t(cid:48))∈G(cid:48)

(h,r,t)

where [x]+ = max(0, x), γ is the margin hyper-
parameter, G is the training set consisting of correct
(h,r,t) = {(h(cid:48), r, t) | h(cid:48) ∈ E, (h(cid:48), r, t) /∈
triples, and G(cid:48)
G} ∪ {(h, r, t(cid:48)) | t(cid:48) ∈ E, (h, r, t(cid:48)) /∈ G} is the set
of incorrect triples generated by corrupting a correct
triple (h, r, t) ∈ G.

We use Stochastic Gradient Descent (SGD) to
minimize L, and impose the following constraints
during training: (cid:107)h(cid:107)2 (cid:54) 1, (cid:107)r(cid:107)2 (cid:54) 1, (cid:107)t(cid:107)2 (cid:54) 1,
(cid:107)Wr,1h(cid:107)2 (cid:54) 1 and (cid:107)Wr,2t(cid:107)2 (cid:54) 1.

3 Related work

Table 1 summarizes related embedding models for
link prediction and KB completion. The models
differ in the score functions fr(h, t) and the algo-
rithms used to optimize the margin-based objective
function, e.g., SGD, AdaGrad (Duchi et al., 2011),
AdaDelta (Zeiler, 2012) and L-BFGS (Liu and No-
cedal, 1989).

DISTMULT (Yang et al., 2015) is based on a
Bilinear model (Nickel et al., 2011; Bordes et al.,
2012; Jenatton et al., 2012) where each relation is
represented by a diagonal rather than a full matrix.
The neural tensor network (NTN) model (Socher et
al., 2013) uses a bilinear tensor operator to represent
each relation while ProjE (Shi and Weninger, 2017)
could be viewed as a simpliﬁed version of NTN
with diagonal matrices. Similar quadratic forms
are used to model entities and relations in KG2E
(He et al., 2015), ComplEx (Trouillon et al., 2016),
TATEC (Garc´ıa-Dur´an et al., 2016) and RSTE (Tay
et al., 2017).
In addition, HolE (Nickel et al.,
2016b) uses circular correlation—a compositional
operator—which could be interpreted as a compres-
sion of the tensor product.

The TransH model (Wang et al., 2014b) asso-
ciates each relation with a relation-speciﬁc hyper-
plane and uses a projection vector to project en-
tity vectors onto that hyperplane. TransD (Ji et al.,

2015) and TransR/CTransR (Lin et al., 2015b) ex-
tend the TransH model using two projection vec-
tors and a matrix to project entity vectors into a
relation-speciﬁc space, respectively. TransD learns
a relation-role speciﬁc mapping just as STransE, but
represents this mapping by projection vectors rather
than full matrices, as in STransE. The lppTransD
model (Yoon et al., 2016) extends TransD to ad-
ditionally use two projection vectors for represent-
ing each relation. In fact, our STransE model and
TranSparse (Ji et al., 2016) can be viewed as direct
extensions of the TransR model, where head and tail
entities are associated with their own projection ma-
trices, rather than using the same matrix for both, as
in TransR and CTransR.

Recently, several authors have shown that relation
paths between entities in KBs provide richer infor-
mation and improve the relationship prediction (Lin
et al., 2015a; Garc´ıa-Dur´an et al., 2015; Guu et al.,
2015; Wang et al., 2016; Feng et al., 2016; Liu et al.,
2016; Niepert, 2016; Wei et al., 2016; Toutanova et
al., 2016; Nguyen et al., 2016). In addition, Nickel
et al. (2016a) reviews other approaches for learning
from KBs and multi-relational data.

4 Experiments

For link prediction evaluation, we conduct experi-
ments and compare the performance of our STransE
model with published results on the benchmark
WN18 and FB15k datasets (Bordes et al., 2013). In-
formation about these datasets is given in Table 2.

#R
18
1,345

#E
40,943
14,951

Dataset
WN18
FB15k

#Train
141,442
483,142

#Valid #Test
5,000
5,000
59,071
50,000
Table 2: Statistics of the experimental datasets used in
this study (and previous works). #E is the number of
entities, #R is the number of relation types, and #Train,
#Valid and #Test are the numbers of triples in the training,
validation and test sets, respectively.

4.1 Task and evaluation protocol

The link prediction task (Bordes et al., 2011; Bordes
et al., 2012; Bordes et al., 2013) predicts the head or
tail entity given the relation type and the other entity,
i.e. predicting h given (?, r, t) or predicting t given
(h, r, ?) where ? denotes the missing element. The

Raw

Filtered

WN18

FB15k

Method

SE (Bordes et al., 2011)
Unstructured (Bordes et al., 2012)
TransE (Bordes et al., 2013)
TransH (Wang et al., 2014b)
TransR (Lin et al., 2015b)
CTransR (Lin et al., 2015b)
KG2E (He et al., 2015)
TransD (Ji et al., 2015)
lppTransD (Yoon et al., 2016)
TranSparse (Ji et al., 2016)
TATEC (Garc´ıa-Dur´an et al., 2016)
NTN (Socher et al., 2013)
DISTMULT (Yang et al., 2015)
HolE (Nickel et al., 2016b)
Our STransE
RTransE (Garc´ıa-Dur´an et al., 2015)
PTransE (Lin et al., 2015a)
GAKE (Feng et al., 2016)
Gaifman (Niepert, 2016)
Hiri (Liu et al., 2016)
NLFeat (Toutanova and Chen, 2015)
TEKE H (Wang and Li, 2016)
SSP (Xiao et al., 2017)

WN18
H10 MRR MR
273
68.5
1074
35.3
243
75.4
212
73.0
198
79.8
199
79.4
174
80.2
194
79.6
195
80.5
187
80.1
-
-
-
-
-
-
-
-
80.9
219
-
-
207
-
228
-
-
-
-
-
-
-
212
80.3
163
81.2
Table 3: Link prediction results. MR, H10 and MRR denote evaluation metrics of mean rank, Hits@10 (in %) and
mean reciprocal rank, respectively. “NLFeat” abbreviates Node+LinkFeat. The results for NTN (Socher et al., 2013)
listed in this table are taken from Yang et al. (2015) since NTN was originally evaluated on different datasets.

FB15k
H10 MRR MR H10 MRR MR H10 MRR
-
28.8
-
4.5
-
34.9
-
45.7
-
48.2
-
48.4
-
48.9
-
53.4
-
53.0
53.5
-
-
-
0.53
-
0.83
-
0.938
-
0.657
51.6
-
-
-
51.4
-
44.5
-
-
0.691
-
0.940
-
-
51.2
57.2
-

-
-
-
-
-
-
-
-
-
-
-
-
-
0.232
0.252
-
-
-
-
-
-
-
-

-
-
-
-
-
-
-
-
-
-
-
0.25
0.35
0.524
0.543
-
-
-
-
0.603
0.822
-
-

-
-
-
-
-
-
-
-
-
-
-
-
-
0.616
0.469
-
-
-
-
-
-
-
-

MR
1011
315
263
401
238
231
342
224
283
223
-
-
-
-
217
-
-
-
-
-
-
127
168

39.8
6.3
47.1
64.4
68.7
70.2
74.0
77.3
78.7
79.5
76.7
41.4
57.7
73.9
79.7
76.2
84.6
64.8
84.2
70.3
87.0
73.0
79.0

80.5
38.2
89.2
86.7
92.0
92.3
92.8
92.2
94.3
93.2
-
66.1
94.2
94.9
93.4
-
-
-
93.9
90.8
94.3
92.9
93.2

162
979
125
87
77
75
59
91
78
82
58
-
-
-
69
50
58
119
75
-
-
108
82

985
304
251
303
225
218
331
212
270
211
-
-
-
-
206
-
-
-
352
-
-
114
156

results are evaluated using the ranking induced by
the score function fr(h, t) on test triples.

For each test triple (h, r, t), we corrupted it by re-
placing either h or t by each of the possible entities
in turn, and then rank these candidates in ascend-
ing order of their implausibility value computed by
the score function. This is called as the “Raw” set-
ting protocol. For the “Filtered” setting protocol de-
scribed in Bordes et al. (2013), we removed any cor-
rupted triples that appear in the knowledge base, to
avoid cases where a correct corrupted triple might
be ranked higher than the test triple. The “Filtered”
setting thus provides a clearer view on the ranking
performance. Following Bordes et al. (2013), we re-
port the mean rank and the Hits@10 (i.e., the pro-
portion of test triples in which the target entity was
ranked in the top 10 predictions) for each model. In
addition, we report the mean reciprocal rank, which
is commonly used in information retrieval. In both
“Raw” and “Filtered” settings, lower mean rank,
higher mean reciprocal rank or higher Hits@10 in-
dicates better link prediction performance.

Following TransR (Lin et al., 2015b), TransD (Ji
et al., 2015), RTransE (Garc´ıa-Dur´an et al., 2015),
PTransE (Lin et al., 2015a), TATEC (Garc´ıa-Dur´an
et al., 2016) and TranSparse (Ji et al., 2016), we used
the entity and relation vectors produced by TransE
(Bordes et al., 2013) to initialize the entity and re-
lation vectors in STransE, and we initialized the re-
lation matrices with identity matrices. We applied
the “Bernoulli” trick used also in previous work for
generating head or tail entities when sampling incor-
rect triples (Wang et al., 2014b; Lin et al., 2015b; He
et al., 2015; Ji et al., 2015; Lin et al., 2015a; Yoon
et al., 2016; Ji et al., 2016). We ran SGD for 2,000
epochs to estimate the model parameters. Following
Bordes et al. (2013) we used a grid search on vali-
dation set to choose either the l1 or l2 norm in the
score function f , as well as to set the SGD learning
rate λ ∈ {0.0001, 0.0005, 0.001, 0.005, 0.01}, the
margin hyper-parameter γ ∈ {1, 3, 5} and the vector
size k ∈ {50, 100}. The lowest ﬁltered mean rank
on the validation set was obtained when using the
l1 norm in f on both WN18 and FB15k, and when

λ = 0.0005, γ = 5, and k = 50 for WN18, and
λ = 0.0001, γ = 1, and k = 100 for FB15k.

4.2 Main results

Table 3 compares the link prediction results of our
STransE model with results reported in prior work,
using the same experimental setup. The ﬁrst 15 rows
report the performance of the models that do not
exploit information about alternative paths between
head and tail entities. The next 5 rows report results
of the models that exploit information about relation
paths. The last 3 rows present results for the models
which make use of textual mentions derived from a
large external corpus.

It is clear that the models with the additional ex-
ternal corpus information obtained best results. In
future work we plan to extend the STransE model
to incorporate such additional information. Table 3
also shows that the models employing path infor-
mation generally achieve better results than mod-
els that do not use such information.
In terms of
models not exploiting path information or exter-
nal information, the STransE model produces the
highest ﬁltered mean rank on WN18 and the high-
est ﬁltered Hits@10 and mean reciprocal rank on
FB15k. Compared to the closely related models SE,
TransE, TransR, CTransR, TransD and TranSparse,
our STransE model does better than these models on
both WN18 and FB15k.

Following Bordes et al. (2013), Table 4 analyzes
Hits@10 results on FB15k with respect to the re-
lation categories deﬁned as follows: for each rela-
tion type r, we computed the averaged number ah of
heads h for a pair (r, t) and the averaged number at
of tails t for a pair (h, r). If ah < 1.5 and at < 1.5,
then r is labeled 1-1. If ah ≥ 1.5 and at < 1.5, then
r is labeled M-1. If ah < 1.5 and at ≥ 1.5, then r is
labeled as 1-M. If ah ≥ 1.5 and at ≥ 1.5, then r is
labeled as M-M. 1.4%, 8.9%, 14.6% and 75.1% of
the test triples belong to a relation type classiﬁed as
1-1, 1-M, M-1 and M-M, respectively.

Table 4 shows that in comparison to prior mod-
els not using path information, STransE obtains the
second highest Hits@10 result for M-M relation cat-
egory at (80.1% + 83.1%)/2 = 81.6% which is
0.5% smaller than the Hits@10 result of TranSparse
for M-M. However, STransE obtains 2.5% higher
Hits@10 result than TranSparse for M-1. In addi-

Predicting head h

Predicting tail t

Method

1.9

6.6

6.1

34.3 4.2

1-1 1-M M-1 M-M 1-1 1-M M-1 M-M
35.6 62.6 17.2 37.5 34.9 14.6 68.3 41.3
SE
34.5 2.5
6.6
Unstr.
43.7 65.7 18.2 47.2 43.7 19.7 66.7 50.0
TransE
TransH
66.8 87.6 28.7 64.5 65.5 39.8 83.3 67.2
78.8 89.2 34.1 69.2 79.2 37.4 90.4 72.1
TransR
81.5 89.0 34.7 71.2 80.8 38.6 90.1 73.8
CTransR
92.3 94.6 66.0 69.6 92.6 67.9 94.4 73.4
KG2E
TATEC
79.3 93.2 42.3 77.2 78.5 51.5 92.7 80.7
86.1 95.5 39.8 78.5 85.4 50.6 94.4 81.2
TransD
lppTransD 86.0 94.2 54.4 82.2 79.7 43.2 95.3 79.7
TranSparse 86.8 95.5 44.3 80.9 86.6 56.6 94.4 83.3
82.8 94.2 50.4 80.1 82.4 56.9 93.4 83.1
STransE

Table 4: Hits@10 (in %) by the relation category on
FB15k. “Unstr.” abbreviates Unstructured.

tion, STransE also performs better than TransD for
1-M and M-1 relation categories. We believe the
improved performance of the STransE model is due
to its use of full matrices, rather than just projection
vectors as in TransD. This permits STransE to model
diverse and complex relation categories (such as 1-
M, M-1 and especially M-M) better than TransD
and other similiar models. However, STransE is not
as good as TransD for the 1-1 relations. Perhaps the
extra parameters in STransE hurt performance in this
case (note that 1-1 relations are relatively rare, so
STransE does better overall).

5 Conclusion and future work

This paper presented a new embedding model for
link prediction and KB completion. Our STransE
combines insights from several simpler embed-
ding models, speciﬁcally the Structured Embedding
model (Bordes et al., 2011) and the TransE model
(Bordes et al., 2013), by using a low-dimensional
vector and two projection matrices to represent each
relation. STransE, while being conceptually sim-
ple, produces highly competitive results on standard
link prediction evaluations, and scores better than
the embedding-based models it builds on. Thus it
is a suitable candidate for serving as future baseline
for more complex models in the link prediction task.
In future work we plan to extend STransE to ex-
ploit relation path information in knowledge bases,
in a manner similar to Lin et al. (2015a), Guu et al.
(2015) or Nguyen et al. (2016).

Acknowledgments

This research was supported by a Google award
through the Natural Language Understanding Fo-
the Australian Re-
cused Program, and under
search Council’s Discovery Projects funding scheme
(project number DP160102156).

NICTA is funded by the Australian Government
through the Department of Communications and the
Australian Research Council through the ICT Centre
of Excellence Program. The ﬁrst author is supported
by an International Postgraduate Research Scholar-
ship and a NICTA NRPA Top-Up Scholarship.

References

[Angeli and Manning2013] Gabor Angeli and Christo-
pher Manning. 2013. Philosophers are Mortal: In-
ferring the Truth of Unseen Facts. In Proceedings of
the Seventeenth Conference on Computational Natural
Language Learning, pages 133–142.

[Bollacker et al.2008] Kurt Bollacker, Colin Evans,
Praveen Paritosh, Tim Sturge, and Jamie Taylor.
2008. Freebase: A Collaboratively Created Graph
Database for Structuring Human Knowledge.
In
Proceedings of
the 2008 ACM SIGMOD Interna-
tional Conference on Management of Data, pages
1247–1250.

[Bordes et al.2011] Antoine Bordes, Jason Weston, Ro-
nan Collobert, and Yoshua Bengio. 2011. Learning
Structured Embeddings of Knowledge Bases. In Pro-
ceedings of the Twenty-Fifth AAAI Conference on Ar-
tiﬁcial Intelligence, pages 301–306.

[Bordes et al.2012] Antoine Bordes, Xavier Glorot, Ja-
son Weston, and Yoshua Bengio. 2012. A Semantic
Matching Energy Function for Learning with Multi-
relational Data. Machine Learning, 94(2):233–259.
[Bordes et al.2013] Antoine Bordes, Nicolas Usunier,
Alberto Garcia-Duran, Jason Weston, and Oksana
Yakhnenko. 2013. Translating Embeddings for Mod-
eling Multi-relational Data. In Advances in Neural In-
formation Processing Systems 26, pages 2787–2795.
[Das et al.2017] Rajarshi Das, Arvind Neelakantan,
David Belanger, and Andrew McCallum.
2017.
Chains of reasoning over entities, relations, and text
In Proceedings of
using recurrent neural networks.
the 15th Conference of the European Chapter of the
Association for Computational Linguistics.

[Duchi et al.2011] John Duchi, Elad Hazan, and Yoram
Singer. 2011. Adaptive Subgradient Methods for On-
line Learning and Stochastic Optimization. The Jour-
nal of Machine Learning Research, 12:2121–2159.

[Fellbaum1998] Christiane D. Fellbaum. 1998. WordNet:

An Electronic Lexical Database. MIT Press.

[Feng et al.2016] Jun Feng, Minlie Huang, Yang Yang,
and xiaoyan zhu. 2016. GAKE: Graph Aware Knowl-
In Proceedings of COLING 2016,
edge Embedding.
the 26th International Conference on Computational
Linguistics: Technical Papers, pages 641–651.

[Garc´ıa-Dur´an et al.2015] Alberto Garc´ıa-Dur´an, An-
toine Bordes, and Nicolas Usunier. 2015. Composing
Relationships with Translations. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, pages 286–290.

[Garc´ıa-Dur´an et al.2016] Alberto Garc´ıa-Dur´an, An-
toine Bordes, Nicolas Usunier, and Yves Grandvalet.
2016.
Combining Two and Three-Way Embed-
ding Models for Link Prediction in Knowledge
Journal of Artiﬁcial Intelligence Research,
Bases.
55:715–742.

[Guu et al.2015] Kelvin Guu, John Miller, and Percy
Liang. 2015. Traversing Knowledge Graphs in Vec-
tor Space. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing,
pages 318–327.

[He et al.2015] Shizhu He, Kang Liu, Guoliang Ji, and
Jun Zhao. 2015. Learning to Represent Knowledge
Graphs with Gaussian Embedding. In Proceedings of
the 24th ACM International on Conference on Infor-
mation and Knowledge Management, pages 623–632.
Jenatton, Nicolas L.
Roux, Antoine Bordes, and Guillaume R Obozinski.
2012. A latent factor model for highly multi-relational
data. In Advances in Neural Information Processing
Systems 25, pages 3167–3175.

[Jenatton et al.2012] Rodolphe

[Ji et al.2015] Guoliang Ji, Shizhu He, Liheng Xu, Kang
Liu, and Jun Zhao. 2015. Knowledge Graph Embed-
ding via Dynamic Mapping Matrix. In Proceedings of
the 53rd Annual Meeting of the Association for Com-
putational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Volume
1: Long Papers), pages 687–696.

[Ji et al.2016] Guoliang Ji, Kang Liu, Shizhu He, and Jun
2016. Knowledge Graph Completion with
Zhao.
In Proceedings of
Adaptive Sparse Transfer Matrix.
the Thirtieth AAAI Conference on Artiﬁcial Intelli-
gence, pages 985–991.

[Krompaß et al.2015] Denis Krompaß, Stephan Baier,
and Volker Tresp. 2015. Type-Constrained Represen-
tation Learning in Knowledge Graphs. In Proceedings
of the 14th International Semantic Web Conference,
pages 640–655.

[Lehmann et al.2015] Jens Lehmann, Robert Isele, Max
Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N.
Mendes, Sebastian Hellmann, Mohamed Morsey,

Patrick van Kleef, S¨oren Auer, and Christian Bizer.
2015. DBpedia - A Large-scale, Multilingual Knowl-
edge Base Extracted from Wikipedia. Semantic Web,
6(2):167–195.

[Lin et al.2015a] Yankai Lin, Zhiyuan Liu, Huanbo Luan,
Maosong Sun, Siwei Rao, and Song Liu. 2015a. Mod-
eling Relation Paths for Representation Learning of
Knowledge Bases. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 705–714.

[Lin et al.2015b] Yankai Lin, Zhiyuan Liu, Maosong Sun,
Yang Liu, and Xuan Zhu. 2015b. Learning Entity
and Relation Embeddings for Knowledge Graph Com-
In Proceedings of the Twenty-Ninth AAAI
pletion.
Conference on Artiﬁcial Intelligence Learning, pages
2181–2187.

[Liu and Nocedal1989] D. C. Liu and J. Nocedal. 1989.
On the Limited Memory BFGS Method for Large
Scale Optimization. Mathematical Programming,
45(3):503–528.

[Liu et al.2016] Qiao Liu, Liuyi Jiang, Minghao Han, Yao
Liu, and Zhiguang Qin. 2016. Hierarchical Random
Walk Inference in Knowledge Graphs. In Proceedings
of the 39th International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 445–454.

[Mikolov et al.2013] Tomas Mikolov, Wen-tau Yih, and
Geoffrey Zweig.
2013. Linguistic Regularities in
Continuous Space Word Representations. In Proceed-
ings of the 2013 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 746–751.
[Nguyen et al.2016] Dat Quoc Nguyen, Kairit Sirts,
Lizhen Qu, and Mark Johnson. 2016. Neighborhood
Mixture Model for Knowledge Base Completion. In
Proceedings of The 20th SIGNLL Conference on Com-
putational Natural Language Learning, pages 40–50.
[Nickel et al.2011] Maximilian Nickel, Volker Tresp, and
Hans-Peter Kriegel. 2011. A Three-Way Model for
Collective Learning on Multi-Relational Data. In Pro-
ceedings of the 28th International Conference on Ma-
chine Learning, pages 809–816.

[Nickel et al.2016a] Maximilian Nickel, Kevin Murphy,
Volker Tresp, and Evgeniy Gabrilovich. 2016a. A Re-
view of Relational Machine Learning for Knowledge
Graphs. Proceedings of the IEEE, 104(1):11–33.

[Nickel et al.2016b] Maximilian

Lorenzo
Holo-
In
the Thirtieth AAAI Conference on

Rosasco, and Tomaso Poggio.
graphic embeddings of knowledge graphs.
Proceedings of
Artiﬁcial Intelligence, pages 1955–1961.

Nickel,

2016b.

[Riedel et al.2013] Sebastian Riedel, Limin Yao, Andrew
McCallum, and Benjamin M. Marlin. 2013. Rela-
tion Extraction with Matrix Factorization and Univer-
sal Schemas. In Proceedings of the 2013 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 74–84.

[Shi and Weninger2017] Baoxu Shi and Tim Weninger.
2017. ProjE: Embedding Projection for Knowledge
Graph Completion. In Proceedings of the 31st AAAI
Conference on Artiﬁcial Intelligence.

[Socher et al.2013] Richard Socher, Danqi Chen, Christo-
pher D Manning, and Andrew Ng. 2013. Reason-
ing With Neural Tensor Networks for Knowledge Base
Completion. In Advances in Neural Information Pro-
cessing Systems 26, pages 926–934.

[Suchanek et al.2007] Fabian M. Suchanek, Gjergji Kas-
neci, and Gerhard Weikum. 2007. YAGO: A Core
of Semantic Knowledge. In Proceedings of the 16th
International Conference on World Wide Web, pages
697–706.

[Taskar et al.2004] Ben Taskar, Ming fai Wong, Pieter
Abbeel, and Daphne Koller. 2004. Link Prediction in
In Advances in Neural Information
Relational Data.
Processing Systems 16, pages 659–666.

[Tay et al.2017] Yi Tay, Anh Tuan Luu, Siu Cheung Hui,
and Falk Brauer. 2017. Random Semantic Tensor
Ensemble for Scalable Knowledge Graph Link Predic-
tion. In Proceedings of the Tenth ACM International
Conference on Web Search and Data Mining, pages
751–760.

[Toutanova and Chen2015] Kristina Toutanova and Danqi
Chen. 2015. Observed Versus Latent Features for
Knowledge Base and Text Inference. In Proceedings
of the 3rd Workshop on Continuous Vector Space Mod-
els and their Compositionality, pages 57–66.

[Toutanova et al.2015] Kristina Toutanova, Danqi Chen,
Patrick Pantel, Hoifung Poon, Pallavi Choudhury, and
Michael Gamon. 2015. Representing Text for Joint
In Pro-
Embedding of Text and Knowledge Bases.
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1499–
1509.

[Toutanova et al.2016] Kristina Toutanova, Victoria Lin,
Wen-tau Yih, Hoifung Poon, and Chris Quirk. 2016.
Compositional Learning of Embeddings for Relation
Paths in Knowledge Base and Text. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers), pages
1434–1444.

[Niepert2016] Mathias Niepert.

2016. Discriminative
Gaifman Models. In Advances in Neural Information
Processing Systems 29, pages 3405–3413.

[Trouillon et al.2016] Th´eo Trouillon, Johannes Welbl,
´Eric Gaussier, and Guillaume
Sebastian Riedel,
Bouchard. 2016. Complex Embeddings for Simple

Link Prediction. In Proceedings of the 33nd Interna-
tional Conference on Machine Learning, pages 2071–
2080.

[Zeiler2012] Matthew D. Zeiler. 2012. ADADELTA:
CoRR,

An Adaptive Learning Rate Method.
abs/1212.5701.

[Zhao et al.2015] Yu Zhao, Sheng Gao, Patrick Gallinari,
and Jun Guo. 2015. Knowledge Base Completion
by Learning Pairwise-Interaction Differentiated Em-
beddings. Data Mining and Knowledge Discovery,
29(5):1486–1504.

[Wang and Li2016] Zhigang Wang and Juan-Zi Li. 2016.
Text-Enhanced Representation Learning for Knowl-
In Proceedings of the Twenty-Fifth In-
edge Graph.
ternational Joint Conference on Artiﬁcial Intelligence,
pages 1293–1299.

[Wang et al.2014a] Zhen Wang, Jianwen Zhang, Jianlin
Feng, and Zheng Chen. 2014a. Knowledge Graph
In Proceedings of the
and Text Jointly Embedding.
2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1591–1601.
[Wang et al.2014b] Zhen Wang, Jianwen Zhang, Jianlin
Feng, and Zheng Chen. 2014b. Knowledge Graph
In Pro-
Embedding by Translating on Hyperplanes.
ceedings of the Twenty-Eighth AAAI Conference on
Artiﬁcial Intelligence, pages 1112–1119.

[Wang et al.2016] Quan Wang, Jing Liu, Yuanfei Luo,
Bin Wang, and Chin-Yew Lin. 2016. Knowledge Base
In Proceed-
Completion via Coupled Path Ranking.
ings of the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 1308–1318.

[Wei et al.2016] Zhuoyu Wei, Jun Zhao, and Kang Liu.
2016. Mining Inference Formulas by Goal-Directed
Random Walks. In Proceedings of the 2016 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1379–1388.

[West et al.2014] Robert West, Evgeniy Gabrilovich,
Kevin Murphy, Shaohua Sun, Rahul Gupta, and
Dekang Lin. 2014. Knowledge Base Completion via
Search-based Question Answering. In Proceedings of
the 23rd International Conference on World Wide Web,
pages 515–526.

[Xiao et al.2017] Han Xiao, Minlie Huang, and Xiaoyan
Zhu. 2017. SSP: semantic space projection for knowl-
edge graph embedding with text descriptions. In Pro-
ceedings of the 31st AAAI Conference on Artiﬁcial In-
telligence.

[Yang et al.2015] Bishan Yang, Wen-tau Yih, Xiaodong
He, Jianfeng Gao, and Li Deng. 2015. Embedding
Entities and Relations for Learning and Inference in
Knowledge Bases. In Proceedings of the International
Conference on Learning Representations.

[Yoon et al.2016] Hee-Geun Yoon, Hyun-Je Song, Seong-
Bae Park, and Se-Young Park. 2016. A Translation-
Based Knowledge Graph Embedding Preserving Log-
ical Property of Relations. In Proceedings of the 2016
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 907–916.

7
1
0
2
 
r
a

M
 
8
 
 
]
L
C
.
s
c
[
 
 
3
v
0
4
1
8
0
.
6
0
6
1
:
v
i
X
r
a

STransE: a novel embedding model of entities and relationships
in knowledge bases∗

Dat Quoc Nguyen1, Kairit Sirts1, Lizhen Qu2 and Mark Johnson1

1 Department of Computing, Macquarie University, Sydney, Australia
dat.nguyen@students.mq.edu.au, {kairit.sirts, mark.johnson}@mq.edu.au
2 NICTA, ACT 2601, Australia
lizhen.qu@nicta.com.au

Abstract

Knowledge bases of real-world facts about
entities and their relationships are useful re-
sources for a variety of natural language pro-
cessing tasks. However, because knowledge
bases are typically incomplete, it is useful to
be able to perform link prediction or knowl-
edge base completion, i.e., predict whether
a relationship not in the knowledge base is
likely to be true. This paper combines insights
from several previous link prediction models
into a new embedding model STransE that
represents each entity as a low-dimensional
vector, and each relation by two matrices and
a translation vector. STransE is a simple com-
bination of the SE and TransE models, but it
obtains better link prediction performance on
two benchmark datasets than previous embed-
ding models. Thus, STransE can serve as a
new baseline for the more complex models in
the link prediction task.

1

Introduction

Knowledge bases (KBs), such as WordNet (Fell-
baum, 1998), YAGO (Suchanek et al., 2007), Free-
base (Bollacker et al., 2008) and DBpedia (Lehmann
et al., 2015), represent relationships between entities
as triples (head entity, relation, tail entity). Even
very large knowledge bases are still far from com-
plete (Socher et al., 2013; West et al., 2014). Link
prediction or knowledge base completion systems
(Nickel et al., 2016a) predict which triples not in
a knowledge base are likely to be true (Taskar et

∗ A revised version of our NAACL-HLT 2016 paper with

additional experimental results and latest related work.

al., 2004; Bordes et al., 2011). A variety of differ-
ent kinds of information is potentially useful here,
including information extracted from external cor-
pora (Riedel et al., 2013; Wang et al., 2014a) and
the other relationships that hold between the enti-
ties (Angeli and Manning, 2013; Zhao et al., 2015).
For example, Toutanova et al. (2015) used informa-
tion from the external ClueWeb-12 corpus to signif-
icantly enhance performance.

While integrating a wide variety of information
sources can produce excellent results (Das et al.,
2017), there are several reasons for studying sim-
pler models that directly optimize a score function
for the triples in a knowledge base, such as the
one presented here. First, additional information
sources might not be available, e.g., for knowledge
bases for specialized domains. Second, models that
don’t exploit external resources are simpler and thus
typically much faster to train than the more com-
plex models using additional information. Third,
the more complex models that exploit external in-
formation are typically extensions of these simpler
models, and are often initialized with parameters es-
timated by such simpler models, so improvements to
the simpler models should yield corresponding im-
provements to the more complex models as well.

Embedding models for KB completion associate
entities and/or relations with dense feature vectors
or matrices. Such models obtain state-of-the-art per-
formance (Nickel et al., 2011; Bordes et al., 2011;
Bordes et al., 2012; Bordes et al., 2013; Socher et
al., 2013; Wang et al., 2014b; Guu et al., 2015) and
generalize to large KBs (Krompaß et al., 2015). Ta-
ble 1 summarizes a number of prominent embedding

Model

SE

Unstructured

TransE

DISTMULT

NTN

TransH

TransD

TransR

TranSparse

Our STransE

Opt.

Score function fr(h, t)
(cid:107)Wr,1h − Wr,2t(cid:107)(cid:96)1/2 ; Wr,1, Wr,2 ∈ Rk×k
(cid:107)h − t(cid:107)(cid:96)1/2
(cid:107)h + r − t(cid:107)(cid:96)1/2 ; r ∈ Rk
h(cid:62)Wrt ; Wr is a diagonal matrix ∈ Rk×k
r tanh(h(cid:62)Mrt + Wr,1h + Wr,2t + br) ; ur, br ∈ Rd; Mr ∈ Rk×k×d; Wr,1, Wr,2 ∈ Rd×k
u(cid:62)
(cid:107)(I − rpr(cid:62)
(cid:107)(I + rph(cid:62)
(cid:107)Wrh + r − Wrt(cid:107)(cid:96)1/2 ; Wr ∈ Rd×k ; r ∈ Rd
r ∈ Rd×k; θh
(cid:107)Wh
r(θt
(cid:107)Wr,1h + r − Wr,2t(cid:107)(cid:96)1/2 ; Wr,1, Wr,2 ∈ Rk×k; r ∈ Rk

p )h + r − (I − rpr(cid:62)
p )h + r − (I + rpt(cid:62)

r ∈ R ; r ∈ Rd

r)t(cid:107)(cid:96)1/2 ; Wh

r )h + r − Wt

r , Wt

r (θh

r , θt

p )t(cid:107)(cid:96)1/2 ; rp, r ∈ Rk ; I: Identity matrix size k × k
p )t(cid:107)(cid:96)1/2 ; rp, r ∈ Rd ; hp, tp ∈ Rk ; I: Identity matrix size d × k AdaDelta

SGD

AdaGrad

L-BFGS

SGD

SGD

SGD

SGD

SGD

SGD

Table 1: The score functions fr(h, t) and the optimization methods (Opt.) of several prominent embedding models
for KB completion. In all of these the entities h and t are represented by vectors h and t ∈ Rk respectively.

models for KB completion.

Let (h, r, t) represent a triple. In all of the models
discussed here, the head entity h and the tail entity
t are represented by vectors h and t ∈ Rk respec-
tively. The Unstructured model (Bordes et al., 2012)
assumes that h ≈ t. As the Unstructured model
does not take the relationship r into account, it can-
not distinguish different relation types. The Struc-
tured Embedding (SE) model (Bordes et al., 2011)
extends the unstructured model by assuming that h
and t are similar only in a relation-dependent sub-
space. It represents each relation r with two matri-
ces Wr,1 and Wr,2 ∈ Rk×k, which are chosen so
that Wr,1h ≈ Wr,2t. The TransE model (Bordes et
al., 2013) is inspired by models such as Word2Vec
(Mikolov et al., 2013) where relationships between
words often correspond to translations in latent fea-
ture space. The TransE model represents each rela-
tion r by a translation vector r ∈ Rk, which is cho-
sen so that h + r ≈ t.

The primary contribution of this paper is that
two very simple relation-prediction models, SE and
TransE, can be combined into a single model, which
we call STransE.1 Speciﬁcally, we use relation-
speciﬁc matrices Wr,1 and Wr,2 as in the SE model
to identify the relation-dependent aspects of both h
and t, and use a vector r as in the TransE model
to describe the relationship between h and t in this
subspace. Speciﬁcally, our new KB completion
model STransE chooses Wr,1, Wr,2 and r so that

Wr,1h + r ≈ Wr,2t. That is, a TransE-style rela-
tionship holds in some relation-dependent subspace,
and crucially, this subspace may involve very dif-
ferent projections of the head h and tail t. So Wr,1
and Wr,2 can highlight, suppress, or even change the
sign of, relation-speciﬁc attributes of h and t. For
example, for the “purchases” relationship, certain
attributes of individuals h (e.g., age, gender, mari-
tal status) are presumably strongly correlated with
very different attributes of objects t (e.g., sports car,
washing machine and the like).

As we show below, STransE performs better than
the SE and TransE models and other state-of-the-art
link prediction models on two standard link predic-
tion datasets WN18 and FB15k, so it can serve as
a new baseline for KB completion. We expect that
the STransE will also be able to serve as the basis
for extended models that exploit a wider variety of
information sources, just as TransE does.

2 Our approach

Let E denote the set of entities and R the set of re-
lation types. For each triple (h, r, t), where h, t ∈ E
and r ∈ R, the STransE model deﬁnes a score func-
tion fr(h, t) of its implausibility. Our goal is to
choose f such that the score fr(h, t) of a plausi-
ble triple (h, r, t) is smaller than the score fr(cid:48)(h(cid:48), t(cid:48))
of an implausible triple (h(cid:48), r(cid:48), t(cid:48)). We deﬁne the
STransE score function f as follows:

fr(h, t) = (cid:107)Wr,1h + r − Wr,2t(cid:107)(cid:96)1/2

1Source code: https://github.com/datquocnguyen/STransE

using either the (cid:96)1 or the (cid:96)2-norm (the choice is made

using validation data; in our experiments we found
that the (cid:96)1 norm gave slightly better results). To
learn the vectors and matrices we minimize the fol-
lowing margin-based objective function:

L =

(cid:88)

[γ + fr(h, t) − fr(h(cid:48), t(cid:48))]+

(h,r,t)∈G

(h(cid:48),r,t(cid:48))∈G(cid:48)

(h,r,t)

where [x]+ = max(0, x), γ is the margin hyper-
parameter, G is the training set consisting of correct
(h,r,t) = {(h(cid:48), r, t) | h(cid:48) ∈ E, (h(cid:48), r, t) /∈
triples, and G(cid:48)
G} ∪ {(h, r, t(cid:48)) | t(cid:48) ∈ E, (h, r, t(cid:48)) /∈ G} is the set
of incorrect triples generated by corrupting a correct
triple (h, r, t) ∈ G.

We use Stochastic Gradient Descent (SGD) to
minimize L, and impose the following constraints
during training: (cid:107)h(cid:107)2 (cid:54) 1, (cid:107)r(cid:107)2 (cid:54) 1, (cid:107)t(cid:107)2 (cid:54) 1,
(cid:107)Wr,1h(cid:107)2 (cid:54) 1 and (cid:107)Wr,2t(cid:107)2 (cid:54) 1.

3 Related work

Table 1 summarizes related embedding models for
link prediction and KB completion. The models
differ in the score functions fr(h, t) and the algo-
rithms used to optimize the margin-based objective
function, e.g., SGD, AdaGrad (Duchi et al., 2011),
AdaDelta (Zeiler, 2012) and L-BFGS (Liu and No-
cedal, 1989).

DISTMULT (Yang et al., 2015) is based on a
Bilinear model (Nickel et al., 2011; Bordes et al.,
2012; Jenatton et al., 2012) where each relation is
represented by a diagonal rather than a full matrix.
The neural tensor network (NTN) model (Socher et
al., 2013) uses a bilinear tensor operator to represent
each relation while ProjE (Shi and Weninger, 2017)
could be viewed as a simpliﬁed version of NTN
with diagonal matrices. Similar quadratic forms
are used to model entities and relations in KG2E
(He et al., 2015), ComplEx (Trouillon et al., 2016),
TATEC (Garc´ıa-Dur´an et al., 2016) and RSTE (Tay
et al., 2017).
In addition, HolE (Nickel et al.,
2016b) uses circular correlation—a compositional
operator—which could be interpreted as a compres-
sion of the tensor product.

The TransH model (Wang et al., 2014b) asso-
ciates each relation with a relation-speciﬁc hyper-
plane and uses a projection vector to project en-
tity vectors onto that hyperplane. TransD (Ji et al.,

2015) and TransR/CTransR (Lin et al., 2015b) ex-
tend the TransH model using two projection vec-
tors and a matrix to project entity vectors into a
relation-speciﬁc space, respectively. TransD learns
a relation-role speciﬁc mapping just as STransE, but
represents this mapping by projection vectors rather
than full matrices, as in STransE. The lppTransD
model (Yoon et al., 2016) extends TransD to ad-
ditionally use two projection vectors for represent-
ing each relation. In fact, our STransE model and
TranSparse (Ji et al., 2016) can be viewed as direct
extensions of the TransR model, where head and tail
entities are associated with their own projection ma-
trices, rather than using the same matrix for both, as
in TransR and CTransR.

Recently, several authors have shown that relation
paths between entities in KBs provide richer infor-
mation and improve the relationship prediction (Lin
et al., 2015a; Garc´ıa-Dur´an et al., 2015; Guu et al.,
2015; Wang et al., 2016; Feng et al., 2016; Liu et al.,
2016; Niepert, 2016; Wei et al., 2016; Toutanova et
al., 2016; Nguyen et al., 2016). In addition, Nickel
et al. (2016a) reviews other approaches for learning
from KBs and multi-relational data.

4 Experiments

For link prediction evaluation, we conduct experi-
ments and compare the performance of our STransE
model with published results on the benchmark
WN18 and FB15k datasets (Bordes et al., 2013). In-
formation about these datasets is given in Table 2.

#R
18
1,345

#E
40,943
14,951

Dataset
WN18
FB15k

#Train
141,442
483,142

#Valid #Test
5,000
5,000
59,071
50,000
Table 2: Statistics of the experimental datasets used in
this study (and previous works). #E is the number of
entities, #R is the number of relation types, and #Train,
#Valid and #Test are the numbers of triples in the training,
validation and test sets, respectively.

4.1 Task and evaluation protocol

The link prediction task (Bordes et al., 2011; Bordes
et al., 2012; Bordes et al., 2013) predicts the head or
tail entity given the relation type and the other entity,
i.e. predicting h given (?, r, t) or predicting t given
(h, r, ?) where ? denotes the missing element. The

Raw

Filtered

WN18

FB15k

Method

SE (Bordes et al., 2011)
Unstructured (Bordes et al., 2012)
TransE (Bordes et al., 2013)
TransH (Wang et al., 2014b)
TransR (Lin et al., 2015b)
CTransR (Lin et al., 2015b)
KG2E (He et al., 2015)
TransD (Ji et al., 2015)
lppTransD (Yoon et al., 2016)
TranSparse (Ji et al., 2016)
TATEC (Garc´ıa-Dur´an et al., 2016)
NTN (Socher et al., 2013)
DISTMULT (Yang et al., 2015)
HolE (Nickel et al., 2016b)
Our STransE
RTransE (Garc´ıa-Dur´an et al., 2015)
PTransE (Lin et al., 2015a)
GAKE (Feng et al., 2016)
Gaifman (Niepert, 2016)
Hiri (Liu et al., 2016)
NLFeat (Toutanova and Chen, 2015)
TEKE H (Wang and Li, 2016)
SSP (Xiao et al., 2017)

WN18
H10 MRR MR
273
68.5
1074
35.3
243
75.4
212
73.0
198
79.8
199
79.4
174
80.2
194
79.6
195
80.5
187
80.1
-
-
-
-
-
-
-
-
80.9
219
-
-
207
-
228
-
-
-
-
-
-
-
212
80.3
163
81.2
Table 3: Link prediction results. MR, H10 and MRR denote evaluation metrics of mean rank, Hits@10 (in %) and
mean reciprocal rank, respectively. “NLFeat” abbreviates Node+LinkFeat. The results for NTN (Socher et al., 2013)
listed in this table are taken from Yang et al. (2015) since NTN was originally evaluated on different datasets.

FB15k
H10 MRR MR H10 MRR MR H10 MRR
-
28.8
-
4.5
-
34.9
-
45.7
-
48.2
-
48.4
-
48.9
-
53.4
-
53.0
53.5
-
-
-
0.53
-
0.83
-
0.938
-
0.657
51.6
-
-
-
51.4
-
44.5
-
-
0.691
-
0.940
-
-
51.2
57.2
-

-
-
-
-
-
-
-
-
-
-
-
-
-
0.232
0.252
-
-
-
-
-
-
-
-

-
-
-
-
-
-
-
-
-
-
-
0.25
0.35
0.524
0.543
-
-
-
-
0.603
0.822
-
-

-
-
-
-
-
-
-
-
-
-
-
-
-
0.616
0.469
-
-
-
-
-
-
-
-

MR
1011
315
263
401
238
231
342
224
283
223
-
-
-
-
217
-
-
-
-
-
-
127
168

39.8
6.3
47.1
64.4
68.7
70.2
74.0
77.3
78.7
79.5
76.7
41.4
57.7
73.9
79.7
76.2
84.6
64.8
84.2
70.3
87.0
73.0
79.0

80.5
38.2
89.2
86.7
92.0
92.3
92.8
92.2
94.3
93.2
-
66.1
94.2
94.9
93.4
-
-
-
93.9
90.8
94.3
92.9
93.2

162
979
125
87
77
75
59
91
78
82
58
-
-
-
69
50
58
119
75
-
-
108
82

985
304
251
303
225
218
331
212
270
211
-
-
-
-
206
-
-
-
352
-
-
114
156

results are evaluated using the ranking induced by
the score function fr(h, t) on test triples.

For each test triple (h, r, t), we corrupted it by re-
placing either h or t by each of the possible entities
in turn, and then rank these candidates in ascend-
ing order of their implausibility value computed by
the score function. This is called as the “Raw” set-
ting protocol. For the “Filtered” setting protocol de-
scribed in Bordes et al. (2013), we removed any cor-
rupted triples that appear in the knowledge base, to
avoid cases where a correct corrupted triple might
be ranked higher than the test triple. The “Filtered”
setting thus provides a clearer view on the ranking
performance. Following Bordes et al. (2013), we re-
port the mean rank and the Hits@10 (i.e., the pro-
portion of test triples in which the target entity was
ranked in the top 10 predictions) for each model. In
addition, we report the mean reciprocal rank, which
is commonly used in information retrieval. In both
“Raw” and “Filtered” settings, lower mean rank,
higher mean reciprocal rank or higher Hits@10 in-
dicates better link prediction performance.

Following TransR (Lin et al., 2015b), TransD (Ji
et al., 2015), RTransE (Garc´ıa-Dur´an et al., 2015),
PTransE (Lin et al., 2015a), TATEC (Garc´ıa-Dur´an
et al., 2016) and TranSparse (Ji et al., 2016), we used
the entity and relation vectors produced by TransE
(Bordes et al., 2013) to initialize the entity and re-
lation vectors in STransE, and we initialized the re-
lation matrices with identity matrices. We applied
the “Bernoulli” trick used also in previous work for
generating head or tail entities when sampling incor-
rect triples (Wang et al., 2014b; Lin et al., 2015b; He
et al., 2015; Ji et al., 2015; Lin et al., 2015a; Yoon
et al., 2016; Ji et al., 2016). We ran SGD for 2,000
epochs to estimate the model parameters. Following
Bordes et al. (2013) we used a grid search on vali-
dation set to choose either the l1 or l2 norm in the
score function f , as well as to set the SGD learning
rate λ ∈ {0.0001, 0.0005, 0.001, 0.005, 0.01}, the
margin hyper-parameter γ ∈ {1, 3, 5} and the vector
size k ∈ {50, 100}. The lowest ﬁltered mean rank
on the validation set was obtained when using the
l1 norm in f on both WN18 and FB15k, and when

λ = 0.0005, γ = 5, and k = 50 for WN18, and
λ = 0.0001, γ = 1, and k = 100 for FB15k.

4.2 Main results

Table 3 compares the link prediction results of our
STransE model with results reported in prior work,
using the same experimental setup. The ﬁrst 15 rows
report the performance of the models that do not
exploit information about alternative paths between
head and tail entities. The next 5 rows report results
of the models that exploit information about relation
paths. The last 3 rows present results for the models
which make use of textual mentions derived from a
large external corpus.

It is clear that the models with the additional ex-
ternal corpus information obtained best results. In
future work we plan to extend the STransE model
to incorporate such additional information. Table 3
also shows that the models employing path infor-
mation generally achieve better results than mod-
els that do not use such information.
In terms of
models not exploiting path information or exter-
nal information, the STransE model produces the
highest ﬁltered mean rank on WN18 and the high-
est ﬁltered Hits@10 and mean reciprocal rank on
FB15k. Compared to the closely related models SE,
TransE, TransR, CTransR, TransD and TranSparse,
our STransE model does better than these models on
both WN18 and FB15k.

Following Bordes et al. (2013), Table 4 analyzes
Hits@10 results on FB15k with respect to the re-
lation categories deﬁned as follows: for each rela-
tion type r, we computed the averaged number ah of
heads h for a pair (r, t) and the averaged number at
of tails t for a pair (h, r). If ah < 1.5 and at < 1.5,
then r is labeled 1-1. If ah ≥ 1.5 and at < 1.5, then
r is labeled M-1. If ah < 1.5 and at ≥ 1.5, then r is
labeled as 1-M. If ah ≥ 1.5 and at ≥ 1.5, then r is
labeled as M-M. 1.4%, 8.9%, 14.6% and 75.1% of
the test triples belong to a relation type classiﬁed as
1-1, 1-M, M-1 and M-M, respectively.

Table 4 shows that in comparison to prior mod-
els not using path information, STransE obtains the
second highest Hits@10 result for M-M relation cat-
egory at (80.1% + 83.1%)/2 = 81.6% which is
0.5% smaller than the Hits@10 result of TranSparse
for M-M. However, STransE obtains 2.5% higher
Hits@10 result than TranSparse for M-1. In addi-

Predicting head h

Predicting tail t

Method

1.9

6.6

6.1

34.3 4.2

1-1 1-M M-1 M-M 1-1 1-M M-1 M-M
35.6 62.6 17.2 37.5 34.9 14.6 68.3 41.3
SE
34.5 2.5
6.6
Unstr.
43.7 65.7 18.2 47.2 43.7 19.7 66.7 50.0
TransE
TransH
66.8 87.6 28.7 64.5 65.5 39.8 83.3 67.2
78.8 89.2 34.1 69.2 79.2 37.4 90.4 72.1
TransR
81.5 89.0 34.7 71.2 80.8 38.6 90.1 73.8
CTransR
92.3 94.6 66.0 69.6 92.6 67.9 94.4 73.4
KG2E
TATEC
79.3 93.2 42.3 77.2 78.5 51.5 92.7 80.7
86.1 95.5 39.8 78.5 85.4 50.6 94.4 81.2
TransD
lppTransD 86.0 94.2 54.4 82.2 79.7 43.2 95.3 79.7
TranSparse 86.8 95.5 44.3 80.9 86.6 56.6 94.4 83.3
82.8 94.2 50.4 80.1 82.4 56.9 93.4 83.1
STransE

Table 4: Hits@10 (in %) by the relation category on
FB15k. “Unstr.” abbreviates Unstructured.

tion, STransE also performs better than TransD for
1-M and M-1 relation categories. We believe the
improved performance of the STransE model is due
to its use of full matrices, rather than just projection
vectors as in TransD. This permits STransE to model
diverse and complex relation categories (such as 1-
M, M-1 and especially M-M) better than TransD
and other similiar models. However, STransE is not
as good as TransD for the 1-1 relations. Perhaps the
extra parameters in STransE hurt performance in this
case (note that 1-1 relations are relatively rare, so
STransE does better overall).

5 Conclusion and future work

This paper presented a new embedding model for
link prediction and KB completion. Our STransE
combines insights from several simpler embed-
ding models, speciﬁcally the Structured Embedding
model (Bordes et al., 2011) and the TransE model
(Bordes et al., 2013), by using a low-dimensional
vector and two projection matrices to represent each
relation. STransE, while being conceptually sim-
ple, produces highly competitive results on standard
link prediction evaluations, and scores better than
the embedding-based models it builds on. Thus it
is a suitable candidate for serving as future baseline
for more complex models in the link prediction task.
In future work we plan to extend STransE to ex-
ploit relation path information in knowledge bases,
in a manner similar to Lin et al. (2015a), Guu et al.
(2015) or Nguyen et al. (2016).

Acknowledgments

This research was supported by a Google award
through the Natural Language Understanding Fo-
the Australian Re-
cused Program, and under
search Council’s Discovery Projects funding scheme
(project number DP160102156).

NICTA is funded by the Australian Government
through the Department of Communications and the
Australian Research Council through the ICT Centre
of Excellence Program. The ﬁrst author is supported
by an International Postgraduate Research Scholar-
ship and a NICTA NRPA Top-Up Scholarship.

References

[Angeli and Manning2013] Gabor Angeli and Christo-
pher Manning. 2013. Philosophers are Mortal: In-
ferring the Truth of Unseen Facts. In Proceedings of
the Seventeenth Conference on Computational Natural
Language Learning, pages 133–142.

[Bollacker et al.2008] Kurt Bollacker, Colin Evans,
Praveen Paritosh, Tim Sturge, and Jamie Taylor.
2008. Freebase: A Collaboratively Created Graph
Database for Structuring Human Knowledge.
In
Proceedings of
the 2008 ACM SIGMOD Interna-
tional Conference on Management of Data, pages
1247–1250.

[Bordes et al.2011] Antoine Bordes, Jason Weston, Ro-
nan Collobert, and Yoshua Bengio. 2011. Learning
Structured Embeddings of Knowledge Bases. In Pro-
ceedings of the Twenty-Fifth AAAI Conference on Ar-
tiﬁcial Intelligence, pages 301–306.

[Bordes et al.2012] Antoine Bordes, Xavier Glorot, Ja-
son Weston, and Yoshua Bengio. 2012. A Semantic
Matching Energy Function for Learning with Multi-
relational Data. Machine Learning, 94(2):233–259.
[Bordes et al.2013] Antoine Bordes, Nicolas Usunier,
Alberto Garcia-Duran, Jason Weston, and Oksana
Yakhnenko. 2013. Translating Embeddings for Mod-
eling Multi-relational Data. In Advances in Neural In-
formation Processing Systems 26, pages 2787–2795.
[Das et al.2017] Rajarshi Das, Arvind Neelakantan,
David Belanger, and Andrew McCallum.
2017.
Chains of reasoning over entities, relations, and text
In Proceedings of
using recurrent neural networks.
the 15th Conference of the European Chapter of the
Association for Computational Linguistics.

[Duchi et al.2011] John Duchi, Elad Hazan, and Yoram
Singer. 2011. Adaptive Subgradient Methods for On-
line Learning and Stochastic Optimization. The Jour-
nal of Machine Learning Research, 12:2121–2159.

[Fellbaum1998] Christiane D. Fellbaum. 1998. WordNet:

An Electronic Lexical Database. MIT Press.

[Feng et al.2016] Jun Feng, Minlie Huang, Yang Yang,
and xiaoyan zhu. 2016. GAKE: Graph Aware Knowl-
In Proceedings of COLING 2016,
edge Embedding.
the 26th International Conference on Computational
Linguistics: Technical Papers, pages 641–651.

[Garc´ıa-Dur´an et al.2015] Alberto Garc´ıa-Dur´an, An-
toine Bordes, and Nicolas Usunier. 2015. Composing
Relationships with Translations. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, pages 286–290.

[Garc´ıa-Dur´an et al.2016] Alberto Garc´ıa-Dur´an, An-
toine Bordes, Nicolas Usunier, and Yves Grandvalet.
2016.
Combining Two and Three-Way Embed-
ding Models for Link Prediction in Knowledge
Journal of Artiﬁcial Intelligence Research,
Bases.
55:715–742.

[Guu et al.2015] Kelvin Guu, John Miller, and Percy
Liang. 2015. Traversing Knowledge Graphs in Vec-
tor Space. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing,
pages 318–327.

[He et al.2015] Shizhu He, Kang Liu, Guoliang Ji, and
Jun Zhao. 2015. Learning to Represent Knowledge
Graphs with Gaussian Embedding. In Proceedings of
the 24th ACM International on Conference on Infor-
mation and Knowledge Management, pages 623–632.
Jenatton, Nicolas L.
Roux, Antoine Bordes, and Guillaume R Obozinski.
2012. A latent factor model for highly multi-relational
data. In Advances in Neural Information Processing
Systems 25, pages 3167–3175.

[Jenatton et al.2012] Rodolphe

[Ji et al.2015] Guoliang Ji, Shizhu He, Liheng Xu, Kang
Liu, and Jun Zhao. 2015. Knowledge Graph Embed-
ding via Dynamic Mapping Matrix. In Proceedings of
the 53rd Annual Meeting of the Association for Com-
putational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Volume
1: Long Papers), pages 687–696.

[Ji et al.2016] Guoliang Ji, Kang Liu, Shizhu He, and Jun
2016. Knowledge Graph Completion with
Zhao.
In Proceedings of
Adaptive Sparse Transfer Matrix.
the Thirtieth AAAI Conference on Artiﬁcial Intelli-
gence, pages 985–991.

[Krompaß et al.2015] Denis Krompaß, Stephan Baier,
and Volker Tresp. 2015. Type-Constrained Represen-
tation Learning in Knowledge Graphs. In Proceedings
of the 14th International Semantic Web Conference,
pages 640–655.

[Lehmann et al.2015] Jens Lehmann, Robert Isele, Max
Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N.
Mendes, Sebastian Hellmann, Mohamed Morsey,

Patrick van Kleef, S¨oren Auer, and Christian Bizer.
2015. DBpedia - A Large-scale, Multilingual Knowl-
edge Base Extracted from Wikipedia. Semantic Web,
6(2):167–195.

[Lin et al.2015a] Yankai Lin, Zhiyuan Liu, Huanbo Luan,
Maosong Sun, Siwei Rao, and Song Liu. 2015a. Mod-
eling Relation Paths for Representation Learning of
Knowledge Bases. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 705–714.

[Lin et al.2015b] Yankai Lin, Zhiyuan Liu, Maosong Sun,
Yang Liu, and Xuan Zhu. 2015b. Learning Entity
and Relation Embeddings for Knowledge Graph Com-
In Proceedings of the Twenty-Ninth AAAI
pletion.
Conference on Artiﬁcial Intelligence Learning, pages
2181–2187.

[Liu and Nocedal1989] D. C. Liu and J. Nocedal. 1989.
On the Limited Memory BFGS Method for Large
Scale Optimization. Mathematical Programming,
45(3):503–528.

[Liu et al.2016] Qiao Liu, Liuyi Jiang, Minghao Han, Yao
Liu, and Zhiguang Qin. 2016. Hierarchical Random
Walk Inference in Knowledge Graphs. In Proceedings
of the 39th International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 445–454.

[Mikolov et al.2013] Tomas Mikolov, Wen-tau Yih, and
Geoffrey Zweig.
2013. Linguistic Regularities in
Continuous Space Word Representations. In Proceed-
ings of the 2013 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 746–751.
[Nguyen et al.2016] Dat Quoc Nguyen, Kairit Sirts,
Lizhen Qu, and Mark Johnson. 2016. Neighborhood
Mixture Model for Knowledge Base Completion. In
Proceedings of The 20th SIGNLL Conference on Com-
putational Natural Language Learning, pages 40–50.
[Nickel et al.2011] Maximilian Nickel, Volker Tresp, and
Hans-Peter Kriegel. 2011. A Three-Way Model for
Collective Learning on Multi-Relational Data. In Pro-
ceedings of the 28th International Conference on Ma-
chine Learning, pages 809–816.

[Nickel et al.2016a] Maximilian Nickel, Kevin Murphy,
Volker Tresp, and Evgeniy Gabrilovich. 2016a. A Re-
view of Relational Machine Learning for Knowledge
Graphs. Proceedings of the IEEE, 104(1):11–33.

[Nickel et al.2016b] Maximilian

Lorenzo
Holo-
In
the Thirtieth AAAI Conference on

Rosasco, and Tomaso Poggio.
graphic embeddings of knowledge graphs.
Proceedings of
Artiﬁcial Intelligence, pages 1955–1961.

Nickel,

2016b.

[Riedel et al.2013] Sebastian Riedel, Limin Yao, Andrew
McCallum, and Benjamin M. Marlin. 2013. Rela-
tion Extraction with Matrix Factorization and Univer-
sal Schemas. In Proceedings of the 2013 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 74–84.

[Shi and Weninger2017] Baoxu Shi and Tim Weninger.
2017. ProjE: Embedding Projection for Knowledge
Graph Completion. In Proceedings of the 31st AAAI
Conference on Artiﬁcial Intelligence.

[Socher et al.2013] Richard Socher, Danqi Chen, Christo-
pher D Manning, and Andrew Ng. 2013. Reason-
ing With Neural Tensor Networks for Knowledge Base
Completion. In Advances in Neural Information Pro-
cessing Systems 26, pages 926–934.

[Suchanek et al.2007] Fabian M. Suchanek, Gjergji Kas-
neci, and Gerhard Weikum. 2007. YAGO: A Core
of Semantic Knowledge. In Proceedings of the 16th
International Conference on World Wide Web, pages
697–706.

[Taskar et al.2004] Ben Taskar, Ming fai Wong, Pieter
Abbeel, and Daphne Koller. 2004. Link Prediction in
In Advances in Neural Information
Relational Data.
Processing Systems 16, pages 659–666.

[Tay et al.2017] Yi Tay, Anh Tuan Luu, Siu Cheung Hui,
and Falk Brauer. 2017. Random Semantic Tensor
Ensemble for Scalable Knowledge Graph Link Predic-
tion. In Proceedings of the Tenth ACM International
Conference on Web Search and Data Mining, pages
751–760.

[Toutanova and Chen2015] Kristina Toutanova and Danqi
Chen. 2015. Observed Versus Latent Features for
Knowledge Base and Text Inference. In Proceedings
of the 3rd Workshop on Continuous Vector Space Mod-
els and their Compositionality, pages 57–66.

[Toutanova et al.2015] Kristina Toutanova, Danqi Chen,
Patrick Pantel, Hoifung Poon, Pallavi Choudhury, and
Michael Gamon. 2015. Representing Text for Joint
In Pro-
Embedding of Text and Knowledge Bases.
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1499–
1509.

[Toutanova et al.2016] Kristina Toutanova, Victoria Lin,
Wen-tau Yih, Hoifung Poon, and Chris Quirk. 2016.
Compositional Learning of Embeddings for Relation
Paths in Knowledge Base and Text. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers), pages
1434–1444.

[Niepert2016] Mathias Niepert.

2016. Discriminative
Gaifman Models. In Advances in Neural Information
Processing Systems 29, pages 3405–3413.

[Trouillon et al.2016] Th´eo Trouillon, Johannes Welbl,
´Eric Gaussier, and Guillaume
Sebastian Riedel,
Bouchard. 2016. Complex Embeddings for Simple

Link Prediction. In Proceedings of the 33nd Interna-
tional Conference on Machine Learning, pages 2071–
2080.

[Zeiler2012] Matthew D. Zeiler. 2012. ADADELTA:
CoRR,

An Adaptive Learning Rate Method.
abs/1212.5701.

[Zhao et al.2015] Yu Zhao, Sheng Gao, Patrick Gallinari,
and Jun Guo. 2015. Knowledge Base Completion
by Learning Pairwise-Interaction Differentiated Em-
beddings. Data Mining and Knowledge Discovery,
29(5):1486–1504.

[Wang and Li2016] Zhigang Wang and Juan-Zi Li. 2016.
Text-Enhanced Representation Learning for Knowl-
In Proceedings of the Twenty-Fifth In-
edge Graph.
ternational Joint Conference on Artiﬁcial Intelligence,
pages 1293–1299.

[Wang et al.2014a] Zhen Wang, Jianwen Zhang, Jianlin
Feng, and Zheng Chen. 2014a. Knowledge Graph
In Proceedings of the
and Text Jointly Embedding.
2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1591–1601.
[Wang et al.2014b] Zhen Wang, Jianwen Zhang, Jianlin
Feng, and Zheng Chen. 2014b. Knowledge Graph
In Pro-
Embedding by Translating on Hyperplanes.
ceedings of the Twenty-Eighth AAAI Conference on
Artiﬁcial Intelligence, pages 1112–1119.

[Wang et al.2016] Quan Wang, Jing Liu, Yuanfei Luo,
Bin Wang, and Chin-Yew Lin. 2016. Knowledge Base
In Proceed-
Completion via Coupled Path Ranking.
ings of the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 1308–1318.

[Wei et al.2016] Zhuoyu Wei, Jun Zhao, and Kang Liu.
2016. Mining Inference Formulas by Goal-Directed
Random Walks. In Proceedings of the 2016 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1379–1388.

[West et al.2014] Robert West, Evgeniy Gabrilovich,
Kevin Murphy, Shaohua Sun, Rahul Gupta, and
Dekang Lin. 2014. Knowledge Base Completion via
Search-based Question Answering. In Proceedings of
the 23rd International Conference on World Wide Web,
pages 515–526.

[Xiao et al.2017] Han Xiao, Minlie Huang, and Xiaoyan
Zhu. 2017. SSP: semantic space projection for knowl-
edge graph embedding with text descriptions. In Pro-
ceedings of the 31st AAAI Conference on Artiﬁcial In-
telligence.

[Yang et al.2015] Bishan Yang, Wen-tau Yih, Xiaodong
He, Jianfeng Gao, and Li Deng. 2015. Embedding
Entities and Relations for Learning and Inference in
Knowledge Bases. In Proceedings of the International
Conference on Learning Representations.

[Yoon et al.2016] Hee-Geun Yoon, Hyun-Je Song, Seong-
Bae Park, and Se-Young Park. 2016. A Translation-
Based Knowledge Graph Embedding Preserving Log-
ical Property of Relations. In Proceedings of the 2016
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 907–916.

7
1
0
2
 
r
a

M
 
8
 
 
]
L
C
.
s
c
[
 
 
3
v
0
4
1
8
0
.
6
0
6
1
:
v
i
X
r
a

STransE: a novel embedding model of entities and relationships
in knowledge bases∗

Dat Quoc Nguyen1, Kairit Sirts1, Lizhen Qu2 and Mark Johnson1

1 Department of Computing, Macquarie University, Sydney, Australia
dat.nguyen@students.mq.edu.au, {kairit.sirts, mark.johnson}@mq.edu.au
2 NICTA, ACT 2601, Australia
lizhen.qu@nicta.com.au

Abstract

Knowledge bases of real-world facts about
entities and their relationships are useful re-
sources for a variety of natural language pro-
cessing tasks. However, because knowledge
bases are typically incomplete, it is useful to
be able to perform link prediction or knowl-
edge base completion, i.e., predict whether
a relationship not in the knowledge base is
likely to be true. This paper combines insights
from several previous link prediction models
into a new embedding model STransE that
represents each entity as a low-dimensional
vector, and each relation by two matrices and
a translation vector. STransE is a simple com-
bination of the SE and TransE models, but it
obtains better link prediction performance on
two benchmark datasets than previous embed-
ding models. Thus, STransE can serve as a
new baseline for the more complex models in
the link prediction task.

1

Introduction

Knowledge bases (KBs), such as WordNet (Fell-
baum, 1998), YAGO (Suchanek et al., 2007), Free-
base (Bollacker et al., 2008) and DBpedia (Lehmann
et al., 2015), represent relationships between entities
as triples (head entity, relation, tail entity). Even
very large knowledge bases are still far from com-
plete (Socher et al., 2013; West et al., 2014). Link
prediction or knowledge base completion systems
(Nickel et al., 2016a) predict which triples not in
a knowledge base are likely to be true (Taskar et

∗ A revised version of our NAACL-HLT 2016 paper with

additional experimental results and latest related work.

al., 2004; Bordes et al., 2011). A variety of differ-
ent kinds of information is potentially useful here,
including information extracted from external cor-
pora (Riedel et al., 2013; Wang et al., 2014a) and
the other relationships that hold between the enti-
ties (Angeli and Manning, 2013; Zhao et al., 2015).
For example, Toutanova et al. (2015) used informa-
tion from the external ClueWeb-12 corpus to signif-
icantly enhance performance.

While integrating a wide variety of information
sources can produce excellent results (Das et al.,
2017), there are several reasons for studying sim-
pler models that directly optimize a score function
for the triples in a knowledge base, such as the
one presented here. First, additional information
sources might not be available, e.g., for knowledge
bases for specialized domains. Second, models that
don’t exploit external resources are simpler and thus
typically much faster to train than the more com-
plex models using additional information. Third,
the more complex models that exploit external in-
formation are typically extensions of these simpler
models, and are often initialized with parameters es-
timated by such simpler models, so improvements to
the simpler models should yield corresponding im-
provements to the more complex models as well.

Embedding models for KB completion associate
entities and/or relations with dense feature vectors
or matrices. Such models obtain state-of-the-art per-
formance (Nickel et al., 2011; Bordes et al., 2011;
Bordes et al., 2012; Bordes et al., 2013; Socher et
al., 2013; Wang et al., 2014b; Guu et al., 2015) and
generalize to large KBs (Krompaß et al., 2015). Ta-
ble 1 summarizes a number of prominent embedding

Model

SE

Unstructured

TransE

DISTMULT

NTN

TransH

TransD

TransR

TranSparse

Our STransE

Opt.

Score function fr(h, t)
(cid:107)Wr,1h − Wr,2t(cid:107)(cid:96)1/2 ; Wr,1, Wr,2 ∈ Rk×k
(cid:107)h − t(cid:107)(cid:96)1/2
(cid:107)h + r − t(cid:107)(cid:96)1/2 ; r ∈ Rk
h(cid:62)Wrt ; Wr is a diagonal matrix ∈ Rk×k
r tanh(h(cid:62)Mrt + Wr,1h + Wr,2t + br) ; ur, br ∈ Rd; Mr ∈ Rk×k×d; Wr,1, Wr,2 ∈ Rd×k
u(cid:62)
(cid:107)(I − rpr(cid:62)
(cid:107)(I + rph(cid:62)
(cid:107)Wrh + r − Wrt(cid:107)(cid:96)1/2 ; Wr ∈ Rd×k ; r ∈ Rd
r ∈ Rd×k; θh
(cid:107)Wh
r(θt
(cid:107)Wr,1h + r − Wr,2t(cid:107)(cid:96)1/2 ; Wr,1, Wr,2 ∈ Rk×k; r ∈ Rk

p )h + r − (I − rpr(cid:62)
p )h + r − (I + rpt(cid:62)

r ∈ R ; r ∈ Rd

r)t(cid:107)(cid:96)1/2 ; Wh

r )h + r − Wt

r , Wt

r (θh

r , θt

p )t(cid:107)(cid:96)1/2 ; rp, r ∈ Rk ; I: Identity matrix size k × k
p )t(cid:107)(cid:96)1/2 ; rp, r ∈ Rd ; hp, tp ∈ Rk ; I: Identity matrix size d × k AdaDelta

SGD

AdaGrad

L-BFGS

SGD

SGD

SGD

SGD

SGD

SGD

Table 1: The score functions fr(h, t) and the optimization methods (Opt.) of several prominent embedding models
for KB completion. In all of these the entities h and t are represented by vectors h and t ∈ Rk respectively.

models for KB completion.

Let (h, r, t) represent a triple. In all of the models
discussed here, the head entity h and the tail entity
t are represented by vectors h and t ∈ Rk respec-
tively. The Unstructured model (Bordes et al., 2012)
assumes that h ≈ t. As the Unstructured model
does not take the relationship r into account, it can-
not distinguish different relation types. The Struc-
tured Embedding (SE) model (Bordes et al., 2011)
extends the unstructured model by assuming that h
and t are similar only in a relation-dependent sub-
space. It represents each relation r with two matri-
ces Wr,1 and Wr,2 ∈ Rk×k, which are chosen so
that Wr,1h ≈ Wr,2t. The TransE model (Bordes et
al., 2013) is inspired by models such as Word2Vec
(Mikolov et al., 2013) where relationships between
words often correspond to translations in latent fea-
ture space. The TransE model represents each rela-
tion r by a translation vector r ∈ Rk, which is cho-
sen so that h + r ≈ t.

The primary contribution of this paper is that
two very simple relation-prediction models, SE and
TransE, can be combined into a single model, which
we call STransE.1 Speciﬁcally, we use relation-
speciﬁc matrices Wr,1 and Wr,2 as in the SE model
to identify the relation-dependent aspects of both h
and t, and use a vector r as in the TransE model
to describe the relationship between h and t in this
subspace. Speciﬁcally, our new KB completion
model STransE chooses Wr,1, Wr,2 and r so that

Wr,1h + r ≈ Wr,2t. That is, a TransE-style rela-
tionship holds in some relation-dependent subspace,
and crucially, this subspace may involve very dif-
ferent projections of the head h and tail t. So Wr,1
and Wr,2 can highlight, suppress, or even change the
sign of, relation-speciﬁc attributes of h and t. For
example, for the “purchases” relationship, certain
attributes of individuals h (e.g., age, gender, mari-
tal status) are presumably strongly correlated with
very different attributes of objects t (e.g., sports car,
washing machine and the like).

As we show below, STransE performs better than
the SE and TransE models and other state-of-the-art
link prediction models on two standard link predic-
tion datasets WN18 and FB15k, so it can serve as
a new baseline for KB completion. We expect that
the STransE will also be able to serve as the basis
for extended models that exploit a wider variety of
information sources, just as TransE does.

2 Our approach

Let E denote the set of entities and R the set of re-
lation types. For each triple (h, r, t), where h, t ∈ E
and r ∈ R, the STransE model deﬁnes a score func-
tion fr(h, t) of its implausibility. Our goal is to
choose f such that the score fr(h, t) of a plausi-
ble triple (h, r, t) is smaller than the score fr(cid:48)(h(cid:48), t(cid:48))
of an implausible triple (h(cid:48), r(cid:48), t(cid:48)). We deﬁne the
STransE score function f as follows:

fr(h, t) = (cid:107)Wr,1h + r − Wr,2t(cid:107)(cid:96)1/2

1Source code: https://github.com/datquocnguyen/STransE

using either the (cid:96)1 or the (cid:96)2-norm (the choice is made

using validation data; in our experiments we found
that the (cid:96)1 norm gave slightly better results). To
learn the vectors and matrices we minimize the fol-
lowing margin-based objective function:

L =

(cid:88)

[γ + fr(h, t) − fr(h(cid:48), t(cid:48))]+

(h,r,t)∈G

(h(cid:48),r,t(cid:48))∈G(cid:48)

(h,r,t)

where [x]+ = max(0, x), γ is the margin hyper-
parameter, G is the training set consisting of correct
(h,r,t) = {(h(cid:48), r, t) | h(cid:48) ∈ E, (h(cid:48), r, t) /∈
triples, and G(cid:48)
G} ∪ {(h, r, t(cid:48)) | t(cid:48) ∈ E, (h, r, t(cid:48)) /∈ G} is the set
of incorrect triples generated by corrupting a correct
triple (h, r, t) ∈ G.

We use Stochastic Gradient Descent (SGD) to
minimize L, and impose the following constraints
during training: (cid:107)h(cid:107)2 (cid:54) 1, (cid:107)r(cid:107)2 (cid:54) 1, (cid:107)t(cid:107)2 (cid:54) 1,
(cid:107)Wr,1h(cid:107)2 (cid:54) 1 and (cid:107)Wr,2t(cid:107)2 (cid:54) 1.

3 Related work

Table 1 summarizes related embedding models for
link prediction and KB completion. The models
differ in the score functions fr(h, t) and the algo-
rithms used to optimize the margin-based objective
function, e.g., SGD, AdaGrad (Duchi et al., 2011),
AdaDelta (Zeiler, 2012) and L-BFGS (Liu and No-
cedal, 1989).

DISTMULT (Yang et al., 2015) is based on a
Bilinear model (Nickel et al., 2011; Bordes et al.,
2012; Jenatton et al., 2012) where each relation is
represented by a diagonal rather than a full matrix.
The neural tensor network (NTN) model (Socher et
al., 2013) uses a bilinear tensor operator to represent
each relation while ProjE (Shi and Weninger, 2017)
could be viewed as a simpliﬁed version of NTN
with diagonal matrices. Similar quadratic forms
are used to model entities and relations in KG2E
(He et al., 2015), ComplEx (Trouillon et al., 2016),
TATEC (Garc´ıa-Dur´an et al., 2016) and RSTE (Tay
et al., 2017).
In addition, HolE (Nickel et al.,
2016b) uses circular correlation—a compositional
operator—which could be interpreted as a compres-
sion of the tensor product.

The TransH model (Wang et al., 2014b) asso-
ciates each relation with a relation-speciﬁc hyper-
plane and uses a projection vector to project en-
tity vectors onto that hyperplane. TransD (Ji et al.,

2015) and TransR/CTransR (Lin et al., 2015b) ex-
tend the TransH model using two projection vec-
tors and a matrix to project entity vectors into a
relation-speciﬁc space, respectively. TransD learns
a relation-role speciﬁc mapping just as STransE, but
represents this mapping by projection vectors rather
than full matrices, as in STransE. The lppTransD
model (Yoon et al., 2016) extends TransD to ad-
ditionally use two projection vectors for represent-
ing each relation. In fact, our STransE model and
TranSparse (Ji et al., 2016) can be viewed as direct
extensions of the TransR model, where head and tail
entities are associated with their own projection ma-
trices, rather than using the same matrix for both, as
in TransR and CTransR.

Recently, several authors have shown that relation
paths between entities in KBs provide richer infor-
mation and improve the relationship prediction (Lin
et al., 2015a; Garc´ıa-Dur´an et al., 2015; Guu et al.,
2015; Wang et al., 2016; Feng et al., 2016; Liu et al.,
2016; Niepert, 2016; Wei et al., 2016; Toutanova et
al., 2016; Nguyen et al., 2016). In addition, Nickel
et al. (2016a) reviews other approaches for learning
from KBs and multi-relational data.

4 Experiments

For link prediction evaluation, we conduct experi-
ments and compare the performance of our STransE
model with published results on the benchmark
WN18 and FB15k datasets (Bordes et al., 2013). In-
formation about these datasets is given in Table 2.

#R
18
1,345

#E
40,943
14,951

Dataset
WN18
FB15k

#Train
141,442
483,142

#Valid #Test
5,000
5,000
59,071
50,000
Table 2: Statistics of the experimental datasets used in
this study (and previous works). #E is the number of
entities, #R is the number of relation types, and #Train,
#Valid and #Test are the numbers of triples in the training,
validation and test sets, respectively.

4.1 Task and evaluation protocol

The link prediction task (Bordes et al., 2011; Bordes
et al., 2012; Bordes et al., 2013) predicts the head or
tail entity given the relation type and the other entity,
i.e. predicting h given (?, r, t) or predicting t given
(h, r, ?) where ? denotes the missing element. The

Raw

Filtered

WN18

FB15k

Method

SE (Bordes et al., 2011)
Unstructured (Bordes et al., 2012)
TransE (Bordes et al., 2013)
TransH (Wang et al., 2014b)
TransR (Lin et al., 2015b)
CTransR (Lin et al., 2015b)
KG2E (He et al., 2015)
TransD (Ji et al., 2015)
lppTransD (Yoon et al., 2016)
TranSparse (Ji et al., 2016)
TATEC (Garc´ıa-Dur´an et al., 2016)
NTN (Socher et al., 2013)
DISTMULT (Yang et al., 2015)
HolE (Nickel et al., 2016b)
Our STransE
RTransE (Garc´ıa-Dur´an et al., 2015)
PTransE (Lin et al., 2015a)
GAKE (Feng et al., 2016)
Gaifman (Niepert, 2016)
Hiri (Liu et al., 2016)
NLFeat (Toutanova and Chen, 2015)
TEKE H (Wang and Li, 2016)
SSP (Xiao et al., 2017)

WN18
H10 MRR MR
273
68.5
1074
35.3
243
75.4
212
73.0
198
79.8
199
79.4
174
80.2
194
79.6
195
80.5
187
80.1
-
-
-
-
-
-
-
-
80.9
219
-
-
207
-
228
-
-
-
-
-
-
-
212
80.3
163
81.2
Table 3: Link prediction results. MR, H10 and MRR denote evaluation metrics of mean rank, Hits@10 (in %) and
mean reciprocal rank, respectively. “NLFeat” abbreviates Node+LinkFeat. The results for NTN (Socher et al., 2013)
listed in this table are taken from Yang et al. (2015) since NTN was originally evaluated on different datasets.

FB15k
H10 MRR MR H10 MRR MR H10 MRR
-
28.8
-
4.5
-
34.9
-
45.7
-
48.2
-
48.4
-
48.9
-
53.4
-
53.0
53.5
-
-
-
0.53
-
0.83
-
0.938
-
0.657
51.6
-
-
-
51.4
-
44.5
-
-
0.691
-
0.940
-
-
51.2
57.2
-

-
-
-
-
-
-
-
-
-
-
-
-
-
0.232
0.252
-
-
-
-
-
-
-
-

-
-
-
-
-
-
-
-
-
-
-
0.25
0.35
0.524
0.543
-
-
-
-
0.603
0.822
-
-

-
-
-
-
-
-
-
-
-
-
-
-
-
0.616
0.469
-
-
-
-
-
-
-
-

MR
1011
315
263
401
238
231
342
224
283
223
-
-
-
-
217
-
-
-
-
-
-
127
168

39.8
6.3
47.1
64.4
68.7
70.2
74.0
77.3
78.7
79.5
76.7
41.4
57.7
73.9
79.7
76.2
84.6
64.8
84.2
70.3
87.0
73.0
79.0

80.5
38.2
89.2
86.7
92.0
92.3
92.8
92.2
94.3
93.2
-
66.1
94.2
94.9
93.4
-
-
-
93.9
90.8
94.3
92.9
93.2

162
979
125
87
77
75
59
91
78
82
58
-
-
-
69
50
58
119
75
-
-
108
82

985
304
251
303
225
218
331
212
270
211
-
-
-
-
206
-
-
-
352
-
-
114
156

results are evaluated using the ranking induced by
the score function fr(h, t) on test triples.

For each test triple (h, r, t), we corrupted it by re-
placing either h or t by each of the possible entities
in turn, and then rank these candidates in ascend-
ing order of their implausibility value computed by
the score function. This is called as the “Raw” set-
ting protocol. For the “Filtered” setting protocol de-
scribed in Bordes et al. (2013), we removed any cor-
rupted triples that appear in the knowledge base, to
avoid cases where a correct corrupted triple might
be ranked higher than the test triple. The “Filtered”
setting thus provides a clearer view on the ranking
performance. Following Bordes et al. (2013), we re-
port the mean rank and the Hits@10 (i.e., the pro-
portion of test triples in which the target entity was
ranked in the top 10 predictions) for each model. In
addition, we report the mean reciprocal rank, which
is commonly used in information retrieval. In both
“Raw” and “Filtered” settings, lower mean rank,
higher mean reciprocal rank or higher Hits@10 in-
dicates better link prediction performance.

Following TransR (Lin et al., 2015b), TransD (Ji
et al., 2015), RTransE (Garc´ıa-Dur´an et al., 2015),
PTransE (Lin et al., 2015a), TATEC (Garc´ıa-Dur´an
et al., 2016) and TranSparse (Ji et al., 2016), we used
the entity and relation vectors produced by TransE
(Bordes et al., 2013) to initialize the entity and re-
lation vectors in STransE, and we initialized the re-
lation matrices with identity matrices. We applied
the “Bernoulli” trick used also in previous work for
generating head or tail entities when sampling incor-
rect triples (Wang et al., 2014b; Lin et al., 2015b; He
et al., 2015; Ji et al., 2015; Lin et al., 2015a; Yoon
et al., 2016; Ji et al., 2016). We ran SGD for 2,000
epochs to estimate the model parameters. Following
Bordes et al. (2013) we used a grid search on vali-
dation set to choose either the l1 or l2 norm in the
score function f , as well as to set the SGD learning
rate λ ∈ {0.0001, 0.0005, 0.001, 0.005, 0.01}, the
margin hyper-parameter γ ∈ {1, 3, 5} and the vector
size k ∈ {50, 100}. The lowest ﬁltered mean rank
on the validation set was obtained when using the
l1 norm in f on both WN18 and FB15k, and when

λ = 0.0005, γ = 5, and k = 50 for WN18, and
λ = 0.0001, γ = 1, and k = 100 for FB15k.

4.2 Main results

Table 3 compares the link prediction results of our
STransE model with results reported in prior work,
using the same experimental setup. The ﬁrst 15 rows
report the performance of the models that do not
exploit information about alternative paths between
head and tail entities. The next 5 rows report results
of the models that exploit information about relation
paths. The last 3 rows present results for the models
which make use of textual mentions derived from a
large external corpus.

It is clear that the models with the additional ex-
ternal corpus information obtained best results. In
future work we plan to extend the STransE model
to incorporate such additional information. Table 3
also shows that the models employing path infor-
mation generally achieve better results than mod-
els that do not use such information.
In terms of
models not exploiting path information or exter-
nal information, the STransE model produces the
highest ﬁltered mean rank on WN18 and the high-
est ﬁltered Hits@10 and mean reciprocal rank on
FB15k. Compared to the closely related models SE,
TransE, TransR, CTransR, TransD and TranSparse,
our STransE model does better than these models on
both WN18 and FB15k.

Following Bordes et al. (2013), Table 4 analyzes
Hits@10 results on FB15k with respect to the re-
lation categories deﬁned as follows: for each rela-
tion type r, we computed the averaged number ah of
heads h for a pair (r, t) and the averaged number at
of tails t for a pair (h, r). If ah < 1.5 and at < 1.5,
then r is labeled 1-1. If ah ≥ 1.5 and at < 1.5, then
r is labeled M-1. If ah < 1.5 and at ≥ 1.5, then r is
labeled as 1-M. If ah ≥ 1.5 and at ≥ 1.5, then r is
labeled as M-M. 1.4%, 8.9%, 14.6% and 75.1% of
the test triples belong to a relation type classiﬁed as
1-1, 1-M, M-1 and M-M, respectively.

Table 4 shows that in comparison to prior mod-
els not using path information, STransE obtains the
second highest Hits@10 result for M-M relation cat-
egory at (80.1% + 83.1%)/2 = 81.6% which is
0.5% smaller than the Hits@10 result of TranSparse
for M-M. However, STransE obtains 2.5% higher
Hits@10 result than TranSparse for M-1. In addi-

Predicting head h

Predicting tail t

Method

1.9

6.6

6.1

34.3 4.2

1-1 1-M M-1 M-M 1-1 1-M M-1 M-M
35.6 62.6 17.2 37.5 34.9 14.6 68.3 41.3
SE
34.5 2.5
6.6
Unstr.
43.7 65.7 18.2 47.2 43.7 19.7 66.7 50.0
TransE
TransH
66.8 87.6 28.7 64.5 65.5 39.8 83.3 67.2
78.8 89.2 34.1 69.2 79.2 37.4 90.4 72.1
TransR
81.5 89.0 34.7 71.2 80.8 38.6 90.1 73.8
CTransR
92.3 94.6 66.0 69.6 92.6 67.9 94.4 73.4
KG2E
TATEC
79.3 93.2 42.3 77.2 78.5 51.5 92.7 80.7
86.1 95.5 39.8 78.5 85.4 50.6 94.4 81.2
TransD
lppTransD 86.0 94.2 54.4 82.2 79.7 43.2 95.3 79.7
TranSparse 86.8 95.5 44.3 80.9 86.6 56.6 94.4 83.3
82.8 94.2 50.4 80.1 82.4 56.9 93.4 83.1
STransE

Table 4: Hits@10 (in %) by the relation category on
FB15k. “Unstr.” abbreviates Unstructured.

tion, STransE also performs better than TransD for
1-M and M-1 relation categories. We believe the
improved performance of the STransE model is due
to its use of full matrices, rather than just projection
vectors as in TransD. This permits STransE to model
diverse and complex relation categories (such as 1-
M, M-1 and especially M-M) better than TransD
and other similiar models. However, STransE is not
as good as TransD for the 1-1 relations. Perhaps the
extra parameters in STransE hurt performance in this
case (note that 1-1 relations are relatively rare, so
STransE does better overall).

5 Conclusion and future work

This paper presented a new embedding model for
link prediction and KB completion. Our STransE
combines insights from several simpler embed-
ding models, speciﬁcally the Structured Embedding
model (Bordes et al., 2011) and the TransE model
(Bordes et al., 2013), by using a low-dimensional
vector and two projection matrices to represent each
relation. STransE, while being conceptually sim-
ple, produces highly competitive results on standard
link prediction evaluations, and scores better than
the embedding-based models it builds on. Thus it
is a suitable candidate for serving as future baseline
for more complex models in the link prediction task.
In future work we plan to extend STransE to ex-
ploit relation path information in knowledge bases,
in a manner similar to Lin et al. (2015a), Guu et al.
(2015) or Nguyen et al. (2016).

Acknowledgments

This research was supported by a Google award
through the Natural Language Understanding Fo-
the Australian Re-
cused Program, and under
search Council’s Discovery Projects funding scheme
(project number DP160102156).

NICTA is funded by the Australian Government
through the Department of Communications and the
Australian Research Council through the ICT Centre
of Excellence Program. The ﬁrst author is supported
by an International Postgraduate Research Scholar-
ship and a NICTA NRPA Top-Up Scholarship.

References

[Angeli and Manning2013] Gabor Angeli and Christo-
pher Manning. 2013. Philosophers are Mortal: In-
ferring the Truth of Unseen Facts. In Proceedings of
the Seventeenth Conference on Computational Natural
Language Learning, pages 133–142.

[Bollacker et al.2008] Kurt Bollacker, Colin Evans,
Praveen Paritosh, Tim Sturge, and Jamie Taylor.
2008. Freebase: A Collaboratively Created Graph
Database for Structuring Human Knowledge.
In
Proceedings of
the 2008 ACM SIGMOD Interna-
tional Conference on Management of Data, pages
1247–1250.

[Bordes et al.2011] Antoine Bordes, Jason Weston, Ro-
nan Collobert, and Yoshua Bengio. 2011. Learning
Structured Embeddings of Knowledge Bases. In Pro-
ceedings of the Twenty-Fifth AAAI Conference on Ar-
tiﬁcial Intelligence, pages 301–306.

[Bordes et al.2012] Antoine Bordes, Xavier Glorot, Ja-
son Weston, and Yoshua Bengio. 2012. A Semantic
Matching Energy Function for Learning with Multi-
relational Data. Machine Learning, 94(2):233–259.
[Bordes et al.2013] Antoine Bordes, Nicolas Usunier,
Alberto Garcia-Duran, Jason Weston, and Oksana
Yakhnenko. 2013. Translating Embeddings for Mod-
eling Multi-relational Data. In Advances in Neural In-
formation Processing Systems 26, pages 2787–2795.
[Das et al.2017] Rajarshi Das, Arvind Neelakantan,
David Belanger, and Andrew McCallum.
2017.
Chains of reasoning over entities, relations, and text
In Proceedings of
using recurrent neural networks.
the 15th Conference of the European Chapter of the
Association for Computational Linguistics.

[Duchi et al.2011] John Duchi, Elad Hazan, and Yoram
Singer. 2011. Adaptive Subgradient Methods for On-
line Learning and Stochastic Optimization. The Jour-
nal of Machine Learning Research, 12:2121–2159.

[Fellbaum1998] Christiane D. Fellbaum. 1998. WordNet:

An Electronic Lexical Database. MIT Press.

[Feng et al.2016] Jun Feng, Minlie Huang, Yang Yang,
and xiaoyan zhu. 2016. GAKE: Graph Aware Knowl-
In Proceedings of COLING 2016,
edge Embedding.
the 26th International Conference on Computational
Linguistics: Technical Papers, pages 641–651.

[Garc´ıa-Dur´an et al.2015] Alberto Garc´ıa-Dur´an, An-
toine Bordes, and Nicolas Usunier. 2015. Composing
Relationships with Translations. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, pages 286–290.

[Garc´ıa-Dur´an et al.2016] Alberto Garc´ıa-Dur´an, An-
toine Bordes, Nicolas Usunier, and Yves Grandvalet.
2016.
Combining Two and Three-Way Embed-
ding Models for Link Prediction in Knowledge
Journal of Artiﬁcial Intelligence Research,
Bases.
55:715–742.

[Guu et al.2015] Kelvin Guu, John Miller, and Percy
Liang. 2015. Traversing Knowledge Graphs in Vec-
tor Space. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing,
pages 318–327.

[He et al.2015] Shizhu He, Kang Liu, Guoliang Ji, and
Jun Zhao. 2015. Learning to Represent Knowledge
Graphs with Gaussian Embedding. In Proceedings of
the 24th ACM International on Conference on Infor-
mation and Knowledge Management, pages 623–632.
Jenatton, Nicolas L.
Roux, Antoine Bordes, and Guillaume R Obozinski.
2012. A latent factor model for highly multi-relational
data. In Advances in Neural Information Processing
Systems 25, pages 3167–3175.

[Jenatton et al.2012] Rodolphe

[Ji et al.2015] Guoliang Ji, Shizhu He, Liheng Xu, Kang
Liu, and Jun Zhao. 2015. Knowledge Graph Embed-
ding via Dynamic Mapping Matrix. In Proceedings of
the 53rd Annual Meeting of the Association for Com-
putational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Volume
1: Long Papers), pages 687–696.

[Ji et al.2016] Guoliang Ji, Kang Liu, Shizhu He, and Jun
2016. Knowledge Graph Completion with
Zhao.
In Proceedings of
Adaptive Sparse Transfer Matrix.
the Thirtieth AAAI Conference on Artiﬁcial Intelli-
gence, pages 985–991.

[Krompaß et al.2015] Denis Krompaß, Stephan Baier,
and Volker Tresp. 2015. Type-Constrained Represen-
tation Learning in Knowledge Graphs. In Proceedings
of the 14th International Semantic Web Conference,
pages 640–655.

[Lehmann et al.2015] Jens Lehmann, Robert Isele, Max
Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N.
Mendes, Sebastian Hellmann, Mohamed Morsey,

Patrick van Kleef, S¨oren Auer, and Christian Bizer.
2015. DBpedia - A Large-scale, Multilingual Knowl-
edge Base Extracted from Wikipedia. Semantic Web,
6(2):167–195.

[Lin et al.2015a] Yankai Lin, Zhiyuan Liu, Huanbo Luan,
Maosong Sun, Siwei Rao, and Song Liu. 2015a. Mod-
eling Relation Paths for Representation Learning of
Knowledge Bases. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 705–714.

[Lin et al.2015b] Yankai Lin, Zhiyuan Liu, Maosong Sun,
Yang Liu, and Xuan Zhu. 2015b. Learning Entity
and Relation Embeddings for Knowledge Graph Com-
In Proceedings of the Twenty-Ninth AAAI
pletion.
Conference on Artiﬁcial Intelligence Learning, pages
2181–2187.

[Liu and Nocedal1989] D. C. Liu and J. Nocedal. 1989.
On the Limited Memory BFGS Method for Large
Scale Optimization. Mathematical Programming,
45(3):503–528.

[Liu et al.2016] Qiao Liu, Liuyi Jiang, Minghao Han, Yao
Liu, and Zhiguang Qin. 2016. Hierarchical Random
Walk Inference in Knowledge Graphs. In Proceedings
of the 39th International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 445–454.

[Mikolov et al.2013] Tomas Mikolov, Wen-tau Yih, and
Geoffrey Zweig.
2013. Linguistic Regularities in
Continuous Space Word Representations. In Proceed-
ings of the 2013 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 746–751.
[Nguyen et al.2016] Dat Quoc Nguyen, Kairit Sirts,
Lizhen Qu, and Mark Johnson. 2016. Neighborhood
Mixture Model for Knowledge Base Completion. In
Proceedings of The 20th SIGNLL Conference on Com-
putational Natural Language Learning, pages 40–50.
[Nickel et al.2011] Maximilian Nickel, Volker Tresp, and
Hans-Peter Kriegel. 2011. A Three-Way Model for
Collective Learning on Multi-Relational Data. In Pro-
ceedings of the 28th International Conference on Ma-
chine Learning, pages 809–816.

[Nickel et al.2016a] Maximilian Nickel, Kevin Murphy,
Volker Tresp, and Evgeniy Gabrilovich. 2016a. A Re-
view of Relational Machine Learning for Knowledge
Graphs. Proceedings of the IEEE, 104(1):11–33.

[Nickel et al.2016b] Maximilian

Lorenzo
Holo-
In
the Thirtieth AAAI Conference on

Rosasco, and Tomaso Poggio.
graphic embeddings of knowledge graphs.
Proceedings of
Artiﬁcial Intelligence, pages 1955–1961.

Nickel,

2016b.

[Riedel et al.2013] Sebastian Riedel, Limin Yao, Andrew
McCallum, and Benjamin M. Marlin. 2013. Rela-
tion Extraction with Matrix Factorization and Univer-
sal Schemas. In Proceedings of the 2013 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 74–84.

[Shi and Weninger2017] Baoxu Shi and Tim Weninger.
2017. ProjE: Embedding Projection for Knowledge
Graph Completion. In Proceedings of the 31st AAAI
Conference on Artiﬁcial Intelligence.

[Socher et al.2013] Richard Socher, Danqi Chen, Christo-
pher D Manning, and Andrew Ng. 2013. Reason-
ing With Neural Tensor Networks for Knowledge Base
Completion. In Advances in Neural Information Pro-
cessing Systems 26, pages 926–934.

[Suchanek et al.2007] Fabian M. Suchanek, Gjergji Kas-
neci, and Gerhard Weikum. 2007. YAGO: A Core
of Semantic Knowledge. In Proceedings of the 16th
International Conference on World Wide Web, pages
697–706.

[Taskar et al.2004] Ben Taskar, Ming fai Wong, Pieter
Abbeel, and Daphne Koller. 2004. Link Prediction in
In Advances in Neural Information
Relational Data.
Processing Systems 16, pages 659–666.

[Tay et al.2017] Yi Tay, Anh Tuan Luu, Siu Cheung Hui,
and Falk Brauer. 2017. Random Semantic Tensor
Ensemble for Scalable Knowledge Graph Link Predic-
tion. In Proceedings of the Tenth ACM International
Conference on Web Search and Data Mining, pages
751–760.

[Toutanova and Chen2015] Kristina Toutanova and Danqi
Chen. 2015. Observed Versus Latent Features for
Knowledge Base and Text Inference. In Proceedings
of the 3rd Workshop on Continuous Vector Space Mod-
els and their Compositionality, pages 57–66.

[Toutanova et al.2015] Kristina Toutanova, Danqi Chen,
Patrick Pantel, Hoifung Poon, Pallavi Choudhury, and
Michael Gamon. 2015. Representing Text for Joint
In Pro-
Embedding of Text and Knowledge Bases.
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1499–
1509.

[Toutanova et al.2016] Kristina Toutanova, Victoria Lin,
Wen-tau Yih, Hoifung Poon, and Chris Quirk. 2016.
Compositional Learning of Embeddings for Relation
Paths in Knowledge Base and Text. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers), pages
1434–1444.

[Niepert2016] Mathias Niepert.

2016. Discriminative
Gaifman Models. In Advances in Neural Information
Processing Systems 29, pages 3405–3413.

[Trouillon et al.2016] Th´eo Trouillon, Johannes Welbl,
´Eric Gaussier, and Guillaume
Sebastian Riedel,
Bouchard. 2016. Complex Embeddings for Simple

Link Prediction. In Proceedings of the 33nd Interna-
tional Conference on Machine Learning, pages 2071–
2080.

[Zeiler2012] Matthew D. Zeiler. 2012. ADADELTA:
CoRR,

An Adaptive Learning Rate Method.
abs/1212.5701.

[Zhao et al.2015] Yu Zhao, Sheng Gao, Patrick Gallinari,
and Jun Guo. 2015. Knowledge Base Completion
by Learning Pairwise-Interaction Differentiated Em-
beddings. Data Mining and Knowledge Discovery,
29(5):1486–1504.

[Wang and Li2016] Zhigang Wang and Juan-Zi Li. 2016.
Text-Enhanced Representation Learning for Knowl-
In Proceedings of the Twenty-Fifth In-
edge Graph.
ternational Joint Conference on Artiﬁcial Intelligence,
pages 1293–1299.

[Wang et al.2014a] Zhen Wang, Jianwen Zhang, Jianlin
Feng, and Zheng Chen. 2014a. Knowledge Graph
In Proceedings of the
and Text Jointly Embedding.
2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1591–1601.
[Wang et al.2014b] Zhen Wang, Jianwen Zhang, Jianlin
Feng, and Zheng Chen. 2014b. Knowledge Graph
In Pro-
Embedding by Translating on Hyperplanes.
ceedings of the Twenty-Eighth AAAI Conference on
Artiﬁcial Intelligence, pages 1112–1119.

[Wang et al.2016] Quan Wang, Jing Liu, Yuanfei Luo,
Bin Wang, and Chin-Yew Lin. 2016. Knowledge Base
In Proceed-
Completion via Coupled Path Ranking.
ings of the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 1308–1318.

[Wei et al.2016] Zhuoyu Wei, Jun Zhao, and Kang Liu.
2016. Mining Inference Formulas by Goal-Directed
Random Walks. In Proceedings of the 2016 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1379–1388.

[West et al.2014] Robert West, Evgeniy Gabrilovich,
Kevin Murphy, Shaohua Sun, Rahul Gupta, and
Dekang Lin. 2014. Knowledge Base Completion via
Search-based Question Answering. In Proceedings of
the 23rd International Conference on World Wide Web,
pages 515–526.

[Xiao et al.2017] Han Xiao, Minlie Huang, and Xiaoyan
Zhu. 2017. SSP: semantic space projection for knowl-
edge graph embedding with text descriptions. In Pro-
ceedings of the 31st AAAI Conference on Artiﬁcial In-
telligence.

[Yang et al.2015] Bishan Yang, Wen-tau Yih, Xiaodong
He, Jianfeng Gao, and Li Deng. 2015. Embedding
Entities and Relations for Learning and Inference in
Knowledge Bases. In Proceedings of the International
Conference on Learning Representations.

[Yoon et al.2016] Hee-Geun Yoon, Hyun-Je Song, Seong-
Bae Park, and Se-Young Park. 2016. A Translation-
Based Knowledge Graph Embedding Preserving Log-
ical Property of Relations. In Proceedings of the 2016
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 907–916.

