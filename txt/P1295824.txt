7
1
0
2
 
y
a
M
 
4
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
6
8
3
2
0
.
1
0
7
1
:
v
i
X
r
a

AdaGAN: Boosting Generative Models

Ilya Tolstikhin1, Sylvain Gelly2, Olivier Bousquet2, Carl-Johann Simon-Gabriel1, and
Bernhard Sch¨olkopf1

1Max Planck Institute for Intelligent Systems
2Google Brain

Abstract

Generative Adversarial Networks (GAN) [1] are an eﬀective method for training generative models of
complex data such as natural images. However, they are notoriously hard to train and can suﬀer from
the problem of missing modes where the model is not able to produce examples in certain regions of the
space. We propose an iterative procedure, called AdaGAN, where at every step we add a new component
into a mixture model by running a GAN algorithm on a reweighted sample. This is inspired by boosting
algorithms, where many potentially weak individual predictors are greedily aggregated to form a strong
composite predictor. We prove that such an incremental procedure leads to convergence to the true
distribution in a ﬁnite number of steps if each step is optimal, and convergence at an exponential rate
otherwise. We also illustrate experimentally that this procedure addresses the problem of missing modes.

1 Introduction

Imagine we have a large corpus, containing unlabeled pictures of animals, and our task is to build a generative
probabilistic model of the data. We run a recently proposed algorithm and end up with a model which
produces impressive pictures of cats and dogs, but not a single giraﬀe. A natural way to ﬁx this would be
to manually remove all cats and dogs from the training set and run the algorithm on the updated corpus.
The algorithm would then have no choice but to produce new animals and, by iterating this process until
there’s only giraﬀes left in the training set, we would arrive at a model generating giraﬀes (assuming suﬃcient
sample size). At the end, we aggregate the models obtained by building a mixture model. Unfortunately, the
described meta-algorithm requires manual work for removing certain pictures from the unlabeled training
set at every iteration.

Let us turn this into an automatic approach, and rather than including or excluding a picture, put
continuous weights on them. To this end, we train a binary classiﬁer to separate “true” pictures of the
original corpus from the set of “synthetic” pictures generated by the mixture of all the models trained so
far. We would expect the classiﬁer to make conﬁdent predictions for the true pictures of animals missed
by the model (giraﬀes), because there are no synthetic pictures nearby to be confused with them. By
a similar argument, the classiﬁer should make less conﬁdent predictions for the true pictures containing
animals already generated by one of the trained models (cats and dogs). For each picture in the corpus,
we can thus use the classiﬁer’s conﬁdence to compute a weight which we use for that picture in the next
iteration, to be performed on the re-weighted dataset.

The present work provides a principled way to perform this re-weighting, with theoretical guarantees

showing that the resulting mixture models indeed approach the true data distribution.1

Before discussing how to build the mixture, let us consider the question of building a single generative
model. A recent trend in modelling high dimensional data such as natural images is to use neural net-
works [2, 1]. One popular approach are Generative Adversarial Networks (GAN) [1], where the generator

1Note that the term “mixture” should not be interpreted to imply that each component models only one mode: the models

to be combined into a mixture can themselves cover multiple modes.

1

is trained adversarially against a classiﬁer, which tries to diﬀerentiate the true from the generated data.
While the original GAN algorithm often produces realistically looking data, several issues were reported in
the literature, among which the missing modes problem, where the generator converges to only one or a few
modes of the data distribution, thus not providing enough variability in the generated data. This seems
to match the situation described earlier, which is why we will most often illustrate our algorithm with a
GAN as the underlying base generator. We call it AdaGAN, for Adaptive GAN, but we could actually use
any other generator: a Gaussian mixture model, a VAE [2], a WGAN [3], or even an unrolled [4] or mode-
regularized GAN [5], which were both already speciﬁcally developed to tackle the missing mode problem.
Thus, we do not aim at improving the original GAN or any other generative algorithm. We rather propose
and analyse a meta-algorithm that can be used on top of any of them. This meta-algorithm is similar in
spirit to AdaBoost [6] in the sense that each iteration corresponds to learning a “weak” generative model
(e.g., GAN) with respect to a re-weighted data distribution. The weights change over time to focus on the
“hard” examples, i.e. those that the mixture has not been able to properly generate so far.

1.1 Boosting via Additive Mixtures

Motivated by the problem of missing modes, in this work we propose to use multiple generative models
combined into a mixture. These generative models are trained iteratively by adding, at each step, another
model to the mixture that should hopefully cover the areas of the space not covered by the previous mixture
components.2 We show analytically that the optimal next mixture component can be obtained by reweighting
the true data, and thus propose to use the reweighted data distribution as the target for the optimization
of the next mixture components. This leads us naturally to a meta-algorithm, which is similar in spirit to
AdaBoost in the sense that each iteration corresponds to learning a “weak” generative model (e.g., GAN)
with respect to a reweighted data distribution. The latter adapts over time to focus on the “hard” examples,
i.e. those that the mixture has not been able to properly generate thus far.

Before diving into the technical details we provide an informal intuitive discussion of our new meta-
algorithm, which we call AdaGAN (a shorthand for Adaptive GAN, similar to AdaBoost). The pseudocode
is presented in Algorithm 1.

On the ﬁrst step we run the GAN algorithm (or some other generative model) in the usual way and
initialize our generative model with the resulting generator G1. On every t-th step we (a) pick the mixture
weight βt for the next component, (b) update weights Wt of examples from the training set in such a way to
bias the next component towards “hard” ones, not covered by the current mixture of generators Gt−1, (c) run
the GAN algorithm, this time importance sampling mini-batches according to the updated weights Wt,
t , and ﬁnally (d) update our mixture of generators Gt = (1 − βt)Gt−1 + βtGc
resulting in a new generator Gc
t
(notation expressing the mixture of Gt−1 and Gc
t with probabilities 1 − βt and βt). This procedure outputs T
generator functions Gc
T and T corresponding non-negative weights α1, . . . , αT , which sum to one. For
sampling from the resulting model we ﬁrst deﬁne a generator Gc
i , by sampling the index i from a multinomial
distribution with parameters α1, . . . , αT , and then we return Gc
i (Z), where Z ∼ PZ is a standard latent noise
variable used in the GAN literature.

1, . . . , Gc

The eﬀect of the described procedure is illustrated in a toy example in Figure 1. On the left images,
the red dots are the training (true data) points, the blue dots are points sampled from the model mixture
of generators Gt. The background colour gives the density of the distribution corresponding to Gt, non
zero around the generated points, (almost) zero everywhere else. On the right images, the color corresponds
to the weights of training points, following the reweighting scheme proposed in this work. The top row
corresponds to the ﬁrst iteration of AdaGAN, and the bottom row to the second iteration. After the ﬁrst
iteration (the result of the vanilla GAN), we see that only the top left mode is covered, while the three
other modes are not covered at all. The new weights (top right) show that the examples from covered mode
are aggressively downweighted. After the second iteration (bottom left), the combined generator can then
generate two modes.

2Note that the term “mixture” should not be interpreted to imply that each component models only one mode: the models

to be combined into a mixture can themselves cover multiple modes already.

2

Algorithm 1: AdaGAN, a meta-algorithm to construct a “strong” mixture of T individual GANs,
trained sequentially. The mixture weight schedule ChooseMixtureWeight and the training set reweight-
ing schedule UpdateTrainingWeights should be provided by the user. Section 3 gives a complete instance
of this family.

Input: Training sample SN := {X1, . . . , XN }.

Output: Mixture generative model G = GT .

Train vanilla GAN:

W1 = (1/N, . . . , 1/N )

G1 = GAN(SN , Wt)

for t = 2, . . . , T do

#Choose a mixture weight for the next component

βt = ChooseMixtureWeight(t)

#Update weights of training examples

Wt = UpdateTrainingWeights(Gt−1, SN , βt)
#Train t-th “weak” component generator Gc
t
Gc

t = GAN(SN , Wt)

#Update the overall generative model
#Notation below means forming a mixture of Gt−1 and Gc
t .
Gt = (1 − βt)Gt−1 + βtGc
t

end for

3

Although motivated by GANs, we cast our results in the general framework of the minimization of an
f -divergence (cf. [7]) with respect to an additive mixture of distributions. We also note that our approach
may be combined with diﬀerent “weak” generative models, including but not limited to GAN.

Figure 1: A toy illustration of the missing mode problem and the eﬀect of sample reweighting, following the
discussion in Section 1.1. On the left images, the red dots are the training (true data) points, the blue dots
are points sampled from the model mixture of generators Gt. On the right images, the color corresponds
to the weights of training points, following the reweighting scheme proposed in this work. The top row
corresponds to the ﬁrst iteration of AdaGAN, and the bottom row to the second iteration.

1.2 Related Work

Several authors [8, 9, 10] have proposed to use boosting techniques in the context of density estimation by
incrementally adding components in the log domain. In particular, the work of Grover and Ermon [10], done
in parallel to and independent of ours, is applying this idea to GANs. A major downside of these approaches
is that the resulting mixture is a product of components and sampling from such a model is nontrivial (at
least when applied to GANs where the model density is not expressed analytically) and requires to use
techniques such as Annealed Importance Sampling [11] for the normalization.

Rosset and Segal [12] proposed to use an additive mixture model in the case where the log likelihood
can be computed. They derived the update rule via computing the steepest descent direction when adding
a component with inﬁnitesimal weight. This leads to an update rule which is degenerate if the generative
model can produce arbitrarily concentrated distributions (indeed the optimal component is just a Dirac
distribution) which is thus not suitable for the GAN setting. Moreover, their results do not apply once the

4

weight β becomes non-inﬁnitesimal. In contrast, for any ﬁxed weight of the new component our approach
gives the overall optimal update (rather than just the best direction), and applies to any f -divergence.
Remarkably, in both theories, improvements of the mixture are guaranteed only if the new “weak” learner
is still good enough (see Conditions 14&15)

Similarly, Barron and Li [13] studied the construction of mixtures minimizing the Kullback divergence
and proposed a greedy procedure for doing so. They also proved that under certain conditions, ﬁnite mixtures
can approximate arbitrary mixtures at a rate 1/k where k is the number of components in the mixture when
the weight of each newly added component is 1/k. These results are speciﬁc to the Kullback divergence but
are consistent with our more general results.

Wang et al. [14] propose an additive procedure similar to ours but with a diﬀerent reweighting scheme,
which is not motivated by a theoretical analysis of optimality conditions. On every new iteration the authors
propose to run GAN on the top k training examples with maximum value of the discriminator from the last
iteration. Empirical results of Section 4 show that this heuristic often fails to address the missing modes
problem.

Finally, many papers investigate completely diﬀerent approaches for addressing the same issue by directly
modifying the training objective of an individual GAN. For instance, Che et al. [5] add an autoencoding cost
to the training objective of GAN, while Metz et al. [4] allow the generator to “look few steps ahead” when
making a gradient step.

The paper is organized as follows. In Section 2 we present our main theoretical results regarding opti-
mization of mixture models under general f -divergences. In particular we show that it is possible to build
an optimal mixture in an incremental fashion, where each additional component is obtained by applying a
GAN-style procedure with a reweighted distribution. In Section 2.5 we show that if the GAN optimization
at each step is perfect, the process converges to the true data distribution at exponential rate (or even
in a ﬁnite number of steps, for which we provide a necessary and suﬃcient condition). Then we show in
Section 2.6 that imperfect GAN solutions still lead to the exponential rate of convergence under certain
“weak learnability” conditions. These results naturally lead us to a new boosting-style iterative procedure
for constructing generative models, which is combined with GAN in Section 3, resulting in a new algorithm
called AdaGAN. Finally, we report initial empirical results in Section 4, where we compare AdaGAN with
several benchmarks, including original GAN, uniform mixture of multiple independently trained GANs, and
iterative procedure of Wang et al. [14].

2 Minimizing f -divergence with Additive Mixtures

In this section we derive a general result on the minimization of f -divergences over mixture models.

2.1 Preliminaries and notations

In this work we will write Pd and Pmodel to denote a real data distribution and our approximate model
distribution, respectively, both deﬁned over the data space X .

Generative Density Estimation In the generative approach to density estimation, instead of building a
probabilistic model of the data directly, one builds a function G : Z → X that transforms a ﬁxed probability
distribution PZ (often called the noise distribution) over a latent space Z into a distribution over X . Hence
Pmodel is the pushforward of PZ, i.e. Pmodel(A) = PZ(G−1(A)). Because of this deﬁnition, it is generally
impossible to compute the density dPmodel(x), hence it is not possible to compute the log-likelihood of the
training data under the model. However, if PZ is a distribution from which one can sample, it is easy to also
sample from Pmodel (simply sampling from PZ and applying G to each example gives a sample from Pmodel).
So the problem of generative density estimation becomes a problem of ﬁnding a function G such that
Pmodel looks like Pd in the sense that samples from Pmodel and from Pd look similar. Another way to state
this problem is to say that we are given a measure of similarity between distributions D(Pmodel(cid:107)Pd) which

5

can be estimated from samples of those distributions, and thus approximately minimized over a class G of
functions.

f -Divergences
of the data we will use an f -divergence deﬁned in the following way:

In order to measure the agreement between the model distribution and the true distribution

Df (Q(cid:107)P ) :=

(cid:90)

(cid:18) dQ
dP

f

(cid:19)

(x)

dP (x)

(1)

(2)

(3)

for any pair of distributions P, Q with densities dP , dQ with respect to some dominating reference measure µ.
In this work we assume that the function f is convex, deﬁned on (0, ∞), and satisﬁes f (1) = 0. The deﬁnition
of Df holds for both continuous and discrete probability measures and does not depend on speciﬁc choice
of µ.3 It is easy to verify that Df ≥ 0 and it is equal to 0 when P = Q. Note that Df is not symmetric,
but Df (P (cid:107)Q) = Df ◦ (Q(cid:107)P ) for f ◦(x) := xf (1/x) and any P and Q. The f -divergence is symmetric when
f (x) = f ◦(x) for all x ∈ (0, ∞), as in this case Df (P, Q) = Df (Q, P ).

We also note that the divergences corresponding to f (x) and f (x) + C · (x − 1) are identical for any
constant C. In some cases, it is thus convenient to work with f0(x) := f (x) − (x − 1)f (cid:48)(1), (where f (cid:48)(1) is
any subderivative of f at 1) as Df (Q(cid:107)P ) = Df0(Q(cid:107)P ) for all Q and P , while f0 is nonnegative, nonincreasing
on (0, 1], and nondecreasing on (1, ∞). In the remainder, we will denote by F the set of functions that are
suitable for f -divergences, i.e. the set of functions of the form f0 for any convex f with f (1) = 0.

Classical examples of f -divergences include the Kullback-Leibler divergence (obtained for f (x) = − log x,
f0(x) = − log x + x − 1), the reverse Kullback-Leibler divergence (obtained for f (x) = x log x, f0(x) =
x log x − x + 1), the Total Variation distance (f (x) = f0(x) = |x − 1|), and the Jensen-Shannon divergence
(f (x) = f0(x) = −(x + 1) log x+1
2 + x log x). More details can be found in Appendix D. Other examples can
be found in [7]. For further details on f -divergences we refer to Section 1.3 of [15] and [16].

GAN and f -divergences We now explain the connection between the GAN algorithm and f -divergences.
The original GAN algorithm [1] consists in optimizing the following criterion:

min
G

max
D

EPd [log D(X)] + EPZ [log(1 − D(G(Z)))] ,

where D and G are two functions represented by neural networks, and this optimization is actually performed
on a pair of samples (one being the training sample, the other one being created from the chosen distribu-
tion PZ), which corresponds to approximating the above criterion by using the empirical distributions. For
a ﬁxed G, it has been shown in [1] that the optimal D for (2) is given by D∗(x) =
dPd(x)+dPg(x) and plugging
this optimal value into (2) gives the following:

dPd(x)

min
G

− log(4) + 2JS(Pd (cid:107) Pg) ,

where JS is the Jensen-Shannon divergence. Of course, the actual GAN algorithm uses an approximation
to D∗ which is computed by training a neural network on a sample, which means that the GAN algorithm
can be considered to minimize an approximation of (3)4. This point of view can be generalized by plugging
another f -divergence into (3), and it turns out that other f -divergences can be written as the solution to
a maximization of a criterion similar to (2). Indeed, as demonstrated in [7], any f -divergence between Pd
and Pg can be seen as the optimal value of a quantity of the form EPd [f1(D(X))] + EPg [f2(D(G(Z)))] for
appropriate f1 and f2, and thus can be optimized by the same adversarial training technique.

There is thus a strong connection between adversarial training of generative models and minimization of

f -divergences, and this is why we cast the results of this section in the context of general f -divergences.

3The integral in (1) is well deﬁned (but may take inﬁnite values) even if P (dQ = 0) > 0 or Q(dP = 0) > 0. In this case the
integral is understood as Df (Q(cid:107)P ) = (cid:82) f (dQ/dP )1[dP (x)>0,dQ(x)>0]dP (x) + f (0)P (dQ = 0) + f ◦(0)Q(dP = 0), where both
f (0) and f ◦(0) may take value ∞ [15]. This is especially important in case of GAN, where it is impossible to constrain Pmodel
to be absolutely continuous with respect to Pd or vice versa.

4Actually the criterion that is minimized is an empirical version of a lower bound of the Jensen-Shannon divergence.

6

Hilbertian Metrics As demonstrated in [17, 18], several commonly used symmetric f -divergences are
Hilbertian metrics, which in particular means that their square root satisﬁes the triangle inequality. This
is true for the Jensen-Shannon divergence5 as well as for the Hellinger distance and the Total Variation
among others. We will denote by FH the set of f functions such that Df is a Hilbertian metric. For those
divergences, we have Df (P (cid:107)Q) ≤ ((cid:112)Df (P (cid:107)R) + (cid:112)Df (R(cid:107)Q))2.

Generative Mixture Models
a mixture model of the following form:

In order to model complex data distributions, it can be convenient to use

P T

model :=

αiPi,

T
(cid:88)

i=1

where αi ≥ 0, (cid:80)
i αi = 1, and each of the T components is a generative density model. This is very natural
in the generative context, since sampling from a mixture corresponds to a two-step sampling, where one ﬁrst
picks the mixture component (according to the multinomial distribution whose parameters are the αi) and
then samples from it. Also, this allows to construct complex models from simpler ones.

2.2 Incremental Mixture Building

As discussed earlier, in the context of generative modeling, we are given a measure of similarity between
distributions. We will restrict ourselves to the case of f -divergences.
Indeed, for any f -divergence, it
is possible (as explained for example in [7]) to estimate Df (Q (cid:107) P ) from two samples (one from Q, one
from P ) by training a “discriminator” function, i.e. by solving an optimization problem (which is a binary
classiﬁcation problem in the case where the divergence is symmetric6).
It turns out that the empirical
estimate ˆD of Df (Q (cid:107) P ) thus obtained provides a criterion for optimizing Q itself. Indeed, ˆD is a function
of Y1, . . . , Yn ∼ Q and X1, . . . , Xn ∼ P , where Yi = G(Zi) for some mapping function G. Hence it is possible
to optimize ˆD with respect to G (and in particular compute gradients with respect to the parameters of G
if G comes from a smoothly parametrized model such as a neural network).

In this work we thus assume that, given an i.i.d. sample from any unknown distribution P we can construct

a simple model Q ∈ G which approximately minimizes

min
Q∈G

Df (Q (cid:107) P ).

Instead of just modelling the data with a single distribution, we now want to model it with a mixture
of the form (4) where each Pi is obtained by a training procedure of the form (5) with (possibly) diﬀerent
target distributions P for each i.

A natural way to build a mixture is to do it incrementally: we train the ﬁrst model P1 to minimize
Df (P1 (cid:107) Pd) and set the corresponding weight to α1 = 1, leading to P 1
model = P1. Then after having trained
t components P1, . . . , Pt ∈ G we can form the (t + 1)-st mixture model by adding a new component Q with
weight β as follows:

P t+1

model :=

(1 − β)αiPi + βQ.

t
(cid:88)

i=1

Df ((1 − β)Pg + βQ (cid:107) Pd),

We are going to choose β ∈ [0, 1] and Q ∈ G greedily, while keeping all the other parameters of the generative
model ﬁxed, so as to minimize

where we denoted Pg := P t

model the current generative mixture model before adding the new component.

We do not necessarily need to ﬁnd the optimal Q that minimizes (7) at each step. Indeed, it would be
suﬃcient to ﬁnd some Q which allows to build a slightly better approximation of Pd. This means that a

5which means such a property can be used in the context of the original GAN algorithm.
6One example of such a setting is running GANs, which are known to approximately minimize the Jensen-Shannon divergence.

(4)

(5)

(6)

(7)

7

more modest goal could be to ﬁnd Q such that, for some c < 1,

Df ((1 − β)Pg + βQ (cid:107) Pd) ≤ c · Df (Pg (cid:107) Pd) .

(8)

However, we observe that this greedy approach has a signiﬁcant drawback in practice. Indeed, as we
build up the mixture, we need to make β decrease (as P t
model approximates Pd better and better, one should
make the correction at each step smaller and smaller). Since we are approximating (7) using samples from
both distributions, this means that the sample from the mixture will only contain a fraction β of examples
from Q. So, as t increases, getting meaningful information from a sample so as to tune Q becomes harder
and harder (the information is “diluted”).

To address this issue, we propose to optimize an upper bound on (7) which involves a term of the
form Df (Q (cid:107) Q0) for some distribution Q0, which can be computed as a reweighting of the original data
distribution Pd.

In the following sections we will analyze the properties of (7) (Section 2.4) and derive upper bounds that
provide practical optimization criteria for building the mixture (Section 2.3). We will also show that under
certain assumptions, the minimization of the upper bound will lead to the optimum of the original criterion.
This procedure is reminiscent of the AdaBoost algorithm [6], which combines multiple weak predictors
into one very accurate strong composition. On each step AdaBoost adds one new predictor to the current
composition, which is trained to minimize the binary loss on the reweighted training set. The weights are
constantly updated in order to bias the next weak learner towards “hard” examples, which were incorrectly
classiﬁed during previous stages.

2.3 Upper Bounds

Next lemma provides two upper bounds on the divergence of the mixture in terms of the divergence of the
additive component Q with respect to some reference distribution R.

Lemma 1 Let f ∈ F. Given two distributions Pd, Pg and some β ∈ [0, 1], for any distribution Q and any
distribution R such that βdR ≤ dPd, we have

Df ((1 − β)Pg + βQ (cid:107) Pd) ≤ βD(Q (cid:107) R) + (1 − β)Df

Pg (cid:107)

(9)

(cid:18)

Pd − βR
1 − β

(cid:19)

.

If furthermore f ∈ FH , then, for any R, we have

Df ((1 − β)Pg + βQ (cid:107) Pd) ≤

βDf (Q (cid:107) R) +

Df ((1 − β)Pg + βR (cid:107) Pd)

.

(10)

(cid:18)(cid:113)

(cid:113)

(cid:19)2

Proof For the ﬁrst inequality, we use the fact that Df is jointly convex. We write Pd = (1 − β) Pd−βR
which is a convex combination of two distributions when the assumptions are satisﬁed.

1−β + βR

The second inequality follows from using the triangle inequality for the square root of the Hilbertian

metric Df and using convexity of Df in its ﬁrst argument.

We can exploit the upper bounds of Lemma 1 by introducing some well-chosen distribution R and
minimizing with respect to Q. A natural choice for R is a distribution that minimizes the last term of the
upper bound (which does not depend on Q).

2.4 Optimal Upper Bounds

In this section we provide general theorems about the optimization of the right-most terms in the upper
bounds of Lemma 1.

For the upper bound (10), this means we need to ﬁnd R minimizing Df ((1 − β)Pg + βR (cid:107) Pd). The

solution for this problem is given in the following theorem.

8

Theorem 1 For any f -divergence Df , with f ∈ F and f diﬀerentiable, any ﬁxed distributions Pd, Pg, and
any β ∈ (0, 1], the solution to the following minimization problem:

where P is a class of all probability distributions, has the density

min
Q∈P

Df ((1 − β)Pg + βQ (cid:107) Pd),

dQ∗

β(x) =

(λ∗dPd(x) − (1 − β)dPg(x))+

1
β

for some unique λ∗ satisfying (cid:82) dQ∗
Also, λ∗ = 1 if and only if Pd((1 − β)dPg > dPd) = 0, which is equivalent to βdQ∗

β = 1. Furthermore, β ≤ λ∗ ≤ min(1, β/δ), where δ := Pd(dPg = 0).

β = dPd − (1 − β)dPg.

Proof See Appendix C.1.

(cid:1)/β,
Remark 1 The form of Q∗
which would make arguments of the f -divergence identical? Unfortunately, it may be the case that dPd(X) <
(1 − β)dPg(X) for some of X ∈ X , leading to the negative values of dQ.

β may look unexpected at ﬁrst glance: why not setting dQ := (cid:0)dPd −(1−β)dPg

For the upper bound (9), we need to minimize Df

. The solution is given in the next

(cid:16)

Pg (cid:107) Pd−βR
1−β

(cid:17)

theorem.

Theorem 2 Given two distributions Pd, Pg and some β ∈ (0, 1], assume

Let f ∈ F. The solution to the minimization problem

Pd (dPg = 0) < β.

min
Q:βdQ≤dPd

Df

Pg (cid:107)

(cid:18)

(cid:19)

Pd − βQ
1 − β

is given by the distribution

dQ†

β(x) =

(cid:0)dPd(x) − λ†(1 − β)dPg(x)(cid:1)

+

1
β

for a unique λ† ≥ 1 satisfying (cid:82) dQ†

β = 1.

Proof See Appendix C.2.

Remark 2 Notice that the term that we optimized in upper bound (10) is exactly the initial objective (7).
So that Theorem 1 also tells us what the form of the optimal distribution is for the initial objective.

Remark 3 Surprisingly, in both Theorem 1 and 2, the solution does not depend on the choice of the func-
tion f , which means that the solution is the same for any f -divergence. This also means that by replacing
f by f ◦, we get similar results for the criterion written in the other direction, with again the same solution.
Hence the order in which we write the divergence does not matter and the optimal solution is optimal for
both orders.

Remark 4 Note that λ∗ is implicitly deﬁned by a ﬁxed-point equation. In Section 3.1 we will show how it
can be computed eﬃciently in the case of empirical distributions.

Remark 5 Obviously, λ† ≥ λ∗, where λ∗ was deﬁned in Theorem 1. Moreover, we have λ∗ ≤ 1/λ†. Indeed,
it is enough to insert λ† = 1/λ∗ into deﬁnition of Q†

β and check that in this case Q†

β ≥ 1.

9

2.5 Convergence Analysis for Optimal Updates

In previous section we derived analytical expressions for the distributions R minimizing last terms in upper
bounds (9) and (10). Assuming Q can perfectly match R, i.e. Df (Q (cid:107) R) = 0, we are now interested in the
convergence of the mixture (6) to the true data distribution Pd for Q = Q∗

β or Q = Q†
β.

We start with simple results showing that adding Q∗

β or Q†

β to the current mixture would yield a strict

improvement of the divergence.

Lemma 2 Under the conditions of Theorem 1, we have

Df

(cid:0)(1 − β)Pg + βQ∗

β

(cid:13)
(cid:13) Pd

(cid:1) ≤ Df

(cid:0)(1 − β)Pg + βPd

(cid:13)
(cid:13) Pd

(cid:1) ≤ (1 − β)Df (Pg (cid:107) Pd).

(11)

Under the conditions of Theorem 2, we have

(cid:32)

Df

Pg

(cid:13)
(cid:13)

(cid:33)

Pd − βQ†
β
1 − β

≤ Df (Pg (cid:107) Pd) ,

Df

(cid:0)(1 − β)Pg + βQ†

(cid:13)
(cid:13) Pd

β

(cid:1) ≤ (1 − β)Df (Pg (cid:107) Pd).

and

then

Proof The ﬁrst inequality follows immediately from the optimality of Q∗
β (hence the value of the objective
at Q∗
β is smaller than at Pd), and the fact that Df is convex in its ﬁrst argument and Df (Pd(cid:107)Pd) = 0.
The second inequality follows from the optimality of Q†
β is smaller
than its value at Pd which itself satisﬁes the condition βdPd ≤ dPd). For the third inequality, we combine
the second inequality with the ﬁrst inequality of Lemma 1 (with Q = R = Q†

β (hence the value of the objective at Q†

β).

The upper bound (11) of Lemma 2 can be reﬁned if the ratio dPg/dPd is almost surely bounded:

Lemma 3 Under the conditions of Theorem 1, if there exists M > 1 such that

Pd((1 − β)dPg > M dPd) = 0

Df

(cid:0)(1 − β)Pg + βQ∗

β

(cid:13)
(cid:13) Pd

(cid:1) ≤ f (λ∗) +

f (M )(1 − λ∗)
M − 1

.

Proof We use Inequality (20) of Lemma 6 with X = β, Y = (1 − β)dPg/dPd, and c = λ∗. We easily
verify that X + Y = ((1 − β)dPg + βdPd)/dPd and max(c, Y ) = ((1 − β)dPg + βdQ∗
β)/dPd and both have
expectation 1 with respect to Pd. We thus obtain:

Df ((1 − β)Pg + βQ∗

β (cid:107) Pd) ≤ f (λ∗) +

f (M ) − f (λ∗)
M − λ∗

(1 − λ∗) .

Since λ∗ ≤ 1 and f is non-increasing on (0, 1) we get

Df ((1 − β)Pg + βQ∗

β (cid:107) Pd) ≤ f (λ∗) +

f (M )(1 − λ∗)
M − 1

.

Remark 6 This upper bound can be tighter than that of Lemma 2 when λ∗ gets close to 1. Indeed, for
λ∗ = 1 the upper bound is exactly 0 and is thus tight, while the upper bound of Lemma 2 will not be zero in
this case.

10

Imagine repeatedly adding T new components to the current mixture Pg, where on every step we use the
same weight β and choose the components described in Theorem 1. In this case Lemma 2 guarantees that the
original objective value Df (Pg (cid:107) Pd) would be reduced at least to (1 − β)T Df (Pg (cid:107) Pd). This exponential rate
of convergence, which at ﬁrst may look surprisingly good, is simply explained by the fact that Q∗
β depends
on the true distribution Pd, which is of course unknown.

Lemma 2 also suggests setting β as large as possible. This is intuitively clear: the smaller the β, the less
we alter our current model Pg. As a consequence, choosing small β when Pg is far away from Pd would lead
to only minor improvements in objective (7). In fact, the global minimum of (7) can be reached by setting
β = 1 and Q = Pd. Nevertheless, in practice we may prefer to keep β relatively small, preserving what we
learned so far through Pg: for instance, when Pg already covered part of the modes of Pd and we want Q to
cover the remaining ones. We provide further discussions on choosing β in Section 3.2.

In the reminder of this section we study the convergence of (7) to 0 in the case where we use the upper
bound (10) and the weight β is ﬁxed (i.e. the same value at each iteration). This analysis can easily be
extended to a variable β.

Lemma 4 For any f ∈ F such that f (x) (cid:54)= 0 for x (cid:54)= 1, the following conditions are equivalent:

(i) Pd((1 − β)dPg > dPd) = 0;

(ii) Df ((1 − β)Pg + βQ∗

β (cid:107) Pd) = 0.

Proof The ﬁrst condition is equivalent to λ∗ = 1 according to Theorem 1. In this case, (1−β)Pg +βQ∗
β = Pd,
hence the divergence is 0. In the other direction, when the divergence is 0, since f is strictly positive for x (cid:54)= 1
(keep in mind that we can always replace f by f0 to get a non-negative function which will be strictly positive
if f (x) (cid:54)= 0 for x (cid:54)= 1), this means that with Pd probability 1 we have the equality dPd = (1 − β)dPg + βdQ∗
β,
which implies that (1 − β)dPg > dPd with Pd probability 1 and also λ∗ = 1.

This result tells that we can not perfectly match Pd by adding a new mixture component to Pg as long as
there are points in the space where our current model Pg severely over-samples. As an example, consider an
extreme case where Pg puts a positive mass in a region outside of the support of Pd. Clearly, unless β = 1,
we will not be able to match Pd.

Finally, we provide a necessary and suﬃcient condition for the iterative process to converge to the data
distribution Pd in ﬁnite number of steps. The criterion is based on the ratio dP1/dPd, where P1 is the ﬁrst
component of our mixture model.

Corollary 1 Take any f ∈ F such that f (x) (cid:54)= 0 for x (cid:54)= 1. Starting from P 1
iteratively according to P t+1
β, where on every step Q∗
with Pg := P t
exists M > 0 such that

model = P1, update the model
β is as deﬁned in Theorem 1
model (cid:107) Pd) will reach 0 in a ﬁnite number of steps if and only if there

model. In this case Df (P t

model = (1 − β)P t

model + βQ∗

Pd((1 − β)dP1 > M dPd) = 0 .

(12)

When the ﬁnite convergence happens, it takes at most − ln max(M, 1)/ ln(1 − β) steps.

Proof From Lemma 4, it is clear that if M ≤ 1 the convergence happens after the ﬁrst update. So let
us assume M > 1. Notice that dP t+1
β = max(λ∗dPd, (1 − β)dP t
model + βdQ∗
model) so that if
Pd((1 − β)dP t
model > M (1 − β)dPd) = 0. This proves that (12) is a
suﬃcient condition.

model > M dPd) = 0, then Pd((1 − β)dP t+1

model = (1 − β)dP t

Now assume the process converged in a ﬁnite number of steps. Let P t

the ﬁnal step. Note that P t
distribution P . According to Lemma 4 we have Pd((1 − β)dP t
immediately imply (12).

model be a mixture right before
model is represented by (1 − β)t−1P1 + (1 − (1 − β)t−1)P for certain probability
model > dPd) = 0. Together these two facts

It is also important to keep in mind that even if (12) is not satisﬁed the process still converges to the true
distribution at exponential rate (see Lemma 2 as well as Corollaries 2 and 3 below)

11

2.6 Weak to Strong Learnability

In practice the component Q that we add to the mixture is not exactly Q∗
β, but rather an approximation
to them. We need to show that if this approximation is good enough, then we retain the property that (8)
is reached. In this section we will show that this is indeed the case.

β or Q†

Looking again at Lemma 1 we notice that the ﬁrst upper bound is less tight than the second one. Indeed,
take the optimal distributions provided by Theorems 1 and 2 and plug them back as R into the upper bounds
of Lemma 1. Also assume that Q can match R exactly, i.e. we can achieve Df (Q (cid:107) R) = 0. In this case both
sides of (10) are equal to Df ((1 − β)Pg + βQ∗
β (cid:107) Pd), which is the optimal value for the original objective (7).
On the other hand, (9) does not become an equality and the r.h.s. is not the optimal one for (7).

This means that using (10) allows to reach the optimal value of the original objective (7), whereas
using (9) does not. However, this is not such a big issue since, as we mentioned earlier, we only need to
improve the mixture by adding the next component (we do not need to add the optimal next component).
So despite the solution of (7) not being reachable with the ﬁrst upper bound, we will still show that (8) can
be reached.

The ﬁrst result provides suﬃcient conditions for strict improvements when we use the upper bound (9).

Corollary 2 Given two distributions Pd, Pg, and some β ∈ (0, 1], assume

Let Q†

β be as deﬁned in Theorem 2. If Q is a distribution satisfying

Pd

(cid:18) dPg
dPd

(cid:19)

= 0

< β.

Df (Q (cid:107) Q†

β) ≤ γDf (Pg (cid:107) Pd)

for γ ∈ [0, 1] then

Df ((1 − β)Pg + βQ (cid:107) Pd) ≤ (1 − β(1 − γ))Df (Pg (cid:107) Pd).

Proof Immediately follows from combining Lemma 1, Theorem 1, and Lemma 2.

Next one holds for Hilbertian metrics and corresponds to the upper bound (10).

Corollary 3 Assume f ∈ FH , i.e. Df is a Hilbertian metric. Take any β ∈ (0, 1], Pd, Pg, and let Q∗
as deﬁned in Theorem 1. If Q is a distribution satisfying

β be

for some γ ∈ [0, 1], then

Df (Q (cid:107) Q∗

β) ≤ γDf (Pg (cid:107) Pd)

Df ((1 − β)Pg + βQ (cid:107) Pd) ≤

(cid:16)(cid:112)γβ + (cid:112)1 − β

(cid:17)2

Df (Pg (cid:107) Pd) .

In particular, the right-hand side is strictly smaller than Df (Pg (cid:107) Pd) as soon as γ < β/4 (and β > 0).

Immediately follows from combining Lemma 1, Theorem 2, and Lemma 2. It is easy to verify that

Proof
for γ < β/4, the coeﬃcient is less than (β/2 +

√

1 − β)2 which is < 1 (for β > 0).

Remark 7 We emphasize once again that the upper bound (10) and Corollary 3 both hold for Jensen-
Shannon, Hellinger, and total variation divergences among others. In particular they can be applied to the
original GAN algorithm.

Conditions 14 and 15 may be compared to the “weak learnability” condition of AdaBoost. As long as our
weak learner is able to solve the surrogate problem (5) of matching respectively Q†
β accurately enough,
the original objective (7) is guaranteed to decrease as well. It should be however noted that Condition 15

β or Q∗

12

(13)

(14)

(15)

Indeed, as already mentioned before,
with γ < β/4 is perhaps too strong to call it “weak learnability”.
the weight β is expected to decrease to zero as the number of components in the mixture distribution Pg
increases. This leads to γ → 0, making it harder to meet Condition 15. This obstacle may be partially
resolved by the fact that we will use a GAN to ﬁt Q, which corresponds to a relatively rich7 class of models
G in (5). In other words, our weak learner is not so weak.

On the other hand, Condition (14) of Corollary 2 is much milder. No matter what γ ∈ [0, 1] and β ∈ (0, 1]
we choose, the new component Q is guaranteed to strictly improve the objective functional. This comes at
the price of the additional Condition (13), which asserts that β should be larger than the mass of true data
Pd missed by the current model Pg. We argue that this is a rather reasonable condition: if Pg misses many
modes of Pd we would prefer assigning a relatively large weight β to the new component Q.

3 AdaGAN

In this section we provide a more detailed description of Algorithm 1 from Section 1.1, in particular how to
reweight the training examples for the next iteration and how to choose the mixture weights.

In a nutshell, at each iteration we want to add a new component Q to the current mixture Pg with
weight β, to create a mixture with distribution (1 − β)Pg + βQ. This component Q should approach an
“optimal target” Q∗

β and we know from Theorem 1 that:

dQ∗

β =

(cid:18)

dPd
β

λ∗ − (1 − β)

(cid:19)

dPg
dPd

.

+

dPg
dPd

(X) = h(cid:0)D(X)(cid:1).

dQ∗

β =

dPd
β

(cid:0)λ∗ − (1 − β)h(D)(cid:1)

+ ,

wi =

pi
β

(cid:0)λ∗ − (1 − β)h(di)(cid:1)

+

Computing this distribution requires to know the density ratio dPg/dPd, which is not directly accessible,
but it can be estimated using the idea of adversarial training. Indeed, we can train a discriminator D to
distinguish between samples from Pd and Pg. It is known that for an arbitrary f -divergence, there exists
a corresponding function h (see [7]) such that the values of the optimal discriminator D are related to the
density ratio in the following way:

In particular, for the Jensen-Shannon divergence, used by the original GAN algorithm, it holds that h(cid:0)D(X)(cid:1) =

1−D(X)
D(X)

. So in this case for the optimal discriminator we have

which can be viewed as a reweighted version of the original data distribution Pd.

In particular, when we compute dQ∗

β on the training sample SN = (X1, . . . , XN ), each example Xi has

the following weight:

with pi = dPd(Xi) and di = D(Xi). In practice, we use the empirical distribution over the training sample
which means we set pi = 1/N .

3.1 How to compute λ∗ of Theorem 1

Next we derive an algorithm to determine λ∗. We need to ﬁnd a value of λ∗ such that the weights wi in (17)
are normalized, i.e.:

(cid:88)

wi =

(cid:0)λ∗ − (1 − β)h(di)(cid:1) = 1 ,

(cid:88)

i

pi
β

i∈I(λ∗)

7The hardness of meeting Condition 15 of course largely depends on the class of models G used to ﬁt Q in (5). For now we

ignore this question and leave it for future research.

13

(16)

(17)

where I(λ) := {i : λ > (1 − β)h(di)}. This in turn yields:

λ∗ =

β
i∈I(λ∗) pi

(cid:80)



1 +

(1 − β)
β

(cid:88)

i∈I(λ∗)



pih(di)

 .

(18)

Now, to compute the r.h.s., we need to know I(λ∗). To do so, we sort the values h(di) in increasing order:
h(d1) ≤ h(d2) ≤ . . . ≤ h(dN ). Then I(λ∗) is simply a set consisting of the ﬁrst k values, where we have to
determine k. Thus, it suﬃces to test successively all positive integers k until the λ given by Equation (18)
veriﬁes:

(1 − β)h(dk) < λ ≤ (1 − β)h(dk+1) .

This procedure is guaranteed to converge, because by Theorem 1, we know that λ∗ exists, and it satisﬁes (18).
In summary, λ∗ can be determined by Algorithm 2.

Algorithm 2: Determining λ∗
1 Sort the values h(di) in increasing order ;
1 + 1−β
2 Initialize λ ← β
p1
3 while (1 − β)h(dk) ≥ λ do
4

β p1h(d1)

(cid:16)

(cid:17)

k ← k + 1;
λ ← β
(cid:80)k

i=1 pi

(cid:16)

5

1 + (1−β)

β

(cid:80)k

(cid:17)
i=1 pih(di)

and k ← 1 ;

3.2 How to choose a mixture weight β

While for every β there is an optimal reweighting scheme, the weights from (17) depend on β. In particular,
if β is large enough to verify dPd(x)λ∗ − (1 − β)dPg(x) ≥ 0 for all x, the optimal component Q∗
β satisﬁes
(1 − β)Pg + βQ∗
β = Pd, as proved in Lemma 4. In other words, in this case we exactly match the data
distribution Pd, assuming the GAN can approximate the target Q∗
β perfectly. This criterion alone would
lead to choosing β = 1. However in practice we know we can’t get a generator that produces exactly the
target distribution Q∗
β. We thus propose a few heuristics one can follow to choose β:

– Any ﬁxed constant value β for all iterations.

– All generators to be combined with equal weights in the ﬁnal mixture model. This corresponds to

setting βt = 1

t , where t is the iteration.

– Instead of choosing directly a value for β one could pick a ratio 0 < r < 1 of examples which should
have a weight wi > 0. Given such an r, there is a unique value of β (βr) resulting in wi > 0 for exactly
N · r training examples. Such a value βr can be determined by binary search over β in Algorithm 2.
Possible choices for r include:

– r constant, chosen experimentally.
– r decreasing with the number of iterations, e.g., r = c1e−c2t for any positive constants c1, c2.

– Alternatively, one can set a particular threshold for the density ratio estimate h(D), compute the
fraction r of training examples that have a value above that threshold and derive β from this ratio
r (as above). Indeed, when h(D) is large, that means that the generator does not generate enough
examples in that region, and the next iteration should be encouraged to generate more there.

14

Algorithm 3: AdaGAN, a meta-algorithm to construct a “strong” mixture of T individual GANs,
trained sequentially. The mixture weight schedule ChooseMixtureWeight should be provided by the
user (see 3.2). This is an instance of the high level Algorithm 1, instantiating UpdateTrainingWeights.

#Compute the new weights of the training examples (UpdateTrainingWeights)

#Compute the discriminator between the original (unweighted) data and the current mixture Gt−1

Input: Training sample SN := {X1, . . . , XN }.

Output: Mixture generative model G = GT .

Train vanilla GAN: G1 = GAN(SN )

for t = 2, . . . , T do

#Choose a mixture weight for the next component

βt = ChooseMixtureWeight(t)

D ← DGAN (SN , Gt−1);
#Compute λ∗ using Algorithm 2
λ∗ ← λ(βt, D)
#Compute the new weight for each example

for i = 1, . . . , N do

W i

t = 1
N βt

(λ∗ − (1 − βt)h(D(Xi)))+

end for
#Train t-th “weak” component generator Gc
t
Gc

t = GAN(SN , Wt)

#Update the overall generative model
#Notation below means forming a mixture of Gt−1 and Gc
t .
Gt = (1 − βt)Gt−1 + βtGc
t

end for

3.3 Complete algorithm

Now we have all the necessary components to introduce the complete AdaGAN meta-algorithm. The algo-
rithm uses any given GAN implementation (which can be the original one of Goodfellow et al. [1] or any
later modiﬁcations) as a building block. Accordingly, Gc ← GAN (SN , W ) returns a generator Gc for a given
set of examples SN = (X1, . . . , XN ) and corresponding weights W = (w1, . . . , wN ). Additionally, we write
D ← DGAN (SN , G) to denote a procedure that returns a discriminator from the GAN algorithm trained
on a given set of true data examples SN and examples sampled from the mixture of generators G. We
also write λ∗(β, D) to denote the optimal λ∗ given by Algorithm 2. The complete algorithm is presented in
Algorithm 3.

4 Experiments

We tested AdaGAN8 on toy datasets, for which we can interpret the missing modes in a clear and reproducible
way, and on MNIST, which is a high-dimensional dataset. The goal of these experiments was not to evaluate

8Code available online at https://github.com/tolstikhin/adagan

15

the visual quality of individual sample points, but to demonstrate that the re-weighting scheme of AdaGAN
promotes diversity and eﬀectively covers the missing modes.

4.1 Toy datasets

The target distribution is deﬁned as a mixture of normal distributions, with diﬀerent variances. The distances
between the means are relatively large compared to the variances, so that each Gaussian of the mixture is
“isolated”. We vary the number of modes to test how well each algorithm performs when there are fewer or
more expected modes.

More precisely, we set X = R2, each Gaussian component is isotropic, and their centers are sampled
uniformly in a square. That particular random seed is ﬁxed for all experiments, which means that for a
given number of modes, the target distribution is always the same. The variance parameter is the same for
each component, and is decreasing with the number of modes, so that the modes stay apart from each other.
This target density is very easy to learn, using a mixture of Gaussians model, and for example the EM
algorithm [19]. If applied to the situation where the generator is producing single Gaussians (i.e. PZ is a
standard Gaussian and G is a linear function), then AdaGAN produces a mixture of Gaussians, however it
does so incrementally unlike EM, which keeps a ﬁxed number of components. In any way AdaGAN was not
tailored for this particular case and we use the Gaussian mixture model simply as a toy example to illustrate
the missing modes problem.

4.1.1 Algorithms

We compare diﬀerent meta-algorithms based on GAN, and the baseline GAN algorithm. All the meta-
algorithms use the same implementation of the underlying GAN procedure.
In all cases, the generator
uses latent space Z = R5, and two ReLU hidden layers, of size 10 and 5 respectively. The corresponding
discriminator has two ReLU hidden layers of size 20 and 10 respectively. We use 64k training examples, and
15 epochs, which is enough compared to the small scale of the problem, and all networks converge properly
and overﬁtting is never an issue. Despite the simplicity of the problem, there are already diﬀerences between
the diﬀerent approaches.

We compare the following algorithms:

– The baseline GAN algorithm, called Vanilla GAN in the results.

– The best model out of T runs of GAN, that is: run T GAN instances independently, then take the run
that performs best on a validation set. This gives an additional baseline with similar computational
complexity as the ensemble approaches. Note that the selection of the best run is done on the reported
target metric (see below), rather than on the internal metric. As a result this baseline is slightly
overestimated. This procedure is called Best of T in the results.

– A mixture of T GAN generators, trained independently, and combined with equal weights (the “bag-

ging” approach). This procedure is called Ensemble in the results.

– A mixture of GAN generators, trained sequentially with diﬀerent choices of data reweighting:

– The AdaGAN algorithm (Algorithm 1), for β = 1/t, i.e. each component will have the same

weight in the resulting mixture (see § 3.2). This procedure is called Boosted in the results.

– The AdaGAN algorithm (Algorithm 1), for a constant β, exploring several values. This procedure

is called for example Beta0.3 for β = 0.3 in the results.

– Reweighting similar to “Cascade GAN” from [14], i.e. keeping the top r fraction of examples,
based on the discriminator corresponding to the previous generator. This procedure is called for
example TopKLast0.3 for r = 0.3.

– Keep the top r fraction of examples, based on the discriminator corresponding to the mixture of

all previous generators. This procedure is called for example TopK0.3 for r = 0.3.

16

4.1.2 Metrics

To evaluate how well the generated distribution matches the target distribution, we use a coverage metric C.
We compute the probability mass of the true data “covered” by the model distribution Pmodel. More
precisely, we compute C := Pd(dPmodel > t) with t such that Pmodel(dPmodel > t) = 0.95. This metric
is more interpretable than the likelihood, making it easier to assess the diﬀerence in performance of the
algorithms. To approximate the density of Pmodel we use a kernel density estimation method, where the
bandwidth is chosen by cross validation. Note that we could also use the discriminator D to approximate
the coverage as well, using the relation from (16).

Another metric is the likelihood of the true data under the generated distribution. More precisely, we
compute L := 1
i log Pmodel(xi), on a sample of N examples from the data. Note that [20] proposes a
N
more general and elegant approach (but less straightforward to implement) to have an objective measure of
GAN. On the simple problems we tackle here, we can precisely estimate the likelihood.

(cid:80)

In the main results we report the metric C and in Appendix E we report both L and C. For a given
metric, we repeat the run 35 times with the same parameters (but diﬀerent random seeds). For each run, the
learning rate is optimized using a grid search on a validation set. We report the median over those multiple
runs, and the interval corresponding to the 5% and 95% percentiles. Note this is not a conﬁdence interval of
the median, which would shrink to a singleton with an inﬁnite number of runs. Instead, this gives a measure
of the stability of each algorithm. The optimizer is a simple SGD: Adam was also tried but gave slightly less
stable results.

4.1.3 Results

With the vanilla GAN algorithm, we observe that not all the modes are covered (see Figure 1 for an
illustration). Diﬀerent modes (and even diﬀerent number of modes) are possibly covered at each restart of
the algorithm, so restarting the algorithm with diﬀerent random seeds and taking the best (“best of T ”) can
improve the results.

Figure 3 summarizes the performance of the main algorithms on the C metric, as a function of the number
of iterations T . Table 1 gives more detailed results, varying the number of modes for the target distribution.
Appendix E contains details on variants for the reweighting heuristics as well as results for the L metric.

As expected, both the ensemble and the boosting approaches signiﬁcantly outperform the vanilla GAN
and the “best of T ” algorithm. Interestingly, the improvements are signiﬁcant even after just one or two
additional iterations (T = 2 or T = 3). The boosted approach converges much faster.
In addition, the
variance is much lower, improving the likelihood that a given run gives good results. On this setup, the
vanilla GAN approach has a signiﬁcant number of catastrophic failures (visible in the lower bound of the
interval).

Empirical results on combining AdaGAN meta-algorithm with the unrolled GANs [4] are available in

Appendix A.

4.2 MNIST and MNIST3

We ran experiments both on the original MNIST and on the 3-digit MNIST (MNIST3) [5, 4] dataset,
obtained by concatenating 3 randomly chosen MNIST images to form a 3-digit number between 0 and 999.
According to [5, 4], MNIST contains 10 modes, while MNIST3 contains 1000 modes, and these modes can
be detected using the pre-trained MNIST classiﬁer. We combined AdaGAN both with simple MLP GANs
and DCGANs [21]. We used T ∈ {5, 10}, tried models of various sizes and performed a reasonable amount
of hyperparameter search. For the details we refer to Appendix B.

Similarly to [4, Sec 3.3.1] we failed to reproduce the missing modes problem for MNIST3 reported in
[5] and found that simple GAN architectures are capable of generating all 1000 numbers. The authors
of [4] proposed to artiﬁcially introduce the missing modes again by limiting the generators’ ﬂexibility. In
our experiments, GANs trained with the architectures reported in [4] were often generating poorly looking
digits. As a result, the pre-trained MNIST classiﬁer was outputting random labels, which again led to full

17

Figure 2: Coverage C of the true data by the model distribution P T
model, as a function of iterations T .
Experiments correspond to the data distribution with 5 modes. Each blue point is the median over 35 runs.
Green intervals are deﬁned by the 5% and 95% percentiles (see Section 4.1.2).
Iteration 0 is equivalent
to one vanilla GAN. The left plot corresponds to taking the best generator out of T runs. The middle
plot corresponds to the “ensemble GAN”, simply taking a uniform mixture of T independently trained
GAN generators. The right plot corresponds to our boosting approach (AdaGAN), carefully reweighting
the examples based on the previous generators, with βt = 1/t. Both the ensemble and boosting approaches
signiﬁcantly outperform the vanilla approach with few additional iterations. They also outperform taking
the best out of T runs. The boosting outperforms all other approaches. For AdaGAN the variance of the
performance is also signiﬁcantly decreased.

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

0.97 (0.9; 1.0)

0.88 (0.4; 1.0)

0.63 (0.5; 1.0)

0.72 (0.5; 0.8)

0.58 (0.4; 0.8)

0.59 (0.2; 0.7)

Best of T (T=3)

0.99 (1.0; 1.0)

0.96 (0.9; 1.0)

0.91 (0.7; 1.0)

0.80 (0.7; 0.9)

0.84 (0.7; 0.9)

0.70 (0.6; 0.8)

Best of T (T=10)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

0.98 (0.8; 1.0)

0.80 (0.8; 0.9)

0.87 (0.8; 0.9)

0.71 (0.7; 0.8)

Ensemble (T=3)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.93 (0.8; 1.0)

0.78 (0.6; 1.0)

0.85 (0.6; 1.0)

0.80 (0.6; 1.0)

Ensemble (T=10)

1.00 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

0.91 (0.8; 1.0)

0.88 (0.8; 1.0)

0.89 (0.7; 1.0)

TopKLast0.5 (T=3)

0.98 (0.9; 1.0)

0.98 (0.9; 1.0)

0.95 (0.9; 1.0)

0.95 (0.8; 1.0)

0.86 (0.7; 1.0)

0.86 (0.6; 0.9)

TopKLast0.5 (T=10)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.98 (1.0; 1.0)

0.99 (0.8; 1.0)

0.99 (0.8; 1.0)

1.00 (0.8; 1.0)

Boosted (T=3)

0.99 (1.0; 1.0)

0.99 (0.9; 1.0)

0.98 (0.9; 1.0)

0.91 (0.8; 1.0)

0.91 (0.8; 1.0)

0.86 (0.7; 1.0)

Boosted (T=10)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

Table 1: Performance of the diﬀerent algorithms on varying number of mixtures of Gaussians. The reported
score is the coverage C, probability mass of Pd covered by the 5th percentile of Pg deﬁned in Section 4.1.2.
See Table 2 for more metrics. The reported scores are the median and interval deﬁned by the 5% and 95%
percentile (in parenthesis) (see Section 4.1.2), over 35 runs for each setting. Note that the 95% interval is
not the usual conﬁdence interval measuring the variance of the experiment itself, but rather measures the
stability of the diﬀerent algorithms (would remain even if each experiment was run an inﬁnite number of
times). Both the ensemble and the boosting approaches signiﬁcantly outperform the vanilla GAN even with
just three iterations (i.e. just two additional components). The boosting approach converges faster to the
optimal coverage and with smaller variance.

18

coverage of the 1000 numbers. We tried to threshold the conﬁdence of the pre-trained classiﬁer, but decided
that this metric was too ad-hoc.

For MNIST we noticed that the re-weighted distribu-
tion was often concentrating its mass on digits having
very speciﬁc strokes: on diﬀerent rounds it could high-
light thick, thin, vertical, or diagonal digits, indicating
that these traits were underrepresented in the generated
samples (see Figure 3). This suggests that AdaGAN does
a reasonable job at picking up diﬀerent modes of the
dataset, but also that there are more than 10 modes in
MNIST (and more than 1000 in MNIST3). It is not clear
how to evaluate the quality of generative models in this
context.

We also tried to use the “inversion” metric discussed
in Section 3.4.1 of [4]. For MNIST3 we noticed that a
single GAN was capable of reconstructing most of the
training points very accurately both visually and in the
(cid:96)2-reconstruction sense.

5 Conclusion

Figure 3: Digits from the MNIST dataset cor-
responding to the smallest (left) and largest
(right) weights, obtained by the AdaGAN pro-
cedure (see Section 3) in one of the runs. Bold
digits (left) are already covered and next GAN
will concentrate on thin (right) digits.

We presented an incremental procedure for constructing
an additive mixture of generative models by minimizing
an f -divergence criterion. Based on this, we derived a boosting-style algorithm for GANs, which we call
AdaGAN. By incrementally adding new generators into a mixture through the optimization of a GAN
criterion on a reweighted data, this algorithm is able to progressively cover all the modes of the true data
distribution. This addresses one of the main practical issues of training GANs.

We also presented a theoretical analysis of the convergence of this incremental procedure and showed
conditions under which the mixture converges to the true distribution either exponentially or in a ﬁnite
number of steps.

Our preliminary experiments (on toy data) show that this algorithm is eﬀectively addressing the missing

modes problem and allows to robustly produce a mixture which covers all modes of the data.

However, since the generative model that we obtain is not a single neural network but a mixture of such
networks, the corresponding latent representation no longer has a smooth structure. This can be seen as a
disadvantage compared to standard GAN where one can perform smooth interpolation in latent space. On
the other hand it also allows to have a partitioned latent representation where one component is discrete.
Future work will explore the possibility of leveraging this structure to model discrete aspects of the dataset,
such as the class in object recognition datasets in a similar spirit to [22].

References

[1] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Pro-
cessing Systems, pages 2672–2680, 2014.

[2] D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In ICLR, 2014.

[3] Martin Arjovsky, Soumith Chintala, and Lon Bottou. Wasserstein GAN. arXiv:1701.07875, 2017.

[4] L. Metz, B. Poole, D. Pfau, and J. Sohl-Dickstein. Unrolled generative adversarial networks.

arXiv:1611.02163, 2017.

19

[5] Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized generative

adversarial networks. arXiv:1612.02136, 2016.

[6] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application

to boosting. Journal of Computer and System Sciences, 55(1):119–139, 1997.

[7] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka.

f-GAN: Training generative neural samplers
using variational divergence minimization. In Advances in Neural Information Processing Systems, 2016.

[8] Max Welling, Richard S. Zemel, and Geoﬀrey E. Hinton. Self supervised boosting.

In Advances in

neural information processing systems, pages 665–672, 2002.

[9] Zhuowen Tu. Learning generative models via discriminative approaches. In 2007 IEEE Conference on

Computer Vision and Pattern Recognition, pages 1–8. IEEE, 2007.

[10] Aditya Grover and Stefano Ermon. Boosted generative models. ICLR 2017 conference submission, 2016.

[11] R. M. Neal. Annealed importance sampling. Statistics and Computing, 11(2):125–139, 2001.

[12] Saharon Rosset and Eran Segal. Boosting density estimation.

In Advances in Neural Information

Processing Systems, pages 641–648, 2002.

[13] A Barron and J Li. Mixture density estimation. Biometrics, 53:603–618, 1997.

[14] Yaxing Wang, Lichao Zhang, and Joost van de Weijer. Ensembles of generative adversarial networks.

arXiv:1612.00991, 2016.

[15] F. Liese and K.-J. Miescke. Statistical Decision Theory. Springer, 2008.

[16] M. D. Reid and R. C. Williamson. Information, divergence and risk for binary experiments. Journal of

Machine Learning Research, 12:731–817, 2011.

[17] Bent Fuglede and Flemming Topsoe. Jensen-shannon divergence and hilbert space embedding. In IEEE

International Symposium on Information Theory, pages 31–31, 2004.

[18] Matthias Hein and Olivier Bousquet. Hilbertian metrics and positive deﬁnite kernels on probability

measures. In AISTATS, pages 136–143, 2005.

[19] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM

algorithm. Journal of the Royal Statistical Society, B, 39:1–38, 1977.

[20] Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger Grosse. On the quantitative analysis of

decoder-based generative models, 2016.

[21] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional

generative adversarial networks. In ICLR, 2016.

[22] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: In-
terpretable representation learning by information maximizing generative adversarial nets. In Advances
in Neural Information Processing Systems, pages 2172–2180, 2016.

20

A Further details on toy experiments

To illustrate the ’meta-algorithm aspect’ of AdaGAN, we also performed experiments with an unrolled
GAN [4] instead of a GAN as the base generator. We trained the GANs both with the Jensen-Shannon
objective (2), and with its modiﬁed version proposed in [1] (and often considered as the baseline GAN),
where log(1 − D(G(Z))) is replaced by − log(D(G(Z))). We use the same network architecture as in the
other toy experiments. Figure 4 illustrates our results. We ﬁnd that AdaGAN works with all underlying
GAN algorithms. Note that, where the usual GAN updates the generator and the discriminator once, an
unrolled GAN with 5 unrolling steps updates the generator once and the discriminator 1 + 5, i.e. 6 times
(and then rolls back 5 steps). Thus, in terms of computation time, training 1 single unrolled GAN roughly
corresponds to doing 3 steps of AdaGAN with a usual GAN. In that sense, Figure 4 shows that AdaGAN
(with a usual GAN) signiﬁcantly outperforms a single unrolled GAN. Additionally, we note that using the
Jensen-Shannon objective (rather than the modiﬁed version) seems to have some mode-regularizing eﬀect.
Surprisingly, using unrolling steps makes no signiﬁcant diﬀerence.

Figure 4: Comparison of AdaGAN ran with a GAN (top row) and with an unrolled GAN [4] (bottom).
Coverage C of the true data by the model distribution P T
model, as a function of iterations T . Experiments
are similar to those of Figure 3, but with 10 modes. Top and bottom rows correspond to the usual and the
unrolled GAN (with 5 unrolling steps) respectively, trained with the Jensen-Shannon objective (2) on the
left, and with the modiﬁed objective originally proposed by [1] on the right. In terms of computation time,
one step of AdaGAN with unrolled GAN corresponds to roughly 3 steps of AdaGAN with a usual GAN. On
all images T = 1 corresponds to vanilla unrolled GAN.

B Further details on MNIST/MNIST3 experiments

GAN Architecture We ran AdaGAN on MNIST (28x28 pixel images) using (de)convolutional networks
with batch normalizations and leaky ReLu. The latent space has dimension 100. We used the following

21

architectures:

Generator: 100 x 1 x 1 → fully connected → 7 x 7 x 16 → deconv → 14 x 14 x 8 →

Discriminator: 28 x 28 x 1 → conv → 14 x 14 x 16 → conv → 7 x 7 x 32 →

→ deconv → 28 x 28 x 4 → deconv → 28 x 28 x 1

→ fully connected → 1

where each arrow consists of a leaky ReLu (with 0.3 leak) followed by a batch normalization, conv and deconv
are convolutions and transposed convolutions with 5x5 ﬁlters, and fully connected are linear layers with bias.
The distribution over Z is uniform over the unit box. We use the Adam optimizer with β1 = 0.5, with 2
G steps for 1 D step and learning rates 0.005 for G, 0.001 for D, and 0.0001 for the classiﬁer C that does
the reweighting of digits. We optimized D and G over 200 epochs and C over 5 epochs, using the original
Jensen-Shannon objective (2), without the log trick, with no unrolling and with minibatches of size 128.

Empirical observations Although we could not ﬁnd any appropriate metric to measure the increase of
diversity promoted by AdaGAN, we observed that the re-weighting scheme indeed focuses on digits with
very speciﬁc strokes. In Figure 5 for example, we see that after one AdaGAN step, the generator produces
overly thick digits (top left image). Thus AdaGAN puts small weights on the thick digits of the dataset
(bottom left) and high weights on the thin ones (bottom right). After the next step, the new GAN produces
both thick and thin digits.

C Proofs

C.1 Proof of Theorem 1

Before proving Theorem 1, we introduce two lemmas. The ﬁrst one is about the determination of the constant
λ, the second one is about comparing the divergences of mixtures.

Lemma 5 Let P and Q be two distributions, γ ∈ [0, 1] and λ ∈ R. The function

(cid:90) (cid:18)

g(λ) :=

λ − γ

(cid:19)

dQ
dP

dP

+

g(cid:48)
+(λ) = P (λ · dP ≥ γ · dQ).

is nonnegative, convex, nondecreasing, satisﬁes g(λ) ≤ λ, and its right derivative is given by

The equation

g(λ) = 1 − γ
has a solution λ∗ (unique when γ < 1) with λ∗ ∈ [1 − γ, 1]. Finally, if P (dQ = 0) ≥ δ for a strictly positive
constant δ then λ∗ ≤ (1 − γ)δ−1.

Proof The convexity of g follows immediately from the convexity of x (cid:55)→ (x)+ and the linearity of the
integral. Similarly, since x (cid:55)→ (x)+ is non-decreasing, g is non-decreasing.

We deﬁne the set I(λ) as follows:

Now let us consider g(λ + (cid:15)) − g(λ) for some small (cid:15) > 0. This can also be written:

I(λ) := {x ∈ X : λ · dP (x) ≥ γ · dQ(x)}.

g(λ + (cid:15)) − g(λ) =

(cid:15)dP +

(λ + (cid:15))dP −

I(λ)

I(λ+(cid:15))\I(λ)

I(λ+(cid:15))\I(λ)

(cid:90)

(cid:90)

(cid:90)

(cid:90)

(cid:90)

γdQ

γdQ.

= (cid:15)P (I(λ)) +

(λ + (cid:15))dP −

I(λ+(cid:15))\I(λ)

I(λ+(cid:15))\I(λ)

22

Figure 5: AdaGAN on MNIST. Bottom row are true MNIST digits with smallest (left) and highest (right)
weights after re-weighting at the end of the ﬁrst AdaGAN step. Those with small weight are thick and
resemble those generated by the GAN after the ﬁrst AdaGAN step (top left). After training with the re-
weighted dataset during the second iteration of AdaGAN, the new mixture produces more thin digits (top
right).

On the set I(λ + (cid:15))\I(λ), we have

(λ + (cid:15))dP − γdQ ∈ [0, (cid:15)].

So that

and thus

(cid:15)P (I(γ)) ≤ g(λ + (cid:15)) − g(λ) ≤ (cid:15)P (I(γ)) + (cid:15)P (cid:0)I(λ + (cid:15))\I(λ)(cid:1) = (cid:15)P (I(λ + (cid:15)))

lim
(cid:15)→0+

g(λ + (cid:15)) − g(λ)
(cid:15)

= lim
(cid:15)→0+

P (I(λ + (cid:15))) = P (I(λ)).

This gives the expression of the right derivative of g. Moreover, notice that for λ, γ > 0

g(cid:48)
+(λ) = P (λ · dP ≥ γ · dQ) = P

≤

= 1 − P

>

≥ 1 − γ/λ

(cid:18) dQ
dP

(cid:19)

λ
γ

(cid:18) dQ
dP

(cid:19)

λ
γ

by Markov’s inequality.

23

It is obvious that g(0) = 0. By Jensen’s inequality applied to the convex function x (cid:55)→ (x)+, we have
g(λ) ≥ (λ − γ)+. So g(1) ≥ 1 − γ. Also, g = 0 on R− and g ≤ λ. This means g is continuous on R and
thus reaches the value 1 − γ on the interval (0, 1] which shows the existence of λ∗ ∈ (0, 1]. To show that λ∗
is unique we notice that since g(x) = 0 on R−, g is convex and non-decreasing, g cannot be constant on an
interval not containing 0, and thus g(x) = 1 − γ has a unique solution for γ < 1.

Also by convexity of g,

g(0) − g(λ∗) ≥ −λ∗g(cid:48)

+(λ∗),

which gives λ∗ ≥ (1 − γ)/g(cid:48)
the fact that g(cid:48)

+ is increasing we conclude that λ∗ ≤ (1 − γ)δ−1.

+(λ∗) ≥ 1 − γ since g(cid:48)

+ ≤ 1. If P (dQ = 0) ≥ δ > 0 then also g(cid:48)

+(0) ≥ δ > 0. Using

Next we introduce some simple convenience lemma for comparing convex functions of random variables.

Lemma 6 Let f be a convex function, X, Y be real-valued random variables and c ∈ R be a constant such
that

E [max(c, Y )] = E [X + Y ] .

Then we have the following bound:

If in addition, Y ≤ M a.s. for M ≥ c, then

E [f (max(c, Y ))] ≤ E [f (X + Y )] − E [X(f (cid:48)(Y ) − f (cid:48)(c))+] ≤ E [f (X + Y )] .

E [f (max(c, Y ))] ≤ f (c) +

(E [X + Y ] − c).

f (M ) − f (c)
M − c

Proof We decompose the expectation with respect to the value of the max, and use the convexity of f :

f (X + Y ) − f (max(c, Y )) = 1[Y ≤c](f (X + Y ) − f (c)) + 1[Y >c](f (X + Y ) − f (Y ))

(19)

(20)

≥ 1[Y ≤c]f (cid:48)(c)(X + Y − c) + 1[Y >c]Xf (cid:48)(Y )
= (1 − 1[Y >c])Xf (cid:48)(c) + f (cid:48)(c)(Y − max(c, Y )) + 1[Y >c]Xf (cid:48)(Y )
= f (cid:48)(c)(X + Y − max(c, Y )) + 1[Y >c]X(f (cid:48)(Y ) − f (cid:48)(c))
= f (cid:48)(c)(X + Y − max(c, Y )) + X(f (cid:48)(Y ) − f (cid:48)(c))+,

where we used that f (cid:48) is non-decreasing in the last step. Taking the expectation gives the ﬁrst inequality.

For the second inequality, we use the convexity of f on the interval [c, M ]:

f (max(c, Y )) ≤ f (c) +

(max(c, Y ) − c).

f (M ) − f (c)
M − c

Taking an expectation on both sides gives the second inequality.

Proof [Theorem 1] We ﬁrst apply Lemma 5 with γ = 1 − β and this proves the existence of λ∗ in the interval
(β, 1], which shows that Q∗

β is indeed well-deﬁned as a distribution.

Then we use Inequality (19) of Lemma 6 with X = βdQ/dPd, Y = (1 − β)dPg/dPd, and c = λ∗. We
β)/dPd and both

easily verify that X + Y = ((1 − β)dPg + βdQ)/dPd and max(c, Y ) = ((1 − β)dPg + βdQ∗
have expectation 1 with respect to Pd. We thus obtain for any distribution Q,

Df ((1 − β)Pg + βQ∗

β (cid:107) Pd) ≤ Df ((1 − β)Pg + βQ (cid:107) Pd) .

This proves the optimality of Q∗
β.

24

C.2 Proof of Theorem 2

Lemma 7 Let P and Q be two distributions, γ ∈ (0, 1), and λ ≥ 0. The function

h(λ) :=

− λ

(cid:90) (cid:18) 1
γ

(cid:19)

dQ
dP

dP

+

h(λ) =

1 − γ
γ

is convex, non-increasing, and its right derivative is given by h(cid:48)
∆ := P (dQ(X)/dP (X) = 0). Then the equation

+(λ) = −Q(1/γ ≥ λdQ(X)/dP (X)). Denote

has no solutions if ∆ > 1 − γ, has a single solution λ† ≥ 1 if ∆ < 1 − γ, and has inﬁnitely many or no
solutions when ∆ = 1 − γ.

Proof The convexity of h follows immediately from the convexity of x (cid:55)→ (a − x)+ and the linearity of the
integral. Similarly, since x (cid:55)→ (a − x)+ is non-increasing, h is non-increasing as well.

We deﬁne the set J (λ) as follows:

(cid:26)

J (λ) :=

x ∈ X :

≥ λ

(x)

.

(cid:27)

1
γ

dQ
dP

Now let us consider h(λ) − h(λ + (cid:15)) for any (cid:15) > 0. Note that J (λ + (cid:15)) ⊆ J (λ). We can write:

h(λ) − h(λ + (cid:15)) =

(cid:90)

(cid:90)

(cid:90)

=

=

(cid:18) 1
γ

J (λ)

(cid:19)

(cid:90)

− λ

dP −

dQ
dP
(cid:18) 1
γ
(cid:18) 1
γ

J (λ+(cid:15))
(cid:90)

(cid:19)

dP +

− λ

dQ
dP

dQ
dP

(cid:19)

J (λ)\J (λ+(cid:15))

J (λ)\J (λ+(cid:15))

J (λ+(cid:15))

(cid:18) 1
γ

(cid:19)

dP

dQ
dP
(cid:19)

dP

− (λ + (cid:15))

(cid:18)

(cid:15)

dQ
dP

− λ

dP + (cid:15) · Q(J (λ + (cid:15))).

Note that for x ∈ J (λ) \ J (λ + (cid:15)) we have

0 ≤

− λ

(x) < (cid:15)

(x).

1
γ

dQ
dP

dQ
dP

This gives the following:

which shows that h is continuous. Also

(cid:15) · Q(J (λ + (cid:15))) ≤ h(λ) − h(λ + (cid:15)) ≤ (cid:15) · Q(J (λ + (cid:15))) + (cid:15) · Q(J (λ) \ J (λ + (cid:15))) = (cid:15) · Q(J (λ)),

lim
(cid:15)→0+

h(λ + (cid:15)) − h(λ)
(cid:15)

= lim
(cid:15)→0+

−Q(J (λ + (cid:15))) = −Q(J (λ)).

It is obvious that h(0) = 1/γ and h ≤ γ−1 for λ ≥ 0. By Jensen’s inequality applied to the convex
+. So h(1) ≥ γ−1 − 1. We conclude that h may reach the

function x (cid:55)→ (a − x)+, we have h(λ) ≥ (cid:0)γ−1 − λ(cid:1)
value (1 − γ)/γ = γ−1 − 1 only on [1, +∞). Note that

h(λ) →

P

(X) = 0

=

≥ 0

as λ → ∞.

1
γ

(cid:18) dQ
dP

(cid:19)

∆
γ

Thus if ∆/γ > γ−1 −1 the equation h(λ) = γ−1 −1 has no solutions, as h is non-increasing. If ∆/γ = γ−1 −1
then either h(λ) > γ−1 − 1 for all λ ≥ 0 and we have no solutions or there is a ﬁnite λ(cid:48) ≥ 1 such that

25

h(λ(cid:48)) = γ−1 − 1, which means that the equation is also satisﬁed by all λ ≥ λ(cid:48), as h is continuous and
non-increasing. Finally, if ∆/γ < γ−1 − 1 then there is a unique λ† such that h(λ†) = γ−1 − 1, which follows
from the convexity of h.

Next we introduce some simple convenience lemma for comparing convex functions of random variables.

Lemma 8 Let f be a convex function, X, Y be real-valued random variables such that X ≤ Y a.s., and
c ∈ R be a constant such that9

Then we have the following lower bound:

E [min(c, Y )] = E [X] .

E [f (X) − f (min(c, Y ))] ≥ 0.

Proof We decompose the expectation with respect to the value of the min, and use the convexity of f :

f (X) − f (min(c, Y )) = 1[Y ≤c](f (X) − f (Y )) + 1[Y >c](f (X) − f (c))

≥ 1[Y ≤c]f (cid:48)(Y )(X − Y ) + 1[Y >c](X − c)f (cid:48)(c)
≥ 1[Y ≤c]f (cid:48)(c)(X − Y ) + 1[Y >c](X − c)f (cid:48)(c)
= Xf (cid:48)(c) − min(Y, c)f (cid:48)(c),

where we used the fact that f (cid:48) is non-decreasing in the previous to last step. Taking the expectation we get
the result.

Lemma 9 Let Pg, Pd be two ﬁxed distributions and β ∈ (0, 1). Assume

Pd

(cid:18) dPg
dPd

(cid:19)

= 0

< β.

Let M(Pd, β) be the set of all probability distributions T such that (1 − β)dT ≤ dPd. Then the following
minimization problem:

has the solution T ∗ with density

min
T ∈M(Pd,β)

Df (T (cid:107) Pg)

dT ∗ := min(dPd/(1 − β), λ†dPg),

where λ† is the unique value in [1, ∞) such that (cid:82) dT ∗ = 1.
Proof We will use Lemma 8 with X = dT (Z)/dPg(Z), Y = dPd(Z)/(cid:0)(1 − β)dPg(Z)(cid:1), and c = λ∗, Z ∼ Pg.
We need to verify that assumptions of Lemma 8 are satisﬁed. Obviously, Y ≥ X. We need to show that
there is a constant c such that

Rewriting this equation we get the following equivalent one:

(cid:90)

(cid:18)

min

c,

(cid:19)

dPd
(1 − β)dPg

dPg = 1.

(cid:90)

β =

(dPd − min (c(1 − β)Pg, dPd)) = (1 − β)

(21)

(cid:90) (cid:18) 1

1 − β

− c

(cid:19)

dPg
dPd

+

dPd.

Using the fact that

9Generally it is not guaranteed that such a constant c always exists. In this result we assume this is the case.

Pd

(cid:18) dPg
dPd

(cid:19)

= 0

< β

26

we may apply Lemma 7 and conclude that there is a unique c ∈ [1, ∞) satisfying (21), which we denote λ†.

To conclude the proof of Theorem 2, observe that from Lemma 9, by making the change of variable

T = (Pd − βQ)/(1 − β) we can rewrite the minimization problem as follows:

min
Q: βdQ≤dPd

Df ◦

Pg (cid:107)

(cid:18)

(cid:19)

Pd − βQ
1 − β

and we verify that the solution has the form dQ†
depend on f , the fact that we optimized Df ◦ is irrelevant and we get the same solution for Df .

(cid:0)dPd − λ†(1 − β)dPg

+. Since this solution does not

β = 1
β

(cid:1)

D f -Divergences

Jensen-Shannon This divergence corresponds to

Df (P (cid:107)Q) = JS(P, Q) =

(cid:90)

X

(cid:18) dP
dQ

f

(cid:19)

(x)

dQ(x)

f (u) = −(u + 1) log

+ u log u.

u + 1
2

with

Indeed,

(cid:90)

X
(cid:18) p(x)
q(x)

JS(P, Q) :=

q(x)

−

+ 1

log





(cid:19)

(cid:18) p(x)
q(x)



p(x)
q(x) + 1
2



 +

p(x)
q(x)

log

p(x)
q(z)



 dx

=

=

(cid:90)

X

(cid:90)

X

q(x)

log

2q(x)
p(x) + q(x)

+ log

p(x) log

2q(x)
p(x) + q(x)

2q(x)
p(x) + q(x)
2q(x)
p(x) + q(x)
(cid:19)

+

p(x)
q(z)

log

(cid:19)

dx

p(x)
q(z)

p(x)
q(z)

dx

+ q(x) log

+ p(x) log

(cid:18)

= KL

Q,

(cid:19)

P + Q
2

(cid:18)

+ KL

P,

P + Q
2

.

E Additional experimental results

At each iteration of the boosting approach, diﬀerent reweighting heuristics are possible. This section contains
more complete results about the following three heuristics:

– Constant β, and using the proposed reweighting scheme given β. See Table 3.

– Reweighting similar to “Cascade GAN” from [14], i.e. keep the top x% of examples, based on the

discriminator corresponding to the previous generator. See Table 4.

– Keep the top x% of examples, based on the discriminator corresponding to the mixture of all previous

generators. See Table 5.

Note that when properly tuned, each reweighting scheme outperforms the baselines, and have similar
performances when used with few iterations. However, they require an additional parameter to tune, and
are worse than the simple β = 1/t heuristic proposed above.

27

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

0.97 (0.9; 1.0)

0.88 (0.4; 1.0)

0.63 (0.5; 1.0)

0.72 (0.5; 0.8)

0.58 (0.4; 0.8)

0.59 (0.2; 0.7)

Best of T (T=3)

0.99 (1.0; 1.0)

0.96 (0.9; 1.0)

0.91 (0.7; 1.0)

0.80 (0.7; 0.9)

0.84 (0.7; 0.9)

0.70 (0.6; 0.8)

Best of T (T=10)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

0.98 (0.8; 1.0)

0.80 (0.8; 0.9)

0.87 (0.8; 0.9)

0.71 (0.7; 0.8)

Ensemble (T=3)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.93 (0.8; 1.0)

0.78 (0.6; 1.0)

0.85 (0.6; 1.0)

0.80 (0.6; 1.0)

Ensemble (T=10)

1.00 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

0.91 (0.8; 1.0)

0.88 (0.8; 1.0)

0.89 (0.7; 1.0)

Boosted (T=3)

0.99 (1.0; 1.0)

0.99 (0.9; 1.0)

0.98 (0.9; 1.0)

0.91 (0.8; 1.0)

0.91 (0.8; 1.0)

0.86 (0.7; 1.0)

Boosted (T=10)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

Best of T (T=3)

Best of T (T=10)

Ensemble (T=3)

Ensemble (T=10)

Boosted (T=3)

Boosted (T=10)

−4.49
(−5.4; −4.4)
−4.39
(−4.6; −4.3)
−4.34
(−4.4; −4.3)
−4.46
(−4.8; −4.4)
−4.52
(−4.7; −4.4)
−4.50
(−4.8; −4.4)
−4.55
(−4.6; −4.4)

−6.02
(−86.8; −5.3)
−5.40
(−24.3; −5.2)
−5.24
(−5.4; −5.2)
−5.59
(−6.6; −5.2)
−5.49
(−6.6; −5.2)
−5.32
(−5.8; −5.2)
−5.30
(−5.5; −5.2)

−16.03
(−59.6; −5.5)
−5.57
(−23.5; −5.4)
−5.45
(−5.6; −5.3)
−4.78
(−5.5; −4.6)
−4.98
(−6.5; −4.6)
−4.80
(−5.8; −4.6)
−5.07
(−5.6; −4.7)

−23.65
(−118.8; −5.7)
−9.91
(−35.8; −5.1)
−5.49
(−9.4; −5.0)
−14.71
(−51.9; −5.4)
−5.44
(−6.0; −5.2)
−5.39
(−19.3; −5.1)
−5.25
(−5.5; −4.6)

−126.87
(−250.4; −12.8)
−36.94
(−90.0; −9.7)
−9.72
(−17.3; −6.5)
−6.70
(−28.7; −5.5)
−5.82
(−6.4; −5.5)
−5.56
(−12.4; −5.2)
−5.03
(−5.5; −4.8)

−55.51
(−185.2; −11.2)
−19.12
(−59.2; −9.7)
−9.12
(−16.8; −6.6)
−8.59
(−25.4; −6.1)
−6.08
(−6.3; −5.7)
−8.03
(−28.7; −6.1)
−5.92
(−6.2; −5.6)

Table 2: Performance of the diﬀerent algorithms on varying number of mixtures of Gaussians. The reported
scores are the median and interval deﬁned by the 5% and 95% percentile (in parenthesis) (see Section
4.1.2), over 35 runs for each setting. The top table reports the coverage C, probability mass of Pd covered
by the 5th percentile of Pg deﬁned in Section 4.1.2. The bottom table reports the log likelihood of the
true data under the model Pg. Note that the 95% interval is not the usual conﬁdence interval measuring
the variance of the experiment itself, but rather measures the stability of the diﬀerent algorithms (would
remain even if each experiment was run an inﬁnite number of times). Both the ensemble and the boosting
approaches signiﬁcantly outperform the vanilla GAN even with just three iterations (i.e. just two additional
components). The boosting approach converges faster to the optimal coverage.

28

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

0.98 (0.9; 1.0)

0.86 (0.5; 1.0)

0.66 (0.5; 1.0)

0.61 (0.5; 0.8)

0.55 (0.4; 0.7)

0.58 (0.3; 0.8)

Boosted (T=3)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.98 (0.9; 1.0)

0.93 (0.8; 1.0)

0.97 (0.8; 1.0)

0.87 (0.6; 1.0)

Boosted (T=10)

1.00 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

0.99 (0.9; 1.0)

0.99 (0.8; 1.0)

0.97 (0.8; 1.0)

Beta0.2 (T=3)

0.99 (1.0; 1.0)

0.97 (0.9; 1.0)

0.97 (0.9; 1.0)

0.95 (0.8; 1.0)

0.96 (0.7; 1.0)

0.88 (0.7; 1.0)

Beta0.2 (T=10)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (0.9; 1.0)

1.00 (0.9; 1.0)

Beta0.3 (T=3)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.98 (0.9; 1.0)

0.96 (0.8; 1.0)

0.96 (0.6; 1.0)

0.88 (0.7; 1.0)

Beta0.3 (T=10)

1.00 (1.0; 1.0)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (0.9; 1.0)

0.99 (0.9; 1.0)

Beta0.4 (T=3)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.95 (0.9; 1.0)

0.94 (0.8; 1.0)

0.89 (0.7; 1.0)

0.89 (0.7; 1.0)

Beta0.4 (T=10)

0.99 (1.0; 1.0)

0.99 (0.9; 1.0)

0.96 (0.9; 1.0)

0.97 (0.8; 1.0)

0.99 (0.8; 1.0)

0.90 (0.8; 1.0)

Beta0.5 (T=3)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.97 (0.8; 1.0)

0.82 (0.8; 1.0)

0.86 (0.7; 1.0)

0.81 (0.6; 1.0)

Beta0.5 (T=10)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.97 (0.9; 1.0)

0.84 (0.8; 1.0)

0.87 (0.7; 1.0)

0.91 (0.8; 1.0)

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

Boosted (T=3)

Boosted (T=10)

Beta0.2 (T=3)

Beta0.2 (T=10)

Beta0.3 (T=3)

Beta0.3 (T=10)

Beta0.4 (T=3)

Beta0.4 (T=10)

Beta0.5 (T=3)

Beta0.5 (T=10)

−4.50
(−5.0; −4.4)
−4.56
(−4.9; −4.4)
−4.56
(−4.7; −4.5)
−4.52
(−4.8; −4.4)
−4.58
(−4.8; −4.5)
−4.60
(−4.9; −4.4)
−4.57
(−4.8; −4.4)
−4.62
(−4.9; −4.4)
−4.49
(−4.7; −4.4)
−4.60
(−4.9; −4.4)
−4.62
(−4.8; −4.4)

−5.65
(−72.7; −5.1)
−5.55
(−5.9; −5.2)
−5.46
(−5.6; −5.3)
−5.31
(−5.6; −5.1)
−5.30
(−5.5; −5.2)
−5.34
(−5.7; −5.2)
−5.37
(−5.5; −5.2)
−5.36
(−5.6; −5.1)
−5.40
(−5.7; −5.3)
−5.40
(−5.7; −5.3)
−5.43
(−5.7; −5.2)

−19.63
(−62.1; −5.6)
−5.01
(−6.7; −4.7)
−5.08
(−5.8; −4.7)
−4.85
(−6.3; −4.6)
−4.94
(−6.6; −4.6)
−5.41
(−5.7; −5.1)
−5.27
(−5.6; −5.0)
−4.74
(−5.3; −4.6)
−5.08
(−6.9; −4.7)
−4.77
(−5.4; −4.6)
−5.12
(−6.6; −4.7)

−28.16
(−293.1; −16.3)
−5.49
(−18.7; −4.9)
−5.04
(−5.5; −4.6)
−5.33
(−14.4; −4.8)
−5.23
(−5.5; −4.7)
−5.33
(−12.9; −4.9)
−5.26
(−5.6; −5.0)
−5.34
(−26.2; −4.9)
−5.49
(−5.9; −5.2)
−5.63
(−24.5; −5.2)
−5.48
(−8.4; −5.1)

−56.94
(−248.1; −14.3)
−5.60
(−14.5; −5.0)
−5.51
(−5.9; −5.1)
−5.68
(−26.2; −5.2)
−5.60
(−6.0; −5.3)
−5.68
(−11.0; −5.4)
−5.71
(−6.0; −5.3)
−5.77
(−37.3; −5.1)
−5.43
(−6.0; −5.1)
−6.05
(−17.9; −5.5)
−5.85
(−6.1; −5.3)

−71.11
(−184.8; −12.5)
−6.86
(−47.3; −5.6)
−5.51
(−6.0; −5.2)
−6.13
(−32.7; −5.7)
−5.98
(−6.1; −5.7)
−6.41
(−29.2; −5.6)
−5.82
(−6.1; −5.4)
−12.37
(−75.9; −5.9)
−5.68
(−6.2; −5.2)
−8.29
(−23.1; −6.1)
−6.31
(−7.7; −6.0)

Table 3: Performance with constant β, exploring a range of possible values. The reported scores are the
median and interval deﬁned by the 5% and 95% percentile (in parenthesis) (see Section 4.1.2), over 35 runs
for each setting. The top table reports the coverage C, probability mass of Pd covered by the 5th percentile
of Pg deﬁned in Section 4.1.2. The bottom table reports the log likelihood of the true data under Pg.

29

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

0.96 (0.9; 1.0)

0.90 (0.5; 1.0)

0.65 (0.5; 1.0)

0.61 (0.5; 0.8)

0.69 (0.3; 0.8)

0.59 (0.3; 0.7)

Boosted (T=3)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.98 (0.9; 1.0)

0.93 (0.8; 1.0)

0.97 (0.8; 1.0)

0.87 (0.6; 1.0)

Boosted (T=10)

1.00 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

0.99 (0.9; 1.0)

0.99 (0.8; 1.0)

0.97 (0.8; 1.0)

TopKLast0.1 (T=3)

0.98 (0.9; 1.0)

0.93 (0.8; 1.0)

0.89 (0.6; 1.0)

0.72 (0.5; 1.0)

0.68 (0.5; 0.9)

0.51 (0.4; 0.7)

TopKLast0.1 (T=10)

0.99 (0.9; 1.0)

0.97 (0.8; 1.0)

0.90 (0.7; 1.0)

0.67 (0.4; 0.9)

0.61 (0.5; 0.8)

0.58 (0.4; 0.8)

TopKLast0.3 (T=3)

0.99 (0.9; 1.0)

0.97 (0.9; 1.0)

0.93 (0.7; 1.0)

0.81 (0.7; 1.0)

0.84 (0.7; 1.0)

0.78 (0.5; 1.0)

TopKLast0.3 (T=10)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.95 (0.7; 1.0)

0.94 (0.7; 1.0)

0.89 (0.7; 1.0)

0.88 (0.7; 1.0)

TopKLast0.5 (T=3)

0.98 (0.9; 1.0)

0.98 (0.9; 1.0)

0.95 (0.9; 1.0)

0.95 (0.8; 1.0)

0.86 (0.7; 1.0)

0.86 (0.6; 0.9)

TopKLast0.5 (T=10)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.98 (1.0; 1.0)

0.99 (0.8; 1.0)

0.99 (0.8; 1.0)

1.00 (0.8; 1.0)

TopKLast0.7 (T=3)

0.98 (1.0; 1.0)

0.98 (0.9; 1.0)

0.94 (0.9; 1.0)

0.83 (0.7; 1.0)

0.87 (0.6; 1.0)

0.82 (0.7; 1.0)

TopKLast0.7 (T=10)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

0.98 (0.8; 1.0)

0.99 (0.9; 1.0)

0.95 (0.8; 1.0)

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

Boosted (T=3)

Boosted (T=10)

TopKLast0.1 (T=3)

TopKLast0.1 (T=10)

TopKLast0.3 (T=3)

TopKLast0.3 (T=10)

TopKLast0.5 (T=3)

TopKLast0.5 (T=10)

TopKLast0.7 (T=3)

TopKLast0.7 (T=10)

−4.94
(−5.5; −4.4)
−4.56
(−4.9; −4.4)
−4.56
(−4.7; −4.5)
−4.98
(−5.2; −4.7)
−4.98
(−5.3; −4.7)
−4.73
(−5.1; −4.5)
−4.62
(−4.8; −4.5)
−4.59
(−4.9; −4.4)
−4.59
(−4.8; −4.5)
−4.56
(−4.7; −4.4)
−4.52
(−4.7; −4.5)

−6.18
(−51.7; −5.6)
−5.55
(−5.9; −5.2)
−5.46
(−5.6; −5.3)
−5.64
(−6.1; −5.4)
−5.57
(−5.9; −5.3)
−5.48
(−6.0; −5.2)
−5.41
(−5.7; −5.2)
−5.29
(−5.7; −5.2)
−5.35
(−5.6; −5.2)
−5.37
(−5.5; −5.2)
−5.29
(−5.4; −5.2)

−31.85
(−100.3; −5.8)
−5.01
(−6.7; −4.7)
−5.08
(−5.8; −4.7)
−5.70
(−6.3; −5.2)
−5.37
(−6.0; −5.0)
−5.22
(−5.7; −4.8)
−4.90
(−5.2; −4.7)
−5.41
(−5.9; −4.9)
−5.12
(−5.5; −4.9)
−5.05
(−11.1; −4.7)
−5.05
(−6.6; −4.7)

−47.73
(−155.1; −14.2)
−5.49
(−18.7; −4.9)
−5.04
(−5.5; −4.6)
−5.39
(−38.4; −5.0)
−5.57
(−45.1; −4.7)
−5.42
(−21.6; −5.0)
−5.24
(−5.8; −4.9)
−5.48
(−18.5; −5.0)
−5.35
(−5.6; −4.8)
−5.63
(−43.1; −5.0)
−5.38
(−5.9; −5.1)

−107.36
(−390.8; −14.8)
−5.60
(−14.5; −5.0)
−5.51
(−5.9; −5.1)
−7.00
(−66.6; −5.4)
−7.34
(−16.1; −5.3)
−5.76
(−13.6; −5.1)
−5.71
(−6.2; −5.1)
−5.82
(−15.6; −5.2)
−5.34
(−5.8; −4.9)
−5.99
(−24.8; −5.4)
−5.77
(−6.3; −5.3)

−59.19
(−264.3; −18.8)
−6.86
(−47.3; −5.6)
−5.51
(−6.0; −5.2)
−12.70
(−44.2; −6.7)
−8.86
(−27.6; −5.5)
−7.26
(−36.2; −5.5)
−5.75
(−7.4; −5.1)
−6.78
(−18.7; −6.0)
−6.00
(−6.3; −5.6)
−7.76
(−25.2; −5.9)
−6.10
(−6.4; −6.0)

Table 4: Reweighting similar to “Cascade GAN” from [14], i.e. keep the top r fraction of examples, based on
the discriminator corresponding to the previous generator. The mixture weights are all equal (i.e. β = 1/t).
The reported scores are the median and interval deﬁned by the 5% and 95% percentile (in parenthesis) (see
Section 4.1.2), over 35 runs for each setting. The top table reports the coverage C, probability mass of Pd
covered by the 5th percentile of Pg deﬁned in Section 4.1.2. The bottom table reports the log likelihood of
the true data under Pg.

30

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

0.97 (0.9; 1.0)

0.77 (0.5; 1.0)

0.65 (0.5; 0.9)

0.70 (0.5; 0.8)

0.61 (0.5; 0.8)

0.58 (0.3; 0.8)

Boosted (T=3)

0.99 (1.0; 1.0)

0.99 (0.9; 1.0)

0.97 (0.9; 1.0)

0.95 (0.8; 1.0)

0.91 (0.8; 1.0)

0.89 (0.8; 1.0)

Boosted (T=10)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

TopK0.1 (T=3)

0.98 (0.9; 1.0)

0.98 (0.8; 1.0)

0.91 (0.7; 1.0)

0.84 (0.7; 1.0)

0.80 (0.5; 0.9)

0.60 (0.4; 0.7)

TopK0.1 (T=10)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

0.98 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

0.96 (0.8; 1.0)

TopK0.3 (T=3)

0.98 (0.9; 1.0)

0.98 (0.9; 1.0)

0.95 (0.9; 1.0)

0.95 (0.8; 1.0)

0.84 (0.6; 1.0)

0.79 (0.5; 1.0)

TopK0.3 (T=10)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

0.98 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

TopK0.5 (T=3)

0.99 (0.9; 1.0)

0.99 (1.0; 1.0)

0.96 (0.9; 1.0)

0.98 (0.8; 1.0)

0.88 (0.7; 1.0)

0.88 (0.6; 1.0)

TopK0.5 (T=10)

1.00 (1.0; 1.0)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

TopK0.7 (T=3)

0.98 (1.0; 1.0)

0.98 (0.9; 1.0)

0.94 (0.8; 1.0)

0.84 (0.8; 1.0)

0.86 (0.7; 1.0)

0.81 (0.7; 1.0)

TopK0.7 (T=10)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (0.8; 1.0)

1.00 (0.9; 1.0)

1.00 (0.9; 1.0)

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

Boosted (T=3)

Boosted (T=10)

TopK0.1 (T=3)

TopK0.1 (T=10)

TopK0.3 (T=3)

TopK0.3 (T=10)

TopK0.5 (T=3)

TopK0.5 (T=10)

TopK0.7 (T=3)

TopK0.7 (T=10)

−4.61
(−5.5; −4.4)
−4.59
(−4.9; −4.4)
−4.61
(−4.7; −4.5)
−4.93
(−5.3; −4.7)
−4.60
(−4.8; −4.5)
−4.65
(−4.9; −4.4)
−4.56
(−4.8; −4.5)
−4.60
(−4.8; −4.5)
−4.59
(−4.7; −4.5)
−4.60
(−5.0; −4.4)
−4.59
(−4.7; −4.5)

−5.92
(−94.2; −5.2)
−5.32
(−5.7; −5.2)
−5.30
(−5.4; −5.2)
−5.85
(−6.2; −5.4)
−5.47
(−5.7; −5.3)
−5.40
(−5.9; −5.3)
−5.32
(−5.5; −5.2)
−5.34
(−5.6; −5.2)
−5.31
(−5.4; −5.2)
−5.44
(−5.6; −5.2)
−5.34
(−5.5; −5.2)

−12.40
(−53.1; −5.3)
−5.60
(−5.8; −5.5)
−5.48
(−5.6; −5.2)
−5.38
(−5.7; −5.0)
−4.81
(−5.1; −4.7)
−4.98
(−6.2; −4.7)
−5.07
(−5.9; −4.7)
−5.34
(−5.7; −5.0)
−5.13
(−5.5; −4.9)
−5.62
(−6.0; −5.4)
−5.51
(−5.6; −5.4)

−59.62
(−154.6; −9.8)
−5.40
(−24.2; −4.5)
−4.84
(−5.1; −4.3)
−5.34
(−5.8; −4.8)
−4.90
(−5.3; −4.2)
−5.25
(−11.4; −4.7)
−5.08
(−5.4; −4.5)
−5.42
(−19.0; −5.0)
−5.35
(−5.7; −4.8)
−5.49
(−22.2; −5.0)
−5.35
(−5.8; −5.0)

−66.95
(−191.5; −9.7)
−5.71
(−14.0; −5.1)
−5.25
(−5.9; −4.8)
−5.79
(−32.1; −5.2)
−4.85
(−5.6; −4.1)
−5.96
(−28.0; −5.5)
−5.16
(−5.9; −4.9)
−5.59
(−34.7; −4.9)
−5.33
(−5.8; −4.8)
−5.64
(−27.7; −5.3)
−5.32
(−6.0; −5.1)

−63.49
(−431.6; −14.5)
−6.96
(−17.1; −5.9)
−5.95
(−6.1; −5.5)
−7.09
(−20.7; −5.9)
−4.57
(−5.3; −4.2)
−7.34
(−25.4; −5.9)
−5.82
(−6.2; −5.3)
−6.15
(−14.8; −5.6)
−5.72
(−6.2; −5.3)
−7.17
(−22.5; −6.0)
−6.11
(−6.4; −5.9)

Table 5: Reweighting using the top r fraction of examples, based on the discriminator corresponding to the
mixture of all previous generators. The mixture weights are all equal (i.e. β = 1/t). The reported scores
are the median and interval deﬁned by the 5% and 95% percentile (in parenthesis) (see Section 4.1.2), over
35 runs for each setting. The top table reports the coverage C, probability mass of Pd covered by the 5th
percentile of Pg deﬁned in Section 4.1.2. The bottom table reports the log likelihood of the true data under
Pg.

31

7
1
0
2
 
y
a
M
 
4
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
6
8
3
2
0
.
1
0
7
1
:
v
i
X
r
a

AdaGAN: Boosting Generative Models

Ilya Tolstikhin1, Sylvain Gelly2, Olivier Bousquet2, Carl-Johann Simon-Gabriel1, and
Bernhard Sch¨olkopf1

1Max Planck Institute for Intelligent Systems
2Google Brain

Abstract

Generative Adversarial Networks (GAN) [1] are an eﬀective method for training generative models of
complex data such as natural images. However, they are notoriously hard to train and can suﬀer from
the problem of missing modes where the model is not able to produce examples in certain regions of the
space. We propose an iterative procedure, called AdaGAN, where at every step we add a new component
into a mixture model by running a GAN algorithm on a reweighted sample. This is inspired by boosting
algorithms, where many potentially weak individual predictors are greedily aggregated to form a strong
composite predictor. We prove that such an incremental procedure leads to convergence to the true
distribution in a ﬁnite number of steps if each step is optimal, and convergence at an exponential rate
otherwise. We also illustrate experimentally that this procedure addresses the problem of missing modes.

1 Introduction

Imagine we have a large corpus, containing unlabeled pictures of animals, and our task is to build a generative
probabilistic model of the data. We run a recently proposed algorithm and end up with a model which
produces impressive pictures of cats and dogs, but not a single giraﬀe. A natural way to ﬁx this would be
to manually remove all cats and dogs from the training set and run the algorithm on the updated corpus.
The algorithm would then have no choice but to produce new animals and, by iterating this process until
there’s only giraﬀes left in the training set, we would arrive at a model generating giraﬀes (assuming suﬃcient
sample size). At the end, we aggregate the models obtained by building a mixture model. Unfortunately, the
described meta-algorithm requires manual work for removing certain pictures from the unlabeled training
set at every iteration.

Let us turn this into an automatic approach, and rather than including or excluding a picture, put
continuous weights on them. To this end, we train a binary classiﬁer to separate “true” pictures of the
original corpus from the set of “synthetic” pictures generated by the mixture of all the models trained so
far. We would expect the classiﬁer to make conﬁdent predictions for the true pictures of animals missed
by the model (giraﬀes), because there are no synthetic pictures nearby to be confused with them. By
a similar argument, the classiﬁer should make less conﬁdent predictions for the true pictures containing
animals already generated by one of the trained models (cats and dogs). For each picture in the corpus,
we can thus use the classiﬁer’s conﬁdence to compute a weight which we use for that picture in the next
iteration, to be performed on the re-weighted dataset.

The present work provides a principled way to perform this re-weighting, with theoretical guarantees

showing that the resulting mixture models indeed approach the true data distribution.1

Before discussing how to build the mixture, let us consider the question of building a single generative
model. A recent trend in modelling high dimensional data such as natural images is to use neural net-
works [2, 1]. One popular approach are Generative Adversarial Networks (GAN) [1], where the generator

1Note that the term “mixture” should not be interpreted to imply that each component models only one mode: the models

to be combined into a mixture can themselves cover multiple modes.

1

is trained adversarially against a classiﬁer, which tries to diﬀerentiate the true from the generated data.
While the original GAN algorithm often produces realistically looking data, several issues were reported in
the literature, among which the missing modes problem, where the generator converges to only one or a few
modes of the data distribution, thus not providing enough variability in the generated data. This seems
to match the situation described earlier, which is why we will most often illustrate our algorithm with a
GAN as the underlying base generator. We call it AdaGAN, for Adaptive GAN, but we could actually use
any other generator: a Gaussian mixture model, a VAE [2], a WGAN [3], or even an unrolled [4] or mode-
regularized GAN [5], which were both already speciﬁcally developed to tackle the missing mode problem.
Thus, we do not aim at improving the original GAN or any other generative algorithm. We rather propose
and analyse a meta-algorithm that can be used on top of any of them. This meta-algorithm is similar in
spirit to AdaBoost [6] in the sense that each iteration corresponds to learning a “weak” generative model
(e.g., GAN) with respect to a re-weighted data distribution. The weights change over time to focus on the
“hard” examples, i.e. those that the mixture has not been able to properly generate so far.

1.1 Boosting via Additive Mixtures

Motivated by the problem of missing modes, in this work we propose to use multiple generative models
combined into a mixture. These generative models are trained iteratively by adding, at each step, another
model to the mixture that should hopefully cover the areas of the space not covered by the previous mixture
components.2 We show analytically that the optimal next mixture component can be obtained by reweighting
the true data, and thus propose to use the reweighted data distribution as the target for the optimization
of the next mixture components. This leads us naturally to a meta-algorithm, which is similar in spirit to
AdaBoost in the sense that each iteration corresponds to learning a “weak” generative model (e.g., GAN)
with respect to a reweighted data distribution. The latter adapts over time to focus on the “hard” examples,
i.e. those that the mixture has not been able to properly generate thus far.

Before diving into the technical details we provide an informal intuitive discussion of our new meta-
algorithm, which we call AdaGAN (a shorthand for Adaptive GAN, similar to AdaBoost). The pseudocode
is presented in Algorithm 1.

On the ﬁrst step we run the GAN algorithm (or some other generative model) in the usual way and
initialize our generative model with the resulting generator G1. On every t-th step we (a) pick the mixture
weight βt for the next component, (b) update weights Wt of examples from the training set in such a way to
bias the next component towards “hard” ones, not covered by the current mixture of generators Gt−1, (c) run
the GAN algorithm, this time importance sampling mini-batches according to the updated weights Wt,
t , and ﬁnally (d) update our mixture of generators Gt = (1 − βt)Gt−1 + βtGc
resulting in a new generator Gc
t
(notation expressing the mixture of Gt−1 and Gc
t with probabilities 1 − βt and βt). This procedure outputs T
generator functions Gc
T and T corresponding non-negative weights α1, . . . , αT , which sum to one. For
sampling from the resulting model we ﬁrst deﬁne a generator Gc
i , by sampling the index i from a multinomial
distribution with parameters α1, . . . , αT , and then we return Gc
i (Z), where Z ∼ PZ is a standard latent noise
variable used in the GAN literature.

1, . . . , Gc

The eﬀect of the described procedure is illustrated in a toy example in Figure 1. On the left images,
the red dots are the training (true data) points, the blue dots are points sampled from the model mixture
of generators Gt. The background colour gives the density of the distribution corresponding to Gt, non
zero around the generated points, (almost) zero everywhere else. On the right images, the color corresponds
to the weights of training points, following the reweighting scheme proposed in this work. The top row
corresponds to the ﬁrst iteration of AdaGAN, and the bottom row to the second iteration. After the ﬁrst
iteration (the result of the vanilla GAN), we see that only the top left mode is covered, while the three
other modes are not covered at all. The new weights (top right) show that the examples from covered mode
are aggressively downweighted. After the second iteration (bottom left), the combined generator can then
generate two modes.

2Note that the term “mixture” should not be interpreted to imply that each component models only one mode: the models

to be combined into a mixture can themselves cover multiple modes already.

2

Algorithm 1: AdaGAN, a meta-algorithm to construct a “strong” mixture of T individual GANs,
trained sequentially. The mixture weight schedule ChooseMixtureWeight and the training set reweight-
ing schedule UpdateTrainingWeights should be provided by the user. Section 3 gives a complete instance
of this family.

Input: Training sample SN := {X1, . . . , XN }.

Output: Mixture generative model G = GT .

Train vanilla GAN:

W1 = (1/N, . . . , 1/N )

G1 = GAN(SN , Wt)

for t = 2, . . . , T do

#Choose a mixture weight for the next component

βt = ChooseMixtureWeight(t)

#Update weights of training examples

Wt = UpdateTrainingWeights(Gt−1, SN , βt)
#Train t-th “weak” component generator Gc
t
Gc

t = GAN(SN , Wt)

#Update the overall generative model
#Notation below means forming a mixture of Gt−1 and Gc
t .
Gt = (1 − βt)Gt−1 + βtGc
t

end for

3

Although motivated by GANs, we cast our results in the general framework of the minimization of an
f -divergence (cf. [7]) with respect to an additive mixture of distributions. We also note that our approach
may be combined with diﬀerent “weak” generative models, including but not limited to GAN.

Figure 1: A toy illustration of the missing mode problem and the eﬀect of sample reweighting, following the
discussion in Section 1.1. On the left images, the red dots are the training (true data) points, the blue dots
are points sampled from the model mixture of generators Gt. On the right images, the color corresponds
to the weights of training points, following the reweighting scheme proposed in this work. The top row
corresponds to the ﬁrst iteration of AdaGAN, and the bottom row to the second iteration.

1.2 Related Work

Several authors [8, 9, 10] have proposed to use boosting techniques in the context of density estimation by
incrementally adding components in the log domain. In particular, the work of Grover and Ermon [10], done
in parallel to and independent of ours, is applying this idea to GANs. A major downside of these approaches
is that the resulting mixture is a product of components and sampling from such a model is nontrivial (at
least when applied to GANs where the model density is not expressed analytically) and requires to use
techniques such as Annealed Importance Sampling [11] for the normalization.

Rosset and Segal [12] proposed to use an additive mixture model in the case where the log likelihood
can be computed. They derived the update rule via computing the steepest descent direction when adding
a component with inﬁnitesimal weight. This leads to an update rule which is degenerate if the generative
model can produce arbitrarily concentrated distributions (indeed the optimal component is just a Dirac
distribution) which is thus not suitable for the GAN setting. Moreover, their results do not apply once the

4

weight β becomes non-inﬁnitesimal. In contrast, for any ﬁxed weight of the new component our approach
gives the overall optimal update (rather than just the best direction), and applies to any f -divergence.
Remarkably, in both theories, improvements of the mixture are guaranteed only if the new “weak” learner
is still good enough (see Conditions 14&15)

Similarly, Barron and Li [13] studied the construction of mixtures minimizing the Kullback divergence
and proposed a greedy procedure for doing so. They also proved that under certain conditions, ﬁnite mixtures
can approximate arbitrary mixtures at a rate 1/k where k is the number of components in the mixture when
the weight of each newly added component is 1/k. These results are speciﬁc to the Kullback divergence but
are consistent with our more general results.

Wang et al. [14] propose an additive procedure similar to ours but with a diﬀerent reweighting scheme,
which is not motivated by a theoretical analysis of optimality conditions. On every new iteration the authors
propose to run GAN on the top k training examples with maximum value of the discriminator from the last
iteration. Empirical results of Section 4 show that this heuristic often fails to address the missing modes
problem.

Finally, many papers investigate completely diﬀerent approaches for addressing the same issue by directly
modifying the training objective of an individual GAN. For instance, Che et al. [5] add an autoencoding cost
to the training objective of GAN, while Metz et al. [4] allow the generator to “look few steps ahead” when
making a gradient step.

The paper is organized as follows. In Section 2 we present our main theoretical results regarding opti-
mization of mixture models under general f -divergences. In particular we show that it is possible to build
an optimal mixture in an incremental fashion, where each additional component is obtained by applying a
GAN-style procedure with a reweighted distribution. In Section 2.5 we show that if the GAN optimization
at each step is perfect, the process converges to the true data distribution at exponential rate (or even
in a ﬁnite number of steps, for which we provide a necessary and suﬃcient condition). Then we show in
Section 2.6 that imperfect GAN solutions still lead to the exponential rate of convergence under certain
“weak learnability” conditions. These results naturally lead us to a new boosting-style iterative procedure
for constructing generative models, which is combined with GAN in Section 3, resulting in a new algorithm
called AdaGAN. Finally, we report initial empirical results in Section 4, where we compare AdaGAN with
several benchmarks, including original GAN, uniform mixture of multiple independently trained GANs, and
iterative procedure of Wang et al. [14].

2 Minimizing f -divergence with Additive Mixtures

In this section we derive a general result on the minimization of f -divergences over mixture models.

2.1 Preliminaries and notations

In this work we will write Pd and Pmodel to denote a real data distribution and our approximate model
distribution, respectively, both deﬁned over the data space X .

Generative Density Estimation In the generative approach to density estimation, instead of building a
probabilistic model of the data directly, one builds a function G : Z → X that transforms a ﬁxed probability
distribution PZ (often called the noise distribution) over a latent space Z into a distribution over X . Hence
Pmodel is the pushforward of PZ, i.e. Pmodel(A) = PZ(G−1(A)). Because of this deﬁnition, it is generally
impossible to compute the density dPmodel(x), hence it is not possible to compute the log-likelihood of the
training data under the model. However, if PZ is a distribution from which one can sample, it is easy to also
sample from Pmodel (simply sampling from PZ and applying G to each example gives a sample from Pmodel).
So the problem of generative density estimation becomes a problem of ﬁnding a function G such that
Pmodel looks like Pd in the sense that samples from Pmodel and from Pd look similar. Another way to state
this problem is to say that we are given a measure of similarity between distributions D(Pmodel(cid:107)Pd) which

5

can be estimated from samples of those distributions, and thus approximately minimized over a class G of
functions.

f -Divergences
of the data we will use an f -divergence deﬁned in the following way:

In order to measure the agreement between the model distribution and the true distribution

Df (Q(cid:107)P ) :=

(cid:90)

(cid:18) dQ
dP

f

(cid:19)

(x)

dP (x)

(1)

(2)

(3)

for any pair of distributions P, Q with densities dP , dQ with respect to some dominating reference measure µ.
In this work we assume that the function f is convex, deﬁned on (0, ∞), and satisﬁes f (1) = 0. The deﬁnition
of Df holds for both continuous and discrete probability measures and does not depend on speciﬁc choice
of µ.3 It is easy to verify that Df ≥ 0 and it is equal to 0 when P = Q. Note that Df is not symmetric,
but Df (P (cid:107)Q) = Df ◦ (Q(cid:107)P ) for f ◦(x) := xf (1/x) and any P and Q. The f -divergence is symmetric when
f (x) = f ◦(x) for all x ∈ (0, ∞), as in this case Df (P, Q) = Df (Q, P ).

We also note that the divergences corresponding to f (x) and f (x) + C · (x − 1) are identical for any
constant C. In some cases, it is thus convenient to work with f0(x) := f (x) − (x − 1)f (cid:48)(1), (where f (cid:48)(1) is
any subderivative of f at 1) as Df (Q(cid:107)P ) = Df0(Q(cid:107)P ) for all Q and P , while f0 is nonnegative, nonincreasing
on (0, 1], and nondecreasing on (1, ∞). In the remainder, we will denote by F the set of functions that are
suitable for f -divergences, i.e. the set of functions of the form f0 for any convex f with f (1) = 0.

Classical examples of f -divergences include the Kullback-Leibler divergence (obtained for f (x) = − log x,
f0(x) = − log x + x − 1), the reverse Kullback-Leibler divergence (obtained for f (x) = x log x, f0(x) =
x log x − x + 1), the Total Variation distance (f (x) = f0(x) = |x − 1|), and the Jensen-Shannon divergence
(f (x) = f0(x) = −(x + 1) log x+1
2 + x log x). More details can be found in Appendix D. Other examples can
be found in [7]. For further details on f -divergences we refer to Section 1.3 of [15] and [16].

GAN and f -divergences We now explain the connection between the GAN algorithm and f -divergences.
The original GAN algorithm [1] consists in optimizing the following criterion:

min
G

max
D

EPd [log D(X)] + EPZ [log(1 − D(G(Z)))] ,

where D and G are two functions represented by neural networks, and this optimization is actually performed
on a pair of samples (one being the training sample, the other one being created from the chosen distribu-
tion PZ), which corresponds to approximating the above criterion by using the empirical distributions. For
a ﬁxed G, it has been shown in [1] that the optimal D for (2) is given by D∗(x) =
dPd(x)+dPg(x) and plugging
this optimal value into (2) gives the following:

dPd(x)

min
G

− log(4) + 2JS(Pd (cid:107) Pg) ,

where JS is the Jensen-Shannon divergence. Of course, the actual GAN algorithm uses an approximation
to D∗ which is computed by training a neural network on a sample, which means that the GAN algorithm
can be considered to minimize an approximation of (3)4. This point of view can be generalized by plugging
another f -divergence into (3), and it turns out that other f -divergences can be written as the solution to
a maximization of a criterion similar to (2). Indeed, as demonstrated in [7], any f -divergence between Pd
and Pg can be seen as the optimal value of a quantity of the form EPd [f1(D(X))] + EPg [f2(D(G(Z)))] for
appropriate f1 and f2, and thus can be optimized by the same adversarial training technique.

There is thus a strong connection between adversarial training of generative models and minimization of

f -divergences, and this is why we cast the results of this section in the context of general f -divergences.

3The integral in (1) is well deﬁned (but may take inﬁnite values) even if P (dQ = 0) > 0 or Q(dP = 0) > 0. In this case the
integral is understood as Df (Q(cid:107)P ) = (cid:82) f (dQ/dP )1[dP (x)>0,dQ(x)>0]dP (x) + f (0)P (dQ = 0) + f ◦(0)Q(dP = 0), where both
f (0) and f ◦(0) may take value ∞ [15]. This is especially important in case of GAN, where it is impossible to constrain Pmodel
to be absolutely continuous with respect to Pd or vice versa.

4Actually the criterion that is minimized is an empirical version of a lower bound of the Jensen-Shannon divergence.

6

Hilbertian Metrics As demonstrated in [17, 18], several commonly used symmetric f -divergences are
Hilbertian metrics, which in particular means that their square root satisﬁes the triangle inequality. This
is true for the Jensen-Shannon divergence5 as well as for the Hellinger distance and the Total Variation
among others. We will denote by FH the set of f functions such that Df is a Hilbertian metric. For those
divergences, we have Df (P (cid:107)Q) ≤ ((cid:112)Df (P (cid:107)R) + (cid:112)Df (R(cid:107)Q))2.

Generative Mixture Models
a mixture model of the following form:

In order to model complex data distributions, it can be convenient to use

P T

model :=

αiPi,

T
(cid:88)

i=1

where αi ≥ 0, (cid:80)
i αi = 1, and each of the T components is a generative density model. This is very natural
in the generative context, since sampling from a mixture corresponds to a two-step sampling, where one ﬁrst
picks the mixture component (according to the multinomial distribution whose parameters are the αi) and
then samples from it. Also, this allows to construct complex models from simpler ones.

2.2 Incremental Mixture Building

As discussed earlier, in the context of generative modeling, we are given a measure of similarity between
distributions. We will restrict ourselves to the case of f -divergences.
Indeed, for any f -divergence, it
is possible (as explained for example in [7]) to estimate Df (Q (cid:107) P ) from two samples (one from Q, one
from P ) by training a “discriminator” function, i.e. by solving an optimization problem (which is a binary
classiﬁcation problem in the case where the divergence is symmetric6).
It turns out that the empirical
estimate ˆD of Df (Q (cid:107) P ) thus obtained provides a criterion for optimizing Q itself. Indeed, ˆD is a function
of Y1, . . . , Yn ∼ Q and X1, . . . , Xn ∼ P , where Yi = G(Zi) for some mapping function G. Hence it is possible
to optimize ˆD with respect to G (and in particular compute gradients with respect to the parameters of G
if G comes from a smoothly parametrized model such as a neural network).

In this work we thus assume that, given an i.i.d. sample from any unknown distribution P we can construct

a simple model Q ∈ G which approximately minimizes

min
Q∈G

Df (Q (cid:107) P ).

Instead of just modelling the data with a single distribution, we now want to model it with a mixture
of the form (4) where each Pi is obtained by a training procedure of the form (5) with (possibly) diﬀerent
target distributions P for each i.

A natural way to build a mixture is to do it incrementally: we train the ﬁrst model P1 to minimize
Df (P1 (cid:107) Pd) and set the corresponding weight to α1 = 1, leading to P 1
model = P1. Then after having trained
t components P1, . . . , Pt ∈ G we can form the (t + 1)-st mixture model by adding a new component Q with
weight β as follows:

P t+1

model :=

(1 − β)αiPi + βQ.

t
(cid:88)

i=1

Df ((1 − β)Pg + βQ (cid:107) Pd),

We are going to choose β ∈ [0, 1] and Q ∈ G greedily, while keeping all the other parameters of the generative
model ﬁxed, so as to minimize

where we denoted Pg := P t

model the current generative mixture model before adding the new component.

We do not necessarily need to ﬁnd the optimal Q that minimizes (7) at each step. Indeed, it would be
suﬃcient to ﬁnd some Q which allows to build a slightly better approximation of Pd. This means that a

5which means such a property can be used in the context of the original GAN algorithm.
6One example of such a setting is running GANs, which are known to approximately minimize the Jensen-Shannon divergence.

(4)

(5)

(6)

(7)

7

more modest goal could be to ﬁnd Q such that, for some c < 1,

Df ((1 − β)Pg + βQ (cid:107) Pd) ≤ c · Df (Pg (cid:107) Pd) .

(8)

However, we observe that this greedy approach has a signiﬁcant drawback in practice. Indeed, as we
build up the mixture, we need to make β decrease (as P t
model approximates Pd better and better, one should
make the correction at each step smaller and smaller). Since we are approximating (7) using samples from
both distributions, this means that the sample from the mixture will only contain a fraction β of examples
from Q. So, as t increases, getting meaningful information from a sample so as to tune Q becomes harder
and harder (the information is “diluted”).

To address this issue, we propose to optimize an upper bound on (7) which involves a term of the
form Df (Q (cid:107) Q0) for some distribution Q0, which can be computed as a reweighting of the original data
distribution Pd.

In the following sections we will analyze the properties of (7) (Section 2.4) and derive upper bounds that
provide practical optimization criteria for building the mixture (Section 2.3). We will also show that under
certain assumptions, the minimization of the upper bound will lead to the optimum of the original criterion.
This procedure is reminiscent of the AdaBoost algorithm [6], which combines multiple weak predictors
into one very accurate strong composition. On each step AdaBoost adds one new predictor to the current
composition, which is trained to minimize the binary loss on the reweighted training set. The weights are
constantly updated in order to bias the next weak learner towards “hard” examples, which were incorrectly
classiﬁed during previous stages.

2.3 Upper Bounds

Next lemma provides two upper bounds on the divergence of the mixture in terms of the divergence of the
additive component Q with respect to some reference distribution R.

Lemma 1 Let f ∈ F. Given two distributions Pd, Pg and some β ∈ [0, 1], for any distribution Q and any
distribution R such that βdR ≤ dPd, we have

Df ((1 − β)Pg + βQ (cid:107) Pd) ≤ βD(Q (cid:107) R) + (1 − β)Df

Pg (cid:107)

(9)

(cid:18)

Pd − βR
1 − β

(cid:19)

.

If furthermore f ∈ FH , then, for any R, we have

Df ((1 − β)Pg + βQ (cid:107) Pd) ≤

βDf (Q (cid:107) R) +

Df ((1 − β)Pg + βR (cid:107) Pd)

.

(10)

(cid:18)(cid:113)

(cid:113)

(cid:19)2

Proof For the ﬁrst inequality, we use the fact that Df is jointly convex. We write Pd = (1 − β) Pd−βR
which is a convex combination of two distributions when the assumptions are satisﬁed.

1−β + βR

The second inequality follows from using the triangle inequality for the square root of the Hilbertian

metric Df and using convexity of Df in its ﬁrst argument.

We can exploit the upper bounds of Lemma 1 by introducing some well-chosen distribution R and
minimizing with respect to Q. A natural choice for R is a distribution that minimizes the last term of the
upper bound (which does not depend on Q).

2.4 Optimal Upper Bounds

In this section we provide general theorems about the optimization of the right-most terms in the upper
bounds of Lemma 1.

For the upper bound (10), this means we need to ﬁnd R minimizing Df ((1 − β)Pg + βR (cid:107) Pd). The

solution for this problem is given in the following theorem.

8

Theorem 1 For any f -divergence Df , with f ∈ F and f diﬀerentiable, any ﬁxed distributions Pd, Pg, and
any β ∈ (0, 1], the solution to the following minimization problem:

where P is a class of all probability distributions, has the density

min
Q∈P

Df ((1 − β)Pg + βQ (cid:107) Pd),

dQ∗

β(x) =

(λ∗dPd(x) − (1 − β)dPg(x))+

1
β

for some unique λ∗ satisfying (cid:82) dQ∗
Also, λ∗ = 1 if and only if Pd((1 − β)dPg > dPd) = 0, which is equivalent to βdQ∗

β = 1. Furthermore, β ≤ λ∗ ≤ min(1, β/δ), where δ := Pd(dPg = 0).

β = dPd − (1 − β)dPg.

Proof See Appendix C.1.

(cid:1)/β,
Remark 1 The form of Q∗
which would make arguments of the f -divergence identical? Unfortunately, it may be the case that dPd(X) <
(1 − β)dPg(X) for some of X ∈ X , leading to the negative values of dQ.

β may look unexpected at ﬁrst glance: why not setting dQ := (cid:0)dPd −(1−β)dPg

For the upper bound (9), we need to minimize Df

. The solution is given in the next

(cid:16)

Pg (cid:107) Pd−βR
1−β

(cid:17)

theorem.

Theorem 2 Given two distributions Pd, Pg and some β ∈ (0, 1], assume

Let f ∈ F. The solution to the minimization problem

Pd (dPg = 0) < β.

min
Q:βdQ≤dPd

Df

Pg (cid:107)

(cid:18)

(cid:19)

Pd − βQ
1 − β

is given by the distribution

dQ†

β(x) =

(cid:0)dPd(x) − λ†(1 − β)dPg(x)(cid:1)

+

1
β

for a unique λ† ≥ 1 satisfying (cid:82) dQ†

β = 1.

Proof See Appendix C.2.

Remark 2 Notice that the term that we optimized in upper bound (10) is exactly the initial objective (7).
So that Theorem 1 also tells us what the form of the optimal distribution is for the initial objective.

Remark 3 Surprisingly, in both Theorem 1 and 2, the solution does not depend on the choice of the func-
tion f , which means that the solution is the same for any f -divergence. This also means that by replacing
f by f ◦, we get similar results for the criterion written in the other direction, with again the same solution.
Hence the order in which we write the divergence does not matter and the optimal solution is optimal for
both orders.

Remark 4 Note that λ∗ is implicitly deﬁned by a ﬁxed-point equation. In Section 3.1 we will show how it
can be computed eﬃciently in the case of empirical distributions.

Remark 5 Obviously, λ† ≥ λ∗, where λ∗ was deﬁned in Theorem 1. Moreover, we have λ∗ ≤ 1/λ†. Indeed,
it is enough to insert λ† = 1/λ∗ into deﬁnition of Q†

β and check that in this case Q†

β ≥ 1.

9

2.5 Convergence Analysis for Optimal Updates

In previous section we derived analytical expressions for the distributions R minimizing last terms in upper
bounds (9) and (10). Assuming Q can perfectly match R, i.e. Df (Q (cid:107) R) = 0, we are now interested in the
convergence of the mixture (6) to the true data distribution Pd for Q = Q∗

β or Q = Q†
β.

We start with simple results showing that adding Q∗

β or Q†

β to the current mixture would yield a strict

improvement of the divergence.

Lemma 2 Under the conditions of Theorem 1, we have

Df

(cid:0)(1 − β)Pg + βQ∗

β

(cid:13)
(cid:13) Pd

(cid:1) ≤ Df

(cid:0)(1 − β)Pg + βPd

(cid:13)
(cid:13) Pd

(cid:1) ≤ (1 − β)Df (Pg (cid:107) Pd).

(11)

Under the conditions of Theorem 2, we have

(cid:32)

Df

Pg

(cid:13)
(cid:13)

(cid:33)

Pd − βQ†
β
1 − β

≤ Df (Pg (cid:107) Pd) ,

Df

(cid:0)(1 − β)Pg + βQ†

(cid:13)
(cid:13) Pd

β

(cid:1) ≤ (1 − β)Df (Pg (cid:107) Pd).

and

then

Proof The ﬁrst inequality follows immediately from the optimality of Q∗
β (hence the value of the objective
at Q∗
β is smaller than at Pd), and the fact that Df is convex in its ﬁrst argument and Df (Pd(cid:107)Pd) = 0.
The second inequality follows from the optimality of Q†
β is smaller
than its value at Pd which itself satisﬁes the condition βdPd ≤ dPd). For the third inequality, we combine
the second inequality with the ﬁrst inequality of Lemma 1 (with Q = R = Q†

β (hence the value of the objective at Q†

β).

The upper bound (11) of Lemma 2 can be reﬁned if the ratio dPg/dPd is almost surely bounded:

Lemma 3 Under the conditions of Theorem 1, if there exists M > 1 such that

Pd((1 − β)dPg > M dPd) = 0

Df

(cid:0)(1 − β)Pg + βQ∗

β

(cid:13)
(cid:13) Pd

(cid:1) ≤ f (λ∗) +

f (M )(1 − λ∗)
M − 1

.

Proof We use Inequality (20) of Lemma 6 with X = β, Y = (1 − β)dPg/dPd, and c = λ∗. We easily
verify that X + Y = ((1 − β)dPg + βdPd)/dPd and max(c, Y ) = ((1 − β)dPg + βdQ∗
β)/dPd and both have
expectation 1 with respect to Pd. We thus obtain:

Df ((1 − β)Pg + βQ∗

β (cid:107) Pd) ≤ f (λ∗) +

f (M ) − f (λ∗)
M − λ∗

(1 − λ∗) .

Since λ∗ ≤ 1 and f is non-increasing on (0, 1) we get

Df ((1 − β)Pg + βQ∗

β (cid:107) Pd) ≤ f (λ∗) +

f (M )(1 − λ∗)
M − 1

.

Remark 6 This upper bound can be tighter than that of Lemma 2 when λ∗ gets close to 1. Indeed, for
λ∗ = 1 the upper bound is exactly 0 and is thus tight, while the upper bound of Lemma 2 will not be zero in
this case.

10

Imagine repeatedly adding T new components to the current mixture Pg, where on every step we use the
same weight β and choose the components described in Theorem 1. In this case Lemma 2 guarantees that the
original objective value Df (Pg (cid:107) Pd) would be reduced at least to (1 − β)T Df (Pg (cid:107) Pd). This exponential rate
of convergence, which at ﬁrst may look surprisingly good, is simply explained by the fact that Q∗
β depends
on the true distribution Pd, which is of course unknown.

Lemma 2 also suggests setting β as large as possible. This is intuitively clear: the smaller the β, the less
we alter our current model Pg. As a consequence, choosing small β when Pg is far away from Pd would lead
to only minor improvements in objective (7). In fact, the global minimum of (7) can be reached by setting
β = 1 and Q = Pd. Nevertheless, in practice we may prefer to keep β relatively small, preserving what we
learned so far through Pg: for instance, when Pg already covered part of the modes of Pd and we want Q to
cover the remaining ones. We provide further discussions on choosing β in Section 3.2.

In the reminder of this section we study the convergence of (7) to 0 in the case where we use the upper
bound (10) and the weight β is ﬁxed (i.e. the same value at each iteration). This analysis can easily be
extended to a variable β.

Lemma 4 For any f ∈ F such that f (x) (cid:54)= 0 for x (cid:54)= 1, the following conditions are equivalent:

(i) Pd((1 − β)dPg > dPd) = 0;

(ii) Df ((1 − β)Pg + βQ∗

β (cid:107) Pd) = 0.

Proof The ﬁrst condition is equivalent to λ∗ = 1 according to Theorem 1. In this case, (1−β)Pg +βQ∗
β = Pd,
hence the divergence is 0. In the other direction, when the divergence is 0, since f is strictly positive for x (cid:54)= 1
(keep in mind that we can always replace f by f0 to get a non-negative function which will be strictly positive
if f (x) (cid:54)= 0 for x (cid:54)= 1), this means that with Pd probability 1 we have the equality dPd = (1 − β)dPg + βdQ∗
β,
which implies that (1 − β)dPg > dPd with Pd probability 1 and also λ∗ = 1.

This result tells that we can not perfectly match Pd by adding a new mixture component to Pg as long as
there are points in the space where our current model Pg severely over-samples. As an example, consider an
extreme case where Pg puts a positive mass in a region outside of the support of Pd. Clearly, unless β = 1,
we will not be able to match Pd.

Finally, we provide a necessary and suﬃcient condition for the iterative process to converge to the data
distribution Pd in ﬁnite number of steps. The criterion is based on the ratio dP1/dPd, where P1 is the ﬁrst
component of our mixture model.

Corollary 1 Take any f ∈ F such that f (x) (cid:54)= 0 for x (cid:54)= 1. Starting from P 1
iteratively according to P t+1
β, where on every step Q∗
with Pg := P t
exists M > 0 such that

model = P1, update the model
β is as deﬁned in Theorem 1
model (cid:107) Pd) will reach 0 in a ﬁnite number of steps if and only if there

model. In this case Df (P t

model = (1 − β)P t

model + βQ∗

Pd((1 − β)dP1 > M dPd) = 0 .

(12)

When the ﬁnite convergence happens, it takes at most − ln max(M, 1)/ ln(1 − β) steps.

Proof From Lemma 4, it is clear that if M ≤ 1 the convergence happens after the ﬁrst update. So let
us assume M > 1. Notice that dP t+1
β = max(λ∗dPd, (1 − β)dP t
model + βdQ∗
model) so that if
Pd((1 − β)dP t
model > M (1 − β)dPd) = 0. This proves that (12) is a
suﬃcient condition.

model > M dPd) = 0, then Pd((1 − β)dP t+1

model = (1 − β)dP t

Now assume the process converged in a ﬁnite number of steps. Let P t

the ﬁnal step. Note that P t
distribution P . According to Lemma 4 we have Pd((1 − β)dP t
immediately imply (12).

model be a mixture right before
model is represented by (1 − β)t−1P1 + (1 − (1 − β)t−1)P for certain probability
model > dPd) = 0. Together these two facts

It is also important to keep in mind that even if (12) is not satisﬁed the process still converges to the true
distribution at exponential rate (see Lemma 2 as well as Corollaries 2 and 3 below)

11

2.6 Weak to Strong Learnability

In practice the component Q that we add to the mixture is not exactly Q∗
β, but rather an approximation
to them. We need to show that if this approximation is good enough, then we retain the property that (8)
is reached. In this section we will show that this is indeed the case.

β or Q†

Looking again at Lemma 1 we notice that the ﬁrst upper bound is less tight than the second one. Indeed,
take the optimal distributions provided by Theorems 1 and 2 and plug them back as R into the upper bounds
of Lemma 1. Also assume that Q can match R exactly, i.e. we can achieve Df (Q (cid:107) R) = 0. In this case both
sides of (10) are equal to Df ((1 − β)Pg + βQ∗
β (cid:107) Pd), which is the optimal value for the original objective (7).
On the other hand, (9) does not become an equality and the r.h.s. is not the optimal one for (7).

This means that using (10) allows to reach the optimal value of the original objective (7), whereas
using (9) does not. However, this is not such a big issue since, as we mentioned earlier, we only need to
improve the mixture by adding the next component (we do not need to add the optimal next component).
So despite the solution of (7) not being reachable with the ﬁrst upper bound, we will still show that (8) can
be reached.

The ﬁrst result provides suﬃcient conditions for strict improvements when we use the upper bound (9).

Corollary 2 Given two distributions Pd, Pg, and some β ∈ (0, 1], assume

Let Q†

β be as deﬁned in Theorem 2. If Q is a distribution satisfying

Pd

(cid:18) dPg
dPd

(cid:19)

= 0

< β.

Df (Q (cid:107) Q†

β) ≤ γDf (Pg (cid:107) Pd)

for γ ∈ [0, 1] then

Df ((1 − β)Pg + βQ (cid:107) Pd) ≤ (1 − β(1 − γ))Df (Pg (cid:107) Pd).

Proof Immediately follows from combining Lemma 1, Theorem 1, and Lemma 2.

Next one holds for Hilbertian metrics and corresponds to the upper bound (10).

Corollary 3 Assume f ∈ FH , i.e. Df is a Hilbertian metric. Take any β ∈ (0, 1], Pd, Pg, and let Q∗
as deﬁned in Theorem 1. If Q is a distribution satisfying

β be

for some γ ∈ [0, 1], then

Df (Q (cid:107) Q∗

β) ≤ γDf (Pg (cid:107) Pd)

Df ((1 − β)Pg + βQ (cid:107) Pd) ≤

(cid:16)(cid:112)γβ + (cid:112)1 − β

(cid:17)2

Df (Pg (cid:107) Pd) .

In particular, the right-hand side is strictly smaller than Df (Pg (cid:107) Pd) as soon as γ < β/4 (and β > 0).

Immediately follows from combining Lemma 1, Theorem 2, and Lemma 2. It is easy to verify that

Proof
for γ < β/4, the coeﬃcient is less than (β/2 +

√

1 − β)2 which is < 1 (for β > 0).

Remark 7 We emphasize once again that the upper bound (10) and Corollary 3 both hold for Jensen-
Shannon, Hellinger, and total variation divergences among others. In particular they can be applied to the
original GAN algorithm.

Conditions 14 and 15 may be compared to the “weak learnability” condition of AdaBoost. As long as our
weak learner is able to solve the surrogate problem (5) of matching respectively Q†
β accurately enough,
the original objective (7) is guaranteed to decrease as well. It should be however noted that Condition 15

β or Q∗

12

(13)

(14)

(15)

Indeed, as already mentioned before,
with γ < β/4 is perhaps too strong to call it “weak learnability”.
the weight β is expected to decrease to zero as the number of components in the mixture distribution Pg
increases. This leads to γ → 0, making it harder to meet Condition 15. This obstacle may be partially
resolved by the fact that we will use a GAN to ﬁt Q, which corresponds to a relatively rich7 class of models
G in (5). In other words, our weak learner is not so weak.

On the other hand, Condition (14) of Corollary 2 is much milder. No matter what γ ∈ [0, 1] and β ∈ (0, 1]
we choose, the new component Q is guaranteed to strictly improve the objective functional. This comes at
the price of the additional Condition (13), which asserts that β should be larger than the mass of true data
Pd missed by the current model Pg. We argue that this is a rather reasonable condition: if Pg misses many
modes of Pd we would prefer assigning a relatively large weight β to the new component Q.

3 AdaGAN

In this section we provide a more detailed description of Algorithm 1 from Section 1.1, in particular how to
reweight the training examples for the next iteration and how to choose the mixture weights.

In a nutshell, at each iteration we want to add a new component Q to the current mixture Pg with
weight β, to create a mixture with distribution (1 − β)Pg + βQ. This component Q should approach an
“optimal target” Q∗

β and we know from Theorem 1 that:

dQ∗

β =

(cid:18)

dPd
β

λ∗ − (1 − β)

(cid:19)

dPg
dPd

.

+

dPg
dPd

(X) = h(cid:0)D(X)(cid:1).

dQ∗

β =

dPd
β

(cid:0)λ∗ − (1 − β)h(D)(cid:1)

+ ,

wi =

pi
β

(cid:0)λ∗ − (1 − β)h(di)(cid:1)

+

Computing this distribution requires to know the density ratio dPg/dPd, which is not directly accessible,
but it can be estimated using the idea of adversarial training. Indeed, we can train a discriminator D to
distinguish between samples from Pd and Pg. It is known that for an arbitrary f -divergence, there exists
a corresponding function h (see [7]) such that the values of the optimal discriminator D are related to the
density ratio in the following way:

In particular, for the Jensen-Shannon divergence, used by the original GAN algorithm, it holds that h(cid:0)D(X)(cid:1) =

1−D(X)
D(X)

. So in this case for the optimal discriminator we have

which can be viewed as a reweighted version of the original data distribution Pd.

In particular, when we compute dQ∗

β on the training sample SN = (X1, . . . , XN ), each example Xi has

the following weight:

with pi = dPd(Xi) and di = D(Xi). In practice, we use the empirical distribution over the training sample
which means we set pi = 1/N .

3.1 How to compute λ∗ of Theorem 1

Next we derive an algorithm to determine λ∗. We need to ﬁnd a value of λ∗ such that the weights wi in (17)
are normalized, i.e.:

(cid:88)

wi =

(cid:0)λ∗ − (1 − β)h(di)(cid:1) = 1 ,

(cid:88)

i

pi
β

i∈I(λ∗)

7The hardness of meeting Condition 15 of course largely depends on the class of models G used to ﬁt Q in (5). For now we

ignore this question and leave it for future research.

13

(16)

(17)

where I(λ) := {i : λ > (1 − β)h(di)}. This in turn yields:

λ∗ =

β
i∈I(λ∗) pi

(cid:80)



1 +

(1 − β)
β

(cid:88)

i∈I(λ∗)



pih(di)

 .

(18)

Now, to compute the r.h.s., we need to know I(λ∗). To do so, we sort the values h(di) in increasing order:
h(d1) ≤ h(d2) ≤ . . . ≤ h(dN ). Then I(λ∗) is simply a set consisting of the ﬁrst k values, where we have to
determine k. Thus, it suﬃces to test successively all positive integers k until the λ given by Equation (18)
veriﬁes:

(1 − β)h(dk) < λ ≤ (1 − β)h(dk+1) .

This procedure is guaranteed to converge, because by Theorem 1, we know that λ∗ exists, and it satisﬁes (18).
In summary, λ∗ can be determined by Algorithm 2.

Algorithm 2: Determining λ∗
1 Sort the values h(di) in increasing order ;
1 + 1−β
2 Initialize λ ← β
p1
3 while (1 − β)h(dk) ≥ λ do
4

β p1h(d1)

(cid:16)

(cid:17)

k ← k + 1;
λ ← β
(cid:80)k

i=1 pi

(cid:16)

5

1 + (1−β)

β

(cid:80)k

(cid:17)
i=1 pih(di)

and k ← 1 ;

3.2 How to choose a mixture weight β

While for every β there is an optimal reweighting scheme, the weights from (17) depend on β. In particular,
if β is large enough to verify dPd(x)λ∗ − (1 − β)dPg(x) ≥ 0 for all x, the optimal component Q∗
β satisﬁes
(1 − β)Pg + βQ∗
β = Pd, as proved in Lemma 4. In other words, in this case we exactly match the data
distribution Pd, assuming the GAN can approximate the target Q∗
β perfectly. This criterion alone would
lead to choosing β = 1. However in practice we know we can’t get a generator that produces exactly the
target distribution Q∗
β. We thus propose a few heuristics one can follow to choose β:

– Any ﬁxed constant value β for all iterations.

– All generators to be combined with equal weights in the ﬁnal mixture model. This corresponds to

setting βt = 1

t , where t is the iteration.

– Instead of choosing directly a value for β one could pick a ratio 0 < r < 1 of examples which should
have a weight wi > 0. Given such an r, there is a unique value of β (βr) resulting in wi > 0 for exactly
N · r training examples. Such a value βr can be determined by binary search over β in Algorithm 2.
Possible choices for r include:

– r constant, chosen experimentally.
– r decreasing with the number of iterations, e.g., r = c1e−c2t for any positive constants c1, c2.

– Alternatively, one can set a particular threshold for the density ratio estimate h(D), compute the
fraction r of training examples that have a value above that threshold and derive β from this ratio
r (as above). Indeed, when h(D) is large, that means that the generator does not generate enough
examples in that region, and the next iteration should be encouraged to generate more there.

14

Algorithm 3: AdaGAN, a meta-algorithm to construct a “strong” mixture of T individual GANs,
trained sequentially. The mixture weight schedule ChooseMixtureWeight should be provided by the
user (see 3.2). This is an instance of the high level Algorithm 1, instantiating UpdateTrainingWeights.

#Compute the new weights of the training examples (UpdateTrainingWeights)

#Compute the discriminator between the original (unweighted) data and the current mixture Gt−1

Input: Training sample SN := {X1, . . . , XN }.

Output: Mixture generative model G = GT .

Train vanilla GAN: G1 = GAN(SN )

for t = 2, . . . , T do

#Choose a mixture weight for the next component

βt = ChooseMixtureWeight(t)

D ← DGAN (SN , Gt−1);
#Compute λ∗ using Algorithm 2
λ∗ ← λ(βt, D)
#Compute the new weight for each example

for i = 1, . . . , N do

W i

t = 1
N βt

(λ∗ − (1 − βt)h(D(Xi)))+

end for
#Train t-th “weak” component generator Gc
t
Gc

t = GAN(SN , Wt)

#Update the overall generative model
#Notation below means forming a mixture of Gt−1 and Gc
t .
Gt = (1 − βt)Gt−1 + βtGc
t

end for

3.3 Complete algorithm

Now we have all the necessary components to introduce the complete AdaGAN meta-algorithm. The algo-
rithm uses any given GAN implementation (which can be the original one of Goodfellow et al. [1] or any
later modiﬁcations) as a building block. Accordingly, Gc ← GAN (SN , W ) returns a generator Gc for a given
set of examples SN = (X1, . . . , XN ) and corresponding weights W = (w1, . . . , wN ). Additionally, we write
D ← DGAN (SN , G) to denote a procedure that returns a discriminator from the GAN algorithm trained
on a given set of true data examples SN and examples sampled from the mixture of generators G. We
also write λ∗(β, D) to denote the optimal λ∗ given by Algorithm 2. The complete algorithm is presented in
Algorithm 3.

4 Experiments

We tested AdaGAN8 on toy datasets, for which we can interpret the missing modes in a clear and reproducible
way, and on MNIST, which is a high-dimensional dataset. The goal of these experiments was not to evaluate

8Code available online at https://github.com/tolstikhin/adagan

15

the visual quality of individual sample points, but to demonstrate that the re-weighting scheme of AdaGAN
promotes diversity and eﬀectively covers the missing modes.

4.1 Toy datasets

The target distribution is deﬁned as a mixture of normal distributions, with diﬀerent variances. The distances
between the means are relatively large compared to the variances, so that each Gaussian of the mixture is
“isolated”. We vary the number of modes to test how well each algorithm performs when there are fewer or
more expected modes.

More precisely, we set X = R2, each Gaussian component is isotropic, and their centers are sampled
uniformly in a square. That particular random seed is ﬁxed for all experiments, which means that for a
given number of modes, the target distribution is always the same. The variance parameter is the same for
each component, and is decreasing with the number of modes, so that the modes stay apart from each other.
This target density is very easy to learn, using a mixture of Gaussians model, and for example the EM
algorithm [19]. If applied to the situation where the generator is producing single Gaussians (i.e. PZ is a
standard Gaussian and G is a linear function), then AdaGAN produces a mixture of Gaussians, however it
does so incrementally unlike EM, which keeps a ﬁxed number of components. In any way AdaGAN was not
tailored for this particular case and we use the Gaussian mixture model simply as a toy example to illustrate
the missing modes problem.

4.1.1 Algorithms

We compare diﬀerent meta-algorithms based on GAN, and the baseline GAN algorithm. All the meta-
algorithms use the same implementation of the underlying GAN procedure.
In all cases, the generator
uses latent space Z = R5, and two ReLU hidden layers, of size 10 and 5 respectively. The corresponding
discriminator has two ReLU hidden layers of size 20 and 10 respectively. We use 64k training examples, and
15 epochs, which is enough compared to the small scale of the problem, and all networks converge properly
and overﬁtting is never an issue. Despite the simplicity of the problem, there are already diﬀerences between
the diﬀerent approaches.

We compare the following algorithms:

– The baseline GAN algorithm, called Vanilla GAN in the results.

– The best model out of T runs of GAN, that is: run T GAN instances independently, then take the run
that performs best on a validation set. This gives an additional baseline with similar computational
complexity as the ensemble approaches. Note that the selection of the best run is done on the reported
target metric (see below), rather than on the internal metric. As a result this baseline is slightly
overestimated. This procedure is called Best of T in the results.

– A mixture of T GAN generators, trained independently, and combined with equal weights (the “bag-

ging” approach). This procedure is called Ensemble in the results.

– A mixture of GAN generators, trained sequentially with diﬀerent choices of data reweighting:

– The AdaGAN algorithm (Algorithm 1), for β = 1/t, i.e. each component will have the same

weight in the resulting mixture (see § 3.2). This procedure is called Boosted in the results.

– The AdaGAN algorithm (Algorithm 1), for a constant β, exploring several values. This procedure

is called for example Beta0.3 for β = 0.3 in the results.

– Reweighting similar to “Cascade GAN” from [14], i.e. keeping the top r fraction of examples,
based on the discriminator corresponding to the previous generator. This procedure is called for
example TopKLast0.3 for r = 0.3.

– Keep the top r fraction of examples, based on the discriminator corresponding to the mixture of

all previous generators. This procedure is called for example TopK0.3 for r = 0.3.

16

4.1.2 Metrics

To evaluate how well the generated distribution matches the target distribution, we use a coverage metric C.
We compute the probability mass of the true data “covered” by the model distribution Pmodel. More
precisely, we compute C := Pd(dPmodel > t) with t such that Pmodel(dPmodel > t) = 0.95. This metric
is more interpretable than the likelihood, making it easier to assess the diﬀerence in performance of the
algorithms. To approximate the density of Pmodel we use a kernel density estimation method, where the
bandwidth is chosen by cross validation. Note that we could also use the discriminator D to approximate
the coverage as well, using the relation from (16).

Another metric is the likelihood of the true data under the generated distribution. More precisely, we
compute L := 1
i log Pmodel(xi), on a sample of N examples from the data. Note that [20] proposes a
N
more general and elegant approach (but less straightforward to implement) to have an objective measure of
GAN. On the simple problems we tackle here, we can precisely estimate the likelihood.

(cid:80)

In the main results we report the metric C and in Appendix E we report both L and C. For a given
metric, we repeat the run 35 times with the same parameters (but diﬀerent random seeds). For each run, the
learning rate is optimized using a grid search on a validation set. We report the median over those multiple
runs, and the interval corresponding to the 5% and 95% percentiles. Note this is not a conﬁdence interval of
the median, which would shrink to a singleton with an inﬁnite number of runs. Instead, this gives a measure
of the stability of each algorithm. The optimizer is a simple SGD: Adam was also tried but gave slightly less
stable results.

4.1.3 Results

With the vanilla GAN algorithm, we observe that not all the modes are covered (see Figure 1 for an
illustration). Diﬀerent modes (and even diﬀerent number of modes) are possibly covered at each restart of
the algorithm, so restarting the algorithm with diﬀerent random seeds and taking the best (“best of T ”) can
improve the results.

Figure 3 summarizes the performance of the main algorithms on the C metric, as a function of the number
of iterations T . Table 1 gives more detailed results, varying the number of modes for the target distribution.
Appendix E contains details on variants for the reweighting heuristics as well as results for the L metric.

As expected, both the ensemble and the boosting approaches signiﬁcantly outperform the vanilla GAN
and the “best of T ” algorithm. Interestingly, the improvements are signiﬁcant even after just one or two
additional iterations (T = 2 or T = 3). The boosted approach converges much faster.
In addition, the
variance is much lower, improving the likelihood that a given run gives good results. On this setup, the
vanilla GAN approach has a signiﬁcant number of catastrophic failures (visible in the lower bound of the
interval).

Empirical results on combining AdaGAN meta-algorithm with the unrolled GANs [4] are available in

Appendix A.

4.2 MNIST and MNIST3

We ran experiments both on the original MNIST and on the 3-digit MNIST (MNIST3) [5, 4] dataset,
obtained by concatenating 3 randomly chosen MNIST images to form a 3-digit number between 0 and 999.
According to [5, 4], MNIST contains 10 modes, while MNIST3 contains 1000 modes, and these modes can
be detected using the pre-trained MNIST classiﬁer. We combined AdaGAN both with simple MLP GANs
and DCGANs [21]. We used T ∈ {5, 10}, tried models of various sizes and performed a reasonable amount
of hyperparameter search. For the details we refer to Appendix B.

Similarly to [4, Sec 3.3.1] we failed to reproduce the missing modes problem for MNIST3 reported in
[5] and found that simple GAN architectures are capable of generating all 1000 numbers. The authors
of [4] proposed to artiﬁcially introduce the missing modes again by limiting the generators’ ﬂexibility. In
our experiments, GANs trained with the architectures reported in [4] were often generating poorly looking
digits. As a result, the pre-trained MNIST classiﬁer was outputting random labels, which again led to full

17

Figure 2: Coverage C of the true data by the model distribution P T
model, as a function of iterations T .
Experiments correspond to the data distribution with 5 modes. Each blue point is the median over 35 runs.
Green intervals are deﬁned by the 5% and 95% percentiles (see Section 4.1.2).
Iteration 0 is equivalent
to one vanilla GAN. The left plot corresponds to taking the best generator out of T runs. The middle
plot corresponds to the “ensemble GAN”, simply taking a uniform mixture of T independently trained
GAN generators. The right plot corresponds to our boosting approach (AdaGAN), carefully reweighting
the examples based on the previous generators, with βt = 1/t. Both the ensemble and boosting approaches
signiﬁcantly outperform the vanilla approach with few additional iterations. They also outperform taking
the best out of T runs. The boosting outperforms all other approaches. For AdaGAN the variance of the
performance is also signiﬁcantly decreased.

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

0.97 (0.9; 1.0)

0.88 (0.4; 1.0)

0.63 (0.5; 1.0)

0.72 (0.5; 0.8)

0.58 (0.4; 0.8)

0.59 (0.2; 0.7)

Best of T (T=3)

0.99 (1.0; 1.0)

0.96 (0.9; 1.0)

0.91 (0.7; 1.0)

0.80 (0.7; 0.9)

0.84 (0.7; 0.9)

0.70 (0.6; 0.8)

Best of T (T=10)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

0.98 (0.8; 1.0)

0.80 (0.8; 0.9)

0.87 (0.8; 0.9)

0.71 (0.7; 0.8)

Ensemble (T=3)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.93 (0.8; 1.0)

0.78 (0.6; 1.0)

0.85 (0.6; 1.0)

0.80 (0.6; 1.0)

Ensemble (T=10)

1.00 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

0.91 (0.8; 1.0)

0.88 (0.8; 1.0)

0.89 (0.7; 1.0)

TopKLast0.5 (T=3)

0.98 (0.9; 1.0)

0.98 (0.9; 1.0)

0.95 (0.9; 1.0)

0.95 (0.8; 1.0)

0.86 (0.7; 1.0)

0.86 (0.6; 0.9)

TopKLast0.5 (T=10)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.98 (1.0; 1.0)

0.99 (0.8; 1.0)

0.99 (0.8; 1.0)

1.00 (0.8; 1.0)

Boosted (T=3)

0.99 (1.0; 1.0)

0.99 (0.9; 1.0)

0.98 (0.9; 1.0)

0.91 (0.8; 1.0)

0.91 (0.8; 1.0)

0.86 (0.7; 1.0)

Boosted (T=10)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

Table 1: Performance of the diﬀerent algorithms on varying number of mixtures of Gaussians. The reported
score is the coverage C, probability mass of Pd covered by the 5th percentile of Pg deﬁned in Section 4.1.2.
See Table 2 for more metrics. The reported scores are the median and interval deﬁned by the 5% and 95%
percentile (in parenthesis) (see Section 4.1.2), over 35 runs for each setting. Note that the 95% interval is
not the usual conﬁdence interval measuring the variance of the experiment itself, but rather measures the
stability of the diﬀerent algorithms (would remain even if each experiment was run an inﬁnite number of
times). Both the ensemble and the boosting approaches signiﬁcantly outperform the vanilla GAN even with
just three iterations (i.e. just two additional components). The boosting approach converges faster to the
optimal coverage and with smaller variance.

18

coverage of the 1000 numbers. We tried to threshold the conﬁdence of the pre-trained classiﬁer, but decided
that this metric was too ad-hoc.

For MNIST we noticed that the re-weighted distribu-
tion was often concentrating its mass on digits having
very speciﬁc strokes: on diﬀerent rounds it could high-
light thick, thin, vertical, or diagonal digits, indicating
that these traits were underrepresented in the generated
samples (see Figure 3). This suggests that AdaGAN does
a reasonable job at picking up diﬀerent modes of the
dataset, but also that there are more than 10 modes in
MNIST (and more than 1000 in MNIST3). It is not clear
how to evaluate the quality of generative models in this
context.

We also tried to use the “inversion” metric discussed
in Section 3.4.1 of [4]. For MNIST3 we noticed that a
single GAN was capable of reconstructing most of the
training points very accurately both visually and in the
(cid:96)2-reconstruction sense.

5 Conclusion

Figure 3: Digits from the MNIST dataset cor-
responding to the smallest (left) and largest
(right) weights, obtained by the AdaGAN pro-
cedure (see Section 3) in one of the runs. Bold
digits (left) are already covered and next GAN
will concentrate on thin (right) digits.

We presented an incremental procedure for constructing
an additive mixture of generative models by minimizing
an f -divergence criterion. Based on this, we derived a boosting-style algorithm for GANs, which we call
AdaGAN. By incrementally adding new generators into a mixture through the optimization of a GAN
criterion on a reweighted data, this algorithm is able to progressively cover all the modes of the true data
distribution. This addresses one of the main practical issues of training GANs.

We also presented a theoretical analysis of the convergence of this incremental procedure and showed
conditions under which the mixture converges to the true distribution either exponentially or in a ﬁnite
number of steps.

Our preliminary experiments (on toy data) show that this algorithm is eﬀectively addressing the missing

modes problem and allows to robustly produce a mixture which covers all modes of the data.

However, since the generative model that we obtain is not a single neural network but a mixture of such
networks, the corresponding latent representation no longer has a smooth structure. This can be seen as a
disadvantage compared to standard GAN where one can perform smooth interpolation in latent space. On
the other hand it also allows to have a partitioned latent representation where one component is discrete.
Future work will explore the possibility of leveraging this structure to model discrete aspects of the dataset,
such as the class in object recognition datasets in a similar spirit to [22].

References

[1] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Pro-
cessing Systems, pages 2672–2680, 2014.

[2] D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In ICLR, 2014.

[3] Martin Arjovsky, Soumith Chintala, and Lon Bottou. Wasserstein GAN. arXiv:1701.07875, 2017.

[4] L. Metz, B. Poole, D. Pfau, and J. Sohl-Dickstein. Unrolled generative adversarial networks.

arXiv:1611.02163, 2017.

19

[5] Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized generative

adversarial networks. arXiv:1612.02136, 2016.

[6] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application

to boosting. Journal of Computer and System Sciences, 55(1):119–139, 1997.

[7] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka.

f-GAN: Training generative neural samplers
using variational divergence minimization. In Advances in Neural Information Processing Systems, 2016.

[8] Max Welling, Richard S. Zemel, and Geoﬀrey E. Hinton. Self supervised boosting.

In Advances in

neural information processing systems, pages 665–672, 2002.

[9] Zhuowen Tu. Learning generative models via discriminative approaches. In 2007 IEEE Conference on

Computer Vision and Pattern Recognition, pages 1–8. IEEE, 2007.

[10] Aditya Grover and Stefano Ermon. Boosted generative models. ICLR 2017 conference submission, 2016.

[11] R. M. Neal. Annealed importance sampling. Statistics and Computing, 11(2):125–139, 2001.

[12] Saharon Rosset and Eran Segal. Boosting density estimation.

In Advances in Neural Information

Processing Systems, pages 641–648, 2002.

[13] A Barron and J Li. Mixture density estimation. Biometrics, 53:603–618, 1997.

[14] Yaxing Wang, Lichao Zhang, and Joost van de Weijer. Ensembles of generative adversarial networks.

arXiv:1612.00991, 2016.

[15] F. Liese and K.-J. Miescke. Statistical Decision Theory. Springer, 2008.

[16] M. D. Reid and R. C. Williamson. Information, divergence and risk for binary experiments. Journal of

Machine Learning Research, 12:731–817, 2011.

[17] Bent Fuglede and Flemming Topsoe. Jensen-shannon divergence and hilbert space embedding. In IEEE

International Symposium on Information Theory, pages 31–31, 2004.

[18] Matthias Hein and Olivier Bousquet. Hilbertian metrics and positive deﬁnite kernels on probability

measures. In AISTATS, pages 136–143, 2005.

[19] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM

algorithm. Journal of the Royal Statistical Society, B, 39:1–38, 1977.

[20] Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger Grosse. On the quantitative analysis of

decoder-based generative models, 2016.

[21] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional

generative adversarial networks. In ICLR, 2016.

[22] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: In-
terpretable representation learning by information maximizing generative adversarial nets. In Advances
in Neural Information Processing Systems, pages 2172–2180, 2016.

20

A Further details on toy experiments

To illustrate the ’meta-algorithm aspect’ of AdaGAN, we also performed experiments with an unrolled
GAN [4] instead of a GAN as the base generator. We trained the GANs both with the Jensen-Shannon
objective (2), and with its modiﬁed version proposed in [1] (and often considered as the baseline GAN),
where log(1 − D(G(Z))) is replaced by − log(D(G(Z))). We use the same network architecture as in the
other toy experiments. Figure 4 illustrates our results. We ﬁnd that AdaGAN works with all underlying
GAN algorithms. Note that, where the usual GAN updates the generator and the discriminator once, an
unrolled GAN with 5 unrolling steps updates the generator once and the discriminator 1 + 5, i.e. 6 times
(and then rolls back 5 steps). Thus, in terms of computation time, training 1 single unrolled GAN roughly
corresponds to doing 3 steps of AdaGAN with a usual GAN. In that sense, Figure 4 shows that AdaGAN
(with a usual GAN) signiﬁcantly outperforms a single unrolled GAN. Additionally, we note that using the
Jensen-Shannon objective (rather than the modiﬁed version) seems to have some mode-regularizing eﬀect.
Surprisingly, using unrolling steps makes no signiﬁcant diﬀerence.

Figure 4: Comparison of AdaGAN ran with a GAN (top row) and with an unrolled GAN [4] (bottom).
Coverage C of the true data by the model distribution P T
model, as a function of iterations T . Experiments
are similar to those of Figure 3, but with 10 modes. Top and bottom rows correspond to the usual and the
unrolled GAN (with 5 unrolling steps) respectively, trained with the Jensen-Shannon objective (2) on the
left, and with the modiﬁed objective originally proposed by [1] on the right. In terms of computation time,
one step of AdaGAN with unrolled GAN corresponds to roughly 3 steps of AdaGAN with a usual GAN. On
all images T = 1 corresponds to vanilla unrolled GAN.

B Further details on MNIST/MNIST3 experiments

GAN Architecture We ran AdaGAN on MNIST (28x28 pixel images) using (de)convolutional networks
with batch normalizations and leaky ReLu. The latent space has dimension 100. We used the following

21

architectures:

Generator: 100 x 1 x 1 → fully connected → 7 x 7 x 16 → deconv → 14 x 14 x 8 →

Discriminator: 28 x 28 x 1 → conv → 14 x 14 x 16 → conv → 7 x 7 x 32 →

→ deconv → 28 x 28 x 4 → deconv → 28 x 28 x 1

→ fully connected → 1

where each arrow consists of a leaky ReLu (with 0.3 leak) followed by a batch normalization, conv and deconv
are convolutions and transposed convolutions with 5x5 ﬁlters, and fully connected are linear layers with bias.
The distribution over Z is uniform over the unit box. We use the Adam optimizer with β1 = 0.5, with 2
G steps for 1 D step and learning rates 0.005 for G, 0.001 for D, and 0.0001 for the classiﬁer C that does
the reweighting of digits. We optimized D and G over 200 epochs and C over 5 epochs, using the original
Jensen-Shannon objective (2), without the log trick, with no unrolling and with minibatches of size 128.

Empirical observations Although we could not ﬁnd any appropriate metric to measure the increase of
diversity promoted by AdaGAN, we observed that the re-weighting scheme indeed focuses on digits with
very speciﬁc strokes. In Figure 5 for example, we see that after one AdaGAN step, the generator produces
overly thick digits (top left image). Thus AdaGAN puts small weights on the thick digits of the dataset
(bottom left) and high weights on the thin ones (bottom right). After the next step, the new GAN produces
both thick and thin digits.

C Proofs

C.1 Proof of Theorem 1

Before proving Theorem 1, we introduce two lemmas. The ﬁrst one is about the determination of the constant
λ, the second one is about comparing the divergences of mixtures.

Lemma 5 Let P and Q be two distributions, γ ∈ [0, 1] and λ ∈ R. The function

(cid:90) (cid:18)

g(λ) :=

λ − γ

(cid:19)

dQ
dP

dP

+

g(cid:48)
+(λ) = P (λ · dP ≥ γ · dQ).

is nonnegative, convex, nondecreasing, satisﬁes g(λ) ≤ λ, and its right derivative is given by

The equation

g(λ) = 1 − γ
has a solution λ∗ (unique when γ < 1) with λ∗ ∈ [1 − γ, 1]. Finally, if P (dQ = 0) ≥ δ for a strictly positive
constant δ then λ∗ ≤ (1 − γ)δ−1.

Proof The convexity of g follows immediately from the convexity of x (cid:55)→ (x)+ and the linearity of the
integral. Similarly, since x (cid:55)→ (x)+ is non-decreasing, g is non-decreasing.

We deﬁne the set I(λ) as follows:

Now let us consider g(λ + (cid:15)) − g(λ) for some small (cid:15) > 0. This can also be written:

I(λ) := {x ∈ X : λ · dP (x) ≥ γ · dQ(x)}.

g(λ + (cid:15)) − g(λ) =

(cid:15)dP +

(λ + (cid:15))dP −

I(λ)

I(λ+(cid:15))\I(λ)

I(λ+(cid:15))\I(λ)

(cid:90)

(cid:90)

(cid:90)

(cid:90)

(cid:90)

γdQ

γdQ.

= (cid:15)P (I(λ)) +

(λ + (cid:15))dP −

I(λ+(cid:15))\I(λ)

I(λ+(cid:15))\I(λ)

22

Figure 5: AdaGAN on MNIST. Bottom row are true MNIST digits with smallest (left) and highest (right)
weights after re-weighting at the end of the ﬁrst AdaGAN step. Those with small weight are thick and
resemble those generated by the GAN after the ﬁrst AdaGAN step (top left). After training with the re-
weighted dataset during the second iteration of AdaGAN, the new mixture produces more thin digits (top
right).

On the set I(λ + (cid:15))\I(λ), we have

(λ + (cid:15))dP − γdQ ∈ [0, (cid:15)].

So that

and thus

(cid:15)P (I(γ)) ≤ g(λ + (cid:15)) − g(λ) ≤ (cid:15)P (I(γ)) + (cid:15)P (cid:0)I(λ + (cid:15))\I(λ)(cid:1) = (cid:15)P (I(λ + (cid:15)))

lim
(cid:15)→0+

g(λ + (cid:15)) − g(λ)
(cid:15)

= lim
(cid:15)→0+

P (I(λ + (cid:15))) = P (I(λ)).

This gives the expression of the right derivative of g. Moreover, notice that for λ, γ > 0

g(cid:48)
+(λ) = P (λ · dP ≥ γ · dQ) = P

≤

= 1 − P

>

≥ 1 − γ/λ

(cid:18) dQ
dP

(cid:19)

λ
γ

(cid:18) dQ
dP

(cid:19)

λ
γ

by Markov’s inequality.

23

It is obvious that g(0) = 0. By Jensen’s inequality applied to the convex function x (cid:55)→ (x)+, we have
g(λ) ≥ (λ − γ)+. So g(1) ≥ 1 − γ. Also, g = 0 on R− and g ≤ λ. This means g is continuous on R and
thus reaches the value 1 − γ on the interval (0, 1] which shows the existence of λ∗ ∈ (0, 1]. To show that λ∗
is unique we notice that since g(x) = 0 on R−, g is convex and non-decreasing, g cannot be constant on an
interval not containing 0, and thus g(x) = 1 − γ has a unique solution for γ < 1.

Also by convexity of g,

g(0) − g(λ∗) ≥ −λ∗g(cid:48)

+(λ∗),

which gives λ∗ ≥ (1 − γ)/g(cid:48)
the fact that g(cid:48)

+ is increasing we conclude that λ∗ ≤ (1 − γ)δ−1.

+(λ∗) ≥ 1 − γ since g(cid:48)

+ ≤ 1. If P (dQ = 0) ≥ δ > 0 then also g(cid:48)

+(0) ≥ δ > 0. Using

Next we introduce some simple convenience lemma for comparing convex functions of random variables.

Lemma 6 Let f be a convex function, X, Y be real-valued random variables and c ∈ R be a constant such
that

E [max(c, Y )] = E [X + Y ] .

Then we have the following bound:

If in addition, Y ≤ M a.s. for M ≥ c, then

E [f (max(c, Y ))] ≤ E [f (X + Y )] − E [X(f (cid:48)(Y ) − f (cid:48)(c))+] ≤ E [f (X + Y )] .

E [f (max(c, Y ))] ≤ f (c) +

(E [X + Y ] − c).

f (M ) − f (c)
M − c

Proof We decompose the expectation with respect to the value of the max, and use the convexity of f :

f (X + Y ) − f (max(c, Y )) = 1[Y ≤c](f (X + Y ) − f (c)) + 1[Y >c](f (X + Y ) − f (Y ))

(19)

(20)

≥ 1[Y ≤c]f (cid:48)(c)(X + Y − c) + 1[Y >c]Xf (cid:48)(Y )
= (1 − 1[Y >c])Xf (cid:48)(c) + f (cid:48)(c)(Y − max(c, Y )) + 1[Y >c]Xf (cid:48)(Y )
= f (cid:48)(c)(X + Y − max(c, Y )) + 1[Y >c]X(f (cid:48)(Y ) − f (cid:48)(c))
= f (cid:48)(c)(X + Y − max(c, Y )) + X(f (cid:48)(Y ) − f (cid:48)(c))+,

where we used that f (cid:48) is non-decreasing in the last step. Taking the expectation gives the ﬁrst inequality.

For the second inequality, we use the convexity of f on the interval [c, M ]:

f (max(c, Y )) ≤ f (c) +

(max(c, Y ) − c).

f (M ) − f (c)
M − c

Taking an expectation on both sides gives the second inequality.

Proof [Theorem 1] We ﬁrst apply Lemma 5 with γ = 1 − β and this proves the existence of λ∗ in the interval
(β, 1], which shows that Q∗

β is indeed well-deﬁned as a distribution.

Then we use Inequality (19) of Lemma 6 with X = βdQ/dPd, Y = (1 − β)dPg/dPd, and c = λ∗. We
β)/dPd and both

easily verify that X + Y = ((1 − β)dPg + βdQ)/dPd and max(c, Y ) = ((1 − β)dPg + βdQ∗
have expectation 1 with respect to Pd. We thus obtain for any distribution Q,

Df ((1 − β)Pg + βQ∗

β (cid:107) Pd) ≤ Df ((1 − β)Pg + βQ (cid:107) Pd) .

This proves the optimality of Q∗
β.

24

C.2 Proof of Theorem 2

Lemma 7 Let P and Q be two distributions, γ ∈ (0, 1), and λ ≥ 0. The function

h(λ) :=

− λ

(cid:90) (cid:18) 1
γ

(cid:19)

dQ
dP

dP

+

h(λ) =

1 − γ
γ

is convex, non-increasing, and its right derivative is given by h(cid:48)
∆ := P (dQ(X)/dP (X) = 0). Then the equation

+(λ) = −Q(1/γ ≥ λdQ(X)/dP (X)). Denote

has no solutions if ∆ > 1 − γ, has a single solution λ† ≥ 1 if ∆ < 1 − γ, and has inﬁnitely many or no
solutions when ∆ = 1 − γ.

Proof The convexity of h follows immediately from the convexity of x (cid:55)→ (a − x)+ and the linearity of the
integral. Similarly, since x (cid:55)→ (a − x)+ is non-increasing, h is non-increasing as well.

We deﬁne the set J (λ) as follows:

(cid:26)

J (λ) :=

x ∈ X :

≥ λ

(x)

.

(cid:27)

1
γ

dQ
dP

Now let us consider h(λ) − h(λ + (cid:15)) for any (cid:15) > 0. Note that J (λ + (cid:15)) ⊆ J (λ). We can write:

h(λ) − h(λ + (cid:15)) =

(cid:90)

(cid:90)

(cid:90)

=

=

(cid:18) 1
γ

J (λ)

(cid:19)

(cid:90)

− λ

dP −

dQ
dP
(cid:18) 1
γ
(cid:18) 1
γ

J (λ+(cid:15))
(cid:90)

(cid:19)

dP +

− λ

dQ
dP

dQ
dP

(cid:19)

J (λ)\J (λ+(cid:15))

J (λ)\J (λ+(cid:15))

J (λ+(cid:15))

(cid:18) 1
γ

(cid:19)

dP

dQ
dP
(cid:19)

dP

− (λ + (cid:15))

(cid:18)

(cid:15)

dQ
dP

− λ

dP + (cid:15) · Q(J (λ + (cid:15))).

Note that for x ∈ J (λ) \ J (λ + (cid:15)) we have

0 ≤

− λ

(x) < (cid:15)

(x).

1
γ

dQ
dP

dQ
dP

This gives the following:

which shows that h is continuous. Also

(cid:15) · Q(J (λ + (cid:15))) ≤ h(λ) − h(λ + (cid:15)) ≤ (cid:15) · Q(J (λ + (cid:15))) + (cid:15) · Q(J (λ) \ J (λ + (cid:15))) = (cid:15) · Q(J (λ)),

lim
(cid:15)→0+

h(λ + (cid:15)) − h(λ)
(cid:15)

= lim
(cid:15)→0+

−Q(J (λ + (cid:15))) = −Q(J (λ)).

It is obvious that h(0) = 1/γ and h ≤ γ−1 for λ ≥ 0. By Jensen’s inequality applied to the convex
+. So h(1) ≥ γ−1 − 1. We conclude that h may reach the

function x (cid:55)→ (a − x)+, we have h(λ) ≥ (cid:0)γ−1 − λ(cid:1)
value (1 − γ)/γ = γ−1 − 1 only on [1, +∞). Note that

h(λ) →

P

(X) = 0

=

≥ 0

as λ → ∞.

1
γ

(cid:18) dQ
dP

(cid:19)

∆
γ

Thus if ∆/γ > γ−1 −1 the equation h(λ) = γ−1 −1 has no solutions, as h is non-increasing. If ∆/γ = γ−1 −1
then either h(λ) > γ−1 − 1 for all λ ≥ 0 and we have no solutions or there is a ﬁnite λ(cid:48) ≥ 1 such that

25

h(λ(cid:48)) = γ−1 − 1, which means that the equation is also satisﬁed by all λ ≥ λ(cid:48), as h is continuous and
non-increasing. Finally, if ∆/γ < γ−1 − 1 then there is a unique λ† such that h(λ†) = γ−1 − 1, which follows
from the convexity of h.

Next we introduce some simple convenience lemma for comparing convex functions of random variables.

Lemma 8 Let f be a convex function, X, Y be real-valued random variables such that X ≤ Y a.s., and
c ∈ R be a constant such that9

Then we have the following lower bound:

E [min(c, Y )] = E [X] .

E [f (X) − f (min(c, Y ))] ≥ 0.

Proof We decompose the expectation with respect to the value of the min, and use the convexity of f :

f (X) − f (min(c, Y )) = 1[Y ≤c](f (X) − f (Y )) + 1[Y >c](f (X) − f (c))

≥ 1[Y ≤c]f (cid:48)(Y )(X − Y ) + 1[Y >c](X − c)f (cid:48)(c)
≥ 1[Y ≤c]f (cid:48)(c)(X − Y ) + 1[Y >c](X − c)f (cid:48)(c)
= Xf (cid:48)(c) − min(Y, c)f (cid:48)(c),

where we used the fact that f (cid:48) is non-decreasing in the previous to last step. Taking the expectation we get
the result.

Lemma 9 Let Pg, Pd be two ﬁxed distributions and β ∈ (0, 1). Assume

Pd

(cid:18) dPg
dPd

(cid:19)

= 0

< β.

Let M(Pd, β) be the set of all probability distributions T such that (1 − β)dT ≤ dPd. Then the following
minimization problem:

has the solution T ∗ with density

min
T ∈M(Pd,β)

Df (T (cid:107) Pg)

dT ∗ := min(dPd/(1 − β), λ†dPg),

where λ† is the unique value in [1, ∞) such that (cid:82) dT ∗ = 1.
Proof We will use Lemma 8 with X = dT (Z)/dPg(Z), Y = dPd(Z)/(cid:0)(1 − β)dPg(Z)(cid:1), and c = λ∗, Z ∼ Pg.
We need to verify that assumptions of Lemma 8 are satisﬁed. Obviously, Y ≥ X. We need to show that
there is a constant c such that

Rewriting this equation we get the following equivalent one:

(cid:90)

(cid:18)

min

c,

(cid:19)

dPd
(1 − β)dPg

dPg = 1.

(cid:90)

β =

(dPd − min (c(1 − β)Pg, dPd)) = (1 − β)

(21)

(cid:90) (cid:18) 1

1 − β

− c

(cid:19)

dPg
dPd

+

dPd.

Using the fact that

9Generally it is not guaranteed that such a constant c always exists. In this result we assume this is the case.

Pd

(cid:18) dPg
dPd

(cid:19)

= 0

< β

26

we may apply Lemma 7 and conclude that there is a unique c ∈ [1, ∞) satisfying (21), which we denote λ†.

To conclude the proof of Theorem 2, observe that from Lemma 9, by making the change of variable

T = (Pd − βQ)/(1 − β) we can rewrite the minimization problem as follows:

min
Q: βdQ≤dPd

Df ◦

Pg (cid:107)

(cid:18)

(cid:19)

Pd − βQ
1 − β

and we verify that the solution has the form dQ†
depend on f , the fact that we optimized Df ◦ is irrelevant and we get the same solution for Df .

(cid:0)dPd − λ†(1 − β)dPg

+. Since this solution does not

β = 1
β

(cid:1)

D f -Divergences

Jensen-Shannon This divergence corresponds to

Df (P (cid:107)Q) = JS(P, Q) =

(cid:90)

X

(cid:18) dP
dQ

f

(cid:19)

(x)

dQ(x)

f (u) = −(u + 1) log

+ u log u.

u + 1
2

with

Indeed,

(cid:90)

X
(cid:18) p(x)
q(x)

JS(P, Q) :=

q(x)

−

+ 1

log





(cid:19)

(cid:18) p(x)
q(x)



p(x)
q(x) + 1
2



 +

p(x)
q(x)

log

p(x)
q(z)



 dx

=

=

(cid:90)

X

(cid:90)

X

q(x)

log

2q(x)
p(x) + q(x)

+ log

p(x) log

2q(x)
p(x) + q(x)

2q(x)
p(x) + q(x)
2q(x)
p(x) + q(x)
(cid:19)

+

p(x)
q(z)

log

(cid:19)

dx

p(x)
q(z)

p(x)
q(z)

dx

+ q(x) log

+ p(x) log

(cid:18)

= KL

Q,

(cid:19)

P + Q
2

(cid:18)

+ KL

P,

P + Q
2

.

E Additional experimental results

At each iteration of the boosting approach, diﬀerent reweighting heuristics are possible. This section contains
more complete results about the following three heuristics:

– Constant β, and using the proposed reweighting scheme given β. See Table 3.

– Reweighting similar to “Cascade GAN” from [14], i.e. keep the top x% of examples, based on the

discriminator corresponding to the previous generator. See Table 4.

– Keep the top x% of examples, based on the discriminator corresponding to the mixture of all previous

generators. See Table 5.

Note that when properly tuned, each reweighting scheme outperforms the baselines, and have similar
performances when used with few iterations. However, they require an additional parameter to tune, and
are worse than the simple β = 1/t heuristic proposed above.

27

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

0.97 (0.9; 1.0)

0.88 (0.4; 1.0)

0.63 (0.5; 1.0)

0.72 (0.5; 0.8)

0.58 (0.4; 0.8)

0.59 (0.2; 0.7)

Best of T (T=3)

0.99 (1.0; 1.0)

0.96 (0.9; 1.0)

0.91 (0.7; 1.0)

0.80 (0.7; 0.9)

0.84 (0.7; 0.9)

0.70 (0.6; 0.8)

Best of T (T=10)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

0.98 (0.8; 1.0)

0.80 (0.8; 0.9)

0.87 (0.8; 0.9)

0.71 (0.7; 0.8)

Ensemble (T=3)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.93 (0.8; 1.0)

0.78 (0.6; 1.0)

0.85 (0.6; 1.0)

0.80 (0.6; 1.0)

Ensemble (T=10)

1.00 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

0.91 (0.8; 1.0)

0.88 (0.8; 1.0)

0.89 (0.7; 1.0)

Boosted (T=3)

0.99 (1.0; 1.0)

0.99 (0.9; 1.0)

0.98 (0.9; 1.0)

0.91 (0.8; 1.0)

0.91 (0.8; 1.0)

0.86 (0.7; 1.0)

Boosted (T=10)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

Best of T (T=3)

Best of T (T=10)

Ensemble (T=3)

Ensemble (T=10)

Boosted (T=3)

Boosted (T=10)

−4.49
(−5.4; −4.4)
−4.39
(−4.6; −4.3)
−4.34
(−4.4; −4.3)
−4.46
(−4.8; −4.4)
−4.52
(−4.7; −4.4)
−4.50
(−4.8; −4.4)
−4.55
(−4.6; −4.4)

−6.02
(−86.8; −5.3)
−5.40
(−24.3; −5.2)
−5.24
(−5.4; −5.2)
−5.59
(−6.6; −5.2)
−5.49
(−6.6; −5.2)
−5.32
(−5.8; −5.2)
−5.30
(−5.5; −5.2)

−16.03
(−59.6; −5.5)
−5.57
(−23.5; −5.4)
−5.45
(−5.6; −5.3)
−4.78
(−5.5; −4.6)
−4.98
(−6.5; −4.6)
−4.80
(−5.8; −4.6)
−5.07
(−5.6; −4.7)

−23.65
(−118.8; −5.7)
−9.91
(−35.8; −5.1)
−5.49
(−9.4; −5.0)
−14.71
(−51.9; −5.4)
−5.44
(−6.0; −5.2)
−5.39
(−19.3; −5.1)
−5.25
(−5.5; −4.6)

−126.87
(−250.4; −12.8)
−36.94
(−90.0; −9.7)
−9.72
(−17.3; −6.5)
−6.70
(−28.7; −5.5)
−5.82
(−6.4; −5.5)
−5.56
(−12.4; −5.2)
−5.03
(−5.5; −4.8)

−55.51
(−185.2; −11.2)
−19.12
(−59.2; −9.7)
−9.12
(−16.8; −6.6)
−8.59
(−25.4; −6.1)
−6.08
(−6.3; −5.7)
−8.03
(−28.7; −6.1)
−5.92
(−6.2; −5.6)

Table 2: Performance of the diﬀerent algorithms on varying number of mixtures of Gaussians. The reported
scores are the median and interval deﬁned by the 5% and 95% percentile (in parenthesis) (see Section
4.1.2), over 35 runs for each setting. The top table reports the coverage C, probability mass of Pd covered
by the 5th percentile of Pg deﬁned in Section 4.1.2. The bottom table reports the log likelihood of the
true data under the model Pg. Note that the 95% interval is not the usual conﬁdence interval measuring
the variance of the experiment itself, but rather measures the stability of the diﬀerent algorithms (would
remain even if each experiment was run an inﬁnite number of times). Both the ensemble and the boosting
approaches signiﬁcantly outperform the vanilla GAN even with just three iterations (i.e. just two additional
components). The boosting approach converges faster to the optimal coverage.

28

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

0.98 (0.9; 1.0)

0.86 (0.5; 1.0)

0.66 (0.5; 1.0)

0.61 (0.5; 0.8)

0.55 (0.4; 0.7)

0.58 (0.3; 0.8)

Boosted (T=3)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.98 (0.9; 1.0)

0.93 (0.8; 1.0)

0.97 (0.8; 1.0)

0.87 (0.6; 1.0)

Boosted (T=10)

1.00 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

0.99 (0.9; 1.0)

0.99 (0.8; 1.0)

0.97 (0.8; 1.0)

Beta0.2 (T=3)

0.99 (1.0; 1.0)

0.97 (0.9; 1.0)

0.97 (0.9; 1.0)

0.95 (0.8; 1.0)

0.96 (0.7; 1.0)

0.88 (0.7; 1.0)

Beta0.2 (T=10)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (0.9; 1.0)

1.00 (0.9; 1.0)

Beta0.3 (T=3)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.98 (0.9; 1.0)

0.96 (0.8; 1.0)

0.96 (0.6; 1.0)

0.88 (0.7; 1.0)

Beta0.3 (T=10)

1.00 (1.0; 1.0)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (0.9; 1.0)

0.99 (0.9; 1.0)

Beta0.4 (T=3)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.95 (0.9; 1.0)

0.94 (0.8; 1.0)

0.89 (0.7; 1.0)

0.89 (0.7; 1.0)

Beta0.4 (T=10)

0.99 (1.0; 1.0)

0.99 (0.9; 1.0)

0.96 (0.9; 1.0)

0.97 (0.8; 1.0)

0.99 (0.8; 1.0)

0.90 (0.8; 1.0)

Beta0.5 (T=3)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.97 (0.8; 1.0)

0.82 (0.8; 1.0)

0.86 (0.7; 1.0)

0.81 (0.6; 1.0)

Beta0.5 (T=10)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.97 (0.9; 1.0)

0.84 (0.8; 1.0)

0.87 (0.7; 1.0)

0.91 (0.8; 1.0)

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

Boosted (T=3)

Boosted (T=10)

Beta0.2 (T=3)

Beta0.2 (T=10)

Beta0.3 (T=3)

Beta0.3 (T=10)

Beta0.4 (T=3)

Beta0.4 (T=10)

Beta0.5 (T=3)

Beta0.5 (T=10)

−4.50
(−5.0; −4.4)
−4.56
(−4.9; −4.4)
−4.56
(−4.7; −4.5)
−4.52
(−4.8; −4.4)
−4.58
(−4.8; −4.5)
−4.60
(−4.9; −4.4)
−4.57
(−4.8; −4.4)
−4.62
(−4.9; −4.4)
−4.49
(−4.7; −4.4)
−4.60
(−4.9; −4.4)
−4.62
(−4.8; −4.4)

−5.65
(−72.7; −5.1)
−5.55
(−5.9; −5.2)
−5.46
(−5.6; −5.3)
−5.31
(−5.6; −5.1)
−5.30
(−5.5; −5.2)
−5.34
(−5.7; −5.2)
−5.37
(−5.5; −5.2)
−5.36
(−5.6; −5.1)
−5.40
(−5.7; −5.3)
−5.40
(−5.7; −5.3)
−5.43
(−5.7; −5.2)

−19.63
(−62.1; −5.6)
−5.01
(−6.7; −4.7)
−5.08
(−5.8; −4.7)
−4.85
(−6.3; −4.6)
−4.94
(−6.6; −4.6)
−5.41
(−5.7; −5.1)
−5.27
(−5.6; −5.0)
−4.74
(−5.3; −4.6)
−5.08
(−6.9; −4.7)
−4.77
(−5.4; −4.6)
−5.12
(−6.6; −4.7)

−28.16
(−293.1; −16.3)
−5.49
(−18.7; −4.9)
−5.04
(−5.5; −4.6)
−5.33
(−14.4; −4.8)
−5.23
(−5.5; −4.7)
−5.33
(−12.9; −4.9)
−5.26
(−5.6; −5.0)
−5.34
(−26.2; −4.9)
−5.49
(−5.9; −5.2)
−5.63
(−24.5; −5.2)
−5.48
(−8.4; −5.1)

−56.94
(−248.1; −14.3)
−5.60
(−14.5; −5.0)
−5.51
(−5.9; −5.1)
−5.68
(−26.2; −5.2)
−5.60
(−6.0; −5.3)
−5.68
(−11.0; −5.4)
−5.71
(−6.0; −5.3)
−5.77
(−37.3; −5.1)
−5.43
(−6.0; −5.1)
−6.05
(−17.9; −5.5)
−5.85
(−6.1; −5.3)

−71.11
(−184.8; −12.5)
−6.86
(−47.3; −5.6)
−5.51
(−6.0; −5.2)
−6.13
(−32.7; −5.7)
−5.98
(−6.1; −5.7)
−6.41
(−29.2; −5.6)
−5.82
(−6.1; −5.4)
−12.37
(−75.9; −5.9)
−5.68
(−6.2; −5.2)
−8.29
(−23.1; −6.1)
−6.31
(−7.7; −6.0)

Table 3: Performance with constant β, exploring a range of possible values. The reported scores are the
median and interval deﬁned by the 5% and 95% percentile (in parenthesis) (see Section 4.1.2), over 35 runs
for each setting. The top table reports the coverage C, probability mass of Pd covered by the 5th percentile
of Pg deﬁned in Section 4.1.2. The bottom table reports the log likelihood of the true data under Pg.

29

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

0.96 (0.9; 1.0)

0.90 (0.5; 1.0)

0.65 (0.5; 1.0)

0.61 (0.5; 0.8)

0.69 (0.3; 0.8)

0.59 (0.3; 0.7)

Boosted (T=3)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.98 (0.9; 1.0)

0.93 (0.8; 1.0)

0.97 (0.8; 1.0)

0.87 (0.6; 1.0)

Boosted (T=10)

1.00 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

0.99 (0.9; 1.0)

0.99 (0.8; 1.0)

0.97 (0.8; 1.0)

TopKLast0.1 (T=3)

0.98 (0.9; 1.0)

0.93 (0.8; 1.0)

0.89 (0.6; 1.0)

0.72 (0.5; 1.0)

0.68 (0.5; 0.9)

0.51 (0.4; 0.7)

TopKLast0.1 (T=10)

0.99 (0.9; 1.0)

0.97 (0.8; 1.0)

0.90 (0.7; 1.0)

0.67 (0.4; 0.9)

0.61 (0.5; 0.8)

0.58 (0.4; 0.8)

TopKLast0.3 (T=3)

0.99 (0.9; 1.0)

0.97 (0.9; 1.0)

0.93 (0.7; 1.0)

0.81 (0.7; 1.0)

0.84 (0.7; 1.0)

0.78 (0.5; 1.0)

TopKLast0.3 (T=10)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.95 (0.7; 1.0)

0.94 (0.7; 1.0)

0.89 (0.7; 1.0)

0.88 (0.7; 1.0)

TopKLast0.5 (T=3)

0.98 (0.9; 1.0)

0.98 (0.9; 1.0)

0.95 (0.9; 1.0)

0.95 (0.8; 1.0)

0.86 (0.7; 1.0)

0.86 (0.6; 0.9)

TopKLast0.5 (T=10)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.98 (1.0; 1.0)

0.99 (0.8; 1.0)

0.99 (0.8; 1.0)

1.00 (0.8; 1.0)

TopKLast0.7 (T=3)

0.98 (1.0; 1.0)

0.98 (0.9; 1.0)

0.94 (0.9; 1.0)

0.83 (0.7; 1.0)

0.87 (0.6; 1.0)

0.82 (0.7; 1.0)

TopKLast0.7 (T=10)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

0.98 (0.8; 1.0)

0.99 (0.9; 1.0)

0.95 (0.8; 1.0)

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

Boosted (T=3)

Boosted (T=10)

TopKLast0.1 (T=3)

TopKLast0.1 (T=10)

TopKLast0.3 (T=3)

TopKLast0.3 (T=10)

TopKLast0.5 (T=3)

TopKLast0.5 (T=10)

TopKLast0.7 (T=3)

TopKLast0.7 (T=10)

−4.94
(−5.5; −4.4)
−4.56
(−4.9; −4.4)
−4.56
(−4.7; −4.5)
−4.98
(−5.2; −4.7)
−4.98
(−5.3; −4.7)
−4.73
(−5.1; −4.5)
−4.62
(−4.8; −4.5)
−4.59
(−4.9; −4.4)
−4.59
(−4.8; −4.5)
−4.56
(−4.7; −4.4)
−4.52
(−4.7; −4.5)

−6.18
(−51.7; −5.6)
−5.55
(−5.9; −5.2)
−5.46
(−5.6; −5.3)
−5.64
(−6.1; −5.4)
−5.57
(−5.9; −5.3)
−5.48
(−6.0; −5.2)
−5.41
(−5.7; −5.2)
−5.29
(−5.7; −5.2)
−5.35
(−5.6; −5.2)
−5.37
(−5.5; −5.2)
−5.29
(−5.4; −5.2)

−31.85
(−100.3; −5.8)
−5.01
(−6.7; −4.7)
−5.08
(−5.8; −4.7)
−5.70
(−6.3; −5.2)
−5.37
(−6.0; −5.0)
−5.22
(−5.7; −4.8)
−4.90
(−5.2; −4.7)
−5.41
(−5.9; −4.9)
−5.12
(−5.5; −4.9)
−5.05
(−11.1; −4.7)
−5.05
(−6.6; −4.7)

−47.73
(−155.1; −14.2)
−5.49
(−18.7; −4.9)
−5.04
(−5.5; −4.6)
−5.39
(−38.4; −5.0)
−5.57
(−45.1; −4.7)
−5.42
(−21.6; −5.0)
−5.24
(−5.8; −4.9)
−5.48
(−18.5; −5.0)
−5.35
(−5.6; −4.8)
−5.63
(−43.1; −5.0)
−5.38
(−5.9; −5.1)

−107.36
(−390.8; −14.8)
−5.60
(−14.5; −5.0)
−5.51
(−5.9; −5.1)
−7.00
(−66.6; −5.4)
−7.34
(−16.1; −5.3)
−5.76
(−13.6; −5.1)
−5.71
(−6.2; −5.1)
−5.82
(−15.6; −5.2)
−5.34
(−5.8; −4.9)
−5.99
(−24.8; −5.4)
−5.77
(−6.3; −5.3)

−59.19
(−264.3; −18.8)
−6.86
(−47.3; −5.6)
−5.51
(−6.0; −5.2)
−12.70
(−44.2; −6.7)
−8.86
(−27.6; −5.5)
−7.26
(−36.2; −5.5)
−5.75
(−7.4; −5.1)
−6.78
(−18.7; −6.0)
−6.00
(−6.3; −5.6)
−7.76
(−25.2; −5.9)
−6.10
(−6.4; −6.0)

Table 4: Reweighting similar to “Cascade GAN” from [14], i.e. keep the top r fraction of examples, based on
the discriminator corresponding to the previous generator. The mixture weights are all equal (i.e. β = 1/t).
The reported scores are the median and interval deﬁned by the 5% and 95% percentile (in parenthesis) (see
Section 4.1.2), over 35 runs for each setting. The top table reports the coverage C, probability mass of Pd
covered by the 5th percentile of Pg deﬁned in Section 4.1.2. The bottom table reports the log likelihood of
the true data under Pg.

30

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

0.97 (0.9; 1.0)

0.77 (0.5; 1.0)

0.65 (0.5; 0.9)

0.70 (0.5; 0.8)

0.61 (0.5; 0.8)

0.58 (0.3; 0.8)

Boosted (T=3)

0.99 (1.0; 1.0)

0.99 (0.9; 1.0)

0.97 (0.9; 1.0)

0.95 (0.8; 1.0)

0.91 (0.8; 1.0)

0.89 (0.8; 1.0)

Boosted (T=10)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

TopK0.1 (T=3)

0.98 (0.9; 1.0)

0.98 (0.8; 1.0)

0.91 (0.7; 1.0)

0.84 (0.7; 1.0)

0.80 (0.5; 0.9)

0.60 (0.4; 0.7)

TopK0.1 (T=10)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

0.98 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

0.96 (0.8; 1.0)

TopK0.3 (T=3)

0.98 (0.9; 1.0)

0.98 (0.9; 1.0)

0.95 (0.9; 1.0)

0.95 (0.8; 1.0)

0.84 (0.6; 1.0)

0.79 (0.5; 1.0)

TopK0.3 (T=10)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

0.98 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

TopK0.5 (T=3)

0.99 (0.9; 1.0)

0.99 (1.0; 1.0)

0.96 (0.9; 1.0)

0.98 (0.8; 1.0)

0.88 (0.7; 1.0)

0.88 (0.6; 1.0)

TopK0.5 (T=10)

1.00 (1.0; 1.0)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

TopK0.7 (T=3)

0.98 (1.0; 1.0)

0.98 (0.9; 1.0)

0.94 (0.8; 1.0)

0.84 (0.8; 1.0)

0.86 (0.7; 1.0)

0.81 (0.7; 1.0)

TopK0.7 (T=10)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (0.8; 1.0)

1.00 (0.9; 1.0)

1.00 (0.9; 1.0)

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

Boosted (T=3)

Boosted (T=10)

TopK0.1 (T=3)

TopK0.1 (T=10)

TopK0.3 (T=3)

TopK0.3 (T=10)

TopK0.5 (T=3)

TopK0.5 (T=10)

TopK0.7 (T=3)

TopK0.7 (T=10)

−4.61
(−5.5; −4.4)
−4.59
(−4.9; −4.4)
−4.61
(−4.7; −4.5)
−4.93
(−5.3; −4.7)
−4.60
(−4.8; −4.5)
−4.65
(−4.9; −4.4)
−4.56
(−4.8; −4.5)
−4.60
(−4.8; −4.5)
−4.59
(−4.7; −4.5)
−4.60
(−5.0; −4.4)
−4.59
(−4.7; −4.5)

−5.92
(−94.2; −5.2)
−5.32
(−5.7; −5.2)
−5.30
(−5.4; −5.2)
−5.85
(−6.2; −5.4)
−5.47
(−5.7; −5.3)
−5.40
(−5.9; −5.3)
−5.32
(−5.5; −5.2)
−5.34
(−5.6; −5.2)
−5.31
(−5.4; −5.2)
−5.44
(−5.6; −5.2)
−5.34
(−5.5; −5.2)

−12.40
(−53.1; −5.3)
−5.60
(−5.8; −5.5)
−5.48
(−5.6; −5.2)
−5.38
(−5.7; −5.0)
−4.81
(−5.1; −4.7)
−4.98
(−6.2; −4.7)
−5.07
(−5.9; −4.7)
−5.34
(−5.7; −5.0)
−5.13
(−5.5; −4.9)
−5.62
(−6.0; −5.4)
−5.51
(−5.6; −5.4)

−59.62
(−154.6; −9.8)
−5.40
(−24.2; −4.5)
−4.84
(−5.1; −4.3)
−5.34
(−5.8; −4.8)
−4.90
(−5.3; −4.2)
−5.25
(−11.4; −4.7)
−5.08
(−5.4; −4.5)
−5.42
(−19.0; −5.0)
−5.35
(−5.7; −4.8)
−5.49
(−22.2; −5.0)
−5.35
(−5.8; −5.0)

−66.95
(−191.5; −9.7)
−5.71
(−14.0; −5.1)
−5.25
(−5.9; −4.8)
−5.79
(−32.1; −5.2)
−4.85
(−5.6; −4.1)
−5.96
(−28.0; −5.5)
−5.16
(−5.9; −4.9)
−5.59
(−34.7; −4.9)
−5.33
(−5.8; −4.8)
−5.64
(−27.7; −5.3)
−5.32
(−6.0; −5.1)

−63.49
(−431.6; −14.5)
−6.96
(−17.1; −5.9)
−5.95
(−6.1; −5.5)
−7.09
(−20.7; −5.9)
−4.57
(−5.3; −4.2)
−7.34
(−25.4; −5.9)
−5.82
(−6.2; −5.3)
−6.15
(−14.8; −5.6)
−5.72
(−6.2; −5.3)
−7.17
(−22.5; −6.0)
−6.11
(−6.4; −5.9)

Table 5: Reweighting using the top r fraction of examples, based on the discriminator corresponding to the
mixture of all previous generators. The mixture weights are all equal (i.e. β = 1/t). The reported scores
are the median and interval deﬁned by the 5% and 95% percentile (in parenthesis) (see Section 4.1.2), over
35 runs for each setting. The top table reports the coverage C, probability mass of Pd covered by the 5th
percentile of Pg deﬁned in Section 4.1.2. The bottom table reports the log likelihood of the true data under
Pg.

31

7
1
0
2
 
y
a
M
 
4
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
6
8
3
2
0
.
1
0
7
1
:
v
i
X
r
a

AdaGAN: Boosting Generative Models

Ilya Tolstikhin1, Sylvain Gelly2, Olivier Bousquet2, Carl-Johann Simon-Gabriel1, and
Bernhard Sch¨olkopf1

1Max Planck Institute for Intelligent Systems
2Google Brain

Abstract

Generative Adversarial Networks (GAN) [1] are an eﬀective method for training generative models of
complex data such as natural images. However, they are notoriously hard to train and can suﬀer from
the problem of missing modes where the model is not able to produce examples in certain regions of the
space. We propose an iterative procedure, called AdaGAN, where at every step we add a new component
into a mixture model by running a GAN algorithm on a reweighted sample. This is inspired by boosting
algorithms, where many potentially weak individual predictors are greedily aggregated to form a strong
composite predictor. We prove that such an incremental procedure leads to convergence to the true
distribution in a ﬁnite number of steps if each step is optimal, and convergence at an exponential rate
otherwise. We also illustrate experimentally that this procedure addresses the problem of missing modes.

1 Introduction

Imagine we have a large corpus, containing unlabeled pictures of animals, and our task is to build a generative
probabilistic model of the data. We run a recently proposed algorithm and end up with a model which
produces impressive pictures of cats and dogs, but not a single giraﬀe. A natural way to ﬁx this would be
to manually remove all cats and dogs from the training set and run the algorithm on the updated corpus.
The algorithm would then have no choice but to produce new animals and, by iterating this process until
there’s only giraﬀes left in the training set, we would arrive at a model generating giraﬀes (assuming suﬃcient
sample size). At the end, we aggregate the models obtained by building a mixture model. Unfortunately, the
described meta-algorithm requires manual work for removing certain pictures from the unlabeled training
set at every iteration.

Let us turn this into an automatic approach, and rather than including or excluding a picture, put
continuous weights on them. To this end, we train a binary classiﬁer to separate “true” pictures of the
original corpus from the set of “synthetic” pictures generated by the mixture of all the models trained so
far. We would expect the classiﬁer to make conﬁdent predictions for the true pictures of animals missed
by the model (giraﬀes), because there are no synthetic pictures nearby to be confused with them. By
a similar argument, the classiﬁer should make less conﬁdent predictions for the true pictures containing
animals already generated by one of the trained models (cats and dogs). For each picture in the corpus,
we can thus use the classiﬁer’s conﬁdence to compute a weight which we use for that picture in the next
iteration, to be performed on the re-weighted dataset.

The present work provides a principled way to perform this re-weighting, with theoretical guarantees

showing that the resulting mixture models indeed approach the true data distribution.1

Before discussing how to build the mixture, let us consider the question of building a single generative
model. A recent trend in modelling high dimensional data such as natural images is to use neural net-
works [2, 1]. One popular approach are Generative Adversarial Networks (GAN) [1], where the generator

1Note that the term “mixture” should not be interpreted to imply that each component models only one mode: the models

to be combined into a mixture can themselves cover multiple modes.

1

is trained adversarially against a classiﬁer, which tries to diﬀerentiate the true from the generated data.
While the original GAN algorithm often produces realistically looking data, several issues were reported in
the literature, among which the missing modes problem, where the generator converges to only one or a few
modes of the data distribution, thus not providing enough variability in the generated data. This seems
to match the situation described earlier, which is why we will most often illustrate our algorithm with a
GAN as the underlying base generator. We call it AdaGAN, for Adaptive GAN, but we could actually use
any other generator: a Gaussian mixture model, a VAE [2], a WGAN [3], or even an unrolled [4] or mode-
regularized GAN [5], which were both already speciﬁcally developed to tackle the missing mode problem.
Thus, we do not aim at improving the original GAN or any other generative algorithm. We rather propose
and analyse a meta-algorithm that can be used on top of any of them. This meta-algorithm is similar in
spirit to AdaBoost [6] in the sense that each iteration corresponds to learning a “weak” generative model
(e.g., GAN) with respect to a re-weighted data distribution. The weights change over time to focus on the
“hard” examples, i.e. those that the mixture has not been able to properly generate so far.

1.1 Boosting via Additive Mixtures

Motivated by the problem of missing modes, in this work we propose to use multiple generative models
combined into a mixture. These generative models are trained iteratively by adding, at each step, another
model to the mixture that should hopefully cover the areas of the space not covered by the previous mixture
components.2 We show analytically that the optimal next mixture component can be obtained by reweighting
the true data, and thus propose to use the reweighted data distribution as the target for the optimization
of the next mixture components. This leads us naturally to a meta-algorithm, which is similar in spirit to
AdaBoost in the sense that each iteration corresponds to learning a “weak” generative model (e.g., GAN)
with respect to a reweighted data distribution. The latter adapts over time to focus on the “hard” examples,
i.e. those that the mixture has not been able to properly generate thus far.

Before diving into the technical details we provide an informal intuitive discussion of our new meta-
algorithm, which we call AdaGAN (a shorthand for Adaptive GAN, similar to AdaBoost). The pseudocode
is presented in Algorithm 1.

On the ﬁrst step we run the GAN algorithm (or some other generative model) in the usual way and
initialize our generative model with the resulting generator G1. On every t-th step we (a) pick the mixture
weight βt for the next component, (b) update weights Wt of examples from the training set in such a way to
bias the next component towards “hard” ones, not covered by the current mixture of generators Gt−1, (c) run
the GAN algorithm, this time importance sampling mini-batches according to the updated weights Wt,
t , and ﬁnally (d) update our mixture of generators Gt = (1 − βt)Gt−1 + βtGc
resulting in a new generator Gc
t
(notation expressing the mixture of Gt−1 and Gc
t with probabilities 1 − βt and βt). This procedure outputs T
generator functions Gc
T and T corresponding non-negative weights α1, . . . , αT , which sum to one. For
sampling from the resulting model we ﬁrst deﬁne a generator Gc
i , by sampling the index i from a multinomial
distribution with parameters α1, . . . , αT , and then we return Gc
i (Z), where Z ∼ PZ is a standard latent noise
variable used in the GAN literature.

1, . . . , Gc

The eﬀect of the described procedure is illustrated in a toy example in Figure 1. On the left images,
the red dots are the training (true data) points, the blue dots are points sampled from the model mixture
of generators Gt. The background colour gives the density of the distribution corresponding to Gt, non
zero around the generated points, (almost) zero everywhere else. On the right images, the color corresponds
to the weights of training points, following the reweighting scheme proposed in this work. The top row
corresponds to the ﬁrst iteration of AdaGAN, and the bottom row to the second iteration. After the ﬁrst
iteration (the result of the vanilla GAN), we see that only the top left mode is covered, while the three
other modes are not covered at all. The new weights (top right) show that the examples from covered mode
are aggressively downweighted. After the second iteration (bottom left), the combined generator can then
generate two modes.

2Note that the term “mixture” should not be interpreted to imply that each component models only one mode: the models

to be combined into a mixture can themselves cover multiple modes already.

2

Algorithm 1: AdaGAN, a meta-algorithm to construct a “strong” mixture of T individual GANs,
trained sequentially. The mixture weight schedule ChooseMixtureWeight and the training set reweight-
ing schedule UpdateTrainingWeights should be provided by the user. Section 3 gives a complete instance
of this family.

Input: Training sample SN := {X1, . . . , XN }.

Output: Mixture generative model G = GT .

Train vanilla GAN:

W1 = (1/N, . . . , 1/N )

G1 = GAN(SN , Wt)

for t = 2, . . . , T do

#Choose a mixture weight for the next component

βt = ChooseMixtureWeight(t)

#Update weights of training examples

Wt = UpdateTrainingWeights(Gt−1, SN , βt)
#Train t-th “weak” component generator Gc
t
Gc

t = GAN(SN , Wt)

#Update the overall generative model
#Notation below means forming a mixture of Gt−1 and Gc
t .
Gt = (1 − βt)Gt−1 + βtGc
t

end for

3

Although motivated by GANs, we cast our results in the general framework of the minimization of an
f -divergence (cf. [7]) with respect to an additive mixture of distributions. We also note that our approach
may be combined with diﬀerent “weak” generative models, including but not limited to GAN.

Figure 1: A toy illustration of the missing mode problem and the eﬀect of sample reweighting, following the
discussion in Section 1.1. On the left images, the red dots are the training (true data) points, the blue dots
are points sampled from the model mixture of generators Gt. On the right images, the color corresponds
to the weights of training points, following the reweighting scheme proposed in this work. The top row
corresponds to the ﬁrst iteration of AdaGAN, and the bottom row to the second iteration.

1.2 Related Work

Several authors [8, 9, 10] have proposed to use boosting techniques in the context of density estimation by
incrementally adding components in the log domain. In particular, the work of Grover and Ermon [10], done
in parallel to and independent of ours, is applying this idea to GANs. A major downside of these approaches
is that the resulting mixture is a product of components and sampling from such a model is nontrivial (at
least when applied to GANs where the model density is not expressed analytically) and requires to use
techniques such as Annealed Importance Sampling [11] for the normalization.

Rosset and Segal [12] proposed to use an additive mixture model in the case where the log likelihood
can be computed. They derived the update rule via computing the steepest descent direction when adding
a component with inﬁnitesimal weight. This leads to an update rule which is degenerate if the generative
model can produce arbitrarily concentrated distributions (indeed the optimal component is just a Dirac
distribution) which is thus not suitable for the GAN setting. Moreover, their results do not apply once the

4

weight β becomes non-inﬁnitesimal. In contrast, for any ﬁxed weight of the new component our approach
gives the overall optimal update (rather than just the best direction), and applies to any f -divergence.
Remarkably, in both theories, improvements of the mixture are guaranteed only if the new “weak” learner
is still good enough (see Conditions 14&15)

Similarly, Barron and Li [13] studied the construction of mixtures minimizing the Kullback divergence
and proposed a greedy procedure for doing so. They also proved that under certain conditions, ﬁnite mixtures
can approximate arbitrary mixtures at a rate 1/k where k is the number of components in the mixture when
the weight of each newly added component is 1/k. These results are speciﬁc to the Kullback divergence but
are consistent with our more general results.

Wang et al. [14] propose an additive procedure similar to ours but with a diﬀerent reweighting scheme,
which is not motivated by a theoretical analysis of optimality conditions. On every new iteration the authors
propose to run GAN on the top k training examples with maximum value of the discriminator from the last
iteration. Empirical results of Section 4 show that this heuristic often fails to address the missing modes
problem.

Finally, many papers investigate completely diﬀerent approaches for addressing the same issue by directly
modifying the training objective of an individual GAN. For instance, Che et al. [5] add an autoencoding cost
to the training objective of GAN, while Metz et al. [4] allow the generator to “look few steps ahead” when
making a gradient step.

The paper is organized as follows. In Section 2 we present our main theoretical results regarding opti-
mization of mixture models under general f -divergences. In particular we show that it is possible to build
an optimal mixture in an incremental fashion, where each additional component is obtained by applying a
GAN-style procedure with a reweighted distribution. In Section 2.5 we show that if the GAN optimization
at each step is perfect, the process converges to the true data distribution at exponential rate (or even
in a ﬁnite number of steps, for which we provide a necessary and suﬃcient condition). Then we show in
Section 2.6 that imperfect GAN solutions still lead to the exponential rate of convergence under certain
“weak learnability” conditions. These results naturally lead us to a new boosting-style iterative procedure
for constructing generative models, which is combined with GAN in Section 3, resulting in a new algorithm
called AdaGAN. Finally, we report initial empirical results in Section 4, where we compare AdaGAN with
several benchmarks, including original GAN, uniform mixture of multiple independently trained GANs, and
iterative procedure of Wang et al. [14].

2 Minimizing f -divergence with Additive Mixtures

In this section we derive a general result on the minimization of f -divergences over mixture models.

2.1 Preliminaries and notations

In this work we will write Pd and Pmodel to denote a real data distribution and our approximate model
distribution, respectively, both deﬁned over the data space X .

Generative Density Estimation In the generative approach to density estimation, instead of building a
probabilistic model of the data directly, one builds a function G : Z → X that transforms a ﬁxed probability
distribution PZ (often called the noise distribution) over a latent space Z into a distribution over X . Hence
Pmodel is the pushforward of PZ, i.e. Pmodel(A) = PZ(G−1(A)). Because of this deﬁnition, it is generally
impossible to compute the density dPmodel(x), hence it is not possible to compute the log-likelihood of the
training data under the model. However, if PZ is a distribution from which one can sample, it is easy to also
sample from Pmodel (simply sampling from PZ and applying G to each example gives a sample from Pmodel).
So the problem of generative density estimation becomes a problem of ﬁnding a function G such that
Pmodel looks like Pd in the sense that samples from Pmodel and from Pd look similar. Another way to state
this problem is to say that we are given a measure of similarity between distributions D(Pmodel(cid:107)Pd) which

5

can be estimated from samples of those distributions, and thus approximately minimized over a class G of
functions.

f -Divergences
of the data we will use an f -divergence deﬁned in the following way:

In order to measure the agreement between the model distribution and the true distribution

Df (Q(cid:107)P ) :=

(cid:90)

(cid:18) dQ
dP

f

(cid:19)

(x)

dP (x)

(1)

(2)

(3)

for any pair of distributions P, Q with densities dP , dQ with respect to some dominating reference measure µ.
In this work we assume that the function f is convex, deﬁned on (0, ∞), and satisﬁes f (1) = 0. The deﬁnition
of Df holds for both continuous and discrete probability measures and does not depend on speciﬁc choice
of µ.3 It is easy to verify that Df ≥ 0 and it is equal to 0 when P = Q. Note that Df is not symmetric,
but Df (P (cid:107)Q) = Df ◦ (Q(cid:107)P ) for f ◦(x) := xf (1/x) and any P and Q. The f -divergence is symmetric when
f (x) = f ◦(x) for all x ∈ (0, ∞), as in this case Df (P, Q) = Df (Q, P ).

We also note that the divergences corresponding to f (x) and f (x) + C · (x − 1) are identical for any
constant C. In some cases, it is thus convenient to work with f0(x) := f (x) − (x − 1)f (cid:48)(1), (where f (cid:48)(1) is
any subderivative of f at 1) as Df (Q(cid:107)P ) = Df0(Q(cid:107)P ) for all Q and P , while f0 is nonnegative, nonincreasing
on (0, 1], and nondecreasing on (1, ∞). In the remainder, we will denote by F the set of functions that are
suitable for f -divergences, i.e. the set of functions of the form f0 for any convex f with f (1) = 0.

Classical examples of f -divergences include the Kullback-Leibler divergence (obtained for f (x) = − log x,
f0(x) = − log x + x − 1), the reverse Kullback-Leibler divergence (obtained for f (x) = x log x, f0(x) =
x log x − x + 1), the Total Variation distance (f (x) = f0(x) = |x − 1|), and the Jensen-Shannon divergence
(f (x) = f0(x) = −(x + 1) log x+1
2 + x log x). More details can be found in Appendix D. Other examples can
be found in [7]. For further details on f -divergences we refer to Section 1.3 of [15] and [16].

GAN and f -divergences We now explain the connection between the GAN algorithm and f -divergences.
The original GAN algorithm [1] consists in optimizing the following criterion:

min
G

max
D

EPd [log D(X)] + EPZ [log(1 − D(G(Z)))] ,

where D and G are two functions represented by neural networks, and this optimization is actually performed
on a pair of samples (one being the training sample, the other one being created from the chosen distribu-
tion PZ), which corresponds to approximating the above criterion by using the empirical distributions. For
a ﬁxed G, it has been shown in [1] that the optimal D for (2) is given by D∗(x) =
dPd(x)+dPg(x) and plugging
this optimal value into (2) gives the following:

dPd(x)

min
G

− log(4) + 2JS(Pd (cid:107) Pg) ,

where JS is the Jensen-Shannon divergence. Of course, the actual GAN algorithm uses an approximation
to D∗ which is computed by training a neural network on a sample, which means that the GAN algorithm
can be considered to minimize an approximation of (3)4. This point of view can be generalized by plugging
another f -divergence into (3), and it turns out that other f -divergences can be written as the solution to
a maximization of a criterion similar to (2). Indeed, as demonstrated in [7], any f -divergence between Pd
and Pg can be seen as the optimal value of a quantity of the form EPd [f1(D(X))] + EPg [f2(D(G(Z)))] for
appropriate f1 and f2, and thus can be optimized by the same adversarial training technique.

There is thus a strong connection between adversarial training of generative models and minimization of

f -divergences, and this is why we cast the results of this section in the context of general f -divergences.

3The integral in (1) is well deﬁned (but may take inﬁnite values) even if P (dQ = 0) > 0 or Q(dP = 0) > 0. In this case the
integral is understood as Df (Q(cid:107)P ) = (cid:82) f (dQ/dP )1[dP (x)>0,dQ(x)>0]dP (x) + f (0)P (dQ = 0) + f ◦(0)Q(dP = 0), where both
f (0) and f ◦(0) may take value ∞ [15]. This is especially important in case of GAN, where it is impossible to constrain Pmodel
to be absolutely continuous with respect to Pd or vice versa.

4Actually the criterion that is minimized is an empirical version of a lower bound of the Jensen-Shannon divergence.

6

Hilbertian Metrics As demonstrated in [17, 18], several commonly used symmetric f -divergences are
Hilbertian metrics, which in particular means that their square root satisﬁes the triangle inequality. This
is true for the Jensen-Shannon divergence5 as well as for the Hellinger distance and the Total Variation
among others. We will denote by FH the set of f functions such that Df is a Hilbertian metric. For those
divergences, we have Df (P (cid:107)Q) ≤ ((cid:112)Df (P (cid:107)R) + (cid:112)Df (R(cid:107)Q))2.

Generative Mixture Models
a mixture model of the following form:

In order to model complex data distributions, it can be convenient to use

P T

model :=

αiPi,

T
(cid:88)

i=1

where αi ≥ 0, (cid:80)
i αi = 1, and each of the T components is a generative density model. This is very natural
in the generative context, since sampling from a mixture corresponds to a two-step sampling, where one ﬁrst
picks the mixture component (according to the multinomial distribution whose parameters are the αi) and
then samples from it. Also, this allows to construct complex models from simpler ones.

2.2 Incremental Mixture Building

As discussed earlier, in the context of generative modeling, we are given a measure of similarity between
distributions. We will restrict ourselves to the case of f -divergences.
Indeed, for any f -divergence, it
is possible (as explained for example in [7]) to estimate Df (Q (cid:107) P ) from two samples (one from Q, one
from P ) by training a “discriminator” function, i.e. by solving an optimization problem (which is a binary
classiﬁcation problem in the case where the divergence is symmetric6).
It turns out that the empirical
estimate ˆD of Df (Q (cid:107) P ) thus obtained provides a criterion for optimizing Q itself. Indeed, ˆD is a function
of Y1, . . . , Yn ∼ Q and X1, . . . , Xn ∼ P , where Yi = G(Zi) for some mapping function G. Hence it is possible
to optimize ˆD with respect to G (and in particular compute gradients with respect to the parameters of G
if G comes from a smoothly parametrized model such as a neural network).

In this work we thus assume that, given an i.i.d. sample from any unknown distribution P we can construct

a simple model Q ∈ G which approximately minimizes

min
Q∈G

Df (Q (cid:107) P ).

Instead of just modelling the data with a single distribution, we now want to model it with a mixture
of the form (4) where each Pi is obtained by a training procedure of the form (5) with (possibly) diﬀerent
target distributions P for each i.

A natural way to build a mixture is to do it incrementally: we train the ﬁrst model P1 to minimize
Df (P1 (cid:107) Pd) and set the corresponding weight to α1 = 1, leading to P 1
model = P1. Then after having trained
t components P1, . . . , Pt ∈ G we can form the (t + 1)-st mixture model by adding a new component Q with
weight β as follows:

P t+1

model :=

(1 − β)αiPi + βQ.

t
(cid:88)

i=1

Df ((1 − β)Pg + βQ (cid:107) Pd),

We are going to choose β ∈ [0, 1] and Q ∈ G greedily, while keeping all the other parameters of the generative
model ﬁxed, so as to minimize

where we denoted Pg := P t

model the current generative mixture model before adding the new component.

We do not necessarily need to ﬁnd the optimal Q that minimizes (7) at each step. Indeed, it would be
suﬃcient to ﬁnd some Q which allows to build a slightly better approximation of Pd. This means that a

5which means such a property can be used in the context of the original GAN algorithm.
6One example of such a setting is running GANs, which are known to approximately minimize the Jensen-Shannon divergence.

(4)

(5)

(6)

(7)

7

more modest goal could be to ﬁnd Q such that, for some c < 1,

Df ((1 − β)Pg + βQ (cid:107) Pd) ≤ c · Df (Pg (cid:107) Pd) .

(8)

However, we observe that this greedy approach has a signiﬁcant drawback in practice. Indeed, as we
build up the mixture, we need to make β decrease (as P t
model approximates Pd better and better, one should
make the correction at each step smaller and smaller). Since we are approximating (7) using samples from
both distributions, this means that the sample from the mixture will only contain a fraction β of examples
from Q. So, as t increases, getting meaningful information from a sample so as to tune Q becomes harder
and harder (the information is “diluted”).

To address this issue, we propose to optimize an upper bound on (7) which involves a term of the
form Df (Q (cid:107) Q0) for some distribution Q0, which can be computed as a reweighting of the original data
distribution Pd.

In the following sections we will analyze the properties of (7) (Section 2.4) and derive upper bounds that
provide practical optimization criteria for building the mixture (Section 2.3). We will also show that under
certain assumptions, the minimization of the upper bound will lead to the optimum of the original criterion.
This procedure is reminiscent of the AdaBoost algorithm [6], which combines multiple weak predictors
into one very accurate strong composition. On each step AdaBoost adds one new predictor to the current
composition, which is trained to minimize the binary loss on the reweighted training set. The weights are
constantly updated in order to bias the next weak learner towards “hard” examples, which were incorrectly
classiﬁed during previous stages.

2.3 Upper Bounds

Next lemma provides two upper bounds on the divergence of the mixture in terms of the divergence of the
additive component Q with respect to some reference distribution R.

Lemma 1 Let f ∈ F. Given two distributions Pd, Pg and some β ∈ [0, 1], for any distribution Q and any
distribution R such that βdR ≤ dPd, we have

Df ((1 − β)Pg + βQ (cid:107) Pd) ≤ βD(Q (cid:107) R) + (1 − β)Df

Pg (cid:107)

(9)

(cid:18)

Pd − βR
1 − β

(cid:19)

.

If furthermore f ∈ FH , then, for any R, we have

Df ((1 − β)Pg + βQ (cid:107) Pd) ≤

βDf (Q (cid:107) R) +

Df ((1 − β)Pg + βR (cid:107) Pd)

.

(10)

(cid:18)(cid:113)

(cid:113)

(cid:19)2

Proof For the ﬁrst inequality, we use the fact that Df is jointly convex. We write Pd = (1 − β) Pd−βR
which is a convex combination of two distributions when the assumptions are satisﬁed.

1−β + βR

The second inequality follows from using the triangle inequality for the square root of the Hilbertian

metric Df and using convexity of Df in its ﬁrst argument.

We can exploit the upper bounds of Lemma 1 by introducing some well-chosen distribution R and
minimizing with respect to Q. A natural choice for R is a distribution that minimizes the last term of the
upper bound (which does not depend on Q).

2.4 Optimal Upper Bounds

In this section we provide general theorems about the optimization of the right-most terms in the upper
bounds of Lemma 1.

For the upper bound (10), this means we need to ﬁnd R minimizing Df ((1 − β)Pg + βR (cid:107) Pd). The

solution for this problem is given in the following theorem.

8

Theorem 1 For any f -divergence Df , with f ∈ F and f diﬀerentiable, any ﬁxed distributions Pd, Pg, and
any β ∈ (0, 1], the solution to the following minimization problem:

where P is a class of all probability distributions, has the density

min
Q∈P

Df ((1 − β)Pg + βQ (cid:107) Pd),

dQ∗

β(x) =

(λ∗dPd(x) − (1 − β)dPg(x))+

1
β

for some unique λ∗ satisfying (cid:82) dQ∗
Also, λ∗ = 1 if and only if Pd((1 − β)dPg > dPd) = 0, which is equivalent to βdQ∗

β = 1. Furthermore, β ≤ λ∗ ≤ min(1, β/δ), where δ := Pd(dPg = 0).

β = dPd − (1 − β)dPg.

Proof See Appendix C.1.

(cid:1)/β,
Remark 1 The form of Q∗
which would make arguments of the f -divergence identical? Unfortunately, it may be the case that dPd(X) <
(1 − β)dPg(X) for some of X ∈ X , leading to the negative values of dQ.

β may look unexpected at ﬁrst glance: why not setting dQ := (cid:0)dPd −(1−β)dPg

For the upper bound (9), we need to minimize Df

. The solution is given in the next

(cid:16)

Pg (cid:107) Pd−βR
1−β

(cid:17)

theorem.

Theorem 2 Given two distributions Pd, Pg and some β ∈ (0, 1], assume

Let f ∈ F. The solution to the minimization problem

Pd (dPg = 0) < β.

min
Q:βdQ≤dPd

Df

Pg (cid:107)

(cid:18)

(cid:19)

Pd − βQ
1 − β

is given by the distribution

dQ†

β(x) =

(cid:0)dPd(x) − λ†(1 − β)dPg(x)(cid:1)

+

1
β

for a unique λ† ≥ 1 satisfying (cid:82) dQ†

β = 1.

Proof See Appendix C.2.

Remark 2 Notice that the term that we optimized in upper bound (10) is exactly the initial objective (7).
So that Theorem 1 also tells us what the form of the optimal distribution is for the initial objective.

Remark 3 Surprisingly, in both Theorem 1 and 2, the solution does not depend on the choice of the func-
tion f , which means that the solution is the same for any f -divergence. This also means that by replacing
f by f ◦, we get similar results for the criterion written in the other direction, with again the same solution.
Hence the order in which we write the divergence does not matter and the optimal solution is optimal for
both orders.

Remark 4 Note that λ∗ is implicitly deﬁned by a ﬁxed-point equation. In Section 3.1 we will show how it
can be computed eﬃciently in the case of empirical distributions.

Remark 5 Obviously, λ† ≥ λ∗, where λ∗ was deﬁned in Theorem 1. Moreover, we have λ∗ ≤ 1/λ†. Indeed,
it is enough to insert λ† = 1/λ∗ into deﬁnition of Q†

β and check that in this case Q†

β ≥ 1.

9

2.5 Convergence Analysis for Optimal Updates

In previous section we derived analytical expressions for the distributions R minimizing last terms in upper
bounds (9) and (10). Assuming Q can perfectly match R, i.e. Df (Q (cid:107) R) = 0, we are now interested in the
convergence of the mixture (6) to the true data distribution Pd for Q = Q∗

β or Q = Q†
β.

We start with simple results showing that adding Q∗

β or Q†

β to the current mixture would yield a strict

improvement of the divergence.

Lemma 2 Under the conditions of Theorem 1, we have

Df

(cid:0)(1 − β)Pg + βQ∗

β

(cid:13)
(cid:13) Pd

(cid:1) ≤ Df

(cid:0)(1 − β)Pg + βPd

(cid:13)
(cid:13) Pd

(cid:1) ≤ (1 − β)Df (Pg (cid:107) Pd).

(11)

Under the conditions of Theorem 2, we have

(cid:32)

Df

Pg

(cid:13)
(cid:13)

(cid:33)

Pd − βQ†
β
1 − β

≤ Df (Pg (cid:107) Pd) ,

Df

(cid:0)(1 − β)Pg + βQ†

(cid:13)
(cid:13) Pd

β

(cid:1) ≤ (1 − β)Df (Pg (cid:107) Pd).

and

then

Proof The ﬁrst inequality follows immediately from the optimality of Q∗
β (hence the value of the objective
at Q∗
β is smaller than at Pd), and the fact that Df is convex in its ﬁrst argument and Df (Pd(cid:107)Pd) = 0.
The second inequality follows from the optimality of Q†
β is smaller
than its value at Pd which itself satisﬁes the condition βdPd ≤ dPd). For the third inequality, we combine
the second inequality with the ﬁrst inequality of Lemma 1 (with Q = R = Q†

β (hence the value of the objective at Q†

β).

The upper bound (11) of Lemma 2 can be reﬁned if the ratio dPg/dPd is almost surely bounded:

Lemma 3 Under the conditions of Theorem 1, if there exists M > 1 such that

Pd((1 − β)dPg > M dPd) = 0

Df

(cid:0)(1 − β)Pg + βQ∗

β

(cid:13)
(cid:13) Pd

(cid:1) ≤ f (λ∗) +

f (M )(1 − λ∗)
M − 1

.

Proof We use Inequality (20) of Lemma 6 with X = β, Y = (1 − β)dPg/dPd, and c = λ∗. We easily
verify that X + Y = ((1 − β)dPg + βdPd)/dPd and max(c, Y ) = ((1 − β)dPg + βdQ∗
β)/dPd and both have
expectation 1 with respect to Pd. We thus obtain:

Df ((1 − β)Pg + βQ∗

β (cid:107) Pd) ≤ f (λ∗) +

f (M ) − f (λ∗)
M − λ∗

(1 − λ∗) .

Since λ∗ ≤ 1 and f is non-increasing on (0, 1) we get

Df ((1 − β)Pg + βQ∗

β (cid:107) Pd) ≤ f (λ∗) +

f (M )(1 − λ∗)
M − 1

.

Remark 6 This upper bound can be tighter than that of Lemma 2 when λ∗ gets close to 1. Indeed, for
λ∗ = 1 the upper bound is exactly 0 and is thus tight, while the upper bound of Lemma 2 will not be zero in
this case.

10

Imagine repeatedly adding T new components to the current mixture Pg, where on every step we use the
same weight β and choose the components described in Theorem 1. In this case Lemma 2 guarantees that the
original objective value Df (Pg (cid:107) Pd) would be reduced at least to (1 − β)T Df (Pg (cid:107) Pd). This exponential rate
of convergence, which at ﬁrst may look surprisingly good, is simply explained by the fact that Q∗
β depends
on the true distribution Pd, which is of course unknown.

Lemma 2 also suggests setting β as large as possible. This is intuitively clear: the smaller the β, the less
we alter our current model Pg. As a consequence, choosing small β when Pg is far away from Pd would lead
to only minor improvements in objective (7). In fact, the global minimum of (7) can be reached by setting
β = 1 and Q = Pd. Nevertheless, in practice we may prefer to keep β relatively small, preserving what we
learned so far through Pg: for instance, when Pg already covered part of the modes of Pd and we want Q to
cover the remaining ones. We provide further discussions on choosing β in Section 3.2.

In the reminder of this section we study the convergence of (7) to 0 in the case where we use the upper
bound (10) and the weight β is ﬁxed (i.e. the same value at each iteration). This analysis can easily be
extended to a variable β.

Lemma 4 For any f ∈ F such that f (x) (cid:54)= 0 for x (cid:54)= 1, the following conditions are equivalent:

(i) Pd((1 − β)dPg > dPd) = 0;

(ii) Df ((1 − β)Pg + βQ∗

β (cid:107) Pd) = 0.

Proof The ﬁrst condition is equivalent to λ∗ = 1 according to Theorem 1. In this case, (1−β)Pg +βQ∗
β = Pd,
hence the divergence is 0. In the other direction, when the divergence is 0, since f is strictly positive for x (cid:54)= 1
(keep in mind that we can always replace f by f0 to get a non-negative function which will be strictly positive
if f (x) (cid:54)= 0 for x (cid:54)= 1), this means that with Pd probability 1 we have the equality dPd = (1 − β)dPg + βdQ∗
β,
which implies that (1 − β)dPg > dPd with Pd probability 1 and also λ∗ = 1.

This result tells that we can not perfectly match Pd by adding a new mixture component to Pg as long as
there are points in the space where our current model Pg severely over-samples. As an example, consider an
extreme case where Pg puts a positive mass in a region outside of the support of Pd. Clearly, unless β = 1,
we will not be able to match Pd.

Finally, we provide a necessary and suﬃcient condition for the iterative process to converge to the data
distribution Pd in ﬁnite number of steps. The criterion is based on the ratio dP1/dPd, where P1 is the ﬁrst
component of our mixture model.

Corollary 1 Take any f ∈ F such that f (x) (cid:54)= 0 for x (cid:54)= 1. Starting from P 1
iteratively according to P t+1
β, where on every step Q∗
with Pg := P t
exists M > 0 such that

model = P1, update the model
β is as deﬁned in Theorem 1
model (cid:107) Pd) will reach 0 in a ﬁnite number of steps if and only if there

model. In this case Df (P t

model = (1 − β)P t

model + βQ∗

Pd((1 − β)dP1 > M dPd) = 0 .

(12)

When the ﬁnite convergence happens, it takes at most − ln max(M, 1)/ ln(1 − β) steps.

Proof From Lemma 4, it is clear that if M ≤ 1 the convergence happens after the ﬁrst update. So let
us assume M > 1. Notice that dP t+1
β = max(λ∗dPd, (1 − β)dP t
model + βdQ∗
model) so that if
Pd((1 − β)dP t
model > M (1 − β)dPd) = 0. This proves that (12) is a
suﬃcient condition.

model > M dPd) = 0, then Pd((1 − β)dP t+1

model = (1 − β)dP t

Now assume the process converged in a ﬁnite number of steps. Let P t

the ﬁnal step. Note that P t
distribution P . According to Lemma 4 we have Pd((1 − β)dP t
immediately imply (12).

model be a mixture right before
model is represented by (1 − β)t−1P1 + (1 − (1 − β)t−1)P for certain probability
model > dPd) = 0. Together these two facts

It is also important to keep in mind that even if (12) is not satisﬁed the process still converges to the true
distribution at exponential rate (see Lemma 2 as well as Corollaries 2 and 3 below)

11

2.6 Weak to Strong Learnability

In practice the component Q that we add to the mixture is not exactly Q∗
β, but rather an approximation
to them. We need to show that if this approximation is good enough, then we retain the property that (8)
is reached. In this section we will show that this is indeed the case.

β or Q†

Looking again at Lemma 1 we notice that the ﬁrst upper bound is less tight than the second one. Indeed,
take the optimal distributions provided by Theorems 1 and 2 and plug them back as R into the upper bounds
of Lemma 1. Also assume that Q can match R exactly, i.e. we can achieve Df (Q (cid:107) R) = 0. In this case both
sides of (10) are equal to Df ((1 − β)Pg + βQ∗
β (cid:107) Pd), which is the optimal value for the original objective (7).
On the other hand, (9) does not become an equality and the r.h.s. is not the optimal one for (7).

This means that using (10) allows to reach the optimal value of the original objective (7), whereas
using (9) does not. However, this is not such a big issue since, as we mentioned earlier, we only need to
improve the mixture by adding the next component (we do not need to add the optimal next component).
So despite the solution of (7) not being reachable with the ﬁrst upper bound, we will still show that (8) can
be reached.

The ﬁrst result provides suﬃcient conditions for strict improvements when we use the upper bound (9).

Corollary 2 Given two distributions Pd, Pg, and some β ∈ (0, 1], assume

Let Q†

β be as deﬁned in Theorem 2. If Q is a distribution satisfying

Pd

(cid:18) dPg
dPd

(cid:19)

= 0

< β.

Df (Q (cid:107) Q†

β) ≤ γDf (Pg (cid:107) Pd)

for γ ∈ [0, 1] then

Df ((1 − β)Pg + βQ (cid:107) Pd) ≤ (1 − β(1 − γ))Df (Pg (cid:107) Pd).

Proof Immediately follows from combining Lemma 1, Theorem 1, and Lemma 2.

Next one holds for Hilbertian metrics and corresponds to the upper bound (10).

Corollary 3 Assume f ∈ FH , i.e. Df is a Hilbertian metric. Take any β ∈ (0, 1], Pd, Pg, and let Q∗
as deﬁned in Theorem 1. If Q is a distribution satisfying

β be

for some γ ∈ [0, 1], then

Df (Q (cid:107) Q∗

β) ≤ γDf (Pg (cid:107) Pd)

Df ((1 − β)Pg + βQ (cid:107) Pd) ≤

(cid:16)(cid:112)γβ + (cid:112)1 − β

(cid:17)2

Df (Pg (cid:107) Pd) .

In particular, the right-hand side is strictly smaller than Df (Pg (cid:107) Pd) as soon as γ < β/4 (and β > 0).

Immediately follows from combining Lemma 1, Theorem 2, and Lemma 2. It is easy to verify that

Proof
for γ < β/4, the coeﬃcient is less than (β/2 +

√

1 − β)2 which is < 1 (for β > 0).

Remark 7 We emphasize once again that the upper bound (10) and Corollary 3 both hold for Jensen-
Shannon, Hellinger, and total variation divergences among others. In particular they can be applied to the
original GAN algorithm.

Conditions 14 and 15 may be compared to the “weak learnability” condition of AdaBoost. As long as our
weak learner is able to solve the surrogate problem (5) of matching respectively Q†
β accurately enough,
the original objective (7) is guaranteed to decrease as well. It should be however noted that Condition 15

β or Q∗

12

(13)

(14)

(15)

Indeed, as already mentioned before,
with γ < β/4 is perhaps too strong to call it “weak learnability”.
the weight β is expected to decrease to zero as the number of components in the mixture distribution Pg
increases. This leads to γ → 0, making it harder to meet Condition 15. This obstacle may be partially
resolved by the fact that we will use a GAN to ﬁt Q, which corresponds to a relatively rich7 class of models
G in (5). In other words, our weak learner is not so weak.

On the other hand, Condition (14) of Corollary 2 is much milder. No matter what γ ∈ [0, 1] and β ∈ (0, 1]
we choose, the new component Q is guaranteed to strictly improve the objective functional. This comes at
the price of the additional Condition (13), which asserts that β should be larger than the mass of true data
Pd missed by the current model Pg. We argue that this is a rather reasonable condition: if Pg misses many
modes of Pd we would prefer assigning a relatively large weight β to the new component Q.

3 AdaGAN

In this section we provide a more detailed description of Algorithm 1 from Section 1.1, in particular how to
reweight the training examples for the next iteration and how to choose the mixture weights.

In a nutshell, at each iteration we want to add a new component Q to the current mixture Pg with
weight β, to create a mixture with distribution (1 − β)Pg + βQ. This component Q should approach an
“optimal target” Q∗

β and we know from Theorem 1 that:

dQ∗

β =

(cid:18)

dPd
β

λ∗ − (1 − β)

(cid:19)

dPg
dPd

.

+

dPg
dPd

(X) = h(cid:0)D(X)(cid:1).

dQ∗

β =

dPd
β

(cid:0)λ∗ − (1 − β)h(D)(cid:1)

+ ,

wi =

pi
β

(cid:0)λ∗ − (1 − β)h(di)(cid:1)

+

Computing this distribution requires to know the density ratio dPg/dPd, which is not directly accessible,
but it can be estimated using the idea of adversarial training. Indeed, we can train a discriminator D to
distinguish between samples from Pd and Pg. It is known that for an arbitrary f -divergence, there exists
a corresponding function h (see [7]) such that the values of the optimal discriminator D are related to the
density ratio in the following way:

In particular, for the Jensen-Shannon divergence, used by the original GAN algorithm, it holds that h(cid:0)D(X)(cid:1) =

1−D(X)
D(X)

. So in this case for the optimal discriminator we have

which can be viewed as a reweighted version of the original data distribution Pd.

In particular, when we compute dQ∗

β on the training sample SN = (X1, . . . , XN ), each example Xi has

the following weight:

with pi = dPd(Xi) and di = D(Xi). In practice, we use the empirical distribution over the training sample
which means we set pi = 1/N .

3.1 How to compute λ∗ of Theorem 1

Next we derive an algorithm to determine λ∗. We need to ﬁnd a value of λ∗ such that the weights wi in (17)
are normalized, i.e.:

(cid:88)

wi =

(cid:0)λ∗ − (1 − β)h(di)(cid:1) = 1 ,

(cid:88)

i

pi
β

i∈I(λ∗)

7The hardness of meeting Condition 15 of course largely depends on the class of models G used to ﬁt Q in (5). For now we

ignore this question and leave it for future research.

13

(16)

(17)

where I(λ) := {i : λ > (1 − β)h(di)}. This in turn yields:

λ∗ =

β
i∈I(λ∗) pi

(cid:80)



1 +

(1 − β)
β

(cid:88)

i∈I(λ∗)



pih(di)

 .

(18)

Now, to compute the r.h.s., we need to know I(λ∗). To do so, we sort the values h(di) in increasing order:
h(d1) ≤ h(d2) ≤ . . . ≤ h(dN ). Then I(λ∗) is simply a set consisting of the ﬁrst k values, where we have to
determine k. Thus, it suﬃces to test successively all positive integers k until the λ given by Equation (18)
veriﬁes:

(1 − β)h(dk) < λ ≤ (1 − β)h(dk+1) .

This procedure is guaranteed to converge, because by Theorem 1, we know that λ∗ exists, and it satisﬁes (18).
In summary, λ∗ can be determined by Algorithm 2.

Algorithm 2: Determining λ∗
1 Sort the values h(di) in increasing order ;
1 + 1−β
2 Initialize λ ← β
p1
3 while (1 − β)h(dk) ≥ λ do
4

β p1h(d1)

(cid:16)

(cid:17)

k ← k + 1;
λ ← β
(cid:80)k

i=1 pi

(cid:16)

5

1 + (1−β)

β

(cid:80)k

(cid:17)
i=1 pih(di)

and k ← 1 ;

3.2 How to choose a mixture weight β

While for every β there is an optimal reweighting scheme, the weights from (17) depend on β. In particular,
if β is large enough to verify dPd(x)λ∗ − (1 − β)dPg(x) ≥ 0 for all x, the optimal component Q∗
β satisﬁes
(1 − β)Pg + βQ∗
β = Pd, as proved in Lemma 4. In other words, in this case we exactly match the data
distribution Pd, assuming the GAN can approximate the target Q∗
β perfectly. This criterion alone would
lead to choosing β = 1. However in practice we know we can’t get a generator that produces exactly the
target distribution Q∗
β. We thus propose a few heuristics one can follow to choose β:

– Any ﬁxed constant value β for all iterations.

– All generators to be combined with equal weights in the ﬁnal mixture model. This corresponds to

setting βt = 1

t , where t is the iteration.

– Instead of choosing directly a value for β one could pick a ratio 0 < r < 1 of examples which should
have a weight wi > 0. Given such an r, there is a unique value of β (βr) resulting in wi > 0 for exactly
N · r training examples. Such a value βr can be determined by binary search over β in Algorithm 2.
Possible choices for r include:

– r constant, chosen experimentally.
– r decreasing with the number of iterations, e.g., r = c1e−c2t for any positive constants c1, c2.

– Alternatively, one can set a particular threshold for the density ratio estimate h(D), compute the
fraction r of training examples that have a value above that threshold and derive β from this ratio
r (as above). Indeed, when h(D) is large, that means that the generator does not generate enough
examples in that region, and the next iteration should be encouraged to generate more there.

14

Algorithm 3: AdaGAN, a meta-algorithm to construct a “strong” mixture of T individual GANs,
trained sequentially. The mixture weight schedule ChooseMixtureWeight should be provided by the
user (see 3.2). This is an instance of the high level Algorithm 1, instantiating UpdateTrainingWeights.

#Compute the new weights of the training examples (UpdateTrainingWeights)

#Compute the discriminator between the original (unweighted) data and the current mixture Gt−1

Input: Training sample SN := {X1, . . . , XN }.

Output: Mixture generative model G = GT .

Train vanilla GAN: G1 = GAN(SN )

for t = 2, . . . , T do

#Choose a mixture weight for the next component

βt = ChooseMixtureWeight(t)

D ← DGAN (SN , Gt−1);
#Compute λ∗ using Algorithm 2
λ∗ ← λ(βt, D)
#Compute the new weight for each example

for i = 1, . . . , N do

W i

t = 1
N βt

(λ∗ − (1 − βt)h(D(Xi)))+

end for
#Train t-th “weak” component generator Gc
t
Gc

t = GAN(SN , Wt)

#Update the overall generative model
#Notation below means forming a mixture of Gt−1 and Gc
t .
Gt = (1 − βt)Gt−1 + βtGc
t

end for

3.3 Complete algorithm

Now we have all the necessary components to introduce the complete AdaGAN meta-algorithm. The algo-
rithm uses any given GAN implementation (which can be the original one of Goodfellow et al. [1] or any
later modiﬁcations) as a building block. Accordingly, Gc ← GAN (SN , W ) returns a generator Gc for a given
set of examples SN = (X1, . . . , XN ) and corresponding weights W = (w1, . . . , wN ). Additionally, we write
D ← DGAN (SN , G) to denote a procedure that returns a discriminator from the GAN algorithm trained
on a given set of true data examples SN and examples sampled from the mixture of generators G. We
also write λ∗(β, D) to denote the optimal λ∗ given by Algorithm 2. The complete algorithm is presented in
Algorithm 3.

4 Experiments

We tested AdaGAN8 on toy datasets, for which we can interpret the missing modes in a clear and reproducible
way, and on MNIST, which is a high-dimensional dataset. The goal of these experiments was not to evaluate

8Code available online at https://github.com/tolstikhin/adagan

15

the visual quality of individual sample points, but to demonstrate that the re-weighting scheme of AdaGAN
promotes diversity and eﬀectively covers the missing modes.

4.1 Toy datasets

The target distribution is deﬁned as a mixture of normal distributions, with diﬀerent variances. The distances
between the means are relatively large compared to the variances, so that each Gaussian of the mixture is
“isolated”. We vary the number of modes to test how well each algorithm performs when there are fewer or
more expected modes.

More precisely, we set X = R2, each Gaussian component is isotropic, and their centers are sampled
uniformly in a square. That particular random seed is ﬁxed for all experiments, which means that for a
given number of modes, the target distribution is always the same. The variance parameter is the same for
each component, and is decreasing with the number of modes, so that the modes stay apart from each other.
This target density is very easy to learn, using a mixture of Gaussians model, and for example the EM
algorithm [19]. If applied to the situation where the generator is producing single Gaussians (i.e. PZ is a
standard Gaussian and G is a linear function), then AdaGAN produces a mixture of Gaussians, however it
does so incrementally unlike EM, which keeps a ﬁxed number of components. In any way AdaGAN was not
tailored for this particular case and we use the Gaussian mixture model simply as a toy example to illustrate
the missing modes problem.

4.1.1 Algorithms

We compare diﬀerent meta-algorithms based on GAN, and the baseline GAN algorithm. All the meta-
algorithms use the same implementation of the underlying GAN procedure.
In all cases, the generator
uses latent space Z = R5, and two ReLU hidden layers, of size 10 and 5 respectively. The corresponding
discriminator has two ReLU hidden layers of size 20 and 10 respectively. We use 64k training examples, and
15 epochs, which is enough compared to the small scale of the problem, and all networks converge properly
and overﬁtting is never an issue. Despite the simplicity of the problem, there are already diﬀerences between
the diﬀerent approaches.

We compare the following algorithms:

– The baseline GAN algorithm, called Vanilla GAN in the results.

– The best model out of T runs of GAN, that is: run T GAN instances independently, then take the run
that performs best on a validation set. This gives an additional baseline with similar computational
complexity as the ensemble approaches. Note that the selection of the best run is done on the reported
target metric (see below), rather than on the internal metric. As a result this baseline is slightly
overestimated. This procedure is called Best of T in the results.

– A mixture of T GAN generators, trained independently, and combined with equal weights (the “bag-

ging” approach). This procedure is called Ensemble in the results.

– A mixture of GAN generators, trained sequentially with diﬀerent choices of data reweighting:

– The AdaGAN algorithm (Algorithm 1), for β = 1/t, i.e. each component will have the same

weight in the resulting mixture (see § 3.2). This procedure is called Boosted in the results.

– The AdaGAN algorithm (Algorithm 1), for a constant β, exploring several values. This procedure

is called for example Beta0.3 for β = 0.3 in the results.

– Reweighting similar to “Cascade GAN” from [14], i.e. keeping the top r fraction of examples,
based on the discriminator corresponding to the previous generator. This procedure is called for
example TopKLast0.3 for r = 0.3.

– Keep the top r fraction of examples, based on the discriminator corresponding to the mixture of

all previous generators. This procedure is called for example TopK0.3 for r = 0.3.

16

4.1.2 Metrics

To evaluate how well the generated distribution matches the target distribution, we use a coverage metric C.
We compute the probability mass of the true data “covered” by the model distribution Pmodel. More
precisely, we compute C := Pd(dPmodel > t) with t such that Pmodel(dPmodel > t) = 0.95. This metric
is more interpretable than the likelihood, making it easier to assess the diﬀerence in performance of the
algorithms. To approximate the density of Pmodel we use a kernel density estimation method, where the
bandwidth is chosen by cross validation. Note that we could also use the discriminator D to approximate
the coverage as well, using the relation from (16).

Another metric is the likelihood of the true data under the generated distribution. More precisely, we
compute L := 1
i log Pmodel(xi), on a sample of N examples from the data. Note that [20] proposes a
N
more general and elegant approach (but less straightforward to implement) to have an objective measure of
GAN. On the simple problems we tackle here, we can precisely estimate the likelihood.

(cid:80)

In the main results we report the metric C and in Appendix E we report both L and C. For a given
metric, we repeat the run 35 times with the same parameters (but diﬀerent random seeds). For each run, the
learning rate is optimized using a grid search on a validation set. We report the median over those multiple
runs, and the interval corresponding to the 5% and 95% percentiles. Note this is not a conﬁdence interval of
the median, which would shrink to a singleton with an inﬁnite number of runs. Instead, this gives a measure
of the stability of each algorithm. The optimizer is a simple SGD: Adam was also tried but gave slightly less
stable results.

4.1.3 Results

With the vanilla GAN algorithm, we observe that not all the modes are covered (see Figure 1 for an
illustration). Diﬀerent modes (and even diﬀerent number of modes) are possibly covered at each restart of
the algorithm, so restarting the algorithm with diﬀerent random seeds and taking the best (“best of T ”) can
improve the results.

Figure 3 summarizes the performance of the main algorithms on the C metric, as a function of the number
of iterations T . Table 1 gives more detailed results, varying the number of modes for the target distribution.
Appendix E contains details on variants for the reweighting heuristics as well as results for the L metric.

As expected, both the ensemble and the boosting approaches signiﬁcantly outperform the vanilla GAN
and the “best of T ” algorithm. Interestingly, the improvements are signiﬁcant even after just one or two
additional iterations (T = 2 or T = 3). The boosted approach converges much faster.
In addition, the
variance is much lower, improving the likelihood that a given run gives good results. On this setup, the
vanilla GAN approach has a signiﬁcant number of catastrophic failures (visible in the lower bound of the
interval).

Empirical results on combining AdaGAN meta-algorithm with the unrolled GANs [4] are available in

Appendix A.

4.2 MNIST and MNIST3

We ran experiments both on the original MNIST and on the 3-digit MNIST (MNIST3) [5, 4] dataset,
obtained by concatenating 3 randomly chosen MNIST images to form a 3-digit number between 0 and 999.
According to [5, 4], MNIST contains 10 modes, while MNIST3 contains 1000 modes, and these modes can
be detected using the pre-trained MNIST classiﬁer. We combined AdaGAN both with simple MLP GANs
and DCGANs [21]. We used T ∈ {5, 10}, tried models of various sizes and performed a reasonable amount
of hyperparameter search. For the details we refer to Appendix B.

Similarly to [4, Sec 3.3.1] we failed to reproduce the missing modes problem for MNIST3 reported in
[5] and found that simple GAN architectures are capable of generating all 1000 numbers. The authors
of [4] proposed to artiﬁcially introduce the missing modes again by limiting the generators’ ﬂexibility. In
our experiments, GANs trained with the architectures reported in [4] were often generating poorly looking
digits. As a result, the pre-trained MNIST classiﬁer was outputting random labels, which again led to full

17

Figure 2: Coverage C of the true data by the model distribution P T
model, as a function of iterations T .
Experiments correspond to the data distribution with 5 modes. Each blue point is the median over 35 runs.
Green intervals are deﬁned by the 5% and 95% percentiles (see Section 4.1.2).
Iteration 0 is equivalent
to one vanilla GAN. The left plot corresponds to taking the best generator out of T runs. The middle
plot corresponds to the “ensemble GAN”, simply taking a uniform mixture of T independently trained
GAN generators. The right plot corresponds to our boosting approach (AdaGAN), carefully reweighting
the examples based on the previous generators, with βt = 1/t. Both the ensemble and boosting approaches
signiﬁcantly outperform the vanilla approach with few additional iterations. They also outperform taking
the best out of T runs. The boosting outperforms all other approaches. For AdaGAN the variance of the
performance is also signiﬁcantly decreased.

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

0.97 (0.9; 1.0)

0.88 (0.4; 1.0)

0.63 (0.5; 1.0)

0.72 (0.5; 0.8)

0.58 (0.4; 0.8)

0.59 (0.2; 0.7)

Best of T (T=3)

0.99 (1.0; 1.0)

0.96 (0.9; 1.0)

0.91 (0.7; 1.0)

0.80 (0.7; 0.9)

0.84 (0.7; 0.9)

0.70 (0.6; 0.8)

Best of T (T=10)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

0.98 (0.8; 1.0)

0.80 (0.8; 0.9)

0.87 (0.8; 0.9)

0.71 (0.7; 0.8)

Ensemble (T=3)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.93 (0.8; 1.0)

0.78 (0.6; 1.0)

0.85 (0.6; 1.0)

0.80 (0.6; 1.0)

Ensemble (T=10)

1.00 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

0.91 (0.8; 1.0)

0.88 (0.8; 1.0)

0.89 (0.7; 1.0)

TopKLast0.5 (T=3)

0.98 (0.9; 1.0)

0.98 (0.9; 1.0)

0.95 (0.9; 1.0)

0.95 (0.8; 1.0)

0.86 (0.7; 1.0)

0.86 (0.6; 0.9)

TopKLast0.5 (T=10)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.98 (1.0; 1.0)

0.99 (0.8; 1.0)

0.99 (0.8; 1.0)

1.00 (0.8; 1.0)

Boosted (T=3)

0.99 (1.0; 1.0)

0.99 (0.9; 1.0)

0.98 (0.9; 1.0)

0.91 (0.8; 1.0)

0.91 (0.8; 1.0)

0.86 (0.7; 1.0)

Boosted (T=10)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

Table 1: Performance of the diﬀerent algorithms on varying number of mixtures of Gaussians. The reported
score is the coverage C, probability mass of Pd covered by the 5th percentile of Pg deﬁned in Section 4.1.2.
See Table 2 for more metrics. The reported scores are the median and interval deﬁned by the 5% and 95%
percentile (in parenthesis) (see Section 4.1.2), over 35 runs for each setting. Note that the 95% interval is
not the usual conﬁdence interval measuring the variance of the experiment itself, but rather measures the
stability of the diﬀerent algorithms (would remain even if each experiment was run an inﬁnite number of
times). Both the ensemble and the boosting approaches signiﬁcantly outperform the vanilla GAN even with
just three iterations (i.e. just two additional components). The boosting approach converges faster to the
optimal coverage and with smaller variance.

18

coverage of the 1000 numbers. We tried to threshold the conﬁdence of the pre-trained classiﬁer, but decided
that this metric was too ad-hoc.

For MNIST we noticed that the re-weighted distribu-
tion was often concentrating its mass on digits having
very speciﬁc strokes: on diﬀerent rounds it could high-
light thick, thin, vertical, or diagonal digits, indicating
that these traits were underrepresented in the generated
samples (see Figure 3). This suggests that AdaGAN does
a reasonable job at picking up diﬀerent modes of the
dataset, but also that there are more than 10 modes in
MNIST (and more than 1000 in MNIST3). It is not clear
how to evaluate the quality of generative models in this
context.

We also tried to use the “inversion” metric discussed
in Section 3.4.1 of [4]. For MNIST3 we noticed that a
single GAN was capable of reconstructing most of the
training points very accurately both visually and in the
(cid:96)2-reconstruction sense.

5 Conclusion

Figure 3: Digits from the MNIST dataset cor-
responding to the smallest (left) and largest
(right) weights, obtained by the AdaGAN pro-
cedure (see Section 3) in one of the runs. Bold
digits (left) are already covered and next GAN
will concentrate on thin (right) digits.

We presented an incremental procedure for constructing
an additive mixture of generative models by minimizing
an f -divergence criterion. Based on this, we derived a boosting-style algorithm for GANs, which we call
AdaGAN. By incrementally adding new generators into a mixture through the optimization of a GAN
criterion on a reweighted data, this algorithm is able to progressively cover all the modes of the true data
distribution. This addresses one of the main practical issues of training GANs.

We also presented a theoretical analysis of the convergence of this incremental procedure and showed
conditions under which the mixture converges to the true distribution either exponentially or in a ﬁnite
number of steps.

Our preliminary experiments (on toy data) show that this algorithm is eﬀectively addressing the missing

modes problem and allows to robustly produce a mixture which covers all modes of the data.

However, since the generative model that we obtain is not a single neural network but a mixture of such
networks, the corresponding latent representation no longer has a smooth structure. This can be seen as a
disadvantage compared to standard GAN where one can perform smooth interpolation in latent space. On
the other hand it also allows to have a partitioned latent representation where one component is discrete.
Future work will explore the possibility of leveraging this structure to model discrete aspects of the dataset,
such as the class in object recognition datasets in a similar spirit to [22].

References

[1] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Pro-
cessing Systems, pages 2672–2680, 2014.

[2] D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In ICLR, 2014.

[3] Martin Arjovsky, Soumith Chintala, and Lon Bottou. Wasserstein GAN. arXiv:1701.07875, 2017.

[4] L. Metz, B. Poole, D. Pfau, and J. Sohl-Dickstein. Unrolled generative adversarial networks.

arXiv:1611.02163, 2017.

19

[5] Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized generative

adversarial networks. arXiv:1612.02136, 2016.

[6] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application

to boosting. Journal of Computer and System Sciences, 55(1):119–139, 1997.

[7] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka.

f-GAN: Training generative neural samplers
using variational divergence minimization. In Advances in Neural Information Processing Systems, 2016.

[8] Max Welling, Richard S. Zemel, and Geoﬀrey E. Hinton. Self supervised boosting.

In Advances in

neural information processing systems, pages 665–672, 2002.

[9] Zhuowen Tu. Learning generative models via discriminative approaches. In 2007 IEEE Conference on

Computer Vision and Pattern Recognition, pages 1–8. IEEE, 2007.

[10] Aditya Grover and Stefano Ermon. Boosted generative models. ICLR 2017 conference submission, 2016.

[11] R. M. Neal. Annealed importance sampling. Statistics and Computing, 11(2):125–139, 2001.

[12] Saharon Rosset and Eran Segal. Boosting density estimation.

In Advances in Neural Information

Processing Systems, pages 641–648, 2002.

[13] A Barron and J Li. Mixture density estimation. Biometrics, 53:603–618, 1997.

[14] Yaxing Wang, Lichao Zhang, and Joost van de Weijer. Ensembles of generative adversarial networks.

arXiv:1612.00991, 2016.

[15] F. Liese and K.-J. Miescke. Statistical Decision Theory. Springer, 2008.

[16] M. D. Reid and R. C. Williamson. Information, divergence and risk for binary experiments. Journal of

Machine Learning Research, 12:731–817, 2011.

[17] Bent Fuglede and Flemming Topsoe. Jensen-shannon divergence and hilbert space embedding. In IEEE

International Symposium on Information Theory, pages 31–31, 2004.

[18] Matthias Hein and Olivier Bousquet. Hilbertian metrics and positive deﬁnite kernels on probability

measures. In AISTATS, pages 136–143, 2005.

[19] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM

algorithm. Journal of the Royal Statistical Society, B, 39:1–38, 1977.

[20] Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger Grosse. On the quantitative analysis of

decoder-based generative models, 2016.

[21] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional

generative adversarial networks. In ICLR, 2016.

[22] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: In-
terpretable representation learning by information maximizing generative adversarial nets. In Advances
in Neural Information Processing Systems, pages 2172–2180, 2016.

20

A Further details on toy experiments

To illustrate the ’meta-algorithm aspect’ of AdaGAN, we also performed experiments with an unrolled
GAN [4] instead of a GAN as the base generator. We trained the GANs both with the Jensen-Shannon
objective (2), and with its modiﬁed version proposed in [1] (and often considered as the baseline GAN),
where log(1 − D(G(Z))) is replaced by − log(D(G(Z))). We use the same network architecture as in the
other toy experiments. Figure 4 illustrates our results. We ﬁnd that AdaGAN works with all underlying
GAN algorithms. Note that, where the usual GAN updates the generator and the discriminator once, an
unrolled GAN with 5 unrolling steps updates the generator once and the discriminator 1 + 5, i.e. 6 times
(and then rolls back 5 steps). Thus, in terms of computation time, training 1 single unrolled GAN roughly
corresponds to doing 3 steps of AdaGAN with a usual GAN. In that sense, Figure 4 shows that AdaGAN
(with a usual GAN) signiﬁcantly outperforms a single unrolled GAN. Additionally, we note that using the
Jensen-Shannon objective (rather than the modiﬁed version) seems to have some mode-regularizing eﬀect.
Surprisingly, using unrolling steps makes no signiﬁcant diﬀerence.

Figure 4: Comparison of AdaGAN ran with a GAN (top row) and with an unrolled GAN [4] (bottom).
Coverage C of the true data by the model distribution P T
model, as a function of iterations T . Experiments
are similar to those of Figure 3, but with 10 modes. Top and bottom rows correspond to the usual and the
unrolled GAN (with 5 unrolling steps) respectively, trained with the Jensen-Shannon objective (2) on the
left, and with the modiﬁed objective originally proposed by [1] on the right. In terms of computation time,
one step of AdaGAN with unrolled GAN corresponds to roughly 3 steps of AdaGAN with a usual GAN. On
all images T = 1 corresponds to vanilla unrolled GAN.

B Further details on MNIST/MNIST3 experiments

GAN Architecture We ran AdaGAN on MNIST (28x28 pixel images) using (de)convolutional networks
with batch normalizations and leaky ReLu. The latent space has dimension 100. We used the following

21

architectures:

Generator: 100 x 1 x 1 → fully connected → 7 x 7 x 16 → deconv → 14 x 14 x 8 →

Discriminator: 28 x 28 x 1 → conv → 14 x 14 x 16 → conv → 7 x 7 x 32 →

→ deconv → 28 x 28 x 4 → deconv → 28 x 28 x 1

→ fully connected → 1

where each arrow consists of a leaky ReLu (with 0.3 leak) followed by a batch normalization, conv and deconv
are convolutions and transposed convolutions with 5x5 ﬁlters, and fully connected are linear layers with bias.
The distribution over Z is uniform over the unit box. We use the Adam optimizer with β1 = 0.5, with 2
G steps for 1 D step and learning rates 0.005 for G, 0.001 for D, and 0.0001 for the classiﬁer C that does
the reweighting of digits. We optimized D and G over 200 epochs and C over 5 epochs, using the original
Jensen-Shannon objective (2), without the log trick, with no unrolling and with minibatches of size 128.

Empirical observations Although we could not ﬁnd any appropriate metric to measure the increase of
diversity promoted by AdaGAN, we observed that the re-weighting scheme indeed focuses on digits with
very speciﬁc strokes. In Figure 5 for example, we see that after one AdaGAN step, the generator produces
overly thick digits (top left image). Thus AdaGAN puts small weights on the thick digits of the dataset
(bottom left) and high weights on the thin ones (bottom right). After the next step, the new GAN produces
both thick and thin digits.

C Proofs

C.1 Proof of Theorem 1

Before proving Theorem 1, we introduce two lemmas. The ﬁrst one is about the determination of the constant
λ, the second one is about comparing the divergences of mixtures.

Lemma 5 Let P and Q be two distributions, γ ∈ [0, 1] and λ ∈ R. The function

(cid:90) (cid:18)

g(λ) :=

λ − γ

(cid:19)

dQ
dP

dP

+

g(cid:48)
+(λ) = P (λ · dP ≥ γ · dQ).

is nonnegative, convex, nondecreasing, satisﬁes g(λ) ≤ λ, and its right derivative is given by

The equation

g(λ) = 1 − γ
has a solution λ∗ (unique when γ < 1) with λ∗ ∈ [1 − γ, 1]. Finally, if P (dQ = 0) ≥ δ for a strictly positive
constant δ then λ∗ ≤ (1 − γ)δ−1.

Proof The convexity of g follows immediately from the convexity of x (cid:55)→ (x)+ and the linearity of the
integral. Similarly, since x (cid:55)→ (x)+ is non-decreasing, g is non-decreasing.

We deﬁne the set I(λ) as follows:

Now let us consider g(λ + (cid:15)) − g(λ) for some small (cid:15) > 0. This can also be written:

I(λ) := {x ∈ X : λ · dP (x) ≥ γ · dQ(x)}.

g(λ + (cid:15)) − g(λ) =

(cid:15)dP +

(λ + (cid:15))dP −

I(λ)

I(λ+(cid:15))\I(λ)

I(λ+(cid:15))\I(λ)

(cid:90)

(cid:90)

(cid:90)

(cid:90)

(cid:90)

γdQ

γdQ.

= (cid:15)P (I(λ)) +

(λ + (cid:15))dP −

I(λ+(cid:15))\I(λ)

I(λ+(cid:15))\I(λ)

22

Figure 5: AdaGAN on MNIST. Bottom row are true MNIST digits with smallest (left) and highest (right)
weights after re-weighting at the end of the ﬁrst AdaGAN step. Those with small weight are thick and
resemble those generated by the GAN after the ﬁrst AdaGAN step (top left). After training with the re-
weighted dataset during the second iteration of AdaGAN, the new mixture produces more thin digits (top
right).

On the set I(λ + (cid:15))\I(λ), we have

(λ + (cid:15))dP − γdQ ∈ [0, (cid:15)].

So that

and thus

(cid:15)P (I(γ)) ≤ g(λ + (cid:15)) − g(λ) ≤ (cid:15)P (I(γ)) + (cid:15)P (cid:0)I(λ + (cid:15))\I(λ)(cid:1) = (cid:15)P (I(λ + (cid:15)))

lim
(cid:15)→0+

g(λ + (cid:15)) − g(λ)
(cid:15)

= lim
(cid:15)→0+

P (I(λ + (cid:15))) = P (I(λ)).

This gives the expression of the right derivative of g. Moreover, notice that for λ, γ > 0

g(cid:48)
+(λ) = P (λ · dP ≥ γ · dQ) = P

≤

= 1 − P

>

≥ 1 − γ/λ

(cid:18) dQ
dP

(cid:19)

λ
γ

(cid:18) dQ
dP

(cid:19)

λ
γ

by Markov’s inequality.

23

It is obvious that g(0) = 0. By Jensen’s inequality applied to the convex function x (cid:55)→ (x)+, we have
g(λ) ≥ (λ − γ)+. So g(1) ≥ 1 − γ. Also, g = 0 on R− and g ≤ λ. This means g is continuous on R and
thus reaches the value 1 − γ on the interval (0, 1] which shows the existence of λ∗ ∈ (0, 1]. To show that λ∗
is unique we notice that since g(x) = 0 on R−, g is convex and non-decreasing, g cannot be constant on an
interval not containing 0, and thus g(x) = 1 − γ has a unique solution for γ < 1.

Also by convexity of g,

g(0) − g(λ∗) ≥ −λ∗g(cid:48)

+(λ∗),

which gives λ∗ ≥ (1 − γ)/g(cid:48)
the fact that g(cid:48)

+ is increasing we conclude that λ∗ ≤ (1 − γ)δ−1.

+(λ∗) ≥ 1 − γ since g(cid:48)

+ ≤ 1. If P (dQ = 0) ≥ δ > 0 then also g(cid:48)

+(0) ≥ δ > 0. Using

Next we introduce some simple convenience lemma for comparing convex functions of random variables.

Lemma 6 Let f be a convex function, X, Y be real-valued random variables and c ∈ R be a constant such
that

E [max(c, Y )] = E [X + Y ] .

Then we have the following bound:

If in addition, Y ≤ M a.s. for M ≥ c, then

E [f (max(c, Y ))] ≤ E [f (X + Y )] − E [X(f (cid:48)(Y ) − f (cid:48)(c))+] ≤ E [f (X + Y )] .

E [f (max(c, Y ))] ≤ f (c) +

(E [X + Y ] − c).

f (M ) − f (c)
M − c

Proof We decompose the expectation with respect to the value of the max, and use the convexity of f :

f (X + Y ) − f (max(c, Y )) = 1[Y ≤c](f (X + Y ) − f (c)) + 1[Y >c](f (X + Y ) − f (Y ))

(19)

(20)

≥ 1[Y ≤c]f (cid:48)(c)(X + Y − c) + 1[Y >c]Xf (cid:48)(Y )
= (1 − 1[Y >c])Xf (cid:48)(c) + f (cid:48)(c)(Y − max(c, Y )) + 1[Y >c]Xf (cid:48)(Y )
= f (cid:48)(c)(X + Y − max(c, Y )) + 1[Y >c]X(f (cid:48)(Y ) − f (cid:48)(c))
= f (cid:48)(c)(X + Y − max(c, Y )) + X(f (cid:48)(Y ) − f (cid:48)(c))+,

where we used that f (cid:48) is non-decreasing in the last step. Taking the expectation gives the ﬁrst inequality.

For the second inequality, we use the convexity of f on the interval [c, M ]:

f (max(c, Y )) ≤ f (c) +

(max(c, Y ) − c).

f (M ) − f (c)
M − c

Taking an expectation on both sides gives the second inequality.

Proof [Theorem 1] We ﬁrst apply Lemma 5 with γ = 1 − β and this proves the existence of λ∗ in the interval
(β, 1], which shows that Q∗

β is indeed well-deﬁned as a distribution.

Then we use Inequality (19) of Lemma 6 with X = βdQ/dPd, Y = (1 − β)dPg/dPd, and c = λ∗. We
β)/dPd and both

easily verify that X + Y = ((1 − β)dPg + βdQ)/dPd and max(c, Y ) = ((1 − β)dPg + βdQ∗
have expectation 1 with respect to Pd. We thus obtain for any distribution Q,

Df ((1 − β)Pg + βQ∗

β (cid:107) Pd) ≤ Df ((1 − β)Pg + βQ (cid:107) Pd) .

This proves the optimality of Q∗
β.

24

C.2 Proof of Theorem 2

Lemma 7 Let P and Q be two distributions, γ ∈ (0, 1), and λ ≥ 0. The function

h(λ) :=

− λ

(cid:90) (cid:18) 1
γ

(cid:19)

dQ
dP

dP

+

h(λ) =

1 − γ
γ

is convex, non-increasing, and its right derivative is given by h(cid:48)
∆ := P (dQ(X)/dP (X) = 0). Then the equation

+(λ) = −Q(1/γ ≥ λdQ(X)/dP (X)). Denote

has no solutions if ∆ > 1 − γ, has a single solution λ† ≥ 1 if ∆ < 1 − γ, and has inﬁnitely many or no
solutions when ∆ = 1 − γ.

Proof The convexity of h follows immediately from the convexity of x (cid:55)→ (a − x)+ and the linearity of the
integral. Similarly, since x (cid:55)→ (a − x)+ is non-increasing, h is non-increasing as well.

We deﬁne the set J (λ) as follows:

(cid:26)

J (λ) :=

x ∈ X :

≥ λ

(x)

.

(cid:27)

1
γ

dQ
dP

Now let us consider h(λ) − h(λ + (cid:15)) for any (cid:15) > 0. Note that J (λ + (cid:15)) ⊆ J (λ). We can write:

h(λ) − h(λ + (cid:15)) =

(cid:90)

(cid:90)

(cid:90)

=

=

(cid:18) 1
γ

J (λ)

(cid:19)

(cid:90)

− λ

dP −

dQ
dP
(cid:18) 1
γ
(cid:18) 1
γ

J (λ+(cid:15))
(cid:90)

(cid:19)

dP +

− λ

dQ
dP

dQ
dP

(cid:19)

J (λ)\J (λ+(cid:15))

J (λ)\J (λ+(cid:15))

J (λ+(cid:15))

(cid:18) 1
γ

(cid:19)

dP

dQ
dP
(cid:19)

dP

− (λ + (cid:15))

(cid:18)

(cid:15)

dQ
dP

− λ

dP + (cid:15) · Q(J (λ + (cid:15))).

Note that for x ∈ J (λ) \ J (λ + (cid:15)) we have

0 ≤

− λ

(x) < (cid:15)

(x).

1
γ

dQ
dP

dQ
dP

This gives the following:

which shows that h is continuous. Also

(cid:15) · Q(J (λ + (cid:15))) ≤ h(λ) − h(λ + (cid:15)) ≤ (cid:15) · Q(J (λ + (cid:15))) + (cid:15) · Q(J (λ) \ J (λ + (cid:15))) = (cid:15) · Q(J (λ)),

lim
(cid:15)→0+

h(λ + (cid:15)) − h(λ)
(cid:15)

= lim
(cid:15)→0+

−Q(J (λ + (cid:15))) = −Q(J (λ)).

It is obvious that h(0) = 1/γ and h ≤ γ−1 for λ ≥ 0. By Jensen’s inequality applied to the convex
+. So h(1) ≥ γ−1 − 1. We conclude that h may reach the

function x (cid:55)→ (a − x)+, we have h(λ) ≥ (cid:0)γ−1 − λ(cid:1)
value (1 − γ)/γ = γ−1 − 1 only on [1, +∞). Note that

h(λ) →

P

(X) = 0

=

≥ 0

as λ → ∞.

1
γ

(cid:18) dQ
dP

(cid:19)

∆
γ

Thus if ∆/γ > γ−1 −1 the equation h(λ) = γ−1 −1 has no solutions, as h is non-increasing. If ∆/γ = γ−1 −1
then either h(λ) > γ−1 − 1 for all λ ≥ 0 and we have no solutions or there is a ﬁnite λ(cid:48) ≥ 1 such that

25

h(λ(cid:48)) = γ−1 − 1, which means that the equation is also satisﬁed by all λ ≥ λ(cid:48), as h is continuous and
non-increasing. Finally, if ∆/γ < γ−1 − 1 then there is a unique λ† such that h(λ†) = γ−1 − 1, which follows
from the convexity of h.

Next we introduce some simple convenience lemma for comparing convex functions of random variables.

Lemma 8 Let f be a convex function, X, Y be real-valued random variables such that X ≤ Y a.s., and
c ∈ R be a constant such that9

Then we have the following lower bound:

E [min(c, Y )] = E [X] .

E [f (X) − f (min(c, Y ))] ≥ 0.

Proof We decompose the expectation with respect to the value of the min, and use the convexity of f :

f (X) − f (min(c, Y )) = 1[Y ≤c](f (X) − f (Y )) + 1[Y >c](f (X) − f (c))

≥ 1[Y ≤c]f (cid:48)(Y )(X − Y ) + 1[Y >c](X − c)f (cid:48)(c)
≥ 1[Y ≤c]f (cid:48)(c)(X − Y ) + 1[Y >c](X − c)f (cid:48)(c)
= Xf (cid:48)(c) − min(Y, c)f (cid:48)(c),

where we used the fact that f (cid:48) is non-decreasing in the previous to last step. Taking the expectation we get
the result.

Lemma 9 Let Pg, Pd be two ﬁxed distributions and β ∈ (0, 1). Assume

Pd

(cid:18) dPg
dPd

(cid:19)

= 0

< β.

Let M(Pd, β) be the set of all probability distributions T such that (1 − β)dT ≤ dPd. Then the following
minimization problem:

has the solution T ∗ with density

min
T ∈M(Pd,β)

Df (T (cid:107) Pg)

dT ∗ := min(dPd/(1 − β), λ†dPg),

where λ† is the unique value in [1, ∞) such that (cid:82) dT ∗ = 1.
Proof We will use Lemma 8 with X = dT (Z)/dPg(Z), Y = dPd(Z)/(cid:0)(1 − β)dPg(Z)(cid:1), and c = λ∗, Z ∼ Pg.
We need to verify that assumptions of Lemma 8 are satisﬁed. Obviously, Y ≥ X. We need to show that
there is a constant c such that

Rewriting this equation we get the following equivalent one:

(cid:90)

(cid:18)

min

c,

(cid:19)

dPd
(1 − β)dPg

dPg = 1.

(cid:90)

β =

(dPd − min (c(1 − β)Pg, dPd)) = (1 − β)

(21)

(cid:90) (cid:18) 1

1 − β

− c

(cid:19)

dPg
dPd

+

dPd.

Using the fact that

9Generally it is not guaranteed that such a constant c always exists. In this result we assume this is the case.

Pd

(cid:18) dPg
dPd

(cid:19)

= 0

< β

26

we may apply Lemma 7 and conclude that there is a unique c ∈ [1, ∞) satisfying (21), which we denote λ†.

To conclude the proof of Theorem 2, observe that from Lemma 9, by making the change of variable

T = (Pd − βQ)/(1 − β) we can rewrite the minimization problem as follows:

min
Q: βdQ≤dPd

Df ◦

Pg (cid:107)

(cid:18)

(cid:19)

Pd − βQ
1 − β

and we verify that the solution has the form dQ†
depend on f , the fact that we optimized Df ◦ is irrelevant and we get the same solution for Df .

(cid:0)dPd − λ†(1 − β)dPg

+. Since this solution does not

β = 1
β

(cid:1)

D f -Divergences

Jensen-Shannon This divergence corresponds to

Df (P (cid:107)Q) = JS(P, Q) =

(cid:90)

X

(cid:18) dP
dQ

f

(cid:19)

(x)

dQ(x)

f (u) = −(u + 1) log

+ u log u.

u + 1
2

with

Indeed,

(cid:90)

X
(cid:18) p(x)
q(x)

JS(P, Q) :=

q(x)

−

+ 1

log





(cid:19)

(cid:18) p(x)
q(x)



p(x)
q(x) + 1
2



 +

p(x)
q(x)

log

p(x)
q(z)



 dx

=

=

(cid:90)

X

(cid:90)

X

q(x)

log

2q(x)
p(x) + q(x)

+ log

p(x) log

2q(x)
p(x) + q(x)

2q(x)
p(x) + q(x)
2q(x)
p(x) + q(x)
(cid:19)

+

p(x)
q(z)

log

(cid:19)

dx

p(x)
q(z)

p(x)
q(z)

dx

+ q(x) log

+ p(x) log

(cid:18)

= KL

Q,

(cid:19)

P + Q
2

(cid:18)

+ KL

P,

P + Q
2

.

E Additional experimental results

At each iteration of the boosting approach, diﬀerent reweighting heuristics are possible. This section contains
more complete results about the following three heuristics:

– Constant β, and using the proposed reweighting scheme given β. See Table 3.

– Reweighting similar to “Cascade GAN” from [14], i.e. keep the top x% of examples, based on the

discriminator corresponding to the previous generator. See Table 4.

– Keep the top x% of examples, based on the discriminator corresponding to the mixture of all previous

generators. See Table 5.

Note that when properly tuned, each reweighting scheme outperforms the baselines, and have similar
performances when used with few iterations. However, they require an additional parameter to tune, and
are worse than the simple β = 1/t heuristic proposed above.

27

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

0.97 (0.9; 1.0)

0.88 (0.4; 1.0)

0.63 (0.5; 1.0)

0.72 (0.5; 0.8)

0.58 (0.4; 0.8)

0.59 (0.2; 0.7)

Best of T (T=3)

0.99 (1.0; 1.0)

0.96 (0.9; 1.0)

0.91 (0.7; 1.0)

0.80 (0.7; 0.9)

0.84 (0.7; 0.9)

0.70 (0.6; 0.8)

Best of T (T=10)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

0.98 (0.8; 1.0)

0.80 (0.8; 0.9)

0.87 (0.8; 0.9)

0.71 (0.7; 0.8)

Ensemble (T=3)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.93 (0.8; 1.0)

0.78 (0.6; 1.0)

0.85 (0.6; 1.0)

0.80 (0.6; 1.0)

Ensemble (T=10)

1.00 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

0.91 (0.8; 1.0)

0.88 (0.8; 1.0)

0.89 (0.7; 1.0)

Boosted (T=3)

0.99 (1.0; 1.0)

0.99 (0.9; 1.0)

0.98 (0.9; 1.0)

0.91 (0.8; 1.0)

0.91 (0.8; 1.0)

0.86 (0.7; 1.0)

Boosted (T=10)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

Best of T (T=3)

Best of T (T=10)

Ensemble (T=3)

Ensemble (T=10)

Boosted (T=3)

Boosted (T=10)

−4.49
(−5.4; −4.4)
−4.39
(−4.6; −4.3)
−4.34
(−4.4; −4.3)
−4.46
(−4.8; −4.4)
−4.52
(−4.7; −4.4)
−4.50
(−4.8; −4.4)
−4.55
(−4.6; −4.4)

−6.02
(−86.8; −5.3)
−5.40
(−24.3; −5.2)
−5.24
(−5.4; −5.2)
−5.59
(−6.6; −5.2)
−5.49
(−6.6; −5.2)
−5.32
(−5.8; −5.2)
−5.30
(−5.5; −5.2)

−16.03
(−59.6; −5.5)
−5.57
(−23.5; −5.4)
−5.45
(−5.6; −5.3)
−4.78
(−5.5; −4.6)
−4.98
(−6.5; −4.6)
−4.80
(−5.8; −4.6)
−5.07
(−5.6; −4.7)

−23.65
(−118.8; −5.7)
−9.91
(−35.8; −5.1)
−5.49
(−9.4; −5.0)
−14.71
(−51.9; −5.4)
−5.44
(−6.0; −5.2)
−5.39
(−19.3; −5.1)
−5.25
(−5.5; −4.6)

−126.87
(−250.4; −12.8)
−36.94
(−90.0; −9.7)
−9.72
(−17.3; −6.5)
−6.70
(−28.7; −5.5)
−5.82
(−6.4; −5.5)
−5.56
(−12.4; −5.2)
−5.03
(−5.5; −4.8)

−55.51
(−185.2; −11.2)
−19.12
(−59.2; −9.7)
−9.12
(−16.8; −6.6)
−8.59
(−25.4; −6.1)
−6.08
(−6.3; −5.7)
−8.03
(−28.7; −6.1)
−5.92
(−6.2; −5.6)

Table 2: Performance of the diﬀerent algorithms on varying number of mixtures of Gaussians. The reported
scores are the median and interval deﬁned by the 5% and 95% percentile (in parenthesis) (see Section
4.1.2), over 35 runs for each setting. The top table reports the coverage C, probability mass of Pd covered
by the 5th percentile of Pg deﬁned in Section 4.1.2. The bottom table reports the log likelihood of the
true data under the model Pg. Note that the 95% interval is not the usual conﬁdence interval measuring
the variance of the experiment itself, but rather measures the stability of the diﬀerent algorithms (would
remain even if each experiment was run an inﬁnite number of times). Both the ensemble and the boosting
approaches signiﬁcantly outperform the vanilla GAN even with just three iterations (i.e. just two additional
components). The boosting approach converges faster to the optimal coverage.

28

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

0.98 (0.9; 1.0)

0.86 (0.5; 1.0)

0.66 (0.5; 1.0)

0.61 (0.5; 0.8)

0.55 (0.4; 0.7)

0.58 (0.3; 0.8)

Boosted (T=3)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.98 (0.9; 1.0)

0.93 (0.8; 1.0)

0.97 (0.8; 1.0)

0.87 (0.6; 1.0)

Boosted (T=10)

1.00 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

0.99 (0.9; 1.0)

0.99 (0.8; 1.0)

0.97 (0.8; 1.0)

Beta0.2 (T=3)

0.99 (1.0; 1.0)

0.97 (0.9; 1.0)

0.97 (0.9; 1.0)

0.95 (0.8; 1.0)

0.96 (0.7; 1.0)

0.88 (0.7; 1.0)

Beta0.2 (T=10)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (0.9; 1.0)

1.00 (0.9; 1.0)

Beta0.3 (T=3)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.98 (0.9; 1.0)

0.96 (0.8; 1.0)

0.96 (0.6; 1.0)

0.88 (0.7; 1.0)

Beta0.3 (T=10)

1.00 (1.0; 1.0)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (0.9; 1.0)

0.99 (0.9; 1.0)

Beta0.4 (T=3)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.95 (0.9; 1.0)

0.94 (0.8; 1.0)

0.89 (0.7; 1.0)

0.89 (0.7; 1.0)

Beta0.4 (T=10)

0.99 (1.0; 1.0)

0.99 (0.9; 1.0)

0.96 (0.9; 1.0)

0.97 (0.8; 1.0)

0.99 (0.8; 1.0)

0.90 (0.8; 1.0)

Beta0.5 (T=3)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.97 (0.8; 1.0)

0.82 (0.8; 1.0)

0.86 (0.7; 1.0)

0.81 (0.6; 1.0)

Beta0.5 (T=10)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.97 (0.9; 1.0)

0.84 (0.8; 1.0)

0.87 (0.7; 1.0)

0.91 (0.8; 1.0)

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

Boosted (T=3)

Boosted (T=10)

Beta0.2 (T=3)

Beta0.2 (T=10)

Beta0.3 (T=3)

Beta0.3 (T=10)

Beta0.4 (T=3)

Beta0.4 (T=10)

Beta0.5 (T=3)

Beta0.5 (T=10)

−4.50
(−5.0; −4.4)
−4.56
(−4.9; −4.4)
−4.56
(−4.7; −4.5)
−4.52
(−4.8; −4.4)
−4.58
(−4.8; −4.5)
−4.60
(−4.9; −4.4)
−4.57
(−4.8; −4.4)
−4.62
(−4.9; −4.4)
−4.49
(−4.7; −4.4)
−4.60
(−4.9; −4.4)
−4.62
(−4.8; −4.4)

−5.65
(−72.7; −5.1)
−5.55
(−5.9; −5.2)
−5.46
(−5.6; −5.3)
−5.31
(−5.6; −5.1)
−5.30
(−5.5; −5.2)
−5.34
(−5.7; −5.2)
−5.37
(−5.5; −5.2)
−5.36
(−5.6; −5.1)
−5.40
(−5.7; −5.3)
−5.40
(−5.7; −5.3)
−5.43
(−5.7; −5.2)

−19.63
(−62.1; −5.6)
−5.01
(−6.7; −4.7)
−5.08
(−5.8; −4.7)
−4.85
(−6.3; −4.6)
−4.94
(−6.6; −4.6)
−5.41
(−5.7; −5.1)
−5.27
(−5.6; −5.0)
−4.74
(−5.3; −4.6)
−5.08
(−6.9; −4.7)
−4.77
(−5.4; −4.6)
−5.12
(−6.6; −4.7)

−28.16
(−293.1; −16.3)
−5.49
(−18.7; −4.9)
−5.04
(−5.5; −4.6)
−5.33
(−14.4; −4.8)
−5.23
(−5.5; −4.7)
−5.33
(−12.9; −4.9)
−5.26
(−5.6; −5.0)
−5.34
(−26.2; −4.9)
−5.49
(−5.9; −5.2)
−5.63
(−24.5; −5.2)
−5.48
(−8.4; −5.1)

−56.94
(−248.1; −14.3)
−5.60
(−14.5; −5.0)
−5.51
(−5.9; −5.1)
−5.68
(−26.2; −5.2)
−5.60
(−6.0; −5.3)
−5.68
(−11.0; −5.4)
−5.71
(−6.0; −5.3)
−5.77
(−37.3; −5.1)
−5.43
(−6.0; −5.1)
−6.05
(−17.9; −5.5)
−5.85
(−6.1; −5.3)

−71.11
(−184.8; −12.5)
−6.86
(−47.3; −5.6)
−5.51
(−6.0; −5.2)
−6.13
(−32.7; −5.7)
−5.98
(−6.1; −5.7)
−6.41
(−29.2; −5.6)
−5.82
(−6.1; −5.4)
−12.37
(−75.9; −5.9)
−5.68
(−6.2; −5.2)
−8.29
(−23.1; −6.1)
−6.31
(−7.7; −6.0)

Table 3: Performance with constant β, exploring a range of possible values. The reported scores are the
median and interval deﬁned by the 5% and 95% percentile (in parenthesis) (see Section 4.1.2), over 35 runs
for each setting. The top table reports the coverage C, probability mass of Pd covered by the 5th percentile
of Pg deﬁned in Section 4.1.2. The bottom table reports the log likelihood of the true data under Pg.

29

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

0.96 (0.9; 1.0)

0.90 (0.5; 1.0)

0.65 (0.5; 1.0)

0.61 (0.5; 0.8)

0.69 (0.3; 0.8)

0.59 (0.3; 0.7)

Boosted (T=3)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.98 (0.9; 1.0)

0.93 (0.8; 1.0)

0.97 (0.8; 1.0)

0.87 (0.6; 1.0)

Boosted (T=10)

1.00 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

0.99 (0.9; 1.0)

0.99 (0.8; 1.0)

0.97 (0.8; 1.0)

TopKLast0.1 (T=3)

0.98 (0.9; 1.0)

0.93 (0.8; 1.0)

0.89 (0.6; 1.0)

0.72 (0.5; 1.0)

0.68 (0.5; 0.9)

0.51 (0.4; 0.7)

TopKLast0.1 (T=10)

0.99 (0.9; 1.0)

0.97 (0.8; 1.0)

0.90 (0.7; 1.0)

0.67 (0.4; 0.9)

0.61 (0.5; 0.8)

0.58 (0.4; 0.8)

TopKLast0.3 (T=3)

0.99 (0.9; 1.0)

0.97 (0.9; 1.0)

0.93 (0.7; 1.0)

0.81 (0.7; 1.0)

0.84 (0.7; 1.0)

0.78 (0.5; 1.0)

TopKLast0.3 (T=10)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.95 (0.7; 1.0)

0.94 (0.7; 1.0)

0.89 (0.7; 1.0)

0.88 (0.7; 1.0)

TopKLast0.5 (T=3)

0.98 (0.9; 1.0)

0.98 (0.9; 1.0)

0.95 (0.9; 1.0)

0.95 (0.8; 1.0)

0.86 (0.7; 1.0)

0.86 (0.6; 0.9)

TopKLast0.5 (T=10)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.98 (1.0; 1.0)

0.99 (0.8; 1.0)

0.99 (0.8; 1.0)

1.00 (0.8; 1.0)

TopKLast0.7 (T=3)

0.98 (1.0; 1.0)

0.98 (0.9; 1.0)

0.94 (0.9; 1.0)

0.83 (0.7; 1.0)

0.87 (0.6; 1.0)

0.82 (0.7; 1.0)

TopKLast0.7 (T=10)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

0.98 (0.8; 1.0)

0.99 (0.9; 1.0)

0.95 (0.8; 1.0)

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

Boosted (T=3)

Boosted (T=10)

TopKLast0.1 (T=3)

TopKLast0.1 (T=10)

TopKLast0.3 (T=3)

TopKLast0.3 (T=10)

TopKLast0.5 (T=3)

TopKLast0.5 (T=10)

TopKLast0.7 (T=3)

TopKLast0.7 (T=10)

−4.94
(−5.5; −4.4)
−4.56
(−4.9; −4.4)
−4.56
(−4.7; −4.5)
−4.98
(−5.2; −4.7)
−4.98
(−5.3; −4.7)
−4.73
(−5.1; −4.5)
−4.62
(−4.8; −4.5)
−4.59
(−4.9; −4.4)
−4.59
(−4.8; −4.5)
−4.56
(−4.7; −4.4)
−4.52
(−4.7; −4.5)

−6.18
(−51.7; −5.6)
−5.55
(−5.9; −5.2)
−5.46
(−5.6; −5.3)
−5.64
(−6.1; −5.4)
−5.57
(−5.9; −5.3)
−5.48
(−6.0; −5.2)
−5.41
(−5.7; −5.2)
−5.29
(−5.7; −5.2)
−5.35
(−5.6; −5.2)
−5.37
(−5.5; −5.2)
−5.29
(−5.4; −5.2)

−31.85
(−100.3; −5.8)
−5.01
(−6.7; −4.7)
−5.08
(−5.8; −4.7)
−5.70
(−6.3; −5.2)
−5.37
(−6.0; −5.0)
−5.22
(−5.7; −4.8)
−4.90
(−5.2; −4.7)
−5.41
(−5.9; −4.9)
−5.12
(−5.5; −4.9)
−5.05
(−11.1; −4.7)
−5.05
(−6.6; −4.7)

−47.73
(−155.1; −14.2)
−5.49
(−18.7; −4.9)
−5.04
(−5.5; −4.6)
−5.39
(−38.4; −5.0)
−5.57
(−45.1; −4.7)
−5.42
(−21.6; −5.0)
−5.24
(−5.8; −4.9)
−5.48
(−18.5; −5.0)
−5.35
(−5.6; −4.8)
−5.63
(−43.1; −5.0)
−5.38
(−5.9; −5.1)

−107.36
(−390.8; −14.8)
−5.60
(−14.5; −5.0)
−5.51
(−5.9; −5.1)
−7.00
(−66.6; −5.4)
−7.34
(−16.1; −5.3)
−5.76
(−13.6; −5.1)
−5.71
(−6.2; −5.1)
−5.82
(−15.6; −5.2)
−5.34
(−5.8; −4.9)
−5.99
(−24.8; −5.4)
−5.77
(−6.3; −5.3)

−59.19
(−264.3; −18.8)
−6.86
(−47.3; −5.6)
−5.51
(−6.0; −5.2)
−12.70
(−44.2; −6.7)
−8.86
(−27.6; −5.5)
−7.26
(−36.2; −5.5)
−5.75
(−7.4; −5.1)
−6.78
(−18.7; −6.0)
−6.00
(−6.3; −5.6)
−7.76
(−25.2; −5.9)
−6.10
(−6.4; −6.0)

Table 4: Reweighting similar to “Cascade GAN” from [14], i.e. keep the top r fraction of examples, based on
the discriminator corresponding to the previous generator. The mixture weights are all equal (i.e. β = 1/t).
The reported scores are the median and interval deﬁned by the 5% and 95% percentile (in parenthesis) (see
Section 4.1.2), over 35 runs for each setting. The top table reports the coverage C, probability mass of Pd
covered by the 5th percentile of Pg deﬁned in Section 4.1.2. The bottom table reports the log likelihood of
the true data under Pg.

30

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

0.97 (0.9; 1.0)

0.77 (0.5; 1.0)

0.65 (0.5; 0.9)

0.70 (0.5; 0.8)

0.61 (0.5; 0.8)

0.58 (0.3; 0.8)

Boosted (T=3)

0.99 (1.0; 1.0)

0.99 (0.9; 1.0)

0.97 (0.9; 1.0)

0.95 (0.8; 1.0)

0.91 (0.8; 1.0)

0.89 (0.8; 1.0)

Boosted (T=10)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

TopK0.1 (T=3)

0.98 (0.9; 1.0)

0.98 (0.8; 1.0)

0.91 (0.7; 1.0)

0.84 (0.7; 1.0)

0.80 (0.5; 0.9)

0.60 (0.4; 0.7)

TopK0.1 (T=10)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

0.98 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

0.96 (0.8; 1.0)

TopK0.3 (T=3)

0.98 (0.9; 1.0)

0.98 (0.9; 1.0)

0.95 (0.9; 1.0)

0.95 (0.8; 1.0)

0.84 (0.6; 1.0)

0.79 (0.5; 1.0)

TopK0.3 (T=10)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

0.98 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

TopK0.5 (T=3)

0.99 (0.9; 1.0)

0.99 (1.0; 1.0)

0.96 (0.9; 1.0)

0.98 (0.8; 1.0)

0.88 (0.7; 1.0)

0.88 (0.6; 1.0)

TopK0.5 (T=10)

1.00 (1.0; 1.0)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

TopK0.7 (T=3)

0.98 (1.0; 1.0)

0.98 (0.9; 1.0)

0.94 (0.8; 1.0)

0.84 (0.8; 1.0)

0.86 (0.7; 1.0)

0.81 (0.7; 1.0)

TopK0.7 (T=10)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (0.8; 1.0)

1.00 (0.9; 1.0)

1.00 (0.9; 1.0)

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

Boosted (T=3)

Boosted (T=10)

TopK0.1 (T=3)

TopK0.1 (T=10)

TopK0.3 (T=3)

TopK0.3 (T=10)

TopK0.5 (T=3)

TopK0.5 (T=10)

TopK0.7 (T=3)

TopK0.7 (T=10)

−4.61
(−5.5; −4.4)
−4.59
(−4.9; −4.4)
−4.61
(−4.7; −4.5)
−4.93
(−5.3; −4.7)
−4.60
(−4.8; −4.5)
−4.65
(−4.9; −4.4)
−4.56
(−4.8; −4.5)
−4.60
(−4.8; −4.5)
−4.59
(−4.7; −4.5)
−4.60
(−5.0; −4.4)
−4.59
(−4.7; −4.5)

−5.92
(−94.2; −5.2)
−5.32
(−5.7; −5.2)
−5.30
(−5.4; −5.2)
−5.85
(−6.2; −5.4)
−5.47
(−5.7; −5.3)
−5.40
(−5.9; −5.3)
−5.32
(−5.5; −5.2)
−5.34
(−5.6; −5.2)
−5.31
(−5.4; −5.2)
−5.44
(−5.6; −5.2)
−5.34
(−5.5; −5.2)

−12.40
(−53.1; −5.3)
−5.60
(−5.8; −5.5)
−5.48
(−5.6; −5.2)
−5.38
(−5.7; −5.0)
−4.81
(−5.1; −4.7)
−4.98
(−6.2; −4.7)
−5.07
(−5.9; −4.7)
−5.34
(−5.7; −5.0)
−5.13
(−5.5; −4.9)
−5.62
(−6.0; −5.4)
−5.51
(−5.6; −5.4)

−59.62
(−154.6; −9.8)
−5.40
(−24.2; −4.5)
−4.84
(−5.1; −4.3)
−5.34
(−5.8; −4.8)
−4.90
(−5.3; −4.2)
−5.25
(−11.4; −4.7)
−5.08
(−5.4; −4.5)
−5.42
(−19.0; −5.0)
−5.35
(−5.7; −4.8)
−5.49
(−22.2; −5.0)
−5.35
(−5.8; −5.0)

−66.95
(−191.5; −9.7)
−5.71
(−14.0; −5.1)
−5.25
(−5.9; −4.8)
−5.79
(−32.1; −5.2)
−4.85
(−5.6; −4.1)
−5.96
(−28.0; −5.5)
−5.16
(−5.9; −4.9)
−5.59
(−34.7; −4.9)
−5.33
(−5.8; −4.8)
−5.64
(−27.7; −5.3)
−5.32
(−6.0; −5.1)

−63.49
(−431.6; −14.5)
−6.96
(−17.1; −5.9)
−5.95
(−6.1; −5.5)
−7.09
(−20.7; −5.9)
−4.57
(−5.3; −4.2)
−7.34
(−25.4; −5.9)
−5.82
(−6.2; −5.3)
−6.15
(−14.8; −5.6)
−5.72
(−6.2; −5.3)
−7.17
(−22.5; −6.0)
−6.11
(−6.4; −5.9)

Table 5: Reweighting using the top r fraction of examples, based on the discriminator corresponding to the
mixture of all previous generators. The mixture weights are all equal (i.e. β = 1/t). The reported scores
are the median and interval deﬁned by the 5% and 95% percentile (in parenthesis) (see Section 4.1.2), over
35 runs for each setting. The top table reports the coverage C, probability mass of Pd covered by the 5th
percentile of Pg deﬁned in Section 4.1.2. The bottom table reports the log likelihood of the true data under
Pg.

31

7
1
0
2
 
y
a
M
 
4
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
6
8
3
2
0
.
1
0
7
1
:
v
i
X
r
a

AdaGAN: Boosting Generative Models

Ilya Tolstikhin1, Sylvain Gelly2, Olivier Bousquet2, Carl-Johann Simon-Gabriel1, and
Bernhard Sch¨olkopf1

1Max Planck Institute for Intelligent Systems
2Google Brain

Abstract

Generative Adversarial Networks (GAN) [1] are an eﬀective method for training generative models of
complex data such as natural images. However, they are notoriously hard to train and can suﬀer from
the problem of missing modes where the model is not able to produce examples in certain regions of the
space. We propose an iterative procedure, called AdaGAN, where at every step we add a new component
into a mixture model by running a GAN algorithm on a reweighted sample. This is inspired by boosting
algorithms, where many potentially weak individual predictors are greedily aggregated to form a strong
composite predictor. We prove that such an incremental procedure leads to convergence to the true
distribution in a ﬁnite number of steps if each step is optimal, and convergence at an exponential rate
otherwise. We also illustrate experimentally that this procedure addresses the problem of missing modes.

1 Introduction

Imagine we have a large corpus, containing unlabeled pictures of animals, and our task is to build a generative
probabilistic model of the data. We run a recently proposed algorithm and end up with a model which
produces impressive pictures of cats and dogs, but not a single giraﬀe. A natural way to ﬁx this would be
to manually remove all cats and dogs from the training set and run the algorithm on the updated corpus.
The algorithm would then have no choice but to produce new animals and, by iterating this process until
there’s only giraﬀes left in the training set, we would arrive at a model generating giraﬀes (assuming suﬃcient
sample size). At the end, we aggregate the models obtained by building a mixture model. Unfortunately, the
described meta-algorithm requires manual work for removing certain pictures from the unlabeled training
set at every iteration.

Let us turn this into an automatic approach, and rather than including or excluding a picture, put
continuous weights on them. To this end, we train a binary classiﬁer to separate “true” pictures of the
original corpus from the set of “synthetic” pictures generated by the mixture of all the models trained so
far. We would expect the classiﬁer to make conﬁdent predictions for the true pictures of animals missed
by the model (giraﬀes), because there are no synthetic pictures nearby to be confused with them. By
a similar argument, the classiﬁer should make less conﬁdent predictions for the true pictures containing
animals already generated by one of the trained models (cats and dogs). For each picture in the corpus,
we can thus use the classiﬁer’s conﬁdence to compute a weight which we use for that picture in the next
iteration, to be performed on the re-weighted dataset.

The present work provides a principled way to perform this re-weighting, with theoretical guarantees

showing that the resulting mixture models indeed approach the true data distribution.1

Before discussing how to build the mixture, let us consider the question of building a single generative
model. A recent trend in modelling high dimensional data such as natural images is to use neural net-
works [2, 1]. One popular approach are Generative Adversarial Networks (GAN) [1], where the generator

1Note that the term “mixture” should not be interpreted to imply that each component models only one mode: the models

to be combined into a mixture can themselves cover multiple modes.

1

is trained adversarially against a classiﬁer, which tries to diﬀerentiate the true from the generated data.
While the original GAN algorithm often produces realistically looking data, several issues were reported in
the literature, among which the missing modes problem, where the generator converges to only one or a few
modes of the data distribution, thus not providing enough variability in the generated data. This seems
to match the situation described earlier, which is why we will most often illustrate our algorithm with a
GAN as the underlying base generator. We call it AdaGAN, for Adaptive GAN, but we could actually use
any other generator: a Gaussian mixture model, a VAE [2], a WGAN [3], or even an unrolled [4] or mode-
regularized GAN [5], which were both already speciﬁcally developed to tackle the missing mode problem.
Thus, we do not aim at improving the original GAN or any other generative algorithm. We rather propose
and analyse a meta-algorithm that can be used on top of any of them. This meta-algorithm is similar in
spirit to AdaBoost [6] in the sense that each iteration corresponds to learning a “weak” generative model
(e.g., GAN) with respect to a re-weighted data distribution. The weights change over time to focus on the
“hard” examples, i.e. those that the mixture has not been able to properly generate so far.

1.1 Boosting via Additive Mixtures

Motivated by the problem of missing modes, in this work we propose to use multiple generative models
combined into a mixture. These generative models are trained iteratively by adding, at each step, another
model to the mixture that should hopefully cover the areas of the space not covered by the previous mixture
components.2 We show analytically that the optimal next mixture component can be obtained by reweighting
the true data, and thus propose to use the reweighted data distribution as the target for the optimization
of the next mixture components. This leads us naturally to a meta-algorithm, which is similar in spirit to
AdaBoost in the sense that each iteration corresponds to learning a “weak” generative model (e.g., GAN)
with respect to a reweighted data distribution. The latter adapts over time to focus on the “hard” examples,
i.e. those that the mixture has not been able to properly generate thus far.

Before diving into the technical details we provide an informal intuitive discussion of our new meta-
algorithm, which we call AdaGAN (a shorthand for Adaptive GAN, similar to AdaBoost). The pseudocode
is presented in Algorithm 1.

On the ﬁrst step we run the GAN algorithm (or some other generative model) in the usual way and
initialize our generative model with the resulting generator G1. On every t-th step we (a) pick the mixture
weight βt for the next component, (b) update weights Wt of examples from the training set in such a way to
bias the next component towards “hard” ones, not covered by the current mixture of generators Gt−1, (c) run
the GAN algorithm, this time importance sampling mini-batches according to the updated weights Wt,
t , and ﬁnally (d) update our mixture of generators Gt = (1 − βt)Gt−1 + βtGc
resulting in a new generator Gc
t
(notation expressing the mixture of Gt−1 and Gc
t with probabilities 1 − βt and βt). This procedure outputs T
generator functions Gc
T and T corresponding non-negative weights α1, . . . , αT , which sum to one. For
sampling from the resulting model we ﬁrst deﬁne a generator Gc
i , by sampling the index i from a multinomial
distribution with parameters α1, . . . , αT , and then we return Gc
i (Z), where Z ∼ PZ is a standard latent noise
variable used in the GAN literature.

1, . . . , Gc

The eﬀect of the described procedure is illustrated in a toy example in Figure 1. On the left images,
the red dots are the training (true data) points, the blue dots are points sampled from the model mixture
of generators Gt. The background colour gives the density of the distribution corresponding to Gt, non
zero around the generated points, (almost) zero everywhere else. On the right images, the color corresponds
to the weights of training points, following the reweighting scheme proposed in this work. The top row
corresponds to the ﬁrst iteration of AdaGAN, and the bottom row to the second iteration. After the ﬁrst
iteration (the result of the vanilla GAN), we see that only the top left mode is covered, while the three
other modes are not covered at all. The new weights (top right) show that the examples from covered mode
are aggressively downweighted. After the second iteration (bottom left), the combined generator can then
generate two modes.

2Note that the term “mixture” should not be interpreted to imply that each component models only one mode: the models

to be combined into a mixture can themselves cover multiple modes already.

2

Algorithm 1: AdaGAN, a meta-algorithm to construct a “strong” mixture of T individual GANs,
trained sequentially. The mixture weight schedule ChooseMixtureWeight and the training set reweight-
ing schedule UpdateTrainingWeights should be provided by the user. Section 3 gives a complete instance
of this family.

Input: Training sample SN := {X1, . . . , XN }.

Output: Mixture generative model G = GT .

Train vanilla GAN:

W1 = (1/N, . . . , 1/N )

G1 = GAN(SN , Wt)

for t = 2, . . . , T do

#Choose a mixture weight for the next component

βt = ChooseMixtureWeight(t)

#Update weights of training examples

Wt = UpdateTrainingWeights(Gt−1, SN , βt)
#Train t-th “weak” component generator Gc
t
Gc

t = GAN(SN , Wt)

#Update the overall generative model
#Notation below means forming a mixture of Gt−1 and Gc
t .
Gt = (1 − βt)Gt−1 + βtGc
t

end for

3

Although motivated by GANs, we cast our results in the general framework of the minimization of an
f -divergence (cf. [7]) with respect to an additive mixture of distributions. We also note that our approach
may be combined with diﬀerent “weak” generative models, including but not limited to GAN.

Figure 1: A toy illustration of the missing mode problem and the eﬀect of sample reweighting, following the
discussion in Section 1.1. On the left images, the red dots are the training (true data) points, the blue dots
are points sampled from the model mixture of generators Gt. On the right images, the color corresponds
to the weights of training points, following the reweighting scheme proposed in this work. The top row
corresponds to the ﬁrst iteration of AdaGAN, and the bottom row to the second iteration.

1.2 Related Work

Several authors [8, 9, 10] have proposed to use boosting techniques in the context of density estimation by
incrementally adding components in the log domain. In particular, the work of Grover and Ermon [10], done
in parallel to and independent of ours, is applying this idea to GANs. A major downside of these approaches
is that the resulting mixture is a product of components and sampling from such a model is nontrivial (at
least when applied to GANs where the model density is not expressed analytically) and requires to use
techniques such as Annealed Importance Sampling [11] for the normalization.

Rosset and Segal [12] proposed to use an additive mixture model in the case where the log likelihood
can be computed. They derived the update rule via computing the steepest descent direction when adding
a component with inﬁnitesimal weight. This leads to an update rule which is degenerate if the generative
model can produce arbitrarily concentrated distributions (indeed the optimal component is just a Dirac
distribution) which is thus not suitable for the GAN setting. Moreover, their results do not apply once the

4

weight β becomes non-inﬁnitesimal. In contrast, for any ﬁxed weight of the new component our approach
gives the overall optimal update (rather than just the best direction), and applies to any f -divergence.
Remarkably, in both theories, improvements of the mixture are guaranteed only if the new “weak” learner
is still good enough (see Conditions 14&15)

Similarly, Barron and Li [13] studied the construction of mixtures minimizing the Kullback divergence
and proposed a greedy procedure for doing so. They also proved that under certain conditions, ﬁnite mixtures
can approximate arbitrary mixtures at a rate 1/k where k is the number of components in the mixture when
the weight of each newly added component is 1/k. These results are speciﬁc to the Kullback divergence but
are consistent with our more general results.

Wang et al. [14] propose an additive procedure similar to ours but with a diﬀerent reweighting scheme,
which is not motivated by a theoretical analysis of optimality conditions. On every new iteration the authors
propose to run GAN on the top k training examples with maximum value of the discriminator from the last
iteration. Empirical results of Section 4 show that this heuristic often fails to address the missing modes
problem.

Finally, many papers investigate completely diﬀerent approaches for addressing the same issue by directly
modifying the training objective of an individual GAN. For instance, Che et al. [5] add an autoencoding cost
to the training objective of GAN, while Metz et al. [4] allow the generator to “look few steps ahead” when
making a gradient step.

The paper is organized as follows. In Section 2 we present our main theoretical results regarding opti-
mization of mixture models under general f -divergences. In particular we show that it is possible to build
an optimal mixture in an incremental fashion, where each additional component is obtained by applying a
GAN-style procedure with a reweighted distribution. In Section 2.5 we show that if the GAN optimization
at each step is perfect, the process converges to the true data distribution at exponential rate (or even
in a ﬁnite number of steps, for which we provide a necessary and suﬃcient condition). Then we show in
Section 2.6 that imperfect GAN solutions still lead to the exponential rate of convergence under certain
“weak learnability” conditions. These results naturally lead us to a new boosting-style iterative procedure
for constructing generative models, which is combined with GAN in Section 3, resulting in a new algorithm
called AdaGAN. Finally, we report initial empirical results in Section 4, where we compare AdaGAN with
several benchmarks, including original GAN, uniform mixture of multiple independently trained GANs, and
iterative procedure of Wang et al. [14].

2 Minimizing f -divergence with Additive Mixtures

In this section we derive a general result on the minimization of f -divergences over mixture models.

2.1 Preliminaries and notations

In this work we will write Pd and Pmodel to denote a real data distribution and our approximate model
distribution, respectively, both deﬁned over the data space X .

Generative Density Estimation In the generative approach to density estimation, instead of building a
probabilistic model of the data directly, one builds a function G : Z → X that transforms a ﬁxed probability
distribution PZ (often called the noise distribution) over a latent space Z into a distribution over X . Hence
Pmodel is the pushforward of PZ, i.e. Pmodel(A) = PZ(G−1(A)). Because of this deﬁnition, it is generally
impossible to compute the density dPmodel(x), hence it is not possible to compute the log-likelihood of the
training data under the model. However, if PZ is a distribution from which one can sample, it is easy to also
sample from Pmodel (simply sampling from PZ and applying G to each example gives a sample from Pmodel).
So the problem of generative density estimation becomes a problem of ﬁnding a function G such that
Pmodel looks like Pd in the sense that samples from Pmodel and from Pd look similar. Another way to state
this problem is to say that we are given a measure of similarity between distributions D(Pmodel(cid:107)Pd) which

5

can be estimated from samples of those distributions, and thus approximately minimized over a class G of
functions.

f -Divergences
of the data we will use an f -divergence deﬁned in the following way:

In order to measure the agreement between the model distribution and the true distribution

Df (Q(cid:107)P ) :=

(cid:90)

(cid:18) dQ
dP

f

(cid:19)

(x)

dP (x)

(1)

(2)

(3)

for any pair of distributions P, Q with densities dP , dQ with respect to some dominating reference measure µ.
In this work we assume that the function f is convex, deﬁned on (0, ∞), and satisﬁes f (1) = 0. The deﬁnition
of Df holds for both continuous and discrete probability measures and does not depend on speciﬁc choice
of µ.3 It is easy to verify that Df ≥ 0 and it is equal to 0 when P = Q. Note that Df is not symmetric,
but Df (P (cid:107)Q) = Df ◦ (Q(cid:107)P ) for f ◦(x) := xf (1/x) and any P and Q. The f -divergence is symmetric when
f (x) = f ◦(x) for all x ∈ (0, ∞), as in this case Df (P, Q) = Df (Q, P ).

We also note that the divergences corresponding to f (x) and f (x) + C · (x − 1) are identical for any
constant C. In some cases, it is thus convenient to work with f0(x) := f (x) − (x − 1)f (cid:48)(1), (where f (cid:48)(1) is
any subderivative of f at 1) as Df (Q(cid:107)P ) = Df0(Q(cid:107)P ) for all Q and P , while f0 is nonnegative, nonincreasing
on (0, 1], and nondecreasing on (1, ∞). In the remainder, we will denote by F the set of functions that are
suitable for f -divergences, i.e. the set of functions of the form f0 for any convex f with f (1) = 0.

Classical examples of f -divergences include the Kullback-Leibler divergence (obtained for f (x) = − log x,
f0(x) = − log x + x − 1), the reverse Kullback-Leibler divergence (obtained for f (x) = x log x, f0(x) =
x log x − x + 1), the Total Variation distance (f (x) = f0(x) = |x − 1|), and the Jensen-Shannon divergence
(f (x) = f0(x) = −(x + 1) log x+1
2 + x log x). More details can be found in Appendix D. Other examples can
be found in [7]. For further details on f -divergences we refer to Section 1.3 of [15] and [16].

GAN and f -divergences We now explain the connection between the GAN algorithm and f -divergences.
The original GAN algorithm [1] consists in optimizing the following criterion:

min
G

max
D

EPd [log D(X)] + EPZ [log(1 − D(G(Z)))] ,

where D and G are two functions represented by neural networks, and this optimization is actually performed
on a pair of samples (one being the training sample, the other one being created from the chosen distribu-
tion PZ), which corresponds to approximating the above criterion by using the empirical distributions. For
a ﬁxed G, it has been shown in [1] that the optimal D for (2) is given by D∗(x) =
dPd(x)+dPg(x) and plugging
this optimal value into (2) gives the following:

dPd(x)

min
G

− log(4) + 2JS(Pd (cid:107) Pg) ,

where JS is the Jensen-Shannon divergence. Of course, the actual GAN algorithm uses an approximation
to D∗ which is computed by training a neural network on a sample, which means that the GAN algorithm
can be considered to minimize an approximation of (3)4. This point of view can be generalized by plugging
another f -divergence into (3), and it turns out that other f -divergences can be written as the solution to
a maximization of a criterion similar to (2). Indeed, as demonstrated in [7], any f -divergence between Pd
and Pg can be seen as the optimal value of a quantity of the form EPd [f1(D(X))] + EPg [f2(D(G(Z)))] for
appropriate f1 and f2, and thus can be optimized by the same adversarial training technique.

There is thus a strong connection between adversarial training of generative models and minimization of

f -divergences, and this is why we cast the results of this section in the context of general f -divergences.

3The integral in (1) is well deﬁned (but may take inﬁnite values) even if P (dQ = 0) > 0 or Q(dP = 0) > 0. In this case the
integral is understood as Df (Q(cid:107)P ) = (cid:82) f (dQ/dP )1[dP (x)>0,dQ(x)>0]dP (x) + f (0)P (dQ = 0) + f ◦(0)Q(dP = 0), where both
f (0) and f ◦(0) may take value ∞ [15]. This is especially important in case of GAN, where it is impossible to constrain Pmodel
to be absolutely continuous with respect to Pd or vice versa.

4Actually the criterion that is minimized is an empirical version of a lower bound of the Jensen-Shannon divergence.

6

Hilbertian Metrics As demonstrated in [17, 18], several commonly used symmetric f -divergences are
Hilbertian metrics, which in particular means that their square root satisﬁes the triangle inequality. This
is true for the Jensen-Shannon divergence5 as well as for the Hellinger distance and the Total Variation
among others. We will denote by FH the set of f functions such that Df is a Hilbertian metric. For those
divergences, we have Df (P (cid:107)Q) ≤ ((cid:112)Df (P (cid:107)R) + (cid:112)Df (R(cid:107)Q))2.

Generative Mixture Models
a mixture model of the following form:

In order to model complex data distributions, it can be convenient to use

P T

model :=

αiPi,

T
(cid:88)

i=1

where αi ≥ 0, (cid:80)
i αi = 1, and each of the T components is a generative density model. This is very natural
in the generative context, since sampling from a mixture corresponds to a two-step sampling, where one ﬁrst
picks the mixture component (according to the multinomial distribution whose parameters are the αi) and
then samples from it. Also, this allows to construct complex models from simpler ones.

2.2 Incremental Mixture Building

As discussed earlier, in the context of generative modeling, we are given a measure of similarity between
distributions. We will restrict ourselves to the case of f -divergences.
Indeed, for any f -divergence, it
is possible (as explained for example in [7]) to estimate Df (Q (cid:107) P ) from two samples (one from Q, one
from P ) by training a “discriminator” function, i.e. by solving an optimization problem (which is a binary
classiﬁcation problem in the case where the divergence is symmetric6).
It turns out that the empirical
estimate ˆD of Df (Q (cid:107) P ) thus obtained provides a criterion for optimizing Q itself. Indeed, ˆD is a function
of Y1, . . . , Yn ∼ Q and X1, . . . , Xn ∼ P , where Yi = G(Zi) for some mapping function G. Hence it is possible
to optimize ˆD with respect to G (and in particular compute gradients with respect to the parameters of G
if G comes from a smoothly parametrized model such as a neural network).

In this work we thus assume that, given an i.i.d. sample from any unknown distribution P we can construct

a simple model Q ∈ G which approximately minimizes

min
Q∈G

Df (Q (cid:107) P ).

Instead of just modelling the data with a single distribution, we now want to model it with a mixture
of the form (4) where each Pi is obtained by a training procedure of the form (5) with (possibly) diﬀerent
target distributions P for each i.

A natural way to build a mixture is to do it incrementally: we train the ﬁrst model P1 to minimize
Df (P1 (cid:107) Pd) and set the corresponding weight to α1 = 1, leading to P 1
model = P1. Then after having trained
t components P1, . . . , Pt ∈ G we can form the (t + 1)-st mixture model by adding a new component Q with
weight β as follows:

P t+1

model :=

(1 − β)αiPi + βQ.

t
(cid:88)

i=1

Df ((1 − β)Pg + βQ (cid:107) Pd),

We are going to choose β ∈ [0, 1] and Q ∈ G greedily, while keeping all the other parameters of the generative
model ﬁxed, so as to minimize

where we denoted Pg := P t

model the current generative mixture model before adding the new component.

We do not necessarily need to ﬁnd the optimal Q that minimizes (7) at each step. Indeed, it would be
suﬃcient to ﬁnd some Q which allows to build a slightly better approximation of Pd. This means that a

5which means such a property can be used in the context of the original GAN algorithm.
6One example of such a setting is running GANs, which are known to approximately minimize the Jensen-Shannon divergence.

(4)

(5)

(6)

(7)

7

more modest goal could be to ﬁnd Q such that, for some c < 1,

Df ((1 − β)Pg + βQ (cid:107) Pd) ≤ c · Df (Pg (cid:107) Pd) .

(8)

However, we observe that this greedy approach has a signiﬁcant drawback in practice. Indeed, as we
build up the mixture, we need to make β decrease (as P t
model approximates Pd better and better, one should
make the correction at each step smaller and smaller). Since we are approximating (7) using samples from
both distributions, this means that the sample from the mixture will only contain a fraction β of examples
from Q. So, as t increases, getting meaningful information from a sample so as to tune Q becomes harder
and harder (the information is “diluted”).

To address this issue, we propose to optimize an upper bound on (7) which involves a term of the
form Df (Q (cid:107) Q0) for some distribution Q0, which can be computed as a reweighting of the original data
distribution Pd.

In the following sections we will analyze the properties of (7) (Section 2.4) and derive upper bounds that
provide practical optimization criteria for building the mixture (Section 2.3). We will also show that under
certain assumptions, the minimization of the upper bound will lead to the optimum of the original criterion.
This procedure is reminiscent of the AdaBoost algorithm [6], which combines multiple weak predictors
into one very accurate strong composition. On each step AdaBoost adds one new predictor to the current
composition, which is trained to minimize the binary loss on the reweighted training set. The weights are
constantly updated in order to bias the next weak learner towards “hard” examples, which were incorrectly
classiﬁed during previous stages.

2.3 Upper Bounds

Next lemma provides two upper bounds on the divergence of the mixture in terms of the divergence of the
additive component Q with respect to some reference distribution R.

Lemma 1 Let f ∈ F. Given two distributions Pd, Pg and some β ∈ [0, 1], for any distribution Q and any
distribution R such that βdR ≤ dPd, we have

Df ((1 − β)Pg + βQ (cid:107) Pd) ≤ βD(Q (cid:107) R) + (1 − β)Df

Pg (cid:107)

(9)

(cid:18)

Pd − βR
1 − β

(cid:19)

.

If furthermore f ∈ FH , then, for any R, we have

Df ((1 − β)Pg + βQ (cid:107) Pd) ≤

βDf (Q (cid:107) R) +

Df ((1 − β)Pg + βR (cid:107) Pd)

.

(10)

(cid:18)(cid:113)

(cid:113)

(cid:19)2

Proof For the ﬁrst inequality, we use the fact that Df is jointly convex. We write Pd = (1 − β) Pd−βR
which is a convex combination of two distributions when the assumptions are satisﬁed.

1−β + βR

The second inequality follows from using the triangle inequality for the square root of the Hilbertian

metric Df and using convexity of Df in its ﬁrst argument.

We can exploit the upper bounds of Lemma 1 by introducing some well-chosen distribution R and
minimizing with respect to Q. A natural choice for R is a distribution that minimizes the last term of the
upper bound (which does not depend on Q).

2.4 Optimal Upper Bounds

In this section we provide general theorems about the optimization of the right-most terms in the upper
bounds of Lemma 1.

For the upper bound (10), this means we need to ﬁnd R minimizing Df ((1 − β)Pg + βR (cid:107) Pd). The

solution for this problem is given in the following theorem.

8

Theorem 1 For any f -divergence Df , with f ∈ F and f diﬀerentiable, any ﬁxed distributions Pd, Pg, and
any β ∈ (0, 1], the solution to the following minimization problem:

where P is a class of all probability distributions, has the density

min
Q∈P

Df ((1 − β)Pg + βQ (cid:107) Pd),

dQ∗

β(x) =

(λ∗dPd(x) − (1 − β)dPg(x))+

1
β

for some unique λ∗ satisfying (cid:82) dQ∗
Also, λ∗ = 1 if and only if Pd((1 − β)dPg > dPd) = 0, which is equivalent to βdQ∗

β = 1. Furthermore, β ≤ λ∗ ≤ min(1, β/δ), where δ := Pd(dPg = 0).

β = dPd − (1 − β)dPg.

Proof See Appendix C.1.

(cid:1)/β,
Remark 1 The form of Q∗
which would make arguments of the f -divergence identical? Unfortunately, it may be the case that dPd(X) <
(1 − β)dPg(X) for some of X ∈ X , leading to the negative values of dQ.

β may look unexpected at ﬁrst glance: why not setting dQ := (cid:0)dPd −(1−β)dPg

For the upper bound (9), we need to minimize Df

. The solution is given in the next

(cid:16)

Pg (cid:107) Pd−βR
1−β

(cid:17)

theorem.

Theorem 2 Given two distributions Pd, Pg and some β ∈ (0, 1], assume

Let f ∈ F. The solution to the minimization problem

Pd (dPg = 0) < β.

min
Q:βdQ≤dPd

Df

Pg (cid:107)

(cid:18)

(cid:19)

Pd − βQ
1 − β

is given by the distribution

dQ†

β(x) =

(cid:0)dPd(x) − λ†(1 − β)dPg(x)(cid:1)

+

1
β

for a unique λ† ≥ 1 satisfying (cid:82) dQ†

β = 1.

Proof See Appendix C.2.

Remark 2 Notice that the term that we optimized in upper bound (10) is exactly the initial objective (7).
So that Theorem 1 also tells us what the form of the optimal distribution is for the initial objective.

Remark 3 Surprisingly, in both Theorem 1 and 2, the solution does not depend on the choice of the func-
tion f , which means that the solution is the same for any f -divergence. This also means that by replacing
f by f ◦, we get similar results for the criterion written in the other direction, with again the same solution.
Hence the order in which we write the divergence does not matter and the optimal solution is optimal for
both orders.

Remark 4 Note that λ∗ is implicitly deﬁned by a ﬁxed-point equation. In Section 3.1 we will show how it
can be computed eﬃciently in the case of empirical distributions.

Remark 5 Obviously, λ† ≥ λ∗, where λ∗ was deﬁned in Theorem 1. Moreover, we have λ∗ ≤ 1/λ†. Indeed,
it is enough to insert λ† = 1/λ∗ into deﬁnition of Q†

β and check that in this case Q†

β ≥ 1.

9

2.5 Convergence Analysis for Optimal Updates

In previous section we derived analytical expressions for the distributions R minimizing last terms in upper
bounds (9) and (10). Assuming Q can perfectly match R, i.e. Df (Q (cid:107) R) = 0, we are now interested in the
convergence of the mixture (6) to the true data distribution Pd for Q = Q∗

β or Q = Q†
β.

We start with simple results showing that adding Q∗

β or Q†

β to the current mixture would yield a strict

improvement of the divergence.

Lemma 2 Under the conditions of Theorem 1, we have

Df

(cid:0)(1 − β)Pg + βQ∗

β

(cid:13)
(cid:13) Pd

(cid:1) ≤ Df

(cid:0)(1 − β)Pg + βPd

(cid:13)
(cid:13) Pd

(cid:1) ≤ (1 − β)Df (Pg (cid:107) Pd).

(11)

Under the conditions of Theorem 2, we have

(cid:32)

Df

Pg

(cid:13)
(cid:13)

(cid:33)

Pd − βQ†
β
1 − β

≤ Df (Pg (cid:107) Pd) ,

Df

(cid:0)(1 − β)Pg + βQ†

(cid:13)
(cid:13) Pd

β

(cid:1) ≤ (1 − β)Df (Pg (cid:107) Pd).

and

then

Proof The ﬁrst inequality follows immediately from the optimality of Q∗
β (hence the value of the objective
at Q∗
β is smaller than at Pd), and the fact that Df is convex in its ﬁrst argument and Df (Pd(cid:107)Pd) = 0.
The second inequality follows from the optimality of Q†
β is smaller
than its value at Pd which itself satisﬁes the condition βdPd ≤ dPd). For the third inequality, we combine
the second inequality with the ﬁrst inequality of Lemma 1 (with Q = R = Q†

β (hence the value of the objective at Q†

β).

The upper bound (11) of Lemma 2 can be reﬁned if the ratio dPg/dPd is almost surely bounded:

Lemma 3 Under the conditions of Theorem 1, if there exists M > 1 such that

Pd((1 − β)dPg > M dPd) = 0

Df

(cid:0)(1 − β)Pg + βQ∗

β

(cid:13)
(cid:13) Pd

(cid:1) ≤ f (λ∗) +

f (M )(1 − λ∗)
M − 1

.

Proof We use Inequality (20) of Lemma 6 with X = β, Y = (1 − β)dPg/dPd, and c = λ∗. We easily
verify that X + Y = ((1 − β)dPg + βdPd)/dPd and max(c, Y ) = ((1 − β)dPg + βdQ∗
β)/dPd and both have
expectation 1 with respect to Pd. We thus obtain:

Df ((1 − β)Pg + βQ∗

β (cid:107) Pd) ≤ f (λ∗) +

f (M ) − f (λ∗)
M − λ∗

(1 − λ∗) .

Since λ∗ ≤ 1 and f is non-increasing on (0, 1) we get

Df ((1 − β)Pg + βQ∗

β (cid:107) Pd) ≤ f (λ∗) +

f (M )(1 − λ∗)
M − 1

.

Remark 6 This upper bound can be tighter than that of Lemma 2 when λ∗ gets close to 1. Indeed, for
λ∗ = 1 the upper bound is exactly 0 and is thus tight, while the upper bound of Lemma 2 will not be zero in
this case.

10

Imagine repeatedly adding T new components to the current mixture Pg, where on every step we use the
same weight β and choose the components described in Theorem 1. In this case Lemma 2 guarantees that the
original objective value Df (Pg (cid:107) Pd) would be reduced at least to (1 − β)T Df (Pg (cid:107) Pd). This exponential rate
of convergence, which at ﬁrst may look surprisingly good, is simply explained by the fact that Q∗
β depends
on the true distribution Pd, which is of course unknown.

Lemma 2 also suggests setting β as large as possible. This is intuitively clear: the smaller the β, the less
we alter our current model Pg. As a consequence, choosing small β when Pg is far away from Pd would lead
to only minor improvements in objective (7). In fact, the global minimum of (7) can be reached by setting
β = 1 and Q = Pd. Nevertheless, in practice we may prefer to keep β relatively small, preserving what we
learned so far through Pg: for instance, when Pg already covered part of the modes of Pd and we want Q to
cover the remaining ones. We provide further discussions on choosing β in Section 3.2.

In the reminder of this section we study the convergence of (7) to 0 in the case where we use the upper
bound (10) and the weight β is ﬁxed (i.e. the same value at each iteration). This analysis can easily be
extended to a variable β.

Lemma 4 For any f ∈ F such that f (x) (cid:54)= 0 for x (cid:54)= 1, the following conditions are equivalent:

(i) Pd((1 − β)dPg > dPd) = 0;

(ii) Df ((1 − β)Pg + βQ∗

β (cid:107) Pd) = 0.

Proof The ﬁrst condition is equivalent to λ∗ = 1 according to Theorem 1. In this case, (1−β)Pg +βQ∗
β = Pd,
hence the divergence is 0. In the other direction, when the divergence is 0, since f is strictly positive for x (cid:54)= 1
(keep in mind that we can always replace f by f0 to get a non-negative function which will be strictly positive
if f (x) (cid:54)= 0 for x (cid:54)= 1), this means that with Pd probability 1 we have the equality dPd = (1 − β)dPg + βdQ∗
β,
which implies that (1 − β)dPg > dPd with Pd probability 1 and also λ∗ = 1.

This result tells that we can not perfectly match Pd by adding a new mixture component to Pg as long as
there are points in the space where our current model Pg severely over-samples. As an example, consider an
extreme case where Pg puts a positive mass in a region outside of the support of Pd. Clearly, unless β = 1,
we will not be able to match Pd.

Finally, we provide a necessary and suﬃcient condition for the iterative process to converge to the data
distribution Pd in ﬁnite number of steps. The criterion is based on the ratio dP1/dPd, where P1 is the ﬁrst
component of our mixture model.

Corollary 1 Take any f ∈ F such that f (x) (cid:54)= 0 for x (cid:54)= 1. Starting from P 1
iteratively according to P t+1
β, where on every step Q∗
with Pg := P t
exists M > 0 such that

model = P1, update the model
β is as deﬁned in Theorem 1
model (cid:107) Pd) will reach 0 in a ﬁnite number of steps if and only if there

model. In this case Df (P t

model = (1 − β)P t

model + βQ∗

Pd((1 − β)dP1 > M dPd) = 0 .

(12)

When the ﬁnite convergence happens, it takes at most − ln max(M, 1)/ ln(1 − β) steps.

Proof From Lemma 4, it is clear that if M ≤ 1 the convergence happens after the ﬁrst update. So let
us assume M > 1. Notice that dP t+1
β = max(λ∗dPd, (1 − β)dP t
model + βdQ∗
model) so that if
Pd((1 − β)dP t
model > M (1 − β)dPd) = 0. This proves that (12) is a
suﬃcient condition.

model > M dPd) = 0, then Pd((1 − β)dP t+1

model = (1 − β)dP t

Now assume the process converged in a ﬁnite number of steps. Let P t

the ﬁnal step. Note that P t
distribution P . According to Lemma 4 we have Pd((1 − β)dP t
immediately imply (12).

model be a mixture right before
model is represented by (1 − β)t−1P1 + (1 − (1 − β)t−1)P for certain probability
model > dPd) = 0. Together these two facts

It is also important to keep in mind that even if (12) is not satisﬁed the process still converges to the true
distribution at exponential rate (see Lemma 2 as well as Corollaries 2 and 3 below)

11

2.6 Weak to Strong Learnability

In practice the component Q that we add to the mixture is not exactly Q∗
β, but rather an approximation
to them. We need to show that if this approximation is good enough, then we retain the property that (8)
is reached. In this section we will show that this is indeed the case.

β or Q†

Looking again at Lemma 1 we notice that the ﬁrst upper bound is less tight than the second one. Indeed,
take the optimal distributions provided by Theorems 1 and 2 and plug them back as R into the upper bounds
of Lemma 1. Also assume that Q can match R exactly, i.e. we can achieve Df (Q (cid:107) R) = 0. In this case both
sides of (10) are equal to Df ((1 − β)Pg + βQ∗
β (cid:107) Pd), which is the optimal value for the original objective (7).
On the other hand, (9) does not become an equality and the r.h.s. is not the optimal one for (7).

This means that using (10) allows to reach the optimal value of the original objective (7), whereas
using (9) does not. However, this is not such a big issue since, as we mentioned earlier, we only need to
improve the mixture by adding the next component (we do not need to add the optimal next component).
So despite the solution of (7) not being reachable with the ﬁrst upper bound, we will still show that (8) can
be reached.

The ﬁrst result provides suﬃcient conditions for strict improvements when we use the upper bound (9).

Corollary 2 Given two distributions Pd, Pg, and some β ∈ (0, 1], assume

Let Q†

β be as deﬁned in Theorem 2. If Q is a distribution satisfying

Pd

(cid:18) dPg
dPd

(cid:19)

= 0

< β.

Df (Q (cid:107) Q†

β) ≤ γDf (Pg (cid:107) Pd)

for γ ∈ [0, 1] then

Df ((1 − β)Pg + βQ (cid:107) Pd) ≤ (1 − β(1 − γ))Df (Pg (cid:107) Pd).

Proof Immediately follows from combining Lemma 1, Theorem 1, and Lemma 2.

Next one holds for Hilbertian metrics and corresponds to the upper bound (10).

Corollary 3 Assume f ∈ FH , i.e. Df is a Hilbertian metric. Take any β ∈ (0, 1], Pd, Pg, and let Q∗
as deﬁned in Theorem 1. If Q is a distribution satisfying

β be

for some γ ∈ [0, 1], then

Df (Q (cid:107) Q∗

β) ≤ γDf (Pg (cid:107) Pd)

Df ((1 − β)Pg + βQ (cid:107) Pd) ≤

(cid:16)(cid:112)γβ + (cid:112)1 − β

(cid:17)2

Df (Pg (cid:107) Pd) .

In particular, the right-hand side is strictly smaller than Df (Pg (cid:107) Pd) as soon as γ < β/4 (and β > 0).

Immediately follows from combining Lemma 1, Theorem 2, and Lemma 2. It is easy to verify that

Proof
for γ < β/4, the coeﬃcient is less than (β/2 +

√

1 − β)2 which is < 1 (for β > 0).

Remark 7 We emphasize once again that the upper bound (10) and Corollary 3 both hold for Jensen-
Shannon, Hellinger, and total variation divergences among others. In particular they can be applied to the
original GAN algorithm.

Conditions 14 and 15 may be compared to the “weak learnability” condition of AdaBoost. As long as our
weak learner is able to solve the surrogate problem (5) of matching respectively Q†
β accurately enough,
the original objective (7) is guaranteed to decrease as well. It should be however noted that Condition 15

β or Q∗

12

(13)

(14)

(15)

Indeed, as already mentioned before,
with γ < β/4 is perhaps too strong to call it “weak learnability”.
the weight β is expected to decrease to zero as the number of components in the mixture distribution Pg
increases. This leads to γ → 0, making it harder to meet Condition 15. This obstacle may be partially
resolved by the fact that we will use a GAN to ﬁt Q, which corresponds to a relatively rich7 class of models
G in (5). In other words, our weak learner is not so weak.

On the other hand, Condition (14) of Corollary 2 is much milder. No matter what γ ∈ [0, 1] and β ∈ (0, 1]
we choose, the new component Q is guaranteed to strictly improve the objective functional. This comes at
the price of the additional Condition (13), which asserts that β should be larger than the mass of true data
Pd missed by the current model Pg. We argue that this is a rather reasonable condition: if Pg misses many
modes of Pd we would prefer assigning a relatively large weight β to the new component Q.

3 AdaGAN

In this section we provide a more detailed description of Algorithm 1 from Section 1.1, in particular how to
reweight the training examples for the next iteration and how to choose the mixture weights.

In a nutshell, at each iteration we want to add a new component Q to the current mixture Pg with
weight β, to create a mixture with distribution (1 − β)Pg + βQ. This component Q should approach an
“optimal target” Q∗

β and we know from Theorem 1 that:

dQ∗

β =

(cid:18)

dPd
β

λ∗ − (1 − β)

(cid:19)

dPg
dPd

.

+

dPg
dPd

(X) = h(cid:0)D(X)(cid:1).

dQ∗

β =

dPd
β

(cid:0)λ∗ − (1 − β)h(D)(cid:1)

+ ,

wi =

pi
β

(cid:0)λ∗ − (1 − β)h(di)(cid:1)

+

Computing this distribution requires to know the density ratio dPg/dPd, which is not directly accessible,
but it can be estimated using the idea of adversarial training. Indeed, we can train a discriminator D to
distinguish between samples from Pd and Pg. It is known that for an arbitrary f -divergence, there exists
a corresponding function h (see [7]) such that the values of the optimal discriminator D are related to the
density ratio in the following way:

In particular, for the Jensen-Shannon divergence, used by the original GAN algorithm, it holds that h(cid:0)D(X)(cid:1) =

1−D(X)
D(X)

. So in this case for the optimal discriminator we have

which can be viewed as a reweighted version of the original data distribution Pd.

In particular, when we compute dQ∗

β on the training sample SN = (X1, . . . , XN ), each example Xi has

the following weight:

with pi = dPd(Xi) and di = D(Xi). In practice, we use the empirical distribution over the training sample
which means we set pi = 1/N .

3.1 How to compute λ∗ of Theorem 1

Next we derive an algorithm to determine λ∗. We need to ﬁnd a value of λ∗ such that the weights wi in (17)
are normalized, i.e.:

(cid:88)

wi =

(cid:0)λ∗ − (1 − β)h(di)(cid:1) = 1 ,

(cid:88)

i

pi
β

i∈I(λ∗)

7The hardness of meeting Condition 15 of course largely depends on the class of models G used to ﬁt Q in (5). For now we

ignore this question and leave it for future research.

13

(16)

(17)

where I(λ) := {i : λ > (1 − β)h(di)}. This in turn yields:

λ∗ =

β
i∈I(λ∗) pi

(cid:80)



1 +

(1 − β)
β

(cid:88)

i∈I(λ∗)



pih(di)

 .

(18)

Now, to compute the r.h.s., we need to know I(λ∗). To do so, we sort the values h(di) in increasing order:
h(d1) ≤ h(d2) ≤ . . . ≤ h(dN ). Then I(λ∗) is simply a set consisting of the ﬁrst k values, where we have to
determine k. Thus, it suﬃces to test successively all positive integers k until the λ given by Equation (18)
veriﬁes:

(1 − β)h(dk) < λ ≤ (1 − β)h(dk+1) .

This procedure is guaranteed to converge, because by Theorem 1, we know that λ∗ exists, and it satisﬁes (18).
In summary, λ∗ can be determined by Algorithm 2.

Algorithm 2: Determining λ∗
1 Sort the values h(di) in increasing order ;
1 + 1−β
2 Initialize λ ← β
p1
3 while (1 − β)h(dk) ≥ λ do
4

β p1h(d1)

(cid:16)

(cid:17)

k ← k + 1;
λ ← β
(cid:80)k

i=1 pi

(cid:16)

5

1 + (1−β)

β

(cid:80)k

(cid:17)
i=1 pih(di)

and k ← 1 ;

3.2 How to choose a mixture weight β

While for every β there is an optimal reweighting scheme, the weights from (17) depend on β. In particular,
if β is large enough to verify dPd(x)λ∗ − (1 − β)dPg(x) ≥ 0 for all x, the optimal component Q∗
β satisﬁes
(1 − β)Pg + βQ∗
β = Pd, as proved in Lemma 4. In other words, in this case we exactly match the data
distribution Pd, assuming the GAN can approximate the target Q∗
β perfectly. This criterion alone would
lead to choosing β = 1. However in practice we know we can’t get a generator that produces exactly the
target distribution Q∗
β. We thus propose a few heuristics one can follow to choose β:

– Any ﬁxed constant value β for all iterations.

– All generators to be combined with equal weights in the ﬁnal mixture model. This corresponds to

setting βt = 1

t , where t is the iteration.

– Instead of choosing directly a value for β one could pick a ratio 0 < r < 1 of examples which should
have a weight wi > 0. Given such an r, there is a unique value of β (βr) resulting in wi > 0 for exactly
N · r training examples. Such a value βr can be determined by binary search over β in Algorithm 2.
Possible choices for r include:

– r constant, chosen experimentally.
– r decreasing with the number of iterations, e.g., r = c1e−c2t for any positive constants c1, c2.

– Alternatively, one can set a particular threshold for the density ratio estimate h(D), compute the
fraction r of training examples that have a value above that threshold and derive β from this ratio
r (as above). Indeed, when h(D) is large, that means that the generator does not generate enough
examples in that region, and the next iteration should be encouraged to generate more there.

14

Algorithm 3: AdaGAN, a meta-algorithm to construct a “strong” mixture of T individual GANs,
trained sequentially. The mixture weight schedule ChooseMixtureWeight should be provided by the
user (see 3.2). This is an instance of the high level Algorithm 1, instantiating UpdateTrainingWeights.

#Compute the new weights of the training examples (UpdateTrainingWeights)

#Compute the discriminator between the original (unweighted) data and the current mixture Gt−1

Input: Training sample SN := {X1, . . . , XN }.

Output: Mixture generative model G = GT .

Train vanilla GAN: G1 = GAN(SN )

for t = 2, . . . , T do

#Choose a mixture weight for the next component

βt = ChooseMixtureWeight(t)

D ← DGAN (SN , Gt−1);
#Compute λ∗ using Algorithm 2
λ∗ ← λ(βt, D)
#Compute the new weight for each example

for i = 1, . . . , N do

W i

t = 1
N βt

(λ∗ − (1 − βt)h(D(Xi)))+

end for
#Train t-th “weak” component generator Gc
t
Gc

t = GAN(SN , Wt)

#Update the overall generative model
#Notation below means forming a mixture of Gt−1 and Gc
t .
Gt = (1 − βt)Gt−1 + βtGc
t

end for

3.3 Complete algorithm

Now we have all the necessary components to introduce the complete AdaGAN meta-algorithm. The algo-
rithm uses any given GAN implementation (which can be the original one of Goodfellow et al. [1] or any
later modiﬁcations) as a building block. Accordingly, Gc ← GAN (SN , W ) returns a generator Gc for a given
set of examples SN = (X1, . . . , XN ) and corresponding weights W = (w1, . . . , wN ). Additionally, we write
D ← DGAN (SN , G) to denote a procedure that returns a discriminator from the GAN algorithm trained
on a given set of true data examples SN and examples sampled from the mixture of generators G. We
also write λ∗(β, D) to denote the optimal λ∗ given by Algorithm 2. The complete algorithm is presented in
Algorithm 3.

4 Experiments

We tested AdaGAN8 on toy datasets, for which we can interpret the missing modes in a clear and reproducible
way, and on MNIST, which is a high-dimensional dataset. The goal of these experiments was not to evaluate

8Code available online at https://github.com/tolstikhin/adagan

15

the visual quality of individual sample points, but to demonstrate that the re-weighting scheme of AdaGAN
promotes diversity and eﬀectively covers the missing modes.

4.1 Toy datasets

The target distribution is deﬁned as a mixture of normal distributions, with diﬀerent variances. The distances
between the means are relatively large compared to the variances, so that each Gaussian of the mixture is
“isolated”. We vary the number of modes to test how well each algorithm performs when there are fewer or
more expected modes.

More precisely, we set X = R2, each Gaussian component is isotropic, and their centers are sampled
uniformly in a square. That particular random seed is ﬁxed for all experiments, which means that for a
given number of modes, the target distribution is always the same. The variance parameter is the same for
each component, and is decreasing with the number of modes, so that the modes stay apart from each other.
This target density is very easy to learn, using a mixture of Gaussians model, and for example the EM
algorithm [19]. If applied to the situation where the generator is producing single Gaussians (i.e. PZ is a
standard Gaussian and G is a linear function), then AdaGAN produces a mixture of Gaussians, however it
does so incrementally unlike EM, which keeps a ﬁxed number of components. In any way AdaGAN was not
tailored for this particular case and we use the Gaussian mixture model simply as a toy example to illustrate
the missing modes problem.

4.1.1 Algorithms

We compare diﬀerent meta-algorithms based on GAN, and the baseline GAN algorithm. All the meta-
algorithms use the same implementation of the underlying GAN procedure.
In all cases, the generator
uses latent space Z = R5, and two ReLU hidden layers, of size 10 and 5 respectively. The corresponding
discriminator has two ReLU hidden layers of size 20 and 10 respectively. We use 64k training examples, and
15 epochs, which is enough compared to the small scale of the problem, and all networks converge properly
and overﬁtting is never an issue. Despite the simplicity of the problem, there are already diﬀerences between
the diﬀerent approaches.

We compare the following algorithms:

– The baseline GAN algorithm, called Vanilla GAN in the results.

– The best model out of T runs of GAN, that is: run T GAN instances independently, then take the run
that performs best on a validation set. This gives an additional baseline with similar computational
complexity as the ensemble approaches. Note that the selection of the best run is done on the reported
target metric (see below), rather than on the internal metric. As a result this baseline is slightly
overestimated. This procedure is called Best of T in the results.

– A mixture of T GAN generators, trained independently, and combined with equal weights (the “bag-

ging” approach). This procedure is called Ensemble in the results.

– A mixture of GAN generators, trained sequentially with diﬀerent choices of data reweighting:

– The AdaGAN algorithm (Algorithm 1), for β = 1/t, i.e. each component will have the same

weight in the resulting mixture (see § 3.2). This procedure is called Boosted in the results.

– The AdaGAN algorithm (Algorithm 1), for a constant β, exploring several values. This procedure

is called for example Beta0.3 for β = 0.3 in the results.

– Reweighting similar to “Cascade GAN” from [14], i.e. keeping the top r fraction of examples,
based on the discriminator corresponding to the previous generator. This procedure is called for
example TopKLast0.3 for r = 0.3.

– Keep the top r fraction of examples, based on the discriminator corresponding to the mixture of

all previous generators. This procedure is called for example TopK0.3 for r = 0.3.

16

4.1.2 Metrics

To evaluate how well the generated distribution matches the target distribution, we use a coverage metric C.
We compute the probability mass of the true data “covered” by the model distribution Pmodel. More
precisely, we compute C := Pd(dPmodel > t) with t such that Pmodel(dPmodel > t) = 0.95. This metric
is more interpretable than the likelihood, making it easier to assess the diﬀerence in performance of the
algorithms. To approximate the density of Pmodel we use a kernel density estimation method, where the
bandwidth is chosen by cross validation. Note that we could also use the discriminator D to approximate
the coverage as well, using the relation from (16).

Another metric is the likelihood of the true data under the generated distribution. More precisely, we
compute L := 1
i log Pmodel(xi), on a sample of N examples from the data. Note that [20] proposes a
N
more general and elegant approach (but less straightforward to implement) to have an objective measure of
GAN. On the simple problems we tackle here, we can precisely estimate the likelihood.

(cid:80)

In the main results we report the metric C and in Appendix E we report both L and C. For a given
metric, we repeat the run 35 times with the same parameters (but diﬀerent random seeds). For each run, the
learning rate is optimized using a grid search on a validation set. We report the median over those multiple
runs, and the interval corresponding to the 5% and 95% percentiles. Note this is not a conﬁdence interval of
the median, which would shrink to a singleton with an inﬁnite number of runs. Instead, this gives a measure
of the stability of each algorithm. The optimizer is a simple SGD: Adam was also tried but gave slightly less
stable results.

4.1.3 Results

With the vanilla GAN algorithm, we observe that not all the modes are covered (see Figure 1 for an
illustration). Diﬀerent modes (and even diﬀerent number of modes) are possibly covered at each restart of
the algorithm, so restarting the algorithm with diﬀerent random seeds and taking the best (“best of T ”) can
improve the results.

Figure 3 summarizes the performance of the main algorithms on the C metric, as a function of the number
of iterations T . Table 1 gives more detailed results, varying the number of modes for the target distribution.
Appendix E contains details on variants for the reweighting heuristics as well as results for the L metric.

As expected, both the ensemble and the boosting approaches signiﬁcantly outperform the vanilla GAN
and the “best of T ” algorithm. Interestingly, the improvements are signiﬁcant even after just one or two
additional iterations (T = 2 or T = 3). The boosted approach converges much faster.
In addition, the
variance is much lower, improving the likelihood that a given run gives good results. On this setup, the
vanilla GAN approach has a signiﬁcant number of catastrophic failures (visible in the lower bound of the
interval).

Empirical results on combining AdaGAN meta-algorithm with the unrolled GANs [4] are available in

Appendix A.

4.2 MNIST and MNIST3

We ran experiments both on the original MNIST and on the 3-digit MNIST (MNIST3) [5, 4] dataset,
obtained by concatenating 3 randomly chosen MNIST images to form a 3-digit number between 0 and 999.
According to [5, 4], MNIST contains 10 modes, while MNIST3 contains 1000 modes, and these modes can
be detected using the pre-trained MNIST classiﬁer. We combined AdaGAN both with simple MLP GANs
and DCGANs [21]. We used T ∈ {5, 10}, tried models of various sizes and performed a reasonable amount
of hyperparameter search. For the details we refer to Appendix B.

Similarly to [4, Sec 3.3.1] we failed to reproduce the missing modes problem for MNIST3 reported in
[5] and found that simple GAN architectures are capable of generating all 1000 numbers. The authors
of [4] proposed to artiﬁcially introduce the missing modes again by limiting the generators’ ﬂexibility. In
our experiments, GANs trained with the architectures reported in [4] were often generating poorly looking
digits. As a result, the pre-trained MNIST classiﬁer was outputting random labels, which again led to full

17

Figure 2: Coverage C of the true data by the model distribution P T
model, as a function of iterations T .
Experiments correspond to the data distribution with 5 modes. Each blue point is the median over 35 runs.
Green intervals are deﬁned by the 5% and 95% percentiles (see Section 4.1.2).
Iteration 0 is equivalent
to one vanilla GAN. The left plot corresponds to taking the best generator out of T runs. The middle
plot corresponds to the “ensemble GAN”, simply taking a uniform mixture of T independently trained
GAN generators. The right plot corresponds to our boosting approach (AdaGAN), carefully reweighting
the examples based on the previous generators, with βt = 1/t. Both the ensemble and boosting approaches
signiﬁcantly outperform the vanilla approach with few additional iterations. They also outperform taking
the best out of T runs. The boosting outperforms all other approaches. For AdaGAN the variance of the
performance is also signiﬁcantly decreased.

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

0.97 (0.9; 1.0)

0.88 (0.4; 1.0)

0.63 (0.5; 1.0)

0.72 (0.5; 0.8)

0.58 (0.4; 0.8)

0.59 (0.2; 0.7)

Best of T (T=3)

0.99 (1.0; 1.0)

0.96 (0.9; 1.0)

0.91 (0.7; 1.0)

0.80 (0.7; 0.9)

0.84 (0.7; 0.9)

0.70 (0.6; 0.8)

Best of T (T=10)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

0.98 (0.8; 1.0)

0.80 (0.8; 0.9)

0.87 (0.8; 0.9)

0.71 (0.7; 0.8)

Ensemble (T=3)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.93 (0.8; 1.0)

0.78 (0.6; 1.0)

0.85 (0.6; 1.0)

0.80 (0.6; 1.0)

Ensemble (T=10)

1.00 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

0.91 (0.8; 1.0)

0.88 (0.8; 1.0)

0.89 (0.7; 1.0)

TopKLast0.5 (T=3)

0.98 (0.9; 1.0)

0.98 (0.9; 1.0)

0.95 (0.9; 1.0)

0.95 (0.8; 1.0)

0.86 (0.7; 1.0)

0.86 (0.6; 0.9)

TopKLast0.5 (T=10)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.98 (1.0; 1.0)

0.99 (0.8; 1.0)

0.99 (0.8; 1.0)

1.00 (0.8; 1.0)

Boosted (T=3)

0.99 (1.0; 1.0)

0.99 (0.9; 1.0)

0.98 (0.9; 1.0)

0.91 (0.8; 1.0)

0.91 (0.8; 1.0)

0.86 (0.7; 1.0)

Boosted (T=10)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

Table 1: Performance of the diﬀerent algorithms on varying number of mixtures of Gaussians. The reported
score is the coverage C, probability mass of Pd covered by the 5th percentile of Pg deﬁned in Section 4.1.2.
See Table 2 for more metrics. The reported scores are the median and interval deﬁned by the 5% and 95%
percentile (in parenthesis) (see Section 4.1.2), over 35 runs for each setting. Note that the 95% interval is
not the usual conﬁdence interval measuring the variance of the experiment itself, but rather measures the
stability of the diﬀerent algorithms (would remain even if each experiment was run an inﬁnite number of
times). Both the ensemble and the boosting approaches signiﬁcantly outperform the vanilla GAN even with
just three iterations (i.e. just two additional components). The boosting approach converges faster to the
optimal coverage and with smaller variance.

18

coverage of the 1000 numbers. We tried to threshold the conﬁdence of the pre-trained classiﬁer, but decided
that this metric was too ad-hoc.

For MNIST we noticed that the re-weighted distribu-
tion was often concentrating its mass on digits having
very speciﬁc strokes: on diﬀerent rounds it could high-
light thick, thin, vertical, or diagonal digits, indicating
that these traits were underrepresented in the generated
samples (see Figure 3). This suggests that AdaGAN does
a reasonable job at picking up diﬀerent modes of the
dataset, but also that there are more than 10 modes in
MNIST (and more than 1000 in MNIST3). It is not clear
how to evaluate the quality of generative models in this
context.

We also tried to use the “inversion” metric discussed
in Section 3.4.1 of [4]. For MNIST3 we noticed that a
single GAN was capable of reconstructing most of the
training points very accurately both visually and in the
(cid:96)2-reconstruction sense.

5 Conclusion

Figure 3: Digits from the MNIST dataset cor-
responding to the smallest (left) and largest
(right) weights, obtained by the AdaGAN pro-
cedure (see Section 3) in one of the runs. Bold
digits (left) are already covered and next GAN
will concentrate on thin (right) digits.

We presented an incremental procedure for constructing
an additive mixture of generative models by minimizing
an f -divergence criterion. Based on this, we derived a boosting-style algorithm for GANs, which we call
AdaGAN. By incrementally adding new generators into a mixture through the optimization of a GAN
criterion on a reweighted data, this algorithm is able to progressively cover all the modes of the true data
distribution. This addresses one of the main practical issues of training GANs.

We also presented a theoretical analysis of the convergence of this incremental procedure and showed
conditions under which the mixture converges to the true distribution either exponentially or in a ﬁnite
number of steps.

Our preliminary experiments (on toy data) show that this algorithm is eﬀectively addressing the missing

modes problem and allows to robustly produce a mixture which covers all modes of the data.

However, since the generative model that we obtain is not a single neural network but a mixture of such
networks, the corresponding latent representation no longer has a smooth structure. This can be seen as a
disadvantage compared to standard GAN where one can perform smooth interpolation in latent space. On
the other hand it also allows to have a partitioned latent representation where one component is discrete.
Future work will explore the possibility of leveraging this structure to model discrete aspects of the dataset,
such as the class in object recognition datasets in a similar spirit to [22].

References

[1] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Pro-
cessing Systems, pages 2672–2680, 2014.

[2] D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In ICLR, 2014.

[3] Martin Arjovsky, Soumith Chintala, and Lon Bottou. Wasserstein GAN. arXiv:1701.07875, 2017.

[4] L. Metz, B. Poole, D. Pfau, and J. Sohl-Dickstein. Unrolled generative adversarial networks.

arXiv:1611.02163, 2017.

19

[5] Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized generative

adversarial networks. arXiv:1612.02136, 2016.

[6] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application

to boosting. Journal of Computer and System Sciences, 55(1):119–139, 1997.

[7] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka.

f-GAN: Training generative neural samplers
using variational divergence minimization. In Advances in Neural Information Processing Systems, 2016.

[8] Max Welling, Richard S. Zemel, and Geoﬀrey E. Hinton. Self supervised boosting.

In Advances in

neural information processing systems, pages 665–672, 2002.

[9] Zhuowen Tu. Learning generative models via discriminative approaches. In 2007 IEEE Conference on

Computer Vision and Pattern Recognition, pages 1–8. IEEE, 2007.

[10] Aditya Grover and Stefano Ermon. Boosted generative models. ICLR 2017 conference submission, 2016.

[11] R. M. Neal. Annealed importance sampling. Statistics and Computing, 11(2):125–139, 2001.

[12] Saharon Rosset and Eran Segal. Boosting density estimation.

In Advances in Neural Information

Processing Systems, pages 641–648, 2002.

[13] A Barron and J Li. Mixture density estimation. Biometrics, 53:603–618, 1997.

[14] Yaxing Wang, Lichao Zhang, and Joost van de Weijer. Ensembles of generative adversarial networks.

arXiv:1612.00991, 2016.

[15] F. Liese and K.-J. Miescke. Statistical Decision Theory. Springer, 2008.

[16] M. D. Reid and R. C. Williamson. Information, divergence and risk for binary experiments. Journal of

Machine Learning Research, 12:731–817, 2011.

[17] Bent Fuglede and Flemming Topsoe. Jensen-shannon divergence and hilbert space embedding. In IEEE

International Symposium on Information Theory, pages 31–31, 2004.

[18] Matthias Hein and Olivier Bousquet. Hilbertian metrics and positive deﬁnite kernels on probability

measures. In AISTATS, pages 136–143, 2005.

[19] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM

algorithm. Journal of the Royal Statistical Society, B, 39:1–38, 1977.

[20] Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger Grosse. On the quantitative analysis of

decoder-based generative models, 2016.

[21] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional

generative adversarial networks. In ICLR, 2016.

[22] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: In-
terpretable representation learning by information maximizing generative adversarial nets. In Advances
in Neural Information Processing Systems, pages 2172–2180, 2016.

20

A Further details on toy experiments

To illustrate the ’meta-algorithm aspect’ of AdaGAN, we also performed experiments with an unrolled
GAN [4] instead of a GAN as the base generator. We trained the GANs both with the Jensen-Shannon
objective (2), and with its modiﬁed version proposed in [1] (and often considered as the baseline GAN),
where log(1 − D(G(Z))) is replaced by − log(D(G(Z))). We use the same network architecture as in the
other toy experiments. Figure 4 illustrates our results. We ﬁnd that AdaGAN works with all underlying
GAN algorithms. Note that, where the usual GAN updates the generator and the discriminator once, an
unrolled GAN with 5 unrolling steps updates the generator once and the discriminator 1 + 5, i.e. 6 times
(and then rolls back 5 steps). Thus, in terms of computation time, training 1 single unrolled GAN roughly
corresponds to doing 3 steps of AdaGAN with a usual GAN. In that sense, Figure 4 shows that AdaGAN
(with a usual GAN) signiﬁcantly outperforms a single unrolled GAN. Additionally, we note that using the
Jensen-Shannon objective (rather than the modiﬁed version) seems to have some mode-regularizing eﬀect.
Surprisingly, using unrolling steps makes no signiﬁcant diﬀerence.

Figure 4: Comparison of AdaGAN ran with a GAN (top row) and with an unrolled GAN [4] (bottom).
Coverage C of the true data by the model distribution P T
model, as a function of iterations T . Experiments
are similar to those of Figure 3, but with 10 modes. Top and bottom rows correspond to the usual and the
unrolled GAN (with 5 unrolling steps) respectively, trained with the Jensen-Shannon objective (2) on the
left, and with the modiﬁed objective originally proposed by [1] on the right. In terms of computation time,
one step of AdaGAN with unrolled GAN corresponds to roughly 3 steps of AdaGAN with a usual GAN. On
all images T = 1 corresponds to vanilla unrolled GAN.

B Further details on MNIST/MNIST3 experiments

GAN Architecture We ran AdaGAN on MNIST (28x28 pixel images) using (de)convolutional networks
with batch normalizations and leaky ReLu. The latent space has dimension 100. We used the following

21

architectures:

Generator: 100 x 1 x 1 → fully connected → 7 x 7 x 16 → deconv → 14 x 14 x 8 →

Discriminator: 28 x 28 x 1 → conv → 14 x 14 x 16 → conv → 7 x 7 x 32 →

→ deconv → 28 x 28 x 4 → deconv → 28 x 28 x 1

→ fully connected → 1

where each arrow consists of a leaky ReLu (with 0.3 leak) followed by a batch normalization, conv and deconv
are convolutions and transposed convolutions with 5x5 ﬁlters, and fully connected are linear layers with bias.
The distribution over Z is uniform over the unit box. We use the Adam optimizer with β1 = 0.5, with 2
G steps for 1 D step and learning rates 0.005 for G, 0.001 for D, and 0.0001 for the classiﬁer C that does
the reweighting of digits. We optimized D and G over 200 epochs and C over 5 epochs, using the original
Jensen-Shannon objective (2), without the log trick, with no unrolling and with minibatches of size 128.

Empirical observations Although we could not ﬁnd any appropriate metric to measure the increase of
diversity promoted by AdaGAN, we observed that the re-weighting scheme indeed focuses on digits with
very speciﬁc strokes. In Figure 5 for example, we see that after one AdaGAN step, the generator produces
overly thick digits (top left image). Thus AdaGAN puts small weights on the thick digits of the dataset
(bottom left) and high weights on the thin ones (bottom right). After the next step, the new GAN produces
both thick and thin digits.

C Proofs

C.1 Proof of Theorem 1

Before proving Theorem 1, we introduce two lemmas. The ﬁrst one is about the determination of the constant
λ, the second one is about comparing the divergences of mixtures.

Lemma 5 Let P and Q be two distributions, γ ∈ [0, 1] and λ ∈ R. The function

(cid:90) (cid:18)

g(λ) :=

λ − γ

(cid:19)

dQ
dP

dP

+

g(cid:48)
+(λ) = P (λ · dP ≥ γ · dQ).

is nonnegative, convex, nondecreasing, satisﬁes g(λ) ≤ λ, and its right derivative is given by

The equation

g(λ) = 1 − γ
has a solution λ∗ (unique when γ < 1) with λ∗ ∈ [1 − γ, 1]. Finally, if P (dQ = 0) ≥ δ for a strictly positive
constant δ then λ∗ ≤ (1 − γ)δ−1.

Proof The convexity of g follows immediately from the convexity of x (cid:55)→ (x)+ and the linearity of the
integral. Similarly, since x (cid:55)→ (x)+ is non-decreasing, g is non-decreasing.

We deﬁne the set I(λ) as follows:

Now let us consider g(λ + (cid:15)) − g(λ) for some small (cid:15) > 0. This can also be written:

I(λ) := {x ∈ X : λ · dP (x) ≥ γ · dQ(x)}.

g(λ + (cid:15)) − g(λ) =

(cid:15)dP +

(λ + (cid:15))dP −

I(λ)

I(λ+(cid:15))\I(λ)

I(λ+(cid:15))\I(λ)

(cid:90)

(cid:90)

(cid:90)

(cid:90)

(cid:90)

γdQ

γdQ.

= (cid:15)P (I(λ)) +

(λ + (cid:15))dP −

I(λ+(cid:15))\I(λ)

I(λ+(cid:15))\I(λ)

22

Figure 5: AdaGAN on MNIST. Bottom row are true MNIST digits with smallest (left) and highest (right)
weights after re-weighting at the end of the ﬁrst AdaGAN step. Those with small weight are thick and
resemble those generated by the GAN after the ﬁrst AdaGAN step (top left). After training with the re-
weighted dataset during the second iteration of AdaGAN, the new mixture produces more thin digits (top
right).

On the set I(λ + (cid:15))\I(λ), we have

(λ + (cid:15))dP − γdQ ∈ [0, (cid:15)].

So that

and thus

(cid:15)P (I(γ)) ≤ g(λ + (cid:15)) − g(λ) ≤ (cid:15)P (I(γ)) + (cid:15)P (cid:0)I(λ + (cid:15))\I(λ)(cid:1) = (cid:15)P (I(λ + (cid:15)))

lim
(cid:15)→0+

g(λ + (cid:15)) − g(λ)
(cid:15)

= lim
(cid:15)→0+

P (I(λ + (cid:15))) = P (I(λ)).

This gives the expression of the right derivative of g. Moreover, notice that for λ, γ > 0

g(cid:48)
+(λ) = P (λ · dP ≥ γ · dQ) = P

≤

= 1 − P

>

≥ 1 − γ/λ

(cid:18) dQ
dP

(cid:19)

λ
γ

(cid:18) dQ
dP

(cid:19)

λ
γ

by Markov’s inequality.

23

It is obvious that g(0) = 0. By Jensen’s inequality applied to the convex function x (cid:55)→ (x)+, we have
g(λ) ≥ (λ − γ)+. So g(1) ≥ 1 − γ. Also, g = 0 on R− and g ≤ λ. This means g is continuous on R and
thus reaches the value 1 − γ on the interval (0, 1] which shows the existence of λ∗ ∈ (0, 1]. To show that λ∗
is unique we notice that since g(x) = 0 on R−, g is convex and non-decreasing, g cannot be constant on an
interval not containing 0, and thus g(x) = 1 − γ has a unique solution for γ < 1.

Also by convexity of g,

g(0) − g(λ∗) ≥ −λ∗g(cid:48)

+(λ∗),

which gives λ∗ ≥ (1 − γ)/g(cid:48)
the fact that g(cid:48)

+ is increasing we conclude that λ∗ ≤ (1 − γ)δ−1.

+(λ∗) ≥ 1 − γ since g(cid:48)

+ ≤ 1. If P (dQ = 0) ≥ δ > 0 then also g(cid:48)

+(0) ≥ δ > 0. Using

Next we introduce some simple convenience lemma for comparing convex functions of random variables.

Lemma 6 Let f be a convex function, X, Y be real-valued random variables and c ∈ R be a constant such
that

E [max(c, Y )] = E [X + Y ] .

Then we have the following bound:

If in addition, Y ≤ M a.s. for M ≥ c, then

E [f (max(c, Y ))] ≤ E [f (X + Y )] − E [X(f (cid:48)(Y ) − f (cid:48)(c))+] ≤ E [f (X + Y )] .

E [f (max(c, Y ))] ≤ f (c) +

(E [X + Y ] − c).

f (M ) − f (c)
M − c

Proof We decompose the expectation with respect to the value of the max, and use the convexity of f :

f (X + Y ) − f (max(c, Y )) = 1[Y ≤c](f (X + Y ) − f (c)) + 1[Y >c](f (X + Y ) − f (Y ))

(19)

(20)

≥ 1[Y ≤c]f (cid:48)(c)(X + Y − c) + 1[Y >c]Xf (cid:48)(Y )
= (1 − 1[Y >c])Xf (cid:48)(c) + f (cid:48)(c)(Y − max(c, Y )) + 1[Y >c]Xf (cid:48)(Y )
= f (cid:48)(c)(X + Y − max(c, Y )) + 1[Y >c]X(f (cid:48)(Y ) − f (cid:48)(c))
= f (cid:48)(c)(X + Y − max(c, Y )) + X(f (cid:48)(Y ) − f (cid:48)(c))+,

where we used that f (cid:48) is non-decreasing in the last step. Taking the expectation gives the ﬁrst inequality.

For the second inequality, we use the convexity of f on the interval [c, M ]:

f (max(c, Y )) ≤ f (c) +

(max(c, Y ) − c).

f (M ) − f (c)
M − c

Taking an expectation on both sides gives the second inequality.

Proof [Theorem 1] We ﬁrst apply Lemma 5 with γ = 1 − β and this proves the existence of λ∗ in the interval
(β, 1], which shows that Q∗

β is indeed well-deﬁned as a distribution.

Then we use Inequality (19) of Lemma 6 with X = βdQ/dPd, Y = (1 − β)dPg/dPd, and c = λ∗. We
β)/dPd and both

easily verify that X + Y = ((1 − β)dPg + βdQ)/dPd and max(c, Y ) = ((1 − β)dPg + βdQ∗
have expectation 1 with respect to Pd. We thus obtain for any distribution Q,

Df ((1 − β)Pg + βQ∗

β (cid:107) Pd) ≤ Df ((1 − β)Pg + βQ (cid:107) Pd) .

This proves the optimality of Q∗
β.

24

C.2 Proof of Theorem 2

Lemma 7 Let P and Q be two distributions, γ ∈ (0, 1), and λ ≥ 0. The function

h(λ) :=

− λ

(cid:90) (cid:18) 1
γ

(cid:19)

dQ
dP

dP

+

h(λ) =

1 − γ
γ

is convex, non-increasing, and its right derivative is given by h(cid:48)
∆ := P (dQ(X)/dP (X) = 0). Then the equation

+(λ) = −Q(1/γ ≥ λdQ(X)/dP (X)). Denote

has no solutions if ∆ > 1 − γ, has a single solution λ† ≥ 1 if ∆ < 1 − γ, and has inﬁnitely many or no
solutions when ∆ = 1 − γ.

Proof The convexity of h follows immediately from the convexity of x (cid:55)→ (a − x)+ and the linearity of the
integral. Similarly, since x (cid:55)→ (a − x)+ is non-increasing, h is non-increasing as well.

We deﬁne the set J (λ) as follows:

(cid:26)

J (λ) :=

x ∈ X :

≥ λ

(x)

.

(cid:27)

1
γ

dQ
dP

Now let us consider h(λ) − h(λ + (cid:15)) for any (cid:15) > 0. Note that J (λ + (cid:15)) ⊆ J (λ). We can write:

h(λ) − h(λ + (cid:15)) =

(cid:90)

(cid:90)

(cid:90)

=

=

(cid:18) 1
γ

J (λ)

(cid:19)

(cid:90)

− λ

dP −

dQ
dP
(cid:18) 1
γ
(cid:18) 1
γ

J (λ+(cid:15))
(cid:90)

(cid:19)

dP +

− λ

dQ
dP

dQ
dP

(cid:19)

J (λ)\J (λ+(cid:15))

J (λ)\J (λ+(cid:15))

J (λ+(cid:15))

(cid:18) 1
γ

(cid:19)

dP

dQ
dP
(cid:19)

dP

− (λ + (cid:15))

(cid:18)

(cid:15)

dQ
dP

− λ

dP + (cid:15) · Q(J (λ + (cid:15))).

Note that for x ∈ J (λ) \ J (λ + (cid:15)) we have

0 ≤

− λ

(x) < (cid:15)

(x).

1
γ

dQ
dP

dQ
dP

This gives the following:

which shows that h is continuous. Also

(cid:15) · Q(J (λ + (cid:15))) ≤ h(λ) − h(λ + (cid:15)) ≤ (cid:15) · Q(J (λ + (cid:15))) + (cid:15) · Q(J (λ) \ J (λ + (cid:15))) = (cid:15) · Q(J (λ)),

lim
(cid:15)→0+

h(λ + (cid:15)) − h(λ)
(cid:15)

= lim
(cid:15)→0+

−Q(J (λ + (cid:15))) = −Q(J (λ)).

It is obvious that h(0) = 1/γ and h ≤ γ−1 for λ ≥ 0. By Jensen’s inequality applied to the convex
+. So h(1) ≥ γ−1 − 1. We conclude that h may reach the

function x (cid:55)→ (a − x)+, we have h(λ) ≥ (cid:0)γ−1 − λ(cid:1)
value (1 − γ)/γ = γ−1 − 1 only on [1, +∞). Note that

h(λ) →

P

(X) = 0

=

≥ 0

as λ → ∞.

1
γ

(cid:18) dQ
dP

(cid:19)

∆
γ

Thus if ∆/γ > γ−1 −1 the equation h(λ) = γ−1 −1 has no solutions, as h is non-increasing. If ∆/γ = γ−1 −1
then either h(λ) > γ−1 − 1 for all λ ≥ 0 and we have no solutions or there is a ﬁnite λ(cid:48) ≥ 1 such that

25

h(λ(cid:48)) = γ−1 − 1, which means that the equation is also satisﬁed by all λ ≥ λ(cid:48), as h is continuous and
non-increasing. Finally, if ∆/γ < γ−1 − 1 then there is a unique λ† such that h(λ†) = γ−1 − 1, which follows
from the convexity of h.

Next we introduce some simple convenience lemma for comparing convex functions of random variables.

Lemma 8 Let f be a convex function, X, Y be real-valued random variables such that X ≤ Y a.s., and
c ∈ R be a constant such that9

Then we have the following lower bound:

E [min(c, Y )] = E [X] .

E [f (X) − f (min(c, Y ))] ≥ 0.

Proof We decompose the expectation with respect to the value of the min, and use the convexity of f :

f (X) − f (min(c, Y )) = 1[Y ≤c](f (X) − f (Y )) + 1[Y >c](f (X) − f (c))

≥ 1[Y ≤c]f (cid:48)(Y )(X − Y ) + 1[Y >c](X − c)f (cid:48)(c)
≥ 1[Y ≤c]f (cid:48)(c)(X − Y ) + 1[Y >c](X − c)f (cid:48)(c)
= Xf (cid:48)(c) − min(Y, c)f (cid:48)(c),

where we used the fact that f (cid:48) is non-decreasing in the previous to last step. Taking the expectation we get
the result.

Lemma 9 Let Pg, Pd be two ﬁxed distributions and β ∈ (0, 1). Assume

Pd

(cid:18) dPg
dPd

(cid:19)

= 0

< β.

Let M(Pd, β) be the set of all probability distributions T such that (1 − β)dT ≤ dPd. Then the following
minimization problem:

has the solution T ∗ with density

min
T ∈M(Pd,β)

Df (T (cid:107) Pg)

dT ∗ := min(dPd/(1 − β), λ†dPg),

where λ† is the unique value in [1, ∞) such that (cid:82) dT ∗ = 1.
Proof We will use Lemma 8 with X = dT (Z)/dPg(Z), Y = dPd(Z)/(cid:0)(1 − β)dPg(Z)(cid:1), and c = λ∗, Z ∼ Pg.
We need to verify that assumptions of Lemma 8 are satisﬁed. Obviously, Y ≥ X. We need to show that
there is a constant c such that

Rewriting this equation we get the following equivalent one:

(cid:90)

(cid:18)

min

c,

(cid:19)

dPd
(1 − β)dPg

dPg = 1.

(cid:90)

β =

(dPd − min (c(1 − β)Pg, dPd)) = (1 − β)

(21)

(cid:90) (cid:18) 1

1 − β

− c

(cid:19)

dPg
dPd

+

dPd.

Using the fact that

9Generally it is not guaranteed that such a constant c always exists. In this result we assume this is the case.

Pd

(cid:18) dPg
dPd

(cid:19)

= 0

< β

26

we may apply Lemma 7 and conclude that there is a unique c ∈ [1, ∞) satisfying (21), which we denote λ†.

To conclude the proof of Theorem 2, observe that from Lemma 9, by making the change of variable

T = (Pd − βQ)/(1 − β) we can rewrite the minimization problem as follows:

min
Q: βdQ≤dPd

Df ◦

Pg (cid:107)

(cid:18)

(cid:19)

Pd − βQ
1 − β

and we verify that the solution has the form dQ†
depend on f , the fact that we optimized Df ◦ is irrelevant and we get the same solution for Df .

(cid:0)dPd − λ†(1 − β)dPg

+. Since this solution does not

β = 1
β

(cid:1)

D f -Divergences

Jensen-Shannon This divergence corresponds to

Df (P (cid:107)Q) = JS(P, Q) =

(cid:90)

X

(cid:18) dP
dQ

f

(cid:19)

(x)

dQ(x)

f (u) = −(u + 1) log

+ u log u.

u + 1
2

with

Indeed,

(cid:90)

X
(cid:18) p(x)
q(x)

JS(P, Q) :=

q(x)

−

+ 1

log





(cid:19)

(cid:18) p(x)
q(x)



p(x)
q(x) + 1
2



 +

p(x)
q(x)

log

p(x)
q(z)



 dx

=

=

(cid:90)

X

(cid:90)

X

q(x)

log

2q(x)
p(x) + q(x)

+ log

p(x) log

2q(x)
p(x) + q(x)

2q(x)
p(x) + q(x)
2q(x)
p(x) + q(x)
(cid:19)

+

p(x)
q(z)

log

(cid:19)

dx

p(x)
q(z)

p(x)
q(z)

dx

+ q(x) log

+ p(x) log

(cid:18)

= KL

Q,

(cid:19)

P + Q
2

(cid:18)

+ KL

P,

P + Q
2

.

E Additional experimental results

At each iteration of the boosting approach, diﬀerent reweighting heuristics are possible. This section contains
more complete results about the following three heuristics:

– Constant β, and using the proposed reweighting scheme given β. See Table 3.

– Reweighting similar to “Cascade GAN” from [14], i.e. keep the top x% of examples, based on the

discriminator corresponding to the previous generator. See Table 4.

– Keep the top x% of examples, based on the discriminator corresponding to the mixture of all previous

generators. See Table 5.

Note that when properly tuned, each reweighting scheme outperforms the baselines, and have similar
performances when used with few iterations. However, they require an additional parameter to tune, and
are worse than the simple β = 1/t heuristic proposed above.

27

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

0.97 (0.9; 1.0)

0.88 (0.4; 1.0)

0.63 (0.5; 1.0)

0.72 (0.5; 0.8)

0.58 (0.4; 0.8)

0.59 (0.2; 0.7)

Best of T (T=3)

0.99 (1.0; 1.0)

0.96 (0.9; 1.0)

0.91 (0.7; 1.0)

0.80 (0.7; 0.9)

0.84 (0.7; 0.9)

0.70 (0.6; 0.8)

Best of T (T=10)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

0.98 (0.8; 1.0)

0.80 (0.8; 0.9)

0.87 (0.8; 0.9)

0.71 (0.7; 0.8)

Ensemble (T=3)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.93 (0.8; 1.0)

0.78 (0.6; 1.0)

0.85 (0.6; 1.0)

0.80 (0.6; 1.0)

Ensemble (T=10)

1.00 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

0.91 (0.8; 1.0)

0.88 (0.8; 1.0)

0.89 (0.7; 1.0)

Boosted (T=3)

0.99 (1.0; 1.0)

0.99 (0.9; 1.0)

0.98 (0.9; 1.0)

0.91 (0.8; 1.0)

0.91 (0.8; 1.0)

0.86 (0.7; 1.0)

Boosted (T=10)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

Best of T (T=3)

Best of T (T=10)

Ensemble (T=3)

Ensemble (T=10)

Boosted (T=3)

Boosted (T=10)

−4.49
(−5.4; −4.4)
−4.39
(−4.6; −4.3)
−4.34
(−4.4; −4.3)
−4.46
(−4.8; −4.4)
−4.52
(−4.7; −4.4)
−4.50
(−4.8; −4.4)
−4.55
(−4.6; −4.4)

−6.02
(−86.8; −5.3)
−5.40
(−24.3; −5.2)
−5.24
(−5.4; −5.2)
−5.59
(−6.6; −5.2)
−5.49
(−6.6; −5.2)
−5.32
(−5.8; −5.2)
−5.30
(−5.5; −5.2)

−16.03
(−59.6; −5.5)
−5.57
(−23.5; −5.4)
−5.45
(−5.6; −5.3)
−4.78
(−5.5; −4.6)
−4.98
(−6.5; −4.6)
−4.80
(−5.8; −4.6)
−5.07
(−5.6; −4.7)

−23.65
(−118.8; −5.7)
−9.91
(−35.8; −5.1)
−5.49
(−9.4; −5.0)
−14.71
(−51.9; −5.4)
−5.44
(−6.0; −5.2)
−5.39
(−19.3; −5.1)
−5.25
(−5.5; −4.6)

−126.87
(−250.4; −12.8)
−36.94
(−90.0; −9.7)
−9.72
(−17.3; −6.5)
−6.70
(−28.7; −5.5)
−5.82
(−6.4; −5.5)
−5.56
(−12.4; −5.2)
−5.03
(−5.5; −4.8)

−55.51
(−185.2; −11.2)
−19.12
(−59.2; −9.7)
−9.12
(−16.8; −6.6)
−8.59
(−25.4; −6.1)
−6.08
(−6.3; −5.7)
−8.03
(−28.7; −6.1)
−5.92
(−6.2; −5.6)

Table 2: Performance of the diﬀerent algorithms on varying number of mixtures of Gaussians. The reported
scores are the median and interval deﬁned by the 5% and 95% percentile (in parenthesis) (see Section
4.1.2), over 35 runs for each setting. The top table reports the coverage C, probability mass of Pd covered
by the 5th percentile of Pg deﬁned in Section 4.1.2. The bottom table reports the log likelihood of the
true data under the model Pg. Note that the 95% interval is not the usual conﬁdence interval measuring
the variance of the experiment itself, but rather measures the stability of the diﬀerent algorithms (would
remain even if each experiment was run an inﬁnite number of times). Both the ensemble and the boosting
approaches signiﬁcantly outperform the vanilla GAN even with just three iterations (i.e. just two additional
components). The boosting approach converges faster to the optimal coverage.

28

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

0.98 (0.9; 1.0)

0.86 (0.5; 1.0)

0.66 (0.5; 1.0)

0.61 (0.5; 0.8)

0.55 (0.4; 0.7)

0.58 (0.3; 0.8)

Boosted (T=3)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.98 (0.9; 1.0)

0.93 (0.8; 1.0)

0.97 (0.8; 1.0)

0.87 (0.6; 1.0)

Boosted (T=10)

1.00 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

0.99 (0.9; 1.0)

0.99 (0.8; 1.0)

0.97 (0.8; 1.0)

Beta0.2 (T=3)

0.99 (1.0; 1.0)

0.97 (0.9; 1.0)

0.97 (0.9; 1.0)

0.95 (0.8; 1.0)

0.96 (0.7; 1.0)

0.88 (0.7; 1.0)

Beta0.2 (T=10)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (0.9; 1.0)

1.00 (0.9; 1.0)

Beta0.3 (T=3)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.98 (0.9; 1.0)

0.96 (0.8; 1.0)

0.96 (0.6; 1.0)

0.88 (0.7; 1.0)

Beta0.3 (T=10)

1.00 (1.0; 1.0)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (0.9; 1.0)

0.99 (0.9; 1.0)

Beta0.4 (T=3)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.95 (0.9; 1.0)

0.94 (0.8; 1.0)

0.89 (0.7; 1.0)

0.89 (0.7; 1.0)

Beta0.4 (T=10)

0.99 (1.0; 1.0)

0.99 (0.9; 1.0)

0.96 (0.9; 1.0)

0.97 (0.8; 1.0)

0.99 (0.8; 1.0)

0.90 (0.8; 1.0)

Beta0.5 (T=3)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.97 (0.8; 1.0)

0.82 (0.8; 1.0)

0.86 (0.7; 1.0)

0.81 (0.6; 1.0)

Beta0.5 (T=10)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.97 (0.9; 1.0)

0.84 (0.8; 1.0)

0.87 (0.7; 1.0)

0.91 (0.8; 1.0)

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

Boosted (T=3)

Boosted (T=10)

Beta0.2 (T=3)

Beta0.2 (T=10)

Beta0.3 (T=3)

Beta0.3 (T=10)

Beta0.4 (T=3)

Beta0.4 (T=10)

Beta0.5 (T=3)

Beta0.5 (T=10)

−4.50
(−5.0; −4.4)
−4.56
(−4.9; −4.4)
−4.56
(−4.7; −4.5)
−4.52
(−4.8; −4.4)
−4.58
(−4.8; −4.5)
−4.60
(−4.9; −4.4)
−4.57
(−4.8; −4.4)
−4.62
(−4.9; −4.4)
−4.49
(−4.7; −4.4)
−4.60
(−4.9; −4.4)
−4.62
(−4.8; −4.4)

−5.65
(−72.7; −5.1)
−5.55
(−5.9; −5.2)
−5.46
(−5.6; −5.3)
−5.31
(−5.6; −5.1)
−5.30
(−5.5; −5.2)
−5.34
(−5.7; −5.2)
−5.37
(−5.5; −5.2)
−5.36
(−5.6; −5.1)
−5.40
(−5.7; −5.3)
−5.40
(−5.7; −5.3)
−5.43
(−5.7; −5.2)

−19.63
(−62.1; −5.6)
−5.01
(−6.7; −4.7)
−5.08
(−5.8; −4.7)
−4.85
(−6.3; −4.6)
−4.94
(−6.6; −4.6)
−5.41
(−5.7; −5.1)
−5.27
(−5.6; −5.0)
−4.74
(−5.3; −4.6)
−5.08
(−6.9; −4.7)
−4.77
(−5.4; −4.6)
−5.12
(−6.6; −4.7)

−28.16
(−293.1; −16.3)
−5.49
(−18.7; −4.9)
−5.04
(−5.5; −4.6)
−5.33
(−14.4; −4.8)
−5.23
(−5.5; −4.7)
−5.33
(−12.9; −4.9)
−5.26
(−5.6; −5.0)
−5.34
(−26.2; −4.9)
−5.49
(−5.9; −5.2)
−5.63
(−24.5; −5.2)
−5.48
(−8.4; −5.1)

−56.94
(−248.1; −14.3)
−5.60
(−14.5; −5.0)
−5.51
(−5.9; −5.1)
−5.68
(−26.2; −5.2)
−5.60
(−6.0; −5.3)
−5.68
(−11.0; −5.4)
−5.71
(−6.0; −5.3)
−5.77
(−37.3; −5.1)
−5.43
(−6.0; −5.1)
−6.05
(−17.9; −5.5)
−5.85
(−6.1; −5.3)

−71.11
(−184.8; −12.5)
−6.86
(−47.3; −5.6)
−5.51
(−6.0; −5.2)
−6.13
(−32.7; −5.7)
−5.98
(−6.1; −5.7)
−6.41
(−29.2; −5.6)
−5.82
(−6.1; −5.4)
−12.37
(−75.9; −5.9)
−5.68
(−6.2; −5.2)
−8.29
(−23.1; −6.1)
−6.31
(−7.7; −6.0)

Table 3: Performance with constant β, exploring a range of possible values. The reported scores are the
median and interval deﬁned by the 5% and 95% percentile (in parenthesis) (see Section 4.1.2), over 35 runs
for each setting. The top table reports the coverage C, probability mass of Pd covered by the 5th percentile
of Pg deﬁned in Section 4.1.2. The bottom table reports the log likelihood of the true data under Pg.

29

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

0.96 (0.9; 1.0)

0.90 (0.5; 1.0)

0.65 (0.5; 1.0)

0.61 (0.5; 0.8)

0.69 (0.3; 0.8)

0.59 (0.3; 0.7)

Boosted (T=3)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.98 (0.9; 1.0)

0.93 (0.8; 1.0)

0.97 (0.8; 1.0)

0.87 (0.6; 1.0)

Boosted (T=10)

1.00 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

0.99 (0.9; 1.0)

0.99 (0.8; 1.0)

0.97 (0.8; 1.0)

TopKLast0.1 (T=3)

0.98 (0.9; 1.0)

0.93 (0.8; 1.0)

0.89 (0.6; 1.0)

0.72 (0.5; 1.0)

0.68 (0.5; 0.9)

0.51 (0.4; 0.7)

TopKLast0.1 (T=10)

0.99 (0.9; 1.0)

0.97 (0.8; 1.0)

0.90 (0.7; 1.0)

0.67 (0.4; 0.9)

0.61 (0.5; 0.8)

0.58 (0.4; 0.8)

TopKLast0.3 (T=3)

0.99 (0.9; 1.0)

0.97 (0.9; 1.0)

0.93 (0.7; 1.0)

0.81 (0.7; 1.0)

0.84 (0.7; 1.0)

0.78 (0.5; 1.0)

TopKLast0.3 (T=10)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.95 (0.7; 1.0)

0.94 (0.7; 1.0)

0.89 (0.7; 1.0)

0.88 (0.7; 1.0)

TopKLast0.5 (T=3)

0.98 (0.9; 1.0)

0.98 (0.9; 1.0)

0.95 (0.9; 1.0)

0.95 (0.8; 1.0)

0.86 (0.7; 1.0)

0.86 (0.6; 0.9)

TopKLast0.5 (T=10)

0.99 (1.0; 1.0)

0.98 (0.9; 1.0)

0.98 (1.0; 1.0)

0.99 (0.8; 1.0)

0.99 (0.8; 1.0)

1.00 (0.8; 1.0)

TopKLast0.7 (T=3)

0.98 (1.0; 1.0)

0.98 (0.9; 1.0)

0.94 (0.9; 1.0)

0.83 (0.7; 1.0)

0.87 (0.6; 1.0)

0.82 (0.7; 1.0)

TopKLast0.7 (T=10)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

0.98 (0.8; 1.0)

0.99 (0.9; 1.0)

0.95 (0.8; 1.0)

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

Boosted (T=3)

Boosted (T=10)

TopKLast0.1 (T=3)

TopKLast0.1 (T=10)

TopKLast0.3 (T=3)

TopKLast0.3 (T=10)

TopKLast0.5 (T=3)

TopKLast0.5 (T=10)

TopKLast0.7 (T=3)

TopKLast0.7 (T=10)

−4.94
(−5.5; −4.4)
−4.56
(−4.9; −4.4)
−4.56
(−4.7; −4.5)
−4.98
(−5.2; −4.7)
−4.98
(−5.3; −4.7)
−4.73
(−5.1; −4.5)
−4.62
(−4.8; −4.5)
−4.59
(−4.9; −4.4)
−4.59
(−4.8; −4.5)
−4.56
(−4.7; −4.4)
−4.52
(−4.7; −4.5)

−6.18
(−51.7; −5.6)
−5.55
(−5.9; −5.2)
−5.46
(−5.6; −5.3)
−5.64
(−6.1; −5.4)
−5.57
(−5.9; −5.3)
−5.48
(−6.0; −5.2)
−5.41
(−5.7; −5.2)
−5.29
(−5.7; −5.2)
−5.35
(−5.6; −5.2)
−5.37
(−5.5; −5.2)
−5.29
(−5.4; −5.2)

−31.85
(−100.3; −5.8)
−5.01
(−6.7; −4.7)
−5.08
(−5.8; −4.7)
−5.70
(−6.3; −5.2)
−5.37
(−6.0; −5.0)
−5.22
(−5.7; −4.8)
−4.90
(−5.2; −4.7)
−5.41
(−5.9; −4.9)
−5.12
(−5.5; −4.9)
−5.05
(−11.1; −4.7)
−5.05
(−6.6; −4.7)

−47.73
(−155.1; −14.2)
−5.49
(−18.7; −4.9)
−5.04
(−5.5; −4.6)
−5.39
(−38.4; −5.0)
−5.57
(−45.1; −4.7)
−5.42
(−21.6; −5.0)
−5.24
(−5.8; −4.9)
−5.48
(−18.5; −5.0)
−5.35
(−5.6; −4.8)
−5.63
(−43.1; −5.0)
−5.38
(−5.9; −5.1)

−107.36
(−390.8; −14.8)
−5.60
(−14.5; −5.0)
−5.51
(−5.9; −5.1)
−7.00
(−66.6; −5.4)
−7.34
(−16.1; −5.3)
−5.76
(−13.6; −5.1)
−5.71
(−6.2; −5.1)
−5.82
(−15.6; −5.2)
−5.34
(−5.8; −4.9)
−5.99
(−24.8; −5.4)
−5.77
(−6.3; −5.3)

−59.19
(−264.3; −18.8)
−6.86
(−47.3; −5.6)
−5.51
(−6.0; −5.2)
−12.70
(−44.2; −6.7)
−8.86
(−27.6; −5.5)
−7.26
(−36.2; −5.5)
−5.75
(−7.4; −5.1)
−6.78
(−18.7; −6.0)
−6.00
(−6.3; −5.6)
−7.76
(−25.2; −5.9)
−6.10
(−6.4; −6.0)

Table 4: Reweighting similar to “Cascade GAN” from [14], i.e. keep the top r fraction of examples, based on
the discriminator corresponding to the previous generator. The mixture weights are all equal (i.e. β = 1/t).
The reported scores are the median and interval deﬁned by the 5% and 95% percentile (in parenthesis) (see
Section 4.1.2), over 35 runs for each setting. The top table reports the coverage C, probability mass of Pd
covered by the 5th percentile of Pg deﬁned in Section 4.1.2. The bottom table reports the log likelihood of
the true data under Pg.

30

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

0.97 (0.9; 1.0)

0.77 (0.5; 1.0)

0.65 (0.5; 0.9)

0.70 (0.5; 0.8)

0.61 (0.5; 0.8)

0.58 (0.3; 0.8)

Boosted (T=3)

0.99 (1.0; 1.0)

0.99 (0.9; 1.0)

0.97 (0.9; 1.0)

0.95 (0.8; 1.0)

0.91 (0.8; 1.0)

0.89 (0.8; 1.0)

Boosted (T=10)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

TopK0.1 (T=3)

0.98 (0.9; 1.0)

0.98 (0.8; 1.0)

0.91 (0.7; 1.0)

0.84 (0.7; 1.0)

0.80 (0.5; 0.9)

0.60 (0.4; 0.7)

TopK0.1 (T=10)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

0.98 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

0.96 (0.8; 1.0)

TopK0.3 (T=3)

0.98 (0.9; 1.0)

0.98 (0.9; 1.0)

0.95 (0.9; 1.0)

0.95 (0.8; 1.0)

0.84 (0.6; 1.0)

0.79 (0.5; 1.0)

TopK0.3 (T=10)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

0.98 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

TopK0.5 (T=3)

0.99 (0.9; 1.0)

0.99 (1.0; 1.0)

0.96 (0.9; 1.0)

0.98 (0.8; 1.0)

0.88 (0.7; 1.0)

0.88 (0.6; 1.0)

TopK0.5 (T=10)

1.00 (1.0; 1.0)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

1.00 (1.0; 1.0)

TopK0.7 (T=3)

0.98 (1.0; 1.0)

0.98 (0.9; 1.0)

0.94 (0.8; 1.0)

0.84 (0.8; 1.0)

0.86 (0.7; 1.0)

0.81 (0.7; 1.0)

TopK0.7 (T=10)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

0.99 (1.0; 1.0)

1.00 (0.8; 1.0)

1.00 (0.9; 1.0)

1.00 (0.9; 1.0)

M odes : 1

M odes : 2

M odes : 3

M odes : 5

M odes : 7

M odes : 10

Vanilla

Boosted (T=3)

Boosted (T=10)

TopK0.1 (T=3)

TopK0.1 (T=10)

TopK0.3 (T=3)

TopK0.3 (T=10)

TopK0.5 (T=3)

TopK0.5 (T=10)

TopK0.7 (T=3)

TopK0.7 (T=10)

−4.61
(−5.5; −4.4)
−4.59
(−4.9; −4.4)
−4.61
(−4.7; −4.5)
−4.93
(−5.3; −4.7)
−4.60
(−4.8; −4.5)
−4.65
(−4.9; −4.4)
−4.56
(−4.8; −4.5)
−4.60
(−4.8; −4.5)
−4.59
(−4.7; −4.5)
−4.60
(−5.0; −4.4)
−4.59
(−4.7; −4.5)

−5.92
(−94.2; −5.2)
−5.32
(−5.7; −5.2)
−5.30
(−5.4; −5.2)
−5.85
(−6.2; −5.4)
−5.47
(−5.7; −5.3)
−5.40
(−5.9; −5.3)
−5.32
(−5.5; −5.2)
−5.34
(−5.6; −5.2)
−5.31
(−5.4; −5.2)
−5.44
(−5.6; −5.2)
−5.34
(−5.5; −5.2)

−12.40
(−53.1; −5.3)
−5.60
(−5.8; −5.5)
−5.48
(−5.6; −5.2)
−5.38
(−5.7; −5.0)
−4.81
(−5.1; −4.7)
−4.98
(−6.2; −4.7)
−5.07
(−5.9; −4.7)
−5.34
(−5.7; −5.0)
−5.13
(−5.5; −4.9)
−5.62
(−6.0; −5.4)
−5.51
(−5.6; −5.4)

−59.62
(−154.6; −9.8)
−5.40
(−24.2; −4.5)
−4.84
(−5.1; −4.3)
−5.34
(−5.8; −4.8)
−4.90
(−5.3; −4.2)
−5.25
(−11.4; −4.7)
−5.08
(−5.4; −4.5)
−5.42
(−19.0; −5.0)
−5.35
(−5.7; −4.8)
−5.49
(−22.2; −5.0)
−5.35
(−5.8; −5.0)

−66.95
(−191.5; −9.7)
−5.71
(−14.0; −5.1)
−5.25
(−5.9; −4.8)
−5.79
(−32.1; −5.2)
−4.85
(−5.6; −4.1)
−5.96
(−28.0; −5.5)
−5.16
(−5.9; −4.9)
−5.59
(−34.7; −4.9)
−5.33
(−5.8; −4.8)
−5.64
(−27.7; −5.3)
−5.32
(−6.0; −5.1)

−63.49
(−431.6; −14.5)
−6.96
(−17.1; −5.9)
−5.95
(−6.1; −5.5)
−7.09
(−20.7; −5.9)
−4.57
(−5.3; −4.2)
−7.34
(−25.4; −5.9)
−5.82
(−6.2; −5.3)
−6.15
(−14.8; −5.6)
−5.72
(−6.2; −5.3)
−7.17
(−22.5; −6.0)
−6.11
(−6.4; −5.9)

Table 5: Reweighting using the top r fraction of examples, based on the discriminator corresponding to the
mixture of all previous generators. The mixture weights are all equal (i.e. β = 1/t). The reported scores
are the median and interval deﬁned by the 5% and 95% percentile (in parenthesis) (see Section 4.1.2), over
35 runs for each setting. The top table reports the coverage C, probability mass of Pd covered by the 5th
percentile of Pg deﬁned in Section 4.1.2. The bottom table reports the log likelihood of the true data under
Pg.

31

