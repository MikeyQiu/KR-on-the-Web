Comment Ranking Diversiﬁcation in Forum Discussions

Curtis G. Northcutt, Kimberly A. Leon, Naichun Chen
Massachusetts Institute of Technology
Cambridge, MA, USA
{cgn, kimleon, naichun}@mit.edu

Its Scriptures, where increased visibility of the diversity of
comments across thousands of learners may aid in debunking
misconceptions held by the majority of forum respondents.
edX forums are organized hierarchically into topics > com-
ments > replies (an example topic is depicted in Figure 1).
Our focus is the ranking of comments and we use the number
of replies as the score for each comment, although by default,
edX comments are ranked chronologically.

0
2
0
2
 
b
e
F
 
7
2
 
 
]

Y
C
.
s
c
[
 
 
1
v
7
5
4
2
1
.
2
0
0
2
:
v
i
X
r
a

ABSTRACT
Viewing consumption of discussion forums with hundreds or
more comments depends on ranking because most users only
view top-ranked comments. When comments are ranked by
an ordered score (e.g. number of replies or up-votes) without
adjusting for semantic similarity of near-ranked comments,
top-ranked comments are more likely to emphasize the major-
ity opinion and incur redundancy. In this paper, we propose a
top K comment diversiﬁcation re-ranking model using Max-
imal Marginal Relevance (MMR) and evaluate its impact in
three categories: (1) semantic diversity, (2) inclusion of the
semantics of lower-ranked comments, and (3) redundancy,
within the context of a HarvardX course discussion forum. We
conducted a double-blind, small-scale evaluation experiment
requiring subjects to select between the top 5 comments of a
diversiﬁed ranking and a baseline ranking ordered by score.
For three subjects, across 100 trials, subjects selected the diver-
siﬁed (75% score, 25% diversiﬁcation) ranking as signiﬁcantly
(1) more diverse, (2) more inclusive, and (3) less redundant.
Within each category, inter-rater reliability showed moderate
consistency, with typical Cohen-Kappa scores near 0.2. Our
ﬁndings suggest that our model improves (1) diversiﬁcation,
(2) inclusion, and (3) redundancy, among top K ranked com-
ments in online discussion forums. Code is open-sourced at
https://github.com/cgnorthcutt/forum-diversification.

INTRODUCTION
Text ranking systems (e.g. Facebook post comments, Ama-
zon product reviews, Reddit forums) are ubiquitous, yet many
face a common problem. When posts (e.g. reviews or com-
ments) are ranked primarily by text content and rating (e.g.
like/unlike, ↑/↓, +/-, number of replies, etc.), similar posts tend
to receive similar scores. Moreover, higher ranking posts tend
to exclusively represent the majority opinion, since there are
more users in the majority group to up-vote posts sharing their
sentiment. For large forums with thousands of posts, view-
ers may only be exposed to the majority opinion when they
only view top-ranked posts. If the ground truth semantics of
each comment were known a priori, comment scores could be
normalized by the number of comments with similar seman-
tics, avoiding this problem. Unfortuantely this is not the case.
Instead, there are a multitude of techniques to approximate
semantic similarity [11, 7, 12].

We consider the comment ranking diversity problem in the con-
text of an online edX course, Harvardx Christianity Through

Published in L@S 2017, April 20 - 21, 2017, Cambridge, MA, USA
DOI: http://dx.doi.org/10.1145/3051457.3054016

Figure 1. An example topic used to illustrate the organization of an edX
discussion forum. edX forums are organized hierarchically into topics >
comments > replies. Our focus is the ranking of comments.

In this paper, we develop an algorithm for forum comment
ranking diversiﬁcation using maximal marginal relevance
(MMR) to linearly interpolate between the original relevance
ranking score of a comment and the diversity of a comment
with other high-ranked comments. We operationalize our no-
tion of diversity using a PCA + TFIDF model on all comments
and evaluate our model using a blind experiment requiring sub-
jects to compare our diversiﬁed ranking to a baseline relevance
ranking.

BACKGROUND AND RELATED WORK
The crux of diversiﬁcation is a well-trained comment embed-
ding model that accurately captures the semantic similarity
between two documents. Text embedding is a well-studied
problem at the word-level [11] and document-level [9]. In this
section, we consider increasingly complex methods for com-
ment similarity, followed by methods for ranking documents
and how it relates to diversiﬁcation.

One of the simplest document embedding representations is
TFIDF [14] which uses a "bag of words" (nBOW) counts

model, normalized by word count per document frequency.
Although TFIDF works well on some tasks [1], it ignores word
ordering and suffers a performance loss for longer documents.
TFIDF performs well when combined with matrix decomposi-
tion methods like PCA or LSA. More sophisticated approaches
such as word2vec [11], LDA [2], and Gated CNN [10] offer
classiﬁcation accuracy improvements, but are task-speciﬁc.
These models are compared in Table 1. A state-of-the-art
(2016) LSTM similarity model uses a Siamese recurrent archi-
tecture to combine the word2vec embeddings of all words in a
document, and trains using a Manhattan loss on the output of
the two LSTMs [12]. Although this method would likely offer
improvements, simpler models were sufﬁcient for our task.

Table 1. A comparison of the comment embedding models evaluated
in this study. Method symbols are abbreviated as: T=Topic, M=Matrix
Factorization, W=Local Window, F=Frequency, S=Semantic

Model

Method Scaling Sensitivity

F
TFIDF
M+S
PCA + TFIDF
M+S
LSA + TFIDF
M+S
NMF + TFIDF
T
LDA + TFIDF
W+S
Word2Vec + TFIDF
W+S
Word2Vec + nBOW
Gated CNN + TFIDF W+S

False
True
True
True
False
False
False
False

The task of forum comment ranking can be thought of as a
search task, where common methods like PageRank [13] and
RankSVM [6]) are used to identify the most relevant document
for a given query. In our case, relevance is determined a priori
by comment score, and instead our focus is diversiﬁcation of
this ranking. Diversiﬁcation has been successfully applied
to the task of online shopping [4], with the task of reducing
abandonment in shopping queries by providing a diversiﬁed
selection of options. In this paper, we elect a more general
approach, MMR [3], which we describe more in Section 3.2.

TECHNICAL APPROACH
Our methodology consists of four ordered components: (1)
Automated generation of gold data, (2) Evaluation of comment
embedding models, (3) Implementing diversiﬁcation in com-
ment rankings, and (4) Measuring efﬁcacy of diversiﬁcation.
We describe these components in the following sections.

Dataset
edX forums are organized hierarchically by topic > comments
> replies as shown in Figure 1. We consider diversiﬁcation at
the comments level (within a single topic). In the context of
this study, we focus on the comment rankings for topics in the
forum discussions of an edX course, HarvardX: HDS3221.2x
Christianity Through Its Scriptures, obtained via web-scraping.
Comment scores were set equal to the number of replies for
each comment. Forum text was tokenized with stop-words
removed and over 100,000 comments were analyzed.

Automated Gold Data Generation
We used a novel method to generate large gold datasets, with-
out human labeling, by sampling comments across highly
differing topics and generating a pairwise cosine similarity

matrix for these comments. This matrix contains binary labels,
a (1) if comments were taken from the same topic (Gold 1
pairs) or (0) if comments were taken from different topics
(Gold 0 pairs). For exclusive sets of topics, we generated
both train and test gold datasets to evaluate our selection of
different comment embedding models discussed in 4.1.

Maximal-Marginal Relevance (MMR)
MMR is an iterative algorithm, at each step selecting the
comment which maximizes a modiﬁed score (Equation 1).

ˆs := λ · s − (1 − λ ) · c

(1)

A single parameter λ adjusts the trade-off between the orig-
inal comment score, s, and its maximum cosine similarity
among all comments that have already been added to the new
ranking, c, to produce the updated score, s(cid:48). For example,
λ = 1 ranks entirely by score and λ = 0 selects maximally
diverse comments irrespective of score. In this study, we eval-
uate two settings of the parameter, λ = 0.75 and λ = 0.25 in
comparison with a baseline where λ = 1.

Comment Embedding Model Selection
Diversiﬁcation with MMR hinges on a comment embedding
model that accurately captures the semantic similarity between
two comments. Eight models were evaluated (Table 1).

Two evaluation metrics were used to compare these models.
(1) The median quantile difference deﬁned as the difference in
average cosine similarity percentile rank (quantile) of Gold 1
pairs minus that of Gold 0 pairs. We recommend this metric as
it is unbiased and captures relative ranking. (2) The accuracy
of logistic regression using a given model’s pairwise comment
cosine similarity matrix as input and the gold binary labels as
output. Our two metrics consistently ranked all models.

Using the best performing model for these two metrics, com-
ment similarity was computed using cosine similarity [8]. In
our case, the best model was PCA + TFIDF comment embed-
dings, as seen in Table 2 in the Results section.

MMR Evaluation Experiment
Since the comment order for the course we experimented on is
chronological, we used ordering by score (number of replies,
λ = 1) for our baseline ranking. We conducted a small-scale
re-ranking evaluation experiment requiring subjects to choose
among two unidentiﬁed ordered lists of comments: (1) the
top 5 comments of our diversiﬁed ranking and (2) the top 5
comments of a baseline ranking ordered only by score, their
true identities unknown. Three subjects evaluated 100 trials.
The Cohen-Kappa score [5] was used to measure inter-rater
reliability. For each trial, subjects were presented with three
tasks (an example trial is shown in Figure 2):

1. The forum’s topic question

2. Two lists, A and B. One of these lists is the top ﬁve com-
ments ordered by score (baseline). The other is the top ﬁve
diversiﬁed (re-ranked) comments

3. A random comment C from this forum not included in (2)
where C’s probability of being chosen was proportional to
number of replies (higher rank = more likely to be chosen).

Embedding
Method

TFIDF
PCA + TFIDF
LSA + TFIDF
NMF + TFIDF
LDA + TFIDF
Word2Vec + TFIDF
Word2Vec + nBOW
Gated CNN + TFIDF

Median
Quantile
Difference
0.338
0.434
0.431
0.416
0.129
0.205
0.167
0.116

Logistic
Regression
Accuracy
0.841
0.867
0.867
0.861
0.815
0.815
0.815
0.786

Table 2. Comparison of various comment embedding methods. Median
quantile difference computes the difference in average cosine similarity
rank (percentile) of Gold 1 pairs - Gold 0 pairs. Logistic regression pre-
dicts the accuracy of the gold labels trained using each model’s pairwise
cosine similarity matrix as input.

Comparing the use of the TFIDF embedding to the use of
PCA and LSA afﬁrms that there is beneﬁt to employing dense
embeddings. More unexpectedly, word2vec and Gated CNN,
when combined with TFIDF, did not perform as well as TFIDF.
A likely suspect is that our word2vec model was trained on
the Google News corpus, which is a semantically different
and much broader corpus than learner comments in an online
course. As a result, word embeddings related to the course
content were compressed into a smaller space relative to the
broader embeddings of the model.

Given that comments were on average 78 words in length, and
"bag of words" ignores ordering and contextual information,
it is less surprising that PCA and LSA outperformed nBOW
and TFIDF models. As PCA offered a marginal performance
improvement over LSA, PCA + TFIDF was chosen as our
ﬁnal comment embedding model.

MMR Evaluation
Table 3 lists the results of the blind evaluation experiment. The
fraction of subject responses selecting the diversiﬁed (MMR)
ranking is depicted in Figure 3. The MMR ranking with
λ = 0.75 (ranked more by score) outperformed the baseline
in every experiment (experiments are described in 3.3), while
rankings with λ = 0.25 (ranked more by diversity) did not
perform signiﬁcantly better or worse than the baseline.

Experiment Trials

Baseline
Trials

λ

0.25

0.75

inclusion
diverse
redundant
inclusion
diverse
redundant

MMR
Trials
0.48
0.54
0.49
0.68
0.63
0.39

0.52
0.46
0.51
0.32
0.37
0.61

225
225
225
75
75
75

Table 3. Depicts the aggregated subject counts of the blind evaluation
experiment. For each (λ , experiment) group, the number of times either
list was chosen is tallied. The two rightmost columns capture the nor-
malized counts. The baseline ranking is generated with MMR and λ = 1
(ranked only by score).

For moderate diversiﬁcation (λ = 0.75), the MMR ranking
was chosen signiﬁcantly more often than the baseline ranking

Figure 2. Example of a single trial in the MMR evaluation experiment.
Each trial was presented to human subjects.

Both the order in which lists A and B were shown to subjects
and trial order were randomized to ensure the true labels for
list A and B were unrecoverable within and across subjects.
For each double-blind trial, each subject answered 3 questions:

1. Inclusion Experiment: Which list, A or B, has a comment
that resembles the semantics of comment C?

2. Diversity Experiment: Which list, A or B, best captures a
diverse set of all potential answers to this question Q?

3. Redundancy Experiment: Which list, A or B, contains
more redundant comments?

If our comment embedding model accurately captures pairwise
semantic similarity, we would expect the diversiﬁed ranking
to be chosen more often for "inclusion" and "diversity", and
less often for "redundancy".

Among the 100 trials for each subject, 75 trials used λ = 0.25
(ranked more by diversity) and 25 trials used λ = 0.75 (ranked
more by score). More trials were taken for λ = 0.25 to offset
increased stochasticity when selecting low-scored (but diverse)
comments. Neglecting comment score increases variation in
ranking. Additional trials mitigated increased variance.

RESULTS AND DISCUSSION
This section is divided into two parts. Since diversiﬁcation
relies on accurate semantic similarity scores, in Section 4.1
we evaluate comment embedding models on our gold dataset.
Then, in Section 4.2, we evaluate our model in a double-blind
subject experiment comparing our diversiﬁed ranking against
a baseline ranking ordered by score.

Comment Embedding Models
For our task, word-level comment embedding methods
(word2vec, Gated CNN, LDA) performed worse than a simple
TFIDF vector representation alone, with a classical application
of dimensionality reduction using PCA achieving highest ac-
curacy on our gold dataset. Table 2 captures the performance
of different embedding models on our gold test set, for both
median quantile difference and logistic regression accuracy.
In the rest of this section, we discuss potential reasons for this.

CONCLUSION
Discussion forums play a vital role in human interactions with
online forums, yet due to large scale, comment rankings often
suffer from majority biases and redundancy. The primary
contributions of this paper are (1) design and evaluation of
a top K comment diversiﬁcation re-ranking algorithm and
(2) experimental evidence suggesting a signiﬁcant increase in
diversity and inclusion and decrease in redundancy when our
algorithm is used to rank comments versus a baseline relevance
ranking. We encourage large-scale commenting platforms, e.g.
Facebook, edX, Reddit, etc., to consider the importance of
ranking diversiﬁcation on learning and user experience, and
hope our ﬁndings inspire future consideration.

ACKNOWLEDGEMENTS
We graciously thank the reviewers who assisted in improving
the manuscript, Y-Lan Boureau (Facebook AI Research) for
her mentorship and suggestion of diversiﬁcation using MMR
with embeddings, and Regina Barzilay (MIT) for her guidance
in model selection and framework.

REFERENCES

1. Akiko Aizawa. 2003. An information-theoretic

perspective of tf–idf measures. Information Processing &
Management 39, 1 (2003), 45–65.

2. David M Blei, Andrew Y Ng, and Michael I Jordan. 2003.
Latent dirichlet allocation. Journal of Machine Learning
Research 3, Jan (2003), 993–1022.

3. Jaime Carbonell and Jade Goldstein. 1998. The use of

MMR, diversity-based reranking for reordering
documents and producing summaries. In Proceedings of
the 21st ACM SIGIR Conference on Research and
Development in Information Retrieval. 335–336.

4. Olivier Chapelle, Shihao Ji, Ciya Liao, Emre

Velipasaoglu, Larry Lai, and Su-Lin Wu. 2011.
Intent-based diversiﬁcation of web search results: metrics
and algorithms. Information Retrieval 14, 6 (2011),
572–592.

5. J. Cohen. 1960. A Coefﬁcient of Agreement for Nominal
Scales. Educ. and Psych. Measurement 20, 1 (1960), 37.

6. Yajuan Duan, Long Jiang, Tao Qin, Ming Zhou, and
Heung-Yeung Shum. 2010. An empirical study on
learning to rank of tweets. In Proceedings of the 23rd
International Conference on Computational Linguistics.
Association for Computational Linguistics, 295–303.

7. Susan T Dumais, George W Furnas, Thomas K Landauer,
Scott Deerwester, and Richard Harshman. 1988. Using
latent semantic analysis to improve access to textual
information. In Proceedings of the SIGCHI Conference
on Human Factors in Computing Systems. ACM,
281–285.

8. Anna Huang. 2008. Similarity measures for text

document clustering. In Proceedings of the 6th New
Zealand Computer Science Research Conference. 49–56.

Figure 3. Depicts the fraction of trials choosing the diversiﬁed (MMR)
ranking for each λ , experiment pair. Higher values for the "diverse"
and "inclusion" experiments and lower values for the "redundant" ex-
periment suggest MMR’s efﬁcacy, with λ = 0.75 outperforming λ = 0.25.
The large, encircled points depict the means of each λ , experiment pair
and the translucent bars depict the standard error of each mean. The
smaller points depict individual rater scores.

for both diversity and inclusion experiments, and signiﬁcantly
less often than the baseline for the redundancy experiment, sug-
gesting our model mitigates redundancy and majority biases
in the top K comments. However, for extreme diversiﬁca-
tion (λ = 0.25) the fraction of responses choosing the MMR
ranking was nearly 0.5 (completely random when compared
with the baseline ranking) across all three experiment groups.
The cause is likely two fold. Firstly, ranking correlates with
relevance, therefore, replacing more high-ranking comments
with diverse, but lower-ranked (and less relevant) comments,
may negatively impact all three experiments. Secondly, lower-
ranked comments may be off-topic, lower quality, or harder to
parse, leading to a simulated random choice.

Reliability and Agreement Among Test Subjects
Since only three subjects were included in our experiment,
each evaluating 100 trials, we consider the inter-rater reliability
among the three subjects to validate the consistency in our
ﬁndings. Table 4 lists the Cohen’s Kappa score for all pairs of
subjects, for each experiment group. Although a small number
of pairs were inconsistent, most showed moderate consistency.

diversity

inclusion

redundancy

other1

other2

subject 1
subject 2
subject 3

subject 1
subject 2
subject 3

subject 1
subject 2
subject 3

-0.011
0.179
-0.011

0.034
0.185
0.034

-0.026
0.211
-0.026

0.274
0.274
0.179

0.147
0.147
0.185

0.136
0.136
0.211

Table 4. Cohen’s Kappa pairwise inter-rater reliability scores.

9. Quoc V Le and Tomas Mikolov. 2014. Distributed
Representations of Sentences and Documents.. In
International Conference of Machine Learning (ICML),
Vol. 14. 1188–1196.

10. Tao Lei, Hrishikesh Joshi, Regina Barzilay, Tommi

Jaakkola, Katerina Tymoshenko, Alessandro Moschitti,
and Lluis Marquez. 2016. Semi-supervised question
retrieval with gated convolutions. In 2016 Conference of
the NAACL Human Language Technologies. ACL,
1279–1289.

11. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S

Corrado, and Jeff Dean. 2013. Distributed representations

of words and phrases and their compositionality. In Adv.
in neural information processing systems. 3111–3119.

12. Jonas Mueller and Aditya Thyagarajan. 2016. Siamese

Recurrent Architectures for Learning Sentence Similarity.
In Thirtieth AAAI Conference on Artiﬁcial Intelligence.

13. Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry

Winograd. 1999. The PageRank citation ranking:
bringing order to the web. (1999).

14. Ho Chung Wu, Robert Wing Pong Luk, Kam Fai Wong,
and Kui Lam Kwok. 2008. Interpreting TF-IDF Term
Weights as Making Relevance Decisions. ACM
Transactions on Information Systems 26, 3 (2008).

