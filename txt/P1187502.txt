8
1
0
2
 
c
e
D
 
7
1
 
 
]
L
C
.
s
c
[
 
 
3
v
7
4
8
7
0
.
4
0
8
1
:
v
i
X
r
a

Joint entity recognition and relation extraction as a
multi-head selection problem

Giannis Bekoulis∗, Johannes Deleu, Thomas Demeester, Chris Develder

Ghent University – imec, IDLab, Department of Information Technology,
Technologiepark Zwijnaarde 15, 9052 Ghent, Belgium

Abstract

State-of-the-art models for joint entity recognition and relation extraction strongly rely

on external natural language processing (NLP) tools such as POS (part-of-speech) tag-

gers and dependency parsers. Thus, the performance of such joint models depends on

the quality of the features obtained from these NLP tools. However, these features

are not always accurate for various languages and contexts.

In this paper, we pro-

pose a joint neural model which performs entity recognition and relation extraction

simultaneously, without the need of any manually extracted features or the use of any

external tool. Speciﬁcally, we model the entity recognition task using a CRF (Condi-

tional Random Fields) layer and the relation extraction task as a multi-head selection

problem (i.e., potentially identify multiple relations for each entity). We present an

extensive experimental setup, to demonstrate the effectiveness of our method using

datasets from various contexts (i.e., news, biomedical, real estate) and languages (i.e.,

English, Dutch). Our model outperforms the previous neural models that use automat-

ically extracted features, while it performs within a reasonable margin of feature-based

neural models, or even beats them.

sequence labeling

Keywords: entity recognition, relation extraction, multi-head selection, joint model,

∗Corresponding author
Email addresses: giannis.bekoulis@ugent.be (Giannis Bekoulis),

johannes.deleu@ugent.be (Johannes Deleu), thomas.demeester@ugent.be (Thomas
Demeester), chris.develder@ugent.be (Chris Develder)

1. Introduction

answering.

The goal of the entity recognition and relation extraction is to discover relational struc-

tures of entity mentions from unstructured texts. It is a central problem in information

extraction since it is critical for tasks such as knowledge base population and question

The problem is traditionally approached as two separate subtasks, namely (i) named

entity recognition (NER) (Nadeau & Sekine, 2007) and (ii) relation extraction (RE) (Bach

& Badaskar, 2007), in a pipeline setting. The main limitations of the pipeline models

are: (i) error propagation between the components (i.e., NER and RE) and (ii) possible

useful information from the one task is not exploited by the other (e.g., identifying a

Works for relation might be helpful for the NER module in detecting the type of the

two entities, i.e., PER, ORG and vice versa). On the other hand, more recent studies

propose to use joint models to detect entities and their relations overcoming the afore-

mentioned issues and achieving state-of-the-art performance (Li & Ji, 2014; Miwa &

Sasaki, 2014).

The previous joint models heavily rely on hand-crafted features. Recent advances

in neural networks alleviate the issue of manual feature engineering, but some of them

still depend on NLP tools (e.g., POS taggers, dependency parsers). Miwa & Bansal

(2016) propose a Recurrent Neural Network (RNN)-based joint model that uses a bidi-

rectional sequential LSTM (Long Short Term Memory) to model the entities and a

tree-LSTM that takes into account dependency tree information to model the relations

between the entities. The dependency information is extracted using an external depen-

dency parser. Similarly, in the work of Li et al. (2017) for entity and relation extraction

from biomedical text, a model which also uses tree-LSTMs is applied to extract de-

pendency information. Gupta et al. (2016) propose a method that relies on RNNs but

uses a lot of hand-crafted features and additional NLP tools to extract features such as

POS-tags, etc. Adel & Sch¨utze (2017) replicate the context around the entities with

Convolutional Neural Networks (CNNs). Note that the aforementioned works exam-

ine pairs of entities for relation extraction, rather than modeling the whole sentence

directly. This means that relations of other pairs of entities in the same sentence —

2

which could be helpful in deciding on the relation type for a particular pair — are not

taken into account. Katiyar & Cardie (2017) propose a neural joint model based on

LSTMs where they model the whole sentence at once, but still they do not have a prin-

cipled way to deal with multiple relations. Bekoulis et al. (2018) introduce a quadratic

scoring layer to model the two tasks simultaneously. The limitation of this approach is

that only a single relation can be assigned to a token, while the time complexity for the

entity recognition task is increased compared to the standard approaches with linear

complexity.

In this work, we focus on a new general purpose joint model that performs the

two tasks of entity recognition and relation extraction simultaneously, and that can

handle multiple relations together. Our model achieves state-of-the-art performance in

a number of different contexts (i.e., news, biomedical, real estate) and languages (i.e.,

English, Dutch) without relying on any manually engineered features nor additional

NLP tools. In summary, our proposed model (which will be detailed next in Section 3)

solves several shortcomings that we identiﬁed in related works (Section 2) for joint

entity recognition and relation extraction: (i) our model does not rely on external NLP

tools nor hand-crafted features, (ii) entities and relations within the same text fragment

(typically a sentence) are extracted simultaneously, where (iii) an entity can be involved

in multiple relations at once.

Speciﬁcally, the model of Miwa & Bansal (2016) depends on dependency parsers,

which perform particularly well on speciﬁc languages (i.e., English) and contexts (i.e.,

news). Yet, our ambition is to develop a model that generalizes well in various setups,

therefore using only automatically extracted features that are learned during training.

For instance, Miwa & Bansal (2016) and Li et al. (2017) use exactly the same model in

different contexts, i.e., news (ACE04) and biomedical data (ADE), respectively. Com-

paring our results to the ADE dataset, we obtain a 1.8% improvement on the NER task

and ∼3% on the RE task. On the other hand, our model performs within a reasonable

margin (∼0.6% in the NER task and ∼1% on the RE task) on the ACE04 dataset with-

out the use of pre-calculated features. This shows that the model of Miwa & Bansal

(2016) strongly relies on the features extracted by the dependency parsers and can-

not generalize well into different contexts where dependency parser features are weak.

3

Comparing to Adel & Sch¨utze (2017), we train our model by modeling all the entities

and the relations of the sentence at once. This type of inference is beneﬁcial in ob-

taining information about neighboring entities and relations instead of just examining

a pair of entities each time. Finally, we solve the underlying problem of the models

proposed by Katiyar & Cardie (2017) and Bekoulis et al. (2017), who essentially as-

sume classes (i.e., relations) to be mutually exclusive: we solve this by phrasing the

relation extraction component as a multi-label prediction problem.1

To demonstrate the effectiveness of the proposed method, we conduct the largest

experimental evaluation to date (to the best of our knowledge) in jointly performing

both entity recognition and relation extraction (see Section 4 and Section 5), using dif-

ferent datasets from various domains (i.e., news, biomedical, real estate) and languages

(i.e., English, Dutch). Speciﬁcally, we apply our method to four datasets, namely

ACE04 (news), Adverse Drug Events (ADE), Dutch Real Estate Classiﬁeds (DREC)

and CoNLL’04 (news). Our method outperforms all state-of-the-art methods that do

not rely on any additional features or tools, while performance is very close (or even

better in the biomedical dataset) compared to methods that do exploit hand-engineered

features or NLP tools.

2. Related work

The tasks of entity recognition and relation extraction can be applied either one by one

in a pipeline setting (Fundel et al., 2007; Gurulingappa et al., 2012a; Bekoulis et al.,

2017) or in a joint model (Miwa & Sasaki, 2014; Miwa & Bansal, 2016; Bekoulis

et al., 2018). In this section, we present related work for each task (i.e., named entity

recognition and relation extraction) as well as prior work into joint entity and relation

extraction.

1Note that another difference is that we use a CRF layer for the NER part, while Katiyar & Cardie
(2017) uses a softmax and Bekoulis et al. (2017) uses a quadratic scoring layer; see further, when we discuss
performance comparison results in Section 5.

4

2.1. Named entity recognition

In our work, NER is the ﬁrst task which we solve in order to address the end-to-end

relation extraction problem. A number of different methods for the NER task that

are based on hand-crafted features have been proposed, such as CRFs (Lafferty et al.,

2001), Maximum Margin Markov Networks (Taskar et al., 2003) and support vector

machines (SVMs) for structured output (Tsochantaridis et al., 2004), to name just a

few. Recently, deep learning methods such as CNN- and RNN-based models have

been combined with CRF loss functions (Collobert et al., 2011; Huang et al., 2015;

Lample et al., 2016; Ma & Hovy, 2016) for NER. These methods achieve state-of-the-

art performance on publicly available NER datasets without relying on hand-crafted

features.

2.2. Relation extraction

We consider relation extraction as the second task of our joint model. The main ap-

proaches for relation extraction rely either on hand-crafted features (Zelenko et al.,

2003; Kambhatla, 2004) or neural networks (Socher et al., 2012; Zeng et al., 2014).

Feature-based methods focus on obtaining effective hand-crafted features, for instance

deﬁning kernel functions (Zelenko et al., 2003; Culotta & Sorensen, 2004) and design-

ing lexical, syntactic, semantic features, etc. (Kambhatla, 2004; Rink & Harabagiu,

2010). Neural network models have been proposed to overcome the issue of manually

designing hand-crafted features leading to improved performance. CNN- (Zeng et al.,

2014; Xu et al., 2015a; dos Santos et al., 2015) and RNN-based (Socher et al., 2013;

Zhang & Wang, 2015; Xu et al., 2015b) models have been introduced to automatically

extract lexical and sentence level features leading to a deeper language understand-

ing. Vu et al. (2016) combine CNNs and RNNs using an ensemble scheme to achieve

state-of-the-art results.

2.3. Joint entity and relation extraction

Entity and relation extraction includes the task of (i) identifying the entities (described

in Section 2.1) and (ii) extracting the relations among them (described in Section 2.2).

Feature-based joint models (Kate & Mooney, 2010; Yang & Cardie, 2013; Li & Ji,

5

2014; Miwa & Sasaki, 2014) have been proposed to simultaneously solve the entity

recognition and relation extraction (RE) subtasks. These methods rely on the availabil-

ity of NLP tools (e.g., POS taggers) or manually designed features and thus (i) require

additional effort for the data preprocessing, (ii) perform poorly in different applica-

tion and language settings where the NLP tools are not reliable, and (iii) increase the

computational complexity. In this paper, we introduce a joint neural network model to

overcome the aforementioned issues and to automatically perform end-to-end relation

extraction without the need of any manual feature engineering or the use of additional

NLP components.

Neural network approaches have been considered to address the problem in a joint

setting (end-to-end relation extraction) and typically include the use of RNNs and

CNNs (Miwa & Bansal, 2016; Zheng et al., 2017; Li et al., 2017). Speciﬁcally, Miwa

& Bansal (2016) propose the use of bidirectional tree-structured RNNs to capture de-

pendency tree information (where parse trees are extracted using state-of-the-art de-

pendency parsers) which has been proven beneﬁcial for relation extraction (Xu et al.,

2015a,b). Li et al. (2017) apply the work of Miwa & Bansal (2016) to biomedical

text, reporting state-of-the-art performance for two biomedical datasets. Gupta et al.

(2016) propose the use of a lot of hand-crafted features along with RNNs. Adel &

Sch¨utze (2017) solve the entity classiﬁcation task (which is different from NER since

in entity classiﬁcation the boundaries of the entities are known and only the type of the

entity should be predicted) and relation extraction problems using an approximation of

a global normalization objective (i.e., CRF): they replicate the context of the sentence

(left and right part of the entities) to feed one entity pair at a time to a CNN for relation

extraction. Thus, they do not simultaneously infer other potential entities and relations

within the same sentence. Katiyar & Cardie (2017) and Bekoulis et al. (2018) inves-

tigate RNNs with attention for extracting relations between entity mentions without

using any dependency parse tree features. Different from Katiyar & Cardie (2017), in

this work, we frame the problem as a multi-head selection problem by using a sigmoid

loss to obtain multiple relations and a CRF loss for the NER component. This way,

we are able to independently predict classes that are not mutually exclusive, instead of

assigning equal probability values among the tokens. We overcome the issue of addi-

6

Figure 1: The multi-head selection model for joint entity and relation extraction. The input of our model is
the words of the sentence which are then represented as word vectors (i.e., embeddings). The BiLSTM layer
extracts a more complex representation for each word. Then the CRF and the sigmoid layers are able to
produce the outputs for the two tasks. The outputs for each token (e.g., Smith) are: (i) an entity recognition
label (e.g., I-PER) and (ii) a set of tuples comprising the head tokens of the entity and the types of relations
between them (e.g., {(Center, Works for), (Atlanta, Lives in)}).

tional complexity described by Bekoulis et al. (2018), by dividing the loss functions

into a NER and a relation extraction component. Moreover, we are able to handle mul-

tiple relations instead of just predicting single ones, as was described for the application

of structured real estate advertisements of Bekoulis et al. (2018).

3. Joint model

In this section, we present our multi-head joint model illustrated in Fig. 1. The model

is able to simultaneously identify the entities (i.e., types and boundaries) and all the

possible relations between them at once. We formulate the problem as a multi-head

selection problem extending previous work (Zhang et al., 2017; Bekoulis et al., 2018)

as described in Section 2.3. By multi-head, we mean that any particular entity may be

involved in multiple relations with other entities. The basic layers of the model, shown

in Fig. 1, are: (i) embedding layer, (ii) bidirectional sequential LSTM (BiLSTM) layer,

(iii) CRF layer and the (iv) sigmoid scoring layer. In Fig. 1, an example sentence from

7

Figure 2: Embedding layer in detail. The characters of the word “Man” are represented by character vectors
(i.e., embeddings) that are learned during training. The character embeddings are fed to a BiLSTM and the
two ﬁnal states (forward and backward) are concatenated. The vector wchars is the character-level repre-
sentation of the word. This vector is then further concatenated to the word-level representation wword2vec
to obtain the complete word embedding vector.

the CoNLL04 dataset is presented. The input of our model is a sequence of tokens

(i.e., words of the sentence) which are then represented as word vectors (i.e., word

embeddings). The BiLSTM layer is able to extract a more complex representation for

each word that incorporates the context via the RNN structure. Then the CRF and the

sigmoid layers are able to produce the outputs for the two tasks. The outputs for each

token (e.g., Smith) are twofold: (i) an entity recognition label (e.g., I-PER, denoting

the token is inside a named entity of type PER) and (ii) a set of tuples comprising the

head tokens of the entity and the types of relations between them (e.g., {(Center, Works

for), (Atlanta, Lives in)}). Since we assume token-based encoding, we consider only

the last token of the entity as head of another token, eliminating redundant relations.

For instance, there is a Works for relation between entities “John Smith” and “Disease

Control Center”.

Instead of connecting all tokens of the entities, we connect only

“Smith” with “Center”. Also, for the case of no relation, we introduce the “N” label

and we predict the token itself as the head.

3.1. Embedding layer

Given a sentence w = w1, ..., wn as a sequence of tokens, the word embedding layer

is responsible to map each token to a word vector (wword2vec). We use pre-trained word

embeddings using the Skip-Gram word2vec model (Mikolov et al., 2013).

8

In this work, we also use character embeddings since they are commonly applied

to neural NER (Ma & Hovy, 2016; Lample et al., 2016). This type of embeddings is

able to capture morphological features such as preﬁxes and sufﬁxes. For instance, in

the Adverse Drug Events (ADE) dataset, the sufﬁx “toxicity” can specify an adverse

drug event entity such as “neurotoxicity” or “hepatotoxicity” and thus it is very infor-

mative. Another example might be the Dutch sufﬁx “kamer” (“room” in English) in the

Dutch Real Estate Classiﬁeds (DREC) dataset which is used to specify the space en-

tities “badkamer” (“bathroom” in English) and “slaapkamer” (“bedroom” in English).

Character-level embeddings are learned during training, similar to Ma & Hovy (2016)

and Lample et al. (2016). In the work of Lample et al. (2016), character embeddings

lead to a performance improvement of up to 3% in terms of NER F1 score. In our

work, by incorporating character embeddings, we report in Table 2 an increase of ∼2%

overall F1 scoring points. For more details, see Section 5.2.

Figure 2 illustrates the neural architecture for word embedding generation based

on its characters. The characters of each word are represented by character vectors

(i.e., embeddings). The character embeddings are fed to a BiLSTM and the two ﬁnal

states (forward and backward) are concatenated. The vector wchars is the character-level

representation of the word. This vector is then further concatenated to the word-level

representation wword2vec to obtain the complete word embedding vector.

3.2. Bidirectional LSTM encoding layer

RNNs are commonly used in modeling sequential data and have been successfully

applied in various NLP tasks (Sutskever et al., 2014; Lample et al., 2016; Miwa &

Bansal, 2016). In this work, we use multi-layer LSTMs, a speciﬁc kind of RNNs which

are able to capture long term dependencies well (Bengio et al., 1994; Pascanu et al.,

2013). We employ a BiLSTM which is able to encode information from left to right

(past to future) and right to left (future to past). This way, we can combine bidirectional
information for each word by concatenating the forward ( (cid:126)hi) and the backward ( (cid:126)hi)

output at timestep i. The BiLSTM output at timestep i can be written as:

hi = [ (cid:126)hi; (cid:126)hi],

i = 0, ..., n

(1)

9

3.3. Named entity recognition

We formulate the entity identiﬁcation task as a sequence labeling problem, similar to

previous work on joint learning models (Miwa & Bansal, 2016; Li et al., 2017; Katiyar

& Cardie, 2017) and named entity recognition (Lample et al., 2016; Ma & Hovy, 2016)

using the BIO (Beginning, Inside, Outside) encoding scheme. Each entity consists of

multiple sequential tokens within the sentence and we should assign a tag for every

token in the sentence. That way we are able to identify the entity arguments (start and

end position) and its type (e.g., ORG). To do so, we assign the B-type (beginning) to

the ﬁrst token of the entity, the I-type (inside) to every other token within the entity

and the O tag (outside) if a token is not part of an entity. Fig. 1 shows an example

of the BIO encoding tags assigned to the tokens of the sentence. In the CRF layer,

one can observe that we assign the B-ORG and I-ORG tags to indicate the beginning

and the inside tokens of the entity “Disease Control Center”, respectively. On top of

the BiLSTM layer, we employ either a softmax or a CRF layer to calculate the most

probable entity tag for each token. We calculate the score of each token wi for each

entity tag:

s(e)(hi) = V (e)f (U (e)hi + b(e))

(2)

where the superscript (e) is used for the notation of the NER task, f (·) is an element-
wise activation function (i.e., relu, tanh), V (e) ∈ Rp×l, U (e) ∈ Rl×2d, b(e) ∈ Rl,

with d as the hidden size of the LSTM, p the number of NER tags (e.g., B-ORG) and

l the layer width. We calculate the probabilities of all the candidate tags for a given
token wi as Pr(tag | wi) = softmax(s(hi)) where Pr(tag | wi) ∈ Rp. In this work,

we employ the softmax approach only for the entity classiﬁcation (EC) task (which is

similar to NER) where we need to predict only the entity types (e.g., PER) for each

token assuming boundaries are given. The CRF approach is used for the NER task

which includes both entity type and boundaries recognition.

In the softmax approach, we assign entity types to tokens in a greedy way at pre-

diction time (i.e., the selected tag is just the highest scoring tag over all possible set

of tags). Although assuming an independent tag distribution is beneﬁcial for entity

classiﬁcation tasks (e.g., POS tagging), this is not the case when there are strong de-

10

pendencies between the tags. Speciﬁcally, in NER, the BIO tagging scheme forces

several restrictions (e.g., B-LOC cannot be followed by I–PER). The softmax method

allows local decisions (i.e., for the tag of each token wi) even though the BiLSTM

captures information about the neighboring words. Still, the neighboring tags are not

taken into account for the tag decision of a speciﬁc token. For example, in the entity

“John Smith”, tagging “Smith” as PER is useful for deciding that “John” is B-PER. To

this end, for NER, we use a linear-chain CRF, similar to Lample et al. (2016) where

an improvement of ∼1% F1 NER points is reported when using CRF. In our case, with

the use of CRF we also report a ∼1% overall performance improvement as observed

in Table 2 (see Section 5.2). Assuming the word vector w, a sequence of score vectors
1 , ..., s(e)
s(e)
deﬁned as:

n and a vector of tag predictions y(e)

n , the linear-chain CRF score is

1 , ..., y(e)

(cid:16)

S

y(e)
1 , . . . , y(e)
n

(cid:17)

=

n
(cid:88)

i=0

s(e)
i,y(e)
i

+

n−1
(cid:88)

i=1

Ty(e)

i

,y(e)
i+1

where S ∈ R, s(e)

i,y(e)
i

is the score of the predicted tag for token wi, T is a square tran-

sition matrix in which each entry represents transition scores from one tag to another.
T ∈ R(p+2)×(p+2) because y(e)
0

n are two auxiliary tags that represent the start-

and y(e)

ing and the ending tags of the sentence, respectively. Then, the probability of a given

sequence of tags over all possible tag sequences for the input sentence w is deﬁned as:

(cid:16)

Pr

y(e)
1 , . . . , y(e)
n

(cid:17)

(cid:12)
(cid:12)
(cid:12) w

=

eS(y(e)

1 ,...,y(e)
n )

(cid:80)

eS( ˜y1

(e),..., ˜yn

(e))

˜y1

(e),..., ˜yn

(e)

We apply Viterbi to obtain the tag sequence ˆy(e) with the highest score. We train

both the softmax (for the EC task) and the CRF layer (for NER) by minimizing the

cross-entropy loss LNER. We also use the entity tags as input to our relation extraction

layer by learning label embeddings, motivated by Miwa & Bansal (2016) where an im-

provement of 2% F1 is reported (with the use of label embeddings). In our case, label

embeddings lead to an increase of 1% F1 score as reported in Table 2 (see Section 5.2).

The input to the next layer is twofold: the output states of the LSTM and the learned

label embedding representation, encoding the intuition that knowledge of named enti-

(3)

(4)

11

ties can be useful for relation extraction. During training, we use the gold entity tags,

while at prediction time we use the predicted entity tags as input to the next layer. The

input to the next layer is the concatenation of the hidden LSTM state hi with the label

embedding gi for token wi:

zi = [hi; gi],

i = 0, ..., n

(5)

3.4. Relation extraction as multi-head selection

In this subsection, we describe the relation extraction task, formulated as a multi-head

selection problem (Zhang et al., 2017; Bekoulis et al., 2018). In the general formulation

of our method, each token wi can have multiple heads (i.e., multiple relations with

other tokens). We predict the tuple (ˆyi, ˆci) where ˆyi is the vector of heads and ˆci is

the vector of the corresponding relations for each token wi. This is different for the

previous standard head selection for dependency parsing method (Zhang et al., 2017)

since (i) it is extended to predict multiple heads and (ii) the decisions for the heads and

the relations are jointly taken (i.e., instead of ﬁrst predicting the heads and then in a next

step the relations by using an additional classiﬁer). Given as input a token sequence w

and a set of relation labels R, our goal is to identify for each token wi, i ∈ {0, ..., n}

the vector of the most probable heads ˆyi ⊆ w and the vector of the most probable

corresponding relation labels ˆri ⊆ R. We calculate the score between tokens wi and

wj given a label rk as follows:

s(r)(zj, zi, rk) = V (r)f (U (r)zj + W (r)zi + b(r))

(6)

where the superscript (r) is used for the notation of the relation task, f (·) is an element-
wise activation function (i.e., relu, tanh), V (r) ∈ Rl, U (r) ∈ Rl×(2d+b), W (r) ∈
Rl×(2d+b), b(r) ∈ Rl, d is the hidden size of the LSTM, b is the size of the label

embeddings and l the layer width. We deﬁne

Pr(head = wj, label = rk | wi) = σ(s(r)(zj, zi, rk))

(7)

to be the probability of token wj to be selected as the head of token wi with the relation

label rk between them, where σ(.) stands for the sigmoid function. We minimize the

12

cross-entropy loss Lrel during training:

n
(cid:88)

m
(cid:88)

i=0

j=0

Lrel =

− log Pr(head = yi,j, relation = ri,j | wi)

(8)

where yi ⊆ w and ri ⊆ R are the ground truth vectors of heads and associated relation

labels of wi and m is the number of relations (heads) for wi. After training, we keep

the combination of heads ˆyi and relation labels ˆri exceeding a threshold based on the

estimated joint probability as deﬁned in Eq. (7). Unlike previous work on joint mod-

els (Katiyar & Cardie, 2017), we are able to predict multiple relations considering the

classes as independent and not mutually exclusive (the probabilities do not necessarily

sum to 1 for different classes). For the joint entity and relation extraction task, we

calculate the ﬁnal objective as LNER + Lrel.

3.5. Edmonds’ algorithm

Our model is able to simultaneously extract entity mentions and the relations between

them. To demonstrate the effectiveness and the general purpose nature of our model, we

also test it on the recently introduced Dutch real estate classiﬁeds (DREC) dataset (Bek-

oulis et al., 2017) where the entities need to form a tree structure. By using thresholded

inference, a tree structure of relations is not guaranteed. Thus we should enforce tree

structure constraints to our model. To this end, we post-process the output of our sys-

tem with Edmonds’ maximum spanning tree algorithm for directed graphs (Chu & Liu,

1965; Edmonds, 1967). A fully connected directed graph G = (V, E) is constructed,

where the vertices V represent the last tokens of the identiﬁed entities (as predicted

by NER) and the edges E represent the highest scoring relations with their scores as

weights. Edmonds’ algorithm is applied in cases a tree is not already formed by thresh-

olded inference.

4. Experimental setup

4.1. Datasets and evaluation metrics

We conduct experiments on four datasets: (i) Automatic Content Extraction, ACE04 (Dod-

dington et al., 2004), (ii) Adverse Drug Events, ADE (Gurulingappa et al., 2012b),

13

(iii) Dutch Real Estate Classiﬁeds, DREC (Bekoulis et al., 2017) and (iv) the CoNLL’04

dataset with entity and relation recognition corpora (Roth & Yih, 2004). Our code is

available in our github codebase.2

ACE04: There are seven main entity types namely Person (PER), Organization (ORG),

Geographical Entities (GPE), Location (LOC), Facility (FAC), Weapon (WEA) and Ve-

hicle (VEH). Also, the dataset deﬁnes seven relation types: Physical (PHYS), Person-

Social (PER-SOC), Employment-Membership-Subsidiary (EMP-ORG), Agent-Artifact (ART),

PER-ORG afﬁliation (Other-AFF), GPE afﬁliation (GPE-AFF), and Discourse (DISC).

We follow the cross-validation setting of Li & Ji (2014) and Miwa & Bansal (2016).

We removed DISC and did 5-fold cross-validation on the bnews and nwire subsets (348

documents). We obtained the preprocessing script from Miwa’s github codebase.3 We

measure the performance of our system using micro F1 scores, Precision and Recall on

both entities and relations. We treat an entity as correct when the entity type and the

region of its head are correct. We treat a relation as correct when its type and argument

entities are correct, similar to Miwa & Bansal (2016) and Katiyar & Cardie (2017). We

refer to this type of evaluation as strict.4 We select the best hyperparameter values on

a randomly selected validation set for each fold, selected from the training set (15% of

the data) since there are no ofﬁcial train and validation splits in the work of Miwa &

Bansal (2016).

CoNLL04: There are four entity types in the dataset (Location, Organization, Person,

and Other) and ﬁve relation types (Kill, Live in, Located in, OrgBased in and Work

for). We use the splits deﬁned by Gupta et al. (2016) and Adel & Sch¨utze (2017). The

dataset consists of 910 training instances, 243 for validation and 288 for testing.5 We

measure the performance by computing the F1 score on the test set. We adopt two

evaluation settings to compare to previous work. Speciﬁcally, we perform an EC task

assuming the entity boundaries are given similar to Gupta et al. (2016) and Adel &

2https://github.com/bekou/multihead_joint_entity_relation_extraction
3https://github.com/tticoin/LSTM-ER/tree/master/data/ace2004
4For the CoNLL04, DREC and ADE datasets, the head region covers the whole entity (start and end

boundaries). The ACE04 already deﬁnes the head region of an entity.

5http://cistern.cis.lmu.de/globalNormalization/globalNormalization_

all.zip

14

Sch¨utze (2017). To obtain comparable results, we omit the entity class “Other” when

computing the EC score. We score a multi-token entity as correct if at least one of its

comprising token types is correct assuming that the boundaries are given; a relation

is correct when the type of the relation and the argument entities are both correct. We

report macro-average F1 scores for EC and RE to obtain comparable results to previous

studies. Moreover, we perform actual NER evaluation instead of just EC, reporting

results using the strict evaluation metric.

DREC: The dataset consists of 2,318 classiﬁeds as described in the work of Bekoulis

et al. (2018). There are 9 entity types: Neighborhood, Floor, Extra building, Subspace,

Invalid, Field, Other, Space and Property. Also, there are two relation classes Part-of

and Equivalent. The goal is to identify important entities of a property (e.g., ﬂoors,

spaces) from classiﬁeds and structuring them into a tree format to get the structured

description of the property. For the evaluation, we use 70% for training, 15% for

validation and 15% as test set in the same splits as deﬁned in Bekoulis et al. (2018).

We measure the performance by computing the F1 score on the test set. To compare our

results with previous work (Bekoulis et al., 2018), we use the boundaries evaluation

setting. In this setting, we count an entity as correct if the boundaries of the entity are

correct. A relation is correct when the relation is correct and the argument entities are

both correct. Also, we report results using the strict evaluation for future reference.

ADE: There are two types of entities (drugs and diseases) in this dataset and the aim of

the task is to identify the types of entities and relate each drug with a disease (adverse

drug events). There are 6,821 sentences in total and similar to previous work (Li et al.,

2016, 2017), we remove ∼130 relations with overlapping entities (e.g., “lithium” is a

drug which is related to “lithium intoxication”). Since there are no ofﬁcial sets, we

evaluate our model using 10-fold cross-validation where 10% of the data was used as

validation and 10% for test set similar to Li et al. (2017). The ﬁnal results are displayed

in F1 metric as a macro-average across the folds. The dataset consists of 10,652 entities

and 6,682 relations. We report results similar to previous work on this dataset using the

strict evaluation metric.

15

4.2. Word embeddings

We use pre-trained word2vec embeddings used in previous work, so as to retain the

same inputs for our model and to obtain comparable results that are not affected by the

input embeddings. Speciﬁcally, we use the 200-dimensional word embeddings used in

the work of Miwa & Bansal (2016) for the ACE04 dataset6 trained on Wikipedia. We

obtained the 50-dimensional word embeddings used by Adel & Sch¨utze (2017)5 trained

also on Wikipedia for the CoNLL04 corpus. We use the 128-dimensional word2vec

embeddings used by Bekoulis et al. (2018) trained on a large collection of 887k Dutch

property advertisements7 for the DREC dataset. Finally, for the ADE dataset, we used

200-dimensional embeddings used by Li et al. (2017) and trained on a combination of

PubMed and PMC texts with texts extracted from English Wikipedia (Moen & Anani-

adou, 2013)8.

4.3. Hyperparameters and implementation details

We have developed our joint model by using Python with the TensorFlow machine

learning library (Abadi et al., 2016). Training is performed using the Adam opti-

mizer (Kingma & Ba, 2015) with a learning rate of 10−3. We ﬁx the size of the LSTM

to d = 64 and the layer width of the neural network to l = 64 (both for the entity and

the relation scoring layers). We use dropout (Srivastava et al., 2014) to regularize our

network. Dropout is applied in the input embeddings and in between the hidden layers

for both tasks. Different dropout rates have been applied but the best dropout values

(0.2 to 0.4) for each dataset have been used. The hidden dimension for the character-

based LSTMs is 25 (for each direction). We also ﬁxed our label embeddings to be of

size b = 25 for all the datasets except for CoNLL04 where the label embeddings were

not beneﬁcial and thus were not used. We experimented with tanh and relu activation

functions (recall that this is the function f (·) from the model description). We use the

6http://tti-coin.jp/data/wikipedia200.bin
7https://drive.google.com/uc?id=1Dvibr-Ps4G_GI6eDx9bMXnJphGhH_M1z&

8http://evexdb.org/pmresources/vec-space-models/wikipedia-pubmed-and-PMC-w2v.

export=download

bin

16

Settings

Evaluation

P

Relation
R

F1

Overall F1

E
C
A

4
0

Miwa & Bansal (2016)
Katiyar & Cardie (2017)
multi-head

L
L
N
o
C

4
0

C
E
R
D

E
D
A

Gupta et al. (2016)
Gupta et al. (2016)
Adel & Sch¨utze (2017)
multi-head EC
Miwa & Sasaki (2014)
multi-head

Bekoulis et al. (2018)
multi-head+E
single-head
multi-head

Li et al. (2016)
Li et al. (2017)
multi-head

Pre-calculated
Features
(cid:51)
(cid:55)
(cid:55)

(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:55)

(cid:55)
(cid:55)
(cid:55)
(cid:55)

(cid:51)
(cid:51)
(cid:55)

strict
strict
strict

relaxed
relaxed
relaxed
relaxed
strict
strict

boundaries
boundaries
strict
strict

strict
strict
strict

80.80
81.20
81.01

92.50
88.50
-
93.41
81.20
83.75

77.93
79.84
78.80
78.97

79.50
82.70
84.72

Entity
R

82.90
78.10
81.31

92.10
88.90
-
93.15
80.20
84.06

80.31
84.92
84.26
83.98

79.60
86.70
88.16

F1

81.80
79.60
81.16

92.40
88.80
82.10
93.26
80.70
83.90

79.11
82.30
81.43
81.39

79.50
84.60
86.40

P

48.70
46.40
50.14

78.50
64.60
-
72.99
76.00
63.75

49.24
50.52
50.57
50.00

64.00
67.50
72.10

48.10
45.53
44.48

63.00
53.10
-
63.37
50.90
60.43

50.17
55.30
54.30
54.73

62.90
75.80
77.24

48.40
45.70
47.14

69.90
58.30
62.50
67.01
61.00
62.04

49.70
52.81
52.37
52.26

63.40
71.40
74.58

65.10
62.65
64.15

81.15
73.60
72.30
80.14
70.85
72.97

64.41
67.56
66.90
66.83

71.45
78.00
80.49

Table 1: Comparison of our method (multi-head) with the state-of-the-art on the ACE04, CoNLL04, DREC
and ADE datasets. The models: (i) multi-head+E (the model + the Edmond algorithm to produce a tree-
structured output), (ii) single-head (the model predicts only one head per token) and (iii) multi-head EC (the
model predicts only the entity classes assuming that the boundaries are given) are slight variations of the
multi-head model adapted for each dataset and evaluation. The (cid:51)and (cid:55) symbols indicate whether or not the
models rely on any hand-crafted features or additional tools. Note that all the variations of our models do not
rely on any additional features. We include here different evaluation types (strict, relaxed and boundaries)
to be able to compare our results against previous studies. Finally, we report results in terms of Precision,
Recall, F1 for the two subtasks as well as overall F1, averaging over both subtasks. Bold entries indicate the
best result among models that only consider automatically learned features.

relu activation only in the ACE04 and tanh in all other datasets. We employ the tech-

nique of early stopping based on the validation set. In all the datasets examined in this

study, we obtain the best hyperparameters after 60 to 200 epochs depending on the size

of the dataset. We select the best epoch according to the results in the validation set.

For more details about the effect of each hyperparameter to the model performance see

the Appendix.

5. Results and discussion

5.1. Results

In Table 1, we present the results of our analysis. The ﬁrst column indicates the con-

sidered dataset. In the second column, we denote the model which is applied (i.e.,

previous work and the proposed models). The proposed models are the following:

(i) multi-head is the proposed model with the CRF layer for NER and the sigmoid loss

for multiple head prediction, (ii) multi-head+E is the proposed model with addition

of Edmonds’ algorithm to guarantee a tree-structured output for the DREC dataset,

17

(iii) single-head is the proposed method but it predicts only one head per token using a

softmax loss instead of a sigmoid, and (iv) multi-head EC is the proposed method with

a softmax to predict the entity classes assuming that the boundaries are given, and the

sigmoid loss for multiple head selection. Table 1 also indicates whether the different

settings include hand-crafted features or features derived from NLP tools (e.g., POS
taggers, dependency parsers). We use the (cid:51) symbol to denote that the model includes
this kind of additional features and the (cid:55) symbol to denote that the model is only based

on automatically extracted features. Note that all the variations of our model do not

rely on any additional features. In the next column, we declare the type of evaluation

conducted for each experiment. We include here different evaluation types to be able

to compare our results against previous studies. Speciﬁcally, we use three evaluation

types, namely:

(i) Strict: an entity is considered correct if the boundaries and the type of the en-

tity are both correct; a relation is correct when the type of the relation and the

argument entities are both correct,

(ii) Boundaries: an entity is considered correct if only the boundaries of the entity

are correct (entity type is not considered); a relation is correct when the type of

the relation and the argument entities are both correct and

(iii) Relaxed: we score a multi-token entity as correct if at least one of its comprising

token types is correct assuming that the boundaries are given; a relation is correct

when the type of the relation and the argument entities are both correct.

In the next three columns, we present the results for the entity identiﬁcation task (Pre-

cision, Recall, F1) and then (in the subsequent three columns) the results of the relation

extraction task (Precision, Recall, F1). Finally, in the last column, we report an addi-

tional F1 measure which is the average F1 performance of the two subtasks. We mark

with bold font in Table 1, the best result for each dataset among those models that use

only automatically extracted features.

Considering the results in the ACE04, we observe that our model outperforms the

model of Katiyar & Cardie (2017) by ∼2% in both tasks. This improvement can be

18

explained by the use of the multi-head selection method which can naturally capture

multiple relations and model them as a multi-label problem. Unlike the work of Kati-

yar & Cardie (2017), the class probabilities do not necessarily sum up to one since the

classes are considered independent. Moreover, we use a CRF-layer to model the NER

task to capture dependencies between sequential tokens. Finally, we obtain more ef-

fective word representations by using character-level embeddings. On the other hand,

our model performs within a reasonable margin (∼0.5% for the NER task and ∼1%

for the RE task) compared to Miwa & Bansal (2016). This difference is explained by

the fact that the model of Miwa & Bansal (2016) relies on POS tagging and syntactic

features derived by dependency parsing. However, this kind of features relies on NLP

tools that are not always accurate for various languages and contexts. For instance, the

same model is adopted by the work of Li et al. (2017) for the ADE biomedical dataset

and in this dataset our model reports more than 3% improvement in the RE task. This

shows that our model is able to produce automatically extracted features which perform

reasonably well in all contexts (e.g., news, biomedical).

For the CoNLL04 dataset, there are two different evaluation settings, namely re-

laxed and strict. In the relaxed setting, we perform an EC task instead of NER as-

suming that the boundaries of the entities are given. We adopt this setting to produce

comparable results with previous studies (Gupta et al., 2016; Adel & Sch¨utze, 2017).

Similar to Adel & Sch¨utze (2017), we present results of single models and no ensem-

bles. We observe that our model outperforms all previous models that do not rely on

complex hand-crafted features by a large margin (>4% for both tasks). Unlike these

previous studies that consider pairs of entities to obtain the entity types and the cor-

responding relations, we model the whole sentence at once. That way, our method is

able to directly infer all entities and relations of a sentence and beneﬁt from their pos-

sible interactions that cannot be modeled when training is performed for each entity

pair individually, one at a time. In the same setting, we also report the results of Gupta

et al. (2016) in which they use multiple complicated hand-crafted features coming from

NLP tools. Our model performs slightly better for the EC task and within a margin of

1% in terms of overall F1 score. The difference in the overall performance is due to

the fact that our model uses only automatically generated features. We also report re-

19

sults on the same dataset conducting NER (i.e., predicting entity types and boundaries)

and evaluating using the strict evaluation measure, similar to Miwa & Sasaki (2014).

Our results are not directly comparable to the work of Miwa & Sasaki (2014) because

we use the splits provided by Gupta et al. (2016). However, in this setting we present

the results from Miwa & Sasaki (2014) as reference. We report an improvement of

∼2% overall F1 score, which suggests that our neural model is able to extract more

informative representations compared to feature-based approaches.

We also report results for the DREC dataset, with two different evaluation settings.

Speciﬁcally, we use the boundaries and the strict settings. We transform the previous

results from Bekoulis et al. (2018) to the boundaries setting to make them comparable

to our model since in their work, they report token-based F1 score, which is not a

common evaluation metric in relation extraction problems. Also, in their work, they

focus on identifying only the boundaries of the entities and not the types (e.g., Floor,

Space). In the boundaries evaluation, we achieve ∼3% improvement for both tasks.

This is due to the fact that their quadratic scoring layer is beneﬁcial for the RE task, yet

complicates NER, which is usually modeled as a sequence labeling task. Moreover,

we report results using the strict evaluation which is used in most related works. Using

the prior knowledge that each entity has only one head, we can simplify our model and

predict only one head each time (i.e., using a softmax loss). The difference between

the single and the multi-head models is marginal (<0.1% for both tasks). This shows

that our model (multi-head) can adapt to various environments, even if the setting is

single head (in terms of the application, and thus also in both training and test data).

Finally, we compare our model with previous work (Li et al., 2016, 2017) on the

ADE dataset. The previous models (Li et al., 2016, 2017) both use hand-crafted fea-

tures or features derived from NLP tools. However, our model is able to outperform

both models using the strict evaluation metric. We report an improvement of ∼2% in

the NER and ∼3% in the RE tasks, respectively. The work of Li et al. (2017) is similar

to Miwa & Bansal (2016) and strongly relies on dependency parsers to extract syntactic

information. A possible explanation for the better result obtained from our model is

that the pre-calculated syntactic information obtained using external tools either is not

so accurate or important for biomedical data.

20

Settings

Multi-head
−Label embeddings
−Character embeddings
−CRF loss

P

81.01
80.61
80.42
80.47

Entity
R

81.31
80.91
79.52
81.50

F1

81.16
80.77
79.97
80.98

P

50.14
50.00
49.06
47.34

Relation
R

F1

Overall F1

44.48
42.92
41.62
42.84

47.14
46.18
45.04
44.98

64.15
63.48
62.50
62.98

Table 2: Ablation tests on the ACE04 test dataset.

5.2. Analysis of feature contribution

We conduct ablation tests on the ACE04 dataset reported in Table 2 to analyze the

effectiveness of the various parts of our joint model. The performance of the RE task

decreases (∼1% in terms of F1 score) when we remove the label embeddings layer and

only use the LSTM hidden states as inputs for the RE task. This shows that the NER

labels, as expected, provide meaningful information for the RE component.

Removing character embeddings also degrades the performance of both NER (∼1%)

and RE (∼2%) tasks by a relatively large margin. This illustrates that composing words

by the representation of characters is effective, and our method beneﬁts from additional

information such as capital letters, sufﬁxes and preﬁxes within the token (i.e., its char-

acter sequences).

Finally, we conduct experiments for the NER task by removing the CRF loss layer

and substituting it with a softmax. Assuming independent distribution of labels (i.e.,

softmax) leads to a slight decrease in the F1 performance of the NER module and a

∼2% decrease in the performance of the RE task. This happens because the CRF loss

is able to capture the strong tag dependencies (e.g., I-LOC cannot follow B-PER) that

are present in the dataset instead of just assuming that the tag decision for each token

is independent from tag decisions of neighboring tokens.

6. Conclusion

In this work, we present a joint neural model to simultaneously extract entities and

relations from textual data. Our model comprises a CRF layer for the entity recogni-

tion task and a sigmoid layer for the relation extraction task. Speciﬁcally, we model

the relation extraction task as a multi-head selection problem since one entity can have

21

multiple relations. Previous models on this task rely heavily on external NLP tools (i.e.,

POS taggers, dependency parsers). Thus, the performance of these models is affected

by the accuracy of the extracted features. Unlike previous studies, our model produces

automatically generated features rather than relying on hand-crafted ones, or existing

NLP tools. Given its independence from such NLP or other feature generating tools,

our approach can be easily adopted for any language and context. We demonstrate

the effectiveness of our approach by conducting a large scale experimental study. Our

model is able to outperform neural methods that automatically generate features while

the results are marginally similar (or sometimes better) compared to feature-based neu-

ral network approaches.

As future work, we aim to explore the effectiveness of entity pre-training for the en-

tity recognition module. This approach has been proven beneﬁcial in the work of Miwa

& Bansal (2016) for both the entity and the relation extraction modules. In addition,

we are planning to explore a way to reduce the calculations in the quadratic relation

scoring layer. For instance, a straightforward way to do so is to use in the sigmoid layer

only the tokens that have been identiﬁed as entities.

References

Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat,

S., Irving, G., Isard, M., Kudlur, M., Levenberg, J., Monga, R., Moore, S., Murray,

D. G., Steiner, B., Tucker, P., Vasudevan, V., Warden, P., Wicke, M., Yu, Y., & Zheng,

X. (2016). Tensorﬂow: A system for large-scale machine learning. In Proceedings

of the 12th USENIX Conference on Operating Systems Design and Implementation

(pp. 265–283). Berkeley, CA, USA.

Adel, H., & Sch¨utze, H. (2017). Global normalization of convolutional neural networks

for joint entity and relation classiﬁcation. In Proceedings of the 2017 Conference

on Empirical Methods in Natural Language Processing. Copenhagen, Denmark:

Association for Computational Linguistics.

Bach, N., & Badaskar, S. (2007). A review of relation extraction. Literature review for

Language and Statistics II, .

22

Bekoulis, G., Deleu, J., Demeester, T., & Develder, C. (2017). Reconstructing the

house from the ad: Structured prediction on real estate classiﬁeds. In Proceedings of

the 15th Conference of the European Chapter of the Association for Computational

Linguistics: (Volume 2, Short Papers) (pp. 274–279). Valencia, Spain.

Bekoulis, G., Deleu, J., Demeester, T., & Develder, C. (2018). An attentive neural

architecture for joint segmentation and parsing and its application to real estate ads.

Expert Systems with Applications, 102, 100 – 112. doi:10.1016/j.eswa.2018.

02.031.

Bengio, Y., Simard, P., & Frasconi, P. (1994). Learning long-term dependencies

with gradient descent is difﬁcult. Transactions on neural networks, 5(2), 157–166.

doi:10.1109/72.279181.

Sinica, 14, 1396–1400.

Chu, Y.-J., & Liu, T.-H. (1965). On shortest arborescence of a directed graph. Scientia

Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011).

Natural language processing (almost) from scratch. Journal of Machine Learning

Research, 12, 2493–2537.

Culotta, A., & Sorensen, J. (2004). Dependency tree kernels for relation extraction. In

Proceedings of the 42nd Annual Meeting on Association for Computational Linguis-

tics (pp. 423–429). Barcelona, Spain. doi:10.3115/1218955.1219009.

Doddington, G. R., Mitchell, A., Przybocki, M. A., Ramshaw, L. A., Strassel, S., &

Weischedel, R. M. (2004). The automatic content extraction (ace) program-tasks,

data, and evaluation. In Proceedings Fourth International Conference on Language

Resources and Evaluation (p. 1). Lisbon, Portugal volume 2.

Edmonds, J. (1967). Optimum branchings. Journal of research of the National Bureau

of Standards, 71B(4), 233–240.

Fundel, K., Kffner, R., & Zimmer, R.

(2007).

Relex-relation extraction

using dependency parse trees.

Bioinformatics, 23(3), 365–371. doi:10.

1093/bioinformatics/btl616.

23

Gupta, P., Sch¨utze, H., & Andrassy, B. (2016). Table ﬁlling multi-task recurrent neural

network for joint entity and relation extraction. In Proceedings of COLING 2016,

the 26th International Conference on Computational Linguistics: Technical Papers

(pp. 2537–2547).

Gurulingappa, H., MateenRajpu, A., & Toldo, L. (2012a). Extraction of potential

adverse drug events from medical case reports. Journal of Biomedical Semantics,

3(1), 1–15. doi:10.1186/2041-1480-3-15.

Gurulingappa, H., Rajput, A. M., Roberts, A., Fluck, J., Hofmann-Apitius, M., &

Toldo, L. (2012b). Development of a benchmark corpus to support the auto-

matic extraction of drug-related adverse effects from medical case reports. Jour-

nal of Biomedical Informatics, 45(5), 885 – 892. doi:https://doi.org/10.

1016/j.jbi.2012.04.008.

Huang, Z., Xu, W., & Yu, K. (2015). Bidirectional LSTM-CRF models for sequence

tagging. arXiv preprint arXiv:1508.01991, .

Kambhatla, N. (2004). Combining lexical, syntactic, and semantic features with maxi-

mum entropy models for extracting relations. In Proceedings of the Annual Meeting

of the Association for Computational Linguistics on Interactive poster and demon-

stration sessions. Barcelona, Spain. doi:10.3115/1219044.1219066.

Kate, R. J., & Mooney, R. (2010).

Joint entity and relation extraction using card-

pyramid parsing. In Proceedings of the 14th Conference on Computational Natural

Language Learning (pp. 203–212). Uppsala, Sweden: Association for Computa-

tional Linguistics.

Katiyar, A., & Cardie, C. (2017). Going out on a limb: Joint extraction of entity

mentions and relations without dependency trees. In Proceedings of the 55st Annual

Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).

Vancouver, Canada.

Kingma, D., & Ba, J. (2015). Adam: A method for stochastic optimization. In Inter-

national Conference on Learning Representations. San Diego, USA.

24

Lafferty, J., McCallum, A., & Pereira, F. (2001). Conditional random ﬁelds: Proba-

bilistic models for segmenting and labeling sequence data. In Proceedings of the

18th International Conference on Machine Learning (pp. 282–289). San Francisco,

USA: Morgan Kaufmann.

Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K., & Dyer, C. (2016).

Neural architectures for named entity recognition. In Proceedings of the 2016 Con-

ference of the North American Chapter of the Association for Computational Lin-

guistics: Human Language Technologies (pp. 260–270). San Diego, California.

Li, F., Zhang, M., Fu, G., & Ji, D. (2017). A neural joint model for entity

and relation extraction from biomedical text. BMC Bioinformatics, 18(1), 1–11.

doi:10.1186/s12859-017-1609-9.

Li, F., Zhang, Y., Zhang, M., & Ji, D. (2016). Joint models for extracting adverse

drug events from biomedical text. In Proceedings of the Twenty-Fifth International

Joint Conference on Artiﬁcial Intelligence (pp. 2838–2844). New York, USA: IJ-

CAI/AAAI Press.

Li, Q., & Ji, H. (2014). Incremental joint extraction of entity mentions and relations.

In Proceedings of the 52nd Annual Meeting of the Association for Computational

Linguistics (Volume 1: Long Papers) (pp. 402–412). Baltimore, USA.

Ma, X., & Hovy, E. (2016). End-to-end sequence labeling via bi-directional LSTM-

CNNs-CRF. In Proceedings of the 54th Annual Meeting of the Association for Com-

putational Linguistics (Volume 1: Long Papers) (pp. 1064–1074). Berlin, Germany.

Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed

representations of words and phrases and their compositionality. In Proceedings of

the 26th International Conference on Neural Information Processing Systems (pp.

3111–3119). Nevada, United States: Curran Associates, Inc.

Miwa, M., & Bansal, M. (2016). End-to-end relation extraction using LSTMs on se-

quences and tree structures. In Proceedings of the 54th Annual Meeting of the As-

25

sociation for Computational Linguistics (Volume 1: Long Papers) (pp. 1105–1116).

Berlin, Germany.

Miwa, M., & Sasaki, Y. (2014). Modeling joint entity and relation extraction with

table representation. In Proceedings of the 2014 Conference on Empirical Methods

in Natural Language Processing (pp. 1858–1869). Doha, Qatar: Association for

Computational Linguistics.

Moen, S., & Ananiadou, T. S. S. (2013). Distributional semantics resources for biomed-

ical text processing.

In Proceedings of the 5th International Symposium on Lan-

guages in Biology and Medicine (pp. 39–43). Tokyo, Japan.

Nadeau, D., & Sekine, S. (2007). A survey of named entity recognition and classiﬁca-

tion. Lingvisticae Investigationes, 30(1), 3–26. doi:10.1075/li.30.1.03nad.

Pascanu, R., Mikolov, T., & Bengio, Y. (2013). On the difﬁculty of training recurrent

neural networks. In Proceedings of the 30th International Conference on Interna-

tional Conference on Machine Learning (pp. 1310–1318). Atlanta, USA: JMLR.org.

Rink, B., & Harabagiu, S. (2010). Utd: Classifying semantic relations by combining

lexical and semantic resources. In Proceedings of the 5th International Workshop

on Semantic Evaluation (pp. 256–259). Los Angeles, California: Association for

Computational Linguistics.

Roth, D., & Yih, W.-t. (2004). A linear programming formulation for global infer-

ence in natural language tasks. In HLT-NAACL 2004 Workshop: Eighth Conference

on Computational Natural Language Learning (CoNLL-2004) (pp. 1–8). Boston,

USA: Association for Computational Linguistics. URL: http://www.aclweb.

org/anthology/W04-2401.

dos Santos, C., Xiang, B., & Zhou, B. (2015). Classifying relations by ranking with

convolutional neural networks. In Proceedings of the 53rd Annual Meeting of the As-

sociation for Computational Linguistics and the 7th International Joint Conference

on Natural Language Processing (Volume 1: Long Papers) (pp. 626–634). Beijing,

China.

26

Socher, R., Chen, D., Manning, C. D., & Ng, A. (2013). Reasoning with neural

tensor networks for knowledge base completion.

In Proceedings of the 26th In-

ternational Conference on Neural Information Processing Systems (pp. 926–934).

Nevada, United States: Curran Associates, Inc.

Socher, R., Huval, B., Manning, C. D., & Ng, A. Y. (2012). Semantic compositionality

through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference

on Empirical Methods in Natural Language Processing and Computational Natural

Language Learning (pp. 1201–1211). Jeju Island, Korea: Association for Computa-

tional Linguistics.

Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014).

Dropout: A simple way to prevent neural networks from overﬁtting. Journal of

Machine Learning Research, 15(1), 1929–1958.

Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with

neural networks.

In Proceedings of the 27th International Conference on Neural

Information Processing Systems (pp. 3104–3112). Montreal, Canada: MIT Press.

Taskar, B., Guestrin, C., & Koller, D. (2003). Max-margin markov networks.

In

Proceedings of the 16th International Conference on Neural Information Processing

Systems (pp. 25–32). Bangkok, Thailand: MIT Press.

Tsochantaridis, I., Hofmann, T., Joachims, T., & Altun, Y. (2004). Support vector

machine learning for interdependent and structured output spaces. In Proceedings

of the 21st International Conference on Machine Learning (pp. 104–112). Helsinki,

Finland: ACM. doi:10.1145/1015330.1015341.

Vu, N. T., Adel, H., Gupta, P., & Sch¨utze, H. (2016). Combining recurrent and con-

volutional neural networks for relation classiﬁcation.

In Proceedings of the 2016

Conference of the North American Chapter of the Association for Computational

Linguistics: Human Language Technologies (pp. 534–539). San Diego, California.

URL: http://www.aclweb.org/anthology/N16-1065.

27

Xu, K., Feng, Y., Huang, S., & Zhao, D. (2015a). Semantic relation classiﬁcation via

convolutional neural networks with simple negative sampling.

In Proceedings of

the 2015 Conference on Empirical Methods in Natural Language Processing (pp.

536–540). Lisbon, Portugal: Association for Computational Linguistics. URL:

http://aclweb.org/anthology/D15-1062.

Xu, Y., Mou, L., Li, G., Chen, Y., Peng, H., & Jin, Z. (2015b). Classifying relations via

long short term memory networks along shortest dependency paths. In Proceedings

of the 2015 Conference on Empirical Methods in Natural Language Processing (pp.

1785–1794). Lisbon, Portugal: Association for Computational Linguistics.

Yang, B., & Cardie, C. (2013). Joint inference for ﬁne-grained opinion extraction.

In Proceedings of the 51st Annual Meeting of the Association for Computational

Linguistics (Volume 1: Long Papers) (pp. 1640–1649). Soﬁa, Bulgaria. URL:

http://www.aclweb.org/anthology/P13-1161.

Zelenko, D., Aone, C., & Richardella, A. (2003). Kernel methods for relation

extraction.

Journal of Machine Learning Research, 3, 1083–1106. doi:10.

3115/1118693.1118703.

Zeng, D., Liu, K., Lai, S., Zhou, G., & Zhao, J. (2014). Relation classiﬁcation via

convolutional deep neural network. In Proceedings of COLING 2014, the 25th In-

ternational Conference on Computational Linguistics: Technical Papers (pp. 2335–

2344).

Spain.

Zhang, D., & Wang, D. (2015). Relation classiﬁcation via recurrent neural network.

arXiv preprint arXiv:1508.01006, .

Zhang, X., Cheng, J., & Lapata, M. (2017). Dependency parsing as head selection.

In Proceedings of the 15th Conference of the European Chapter of the Association

for Computational Linguistics: (Volume 1, Long Papers) (pp. 665–676). Valencia,

Zheng, S., Hao, Y., Lu, D., Bao, H., Xu, J., Hao, H., & Xu, B. (2017). Joint entity and

28

relation extraction based on a hybrid neural network. Neurocomputing, 257, 59 –

66. doi:10.1016/j.neucom.2016.12.075.

29

Appendix

In this section, we report additional results for our multi-head selection framework.

Speciﬁcally, we (i) compare our model with the model of Lample et al. (2016) (i.e.,

optimize only over the NER task), (ii) explore several hyperparameters of the network

(e.g., dropout, LSTM size, character embeddings size), and (iii) report F1 score using

different word embeddings compared to the embeddings used in previous works.

In Table 1 of the main paper, we focused on comparing our model against other

joint models that are able to solve the two tasks (i.e., NER and relation extraction) si-

multaneously, mainly demonstrating superiority of phrasing the relation extraction as

a multi-head selection problem (enabling the extraction of multiple relations at once).

Here, in Table A1, we evaluate the performance of just the ﬁrst module of our joint

multi-head model: we compare the performance of the NER component of our model

against the state-of-the-art NER model of Lample et al. (2016). The results indicate

a marginal performance improvement of our model over Lample’s NER baseline in 3

out of 4 datasets. The improvement of our model’s NER part is not substantial, since

(i) our NER part is almost identical to Lample’s, and (ii) recent advances in NER per-

formance among neural systems are relatively small (improvements in the order of few

0.1 F1 points – for instance, the contribution of Ma & Hovy (2016) and Lample et al.

(2016) on the CoNLL-2003 test set is 0.01% and 0.17% F1 points, respectively). This

slight improvement suggests that the interaction of the two components by sharing the

underlying LSTM layer is indeed beneﬁcial (e.g., identifying a Works for relation might

be helpful for the NER module in detecting the type of the two entities, i.e., PER, ORG

and vice versa). Note that improving NER in isolation was not the objective of our

multi-head model, but we rather aimed to compare our model against other joint mod-

els that solve the task of entity recognition and relation identiﬁcation simultaneously.

We thus did not envision to claim or achieve state-of-the-art performance in each of the

individual building blocks of our joint model.

Tables A2, A3 and A4 show the performance of our model on the test set for dif-

ferent values of the embedding dropout, LSTM layer dropout and the LSTM output

dropout hyperparameters, respectively. Note that the hyperparameter values used for

1

the results in Section 5 were obtained by tuning over the development set, and these

are indicated in bold face in the tables below. We vary one hyperparameter at a time

in order to assess the effect of a particular hyperparameter. The main outcomes from

these tables are twofold: (i) low dropout values (e.g., 0, 0.1) lead to a performance

decrease in the overall F1 score (see Table A3 where a ∼3% F1 decrease is reported on

the ACE04 dataset) and (ii) average dropout values (i.e., 0.2-0.4) lead to consistently

similar results.

In Tables A5, A6, A7 and A8, we report results for different values of the LSTM

size, the size of the character embeddings, the size of the label embeddings and the

layer width of the neural network l (both for the entity and the relation scoring layers),

respectively. The reported results show that different hyperparameters settings do lead

to noticeable performance differences, but we do not observe any clear trend. More-

over, we have not observed any signiﬁcant performance improvement that affects the

overall ranking of the models as reported in Table 1. On the other hand, the results indi-

cate that increasing (character and label) embedding size and layer dimensions leads to

a slight decrease in performance for the CoNLL04 dataset. This can be explained by the

fact that the CoNLL04 dataset is relatively small and using more trainable model pa-

rameters (i.e., larger hyperparameter values) can make our multi-head selection method

to overﬁt quickly on the training set. In almost any other case, variation of the hyper-

parameters does not affect the ranking of the models reported in Table 1.

Model

ACE
04

NER baseline
multi-head

CoNLL
04

NER baseline
multi-head

DREC

ADE

NER baseline
multi-head

NER baseline
multi-head

Entity
R

81.13
81.31

83.13
84.06

84.89
83.98

88.59
88.16

P

81.06
81.01

84.38
83.75

78.22
78.97

83.97
84.72

F1

81.10
81.16

83.75
83.90

81.42
81.39

86.22
86.40

Table A1: Comparison of the multi-head selection model (only the NER component) against the NER base-
line of Lample et al. (2016). Bold font indicates the best results for each dataset.

In the main results (see Section 5), to guarantee a fair comparison to previous work

and to obtain comparable results that are not affected by the input embeddings, we use

embeddings used also in prior studies. To assess the performance of our system to input

2

Table A2: Model performance for different embedding dropout values. Bold entries indicate the result
reported in Section 5.

LSTM
Dropout

Relation
R

F1

Overall F1

Embedding
Dropout

Relation
R

F1

Overall F1

43.28
43.55
44.48
42.45
42.92
43.55

52.37
51.42
57.82
54.74
60.43
56.64

53.87
53.57
54.73
54.19
52.69
52.61

78.87
77.90
77.24
77.37
76.71
75.98

38.86
42.06
44.12
44.48
41.52
39.32

60.19
62.56
60.66
59.48
60.43
52.13

53.61
54.64
54.73
53.79
53.10
49.96

78.15
78.00
78.86
77.24
77.51
76.99

45.37
46.51
47.14
45.84
45.20
45.21

59.65
57.48
61.08
62.18
62.04
61.05

52.46
52.71
52.26
52.86
50.99
51.59

75.62
74.80
74.58
74.80
74.76
73.43

43.05
45.91
47.01
47.14
44.61
42.71

59.76
63.16
62.82
63.46
62.04
57.89

51.44
52.56
52.26
51.39
52.22
50.18

75.34
74.99
75.39
74.58
74.66
73.61

63.10
63.84
64.15
63.59
63.08
62.77

71.36
70.42
72.14
73.26
72.97
72.39

66.85
67.10
66.83
66.94
66.01
65.96

81.14
80.56
80.49
80.54
80.51
79.37

61.61
63.59
64.21
64.15
62.95
61.47

72.49
74.05
74.45
74.39
72.97
70.67

66.30
66.89
66.83
66.03
66.71
64.78

80.98
80.73
80.98
80.49
80.40
79.64

E
C
A

4
0

L
L
N
o
C

4
0

C
E
R
D

E
D
A

0.5
0.4
0.3
0.2
0.1
0

0.5
0.4
0.3
0.2
0.1
0

0.5
0.4
0.3
0.2
0.1
0

0.5
0.4
0.3
0.2
0.1
0

P

80.66
80.97
81.01
81.15
80.86
80.21

82.53
83.66
82.19
84.07
83.75
82.79

78.19
78.47
78.97
78.16
78.83
78.42

84.73
84.51
84.72
84.66
85.10
83.67

Entity
R

81.03
81.39
81.31
81.54
81.06
80.45

83.60
83.04
84.24
84.62
84.06
84.71

84.51
84.73
83.98
84.11
83.34
82.34

88.68
88.21
88.16
87.98
87.43
87.01

F1

80.84
81.18
81.16
81.34
80.96
80.32

83.06
83.35
83.20
84.34
83.90
83.74

81.23
81.48
81.39
81.02
81.02
80.33

86.66
86.32
86.40
86.29
86.25
85.31

P

47.66
49.90
50.14
49.81
47.74
47.00

69.28
65.17
64.72
71.96
63.75
66.21

51.12
51.87
50.00
51.60
49.38
50.62

72.63
71.93
72.10
72.39
72.91
71.04

P

80.27
81.18
81.19
81.01
81.27
80.54

84.18
84.43
86.44
84.73
83.75
84.16

77.76
78.66
78.97
77.85
78.94
78.59

85.01
84.66
84.60
84.72
84.36
83.80

Entity
R

80.08
81.36
81.63
81.31
81.32
79.94

86.28
85.45
85.73
85.91
84.06
82.76

84.83
83.98
83.98
83.68
83.62
80.18

88.29
88.37
88.66
88.16
87.98
87.64

F1

80.18
81.27
81.41
81.16
81.29
80.24

85.22
84.94
86.09
85.32
83.90
83.45

81.15
81.23
81.39
80.66
81.21
79.38

86.62
86.47
86.58
86.40
86.13
85.68

P

48.25
50.54
50.31
50.14
48.20
46.73

59.35
63.77
65.14
68.02
63.75
65.09

49.43
50.63
50.00
49.21
51.37
50.39

72.72
72.20
72.21
72.10
72.03
70.50

E
C
A

4
0

L
L
N
o
C

4
0

C
E
R
D

E
D
A

0.5
0.4
0.3
0.2
0.1
0

0.5
0.4
0.3
0.2
0.1
0

0.5
0.4
0.3
0.2
0.1
0

0.5
0.4
0.3
0.2
0.1
0

3

Table A3: Model performance for different LSTM layer dropout values. Bold entries indicate the result
reported in Section 5.

Table A4: Model performance for different LSTM output dropout values. Bold entries indicate the best
result reported in Section 5.

LSTM output
Dropout

Relation
R

F1

Overall F1

E
C
A

4
0

L
L
N
o
C

4
0

C
E
R
D

E
D
A

0.5
0.4
0.3
0.2
0.1
0

0.5
0.4
0.3
0.2
0.1
0

0.5
0.4
0.3
0.2
0.1
0

0.5
0.4
0.3
0.2
0.1
0

P

81.25
81.23
81.31
81.01
81.01
80.10

85.81
83.27
85.13
84.13
83.75
83.65

78.74
78.45
78.97
77.82
78.84
77.63

84.33
85.16
84.27
84.72
84.65
84.44

Entity
R

81.79
81.70
81.72
81.31
81.12
80.69

86.84
84.89
84.89
84.52
84.06
84.89

84.22
85.20
83.98
84.68
83.75
83.85

87.95
88.16
88.00
88.16
88.04
88.14

F1

81.52
81.47
81.51
81.16
81.07
80.39

86.32
84.08
85.01
84.32
83.90
84.27

81.39
81.69
81.39
81.11
81.22
80.62

86.10
86.63
86.10
86.40
86.31
86.25

P

51.16
51.44
48.69
50.14
47.55
47.20

64.18
66.07
64.82
66.03
63.75
65.23

51.24
50.34
50.00
51.05
51.74
51.16

71.54
72.87
71.83
72.10
72.38
71.64

41.94
42.77
44.21
44.48
42.82
40.54

59.01
61.37
55.45
57.58
60.43
53.79

52.69
55.45
54.73
54.19
54.75
51.39

77.27
77.81
77.42
77.24
77.49
77.82

46.09
46.71
46.35
47.14
45.06
43.61

61.48
63.63
59.77
61.52
62.04
58.96

51.96
52.77
52.26
52.57
53.20
51.28

74.29
75.26
74.52
74.58
74.85
74.61

63.81
64.09
63.93
64.15
63.07
62.00

73.90
73.85
72.39
72.92
72.97
71.61

66.68
67.23
66.83
66.84
67.21
65.95

80.20
80.95
80.31
80.49
80.58
80.43

Relation
R

F1

Overall F1

LSTM
Size

32
64
128

32
64
128

32
64
128

32
64
128

P

80.99
81.01
80.31

82.83
83.75
82.43

77.74
78.97
79.04

83.89
84.72
84.27

Entity
R

81.25
81.31
80.87

83.13
84.06
83.04

85.43
83.98
83.49

87.78
88.16
87.87

F1

81.12
81.16
80.59

82.98
83.90
82.73

81.40
81.39
81.20

85.79
86.40
86.04

P

50.33
50.14
47.30

65.78
63.75
64.86

50.92
50.00
51.27

70.46
72.10
71.36

42.60
44.48
41.77

58.29
60.43
53.79

52.31
54.73
53.64

76.89
77.24
76.77

46.14
47.14
44.36

61.81
62.04
58.81

51.60
52.26
52.42

73.54
74.58
73.97

Character
Embeddings

Relation
R

F1

Overall F1

P

81.02
81.01
81.32

83.33
83.75
85.15

79.73
78.97
78.08

84.80
84.72
84.65

Entity
R

81.57
81.31
81.54

84.34
84.06
82.95

84.17
83.98
84.80

88.00
88.16
88.08

15
25
50

15
25
50

15
25
50

15
25
50

P

47.87
50.14
49.77

66.03
63.75
59.84

52.52
50.00
51.03

72.74
72.10
72.17

44.78
44.48
44.02

57.11
60.43
52.61

55.30
54.73
54.28

77.51
77.24
77.45

46.27
47.14
46.72

61.25
62.04
55.99

53.88
52.26
52.60

75.05
74.58
74.72

63.63
64.15
62.47

72.39
72.97
70.77

66.50
66.83
66.81

79.66
80.49
80.00

63.78
64.15
64.07

72.54
72.97
70.01

67.89
66.83
66.95

80.71
80.49
80.52

E
C
A

4
0

4
0

L
L
N
o
C

C
E
R
D

E
D
A

E
C
A

4
0

4
0

L
L
N
o
C

C
E
R
D

E
D
A

F1

81.29
81.16
81.43

83.83
83.90
84.04

81.89
81.39
81.30

86.37
86.40
86.33

4

Table A5: Model performance for different LSTM size values. Bold entries indicate the result reported in
Section 5.

Table A6: Model performance for different character embeddings size values. Bold entries indicate the result
reported in Section 5.

E
C
A

4
0

4
0

L
L
N
o
C

C
E
R
D

E
D
A

E
C
A

4
0

4
0

L
L
N
o
C

C
E
R
D

E
D
A

15
25
50

15
0
50

15
25
50

15
25
50

32
64
128

32
64
128

32
64
128

32
64
128

Label
Embeddings

Relation
R

F1

Overall F1

P

80.95
81.01
81.17

84.68
83.75
82.32

78.48
78.97
78.92

84.47
84.72
84.81

Entity
R

81.27
81.31
81.61

83.50
84.06
84.15

84.81
83.98
84.88

88.18
88.16
88.65

F1

81.11
81.16
81.39

84.08
83.90
83.23

81.53
81.39
81.79

86.29
86.40
86.69

P

49.27
50.14
48.01

62.21
63.75
59.30

51.83
50.00
51.35

71.93
72.10
72.46

43.80
44.48
44.48

56.16
60.43
55.92

53.21
54.73
53.23

77.49
77.24
78.68

46.37
47.14
46.18

59.03
62.04
57.56

52.51
52.26
52.27

74.61
74.58
75.44

63.74
64.15
63.78

71.56
72.97
70.39

67.02
66.83
67.03

80.45
80.49
81.06

Table A7: Model performance for different label embeddings size values. Bold entries indicate the result
reported in Section 5.

variations, we also report results using different word embeddings (see Table A9) (i.e.,

Adel & Sch¨utze (2017); Li et al. (2017)) on the ACE04 dataset. Our results showcase

that our model, even when using different word embeddings, is still performing better

compared to other works that, like ours, do not rely on additional NLP tools.

Hidden layer
Size

Relation
R

F1

Overall F1

P

81.01
81.01
81.30

82.26
83.75
82.69

79.66
78.97
78.35

84.31
84.72
84.81

Entity
R

81.02
81.31
81.32

84.24
84.06
83.69

84.23
83.98
84.47

88.56
88.16
88.54

F1

81.02
81.16
81.31

83.24
83.90
83.19

81.89
81.39
81.30

86.38
86.40
86.63

P

48.81
50.14
51.58

65.96
63.75
64.46

52.42
50.00
48.53

71.67
72.10
72.29

43.26
44.48
43.68

59.24
60.43
55.45

51.45
54.73
53.08

78.51
77.24
78.20

45.87
47.14
47.30

62.42
62.04
59.62

51.93
52.26
50.70

74.93
74.58
75.13

63.44
64.15
64.31

72.83
72.97
71.40

66.91
66.83
66.00

80.66
80.49
80.87

Table A8: Model performance for different layer widths l of the neural network (both for the entity and the
relation scoring layers). Bold entries indicate the result reported in Section 5.

Embeddings

Size

Miwa & Bansal (2016)
Adel & Sch¨utze (2017)
Li et al. (2017)

200
50
200

P

81.01
82.18
81.51

Entity
R

81.31
79.83
81.35

F1

81.16
80.99
81.43

P

50.14
49.10
46.59

Relation
R

F1

Overall F1

44.48
41.40
44.43

47.14
44.92
45.49

64.15
62.96
63.46

Table A9: Model performance for different embeddings on the ACE04 dataset. Bold entries indicate the
result reported in Section 5.

5

8
1
0
2
 
c
e
D
 
7
1
 
 
]
L
C
.
s
c
[
 
 
3
v
7
4
8
7
0
.
4
0
8
1
:
v
i
X
r
a

Joint entity recognition and relation extraction as a
multi-head selection problem

Giannis Bekoulis∗, Johannes Deleu, Thomas Demeester, Chris Develder

Ghent University – imec, IDLab, Department of Information Technology,
Technologiepark Zwijnaarde 15, 9052 Ghent, Belgium

Abstract

State-of-the-art models for joint entity recognition and relation extraction strongly rely

on external natural language processing (NLP) tools such as POS (part-of-speech) tag-

gers and dependency parsers. Thus, the performance of such joint models depends on

the quality of the features obtained from these NLP tools. However, these features

are not always accurate for various languages and contexts.

In this paper, we pro-

pose a joint neural model which performs entity recognition and relation extraction

simultaneously, without the need of any manually extracted features or the use of any

external tool. Speciﬁcally, we model the entity recognition task using a CRF (Condi-

tional Random Fields) layer and the relation extraction task as a multi-head selection

problem (i.e., potentially identify multiple relations for each entity). We present an

extensive experimental setup, to demonstrate the effectiveness of our method using

datasets from various contexts (i.e., news, biomedical, real estate) and languages (i.e.,

English, Dutch). Our model outperforms the previous neural models that use automat-

ically extracted features, while it performs within a reasonable margin of feature-based

neural models, or even beats them.

sequence labeling

Keywords: entity recognition, relation extraction, multi-head selection, joint model,

∗Corresponding author
Email addresses: giannis.bekoulis@ugent.be (Giannis Bekoulis),

johannes.deleu@ugent.be (Johannes Deleu), thomas.demeester@ugent.be (Thomas
Demeester), chris.develder@ugent.be (Chris Develder)

1. Introduction

answering.

The goal of the entity recognition and relation extraction is to discover relational struc-

tures of entity mentions from unstructured texts. It is a central problem in information

extraction since it is critical for tasks such as knowledge base population and question

The problem is traditionally approached as two separate subtasks, namely (i) named

entity recognition (NER) (Nadeau & Sekine, 2007) and (ii) relation extraction (RE) (Bach

& Badaskar, 2007), in a pipeline setting. The main limitations of the pipeline models

are: (i) error propagation between the components (i.e., NER and RE) and (ii) possible

useful information from the one task is not exploited by the other (e.g., identifying a

Works for relation might be helpful for the NER module in detecting the type of the

two entities, i.e., PER, ORG and vice versa). On the other hand, more recent studies

propose to use joint models to detect entities and their relations overcoming the afore-

mentioned issues and achieving state-of-the-art performance (Li & Ji, 2014; Miwa &

Sasaki, 2014).

The previous joint models heavily rely on hand-crafted features. Recent advances

in neural networks alleviate the issue of manual feature engineering, but some of them

still depend on NLP tools (e.g., POS taggers, dependency parsers). Miwa & Bansal

(2016) propose a Recurrent Neural Network (RNN)-based joint model that uses a bidi-

rectional sequential LSTM (Long Short Term Memory) to model the entities and a

tree-LSTM that takes into account dependency tree information to model the relations

between the entities. The dependency information is extracted using an external depen-

dency parser. Similarly, in the work of Li et al. (2017) for entity and relation extraction

from biomedical text, a model which also uses tree-LSTMs is applied to extract de-

pendency information. Gupta et al. (2016) propose a method that relies on RNNs but

uses a lot of hand-crafted features and additional NLP tools to extract features such as

POS-tags, etc. Adel & Sch¨utze (2017) replicate the context around the entities with

Convolutional Neural Networks (CNNs). Note that the aforementioned works exam-

ine pairs of entities for relation extraction, rather than modeling the whole sentence

directly. This means that relations of other pairs of entities in the same sentence —

2

which could be helpful in deciding on the relation type for a particular pair — are not

taken into account. Katiyar & Cardie (2017) propose a neural joint model based on

LSTMs where they model the whole sentence at once, but still they do not have a prin-

cipled way to deal with multiple relations. Bekoulis et al. (2018) introduce a quadratic

scoring layer to model the two tasks simultaneously. The limitation of this approach is

that only a single relation can be assigned to a token, while the time complexity for the

entity recognition task is increased compared to the standard approaches with linear

complexity.

In this work, we focus on a new general purpose joint model that performs the

two tasks of entity recognition and relation extraction simultaneously, and that can

handle multiple relations together. Our model achieves state-of-the-art performance in

a number of different contexts (i.e., news, biomedical, real estate) and languages (i.e.,

English, Dutch) without relying on any manually engineered features nor additional

NLP tools. In summary, our proposed model (which will be detailed next in Section 3)

solves several shortcomings that we identiﬁed in related works (Section 2) for joint

entity recognition and relation extraction: (i) our model does not rely on external NLP

tools nor hand-crafted features, (ii) entities and relations within the same text fragment

(typically a sentence) are extracted simultaneously, where (iii) an entity can be involved

in multiple relations at once.

Speciﬁcally, the model of Miwa & Bansal (2016) depends on dependency parsers,

which perform particularly well on speciﬁc languages (i.e., English) and contexts (i.e.,

news). Yet, our ambition is to develop a model that generalizes well in various setups,

therefore using only automatically extracted features that are learned during training.

For instance, Miwa & Bansal (2016) and Li et al. (2017) use exactly the same model in

different contexts, i.e., news (ACE04) and biomedical data (ADE), respectively. Com-

paring our results to the ADE dataset, we obtain a 1.8% improvement on the NER task

and ∼3% on the RE task. On the other hand, our model performs within a reasonable

margin (∼0.6% in the NER task and ∼1% on the RE task) on the ACE04 dataset with-

out the use of pre-calculated features. This shows that the model of Miwa & Bansal

(2016) strongly relies on the features extracted by the dependency parsers and can-

not generalize well into different contexts where dependency parser features are weak.

3

Comparing to Adel & Sch¨utze (2017), we train our model by modeling all the entities

and the relations of the sentence at once. This type of inference is beneﬁcial in ob-

taining information about neighboring entities and relations instead of just examining

a pair of entities each time. Finally, we solve the underlying problem of the models

proposed by Katiyar & Cardie (2017) and Bekoulis et al. (2017), who essentially as-

sume classes (i.e., relations) to be mutually exclusive: we solve this by phrasing the

relation extraction component as a multi-label prediction problem.1

To demonstrate the effectiveness of the proposed method, we conduct the largest

experimental evaluation to date (to the best of our knowledge) in jointly performing

both entity recognition and relation extraction (see Section 4 and Section 5), using dif-

ferent datasets from various domains (i.e., news, biomedical, real estate) and languages

(i.e., English, Dutch). Speciﬁcally, we apply our method to four datasets, namely

ACE04 (news), Adverse Drug Events (ADE), Dutch Real Estate Classiﬁeds (DREC)

and CoNLL’04 (news). Our method outperforms all state-of-the-art methods that do

not rely on any additional features or tools, while performance is very close (or even

better in the biomedical dataset) compared to methods that do exploit hand-engineered

features or NLP tools.

2. Related work

The tasks of entity recognition and relation extraction can be applied either one by one

in a pipeline setting (Fundel et al., 2007; Gurulingappa et al., 2012a; Bekoulis et al.,

2017) or in a joint model (Miwa & Sasaki, 2014; Miwa & Bansal, 2016; Bekoulis

et al., 2018). In this section, we present related work for each task (i.e., named entity

recognition and relation extraction) as well as prior work into joint entity and relation

extraction.

1Note that another difference is that we use a CRF layer for the NER part, while Katiyar & Cardie
(2017) uses a softmax and Bekoulis et al. (2017) uses a quadratic scoring layer; see further, when we discuss
performance comparison results in Section 5.

4

2.1. Named entity recognition

In our work, NER is the ﬁrst task which we solve in order to address the end-to-end

relation extraction problem. A number of different methods for the NER task that

are based on hand-crafted features have been proposed, such as CRFs (Lafferty et al.,

2001), Maximum Margin Markov Networks (Taskar et al., 2003) and support vector

machines (SVMs) for structured output (Tsochantaridis et al., 2004), to name just a

few. Recently, deep learning methods such as CNN- and RNN-based models have

been combined with CRF loss functions (Collobert et al., 2011; Huang et al., 2015;

Lample et al., 2016; Ma & Hovy, 2016) for NER. These methods achieve state-of-the-

art performance on publicly available NER datasets without relying on hand-crafted

features.

2.2. Relation extraction

We consider relation extraction as the second task of our joint model. The main ap-

proaches for relation extraction rely either on hand-crafted features (Zelenko et al.,

2003; Kambhatla, 2004) or neural networks (Socher et al., 2012; Zeng et al., 2014).

Feature-based methods focus on obtaining effective hand-crafted features, for instance

deﬁning kernel functions (Zelenko et al., 2003; Culotta & Sorensen, 2004) and design-

ing lexical, syntactic, semantic features, etc. (Kambhatla, 2004; Rink & Harabagiu,

2010). Neural network models have been proposed to overcome the issue of manually

designing hand-crafted features leading to improved performance. CNN- (Zeng et al.,

2014; Xu et al., 2015a; dos Santos et al., 2015) and RNN-based (Socher et al., 2013;

Zhang & Wang, 2015; Xu et al., 2015b) models have been introduced to automatically

extract lexical and sentence level features leading to a deeper language understand-

ing. Vu et al. (2016) combine CNNs and RNNs using an ensemble scheme to achieve

state-of-the-art results.

2.3. Joint entity and relation extraction

Entity and relation extraction includes the task of (i) identifying the entities (described

in Section 2.1) and (ii) extracting the relations among them (described in Section 2.2).

Feature-based joint models (Kate & Mooney, 2010; Yang & Cardie, 2013; Li & Ji,

5

2014; Miwa & Sasaki, 2014) have been proposed to simultaneously solve the entity

recognition and relation extraction (RE) subtasks. These methods rely on the availabil-

ity of NLP tools (e.g., POS taggers) or manually designed features and thus (i) require

additional effort for the data preprocessing, (ii) perform poorly in different applica-

tion and language settings where the NLP tools are not reliable, and (iii) increase the

computational complexity. In this paper, we introduce a joint neural network model to

overcome the aforementioned issues and to automatically perform end-to-end relation

extraction without the need of any manual feature engineering or the use of additional

NLP components.

Neural network approaches have been considered to address the problem in a joint

setting (end-to-end relation extraction) and typically include the use of RNNs and

CNNs (Miwa & Bansal, 2016; Zheng et al., 2017; Li et al., 2017). Speciﬁcally, Miwa

& Bansal (2016) propose the use of bidirectional tree-structured RNNs to capture de-

pendency tree information (where parse trees are extracted using state-of-the-art de-

pendency parsers) which has been proven beneﬁcial for relation extraction (Xu et al.,

2015a,b). Li et al. (2017) apply the work of Miwa & Bansal (2016) to biomedical

text, reporting state-of-the-art performance for two biomedical datasets. Gupta et al.

(2016) propose the use of a lot of hand-crafted features along with RNNs. Adel &

Sch¨utze (2017) solve the entity classiﬁcation task (which is different from NER since

in entity classiﬁcation the boundaries of the entities are known and only the type of the

entity should be predicted) and relation extraction problems using an approximation of

a global normalization objective (i.e., CRF): they replicate the context of the sentence

(left and right part of the entities) to feed one entity pair at a time to a CNN for relation

extraction. Thus, they do not simultaneously infer other potential entities and relations

within the same sentence. Katiyar & Cardie (2017) and Bekoulis et al. (2018) inves-

tigate RNNs with attention for extracting relations between entity mentions without

using any dependency parse tree features. Different from Katiyar & Cardie (2017), in

this work, we frame the problem as a multi-head selection problem by using a sigmoid

loss to obtain multiple relations and a CRF loss for the NER component. This way,

we are able to independently predict classes that are not mutually exclusive, instead of

assigning equal probability values among the tokens. We overcome the issue of addi-

6

Figure 1: The multi-head selection model for joint entity and relation extraction. The input of our model is
the words of the sentence which are then represented as word vectors (i.e., embeddings). The BiLSTM layer
extracts a more complex representation for each word. Then the CRF and the sigmoid layers are able to
produce the outputs for the two tasks. The outputs for each token (e.g., Smith) are: (i) an entity recognition
label (e.g., I-PER) and (ii) a set of tuples comprising the head tokens of the entity and the types of relations
between them (e.g., {(Center, Works for), (Atlanta, Lives in)}).

tional complexity described by Bekoulis et al. (2018), by dividing the loss functions

into a NER and a relation extraction component. Moreover, we are able to handle mul-

tiple relations instead of just predicting single ones, as was described for the application

of structured real estate advertisements of Bekoulis et al. (2018).

3. Joint model

In this section, we present our multi-head joint model illustrated in Fig. 1. The model

is able to simultaneously identify the entities (i.e., types and boundaries) and all the

possible relations between them at once. We formulate the problem as a multi-head

selection problem extending previous work (Zhang et al., 2017; Bekoulis et al., 2018)

as described in Section 2.3. By multi-head, we mean that any particular entity may be

involved in multiple relations with other entities. The basic layers of the model, shown

in Fig. 1, are: (i) embedding layer, (ii) bidirectional sequential LSTM (BiLSTM) layer,

(iii) CRF layer and the (iv) sigmoid scoring layer. In Fig. 1, an example sentence from

7

Figure 2: Embedding layer in detail. The characters of the word “Man” are represented by character vectors
(i.e., embeddings) that are learned during training. The character embeddings are fed to a BiLSTM and the
two ﬁnal states (forward and backward) are concatenated. The vector wchars is the character-level repre-
sentation of the word. This vector is then further concatenated to the word-level representation wword2vec
to obtain the complete word embedding vector.

the CoNLL04 dataset is presented. The input of our model is a sequence of tokens

(i.e., words of the sentence) which are then represented as word vectors (i.e., word

embeddings). The BiLSTM layer is able to extract a more complex representation for

each word that incorporates the context via the RNN structure. Then the CRF and the

sigmoid layers are able to produce the outputs for the two tasks. The outputs for each

token (e.g., Smith) are twofold: (i) an entity recognition label (e.g., I-PER, denoting

the token is inside a named entity of type PER) and (ii) a set of tuples comprising the

head tokens of the entity and the types of relations between them (e.g., {(Center, Works

for), (Atlanta, Lives in)}). Since we assume token-based encoding, we consider only

the last token of the entity as head of another token, eliminating redundant relations.

For instance, there is a Works for relation between entities “John Smith” and “Disease

Control Center”.

Instead of connecting all tokens of the entities, we connect only

“Smith” with “Center”. Also, for the case of no relation, we introduce the “N” label

and we predict the token itself as the head.

3.1. Embedding layer

Given a sentence w = w1, ..., wn as a sequence of tokens, the word embedding layer

is responsible to map each token to a word vector (wword2vec). We use pre-trained word

embeddings using the Skip-Gram word2vec model (Mikolov et al., 2013).

8

In this work, we also use character embeddings since they are commonly applied

to neural NER (Ma & Hovy, 2016; Lample et al., 2016). This type of embeddings is

able to capture morphological features such as preﬁxes and sufﬁxes. For instance, in

the Adverse Drug Events (ADE) dataset, the sufﬁx “toxicity” can specify an adverse

drug event entity such as “neurotoxicity” or “hepatotoxicity” and thus it is very infor-

mative. Another example might be the Dutch sufﬁx “kamer” (“room” in English) in the

Dutch Real Estate Classiﬁeds (DREC) dataset which is used to specify the space en-

tities “badkamer” (“bathroom” in English) and “slaapkamer” (“bedroom” in English).

Character-level embeddings are learned during training, similar to Ma & Hovy (2016)

and Lample et al. (2016). In the work of Lample et al. (2016), character embeddings

lead to a performance improvement of up to 3% in terms of NER F1 score. In our

work, by incorporating character embeddings, we report in Table 2 an increase of ∼2%

overall F1 scoring points. For more details, see Section 5.2.

Figure 2 illustrates the neural architecture for word embedding generation based

on its characters. The characters of each word are represented by character vectors

(i.e., embeddings). The character embeddings are fed to a BiLSTM and the two ﬁnal

states (forward and backward) are concatenated. The vector wchars is the character-level

representation of the word. This vector is then further concatenated to the word-level

representation wword2vec to obtain the complete word embedding vector.

3.2. Bidirectional LSTM encoding layer

RNNs are commonly used in modeling sequential data and have been successfully

applied in various NLP tasks (Sutskever et al., 2014; Lample et al., 2016; Miwa &

Bansal, 2016). In this work, we use multi-layer LSTMs, a speciﬁc kind of RNNs which

are able to capture long term dependencies well (Bengio et al., 1994; Pascanu et al.,

2013). We employ a BiLSTM which is able to encode information from left to right

(past to future) and right to left (future to past). This way, we can combine bidirectional
information for each word by concatenating the forward ( (cid:126)hi) and the backward ( (cid:126)hi)

output at timestep i. The BiLSTM output at timestep i can be written as:

hi = [ (cid:126)hi; (cid:126)hi],

i = 0, ..., n

(1)

9

3.3. Named entity recognition

We formulate the entity identiﬁcation task as a sequence labeling problem, similar to

previous work on joint learning models (Miwa & Bansal, 2016; Li et al., 2017; Katiyar

& Cardie, 2017) and named entity recognition (Lample et al., 2016; Ma & Hovy, 2016)

using the BIO (Beginning, Inside, Outside) encoding scheme. Each entity consists of

multiple sequential tokens within the sentence and we should assign a tag for every

token in the sentence. That way we are able to identify the entity arguments (start and

end position) and its type (e.g., ORG). To do so, we assign the B-type (beginning) to

the ﬁrst token of the entity, the I-type (inside) to every other token within the entity

and the O tag (outside) if a token is not part of an entity. Fig. 1 shows an example

of the BIO encoding tags assigned to the tokens of the sentence. In the CRF layer,

one can observe that we assign the B-ORG and I-ORG tags to indicate the beginning

and the inside tokens of the entity “Disease Control Center”, respectively. On top of

the BiLSTM layer, we employ either a softmax or a CRF layer to calculate the most

probable entity tag for each token. We calculate the score of each token wi for each

entity tag:

s(e)(hi) = V (e)f (U (e)hi + b(e))

(2)

where the superscript (e) is used for the notation of the NER task, f (·) is an element-
wise activation function (i.e., relu, tanh), V (e) ∈ Rp×l, U (e) ∈ Rl×2d, b(e) ∈ Rl,

with d as the hidden size of the LSTM, p the number of NER tags (e.g., B-ORG) and

l the layer width. We calculate the probabilities of all the candidate tags for a given
token wi as Pr(tag | wi) = softmax(s(hi)) where Pr(tag | wi) ∈ Rp. In this work,

we employ the softmax approach only for the entity classiﬁcation (EC) task (which is

similar to NER) where we need to predict only the entity types (e.g., PER) for each

token assuming boundaries are given. The CRF approach is used for the NER task

which includes both entity type and boundaries recognition.

In the softmax approach, we assign entity types to tokens in a greedy way at pre-

diction time (i.e., the selected tag is just the highest scoring tag over all possible set

of tags). Although assuming an independent tag distribution is beneﬁcial for entity

classiﬁcation tasks (e.g., POS tagging), this is not the case when there are strong de-

10

pendencies between the tags. Speciﬁcally, in NER, the BIO tagging scheme forces

several restrictions (e.g., B-LOC cannot be followed by I–PER). The softmax method

allows local decisions (i.e., for the tag of each token wi) even though the BiLSTM

captures information about the neighboring words. Still, the neighboring tags are not

taken into account for the tag decision of a speciﬁc token. For example, in the entity

“John Smith”, tagging “Smith” as PER is useful for deciding that “John” is B-PER. To

this end, for NER, we use a linear-chain CRF, similar to Lample et al. (2016) where

an improvement of ∼1% F1 NER points is reported when using CRF. In our case, with

the use of CRF we also report a ∼1% overall performance improvement as observed

in Table 2 (see Section 5.2). Assuming the word vector w, a sequence of score vectors
1 , ..., s(e)
s(e)
deﬁned as:

n and a vector of tag predictions y(e)

n , the linear-chain CRF score is

1 , ..., y(e)

(cid:16)

S

y(e)
1 , . . . , y(e)
n

(cid:17)

=

n
(cid:88)

i=0

s(e)
i,y(e)
i

+

n−1
(cid:88)

i=1

Ty(e)

i

,y(e)
i+1

where S ∈ R, s(e)

i,y(e)
i

is the score of the predicted tag for token wi, T is a square tran-

sition matrix in which each entry represents transition scores from one tag to another.
T ∈ R(p+2)×(p+2) because y(e)
0

n are two auxiliary tags that represent the start-

and y(e)

ing and the ending tags of the sentence, respectively. Then, the probability of a given

sequence of tags over all possible tag sequences for the input sentence w is deﬁned as:

(cid:16)

Pr

y(e)
1 , . . . , y(e)
n

(cid:17)

(cid:12)
(cid:12)
(cid:12) w

=

eS(y(e)

1 ,...,y(e)
n )

(cid:80)

eS( ˜y1

(e),..., ˜yn

(e))

˜y1

(e),..., ˜yn

(e)

We apply Viterbi to obtain the tag sequence ˆy(e) with the highest score. We train

both the softmax (for the EC task) and the CRF layer (for NER) by minimizing the

cross-entropy loss LNER. We also use the entity tags as input to our relation extraction

layer by learning label embeddings, motivated by Miwa & Bansal (2016) where an im-

provement of 2% F1 is reported (with the use of label embeddings). In our case, label

embeddings lead to an increase of 1% F1 score as reported in Table 2 (see Section 5.2).

The input to the next layer is twofold: the output states of the LSTM and the learned

label embedding representation, encoding the intuition that knowledge of named enti-

(3)

(4)

11

ties can be useful for relation extraction. During training, we use the gold entity tags,

while at prediction time we use the predicted entity tags as input to the next layer. The

input to the next layer is the concatenation of the hidden LSTM state hi with the label

embedding gi for token wi:

zi = [hi; gi],

i = 0, ..., n

(5)

3.4. Relation extraction as multi-head selection

In this subsection, we describe the relation extraction task, formulated as a multi-head

selection problem (Zhang et al., 2017; Bekoulis et al., 2018). In the general formulation

of our method, each token wi can have multiple heads (i.e., multiple relations with

other tokens). We predict the tuple (ˆyi, ˆci) where ˆyi is the vector of heads and ˆci is

the vector of the corresponding relations for each token wi. This is different for the

previous standard head selection for dependency parsing method (Zhang et al., 2017)

since (i) it is extended to predict multiple heads and (ii) the decisions for the heads and

the relations are jointly taken (i.e., instead of ﬁrst predicting the heads and then in a next

step the relations by using an additional classiﬁer). Given as input a token sequence w

and a set of relation labels R, our goal is to identify for each token wi, i ∈ {0, ..., n}

the vector of the most probable heads ˆyi ⊆ w and the vector of the most probable

corresponding relation labels ˆri ⊆ R. We calculate the score between tokens wi and

wj given a label rk as follows:

s(r)(zj, zi, rk) = V (r)f (U (r)zj + W (r)zi + b(r))

(6)

where the superscript (r) is used for the notation of the relation task, f (·) is an element-
wise activation function (i.e., relu, tanh), V (r) ∈ Rl, U (r) ∈ Rl×(2d+b), W (r) ∈
Rl×(2d+b), b(r) ∈ Rl, d is the hidden size of the LSTM, b is the size of the label

embeddings and l the layer width. We deﬁne

Pr(head = wj, label = rk | wi) = σ(s(r)(zj, zi, rk))

(7)

to be the probability of token wj to be selected as the head of token wi with the relation

label rk between them, where σ(.) stands for the sigmoid function. We minimize the

12

cross-entropy loss Lrel during training:

n
(cid:88)

m
(cid:88)

i=0

j=0

Lrel =

− log Pr(head = yi,j, relation = ri,j | wi)

(8)

where yi ⊆ w and ri ⊆ R are the ground truth vectors of heads and associated relation

labels of wi and m is the number of relations (heads) for wi. After training, we keep

the combination of heads ˆyi and relation labels ˆri exceeding a threshold based on the

estimated joint probability as deﬁned in Eq. (7). Unlike previous work on joint mod-

els (Katiyar & Cardie, 2017), we are able to predict multiple relations considering the

classes as independent and not mutually exclusive (the probabilities do not necessarily

sum to 1 for different classes). For the joint entity and relation extraction task, we

calculate the ﬁnal objective as LNER + Lrel.

3.5. Edmonds’ algorithm

Our model is able to simultaneously extract entity mentions and the relations between

them. To demonstrate the effectiveness and the general purpose nature of our model, we

also test it on the recently introduced Dutch real estate classiﬁeds (DREC) dataset (Bek-

oulis et al., 2017) where the entities need to form a tree structure. By using thresholded

inference, a tree structure of relations is not guaranteed. Thus we should enforce tree

structure constraints to our model. To this end, we post-process the output of our sys-

tem with Edmonds’ maximum spanning tree algorithm for directed graphs (Chu & Liu,

1965; Edmonds, 1967). A fully connected directed graph G = (V, E) is constructed,

where the vertices V represent the last tokens of the identiﬁed entities (as predicted

by NER) and the edges E represent the highest scoring relations with their scores as

weights. Edmonds’ algorithm is applied in cases a tree is not already formed by thresh-

olded inference.

4. Experimental setup

4.1. Datasets and evaluation metrics

We conduct experiments on four datasets: (i) Automatic Content Extraction, ACE04 (Dod-

dington et al., 2004), (ii) Adverse Drug Events, ADE (Gurulingappa et al., 2012b),

13

(iii) Dutch Real Estate Classiﬁeds, DREC (Bekoulis et al., 2017) and (iv) the CoNLL’04

dataset with entity and relation recognition corpora (Roth & Yih, 2004). Our code is

available in our github codebase.2

ACE04: There are seven main entity types namely Person (PER), Organization (ORG),

Geographical Entities (GPE), Location (LOC), Facility (FAC), Weapon (WEA) and Ve-

hicle (VEH). Also, the dataset deﬁnes seven relation types: Physical (PHYS), Person-

Social (PER-SOC), Employment-Membership-Subsidiary (EMP-ORG), Agent-Artifact (ART),

PER-ORG afﬁliation (Other-AFF), GPE afﬁliation (GPE-AFF), and Discourse (DISC).

We follow the cross-validation setting of Li & Ji (2014) and Miwa & Bansal (2016).

We removed DISC and did 5-fold cross-validation on the bnews and nwire subsets (348

documents). We obtained the preprocessing script from Miwa’s github codebase.3 We

measure the performance of our system using micro F1 scores, Precision and Recall on

both entities and relations. We treat an entity as correct when the entity type and the

region of its head are correct. We treat a relation as correct when its type and argument

entities are correct, similar to Miwa & Bansal (2016) and Katiyar & Cardie (2017). We

refer to this type of evaluation as strict.4 We select the best hyperparameter values on

a randomly selected validation set for each fold, selected from the training set (15% of

the data) since there are no ofﬁcial train and validation splits in the work of Miwa &

Bansal (2016).

CoNLL04: There are four entity types in the dataset (Location, Organization, Person,

and Other) and ﬁve relation types (Kill, Live in, Located in, OrgBased in and Work

for). We use the splits deﬁned by Gupta et al. (2016) and Adel & Sch¨utze (2017). The

dataset consists of 910 training instances, 243 for validation and 288 for testing.5 We

measure the performance by computing the F1 score on the test set. We adopt two

evaluation settings to compare to previous work. Speciﬁcally, we perform an EC task

assuming the entity boundaries are given similar to Gupta et al. (2016) and Adel &

2https://github.com/bekou/multihead_joint_entity_relation_extraction
3https://github.com/tticoin/LSTM-ER/tree/master/data/ace2004
4For the CoNLL04, DREC and ADE datasets, the head region covers the whole entity (start and end

boundaries). The ACE04 already deﬁnes the head region of an entity.

5http://cistern.cis.lmu.de/globalNormalization/globalNormalization_

all.zip

14

Sch¨utze (2017). To obtain comparable results, we omit the entity class “Other” when

computing the EC score. We score a multi-token entity as correct if at least one of its

comprising token types is correct assuming that the boundaries are given; a relation

is correct when the type of the relation and the argument entities are both correct. We

report macro-average F1 scores for EC and RE to obtain comparable results to previous

studies. Moreover, we perform actual NER evaluation instead of just EC, reporting

results using the strict evaluation metric.

DREC: The dataset consists of 2,318 classiﬁeds as described in the work of Bekoulis

et al. (2018). There are 9 entity types: Neighborhood, Floor, Extra building, Subspace,

Invalid, Field, Other, Space and Property. Also, there are two relation classes Part-of

and Equivalent. The goal is to identify important entities of a property (e.g., ﬂoors,

spaces) from classiﬁeds and structuring them into a tree format to get the structured

description of the property. For the evaluation, we use 70% for training, 15% for

validation and 15% as test set in the same splits as deﬁned in Bekoulis et al. (2018).

We measure the performance by computing the F1 score on the test set. To compare our

results with previous work (Bekoulis et al., 2018), we use the boundaries evaluation

setting. In this setting, we count an entity as correct if the boundaries of the entity are

correct. A relation is correct when the relation is correct and the argument entities are

both correct. Also, we report results using the strict evaluation for future reference.

ADE: There are two types of entities (drugs and diseases) in this dataset and the aim of

the task is to identify the types of entities and relate each drug with a disease (adverse

drug events). There are 6,821 sentences in total and similar to previous work (Li et al.,

2016, 2017), we remove ∼130 relations with overlapping entities (e.g., “lithium” is a

drug which is related to “lithium intoxication”). Since there are no ofﬁcial sets, we

evaluate our model using 10-fold cross-validation where 10% of the data was used as

validation and 10% for test set similar to Li et al. (2017). The ﬁnal results are displayed

in F1 metric as a macro-average across the folds. The dataset consists of 10,652 entities

and 6,682 relations. We report results similar to previous work on this dataset using the

strict evaluation metric.

15

4.2. Word embeddings

We use pre-trained word2vec embeddings used in previous work, so as to retain the

same inputs for our model and to obtain comparable results that are not affected by the

input embeddings. Speciﬁcally, we use the 200-dimensional word embeddings used in

the work of Miwa & Bansal (2016) for the ACE04 dataset6 trained on Wikipedia. We

obtained the 50-dimensional word embeddings used by Adel & Sch¨utze (2017)5 trained

also on Wikipedia for the CoNLL04 corpus. We use the 128-dimensional word2vec

embeddings used by Bekoulis et al. (2018) trained on a large collection of 887k Dutch

property advertisements7 for the DREC dataset. Finally, for the ADE dataset, we used

200-dimensional embeddings used by Li et al. (2017) and trained on a combination of

PubMed and PMC texts with texts extracted from English Wikipedia (Moen & Anani-

adou, 2013)8.

4.3. Hyperparameters and implementation details

We have developed our joint model by using Python with the TensorFlow machine

learning library (Abadi et al., 2016). Training is performed using the Adam opti-

mizer (Kingma & Ba, 2015) with a learning rate of 10−3. We ﬁx the size of the LSTM

to d = 64 and the layer width of the neural network to l = 64 (both for the entity and

the relation scoring layers). We use dropout (Srivastava et al., 2014) to regularize our

network. Dropout is applied in the input embeddings and in between the hidden layers

for both tasks. Different dropout rates have been applied but the best dropout values

(0.2 to 0.4) for each dataset have been used. The hidden dimension for the character-

based LSTMs is 25 (for each direction). We also ﬁxed our label embeddings to be of

size b = 25 for all the datasets except for CoNLL04 where the label embeddings were

not beneﬁcial and thus were not used. We experimented with tanh and relu activation

functions (recall that this is the function f (·) from the model description). We use the

6http://tti-coin.jp/data/wikipedia200.bin
7https://drive.google.com/uc?id=1Dvibr-Ps4G_GI6eDx9bMXnJphGhH_M1z&

8http://evexdb.org/pmresources/vec-space-models/wikipedia-pubmed-and-PMC-w2v.

export=download

bin

16

Settings

Evaluation

P

Relation
R

F1

Overall F1

E
C
A

4
0

Miwa & Bansal (2016)
Katiyar & Cardie (2017)
multi-head

L
L
N
o
C

4
0

C
E
R
D

E
D
A

Gupta et al. (2016)
Gupta et al. (2016)
Adel & Sch¨utze (2017)
multi-head EC
Miwa & Sasaki (2014)
multi-head

Bekoulis et al. (2018)
multi-head+E
single-head
multi-head

Li et al. (2016)
Li et al. (2017)
multi-head

Pre-calculated
Features
(cid:51)
(cid:55)
(cid:55)

(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:55)

(cid:55)
(cid:55)
(cid:55)
(cid:55)

(cid:51)
(cid:51)
(cid:55)

strict
strict
strict

relaxed
relaxed
relaxed
relaxed
strict
strict

boundaries
boundaries
strict
strict

strict
strict
strict

80.80
81.20
81.01

92.50
88.50
-
93.41
81.20
83.75

77.93
79.84
78.80
78.97

79.50
82.70
84.72

Entity
R

82.90
78.10
81.31

92.10
88.90
-
93.15
80.20
84.06

80.31
84.92
84.26
83.98

79.60
86.70
88.16

F1

81.80
79.60
81.16

92.40
88.80
82.10
93.26
80.70
83.90

79.11
82.30
81.43
81.39

79.50
84.60
86.40

P

48.70
46.40
50.14

78.50
64.60
-
72.99
76.00
63.75

49.24
50.52
50.57
50.00

64.00
67.50
72.10

48.10
45.53
44.48

63.00
53.10
-
63.37
50.90
60.43

50.17
55.30
54.30
54.73

62.90
75.80
77.24

48.40
45.70
47.14

69.90
58.30
62.50
67.01
61.00
62.04

49.70
52.81
52.37
52.26

63.40
71.40
74.58

65.10
62.65
64.15

81.15
73.60
72.30
80.14
70.85
72.97

64.41
67.56
66.90
66.83

71.45
78.00
80.49

Table 1: Comparison of our method (multi-head) with the state-of-the-art on the ACE04, CoNLL04, DREC
and ADE datasets. The models: (i) multi-head+E (the model + the Edmond algorithm to produce a tree-
structured output), (ii) single-head (the model predicts only one head per token) and (iii) multi-head EC (the
model predicts only the entity classes assuming that the boundaries are given) are slight variations of the
multi-head model adapted for each dataset and evaluation. The (cid:51)and (cid:55) symbols indicate whether or not the
models rely on any hand-crafted features or additional tools. Note that all the variations of our models do not
rely on any additional features. We include here different evaluation types (strict, relaxed and boundaries)
to be able to compare our results against previous studies. Finally, we report results in terms of Precision,
Recall, F1 for the two subtasks as well as overall F1, averaging over both subtasks. Bold entries indicate the
best result among models that only consider automatically learned features.

relu activation only in the ACE04 and tanh in all other datasets. We employ the tech-

nique of early stopping based on the validation set. In all the datasets examined in this

study, we obtain the best hyperparameters after 60 to 200 epochs depending on the size

of the dataset. We select the best epoch according to the results in the validation set.

For more details about the effect of each hyperparameter to the model performance see

the Appendix.

5. Results and discussion

5.1. Results

In Table 1, we present the results of our analysis. The ﬁrst column indicates the con-

sidered dataset. In the second column, we denote the model which is applied (i.e.,

previous work and the proposed models). The proposed models are the following:

(i) multi-head is the proposed model with the CRF layer for NER and the sigmoid loss

for multiple head prediction, (ii) multi-head+E is the proposed model with addition

of Edmonds’ algorithm to guarantee a tree-structured output for the DREC dataset,

17

(iii) single-head is the proposed method but it predicts only one head per token using a

softmax loss instead of a sigmoid, and (iv) multi-head EC is the proposed method with

a softmax to predict the entity classes assuming that the boundaries are given, and the

sigmoid loss for multiple head selection. Table 1 also indicates whether the different

settings include hand-crafted features or features derived from NLP tools (e.g., POS
taggers, dependency parsers). We use the (cid:51) symbol to denote that the model includes
this kind of additional features and the (cid:55) symbol to denote that the model is only based

on automatically extracted features. Note that all the variations of our model do not

rely on any additional features. In the next column, we declare the type of evaluation

conducted for each experiment. We include here different evaluation types to be able

to compare our results against previous studies. Speciﬁcally, we use three evaluation

types, namely:

(i) Strict: an entity is considered correct if the boundaries and the type of the en-

tity are both correct; a relation is correct when the type of the relation and the

argument entities are both correct,

(ii) Boundaries: an entity is considered correct if only the boundaries of the entity

are correct (entity type is not considered); a relation is correct when the type of

the relation and the argument entities are both correct and

(iii) Relaxed: we score a multi-token entity as correct if at least one of its comprising

token types is correct assuming that the boundaries are given; a relation is correct

when the type of the relation and the argument entities are both correct.

In the next three columns, we present the results for the entity identiﬁcation task (Pre-

cision, Recall, F1) and then (in the subsequent three columns) the results of the relation

extraction task (Precision, Recall, F1). Finally, in the last column, we report an addi-

tional F1 measure which is the average F1 performance of the two subtasks. We mark

with bold font in Table 1, the best result for each dataset among those models that use

only automatically extracted features.

Considering the results in the ACE04, we observe that our model outperforms the

model of Katiyar & Cardie (2017) by ∼2% in both tasks. This improvement can be

18

explained by the use of the multi-head selection method which can naturally capture

multiple relations and model them as a multi-label problem. Unlike the work of Kati-

yar & Cardie (2017), the class probabilities do not necessarily sum up to one since the

classes are considered independent. Moreover, we use a CRF-layer to model the NER

task to capture dependencies between sequential tokens. Finally, we obtain more ef-

fective word representations by using character-level embeddings. On the other hand,

our model performs within a reasonable margin (∼0.5% for the NER task and ∼1%

for the RE task) compared to Miwa & Bansal (2016). This difference is explained by

the fact that the model of Miwa & Bansal (2016) relies on POS tagging and syntactic

features derived by dependency parsing. However, this kind of features relies on NLP

tools that are not always accurate for various languages and contexts. For instance, the

same model is adopted by the work of Li et al. (2017) for the ADE biomedical dataset

and in this dataset our model reports more than 3% improvement in the RE task. This

shows that our model is able to produce automatically extracted features which perform

reasonably well in all contexts (e.g., news, biomedical).

For the CoNLL04 dataset, there are two different evaluation settings, namely re-

laxed and strict. In the relaxed setting, we perform an EC task instead of NER as-

suming that the boundaries of the entities are given. We adopt this setting to produce

comparable results with previous studies (Gupta et al., 2016; Adel & Sch¨utze, 2017).

Similar to Adel & Sch¨utze (2017), we present results of single models and no ensem-

bles. We observe that our model outperforms all previous models that do not rely on

complex hand-crafted features by a large margin (>4% for both tasks). Unlike these

previous studies that consider pairs of entities to obtain the entity types and the cor-

responding relations, we model the whole sentence at once. That way, our method is

able to directly infer all entities and relations of a sentence and beneﬁt from their pos-

sible interactions that cannot be modeled when training is performed for each entity

pair individually, one at a time. In the same setting, we also report the results of Gupta

et al. (2016) in which they use multiple complicated hand-crafted features coming from

NLP tools. Our model performs slightly better for the EC task and within a margin of

1% in terms of overall F1 score. The difference in the overall performance is due to

the fact that our model uses only automatically generated features. We also report re-

19

sults on the same dataset conducting NER (i.e., predicting entity types and boundaries)

and evaluating using the strict evaluation measure, similar to Miwa & Sasaki (2014).

Our results are not directly comparable to the work of Miwa & Sasaki (2014) because

we use the splits provided by Gupta et al. (2016). However, in this setting we present

the results from Miwa & Sasaki (2014) as reference. We report an improvement of

∼2% overall F1 score, which suggests that our neural model is able to extract more

informative representations compared to feature-based approaches.

We also report results for the DREC dataset, with two different evaluation settings.

Speciﬁcally, we use the boundaries and the strict settings. We transform the previous

results from Bekoulis et al. (2018) to the boundaries setting to make them comparable

to our model since in their work, they report token-based F1 score, which is not a

common evaluation metric in relation extraction problems. Also, in their work, they

focus on identifying only the boundaries of the entities and not the types (e.g., Floor,

Space). In the boundaries evaluation, we achieve ∼3% improvement for both tasks.

This is due to the fact that their quadratic scoring layer is beneﬁcial for the RE task, yet

complicates NER, which is usually modeled as a sequence labeling task. Moreover,

we report results using the strict evaluation which is used in most related works. Using

the prior knowledge that each entity has only one head, we can simplify our model and

predict only one head each time (i.e., using a softmax loss). The difference between

the single and the multi-head models is marginal (<0.1% for both tasks). This shows

that our model (multi-head) can adapt to various environments, even if the setting is

single head (in terms of the application, and thus also in both training and test data).

Finally, we compare our model with previous work (Li et al., 2016, 2017) on the

ADE dataset. The previous models (Li et al., 2016, 2017) both use hand-crafted fea-

tures or features derived from NLP tools. However, our model is able to outperform

both models using the strict evaluation metric. We report an improvement of ∼2% in

the NER and ∼3% in the RE tasks, respectively. The work of Li et al. (2017) is similar

to Miwa & Bansal (2016) and strongly relies on dependency parsers to extract syntactic

information. A possible explanation for the better result obtained from our model is

that the pre-calculated syntactic information obtained using external tools either is not

so accurate or important for biomedical data.

20

Settings

Multi-head
−Label embeddings
−Character embeddings
−CRF loss

P

81.01
80.61
80.42
80.47

Entity
R

81.31
80.91
79.52
81.50

F1

81.16
80.77
79.97
80.98

P

50.14
50.00
49.06
47.34

Relation
R

F1

Overall F1

44.48
42.92
41.62
42.84

47.14
46.18
45.04
44.98

64.15
63.48
62.50
62.98

Table 2: Ablation tests on the ACE04 test dataset.

5.2. Analysis of feature contribution

We conduct ablation tests on the ACE04 dataset reported in Table 2 to analyze the

effectiveness of the various parts of our joint model. The performance of the RE task

decreases (∼1% in terms of F1 score) when we remove the label embeddings layer and

only use the LSTM hidden states as inputs for the RE task. This shows that the NER

labels, as expected, provide meaningful information for the RE component.

Removing character embeddings also degrades the performance of both NER (∼1%)

and RE (∼2%) tasks by a relatively large margin. This illustrates that composing words

by the representation of characters is effective, and our method beneﬁts from additional

information such as capital letters, sufﬁxes and preﬁxes within the token (i.e., its char-

acter sequences).

Finally, we conduct experiments for the NER task by removing the CRF loss layer

and substituting it with a softmax. Assuming independent distribution of labels (i.e.,

softmax) leads to a slight decrease in the F1 performance of the NER module and a

∼2% decrease in the performance of the RE task. This happens because the CRF loss

is able to capture the strong tag dependencies (e.g., I-LOC cannot follow B-PER) that

are present in the dataset instead of just assuming that the tag decision for each token

is independent from tag decisions of neighboring tokens.

6. Conclusion

In this work, we present a joint neural model to simultaneously extract entities and

relations from textual data. Our model comprises a CRF layer for the entity recogni-

tion task and a sigmoid layer for the relation extraction task. Speciﬁcally, we model

the relation extraction task as a multi-head selection problem since one entity can have

21

multiple relations. Previous models on this task rely heavily on external NLP tools (i.e.,

POS taggers, dependency parsers). Thus, the performance of these models is affected

by the accuracy of the extracted features. Unlike previous studies, our model produces

automatically generated features rather than relying on hand-crafted ones, or existing

NLP tools. Given its independence from such NLP or other feature generating tools,

our approach can be easily adopted for any language and context. We demonstrate

the effectiveness of our approach by conducting a large scale experimental study. Our

model is able to outperform neural methods that automatically generate features while

the results are marginally similar (or sometimes better) compared to feature-based neu-

ral network approaches.

As future work, we aim to explore the effectiveness of entity pre-training for the en-

tity recognition module. This approach has been proven beneﬁcial in the work of Miwa

& Bansal (2016) for both the entity and the relation extraction modules. In addition,

we are planning to explore a way to reduce the calculations in the quadratic relation

scoring layer. For instance, a straightforward way to do so is to use in the sigmoid layer

only the tokens that have been identiﬁed as entities.

References

Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat,

S., Irving, G., Isard, M., Kudlur, M., Levenberg, J., Monga, R., Moore, S., Murray,

D. G., Steiner, B., Tucker, P., Vasudevan, V., Warden, P., Wicke, M., Yu, Y., & Zheng,

X. (2016). Tensorﬂow: A system for large-scale machine learning. In Proceedings

of the 12th USENIX Conference on Operating Systems Design and Implementation

(pp. 265–283). Berkeley, CA, USA.

Adel, H., & Sch¨utze, H. (2017). Global normalization of convolutional neural networks

for joint entity and relation classiﬁcation. In Proceedings of the 2017 Conference

on Empirical Methods in Natural Language Processing. Copenhagen, Denmark:

Association for Computational Linguistics.

Bach, N., & Badaskar, S. (2007). A review of relation extraction. Literature review for

Language and Statistics II, .

22

Bekoulis, G., Deleu, J., Demeester, T., & Develder, C. (2017). Reconstructing the

house from the ad: Structured prediction on real estate classiﬁeds. In Proceedings of

the 15th Conference of the European Chapter of the Association for Computational

Linguistics: (Volume 2, Short Papers) (pp. 274–279). Valencia, Spain.

Bekoulis, G., Deleu, J., Demeester, T., & Develder, C. (2018). An attentive neural

architecture for joint segmentation and parsing and its application to real estate ads.

Expert Systems with Applications, 102, 100 – 112. doi:10.1016/j.eswa.2018.

02.031.

Bengio, Y., Simard, P., & Frasconi, P. (1994). Learning long-term dependencies

with gradient descent is difﬁcult. Transactions on neural networks, 5(2), 157–166.

doi:10.1109/72.279181.

Sinica, 14, 1396–1400.

Chu, Y.-J., & Liu, T.-H. (1965). On shortest arborescence of a directed graph. Scientia

Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011).

Natural language processing (almost) from scratch. Journal of Machine Learning

Research, 12, 2493–2537.

Culotta, A., & Sorensen, J. (2004). Dependency tree kernels for relation extraction. In

Proceedings of the 42nd Annual Meeting on Association for Computational Linguis-

tics (pp. 423–429). Barcelona, Spain. doi:10.3115/1218955.1219009.

Doddington, G. R., Mitchell, A., Przybocki, M. A., Ramshaw, L. A., Strassel, S., &

Weischedel, R. M. (2004). The automatic content extraction (ace) program-tasks,

data, and evaluation. In Proceedings Fourth International Conference on Language

Resources and Evaluation (p. 1). Lisbon, Portugal volume 2.

Edmonds, J. (1967). Optimum branchings. Journal of research of the National Bureau

of Standards, 71B(4), 233–240.

Fundel, K., Kffner, R., & Zimmer, R.

(2007).

Relex-relation extraction

using dependency parse trees.

Bioinformatics, 23(3), 365–371. doi:10.

1093/bioinformatics/btl616.

23

Gupta, P., Sch¨utze, H., & Andrassy, B. (2016). Table ﬁlling multi-task recurrent neural

network for joint entity and relation extraction. In Proceedings of COLING 2016,

the 26th International Conference on Computational Linguistics: Technical Papers

(pp. 2537–2547).

Gurulingappa, H., MateenRajpu, A., & Toldo, L. (2012a). Extraction of potential

adverse drug events from medical case reports. Journal of Biomedical Semantics,

3(1), 1–15. doi:10.1186/2041-1480-3-15.

Gurulingappa, H., Rajput, A. M., Roberts, A., Fluck, J., Hofmann-Apitius, M., &

Toldo, L. (2012b). Development of a benchmark corpus to support the auto-

matic extraction of drug-related adverse effects from medical case reports. Jour-

nal of Biomedical Informatics, 45(5), 885 – 892. doi:https://doi.org/10.

1016/j.jbi.2012.04.008.

Huang, Z., Xu, W., & Yu, K. (2015). Bidirectional LSTM-CRF models for sequence

tagging. arXiv preprint arXiv:1508.01991, .

Kambhatla, N. (2004). Combining lexical, syntactic, and semantic features with maxi-

mum entropy models for extracting relations. In Proceedings of the Annual Meeting

of the Association for Computational Linguistics on Interactive poster and demon-

stration sessions. Barcelona, Spain. doi:10.3115/1219044.1219066.

Kate, R. J., & Mooney, R. (2010).

Joint entity and relation extraction using card-

pyramid parsing. In Proceedings of the 14th Conference on Computational Natural

Language Learning (pp. 203–212). Uppsala, Sweden: Association for Computa-

tional Linguistics.

Katiyar, A., & Cardie, C. (2017). Going out on a limb: Joint extraction of entity

mentions and relations without dependency trees. In Proceedings of the 55st Annual

Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).

Vancouver, Canada.

Kingma, D., & Ba, J. (2015). Adam: A method for stochastic optimization. In Inter-

national Conference on Learning Representations. San Diego, USA.

24

Lafferty, J., McCallum, A., & Pereira, F. (2001). Conditional random ﬁelds: Proba-

bilistic models for segmenting and labeling sequence data. In Proceedings of the

18th International Conference on Machine Learning (pp. 282–289). San Francisco,

USA: Morgan Kaufmann.

Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K., & Dyer, C. (2016).

Neural architectures for named entity recognition. In Proceedings of the 2016 Con-

ference of the North American Chapter of the Association for Computational Lin-

guistics: Human Language Technologies (pp. 260–270). San Diego, California.

Li, F., Zhang, M., Fu, G., & Ji, D. (2017). A neural joint model for entity

and relation extraction from biomedical text. BMC Bioinformatics, 18(1), 1–11.

doi:10.1186/s12859-017-1609-9.

Li, F., Zhang, Y., Zhang, M., & Ji, D. (2016). Joint models for extracting adverse

drug events from biomedical text. In Proceedings of the Twenty-Fifth International

Joint Conference on Artiﬁcial Intelligence (pp. 2838–2844). New York, USA: IJ-

CAI/AAAI Press.

Li, Q., & Ji, H. (2014). Incremental joint extraction of entity mentions and relations.

In Proceedings of the 52nd Annual Meeting of the Association for Computational

Linguistics (Volume 1: Long Papers) (pp. 402–412). Baltimore, USA.

Ma, X., & Hovy, E. (2016). End-to-end sequence labeling via bi-directional LSTM-

CNNs-CRF. In Proceedings of the 54th Annual Meeting of the Association for Com-

putational Linguistics (Volume 1: Long Papers) (pp. 1064–1074). Berlin, Germany.

Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed

representations of words and phrases and their compositionality. In Proceedings of

the 26th International Conference on Neural Information Processing Systems (pp.

3111–3119). Nevada, United States: Curran Associates, Inc.

Miwa, M., & Bansal, M. (2016). End-to-end relation extraction using LSTMs on se-

quences and tree structures. In Proceedings of the 54th Annual Meeting of the As-

25

sociation for Computational Linguistics (Volume 1: Long Papers) (pp. 1105–1116).

Berlin, Germany.

Miwa, M., & Sasaki, Y. (2014). Modeling joint entity and relation extraction with

table representation. In Proceedings of the 2014 Conference on Empirical Methods

in Natural Language Processing (pp. 1858–1869). Doha, Qatar: Association for

Computational Linguistics.

Moen, S., & Ananiadou, T. S. S. (2013). Distributional semantics resources for biomed-

ical text processing.

In Proceedings of the 5th International Symposium on Lan-

guages in Biology and Medicine (pp. 39–43). Tokyo, Japan.

Nadeau, D., & Sekine, S. (2007). A survey of named entity recognition and classiﬁca-

tion. Lingvisticae Investigationes, 30(1), 3–26. doi:10.1075/li.30.1.03nad.

Pascanu, R., Mikolov, T., & Bengio, Y. (2013). On the difﬁculty of training recurrent

neural networks. In Proceedings of the 30th International Conference on Interna-

tional Conference on Machine Learning (pp. 1310–1318). Atlanta, USA: JMLR.org.

Rink, B., & Harabagiu, S. (2010). Utd: Classifying semantic relations by combining

lexical and semantic resources. In Proceedings of the 5th International Workshop

on Semantic Evaluation (pp. 256–259). Los Angeles, California: Association for

Computational Linguistics.

Roth, D., & Yih, W.-t. (2004). A linear programming formulation for global infer-

ence in natural language tasks. In HLT-NAACL 2004 Workshop: Eighth Conference

on Computational Natural Language Learning (CoNLL-2004) (pp. 1–8). Boston,

USA: Association for Computational Linguistics. URL: http://www.aclweb.

org/anthology/W04-2401.

dos Santos, C., Xiang, B., & Zhou, B. (2015). Classifying relations by ranking with

convolutional neural networks. In Proceedings of the 53rd Annual Meeting of the As-

sociation for Computational Linguistics and the 7th International Joint Conference

on Natural Language Processing (Volume 1: Long Papers) (pp. 626–634). Beijing,

China.

26

Socher, R., Chen, D., Manning, C. D., & Ng, A. (2013). Reasoning with neural

tensor networks for knowledge base completion.

In Proceedings of the 26th In-

ternational Conference on Neural Information Processing Systems (pp. 926–934).

Nevada, United States: Curran Associates, Inc.

Socher, R., Huval, B., Manning, C. D., & Ng, A. Y. (2012). Semantic compositionality

through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference

on Empirical Methods in Natural Language Processing and Computational Natural

Language Learning (pp. 1201–1211). Jeju Island, Korea: Association for Computa-

tional Linguistics.

Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014).

Dropout: A simple way to prevent neural networks from overﬁtting. Journal of

Machine Learning Research, 15(1), 1929–1958.

Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with

neural networks.

In Proceedings of the 27th International Conference on Neural

Information Processing Systems (pp. 3104–3112). Montreal, Canada: MIT Press.

Taskar, B., Guestrin, C., & Koller, D. (2003). Max-margin markov networks.

In

Proceedings of the 16th International Conference on Neural Information Processing

Systems (pp. 25–32). Bangkok, Thailand: MIT Press.

Tsochantaridis, I., Hofmann, T., Joachims, T., & Altun, Y. (2004). Support vector

machine learning for interdependent and structured output spaces. In Proceedings

of the 21st International Conference on Machine Learning (pp. 104–112). Helsinki,

Finland: ACM. doi:10.1145/1015330.1015341.

Vu, N. T., Adel, H., Gupta, P., & Sch¨utze, H. (2016). Combining recurrent and con-

volutional neural networks for relation classiﬁcation.

In Proceedings of the 2016

Conference of the North American Chapter of the Association for Computational

Linguistics: Human Language Technologies (pp. 534–539). San Diego, California.

URL: http://www.aclweb.org/anthology/N16-1065.

27

Xu, K., Feng, Y., Huang, S., & Zhao, D. (2015a). Semantic relation classiﬁcation via

convolutional neural networks with simple negative sampling.

In Proceedings of

the 2015 Conference on Empirical Methods in Natural Language Processing (pp.

536–540). Lisbon, Portugal: Association for Computational Linguistics. URL:

http://aclweb.org/anthology/D15-1062.

Xu, Y., Mou, L., Li, G., Chen, Y., Peng, H., & Jin, Z. (2015b). Classifying relations via

long short term memory networks along shortest dependency paths. In Proceedings

of the 2015 Conference on Empirical Methods in Natural Language Processing (pp.

1785–1794). Lisbon, Portugal: Association for Computational Linguistics.

Yang, B., & Cardie, C. (2013). Joint inference for ﬁne-grained opinion extraction.

In Proceedings of the 51st Annual Meeting of the Association for Computational

Linguistics (Volume 1: Long Papers) (pp. 1640–1649). Soﬁa, Bulgaria. URL:

http://www.aclweb.org/anthology/P13-1161.

Zelenko, D., Aone, C., & Richardella, A. (2003). Kernel methods for relation

extraction.

Journal of Machine Learning Research, 3, 1083–1106. doi:10.

3115/1118693.1118703.

Zeng, D., Liu, K., Lai, S., Zhou, G., & Zhao, J. (2014). Relation classiﬁcation via

convolutional deep neural network. In Proceedings of COLING 2014, the 25th In-

ternational Conference on Computational Linguistics: Technical Papers (pp. 2335–

2344).

Spain.

Zhang, D., & Wang, D. (2015). Relation classiﬁcation via recurrent neural network.

arXiv preprint arXiv:1508.01006, .

Zhang, X., Cheng, J., & Lapata, M. (2017). Dependency parsing as head selection.

In Proceedings of the 15th Conference of the European Chapter of the Association

for Computational Linguistics: (Volume 1, Long Papers) (pp. 665–676). Valencia,

Zheng, S., Hao, Y., Lu, D., Bao, H., Xu, J., Hao, H., & Xu, B. (2017). Joint entity and

28

relation extraction based on a hybrid neural network. Neurocomputing, 257, 59 –

66. doi:10.1016/j.neucom.2016.12.075.

29

Appendix

In this section, we report additional results for our multi-head selection framework.

Speciﬁcally, we (i) compare our model with the model of Lample et al. (2016) (i.e.,

optimize only over the NER task), (ii) explore several hyperparameters of the network

(e.g., dropout, LSTM size, character embeddings size), and (iii) report F1 score using

different word embeddings compared to the embeddings used in previous works.

In Table 1 of the main paper, we focused on comparing our model against other

joint models that are able to solve the two tasks (i.e., NER and relation extraction) si-

multaneously, mainly demonstrating superiority of phrasing the relation extraction as

a multi-head selection problem (enabling the extraction of multiple relations at once).

Here, in Table A1, we evaluate the performance of just the ﬁrst module of our joint

multi-head model: we compare the performance of the NER component of our model

against the state-of-the-art NER model of Lample et al. (2016). The results indicate

a marginal performance improvement of our model over Lample’s NER baseline in 3

out of 4 datasets. The improvement of our model’s NER part is not substantial, since

(i) our NER part is almost identical to Lample’s, and (ii) recent advances in NER per-

formance among neural systems are relatively small (improvements in the order of few

0.1 F1 points – for instance, the contribution of Ma & Hovy (2016) and Lample et al.

(2016) on the CoNLL-2003 test set is 0.01% and 0.17% F1 points, respectively). This

slight improvement suggests that the interaction of the two components by sharing the

underlying LSTM layer is indeed beneﬁcial (e.g., identifying a Works for relation might

be helpful for the NER module in detecting the type of the two entities, i.e., PER, ORG

and vice versa). Note that improving NER in isolation was not the objective of our

multi-head model, but we rather aimed to compare our model against other joint mod-

els that solve the task of entity recognition and relation identiﬁcation simultaneously.

We thus did not envision to claim or achieve state-of-the-art performance in each of the

individual building blocks of our joint model.

Tables A2, A3 and A4 show the performance of our model on the test set for dif-

ferent values of the embedding dropout, LSTM layer dropout and the LSTM output

dropout hyperparameters, respectively. Note that the hyperparameter values used for

1

the results in Section 5 were obtained by tuning over the development set, and these

are indicated in bold face in the tables below. We vary one hyperparameter at a time

in order to assess the effect of a particular hyperparameter. The main outcomes from

these tables are twofold: (i) low dropout values (e.g., 0, 0.1) lead to a performance

decrease in the overall F1 score (see Table A3 where a ∼3% F1 decrease is reported on

the ACE04 dataset) and (ii) average dropout values (i.e., 0.2-0.4) lead to consistently

similar results.

In Tables A5, A6, A7 and A8, we report results for different values of the LSTM

size, the size of the character embeddings, the size of the label embeddings and the

layer width of the neural network l (both for the entity and the relation scoring layers),

respectively. The reported results show that different hyperparameters settings do lead

to noticeable performance differences, but we do not observe any clear trend. More-

over, we have not observed any signiﬁcant performance improvement that affects the

overall ranking of the models as reported in Table 1. On the other hand, the results indi-

cate that increasing (character and label) embedding size and layer dimensions leads to

a slight decrease in performance for the CoNLL04 dataset. This can be explained by the

fact that the CoNLL04 dataset is relatively small and using more trainable model pa-

rameters (i.e., larger hyperparameter values) can make our multi-head selection method

to overﬁt quickly on the training set. In almost any other case, variation of the hyper-

parameters does not affect the ranking of the models reported in Table 1.

Model

ACE
04

NER baseline
multi-head

CoNLL
04

NER baseline
multi-head

DREC

ADE

NER baseline
multi-head

NER baseline
multi-head

Entity
R

81.13
81.31

83.13
84.06

84.89
83.98

88.59
88.16

P

81.06
81.01

84.38
83.75

78.22
78.97

83.97
84.72

F1

81.10
81.16

83.75
83.90

81.42
81.39

86.22
86.40

Table A1: Comparison of the multi-head selection model (only the NER component) against the NER base-
line of Lample et al. (2016). Bold font indicates the best results for each dataset.

In the main results (see Section 5), to guarantee a fair comparison to previous work

and to obtain comparable results that are not affected by the input embeddings, we use

embeddings used also in prior studies. To assess the performance of our system to input

2

Table A2: Model performance for different embedding dropout values. Bold entries indicate the result
reported in Section 5.

LSTM
Dropout

Relation
R

F1

Overall F1

Embedding
Dropout

Relation
R

F1

Overall F1

43.28
43.55
44.48
42.45
42.92
43.55

52.37
51.42
57.82
54.74
60.43
56.64

53.87
53.57
54.73
54.19
52.69
52.61

78.87
77.90
77.24
77.37
76.71
75.98

38.86
42.06
44.12
44.48
41.52
39.32

60.19
62.56
60.66
59.48
60.43
52.13

53.61
54.64
54.73
53.79
53.10
49.96

78.15
78.00
78.86
77.24
77.51
76.99

45.37
46.51
47.14
45.84
45.20
45.21

59.65
57.48
61.08
62.18
62.04
61.05

52.46
52.71
52.26
52.86
50.99
51.59

75.62
74.80
74.58
74.80
74.76
73.43

43.05
45.91
47.01
47.14
44.61
42.71

59.76
63.16
62.82
63.46
62.04
57.89

51.44
52.56
52.26
51.39
52.22
50.18

75.34
74.99
75.39
74.58
74.66
73.61

63.10
63.84
64.15
63.59
63.08
62.77

71.36
70.42
72.14
73.26
72.97
72.39

66.85
67.10
66.83
66.94
66.01
65.96

81.14
80.56
80.49
80.54
80.51
79.37

61.61
63.59
64.21
64.15
62.95
61.47

72.49
74.05
74.45
74.39
72.97
70.67

66.30
66.89
66.83
66.03
66.71
64.78

80.98
80.73
80.98
80.49
80.40
79.64

E
C
A

4
0

L
L
N
o
C

4
0

C
E
R
D

E
D
A

0.5
0.4
0.3
0.2
0.1
0

0.5
0.4
0.3
0.2
0.1
0

0.5
0.4
0.3
0.2
0.1
0

0.5
0.4
0.3
0.2
0.1
0

P

80.66
80.97
81.01
81.15
80.86
80.21

82.53
83.66
82.19
84.07
83.75
82.79

78.19
78.47
78.97
78.16
78.83
78.42

84.73
84.51
84.72
84.66
85.10
83.67

Entity
R

81.03
81.39
81.31
81.54
81.06
80.45

83.60
83.04
84.24
84.62
84.06
84.71

84.51
84.73
83.98
84.11
83.34
82.34

88.68
88.21
88.16
87.98
87.43
87.01

F1

80.84
81.18
81.16
81.34
80.96
80.32

83.06
83.35
83.20
84.34
83.90
83.74

81.23
81.48
81.39
81.02
81.02
80.33

86.66
86.32
86.40
86.29
86.25
85.31

P

47.66
49.90
50.14
49.81
47.74
47.00

69.28
65.17
64.72
71.96
63.75
66.21

51.12
51.87
50.00
51.60
49.38
50.62

72.63
71.93
72.10
72.39
72.91
71.04

P

80.27
81.18
81.19
81.01
81.27
80.54

84.18
84.43
86.44
84.73
83.75
84.16

77.76
78.66
78.97
77.85
78.94
78.59

85.01
84.66
84.60
84.72
84.36
83.80

Entity
R

80.08
81.36
81.63
81.31
81.32
79.94

86.28
85.45
85.73
85.91
84.06
82.76

84.83
83.98
83.98
83.68
83.62
80.18

88.29
88.37
88.66
88.16
87.98
87.64

F1

80.18
81.27
81.41
81.16
81.29
80.24

85.22
84.94
86.09
85.32
83.90
83.45

81.15
81.23
81.39
80.66
81.21
79.38

86.62
86.47
86.58
86.40
86.13
85.68

P

48.25
50.54
50.31
50.14
48.20
46.73

59.35
63.77
65.14
68.02
63.75
65.09

49.43
50.63
50.00
49.21
51.37
50.39

72.72
72.20
72.21
72.10
72.03
70.50

E
C
A

4
0

L
L
N
o
C

4
0

C
E
R
D

E
D
A

0.5
0.4
0.3
0.2
0.1
0

0.5
0.4
0.3
0.2
0.1
0

0.5
0.4
0.3
0.2
0.1
0

0.5
0.4
0.3
0.2
0.1
0

3

Table A3: Model performance for different LSTM layer dropout values. Bold entries indicate the result
reported in Section 5.

Table A4: Model performance for different LSTM output dropout values. Bold entries indicate the best
result reported in Section 5.

LSTM output
Dropout

Relation
R

F1

Overall F1

E
C
A

4
0

L
L
N
o
C

4
0

C
E
R
D

E
D
A

0.5
0.4
0.3
0.2
0.1
0

0.5
0.4
0.3
0.2
0.1
0

0.5
0.4
0.3
0.2
0.1
0

0.5
0.4
0.3
0.2
0.1
0

P

81.25
81.23
81.31
81.01
81.01
80.10

85.81
83.27
85.13
84.13
83.75
83.65

78.74
78.45
78.97
77.82
78.84
77.63

84.33
85.16
84.27
84.72
84.65
84.44

Entity
R

81.79
81.70
81.72
81.31
81.12
80.69

86.84
84.89
84.89
84.52
84.06
84.89

84.22
85.20
83.98
84.68
83.75
83.85

87.95
88.16
88.00
88.16
88.04
88.14

F1

81.52
81.47
81.51
81.16
81.07
80.39

86.32
84.08
85.01
84.32
83.90
84.27

81.39
81.69
81.39
81.11
81.22
80.62

86.10
86.63
86.10
86.40
86.31
86.25

P

51.16
51.44
48.69
50.14
47.55
47.20

64.18
66.07
64.82
66.03
63.75
65.23

51.24
50.34
50.00
51.05
51.74
51.16

71.54
72.87
71.83
72.10
72.38
71.64

41.94
42.77
44.21
44.48
42.82
40.54

59.01
61.37
55.45
57.58
60.43
53.79

52.69
55.45
54.73
54.19
54.75
51.39

77.27
77.81
77.42
77.24
77.49
77.82

46.09
46.71
46.35
47.14
45.06
43.61

61.48
63.63
59.77
61.52
62.04
58.96

51.96
52.77
52.26
52.57
53.20
51.28

74.29
75.26
74.52
74.58
74.85
74.61

63.81
64.09
63.93
64.15
63.07
62.00

73.90
73.85
72.39
72.92
72.97
71.61

66.68
67.23
66.83
66.84
67.21
65.95

80.20
80.95
80.31
80.49
80.58
80.43

Relation
R

F1

Overall F1

LSTM
Size

32
64
128

32
64
128

32
64
128

32
64
128

P

80.99
81.01
80.31

82.83
83.75
82.43

77.74
78.97
79.04

83.89
84.72
84.27

Entity
R

81.25
81.31
80.87

83.13
84.06
83.04

85.43
83.98
83.49

87.78
88.16
87.87

F1

81.12
81.16
80.59

82.98
83.90
82.73

81.40
81.39
81.20

85.79
86.40
86.04

P

50.33
50.14
47.30

65.78
63.75
64.86

50.92
50.00
51.27

70.46
72.10
71.36

42.60
44.48
41.77

58.29
60.43
53.79

52.31
54.73
53.64

76.89
77.24
76.77

46.14
47.14
44.36

61.81
62.04
58.81

51.60
52.26
52.42

73.54
74.58
73.97

Character
Embeddings

Relation
R

F1

Overall F1

P

81.02
81.01
81.32

83.33
83.75
85.15

79.73
78.97
78.08

84.80
84.72
84.65

Entity
R

81.57
81.31
81.54

84.34
84.06
82.95

84.17
83.98
84.80

88.00
88.16
88.08

15
25
50

15
25
50

15
25
50

15
25
50

P

47.87
50.14
49.77

66.03
63.75
59.84

52.52
50.00
51.03

72.74
72.10
72.17

44.78
44.48
44.02

57.11
60.43
52.61

55.30
54.73
54.28

77.51
77.24
77.45

46.27
47.14
46.72

61.25
62.04
55.99

53.88
52.26
52.60

75.05
74.58
74.72

63.63
64.15
62.47

72.39
72.97
70.77

66.50
66.83
66.81

79.66
80.49
80.00

63.78
64.15
64.07

72.54
72.97
70.01

67.89
66.83
66.95

80.71
80.49
80.52

E
C
A

4
0

4
0

L
L
N
o
C

C
E
R
D

E
D
A

E
C
A

4
0

4
0

L
L
N
o
C

C
E
R
D

E
D
A

F1

81.29
81.16
81.43

83.83
83.90
84.04

81.89
81.39
81.30

86.37
86.40
86.33

4

Table A5: Model performance for different LSTM size values. Bold entries indicate the result reported in
Section 5.

Table A6: Model performance for different character embeddings size values. Bold entries indicate the result
reported in Section 5.

E
C
A

4
0

4
0

L
L
N
o
C

C
E
R
D

E
D
A

E
C
A

4
0

4
0

L
L
N
o
C

C
E
R
D

E
D
A

15
25
50

15
0
50

15
25
50

15
25
50

32
64
128

32
64
128

32
64
128

32
64
128

Label
Embeddings

Relation
R

F1

Overall F1

P

80.95
81.01
81.17

84.68
83.75
82.32

78.48
78.97
78.92

84.47
84.72
84.81

Entity
R

81.27
81.31
81.61

83.50
84.06
84.15

84.81
83.98
84.88

88.18
88.16
88.65

F1

81.11
81.16
81.39

84.08
83.90
83.23

81.53
81.39
81.79

86.29
86.40
86.69

P

49.27
50.14
48.01

62.21
63.75
59.30

51.83
50.00
51.35

71.93
72.10
72.46

43.80
44.48
44.48

56.16
60.43
55.92

53.21
54.73
53.23

77.49
77.24
78.68

46.37
47.14
46.18

59.03
62.04
57.56

52.51
52.26
52.27

74.61
74.58
75.44

63.74
64.15
63.78

71.56
72.97
70.39

67.02
66.83
67.03

80.45
80.49
81.06

Table A7: Model performance for different label embeddings size values. Bold entries indicate the result
reported in Section 5.

variations, we also report results using different word embeddings (see Table A9) (i.e.,

Adel & Sch¨utze (2017); Li et al. (2017)) on the ACE04 dataset. Our results showcase

that our model, even when using different word embeddings, is still performing better

compared to other works that, like ours, do not rely on additional NLP tools.

Hidden layer
Size

Relation
R

F1

Overall F1

P

81.01
81.01
81.30

82.26
83.75
82.69

79.66
78.97
78.35

84.31
84.72
84.81

Entity
R

81.02
81.31
81.32

84.24
84.06
83.69

84.23
83.98
84.47

88.56
88.16
88.54

F1

81.02
81.16
81.31

83.24
83.90
83.19

81.89
81.39
81.30

86.38
86.40
86.63

P

48.81
50.14
51.58

65.96
63.75
64.46

52.42
50.00
48.53

71.67
72.10
72.29

43.26
44.48
43.68

59.24
60.43
55.45

51.45
54.73
53.08

78.51
77.24
78.20

45.87
47.14
47.30

62.42
62.04
59.62

51.93
52.26
50.70

74.93
74.58
75.13

63.44
64.15
64.31

72.83
72.97
71.40

66.91
66.83
66.00

80.66
80.49
80.87

Table A8: Model performance for different layer widths l of the neural network (both for the entity and the
relation scoring layers). Bold entries indicate the result reported in Section 5.

Embeddings

Size

Miwa & Bansal (2016)
Adel & Sch¨utze (2017)
Li et al. (2017)

200
50
200

P

81.01
82.18
81.51

Entity
R

81.31
79.83
81.35

F1

81.16
80.99
81.43

P

50.14
49.10
46.59

Relation
R

F1

Overall F1

44.48
41.40
44.43

47.14
44.92
45.49

64.15
62.96
63.46

Table A9: Model performance for different embeddings on the ACE04 dataset. Bold entries indicate the
result reported in Section 5.

5

8
1
0
2
 
c
e
D
 
7
1
 
 
]
L
C
.
s
c
[
 
 
3
v
7
4
8
7
0
.
4
0
8
1
:
v
i
X
r
a

Joint entity recognition and relation extraction as a
multi-head selection problem

Giannis Bekoulis∗, Johannes Deleu, Thomas Demeester, Chris Develder

Ghent University – imec, IDLab, Department of Information Technology,
Technologiepark Zwijnaarde 15, 9052 Ghent, Belgium

Abstract

State-of-the-art models for joint entity recognition and relation extraction strongly rely

on external natural language processing (NLP) tools such as POS (part-of-speech) tag-

gers and dependency parsers. Thus, the performance of such joint models depends on

the quality of the features obtained from these NLP tools. However, these features

are not always accurate for various languages and contexts.

In this paper, we pro-

pose a joint neural model which performs entity recognition and relation extraction

simultaneously, without the need of any manually extracted features or the use of any

external tool. Speciﬁcally, we model the entity recognition task using a CRF (Condi-

tional Random Fields) layer and the relation extraction task as a multi-head selection

problem (i.e., potentially identify multiple relations for each entity). We present an

extensive experimental setup, to demonstrate the effectiveness of our method using

datasets from various contexts (i.e., news, biomedical, real estate) and languages (i.e.,

English, Dutch). Our model outperforms the previous neural models that use automat-

ically extracted features, while it performs within a reasonable margin of feature-based

neural models, or even beats them.

sequence labeling

Keywords: entity recognition, relation extraction, multi-head selection, joint model,

∗Corresponding author
Email addresses: giannis.bekoulis@ugent.be (Giannis Bekoulis),

johannes.deleu@ugent.be (Johannes Deleu), thomas.demeester@ugent.be (Thomas
Demeester), chris.develder@ugent.be (Chris Develder)

1. Introduction

answering.

The goal of the entity recognition and relation extraction is to discover relational struc-

tures of entity mentions from unstructured texts. It is a central problem in information

extraction since it is critical for tasks such as knowledge base population and question

The problem is traditionally approached as two separate subtasks, namely (i) named

entity recognition (NER) (Nadeau & Sekine, 2007) and (ii) relation extraction (RE) (Bach

& Badaskar, 2007), in a pipeline setting. The main limitations of the pipeline models

are: (i) error propagation between the components (i.e., NER and RE) and (ii) possible

useful information from the one task is not exploited by the other (e.g., identifying a

Works for relation might be helpful for the NER module in detecting the type of the

two entities, i.e., PER, ORG and vice versa). On the other hand, more recent studies

propose to use joint models to detect entities and their relations overcoming the afore-

mentioned issues and achieving state-of-the-art performance (Li & Ji, 2014; Miwa &

Sasaki, 2014).

The previous joint models heavily rely on hand-crafted features. Recent advances

in neural networks alleviate the issue of manual feature engineering, but some of them

still depend on NLP tools (e.g., POS taggers, dependency parsers). Miwa & Bansal

(2016) propose a Recurrent Neural Network (RNN)-based joint model that uses a bidi-

rectional sequential LSTM (Long Short Term Memory) to model the entities and a

tree-LSTM that takes into account dependency tree information to model the relations

between the entities. The dependency information is extracted using an external depen-

dency parser. Similarly, in the work of Li et al. (2017) for entity and relation extraction

from biomedical text, a model which also uses tree-LSTMs is applied to extract de-

pendency information. Gupta et al. (2016) propose a method that relies on RNNs but

uses a lot of hand-crafted features and additional NLP tools to extract features such as

POS-tags, etc. Adel & Sch¨utze (2017) replicate the context around the entities with

Convolutional Neural Networks (CNNs). Note that the aforementioned works exam-

ine pairs of entities for relation extraction, rather than modeling the whole sentence

directly. This means that relations of other pairs of entities in the same sentence —

2

which could be helpful in deciding on the relation type for a particular pair — are not

taken into account. Katiyar & Cardie (2017) propose a neural joint model based on

LSTMs where they model the whole sentence at once, but still they do not have a prin-

cipled way to deal with multiple relations. Bekoulis et al. (2018) introduce a quadratic

scoring layer to model the two tasks simultaneously. The limitation of this approach is

that only a single relation can be assigned to a token, while the time complexity for the

entity recognition task is increased compared to the standard approaches with linear

complexity.

In this work, we focus on a new general purpose joint model that performs the

two tasks of entity recognition and relation extraction simultaneously, and that can

handle multiple relations together. Our model achieves state-of-the-art performance in

a number of different contexts (i.e., news, biomedical, real estate) and languages (i.e.,

English, Dutch) without relying on any manually engineered features nor additional

NLP tools. In summary, our proposed model (which will be detailed next in Section 3)

solves several shortcomings that we identiﬁed in related works (Section 2) for joint

entity recognition and relation extraction: (i) our model does not rely on external NLP

tools nor hand-crafted features, (ii) entities and relations within the same text fragment

(typically a sentence) are extracted simultaneously, where (iii) an entity can be involved

in multiple relations at once.

Speciﬁcally, the model of Miwa & Bansal (2016) depends on dependency parsers,

which perform particularly well on speciﬁc languages (i.e., English) and contexts (i.e.,

news). Yet, our ambition is to develop a model that generalizes well in various setups,

therefore using only automatically extracted features that are learned during training.

For instance, Miwa & Bansal (2016) and Li et al. (2017) use exactly the same model in

different contexts, i.e., news (ACE04) and biomedical data (ADE), respectively. Com-

paring our results to the ADE dataset, we obtain a 1.8% improvement on the NER task

and ∼3% on the RE task. On the other hand, our model performs within a reasonable

margin (∼0.6% in the NER task and ∼1% on the RE task) on the ACE04 dataset with-

out the use of pre-calculated features. This shows that the model of Miwa & Bansal

(2016) strongly relies on the features extracted by the dependency parsers and can-

not generalize well into different contexts where dependency parser features are weak.

3

Comparing to Adel & Sch¨utze (2017), we train our model by modeling all the entities

and the relations of the sentence at once. This type of inference is beneﬁcial in ob-

taining information about neighboring entities and relations instead of just examining

a pair of entities each time. Finally, we solve the underlying problem of the models

proposed by Katiyar & Cardie (2017) and Bekoulis et al. (2017), who essentially as-

sume classes (i.e., relations) to be mutually exclusive: we solve this by phrasing the

relation extraction component as a multi-label prediction problem.1

To demonstrate the effectiveness of the proposed method, we conduct the largest

experimental evaluation to date (to the best of our knowledge) in jointly performing

both entity recognition and relation extraction (see Section 4 and Section 5), using dif-

ferent datasets from various domains (i.e., news, biomedical, real estate) and languages

(i.e., English, Dutch). Speciﬁcally, we apply our method to four datasets, namely

ACE04 (news), Adverse Drug Events (ADE), Dutch Real Estate Classiﬁeds (DREC)

and CoNLL’04 (news). Our method outperforms all state-of-the-art methods that do

not rely on any additional features or tools, while performance is very close (or even

better in the biomedical dataset) compared to methods that do exploit hand-engineered

features or NLP tools.

2. Related work

The tasks of entity recognition and relation extraction can be applied either one by one

in a pipeline setting (Fundel et al., 2007; Gurulingappa et al., 2012a; Bekoulis et al.,

2017) or in a joint model (Miwa & Sasaki, 2014; Miwa & Bansal, 2016; Bekoulis

et al., 2018). In this section, we present related work for each task (i.e., named entity

recognition and relation extraction) as well as prior work into joint entity and relation

extraction.

1Note that another difference is that we use a CRF layer for the NER part, while Katiyar & Cardie
(2017) uses a softmax and Bekoulis et al. (2017) uses a quadratic scoring layer; see further, when we discuss
performance comparison results in Section 5.

4

2.1. Named entity recognition

In our work, NER is the ﬁrst task which we solve in order to address the end-to-end

relation extraction problem. A number of different methods for the NER task that

are based on hand-crafted features have been proposed, such as CRFs (Lafferty et al.,

2001), Maximum Margin Markov Networks (Taskar et al., 2003) and support vector

machines (SVMs) for structured output (Tsochantaridis et al., 2004), to name just a

few. Recently, deep learning methods such as CNN- and RNN-based models have

been combined with CRF loss functions (Collobert et al., 2011; Huang et al., 2015;

Lample et al., 2016; Ma & Hovy, 2016) for NER. These methods achieve state-of-the-

art performance on publicly available NER datasets without relying on hand-crafted

features.

2.2. Relation extraction

We consider relation extraction as the second task of our joint model. The main ap-

proaches for relation extraction rely either on hand-crafted features (Zelenko et al.,

2003; Kambhatla, 2004) or neural networks (Socher et al., 2012; Zeng et al., 2014).

Feature-based methods focus on obtaining effective hand-crafted features, for instance

deﬁning kernel functions (Zelenko et al., 2003; Culotta & Sorensen, 2004) and design-

ing lexical, syntactic, semantic features, etc. (Kambhatla, 2004; Rink & Harabagiu,

2010). Neural network models have been proposed to overcome the issue of manually

designing hand-crafted features leading to improved performance. CNN- (Zeng et al.,

2014; Xu et al., 2015a; dos Santos et al., 2015) and RNN-based (Socher et al., 2013;

Zhang & Wang, 2015; Xu et al., 2015b) models have been introduced to automatically

extract lexical and sentence level features leading to a deeper language understand-

ing. Vu et al. (2016) combine CNNs and RNNs using an ensemble scheme to achieve

state-of-the-art results.

2.3. Joint entity and relation extraction

Entity and relation extraction includes the task of (i) identifying the entities (described

in Section 2.1) and (ii) extracting the relations among them (described in Section 2.2).

Feature-based joint models (Kate & Mooney, 2010; Yang & Cardie, 2013; Li & Ji,

5

2014; Miwa & Sasaki, 2014) have been proposed to simultaneously solve the entity

recognition and relation extraction (RE) subtasks. These methods rely on the availabil-

ity of NLP tools (e.g., POS taggers) or manually designed features and thus (i) require

additional effort for the data preprocessing, (ii) perform poorly in different applica-

tion and language settings where the NLP tools are not reliable, and (iii) increase the

computational complexity. In this paper, we introduce a joint neural network model to

overcome the aforementioned issues and to automatically perform end-to-end relation

extraction without the need of any manual feature engineering or the use of additional

NLP components.

Neural network approaches have been considered to address the problem in a joint

setting (end-to-end relation extraction) and typically include the use of RNNs and

CNNs (Miwa & Bansal, 2016; Zheng et al., 2017; Li et al., 2017). Speciﬁcally, Miwa

& Bansal (2016) propose the use of bidirectional tree-structured RNNs to capture de-

pendency tree information (where parse trees are extracted using state-of-the-art de-

pendency parsers) which has been proven beneﬁcial for relation extraction (Xu et al.,

2015a,b). Li et al. (2017) apply the work of Miwa & Bansal (2016) to biomedical

text, reporting state-of-the-art performance for two biomedical datasets. Gupta et al.

(2016) propose the use of a lot of hand-crafted features along with RNNs. Adel &

Sch¨utze (2017) solve the entity classiﬁcation task (which is different from NER since

in entity classiﬁcation the boundaries of the entities are known and only the type of the

entity should be predicted) and relation extraction problems using an approximation of

a global normalization objective (i.e., CRF): they replicate the context of the sentence

(left and right part of the entities) to feed one entity pair at a time to a CNN for relation

extraction. Thus, they do not simultaneously infer other potential entities and relations

within the same sentence. Katiyar & Cardie (2017) and Bekoulis et al. (2018) inves-

tigate RNNs with attention for extracting relations between entity mentions without

using any dependency parse tree features. Different from Katiyar & Cardie (2017), in

this work, we frame the problem as a multi-head selection problem by using a sigmoid

loss to obtain multiple relations and a CRF loss for the NER component. This way,

we are able to independently predict classes that are not mutually exclusive, instead of

assigning equal probability values among the tokens. We overcome the issue of addi-

6

Figure 1: The multi-head selection model for joint entity and relation extraction. The input of our model is
the words of the sentence which are then represented as word vectors (i.e., embeddings). The BiLSTM layer
extracts a more complex representation for each word. Then the CRF and the sigmoid layers are able to
produce the outputs for the two tasks. The outputs for each token (e.g., Smith) are: (i) an entity recognition
label (e.g., I-PER) and (ii) a set of tuples comprising the head tokens of the entity and the types of relations
between them (e.g., {(Center, Works for), (Atlanta, Lives in)}).

tional complexity described by Bekoulis et al. (2018), by dividing the loss functions

into a NER and a relation extraction component. Moreover, we are able to handle mul-

tiple relations instead of just predicting single ones, as was described for the application

of structured real estate advertisements of Bekoulis et al. (2018).

3. Joint model

In this section, we present our multi-head joint model illustrated in Fig. 1. The model

is able to simultaneously identify the entities (i.e., types and boundaries) and all the

possible relations between them at once. We formulate the problem as a multi-head

selection problem extending previous work (Zhang et al., 2017; Bekoulis et al., 2018)

as described in Section 2.3. By multi-head, we mean that any particular entity may be

involved in multiple relations with other entities. The basic layers of the model, shown

in Fig. 1, are: (i) embedding layer, (ii) bidirectional sequential LSTM (BiLSTM) layer,

(iii) CRF layer and the (iv) sigmoid scoring layer. In Fig. 1, an example sentence from

7

Figure 2: Embedding layer in detail. The characters of the word “Man” are represented by character vectors
(i.e., embeddings) that are learned during training. The character embeddings are fed to a BiLSTM and the
two ﬁnal states (forward and backward) are concatenated. The vector wchars is the character-level repre-
sentation of the word. This vector is then further concatenated to the word-level representation wword2vec
to obtain the complete word embedding vector.

the CoNLL04 dataset is presented. The input of our model is a sequence of tokens

(i.e., words of the sentence) which are then represented as word vectors (i.e., word

embeddings). The BiLSTM layer is able to extract a more complex representation for

each word that incorporates the context via the RNN structure. Then the CRF and the

sigmoid layers are able to produce the outputs for the two tasks. The outputs for each

token (e.g., Smith) are twofold: (i) an entity recognition label (e.g., I-PER, denoting

the token is inside a named entity of type PER) and (ii) a set of tuples comprising the

head tokens of the entity and the types of relations between them (e.g., {(Center, Works

for), (Atlanta, Lives in)}). Since we assume token-based encoding, we consider only

the last token of the entity as head of another token, eliminating redundant relations.

For instance, there is a Works for relation between entities “John Smith” and “Disease

Control Center”.

Instead of connecting all tokens of the entities, we connect only

“Smith” with “Center”. Also, for the case of no relation, we introduce the “N” label

and we predict the token itself as the head.

3.1. Embedding layer

Given a sentence w = w1, ..., wn as a sequence of tokens, the word embedding layer

is responsible to map each token to a word vector (wword2vec). We use pre-trained word

embeddings using the Skip-Gram word2vec model (Mikolov et al., 2013).

8

In this work, we also use character embeddings since they are commonly applied

to neural NER (Ma & Hovy, 2016; Lample et al., 2016). This type of embeddings is

able to capture morphological features such as preﬁxes and sufﬁxes. For instance, in

the Adverse Drug Events (ADE) dataset, the sufﬁx “toxicity” can specify an adverse

drug event entity such as “neurotoxicity” or “hepatotoxicity” and thus it is very infor-

mative. Another example might be the Dutch sufﬁx “kamer” (“room” in English) in the

Dutch Real Estate Classiﬁeds (DREC) dataset which is used to specify the space en-

tities “badkamer” (“bathroom” in English) and “slaapkamer” (“bedroom” in English).

Character-level embeddings are learned during training, similar to Ma & Hovy (2016)

and Lample et al. (2016). In the work of Lample et al. (2016), character embeddings

lead to a performance improvement of up to 3% in terms of NER F1 score. In our

work, by incorporating character embeddings, we report in Table 2 an increase of ∼2%

overall F1 scoring points. For more details, see Section 5.2.

Figure 2 illustrates the neural architecture for word embedding generation based

on its characters. The characters of each word are represented by character vectors

(i.e., embeddings). The character embeddings are fed to a BiLSTM and the two ﬁnal

states (forward and backward) are concatenated. The vector wchars is the character-level

representation of the word. This vector is then further concatenated to the word-level

representation wword2vec to obtain the complete word embedding vector.

3.2. Bidirectional LSTM encoding layer

RNNs are commonly used in modeling sequential data and have been successfully

applied in various NLP tasks (Sutskever et al., 2014; Lample et al., 2016; Miwa &

Bansal, 2016). In this work, we use multi-layer LSTMs, a speciﬁc kind of RNNs which

are able to capture long term dependencies well (Bengio et al., 1994; Pascanu et al.,

2013). We employ a BiLSTM which is able to encode information from left to right

(past to future) and right to left (future to past). This way, we can combine bidirectional
information for each word by concatenating the forward ( (cid:126)hi) and the backward ( (cid:126)hi)

output at timestep i. The BiLSTM output at timestep i can be written as:

hi = [ (cid:126)hi; (cid:126)hi],

i = 0, ..., n

(1)

9

3.3. Named entity recognition

We formulate the entity identiﬁcation task as a sequence labeling problem, similar to

previous work on joint learning models (Miwa & Bansal, 2016; Li et al., 2017; Katiyar

& Cardie, 2017) and named entity recognition (Lample et al., 2016; Ma & Hovy, 2016)

using the BIO (Beginning, Inside, Outside) encoding scheme. Each entity consists of

multiple sequential tokens within the sentence and we should assign a tag for every

token in the sentence. That way we are able to identify the entity arguments (start and

end position) and its type (e.g., ORG). To do so, we assign the B-type (beginning) to

the ﬁrst token of the entity, the I-type (inside) to every other token within the entity

and the O tag (outside) if a token is not part of an entity. Fig. 1 shows an example

of the BIO encoding tags assigned to the tokens of the sentence. In the CRF layer,

one can observe that we assign the B-ORG and I-ORG tags to indicate the beginning

and the inside tokens of the entity “Disease Control Center”, respectively. On top of

the BiLSTM layer, we employ either a softmax or a CRF layer to calculate the most

probable entity tag for each token. We calculate the score of each token wi for each

entity tag:

s(e)(hi) = V (e)f (U (e)hi + b(e))

(2)

where the superscript (e) is used for the notation of the NER task, f (·) is an element-
wise activation function (i.e., relu, tanh), V (e) ∈ Rp×l, U (e) ∈ Rl×2d, b(e) ∈ Rl,

with d as the hidden size of the LSTM, p the number of NER tags (e.g., B-ORG) and

l the layer width. We calculate the probabilities of all the candidate tags for a given
token wi as Pr(tag | wi) = softmax(s(hi)) where Pr(tag | wi) ∈ Rp. In this work,

we employ the softmax approach only for the entity classiﬁcation (EC) task (which is

similar to NER) where we need to predict only the entity types (e.g., PER) for each

token assuming boundaries are given. The CRF approach is used for the NER task

which includes both entity type and boundaries recognition.

In the softmax approach, we assign entity types to tokens in a greedy way at pre-

diction time (i.e., the selected tag is just the highest scoring tag over all possible set

of tags). Although assuming an independent tag distribution is beneﬁcial for entity

classiﬁcation tasks (e.g., POS tagging), this is not the case when there are strong de-

10

pendencies between the tags. Speciﬁcally, in NER, the BIO tagging scheme forces

several restrictions (e.g., B-LOC cannot be followed by I–PER). The softmax method

allows local decisions (i.e., for the tag of each token wi) even though the BiLSTM

captures information about the neighboring words. Still, the neighboring tags are not

taken into account for the tag decision of a speciﬁc token. For example, in the entity

“John Smith”, tagging “Smith” as PER is useful for deciding that “John” is B-PER. To

this end, for NER, we use a linear-chain CRF, similar to Lample et al. (2016) where

an improvement of ∼1% F1 NER points is reported when using CRF. In our case, with

the use of CRF we also report a ∼1% overall performance improvement as observed

in Table 2 (see Section 5.2). Assuming the word vector w, a sequence of score vectors
1 , ..., s(e)
s(e)
deﬁned as:

n and a vector of tag predictions y(e)

n , the linear-chain CRF score is

1 , ..., y(e)

(cid:16)

S

y(e)
1 , . . . , y(e)
n

(cid:17)

=

n
(cid:88)

i=0

s(e)
i,y(e)
i

+

n−1
(cid:88)

i=1

Ty(e)

i

,y(e)
i+1

where S ∈ R, s(e)

i,y(e)
i

is the score of the predicted tag for token wi, T is a square tran-

sition matrix in which each entry represents transition scores from one tag to another.
T ∈ R(p+2)×(p+2) because y(e)
0

n are two auxiliary tags that represent the start-

and y(e)

ing and the ending tags of the sentence, respectively. Then, the probability of a given

sequence of tags over all possible tag sequences for the input sentence w is deﬁned as:

(cid:16)

Pr

y(e)
1 , . . . , y(e)
n

(cid:17)

(cid:12)
(cid:12)
(cid:12) w

=

eS(y(e)

1 ,...,y(e)
n )

(cid:80)

eS( ˜y1

(e),..., ˜yn

(e))

˜y1

(e),..., ˜yn

(e)

We apply Viterbi to obtain the tag sequence ˆy(e) with the highest score. We train

both the softmax (for the EC task) and the CRF layer (for NER) by minimizing the

cross-entropy loss LNER. We also use the entity tags as input to our relation extraction

layer by learning label embeddings, motivated by Miwa & Bansal (2016) where an im-

provement of 2% F1 is reported (with the use of label embeddings). In our case, label

embeddings lead to an increase of 1% F1 score as reported in Table 2 (see Section 5.2).

The input to the next layer is twofold: the output states of the LSTM and the learned

label embedding representation, encoding the intuition that knowledge of named enti-

(3)

(4)

11

ties can be useful for relation extraction. During training, we use the gold entity tags,

while at prediction time we use the predicted entity tags as input to the next layer. The

input to the next layer is the concatenation of the hidden LSTM state hi with the label

embedding gi for token wi:

zi = [hi; gi],

i = 0, ..., n

(5)

3.4. Relation extraction as multi-head selection

In this subsection, we describe the relation extraction task, formulated as a multi-head

selection problem (Zhang et al., 2017; Bekoulis et al., 2018). In the general formulation

of our method, each token wi can have multiple heads (i.e., multiple relations with

other tokens). We predict the tuple (ˆyi, ˆci) where ˆyi is the vector of heads and ˆci is

the vector of the corresponding relations for each token wi. This is different for the

previous standard head selection for dependency parsing method (Zhang et al., 2017)

since (i) it is extended to predict multiple heads and (ii) the decisions for the heads and

the relations are jointly taken (i.e., instead of ﬁrst predicting the heads and then in a next

step the relations by using an additional classiﬁer). Given as input a token sequence w

and a set of relation labels R, our goal is to identify for each token wi, i ∈ {0, ..., n}

the vector of the most probable heads ˆyi ⊆ w and the vector of the most probable

corresponding relation labels ˆri ⊆ R. We calculate the score between tokens wi and

wj given a label rk as follows:

s(r)(zj, zi, rk) = V (r)f (U (r)zj + W (r)zi + b(r))

(6)

where the superscript (r) is used for the notation of the relation task, f (·) is an element-
wise activation function (i.e., relu, tanh), V (r) ∈ Rl, U (r) ∈ Rl×(2d+b), W (r) ∈
Rl×(2d+b), b(r) ∈ Rl, d is the hidden size of the LSTM, b is the size of the label

embeddings and l the layer width. We deﬁne

Pr(head = wj, label = rk | wi) = σ(s(r)(zj, zi, rk))

(7)

to be the probability of token wj to be selected as the head of token wi with the relation

label rk between them, where σ(.) stands for the sigmoid function. We minimize the

12

cross-entropy loss Lrel during training:

n
(cid:88)

m
(cid:88)

i=0

j=0

Lrel =

− log Pr(head = yi,j, relation = ri,j | wi)

(8)

where yi ⊆ w and ri ⊆ R are the ground truth vectors of heads and associated relation

labels of wi and m is the number of relations (heads) for wi. After training, we keep

the combination of heads ˆyi and relation labels ˆri exceeding a threshold based on the

estimated joint probability as deﬁned in Eq. (7). Unlike previous work on joint mod-

els (Katiyar & Cardie, 2017), we are able to predict multiple relations considering the

classes as independent and not mutually exclusive (the probabilities do not necessarily

sum to 1 for different classes). For the joint entity and relation extraction task, we

calculate the ﬁnal objective as LNER + Lrel.

3.5. Edmonds’ algorithm

Our model is able to simultaneously extract entity mentions and the relations between

them. To demonstrate the effectiveness and the general purpose nature of our model, we

also test it on the recently introduced Dutch real estate classiﬁeds (DREC) dataset (Bek-

oulis et al., 2017) where the entities need to form a tree structure. By using thresholded

inference, a tree structure of relations is not guaranteed. Thus we should enforce tree

structure constraints to our model. To this end, we post-process the output of our sys-

tem with Edmonds’ maximum spanning tree algorithm for directed graphs (Chu & Liu,

1965; Edmonds, 1967). A fully connected directed graph G = (V, E) is constructed,

where the vertices V represent the last tokens of the identiﬁed entities (as predicted

by NER) and the edges E represent the highest scoring relations with their scores as

weights. Edmonds’ algorithm is applied in cases a tree is not already formed by thresh-

olded inference.

4. Experimental setup

4.1. Datasets and evaluation metrics

We conduct experiments on four datasets: (i) Automatic Content Extraction, ACE04 (Dod-

dington et al., 2004), (ii) Adverse Drug Events, ADE (Gurulingappa et al., 2012b),

13

(iii) Dutch Real Estate Classiﬁeds, DREC (Bekoulis et al., 2017) and (iv) the CoNLL’04

dataset with entity and relation recognition corpora (Roth & Yih, 2004). Our code is

available in our github codebase.2

ACE04: There are seven main entity types namely Person (PER), Organization (ORG),

Geographical Entities (GPE), Location (LOC), Facility (FAC), Weapon (WEA) and Ve-

hicle (VEH). Also, the dataset deﬁnes seven relation types: Physical (PHYS), Person-

Social (PER-SOC), Employment-Membership-Subsidiary (EMP-ORG), Agent-Artifact (ART),

PER-ORG afﬁliation (Other-AFF), GPE afﬁliation (GPE-AFF), and Discourse (DISC).

We follow the cross-validation setting of Li & Ji (2014) and Miwa & Bansal (2016).

We removed DISC and did 5-fold cross-validation on the bnews and nwire subsets (348

documents). We obtained the preprocessing script from Miwa’s github codebase.3 We

measure the performance of our system using micro F1 scores, Precision and Recall on

both entities and relations. We treat an entity as correct when the entity type and the

region of its head are correct. We treat a relation as correct when its type and argument

entities are correct, similar to Miwa & Bansal (2016) and Katiyar & Cardie (2017). We

refer to this type of evaluation as strict.4 We select the best hyperparameter values on

a randomly selected validation set for each fold, selected from the training set (15% of

the data) since there are no ofﬁcial train and validation splits in the work of Miwa &

Bansal (2016).

CoNLL04: There are four entity types in the dataset (Location, Organization, Person,

and Other) and ﬁve relation types (Kill, Live in, Located in, OrgBased in and Work

for). We use the splits deﬁned by Gupta et al. (2016) and Adel & Sch¨utze (2017). The

dataset consists of 910 training instances, 243 for validation and 288 for testing.5 We

measure the performance by computing the F1 score on the test set. We adopt two

evaluation settings to compare to previous work. Speciﬁcally, we perform an EC task

assuming the entity boundaries are given similar to Gupta et al. (2016) and Adel &

2https://github.com/bekou/multihead_joint_entity_relation_extraction
3https://github.com/tticoin/LSTM-ER/tree/master/data/ace2004
4For the CoNLL04, DREC and ADE datasets, the head region covers the whole entity (start and end

boundaries). The ACE04 already deﬁnes the head region of an entity.

5http://cistern.cis.lmu.de/globalNormalization/globalNormalization_

all.zip

14

Sch¨utze (2017). To obtain comparable results, we omit the entity class “Other” when

computing the EC score. We score a multi-token entity as correct if at least one of its

comprising token types is correct assuming that the boundaries are given; a relation

is correct when the type of the relation and the argument entities are both correct. We

report macro-average F1 scores for EC and RE to obtain comparable results to previous

studies. Moreover, we perform actual NER evaluation instead of just EC, reporting

results using the strict evaluation metric.

DREC: The dataset consists of 2,318 classiﬁeds as described in the work of Bekoulis

et al. (2018). There are 9 entity types: Neighborhood, Floor, Extra building, Subspace,

Invalid, Field, Other, Space and Property. Also, there are two relation classes Part-of

and Equivalent. The goal is to identify important entities of a property (e.g., ﬂoors,

spaces) from classiﬁeds and structuring them into a tree format to get the structured

description of the property. For the evaluation, we use 70% for training, 15% for

validation and 15% as test set in the same splits as deﬁned in Bekoulis et al. (2018).

We measure the performance by computing the F1 score on the test set. To compare our

results with previous work (Bekoulis et al., 2018), we use the boundaries evaluation

setting. In this setting, we count an entity as correct if the boundaries of the entity are

correct. A relation is correct when the relation is correct and the argument entities are

both correct. Also, we report results using the strict evaluation for future reference.

ADE: There are two types of entities (drugs and diseases) in this dataset and the aim of

the task is to identify the types of entities and relate each drug with a disease (adverse

drug events). There are 6,821 sentences in total and similar to previous work (Li et al.,

2016, 2017), we remove ∼130 relations with overlapping entities (e.g., “lithium” is a

drug which is related to “lithium intoxication”). Since there are no ofﬁcial sets, we

evaluate our model using 10-fold cross-validation where 10% of the data was used as

validation and 10% for test set similar to Li et al. (2017). The ﬁnal results are displayed

in F1 metric as a macro-average across the folds. The dataset consists of 10,652 entities

and 6,682 relations. We report results similar to previous work on this dataset using the

strict evaluation metric.

15

4.2. Word embeddings

We use pre-trained word2vec embeddings used in previous work, so as to retain the

same inputs for our model and to obtain comparable results that are not affected by the

input embeddings. Speciﬁcally, we use the 200-dimensional word embeddings used in

the work of Miwa & Bansal (2016) for the ACE04 dataset6 trained on Wikipedia. We

obtained the 50-dimensional word embeddings used by Adel & Sch¨utze (2017)5 trained

also on Wikipedia for the CoNLL04 corpus. We use the 128-dimensional word2vec

embeddings used by Bekoulis et al. (2018) trained on a large collection of 887k Dutch

property advertisements7 for the DREC dataset. Finally, for the ADE dataset, we used

200-dimensional embeddings used by Li et al. (2017) and trained on a combination of

PubMed and PMC texts with texts extracted from English Wikipedia (Moen & Anani-

adou, 2013)8.

4.3. Hyperparameters and implementation details

We have developed our joint model by using Python with the TensorFlow machine

learning library (Abadi et al., 2016). Training is performed using the Adam opti-

mizer (Kingma & Ba, 2015) with a learning rate of 10−3. We ﬁx the size of the LSTM

to d = 64 and the layer width of the neural network to l = 64 (both for the entity and

the relation scoring layers). We use dropout (Srivastava et al., 2014) to regularize our

network. Dropout is applied in the input embeddings and in between the hidden layers

for both tasks. Different dropout rates have been applied but the best dropout values

(0.2 to 0.4) for each dataset have been used. The hidden dimension for the character-

based LSTMs is 25 (for each direction). We also ﬁxed our label embeddings to be of

size b = 25 for all the datasets except for CoNLL04 where the label embeddings were

not beneﬁcial and thus were not used. We experimented with tanh and relu activation

functions (recall that this is the function f (·) from the model description). We use the

6http://tti-coin.jp/data/wikipedia200.bin
7https://drive.google.com/uc?id=1Dvibr-Ps4G_GI6eDx9bMXnJphGhH_M1z&

8http://evexdb.org/pmresources/vec-space-models/wikipedia-pubmed-and-PMC-w2v.

export=download

bin

16

Settings

Evaluation

P

Relation
R

F1

Overall F1

E
C
A

4
0

Miwa & Bansal (2016)
Katiyar & Cardie (2017)
multi-head

L
L
N
o
C

4
0

C
E
R
D

E
D
A

Gupta et al. (2016)
Gupta et al. (2016)
Adel & Sch¨utze (2017)
multi-head EC
Miwa & Sasaki (2014)
multi-head

Bekoulis et al. (2018)
multi-head+E
single-head
multi-head

Li et al. (2016)
Li et al. (2017)
multi-head

Pre-calculated
Features
(cid:51)
(cid:55)
(cid:55)

(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:55)

(cid:55)
(cid:55)
(cid:55)
(cid:55)

(cid:51)
(cid:51)
(cid:55)

strict
strict
strict

relaxed
relaxed
relaxed
relaxed
strict
strict

boundaries
boundaries
strict
strict

strict
strict
strict

80.80
81.20
81.01

92.50
88.50
-
93.41
81.20
83.75

77.93
79.84
78.80
78.97

79.50
82.70
84.72

Entity
R

82.90
78.10
81.31

92.10
88.90
-
93.15
80.20
84.06

80.31
84.92
84.26
83.98

79.60
86.70
88.16

F1

81.80
79.60
81.16

92.40
88.80
82.10
93.26
80.70
83.90

79.11
82.30
81.43
81.39

79.50
84.60
86.40

P

48.70
46.40
50.14

78.50
64.60
-
72.99
76.00
63.75

49.24
50.52
50.57
50.00

64.00
67.50
72.10

48.10
45.53
44.48

63.00
53.10
-
63.37
50.90
60.43

50.17
55.30
54.30
54.73

62.90
75.80
77.24

48.40
45.70
47.14

69.90
58.30
62.50
67.01
61.00
62.04

49.70
52.81
52.37
52.26

63.40
71.40
74.58

65.10
62.65
64.15

81.15
73.60
72.30
80.14
70.85
72.97

64.41
67.56
66.90
66.83

71.45
78.00
80.49

Table 1: Comparison of our method (multi-head) with the state-of-the-art on the ACE04, CoNLL04, DREC
and ADE datasets. The models: (i) multi-head+E (the model + the Edmond algorithm to produce a tree-
structured output), (ii) single-head (the model predicts only one head per token) and (iii) multi-head EC (the
model predicts only the entity classes assuming that the boundaries are given) are slight variations of the
multi-head model adapted for each dataset and evaluation. The (cid:51)and (cid:55) symbols indicate whether or not the
models rely on any hand-crafted features or additional tools. Note that all the variations of our models do not
rely on any additional features. We include here different evaluation types (strict, relaxed and boundaries)
to be able to compare our results against previous studies. Finally, we report results in terms of Precision,
Recall, F1 for the two subtasks as well as overall F1, averaging over both subtasks. Bold entries indicate the
best result among models that only consider automatically learned features.

relu activation only in the ACE04 and tanh in all other datasets. We employ the tech-

nique of early stopping based on the validation set. In all the datasets examined in this

study, we obtain the best hyperparameters after 60 to 200 epochs depending on the size

of the dataset. We select the best epoch according to the results in the validation set.

For more details about the effect of each hyperparameter to the model performance see

the Appendix.

5. Results and discussion

5.1. Results

In Table 1, we present the results of our analysis. The ﬁrst column indicates the con-

sidered dataset. In the second column, we denote the model which is applied (i.e.,

previous work and the proposed models). The proposed models are the following:

(i) multi-head is the proposed model with the CRF layer for NER and the sigmoid loss

for multiple head prediction, (ii) multi-head+E is the proposed model with addition

of Edmonds’ algorithm to guarantee a tree-structured output for the DREC dataset,

17

(iii) single-head is the proposed method but it predicts only one head per token using a

softmax loss instead of a sigmoid, and (iv) multi-head EC is the proposed method with

a softmax to predict the entity classes assuming that the boundaries are given, and the

sigmoid loss for multiple head selection. Table 1 also indicates whether the different

settings include hand-crafted features or features derived from NLP tools (e.g., POS
taggers, dependency parsers). We use the (cid:51) symbol to denote that the model includes
this kind of additional features and the (cid:55) symbol to denote that the model is only based

on automatically extracted features. Note that all the variations of our model do not

rely on any additional features. In the next column, we declare the type of evaluation

conducted for each experiment. We include here different evaluation types to be able

to compare our results against previous studies. Speciﬁcally, we use three evaluation

types, namely:

(i) Strict: an entity is considered correct if the boundaries and the type of the en-

tity are both correct; a relation is correct when the type of the relation and the

argument entities are both correct,

(ii) Boundaries: an entity is considered correct if only the boundaries of the entity

are correct (entity type is not considered); a relation is correct when the type of

the relation and the argument entities are both correct and

(iii) Relaxed: we score a multi-token entity as correct if at least one of its comprising

token types is correct assuming that the boundaries are given; a relation is correct

when the type of the relation and the argument entities are both correct.

In the next three columns, we present the results for the entity identiﬁcation task (Pre-

cision, Recall, F1) and then (in the subsequent three columns) the results of the relation

extraction task (Precision, Recall, F1). Finally, in the last column, we report an addi-

tional F1 measure which is the average F1 performance of the two subtasks. We mark

with bold font in Table 1, the best result for each dataset among those models that use

only automatically extracted features.

Considering the results in the ACE04, we observe that our model outperforms the

model of Katiyar & Cardie (2017) by ∼2% in both tasks. This improvement can be

18

explained by the use of the multi-head selection method which can naturally capture

multiple relations and model them as a multi-label problem. Unlike the work of Kati-

yar & Cardie (2017), the class probabilities do not necessarily sum up to one since the

classes are considered independent. Moreover, we use a CRF-layer to model the NER

task to capture dependencies between sequential tokens. Finally, we obtain more ef-

fective word representations by using character-level embeddings. On the other hand,

our model performs within a reasonable margin (∼0.5% for the NER task and ∼1%

for the RE task) compared to Miwa & Bansal (2016). This difference is explained by

the fact that the model of Miwa & Bansal (2016) relies on POS tagging and syntactic

features derived by dependency parsing. However, this kind of features relies on NLP

tools that are not always accurate for various languages and contexts. For instance, the

same model is adopted by the work of Li et al. (2017) for the ADE biomedical dataset

and in this dataset our model reports more than 3% improvement in the RE task. This

shows that our model is able to produce automatically extracted features which perform

reasonably well in all contexts (e.g., news, biomedical).

For the CoNLL04 dataset, there are two different evaluation settings, namely re-

laxed and strict. In the relaxed setting, we perform an EC task instead of NER as-

suming that the boundaries of the entities are given. We adopt this setting to produce

comparable results with previous studies (Gupta et al., 2016; Adel & Sch¨utze, 2017).

Similar to Adel & Sch¨utze (2017), we present results of single models and no ensem-

bles. We observe that our model outperforms all previous models that do not rely on

complex hand-crafted features by a large margin (>4% for both tasks). Unlike these

previous studies that consider pairs of entities to obtain the entity types and the cor-

responding relations, we model the whole sentence at once. That way, our method is

able to directly infer all entities and relations of a sentence and beneﬁt from their pos-

sible interactions that cannot be modeled when training is performed for each entity

pair individually, one at a time. In the same setting, we also report the results of Gupta

et al. (2016) in which they use multiple complicated hand-crafted features coming from

NLP tools. Our model performs slightly better for the EC task and within a margin of

1% in terms of overall F1 score. The difference in the overall performance is due to

the fact that our model uses only automatically generated features. We also report re-

19

sults on the same dataset conducting NER (i.e., predicting entity types and boundaries)

and evaluating using the strict evaluation measure, similar to Miwa & Sasaki (2014).

Our results are not directly comparable to the work of Miwa & Sasaki (2014) because

we use the splits provided by Gupta et al. (2016). However, in this setting we present

the results from Miwa & Sasaki (2014) as reference. We report an improvement of

∼2% overall F1 score, which suggests that our neural model is able to extract more

informative representations compared to feature-based approaches.

We also report results for the DREC dataset, with two different evaluation settings.

Speciﬁcally, we use the boundaries and the strict settings. We transform the previous

results from Bekoulis et al. (2018) to the boundaries setting to make them comparable

to our model since in their work, they report token-based F1 score, which is not a

common evaluation metric in relation extraction problems. Also, in their work, they

focus on identifying only the boundaries of the entities and not the types (e.g., Floor,

Space). In the boundaries evaluation, we achieve ∼3% improvement for both tasks.

This is due to the fact that their quadratic scoring layer is beneﬁcial for the RE task, yet

complicates NER, which is usually modeled as a sequence labeling task. Moreover,

we report results using the strict evaluation which is used in most related works. Using

the prior knowledge that each entity has only one head, we can simplify our model and

predict only one head each time (i.e., using a softmax loss). The difference between

the single and the multi-head models is marginal (<0.1% for both tasks). This shows

that our model (multi-head) can adapt to various environments, even if the setting is

single head (in terms of the application, and thus also in both training and test data).

Finally, we compare our model with previous work (Li et al., 2016, 2017) on the

ADE dataset. The previous models (Li et al., 2016, 2017) both use hand-crafted fea-

tures or features derived from NLP tools. However, our model is able to outperform

both models using the strict evaluation metric. We report an improvement of ∼2% in

the NER and ∼3% in the RE tasks, respectively. The work of Li et al. (2017) is similar

to Miwa & Bansal (2016) and strongly relies on dependency parsers to extract syntactic

information. A possible explanation for the better result obtained from our model is

that the pre-calculated syntactic information obtained using external tools either is not

so accurate or important for biomedical data.

20

Settings

Multi-head
−Label embeddings
−Character embeddings
−CRF loss

P

81.01
80.61
80.42
80.47

Entity
R

81.31
80.91
79.52
81.50

F1

81.16
80.77
79.97
80.98

P

50.14
50.00
49.06
47.34

Relation
R

F1

Overall F1

44.48
42.92
41.62
42.84

47.14
46.18
45.04
44.98

64.15
63.48
62.50
62.98

Table 2: Ablation tests on the ACE04 test dataset.

5.2. Analysis of feature contribution

We conduct ablation tests on the ACE04 dataset reported in Table 2 to analyze the

effectiveness of the various parts of our joint model. The performance of the RE task

decreases (∼1% in terms of F1 score) when we remove the label embeddings layer and

only use the LSTM hidden states as inputs for the RE task. This shows that the NER

labels, as expected, provide meaningful information for the RE component.

Removing character embeddings also degrades the performance of both NER (∼1%)

and RE (∼2%) tasks by a relatively large margin. This illustrates that composing words

by the representation of characters is effective, and our method beneﬁts from additional

information such as capital letters, sufﬁxes and preﬁxes within the token (i.e., its char-

acter sequences).

Finally, we conduct experiments for the NER task by removing the CRF loss layer

and substituting it with a softmax. Assuming independent distribution of labels (i.e.,

softmax) leads to a slight decrease in the F1 performance of the NER module and a

∼2% decrease in the performance of the RE task. This happens because the CRF loss

is able to capture the strong tag dependencies (e.g., I-LOC cannot follow B-PER) that

are present in the dataset instead of just assuming that the tag decision for each token

is independent from tag decisions of neighboring tokens.

6. Conclusion

In this work, we present a joint neural model to simultaneously extract entities and

relations from textual data. Our model comprises a CRF layer for the entity recogni-

tion task and a sigmoid layer for the relation extraction task. Speciﬁcally, we model

the relation extraction task as a multi-head selection problem since one entity can have

21

multiple relations. Previous models on this task rely heavily on external NLP tools (i.e.,

POS taggers, dependency parsers). Thus, the performance of these models is affected

by the accuracy of the extracted features. Unlike previous studies, our model produces

automatically generated features rather than relying on hand-crafted ones, or existing

NLP tools. Given its independence from such NLP or other feature generating tools,

our approach can be easily adopted for any language and context. We demonstrate

the effectiveness of our approach by conducting a large scale experimental study. Our

model is able to outperform neural methods that automatically generate features while

the results are marginally similar (or sometimes better) compared to feature-based neu-

ral network approaches.

As future work, we aim to explore the effectiveness of entity pre-training for the en-

tity recognition module. This approach has been proven beneﬁcial in the work of Miwa

& Bansal (2016) for both the entity and the relation extraction modules. In addition,

we are planning to explore a way to reduce the calculations in the quadratic relation

scoring layer. For instance, a straightforward way to do so is to use in the sigmoid layer

only the tokens that have been identiﬁed as entities.

References

Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat,

S., Irving, G., Isard, M., Kudlur, M., Levenberg, J., Monga, R., Moore, S., Murray,

D. G., Steiner, B., Tucker, P., Vasudevan, V., Warden, P., Wicke, M., Yu, Y., & Zheng,

X. (2016). Tensorﬂow: A system for large-scale machine learning. In Proceedings

of the 12th USENIX Conference on Operating Systems Design and Implementation

(pp. 265–283). Berkeley, CA, USA.

Adel, H., & Sch¨utze, H. (2017). Global normalization of convolutional neural networks

for joint entity and relation classiﬁcation. In Proceedings of the 2017 Conference

on Empirical Methods in Natural Language Processing. Copenhagen, Denmark:

Association for Computational Linguistics.

Bach, N., & Badaskar, S. (2007). A review of relation extraction. Literature review for

Language and Statistics II, .

22

Bekoulis, G., Deleu, J., Demeester, T., & Develder, C. (2017). Reconstructing the

house from the ad: Structured prediction on real estate classiﬁeds. In Proceedings of

the 15th Conference of the European Chapter of the Association for Computational

Linguistics: (Volume 2, Short Papers) (pp. 274–279). Valencia, Spain.

Bekoulis, G., Deleu, J., Demeester, T., & Develder, C. (2018). An attentive neural

architecture for joint segmentation and parsing and its application to real estate ads.

Expert Systems with Applications, 102, 100 – 112. doi:10.1016/j.eswa.2018.

02.031.

Bengio, Y., Simard, P., & Frasconi, P. (1994). Learning long-term dependencies

with gradient descent is difﬁcult. Transactions on neural networks, 5(2), 157–166.

doi:10.1109/72.279181.

Sinica, 14, 1396–1400.

Chu, Y.-J., & Liu, T.-H. (1965). On shortest arborescence of a directed graph. Scientia

Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011).

Natural language processing (almost) from scratch. Journal of Machine Learning

Research, 12, 2493–2537.

Culotta, A., & Sorensen, J. (2004). Dependency tree kernels for relation extraction. In

Proceedings of the 42nd Annual Meeting on Association for Computational Linguis-

tics (pp. 423–429). Barcelona, Spain. doi:10.3115/1218955.1219009.

Doddington, G. R., Mitchell, A., Przybocki, M. A., Ramshaw, L. A., Strassel, S., &

Weischedel, R. M. (2004). The automatic content extraction (ace) program-tasks,

data, and evaluation. In Proceedings Fourth International Conference on Language

Resources and Evaluation (p. 1). Lisbon, Portugal volume 2.

Edmonds, J. (1967). Optimum branchings. Journal of research of the National Bureau

of Standards, 71B(4), 233–240.

Fundel, K., Kffner, R., & Zimmer, R.

(2007).

Relex-relation extraction

using dependency parse trees.

Bioinformatics, 23(3), 365–371. doi:10.

1093/bioinformatics/btl616.

23

Gupta, P., Sch¨utze, H., & Andrassy, B. (2016). Table ﬁlling multi-task recurrent neural

network for joint entity and relation extraction. In Proceedings of COLING 2016,

the 26th International Conference on Computational Linguistics: Technical Papers

(pp. 2537–2547).

Gurulingappa, H., MateenRajpu, A., & Toldo, L. (2012a). Extraction of potential

adverse drug events from medical case reports. Journal of Biomedical Semantics,

3(1), 1–15. doi:10.1186/2041-1480-3-15.

Gurulingappa, H., Rajput, A. M., Roberts, A., Fluck, J., Hofmann-Apitius, M., &

Toldo, L. (2012b). Development of a benchmark corpus to support the auto-

matic extraction of drug-related adverse effects from medical case reports. Jour-

nal of Biomedical Informatics, 45(5), 885 – 892. doi:https://doi.org/10.

1016/j.jbi.2012.04.008.

Huang, Z., Xu, W., & Yu, K. (2015). Bidirectional LSTM-CRF models for sequence

tagging. arXiv preprint arXiv:1508.01991, .

Kambhatla, N. (2004). Combining lexical, syntactic, and semantic features with maxi-

mum entropy models for extracting relations. In Proceedings of the Annual Meeting

of the Association for Computational Linguistics on Interactive poster and demon-

stration sessions. Barcelona, Spain. doi:10.3115/1219044.1219066.

Kate, R. J., & Mooney, R. (2010).

Joint entity and relation extraction using card-

pyramid parsing. In Proceedings of the 14th Conference on Computational Natural

Language Learning (pp. 203–212). Uppsala, Sweden: Association for Computa-

tional Linguistics.

Katiyar, A., & Cardie, C. (2017). Going out on a limb: Joint extraction of entity

mentions and relations without dependency trees. In Proceedings of the 55st Annual

Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).

Vancouver, Canada.

Kingma, D., & Ba, J. (2015). Adam: A method for stochastic optimization. In Inter-

national Conference on Learning Representations. San Diego, USA.

24

Lafferty, J., McCallum, A., & Pereira, F. (2001). Conditional random ﬁelds: Proba-

bilistic models for segmenting and labeling sequence data. In Proceedings of the

18th International Conference on Machine Learning (pp. 282–289). San Francisco,

USA: Morgan Kaufmann.

Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K., & Dyer, C. (2016).

Neural architectures for named entity recognition. In Proceedings of the 2016 Con-

ference of the North American Chapter of the Association for Computational Lin-

guistics: Human Language Technologies (pp. 260–270). San Diego, California.

Li, F., Zhang, M., Fu, G., & Ji, D. (2017). A neural joint model for entity

and relation extraction from biomedical text. BMC Bioinformatics, 18(1), 1–11.

doi:10.1186/s12859-017-1609-9.

Li, F., Zhang, Y., Zhang, M., & Ji, D. (2016). Joint models for extracting adverse

drug events from biomedical text. In Proceedings of the Twenty-Fifth International

Joint Conference on Artiﬁcial Intelligence (pp. 2838–2844). New York, USA: IJ-

CAI/AAAI Press.

Li, Q., & Ji, H. (2014). Incremental joint extraction of entity mentions and relations.

In Proceedings of the 52nd Annual Meeting of the Association for Computational

Linguistics (Volume 1: Long Papers) (pp. 402–412). Baltimore, USA.

Ma, X., & Hovy, E. (2016). End-to-end sequence labeling via bi-directional LSTM-

CNNs-CRF. In Proceedings of the 54th Annual Meeting of the Association for Com-

putational Linguistics (Volume 1: Long Papers) (pp. 1064–1074). Berlin, Germany.

Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed

representations of words and phrases and their compositionality. In Proceedings of

the 26th International Conference on Neural Information Processing Systems (pp.

3111–3119). Nevada, United States: Curran Associates, Inc.

Miwa, M., & Bansal, M. (2016). End-to-end relation extraction using LSTMs on se-

quences and tree structures. In Proceedings of the 54th Annual Meeting of the As-

25

sociation for Computational Linguistics (Volume 1: Long Papers) (pp. 1105–1116).

Berlin, Germany.

Miwa, M., & Sasaki, Y. (2014). Modeling joint entity and relation extraction with

table representation. In Proceedings of the 2014 Conference on Empirical Methods

in Natural Language Processing (pp. 1858–1869). Doha, Qatar: Association for

Computational Linguistics.

Moen, S., & Ananiadou, T. S. S. (2013). Distributional semantics resources for biomed-

ical text processing.

In Proceedings of the 5th International Symposium on Lan-

guages in Biology and Medicine (pp. 39–43). Tokyo, Japan.

Nadeau, D., & Sekine, S. (2007). A survey of named entity recognition and classiﬁca-

tion. Lingvisticae Investigationes, 30(1), 3–26. doi:10.1075/li.30.1.03nad.

Pascanu, R., Mikolov, T., & Bengio, Y. (2013). On the difﬁculty of training recurrent

neural networks. In Proceedings of the 30th International Conference on Interna-

tional Conference on Machine Learning (pp. 1310–1318). Atlanta, USA: JMLR.org.

Rink, B., & Harabagiu, S. (2010). Utd: Classifying semantic relations by combining

lexical and semantic resources. In Proceedings of the 5th International Workshop

on Semantic Evaluation (pp. 256–259). Los Angeles, California: Association for

Computational Linguistics.

Roth, D., & Yih, W.-t. (2004). A linear programming formulation for global infer-

ence in natural language tasks. In HLT-NAACL 2004 Workshop: Eighth Conference

on Computational Natural Language Learning (CoNLL-2004) (pp. 1–8). Boston,

USA: Association for Computational Linguistics. URL: http://www.aclweb.

org/anthology/W04-2401.

dos Santos, C., Xiang, B., & Zhou, B. (2015). Classifying relations by ranking with

convolutional neural networks. In Proceedings of the 53rd Annual Meeting of the As-

sociation for Computational Linguistics and the 7th International Joint Conference

on Natural Language Processing (Volume 1: Long Papers) (pp. 626–634). Beijing,

China.

26

Socher, R., Chen, D., Manning, C. D., & Ng, A. (2013). Reasoning with neural

tensor networks for knowledge base completion.

In Proceedings of the 26th In-

ternational Conference on Neural Information Processing Systems (pp. 926–934).

Nevada, United States: Curran Associates, Inc.

Socher, R., Huval, B., Manning, C. D., & Ng, A. Y. (2012). Semantic compositionality

through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference

on Empirical Methods in Natural Language Processing and Computational Natural

Language Learning (pp. 1201–1211). Jeju Island, Korea: Association for Computa-

tional Linguistics.

Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014).

Dropout: A simple way to prevent neural networks from overﬁtting. Journal of

Machine Learning Research, 15(1), 1929–1958.

Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with

neural networks.

In Proceedings of the 27th International Conference on Neural

Information Processing Systems (pp. 3104–3112). Montreal, Canada: MIT Press.

Taskar, B., Guestrin, C., & Koller, D. (2003). Max-margin markov networks.

In

Proceedings of the 16th International Conference on Neural Information Processing

Systems (pp. 25–32). Bangkok, Thailand: MIT Press.

Tsochantaridis, I., Hofmann, T., Joachims, T., & Altun, Y. (2004). Support vector

machine learning for interdependent and structured output spaces. In Proceedings

of the 21st International Conference on Machine Learning (pp. 104–112). Helsinki,

Finland: ACM. doi:10.1145/1015330.1015341.

Vu, N. T., Adel, H., Gupta, P., & Sch¨utze, H. (2016). Combining recurrent and con-

volutional neural networks for relation classiﬁcation.

In Proceedings of the 2016

Conference of the North American Chapter of the Association for Computational

Linguistics: Human Language Technologies (pp. 534–539). San Diego, California.

URL: http://www.aclweb.org/anthology/N16-1065.

27

Xu, K., Feng, Y., Huang, S., & Zhao, D. (2015a). Semantic relation classiﬁcation via

convolutional neural networks with simple negative sampling.

In Proceedings of

the 2015 Conference on Empirical Methods in Natural Language Processing (pp.

536–540). Lisbon, Portugal: Association for Computational Linguistics. URL:

http://aclweb.org/anthology/D15-1062.

Xu, Y., Mou, L., Li, G., Chen, Y., Peng, H., & Jin, Z. (2015b). Classifying relations via

long short term memory networks along shortest dependency paths. In Proceedings

of the 2015 Conference on Empirical Methods in Natural Language Processing (pp.

1785–1794). Lisbon, Portugal: Association for Computational Linguistics.

Yang, B., & Cardie, C. (2013). Joint inference for ﬁne-grained opinion extraction.

In Proceedings of the 51st Annual Meeting of the Association for Computational

Linguistics (Volume 1: Long Papers) (pp. 1640–1649). Soﬁa, Bulgaria. URL:

http://www.aclweb.org/anthology/P13-1161.

Zelenko, D., Aone, C., & Richardella, A. (2003). Kernel methods for relation

extraction.

Journal of Machine Learning Research, 3, 1083–1106. doi:10.

3115/1118693.1118703.

Zeng, D., Liu, K., Lai, S., Zhou, G., & Zhao, J. (2014). Relation classiﬁcation via

convolutional deep neural network. In Proceedings of COLING 2014, the 25th In-

ternational Conference on Computational Linguistics: Technical Papers (pp. 2335–

2344).

Spain.

Zhang, D., & Wang, D. (2015). Relation classiﬁcation via recurrent neural network.

arXiv preprint arXiv:1508.01006, .

Zhang, X., Cheng, J., & Lapata, M. (2017). Dependency parsing as head selection.

In Proceedings of the 15th Conference of the European Chapter of the Association

for Computational Linguistics: (Volume 1, Long Papers) (pp. 665–676). Valencia,

Zheng, S., Hao, Y., Lu, D., Bao, H., Xu, J., Hao, H., & Xu, B. (2017). Joint entity and

28

relation extraction based on a hybrid neural network. Neurocomputing, 257, 59 –

66. doi:10.1016/j.neucom.2016.12.075.

29

Appendix

In this section, we report additional results for our multi-head selection framework.

Speciﬁcally, we (i) compare our model with the model of Lample et al. (2016) (i.e.,

optimize only over the NER task), (ii) explore several hyperparameters of the network

(e.g., dropout, LSTM size, character embeddings size), and (iii) report F1 score using

different word embeddings compared to the embeddings used in previous works.

In Table 1 of the main paper, we focused on comparing our model against other

joint models that are able to solve the two tasks (i.e., NER and relation extraction) si-

multaneously, mainly demonstrating superiority of phrasing the relation extraction as

a multi-head selection problem (enabling the extraction of multiple relations at once).

Here, in Table A1, we evaluate the performance of just the ﬁrst module of our joint

multi-head model: we compare the performance of the NER component of our model

against the state-of-the-art NER model of Lample et al. (2016). The results indicate

a marginal performance improvement of our model over Lample’s NER baseline in 3

out of 4 datasets. The improvement of our model’s NER part is not substantial, since

(i) our NER part is almost identical to Lample’s, and (ii) recent advances in NER per-

formance among neural systems are relatively small (improvements in the order of few

0.1 F1 points – for instance, the contribution of Ma & Hovy (2016) and Lample et al.

(2016) on the CoNLL-2003 test set is 0.01% and 0.17% F1 points, respectively). This

slight improvement suggests that the interaction of the two components by sharing the

underlying LSTM layer is indeed beneﬁcial (e.g., identifying a Works for relation might

be helpful for the NER module in detecting the type of the two entities, i.e., PER, ORG

and vice versa). Note that improving NER in isolation was not the objective of our

multi-head model, but we rather aimed to compare our model against other joint mod-

els that solve the task of entity recognition and relation identiﬁcation simultaneously.

We thus did not envision to claim or achieve state-of-the-art performance in each of the

individual building blocks of our joint model.

Tables A2, A3 and A4 show the performance of our model on the test set for dif-

ferent values of the embedding dropout, LSTM layer dropout and the LSTM output

dropout hyperparameters, respectively. Note that the hyperparameter values used for

1

the results in Section 5 were obtained by tuning over the development set, and these

are indicated in bold face in the tables below. We vary one hyperparameter at a time

in order to assess the effect of a particular hyperparameter. The main outcomes from

these tables are twofold: (i) low dropout values (e.g., 0, 0.1) lead to a performance

decrease in the overall F1 score (see Table A3 where a ∼3% F1 decrease is reported on

the ACE04 dataset) and (ii) average dropout values (i.e., 0.2-0.4) lead to consistently

similar results.

In Tables A5, A6, A7 and A8, we report results for different values of the LSTM

size, the size of the character embeddings, the size of the label embeddings and the

layer width of the neural network l (both for the entity and the relation scoring layers),

respectively. The reported results show that different hyperparameters settings do lead

to noticeable performance differences, but we do not observe any clear trend. More-

over, we have not observed any signiﬁcant performance improvement that affects the

overall ranking of the models as reported in Table 1. On the other hand, the results indi-

cate that increasing (character and label) embedding size and layer dimensions leads to

a slight decrease in performance for the CoNLL04 dataset. This can be explained by the

fact that the CoNLL04 dataset is relatively small and using more trainable model pa-

rameters (i.e., larger hyperparameter values) can make our multi-head selection method

to overﬁt quickly on the training set. In almost any other case, variation of the hyper-

parameters does not affect the ranking of the models reported in Table 1.

Model

ACE
04

NER baseline
multi-head

CoNLL
04

NER baseline
multi-head

DREC

ADE

NER baseline
multi-head

NER baseline
multi-head

Entity
R

81.13
81.31

83.13
84.06

84.89
83.98

88.59
88.16

P

81.06
81.01

84.38
83.75

78.22
78.97

83.97
84.72

F1

81.10
81.16

83.75
83.90

81.42
81.39

86.22
86.40

Table A1: Comparison of the multi-head selection model (only the NER component) against the NER base-
line of Lample et al. (2016). Bold font indicates the best results for each dataset.

In the main results (see Section 5), to guarantee a fair comparison to previous work

and to obtain comparable results that are not affected by the input embeddings, we use

embeddings used also in prior studies. To assess the performance of our system to input

2

Table A2: Model performance for different embedding dropout values. Bold entries indicate the result
reported in Section 5.

LSTM
Dropout

Relation
R

F1

Overall F1

Embedding
Dropout

Relation
R

F1

Overall F1

43.28
43.55
44.48
42.45
42.92
43.55

52.37
51.42
57.82
54.74
60.43
56.64

53.87
53.57
54.73
54.19
52.69
52.61

78.87
77.90
77.24
77.37
76.71
75.98

38.86
42.06
44.12
44.48
41.52
39.32

60.19
62.56
60.66
59.48
60.43
52.13

53.61
54.64
54.73
53.79
53.10
49.96

78.15
78.00
78.86
77.24
77.51
76.99

45.37
46.51
47.14
45.84
45.20
45.21

59.65
57.48
61.08
62.18
62.04
61.05

52.46
52.71
52.26
52.86
50.99
51.59

75.62
74.80
74.58
74.80
74.76
73.43

43.05
45.91
47.01
47.14
44.61
42.71

59.76
63.16
62.82
63.46
62.04
57.89

51.44
52.56
52.26
51.39
52.22
50.18

75.34
74.99
75.39
74.58
74.66
73.61

63.10
63.84
64.15
63.59
63.08
62.77

71.36
70.42
72.14
73.26
72.97
72.39

66.85
67.10
66.83
66.94
66.01
65.96

81.14
80.56
80.49
80.54
80.51
79.37

61.61
63.59
64.21
64.15
62.95
61.47

72.49
74.05
74.45
74.39
72.97
70.67

66.30
66.89
66.83
66.03
66.71
64.78

80.98
80.73
80.98
80.49
80.40
79.64

E
C
A

4
0

L
L
N
o
C

4
0

C
E
R
D

E
D
A

0.5
0.4
0.3
0.2
0.1
0

0.5
0.4
0.3
0.2
0.1
0

0.5
0.4
0.3
0.2
0.1
0

0.5
0.4
0.3
0.2
0.1
0

P

80.66
80.97
81.01
81.15
80.86
80.21

82.53
83.66
82.19
84.07
83.75
82.79

78.19
78.47
78.97
78.16
78.83
78.42

84.73
84.51
84.72
84.66
85.10
83.67

Entity
R

81.03
81.39
81.31
81.54
81.06
80.45

83.60
83.04
84.24
84.62
84.06
84.71

84.51
84.73
83.98
84.11
83.34
82.34

88.68
88.21
88.16
87.98
87.43
87.01

F1

80.84
81.18
81.16
81.34
80.96
80.32

83.06
83.35
83.20
84.34
83.90
83.74

81.23
81.48
81.39
81.02
81.02
80.33

86.66
86.32
86.40
86.29
86.25
85.31

P

47.66
49.90
50.14
49.81
47.74
47.00

69.28
65.17
64.72
71.96
63.75
66.21

51.12
51.87
50.00
51.60
49.38
50.62

72.63
71.93
72.10
72.39
72.91
71.04

P

80.27
81.18
81.19
81.01
81.27
80.54

84.18
84.43
86.44
84.73
83.75
84.16

77.76
78.66
78.97
77.85
78.94
78.59

85.01
84.66
84.60
84.72
84.36
83.80

Entity
R

80.08
81.36
81.63
81.31
81.32
79.94

86.28
85.45
85.73
85.91
84.06
82.76

84.83
83.98
83.98
83.68
83.62
80.18

88.29
88.37
88.66
88.16
87.98
87.64

F1

80.18
81.27
81.41
81.16
81.29
80.24

85.22
84.94
86.09
85.32
83.90
83.45

81.15
81.23
81.39
80.66
81.21
79.38

86.62
86.47
86.58
86.40
86.13
85.68

P

48.25
50.54
50.31
50.14
48.20
46.73

59.35
63.77
65.14
68.02
63.75
65.09

49.43
50.63
50.00
49.21
51.37
50.39

72.72
72.20
72.21
72.10
72.03
70.50

E
C
A

4
0

L
L
N
o
C

4
0

C
E
R
D

E
D
A

0.5
0.4
0.3
0.2
0.1
0

0.5
0.4
0.3
0.2
0.1
0

0.5
0.4
0.3
0.2
0.1
0

0.5
0.4
0.3
0.2
0.1
0

3

Table A3: Model performance for different LSTM layer dropout values. Bold entries indicate the result
reported in Section 5.

Table A4: Model performance for different LSTM output dropout values. Bold entries indicate the best
result reported in Section 5.

LSTM output
Dropout

Relation
R

F1

Overall F1

E
C
A

4
0

L
L
N
o
C

4
0

C
E
R
D

E
D
A

0.5
0.4
0.3
0.2
0.1
0

0.5
0.4
0.3
0.2
0.1
0

0.5
0.4
0.3
0.2
0.1
0

0.5
0.4
0.3
0.2
0.1
0

P

81.25
81.23
81.31
81.01
81.01
80.10

85.81
83.27
85.13
84.13
83.75
83.65

78.74
78.45
78.97
77.82
78.84
77.63

84.33
85.16
84.27
84.72
84.65
84.44

Entity
R

81.79
81.70
81.72
81.31
81.12
80.69

86.84
84.89
84.89
84.52
84.06
84.89

84.22
85.20
83.98
84.68
83.75
83.85

87.95
88.16
88.00
88.16
88.04
88.14

F1

81.52
81.47
81.51
81.16
81.07
80.39

86.32
84.08
85.01
84.32
83.90
84.27

81.39
81.69
81.39
81.11
81.22
80.62

86.10
86.63
86.10
86.40
86.31
86.25

P

51.16
51.44
48.69
50.14
47.55
47.20

64.18
66.07
64.82
66.03
63.75
65.23

51.24
50.34
50.00
51.05
51.74
51.16

71.54
72.87
71.83
72.10
72.38
71.64

41.94
42.77
44.21
44.48
42.82
40.54

59.01
61.37
55.45
57.58
60.43
53.79

52.69
55.45
54.73
54.19
54.75
51.39

77.27
77.81
77.42
77.24
77.49
77.82

46.09
46.71
46.35
47.14
45.06
43.61

61.48
63.63
59.77
61.52
62.04
58.96

51.96
52.77
52.26
52.57
53.20
51.28

74.29
75.26
74.52
74.58
74.85
74.61

63.81
64.09
63.93
64.15
63.07
62.00

73.90
73.85
72.39
72.92
72.97
71.61

66.68
67.23
66.83
66.84
67.21
65.95

80.20
80.95
80.31
80.49
80.58
80.43

Relation
R

F1

Overall F1

LSTM
Size

32
64
128

32
64
128

32
64
128

32
64
128

P

80.99
81.01
80.31

82.83
83.75
82.43

77.74
78.97
79.04

83.89
84.72
84.27

Entity
R

81.25
81.31
80.87

83.13
84.06
83.04

85.43
83.98
83.49

87.78
88.16
87.87

F1

81.12
81.16
80.59

82.98
83.90
82.73

81.40
81.39
81.20

85.79
86.40
86.04

P

50.33
50.14
47.30

65.78
63.75
64.86

50.92
50.00
51.27

70.46
72.10
71.36

42.60
44.48
41.77

58.29
60.43
53.79

52.31
54.73
53.64

76.89
77.24
76.77

46.14
47.14
44.36

61.81
62.04
58.81

51.60
52.26
52.42

73.54
74.58
73.97

Character
Embeddings

Relation
R

F1

Overall F1

P

81.02
81.01
81.32

83.33
83.75
85.15

79.73
78.97
78.08

84.80
84.72
84.65

Entity
R

81.57
81.31
81.54

84.34
84.06
82.95

84.17
83.98
84.80

88.00
88.16
88.08

15
25
50

15
25
50

15
25
50

15
25
50

P

47.87
50.14
49.77

66.03
63.75
59.84

52.52
50.00
51.03

72.74
72.10
72.17

44.78
44.48
44.02

57.11
60.43
52.61

55.30
54.73
54.28

77.51
77.24
77.45

46.27
47.14
46.72

61.25
62.04
55.99

53.88
52.26
52.60

75.05
74.58
74.72

63.63
64.15
62.47

72.39
72.97
70.77

66.50
66.83
66.81

79.66
80.49
80.00

63.78
64.15
64.07

72.54
72.97
70.01

67.89
66.83
66.95

80.71
80.49
80.52

E
C
A

4
0

4
0

L
L
N
o
C

C
E
R
D

E
D
A

E
C
A

4
0

4
0

L
L
N
o
C

C
E
R
D

E
D
A

F1

81.29
81.16
81.43

83.83
83.90
84.04

81.89
81.39
81.30

86.37
86.40
86.33

4

Table A5: Model performance for different LSTM size values. Bold entries indicate the result reported in
Section 5.

Table A6: Model performance for different character embeddings size values. Bold entries indicate the result
reported in Section 5.

E
C
A

4
0

4
0

L
L
N
o
C

C
E
R
D

E
D
A

E
C
A

4
0

4
0

L
L
N
o
C

C
E
R
D

E
D
A

15
25
50

15
0
50

15
25
50

15
25
50

32
64
128

32
64
128

32
64
128

32
64
128

Label
Embeddings

Relation
R

F1

Overall F1

P

80.95
81.01
81.17

84.68
83.75
82.32

78.48
78.97
78.92

84.47
84.72
84.81

Entity
R

81.27
81.31
81.61

83.50
84.06
84.15

84.81
83.98
84.88

88.18
88.16
88.65

F1

81.11
81.16
81.39

84.08
83.90
83.23

81.53
81.39
81.79

86.29
86.40
86.69

P

49.27
50.14
48.01

62.21
63.75
59.30

51.83
50.00
51.35

71.93
72.10
72.46

43.80
44.48
44.48

56.16
60.43
55.92

53.21
54.73
53.23

77.49
77.24
78.68

46.37
47.14
46.18

59.03
62.04
57.56

52.51
52.26
52.27

74.61
74.58
75.44

63.74
64.15
63.78

71.56
72.97
70.39

67.02
66.83
67.03

80.45
80.49
81.06

Table A7: Model performance for different label embeddings size values. Bold entries indicate the result
reported in Section 5.

variations, we also report results using different word embeddings (see Table A9) (i.e.,

Adel & Sch¨utze (2017); Li et al. (2017)) on the ACE04 dataset. Our results showcase

that our model, even when using different word embeddings, is still performing better

compared to other works that, like ours, do not rely on additional NLP tools.

Hidden layer
Size

Relation
R

F1

Overall F1

P

81.01
81.01
81.30

82.26
83.75
82.69

79.66
78.97
78.35

84.31
84.72
84.81

Entity
R

81.02
81.31
81.32

84.24
84.06
83.69

84.23
83.98
84.47

88.56
88.16
88.54

F1

81.02
81.16
81.31

83.24
83.90
83.19

81.89
81.39
81.30

86.38
86.40
86.63

P

48.81
50.14
51.58

65.96
63.75
64.46

52.42
50.00
48.53

71.67
72.10
72.29

43.26
44.48
43.68

59.24
60.43
55.45

51.45
54.73
53.08

78.51
77.24
78.20

45.87
47.14
47.30

62.42
62.04
59.62

51.93
52.26
50.70

74.93
74.58
75.13

63.44
64.15
64.31

72.83
72.97
71.40

66.91
66.83
66.00

80.66
80.49
80.87

Table A8: Model performance for different layer widths l of the neural network (both for the entity and the
relation scoring layers). Bold entries indicate the result reported in Section 5.

Embeddings

Size

Miwa & Bansal (2016)
Adel & Sch¨utze (2017)
Li et al. (2017)

200
50
200

P

81.01
82.18
81.51

Entity
R

81.31
79.83
81.35

F1

81.16
80.99
81.43

P

50.14
49.10
46.59

Relation
R

F1

Overall F1

44.48
41.40
44.43

47.14
44.92
45.49

64.15
62.96
63.46

Table A9: Model performance for different embeddings on the ACE04 dataset. Bold entries indicate the
result reported in Section 5.

5

