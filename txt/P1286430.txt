RL-Duet: Online Music Accompaniment Generation Using Deep Reinforcement
Learning

Nan Jiang1 Sheng Jin1 Zhiyao Duan2 Changshui Zhang1
1 Department of Automation, Tsinghua University
State Key Lab of Intelligent Technologies and Systems
Institute for Artiﬁcial Intelligence, Tsinghua University (THUAI)
Beijing National Research Center for Information Science and Technology (BNRist)
2 Department of Electrical and Computer Engineering, University of Rochester
{jiangn15, js17}@mails.tsinghua.edu.cn,
zhiyao.duan@rochester.edu, zcs@mail.tsinghua.edu.cn

0
2
0
2
 
b
e
F
 
8
 
 
]

G
L
.
s
c
[
 
 
1
v
2
8
0
3
0
.
2
0
0
2
:
v
i
X
r
a

Abstract

This paper presents a deep reinforcement learning algorithm
for online accompaniment generation, with potential for real-
time interactive human-machine duet improvisation. Differ-
ent from ofﬂine music generation and harmonization, online
music accompaniment requires the algorithm to respond to
human input and generate the machine counterpart in a se-
quential order. We cast this as a reinforcement learning prob-
lem, where the generation agent learns a policy to generate
a musical note (action) based on previously generated con-
text (state). The key of this algorithm is the well-functioning
reward model. Instead of deﬁning it using music composi-
tion rules, we learn this model from monophonic and poly-
phonic training data. This model considers the compatibil-
ity of the machine-generated note with both the machine-
generated context and the human-generated context. Exper-
iments show that this algorithm is able to respond to the
human part and generate a melodic, harmonic and diverse
machine part. Subjective evaluations on preferences show
that the proposed algorithm generates music pieces of higher
quality than the baseline method.

Introduction
Learning based automatic music generation has been an ac-
tive research area (Tatar and Pasquier 2019; Briot, Hadjeres,
and Pachet 2017). Existing learning based music generation
methods, however, have paid less attention to real-time in-
teractive music generation (e.g., interactive duet improvisa-
tion) between humans and machines: A machine agent and
a human player collaboratively create music by listening to
each other. In this work, we take one step towards this goal
and aim to solve the online music accompaniment genera-
tion as the ﬁrst step. Online music accompaniment genera-
tion is an interesting but challenging problem. It is interest-
ing because it requires comprehensive musicianship skills
including perception and composition, and the ability of fast
decision making. It further requires machines to adapt and
respond to human input in a timely fashion, leading to suc-
cessful human-computer collaboration.

In this work, we propose a novel algorithm (RL-Duet) for
online music accompaniment generation that could be fur-
ther developed toward a real-time interactive duet improvi-
sation system. In this setting, a human performer would im-

Figure 1: Online music accompaniment generation requires
the agent to adapt/respond to human input without delay.

provise a melody, and the machine would generate a coun-
terpoint accompaniment in real-time. We focus on the de-
sign of the algorithm instead of the system; the latter would
require signiﬁcant considerations on the interactivity and us-
ability through systematic user studies, but is beyond the
scope of this paper. We cast the problem of online mu-
sic accompaniment as a reinforcement learning problem.
The generation agent takes into consideration both the long-
term temporal structure and the inter-part harmonization,
and generates musical notes based on the context.

Reinforcement learning-based music generation has been
explored in (Jaques et al. 2017), where hand-crafted compo-
sition rules or criteria are used as the reward function for mu-
sic generation. The main disadvantages of rule-based reward
models, we argue, are two-fold: 1) It is difﬁcult to deﬁne a
comprehensive list of rules. There is a long list of compo-
sition rules or evaluation criteria in literature (Green 1989;
Jaques et al. 2017; Dong et al. 2018; Trieu and Keller 2018;
Yang and Lerch 2018). Some of them are hard rules (e.g. ,
note duration greater than 32nd note), while others are soft
rules (e.g., less empty bars, and more pitch variations). Most
of them only focus on one limited aspect, enforcing some
constraints on the process of music composition. 2) It is dif-
ﬁcult to balance different rules. First, the objective evalua-
tion of music composition is multi-criteria, where good rat-
ings on one criterion does not necessarily imply good rat-
ings on another. In addition, some of the rules can be contra-

dictory to each other. For example, some hand-crafted rules
punish ﬂat pitch contours and give high rewards to random
note ﬂuctuations, while other rules encourage repetition. Fi-
nally, the integration and balancing of these rules depend on
the music style and vary with music context and progression.
The aforementioned problems of rule-based rewards/criteria
have also been discussed in other research domains, such as
image generation (Theis, van den Oord, and Bethge 2016).
Instead of laboriously designing rule-based reward func-
tions, we directly learn the reward model from monophonic
and polyphonic music. This reward model considers both the
inter-part and intra-part compatibility of the generated notes.
It avoids unnecessary or incorrect constraints enforced by
human specialists. Therefore, it has the potential of generat-
ing more melodic and creative melodies.

To better understand our proposed model, we present
qualitative analysis, probing the intriguing nature of our pro-
posed reward model. We also conduct a user study for sub-
jective evaluation, which demonstrates the effectiveness of
our proposed method.

Our main contributions are as follows:

1. To our best knowledge, RL-Duet is the ﬁrst reinforcement
learning model for online music accompaniment genera-
tion using an ensemble of reward models.

2. Instead of relying on a rule-based objective function, RL-
Duet learns a comprehensive reward model from data,
considering both the inter-part and intra-part harmoniza-
tion of human and machine inputs.

3. Subjective evaluation shows that RL-Duet generates mu-
sic pieces of higher preferences than the baseline method.

Related Work

Symbolic Music Generation with Deep Learning
There is a long history of generating music with neural
networks (Briot, Hadjeres, and Pachet 2017). while a few
works directly generate audio signals (Oord et al. 2016; En-
gel et al. 2019), here we focus on symbolic-domain genera-
tion. Most works (Mozer 1994; Eck and Schmidhuber 2002;
Hutchings and McCormack 2017; Zhu et al. 2018) use RNN
(or LSTM, GRU) to generate single-track or multi-track mu-
sic sequentially. Other works employ the hierarchical struc-
ture (Chu, Urtasun, and Fidler 2017) or the attention mecha-
nism (Waite et al. 2016; Huang et al. 2019b) to model long-
term temporal dependencies. Generative Adversarial Net-
works (GAN) have also been used to learn the distribution of
music pieces (Yang, Chou, and Yang 2017; Dong et al. 2018;
Mogren 2016; Yu et al. 2017). However, GAN demands a
large amount of data and it is hard to stabilize the training
process.

Recently, the Gibbs sampling approach (Hadjeres, Pa-
chet, and Nielsen 2017; Huang et al. 2017; Yan et al. 2018)
has shown its effectiveness in generating coherent chorales,
since the generation process of Gibbs sampling resembles
how humans compose music: Iteratively modifying the mu-
sic fragments based on the whole music context. However,
Gibbs sampling could only be used ofﬂine. In this work, we
focus on online music accompaniment generation toward

real-time human machine interactive improvisation, which
requires the generation be online and in a sequential order
without delay.

Interactive Music Generation Systems

A recent review (Tatar and Pasquier 2019) presents a de-
tailed survey of interactive musical agent systems, from rule-
based to learning-based systems. According to (Tatar and
Pasquier 2019), interactive music generation methods can
be categorized into call and response and accompaniment.

For call and response, the agent learns to trade solos with
the user in real-time, i.e., the agent and the user take turns
to play. Evolutionary computation-based algorithms (Biles
and others 1994; Papadopoulos and Wiggins 1998) employ
a ﬁtness function to perform the genetic selection. Usually,
the ﬁtness function relies heavily on the musical knowledge.
Rule-based algorithms (Thom 2000) learn the playing mode
of a human performer, and applies percept-to-action func-
tions for solo trading.

This work belongs to online music accompaniment gen-
eration, which requires the machine agent to adapt and sup-
port the human player with accompaniment. In the early
Music Information Retrieval (MIR) literature, “music ac-
companiment” often refers to algorithms and systems that
are able to render pre-recorded/synthesized accompaniment
part following the real-time human solo performance (Dan-
nenberg 1984; Raphael 2010). These systems do not gen-
erate or improvise music. In this paper, we focus on ac-
companiment generation instead of accompaniment match-
ing. Recently, MuseGAN (Dong et al. 2018) is proposed
to use GAN-based models to generate multi-track music.
It could be extended to human-AI cooperative music gen-
eration. However, it generates additional tracks following
a speciﬁc track composed by human ofﬂine, not designed
for interactive human-machine duet improvisation. Google
built an interactive Bach Doodle (Huang et al. 2019a) to
harmonize user created melody. It uses Gibbs sampling to
generates the other three parts ofﬂine as well. Our proposed
RL-Duet algorithm, on the other hand, responds to the hu-
man input and generates accompaniment in an online fash-
ion without any delay. RL-Duet could be integrated to a real-
time human-machine interactive improvisation system, such
as our other work (Benetatos and Duan 2019). (Roberts et
al. 2016) presents a demo for generating the responses to
short melodic calls and bass accompaniment. However, no
technical details nor experiments were provided.

Reinforcement Learning for Sequence Generation

Recently, in the ﬁeld of natural language processing, many
works have employed reinforcement learning (RL) to di-
rectly optimize the tasks’ evaluation metrics, such as BLEU
or ROUGE (Ranzato et al. 2016; Williams 1992; Bahdanau
et al. 2017). These metrics provide a reasonable way to mea-
sure the quality of the generated sequences. For music gen-
eration, however, a key difﬁculty is on the design of rewards
that can measure music generation quality.

For online music generation, maximum likelihood esti-
mation is the most popular training paradigm. Despite the

differences in framework, data representation, and model ar-
chitecture, they all learn to predict the probability of the next
token conditioned on the previous tokens and then maximize
the likelihood of the whole dataset. A few recent works have
incorporated reinforcement learning (RL) for music genera-
tion (Guimaraes et al. 2017; Jaques et al. 2017). RL uses a
different training target which considers the long-term dis-
counted reward of the generated music. Among them, Se-
quenceTutor (Jaques et al. 2017) is mostly related to our RL-
Duet. It also uses a trained neural network to give a partial
reward, but mostly relies on more than ten hand-crafted mu-
sical rules, and therefore suffers from several problems men-
tioned in the Introduction Section. Moreover, SequenceTu-
tor does not consider the inter-part harmonization between
polyphonic parts, which is critical for human-machine inter-
active improvisation.

RL-Duet

Motivation for Reinforcement Learning
Most existing works on online music generation use Recur-
rent Neural Networks (RNN) trained by maximum likeli-
hood estimation (MLE) (Bharucha and Todd 1989; Mozer
1994; Eck and Schmidhuber 2002). They train RNNs to
learn the probability distribution of the next token condi-
tioned on the previous sequence. While such MLE methods
have shown decent results, they suffer from two major draw-
backs in sequence generation tasks.

Mismatch between training and testing conditions (ex-
posure bias): The model is typically trained to predict
the next token conditioned on the past sequence using
clean human-composed music. However, during testing, the
model has to make predictions based on its previous pre-
dictions. Any prior defective prediction will deteriorate the
quality of the following generation, and this deterioration
may accumulate for long sequences.

Mismatch between training criterion and generation
objectives: To maximize the likelihood of the whole dataset,
the MLE model minimizes the cross-entropy loss of the to-
ken at each time-step (training criterion), whereas the quality
of the generated music relates to the global coherence and
harmonization of the entire music piece (generation objec-
tives). In other words, when generating a sequence of notes,
MLE model chooses a token with the maximum probability
at each time-step, and will not take into account its long-
term effect on future tokens of the generated sequence. This
mismatch is the reason why beam search is often used to
improve sequence generation, which is not applicable in the
online setting, however.

We propose to cast the music generation problem as a
sequential decision-making problem, and use reinforcement
learning (RL) to address the above two problems. The agent
learns to maximize the long-term discounted reward by trial
and error, directly using model predictions at the training
time. Second, we learn a comprehensive reward model from
data, which captures 1) the global temporal and rhythmic
structure and 2) the harmonic inter-dependency between ma-
chine and human parts.

The MLE model can achieve a high accuracy (83.75%) of

Figure 2: Typical problems of maximum likelihood estima-
tion (MLE) in online accompaniment generation (H: human,
M: machine). The MLE model tends to produce many re-
peated notes, while the proposed RL-Duet model shows bet-
ter diversity in the melodic contour.

token prediction when it takes a duet composed by Bach as
the model input, but when generating on its own, it greedily
chooses the note with the highest probability at each time-
step, yielding ﬂat and monotonous music. Fig. 2 demon-
strates this typical failure mode of the MLE model. In com-
parison, RL-Duet better preserves the temporal and rhyth-
mic structures, diversity and inter-part harmonization.

Representation
Pitch, Duration, Articulation We use symbolic MIDI
pitches to encode notes. We quantize time into sixteenth
note, which is the shortest duration of notes in our dataset.
For notes with a longer duration, we use “hold” symbols to
encode their length. Two kinds of pitch representations are
used in RL-Duet. One uses a single “hold” symbol for all
the pitches, the other uses a separate “hold” symbol for each
pitch. These two representations are referred as “multi-hold”
and “single-hold” respectively. For example, a C4 quarter
note will be encoded as four time-steps: [C4, hold, hold,
hold] or [C4, C4 hold, C4 hold, C4 hold] in these two rep-
resentations. The “single-hold” representation is common in
symbolic music generation literature. However, our exper-
iments show that the “multi-hold” representation will gen-
erate music of more diversity, as this mitigates the extreme
imbalance of the number of the hold symbol and pitch num-
bers in the single “hold” representation. We further encode
rests with an additional “rest” symbol.

Beat Information Besides the pitch number of each note,
the music also contains additional rhythmic information.
Following DeepBach (Hadjeres, Pachet, and Nielsen 2017),
we make use of a subdivision list to encode the beat in-
formation. We quantize time with sixteenth notes, and each
beat is subdivided into four parts. The subdivision list, B =
[b0, b1, ..., bT ], containing each time-step’s subdivision beat
index bt, is a repeated list of [1, 2, 3, 4, 1, 2, 3, 4...].

Model Concepts An interactive duet improvisation con-
sists of two parts, the human part H = [h0, h1, ..., hT ] and
the machine part M = [m0, m1, ..., mT ], where ht and mt
are tokens at time-step t in the human part and the machine
part, respectively. During improvisation, the state at time-
step t is denoted as the joint state of the human part and the
machine part, st = (sh
t = h0:t−1
and sm
t = m0:t−1, representing previous tokens of the hu-

t ). We abbreviate sh

t , sm

Figure 3: Framework of RL-Duet. Generation Phase: The generation agent sequentially produces notes along with the sequen-
tially received human input. Reward Phase: The reward agent computes a reward for each generated note. It consists of four
neural network based submodules and a rule-based reward, which gives a negative punishment if the same pitch is repeated for
many times. Each of the submodules is a neural network, which receives the yellow masked area as the input context and out-
puts the probability (i.e., reward) of the notes in the dashed blue rectangle region. (a) Joint modeling with pre-context. (b) Joint
modeling with both pre- and post- context. (c) Horizontal temporal consistency modeling. (d) Vertical harmonization modeling.

man part and the machine part, respectively.

The generation model learns the probability distribution
p(mt|st): Given the state at each time-step, it outputs the
probability of the next token of the machine part. In our
model, we use a sliding window over the previous notes of
both parts to represent the state, ignoring the notes far be-
fore. Therefore, without ambiguity, the input st of our model
only contains the clipped pre-tokens, from time-step t − L
to t − 1, if the sliding window size is L.

RL Framework
We use the reinforcement learning algorithm, actor-critic
with generalized advantage estimator (GAE) (Schulman et
al. 2016), to train the generation agent. The optimization
objective of reinforcement learning is different from that of
MLE models. For an MLE model, it trains to learn p(i|st)
and maximize the likelihood of the whole dataset; Dur-
ing generation, it greedily selects the token with the high-
est probability as the next token, mt = arg maxi p(i|st).
With the MLE training criterion and generation policy, their
model only focuses on the token-level predictions, ignoring
the long-term global consistency and harmonization of the
whole music. Reinforcement learning learns a policy that
maximizes the expected long-term total reward.

In contrast, reinforcement learning (RL) optimizes the ex-
pected long-term discounted reward EπR. The discounted
reward at time-step t is Rt = (cid:80)T
i=0 γiri+t, where the re-
ward rt reﬂects the quality of the generated note mt, and γ is
the discount factor. We will explain in detail how we obtain a
comprehensive reward rt in the following section. Note that
when γ = 0, rt = p(mt|m0:t−1, h0:t−1), the RL objective
only considers the expected reward of a single step, which is
the same as that of MLE models. For γ > 0, the model takes
the long-term future reward into account.

In actor-critic with generalized advantage estimator, it
learns an action policy πθa (at|st) and a value function
Vθv (st): the agent gives an action at (a note token) based
on the current state st (tokens of the previous time-steps).

Here, at is equivalent to the mt in MLE model, and π(·|·) to
p(·|·) as well.

The discounted policy gradient is

(cid:35)

(cid:35)

(cid:34) T

(cid:88)

t=0
(cid:34) T

(cid:88)

t=0

gγ ≈ E

∇θa log πθa (at|st) ˆAGAE(γ,λ)

t

(1)

= E

∇θa log πθ(at|st)

(γλ)lδV

t+l

,

T
(cid:88)

l=0

t = rt + γV (st+1) − V (st) is the temporal differ-

where δV
ence (TD) residual of the value function V .
The gradient for the value function is

(cid:34) T

(cid:88)

E

t=0

∇θv ((cid:107)Vθv (st) − Rt(cid:107)2)

.

(2)

(cid:35)

The framework of RL-Duet is shown in Fig 3. The train-
ing process of RL-Duet iterates between two phases: gener-
ation phase and reward phase. In the generation phase, the
generation agent sequentially produces the counterpart to-
ken at, based on the state st, generated by both human and
machine. After the improvisation of the whole music, the
reward agent computes the reward for the action at at each
time-step. Then these rewards are backtracked to calculate
Rt and are used to update the generation agent πθa (at|st)
and the reward function Vθv (st) (not shown in Fig 3).

Reward Model
Previous RL based models (Jaques et al. 2017) rely on music
composition rules and heuristics hand-crafted by human spe-
cialists. Our algorithm replaces these rules with a set of deep
neural networks (reward models) that require no domain
knowledge, leading to more creative and diverse melodies.

A well-functioning reward model is key to the success
of our algorithm. It is natural to use a single compact
reward model, which models p(mt|st), to score the mu-
sic sequences. However, deep neural networks are easily

fooled (Nguyen, Yosinski, and Clune 2015). For a single
model, there may exist “undesired regions” where the re-
ward model assigns unreasonably high scores to unpleas-
ant or boring music excerpts. We call such kind of music
sequences the fooling instances. From the perspective of
improving the model robustness, (Abbasi and Gagn´e 2017)
claim that an ensemble of several specialists will have higher
entropy (disagreement) over the scoring of fooling instances,
thus is more likely to identify and reject the undesired fool-
ing instances. Motivated by (Abbasi and Gagn´e 2017), we
design an ensemble of the reward model specialists to avoid
these “undesired regions”.

Our reward agent considers both the horizontal tempo-
ral consistency (Briot, Hadjeres, and Pachet 2017) over
the machine-generated part, and the vertical harmony re-
lations (Briot, Hadjeres, and Pachet 2017) between the hu-
man and machine parts. There are four reward models in
RL-Duet, which are shown in Fig 3. Considering the rela-
tive positions of the model’s input and output, they could be
classiﬁed into three types.

Joint Modeling: One way to construct the reward model
is to directly learn p(mt|st) from the dataset. This reward
model contains the same input and output of the generation
model, as shown in Fig. 3(a). However, this model only tries
to capture the relation between the t-th token and the pre-
context. We improve it by jointly considering both the pre-
and post- contexts, p(mt:t+∆| ˜st), as shown in Fig. 3(b). We
choose to model the probability of a range of notes, from
mt to mt+∆, for better structured learning. Related to st,
the pre-contextual notes in the human and machine parts, ˜st
is the augmented pre- and post- contextual notes. Similarly,
we use a sliding window to preserve only the local music
excerpts around the tokens mt:t+∆.

In order to explicitly model the horizontal and vertical
view, we propose to disentangle the joint modeling reward
model into two parts.

Horizontal View: Another reward model learns to cap-
ture the intra-part temporal consistency, incorporating both
the pre- and post- contexts of only the machine part. As
shown in Fig. 3(c), the model is trained with the unsuper-
vised self-prediction Cloze task (Taylor 1953; Devlin et al.
2019), aiming to model the joint probability of the central
masked tokens, p(mt:t+∆|˜sm
t ), given its pre- and post- con-
textual notes. Similar to the ˜st in Joint Modeling reward
models, ˜sm
is the augmented pre- and post- contextual notes
t
of the machine part. Moreover, this disentanglement enables
the utilization of massive monophonic music sequences to
aid the training of this reward model.

t ). Likewise, ˜sh

Vertical View: Fig. 3(d) shows the last reward model,
which is designed to capture the inter-part harmony,
p(mt:t+∆|˜sh
t is the augmented pre- and post-
contextual notes of the human part. This reward model ig-
nores the pre- and post context of the machine, only model-
ing the harmonization of the central excerpt of the machine
part and the whole human part.

These four types of reward models serve a comprehen-
sive purpose of achieving inter-part and intra-part coherence.
Rewards given by all the reward models are averaged to
give a model-based reward. In practice, a user could tune

their weights based on his/her preference and experience.
Besides, we augment the model-based reward with a simple
rule-based reward, a negative punishment, -1, when a note is
excessively repeated, adapted from SequenceTutor (Jaques
et al. 2017). The total reward at each time-step is the sum of
the model-based reward and the rule-based reward.

Experiments

Experimental Setup
Datasets The model is trained on the Bach Chorale dataset
in Music21 (Cuthbert and Ariza 2010). We use chorales
with four monophonic parts, in the SATB format (Soprano,
Alto, Tenor, and Bass) as the training and validation datasets,
with 327 and 37 chorales respectively. When training reward
models, the human part and the machine part are randomly
chosen from four parts of one chorale to form a duet. We
perform data augmentation by transposing the chorales such
that the transposed ones do not exceed the highest and the
lowest pitches of the original dataset. The MIDI pitches lie
in the range from MIDI number 36 to 81.

When RL-Duet is eventually deployed in a real-time
human-machine interactive improvisation system, and the
human improvisation would also be inﬂuenced by the ma-
chine generation. As this paper focuses on the algorithm de-
sign and such interactive system is not in place yet, for our
subjective evaluation, the human part is drawn from the re-
maining 37 Bach chorales not used in training and valida-
tion. We form 460 duet pairs from these chorales as our test
data. For each duet, the human part is ﬁxed, so is the ﬁrst
two measures of the machine part. This provides an initial-
ization for the generation models to generate the rest of the
machine part to accompany the human part. Each genera-
tion model greedily generates one accompaniment part for
the human part. The objective and subjective evaluations of
each generation model are based on the 460 generated duets.

Model Speciﬁcation Inspired by (Bahdanau, Cho, and
Bengio 2015; Rush, Chopra, and Weston 2015; Yin et al.
2016), we design an attention-guided recurrent convolution
neural network, which enhances CNN with a temporal at-
tention mechanism. This attention mechanism enables the
model to capture some aspects of musical structure such as
repeated motifs. The model architecture for the generation
and the reward model (a) is shown in Fig. 4. It uses embed-
ding layers to encode the note tokens and the beat tokens,
followed by bi-directional GRU layers to capture long-term
dependencies. Afterwards, a Temporal Context Summarizer
uses the pooling mechanism and the attention mechanism to
adaptively aggregate the temporal information. At last, both
the summarized context information and the feature of the
current beat are concatenated to output a probability distri-
bution. The architecture of the reward model (b)(c)(d) are
almost the same as Fig. 4, except that their inputs to the
Temporal Context Summarizer are different (shown in Fig
3), and their ﬁnal fc layer outputs the probability of a range
of notes, from mt to mt+∆, ∆ = 16.

As introduced above, the reward agent of RL-Duet con-
tains four types of reward models. We use six pre-trained
reward models, three of which are of the type (a), trained

Dataset
MLE
RL-Rules
RL-Duet

PC/bar
3.25
-0.90
+0.96
+0.12

PI
4.57
+3.01
-1.13
-0.48

IOI
3.84
+0.82
-0.46
-0.09

PCH
-
0.0067
0.0079
0.0057

NLH
-
0.039
0.043
0.042

Table 1: Objective evaluation results. We report the average
PC/bar, PI, and IOI of pieces in the test dataset. For the three
generation models, we report their differences from the test
dataset. We also report the earth moving distance on PCH
and NLH between the generated music and the test dataset.
The smaller difference or distance suggests better style imi-
tation of the dataset.

rule-based rewards, and the weights between them are ﬁne-
tuned. In our implementation, we use the pre-trained reward
model (a) to provide the model-based reward, with some of
the rule-based rewards from SequenceTutor which is appli-
cable in our setting and the same weights. We do not use
all of the rule-based rewards from it, since we have differ-
ent pitch and beat representation from them and we do not
transpose all the music to the same C-major key. Note that,
the rules they designed only care about the harmony of the
single machine generated part, and contain no information
on the coherence between the human part and machine part.
We follow (Yang and Lerch 2018) to perform some ob-
jective evaluation. Speciﬁcally, we use pitch count per bar
(PC/bar), average pitch interval (PI) and average inter-onset-
interval (IOI). Table 1 shows that these metrics of RL-Duet
are closer to those of the test set than the baselines. We also
calculate the histogram-like features, namely pitch class his-
togram (PCH) and note length histogram (NLH). We use the
earth moving distance to measure the difference between the
distributions of the histogram-like features from the genera-
tive models and those of the test set. As shown in Table 1,
our proposed RL-Duet generates pieces that are more simi-
lar to the test set than both baselines in terms of PCH, and is
competitive for NLH.

Considering that the ﬁrst two measures of the machine
part were always initialized with the ground-truth score, we
observed that there exists some degeneration of the gener-
ated part by all the three algorithms in comparison. There-
fore, we calculate how the objective metrics evolve over time
through the generation of each piece using a sliding window
in Figure 5. The horizontal axis shows the measure indices.
We take a one-sided 4-measure sliding window, i.e., the ob-
jective metrics corresponding to measure index 1 is calcu-
lated on Measures 1-4. Most of the duets in the test dataset
are shorter than 15 measures, while only 20% of duets in
the test dataset are longer than 24 measures. Therefore, the
evolution of the metrics is calculated from measure 1 to mea-
sure 20. RL-Duet is the most robust and achieves the clos-
est evaluation metrics to the ground-truth. Interestingly, we
also ﬁnd that MLE captures the trend of these metrics of
the dataset. However, due to the exposure bias (described in
Motivation Section), generation problems tend to accumu-
late over time, leading to signiﬁcant drifts and divergence of
the objective metrics. It is clear from Figure 5b that, as the

Figure 4: Model architecture of the generation model. The
concatenation of the output of the human branch, machine
branch and beat branch are fed into the Temporal Context
Summarizer. Reward model (a) uses the same architecture,
while reward models (b)(c)(d) use a similar architecture (see
main text for details).

for 20 epochs with learning rate of 0.005, 0.01 and 0.05, re-
spectively. The remaining three reward models are of type
(b)(c)(d), respectively, trained with learning rate 0.05 for
20 epochs. We choose to add more reward models of type
(a), since it shares the same architecture with the genera-
tion model (see Fig. 3(a)). The generation model and reward
model (a) use the “multi-hold” pitch representation as it al-
leviates the extreme imbalance of the amount of the hold
symbol and pitch numbers in the “single-hold” representa-
tion. However, we still use the “single-hold” representation
for reward models (b)(c)(d), since our empirical results show
that the “multi-hold” representation will generate uncom-
mon rhythms, such as a dotted eighth note followed by a
sixteenth note in the generated music. We suggest that it may
be because that in these cases, the model predicts a range of
notes, from mt to mt+∆ at the same time, making the evo-
lution of the temporal patterns difﬁcult to model.

Training Process There are two training stages in RL-
Duet. First, the reward models are trained with maximum
likelihood estimation. And then, the generation model is
trained with the reinforcement learning objective for 100K
training duets, while all the reward models are only used
for inference. The discount factor γ = 0.5. The λ for the
generalized advantage estimator is 1. The pretrained joint-
modeling reward model (a) with learning rate 0.01 is used to
initialize the weights of the generation model.

Objective Evaluation
In this section, we compared the performance of RL-Duet
with a baseline MLE model and a rule-based reinforcement
leaning model. The rule-based reinforcement leaning model
is our implementation of SequenceTutor (Jaques et al. 2017).
SequenceTutor uses a model-based reward along with many

(a) Pitch Count per Bar

(b) Pitch Interval

(c) Inter-Onset-Interval

Figure 5: Average objective metrics (vertical) versus measure indices (horizontal) across test pieces, showing how the generation
quality changes over time within a music piece.

generation goes on, the MLE model is prone to repeating
the same pitch, which accords with the MLE RNN’s “bad
behaviors” in SequenceTutor (Jaques et al. 2017).

Subjective User Study
Since music appreciation is very subjective and different
people may have different opinions on the quality of mu-
sic, we conduct a wide-range subjective user study. A simple
survey on their music background is conducted beforehand:

1. Have you learned to play a music instrument (no less than
5 hours a week) for more than 5 years in total in the past?
-Yes -No

2. How much time do you spend in listening to classical mu-

sic each week?
- Less than 1 hour - Between 1 and 3 hours - Between 3
and 5 hours - More than 5 hours

To perform the subjective evaluation, we compared the
performance of RL-Duet with the baseline MLE model. The
RL-Rules model is excluded since we observed some appar-
ently bad generation of RL-Rules. Fig. 6 demonstrates im-
mediate repetition, but with bizarre rhythmic structure, and
large pitch leaps in the generated music of RL-Rules. The
reason why rule-based model fails may be the inconsistency
of the rule-based rewards. These rewards and the weights
balancing them are carefully designed to adapt to the set-
tings in SequenceTutor. However, when directly transplant-
ing them in our setting, it easily fails.

We performed paired comparisons: Each subject was pre-
sented with a pair of two short duet excerpts (8 seconds) with
the same human part. These excerpts were randomly trun-
cated from generated duets, which were randomly chosen
for each subject. After listening to each pair of duet samples,
the participant was asked to choose which was preferred.
Each pair of duet samples was evaluated by 20 subjects. 125
subjects took the test and 2000 valid votes were collected. 19
of them (with 233 votes) had learned a musical instrument
before, and 28 of them (with 617 votes) spent more than 1
hour listening to classical music per week.

Fig 7 shows the paired comparison results. When the level
of classical music familiarity is higher, the distinction be-
tween the RL-Duet and MLE model is more obvious. (An
exception is the result of those who spent more than 5 hours

Figure 6: Examples showing typical problems of the rule-
based model. The upper part is the human part, while the
lower part is generated by RL-Rules. Problems: (a) Immedi-
ate repetition and misalignment between the repetitions and
the rhythmic structure. (b) Large pitch leap.

per week listening to classical music, where too few votes
are available and the results may be unreliable.) In total,
around 58.42% subjects prefer duets generated by RL-Duet
to those generated by the MLE model. Considering that the
paired two duets share the same human part, it can be hard to
distinguish the two duets if the subject has little music back-
ground. For subjects with more performance and listening
experiences, the preference of RL-Duet over MLE is more
pronounced.

Interactive Generation
In previous sections, the roles of human and machine part
are assumed to be ﬁxed: the human part is always produced
by the human, and the machine part is generated by RL-
Duet. In the following, we extend RL-Duet to a more inter-
active setting, where the role of the human and the machine
can be switched. Fig. 8 shows an example. Starting in Mea-
sure 6, the roles of the human and the machine are switched,
and then in Measure 10, the roles are switched back. The hu-
man (machine) plays Part 1 (2) before the switch but plays
Part 2 (1) after the switch. Even though the human part in
this example is also a ﬁxed part from the test data, which
is invariant during generation, the machine part shows in-
teresting responses to previous generated notes, by both the
human and the machine. Fig. 8 shows three motifs in the ma-
chine part, in which the ﬁrst relates to the human-generated
part, while the other two relate to the machine part.

Conclusion
This paper presents RL-Duet, the ﬁrst reinforcement learn-
ing model for online music accompaniment generation using

(a) Musical Instrument Skills

(b) Classical Music Listening

(c) Summary

Figure 7: Subjective evaluation results. The vertical axis is the number of votes. Blue bars and yellow bars mean the votes
that prefer RL-Duet and the MLE model respectively. The subjects are classiﬁed into several groups according to their musical
background: (a) with or without musical instrument skills, and (b) time spent in listening to the classical music per week. (c)
The summary of the subjective evaluation. In total, 58.42% votes prefer RL-Duet.

cial Intelligence (BAAI), and National Science Foundation
grant No. 1846184. We also thank Yujia Yan for the valuable
discussions on music composition.

References
[Abbasi and Gagn´e 2017] Abbasi, M., and Gagn´e, C. 2017.
Robustness to adversarial examples through an ensemble of
specialists. ICLR workshop.
[Bahdanau et al. 2017] Bahdanau, D.; Brakel, P.; Xu, K.;
Goyal, A.; Lowe, R.; Pineau, J.; Courville, A.; and Bengio,
Y. 2017. An actor-critic algorithm for sequence prediction.
In ICLR.
[Bahdanau, Cho, and Bengio 2015] Bahdanau, D.; Cho, K.;
and Bengio, Y. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR.
[Benetatos and Duan 2019] Benetatos, C., and Duan, Z.
2019. BachDuet: A human-machine duet improvisation sys-
tem. In ISMIR Late Breaking & Demo.
[Bharucha and Todd 1989] Bharucha, J. J., and Todd, P. M.
1989. Modeling the perception of tonal structure with neural
nets. Computer Music Journal 13(4):44–53.
[Biles and others 1994] Biles, J. A., et al. 1994. Genjam:
In ICMC,
A genetic algorithm for generating jazz solos.
volume 94, 131–137.
[Briot, Hadjeres, and Pachet 2017] Briot, J.-P.; Hadjeres, G.;
and Pachet, F. 2017. Deep learning techniques for music
generation-a survey. arXiv preprint arXiv:1709.01620.
[Chu, Urtasun, and Fidler 2017] Chu, H.; Urtasun, R.; and
Fidler, S. 2017. Song from pi: A musically plausible net-
work for pop music generation. In ICLR.
[Cuthbert and Ariza 2010] Cuthbert, M. S., and Ariza, C.
2010. music21: A toolkit for computer-aided musicology
and symbolic music data. In ISMIR.
[Dannenberg 1984] Dannenberg, R. B. 1984. An on-line al-
gorithm for real-time accompaniment. In ICMC, volume 84,
193–198.

Figure 8: Interactive Generation. The ﬁrst two measures of
the machine part are given as the seed. The machine part
generated by RL-Duet is covered with blue masks. Three
motifs are highlighted with yellow arrows.

an ensemble of reward models, in order to support real-time
interactive music generation in a human-machine duet setup.
Apart from the MLE model, RL-Duet generates music with
better global coherence of the whole generated sequence.
A comprehensive reward model considers the compatibil-
ity of the machine-generated notes with both the intra-part
and inter-part contexts from the horizontal and the vertical
views. Objective evaluation shows that RL-Duet has better
style imitation of the dataset than an MLE baseline and a
rule-based baseline. Subjective evaluation shows a higher
preference on pieces generated by RL-Duet than those gen-
erated by the MLE baseline. For future work, we plan to
integrate this model in an interactive human-machine duet
improvisation system and investigate how the human musi-
cian interacts with the machine partner.

Acknowledgments
This work is (jointly or partly) funded by the NSFC (Grant
No. 61876095 and 61751308), Beijing Academy of Artiﬁ-

L.;

[Devlin et al. 2019] Devlin, J.; Chang, M.-W.; Lee, K.; and
Toutanova, K. 2019. Bert: Pre-training of deep bidirectional
transformers for language understanding. In ACL.
[Dong et al. 2018] Dong, H.-W.; Hsiao, W.-Y.; Yang, L.-C.;
and Yang, Y.-H. 2018. Musegan: Multi-track sequential gen-
erative adversarial networks for symbolic music generation
and accompaniment. In AAAI.
[Eck and Schmidhuber 2002] Eck, D., and Schmidhuber, J.
2002. A ﬁrst look at music composition using lstm recurrent
neural networks. Istituto Dalle Molle Di Studi Sull Intelli-
genza Artiﬁciale 103:48.
[Engel et al. 2019] Engel, J.; Agrawal, K. K.; Chen, S.; Gul-
rajani, I.; Donahue, C.; and Roberts, A. 2019. GANSynth:
Adversarial neural audio synthesis. In ICLR.
[Green 1989] Green, D. M. 1989. A practical approach to
eighteenth-century counterpoint.
[Guimaraes et al. 2017] Guimaraes, G.
Sanchez-
Lengeling, B.; Outeiral, C.; Farias, P. L. C.; and Aspuru-
Guzik, A. 2017. Objective-reinforced generative adversarial
networks (organ) for sequence generation models. arXiv
preprint arXiv:1705.10843.
[Hadjeres, Pachet, and Nielsen 2017] Hadjeres, G.; Pachet,
F.; and Nielsen, F. 2017. Deepbach: a steerable model for
bach chorales generation. In ICML.
[Huang et al. 2017] Huang, C.-Z. A.; Cooijmans, T.;
Roberts, A.; Courville, A.; and Eck, D. 2017. Counterpoint
by convolution. In ISMIR.
[Huang et al. 2019a] Huang, C.-Z. A.; Hawthorne, C.;
Roberts, A.; Dinculescu, M.; Wexler, J.; Hong, L.; and
Howcroft, J. 2019a. The bach doodle: Approachable music
composition with machine learning at scale. arXiv preprint
arXiv:1907.06637.
[Huang et al. 2019b] Huang, C.-Z. A.; Vaswani, A.; Uszko-
reit, J.; Simon, I.; Hawthorne, C.; Shazeer, N.; Dai, A. M.;
Hoffman, M. D.; Dinculescu, M.; and Eck, D. 2019b. Music
transformer: Generating music with long-term structure. In
ICLR.
[Hutchings and McCormack 2017] Hutchings, P., and Mc-
Cormack, J. 2017. Using autonomous agents to improvise
music compositions in real-time. In EvoMUSART. Springer.
[Jaques et al. 2017] Jaques, N.; Gu, S.; Bahdanau, D.;
Hern´andez-Lobato, J. M.; Turner, R. E.; and Eck, D. 2017.
Sequence tutor: Conservative ﬁne-tuning of sequence gener-
ation models with kl-control. In ICML.
[Mogren 2016] Mogren, O. 2016. C-rnn-gan: Continuous
recurrent neural networks with adversarial training. NIPS
Workshop.
[Mozer 1994] Mozer, M. C. 1994. Neural network music
composition by prediction: Exploring the beneﬁts of psy-
choacoustic constraints and multi-scale processing. Connec-
tion Science 6(2-3):247–280.
[Nguyen, Yosinski, and Clune 2015] Nguyen, A.; Yosinski,
J.; and Clune, J. 2015. Deep neural networks are easily
fooled: High conﬁdence predictions for unrecognizable im-
ages. In CVPR, 427–436.

[Oord et al. 2016] Oord, A. v. d.; Dieleman, S.; Zen, H.; Si-
monyan, K.; Vinyals, O.; Graves, A.; Kalchbrenner, N.; Se-
nior, A.; and Kavukcuoglu, K. 2016. Wavenet: A generative
model for raw audio. arXiv preprint arXiv:1609.03499.
[Papadopoulos and Wiggins 1998] Papadopoulos, G., and
Wiggins, G. 1998. A genetic algorithm for the generation of
jazz melodies. Proceedings of STEP 98.
[Ranzato et al. 2016] Ranzato, M.; Chopra, S.; Auli, M.; and
Zaremba, W. 2016. Sequence level training with recurrent
neural networks. In ICLR.
[Raphael 2010] Raphael, C. 2010. Music plus one and ma-
chine learning. In ICML.
[Roberts et al. 2016] Roberts, A.; Engel, J.; Hawthorne, C.;
Simon, I.; Waite, E.; Oore, S.; Jaques, N.; Resnick, C.; and
Eck, D. 2016. Interactive musical improvisation with ma-
genta. NIPS.
[Rush, Chopra, and Weston 2015] Rush, A. M.; Chopra, S.;
and Weston, J. 2015. A neural attention model for abstrac-
tive sentence summarization. In EMNLP.
[Schulman et al. 2016] Schulman, J.; Moritz, P.; Levine, S.;
Jordan, M.; and Abbeel, P. 2016. High-dimensional con-
tinuous control using generalized advantage estimation. In
ICLR.
[Tatar and Pasquier 2019] Tatar, K., and Pasquier, P. 2019.
Musical agents: A typology and state of the art towards
Journal of New Music Research
musical metacreation.
48(1):56–105.
1953.
[Taylor 1953] Taylor, W. L.
new tool for measuring readability.
30(4):415–433.
[Theis, van den Oord, and Bethge 2016] Theis, L.; van den
Oord, A.; and Bethge, M. 2016. A note on the evaluation of
generative models. In ICLR.
[Thom 2000] Thom, B. 2000. Bob: an interactive improvi-
sational music companion. In ICAA.
[Trieu and Keller 2018] Trieu, N., and Keller, R. 2018. Jaz-
zgan: Improvising with generative adversarial networks.
MUME workshop.
[Waite et al. 2016] Waite, E.; Eck, D.; Roberts, A.; and Abo-
laﬁa, D. 2016. Project magenta: generating long-term struc-
ture in songs and stories.
[Williams 1992] Williams, R. J. 1992. Simple statistical
gradient-following algorithms for connectionist reinforce-
ment learning. Machine learning 8(3-4):229–256.
[Yan et al. 2018] Yan, Y.; Lustig, E.; VanderStel, J.; and
Duan, Z. 2018. Part-invariant model for music generation
and harmonization. In ISMIR.
[Yang and Lerch 2018] Yang, L.-C., and Lerch, A. 2018. On
the evaluation of generative models in music. Neural Com-
puting and Applications 1–12.
[Yang, Chou, and Yang 2017] Yang, L.-C.; Chou, S.-Y.; and
Yang, Y.-H. 2017. Midinet: A convolutional generative ad-
versarial network for symbolic-domain music generation. In
ISMIR.

cloze procedure: A
Journalism Bulletin

[Yin et al. 2016] Yin, W.; Sch¨utze, H.; Xiang, B.; and Zhou,
B. 2016. Abcnn: Attention-based convolutional neural net-
work for modeling sentence pairs. In ACL.
[Yu et al. 2017] Yu, L.; Zhang, W.; Wang, J.; and Yu, Y.
2017. Seqgan: Sequence generative adversarial nets with
policy gradient. In AAAI.
[Zhu et al. 2018] Zhu, H.; Liu, Q.; Yuan, N. J.; Qin, C.; Li,
J.; Zhang, K.; Zhou, G.; Wei, F.; Xu, Y.; and Chen, E. 2018.
Xiaoice band: A melody and arrangement generation frame-
work for pop music. In ACM SIGKDD, 2837–2846. ACM.

