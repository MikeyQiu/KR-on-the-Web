Coarse-to-Fine Lifted MAP Inference in Computer Vision

Haroun Habeeb and Ankit Anand and Mausam and Parag Singla
Indian Institute of Technology Delhi
haroun7@gmail.com and {ankit.anand,mausam,parags}@cse.iitd.ac.in

7
1
0
2
 
l
u
J
 
2
2
 
 
]

V
C
.
s
c
[
 
 
1
v
5
6
1
7
0
.
7
0
7
1
:
v
i
X
r
a

Abstract

There is a vast body of theoretical research on
lifted inference in probabilistic graphical models
(PGMs). However, few demonstrations exist where
lifting is applied in conjunction with top of the line
applied algorithms. We pursue the applicability
of lifted inference for computer vision (CV), with
the insight that a globally optimal (MAP) labeling
will likely have the same label for two symmetric
pixels. The success of our approach lies in efﬁ-
ciently handling a distinct unary potential on ev-
ery node (pixel), typical of CV applications. This
allows us to lift the large class of algorithms that
model a CV problem via PGM inference. We pro-
pose a generic template for coarse-to-ﬁne (C2F) in-
ference in CV, which progressively reﬁnes an ini-
tial coarsely lifted PGM for varying quality-time
trade-offs. We demonstrate the performance of
C2F inference by developing lifted versions of two
near state-of-the-art CV algorithms for stereo vi-
sion and interactive image segmentation. We ﬁnd
that, against ﬂat algorithms, the lifted versions have
a much superior anytime performance, without any
loss in ﬁnal solution quality.

1 Introduction
Lifted inference in probabilistic graphical models (PGMs)
refers to the set of the techniques that carry out inference
over groups of random variables (or states) that behave simi-
larly [Jha et al., 2010; Kimmig et al., 2015]. A vast body of
theoretical work develops a variety of lifted inference tech-
niques, both exact (e.g., [Poole, 2003; Braz et al., 2005;
Singla and Domingos, 2008; Kersting, 2012]) and approxi-
mate (e.g., [Singla et al., 2014; Van den Broeck and Niepert,
2015]). Most of these works develop technical ideas appli-
cable to generic subclasses of PGMs, and the accompanying
experiments are aimed at providing ﬁrst proofs of concepts.
However, little work exists on transferring these ideas to the
top domain-speciﬁc algorithms for real-world applications.

Algorithms for NLP, computational biology, and computer
vision (CV) problems make heavy use of PGM machinery
(e.g., [Blei et al., 2003; Friedman, 2004; Szeliski et al.,
2008]). But, they also include signiﬁcant problem-speciﬁc

insights to get high performance. Barring a handful of excep-
tions [Jernite et al., 2015; Nath and Domingos, 2016], lifted
inference hasn’t been applied directly to such algorithms.

We study the potential value of lifting to CV problems such
as image denoising, stereo vision, and image segmentation.
Most CV problems are structured output prediction tasks, typ-
ically assigning a label to each pixel. A large class of solu-
tions are PGM-based: they deﬁne a Markov Random Field
(MRF) that has each pixel as a node, with unary potential that
depends on pixel value, and pairwise neighborhood potentials
that favor similar labels to neighboring pixels.

We see three main challenges in applying existing lifted
inference literature to these problems. First, most existing al-
gorithms focus on computing marginals [Singla and Domin-
gos, 2008; Kersting et al., 2009; Gogate and Domingos, 2011;
Niepert, 2012; Anand et al., 2016; Anand et al., 2017] instead
of MAP inference. Second, among the algorithms perform-
ing lifted MAP [Noessner et al., 2013; Mladenov et al., 2014;
Sarkhel et al., 2014; Mittal et al., 2014], many of the algo-
rithms focus on exact lifting. This breaks the kind of symme-
tries we need to compute since different pixels may not have
exact same neighborhood. Third, the few algorithms that per-
form approximate lifting for MAP, e.g. [Sarkhel et al., 2015],
can’t handle a distinct unary potential on every node. This is
essential for our application since image pixels take ordinal
values in three channels.

In response, we develop an approximate lifted MAP infer-
ence algorithm which can effectively handle unary potentials.
We initialize our algorithm by merging together pixels hav-
ing the same order of top-k labels based on the unary po-
tential values. We then adapt an existing symmetry ﬁnding
algorithm [Kersting et al., 2009] to discover groupings which
also have similar neighborhoods. We refer to our groupings
as lifted pixels. We impose the constraint that all pixels in a
lifted pixel must be assigned the same label. Our approximate
lifting reduces the model size drastically leading to signiﬁ-
cant time savings. Unfortunately, such approximate lifting
could adversely impact solution quality. However, we vary
the degree of approximation in symmetry ﬁnding to output a
sequence of coarse-to-ﬁne models with varying quality-time
trade-offs. By switching between such models, we develop a
coarse-to-ﬁne (C2F) inference procedure applicable to many
CV problems.

We formalize these ideas in a novel template for using

lifted inference in CV. We test C2F lifted inference on two
problems: stereo matching and image segmentation. We start
with one of the best MRF-based solvers each for both prob-
lems – neither of these are vanilla MRF solvers. Mozerov
& Weijer [2015] use a two-way energy minimization to ef-
fectively handle occluded regions in stereo matching. Co-
operative cuts [Kohli et al., 2013] for image segmentation use
concave functions over a predeﬁned set of pixel pairs to cor-
rectly segment images with sharp edges. We implement C2F
inference on top of both these algorithms and ﬁnd that C2F
versions have a strong anytime behavior – given any amount
of inference time, they output a much higher quality (and are
never worse) than their unlifted counterparts, and don’t suffer
any loss in the ﬁnal quality. Overall, our contributions are:

1. We present an approximate lifted MAP algorithm that
can efﬁciently handle a large number of distinct unary
potentials.

2. We develop a novel template for applying lifted infer-
ence in structured prediction tasks in CV. We provide
methods that output progressively ﬁner approx. symme-
tries, leading to a C2F lifted inference procedure.

3. We implement C2F inference over a near state-of-the-
art stereo matching algorithm, and one of the best MRF-
based image segmentation algorithms. We release our
implementation for wider use by the community.1

4. We ﬁnd that C2F has a much superior anytime behav-
ior. For stereo matching it achieves 60% better quality
on average in time-constrained settings. For image seg-
mentation C2F reaches convergence in 33% less time.

2 Background
2.1 Computer Vision Problems as MRFs
Most computer vision problems are structured output predic-
tion problems and their PGM-based solutions often follow
similar formulations. They cast the tasks into the problem
of ﬁnding the lowest energy assignment over grid-structured
MRFs (denoted by G = (X , γ)). The random variables in
these MRFs are the set of pixels X in the input image. Given
a set of labels L : {1, 2, . . . , |L|}, the task of structured output
prediction is to label each pixel X with a label from L. The
MRFs have two kinds of potentials (γ) – unary and higher-
order. Unary potentials are deﬁned over each individual pixel,
and usually incorporate pixel intensity, color, and other pixel
features. Higher order potentials operate over cliques (pairs
or more) of neighboring pixels and typically express some
form of spatial homophily – “neighboring pixels are more
likely to have similar labels.” While the general PGM struc-
ture of various tasks are similar, the speciﬁc potential tables
and label spaces are task-dependent.

The goal is to ﬁnd the MAP assignment over this MRF,
which is equivalent to energy minimization (by deﬁning en-
ergy as negative log of potentials). We denote the negative
log of unary potentials by φ, and that of higher-order poten-
tials by ψ.2 Thus, energy of a complete assignment x ∈ L|X |

1https://github.com/dair-iitd/c2ﬁ4cv/
2In the interest of readability, we say ‘potential’ to mean ‘nega-

tive log of potential’ in the rest of the paper.

can be deﬁned as:

E(x) =

φ(xi) +

ψj(ˆxj)

(1)

(cid:88)

i∈1..|X |

(cid:88)

j

Here ˆxj denotes the assignment x restricted to the set of
variables in the potential ψj. And the output of the algorithm
is the assignment xMAP:

xMAP = arg min
x∈L|X |

E(x)

(2)

The problem is in general intractable. Efﬁcient approxi-
mations exploit special characteristics of potentials like sub-
modularity [Jegelka and Bilmes, 2011], or use variants of
graph cut or loopy belief propagation [Boykov et al., 2001;
Freeman et al., 2000].

2.2 Symmetries in Graphical Models
Lifting an algorithm often requires computing a set of sym-
metries that can be exploited by that algorithm. For PGMs,
two popular methods for symmetry computation are color
passing for computing symmetries of variables [Kersting et
al., 2009], and graph isomorphism for symmetries of states
[Niepert, 2012; Bui et al., 2013]. Since our work is based on
color passing, we explain it in more detail.

Color passing for an MRF operates over a colored bipar-
tite graph containing nodes for all variables and potentials,
and each node is assigned a color. The graph is initialized
as follows: all variables nodes get a common color; all po-
tential nodes with exactly same potential tables are assigned
a unique color. Now, in an iterative color passing scheme,
in each iteration, each variable node passes its color to all
neighboring potential nodes. The potential nodes store in-
coming color signatures in a vector, append their own color
to it, and send the vector back to variable nodes. The variable
nodes stack these incoming vectors. New colors are assigned
to each node based on the set of incoming messages such that
two nodes with same messages are assigned the same unique
color. This process is repeated until convergence, i.e., no fur-
ther change in colors.

A coloring of the bipartite graph deﬁnes a partition of vari-
able nodes such that all nodes of the same color form a par-
tition element. Each iteration of color passing creates suc-
cessively ﬁner partitions, since two variable nodes, once as-
signed different colors, can never get the same color.

3 Lifted Computer Vision Framework
In this section, we will describe our generic template which
can be used to lift a large class of vision applications includ-
ing those in stereo, segmentation etc. Our template can be
seen as transforming the original problem space to a reduced
problem space over which the original inference algorithm
can now be applied much more efﬁciently. Speciﬁcally, our
description in this section is entirely algorithm independent.
We will focus on MAP inference which is the inference
task of choice for most vision applications (refer Section 2).
The key insight in our formulation is based on the realiza-
tion that pixels which are involved in the same (or similar)

kinds of unary and higher order potentials, and have the same
(or similar) neighborhoods, are likely to have the same MAP
value. Therefore, if somehow we could discover such sets
of pixels a priori, we could explicitly enforce these pixels to
have the same value while searching for the solution, substan-
tially reducing the problem size and still preserving the opti-
mal MAP assignment(s). Since in general doing this exactly
may lead to a degenerate network, we do it approximately.
Hence trading-off speed for marginal loss in solution quality.
The loss in solution quality is offset by resorting to coarse-
to-ﬁne inference where we start with a crude approximation,
and gradually make it ﬁner, to guarantee optimality at the end
while still obtaining signiﬁcant gains. We next describe the
details of our approach.

3.1 Obtaining a Reduced Problem

1 , Y P

2 , · · · , Y P

k ⊆ X , Y P
k1

Consider an energy minimization problem over a PGM G =
(X , γ). Let L = {1, 2, · · · , |L|} denote the set of labels
over which variables in the set X can vary. Let Y P =
{Y P
r } denote a partition of X into r disjoint
∩ Y P
subsets, i.e., ∀k, Y P
= ∅ when k1 (cid:54)= k2,
k2
and ∪kY P
k = X . We refer to each Y P
k as a partition element.
Correspondingly, let us deﬁne Y = {Y1, Y2, · · · , Yr} as a set
of partition variables, where there is a one to one correspon-
dence between partition elements and the partition variables
and each partition variable Yk takes values in the set L. Let
part(Xi) denote the partition element to which Xi belongs.
Let ˆXj ⊆ X denote a subset of variables. We say that a par-
k is represented in the set ˆXj if ∃Xi ∈ ˆXj
tition element Y P
s.t. part(Xj) = Yk.
Given a subset of variables ˆXj, let γj( ˆXj) be a potential de-
ﬁned over ˆXj. Let ˆxj denote an assignment to variables in
the set ˆXj. Let ˆxj.elem(i) denote the value taken by a vari-
able Xi in ˆXj. We say that an assignment ˆXj = ˆxj respects
a partition Y P if the variables in ˆXj belonging to the same
partition element have the same label in ˆxj, i.e., part(Xi) =
part(Xi(cid:48)) ⇒ ˆxj.elem(i) = ˆxj.elem(i(cid:48)), ∀Xi, Xi(cid:48) ∈ ˆXj.
Next, we introduce the notion of a reduced potential.

Deﬁnition 3.1 Let X be a set of variables and let Y P de-
note its partition. Given the potential γj( ˆXj), the reduced
potential Γj is deﬁned to be the restriction of γj( ˆXj) to those
labeling assignments of ˆXj which respect the partition Y P .
Equivalently, we can deﬁne the reduced potential Γj( ˆYj) over
the set of partition variables ˆYj which are represented in the
set ˆXj.

For example, consider a potential γ(X1, X2, X3) deﬁned
over three Boolean variables. The table for γ would have
8 entries. Consider the partition Y P = {Y P
2 } where
1 = {X1, X2} and Y P
Y P
2 = {X3}. Then, the reduced poten-
tial Γ is the restriction of γ to those rows in the table where
X1 = X2. Hence Γ has four rows in its table and equivalently
can be thought of deﬁning a potential over the 4 possible com-
binations of Y1 and Y2 variables. We are now ready to deﬁne
a reduced graphical model.

1 , Y P

Deﬁnition 3.2 Let G = (X , γ) represent a PGM. Given a
partition Y P of X , the reduced graphical model G(Y, Γ) is
the graphical model deﬁned over the set of partition variables
Y such that every potential γj ∈ γ in G is replaced by the
corresponding reduced potential Γj ∈ Γ in G.
Let E(x) and E(y) denote the energies of the states x and y
in G and G, respectively. The following theorem relates the
energies of the states in the two graphical models.

Theorem 3.1 For every assignment y of Y in G, there is a
corresponding assignment x of X such that E(y) = E(x).

The theorem can be proved by noting that each potential
Γj( ˆYj) in G was obtained by restricting the original poten-
tial γj( ˆXj) to those assignments where variables in Xj be-
longing to the same partition took the same label. Since this
correspondence is true for every potential in the reduced set,
to obtain the desired state x, for every variable Xi ∈ X we
simply assign it the label of its partition in y.
Corollary 3.1 Let xMAP and yMAP be the MAP states (i.e.
having the minimum energy) for G and G, respectively. Then,
E(yMAP) ≥ E(xMAP).
The process of reduction can be seen as curtailing the en-
tire search space to those assignments where variables in the
same partition take the same label. A reduction in the prob-
lem space will lead to computational gains but might result
in loss of solution quality, where the solution quality can be
captured by the difference between E(yMAP) and E(xMAP).
Therefore, we need to trade-off the balance between the two.
Intuitively, a good problem reduction will keep those vari-
ables in the same partition which are likely to have the same
value in the optimal assignment for the original problem.
How do we ﬁnd such variables without actually solving the
inference task? We will describe one such technique in Sec-
tion 3.3.

There is another perspective.

Instead of solving one re-
duced problem, we can instead work with a series of reduced
problems which successively get closer to the optimal solu-
tion. The initial reductions are coarser and far from optimal,
but can be solved efﬁciently to quickly reach in the region
where the solution lies. The successive iterations can then re-
ﬁne the solution iteratively getting closer to the optimal. This
leads us to the coarse-to-ﬁne inference described next.

3.2 Coarse to Fine Inference
We will deﬁne a framework for C2F (coarse-to-ﬁne) infer-
ence so that we maintain the computational advantage while
still preserving optimality. In the following, for ease of no-
tation, we will drop the superscript P in Y p to denote the
partition of X . Therefore, Y will refer to both the partition as
well as the set of partition variables. Before we describe our
algorithm, let us start with some deﬁnitions.
Deﬁnition 3.3 Let Y and Y (cid:48) be two partitions of X . We
say that Y is coarser than Y (cid:48), denoted as Y (cid:22) Y (cid:48),
if
∀y(cid:48) ∈ Y (cid:48)∃y ∈ Y such that y(cid:48) ⊆ y. We equivalently say
that Y (cid:48) is ﬁner than Y.
It is easy to see that X deﬁnes a partition of itself which is the
ﬁnest among all partitions, i.e., ∀Y such that Y is a partition

of X , Y (cid:22) X . We also refer it to as the degenerate partition.
For ease of notation, we will denote the ﬁnest partition by
Y ∗ (same as X ). We will refer to the corresponding PGM as
G∗ (same as G). Next, we state a theorem which relates two
partitions with each other.
Lemma 1 Let Y and Y (cid:48) be two partitions of X such that Y (cid:22)
Y (cid:48). Then Y (cid:48) can be seen as a partition of the set Y.

The proof of this lemma is straightforward and is omitted
due to lack of space. Consider a set Y of coarse to ﬁne
partitions given as Y 0 (cid:22) Y 1, · · · , (cid:22), Y t, (cid:22), · · · , Y ∗. Let
Gt, E t, yt
M AP respectively denote the reduced problem, en-
ergy function and MAP assignment for the partition Y t. Us-
ing Lemma 1, Y t+1 is a partition of Y t. Then, using Theo-
rem 3.1, we have for every assignment yt to variables in Y t,
there is an assignment yt+1 to variables in Y t+1 such that
E t(yt) = E t+1(yt+1). Also, using Corollary 3.1, we have
∀t E t(yt
MAP). Together, these two state-
ments imply that starting from the coarsest partition, we can
gradually keep on improving the solution as we move to ﬁner
partitions.

MAP) ≥ E t+1(yt+1

Our C2F set-up assumes an iterative MAP inference algo-
rithm A which has the anytime property i.e., can produce so-
lutions of increasing quality with time. C2F Function (see
Algorithm 1) takes 3 inputs: a set of C2F partitions Y, infer-
ence algorithm A, and a stopping criteria C. The algorithm A
in turn takes three inputs: PGM Gt, starting assignment yt,
stopping criteria C. A outputs an approximation to the MAP
solution once the stopping criteria C is met. Starting with the
coarsest partition (t = 0 in line 2), a start state is picked for
the coarsest problem to be solved (line 3). In each iteration
(line 4), C2F ﬁnds the MAP estimate for the current problem
(Gt) using algorithm A (line 5). This solution is then mapped
to a same energy solution of the next ﬁner partition (line 6)
which becomes the starting state for the next run of A. The
solution is thus successively reﬁned in each iteration. The
process is repeated until we reach the ﬁnest level of partition.
In the end, A is run on the ﬁnest partition and the resultant
solution is output (lines 9,10). Since the last partition in the
set is the original problem G∗, optimality with respect to A is
guaranteed.

Next, we describe how to use the color passing algorithm
(Section 2) to get a series of partitions which get successively
ﬁner. Our C2F algorithm can then be applied on this set of
partitions to get anytime solutions of high quality while being
computationally efﬁcient.

Algorithm 1 Coarse-to-Fine Lifted MAP Algorithm
1:C2F Lifted MAP(C2F Partitions Y, Algo A,Criteria C)
2: t = 0; T = |Y|;
3: yt = getInitState(Gt);
4: While (t < T );
MAP = A(Gt, yt, C);
yt
5:
yt+1 = getEquivAssignment(Y t, Y t+1, yt
6:
t = t + 1;
7:
8: END While
9: yT
10: return yT

MAP = A(GT , yT , C);
MAP

MAP);

3.3 C2F Partitioning for Computer Vision
We now adapt the general color passing algorithm to MRFs
for CV problems. Unfortunately, unary potentials make color
passing highly ineffective. Different pixels have different
RGB values and intensities, leading to almost every pixel get-
ting a different unary potential. Naive application of color
passing splits almost all variables into their own partitions,
and lifting offers little value.

A natural approximation is to deﬁne a threshold, such that
two unary potentials within that threshold be initialized with
the same color. Our experiments show limited success with
this scheme because because two pixels may have the same
label even when their actual unary potentials are very differ-
ent. What is more important is relative importance given to
each label than the actual potential value.

In response, we adapt color passing for CV by initializing
it as before, but with one key change: we initialize two unary
potential nodes with the same color if their lowest energy la-
bels have the same order for the top NL labels (we call this
unary split threshold). Experiments reveal that this approxi-
mation leads to effective partitions for lifted inference.

Finally, we can easily construct a sequence of coarse-to-
ﬁne partitions in the natural course of color passing’s execu-
tion – every iteration of color passing creates a ﬁner partition.
Moreover, as an alternative approach, we may also increase
NL. In our implementations, we intersperse the two, i.e., be-
fore every next step we pick one of two choices: either, we
run another iteration of color passing; or, we increase NL by
one, and split each variable partition based on the N th
L lowest
energy labels of its constituent variables.

We parameterize CP (NL, Niter) to denote the partition
from the current state of color passing, which has been run
till Niter iterations and unary split threshold is NL.
It
is easy to prove that another iteration of color passing or
splitting by increasing NL as above leads to a ﬁner par-
I.e., CP (NL, Niter) (cid:22) CP (NL + 1, Niter) and
tition.
CP (NL, Niter) (cid:22) CP (NL, Niter + 1). We refer to each
element of a partition of variables as a lifted pixel, since it is
a subset of pixels.

4 Lifted Inference for Stereo Matching
We ﬁrst demonstrate the value of lifted inference in the con-
text of stereo matching [Scharstein and Szeliski, 2002].
It
aims to ﬁnd pixel correspondences in a set of images of the
same scene, which can be used to further estimate the 3D
scene. Formally, two images I l and I r corresponding to
images of the scene from a left camera and a right cam-
era are taken such that both cameras are at same horizon-
tal level. The goal is to compute a disparity labeling Dl
for every pixel X = (a, b) such that I l[a][b] corresponds to
I r[a−Dl[a][b]][b]. We build a lifted version of TSGO [Moze-
rov and van de Weijer, 2015], as it is MRF-based and ranks
2nd on the Middlebury Stereo Evaluation Version 2 leader-
board.3
Background on TSGO: TSGO treats stereo matching as
a two-step energy minimization, where the ﬁrst step is on a

3http://vision.middlebury.edu/stereo/eval/

(a)

(b)

(c)

Figure 1: (a) Average (normalized) energy vs. inference time (b) Average pixel error vs. time. C2F TSGO achieves roughly 60% reduction in
time for reaching the optima. It has best anytime performance compared to vanilla TSGO and static lifted versions. (c) Average (normalized)
energy vs. time for different thresholding values and CP partitions. Plots with the same marker have MRFs of similar sizes.

(a)

(b)

(c)

(d)

(e)

Figure 2: Qualitative results for Doll image at convergence. C2F-TSGO is similar to base TSGO.(a) Left and Right Images (b) Ground
Truth (c) Disparity Map by TSGO (d) Disparity Map by C2F TSGO (e) Each colored region (other than black) is one among the 10 largest
partition elements from CP(1,1). Each color represents one partition element. Partition elements form non-contiguous regions

fully connected MRF with pairwise potentials and the second
is on a conventional locally connected MRF. Lack of space
precludes a detailed description of the ﬁrst step. At the high
level, TSGO runs one iteration of message passing on fully
connected MRF, computes marginals of each pixel X, which
act as unary potentials φ(X) for the MRF of second step.

The pairwise potential ψ used in step two is ψ(X, X (cid:48)) =
w(X, X (cid:48))ϕ(X, X (cid:48)), where ϕ(X, X (cid:48)) is a truncated linear
function of (cid:107)X − X (cid:48)(cid:107), and w(X, X (cid:48)) takes one of three dis-
tinct values depending on color difference between pixels.
The MAP assignment xMAP computes the lowest energy as-
signment of disparities Dl for every pixel for this MRF.
Lifted TSGO: Since step two is costlier, we build its lifted
version as discussed in previous section. For color passing,
two unary potential nodes are initialized with the same color
if their lowest energy labels exactly match (NL = 1). Other
initializations are consistent with original color passing for
general MRFs. A sequence of coarse-to-ﬁne models is out-
putted as per Section 3.3. C2F TSGO uses outputs from the
sequence CP (1, 1), CP (2, 1), CP (3, 1) and then reﬁnes to
the original MRF. Model reﬁnement is triggered whenever
energy hasn’t decreased in the last four iterations of alpha ex-
pansion (this becomes the stopping criteria C in Algorithm
1).
Experiments: Our experiments build on top of the exist-
ing TSGO implementation4, but we change the minimization
algorithm in step two to alpha expansion fusion [Lempitsky
et al., 2010] from OpenGM2 library [Andres et al., 2010;
Kappes et al., 2015], as it improves the speed of the base

4http://www.cvc.uab.es/∼mozerov/Stereo/

implementation. We use the benchmark Middlebury Stereo
datasets of 2003, 2005 and 2006 [Scharstein and Szeliski,
2003; Hirschmuller and Scharstein, 2007]. For the 2003
dataset, quarter-size images are used and for others, third-size
images are used. The label space is of size 85 (85 distinct dis-
parity labels).

We

our

compare

coarse-to-ﬁne

TSGO (using
CP (NL, Niter) partitions) against vanilla TSGO. Fig-
ures 1(a,b) show the aggregate plots of energy (and error)
vs.
time. We observe that C2F TSGO reaches the same
optima as TSGO, but in less than half the time. It has a much
superior anytime performance – if inference time is given as
a deadline, C2F TSGO obtains 59.59% less error on average
over randomly sampled deadlines. We also eyeball the out-
puts of C2F TSGO and TSGO and ﬁnd them to be visually
similar. Figure 2 shows a sample qualitative comparison.
Figure 2(e) shows ﬁve of the ten largest partition elements in
the partition from CP (1, 1). Clearly, the partition elements
formed are not contiguous, and seem to capture variables that
are likely to get the same assignment. This underscores the
value of our lifting framework for CV problems.

We also compare our CP (NL, Niter) partitioning strat-
egy with threshold partitioning discussed in Section 3.3. We
merge two pixels in thresholding scheme if the L1-norm dis-
tance of their unary potentials is less than a threshold. For
each partition induced by our approach, we ﬁnd a value of
threshold that has roughly the same number of lifted pix-
els. Figure 1(c) shows that partitions based on CP (1, 1) and
CP (3, 1) converges to a much lower energy quickly com-
pared to the corresponding threshold values (T hr = 50 and
T hr = 1 respectively). For CP (2, 1), convergence is slower

compared to corresponding threshold (T hr = 5) but eventu-
ally CP (2, 1) has signiﬁcantly better quality.

5 Lifted Inference for Image Segmentation
We now demonstrate the general nature of our lifted CV
framework by applying it to a second task. We choose multi-
label interactive image segmentation, where the goal is to
segment an image I based on a seed labeling (true labels
for a few pixels) provided as input. Like many other CV
problems, this also has an MRF-based solution, with the best
label-assignment generally obtained by MAP inference using
graph cuts or loopy belief propagation [Boykov et al., 2001;
Szeliski et al., 2008].

However, MRFs with only pairwise potentials are known to
suffer from short-boundary bias – they prefer segmentations
with shorter boundaries, because pairwise potentials penalize
every pair of boundary pixels. This leads to incorrect labeling
for sharp edge objects. Kohli et al. [2013] use CoGC, coop-
erative graph cuts [Jegelka and Bilmes, 2011], to develop one
of the best MRF-based solvers that overcome this bias.
Background on CoGC: Traditional MRFs linearly penalize
the number of label discontinuities at edges (boundary pixel
pairs), but CoGC penalizes the number of types of label dis-
continuities through the use of a concave energy function over
groups of ordered edges. It ﬁrst clusters all edges on the ba-
sis of color differences, and later applies a concave function
separately over the number of times a speciﬁc discontinuity
type is present in each edge group g ∈ G. Their carefully
engineered CoGC energy function is as follows:

E(x) =

φi(xi)+

w(x, x(cid:48)).I(x = l, x(cid:48) (cid:54)= l)

|X |
(cid:88)

i=1



F



(cid:88)

(cid:88)

(cid:88)

g∈G

l∈L

(x,x(cid:48))∈g





where unary potentials φ depend on colors of seed pixels, F
is a concave function, I the indicator function, and w(x, x(cid:48))
depends on the color difference between x, x(cid:48).
Intuitively,
F collects all edges with similar discontinuities and penal-
izes them sub-linearly, thus reducing the short-boundary bias
in the model. The usage of a concave function makes the
MRF higher order with cliques over edge groups. However,
the model is shown to reduce to a pairwise hierarchical MRF
through the addition of auxiliary variables.
Lifted CoGC: CoGC is lifted using the framework of Sec-
tion 3, with one additional change. We cluster edge groups
using color difference and the position of the edge. Edge
groups that are formed only on the basis of color difference
make the error of grouping different segment’s boundaries
into a single group. For e.g., it erroneously cluster boundaries
between white cow and grass, and sky and grass together in
the top image in Figure 3.

2 (cid:101), 2), CP ((cid:100) L

Coarse-to-ﬁne partitions are obtained by the method de-
scribed in Section 3.3. C2F CoGC uses outputs from the se-
quence CP ((cid:100) L
2 (cid:101), 3) before reﬁning to the orig-
inal MRF. Model reﬁnement is triggered if energy has not
reduced over the last |L| iterations.
Experiments: Our experiments use the implementation of
Cooperative Graph Cuts as provided by [Kohli et al., 2013].5

5Available at https://github.com/aosokin/coopCuts CVPR2013

Energy minimization is performed using alpha expansion
[Boykov et al., 2001]. The implementation of CoGC per-
forms a greedy descent on auxiliary variables while perform-
ing alpha expansion on the remaining variables, as described
in Kohli et. al. [2013]. The dataset used is provided with the
implementation. It is a part of the MSRC V2 dataset.6.

Figure 3 shows three individual energy vs. time plots. Re-
sults on other images are similar. We ﬁnd that C2F CoGC
algorithm converges to the same energy as CoGC in about
two-thirds the time on average. Overall, C2F CoGC achieves
a much better anytime performance than other lifted and un-
lifted CoGC.

Similar to Section 4, reﬁned partitions attain better quality
than coarser ones at the expense of time. Since the implemen-
tation performs a greedy descent over auxiliary variables, re-
ﬁnement of current partition also resets the auxiliary variables
to the last value that produced a change. Notice that energy
minimization on output of CP (2, 3) attains a lower energy
than on CP (3, 2). This observation drives our decision to
reﬁne by increasing Niter. Qualitatively, C2F CoGC pro-
duces the same labeling as CoGC. Finally, similar to stereo
matching, partitions based on thresholding scheme perform
signiﬁcantly worse compared to CP (NL, Niter) for image
segmentation as well.

6 Related Work

There is a large body of work on exact
lifting, both
marginal [Kersting et al., 2009; Gogate and Domingos, 2011;
Niepert, 2012; Mittal et al., 2015] and MAP [Kersting
et al., 2009; Gogate and Domingos, 2011; Niepert, 2012;
Sarkhel et al., 2014; Mittal et al., 2014], which is not di-
rectly applicable to our setting. There is some recent work
on approximate lifting [Van den Broeck and Darwiche, 2013;
Venugopal and Gogate, 2014; Singla et al., 2014; Sarkhel et
al., 2015; Van den Broeck and Niepert, 2015] but it’s fo-
cus is on marginal inference whereas we are interested in
lifted MAP. Further, this work can’t handle a distinct unary
potential on every node. An exception is work by Bui et
al. [2012] which explicitly deals with lifting in presence of
distinct unary potentials. Unfortunately, they make a very
strong assumption of exchangeability in the absence of unar-
ies which does not hold true in our setting since each pixel
has its own unique neighborhood.

Work by Sarkhel et al. [2015] is probably the closest to our
work. They design a C2F hierarchy to cluster constants for
approximate lifted MAP inference in Markov logic. In con-
trast, we partition ground atoms in a PGM. Like other work
on approximate lifting, they can’t handle distinct unary po-
tentials. Furthermore, they assume that their theory is pro-
vided in a normal form, i.e., without evidence, which can be
a severe restriction for most practical applications. Kiddon &
Domingos [2011] also propose C2F inference for an underly-
ing Markov logic theory. They use a hierarchy of partitions
based on a pre-speciﬁed ontology. CV does not have any such

6Available

at

https://www.microsoft.com/en-us/research/

project/image-understanding/?from=http%3A%2F%2Fresearch.
microsoft.com%2Fvision%2Fcambridge%2Frecognition%2F

Figure 3: (a-c) Qualitative Results for Segmentation. C2F has quality similar to CoGC algorithm (a) Original Image (b) Segmentation by
CoGC (c) Segmentation by C2F CoGC (d) C2F CoGC has lower energy compared to CoGC and other lifted variant at all times

ontology available, and needs to discover partitions using the
PGM directly.

Nath & Domingos [2010] exploit (approximate) lifted in-
ference for video segmentation. They experiment on a spe-
ciﬁc video problem (different from ours), and they only com-
pare against vanilla BP. Their initial partitioning scheme is
similar to our thresholding approach, which does not work
well in our experiments.

In computer vision, a popular approach to reduce the com-
plexity of inference is to use superpixels [Achanta et al.,
2012; Van den Bergh et al., 2012]. Superpixels are obtained
by merging neighboring nodes that have similar character-
istics. All pixel nodes in the same superpixel are assigned
the same value during MAP inference. SLIC [Achanta et al.,
2012] is one of the most popular algorithms for discovering
superpixels. Our approach differs from SLIC in some signiﬁ-
cant ways. First, their superpixels are local in nature whereas
our algorithm can merge pixels that are far apart. This can
help in merging two disconnected regions of the same object
in a single lifted pixel. Second, they obtain superpixels inde-
pendent of the inference algorithm, whereas we tightly inte-
grate our lifting with the underlying inference algorithm. This
can potentially lead to discovery of better partitions; indeed,
this helped us tremendously in image segmentation. Third,
they do not provide a C2F version of their algorithm and
we did not ﬁnd it straightforward to extend their approach
to discover successively ﬁner partitions. There is some recent
work [Wei et al., 2016] which addresses last two of these
challenges by introducing a hierarchy of superpixels. In our
preliminary experiments, we found that SLIC and superpixel
hierarchy perform worse than our lifting approach. Perform-
ing more rigorous comparisons is a direction for future work.

7 Conclusion and Future Work
We develop a generic template for applying lifted inference
to structured output prediction tasks in computer vision. We
show that MRF-based CV algorithms can be lifted at different
levels of abstraction, leading to methods for coarse to ﬁne
inference over a sequence of lifted models. We test our ideas
on two different CV tasks of stereo matching and interactive
image segmentation. We ﬁnd that C2F lifting is vastly more
efﬁcient than unlifted algorithms on both tasks obtaining a
superior anytime performance, and without any loss in ﬁnal
solution quality. To the best of our knowledge, this is the ﬁrst
demonstration of lifted inference in conjunction with top of
the line task-speciﬁc algorithms. Although we restrict to CV
in this work, we believe that our ideas are general and can
be adapted to other domains such as NLP, and computational
biology. We plan to explore this in the future.

Acknowledgements
We thank anonymous reviewers for their comments and sug-
gestions. Ankit Anand is being supported by the TCS Re-
search Scholars Program. Mausam is being supported by
grants from Google and Bloomberg. Parag Singla is being
supported by a DARPA grant funded under the Explainable
AI (XAI) program. Both Mausam and Parag Singla are being
supported by the Visvesvaraya Young Faculty Fellowships by
Govt. of India. Any opinions, ﬁndings, conclusions or rec-
ommendations expressed in this paper are those of the authors
and do not necessarily reﬂect the views or ofﬁcial policies, ei-
ther expressed or implied, of the funding agencies.

References
[Achanta et al., 2012] R. Achanta, A. Shaji, K. Smith, A. Lucchi,
P. Fua, and S. Ssstrunk. SLIC Superpixels Compared to State-of-

the-Art Superpixel Methods. In PAMI, Nov 2012.

[Anand et al., 2016] A. Anand, A. Grover, Mausam, and P. Singla.
Contextual Symmetries in Probabilistic Graphical Models. In IJ-
CAI, 2016.

[Anand et al., 2017] A. Anand, R. Noothigattu, P. Singla, and
Mausam. Non-Count Symmetries in Boolean & Multi-Valued
Prob. Graphical Models. In AISTATS, 2017.

[Andres et al., 2010] B. Andres,

J. H. Kappes, U. K¨othe,
C. Schn¨orr, and F. A. Hamprecht. An Empirical Comparison of
Inference Algorithms for Graphical Models with Higher Order
Factors Using OpenGM. In Pattern Recognition. 2010.

[Blei et al., 2003] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet

Allocation. JMLR, 3, March 2003.

[Boykov et al., 2001] Y. Boykov, O. Veksler, and R. Zabih. Fast
In PAMI,

Approximate Energy Minimization via Graph Cuts.
23(11), November 2001.

[Braz et al., 2005] R. Braz, E. Amir, and D. Roth. Lifted First-

Order Probabilistic Inference. In IJCAI, 2005.

[Bui et al., 2012] H. Bui, T. Huynh, and R. De Salvo Braz. Exact
Lifted Inference with Distinct Soft Evidence on Every Object. In
AAAI, 2012.

[Bui et al., 2013] H. Bui, T. Huynh, and S. Riedel. Automorphism
Groups of Graphical Models and Lifted Variational Inference. In
UAI, 2013.

[Freeman et al., 2000] W. Freeman, E. Pasztor, and O. Carmichael.

Learning Low-Level Vision. In IJCV, 40, 2000.

[Friedman, 2004] N. Friedman. Inferring Cellular Networks using

Probabilistic Graphical Models. Science, 303, 2004.

[Hirschmuller and Scharstein, 2007] H.

and
Evaluation of Cost Functions for Stereo

Hirschmuller

D. Scharstein.
Matching. In CVPR, 2007.

[Jegelka and Bilmes, 2011] S. Jegelka and J. Bilmes. Submodu-
larity Beyond Submodular Energies: Coupling Edges in Graph
Cuts. In CVPR, 2011.

[Jernite et al., 2015] Y. Jernite, A. Rush, and D. Sontag. A Fast
Variational Approach for Learning Markov Random Field Lan-
guage Models. In ICML, 2015.

[Jha et al., 2010] A. Jha, V. Gogate, A. Meliou, and D. Suciu.
Lifted Inference Seen from the Other Side : The Tractable Fea-
tures. In NIPS, 2010.

[Kappes et al., 2015] J. Kappes, B. Andres, A. Hamprecht,
C. Schn¨orr, S. Nowozin, D. Batra, S. Kim, B. Kausler, T. Kr¨oger,
J. Lellmann, N. Komodakis, B. Savchynskyy, and C. Rother. A
Comparative Study of Modern Inference Techniques for Struc-
tured Discrete Energy Minimization Problems. In IJCV, 2015.
[Kersting et al., 2009] K. Kersting, B. Ahmadi, and S. Natarajan.

Counting Belief Propagation. In UAI, 2009.

[Kersting, 2012] K. Kersting. Lifted Probabilistic Inference.

In

ECAI, 2012.

[Kiddon and Domingos, 2011] C. Kiddon and P. Domingos.
Coarse-to-Fine Inference and Learning for First-Order Proba-
bilistic Models. In AAAI, 2011.

[Kimmig et al., 2015] A. Kimmig, L. Mihalkova, and L. Getoor.
Lifted Graphical Models: A Survey. Machine Learning, 2015.

[Kohli et al., 2013] P. Kohli, A. Osokin, and S. Jegelka. A Prin-
cipled Deep Random Field Model for Image Segmentation. In
CVPR, 2013.

[Lempitsky et al., 2010] V. Lempitsky, C. Rother, S. Roth, and
A. Blake. Fusion Moves for Markov Random Field Optimiza-
tion. In PAMI, Aug 2010.

[Mittal et al., 2014] H. Mittal, P. Goyal, V. Gogate, and P. Singla.
New Rules for Domain Independent Lifted MAP Inference. In
NIPS, 2014.

[Mittal et al., 2015] H. Mittal, A. Mahajan, V. Gogate, and
In NIPS,

P. Singla. Lifted Inference Rules With Constraints.
2015.

[Mladenov et al., 2014] M. Mladenov, K. Kersting, and A. Glober-
son. Efﬁcient Lifting of MAP LP Relaxations Using k-Locality.
In AISTATS, 2014.

[Mozerov and van de Weijer, 2015] M. G. Mozerov and J. van de
Weijer. Accurate Stereo Matching by Two-Step Energy Mini-
mization. IEEE Transactions on Image Processing, March 2015.

[Nath and Domingos, 2010] A. Nath and P. Domingos. Efﬁcient
Lifting for Online Probabilistic Inference. In AAAIWS, 2010.

[Nath and Domingos, 2016] A. Nath and P. Domingos. Learning
Tractable Probabilistic Models for Fault Localization. In AAAI,
2016.

[Niepert, 2012] M. Niepert. Markov Chains on Orbits of Permuta-

tion Groups. In UAI, 2012.

[Noessner et al., 2013] J. Noessner, M. Niepert, and H. Stucken-
schmidt. RockIt: Exploiting Parallelism and Symmetry for MAP
Inference in Statistical Relational Models. In AAAI, 2013.

[Poole, 2003] D. Poole. First-Order Probabilistic Inference. In IJ-

[Sarkhel et al., 2014] S. Sarkhel, D. Venugopal, P. Singla, and
V. Gogate. Lifted MAP inference for Markov logic networks.
In AISTATS, 2014.

[Sarkhel et al., 2015] S. Sarkhel, P. Singla, and V. Gogate. Fast

Lifted MAP Inference via Partitioning. In NIPS, 2015.

[Scharstein and Szeliski, 2002] D. Scharstein and R. Szeliski. A
Taxonomy and Evaluation of Dense Two-Frame Stereo Corre-
spondence Algorithms. In IJCV, 2002.

[Scharstein and Szeliski, 2003] D. Scharstein and R. Szeliski.
High-accuracy Stereo Depth Maps Using Structured Light.
In
CVPR, 2003.

[Singla and Domingos, 2008] P. Singla and P. Domingos. Lifted

First-Order Belief Propagation. In AAAI, 2008.

[Singla et al., 2014] P. Singla, A. Nath, and P. Domingos. Approx-
imate Lifting Techniques for Belief Propagation. In AAAI, 2014.

[Szeliski et al., 2008] R. Szeliski, R. Zabih, D. Scharstein, O. Vek-
sler, V. Kolmogorov, A. Agarwala, M. Tappen, and C. Rother. A
Comparative Study of Energy Minimization Methods for Markov
In PAMI, June
Random Fields with Smoothness-Based Priors.
2008.

[Van den Bergh et al., 2012] M. Van den Bergh, X. Boix, G. Roig,
B. de Capitani, and L. Van Gool. SEEDS: Superpixels Extracted
via Energy-Driven Sampling. In ECCV, 2012.

[Van den Broeck and Darwiche, 2013] G. Van den Broeck and
A. Darwiche. On the Complexity and Approximation of Binary
Evidence in Lifted Inference. In NIPS, 2013.

[Gogate and Domingos, 2011] V. Gogate and P. Domingos. Proba-

bilisitic Theorem Proving. In UAI, 2011.

CAI, 2003.

[Van den Broeck and Niepert, 2015] G. Van den Broeck and
Lifted Probabilistic Inference for Asymmetric

M. Niepert.
Graphical Models. In AAAI, 2015.

[Venugopal and Gogate, 2014] D. Venugopal

and V. Gogate.
Evidence-Based Clustering for Scalable Inference in Markov
Logic. In Joint ECML-KDD, 2014.

[Wei et al., 2016] X. Wei, Q. Yang, Y. Gong, M. Yang, and
N. Ahuja. Superpixel Hierarchy. CoRR, abs/1605.06325, 2016.

Coarse-to-Fine Lifted MAP Inference in Computer Vision

Haroun Habeeb and Ankit Anand and Mausam and Parag Singla
Indian Institute of Technology Delhi
haroun7@gmail.com and {ankit.anand,mausam,parags}@cse.iitd.ac.in

7
1
0
2
 
l
u
J
 
2
2
 
 
]

V
C
.
s
c
[
 
 
1
v
5
6
1
7
0
.
7
0
7
1
:
v
i
X
r
a

Abstract

There is a vast body of theoretical research on
lifted inference in probabilistic graphical models
(PGMs). However, few demonstrations exist where
lifting is applied in conjunction with top of the line
applied algorithms. We pursue the applicability
of lifted inference for computer vision (CV), with
the insight that a globally optimal (MAP) labeling
will likely have the same label for two symmetric
pixels. The success of our approach lies in efﬁ-
ciently handling a distinct unary potential on ev-
ery node (pixel), typical of CV applications. This
allows us to lift the large class of algorithms that
model a CV problem via PGM inference. We pro-
pose a generic template for coarse-to-ﬁne (C2F) in-
ference in CV, which progressively reﬁnes an ini-
tial coarsely lifted PGM for varying quality-time
trade-offs. We demonstrate the performance of
C2F inference by developing lifted versions of two
near state-of-the-art CV algorithms for stereo vi-
sion and interactive image segmentation. We ﬁnd
that, against ﬂat algorithms, the lifted versions have
a much superior anytime performance, without any
loss in ﬁnal solution quality.

1 Introduction
Lifted inference in probabilistic graphical models (PGMs)
refers to the set of the techniques that carry out inference
over groups of random variables (or states) that behave simi-
larly [Jha et al., 2010; Kimmig et al., 2015]. A vast body of
theoretical work develops a variety of lifted inference tech-
niques, both exact (e.g., [Poole, 2003; Braz et al., 2005;
Singla and Domingos, 2008; Kersting, 2012]) and approxi-
mate (e.g., [Singla et al., 2014; Van den Broeck and Niepert,
2015]). Most of these works develop technical ideas appli-
cable to generic subclasses of PGMs, and the accompanying
experiments are aimed at providing ﬁrst proofs of concepts.
However, little work exists on transferring these ideas to the
top domain-speciﬁc algorithms for real-world applications.

Algorithms for NLP, computational biology, and computer
vision (CV) problems make heavy use of PGM machinery
(e.g., [Blei et al., 2003; Friedman, 2004; Szeliski et al.,
2008]). But, they also include signiﬁcant problem-speciﬁc

insights to get high performance. Barring a handful of excep-
tions [Jernite et al., 2015; Nath and Domingos, 2016], lifted
inference hasn’t been applied directly to such algorithms.

We study the potential value of lifting to CV problems such
as image denoising, stereo vision, and image segmentation.
Most CV problems are structured output prediction tasks, typ-
ically assigning a label to each pixel. A large class of solu-
tions are PGM-based: they deﬁne a Markov Random Field
(MRF) that has each pixel as a node, with unary potential that
depends on pixel value, and pairwise neighborhood potentials
that favor similar labels to neighboring pixels.

We see three main challenges in applying existing lifted
inference literature to these problems. First, most existing al-
gorithms focus on computing marginals [Singla and Domin-
gos, 2008; Kersting et al., 2009; Gogate and Domingos, 2011;
Niepert, 2012; Anand et al., 2016; Anand et al., 2017] instead
of MAP inference. Second, among the algorithms perform-
ing lifted MAP [Noessner et al., 2013; Mladenov et al., 2014;
Sarkhel et al., 2014; Mittal et al., 2014], many of the algo-
rithms focus on exact lifting. This breaks the kind of symme-
tries we need to compute since different pixels may not have
exact same neighborhood. Third, the few algorithms that per-
form approximate lifting for MAP, e.g. [Sarkhel et al., 2015],
can’t handle a distinct unary potential on every node. This is
essential for our application since image pixels take ordinal
values in three channels.

In response, we develop an approximate lifted MAP infer-
ence algorithm which can effectively handle unary potentials.
We initialize our algorithm by merging together pixels hav-
ing the same order of top-k labels based on the unary po-
tential values. We then adapt an existing symmetry ﬁnding
algorithm [Kersting et al., 2009] to discover groupings which
also have similar neighborhoods. We refer to our groupings
as lifted pixels. We impose the constraint that all pixels in a
lifted pixel must be assigned the same label. Our approximate
lifting reduces the model size drastically leading to signiﬁ-
cant time savings. Unfortunately, such approximate lifting
could adversely impact solution quality. However, we vary
the degree of approximation in symmetry ﬁnding to output a
sequence of coarse-to-ﬁne models with varying quality-time
trade-offs. By switching between such models, we develop a
coarse-to-ﬁne (C2F) inference procedure applicable to many
CV problems.

We formalize these ideas in a novel template for using

lifted inference in CV. We test C2F lifted inference on two
problems: stereo matching and image segmentation. We start
with one of the best MRF-based solvers each for both prob-
lems – neither of these are vanilla MRF solvers. Mozerov
& Weijer [2015] use a two-way energy minimization to ef-
fectively handle occluded regions in stereo matching. Co-
operative cuts [Kohli et al., 2013] for image segmentation use
concave functions over a predeﬁned set of pixel pairs to cor-
rectly segment images with sharp edges. We implement C2F
inference on top of both these algorithms and ﬁnd that C2F
versions have a strong anytime behavior – given any amount
of inference time, they output a much higher quality (and are
never worse) than their unlifted counterparts, and don’t suffer
any loss in the ﬁnal quality. Overall, our contributions are:

1. We present an approximate lifted MAP algorithm that
can efﬁciently handle a large number of distinct unary
potentials.

2. We develop a novel template for applying lifted infer-
ence in structured prediction tasks in CV. We provide
methods that output progressively ﬁner approx. symme-
tries, leading to a C2F lifted inference procedure.

3. We implement C2F inference over a near state-of-the-
art stereo matching algorithm, and one of the best MRF-
based image segmentation algorithms. We release our
implementation for wider use by the community.1

4. We ﬁnd that C2F has a much superior anytime behav-
ior. For stereo matching it achieves 60% better quality
on average in time-constrained settings. For image seg-
mentation C2F reaches convergence in 33% less time.

2 Background
2.1 Computer Vision Problems as MRFs
Most computer vision problems are structured output predic-
tion problems and their PGM-based solutions often follow
similar formulations. They cast the tasks into the problem
of ﬁnding the lowest energy assignment over grid-structured
MRFs (denoted by G = (X , γ)). The random variables in
these MRFs are the set of pixels X in the input image. Given
a set of labels L : {1, 2, . . . , |L|}, the task of structured output
prediction is to label each pixel X with a label from L. The
MRFs have two kinds of potentials (γ) – unary and higher-
order. Unary potentials are deﬁned over each individual pixel,
and usually incorporate pixel intensity, color, and other pixel
features. Higher order potentials operate over cliques (pairs
or more) of neighboring pixels and typically express some
form of spatial homophily – “neighboring pixels are more
likely to have similar labels.” While the general PGM struc-
ture of various tasks are similar, the speciﬁc potential tables
and label spaces are task-dependent.

The goal is to ﬁnd the MAP assignment over this MRF,
which is equivalent to energy minimization (by deﬁning en-
ergy as negative log of potentials). We denote the negative
log of unary potentials by φ, and that of higher-order poten-
tials by ψ.2 Thus, energy of a complete assignment x ∈ L|X |

1https://github.com/dair-iitd/c2ﬁ4cv/
2In the interest of readability, we say ‘potential’ to mean ‘nega-

tive log of potential’ in the rest of the paper.

can be deﬁned as:

E(x) =

φ(xi) +

ψj(ˆxj)

(1)

(cid:88)

i∈1..|X |

(cid:88)

j

Here ˆxj denotes the assignment x restricted to the set of
variables in the potential ψj. And the output of the algorithm
is the assignment xMAP:

xMAP = arg min
x∈L|X |

E(x)

(2)

The problem is in general intractable. Efﬁcient approxi-
mations exploit special characteristics of potentials like sub-
modularity [Jegelka and Bilmes, 2011], or use variants of
graph cut or loopy belief propagation [Boykov et al., 2001;
Freeman et al., 2000].

2.2 Symmetries in Graphical Models
Lifting an algorithm often requires computing a set of sym-
metries that can be exploited by that algorithm. For PGMs,
two popular methods for symmetry computation are color
passing for computing symmetries of variables [Kersting et
al., 2009], and graph isomorphism for symmetries of states
[Niepert, 2012; Bui et al., 2013]. Since our work is based on
color passing, we explain it in more detail.

Color passing for an MRF operates over a colored bipar-
tite graph containing nodes for all variables and potentials,
and each node is assigned a color. The graph is initialized
as follows: all variables nodes get a common color; all po-
tential nodes with exactly same potential tables are assigned
a unique color. Now, in an iterative color passing scheme,
in each iteration, each variable node passes its color to all
neighboring potential nodes. The potential nodes store in-
coming color signatures in a vector, append their own color
to it, and send the vector back to variable nodes. The variable
nodes stack these incoming vectors. New colors are assigned
to each node based on the set of incoming messages such that
two nodes with same messages are assigned the same unique
color. This process is repeated until convergence, i.e., no fur-
ther change in colors.

A coloring of the bipartite graph deﬁnes a partition of vari-
able nodes such that all nodes of the same color form a par-
tition element. Each iteration of color passing creates suc-
cessively ﬁner partitions, since two variable nodes, once as-
signed different colors, can never get the same color.

3 Lifted Computer Vision Framework
In this section, we will describe our generic template which
can be used to lift a large class of vision applications includ-
ing those in stereo, segmentation etc. Our template can be
seen as transforming the original problem space to a reduced
problem space over which the original inference algorithm
can now be applied much more efﬁciently. Speciﬁcally, our
description in this section is entirely algorithm independent.
We will focus on MAP inference which is the inference
task of choice for most vision applications (refer Section 2).
The key insight in our formulation is based on the realiza-
tion that pixels which are involved in the same (or similar)

kinds of unary and higher order potentials, and have the same
(or similar) neighborhoods, are likely to have the same MAP
value. Therefore, if somehow we could discover such sets
of pixels a priori, we could explicitly enforce these pixels to
have the same value while searching for the solution, substan-
tially reducing the problem size and still preserving the opti-
mal MAP assignment(s). Since in general doing this exactly
may lead to a degenerate network, we do it approximately.
Hence trading-off speed for marginal loss in solution quality.
The loss in solution quality is offset by resorting to coarse-
to-ﬁne inference where we start with a crude approximation,
and gradually make it ﬁner, to guarantee optimality at the end
while still obtaining signiﬁcant gains. We next describe the
details of our approach.

3.1 Obtaining a Reduced Problem

1 , Y P

2 , · · · , Y P

k ⊆ X , Y P
k1

Consider an energy minimization problem over a PGM G =
(X , γ). Let L = {1, 2, · · · , |L|} denote the set of labels
over which variables in the set X can vary. Let Y P =
{Y P
r } denote a partition of X into r disjoint
∩ Y P
subsets, i.e., ∀k, Y P
= ∅ when k1 (cid:54)= k2,
k2
and ∪kY P
k = X . We refer to each Y P
k as a partition element.
Correspondingly, let us deﬁne Y = {Y1, Y2, · · · , Yr} as a set
of partition variables, where there is a one to one correspon-
dence between partition elements and the partition variables
and each partition variable Yk takes values in the set L. Let
part(Xi) denote the partition element to which Xi belongs.
Let ˆXj ⊆ X denote a subset of variables. We say that a par-
k is represented in the set ˆXj if ∃Xi ∈ ˆXj
tition element Y P
s.t. part(Xj) = Yk.
Given a subset of variables ˆXj, let γj( ˆXj) be a potential de-
ﬁned over ˆXj. Let ˆxj denote an assignment to variables in
the set ˆXj. Let ˆxj.elem(i) denote the value taken by a vari-
able Xi in ˆXj. We say that an assignment ˆXj = ˆxj respects
a partition Y P if the variables in ˆXj belonging to the same
partition element have the same label in ˆxj, i.e., part(Xi) =
part(Xi(cid:48)) ⇒ ˆxj.elem(i) = ˆxj.elem(i(cid:48)), ∀Xi, Xi(cid:48) ∈ ˆXj.
Next, we introduce the notion of a reduced potential.

Deﬁnition 3.1 Let X be a set of variables and let Y P de-
note its partition. Given the potential γj( ˆXj), the reduced
potential Γj is deﬁned to be the restriction of γj( ˆXj) to those
labeling assignments of ˆXj which respect the partition Y P .
Equivalently, we can deﬁne the reduced potential Γj( ˆYj) over
the set of partition variables ˆYj which are represented in the
set ˆXj.

For example, consider a potential γ(X1, X2, X3) deﬁned
over three Boolean variables. The table for γ would have
8 entries. Consider the partition Y P = {Y P
2 } where
1 = {X1, X2} and Y P
Y P
2 = {X3}. Then, the reduced poten-
tial Γ is the restriction of γ to those rows in the table where
X1 = X2. Hence Γ has four rows in its table and equivalently
can be thought of deﬁning a potential over the 4 possible com-
binations of Y1 and Y2 variables. We are now ready to deﬁne
a reduced graphical model.

1 , Y P

Deﬁnition 3.2 Let G = (X , γ) represent a PGM. Given a
partition Y P of X , the reduced graphical model G(Y, Γ) is
the graphical model deﬁned over the set of partition variables
Y such that every potential γj ∈ γ in G is replaced by the
corresponding reduced potential Γj ∈ Γ in G.
Let E(x) and E(y) denote the energies of the states x and y
in G and G, respectively. The following theorem relates the
energies of the states in the two graphical models.

Theorem 3.1 For every assignment y of Y in G, there is a
corresponding assignment x of X such that E(y) = E(x).

The theorem can be proved by noting that each potential
Γj( ˆYj) in G was obtained by restricting the original poten-
tial γj( ˆXj) to those assignments where variables in Xj be-
longing to the same partition took the same label. Since this
correspondence is true for every potential in the reduced set,
to obtain the desired state x, for every variable Xi ∈ X we
simply assign it the label of its partition in y.
Corollary 3.1 Let xMAP and yMAP be the MAP states (i.e.
having the minimum energy) for G and G, respectively. Then,
E(yMAP) ≥ E(xMAP).
The process of reduction can be seen as curtailing the en-
tire search space to those assignments where variables in the
same partition take the same label. A reduction in the prob-
lem space will lead to computational gains but might result
in loss of solution quality, where the solution quality can be
captured by the difference between E(yMAP) and E(xMAP).
Therefore, we need to trade-off the balance between the two.
Intuitively, a good problem reduction will keep those vari-
ables in the same partition which are likely to have the same
value in the optimal assignment for the original problem.
How do we ﬁnd such variables without actually solving the
inference task? We will describe one such technique in Sec-
tion 3.3.

There is another perspective.

Instead of solving one re-
duced problem, we can instead work with a series of reduced
problems which successively get closer to the optimal solu-
tion. The initial reductions are coarser and far from optimal,
but can be solved efﬁciently to quickly reach in the region
where the solution lies. The successive iterations can then re-
ﬁne the solution iteratively getting closer to the optimal. This
leads us to the coarse-to-ﬁne inference described next.

3.2 Coarse to Fine Inference
We will deﬁne a framework for C2F (coarse-to-ﬁne) infer-
ence so that we maintain the computational advantage while
still preserving optimality. In the following, for ease of no-
tation, we will drop the superscript P in Y p to denote the
partition of X . Therefore, Y will refer to both the partition as
well as the set of partition variables. Before we describe our
algorithm, let us start with some deﬁnitions.
Deﬁnition 3.3 Let Y and Y (cid:48) be two partitions of X . We
say that Y is coarser than Y (cid:48), denoted as Y (cid:22) Y (cid:48),
if
∀y(cid:48) ∈ Y (cid:48)∃y ∈ Y such that y(cid:48) ⊆ y. We equivalently say
that Y (cid:48) is ﬁner than Y.
It is easy to see that X deﬁnes a partition of itself which is the
ﬁnest among all partitions, i.e., ∀Y such that Y is a partition

of X , Y (cid:22) X . We also refer it to as the degenerate partition.
For ease of notation, we will denote the ﬁnest partition by
Y ∗ (same as X ). We will refer to the corresponding PGM as
G∗ (same as G). Next, we state a theorem which relates two
partitions with each other.
Lemma 1 Let Y and Y (cid:48) be two partitions of X such that Y (cid:22)
Y (cid:48). Then Y (cid:48) can be seen as a partition of the set Y.

The proof of this lemma is straightforward and is omitted
due to lack of space. Consider a set Y of coarse to ﬁne
partitions given as Y 0 (cid:22) Y 1, · · · , (cid:22), Y t, (cid:22), · · · , Y ∗. Let
Gt, E t, yt
M AP respectively denote the reduced problem, en-
ergy function and MAP assignment for the partition Y t. Us-
ing Lemma 1, Y t+1 is a partition of Y t. Then, using Theo-
rem 3.1, we have for every assignment yt to variables in Y t,
there is an assignment yt+1 to variables in Y t+1 such that
E t(yt) = E t+1(yt+1). Also, using Corollary 3.1, we have
∀t E t(yt
MAP). Together, these two state-
ments imply that starting from the coarsest partition, we can
gradually keep on improving the solution as we move to ﬁner
partitions.

MAP) ≥ E t+1(yt+1

Our C2F set-up assumes an iterative MAP inference algo-
rithm A which has the anytime property i.e., can produce so-
lutions of increasing quality with time. C2F Function (see
Algorithm 1) takes 3 inputs: a set of C2F partitions Y, infer-
ence algorithm A, and a stopping criteria C. The algorithm A
in turn takes three inputs: PGM Gt, starting assignment yt,
stopping criteria C. A outputs an approximation to the MAP
solution once the stopping criteria C is met. Starting with the
coarsest partition (t = 0 in line 2), a start state is picked for
the coarsest problem to be solved (line 3). In each iteration
(line 4), C2F ﬁnds the MAP estimate for the current problem
(Gt) using algorithm A (line 5). This solution is then mapped
to a same energy solution of the next ﬁner partition (line 6)
which becomes the starting state for the next run of A. The
solution is thus successively reﬁned in each iteration. The
process is repeated until we reach the ﬁnest level of partition.
In the end, A is run on the ﬁnest partition and the resultant
solution is output (lines 9,10). Since the last partition in the
set is the original problem G∗, optimality with respect to A is
guaranteed.

Next, we describe how to use the color passing algorithm
(Section 2) to get a series of partitions which get successively
ﬁner. Our C2F algorithm can then be applied on this set of
partitions to get anytime solutions of high quality while being
computationally efﬁcient.

Algorithm 1 Coarse-to-Fine Lifted MAP Algorithm
1:C2F Lifted MAP(C2F Partitions Y, Algo A,Criteria C)
2: t = 0; T = |Y|;
3: yt = getInitState(Gt);
4: While (t < T );
MAP = A(Gt, yt, C);
yt
5:
yt+1 = getEquivAssignment(Y t, Y t+1, yt
6:
t = t + 1;
7:
8: END While
9: yT
10: return yT

MAP = A(GT , yT , C);
MAP

MAP);

3.3 C2F Partitioning for Computer Vision
We now adapt the general color passing algorithm to MRFs
for CV problems. Unfortunately, unary potentials make color
passing highly ineffective. Different pixels have different
RGB values and intensities, leading to almost every pixel get-
ting a different unary potential. Naive application of color
passing splits almost all variables into their own partitions,
and lifting offers little value.

A natural approximation is to deﬁne a threshold, such that
two unary potentials within that threshold be initialized with
the same color. Our experiments show limited success with
this scheme because because two pixels may have the same
label even when their actual unary potentials are very differ-
ent. What is more important is relative importance given to
each label than the actual potential value.

In response, we adapt color passing for CV by initializing
it as before, but with one key change: we initialize two unary
potential nodes with the same color if their lowest energy la-
bels have the same order for the top NL labels (we call this
unary split threshold). Experiments reveal that this approxi-
mation leads to effective partitions for lifted inference.

Finally, we can easily construct a sequence of coarse-to-
ﬁne partitions in the natural course of color passing’s execu-
tion – every iteration of color passing creates a ﬁner partition.
Moreover, as an alternative approach, we may also increase
NL. In our implementations, we intersperse the two, i.e., be-
fore every next step we pick one of two choices: either, we
run another iteration of color passing; or, we increase NL by
one, and split each variable partition based on the N th
L lowest
energy labels of its constituent variables.

We parameterize CP (NL, Niter) to denote the partition
from the current state of color passing, which has been run
till Niter iterations and unary split threshold is NL.
It
is easy to prove that another iteration of color passing or
splitting by increasing NL as above leads to a ﬁner par-
I.e., CP (NL, Niter) (cid:22) CP (NL + 1, Niter) and
tition.
CP (NL, Niter) (cid:22) CP (NL, Niter + 1). We refer to each
element of a partition of variables as a lifted pixel, since it is
a subset of pixels.

4 Lifted Inference for Stereo Matching
We ﬁrst demonstrate the value of lifted inference in the con-
text of stereo matching [Scharstein and Szeliski, 2002].
It
aims to ﬁnd pixel correspondences in a set of images of the
same scene, which can be used to further estimate the 3D
scene. Formally, two images I l and I r corresponding to
images of the scene from a left camera and a right cam-
era are taken such that both cameras are at same horizon-
tal level. The goal is to compute a disparity labeling Dl
for every pixel X = (a, b) such that I l[a][b] corresponds to
I r[a−Dl[a][b]][b]. We build a lifted version of TSGO [Moze-
rov and van de Weijer, 2015], as it is MRF-based and ranks
2nd on the Middlebury Stereo Evaluation Version 2 leader-
board.3
Background on TSGO: TSGO treats stereo matching as
a two-step energy minimization, where the ﬁrst step is on a

3http://vision.middlebury.edu/stereo/eval/

(a)

(b)

(c)

Figure 1: (a) Average (normalized) energy vs. inference time (b) Average pixel error vs. time. C2F TSGO achieves roughly 60% reduction in
time for reaching the optima. It has best anytime performance compared to vanilla TSGO and static lifted versions. (c) Average (normalized)
energy vs. time for different thresholding values and CP partitions. Plots with the same marker have MRFs of similar sizes.

(a)

(b)

(c)

(d)

(e)

Figure 2: Qualitative results for Doll image at convergence. C2F-TSGO is similar to base TSGO.(a) Left and Right Images (b) Ground
Truth (c) Disparity Map by TSGO (d) Disparity Map by C2F TSGO (e) Each colored region (other than black) is one among the 10 largest
partition elements from CP(1,1). Each color represents one partition element. Partition elements form non-contiguous regions

fully connected MRF with pairwise potentials and the second
is on a conventional locally connected MRF. Lack of space
precludes a detailed description of the ﬁrst step. At the high
level, TSGO runs one iteration of message passing on fully
connected MRF, computes marginals of each pixel X, which
act as unary potentials φ(X) for the MRF of second step.

The pairwise potential ψ used in step two is ψ(X, X (cid:48)) =
w(X, X (cid:48))ϕ(X, X (cid:48)), where ϕ(X, X (cid:48)) is a truncated linear
function of (cid:107)X − X (cid:48)(cid:107), and w(X, X (cid:48)) takes one of three dis-
tinct values depending on color difference between pixels.
The MAP assignment xMAP computes the lowest energy as-
signment of disparities Dl for every pixel for this MRF.
Lifted TSGO: Since step two is costlier, we build its lifted
version as discussed in previous section. For color passing,
two unary potential nodes are initialized with the same color
if their lowest energy labels exactly match (NL = 1). Other
initializations are consistent with original color passing for
general MRFs. A sequence of coarse-to-ﬁne models is out-
putted as per Section 3.3. C2F TSGO uses outputs from the
sequence CP (1, 1), CP (2, 1), CP (3, 1) and then reﬁnes to
the original MRF. Model reﬁnement is triggered whenever
energy hasn’t decreased in the last four iterations of alpha ex-
pansion (this becomes the stopping criteria C in Algorithm
1).
Experiments: Our experiments build on top of the exist-
ing TSGO implementation4, but we change the minimization
algorithm in step two to alpha expansion fusion [Lempitsky
et al., 2010] from OpenGM2 library [Andres et al., 2010;
Kappes et al., 2015], as it improves the speed of the base

4http://www.cvc.uab.es/∼mozerov/Stereo/

implementation. We use the benchmark Middlebury Stereo
datasets of 2003, 2005 and 2006 [Scharstein and Szeliski,
2003; Hirschmuller and Scharstein, 2007]. For the 2003
dataset, quarter-size images are used and for others, third-size
images are used. The label space is of size 85 (85 distinct dis-
parity labels).

We

our

compare

coarse-to-ﬁne

TSGO (using
CP (NL, Niter) partitions) against vanilla TSGO. Fig-
ures 1(a,b) show the aggregate plots of energy (and error)
vs.
time. We observe that C2F TSGO reaches the same
optima as TSGO, but in less than half the time. It has a much
superior anytime performance – if inference time is given as
a deadline, C2F TSGO obtains 59.59% less error on average
over randomly sampled deadlines. We also eyeball the out-
puts of C2F TSGO and TSGO and ﬁnd them to be visually
similar. Figure 2 shows a sample qualitative comparison.
Figure 2(e) shows ﬁve of the ten largest partition elements in
the partition from CP (1, 1). Clearly, the partition elements
formed are not contiguous, and seem to capture variables that
are likely to get the same assignment. This underscores the
value of our lifting framework for CV problems.

We also compare our CP (NL, Niter) partitioning strat-
egy with threshold partitioning discussed in Section 3.3. We
merge two pixels in thresholding scheme if the L1-norm dis-
tance of their unary potentials is less than a threshold. For
each partition induced by our approach, we ﬁnd a value of
threshold that has roughly the same number of lifted pix-
els. Figure 1(c) shows that partitions based on CP (1, 1) and
CP (3, 1) converges to a much lower energy quickly com-
pared to the corresponding threshold values (T hr = 50 and
T hr = 1 respectively). For CP (2, 1), convergence is slower

compared to corresponding threshold (T hr = 5) but eventu-
ally CP (2, 1) has signiﬁcantly better quality.

5 Lifted Inference for Image Segmentation
We now demonstrate the general nature of our lifted CV
framework by applying it to a second task. We choose multi-
label interactive image segmentation, where the goal is to
segment an image I based on a seed labeling (true labels
for a few pixels) provided as input. Like many other CV
problems, this also has an MRF-based solution, with the best
label-assignment generally obtained by MAP inference using
graph cuts or loopy belief propagation [Boykov et al., 2001;
Szeliski et al., 2008].

However, MRFs with only pairwise potentials are known to
suffer from short-boundary bias – they prefer segmentations
with shorter boundaries, because pairwise potentials penalize
every pair of boundary pixels. This leads to incorrect labeling
for sharp edge objects. Kohli et al. [2013] use CoGC, coop-
erative graph cuts [Jegelka and Bilmes, 2011], to develop one
of the best MRF-based solvers that overcome this bias.
Background on CoGC: Traditional MRFs linearly penalize
the number of label discontinuities at edges (boundary pixel
pairs), but CoGC penalizes the number of types of label dis-
continuities through the use of a concave energy function over
groups of ordered edges. It ﬁrst clusters all edges on the ba-
sis of color differences, and later applies a concave function
separately over the number of times a speciﬁc discontinuity
type is present in each edge group g ∈ G. Their carefully
engineered CoGC energy function is as follows:

E(x) =

φi(xi)+

w(x, x(cid:48)).I(x = l, x(cid:48) (cid:54)= l)

|X |
(cid:88)

i=1



F



(cid:88)

(cid:88)

(cid:88)

g∈G

l∈L

(x,x(cid:48))∈g





where unary potentials φ depend on colors of seed pixels, F
is a concave function, I the indicator function, and w(x, x(cid:48))
depends on the color difference between x, x(cid:48).
Intuitively,
F collects all edges with similar discontinuities and penal-
izes them sub-linearly, thus reducing the short-boundary bias
in the model. The usage of a concave function makes the
MRF higher order with cliques over edge groups. However,
the model is shown to reduce to a pairwise hierarchical MRF
through the addition of auxiliary variables.
Lifted CoGC: CoGC is lifted using the framework of Sec-
tion 3, with one additional change. We cluster edge groups
using color difference and the position of the edge. Edge
groups that are formed only on the basis of color difference
make the error of grouping different segment’s boundaries
into a single group. For e.g., it erroneously cluster boundaries
between white cow and grass, and sky and grass together in
the top image in Figure 3.

2 (cid:101), 2), CP ((cid:100) L

Coarse-to-ﬁne partitions are obtained by the method de-
scribed in Section 3.3. C2F CoGC uses outputs from the se-
quence CP ((cid:100) L
2 (cid:101), 3) before reﬁning to the orig-
inal MRF. Model reﬁnement is triggered if energy has not
reduced over the last |L| iterations.
Experiments: Our experiments use the implementation of
Cooperative Graph Cuts as provided by [Kohli et al., 2013].5

5Available at https://github.com/aosokin/coopCuts CVPR2013

Energy minimization is performed using alpha expansion
[Boykov et al., 2001]. The implementation of CoGC per-
forms a greedy descent on auxiliary variables while perform-
ing alpha expansion on the remaining variables, as described
in Kohli et. al. [2013]. The dataset used is provided with the
implementation. It is a part of the MSRC V2 dataset.6.

Figure 3 shows three individual energy vs. time plots. Re-
sults on other images are similar. We ﬁnd that C2F CoGC
algorithm converges to the same energy as CoGC in about
two-thirds the time on average. Overall, C2F CoGC achieves
a much better anytime performance than other lifted and un-
lifted CoGC.

Similar to Section 4, reﬁned partitions attain better quality
than coarser ones at the expense of time. Since the implemen-
tation performs a greedy descent over auxiliary variables, re-
ﬁnement of current partition also resets the auxiliary variables
to the last value that produced a change. Notice that energy
minimization on output of CP (2, 3) attains a lower energy
than on CP (3, 2). This observation drives our decision to
reﬁne by increasing Niter. Qualitatively, C2F CoGC pro-
duces the same labeling as CoGC. Finally, similar to stereo
matching, partitions based on thresholding scheme perform
signiﬁcantly worse compared to CP (NL, Niter) for image
segmentation as well.

6 Related Work

There is a large body of work on exact
lifting, both
marginal [Kersting et al., 2009; Gogate and Domingos, 2011;
Niepert, 2012; Mittal et al., 2015] and MAP [Kersting
et al., 2009; Gogate and Domingos, 2011; Niepert, 2012;
Sarkhel et al., 2014; Mittal et al., 2014], which is not di-
rectly applicable to our setting. There is some recent work
on approximate lifting [Van den Broeck and Darwiche, 2013;
Venugopal and Gogate, 2014; Singla et al., 2014; Sarkhel et
al., 2015; Van den Broeck and Niepert, 2015] but it’s fo-
cus is on marginal inference whereas we are interested in
lifted MAP. Further, this work can’t handle a distinct unary
potential on every node. An exception is work by Bui et
al. [2012] which explicitly deals with lifting in presence of
distinct unary potentials. Unfortunately, they make a very
strong assumption of exchangeability in the absence of unar-
ies which does not hold true in our setting since each pixel
has its own unique neighborhood.

Work by Sarkhel et al. [2015] is probably the closest to our
work. They design a C2F hierarchy to cluster constants for
approximate lifted MAP inference in Markov logic. In con-
trast, we partition ground atoms in a PGM. Like other work
on approximate lifting, they can’t handle distinct unary po-
tentials. Furthermore, they assume that their theory is pro-
vided in a normal form, i.e., without evidence, which can be
a severe restriction for most practical applications. Kiddon &
Domingos [2011] also propose C2F inference for an underly-
ing Markov logic theory. They use a hierarchy of partitions
based on a pre-speciﬁed ontology. CV does not have any such

6Available

at

https://www.microsoft.com/en-us/research/

project/image-understanding/?from=http%3A%2F%2Fresearch.
microsoft.com%2Fvision%2Fcambridge%2Frecognition%2F

Figure 3: (a-c) Qualitative Results for Segmentation. C2F has quality similar to CoGC algorithm (a) Original Image (b) Segmentation by
CoGC (c) Segmentation by C2F CoGC (d) C2F CoGC has lower energy compared to CoGC and other lifted variant at all times

ontology available, and needs to discover partitions using the
PGM directly.

Nath & Domingos [2010] exploit (approximate) lifted in-
ference for video segmentation. They experiment on a spe-
ciﬁc video problem (different from ours), and they only com-
pare against vanilla BP. Their initial partitioning scheme is
similar to our thresholding approach, which does not work
well in our experiments.

In computer vision, a popular approach to reduce the com-
plexity of inference is to use superpixels [Achanta et al.,
2012; Van den Bergh et al., 2012]. Superpixels are obtained
by merging neighboring nodes that have similar character-
istics. All pixel nodes in the same superpixel are assigned
the same value during MAP inference. SLIC [Achanta et al.,
2012] is one of the most popular algorithms for discovering
superpixels. Our approach differs from SLIC in some signiﬁ-
cant ways. First, their superpixels are local in nature whereas
our algorithm can merge pixels that are far apart. This can
help in merging two disconnected regions of the same object
in a single lifted pixel. Second, they obtain superpixels inde-
pendent of the inference algorithm, whereas we tightly inte-
grate our lifting with the underlying inference algorithm. This
can potentially lead to discovery of better partitions; indeed,
this helped us tremendously in image segmentation. Third,
they do not provide a C2F version of their algorithm and
we did not ﬁnd it straightforward to extend their approach
to discover successively ﬁner partitions. There is some recent
work [Wei et al., 2016] which addresses last two of these
challenges by introducing a hierarchy of superpixels. In our
preliminary experiments, we found that SLIC and superpixel
hierarchy perform worse than our lifting approach. Perform-
ing more rigorous comparisons is a direction for future work.

7 Conclusion and Future Work
We develop a generic template for applying lifted inference
to structured output prediction tasks in computer vision. We
show that MRF-based CV algorithms can be lifted at different
levels of abstraction, leading to methods for coarse to ﬁne
inference over a sequence of lifted models. We test our ideas
on two different CV tasks of stereo matching and interactive
image segmentation. We ﬁnd that C2F lifting is vastly more
efﬁcient than unlifted algorithms on both tasks obtaining a
superior anytime performance, and without any loss in ﬁnal
solution quality. To the best of our knowledge, this is the ﬁrst
demonstration of lifted inference in conjunction with top of
the line task-speciﬁc algorithms. Although we restrict to CV
in this work, we believe that our ideas are general and can
be adapted to other domains such as NLP, and computational
biology. We plan to explore this in the future.

Acknowledgements
We thank anonymous reviewers for their comments and sug-
gestions. Ankit Anand is being supported by the TCS Re-
search Scholars Program. Mausam is being supported by
grants from Google and Bloomberg. Parag Singla is being
supported by a DARPA grant funded under the Explainable
AI (XAI) program. Both Mausam and Parag Singla are being
supported by the Visvesvaraya Young Faculty Fellowships by
Govt. of India. Any opinions, ﬁndings, conclusions or rec-
ommendations expressed in this paper are those of the authors
and do not necessarily reﬂect the views or ofﬁcial policies, ei-
ther expressed or implied, of the funding agencies.

References
[Achanta et al., 2012] R. Achanta, A. Shaji, K. Smith, A. Lucchi,
P. Fua, and S. Ssstrunk. SLIC Superpixels Compared to State-of-

the-Art Superpixel Methods. In PAMI, Nov 2012.

[Anand et al., 2016] A. Anand, A. Grover, Mausam, and P. Singla.
Contextual Symmetries in Probabilistic Graphical Models. In IJ-
CAI, 2016.

[Anand et al., 2017] A. Anand, R. Noothigattu, P. Singla, and
Mausam. Non-Count Symmetries in Boolean & Multi-Valued
Prob. Graphical Models. In AISTATS, 2017.

[Andres et al., 2010] B. Andres,

J. H. Kappes, U. K¨othe,
C. Schn¨orr, and F. A. Hamprecht. An Empirical Comparison of
Inference Algorithms for Graphical Models with Higher Order
Factors Using OpenGM. In Pattern Recognition. 2010.

[Blei et al., 2003] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet

Allocation. JMLR, 3, March 2003.

[Boykov et al., 2001] Y. Boykov, O. Veksler, and R. Zabih. Fast
In PAMI,

Approximate Energy Minimization via Graph Cuts.
23(11), November 2001.

[Braz et al., 2005] R. Braz, E. Amir, and D. Roth. Lifted First-

Order Probabilistic Inference. In IJCAI, 2005.

[Bui et al., 2012] H. Bui, T. Huynh, and R. De Salvo Braz. Exact
Lifted Inference with Distinct Soft Evidence on Every Object. In
AAAI, 2012.

[Bui et al., 2013] H. Bui, T. Huynh, and S. Riedel. Automorphism
Groups of Graphical Models and Lifted Variational Inference. In
UAI, 2013.

[Freeman et al., 2000] W. Freeman, E. Pasztor, and O. Carmichael.

Learning Low-Level Vision. In IJCV, 40, 2000.

[Friedman, 2004] N. Friedman. Inferring Cellular Networks using

Probabilistic Graphical Models. Science, 303, 2004.

[Hirschmuller and Scharstein, 2007] H.

and
Evaluation of Cost Functions for Stereo

Hirschmuller

D. Scharstein.
Matching. In CVPR, 2007.

[Jegelka and Bilmes, 2011] S. Jegelka and J. Bilmes. Submodu-
larity Beyond Submodular Energies: Coupling Edges in Graph
Cuts. In CVPR, 2011.

[Jernite et al., 2015] Y. Jernite, A. Rush, and D. Sontag. A Fast
Variational Approach for Learning Markov Random Field Lan-
guage Models. In ICML, 2015.

[Jha et al., 2010] A. Jha, V. Gogate, A. Meliou, and D. Suciu.
Lifted Inference Seen from the Other Side : The Tractable Fea-
tures. In NIPS, 2010.

[Kappes et al., 2015] J. Kappes, B. Andres, A. Hamprecht,
C. Schn¨orr, S. Nowozin, D. Batra, S. Kim, B. Kausler, T. Kr¨oger,
J. Lellmann, N. Komodakis, B. Savchynskyy, and C. Rother. A
Comparative Study of Modern Inference Techniques for Struc-
tured Discrete Energy Minimization Problems. In IJCV, 2015.
[Kersting et al., 2009] K. Kersting, B. Ahmadi, and S. Natarajan.

Counting Belief Propagation. In UAI, 2009.

[Kersting, 2012] K. Kersting. Lifted Probabilistic Inference.

In

ECAI, 2012.

[Kiddon and Domingos, 2011] C. Kiddon and P. Domingos.
Coarse-to-Fine Inference and Learning for First-Order Proba-
bilistic Models. In AAAI, 2011.

[Kimmig et al., 2015] A. Kimmig, L. Mihalkova, and L. Getoor.
Lifted Graphical Models: A Survey. Machine Learning, 2015.

[Kohli et al., 2013] P. Kohli, A. Osokin, and S. Jegelka. A Prin-
cipled Deep Random Field Model for Image Segmentation. In
CVPR, 2013.

[Lempitsky et al., 2010] V. Lempitsky, C. Rother, S. Roth, and
A. Blake. Fusion Moves for Markov Random Field Optimiza-
tion. In PAMI, Aug 2010.

[Mittal et al., 2014] H. Mittal, P. Goyal, V. Gogate, and P. Singla.
New Rules for Domain Independent Lifted MAP Inference. In
NIPS, 2014.

[Mittal et al., 2015] H. Mittal, A. Mahajan, V. Gogate, and
In NIPS,

P. Singla. Lifted Inference Rules With Constraints.
2015.

[Mladenov et al., 2014] M. Mladenov, K. Kersting, and A. Glober-
son. Efﬁcient Lifting of MAP LP Relaxations Using k-Locality.
In AISTATS, 2014.

[Mozerov and van de Weijer, 2015] M. G. Mozerov and J. van de
Weijer. Accurate Stereo Matching by Two-Step Energy Mini-
mization. IEEE Transactions on Image Processing, March 2015.

[Nath and Domingos, 2010] A. Nath and P. Domingos. Efﬁcient
Lifting for Online Probabilistic Inference. In AAAIWS, 2010.

[Nath and Domingos, 2016] A. Nath and P. Domingos. Learning
Tractable Probabilistic Models for Fault Localization. In AAAI,
2016.

[Niepert, 2012] M. Niepert. Markov Chains on Orbits of Permuta-

tion Groups. In UAI, 2012.

[Noessner et al., 2013] J. Noessner, M. Niepert, and H. Stucken-
schmidt. RockIt: Exploiting Parallelism and Symmetry for MAP
Inference in Statistical Relational Models. In AAAI, 2013.

[Poole, 2003] D. Poole. First-Order Probabilistic Inference. In IJ-

[Sarkhel et al., 2014] S. Sarkhel, D. Venugopal, P. Singla, and
V. Gogate. Lifted MAP inference for Markov logic networks.
In AISTATS, 2014.

[Sarkhel et al., 2015] S. Sarkhel, P. Singla, and V. Gogate. Fast

Lifted MAP Inference via Partitioning. In NIPS, 2015.

[Scharstein and Szeliski, 2002] D. Scharstein and R. Szeliski. A
Taxonomy and Evaluation of Dense Two-Frame Stereo Corre-
spondence Algorithms. In IJCV, 2002.

[Scharstein and Szeliski, 2003] D. Scharstein and R. Szeliski.
High-accuracy Stereo Depth Maps Using Structured Light.
In
CVPR, 2003.

[Singla and Domingos, 2008] P. Singla and P. Domingos. Lifted

First-Order Belief Propagation. In AAAI, 2008.

[Singla et al., 2014] P. Singla, A. Nath, and P. Domingos. Approx-
imate Lifting Techniques for Belief Propagation. In AAAI, 2014.

[Szeliski et al., 2008] R. Szeliski, R. Zabih, D. Scharstein, O. Vek-
sler, V. Kolmogorov, A. Agarwala, M. Tappen, and C. Rother. A
Comparative Study of Energy Minimization Methods for Markov
In PAMI, June
Random Fields with Smoothness-Based Priors.
2008.

[Van den Bergh et al., 2012] M. Van den Bergh, X. Boix, G. Roig,
B. de Capitani, and L. Van Gool. SEEDS: Superpixels Extracted
via Energy-Driven Sampling. In ECCV, 2012.

[Van den Broeck and Darwiche, 2013] G. Van den Broeck and
A. Darwiche. On the Complexity and Approximation of Binary
Evidence in Lifted Inference. In NIPS, 2013.

[Gogate and Domingos, 2011] V. Gogate and P. Domingos. Proba-

bilisitic Theorem Proving. In UAI, 2011.

CAI, 2003.

[Van den Broeck and Niepert, 2015] G. Van den Broeck and
Lifted Probabilistic Inference for Asymmetric

M. Niepert.
Graphical Models. In AAAI, 2015.

[Venugopal and Gogate, 2014] D. Venugopal

and V. Gogate.
Evidence-Based Clustering for Scalable Inference in Markov
Logic. In Joint ECML-KDD, 2014.

[Wei et al., 2016] X. Wei, Q. Yang, Y. Gong, M. Yang, and
N. Ahuja. Superpixel Hierarchy. CoRR, abs/1605.06325, 2016.

Coarse-to-Fine Lifted MAP Inference in Computer Vision

Haroun Habeeb and Ankit Anand and Mausam and Parag Singla
Indian Institute of Technology Delhi
haroun7@gmail.com and {ankit.anand,mausam,parags}@cse.iitd.ac.in

7
1
0
2
 
l
u
J
 
2
2
 
 
]

V
C
.
s
c
[
 
 
1
v
5
6
1
7
0
.
7
0
7
1
:
v
i
X
r
a

Abstract

There is a vast body of theoretical research on
lifted inference in probabilistic graphical models
(PGMs). However, few demonstrations exist where
lifting is applied in conjunction with top of the line
applied algorithms. We pursue the applicability
of lifted inference for computer vision (CV), with
the insight that a globally optimal (MAP) labeling
will likely have the same label for two symmetric
pixels. The success of our approach lies in efﬁ-
ciently handling a distinct unary potential on ev-
ery node (pixel), typical of CV applications. This
allows us to lift the large class of algorithms that
model a CV problem via PGM inference. We pro-
pose a generic template for coarse-to-ﬁne (C2F) in-
ference in CV, which progressively reﬁnes an ini-
tial coarsely lifted PGM for varying quality-time
trade-offs. We demonstrate the performance of
C2F inference by developing lifted versions of two
near state-of-the-art CV algorithms for stereo vi-
sion and interactive image segmentation. We ﬁnd
that, against ﬂat algorithms, the lifted versions have
a much superior anytime performance, without any
loss in ﬁnal solution quality.

1 Introduction
Lifted inference in probabilistic graphical models (PGMs)
refers to the set of the techniques that carry out inference
over groups of random variables (or states) that behave simi-
larly [Jha et al., 2010; Kimmig et al., 2015]. A vast body of
theoretical work develops a variety of lifted inference tech-
niques, both exact (e.g., [Poole, 2003; Braz et al., 2005;
Singla and Domingos, 2008; Kersting, 2012]) and approxi-
mate (e.g., [Singla et al., 2014; Van den Broeck and Niepert,
2015]). Most of these works develop technical ideas appli-
cable to generic subclasses of PGMs, and the accompanying
experiments are aimed at providing ﬁrst proofs of concepts.
However, little work exists on transferring these ideas to the
top domain-speciﬁc algorithms for real-world applications.

Algorithms for NLP, computational biology, and computer
vision (CV) problems make heavy use of PGM machinery
(e.g., [Blei et al., 2003; Friedman, 2004; Szeliski et al.,
2008]). But, they also include signiﬁcant problem-speciﬁc

insights to get high performance. Barring a handful of excep-
tions [Jernite et al., 2015; Nath and Domingos, 2016], lifted
inference hasn’t been applied directly to such algorithms.

We study the potential value of lifting to CV problems such
as image denoising, stereo vision, and image segmentation.
Most CV problems are structured output prediction tasks, typ-
ically assigning a label to each pixel. A large class of solu-
tions are PGM-based: they deﬁne a Markov Random Field
(MRF) that has each pixel as a node, with unary potential that
depends on pixel value, and pairwise neighborhood potentials
that favor similar labels to neighboring pixels.

We see three main challenges in applying existing lifted
inference literature to these problems. First, most existing al-
gorithms focus on computing marginals [Singla and Domin-
gos, 2008; Kersting et al., 2009; Gogate and Domingos, 2011;
Niepert, 2012; Anand et al., 2016; Anand et al., 2017] instead
of MAP inference. Second, among the algorithms perform-
ing lifted MAP [Noessner et al., 2013; Mladenov et al., 2014;
Sarkhel et al., 2014; Mittal et al., 2014], many of the algo-
rithms focus on exact lifting. This breaks the kind of symme-
tries we need to compute since different pixels may not have
exact same neighborhood. Third, the few algorithms that per-
form approximate lifting for MAP, e.g. [Sarkhel et al., 2015],
can’t handle a distinct unary potential on every node. This is
essential for our application since image pixels take ordinal
values in three channels.

In response, we develop an approximate lifted MAP infer-
ence algorithm which can effectively handle unary potentials.
We initialize our algorithm by merging together pixels hav-
ing the same order of top-k labels based on the unary po-
tential values. We then adapt an existing symmetry ﬁnding
algorithm [Kersting et al., 2009] to discover groupings which
also have similar neighborhoods. We refer to our groupings
as lifted pixels. We impose the constraint that all pixels in a
lifted pixel must be assigned the same label. Our approximate
lifting reduces the model size drastically leading to signiﬁ-
cant time savings. Unfortunately, such approximate lifting
could adversely impact solution quality. However, we vary
the degree of approximation in symmetry ﬁnding to output a
sequence of coarse-to-ﬁne models with varying quality-time
trade-offs. By switching between such models, we develop a
coarse-to-ﬁne (C2F) inference procedure applicable to many
CV problems.

We formalize these ideas in a novel template for using

lifted inference in CV. We test C2F lifted inference on two
problems: stereo matching and image segmentation. We start
with one of the best MRF-based solvers each for both prob-
lems – neither of these are vanilla MRF solvers. Mozerov
& Weijer [2015] use a two-way energy minimization to ef-
fectively handle occluded regions in stereo matching. Co-
operative cuts [Kohli et al., 2013] for image segmentation use
concave functions over a predeﬁned set of pixel pairs to cor-
rectly segment images with sharp edges. We implement C2F
inference on top of both these algorithms and ﬁnd that C2F
versions have a strong anytime behavior – given any amount
of inference time, they output a much higher quality (and are
never worse) than their unlifted counterparts, and don’t suffer
any loss in the ﬁnal quality. Overall, our contributions are:

1. We present an approximate lifted MAP algorithm that
can efﬁciently handle a large number of distinct unary
potentials.

2. We develop a novel template for applying lifted infer-
ence in structured prediction tasks in CV. We provide
methods that output progressively ﬁner approx. symme-
tries, leading to a C2F lifted inference procedure.

3. We implement C2F inference over a near state-of-the-
art stereo matching algorithm, and one of the best MRF-
based image segmentation algorithms. We release our
implementation for wider use by the community.1

4. We ﬁnd that C2F has a much superior anytime behav-
ior. For stereo matching it achieves 60% better quality
on average in time-constrained settings. For image seg-
mentation C2F reaches convergence in 33% less time.

2 Background
2.1 Computer Vision Problems as MRFs
Most computer vision problems are structured output predic-
tion problems and their PGM-based solutions often follow
similar formulations. They cast the tasks into the problem
of ﬁnding the lowest energy assignment over grid-structured
MRFs (denoted by G = (X , γ)). The random variables in
these MRFs are the set of pixels X in the input image. Given
a set of labels L : {1, 2, . . . , |L|}, the task of structured output
prediction is to label each pixel X with a label from L. The
MRFs have two kinds of potentials (γ) – unary and higher-
order. Unary potentials are deﬁned over each individual pixel,
and usually incorporate pixel intensity, color, and other pixel
features. Higher order potentials operate over cliques (pairs
or more) of neighboring pixels and typically express some
form of spatial homophily – “neighboring pixels are more
likely to have similar labels.” While the general PGM struc-
ture of various tasks are similar, the speciﬁc potential tables
and label spaces are task-dependent.

The goal is to ﬁnd the MAP assignment over this MRF,
which is equivalent to energy minimization (by deﬁning en-
ergy as negative log of potentials). We denote the negative
log of unary potentials by φ, and that of higher-order poten-
tials by ψ.2 Thus, energy of a complete assignment x ∈ L|X |

1https://github.com/dair-iitd/c2ﬁ4cv/
2In the interest of readability, we say ‘potential’ to mean ‘nega-

tive log of potential’ in the rest of the paper.

can be deﬁned as:

E(x) =

φ(xi) +

ψj(ˆxj)

(1)

(cid:88)

i∈1..|X |

(cid:88)

j

Here ˆxj denotes the assignment x restricted to the set of
variables in the potential ψj. And the output of the algorithm
is the assignment xMAP:

xMAP = arg min
x∈L|X |

E(x)

(2)

The problem is in general intractable. Efﬁcient approxi-
mations exploit special characteristics of potentials like sub-
modularity [Jegelka and Bilmes, 2011], or use variants of
graph cut or loopy belief propagation [Boykov et al., 2001;
Freeman et al., 2000].

2.2 Symmetries in Graphical Models
Lifting an algorithm often requires computing a set of sym-
metries that can be exploited by that algorithm. For PGMs,
two popular methods for symmetry computation are color
passing for computing symmetries of variables [Kersting et
al., 2009], and graph isomorphism for symmetries of states
[Niepert, 2012; Bui et al., 2013]. Since our work is based on
color passing, we explain it in more detail.

Color passing for an MRF operates over a colored bipar-
tite graph containing nodes for all variables and potentials,
and each node is assigned a color. The graph is initialized
as follows: all variables nodes get a common color; all po-
tential nodes with exactly same potential tables are assigned
a unique color. Now, in an iterative color passing scheme,
in each iteration, each variable node passes its color to all
neighboring potential nodes. The potential nodes store in-
coming color signatures in a vector, append their own color
to it, and send the vector back to variable nodes. The variable
nodes stack these incoming vectors. New colors are assigned
to each node based on the set of incoming messages such that
two nodes with same messages are assigned the same unique
color. This process is repeated until convergence, i.e., no fur-
ther change in colors.

A coloring of the bipartite graph deﬁnes a partition of vari-
able nodes such that all nodes of the same color form a par-
tition element. Each iteration of color passing creates suc-
cessively ﬁner partitions, since two variable nodes, once as-
signed different colors, can never get the same color.

3 Lifted Computer Vision Framework
In this section, we will describe our generic template which
can be used to lift a large class of vision applications includ-
ing those in stereo, segmentation etc. Our template can be
seen as transforming the original problem space to a reduced
problem space over which the original inference algorithm
can now be applied much more efﬁciently. Speciﬁcally, our
description in this section is entirely algorithm independent.
We will focus on MAP inference which is the inference
task of choice for most vision applications (refer Section 2).
The key insight in our formulation is based on the realiza-
tion that pixels which are involved in the same (or similar)

kinds of unary and higher order potentials, and have the same
(or similar) neighborhoods, are likely to have the same MAP
value. Therefore, if somehow we could discover such sets
of pixels a priori, we could explicitly enforce these pixels to
have the same value while searching for the solution, substan-
tially reducing the problem size and still preserving the opti-
mal MAP assignment(s). Since in general doing this exactly
may lead to a degenerate network, we do it approximately.
Hence trading-off speed for marginal loss in solution quality.
The loss in solution quality is offset by resorting to coarse-
to-ﬁne inference where we start with a crude approximation,
and gradually make it ﬁner, to guarantee optimality at the end
while still obtaining signiﬁcant gains. We next describe the
details of our approach.

3.1 Obtaining a Reduced Problem

1 , Y P

2 , · · · , Y P

k ⊆ X , Y P
k1

Consider an energy minimization problem over a PGM G =
(X , γ). Let L = {1, 2, · · · , |L|} denote the set of labels
over which variables in the set X can vary. Let Y P =
{Y P
r } denote a partition of X into r disjoint
∩ Y P
subsets, i.e., ∀k, Y P
= ∅ when k1 (cid:54)= k2,
k2
and ∪kY P
k = X . We refer to each Y P
k as a partition element.
Correspondingly, let us deﬁne Y = {Y1, Y2, · · · , Yr} as a set
of partition variables, where there is a one to one correspon-
dence between partition elements and the partition variables
and each partition variable Yk takes values in the set L. Let
part(Xi) denote the partition element to which Xi belongs.
Let ˆXj ⊆ X denote a subset of variables. We say that a par-
k is represented in the set ˆXj if ∃Xi ∈ ˆXj
tition element Y P
s.t. part(Xj) = Yk.
Given a subset of variables ˆXj, let γj( ˆXj) be a potential de-
ﬁned over ˆXj. Let ˆxj denote an assignment to variables in
the set ˆXj. Let ˆxj.elem(i) denote the value taken by a vari-
able Xi in ˆXj. We say that an assignment ˆXj = ˆxj respects
a partition Y P if the variables in ˆXj belonging to the same
partition element have the same label in ˆxj, i.e., part(Xi) =
part(Xi(cid:48)) ⇒ ˆxj.elem(i) = ˆxj.elem(i(cid:48)), ∀Xi, Xi(cid:48) ∈ ˆXj.
Next, we introduce the notion of a reduced potential.

Deﬁnition 3.1 Let X be a set of variables and let Y P de-
note its partition. Given the potential γj( ˆXj), the reduced
potential Γj is deﬁned to be the restriction of γj( ˆXj) to those
labeling assignments of ˆXj which respect the partition Y P .
Equivalently, we can deﬁne the reduced potential Γj( ˆYj) over
the set of partition variables ˆYj which are represented in the
set ˆXj.

For example, consider a potential γ(X1, X2, X3) deﬁned
over three Boolean variables. The table for γ would have
8 entries. Consider the partition Y P = {Y P
2 } where
1 = {X1, X2} and Y P
Y P
2 = {X3}. Then, the reduced poten-
tial Γ is the restriction of γ to those rows in the table where
X1 = X2. Hence Γ has four rows in its table and equivalently
can be thought of deﬁning a potential over the 4 possible com-
binations of Y1 and Y2 variables. We are now ready to deﬁne
a reduced graphical model.

1 , Y P

Deﬁnition 3.2 Let G = (X , γ) represent a PGM. Given a
partition Y P of X , the reduced graphical model G(Y, Γ) is
the graphical model deﬁned over the set of partition variables
Y such that every potential γj ∈ γ in G is replaced by the
corresponding reduced potential Γj ∈ Γ in G.
Let E(x) and E(y) denote the energies of the states x and y
in G and G, respectively. The following theorem relates the
energies of the states in the two graphical models.

Theorem 3.1 For every assignment y of Y in G, there is a
corresponding assignment x of X such that E(y) = E(x).

The theorem can be proved by noting that each potential
Γj( ˆYj) in G was obtained by restricting the original poten-
tial γj( ˆXj) to those assignments where variables in Xj be-
longing to the same partition took the same label. Since this
correspondence is true for every potential in the reduced set,
to obtain the desired state x, for every variable Xi ∈ X we
simply assign it the label of its partition in y.
Corollary 3.1 Let xMAP and yMAP be the MAP states (i.e.
having the minimum energy) for G and G, respectively. Then,
E(yMAP) ≥ E(xMAP).
The process of reduction can be seen as curtailing the en-
tire search space to those assignments where variables in the
same partition take the same label. A reduction in the prob-
lem space will lead to computational gains but might result
in loss of solution quality, where the solution quality can be
captured by the difference between E(yMAP) and E(xMAP).
Therefore, we need to trade-off the balance between the two.
Intuitively, a good problem reduction will keep those vari-
ables in the same partition which are likely to have the same
value in the optimal assignment for the original problem.
How do we ﬁnd such variables without actually solving the
inference task? We will describe one such technique in Sec-
tion 3.3.

There is another perspective.

Instead of solving one re-
duced problem, we can instead work with a series of reduced
problems which successively get closer to the optimal solu-
tion. The initial reductions are coarser and far from optimal,
but can be solved efﬁciently to quickly reach in the region
where the solution lies. The successive iterations can then re-
ﬁne the solution iteratively getting closer to the optimal. This
leads us to the coarse-to-ﬁne inference described next.

3.2 Coarse to Fine Inference
We will deﬁne a framework for C2F (coarse-to-ﬁne) infer-
ence so that we maintain the computational advantage while
still preserving optimality. In the following, for ease of no-
tation, we will drop the superscript P in Y p to denote the
partition of X . Therefore, Y will refer to both the partition as
well as the set of partition variables. Before we describe our
algorithm, let us start with some deﬁnitions.
Deﬁnition 3.3 Let Y and Y (cid:48) be two partitions of X . We
say that Y is coarser than Y (cid:48), denoted as Y (cid:22) Y (cid:48),
if
∀y(cid:48) ∈ Y (cid:48)∃y ∈ Y such that y(cid:48) ⊆ y. We equivalently say
that Y (cid:48) is ﬁner than Y.
It is easy to see that X deﬁnes a partition of itself which is the
ﬁnest among all partitions, i.e., ∀Y such that Y is a partition

of X , Y (cid:22) X . We also refer it to as the degenerate partition.
For ease of notation, we will denote the ﬁnest partition by
Y ∗ (same as X ). We will refer to the corresponding PGM as
G∗ (same as G). Next, we state a theorem which relates two
partitions with each other.
Lemma 1 Let Y and Y (cid:48) be two partitions of X such that Y (cid:22)
Y (cid:48). Then Y (cid:48) can be seen as a partition of the set Y.

The proof of this lemma is straightforward and is omitted
due to lack of space. Consider a set Y of coarse to ﬁne
partitions given as Y 0 (cid:22) Y 1, · · · , (cid:22), Y t, (cid:22), · · · , Y ∗. Let
Gt, E t, yt
M AP respectively denote the reduced problem, en-
ergy function and MAP assignment for the partition Y t. Us-
ing Lemma 1, Y t+1 is a partition of Y t. Then, using Theo-
rem 3.1, we have for every assignment yt to variables in Y t,
there is an assignment yt+1 to variables in Y t+1 such that
E t(yt) = E t+1(yt+1). Also, using Corollary 3.1, we have
∀t E t(yt
MAP). Together, these two state-
ments imply that starting from the coarsest partition, we can
gradually keep on improving the solution as we move to ﬁner
partitions.

MAP) ≥ E t+1(yt+1

Our C2F set-up assumes an iterative MAP inference algo-
rithm A which has the anytime property i.e., can produce so-
lutions of increasing quality with time. C2F Function (see
Algorithm 1) takes 3 inputs: a set of C2F partitions Y, infer-
ence algorithm A, and a stopping criteria C. The algorithm A
in turn takes three inputs: PGM Gt, starting assignment yt,
stopping criteria C. A outputs an approximation to the MAP
solution once the stopping criteria C is met. Starting with the
coarsest partition (t = 0 in line 2), a start state is picked for
the coarsest problem to be solved (line 3). In each iteration
(line 4), C2F ﬁnds the MAP estimate for the current problem
(Gt) using algorithm A (line 5). This solution is then mapped
to a same energy solution of the next ﬁner partition (line 6)
which becomes the starting state for the next run of A. The
solution is thus successively reﬁned in each iteration. The
process is repeated until we reach the ﬁnest level of partition.
In the end, A is run on the ﬁnest partition and the resultant
solution is output (lines 9,10). Since the last partition in the
set is the original problem G∗, optimality with respect to A is
guaranteed.

Next, we describe how to use the color passing algorithm
(Section 2) to get a series of partitions which get successively
ﬁner. Our C2F algorithm can then be applied on this set of
partitions to get anytime solutions of high quality while being
computationally efﬁcient.

Algorithm 1 Coarse-to-Fine Lifted MAP Algorithm
1:C2F Lifted MAP(C2F Partitions Y, Algo A,Criteria C)
2: t = 0; T = |Y|;
3: yt = getInitState(Gt);
4: While (t < T );
MAP = A(Gt, yt, C);
yt
5:
yt+1 = getEquivAssignment(Y t, Y t+1, yt
6:
t = t + 1;
7:
8: END While
9: yT
10: return yT

MAP = A(GT , yT , C);
MAP

MAP);

3.3 C2F Partitioning for Computer Vision
We now adapt the general color passing algorithm to MRFs
for CV problems. Unfortunately, unary potentials make color
passing highly ineffective. Different pixels have different
RGB values and intensities, leading to almost every pixel get-
ting a different unary potential. Naive application of color
passing splits almost all variables into their own partitions,
and lifting offers little value.

A natural approximation is to deﬁne a threshold, such that
two unary potentials within that threshold be initialized with
the same color. Our experiments show limited success with
this scheme because because two pixels may have the same
label even when their actual unary potentials are very differ-
ent. What is more important is relative importance given to
each label than the actual potential value.

In response, we adapt color passing for CV by initializing
it as before, but with one key change: we initialize two unary
potential nodes with the same color if their lowest energy la-
bels have the same order for the top NL labels (we call this
unary split threshold). Experiments reveal that this approxi-
mation leads to effective partitions for lifted inference.

Finally, we can easily construct a sequence of coarse-to-
ﬁne partitions in the natural course of color passing’s execu-
tion – every iteration of color passing creates a ﬁner partition.
Moreover, as an alternative approach, we may also increase
NL. In our implementations, we intersperse the two, i.e., be-
fore every next step we pick one of two choices: either, we
run another iteration of color passing; or, we increase NL by
one, and split each variable partition based on the N th
L lowest
energy labels of its constituent variables.

We parameterize CP (NL, Niter) to denote the partition
from the current state of color passing, which has been run
till Niter iterations and unary split threshold is NL.
It
is easy to prove that another iteration of color passing or
splitting by increasing NL as above leads to a ﬁner par-
I.e., CP (NL, Niter) (cid:22) CP (NL + 1, Niter) and
tition.
CP (NL, Niter) (cid:22) CP (NL, Niter + 1). We refer to each
element of a partition of variables as a lifted pixel, since it is
a subset of pixels.

4 Lifted Inference for Stereo Matching
We ﬁrst demonstrate the value of lifted inference in the con-
text of stereo matching [Scharstein and Szeliski, 2002].
It
aims to ﬁnd pixel correspondences in a set of images of the
same scene, which can be used to further estimate the 3D
scene. Formally, two images I l and I r corresponding to
images of the scene from a left camera and a right cam-
era are taken such that both cameras are at same horizon-
tal level. The goal is to compute a disparity labeling Dl
for every pixel X = (a, b) such that I l[a][b] corresponds to
I r[a−Dl[a][b]][b]. We build a lifted version of TSGO [Moze-
rov and van de Weijer, 2015], as it is MRF-based and ranks
2nd on the Middlebury Stereo Evaluation Version 2 leader-
board.3
Background on TSGO: TSGO treats stereo matching as
a two-step energy minimization, where the ﬁrst step is on a

3http://vision.middlebury.edu/stereo/eval/

(a)

(b)

(c)

Figure 1: (a) Average (normalized) energy vs. inference time (b) Average pixel error vs. time. C2F TSGO achieves roughly 60% reduction in
time for reaching the optima. It has best anytime performance compared to vanilla TSGO and static lifted versions. (c) Average (normalized)
energy vs. time for different thresholding values and CP partitions. Plots with the same marker have MRFs of similar sizes.

(a)

(b)

(c)

(d)

(e)

Figure 2: Qualitative results for Doll image at convergence. C2F-TSGO is similar to base TSGO.(a) Left and Right Images (b) Ground
Truth (c) Disparity Map by TSGO (d) Disparity Map by C2F TSGO (e) Each colored region (other than black) is one among the 10 largest
partition elements from CP(1,1). Each color represents one partition element. Partition elements form non-contiguous regions

fully connected MRF with pairwise potentials and the second
is on a conventional locally connected MRF. Lack of space
precludes a detailed description of the ﬁrst step. At the high
level, TSGO runs one iteration of message passing on fully
connected MRF, computes marginals of each pixel X, which
act as unary potentials φ(X) for the MRF of second step.

The pairwise potential ψ used in step two is ψ(X, X (cid:48)) =
w(X, X (cid:48))ϕ(X, X (cid:48)), where ϕ(X, X (cid:48)) is a truncated linear
function of (cid:107)X − X (cid:48)(cid:107), and w(X, X (cid:48)) takes one of three dis-
tinct values depending on color difference between pixels.
The MAP assignment xMAP computes the lowest energy as-
signment of disparities Dl for every pixel for this MRF.
Lifted TSGO: Since step two is costlier, we build its lifted
version as discussed in previous section. For color passing,
two unary potential nodes are initialized with the same color
if their lowest energy labels exactly match (NL = 1). Other
initializations are consistent with original color passing for
general MRFs. A sequence of coarse-to-ﬁne models is out-
putted as per Section 3.3. C2F TSGO uses outputs from the
sequence CP (1, 1), CP (2, 1), CP (3, 1) and then reﬁnes to
the original MRF. Model reﬁnement is triggered whenever
energy hasn’t decreased in the last four iterations of alpha ex-
pansion (this becomes the stopping criteria C in Algorithm
1).
Experiments: Our experiments build on top of the exist-
ing TSGO implementation4, but we change the minimization
algorithm in step two to alpha expansion fusion [Lempitsky
et al., 2010] from OpenGM2 library [Andres et al., 2010;
Kappes et al., 2015], as it improves the speed of the base

4http://www.cvc.uab.es/∼mozerov/Stereo/

implementation. We use the benchmark Middlebury Stereo
datasets of 2003, 2005 and 2006 [Scharstein and Szeliski,
2003; Hirschmuller and Scharstein, 2007]. For the 2003
dataset, quarter-size images are used and for others, third-size
images are used. The label space is of size 85 (85 distinct dis-
parity labels).

We

our

compare

coarse-to-ﬁne

TSGO (using
CP (NL, Niter) partitions) against vanilla TSGO. Fig-
ures 1(a,b) show the aggregate plots of energy (and error)
vs.
time. We observe that C2F TSGO reaches the same
optima as TSGO, but in less than half the time. It has a much
superior anytime performance – if inference time is given as
a deadline, C2F TSGO obtains 59.59% less error on average
over randomly sampled deadlines. We also eyeball the out-
puts of C2F TSGO and TSGO and ﬁnd them to be visually
similar. Figure 2 shows a sample qualitative comparison.
Figure 2(e) shows ﬁve of the ten largest partition elements in
the partition from CP (1, 1). Clearly, the partition elements
formed are not contiguous, and seem to capture variables that
are likely to get the same assignment. This underscores the
value of our lifting framework for CV problems.

We also compare our CP (NL, Niter) partitioning strat-
egy with threshold partitioning discussed in Section 3.3. We
merge two pixels in thresholding scheme if the L1-norm dis-
tance of their unary potentials is less than a threshold. For
each partition induced by our approach, we ﬁnd a value of
threshold that has roughly the same number of lifted pix-
els. Figure 1(c) shows that partitions based on CP (1, 1) and
CP (3, 1) converges to a much lower energy quickly com-
pared to the corresponding threshold values (T hr = 50 and
T hr = 1 respectively). For CP (2, 1), convergence is slower

compared to corresponding threshold (T hr = 5) but eventu-
ally CP (2, 1) has signiﬁcantly better quality.

5 Lifted Inference for Image Segmentation
We now demonstrate the general nature of our lifted CV
framework by applying it to a second task. We choose multi-
label interactive image segmentation, where the goal is to
segment an image I based on a seed labeling (true labels
for a few pixels) provided as input. Like many other CV
problems, this also has an MRF-based solution, with the best
label-assignment generally obtained by MAP inference using
graph cuts or loopy belief propagation [Boykov et al., 2001;
Szeliski et al., 2008].

However, MRFs with only pairwise potentials are known to
suffer from short-boundary bias – they prefer segmentations
with shorter boundaries, because pairwise potentials penalize
every pair of boundary pixels. This leads to incorrect labeling
for sharp edge objects. Kohli et al. [2013] use CoGC, coop-
erative graph cuts [Jegelka and Bilmes, 2011], to develop one
of the best MRF-based solvers that overcome this bias.
Background on CoGC: Traditional MRFs linearly penalize
the number of label discontinuities at edges (boundary pixel
pairs), but CoGC penalizes the number of types of label dis-
continuities through the use of a concave energy function over
groups of ordered edges. It ﬁrst clusters all edges on the ba-
sis of color differences, and later applies a concave function
separately over the number of times a speciﬁc discontinuity
type is present in each edge group g ∈ G. Their carefully
engineered CoGC energy function is as follows:

E(x) =

φi(xi)+

w(x, x(cid:48)).I(x = l, x(cid:48) (cid:54)= l)

|X |
(cid:88)

i=1



F



(cid:88)

(cid:88)

(cid:88)

g∈G

l∈L

(x,x(cid:48))∈g





where unary potentials φ depend on colors of seed pixels, F
is a concave function, I the indicator function, and w(x, x(cid:48))
depends on the color difference between x, x(cid:48).
Intuitively,
F collects all edges with similar discontinuities and penal-
izes them sub-linearly, thus reducing the short-boundary bias
in the model. The usage of a concave function makes the
MRF higher order with cliques over edge groups. However,
the model is shown to reduce to a pairwise hierarchical MRF
through the addition of auxiliary variables.
Lifted CoGC: CoGC is lifted using the framework of Sec-
tion 3, with one additional change. We cluster edge groups
using color difference and the position of the edge. Edge
groups that are formed only on the basis of color difference
make the error of grouping different segment’s boundaries
into a single group. For e.g., it erroneously cluster boundaries
between white cow and grass, and sky and grass together in
the top image in Figure 3.

2 (cid:101), 2), CP ((cid:100) L

Coarse-to-ﬁne partitions are obtained by the method de-
scribed in Section 3.3. C2F CoGC uses outputs from the se-
quence CP ((cid:100) L
2 (cid:101), 3) before reﬁning to the orig-
inal MRF. Model reﬁnement is triggered if energy has not
reduced over the last |L| iterations.
Experiments: Our experiments use the implementation of
Cooperative Graph Cuts as provided by [Kohli et al., 2013].5

5Available at https://github.com/aosokin/coopCuts CVPR2013

Energy minimization is performed using alpha expansion
[Boykov et al., 2001]. The implementation of CoGC per-
forms a greedy descent on auxiliary variables while perform-
ing alpha expansion on the remaining variables, as described
in Kohli et. al. [2013]. The dataset used is provided with the
implementation. It is a part of the MSRC V2 dataset.6.

Figure 3 shows three individual energy vs. time plots. Re-
sults on other images are similar. We ﬁnd that C2F CoGC
algorithm converges to the same energy as CoGC in about
two-thirds the time on average. Overall, C2F CoGC achieves
a much better anytime performance than other lifted and un-
lifted CoGC.

Similar to Section 4, reﬁned partitions attain better quality
than coarser ones at the expense of time. Since the implemen-
tation performs a greedy descent over auxiliary variables, re-
ﬁnement of current partition also resets the auxiliary variables
to the last value that produced a change. Notice that energy
minimization on output of CP (2, 3) attains a lower energy
than on CP (3, 2). This observation drives our decision to
reﬁne by increasing Niter. Qualitatively, C2F CoGC pro-
duces the same labeling as CoGC. Finally, similar to stereo
matching, partitions based on thresholding scheme perform
signiﬁcantly worse compared to CP (NL, Niter) for image
segmentation as well.

6 Related Work

There is a large body of work on exact
lifting, both
marginal [Kersting et al., 2009; Gogate and Domingos, 2011;
Niepert, 2012; Mittal et al., 2015] and MAP [Kersting
et al., 2009; Gogate and Domingos, 2011; Niepert, 2012;
Sarkhel et al., 2014; Mittal et al., 2014], which is not di-
rectly applicable to our setting. There is some recent work
on approximate lifting [Van den Broeck and Darwiche, 2013;
Venugopal and Gogate, 2014; Singla et al., 2014; Sarkhel et
al., 2015; Van den Broeck and Niepert, 2015] but it’s fo-
cus is on marginal inference whereas we are interested in
lifted MAP. Further, this work can’t handle a distinct unary
potential on every node. An exception is work by Bui et
al. [2012] which explicitly deals with lifting in presence of
distinct unary potentials. Unfortunately, they make a very
strong assumption of exchangeability in the absence of unar-
ies which does not hold true in our setting since each pixel
has its own unique neighborhood.

Work by Sarkhel et al. [2015] is probably the closest to our
work. They design a C2F hierarchy to cluster constants for
approximate lifted MAP inference in Markov logic. In con-
trast, we partition ground atoms in a PGM. Like other work
on approximate lifting, they can’t handle distinct unary po-
tentials. Furthermore, they assume that their theory is pro-
vided in a normal form, i.e., without evidence, which can be
a severe restriction for most practical applications. Kiddon &
Domingos [2011] also propose C2F inference for an underly-
ing Markov logic theory. They use a hierarchy of partitions
based on a pre-speciﬁed ontology. CV does not have any such

6Available

at

https://www.microsoft.com/en-us/research/

project/image-understanding/?from=http%3A%2F%2Fresearch.
microsoft.com%2Fvision%2Fcambridge%2Frecognition%2F

Figure 3: (a-c) Qualitative Results for Segmentation. C2F has quality similar to CoGC algorithm (a) Original Image (b) Segmentation by
CoGC (c) Segmentation by C2F CoGC (d) C2F CoGC has lower energy compared to CoGC and other lifted variant at all times

ontology available, and needs to discover partitions using the
PGM directly.

Nath & Domingos [2010] exploit (approximate) lifted in-
ference for video segmentation. They experiment on a spe-
ciﬁc video problem (different from ours), and they only com-
pare against vanilla BP. Their initial partitioning scheme is
similar to our thresholding approach, which does not work
well in our experiments.

In computer vision, a popular approach to reduce the com-
plexity of inference is to use superpixels [Achanta et al.,
2012; Van den Bergh et al., 2012]. Superpixels are obtained
by merging neighboring nodes that have similar character-
istics. All pixel nodes in the same superpixel are assigned
the same value during MAP inference. SLIC [Achanta et al.,
2012] is one of the most popular algorithms for discovering
superpixels. Our approach differs from SLIC in some signiﬁ-
cant ways. First, their superpixels are local in nature whereas
our algorithm can merge pixels that are far apart. This can
help in merging two disconnected regions of the same object
in a single lifted pixel. Second, they obtain superpixels inde-
pendent of the inference algorithm, whereas we tightly inte-
grate our lifting with the underlying inference algorithm. This
can potentially lead to discovery of better partitions; indeed,
this helped us tremendously in image segmentation. Third,
they do not provide a C2F version of their algorithm and
we did not ﬁnd it straightforward to extend their approach
to discover successively ﬁner partitions. There is some recent
work [Wei et al., 2016] which addresses last two of these
challenges by introducing a hierarchy of superpixels. In our
preliminary experiments, we found that SLIC and superpixel
hierarchy perform worse than our lifting approach. Perform-
ing more rigorous comparisons is a direction for future work.

7 Conclusion and Future Work
We develop a generic template for applying lifted inference
to structured output prediction tasks in computer vision. We
show that MRF-based CV algorithms can be lifted at different
levels of abstraction, leading to methods for coarse to ﬁne
inference over a sequence of lifted models. We test our ideas
on two different CV tasks of stereo matching and interactive
image segmentation. We ﬁnd that C2F lifting is vastly more
efﬁcient than unlifted algorithms on both tasks obtaining a
superior anytime performance, and without any loss in ﬁnal
solution quality. To the best of our knowledge, this is the ﬁrst
demonstration of lifted inference in conjunction with top of
the line task-speciﬁc algorithms. Although we restrict to CV
in this work, we believe that our ideas are general and can
be adapted to other domains such as NLP, and computational
biology. We plan to explore this in the future.

Acknowledgements
We thank anonymous reviewers for their comments and sug-
gestions. Ankit Anand is being supported by the TCS Re-
search Scholars Program. Mausam is being supported by
grants from Google and Bloomberg. Parag Singla is being
supported by a DARPA grant funded under the Explainable
AI (XAI) program. Both Mausam and Parag Singla are being
supported by the Visvesvaraya Young Faculty Fellowships by
Govt. of India. Any opinions, ﬁndings, conclusions or rec-
ommendations expressed in this paper are those of the authors
and do not necessarily reﬂect the views or ofﬁcial policies, ei-
ther expressed or implied, of the funding agencies.

References
[Achanta et al., 2012] R. Achanta, A. Shaji, K. Smith, A. Lucchi,
P. Fua, and S. Ssstrunk. SLIC Superpixels Compared to State-of-

the-Art Superpixel Methods. In PAMI, Nov 2012.

[Anand et al., 2016] A. Anand, A. Grover, Mausam, and P. Singla.
Contextual Symmetries in Probabilistic Graphical Models. In IJ-
CAI, 2016.

[Anand et al., 2017] A. Anand, R. Noothigattu, P. Singla, and
Mausam. Non-Count Symmetries in Boolean & Multi-Valued
Prob. Graphical Models. In AISTATS, 2017.

[Andres et al., 2010] B. Andres,

J. H. Kappes, U. K¨othe,
C. Schn¨orr, and F. A. Hamprecht. An Empirical Comparison of
Inference Algorithms for Graphical Models with Higher Order
Factors Using OpenGM. In Pattern Recognition. 2010.

[Blei et al., 2003] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet

Allocation. JMLR, 3, March 2003.

[Boykov et al., 2001] Y. Boykov, O. Veksler, and R. Zabih. Fast
In PAMI,

Approximate Energy Minimization via Graph Cuts.
23(11), November 2001.

[Braz et al., 2005] R. Braz, E. Amir, and D. Roth. Lifted First-

Order Probabilistic Inference. In IJCAI, 2005.

[Bui et al., 2012] H. Bui, T. Huynh, and R. De Salvo Braz. Exact
Lifted Inference with Distinct Soft Evidence on Every Object. In
AAAI, 2012.

[Bui et al., 2013] H. Bui, T. Huynh, and S. Riedel. Automorphism
Groups of Graphical Models and Lifted Variational Inference. In
UAI, 2013.

[Freeman et al., 2000] W. Freeman, E. Pasztor, and O. Carmichael.

Learning Low-Level Vision. In IJCV, 40, 2000.

[Friedman, 2004] N. Friedman. Inferring Cellular Networks using

Probabilistic Graphical Models. Science, 303, 2004.

[Hirschmuller and Scharstein, 2007] H.

and
Evaluation of Cost Functions for Stereo

Hirschmuller

D. Scharstein.
Matching. In CVPR, 2007.

[Jegelka and Bilmes, 2011] S. Jegelka and J. Bilmes. Submodu-
larity Beyond Submodular Energies: Coupling Edges in Graph
Cuts. In CVPR, 2011.

[Jernite et al., 2015] Y. Jernite, A. Rush, and D. Sontag. A Fast
Variational Approach for Learning Markov Random Field Lan-
guage Models. In ICML, 2015.

[Jha et al., 2010] A. Jha, V. Gogate, A. Meliou, and D. Suciu.
Lifted Inference Seen from the Other Side : The Tractable Fea-
tures. In NIPS, 2010.

[Kappes et al., 2015] J. Kappes, B. Andres, A. Hamprecht,
C. Schn¨orr, S. Nowozin, D. Batra, S. Kim, B. Kausler, T. Kr¨oger,
J. Lellmann, N. Komodakis, B. Savchynskyy, and C. Rother. A
Comparative Study of Modern Inference Techniques for Struc-
tured Discrete Energy Minimization Problems. In IJCV, 2015.
[Kersting et al., 2009] K. Kersting, B. Ahmadi, and S. Natarajan.

Counting Belief Propagation. In UAI, 2009.

[Kersting, 2012] K. Kersting. Lifted Probabilistic Inference.

In

ECAI, 2012.

[Kiddon and Domingos, 2011] C. Kiddon and P. Domingos.
Coarse-to-Fine Inference and Learning for First-Order Proba-
bilistic Models. In AAAI, 2011.

[Kimmig et al., 2015] A. Kimmig, L. Mihalkova, and L. Getoor.
Lifted Graphical Models: A Survey. Machine Learning, 2015.

[Kohli et al., 2013] P. Kohli, A. Osokin, and S. Jegelka. A Prin-
cipled Deep Random Field Model for Image Segmentation. In
CVPR, 2013.

[Lempitsky et al., 2010] V. Lempitsky, C. Rother, S. Roth, and
A. Blake. Fusion Moves for Markov Random Field Optimiza-
tion. In PAMI, Aug 2010.

[Mittal et al., 2014] H. Mittal, P. Goyal, V. Gogate, and P. Singla.
New Rules for Domain Independent Lifted MAP Inference. In
NIPS, 2014.

[Mittal et al., 2015] H. Mittal, A. Mahajan, V. Gogate, and
In NIPS,

P. Singla. Lifted Inference Rules With Constraints.
2015.

[Mladenov et al., 2014] M. Mladenov, K. Kersting, and A. Glober-
son. Efﬁcient Lifting of MAP LP Relaxations Using k-Locality.
In AISTATS, 2014.

[Mozerov and van de Weijer, 2015] M. G. Mozerov and J. van de
Weijer. Accurate Stereo Matching by Two-Step Energy Mini-
mization. IEEE Transactions on Image Processing, March 2015.

[Nath and Domingos, 2010] A. Nath and P. Domingos. Efﬁcient
Lifting for Online Probabilistic Inference. In AAAIWS, 2010.

[Nath and Domingos, 2016] A. Nath and P. Domingos. Learning
Tractable Probabilistic Models for Fault Localization. In AAAI,
2016.

[Niepert, 2012] M. Niepert. Markov Chains on Orbits of Permuta-

tion Groups. In UAI, 2012.

[Noessner et al., 2013] J. Noessner, M. Niepert, and H. Stucken-
schmidt. RockIt: Exploiting Parallelism and Symmetry for MAP
Inference in Statistical Relational Models. In AAAI, 2013.

[Poole, 2003] D. Poole. First-Order Probabilistic Inference. In IJ-

[Sarkhel et al., 2014] S. Sarkhel, D. Venugopal, P. Singla, and
V. Gogate. Lifted MAP inference for Markov logic networks.
In AISTATS, 2014.

[Sarkhel et al., 2015] S. Sarkhel, P. Singla, and V. Gogate. Fast

Lifted MAP Inference via Partitioning. In NIPS, 2015.

[Scharstein and Szeliski, 2002] D. Scharstein and R. Szeliski. A
Taxonomy and Evaluation of Dense Two-Frame Stereo Corre-
spondence Algorithms. In IJCV, 2002.

[Scharstein and Szeliski, 2003] D. Scharstein and R. Szeliski.
High-accuracy Stereo Depth Maps Using Structured Light.
In
CVPR, 2003.

[Singla and Domingos, 2008] P. Singla and P. Domingos. Lifted

First-Order Belief Propagation. In AAAI, 2008.

[Singla et al., 2014] P. Singla, A. Nath, and P. Domingos. Approx-
imate Lifting Techniques for Belief Propagation. In AAAI, 2014.

[Szeliski et al., 2008] R. Szeliski, R. Zabih, D. Scharstein, O. Vek-
sler, V. Kolmogorov, A. Agarwala, M. Tappen, and C. Rother. A
Comparative Study of Energy Minimization Methods for Markov
In PAMI, June
Random Fields with Smoothness-Based Priors.
2008.

[Van den Bergh et al., 2012] M. Van den Bergh, X. Boix, G. Roig,
B. de Capitani, and L. Van Gool. SEEDS: Superpixels Extracted
via Energy-Driven Sampling. In ECCV, 2012.

[Van den Broeck and Darwiche, 2013] G. Van den Broeck and
A. Darwiche. On the Complexity and Approximation of Binary
Evidence in Lifted Inference. In NIPS, 2013.

[Gogate and Domingos, 2011] V. Gogate and P. Domingos. Proba-

bilisitic Theorem Proving. In UAI, 2011.

CAI, 2003.

[Van den Broeck and Niepert, 2015] G. Van den Broeck and
Lifted Probabilistic Inference for Asymmetric

M. Niepert.
Graphical Models. In AAAI, 2015.

[Venugopal and Gogate, 2014] D. Venugopal

and V. Gogate.
Evidence-Based Clustering for Scalable Inference in Markov
Logic. In Joint ECML-KDD, 2014.

[Wei et al., 2016] X. Wei, Q. Yang, Y. Gong, M. Yang, and
N. Ahuja. Superpixel Hierarchy. CoRR, abs/1605.06325, 2016.

Coarse-to-Fine Lifted MAP Inference in Computer Vision

Haroun Habeeb and Ankit Anand and Mausam and Parag Singla
Indian Institute of Technology Delhi
haroun7@gmail.com and {ankit.anand,mausam,parags}@cse.iitd.ac.in

7
1
0
2
 
l
u
J
 
2
2
 
 
]

V
C
.
s
c
[
 
 
1
v
5
6
1
7
0
.
7
0
7
1
:
v
i
X
r
a

Abstract

There is a vast body of theoretical research on
lifted inference in probabilistic graphical models
(PGMs). However, few demonstrations exist where
lifting is applied in conjunction with top of the line
applied algorithms. We pursue the applicability
of lifted inference for computer vision (CV), with
the insight that a globally optimal (MAP) labeling
will likely have the same label for two symmetric
pixels. The success of our approach lies in efﬁ-
ciently handling a distinct unary potential on ev-
ery node (pixel), typical of CV applications. This
allows us to lift the large class of algorithms that
model a CV problem via PGM inference. We pro-
pose a generic template for coarse-to-ﬁne (C2F) in-
ference in CV, which progressively reﬁnes an ini-
tial coarsely lifted PGM for varying quality-time
trade-offs. We demonstrate the performance of
C2F inference by developing lifted versions of two
near state-of-the-art CV algorithms for stereo vi-
sion and interactive image segmentation. We ﬁnd
that, against ﬂat algorithms, the lifted versions have
a much superior anytime performance, without any
loss in ﬁnal solution quality.

1 Introduction
Lifted inference in probabilistic graphical models (PGMs)
refers to the set of the techniques that carry out inference
over groups of random variables (or states) that behave simi-
larly [Jha et al., 2010; Kimmig et al., 2015]. A vast body of
theoretical work develops a variety of lifted inference tech-
niques, both exact (e.g., [Poole, 2003; Braz et al., 2005;
Singla and Domingos, 2008; Kersting, 2012]) and approxi-
mate (e.g., [Singla et al., 2014; Van den Broeck and Niepert,
2015]). Most of these works develop technical ideas appli-
cable to generic subclasses of PGMs, and the accompanying
experiments are aimed at providing ﬁrst proofs of concepts.
However, little work exists on transferring these ideas to the
top domain-speciﬁc algorithms for real-world applications.

Algorithms for NLP, computational biology, and computer
vision (CV) problems make heavy use of PGM machinery
(e.g., [Blei et al., 2003; Friedman, 2004; Szeliski et al.,
2008]). But, they also include signiﬁcant problem-speciﬁc

insights to get high performance. Barring a handful of excep-
tions [Jernite et al., 2015; Nath and Domingos, 2016], lifted
inference hasn’t been applied directly to such algorithms.

We study the potential value of lifting to CV problems such
as image denoising, stereo vision, and image segmentation.
Most CV problems are structured output prediction tasks, typ-
ically assigning a label to each pixel. A large class of solu-
tions are PGM-based: they deﬁne a Markov Random Field
(MRF) that has each pixel as a node, with unary potential that
depends on pixel value, and pairwise neighborhood potentials
that favor similar labels to neighboring pixels.

We see three main challenges in applying existing lifted
inference literature to these problems. First, most existing al-
gorithms focus on computing marginals [Singla and Domin-
gos, 2008; Kersting et al., 2009; Gogate and Domingos, 2011;
Niepert, 2012; Anand et al., 2016; Anand et al., 2017] instead
of MAP inference. Second, among the algorithms perform-
ing lifted MAP [Noessner et al., 2013; Mladenov et al., 2014;
Sarkhel et al., 2014; Mittal et al., 2014], many of the algo-
rithms focus on exact lifting. This breaks the kind of symme-
tries we need to compute since different pixels may not have
exact same neighborhood. Third, the few algorithms that per-
form approximate lifting for MAP, e.g. [Sarkhel et al., 2015],
can’t handle a distinct unary potential on every node. This is
essential for our application since image pixels take ordinal
values in three channels.

In response, we develop an approximate lifted MAP infer-
ence algorithm which can effectively handle unary potentials.
We initialize our algorithm by merging together pixels hav-
ing the same order of top-k labels based on the unary po-
tential values. We then adapt an existing symmetry ﬁnding
algorithm [Kersting et al., 2009] to discover groupings which
also have similar neighborhoods. We refer to our groupings
as lifted pixels. We impose the constraint that all pixels in a
lifted pixel must be assigned the same label. Our approximate
lifting reduces the model size drastically leading to signiﬁ-
cant time savings. Unfortunately, such approximate lifting
could adversely impact solution quality. However, we vary
the degree of approximation in symmetry ﬁnding to output a
sequence of coarse-to-ﬁne models with varying quality-time
trade-offs. By switching between such models, we develop a
coarse-to-ﬁne (C2F) inference procedure applicable to many
CV problems.

We formalize these ideas in a novel template for using

lifted inference in CV. We test C2F lifted inference on two
problems: stereo matching and image segmentation. We start
with one of the best MRF-based solvers each for both prob-
lems – neither of these are vanilla MRF solvers. Mozerov
& Weijer [2015] use a two-way energy minimization to ef-
fectively handle occluded regions in stereo matching. Co-
operative cuts [Kohli et al., 2013] for image segmentation use
concave functions over a predeﬁned set of pixel pairs to cor-
rectly segment images with sharp edges. We implement C2F
inference on top of both these algorithms and ﬁnd that C2F
versions have a strong anytime behavior – given any amount
of inference time, they output a much higher quality (and are
never worse) than their unlifted counterparts, and don’t suffer
any loss in the ﬁnal quality. Overall, our contributions are:

1. We present an approximate lifted MAP algorithm that
can efﬁciently handle a large number of distinct unary
potentials.

2. We develop a novel template for applying lifted infer-
ence in structured prediction tasks in CV. We provide
methods that output progressively ﬁner approx. symme-
tries, leading to a C2F lifted inference procedure.

3. We implement C2F inference over a near state-of-the-
art stereo matching algorithm, and one of the best MRF-
based image segmentation algorithms. We release our
implementation for wider use by the community.1

4. We ﬁnd that C2F has a much superior anytime behav-
ior. For stereo matching it achieves 60% better quality
on average in time-constrained settings. For image seg-
mentation C2F reaches convergence in 33% less time.

2 Background
2.1 Computer Vision Problems as MRFs
Most computer vision problems are structured output predic-
tion problems and their PGM-based solutions often follow
similar formulations. They cast the tasks into the problem
of ﬁnding the lowest energy assignment over grid-structured
MRFs (denoted by G = (X , γ)). The random variables in
these MRFs are the set of pixels X in the input image. Given
a set of labels L : {1, 2, . . . , |L|}, the task of structured output
prediction is to label each pixel X with a label from L. The
MRFs have two kinds of potentials (γ) – unary and higher-
order. Unary potentials are deﬁned over each individual pixel,
and usually incorporate pixel intensity, color, and other pixel
features. Higher order potentials operate over cliques (pairs
or more) of neighboring pixels and typically express some
form of spatial homophily – “neighboring pixels are more
likely to have similar labels.” While the general PGM struc-
ture of various tasks are similar, the speciﬁc potential tables
and label spaces are task-dependent.

The goal is to ﬁnd the MAP assignment over this MRF,
which is equivalent to energy minimization (by deﬁning en-
ergy as negative log of potentials). We denote the negative
log of unary potentials by φ, and that of higher-order poten-
tials by ψ.2 Thus, energy of a complete assignment x ∈ L|X |

1https://github.com/dair-iitd/c2ﬁ4cv/
2In the interest of readability, we say ‘potential’ to mean ‘nega-

tive log of potential’ in the rest of the paper.

can be deﬁned as:

E(x) =

φ(xi) +

ψj(ˆxj)

(1)

(cid:88)

i∈1..|X |

(cid:88)

j

Here ˆxj denotes the assignment x restricted to the set of
variables in the potential ψj. And the output of the algorithm
is the assignment xMAP:

xMAP = arg min
x∈L|X |

E(x)

(2)

The problem is in general intractable. Efﬁcient approxi-
mations exploit special characteristics of potentials like sub-
modularity [Jegelka and Bilmes, 2011], or use variants of
graph cut or loopy belief propagation [Boykov et al., 2001;
Freeman et al., 2000].

2.2 Symmetries in Graphical Models
Lifting an algorithm often requires computing a set of sym-
metries that can be exploited by that algorithm. For PGMs,
two popular methods for symmetry computation are color
passing for computing symmetries of variables [Kersting et
al., 2009], and graph isomorphism for symmetries of states
[Niepert, 2012; Bui et al., 2013]. Since our work is based on
color passing, we explain it in more detail.

Color passing for an MRF operates over a colored bipar-
tite graph containing nodes for all variables and potentials,
and each node is assigned a color. The graph is initialized
as follows: all variables nodes get a common color; all po-
tential nodes with exactly same potential tables are assigned
a unique color. Now, in an iterative color passing scheme,
in each iteration, each variable node passes its color to all
neighboring potential nodes. The potential nodes store in-
coming color signatures in a vector, append their own color
to it, and send the vector back to variable nodes. The variable
nodes stack these incoming vectors. New colors are assigned
to each node based on the set of incoming messages such that
two nodes with same messages are assigned the same unique
color. This process is repeated until convergence, i.e., no fur-
ther change in colors.

A coloring of the bipartite graph deﬁnes a partition of vari-
able nodes such that all nodes of the same color form a par-
tition element. Each iteration of color passing creates suc-
cessively ﬁner partitions, since two variable nodes, once as-
signed different colors, can never get the same color.

3 Lifted Computer Vision Framework
In this section, we will describe our generic template which
can be used to lift a large class of vision applications includ-
ing those in stereo, segmentation etc. Our template can be
seen as transforming the original problem space to a reduced
problem space over which the original inference algorithm
can now be applied much more efﬁciently. Speciﬁcally, our
description in this section is entirely algorithm independent.
We will focus on MAP inference which is the inference
task of choice for most vision applications (refer Section 2).
The key insight in our formulation is based on the realiza-
tion that pixels which are involved in the same (or similar)

kinds of unary and higher order potentials, and have the same
(or similar) neighborhoods, are likely to have the same MAP
value. Therefore, if somehow we could discover such sets
of pixels a priori, we could explicitly enforce these pixels to
have the same value while searching for the solution, substan-
tially reducing the problem size and still preserving the opti-
mal MAP assignment(s). Since in general doing this exactly
may lead to a degenerate network, we do it approximately.
Hence trading-off speed for marginal loss in solution quality.
The loss in solution quality is offset by resorting to coarse-
to-ﬁne inference where we start with a crude approximation,
and gradually make it ﬁner, to guarantee optimality at the end
while still obtaining signiﬁcant gains. We next describe the
details of our approach.

3.1 Obtaining a Reduced Problem

1 , Y P

2 , · · · , Y P

k ⊆ X , Y P
k1

Consider an energy minimization problem over a PGM G =
(X , γ). Let L = {1, 2, · · · , |L|} denote the set of labels
over which variables in the set X can vary. Let Y P =
{Y P
r } denote a partition of X into r disjoint
∩ Y P
subsets, i.e., ∀k, Y P
= ∅ when k1 (cid:54)= k2,
k2
and ∪kY P
k = X . We refer to each Y P
k as a partition element.
Correspondingly, let us deﬁne Y = {Y1, Y2, · · · , Yr} as a set
of partition variables, where there is a one to one correspon-
dence between partition elements and the partition variables
and each partition variable Yk takes values in the set L. Let
part(Xi) denote the partition element to which Xi belongs.
Let ˆXj ⊆ X denote a subset of variables. We say that a par-
k is represented in the set ˆXj if ∃Xi ∈ ˆXj
tition element Y P
s.t. part(Xj) = Yk.
Given a subset of variables ˆXj, let γj( ˆXj) be a potential de-
ﬁned over ˆXj. Let ˆxj denote an assignment to variables in
the set ˆXj. Let ˆxj.elem(i) denote the value taken by a vari-
able Xi in ˆXj. We say that an assignment ˆXj = ˆxj respects
a partition Y P if the variables in ˆXj belonging to the same
partition element have the same label in ˆxj, i.e., part(Xi) =
part(Xi(cid:48)) ⇒ ˆxj.elem(i) = ˆxj.elem(i(cid:48)), ∀Xi, Xi(cid:48) ∈ ˆXj.
Next, we introduce the notion of a reduced potential.

Deﬁnition 3.1 Let X be a set of variables and let Y P de-
note its partition. Given the potential γj( ˆXj), the reduced
potential Γj is deﬁned to be the restriction of γj( ˆXj) to those
labeling assignments of ˆXj which respect the partition Y P .
Equivalently, we can deﬁne the reduced potential Γj( ˆYj) over
the set of partition variables ˆYj which are represented in the
set ˆXj.

For example, consider a potential γ(X1, X2, X3) deﬁned
over three Boolean variables. The table for γ would have
8 entries. Consider the partition Y P = {Y P
2 } where
1 = {X1, X2} and Y P
Y P
2 = {X3}. Then, the reduced poten-
tial Γ is the restriction of γ to those rows in the table where
X1 = X2. Hence Γ has four rows in its table and equivalently
can be thought of deﬁning a potential over the 4 possible com-
binations of Y1 and Y2 variables. We are now ready to deﬁne
a reduced graphical model.

1 , Y P

Deﬁnition 3.2 Let G = (X , γ) represent a PGM. Given a
partition Y P of X , the reduced graphical model G(Y, Γ) is
the graphical model deﬁned over the set of partition variables
Y such that every potential γj ∈ γ in G is replaced by the
corresponding reduced potential Γj ∈ Γ in G.
Let E(x) and E(y) denote the energies of the states x and y
in G and G, respectively. The following theorem relates the
energies of the states in the two graphical models.

Theorem 3.1 For every assignment y of Y in G, there is a
corresponding assignment x of X such that E(y) = E(x).

The theorem can be proved by noting that each potential
Γj( ˆYj) in G was obtained by restricting the original poten-
tial γj( ˆXj) to those assignments where variables in Xj be-
longing to the same partition took the same label. Since this
correspondence is true for every potential in the reduced set,
to obtain the desired state x, for every variable Xi ∈ X we
simply assign it the label of its partition in y.
Corollary 3.1 Let xMAP and yMAP be the MAP states (i.e.
having the minimum energy) for G and G, respectively. Then,
E(yMAP) ≥ E(xMAP).
The process of reduction can be seen as curtailing the en-
tire search space to those assignments where variables in the
same partition take the same label. A reduction in the prob-
lem space will lead to computational gains but might result
in loss of solution quality, where the solution quality can be
captured by the difference between E(yMAP) and E(xMAP).
Therefore, we need to trade-off the balance between the two.
Intuitively, a good problem reduction will keep those vari-
ables in the same partition which are likely to have the same
value in the optimal assignment for the original problem.
How do we ﬁnd such variables without actually solving the
inference task? We will describe one such technique in Sec-
tion 3.3.

There is another perspective.

Instead of solving one re-
duced problem, we can instead work with a series of reduced
problems which successively get closer to the optimal solu-
tion. The initial reductions are coarser and far from optimal,
but can be solved efﬁciently to quickly reach in the region
where the solution lies. The successive iterations can then re-
ﬁne the solution iteratively getting closer to the optimal. This
leads us to the coarse-to-ﬁne inference described next.

3.2 Coarse to Fine Inference
We will deﬁne a framework for C2F (coarse-to-ﬁne) infer-
ence so that we maintain the computational advantage while
still preserving optimality. In the following, for ease of no-
tation, we will drop the superscript P in Y p to denote the
partition of X . Therefore, Y will refer to both the partition as
well as the set of partition variables. Before we describe our
algorithm, let us start with some deﬁnitions.
Deﬁnition 3.3 Let Y and Y (cid:48) be two partitions of X . We
say that Y is coarser than Y (cid:48), denoted as Y (cid:22) Y (cid:48),
if
∀y(cid:48) ∈ Y (cid:48)∃y ∈ Y such that y(cid:48) ⊆ y. We equivalently say
that Y (cid:48) is ﬁner than Y.
It is easy to see that X deﬁnes a partition of itself which is the
ﬁnest among all partitions, i.e., ∀Y such that Y is a partition

of X , Y (cid:22) X . We also refer it to as the degenerate partition.
For ease of notation, we will denote the ﬁnest partition by
Y ∗ (same as X ). We will refer to the corresponding PGM as
G∗ (same as G). Next, we state a theorem which relates two
partitions with each other.
Lemma 1 Let Y and Y (cid:48) be two partitions of X such that Y (cid:22)
Y (cid:48). Then Y (cid:48) can be seen as a partition of the set Y.

The proof of this lemma is straightforward and is omitted
due to lack of space. Consider a set Y of coarse to ﬁne
partitions given as Y 0 (cid:22) Y 1, · · · , (cid:22), Y t, (cid:22), · · · , Y ∗. Let
Gt, E t, yt
M AP respectively denote the reduced problem, en-
ergy function and MAP assignment for the partition Y t. Us-
ing Lemma 1, Y t+1 is a partition of Y t. Then, using Theo-
rem 3.1, we have for every assignment yt to variables in Y t,
there is an assignment yt+1 to variables in Y t+1 such that
E t(yt) = E t+1(yt+1). Also, using Corollary 3.1, we have
∀t E t(yt
MAP). Together, these two state-
ments imply that starting from the coarsest partition, we can
gradually keep on improving the solution as we move to ﬁner
partitions.

MAP) ≥ E t+1(yt+1

Our C2F set-up assumes an iterative MAP inference algo-
rithm A which has the anytime property i.e., can produce so-
lutions of increasing quality with time. C2F Function (see
Algorithm 1) takes 3 inputs: a set of C2F partitions Y, infer-
ence algorithm A, and a stopping criteria C. The algorithm A
in turn takes three inputs: PGM Gt, starting assignment yt,
stopping criteria C. A outputs an approximation to the MAP
solution once the stopping criteria C is met. Starting with the
coarsest partition (t = 0 in line 2), a start state is picked for
the coarsest problem to be solved (line 3). In each iteration
(line 4), C2F ﬁnds the MAP estimate for the current problem
(Gt) using algorithm A (line 5). This solution is then mapped
to a same energy solution of the next ﬁner partition (line 6)
which becomes the starting state for the next run of A. The
solution is thus successively reﬁned in each iteration. The
process is repeated until we reach the ﬁnest level of partition.
In the end, A is run on the ﬁnest partition and the resultant
solution is output (lines 9,10). Since the last partition in the
set is the original problem G∗, optimality with respect to A is
guaranteed.

Next, we describe how to use the color passing algorithm
(Section 2) to get a series of partitions which get successively
ﬁner. Our C2F algorithm can then be applied on this set of
partitions to get anytime solutions of high quality while being
computationally efﬁcient.

Algorithm 1 Coarse-to-Fine Lifted MAP Algorithm
1:C2F Lifted MAP(C2F Partitions Y, Algo A,Criteria C)
2: t = 0; T = |Y|;
3: yt = getInitState(Gt);
4: While (t < T );
MAP = A(Gt, yt, C);
yt
5:
yt+1 = getEquivAssignment(Y t, Y t+1, yt
6:
t = t + 1;
7:
8: END While
9: yT
10: return yT

MAP = A(GT , yT , C);
MAP

MAP);

3.3 C2F Partitioning for Computer Vision
We now adapt the general color passing algorithm to MRFs
for CV problems. Unfortunately, unary potentials make color
passing highly ineffective. Different pixels have different
RGB values and intensities, leading to almost every pixel get-
ting a different unary potential. Naive application of color
passing splits almost all variables into their own partitions,
and lifting offers little value.

A natural approximation is to deﬁne a threshold, such that
two unary potentials within that threshold be initialized with
the same color. Our experiments show limited success with
this scheme because because two pixels may have the same
label even when their actual unary potentials are very differ-
ent. What is more important is relative importance given to
each label than the actual potential value.

In response, we adapt color passing for CV by initializing
it as before, but with one key change: we initialize two unary
potential nodes with the same color if their lowest energy la-
bels have the same order for the top NL labels (we call this
unary split threshold). Experiments reveal that this approxi-
mation leads to effective partitions for lifted inference.

Finally, we can easily construct a sequence of coarse-to-
ﬁne partitions in the natural course of color passing’s execu-
tion – every iteration of color passing creates a ﬁner partition.
Moreover, as an alternative approach, we may also increase
NL. In our implementations, we intersperse the two, i.e., be-
fore every next step we pick one of two choices: either, we
run another iteration of color passing; or, we increase NL by
one, and split each variable partition based on the N th
L lowest
energy labels of its constituent variables.

We parameterize CP (NL, Niter) to denote the partition
from the current state of color passing, which has been run
till Niter iterations and unary split threshold is NL.
It
is easy to prove that another iteration of color passing or
splitting by increasing NL as above leads to a ﬁner par-
I.e., CP (NL, Niter) (cid:22) CP (NL + 1, Niter) and
tition.
CP (NL, Niter) (cid:22) CP (NL, Niter + 1). We refer to each
element of a partition of variables as a lifted pixel, since it is
a subset of pixels.

4 Lifted Inference for Stereo Matching
We ﬁrst demonstrate the value of lifted inference in the con-
text of stereo matching [Scharstein and Szeliski, 2002].
It
aims to ﬁnd pixel correspondences in a set of images of the
same scene, which can be used to further estimate the 3D
scene. Formally, two images I l and I r corresponding to
images of the scene from a left camera and a right cam-
era are taken such that both cameras are at same horizon-
tal level. The goal is to compute a disparity labeling Dl
for every pixel X = (a, b) such that I l[a][b] corresponds to
I r[a−Dl[a][b]][b]. We build a lifted version of TSGO [Moze-
rov and van de Weijer, 2015], as it is MRF-based and ranks
2nd on the Middlebury Stereo Evaluation Version 2 leader-
board.3
Background on TSGO: TSGO treats stereo matching as
a two-step energy minimization, where the ﬁrst step is on a

3http://vision.middlebury.edu/stereo/eval/

(a)

(b)

(c)

Figure 1: (a) Average (normalized) energy vs. inference time (b) Average pixel error vs. time. C2F TSGO achieves roughly 60% reduction in
time for reaching the optima. It has best anytime performance compared to vanilla TSGO and static lifted versions. (c) Average (normalized)
energy vs. time for different thresholding values and CP partitions. Plots with the same marker have MRFs of similar sizes.

(a)

(b)

(c)

(d)

(e)

Figure 2: Qualitative results for Doll image at convergence. C2F-TSGO is similar to base TSGO.(a) Left and Right Images (b) Ground
Truth (c) Disparity Map by TSGO (d) Disparity Map by C2F TSGO (e) Each colored region (other than black) is one among the 10 largest
partition elements from CP(1,1). Each color represents one partition element. Partition elements form non-contiguous regions

fully connected MRF with pairwise potentials and the second
is on a conventional locally connected MRF. Lack of space
precludes a detailed description of the ﬁrst step. At the high
level, TSGO runs one iteration of message passing on fully
connected MRF, computes marginals of each pixel X, which
act as unary potentials φ(X) for the MRF of second step.

The pairwise potential ψ used in step two is ψ(X, X (cid:48)) =
w(X, X (cid:48))ϕ(X, X (cid:48)), where ϕ(X, X (cid:48)) is a truncated linear
function of (cid:107)X − X (cid:48)(cid:107), and w(X, X (cid:48)) takes one of three dis-
tinct values depending on color difference between pixels.
The MAP assignment xMAP computes the lowest energy as-
signment of disparities Dl for every pixel for this MRF.
Lifted TSGO: Since step two is costlier, we build its lifted
version as discussed in previous section. For color passing,
two unary potential nodes are initialized with the same color
if their lowest energy labels exactly match (NL = 1). Other
initializations are consistent with original color passing for
general MRFs. A sequence of coarse-to-ﬁne models is out-
putted as per Section 3.3. C2F TSGO uses outputs from the
sequence CP (1, 1), CP (2, 1), CP (3, 1) and then reﬁnes to
the original MRF. Model reﬁnement is triggered whenever
energy hasn’t decreased in the last four iterations of alpha ex-
pansion (this becomes the stopping criteria C in Algorithm
1).
Experiments: Our experiments build on top of the exist-
ing TSGO implementation4, but we change the minimization
algorithm in step two to alpha expansion fusion [Lempitsky
et al., 2010] from OpenGM2 library [Andres et al., 2010;
Kappes et al., 2015], as it improves the speed of the base

4http://www.cvc.uab.es/∼mozerov/Stereo/

implementation. We use the benchmark Middlebury Stereo
datasets of 2003, 2005 and 2006 [Scharstein and Szeliski,
2003; Hirschmuller and Scharstein, 2007]. For the 2003
dataset, quarter-size images are used and for others, third-size
images are used. The label space is of size 85 (85 distinct dis-
parity labels).

We

our

compare

coarse-to-ﬁne

TSGO (using
CP (NL, Niter) partitions) against vanilla TSGO. Fig-
ures 1(a,b) show the aggregate plots of energy (and error)
vs.
time. We observe that C2F TSGO reaches the same
optima as TSGO, but in less than half the time. It has a much
superior anytime performance – if inference time is given as
a deadline, C2F TSGO obtains 59.59% less error on average
over randomly sampled deadlines. We also eyeball the out-
puts of C2F TSGO and TSGO and ﬁnd them to be visually
similar. Figure 2 shows a sample qualitative comparison.
Figure 2(e) shows ﬁve of the ten largest partition elements in
the partition from CP (1, 1). Clearly, the partition elements
formed are not contiguous, and seem to capture variables that
are likely to get the same assignment. This underscores the
value of our lifting framework for CV problems.

We also compare our CP (NL, Niter) partitioning strat-
egy with threshold partitioning discussed in Section 3.3. We
merge two pixels in thresholding scheme if the L1-norm dis-
tance of their unary potentials is less than a threshold. For
each partition induced by our approach, we ﬁnd a value of
threshold that has roughly the same number of lifted pix-
els. Figure 1(c) shows that partitions based on CP (1, 1) and
CP (3, 1) converges to a much lower energy quickly com-
pared to the corresponding threshold values (T hr = 50 and
T hr = 1 respectively). For CP (2, 1), convergence is slower

compared to corresponding threshold (T hr = 5) but eventu-
ally CP (2, 1) has signiﬁcantly better quality.

5 Lifted Inference for Image Segmentation
We now demonstrate the general nature of our lifted CV
framework by applying it to a second task. We choose multi-
label interactive image segmentation, where the goal is to
segment an image I based on a seed labeling (true labels
for a few pixels) provided as input. Like many other CV
problems, this also has an MRF-based solution, with the best
label-assignment generally obtained by MAP inference using
graph cuts or loopy belief propagation [Boykov et al., 2001;
Szeliski et al., 2008].

However, MRFs with only pairwise potentials are known to
suffer from short-boundary bias – they prefer segmentations
with shorter boundaries, because pairwise potentials penalize
every pair of boundary pixels. This leads to incorrect labeling
for sharp edge objects. Kohli et al. [2013] use CoGC, coop-
erative graph cuts [Jegelka and Bilmes, 2011], to develop one
of the best MRF-based solvers that overcome this bias.
Background on CoGC: Traditional MRFs linearly penalize
the number of label discontinuities at edges (boundary pixel
pairs), but CoGC penalizes the number of types of label dis-
continuities through the use of a concave energy function over
groups of ordered edges. It ﬁrst clusters all edges on the ba-
sis of color differences, and later applies a concave function
separately over the number of times a speciﬁc discontinuity
type is present in each edge group g ∈ G. Their carefully
engineered CoGC energy function is as follows:

E(x) =

φi(xi)+

w(x, x(cid:48)).I(x = l, x(cid:48) (cid:54)= l)

|X |
(cid:88)

i=1



F



(cid:88)

(cid:88)

(cid:88)

g∈G

l∈L

(x,x(cid:48))∈g





where unary potentials φ depend on colors of seed pixels, F
is a concave function, I the indicator function, and w(x, x(cid:48))
depends on the color difference between x, x(cid:48).
Intuitively,
F collects all edges with similar discontinuities and penal-
izes them sub-linearly, thus reducing the short-boundary bias
in the model. The usage of a concave function makes the
MRF higher order with cliques over edge groups. However,
the model is shown to reduce to a pairwise hierarchical MRF
through the addition of auxiliary variables.
Lifted CoGC: CoGC is lifted using the framework of Sec-
tion 3, with one additional change. We cluster edge groups
using color difference and the position of the edge. Edge
groups that are formed only on the basis of color difference
make the error of grouping different segment’s boundaries
into a single group. For e.g., it erroneously cluster boundaries
between white cow and grass, and sky and grass together in
the top image in Figure 3.

2 (cid:101), 2), CP ((cid:100) L

Coarse-to-ﬁne partitions are obtained by the method de-
scribed in Section 3.3. C2F CoGC uses outputs from the se-
quence CP ((cid:100) L
2 (cid:101), 3) before reﬁning to the orig-
inal MRF. Model reﬁnement is triggered if energy has not
reduced over the last |L| iterations.
Experiments: Our experiments use the implementation of
Cooperative Graph Cuts as provided by [Kohli et al., 2013].5

5Available at https://github.com/aosokin/coopCuts CVPR2013

Energy minimization is performed using alpha expansion
[Boykov et al., 2001]. The implementation of CoGC per-
forms a greedy descent on auxiliary variables while perform-
ing alpha expansion on the remaining variables, as described
in Kohli et. al. [2013]. The dataset used is provided with the
implementation. It is a part of the MSRC V2 dataset.6.

Figure 3 shows three individual energy vs. time plots. Re-
sults on other images are similar. We ﬁnd that C2F CoGC
algorithm converges to the same energy as CoGC in about
two-thirds the time on average. Overall, C2F CoGC achieves
a much better anytime performance than other lifted and un-
lifted CoGC.

Similar to Section 4, reﬁned partitions attain better quality
than coarser ones at the expense of time. Since the implemen-
tation performs a greedy descent over auxiliary variables, re-
ﬁnement of current partition also resets the auxiliary variables
to the last value that produced a change. Notice that energy
minimization on output of CP (2, 3) attains a lower energy
than on CP (3, 2). This observation drives our decision to
reﬁne by increasing Niter. Qualitatively, C2F CoGC pro-
duces the same labeling as CoGC. Finally, similar to stereo
matching, partitions based on thresholding scheme perform
signiﬁcantly worse compared to CP (NL, Niter) for image
segmentation as well.

6 Related Work

There is a large body of work on exact
lifting, both
marginal [Kersting et al., 2009; Gogate and Domingos, 2011;
Niepert, 2012; Mittal et al., 2015] and MAP [Kersting
et al., 2009; Gogate and Domingos, 2011; Niepert, 2012;
Sarkhel et al., 2014; Mittal et al., 2014], which is not di-
rectly applicable to our setting. There is some recent work
on approximate lifting [Van den Broeck and Darwiche, 2013;
Venugopal and Gogate, 2014; Singla et al., 2014; Sarkhel et
al., 2015; Van den Broeck and Niepert, 2015] but it’s fo-
cus is on marginal inference whereas we are interested in
lifted MAP. Further, this work can’t handle a distinct unary
potential on every node. An exception is work by Bui et
al. [2012] which explicitly deals with lifting in presence of
distinct unary potentials. Unfortunately, they make a very
strong assumption of exchangeability in the absence of unar-
ies which does not hold true in our setting since each pixel
has its own unique neighborhood.

Work by Sarkhel et al. [2015] is probably the closest to our
work. They design a C2F hierarchy to cluster constants for
approximate lifted MAP inference in Markov logic. In con-
trast, we partition ground atoms in a PGM. Like other work
on approximate lifting, they can’t handle distinct unary po-
tentials. Furthermore, they assume that their theory is pro-
vided in a normal form, i.e., without evidence, which can be
a severe restriction for most practical applications. Kiddon &
Domingos [2011] also propose C2F inference for an underly-
ing Markov logic theory. They use a hierarchy of partitions
based on a pre-speciﬁed ontology. CV does not have any such

6Available

at

https://www.microsoft.com/en-us/research/

project/image-understanding/?from=http%3A%2F%2Fresearch.
microsoft.com%2Fvision%2Fcambridge%2Frecognition%2F

Figure 3: (a-c) Qualitative Results for Segmentation. C2F has quality similar to CoGC algorithm (a) Original Image (b) Segmentation by
CoGC (c) Segmentation by C2F CoGC (d) C2F CoGC has lower energy compared to CoGC and other lifted variant at all times

ontology available, and needs to discover partitions using the
PGM directly.

Nath & Domingos [2010] exploit (approximate) lifted in-
ference for video segmentation. They experiment on a spe-
ciﬁc video problem (different from ours), and they only com-
pare against vanilla BP. Their initial partitioning scheme is
similar to our thresholding approach, which does not work
well in our experiments.

In computer vision, a popular approach to reduce the com-
plexity of inference is to use superpixels [Achanta et al.,
2012; Van den Bergh et al., 2012]. Superpixels are obtained
by merging neighboring nodes that have similar character-
istics. All pixel nodes in the same superpixel are assigned
the same value during MAP inference. SLIC [Achanta et al.,
2012] is one of the most popular algorithms for discovering
superpixels. Our approach differs from SLIC in some signiﬁ-
cant ways. First, their superpixels are local in nature whereas
our algorithm can merge pixels that are far apart. This can
help in merging two disconnected regions of the same object
in a single lifted pixel. Second, they obtain superpixels inde-
pendent of the inference algorithm, whereas we tightly inte-
grate our lifting with the underlying inference algorithm. This
can potentially lead to discovery of better partitions; indeed,
this helped us tremendously in image segmentation. Third,
they do not provide a C2F version of their algorithm and
we did not ﬁnd it straightforward to extend their approach
to discover successively ﬁner partitions. There is some recent
work [Wei et al., 2016] which addresses last two of these
challenges by introducing a hierarchy of superpixels. In our
preliminary experiments, we found that SLIC and superpixel
hierarchy perform worse than our lifting approach. Perform-
ing more rigorous comparisons is a direction for future work.

7 Conclusion and Future Work
We develop a generic template for applying lifted inference
to structured output prediction tasks in computer vision. We
show that MRF-based CV algorithms can be lifted at different
levels of abstraction, leading to methods for coarse to ﬁne
inference over a sequence of lifted models. We test our ideas
on two different CV tasks of stereo matching and interactive
image segmentation. We ﬁnd that C2F lifting is vastly more
efﬁcient than unlifted algorithms on both tasks obtaining a
superior anytime performance, and without any loss in ﬁnal
solution quality. To the best of our knowledge, this is the ﬁrst
demonstration of lifted inference in conjunction with top of
the line task-speciﬁc algorithms. Although we restrict to CV
in this work, we believe that our ideas are general and can
be adapted to other domains such as NLP, and computational
biology. We plan to explore this in the future.

Acknowledgements
We thank anonymous reviewers for their comments and sug-
gestions. Ankit Anand is being supported by the TCS Re-
search Scholars Program. Mausam is being supported by
grants from Google and Bloomberg. Parag Singla is being
supported by a DARPA grant funded under the Explainable
AI (XAI) program. Both Mausam and Parag Singla are being
supported by the Visvesvaraya Young Faculty Fellowships by
Govt. of India. Any opinions, ﬁndings, conclusions or rec-
ommendations expressed in this paper are those of the authors
and do not necessarily reﬂect the views or ofﬁcial policies, ei-
ther expressed or implied, of the funding agencies.

References
[Achanta et al., 2012] R. Achanta, A. Shaji, K. Smith, A. Lucchi,
P. Fua, and S. Ssstrunk. SLIC Superpixels Compared to State-of-

the-Art Superpixel Methods. In PAMI, Nov 2012.

[Anand et al., 2016] A. Anand, A. Grover, Mausam, and P. Singla.
Contextual Symmetries in Probabilistic Graphical Models. In IJ-
CAI, 2016.

[Anand et al., 2017] A. Anand, R. Noothigattu, P. Singla, and
Mausam. Non-Count Symmetries in Boolean & Multi-Valued
Prob. Graphical Models. In AISTATS, 2017.

[Andres et al., 2010] B. Andres,

J. H. Kappes, U. K¨othe,
C. Schn¨orr, and F. A. Hamprecht. An Empirical Comparison of
Inference Algorithms for Graphical Models with Higher Order
Factors Using OpenGM. In Pattern Recognition. 2010.

[Blei et al., 2003] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet

Allocation. JMLR, 3, March 2003.

[Boykov et al., 2001] Y. Boykov, O. Veksler, and R. Zabih. Fast
In PAMI,

Approximate Energy Minimization via Graph Cuts.
23(11), November 2001.

[Braz et al., 2005] R. Braz, E. Amir, and D. Roth. Lifted First-

Order Probabilistic Inference. In IJCAI, 2005.

[Bui et al., 2012] H. Bui, T. Huynh, and R. De Salvo Braz. Exact
Lifted Inference with Distinct Soft Evidence on Every Object. In
AAAI, 2012.

[Bui et al., 2013] H. Bui, T. Huynh, and S. Riedel. Automorphism
Groups of Graphical Models and Lifted Variational Inference. In
UAI, 2013.

[Freeman et al., 2000] W. Freeman, E. Pasztor, and O. Carmichael.

Learning Low-Level Vision. In IJCV, 40, 2000.

[Friedman, 2004] N. Friedman. Inferring Cellular Networks using

Probabilistic Graphical Models. Science, 303, 2004.

[Hirschmuller and Scharstein, 2007] H.

and
Evaluation of Cost Functions for Stereo

Hirschmuller

D. Scharstein.
Matching. In CVPR, 2007.

[Jegelka and Bilmes, 2011] S. Jegelka and J. Bilmes. Submodu-
larity Beyond Submodular Energies: Coupling Edges in Graph
Cuts. In CVPR, 2011.

[Jernite et al., 2015] Y. Jernite, A. Rush, and D. Sontag. A Fast
Variational Approach for Learning Markov Random Field Lan-
guage Models. In ICML, 2015.

[Jha et al., 2010] A. Jha, V. Gogate, A. Meliou, and D. Suciu.
Lifted Inference Seen from the Other Side : The Tractable Fea-
tures. In NIPS, 2010.

[Kappes et al., 2015] J. Kappes, B. Andres, A. Hamprecht,
C. Schn¨orr, S. Nowozin, D. Batra, S. Kim, B. Kausler, T. Kr¨oger,
J. Lellmann, N. Komodakis, B. Savchynskyy, and C. Rother. A
Comparative Study of Modern Inference Techniques for Struc-
tured Discrete Energy Minimization Problems. In IJCV, 2015.
[Kersting et al., 2009] K. Kersting, B. Ahmadi, and S. Natarajan.

Counting Belief Propagation. In UAI, 2009.

[Kersting, 2012] K. Kersting. Lifted Probabilistic Inference.

In

ECAI, 2012.

[Kiddon and Domingos, 2011] C. Kiddon and P. Domingos.
Coarse-to-Fine Inference and Learning for First-Order Proba-
bilistic Models. In AAAI, 2011.

[Kimmig et al., 2015] A. Kimmig, L. Mihalkova, and L. Getoor.
Lifted Graphical Models: A Survey. Machine Learning, 2015.

[Kohli et al., 2013] P. Kohli, A. Osokin, and S. Jegelka. A Prin-
cipled Deep Random Field Model for Image Segmentation. In
CVPR, 2013.

[Lempitsky et al., 2010] V. Lempitsky, C. Rother, S. Roth, and
A. Blake. Fusion Moves for Markov Random Field Optimiza-
tion. In PAMI, Aug 2010.

[Mittal et al., 2014] H. Mittal, P. Goyal, V. Gogate, and P. Singla.
New Rules for Domain Independent Lifted MAP Inference. In
NIPS, 2014.

[Mittal et al., 2015] H. Mittal, A. Mahajan, V. Gogate, and
In NIPS,

P. Singla. Lifted Inference Rules With Constraints.
2015.

[Mladenov et al., 2014] M. Mladenov, K. Kersting, and A. Glober-
son. Efﬁcient Lifting of MAP LP Relaxations Using k-Locality.
In AISTATS, 2014.

[Mozerov and van de Weijer, 2015] M. G. Mozerov and J. van de
Weijer. Accurate Stereo Matching by Two-Step Energy Mini-
mization. IEEE Transactions on Image Processing, March 2015.

[Nath and Domingos, 2010] A. Nath and P. Domingos. Efﬁcient
Lifting for Online Probabilistic Inference. In AAAIWS, 2010.

[Nath and Domingos, 2016] A. Nath and P. Domingos. Learning
Tractable Probabilistic Models for Fault Localization. In AAAI,
2016.

[Niepert, 2012] M. Niepert. Markov Chains on Orbits of Permuta-

tion Groups. In UAI, 2012.

[Noessner et al., 2013] J. Noessner, M. Niepert, and H. Stucken-
schmidt. RockIt: Exploiting Parallelism and Symmetry for MAP
Inference in Statistical Relational Models. In AAAI, 2013.

[Poole, 2003] D. Poole. First-Order Probabilistic Inference. In IJ-

[Sarkhel et al., 2014] S. Sarkhel, D. Venugopal, P. Singla, and
V. Gogate. Lifted MAP inference for Markov logic networks.
In AISTATS, 2014.

[Sarkhel et al., 2015] S. Sarkhel, P. Singla, and V. Gogate. Fast

Lifted MAP Inference via Partitioning. In NIPS, 2015.

[Scharstein and Szeliski, 2002] D. Scharstein and R. Szeliski. A
Taxonomy and Evaluation of Dense Two-Frame Stereo Corre-
spondence Algorithms. In IJCV, 2002.

[Scharstein and Szeliski, 2003] D. Scharstein and R. Szeliski.
High-accuracy Stereo Depth Maps Using Structured Light.
In
CVPR, 2003.

[Singla and Domingos, 2008] P. Singla and P. Domingos. Lifted

First-Order Belief Propagation. In AAAI, 2008.

[Singla et al., 2014] P. Singla, A. Nath, and P. Domingos. Approx-
imate Lifting Techniques for Belief Propagation. In AAAI, 2014.

[Szeliski et al., 2008] R. Szeliski, R. Zabih, D. Scharstein, O. Vek-
sler, V. Kolmogorov, A. Agarwala, M. Tappen, and C. Rother. A
Comparative Study of Energy Minimization Methods for Markov
In PAMI, June
Random Fields with Smoothness-Based Priors.
2008.

[Van den Bergh et al., 2012] M. Van den Bergh, X. Boix, G. Roig,
B. de Capitani, and L. Van Gool. SEEDS: Superpixels Extracted
via Energy-Driven Sampling. In ECCV, 2012.

[Van den Broeck and Darwiche, 2013] G. Van den Broeck and
A. Darwiche. On the Complexity and Approximation of Binary
Evidence in Lifted Inference. In NIPS, 2013.

[Gogate and Domingos, 2011] V. Gogate and P. Domingos. Proba-

bilisitic Theorem Proving. In UAI, 2011.

CAI, 2003.

[Van den Broeck and Niepert, 2015] G. Van den Broeck and
Lifted Probabilistic Inference for Asymmetric

M. Niepert.
Graphical Models. In AAAI, 2015.

[Venugopal and Gogate, 2014] D. Venugopal

and V. Gogate.
Evidence-Based Clustering for Scalable Inference in Markov
Logic. In Joint ECML-KDD, 2014.

[Wei et al., 2016] X. Wei, Q. Yang, Y. Gong, M. Yang, and
N. Ahuja. Superpixel Hierarchy. CoRR, abs/1605.06325, 2016.

Coarse-to-Fine Lifted MAP Inference in Computer Vision

Haroun Habeeb and Ankit Anand and Mausam and Parag Singla
Indian Institute of Technology Delhi
haroun7@gmail.com and {ankit.anand,mausam,parags}@cse.iitd.ac.in

7
1
0
2
 
l
u
J
 
2
2
 
 
]

V
C
.
s
c
[
 
 
1
v
5
6
1
7
0
.
7
0
7
1
:
v
i
X
r
a

Abstract

There is a vast body of theoretical research on
lifted inference in probabilistic graphical models
(PGMs). However, few demonstrations exist where
lifting is applied in conjunction with top of the line
applied algorithms. We pursue the applicability
of lifted inference for computer vision (CV), with
the insight that a globally optimal (MAP) labeling
will likely have the same label for two symmetric
pixels. The success of our approach lies in efﬁ-
ciently handling a distinct unary potential on ev-
ery node (pixel), typical of CV applications. This
allows us to lift the large class of algorithms that
model a CV problem via PGM inference. We pro-
pose a generic template for coarse-to-ﬁne (C2F) in-
ference in CV, which progressively reﬁnes an ini-
tial coarsely lifted PGM for varying quality-time
trade-offs. We demonstrate the performance of
C2F inference by developing lifted versions of two
near state-of-the-art CV algorithms for stereo vi-
sion and interactive image segmentation. We ﬁnd
that, against ﬂat algorithms, the lifted versions have
a much superior anytime performance, without any
loss in ﬁnal solution quality.

1 Introduction
Lifted inference in probabilistic graphical models (PGMs)
refers to the set of the techniques that carry out inference
over groups of random variables (or states) that behave simi-
larly [Jha et al., 2010; Kimmig et al., 2015]. A vast body of
theoretical work develops a variety of lifted inference tech-
niques, both exact (e.g., [Poole, 2003; Braz et al., 2005;
Singla and Domingos, 2008; Kersting, 2012]) and approxi-
mate (e.g., [Singla et al., 2014; Van den Broeck and Niepert,
2015]). Most of these works develop technical ideas appli-
cable to generic subclasses of PGMs, and the accompanying
experiments are aimed at providing ﬁrst proofs of concepts.
However, little work exists on transferring these ideas to the
top domain-speciﬁc algorithms for real-world applications.

Algorithms for NLP, computational biology, and computer
vision (CV) problems make heavy use of PGM machinery
(e.g., [Blei et al., 2003; Friedman, 2004; Szeliski et al.,
2008]). But, they also include signiﬁcant problem-speciﬁc

insights to get high performance. Barring a handful of excep-
tions [Jernite et al., 2015; Nath and Domingos, 2016], lifted
inference hasn’t been applied directly to such algorithms.

We study the potential value of lifting to CV problems such
as image denoising, stereo vision, and image segmentation.
Most CV problems are structured output prediction tasks, typ-
ically assigning a label to each pixel. A large class of solu-
tions are PGM-based: they deﬁne a Markov Random Field
(MRF) that has each pixel as a node, with unary potential that
depends on pixel value, and pairwise neighborhood potentials
that favor similar labels to neighboring pixels.

We see three main challenges in applying existing lifted
inference literature to these problems. First, most existing al-
gorithms focus on computing marginals [Singla and Domin-
gos, 2008; Kersting et al., 2009; Gogate and Domingos, 2011;
Niepert, 2012; Anand et al., 2016; Anand et al., 2017] instead
of MAP inference. Second, among the algorithms perform-
ing lifted MAP [Noessner et al., 2013; Mladenov et al., 2014;
Sarkhel et al., 2014; Mittal et al., 2014], many of the algo-
rithms focus on exact lifting. This breaks the kind of symme-
tries we need to compute since different pixels may not have
exact same neighborhood. Third, the few algorithms that per-
form approximate lifting for MAP, e.g. [Sarkhel et al., 2015],
can’t handle a distinct unary potential on every node. This is
essential for our application since image pixels take ordinal
values in three channels.

In response, we develop an approximate lifted MAP infer-
ence algorithm which can effectively handle unary potentials.
We initialize our algorithm by merging together pixels hav-
ing the same order of top-k labels based on the unary po-
tential values. We then adapt an existing symmetry ﬁnding
algorithm [Kersting et al., 2009] to discover groupings which
also have similar neighborhoods. We refer to our groupings
as lifted pixels. We impose the constraint that all pixels in a
lifted pixel must be assigned the same label. Our approximate
lifting reduces the model size drastically leading to signiﬁ-
cant time savings. Unfortunately, such approximate lifting
could adversely impact solution quality. However, we vary
the degree of approximation in symmetry ﬁnding to output a
sequence of coarse-to-ﬁne models with varying quality-time
trade-offs. By switching between such models, we develop a
coarse-to-ﬁne (C2F) inference procedure applicable to many
CV problems.

We formalize these ideas in a novel template for using

lifted inference in CV. We test C2F lifted inference on two
problems: stereo matching and image segmentation. We start
with one of the best MRF-based solvers each for both prob-
lems – neither of these are vanilla MRF solvers. Mozerov
& Weijer [2015] use a two-way energy minimization to ef-
fectively handle occluded regions in stereo matching. Co-
operative cuts [Kohli et al., 2013] for image segmentation use
concave functions over a predeﬁned set of pixel pairs to cor-
rectly segment images with sharp edges. We implement C2F
inference on top of both these algorithms and ﬁnd that C2F
versions have a strong anytime behavior – given any amount
of inference time, they output a much higher quality (and are
never worse) than their unlifted counterparts, and don’t suffer
any loss in the ﬁnal quality. Overall, our contributions are:

1. We present an approximate lifted MAP algorithm that
can efﬁciently handle a large number of distinct unary
potentials.

2. We develop a novel template for applying lifted infer-
ence in structured prediction tasks in CV. We provide
methods that output progressively ﬁner approx. symme-
tries, leading to a C2F lifted inference procedure.

3. We implement C2F inference over a near state-of-the-
art stereo matching algorithm, and one of the best MRF-
based image segmentation algorithms. We release our
implementation for wider use by the community.1

4. We ﬁnd that C2F has a much superior anytime behav-
ior. For stereo matching it achieves 60% better quality
on average in time-constrained settings. For image seg-
mentation C2F reaches convergence in 33% less time.

2 Background
2.1 Computer Vision Problems as MRFs
Most computer vision problems are structured output predic-
tion problems and their PGM-based solutions often follow
similar formulations. They cast the tasks into the problem
of ﬁnding the lowest energy assignment over grid-structured
MRFs (denoted by G = (X , γ)). The random variables in
these MRFs are the set of pixels X in the input image. Given
a set of labels L : {1, 2, . . . , |L|}, the task of structured output
prediction is to label each pixel X with a label from L. The
MRFs have two kinds of potentials (γ) – unary and higher-
order. Unary potentials are deﬁned over each individual pixel,
and usually incorporate pixel intensity, color, and other pixel
features. Higher order potentials operate over cliques (pairs
or more) of neighboring pixels and typically express some
form of spatial homophily – “neighboring pixels are more
likely to have similar labels.” While the general PGM struc-
ture of various tasks are similar, the speciﬁc potential tables
and label spaces are task-dependent.

The goal is to ﬁnd the MAP assignment over this MRF,
which is equivalent to energy minimization (by deﬁning en-
ergy as negative log of potentials). We denote the negative
log of unary potentials by φ, and that of higher-order poten-
tials by ψ.2 Thus, energy of a complete assignment x ∈ L|X |

1https://github.com/dair-iitd/c2ﬁ4cv/
2In the interest of readability, we say ‘potential’ to mean ‘nega-

tive log of potential’ in the rest of the paper.

can be deﬁned as:

E(x) =

φ(xi) +

ψj(ˆxj)

(1)

(cid:88)

i∈1..|X |

(cid:88)

j

Here ˆxj denotes the assignment x restricted to the set of
variables in the potential ψj. And the output of the algorithm
is the assignment xMAP:

xMAP = arg min
x∈L|X |

E(x)

(2)

The problem is in general intractable. Efﬁcient approxi-
mations exploit special characteristics of potentials like sub-
modularity [Jegelka and Bilmes, 2011], or use variants of
graph cut or loopy belief propagation [Boykov et al., 2001;
Freeman et al., 2000].

2.2 Symmetries in Graphical Models
Lifting an algorithm often requires computing a set of sym-
metries that can be exploited by that algorithm. For PGMs,
two popular methods for symmetry computation are color
passing for computing symmetries of variables [Kersting et
al., 2009], and graph isomorphism for symmetries of states
[Niepert, 2012; Bui et al., 2013]. Since our work is based on
color passing, we explain it in more detail.

Color passing for an MRF operates over a colored bipar-
tite graph containing nodes for all variables and potentials,
and each node is assigned a color. The graph is initialized
as follows: all variables nodes get a common color; all po-
tential nodes with exactly same potential tables are assigned
a unique color. Now, in an iterative color passing scheme,
in each iteration, each variable node passes its color to all
neighboring potential nodes. The potential nodes store in-
coming color signatures in a vector, append their own color
to it, and send the vector back to variable nodes. The variable
nodes stack these incoming vectors. New colors are assigned
to each node based on the set of incoming messages such that
two nodes with same messages are assigned the same unique
color. This process is repeated until convergence, i.e., no fur-
ther change in colors.

A coloring of the bipartite graph deﬁnes a partition of vari-
able nodes such that all nodes of the same color form a par-
tition element. Each iteration of color passing creates suc-
cessively ﬁner partitions, since two variable nodes, once as-
signed different colors, can never get the same color.

3 Lifted Computer Vision Framework
In this section, we will describe our generic template which
can be used to lift a large class of vision applications includ-
ing those in stereo, segmentation etc. Our template can be
seen as transforming the original problem space to a reduced
problem space over which the original inference algorithm
can now be applied much more efﬁciently. Speciﬁcally, our
description in this section is entirely algorithm independent.
We will focus on MAP inference which is the inference
task of choice for most vision applications (refer Section 2).
The key insight in our formulation is based on the realiza-
tion that pixels which are involved in the same (or similar)

kinds of unary and higher order potentials, and have the same
(or similar) neighborhoods, are likely to have the same MAP
value. Therefore, if somehow we could discover such sets
of pixels a priori, we could explicitly enforce these pixels to
have the same value while searching for the solution, substan-
tially reducing the problem size and still preserving the opti-
mal MAP assignment(s). Since in general doing this exactly
may lead to a degenerate network, we do it approximately.
Hence trading-off speed for marginal loss in solution quality.
The loss in solution quality is offset by resorting to coarse-
to-ﬁne inference where we start with a crude approximation,
and gradually make it ﬁner, to guarantee optimality at the end
while still obtaining signiﬁcant gains. We next describe the
details of our approach.

3.1 Obtaining a Reduced Problem

1 , Y P

2 , · · · , Y P

k ⊆ X , Y P
k1

Consider an energy minimization problem over a PGM G =
(X , γ). Let L = {1, 2, · · · , |L|} denote the set of labels
over which variables in the set X can vary. Let Y P =
{Y P
r } denote a partition of X into r disjoint
∩ Y P
subsets, i.e., ∀k, Y P
= ∅ when k1 (cid:54)= k2,
k2
and ∪kY P
k = X . We refer to each Y P
k as a partition element.
Correspondingly, let us deﬁne Y = {Y1, Y2, · · · , Yr} as a set
of partition variables, where there is a one to one correspon-
dence between partition elements and the partition variables
and each partition variable Yk takes values in the set L. Let
part(Xi) denote the partition element to which Xi belongs.
Let ˆXj ⊆ X denote a subset of variables. We say that a par-
k is represented in the set ˆXj if ∃Xi ∈ ˆXj
tition element Y P
s.t. part(Xj) = Yk.
Given a subset of variables ˆXj, let γj( ˆXj) be a potential de-
ﬁned over ˆXj. Let ˆxj denote an assignment to variables in
the set ˆXj. Let ˆxj.elem(i) denote the value taken by a vari-
able Xi in ˆXj. We say that an assignment ˆXj = ˆxj respects
a partition Y P if the variables in ˆXj belonging to the same
partition element have the same label in ˆxj, i.e., part(Xi) =
part(Xi(cid:48)) ⇒ ˆxj.elem(i) = ˆxj.elem(i(cid:48)), ∀Xi, Xi(cid:48) ∈ ˆXj.
Next, we introduce the notion of a reduced potential.

Deﬁnition 3.1 Let X be a set of variables and let Y P de-
note its partition. Given the potential γj( ˆXj), the reduced
potential Γj is deﬁned to be the restriction of γj( ˆXj) to those
labeling assignments of ˆXj which respect the partition Y P .
Equivalently, we can deﬁne the reduced potential Γj( ˆYj) over
the set of partition variables ˆYj which are represented in the
set ˆXj.

For example, consider a potential γ(X1, X2, X3) deﬁned
over three Boolean variables. The table for γ would have
8 entries. Consider the partition Y P = {Y P
2 } where
1 = {X1, X2} and Y P
Y P
2 = {X3}. Then, the reduced poten-
tial Γ is the restriction of γ to those rows in the table where
X1 = X2. Hence Γ has four rows in its table and equivalently
can be thought of deﬁning a potential over the 4 possible com-
binations of Y1 and Y2 variables. We are now ready to deﬁne
a reduced graphical model.

1 , Y P

Deﬁnition 3.2 Let G = (X , γ) represent a PGM. Given a
partition Y P of X , the reduced graphical model G(Y, Γ) is
the graphical model deﬁned over the set of partition variables
Y such that every potential γj ∈ γ in G is replaced by the
corresponding reduced potential Γj ∈ Γ in G.
Let E(x) and E(y) denote the energies of the states x and y
in G and G, respectively. The following theorem relates the
energies of the states in the two graphical models.

Theorem 3.1 For every assignment y of Y in G, there is a
corresponding assignment x of X such that E(y) = E(x).

The theorem can be proved by noting that each potential
Γj( ˆYj) in G was obtained by restricting the original poten-
tial γj( ˆXj) to those assignments where variables in Xj be-
longing to the same partition took the same label. Since this
correspondence is true for every potential in the reduced set,
to obtain the desired state x, for every variable Xi ∈ X we
simply assign it the label of its partition in y.
Corollary 3.1 Let xMAP and yMAP be the MAP states (i.e.
having the minimum energy) for G and G, respectively. Then,
E(yMAP) ≥ E(xMAP).
The process of reduction can be seen as curtailing the en-
tire search space to those assignments where variables in the
same partition take the same label. A reduction in the prob-
lem space will lead to computational gains but might result
in loss of solution quality, where the solution quality can be
captured by the difference between E(yMAP) and E(xMAP).
Therefore, we need to trade-off the balance between the two.
Intuitively, a good problem reduction will keep those vari-
ables in the same partition which are likely to have the same
value in the optimal assignment for the original problem.
How do we ﬁnd such variables without actually solving the
inference task? We will describe one such technique in Sec-
tion 3.3.

There is another perspective.

Instead of solving one re-
duced problem, we can instead work with a series of reduced
problems which successively get closer to the optimal solu-
tion. The initial reductions are coarser and far from optimal,
but can be solved efﬁciently to quickly reach in the region
where the solution lies. The successive iterations can then re-
ﬁne the solution iteratively getting closer to the optimal. This
leads us to the coarse-to-ﬁne inference described next.

3.2 Coarse to Fine Inference
We will deﬁne a framework for C2F (coarse-to-ﬁne) infer-
ence so that we maintain the computational advantage while
still preserving optimality. In the following, for ease of no-
tation, we will drop the superscript P in Y p to denote the
partition of X . Therefore, Y will refer to both the partition as
well as the set of partition variables. Before we describe our
algorithm, let us start with some deﬁnitions.
Deﬁnition 3.3 Let Y and Y (cid:48) be two partitions of X . We
say that Y is coarser than Y (cid:48), denoted as Y (cid:22) Y (cid:48),
if
∀y(cid:48) ∈ Y (cid:48)∃y ∈ Y such that y(cid:48) ⊆ y. We equivalently say
that Y (cid:48) is ﬁner than Y.
It is easy to see that X deﬁnes a partition of itself which is the
ﬁnest among all partitions, i.e., ∀Y such that Y is a partition

of X , Y (cid:22) X . We also refer it to as the degenerate partition.
For ease of notation, we will denote the ﬁnest partition by
Y ∗ (same as X ). We will refer to the corresponding PGM as
G∗ (same as G). Next, we state a theorem which relates two
partitions with each other.
Lemma 1 Let Y and Y (cid:48) be two partitions of X such that Y (cid:22)
Y (cid:48). Then Y (cid:48) can be seen as a partition of the set Y.

The proof of this lemma is straightforward and is omitted
due to lack of space. Consider a set Y of coarse to ﬁne
partitions given as Y 0 (cid:22) Y 1, · · · , (cid:22), Y t, (cid:22), · · · , Y ∗. Let
Gt, E t, yt
M AP respectively denote the reduced problem, en-
ergy function and MAP assignment for the partition Y t. Us-
ing Lemma 1, Y t+1 is a partition of Y t. Then, using Theo-
rem 3.1, we have for every assignment yt to variables in Y t,
there is an assignment yt+1 to variables in Y t+1 such that
E t(yt) = E t+1(yt+1). Also, using Corollary 3.1, we have
∀t E t(yt
MAP). Together, these two state-
ments imply that starting from the coarsest partition, we can
gradually keep on improving the solution as we move to ﬁner
partitions.

MAP) ≥ E t+1(yt+1

Our C2F set-up assumes an iterative MAP inference algo-
rithm A which has the anytime property i.e., can produce so-
lutions of increasing quality with time. C2F Function (see
Algorithm 1) takes 3 inputs: a set of C2F partitions Y, infer-
ence algorithm A, and a stopping criteria C. The algorithm A
in turn takes three inputs: PGM Gt, starting assignment yt,
stopping criteria C. A outputs an approximation to the MAP
solution once the stopping criteria C is met. Starting with the
coarsest partition (t = 0 in line 2), a start state is picked for
the coarsest problem to be solved (line 3). In each iteration
(line 4), C2F ﬁnds the MAP estimate for the current problem
(Gt) using algorithm A (line 5). This solution is then mapped
to a same energy solution of the next ﬁner partition (line 6)
which becomes the starting state for the next run of A. The
solution is thus successively reﬁned in each iteration. The
process is repeated until we reach the ﬁnest level of partition.
In the end, A is run on the ﬁnest partition and the resultant
solution is output (lines 9,10). Since the last partition in the
set is the original problem G∗, optimality with respect to A is
guaranteed.

Next, we describe how to use the color passing algorithm
(Section 2) to get a series of partitions which get successively
ﬁner. Our C2F algorithm can then be applied on this set of
partitions to get anytime solutions of high quality while being
computationally efﬁcient.

Algorithm 1 Coarse-to-Fine Lifted MAP Algorithm
1:C2F Lifted MAP(C2F Partitions Y, Algo A,Criteria C)
2: t = 0; T = |Y|;
3: yt = getInitState(Gt);
4: While (t < T );
MAP = A(Gt, yt, C);
yt
5:
yt+1 = getEquivAssignment(Y t, Y t+1, yt
6:
t = t + 1;
7:
8: END While
9: yT
10: return yT

MAP = A(GT , yT , C);
MAP

MAP);

3.3 C2F Partitioning for Computer Vision
We now adapt the general color passing algorithm to MRFs
for CV problems. Unfortunately, unary potentials make color
passing highly ineffective. Different pixels have different
RGB values and intensities, leading to almost every pixel get-
ting a different unary potential. Naive application of color
passing splits almost all variables into their own partitions,
and lifting offers little value.

A natural approximation is to deﬁne a threshold, such that
two unary potentials within that threshold be initialized with
the same color. Our experiments show limited success with
this scheme because because two pixels may have the same
label even when their actual unary potentials are very differ-
ent. What is more important is relative importance given to
each label than the actual potential value.

In response, we adapt color passing for CV by initializing
it as before, but with one key change: we initialize two unary
potential nodes with the same color if their lowest energy la-
bels have the same order for the top NL labels (we call this
unary split threshold). Experiments reveal that this approxi-
mation leads to effective partitions for lifted inference.

Finally, we can easily construct a sequence of coarse-to-
ﬁne partitions in the natural course of color passing’s execu-
tion – every iteration of color passing creates a ﬁner partition.
Moreover, as an alternative approach, we may also increase
NL. In our implementations, we intersperse the two, i.e., be-
fore every next step we pick one of two choices: either, we
run another iteration of color passing; or, we increase NL by
one, and split each variable partition based on the N th
L lowest
energy labels of its constituent variables.

We parameterize CP (NL, Niter) to denote the partition
from the current state of color passing, which has been run
till Niter iterations and unary split threshold is NL.
It
is easy to prove that another iteration of color passing or
splitting by increasing NL as above leads to a ﬁner par-
I.e., CP (NL, Niter) (cid:22) CP (NL + 1, Niter) and
tition.
CP (NL, Niter) (cid:22) CP (NL, Niter + 1). We refer to each
element of a partition of variables as a lifted pixel, since it is
a subset of pixels.

4 Lifted Inference for Stereo Matching
We ﬁrst demonstrate the value of lifted inference in the con-
text of stereo matching [Scharstein and Szeliski, 2002].
It
aims to ﬁnd pixel correspondences in a set of images of the
same scene, which can be used to further estimate the 3D
scene. Formally, two images I l and I r corresponding to
images of the scene from a left camera and a right cam-
era are taken such that both cameras are at same horizon-
tal level. The goal is to compute a disparity labeling Dl
for every pixel X = (a, b) such that I l[a][b] corresponds to
I r[a−Dl[a][b]][b]. We build a lifted version of TSGO [Moze-
rov and van de Weijer, 2015], as it is MRF-based and ranks
2nd on the Middlebury Stereo Evaluation Version 2 leader-
board.3
Background on TSGO: TSGO treats stereo matching as
a two-step energy minimization, where the ﬁrst step is on a

3http://vision.middlebury.edu/stereo/eval/

(a)

(b)

(c)

Figure 1: (a) Average (normalized) energy vs. inference time (b) Average pixel error vs. time. C2F TSGO achieves roughly 60% reduction in
time for reaching the optima. It has best anytime performance compared to vanilla TSGO and static lifted versions. (c) Average (normalized)
energy vs. time for different thresholding values and CP partitions. Plots with the same marker have MRFs of similar sizes.

(a)

(b)

(c)

(d)

(e)

Figure 2: Qualitative results for Doll image at convergence. C2F-TSGO is similar to base TSGO.(a) Left and Right Images (b) Ground
Truth (c) Disparity Map by TSGO (d) Disparity Map by C2F TSGO (e) Each colored region (other than black) is one among the 10 largest
partition elements from CP(1,1). Each color represents one partition element. Partition elements form non-contiguous regions

fully connected MRF with pairwise potentials and the second
is on a conventional locally connected MRF. Lack of space
precludes a detailed description of the ﬁrst step. At the high
level, TSGO runs one iteration of message passing on fully
connected MRF, computes marginals of each pixel X, which
act as unary potentials φ(X) for the MRF of second step.

The pairwise potential ψ used in step two is ψ(X, X (cid:48)) =
w(X, X (cid:48))ϕ(X, X (cid:48)), where ϕ(X, X (cid:48)) is a truncated linear
function of (cid:107)X − X (cid:48)(cid:107), and w(X, X (cid:48)) takes one of three dis-
tinct values depending on color difference between pixels.
The MAP assignment xMAP computes the lowest energy as-
signment of disparities Dl for every pixel for this MRF.
Lifted TSGO: Since step two is costlier, we build its lifted
version as discussed in previous section. For color passing,
two unary potential nodes are initialized with the same color
if their lowest energy labels exactly match (NL = 1). Other
initializations are consistent with original color passing for
general MRFs. A sequence of coarse-to-ﬁne models is out-
putted as per Section 3.3. C2F TSGO uses outputs from the
sequence CP (1, 1), CP (2, 1), CP (3, 1) and then reﬁnes to
the original MRF. Model reﬁnement is triggered whenever
energy hasn’t decreased in the last four iterations of alpha ex-
pansion (this becomes the stopping criteria C in Algorithm
1).
Experiments: Our experiments build on top of the exist-
ing TSGO implementation4, but we change the minimization
algorithm in step two to alpha expansion fusion [Lempitsky
et al., 2010] from OpenGM2 library [Andres et al., 2010;
Kappes et al., 2015], as it improves the speed of the base

4http://www.cvc.uab.es/∼mozerov/Stereo/

implementation. We use the benchmark Middlebury Stereo
datasets of 2003, 2005 and 2006 [Scharstein and Szeliski,
2003; Hirschmuller and Scharstein, 2007]. For the 2003
dataset, quarter-size images are used and for others, third-size
images are used. The label space is of size 85 (85 distinct dis-
parity labels).

We

our

compare

coarse-to-ﬁne

TSGO (using
CP (NL, Niter) partitions) against vanilla TSGO. Fig-
ures 1(a,b) show the aggregate plots of energy (and error)
vs.
time. We observe that C2F TSGO reaches the same
optima as TSGO, but in less than half the time. It has a much
superior anytime performance – if inference time is given as
a deadline, C2F TSGO obtains 59.59% less error on average
over randomly sampled deadlines. We also eyeball the out-
puts of C2F TSGO and TSGO and ﬁnd them to be visually
similar. Figure 2 shows a sample qualitative comparison.
Figure 2(e) shows ﬁve of the ten largest partition elements in
the partition from CP (1, 1). Clearly, the partition elements
formed are not contiguous, and seem to capture variables that
are likely to get the same assignment. This underscores the
value of our lifting framework for CV problems.

We also compare our CP (NL, Niter) partitioning strat-
egy with threshold partitioning discussed in Section 3.3. We
merge two pixels in thresholding scheme if the L1-norm dis-
tance of their unary potentials is less than a threshold. For
each partition induced by our approach, we ﬁnd a value of
threshold that has roughly the same number of lifted pix-
els. Figure 1(c) shows that partitions based on CP (1, 1) and
CP (3, 1) converges to a much lower energy quickly com-
pared to the corresponding threshold values (T hr = 50 and
T hr = 1 respectively). For CP (2, 1), convergence is slower

compared to corresponding threshold (T hr = 5) but eventu-
ally CP (2, 1) has signiﬁcantly better quality.

5 Lifted Inference for Image Segmentation
We now demonstrate the general nature of our lifted CV
framework by applying it to a second task. We choose multi-
label interactive image segmentation, where the goal is to
segment an image I based on a seed labeling (true labels
for a few pixels) provided as input. Like many other CV
problems, this also has an MRF-based solution, with the best
label-assignment generally obtained by MAP inference using
graph cuts or loopy belief propagation [Boykov et al., 2001;
Szeliski et al., 2008].

However, MRFs with only pairwise potentials are known to
suffer from short-boundary bias – they prefer segmentations
with shorter boundaries, because pairwise potentials penalize
every pair of boundary pixels. This leads to incorrect labeling
for sharp edge objects. Kohli et al. [2013] use CoGC, coop-
erative graph cuts [Jegelka and Bilmes, 2011], to develop one
of the best MRF-based solvers that overcome this bias.
Background on CoGC: Traditional MRFs linearly penalize
the number of label discontinuities at edges (boundary pixel
pairs), but CoGC penalizes the number of types of label dis-
continuities through the use of a concave energy function over
groups of ordered edges. It ﬁrst clusters all edges on the ba-
sis of color differences, and later applies a concave function
separately over the number of times a speciﬁc discontinuity
type is present in each edge group g ∈ G. Their carefully
engineered CoGC energy function is as follows:

E(x) =

φi(xi)+

w(x, x(cid:48)).I(x = l, x(cid:48) (cid:54)= l)

|X |
(cid:88)

i=1



F



(cid:88)

(cid:88)

(cid:88)

g∈G

l∈L

(x,x(cid:48))∈g





where unary potentials φ depend on colors of seed pixels, F
is a concave function, I the indicator function, and w(x, x(cid:48))
depends on the color difference between x, x(cid:48).
Intuitively,
F collects all edges with similar discontinuities and penal-
izes them sub-linearly, thus reducing the short-boundary bias
in the model. The usage of a concave function makes the
MRF higher order with cliques over edge groups. However,
the model is shown to reduce to a pairwise hierarchical MRF
through the addition of auxiliary variables.
Lifted CoGC: CoGC is lifted using the framework of Sec-
tion 3, with one additional change. We cluster edge groups
using color difference and the position of the edge. Edge
groups that are formed only on the basis of color difference
make the error of grouping different segment’s boundaries
into a single group. For e.g., it erroneously cluster boundaries
between white cow and grass, and sky and grass together in
the top image in Figure 3.

2 (cid:101), 2), CP ((cid:100) L

Coarse-to-ﬁne partitions are obtained by the method de-
scribed in Section 3.3. C2F CoGC uses outputs from the se-
quence CP ((cid:100) L
2 (cid:101), 3) before reﬁning to the orig-
inal MRF. Model reﬁnement is triggered if energy has not
reduced over the last |L| iterations.
Experiments: Our experiments use the implementation of
Cooperative Graph Cuts as provided by [Kohli et al., 2013].5

5Available at https://github.com/aosokin/coopCuts CVPR2013

Energy minimization is performed using alpha expansion
[Boykov et al., 2001]. The implementation of CoGC per-
forms a greedy descent on auxiliary variables while perform-
ing alpha expansion on the remaining variables, as described
in Kohli et. al. [2013]. The dataset used is provided with the
implementation. It is a part of the MSRC V2 dataset.6.

Figure 3 shows three individual energy vs. time plots. Re-
sults on other images are similar. We ﬁnd that C2F CoGC
algorithm converges to the same energy as CoGC in about
two-thirds the time on average. Overall, C2F CoGC achieves
a much better anytime performance than other lifted and un-
lifted CoGC.

Similar to Section 4, reﬁned partitions attain better quality
than coarser ones at the expense of time. Since the implemen-
tation performs a greedy descent over auxiliary variables, re-
ﬁnement of current partition also resets the auxiliary variables
to the last value that produced a change. Notice that energy
minimization on output of CP (2, 3) attains a lower energy
than on CP (3, 2). This observation drives our decision to
reﬁne by increasing Niter. Qualitatively, C2F CoGC pro-
duces the same labeling as CoGC. Finally, similar to stereo
matching, partitions based on thresholding scheme perform
signiﬁcantly worse compared to CP (NL, Niter) for image
segmentation as well.

6 Related Work

There is a large body of work on exact
lifting, both
marginal [Kersting et al., 2009; Gogate and Domingos, 2011;
Niepert, 2012; Mittal et al., 2015] and MAP [Kersting
et al., 2009; Gogate and Domingos, 2011; Niepert, 2012;
Sarkhel et al., 2014; Mittal et al., 2014], which is not di-
rectly applicable to our setting. There is some recent work
on approximate lifting [Van den Broeck and Darwiche, 2013;
Venugopal and Gogate, 2014; Singla et al., 2014; Sarkhel et
al., 2015; Van den Broeck and Niepert, 2015] but it’s fo-
cus is on marginal inference whereas we are interested in
lifted MAP. Further, this work can’t handle a distinct unary
potential on every node. An exception is work by Bui et
al. [2012] which explicitly deals with lifting in presence of
distinct unary potentials. Unfortunately, they make a very
strong assumption of exchangeability in the absence of unar-
ies which does not hold true in our setting since each pixel
has its own unique neighborhood.

Work by Sarkhel et al. [2015] is probably the closest to our
work. They design a C2F hierarchy to cluster constants for
approximate lifted MAP inference in Markov logic. In con-
trast, we partition ground atoms in a PGM. Like other work
on approximate lifting, they can’t handle distinct unary po-
tentials. Furthermore, they assume that their theory is pro-
vided in a normal form, i.e., without evidence, which can be
a severe restriction for most practical applications. Kiddon &
Domingos [2011] also propose C2F inference for an underly-
ing Markov logic theory. They use a hierarchy of partitions
based on a pre-speciﬁed ontology. CV does not have any such

6Available

at

https://www.microsoft.com/en-us/research/

project/image-understanding/?from=http%3A%2F%2Fresearch.
microsoft.com%2Fvision%2Fcambridge%2Frecognition%2F

Figure 3: (a-c) Qualitative Results for Segmentation. C2F has quality similar to CoGC algorithm (a) Original Image (b) Segmentation by
CoGC (c) Segmentation by C2F CoGC (d) C2F CoGC has lower energy compared to CoGC and other lifted variant at all times

ontology available, and needs to discover partitions using the
PGM directly.

Nath & Domingos [2010] exploit (approximate) lifted in-
ference for video segmentation. They experiment on a spe-
ciﬁc video problem (different from ours), and they only com-
pare against vanilla BP. Their initial partitioning scheme is
similar to our thresholding approach, which does not work
well in our experiments.

In computer vision, a popular approach to reduce the com-
plexity of inference is to use superpixels [Achanta et al.,
2012; Van den Bergh et al., 2012]. Superpixels are obtained
by merging neighboring nodes that have similar character-
istics. All pixel nodes in the same superpixel are assigned
the same value during MAP inference. SLIC [Achanta et al.,
2012] is one of the most popular algorithms for discovering
superpixels. Our approach differs from SLIC in some signiﬁ-
cant ways. First, their superpixels are local in nature whereas
our algorithm can merge pixels that are far apart. This can
help in merging two disconnected regions of the same object
in a single lifted pixel. Second, they obtain superpixels inde-
pendent of the inference algorithm, whereas we tightly inte-
grate our lifting with the underlying inference algorithm. This
can potentially lead to discovery of better partitions; indeed,
this helped us tremendously in image segmentation. Third,
they do not provide a C2F version of their algorithm and
we did not ﬁnd it straightforward to extend their approach
to discover successively ﬁner partitions. There is some recent
work [Wei et al., 2016] which addresses last two of these
challenges by introducing a hierarchy of superpixels. In our
preliminary experiments, we found that SLIC and superpixel
hierarchy perform worse than our lifting approach. Perform-
ing more rigorous comparisons is a direction for future work.

7 Conclusion and Future Work
We develop a generic template for applying lifted inference
to structured output prediction tasks in computer vision. We
show that MRF-based CV algorithms can be lifted at different
levels of abstraction, leading to methods for coarse to ﬁne
inference over a sequence of lifted models. We test our ideas
on two different CV tasks of stereo matching and interactive
image segmentation. We ﬁnd that C2F lifting is vastly more
efﬁcient than unlifted algorithms on both tasks obtaining a
superior anytime performance, and without any loss in ﬁnal
solution quality. To the best of our knowledge, this is the ﬁrst
demonstration of lifted inference in conjunction with top of
the line task-speciﬁc algorithms. Although we restrict to CV
in this work, we believe that our ideas are general and can
be adapted to other domains such as NLP, and computational
biology. We plan to explore this in the future.

Acknowledgements
We thank anonymous reviewers for their comments and sug-
gestions. Ankit Anand is being supported by the TCS Re-
search Scholars Program. Mausam is being supported by
grants from Google and Bloomberg. Parag Singla is being
supported by a DARPA grant funded under the Explainable
AI (XAI) program. Both Mausam and Parag Singla are being
supported by the Visvesvaraya Young Faculty Fellowships by
Govt. of India. Any opinions, ﬁndings, conclusions or rec-
ommendations expressed in this paper are those of the authors
and do not necessarily reﬂect the views or ofﬁcial policies, ei-
ther expressed or implied, of the funding agencies.

References
[Achanta et al., 2012] R. Achanta, A. Shaji, K. Smith, A. Lucchi,
P. Fua, and S. Ssstrunk. SLIC Superpixels Compared to State-of-

the-Art Superpixel Methods. In PAMI, Nov 2012.

[Anand et al., 2016] A. Anand, A. Grover, Mausam, and P. Singla.
Contextual Symmetries in Probabilistic Graphical Models. In IJ-
CAI, 2016.

[Anand et al., 2017] A. Anand, R. Noothigattu, P. Singla, and
Mausam. Non-Count Symmetries in Boolean & Multi-Valued
Prob. Graphical Models. In AISTATS, 2017.

[Andres et al., 2010] B. Andres,

J. H. Kappes, U. K¨othe,
C. Schn¨orr, and F. A. Hamprecht. An Empirical Comparison of
Inference Algorithms for Graphical Models with Higher Order
Factors Using OpenGM. In Pattern Recognition. 2010.

[Blei et al., 2003] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet

Allocation. JMLR, 3, March 2003.

[Boykov et al., 2001] Y. Boykov, O. Veksler, and R. Zabih. Fast
In PAMI,

Approximate Energy Minimization via Graph Cuts.
23(11), November 2001.

[Braz et al., 2005] R. Braz, E. Amir, and D. Roth. Lifted First-

Order Probabilistic Inference. In IJCAI, 2005.

[Bui et al., 2012] H. Bui, T. Huynh, and R. De Salvo Braz. Exact
Lifted Inference with Distinct Soft Evidence on Every Object. In
AAAI, 2012.

[Bui et al., 2013] H. Bui, T. Huynh, and S. Riedel. Automorphism
Groups of Graphical Models and Lifted Variational Inference. In
UAI, 2013.

[Freeman et al., 2000] W. Freeman, E. Pasztor, and O. Carmichael.

Learning Low-Level Vision. In IJCV, 40, 2000.

[Friedman, 2004] N. Friedman. Inferring Cellular Networks using

Probabilistic Graphical Models. Science, 303, 2004.

[Hirschmuller and Scharstein, 2007] H.

and
Evaluation of Cost Functions for Stereo

Hirschmuller

D. Scharstein.
Matching. In CVPR, 2007.

[Jegelka and Bilmes, 2011] S. Jegelka and J. Bilmes. Submodu-
larity Beyond Submodular Energies: Coupling Edges in Graph
Cuts. In CVPR, 2011.

[Jernite et al., 2015] Y. Jernite, A. Rush, and D. Sontag. A Fast
Variational Approach for Learning Markov Random Field Lan-
guage Models. In ICML, 2015.

[Jha et al., 2010] A. Jha, V. Gogate, A. Meliou, and D. Suciu.
Lifted Inference Seen from the Other Side : The Tractable Fea-
tures. In NIPS, 2010.

[Kappes et al., 2015] J. Kappes, B. Andres, A. Hamprecht,
C. Schn¨orr, S. Nowozin, D. Batra, S. Kim, B. Kausler, T. Kr¨oger,
J. Lellmann, N. Komodakis, B. Savchynskyy, and C. Rother. A
Comparative Study of Modern Inference Techniques for Struc-
tured Discrete Energy Minimization Problems. In IJCV, 2015.
[Kersting et al., 2009] K. Kersting, B. Ahmadi, and S. Natarajan.

Counting Belief Propagation. In UAI, 2009.

[Kersting, 2012] K. Kersting. Lifted Probabilistic Inference.

In

ECAI, 2012.

[Kiddon and Domingos, 2011] C. Kiddon and P. Domingos.
Coarse-to-Fine Inference and Learning for First-Order Proba-
bilistic Models. In AAAI, 2011.

[Kimmig et al., 2015] A. Kimmig, L. Mihalkova, and L. Getoor.
Lifted Graphical Models: A Survey. Machine Learning, 2015.

[Kohli et al., 2013] P. Kohli, A. Osokin, and S. Jegelka. A Prin-
cipled Deep Random Field Model for Image Segmentation. In
CVPR, 2013.

[Lempitsky et al., 2010] V. Lempitsky, C. Rother, S. Roth, and
A. Blake. Fusion Moves for Markov Random Field Optimiza-
tion. In PAMI, Aug 2010.

[Mittal et al., 2014] H. Mittal, P. Goyal, V. Gogate, and P. Singla.
New Rules for Domain Independent Lifted MAP Inference. In
NIPS, 2014.

[Mittal et al., 2015] H. Mittal, A. Mahajan, V. Gogate, and
In NIPS,

P. Singla. Lifted Inference Rules With Constraints.
2015.

[Mladenov et al., 2014] M. Mladenov, K. Kersting, and A. Glober-
son. Efﬁcient Lifting of MAP LP Relaxations Using k-Locality.
In AISTATS, 2014.

[Mozerov and van de Weijer, 2015] M. G. Mozerov and J. van de
Weijer. Accurate Stereo Matching by Two-Step Energy Mini-
mization. IEEE Transactions on Image Processing, March 2015.

[Nath and Domingos, 2010] A. Nath and P. Domingos. Efﬁcient
Lifting for Online Probabilistic Inference. In AAAIWS, 2010.

[Nath and Domingos, 2016] A. Nath and P. Domingos. Learning
Tractable Probabilistic Models for Fault Localization. In AAAI,
2016.

[Niepert, 2012] M. Niepert. Markov Chains on Orbits of Permuta-

tion Groups. In UAI, 2012.

[Noessner et al., 2013] J. Noessner, M. Niepert, and H. Stucken-
schmidt. RockIt: Exploiting Parallelism and Symmetry for MAP
Inference in Statistical Relational Models. In AAAI, 2013.

[Poole, 2003] D. Poole. First-Order Probabilistic Inference. In IJ-

[Sarkhel et al., 2014] S. Sarkhel, D. Venugopal, P. Singla, and
V. Gogate. Lifted MAP inference for Markov logic networks.
In AISTATS, 2014.

[Sarkhel et al., 2015] S. Sarkhel, P. Singla, and V. Gogate. Fast

Lifted MAP Inference via Partitioning. In NIPS, 2015.

[Scharstein and Szeliski, 2002] D. Scharstein and R. Szeliski. A
Taxonomy and Evaluation of Dense Two-Frame Stereo Corre-
spondence Algorithms. In IJCV, 2002.

[Scharstein and Szeliski, 2003] D. Scharstein and R. Szeliski.
High-accuracy Stereo Depth Maps Using Structured Light.
In
CVPR, 2003.

[Singla and Domingos, 2008] P. Singla and P. Domingos. Lifted

First-Order Belief Propagation. In AAAI, 2008.

[Singla et al., 2014] P. Singla, A. Nath, and P. Domingos. Approx-
imate Lifting Techniques for Belief Propagation. In AAAI, 2014.

[Szeliski et al., 2008] R. Szeliski, R. Zabih, D. Scharstein, O. Vek-
sler, V. Kolmogorov, A. Agarwala, M. Tappen, and C. Rother. A
Comparative Study of Energy Minimization Methods for Markov
In PAMI, June
Random Fields with Smoothness-Based Priors.
2008.

[Van den Bergh et al., 2012] M. Van den Bergh, X. Boix, G. Roig,
B. de Capitani, and L. Van Gool. SEEDS: Superpixels Extracted
via Energy-Driven Sampling. In ECCV, 2012.

[Van den Broeck and Darwiche, 2013] G. Van den Broeck and
A. Darwiche. On the Complexity and Approximation of Binary
Evidence in Lifted Inference. In NIPS, 2013.

[Gogate and Domingos, 2011] V. Gogate and P. Domingos. Proba-

bilisitic Theorem Proving. In UAI, 2011.

CAI, 2003.

[Van den Broeck and Niepert, 2015] G. Van den Broeck and
Lifted Probabilistic Inference for Asymmetric

M. Niepert.
Graphical Models. In AAAI, 2015.

[Venugopal and Gogate, 2014] D. Venugopal

and V. Gogate.
Evidence-Based Clustering for Scalable Inference in Markov
Logic. In Joint ECML-KDD, 2014.

[Wei et al., 2016] X. Wei, Q. Yang, Y. Gong, M. Yang, and
N. Ahuja. Superpixel Hierarchy. CoRR, abs/1605.06325, 2016.

Coarse-to-Fine Lifted MAP Inference in Computer Vision

Haroun Habeeb and Ankit Anand and Mausam and Parag Singla
Indian Institute of Technology Delhi
haroun7@gmail.com and {ankit.anand,mausam,parags}@cse.iitd.ac.in

7
1
0
2
 
l
u
J
 
2
2
 
 
]

V
C
.
s
c
[
 
 
1
v
5
6
1
7
0
.
7
0
7
1
:
v
i
X
r
a

Abstract

There is a vast body of theoretical research on
lifted inference in probabilistic graphical models
(PGMs). However, few demonstrations exist where
lifting is applied in conjunction with top of the line
applied algorithms. We pursue the applicability
of lifted inference for computer vision (CV), with
the insight that a globally optimal (MAP) labeling
will likely have the same label for two symmetric
pixels. The success of our approach lies in efﬁ-
ciently handling a distinct unary potential on ev-
ery node (pixel), typical of CV applications. This
allows us to lift the large class of algorithms that
model a CV problem via PGM inference. We pro-
pose a generic template for coarse-to-ﬁne (C2F) in-
ference in CV, which progressively reﬁnes an ini-
tial coarsely lifted PGM for varying quality-time
trade-offs. We demonstrate the performance of
C2F inference by developing lifted versions of two
near state-of-the-art CV algorithms for stereo vi-
sion and interactive image segmentation. We ﬁnd
that, against ﬂat algorithms, the lifted versions have
a much superior anytime performance, without any
loss in ﬁnal solution quality.

1 Introduction
Lifted inference in probabilistic graphical models (PGMs)
refers to the set of the techniques that carry out inference
over groups of random variables (or states) that behave simi-
larly [Jha et al., 2010; Kimmig et al., 2015]. A vast body of
theoretical work develops a variety of lifted inference tech-
niques, both exact (e.g., [Poole, 2003; Braz et al., 2005;
Singla and Domingos, 2008; Kersting, 2012]) and approxi-
mate (e.g., [Singla et al., 2014; Van den Broeck and Niepert,
2015]). Most of these works develop technical ideas appli-
cable to generic subclasses of PGMs, and the accompanying
experiments are aimed at providing ﬁrst proofs of concepts.
However, little work exists on transferring these ideas to the
top domain-speciﬁc algorithms for real-world applications.

Algorithms for NLP, computational biology, and computer
vision (CV) problems make heavy use of PGM machinery
(e.g., [Blei et al., 2003; Friedman, 2004; Szeliski et al.,
2008]). But, they also include signiﬁcant problem-speciﬁc

insights to get high performance. Barring a handful of excep-
tions [Jernite et al., 2015; Nath and Domingos, 2016], lifted
inference hasn’t been applied directly to such algorithms.

We study the potential value of lifting to CV problems such
as image denoising, stereo vision, and image segmentation.
Most CV problems are structured output prediction tasks, typ-
ically assigning a label to each pixel. A large class of solu-
tions are PGM-based: they deﬁne a Markov Random Field
(MRF) that has each pixel as a node, with unary potential that
depends on pixel value, and pairwise neighborhood potentials
that favor similar labels to neighboring pixels.

We see three main challenges in applying existing lifted
inference literature to these problems. First, most existing al-
gorithms focus on computing marginals [Singla and Domin-
gos, 2008; Kersting et al., 2009; Gogate and Domingos, 2011;
Niepert, 2012; Anand et al., 2016; Anand et al., 2017] instead
of MAP inference. Second, among the algorithms perform-
ing lifted MAP [Noessner et al., 2013; Mladenov et al., 2014;
Sarkhel et al., 2014; Mittal et al., 2014], many of the algo-
rithms focus on exact lifting. This breaks the kind of symme-
tries we need to compute since different pixels may not have
exact same neighborhood. Third, the few algorithms that per-
form approximate lifting for MAP, e.g. [Sarkhel et al., 2015],
can’t handle a distinct unary potential on every node. This is
essential for our application since image pixels take ordinal
values in three channels.

In response, we develop an approximate lifted MAP infer-
ence algorithm which can effectively handle unary potentials.
We initialize our algorithm by merging together pixels hav-
ing the same order of top-k labels based on the unary po-
tential values. We then adapt an existing symmetry ﬁnding
algorithm [Kersting et al., 2009] to discover groupings which
also have similar neighborhoods. We refer to our groupings
as lifted pixels. We impose the constraint that all pixels in a
lifted pixel must be assigned the same label. Our approximate
lifting reduces the model size drastically leading to signiﬁ-
cant time savings. Unfortunately, such approximate lifting
could adversely impact solution quality. However, we vary
the degree of approximation in symmetry ﬁnding to output a
sequence of coarse-to-ﬁne models with varying quality-time
trade-offs. By switching between such models, we develop a
coarse-to-ﬁne (C2F) inference procedure applicable to many
CV problems.

We formalize these ideas in a novel template for using

lifted inference in CV. We test C2F lifted inference on two
problems: stereo matching and image segmentation. We start
with one of the best MRF-based solvers each for both prob-
lems – neither of these are vanilla MRF solvers. Mozerov
& Weijer [2015] use a two-way energy minimization to ef-
fectively handle occluded regions in stereo matching. Co-
operative cuts [Kohli et al., 2013] for image segmentation use
concave functions over a predeﬁned set of pixel pairs to cor-
rectly segment images with sharp edges. We implement C2F
inference on top of both these algorithms and ﬁnd that C2F
versions have a strong anytime behavior – given any amount
of inference time, they output a much higher quality (and are
never worse) than their unlifted counterparts, and don’t suffer
any loss in the ﬁnal quality. Overall, our contributions are:

1. We present an approximate lifted MAP algorithm that
can efﬁciently handle a large number of distinct unary
potentials.

2. We develop a novel template for applying lifted infer-
ence in structured prediction tasks in CV. We provide
methods that output progressively ﬁner approx. symme-
tries, leading to a C2F lifted inference procedure.

3. We implement C2F inference over a near state-of-the-
art stereo matching algorithm, and one of the best MRF-
based image segmentation algorithms. We release our
implementation for wider use by the community.1

4. We ﬁnd that C2F has a much superior anytime behav-
ior. For stereo matching it achieves 60% better quality
on average in time-constrained settings. For image seg-
mentation C2F reaches convergence in 33% less time.

2 Background
2.1 Computer Vision Problems as MRFs
Most computer vision problems are structured output predic-
tion problems and their PGM-based solutions often follow
similar formulations. They cast the tasks into the problem
of ﬁnding the lowest energy assignment over grid-structured
MRFs (denoted by G = (X , γ)). The random variables in
these MRFs are the set of pixels X in the input image. Given
a set of labels L : {1, 2, . . . , |L|}, the task of structured output
prediction is to label each pixel X with a label from L. The
MRFs have two kinds of potentials (γ) – unary and higher-
order. Unary potentials are deﬁned over each individual pixel,
and usually incorporate pixel intensity, color, and other pixel
features. Higher order potentials operate over cliques (pairs
or more) of neighboring pixels and typically express some
form of spatial homophily – “neighboring pixels are more
likely to have similar labels.” While the general PGM struc-
ture of various tasks are similar, the speciﬁc potential tables
and label spaces are task-dependent.

The goal is to ﬁnd the MAP assignment over this MRF,
which is equivalent to energy minimization (by deﬁning en-
ergy as negative log of potentials). We denote the negative
log of unary potentials by φ, and that of higher-order poten-
tials by ψ.2 Thus, energy of a complete assignment x ∈ L|X |

1https://github.com/dair-iitd/c2ﬁ4cv/
2In the interest of readability, we say ‘potential’ to mean ‘nega-

tive log of potential’ in the rest of the paper.

can be deﬁned as:

E(x) =

φ(xi) +

ψj(ˆxj)

(1)

(cid:88)

i∈1..|X |

(cid:88)

j

Here ˆxj denotes the assignment x restricted to the set of
variables in the potential ψj. And the output of the algorithm
is the assignment xMAP:

xMAP = arg min
x∈L|X |

E(x)

(2)

The problem is in general intractable. Efﬁcient approxi-
mations exploit special characteristics of potentials like sub-
modularity [Jegelka and Bilmes, 2011], or use variants of
graph cut or loopy belief propagation [Boykov et al., 2001;
Freeman et al., 2000].

2.2 Symmetries in Graphical Models
Lifting an algorithm often requires computing a set of sym-
metries that can be exploited by that algorithm. For PGMs,
two popular methods for symmetry computation are color
passing for computing symmetries of variables [Kersting et
al., 2009], and graph isomorphism for symmetries of states
[Niepert, 2012; Bui et al., 2013]. Since our work is based on
color passing, we explain it in more detail.

Color passing for an MRF operates over a colored bipar-
tite graph containing nodes for all variables and potentials,
and each node is assigned a color. The graph is initialized
as follows: all variables nodes get a common color; all po-
tential nodes with exactly same potential tables are assigned
a unique color. Now, in an iterative color passing scheme,
in each iteration, each variable node passes its color to all
neighboring potential nodes. The potential nodes store in-
coming color signatures in a vector, append their own color
to it, and send the vector back to variable nodes. The variable
nodes stack these incoming vectors. New colors are assigned
to each node based on the set of incoming messages such that
two nodes with same messages are assigned the same unique
color. This process is repeated until convergence, i.e., no fur-
ther change in colors.

A coloring of the bipartite graph deﬁnes a partition of vari-
able nodes such that all nodes of the same color form a par-
tition element. Each iteration of color passing creates suc-
cessively ﬁner partitions, since two variable nodes, once as-
signed different colors, can never get the same color.

3 Lifted Computer Vision Framework
In this section, we will describe our generic template which
can be used to lift a large class of vision applications includ-
ing those in stereo, segmentation etc. Our template can be
seen as transforming the original problem space to a reduced
problem space over which the original inference algorithm
can now be applied much more efﬁciently. Speciﬁcally, our
description in this section is entirely algorithm independent.
We will focus on MAP inference which is the inference
task of choice for most vision applications (refer Section 2).
The key insight in our formulation is based on the realiza-
tion that pixels which are involved in the same (or similar)

kinds of unary and higher order potentials, and have the same
(or similar) neighborhoods, are likely to have the same MAP
value. Therefore, if somehow we could discover such sets
of pixels a priori, we could explicitly enforce these pixels to
have the same value while searching for the solution, substan-
tially reducing the problem size and still preserving the opti-
mal MAP assignment(s). Since in general doing this exactly
may lead to a degenerate network, we do it approximately.
Hence trading-off speed for marginal loss in solution quality.
The loss in solution quality is offset by resorting to coarse-
to-ﬁne inference where we start with a crude approximation,
and gradually make it ﬁner, to guarantee optimality at the end
while still obtaining signiﬁcant gains. We next describe the
details of our approach.

3.1 Obtaining a Reduced Problem

1 , Y P

2 , · · · , Y P

k ⊆ X , Y P
k1

Consider an energy minimization problem over a PGM G =
(X , γ). Let L = {1, 2, · · · , |L|} denote the set of labels
over which variables in the set X can vary. Let Y P =
{Y P
r } denote a partition of X into r disjoint
∩ Y P
subsets, i.e., ∀k, Y P
= ∅ when k1 (cid:54)= k2,
k2
and ∪kY P
k = X . We refer to each Y P
k as a partition element.
Correspondingly, let us deﬁne Y = {Y1, Y2, · · · , Yr} as a set
of partition variables, where there is a one to one correspon-
dence between partition elements and the partition variables
and each partition variable Yk takes values in the set L. Let
part(Xi) denote the partition element to which Xi belongs.
Let ˆXj ⊆ X denote a subset of variables. We say that a par-
k is represented in the set ˆXj if ∃Xi ∈ ˆXj
tition element Y P
s.t. part(Xj) = Yk.
Given a subset of variables ˆXj, let γj( ˆXj) be a potential de-
ﬁned over ˆXj. Let ˆxj denote an assignment to variables in
the set ˆXj. Let ˆxj.elem(i) denote the value taken by a vari-
able Xi in ˆXj. We say that an assignment ˆXj = ˆxj respects
a partition Y P if the variables in ˆXj belonging to the same
partition element have the same label in ˆxj, i.e., part(Xi) =
part(Xi(cid:48)) ⇒ ˆxj.elem(i) = ˆxj.elem(i(cid:48)), ∀Xi, Xi(cid:48) ∈ ˆXj.
Next, we introduce the notion of a reduced potential.

Deﬁnition 3.1 Let X be a set of variables and let Y P de-
note its partition. Given the potential γj( ˆXj), the reduced
potential Γj is deﬁned to be the restriction of γj( ˆXj) to those
labeling assignments of ˆXj which respect the partition Y P .
Equivalently, we can deﬁne the reduced potential Γj( ˆYj) over
the set of partition variables ˆYj which are represented in the
set ˆXj.

For example, consider a potential γ(X1, X2, X3) deﬁned
over three Boolean variables. The table for γ would have
8 entries. Consider the partition Y P = {Y P
2 } where
1 = {X1, X2} and Y P
Y P
2 = {X3}. Then, the reduced poten-
tial Γ is the restriction of γ to those rows in the table where
X1 = X2. Hence Γ has four rows in its table and equivalently
can be thought of deﬁning a potential over the 4 possible com-
binations of Y1 and Y2 variables. We are now ready to deﬁne
a reduced graphical model.

1 , Y P

Deﬁnition 3.2 Let G = (X , γ) represent a PGM. Given a
partition Y P of X , the reduced graphical model G(Y, Γ) is
the graphical model deﬁned over the set of partition variables
Y such that every potential γj ∈ γ in G is replaced by the
corresponding reduced potential Γj ∈ Γ in G.
Let E(x) and E(y) denote the energies of the states x and y
in G and G, respectively. The following theorem relates the
energies of the states in the two graphical models.

Theorem 3.1 For every assignment y of Y in G, there is a
corresponding assignment x of X such that E(y) = E(x).

The theorem can be proved by noting that each potential
Γj( ˆYj) in G was obtained by restricting the original poten-
tial γj( ˆXj) to those assignments where variables in Xj be-
longing to the same partition took the same label. Since this
correspondence is true for every potential in the reduced set,
to obtain the desired state x, for every variable Xi ∈ X we
simply assign it the label of its partition in y.
Corollary 3.1 Let xMAP and yMAP be the MAP states (i.e.
having the minimum energy) for G and G, respectively. Then,
E(yMAP) ≥ E(xMAP).
The process of reduction can be seen as curtailing the en-
tire search space to those assignments where variables in the
same partition take the same label. A reduction in the prob-
lem space will lead to computational gains but might result
in loss of solution quality, where the solution quality can be
captured by the difference between E(yMAP) and E(xMAP).
Therefore, we need to trade-off the balance between the two.
Intuitively, a good problem reduction will keep those vari-
ables in the same partition which are likely to have the same
value in the optimal assignment for the original problem.
How do we ﬁnd such variables without actually solving the
inference task? We will describe one such technique in Sec-
tion 3.3.

There is another perspective.

Instead of solving one re-
duced problem, we can instead work with a series of reduced
problems which successively get closer to the optimal solu-
tion. The initial reductions are coarser and far from optimal,
but can be solved efﬁciently to quickly reach in the region
where the solution lies. The successive iterations can then re-
ﬁne the solution iteratively getting closer to the optimal. This
leads us to the coarse-to-ﬁne inference described next.

3.2 Coarse to Fine Inference
We will deﬁne a framework for C2F (coarse-to-ﬁne) infer-
ence so that we maintain the computational advantage while
still preserving optimality. In the following, for ease of no-
tation, we will drop the superscript P in Y p to denote the
partition of X . Therefore, Y will refer to both the partition as
well as the set of partition variables. Before we describe our
algorithm, let us start with some deﬁnitions.
Deﬁnition 3.3 Let Y and Y (cid:48) be two partitions of X . We
say that Y is coarser than Y (cid:48), denoted as Y (cid:22) Y (cid:48),
if
∀y(cid:48) ∈ Y (cid:48)∃y ∈ Y such that y(cid:48) ⊆ y. We equivalently say
that Y (cid:48) is ﬁner than Y.
It is easy to see that X deﬁnes a partition of itself which is the
ﬁnest among all partitions, i.e., ∀Y such that Y is a partition

of X , Y (cid:22) X . We also refer it to as the degenerate partition.
For ease of notation, we will denote the ﬁnest partition by
Y ∗ (same as X ). We will refer to the corresponding PGM as
G∗ (same as G). Next, we state a theorem which relates two
partitions with each other.
Lemma 1 Let Y and Y (cid:48) be two partitions of X such that Y (cid:22)
Y (cid:48). Then Y (cid:48) can be seen as a partition of the set Y.

The proof of this lemma is straightforward and is omitted
due to lack of space. Consider a set Y of coarse to ﬁne
partitions given as Y 0 (cid:22) Y 1, · · · , (cid:22), Y t, (cid:22), · · · , Y ∗. Let
Gt, E t, yt
M AP respectively denote the reduced problem, en-
ergy function and MAP assignment for the partition Y t. Us-
ing Lemma 1, Y t+1 is a partition of Y t. Then, using Theo-
rem 3.1, we have for every assignment yt to variables in Y t,
there is an assignment yt+1 to variables in Y t+1 such that
E t(yt) = E t+1(yt+1). Also, using Corollary 3.1, we have
∀t E t(yt
MAP). Together, these two state-
ments imply that starting from the coarsest partition, we can
gradually keep on improving the solution as we move to ﬁner
partitions.

MAP) ≥ E t+1(yt+1

Our C2F set-up assumes an iterative MAP inference algo-
rithm A which has the anytime property i.e., can produce so-
lutions of increasing quality with time. C2F Function (see
Algorithm 1) takes 3 inputs: a set of C2F partitions Y, infer-
ence algorithm A, and a stopping criteria C. The algorithm A
in turn takes three inputs: PGM Gt, starting assignment yt,
stopping criteria C. A outputs an approximation to the MAP
solution once the stopping criteria C is met. Starting with the
coarsest partition (t = 0 in line 2), a start state is picked for
the coarsest problem to be solved (line 3). In each iteration
(line 4), C2F ﬁnds the MAP estimate for the current problem
(Gt) using algorithm A (line 5). This solution is then mapped
to a same energy solution of the next ﬁner partition (line 6)
which becomes the starting state for the next run of A. The
solution is thus successively reﬁned in each iteration. The
process is repeated until we reach the ﬁnest level of partition.
In the end, A is run on the ﬁnest partition and the resultant
solution is output (lines 9,10). Since the last partition in the
set is the original problem G∗, optimality with respect to A is
guaranteed.

Next, we describe how to use the color passing algorithm
(Section 2) to get a series of partitions which get successively
ﬁner. Our C2F algorithm can then be applied on this set of
partitions to get anytime solutions of high quality while being
computationally efﬁcient.

Algorithm 1 Coarse-to-Fine Lifted MAP Algorithm
1:C2F Lifted MAP(C2F Partitions Y, Algo A,Criteria C)
2: t = 0; T = |Y|;
3: yt = getInitState(Gt);
4: While (t < T );
MAP = A(Gt, yt, C);
yt
5:
yt+1 = getEquivAssignment(Y t, Y t+1, yt
6:
t = t + 1;
7:
8: END While
9: yT
10: return yT

MAP = A(GT , yT , C);
MAP

MAP);

3.3 C2F Partitioning for Computer Vision
We now adapt the general color passing algorithm to MRFs
for CV problems. Unfortunately, unary potentials make color
passing highly ineffective. Different pixels have different
RGB values and intensities, leading to almost every pixel get-
ting a different unary potential. Naive application of color
passing splits almost all variables into their own partitions,
and lifting offers little value.

A natural approximation is to deﬁne a threshold, such that
two unary potentials within that threshold be initialized with
the same color. Our experiments show limited success with
this scheme because because two pixels may have the same
label even when their actual unary potentials are very differ-
ent. What is more important is relative importance given to
each label than the actual potential value.

In response, we adapt color passing for CV by initializing
it as before, but with one key change: we initialize two unary
potential nodes with the same color if their lowest energy la-
bels have the same order for the top NL labels (we call this
unary split threshold). Experiments reveal that this approxi-
mation leads to effective partitions for lifted inference.

Finally, we can easily construct a sequence of coarse-to-
ﬁne partitions in the natural course of color passing’s execu-
tion – every iteration of color passing creates a ﬁner partition.
Moreover, as an alternative approach, we may also increase
NL. In our implementations, we intersperse the two, i.e., be-
fore every next step we pick one of two choices: either, we
run another iteration of color passing; or, we increase NL by
one, and split each variable partition based on the N th
L lowest
energy labels of its constituent variables.

We parameterize CP (NL, Niter) to denote the partition
from the current state of color passing, which has been run
till Niter iterations and unary split threshold is NL.
It
is easy to prove that another iteration of color passing or
splitting by increasing NL as above leads to a ﬁner par-
I.e., CP (NL, Niter) (cid:22) CP (NL + 1, Niter) and
tition.
CP (NL, Niter) (cid:22) CP (NL, Niter + 1). We refer to each
element of a partition of variables as a lifted pixel, since it is
a subset of pixels.

4 Lifted Inference for Stereo Matching
We ﬁrst demonstrate the value of lifted inference in the con-
text of stereo matching [Scharstein and Szeliski, 2002].
It
aims to ﬁnd pixel correspondences in a set of images of the
same scene, which can be used to further estimate the 3D
scene. Formally, two images I l and I r corresponding to
images of the scene from a left camera and a right cam-
era are taken such that both cameras are at same horizon-
tal level. The goal is to compute a disparity labeling Dl
for every pixel X = (a, b) such that I l[a][b] corresponds to
I r[a−Dl[a][b]][b]. We build a lifted version of TSGO [Moze-
rov and van de Weijer, 2015], as it is MRF-based and ranks
2nd on the Middlebury Stereo Evaluation Version 2 leader-
board.3
Background on TSGO: TSGO treats stereo matching as
a two-step energy minimization, where the ﬁrst step is on a

3http://vision.middlebury.edu/stereo/eval/

(a)

(b)

(c)

Figure 1: (a) Average (normalized) energy vs. inference time (b) Average pixel error vs. time. C2F TSGO achieves roughly 60% reduction in
time for reaching the optima. It has best anytime performance compared to vanilla TSGO and static lifted versions. (c) Average (normalized)
energy vs. time for different thresholding values and CP partitions. Plots with the same marker have MRFs of similar sizes.

(a)

(b)

(c)

(d)

(e)

Figure 2: Qualitative results for Doll image at convergence. C2F-TSGO is similar to base TSGO.(a) Left and Right Images (b) Ground
Truth (c) Disparity Map by TSGO (d) Disparity Map by C2F TSGO (e) Each colored region (other than black) is one among the 10 largest
partition elements from CP(1,1). Each color represents one partition element. Partition elements form non-contiguous regions

fully connected MRF with pairwise potentials and the second
is on a conventional locally connected MRF. Lack of space
precludes a detailed description of the ﬁrst step. At the high
level, TSGO runs one iteration of message passing on fully
connected MRF, computes marginals of each pixel X, which
act as unary potentials φ(X) for the MRF of second step.

The pairwise potential ψ used in step two is ψ(X, X (cid:48)) =
w(X, X (cid:48))ϕ(X, X (cid:48)), where ϕ(X, X (cid:48)) is a truncated linear
function of (cid:107)X − X (cid:48)(cid:107), and w(X, X (cid:48)) takes one of three dis-
tinct values depending on color difference between pixels.
The MAP assignment xMAP computes the lowest energy as-
signment of disparities Dl for every pixel for this MRF.
Lifted TSGO: Since step two is costlier, we build its lifted
version as discussed in previous section. For color passing,
two unary potential nodes are initialized with the same color
if their lowest energy labels exactly match (NL = 1). Other
initializations are consistent with original color passing for
general MRFs. A sequence of coarse-to-ﬁne models is out-
putted as per Section 3.3. C2F TSGO uses outputs from the
sequence CP (1, 1), CP (2, 1), CP (3, 1) and then reﬁnes to
the original MRF. Model reﬁnement is triggered whenever
energy hasn’t decreased in the last four iterations of alpha ex-
pansion (this becomes the stopping criteria C in Algorithm
1).
Experiments: Our experiments build on top of the exist-
ing TSGO implementation4, but we change the minimization
algorithm in step two to alpha expansion fusion [Lempitsky
et al., 2010] from OpenGM2 library [Andres et al., 2010;
Kappes et al., 2015], as it improves the speed of the base

4http://www.cvc.uab.es/∼mozerov/Stereo/

implementation. We use the benchmark Middlebury Stereo
datasets of 2003, 2005 and 2006 [Scharstein and Szeliski,
2003; Hirschmuller and Scharstein, 2007]. For the 2003
dataset, quarter-size images are used and for others, third-size
images are used. The label space is of size 85 (85 distinct dis-
parity labels).

We

our

compare

coarse-to-ﬁne

TSGO (using
CP (NL, Niter) partitions) against vanilla TSGO. Fig-
ures 1(a,b) show the aggregate plots of energy (and error)
vs.
time. We observe that C2F TSGO reaches the same
optima as TSGO, but in less than half the time. It has a much
superior anytime performance – if inference time is given as
a deadline, C2F TSGO obtains 59.59% less error on average
over randomly sampled deadlines. We also eyeball the out-
puts of C2F TSGO and TSGO and ﬁnd them to be visually
similar. Figure 2 shows a sample qualitative comparison.
Figure 2(e) shows ﬁve of the ten largest partition elements in
the partition from CP (1, 1). Clearly, the partition elements
formed are not contiguous, and seem to capture variables that
are likely to get the same assignment. This underscores the
value of our lifting framework for CV problems.

We also compare our CP (NL, Niter) partitioning strat-
egy with threshold partitioning discussed in Section 3.3. We
merge two pixels in thresholding scheme if the L1-norm dis-
tance of their unary potentials is less than a threshold. For
each partition induced by our approach, we ﬁnd a value of
threshold that has roughly the same number of lifted pix-
els. Figure 1(c) shows that partitions based on CP (1, 1) and
CP (3, 1) converges to a much lower energy quickly com-
pared to the corresponding threshold values (T hr = 50 and
T hr = 1 respectively). For CP (2, 1), convergence is slower

compared to corresponding threshold (T hr = 5) but eventu-
ally CP (2, 1) has signiﬁcantly better quality.

5 Lifted Inference for Image Segmentation
We now demonstrate the general nature of our lifted CV
framework by applying it to a second task. We choose multi-
label interactive image segmentation, where the goal is to
segment an image I based on a seed labeling (true labels
for a few pixels) provided as input. Like many other CV
problems, this also has an MRF-based solution, with the best
label-assignment generally obtained by MAP inference using
graph cuts or loopy belief propagation [Boykov et al., 2001;
Szeliski et al., 2008].

However, MRFs with only pairwise potentials are known to
suffer from short-boundary bias – they prefer segmentations
with shorter boundaries, because pairwise potentials penalize
every pair of boundary pixels. This leads to incorrect labeling
for sharp edge objects. Kohli et al. [2013] use CoGC, coop-
erative graph cuts [Jegelka and Bilmes, 2011], to develop one
of the best MRF-based solvers that overcome this bias.
Background on CoGC: Traditional MRFs linearly penalize
the number of label discontinuities at edges (boundary pixel
pairs), but CoGC penalizes the number of types of label dis-
continuities through the use of a concave energy function over
groups of ordered edges. It ﬁrst clusters all edges on the ba-
sis of color differences, and later applies a concave function
separately over the number of times a speciﬁc discontinuity
type is present in each edge group g ∈ G. Their carefully
engineered CoGC energy function is as follows:

E(x) =

φi(xi)+

w(x, x(cid:48)).I(x = l, x(cid:48) (cid:54)= l)

|X |
(cid:88)

i=1



F



(cid:88)

(cid:88)

(cid:88)

g∈G

l∈L

(x,x(cid:48))∈g





where unary potentials φ depend on colors of seed pixels, F
is a concave function, I the indicator function, and w(x, x(cid:48))
depends on the color difference between x, x(cid:48).
Intuitively,
F collects all edges with similar discontinuities and penal-
izes them sub-linearly, thus reducing the short-boundary bias
in the model. The usage of a concave function makes the
MRF higher order with cliques over edge groups. However,
the model is shown to reduce to a pairwise hierarchical MRF
through the addition of auxiliary variables.
Lifted CoGC: CoGC is lifted using the framework of Sec-
tion 3, with one additional change. We cluster edge groups
using color difference and the position of the edge. Edge
groups that are formed only on the basis of color difference
make the error of grouping different segment’s boundaries
into a single group. For e.g., it erroneously cluster boundaries
between white cow and grass, and sky and grass together in
the top image in Figure 3.

2 (cid:101), 2), CP ((cid:100) L

Coarse-to-ﬁne partitions are obtained by the method de-
scribed in Section 3.3. C2F CoGC uses outputs from the se-
quence CP ((cid:100) L
2 (cid:101), 3) before reﬁning to the orig-
inal MRF. Model reﬁnement is triggered if energy has not
reduced over the last |L| iterations.
Experiments: Our experiments use the implementation of
Cooperative Graph Cuts as provided by [Kohli et al., 2013].5

5Available at https://github.com/aosokin/coopCuts CVPR2013

Energy minimization is performed using alpha expansion
[Boykov et al., 2001]. The implementation of CoGC per-
forms a greedy descent on auxiliary variables while perform-
ing alpha expansion on the remaining variables, as described
in Kohli et. al. [2013]. The dataset used is provided with the
implementation. It is a part of the MSRC V2 dataset.6.

Figure 3 shows three individual energy vs. time plots. Re-
sults on other images are similar. We ﬁnd that C2F CoGC
algorithm converges to the same energy as CoGC in about
two-thirds the time on average. Overall, C2F CoGC achieves
a much better anytime performance than other lifted and un-
lifted CoGC.

Similar to Section 4, reﬁned partitions attain better quality
than coarser ones at the expense of time. Since the implemen-
tation performs a greedy descent over auxiliary variables, re-
ﬁnement of current partition also resets the auxiliary variables
to the last value that produced a change. Notice that energy
minimization on output of CP (2, 3) attains a lower energy
than on CP (3, 2). This observation drives our decision to
reﬁne by increasing Niter. Qualitatively, C2F CoGC pro-
duces the same labeling as CoGC. Finally, similar to stereo
matching, partitions based on thresholding scheme perform
signiﬁcantly worse compared to CP (NL, Niter) for image
segmentation as well.

6 Related Work

There is a large body of work on exact
lifting, both
marginal [Kersting et al., 2009; Gogate and Domingos, 2011;
Niepert, 2012; Mittal et al., 2015] and MAP [Kersting
et al., 2009; Gogate and Domingos, 2011; Niepert, 2012;
Sarkhel et al., 2014; Mittal et al., 2014], which is not di-
rectly applicable to our setting. There is some recent work
on approximate lifting [Van den Broeck and Darwiche, 2013;
Venugopal and Gogate, 2014; Singla et al., 2014; Sarkhel et
al., 2015; Van den Broeck and Niepert, 2015] but it’s fo-
cus is on marginal inference whereas we are interested in
lifted MAP. Further, this work can’t handle a distinct unary
potential on every node. An exception is work by Bui et
al. [2012] which explicitly deals with lifting in presence of
distinct unary potentials. Unfortunately, they make a very
strong assumption of exchangeability in the absence of unar-
ies which does not hold true in our setting since each pixel
has its own unique neighborhood.

Work by Sarkhel et al. [2015] is probably the closest to our
work. They design a C2F hierarchy to cluster constants for
approximate lifted MAP inference in Markov logic. In con-
trast, we partition ground atoms in a PGM. Like other work
on approximate lifting, they can’t handle distinct unary po-
tentials. Furthermore, they assume that their theory is pro-
vided in a normal form, i.e., without evidence, which can be
a severe restriction for most practical applications. Kiddon &
Domingos [2011] also propose C2F inference for an underly-
ing Markov logic theory. They use a hierarchy of partitions
based on a pre-speciﬁed ontology. CV does not have any such

6Available

at

https://www.microsoft.com/en-us/research/

project/image-understanding/?from=http%3A%2F%2Fresearch.
microsoft.com%2Fvision%2Fcambridge%2Frecognition%2F

Figure 3: (a-c) Qualitative Results for Segmentation. C2F has quality similar to CoGC algorithm (a) Original Image (b) Segmentation by
CoGC (c) Segmentation by C2F CoGC (d) C2F CoGC has lower energy compared to CoGC and other lifted variant at all times

ontology available, and needs to discover partitions using the
PGM directly.

Nath & Domingos [2010] exploit (approximate) lifted in-
ference for video segmentation. They experiment on a spe-
ciﬁc video problem (different from ours), and they only com-
pare against vanilla BP. Their initial partitioning scheme is
similar to our thresholding approach, which does not work
well in our experiments.

In computer vision, a popular approach to reduce the com-
plexity of inference is to use superpixels [Achanta et al.,
2012; Van den Bergh et al., 2012]. Superpixels are obtained
by merging neighboring nodes that have similar character-
istics. All pixel nodes in the same superpixel are assigned
the same value during MAP inference. SLIC [Achanta et al.,
2012] is one of the most popular algorithms for discovering
superpixels. Our approach differs from SLIC in some signiﬁ-
cant ways. First, their superpixels are local in nature whereas
our algorithm can merge pixels that are far apart. This can
help in merging two disconnected regions of the same object
in a single lifted pixel. Second, they obtain superpixels inde-
pendent of the inference algorithm, whereas we tightly inte-
grate our lifting with the underlying inference algorithm. This
can potentially lead to discovery of better partitions; indeed,
this helped us tremendously in image segmentation. Third,
they do not provide a C2F version of their algorithm and
we did not ﬁnd it straightforward to extend their approach
to discover successively ﬁner partitions. There is some recent
work [Wei et al., 2016] which addresses last two of these
challenges by introducing a hierarchy of superpixels. In our
preliminary experiments, we found that SLIC and superpixel
hierarchy perform worse than our lifting approach. Perform-
ing more rigorous comparisons is a direction for future work.

7 Conclusion and Future Work
We develop a generic template for applying lifted inference
to structured output prediction tasks in computer vision. We
show that MRF-based CV algorithms can be lifted at different
levels of abstraction, leading to methods for coarse to ﬁne
inference over a sequence of lifted models. We test our ideas
on two different CV tasks of stereo matching and interactive
image segmentation. We ﬁnd that C2F lifting is vastly more
efﬁcient than unlifted algorithms on both tasks obtaining a
superior anytime performance, and without any loss in ﬁnal
solution quality. To the best of our knowledge, this is the ﬁrst
demonstration of lifted inference in conjunction with top of
the line task-speciﬁc algorithms. Although we restrict to CV
in this work, we believe that our ideas are general and can
be adapted to other domains such as NLP, and computational
biology. We plan to explore this in the future.

Acknowledgements
We thank anonymous reviewers for their comments and sug-
gestions. Ankit Anand is being supported by the TCS Re-
search Scholars Program. Mausam is being supported by
grants from Google and Bloomberg. Parag Singla is being
supported by a DARPA grant funded under the Explainable
AI (XAI) program. Both Mausam and Parag Singla are being
supported by the Visvesvaraya Young Faculty Fellowships by
Govt. of India. Any opinions, ﬁndings, conclusions or rec-
ommendations expressed in this paper are those of the authors
and do not necessarily reﬂect the views or ofﬁcial policies, ei-
ther expressed or implied, of the funding agencies.

References
[Achanta et al., 2012] R. Achanta, A. Shaji, K. Smith, A. Lucchi,
P. Fua, and S. Ssstrunk. SLIC Superpixels Compared to State-of-

the-Art Superpixel Methods. In PAMI, Nov 2012.

[Anand et al., 2016] A. Anand, A. Grover, Mausam, and P. Singla.
Contextual Symmetries in Probabilistic Graphical Models. In IJ-
CAI, 2016.

[Anand et al., 2017] A. Anand, R. Noothigattu, P. Singla, and
Mausam. Non-Count Symmetries in Boolean & Multi-Valued
Prob. Graphical Models. In AISTATS, 2017.

[Andres et al., 2010] B. Andres,

J. H. Kappes, U. K¨othe,
C. Schn¨orr, and F. A. Hamprecht. An Empirical Comparison of
Inference Algorithms for Graphical Models with Higher Order
Factors Using OpenGM. In Pattern Recognition. 2010.

[Blei et al., 2003] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet

Allocation. JMLR, 3, March 2003.

[Boykov et al., 2001] Y. Boykov, O. Veksler, and R. Zabih. Fast
In PAMI,

Approximate Energy Minimization via Graph Cuts.
23(11), November 2001.

[Braz et al., 2005] R. Braz, E. Amir, and D. Roth. Lifted First-

Order Probabilistic Inference. In IJCAI, 2005.

[Bui et al., 2012] H. Bui, T. Huynh, and R. De Salvo Braz. Exact
Lifted Inference with Distinct Soft Evidence on Every Object. In
AAAI, 2012.

[Bui et al., 2013] H. Bui, T. Huynh, and S. Riedel. Automorphism
Groups of Graphical Models and Lifted Variational Inference. In
UAI, 2013.

[Freeman et al., 2000] W. Freeman, E. Pasztor, and O. Carmichael.

Learning Low-Level Vision. In IJCV, 40, 2000.

[Friedman, 2004] N. Friedman. Inferring Cellular Networks using

Probabilistic Graphical Models. Science, 303, 2004.

[Hirschmuller and Scharstein, 2007] H.

and
Evaluation of Cost Functions for Stereo

Hirschmuller

D. Scharstein.
Matching. In CVPR, 2007.

[Jegelka and Bilmes, 2011] S. Jegelka and J. Bilmes. Submodu-
larity Beyond Submodular Energies: Coupling Edges in Graph
Cuts. In CVPR, 2011.

[Jernite et al., 2015] Y. Jernite, A. Rush, and D. Sontag. A Fast
Variational Approach for Learning Markov Random Field Lan-
guage Models. In ICML, 2015.

[Jha et al., 2010] A. Jha, V. Gogate, A. Meliou, and D. Suciu.
Lifted Inference Seen from the Other Side : The Tractable Fea-
tures. In NIPS, 2010.

[Kappes et al., 2015] J. Kappes, B. Andres, A. Hamprecht,
C. Schn¨orr, S. Nowozin, D. Batra, S. Kim, B. Kausler, T. Kr¨oger,
J. Lellmann, N. Komodakis, B. Savchynskyy, and C. Rother. A
Comparative Study of Modern Inference Techniques for Struc-
tured Discrete Energy Minimization Problems. In IJCV, 2015.
[Kersting et al., 2009] K. Kersting, B. Ahmadi, and S. Natarajan.

Counting Belief Propagation. In UAI, 2009.

[Kersting, 2012] K. Kersting. Lifted Probabilistic Inference.

In

ECAI, 2012.

[Kiddon and Domingos, 2011] C. Kiddon and P. Domingos.
Coarse-to-Fine Inference and Learning for First-Order Proba-
bilistic Models. In AAAI, 2011.

[Kimmig et al., 2015] A. Kimmig, L. Mihalkova, and L. Getoor.
Lifted Graphical Models: A Survey. Machine Learning, 2015.

[Kohli et al., 2013] P. Kohli, A. Osokin, and S. Jegelka. A Prin-
cipled Deep Random Field Model for Image Segmentation. In
CVPR, 2013.

[Lempitsky et al., 2010] V. Lempitsky, C. Rother, S. Roth, and
A. Blake. Fusion Moves for Markov Random Field Optimiza-
tion. In PAMI, Aug 2010.

[Mittal et al., 2014] H. Mittal, P. Goyal, V. Gogate, and P. Singla.
New Rules for Domain Independent Lifted MAP Inference. In
NIPS, 2014.

[Mittal et al., 2015] H. Mittal, A. Mahajan, V. Gogate, and
In NIPS,

P. Singla. Lifted Inference Rules With Constraints.
2015.

[Mladenov et al., 2014] M. Mladenov, K. Kersting, and A. Glober-
son. Efﬁcient Lifting of MAP LP Relaxations Using k-Locality.
In AISTATS, 2014.

[Mozerov and van de Weijer, 2015] M. G. Mozerov and J. van de
Weijer. Accurate Stereo Matching by Two-Step Energy Mini-
mization. IEEE Transactions on Image Processing, March 2015.

[Nath and Domingos, 2010] A. Nath and P. Domingos. Efﬁcient
Lifting for Online Probabilistic Inference. In AAAIWS, 2010.

[Nath and Domingos, 2016] A. Nath and P. Domingos. Learning
Tractable Probabilistic Models for Fault Localization. In AAAI,
2016.

[Niepert, 2012] M. Niepert. Markov Chains on Orbits of Permuta-

tion Groups. In UAI, 2012.

[Noessner et al., 2013] J. Noessner, M. Niepert, and H. Stucken-
schmidt. RockIt: Exploiting Parallelism and Symmetry for MAP
Inference in Statistical Relational Models. In AAAI, 2013.

[Poole, 2003] D. Poole. First-Order Probabilistic Inference. In IJ-

[Sarkhel et al., 2014] S. Sarkhel, D. Venugopal, P. Singla, and
V. Gogate. Lifted MAP inference for Markov logic networks.
In AISTATS, 2014.

[Sarkhel et al., 2015] S. Sarkhel, P. Singla, and V. Gogate. Fast

Lifted MAP Inference via Partitioning. In NIPS, 2015.

[Scharstein and Szeliski, 2002] D. Scharstein and R. Szeliski. A
Taxonomy and Evaluation of Dense Two-Frame Stereo Corre-
spondence Algorithms. In IJCV, 2002.

[Scharstein and Szeliski, 2003] D. Scharstein and R. Szeliski.
High-accuracy Stereo Depth Maps Using Structured Light.
In
CVPR, 2003.

[Singla and Domingos, 2008] P. Singla and P. Domingos. Lifted

First-Order Belief Propagation. In AAAI, 2008.

[Singla et al., 2014] P. Singla, A. Nath, and P. Domingos. Approx-
imate Lifting Techniques for Belief Propagation. In AAAI, 2014.

[Szeliski et al., 2008] R. Szeliski, R. Zabih, D. Scharstein, O. Vek-
sler, V. Kolmogorov, A. Agarwala, M. Tappen, and C. Rother. A
Comparative Study of Energy Minimization Methods for Markov
In PAMI, June
Random Fields with Smoothness-Based Priors.
2008.

[Van den Bergh et al., 2012] M. Van den Bergh, X. Boix, G. Roig,
B. de Capitani, and L. Van Gool. SEEDS: Superpixels Extracted
via Energy-Driven Sampling. In ECCV, 2012.

[Van den Broeck and Darwiche, 2013] G. Van den Broeck and
A. Darwiche. On the Complexity and Approximation of Binary
Evidence in Lifted Inference. In NIPS, 2013.

[Gogate and Domingos, 2011] V. Gogate and P. Domingos. Proba-

bilisitic Theorem Proving. In UAI, 2011.

CAI, 2003.

[Van den Broeck and Niepert, 2015] G. Van den Broeck and
Lifted Probabilistic Inference for Asymmetric

M. Niepert.
Graphical Models. In AAAI, 2015.

[Venugopal and Gogate, 2014] D. Venugopal

and V. Gogate.
Evidence-Based Clustering for Scalable Inference in Markov
Logic. In Joint ECML-KDD, 2014.

[Wei et al., 2016] X. Wei, Q. Yang, Y. Gong, M. Yang, and
N. Ahuja. Superpixel Hierarchy. CoRR, abs/1605.06325, 2016.

Coarse-to-Fine Lifted MAP Inference in Computer Vision

Haroun Habeeb and Ankit Anand and Mausam and Parag Singla
Indian Institute of Technology Delhi
haroun7@gmail.com and {ankit.anand,mausam,parags}@cse.iitd.ac.in

7
1
0
2
 
l
u
J
 
2
2
 
 
]

V
C
.
s
c
[
 
 
1
v
5
6
1
7
0
.
7
0
7
1
:
v
i
X
r
a

Abstract

There is a vast body of theoretical research on
lifted inference in probabilistic graphical models
(PGMs). However, few demonstrations exist where
lifting is applied in conjunction with top of the line
applied algorithms. We pursue the applicability
of lifted inference for computer vision (CV), with
the insight that a globally optimal (MAP) labeling
will likely have the same label for two symmetric
pixels. The success of our approach lies in efﬁ-
ciently handling a distinct unary potential on ev-
ery node (pixel), typical of CV applications. This
allows us to lift the large class of algorithms that
model a CV problem via PGM inference. We pro-
pose a generic template for coarse-to-ﬁne (C2F) in-
ference in CV, which progressively reﬁnes an ini-
tial coarsely lifted PGM for varying quality-time
trade-offs. We demonstrate the performance of
C2F inference by developing lifted versions of two
near state-of-the-art CV algorithms for stereo vi-
sion and interactive image segmentation. We ﬁnd
that, against ﬂat algorithms, the lifted versions have
a much superior anytime performance, without any
loss in ﬁnal solution quality.

1 Introduction
Lifted inference in probabilistic graphical models (PGMs)
refers to the set of the techniques that carry out inference
over groups of random variables (or states) that behave simi-
larly [Jha et al., 2010; Kimmig et al., 2015]. A vast body of
theoretical work develops a variety of lifted inference tech-
niques, both exact (e.g., [Poole, 2003; Braz et al., 2005;
Singla and Domingos, 2008; Kersting, 2012]) and approxi-
mate (e.g., [Singla et al., 2014; Van den Broeck and Niepert,
2015]). Most of these works develop technical ideas appli-
cable to generic subclasses of PGMs, and the accompanying
experiments are aimed at providing ﬁrst proofs of concepts.
However, little work exists on transferring these ideas to the
top domain-speciﬁc algorithms for real-world applications.

Algorithms for NLP, computational biology, and computer
vision (CV) problems make heavy use of PGM machinery
(e.g., [Blei et al., 2003; Friedman, 2004; Szeliski et al.,
2008]). But, they also include signiﬁcant problem-speciﬁc

insights to get high performance. Barring a handful of excep-
tions [Jernite et al., 2015; Nath and Domingos, 2016], lifted
inference hasn’t been applied directly to such algorithms.

We study the potential value of lifting to CV problems such
as image denoising, stereo vision, and image segmentation.
Most CV problems are structured output prediction tasks, typ-
ically assigning a label to each pixel. A large class of solu-
tions are PGM-based: they deﬁne a Markov Random Field
(MRF) that has each pixel as a node, with unary potential that
depends on pixel value, and pairwise neighborhood potentials
that favor similar labels to neighboring pixels.

We see three main challenges in applying existing lifted
inference literature to these problems. First, most existing al-
gorithms focus on computing marginals [Singla and Domin-
gos, 2008; Kersting et al., 2009; Gogate and Domingos, 2011;
Niepert, 2012; Anand et al., 2016; Anand et al., 2017] instead
of MAP inference. Second, among the algorithms perform-
ing lifted MAP [Noessner et al., 2013; Mladenov et al., 2014;
Sarkhel et al., 2014; Mittal et al., 2014], many of the algo-
rithms focus on exact lifting. This breaks the kind of symme-
tries we need to compute since different pixels may not have
exact same neighborhood. Third, the few algorithms that per-
form approximate lifting for MAP, e.g. [Sarkhel et al., 2015],
can’t handle a distinct unary potential on every node. This is
essential for our application since image pixels take ordinal
values in three channels.

In response, we develop an approximate lifted MAP infer-
ence algorithm which can effectively handle unary potentials.
We initialize our algorithm by merging together pixels hav-
ing the same order of top-k labels based on the unary po-
tential values. We then adapt an existing symmetry ﬁnding
algorithm [Kersting et al., 2009] to discover groupings which
also have similar neighborhoods. We refer to our groupings
as lifted pixels. We impose the constraint that all pixels in a
lifted pixel must be assigned the same label. Our approximate
lifting reduces the model size drastically leading to signiﬁ-
cant time savings. Unfortunately, such approximate lifting
could adversely impact solution quality. However, we vary
the degree of approximation in symmetry ﬁnding to output a
sequence of coarse-to-ﬁne models with varying quality-time
trade-offs. By switching between such models, we develop a
coarse-to-ﬁne (C2F) inference procedure applicable to many
CV problems.

We formalize these ideas in a novel template for using

lifted inference in CV. We test C2F lifted inference on two
problems: stereo matching and image segmentation. We start
with one of the best MRF-based solvers each for both prob-
lems – neither of these are vanilla MRF solvers. Mozerov
& Weijer [2015] use a two-way energy minimization to ef-
fectively handle occluded regions in stereo matching. Co-
operative cuts [Kohli et al., 2013] for image segmentation use
concave functions over a predeﬁned set of pixel pairs to cor-
rectly segment images with sharp edges. We implement C2F
inference on top of both these algorithms and ﬁnd that C2F
versions have a strong anytime behavior – given any amount
of inference time, they output a much higher quality (and are
never worse) than their unlifted counterparts, and don’t suffer
any loss in the ﬁnal quality. Overall, our contributions are:

1. We present an approximate lifted MAP algorithm that
can efﬁciently handle a large number of distinct unary
potentials.

2. We develop a novel template for applying lifted infer-
ence in structured prediction tasks in CV. We provide
methods that output progressively ﬁner approx. symme-
tries, leading to a C2F lifted inference procedure.

3. We implement C2F inference over a near state-of-the-
art stereo matching algorithm, and one of the best MRF-
based image segmentation algorithms. We release our
implementation for wider use by the community.1

4. We ﬁnd that C2F has a much superior anytime behav-
ior. For stereo matching it achieves 60% better quality
on average in time-constrained settings. For image seg-
mentation C2F reaches convergence in 33% less time.

2 Background
2.1 Computer Vision Problems as MRFs
Most computer vision problems are structured output predic-
tion problems and their PGM-based solutions often follow
similar formulations. They cast the tasks into the problem
of ﬁnding the lowest energy assignment over grid-structured
MRFs (denoted by G = (X , γ)). The random variables in
these MRFs are the set of pixels X in the input image. Given
a set of labels L : {1, 2, . . . , |L|}, the task of structured output
prediction is to label each pixel X with a label from L. The
MRFs have two kinds of potentials (γ) – unary and higher-
order. Unary potentials are deﬁned over each individual pixel,
and usually incorporate pixel intensity, color, and other pixel
features. Higher order potentials operate over cliques (pairs
or more) of neighboring pixels and typically express some
form of spatial homophily – “neighboring pixels are more
likely to have similar labels.” While the general PGM struc-
ture of various tasks are similar, the speciﬁc potential tables
and label spaces are task-dependent.

The goal is to ﬁnd the MAP assignment over this MRF,
which is equivalent to energy minimization (by deﬁning en-
ergy as negative log of potentials). We denote the negative
log of unary potentials by φ, and that of higher-order poten-
tials by ψ.2 Thus, energy of a complete assignment x ∈ L|X |

1https://github.com/dair-iitd/c2ﬁ4cv/
2In the interest of readability, we say ‘potential’ to mean ‘nega-

tive log of potential’ in the rest of the paper.

can be deﬁned as:

E(x) =

φ(xi) +

ψj(ˆxj)

(1)

(cid:88)

i∈1..|X |

(cid:88)

j

Here ˆxj denotes the assignment x restricted to the set of
variables in the potential ψj. And the output of the algorithm
is the assignment xMAP:

xMAP = arg min
x∈L|X |

E(x)

(2)

The problem is in general intractable. Efﬁcient approxi-
mations exploit special characteristics of potentials like sub-
modularity [Jegelka and Bilmes, 2011], or use variants of
graph cut or loopy belief propagation [Boykov et al., 2001;
Freeman et al., 2000].

2.2 Symmetries in Graphical Models
Lifting an algorithm often requires computing a set of sym-
metries that can be exploited by that algorithm. For PGMs,
two popular methods for symmetry computation are color
passing for computing symmetries of variables [Kersting et
al., 2009], and graph isomorphism for symmetries of states
[Niepert, 2012; Bui et al., 2013]. Since our work is based on
color passing, we explain it in more detail.

Color passing for an MRF operates over a colored bipar-
tite graph containing nodes for all variables and potentials,
and each node is assigned a color. The graph is initialized
as follows: all variables nodes get a common color; all po-
tential nodes with exactly same potential tables are assigned
a unique color. Now, in an iterative color passing scheme,
in each iteration, each variable node passes its color to all
neighboring potential nodes. The potential nodes store in-
coming color signatures in a vector, append their own color
to it, and send the vector back to variable nodes. The variable
nodes stack these incoming vectors. New colors are assigned
to each node based on the set of incoming messages such that
two nodes with same messages are assigned the same unique
color. This process is repeated until convergence, i.e., no fur-
ther change in colors.

A coloring of the bipartite graph deﬁnes a partition of vari-
able nodes such that all nodes of the same color form a par-
tition element. Each iteration of color passing creates suc-
cessively ﬁner partitions, since two variable nodes, once as-
signed different colors, can never get the same color.

3 Lifted Computer Vision Framework
In this section, we will describe our generic template which
can be used to lift a large class of vision applications includ-
ing those in stereo, segmentation etc. Our template can be
seen as transforming the original problem space to a reduced
problem space over which the original inference algorithm
can now be applied much more efﬁciently. Speciﬁcally, our
description in this section is entirely algorithm independent.
We will focus on MAP inference which is the inference
task of choice for most vision applications (refer Section 2).
The key insight in our formulation is based on the realiza-
tion that pixels which are involved in the same (or similar)

kinds of unary and higher order potentials, and have the same
(or similar) neighborhoods, are likely to have the same MAP
value. Therefore, if somehow we could discover such sets
of pixels a priori, we could explicitly enforce these pixels to
have the same value while searching for the solution, substan-
tially reducing the problem size and still preserving the opti-
mal MAP assignment(s). Since in general doing this exactly
may lead to a degenerate network, we do it approximately.
Hence trading-off speed for marginal loss in solution quality.
The loss in solution quality is offset by resorting to coarse-
to-ﬁne inference where we start with a crude approximation,
and gradually make it ﬁner, to guarantee optimality at the end
while still obtaining signiﬁcant gains. We next describe the
details of our approach.

3.1 Obtaining a Reduced Problem

1 , Y P

2 , · · · , Y P

k ⊆ X , Y P
k1

Consider an energy minimization problem over a PGM G =
(X , γ). Let L = {1, 2, · · · , |L|} denote the set of labels
over which variables in the set X can vary. Let Y P =
{Y P
r } denote a partition of X into r disjoint
∩ Y P
subsets, i.e., ∀k, Y P
= ∅ when k1 (cid:54)= k2,
k2
and ∪kY P
k = X . We refer to each Y P
k as a partition element.
Correspondingly, let us deﬁne Y = {Y1, Y2, · · · , Yr} as a set
of partition variables, where there is a one to one correspon-
dence between partition elements and the partition variables
and each partition variable Yk takes values in the set L. Let
part(Xi) denote the partition element to which Xi belongs.
Let ˆXj ⊆ X denote a subset of variables. We say that a par-
k is represented in the set ˆXj if ∃Xi ∈ ˆXj
tition element Y P
s.t. part(Xj) = Yk.
Given a subset of variables ˆXj, let γj( ˆXj) be a potential de-
ﬁned over ˆXj. Let ˆxj denote an assignment to variables in
the set ˆXj. Let ˆxj.elem(i) denote the value taken by a vari-
able Xi in ˆXj. We say that an assignment ˆXj = ˆxj respects
a partition Y P if the variables in ˆXj belonging to the same
partition element have the same label in ˆxj, i.e., part(Xi) =
part(Xi(cid:48)) ⇒ ˆxj.elem(i) = ˆxj.elem(i(cid:48)), ∀Xi, Xi(cid:48) ∈ ˆXj.
Next, we introduce the notion of a reduced potential.

Deﬁnition 3.1 Let X be a set of variables and let Y P de-
note its partition. Given the potential γj( ˆXj), the reduced
potential Γj is deﬁned to be the restriction of γj( ˆXj) to those
labeling assignments of ˆXj which respect the partition Y P .
Equivalently, we can deﬁne the reduced potential Γj( ˆYj) over
the set of partition variables ˆYj which are represented in the
set ˆXj.

For example, consider a potential γ(X1, X2, X3) deﬁned
over three Boolean variables. The table for γ would have
8 entries. Consider the partition Y P = {Y P
2 } where
1 = {X1, X2} and Y P
Y P
2 = {X3}. Then, the reduced poten-
tial Γ is the restriction of γ to those rows in the table where
X1 = X2. Hence Γ has four rows in its table and equivalently
can be thought of deﬁning a potential over the 4 possible com-
binations of Y1 and Y2 variables. We are now ready to deﬁne
a reduced graphical model.

1 , Y P

Deﬁnition 3.2 Let G = (X , γ) represent a PGM. Given a
partition Y P of X , the reduced graphical model G(Y, Γ) is
the graphical model deﬁned over the set of partition variables
Y such that every potential γj ∈ γ in G is replaced by the
corresponding reduced potential Γj ∈ Γ in G.
Let E(x) and E(y) denote the energies of the states x and y
in G and G, respectively. The following theorem relates the
energies of the states in the two graphical models.

Theorem 3.1 For every assignment y of Y in G, there is a
corresponding assignment x of X such that E(y) = E(x).

The theorem can be proved by noting that each potential
Γj( ˆYj) in G was obtained by restricting the original poten-
tial γj( ˆXj) to those assignments where variables in Xj be-
longing to the same partition took the same label. Since this
correspondence is true for every potential in the reduced set,
to obtain the desired state x, for every variable Xi ∈ X we
simply assign it the label of its partition in y.
Corollary 3.1 Let xMAP and yMAP be the MAP states (i.e.
having the minimum energy) for G and G, respectively. Then,
E(yMAP) ≥ E(xMAP).
The process of reduction can be seen as curtailing the en-
tire search space to those assignments where variables in the
same partition take the same label. A reduction in the prob-
lem space will lead to computational gains but might result
in loss of solution quality, where the solution quality can be
captured by the difference between E(yMAP) and E(xMAP).
Therefore, we need to trade-off the balance between the two.
Intuitively, a good problem reduction will keep those vari-
ables in the same partition which are likely to have the same
value in the optimal assignment for the original problem.
How do we ﬁnd such variables without actually solving the
inference task? We will describe one such technique in Sec-
tion 3.3.

There is another perspective.

Instead of solving one re-
duced problem, we can instead work with a series of reduced
problems which successively get closer to the optimal solu-
tion. The initial reductions are coarser and far from optimal,
but can be solved efﬁciently to quickly reach in the region
where the solution lies. The successive iterations can then re-
ﬁne the solution iteratively getting closer to the optimal. This
leads us to the coarse-to-ﬁne inference described next.

3.2 Coarse to Fine Inference
We will deﬁne a framework for C2F (coarse-to-ﬁne) infer-
ence so that we maintain the computational advantage while
still preserving optimality. In the following, for ease of no-
tation, we will drop the superscript P in Y p to denote the
partition of X . Therefore, Y will refer to both the partition as
well as the set of partition variables. Before we describe our
algorithm, let us start with some deﬁnitions.
Deﬁnition 3.3 Let Y and Y (cid:48) be two partitions of X . We
say that Y is coarser than Y (cid:48), denoted as Y (cid:22) Y (cid:48),
if
∀y(cid:48) ∈ Y (cid:48)∃y ∈ Y such that y(cid:48) ⊆ y. We equivalently say
that Y (cid:48) is ﬁner than Y.
It is easy to see that X deﬁnes a partition of itself which is the
ﬁnest among all partitions, i.e., ∀Y such that Y is a partition

of X , Y (cid:22) X . We also refer it to as the degenerate partition.
For ease of notation, we will denote the ﬁnest partition by
Y ∗ (same as X ). We will refer to the corresponding PGM as
G∗ (same as G). Next, we state a theorem which relates two
partitions with each other.
Lemma 1 Let Y and Y (cid:48) be two partitions of X such that Y (cid:22)
Y (cid:48). Then Y (cid:48) can be seen as a partition of the set Y.

The proof of this lemma is straightforward and is omitted
due to lack of space. Consider a set Y of coarse to ﬁne
partitions given as Y 0 (cid:22) Y 1, · · · , (cid:22), Y t, (cid:22), · · · , Y ∗. Let
Gt, E t, yt
M AP respectively denote the reduced problem, en-
ergy function and MAP assignment for the partition Y t. Us-
ing Lemma 1, Y t+1 is a partition of Y t. Then, using Theo-
rem 3.1, we have for every assignment yt to variables in Y t,
there is an assignment yt+1 to variables in Y t+1 such that
E t(yt) = E t+1(yt+1). Also, using Corollary 3.1, we have
∀t E t(yt
MAP). Together, these two state-
ments imply that starting from the coarsest partition, we can
gradually keep on improving the solution as we move to ﬁner
partitions.

MAP) ≥ E t+1(yt+1

Our C2F set-up assumes an iterative MAP inference algo-
rithm A which has the anytime property i.e., can produce so-
lutions of increasing quality with time. C2F Function (see
Algorithm 1) takes 3 inputs: a set of C2F partitions Y, infer-
ence algorithm A, and a stopping criteria C. The algorithm A
in turn takes three inputs: PGM Gt, starting assignment yt,
stopping criteria C. A outputs an approximation to the MAP
solution once the stopping criteria C is met. Starting with the
coarsest partition (t = 0 in line 2), a start state is picked for
the coarsest problem to be solved (line 3). In each iteration
(line 4), C2F ﬁnds the MAP estimate for the current problem
(Gt) using algorithm A (line 5). This solution is then mapped
to a same energy solution of the next ﬁner partition (line 6)
which becomes the starting state for the next run of A. The
solution is thus successively reﬁned in each iteration. The
process is repeated until we reach the ﬁnest level of partition.
In the end, A is run on the ﬁnest partition and the resultant
solution is output (lines 9,10). Since the last partition in the
set is the original problem G∗, optimality with respect to A is
guaranteed.

Next, we describe how to use the color passing algorithm
(Section 2) to get a series of partitions which get successively
ﬁner. Our C2F algorithm can then be applied on this set of
partitions to get anytime solutions of high quality while being
computationally efﬁcient.

Algorithm 1 Coarse-to-Fine Lifted MAP Algorithm
1:C2F Lifted MAP(C2F Partitions Y, Algo A,Criteria C)
2: t = 0; T = |Y|;
3: yt = getInitState(Gt);
4: While (t < T );
MAP = A(Gt, yt, C);
yt
5:
yt+1 = getEquivAssignment(Y t, Y t+1, yt
6:
t = t + 1;
7:
8: END While
9: yT
10: return yT

MAP = A(GT , yT , C);
MAP

MAP);

3.3 C2F Partitioning for Computer Vision
We now adapt the general color passing algorithm to MRFs
for CV problems. Unfortunately, unary potentials make color
passing highly ineffective. Different pixels have different
RGB values and intensities, leading to almost every pixel get-
ting a different unary potential. Naive application of color
passing splits almost all variables into their own partitions,
and lifting offers little value.

A natural approximation is to deﬁne a threshold, such that
two unary potentials within that threshold be initialized with
the same color. Our experiments show limited success with
this scheme because because two pixels may have the same
label even when their actual unary potentials are very differ-
ent. What is more important is relative importance given to
each label than the actual potential value.

In response, we adapt color passing for CV by initializing
it as before, but with one key change: we initialize two unary
potential nodes with the same color if their lowest energy la-
bels have the same order for the top NL labels (we call this
unary split threshold). Experiments reveal that this approxi-
mation leads to effective partitions for lifted inference.

Finally, we can easily construct a sequence of coarse-to-
ﬁne partitions in the natural course of color passing’s execu-
tion – every iteration of color passing creates a ﬁner partition.
Moreover, as an alternative approach, we may also increase
NL. In our implementations, we intersperse the two, i.e., be-
fore every next step we pick one of two choices: either, we
run another iteration of color passing; or, we increase NL by
one, and split each variable partition based on the N th
L lowest
energy labels of its constituent variables.

We parameterize CP (NL, Niter) to denote the partition
from the current state of color passing, which has been run
till Niter iterations and unary split threshold is NL.
It
is easy to prove that another iteration of color passing or
splitting by increasing NL as above leads to a ﬁner par-
I.e., CP (NL, Niter) (cid:22) CP (NL + 1, Niter) and
tition.
CP (NL, Niter) (cid:22) CP (NL, Niter + 1). We refer to each
element of a partition of variables as a lifted pixel, since it is
a subset of pixels.

4 Lifted Inference for Stereo Matching
We ﬁrst demonstrate the value of lifted inference in the con-
text of stereo matching [Scharstein and Szeliski, 2002].
It
aims to ﬁnd pixel correspondences in a set of images of the
same scene, which can be used to further estimate the 3D
scene. Formally, two images I l and I r corresponding to
images of the scene from a left camera and a right cam-
era are taken such that both cameras are at same horizon-
tal level. The goal is to compute a disparity labeling Dl
for every pixel X = (a, b) such that I l[a][b] corresponds to
I r[a−Dl[a][b]][b]. We build a lifted version of TSGO [Moze-
rov and van de Weijer, 2015], as it is MRF-based and ranks
2nd on the Middlebury Stereo Evaluation Version 2 leader-
board.3
Background on TSGO: TSGO treats stereo matching as
a two-step energy minimization, where the ﬁrst step is on a

3http://vision.middlebury.edu/stereo/eval/

(a)

(b)

(c)

Figure 1: (a) Average (normalized) energy vs. inference time (b) Average pixel error vs. time. C2F TSGO achieves roughly 60% reduction in
time for reaching the optima. It has best anytime performance compared to vanilla TSGO and static lifted versions. (c) Average (normalized)
energy vs. time for different thresholding values and CP partitions. Plots with the same marker have MRFs of similar sizes.

(a)

(b)

(c)

(d)

(e)

Figure 2: Qualitative results for Doll image at convergence. C2F-TSGO is similar to base TSGO.(a) Left and Right Images (b) Ground
Truth (c) Disparity Map by TSGO (d) Disparity Map by C2F TSGO (e) Each colored region (other than black) is one among the 10 largest
partition elements from CP(1,1). Each color represents one partition element. Partition elements form non-contiguous regions

fully connected MRF with pairwise potentials and the second
is on a conventional locally connected MRF. Lack of space
precludes a detailed description of the ﬁrst step. At the high
level, TSGO runs one iteration of message passing on fully
connected MRF, computes marginals of each pixel X, which
act as unary potentials φ(X) for the MRF of second step.

The pairwise potential ψ used in step two is ψ(X, X (cid:48)) =
w(X, X (cid:48))ϕ(X, X (cid:48)), where ϕ(X, X (cid:48)) is a truncated linear
function of (cid:107)X − X (cid:48)(cid:107), and w(X, X (cid:48)) takes one of three dis-
tinct values depending on color difference between pixels.
The MAP assignment xMAP computes the lowest energy as-
signment of disparities Dl for every pixel for this MRF.
Lifted TSGO: Since step two is costlier, we build its lifted
version as discussed in previous section. For color passing,
two unary potential nodes are initialized with the same color
if their lowest energy labels exactly match (NL = 1). Other
initializations are consistent with original color passing for
general MRFs. A sequence of coarse-to-ﬁne models is out-
putted as per Section 3.3. C2F TSGO uses outputs from the
sequence CP (1, 1), CP (2, 1), CP (3, 1) and then reﬁnes to
the original MRF. Model reﬁnement is triggered whenever
energy hasn’t decreased in the last four iterations of alpha ex-
pansion (this becomes the stopping criteria C in Algorithm
1).
Experiments: Our experiments build on top of the exist-
ing TSGO implementation4, but we change the minimization
algorithm in step two to alpha expansion fusion [Lempitsky
et al., 2010] from OpenGM2 library [Andres et al., 2010;
Kappes et al., 2015], as it improves the speed of the base

4http://www.cvc.uab.es/∼mozerov/Stereo/

implementation. We use the benchmark Middlebury Stereo
datasets of 2003, 2005 and 2006 [Scharstein and Szeliski,
2003; Hirschmuller and Scharstein, 2007]. For the 2003
dataset, quarter-size images are used and for others, third-size
images are used. The label space is of size 85 (85 distinct dis-
parity labels).

We

our

compare

coarse-to-ﬁne

TSGO (using
CP (NL, Niter) partitions) against vanilla TSGO. Fig-
ures 1(a,b) show the aggregate plots of energy (and error)
vs.
time. We observe that C2F TSGO reaches the same
optima as TSGO, but in less than half the time. It has a much
superior anytime performance – if inference time is given as
a deadline, C2F TSGO obtains 59.59% less error on average
over randomly sampled deadlines. We also eyeball the out-
puts of C2F TSGO and TSGO and ﬁnd them to be visually
similar. Figure 2 shows a sample qualitative comparison.
Figure 2(e) shows ﬁve of the ten largest partition elements in
the partition from CP (1, 1). Clearly, the partition elements
formed are not contiguous, and seem to capture variables that
are likely to get the same assignment. This underscores the
value of our lifting framework for CV problems.

We also compare our CP (NL, Niter) partitioning strat-
egy with threshold partitioning discussed in Section 3.3. We
merge two pixels in thresholding scheme if the L1-norm dis-
tance of their unary potentials is less than a threshold. For
each partition induced by our approach, we ﬁnd a value of
threshold that has roughly the same number of lifted pix-
els. Figure 1(c) shows that partitions based on CP (1, 1) and
CP (3, 1) converges to a much lower energy quickly com-
pared to the corresponding threshold values (T hr = 50 and
T hr = 1 respectively). For CP (2, 1), convergence is slower

compared to corresponding threshold (T hr = 5) but eventu-
ally CP (2, 1) has signiﬁcantly better quality.

5 Lifted Inference for Image Segmentation
We now demonstrate the general nature of our lifted CV
framework by applying it to a second task. We choose multi-
label interactive image segmentation, where the goal is to
segment an image I based on a seed labeling (true labels
for a few pixels) provided as input. Like many other CV
problems, this also has an MRF-based solution, with the best
label-assignment generally obtained by MAP inference using
graph cuts or loopy belief propagation [Boykov et al., 2001;
Szeliski et al., 2008].

However, MRFs with only pairwise potentials are known to
suffer from short-boundary bias – they prefer segmentations
with shorter boundaries, because pairwise potentials penalize
every pair of boundary pixels. This leads to incorrect labeling
for sharp edge objects. Kohli et al. [2013] use CoGC, coop-
erative graph cuts [Jegelka and Bilmes, 2011], to develop one
of the best MRF-based solvers that overcome this bias.
Background on CoGC: Traditional MRFs linearly penalize
the number of label discontinuities at edges (boundary pixel
pairs), but CoGC penalizes the number of types of label dis-
continuities through the use of a concave energy function over
groups of ordered edges. It ﬁrst clusters all edges on the ba-
sis of color differences, and later applies a concave function
separately over the number of times a speciﬁc discontinuity
type is present in each edge group g ∈ G. Their carefully
engineered CoGC energy function is as follows:

E(x) =

φi(xi)+

w(x, x(cid:48)).I(x = l, x(cid:48) (cid:54)= l)

|X |
(cid:88)

i=1



F



(cid:88)

(cid:88)

(cid:88)

g∈G

l∈L

(x,x(cid:48))∈g





where unary potentials φ depend on colors of seed pixels, F
is a concave function, I the indicator function, and w(x, x(cid:48))
depends on the color difference between x, x(cid:48).
Intuitively,
F collects all edges with similar discontinuities and penal-
izes them sub-linearly, thus reducing the short-boundary bias
in the model. The usage of a concave function makes the
MRF higher order with cliques over edge groups. However,
the model is shown to reduce to a pairwise hierarchical MRF
through the addition of auxiliary variables.
Lifted CoGC: CoGC is lifted using the framework of Sec-
tion 3, with one additional change. We cluster edge groups
using color difference and the position of the edge. Edge
groups that are formed only on the basis of color difference
make the error of grouping different segment’s boundaries
into a single group. For e.g., it erroneously cluster boundaries
between white cow and grass, and sky and grass together in
the top image in Figure 3.

2 (cid:101), 2), CP ((cid:100) L

Coarse-to-ﬁne partitions are obtained by the method de-
scribed in Section 3.3. C2F CoGC uses outputs from the se-
quence CP ((cid:100) L
2 (cid:101), 3) before reﬁning to the orig-
inal MRF. Model reﬁnement is triggered if energy has not
reduced over the last |L| iterations.
Experiments: Our experiments use the implementation of
Cooperative Graph Cuts as provided by [Kohli et al., 2013].5

5Available at https://github.com/aosokin/coopCuts CVPR2013

Energy minimization is performed using alpha expansion
[Boykov et al., 2001]. The implementation of CoGC per-
forms a greedy descent on auxiliary variables while perform-
ing alpha expansion on the remaining variables, as described
in Kohli et. al. [2013]. The dataset used is provided with the
implementation. It is a part of the MSRC V2 dataset.6.

Figure 3 shows three individual energy vs. time plots. Re-
sults on other images are similar. We ﬁnd that C2F CoGC
algorithm converges to the same energy as CoGC in about
two-thirds the time on average. Overall, C2F CoGC achieves
a much better anytime performance than other lifted and un-
lifted CoGC.

Similar to Section 4, reﬁned partitions attain better quality
than coarser ones at the expense of time. Since the implemen-
tation performs a greedy descent over auxiliary variables, re-
ﬁnement of current partition also resets the auxiliary variables
to the last value that produced a change. Notice that energy
minimization on output of CP (2, 3) attains a lower energy
than on CP (3, 2). This observation drives our decision to
reﬁne by increasing Niter. Qualitatively, C2F CoGC pro-
duces the same labeling as CoGC. Finally, similar to stereo
matching, partitions based on thresholding scheme perform
signiﬁcantly worse compared to CP (NL, Niter) for image
segmentation as well.

6 Related Work

There is a large body of work on exact
lifting, both
marginal [Kersting et al., 2009; Gogate and Domingos, 2011;
Niepert, 2012; Mittal et al., 2015] and MAP [Kersting
et al., 2009; Gogate and Domingos, 2011; Niepert, 2012;
Sarkhel et al., 2014; Mittal et al., 2014], which is not di-
rectly applicable to our setting. There is some recent work
on approximate lifting [Van den Broeck and Darwiche, 2013;
Venugopal and Gogate, 2014; Singla et al., 2014; Sarkhel et
al., 2015; Van den Broeck and Niepert, 2015] but it’s fo-
cus is on marginal inference whereas we are interested in
lifted MAP. Further, this work can’t handle a distinct unary
potential on every node. An exception is work by Bui et
al. [2012] which explicitly deals with lifting in presence of
distinct unary potentials. Unfortunately, they make a very
strong assumption of exchangeability in the absence of unar-
ies which does not hold true in our setting since each pixel
has its own unique neighborhood.

Work by Sarkhel et al. [2015] is probably the closest to our
work. They design a C2F hierarchy to cluster constants for
approximate lifted MAP inference in Markov logic. In con-
trast, we partition ground atoms in a PGM. Like other work
on approximate lifting, they can’t handle distinct unary po-
tentials. Furthermore, they assume that their theory is pro-
vided in a normal form, i.e., without evidence, which can be
a severe restriction for most practical applications. Kiddon &
Domingos [2011] also propose C2F inference for an underly-
ing Markov logic theory. They use a hierarchy of partitions
based on a pre-speciﬁed ontology. CV does not have any such

6Available

at

https://www.microsoft.com/en-us/research/

project/image-understanding/?from=http%3A%2F%2Fresearch.
microsoft.com%2Fvision%2Fcambridge%2Frecognition%2F

Figure 3: (a-c) Qualitative Results for Segmentation. C2F has quality similar to CoGC algorithm (a) Original Image (b) Segmentation by
CoGC (c) Segmentation by C2F CoGC (d) C2F CoGC has lower energy compared to CoGC and other lifted variant at all times

ontology available, and needs to discover partitions using the
PGM directly.

Nath & Domingos [2010] exploit (approximate) lifted in-
ference for video segmentation. They experiment on a spe-
ciﬁc video problem (different from ours), and they only com-
pare against vanilla BP. Their initial partitioning scheme is
similar to our thresholding approach, which does not work
well in our experiments.

In computer vision, a popular approach to reduce the com-
plexity of inference is to use superpixels [Achanta et al.,
2012; Van den Bergh et al., 2012]. Superpixels are obtained
by merging neighboring nodes that have similar character-
istics. All pixel nodes in the same superpixel are assigned
the same value during MAP inference. SLIC [Achanta et al.,
2012] is one of the most popular algorithms for discovering
superpixels. Our approach differs from SLIC in some signiﬁ-
cant ways. First, their superpixels are local in nature whereas
our algorithm can merge pixels that are far apart. This can
help in merging two disconnected regions of the same object
in a single lifted pixel. Second, they obtain superpixels inde-
pendent of the inference algorithm, whereas we tightly inte-
grate our lifting with the underlying inference algorithm. This
can potentially lead to discovery of better partitions; indeed,
this helped us tremendously in image segmentation. Third,
they do not provide a C2F version of their algorithm and
we did not ﬁnd it straightforward to extend their approach
to discover successively ﬁner partitions. There is some recent
work [Wei et al., 2016] which addresses last two of these
challenges by introducing a hierarchy of superpixels. In our
preliminary experiments, we found that SLIC and superpixel
hierarchy perform worse than our lifting approach. Perform-
ing more rigorous comparisons is a direction for future work.

7 Conclusion and Future Work
We develop a generic template for applying lifted inference
to structured output prediction tasks in computer vision. We
show that MRF-based CV algorithms can be lifted at different
levels of abstraction, leading to methods for coarse to ﬁne
inference over a sequence of lifted models. We test our ideas
on two different CV tasks of stereo matching and interactive
image segmentation. We ﬁnd that C2F lifting is vastly more
efﬁcient than unlifted algorithms on both tasks obtaining a
superior anytime performance, and without any loss in ﬁnal
solution quality. To the best of our knowledge, this is the ﬁrst
demonstration of lifted inference in conjunction with top of
the line task-speciﬁc algorithms. Although we restrict to CV
in this work, we believe that our ideas are general and can
be adapted to other domains such as NLP, and computational
biology. We plan to explore this in the future.

Acknowledgements
We thank anonymous reviewers for their comments and sug-
gestions. Ankit Anand is being supported by the TCS Re-
search Scholars Program. Mausam is being supported by
grants from Google and Bloomberg. Parag Singla is being
supported by a DARPA grant funded under the Explainable
AI (XAI) program. Both Mausam and Parag Singla are being
supported by the Visvesvaraya Young Faculty Fellowships by
Govt. of India. Any opinions, ﬁndings, conclusions or rec-
ommendations expressed in this paper are those of the authors
and do not necessarily reﬂect the views or ofﬁcial policies, ei-
ther expressed or implied, of the funding agencies.

References
[Achanta et al., 2012] R. Achanta, A. Shaji, K. Smith, A. Lucchi,
P. Fua, and S. Ssstrunk. SLIC Superpixels Compared to State-of-

the-Art Superpixel Methods. In PAMI, Nov 2012.

[Anand et al., 2016] A. Anand, A. Grover, Mausam, and P. Singla.
Contextual Symmetries in Probabilistic Graphical Models. In IJ-
CAI, 2016.

[Anand et al., 2017] A. Anand, R. Noothigattu, P. Singla, and
Mausam. Non-Count Symmetries in Boolean & Multi-Valued
Prob. Graphical Models. In AISTATS, 2017.

[Andres et al., 2010] B. Andres,

J. H. Kappes, U. K¨othe,
C. Schn¨orr, and F. A. Hamprecht. An Empirical Comparison of
Inference Algorithms for Graphical Models with Higher Order
Factors Using OpenGM. In Pattern Recognition. 2010.

[Blei et al., 2003] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet

Allocation. JMLR, 3, March 2003.

[Boykov et al., 2001] Y. Boykov, O. Veksler, and R. Zabih. Fast
In PAMI,

Approximate Energy Minimization via Graph Cuts.
23(11), November 2001.

[Braz et al., 2005] R. Braz, E. Amir, and D. Roth. Lifted First-

Order Probabilistic Inference. In IJCAI, 2005.

[Bui et al., 2012] H. Bui, T. Huynh, and R. De Salvo Braz. Exact
Lifted Inference with Distinct Soft Evidence on Every Object. In
AAAI, 2012.

[Bui et al., 2013] H. Bui, T. Huynh, and S. Riedel. Automorphism
Groups of Graphical Models and Lifted Variational Inference. In
UAI, 2013.

[Freeman et al., 2000] W. Freeman, E. Pasztor, and O. Carmichael.

Learning Low-Level Vision. In IJCV, 40, 2000.

[Friedman, 2004] N. Friedman. Inferring Cellular Networks using

Probabilistic Graphical Models. Science, 303, 2004.

[Hirschmuller and Scharstein, 2007] H.

and
Evaluation of Cost Functions for Stereo

Hirschmuller

D. Scharstein.
Matching. In CVPR, 2007.

[Jegelka and Bilmes, 2011] S. Jegelka and J. Bilmes. Submodu-
larity Beyond Submodular Energies: Coupling Edges in Graph
Cuts. In CVPR, 2011.

[Jernite et al., 2015] Y. Jernite, A. Rush, and D. Sontag. A Fast
Variational Approach for Learning Markov Random Field Lan-
guage Models. In ICML, 2015.

[Jha et al., 2010] A. Jha, V. Gogate, A. Meliou, and D. Suciu.
Lifted Inference Seen from the Other Side : The Tractable Fea-
tures. In NIPS, 2010.

[Kappes et al., 2015] J. Kappes, B. Andres, A. Hamprecht,
C. Schn¨orr, S. Nowozin, D. Batra, S. Kim, B. Kausler, T. Kr¨oger,
J. Lellmann, N. Komodakis, B. Savchynskyy, and C. Rother. A
Comparative Study of Modern Inference Techniques for Struc-
tured Discrete Energy Minimization Problems. In IJCV, 2015.
[Kersting et al., 2009] K. Kersting, B. Ahmadi, and S. Natarajan.

Counting Belief Propagation. In UAI, 2009.

[Kersting, 2012] K. Kersting. Lifted Probabilistic Inference.

In

ECAI, 2012.

[Kiddon and Domingos, 2011] C. Kiddon and P. Domingos.
Coarse-to-Fine Inference and Learning for First-Order Proba-
bilistic Models. In AAAI, 2011.

[Kimmig et al., 2015] A. Kimmig, L. Mihalkova, and L. Getoor.
Lifted Graphical Models: A Survey. Machine Learning, 2015.

[Kohli et al., 2013] P. Kohli, A. Osokin, and S. Jegelka. A Prin-
cipled Deep Random Field Model for Image Segmentation. In
CVPR, 2013.

[Lempitsky et al., 2010] V. Lempitsky, C. Rother, S. Roth, and
A. Blake. Fusion Moves for Markov Random Field Optimiza-
tion. In PAMI, Aug 2010.

[Mittal et al., 2014] H. Mittal, P. Goyal, V. Gogate, and P. Singla.
New Rules for Domain Independent Lifted MAP Inference. In
NIPS, 2014.

[Mittal et al., 2015] H. Mittal, A. Mahajan, V. Gogate, and
In NIPS,

P. Singla. Lifted Inference Rules With Constraints.
2015.

[Mladenov et al., 2014] M. Mladenov, K. Kersting, and A. Glober-
son. Efﬁcient Lifting of MAP LP Relaxations Using k-Locality.
In AISTATS, 2014.

[Mozerov and van de Weijer, 2015] M. G. Mozerov and J. van de
Weijer. Accurate Stereo Matching by Two-Step Energy Mini-
mization. IEEE Transactions on Image Processing, March 2015.

[Nath and Domingos, 2010] A. Nath and P. Domingos. Efﬁcient
Lifting for Online Probabilistic Inference. In AAAIWS, 2010.

[Nath and Domingos, 2016] A. Nath and P. Domingos. Learning
Tractable Probabilistic Models for Fault Localization. In AAAI,
2016.

[Niepert, 2012] M. Niepert. Markov Chains on Orbits of Permuta-

tion Groups. In UAI, 2012.

[Noessner et al., 2013] J. Noessner, M. Niepert, and H. Stucken-
schmidt. RockIt: Exploiting Parallelism and Symmetry for MAP
Inference in Statistical Relational Models. In AAAI, 2013.

[Poole, 2003] D. Poole. First-Order Probabilistic Inference. In IJ-

[Sarkhel et al., 2014] S. Sarkhel, D. Venugopal, P. Singla, and
V. Gogate. Lifted MAP inference for Markov logic networks.
In AISTATS, 2014.

[Sarkhel et al., 2015] S. Sarkhel, P. Singla, and V. Gogate. Fast

Lifted MAP Inference via Partitioning. In NIPS, 2015.

[Scharstein and Szeliski, 2002] D. Scharstein and R. Szeliski. A
Taxonomy and Evaluation of Dense Two-Frame Stereo Corre-
spondence Algorithms. In IJCV, 2002.

[Scharstein and Szeliski, 2003] D. Scharstein and R. Szeliski.
High-accuracy Stereo Depth Maps Using Structured Light.
In
CVPR, 2003.

[Singla and Domingos, 2008] P. Singla and P. Domingos. Lifted

First-Order Belief Propagation. In AAAI, 2008.

[Singla et al., 2014] P. Singla, A. Nath, and P. Domingos. Approx-
imate Lifting Techniques for Belief Propagation. In AAAI, 2014.

[Szeliski et al., 2008] R. Szeliski, R. Zabih, D. Scharstein, O. Vek-
sler, V. Kolmogorov, A. Agarwala, M. Tappen, and C. Rother. A
Comparative Study of Energy Minimization Methods for Markov
In PAMI, June
Random Fields with Smoothness-Based Priors.
2008.

[Van den Bergh et al., 2012] M. Van den Bergh, X. Boix, G. Roig,
B. de Capitani, and L. Van Gool. SEEDS: Superpixels Extracted
via Energy-Driven Sampling. In ECCV, 2012.

[Van den Broeck and Darwiche, 2013] G. Van den Broeck and
A. Darwiche. On the Complexity and Approximation of Binary
Evidence in Lifted Inference. In NIPS, 2013.

[Gogate and Domingos, 2011] V. Gogate and P. Domingos. Proba-

bilisitic Theorem Proving. In UAI, 2011.

CAI, 2003.

[Van den Broeck and Niepert, 2015] G. Van den Broeck and
Lifted Probabilistic Inference for Asymmetric

M. Niepert.
Graphical Models. In AAAI, 2015.

[Venugopal and Gogate, 2014] D. Venugopal

and V. Gogate.
Evidence-Based Clustering for Scalable Inference in Markov
Logic. In Joint ECML-KDD, 2014.

[Wei et al., 2016] X. Wei, Q. Yang, Y. Gong, M. Yang, and
N. Ahuja. Superpixel Hierarchy. CoRR, abs/1605.06325, 2016.

