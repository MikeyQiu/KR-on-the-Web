Neural Chinese Address Parsing

Hao Li and Wei Lu
StatNLP Research Group
Singapore University of Technology and Design
hao li@mymail.sutd.edu.sg
luwei@sutd.edu.sg

Pengjun Xie and Linlin Li
DAMO Academy
Alibaba Group
chengchen.xpj@alibaba-inc.com
linyan.lll@alibaba-inc.com

Abstract

This paper introduces a new task – Chinese
address parsing – the task of mapping Chi-
nese addresses into semantically meaningful
chunks. While it is possible to model this
problem using a conventional sequence la-
belling approach, our observation is that there
exist complex dependencies between labels
that cannot be readily captured by a simple
linear-chain structure. We investigate neural
structured prediction models with latent vari-
ables to capture such rich structural informa-
tion within Chinese addresses. We create and
publicly release a new dataset consisting of
15,000 Chinese addresses, and conduct ex-
tensive experiments on the dataset to investi-
gate the model effectiveness and robustness.
We release our code and data at http://
statnlp.org/research/sp.

1

Introduction

Addresses play an important role in modern soci-
ety. They are typically used as identiﬁers to lo-
cations and entities in the world that can be used
to facilitate various social activities, such as busi-
ness correspondences, meetings and events. Re-
cent research efforts show that systems that per-
form address parsing, the task of automatically
parsing addresses into semantically meaningful
structures, can be useful for tasks such as build-
ing e-commerce or product recommendation sys-
tems (Jia et al., 2017; Avvenuti et al., 2018). Due
to historical reasons, the English addresses come
with a standardized format, mostly written in order
from most speciﬁc to most general. Meaningful
chunks in an English address are also separated by
punctuation or the new-line symbols. Such char-
acteristics make parsing English addresses a rela-
tively easy task.

However, addresses written in eastern Asian
languages such as Chinese present several unique

浙江省杭州市拱墅区登云路639号1号楼电子市场230飞阳电子
浙江省PROVINCE 杭州市CITY 拱墅区DISTRICT
(Zhejiang Province)
(Hangzhou City)
(Gongshu District)
登云路ROAD 639号ROADNO 1号楼HOUSENO
(Dengyun Road)
电子市场POI 230ROOMNO 飞阳电子SUBPOI
(Feiyang Dianzi LLC.)
(Electronic Market)

(No. 639)

(Unit #1)

观沙街道观沙小区观沙嘉园安置小区9栋5单元9栋5单元1705
观沙街道TOWN 观沙小区POI 观沙嘉园SUBPOI
(Guansha Town)
(Guansha Residence) (Guansha Sub-residence)
安置小区PERSON 9栋HOUSENO 5单元CELLNO
(Anzhi Sector)
(Block 9)
9栋REDUNDANT 5单元REDUNDANT 1705ROOMNO
(Block 9)

(Unit #5)

(Unit #5)

Figure 1: Two example Chinese addresses and the ex-
pected structures after parsing. Each chunk is under-
lined with its corresponding label in blue.

challenges. Unlike English addresses, Chinese
addresses are typically written in the form of a
consecutive sequence of Chinese characters (pos-
sibly intermixed with digits and English letters).
Figure 1 presents two example Chinese addresses
and their desired output structures after parsing –
chunks annotated with their labels indicating se-
mantics (such as province, road, etc). The Chi-
nese addressing system is also different from that
of English. Though it is generally believed that the
system uses the opposite ordering – starting from
most general (e.g., province) and ending with most
speciﬁc (e.g., room no.), in practice it can be ob-
served that the format is far less rigorous than ex-
pected. The lack of rigor also leads to other issues
– the addresses may come with incomplete, redun-
dant or even inaccurate information, as we can see
from the second example listed in Figure 1. Such
unique challenges make the design of an effective
Chinese address parser non-trivial.

Label

Order ID Unique#

Train

Dev

Test

Interpretation

20
19
18
17
16
15
14
13
12
11
10
09
08
07
06
05
04
03
02
01
00

COUNTRY

PROVINCE

CITY

DISTRICT

DEVZONE

TOWN

COMMUNITY

ROAD

SUBROAD

ROADNO

SUBROADNO

POI

SUBPOI

HOUSENO

CELLNO

FLOORNO

ROOMNO

PERSON

ASSIST

REDUNDANT

OTHERINFO

Total Instances
Total Chunks

00,002
00,065
00,377
0,1137
00,297
02,382
01,867
05,037
00,486
02,676
00,215
08,662
01,642
01,309
00,295
00,258
03,245
00,903
00,264
01,009
00,005

32,133

00,041
03,794
04,824
05,881
00,330
03,972
01,279
05,410
00,333
04,316
00,170
06,312
01,435
02,993
01,134
01,119
02,702
00,650
00,718
03,517
00,004

08,957
50,934

00,015
01,317
01,662
02,027
00,118
01,300
00,415
01,788
00,109
01,435
00,068
02,093
00,461
00,978
00,388
00,346
00,883
00,207
00,207
01,208
00,000

02,985
17,024

00,013
01,265
01,613
01,921
00,107
01,308
00,416
01,801
00,130
01,401
00,075
02,122
00,487
00,943
00,358
00,331
00,824
00,208
00,241
01,137
00,003

02,985
16,704

name of a country
name of a province
name of a city
name of a district in a city
name of an economic development zone
name of a town or a boulevard
name of a community or a village
name of a road
name of a lane
road number
road number for a subroad
name of the point of interest
name of the second point of interest
house number
cell number
ﬂoor number
room number
name of the third point of interest or a person
a phrase for indicating relative position
redundant characters as well as repeated characters
a chunk which cannot be assigned any label above

Example
中国(China)
浙江省(Zhejiang Province)
北京市(Beijing)
朝阳区(Chaoyang District)
下沙开发区(Xiasha Development Zone)
乔司镇(Qiaosi Street)
荆山社区(Jingshan Community)
中山路(Zhongshan Road)
丹心巷(Danxin Road)
4-5号(#4-#5)
8号(#8)
萧山医院(Xiaoshan Hospital)
西三苑(Xisan Sub-residence)
3幢(Block #3)
1单元(Unit #1)
5层(Level 5)
402室(Room 402)
大厅(the hall)
对面(opposite)
-,!

Table 1: Statistics of different labels in our Chinese Address corpus.

Parsing a Chinese address into semantically
meaningful structures can be regarded as a spe-
cial type of chunking task (Abney, 1991), where
we need to perform address-speciﬁc Chinese word
segmentation (Xue, 2003; Peng et al., 2004; Zhao
et al., 2006) while assigning a semantic label to
each chunk. However, existing models designed
for chunking may not be readily applicable in this
task. Our observations show that there are a few
characteristics associated with the task. We found
that while generally there exists certain ordering
information among the chunks of different labels
in the addresses, such ordering information is bet-
ter preserved among the chunks that appear at the
beginning of the addresses. For the chunks appear-
ing towards the end of the addresses, chunks of
different types often appear in more ﬂexible order.
On top of the above observations, we propose a
speciﬁc model based on neural networks for the
task of Chinese address parsing. The model is
able to encode the regular patterns among chunks
that appear at the beginning of a Chinese address,
while ﬂexibly capturing the irregular patterns and
rich dependencies among the chunks of different
types that appear towards the end of the address.
This is achieved by designing a novel structured
representation integrating both a linear structure
and a latent-variable tree structure.

Our main contributions in this work can be sum-

marized as follows:

• We create and publicly release a new cor-
pus consisting of 15K Chinese address en-
tries fully annotated with chunk boundaries
and address labels. To the best of our knowl-

edge, this is the ﬁrst and largest annotated
Chinese address corpus.

• We introduce a novel neural approach to Chi-
nese address parsing with latent variables to
ﬂexibly capture both prior ordering informa-
tion and rich dependencies among labels.
• Through extensive experiments, we demon-
strate the effectiveness of our approach. The
experimental results show that our approach
outperforms several baselines signiﬁcantly.

2 Data

In this work, we created a Chinese Address corpus.
To do so, we crawled a large number of publicly
available addresses from the Chinese websites in-
cluding online business directory websites (e.g.,
b2b.huangye88.com), social media websites
(e.g., www.dianping.com), and an online API
service translating a geo-location to a Chinese ad-
dress (lbs.amap.com). In order to protect pri-
vacy, we discarded sensitive addresses (such as
those involving military locations) and randomly
altered the digits in the collected addresses.

Due to the lack of Chinese address standard for-
mat as well as complicated and different writing
preferences in different regions (e.g., people living
in southern China prefer the word “弄” as the suf-
ﬁx of the name of a lane or sub-road over the word
“胡同” which is widely used in northern China),
we create an annotation guideline1 by summariz-
ing different writing preferences. We proposed
21 chunk labels listed in Table 1. The meaning

1The annotation guideline can be found at http://

statnlp.org/research/sp.

of most labels can be inferred from their names.
We hire 3 annotators to annotate chunk boundaries
and chunk labels for each Chinese address follow-
ing the annotation guideline. In order to maintain
high annotation quality, we also hire 2 additional
quality controllers to sample 20 sentences from
each batch of 1,000 annotated sentences for hu-
man evaluation. Re-annotation for that batch will
be performed should the accuracy of human eval-
uation fall below 95%.

We randomly split the annotated data into 3 por-
tions following the ratio of 60%, 20%, and 20%,
yielding training, development, and test sets. The
complete statistics of our data can be found in Ta-
ble 1. From the table we can observe that the
chunk label POI (point of interest) occurs most fre-
quently. Indeed, such a label has a high level of
importance. This is because location-based infor-
mation can be extracted from such chunks, which
is crucial for recommendation services (Gao et al.,
2015; Xie et al., 2016). In addition, we report the
number of distinctive chunks (unique#) that ap-
pear in the data for each label, from which we can
see our corpus has a good coverage on PROVINCE,
CITY, and DISTRICT2.

We empirically assign each label a order ID in-
dicating its level of speciﬁcity. For example, the
label COUNTRY is used for describing a country,
and is the most general concept. It is thus assigned
the order ID 20, which is the highest among all la-
bels. As another example, the label PERSON gets
assigned an order ID 3, as it is used to describe one
of the most speciﬁc concepts. Such order ID in-
formation will be useful later when designing our
models for Chinese address parsing.

3 Approach

Our objective is to design a model for parsing Chi-
nese addresses into semantically meaningful struc-
tures in the form of consecutive chunks, where
each chunk is assigned a label as described in the
previous section. As we have mentioned before,
we believe there exist Chinese address-speciﬁc
characteristics associated with address texts that
can be exploited in designing a parsing model.
Speciﬁcally, we argue there are two types of struc-
tured information within Chinese addresses that
can be exploited when designing our parser – the
latent tree structures and the regular chain struc-

tures. The former is used for capturing rich de-
pendencies among chunks that appear towards the
end of each address. The latter is used for captur-
ing the structural patterns associated with chunks
appearing at the beginning of each address.

3.1 Latent Tree Structures

We focus our discussions on the latent tree struc-
tures ﬁrst. Given a consecutive sequence of la-
beled chunks, we can construct a binary tree struc-
ture whose yield exactly corresponds to the se-
quence of labeled chunks. We build the latent tree
structures to capture complex dependencies based
on the observation that chunks appearing towards
the end of a given address do not follow a rigorous
order. For instance, as we can see in the second ex-
ample in Figure 1, chunks towards the end of the
address consist of some labels related to numbers
as well as the label REDUNDANT. These labels
are either optional or do not follow some regular
patterns in terms of order, which makes capturing
dependencies among labels challenging.

We ﬁrst introduce auxiliary labels based on the
set of original labels we deﬁned in the previous
section. Such auxiliary labels are assigned to the
internal nodes within a parse tree. Speciﬁcally, for
each original label X, we introduce the auxiliary
label X. For example, the auxiliary label for ROAD
would be ROAD.

We now illustrate how a latent tree is con-
structed from a sequence of labeled chunks. These
chunks will be regarded as a sequence of leaf
nodes, each of which contains the corresponding
chunk boundary and chunk type information. To
simplify the construction process, we focus on
building a speciﬁc type of binary trees with each
non-leaf node containing at least 1 leaf node as one
of its child.3 We start the process by selecting any
chunk ﬁrst as one leaf node. Next we take a chunk
that is either on the left or on the right of the se-
lected chunk as its binary sibling node, and create
a parent node by assigning the two selected leaf
nodes as child nodes. To determine the label of
the newly created parent node, we choose the aux-
iliary label based on the label with a higher order
ID between the two of the child nodes. The newly
created parent node will replace the 2 child nodes
in the sequence and now the parent node becomes
a selected node. We repeat this construction pro-

2We found these numbers are comparable with statistics

3Preliminary results show that considering arbitrary bi-

on www.stats.gov.cn.

nary trees would lead to slightly worse results for our task.

cess until there is only 1 node left in the sequence.
Note that the construction process makes use of
the label order information.

the non-leaf node label POI

Figure 2 shows an example tree that the gold
From the example we
chunks correspond to.
can see that
that
appears twice has connections to other non-leaf
node labels such as ROADNO and POI. Such tree
structures will allow us to capture rich Chinese
address-speciﬁc structural information among la-
bels. Since there are many latent trees correspond-
ing to the given address consisting of consecutive
labeled chunks, the model is facilitated to learn
such complicated patterns, which is potentially
beneﬁcial for the address parsing task.

3.2 Regular Chain Structures

The latent tree structures allow complex depen-
dencies between different chunks to be captured
within a Chinese address. Such dependencies
would be helpful when there exist irregular pat-
terns within an address. However,
if we be-
lieve there are regular patterns among the labeled
chunks, using an alternative assumption on the de-
pendencies to properly capture such patterns may
be more desirable. For instance, the ﬁrst example
in Figure 1 illustrates a common regular pattern at
the beginning of the address, which is the order of
(PROVINCE, CITY, DISTRICT). This motivates us
to employ an alternative representation for captur-
ing dependencies within chunks that appear at the
beginning of the addresses, which are believed to
exhibit more regular patterns.

Speciﬁcally, we employ a chain structure to
capture the dependencies between adjacent la-
beled chunks. For example, given a sequence of
chunks, we may always consider a right-branching
tree structure to connect all these chunks. The re-
sulting structure will be able to capture ﬁrst-order
dependencies between adjacent labeled chunks,
which allows the regular orders among the labels
to be learned. For example, consider the ﬁrst two
chunks that appear within the address as illustrated
in Figure 2. The ﬁrst two chunks form a right-
branching tree structure. The construction process
for such chain structures is similar to that of the la-
tent trees, except that there is a single ﬁxed (right-
branching tree) structure for given labeled chunks.
Based on the observation that regular patterns
appear mostly at the beginning of an address, we
deﬁne the space H(x, y, sp) that consists of all la-

ROAD

ROADNO

ROAD

ROADNO

POI

POI

SUBPOI

POI

ROOMNO

登云路 639号 电子市场

230

飞阳电子

(Dengyun Road) (No. 639)

(Electronic Market)

(Room 230)

(Feiyang Dianzi LLC.)

Figure 2: An example latent tree for given gold chunks
where sp = POI. The English translation is listed be-
low each chunk. Leaf nodes are in gray, and the in-
ternal nodes are in pink (labeled with white auxiliary
labels). The tree structure within the triangle is latent –
we show one of the many possible structures for illus-
tration only.

tent tree structures that are consistent with the in-
put character sequence x, the gold labeled chunks
y and sp which determines the split point. For-
mally, we deﬁne the split point of a given address
as speciﬁed by sp as the left boundary of the right-
most chunk whose label order ID is larger than or
equal to sp. The split point divides the chunks
into two groups – those appearing on the left of sp
will form a chain structure while those on the right
will form a tree structure where the correct con-
struction is latent. Both structures are then merged
to form a single representation, which is used for
building our address parsing model.

Notice that when sp is set to −1 (denoted as
sp = LAST), the split point is on the right of
In this case the latent structured
the last chunk.
space H(x, y, sp) consists of only one single right-
branching tree. On the other hand, when sp is
set to its maximal value 20, the label order ID of
COUNTRY (denoted as sp = COUNTRY), the la-
tent structured space does not contain any struc-
ture that involves a partial regular chain compo-
nent. Different values of sp leads to different in-
terpolations between the two types of structural
assumptions, resulting in different variants of our
models. We will discuss the effect of different sp
values in the experiments section.

3.3 Chunk Representation

A parse tree corresponds to a collection of la-
beled chunks as leaves. We adopt a bi-directional
LSTM over a given input to compute the span-
level representation. At each position i in the orig-

inal input consisting of a sequence of characters,
we use fi and bi to denote the outputs of forward
LSTM and backward LSTM respectively. We use
ci,j = [fj − fi; bi − bj] to denote the vector rep-
resentation of the span covering characters from
position i to position j (Wang and Chang, 2016).
Motivated by Stern et al. (2017), we deﬁne the la-
bel score as follows:

s(i, j) = F (ci,j)

where F is a 2-layer feed-forward neural network
with output dimension being the number of chunk
labels. In addition, we denote the score of the span
with a speciﬁc label l as the value of the l-th ele-
ment in the vector s(i, j):

s(i, j, l) = [s(i, j)]l

(1)

3.4 Model

Inspired by Stern et al. (2017), we build a chart-
based parsing model. Unlike that work, how-
ever, our model involves latent structures as men-
tioned in Section 3.1. For a given sequence of
labeled chunks, our model considers all possible
constituent trees whose yield are exactly the la-
beled chunks.

Consider a tree t that can be represented by a
set of labeled spans, where each span is uniquely
deﬁned by the boundary (i, j) and the label l:

t := {(in, jn, ln) : n = 1, . . . , |t|}.

(2)

The score of the tree t can be deﬁned as follows:

3.5 Training and Decoding

Inspired by the structural support vector machines
with latent variables (Yu and Joachims, 2009), we
employ a (per instance) hinge loss during training:

L = max
t∈H(x)

[0, ∆(t, t∗) + S(t) − S(t∗)]

(5)

where H(x) refers to the set of all possible trees
for the given input x, and t∗ denotes the best tree
in the latent space H(x, y, sp):

t∗ = max

S(t(cid:48))

t(cid:48)∈H(x,y,c)

(6)

Here ∆(t, t∗) represents the Hamming loss on
labeled spans, measuring the similarity between
the predicted tree and the best latent tree that cor-
responds to the gold chunks.

During decoding, we aim to obtain the best tree
as the prediction ˆt for a new address x(cid:48) among all
the possible trees:

ˆt = arg max
t∈H(x(cid:48))

[S(t)]

(7)

The yield of the predicted tree ˆt gives us the list

of labeled chunks.4

4 Experimental Setup

We call our model Address Parser with Latent
Trees (APLT). We conducted experiments based
on different settings of the sp values,
leading
to many model variants. We describe baselines,
model hyperparameters as well as evaluation met-
rics in this section.

S(t) =

[s(i, j, l)]

(cid:88)

(i,j,l)∈t

(3)

Baselines To understand the effectiveness of our
models, we build the following baselines:

Similar to (Stern et al., 2017), we use a CKY-
style algorithm to calculate the score of the opti-
mal sub-tree that spans the interval (i, j) recursive
using the following formula:

π(i, j) = max

[s(i, j, l)]+
(cid:110)

l
max
k

max
l

[s(i, k, l)] + π(k, j),
(cid:111)

π(i, k) + max

[s(k, j, l)]

(4)

l

The base case is when the text span (i, j) corre-
sponds to a leaf node (a chunk) in the tree; in this
case we have: π(i, j) = max

[s(i, j, l)].

l

• (cid:96)CRF is the standard ﬁrst-order linear CRF
model (Lafferty et al., 2001) with discrete
features for sequence labeling tasks.

• sCRF is based on the standard semi-Markov
CRF (Sarawagi and Cohen, 2004) with dis-
crete features5.

• LSTM is the standard bi-directional LSTM

model for sequence labeling tasks.

4In some cases, it is possible to predict a tree with one or
more leaf chunks labeled with auxiliary labels (e.g., ROAD).
We have a post-processing step that converts such labels into
their corresponding original labels (e.g., ROAD).

5See the supplementary material for details on the features
for (cid:96)CRF and sCRF. For sCRF (and LSTM-sCRF),
maximal chunk length is set to 36, which is the length of the
longest chunk appearing in the training set.

• LSTM-(cid:96)CRF is proposed by Lample et al.
(2016) which is the state-of-the-art for many
sequence labeling tasks

• LSTM-sCRF is based on segmental recur-
rent neural network (Kong et al., 2016) which
is the neural network version of semi-Markov
CRF (Sarawagi and Cohen, 2004).

• TP is a transition-based parser for chunking
based on Lample et al. (2016), which makes
use of the stack LSTM (Dyer et al., 2015) to
encode the representation of the stack.

Hyperparameters We conducted all the experi-
ments based on our Chinese Address corpus. We
pre-trained Chinese character embeddings based
on the Chinese Gigaword corpus (Graff and Chen,
2005), using the skip-gram model with hierar-
chical softmax implemented within the word2vec
toolkit (Mikolov et al., 2013) where we set the
sample rate to 10−5 and embedding size to 100.

We use a 2-layer LSTM (for both directions)
with a hidden dimension of 200. For optimiza-
tion, we adopt the Adam (Kingma and Ba, 2014)
optimizer to optimize the model with batch size
1 and dropout rate 0.4. We randomly replace the
low frequency words with the UNK token and nor-
malize all numbers by replacing each digit (includ-
ing Chinese characters representing numbers from
0-9) to 0. We train our model for a maximal of
30 epochs and select the model parameters based
on the F1 score after each epoch on the develop-
ment set. The selected model is then applied to
the test set for evaluation. Our model, as well as
the baseline neural models, are implemented us-
ing DyNet (Neubig et al., 2017). All the neural
weights are initialized following the default initial-
ization method used in DyNet.

Evaluation Metrics We use the standard eval-
uation metrics from the CoNLL-2000 shared
task (Tjong Kim Sang and Buchholz, 2000), re-
porting precision (P.), recall (R.) and F1 percent-
age scores.

5 Result and Discussion

5.1 Main Results

We present our main results in Table 2, where we
report the overall performance as well as speciﬁc
results on the POI label. For our model, we report
results for sp=20, −1 as two special cases – the
former learns latent tree structures only and the
latter assumes a single right-branching tree. We

Model

(cid:96)CRF
sCRF
LSTM
LSTM-(cid:96)CRF
LSTM-sCRF
TP
APLT sp = 20 (COUNTRY)
APLT sp = −1 (LAST)
APLT sp = 7 (HOUSENO)

P.

69.76
74.95
70.11
77.94
77.80
77.61
80.36
79.64
79.75

POI
R.

72.68
77.14
76.90
75.62
77.84
75.67
78.46
78.89
79.26

F1

P.

OVERALL
R.

71.19
76.03
73.35
76.76
77.82
76.63
79.40
79.26
79.51

87.78
88.64
85.63
88.83
89.21
88.80
90.10
90.06
90.65

85.33
87.36
88.11
88.88
88.52
88.75
88.64
89.07
89.21

F1

86.53
87.99
86.85
88.86
88.86
88.77
89.37
89.56
89.93

Table 2: Main results.

also report results for sp=7 which is selected based
on the optimal results on the development set.

Among all the baselines, LSTM-(cid:96)CRF per-
forms better than LSTM and TP, which is
consistent with the ﬁnding reported in (Lample
et al., 2016). The two models LSTM-(cid:96)CRF
and LSTM-sCRF both achieve similar results,
which is also consistent with the ﬁnding reported
in (Liu et al., 2016). The two non-neural models
(cid:96)CRF and sCRF perform substantially worse
than their neural counterparts, which we believe is
mainly due to the use of only handcrafted features
in such systems. All these baseline models are
capable of encoding transition patterns between
neighboring chunks, which can partially capture
certain structural information. However, certain
Chinese address-speciﬁc structural information is
not explicitly captured in such models.

Our model APLT (sp=7) achieves the best
overall results, as well as the best results when
Compared with the
evaluated on POI only.
strongest baselines LSTM-(cid:96)CRF and LSTM-
sCRF, APLT (sp=7) outperforms them sig-
niﬁcantly by more than 1 F1 point overall
(p < 10−5)6. Furthermore, the APLT (sp=7)
model obtains the best F1 scores among all the
models on POI. Note that our APLT model
is able to learn richer dependencies among la-
bels including label order information, regular pat-
terns and irregular patterns among labels. Overall,
the model APLT (sp=7) also outperforms both
APLT (sp=−1) (p < 0.05) and APLT (sp=20)
(p < 0.005) signiﬁcantly. Such a result implies
the importance of capturing the various Chinese
address-speciﬁc structural information mentioned
above within our model.

To understand the results better, we conduct
detailed analysis of our results. Table 3 shows
the F1 scores of each label as well as the per-
centage of each label in the test data among four

6We perform the bootstrap resampling signiﬁcant test.

Label

%

LSTM LSTM APLT APLT
sp=7
sCRF sp=−1
(cid:96)CRF

Overall

0.100

88.86

POI

CITY

ROAD

TOWN

ROADNO

ROOMNO

DISTRICT

HOUSENO

PROVINCE

REDUNDANT

12.70
11.55
10.78
09.66
08.39
07.83
07.57
06.81
05.65
04.93
02.92
SUBPOI
COMMUNITY 02.49
02.14
CELLNO
01.98
01.44
01.25
00.78
00.64
00.45
00.08
00.02

SUBROADNO

OTHERINFO

COUNTRY

DEVZONE

SUBROAD

FLOORNO

PERSON

ASSIST

76.76
95.04
94.76
96.25
95.32
92.07
97.91
83.54
90.62
91.15
57.70
74.79
92.29
98.03
77.64
61.58
77.11
63.85
70.50
96.00
00.00

88.86

77.82
95.04
94.33
95.99
95.06
92.05
97.69
82.43
90.07
90.60
60.63
75.06
91.01
97.01
77.73
61.68
73.36
63.11
63.24
96.00
00.00

89.56

79.26
95.46
95.28
96.80
94.37
93.09
98.30
84.80
89.83
90.47
60.58
76.53
92.35
96.96
73.78
61.69
80.00
66.67
71.01
96.00
00.00

89.93

79.51
96.12
95.03
97.03
94.74
92.90
98.42
85.98
91.30
91.11
59.41
76.58
90.78
97.59
78.32
63.92
75.81
64.13
67.65
96.00
00.00

Table 3: F1 score comparison on test data for each label
among 4 models as well as the percentage of each label
in the gold data.

models LSTM-(cid:96)CRF, LSTM-sCRF, APLT
(sp=−1) and APLT (sp=7). Note that the re-
sults for the top 4 labels POI, DISTRICT, ROAD and
CITY, which take up 45% of total chunks, all get
improved when using our APLT models. More-
over, it achieves better or comparable F 1 scores
on 15 labels in the table among the total 21 la-
bels, especially on POI, DISTRICT, REDUNDANT,
COMMUNITY and PERSON with at least 1 point
improvement in F1. Interestingly, our models per-
form worse than LSTM-(cid:96)CRF on labels such
as ROADNO, ROOMNO, and FLOORNO, which are
mostly related to numbers. We note that, however,
chunks with such labels do not constitute a large
proportion of all chunks. Results suggest that our
models somehow learned to focus on optimization
performance for chunks with more prominent la-
bels such as POI and DISTRICT.

5.2 Effectiveness of Structural Information

In order to investigate how tree structures affect
the ﬁnal performance, we also conducted experi-
ments with different values for sp, which is used
for determining the split point. Figure 3 shows
the moving-averaged F1 scores on the test set ob-
tained when choosing sp around speciﬁc values (a
similar distribution can be observed on the devel-
opment set). From the bottom (COUNTRY,20) to
the top (LAST,-1) along y axis, the lower the sp
is, the more constraints are applied to the latent
space H(x, y, sp). Note that when sp=−1 (LAST),

Figure 3: Effect of sp.

the gold input only corresponds to a single right-
branching tree. We exclude the following labels:
REDUNDANT, ASSIST and OTHERINFO, because
we found these labels may appear at any place
within a given address, which make them unsuit-
able for determining the split point.

From Figure 3 we can observe that the F1 score
generally increases as we decrease sp, starting
from COUNTRY(with order ID 20). The perfor-
mance reaches the maximum when the sp is set to
a value within the range [SUBPOI, CELLNO]. This
observation implies that there does exist ordering
information among labels, and introducing more
constraints on the latent space will have the ben-
eﬁt of modeling the regular patterns around the
beginning part of a given address. After reach-
ing the best value, as we further decrease sp, the
performance drops slightly and oscillates around
the range [FLOORNO, LAST]. From here we can
observe that the latent trees are able to help cap-
ture irregular patterns within labels that appear to-
wards the end of the address. Overall, these results
suggest the importance of designing a model like
ours that is capable of capturing Chinese address-
speciﬁc characteristics.

5.3 Error Analysis

We conduct error analysis on two strongest base-
lines LSTM-(cid:96)CRF and LSTM-sCRF as well
as two best-performing APLT models respec-
tively. We examined the list of top-10 labels with
most errors for each model, and found most of the
errors come from labels such as POI, SUBPOI and
REDUNDANT – this implies they are the most chal-
lenging labels for this task. We also found labels
such as ROOMNO appear in the list for APLT
models, but not for the LSTM-(cid:96)CRF model,
showing that APLT models are still not good at
handling numbers as we discussed above.

There are two major types of errors. The type-

Gold

后湖村POI
(Houhu Village Residence)
Prediction 后湖村COMMUNITY 9栋HOUSENO

9栋HOUSENO
(Block 9)

Gold

四季青TOWN 老市场POI
(Si Ji Qing)
Prediction 四季青老市场POI

(Old Market)

Gold

萧宏大厦POI
(Xiaohong Plaza)
Prediction 萧宏大厦POI

124CROADNO
(#124C)
124CROOMNO

Figure 4: Example outputs from APLT(sp=7).

I error refers to the case where the boundary of a
chunk is predicted correctly but not its label. The
type-II error is the case where even the bound-
ary of a predicted chunk is incorrect. We found
that APLT (sp=−1) and APLT (sp=7) produce
less type-I errors (45.04% and 42.95% respec-
tively) than LSTM-(cid:96)CRF and LSTM-sCRF
(49.87% and 47.26% respectively). Moreover, we
ﬁnd that APLT (sp=7) model produces the least
number of type-I errors as well as type-II errors.

Looking into the type-I errors of both two
APLT models, we ﬁnd chunks with label POI are
often incorrectly labeled as COMMUNITY, which
is a major source of errors (9% of total errors). As
a typical example, we show a partial prediction in
Figure 4, where our model fails to recognize “后
湖村”(Houhu Village Residence) as a POI. Here the
character “村”(Village) is a common sufﬁx for the
name of either a village or a residence, hence the
confusion.

The second example in Figure 4 demonstrates
another typical kind of errors produced by our
models around the POI labels. Here, “四季青(Si
Ji Qing)” is actually the name of a town. However,
as most names of towns end with “镇(Town)” as
the sufﬁx, our models as well as baseline models
all fail to identify the correct chunk boundaries.

We also investigate the errors around the num-
ber labels. We choose to look into the results on
ROADNO because it is the ﬁfth most popular la-
bel in the test data. Based on the error analy-
sis, we found that many chunks of label ROADNO
were incorrectly assigned other types of number
labels. As we can see from the third example in
Figure 4, the ROADNO “124C” is incorrectly pre-
dicted as a ROOMNO.
Indeed, this chunk does
look like a room number, though in fact it refers
to a road within a “plaza” (大厦) rather than an
ofﬁce within a “building” (another interpretation
of 大厦). From these examples we can observe
that many ambiguities may not be easily resolved

Length

%

LSTM LSTM APLT APLT
sp=7
sCRF sp=−1
(cid:96)CRF

≥1
≥2
≥3
≥4
≥5
≥6
≥7
≥8

09.39
23.73
44.60
13.31
03.70
02.14
01.16
01.97

92.16
86.69
92.26
86.49
74.57
68.88
64.61
63.31

92.14
86.13
92.04
87.48
76.41
70.19
68.14
62.57

91.65
87.16
93.03
88.05
77.55
70.87
67.59
63.33

91.77
87.77
93.51
88.18
79.43
73.73
68.22
60.19

Table 4: Results for different chunk lengths.

without further background knowledge.

5.4 Robustness Analysis

We analyze the model robustness by assessing the
performance on chunks of different lengths for
each of the four models discussed above. We
group chunks into 8 categories based on their
lengths and present the results in Table 4 where the
distribution information is also included. As we
can see, all the models achieve at least a F1 score
of 86 when considering chunks whose lengths are
less than 5. As the length increases, the perfor-
mance of all models drop gradually. For chunks
whose lengths are at least 8, the F1 score is around
60-63 for all models. Considering chunks whose
lengths are either 2, 3, or 4 only (such chunks con-
stitute over 80% of total chunks), we can observe
that APLT (sp=7) outperforms two baselines sig-
niﬁcantly by more than 1 point for each category.
These results demonstrate the robustness of our
model when handling chunks of different lengths.
Comparing the two APLT models, we can
see the model APLT (sp=7) outperforms APLT
(sp=−1) for each chunk category, except for
chunks whose lengths are greater than or equal
to 8. These two models differ in their latent
spaces. APLT (sp=7) with a richer latent space
appears to be better at handling chunks with short
or medium lengths.

In addition, we conducted a further experiment
to understand how each model is able to handle
new chunks – the chunks that appear in the test
set (according to the gold labels) but do not ap-
pear in the training set. We found empirically
there are 31% of the chunks in the test set that are
new chunks. Such an experiment allows us to as-
sess the robustness of each model when new data
is available. We report the accuracy for the new
chunks in Table 5. As we can see, two APLT
models outperform two baselines, indicating our
APLT models appear to be better at handling new
chunks. We believe this is due to the tree models

LSTM LSTM APLT APLT
sp=7
sCRF sp=−1
(cid:96)CRF

80.17

79.92

80.94

80.94

Table 5: Accuracy on test data for the new chunks.

that we used, which are capable of capturing com-
plex dependencies among chunks.

6 Related Work

While the Chinese address parsing task is new, it
is related to the following traditional tasks within
the ﬁeld of natural language processing (NLP)
– chunking, named entity recognition, word seg-
mentation and parsing. We brieﬂy survey research
efforts which are most related to our task below.

Chunking as a fundamental task in NLP has
been investigated for decades (Abney, 1991).
Chunking for Chinese can typically be regarded
as a sequence labeling problem solvable by mod-
els such as conditional random ﬁelds (Chen et al.,
2006; Tan et al., 2005; Zhou et al., 2012), hidden
Markov models (Li et al., 2003), support vector
machines (Tan et al., 2004) and the maximum en-
tropy model (Wu et al., 2005). Our task can also
be regarded as a chunking task where we need to
assign an address-speciﬁc label to each chunk.

Named entity recognition (NER) is another fun-
damental task close to chunking within the ﬁeld of
NLP, which focuses on the extraction of semanti-
cally meaningful entities from the text. The state-
of-the-art approach by Lample et al. (2016) em-
ploys a LSTM-CRF model. Ma and Hovy (2016)
proposed a LSTM-CNNs-CRF model that utilizes
convolutional neural networks (CNNs) to extract
character-level features besides word-level fea-
tures. Zhai et al. (2017) suggested a neural chunk-
ing model based on pointer networks (Vinyals
et al., 2015) to resolve the issue of being difﬁ-
cult to use chunk-level features such as the length
of the chunk for segmentation. Zhang and Yang
(2018) tackled the problem of Chinese NER by de-
ploying a lattice LSTM leveraging lexicons.

Another task closely related to our task is the
Chinese word segmentation task which at least
dates back to the 1990s (Sproat et al., 1994).
The segmentation task is typically casted as a
character-based sequence labeling problem (Xue,
2003) which can be solved by CRF based mod-
els (Peng et al., 2004; Zhao et al., 2006), their
latent-variable variants (Sun et al., 2009), or max-
margin based models (Zhang and Clark, 2007).

Recently, Zhang et al. (2016) proposed a neural
transition-based segmentation approach by encod-
ing both words and characters as well as the his-
tory action sequence. Yang et al. (2017) suggested
to perform segmentation with a neural transition-
based method with rich pre-training.

Constituent parsing is another line of work that
is related to our task. The state-of-the-art ap-
proaches to parsing include transition-based mod-
els (Dyer et al., 2016) and chart-based mod-
els (Stern et al., 2017; Kitaev and Klein, 2018).Our
model is motivated by the latter approaches, where
we additionally introduce latent variables for cap-
turing complex dependencies among chunks.

7 Conclusion

In this work, we introduce a new task – Chi-
nese address parsing, which is to segment a given
Chinese address text into chunks while assigning
each chunk a semantically meaningful label. We
create and publish a Chinese address corpus that
consists of 15K fully labeled Chinese addresses.
We identify interesting characteristics associated
with the task and design a novel neural parsing
model with latent variables for this task, which is
able to capture Chinese address-speciﬁc structural
information. We conduct extensive experiments
and compare our approach with strong baselines
through detailed analysis. We show that our pro-
posed model outperforms baseline approaches sig-
niﬁcantly, due to its ability in capturing rich struc-
tural information present in the Chinese addresses.
leveraging external
knowledge bases to disambiguate chunks and
entities that appear within Chinese addresses,
as well as designing algorithms that are able to
capture longer-range dependencies among chunks
using alternative structures.

Future work includes

Acknowledgments

We would like to thank the anonymous reviewers
for their constructive comments on this work. This
work is done under a collaborative agreement be-
tween SUTD and Alibaba on an Alibaba Innova-
tive Research (AIR) Program funded by Alibaba,
where Alibaba provided data. We appreciate Al-
ibaba’s generosity in the agreement that makes it
possible for us to make all data and code in this re-
search publicly available upon acceptance of this
paper. This work is also partially supported by
SUTD project PIE-SGP-AI-2018-01.

References

Steven P Abney. 1991.

In
Principle-based parsing, pages 257–278. Springer.

Parsing by chunks.

Marco Avvenuti, Stefano Cresci, Leonardo Nizzoli,
and Maurizio Tesconi. 2018. Gsp (geo-semantic-
parsing): Geoparsing and geotagging with machine
learning on top of linked data. In Proc. of ESWC.

Wenliang Chen, Yujie Zhang, and Hitoshi Isahara.
2006. An empirical study of chinese chunking. In
Proc. of ACL.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proc. of ACL.

Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,
and Noah A Smith. 2016. Recurrent neural network
grammars. In Proc. of NAACL.

Huiji Gao, Jiliang Tang, Xia Hu, and Huan Liu. 2015.
Content-aware point of interest recommendation on
location-based social networks. In Proc. of AAAI.

David Graff and Ke Chen. 2005. Chinese gigaword.
LDC Catalog No.: LDC2003T09, ISBN, 1:58563–
58230.

Ya-Hui Jia, Wei-Neng Chen, Tianlong Gu, Huaxiang
Zhang, Huaqiang Yuan, Ying Lin, Wei-Jie Yu, and
Jun Zhang. 2017. A dynamic logistic dispatching
system with set-based particle swarm optimization.
IEEE Transactions on Systems, Man, and Cybernet-
ics: Systems.

Diederik P Kingma and Jimmy Ba. 2014. Adam:
In Proc. of

A method for stochastic optimization.
ICLR.

Nikita Kitaev and Dan Klein. 2018. Constituency pars-
ing with a self-attentive encoder. In Proc. of ACL.

Lingpeng Kong, Chris Dyer, and Noah A Smith. 2016.
In Proc. of

Segmental recurrent neural networks.
ICLR.

John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random ﬁelds: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. of ICML.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proc. of NAACL.

Heng Li, Jonathan J Webster, Chunyu Kit, and Tian-
shun Yao. 2003. Transductive hmm based chinese
text chunking. In Natural Language Processing and
Knowledge Engineering, 2003. Proceedings. 2003
International Conference on, pages 257–262. IEEE.

Yijia Liu, Wanxiang Che, Jiang Guo, Bing Qin, and
Ting Liu. 2016. Exploring segment representations
for neural segmentation models. In Proc. of IJCAI.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf. In
Proc. of ACL.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proc. of NIPS.

Graham Neubig, Chris Dyer, Yoav Goldberg, Austin
Matthews, Waleed Ammar, Antonios Anastasopou-
los, Miguel Ballesteros, David Chiang, Daniel
Clothiaux, Trevor Cohn, et al. 2017. Dynet: The
arXiv preprint
dynamic neural network toolkit.
arXiv:1701.03980.

Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detec-
In Proc. of
tion using conditional random ﬁelds.
ACL.

Sunita Sarawagi and William W Cohen. 2004. Semi-
markov conditional random ﬁelds for information
extraction. In Proc. of NIPS.

Richard Sproat, Chilin Shih, William Gale, and Nancy
A stochastic ﬁnite-state word-
In Proc. of

Chang. 1994.
segmentation algorithm for chinese.
ACL.

Mitchell Stern, Jacob Andreas, and Dan Klein. 2017. A
minimal span-based neural constituency parser. In
Proc. of ACL.

Xu Sun, Yaozhong Zhang, Takuya Matsuzaki, Yoshi-
masa Tsuruoka, and Jun’ichi Tsujii. 2009. A dis-
criminative latent variable chinese segmenter with
In Proc. of
hybrid word/character information.
NAACL.

Yongmei Tan, Tianshun Yao, Qing Chen, and Jingbo
Zhu. 2004. Chinese chunk identiﬁcation using svms
plus sigmoid. In Proc. of IJCNLP.

Yongmei Tan, Tianshun Yao, Qing Chen, and Jingbo
Zhu. 2005. Applying conditional random ﬁelds to
chinese shallow parsing. In Proc. of CICLing.

Erik F Tjong Kim Sang and Sabine Buchholz. 2000.
Introduction to the conll-2000 shared task: Chunk-
ing. In Proc. of CoNLL.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.

2015. Pointer networks. In Proc. of NIPS.

Wenhui Wang and Baobao Chang. 2016. Graph-based
dependency parsing with bidirectional lstm. In Proc.
of ACL.

Shih-Hung Wu, Cheng-Wei Shih, Chia-Wei Wu,
Tzong-Han Tsai, and Wen-Lian Hsu. 2005. Ap-
plying maximum entropy to robust chinese shallow
parsing. In Proc. of IJCLCLP.

Min Xie, Hongzhi Yin, Hao Wang, Fanjiang Xu,
Weitong Chen, and Sen Wang. 2016. Learning
graph-based poi embedding for location-based rec-
ommendation. In Proc. of CIKM.

Nianwen Xue. 2003. Chinese word segmentation as
character tagging. International Journal of Compu-
tational Linguistics & Chinese Language Process-
ing, Volume 8, Number 1, February 2003: Special
Issue on Word Formation and Chinese Language
Processing, 8(1):29–48.

Jie Yang, Yue Zhang, and Fei Dong. 2017. Neural
In Proc.

word segmentation with rich pretraining.
of ACL.

Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural svms with latent variables.
In
Proc. of ICML.

Feifei Zhai, Saloni Potdar, Bing Xiang, and Bowen
Zhou. 2017. Neural models for sequence chunking.
In Proc. of AAAI.

Meishan Zhang, Yue Zhang, and Guohong Fu. 2016.
In

Transition-based neural word segmentation.
Proc. of ACL.

Yue Zhang and Stephen Clark. 2007. Chinese segmen-
tation with a word-based perceptron algorithm.
In
Proc. of ACL.

Yue Zhang and Jie Yang. 2018. Chinese ner using lat-

tice lstm. In Proc. of ACL.

Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2006. Effective tag set selection in chinese word
segmentation via conditional random ﬁeld model-
ing. In Proc. of PACLIC.

Junsheng Zhou, Weiguang Qu, and Fen Zhang. 2012.
Exploiting chunk-level features to improve phrase
chunking. In Proc. of EMNLP.

