Adversarial Transfer Learning for Chinese Named Entity Recognition
with Self-Attention Mechanism

Pengfei Cao1,2, Yubo Chen1, Kang Liu1,2, Jun Zhao1,2 and Shengping Liu3
1 National Laboratory of Pattern Recognition, Institute of Automation,
Chinese Academy of Sciences, Beijing, 100190, China
2 University of Chinese Academy of Sciences, Beijing, 100049, China
3 Beijing Unisound Information Technology Co., Ltd, Beijing, 100028, China
{pengfei.cao, yubo.chen, kliu, jzhao}@nlpr.ia.ac.cn, liushengping@unisound.com

Abstract

Named entity recognition (NER) is an impor-
tant task in natural language processing area,
which needs to determine entities boundaries
and classify them into pre-deﬁned categories.
For Chinese NER task, there is only a very s-
mall amount of annotated data available. Chi-
nese NER task and Chinese word segmen-
tation (CWS) task have many similar word
boundaries. There are also speciﬁcities in each
task. However, existing methods for Chinese
NER either do not exploit word boundary in-
formation from CWS or cannot ﬁlter the spe-
ciﬁc information of CWS. In this paper, we
propose a novel adversarial transfer learning
framework to make full use of task-shared
boundaries information and prevent the task-
speciﬁc features of CWS. Besides, since ar-
bitrary character can provide important cues
when predicting entity type, we exploit self-
attention to explicitly capture long range de-
pendencies between two tokens. Experimental
results on two different widely used dataset-
s show that our proposed model signiﬁcant-
ly and consistently outperforms other state-of-
the-art methods.

1

Introduction

The task of named entity recognition (NER) is to
recognize the named entities in given text. N-
ER is a preliminary and important task in natural
language processing (NLP) area and can be used
in many downstream NLP tasks, such as relation
extraction (Bunescu and Mooney, 2005), even-
t extraction (Chen et al., 2015) and question an-
In recent
swering (Yao and Van Durme, 2014).
years, numerous methods have been carefully s-
tudied for NER task, including Hidden Markov
Models (HMMs) (Bikel et al., 1997), Support Vec-
tor Machines (SVMs) (Isozaki and Kazawa, 2002)
and Conditional Random Fields (CRFs) (Laffer-
ty et al., 2001). Currently, with the development

Figure 1: An example of illustrating the similarities
and speciﬁcities between Chinese NER and CWS.

of deep learning, neural networks (Lample et al.,
2016; Peng and Dredze, 2016; Luo and Yang,
2016) have been introduced to NER task. All these
methods need to determine entities boundaries
and classify them into pre-deﬁned categories.

Although great

improvements have been
achieved by these methods on Chinese NER task,
some issues still have not been well addressed.
One signiﬁcant drawback is that there is only a
very small amount of annotated data available.
Weibo NER dataset (Peng and Dredze, 2015; He
and Sun, 2017a) and Sighan2006 NER dataset
(Levow, 2006) are two widely used datasets
for Chinese NER task, containing 1.3k and 45k
training examples,
respectively. On the two
datasets, the highest F1 scores are 48.41% and
89.21%, respectively. As a basic task in NLP area,
the performance is not satisfactory. Fortunately,
Chinese word segmentation (CWS) task is to
recognize word boundaries and the amount of
supervised training data for CWS is abundant
compared with NER. There are many similarities
between Chinese NER task and CWS task, which
we call task-shared information. As shown in
Figure 1, given a sentence “(cid:12)(cid:20)(cid:127)»(cid:0)(cid:17)ﬂ(cid:127):
: (Hilton leaves Houston Airport)”, the two tasks
have the same boundaries for some words such
as “(cid:12)(cid:20)(cid:127) (Hilton)” and “»(cid:0) (leaves)”, while
Chinese NER has more coarse-grained boundaries

than CWS task for certain word such as “(cid:17)ﬂ(cid:127)
:: (Houston Airport)” in the example of Figure
1, which we call task-speciﬁc information.
In
order to incorporate word boundary information
from CWS task into NER task, Peng and Dredze
(2016) propose a joint model
that performs
Chinese NER with CWS task. However, their
proposed model only focuses on task-shared
information between Chinese NER and CWS,
and ignores ﬁltering the speciﬁcities of each
task, which will bring noise for both of the tasks.
For example, the CWS task splits “(cid:17)ﬂ(cid:127)::
(Houston Airport)” into “(cid:17)ﬂ(cid:127) (Houston)” and
“:: (Airport)”, while the NER task takes “(cid:17)
ﬂ(cid:127):: (Houston Airport)” as a whole entity.
Thus, how to exploit task-shared information and
prevent the noise brought by CWS task to Chinese
NER task is a challenging problem.

Another issue is that most proposed models
cannot explicitly model long range dependencies
when predicting entity type. Though bidirection-
al long short term memory (BiLSTM) can learn
long-distance dependencies, it cannot conduct di-
rect connections between arbitrary two characters.
As shown in Figure 1, if the model only focuses
on the word “(cid:12)(cid:20)(cid:127) (Hilton)”, it can be a person
or organization. However, when the model explic-
itly captures the dependencies between “(cid:12)(cid:20)(cid:127)
(Hilton)” and “»(cid:0) (leaves)”, it is easy to classify
“(cid:12)(cid:20)(cid:127) (Hilton)” into “person” category. Con-
text information is very crucial for determining the
entity type. While in the sentence “(cid:17)(cid:6)O((cid:12)(cid:20)
(cid:127) (I will be staying at the Hilton)”, the entity type
of “(cid:12)(cid:20)(cid:127) (Hilton)” is “organization”. Thus, how
to better capture the global dependencies of the w-
hole sentence is another challenging problem.

To address the above problems, we propose an
adversarial transfer learning framework to inte-
grate the task-shared word boundary information
into Chinese NER task in this paper. The adver-
sarial transfer learning is incorporating adversari-
al training into transfer learning. To better capture
long range dependencies and synthesize the infor-
mation of the sentence, we extend self-attention
mechanism into the framework. Speciﬁcally, we
try to improve Chinese NER task performance by
incorporating shared boundary information from
CWS task. To prevent the speciﬁc information
of CWS task from lowering the performance of
the Chinese NER task, we introduce adversarial
training to ensure that the Chinese NER task on-

ly exploits task-shared word boundary informa-
tion. Then, for tackling the long range dependen-
cy problems, we utilize self-attention to synthe-
size the hidden representation of BiLSTM. Final-
ly, we evaluate our model on two different widely
used Chinese NER datasets. Experimental results
show that our proposed model achieves better per-
formance than other state-of-the-art methods and
gains new benchmarks.

In summary, the contributions of this paper are

as follows:

• We propose an adversarial transfer learning
framework to incorporate task-shared word
boundary information from CWS task into
Chinese NER task. To our best knowledge,
it is the ﬁrst work to apply adversarial trans-
fer learning method into NER task.

• We introduce self-attention mechanism into
our model, which aims to capture the global
dependencies of the whole sentence and learn
inner structure features of sentence.

• We conduct our experiment on two dif-
ferent widely used Chinese NER datasets,
and the experimental results demonstrate that
our proposed model signiﬁcantly and consis-
tently outperforms previous state-of-the-art
methods. We release the source code publicly
for further research1.

2 Related Work

NER Many methods have been proposed for N-
ER task. Early studies on NER often exploit
SVMs (Isozaki and Kazawa, 2002), HMMs (Bikel
et al., 1997) and CRFs (Lafferty et al., 2001),
heavily relying on feature engineering. Zhou et al.
(2013) formulate Chinese NER as a joint identi-
ﬁcation and categorization task. In recent years,
neural network models have been introduced to N-
ER task (Collobert et al., 2011; Huang et al., 2015;
Peng and Dredze, 2016). Huang et al. (2015) ex-
ploit BiLSTM to extract features and feed them
into CRF decoder. After that, the BiLSTM-CRF
model is usually exploited as the baseline. Lam-
ple et al. (2016) use a character LSTM to represent
spelling characteristics. In addition, Wang et al.
(2017) propose a gated convolutional neural net-
work (GCNN) model for Chinese NER. Peng and
Dredze (2016) propose a joint model for Chinese

1https://github.com/CPF-NLPR/AT4ChineseNER

Figure 2: The general architecture of our proposed model. The left and right part are Chinese NER space
and CWS private space, respectively, including embedding layer, feature extractor (Private BiLSTM),
self-attention and CRF layer. The middle part is shared space consisting of feature extractor (Shared
BiLSTM), self-attention and task discriminator.

NER, which are jointly trained with CWS task.
However, the speciﬁc features brought by CWS
task can lower the performance of the Chinese N-
ER task.

Adversarial Training Adversarial networks
have achieved great success in computer vision
(Goodfellow et al., 2014; Denton et al., 2015).
In NLP area, adversarial training has been intro-
duced for domain adaptation (Ganin and Lempit-
sky, 2014; Zhang et al., 2017; Gui et al., 2017),
cross-lingual transfer learning (Chen et al., 2016;
Kim et al., 2017), multi-task learning (Chen et al.,
2017; Liu et al., 2017) and crowdsourcing learning
(Yang et al., 2018). Bousmalis et al. (2016) pro-
pose shared-private model in domain separation
network. Different from these works, we exploit
adversarial network to jointly train Chinese NER
task and CWS task, aiming to extract task-shared
word boundary information from CWS task. To
our knowledge, it is the ﬁrst work to apply adver-
sarial transfer learning framework to Chinese NER
task.

Self-Attention Self-attention has been intro-
duced to machine translation by Vaswani et al.
(2017) for capturing global dependencies between
input and output and achieves state-of-the-art per-
formance. For language understanding task, Shen
et al. (2017) exploit self-attention to learn long
Tan et al. (2017) apply
range dependencies.
self-attention to semantic role labelling task and
achieve state-of-the-art results. We are the ﬁrst to

introduce self-attention mechanism to Chinese N-
ER task.

3 Method

In this paper, we propose a novel adversarial trans-
fer learning framework that will learn task-shared
word boundary information from CWS task, ﬁlter
speciﬁc information of CWS and explicitly cap-
ture the long range dependencies between arbi-
trary two characters in sentence. The architecture
of our proposed model is illustrated in Figure 2.
The model mainly consists of ﬁve components:
embedding layer, shared-private feature extractor,
self-attention, task-speciﬁc CRF and task discrim-
inator. In the following sections, we will describe
each part of our proposed model in detail.

3.1 Embedding Layer

Similar to other neural network models, the ﬁrst
step of our proposed model is to map discrete
characters into the distributed representations. For
a given Chinese sentence x = {c1, c2, . . . , cN }
from Chinese NER dataset or CWS dataset, we
lookup embedding vector from pre-trained embed-
ding matrix for each character ci as xi ∈ Rde.

3.2 Shared-Private Feature Extractor

Long short term memory (LSTM) (Hochreiter and
Schmidhuber, 1997) is a variant of recurrent neu-
ral network (RNN) (Elman, 1990), which enables
to address the gradient vanishing and exploding

problems in RNN via introducing gate mechanism
and memory cell. The unidirectional LSTM on-
ly leverages information from the past, ignoring
the future information. In order to incorporate in-
formation from both sides of sequence, we adopt
BiLSTM to extract features. Specially, the hidden
state of BiLSTM could be expressed as follows:

−→
hi =
←−
hi =

hi =

−−−−→
LSTM(
←−−−−
LSTM(
←−
−→
hi ⊕
hi

−→
h i−1, xi)
←−
h i+1, xi)

(1)

(2)

(3)

−→
hi ∈ Rdh and

←−
hi ∈ Rdh are the hidden
where
states of the forward and backward LSTM at po-
sition i, respectively. ⊕ denotes concatenation op-
eration.

As shown in Figure 2, we propose a shared-
private feature extractor, which assigns a private
BiLSTM layer and shared BiLSTM layer for task
k ∈ {N ER, CW S}. The private BiLSTM lay-
er is used to extract task-speciﬁc features, and the
shared BiLSTM layer is used to learn task-shared
word boundaries. Formally, for any sentence in
dataset of task k, the hidden states of shared and
private BiLSTM layer can be computed as fol-
lows:

sk
i = BiLSTM(xk
i = BiLSTM(xk
hk

i , sk
i , hk

i−1; θs)
i−1; θk)

(4)

(5)

where θs and θk are the shared BiLSTM param-
eters and private BiLSTM parameters of task k,
respectively.

3.3 Self-Attention

Inspired by the self-attention applied to machine
translation (Vaswani et al., 2017) and semantic
role labelling (Tan et al., 2017), we exploit self-
attention to explicitly learn the dependencies be-
tween any two characters in sentence and capture
the inner structure information of sentence.
In
this paper, we adopt the multi-head self-attention
mechanism. H = {h1, h2, . . . , hN } denotes the
output of private BiLSTM. Correspondingly, S =
{s1, s2, . . . , sN } is the output of shared BiLSTM.
We will take the self-attention in private space as
example to illustrate how it works. The scaled dot-
product attention can be precisely described as fol-
lows:

Attention(Q, K, V) = softmax(

)V (6)

QKT
√
d

where Q ∈ RN ×2dh, K ∈ RN ×2dh and V ∈
RN ×2dh are query matrix, keys matrix and value
In our setting, Q = K =
matrix, respectively.
V = H. d is the dimension of hidden units of BiL-
STM, which equals to 2dh.

Multi-head attention ﬁrst linearly projects the
queries, keys and values h times by using differ-
ent linear projections. Then h projections perfor-
m the scaled dot-product attention in parallel. Fi-
nally, these results of attention are concatenated
and once again projected to get the new represen-
tation. Formally, the multi-head attention can be
expressed as follows:

headi = Attention(QWQ
(cid:48)
H

i , KWK
= (headi ⊕ . . . ⊕ headh)Wo

i , VWV
i )

(7)

(8)

i ∈ R2dh×dk , WK

where WQ
i ∈ R2dh×dk and
i ∈ R2dh×dk are trainable projection parame-
WV
ters and dk = 2dh/h. Wo ∈ R2dh×2dh is also
trainable parameter.

3.4 Task-Speciﬁc CRF

For a sentence in dataset of task k, we compute
the ﬁnal representation via concatenating the rep-
resentations from private space and shared space
after self-attention layer:

(cid:48)(cid:48)k = H

(cid:48)k
(cid:48)k ⊕ S

H

(9)

(cid:48)k and S

(cid:48)k are the outputs of private self-
where H
attention and shared self-attention of task k, re-
spectively.

Considering the dependencies between succes-
sive labels, we exploit CRF (Lafferty et al., 2001)
to inference tags instead of making tagging de-
(cid:48)(cid:48)
cisions using h
independently. Due to the d-
i
ifference of labels, we introduce a speciﬁc CR-
F layer for each task. Given a sentence x =
{c1, c2, . . . , cN } with a predicted tag sequence
y = {y1, y2, . . . , yN }, the CRF tagging process
can be formalized as follows:

s(x, y) =

(oi,yi + Tyi−1,yi)

oi = Wsh

(cid:48)(cid:48)

i + bs
N
(cid:88)

i=1
¯y = arg max
y∈Yx

s(x, y)

(10)

(11)

(12)

where Ws ∈ R|T |×4dh and bs ∈ R|T | are train-
able parameters. |T | denotes the number of output
labels. oi,yi represents the score of the yi-th tag

of the character ci. T is a transition score matrix
which deﬁnes the scores of two successive label-
s. Yx represents all candidate tag sequences for
given sentence x. In decoding, we use Viterbi al-
gorithm to get the predicted tag sequence ¯y.

For training, we exploit negative log-likelihood
objective as the loss function. The probability of
the ground-truth label sequence is computed by:

p(ˆy|x) =

(cid:80)

es(x,ˆy)

es(x,(cid:101)y)

(cid:101)y∈Yx

(13)

where ˆy denotes the ground-truth label sequence.
Given T training examples (x(i); ˆy(i)), the loss
function LT ask can be deﬁned as follows:

LT ask = −

logp(ˆy(i)|x(i))

(14)

T
(cid:88)

i=1

task k. x(i)
is the i-th example of task k. There
k
is a minimax optimization that the shared BiLST-
M generates a representation to mislead the task
discriminator and the discriminator tries its best to
correctly determine the type of task.

We add a gradient reversal layer (Ganin and
Lempitsky, 2014) below the softmax layer to ad-
dress the minimax optimization problem. In the
training phrase, we minimize the task discrimi-
nator errors, and through gradient reversal layer
the gradients will become opposed sign to adver-
sarially encourage the shared feature extractor to
learn task-shared word boundary information. Af-
ter training phrase, the shared feature extractor and
task discriminator reach a point where the discrim-
inator cannot differentiate the tasks according to
the representations learned from shared feature ex-
tractor.

We use gradient back-propagation method to min-
imize the loss function.

3.6 Training

3.5 Task Discriminator

Inspired by adversarial networks (Goodfellow
et al., 2014), we incorporate adversarial training
into shared space to guarantee that speciﬁc fea-
tures of tasks do not exist in shared space. We pro-
pose a task discriminator to estimate which task
the sentence comes from. Formally, the task dis-
criminator can be expressed as follows:

(cid:48)k = Maxpooling(S
s

(cid:48)k)

D(s

(cid:48)k; θd) = softmax(Wds

(cid:48)k + bd)

(15)

(16)

where θd indicates the parameters of task discrim-
inator. Wd ∈ RK×2dh and bd ∈ RK are trainable
parameters. K is the number of tasks.

Besides the task loss LT ask, we introduce an ad-
versarial loss LAdv to prevent speciﬁc features of
CWS task from creeping into shared space. The
adversarial loss trains the shared model to produce
shared features such that the task discriminator
cannot reliably recognize which task the sentence
comes from. The adversarial loss can be computed
as follows:

The ﬁnal loss function of our proposed model can
be written as follows:

L = LN ER · I(x) + LCW S · (1 − I(x)) + λLAdv
(18)
where λ is a hyper-parameter. LN ER and LCW S
can be computed via Eq.14. I(x) is a switching
function to identify which task the input comes
from. It is deﬁned as follows:

(cid:40)

I(x) =

1,

0,

if

if

x ∈ DN ER
x ∈ DCW S

(19)

where DN ER and DCW S are Chinese NER train-
ing corpora and CWS training corpora, respective-
ly.

In the training phrase, at each iteration, we ﬁrst
select a task from {N ER, CW S} in turn. Then,
we sample a batch of training instances from the
given task to update the parameters. We use Adam
(Kingma and Ba, 2014) algorithm to optimize the
ﬁnal loss function. Since Chinese NER task and
CWS task may have different convergence rate, we
repeat the above iterations until early stopping ac-
cording to the Chinese NER task performance.

LAdv = min
θs

(max
θd

K
(cid:88)

Tk(cid:88)

k=1

i=1

logD(Es(x(i)

k )))

4 Experiments

4.1 Datasets

(17)
where θs denotes the trainable parameters of
shared BiLSTM. Es denotes the shared feature ex-
tractor. Tk is the number of training examples of

To evaluate our proposed model on Chinese N-
ER, we experiment on two different widely used
datasets,
including Weibo NER dataset (Wei-
boNER) (Peng and Dredze, 2015; He and Sun,

Dataset

Task

WeiboNER Chinese NER
SighanNER Chinese NER

MSR

CWS

# Train sent
1350
41728
86924

# Dev sent
270
4636
—

# Test sent
270
4365
3985

Table 1: Statistics of the datasets.

Models
CRF (Peng and Dredze, 2015)
CRF+word (Peng and Dredze, 2015)
CRF+character (Peng and Dredze, 2015)
CRF+character+position (Peng and Dredze, 2015)
Joint(cp) (main) (Peng and Dredze, 2015)
Pipeline Seg.Repr.+NER (Peng and Dredze, 2016)
Jointly Train Char.Emb (Peng and Dredze, 2016)
Jointly Train LSTM Hidden (Peng and Dredze, 2016)
Jointly Train LSTM+Emb (main) (Peng and Dredze, 2016)
BiLSTM+CRF+adversarial+self-attention

P(%) R(%)
25.26
56.98
25.77
64.94
34.02
57.89
34.53
57.26
35.57
57.98
36.08
64.22
37.11
63.16
38.66
63.03
39.18
63.33
50.68
55.72

F1(%)
35.00
36.90
42.86
43.09
44.09
46.20
46.75
47.92
48.41
53.08

Table 2: NER results for named entities on the original WeiboNER dataset (Peng and Dredze, 2015).
There are three blocks. The ﬁrst two blocks contain the main and simpliﬁed models proposed by Peng
and Dredze (2015) and Peng and Dredze (2016), respectively. The last block lists the performance of our
proposed model.

2017a) and SIGHAN2006 NER dataset (Sighan-
NER) (Levow, 2006). We use the MSR dataset
(from SIGHAN2005) for CWS task.

The number of projections h is 8. We set the batch
size of SighanNER and WeiboNER as 64 and 20,
respectively.

The WeiboNER is annotated with four enti-
ty types (person, location, organization and geo-
political entities), including named entities and
nominal mentions. The SighanNER is simpliﬁed
Chinese, which contains three entity types (per-
son, location and organization). For WeiboNER,
we use the same training, development and test-
ing splits as Peng and Dredze (2015). Since the
SighanNER does not have development set, we
sample 10% data of training set as development
set. We use MSR dataset to improve the perfor-
mance of the Chinese NER task. Table 1 gives the
details of the three datasets.

4.2 Settings

For evaluation, we use the Precision (P), Recall
(R) and F1 score as metrics in our experiment.

For hyper-parameter conﬁgurations, we adjust
them according to the performance on develop-
ment set of Chinese NER task. We set the charac-
ter embedding size de to 100. The dimensionality
of LSTM hidden states dh is 120. The initial learn-
ing rate is set to 0.001. The loss weight coefﬁcient
λ is set to 0.06. We set the dropout rate to 0.3.

For trainable parameters initialization, we use
xavier initializer (Glorot and Bengio, 2010) to
initialize parameters. The character embeddings
used in our experiment are pre-trained on Baidu
Encyclopedia corpus and Weibo corpus by using
word2vec toolkit (Mikolov et al., 2013).

4.3 Compared with State-of-the-art Methods

In this section, we will give the experimental re-
sults of our proposed model and previous state-
of-the-art methods on WeiboNER dataset and
SighanNER dataset, respectively.

4.3.1 Evaluation on WeiboNER

We compare our proposed model with the latest
models on WeiboNER dataset. Table 2 shows the
experimental results for named entities on the o-
riginal WeiboNER dataset.

In the ﬁrst block of Table 2, we give the per-
formance of the main model and baselines pro-
posed by Peng and Dredze (2015). They propose a
CRF-based model to jointly train the embeddings
with NER task, which achieves better results than
pipeline models. In addition, they consider the po-

Models

Peng and Dredze (2015)
Peng and Dredze (2016)
He and Sun (2017a)
He and Sun (2017b)
BiLSTM+CRF+adv+self-attention

Named Entity

Nominal Mention

P(%) R(%)
39.81
74.78
47.22
66.67
40.67
66.93
48.82
61.68
50.00
59.51

F1(%) P(%) R(%)
53.03
71.92
51.96
54.55
74.48
55.28
53.57
66.46
50.60
53.54
74.13
54.50
47.90
71.43
54.34

F1(%)
61.05
62.97
59.32
62.17
57.35

Overall
F1(%)
56.05
58.99
54.82
58.23
58.70

Table 3: Experimental results on the updated WeiboNER dataset (He and Sun, 2017a). There are two
blocks. The ﬁrst block is the performance of latest models. The second block reports the performance of
our proposed model. With the limited length of the page, we use “adv” to denote “adversarial”.

Models
Chen et al. (2006)
Zhou et al. (2006)
Luo and Yang (2016)
BiLSTM+CRF+adversarial+self-attention

P(%) R(%)
81.71
91.22
84.20
88.94
87.22
91.30
89.58
91.73

F1(%)
86.20
86.51
89.21
90.64

Table 4: Results on SighanNER dataset. There are two blocks. The ﬁrst block reports the result of
previous methods. The second block gives the performance of our proposed model.

sition of each character in a word to train character
and position embeddings.

In the second block of Table 2, we report the
performance of the main model and baselines pro-
posed by Peng and Dredze (2016). Aiming to in-
corporate word boundary information into the N-
ER task, they propose an integrated model that can
joint training CWS task, improving the F1 score
from 46.20% to 48.41% as compared with pipeline
model (Pipeline Seg.Repr.+NER).

In the last block of Table 2, we give
the experimental result of our proposed model
(BiLSTM+CRF+adversarial+self-attention). We
can observe that our proposed model signiﬁcant-
ly outperforms other models. Compared with the
model proposed by Peng and Dredze (2016), our
method gains 4.67% improvement in F1 score. In-
terestingly, WeiboNER dataset and MSR dataset
are different domains. The WeiboNER dataset
is social media domain, while the MSR dataset
can be regard as news domain. The improvement
of performance indicates that our proposed adver-
sarial transfer learning framework may not only
learn task-shared word boundary information from
CWS task but also tackle the domain adaptation
problem.

We also conduct an experiment on the updated
WeiboNER dataset. Table 3 lists the performance
of the latest models and our proposed model on
the updated dataset. In the ﬁrst block of Table 3,

we report the performance of the latest models.
The model proposed by Peng and Dredze (2015)
achieves F1 score of 56.05% on overall perfor-
mance. He and Sun (2017b) propose an uniﬁed
model for Chinese NER task to exploit the data
from out-of-domain corpus and in-domain unla-
belled texts. The uniﬁed model improves the F1
score from 54.82% to 58.23% compared with the
model proposed by He and Sun (2017a).

In the second block of Table 3, we give the re-
sult of our proposed model.
It can be observed
that our proposed model achieves a very competi-
tive performance. Compared with the latest model
proposed by He and Sun (2017b), our model im-
proves the F1 score from 58.23% to 58.70% on
overall performance. The improvement demon-
strates the effectiveness of our proposed model.

4.3.2 Evaluation on SighanNER

Table 4 lists the comparisons on SighanNER
dataset. We observe that our proposed model
achieves new state-of-the-art performance.

In the ﬁrst block, we give the performance
of previous methods for Chinese NER task on
SighanNER dataset. Chen et al. (2006) propose
a character-based CRF model for Chinese NER
task. Zhou et al. (2006) introduce a pipeline mod-
el, which ﬁrst segments the text with character-
level CRF model and then applies word-level CRF
to tag. Luo and Yang (2016) ﬁrst train a word seg-
menter and then use word segmentation as addi-

Models

BiLSTM+CRF
BiLSTM+CRF+transfer
BiLSTM+CRF+adversarial
BiLSTM+CRF+self-attention
BiLSTM+CRF+adversarial+self-attention

SighanNER

WeiboNER

P(%) R(%)
88.42
89.84
89.19
90.60
89.56
90.52
88.81
90.62
89.58
91.73

F1(%) P(%) R(%)
44.93
58.99
89.13
46.03
60.00
89.89
45.48
61.94
90.04
47.67
57.81
89.71
50.68
90.64
55.72

F1(%)
51.01
52.09
52.45
52.25
53.08

Table 5: Comparison between our proposed model and simpliﬁed models on SighanNER dataset and
original WeiboNER dataset.

(a) Example for the effectiveness of boundary information.

(b) Example for the effectiveness of self-attention.

Figure 3: The analysis of Chinese NER cases from WeiboNER dataset.

tional features for sequence tagging. Although the
model achieves competitive performance, giving
the F1 score of 89.21%, it suffers from the error
propagation problem.

In the second block, we report the result of
our proposed model. Compared with the state-of-
the-art model proposed by Luo and Yang (2016),
our method improves the F1 score from 89.21%
to 90.64% without any additional features, which
demonstrates the effectiveness of our proposed
model.

4.4 Effectiveness of Adversarial Transfer

Learning and Self-Attention

Table 5 provides the experimental results of our
proposed model and baseline as well as its simpli-
ﬁed models on SighanNER dataset and WeiboN-
ER dataset. The simpliﬁed models are described
as follows:

• BiLSTM+CRF: The model is used as strong
baseline in our work, which is trained using
Chinese NER training data.

• BiLSTM+CRF+transfer: We apply transfer
learning to BiLSTM+CRF model without ad-
versarial loss and self-attention mechanism.

M+CRF+adversarial model incorporates ad-
versarial training.

• BiLSTM+CRF+self-attention: The model
integrates the self-attention mechanism based
on BiLSTM+CRF model.

From the experimental results of Table 5, we

have following observations:

• Effectiveness of transfer learning. BiL-
STM+CRF+transfer improves F1 score from
89.13% to 89.89% as compared with BiLST-
M+CRF on SighanNER dataset and achieves
1.08% improvement on WeiboNER dataset,
which indicates the word boundary informa-
tion from CWS is very effective for Chinese
NER task.

• Effectiveness of adversarial training. By
training, BiLST-
introducing adversarial
M+CRF+adversarial boosts the performance
as compared with BiLSTM+CRF+transfer
model, showing 0.15% and 0.36% improve-
ment on SighanNER dataset and WeiboNER
dataset, respectively. It proves that adversar-
ial training can prevent speciﬁc features of
CWS task from creeping into shared space.

• BiLSTM+CRF+adversarial: Compared with
BiLSTM+CRF+transfer model, the BiLST-

• Effectiveness of self-attention mechanism.
the
When compared with BiLSTM+CRF,

BiLSTM+CRF+self-attention
signiﬁcantly
improves the performance on the two dif-
ferent datasets with the help of information
learned from self-attention, which veriﬁes
that the self-attention mechanism is effective
for Chinese NER task.

We observe that our proposed adversarial trans-
fer learning framework and self-attention lead to
noticeable improvements over the baseline, im-
proving F1 score from 51.01% to 53.08% on Wei-
boNER dataset and giving 1.51% improvement on
SighanNER dataset.

4.5 Detailed Analysis

4.5.1 Case Study

Word boundary information from CWS task is
very important for Chinese NER task, especially
when different entities appear together, . We take
a sentence in WeiboNER test set as example for
illustrating the effectiveness of our proposed mod-
el. As shown in Figure 4(a), when two “person”
entities appearing together, our proposed method
exploits word segmentation information to deter-
mine the boundary between them and then make
correct taggings.
In Figure 4(b), when labelling
the word “(cid:10)ł (the boss)”, the self-attention ex-
plicitly learns the dependencies with “(cid:10) ˝ (re-
spect)”, therefore, our model enables to correctly
classify the word into “person” category. It veri-
ﬁes that the self-attention is very effective for Chi-
nese NER task.

4.5.2 Error Analysis

According to the results of Table 2 and Table 4,
our proposed model achieves 4.67% and 1.43%
improvement as compared with previous state-
of-the-art methods on WeiboNER dataset and
SighanNER dataset, respectively. However, the
overall performance on WeiboNER dataset is rel-
atively low. Two reasons can be explained for this
issue. One reason is that the number of training
examples in WeiboNER dataset is very limited as
compared with SighanNER dataset. There are on-
ly 1.3k examples in WeiboNER training corpora,
which is not enough to train deep neural network-
s. Another reason is that the expression is informal
in social media, lowering the performance on Wei-
boNER dataset. While the greater improvement
on WeiboNER dataset proves that our method is
helpful to solve the problem.

5 Conclusions

In this paper, we propose a novel adversarial trans-
fer learning framework for Chinese NER task,
which can exploit task-shared word boundaries
features and prevent the speciﬁc information of
CWS task. Besides, we introduce self-attention
mechanism to capture the dependencies of arbi-
trary two characters and learn the inner structure
information of sentence. Experiments on two d-
ifferent widely used datasets demonstrate that our
method signiﬁcantly and consistently outperforms
previous state-of-the-art models.

Acknowledgments

The research work is supported by the Natu-
ral Science Foundation of China (No.61533018
and No.61702512), and the independent research
project of National Laboratory of Pattern Recogni-
tion. This work is also supported in part by Beijing
Unisound Information Technology Co., Ltd.

References

Daniel M Bikel, Scott Miller, Richard Schwartz,
and Ralph Weischedel. 1997. Nymble: a high-
performance learning name-ﬁnder. In Proceedings
of the ﬁfth conference on Applied natural language
processing, pages 194–201. Association for Compu-
tational Linguistics.

Konstantinos Bousmalis, George Trigeorgis, Nathan
Silberman, Dilip Krishnan, and Dumitru Erhan.
2016. Domain separation networks. In Advances in
Neural Information Processing Systems, pages 343–
351.

Razvan C Bunescu and Raymond J Mooney. 2005. A
shortest path dependency kernel for relation extrac-
In Proceedings of the conference on human
tion.
language technology and empirical methods in nat-
ural language processing, pages 724–731. Associa-
tion for Computational Linguistics.

Aitao Chen, Fuchun Peng, Roy Shan, and Gordon Sun.
2006. Chinese named entity recognition with con-
ditional probabilistic models. In Proceedings of the
Fifth SIGHAN Workshop on Chinese Language Pro-
cessing, pages 173–176.

Xilun Chen, Yu Sun, Ben Athiwaratkun, Claire Cardie,
and Kilian Weinberger. 2016. Adversarial deep av-
eraging networks for cross-lingual sentiment classi-
ﬁcation. arXiv preprint arXiv:1606.01614.

Xinchi Chen, Zhan Shi, Xipeng Qiu, and Xuanjing
Huang. 2017. Adversarial multi-criteria learning
for chinese word segmentation. arXiv preprint arX-
iv:1704.07556.

Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng,
Event extraction via dy-
and Jun Zhao. 2015.
namic multi-pooling convolutional neural network-
s. In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing, volume 1, pages 167–176.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
Journal of Machine Learning Research,
scratch.
12(Aug):2493–2537.

Emily L Denton, Soumith Chintala, Rob Fergus, et al.
2015. Deep generative image models using a lapla-
cian pyramid of adversarial networks. In Advances
in neural information processing systems, pages
1486–1494.

Jeffrey L Elman. 1990. Finding structure in time. Cog-

nitive science, 14(2):179–211.

Yaroslav Ganin and Victor Lempitsky. 2014. Unsuper-
vised domain adaptation by backpropagation. arXiv
preprint arXiv:1409.7495.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difﬁculty of training deep feedforward neu-
In Proceedings of the thirteenth in-
ral networks.
ternational conference on artiﬁcial intelligence and
statistics, pages 249–256.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
In Advances in neural information
versarial nets.
processing systems, pages 2672–2680.

Tao Gui, Qi Zhang, Haoran Huang, Minlong Peng, and
Xuanjing Huang. 2017. Part-of-speech tagging for
In Pro-
twitter with adversarial neural networks.
ceedings of the 2017 Conference on Empirical Meth-
ods in Natural Language Processing, pages 2411–
2420.

Hangfeng He and Xu Sun. 2017a. F-score driven max
margin neural network for named entity recognition
in chinese social media. EACL 2017, page 713.

Hangfeng He and Xu Sun. 2017b. A uniﬁed model
for cross-domain and semi-supervised named entity
recognition in chinese social media. In AAAI, pages
3216–3222.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-
tional lstm-crf models for sequence tagging. arXiv
preprint arXiv:1508.01991.

Hideki Isozaki and Hideto Kazawa. 2002. Efﬁcien-
t support vector classiﬁers for named entity recog-
In Proceedings of the 19th international
nition.

conference on Computational linguistics-Volume 1,
pages 1–7. Association for Computational Linguis-
tics.

Joo-Kyung Kim, Young-Bum Kim, Ruhi Sarikaya, and
Eric Fosler-Lussier. 2017. Cross-lingual transfer
learning for pos tagging without cross-lingual re-
sources. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 2832–2838.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random ﬁelds: Prob-
abilistic models for segmenting and labeling se-
quence data.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
arXiv preprint arXiv:1603.01360.

Gina-Anne Levow. 2006. The third international chi-
nese language processing bakeoff: Word segmen-
In Proceed-
tation and named entity recognition.
ings of the Fifth SIGHAN Workshop on Chinese Lan-
guage Processing, pages 108–117.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2017.
Adversarial multi-task learning for text classiﬁca-
tion. arXiv preprint arXiv:1704.05742.

Wencan Luo and Fan Yang. 2016. An empirical study
of automatic chinese word segmentation for spoken
language understanding and named entity recogni-
In Proceedings of the 2016 Conference of
tion.
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 238–248.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efﬁcient estimation of word rep-
resentations in vector space. arXiv preprint arX-
iv:1301.3781.

Nanyun Peng and Mark Dredze. 2015. Named enti-
ty recognition for chinese social media with joint-
ly trained embeddings. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 548–554.

Nanyun Peng and Mark Dredze. 2016.

Improving
named entity recognition for chinese social media
with word segmentation representation learning. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics, volume 2,
pages 149–155.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Dis-
Shirui Pan, and Chengqi Zhang. 2017.
an: Directional self-attention network for rnn/cnn-
free language understanding. arXiv preprint arX-
iv:1709.04696.

Zhixing Tan, Mingxuan Wang, Jun Xie, Yidong Chen,
and Xiaodong Shi. 2017. Deep semantic role la-
arXiv preprint arX-
beling with self-attention.
iv:1712.01586.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 6000–6010.

Chunqi Wang, Wei Chen, and Bo Xu. 2017. Named en-
tity recognition with gated convolutional neural net-
In Chinese Computational Linguistics and
works.
Natural Language Processing Based on Naturally
Annotated Big Data, pages 110–121. Springer.

YaoSheng Yang, Meishan Zhang, Wenliang Chen, Wei
Zhang, Haofen Wang, and Min Zhang. 2018. Ad-
versarial learning for chinese ner from crowd anno-
tations. arXiv preprint arXiv:1801.05147.

Xuchen Yao and Benjamin Van Durme. 2014.

Infor-
mation extraction over structured data: Question an-
swering with freebase. In Proceedings of the 52nd
Annual Meeting of the Association for Computation-
al Linguistics, volume 1, pages 956–966.

Yuan Zhang, Regina Barzilay, and Tommi Jaakko-
la. 2017. Aspect-augmented adversarial network-
arXiv preprint arX-
s for domain adaptation.
iv:1701.00188.

Junsheng Zhou, Liang He, Xinyu Dai, and Jiajun Chen.
2006. Chinese named entity recognition with a
In Proceedings of the Fifth
multi-phase model.
SIGHAN Workshop on Chinese Language Process-
ing, pages 213–216.

Junsheng Zhou, Weiguang Qu, and Fen Zhang. 2013.
Chinese named entity recognition via joint identiﬁ-
cation and categorization. Chinese journal of elec-
tronics, 22(2):225–230.

Adversarial Transfer Learning for Chinese Named Entity Recognition
with Self-Attention Mechanism

Pengfei Cao1,2, Yubo Chen1, Kang Liu1,2, Jun Zhao1,2 and Shengping Liu3
1 National Laboratory of Pattern Recognition, Institute of Automation,
Chinese Academy of Sciences, Beijing, 100190, China
2 University of Chinese Academy of Sciences, Beijing, 100049, China
3 Beijing Unisound Information Technology Co., Ltd, Beijing, 100028, China
{pengfei.cao, yubo.chen, kliu, jzhao}@nlpr.ia.ac.cn, liushengping@unisound.com

Abstract

Named entity recognition (NER) is an impor-
tant task in natural language processing area,
which needs to determine entities boundaries
and classify them into pre-deﬁned categories.
For Chinese NER task, there is only a very s-
mall amount of annotated data available. Chi-
nese NER task and Chinese word segmen-
tation (CWS) task have many similar word
boundaries. There are also speciﬁcities in each
task. However, existing methods for Chinese
NER either do not exploit word boundary in-
formation from CWS or cannot ﬁlter the spe-
ciﬁc information of CWS. In this paper, we
propose a novel adversarial transfer learning
framework to make full use of task-shared
boundaries information and prevent the task-
speciﬁc features of CWS. Besides, since ar-
bitrary character can provide important cues
when predicting entity type, we exploit self-
attention to explicitly capture long range de-
pendencies between two tokens. Experimental
results on two different widely used dataset-
s show that our proposed model signiﬁcant-
ly and consistently outperforms other state-of-
the-art methods.

1

Introduction

The task of named entity recognition (NER) is to
recognize the named entities in given text. N-
ER is a preliminary and important task in natural
language processing (NLP) area and can be used
in many downstream NLP tasks, such as relation
extraction (Bunescu and Mooney, 2005), even-
t extraction (Chen et al., 2015) and question an-
In recent
swering (Yao and Van Durme, 2014).
years, numerous methods have been carefully s-
tudied for NER task, including Hidden Markov
Models (HMMs) (Bikel et al., 1997), Support Vec-
tor Machines (SVMs) (Isozaki and Kazawa, 2002)
and Conditional Random Fields (CRFs) (Laffer-
ty et al., 2001). Currently, with the development

Figure 1: An example of illustrating the similarities
and speciﬁcities between Chinese NER and CWS.

of deep learning, neural networks (Lample et al.,
2016; Peng and Dredze, 2016; Luo and Yang,
2016) have been introduced to NER task. All these
methods need to determine entities boundaries
and classify them into pre-deﬁned categories.

Although great

improvements have been
achieved by these methods on Chinese NER task,
some issues still have not been well addressed.
One signiﬁcant drawback is that there is only a
very small amount of annotated data available.
Weibo NER dataset (Peng and Dredze, 2015; He
and Sun, 2017a) and Sighan2006 NER dataset
(Levow, 2006) are two widely used datasets
for Chinese NER task, containing 1.3k and 45k
training examples,
respectively. On the two
datasets, the highest F1 scores are 48.41% and
89.21%, respectively. As a basic task in NLP area,
the performance is not satisfactory. Fortunately,
Chinese word segmentation (CWS) task is to
recognize word boundaries and the amount of
supervised training data for CWS is abundant
compared with NER. There are many similarities
between Chinese NER task and CWS task, which
we call task-shared information. As shown in
Figure 1, given a sentence “(cid:12)(cid:20)(cid:127)»(cid:0)(cid:17)ﬂ(cid:127):
: (Hilton leaves Houston Airport)”, the two tasks
have the same boundaries for some words such
as “(cid:12)(cid:20)(cid:127) (Hilton)” and “»(cid:0) (leaves)”, while
Chinese NER has more coarse-grained boundaries

than CWS task for certain word such as “(cid:17)ﬂ(cid:127)
:: (Houston Airport)” in the example of Figure
1, which we call task-speciﬁc information.
In
order to incorporate word boundary information
from CWS task into NER task, Peng and Dredze
(2016) propose a joint model
that performs
Chinese NER with CWS task. However, their
proposed model only focuses on task-shared
information between Chinese NER and CWS,
and ignores ﬁltering the speciﬁcities of each
task, which will bring noise for both of the tasks.
For example, the CWS task splits “(cid:17)ﬂ(cid:127)::
(Houston Airport)” into “(cid:17)ﬂ(cid:127) (Houston)” and
“:: (Airport)”, while the NER task takes “(cid:17)
ﬂ(cid:127):: (Houston Airport)” as a whole entity.
Thus, how to exploit task-shared information and
prevent the noise brought by CWS task to Chinese
NER task is a challenging problem.

Another issue is that most proposed models
cannot explicitly model long range dependencies
when predicting entity type. Though bidirection-
al long short term memory (BiLSTM) can learn
long-distance dependencies, it cannot conduct di-
rect connections between arbitrary two characters.
As shown in Figure 1, if the model only focuses
on the word “(cid:12)(cid:20)(cid:127) (Hilton)”, it can be a person
or organization. However, when the model explic-
itly captures the dependencies between “(cid:12)(cid:20)(cid:127)
(Hilton)” and “»(cid:0) (leaves)”, it is easy to classify
“(cid:12)(cid:20)(cid:127) (Hilton)” into “person” category. Con-
text information is very crucial for determining the
entity type. While in the sentence “(cid:17)(cid:6)O((cid:12)(cid:20)
(cid:127) (I will be staying at the Hilton)”, the entity type
of “(cid:12)(cid:20)(cid:127) (Hilton)” is “organization”. Thus, how
to better capture the global dependencies of the w-
hole sentence is another challenging problem.

To address the above problems, we propose an
adversarial transfer learning framework to inte-
grate the task-shared word boundary information
into Chinese NER task in this paper. The adver-
sarial transfer learning is incorporating adversari-
al training into transfer learning. To better capture
long range dependencies and synthesize the infor-
mation of the sentence, we extend self-attention
mechanism into the framework. Speciﬁcally, we
try to improve Chinese NER task performance by
incorporating shared boundary information from
CWS task. To prevent the speciﬁc information
of CWS task from lowering the performance of
the Chinese NER task, we introduce adversarial
training to ensure that the Chinese NER task on-

ly exploits task-shared word boundary informa-
tion. Then, for tackling the long range dependen-
cy problems, we utilize self-attention to synthe-
size the hidden representation of BiLSTM. Final-
ly, we evaluate our model on two different widely
used Chinese NER datasets. Experimental results
show that our proposed model achieves better per-
formance than other state-of-the-art methods and
gains new benchmarks.

In summary, the contributions of this paper are

as follows:

• We propose an adversarial transfer learning
framework to incorporate task-shared word
boundary information from CWS task into
Chinese NER task. To our best knowledge,
it is the ﬁrst work to apply adversarial trans-
fer learning method into NER task.

• We introduce self-attention mechanism into
our model, which aims to capture the global
dependencies of the whole sentence and learn
inner structure features of sentence.

• We conduct our experiment on two dif-
ferent widely used Chinese NER datasets,
and the experimental results demonstrate that
our proposed model signiﬁcantly and consis-
tently outperforms previous state-of-the-art
methods. We release the source code publicly
for further research1.

2 Related Work

NER Many methods have been proposed for N-
ER task. Early studies on NER often exploit
SVMs (Isozaki and Kazawa, 2002), HMMs (Bikel
et al., 1997) and CRFs (Lafferty et al., 2001),
heavily relying on feature engineering. Zhou et al.
(2013) formulate Chinese NER as a joint identi-
ﬁcation and categorization task. In recent years,
neural network models have been introduced to N-
ER task (Collobert et al., 2011; Huang et al., 2015;
Peng and Dredze, 2016). Huang et al. (2015) ex-
ploit BiLSTM to extract features and feed them
into CRF decoder. After that, the BiLSTM-CRF
model is usually exploited as the baseline. Lam-
ple et al. (2016) use a character LSTM to represent
spelling characteristics. In addition, Wang et al.
(2017) propose a gated convolutional neural net-
work (GCNN) model for Chinese NER. Peng and
Dredze (2016) propose a joint model for Chinese

1https://github.com/CPF-NLPR/AT4ChineseNER

Figure 2: The general architecture of our proposed model. The left and right part are Chinese NER space
and CWS private space, respectively, including embedding layer, feature extractor (Private BiLSTM),
self-attention and CRF layer. The middle part is shared space consisting of feature extractor (Shared
BiLSTM), self-attention and task discriminator.

NER, which are jointly trained with CWS task.
However, the speciﬁc features brought by CWS
task can lower the performance of the Chinese N-
ER task.

Adversarial Training Adversarial networks
have achieved great success in computer vision
(Goodfellow et al., 2014; Denton et al., 2015).
In NLP area, adversarial training has been intro-
duced for domain adaptation (Ganin and Lempit-
sky, 2014; Zhang et al., 2017; Gui et al., 2017),
cross-lingual transfer learning (Chen et al., 2016;
Kim et al., 2017), multi-task learning (Chen et al.,
2017; Liu et al., 2017) and crowdsourcing learning
(Yang et al., 2018). Bousmalis et al. (2016) pro-
pose shared-private model in domain separation
network. Different from these works, we exploit
adversarial network to jointly train Chinese NER
task and CWS task, aiming to extract task-shared
word boundary information from CWS task. To
our knowledge, it is the ﬁrst work to apply adver-
sarial transfer learning framework to Chinese NER
task.

Self-Attention Self-attention has been intro-
duced to machine translation by Vaswani et al.
(2017) for capturing global dependencies between
input and output and achieves state-of-the-art per-
formance. For language understanding task, Shen
et al. (2017) exploit self-attention to learn long
Tan et al. (2017) apply
range dependencies.
self-attention to semantic role labelling task and
achieve state-of-the-art results. We are the ﬁrst to

introduce self-attention mechanism to Chinese N-
ER task.

3 Method

In this paper, we propose a novel adversarial trans-
fer learning framework that will learn task-shared
word boundary information from CWS task, ﬁlter
speciﬁc information of CWS and explicitly cap-
ture the long range dependencies between arbi-
trary two characters in sentence. The architecture
of our proposed model is illustrated in Figure 2.
The model mainly consists of ﬁve components:
embedding layer, shared-private feature extractor,
self-attention, task-speciﬁc CRF and task discrim-
inator. In the following sections, we will describe
each part of our proposed model in detail.

3.1 Embedding Layer

Similar to other neural network models, the ﬁrst
step of our proposed model is to map discrete
characters into the distributed representations. For
a given Chinese sentence x = {c1, c2, . . . , cN }
from Chinese NER dataset or CWS dataset, we
lookup embedding vector from pre-trained embed-
ding matrix for each character ci as xi ∈ Rde.

3.2 Shared-Private Feature Extractor

Long short term memory (LSTM) (Hochreiter and
Schmidhuber, 1997) is a variant of recurrent neu-
ral network (RNN) (Elman, 1990), which enables
to address the gradient vanishing and exploding

problems in RNN via introducing gate mechanism
and memory cell. The unidirectional LSTM on-
ly leverages information from the past, ignoring
the future information. In order to incorporate in-
formation from both sides of sequence, we adopt
BiLSTM to extract features. Specially, the hidden
state of BiLSTM could be expressed as follows:

−→
hi =
←−
hi =

hi =

−−−−→
LSTM(
←−−−−
LSTM(
←−
−→
hi ⊕
hi

−→
h i−1, xi)
←−
h i+1, xi)

(1)

(2)

(3)

−→
hi ∈ Rdh and

←−
hi ∈ Rdh are the hidden
where
states of the forward and backward LSTM at po-
sition i, respectively. ⊕ denotes concatenation op-
eration.

As shown in Figure 2, we propose a shared-
private feature extractor, which assigns a private
BiLSTM layer and shared BiLSTM layer for task
k ∈ {N ER, CW S}. The private BiLSTM lay-
er is used to extract task-speciﬁc features, and the
shared BiLSTM layer is used to learn task-shared
word boundaries. Formally, for any sentence in
dataset of task k, the hidden states of shared and
private BiLSTM layer can be computed as fol-
lows:

sk
i = BiLSTM(xk
i = BiLSTM(xk
hk

i , sk
i , hk

i−1; θs)
i−1; θk)

(4)

(5)

where θs and θk are the shared BiLSTM param-
eters and private BiLSTM parameters of task k,
respectively.

3.3 Self-Attention

Inspired by the self-attention applied to machine
translation (Vaswani et al., 2017) and semantic
role labelling (Tan et al., 2017), we exploit self-
attention to explicitly learn the dependencies be-
tween any two characters in sentence and capture
the inner structure information of sentence.
In
this paper, we adopt the multi-head self-attention
mechanism. H = {h1, h2, . . . , hN } denotes the
output of private BiLSTM. Correspondingly, S =
{s1, s2, . . . , sN } is the output of shared BiLSTM.
We will take the self-attention in private space as
example to illustrate how it works. The scaled dot-
product attention can be precisely described as fol-
lows:

Attention(Q, K, V) = softmax(

)V (6)

QKT
√
d

where Q ∈ RN ×2dh, K ∈ RN ×2dh and V ∈
RN ×2dh are query matrix, keys matrix and value
In our setting, Q = K =
matrix, respectively.
V = H. d is the dimension of hidden units of BiL-
STM, which equals to 2dh.

Multi-head attention ﬁrst linearly projects the
queries, keys and values h times by using differ-
ent linear projections. Then h projections perfor-
m the scaled dot-product attention in parallel. Fi-
nally, these results of attention are concatenated
and once again projected to get the new represen-
tation. Formally, the multi-head attention can be
expressed as follows:

headi = Attention(QWQ
(cid:48)
H

i , KWK
= (headi ⊕ . . . ⊕ headh)Wo

i , VWV
i )

(7)

(8)

i ∈ R2dh×dk , WK

where WQ
i ∈ R2dh×dk and
i ∈ R2dh×dk are trainable projection parame-
WV
ters and dk = 2dh/h. Wo ∈ R2dh×2dh is also
trainable parameter.

3.4 Task-Speciﬁc CRF

For a sentence in dataset of task k, we compute
the ﬁnal representation via concatenating the rep-
resentations from private space and shared space
after self-attention layer:

(cid:48)(cid:48)k = H

(cid:48)k
(cid:48)k ⊕ S

H

(9)

(cid:48)k and S

(cid:48)k are the outputs of private self-
where H
attention and shared self-attention of task k, re-
spectively.

Considering the dependencies between succes-
sive labels, we exploit CRF (Lafferty et al., 2001)
to inference tags instead of making tagging de-
(cid:48)(cid:48)
cisions using h
independently. Due to the d-
i
ifference of labels, we introduce a speciﬁc CR-
F layer for each task. Given a sentence x =
{c1, c2, . . . , cN } with a predicted tag sequence
y = {y1, y2, . . . , yN }, the CRF tagging process
can be formalized as follows:

s(x, y) =

(oi,yi + Tyi−1,yi)

oi = Wsh

(cid:48)(cid:48)

i + bs
N
(cid:88)

i=1
¯y = arg max
y∈Yx

s(x, y)

(10)

(11)

(12)

where Ws ∈ R|T |×4dh and bs ∈ R|T | are train-
able parameters. |T | denotes the number of output
labels. oi,yi represents the score of the yi-th tag

of the character ci. T is a transition score matrix
which deﬁnes the scores of two successive label-
s. Yx represents all candidate tag sequences for
given sentence x. In decoding, we use Viterbi al-
gorithm to get the predicted tag sequence ¯y.

For training, we exploit negative log-likelihood
objective as the loss function. The probability of
the ground-truth label sequence is computed by:

p(ˆy|x) =

(cid:80)

es(x,ˆy)

es(x,(cid:101)y)

(cid:101)y∈Yx

(13)

where ˆy denotes the ground-truth label sequence.
Given T training examples (x(i); ˆy(i)), the loss
function LT ask can be deﬁned as follows:

LT ask = −

logp(ˆy(i)|x(i))

(14)

T
(cid:88)

i=1

task k. x(i)
is the i-th example of task k. There
k
is a minimax optimization that the shared BiLST-
M generates a representation to mislead the task
discriminator and the discriminator tries its best to
correctly determine the type of task.

We add a gradient reversal layer (Ganin and
Lempitsky, 2014) below the softmax layer to ad-
dress the minimax optimization problem. In the
training phrase, we minimize the task discrimi-
nator errors, and through gradient reversal layer
the gradients will become opposed sign to adver-
sarially encourage the shared feature extractor to
learn task-shared word boundary information. Af-
ter training phrase, the shared feature extractor and
task discriminator reach a point where the discrim-
inator cannot differentiate the tasks according to
the representations learned from shared feature ex-
tractor.

We use gradient back-propagation method to min-
imize the loss function.

3.6 Training

3.5 Task Discriminator

Inspired by adversarial networks (Goodfellow
et al., 2014), we incorporate adversarial training
into shared space to guarantee that speciﬁc fea-
tures of tasks do not exist in shared space. We pro-
pose a task discriminator to estimate which task
the sentence comes from. Formally, the task dis-
criminator can be expressed as follows:

(cid:48)k = Maxpooling(S
s

(cid:48)k)

D(s

(cid:48)k; θd) = softmax(Wds

(cid:48)k + bd)

(15)

(16)

where θd indicates the parameters of task discrim-
inator. Wd ∈ RK×2dh and bd ∈ RK are trainable
parameters. K is the number of tasks.

Besides the task loss LT ask, we introduce an ad-
versarial loss LAdv to prevent speciﬁc features of
CWS task from creeping into shared space. The
adversarial loss trains the shared model to produce
shared features such that the task discriminator
cannot reliably recognize which task the sentence
comes from. The adversarial loss can be computed
as follows:

The ﬁnal loss function of our proposed model can
be written as follows:

L = LN ER · I(x) + LCW S · (1 − I(x)) + λLAdv
(18)
where λ is a hyper-parameter. LN ER and LCW S
can be computed via Eq.14. I(x) is a switching
function to identify which task the input comes
from. It is deﬁned as follows:

(cid:40)

I(x) =

1,

0,

if

if

x ∈ DN ER
x ∈ DCW S

(19)

where DN ER and DCW S are Chinese NER train-
ing corpora and CWS training corpora, respective-
ly.

In the training phrase, at each iteration, we ﬁrst
select a task from {N ER, CW S} in turn. Then,
we sample a batch of training instances from the
given task to update the parameters. We use Adam
(Kingma and Ba, 2014) algorithm to optimize the
ﬁnal loss function. Since Chinese NER task and
CWS task may have different convergence rate, we
repeat the above iterations until early stopping ac-
cording to the Chinese NER task performance.

LAdv = min
θs

(max
θd

K
(cid:88)

Tk(cid:88)

k=1

i=1

logD(Es(x(i)

k )))

4 Experiments

4.1 Datasets

(17)
where θs denotes the trainable parameters of
shared BiLSTM. Es denotes the shared feature ex-
tractor. Tk is the number of training examples of

To evaluate our proposed model on Chinese N-
ER, we experiment on two different widely used
datasets,
including Weibo NER dataset (Wei-
boNER) (Peng and Dredze, 2015; He and Sun,

Dataset

Task

WeiboNER Chinese NER
SighanNER Chinese NER

MSR

CWS

# Train sent
1350
41728
86924

# Dev sent
270
4636
—

# Test sent
270
4365
3985

Table 1: Statistics of the datasets.

Models
CRF (Peng and Dredze, 2015)
CRF+word (Peng and Dredze, 2015)
CRF+character (Peng and Dredze, 2015)
CRF+character+position (Peng and Dredze, 2015)
Joint(cp) (main) (Peng and Dredze, 2015)
Pipeline Seg.Repr.+NER (Peng and Dredze, 2016)
Jointly Train Char.Emb (Peng and Dredze, 2016)
Jointly Train LSTM Hidden (Peng and Dredze, 2016)
Jointly Train LSTM+Emb (main) (Peng and Dredze, 2016)
BiLSTM+CRF+adversarial+self-attention

P(%) R(%)
25.26
56.98
25.77
64.94
34.02
57.89
34.53
57.26
35.57
57.98
36.08
64.22
37.11
63.16
38.66
63.03
39.18
63.33
50.68
55.72

F1(%)
35.00
36.90
42.86
43.09
44.09
46.20
46.75
47.92
48.41
53.08

Table 2: NER results for named entities on the original WeiboNER dataset (Peng and Dredze, 2015).
There are three blocks. The ﬁrst two blocks contain the main and simpliﬁed models proposed by Peng
and Dredze (2015) and Peng and Dredze (2016), respectively. The last block lists the performance of our
proposed model.

2017a) and SIGHAN2006 NER dataset (Sighan-
NER) (Levow, 2006). We use the MSR dataset
(from SIGHAN2005) for CWS task.

The number of projections h is 8. We set the batch
size of SighanNER and WeiboNER as 64 and 20,
respectively.

The WeiboNER is annotated with four enti-
ty types (person, location, organization and geo-
political entities), including named entities and
nominal mentions. The SighanNER is simpliﬁed
Chinese, which contains three entity types (per-
son, location and organization). For WeiboNER,
we use the same training, development and test-
ing splits as Peng and Dredze (2015). Since the
SighanNER does not have development set, we
sample 10% data of training set as development
set. We use MSR dataset to improve the perfor-
mance of the Chinese NER task. Table 1 gives the
details of the three datasets.

4.2 Settings

For evaluation, we use the Precision (P), Recall
(R) and F1 score as metrics in our experiment.

For hyper-parameter conﬁgurations, we adjust
them according to the performance on develop-
ment set of Chinese NER task. We set the charac-
ter embedding size de to 100. The dimensionality
of LSTM hidden states dh is 120. The initial learn-
ing rate is set to 0.001. The loss weight coefﬁcient
λ is set to 0.06. We set the dropout rate to 0.3.

For trainable parameters initialization, we use
xavier initializer (Glorot and Bengio, 2010) to
initialize parameters. The character embeddings
used in our experiment are pre-trained on Baidu
Encyclopedia corpus and Weibo corpus by using
word2vec toolkit (Mikolov et al., 2013).

4.3 Compared with State-of-the-art Methods

In this section, we will give the experimental re-
sults of our proposed model and previous state-
of-the-art methods on WeiboNER dataset and
SighanNER dataset, respectively.

4.3.1 Evaluation on WeiboNER

We compare our proposed model with the latest
models on WeiboNER dataset. Table 2 shows the
experimental results for named entities on the o-
riginal WeiboNER dataset.

In the ﬁrst block of Table 2, we give the per-
formance of the main model and baselines pro-
posed by Peng and Dredze (2015). They propose a
CRF-based model to jointly train the embeddings
with NER task, which achieves better results than
pipeline models. In addition, they consider the po-

Models

Peng and Dredze (2015)
Peng and Dredze (2016)
He and Sun (2017a)
He and Sun (2017b)
BiLSTM+CRF+adv+self-attention

Named Entity

Nominal Mention

P(%) R(%)
39.81
74.78
47.22
66.67
40.67
66.93
48.82
61.68
50.00
59.51

F1(%) P(%) R(%)
53.03
71.92
51.96
54.55
74.48
55.28
53.57
66.46
50.60
53.54
74.13
54.50
47.90
71.43
54.34

F1(%)
61.05
62.97
59.32
62.17
57.35

Overall
F1(%)
56.05
58.99
54.82
58.23
58.70

Table 3: Experimental results on the updated WeiboNER dataset (He and Sun, 2017a). There are two
blocks. The ﬁrst block is the performance of latest models. The second block reports the performance of
our proposed model. With the limited length of the page, we use “adv” to denote “adversarial”.

Models
Chen et al. (2006)
Zhou et al. (2006)
Luo and Yang (2016)
BiLSTM+CRF+adversarial+self-attention

P(%) R(%)
81.71
91.22
84.20
88.94
87.22
91.30
89.58
91.73

F1(%)
86.20
86.51
89.21
90.64

Table 4: Results on SighanNER dataset. There are two blocks. The ﬁrst block reports the result of
previous methods. The second block gives the performance of our proposed model.

sition of each character in a word to train character
and position embeddings.

In the second block of Table 2, we report the
performance of the main model and baselines pro-
posed by Peng and Dredze (2016). Aiming to in-
corporate word boundary information into the N-
ER task, they propose an integrated model that can
joint training CWS task, improving the F1 score
from 46.20% to 48.41% as compared with pipeline
model (Pipeline Seg.Repr.+NER).

In the last block of Table 2, we give
the experimental result of our proposed model
(BiLSTM+CRF+adversarial+self-attention). We
can observe that our proposed model signiﬁcant-
ly outperforms other models. Compared with the
model proposed by Peng and Dredze (2016), our
method gains 4.67% improvement in F1 score. In-
terestingly, WeiboNER dataset and MSR dataset
are different domains. The WeiboNER dataset
is social media domain, while the MSR dataset
can be regard as news domain. The improvement
of performance indicates that our proposed adver-
sarial transfer learning framework may not only
learn task-shared word boundary information from
CWS task but also tackle the domain adaptation
problem.

We also conduct an experiment on the updated
WeiboNER dataset. Table 3 lists the performance
of the latest models and our proposed model on
the updated dataset. In the ﬁrst block of Table 3,

we report the performance of the latest models.
The model proposed by Peng and Dredze (2015)
achieves F1 score of 56.05% on overall perfor-
mance. He and Sun (2017b) propose an uniﬁed
model for Chinese NER task to exploit the data
from out-of-domain corpus and in-domain unla-
belled texts. The uniﬁed model improves the F1
score from 54.82% to 58.23% compared with the
model proposed by He and Sun (2017a).

In the second block of Table 3, we give the re-
sult of our proposed model.
It can be observed
that our proposed model achieves a very competi-
tive performance. Compared with the latest model
proposed by He and Sun (2017b), our model im-
proves the F1 score from 58.23% to 58.70% on
overall performance. The improvement demon-
strates the effectiveness of our proposed model.

4.3.2 Evaluation on SighanNER

Table 4 lists the comparisons on SighanNER
dataset. We observe that our proposed model
achieves new state-of-the-art performance.

In the ﬁrst block, we give the performance
of previous methods for Chinese NER task on
SighanNER dataset. Chen et al. (2006) propose
a character-based CRF model for Chinese NER
task. Zhou et al. (2006) introduce a pipeline mod-
el, which ﬁrst segments the text with character-
level CRF model and then applies word-level CRF
to tag. Luo and Yang (2016) ﬁrst train a word seg-
menter and then use word segmentation as addi-

Models

BiLSTM+CRF
BiLSTM+CRF+transfer
BiLSTM+CRF+adversarial
BiLSTM+CRF+self-attention
BiLSTM+CRF+adversarial+self-attention

SighanNER

WeiboNER

P(%) R(%)
88.42
89.84
89.19
90.60
89.56
90.52
88.81
90.62
89.58
91.73

F1(%) P(%) R(%)
44.93
58.99
89.13
46.03
60.00
89.89
45.48
61.94
90.04
47.67
57.81
89.71
50.68
90.64
55.72

F1(%)
51.01
52.09
52.45
52.25
53.08

Table 5: Comparison between our proposed model and simpliﬁed models on SighanNER dataset and
original WeiboNER dataset.

(a) Example for the effectiveness of boundary information.

(b) Example for the effectiveness of self-attention.

Figure 3: The analysis of Chinese NER cases from WeiboNER dataset.

tional features for sequence tagging. Although the
model achieves competitive performance, giving
the F1 score of 89.21%, it suffers from the error
propagation problem.

In the second block, we report the result of
our proposed model. Compared with the state-of-
the-art model proposed by Luo and Yang (2016),
our method improves the F1 score from 89.21%
to 90.64% without any additional features, which
demonstrates the effectiveness of our proposed
model.

4.4 Effectiveness of Adversarial Transfer

Learning and Self-Attention

Table 5 provides the experimental results of our
proposed model and baseline as well as its simpli-
ﬁed models on SighanNER dataset and WeiboN-
ER dataset. The simpliﬁed models are described
as follows:

• BiLSTM+CRF: The model is used as strong
baseline in our work, which is trained using
Chinese NER training data.

• BiLSTM+CRF+transfer: We apply transfer
learning to BiLSTM+CRF model without ad-
versarial loss and self-attention mechanism.

M+CRF+adversarial model incorporates ad-
versarial training.

• BiLSTM+CRF+self-attention: The model
integrates the self-attention mechanism based
on BiLSTM+CRF model.

From the experimental results of Table 5, we

have following observations:

• Effectiveness of transfer learning. BiL-
STM+CRF+transfer improves F1 score from
89.13% to 89.89% as compared with BiLST-
M+CRF on SighanNER dataset and achieves
1.08% improvement on WeiboNER dataset,
which indicates the word boundary informa-
tion from CWS is very effective for Chinese
NER task.

• Effectiveness of adversarial training. By
training, BiLST-
introducing adversarial
M+CRF+adversarial boosts the performance
as compared with BiLSTM+CRF+transfer
model, showing 0.15% and 0.36% improve-
ment on SighanNER dataset and WeiboNER
dataset, respectively. It proves that adversar-
ial training can prevent speciﬁc features of
CWS task from creeping into shared space.

• BiLSTM+CRF+adversarial: Compared with
BiLSTM+CRF+transfer model, the BiLST-

• Effectiveness of self-attention mechanism.
the
When compared with BiLSTM+CRF,

BiLSTM+CRF+self-attention
signiﬁcantly
improves the performance on the two dif-
ferent datasets with the help of information
learned from self-attention, which veriﬁes
that the self-attention mechanism is effective
for Chinese NER task.

We observe that our proposed adversarial trans-
fer learning framework and self-attention lead to
noticeable improvements over the baseline, im-
proving F1 score from 51.01% to 53.08% on Wei-
boNER dataset and giving 1.51% improvement on
SighanNER dataset.

4.5 Detailed Analysis

4.5.1 Case Study

Word boundary information from CWS task is
very important for Chinese NER task, especially
when different entities appear together, . We take
a sentence in WeiboNER test set as example for
illustrating the effectiveness of our proposed mod-
el. As shown in Figure 4(a), when two “person”
entities appearing together, our proposed method
exploits word segmentation information to deter-
mine the boundary between them and then make
correct taggings.
In Figure 4(b), when labelling
the word “(cid:10)ł (the boss)”, the self-attention ex-
plicitly learns the dependencies with “(cid:10) ˝ (re-
spect)”, therefore, our model enables to correctly
classify the word into “person” category. It veri-
ﬁes that the self-attention is very effective for Chi-
nese NER task.

4.5.2 Error Analysis

According to the results of Table 2 and Table 4,
our proposed model achieves 4.67% and 1.43%
improvement as compared with previous state-
of-the-art methods on WeiboNER dataset and
SighanNER dataset, respectively. However, the
overall performance on WeiboNER dataset is rel-
atively low. Two reasons can be explained for this
issue. One reason is that the number of training
examples in WeiboNER dataset is very limited as
compared with SighanNER dataset. There are on-
ly 1.3k examples in WeiboNER training corpora,
which is not enough to train deep neural network-
s. Another reason is that the expression is informal
in social media, lowering the performance on Wei-
boNER dataset. While the greater improvement
on WeiboNER dataset proves that our method is
helpful to solve the problem.

5 Conclusions

In this paper, we propose a novel adversarial trans-
fer learning framework for Chinese NER task,
which can exploit task-shared word boundaries
features and prevent the speciﬁc information of
CWS task. Besides, we introduce self-attention
mechanism to capture the dependencies of arbi-
trary two characters and learn the inner structure
information of sentence. Experiments on two d-
ifferent widely used datasets demonstrate that our
method signiﬁcantly and consistently outperforms
previous state-of-the-art models.

Acknowledgments

The research work is supported by the Natu-
ral Science Foundation of China (No.61533018
and No.61702512), and the independent research
project of National Laboratory of Pattern Recogni-
tion. This work is also supported in part by Beijing
Unisound Information Technology Co., Ltd.

References

Daniel M Bikel, Scott Miller, Richard Schwartz,
and Ralph Weischedel. 1997. Nymble: a high-
performance learning name-ﬁnder. In Proceedings
of the ﬁfth conference on Applied natural language
processing, pages 194–201. Association for Compu-
tational Linguistics.

Konstantinos Bousmalis, George Trigeorgis, Nathan
Silberman, Dilip Krishnan, and Dumitru Erhan.
2016. Domain separation networks. In Advances in
Neural Information Processing Systems, pages 343–
351.

Razvan C Bunescu and Raymond J Mooney. 2005. A
shortest path dependency kernel for relation extrac-
In Proceedings of the conference on human
tion.
language technology and empirical methods in nat-
ural language processing, pages 724–731. Associa-
tion for Computational Linguistics.

Aitao Chen, Fuchun Peng, Roy Shan, and Gordon Sun.
2006. Chinese named entity recognition with con-
ditional probabilistic models. In Proceedings of the
Fifth SIGHAN Workshop on Chinese Language Pro-
cessing, pages 173–176.

Xilun Chen, Yu Sun, Ben Athiwaratkun, Claire Cardie,
and Kilian Weinberger. 2016. Adversarial deep av-
eraging networks for cross-lingual sentiment classi-
ﬁcation. arXiv preprint arXiv:1606.01614.

Xinchi Chen, Zhan Shi, Xipeng Qiu, and Xuanjing
Huang. 2017. Adversarial multi-criteria learning
for chinese word segmentation. arXiv preprint arX-
iv:1704.07556.

Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng,
Event extraction via dy-
and Jun Zhao. 2015.
namic multi-pooling convolutional neural network-
s. In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing, volume 1, pages 167–176.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
Journal of Machine Learning Research,
scratch.
12(Aug):2493–2537.

Emily L Denton, Soumith Chintala, Rob Fergus, et al.
2015. Deep generative image models using a lapla-
cian pyramid of adversarial networks. In Advances
in neural information processing systems, pages
1486–1494.

Jeffrey L Elman. 1990. Finding structure in time. Cog-

nitive science, 14(2):179–211.

Yaroslav Ganin and Victor Lempitsky. 2014. Unsuper-
vised domain adaptation by backpropagation. arXiv
preprint arXiv:1409.7495.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difﬁculty of training deep feedforward neu-
In Proceedings of the thirteenth in-
ral networks.
ternational conference on artiﬁcial intelligence and
statistics, pages 249–256.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
In Advances in neural information
versarial nets.
processing systems, pages 2672–2680.

Tao Gui, Qi Zhang, Haoran Huang, Minlong Peng, and
Xuanjing Huang. 2017. Part-of-speech tagging for
In Pro-
twitter with adversarial neural networks.
ceedings of the 2017 Conference on Empirical Meth-
ods in Natural Language Processing, pages 2411–
2420.

Hangfeng He and Xu Sun. 2017a. F-score driven max
margin neural network for named entity recognition
in chinese social media. EACL 2017, page 713.

Hangfeng He and Xu Sun. 2017b. A uniﬁed model
for cross-domain and semi-supervised named entity
recognition in chinese social media. In AAAI, pages
3216–3222.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-
tional lstm-crf models for sequence tagging. arXiv
preprint arXiv:1508.01991.

Hideki Isozaki and Hideto Kazawa. 2002. Efﬁcien-
t support vector classiﬁers for named entity recog-
In Proceedings of the 19th international
nition.

conference on Computational linguistics-Volume 1,
pages 1–7. Association for Computational Linguis-
tics.

Joo-Kyung Kim, Young-Bum Kim, Ruhi Sarikaya, and
Eric Fosler-Lussier. 2017. Cross-lingual transfer
learning for pos tagging without cross-lingual re-
sources. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 2832–2838.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random ﬁelds: Prob-
abilistic models for segmenting and labeling se-
quence data.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
arXiv preprint arXiv:1603.01360.

Gina-Anne Levow. 2006. The third international chi-
nese language processing bakeoff: Word segmen-
In Proceed-
tation and named entity recognition.
ings of the Fifth SIGHAN Workshop on Chinese Lan-
guage Processing, pages 108–117.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2017.
Adversarial multi-task learning for text classiﬁca-
tion. arXiv preprint arXiv:1704.05742.

Wencan Luo and Fan Yang. 2016. An empirical study
of automatic chinese word segmentation for spoken
language understanding and named entity recogni-
In Proceedings of the 2016 Conference of
tion.
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 238–248.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efﬁcient estimation of word rep-
resentations in vector space. arXiv preprint arX-
iv:1301.3781.

Nanyun Peng and Mark Dredze. 2015. Named enti-
ty recognition for chinese social media with joint-
ly trained embeddings. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 548–554.

Nanyun Peng and Mark Dredze. 2016.

Improving
named entity recognition for chinese social media
with word segmentation representation learning. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics, volume 2,
pages 149–155.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Dis-
Shirui Pan, and Chengqi Zhang. 2017.
an: Directional self-attention network for rnn/cnn-
free language understanding. arXiv preprint arX-
iv:1709.04696.

Zhixing Tan, Mingxuan Wang, Jun Xie, Yidong Chen,
and Xiaodong Shi. 2017. Deep semantic role la-
arXiv preprint arX-
beling with self-attention.
iv:1712.01586.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 6000–6010.

Chunqi Wang, Wei Chen, and Bo Xu. 2017. Named en-
tity recognition with gated convolutional neural net-
In Chinese Computational Linguistics and
works.
Natural Language Processing Based on Naturally
Annotated Big Data, pages 110–121. Springer.

YaoSheng Yang, Meishan Zhang, Wenliang Chen, Wei
Zhang, Haofen Wang, and Min Zhang. 2018. Ad-
versarial learning for chinese ner from crowd anno-
tations. arXiv preprint arXiv:1801.05147.

Xuchen Yao and Benjamin Van Durme. 2014.

Infor-
mation extraction over structured data: Question an-
swering with freebase. In Proceedings of the 52nd
Annual Meeting of the Association for Computation-
al Linguistics, volume 1, pages 956–966.

Yuan Zhang, Regina Barzilay, and Tommi Jaakko-
la. 2017. Aspect-augmented adversarial network-
arXiv preprint arX-
s for domain adaptation.
iv:1701.00188.

Junsheng Zhou, Liang He, Xinyu Dai, and Jiajun Chen.
2006. Chinese named entity recognition with a
In Proceedings of the Fifth
multi-phase model.
SIGHAN Workshop on Chinese Language Process-
ing, pages 213–216.

Junsheng Zhou, Weiguang Qu, and Fen Zhang. 2013.
Chinese named entity recognition via joint identiﬁ-
cation and categorization. Chinese journal of elec-
tronics, 22(2):225–230.

