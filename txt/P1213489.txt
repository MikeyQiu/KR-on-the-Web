Computational Baby Learning

Xiaodan Liang† (cid:63)
† National University of Singapore

Si Liu† Yunchao Wei† ∗

(cid:63) Sun Yat-sen University

Luoqi Liu†

Liang Lin(cid:63)

Shuicheng Yan †
∗ Beijing Jiaotong university

{xdliang328,fifthzombiesi,wychao1987,llq667}@gmail.com

linliang@ieee.org

eleyans@nus.edu.sg

5
1
0
2
 
y
a
M
 
4
 
 
]

V
C
.
s
c
[
 
 
3
v
1
6
8
2
.
1
1
4
1
:
v
i
X
r
a

Abstract

Intuitive observations show that a baby may inherently
possess the capability of recognizing a new visual concept
(e.g., chair, dog) by learning from only very few positive
instances taught by parent(s) or others, and this recogni-
tion capability can be gradually further improved by explor-
ing and/or interacting with the real instances in the physi-
Inspired by these observations, we propose a
cal world.
computational model for slightly-supervised object detec-
tion, based on prior knowledge modelling, exemplar learn-
ing and learning with video contexts. The prior knowledge
is modeled with a pre-trained Convolutional Neural Net-
work (CNN). When very few instances of a new concept are
given, an initial concept detector is built by exemplar learn-
ing over the deep features from the pre-trained CNN. Simu-
lating the baby’s interaction with physical world, the well-
designed tracking solution is then used to discover more di-
verse instances from the massive online unlabeled videos.
Once a positive instance is detected/identiﬁed with high
score in each video, more variable instances possibly from
different view-angles and/or different distances are tracked
and accumulated. Then the concept detector can be ﬁne-
tuned based on these new instances. This process can be re-
peated again and again till we obtain a very mature concept
detector. Extensive experiments on Pascal VOC-07/10/12
object detection datasets [8] well demonstrate the effec-
tiveness of our framework. It can beat the state-of-the-art
full-training based performances by learning from very few
samples for each object category, along with about 20,000
unlabeled videos.

1. Introduction

Empirically, we may have the following intuitive obser-
vations on how a baby learns1: after the parent(s) or others
teach the baby a few instances about a new concept, the ini-
tial recognition capability about the concept can be built.

1Note that it does not necessarily mean baby truly learns in this way

from neuron-science perspective.

During continuously exploring and/or interacting with di-
verse instances and scenes in real life, the baby can asso-
ciate the initial simple instances with other variants by us-
ing various information linkages. Based on the accumu-
lated instances about the concept, the baby can gradually
improve its recognition capability and recognize diverse in-
stances he/she never saw.

Recent successes in computer vision [27], however,
largely rely on the large number of labeled instances of vi-
sual concepts, which may require considerable human ef-
forts. The construction of an appearance-based object de-
tector is costly and difﬁcult because the number of training
examples must be large enough to capture different varia-
tions in the object appearance. Some researchers have made
efforts on improving the initial models by using very few
labeled data, along with the detection/search results from
web images [4] [7] [5] or weakly annotated videos [25] [3].
In this paper, we make the ﬁrst attempt and build a com-
putational model for slightly-supervised object detection by
drawing inspiration from the baby learning process. As il-
lustrated in Figure 1, we propose a robust learning frame-
work which can effectively model the prior knowledge,
build the initial model by exemplar learning with very few
positive instances for a new concept, and gradually learn a
mature object detector by exploring more diverse instances
in real-world unlabeled videos.

First, we model the prior knowledge (i.e.

feature rep-
resentation) with a pre-trained Convolutional Neural net-
work (CNN) in two steps. We ﬁrst train a generic CNN
by the large image classiﬁcation dataset. The learned con-
volutional layers provide the effective feature representa-
tions for object recognition. We then ﬁne-tune the CNN
with the instances of previously learned visual concepts for
the domain adaption from object classiﬁcation to the detec-
tion task. Second, when very few positive instances of a
new concept are given, the initial concept detector is built
by exemplar learning [21], which trains a separate linear
classiﬁer for every exemplar in the training set based on
the deep features from intermediate layers of the pre-trained
CNN. Other learned visual concepts are used as negative in-

1

Figure 1. Illustration of our computational baby learning framework. Inspired by the baby learning process, we integrate prior knowledge
modelling, exemplar learning, learning with video contexts for slightly-supervised object detection. The prior knowledge (i.e. feature
representation) is modelled with a pre-trained CNN. When very few positive instances of a new concept (e.g., horse) are given, an initial
concept detector can be built by exemplar learning based on the feature representation from CNN. Once a positive instance in a frame is
detected with the highest score, more variable instances (green dashed box) can be tracked by harnessing video contexts. The concept
detector can be gradually improved with these new instances and we repeat this process again and again. In addition, the pre-trained CNN
will be gradually ﬁne-tuned if enough instances are collected, which leads to more informative features for training detectors.

stances to enhance the discriminative capability. Third, we
accumulate more variable instances by exploring the mas-
sive unlabeled video clips from the online video sharing
websites (e.g., YouTube.com). The positive instance with
highest conﬁdence in each clip is selected as the seed, and
then region-based video tracking is performed to accumu-
late the variable instances by constraining the appearance
consistency and spatial correspondence. The concept de-
tector can thus be progressively improved based on these
newly tracked instances. After this process repeats again
and again, a very mature concept detector can be obtained.
With enough instances for the new concept, the pre-trained
CNN can also be further improved/ﬁne-tuned, which can
provide better deep features for learning concept detectors.
Our framework can thus effortlessly improve a new con-
cept detector based on very few positive instances and large
easily-obtained video data. The new concept detector is
gradually improved in a never ending way as long as more
videos are continuously explored.

Extensive experiments on three challenging object de-
tection datasets (Pascal VOC 07/10/12) well demonstrate
the superiority of our computational baby learning frame-
work over other state-of-the-arts [13] [24] [32] [12]. For all
three datasets, we only need to learn one detector for each
concept, while all previous works train different models for
different datasets. Our framework beats other state-of-the-
arts by learning from very few positive instances along with
about 20,000 unlabeled videos for each object category.

The contributions of this paper can be summarized as
the followings. 1) To the best of our knowledge, the pro-
posed framework makes the ﬁrst attempt to build an effec-
tive computational framework for slightly-supervised object
detection with inspiration from the baby learning process,
where the prior knowledge modelling, exemplar learning

and learning with video contexts are integrated. 2) Only two
positive instances are required for learning a new concept
detector and then the detector is reﬁned with new variable
instances from fully unlabeled videos. There is no assump-
tion that a video must contain a speciﬁc object, which makes
our framework scalable and robust for learning concept de-
tectors in an online way. 3) The knowledge of learned con-
cepts can be effectively retained in our model and conve-
niently utilized to learn new concepts.
2. Related Work

Supervised Learning. Recently, Convolutional Neural
Networks (CNNs) have been shown to perform well in a
variety of vision tasks with millions of annotated training
images and thousands of categories, including classiﬁca-
tion [27], detection [13] and segmentation [9]. Notably,
Krizhevsky et al. [17] and Szegedy et al. [27] achieved great
progress in the classiﬁcation task with large and deep super-
vised CNN training. Girshick et al. [13] proposed to ﬁne-
tune the pre-trained Krizhevsky’s network with the PAS-
CAL VOC dataset and achieved the state-of-the-art object
detection performance. However, the large performance in-
crease achieved by these methods is only possible due to
massive efforts on manually annotating millions of images
which can provide good coverage of the space of possible
appearance variations.

Semi-Supervised Learning. To minimize human ef-
forts, some attempts have been devoted to learning reli-
able models with very few labeled data. Those methods
can be summarized into two categories: learning from un-
labeled web images (image-based) or video data (video-
based). For the ﬁrst category, existing image-based ap-
proaches [4] [7] iteratively used image search and detec-
tion results to cover more variations. Also text-based [7]
and semantic relationships [4] were further used to pro-

vide more constraints on selecting instances. One problem
with these approaches is that the data variations (e.g., dif-
ferent viewpoints or background clutters) cannot be effec-
tively expanded when only with image-based visual sim-
ilarities. Some other works proposed to transfer the an-
notated image-level labels [11] or ground-truth bounding
boxes [30] from labeled images to unlabeled images for se-
mantically related classes. However, still a lot of labeled im-
ages are required to build the adequate subspaces for knowl-
edge transferring. For the second category, video-based ap-
proaches [23] [34] [31] utilized motion cues and appearance
correlations within temporal adjacent frames to augment the
model training. For example,
[23] used videos with one
class label while our method utilizes many unlabeled videos
and very few seed instances. Yang et al. [34] used the pre-
trained object detector to detect conﬁdent or hard samples.
Different from [34], our method investigates how to utilize
the video contexts to mine more informative instances. Our
slightly-supervised learning means that extremely scare an-
notated samples (e.g., one or two samples) are used, which
is a special case of semi-supervised learning.

One-shot Learning. Our learning framework is par-
tially similar to the one-shot learning [10] which learns vi-
sual object classiﬁers by using very few samples. Most of
the one-shot learning methods are based on the feature rep-
resentation transfer [2], similar geometric context [15] or
cross-modal knowledge transfer [26]. However, their per-
formance is far from that of the state-of-the-art object clas-
siﬁers. By continuously learning from video context, our
framework can achieve the state-of-the-art detection results.
Self-paced Learning. Another related learning pipeline
is self-paced learning [18] which learns ﬁrst from easy sam-
ples and then from complex ones. Various vision applica-
tions based on self-paced learning have been proposed very
recently [28] [16]. For example, Tang et al. [28] adapted
object detectors learned from images to videos by starting
with easy samples. Jiang et al. [16] addressed the data diver-
sity. Our method is a slightly-supervised self-paced learn-
ing framework, where very few samples are used as seeds
and more instances are iteratively accumulated and learned.
3. Computational Baby Learning Framework
Inspired
by intuitive observations of the baby learning process,
our method integrates prior knowledge modelling, exem-
plar learning,
learning with video contexts for slightly-
supervised object detection task. More speciﬁcally, the
prior knowledge is modeled with a pre-trained CNN. Given
very few instances for each new concept, an initial con-
cept detector can be learned with exemplar learning over
the deep features from the pre-trained CNN. More difﬁcult
instances can be obtained by exploring from real-world un-
labelled videos. After that, the detector can be ﬁne-tuned
based on these new instances. This process is repeated again

Figure 1 shows our proposed framework.

and again to obtain a mature detector. The pre-trained CNN
can thus be ﬁne-tuned to generate more informative features
based on massive mined instances, which may improve the
concept detector in turn.

3.1. Prior Knowledge Modelling

We model the prior knowledge with two steps. First, we
pre-train a general CNN on the ImageNet [6] with image-
In this work, we explore two CNN
level annotations.
the 7-layer architecture by
architectures for pre-training:
Krizhevsky et al. [17] and the Network in Network (NIN)
proposed by Lin et al. [1] [19]. We use the same parame-
ter settings for these two network architectures as in [17]
and [19]. Second, we ﬁne-tune the previous pre-trained
CNN with the previously learned concepts for the domain
adaption from object classiﬁcation network into detection
network. Since we validate our framework on the PAS-
CAL VOC challenge, we thus use the 179 object classes
on the ILSVRC2013 detection dataset as the learned con-
cepts, which excludes the corresponding 21 classes related
with the VOC 20 classes. During ﬁne-tuning, we only re-
place the 1000-way classiﬁcation layer of the pre-trained
CNN with a randomly initialized (N+1)-way classiﬁcation
layer, where N is the number of learned concepts, plus one
for background. In our setting, N = 179. We use the val-
idation set (20,121 images) in the ILSVRC2013 detection
dataset and only the images that contain at least one object
of the 179 classes are used. All region proposals with ≥ 0.5
intersection-over-union (IoU) overlap with a ground-truth
box are regarded as positives for 179 learned concepts and
the rest as negatives. Though our framework can use any
category-independent region proposal method, we choose
the selective search [29] to enable a controlled comparison
with the previous work [13]. The CNN ﬁne-tuning starts
SGD with a learning rate of 0.001 for both two networks.
For the 7-layer architecture [17], we uniformly sample 32
positive windows (over all classes) and 96 background win-
dows to construct a mini-batch of size 128. The ﬁne-tuning
is run for 70k SGD iterations and takes 9 hours on a single
NVIDIA GeForce GTX TITAN GPU. For NIN [19], a mini-
batch of size 80, consisting of 20 positive windows and 60
background windows, is used. The ﬁne-tuning is run for
150k SGD iterations and takes 16 hours. Alternatively, our
framework can also bootstrap any concept detector without
any previously learned concepts, which can be simply im-
plemented by eliminating the ﬁne-tuning step with learned
concepts. Our experiments also report the performances
when with/without ﬁnetuning 179 learned concepts.
3.2. Exemplar Learning

The learned convolution layers in the above pre-trained
CNN form the basic feature representation, and then an ini-
tial concept detector can be learned based on these deep fea-
tures and very few positive instances of a new concept.

Feature extraction. For all positive and negative in-

Figure 2. Some exemplar negative samples. Top row shows the
collected general background images and bottom row shows the
exemplar instances of previously learned concepts.
stances, we enlarge the tight bounding boxes to contain 16
pixels of image contexts and then wrap it into a ﬁxed 227 ×
227 size as used in [13]. Deep features are computed as the
outputs from the penultimate fully-connected layer (4096-
dimension) by forward propagating a mean-subtracted 227
× 227 image through the pre-trained CNN.

Selection of seed instances. Our selection strategy of
the seeds (including the number of seeds) is optional and
our framework can be bootstrapped with any number of
seeds of the new concept. For most of our results, we select
two common positive instances for each concept from the
PASCAL VOC 2007 training set. Speciﬁcally, we cluster
all positive instances into 10 clusters by k-means. For the
top-2 largest clusters, the nearest two samples to the two
centroids are selected as the seeds for each concept. Differ-
ent seed initialization may lead to different model updating
and different mined instances in each iteration. However,
due to the large number of unlabeled videos, the ﬁnal de-
tection results can be robust to different seed instances of
the same number. We report extensive experiments on the
different seed selections (i.e. different seed number and ran-
dom many times for obtaining different seeds) in Table 7.

Negative set collection. To fairly justify our method,
we do not use any annotations of PASCAL VOC Challenge
to obtain the negative instances. The negative set used in
our framework contains a batch of general background im-
ages (i.e., no speciﬁc object is included) and learned con-
cept instances. As illustrated in the top row of Figure 2,
we collect 4, 000 general scene images from Flickr and use
the categories in the SUN scene dataset [33] as the search
keywords. All region proposals in these background images
are used as negative samples. For the learned concepts, the
region proposals with ≥ 0.5 IoU overlap with the bounding
box of 179 object class instances in the ILSVRC 2013 de-
tection validation set are also treated as negative samples, as
shown in the bottom row of Figure 2. Our initial experiment
indicates that using general background images, versus our
negative set, can result in about 4% drop in mAP. The pos-
sible reason may be that more hard negative samples are in-
cluded in other object-level concept instances. After more
instances of new concepts are collected, our negative set
will be gradually enlarged.

Exemplar SVM training. Inspired by [21], we train a
separate linear SVM classiﬁer for each positive instance,
and each SVM classiﬁer is highly tuned according to the
exemplar’s appearance. The exemplar’s decision boundary

Figure 3. Region-based video tracking. Given the seed instance,
we track other reliable instances from other regions. The afﬁnity
graph is built by combining both the pair-wise similarity and the
detectiveness of each region. Then dense subgraphs are detected
within the afﬁnity graph by graph shift. The subgraph containing
the seed instance (red point) is selected. Two instances with top-2
largest similarities with seed instances are placed into the training
image pool for ﬁne-tuning the detector in next iteration.

is thus decided, in large part, by the negative samples. For
each test image, we thus independently run each exemplar
detector and use the non-maximum suppression to create a
ﬁnal set of detections.
3.3. Learning with Video Contexts

We iteratively improve the concept detectors by min-
ing more variable instances from unlabeled videos. About
20,000 videos for each new concept are crawled from
YouTube. Due to the computational limitation, we use the
keywords from the VOC dataset collection to prune the
videos unrelated to the new concept. The collected video
set includes approximate 30% “noisy” videos that contain
none of the instances of the concept. No manual annotation
is performed. In each iteration, we select one seed instance,
and region-based tracking is then performed to accumulate
the variable instances. The detector and knowledge updat-
ing are then performed.

Seed instance selection.

In each video clip, there is
much redundant information with few appearance differ-
ences in temporal adjacent frames. To guarantee appear-
ance variance of tracked instances and limit computational
complexity, only key frames of each video are analyzed. We
select the image with (cid:96)2 norm of the global GIST [22] fea-
ture difference larger than 0.01 as a key frame, compared
with its temporal adjacent frames. For all key frames, we
perform object detection with the initial detector. We only
select the video containing the region with detection score
larger than 1, and the region with the highest score in this
video is selected as the seed positive instance.

Region-based video tracking. The region-based video
tracking is performed on the selected videos and initial-
ized with their seed instances as illustrated in Figure 3. In
our framework, we treat the tracking task as a region-based
cluster mining problem for both moving and static concepts.
Speciﬁcally, we extract a batch of region proposals in all key

i , cy

frames using selective search [29] and represent each region
ri with both the deep feature xi and the spatial coordinates
pi = (cx
i , wi, hi) corresponding to the position, width
and height. Since we wish to select the instances from dif-
ferent frames, which may capture more diverse visual pat-
terns, the similarity of two regions from the same frame is
thus set as zero. The similarity Ai,j for each pair (ri, rj)
from different frames is thus deﬁned by fusing the appear-
ance similarity and the localization similarity,

Ai,j = exp{

} + α(exp{

}),

(1)

||xi − xj||2
δ2
1

||pi − pj||2
δ2
2

where δ1 and δ2 are the empirical variances of x and p,
respectively, and we set α = 0.3 because the appearance
similarity is more important considering the camera mov-
ing and static objects. In addition, to make our framework
robust to outliers (i.e., noisy regions), we also constrain the
detection score to enlarge the possibility of the region to
contain the concept. We thus deﬁne the detectiveness Oi of
each region ri by thresholding the detection score, where Oi
of the region with detection score larger than −3 is set as 1,
otherwise as 0. Note that we do not directly use the detector
scores because these variable instances may not be detected
by the current detector but can bring more data diversity for
further improving the detectors.

To seamlessly integrate the detectiveness of the region
and the pairwise similarity, we use the graph shift algo-
rithm [20] to obtain the variable instances, which is efﬁ-
cient and robust for graph mode seeking. This algorithm is
particularly suitable for our task as it directly operates on
the afﬁnity graph and leaves the outlier points ungrouped.
Formally, we deﬁne an individual graph G = (R, A) for
each video. R = {r1, . . . , rn} represents all the regions
and A is a symmetric matrix with non-negative elements.
The diagonal elements of A represent the detectiveness of
the regions while the non-diagonal elements measure the
pair-wise similarities between regions. The modes of a
graph G are deﬁned as local maximizers of the graph den-
sity function g(y) = yT Ay, y ∈ ∆n, where ∆n = {y ∈
Rn : y ≥ 0 and ||y||1 = 1}. The strongly connected sub-
graphs correspond to large local maxima of g(y) over sim-
plex which is an approximate measure of the average afﬁn-
ity score of these subgraphs. And these subgraphs can be
found by solving the quadratic optimization problem, i.e.,
max g(y) = yT Ay, y ∈ ∆n, as in [20]. The graph shift
algorithm starts from an individual sample and evolves to-
wards the mode of G. The instances reaching the same
mode are grouped as a cluster. We can thus select the tar-
get subgraph that contains the seed instance. To prevent
the rapid semantic drift, we only select two instances in this
subgraph, which appear in different frames and have highest
similarities with the seed instance. We can thus accumulate
much more instances to improve detectors iteratively.

Detector Updating. After accumulating more instances

from unlabeled videos, a large set of positive instances of
the new concept is collected, which can help improve the
concept detector in the next iteration. The frames selected
in the previous iterations will not be considered in later it-
erations, which makes the model equipped with different
instances in every iteration.
In order to update the con-
cept detector, these newly mined instances are added into
the positive set. The regions from general background im-
ages and learned concepts are treated as negatives. We re-
train one linear SVM classiﬁer for each new concept and
the hard negative mining method is also used. After this
process repeats again and again, we can achieve a very ma-
ture concept detector with a considerable number of mined
instances. For fair comparison, we use the same detection
strategies as the previous work [13] in testing phase.

Knowledge Updating. Once enough instances of each
new concept (about 10,000 instances) are obtained, the pre-
trained CNN can be further improved to generate more
informative features by ﬁne-tuning it with these new in-
stances. During ﬁne-tuning, we replace the (N+1)-way out-
put layer of the pre-trained CNN in Section 3.1 with a ran-
domly initialized (M+1)-way classiﬁcation layer (including
M new concepts and one for background). We set M = 20
in our experiments. Since these mined instances may con-
tain some noisy data (e.g., inaccurate bounding box of the
object), we only use the original set of mined instances dur-
ing ﬁne-tuning and no additional data augmentation (e.g.,
≥ 0.5 IoU overlap) is performed. The negatives for training
concept detectors are used as background. The ﬁne-tuning
is run for 50K SGD iterations for the 7-lay architecture [17]
and 100K iterations for NIN [19], respectively.

Finally, a bounding box regression model is learned to ﬁx
many localization errors in the testing as in [13]. From the
mined instances, we select 2, 000 detected instances with
highest detection scores in the later iterations as ground-
truth boxes for training the regression model. The concept
detector in the later iterations will be very mature and these
top detection boxes have high possibilities to locate the pre-
cise object locations. We only consider a region proposal if
it has an IoU with ground-truth box greater than 0.8.

4. Experiments

We evaluate our framework on the PASCAL Visual Ob-
ject Classes (VOC) datasets [8], which are widely used as
the benchmark for object detection. PASCAL VOC 2007,
VOC 2010 and VOC 2012 are all tested. For each object
class, we train the object detector by using two seeds and
about 20, 000 unlabeled videos. Note that our method is
independent of any speciﬁc test set and only one object
detector is used for testing the three datasets. For VOC
2010 and VOC 2012, we evaluate test results on the on-
line evaluation server. We compare our method with the
state-of-the-art baselines, including DPM HSC [24], Re-

Table 1. Detection average precision (%) on PASCAL VOC2007 test. Rows 1-4 show the baselines. Rows 5-6 are the results of R-CNN
based on the NIN [19]. Rows 7-8 show R-CNN results ﬁne-tuned with 179 extra classes. Rows 9-13 show our results in different iterations,
with/without ﬁne-tuning and bounding box regression. “B initial” and “B I15” represent the results in the beginning with two seeds and
after the 15th iteration, respectively. “B FT” and “B FT I2” are the results after ﬁne-tuning with the mined instances and running 2 more
iterations, respectively. “B FT I2 no179 BB” represents the results after directly using the classiﬁcation network, and no ﬁne-tuning with
179 detection classes is performed. Rows 15-18 show the results based on NIN [19].

VOC 2007 test
DPM HSC[24]
R-CNN[13]
R-CNN BB[13]
Ngrams [7]
R-CNN NIN
R-CNN NIN BB
R-CNN 179
R-CNN 179 BB
B Initial
B I15
B FT
B FT I2
B FT I2 BB

aero
32.2
64.2
68.1
14.0
72.2
72.1
62.5
69.8
26.3
61.1
65.2
68.9
72.2
B FT I2 no179 BB 72.0
69.0
71.1
71.0
B NIN FT I2 BB 75.9

B NIN I15
B NIN FT
B NIN FT I2

bike
58.3
69.7
72.8
36.2
74.5
78.2
70.2
73.2
11.9
65.7
71.6
70.5
72.8
70.2
69.4
71.5
73.6
76.8

bird
11.5
50.0
56.8
12.5
58.6
64.3
54.4
60.2
3.2
51.1
53.8
55.6
61.8
56.5
52.3
59.0
61.3
66.9

boat
16.3
41.9
43.0
10.3
47.4
49.8
42.7
43.8
12.9
38.8
39.5
42.7
46.7
40.7
42.4
43.7
46.3
49.0

bottle
30.6
32.0
36.8
9.2
38.7
42.2
35.4
38.7
9.3
29.8
32.2
37.0
42.0
37.2
36.3
37.1
40.6
47.9

bus
49.9
62.6
66.3
35.0
68.1
71.6
63.1
66.2
16.0
57.4
64.1
64.1
66.1
61.7
65.6
68.1
70.3
72.1

car
54.8
71.0
74.2
35.9
75.4
77.1
71.9
75.2
2.5
63.8
70.4
71.1
74.2
60.9
68.9
73.1
73.8
77.2

cat
23.5
60.7
67.6
8.4
72.1
77.8
61.5
65.3
6.4
57.8
63.0
66.1
74.6
75.8
67.5
72.8
74.0
77.9

chair
21.5
32.7
34.4
10.0
38.3
41.7
34.0
36.1
0.9
26.8
33.9
34.5
37.3
33.7
33.2
39.8
43.7
48.6

cow table
34.0
27.7
46.5
58.5
54.5
63.5
6.5
17.5
57.2
69.9
61.3
72.7
47.1
61.0
56.1
66.8
4.1
14.3
44.3
57.1
50.2
60.9
51.8
63.1
56.8
68.3
44.4
66.6
50.3
70.7
55.3
72.1
55.6
72.9
65.0
78.5

dog
13.7
56.1
61.2
12.9
69.5
73.6
60.7
65.0
9.7
57.2
58.5
60.9
65.7
76.5
68.1
68.3
68.5
73.9

horse mbike person
39.9
51.6
58.1
54.2
66.8
60.6
58.7
68.6
69.1
6.0
27.5
30.6
59.4
72.4
67.5
64.2
73.6
77.3
56.8
67.9
64.1
60.6
70.8
70.7
13.7
21.6
13.2
45.5
61.3
55.7
54.0
65.9
64.8
52.8
67.1
63.0
58.0
68.4
71.3
76.9
58.5
69.7
40.1
68.9
68.8
54.8
70.7
67.6
57.6
70.7
69.2
77.3
62.7
73.6

plant
12.4
31.5
33.4
1.5
34.8
37.2
32.6
33.7
6.0
27.2
27.4
31.6
35.1
29.3
26.3
35.4
37.7
40.4

sheep
23.5
52.8
62.9
18.8
61.5
64.9
58.2
64.2
15.9
57.1
60.6
62.1
66.3
66.9
67.3
68.4
69.3
73.5

sofa
34.4
48.9
51.1
10.3
60.3
64.5
45.7
49.1
3.5
38.0
45.8
45.8
47.2
49.7
57.1
58.2
59.6
64.4

train
47.4
57.9
62.5
23.5
67.4
70.2
59.2
64.2
11.1
50.4
59.3
57.6
64.0
61.1
61.5
64.9
65.3
69.2

tv mAP
34.3
45.2
54.2
64.7
58.5
64.8
17.2
16.4
61.8
69.9
72.8
65.4
55.7
64.5
59.7
65.2
11.7
31.4
50.3
58.0
55.1
60.7
56.5
64.2
60.7
65.7
58.3
57.6
57.6
67.6
60.9
66.2
62.5
68.7
67.1
70.6

Table 2. Detection average precision (%) on PASCAL VOC2010 test.

VOC 2010 test
Regionlets[32]
SegDPM[12]
R-CNN BB[13]
R-CNN 179 BB
B FT I2 BB

aero
bike
65.0 48.9
61.4 53.4
71.8 65.8
73.1 66.2
75.7 68.0
B NIN FT I2 BB 77.7 73.8

bird
25.9
25.6
53.0
55.2
59.2
62.3

boat
24.6
25.2
36.8
38.7
42.6
48.7

bottle
24.5
35.5
35.9
36.5
40.0
45.4

bus
56.1
51.7
59.7
59.2
62.4
67.3

car
54.5
50.6
60.0
60.2
62.0
67.0

cat
51.2
50.8
69.9
71.8
72.3
80.3

chair
17.0
19.3
27.9
27.1
29.5
41.3

cow table
30.2
28.9
26.8
33.8
41.4
50.6
42.0
51.6
40.8
58.2
49.7
70.8

dog
35.8
40.4
70.0
69.5
72.0
79.5

horse mbike
55.7
40.2
54.4
48.3
69.0
62.0
71.0
63.9
72.8
66.3
78.6
74.7

person
43.5
47.1
58.1
59.2
59.9
64.5

plant
14.3
14.8
29.5
29.7
30.9
36.0

sheep
43.9
38.7
59.4
60.0
62.6
69.9

sofa
32.6
35.0
39.3
38.5
39.9
55.7

train
54.0
52.8
61.2
61.8
59.0
70.4

tv mAP
39.7
45.9
40.4
43.1
53.7
52.4
54.3
51.2
56.5
55.7
63.8
61.7

gionlets [32], SegDPM [12] and R-CNN [13]. They used
all data in the VOC train and val set for training detectors.
We also compare our method with the recent weakly super-
vised method [7], which discovers instances from web im-
ages. We use 179 extra classes in the ILSVRC 2013 detec-
tion task to ﬁne-tune the pre-trained CNN by 1000 classes
in the ILSVRC 2012 classiﬁcation. We implement two ver-
sions of R-CNN (i.e., “R-CNN 179” and “R-CNN 179 BB”
with bounding-box regression), which ﬁrstly ﬁne-tune the
classiﬁcation CNN with 179 extra classes and then ﬁne-
tune the CNN with VOC 20 classes following the settings
in [13]. We also report the performances of two version of
R-CNN (i.e., “R-CNN NIN” and “R-CNN NIN BB”) using
the Network-in-Network (NIN) [19].
4.1. Comparison with the state-of-the-arts

Table 1, 2 and 3 shows the results on the VOC 2007,
2010 and 2012, respectively. “R-CNN NIN BB” can sig-
niﬁcantly increase the mAP on VOC 2007 achieved by
[13] from 58.5% to 65.4% and mAP on VOC 2012 of
[13] from 53.3% to 62.4%, respectively.
Its superiority
largely beneﬁts from the better neural network architec-
ture. The “R-CNN 179 BB” only performs slightly bet-
ter than “R-CNN BB” (e.g., smaller than 0.6% increase
on VOC 2010 and VOC 2012). The main reason is that
the samples from 179 extra classes provide limited addi-
tional information when enough instances of 20 classes are
already used in [13]. However, when only two instances
of a new concept are given, our method can beneﬁt from

these instances for domain-speciﬁc ﬁne-tuning. All our
variants strongly outperform the methods [24] [32] [12]
based on hand-crafted features learning and deformable part
models. Based on the 7-layer network [17], our method
(“B FT I2 BB”) achieves 60.7% in mAP, which is signif-
icantly superior to 58.5% of “R-CNN” [13] and 34.3% of
“DPM HSC” [13]. Compared to R-CNN, our method in-
creases the performance by 2.8% and 2.7% on VOC2010
and VOC2012, respectively. After ﬁne-tuning the classi-
ﬁcation network with 179 extra classes, the performance
of our method (“B FT I2 BB”) can increase by 2.4% over
“B FT I2 no179 BB”. It well veriﬁes the importance of do-
main adaptation from classiﬁcation into detection. When
ﬁne-tuning the CNN based on the Network in Network
(NIN) [19], our method (“B NIN FT I2 BB”) can achieve
67.1% on VOC2007, 63.8% on VOC 2010, and 63.2% on
VOC2012, which outperforms the “R-CNN BB [13]” by
a large margin of more than 8% on all three test sets and
signiﬁcantly outperforms the “R-CNN NIN BB” by 1.7%
on VOC2007. The bounding box regression can further ﬁx
a large number of mislocalized detections. Note that our
method only uses two positive instances and trains one sin-
gle detector for all three datasets, while the baselines use
different large training sets and carefully tune the model pa-
rameters for different test sets. This superiority well veri-
ﬁes the effectiveness and generality of our framework that
automatically learns a signiﬁcantly better detector than the
fully supervised methods. The recent weakly supervised

Table 3. Detection average precision (%) on PASCAL VOC2012 test.

VOC 2012 test
SDS[14]
R-CNN BB[13]
R-CNN NIN BB
R-CNN 179 BB
B FT I2 BB

bike
aero
69.7 58.4
71.8 65.8
77.9 73.1
72.7 66.0
75.8 68.2
B NIN FT I2 BB 78.0 74.2

bird
48.5
52.0
62.6
55.0
58.2
61.3

boat
28.3
34.1
39.5
34.0
39.6
45.7

bottle
28.8
32.6
43.3
32.0
37.0
42.7

bus
61.3
59.6
69.1
59.5
63.2
68.2

car
57.5
60.0
66.4
60.5
62.2
66.8

cat
70.8
69.8
78.9
70.9
72.3
80.2

chair
24.1
27.6
39.1
29.2
29.3
40.6

cow table
35.9
50.7
41.7
52.0
50.0
68.1
40.2
51.5
40.8
59.0
70.0
49.8

dog
64.9
69.6
77.2
70.0
71.4
79.0

horse mbike
65.8
59.1
68.3
61.3
76.1
71.3
68.4
62.3
71.3
66.2
77.9
74.5

person
57.1
57.8
64.7
58.9
59.4
64.0

plant
26.0
29.6
38.4
30.8
30.9
35.3

sheep
58.8
57.8
66.9
58.2
61.2
67.9

sofa
38.6
40.9
56.2
37.7
41.1
55.7

train
58.9
59.3
66.9
58.3
57.3
68.7

tv mAP
50.7
50.7
53.3
54.1
62.7
62.4
53.5
53.9
56.0
56.5
63.2
62.6

Table 4. Detection average precision (%) on PASCAL VOC2007 test by the version initialized with more training data.

VOC 2007 test
R-CNN BB[13]
R-CNN NIN BB
B VOC I2
B VOC I2 BB
B NIN VOC I2

aero bike
68.1 72.8 56.8 43.0
72.1 78.2 64.3 49.8
69.2 70.3 58.5 42.7
72.3 72.7 64.0 46.7
73.6 74.8 65.5 48.4
B NIN VOC I2 BB 76.2 77.8 69.9 51.0

bird boat bottle
36.8
42.2
38.3
43.5
46.4
52.0

bus
cat
car
66.3 74.2 67.6
71.6 77.1 77.8
64.6 71.7 67.3
67.5 74.8 74.5
71.2 74.6 76.0
73.6 77.5 78.0

chair
34.4
41.7
37.2
39.4
46.1
49.4

cow table
54.5
63.5
61.3
72.7
52.1
64.7
57.7
70.1
56.8
75.3
65.2
82.1

dog
61.2
73.6
62.6
68.3
72.6
76.8

horse mbike person plant
33.4
69.1
37.2
77.3
33.3
64.6
36.9
71.5
40.5
73.6
41.6
79.1

68.6
73.6
69.1
70.2
73.8
74.9

58.7
64.2
54.1
58.5
60.1
64.5

sheep sofa
62.9
64.9
62.7
68.1
71.6
74.7

train
51.1 62.5 64.8
64.5 70.2 72.8
46.6 58.8 66.8
49.1 65.5 67.9
68.3 67.4 73.1
70.8 69.8 73.6

tv mAP
58.5
65.4
57.8
62.0
65.5
68.9

Figure 4. Performances in different iterations of our framework on
VOC 2007. We run 15 iterations for mining more instances. Then
we ﬁne-tune the CNN with mined instances (“FT”) and 2 iterations
are further performed to improve the detectors (“FT I2”).

method [7] only obtained 17.2% in mAP on VOC 2007,
which is much worse than the proposed method.
4.2. Computational Baby Learning Results

Figure 4 shows our performances in different iterations
as well as before/after ﬁne-tuning with more variable in-
stances. We show the results based on the 7-layer network
on VOC 2007 and the corresponding AP for each class is
presented in Table 1.
In the beginning, we only obtain
11.7% in mAP with only two seeds. After the ﬁrst round
of learning with video context, we can substantially im-
prove the mAP to 36.1%, which is even higher than mAP
of DPM HSC [24]. Most of the easy test samples can be
detected by our updated model. After 15 iterations are
performed, we can achieve 50.3% in mAP (“B I15”) and
collect about 10,000 instances for each class. We then
further ﬁne-tune the pretrained CNN with these mined in-
stances and 4.8% improvement is achieved (“B FT”). Us-
ing NIN as the CNN architecture, we achieve 57.6% after
15 iterations (“B NIN I15”) and obtain 60.9% after ﬁne-
tuning (“B NIN FT”). It proves that more informative fea-
tures can be learned by ﬁne-tuning. And the superiority
of “B NIN FT” over “B FT” well demonstrates that more
informative CNNs can lead to better initialization of our
framework and learning capabilities during the repetition
of the process. We then further improve the detectors by
running 2 more iterations based the ﬁne-tuned CNNs and
better detection performances can be achieved, shown by
“B FT I2”, “B NIN FT I2”. For both static (e.g., chair)
and moving objects (e.g., bicycle), our model can be grad-
ually improved, which speaks well for the effectiveness of

Table 7. Detection average precision (%) on aeroplane class of
PASCAL VOC2007 by different seed selections. We run our ver-
sion “B FT I2” with the same setting by using different numbers
of randomized seed instances. We report the results based on dif-
ferent numbers of seeds, i.e., one, two and ﬁve, as well as different
seed instances randomly ten times for each number of seed.

seed number
1
2
5

1

2

3

4

5

6

7

8

9

65.1 66.1 68.1 67.3 67.2 68.2 65.4 66.1 67.8 66.7
67.0 66.8 70.4 69.7 68.3 69.2 66.9 67.9 69.2 69.8
69.7 69.3 71.2 72.3 70.1 70.8 71.5 70.9 70.2 71.3

10 mean
66.8
68.5
70.7

our region-based video tracking. Limited by the computa-
tional cost, our experiments only report the current results
at two iterations after ﬁne-tuning CNN. With better CNN
architectures (e.g., googleLeNet [27]), it is predictable that
our detectors can be further improved.
4.3. Discussion on Different Seed Selections

We extensively evaluate how our framework performs
with different seed selections. Due to the computational
limitation, we only test on one speciﬁc object class, i.e.,
aeroplane, as reported in Table 7. We test three num-
bers of seeds during the initialization. For each number,
we generate different seeds randomly ten times to evalu-
ate the robustness on seed selections. It can be seen that
our method can archive better performance with more ini-
tial seeds. With different randomized seeds of each number,
we obtain slightly different results and their mean 68.5% is
only slightly worse than 68.9% by our version with two se-
lected instances in Table 1. The CNN ﬁne-tuning only with
the aeroplane class may lead to this slightly decrease. The
main reason for the robustness may be the usage of the large
number of unlabelled videos. By mining various instances
with different views or appearance changes, we can easily
introduce greater data diversity into the model training.
4.4. Initialization with More Training Data

We evaluate the state-of-the-arts (e.g., R-CNN) trained
training sets can be further improved by us-
with all
ing our framework.
The results of “B VOC I2” and
“B VOC I2 BB” are shown in Table 4, in which all the
images in VOC 2007 are used. The detectors are trained
over the deep features from the 7-layer CNN and two
more iterations are performed to mine more instances from
videos. We obtain 62.0% mAP on VOC 2007, 3.5% higher

Table 5. Detection average precision (%) on PASCAL VOC2012 test by the version initialized with more training data.

VOC 2012 test
SDS[14]
R-CNN BB[13]
R-CNN NIN BB
B NIN VOC I2

aero bike
69.7 58.4 48.5 28.3
71.8 65.8 52.0 34.1
77.9 73.1 62.6 39.5
77.6 71.7 60.9 41.3
B NIN VOC I2 BB 80.2 75.0 64.9 45.8

bird boat bottle
28.8
32.6
43.3
38.2
44.0

cat
car
bus
61.3 57.5 70.8
59.6 60.0 69.8
69.1 66.4 78.9
65.5 64.5 80.0
70.1 67.6 81.4

chair
24.1
27.6
39.1
38.1
40.8

cow table
35.9
50.7
41.7
52.0
50.0
68.1
47.1
69.9
51.9
71.4

dog
64.9
69.6
77.2
79.3
81.0

horse mbike person plant
26.0
59.1
29.6
61.3
38.4
71.3
33.5
74.7
75.6
37.6

57.1
57.8
64.7
61.9
66.1

65.8
68.3
76.1
76.1
78.2

sheep sofa
58.8
57.8
66.9
67.7
68.5

train
38.6 58.9 50.7
40.9 59.3 54.1
56.2 66.9 62.7
54.8 62.6 63.2
59.4 68.0 65.2

tv mAP
50.7
53.3
62.4
61.4
64.6

Table 6. Detection average precision (%) on PASCAL VOC2007 test by excluding the VOC classes during pretraining the CNN.

VOC 2007 test
B NIN noVOC Initial
B NIN noVOC I15
B NIN noVOC FT
B NIN noVOC FT I2

aero bike bird boat bottle bus
4.0
20.6
11.7 24.1
14.2
64.1 70.2 62.8 34.3 61.0 50.3 59.4
65.0 71.6 53.9 39.7
66.4 74.2 69.4 36.3 66.7 57.9 65.3
70.4 73.0 59.1 42.8
67.3 74.0 72.7 39.5 71.0 56.3 66.2
69.3 72.9 58.0 44.6
71.3 77.0 75.8 44.1 74.7 64.4 71.9
B NIN noVOC FT I2 BB 73.4 76.3 63.1 50.2

chair cow table dog horse mbike person plant
10.9
28.9
32.9
34.8
37.5

cat
car
12.4 20.4 11.4

2.9
66.4
66.7
73.0
74.5

12.1
54.2
56.7
58.4
64.0

3.6
31.9
36.6
40.5
44.9

13.5
65.1
70.4
67.4
74.4

3.4

0.5

0.4

tv mAP
sheep sofa train
11.0
2.1
23.4
2.9
25.9
55.3
45.5 59.4 61.1
61.4
59.5
53.6 63.3 64.4
65.0
61.2
60.2 65.0 67.4
64.5
65.3
63.6 69.5 68.7
67.4

Figure 5. Visualization of our tracking results for bird and bicycle.
For each class, the left column shows the initialized two seeds. The
top row shows the detected seeds in each video and two tracked
instances are presented within the dashed box.

than the original “R-CNN BB” (58.5% in mAP). Based on
the Network-in-Network, we also achieve superior perfor-
mances over “R-CNN NIN BB” (68.9% vs 65.4% in mAP
on VOC 2007 and 64.6% vs 62.4% in mAP on VOC 2012
in Table 4 and Table 5, respectively).

4.5. Pretraining Prior Knowledge CNN After Ex-

cluding the VOC Classes

Recall that in Section 3.1, we pre-train the CNN on the
ImageNet dataset with 1000 classes. The 1000 classes on
ImageNet include similar classes or sub-classes with VOC
20 categories (e.g., space shuttle vs aeroplane and Chi-
huahua vs dog). To make more strict comparisons and
analyses, we conduct experiments by excluding the similar
classes with the VOC 20 classes when training the general
CNN. We manually exclude the 214 similar classes with
VOC 20 classes of 1000 classes in ImageNet, such as stu-
dio couch and day bed vs sofa, laptop computer vs tvmon-
itor in ImageNet and VOC classes, respectively. Then only
789 classes in ImageNet are used for pretraining. We train
this CNN based on the Network in Network (NIN) and use
the same parameter setting as in [19]. Based on this re-
trained CNN, we report the corresponding performance on
VOC 2007 in Table 6. Our method can still substantially
improve its initial performance with only two seeds. The

Figure 6. Some exemplar detection results. All detections with
precision greater than 0.5 are shown. Each detection is labeled
with the predicted class and the detection score from the detector.
View digitally with recommended zoom.

mAP achieved by “B NIN noVOC FT I2 BB” is compara-
ble with the fully-trained “R-CNN NIN BB” version, i.e.,
65.3% vs 65.4% on VOC 2007, respectively.
4.6. Visualizations

We show the two selected instances and some mined in-
stances for four classes in Figure 5. We randomly select
some mined instances from all iterations. It can be observed
that our method successfully tracks variable instances with
different view-angles, occlusions or appearance variance.
Many qualitative detection results on the VOC 2007 test set
are presented in Figure 6, which are obtained from our best
model “B NIN FT I2 BB”, each image is selected due to it
is impressive and accurate.
5. Conclusion and Future Work

In this paper, inspired by the intuitive observation of the
baby learning process, we presented a novel computational
slightly-supervised learning framework for object detection
by combining prior knowledge modeling, exemplar learn-
ing, and learning with video contexts. Signiﬁcant improve-
ments over fully-training based methods were achieved by
our framework on PASCAL VOC 07/10/12 with only two
positive instances along with about 20,000 unlabeled real-
world videos. In the future, we will explore how to ade-
quately utilize more contextual information (e.g. scene, hu-
man actions, other objects) to mine more accurate and di-
verse instances. Our framework can also be easily extended
to improve various vision tasks, such as face age recogni-
tion, people identiﬁcation and scene classiﬁcation.

References

[1] http://image-net.org/challenges/lsvrc/2014/results. 2014. 3
[2] E. Bart and S. Ullman. Cross-generalization: Learning novel
classes from a single example by feature replacement.
In
Computer Vision and Pattern Recognition, volume 1, pages
672–679, 2005. 3

[3] C.-Y. Chen and K. Grauman. Watching unlabeled video
helps learn new human actions from very few labeled snap-
shots. In Computer Vision and Pattern Recognition, pages
572–579, 2013. 1

[4] X. Chen, A. Shrivastava, and A. Gupta. Neil: Extracting vi-
sual knowledge from web data. In International Conference
on Computer Vision, pages 1409–1416, 2013. 1, 2

[5] X. Chen, A. Shrivastava, and A. Gupta. Enriching vi-
sual knowledge bases via object discovery and segmentation.
Computer Vision and Pattern Recognition, 2014. 1

[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
Computer Vision and Pattern Recognition, 2009. 3

[7] S. K. Divvala, A. Farhadi, and C. Guestrin. Learning ev-
erything about anything: Webly-supervised visual concept
learning. In Computer Vision and Pattern Recognition, 2014.
1, 2, 6

[8] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and
A. Zisserman. The pascal visual object classes (voc) chal-
lenge. International Journal of Computer Vision, 88(2):303–
338, 2010. 1, 5

[9] C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Learning
hierarchical features for scene labeling. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 35(8):1915–
1929, 2013. 2

[10] L. Fei-Fei, R. Fergus, and P. Perona. One-shot learning of ob-
ject categories. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 28(4):594–611, 2006. 3

[11] R. Fergus, Y. Weiss, and A. Torralba. Semi-supervised learn-
ing in gigantic image collections. In Advances in neural in-
formation processing systems, pages 522–530, 2009. 3
[12] S. Fidler, R. Mottaghi, A. Yuille, and R. Urtasun. Bottom-
up segmentation for top-down detection. In Computer Vision
and Pattern Recognition, pages 3294–3301, 2013. 2, 6
[13] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
segmentation. arXiv preprint arXiv:1311.2524, 2013. 2, 3,
4, 5, 6, 7, 8

[14] B. Hariharan, P. Arbel´aez, R. Girshick, and J. Malik. Simul-
In European Confer-

taneous detection and segmentation.
ence on Computer Vision, pages 297–312. 2014. 7, 8
[15] D. Hoiem, A. A. Efros, and M. Hebert. Geometric context
from a single image. In International Conference on Com-
puter Vision, volume 1, pages 654–661, 2005. 3

[16] L. Jiang, D. Meng, S. Yu, Z. Lan, S. Shan, and A. G. Haupt-
In Advances in

mann. Self-paced learning with diversity.
Neural Information Processing Systems, 2014. 3
[17] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
In
classiﬁcation with deep convolutional neural networks.
Advances in neural information processing systems, pages
1097–1105, 2012. 2, 3, 5, 6

[18] M. P. Kumar, B. Packer, and D. Koller. Self-paced learning
for latent variable models. In Advances in Neural Informa-
tion Processing Systems, pages 1189–1197, 2010. 3

[19] M. Lin, Q. Cheng, and S. Yan. Network in network. Inter-
national Conference on Learning Representations, 2014. 3,
5, 6, 8

[20] H. Liu and S. Yan. Robust graph mode seeking by graph
shift. International Conference on Machine Learning, pages
671–678, 2010. 5

[21] T. Malisiewicz, A. Gupta, and A. A. Efros. Ensemble of
exemplar-svms for object detection and beyond. In Interna-
tional Conference on Computer Vision, 2011. 1, 4

[22] A. Oliva and A. Torralba. Modeling the shape of the scene: A
holistic representation of the spatial envelope. International
Journal of Computer Vision, 42(3):145–175, 2001. 4
[23] A. Prest, C. Leistner, J. Civera, C. Schmid, and V. Fer-
rari. Learning object class detectors from weakly annotated
video. In Computer Vision and Pattern Recognition, 2012. 3
[24] X. Ren and D. Ramanan. Histograms of sparse codes for ob-
ject detection. In Computer Vision and Pattern Recognition,
pages 3246–3253, 2013. 2, 5, 6, 7

[25] C. Rosenberg, M. Hebert, and H. Schneiderman. Semi-
supervised self-training of object detection models. In Com-
puter Vision and Pattern Recognition, pages 29–36, 2005. 1
[26] R. Socher, M. Ganjoo, C. D. Manning, and A. Ng. Zero-shot
learning through cross-modal transfer. In Advances in Neural
Information Processing Systems, pages 935–943, 2013. 3

[27] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabi-
novich. Going deeper with convolutions. arXiv preprint
arXiv:1409.4842, 2014. 1, 2, 7

[28] K. Tang, V. Ramanathan, L. Fei-Fei, and D. Koller. Shifting
weights: Adapting object detectors from image to video. In
Advances in Neural Information Processing Systems, 2012.
3

[29] K. E. Van de Sande, J. R. Uijlings, T. Gevers, and A. W.
Segmentation as selective search for object
In International Conference on Computer Vi-

Smeulders.
recognition.
sion, pages 1879–1886, 2011. 3, 5

[30] A. Vezhnevets and V. Ferrari. Associative embeddings for
large-scale knowledge transfer with self-assessment. arXiv
preprint arXiv:1312.3240, 2013. 3

[31] L. Wang, G. Hua, R. Sukthankar, J. Xue, and N. Zheng.
Video object discovery and co-segmentation with extremely
weak supervision. In European Conference on Computer Vi-
sion, 2014. 3

[32] X. Wang, M. Yang, S. Zhu, and Y. Lin. Regionlets for generic
object detection. In International Conference on Computer
Vision, pages 17–24, 2013. 2, 5, 6

[33] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba.
Sun database: Large-scale scene recognition from abbey to
In Computer Vision and Pattern Recognition, pages
zoo.
3485–3492, 2010. 4

[34] Y. Yang, G. Shu, and M. Shah. Semi-supervised learn-
ing of feature hierarchies for object detection in a video.
In Computer Vision and Pattern Recognition, pages 1650–
1657, 2013. 3

