Question Answering as Global Reasoning over Semantic Abstractions

Daniel Khashabi ∗
University of Pennsylvania
danielkh@cis.upenn.edu

Tushar Khot

Ashish Sabharwal
Allen Institute for Artiﬁcial Intelligence (AI2)
tushark,ashishs@allenai.org

Dan Roth∗
University of Pennsylvania
danroth@cis.upenn.edu

AAAI’18

9
1
0
2
 
n
u
J
 
9
 
 
]
L
C
.
s
c
[
 
 
1
v
2
7
6
3
0
.
6
0
9
1
:
v
i
X
r
a

Abstract

We propose a novel method for exploiting the semantic struc-
ture of text to answer multiple-choice questions. The ap-
proach is especially suitable for domains that require reason-
ing over a diverse set of linguistic constructs but have limited
training data. To address these challenges, we present the
ﬁrst system, to the best of our knowledge, that reasons over
a wide range of semantic abstractions of the text, which are
derived using off-the-shelf, general-purpose, pre-trained nat-
ural language modules such as semantic role labelers, coref-
erence resolvers, and dependency parsers. Representing mul-
tiple abstractions as a family of graphs, we translate question
answering (QA) into a search for an optimal subgraph that
satisﬁes certain global and local properties. This formulation
generalizes several prior structured QA systems. Our system,
SEMANTICILP, demonstrates strong performance on two do-
mains simultaneously. In particular, on a collection of chal-
lenging science QA datasets, it outperforms various state-of-
the-art approaches, including neural models, broad coverage
information retrieval, and specialized techniques using struc-
tured knowledge bases, by 2%-6%.

Introduction
Question answering (QA) is a fundamental challenge in AI,
particularly in natural language processing (NLP) and rea-
soning. We consider the multiple-choice setting where Q is
a question, A is a set of answer candidates, and the knowl-
edge required for answering Q is available in the form of
raw text P . This setting applies naturally to reading compre-
hension questions, as well as to open format multiple-choice
questions for which information retrieval (IR) techniques are
able to obtain relevant text from a corpus.

Several state-of-the-art QA systems excel at locating the
correct answer in P based on proximity to question words,
the distributional semantics of the text, answer type, etc
(Clark et al. 2016), however, often struggle with questions
that require some form of reasoning or appeal to a more sub-
tle understanding of the supporting text or the question. We
demonstrate that we can use existing NLP modules, such as
semantic role labeling (SRL) systems with respect to multi-
ple predicate types (verbs, prepositions, nominals, etc.), to

∗Part of the work was done when the ﬁrst and last authors were

afﬁliated with the University of Illinois, Urbana-Champaign.
Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

derive multiple semantic views of the text and perform rea-
soning over these views to answer a variety of questions.

As an example, consider the following snippet of sports

news text and an associated question:

P : Teams are under pressure after PSG purchased Ney-
mar this season. Chelsea purchased Morata. The Spaniard
looked like he was set for a move to Old Trafford for the
majority of the summer only for Manchester United to sign
Romelu Lukaku instead, paving the way for Morata to ﬁnally
move to Chelsea for an initial £56m.
Q: Who did Chelsea purchase this season?
A: {(cid:88)Alvaro Morata, Neymar, Romelu Lukaku }

Given the bold-faced text P (cid:48) in P , simple word-matching
sufﬁces to correctly answer Q. However, P (cid:48) could have
stated the same information in many different ways. As
paraphrases become more complex, they begin to involve
more linguistic constructs such as coreference, punctuation,
prepositions, and nominals. This makes understanding the
text, and thus the QA task, more challenging.

For instead, P (cid:48) could instead say Morata is the recent ac-
quisition by Chelsea. This simple looking transformation
can be surprisingly confusing for highly successful systems
such as BIDAF (Seo et al. 2016), which produces the par-
tially correct phrase “Neymar this season. Morata”. On the
other hand, one can still answer the question conﬁdently by
abstracting relevant parts of Q and P , and connecting them
appropriately. Speciﬁcally, a verb SRL frame for Q would
indicate that we seek the object of the verb purchase, a nomi-
nal SRL frame for P (cid:48) would capture that the acquisition was
of Morata and was done by Chelsea, and textual similarity
would align purchase with acquisition.

Similarly, suppose P (cid:48) instead said Morata, the recent
acquisition by Chelsea, will start for the team tomorrow.
BIDAF now incorrectly chooses Neymar as the answer, pre-
sumably due to its proximity to the words purchased and
this season. However, with the right abstractions, one could
still arrive at the correct answer as depicted in Figure 1 for
our proposed system, SEMANTICILP. This reasoning uses
comma SRL to realize that the Morata is referring to the ac-
quisition, and a preposition SRL frame to capture that the
acquisition was done by Chelsea.

One can continue to make P (cid:48) more complex. For exam-

mantic abstractions of the raw text1 in a domain independent
fashion, as illustrated for an example in Figure 1. We repre-
sent these semantic abstractions as families of graphs, where
the family (e.g., trees, clusters, labeled predicate-argument
graphs, etc.) is chosen to match the nature of the abstraction
(e.g., parse tree, coreference sets, SRL frames, etc., respec-
tively). The collection of semantic graphs is then augmented
with inter- and intra-graph edges capturing lexical similarity
(e.g., word-overlap score or word2vec distance).

This semantic graph representation allows us to formu-
late QA as the search for an optimal support graph, a sub-
graph G of the above augmented graph connecting (the se-
mantic graphs of) Q and A via P . The reasoning used to
answer the question is captured by a variety of requirements
or constraints that G must satisfy, as well as a number of de-
sired properties, encapsulating the “correct” reasoning, that
makes G preferable over other valid support graphs. For in-
stance, a simple requirement is that G must be connected
and it must touch both Q and A. Similarly, if G includes
a verb from an SRL frame, it is preferable to also include
the corresponding subject. Finally, the resulting constrained
optimization problem is formulated as an Integer Linear Pro-
gram (ILP), and optimized using an off-the-shelf ILP solver.
This formalism may be viewed as a generalization of sys-
tems that reason over tables of knowledge (Cohen 2000;
Khashabi et al. 2016): instead of operating over table rows
(which are akin to labeled sequence graphs or predicate-
argument graphs), we operate over a much richer class of
semantic graphs. It can also be viewed as a generalization
of the recent TUPLEINF system (Khot, Sabharwal, and Clark
2017), which converts P into a particular kind of semantic
abstraction, namely Open IE tuples (Banko et al. 2007).

This generalization to multiple semantic abstractions
poses two key technical challenges:
(a) unlike clean
knowledge-bases (e.g., Dong et al. (2015)) used in many QA
systems, abstractions generated from NLP tools (e.g., SRL)
are noisy; and (b) even if perfect, using their output for QA
requires delineating what information in Q, A, and P is rel-
evant for a given question, and what constitutes valid rea-
soning. The latter is especially challenging when combin-
ing information from diverse abstractions that, even though
grounded in the same raw text, may not perfectly align. We
address these challenges via our ILP formulation, by using
our linguistic knowledge about the abstractions to design re-
quirements and preferences for linking these abstractions.

We present a new QA system, SEMANTICILP,2 based
on these ideas, and evaluate it on multiple-choice ques-
tions from two domains involving rich linguistic structure
and reasoning: elementary and middle-school level science
exams, and early-college level biology reading comprehen-
sion. Their data sparsity, as we show, limits the performance
of state-of-the-art neural methods such as BiDAF (Seo et al.
2016). SEMANTICILP, on the other hand, is able to suc-
cessfully capitalize on existing general-purpose NLP tools
in order to outperform existing baselines by 2%-6% on the
science exams, leading to a new state of the art. It also gen-

1This applies to all three inputs of the system: Q, A, and P .
2Code available at: https://github.com/allenai/semanticilp

Figure 1: Depiction of SEMANTICILP reasoning for the ex-
ample paragraph given in the text. Semantic abstractions of
the question, answers, knowledge snippet are shown in dif-
ferent colored boxes (blue, green, and yellow, resp.). Red
nodes and edges are the elements that are aligned (used) for
supporting the correct answer. There are many other un-
aligned (unused) annotations associated with each piece of
text that are omitted for clarity.

ple, P (cid:48) could introduce the need for coreference resolution
by phrasing the information as: Chelsea is hoping to have
a great start this season by actively hunting for new players
in the transfer period. Morata, the recent acquisition by the
team, will start for the team tomorrow. Nevertheless, with
appropriate semantic abstractions of the text, the underlying
reasoning remains relatively simple.

Given sufﬁciently large QA training data, one could
conceivably perform end-to-end training (e.g., using a
deep learning method) to address these linguistic chal-
lenges. However, existing large scale QA datasets such as
SQuAD (Rajpurkar et al. 2016) often either have a limited
linguistic richness or do not necessarily need reasoning to
arrive at the answer (Jia and Liang 2017). Consequently, the
resulting models do not transfer easily to other domains. For
instance, the above mentioned BiDAF model trained on the
SQuAD dataset performs substantially worse than a simple
IR approach on our datasets. On the other hand, many of the
QA collections in domains that require some form of reason-
ing, such as the science questions we use, are small (100s to
1000s of questions). This brings into question the viability
of the aforementioned paradigm that attempts to learn every-
thing from only the QA training data.

Towards the goal of effective structured reasoning in the
presence of data sparsity, we propose to use a rich set of
general-purpose, pre-trained NLP tools to create various se-

eralizes well, as demonstrated by its strong performance on
biology questions in the PROCESSBANK dataset (Berant et
al. 2014). Notably, while the best existing system for the lat-
ter relies on domain-speciﬁc structural annotation and ques-
tion processing, SEMANTICILP needs neither.

Related Work
There are numerous QA systems operating over large
knowledge-bases. Our work, however, is most closely re-
lated to systems that work either directly on the input text
or on a semantic representation derived on-the-ﬂy from it.
In the former category are IR and statistical correlation
based methods with surprisingly strong performance (Clark
et al. 2016), as well as a number of recent neural archi-
tectures such as BiDAF (Seo et al. 2016), Decomposable
In the latter
Attention Model (Parikh et al. 2016), etc.
category are approaches that perform structured reason-
ing over some abstraction of text. For instance, Khashabi
et al. (2016) perform reasoning on the knowledge tables
constructed using semi-automatical methods, Khot, Sabhar-
wal, and Clark (2017) use Open IE subject-verb-object rela-
tions (Etzioni et al. 2008), Banarescu et al. (2013) use AMR
annotators (Wang et al. 2015), and Krishnamurthy, Tafjord,
and Kembhavi (2016) use a semantic parser (Zettlemoyer
and Collins 2005) to answer a given question. Our work dif-
fers in that we use of a wide variety of semantic abstractions
simultaneously, and perform joint reasoning over them to
identify which abstractions are relevant for a given question
and how best to combine information from them.

Our formalism can be seen as an extension of some of
the prior work on structured reasoning over semi-structured
text. For instance, in our formalism, each table used by
Khashabi et al. (2016) can be viewed as a semantic frame
and represented as a predicate-argument graph. The table-
chaining rules used there are equivalent to the reasoning we
deﬁne when combining two annotation components. Simi-
larly, Open IE tuples used by (Khot, Sabharwal, and Clark
2017) can also be viewed as a predicate-argument structure.
One key abstraction we use is the predicate-argument
structure provided by Semantic Role Labeling (SRL). Many
SRL systems have been designed (Gildea and Jurafsky 2002;
Punyakanok, Roth, and Yih 2008) using linguistic resources
such as FrameNet (Baker, Fillmore, and Lowe 1998), Prop-
Bank (Kingsbury and Palmer 2002), and NomBank (Meyers
et al. 2004). These systems are meant to convey high-level
information about predicates (which can be a verb, a noun,
etc.) and related elements in the text. The meaning of each
predicate is conveyed by a frame, the schematic representa-
tions of a situation. Phrases with similar semantics ideally
map to the same frame and roles. Our system also uses other
NLP modules, such as for coreference resolution (Lee et al.
2013) and dependency parsing (Chang et al. 2015).

While it appears simple to use SRL systems for
QA (Palmer, Gildea, and Kingsbury 2005), this has found
limited success (Kaisser and Webber 2007; Pizzato and
Moll´a 2008; Moreda et al. 2011). The challenges earlier
approaches faced were due to making use of VerbSRL only,
while QA depends on richer information, not only verb pred-
icates and their arguments, along with some level of brittle-

ness of all NLP systems. Shen and Lapata (2007) have partly
addressed the latter challenge with an inference framework
that formulates the task as a bipartite matching problem over
the assignment of semantic roles, and managed to slightly
improve QA. In this work we address both these challenges
and go beyond the limitations of using a single predicate
SRL system; we make use of SRL abstractions that are
based on verbs, nominals, prepositions, and comma predi-
cates, as well as textual similarity. We then develop an infer-
ence framework capable of exploiting combinations of these
multiple SRL (and other) views, thus operating over a more
complete semantic representation of the text.

A key aspect of QA is handling textual variations, on
which there has been prior work using dependency-parse
transformations (Punyakanok, Roth, and Yih 2004). These
approaches often deﬁne inference rules, which can generate
new trees starting from a base tree. Bar-Haim, Dagan, and
Berant (2015) and Stern et al. (2012) search over a space
of a pre-deﬁned set of text transformations (e.g., corefer-
ence substitutions, passive to active). Our work differs in
that we consider a much wider range of textual variations
by combining multiple abstractions, and make use of a more
expressive inference framework.

Another aspect of our approach is the graphical represen-
tation of knowledge and the reasoning over it. We model
the QA problem as a search in the space of potential sup-
port graphs. Search-based reasoning systems have been suc-
cessful in several NLP areas (Roth and Yih 2004; Chang
et al. 2010; Berant, Dagan, and Goldberger 2010; Srikumar
and Roth 2011; Goldwasser and Roth 2011; Sch¨uller 2014;
Fei et al. 2015). Compared to previous works, we use a
larger set of annotators to account for diverse linguistic phe-
nomena in questions and to handle errors of individual an-
notators.

Knowledge Abstraction and Representation
We begin with our formalism for abstracting knowledge
from text and representing it as a family of graphs, followed
by speciﬁc instantiations of these abstractions using off-the-
shelf NLP modules.

Semantic Abstractions
The pivotal ingredient of the abstraction is raw text. This
representation is used for question Q, each answer option
Ai and the knowledge snippet P , which potentially contains
the answer to the question. The KB for a given raw text,
consists of the text itself, embellished with various Seman-
ticGraphs attached to it, as depicted in Figure 2.

Each SemanticGraph is representable from a family of
graphs. In principle there need not be any constraints on the
permitted graph families; however for ease of representation
we choose the graphs to belong to one of the 5 following
families: Sequence graphs represent labels for each token in
the sentence. Span family represents labels for spans of the
text. Tree, is a tree representation of text spans. Cluster fam-
ily, contain spans of text in different groups. PredArg fam-
ily represents predicates and their arguments; in this view
edges represent the connections between each single predi-

Given the instance graphs, creating augmented graph will be
done implicitly as an optimization problem in the next step.

QA as Reasoning Over Semantic Graphs
We introduce our treatment of QA as an optimal subgraph
selection problem over knowledge. We treat question an-
swering as the task of ﬁnding the best support in the knowl-
edge snippet, for a given question and answer pair, measured
in terms of the strength of a “support graph” deﬁned as fol-
lows.

The inputs to the QA system are, a question K(Q), the
set of answers {K(Ai)} and given a knowledge snippet
K(P ).4 Given such representations, we will form a reason-
ing problem, which is formulated as an optimization prob-
lem, searching for a “support graph” that connects the ques-
tion nodes to a unique answer option through nodes and
edges of a snippet.

Deﬁne the instance graph I = I(Q, {Ai} , P ) as the
union of knowledge graphs: I (cid:44) K(Q) ∪ (K(Ai)) ∪ K(P ).
Intuitively, we would like the support graph to be connected,
and to include nodes from the question, the answer option,
and the knowledge. Since the SemanticGraph is composed
of many disjoint sub-graph, we deﬁne augmented graph I +
to model a bigger structure over the instance graphs I. Es-
sentially we augment the instance graph and weight the new
edges. Deﬁne a scoring function f : (v1, v2) labels pair of
nodes v1 and v2 with an score which represents their phrase-
level entailment or similarity.
Deﬁnition 1. An augmented graph I +, for a question Q, an-
swers {Ai} and knowledge P , is deﬁned with the following
properties:
1. Nodes: v(I +) = v(I(Q, {Ai} , P ))
2. Edges:5

e(I +) = e(I) ∪ K(Q) ⊗ K(P ) ∪ [∪iK(P ) ⊗ K(Ai)]

3. Edge weights: for any e ∈ I +:

• If e /∈ I, the edge connects two nodes in different con-

nected components:

∀e = (v1, v2) /∈ I : w(e) = f (v1, v2)

• If e ∈ I, the edge belongs to a connected component,
and the edge weight information about the reliability of
the SemanticGraph and semantics of the two nodes.
∀g ∈ I, ∀e ∈ g : w(e) = f (cid:48)(e, g)

Next, we have to deﬁne support graphs, the set of graphs
that support the reasoning of a question. For this we will
apply some structured constraints on the augmented graph.
Deﬁnition 2. A support graph G = G(Q, {Ai} , P ) for a
question Q, answer-options {Ai} and paragraph P , is a sub-
graph (V, E) of I + with the following properties:

4For simplicity, from now on, we drop “knowledge”; e.g., in-

stead of saying “question knowledge”, we say “question”.

5Deﬁne K(T1) ⊗ K(T2) (cid:44) (cid:83)

(g1,g2)∈
K(T1)×K(T2)

v(g1) × v(g2),

Figure 2: Knowledge Representation used in our formula-
tion. Raw text is associated with a collection of Seman-
ticGraphs, which convey certain information about the text.
There are implicit similarity edges among the nodes of the
connected components of the graphs, and from nodes to the
corresponding raw-text spans.

cates and its arguments. Each SemanticGraph belongs to
one of the graph families and its content is determined by the
semantics of the information it represents and the text itself.
We deﬁne the knowledge more formally here. For a given
paragraph, T , its representation K(T ) consists of a set of
semantic graphs K(T ) = {g1, g2, . . .}. We deﬁne v(g) =
{ci} and e(g) = {(ci, cj)} to be the set of nodes and edges
of a given graph, respectively.

Semantic Graph Generators
Having introduced a graph-based abstraction for knowledge
and categorized it into a family of graphs, we now delin-
eate the instantiations we used for each family. Many of
the pre-trained extraction tools we use are available in COG-
COMPNLP.3
• Sequence or labels for sequence of tokens; for example

Lemma and POS (Roth and Zelenko 1998).

• Span which can contains labels for spans of text; we in-
stantiated Shallow-Parse (Punyakanok and Roth 2001),
Quantities (Roy, Vieira, and Roth 2015), NER (Ratinov
and Roth 2009; Redman, Sammons, and Roth 2016)).
• Tree, a tree representation connecting spans of text
for this we used Dependency of Chang et

as nodes;
al. (2015).

• Cluster, or spans of text clustered in groups. An example

is Coreference (Lee et al. 2011).

• PredArg;

for this view we used Verb-SRL and Nom-
SRL(Punyakanok, Roth, and Yih 2008; Roth and Lapata
2016), Prep-SRL (Srikumar and Roth 2013), Comma-SRL
(Arivazhagan, Christodoulopoulos, and Roth 2016).

Given SemanticGraph generators we have the question,
answers and paragraph represented as a collection of graphs.

3Available at: http://github.com/CogComp/cogcomp-nlp

where v(g1) × v(g2) = {(v, w); v ∈ v(g1), w ∈ v(g2)} .

Sem. Graph

PredArg

Cluster
Tree
SpanLa-
belView

Property
Use at least (a) a predicate and its
argument, or (b) two arguments
Use at least two nodes
Use two nodes with distance less than k

Use at least k nodes

Table 1: Minimum requirements for using each family of
graphs. Each graph connected component (e.g. a PredArg
frame, or a Coreference chain) cannot be used unless the
above-mentioned conditioned is satisﬁed.

1. G is connected.
2. G has intersection with the question, the knowledge, and

exactly one answer candidate:6

G ∩ K(Q) (cid:54)= ∅, G ∩ K(P ) (cid:54)= ∅, ∃! i : G ∩ K(Ai) (cid:54)= ∅

3. G satisﬁes structural properties per each connected com-

ponent, as summarized in Table 1.
Deﬁnition 2 characterizes what we call a potential solu-
tion to a question. A given question and paragraph give rise
to a large number of possible support graphs. We deﬁne the
space of feasible support graphs as G (i.e., all the graphs that
satisfy Deﬁnition 2, for a given (Q, {Ai} , P )). To rank var-
ious feasible support graphs in such a large space, we deﬁne
a scoring function score(G) as:

(cid:88)

(cid:88)

w(v) +

w(e) −

wc 1{c is violated} (1)

v∈v(G)

e∈e(G)

(cid:88)

c∈C

for some set of preferences (or soft-constraints) C. When c is
violated, denoted by the indicator function 1{c is violated}
in Eq. (1), we penalize the objective value by some ﬁxed
amount wc. The second term is supposed to bring more
sparsity to the desired solutions, just like how regulariza-
tion terms act in machine learning models (Natarajan 1995).
The ﬁrst term is the sum of weights we deﬁned when con-
structing the augmented-graph, and is supposed to give more
weight to the models that have better and more reliable
alignments between its nodes. The role of the inference pro-
cess will be to choose the “best” one under our notion of
desirable support graphs:

G∗ = arg max

score(G)

G∈G

(2)

ILP Formulation
Our QA system, SEMANTICILP, models the above support
graph search of Eq. (2) as an ILP optimization problem, i.e.,
as maximizing a linear objective function over a ﬁnite set of
variables, subject to a set of linear inequality constraints. A
summary of the model is given below.

The augmented graph is not explicitly created; instead, it
is implicitly created. The nodes and edges of the augmented
graph are encoded as a set of binary variables. The value
of the binary variables reﬂects whether a node or an edge

6∃! here denotes the uniqueness quantiﬁer, meaning “there ex-

ists one and only one”.

- Number of sentences used is more than k
- Active edges connected to each chunk of the answer
option, more than k
- More than k chunks in the active answer-option
- More than k edges to each question constituent
- Number of active question-terms
- If using PredArgof K(Q), at least an argument should
be used
- If using PredArg(Verb-SRL) of K(Q), at least one
predicate should be used.

Table 2: The set of preferences functions in the objective.

is used in the optimal graph G∗. The properties listed in
Table 1 are implemented as weighted linear constraints using
the variables deﬁned for the nodes and edges.

As mentioned, edge weights in the augmented graph
come from a function, f , which captures (soft) phrasal en-
tailment between question and paragraph nodes, or para-
graph and answer nodes, to account for lexical variability.
In our evaluations, we use two types of f .
(a) Similar to
Khashabi et al. (2016), we use a WordNet-based (Miller
1995) function to score word-to-word alignments, and use
this as a building block to compute a phrase-level alignment
score as the weighted sum of word-level alignment scores.
Word-level scores are computed using WordNet’s hypernym
and synonym relations, and weighted using relevant word-
sense frequency. f for similarity (as opposed to entailment)
is taken to be the average of the entailment scores in both
directions.
(b) For longer phrasal alignments (e.g., when
aligning phrasal verbs) we use the Paragram system of Wi-
eting et al. (2015).

The ﬁnal optimization is done on Eq. (1). The ﬁrst part
of the objective is the sum of the weights of the sub-graph,
which is what an ILP does, since the nodes and edges are
modeled as variables in the ILP. The second part of Eq. (1)
contains a set of preferences C, summarized in Table 2,
meant to apply soft structural properties that partly depen-
dant on the knowledge instantiation. These preferences are
soft in the sense that they are applied with a weight to the
overall scoring function (as compare to a hard constraint).
For each preference function c there is an associated binary
or integer variable with weight wc, and we create appropri-
ate constraints to simulate the corresponding behavior.

We note that the ILP objective and constraints aren’t tied
to the particular domain of evaluation; they represent gen-
eral properties that capture what constitutes a well supported
answer for a given question.

Empirical Evaluation
We evaluate on two domains that differ in the nature of the
supporting text (concatenated individual sentences vs. a co-
herent paragraph), the underlying reasoning, and the way
questions are framed. We show that SEMANTICILP outper-
forms a variety of baselines, including retrieval-based meth-
ods, neural-networks, structured systems, and the current

best system for each domain. These datasets and systems
are described next, followed by results.

Question Sets

For the ﬁrst domain, we have a collection of question sets
containing elementary-level science questions from stan-
dardized tests (Clark et al. 2016; Khot, Sabharwal, and
Clark 2017). Speciﬁcally, REGENTS 4TH contains all non-
diagram multiple choice questions from 6 years of NY Re-
gents 4th grade science exams (127 train questions, 129
test). REGENTS 8TH similarly contains 8th grade questions
(147 train, 144 test). The corresponding expanded datasets
are AI2PUBLIC 4TH (432 train, 339 test) and AI2PUBLIC
8TH (293 train, 282 test).7

For the second domain, we use the PROCESSBANK8
dataset for the reading comprehension task proposed by Be-
rant et al. (2014).
It contains paragraphs about biologi-
cal processes and two-way multiple choice questions about
them. We used a broad subset of this dataset that asks about
events or about an argument that depends on another event
or argument.9. The resulting dataset has 293 train and 109
test questions, based on 147 biology paragraphs.

Test scores are reported as percentages. For each ques-
tion, a system gets a score of 1 if it chooses the correct an-
swer, 1/k if it reports a k-way tie that includes the correct
answer, and 0 otherwise.

Question Answering Systems

We consider a variety of baselines, including the best system
for each domain.

IR (information retrieval baseline). We use the IR solver
from Clark et al. (2016), which selects the answer option that
has the best matching sentence in a corpus. The sentence is
forced to have a non-stopword overlap with both q and a.

SEMANTICILP (our approach). Given the input instance
(question, answer options, and a paragraph), we invoke var-
ious NLP modules to extract semantic graphs. We then gen-
erate an ILP and solve it using the open source SCIP en-
gine (Achterberg 2009), returning the active answer option
am from the optimal solution found. To check for ties, we
disable am, re-solve the ILP, and compare the score of the
second-best answer, if any, with that of the best score.

For the science question sets, where we don’t have any
paragraphs attached to each question, we create a passage
by using the above IR solver to retrieve scored sentences
for each answer option and then combining the top 8 unique
sentences (across all answer options) to form a paragraph.

While the sub-graph optimization can be done over the
entire augmented graph in one shot, our current implemen-
tation uses multiple simpliﬁed solvers, each performing rea-
soning over augmented graphs for a commonly occurring
annotator combination, as listed in Table 3. For all of these

7AI2 Science Questions V1 at http://data.allenai.org/ai2-

science-questions

8https://nlp.stanford.edu/software/bioprocess
9These are referred to as “dependency questions” by Berant et

al. (2014), and cover around 70% of all questions.

Combination

Representation

Comb-1

Comb-2

Comb-3

Comb-4

Comb-5

K(Q) = {Shallow-Parse, Tokens}
K(P ) = {Shallow-Parse, Tokens, Dependency}
K(Q) = {Verb-SRL, Shallow-Parse}
K(P ) = {Verb-SRL}
K(Q) = {Verb-SRL, Shallow-Parse}
K(P ) = {Verb-SRL, Coreference}
K(Q) = {Verb-SRL, Shallow-Parse}
K(P ) = {Comma-SRL}
K(Q) = {Verb-SRL, Shallow-Parse}
K(P ) = {Prep-SRL}

Table 3: The semantic annotator combinations used in our
implementation of SEMANTICILP.

annotator combinations, we let the representation of the an-
swers be K(A) = {Shallow-Parse, Tokens}.
Importantly,
our choice of working with a few annotator combinations
is mainly for simplicity of implementation and sufﬁces to
demonstrate that reasoning over even just two annotators at a
time can be surprisingly powerful. There is no fundamental
limitation in implementing SEMANTICILP using one single
optimization problem as stated in Eq. (2).

Each simpliﬁed solver associated with an annotator com-
bination in Table 3 produces a conﬁdence score for each an-
swer option. We create an ensemble of these solvers as a
linear combination of these scores, with weights trained us-
ing the union of training data from all questions sets.

BIDAF (neural network baseline). We use the re-
cent deep learning reading comprehension model of Seo et
al. (2016), which is one of the top performing systems on
the SQuAD dataset and has been shown to generalize to an-
other domain as well (Min, Seo, and Hajishirzi 2017). Since
BIDAF was designed for ﬁll-in-the-blank style questions,
we follow the variation used by Kembhavi et al. (2017) to
apply it to our multiple-choice setting. Speciﬁcally, we com-
pare the predicted answer span to each answer candidate and
report the one with the highest similarity.

We use two variants: the original system, BIDAF, pre-
trained on 100,000+ SQuAD questions, as well as an ex-
tended version, BIDAF’, obtained by performing continu-
ous training to ﬁne-tune the SQuAD-trained parameters us-
ing our (smaller) training sets. For the latter, we convert
multiple-choice questions into reading comprehension ques-
tions by generating all possible text-spans within sentences,
with token-length at most correct answer length + 2, and
choose the ones with the highest similarity score with the
correct answer. We use the ALLENNLP re-implementation
of BIDAF10, train it on SQuAD, followed by training it on
our dataset. We tried different variations (epochs and learn-
ing rates) and selected the model which gives the best aver-
age score across all the datasets. As we will see, the variant
that was further trained on our data often gives better results.
TUPLEINF (semi-structured inference baseline). Re-
cently proposed by Khot, Sabharwal, and Clark (2017), this
is a state-of-the-art system designed for science questions.
It uses Open IE (Banko et al. 2007) tuples derived from the

10Available at: https://github.com/allenai/allennlp

text as the knowledge representation, and performs reason-
ing over it via an ILP. It has access to a large knowledge
base of Open IE tuples, and exploits redundancy to over-
come challenges introduced by noise and linguistic variabil-
ity.

PROREAD and SYNTPROX. PROREADis a specialized
and best performing system on the PROCESSBANK ques-
tion set. Berant et al. (2014) annotated the training data with
events and event relations, and trained a system to extract
the process structure. Given a question, PROREAD converts
it into a query (using regular expression patterns and key-
words) and executes it on the process structure as the knowl-
edge base. Its reliance on a question-dependent query gen-
erator and on a process structure extractor makes it difﬁcult
to apply to other domains.

SYNTPROX is another solver suggested by (Berant et al.
2014). It aligns content word lemmas in both the question
and the answer against the paragraph, and select the answer
tokens that are closer to the aligned tokens of the questions.
The distance is measured using dependency tree edges. To
support multiple sentences they connect roots of adjacent
sentences with bidirectional edges.

Experimental Results
We evaluate various QA systems on datasets from the two
domains. The results are summarized below, followed by
some some insights into SEMANTICILP’s behavior and an
error analysis.

Science Exams. The results of experimenting on different
grades’ science exams are summarized in Table 4, which
shows the exam scores as a percentage. The table demon-
strates that SEMANTICILP consistently outperforms the
best baselines in each case by 2%-6%.

Further, there is no absolute winner among the baselines;
while IR is good on the 8th grade questions, TUPLEINF and
BIDAF’ are better on 4th grade questions. This highlights
the differing nature of questions for different grades.

Dataset
REGENTS 4TH
AI2PUBLIC 4TH
REGENTS 8TH
AI2PUBLIC 8TH

BIDAF BIDAF’

56.3
50.7
53.5
47.7

53.1
57.4
62.8
51.9

IR TUPLEINF SEMANTICILP
59.3
54.9
64.2
52.8

67.6
59.7
66.0
55.9

61.4
56.1
61.3
51.6

Table 4: Science test scores as a percentage. On elemen-
tary level science exams, SEMANTICILP consistently out-
performs baselines. In each row, the best score is in bold
and the best baseline is italicized.

Biology Exam. The results on the PROCESSBANK dataset
are summarized in Table 5. While SEMANTICILP’s perfor-
mance is substantially better than most baselines and close to
that of PROREAD, it is important to note that this latter base-
line enjoys additional supervision of domain-speciﬁc event
annotations. This, unlike our other relatively general base-
lines, makes it limited to this dataset, which is also why we
don’t include it in Table 4.

We evaluate IR on this reading comprehension dataset by
creating an ElasticSearch index, containing the sentences of
the knowledge paragraphs.

PROREAD SYNTPROX IR BIDAF BIDAF’ SEMANTICILP

68.1

61.9

63.8

58.7

61.3

67.9

Table 5: Biology test scores as a percentage. SEMANTI-
CILP outperforms various baselines on the PROCESSBANK
dataset and roughly matches the specialized best method.

Error and Timing Analysis

For some insight into the results, we include a brief analysis
of our system’s output compared to that of other systems.

We identify a few main reasons for SEMANTICILP’s er-
rors. Not surprisingly, some mistakes (see Appendix ﬁgure
for an example) can be traced back to failures in generat-
ing proper annotation (SemanticGraph). Improvement in
SRL modules or redundancy can help address this. Some
mistakes are from the current ILP model not supporting the
ideal reasoning, i.e., the requisite knowledge exists in the
annotations, but the reasoning fails to exploit it. Another
group of mistakes is due to the complexity of the sentences,
and the system lacking a way to represent the underlying
phenomena with our current annotators.

A weakness (that doesn’t seem to be particular to our
solver) is reliance on explicit mentions. If there is a meaning
indirectly implied by the context and our annotators are not
able to capture it, our solver will miss such questions. There
will be more room for improvement on such questions with
the development of discourse analysis systems.

When solving the questions that don’t have an attached
paragraph, relevant sentences need to be fetched from a cor-
pus. A subset of mistakes on this dataset occurs because the
extracted knowledge does not contain the correct answer.

ILP Solution Properties. Our system is implemented us-
ing many constraints, requires using many linear inequalities
which get instantiated on each input instanced, hence there
are a different number of variables and inequalities for each
input instance. There is an overhead time for pre-processing
an input instance, and convert it into an instance graph. Here
in the timing analysis we provide we ignore the annotation
time, as it is done by black-boxes outside our solver.

Table 6 summarizes various ILP and support graph statis-
tics for SEMANTICILP, averaged across PROCESSBANK
questions. Next to SEMANTICILP we have included num-
bers from TABLEILP which has similar implementation ma-
chinery, but on a very different representation. While the
size of the model is a function of the input instance, on aver-
age, SEMANTICILP tends to have a bigger model (number
of constraints and variables). The model creation time is
signiﬁcantly time-consuming in SEMANTICILP as involves
many graph traversal operations and jumps between nodes
and edges. We also providing times statistics for TUPLE-
INF which takes roughly half the time of TABLEILP, which
means that it is faster than SEMANTICILP.

Category

Quantity

Avg.
(SEMANTICILP)

Avg.
(TABLEILP)

Avg.
(TUPLEINF)

ILP complexity

Timing stats

#variables
#constraints
model creation
solving the ILP

2254.9
4518.7
5.3 sec
1.8 sec

1043.8
4417.8
1.9 sec
2.1 sec

1691.0
4440.0
1.7 sec
0.3 sec

Table 6: SEMANTICILP statistics averaged across ques-
tions, as compared to TABLEILP and TUPLEINF statistics.

Ablation Study
In order to better understand the results, we ablate the contri-
bution of different annotation combinations, where we drop
different combination from the ensemble model. We retrain
the ensemble, after dropping each combination.

The results are summarized in Table 7. While Comb-1
seems to be important for science tests, it has limited con-
tribution to the biology tests. On 8th grade exams, the Verb-
SRL and Comma-SRL-based alignments provide high value.
Structured combinations (e.g., Verb-SRL-based alignments)
are generally more important for the biology domain.

Full SEMANTICILP
no Comb-1
no Comb-2
no Comb-3
no Comb-4
no Comb-5

AI2PUBLIC 8TH
55.9
-3.1
-2.0
-0.6
-3.1
-0.1

PROCESSBANK
67.9
-1.8
-4.6
-1.8
-1.8
-5.1

Table 7: Ablation study of SEMANTICILP components on
various datasets. The ﬁrst row shows the overall test score
of the full system, while other rows report the change in the
score as a result of dropping an individual combination. The
combinations are listed in Table 3.

Complementarity to IR. Given that in the science do-
main the input snippets fed to SEMANTICILP are retrieved
through a process similar to the IR solver, one might natu-
rally expect some similarity in the predictions. The pie-chart
in Figure 3 shows the overlap between mistakes and correct
predictions of SEMANTICILP and IR on 50 randomly cho-
sen training questions from AI2PUBLIC 4TH. While there
is substantial overlap in questions that both answer correctly
(the yellow slice) and both miss (the red slice), there is also
a signiﬁcant number of questions solved by SEMANTICILP
but not IR (the blue slice), almost twice as much as the ques-
tions solved by IRbut not SEMANTICILP (the green slice).

In Tables 4 and 5, we presented one sin-
Cascade Solvers.
gle instance of SEMANTICILP with state-of-art results on
multiple datasets, where the solver was an ensemble of se-
mantic combinations (presented in Table 3). Here we show a
simpler approach that achieves stronger results on individual
datasets, at the cost of losing a little generalization across do-
mains. Speciﬁcally, we create two “cascades” (i.e., decision
lists) of combinations, where the ordering of combinations
in the cascade is determined by the training set precision of

Figure 3: Overlap of the predictions of SEMANTICILP and
IR on 50 randomly-chosen questions from AI2PUBLIC 4TH.

the simpliﬁed solver representing an annotator combination
(combinations with higher precision appear earlier). One
cascade solver targets science exams and the other the biol-
ogy dataset.

The results are reported in Table 8. On the 8th grade data,
the cascade solver created for science test achieves higher
scores than the generic ensemble solver. Similarly, the cas-
cade solver on the biology domain outperforms the ensem-
ble solver on the PROCESSBANK dataset.

Dataset

Ensemble

e REGENTS 4TH

c
n
e
i
c
S

AI2PUBLIC 4TH
REGENTS 8TH
AI2PUBLIC 8TH
PROCESSBANK

67.6
59.7
66.0
55.9
67.9

Cascade
(Science)
64.7
56.7
69.4
56.5
59.6

Cascade
(Biology)
63.1
55.7
60.3
54.3
68.8

Table 8: Comparison of test scores of SEMANTICILP using
a generic ensemble vs. domain-targeted cascades of annota-
tion combinations.

Effect of Varying Knowledge Length. We analyze the
performance of the system as a function of the length of the
paragraph fed into SEMANTICILP, for 50 randomly selected
training questions from the REGENTS 4TH set. Figure 4
(left) shows the overall system, for two combinations intro-
duced earlier, as a function of knowledge length, counted as
the number of sentences in the paragraph.

As expected, the solver improves with more sentences,
until around 12-15 sentences, after which it starts to worsen
with the addition of more irrelevant knowledge. While the
cascade combinations did not show much generalization
across domains, they have the advantage of a smaller drop
when adding irrelevant knowledge compared to the ensem-
ble solver. This can be explained by the simplicity of cas-
cading and minimal training compared to the ensemble of
annotation combinations.

Figure 4: Performance change for varying knowledge length.

Figure 4 (right) shows the performance of individual com-
binations as a function of knowledge length. It is worth high-
lighting that while Comb-1 (blue) often achieves higher cov-
erage and good scores in simple paragraphs (e.g., science ex-
ams), it is highly sensitive to knowledge length. On the other
hand, highly-constrained combinations have a more consis-
tent performance with increasing knowledge length, at the
cost of lower coverage.

Conclusion

Question answering is challenging in the presence of lin-
guistic richness and diversity, especially when arriving at
the correct answer requires some level of reasoning. De-
parting from the currently popular paradigm of generating
a very large dataset and learning “everything” from it in an
end-to-end fashion, we argue—and demonstrate via our QA
system—that one can successfully leverage pre-trained NLP
modules to extract a sufﬁciently complete linguistic abstrac-
tion of the text that allows answering interesting questions
about it. This approach is particularly valuable in settings
where there is a small amount of data. Instead of exploiting
peculiarities of a large but homogeneous dataset, as many
state-of-the-art QA systems end up doing, we focus on con-
ﬁdently performing certain kinds of reasoning, as captured
by our semantic graphs and the ILP formulation of support
graph search over them. Bringing these ideas to practice,
our system, SEMANTICILP, achieves state-of-the-art perfor-
mance on two domains with very different characteristics,
outperforming both traditional and neural models.

Acknowledgments

The authors would like to thank Peter Clark, Bhavana Dalvi,
Minjoon Seo, Oyvind Tafjord, Niket Tandon, and Luke
Zettlemoyer for valuable discussions and help.

This work is supported by a gift from AI2 and by con-
tract FA8750-13-2-0008 with the US Defense Advanced Re-
search Projects Agency (DARPA). The views expressed are
those of the authors and do not reﬂect the ofﬁcial policy or
position of the U.S. Government.

References
Achterberg, T. 2009. SCIP: solving constraint integer programs.
Math. Prog. Computation 1(1):1–41.
Arivazhagan, N.; Christodoulopoulos, C.; and Roth, D. 2016. La-
beling the semantic roles of commas. In AAAI, 2885–2891.
Baker, C. F.; Fillmore, C. J.; and Lowe, J. B. 1998. The berkeley
framenet project. In ACL, 86–90.
Banarescu, L.; Bonial, C.; Cai, S.; Georgescu, M.; Grifﬁtt, K.;
Hermjakob, U.; Knight, K.; Palmer, M.; and Schneider, N. 2013.
Abstract meaning representation for sembanking. In Linguistic An-
notation Workshop and Interoperability with Discourse.
Banko, M.; Cafarella, M. J.; Soderland, S.; Broadhead, M.; and
Etzioni, O. 2007. Open information extraction from the web. In
IJCAI.
Bar-Haim, R.; Dagan, I.; and Berant, J. 2015. Knowledge-based
J. Artif. Intell.
textual inference via parse-tree transformations.
Res.(JAIR) 54:1–57.
Berant, J.; Srikumar, V.; Chen, P.-C.; Vander Linden, A.; Harding,
B.; Huang, B.; Clark, P.; and Manning, C. D. 2014. Modeling
biological processes for reading comprehension. In EMNLP.
Berant, J.; Dagan, I.; and Goldberger, J. 2010. Global learning of
focused entailment graphs. In ACL, 1220–1229.
Chang, M.-W.; Goldwasser, D.; Roth, D.; and Srikumar, V. 2010.
Discriminative learning over constrained latent representations. In
NAACL, 429–437.
Chang, K.-W.; Upadhyay, S.; Chang, M.-W.; Srikumar, V.; and
Roth, D. 2015. Illinois-SL: A java library for structured prediction.
arXiv preprint arXiv:1509.07179.
Clark, P.; Etzioni, O.; Khot, T.; Sabharwal, A.; Tafjord, O.; Turney,
P.; and Khashabi, D. 2016. Combining retrieval, statistics, and
inference to answer elementary science questions. In AAAI.
Cohen, W. W. 2000. Data integration using similarity joins and
a word-based information representation language. ACM Transac-
tions on Information Systems 18(3):288–321.
Dong, L.; Wei, F.; Zhou, M.; and Xu, K. 2015. Question answering
over Freebase with multi-column convolutional neural networks. In
ACL.
Etzioni, O.; Banko, M.; Soderland, S.; and Weld, D. S. 2008. Open
information extraction from the web. Communications of the ACM
51(12):68–74.
Fei, Z.; Khashabi, D.; Peng, H.; Wu, H.; and Roth, D. 2015.
Illinois-Proﬁler: knowledge schemas at scale. Workshop on Cog-
nitive Knowledge Acquisition and Applications (Cognitum).

Punyakanok, V.; Roth, D.; and Yih, W. 2008. The importance of
syntactic parsing and inference in semantic role labeling. Compu-
tational Linguistics 34(2).
Rajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P.
2016.
SQuAD: 100,000+ questions for machine comprehension of text.
In EMNLP.
Ratinov, L., and Roth, D. 2009. Design challenges and misconcep-
tions in named entity recognition. In CONLL.
Redman, T.; Sammons, M.; and Roth, D. 2016.
Illinois named
entity recognizer: Addendum to Ratinov and Roth ’09 reporting
improved results. Tech Report.
Roth, M., and Lapata, M. 2016. Neural semantic role labeling with
dependency path embeddings. ACL.
Roth, D., and Yih, W. 2004. A linear programming formulation
for global inference in natural language tasks. In CONLL, 1–8.
Roth, D., and Zelenko, D. 1998. Part of speech tagging using a
network of linear separators. In COLING-ACL, 1136–1142.
Roy, S.; Vieira, T.; and Roth, D. 2015. Reasoning about quantities
in natural language. TACL 3.
Sch¨uller, P. 2014. Tackling winograd schemas by formalizing rele-
vance theory in knowledge graphs. In International Conference on
the Principles of Knowledge Representation and Reasoning.
Seo, M.; Kembhavi, A.; Farhadi, A.; and Hajishirzi, H. 2016.
Bidirectional attention ﬂow for machine comprehension. ICLR.
Shen, D., and Lapata, M. 2007. Using semantic roles to improve
question answering. In EMNLP-CoNLL, 12–21.
Srikumar, V., and Roth, D. 2011. A joint model for extended
semantic role labeling. In EMNLP.
Srikumar, V., and Roth, D. 2013. Modeling semantic relations
expressed by prepositions. TACL 1:231–242.
Stern, A.; Stern, R.; Dagan, I.; and Felner, A. 2012. Efﬁcient
search for transformation-based inference. In ACL, 283–291.
Wang, C.; Xue, N.; Pradhan, S.; and Pradhan, S.
2015. A
transition-based algorithm for amr parsing. In HLT-NAACL, 366–
375.
Wieting, J.; Bansal, M.; Gimpel, K.; Livescu, K.; and Roth,
D. 2015. From paraphrase database to compositional paraphrase
model and back. TACL 3:345–358.
Zettlemoyer, L. S., and Collins, M. 2005. Learning to map sen-
tences to logical form: Structured classiﬁcation with probabilistic
categorial grammars. UAI.

Gildea, D., and Jurafsky, D. 2002. Automatic labeling of semantic
roles. Computational linguistics 28(3):245–288.
Goldwasser, D., and Roth, D. 2011. Learning from natural instruc-
tions. In IJCAI.
Jia, P., and Liang, P. 2017. Adversarial examples for evaluating
reading comprehension systems. EMNLP.
Kaisser, M., and Webber, B. 2007. Question answering based on
semantic roles. In Proceedings of the workshop on deep linguistic
processing, 41–48.
Kembhavi, A.; Seo, M.; Schwenk, D.; Choi, J.; Farhadi, A.; and
Hajishirzi, H. 2017. Are you smarter than a sixth grader? text-
book question answering for multimodal machine comprehension.
CVPR.
Khashabi, D.; Khot, T.; Sabharwal, A.; Clark, P.; Etzioni, O.; and
Roth, D. 2016. Question answering via integer programming over
semi-structured knowledge. In IJCAI.
Khot, T.; Sabharwal, A.; and Clark, P. 2017. Answering complex
questions using open information extraction. ACL.
Kingsbury, P., and Palmer, M. 2002. From treebank to propbank.
In LREC, 1989–1993.
Krishnamurthy, J.; Tafjord, O.; and Kembhavi, A. 2016. Semantic
parsing to probabilistic programs for situated question answering.
EMNLP.
Lee, H.; Peirsman, Y.; Chang, A.; Chambers, N.; Surdeanu, M.;
and Jurafsky, D. 2011. Stanford’s multi-pass sieve coreference
resolution system at the conll-2011 shared task. In CONLL Shared
Task, 28–34.
Lee, H.; Chang, A.; Peirsman, Y.; Chambers, N.; Surdeanu, M.;
and Jurafsky, D. 2013. Deterministic coreference resolution based
on entity-centric, precision-ranked rules. Computational Linguis-
tics 39(4):885–916.
Meyers, A.; Reeves, R.; Macleod, C.; Szekely, R.; Zielinska, V.;
Young, B.; and Grishman, R. 2004. The nombank project: An
interim report. In HLT-NAACL 2004 workshop: Frontiers in corpus
annotation, volume 24, 31.
Miller, G. 1995. Wordnet: a lexical database for english. Commu-
nications of the ACM 38(11):39–41.
Min, S.; Seo, M. J.; and Hajishirzi, H. 2017. Question answering
through transfer learning from large ﬁne-grained supervision data.
In ACL.
Moreda, P.; Llorens, H.; Bor´o, E. S.; and Palomar, M. 2011. Com-
bining semantic information in question answering systems. Inf.
Process. Manage. 47:870–885.
Natarajan, B. K. 1995. Sparse approximate solutions to linear
systems. SIAM journal on computing 24(2):227–234.
Palmer, M.; Gildea, D.; and Kingsbury, P. 2005. The proposi-
tion bank: An annotated corpus of semantic roles. Computational
linguistics 31(1):71–106.
Parikh, A. P.; T¨ackstr¨om, O.; Das, D.; and Uszkoreit, J. 2016. A
decomposable attention model for natural language inference. In
EMNLP.
Pizzato, L. A., and Moll´a, D. 2008. Indexing on semantic roles for
question answering. In 2nd workshop on Information Retrieval for
Question Answering, 74–81.
Punyakanok, V., and Roth, D. 2001. The use of classiﬁers in se-
quential inference. In NIPS, 995–1001. MIT Press.
Punyakanok, V.; Roth, D.; and Yih, W. 2004. Mapping dependen-
cies trees: An application to question answering. AIM.

Appendix: Reasoning Formulation
Here we provide some details of our reasoning formulation
and its implementation as an ILP.

The support graph search for QA is modeled as an ILP
optimization problem, i.e., as maximizing a linear objective
function over a ﬁnite set of variables, subject to a set of lin-
ear inequality constraints. We note that the ILP objective
and constraints aren’t tied to the particular domain of eval-
uation; they represent general properties that capture what
constitutes a well-supported answer for a given question.

Our formulation consists of multiple kinds of reasoning,
encapsulating our semantic understanding of the types of
knowledge (e.g., verbs, corefs, etc.) extracted from the text,
the family to graphs used to represent them, and how they
interact in order to provide support for an answer candidate.
Each kind of reasoning is added to a general body, deﬁned
in Table 14, that is shared among different reasoning types.
This general body encapsulates basic requirements such as
at most one answer candidate being chosen in the resulting
support graph.

In what follows we delineate various forms of reasoning,
capturing different semantic abstractions and valid interac-
tions between them.

Comb-1 (Shallow Alignment) This reasoning captures
the least-constrained formulation for answering questions.
We create alignments between Tokens view of the question,
and spans of Shallow-Parse view of the paragraph. Align-
ments are scored with the entailment module; the closer the
surface strings are, the higher score their edge gets. There
are variables to induce sparsity in the output; for example,
penalty variables for using too many sentences in the para-
graph, or using too many spans in the paragraph. To Add
more structure to this, we use Coreference and Dependency
views of the paragraph; for example, alignments that are
closer to each other according the dependency parse, get
a higher score. The Coreference links let the reasoning to
jump in between sentences. Figure 7 shows an example out-
put of this type of reasoning. As it can be seen, the alignment
is using multiple terms in the questions and multiple spans
in the paragraph, and the spans belong to one single sentence
in the paragraph.

Comb-2 (Verb-SRL Alignment) This reasoning is using
Verb-SRL views in both question and paragraph. The core
of this alignment is creating connections between the pred-
icate/arguments and the predicate/arguments of the para-
graph, respectively. The edges are scored with our entail-
ment system; hence the bigger the edge weight is, the higher
chance of appearing in the output. An example outputs are
in Figure 7.

Comb-5 (Verb-SRL+Prep-SRL Alignment)
In this type
of reasoning we observe the combination of Verb-SRL and
Prep-SRL. This can be considered as an extension of Comb-
2 (Verb-SRL alignment), we the alignment is in between
Verb-SRL in question, Verb-SRL and Prep-SRL in the para-
graph. The arguments of the Verb-SRLin the question are
aligned to the arguments of either Verb-SRL and Prep-SRL
in the paragraph. Predicates of the Verb-SRL are aligned to

Similar

the predicates Verb-SRL. An example output prediction is
shown in Figure 7.
to the
different

can
as Comb-
identify
3(Verb-SRL+Coreference Alignment),
Comb-4(Verb-
SRL+Comma-SRL Alignment) or Verb-SRL+Nom-SRL
Alignment.

combinations
combinations,

explained,
such

one

* Variables:

Active answer option variable
Active answer option chunk variable

* Active variable constraints:

If any edge connected to any active variable that belongs to
answer option, the answer should be active
If an answer is active, there must be at least one active edge
connected to it.

* Consistency constraints:

Only one answer option should be active.

Table 9: Generic template used as part of each reasoning

* Basic variables:
Active question-terms
Active paragraph-spans
Active paragraph sentence
Question-term to paragraph-span alignment variable
Paragraph-span alignment to answer-option term
alignment variable

* Active variable constraints:

Question-term should be active, if any edge connected to it
is active.
If a question-term is active, at least one edge connected to
it should be active.
Sentence should be active, if anything connected to it is
active.
If a sentence is active, at least one incoming edge to one of
its terms/spans should be active.

* Question sparsity-inducing variables:

More than k active question-term penalty. (for k = 1, 2, 3)
More than k active alignments to each question-term
penalty. (for k = 1, 2, 3)

* Paragraph sparsity-inducing variables:

- Active sentence penalty variable.
* Proximity-inducing variables:

Active dependency-edge boost variable: if two variables
are connected in dependency path, and are both active, this
variable can be active.
- Word-pair distance ≤ k words boost: variable between
any two word-pair with distance less than k and active if
both ends are active. (for k = 1, 2, 3)

* Sparsity-inducing variables in answer options:

- More than k active chunks in the active answer-option.
(for k = 1, 2, 3)
- More than k active edges connected to each chunk of the
active answer option. (for k = 1, 2, 3)

Table 10: Comb-1 (Shallow Alignment)

Figure 5: Examples of system output for (1) top: Comb-1 (Shallow alignment) (2) middle: Comb-2 (Verb-SRL alignment) (3)
bottom: Comb-5 (Verb-SRL+ Prep-SRL alignment).

* Variables:

Active Verb-SRL constituents in question (both predicates
and arguments), 0.01.
Active Verb-SRL constituents in the paragraph (including
predicates and arguments), with weight 0.01.
Edge between question-Verb-SRL-argument and
paragraph-Verb-SRL-argument (if the entailment score
> 0.6).
Edge between question-Verb-SRL-predicate and
paragraph-Verb-SRL-predicate (if the entailment score
> 0.6).
Edge between paragraph-Verb-SRL-argument and
answer-options (if the entailment score > 0.6).
Edges between predicate and its arguments argument,
inside each Verb-SRL frame in the paragraph, each edge
with weight 0.01.

* Consistency constraints:

Constraints to take care of active variables (e.g. edge
variable is active iff its two nodes are active).
At least k = 2 constituents in the question should be
active.
At least k = 1 predicates in the question should be active.
At most k = 1 predicates in the paragraph should be
active.
For each Verb-SRL frame in the paragraph, if the predicate
is inactive, its arguments should be inactive as well.

Table 11: Comb-2 (Verb-SRL alignment)

Figure 6: Example output of a question SEMANTICILP answered incorrectly due to a mistake in annotations. “eating” in the
paragraph is incorrectly recognized as a verb predicate, with “breathing” as subject, resulting in this incorrect alignment.

* Variables:

Active Verb-SRL constituents in question (both predicates
and arguments), with weight 0.001.
Active Verb-SRL constituents in the paragraph (including
predicates and arguments), with weight 0.001.
Active Prep-SRL constituents in the paragraph (including
predicates and arguments), with weight 0.001.
Edges between any pair of Prep-SRL arguments in the
paragraph and Verb-SRL arguments, with weights
extracted from PARAGRAM(if these scores are > 0.7).
Edges between predicates and arguments of the Prep-SRL
frames, with weight 0.02.
Edges between predicates and arguments of the Verb-SRL
frames, with weight 0.01.
Edge between question-Verb-SRL-argument and
paragraph coref-constituents (if cell-cell entailment score
> 0.6)
Edge between question-Verb-SRL-predicate and
paragraph Verb-SRL-predicate (if phrase-sim score > 0.5)
Edge between paragraph Verb-SRL-arguments and answer
options (if cell-cell score is > 0.7)

* Consistency constraints:

Each Prep-SRL argument has to have at least an incoming
edge (in addition to the edge from its predicate)
Not possible to have a Verb-SRL argument (in the
paragraph)
connected to two Prep-SRL arguments of the same frame
(no loop)
Exactly one Prep-SRL predicate in the paragraph
At least one Verb-SRL predicate in the paragraph
At most one Verb-SRL predicate in the paragraph
Not more than one argument of a frame can be connected
to the answer option
Each Verb-SRL argument in the paragraph should have at
least two active edges connected to.

Table 12: Comb-5 (Verb-SRL+Prep-SRLalignment)

* Variables:

Active Verb-SRL constituents in question (including
predicates and arguments), with weight 0.001.
Active Verb-SRL constituents in the paragraph (including
predicates and arguments), with weight 0.001.
Active Coreference constituents in the paragraph, with
weight 0.001.
Active Coreference chains in the paragraph, with weight
−0.0001.
Edges between any pair of Coreference-constituent in the
paragraph that belong to the same Coreference chain,
with weight 0.02.
Edge between question-Verb-SRL-argument and
paragraph Coreference-constituents (if entailment score
> 0.6)
Edge between question-Verb-SRL-predicate and
paragraph Verb-SRL-predicate (if phrase-sim score > 0.4)
Edge between paragraph Verb-SRL arguments and answer
options (if symmetric entailment score is > 0.65)

* Consistency constraints:

Constraints to take care of active variables (e.g. edge
variable is active iff its two nodes are active).
At most k = 1 Coreference chain in the paragraph.
At least k = 1 constituents in the question should be
active.
At most k = 1 Verb-SRL predicates in the paragraph.
(could relax this and have alignment between multiple
SRL frames in the paragraph)
The question constituents can be active, only if at least one
of the edges connected to it is active.
Paragraph Verb-SRL-predicate should have at least two
active edges.
If paragraph Verb-SRL-predicate is inactive, the whole
frame should be inactive.

Table 13: Comb-3 (Verb-SRL+Coreference alignment)

Appendix: Reasoning Formulation
Here we provide some details of our reasoning formulation
and its implementation as an ILP.

The support graph search for QA is modeled as an ILP
optimization problem, i.e., as maximizing a linear objective
function over a ﬁnite set of variables, subject to a set of lin-
ear inequality constraints. We note that the ILP objective
and constraints aren’t tied to the particular domain of eval-
uation; they represent general properties that capture what
constitutes a well-supported answer for a given question.

Our formulation consists of multiple kinds of reasoning,
encapsulating our semantic understanding of the types of
knowledge (e.g., verbs, corefs, etc.) extracted from the text,
the family to graphs used to represent them, and how they
interact in order to provide support for an answer candidate.
Each kind of reasoning is added to a general body, deﬁned
in Table 14, that is shared among different reasoning types.
This general body encapsulates basic requirements such as
at most one answer candidate being chosen in the resulting
support graph.

In what follows we delineate various forms of reasoning,
capturing different semantic abstractions and valid interac-
tions between them.

Comb-1 (Shallow Alignment) This reasoning captures
the least-constrained formulation for answering questions.
We create alignments between Tokens view of the question,
and spans of Shallow-Parse view of the paragraph. Align-
ments are scored with the entailment module; the closer the
surface strings are, the higher score their edge gets. There
are variables to induce sparsity in the output; for example,
penalty variables for using too many sentences in the para-
graph, or using too many spans in the paragraph. To Add
more structure to this, we use Coreference and Dependency
views of the paragraph; for example, alignments that are
closer to each other according the dependency parse, get
a higher score. The Coreference links let the reasoning to
jump in between sentences. Figure 7 shows an example out-
put of this type of reasoning. As it can be seen, the alignment
is using multiple terms in the questions and multiple spans
in the paragraph, and the spans belong to one single sentence
in the paragraph.

Comb-2 (Verb-SRL Alignment) This reasoning is using
Verb-SRL views in both question and paragraph. The core
of this alignment is creating connections between the pred-
icate/arguments and the predicate/arguments of the para-
graph, respectively. The edges are scored with our entail-
ment system; hence the bigger the edge weight is, the higher
chance of appearing in the output. An example outputs are
in Figure 7.

Comb-5 (Verb-SRL+Prep-SRL Alignment)
In this type
of reasoning we observe the combination of Verb-SRL and
Prep-SRL. This can be considered as an extension of Comb-
2 (Verb-SRL alignment), we the alignment is in between
Verb-SRL in question, Verb-SRL and Prep-SRL in the para-
graph. The arguments of the Verb-SRLin the question are
aligned to the arguments of either Verb-SRL and Prep-SRL
in the paragraph. Predicates of the Verb-SRL are aligned to

Similar

the predicates Verb-SRL. An example output prediction is
shown in Figure 7.
to the
different

can
as Comb-
identify
3(Verb-SRL+Coreference Alignment),
Comb-4(Verb-
SRL+Comma-SRL Alignment) or Verb-SRL+Nom-SRL
Alignment.

combinations
combinations,

explained,
such

one

* Variables:

Active answer option variable
Active answer option chunk variable

* Active variable constraints:

If any edge connected to any active variable that belongs to
answer option, the answer should be active
If an answer is active, there must be at least one active edge
connected to it.

* Consistency constraints:

Only one answer option should be active.

Table 14: Generic template used as part of each reasoning

* Basic variables:
Active question-terms
Active paragraph-spans
Active paragraph sentence
Question-term to paragraph-span alignment variable
Paragraph-span alignment to answer-option term
alignment variable

* Active variable constraints:

Question-term should be active, if any edge connected to it
is active.
If a question-term is active, at least one edge connected to
it should be active.
Sentence should be active, if anything connected to it is
active.
If a sentence is active, at least one incoming edge to one of
its terms/spans should be active.

* Question sparsity-inducing variables:

More than k active question-term penalty. (for k = 1, 2, 3)
More than k active alignments to each question-term
penalty. (for k = 1, 2, 3)

* Paragraph sparsity-inducing variables:

- Active sentence penalty variable.
* Proximity-inducing variables:

Active dependency-edge boost variable: if two variables
are connected in dependency path, and are both active, this
variable can be active.
- Word-pair distance ≤ k words boost: variable between
any two word-pair with distance less than k and active if
both ends are active. (for k = 1, 2, 3)

* Sparsity-inducing variables in answer options:

- More than k active chunks in the active answer-option.
(for k = 1, 2, 3)
- More than k active edges connected to each chunk of the
active answer option. (for k = 1, 2, 3)

Table 15: Comb-1 (Shallow Alignment)

Figure 7: Examples of system output for (1) top: Comb-1 (Shallow alignment) (2) middle: Comb-2 (Verb-SRL alignment) (3)
bottom: Comb-5 (Verb-SRL+ Prep-SRL alignment).

* Variables:

Active Verb-SRL constituents in question (both predicates
and arguments), 0.01.
Active Verb-SRL constituents in the paragraph (including
predicates and arguments), with weight 0.01.
Edge between question-Verb-SRL-argument and
paragraph-Verb-SRL-argument (if the entailment score
> 0.6).
Edge between question-Verb-SRL-predicate and
paragraph-Verb-SRL-predicate (if the entailment score
> 0.6).
Edge between paragraph-Verb-SRL-argument and
answer-options (if the entailment score > 0.6).
Edges between predicate and its arguments argument,
inside each Verb-SRL frame in the paragraph, each edge
with weight 0.01.

* Consistency constraints:

Constraints to take care of active variables (e.g. edge
variable is active iff its two nodes are active).
At least k = 2 constituents in the question should be
active.
At least k = 1 predicates in the question should be active.
At most k = 1 predicates in the paragraph should be
active.
For each Verb-SRL frame in the paragraph, if the predicate
is inactive, its arguments should be inactive as well.

Table 16: Comb-2 (Verb-SRL alignment)

Figure 8: Example output of a question SEMANTICILP answered incorrectly due to a mistake in annotations. “eating” in the
paragraph is incorrectly recognized as a verb predicate, with “breathing” as subject, resulting in this incorrect alignment.

* Variables:

Active Verb-SRL constituents in question (both predicates
and arguments), with weight 0.001.
Active Verb-SRL constituents in the paragraph (including
predicates and arguments), with weight 0.001.
Active Prep-SRL constituents in the paragraph (including
predicates and arguments), with weight 0.001.
Edges between any pair of Prep-SRL arguments in the
paragraph and Verb-SRL arguments, with weights
extracted from PARAGRAM(if these scores are > 0.7).
Edges between predicates and arguments of the Prep-SRL
frames, with weight 0.02.
Edges between predicates and arguments of the Verb-SRL
frames, with weight 0.01.
Edge between question-Verb-SRL-argument and
paragraph coref-constituents (if cell-cell entailment score
> 0.6)
Edge between question-Verb-SRL-predicate and
paragraph Verb-SRL-predicate (if phrase-sim score > 0.5)
Edge between paragraph Verb-SRL-arguments and answer
options (if cell-cell score is > 0.7)

* Consistency constraints:

Each Prep-SRL argument has to have at least an incoming
edge (in addition to the edge from its predicate)
Not possible to have a Verb-SRL argument (in the
paragraph)
connected to two Prep-SRL arguments of the same frame
(no loop)
Exactly one Prep-SRL predicate in the paragraph
At least one Verb-SRL predicate in the paragraph
At most one Verb-SRL predicate in the paragraph
Not more than one argument of a frame can be connected
to the answer option
Each Verb-SRL argument in the paragraph should have at
least two active edges connected to.

Table 17: Comb-5 (Verb-SRL+Prep-SRLalignment)

* Variables:

Active Verb-SRL constituents in question (including
predicates and arguments), with weight 0.001.
Active Verb-SRL constituents in the paragraph (including
predicates and arguments), with weight 0.001.
Active Coreference constituents in the paragraph, with
weight 0.001.
Active Coreference chains in the paragraph, with weight
−0.0001.
Edges between any pair of Coreference-constituent in the
paragraph that belong to the same Coreference chain,
with weight 0.02.
Edge between question-Verb-SRL-argument and
paragraph Coreference-constituents (if entailment score
> 0.6)
Edge between question-Verb-SRL-predicate and
paragraph Verb-SRL-predicate (if phrase-sim score > 0.4)
Edge between paragraph Verb-SRL arguments and answer
options (if symmetric entailment score is > 0.65)

* Consistency constraints:

Constraints to take care of active variables (e.g. edge
variable is active iff its two nodes are active).
At most k = 1 Coreference chain in the paragraph.
At least k = 1 constituents in the question should be
active.
At most k = 1 Verb-SRL predicates in the paragraph.
(could relax this and have alignment between multiple
SRL frames in the paragraph)
The question constituents can be active, only if at least one
of the edges connected to it is active.
Paragraph Verb-SRL-predicate should have at least two
active edges.
If paragraph Verb-SRL-predicate is inactive, the whole
frame should be inactive.

Table 18: Comb-3 (Verb-SRL+Coreference alignment)

Question Answering as Global Reasoning over Semantic Abstractions

Daniel Khashabi ∗
University of Pennsylvania
danielkh@cis.upenn.edu

Tushar Khot

Ashish Sabharwal
Allen Institute for Artiﬁcial Intelligence (AI2)
tushark,ashishs@allenai.org

Dan Roth∗
University of Pennsylvania
danroth@cis.upenn.edu

AAAI’18

9
1
0
2
 
n
u
J
 
9
 
 
]
L
C
.
s
c
[
 
 
1
v
2
7
6
3
0
.
6
0
9
1
:
v
i
X
r
a

Abstract

We propose a novel method for exploiting the semantic struc-
ture of text to answer multiple-choice questions. The ap-
proach is especially suitable for domains that require reason-
ing over a diverse set of linguistic constructs but have limited
training data. To address these challenges, we present the
ﬁrst system, to the best of our knowledge, that reasons over
a wide range of semantic abstractions of the text, which are
derived using off-the-shelf, general-purpose, pre-trained nat-
ural language modules such as semantic role labelers, coref-
erence resolvers, and dependency parsers. Representing mul-
tiple abstractions as a family of graphs, we translate question
answering (QA) into a search for an optimal subgraph that
satisﬁes certain global and local properties. This formulation
generalizes several prior structured QA systems. Our system,
SEMANTICILP, demonstrates strong performance on two do-
mains simultaneously. In particular, on a collection of chal-
lenging science QA datasets, it outperforms various state-of-
the-art approaches, including neural models, broad coverage
information retrieval, and specialized techniques using struc-
tured knowledge bases, by 2%-6%.

Introduction
Question answering (QA) is a fundamental challenge in AI,
particularly in natural language processing (NLP) and rea-
soning. We consider the multiple-choice setting where Q is
a question, A is a set of answer candidates, and the knowl-
edge required for answering Q is available in the form of
raw text P . This setting applies naturally to reading compre-
hension questions, as well as to open format multiple-choice
questions for which information retrieval (IR) techniques are
able to obtain relevant text from a corpus.

Several state-of-the-art QA systems excel at locating the
correct answer in P based on proximity to question words,
the distributional semantics of the text, answer type, etc
(Clark et al. 2016), however, often struggle with questions
that require some form of reasoning or appeal to a more sub-
tle understanding of the supporting text or the question. We
demonstrate that we can use existing NLP modules, such as
semantic role labeling (SRL) systems with respect to multi-
ple predicate types (verbs, prepositions, nominals, etc.), to

∗Part of the work was done when the ﬁrst and last authors were

afﬁliated with the University of Illinois, Urbana-Champaign.
Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

derive multiple semantic views of the text and perform rea-
soning over these views to answer a variety of questions.

As an example, consider the following snippet of sports

news text and an associated question:

P : Teams are under pressure after PSG purchased Ney-
mar this season. Chelsea purchased Morata. The Spaniard
looked like he was set for a move to Old Trafford for the
majority of the summer only for Manchester United to sign
Romelu Lukaku instead, paving the way for Morata to ﬁnally
move to Chelsea for an initial £56m.
Q: Who did Chelsea purchase this season?
A: {(cid:88)Alvaro Morata, Neymar, Romelu Lukaku }

Given the bold-faced text P (cid:48) in P , simple word-matching
sufﬁces to correctly answer Q. However, P (cid:48) could have
stated the same information in many different ways. As
paraphrases become more complex, they begin to involve
more linguistic constructs such as coreference, punctuation,
prepositions, and nominals. This makes understanding the
text, and thus the QA task, more challenging.

For instead, P (cid:48) could instead say Morata is the recent ac-
quisition by Chelsea. This simple looking transformation
can be surprisingly confusing for highly successful systems
such as BIDAF (Seo et al. 2016), which produces the par-
tially correct phrase “Neymar this season. Morata”. On the
other hand, one can still answer the question conﬁdently by
abstracting relevant parts of Q and P , and connecting them
appropriately. Speciﬁcally, a verb SRL frame for Q would
indicate that we seek the object of the verb purchase, a nomi-
nal SRL frame for P (cid:48) would capture that the acquisition was
of Morata and was done by Chelsea, and textual similarity
would align purchase with acquisition.

Similarly, suppose P (cid:48) instead said Morata, the recent
acquisition by Chelsea, will start for the team tomorrow.
BIDAF now incorrectly chooses Neymar as the answer, pre-
sumably due to its proximity to the words purchased and
this season. However, with the right abstractions, one could
still arrive at the correct answer as depicted in Figure 1 for
our proposed system, SEMANTICILP. This reasoning uses
comma SRL to realize that the Morata is referring to the ac-
quisition, and a preposition SRL frame to capture that the
acquisition was done by Chelsea.

One can continue to make P (cid:48) more complex. For exam-

mantic abstractions of the raw text1 in a domain independent
fashion, as illustrated for an example in Figure 1. We repre-
sent these semantic abstractions as families of graphs, where
the family (e.g., trees, clusters, labeled predicate-argument
graphs, etc.) is chosen to match the nature of the abstraction
(e.g., parse tree, coreference sets, SRL frames, etc., respec-
tively). The collection of semantic graphs is then augmented
with inter- and intra-graph edges capturing lexical similarity
(e.g., word-overlap score or word2vec distance).

This semantic graph representation allows us to formu-
late QA as the search for an optimal support graph, a sub-
graph G of the above augmented graph connecting (the se-
mantic graphs of) Q and A via P . The reasoning used to
answer the question is captured by a variety of requirements
or constraints that G must satisfy, as well as a number of de-
sired properties, encapsulating the “correct” reasoning, that
makes G preferable over other valid support graphs. For in-
stance, a simple requirement is that G must be connected
and it must touch both Q and A. Similarly, if G includes
a verb from an SRL frame, it is preferable to also include
the corresponding subject. Finally, the resulting constrained
optimization problem is formulated as an Integer Linear Pro-
gram (ILP), and optimized using an off-the-shelf ILP solver.
This formalism may be viewed as a generalization of sys-
tems that reason over tables of knowledge (Cohen 2000;
Khashabi et al. 2016): instead of operating over table rows
(which are akin to labeled sequence graphs or predicate-
argument graphs), we operate over a much richer class of
semantic graphs. It can also be viewed as a generalization
of the recent TUPLEINF system (Khot, Sabharwal, and Clark
2017), which converts P into a particular kind of semantic
abstraction, namely Open IE tuples (Banko et al. 2007).

This generalization to multiple semantic abstractions
poses two key technical challenges:
(a) unlike clean
knowledge-bases (e.g., Dong et al. (2015)) used in many QA
systems, abstractions generated from NLP tools (e.g., SRL)
are noisy; and (b) even if perfect, using their output for QA
requires delineating what information in Q, A, and P is rel-
evant for a given question, and what constitutes valid rea-
soning. The latter is especially challenging when combin-
ing information from diverse abstractions that, even though
grounded in the same raw text, may not perfectly align. We
address these challenges via our ILP formulation, by using
our linguistic knowledge about the abstractions to design re-
quirements and preferences for linking these abstractions.

We present a new QA system, SEMANTICILP,2 based
on these ideas, and evaluate it on multiple-choice ques-
tions from two domains involving rich linguistic structure
and reasoning: elementary and middle-school level science
exams, and early-college level biology reading comprehen-
sion. Their data sparsity, as we show, limits the performance
of state-of-the-art neural methods such as BiDAF (Seo et al.
2016). SEMANTICILP, on the other hand, is able to suc-
cessfully capitalize on existing general-purpose NLP tools
in order to outperform existing baselines by 2%-6% on the
science exams, leading to a new state of the art. It also gen-

1This applies to all three inputs of the system: Q, A, and P .
2Code available at: https://github.com/allenai/semanticilp

Figure 1: Depiction of SEMANTICILP reasoning for the ex-
ample paragraph given in the text. Semantic abstractions of
the question, answers, knowledge snippet are shown in dif-
ferent colored boxes (blue, green, and yellow, resp.). Red
nodes and edges are the elements that are aligned (used) for
supporting the correct answer. There are many other un-
aligned (unused) annotations associated with each piece of
text that are omitted for clarity.

ple, P (cid:48) could introduce the need for coreference resolution
by phrasing the information as: Chelsea is hoping to have
a great start this season by actively hunting for new players
in the transfer period. Morata, the recent acquisition by the
team, will start for the team tomorrow. Nevertheless, with
appropriate semantic abstractions of the text, the underlying
reasoning remains relatively simple.

Given sufﬁciently large QA training data, one could
conceivably perform end-to-end training (e.g., using a
deep learning method) to address these linguistic chal-
lenges. However, existing large scale QA datasets such as
SQuAD (Rajpurkar et al. 2016) often either have a limited
linguistic richness or do not necessarily need reasoning to
arrive at the answer (Jia and Liang 2017). Consequently, the
resulting models do not transfer easily to other domains. For
instance, the above mentioned BiDAF model trained on the
SQuAD dataset performs substantially worse than a simple
IR approach on our datasets. On the other hand, many of the
QA collections in domains that require some form of reason-
ing, such as the science questions we use, are small (100s to
1000s of questions). This brings into question the viability
of the aforementioned paradigm that attempts to learn every-
thing from only the QA training data.

Towards the goal of effective structured reasoning in the
presence of data sparsity, we propose to use a rich set of
general-purpose, pre-trained NLP tools to create various se-

eralizes well, as demonstrated by its strong performance on
biology questions in the PROCESSBANK dataset (Berant et
al. 2014). Notably, while the best existing system for the lat-
ter relies on domain-speciﬁc structural annotation and ques-
tion processing, SEMANTICILP needs neither.

Related Work
There are numerous QA systems operating over large
knowledge-bases. Our work, however, is most closely re-
lated to systems that work either directly on the input text
or on a semantic representation derived on-the-ﬂy from it.
In the former category are IR and statistical correlation
based methods with surprisingly strong performance (Clark
et al. 2016), as well as a number of recent neural archi-
tectures such as BiDAF (Seo et al. 2016), Decomposable
In the latter
Attention Model (Parikh et al. 2016), etc.
category are approaches that perform structured reason-
ing over some abstraction of text. For instance, Khashabi
et al. (2016) perform reasoning on the knowledge tables
constructed using semi-automatical methods, Khot, Sabhar-
wal, and Clark (2017) use Open IE subject-verb-object rela-
tions (Etzioni et al. 2008), Banarescu et al. (2013) use AMR
annotators (Wang et al. 2015), and Krishnamurthy, Tafjord,
and Kembhavi (2016) use a semantic parser (Zettlemoyer
and Collins 2005) to answer a given question. Our work dif-
fers in that we use of a wide variety of semantic abstractions
simultaneously, and perform joint reasoning over them to
identify which abstractions are relevant for a given question
and how best to combine information from them.

Our formalism can be seen as an extension of some of
the prior work on structured reasoning over semi-structured
text. For instance, in our formalism, each table used by
Khashabi et al. (2016) can be viewed as a semantic frame
and represented as a predicate-argument graph. The table-
chaining rules used there are equivalent to the reasoning we
deﬁne when combining two annotation components. Simi-
larly, Open IE tuples used by (Khot, Sabharwal, and Clark
2017) can also be viewed as a predicate-argument structure.
One key abstraction we use is the predicate-argument
structure provided by Semantic Role Labeling (SRL). Many
SRL systems have been designed (Gildea and Jurafsky 2002;
Punyakanok, Roth, and Yih 2008) using linguistic resources
such as FrameNet (Baker, Fillmore, and Lowe 1998), Prop-
Bank (Kingsbury and Palmer 2002), and NomBank (Meyers
et al. 2004). These systems are meant to convey high-level
information about predicates (which can be a verb, a noun,
etc.) and related elements in the text. The meaning of each
predicate is conveyed by a frame, the schematic representa-
tions of a situation. Phrases with similar semantics ideally
map to the same frame and roles. Our system also uses other
NLP modules, such as for coreference resolution (Lee et al.
2013) and dependency parsing (Chang et al. 2015).

While it appears simple to use SRL systems for
QA (Palmer, Gildea, and Kingsbury 2005), this has found
limited success (Kaisser and Webber 2007; Pizzato and
Moll´a 2008; Moreda et al. 2011). The challenges earlier
approaches faced were due to making use of VerbSRL only,
while QA depends on richer information, not only verb pred-
icates and their arguments, along with some level of brittle-

ness of all NLP systems. Shen and Lapata (2007) have partly
addressed the latter challenge with an inference framework
that formulates the task as a bipartite matching problem over
the assignment of semantic roles, and managed to slightly
improve QA. In this work we address both these challenges
and go beyond the limitations of using a single predicate
SRL system; we make use of SRL abstractions that are
based on verbs, nominals, prepositions, and comma predi-
cates, as well as textual similarity. We then develop an infer-
ence framework capable of exploiting combinations of these
multiple SRL (and other) views, thus operating over a more
complete semantic representation of the text.

A key aspect of QA is handling textual variations, on
which there has been prior work using dependency-parse
transformations (Punyakanok, Roth, and Yih 2004). These
approaches often deﬁne inference rules, which can generate
new trees starting from a base tree. Bar-Haim, Dagan, and
Berant (2015) and Stern et al. (2012) search over a space
of a pre-deﬁned set of text transformations (e.g., corefer-
ence substitutions, passive to active). Our work differs in
that we consider a much wider range of textual variations
by combining multiple abstractions, and make use of a more
expressive inference framework.

Another aspect of our approach is the graphical represen-
tation of knowledge and the reasoning over it. We model
the QA problem as a search in the space of potential sup-
port graphs. Search-based reasoning systems have been suc-
cessful in several NLP areas (Roth and Yih 2004; Chang
et al. 2010; Berant, Dagan, and Goldberger 2010; Srikumar
and Roth 2011; Goldwasser and Roth 2011; Sch¨uller 2014;
Fei et al. 2015). Compared to previous works, we use a
larger set of annotators to account for diverse linguistic phe-
nomena in questions and to handle errors of individual an-
notators.

Knowledge Abstraction and Representation
We begin with our formalism for abstracting knowledge
from text and representing it as a family of graphs, followed
by speciﬁc instantiations of these abstractions using off-the-
shelf NLP modules.

Semantic Abstractions
The pivotal ingredient of the abstraction is raw text. This
representation is used for question Q, each answer option
Ai and the knowledge snippet P , which potentially contains
the answer to the question. The KB for a given raw text,
consists of the text itself, embellished with various Seman-
ticGraphs attached to it, as depicted in Figure 2.

Each SemanticGraph is representable from a family of
graphs. In principle there need not be any constraints on the
permitted graph families; however for ease of representation
we choose the graphs to belong to one of the 5 following
families: Sequence graphs represent labels for each token in
the sentence. Span family represents labels for spans of the
text. Tree, is a tree representation of text spans. Cluster fam-
ily, contain spans of text in different groups. PredArg fam-
ily represents predicates and their arguments; in this view
edges represent the connections between each single predi-

Given the instance graphs, creating augmented graph will be
done implicitly as an optimization problem in the next step.

QA as Reasoning Over Semantic Graphs
We introduce our treatment of QA as an optimal subgraph
selection problem over knowledge. We treat question an-
swering as the task of ﬁnding the best support in the knowl-
edge snippet, for a given question and answer pair, measured
in terms of the strength of a “support graph” deﬁned as fol-
lows.

The inputs to the QA system are, a question K(Q), the
set of answers {K(Ai)} and given a knowledge snippet
K(P ).4 Given such representations, we will form a reason-
ing problem, which is formulated as an optimization prob-
lem, searching for a “support graph” that connects the ques-
tion nodes to a unique answer option through nodes and
edges of a snippet.

Deﬁne the instance graph I = I(Q, {Ai} , P ) as the
union of knowledge graphs: I (cid:44) K(Q) ∪ (K(Ai)) ∪ K(P ).
Intuitively, we would like the support graph to be connected,
and to include nodes from the question, the answer option,
and the knowledge. Since the SemanticGraph is composed
of many disjoint sub-graph, we deﬁne augmented graph I +
to model a bigger structure over the instance graphs I. Es-
sentially we augment the instance graph and weight the new
edges. Deﬁne a scoring function f : (v1, v2) labels pair of
nodes v1 and v2 with an score which represents their phrase-
level entailment or similarity.
Deﬁnition 1. An augmented graph I +, for a question Q, an-
swers {Ai} and knowledge P , is deﬁned with the following
properties:
1. Nodes: v(I +) = v(I(Q, {Ai} , P ))
2. Edges:5

e(I +) = e(I) ∪ K(Q) ⊗ K(P ) ∪ [∪iK(P ) ⊗ K(Ai)]

3. Edge weights: for any e ∈ I +:

• If e /∈ I, the edge connects two nodes in different con-

nected components:

∀e = (v1, v2) /∈ I : w(e) = f (v1, v2)

• If e ∈ I, the edge belongs to a connected component,
and the edge weight information about the reliability of
the SemanticGraph and semantics of the two nodes.
∀g ∈ I, ∀e ∈ g : w(e) = f (cid:48)(e, g)

Next, we have to deﬁne support graphs, the set of graphs
that support the reasoning of a question. For this we will
apply some structured constraints on the augmented graph.
Deﬁnition 2. A support graph G = G(Q, {Ai} , P ) for a
question Q, answer-options {Ai} and paragraph P , is a sub-
graph (V, E) of I + with the following properties:

4For simplicity, from now on, we drop “knowledge”; e.g., in-

stead of saying “question knowledge”, we say “question”.

5Deﬁne K(T1) ⊗ K(T2) (cid:44) (cid:83)

(g1,g2)∈
K(T1)×K(T2)

v(g1) × v(g2),

Figure 2: Knowledge Representation used in our formula-
tion. Raw text is associated with a collection of Seman-
ticGraphs, which convey certain information about the text.
There are implicit similarity edges among the nodes of the
connected components of the graphs, and from nodes to the
corresponding raw-text spans.

cates and its arguments. Each SemanticGraph belongs to
one of the graph families and its content is determined by the
semantics of the information it represents and the text itself.
We deﬁne the knowledge more formally here. For a given
paragraph, T , its representation K(T ) consists of a set of
semantic graphs K(T ) = {g1, g2, . . .}. We deﬁne v(g) =
{ci} and e(g) = {(ci, cj)} to be the set of nodes and edges
of a given graph, respectively.

Semantic Graph Generators
Having introduced a graph-based abstraction for knowledge
and categorized it into a family of graphs, we now delin-
eate the instantiations we used for each family. Many of
the pre-trained extraction tools we use are available in COG-
COMPNLP.3
• Sequence or labels for sequence of tokens; for example

Lemma and POS (Roth and Zelenko 1998).

• Span which can contains labels for spans of text; we in-
stantiated Shallow-Parse (Punyakanok and Roth 2001),
Quantities (Roy, Vieira, and Roth 2015), NER (Ratinov
and Roth 2009; Redman, Sammons, and Roth 2016)).
• Tree, a tree representation connecting spans of text
for this we used Dependency of Chang et

as nodes;
al. (2015).

• Cluster, or spans of text clustered in groups. An example

is Coreference (Lee et al. 2011).

• PredArg;

for this view we used Verb-SRL and Nom-
SRL(Punyakanok, Roth, and Yih 2008; Roth and Lapata
2016), Prep-SRL (Srikumar and Roth 2013), Comma-SRL
(Arivazhagan, Christodoulopoulos, and Roth 2016).

Given SemanticGraph generators we have the question,
answers and paragraph represented as a collection of graphs.

3Available at: http://github.com/CogComp/cogcomp-nlp

where v(g1) × v(g2) = {(v, w); v ∈ v(g1), w ∈ v(g2)} .

Sem. Graph

PredArg

Cluster
Tree
SpanLa-
belView

Property
Use at least (a) a predicate and its
argument, or (b) two arguments
Use at least two nodes
Use two nodes with distance less than k

Use at least k nodes

Table 1: Minimum requirements for using each family of
graphs. Each graph connected component (e.g. a PredArg
frame, or a Coreference chain) cannot be used unless the
above-mentioned conditioned is satisﬁed.

1. G is connected.
2. G has intersection with the question, the knowledge, and

exactly one answer candidate:6

G ∩ K(Q) (cid:54)= ∅, G ∩ K(P ) (cid:54)= ∅, ∃! i : G ∩ K(Ai) (cid:54)= ∅

3. G satisﬁes structural properties per each connected com-

ponent, as summarized in Table 1.
Deﬁnition 2 characterizes what we call a potential solu-
tion to a question. A given question and paragraph give rise
to a large number of possible support graphs. We deﬁne the
space of feasible support graphs as G (i.e., all the graphs that
satisfy Deﬁnition 2, for a given (Q, {Ai} , P )). To rank var-
ious feasible support graphs in such a large space, we deﬁne
a scoring function score(G) as:

(cid:88)

(cid:88)

w(v) +

w(e) −

wc 1{c is violated} (1)

v∈v(G)

e∈e(G)

(cid:88)

c∈C

for some set of preferences (or soft-constraints) C. When c is
violated, denoted by the indicator function 1{c is violated}
in Eq. (1), we penalize the objective value by some ﬁxed
amount wc. The second term is supposed to bring more
sparsity to the desired solutions, just like how regulariza-
tion terms act in machine learning models (Natarajan 1995).
The ﬁrst term is the sum of weights we deﬁned when con-
structing the augmented-graph, and is supposed to give more
weight to the models that have better and more reliable
alignments between its nodes. The role of the inference pro-
cess will be to choose the “best” one under our notion of
desirable support graphs:

G∗ = arg max

score(G)

G∈G

(2)

ILP Formulation
Our QA system, SEMANTICILP, models the above support
graph search of Eq. (2) as an ILP optimization problem, i.e.,
as maximizing a linear objective function over a ﬁnite set of
variables, subject to a set of linear inequality constraints. A
summary of the model is given below.

The augmented graph is not explicitly created; instead, it
is implicitly created. The nodes and edges of the augmented
graph are encoded as a set of binary variables. The value
of the binary variables reﬂects whether a node or an edge

6∃! here denotes the uniqueness quantiﬁer, meaning “there ex-

ists one and only one”.

- Number of sentences used is more than k
- Active edges connected to each chunk of the answer
option, more than k
- More than k chunks in the active answer-option
- More than k edges to each question constituent
- Number of active question-terms
- If using PredArgof K(Q), at least an argument should
be used
- If using PredArg(Verb-SRL) of K(Q), at least one
predicate should be used.

Table 2: The set of preferences functions in the objective.

is used in the optimal graph G∗. The properties listed in
Table 1 are implemented as weighted linear constraints using
the variables deﬁned for the nodes and edges.

As mentioned, edge weights in the augmented graph
come from a function, f , which captures (soft) phrasal en-
tailment between question and paragraph nodes, or para-
graph and answer nodes, to account for lexical variability.
In our evaluations, we use two types of f .
(a) Similar to
Khashabi et al. (2016), we use a WordNet-based (Miller
1995) function to score word-to-word alignments, and use
this as a building block to compute a phrase-level alignment
score as the weighted sum of word-level alignment scores.
Word-level scores are computed using WordNet’s hypernym
and synonym relations, and weighted using relevant word-
sense frequency. f for similarity (as opposed to entailment)
is taken to be the average of the entailment scores in both
directions.
(b) For longer phrasal alignments (e.g., when
aligning phrasal verbs) we use the Paragram system of Wi-
eting et al. (2015).

The ﬁnal optimization is done on Eq. (1). The ﬁrst part
of the objective is the sum of the weights of the sub-graph,
which is what an ILP does, since the nodes and edges are
modeled as variables in the ILP. The second part of Eq. (1)
contains a set of preferences C, summarized in Table 2,
meant to apply soft structural properties that partly depen-
dant on the knowledge instantiation. These preferences are
soft in the sense that they are applied with a weight to the
overall scoring function (as compare to a hard constraint).
For each preference function c there is an associated binary
or integer variable with weight wc, and we create appropri-
ate constraints to simulate the corresponding behavior.

We note that the ILP objective and constraints aren’t tied
to the particular domain of evaluation; they represent gen-
eral properties that capture what constitutes a well supported
answer for a given question.

Empirical Evaluation
We evaluate on two domains that differ in the nature of the
supporting text (concatenated individual sentences vs. a co-
herent paragraph), the underlying reasoning, and the way
questions are framed. We show that SEMANTICILP outper-
forms a variety of baselines, including retrieval-based meth-
ods, neural-networks, structured systems, and the current

best system for each domain. These datasets and systems
are described next, followed by results.

Question Sets

For the ﬁrst domain, we have a collection of question sets
containing elementary-level science questions from stan-
dardized tests (Clark et al. 2016; Khot, Sabharwal, and
Clark 2017). Speciﬁcally, REGENTS 4TH contains all non-
diagram multiple choice questions from 6 years of NY Re-
gents 4th grade science exams (127 train questions, 129
test). REGENTS 8TH similarly contains 8th grade questions
(147 train, 144 test). The corresponding expanded datasets
are AI2PUBLIC 4TH (432 train, 339 test) and AI2PUBLIC
8TH (293 train, 282 test).7

For the second domain, we use the PROCESSBANK8
dataset for the reading comprehension task proposed by Be-
rant et al. (2014).
It contains paragraphs about biologi-
cal processes and two-way multiple choice questions about
them. We used a broad subset of this dataset that asks about
events or about an argument that depends on another event
or argument.9. The resulting dataset has 293 train and 109
test questions, based on 147 biology paragraphs.

Test scores are reported as percentages. For each ques-
tion, a system gets a score of 1 if it chooses the correct an-
swer, 1/k if it reports a k-way tie that includes the correct
answer, and 0 otherwise.

Question Answering Systems

We consider a variety of baselines, including the best system
for each domain.

IR (information retrieval baseline). We use the IR solver
from Clark et al. (2016), which selects the answer option that
has the best matching sentence in a corpus. The sentence is
forced to have a non-stopword overlap with both q and a.

SEMANTICILP (our approach). Given the input instance
(question, answer options, and a paragraph), we invoke var-
ious NLP modules to extract semantic graphs. We then gen-
erate an ILP and solve it using the open source SCIP en-
gine (Achterberg 2009), returning the active answer option
am from the optimal solution found. To check for ties, we
disable am, re-solve the ILP, and compare the score of the
second-best answer, if any, with that of the best score.

For the science question sets, where we don’t have any
paragraphs attached to each question, we create a passage
by using the above IR solver to retrieve scored sentences
for each answer option and then combining the top 8 unique
sentences (across all answer options) to form a paragraph.

While the sub-graph optimization can be done over the
entire augmented graph in one shot, our current implemen-
tation uses multiple simpliﬁed solvers, each performing rea-
soning over augmented graphs for a commonly occurring
annotator combination, as listed in Table 3. For all of these

7AI2 Science Questions V1 at http://data.allenai.org/ai2-

science-questions

8https://nlp.stanford.edu/software/bioprocess
9These are referred to as “dependency questions” by Berant et

al. (2014), and cover around 70% of all questions.

Combination

Representation

Comb-1

Comb-2

Comb-3

Comb-4

Comb-5

K(Q) = {Shallow-Parse, Tokens}
K(P ) = {Shallow-Parse, Tokens, Dependency}
K(Q) = {Verb-SRL, Shallow-Parse}
K(P ) = {Verb-SRL}
K(Q) = {Verb-SRL, Shallow-Parse}
K(P ) = {Verb-SRL, Coreference}
K(Q) = {Verb-SRL, Shallow-Parse}
K(P ) = {Comma-SRL}
K(Q) = {Verb-SRL, Shallow-Parse}
K(P ) = {Prep-SRL}

Table 3: The semantic annotator combinations used in our
implementation of SEMANTICILP.

annotator combinations, we let the representation of the an-
swers be K(A) = {Shallow-Parse, Tokens}.
Importantly,
our choice of working with a few annotator combinations
is mainly for simplicity of implementation and sufﬁces to
demonstrate that reasoning over even just two annotators at a
time can be surprisingly powerful. There is no fundamental
limitation in implementing SEMANTICILP using one single
optimization problem as stated in Eq. (2).

Each simpliﬁed solver associated with an annotator com-
bination in Table 3 produces a conﬁdence score for each an-
swer option. We create an ensemble of these solvers as a
linear combination of these scores, with weights trained us-
ing the union of training data from all questions sets.

BIDAF (neural network baseline). We use the re-
cent deep learning reading comprehension model of Seo et
al. (2016), which is one of the top performing systems on
the SQuAD dataset and has been shown to generalize to an-
other domain as well (Min, Seo, and Hajishirzi 2017). Since
BIDAF was designed for ﬁll-in-the-blank style questions,
we follow the variation used by Kembhavi et al. (2017) to
apply it to our multiple-choice setting. Speciﬁcally, we com-
pare the predicted answer span to each answer candidate and
report the one with the highest similarity.

We use two variants: the original system, BIDAF, pre-
trained on 100,000+ SQuAD questions, as well as an ex-
tended version, BIDAF’, obtained by performing continu-
ous training to ﬁne-tune the SQuAD-trained parameters us-
ing our (smaller) training sets. For the latter, we convert
multiple-choice questions into reading comprehension ques-
tions by generating all possible text-spans within sentences,
with token-length at most correct answer length + 2, and
choose the ones with the highest similarity score with the
correct answer. We use the ALLENNLP re-implementation
of BIDAF10, train it on SQuAD, followed by training it on
our dataset. We tried different variations (epochs and learn-
ing rates) and selected the model which gives the best aver-
age score across all the datasets. As we will see, the variant
that was further trained on our data often gives better results.
TUPLEINF (semi-structured inference baseline). Re-
cently proposed by Khot, Sabharwal, and Clark (2017), this
is a state-of-the-art system designed for science questions.
It uses Open IE (Banko et al. 2007) tuples derived from the

10Available at: https://github.com/allenai/allennlp

text as the knowledge representation, and performs reason-
ing over it via an ILP. It has access to a large knowledge
base of Open IE tuples, and exploits redundancy to over-
come challenges introduced by noise and linguistic variabil-
ity.

PROREAD and SYNTPROX. PROREADis a specialized
and best performing system on the PROCESSBANK ques-
tion set. Berant et al. (2014) annotated the training data with
events and event relations, and trained a system to extract
the process structure. Given a question, PROREAD converts
it into a query (using regular expression patterns and key-
words) and executes it on the process structure as the knowl-
edge base. Its reliance on a question-dependent query gen-
erator and on a process structure extractor makes it difﬁcult
to apply to other domains.

SYNTPROX is another solver suggested by (Berant et al.
2014). It aligns content word lemmas in both the question
and the answer against the paragraph, and select the answer
tokens that are closer to the aligned tokens of the questions.
The distance is measured using dependency tree edges. To
support multiple sentences they connect roots of adjacent
sentences with bidirectional edges.

Experimental Results
We evaluate various QA systems on datasets from the two
domains. The results are summarized below, followed by
some some insights into SEMANTICILP’s behavior and an
error analysis.

Science Exams. The results of experimenting on different
grades’ science exams are summarized in Table 4, which
shows the exam scores as a percentage. The table demon-
strates that SEMANTICILP consistently outperforms the
best baselines in each case by 2%-6%.

Further, there is no absolute winner among the baselines;
while IR is good on the 8th grade questions, TUPLEINF and
BIDAF’ are better on 4th grade questions. This highlights
the differing nature of questions for different grades.

Dataset
REGENTS 4TH
AI2PUBLIC 4TH
REGENTS 8TH
AI2PUBLIC 8TH

BIDAF BIDAF’

56.3
50.7
53.5
47.7

53.1
57.4
62.8
51.9

IR TUPLEINF SEMANTICILP
59.3
54.9
64.2
52.8

67.6
59.7
66.0
55.9

61.4
56.1
61.3
51.6

Table 4: Science test scores as a percentage. On elemen-
tary level science exams, SEMANTICILP consistently out-
performs baselines. In each row, the best score is in bold
and the best baseline is italicized.

Biology Exam. The results on the PROCESSBANK dataset
are summarized in Table 5. While SEMANTICILP’s perfor-
mance is substantially better than most baselines and close to
that of PROREAD, it is important to note that this latter base-
line enjoys additional supervision of domain-speciﬁc event
annotations. This, unlike our other relatively general base-
lines, makes it limited to this dataset, which is also why we
don’t include it in Table 4.

We evaluate IR on this reading comprehension dataset by
creating an ElasticSearch index, containing the sentences of
the knowledge paragraphs.

PROREAD SYNTPROX IR BIDAF BIDAF’ SEMANTICILP

68.1

61.9

63.8

58.7

61.3

67.9

Table 5: Biology test scores as a percentage. SEMANTI-
CILP outperforms various baselines on the PROCESSBANK
dataset and roughly matches the specialized best method.

Error and Timing Analysis

For some insight into the results, we include a brief analysis
of our system’s output compared to that of other systems.

We identify a few main reasons for SEMANTICILP’s er-
rors. Not surprisingly, some mistakes (see Appendix ﬁgure
for an example) can be traced back to failures in generat-
ing proper annotation (SemanticGraph). Improvement in
SRL modules or redundancy can help address this. Some
mistakes are from the current ILP model not supporting the
ideal reasoning, i.e., the requisite knowledge exists in the
annotations, but the reasoning fails to exploit it. Another
group of mistakes is due to the complexity of the sentences,
and the system lacking a way to represent the underlying
phenomena with our current annotators.

A weakness (that doesn’t seem to be particular to our
solver) is reliance on explicit mentions. If there is a meaning
indirectly implied by the context and our annotators are not
able to capture it, our solver will miss such questions. There
will be more room for improvement on such questions with
the development of discourse analysis systems.

When solving the questions that don’t have an attached
paragraph, relevant sentences need to be fetched from a cor-
pus. A subset of mistakes on this dataset occurs because the
extracted knowledge does not contain the correct answer.

ILP Solution Properties. Our system is implemented us-
ing many constraints, requires using many linear inequalities
which get instantiated on each input instanced, hence there
are a different number of variables and inequalities for each
input instance. There is an overhead time for pre-processing
an input instance, and convert it into an instance graph. Here
in the timing analysis we provide we ignore the annotation
time, as it is done by black-boxes outside our solver.

Table 6 summarizes various ILP and support graph statis-
tics for SEMANTICILP, averaged across PROCESSBANK
questions. Next to SEMANTICILP we have included num-
bers from TABLEILP which has similar implementation ma-
chinery, but on a very different representation. While the
size of the model is a function of the input instance, on aver-
age, SEMANTICILP tends to have a bigger model (number
of constraints and variables). The model creation time is
signiﬁcantly time-consuming in SEMANTICILP as involves
many graph traversal operations and jumps between nodes
and edges. We also providing times statistics for TUPLE-
INF which takes roughly half the time of TABLEILP, which
means that it is faster than SEMANTICILP.

Category

Quantity

Avg.
(SEMANTICILP)

Avg.
(TABLEILP)

Avg.
(TUPLEINF)

ILP complexity

Timing stats

#variables
#constraints
model creation
solving the ILP

2254.9
4518.7
5.3 sec
1.8 sec

1043.8
4417.8
1.9 sec
2.1 sec

1691.0
4440.0
1.7 sec
0.3 sec

Table 6: SEMANTICILP statistics averaged across ques-
tions, as compared to TABLEILP and TUPLEINF statistics.

Ablation Study
In order to better understand the results, we ablate the contri-
bution of different annotation combinations, where we drop
different combination from the ensemble model. We retrain
the ensemble, after dropping each combination.

The results are summarized in Table 7. While Comb-1
seems to be important for science tests, it has limited con-
tribution to the biology tests. On 8th grade exams, the Verb-
SRL and Comma-SRL-based alignments provide high value.
Structured combinations (e.g., Verb-SRL-based alignments)
are generally more important for the biology domain.

Full SEMANTICILP
no Comb-1
no Comb-2
no Comb-3
no Comb-4
no Comb-5

AI2PUBLIC 8TH
55.9
-3.1
-2.0
-0.6
-3.1
-0.1

PROCESSBANK
67.9
-1.8
-4.6
-1.8
-1.8
-5.1

Table 7: Ablation study of SEMANTICILP components on
various datasets. The ﬁrst row shows the overall test score
of the full system, while other rows report the change in the
score as a result of dropping an individual combination. The
combinations are listed in Table 3.

Complementarity to IR. Given that in the science do-
main the input snippets fed to SEMANTICILP are retrieved
through a process similar to the IR solver, one might natu-
rally expect some similarity in the predictions. The pie-chart
in Figure 3 shows the overlap between mistakes and correct
predictions of SEMANTICILP and IR on 50 randomly cho-
sen training questions from AI2PUBLIC 4TH. While there
is substantial overlap in questions that both answer correctly
(the yellow slice) and both miss (the red slice), there is also
a signiﬁcant number of questions solved by SEMANTICILP
but not IR (the blue slice), almost twice as much as the ques-
tions solved by IRbut not SEMANTICILP (the green slice).

In Tables 4 and 5, we presented one sin-
Cascade Solvers.
gle instance of SEMANTICILP with state-of-art results on
multiple datasets, where the solver was an ensemble of se-
mantic combinations (presented in Table 3). Here we show a
simpler approach that achieves stronger results on individual
datasets, at the cost of losing a little generalization across do-
mains. Speciﬁcally, we create two “cascades” (i.e., decision
lists) of combinations, where the ordering of combinations
in the cascade is determined by the training set precision of

Figure 3: Overlap of the predictions of SEMANTICILP and
IR on 50 randomly-chosen questions from AI2PUBLIC 4TH.

the simpliﬁed solver representing an annotator combination
(combinations with higher precision appear earlier). One
cascade solver targets science exams and the other the biol-
ogy dataset.

The results are reported in Table 8. On the 8th grade data,
the cascade solver created for science test achieves higher
scores than the generic ensemble solver. Similarly, the cas-
cade solver on the biology domain outperforms the ensem-
ble solver on the PROCESSBANK dataset.

Dataset

Ensemble

e REGENTS 4TH

c
n
e
i
c
S

AI2PUBLIC 4TH
REGENTS 8TH
AI2PUBLIC 8TH
PROCESSBANK

67.6
59.7
66.0
55.9
67.9

Cascade
(Science)
64.7
56.7
69.4
56.5
59.6

Cascade
(Biology)
63.1
55.7
60.3
54.3
68.8

Table 8: Comparison of test scores of SEMANTICILP using
a generic ensemble vs. domain-targeted cascades of annota-
tion combinations.

Effect of Varying Knowledge Length. We analyze the
performance of the system as a function of the length of the
paragraph fed into SEMANTICILP, for 50 randomly selected
training questions from the REGENTS 4TH set. Figure 4
(left) shows the overall system, for two combinations intro-
duced earlier, as a function of knowledge length, counted as
the number of sentences in the paragraph.

As expected, the solver improves with more sentences,
until around 12-15 sentences, after which it starts to worsen
with the addition of more irrelevant knowledge. While the
cascade combinations did not show much generalization
across domains, they have the advantage of a smaller drop
when adding irrelevant knowledge compared to the ensem-
ble solver. This can be explained by the simplicity of cas-
cading and minimal training compared to the ensemble of
annotation combinations.

Figure 4: Performance change for varying knowledge length.

Figure 4 (right) shows the performance of individual com-
binations as a function of knowledge length. It is worth high-
lighting that while Comb-1 (blue) often achieves higher cov-
erage and good scores in simple paragraphs (e.g., science ex-
ams), it is highly sensitive to knowledge length. On the other
hand, highly-constrained combinations have a more consis-
tent performance with increasing knowledge length, at the
cost of lower coverage.

Conclusion

Question answering is challenging in the presence of lin-
guistic richness and diversity, especially when arriving at
the correct answer requires some level of reasoning. De-
parting from the currently popular paradigm of generating
a very large dataset and learning “everything” from it in an
end-to-end fashion, we argue—and demonstrate via our QA
system—that one can successfully leverage pre-trained NLP
modules to extract a sufﬁciently complete linguistic abstrac-
tion of the text that allows answering interesting questions
about it. This approach is particularly valuable in settings
where there is a small amount of data. Instead of exploiting
peculiarities of a large but homogeneous dataset, as many
state-of-the-art QA systems end up doing, we focus on con-
ﬁdently performing certain kinds of reasoning, as captured
by our semantic graphs and the ILP formulation of support
graph search over them. Bringing these ideas to practice,
our system, SEMANTICILP, achieves state-of-the-art perfor-
mance on two domains with very different characteristics,
outperforming both traditional and neural models.

Acknowledgments

The authors would like to thank Peter Clark, Bhavana Dalvi,
Minjoon Seo, Oyvind Tafjord, Niket Tandon, and Luke
Zettlemoyer for valuable discussions and help.

This work is supported by a gift from AI2 and by con-
tract FA8750-13-2-0008 with the US Defense Advanced Re-
search Projects Agency (DARPA). The views expressed are
those of the authors and do not reﬂect the ofﬁcial policy or
position of the U.S. Government.

References
Achterberg, T. 2009. SCIP: solving constraint integer programs.
Math. Prog. Computation 1(1):1–41.
Arivazhagan, N.; Christodoulopoulos, C.; and Roth, D. 2016. La-
beling the semantic roles of commas. In AAAI, 2885–2891.
Baker, C. F.; Fillmore, C. J.; and Lowe, J. B. 1998. The berkeley
framenet project. In ACL, 86–90.
Banarescu, L.; Bonial, C.; Cai, S.; Georgescu, M.; Grifﬁtt, K.;
Hermjakob, U.; Knight, K.; Palmer, M.; and Schneider, N. 2013.
Abstract meaning representation for sembanking. In Linguistic An-
notation Workshop and Interoperability with Discourse.
Banko, M.; Cafarella, M. J.; Soderland, S.; Broadhead, M.; and
Etzioni, O. 2007. Open information extraction from the web. In
IJCAI.
Bar-Haim, R.; Dagan, I.; and Berant, J. 2015. Knowledge-based
J. Artif. Intell.
textual inference via parse-tree transformations.
Res.(JAIR) 54:1–57.
Berant, J.; Srikumar, V.; Chen, P.-C.; Vander Linden, A.; Harding,
B.; Huang, B.; Clark, P.; and Manning, C. D. 2014. Modeling
biological processes for reading comprehension. In EMNLP.
Berant, J.; Dagan, I.; and Goldberger, J. 2010. Global learning of
focused entailment graphs. In ACL, 1220–1229.
Chang, M.-W.; Goldwasser, D.; Roth, D.; and Srikumar, V. 2010.
Discriminative learning over constrained latent representations. In
NAACL, 429–437.
Chang, K.-W.; Upadhyay, S.; Chang, M.-W.; Srikumar, V.; and
Roth, D. 2015. Illinois-SL: A java library for structured prediction.
arXiv preprint arXiv:1509.07179.
Clark, P.; Etzioni, O.; Khot, T.; Sabharwal, A.; Tafjord, O.; Turney,
P.; and Khashabi, D. 2016. Combining retrieval, statistics, and
inference to answer elementary science questions. In AAAI.
Cohen, W. W. 2000. Data integration using similarity joins and
a word-based information representation language. ACM Transac-
tions on Information Systems 18(3):288–321.
Dong, L.; Wei, F.; Zhou, M.; and Xu, K. 2015. Question answering
over Freebase with multi-column convolutional neural networks. In
ACL.
Etzioni, O.; Banko, M.; Soderland, S.; and Weld, D. S. 2008. Open
information extraction from the web. Communications of the ACM
51(12):68–74.
Fei, Z.; Khashabi, D.; Peng, H.; Wu, H.; and Roth, D. 2015.
Illinois-Proﬁler: knowledge schemas at scale. Workshop on Cog-
nitive Knowledge Acquisition and Applications (Cognitum).

Punyakanok, V.; Roth, D.; and Yih, W. 2008. The importance of
syntactic parsing and inference in semantic role labeling. Compu-
tational Linguistics 34(2).
Rajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P.
2016.
SQuAD: 100,000+ questions for machine comprehension of text.
In EMNLP.
Ratinov, L., and Roth, D. 2009. Design challenges and misconcep-
tions in named entity recognition. In CONLL.
Redman, T.; Sammons, M.; and Roth, D. 2016.
Illinois named
entity recognizer: Addendum to Ratinov and Roth ’09 reporting
improved results. Tech Report.
Roth, M., and Lapata, M. 2016. Neural semantic role labeling with
dependency path embeddings. ACL.
Roth, D., and Yih, W. 2004. A linear programming formulation
for global inference in natural language tasks. In CONLL, 1–8.
Roth, D., and Zelenko, D. 1998. Part of speech tagging using a
network of linear separators. In COLING-ACL, 1136–1142.
Roy, S.; Vieira, T.; and Roth, D. 2015. Reasoning about quantities
in natural language. TACL 3.
Sch¨uller, P. 2014. Tackling winograd schemas by formalizing rele-
vance theory in knowledge graphs. In International Conference on
the Principles of Knowledge Representation and Reasoning.
Seo, M.; Kembhavi, A.; Farhadi, A.; and Hajishirzi, H. 2016.
Bidirectional attention ﬂow for machine comprehension. ICLR.
Shen, D., and Lapata, M. 2007. Using semantic roles to improve
question answering. In EMNLP-CoNLL, 12–21.
Srikumar, V., and Roth, D. 2011. A joint model for extended
semantic role labeling. In EMNLP.
Srikumar, V., and Roth, D. 2013. Modeling semantic relations
expressed by prepositions. TACL 1:231–242.
Stern, A.; Stern, R.; Dagan, I.; and Felner, A. 2012. Efﬁcient
search for transformation-based inference. In ACL, 283–291.
Wang, C.; Xue, N.; Pradhan, S.; and Pradhan, S.
2015. A
transition-based algorithm for amr parsing. In HLT-NAACL, 366–
375.
Wieting, J.; Bansal, M.; Gimpel, K.; Livescu, K.; and Roth,
D. 2015. From paraphrase database to compositional paraphrase
model and back. TACL 3:345–358.
Zettlemoyer, L. S., and Collins, M. 2005. Learning to map sen-
tences to logical form: Structured classiﬁcation with probabilistic
categorial grammars. UAI.

Gildea, D., and Jurafsky, D. 2002. Automatic labeling of semantic
roles. Computational linguistics 28(3):245–288.
Goldwasser, D., and Roth, D. 2011. Learning from natural instruc-
tions. In IJCAI.
Jia, P., and Liang, P. 2017. Adversarial examples for evaluating
reading comprehension systems. EMNLP.
Kaisser, M., and Webber, B. 2007. Question answering based on
semantic roles. In Proceedings of the workshop on deep linguistic
processing, 41–48.
Kembhavi, A.; Seo, M.; Schwenk, D.; Choi, J.; Farhadi, A.; and
Hajishirzi, H. 2017. Are you smarter than a sixth grader? text-
book question answering for multimodal machine comprehension.
CVPR.
Khashabi, D.; Khot, T.; Sabharwal, A.; Clark, P.; Etzioni, O.; and
Roth, D. 2016. Question answering via integer programming over
semi-structured knowledge. In IJCAI.
Khot, T.; Sabharwal, A.; and Clark, P. 2017. Answering complex
questions using open information extraction. ACL.
Kingsbury, P., and Palmer, M. 2002. From treebank to propbank.
In LREC, 1989–1993.
Krishnamurthy, J.; Tafjord, O.; and Kembhavi, A. 2016. Semantic
parsing to probabilistic programs for situated question answering.
EMNLP.
Lee, H.; Peirsman, Y.; Chang, A.; Chambers, N.; Surdeanu, M.;
and Jurafsky, D. 2011. Stanford’s multi-pass sieve coreference
resolution system at the conll-2011 shared task. In CONLL Shared
Task, 28–34.
Lee, H.; Chang, A.; Peirsman, Y.; Chambers, N.; Surdeanu, M.;
and Jurafsky, D. 2013. Deterministic coreference resolution based
on entity-centric, precision-ranked rules. Computational Linguis-
tics 39(4):885–916.
Meyers, A.; Reeves, R.; Macleod, C.; Szekely, R.; Zielinska, V.;
Young, B.; and Grishman, R. 2004. The nombank project: An
interim report. In HLT-NAACL 2004 workshop: Frontiers in corpus
annotation, volume 24, 31.
Miller, G. 1995. Wordnet: a lexical database for english. Commu-
nications of the ACM 38(11):39–41.
Min, S.; Seo, M. J.; and Hajishirzi, H. 2017. Question answering
through transfer learning from large ﬁne-grained supervision data.
In ACL.
Moreda, P.; Llorens, H.; Bor´o, E. S.; and Palomar, M. 2011. Com-
bining semantic information in question answering systems. Inf.
Process. Manage. 47:870–885.
Natarajan, B. K. 1995. Sparse approximate solutions to linear
systems. SIAM journal on computing 24(2):227–234.
Palmer, M.; Gildea, D.; and Kingsbury, P. 2005. The proposi-
tion bank: An annotated corpus of semantic roles. Computational
linguistics 31(1):71–106.
Parikh, A. P.; T¨ackstr¨om, O.; Das, D.; and Uszkoreit, J. 2016. A
decomposable attention model for natural language inference. In
EMNLP.
Pizzato, L. A., and Moll´a, D. 2008. Indexing on semantic roles for
question answering. In 2nd workshop on Information Retrieval for
Question Answering, 74–81.
Punyakanok, V., and Roth, D. 2001. The use of classiﬁers in se-
quential inference. In NIPS, 995–1001. MIT Press.
Punyakanok, V.; Roth, D.; and Yih, W. 2004. Mapping dependen-
cies trees: An application to question answering. AIM.

Appendix: Reasoning Formulation
Here we provide some details of our reasoning formulation
and its implementation as an ILP.

The support graph search for QA is modeled as an ILP
optimization problem, i.e., as maximizing a linear objective
function over a ﬁnite set of variables, subject to a set of lin-
ear inequality constraints. We note that the ILP objective
and constraints aren’t tied to the particular domain of eval-
uation; they represent general properties that capture what
constitutes a well-supported answer for a given question.

Our formulation consists of multiple kinds of reasoning,
encapsulating our semantic understanding of the types of
knowledge (e.g., verbs, corefs, etc.) extracted from the text,
the family to graphs used to represent them, and how they
interact in order to provide support for an answer candidate.
Each kind of reasoning is added to a general body, deﬁned
in Table 14, that is shared among different reasoning types.
This general body encapsulates basic requirements such as
at most one answer candidate being chosen in the resulting
support graph.

In what follows we delineate various forms of reasoning,
capturing different semantic abstractions and valid interac-
tions between them.

Comb-1 (Shallow Alignment) This reasoning captures
the least-constrained formulation for answering questions.
We create alignments between Tokens view of the question,
and spans of Shallow-Parse view of the paragraph. Align-
ments are scored with the entailment module; the closer the
surface strings are, the higher score their edge gets. There
are variables to induce sparsity in the output; for example,
penalty variables for using too many sentences in the para-
graph, or using too many spans in the paragraph. To Add
more structure to this, we use Coreference and Dependency
views of the paragraph; for example, alignments that are
closer to each other according the dependency parse, get
a higher score. The Coreference links let the reasoning to
jump in between sentences. Figure 7 shows an example out-
put of this type of reasoning. As it can be seen, the alignment
is using multiple terms in the questions and multiple spans
in the paragraph, and the spans belong to one single sentence
in the paragraph.

Comb-2 (Verb-SRL Alignment) This reasoning is using
Verb-SRL views in both question and paragraph. The core
of this alignment is creating connections between the pred-
icate/arguments and the predicate/arguments of the para-
graph, respectively. The edges are scored with our entail-
ment system; hence the bigger the edge weight is, the higher
chance of appearing in the output. An example outputs are
in Figure 7.

Comb-5 (Verb-SRL+Prep-SRL Alignment)
In this type
of reasoning we observe the combination of Verb-SRL and
Prep-SRL. This can be considered as an extension of Comb-
2 (Verb-SRL alignment), we the alignment is in between
Verb-SRL in question, Verb-SRL and Prep-SRL in the para-
graph. The arguments of the Verb-SRLin the question are
aligned to the arguments of either Verb-SRL and Prep-SRL
in the paragraph. Predicates of the Verb-SRL are aligned to

Similar

the predicates Verb-SRL. An example output prediction is
shown in Figure 7.
to the
different

can
as Comb-
identify
3(Verb-SRL+Coreference Alignment),
Comb-4(Verb-
SRL+Comma-SRL Alignment) or Verb-SRL+Nom-SRL
Alignment.

combinations
combinations,

explained,
such

one

* Variables:

Active answer option variable
Active answer option chunk variable

* Active variable constraints:

If any edge connected to any active variable that belongs to
answer option, the answer should be active
If an answer is active, there must be at least one active edge
connected to it.

* Consistency constraints:

Only one answer option should be active.

Table 9: Generic template used as part of each reasoning

* Basic variables:
Active question-terms
Active paragraph-spans
Active paragraph sentence
Question-term to paragraph-span alignment variable
Paragraph-span alignment to answer-option term
alignment variable

* Active variable constraints:

Question-term should be active, if any edge connected to it
is active.
If a question-term is active, at least one edge connected to
it should be active.
Sentence should be active, if anything connected to it is
active.
If a sentence is active, at least one incoming edge to one of
its terms/spans should be active.

* Question sparsity-inducing variables:

More than k active question-term penalty. (for k = 1, 2, 3)
More than k active alignments to each question-term
penalty. (for k = 1, 2, 3)

* Paragraph sparsity-inducing variables:

- Active sentence penalty variable.
* Proximity-inducing variables:

Active dependency-edge boost variable: if two variables
are connected in dependency path, and are both active, this
variable can be active.
- Word-pair distance ≤ k words boost: variable between
any two word-pair with distance less than k and active if
both ends are active. (for k = 1, 2, 3)

* Sparsity-inducing variables in answer options:

- More than k active chunks in the active answer-option.
(for k = 1, 2, 3)
- More than k active edges connected to each chunk of the
active answer option. (for k = 1, 2, 3)

Table 10: Comb-1 (Shallow Alignment)

Figure 5: Examples of system output for (1) top: Comb-1 (Shallow alignment) (2) middle: Comb-2 (Verb-SRL alignment) (3)
bottom: Comb-5 (Verb-SRL+ Prep-SRL alignment).

* Variables:

Active Verb-SRL constituents in question (both predicates
and arguments), 0.01.
Active Verb-SRL constituents in the paragraph (including
predicates and arguments), with weight 0.01.
Edge between question-Verb-SRL-argument and
paragraph-Verb-SRL-argument (if the entailment score
> 0.6).
Edge between question-Verb-SRL-predicate and
paragraph-Verb-SRL-predicate (if the entailment score
> 0.6).
Edge between paragraph-Verb-SRL-argument and
answer-options (if the entailment score > 0.6).
Edges between predicate and its arguments argument,
inside each Verb-SRL frame in the paragraph, each edge
with weight 0.01.

* Consistency constraints:

Constraints to take care of active variables (e.g. edge
variable is active iff its two nodes are active).
At least k = 2 constituents in the question should be
active.
At least k = 1 predicates in the question should be active.
At most k = 1 predicates in the paragraph should be
active.
For each Verb-SRL frame in the paragraph, if the predicate
is inactive, its arguments should be inactive as well.

Table 11: Comb-2 (Verb-SRL alignment)

Figure 6: Example output of a question SEMANTICILP answered incorrectly due to a mistake in annotations. “eating” in the
paragraph is incorrectly recognized as a verb predicate, with “breathing” as subject, resulting in this incorrect alignment.

* Variables:

Active Verb-SRL constituents in question (both predicates
and arguments), with weight 0.001.
Active Verb-SRL constituents in the paragraph (including
predicates and arguments), with weight 0.001.
Active Prep-SRL constituents in the paragraph (including
predicates and arguments), with weight 0.001.
Edges between any pair of Prep-SRL arguments in the
paragraph and Verb-SRL arguments, with weights
extracted from PARAGRAM(if these scores are > 0.7).
Edges between predicates and arguments of the Prep-SRL
frames, with weight 0.02.
Edges between predicates and arguments of the Verb-SRL
frames, with weight 0.01.
Edge between question-Verb-SRL-argument and
paragraph coref-constituents (if cell-cell entailment score
> 0.6)
Edge between question-Verb-SRL-predicate and
paragraph Verb-SRL-predicate (if phrase-sim score > 0.5)
Edge between paragraph Verb-SRL-arguments and answer
options (if cell-cell score is > 0.7)

* Consistency constraints:

Each Prep-SRL argument has to have at least an incoming
edge (in addition to the edge from its predicate)
Not possible to have a Verb-SRL argument (in the
paragraph)
connected to two Prep-SRL arguments of the same frame
(no loop)
Exactly one Prep-SRL predicate in the paragraph
At least one Verb-SRL predicate in the paragraph
At most one Verb-SRL predicate in the paragraph
Not more than one argument of a frame can be connected
to the answer option
Each Verb-SRL argument in the paragraph should have at
least two active edges connected to.

Table 12: Comb-5 (Verb-SRL+Prep-SRLalignment)

* Variables:

Active Verb-SRL constituents in question (including
predicates and arguments), with weight 0.001.
Active Verb-SRL constituents in the paragraph (including
predicates and arguments), with weight 0.001.
Active Coreference constituents in the paragraph, with
weight 0.001.
Active Coreference chains in the paragraph, with weight
−0.0001.
Edges between any pair of Coreference-constituent in the
paragraph that belong to the same Coreference chain,
with weight 0.02.
Edge between question-Verb-SRL-argument and
paragraph Coreference-constituents (if entailment score
> 0.6)
Edge between question-Verb-SRL-predicate and
paragraph Verb-SRL-predicate (if phrase-sim score > 0.4)
Edge between paragraph Verb-SRL arguments and answer
options (if symmetric entailment score is > 0.65)

* Consistency constraints:

Constraints to take care of active variables (e.g. edge
variable is active iff its two nodes are active).
At most k = 1 Coreference chain in the paragraph.
At least k = 1 constituents in the question should be
active.
At most k = 1 Verb-SRL predicates in the paragraph.
(could relax this and have alignment between multiple
SRL frames in the paragraph)
The question constituents can be active, only if at least one
of the edges connected to it is active.
Paragraph Verb-SRL-predicate should have at least two
active edges.
If paragraph Verb-SRL-predicate is inactive, the whole
frame should be inactive.

Table 13: Comb-3 (Verb-SRL+Coreference alignment)

Appendix: Reasoning Formulation
Here we provide some details of our reasoning formulation
and its implementation as an ILP.

The support graph search for QA is modeled as an ILP
optimization problem, i.e., as maximizing a linear objective
function over a ﬁnite set of variables, subject to a set of lin-
ear inequality constraints. We note that the ILP objective
and constraints aren’t tied to the particular domain of eval-
uation; they represent general properties that capture what
constitutes a well-supported answer for a given question.

Our formulation consists of multiple kinds of reasoning,
encapsulating our semantic understanding of the types of
knowledge (e.g., verbs, corefs, etc.) extracted from the text,
the family to graphs used to represent them, and how they
interact in order to provide support for an answer candidate.
Each kind of reasoning is added to a general body, deﬁned
in Table 14, that is shared among different reasoning types.
This general body encapsulates basic requirements such as
at most one answer candidate being chosen in the resulting
support graph.

In what follows we delineate various forms of reasoning,
capturing different semantic abstractions and valid interac-
tions between them.

Comb-1 (Shallow Alignment) This reasoning captures
the least-constrained formulation for answering questions.
We create alignments between Tokens view of the question,
and spans of Shallow-Parse view of the paragraph. Align-
ments are scored with the entailment module; the closer the
surface strings are, the higher score their edge gets. There
are variables to induce sparsity in the output; for example,
penalty variables for using too many sentences in the para-
graph, or using too many spans in the paragraph. To Add
more structure to this, we use Coreference and Dependency
views of the paragraph; for example, alignments that are
closer to each other according the dependency parse, get
a higher score. The Coreference links let the reasoning to
jump in between sentences. Figure 7 shows an example out-
put of this type of reasoning. As it can be seen, the alignment
is using multiple terms in the questions and multiple spans
in the paragraph, and the spans belong to one single sentence
in the paragraph.

Comb-2 (Verb-SRL Alignment) This reasoning is using
Verb-SRL views in both question and paragraph. The core
of this alignment is creating connections between the pred-
icate/arguments and the predicate/arguments of the para-
graph, respectively. The edges are scored with our entail-
ment system; hence the bigger the edge weight is, the higher
chance of appearing in the output. An example outputs are
in Figure 7.

Comb-5 (Verb-SRL+Prep-SRL Alignment)
In this type
of reasoning we observe the combination of Verb-SRL and
Prep-SRL. This can be considered as an extension of Comb-
2 (Verb-SRL alignment), we the alignment is in between
Verb-SRL in question, Verb-SRL and Prep-SRL in the para-
graph. The arguments of the Verb-SRLin the question are
aligned to the arguments of either Verb-SRL and Prep-SRL
in the paragraph. Predicates of the Verb-SRL are aligned to

Similar

the predicates Verb-SRL. An example output prediction is
shown in Figure 7.
to the
different

can
as Comb-
identify
3(Verb-SRL+Coreference Alignment),
Comb-4(Verb-
SRL+Comma-SRL Alignment) or Verb-SRL+Nom-SRL
Alignment.

combinations
combinations,

explained,
such

one

* Variables:

Active answer option variable
Active answer option chunk variable

* Active variable constraints:

If any edge connected to any active variable that belongs to
answer option, the answer should be active
If an answer is active, there must be at least one active edge
connected to it.

* Consistency constraints:

Only one answer option should be active.

Table 14: Generic template used as part of each reasoning

* Basic variables:
Active question-terms
Active paragraph-spans
Active paragraph sentence
Question-term to paragraph-span alignment variable
Paragraph-span alignment to answer-option term
alignment variable

* Active variable constraints:

Question-term should be active, if any edge connected to it
is active.
If a question-term is active, at least one edge connected to
it should be active.
Sentence should be active, if anything connected to it is
active.
If a sentence is active, at least one incoming edge to one of
its terms/spans should be active.

* Question sparsity-inducing variables:

More than k active question-term penalty. (for k = 1, 2, 3)
More than k active alignments to each question-term
penalty. (for k = 1, 2, 3)

* Paragraph sparsity-inducing variables:

- Active sentence penalty variable.
* Proximity-inducing variables:

Active dependency-edge boost variable: if two variables
are connected in dependency path, and are both active, this
variable can be active.
- Word-pair distance ≤ k words boost: variable between
any two word-pair with distance less than k and active if
both ends are active. (for k = 1, 2, 3)

* Sparsity-inducing variables in answer options:

- More than k active chunks in the active answer-option.
(for k = 1, 2, 3)
- More than k active edges connected to each chunk of the
active answer option. (for k = 1, 2, 3)

Table 15: Comb-1 (Shallow Alignment)

Figure 7: Examples of system output for (1) top: Comb-1 (Shallow alignment) (2) middle: Comb-2 (Verb-SRL alignment) (3)
bottom: Comb-5 (Verb-SRL+ Prep-SRL alignment).

* Variables:

Active Verb-SRL constituents in question (both predicates
and arguments), 0.01.
Active Verb-SRL constituents in the paragraph (including
predicates and arguments), with weight 0.01.
Edge between question-Verb-SRL-argument and
paragraph-Verb-SRL-argument (if the entailment score
> 0.6).
Edge between question-Verb-SRL-predicate and
paragraph-Verb-SRL-predicate (if the entailment score
> 0.6).
Edge between paragraph-Verb-SRL-argument and
answer-options (if the entailment score > 0.6).
Edges between predicate and its arguments argument,
inside each Verb-SRL frame in the paragraph, each edge
with weight 0.01.

* Consistency constraints:

Constraints to take care of active variables (e.g. edge
variable is active iff its two nodes are active).
At least k = 2 constituents in the question should be
active.
At least k = 1 predicates in the question should be active.
At most k = 1 predicates in the paragraph should be
active.
For each Verb-SRL frame in the paragraph, if the predicate
is inactive, its arguments should be inactive as well.

Table 16: Comb-2 (Verb-SRL alignment)

Figure 8: Example output of a question SEMANTICILP answered incorrectly due to a mistake in annotations. “eating” in the
paragraph is incorrectly recognized as a verb predicate, with “breathing” as subject, resulting in this incorrect alignment.

* Variables:

Active Verb-SRL constituents in question (both predicates
and arguments), with weight 0.001.
Active Verb-SRL constituents in the paragraph (including
predicates and arguments), with weight 0.001.
Active Prep-SRL constituents in the paragraph (including
predicates and arguments), with weight 0.001.
Edges between any pair of Prep-SRL arguments in the
paragraph and Verb-SRL arguments, with weights
extracted from PARAGRAM(if these scores are > 0.7).
Edges between predicates and arguments of the Prep-SRL
frames, with weight 0.02.
Edges between predicates and arguments of the Verb-SRL
frames, with weight 0.01.
Edge between question-Verb-SRL-argument and
paragraph coref-constituents (if cell-cell entailment score
> 0.6)
Edge between question-Verb-SRL-predicate and
paragraph Verb-SRL-predicate (if phrase-sim score > 0.5)
Edge between paragraph Verb-SRL-arguments and answer
options (if cell-cell score is > 0.7)

* Consistency constraints:

Each Prep-SRL argument has to have at least an incoming
edge (in addition to the edge from its predicate)
Not possible to have a Verb-SRL argument (in the
paragraph)
connected to two Prep-SRL arguments of the same frame
(no loop)
Exactly one Prep-SRL predicate in the paragraph
At least one Verb-SRL predicate in the paragraph
At most one Verb-SRL predicate in the paragraph
Not more than one argument of a frame can be connected
to the answer option
Each Verb-SRL argument in the paragraph should have at
least two active edges connected to.

Table 17: Comb-5 (Verb-SRL+Prep-SRLalignment)

* Variables:

Active Verb-SRL constituents in question (including
predicates and arguments), with weight 0.001.
Active Verb-SRL constituents in the paragraph (including
predicates and arguments), with weight 0.001.
Active Coreference constituents in the paragraph, with
weight 0.001.
Active Coreference chains in the paragraph, with weight
−0.0001.
Edges between any pair of Coreference-constituent in the
paragraph that belong to the same Coreference chain,
with weight 0.02.
Edge between question-Verb-SRL-argument and
paragraph Coreference-constituents (if entailment score
> 0.6)
Edge between question-Verb-SRL-predicate and
paragraph Verb-SRL-predicate (if phrase-sim score > 0.4)
Edge between paragraph Verb-SRL arguments and answer
options (if symmetric entailment score is > 0.65)

* Consistency constraints:

Constraints to take care of active variables (e.g. edge
variable is active iff its two nodes are active).
At most k = 1 Coreference chain in the paragraph.
At least k = 1 constituents in the question should be
active.
At most k = 1 Verb-SRL predicates in the paragraph.
(could relax this and have alignment between multiple
SRL frames in the paragraph)
The question constituents can be active, only if at least one
of the edges connected to it is active.
Paragraph Verb-SRL-predicate should have at least two
active edges.
If paragraph Verb-SRL-predicate is inactive, the whole
frame should be inactive.

Table 18: Comb-3 (Verb-SRL+Coreference alignment)

