LIUM-CVC Submissions for WMT17 Multimodal Translation Task

Ozan Caglayan†, Walid Aransa, Adrien Bardet, Mercedes Garc´ıa-Mart´ınez,
Fethi Bougares, Lo¨ıc Barrault
LIUM, University of Le Mans
†ozancag@gmail.com
FirstName.LastName@univ-lemans.fr

Marc Masana, Luis Herranz and Joost van de Weijer
CVC, Universitat Autonoma de Barcelona
{joost,mmasana,lherranz}@cvc.uab.es

7
1
0
2
 
l
u
J
 
4
1
 
 
]
L
C
.
s
c
[
 
 
1
v
1
8
4
4
0
.
7
0
7
1
:
v
i
X
r
a

Abstract

This paper describes the monomodal and
multimodal Neural Machine Translation
systems developed by LIUM and CVC
for WMT17 Shared Task on Multimodal
Translation. We mainly explored two mul-
timodal architectures where either global
visual features or convolutional feature
maps are integrated in order to beneﬁt
from visual context. Our ﬁnal systems
ranked ﬁrst for both En→De and En→Fr
language pairs according to the automatic
evaluation metrics METEOR and BLEU.

1

Introduction

With the recent advances in deep learning, purely
neural approaches to machine translation, such as
Neural Machine Translation (NMT), (Sutskever
et al., 2014; Bahdanau et al., 2014) have received
a lot of attention because of their competitive per-
formance (Toral and S´anchez-Cartagena, 2017).
Another reason for the popularity of NMT is its
ﬂexible nature allowing researchers to fuse auxil-
iary information sources in order to design sophis-
ticated networks like multi-task, multi-way and
multi-lingual systems to name a few (Luong et al.,
2015; Johnson et al., 2016; Firat et al., 2017).

Multimodal Machine Translation (MMT) aims
to achieve better translation performance by vi-
sually grounding the textual representations. Re-
cently, a new shared task on Multimodal Machine
Translation and Crosslingual Image Captioning
(CIC) was proposed along with WMT16 (Specia
et al., 2016). In this paper, we present MMT sys-
tems jointly designed by LIUM and CVC for the
second edition of this task within WMT17.

Last year we proposed a multimodal atten-
tion mechanism where two different attention dis-
tributions were estimated over textual and im-
age representations using shared transformations
(Caglayan et al., 2016a). More speciﬁcally, convo-
lutional feature maps extracted from a ResNet-50
CNN (He et al., 2016) pre-trained on the ImageNet
classiﬁcation task (Russakovsky et al., 2015) were
used to represent visual information. Although our
submission ranked ﬁrst among multimodal sys-
tems for CIC task, it was not able to improve
over purely textual NMT baselines in neither tasks
(Specia et al., 2016). The winning submission for
MMT (Caglayan et al., 2016a) was a phrase-based
MT system rescored using a language model en-
riched with FC7 global visual features extracted
from a pre-trained VGG-19 CNN (Simonyan and
Zisserman, 2014).

State-of-the-art

results were obtained after
WMT16 by using a separate attention mecha-
nism for different modalities in the context of CIC
(Caglayan et al., 2016b) and MMT (Calixto et al.,
2017a). Besides experimenting with multimodal
attention, Calixto et al. (2017a) and Libovick´y and
Helcl (2017) also proposed a gating extension in-
spired from Xu et al. (2015) which is believed to
allow the decoder to learn when to attend to a
particular modality although Libovick´y and Helcl
(2017) report no improvement over baseline NMT.

There have also been attempts to beneﬁt from
different types of visual information instead of
relying on features extracted from a CNN pre-
trained on ImageNet. One such study from Huang
et al. (2016) extended the sequence of source em-
beddings consumed by the RNN with several re-
gional features extracted from a region-proposal

network (Ren et al., 2015). The architecture thus
predicts a single attention distribution over a se-
quence of mixed-modality representations leading
to signiﬁcant improvement over their NMT base-
line.

More recently, a radically different multi-task
architecture called Imagination (Elliott and K´ad´ar,
2017) is proposed to learn visually grounded rep-
resentations by sharing an encoder between two
tasks: a classical encoder-decoder NMT and a
visual feature reconstruction using as input the
source sentence representation.

This year, we experiment1 with both con-
volutional and global visual vectors provided
by the organizers to better exploit multimodal-
ity (Section 3). Data preprocessing for both
English→{German,French} and training hyper-
parameters are detailed respectively in Section 2
and Section 4. The results based on automatic
evaluation metrics are reported in Section 5. The
paper ends with a discussion in Section 6.

2 Data

and

We use the Multi30k (Elliott et al., 2016) dataset
provided by the organizers which contains 29000,
1000 English→{German,French}
1014
training,
image-caption pairs respectively for
validation and Test2016 (the ofﬁcial evaluation set
of WMT16 campaign) set. Following task rules
we normalized punctuations, applied tokenization
and lowercasing. A Byte Pair Encoding (BPE)
model (Sennrich et al., 2016) with 10K merge
operations is learned for each language pair result-
ing in 5234→7052 tokens for English→German
and 5945→6547 tokens for English→French
respectively.

We report results on Flickr Test2017 set con-
taining 1000 image-caption pairs and the optional
MSCOCO test set of 461 image-caption pairs
which is considered as an out-of-domain set with
ambiguous verbs.

Image Features We experimented with several
types of visual representation using deep fea-
tures extracted from convolutional neural net-
works (CNN) trained on large visual datasets. Fol-
lowing the current state-of-the-art in visual repre-
sentation, we used a network with the ResNet-50

1A detailed tutorial for reproducing the results of this pa-
per is provided at https://github.com/lium-lst/
wmt17-mmt.

architecture (He et al., 2016) trained on the Im-
ageNet dataset (Russakovsky et al., 2015) to ex-
tract two types of features: the 2048-dimensional
features from the pool5 layer and the 14x14x1024
features from the res4f relu layer. Note that the
former is a global feature while the latter is a fea-
ture map with roughly localized spatial informa-
tion.

3 Architecture

Our baseline NMT is an attentive encoder-decoder
(Bahdanau et al., 2014) variant with a Conditional
GRU (CGRU) (Firat and Cho, 2016) decoder.

Let us denote source and target sequences X
and Y with respective lengths M and N as follows
where xi and yj are embeddings of dimension E:

X = (x1, . . . , xM )
Y = (y1, . . . , yN )

Encoder Two GRU (Chung et al., 2014) en-
coders with R hidden units each, process the
source sequence X in forward and backward di-
rections. Their hidden states are concatenated to
form a set of source annotations S where each el-
ement si is a vector of dimension C = 2 × R:

S =





GRUForw(

GRUBack(

#»
X)
#»
X)


 ∈ RM ×C

Both encoders are equipped with layer nor-
malization (Ba et al., 2016) where each hidden
unit adaptively normalizes its incoming activa-
tions with a learnable gain and bias.

Decoder A decoder block namely CGRU (two
stacked GRUs where the hidden state of the ﬁrst
GRU is used for attention computation) is used to
estimate a probability distribution over target to-
kens at each decoding step t.

The hidden state h0 of the CGRU is initialized
using a non-linear transformation of the average
source annotation:

h0 = tanh

Winit ·

si

, si ∈ S (1)

(cid:32)

(cid:33)

1
M

M
(cid:88)

i

Attention At each decoding timestep t, an un-
normalized attention score gi is computed for each
source annotation si using the ﬁrst GRU’s hidden
state ht and si itself:
(Wa ∈ RC, Ws ∈ RC×C and Wh ∈ RC×R)

gi = WT

a tanh (Wssi + bs + Whht) + ba

(2)

The context vector ct is a weighted sum of si and
its respective attention probability αi obtained us-
ing a softmax operation over all the unnormalized
scores:

αi = softmax ([g1, g2, . . . , gM ])i

ct =

αisi

M
(cid:88)

i

The ﬁnal hidden state (cid:101)ht is computed by the sec-
ond GRU using the context vector ct and the hid-
den state of the ﬁrst GRU ht.

Output The probability distribution over the tar-
get tokens is conditioned on the previous token
embedding yt−1, the hidden state of the decoder
(cid:101)ht and the context vector ct, the latter two trans-
formed with Wdec and Wctx respectively:

ot = tanh(yt−1 + Wdec(cid:101)ht + Wctxct)

P (yt|yt−1, (cid:101)ht, ct) = softmax(Woot)

3.1 Multimodal NMT

3.1.1 Convolutional Features

The fusion-conv architecture extends the CGRU
decoder to a multimodal decoder (Caglayan et al.,
2016b) where convolutional
feature maps of
14x14x1024 are regarded as 196 spatial annota-
tions s(cid:48)
j of 1024-dimension each. For each spatial
annotation, an unnormalized attention score g(cid:48)
j is
computed (Equation 2) except that the weights and
biases are speciﬁc to the visual modality and thus
not shared with the textual attention:

j = W(cid:48) T
g(cid:48)
a

tanh (cid:0)Ws

(cid:48)s(cid:48)

j + b(cid:48)

s + Wh

(cid:48)ht

(cid:1) + b(cid:48)
a

The visual context vector vt is computed as a
weighted sum of the spatial annotations s(cid:48)
j and
their respective attention probabilities βj:

βj = softmax (cid:0)[g(cid:48)

1, g(cid:48)

2, . . . , g(cid:48)

196](cid:1)

j

vt =

βjs(cid:48)
j

196
(cid:88)

j

The output of the network is now conditioned on a
multimodal context vector which is the concatena-
tion of the original context vector ct and the newly
computed visual context vector vt.

3.1.2 Global pool5 Features
In this section, we present 5 architectures guided
with global 2048-dimensional visual representa-
tion V in different ways. In contrast to the baseline
NMT, the decoder’s hidden state h0 is initialized
with an all-zero vector unless otherwise speciﬁed.

dec-init
ing Equation 1 with the following:

initializes the decoder with V by replac-

h0 = tanh (Wimg · V )

(Calixto et al., 2017b) previously explored a simi-
lar conﬁguration (IMGD) where the decoder is ini-
tialized with the sum of global visual features ex-
tracted from FC7 layer of a pre-trained VGG-19
CNN and the last source annotation.

encdec-init
initializes the bi-directional encoder
and the decoder with V where e0 represents the
initial state of encoder (Note that in the baseline
NMT, e0 is an all-zero vector) :

e0 = h0 = tanh (Wimg · V )

ctx-mul modulates each source annotation si
with V using element-wise multiplication:

si = si (cid:12) tanh (Wimg · V )

trg-mul modulates each target embedding yj
with V using element-wise multiplication:

yj = yj (cid:12) tanh (Wimg · V )

dec-init-ctx-trg-mul
combines the latter two ar-
chitectures with dec-init and uses separate trans-
formation layers for each of them:

h0 = tanh (Wimg · V )
si = si (cid:12) tanh (cid:0)W(cid:48)
yj = yj (cid:12) tanh (cid:0)W(cid:48)(cid:48)

img · V (cid:1)
img · V (cid:1)

4 Training

We use ADAM (Kingma and Ba, 2014) with a
learning rate of 4e−4 and a batch size of 32. All
weights are initialized using Xavier method (Glo-
rot and Bengio, 2010) and the total gradient norm
is clipped to 5 (Pascanu et al., 2013). Dropout
(Srivastava et al., 2014) is enabled after source
embeddings X, source annotations S and pre-
softmax activations ot with dropout probabilities
of (0.3, 0.5, 0.5) respectively. ((0.2, 0.4, 0.4) for

En→De Flickr

# Params

Test2016 (µ ± σ/Ensemble)
METEOR
BLEU

Test2017 (µ ± σ/Ensemble)
METEOR
BLEU

Caglayan et al. (2016a)
Huang et al. (2016)
Calixto et al. (2017a)
Calixto et al. (2017b)
Elliott and K´ad´ar (2017)

-

62.0M 29.2
36.5
213M 36.5
37.3
36.8

-
-

48.5
54.1
55.0
55.1
55.8

Baseline NMT
(D1) fusion-conv
(D2) dec-init-ctx-trg-mul
(D3) dec-init
(D4) encdec-init
(D5) ctx-mul
(D6) trg-mul

4.6M
6.0M
6.3M
5.0M
5.0M
4.6M
4.7M

38.1 ± 0.8 / 40.7
37.0 ± 0.8 / 39.9
38.0 ± 0.9 / 40.2
38.8 ± 0.5 / 41.2
38.2 ± 0.7 / 40.6
38.4 ± 0.3 / 40.4
37.8 ± 0.9 / 41.0

57.3 ± 0.5 / 59.2
57.0 ± 0.3 / 59.1
57.3 ± 0.3 / 59.3
57.5 ± 0.2 / 59.4
57.6 ± 0.3 / 59.5
57.8 ± 0.5 / 59.6
57.7 ± 0.5 / 60.4

30.8 ± 1.0 / 33.2
29.8 ± 0.9 / 32.7
30.9 ± 1.0 / 33.2
31.2 ± 0.7 / 33.4
31.4 ± 0.4 / 33.5
31.1 ± 0.7 / 33.5
30.7 ± 1.0 / 33.4

51.6 ± 0.5 / 53.8
51.2 ± 0.3 / 53.4
51.4 ± 0.3 / 53.7
51.3 ± 0.3 / 53.2
51.9 ± 0.4 / 53.7
51.9 ± 0.2 / 53.8
52.2 ± 0.4 / 54.0

Table 1: Flickr En→De results: underlined METEOR scores are from systems signiﬁcantly different
(p-value ≤ 0.05) than the baseline using the approximate randomization test of multeval for 5 runs. (D6)
is the ofﬁcial submission of LIUM-CVC.

En→Fr.) An L2 regularization term with a fac-
tor of 1e−5 is also applied to avoid overﬁtting un-
less otherwise stated. Finally, we set E=128 and
R=256 (Section 3) respectively for embedding and
GRU dimensions.

All models are implemented and trained with
the nmtpy framework2 (Caglayan et al., 2017)
using Theano v0.9 (Theano Development Team,
2016). Each experiment is repeated with 5 dif-
ferent seeds to mitigate the variance of BLEU
(Papineni et al., 2002) and METEOR (Lavie and
Agarwal, 2007) and to beneﬁt from ensembling.
The training is early stopped if validation set ME-
TEOR does not improve for 10 validations per-
formed per 1000 updates. A beam-search with a
beam size of 12 is used for translation decoding.

All results are computed using multeval (Clark
et al., 2011) with tokenized sentences.

5 Results

5.1 En→De

Table 1 summarizes BLEU and METEOR scores
obtained by our systems. It should be noted that
since we trained each system with 5 different
seeds, we report results obtained by ensembling
5 runs as well as the mean/deviation over these 5
runs. The ﬁnal system to be submitted is selected
based on ensemble Test2016 METEOR.

First of all, multimodal systems which use
global pool5 features generally obtain compara-

2https://github.com/lium-lst/nmtpy

ble scores which are better than the baseline NMT
in contrast to fusion-conv which fails to improve
over it. Our submitted system (D6) achieves an
ensembling score of 60.4 METEOR which is 1.2
better than NMT. Although the improvements are
smaller, (D6) is still the best system on Test2017 in
terms of ensembling/mean METEOR scores. One
interesting point to be stressed at this level is that
in terms of mean BLEU, (D6) performs worse than
baseline on both test sets. Similarly, (D3) which
has the best BLEU on Test2016, is the worst sys-
tem on Test2017 according to METEOR. This is
clearly a discrepancy between these metrics where
an improvement in one does not necessarily yield
an improvement in the other.

En→De

MSCOCO (µ ± σ/Ensemble)
METEOR

BLEU

Baseline NMT

26.4 ± 0.2 / 28.7

46.8 ± 0.7 / 48.9

(D1) fusion-conv
(D2) dec-init-ctx-trg-mul
(D3) dec-init
(D4) encdec-init
(D5) ctx-mul
(D6) trg-mul

25.1 ± 0.7 / 28.0
26.3 ± 0.9 / 28.8
26.8 ± 0.5 / 28.8
27.1 ± 0.9 / 29.4
27.0 ± 0.7 / 29.3
26.4 ± 0.9 / 28.5

46.0 ± 0.6 / 48.0
46.5 ± 0.4 / 48.5
46.5 ± 0.6 / 48.4
47.2 ± 0.6 / 49.2
47.1 ± 0.7 / 48.7
47.4 ± 0.3 / 48.8

Table 2: MSCOCO En→De results:
the best
Flickr system trg-mul (Table 1) has been used for
this submission as well.

For the MSCOCO set no held-out set for model
selection was available. Therefore, we submit-
ted the system (D6) with best METEOR on Flickr
Test2016.

En→Fr

Test2016 (µ ± σ / Ensemble)
METEOR

BLEU

Test2017 (µ ± σ / Ensemble)
METEOR

BLEU

Baseline NMT
(F1) NMT + nol2reg

52.5 ± 0.3 / 54.3
52.6 ± 0.8 / 55.3

69.6 ± 0.1 / 71.3
69.6 ± 0.6 / 71.7

50.4 ± 0.9 / 53.0
50.0 ± 0.9 / 52.5

67.5 ± 0.7 / 69.8
67.6 ± 0.7 / 70.0

(F2) fusion-conv
(F3) dec-init
(F4) ctx-mul
(F5) trg-mul

ens-nmt-7
ens-mmt-6

53.5 ± 0.8 / 56.5
54.5 ± 0.8 / 56.7
54.6 ± 0.8 / 56.7
54.7 ± 0.8 / 56.7

70.4 ± 0.6 / 72.8
71.2 ± 0.4 / 73.0
71.4 ± 0.6 / 73.0
71.3 ± 0.6 / 73.0

51.6 ± 0.9 / 55.5
52.7 ± 0.9 / 55.5
52.6 ± 0.9 / 55.7
52.7 ± 0.9 / 55.5

68.6 ± 0.7 / 71.7
69.4 ± 0.7 / 71.9
69.5 ± 0.7 / 71.9
69.5 ± 0.7 / 71.7

54.6
57.4

71.6
73.6

53.3
55.9

70.1
72.2

Table 3: Flickr En→Fr results: Scores are averages over 5 runs and given with their standard deviation
(σ) and the score obtained by ensembling the 5 runs. ens-nmt-7 and ens-mmt-6 are the submitted ensem-
bles which correspond to the combination of 7 monomodal and 6 multimodal (global pool5) systems,
respectively.

After scoring all the available systems (Table 2)
we observe that (D4) is the best system accord-
ing to ensemble metrics. This can be explained by
the out-of-domain/ambiguous nature of MSCOCO
where best generalization performance on Flickr is
not necessarily transferred to this set.

(D4),

Overall,

(D5) and (D6) are the top
systems according to METEOR on Flickr and
MSCOCO test sets.

5.2 En→Fr

Table 5.1 shows the results of our systems on the
ofﬁcial test set of last year (Test2016) and this
year (test2017). F1 is a variant of the baseline
NMT without L2 regularization. F2 is a multi-
modal system using convolutional feature maps as
visual features while F3 to F5 are multimodal sys-
tems using pool5 global visual features. We note
that all multimodal systems perform better than
monomodal ones.

Compared to the MMT 2016 results, we can see
that the fusion-conv (F2) system with separate at-
tention over both modalities achieve better perfor-
mance than monomodal systems. The results are
further improved by systems F3 to F5 which use
pool5 global visual features. We conjecture that
the way of integrating the global visual features
into these systems does not seem to affect the ﬁnal
results since they all perform equally well on both
test sets.

The submitted systems are presented in the last
two lines of Table 5.1. Since we did not have all
5 runs with different seeds ready by the submis-
sion deadline, heterogeneous ensembles of differ-

ent architectures and different seeds were consid-
ered. ens-nmt-7 (contrastive monomodal submis-
sion) and ens-mmt-6 (primary multimodal submis-
sion) correspond to ensembles of 7 monomodal
and 6 multimodal (pool5) systems respectively.
ens-mmt-6 beneﬁts from the heterogeneity of the
included systems resulting in a slight improvement
of BLEU and METEOR.

En→Fr

MSCOCO (µ ± σ / ensemble)
METEOR

BLEU

Baseline NMT
(F1) NMT + nol2reg

41.2 ±1.2 / 43.3
40.6 ±1.2 / 43.5

61.3 ±0.9 / 63.3
61.1 ±0.9 / 63.7

(F2) fusion-conv
(F3) dec-init
(F4) ctx-mul
(F5) trg-mul

ens-nmt-7
ens-mmt-6

43.2 ±1.2 / 45.9
43.3 ±1.2 / 46.2
43.3 ±1.2 / 45.6
43.5 ±1.2 / 45.5

63.1 ±0.9 / 65.6
63.4 ±0.9 / 66.0
63.4 ±0.9 / 65.4
63.2 ±0.9 / 65.1

43.6
45.9

63.4
65.9

Table 4: MSCOCO En→Fr results: ens-mmt-6,
the best performing ensemble on Test2016 corpus
(see Table 5.1) has been used for this submission
as well.

Results on the ambiguous dataset extracted
from MSCOCO are presented in Table 4. We can
observe a slightly different behaviour compared
to the results in Table 5.1. The systems using
the convolutional features are performing equally
well compared to those using pool5 features. One
should note that no speciﬁc tuning was performed
for this additional task since no speciﬁc validation
data was provided.

6 Conclusion

We have presented the LIUM-CVC systems for
English to German and English to French Multi-
modal Machine Translation evaluation campaign.
Our systems were ranked ﬁrst for both tasks in
terms of automatic metrics. Using the pool5
global visual features resulted in a better perfor-
mance compared to multimodal attention architec-
ture which makes use of convolutional features.
This might be explained by the fact that the atten-
tion mechanism over spatial feature vectors cannot
capture useful information from the extracted fea-
tures maps. Another explanation for this is that
source sentences contain most necessary informa-
tion to produce the translation and the visual con-
tent is only useful to disambiguate a few speciﬁc
cases. We also believe that reducing the number
of parameters aggressively to around 5M allowed
us to avoid overﬁtting leading to better scores in
overall.

Acknowledgments

This work was supported by the French National
Research Agency (ANR) through the CHIST-
ERA M2CR project3, under the contract num-
ber ANR-15-CHR2-0006-01 and by MINECO
through APCIN 2015 under the contract number
PCIN-2015-251.

References

Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
arXiv preprint
ton. 2016. Layer normalization.
arXiv:1607.06450 http://arxiv.org/abs/1607.06450.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Neural machine translation by
CoRR

Bengio. 2014.
jointly learning to align and translate.
abs/1409.0473. http://arxiv.org/abs/1409.0473.

Ozan Caglayan, Walid Aransa, Yaxing Wang,
Marc Masana, Mercedes Garc´ıa-Mart´ınez, Fethi
Bougares, Lo¨ıc Barrault, and Joost van de Wei-
jer. 2016a. Does multimodality help human and
translation and image captioning?
machine for
In Proceedings of
the First Conference on Ma-
chine Translation. Association for Computational
Linguistics, Berlin, Germany, pages 627–633.
http://www.aclweb.org/anthology/W/W16/W16-
2358.pdf.

Ozan Caglayan, Lo¨ıc Barrault, and Fethi Bougares.
Multimodal attention for neural ma-
CoRR abs/1609.03976.

2016b.
chine
http://arxiv.org/abs/1609.03976.

translation.

3http://m2cr.univ-lemans.fr

Ozan Caglayan, Mercedes Garc´ıa-Mart´ınez, Adrien
and
A ﬂexible
for advanced neural machine transla-
arXiv preprint arXiv:1706.00457

Bardet, Walid Aransa, Fethi Bougares,
Lo¨ıc Barrault. 2017.
toolkit
tion systems.
http://arxiv.org/abs/1706.00457.

Nmtpy:

Iacer Calixto, Qun Liu,

and Nick Campbell.
2017a.
for multi-
Doubly-attentive decoder
modal neural machine translation. arXiv preprint
arXiv:1702.01287 http://arxiv.org/abs/1702.01287.

Iacer Calixto, Qun Liu, and Nick Campbell. 2017b.
Incorporating global visual features into attention-
based neural machine translation. arXiv preprint
arXiv:1701.06521 http://arxiv.org/abs/1701.06521.

and Yoshua Bengio. 2014.

Junyoung Chung, C¸ aglar G¨ulc¸ehre, KyungHyun
Cho,
Empirical
evaluation of gated recurrent neural networks
CoRR abs/1412.3555.
on sequence modeling.
http://arxiv.org/abs/1412.3555.

instability.

Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing
for statistical machine translation: Controlling
In Proceedings of
for optimizer
the Association
the 49th Annual Meeting of
Human Lan-
for Computational Linguistics:
guage Technologies:
- Volume
2. Association for Computational Linguistics,
Stroudsburg, PA, USA, HLT ’11, pages 176–181.
http://dl.acm.org/citation.cfm?id=2002736.2002774.

Short Papers

Desmond Elliott, Stella Frank, Khalil Sima’an, and Lu-
cia Specia. 2016. Multi30k: Multilingual english-
In Proceedings of the
german image descriptions.
5th Workshop on Vision and Language. Association
for Computational Linguistics, Berlin, Germany,
pages 70–74.
http://anthology.aclweb.org/W16-
3210.

Desmond Elliott and ´Akos K´ad´ar. 2017.
translation.

nation improves multimodal
abs/1705.04350. http://arxiv.org/abs/1705.04350.

Imagi-
CoRR

Orhan Firat and Kyunghyun Cho. 2016.

gated

ditional
mechanism.
tutorial/blob/master/docs/cgru.pdf.

recurrent

Con-
attention
unit with
http://github.com/nyu-dl/dl4mt-

Orhan Firat, Kyunghyun Cho, Baskaran Sankaran,
Fatos T. Yarman Vural, and Yoshua Bengio. 2017.
Multi-way, multilingual neural machine transla-
Comput. Speech Lang. 45(C):236–252.
tion.
https://doi.org/10.1016/j.csl.2016.10.006.

Xavier Glorot and Yoshua Bengio. 2010. Under-
standing the difﬁculty of training deep feedfor-
ward neural networks. In Proceedings of the Thir-
teenth International Conference on Artiﬁcial Intelli-
gence and Statistics. PMLR, volume 9 of Proceed-
ings of Machine Learning Research, pages 249–256.
http://proceedings.mlr.press/v9/glorot10a.html.

K. He, X. Zhang, S. Ren, and J. Sun. 2016.
image recognition.
learning for
Deep residual
In 2016 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR). pages 770–778.
https://doi.org/10.1109/CVPR.2016.90.

Po-Yao Huang, Frederick Liu, Sz-Rung Shiang,
Attention-
Jean Oh, and Chris Dyer. 2016.
In
based multimodal neural machine translation.
Proceedings of
the First Conference on Ma-
chine Translation. Association for Computational
Linguistics, Berlin, Germany, pages 639–645.
http://www.aclweb.org/anthology/W16-2360.

Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Tho-
rat, Fernanda Vigas, Martin Wattenberg, Greg Cor-
rado, Macduff Hughes, and Jeffrey Dean. 2016.
Google’s multilingual neural machine translation
system: Enabling zero-shot translation. Technical
report, Google. https://arxiv.org/abs/1611.04558.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 http://arxiv.org/abs/1412.6980.

Alon Lavie and Abhaya Agarwal. 2007. Meteor:
An automatic metric for mt evaluation with
high levels of correlation with human judg-
In Proceedings of the Second Workshop
ments.
on Statistical Machine Translation. Associa-
tion for Computational Linguistics,
Strouds-
burg, PA, USA, StatMT ’07, pages 228–231.
http://dl.acm.org/citation.cfm?id=1626355.1626389.

Jindrich Libovick´y and Jindrich Helcl. 2017. At-
sequence-
CoRR abs/1704.06567.

tention
to-sequence learning.
http://arxiv.org/abs/1704.06567.

for multi-source

strategies

Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol
Vinyals, and Lukasz Kaiser. 2015. Multi-task
arXiv preprint
sequence to sequence learning.
arXiv:1511.06114 http://arxiv.org/abs/1511.06114.

Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: A method for au-
tomatic evaluation of machine translation.
In
Proceedings of the 40th Annual Meeting on As-
sociation for Computational Linguistics. Asso-
ciation for Computational Linguistics, Strouds-
burg, PA, USA, ACL ’02,
pages 311–318.
https://doi.org/10.3115/1073083.1073135.

Razvan Pascanu, Tomas Mikolov,

and Yoshua
Bengio. 2013. On the difﬁculty of training re-
In Proceedings of the
current neural networks.
30th International Conference on International
Conference on Machine Learning - Volume 28.
JMLR.org,
ICML’13, pages III–1310–III–1318.
http://dl.acm.org/citation.cfm?id=3042817.3043083.

Shaoqing Ren, Kaiming He, Ross Girshick, and Jian
Sun. 2015. Faster r-cnn: Towards real-time object
In Pro-
detection with region proposal networks.
ceedings of the 28th International Conference on

Neural Information Processing Systems. MIT Press,
Cambridge, MA, USA, NIPS’15, pages 91–99.
http://dl.acm.org/citation.cfm?id=2969239.2969250.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,
Sanjeev Satheesh, Sean Ma, Zhiheng Huang, An-
drej Karpathy, Aditya Khosla, Michael Bernstein,
Alexander C. Berg, and Li Fei-Fei. 2015.
Ima-
geNet Large Scale Visual Recognition Challenge.
International Journal of Computer Vision (IJCV)
https://doi.org/10.1007/s11263-
115(3):211–252.
015-0816-y.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers). Association for
Computational Linguistics, Berlin, Germany, pages
1715–1725. http://www.aclweb.org/anthology/P16-
1162.

Karen Simonyan and Andrew Zisserman. 2014. Very
deep convolutional networks for large-scale im-
arXiv preprint arXiv:1409.1556
age recognition.
http://arxiv.org/abs/1409.1556.

Lucia Specia, Stella Frank, Khalil Sima’an, and
Desmond Elliott. 2016. A shared task on multi-
modal machine translation and crosslingual image
description. In Proceedings of the First Conference
on Machine Translation. Association for Computa-
tional Linguistics, Berlin, Germany, pages 543–553.
http://www.aclweb.org/anthology/W/W16/W16-
2346.

Nitish

Ilya

Srivastava,

Geoffrey
Sutskever,

Alex
Ruslan
Krizhevsky,
A simple
Salakhutdinov. 2014.
way to prevent neural networks from overﬁt-
J. Mach. Learn. Res. 15(1):1929–1958.
ting.
http://dl.acm.org/citation.cfm?id=2627435.2670313.

Hinton,
and

Dropout:

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.
Sequence to sequence learning with
2014.
the 27th
In Proceedings of
neural networks.
International Conference on Neural
Informa-
tion Processing Systems. MIT Press, Cam-
bridge, MA, USA, NIPS’14, pages 3104–3112.
http://dl.acm.org/citation.cfm?id=2969033.2969173.

Theano Development Team. 2016.

Theano: A
Python framework for fast computation of mathe-
matical expressions. arXiv e-prints abs/1605.02688.
http://arxiv.org/abs/1605.02688.

Antonio Toral and V´ıctor M. S´anchez-Cartagena. 2017.
A multifaceted evaluation of neural versus phrase-
based machine translation for 9 language directions.
In Proceedings of the 15th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics: Volume 1, Long Papers. Association for
Computational Linguistics, Valencia, Spain, pages
1063–1073. http://www.aclweb.org/anthology/E17-
1100.

Kelvin Xu,

Jimmy Ba, Ryan Kiros, Kyunghyun
Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel,
Show,
and Yoshua Bengio. 2015.
image caption gen-
attend and tell: Neural
In Proceed-
eration with visual attention.
ings of
the 32nd International Conference on
Machine Learning (ICML-15). JMLR Workshop
and Conference Proceedings, pages 2048–2057.
http://jmlr.org/proceedings/papers/v37/xuc15.pdf.

