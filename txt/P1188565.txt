Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in
Generative Models

Aditya Grover, Manik Dhar, Stefano Ermon
Computer Science Department
Stanford University
{adityag, dmanik, ermon}@cs.stanford.edu

8
1
0
2
 
n
a
J
 
3
 
 
]

G
L
.
s
c
[
 
 
2
v
8
6
8
8
0
.
5
0
7
1
:
v
i
X
r
a

Abstract

Adversarial learning of probabilistic models has recently
emerged as a promising alternative to maximum likelihood.
Implicit models such as generative adversarial networks
(GAN) often generate better samples compared to explicit
models trained by maximum likelihood. Yet, GANs sidestep
the characterization of an explicit density which makes quan-
titative evaluations challenging. To bridge this gap, we pro-
pose Flow-GANs, a generative adversarial network for which
we can perform exact likelihood evaluation, thus support-
ing both adversarial and maximum likelihood training. When
trained adversarially, Flow-GANs generate high-quality sam-
ples but attain extremely poor log-likelihood scores, inferior
even to a mixture model memorizing the training data; the op-
posite is true when trained by maximum likelihood. Results
on MNIST and CIFAR-10 demonstrate that hybrid training
can attain high held-out likelihoods while retaining visual ﬁ-
delity in the generated samples.

1

Introduction

Highly expressive parametric models have enjoyed great
success in supervised learning, where learning objectives
and evaluation metrics are typically well-speciﬁed and easy
to compute. On the other hand, the learning objective for un-
supervised settings is less clear. At a fundamental level, the
idea is to learn a generative model that minimizes some no-
tion of divergence with respect to the data distribution. Min-
imizing the Kullback-Liebler divergence between the data
distribution and the model, for instance, is equivalent to per-
forming maximum likelihood estimation (MLE) on the ob-
served data. Maximum likelihood estimators are asymptoti-
cally statistically efﬁcient, and serve as natural objectives for
learning prescribed generative models (Mohamed and Lak-
shminarayanan 2016).

In contrast, an alternate principle that has recently at-
tracted much attention is based on adversarial learning,
where the objective is to generate data indistinguishable
from the training data. Adversarially learned models such
as generative adversarial networks (GAN; (Goodfellow et
al. 2014)) can sidestep specifying an explicit density for any
data point and belong to the class of implicit generative mod-
els (Diggle and Gratton 1984).

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

The lack of characterization of an explicit density in
GANs is however problematic for two reasons. Several ap-
plication areas of deep generative models rely on density
estimates; for instance, count based exploration strategies
based on density estimation using generative models have
recently achieved state-of-the-art performance on challeng-
ing reinforcement learning environments (Ostrovski et al.
2017). Secondly, it makes the quantitative evaluation of the
generalization performance of such models challenging. The
typical evaluation criteria based on ad-hoc sample quality
metrics (Salimans et al. 2016; Che et al. 2017) do not address
this issue since it is possible to generate good samples by
memorizing the training data, or missing important modes
of the distribution, or both (Theis, Oord, and Bethge 2016).
Alternatively, density estimates based on approximate in-
ference techniques such as annealed importance sampling
(AIS; (Neal 2001; Wu et al. 2017)) and non-parameteric
methods such as kernel density estimation (KDE; (Parzen
1962; Goodfellow et al. 2014)) are computationally slow
and crucially rely on assumptions of a Gaussian observa-
tion model for the likelihood that could lead to misleading
estimates as we shall demonstrate in this paper.

To sidestep the above issues, we propose Flow-GANs,
a generative adversarial network with a normalizing ﬂow
generator. A Flow-GAN generator transforms a prior noise
density into a model density through a sequence of invert-
ible transformations. By using an invertible generator, Flow-
GANs allow us to tractably evaluate exact likelihoods using
the change-of-variables formula and perform exact posterior
inference over the latent variables while still permitting efﬁ-
cient ancestral sampling, desirable properties of any proba-
bilistic model that a typical GAN would not provide.

Using a Flow-GAN, we perform a principled quantitative
comparison of maximum likelihood and adversarial learning
on benchmark datasets viz. MNIST and CIFAR-10. While
adversarial learning outperforms MLE on sample quality
metrics as expected based on strong evidence in prior work,
the log-likelihood estimates of adversarial learning are or-
ders of magnitude worse than those of MLE. The difference
is so stark that a simple Gaussian mixture model baseline
outperforms adversarially learned models on both sample
quality and held-out likelihoods. Our quantitative analysis
reveals that the poor likelihoods of adversarial learning can
be explained as a result of an ill-conditioned Jacobian ma-

trix for the generator function suggesting a mode collapse,
rather than overﬁtting to the training dataset.

To resolve the dichotomy of perceptually good-looking
samples at the expense of held-out likelihoods in the case
of adversarial learning (and vice versa in the case of MLE),
we propose a hybrid objective that bridges implicit and pre-
scribed learning by augmenting the adversarial training ob-
jective with an additional term corresponding to the log-
likelihood of the observed data. While the hybrid objective
achieves the intended effect of smoothly trading-off the two
goals in the case of CIFAR-10, it has a regularizing effect on
MNIST where it outperforms MLE and adversarial learning
on both held-out likelihoods and sample quality metrics.
Overall, this paper makes the following contributions:

1. We propose Flow-GANs, a generative adversarial net-
work with an invertible generator that can perform efﬁ-
cient ancestral sampling and exact likelihood evaluation.

2. We propose a hybrid learning objective for Flow-GANs
that attains good log-likelihoods and generates high-
quality samples on MNIST and CIFAR-10 datasets.

3. We demonstrate the limitations of AIS and KDE for log-
likelihood evaluation and ranking of implicit models.

4. We analyze the singular value distribution for the Jaco-
bian of the generator function to explain the low log-
likelihoods observed due to adversarial learning.

2 Preliminaries
We begin with a review of maximum likelihood estimation
and adversarial learning in the context of generative models.
For ease of presentation, all distributions are w.r.t. any arbi-
trary x ∈ Rd, unless otherwise speciﬁed. We use upper-case
to denote probability distributions and assume they all admit
absolutely continuous densities (denoted by the correspond-
ing lower-case notation) on a reference measure dx.

Consider the following setting for learning generative
models. Given some data X = {xi ∈ Rd}m
i=1 sampled i.i.d.
from an unknown probability density pdata, we are inter-
ested in learning a probability density pθ where θ denotes the
parameters of a model. Given a parameteric family of mod-
els M, the typical approach to learn θ ∈ M is to minimize
a notion of divergence between Pdata and Pθ. The choice of
divergence and the optimization procedure dictate learning,
leading to the following two objectives.

2.1 Maximum likelihood estimation
In maximum likelihood estimation (MLE), we minimize the
Kullback-Liebler (KL) divergence between the data distri-
bution and the model distribution. Formally, the learning ob-
jective can be expressed as:

min
θ∈M

KL(Pdata, Pθ) = Ex∼Pdata

log

(cid:20)

(cid:21)

pdata(x)
pθ(x)

Since pdata is independent of θ, the above optimization
problem can be equivalently expressed as:
Ex∼Pdata [log pθ(x)]

(1)

max
θ∈M

Hence, evaluating the learning objective for MLE in Eq. (1)
requires the ability to evaluate the model density pθ(x).
Models that provide an explicit characterization of the like-
lihood function are referred to as prescribed generative mod-
els (Mohamed and Lakshminarayanan 2016).

2.2 Adversarial learning
A generative model can be learned to optimize divergence
notions beyond the KL divergence. A large family of diver-
gences can be conveniently expressed as:
Ex∼Pθ [hφ(x)] − Ex∼Pdata

φ(x)(cid:3)

(cid:2)h(cid:48)

(2)

max
φ∈F

where F denotes a set of parameters, hφ and h(cid:48)
φ are ap-
propriate real-valued functions parameterized by φ. Differ-
ent choices of F, hφ and h(cid:48)
φ can lead to a variety of f -
divergences such as Jenson-Shannon divergence and inte-
gral probability metrics such as the Wasserstein distance.
For instance, the GAN objective proposed by Goodfellow
et al. (2014) can also be cast in the form of Eq. (2) below:

max
φ∈F

Ex∼Pθ [log (1 − Dφ(x))] + Ex∼Pdata [Dφ(x)]

(3)

where φ denotes the parameters of a neural network function
Dφ. We refer the reader to (Nowozin, Cseke, and Tomioka
2016; Mescheder, Nowozin, and Geiger 2017b) for further
details on other possible choices of divergences. Impor-
tantly, a Monte Carlo estimate of the objective in Eq. (2)
requires only samples from the model. Hence, any model
that allows tractable sampling can be used to evaluate the
following minimax objective:

min
θ∈M

max
φ∈F

Ex∼Pθ [hφ(x)] − Ex∼Pdata

(cid:2)h(cid:48)

φ(x)(cid:3) .

(4)

As a result, even differentiable implicit models which do
not provide a characterization of the model likelihood1 but
allow tractable sampling can be learned adversarially by op-
timizing minimax objectives of the form given in Eq. (4).

2.3 Adversarial learning of latent variable models
From a statistical perspective, maximum likelihood estima-
tors are statistically efﬁcient asymptotically (under some
conditions) and hence minimizing the KL divergence is a
natural objective for many prescribed models (Huber 1967).
However, not all models allow for a well-deﬁned, tractable,
and easy-to-optimize likelihood.

For example, exact likelihood evaluation and sampling are
tractable in directed, fully observed models such as Bayesian
networks and autoregressive models (Larochelle and Mur-
ray 2011; Oord, Kalchbrenner, and Kavukcuoglu 2016).
Hence, they are usually trained by maximum likelihood.
Undirected models, on the other hand, provide only unnor-
malized likelihoods and are sampled from using expensive
Markov chains. Hence, they are usually learned by approx-
imating the likelihood using methods such as contrastive
divergence (Carreira-Perpinan and Hinton 2005) and pseu-
dolikelihood (Besag 1977). The likelihood is generally in-
tractable to compute in latent variable models (even directed

1This could be either due to computational intractability in eval-

uating likelihoods or because the likelihood is ill-deﬁned.

ones) as it requires marginalization. These models are typi-
cally learned by optimizing a stochastic lower bound to the
log-likelihood using variational Bayes approaches (Kingma
and Welling 2014).

Directed latent variable models allow for efﬁcient ances-
tral sampling and hence these models can also be trained
using other divergences, e.g., adversarially (Mescheder,
Nowozin, and Geiger 2017a; Mao et al. 2017; Song, Zhao,
and Ermon 2017). A popular class of latent variable models
learned adversarially consist of generative adversarial net-
works (GAN; (Goodfellow et al. 2014)). GANs comprise of
a pair of generator and discriminator networks. The gener-
ator Gθ : Rk → Rd is a deterministic function differen-
tiable with respect to the parameters θ. The function takes
as input a source of randomness z ∈ Rk sampled from a
tractable prior density p(z) and transforms it to a sample
Gθ(z) through a forward pass. Evaluating likelihoods as-
signed by a GAN is challenging because the model density
pθ is speciﬁed only implicitly using the prior density p(z)
and the generator function Gθ. In fact, the likelihood for any
data point is ill-deﬁned (with respect to the Lesbegue mea-
sure over Rn) if the prior distribution over z is deﬁned over
a support smaller than the support of the data distribution.

GANs are typically learned adversarially with the help of
a discriminator network. The discriminator Dφ : Rd → R
is another real-valued function that is differentiable with re-
spect to a set of parameters φ. Given the discriminator func-
tion, we can express the functions h and h(cid:48) in Eq. (4) as
compositions of Dφ with divergence-speciﬁc functions. For
instance, the Wasserstein GAN (WGAN; (Arjovsky, Chin-
tala, and Bottou 2017)) optimizes the following objective:

min
θ

max
φ∈F

Ex∼Pdata [Dφ(x)] − Ez∼Pz [Dφ(Gθ(z))]

(5)

where F is deﬁned such that Dφ is 1-Lipschitz. Empirically,
GANs generate excellent samples of natural images (Rad-
ford, Metz, and Chintala 2015), audio signals (Pascual,
Bonafonte, and Serr`a 2017), and of behaviors in imitation
learning (Ho and Ermon 2016; Li, Song, and Ermon 2017).

3 Flow Generative Adversarial Networks
As discussed above, generative adversarial networks can
tractably generate high-quality samples but have intractable
or ill-deﬁned likelihoods. Monte Carlo techniques such as
AIS and non-parameteric density estimation methods such
as KDE get around this by assuming a Gaussian observation
model pθ(x|z) for the generator.2 This assumption alone is
not sufﬁcient for quantitative evaluation since the marginal
likelihood of the observed data, pθ(x) = (cid:82) pθ(x, z)dz in
this case would be intractable as it requires integrating over
all the latent factors of variation. This would then require ap-
proximate inference (e.g., Monte Carlo or variational meth-
ods) which in itself is a computational challenge for high-
dimensional distributions. To circumvent these issues, we
propose ﬂow generative adversarial networks (Flow-GAN).

2The true observation model for a GAN is a Dirac delta distribu-
tion, i.e., pθ(x|z) is inﬁnite when x = Gθ(z) and zero otherwise.

A Flow-GAN consists of a pair of generator-discriminator
networks with the generator speciﬁed as a normalizing ﬂow
model (Dinh, Krueger, and Bengio 2014). A normalizing
ﬂow model speciﬁes a parametric transformation from a
prior density p(z) : Rd → R+
0 to another density over the
same space, pθ(x) : Rd → R+
0 where R+
0 is the set of non-
negative reals. The generator transformation Gθ : Rd →
Rd is invertible, such that there exists an inverse function
fθ = G−1
θ . Using the change-of-variables formula and let-
ting z = fθ(x), we have:

pθ(x) = p(z)

det

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂fθ(x)
∂x

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(6)

∂x

where ∂fθ(x)
denotes the Jacobian of fθ at x. The above
formula can be applied recursively over compositions of
many invertible transformations to produce a complex ﬁnal
density. Hence, we can evaluate and optimize for the log-
likelihood assigned by the model to a data point as long as
the prior density is tractable and the determinant of the Ja-
cobian of fθ evaluated at x can be efﬁciently computed.

Evaluating the likelihood assigned by a Flow-GAN model
in Eq. (6) requires overcoming two major challenges. First,
requiring the generator function Gθ to be reversible imposes
a constraint on the dimensionality of the latent variable z to
match that of the data x. Thereafter, we require the trans-
formations between the various layers of the generator to be
invertible such that their overall composition results in an
invertible Gθ. Secondly, the Jacobian of high-dimensional
distributions can however be computationally expensive to
compute. If the transformations are designed such that the
Jacobian is an upper or lower triangular matrix, then the de-
terminant can be easily evaluated as the product of its diago-
nal entries. We consider two such family of transformations.
1. Volume preserving transformations. Here, the Jacobian of
the transformations have a unit determinant. For exam-
ple, the NICE model consists of several layers perform-
ing a location transformation (Dinh, Krueger, and Bengio
2014). The top layer is a diagonal scaling matrix with non-
zero log determinant.

2. Non-volume preserving transformations. The determinant
of the Jacobian of the transformations is not necessarily
unity. For example, in Real-NVP, layers performs both
location and scale transformations (Dinh, Sohl-Dickstein,
and Bengio 2017).
For brevity, we direct the reader to Dinh, Krueger, and
Bengio (2014) and Dinh, Sohl-Dickstein, and Bengio (2017)
for the speciﬁcations of NICE and Real-NVP respectively.
Crucially, both volume preserving and non-volume preserv-
ing transformations are invertible such that the determinant
of the Jacobian can be computed tractably.

3.1 Learning objectives
In a Flow-GAN, the likelihood is well-deﬁned and compu-
tationally tractable for exact evaluation of even expressive
volume preserving and non-volume preserving transforma-
tions. Hence, a Flow-GAN can be trained via maximum like-
lihood estimation using Eq. (1) in which case the discrimi-
nator is redundant. Additionally, we can perform ancestral

(a) MLE

(b) ADV

(c) Hybrid

Figure 1: Samples generated by Flow-GAN models with different objectives for MNIST (top) and CIFAR-10 (bottom).

sampling just like a regular GAN whereby we sample a ran-
dom vector z ∼ Pz and transform it to a model generated
sample via Gθ = f −1
. This makes it possible to learn a
Flow-GAN using an adversarial learning objective (for ex-
ample, the WGAN objective in Eq. (5)).

θ

A natural question to ask is why should one use adversar-
ial learning given that MLE is statistically efﬁcient asymp-
totically (under some conditions). Besides difﬁculties that
could arise due to optimization (in both MLE and adversar-
ial learning), the optimality of MLE holds only when there is
no model misspeciﬁcation for the generator i.e., the true data
distribution Pdata is a member of the parametric family of
distributions under consideration (White 1982). This is gen-
erally not the case for high-dimensional distributions, and
hence the choice of the learning objective becomes largely
an empirical question. Unlike other models, a Flow-GAN
allows both maximum likelihood and adversarial learning,
and hence we can investigate this question experimentally.

3.2 Evaluation metrics and experimental setup

Our criteria for evaluation is based on held-out
log-
likelihoods and sample quality metrics. We focus on nat-
ural images since they allow visual inspection as well as
quantiﬁcation using recently proposed metrics. A “good”
generative model should generalize to images outside the
training data and assign high log-likelihoods to held-out
data. The Inception and MODE scores are standard quan-
titative measures of the quality of generated samples of
natural images for labelled datasets (Salimans et al. 2016;

Che et al. 2017). The Inception scores are computed as:

exp (Ex∈Pθ [KL(p(y|x)(cid:107)p(y)])
where x is a sample generated by the model, p(y|x) is the
softmax probability for the labels y assigned by a pretrained
classiﬁer for x, and p(y) is the overall distribution of la-
bels in the generated samples (as predicted by the pretrained
classiﬁer). The intuition is that the conditional distribution
p(y|x) should have low entropy for good looking images
while the marginal distribution p(y) has high entropy to en-
sure sample diversity. Hence, a generative model can per-
form well on this metric if the KL divergence between the
two distributions (and consequently, the Inception score for
the generated samples) is large. The MODE score given be-
low modiﬁes the Inception score to take into account the
distribution of labels in the training data, p∗(y):

exp (Ex∈Pθ [KL(p(y|x)(cid:107)p∗(y)] − KL(p∗(y)(cid:107)p(y))) .
We compare learning of Flow-GANs using MLE and ad-
versarial learning (ADV) for the MNIST dataset of hand-
written digits (LeCun, Cortes, and Burges 2010) and the
CIFAR-10 dataset of natural images (Krizhevsky and Hin-
ton 2009). The normalizing ﬂow generator architectures
are chosen to be NICE (Dinh, Krueger, and Bengio 2014)
and Real-NVP (Dinh, Sohl-Dickstein, and Bengio 2017)
for MNIST and CIFAR-10 respectively. We ﬁx the Wasser-
stein distance as the choice of the divergence being op-
timized by ADV (see Eq. (5)) with the Lipschitz con-
straint over the critic imposed by penalizing the norm of
the gradient with respect to the input (Arjovsky, Chintala,
and Bottou 2017; Gulrajani et al. 2017). The discrimina-
tor is based on the DCGAN architecture (Radford, Metz,

and Chintala 2015). The above choices are among the
current state-of-the-art in maximum likelihood estimation
and adversarial learning and greatly stabilize GAN train-
ing. Further experimental setup details are provided in Ap-
pendix A. The code for reproducing the results is available at
https://github.com/ermongroup/flow-gan.

(a) MLE

(b) ADV

Figure 2: Learning curves for negative log-likelihood (NLL)
evaluation on MNIST (top, in nats) and CIFAR (bottom, in
bits/dim). Lower NLLs are better.

3.3 Evaluation results
Log-likelihood. The log-likelihood learning curves for
Flow-GAN models learned using MLE and ADV are shown
in Figure 2a and Figure 2b respectively. Following conven-
tion, we report the negative log-likelihoods (NLL) in nats for
MNIST and bits/dimension for CIFAR-10.

MLE. In Figure 2a, we see that normalizing ﬂow models
attain low validation NLLs (blue curves) after few gradient
updates as expected because it is explicitly optimizing for
the MLE objective in Eq. (1). Continued training however
could lead to overﬁtting as the train NLLs (red curves) begin
to diverge from the validation NLLs.

ADV. Surprisingly, ADV models show a consistent in-
crease in validation NLLs as training progresses as shown
in Figure 2b (for CIFAR-10, the estimates are reported on a
log scale!). Based on the learning curves, we can disregard
overﬁtting as an explanation since the increase in NLLs is
observed even on the training data. The training and vali-
dation NLLs closely track each other suggesting that ADV
models are not simply memorizing the training data.

Comparing the left vs. right panels in Figure 2, we see that
the log-likelihoods attained by ADV are orders of magnitude
worse than those attained by MLE after sufﬁcient training.
Finally, we note that the WGAN loss (green curves) does
not correlate well with NLL estimates. While the WGAN
loss stabilizes after few iterations of training, the NLLs con-
tinue to increase. This observation is in contrast to prior
work showing the loss to be strongly correlated with sam-
ple quality metrics (Arjovsky, Chintala, and Bottou 2017).

Sample quality. Samples generated from MLE and ADV-
based models with the best MODE/Inception are shown
in Figure 1a and Figure 1b respectively. ADV models
signiﬁcantly outperform MLE with respect
to the ﬁnal
MODE/Inception scores achieved. Visual inspection of sam-
ples conﬁrms the observations made on the based of the sam-
ple quality metrics. Curves monitoring the sample quality
metrics at every training iteration are given in Appendix B.

3.4 Gaussian mixture models

The above experiments suggest that ADV can produce ex-
cellent samples but assigns low likelihoods to the observed
data. However, a direct comparison of ADV with the log-
likelihoods of MLE is unfair since the latter is explicitly
optimizing for the desired objective. To highlight that gen-
erating good samples at the expense of low likelihoods is
not a challenging goal, we propose a simple baseline. We
compare the adversarially learned Flow-GAN models that
achieves the highest MODE/Inception score with a Gaussian
Mixture Model consisting of m isotropic Gaussians with
equal weights centered at each of the m training points as
the baseline Gaussian Mixture Model (GMM). The band-
width hyperparameter, σ, is the same for each of the mixture
components and optimized for the lowest validation NLL by
doing a line search in (0, 1]. We show results for CIFAR-
10 in Figure 3. Our observations below hold for MNIST as
well; results deferred to Appendix C.

We overload the y-axis in Figure 3 to report both NLLs
and sample quality metrics. The horizontal maroon and cyan
dashed lines denote the best attainable MODE/Inception
scores and corresponding validation NLLs respectively at-
tained by the adversarially learned Flow-GAN model. The
GMM can clearly attain better sample quality metrics since
it is explicitly overﬁtting to the training data for low values
of the bandwidth parameter (any σ for which the red curve
is above the maroon dashed line). Surprisingly, the simple
GMM also outperforms the adversarially learned model with
respect to NLLs attained for several values of the bandwidth
parameter (any σ for which the blue curve is below the cyan
dashed line). Bandwidth parameters for which GMM mod-
els outperform the adversarially learned model on both log-
likelihoods and sample quality metrics are highlighted using
the green shaded area. We show samples from the GMM
in the appendix. Hence, a trivial baseline that is memo-
rizing the training data can generate high quality samples
and better held-out log-likelihoods, suggesting that the log-
likelihoods attained by adversarial training are very poor.

Table 1: Best MODE scores and test negative log-likelihood
estimates for Flow-GAN models on MNIST.

Objective
MLE
ADV
Hybrid (λ = 0.1)

MODE Score Test NLL (in nats)

7.42
9.24
9.37

−3334.56
−1604.09
−3342.95

Table 2: Best Inception scores and test negative log-
likelihood estimates for Flow-GAN models on CIFAR-10.

Inception Score Test NLL (in bits/dim)

Objective
MLE
ADV
Hybrid (λ = 1)

2.92
5.76
3.90

3.54
8.53
4.21

5

Interpreting the results

Our ﬁndings are in contrast with prior work which report
much better log-likelihoods for adversarially learned mod-
els with a standard generator architecture based on an-
nealed importance sampling (AIS; (Wu et al. 2017)) and
kernel density estimation (KDE; (Goodfellow et al. 2014)).
These methods rely on approximate inference techniques
for log-likelihood evaluation and make assumptions about
a Gaussian observation model which does not hold for
GANs. Since Flow-GANs allow us to compute exact log-
likelihoods, we can evaluate the quality of approximation
made by AIS and KDE for density estimation of invertible
generators. For a detailed description of the methods, we re-
fer the reader to prior work (Neal 2001; Parzen 1962).

We consider the MNIST dataset where these meth-
ods have been previously applied to by Wu et al. (2017)
and Goodfellow et al. (2014) respectively. Since both AIS
and KDE inherently rely on the samples generated, we eval-
uate these methods for the MLE, ADV, and Hybrid Flow-
GAN model checkpoints corresponding to the best MODE
scores observed during training. In Table 3, we observe that
both AIS and KDE produce estimates of log-likelihood that
are far from the ground truth, accessible through the ex-
act Flow-GAN log-likelihoods. Even worse, the ranking of
log-likelihood estimates for AIS (ADV>Hybrid>MLE) and
KDE (Hybrid>MLE>ADV) do not obey the relative rank-
ings of the Flow-GAN estimates (MLE>Hybrid>ADV).

5.1 Explaining log-likelihood trends
In order to explain the variation in log-likelihoods attained
by various Flow-GAN learning objectives, we investigate
the distribution of the magnitudes of singular values for
the Jacobian matrix of several generator functions, Gθ for
MNIST in Figure 4 evaluated at 64 noise vectors z randomly
sampled from the prior density p(z). The x-axis of the ﬁgure
shows the singular value magnitudes on a log scale and for
each singular value s, we show the corresponding cumula-
tive distribution function value on the y-axis which signiﬁes
the fraction of singular values less than s. The results on
CIFAR-10 in Appendix D show a similar trend.

The Jacobian is a good ﬁrst-order approximation of the
generator function locally. In Figure 4, we observe that the

Figure 3: Gaussian Mixture Models outperform adversar-
ially learned models on both held-out log-likelihoods and
sampling metrics on CIFAR-10 (green shaded region).

4 Hybrid learning of Flow-GANs
In the previous section, we observed that adversarially learn-
ing Flow-GANs models attain poor held-out log-likelihoods.
This makes it challenging to use such models for applica-
tions requiring density estimation. On the other hand, Flow-
GANs learned using MLE are “mode covering” but do not
generate high quality samples. With a Flow-GAN, it is pos-
sible to trade-off the two goals by combining the learning
objectives corresponding to both these inductive principles.
Without loss of generality, let V (Gθ, Dφ) denote the min-
imax objective of any GAN model (such as WGAN). The
hybrid objective of a Flow-GAN can be expressed as:
V (Gθ, Dφ) − λEx∼Pdata [log pθ(x)]

(7)

min
θ

max
φ

where λ ≥ 0 is a hyperparameter for the algorithm. By vary-
ing λ, we can interpolate between plain adversarial training
(λ = 0) and MLE (very high λ).

We summarize the results from MLE, ADV, and Hy-
brid for log-likelihood and sample quality evaluation in Ta-
ble 1 and Table 2 for MNIST and CIFAR-10 respectively.
The tables report the test log-likelihoods corresponding to
the best validated MLE and ADV models and the highest
MODE/Inception scores observed during training. The sam-
ples generated by models with the best MODE/Inception
scores for each objective are shown in Figure 1c.

While the results on CIFAR-10 are along expected lines,
the hybrid objective interestingly outperforms MLE and
ADV on both test log-likelihoods and sample quality met-
rics in the case of MNIST. One potential explanation for this
is that the ADV objective can regularize MLE to generalize
to the test set and in turn, the MLE objective can stabilize
the optimization of the ADV objective. Hence, the hybrid
objective in Eq. (7) can smoothly balance the two objectives
using the tunable hyperparameter λ, and in some cases such
as MNIST, the performance on both tasks could improve as
a result of the hybrid objective.

Table 3: Comparison of inference techniques for negative
log-likelihood estimation of Flow-GAN models on MNIST.

Objective
MLE
ADV
Hybrid

Flow-GAN NLL
-3287.69
26350.30
-3121.53

AIS
-2584.40
-2916.10
-2703.03

KDE
-167.10
-3.03
-205.69

singular value distribution for the Jacobian of an invertible
generator learned using MLE (orange curves) is concen-
trated in a narrow range, and hence the Jacobian matrix is
well-conditioned and easy to invert. In the case of invertible
generators learned using ADV with Wasserstein distance
(green curves) however, the spread of singular values is very
wide, and hence the Jacobian matrix is ill-conditioned.

The average log determinant of the Jacobian matrices for
MLE, ADV, and Hybrid models are −4170.34, −15588.34,
and −5184.40 respectively which translates to the trend
ADV<Hybrid<MLE. This indicates that the ADV models
are trying to squish a sphere of unit volume centered at a la-
tent vector z to a very small volume in the observed space
x. Tiny perturbations of training as well as held-out data-
points can hence manifest as poor log-likelihoods. In spite
of not being limited in the representational capacity to cover
the entire space of the data distribution (the dimensions of z
(i.e., k) and x (i.e., d) match for invertible generators), ADV
prefers to learn a distribution over a smaller support.

The Hybrid learning objective (blue curves), however, is
able to correct for this behavior, and the distribution of sin-
gular value magnitudes matches closely to that of MLE. We
also considered variations involving the standard DCGAN
architectures with k = d minimizing the Wasserstein dis-
tance (red curves) and Jenson-Shannon divergence (purple
curves). The relative shift in distribution of singular value
magnitudes to lower values is apparent even in these cases.

6 Discussion
Any model which allows for efﬁcient likelihood evaluation
and sampling can be trained using maximum likelihood and
adversarial learning. This line of reasoning has been ex-
plored to some extent in prior work that combine the ob-
jectives of prescribed latent variable models such as VAEs
(maximizing an evidence lower bound on the data) with ad-
versarial learning (Larsen et al. 2015; Mescheder, Nowozin,
and Geiger 2017a; Srivastava et al. 2017). However, the ben-
eﬁts of such procedures do not come for “free” since we still
need some form of approximate inference to get a handle
on the log-likelihoods. This could be expensive, for instance
combining a VAE with a GAN introduces an additional in-
ference network that increases the overall model complexity.
Our approach sidesteps the additional complexity due to
approximate inference by considering a normalizing ﬂow
model. The trade-off made by a normalizing ﬂow model
is that the generator function needs to be invertible while
other generative models such as VAEs have no such re-
quirement. On the positive side, we can tractably evaluate
exact log-likelihoods assigned by the model for any data
point. Normalizing ﬂow models have been previously used

Figure 4: CDF of the singular values magnitudes for the Ja-
cobian of the generator functions trained on MNIST.

in the context of maximum likelihood estimation of fully ob-
served and latent variable models (Dinh, Krueger, and Ben-
gio 2014; Rezende and Mohamed 2015; Kingma, Salimans,
and Welling 2016; Dinh, Sohl-Dickstein, and Bengio 2017).
The low dimensional support of the distributions learned
by adversarial learning often manifests as lack of sample di-
versity and is referred to as mode collapse. In prior work,
mode collapse is detected based on visual inspection or
heuristic techniques (Goodfellow 2016; Arora and Zhang
2017). Techniques for avoiding mode collapse explicitly fo-
cus on stabilizing GAN training such as (Metz et al. 2016;
Che et al. 2017; Mescheder, Nowozin, and Geiger 2017b)
rather than quantitative methods based on likelihoods.

7 Conclusion
As an attempt to more quantitatively evaluate generative
models, we introduced Flow-GAN. It is a generative adver-
sarial network which allows for tractable likelihood evalu-
ation, exactly like in a ﬂow model. Since it can be trained
both adversarially (like a GAN) and in terms of MLE (like
a ﬂow model), we can quantitatively evaluate the trade-offs
involved. We observe that adversarial learning assigns very
low-likelihoods to both training and validation data while
generating superior quality samples. To put this observa-
tion in perspective, we demonstrate how a naive Gaussian
mixture model can outperform adversarially learned models
on both log-likelihood estimates and sample quality metrics.
Quantitative evaluation methods based on AIS and KDE fail
to detect this behavior and can be poor approximations of the
true log-likelihood (at least for the models we considered).
Analyzing the Jacobian of the generator provides insights
into the contrast between maximum likelihood estimation
and adversarial learning. The latter have a tendency to learn
distributions of low support, which can lead to low likeli-
hoods. To correct for this behavior, we proposed a hybrid ob-
jective function which involves loss terms corresponding to
both MLE and adversarial learning. The use of such models
in applications requiring both density estimation and sample
generation is an exciting direction for future work.

Acknowledgements
We are thankful to Ben Poole and Daniel Levy for helpful
discussions. This research was supported by a Microsoft Re-
search PhD fellowship in machine learning for the ﬁrst au-
thor, NSF grants #1651565, #1522054, #1733686, a Fu-
ture of Life Institute grant, and Intel.

References
Arjovsky, M.; Chintala, S.; and Bottou, L. 2017. Wasserstein
GAN. In International Conference on Machine Learning.
2017. Do GANs actually
Arora, S., and Zhang, Y.
learn the distribution? An empirical study. arXiv preprint
arXiv:1706.08224.
Besag, J. 1977. Efﬁciency of pseudolikelihood estimation
for simple Gaussian ﬁelds. Biometrika 616–618.
Carreira-Perpinan, M. A., and Hinton, G. E. 2005. On con-
In Artiﬁcial Intelligence and
trastive divergence learning.
Statistics.
Che, T.; Li, Y.; Jacob, A. P.; Bengio, Y.; and Li, W. 2017.
Mode regularized generative adversarial networks. In Inter-
national Conference on Learning Representations.
Diggle, P. J., and Gratton, R. J. 1984. Monte Carlo methods
of inference for implicit statistical models. Journal of the
Royal Statistical Society. 193–227.
Dinh, L.; Krueger, D.; and Bengio, Y. 2014. NICE: Non-
linear independent components estimation. arXiv preprint
arXiv:1410.8516.
Dinh, L.; Sohl-Dickstein, J.; and Bengio, S. 2017. Density
estimation using Real NVP. In International Conference on
Learning Representations.
Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;
Warde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y.
2014. Generative adversarial nets. In Advances in Neural
Information Processing Systems.
Goodfellow, I. 2016. NIPS 2016 tutorial: Generative adver-
sarial networks. arXiv preprint arXiv:1701.00160.
Gulrajani, I.; Ahmed, F.; Arjovsky, M.; Dumoulin, V.; and
Courville, A.
Improved training of Wasserstein
GANs. In Advances in Neural Information Processing Sys-
tems.
Ho, J., and Ermon, S. 2016. Generative adversarial imita-
tion learning. In Advances in Neural Information Processing
Systems.
Huber, P. J. 1967. The behavior of maximum likelihood
In ﬁfth Berkeley
estimates under nonstandard conditions.
symposium on mathematical statistics and probability.
Kingma, D. P., and Welling, M. 2014. Auto-encoding varia-
tional bayes. In International Conference on Learning Rep-
resentations.
Kingma, D. P.; Salimans, T.; and Welling, M. 2016. Improv-
ing variational inference with inverse autoregressive ﬂow. In
International Conference on Learning Representations.
Krizhevsky, A., and Hinton, G. 2009. Learning multiple
layers of features from tiny images. Technical Report.

2017.

Larochelle, H., and Murray, I. 2011. The neural autore-
gressive distribution estimator. In Artiﬁcial Intelligence and
Statistics.
Larsen, A. B. L.; Sønderby, S. K.; Larochelle, H.; and
Winther, O. 2015. Autoencoding beyond pixels using a
learned similarity metric. arXiv preprint arXiv:1512.09300.
LeCun, Y.; Cortes, C.; and Burges, C. J. 2010. MNIST hand-
written digit database. http://yann. lecun. com/exdb/mnist.
Li, Y.; Song, J.; and Ermon, S. 2017.
InfoGAIL: Inter-
pretable imitation learning from visual demonstrations. In
Advances in Neural Information Processing Systems.
Mao, X.; Li, Q.; Xie, H.; Lau, R. Y.; Wang, Z.; and Smolley,
S. P. 2017. Least squares generative adversarial networks.
In International Conference on Computer Vision.
Mescheder, L.; Nowozin, S.; and Geiger, A. 2017a. Adver-
sarial variational Bayes: Unifying variational autoencoders
and generative adversarial networks. In International Con-
ference on Machine Learning.
Mescheder, L.; Nowozin, S.; and Geiger, A. 2017b. The
numerics of GANs. In Advances in Neural Information Pro-
cessing Systems.
Metz, L.; Poole, B.; Pfau, D.; and Sohl-Dickstein, J. 2016.
Unrolled generative adversarial networks. arXiv preprint
arXiv:1611.02163.
Mohamed, S., and Lakshminarayanan, B. 2016. Learn-
arXiv preprint
ing in implicit generative models.
arXiv:1610.03483.
Neal, R. M. 2001. Annealed importance sampling. Statistics
and Computing 11(2):125–139.
Nowozin, S.; Cseke, B.; and Tomioka, R. 2016.
f-GAN:
Training generative neural samplers using variational diver-
In Advances in Neural Information
gence minimization.
Processing Systems.
Oord, A. v. d.; Kalchbrenner, N.; and Kavukcuoglu, K. 2016.
Pixel recurrent neural networks. In International Conference
on Machine Learning.
Ostrovski, G.; Bellemare, M. G.; Oord, A. v. d.; and Munos,
R. 2017. Count-based exploration with neural density mod-
els. In International Conference on Machine Learning.
Parzen, E. 1962. On estimation of a probability density
function and mode. The Annals of Mathematical Statistics
33(3):1065–1076.
Pascual, S.; Bonafonte, A.; and Serr`a, J. 2017. SEGAN:
Speech enhancement generative adversarial network. arXiv
preprint arXiv:1703.09452.
Radford, A.; Metz, L.; and Chintala, S. 2015. Unsupervised
representation learning with deep convolutional generative
adversarial networks. arXiv preprint arXiv:1511.06434.
Rezende, D. J., and Mohamed, S. 2015. Variational infer-
In International Conference
ence with normalizing ﬂows.
on Machine Learning.
Salimans, T.; Goodfellow, I.; Zaremba, W.; Cheung, V.; Rad-
ford, A.; and Chen, X. 2016. Improved techniques for train-
ing GANs. In Advances in Neural Information Processing
Systems.

Song, J.; Zhao, S.; and Ermon, S. 2017. A-NICE-MC: Ad-
versarial training for MCMC. In Advances in Neural Infor-
mation Processing Systems.
Srivastava, A.; Valkov, L.; Russell, C.; Gutmann, M.; and
Sutton, C. 2017. VEEGAN: Reducing mode collapse in
In Advances in
GANs using implicit variational learning.
Neural Information Processing Systems.
Theis, L.; Oord, A. v. d.; and Bethge, M. 2016. A note on
the evaluation of generative models. In International Con-
ference on Learning Representations.
Uria, B.; Murray, I.; and Larochelle, H. 2013. RNADE:
The real-valued neural autoregressive density-estimator. In
Advances in Neural Information Processing Systems.
White, H. 1982. Maximum likelihood estimation of mis-
speciﬁed models. Econometrica: Journal of the Economet-
ric Society 1–25.
Wu, Y.; Burda, Y.; Salakhutdinov, R.; and Grosse, R. 2017.
On the quantitative analysis of decoder-based generative
In International Conference on Learning Repre-
models.
sentations.

Appendices
A Experimental setup details
Datasets. The MNIST dataset contains 50, 000 train,
10, 000 validation, and 10, 000 test images of dimensions
28 × 28 (LeCun, Cortes, and Burges 2010). The CIFAR-10
dataset contains 50, 000 train and 10, 000 test images of di-
mensions 32 × 32 × 3 by default (Krizhevsky and Hinton
2009). We held out a random subset of 5, 000 training set
images as validation set.

Since we are modeling densities for discrete datasets
(pixels can take a ﬁnite set of values ranging from 1 to
255), the model can assign arbitrarily high log-likelihoods
to these discrete points. Following Uria, Murray, and
Larochelle (2013), we dequantize the data by adding uni-
form noise between 0 and 1 to every pixel. Finally, we scale
the pixels to lie in the range [0, 1].

Model priors and hyperparameters. The Flow-GAN
architectures trained on MNIST and CIFAR-10 used a
logistic and an isotropic prior density respectively con-
sistent with prior work (Dinh, Krueger, and Bengio
2014; Dinh, Sohl-Dickstein, and Bengio 2017). Hyper-
parameter details for learning all
the Flow-GAN mod-
els are included in the README of the code repository:
https://github.com/ermongroup/flow-gan

(a) MNIST (MODE)

(b) CIFAR-10 (Inception)

Figure 5: Sample quality curves during training.

B Sample quality
The progression of sample quality metrics for MLE and
ADV objectives during training is shown in Figures 5 (a) and
(b) for MNIST and CIFAR-10 respectively. Higher scores
are reﬂective of better sample quality. ADV (maroon curves)
signiﬁcantly outperform MLE (cyan curves) with respect to
the ﬁnal MODE/Inception scores achieved.

Figure 6: Gaussian Mixture Models outperform adversar-
ially learned models on both held-out log-likelihoods and
sampling metrics on MNIST (green shaded region).

C Gaussian mixture models
The comparison of GMMs with Flow-GANs trained using
adversarial learning is shown in Figure 6. Similar to the ob-
servations made for CIFAR-10, the simple GMM outper-
forms the adversarially learned model with respect to NLLs
and sample quality metrics for any bandwidth parameter
within the green shaded area.

The samples obtained from the GMM are shown in Fig-
ure 7. Since the baseline is ﬁtting Gaussian densities around
every training point, the samples obtained for relatively
small bandwidths are of high quality. Yet, even the held-
out likelihoods for these bandwidths are better than those
of ADV models with the best MODE/Inception scores.

D Explaining log-likelihood trends
The CDF of singular value magnitudes for the CIFAR-10
dataset in Figure 8 again suggests that the Jacobian matrix
for the generator function is ill-conditioned for the ADV
models (green, red, purple curves) since the distributions
have a large spread. Using a hybrid objective (blue curves)
can correct for this behavior with the distribution of singu-
lar values much more concentrated similar to MLE (orange
curves).

The log determinant of the Jacobian for the MLE, ADV,
and Hybrid models are −12818.84, −21848.09, −14729.51
respectively reﬂecting the trend ADV<Hybrid<MLE, pro-
viding further empirical evidence to suggest that adversarial
training shows a strong preference for learning distributions
with smaller support.

(a) σ = 0.1

(b) σ = 0.07

Figure 7: Samples from the Gaussian Mixture Model base-
line for MNIST (top) and CIFAR-10 (bottom) with better
MODE/Inception scores than ADV models.

Figure 8: CDF of singular values magnitudes for the Jaco-
bian of the generator function on CIFAR-10.

Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in
Generative Models

Aditya Grover, Manik Dhar, Stefano Ermon
Computer Science Department
Stanford University
{adityag, dmanik, ermon}@cs.stanford.edu

8
1
0
2
 
n
a
J
 
3
 
 
]

G
L
.
s
c
[
 
 
2
v
8
6
8
8
0
.
5
0
7
1
:
v
i
X
r
a

Abstract

Adversarial learning of probabilistic models has recently
emerged as a promising alternative to maximum likelihood.
Implicit models such as generative adversarial networks
(GAN) often generate better samples compared to explicit
models trained by maximum likelihood. Yet, GANs sidestep
the characterization of an explicit density which makes quan-
titative evaluations challenging. To bridge this gap, we pro-
pose Flow-GANs, a generative adversarial network for which
we can perform exact likelihood evaluation, thus support-
ing both adversarial and maximum likelihood training. When
trained adversarially, Flow-GANs generate high-quality sam-
ples but attain extremely poor log-likelihood scores, inferior
even to a mixture model memorizing the training data; the op-
posite is true when trained by maximum likelihood. Results
on MNIST and CIFAR-10 demonstrate that hybrid training
can attain high held-out likelihoods while retaining visual ﬁ-
delity in the generated samples.

1

Introduction

Highly expressive parametric models have enjoyed great
success in supervised learning, where learning objectives
and evaluation metrics are typically well-speciﬁed and easy
to compute. On the other hand, the learning objective for un-
supervised settings is less clear. At a fundamental level, the
idea is to learn a generative model that minimizes some no-
tion of divergence with respect to the data distribution. Min-
imizing the Kullback-Liebler divergence between the data
distribution and the model, for instance, is equivalent to per-
forming maximum likelihood estimation (MLE) on the ob-
served data. Maximum likelihood estimators are asymptoti-
cally statistically efﬁcient, and serve as natural objectives for
learning prescribed generative models (Mohamed and Lak-
shminarayanan 2016).

In contrast, an alternate principle that has recently at-
tracted much attention is based on adversarial learning,
where the objective is to generate data indistinguishable
from the training data. Adversarially learned models such
as generative adversarial networks (GAN; (Goodfellow et
al. 2014)) can sidestep specifying an explicit density for any
data point and belong to the class of implicit generative mod-
els (Diggle and Gratton 1984).

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

The lack of characterization of an explicit density in
GANs is however problematic for two reasons. Several ap-
plication areas of deep generative models rely on density
estimates; for instance, count based exploration strategies
based on density estimation using generative models have
recently achieved state-of-the-art performance on challeng-
ing reinforcement learning environments (Ostrovski et al.
2017). Secondly, it makes the quantitative evaluation of the
generalization performance of such models challenging. The
typical evaluation criteria based on ad-hoc sample quality
metrics (Salimans et al. 2016; Che et al. 2017) do not address
this issue since it is possible to generate good samples by
memorizing the training data, or missing important modes
of the distribution, or both (Theis, Oord, and Bethge 2016).
Alternatively, density estimates based on approximate in-
ference techniques such as annealed importance sampling
(AIS; (Neal 2001; Wu et al. 2017)) and non-parameteric
methods such as kernel density estimation (KDE; (Parzen
1962; Goodfellow et al. 2014)) are computationally slow
and crucially rely on assumptions of a Gaussian observa-
tion model for the likelihood that could lead to misleading
estimates as we shall demonstrate in this paper.

To sidestep the above issues, we propose Flow-GANs,
a generative adversarial network with a normalizing ﬂow
generator. A Flow-GAN generator transforms a prior noise
density into a model density through a sequence of invert-
ible transformations. By using an invertible generator, Flow-
GANs allow us to tractably evaluate exact likelihoods using
the change-of-variables formula and perform exact posterior
inference over the latent variables while still permitting efﬁ-
cient ancestral sampling, desirable properties of any proba-
bilistic model that a typical GAN would not provide.

Using a Flow-GAN, we perform a principled quantitative
comparison of maximum likelihood and adversarial learning
on benchmark datasets viz. MNIST and CIFAR-10. While
adversarial learning outperforms MLE on sample quality
metrics as expected based on strong evidence in prior work,
the log-likelihood estimates of adversarial learning are or-
ders of magnitude worse than those of MLE. The difference
is so stark that a simple Gaussian mixture model baseline
outperforms adversarially learned models on both sample
quality and held-out likelihoods. Our quantitative analysis
reveals that the poor likelihoods of adversarial learning can
be explained as a result of an ill-conditioned Jacobian ma-

trix for the generator function suggesting a mode collapse,
rather than overﬁtting to the training dataset.

To resolve the dichotomy of perceptually good-looking
samples at the expense of held-out likelihoods in the case
of adversarial learning (and vice versa in the case of MLE),
we propose a hybrid objective that bridges implicit and pre-
scribed learning by augmenting the adversarial training ob-
jective with an additional term corresponding to the log-
likelihood of the observed data. While the hybrid objective
achieves the intended effect of smoothly trading-off the two
goals in the case of CIFAR-10, it has a regularizing effect on
MNIST where it outperforms MLE and adversarial learning
on both held-out likelihoods and sample quality metrics.
Overall, this paper makes the following contributions:

1. We propose Flow-GANs, a generative adversarial net-
work with an invertible generator that can perform efﬁ-
cient ancestral sampling and exact likelihood evaluation.

2. We propose a hybrid learning objective for Flow-GANs
that attains good log-likelihoods and generates high-
quality samples on MNIST and CIFAR-10 datasets.

3. We demonstrate the limitations of AIS and KDE for log-
likelihood evaluation and ranking of implicit models.

4. We analyze the singular value distribution for the Jaco-
bian of the generator function to explain the low log-
likelihoods observed due to adversarial learning.

2 Preliminaries
We begin with a review of maximum likelihood estimation
and adversarial learning in the context of generative models.
For ease of presentation, all distributions are w.r.t. any arbi-
trary x ∈ Rd, unless otherwise speciﬁed. We use upper-case
to denote probability distributions and assume they all admit
absolutely continuous densities (denoted by the correspond-
ing lower-case notation) on a reference measure dx.

Consider the following setting for learning generative
models. Given some data X = {xi ∈ Rd}m
i=1 sampled i.i.d.
from an unknown probability density pdata, we are inter-
ested in learning a probability density pθ where θ denotes the
parameters of a model. Given a parameteric family of mod-
els M, the typical approach to learn θ ∈ M is to minimize
a notion of divergence between Pdata and Pθ. The choice of
divergence and the optimization procedure dictate learning,
leading to the following two objectives.

2.1 Maximum likelihood estimation
In maximum likelihood estimation (MLE), we minimize the
Kullback-Liebler (KL) divergence between the data distri-
bution and the model distribution. Formally, the learning ob-
jective can be expressed as:

min
θ∈M

KL(Pdata, Pθ) = Ex∼Pdata

log

(cid:20)

(cid:21)

pdata(x)
pθ(x)

Since pdata is independent of θ, the above optimization
problem can be equivalently expressed as:
Ex∼Pdata [log pθ(x)]

(1)

max
θ∈M

Hence, evaluating the learning objective for MLE in Eq. (1)
requires the ability to evaluate the model density pθ(x).
Models that provide an explicit characterization of the like-
lihood function are referred to as prescribed generative mod-
els (Mohamed and Lakshminarayanan 2016).

2.2 Adversarial learning
A generative model can be learned to optimize divergence
notions beyond the KL divergence. A large family of diver-
gences can be conveniently expressed as:
Ex∼Pθ [hφ(x)] − Ex∼Pdata

φ(x)(cid:3)

(cid:2)h(cid:48)

(2)

max
φ∈F

where F denotes a set of parameters, hφ and h(cid:48)
φ are ap-
propriate real-valued functions parameterized by φ. Differ-
ent choices of F, hφ and h(cid:48)
φ can lead to a variety of f -
divergences such as Jenson-Shannon divergence and inte-
gral probability metrics such as the Wasserstein distance.
For instance, the GAN objective proposed by Goodfellow
et al. (2014) can also be cast in the form of Eq. (2) below:

max
φ∈F

Ex∼Pθ [log (1 − Dφ(x))] + Ex∼Pdata [Dφ(x)]

(3)

where φ denotes the parameters of a neural network function
Dφ. We refer the reader to (Nowozin, Cseke, and Tomioka
2016; Mescheder, Nowozin, and Geiger 2017b) for further
details on other possible choices of divergences. Impor-
tantly, a Monte Carlo estimate of the objective in Eq. (2)
requires only samples from the model. Hence, any model
that allows tractable sampling can be used to evaluate the
following minimax objective:

min
θ∈M

max
φ∈F

Ex∼Pθ [hφ(x)] − Ex∼Pdata

(cid:2)h(cid:48)

φ(x)(cid:3) .

(4)

As a result, even differentiable implicit models which do
not provide a characterization of the model likelihood1 but
allow tractable sampling can be learned adversarially by op-
timizing minimax objectives of the form given in Eq. (4).

2.3 Adversarial learning of latent variable models
From a statistical perspective, maximum likelihood estima-
tors are statistically efﬁcient asymptotically (under some
conditions) and hence minimizing the KL divergence is a
natural objective for many prescribed models (Huber 1967).
However, not all models allow for a well-deﬁned, tractable,
and easy-to-optimize likelihood.

For example, exact likelihood evaluation and sampling are
tractable in directed, fully observed models such as Bayesian
networks and autoregressive models (Larochelle and Mur-
ray 2011; Oord, Kalchbrenner, and Kavukcuoglu 2016).
Hence, they are usually trained by maximum likelihood.
Undirected models, on the other hand, provide only unnor-
malized likelihoods and are sampled from using expensive
Markov chains. Hence, they are usually learned by approx-
imating the likelihood using methods such as contrastive
divergence (Carreira-Perpinan and Hinton 2005) and pseu-
dolikelihood (Besag 1977). The likelihood is generally in-
tractable to compute in latent variable models (even directed

1This could be either due to computational intractability in eval-

uating likelihoods or because the likelihood is ill-deﬁned.

ones) as it requires marginalization. These models are typi-
cally learned by optimizing a stochastic lower bound to the
log-likelihood using variational Bayes approaches (Kingma
and Welling 2014).

Directed latent variable models allow for efﬁcient ances-
tral sampling and hence these models can also be trained
using other divergences, e.g., adversarially (Mescheder,
Nowozin, and Geiger 2017a; Mao et al. 2017; Song, Zhao,
and Ermon 2017). A popular class of latent variable models
learned adversarially consist of generative adversarial net-
works (GAN; (Goodfellow et al. 2014)). GANs comprise of
a pair of generator and discriminator networks. The gener-
ator Gθ : Rk → Rd is a deterministic function differen-
tiable with respect to the parameters θ. The function takes
as input a source of randomness z ∈ Rk sampled from a
tractable prior density p(z) and transforms it to a sample
Gθ(z) through a forward pass. Evaluating likelihoods as-
signed by a GAN is challenging because the model density
pθ is speciﬁed only implicitly using the prior density p(z)
and the generator function Gθ. In fact, the likelihood for any
data point is ill-deﬁned (with respect to the Lesbegue mea-
sure over Rn) if the prior distribution over z is deﬁned over
a support smaller than the support of the data distribution.

GANs are typically learned adversarially with the help of
a discriminator network. The discriminator Dφ : Rd → R
is another real-valued function that is differentiable with re-
spect to a set of parameters φ. Given the discriminator func-
tion, we can express the functions h and h(cid:48) in Eq. (4) as
compositions of Dφ with divergence-speciﬁc functions. For
instance, the Wasserstein GAN (WGAN; (Arjovsky, Chin-
tala, and Bottou 2017)) optimizes the following objective:

min
θ

max
φ∈F

Ex∼Pdata [Dφ(x)] − Ez∼Pz [Dφ(Gθ(z))]

(5)

where F is deﬁned such that Dφ is 1-Lipschitz. Empirically,
GANs generate excellent samples of natural images (Rad-
ford, Metz, and Chintala 2015), audio signals (Pascual,
Bonafonte, and Serr`a 2017), and of behaviors in imitation
learning (Ho and Ermon 2016; Li, Song, and Ermon 2017).

3 Flow Generative Adversarial Networks
As discussed above, generative adversarial networks can
tractably generate high-quality samples but have intractable
or ill-deﬁned likelihoods. Monte Carlo techniques such as
AIS and non-parameteric density estimation methods such
as KDE get around this by assuming a Gaussian observation
model pθ(x|z) for the generator.2 This assumption alone is
not sufﬁcient for quantitative evaluation since the marginal
likelihood of the observed data, pθ(x) = (cid:82) pθ(x, z)dz in
this case would be intractable as it requires integrating over
all the latent factors of variation. This would then require ap-
proximate inference (e.g., Monte Carlo or variational meth-
ods) which in itself is a computational challenge for high-
dimensional distributions. To circumvent these issues, we
propose ﬂow generative adversarial networks (Flow-GAN).

2The true observation model for a GAN is a Dirac delta distribu-
tion, i.e., pθ(x|z) is inﬁnite when x = Gθ(z) and zero otherwise.

A Flow-GAN consists of a pair of generator-discriminator
networks with the generator speciﬁed as a normalizing ﬂow
model (Dinh, Krueger, and Bengio 2014). A normalizing
ﬂow model speciﬁes a parametric transformation from a
prior density p(z) : Rd → R+
0 to another density over the
same space, pθ(x) : Rd → R+
0 where R+
0 is the set of non-
negative reals. The generator transformation Gθ : Rd →
Rd is invertible, such that there exists an inverse function
fθ = G−1
θ . Using the change-of-variables formula and let-
ting z = fθ(x), we have:

pθ(x) = p(z)

det

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂fθ(x)
∂x

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(6)

∂x

where ∂fθ(x)
denotes the Jacobian of fθ at x. The above
formula can be applied recursively over compositions of
many invertible transformations to produce a complex ﬁnal
density. Hence, we can evaluate and optimize for the log-
likelihood assigned by the model to a data point as long as
the prior density is tractable and the determinant of the Ja-
cobian of fθ evaluated at x can be efﬁciently computed.

Evaluating the likelihood assigned by a Flow-GAN model
in Eq. (6) requires overcoming two major challenges. First,
requiring the generator function Gθ to be reversible imposes
a constraint on the dimensionality of the latent variable z to
match that of the data x. Thereafter, we require the trans-
formations between the various layers of the generator to be
invertible such that their overall composition results in an
invertible Gθ. Secondly, the Jacobian of high-dimensional
distributions can however be computationally expensive to
compute. If the transformations are designed such that the
Jacobian is an upper or lower triangular matrix, then the de-
terminant can be easily evaluated as the product of its diago-
nal entries. We consider two such family of transformations.
1. Volume preserving transformations. Here, the Jacobian of
the transformations have a unit determinant. For exam-
ple, the NICE model consists of several layers perform-
ing a location transformation (Dinh, Krueger, and Bengio
2014). The top layer is a diagonal scaling matrix with non-
zero log determinant.

2. Non-volume preserving transformations. The determinant
of the Jacobian of the transformations is not necessarily
unity. For example, in Real-NVP, layers performs both
location and scale transformations (Dinh, Sohl-Dickstein,
and Bengio 2017).
For brevity, we direct the reader to Dinh, Krueger, and
Bengio (2014) and Dinh, Sohl-Dickstein, and Bengio (2017)
for the speciﬁcations of NICE and Real-NVP respectively.
Crucially, both volume preserving and non-volume preserv-
ing transformations are invertible such that the determinant
of the Jacobian can be computed tractably.

3.1 Learning objectives
In a Flow-GAN, the likelihood is well-deﬁned and compu-
tationally tractable for exact evaluation of even expressive
volume preserving and non-volume preserving transforma-
tions. Hence, a Flow-GAN can be trained via maximum like-
lihood estimation using Eq. (1) in which case the discrimi-
nator is redundant. Additionally, we can perform ancestral

(a) MLE

(b) ADV

(c) Hybrid

Figure 1: Samples generated by Flow-GAN models with different objectives for MNIST (top) and CIFAR-10 (bottom).

sampling just like a regular GAN whereby we sample a ran-
dom vector z ∼ Pz and transform it to a model generated
sample via Gθ = f −1
. This makes it possible to learn a
Flow-GAN using an adversarial learning objective (for ex-
ample, the WGAN objective in Eq. (5)).

θ

A natural question to ask is why should one use adversar-
ial learning given that MLE is statistically efﬁcient asymp-
totically (under some conditions). Besides difﬁculties that
could arise due to optimization (in both MLE and adversar-
ial learning), the optimality of MLE holds only when there is
no model misspeciﬁcation for the generator i.e., the true data
distribution Pdata is a member of the parametric family of
distributions under consideration (White 1982). This is gen-
erally not the case for high-dimensional distributions, and
hence the choice of the learning objective becomes largely
an empirical question. Unlike other models, a Flow-GAN
allows both maximum likelihood and adversarial learning,
and hence we can investigate this question experimentally.

3.2 Evaluation metrics and experimental setup

Our criteria for evaluation is based on held-out
log-
likelihoods and sample quality metrics. We focus on nat-
ural images since they allow visual inspection as well as
quantiﬁcation using recently proposed metrics. A “good”
generative model should generalize to images outside the
training data and assign high log-likelihoods to held-out
data. The Inception and MODE scores are standard quan-
titative measures of the quality of generated samples of
natural images for labelled datasets (Salimans et al. 2016;

Che et al. 2017). The Inception scores are computed as:

exp (Ex∈Pθ [KL(p(y|x)(cid:107)p(y)])
where x is a sample generated by the model, p(y|x) is the
softmax probability for the labels y assigned by a pretrained
classiﬁer for x, and p(y) is the overall distribution of la-
bels in the generated samples (as predicted by the pretrained
classiﬁer). The intuition is that the conditional distribution
p(y|x) should have low entropy for good looking images
while the marginal distribution p(y) has high entropy to en-
sure sample diversity. Hence, a generative model can per-
form well on this metric if the KL divergence between the
two distributions (and consequently, the Inception score for
the generated samples) is large. The MODE score given be-
low modiﬁes the Inception score to take into account the
distribution of labels in the training data, p∗(y):

exp (Ex∈Pθ [KL(p(y|x)(cid:107)p∗(y)] − KL(p∗(y)(cid:107)p(y))) .
We compare learning of Flow-GANs using MLE and ad-
versarial learning (ADV) for the MNIST dataset of hand-
written digits (LeCun, Cortes, and Burges 2010) and the
CIFAR-10 dataset of natural images (Krizhevsky and Hin-
ton 2009). The normalizing ﬂow generator architectures
are chosen to be NICE (Dinh, Krueger, and Bengio 2014)
and Real-NVP (Dinh, Sohl-Dickstein, and Bengio 2017)
for MNIST and CIFAR-10 respectively. We ﬁx the Wasser-
stein distance as the choice of the divergence being op-
timized by ADV (see Eq. (5)) with the Lipschitz con-
straint over the critic imposed by penalizing the norm of
the gradient with respect to the input (Arjovsky, Chintala,
and Bottou 2017; Gulrajani et al. 2017). The discrimina-
tor is based on the DCGAN architecture (Radford, Metz,

and Chintala 2015). The above choices are among the
current state-of-the-art in maximum likelihood estimation
and adversarial learning and greatly stabilize GAN train-
ing. Further experimental setup details are provided in Ap-
pendix A. The code for reproducing the results is available at
https://github.com/ermongroup/flow-gan.

(a) MLE

(b) ADV

Figure 2: Learning curves for negative log-likelihood (NLL)
evaluation on MNIST (top, in nats) and CIFAR (bottom, in
bits/dim). Lower NLLs are better.

3.3 Evaluation results
Log-likelihood. The log-likelihood learning curves for
Flow-GAN models learned using MLE and ADV are shown
in Figure 2a and Figure 2b respectively. Following conven-
tion, we report the negative log-likelihoods (NLL) in nats for
MNIST and bits/dimension for CIFAR-10.

MLE. In Figure 2a, we see that normalizing ﬂow models
attain low validation NLLs (blue curves) after few gradient
updates as expected because it is explicitly optimizing for
the MLE objective in Eq. (1). Continued training however
could lead to overﬁtting as the train NLLs (red curves) begin
to diverge from the validation NLLs.

ADV. Surprisingly, ADV models show a consistent in-
crease in validation NLLs as training progresses as shown
in Figure 2b (for CIFAR-10, the estimates are reported on a
log scale!). Based on the learning curves, we can disregard
overﬁtting as an explanation since the increase in NLLs is
observed even on the training data. The training and vali-
dation NLLs closely track each other suggesting that ADV
models are not simply memorizing the training data.

Comparing the left vs. right panels in Figure 2, we see that
the log-likelihoods attained by ADV are orders of magnitude
worse than those attained by MLE after sufﬁcient training.
Finally, we note that the WGAN loss (green curves) does
not correlate well with NLL estimates. While the WGAN
loss stabilizes after few iterations of training, the NLLs con-
tinue to increase. This observation is in contrast to prior
work showing the loss to be strongly correlated with sam-
ple quality metrics (Arjovsky, Chintala, and Bottou 2017).

Sample quality. Samples generated from MLE and ADV-
based models with the best MODE/Inception are shown
in Figure 1a and Figure 1b respectively. ADV models
signiﬁcantly outperform MLE with respect
to the ﬁnal
MODE/Inception scores achieved. Visual inspection of sam-
ples conﬁrms the observations made on the based of the sam-
ple quality metrics. Curves monitoring the sample quality
metrics at every training iteration are given in Appendix B.

3.4 Gaussian mixture models

The above experiments suggest that ADV can produce ex-
cellent samples but assigns low likelihoods to the observed
data. However, a direct comparison of ADV with the log-
likelihoods of MLE is unfair since the latter is explicitly
optimizing for the desired objective. To highlight that gen-
erating good samples at the expense of low likelihoods is
not a challenging goal, we propose a simple baseline. We
compare the adversarially learned Flow-GAN models that
achieves the highest MODE/Inception score with a Gaussian
Mixture Model consisting of m isotropic Gaussians with
equal weights centered at each of the m training points as
the baseline Gaussian Mixture Model (GMM). The band-
width hyperparameter, σ, is the same for each of the mixture
components and optimized for the lowest validation NLL by
doing a line search in (0, 1]. We show results for CIFAR-
10 in Figure 3. Our observations below hold for MNIST as
well; results deferred to Appendix C.

We overload the y-axis in Figure 3 to report both NLLs
and sample quality metrics. The horizontal maroon and cyan
dashed lines denote the best attainable MODE/Inception
scores and corresponding validation NLLs respectively at-
tained by the adversarially learned Flow-GAN model. The
GMM can clearly attain better sample quality metrics since
it is explicitly overﬁtting to the training data for low values
of the bandwidth parameter (any σ for which the red curve
is above the maroon dashed line). Surprisingly, the simple
GMM also outperforms the adversarially learned model with
respect to NLLs attained for several values of the bandwidth
parameter (any σ for which the blue curve is below the cyan
dashed line). Bandwidth parameters for which GMM mod-
els outperform the adversarially learned model on both log-
likelihoods and sample quality metrics are highlighted using
the green shaded area. We show samples from the GMM
in the appendix. Hence, a trivial baseline that is memo-
rizing the training data can generate high quality samples
and better held-out log-likelihoods, suggesting that the log-
likelihoods attained by adversarial training are very poor.

Table 1: Best MODE scores and test negative log-likelihood
estimates for Flow-GAN models on MNIST.

Objective
MLE
ADV
Hybrid (λ = 0.1)

MODE Score Test NLL (in nats)

7.42
9.24
9.37

−3334.56
−1604.09
−3342.95

Table 2: Best Inception scores and test negative log-
likelihood estimates for Flow-GAN models on CIFAR-10.

Inception Score Test NLL (in bits/dim)

Objective
MLE
ADV
Hybrid (λ = 1)

2.92
5.76
3.90

3.54
8.53
4.21

5

Interpreting the results

Our ﬁndings are in contrast with prior work which report
much better log-likelihoods for adversarially learned mod-
els with a standard generator architecture based on an-
nealed importance sampling (AIS; (Wu et al. 2017)) and
kernel density estimation (KDE; (Goodfellow et al. 2014)).
These methods rely on approximate inference techniques
for log-likelihood evaluation and make assumptions about
a Gaussian observation model which does not hold for
GANs. Since Flow-GANs allow us to compute exact log-
likelihoods, we can evaluate the quality of approximation
made by AIS and KDE for density estimation of invertible
generators. For a detailed description of the methods, we re-
fer the reader to prior work (Neal 2001; Parzen 1962).

We consider the MNIST dataset where these meth-
ods have been previously applied to by Wu et al. (2017)
and Goodfellow et al. (2014) respectively. Since both AIS
and KDE inherently rely on the samples generated, we eval-
uate these methods for the MLE, ADV, and Hybrid Flow-
GAN model checkpoints corresponding to the best MODE
scores observed during training. In Table 3, we observe that
both AIS and KDE produce estimates of log-likelihood that
are far from the ground truth, accessible through the ex-
act Flow-GAN log-likelihoods. Even worse, the ranking of
log-likelihood estimates for AIS (ADV>Hybrid>MLE) and
KDE (Hybrid>MLE>ADV) do not obey the relative rank-
ings of the Flow-GAN estimates (MLE>Hybrid>ADV).

5.1 Explaining log-likelihood trends
In order to explain the variation in log-likelihoods attained
by various Flow-GAN learning objectives, we investigate
the distribution of the magnitudes of singular values for
the Jacobian matrix of several generator functions, Gθ for
MNIST in Figure 4 evaluated at 64 noise vectors z randomly
sampled from the prior density p(z). The x-axis of the ﬁgure
shows the singular value magnitudes on a log scale and for
each singular value s, we show the corresponding cumula-
tive distribution function value on the y-axis which signiﬁes
the fraction of singular values less than s. The results on
CIFAR-10 in Appendix D show a similar trend.

The Jacobian is a good ﬁrst-order approximation of the
generator function locally. In Figure 4, we observe that the

Figure 3: Gaussian Mixture Models outperform adversar-
ially learned models on both held-out log-likelihoods and
sampling metrics on CIFAR-10 (green shaded region).

4 Hybrid learning of Flow-GANs
In the previous section, we observed that adversarially learn-
ing Flow-GANs models attain poor held-out log-likelihoods.
This makes it challenging to use such models for applica-
tions requiring density estimation. On the other hand, Flow-
GANs learned using MLE are “mode covering” but do not
generate high quality samples. With a Flow-GAN, it is pos-
sible to trade-off the two goals by combining the learning
objectives corresponding to both these inductive principles.
Without loss of generality, let V (Gθ, Dφ) denote the min-
imax objective of any GAN model (such as WGAN). The
hybrid objective of a Flow-GAN can be expressed as:
V (Gθ, Dφ) − λEx∼Pdata [log pθ(x)]

(7)

min
θ

max
φ

where λ ≥ 0 is a hyperparameter for the algorithm. By vary-
ing λ, we can interpolate between plain adversarial training
(λ = 0) and MLE (very high λ).

We summarize the results from MLE, ADV, and Hy-
brid for log-likelihood and sample quality evaluation in Ta-
ble 1 and Table 2 for MNIST and CIFAR-10 respectively.
The tables report the test log-likelihoods corresponding to
the best validated MLE and ADV models and the highest
MODE/Inception scores observed during training. The sam-
ples generated by models with the best MODE/Inception
scores for each objective are shown in Figure 1c.

While the results on CIFAR-10 are along expected lines,
the hybrid objective interestingly outperforms MLE and
ADV on both test log-likelihoods and sample quality met-
rics in the case of MNIST. One potential explanation for this
is that the ADV objective can regularize MLE to generalize
to the test set and in turn, the MLE objective can stabilize
the optimization of the ADV objective. Hence, the hybrid
objective in Eq. (7) can smoothly balance the two objectives
using the tunable hyperparameter λ, and in some cases such
as MNIST, the performance on both tasks could improve as
a result of the hybrid objective.

Table 3: Comparison of inference techniques for negative
log-likelihood estimation of Flow-GAN models on MNIST.

Objective
MLE
ADV
Hybrid

Flow-GAN NLL
-3287.69
26350.30
-3121.53

AIS
-2584.40
-2916.10
-2703.03

KDE
-167.10
-3.03
-205.69

singular value distribution for the Jacobian of an invertible
generator learned using MLE (orange curves) is concen-
trated in a narrow range, and hence the Jacobian matrix is
well-conditioned and easy to invert. In the case of invertible
generators learned using ADV with Wasserstein distance
(green curves) however, the spread of singular values is very
wide, and hence the Jacobian matrix is ill-conditioned.

The average log determinant of the Jacobian matrices for
MLE, ADV, and Hybrid models are −4170.34, −15588.34,
and −5184.40 respectively which translates to the trend
ADV<Hybrid<MLE. This indicates that the ADV models
are trying to squish a sphere of unit volume centered at a la-
tent vector z to a very small volume in the observed space
x. Tiny perturbations of training as well as held-out data-
points can hence manifest as poor log-likelihoods. In spite
of not being limited in the representational capacity to cover
the entire space of the data distribution (the dimensions of z
(i.e., k) and x (i.e., d) match for invertible generators), ADV
prefers to learn a distribution over a smaller support.

The Hybrid learning objective (blue curves), however, is
able to correct for this behavior, and the distribution of sin-
gular value magnitudes matches closely to that of MLE. We
also considered variations involving the standard DCGAN
architectures with k = d minimizing the Wasserstein dis-
tance (red curves) and Jenson-Shannon divergence (purple
curves). The relative shift in distribution of singular value
magnitudes to lower values is apparent even in these cases.

6 Discussion
Any model which allows for efﬁcient likelihood evaluation
and sampling can be trained using maximum likelihood and
adversarial learning. This line of reasoning has been ex-
plored to some extent in prior work that combine the ob-
jectives of prescribed latent variable models such as VAEs
(maximizing an evidence lower bound on the data) with ad-
versarial learning (Larsen et al. 2015; Mescheder, Nowozin,
and Geiger 2017a; Srivastava et al. 2017). However, the ben-
eﬁts of such procedures do not come for “free” since we still
need some form of approximate inference to get a handle
on the log-likelihoods. This could be expensive, for instance
combining a VAE with a GAN introduces an additional in-
ference network that increases the overall model complexity.
Our approach sidesteps the additional complexity due to
approximate inference by considering a normalizing ﬂow
model. The trade-off made by a normalizing ﬂow model
is that the generator function needs to be invertible while
other generative models such as VAEs have no such re-
quirement. On the positive side, we can tractably evaluate
exact log-likelihoods assigned by the model for any data
point. Normalizing ﬂow models have been previously used

Figure 4: CDF of the singular values magnitudes for the Ja-
cobian of the generator functions trained on MNIST.

in the context of maximum likelihood estimation of fully ob-
served and latent variable models (Dinh, Krueger, and Ben-
gio 2014; Rezende and Mohamed 2015; Kingma, Salimans,
and Welling 2016; Dinh, Sohl-Dickstein, and Bengio 2017).
The low dimensional support of the distributions learned
by adversarial learning often manifests as lack of sample di-
versity and is referred to as mode collapse. In prior work,
mode collapse is detected based on visual inspection or
heuristic techniques (Goodfellow 2016; Arora and Zhang
2017). Techniques for avoiding mode collapse explicitly fo-
cus on stabilizing GAN training such as (Metz et al. 2016;
Che et al. 2017; Mescheder, Nowozin, and Geiger 2017b)
rather than quantitative methods based on likelihoods.

7 Conclusion
As an attempt to more quantitatively evaluate generative
models, we introduced Flow-GAN. It is a generative adver-
sarial network which allows for tractable likelihood evalu-
ation, exactly like in a ﬂow model. Since it can be trained
both adversarially (like a GAN) and in terms of MLE (like
a ﬂow model), we can quantitatively evaluate the trade-offs
involved. We observe that adversarial learning assigns very
low-likelihoods to both training and validation data while
generating superior quality samples. To put this observa-
tion in perspective, we demonstrate how a naive Gaussian
mixture model can outperform adversarially learned models
on both log-likelihood estimates and sample quality metrics.
Quantitative evaluation methods based on AIS and KDE fail
to detect this behavior and can be poor approximations of the
true log-likelihood (at least for the models we considered).
Analyzing the Jacobian of the generator provides insights
into the contrast between maximum likelihood estimation
and adversarial learning. The latter have a tendency to learn
distributions of low support, which can lead to low likeli-
hoods. To correct for this behavior, we proposed a hybrid ob-
jective function which involves loss terms corresponding to
both MLE and adversarial learning. The use of such models
in applications requiring both density estimation and sample
generation is an exciting direction for future work.

Acknowledgements
We are thankful to Ben Poole and Daniel Levy for helpful
discussions. This research was supported by a Microsoft Re-
search PhD fellowship in machine learning for the ﬁrst au-
thor, NSF grants #1651565, #1522054, #1733686, a Fu-
ture of Life Institute grant, and Intel.

References
Arjovsky, M.; Chintala, S.; and Bottou, L. 2017. Wasserstein
GAN. In International Conference on Machine Learning.
2017. Do GANs actually
Arora, S., and Zhang, Y.
learn the distribution? An empirical study. arXiv preprint
arXiv:1706.08224.
Besag, J. 1977. Efﬁciency of pseudolikelihood estimation
for simple Gaussian ﬁelds. Biometrika 616–618.
Carreira-Perpinan, M. A., and Hinton, G. E. 2005. On con-
In Artiﬁcial Intelligence and
trastive divergence learning.
Statistics.
Che, T.; Li, Y.; Jacob, A. P.; Bengio, Y.; and Li, W. 2017.
Mode regularized generative adversarial networks. In Inter-
national Conference on Learning Representations.
Diggle, P. J., and Gratton, R. J. 1984. Monte Carlo methods
of inference for implicit statistical models. Journal of the
Royal Statistical Society. 193–227.
Dinh, L.; Krueger, D.; and Bengio, Y. 2014. NICE: Non-
linear independent components estimation. arXiv preprint
arXiv:1410.8516.
Dinh, L.; Sohl-Dickstein, J.; and Bengio, S. 2017. Density
estimation using Real NVP. In International Conference on
Learning Representations.
Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;
Warde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y.
2014. Generative adversarial nets. In Advances in Neural
Information Processing Systems.
Goodfellow, I. 2016. NIPS 2016 tutorial: Generative adver-
sarial networks. arXiv preprint arXiv:1701.00160.
Gulrajani, I.; Ahmed, F.; Arjovsky, M.; Dumoulin, V.; and
Courville, A.
Improved training of Wasserstein
GANs. In Advances in Neural Information Processing Sys-
tems.
Ho, J., and Ermon, S. 2016. Generative adversarial imita-
tion learning. In Advances in Neural Information Processing
Systems.
Huber, P. J. 1967. The behavior of maximum likelihood
In ﬁfth Berkeley
estimates under nonstandard conditions.
symposium on mathematical statistics and probability.
Kingma, D. P., and Welling, M. 2014. Auto-encoding varia-
tional bayes. In International Conference on Learning Rep-
resentations.
Kingma, D. P.; Salimans, T.; and Welling, M. 2016. Improv-
ing variational inference with inverse autoregressive ﬂow. In
International Conference on Learning Representations.
Krizhevsky, A., and Hinton, G. 2009. Learning multiple
layers of features from tiny images. Technical Report.

2017.

Larochelle, H., and Murray, I. 2011. The neural autore-
gressive distribution estimator. In Artiﬁcial Intelligence and
Statistics.
Larsen, A. B. L.; Sønderby, S. K.; Larochelle, H.; and
Winther, O. 2015. Autoencoding beyond pixels using a
learned similarity metric. arXiv preprint arXiv:1512.09300.
LeCun, Y.; Cortes, C.; and Burges, C. J. 2010. MNIST hand-
written digit database. http://yann. lecun. com/exdb/mnist.
Li, Y.; Song, J.; and Ermon, S. 2017.
InfoGAIL: Inter-
pretable imitation learning from visual demonstrations. In
Advances in Neural Information Processing Systems.
Mao, X.; Li, Q.; Xie, H.; Lau, R. Y.; Wang, Z.; and Smolley,
S. P. 2017. Least squares generative adversarial networks.
In International Conference on Computer Vision.
Mescheder, L.; Nowozin, S.; and Geiger, A. 2017a. Adver-
sarial variational Bayes: Unifying variational autoencoders
and generative adversarial networks. In International Con-
ference on Machine Learning.
Mescheder, L.; Nowozin, S.; and Geiger, A. 2017b. The
numerics of GANs. In Advances in Neural Information Pro-
cessing Systems.
Metz, L.; Poole, B.; Pfau, D.; and Sohl-Dickstein, J. 2016.
Unrolled generative adversarial networks. arXiv preprint
arXiv:1611.02163.
Mohamed, S., and Lakshminarayanan, B. 2016. Learn-
arXiv preprint
ing in implicit generative models.
arXiv:1610.03483.
Neal, R. M. 2001. Annealed importance sampling. Statistics
and Computing 11(2):125–139.
Nowozin, S.; Cseke, B.; and Tomioka, R. 2016.
f-GAN:
Training generative neural samplers using variational diver-
In Advances in Neural Information
gence minimization.
Processing Systems.
Oord, A. v. d.; Kalchbrenner, N.; and Kavukcuoglu, K. 2016.
Pixel recurrent neural networks. In International Conference
on Machine Learning.
Ostrovski, G.; Bellemare, M. G.; Oord, A. v. d.; and Munos,
R. 2017. Count-based exploration with neural density mod-
els. In International Conference on Machine Learning.
Parzen, E. 1962. On estimation of a probability density
function and mode. The Annals of Mathematical Statistics
33(3):1065–1076.
Pascual, S.; Bonafonte, A.; and Serr`a, J. 2017. SEGAN:
Speech enhancement generative adversarial network. arXiv
preprint arXiv:1703.09452.
Radford, A.; Metz, L.; and Chintala, S. 2015. Unsupervised
representation learning with deep convolutional generative
adversarial networks. arXiv preprint arXiv:1511.06434.
Rezende, D. J., and Mohamed, S. 2015. Variational infer-
In International Conference
ence with normalizing ﬂows.
on Machine Learning.
Salimans, T.; Goodfellow, I.; Zaremba, W.; Cheung, V.; Rad-
ford, A.; and Chen, X. 2016. Improved techniques for train-
ing GANs. In Advances in Neural Information Processing
Systems.

Song, J.; Zhao, S.; and Ermon, S. 2017. A-NICE-MC: Ad-
versarial training for MCMC. In Advances in Neural Infor-
mation Processing Systems.
Srivastava, A.; Valkov, L.; Russell, C.; Gutmann, M.; and
Sutton, C. 2017. VEEGAN: Reducing mode collapse in
In Advances in
GANs using implicit variational learning.
Neural Information Processing Systems.
Theis, L.; Oord, A. v. d.; and Bethge, M. 2016. A note on
the evaluation of generative models. In International Con-
ference on Learning Representations.
Uria, B.; Murray, I.; and Larochelle, H. 2013. RNADE:
The real-valued neural autoregressive density-estimator. In
Advances in Neural Information Processing Systems.
White, H. 1982. Maximum likelihood estimation of mis-
speciﬁed models. Econometrica: Journal of the Economet-
ric Society 1–25.
Wu, Y.; Burda, Y.; Salakhutdinov, R.; and Grosse, R. 2017.
On the quantitative analysis of decoder-based generative
In International Conference on Learning Repre-
models.
sentations.

Appendices
A Experimental setup details
Datasets. The MNIST dataset contains 50, 000 train,
10, 000 validation, and 10, 000 test images of dimensions
28 × 28 (LeCun, Cortes, and Burges 2010). The CIFAR-10
dataset contains 50, 000 train and 10, 000 test images of di-
mensions 32 × 32 × 3 by default (Krizhevsky and Hinton
2009). We held out a random subset of 5, 000 training set
images as validation set.

Since we are modeling densities for discrete datasets
(pixels can take a ﬁnite set of values ranging from 1 to
255), the model can assign arbitrarily high log-likelihoods
to these discrete points. Following Uria, Murray, and
Larochelle (2013), we dequantize the data by adding uni-
form noise between 0 and 1 to every pixel. Finally, we scale
the pixels to lie in the range [0, 1].

Model priors and hyperparameters. The Flow-GAN
architectures trained on MNIST and CIFAR-10 used a
logistic and an isotropic prior density respectively con-
sistent with prior work (Dinh, Krueger, and Bengio
2014; Dinh, Sohl-Dickstein, and Bengio 2017). Hyper-
parameter details for learning all
the Flow-GAN mod-
els are included in the README of the code repository:
https://github.com/ermongroup/flow-gan

(a) MNIST (MODE)

(b) CIFAR-10 (Inception)

Figure 5: Sample quality curves during training.

B Sample quality
The progression of sample quality metrics for MLE and
ADV objectives during training is shown in Figures 5 (a) and
(b) for MNIST and CIFAR-10 respectively. Higher scores
are reﬂective of better sample quality. ADV (maroon curves)
signiﬁcantly outperform MLE (cyan curves) with respect to
the ﬁnal MODE/Inception scores achieved.

Figure 6: Gaussian Mixture Models outperform adversar-
ially learned models on both held-out log-likelihoods and
sampling metrics on MNIST (green shaded region).

C Gaussian mixture models
The comparison of GMMs with Flow-GANs trained using
adversarial learning is shown in Figure 6. Similar to the ob-
servations made for CIFAR-10, the simple GMM outper-
forms the adversarially learned model with respect to NLLs
and sample quality metrics for any bandwidth parameter
within the green shaded area.

The samples obtained from the GMM are shown in Fig-
ure 7. Since the baseline is ﬁtting Gaussian densities around
every training point, the samples obtained for relatively
small bandwidths are of high quality. Yet, even the held-
out likelihoods for these bandwidths are better than those
of ADV models with the best MODE/Inception scores.

D Explaining log-likelihood trends
The CDF of singular value magnitudes for the CIFAR-10
dataset in Figure 8 again suggests that the Jacobian matrix
for the generator function is ill-conditioned for the ADV
models (green, red, purple curves) since the distributions
have a large spread. Using a hybrid objective (blue curves)
can correct for this behavior with the distribution of singu-
lar values much more concentrated similar to MLE (orange
curves).

The log determinant of the Jacobian for the MLE, ADV,
and Hybrid models are −12818.84, −21848.09, −14729.51
respectively reﬂecting the trend ADV<Hybrid<MLE, pro-
viding further empirical evidence to suggest that adversarial
training shows a strong preference for learning distributions
with smaller support.

(a) σ = 0.1

(b) σ = 0.07

Figure 7: Samples from the Gaussian Mixture Model base-
line for MNIST (top) and CIFAR-10 (bottom) with better
MODE/Inception scores than ADV models.

Figure 8: CDF of singular values magnitudes for the Jaco-
bian of the generator function on CIFAR-10.

Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in
Generative Models

Aditya Grover, Manik Dhar, Stefano Ermon
Computer Science Department
Stanford University
{adityag, dmanik, ermon}@cs.stanford.edu

8
1
0
2
 
n
a
J
 
3
 
 
]

G
L
.
s
c
[
 
 
2
v
8
6
8
8
0
.
5
0
7
1
:
v
i
X
r
a

Abstract

Adversarial learning of probabilistic models has recently
emerged as a promising alternative to maximum likelihood.
Implicit models such as generative adversarial networks
(GAN) often generate better samples compared to explicit
models trained by maximum likelihood. Yet, GANs sidestep
the characterization of an explicit density which makes quan-
titative evaluations challenging. To bridge this gap, we pro-
pose Flow-GANs, a generative adversarial network for which
we can perform exact likelihood evaluation, thus support-
ing both adversarial and maximum likelihood training. When
trained adversarially, Flow-GANs generate high-quality sam-
ples but attain extremely poor log-likelihood scores, inferior
even to a mixture model memorizing the training data; the op-
posite is true when trained by maximum likelihood. Results
on MNIST and CIFAR-10 demonstrate that hybrid training
can attain high held-out likelihoods while retaining visual ﬁ-
delity in the generated samples.

1

Introduction

Highly expressive parametric models have enjoyed great
success in supervised learning, where learning objectives
and evaluation metrics are typically well-speciﬁed and easy
to compute. On the other hand, the learning objective for un-
supervised settings is less clear. At a fundamental level, the
idea is to learn a generative model that minimizes some no-
tion of divergence with respect to the data distribution. Min-
imizing the Kullback-Liebler divergence between the data
distribution and the model, for instance, is equivalent to per-
forming maximum likelihood estimation (MLE) on the ob-
served data. Maximum likelihood estimators are asymptoti-
cally statistically efﬁcient, and serve as natural objectives for
learning prescribed generative models (Mohamed and Lak-
shminarayanan 2016).

In contrast, an alternate principle that has recently at-
tracted much attention is based on adversarial learning,
where the objective is to generate data indistinguishable
from the training data. Adversarially learned models such
as generative adversarial networks (GAN; (Goodfellow et
al. 2014)) can sidestep specifying an explicit density for any
data point and belong to the class of implicit generative mod-
els (Diggle and Gratton 1984).

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

The lack of characterization of an explicit density in
GANs is however problematic for two reasons. Several ap-
plication areas of deep generative models rely on density
estimates; for instance, count based exploration strategies
based on density estimation using generative models have
recently achieved state-of-the-art performance on challeng-
ing reinforcement learning environments (Ostrovski et al.
2017). Secondly, it makes the quantitative evaluation of the
generalization performance of such models challenging. The
typical evaluation criteria based on ad-hoc sample quality
metrics (Salimans et al. 2016; Che et al. 2017) do not address
this issue since it is possible to generate good samples by
memorizing the training data, or missing important modes
of the distribution, or both (Theis, Oord, and Bethge 2016).
Alternatively, density estimates based on approximate in-
ference techniques such as annealed importance sampling
(AIS; (Neal 2001; Wu et al. 2017)) and non-parameteric
methods such as kernel density estimation (KDE; (Parzen
1962; Goodfellow et al. 2014)) are computationally slow
and crucially rely on assumptions of a Gaussian observa-
tion model for the likelihood that could lead to misleading
estimates as we shall demonstrate in this paper.

To sidestep the above issues, we propose Flow-GANs,
a generative adversarial network with a normalizing ﬂow
generator. A Flow-GAN generator transforms a prior noise
density into a model density through a sequence of invert-
ible transformations. By using an invertible generator, Flow-
GANs allow us to tractably evaluate exact likelihoods using
the change-of-variables formula and perform exact posterior
inference over the latent variables while still permitting efﬁ-
cient ancestral sampling, desirable properties of any proba-
bilistic model that a typical GAN would not provide.

Using a Flow-GAN, we perform a principled quantitative
comparison of maximum likelihood and adversarial learning
on benchmark datasets viz. MNIST and CIFAR-10. While
adversarial learning outperforms MLE on sample quality
metrics as expected based on strong evidence in prior work,
the log-likelihood estimates of adversarial learning are or-
ders of magnitude worse than those of MLE. The difference
is so stark that a simple Gaussian mixture model baseline
outperforms adversarially learned models on both sample
quality and held-out likelihoods. Our quantitative analysis
reveals that the poor likelihoods of adversarial learning can
be explained as a result of an ill-conditioned Jacobian ma-

trix for the generator function suggesting a mode collapse,
rather than overﬁtting to the training dataset.

To resolve the dichotomy of perceptually good-looking
samples at the expense of held-out likelihoods in the case
of adversarial learning (and vice versa in the case of MLE),
we propose a hybrid objective that bridges implicit and pre-
scribed learning by augmenting the adversarial training ob-
jective with an additional term corresponding to the log-
likelihood of the observed data. While the hybrid objective
achieves the intended effect of smoothly trading-off the two
goals in the case of CIFAR-10, it has a regularizing effect on
MNIST where it outperforms MLE and adversarial learning
on both held-out likelihoods and sample quality metrics.
Overall, this paper makes the following contributions:

1. We propose Flow-GANs, a generative adversarial net-
work with an invertible generator that can perform efﬁ-
cient ancestral sampling and exact likelihood evaluation.

2. We propose a hybrid learning objective for Flow-GANs
that attains good log-likelihoods and generates high-
quality samples on MNIST and CIFAR-10 datasets.

3. We demonstrate the limitations of AIS and KDE for log-
likelihood evaluation and ranking of implicit models.

4. We analyze the singular value distribution for the Jaco-
bian of the generator function to explain the low log-
likelihoods observed due to adversarial learning.

2 Preliminaries
We begin with a review of maximum likelihood estimation
and adversarial learning in the context of generative models.
For ease of presentation, all distributions are w.r.t. any arbi-
trary x ∈ Rd, unless otherwise speciﬁed. We use upper-case
to denote probability distributions and assume they all admit
absolutely continuous densities (denoted by the correspond-
ing lower-case notation) on a reference measure dx.

Consider the following setting for learning generative
models. Given some data X = {xi ∈ Rd}m
i=1 sampled i.i.d.
from an unknown probability density pdata, we are inter-
ested in learning a probability density pθ where θ denotes the
parameters of a model. Given a parameteric family of mod-
els M, the typical approach to learn θ ∈ M is to minimize
a notion of divergence between Pdata and Pθ. The choice of
divergence and the optimization procedure dictate learning,
leading to the following two objectives.

2.1 Maximum likelihood estimation
In maximum likelihood estimation (MLE), we minimize the
Kullback-Liebler (KL) divergence between the data distri-
bution and the model distribution. Formally, the learning ob-
jective can be expressed as:

min
θ∈M

KL(Pdata, Pθ) = Ex∼Pdata

log

(cid:20)

(cid:21)

pdata(x)
pθ(x)

Since pdata is independent of θ, the above optimization
problem can be equivalently expressed as:
Ex∼Pdata [log pθ(x)]

(1)

max
θ∈M

Hence, evaluating the learning objective for MLE in Eq. (1)
requires the ability to evaluate the model density pθ(x).
Models that provide an explicit characterization of the like-
lihood function are referred to as prescribed generative mod-
els (Mohamed and Lakshminarayanan 2016).

2.2 Adversarial learning
A generative model can be learned to optimize divergence
notions beyond the KL divergence. A large family of diver-
gences can be conveniently expressed as:
Ex∼Pθ [hφ(x)] − Ex∼Pdata

φ(x)(cid:3)

(cid:2)h(cid:48)

(2)

max
φ∈F

where F denotes a set of parameters, hφ and h(cid:48)
φ are ap-
propriate real-valued functions parameterized by φ. Differ-
ent choices of F, hφ and h(cid:48)
φ can lead to a variety of f -
divergences such as Jenson-Shannon divergence and inte-
gral probability metrics such as the Wasserstein distance.
For instance, the GAN objective proposed by Goodfellow
et al. (2014) can also be cast in the form of Eq. (2) below:

max
φ∈F

Ex∼Pθ [log (1 − Dφ(x))] + Ex∼Pdata [Dφ(x)]

(3)

where φ denotes the parameters of a neural network function
Dφ. We refer the reader to (Nowozin, Cseke, and Tomioka
2016; Mescheder, Nowozin, and Geiger 2017b) for further
details on other possible choices of divergences. Impor-
tantly, a Monte Carlo estimate of the objective in Eq. (2)
requires only samples from the model. Hence, any model
that allows tractable sampling can be used to evaluate the
following minimax objective:

min
θ∈M

max
φ∈F

Ex∼Pθ [hφ(x)] − Ex∼Pdata

(cid:2)h(cid:48)

φ(x)(cid:3) .

(4)

As a result, even differentiable implicit models which do
not provide a characterization of the model likelihood1 but
allow tractable sampling can be learned adversarially by op-
timizing minimax objectives of the form given in Eq. (4).

2.3 Adversarial learning of latent variable models
From a statistical perspective, maximum likelihood estima-
tors are statistically efﬁcient asymptotically (under some
conditions) and hence minimizing the KL divergence is a
natural objective for many prescribed models (Huber 1967).
However, not all models allow for a well-deﬁned, tractable,
and easy-to-optimize likelihood.

For example, exact likelihood evaluation and sampling are
tractable in directed, fully observed models such as Bayesian
networks and autoregressive models (Larochelle and Mur-
ray 2011; Oord, Kalchbrenner, and Kavukcuoglu 2016).
Hence, they are usually trained by maximum likelihood.
Undirected models, on the other hand, provide only unnor-
malized likelihoods and are sampled from using expensive
Markov chains. Hence, they are usually learned by approx-
imating the likelihood using methods such as contrastive
divergence (Carreira-Perpinan and Hinton 2005) and pseu-
dolikelihood (Besag 1977). The likelihood is generally in-
tractable to compute in latent variable models (even directed

1This could be either due to computational intractability in eval-

uating likelihoods or because the likelihood is ill-deﬁned.

ones) as it requires marginalization. These models are typi-
cally learned by optimizing a stochastic lower bound to the
log-likelihood using variational Bayes approaches (Kingma
and Welling 2014).

Directed latent variable models allow for efﬁcient ances-
tral sampling and hence these models can also be trained
using other divergences, e.g., adversarially (Mescheder,
Nowozin, and Geiger 2017a; Mao et al. 2017; Song, Zhao,
and Ermon 2017). A popular class of latent variable models
learned adversarially consist of generative adversarial net-
works (GAN; (Goodfellow et al. 2014)). GANs comprise of
a pair of generator and discriminator networks. The gener-
ator Gθ : Rk → Rd is a deterministic function differen-
tiable with respect to the parameters θ. The function takes
as input a source of randomness z ∈ Rk sampled from a
tractable prior density p(z) and transforms it to a sample
Gθ(z) through a forward pass. Evaluating likelihoods as-
signed by a GAN is challenging because the model density
pθ is speciﬁed only implicitly using the prior density p(z)
and the generator function Gθ. In fact, the likelihood for any
data point is ill-deﬁned (with respect to the Lesbegue mea-
sure over Rn) if the prior distribution over z is deﬁned over
a support smaller than the support of the data distribution.

GANs are typically learned adversarially with the help of
a discriminator network. The discriminator Dφ : Rd → R
is another real-valued function that is differentiable with re-
spect to a set of parameters φ. Given the discriminator func-
tion, we can express the functions h and h(cid:48) in Eq. (4) as
compositions of Dφ with divergence-speciﬁc functions. For
instance, the Wasserstein GAN (WGAN; (Arjovsky, Chin-
tala, and Bottou 2017)) optimizes the following objective:

min
θ

max
φ∈F

Ex∼Pdata [Dφ(x)] − Ez∼Pz [Dφ(Gθ(z))]

(5)

where F is deﬁned such that Dφ is 1-Lipschitz. Empirically,
GANs generate excellent samples of natural images (Rad-
ford, Metz, and Chintala 2015), audio signals (Pascual,
Bonafonte, and Serr`a 2017), and of behaviors in imitation
learning (Ho and Ermon 2016; Li, Song, and Ermon 2017).

3 Flow Generative Adversarial Networks
As discussed above, generative adversarial networks can
tractably generate high-quality samples but have intractable
or ill-deﬁned likelihoods. Monte Carlo techniques such as
AIS and non-parameteric density estimation methods such
as KDE get around this by assuming a Gaussian observation
model pθ(x|z) for the generator.2 This assumption alone is
not sufﬁcient for quantitative evaluation since the marginal
likelihood of the observed data, pθ(x) = (cid:82) pθ(x, z)dz in
this case would be intractable as it requires integrating over
all the latent factors of variation. This would then require ap-
proximate inference (e.g., Monte Carlo or variational meth-
ods) which in itself is a computational challenge for high-
dimensional distributions. To circumvent these issues, we
propose ﬂow generative adversarial networks (Flow-GAN).

2The true observation model for a GAN is a Dirac delta distribu-
tion, i.e., pθ(x|z) is inﬁnite when x = Gθ(z) and zero otherwise.

A Flow-GAN consists of a pair of generator-discriminator
networks with the generator speciﬁed as a normalizing ﬂow
model (Dinh, Krueger, and Bengio 2014). A normalizing
ﬂow model speciﬁes a parametric transformation from a
prior density p(z) : Rd → R+
0 to another density over the
same space, pθ(x) : Rd → R+
0 where R+
0 is the set of non-
negative reals. The generator transformation Gθ : Rd →
Rd is invertible, such that there exists an inverse function
fθ = G−1
θ . Using the change-of-variables formula and let-
ting z = fθ(x), we have:

pθ(x) = p(z)

det

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂fθ(x)
∂x

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(6)

∂x

where ∂fθ(x)
denotes the Jacobian of fθ at x. The above
formula can be applied recursively over compositions of
many invertible transformations to produce a complex ﬁnal
density. Hence, we can evaluate and optimize for the log-
likelihood assigned by the model to a data point as long as
the prior density is tractable and the determinant of the Ja-
cobian of fθ evaluated at x can be efﬁciently computed.

Evaluating the likelihood assigned by a Flow-GAN model
in Eq. (6) requires overcoming two major challenges. First,
requiring the generator function Gθ to be reversible imposes
a constraint on the dimensionality of the latent variable z to
match that of the data x. Thereafter, we require the trans-
formations between the various layers of the generator to be
invertible such that their overall composition results in an
invertible Gθ. Secondly, the Jacobian of high-dimensional
distributions can however be computationally expensive to
compute. If the transformations are designed such that the
Jacobian is an upper or lower triangular matrix, then the de-
terminant can be easily evaluated as the product of its diago-
nal entries. We consider two such family of transformations.
1. Volume preserving transformations. Here, the Jacobian of
the transformations have a unit determinant. For exam-
ple, the NICE model consists of several layers perform-
ing a location transformation (Dinh, Krueger, and Bengio
2014). The top layer is a diagonal scaling matrix with non-
zero log determinant.

2. Non-volume preserving transformations. The determinant
of the Jacobian of the transformations is not necessarily
unity. For example, in Real-NVP, layers performs both
location and scale transformations (Dinh, Sohl-Dickstein,
and Bengio 2017).
For brevity, we direct the reader to Dinh, Krueger, and
Bengio (2014) and Dinh, Sohl-Dickstein, and Bengio (2017)
for the speciﬁcations of NICE and Real-NVP respectively.
Crucially, both volume preserving and non-volume preserv-
ing transformations are invertible such that the determinant
of the Jacobian can be computed tractably.

3.1 Learning objectives
In a Flow-GAN, the likelihood is well-deﬁned and compu-
tationally tractable for exact evaluation of even expressive
volume preserving and non-volume preserving transforma-
tions. Hence, a Flow-GAN can be trained via maximum like-
lihood estimation using Eq. (1) in which case the discrimi-
nator is redundant. Additionally, we can perform ancestral

(a) MLE

(b) ADV

(c) Hybrid

Figure 1: Samples generated by Flow-GAN models with different objectives for MNIST (top) and CIFAR-10 (bottom).

sampling just like a regular GAN whereby we sample a ran-
dom vector z ∼ Pz and transform it to a model generated
sample via Gθ = f −1
. This makes it possible to learn a
Flow-GAN using an adversarial learning objective (for ex-
ample, the WGAN objective in Eq. (5)).

θ

A natural question to ask is why should one use adversar-
ial learning given that MLE is statistically efﬁcient asymp-
totically (under some conditions). Besides difﬁculties that
could arise due to optimization (in both MLE and adversar-
ial learning), the optimality of MLE holds only when there is
no model misspeciﬁcation for the generator i.e., the true data
distribution Pdata is a member of the parametric family of
distributions under consideration (White 1982). This is gen-
erally not the case for high-dimensional distributions, and
hence the choice of the learning objective becomes largely
an empirical question. Unlike other models, a Flow-GAN
allows both maximum likelihood and adversarial learning,
and hence we can investigate this question experimentally.

3.2 Evaluation metrics and experimental setup

Our criteria for evaluation is based on held-out
log-
likelihoods and sample quality metrics. We focus on nat-
ural images since they allow visual inspection as well as
quantiﬁcation using recently proposed metrics. A “good”
generative model should generalize to images outside the
training data and assign high log-likelihoods to held-out
data. The Inception and MODE scores are standard quan-
titative measures of the quality of generated samples of
natural images for labelled datasets (Salimans et al. 2016;

Che et al. 2017). The Inception scores are computed as:

exp (Ex∈Pθ [KL(p(y|x)(cid:107)p(y)])
where x is a sample generated by the model, p(y|x) is the
softmax probability for the labels y assigned by a pretrained
classiﬁer for x, and p(y) is the overall distribution of la-
bels in the generated samples (as predicted by the pretrained
classiﬁer). The intuition is that the conditional distribution
p(y|x) should have low entropy for good looking images
while the marginal distribution p(y) has high entropy to en-
sure sample diversity. Hence, a generative model can per-
form well on this metric if the KL divergence between the
two distributions (and consequently, the Inception score for
the generated samples) is large. The MODE score given be-
low modiﬁes the Inception score to take into account the
distribution of labels in the training data, p∗(y):

exp (Ex∈Pθ [KL(p(y|x)(cid:107)p∗(y)] − KL(p∗(y)(cid:107)p(y))) .
We compare learning of Flow-GANs using MLE and ad-
versarial learning (ADV) for the MNIST dataset of hand-
written digits (LeCun, Cortes, and Burges 2010) and the
CIFAR-10 dataset of natural images (Krizhevsky and Hin-
ton 2009). The normalizing ﬂow generator architectures
are chosen to be NICE (Dinh, Krueger, and Bengio 2014)
and Real-NVP (Dinh, Sohl-Dickstein, and Bengio 2017)
for MNIST and CIFAR-10 respectively. We ﬁx the Wasser-
stein distance as the choice of the divergence being op-
timized by ADV (see Eq. (5)) with the Lipschitz con-
straint over the critic imposed by penalizing the norm of
the gradient with respect to the input (Arjovsky, Chintala,
and Bottou 2017; Gulrajani et al. 2017). The discrimina-
tor is based on the DCGAN architecture (Radford, Metz,

and Chintala 2015). The above choices are among the
current state-of-the-art in maximum likelihood estimation
and adversarial learning and greatly stabilize GAN train-
ing. Further experimental setup details are provided in Ap-
pendix A. The code for reproducing the results is available at
https://github.com/ermongroup/flow-gan.

(a) MLE

(b) ADV

Figure 2: Learning curves for negative log-likelihood (NLL)
evaluation on MNIST (top, in nats) and CIFAR (bottom, in
bits/dim). Lower NLLs are better.

3.3 Evaluation results
Log-likelihood. The log-likelihood learning curves for
Flow-GAN models learned using MLE and ADV are shown
in Figure 2a and Figure 2b respectively. Following conven-
tion, we report the negative log-likelihoods (NLL) in nats for
MNIST and bits/dimension for CIFAR-10.

MLE. In Figure 2a, we see that normalizing ﬂow models
attain low validation NLLs (blue curves) after few gradient
updates as expected because it is explicitly optimizing for
the MLE objective in Eq. (1). Continued training however
could lead to overﬁtting as the train NLLs (red curves) begin
to diverge from the validation NLLs.

ADV. Surprisingly, ADV models show a consistent in-
crease in validation NLLs as training progresses as shown
in Figure 2b (for CIFAR-10, the estimates are reported on a
log scale!). Based on the learning curves, we can disregard
overﬁtting as an explanation since the increase in NLLs is
observed even on the training data. The training and vali-
dation NLLs closely track each other suggesting that ADV
models are not simply memorizing the training data.

Comparing the left vs. right panels in Figure 2, we see that
the log-likelihoods attained by ADV are orders of magnitude
worse than those attained by MLE after sufﬁcient training.
Finally, we note that the WGAN loss (green curves) does
not correlate well with NLL estimates. While the WGAN
loss stabilizes after few iterations of training, the NLLs con-
tinue to increase. This observation is in contrast to prior
work showing the loss to be strongly correlated with sam-
ple quality metrics (Arjovsky, Chintala, and Bottou 2017).

Sample quality. Samples generated from MLE and ADV-
based models with the best MODE/Inception are shown
in Figure 1a and Figure 1b respectively. ADV models
signiﬁcantly outperform MLE with respect
to the ﬁnal
MODE/Inception scores achieved. Visual inspection of sam-
ples conﬁrms the observations made on the based of the sam-
ple quality metrics. Curves monitoring the sample quality
metrics at every training iteration are given in Appendix B.

3.4 Gaussian mixture models

The above experiments suggest that ADV can produce ex-
cellent samples but assigns low likelihoods to the observed
data. However, a direct comparison of ADV with the log-
likelihoods of MLE is unfair since the latter is explicitly
optimizing for the desired objective. To highlight that gen-
erating good samples at the expense of low likelihoods is
not a challenging goal, we propose a simple baseline. We
compare the adversarially learned Flow-GAN models that
achieves the highest MODE/Inception score with a Gaussian
Mixture Model consisting of m isotropic Gaussians with
equal weights centered at each of the m training points as
the baseline Gaussian Mixture Model (GMM). The band-
width hyperparameter, σ, is the same for each of the mixture
components and optimized for the lowest validation NLL by
doing a line search in (0, 1]. We show results for CIFAR-
10 in Figure 3. Our observations below hold for MNIST as
well; results deferred to Appendix C.

We overload the y-axis in Figure 3 to report both NLLs
and sample quality metrics. The horizontal maroon and cyan
dashed lines denote the best attainable MODE/Inception
scores and corresponding validation NLLs respectively at-
tained by the adversarially learned Flow-GAN model. The
GMM can clearly attain better sample quality metrics since
it is explicitly overﬁtting to the training data for low values
of the bandwidth parameter (any σ for which the red curve
is above the maroon dashed line). Surprisingly, the simple
GMM also outperforms the adversarially learned model with
respect to NLLs attained for several values of the bandwidth
parameter (any σ for which the blue curve is below the cyan
dashed line). Bandwidth parameters for which GMM mod-
els outperform the adversarially learned model on both log-
likelihoods and sample quality metrics are highlighted using
the green shaded area. We show samples from the GMM
in the appendix. Hence, a trivial baseline that is memo-
rizing the training data can generate high quality samples
and better held-out log-likelihoods, suggesting that the log-
likelihoods attained by adversarial training are very poor.

Table 1: Best MODE scores and test negative log-likelihood
estimates for Flow-GAN models on MNIST.

Objective
MLE
ADV
Hybrid (λ = 0.1)

MODE Score Test NLL (in nats)

7.42
9.24
9.37

−3334.56
−1604.09
−3342.95

Table 2: Best Inception scores and test negative log-
likelihood estimates for Flow-GAN models on CIFAR-10.

Inception Score Test NLL (in bits/dim)

Objective
MLE
ADV
Hybrid (λ = 1)

2.92
5.76
3.90

3.54
8.53
4.21

5

Interpreting the results

Our ﬁndings are in contrast with prior work which report
much better log-likelihoods for adversarially learned mod-
els with a standard generator architecture based on an-
nealed importance sampling (AIS; (Wu et al. 2017)) and
kernel density estimation (KDE; (Goodfellow et al. 2014)).
These methods rely on approximate inference techniques
for log-likelihood evaluation and make assumptions about
a Gaussian observation model which does not hold for
GANs. Since Flow-GANs allow us to compute exact log-
likelihoods, we can evaluate the quality of approximation
made by AIS and KDE for density estimation of invertible
generators. For a detailed description of the methods, we re-
fer the reader to prior work (Neal 2001; Parzen 1962).

We consider the MNIST dataset where these meth-
ods have been previously applied to by Wu et al. (2017)
and Goodfellow et al. (2014) respectively. Since both AIS
and KDE inherently rely on the samples generated, we eval-
uate these methods for the MLE, ADV, and Hybrid Flow-
GAN model checkpoints corresponding to the best MODE
scores observed during training. In Table 3, we observe that
both AIS and KDE produce estimates of log-likelihood that
are far from the ground truth, accessible through the ex-
act Flow-GAN log-likelihoods. Even worse, the ranking of
log-likelihood estimates for AIS (ADV>Hybrid>MLE) and
KDE (Hybrid>MLE>ADV) do not obey the relative rank-
ings of the Flow-GAN estimates (MLE>Hybrid>ADV).

5.1 Explaining log-likelihood trends
In order to explain the variation in log-likelihoods attained
by various Flow-GAN learning objectives, we investigate
the distribution of the magnitudes of singular values for
the Jacobian matrix of several generator functions, Gθ for
MNIST in Figure 4 evaluated at 64 noise vectors z randomly
sampled from the prior density p(z). The x-axis of the ﬁgure
shows the singular value magnitudes on a log scale and for
each singular value s, we show the corresponding cumula-
tive distribution function value on the y-axis which signiﬁes
the fraction of singular values less than s. The results on
CIFAR-10 in Appendix D show a similar trend.

The Jacobian is a good ﬁrst-order approximation of the
generator function locally. In Figure 4, we observe that the

Figure 3: Gaussian Mixture Models outperform adversar-
ially learned models on both held-out log-likelihoods and
sampling metrics on CIFAR-10 (green shaded region).

4 Hybrid learning of Flow-GANs
In the previous section, we observed that adversarially learn-
ing Flow-GANs models attain poor held-out log-likelihoods.
This makes it challenging to use such models for applica-
tions requiring density estimation. On the other hand, Flow-
GANs learned using MLE are “mode covering” but do not
generate high quality samples. With a Flow-GAN, it is pos-
sible to trade-off the two goals by combining the learning
objectives corresponding to both these inductive principles.
Without loss of generality, let V (Gθ, Dφ) denote the min-
imax objective of any GAN model (such as WGAN). The
hybrid objective of a Flow-GAN can be expressed as:
V (Gθ, Dφ) − λEx∼Pdata [log pθ(x)]

(7)

min
θ

max
φ

where λ ≥ 0 is a hyperparameter for the algorithm. By vary-
ing λ, we can interpolate between plain adversarial training
(λ = 0) and MLE (very high λ).

We summarize the results from MLE, ADV, and Hy-
brid for log-likelihood and sample quality evaluation in Ta-
ble 1 and Table 2 for MNIST and CIFAR-10 respectively.
The tables report the test log-likelihoods corresponding to
the best validated MLE and ADV models and the highest
MODE/Inception scores observed during training. The sam-
ples generated by models with the best MODE/Inception
scores for each objective are shown in Figure 1c.

While the results on CIFAR-10 are along expected lines,
the hybrid objective interestingly outperforms MLE and
ADV on both test log-likelihoods and sample quality met-
rics in the case of MNIST. One potential explanation for this
is that the ADV objective can regularize MLE to generalize
to the test set and in turn, the MLE objective can stabilize
the optimization of the ADV objective. Hence, the hybrid
objective in Eq. (7) can smoothly balance the two objectives
using the tunable hyperparameter λ, and in some cases such
as MNIST, the performance on both tasks could improve as
a result of the hybrid objective.

Table 3: Comparison of inference techniques for negative
log-likelihood estimation of Flow-GAN models on MNIST.

Objective
MLE
ADV
Hybrid

Flow-GAN NLL
-3287.69
26350.30
-3121.53

AIS
-2584.40
-2916.10
-2703.03

KDE
-167.10
-3.03
-205.69

singular value distribution for the Jacobian of an invertible
generator learned using MLE (orange curves) is concen-
trated in a narrow range, and hence the Jacobian matrix is
well-conditioned and easy to invert. In the case of invertible
generators learned using ADV with Wasserstein distance
(green curves) however, the spread of singular values is very
wide, and hence the Jacobian matrix is ill-conditioned.

The average log determinant of the Jacobian matrices for
MLE, ADV, and Hybrid models are −4170.34, −15588.34,
and −5184.40 respectively which translates to the trend
ADV<Hybrid<MLE. This indicates that the ADV models
are trying to squish a sphere of unit volume centered at a la-
tent vector z to a very small volume in the observed space
x. Tiny perturbations of training as well as held-out data-
points can hence manifest as poor log-likelihoods. In spite
of not being limited in the representational capacity to cover
the entire space of the data distribution (the dimensions of z
(i.e., k) and x (i.e., d) match for invertible generators), ADV
prefers to learn a distribution over a smaller support.

The Hybrid learning objective (blue curves), however, is
able to correct for this behavior, and the distribution of sin-
gular value magnitudes matches closely to that of MLE. We
also considered variations involving the standard DCGAN
architectures with k = d minimizing the Wasserstein dis-
tance (red curves) and Jenson-Shannon divergence (purple
curves). The relative shift in distribution of singular value
magnitudes to lower values is apparent even in these cases.

6 Discussion
Any model which allows for efﬁcient likelihood evaluation
and sampling can be trained using maximum likelihood and
adversarial learning. This line of reasoning has been ex-
plored to some extent in prior work that combine the ob-
jectives of prescribed latent variable models such as VAEs
(maximizing an evidence lower bound on the data) with ad-
versarial learning (Larsen et al. 2015; Mescheder, Nowozin,
and Geiger 2017a; Srivastava et al. 2017). However, the ben-
eﬁts of such procedures do not come for “free” since we still
need some form of approximate inference to get a handle
on the log-likelihoods. This could be expensive, for instance
combining a VAE with a GAN introduces an additional in-
ference network that increases the overall model complexity.
Our approach sidesteps the additional complexity due to
approximate inference by considering a normalizing ﬂow
model. The trade-off made by a normalizing ﬂow model
is that the generator function needs to be invertible while
other generative models such as VAEs have no such re-
quirement. On the positive side, we can tractably evaluate
exact log-likelihoods assigned by the model for any data
point. Normalizing ﬂow models have been previously used

Figure 4: CDF of the singular values magnitudes for the Ja-
cobian of the generator functions trained on MNIST.

in the context of maximum likelihood estimation of fully ob-
served and latent variable models (Dinh, Krueger, and Ben-
gio 2014; Rezende and Mohamed 2015; Kingma, Salimans,
and Welling 2016; Dinh, Sohl-Dickstein, and Bengio 2017).
The low dimensional support of the distributions learned
by adversarial learning often manifests as lack of sample di-
versity and is referred to as mode collapse. In prior work,
mode collapse is detected based on visual inspection or
heuristic techniques (Goodfellow 2016; Arora and Zhang
2017). Techniques for avoiding mode collapse explicitly fo-
cus on stabilizing GAN training such as (Metz et al. 2016;
Che et al. 2017; Mescheder, Nowozin, and Geiger 2017b)
rather than quantitative methods based on likelihoods.

7 Conclusion
As an attempt to more quantitatively evaluate generative
models, we introduced Flow-GAN. It is a generative adver-
sarial network which allows for tractable likelihood evalu-
ation, exactly like in a ﬂow model. Since it can be trained
both adversarially (like a GAN) and in terms of MLE (like
a ﬂow model), we can quantitatively evaluate the trade-offs
involved. We observe that adversarial learning assigns very
low-likelihoods to both training and validation data while
generating superior quality samples. To put this observa-
tion in perspective, we demonstrate how a naive Gaussian
mixture model can outperform adversarially learned models
on both log-likelihood estimates and sample quality metrics.
Quantitative evaluation methods based on AIS and KDE fail
to detect this behavior and can be poor approximations of the
true log-likelihood (at least for the models we considered).
Analyzing the Jacobian of the generator provides insights
into the contrast between maximum likelihood estimation
and adversarial learning. The latter have a tendency to learn
distributions of low support, which can lead to low likeli-
hoods. To correct for this behavior, we proposed a hybrid ob-
jective function which involves loss terms corresponding to
both MLE and adversarial learning. The use of such models
in applications requiring both density estimation and sample
generation is an exciting direction for future work.

Acknowledgements
We are thankful to Ben Poole and Daniel Levy for helpful
discussions. This research was supported by a Microsoft Re-
search PhD fellowship in machine learning for the ﬁrst au-
thor, NSF grants #1651565, #1522054, #1733686, a Fu-
ture of Life Institute grant, and Intel.

References
Arjovsky, M.; Chintala, S.; and Bottou, L. 2017. Wasserstein
GAN. In International Conference on Machine Learning.
2017. Do GANs actually
Arora, S., and Zhang, Y.
learn the distribution? An empirical study. arXiv preprint
arXiv:1706.08224.
Besag, J. 1977. Efﬁciency of pseudolikelihood estimation
for simple Gaussian ﬁelds. Biometrika 616–618.
Carreira-Perpinan, M. A., and Hinton, G. E. 2005. On con-
In Artiﬁcial Intelligence and
trastive divergence learning.
Statistics.
Che, T.; Li, Y.; Jacob, A. P.; Bengio, Y.; and Li, W. 2017.
Mode regularized generative adversarial networks. In Inter-
national Conference on Learning Representations.
Diggle, P. J., and Gratton, R. J. 1984. Monte Carlo methods
of inference for implicit statistical models. Journal of the
Royal Statistical Society. 193–227.
Dinh, L.; Krueger, D.; and Bengio, Y. 2014. NICE: Non-
linear independent components estimation. arXiv preprint
arXiv:1410.8516.
Dinh, L.; Sohl-Dickstein, J.; and Bengio, S. 2017. Density
estimation using Real NVP. In International Conference on
Learning Representations.
Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;
Warde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y.
2014. Generative adversarial nets. In Advances in Neural
Information Processing Systems.
Goodfellow, I. 2016. NIPS 2016 tutorial: Generative adver-
sarial networks. arXiv preprint arXiv:1701.00160.
Gulrajani, I.; Ahmed, F.; Arjovsky, M.; Dumoulin, V.; and
Courville, A.
Improved training of Wasserstein
GANs. In Advances in Neural Information Processing Sys-
tems.
Ho, J., and Ermon, S. 2016. Generative adversarial imita-
tion learning. In Advances in Neural Information Processing
Systems.
Huber, P. J. 1967. The behavior of maximum likelihood
In ﬁfth Berkeley
estimates under nonstandard conditions.
symposium on mathematical statistics and probability.
Kingma, D. P., and Welling, M. 2014. Auto-encoding varia-
tional bayes. In International Conference on Learning Rep-
resentations.
Kingma, D. P.; Salimans, T.; and Welling, M. 2016. Improv-
ing variational inference with inverse autoregressive ﬂow. In
International Conference on Learning Representations.
Krizhevsky, A., and Hinton, G. 2009. Learning multiple
layers of features from tiny images. Technical Report.

2017.

Larochelle, H., and Murray, I. 2011. The neural autore-
gressive distribution estimator. In Artiﬁcial Intelligence and
Statistics.
Larsen, A. B. L.; Sønderby, S. K.; Larochelle, H.; and
Winther, O. 2015. Autoencoding beyond pixels using a
learned similarity metric. arXiv preprint arXiv:1512.09300.
LeCun, Y.; Cortes, C.; and Burges, C. J. 2010. MNIST hand-
written digit database. http://yann. lecun. com/exdb/mnist.
Li, Y.; Song, J.; and Ermon, S. 2017.
InfoGAIL: Inter-
pretable imitation learning from visual demonstrations. In
Advances in Neural Information Processing Systems.
Mao, X.; Li, Q.; Xie, H.; Lau, R. Y.; Wang, Z.; and Smolley,
S. P. 2017. Least squares generative adversarial networks.
In International Conference on Computer Vision.
Mescheder, L.; Nowozin, S.; and Geiger, A. 2017a. Adver-
sarial variational Bayes: Unifying variational autoencoders
and generative adversarial networks. In International Con-
ference on Machine Learning.
Mescheder, L.; Nowozin, S.; and Geiger, A. 2017b. The
numerics of GANs. In Advances in Neural Information Pro-
cessing Systems.
Metz, L.; Poole, B.; Pfau, D.; and Sohl-Dickstein, J. 2016.
Unrolled generative adversarial networks. arXiv preprint
arXiv:1611.02163.
Mohamed, S., and Lakshminarayanan, B. 2016. Learn-
arXiv preprint
ing in implicit generative models.
arXiv:1610.03483.
Neal, R. M. 2001. Annealed importance sampling. Statistics
and Computing 11(2):125–139.
Nowozin, S.; Cseke, B.; and Tomioka, R. 2016.
f-GAN:
Training generative neural samplers using variational diver-
In Advances in Neural Information
gence minimization.
Processing Systems.
Oord, A. v. d.; Kalchbrenner, N.; and Kavukcuoglu, K. 2016.
Pixel recurrent neural networks. In International Conference
on Machine Learning.
Ostrovski, G.; Bellemare, M. G.; Oord, A. v. d.; and Munos,
R. 2017. Count-based exploration with neural density mod-
els. In International Conference on Machine Learning.
Parzen, E. 1962. On estimation of a probability density
function and mode. The Annals of Mathematical Statistics
33(3):1065–1076.
Pascual, S.; Bonafonte, A.; and Serr`a, J. 2017. SEGAN:
Speech enhancement generative adversarial network. arXiv
preprint arXiv:1703.09452.
Radford, A.; Metz, L.; and Chintala, S. 2015. Unsupervised
representation learning with deep convolutional generative
adversarial networks. arXiv preprint arXiv:1511.06434.
Rezende, D. J., and Mohamed, S. 2015. Variational infer-
In International Conference
ence with normalizing ﬂows.
on Machine Learning.
Salimans, T.; Goodfellow, I.; Zaremba, W.; Cheung, V.; Rad-
ford, A.; and Chen, X. 2016. Improved techniques for train-
ing GANs. In Advances in Neural Information Processing
Systems.

Song, J.; Zhao, S.; and Ermon, S. 2017. A-NICE-MC: Ad-
versarial training for MCMC. In Advances in Neural Infor-
mation Processing Systems.
Srivastava, A.; Valkov, L.; Russell, C.; Gutmann, M.; and
Sutton, C. 2017. VEEGAN: Reducing mode collapse in
In Advances in
GANs using implicit variational learning.
Neural Information Processing Systems.
Theis, L.; Oord, A. v. d.; and Bethge, M. 2016. A note on
the evaluation of generative models. In International Con-
ference on Learning Representations.
Uria, B.; Murray, I.; and Larochelle, H. 2013. RNADE:
The real-valued neural autoregressive density-estimator. In
Advances in Neural Information Processing Systems.
White, H. 1982. Maximum likelihood estimation of mis-
speciﬁed models. Econometrica: Journal of the Economet-
ric Society 1–25.
Wu, Y.; Burda, Y.; Salakhutdinov, R.; and Grosse, R. 2017.
On the quantitative analysis of decoder-based generative
In International Conference on Learning Repre-
models.
sentations.

Appendices
A Experimental setup details
Datasets. The MNIST dataset contains 50, 000 train,
10, 000 validation, and 10, 000 test images of dimensions
28 × 28 (LeCun, Cortes, and Burges 2010). The CIFAR-10
dataset contains 50, 000 train and 10, 000 test images of di-
mensions 32 × 32 × 3 by default (Krizhevsky and Hinton
2009). We held out a random subset of 5, 000 training set
images as validation set.

Since we are modeling densities for discrete datasets
(pixels can take a ﬁnite set of values ranging from 1 to
255), the model can assign arbitrarily high log-likelihoods
to these discrete points. Following Uria, Murray, and
Larochelle (2013), we dequantize the data by adding uni-
form noise between 0 and 1 to every pixel. Finally, we scale
the pixels to lie in the range [0, 1].

Model priors and hyperparameters. The Flow-GAN
architectures trained on MNIST and CIFAR-10 used a
logistic and an isotropic prior density respectively con-
sistent with prior work (Dinh, Krueger, and Bengio
2014; Dinh, Sohl-Dickstein, and Bengio 2017). Hyper-
parameter details for learning all
the Flow-GAN mod-
els are included in the README of the code repository:
https://github.com/ermongroup/flow-gan

(a) MNIST (MODE)

(b) CIFAR-10 (Inception)

Figure 5: Sample quality curves during training.

B Sample quality
The progression of sample quality metrics for MLE and
ADV objectives during training is shown in Figures 5 (a) and
(b) for MNIST and CIFAR-10 respectively. Higher scores
are reﬂective of better sample quality. ADV (maroon curves)
signiﬁcantly outperform MLE (cyan curves) with respect to
the ﬁnal MODE/Inception scores achieved.

Figure 6: Gaussian Mixture Models outperform adversar-
ially learned models on both held-out log-likelihoods and
sampling metrics on MNIST (green shaded region).

C Gaussian mixture models
The comparison of GMMs with Flow-GANs trained using
adversarial learning is shown in Figure 6. Similar to the ob-
servations made for CIFAR-10, the simple GMM outper-
forms the adversarially learned model with respect to NLLs
and sample quality metrics for any bandwidth parameter
within the green shaded area.

The samples obtained from the GMM are shown in Fig-
ure 7. Since the baseline is ﬁtting Gaussian densities around
every training point, the samples obtained for relatively
small bandwidths are of high quality. Yet, even the held-
out likelihoods for these bandwidths are better than those
of ADV models with the best MODE/Inception scores.

D Explaining log-likelihood trends
The CDF of singular value magnitudes for the CIFAR-10
dataset in Figure 8 again suggests that the Jacobian matrix
for the generator function is ill-conditioned for the ADV
models (green, red, purple curves) since the distributions
have a large spread. Using a hybrid objective (blue curves)
can correct for this behavior with the distribution of singu-
lar values much more concentrated similar to MLE (orange
curves).

The log determinant of the Jacobian for the MLE, ADV,
and Hybrid models are −12818.84, −21848.09, −14729.51
respectively reﬂecting the trend ADV<Hybrid<MLE, pro-
viding further empirical evidence to suggest that adversarial
training shows a strong preference for learning distributions
with smaller support.

(a) σ = 0.1

(b) σ = 0.07

Figure 7: Samples from the Gaussian Mixture Model base-
line for MNIST (top) and CIFAR-10 (bottom) with better
MODE/Inception scores than ADV models.

Figure 8: CDF of singular values magnitudes for the Jaco-
bian of the generator function on CIFAR-10.

