5
1
0
2
 
n
u
J
 
0
3
 
 
]

G
L
.
s
c
[
 
 
2
v
0
9
2
0
0
.
5
0
5
1
:
v
i
X
r
a

Algorithms for Lipschitz Learning on Graphs ∗†

Rasmus Kyng
Yale University
rasmus.kyng@yale.edu

Anup Rao
Yale University
anup.rao@yale.edu

Sushant Sachdeva
Yale University
sachdeva@cs.yale.edu

Daniel A. Spielman
Yale University
spielman@cs.yale.edu

July 1, 2015

Abstract

We develop fast algorithms for solving regression problems on graphs where one is given the value of a function
at some vertices, and must ﬁnd its smoothest possible extension to all vertices. The extension we compute is the
absolutely minimal Lipschitz extension, and is the limit for large p of p-Laplacian regularization. We present an
algorithm that computes a minimal Lipschitz extension in expected linear time, and an algorithm that computes
an absolutely minimal Lipschitz extension in expected time eO(mn). The latter algorithm has variants that seem
to run much faster in practice. These extensions are particularly amenable to regularization: we can perform l0-
regularization on the given values in polynomial time and l1-regularization on the initial function values and on graph
edge weights in time eO(m3/2).

Our deﬁnitions and algorithms naturally extend to directed graphs.

1 Introduction

We consider a problem in which we are given a weighted undirected graph G = (V, E, ℓ) and values v0 : T → R
on a subset T of its vertices. We view the weights ℓ as indicating the lengths of edges, with shorter length indicating
greater similarity. Our goal it to assign values to every vertex v ∈ V \T so that the values assigned are as smooth as
possible across edges. A minimal Lipschitz extension of v0 is a vector v that minimizes

max
(x,y)∈E

(ℓ(x, y))−1

v(x) − v(y)

,

(cid:12)
(cid:12)

(cid:12)
(cid:12)

subject to v(x) = v0(x) for all x ∈ T . We call such a vector an inf-minimizer. Inf-minimizers are not unique. So,
among inf-minimizers we seek vectors that minimize the second-largest absolute value of ℓ(x, y)−1
v(x) − v(y)
across edges, and then the third-largest given that, and so on. We call such a vector v a lex-minimizer. It is also known
(cid:12)
as an absolutely minimal Lipschitz extension of v0.
(cid:12)
These are the limit of the solution to p-Laplacian minimization problems for large p, namely the vectors that solve

(cid:12)
(cid:12)

(1)

(2)

min
v∈Rn

v|T =v0|T X(x,y)∈E

(ℓ(x, y))−p|v(x) − v(y)|p.

The use of p = 2 was suggested in the foundational paper of Zhu et al. (2003), and is particularly nice because it can
be obtained by solving a system of linear equations in a symmetric diagonally dominant matrix, which can be done

∗This research was partially supported by AFOSR Award FA9550-12-1-0175, NSF grant CCF-1111257, a Simons Investigator Award to Daniel

Spielman, and a MacArthur Fellowship.

†Code used in this work is available at https://github.com/danspielman/YINSlex

1

very quickly (Cohen et al. (2014)). The use of larger values of p has been discussed by Alamgir and Luxburg (2011),
and by Bridle and Zhu (2013), but it is much more complicated to compute. The fastest algorithms we know for this
problem require convex programming, and then require very high accuracy to obtain the values at most vertices. By
taking the limit as p goes to inﬁnity, we recover the lex-minimizer, which we will show can be computed quickly.

The lex-minimization problem has a remarkable amount of structure. For example, in uniformly weighted graphs
the value of the lex-minimizer at every vertex not in T is equal to the average of the minimum and maximum of the
values at its neighbors. This is analogous to the property of the 2-Laplacian minimizer that the value at every vertex
not in T equals the average of the values at its neighbors.

1.1 Contributions

We ﬁrst present several important structural properties of lex-minimizers in Section 3.2. As we shall point out, some
of these were known from previous work, sometimes in restricted settings. We state them generally and prove them
for completeness. We also prove that the lex-minimizer is as stable as possible under perturbations of v0 (Section 3.1).
The structure of the lex-minimization problem has led us to develop elegant algorithms for its solution. Both the
algorithms and their analyses could be taught to undergraduates. We believe that these algorithms could be used in
place of 2-Laplacian minimization in many applications.

We present algorithms for the following problems. Throughout, m = |E| and n = |V |.

Inf-minimization: An algorithm that runs in expected time O(m + n log n) (Section 4.3).

Lex-minimization: An algorithm that runs in expected time O(n(m + n log n)) (Section 4), along with a variant that

runs quickly in practice (Section 4.4).

l1-regularization of edge lengths for inf-minimization: The problem of minimizing (1) given a limited budget with
O(m3/2)
which one can increase edge lengths is a linear programming problem. We show how to solve it in time
with an interior point method by using fast Laplacian solvers (Section 8). The same algorithm can accommodate
l1-regularization of the values given in v0.

e

l0-regularization of vertex values for inf-minimization: We give a polynomial time algorithm for l0-regularization
of the values at vertices. That is, we minimize (1) given a budget of a number of vertices that can be proclaimed
outliers and removed from T (Section 7.1). We solve this problem by reducing it to the problem of computing
minimum vertex covers on transitively closed directed acyclic graphs, a special case of minimum vertex cover
that can be solved in polynomial time.

After any regularization for inf-minimization, we suggest computing the lex-minimizer. We ﬁnd the result for l0-
regularization of vertex values to be particularly surprising, especially because we prove that the analogous problem
for 2-Laplacian minimization is NP-Hard (Section 7.2).

All of our algorithms extend naturally to directed graphs (Section 5). This is in contrast with the problem of
minimizing 2-Laplacians on directed graphs, which corresponds to computing electrical ﬂows in networks of resistors
and diodes, for which fast algorithms are not presently known.

We present a few experiments on examples demonstrating that the lex-minimizer can overcome known deﬁcien-
cies of the 2-Laplacian minimizer (Section 1.2, Figures 1,2), as well as a demonstration of the performance of the
directed analog of our algorithms on the WebSpam dataset of Castillo et al. (2006) (Section 6). In the WebSpam prob-
lem we use the link structure of a collection of web sites to ﬂag some sites as spam, given a small number of labeled
sites known to be spam or normal.

1.2 Relation to Prior Work

We ﬁrst encountered the idea of using the minimizer of the 2-Laplacian given by (2) for regression and classiﬁca-
tion on graphs in the work of Zhu et al. (2003) and Belkin et al. (2004) on semi-supervised learning. These works
transformed learning problems on sets of vectors into problems on graphs by identifying vectors with vertices and
constructing graphs with edges between nearby vectors. One shortcoming of this approach (see Nadler et al. (2009),

2

e
g
a

t
l

 

o
V
d
e
r
r
e

f

n

I

1

0.8

0.6

0.4

0.2

0

-0.2

-0.4

-0.6

-0.8

-1

-4

50 lex
50 l2
100 lex
100 l2
500 lex
500 l2
1000 lex
1000 l2

0.25

0.2

r
o
r
r
e
 
1
l
 
n
a
e
M

0.15

0.1

0.05

0
5000 

lex
2-Lap
labels

-2

0

2
Vertex position on real line

4

6

8

Figure 1: Lex vs 2-Laplacian on 1D gaussian clus-
ters.

Figure 2: kNN graphs on samples from 4D cube.

10000

20000

40000

80000

Number of Vertices

Alamgir and Luxburg (2011), Bridle and Zhu (2013)) is that if the number of vectors grows while the number of la-
beled vectors remains ﬁxed, then almost all the values of the 2-Laplacian minimizer converge to the mean of the
labels on most natural examples. For example, Nadler et al. (2009) consider sampling points from two Gaussian
distributions centered at 0 and 4 on the real line. They place edges between every pair of points (x, y) with length
exp(|x − y|2 /2σ2) for σ = 0.4, and provide only the labels v0(0) = −1 and v0(4) = 1. Figure 1 shows the values
of the 2-Laplacian minimizer in red, which are all approximately zero. In contrast, the values of the lex-minimizer in
blue, which are smoothly distributed between the labeled points, are shown.

The “manifold hypothesis” (see Chapelle et al. (2010), Ma and Fu (2011)) holds that much natural data lies near a
low-dimensional manifold and that natural functions we would like to learn on this data are smooth functions on the
manifold. Under this assumption, one should expect lex-minimizers to interpolate well. In contrast, the 2-Laplacian
minimizers degrade (dotted lines) if the number of labeled points remains ﬁxed while the total number of points grows.
In Figure 2, we demonstrate this by sampling many points uniformly from the unit cube in 4 dimensions, form their
8-nearest neighbor graph, and consider the problem of regressing the ﬁrst coordinate. We performed 8 experiments,
varying the number of labeled points in {50, 100, 500, 1000}. Each data point is the mean average l1 error over 100
experiments. The plots for root mean squared error are similar. The standard deviation of the estimations of the mean
are within one pixel, and so are not displayed. The performance of the lex-minimizer (solid lines) does not degrade as
the number of unlabeled points grows.

Analogous to our inf-minimizers, minimal Lipschitz extensions of functions in Euclidean space and over more
general metric spaces have been studied extensively in Mathematics (Kirszbraun (1934), McShane (1934), Whitney
(1934)). von Luxburg and Bousquet (2003) employ Lipschitz extensions on metric spaces for classiﬁcation and relate
these to Support Vector Machines. Their work inspired improvements in classiﬁcation and regression in metric spaces
with low doubling dimension (Gottlieb et al. (2013), Gottlieb et al. (2013b)). Theoretically fast, although not actually
practical, algorithms have been given for constructing minimal Lipschitz extensions of functions on low-dimensional
Euclidean spaces (Fefferman (2009a), Fefferman and Klartag (2009), Fefferman (2009b)). Sinop and Grady (2007)
suggest using inf-minimizers for binary classiﬁcation problems on graphs. For this special case, where all of the
given values are either 0 or 1, they present an O(m + n log n) time algorithm for computing an inf-minimizer. The
case of general given values, which we solve in this paper, is much more complicated. To compensate for the non-
uniqueness of inf-minimizers, they suggest choosing the inf-minimizer that minimizes (2) with p = 2. We believe that
the lex-minimizer is a more natural choice.

The analog of our lex-minimizer over continuous spaces is called the absolutely minimal Lipschitz extension
(AMLE). Starting with the work of Aronsson (1967), there have been several characterizations and proofs of the ex-
istence and uniqueness of the AMLE (Jensen (1993), Crandall et al. (2001), Barles and Busca (2001), Aronsson et al.
(2004)). Many of these results were later extended to general metric spaces, including graphs (Milman (1999),
Peres et al. (2011), Naor and Shefﬁeld (2010), Shefﬁeld and Smart (2010)). However, to the best of our knowledge,
fast algorithms for computing lex-minimizers on graphs were not known. For the special case of undirected, un-
weighted graphs, Lazarus et al. (1999) presented both a polynomial-time algorithm and an iterative method. Oberman

3

(2011) suggested computing the AMLE in Euclidean space by ﬁrst discretizing the problem and then solving the cor-
responding graph problem by an iterative method. However, no run-time guarantees were obtained for either iterative
method.

2 Notation and Basic Deﬁnitions

Lexicographic Ordering. Given a vector r ∈ Rm, let πr denote a permutation that sorts r in non-increasing order
by absolute value, i.e., ∀i ∈ [m − 1], |r(πr(i))| ≥ |r(πr(i + 1))|. Given two vectors r, s ∈ Rm, we write r (cid:22) s to
indicate that r is smaller than s in the lexicographic ordering on sorted absolute values, i.e.

∃j ∈ [m],

r(πr(j))

<

s(πs(j))

and ∀i ∈ [j − 1],

r(πr(i))

=

s(πs(i))

or ∀i ∈ [m],

=

r(πr(i))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.
(cid:12)
(cid:12)

s(πs(i))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

Note that it is possible that r (cid:22) s and s (cid:22) r while r 6= s. It is a total relation: for every r and s at least one of r (cid:22) s
or s (cid:22) r is true.

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Graphs and Matrices. We will work with weighted graphs. Unless explicitly stated, we will assume that they are
undirected. For a graph G, we let VG be its set of vertices, EG be its set of edges, and ℓG : EG → R+ be the
assignment of positive lengths to the edges. We let |VG| = n, and |EG| = m. We assume ℓG is symmetric, i.e.,
ℓG(x, y) = ℓG(y, x). When G is clear from the context, we drop the subscript.

A path P in G is an ordered sequence of (not necessarily distinct) vertices P = (x0, x1, . . . , xk), such that
(xi−1, xi) ∈ E for i ∈ [k]. The endpoints of P are denoted by ∂0P = x0, ∂1P = xk. The set of interior vertices
of P is deﬁned to be int(P ) = {xi : 0 < i < k}. For 0 ≤ i < j ≤ k, we use the notation P [xi : xj] to denote the
k
subpath (xi, . . . , xj). The length of P is ℓ(P ) =
i=1 ℓ(xi−1, xi).
A function v0 : V → R ∪ {∗} is called a voltage assignment (to G). A vertex x ∈ V is a terminal with
respect to v0 iff v0(x) 6= ∗. The other vertices, for which v0(x) = ∗, are non-terminals. We let T (v0) denote the
set of terminals with respect to v0. If T (v0) = V, we call v0 a complete voltage assignment (to G). We say that an
assignment v : V → R ∪ {∗} extends v0 if v(x) = v0(x) for all x such that v0(x) 6= ∗.

Given an assignment v0 : V → R ∪ {∗}, and two terminals x, y ∈ T (v0) for which (x, y) ∈ E, we deﬁne the

P

gradient on (x, y) due to v0 to be

gradG[v0](x, y) =

v0(x) − v0(y)
ℓ(x, y)

.

It may be useful to view gradG[v0](x, y) as the current in the edge (x, y) induced by voltages v0. When v0 is a
complete voltage assignment, we interpret gradG[v0] as a vector in Rm, with one entry for each edge. However, for
convenience, we deﬁne gradG[v0](x, y) = −gradG[v0](y, x). When G is clear from the context, we drop the subscript.
A graph G along with a voltage assignment v to G is called a partially-labeled graph, denoted (G, v). We say
that a partially-labeled graph (G, v0) is a well-posed instance if for every maximal connected component H of G, we
have T (v0) ∩ VH 6= ∅.

A path P in a partially-labeled graph (G, v0) is called a terminal path if both endpoints are terminals. We deﬁne

∇P (v0) to be its gradient:

∇P (v0) =

v0(∂0P ) − v0(∂1P )
ℓ(P )

.

If P contains no terminal-terminal edges (and hence, contains at least one non-terminal), it is a free terminal path.

Lex-Minimization. An instance of the LEX-MINIMIZATION problem is described by a partially-labeled graph
(G, v0). The objective is to compute a complete voltage assignment v : VG → R extending v0 that lex-minimizes
grad[v].

Deﬁnition 2.1 (Lex-minimizer) Given a partially-labeled graph (G, v0), we deﬁne lexG[v0] to be a complete voltage
assignment to V that extends v0, and such that for every other complete assignment v′ : VG → R that extends v0, we
have gradG[lexG[v0]] (cid:22) gradG[v′]. That is, lexG[v0] achieves a lexicographically-minimal gradient assignment to the
edges.

We call lexG[v0] the lex-minimizer for (G, v0). Note that if T (v0) = VG, then trivially, lexG[v0] = v0.

4

3 Basic Properties of Lex-Minimizers

Lazarus et al. (1999) established that lex-minimizers in unweighted and undirected graphs exist, are unique, and may
be computed by an elementary meta-algorithm. We state and prove these facts for undirected weighted graphs, and
defer the discussion of the directed case to Section 5. We also state for directed and weighted graphs characterizations
of lex-minimizers that were established by Peres et al. (2011), Naor and Shefﬁeld (2010) and Shefﬁeld and Smart
(2010) for unweighted graphs. These results are essential for the analyses of our algorithms. We defer most proofs to
Appendix A.

Deﬁnition 3.1 A steepest ﬁxable path in an instance (G, v0) is a free terminal path P that has the largest gradient
∇P (v0) amongst such paths.

Observe that a steepest ﬁxable path with ∇P (v0) 6= 0 must be a simple path.
Deﬁnition 3.2 Given a steepest ﬁxable path P in an instance (G, v0), we deﬁne ﬁxG[v0, P ] : VG → R ∪ {∗} to be the
voltage assignment deﬁned as follows

ﬁxG[v0, P ](x) =

v0(∂0P ) − ∇P (v0) · ℓG(P [∂0P : x]) x ∈ int(P ) \ T (v0),
v0(x)

otherwise.

(

We say that the vertices x ∈ int(P ) are ﬁxed by the operation ﬁx[v0, P ]. If we deﬁne v1 = ﬁxG[v0, P ], where
P = (x0, . . . , xr) is the steepest ﬁxable path in (G, v0), then it is easy to argue that for every i ∈ [r], we have
grad[v1](xi−1, xi) = ∇P (see Lemma A.5). The meta-algorithm META-LEX, spelled out as Algorithm 1, entails
repeatedly ﬁxing steepest ﬁxable paths. While it is possible to have multiple steepest ﬁxable paths, the result of ﬁxing
all of them does not depend on the order in which they are ﬁxed.

Theorem 3.3 Given a well-posed instance (G, v0), the meta-algorithm META-LEX, which repeatedly ﬁxes steepest
ﬁxable paths, produces the unique lex-minimizer extending v0.

Corollary 3.4 Given a well-posed instance (G, v0) such that T (v0) 6= VG, let P be a steepest ﬁxable path in (G, v0).
Then, (G, ﬁx[v0, P ]) is also a well-posed instance, and lexG[ﬁx[v0, P ]] = lexG[v0].

Since a lex-minimal element must be an inf-minimizer, we also obtain the following corollary, that can also be

proved using LP duality.

Lemma 3.5 Suppose we have a well-posed instance (G, v0). Then, there exists a complete voltage assignment v
extending v0 such that

grad[v]

∞ ≤ α, iff every terminal path P in (G, v0) satisﬁes ∇P (v0) ≤ α.
(cid:13)
(cid:13)

3.1 Stability

(cid:13)
(cid:13)

The following theorem states that lexG[v0] is monotonic with respect to v0 and it respects scaling and translation of
v0.

Theorem 3.6 Let (G, v0) be a well-posed instance with T := T (v0) as the set of terminals. Then the following
statements hold.

1. For any c, d ∈ R, v1 a partial assignment with terminals T (v1) = T and v1(t) = cv0(t) + d for all t ∈ T .

Then, lexG[v1](i) = c · lexG[v0](i) + d for all i ∈ VG.

2. v1 a partial assignment with terminals T (v1) = T. Suppose further that v1(t) ≥ v0(t) for all t ∈ T. Then,

lexG[v1](i) ≥ lexG[v0](i) for all i ∈ VG.

As a corollary, the above theorem gives a nice stability property that lex-minimal elements satisfy.

Corollary 3.7 Given well-posed instances (G, v0), (G, v1) such that T := T (v0) = T (v1), let ǫ := maxt∈T |v0(t) −
v1(t)|. Then |lexG[v0](i) − lexG[v1](i)| ≤ ǫ for all i ∈ VG.

5

3.2 Alternate Characterizations

There are at least two other seemingly disparate deﬁnitions that are equivalent to lex-minimal voltages.

lp-norm Minimizers. As mentioned in the introduction, for a well-posed instance (G, v0) the lex-minimizer is also
the limit of lp minimizers. This follows from existing results about the limit of lp-minimizers (Egger and Huotari
(1990)) in afﬁne spaces, since {grad[v] | v is complete, v extends v0} forms an afﬁne subspace of Rm. Thus, we have
the following theorem:

Theorem 3.8 (Limit of lp-minimizers, follows from Egger and Huotari (1990)) For any p ∈ (1, ∞), given a well-
posed instance (G, v0) deﬁne vp to be the unique complete voltage assignment extending v0 and minimizing
p ,
i.e.

grad[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Then, limp→∞ vp = lexG[v0].

vp = arg min
v is complete
v extends v0 (cid:13)
(cid:13)

grad[v]

p .

(cid:13)
(cid:13)

Max-Min Gradient Averaging. Consider a well-posed instance (G, v0), and a complete voltage assignment v ex-
tending v0. If G is such that ℓ(e) = 1 for all e ∈ EG, it is easy to see that lex = lexG[v0] satisﬁes the following simple
condition for all x ∈ VG \ T (v0),

lex(x) =

1
2  

max
(x,y)∈EG

lex(y) + min

lex(z)

.

(x,z)∈EG

!

This condition should be contrasted to the optimality condition for l2-regularization on these instances, which gives
for all non-terminals x, the optimal voltage v satisﬁes v(x) = 1

y:(x,y)∈EG v(y).

deg(x)

To prove the above claim, consider locally changing lex at x and observe that the gradients of edges not incident
at x remain unchanged, and at least one of edges incident at x will have a strictly larger gradient, contradicting lex-
minimality. For general graphs, this condition of local optimality can still be characterized by a simple max-min
gradient averaging property as described below.

P

Deﬁnition 3.9 (Max-Min Gradient Averaging) Given a well-posed instance (G, v0), and a complete voltage as-
signment v extending v0, we say that v satisﬁes the max-min gradient averaging property (w.r.t. (G, v0)) if for every
x ∈ VG \ T (v0), we have

grad[v](x, y) = − min

grad[v](x, y).

max
y:(x,y)∈EG

y:(x,y)∈EG

As stated in the theorem below, lexG[v0] is the unique assignment satisfying max-min gradient averaging property.
Shefﬁeld and Smart (2010) proved a variant of this statement for weighted graphs. For completeness, we present a
proof in the appendix.

Theorem 3.10 Given a well-posed instance (G, v0), lexG[v0] satisﬁes max-min gradient averaging property. More-
over, it is the unique complete voltage assignment extending v0 that satisﬁes this property w.r.t. (G, v0).

An advantage of this characterization is that it can be veriﬁed quickly. This is particularly useful for implementations
for computing the lex-minimizer.

4 Algorithms

We now sketch the ideas behind our algorithms and give precise statements of our results. A full description of all the
algorithms is included in the appendix.

We deﬁne the pressure of a vertex to be the gradient of the steepest terminal path through it:

pressure[v0](x) = max{∇P (v0) | P is a terminal path in (G, v0) and x ∈ P }.

6

Observe that in a graph with no terminal-terminal edges, a free terminal path is a steepest ﬁxable path iff its gradient
is equal to the highest pressure amongst all vertices. Moreover, vertices that lie on steepest ﬁxable paths are exactly
the vertices with the highest pressure. For a given α > 0, in order to identify vertices with pressure exceeding α, we
compute vectors vHigh[α](x) and vLow[α](x) deﬁned as follows in terms of dist, the metric on V induced by ℓ:

vLow[α](x) = min

{v0(t) + α · dist(x, t)}

t∈T (v0)

vHigh[α](x) = max
t∈T (v0)

{v0(t) − α · dist(t, x)}.

4.1 Lex-minimization on Star Graphs

We ﬁrst consider the problem of computing the lex-minimizer on a star graph in which every vertex but the center is a
terminal. This special case is a subroutine in the general algorithm, and also motivates some of our techniques.

Let x be the center vertex, T be the set of terminals, and all edges be of the form (x, t) with t ∈ T . The initial
voltage assignment is given by v : T → R, and we abbreviate dist(x, t) by d(t) = ℓ(x, t). From Corollary 3.4 we know
that we can determine the value of the lex minimizer at x by ﬁnding a steepest ﬁxable path. By deﬁnition, we need to
ﬁnd t1, t2 ∈ T that maximize the gradient of the path from t1 to t2, ∇(t1, t2) = v(t1)−v(t2)
d(t2)+d(t2) . As observed above, this
is equivalent to ﬁnding a terminal with the highest pressure. We now present a simple randomized algorithm for this
problem that runs in expected linear time.

Given a terminal t1, we can compute its pressure α along with the terminal t2 such that |∇(t1, t2)| = α in time
O(|T |) by scanning over the terminals in T . Consider doing this for a random terminal t1. We will show that in linear
time one can then ﬁnd the subset of terminals T ′ ⊂ T whose pressure is greater than α. Assuming this, we complete
the analysis of the algorithm. If T ′ = ∅, t1 is a vertex with highest pressure. Hence the path from t1 to t2 is a steepest
ﬁxable path, and we return (t1, t2). If T ′ 6= ∅, the terminal with the highest pressure must be in T ′, and we recurse by
picking a new random t1 ∈ T ′. As the size of T ′ will halve in expectation at each iteration, the expected time of the
algorithm on the star is O(|T |).

To determine which terminals have pressure exceeding α, we observe that the condition ∃t2 : α < ∇(t1, t2) =
v(t1)−v(t2)
d(t1)+d(t2) , is equivalent to ∃t2 : v(t2)+αd(t2) < v(t1)−αd(t1). This, in turn, is equivalent to vLow[α](x) < v(t1)−
αd(t1). We can compute vLow[α](x) in deterministic O(|T |) time. Similarly, we can check if ∃t2 : α < ∇(t2, t1) by
checking if vHigh[α](x) > vt1 + αd(t1). Thus, in linear time, we can compute the set T ′ of terminals with pressure
exceeding α. The above algorithm is described in Algorithm 10.

Theorem 4.1 Given a set of terminals T, initial voltages v : T → R, and distances d : T → R+, STARSTEEPESTPATH(T, v, d)
returns (t1, t2) maximizing v(t1)−v(t2)

d(t1)+d(t2) , and runs in expected time O(|T |).

4.2 Lex-minimization on General Graphs

Theorem 3.3, tells us that META-LEX will compute lex-minimizers given an algorithm for ﬁnding a steepest ﬁxable
path in (G, v0). Recall that ﬁnding a steepest ﬁxable path is equivalent to ﬁnding a path with gradient equal to the
highest pressure amongst all vertices. In this section, we show how to do this in expected time O(m + n log n).

We describe an algorithm VERTEXSTEEPESTPATH that ﬁnds a terminal path P through any vertex x such that
∇P (v0) = pressure[v0](x) in expected O(m + n log n) time. Using Dijkstra’s algorithm, we compute dist(x, t) for
all t ∈ T. If x ∈ T (v0), then there must be a terminal path P that starts at x that has ∇P (v0) = pressure[v0](x). To
compute such a P we examine all t ∈ T (v0) in O(|T |) time to ﬁnd the t that maximizes |∇(x, t)| = |v(x)−v(t)|
, and
dist(x,t)
then return a shortest path between x and that t.

If x /∈ T (v0), then the steepest path through x between terminals t1 and t2 must consist of shortest paths between
x and t1 and between x and t2. Thus, we can reduce the problem to that of ﬁnding the steepest path in a star graph
where x is the only non-terminal and is connected to each terminal t by an edge of length dist(x, t). By Theorem 4.1,
we can ﬁnd this steepest path in O(|T |) expected time. The above algorithm is formally described as Algorithm 9.

Theorem 4.2 Given a well-posed instance (G, v0), and a vertex x ∈ VG, VERTEXSTEEPESTPATH(G, v0, x) returns
a terminal path P through x such that ∇P (v0) = pressure[v0](x), in O(m + n log n) expected time.

7

As in the algorithm for the star graph, we need to identify the vertices whose pressure exceeds a given α. For a ﬁxed
α, we can compute vLow[α](x) and vHigh[α](x) for all x ∈ VG using a simple modiﬁcation of Dijkstra’s algorithm in
O(m + n log n) time. We describe the algorithms COMPVHIGH, COMPVLOW for these tasks in Algorithms 3 and 4.
The following lemma encapsulates the usefulness of vLow and vHigh.

Lemma 4.3 For every x ∈ VG, pressure[v0](x) > α iff vHigh[α](x) > vLow[α](x).

It immediately follows that the algorithm COMPHIGHPRESSGRAPH(G, v0, α) described in Algorithm 6 computes

the vertex induced subgraph on the vertex set {x ∈ VG| pressure[v0](x) > α}.

We can combine these algorithms into an algorithm STEEPESTPATH that ﬁnds the steepest ﬁxable path in (G, v0)
in O(m + n log n) expected time. We may assume that there are no terminal-terminal edges in G. We sample an edge
(x1, x2) uniformly at random from EG, and a terminal x3 uniformly at random from VG. For i = 1, 2, 3, we compute
the steepest terminal path Pi containing xi. By Theorem 4.2, this can be done in O(m + n log n) expected time. Let α
be the largest gradient maxi ∇Pi. As mentioned above, we can identify G′, the induced subgraph on vertices x with
pressure exceeding α, in O(m + n log n) time. If G′ is empty, we know that the path Pi with largest gradient is a
steepest ﬁxable path. If not, a steepest ﬁxable path in (G, v0) must be in G′, and hence we can recurse on G′. Since
we picked a uniformly random edge, and a uniformly random vertex, the expected size of G′ is at most half that of G.
Thus, we obtain an expected running time of O(m + n log n). This algorithm is described in detail in Algorithm 7.

Theorem 4.4 Given a well-posed instance (G, v0) with EG ∩ (T (v0) × T (v0)) = ∅, STEEPESTPATH(G, v0) returns
a steepest ﬁxable path in (G, v0), and runs in O(m + n log n) expected time.

By using STEEPESTPATH in META-LEX, we get the COMPLEXMIN, shown in Algorithm 1. From Theorem 3.3 and
Theorem 4.4, we immediately get the following corollary.

Corollary 4.5 Given a well-posed instance (G, v0) as input, algorithm COMPLEXMIN computes a lex-minimizing
assignment that extends v0 in O(n(m + n log n)) expected time.

4.3 Linear-time Algorithm for Inf-minimization

Given the algorithms in the previous section, it is straightforward to construct an inﬁnity minimizer. Let α⋆ be the
gradient of the steepest terminal path. From Lemma 3.5, we know that the norm of the inf minimizer is α⋆. Considering
all trivial terminal paths (terminal-terminal edges), and using STEEPESTPATH, we can compute α⋆ in randomized
O(m+n log n) time. It is well known (McShane (1934); Whitney (1934)) that v1 = vLow[α⋆] and v2 = vHigh[α⋆] are
inf-minimizers. It is also known that 1
2 (v1 + v2) is the inf-minimizer that minimizes the maximum ℓ∞-norm distance
to all inf-minimizers. In the case of path graphs, this was observed by Gaffney and Powell (1976) and independently
by Micchelli et al. (1976). For completeness, the algorithm is presented as Algorithm 5, and we have the following
result.

Theorem 4.6 Given a well-posed instance (G, v0), COMPINFMIN(G, v0) returns a complete voltage assignment v
for G extending v0 that minimizes

∞ , and runs in randomized O(m + n log n) time.

grad[v]

4.4 Faster Algorithms for Lex-minimization

(cid:13)
(cid:13)

(cid:13)
(cid:13)

The lex-minimizer has additional structure that allows one to compute it by more efﬁcient algorithms. One observation
that leads to a faster implementation is that ﬁxing a steepest ﬁxable path does not increase the pressure at vertices,
provided that one appropriately ignores terminal-terminal edges. Thus, if G(α) is a subgraph that we identiﬁed with
pressure greater than α, we can iteratively ﬁx all steepest ﬁxable paths P in G(α) with ∇P > α. Another simple
observation is that if G(α) is disconnected, we can simply recurse on each of the connected components. A complete
description of an the algorithm COMPFASTLEXMIN based on these idea is given in Algorithm 11. The algorithm
provably computes lexG(v0), and it is possible to implement it so that the space requirement is only O(m + n).
Although, we are unable to prove theoretical bounds on the running time that are better than O(n(m + n log n)),
it runs extremely quickly in practice. We used it to perform the experiments in this paper. For random regular
graphs and Delaunay graphs, with n = 0.5 × 106 vertices and around 2 million edges m ∼ 1.5 − 2 × 106, it

8

takes a couple of minutes on a 2009 MacBook Pro. Similar times are observed for other model graphs of this
size such as random regular graphs and real world networks. An implementation of this algorithm may be found
at https://github.com/danspielman/YINSlex.

5 Directed Graphs

Our deﬁnitions and algorithms, including those for regularization, extend to directed graphs with only small modiﬁ-
cations. We view directed edges as diodes and only consider potential differences in the direction of the edge. For
a complete voltage assignment v on the vertices of a directed graph G, we deﬁne the directed gradient on (x, y) due
to v to be grad+
. Given a partially-labelled directed graph (G, v0), we say that a a
complete voltage assignment v is a lex-minimizer if it extends v0 and for other complete voltage assignment v′ that
extends v0 we have grad+
G[v′]. We say that a partially-labelled directed graph (G, v0) is a well-posed
directed instance if every free vertex appears in a directed path between two terminals.

G[v](x, y) = max

G[v] (cid:22) grad+

v(x)−v(y)
ℓ(x,y)

, 0

n

o

The main difference between the directed and undirected cases is that the directed lex-minimizer is not necessarily
unique. To maintain clarity of exposition, we chose to focus on undirected graphs so far. For directed graphs, we have
the following corresponding structural results.

Theorem 5.1 Given a well-posed instance (G, v0) on a directed graph G, there exists a lex-minimizer, and the set of
all lex-minimizers is a convex set. Moreover, for every two lex-minimizers v and v′, we have grad+

G[v] = grad+

G[v′].

However, note that in the case of directed graphs, the lex-minimizer need not be unique. We still have a weaker version
of Theorem 3.3 for directed graphs.

Theorem 5.2 Given a well-posed instance (G, v0) on a directed graph G, let v1 be the partial voltage assignment
extending v0 obtained by repeatedly ﬁxing steepest ﬁxable (directed) paths P with ∇P > 0. Then, any lex-minimizer
of (G, v0) must extend v1. Moreover, for every edge e ∈ EG \ (T (V1) × T (V1)), any lex-minimizer v of (G, v0) must
satisfy grad+[v](e) = 0.

When the value of the lex-minimizer at a vertex is not uniquely determined, it is constrained to an interval. In our
experiments, we pick the convention that when the voltage at a vertex is constrained to an interval (−∞, a] or [a, ∞),
we assign a to the terminal. When it is constrained to a ﬁnite interval, we assign a voltage closest to the median of the
original voltages.

6 Experiments on WebSpam

We demonstrate the performance of our lex-minimization algorithms on directed graphs by using them to detect spam
webpages as in Zhou et al. (2007). We use the dataset webspam-uk2006-2.0 described in Castillo et al. (2006).
This collection includes 11,402 hosts, out of which 7,473 (65.5 %) are labeled, either as spam or normal. Each host
corresponds to the collection of web pages it serves. Of the hosts, 1924 are labeled spam (25.7 % of all labels). We
consider the problem of ﬂagging some hosts as spam, given only a small fraction of the labels for training. We assign
a value of 1 to the spam hosts, and a value of 0 to the normal ones. We then compute a lex minimizer and examine the
effect of ﬂagging as spam all hosts with a value greater than some threshold.

Following Zhou et al. (2007), we create edges between hosts with lengths equal to the reciprocal of the number of
links from one to the other. We run our experiments only on the largest strongly connected component of the graph,
which contains 7945 hosts of which 5552 are labeled. 16 % of the nodes in this subgraph are labeled spam. To create
training and test data, for a given value p, we select a random subset of p % of the spam labels and a random subset
of p % of the normal labels to use for training. The remaining labels are used for testing. We report results for p = 5
and p = 20.

Again following Zhou et al. (2007), we plot the precision and recall of different choices of threshold for ﬂagging
pages as spam. Recall is the fraction of spam pages our algorithm ﬂags as spam, and precision is the fraction of pages
our algorithm ﬂags as spam that actually are spam. Amongst the algorithms studied by Zhou et al. (2007), the top

9

performer was their algorithm based on sampling according to a random-walk that follows in-links from other hosts.
We compare their algorithm with the classiﬁcation we get by directing edges in the opposite directions of links. This
has the effect that a link to a spam host is evidence of spamminess, and a link from a normal host is evidence of
normality.

Results are shown in Figure 3. While we are not able to reliably ﬂag all spam hosts, we see that in the range of
10-50 % recall, we are able to ﬂag spam with precision above 82 %. We see that the performance of directed lex-
minimization does not degrade rapidly when from the “large training set” regime of p = 20, to the “small training set”
regime of p = 5.

5 % labels for training

20 % labels for training

RandWalk
DirectedLex

RandWalk
DirectedLex

i

i

n
o
s
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
0.1

0.2

0.3

0.4

0.7

0.8

0.9

1

0.2

0.3

0.4

0.7

0.8

0.9

1

0.6
0.5
Recall

0.6
0.5
Recall

Figure 3: Recall and precision in the web spam classiﬁcation experiment. Each data point shown was computed as an average over
100 runs. The largest standard deviation of the mean precision across the plotted recall values was less than 1.3 %. The algorithm
of Zhou et al. (2007) appears as RANDWALK. Our directed lex-minimization algorithm appears as DIRECTEDLEX.

For comparison, in Appendix C, we show the performance of our algorithm and that of Zhou et al. (2007) both
with link directions reversed, as well as the performance of undirected lex-minimization and Laplacian inference, all
of which are signiﬁcantly worse.

7 l0-Regularization of Vertex Values

We now explain how we can accommodate noise in both the given voltages and in the given lengths of edges. We can
ﬁnd the minimum number of labels to ignore, or the minimum increase in edges lengths needed so that there exists an
extension whose gradients have l∞-norm lower than a given target. After determining which labels to ignore or the
needed increment in edge lengths, we recommend computing a lex minimizer.

The algorithms we present in this section are essentially the same for directed and undirected graphs.

7.1 l0-Vertex Regularization for Inf-minimization

The l0-regularization of vertex labels can be viewed as a problem of outlier removal: the vector we compute is allowed
to disagree with v0 on up to k terminals. Given a voltage assignment v and a subset T ⊂ V of the vertices, by v(T )
we mean the vector obtained by restricting v to T . We deﬁne the l0-Vertex Regularization for l∞ problem to be

where v(T ) is the vector of values of v on the terminals T .

min
v∈IRn

gradG[v]

∞

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

0 ≤ k,
(cid:13)
(cid:13)

subject to

v(T ) − v0(T )

(3)

In Appendix D, we describe an approximation algorithm APPROX-OUTLIER that approximately solves program (3).

The precise statement we prove in Appendix D is given in the following theorem.

1

0.9

0.8

0.7

i

i

n
o
s
c
e
r
P

0.6

0.5

0.4

0.3

0.2

0.1

0
0.1

10

Theorem 7.1 (Approximate l0-vertex regularization) The algorithm APPROX-OUTLIER takes a positive integer k
and a partially-labeled graph (G, v0), and outputs an assignment v with
0 ≤ 2k, and
∞ ≤
α∗, where α∗ is the optimum value of program (3). The algorithm runs in time O(k(m + n log n)).
(cid:13)
(cid:13)
(cid:13)
(cid:13)

In Appendix D, we also describe an algorithm OUTLIER that exactly solves program (3) in polynomial time, and we
prove its correctness.

v(T ) − v0(T )

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Theorem 7.2 (Exact l0-vertex regularization) The algorithm OUTLIER takes a positive integer k and a partially-
labeled graph (G, v0) solves program (3) exactly. The algorithm runs in polynomial time.

We give a proof of Theorem 7.2 in Appendix D. To do this, we reduce the program (3) to the problem of minimizing
the required l0-budget needed to achieve a ﬁxed gradient α using a binary search over a set of O(n2) gradients. This
latter problem we reduce in polynomial time to Minimum Vertex Cover (VC) on a transitively closed, directed acyclic
graph (a TC-DAG). VC on a TC-DAG can be solved exactly in polynomial time by a reduction to the Maximum
Bipartite Matching Problem (Fulkerson (1956)). The problem was phrased by Fulkerson as one of ﬁnding a maximum
antichain of a ﬁnite poset. Any transitively closed DAG corresponds directly to the comparability graph of a poset. A
maximum antichain of a poset is a maximum independent set of a the comparability graph of the poset, and hence its
complement is a minimum vertex cover of the comparability graph. We refer to the algorithm developed by Fulkerson
as KONIG-COVER.

Theorem 7.3 The algorithm KONIG-COVER computes a minimum vertex cover for any transitively closed DAG G in
polynomial time.

7.2 Hardness of l0 regularization for l2

The result that l0-regularized inf-minimization can be solved exactly in polynomial time is surprising, especially
because the analogous problem for 2-Laplacian minimization turns out to be NP-Hard.

We deﬁne the the l0 vertex regularization for l2 for a partially-labeled graph (G, v0) and an integer k by

min
v∈Rn:kv(T )−v0(T )k0

≤k

vT Lv,

where L is the Laplacian of G.

Theorem 7.4 l0 vertex regularization for l2 is NP-Hard.

In Appendix E we prove Theorem 7.4 by giving a polynomial time (Karp) reduction from the NP-Hard minimum
bisection problem to l0 vertex regularization for l2.

8 l1-Edge and Vertex Regularization of Inf-minimizers

Consider a partially-labeled graph (G, v0) and an α > 0. The set of voltage assignments given by

v : v extends v0 and

gradG[v]

∞ ≤ α

n

(cid:13)
(cid:13)

(cid:13)
(cid:13)

o

is convex. Going further, let us consider the edge lengths in a graph to be speciﬁed by a vector ℓ ∈ IRE. Now the set
of voltages v and and lengths ℓ which achieve kgradG(ℓ)[v]k∞ ≤ α is jointly convex in v and ℓ. To see this, observe
that

kgradG(ℓ)[v]k∞ ≤ α ⇔ ∀(u, v) ∈ E : −αℓ(u, v) ≤ v(u) − v(v) ≤ αℓ(u, v).
Furthermore, the condition “v extends v0” is a linear constraint on v, which we express as v(T ) = v0(T ). From
the above, it is clear that the gradient condition corresponds to a convex set, as it is an intersection of half-spaces.
These half-spaces are given by O(m) linear inequalities. We can leverage this to phrase many regularized variants of
inf-minimization as convex programs, and in some cases linear programs.

(4)

11

For example, we may consider a variant of inf-minimization combined with an l1-budget for changing lengths of
edges and values on terminals. Given a parameter γ > 0 which speciﬁes the relative cost of regularizing terminals to
regularizing edges, the problem is as follows

arg min
v∈IRn,s∈IRm,s≥0

ksk1 + γ

v(T ) − v0(T )

1

subject to

gradG(ℓ+s)[v]

≤ α.

(5)

(cid:13)
(cid:13)
From our observation (4), it follows that problem (5) may be expressed as a linear program with O(n) variables
and O(m) constraints. We can use ideas from Daitch and Spielman (2008) to solve the resulting linear program in
O(m1.5) by an interior point method with a special purpose linear equation solver. The reason is that the linear
time
equations the IPM must solve at each iteration may be reduced to linear equations in symmetric, diagonally dominant
matrices, and these may be solved in nearly-linear time (Cohen et al. (2014)).

(cid:13)
(cid:13)

e

(cid:13)
(cid:13)
(cid:13)

∞

(cid:13)
(cid:13)
(cid:13)

Conclusion. We propose the use of inf and lex minimizers for regression on graphs. We present simple algorithms
for computing them that are provably fast and correct, and can also be implemented efﬁciently. We also present a
framework and polynomial time algorithms for regularization in this setting. The initial experiments reported in the
paper indicate that these algorithms give pretty good results on real and synthetic datasets. The results seem to compare
quite favorably to other algorithms, particularly in the regime of tiny labeled sets. We are testing these algorithms on
several other graph learning questions, and plan to report on them in a forthcoming experimental paper. We believe
that inf and lex minimizers, and the associated ideas presented in the paper, should be useful primitives that can be
proﬁtably combined with other approaches to learning on graphs.

We thank anonymous reviewers for helpful comments. We thank Santosh Vempala and Bartosz Walczak for pointing
out that it was already known how to compute a minimum vertex cover of a transitively closed DAG in polynomial
time.

Acknowledgements

References

Morteza Alamgir
In Advances
Information Processing
http://books.nips.cc/papers/files/nips24/NIPS2011_0278.pdf.

and Ulrike V. Luxburg.

transition
24,

in
pages

in Neural

Systems

Phase

the

family
379–387.

of
2011.

p-resistances.
URL

Gunnar Aronsson. Extension of functions satisfying lipschitz conditions. Arkiv fr Matematik, 6(6):551–561, 1967.

ISSN 0004-2080. doi: 10.1007/BF02591928. URL http://dx.doi.org/10.1007/BF02591928.

Gunnar Aronsson, Michael G. Crandall, and Petri Juutinen. A tour of the theory of absolutely minimizing functions.
ISSN 0273-0979. doi: 10.1090/S0273-0979-04-01035-3.

Bull. Amer. Math. Soc. (N.S.), 41(4):439–505, 2004.
URL http://dx.doi.org/10.1090/S0273-0979-04-01035-3.

Guy Barles and J´erˆome Busca. Existence and comparison results for fully nonlinear degenerate elliptic equations

without zeroth-order term. Comm. Partial Differential Equations, 26:2323–2337, 2001.

Mikhail Belkin, Irina Matveeva, and Partha Niyogi.

Regularization and semi-supervised learning on large
In Learning Theory, volume 3120 of Lecture Notes in Computer Science, pages 624–638.
doi: 10.1007/978-3-540-27819-1 43. URL

graphs.
Springer Berlin Heidelberg, 2004.
http://dx.doi.org/10.1007/978-3-540-27819-1_43.

ISBN 978-3-540-22282-8.

Nick Bridle and Xiaojin Zhu. p-voltages: Laplacian regularization for semi-supervised learning on high-dimensional

data. In Eleventh Workshop on Mining and Learning with Graphs (MLG2013), 2013.

12

Carlos Castillo, Debora Donato, Luca Becchetti, Paolo Boldi, Stefano Leonardi, Massimo Santini, and Sebastiano
Vigna. A reference collection for web spam. SIGIR Forum, 40(2):11–24, December 2006. ISSN 0163-5840. doi:
10.1145/1189702.1189703. URL http://doi.acm.org/10.1145/1189702.1189703.

Olivier Chapelle, Bernhard Schlkopf, and Alexander Zien. Semi-Supervised Learning. The MIT Press, 1st edition,

2010. ISBN 0262514125, 9780262514125.

Michael B Cohen, Rasmus Kyng, Gary L Miller, Jakub W Pachocki, Richard Peng, Anup B Rao, and Shen Chen Xu.
Solving SDD linear systems in nearly m log1/2 n time. In Proceedings of the 46th Annual ACM Symposium on
Theory of Computing, pages 343–352. ACM, 2014.

M.G. Crandall, L.C. Evans, and R.F. Gariepy. Optimal lipschitz extensions and the inﬁnity laplacian. Calculus of Vari-
ations and Partial Differential Equations, 13(2):123–139, 2001. ISSN 0944-2669. doi: 10.1007/s005260000065.
URL http://dx.doi.org/10.1007/s005260000065.

Samuel I. Daitch and Daniel A. Spielman. Faster approximate lossy generalized ﬂow via interior point algo-
rithms.
In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing, STOC ’08, pages
451–460, New York, NY, USA, 2008. ACM. ISBN 978-1-60558-047-0. doi: 10.1145/1374376.1374441. URL
http://doi.acm.org/10.1145/1374376.1374441.

Alan Egger and Robert Huotari. Rate of convergence of the discrete polya algorithm. Journal of Approximation
ISSN 0021-9045. doi: http://dx.doi.org/10.1016/0021-9045(90)90070-7. URL

Theory, 60(1):24 – 30, 1990.
http://www.sciencedirect.com/science/article/pii/0021904590900707.

Charles Fefferman. Whitney’s extension problems and interpolation of data.

(N.S.), 46(2):207–220, 2009a.
http://dx.doi.org/10.1090/S0273-0979-08-01240-8.

ISSN 0273-0979.

doi:

10.1090/S0273-0979-08-01240-8.

Bull. Amer. Math. Soc.
URL

Charles Fefferman. Fitting a [image] -smooth function to data, iii. Annals of Mathematics, 170(1):pp. 427–441, 2009b.

ISSN 0003486X. URL http://www.jstor.org/stable/40345469.

Charles Fefferman and Bo’az Klartag. Fitting a cm -smooth function to data i. Annals of Mathematics, 169(1):pp.

315–346, 2009. ISSN 0003486X. URL http://www.jstor.org/stable/40345445.

D. R. Fulkerson. Note on dilworths decomposition theorem for partially ordered sets. Proc. Amer. Math. Soc, 1956.

P.W. Gaffney and M.J.D. Powell. Optimal interpolation. In Numerical Analysis, volume 506 of Lecture Notes in Math-
ematics, pages 90–99. Springer Berlin Heidelberg, 1976. ISBN 978-3-540-07610-0. doi: 10.1007/BFb0080117.
URL http://dx.doi.org/10.1007/BFb0080117.

L.-A. Gottlieb, A. Kontorovich, and R. Krauthgamer. Efﬁcient classiﬁcation for metric data. CoRR, abs/1306.2547,

2013. URL http://arxiv.org/abs/1306.2547.

L.-A. Gottlieb, A. Kontorovich, and R. Krauthgamer. Efﬁcient regression in metric spaces via approximate lipschitz
extension. In Similarity-Based Pattern Recognition, volume 7953 of Lecture Notes in Computer Science, pages
43–58. Springer Berlin Heidelberg, 2013b. ISBN 978-3-642-39139-2. doi: 10.1007/978-3-642-39140-8 3. URL
http://dx.doi.org/10.1007/978-3-642-39140-8_3.

Robert Jensen. Uniqueness of lipschitz extensions: Minimizing the sup norm of the gradient. Archive for Ra-
doi: 10.1007/BF00386368. URL

ISSN 0003-9527.

tional Mechanics and Analysis, 123(1):51–74, 1993.
http://dx.doi.org/10.1007/BF00386368.

M. Kirszbraun. ber die zusammenziehende und lipschitzsche transformationen. Fundamenta Mathematicae, 22(1):

77–108, 1934. URL http://eudml.org/doc/212681.

13

Andrew J. Lazarus, Daniel E. Loeb,

James G. Propp, Walter R. Stromquist,

Combinatorial games under

man.
229 – 264,
http://www.sciencedirect.com/science/article/pii/S0899825698906765.

http://dx.doi.org/10.1006/game.1998.0676.

and Economic Behavior,

ISSN 0899-8256.

auction play.

Games

1999.

doi:

and Daniel H. Ull-
27(2):
URL

Yunqian Ma and Yun Fu. Manifold Learning Theory and Applications. CRC Press, Inc., Boca Raton, FL, USA, 1st

edition, 2011. ISBN 1439871094, 9781439871096.

E. J. McShane. Extension of range of functions. Bull. Amer. Math. Soc., 40(12):837–842, 12 1934. URL

http://projecteuclid.org/euclid.bams/1183497871.

C.A. Micchelli, T.J. Rivlin,

and S. Winograd.

merische Mathematik, 26(2):191–200, 1976.
http://dx.doi.org/10.1007/BF01395972.

The optimal
ISSN 0029-599X.

recovery of
doi:

smooth functions.
10.1007/BF01395972.

Nu-
URL

V. A. Milman.

Absolutely minimal extensions of

functions on metric spaces.

1999.

URL

http://iopscience.iop.org/1064-5616/190/6/A05/pdf/MSB_190_6_A05.pdf.

Boaz Nadler, Nathan Srebro, and Xueyuan Zhou. Statistical analysis of semi-supervised learning: The limit of inﬁnite
unlabelled data. 2009. URL http://ttic.uchicago.edu/˜nati/Publications/NSZnips09.pdf.

A. Naor and S. Shefﬁeld. Absolutely minimal Lipschitz extension of tree-valued mappings. CoRR, abs/1005.2535,

May 2010. URL http://arxiv.org/abs/1005.2535.

A. M. Oberman. Finite difference methods for the Inﬁnity Laplace and p-Laplace equations. CoRR, abs/1107.5278,

July 2011. URL http://arxiv.org/abs/1107.5278.

Yuval Peres, Oded Schramm, Scott Shefﬁeld, and DavidB. Wilson.

Tug-of-war and the inﬁnity lapla-
In Selected Works of Oded Schramm, Selected Works in Probability and Statistics, pages 595–
doi: 10.1007/978-1-4419-9675-6 18. URL

cian.
638. Springer New York, 2011.
http://dx.doi.org/10.1007/978-1-4419-9675-6_18.

ISBN 978-1-4419-9674-9.

S. Shefﬁeld and C. K. Smart. Vector-valued optimal Lipschitz extensions. CoRR, abs/1006.1741, June 2010. URL

http://arxiv.org/abs/1006.1741.

Ali Kemal Sinop and Leo Grady. A seeded image segmentation framework unifying graph cuts and random walker
which yields a new algorithm. In Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on,
pages 1–8. IEEE, 2007.

Vijay V. Vazirani. Approximation Algorithms. Springer-Verlag New York, Inc., New York, NY, USA, 2001. ISBN

3-540-65367-8.

Ulrike von Luxburg and Olivier Bousquet. Distance-based classiﬁcation with lipschitz functions.

In Learn-
ing Theory and Kernel Machines, volume 2777 of Lecture Notes in Computer Science, pages 314–328.
doi: 10.1007/978-3-540-45167-9 24. URL
Springer Berlin Heidelberg, 2003.
http://dx.doi.org/10.1007/978-3-540-45167-9_24.

ISBN 978-3-540-40720-1.

Hassler Whitney.

Analytic extensions of differentiable functions deﬁned in closed sets.

tions of
http://www.jstor.org/stable/1989708.

the American Mathematical Society, 36(1):pp. 63–89, 1934.

ISSN 00029947.

Transac-
URL

Dengyong Zhou, Christopher J. C. Burges, and Tao Tao. Transductive link spam detection.

In Proceedings
of the 3rd International Workshop on Adversarial Information Retrieval on the Web, AIRWeb ’07, pages 21–
ISBN 978-1-59593-732-2. doi: 10.1145/1244408.1244413. URL
28, New York, NY, USA, 2007. ACM.
http://doi.acm.org/10.1145/1244408.1244413.

Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic

functions. In IN ICML, pages 912–919, 2003.

14

A Basic Properties of Lex-Minimizers

A.1 Meta Algorithm

Algorithm 1: Algorithm META-LEX: Given a well-posed instance (G, v0), outputs lexG[v0].
for i = 1, 2, . . . :

1. if T (vi−1) = VG, then return vi−1.
2. E′ = EG \ (T (vi−1) × T (vi−1)), G′ := (VG, E′).
3. Let P ⋆
4. vi ← ﬁx[vi−1, P ⋆
i ].

i be a steepest ﬁxable path in (G′, vi−1). Let α⋆

i ← ∇P ⋆(vi−1).

In this subsection, we prove the results that appeared in section 2. We start with a simple observation.

Proposition A.1 Given a well-posed instance (G, v0) such that T (v0) 6= V, let P be a steepest ﬁxable path in (G, v0).
Then, ﬁx[v0, P ] extends v0, and (G, ﬁx[v0, P ]) is also a well-posed instance.

The properties we prove below do not depend on the choice of the steepest ﬁxable path.

Proposition A.2 For any well-posed instance (G, v0), with |VG| = n, META-LEX(G, v0) terminates in at most n
iterations, and outputs a complete voltage assignment v that extends v0.

Proof of Proposition A.2: By Proposition A.1, at any iteration i, vi−1 extends v0 and (G′, vi−1) is a well-posed
instance. META-LEX only outputs vi−1 iff T (vi−1) = V, which means vi−1 is a complete voltage assignment. For
any vi−1 that is not complete, for any x ∈ V \T (vi−1), we must have a free terminal path in (G′, vi−1) that contains x.
i exists in (G′, vi−1). Since P ⋆
Hence, a steepest ﬁxable path P ⋆
i ] ﬁxes the voltage
i
✷
for at least one non-terminal. Thus, META-LEX(G, v0) must complete in at most n iterations.

is a free terminal path, ﬁx[vi−1, P ⋆

For the following lemmas, consider a run of META-LEX with well-posed instance (G, v0) as input. Let vout be the
complete voltage assignment output by META-LEX. Let Ei be the set of edges E′ and Gi be the graph G′ constructed
in iteration i of META-LEX.

Lemma A.3 For every edge e ∈ Ei−1 \ Ei, we have

grad[vout](e)

≤ α⋆

i . Moreover, α⋆

i is non-increasing with i.

Proof of Lemma A.3: Let P ⋆
i = (x0, . . . , xr) be a steepest ﬁxable path in iteration i (when we deal with instance
(Gi−1, vi−1)). Consider a terminal path Pi+1 in (Gi, vi) such that {∂0Pi+1, ∂1Pi+1} ∩ (T (vi) \ T (vi−1)) 6= ∅. We
i . On the contrary, assume that ∇Pi+1(vi) > α⋆
claim that ∇Pi+1(vi) ≤ α⋆
i . Consider the case ∂0Pi+1 ∈ T (vi) \
T (vi−1), ∂1P1 ∈ T (vi−1). By the deﬁnition of vi, we must have ∂0Pi+1 = xj for some j ∈ [r − 1]. Let P ′
i+1 be the
path formed by joining paths P ⋆

i+1 is a free terminal path in (Gi−1, vi−1). We have,

i [x0 : xj] and Pi+1. P ′

(cid:12)
(cid:12)

(cid:12)
(cid:12)

vi−1(x0) − vi−1(∂1Pi+1) = (vi(x0) − vi(xj )) + (vi(∂0Pi+1) − vi(∂1Pi+1))
i · ℓ(P ′

i · ℓ(Pi+1) = α⋆

i [x0 : xj]) + α⋆

i · ℓ(P ⋆

> α⋆

i+1),

giving ∇P ′
The other cases can be handled similarly.

i+1(vi) > α⋆

i , which is a contradiction since the steepest ﬁxable path P ⋆
i

in (Gi−1, vi−1) has gradient α⋆
i .

Applying the above claim to an edge e ∈ Ei−1 \ Ei, whose gradient is ﬁxed for the ﬁrst time in iteration i, we
i . If v is the complete voltage assignment output by META-LEX, since v extends vi+1,
i , implying

i . Applying the claim to the symmetric edge, we obtain −grad[vout](e) ≤ α⋆

obtain that grad[vi+1](e) ≤ α⋆
we get grad[vout](e) ≤ α⋆
|grad[vout](e)| ≤ α⋆
i .

Consider any free terminal path Pi+1 in (Gi, vi). If Pi+1 is also a terminal path in (Gi−1, vi−1), it is a free
terminal path in (Gi−1, vi−1). In addition, since a steepest ﬁxable path P ⋆
i , we get
i
∇Pi+1(vi) = ∇Pi+1(vi−1) ≤ α⋆
i . Otherwise, we must have {∂0Pi+1, ∂1Pi+1} ∩ (T (vi) \ T (vi−1)) 6= ∅, and we can
deduce ∇Pi+1(vi) ≤ α⋆
i using the above claim. Thus, all free terminal paths Pi+1 in (Gi, vi) satisfy ∇Pi+1(vi) ≤ α⋆
i .
✷
In particular, α⋆

in (Gi−1, vi−1) has ∇P ⋆

i = α⋆

i is non-increasing with i.

i+1(vi) ≤ α⋆

i+1 = ∇P ⋆

i . Thus, α⋆

15

Lemma A.4 For any complete voltage assignment v for G that extends v0, if v 6= vout, we have grad[v] 6(cid:22) grad[vout],
and hence grad[vout] (cid:22) grad[v].

Proof of Lemma A.4: Consider any complete voltage assignment v for G that extends v0, such that v 6= vout. Thus,
there exists a unique i such that v extends vi−1 but does not extend vi. We will argue that grad[v] 6(cid:22) grad[vout], and
hence grad[vout] (cid:22) grad[v]. For every edge e ∈ E \ Ei−1 that has been ﬁxed so far, grad[v](e) = grad[vi−1](e) =
grad[vout](e), and hence we can ignore these edges.

Since v extends vi−1 but not vi, there exists an x ∈ T (vi) \ T (vi−1) such that v(x) 6= vi(x) = vout(x). Assume
i picked

i = (x0, . . . , xr) is the steepest ﬁxable path with gradient α⋆

v(x) < vi(x) (the other case is symmetric). If P ⋆
in iteration i, we must have x = xj for some j ∈ [r − 1]. Thus,

j

j

(v(xk−1) − v(xk)) = v(x0) − v(xj ) > vi(x0) − vi(xj ) = α⋆

i · ℓ(P ⋆

i [x0 : xj ]) = α⋆
i ·

ℓ(xk−1, xk).

Xk=1

Xk=1
Thus, for some k ∈ [j], we must have grad[v](xk−1, xk) > α⋆
is a path in Gi−1, we have {xk−1, xk} 6⊆
T (vi−1). This gives (xk−1, xk) ∈ (Ei−1 \ Ei). But then, from Lemma A.3, it follows that for all e ∈ (Ei−1 \ Ei), we
✷
have |grad[vout](e)| ≤ α⋆

i . Thus, we have grad[v] 6(cid:22) grad[vout].

i . Since P ∗
i

Lemma A.5 Let P = (x0, . . . , xr) be a steepest ﬁxable path such that it does not have any edges in T (v0) × T (v0)
and v1 = ﬁxG[v0, P ]. Then for every i ∈ [r], we have grad[v1](xi−1, xi) = ∇P.

Proof of Lemma A.5: Suppose this is not true and let j ∈ [r] be the minimum number such that grad[v1](xj−1, xj) 6=
∇P. By deﬁnition of v1 we would necessarily have j < r and vj ∈ T (v0). Suppose grad[v1](xj−1, xj ) < ∇P. We
would then have v1(x0) − v1(xj ) < ∇P ∗ ℓ(P [x0 : xj]). Since P does not have any edges in T (v0) × T (v0),
P1 := (xj, ..., xr) would be a free terminal path with ∇P1 > ∇P. This is a contradiction. Other cases can be ruled
out similarly.

✷

Proof of Theorem 3.3: Consider an arbitrary run of META-LEX on (G, v0). Let vout be the complete voltage
assignment output by META-LEX. Proposition A.1 implies that vout extends v0. Lemma A.4 implies that for any
complete voltage assignment v 6= vout that extends v0, we have grad[vout] (cid:22) grad[v]. Thus, vout is a lex-minimizer.
Moreover, the lemma also gives that for any such v, grad[v] 6(cid:22) grad[vout]. and hence vout is a unique lex-minimizer.
Thus, vout is the unique voltage assignment satisfying Def. 2.1, and we denote it as lexG[v0]. Since we started with an
✷
arbitrary run of META-LEX, uniqueness implies that every run of META-LEX on (G, v0) must output lexG[v0].

Proof of Lemma 3.5: Suppose we have a complete voltage assignment v extending v0, such that
For any terminal path P = (x0, . . . , xr), we get,

grad[v]

∞ ≤ α.

∇P (v0) = v0(∂0P ) − v0(∂1P ) = v(∂0P ) − v(∂1P ) =

grad[v](xi−1, xi) ≤ α ·

ℓ(xi−1, xi) = α · ℓ(P ),

(cid:13)
(cid:13)

(cid:13)
(cid:13)

r

i=1
X

giving ∇P (v0) ≤ α.

On the other hand, suppose every terminal path P in (G, v0) satisﬁes ∇P (v0) ≤ α. Consider v = lexG[v0]. We
know that v extends v0. For every edge e ∈ EG ∩ T (v0) × T (v0), e is a (trivial) terminal path in (G, v0), and hence
has satisﬁes grad[v](e) = grad[v0](e) = ∇e(v0) ≤ α. Considering the reverse edge, we also obtain −grad[v](e) ≤ α.
Thus, |grad[v](e)| ≤ α. Moreover, using Lemma A.3, we know that for edge e ∈ EG \ T (v0) × T (v0), |grad[v](e)| ≤
1 = ∇P ⋆
α⋆
1 ≤ α since P1 is a terminal path in (G, v0). Thus, for every e ∈ EG, |grad[v](e)| ≤ α, and hence
✷
grad[v]
∞ ≤ α.
(cid:13)
(cid:13)
(cid:13)
(cid:13)
A.2 Stability

In this subsection, we sketch a proof of the monotonicity of lex-minimizers and show how it implies the stability
property claimed earlier.

For any well-posed (G, v0), there could be several possible executions of META-LEX, each characterized by the

sequence of paths P ⋆

i . We can apply Theorem 3.3 to deduce the following structural result about the lex-minimizer.

r

i=1
X

16

Corollary A.6 For any well-posed instance (G, v0), consider a sequence of paths (P1, . . . , Pr) and voltage assign-
ments (v1, . . . , vr) for some positive integer r such that:

1. P ⋆

i is a steepest ﬁxable path in (Gi−1, vi−1) for i = 1, . . . , r.

2. vi = ﬁx[vi−1, P ⋆

i ] for i = 1, . . . , r.

3. T (vr) = VG.

Then, we have vr = lexG[v0].

We call such a sequence of paths and voltages to be a decomposition of lexG[v0]. Again, note that lexG[v0] can
possibly have multiple decompositions. However, any two such decompositions are consistent in the sense that they
produce the same voltage assignment.

Proof of Corollary 3.7: We ﬁrst deﬁne some operations on partial assignments which simpliﬁes the notation. Let
v0, v1 be any two partial assignments with the same set of terminals T := T (v0) = T (v1) and c, d ∈ R. By cv0 + d
we mean a partial assignment v with T (v) = T satisfying v(t) = cv0(t) + d for all t ∈ T . Also, by v0 + v1 we
mean a partial assignment v with T (v) = T satisfying v(t) = v0(t) + v1(t) for all t ∈ T. Also, we say v1 ≥ v0 if
v1(t) ≥ v0(t) for all t ∈ T .

Now we can show how Corollary 3.7 follows from Theorem 3.6. Let v := v1 − v0, and kvk∞ = ǫ, for some ǫ > 0.
Therefore, v0 + ǫ ≥ v1 ≥ v0 − ǫ. Theorem 3.6 then implies that lexG[v0] + ǫ ≥ lex[v1] ≥ lex[v0] − ǫ, hence proving
✷
the corollary.

Proof sketch of Theorem 3.6:
It is easy to see that the ﬁrst statement holds. For the second statement, we ﬁrst
observe that if there is a sequence of paths P1, ..., Pr that is simultaneously a decomposition of both lex[v0] and
lex[v1], then this is easy to see. If such a path sequence doesn’t exist, then we look at vt := v0 + t(v1 − v0). We
state here without a proof (though the proof is elementary) that we can then split the interval [0, 1] into ﬁnitely many
subintervals [a0, a1], [a1, a2], .., [ak−1, ak], with a0 = 0, ak = 1, such that for any i, there is a path sequence P1, ..., Pr
which is a decomposition of lex[vt] for all t ∈ [ai, ai+1]. We then observe that v0 = va0 ≤ va1 ≤ ...vak = v1. Since
for every ai, ai+1, there is a path sequence which is simultaneously a decomposition of both lex[vai ] and lex[vai+1 ],
we immediately get

lex[v0] = lex[va0 ] ≤ lex[va1] ≤ ... ≤ lex[vak ] = lex[v1].

✷

A.3 Alternate Characterizations

Proof of Theorem 3.10: We know that lexG[v0] extends v0. We ﬁrst prove that v = lexG[v0] satisﬁes the max-min
gradient averaging property. Assume to the contrary. Thus, there exists x ∈ VG \ T (v0) such that

max
y:(x,y)∈EG

grad[v](x, y) 6= − min

grad[v](x, y).

y:(x,y)∈EG

Assume that max(x,y)∈EG grad[v](x, y) ≥ − min(x,y)∈EG grad[v](x, y). Then, consider v′ extending v0 that is iden-
tical to v except for v′(x) = v(x) − ǫ for ǫ > 0. For ǫ small enough, we get that

and

max
y:(x,y)∈EG

grad[v′](x, y) < max

grad[v](x, y)

y:(x,y)∈EG

− min

y:(x,y)∈EG

grad[v′](x, y) < max

grad[v](x, y).

y:(x,y)∈EG

The gradient of edges not incident on the vertex x is left unchanged. This implies that grad[v]

6(cid:22) grad[v′],

contradicting the assumption that v is the lex-minimizer. (The other case is similar).

17

For the other direction. Consider a complete voltage assignment v extending v0 that satisﬁes the max-min gradient

averaging property w.r.t. (G, v0). Let

α = max

grad[v](x, y) ≥ 0

(x,y)∈EG
x∈V \T (v0)

be the maximum edge gradient, and consider any edge (x0, x1) ∈ EG such that grad[v](x1, x0) = α, with x1 ∈
V \ T (v0). If α = 0, grad[v] is identically zero, and is trivially the lex-minimal gradient assignment. Thus, both v and
lexG[v0] are constant on each connected component. Since (G, v0) is well-posed, there is at least one terminal in each
component, and hence v and lexG[v0] must be identical.

Now assume α > 0. By the max-min gradient averaging property, ∃x2 ∈ VG such that (x1, x2) ∈ EG and

grad[v](x1, x2) =

min
y:(x1,y)∈EG

grad[v](x1, y) = − max

grad[v](x1, y)

y:(x1,y)∈EG

≤ −grad[v](x1, x0) = −α.

Thus, grad[v](x2, x1) ≥ α. Since α is the maximum edge gradient, we must have grad[v](x2, x1) = α. More-
over, v(x2) > v(x1) > v(x0), thus x2 6= x0. We can inductively apply this argument at x2 until we hit a ter-
minal. Similarly, if x0 /∈ T (v0) we can extend the path in the other direction. Consequently, we obtain a path
P = (xj , . . . , x2, x1, x0, x−1, . . . , xk) with all vertices as distinct, such that xj , xk ∈ T (v0), and xi ∈ V \ T (v0)
for all i ∈ [j + 1, k − 1]. Moreover, grad[v](xi, xi−1) = α for all j < i ≤ k. Thus, P is a free terminal path with
∇P [v0] = α.

Moreover, since v is a voltage assignment extending v0 with

∞ = α, using Lemma 3.5, we know that
every terminal path P ′ in (G, v0) must satisfy ∇P ′(v0) ≤ α. Thus, P is a steepest ﬁxable path in (G, v0). Thus,
letting v1 = ﬁx[v0, P ], using Corollary 3.4, we obtain that lexG[v1] = lexG[v0]. Moreover, since α = ∇P [v0] =
grad[v](xi, xi−1) for all i ∈ (j, k], we get v1(xi) = v(xi) for all i ∈ (j, k). Thus, v extends v1.

grad[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We can iterate this argument for r iterations until T (vr) = VG, giving v = vr and vr = lexG[vr] = lexG[v0].
(Since we are ﬁxing at least one terminal at each iteration, this procedure terminates). Thus, we get v = lexG[v0]. ✷

B Description of the Algorithms

Algorithm 2: MODDIJKSTRA(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs a complete
voltage assignment v for G, and an array parent : V → V ∪ {null}.

Add x to a ﬁbonacci heap, with key(x) = +∞.
ﬁnished(x) ← false

Decrease key(x) to v0(x).
parent(x) ← null.

1. for x ∈ VG,
2.
3.
4. for x ∈ T (v0)
5.
6.
7. while heap is not empty
8.
9.
10.
11.
12.
13.
14.
15. return (v, parent)

x ← pop element with minimum key from heap
v(x) ← key(x). ﬁnished(x) ← true .
for y : (x, y) ∈ EG

if ﬁnished(y) = false

if key(y) > v(x) + α · ℓ(x, y)

Decrease key(y) to v(x) + α · ℓ(x, y).
parent(y) ← x.

Theorem B.1 For a well-posed instance (G, V0) and a gradient value α ≥ 0, let (v, parent) ← MODDIJKSTRA(G, v0, α).
Then, v is a complete voltage assignment such that, ∀x ∈ VG, v(x) = mint∈T (v0){v0(t) + αdist(x, t)}. Moreover, the
pointer array parent satisﬁes ∀x /∈ T (v0), parent(x) 6= null and v(x) = v(parent(x)) + α · ℓ(x, parent(x)).

18

Algorithm 3: Algorithm COMPVLOW(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs
vLow, a complete voltage assignment for G, and an array LParent : V → V ∪ {null}.

1. (vLow, LParent) ← MODDIJKSTRA(G, v0, α)
2. return (vLow, LParent)

Algorithm 4: Algorithm COMPVHIGH(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs
vHigh, a complete voltage assignment for G, and an array HParent : V → V ∪ {null}.

if x ∈ T (v0) then v1(x) ← −v0(x) else v1(x) ← v1(x).

1. for x ∈ VG
2.
3. (temp, HParent) ← MODDIJKSTRA(G, v1, α)
4. for x ∈ VG : vHigh(x) ← −temp(x)
5. return (vHigh, HParent)

Corollary B.2 For a well-posed instance (G, V0) and a gradient value α ≥ 0, let (vLow[α], LParent) ← COMPVLOW(G, v0, α)
and (vHigh[α], HParent) ← COMPVHIGH(G, v0, α). Then, vLow[α], vHigh[α] are complete voltage assignments for
G such that, ∀x ∈ VG,

vLow[α](x) = min

{v0(t) + α · dist(x, t)}

t∈T (v0)

vHigh[α](x) = max
t∈T (v0)

{v0(t) − α · dist(t, x)}.

Moreover, the pointer arrays LParent, HParent satisfy ∀x /∈ T (v0), LParent(x), HParent(x) 6= null and

vLow[α](x) = vLow[α](LParent(x)) + α · ℓ(x, LParent(x)),
vHigh[α](x) = vHigh[α](HParent(x)) − α · ℓ(x, HParent(x)).

Algorithm 5: Algorithm COMPINFMIN(G, v0): Given a well-posed instance (G, v0), outputs a complete voltage assignment
v for G, extending v0 that minimizes (cid:13)

(cid:13)grad[v](cid:13)

(cid:13)∞.

1. α ← max{|grad[v0](e)| | e ∈ EG ∩ (T (v0) × T (v0))}.
2. EG ← EG \ (T (v0) × T (v0))
3. P ←STEEPESTPATH(G, v0).
4. α ← max{α, ∇P (v0)}
5. (vLow, LParent) ← COMPVLOW(G, v0, α)
6. (vHigh, HParent) ← COMPVHIGH(G, v0, α)
7. for x ∈ VG
8.
9.
10.
11. return v

then v(x) ← v0(x)
else v(x) ← 1

2 · (vLow(x) + vHigh(x)).

if x ∈ T (v0)

1. (vLow, LParent) ← COMPVLOW(G, v0, α)
2. (vHigh, HParent) ← COMPVHIGH(G, v0, α)
3. VG′ ← {x ∈ VG | vHigh(x) > vLow(x) }
4. EG′ ← {(x, y) ∈ EG | x, y ∈ VG′ }.

19

Algorithm 6: Algorithm COMPHIGHPRESSGRAPH(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0,
outputs a minimal induced subgraph G′ of G where every vertex has pressure[v0](·) > α.

5. G′ ← (V ′, E′, ℓ)
6. return G′

Proof of Lemma 4.3:

is equivalent to

vHigh[α](x) > vLow[α](x)

max
t∈T (v0)

{v0(t) − α · dist(t, x)} > min

{v0(t) + α · dist(x, t)},

t∈T (v0)

which implies that there exists terminals s, t ∈ T (v0) such that

thus,

Hence,

v0(t) − α · dist(t, x) > v0(s) + α · dist(x, s)

pressure[v0](x) ≥

v0(t) − v0(s)
dist(t, x) + dist(x, s)

> α.

v0(t) − v0(s)
dist(t, x) + dist(x, s)

= pressure[v0](x) > α.

v0(t) − α · dist(t, x) > v0(s) + α · dist(x, s)

So the inequality on vHigh and vLow implies that pressure is strictly greater than α. On the other hand, if pressure[v0](x) >
α, there exists terminals s, t ∈ T (v0) such that

which implies vHigh[α](x) > vLow[α](x).

✷

Algorithm 7: Algorithm STEEPESTPATH(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs a steepest
free terminal path P in (G, v0).

P ← VERTEXSTEEPESTPATH(G, v0, xi)

1. Sample uniformly random e ∈ EG. Let e = (x1, x2).
2. Sample uniformly random x3 ∈ VG.
3. for i = 1 to 3
4.
5. Let j ∈ arg maxj∈{1,2,3} ∇Pj (v0)
6. G′ ← COMPHIGHPRESSGRAPH(G, v0, ∇Pj (v0))
7. if EG′ = ∅,
8.
9.

then return Pj
else return STEEPESTPATH(G′, v0|VG′ )

1. while T (v0) 6= VG
2.
3.
4.
5. return v0

EG ← EG \ (T (v0) × T (v0))
P ← STEEPESTPATH(G, v0)
v0 ← ﬁx[v0, P ]

Algorithm 8: Algorithm COMPLEXMIN(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs lexG[v0].

Algorithm 9: Algorithm VERTEXSTEEPESTPATH(G,v0, x): Given a well-posed instance (G, v0), and a vertex x ∈ VG,
outputs a steepest terminal path in (G, v0) through x.

1. Using Dijkstra’s algorithm, compute dist(x, t) for all t ∈ T (v0)

20

y ← arg maxy∈T (v0)
if v0(x) ≥ v0(y)

|v0(x)−v0(y)|
dist(x,y)

then return a shortest path from x to y
else return a shortest path from y to x

2. if x ∈ T (v0)
3.
4.
5.
6.
7. else
8.
9.
10.
11.

for t /∈ T (v0), d(t) ← dist(x, t)
(t1, t2) ← STARSTEEPESTPATH(T (v0), v0|T (v0), d)
Let P1 be a shortest path from t1 to x. Let P2 be a shortest path from x to t2.
P ← (P1, P2). return P.

Algorithm 10: STARSTEEPESTPATH(T, v, d): Returns the steepest path in a star graph, with a single non-terminal connected
to terminals in T, with lengths given by d, and voltages given by v.

|v(t1)−v(t)|
d(t1)+d(t)

1. Sample t1 uniformly and randomly from T
2. Compute t2 ∈ arg maxt∈T
3. α ← |v(t2)−v(t1)|
d(t1)+d(t2)
4. Compute vlow ← mint∈T (v(t) + α · d(t))
5. Tlow ← {t ∈ T | v(t) > vlow + α · d(t)}
6. Compute vhigh ← maxt∈T (v(t) − α · d(t))
7. Thigh ← {t ∈ T | v(t) < vhigh − α · d(t)}
8. T ′ ← Tlow ∪ Thigh.
9. if T ′ = ∅
10.
11.

then if v(t1) ≥ v(t2) then return (t1, t2) else return (t2, t1)
else return STARSTEEPESTPATH(T ′, v|T ′, dT ′ )

B.1 Faster Lex-minimization

Algorithm 11: Algorithm COMPFASTLEXMIN(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs
lexG[v0].

1. while T (v0) 6= VG
2.
3. return v0

v0 ← FIXPATHSABOVEPRESS(G, v0, 0)

Algorithm 12: Algorithm FIXPATHSABOVEPRESS(G, v0, α): Given a well-posed instance (G, v0), with T (v0) 6= VG, and
a gradient value α, iteratively ﬁxes all paths with gradient > α.

EG ← EG \ (T (v0) × T (v0))
Sample uniformly random e ∈ EG. Let e = (x1, x2).
Sample uniformly random x3 ∈ VG.
for i = 1 to 3

Pi ← VERTEXSTEEPESTPATH(G, v0, xi)

Let j ∈ arg maxj∈{1,2,3} ∇Pj(v0)
G′ ← COMPHIGHPRESSGRAPH(G, v0, ∇Pj (v0))
if EG′ = ∅,

1. while T (v0) 6= VG
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.

then v0 ← ﬁx[v0, P ]
else Let G′

for i = 1, . . . , r

i, i = 1, . . . , r be the connected components of G′.

21

vi ← FIXPATHSABOVEPRESS(G′
for x ∈ VG′

i, set v0(x) ← vi(x)

i, v0|VG′

i

, ∇Pj (v0))

if α > 0 then G ←COMPHIGHPRESSGRAPH(G, v0, α)

13.
14.
15.
16. return v0

C Experiments on WebSpam: Testing More Algorithms

For completeness, in this appendix we show how a number of algorithms perform on the web spam experiment of
Section 6. We consider the following algorithms:

• RANDWALK along in-links. For a detailed description see Zhou et al. (2007). This algorithm essentially per-
forms a Personalized PageRank random walk from each vertex x and computes a spam-value for the vertex x by
taking a weighted average of the labels of the vertices where the random walk from x terminates. Also shown in
Section 6.

• DIRECTEDLEX, with edges in the opposite directions of links. This has the effect that a link to a spam host is

evidence of spam, and a link from a normal host is evidence of normality. Also shown in Section 6.

• RANDWALK along out-links.

• DIRECTEDLEX, with edges in the directions of links. This has the effect that a link from to a spam host is

evidence of spam, and a link to a normal host is evidence of normality.

• UNDIRECTEDLEX: Lex-minimization with links treated as undirected edges.

• LAPLACIAN: l2-regression with links treated as undirected edges.

• DIRECTED 1-NEAREST NEIGHBOR: Uses shortest distance along paths following out-links. Spam-ratio is
deﬁned distance from normal hosts, divided by distance to spam hosts. Sites are ﬂagged as spam when spam-
ratio exceeds some threshold. We also tried following paths along in-links instead, but that gave much worse
results.

We use the experimental setup described in Section 6. Results are shown in Figure 4. The alternative convention
for DIRECTEDLEX orients edges in the directions of links. This takes a link from a spam host to be evidence of
spam, and a link to a normal host to be evidence of normality. This approach performs signiﬁcantly worse than our
preferred convention, as one would intuitively expect. UNDIRECTEDLEX and LAPLACIAN approaches also perform
signiﬁcantly worse. DIRECTED 1-NEAREST NEIGHBOR performs poorly, demonstrating that DIRECTEDLEX is very
different from that approach. As observed by Zhou et al. (2007), sampling based on a random walk following out-links
performs worse than following in-links. Up to 60 % recall, DIRECTEDLEX performs best, both in the regime of 5 %
labels for training and in the regime of 20 % labels for training.

22

5 % labels for training

RandWalk along in-links
DirectedLex
RandWalk along out-links
DirectedLex, opposite link direction
UndirectedLex
Laplacian
Directed 1NN

0
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Recall

20 % labels for training

RandWalk along in-links
DirectedLex
RandWalk along out-links
DirectedLex, opposite link direction
UndirectedLex
Laplacian
Directed 1NN

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

i

i

n
o
s
c
e
r
P

i

i

n
o
s
c
e
r
P

0
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Recall

Figure 4: Recall and precision in the WebSpam classiﬁcation experiment. Each data point shown was computed as an average
over 100 runs. The largest standard deviation of the mean precision across the plotted recall values was less than 1.5 %. The
algorithm of Zhou et al. (2007) appears as RANDWALK (along in-links). We also show RANDWALK along out-links. Our directed
lex-minimization algorithm appears as DIRECTEDLEX. We also show DIRECTEDLEX with link directions reversed, along with
UNDIRECTEDLEX and LAPLACIAN.

D l0-Vertex Regularization Proofs

In this appendix, we prove Theorem 7.1 and Theorem 7.2. For the purposes of proving the second theorem, we intro-
duce an alternative version of problem (3). The optimization problem here requires us to minimize l0-regularization

23

budget required to obtain an inf-minimizer with gradient below a given threshold:

min
v∈IRn
subject to

(cid:13)
(cid:13)

v(T ) − v0(T )

0

gradG[v]

(cid:13)
∞ ≤ α.
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We will also need the following graph construction.

Deﬁnition D.1 The α-pressure terminal graph of a partially-labeled graph (G, v0) is a directed unweighted graph
Gα = (T (v0),

E if and only if there is a terminal path P from s to t in G with

E) such that (s, t) ∈

b

b

∇P (v0) > α.

Note that the α-pressure terminal graph has O(n) vertices but may be dense, even when G is not.

Algorithm 13: Algorithm TERM-PRESSURE: Given a well-posed instance (G, v0) and α ≥ 0, outputs α pressure terminal
graph Gα.
Initialize Gα with vertex set Vα = T (v0) and edge set
for each terminal s ∈ T (v0)

E = ∅.

1. Compute the distances to every other terminal t by running Dijktra’s algorithm, allowing shortest paths

b

2. Use the resulting distances to check for every other terminal t if there is a terminal path P from s to t with

that run through other terminals.

∇P (v0) > α. If there is, add edge (s, t) to

E.

Lemma D.2 The α-pressure terminal graph of a voltage problem (G, v0) can be computed in O((m + n log n)n) time
using algorithm TERM-PRESSURE (Algorithm 13).

b

Proof: The correctness of the algorithm follows from the fact that Dijkstra’s algorithm will identify all shortest
distances between the terminals, and the pressure check will ensure that terminal pairs (s, t) are added to
E if and
only if they are the endpoints of a terminal path P with ∇P (v0) > α. The running time is dominated by performing
Dijkstra’s algorithm once for each terminal. A single run of Dijkstra’s algorithm takes O(m + n log n) time, and this
✷
is performed at most n times, for a total running time of O((m + n log n)n).

b

We make three observations that will turn out to be crucial for proving Theorems 7.1 and 7.2.

Observation D.3 Gα is a subgraph of Gβ for α ≥ β.

Proof: Suppose edge (s, t) appears in Gα, then for some path P

∇P (v0) > α ≥ β,

so the edge also appears in Gβ.

Observation D.4 Gα is transitively closed.

Proof: Suppose edges (s, t) and (t, r) appear in Gα. Let P(s,t), P(t,r), P(s,r) be the respective shortest paths in G
between these terminal pairs. Then

∇P(s,r)(v0) =

v0(s) − v0(r)
ℓ(P(s,r))

≥

v0(s) − v0(r)
ℓ(P(s,t)) + ℓ(P(t,r))

=

v0(s) − v0(t) + v0(t) − v0(r)
ℓ(P(s,t)) + ℓ(P(t,r))

≥ min

v0(s) − v0(t)
ℓ(P(s,t))

,

 

v0(t) − v0(r)

ℓ(P(t,r)) !

> α.

So edge (s, r) also appears in Gα. This is sufﬁcient for Gα to be transitively closed.

24

(6)

✷

(7)

✷

Observation D.5 Gα is a directed acyclic graph.

Proof: Suppose for a contradiction that a directed cycle appears in Gα. Let s and t be two vertices in this cycle. Let
P(s,t) and P(t,s) be the respective shortest paths in G between these terminal pairs. Because Gα is transitively closed,
both edges (s, t) and (t, s) must appear in Gα. But (s, t) ∈

E implies

and similarly (t, s) ∈

E implies

b
This is a contradiction.

v0(s) − v0(t) > αℓ(P(s,t)) > 0,

b

v0(t) − v0(s) > αℓ(P(t,s)) > 0.

✷

The usefulness of the α-pressure terminal graph is captured in the following lemma. We deﬁne a vertex cover of a
directed graph to be a vertex set that constitutes a vertex cover in the same graph with all edges taken to be undirected.

Lemma D.6 Given a partially-labeled graph (G, v0) and a set U ⊆ V , there exists a voltage assignment v ∈ IRn that
satisﬁes

if and only if U is a vertex cover in the α-pressure terminal graph Gα of (G, v0).
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:8)

(cid:9)

t ∈ T (v0) : v(t) 6= v0(t)

⊆ U and

gradG[v]

∞ ≤ α,

Proof: We ﬁrst show the “only if” direction. Suppose for a contradiction that there exists a voltage assignment v for
which
∞ ≤ α, but U is not a vertex cover in Gα. Let (s, t) be an edge Gα which is not covered by U . The
presence of this edge in Gα implies that there exists a terminal path P from s to t in G for which

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∇P (v0) > α.

But, by Lemma 3.5 this means there is no assignment v for G which agrees with v0 on s and t and has
α. This contradicts our assumption.

∞ ≤
(cid:13)
Now we show the “if” direction. Consider an arbitrary vertex cover U of Gα. Suppose for a contradiction that
(cid:13)
⊆ U .

t ∈ T (v0) : v(t) 6= v0(t)

gradG[v]

(cid:13)
(cid:13)

gradG[v]

there does not exist a voltage assignment v for G with
Deﬁne a partial voltage assignment vU given by

∞ ≤ α and

(cid:8)

(cid:9)

vU (t) =

v0(t)
∗

(

(cid:13)
(cid:13)

(cid:13)
(cid:13)
if t ∈ T (v0) \ U
o.w.

∞ ≤ α. By
The preceding statement is equivalent to saying that there is no v that extends vU and has
Lemma 3.5, this means there is terminal path between s, t ∈ T (vU ) with gradient strictly larger than α. But this
means an edge (s, t) is present in Gα and is not covered. This contradicts our assumption that U is a vertex cover. ✷

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We are now ready to prove Theorem 7.2.

∞

(cid:13)
(cid:13)

Proof of Theorem 7.2: We describe and prove the algorithm OUTLIER. The algorithm will reduce problem (3)
to problem (6): Suppose v∗ is an optimal assignment for problem (3).
It achieves a maximum gradient α∗ =
gradG[v∗]
. Using Dijkstra’s algorithm we compute the pairwise shortest distances between all terminals in G.
From these distances and the terminal voltages, we compute the gradient on the shortest path between each terminal
(cid:13)
pair. By Lemma 3.5, α∗ must equal one of these gradients. So we can solve problem (3) by iterating over the set of
(cid:13)
gradients between terminals and solving problem (6) for each of these O(n2) gradients. Among the assignments with
v(T ) − v0(T )

0 ≤ k, we then pick the solution that minimizes
(cid:13)
(cid:13)

In fact, we can do better. By Observation D.3, Gα is a subgraph of Gβ for α ≥ β. This means a vertex cover
(cid:13)
of Gα is also a vertex cover of Gβ, and hence the minimum vertex cover for Gβ is at least as large as the minimum
(cid:13)
vertex cover for Gα. This means we can do a binary search on the set of O(n2) terminal gradients to ﬁnd the minimum
gradient for which there exists an assignment with
0 ≤ k. This way, we only make O(log n) calls to
v(T ) − v0(T )
problem (6), in order to solve problem (3).
(cid:13)
(cid:13)

We use the following algorithm to solve problem (6).

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∞

.

25

1. Compute the α-pressure terminal graph Gα of G using the algorithm TERM-PRESSURE.
2. Compute a minimum vertex cover U of Gα using the algorithm KONIG-COVER from Theorem 7.3.
3. Deﬁne a partial voltage assignment vU given by

vU (t) =

v0(t)
∗

(

if t ∈ T (v0) \ U,
otherwise.

4. Using Algorithm 5, compute voltages v that extend vU and output v.

From Lemma D.2, it follows that step 1 computes the α-pressure terminal graph in polynomial time. From The-
orem 7.3 it follows that step 2 computes the a minimum vertex cover of the α-pressure terminal graph in polynomial
time, because our observations D.4 and D.5 establish that the graph is a TC-DAG. From Lemma D.6 and Theorem 4.6,
it follows that the output voltages solve program (6).

✷

To prove Theorem 7.1, we use the standard greedy approximation algorithm for MIN-VC (Vazirani (2001)).

Theorem D.7 2-Approximation Algorithm for Vertex Cover. The following algorithm gives a 2-approximation to
the Minimum Vertex Cover problem on a graph G = (V, E).

0. Initialize U = ∅.
1. Pick an edge (u, v) ∈ E that is not covered by U .
2. Add u and v to the set U .
3. Repeat from step 1 if there are still edges not covered by U .
4. Output U .

We are now in a position to prove Theorem 7.1

Proof of Theorem 7.1: Given an arbitrary k and a partially-labeled graph (G, v0), let α∗ be the optimum value
of program (3). Observe that by Lemma D.6, this implies that Gα∗ has a vertex cover of size k. Given the partial
assignment v0, for every vertex set U , we deﬁne

vU (t) =

v0(t)
∗

(

if t ∈ T (v0) \ U
o.w.

We claim the following algorithm APPROX-OUTLIER outputs a voltage assignment v with

gradG[v]

∞ ≤ α∗

(cid:13)
(cid:13)

(cid:13)
(cid:13)

and

v(T ) − v0(T )

(cid:13)
(cid:13)

Algorithm APPROX-OUTLIER:

0 ≤ 2k.
(cid:13)
(cid:13)

0. Initialize U = ∅.
1. Using the algorithm STEEPESTPATH (Algorithm 7), ﬁnd a steepest terminal path in G w.r.t. vU . Denote
this path P and let s and t be its terminal endpoints. If there is no terminal path with positive gradient, skip
to step 4.

2. Add s and t to the set U .
3. If |U | ≤ 2k − 2 then repeat from step 1.
4. Using the algorithm COMPINFMIN (Algorithm 5), compute voltages v that extend vU and output v.

From the stopping conditions, it is clear that |U | ≤ 2k. If in step 1 we ever ﬁnd that no terminal paths have positive
∞ = 0 ≤ α∗, by Lemma 3.5. Similarly if we ﬁnd a steepest
gradient then our v that extends vU will have
(cid:13)
(cid:13)

gradG[v]

(cid:13)
(cid:13)

26

gradG[v]

∞ ≤ α∗.

∞ ≤ α∗.
path with gradient less than α∗ w.r.t. vU , then for this U there exists v that extends vU and has
This will continue to hold when if we add vertices to U . Therefore, for the ﬁnal U , there will exist an v that extends
vU and has

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

If we never ﬁnd a steepest terminal path P with ∇P (v0) ≤ α∗, then each steepest path we ﬁnd corresponds to an
edge in Gα∗ that is not yet covered by U and our algorithm in fact implements the greedy approximation algorithm
for vertex cover described in Theorem D.7. This implies that the ﬁnal U is a vertex cover of Gα∗ of size at most 2k.
∞ ≤ α∗. This
By Lemma D.6, this implies that there exists a voltage assignment u extending vU that has
implies by Theorem 4.6 that the v we output has
(cid:13)
(cid:13)
In all cases, the v we output extends vU , so

∞ ≤ α∗.

gradG[u]

(cid:13)
(cid:13)

✷

gradG[v]
v(T ) − v0(T )
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

0 ≤ |U | ≤ 2k.
(cid:13)
(cid:13)

E Proof of Hardness of l0 regularization for l2

We will prove Theorem 7.4, by a reduction from minimum bisection. To this end, let G = (V, E) be any graph. We
will reduce the minimum bisection problem on G to our regularization problem. Let n = |V |. The graph on which we
will perform regularization will have vertex set

V ∪

V ,

V is a set of n vertices that are in 1-to-1 correspondence with V . We assume that every edge in G has weight 1.
V to the corresponding vertex in V by an edge of weight B, for some large B to be
V to each other by edges of weight B3. So, we have a complete
V to V , and the original graph G on V .

where
We now connect every vertex in
determined later. We also connect all of the vertices in
graph of weight B3 edges on
b
The input potential function will be

V , a matching of weight B edges connecting

b

b

b

v(a) =

b
0 for a ∈
1 for a ∈ V .
b

(

V , and

b

Now set k = n/2. We claim that we will be able to determine the value of the minimum bisection from the solution
to the regularization problem.

If S is the set of vertices on which v and w differ, then we know that the w is harmonic on S: for every a ∈ S,

w(a) is the weighted average of the values at its neighbors. In the following, we exploit the fact that |S| ≤ n/2.

Claim E.1 For every a ∈ S ∩

V , w(a) ≤ 2/nB2.

Proof: Let a be the vertex in S ∩
w-value equal to 0 by edges of weight B3. On the other hand, a has only one neighbor that is not in
w-value at most 1, and it is connected to that vertex by an edge of weight B. Call that vertex c. We have

V that maximizes w(a). So, a is connected to at least n/2 neighbors in

V with
V , that vertex has

b

b

b

((n − 1)B3 + B)w(a) = Bw(c) +

B3w(b)

b

b
V ,b6=a
Xb∈

= Bw(c) +

B3w(b) +

B3w(b)

b
V ∩S,b6=a
Xb∈

B3w(a)

≤ B +

b
V ∩S,b6=a
Xb∈
≤ B + (n/2 − 1)B3w(a).

b
V −S
Xb∈

Subtracting (n/2 − 1)B3w(a) from both sides gives

((n/2)B3 + B)w(a) ≤ B,

which implies the claim.

Claim E.2 For a ∈ S ∩ V , w(a) ≤ n/B.

27

✷

V . Let’s call that neighbor c. We know that w(c) ≤ 2/B2n. On the
Proof: Vertex a has exactly one neighbor in
other hand, vertex a has fewer than n − 1 neighbors in V , and each of these have w-value at most 1. Let da denote the
degree of a in G. Then,

b

So,

Let

and

bisection.

and at most

(B + da)w(a) ≤ da + B

2
B2n

.

w(a) ≤

da + 2/Bn
da + B
n + (2/Bn)
B + n

≤

≤ n/B.

|S| = k = n/2.

T = S ∩ V,

t = |T | .

(n − t)B − 4/B
b

(n − t)B + tn2/B.

We now estimate the value of the regularized objective function. To this end, we assume that

We will prove that S ⊂ V and so S = T and t = n/2.

Let δ denote the number of edges on the boundary of T in V . Once we know that t = n/2, δ is the size of a

Claim E.3 The contribution of the edges between V and

V to the objective function is at least

Proof: For the lower bound, we just count the edges between vertices in V \ T and
edges, and each of them has weight B. The endpoint in V \ T has w-value 1, and the endpoint in
most 2/nB2. So, the contribution of these edges is at least

V . There are n − t of these
V has w-value at

b

(n − t)B(1 − 2/nB2)2 ≥ (n − t)B(1 − 4/nB2) ≥ (n − t)B − 4/B.

b

For the upper bound, we observe that the difference in w-values across each of these n − t edges is at most 1, so their
total contribution is at most

Since for every vertex a ∈ T , w(a) ≤ n/B, and also every vertex b ∈
edges between T and

V is at most

t(n/B)2B = tn2/B.

b

b

V , w(b) ≤ 2/nB2, the contribution due to

We will see that this is the dominant term in the objective function. The next-most important term comes from the

edges in G.

(n − t)B.

28

✷

✷

Claim E.4 The contribution of the edges in G to the objective function is at least

and at most

δ(1 − 2n/B)

δ + (t2/2)(n/B)2

δ(1 − 2n/B) and δ.

(t2/2)(n/B)2.

Proof: Let (a, b) ∈ E. If neither a nor b is in T , then w(a) = w(b) = 1, and so this edge has no contribution. If
a ∈ T but b 6∈ T , then the difference in w-values on them is between (1 − n/B) and 1. So, the contribution of such
edges to the objective function is between

Finally, if a and b are in T , then the difference in w-values on them is at most n/B, and so the contribution of all such
edges to the objective function is at most

Claim E.5 The edges between pairs of vertices in

V contribute at most 2/B to the objective function.

Proof: As 0 ≤ w(a) ≤ 2/B2n for every a ∈

V , every edge between two vertices in

V can contribute at most

b

As there are fewer than n2/2 such edges, their total contribution to the objective function is at most

B3(2/B2n)2 = 4/Bn2.
b

b

(n2/2)(4/Bn2) = 2/B.

Lemma E.6 If n ≥ 4 and B = 2n3, the value of the objective function is at least

and at most

(n − t)B + δ − 1/2

(n − t)B + δ + 1/3.

Proof: Summing the contributions in the preceding three claims, we see that the value of the objective function is at
least

(n − t)B − 4/B + δ(1 − 2n/B) ≥ (n − t)B + δ − 4/B − 2nδ/B

≥ (n − t)B + δ − n3/B
≥ (n − t)B + δ − 1/2,

as δ ≤ (n/2)2.

Similarly, the objective function is at most

(n − t)B + tn2/B + δ + (t2/2)(n/B)2 + 2/B ≤ (n − t)B + n3/2B + δ + n4/8B2 + 2/B
≤ (n − t)B + n3/2B + δ + 1/32n2 + 1/n3
≤ (n − t)B + δ + 1/3.

Claim E.7 If n ≥ 2 and B = 2n3, then S ⊂ V .

Proof: The objective function is minimized by making t as large as possible, so t = n/2 and S ⊂ V .

29

✷

✷

✷

✷

Theorem E.8 The value of the objective function reveals the value of the minimum bisection in G.

Proof: The value of the objective function will be between

and

(n/2)B + δ − 1/2

(n/2)B + δ + 1/3.

So, the objective function will be smallest when δ is as small as possible.

✷

Theorem E.8 immediately implies Theorem 7.4.

30

5
1
0
2
 
n
u
J
 
0
3
 
 
]

G
L
.
s
c
[
 
 
2
v
0
9
2
0
0
.
5
0
5
1
:
v
i
X
r
a

Algorithms for Lipschitz Learning on Graphs ∗†

Rasmus Kyng
Yale University
rasmus.kyng@yale.edu

Anup Rao
Yale University
anup.rao@yale.edu

Sushant Sachdeva
Yale University
sachdeva@cs.yale.edu

Daniel A. Spielman
Yale University
spielman@cs.yale.edu

July 1, 2015

Abstract

We develop fast algorithms for solving regression problems on graphs where one is given the value of a function
at some vertices, and must ﬁnd its smoothest possible extension to all vertices. The extension we compute is the
absolutely minimal Lipschitz extension, and is the limit for large p of p-Laplacian regularization. We present an
algorithm that computes a minimal Lipschitz extension in expected linear time, and an algorithm that computes
an absolutely minimal Lipschitz extension in expected time eO(mn). The latter algorithm has variants that seem
to run much faster in practice. These extensions are particularly amenable to regularization: we can perform l0-
regularization on the given values in polynomial time and l1-regularization on the initial function values and on graph
edge weights in time eO(m3/2).

Our deﬁnitions and algorithms naturally extend to directed graphs.

1 Introduction

We consider a problem in which we are given a weighted undirected graph G = (V, E, ℓ) and values v0 : T → R
on a subset T of its vertices. We view the weights ℓ as indicating the lengths of edges, with shorter length indicating
greater similarity. Our goal it to assign values to every vertex v ∈ V \T so that the values assigned are as smooth as
possible across edges. A minimal Lipschitz extension of v0 is a vector v that minimizes

max
(x,y)∈E

(ℓ(x, y))−1

v(x) − v(y)

,

(cid:12)
(cid:12)

(cid:12)
(cid:12)

subject to v(x) = v0(x) for all x ∈ T . We call such a vector an inf-minimizer. Inf-minimizers are not unique. So,
among inf-minimizers we seek vectors that minimize the second-largest absolute value of ℓ(x, y)−1
v(x) − v(y)
across edges, and then the third-largest given that, and so on. We call such a vector v a lex-minimizer. It is also known
(cid:12)
as an absolutely minimal Lipschitz extension of v0.
(cid:12)
These are the limit of the solution to p-Laplacian minimization problems for large p, namely the vectors that solve

(cid:12)
(cid:12)

(1)

(2)

min
v∈Rn

v|T =v0|T X(x,y)∈E

(ℓ(x, y))−p|v(x) − v(y)|p.

The use of p = 2 was suggested in the foundational paper of Zhu et al. (2003), and is particularly nice because it can
be obtained by solving a system of linear equations in a symmetric diagonally dominant matrix, which can be done

∗This research was partially supported by AFOSR Award FA9550-12-1-0175, NSF grant CCF-1111257, a Simons Investigator Award to Daniel

Spielman, and a MacArthur Fellowship.

†Code used in this work is available at https://github.com/danspielman/YINSlex

1

very quickly (Cohen et al. (2014)). The use of larger values of p has been discussed by Alamgir and Luxburg (2011),
and by Bridle and Zhu (2013), but it is much more complicated to compute. The fastest algorithms we know for this
problem require convex programming, and then require very high accuracy to obtain the values at most vertices. By
taking the limit as p goes to inﬁnity, we recover the lex-minimizer, which we will show can be computed quickly.

The lex-minimization problem has a remarkable amount of structure. For example, in uniformly weighted graphs
the value of the lex-minimizer at every vertex not in T is equal to the average of the minimum and maximum of the
values at its neighbors. This is analogous to the property of the 2-Laplacian minimizer that the value at every vertex
not in T equals the average of the values at its neighbors.

1.1 Contributions

We ﬁrst present several important structural properties of lex-minimizers in Section 3.2. As we shall point out, some
of these were known from previous work, sometimes in restricted settings. We state them generally and prove them
for completeness. We also prove that the lex-minimizer is as stable as possible under perturbations of v0 (Section 3.1).
The structure of the lex-minimization problem has led us to develop elegant algorithms for its solution. Both the
algorithms and their analyses could be taught to undergraduates. We believe that these algorithms could be used in
place of 2-Laplacian minimization in many applications.

We present algorithms for the following problems. Throughout, m = |E| and n = |V |.

Inf-minimization: An algorithm that runs in expected time O(m + n log n) (Section 4.3).

Lex-minimization: An algorithm that runs in expected time O(n(m + n log n)) (Section 4), along with a variant that

runs quickly in practice (Section 4.4).

l1-regularization of edge lengths for inf-minimization: The problem of minimizing (1) given a limited budget with
O(m3/2)
which one can increase edge lengths is a linear programming problem. We show how to solve it in time
with an interior point method by using fast Laplacian solvers (Section 8). The same algorithm can accommodate
l1-regularization of the values given in v0.

e

l0-regularization of vertex values for inf-minimization: We give a polynomial time algorithm for l0-regularization
of the values at vertices. That is, we minimize (1) given a budget of a number of vertices that can be proclaimed
outliers and removed from T (Section 7.1). We solve this problem by reducing it to the problem of computing
minimum vertex covers on transitively closed directed acyclic graphs, a special case of minimum vertex cover
that can be solved in polynomial time.

After any regularization for inf-minimization, we suggest computing the lex-minimizer. We ﬁnd the result for l0-
regularization of vertex values to be particularly surprising, especially because we prove that the analogous problem
for 2-Laplacian minimization is NP-Hard (Section 7.2).

All of our algorithms extend naturally to directed graphs (Section 5). This is in contrast with the problem of
minimizing 2-Laplacians on directed graphs, which corresponds to computing electrical ﬂows in networks of resistors
and diodes, for which fast algorithms are not presently known.

We present a few experiments on examples demonstrating that the lex-minimizer can overcome known deﬁcien-
cies of the 2-Laplacian minimizer (Section 1.2, Figures 1,2), as well as a demonstration of the performance of the
directed analog of our algorithms on the WebSpam dataset of Castillo et al. (2006) (Section 6). In the WebSpam prob-
lem we use the link structure of a collection of web sites to ﬂag some sites as spam, given a small number of labeled
sites known to be spam or normal.

1.2 Relation to Prior Work

We ﬁrst encountered the idea of using the minimizer of the 2-Laplacian given by (2) for regression and classiﬁca-
tion on graphs in the work of Zhu et al. (2003) and Belkin et al. (2004) on semi-supervised learning. These works
transformed learning problems on sets of vectors into problems on graphs by identifying vectors with vertices and
constructing graphs with edges between nearby vectors. One shortcoming of this approach (see Nadler et al. (2009),

2

e
g
a

t
l

 

o
V
d
e
r
r
e

f

n

I

1

0.8

0.6

0.4

0.2

0

-0.2

-0.4

-0.6

-0.8

-1

-4

50 lex
50 l2
100 lex
100 l2
500 lex
500 l2
1000 lex
1000 l2

0.25

0.2

r
o
r
r
e
 
1
l
 
n
a
e
M

0.15

0.1

0.05

0
5000 

lex
2-Lap
labels

-2

0

2
Vertex position on real line

4

6

8

Figure 1: Lex vs 2-Laplacian on 1D gaussian clus-
ters.

Figure 2: kNN graphs on samples from 4D cube.

10000

20000

40000

80000

Number of Vertices

Alamgir and Luxburg (2011), Bridle and Zhu (2013)) is that if the number of vectors grows while the number of la-
beled vectors remains ﬁxed, then almost all the values of the 2-Laplacian minimizer converge to the mean of the
labels on most natural examples. For example, Nadler et al. (2009) consider sampling points from two Gaussian
distributions centered at 0 and 4 on the real line. They place edges between every pair of points (x, y) with length
exp(|x − y|2 /2σ2) for σ = 0.4, and provide only the labels v0(0) = −1 and v0(4) = 1. Figure 1 shows the values
of the 2-Laplacian minimizer in red, which are all approximately zero. In contrast, the values of the lex-minimizer in
blue, which are smoothly distributed between the labeled points, are shown.

The “manifold hypothesis” (see Chapelle et al. (2010), Ma and Fu (2011)) holds that much natural data lies near a
low-dimensional manifold and that natural functions we would like to learn on this data are smooth functions on the
manifold. Under this assumption, one should expect lex-minimizers to interpolate well. In contrast, the 2-Laplacian
minimizers degrade (dotted lines) if the number of labeled points remains ﬁxed while the total number of points grows.
In Figure 2, we demonstrate this by sampling many points uniformly from the unit cube in 4 dimensions, form their
8-nearest neighbor graph, and consider the problem of regressing the ﬁrst coordinate. We performed 8 experiments,
varying the number of labeled points in {50, 100, 500, 1000}. Each data point is the mean average l1 error over 100
experiments. The plots for root mean squared error are similar. The standard deviation of the estimations of the mean
are within one pixel, and so are not displayed. The performance of the lex-minimizer (solid lines) does not degrade as
the number of unlabeled points grows.

Analogous to our inf-minimizers, minimal Lipschitz extensions of functions in Euclidean space and over more
general metric spaces have been studied extensively in Mathematics (Kirszbraun (1934), McShane (1934), Whitney
(1934)). von Luxburg and Bousquet (2003) employ Lipschitz extensions on metric spaces for classiﬁcation and relate
these to Support Vector Machines. Their work inspired improvements in classiﬁcation and regression in metric spaces
with low doubling dimension (Gottlieb et al. (2013), Gottlieb et al. (2013b)). Theoretically fast, although not actually
practical, algorithms have been given for constructing minimal Lipschitz extensions of functions on low-dimensional
Euclidean spaces (Fefferman (2009a), Fefferman and Klartag (2009), Fefferman (2009b)). Sinop and Grady (2007)
suggest using inf-minimizers for binary classiﬁcation problems on graphs. For this special case, where all of the
given values are either 0 or 1, they present an O(m + n log n) time algorithm for computing an inf-minimizer. The
case of general given values, which we solve in this paper, is much more complicated. To compensate for the non-
uniqueness of inf-minimizers, they suggest choosing the inf-minimizer that minimizes (2) with p = 2. We believe that
the lex-minimizer is a more natural choice.

The analog of our lex-minimizer over continuous spaces is called the absolutely minimal Lipschitz extension
(AMLE). Starting with the work of Aronsson (1967), there have been several characterizations and proofs of the ex-
istence and uniqueness of the AMLE (Jensen (1993), Crandall et al. (2001), Barles and Busca (2001), Aronsson et al.
(2004)). Many of these results were later extended to general metric spaces, including graphs (Milman (1999),
Peres et al. (2011), Naor and Shefﬁeld (2010), Shefﬁeld and Smart (2010)). However, to the best of our knowledge,
fast algorithms for computing lex-minimizers on graphs were not known. For the special case of undirected, un-
weighted graphs, Lazarus et al. (1999) presented both a polynomial-time algorithm and an iterative method. Oberman

3

(2011) suggested computing the AMLE in Euclidean space by ﬁrst discretizing the problem and then solving the cor-
responding graph problem by an iterative method. However, no run-time guarantees were obtained for either iterative
method.

2 Notation and Basic Deﬁnitions

Lexicographic Ordering. Given a vector r ∈ Rm, let πr denote a permutation that sorts r in non-increasing order
by absolute value, i.e., ∀i ∈ [m − 1], |r(πr(i))| ≥ |r(πr(i + 1))|. Given two vectors r, s ∈ Rm, we write r (cid:22) s to
indicate that r is smaller than s in the lexicographic ordering on sorted absolute values, i.e.

∃j ∈ [m],

r(πr(j))

<

s(πs(j))

and ∀i ∈ [j − 1],

r(πr(i))

=

s(πs(i))

or ∀i ∈ [m],

=

r(πr(i))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.
(cid:12)
(cid:12)

s(πs(i))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

Note that it is possible that r (cid:22) s and s (cid:22) r while r 6= s. It is a total relation: for every r and s at least one of r (cid:22) s
or s (cid:22) r is true.

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Graphs and Matrices. We will work with weighted graphs. Unless explicitly stated, we will assume that they are
undirected. For a graph G, we let VG be its set of vertices, EG be its set of edges, and ℓG : EG → R+ be the
assignment of positive lengths to the edges. We let |VG| = n, and |EG| = m. We assume ℓG is symmetric, i.e.,
ℓG(x, y) = ℓG(y, x). When G is clear from the context, we drop the subscript.

A path P in G is an ordered sequence of (not necessarily distinct) vertices P = (x0, x1, . . . , xk), such that
(xi−1, xi) ∈ E for i ∈ [k]. The endpoints of P are denoted by ∂0P = x0, ∂1P = xk. The set of interior vertices
of P is deﬁned to be int(P ) = {xi : 0 < i < k}. For 0 ≤ i < j ≤ k, we use the notation P [xi : xj] to denote the
k
subpath (xi, . . . , xj). The length of P is ℓ(P ) =
i=1 ℓ(xi−1, xi).
A function v0 : V → R ∪ {∗} is called a voltage assignment (to G). A vertex x ∈ V is a terminal with
respect to v0 iff v0(x) 6= ∗. The other vertices, for which v0(x) = ∗, are non-terminals. We let T (v0) denote the
set of terminals with respect to v0. If T (v0) = V, we call v0 a complete voltage assignment (to G). We say that an
assignment v : V → R ∪ {∗} extends v0 if v(x) = v0(x) for all x such that v0(x) 6= ∗.

Given an assignment v0 : V → R ∪ {∗}, and two terminals x, y ∈ T (v0) for which (x, y) ∈ E, we deﬁne the

P

gradient on (x, y) due to v0 to be

gradG[v0](x, y) =

v0(x) − v0(y)
ℓ(x, y)

.

It may be useful to view gradG[v0](x, y) as the current in the edge (x, y) induced by voltages v0. When v0 is a
complete voltage assignment, we interpret gradG[v0] as a vector in Rm, with one entry for each edge. However, for
convenience, we deﬁne gradG[v0](x, y) = −gradG[v0](y, x). When G is clear from the context, we drop the subscript.
A graph G along with a voltage assignment v to G is called a partially-labeled graph, denoted (G, v). We say
that a partially-labeled graph (G, v0) is a well-posed instance if for every maximal connected component H of G, we
have T (v0) ∩ VH 6= ∅.

A path P in a partially-labeled graph (G, v0) is called a terminal path if both endpoints are terminals. We deﬁne

∇P (v0) to be its gradient:

∇P (v0) =

v0(∂0P ) − v0(∂1P )
ℓ(P )

.

If P contains no terminal-terminal edges (and hence, contains at least one non-terminal), it is a free terminal path.

Lex-Minimization. An instance of the LEX-MINIMIZATION problem is described by a partially-labeled graph
(G, v0). The objective is to compute a complete voltage assignment v : VG → R extending v0 that lex-minimizes
grad[v].

Deﬁnition 2.1 (Lex-minimizer) Given a partially-labeled graph (G, v0), we deﬁne lexG[v0] to be a complete voltage
assignment to V that extends v0, and such that for every other complete assignment v′ : VG → R that extends v0, we
have gradG[lexG[v0]] (cid:22) gradG[v′]. That is, lexG[v0] achieves a lexicographically-minimal gradient assignment to the
edges.

We call lexG[v0] the lex-minimizer for (G, v0). Note that if T (v0) = VG, then trivially, lexG[v0] = v0.

4

3 Basic Properties of Lex-Minimizers

Lazarus et al. (1999) established that lex-minimizers in unweighted and undirected graphs exist, are unique, and may
be computed by an elementary meta-algorithm. We state and prove these facts for undirected weighted graphs, and
defer the discussion of the directed case to Section 5. We also state for directed and weighted graphs characterizations
of lex-minimizers that were established by Peres et al. (2011), Naor and Shefﬁeld (2010) and Shefﬁeld and Smart
(2010) for unweighted graphs. These results are essential for the analyses of our algorithms. We defer most proofs to
Appendix A.

Deﬁnition 3.1 A steepest ﬁxable path in an instance (G, v0) is a free terminal path P that has the largest gradient
∇P (v0) amongst such paths.

Observe that a steepest ﬁxable path with ∇P (v0) 6= 0 must be a simple path.
Deﬁnition 3.2 Given a steepest ﬁxable path P in an instance (G, v0), we deﬁne ﬁxG[v0, P ] : VG → R ∪ {∗} to be the
voltage assignment deﬁned as follows

ﬁxG[v0, P ](x) =

v0(∂0P ) − ∇P (v0) · ℓG(P [∂0P : x]) x ∈ int(P ) \ T (v0),
v0(x)

otherwise.

(

We say that the vertices x ∈ int(P ) are ﬁxed by the operation ﬁx[v0, P ]. If we deﬁne v1 = ﬁxG[v0, P ], where
P = (x0, . . . , xr) is the steepest ﬁxable path in (G, v0), then it is easy to argue that for every i ∈ [r], we have
grad[v1](xi−1, xi) = ∇P (see Lemma A.5). The meta-algorithm META-LEX, spelled out as Algorithm 1, entails
repeatedly ﬁxing steepest ﬁxable paths. While it is possible to have multiple steepest ﬁxable paths, the result of ﬁxing
all of them does not depend on the order in which they are ﬁxed.

Theorem 3.3 Given a well-posed instance (G, v0), the meta-algorithm META-LEX, which repeatedly ﬁxes steepest
ﬁxable paths, produces the unique lex-minimizer extending v0.

Corollary 3.4 Given a well-posed instance (G, v0) such that T (v0) 6= VG, let P be a steepest ﬁxable path in (G, v0).
Then, (G, ﬁx[v0, P ]) is also a well-posed instance, and lexG[ﬁx[v0, P ]] = lexG[v0].

Since a lex-minimal element must be an inf-minimizer, we also obtain the following corollary, that can also be

proved using LP duality.

Lemma 3.5 Suppose we have a well-posed instance (G, v0). Then, there exists a complete voltage assignment v
extending v0 such that

grad[v]

∞ ≤ α, iff every terminal path P in (G, v0) satisﬁes ∇P (v0) ≤ α.
(cid:13)
(cid:13)

3.1 Stability

(cid:13)
(cid:13)

The following theorem states that lexG[v0] is monotonic with respect to v0 and it respects scaling and translation of
v0.

Theorem 3.6 Let (G, v0) be a well-posed instance with T := T (v0) as the set of terminals. Then the following
statements hold.

1. For any c, d ∈ R, v1 a partial assignment with terminals T (v1) = T and v1(t) = cv0(t) + d for all t ∈ T .

Then, lexG[v1](i) = c · lexG[v0](i) + d for all i ∈ VG.

2. v1 a partial assignment with terminals T (v1) = T. Suppose further that v1(t) ≥ v0(t) for all t ∈ T. Then,

lexG[v1](i) ≥ lexG[v0](i) for all i ∈ VG.

As a corollary, the above theorem gives a nice stability property that lex-minimal elements satisfy.

Corollary 3.7 Given well-posed instances (G, v0), (G, v1) such that T := T (v0) = T (v1), let ǫ := maxt∈T |v0(t) −
v1(t)|. Then |lexG[v0](i) − lexG[v1](i)| ≤ ǫ for all i ∈ VG.

5

3.2 Alternate Characterizations

There are at least two other seemingly disparate deﬁnitions that are equivalent to lex-minimal voltages.

lp-norm Minimizers. As mentioned in the introduction, for a well-posed instance (G, v0) the lex-minimizer is also
the limit of lp minimizers. This follows from existing results about the limit of lp-minimizers (Egger and Huotari
(1990)) in afﬁne spaces, since {grad[v] | v is complete, v extends v0} forms an afﬁne subspace of Rm. Thus, we have
the following theorem:

Theorem 3.8 (Limit of lp-minimizers, follows from Egger and Huotari (1990)) For any p ∈ (1, ∞), given a well-
posed instance (G, v0) deﬁne vp to be the unique complete voltage assignment extending v0 and minimizing
p ,
i.e.

grad[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Then, limp→∞ vp = lexG[v0].

vp = arg min
v is complete
v extends v0 (cid:13)
(cid:13)

grad[v]

p .

(cid:13)
(cid:13)

Max-Min Gradient Averaging. Consider a well-posed instance (G, v0), and a complete voltage assignment v ex-
tending v0. If G is such that ℓ(e) = 1 for all e ∈ EG, it is easy to see that lex = lexG[v0] satisﬁes the following simple
condition for all x ∈ VG \ T (v0),

lex(x) =

1
2  

max
(x,y)∈EG

lex(y) + min

lex(z)

.

(x,z)∈EG

!

This condition should be contrasted to the optimality condition for l2-regularization on these instances, which gives
for all non-terminals x, the optimal voltage v satisﬁes v(x) = 1

y:(x,y)∈EG v(y).

deg(x)

To prove the above claim, consider locally changing lex at x and observe that the gradients of edges not incident
at x remain unchanged, and at least one of edges incident at x will have a strictly larger gradient, contradicting lex-
minimality. For general graphs, this condition of local optimality can still be characterized by a simple max-min
gradient averaging property as described below.

P

Deﬁnition 3.9 (Max-Min Gradient Averaging) Given a well-posed instance (G, v0), and a complete voltage as-
signment v extending v0, we say that v satisﬁes the max-min gradient averaging property (w.r.t. (G, v0)) if for every
x ∈ VG \ T (v0), we have

grad[v](x, y) = − min

grad[v](x, y).

max
y:(x,y)∈EG

y:(x,y)∈EG

As stated in the theorem below, lexG[v0] is the unique assignment satisfying max-min gradient averaging property.
Shefﬁeld and Smart (2010) proved a variant of this statement for weighted graphs. For completeness, we present a
proof in the appendix.

Theorem 3.10 Given a well-posed instance (G, v0), lexG[v0] satisﬁes max-min gradient averaging property. More-
over, it is the unique complete voltage assignment extending v0 that satisﬁes this property w.r.t. (G, v0).

An advantage of this characterization is that it can be veriﬁed quickly. This is particularly useful for implementations
for computing the lex-minimizer.

4 Algorithms

We now sketch the ideas behind our algorithms and give precise statements of our results. A full description of all the
algorithms is included in the appendix.

We deﬁne the pressure of a vertex to be the gradient of the steepest terminal path through it:

pressure[v0](x) = max{∇P (v0) | P is a terminal path in (G, v0) and x ∈ P }.

6

Observe that in a graph with no terminal-terminal edges, a free terminal path is a steepest ﬁxable path iff its gradient
is equal to the highest pressure amongst all vertices. Moreover, vertices that lie on steepest ﬁxable paths are exactly
the vertices with the highest pressure. For a given α > 0, in order to identify vertices with pressure exceeding α, we
compute vectors vHigh[α](x) and vLow[α](x) deﬁned as follows in terms of dist, the metric on V induced by ℓ:

vLow[α](x) = min

{v0(t) + α · dist(x, t)}

t∈T (v0)

vHigh[α](x) = max
t∈T (v0)

{v0(t) − α · dist(t, x)}.

4.1 Lex-minimization on Star Graphs

We ﬁrst consider the problem of computing the lex-minimizer on a star graph in which every vertex but the center is a
terminal. This special case is a subroutine in the general algorithm, and also motivates some of our techniques.

Let x be the center vertex, T be the set of terminals, and all edges be of the form (x, t) with t ∈ T . The initial
voltage assignment is given by v : T → R, and we abbreviate dist(x, t) by d(t) = ℓ(x, t). From Corollary 3.4 we know
that we can determine the value of the lex minimizer at x by ﬁnding a steepest ﬁxable path. By deﬁnition, we need to
ﬁnd t1, t2 ∈ T that maximize the gradient of the path from t1 to t2, ∇(t1, t2) = v(t1)−v(t2)
d(t2)+d(t2) . As observed above, this
is equivalent to ﬁnding a terminal with the highest pressure. We now present a simple randomized algorithm for this
problem that runs in expected linear time.

Given a terminal t1, we can compute its pressure α along with the terminal t2 such that |∇(t1, t2)| = α in time
O(|T |) by scanning over the terminals in T . Consider doing this for a random terminal t1. We will show that in linear
time one can then ﬁnd the subset of terminals T ′ ⊂ T whose pressure is greater than α. Assuming this, we complete
the analysis of the algorithm. If T ′ = ∅, t1 is a vertex with highest pressure. Hence the path from t1 to t2 is a steepest
ﬁxable path, and we return (t1, t2). If T ′ 6= ∅, the terminal with the highest pressure must be in T ′, and we recurse by
picking a new random t1 ∈ T ′. As the size of T ′ will halve in expectation at each iteration, the expected time of the
algorithm on the star is O(|T |).

To determine which terminals have pressure exceeding α, we observe that the condition ∃t2 : α < ∇(t1, t2) =
v(t1)−v(t2)
d(t1)+d(t2) , is equivalent to ∃t2 : v(t2)+αd(t2) < v(t1)−αd(t1). This, in turn, is equivalent to vLow[α](x) < v(t1)−
αd(t1). We can compute vLow[α](x) in deterministic O(|T |) time. Similarly, we can check if ∃t2 : α < ∇(t2, t1) by
checking if vHigh[α](x) > vt1 + αd(t1). Thus, in linear time, we can compute the set T ′ of terminals with pressure
exceeding α. The above algorithm is described in Algorithm 10.

Theorem 4.1 Given a set of terminals T, initial voltages v : T → R, and distances d : T → R+, STARSTEEPESTPATH(T, v, d)
returns (t1, t2) maximizing v(t1)−v(t2)

d(t1)+d(t2) , and runs in expected time O(|T |).

4.2 Lex-minimization on General Graphs

Theorem 3.3, tells us that META-LEX will compute lex-minimizers given an algorithm for ﬁnding a steepest ﬁxable
path in (G, v0). Recall that ﬁnding a steepest ﬁxable path is equivalent to ﬁnding a path with gradient equal to the
highest pressure amongst all vertices. In this section, we show how to do this in expected time O(m + n log n).

We describe an algorithm VERTEXSTEEPESTPATH that ﬁnds a terminal path P through any vertex x such that
∇P (v0) = pressure[v0](x) in expected O(m + n log n) time. Using Dijkstra’s algorithm, we compute dist(x, t) for
all t ∈ T. If x ∈ T (v0), then there must be a terminal path P that starts at x that has ∇P (v0) = pressure[v0](x). To
compute such a P we examine all t ∈ T (v0) in O(|T |) time to ﬁnd the t that maximizes |∇(x, t)| = |v(x)−v(t)|
, and
dist(x,t)
then return a shortest path between x and that t.

If x /∈ T (v0), then the steepest path through x between terminals t1 and t2 must consist of shortest paths between
x and t1 and between x and t2. Thus, we can reduce the problem to that of ﬁnding the steepest path in a star graph
where x is the only non-terminal and is connected to each terminal t by an edge of length dist(x, t). By Theorem 4.1,
we can ﬁnd this steepest path in O(|T |) expected time. The above algorithm is formally described as Algorithm 9.

Theorem 4.2 Given a well-posed instance (G, v0), and a vertex x ∈ VG, VERTEXSTEEPESTPATH(G, v0, x) returns
a terminal path P through x such that ∇P (v0) = pressure[v0](x), in O(m + n log n) expected time.

7

As in the algorithm for the star graph, we need to identify the vertices whose pressure exceeds a given α. For a ﬁxed
α, we can compute vLow[α](x) and vHigh[α](x) for all x ∈ VG using a simple modiﬁcation of Dijkstra’s algorithm in
O(m + n log n) time. We describe the algorithms COMPVHIGH, COMPVLOW for these tasks in Algorithms 3 and 4.
The following lemma encapsulates the usefulness of vLow and vHigh.

Lemma 4.3 For every x ∈ VG, pressure[v0](x) > α iff vHigh[α](x) > vLow[α](x).

It immediately follows that the algorithm COMPHIGHPRESSGRAPH(G, v0, α) described in Algorithm 6 computes

the vertex induced subgraph on the vertex set {x ∈ VG| pressure[v0](x) > α}.

We can combine these algorithms into an algorithm STEEPESTPATH that ﬁnds the steepest ﬁxable path in (G, v0)
in O(m + n log n) expected time. We may assume that there are no terminal-terminal edges in G. We sample an edge
(x1, x2) uniformly at random from EG, and a terminal x3 uniformly at random from VG. For i = 1, 2, 3, we compute
the steepest terminal path Pi containing xi. By Theorem 4.2, this can be done in O(m + n log n) expected time. Let α
be the largest gradient maxi ∇Pi. As mentioned above, we can identify G′, the induced subgraph on vertices x with
pressure exceeding α, in O(m + n log n) time. If G′ is empty, we know that the path Pi with largest gradient is a
steepest ﬁxable path. If not, a steepest ﬁxable path in (G, v0) must be in G′, and hence we can recurse on G′. Since
we picked a uniformly random edge, and a uniformly random vertex, the expected size of G′ is at most half that of G.
Thus, we obtain an expected running time of O(m + n log n). This algorithm is described in detail in Algorithm 7.

Theorem 4.4 Given a well-posed instance (G, v0) with EG ∩ (T (v0) × T (v0)) = ∅, STEEPESTPATH(G, v0) returns
a steepest ﬁxable path in (G, v0), and runs in O(m + n log n) expected time.

By using STEEPESTPATH in META-LEX, we get the COMPLEXMIN, shown in Algorithm 1. From Theorem 3.3 and
Theorem 4.4, we immediately get the following corollary.

Corollary 4.5 Given a well-posed instance (G, v0) as input, algorithm COMPLEXMIN computes a lex-minimizing
assignment that extends v0 in O(n(m + n log n)) expected time.

4.3 Linear-time Algorithm for Inf-minimization

Given the algorithms in the previous section, it is straightforward to construct an inﬁnity minimizer. Let α⋆ be the
gradient of the steepest terminal path. From Lemma 3.5, we know that the norm of the inf minimizer is α⋆. Considering
all trivial terminal paths (terminal-terminal edges), and using STEEPESTPATH, we can compute α⋆ in randomized
O(m+n log n) time. It is well known (McShane (1934); Whitney (1934)) that v1 = vLow[α⋆] and v2 = vHigh[α⋆] are
inf-minimizers. It is also known that 1
2 (v1 + v2) is the inf-minimizer that minimizes the maximum ℓ∞-norm distance
to all inf-minimizers. In the case of path graphs, this was observed by Gaffney and Powell (1976) and independently
by Micchelli et al. (1976). For completeness, the algorithm is presented as Algorithm 5, and we have the following
result.

Theorem 4.6 Given a well-posed instance (G, v0), COMPINFMIN(G, v0) returns a complete voltage assignment v
for G extending v0 that minimizes

∞ , and runs in randomized O(m + n log n) time.

grad[v]

4.4 Faster Algorithms for Lex-minimization

(cid:13)
(cid:13)

(cid:13)
(cid:13)

The lex-minimizer has additional structure that allows one to compute it by more efﬁcient algorithms. One observation
that leads to a faster implementation is that ﬁxing a steepest ﬁxable path does not increase the pressure at vertices,
provided that one appropriately ignores terminal-terminal edges. Thus, if G(α) is a subgraph that we identiﬁed with
pressure greater than α, we can iteratively ﬁx all steepest ﬁxable paths P in G(α) with ∇P > α. Another simple
observation is that if G(α) is disconnected, we can simply recurse on each of the connected components. A complete
description of an the algorithm COMPFASTLEXMIN based on these idea is given in Algorithm 11. The algorithm
provably computes lexG(v0), and it is possible to implement it so that the space requirement is only O(m + n).
Although, we are unable to prove theoretical bounds on the running time that are better than O(n(m + n log n)),
it runs extremely quickly in practice. We used it to perform the experiments in this paper. For random regular
graphs and Delaunay graphs, with n = 0.5 × 106 vertices and around 2 million edges m ∼ 1.5 − 2 × 106, it

8

takes a couple of minutes on a 2009 MacBook Pro. Similar times are observed for other model graphs of this
size such as random regular graphs and real world networks. An implementation of this algorithm may be found
at https://github.com/danspielman/YINSlex.

5 Directed Graphs

Our deﬁnitions and algorithms, including those for regularization, extend to directed graphs with only small modiﬁ-
cations. We view directed edges as diodes and only consider potential differences in the direction of the edge. For
a complete voltage assignment v on the vertices of a directed graph G, we deﬁne the directed gradient on (x, y) due
to v to be grad+
. Given a partially-labelled directed graph (G, v0), we say that a a
complete voltage assignment v is a lex-minimizer if it extends v0 and for other complete voltage assignment v′ that
extends v0 we have grad+
G[v′]. We say that a partially-labelled directed graph (G, v0) is a well-posed
directed instance if every free vertex appears in a directed path between two terminals.

G[v](x, y) = max

G[v] (cid:22) grad+

v(x)−v(y)
ℓ(x,y)

, 0

n

o

The main difference between the directed and undirected cases is that the directed lex-minimizer is not necessarily
unique. To maintain clarity of exposition, we chose to focus on undirected graphs so far. For directed graphs, we have
the following corresponding structural results.

Theorem 5.1 Given a well-posed instance (G, v0) on a directed graph G, there exists a lex-minimizer, and the set of
all lex-minimizers is a convex set. Moreover, for every two lex-minimizers v and v′, we have grad+

G[v] = grad+

G[v′].

However, note that in the case of directed graphs, the lex-minimizer need not be unique. We still have a weaker version
of Theorem 3.3 for directed graphs.

Theorem 5.2 Given a well-posed instance (G, v0) on a directed graph G, let v1 be the partial voltage assignment
extending v0 obtained by repeatedly ﬁxing steepest ﬁxable (directed) paths P with ∇P > 0. Then, any lex-minimizer
of (G, v0) must extend v1. Moreover, for every edge e ∈ EG \ (T (V1) × T (V1)), any lex-minimizer v of (G, v0) must
satisfy grad+[v](e) = 0.

When the value of the lex-minimizer at a vertex is not uniquely determined, it is constrained to an interval. In our
experiments, we pick the convention that when the voltage at a vertex is constrained to an interval (−∞, a] or [a, ∞),
we assign a to the terminal. When it is constrained to a ﬁnite interval, we assign a voltage closest to the median of the
original voltages.

6 Experiments on WebSpam

We demonstrate the performance of our lex-minimization algorithms on directed graphs by using them to detect spam
webpages as in Zhou et al. (2007). We use the dataset webspam-uk2006-2.0 described in Castillo et al. (2006).
This collection includes 11,402 hosts, out of which 7,473 (65.5 %) are labeled, either as spam or normal. Each host
corresponds to the collection of web pages it serves. Of the hosts, 1924 are labeled spam (25.7 % of all labels). We
consider the problem of ﬂagging some hosts as spam, given only a small fraction of the labels for training. We assign
a value of 1 to the spam hosts, and a value of 0 to the normal ones. We then compute a lex minimizer and examine the
effect of ﬂagging as spam all hosts with a value greater than some threshold.

Following Zhou et al. (2007), we create edges between hosts with lengths equal to the reciprocal of the number of
links from one to the other. We run our experiments only on the largest strongly connected component of the graph,
which contains 7945 hosts of which 5552 are labeled. 16 % of the nodes in this subgraph are labeled spam. To create
training and test data, for a given value p, we select a random subset of p % of the spam labels and a random subset
of p % of the normal labels to use for training. The remaining labels are used for testing. We report results for p = 5
and p = 20.

Again following Zhou et al. (2007), we plot the precision and recall of different choices of threshold for ﬂagging
pages as spam. Recall is the fraction of spam pages our algorithm ﬂags as spam, and precision is the fraction of pages
our algorithm ﬂags as spam that actually are spam. Amongst the algorithms studied by Zhou et al. (2007), the top

9

performer was their algorithm based on sampling according to a random-walk that follows in-links from other hosts.
We compare their algorithm with the classiﬁcation we get by directing edges in the opposite directions of links. This
has the effect that a link to a spam host is evidence of spamminess, and a link from a normal host is evidence of
normality.

Results are shown in Figure 3. While we are not able to reliably ﬂag all spam hosts, we see that in the range of
10-50 % recall, we are able to ﬂag spam with precision above 82 %. We see that the performance of directed lex-
minimization does not degrade rapidly when from the “large training set” regime of p = 20, to the “small training set”
regime of p = 5.

5 % labels for training

20 % labels for training

RandWalk
DirectedLex

RandWalk
DirectedLex

1

0.9

0.8

0.7

i

i

n
o
s
c
e
r
P

0.6

0.5

0.4

0.3

0.2

0.1

0
0.1

0.2

0.3

0.4

0.7

0.8

0.9

1

0.2

0.3

0.4

0.7

0.8

0.9

1

0.6
0.5
Recall

0.6
0.5
Recall

Figure 3: Recall and precision in the web spam classiﬁcation experiment. Each data point shown was computed as an average over
100 runs. The largest standard deviation of the mean precision across the plotted recall values was less than 1.3 %. The algorithm
of Zhou et al. (2007) appears as RANDWALK. Our directed lex-minimization algorithm appears as DIRECTEDLEX.

For comparison, in Appendix C, we show the performance of our algorithm and that of Zhou et al. (2007) both
with link directions reversed, as well as the performance of undirected lex-minimization and Laplacian inference, all
of which are signiﬁcantly worse.

7 l0-Regularization of Vertex Values

We now explain how we can accommodate noise in both the given voltages and in the given lengths of edges. We can
ﬁnd the minimum number of labels to ignore, or the minimum increase in edges lengths needed so that there exists an
extension whose gradients have l∞-norm lower than a given target. After determining which labels to ignore or the
needed increment in edge lengths, we recommend computing a lex minimizer.

The algorithms we present in this section are essentially the same for directed and undirected graphs.

7.1 l0-Vertex Regularization for Inf-minimization

The l0-regularization of vertex labels can be viewed as a problem of outlier removal: the vector we compute is allowed
to disagree with v0 on up to k terminals. Given a voltage assignment v and a subset T ⊂ V of the vertices, by v(T )
we mean the vector obtained by restricting v to T . We deﬁne the l0-Vertex Regularization for l∞ problem to be

where v(T ) is the vector of values of v on the terminals T .

min
v∈IRn

gradG[v]

∞

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

0 ≤ k,
(cid:13)
(cid:13)

subject to

v(T ) − v0(T )

(3)

In Appendix D, we describe an approximation algorithm APPROX-OUTLIER that approximately solves program (3).

The precise statement we prove in Appendix D is given in the following theorem.

1

0.9

0.8

0.7

i

i

n
o
s
c
e
r
P

0.6

0.5

0.4

0.3

0.2

0.1

0
0.1

10

Theorem 7.1 (Approximate l0-vertex regularization) The algorithm APPROX-OUTLIER takes a positive integer k
and a partially-labeled graph (G, v0), and outputs an assignment v with
0 ≤ 2k, and
∞ ≤
α∗, where α∗ is the optimum value of program (3). The algorithm runs in time O(k(m + n log n)).
(cid:13)
(cid:13)
(cid:13)
(cid:13)

In Appendix D, we also describe an algorithm OUTLIER that exactly solves program (3) in polynomial time, and we
prove its correctness.

v(T ) − v0(T )

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Theorem 7.2 (Exact l0-vertex regularization) The algorithm OUTLIER takes a positive integer k and a partially-
labeled graph (G, v0) solves program (3) exactly. The algorithm runs in polynomial time.

We give a proof of Theorem 7.2 in Appendix D. To do this, we reduce the program (3) to the problem of minimizing
the required l0-budget needed to achieve a ﬁxed gradient α using a binary search over a set of O(n2) gradients. This
latter problem we reduce in polynomial time to Minimum Vertex Cover (VC) on a transitively closed, directed acyclic
graph (a TC-DAG). VC on a TC-DAG can be solved exactly in polynomial time by a reduction to the Maximum
Bipartite Matching Problem (Fulkerson (1956)). The problem was phrased by Fulkerson as one of ﬁnding a maximum
antichain of a ﬁnite poset. Any transitively closed DAG corresponds directly to the comparability graph of a poset. A
maximum antichain of a poset is a maximum independent set of a the comparability graph of the poset, and hence its
complement is a minimum vertex cover of the comparability graph. We refer to the algorithm developed by Fulkerson
as KONIG-COVER.

Theorem 7.3 The algorithm KONIG-COVER computes a minimum vertex cover for any transitively closed DAG G in
polynomial time.

7.2 Hardness of l0 regularization for l2

The result that l0-regularized inf-minimization can be solved exactly in polynomial time is surprising, especially
because the analogous problem for 2-Laplacian minimization turns out to be NP-Hard.

We deﬁne the the l0 vertex regularization for l2 for a partially-labeled graph (G, v0) and an integer k by

min
v∈Rn:kv(T )−v0(T )k0

≤k

vT Lv,

where L is the Laplacian of G.

Theorem 7.4 l0 vertex regularization for l2 is NP-Hard.

In Appendix E we prove Theorem 7.4 by giving a polynomial time (Karp) reduction from the NP-Hard minimum
bisection problem to l0 vertex regularization for l2.

8 l1-Edge and Vertex Regularization of Inf-minimizers

Consider a partially-labeled graph (G, v0) and an α > 0. The set of voltage assignments given by

v : v extends v0 and

gradG[v]

∞ ≤ α

n

(cid:13)
(cid:13)

(cid:13)
(cid:13)

o

is convex. Going further, let us consider the edge lengths in a graph to be speciﬁed by a vector ℓ ∈ IRE. Now the set
of voltages v and and lengths ℓ which achieve kgradG(ℓ)[v]k∞ ≤ α is jointly convex in v and ℓ. To see this, observe
that

kgradG(ℓ)[v]k∞ ≤ α ⇔ ∀(u, v) ∈ E : −αℓ(u, v) ≤ v(u) − v(v) ≤ αℓ(u, v).
Furthermore, the condition “v extends v0” is a linear constraint on v, which we express as v(T ) = v0(T ). From
the above, it is clear that the gradient condition corresponds to a convex set, as it is an intersection of half-spaces.
These half-spaces are given by O(m) linear inequalities. We can leverage this to phrase many regularized variants of
inf-minimization as convex programs, and in some cases linear programs.

(4)

11

For example, we may consider a variant of inf-minimization combined with an l1-budget for changing lengths of
edges and values on terminals. Given a parameter γ > 0 which speciﬁes the relative cost of regularizing terminals to
regularizing edges, the problem is as follows

arg min
v∈IRn,s∈IRm,s≥0

ksk1 + γ

v(T ) − v0(T )

1

subject to

gradG(ℓ+s)[v]

≤ α.

(5)

(cid:13)
(cid:13)
From our observation (4), it follows that problem (5) may be expressed as a linear program with O(n) variables
and O(m) constraints. We can use ideas from Daitch and Spielman (2008) to solve the resulting linear program in
O(m1.5) by an interior point method with a special purpose linear equation solver. The reason is that the linear
time
equations the IPM must solve at each iteration may be reduced to linear equations in symmetric, diagonally dominant
matrices, and these may be solved in nearly-linear time (Cohen et al. (2014)).

(cid:13)
(cid:13)

e

(cid:13)
(cid:13)
(cid:13)

∞

(cid:13)
(cid:13)
(cid:13)

Conclusion. We propose the use of inf and lex minimizers for regression on graphs. We present simple algorithms
for computing them that are provably fast and correct, and can also be implemented efﬁciently. We also present a
framework and polynomial time algorithms for regularization in this setting. The initial experiments reported in the
paper indicate that these algorithms give pretty good results on real and synthetic datasets. The results seem to compare
quite favorably to other algorithms, particularly in the regime of tiny labeled sets. We are testing these algorithms on
several other graph learning questions, and plan to report on them in a forthcoming experimental paper. We believe
that inf and lex minimizers, and the associated ideas presented in the paper, should be useful primitives that can be
proﬁtably combined with other approaches to learning on graphs.

We thank anonymous reviewers for helpful comments. We thank Santosh Vempala and Bartosz Walczak for pointing
out that it was already known how to compute a minimum vertex cover of a transitively closed DAG in polynomial
time.

Acknowledgements

References

Morteza Alamgir
In Advances
Information Processing
http://books.nips.cc/papers/files/nips24/NIPS2011_0278.pdf.

and Ulrike V. Luxburg.

transition
24,

in
pages

in Neural

Systems

Phase

the

family
379–387.

of
2011.

p-resistances.
URL

Gunnar Aronsson. Extension of functions satisfying lipschitz conditions. Arkiv fr Matematik, 6(6):551–561, 1967.

ISSN 0004-2080. doi: 10.1007/BF02591928. URL http://dx.doi.org/10.1007/BF02591928.

Gunnar Aronsson, Michael G. Crandall, and Petri Juutinen. A tour of the theory of absolutely minimizing functions.
ISSN 0273-0979. doi: 10.1090/S0273-0979-04-01035-3.

Bull. Amer. Math. Soc. (N.S.), 41(4):439–505, 2004.
URL http://dx.doi.org/10.1090/S0273-0979-04-01035-3.

Guy Barles and J´erˆome Busca. Existence and comparison results for fully nonlinear degenerate elliptic equations

without zeroth-order term. Comm. Partial Differential Equations, 26:2323–2337, 2001.

Mikhail Belkin, Irina Matveeva, and Partha Niyogi.

Regularization and semi-supervised learning on large
In Learning Theory, volume 3120 of Lecture Notes in Computer Science, pages 624–638.
doi: 10.1007/978-3-540-27819-1 43. URL

graphs.
Springer Berlin Heidelberg, 2004.
http://dx.doi.org/10.1007/978-3-540-27819-1_43.

ISBN 978-3-540-22282-8.

Nick Bridle and Xiaojin Zhu. p-voltages: Laplacian regularization for semi-supervised learning on high-dimensional

data. In Eleventh Workshop on Mining and Learning with Graphs (MLG2013), 2013.

12

Carlos Castillo, Debora Donato, Luca Becchetti, Paolo Boldi, Stefano Leonardi, Massimo Santini, and Sebastiano
Vigna. A reference collection for web spam. SIGIR Forum, 40(2):11–24, December 2006. ISSN 0163-5840. doi:
10.1145/1189702.1189703. URL http://doi.acm.org/10.1145/1189702.1189703.

Olivier Chapelle, Bernhard Schlkopf, and Alexander Zien. Semi-Supervised Learning. The MIT Press, 1st edition,

2010. ISBN 0262514125, 9780262514125.

Michael B Cohen, Rasmus Kyng, Gary L Miller, Jakub W Pachocki, Richard Peng, Anup B Rao, and Shen Chen Xu.
Solving SDD linear systems in nearly m log1/2 n time. In Proceedings of the 46th Annual ACM Symposium on
Theory of Computing, pages 343–352. ACM, 2014.

M.G. Crandall, L.C. Evans, and R.F. Gariepy. Optimal lipschitz extensions and the inﬁnity laplacian. Calculus of Vari-
ations and Partial Differential Equations, 13(2):123–139, 2001. ISSN 0944-2669. doi: 10.1007/s005260000065.
URL http://dx.doi.org/10.1007/s005260000065.

Samuel I. Daitch and Daniel A. Spielman. Faster approximate lossy generalized ﬂow via interior point algo-
rithms.
In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing, STOC ’08, pages
451–460, New York, NY, USA, 2008. ACM. ISBN 978-1-60558-047-0. doi: 10.1145/1374376.1374441. URL
http://doi.acm.org/10.1145/1374376.1374441.

Alan Egger and Robert Huotari. Rate of convergence of the discrete polya algorithm. Journal of Approximation
ISSN 0021-9045. doi: http://dx.doi.org/10.1016/0021-9045(90)90070-7. URL

Theory, 60(1):24 – 30, 1990.
http://www.sciencedirect.com/science/article/pii/0021904590900707.

Charles Fefferman. Whitney’s extension problems and interpolation of data.

(N.S.), 46(2):207–220, 2009a.
http://dx.doi.org/10.1090/S0273-0979-08-01240-8.

ISSN 0273-0979.

doi:

10.1090/S0273-0979-08-01240-8.

Bull. Amer. Math. Soc.
URL

Charles Fefferman. Fitting a [image] -smooth function to data, iii. Annals of Mathematics, 170(1):pp. 427–441, 2009b.

ISSN 0003486X. URL http://www.jstor.org/stable/40345469.

Charles Fefferman and Bo’az Klartag. Fitting a cm -smooth function to data i. Annals of Mathematics, 169(1):pp.

315–346, 2009. ISSN 0003486X. URL http://www.jstor.org/stable/40345445.

D. R. Fulkerson. Note on dilworths decomposition theorem for partially ordered sets. Proc. Amer. Math. Soc, 1956.

P.W. Gaffney and M.J.D. Powell. Optimal interpolation. In Numerical Analysis, volume 506 of Lecture Notes in Math-
ematics, pages 90–99. Springer Berlin Heidelberg, 1976. ISBN 978-3-540-07610-0. doi: 10.1007/BFb0080117.
URL http://dx.doi.org/10.1007/BFb0080117.

L.-A. Gottlieb, A. Kontorovich, and R. Krauthgamer. Efﬁcient classiﬁcation for metric data. CoRR, abs/1306.2547,

2013. URL http://arxiv.org/abs/1306.2547.

L.-A. Gottlieb, A. Kontorovich, and R. Krauthgamer. Efﬁcient regression in metric spaces via approximate lipschitz
extension. In Similarity-Based Pattern Recognition, volume 7953 of Lecture Notes in Computer Science, pages
43–58. Springer Berlin Heidelberg, 2013b. ISBN 978-3-642-39139-2. doi: 10.1007/978-3-642-39140-8 3. URL
http://dx.doi.org/10.1007/978-3-642-39140-8_3.

Robert Jensen. Uniqueness of lipschitz extensions: Minimizing the sup norm of the gradient. Archive for Ra-
doi: 10.1007/BF00386368. URL

ISSN 0003-9527.

tional Mechanics and Analysis, 123(1):51–74, 1993.
http://dx.doi.org/10.1007/BF00386368.

M. Kirszbraun. ber die zusammenziehende und lipschitzsche transformationen. Fundamenta Mathematicae, 22(1):

77–108, 1934. URL http://eudml.org/doc/212681.

13

Andrew J. Lazarus, Daniel E. Loeb,

James G. Propp, Walter R. Stromquist,

Combinatorial games under

man.
229 – 264,
http://www.sciencedirect.com/science/article/pii/S0899825698906765.

http://dx.doi.org/10.1006/game.1998.0676.

and Economic Behavior,

ISSN 0899-8256.

auction play.

Games

1999.

doi:

and Daniel H. Ull-
27(2):
URL

Yunqian Ma and Yun Fu. Manifold Learning Theory and Applications. CRC Press, Inc., Boca Raton, FL, USA, 1st

edition, 2011. ISBN 1439871094, 9781439871096.

E. J. McShane. Extension of range of functions. Bull. Amer. Math. Soc., 40(12):837–842, 12 1934. URL

http://projecteuclid.org/euclid.bams/1183497871.

C.A. Micchelli, T.J. Rivlin,

and S. Winograd.

merische Mathematik, 26(2):191–200, 1976.
http://dx.doi.org/10.1007/BF01395972.

The optimal
ISSN 0029-599X.

recovery of
doi:

smooth functions.
10.1007/BF01395972.

Nu-
URL

V. A. Milman.

Absolutely minimal extensions of

functions on metric spaces.

1999.

URL

http://iopscience.iop.org/1064-5616/190/6/A05/pdf/MSB_190_6_A05.pdf.

Boaz Nadler, Nathan Srebro, and Xueyuan Zhou. Statistical analysis of semi-supervised learning: The limit of inﬁnite
unlabelled data. 2009. URL http://ttic.uchicago.edu/˜nati/Publications/NSZnips09.pdf.

A. Naor and S. Shefﬁeld. Absolutely minimal Lipschitz extension of tree-valued mappings. CoRR, abs/1005.2535,

May 2010. URL http://arxiv.org/abs/1005.2535.

A. M. Oberman. Finite difference methods for the Inﬁnity Laplace and p-Laplace equations. CoRR, abs/1107.5278,

July 2011. URL http://arxiv.org/abs/1107.5278.

Yuval Peres, Oded Schramm, Scott Shefﬁeld, and DavidB. Wilson.

Tug-of-war and the inﬁnity lapla-
In Selected Works of Oded Schramm, Selected Works in Probability and Statistics, pages 595–
doi: 10.1007/978-1-4419-9675-6 18. URL

cian.
638. Springer New York, 2011.
http://dx.doi.org/10.1007/978-1-4419-9675-6_18.

ISBN 978-1-4419-9674-9.

S. Shefﬁeld and C. K. Smart. Vector-valued optimal Lipschitz extensions. CoRR, abs/1006.1741, June 2010. URL

http://arxiv.org/abs/1006.1741.

Ali Kemal Sinop and Leo Grady. A seeded image segmentation framework unifying graph cuts and random walker
which yields a new algorithm. In Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on,
pages 1–8. IEEE, 2007.

Vijay V. Vazirani. Approximation Algorithms. Springer-Verlag New York, Inc., New York, NY, USA, 2001. ISBN

3-540-65367-8.

Ulrike von Luxburg and Olivier Bousquet. Distance-based classiﬁcation with lipschitz functions.

In Learn-
ing Theory and Kernel Machines, volume 2777 of Lecture Notes in Computer Science, pages 314–328.
doi: 10.1007/978-3-540-45167-9 24. URL
Springer Berlin Heidelberg, 2003.
http://dx.doi.org/10.1007/978-3-540-45167-9_24.

ISBN 978-3-540-40720-1.

Hassler Whitney.

Analytic extensions of differentiable functions deﬁned in closed sets.

tions of
http://www.jstor.org/stable/1989708.

the American Mathematical Society, 36(1):pp. 63–89, 1934.

ISSN 00029947.

Transac-
URL

Dengyong Zhou, Christopher J. C. Burges, and Tao Tao. Transductive link spam detection.

In Proceedings
of the 3rd International Workshop on Adversarial Information Retrieval on the Web, AIRWeb ’07, pages 21–
ISBN 978-1-59593-732-2. doi: 10.1145/1244408.1244413. URL
28, New York, NY, USA, 2007. ACM.
http://doi.acm.org/10.1145/1244408.1244413.

Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic

functions. In IN ICML, pages 912–919, 2003.

14

A Basic Properties of Lex-Minimizers

A.1 Meta Algorithm

Algorithm 1: Algorithm META-LEX: Given a well-posed instance (G, v0), outputs lexG[v0].
for i = 1, 2, . . . :

1. if T (vi−1) = VG, then return vi−1.
2. E′ = EG \ (T (vi−1) × T (vi−1)), G′ := (VG, E′).
3. Let P ⋆
4. vi ← ﬁx[vi−1, P ⋆
i ].

i be a steepest ﬁxable path in (G′, vi−1). Let α⋆

i ← ∇P ⋆(vi−1).

In this subsection, we prove the results that appeared in section 2. We start with a simple observation.

Proposition A.1 Given a well-posed instance (G, v0) such that T (v0) 6= V, let P be a steepest ﬁxable path in (G, v0).
Then, ﬁx[v0, P ] extends v0, and (G, ﬁx[v0, P ]) is also a well-posed instance.

The properties we prove below do not depend on the choice of the steepest ﬁxable path.

Proposition A.2 For any well-posed instance (G, v0), with |VG| = n, META-LEX(G, v0) terminates in at most n
iterations, and outputs a complete voltage assignment v that extends v0.

Proof of Proposition A.2: By Proposition A.1, at any iteration i, vi−1 extends v0 and (G′, vi−1) is a well-posed
instance. META-LEX only outputs vi−1 iff T (vi−1) = V, which means vi−1 is a complete voltage assignment. For
any vi−1 that is not complete, for any x ∈ V \T (vi−1), we must have a free terminal path in (G′, vi−1) that contains x.
i exists in (G′, vi−1). Since P ⋆
Hence, a steepest ﬁxable path P ⋆
i ] ﬁxes the voltage
i
✷
for at least one non-terminal. Thus, META-LEX(G, v0) must complete in at most n iterations.

is a free terminal path, ﬁx[vi−1, P ⋆

For the following lemmas, consider a run of META-LEX with well-posed instance (G, v0) as input. Let vout be the
complete voltage assignment output by META-LEX. Let Ei be the set of edges E′ and Gi be the graph G′ constructed
in iteration i of META-LEX.

Lemma A.3 For every edge e ∈ Ei−1 \ Ei, we have

grad[vout](e)

≤ α⋆

i . Moreover, α⋆

i is non-increasing with i.

Proof of Lemma A.3: Let P ⋆
i = (x0, . . . , xr) be a steepest ﬁxable path in iteration i (when we deal with instance
(Gi−1, vi−1)). Consider a terminal path Pi+1 in (Gi, vi) such that {∂0Pi+1, ∂1Pi+1} ∩ (T (vi) \ T (vi−1)) 6= ∅. We
i . On the contrary, assume that ∇Pi+1(vi) > α⋆
claim that ∇Pi+1(vi) ≤ α⋆
i . Consider the case ∂0Pi+1 ∈ T (vi) \
T (vi−1), ∂1P1 ∈ T (vi−1). By the deﬁnition of vi, we must have ∂0Pi+1 = xj for some j ∈ [r − 1]. Let P ′
i+1 be the
path formed by joining paths P ⋆

i+1 is a free terminal path in (Gi−1, vi−1). We have,

i [x0 : xj] and Pi+1. P ′

(cid:12)
(cid:12)

(cid:12)
(cid:12)

vi−1(x0) − vi−1(∂1Pi+1) = (vi(x0) − vi(xj )) + (vi(∂0Pi+1) − vi(∂1Pi+1))
i · ℓ(P ′

i · ℓ(Pi+1) = α⋆

i [x0 : xj]) + α⋆

i · ℓ(P ⋆

> α⋆

i+1),

giving ∇P ′
The other cases can be handled similarly.

i+1(vi) > α⋆

i , which is a contradiction since the steepest ﬁxable path P ⋆
i

in (Gi−1, vi−1) has gradient α⋆
i .

Applying the above claim to an edge e ∈ Ei−1 \ Ei, whose gradient is ﬁxed for the ﬁrst time in iteration i, we
i . If v is the complete voltage assignment output by META-LEX, since v extends vi+1,
i , implying

i . Applying the claim to the symmetric edge, we obtain −grad[vout](e) ≤ α⋆

obtain that grad[vi+1](e) ≤ α⋆
we get grad[vout](e) ≤ α⋆
|grad[vout](e)| ≤ α⋆
i .

Consider any free terminal path Pi+1 in (Gi, vi). If Pi+1 is also a terminal path in (Gi−1, vi−1), it is a free
terminal path in (Gi−1, vi−1). In addition, since a steepest ﬁxable path P ⋆
i , we get
i
∇Pi+1(vi) = ∇Pi+1(vi−1) ≤ α⋆
i . Otherwise, we must have {∂0Pi+1, ∂1Pi+1} ∩ (T (vi) \ T (vi−1)) 6= ∅, and we can
deduce ∇Pi+1(vi) ≤ α⋆
i using the above claim. Thus, all free terminal paths Pi+1 in (Gi, vi) satisfy ∇Pi+1(vi) ≤ α⋆
i .
✷
In particular, α⋆

in (Gi−1, vi−1) has ∇P ⋆

i = α⋆

i is non-increasing with i.

i+1(vi) ≤ α⋆

i+1 = ∇P ⋆

i . Thus, α⋆

15

Lemma A.4 For any complete voltage assignment v for G that extends v0, if v 6= vout, we have grad[v] 6(cid:22) grad[vout],
and hence grad[vout] (cid:22) grad[v].

Proof of Lemma A.4: Consider any complete voltage assignment v for G that extends v0, such that v 6= vout. Thus,
there exists a unique i such that v extends vi−1 but does not extend vi. We will argue that grad[v] 6(cid:22) grad[vout], and
hence grad[vout] (cid:22) grad[v]. For every edge e ∈ E \ Ei−1 that has been ﬁxed so far, grad[v](e) = grad[vi−1](e) =
grad[vout](e), and hence we can ignore these edges.

Since v extends vi−1 but not vi, there exists an x ∈ T (vi) \ T (vi−1) such that v(x) 6= vi(x) = vout(x). Assume
i picked

i = (x0, . . . , xr) is the steepest ﬁxable path with gradient α⋆

v(x) < vi(x) (the other case is symmetric). If P ⋆
in iteration i, we must have x = xj for some j ∈ [r − 1]. Thus,

j

j

(v(xk−1) − v(xk)) = v(x0) − v(xj ) > vi(x0) − vi(xj ) = α⋆

i · ℓ(P ⋆

i [x0 : xj ]) = α⋆
i ·

ℓ(xk−1, xk).

Xk=1

Xk=1
Thus, for some k ∈ [j], we must have grad[v](xk−1, xk) > α⋆
is a path in Gi−1, we have {xk−1, xk} 6⊆
T (vi−1). This gives (xk−1, xk) ∈ (Ei−1 \ Ei). But then, from Lemma A.3, it follows that for all e ∈ (Ei−1 \ Ei), we
✷
have |grad[vout](e)| ≤ α⋆

i . Thus, we have grad[v] 6(cid:22) grad[vout].

i . Since P ∗
i

Lemma A.5 Let P = (x0, . . . , xr) be a steepest ﬁxable path such that it does not have any edges in T (v0) × T (v0)
and v1 = ﬁxG[v0, P ]. Then for every i ∈ [r], we have grad[v1](xi−1, xi) = ∇P.

Proof of Lemma A.5: Suppose this is not true and let j ∈ [r] be the minimum number such that grad[v1](xj−1, xj) 6=
∇P. By deﬁnition of v1 we would necessarily have j < r and vj ∈ T (v0). Suppose grad[v1](xj−1, xj ) < ∇P. We
would then have v1(x0) − v1(xj ) < ∇P ∗ ℓ(P [x0 : xj]). Since P does not have any edges in T (v0) × T (v0),
P1 := (xj, ..., xr) would be a free terminal path with ∇P1 > ∇P. This is a contradiction. Other cases can be ruled
out similarly.

✷

Proof of Theorem 3.3: Consider an arbitrary run of META-LEX on (G, v0). Let vout be the complete voltage
assignment output by META-LEX. Proposition A.1 implies that vout extends v0. Lemma A.4 implies that for any
complete voltage assignment v 6= vout that extends v0, we have grad[vout] (cid:22) grad[v]. Thus, vout is a lex-minimizer.
Moreover, the lemma also gives that for any such v, grad[v] 6(cid:22) grad[vout]. and hence vout is a unique lex-minimizer.
Thus, vout is the unique voltage assignment satisfying Def. 2.1, and we denote it as lexG[v0]. Since we started with an
✷
arbitrary run of META-LEX, uniqueness implies that every run of META-LEX on (G, v0) must output lexG[v0].

Proof of Lemma 3.5: Suppose we have a complete voltage assignment v extending v0, such that
For any terminal path P = (x0, . . . , xr), we get,

grad[v]

∞ ≤ α.

∇P (v0) = v0(∂0P ) − v0(∂1P ) = v(∂0P ) − v(∂1P ) =

grad[v](xi−1, xi) ≤ α ·

ℓ(xi−1, xi) = α · ℓ(P ),

(cid:13)
(cid:13)

(cid:13)
(cid:13)

r

i=1
X

giving ∇P (v0) ≤ α.

On the other hand, suppose every terminal path P in (G, v0) satisﬁes ∇P (v0) ≤ α. Consider v = lexG[v0]. We
know that v extends v0. For every edge e ∈ EG ∩ T (v0) × T (v0), e is a (trivial) terminal path in (G, v0), and hence
has satisﬁes grad[v](e) = grad[v0](e) = ∇e(v0) ≤ α. Considering the reverse edge, we also obtain −grad[v](e) ≤ α.
Thus, |grad[v](e)| ≤ α. Moreover, using Lemma A.3, we know that for edge e ∈ EG \ T (v0) × T (v0), |grad[v](e)| ≤
1 = ∇P ⋆
α⋆
1 ≤ α since P1 is a terminal path in (G, v0). Thus, for every e ∈ EG, |grad[v](e)| ≤ α, and hence
✷
grad[v]
∞ ≤ α.
(cid:13)
(cid:13)
(cid:13)
(cid:13)
A.2 Stability

In this subsection, we sketch a proof of the monotonicity of lex-minimizers and show how it implies the stability
property claimed earlier.

For any well-posed (G, v0), there could be several possible executions of META-LEX, each characterized by the

sequence of paths P ⋆

i . We can apply Theorem 3.3 to deduce the following structural result about the lex-minimizer.

r

i=1
X

16

Corollary A.6 For any well-posed instance (G, v0), consider a sequence of paths (P1, . . . , Pr) and voltage assign-
ments (v1, . . . , vr) for some positive integer r such that:

1. P ⋆

i is a steepest ﬁxable path in (Gi−1, vi−1) for i = 1, . . . , r.

2. vi = ﬁx[vi−1, P ⋆

i ] for i = 1, . . . , r.

3. T (vr) = VG.

Then, we have vr = lexG[v0].

We call such a sequence of paths and voltages to be a decomposition of lexG[v0]. Again, note that lexG[v0] can
possibly have multiple decompositions. However, any two such decompositions are consistent in the sense that they
produce the same voltage assignment.

Proof of Corollary 3.7: We ﬁrst deﬁne some operations on partial assignments which simpliﬁes the notation. Let
v0, v1 be any two partial assignments with the same set of terminals T := T (v0) = T (v1) and c, d ∈ R. By cv0 + d
we mean a partial assignment v with T (v) = T satisfying v(t) = cv0(t) + d for all t ∈ T . Also, by v0 + v1 we
mean a partial assignment v with T (v) = T satisfying v(t) = v0(t) + v1(t) for all t ∈ T. Also, we say v1 ≥ v0 if
v1(t) ≥ v0(t) for all t ∈ T .

Now we can show how Corollary 3.7 follows from Theorem 3.6. Let v := v1 − v0, and kvk∞ = ǫ, for some ǫ > 0.
Therefore, v0 + ǫ ≥ v1 ≥ v0 − ǫ. Theorem 3.6 then implies that lexG[v0] + ǫ ≥ lex[v1] ≥ lex[v0] − ǫ, hence proving
✷
the corollary.

Proof sketch of Theorem 3.6:
It is easy to see that the ﬁrst statement holds. For the second statement, we ﬁrst
observe that if there is a sequence of paths P1, ..., Pr that is simultaneously a decomposition of both lex[v0] and
lex[v1], then this is easy to see. If such a path sequence doesn’t exist, then we look at vt := v0 + t(v1 − v0). We
state here without a proof (though the proof is elementary) that we can then split the interval [0, 1] into ﬁnitely many
subintervals [a0, a1], [a1, a2], .., [ak−1, ak], with a0 = 0, ak = 1, such that for any i, there is a path sequence P1, ..., Pr
which is a decomposition of lex[vt] for all t ∈ [ai, ai+1]. We then observe that v0 = va0 ≤ va1 ≤ ...vak = v1. Since
for every ai, ai+1, there is a path sequence which is simultaneously a decomposition of both lex[vai ] and lex[vai+1 ],
we immediately get

lex[v0] = lex[va0 ] ≤ lex[va1] ≤ ... ≤ lex[vak ] = lex[v1].

✷

A.3 Alternate Characterizations

Proof of Theorem 3.10: We know that lexG[v0] extends v0. We ﬁrst prove that v = lexG[v0] satisﬁes the max-min
gradient averaging property. Assume to the contrary. Thus, there exists x ∈ VG \ T (v0) such that

max
y:(x,y)∈EG

grad[v](x, y) 6= − min

grad[v](x, y).

y:(x,y)∈EG

Assume that max(x,y)∈EG grad[v](x, y) ≥ − min(x,y)∈EG grad[v](x, y). Then, consider v′ extending v0 that is iden-
tical to v except for v′(x) = v(x) − ǫ for ǫ > 0. For ǫ small enough, we get that

and

max
y:(x,y)∈EG

grad[v′](x, y) < max

grad[v](x, y)

y:(x,y)∈EG

− min

y:(x,y)∈EG

grad[v′](x, y) < max

grad[v](x, y).

y:(x,y)∈EG

The gradient of edges not incident on the vertex x is left unchanged. This implies that grad[v]

6(cid:22) grad[v′],

contradicting the assumption that v is the lex-minimizer. (The other case is similar).

17

For the other direction. Consider a complete voltage assignment v extending v0 that satisﬁes the max-min gradient

averaging property w.r.t. (G, v0). Let

α = max

grad[v](x, y) ≥ 0

(x,y)∈EG
x∈V \T (v0)

be the maximum edge gradient, and consider any edge (x0, x1) ∈ EG such that grad[v](x1, x0) = α, with x1 ∈
V \ T (v0). If α = 0, grad[v] is identically zero, and is trivially the lex-minimal gradient assignment. Thus, both v and
lexG[v0] are constant on each connected component. Since (G, v0) is well-posed, there is at least one terminal in each
component, and hence v and lexG[v0] must be identical.

Now assume α > 0. By the max-min gradient averaging property, ∃x2 ∈ VG such that (x1, x2) ∈ EG and

grad[v](x1, x2) =

min
y:(x1,y)∈EG

grad[v](x1, y) = − max

grad[v](x1, y)

y:(x1,y)∈EG

≤ −grad[v](x1, x0) = −α.

Thus, grad[v](x2, x1) ≥ α. Since α is the maximum edge gradient, we must have grad[v](x2, x1) = α. More-
over, v(x2) > v(x1) > v(x0), thus x2 6= x0. We can inductively apply this argument at x2 until we hit a ter-
minal. Similarly, if x0 /∈ T (v0) we can extend the path in the other direction. Consequently, we obtain a path
P = (xj , . . . , x2, x1, x0, x−1, . . . , xk) with all vertices as distinct, such that xj , xk ∈ T (v0), and xi ∈ V \ T (v0)
for all i ∈ [j + 1, k − 1]. Moreover, grad[v](xi, xi−1) = α for all j < i ≤ k. Thus, P is a free terminal path with
∇P [v0] = α.

Moreover, since v is a voltage assignment extending v0 with

∞ = α, using Lemma 3.5, we know that
every terminal path P ′ in (G, v0) must satisfy ∇P ′(v0) ≤ α. Thus, P is a steepest ﬁxable path in (G, v0). Thus,
letting v1 = ﬁx[v0, P ], using Corollary 3.4, we obtain that lexG[v1] = lexG[v0]. Moreover, since α = ∇P [v0] =
grad[v](xi, xi−1) for all i ∈ (j, k], we get v1(xi) = v(xi) for all i ∈ (j, k). Thus, v extends v1.

grad[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We can iterate this argument for r iterations until T (vr) = VG, giving v = vr and vr = lexG[vr] = lexG[v0].
(Since we are ﬁxing at least one terminal at each iteration, this procedure terminates). Thus, we get v = lexG[v0]. ✷

B Description of the Algorithms

Algorithm 2: MODDIJKSTRA(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs a complete
voltage assignment v for G, and an array parent : V → V ∪ {null}.

Add x to a ﬁbonacci heap, with key(x) = +∞.
ﬁnished(x) ← false

Decrease key(x) to v0(x).
parent(x) ← null.

1. for x ∈ VG,
2.
3.
4. for x ∈ T (v0)
5.
6.
7. while heap is not empty
8.
9.
10.
11.
12.
13.
14.
15. return (v, parent)

x ← pop element with minimum key from heap
v(x) ← key(x). ﬁnished(x) ← true .
for y : (x, y) ∈ EG

if ﬁnished(y) = false

if key(y) > v(x) + α · ℓ(x, y)

Decrease key(y) to v(x) + α · ℓ(x, y).
parent(y) ← x.

Theorem B.1 For a well-posed instance (G, V0) and a gradient value α ≥ 0, let (v, parent) ← MODDIJKSTRA(G, v0, α).
Then, v is a complete voltage assignment such that, ∀x ∈ VG, v(x) = mint∈T (v0){v0(t) + αdist(x, t)}. Moreover, the
pointer array parent satisﬁes ∀x /∈ T (v0), parent(x) 6= null and v(x) = v(parent(x)) + α · ℓ(x, parent(x)).

18

Algorithm 3: Algorithm COMPVLOW(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs
vLow, a complete voltage assignment for G, and an array LParent : V → V ∪ {null}.

1. (vLow, LParent) ← MODDIJKSTRA(G, v0, α)
2. return (vLow, LParent)

Algorithm 4: Algorithm COMPVHIGH(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs
vHigh, a complete voltage assignment for G, and an array HParent : V → V ∪ {null}.

if x ∈ T (v0) then v1(x) ← −v0(x) else v1(x) ← v1(x).

1. for x ∈ VG
2.
3. (temp, HParent) ← MODDIJKSTRA(G, v1, α)
4. for x ∈ VG : vHigh(x) ← −temp(x)
5. return (vHigh, HParent)

Corollary B.2 For a well-posed instance (G, V0) and a gradient value α ≥ 0, let (vLow[α], LParent) ← COMPVLOW(G, v0, α)
and (vHigh[α], HParent) ← COMPVHIGH(G, v0, α). Then, vLow[α], vHigh[α] are complete voltage assignments for
G such that, ∀x ∈ VG,

vLow[α](x) = min

{v0(t) + α · dist(x, t)}

t∈T (v0)

vHigh[α](x) = max
t∈T (v0)

{v0(t) − α · dist(t, x)}.

Moreover, the pointer arrays LParent, HParent satisfy ∀x /∈ T (v0), LParent(x), HParent(x) 6= null and

vLow[α](x) = vLow[α](LParent(x)) + α · ℓ(x, LParent(x)),
vHigh[α](x) = vHigh[α](HParent(x)) − α · ℓ(x, HParent(x)).

Algorithm 5: Algorithm COMPINFMIN(G, v0): Given a well-posed instance (G, v0), outputs a complete voltage assignment
v for G, extending v0 that minimizes (cid:13)

(cid:13)grad[v](cid:13)

(cid:13)∞.

1. α ← max{|grad[v0](e)| | e ∈ EG ∩ (T (v0) × T (v0))}.
2. EG ← EG \ (T (v0) × T (v0))
3. P ←STEEPESTPATH(G, v0).
4. α ← max{α, ∇P (v0)}
5. (vLow, LParent) ← COMPVLOW(G, v0, α)
6. (vHigh, HParent) ← COMPVHIGH(G, v0, α)
7. for x ∈ VG
8.
9.
10.
11. return v

then v(x) ← v0(x)
else v(x) ← 1

2 · (vLow(x) + vHigh(x)).

if x ∈ T (v0)

1. (vLow, LParent) ← COMPVLOW(G, v0, α)
2. (vHigh, HParent) ← COMPVHIGH(G, v0, α)
3. VG′ ← {x ∈ VG | vHigh(x) > vLow(x) }
4. EG′ ← {(x, y) ∈ EG | x, y ∈ VG′ }.

19

Algorithm 6: Algorithm COMPHIGHPRESSGRAPH(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0,
outputs a minimal induced subgraph G′ of G where every vertex has pressure[v0](·) > α.

5. G′ ← (V ′, E′, ℓ)
6. return G′

Proof of Lemma 4.3:

is equivalent to

vHigh[α](x) > vLow[α](x)

max
t∈T (v0)

{v0(t) − α · dist(t, x)} > min

{v0(t) + α · dist(x, t)},

t∈T (v0)

which implies that there exists terminals s, t ∈ T (v0) such that

thus,

Hence,

v0(t) − α · dist(t, x) > v0(s) + α · dist(x, s)

pressure[v0](x) ≥

v0(t) − v0(s)
dist(t, x) + dist(x, s)

> α.

v0(t) − v0(s)
dist(t, x) + dist(x, s)

= pressure[v0](x) > α.

v0(t) − α · dist(t, x) > v0(s) + α · dist(x, s)

So the inequality on vHigh and vLow implies that pressure is strictly greater than α. On the other hand, if pressure[v0](x) >
α, there exists terminals s, t ∈ T (v0) such that

which implies vHigh[α](x) > vLow[α](x).

✷

Algorithm 7: Algorithm STEEPESTPATH(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs a steepest
free terminal path P in (G, v0).

P ← VERTEXSTEEPESTPATH(G, v0, xi)

1. Sample uniformly random e ∈ EG. Let e = (x1, x2).
2. Sample uniformly random x3 ∈ VG.
3. for i = 1 to 3
4.
5. Let j ∈ arg maxj∈{1,2,3} ∇Pj (v0)
6. G′ ← COMPHIGHPRESSGRAPH(G, v0, ∇Pj (v0))
7. if EG′ = ∅,
8.
9.

then return Pj
else return STEEPESTPATH(G′, v0|VG′ )

1. while T (v0) 6= VG
2.
3.
4.
5. return v0

EG ← EG \ (T (v0) × T (v0))
P ← STEEPESTPATH(G, v0)
v0 ← ﬁx[v0, P ]

Algorithm 8: Algorithm COMPLEXMIN(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs lexG[v0].

Algorithm 9: Algorithm VERTEXSTEEPESTPATH(G,v0, x): Given a well-posed instance (G, v0), and a vertex x ∈ VG,
outputs a steepest terminal path in (G, v0) through x.

1. Using Dijkstra’s algorithm, compute dist(x, t) for all t ∈ T (v0)

20

y ← arg maxy∈T (v0)
if v0(x) ≥ v0(y)

|v0(x)−v0(y)|
dist(x,y)

then return a shortest path from x to y
else return a shortest path from y to x

2. if x ∈ T (v0)
3.
4.
5.
6.
7. else
8.
9.
10.
11.

for t /∈ T (v0), d(t) ← dist(x, t)
(t1, t2) ← STARSTEEPESTPATH(T (v0), v0|T (v0), d)
Let P1 be a shortest path from t1 to x. Let P2 be a shortest path from x to t2.
P ← (P1, P2). return P.

Algorithm 10: STARSTEEPESTPATH(T, v, d): Returns the steepest path in a star graph, with a single non-terminal connected
to terminals in T, with lengths given by d, and voltages given by v.

|v(t1)−v(t)|
d(t1)+d(t)

1. Sample t1 uniformly and randomly from T
2. Compute t2 ∈ arg maxt∈T
3. α ← |v(t2)−v(t1)|
d(t1)+d(t2)
4. Compute vlow ← mint∈T (v(t) + α · d(t))
5. Tlow ← {t ∈ T | v(t) > vlow + α · d(t)}
6. Compute vhigh ← maxt∈T (v(t) − α · d(t))
7. Thigh ← {t ∈ T | v(t) < vhigh − α · d(t)}
8. T ′ ← Tlow ∪ Thigh.
9. if T ′ = ∅
10.
11.

then if v(t1) ≥ v(t2) then return (t1, t2) else return (t2, t1)
else return STARSTEEPESTPATH(T ′, v|T ′, dT ′ )

B.1 Faster Lex-minimization

Algorithm 11: Algorithm COMPFASTLEXMIN(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs
lexG[v0].

1. while T (v0) 6= VG
2.
3. return v0

v0 ← FIXPATHSABOVEPRESS(G, v0, 0)

Algorithm 12: Algorithm FIXPATHSABOVEPRESS(G, v0, α): Given a well-posed instance (G, v0), with T (v0) 6= VG, and
a gradient value α, iteratively ﬁxes all paths with gradient > α.

EG ← EG \ (T (v0) × T (v0))
Sample uniformly random e ∈ EG. Let e = (x1, x2).
Sample uniformly random x3 ∈ VG.
for i = 1 to 3

Pi ← VERTEXSTEEPESTPATH(G, v0, xi)

Let j ∈ arg maxj∈{1,2,3} ∇Pj(v0)
G′ ← COMPHIGHPRESSGRAPH(G, v0, ∇Pj (v0))
if EG′ = ∅,

1. while T (v0) 6= VG
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.

then v0 ← ﬁx[v0, P ]
else Let G′

for i = 1, . . . , r

i, i = 1, . . . , r be the connected components of G′.

21

vi ← FIXPATHSABOVEPRESS(G′
for x ∈ VG′

i, set v0(x) ← vi(x)

i, v0|VG′

i

, ∇Pj (v0))

if α > 0 then G ←COMPHIGHPRESSGRAPH(G, v0, α)

13.
14.
15.
16. return v0

C Experiments on WebSpam: Testing More Algorithms

For completeness, in this appendix we show how a number of algorithms perform on the web spam experiment of
Section 6. We consider the following algorithms:

• RANDWALK along in-links. For a detailed description see Zhou et al. (2007). This algorithm essentially per-
forms a Personalized PageRank random walk from each vertex x and computes a spam-value for the vertex x by
taking a weighted average of the labels of the vertices where the random walk from x terminates. Also shown in
Section 6.

• DIRECTEDLEX, with edges in the opposite directions of links. This has the effect that a link to a spam host is

evidence of spam, and a link from a normal host is evidence of normality. Also shown in Section 6.

• RANDWALK along out-links.

• DIRECTEDLEX, with edges in the directions of links. This has the effect that a link from to a spam host is

evidence of spam, and a link to a normal host is evidence of normality.

• UNDIRECTEDLEX: Lex-minimization with links treated as undirected edges.

• LAPLACIAN: l2-regression with links treated as undirected edges.

• DIRECTED 1-NEAREST NEIGHBOR: Uses shortest distance along paths following out-links. Spam-ratio is
deﬁned distance from normal hosts, divided by distance to spam hosts. Sites are ﬂagged as spam when spam-
ratio exceeds some threshold. We also tried following paths along in-links instead, but that gave much worse
results.

We use the experimental setup described in Section 6. Results are shown in Figure 4. The alternative convention
for DIRECTEDLEX orients edges in the directions of links. This takes a link from a spam host to be evidence of
spam, and a link to a normal host to be evidence of normality. This approach performs signiﬁcantly worse than our
preferred convention, as one would intuitively expect. UNDIRECTEDLEX and LAPLACIAN approaches also perform
signiﬁcantly worse. DIRECTED 1-NEAREST NEIGHBOR performs poorly, demonstrating that DIRECTEDLEX is very
different from that approach. As observed by Zhou et al. (2007), sampling based on a random walk following out-links
performs worse than following in-links. Up to 60 % recall, DIRECTEDLEX performs best, both in the regime of 5 %
labels for training and in the regime of 20 % labels for training.

22

5 % labels for training

RandWalk along in-links
DirectedLex
RandWalk along out-links
DirectedLex, opposite link direction
UndirectedLex
Laplacian
Directed 1NN

0
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Recall

20 % labels for training

RandWalk along in-links
DirectedLex
RandWalk along out-links
DirectedLex, opposite link direction
UndirectedLex
Laplacian
Directed 1NN

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

i

i

n
o
s
c
e
r
P

i

i

n
o
s
c
e
r
P

0
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Recall

Figure 4: Recall and precision in the WebSpam classiﬁcation experiment. Each data point shown was computed as an average
over 100 runs. The largest standard deviation of the mean precision across the plotted recall values was less than 1.5 %. The
algorithm of Zhou et al. (2007) appears as RANDWALK (along in-links). We also show RANDWALK along out-links. Our directed
lex-minimization algorithm appears as DIRECTEDLEX. We also show DIRECTEDLEX with link directions reversed, along with
UNDIRECTEDLEX and LAPLACIAN.

D l0-Vertex Regularization Proofs

In this appendix, we prove Theorem 7.1 and Theorem 7.2. For the purposes of proving the second theorem, we intro-
duce an alternative version of problem (3). The optimization problem here requires us to minimize l0-regularization

23

budget required to obtain an inf-minimizer with gradient below a given threshold:

min
v∈IRn
subject to

(cid:13)
(cid:13)

v(T ) − v0(T )

0

gradG[v]

(cid:13)
∞ ≤ α.
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We will also need the following graph construction.

Deﬁnition D.1 The α-pressure terminal graph of a partially-labeled graph (G, v0) is a directed unweighted graph
Gα = (T (v0),

E if and only if there is a terminal path P from s to t in G with

E) such that (s, t) ∈

b

b

∇P (v0) > α.

Note that the α-pressure terminal graph has O(n) vertices but may be dense, even when G is not.

Algorithm 13: Algorithm TERM-PRESSURE: Given a well-posed instance (G, v0) and α ≥ 0, outputs α pressure terminal
graph Gα.
Initialize Gα with vertex set Vα = T (v0) and edge set
for each terminal s ∈ T (v0)

E = ∅.

1. Compute the distances to every other terminal t by running Dijktra’s algorithm, allowing shortest paths

b

2. Use the resulting distances to check for every other terminal t if there is a terminal path P from s to t with

that run through other terminals.

∇P (v0) > α. If there is, add edge (s, t) to

E.

Lemma D.2 The α-pressure terminal graph of a voltage problem (G, v0) can be computed in O((m + n log n)n) time
using algorithm TERM-PRESSURE (Algorithm 13).

b

Proof: The correctness of the algorithm follows from the fact that Dijkstra’s algorithm will identify all shortest
distances between the terminals, and the pressure check will ensure that terminal pairs (s, t) are added to
E if and
only if they are the endpoints of a terminal path P with ∇P (v0) > α. The running time is dominated by performing
Dijkstra’s algorithm once for each terminal. A single run of Dijkstra’s algorithm takes O(m + n log n) time, and this
✷
is performed at most n times, for a total running time of O((m + n log n)n).

b

We make three observations that will turn out to be crucial for proving Theorems 7.1 and 7.2.

Observation D.3 Gα is a subgraph of Gβ for α ≥ β.

Proof: Suppose edge (s, t) appears in Gα, then for some path P

∇P (v0) > α ≥ β,

so the edge also appears in Gβ.

Observation D.4 Gα is transitively closed.

Proof: Suppose edges (s, t) and (t, r) appear in Gα. Let P(s,t), P(t,r), P(s,r) be the respective shortest paths in G
between these terminal pairs. Then

∇P(s,r)(v0) =

v0(s) − v0(r)
ℓ(P(s,r))

≥

v0(s) − v0(r)
ℓ(P(s,t)) + ℓ(P(t,r))

=

v0(s) − v0(t) + v0(t) − v0(r)
ℓ(P(s,t)) + ℓ(P(t,r))

≥ min

v0(s) − v0(t)
ℓ(P(s,t))

,

 

v0(t) − v0(r)

ℓ(P(t,r)) !

> α.

So edge (s, r) also appears in Gα. This is sufﬁcient for Gα to be transitively closed.

24

(6)

✷

(7)

✷

Observation D.5 Gα is a directed acyclic graph.

Proof: Suppose for a contradiction that a directed cycle appears in Gα. Let s and t be two vertices in this cycle. Let
P(s,t) and P(t,s) be the respective shortest paths in G between these terminal pairs. Because Gα is transitively closed,
both edges (s, t) and (t, s) must appear in Gα. But (s, t) ∈

E implies

and similarly (t, s) ∈

E implies

b
This is a contradiction.

v0(s) − v0(t) > αℓ(P(s,t)) > 0,

b

v0(t) − v0(s) > αℓ(P(t,s)) > 0.

✷

The usefulness of the α-pressure terminal graph is captured in the following lemma. We deﬁne a vertex cover of a
directed graph to be a vertex set that constitutes a vertex cover in the same graph with all edges taken to be undirected.

Lemma D.6 Given a partially-labeled graph (G, v0) and a set U ⊆ V , there exists a voltage assignment v ∈ IRn that
satisﬁes

if and only if U is a vertex cover in the α-pressure terminal graph Gα of (G, v0).
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:8)

(cid:9)

t ∈ T (v0) : v(t) 6= v0(t)

⊆ U and

gradG[v]

∞ ≤ α,

Proof: We ﬁrst show the “only if” direction. Suppose for a contradiction that there exists a voltage assignment v for
which
∞ ≤ α, but U is not a vertex cover in Gα. Let (s, t) be an edge Gα which is not covered by U . The
presence of this edge in Gα implies that there exists a terminal path P from s to t in G for which

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∇P (v0) > α.

But, by Lemma 3.5 this means there is no assignment v for G which agrees with v0 on s and t and has
α. This contradicts our assumption.

∞ ≤
(cid:13)
Now we show the “if” direction. Consider an arbitrary vertex cover U of Gα. Suppose for a contradiction that
(cid:13)
⊆ U .

t ∈ T (v0) : v(t) 6= v0(t)

gradG[v]

(cid:13)
(cid:13)

gradG[v]

there does not exist a voltage assignment v for G with
Deﬁne a partial voltage assignment vU given by

∞ ≤ α and

(cid:8)

(cid:9)

vU (t) =

v0(t)
∗

(

(cid:13)
(cid:13)

(cid:13)
(cid:13)
if t ∈ T (v0) \ U
o.w.

∞ ≤ α. By
The preceding statement is equivalent to saying that there is no v that extends vU and has
Lemma 3.5, this means there is terminal path between s, t ∈ T (vU ) with gradient strictly larger than α. But this
means an edge (s, t) is present in Gα and is not covered. This contradicts our assumption that U is a vertex cover. ✷

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We are now ready to prove Theorem 7.2.

∞

(cid:13)
(cid:13)

Proof of Theorem 7.2: We describe and prove the algorithm OUTLIER. The algorithm will reduce problem (3)
to problem (6): Suppose v∗ is an optimal assignment for problem (3).
It achieves a maximum gradient α∗ =
gradG[v∗]
. Using Dijkstra’s algorithm we compute the pairwise shortest distances between all terminals in G.
From these distances and the terminal voltages, we compute the gradient on the shortest path between each terminal
(cid:13)
pair. By Lemma 3.5, α∗ must equal one of these gradients. So we can solve problem (3) by iterating over the set of
(cid:13)
gradients between terminals and solving problem (6) for each of these O(n2) gradients. Among the assignments with
v(T ) − v0(T )

0 ≤ k, we then pick the solution that minimizes
(cid:13)
(cid:13)

In fact, we can do better. By Observation D.3, Gα is a subgraph of Gβ for α ≥ β. This means a vertex cover
(cid:13)
of Gα is also a vertex cover of Gβ, and hence the minimum vertex cover for Gβ is at least as large as the minimum
(cid:13)
vertex cover for Gα. This means we can do a binary search on the set of O(n2) terminal gradients to ﬁnd the minimum
gradient for which there exists an assignment with
0 ≤ k. This way, we only make O(log n) calls to
v(T ) − v0(T )
problem (6), in order to solve problem (3).
(cid:13)
(cid:13)

We use the following algorithm to solve problem (6).

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∞

.

25

1. Compute the α-pressure terminal graph Gα of G using the algorithm TERM-PRESSURE.
2. Compute a minimum vertex cover U of Gα using the algorithm KONIG-COVER from Theorem 7.3.
3. Deﬁne a partial voltage assignment vU given by

vU (t) =

v0(t)
∗

(

if t ∈ T (v0) \ U,
otherwise.

4. Using Algorithm 5, compute voltages v that extend vU and output v.

From Lemma D.2, it follows that step 1 computes the α-pressure terminal graph in polynomial time. From The-
orem 7.3 it follows that step 2 computes the a minimum vertex cover of the α-pressure terminal graph in polynomial
time, because our observations D.4 and D.5 establish that the graph is a TC-DAG. From Lemma D.6 and Theorem 4.6,
it follows that the output voltages solve program (6).

✷

To prove Theorem 7.1, we use the standard greedy approximation algorithm for MIN-VC (Vazirani (2001)).

Theorem D.7 2-Approximation Algorithm for Vertex Cover. The following algorithm gives a 2-approximation to
the Minimum Vertex Cover problem on a graph G = (V, E).

0. Initialize U = ∅.
1. Pick an edge (u, v) ∈ E that is not covered by U .
2. Add u and v to the set U .
3. Repeat from step 1 if there are still edges not covered by U .
4. Output U .

We are now in a position to prove Theorem 7.1

Proof of Theorem 7.1: Given an arbitrary k and a partially-labeled graph (G, v0), let α∗ be the optimum value
of program (3). Observe that by Lemma D.6, this implies that Gα∗ has a vertex cover of size k. Given the partial
assignment v0, for every vertex set U , we deﬁne

vU (t) =

v0(t)
∗

(

if t ∈ T (v0) \ U
o.w.

We claim the following algorithm APPROX-OUTLIER outputs a voltage assignment v with

gradG[v]

∞ ≤ α∗

(cid:13)
(cid:13)

(cid:13)
(cid:13)

and

v(T ) − v0(T )

(cid:13)
(cid:13)

Algorithm APPROX-OUTLIER:

0 ≤ 2k.
(cid:13)
(cid:13)

0. Initialize U = ∅.
1. Using the algorithm STEEPESTPATH (Algorithm 7), ﬁnd a steepest terminal path in G w.r.t. vU . Denote
this path P and let s and t be its terminal endpoints. If there is no terminal path with positive gradient, skip
to step 4.

2. Add s and t to the set U .
3. If |U | ≤ 2k − 2 then repeat from step 1.
4. Using the algorithm COMPINFMIN (Algorithm 5), compute voltages v that extend vU and output v.

From the stopping conditions, it is clear that |U | ≤ 2k. If in step 1 we ever ﬁnd that no terminal paths have positive
∞ = 0 ≤ α∗, by Lemma 3.5. Similarly if we ﬁnd a steepest
gradient then our v that extends vU will have
(cid:13)
(cid:13)

gradG[v]

(cid:13)
(cid:13)

26

gradG[v]

∞ ≤ α∗.

∞ ≤ α∗.
path with gradient less than α∗ w.r.t. vU , then for this U there exists v that extends vU and has
This will continue to hold when if we add vertices to U . Therefore, for the ﬁnal U , there will exist an v that extends
vU and has

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

If we never ﬁnd a steepest terminal path P with ∇P (v0) ≤ α∗, then each steepest path we ﬁnd corresponds to an
edge in Gα∗ that is not yet covered by U and our algorithm in fact implements the greedy approximation algorithm
for vertex cover described in Theorem D.7. This implies that the ﬁnal U is a vertex cover of Gα∗ of size at most 2k.
∞ ≤ α∗. This
By Lemma D.6, this implies that there exists a voltage assignment u extending vU that has
implies by Theorem 4.6 that the v we output has
(cid:13)
(cid:13)
In all cases, the v we output extends vU , so

∞ ≤ α∗.

gradG[u]

(cid:13)
(cid:13)

✷

gradG[v]
v(T ) − v0(T )
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

0 ≤ |U | ≤ 2k.
(cid:13)
(cid:13)

E Proof of Hardness of l0 regularization for l2

We will prove Theorem 7.4, by a reduction from minimum bisection. To this end, let G = (V, E) be any graph. We
will reduce the minimum bisection problem on G to our regularization problem. Let n = |V |. The graph on which we
will perform regularization will have vertex set

V ∪

V ,

V is a set of n vertices that are in 1-to-1 correspondence with V . We assume that every edge in G has weight 1.
V to the corresponding vertex in V by an edge of weight B, for some large B to be
V to each other by edges of weight B3. So, we have a complete
V to V , and the original graph G on V .

where
We now connect every vertex in
determined later. We also connect all of the vertices in
graph of weight B3 edges on
b
The input potential function will be

V , a matching of weight B edges connecting

b

b

b

v(a) =

b
0 for a ∈
1 for a ∈ V .
b

(

V , and

b

Now set k = n/2. We claim that we will be able to determine the value of the minimum bisection from the solution
to the regularization problem.

If S is the set of vertices on which v and w differ, then we know that the w is harmonic on S: for every a ∈ S,

w(a) is the weighted average of the values at its neighbors. In the following, we exploit the fact that |S| ≤ n/2.

Claim E.1 For every a ∈ S ∩

V , w(a) ≤ 2/nB2.

Proof: Let a be the vertex in S ∩
w-value equal to 0 by edges of weight B3. On the other hand, a has only one neighbor that is not in
w-value at most 1, and it is connected to that vertex by an edge of weight B. Call that vertex c. We have

V that maximizes w(a). So, a is connected to at least n/2 neighbors in

V with
V , that vertex has

b

b

b

((n − 1)B3 + B)w(a) = Bw(c) +

B3w(b)

b

b
V ,b6=a
Xb∈

= Bw(c) +

B3w(b) +

B3w(b)

b
V ∩S,b6=a
Xb∈

B3w(a)

≤ B +

b
V ∩S,b6=a
Xb∈
≤ B + (n/2 − 1)B3w(a).

b
V −S
Xb∈

Subtracting (n/2 − 1)B3w(a) from both sides gives

((n/2)B3 + B)w(a) ≤ B,

which implies the claim.

Claim E.2 For a ∈ S ∩ V , w(a) ≤ n/B.

27

✷

V . Let’s call that neighbor c. We know that w(c) ≤ 2/B2n. On the
Proof: Vertex a has exactly one neighbor in
other hand, vertex a has fewer than n − 1 neighbors in V , and each of these have w-value at most 1. Let da denote the
degree of a in G. Then,

b

So,

Let

and

bisection.

and at most

(B + da)w(a) ≤ da + B

2
B2n

.

w(a) ≤

da + 2/Bn
da + B
n + (2/Bn)
B + n

≤

≤ n/B.

|S| = k = n/2.

T = S ∩ V,

t = |T | .

(n − t)B − 4/B
b

(n − t)B + tn2/B.

We now estimate the value of the regularized objective function. To this end, we assume that

We will prove that S ⊂ V and so S = T and t = n/2.

Let δ denote the number of edges on the boundary of T in V . Once we know that t = n/2, δ is the size of a

Claim E.3 The contribution of the edges between V and

V to the objective function is at least

Proof: For the lower bound, we just count the edges between vertices in V \ T and
edges, and each of them has weight B. The endpoint in V \ T has w-value 1, and the endpoint in
most 2/nB2. So, the contribution of these edges is at least

V . There are n − t of these
V has w-value at

b

(n − t)B(1 − 2/nB2)2 ≥ (n − t)B(1 − 4/nB2) ≥ (n − t)B − 4/B.

b

For the upper bound, we observe that the difference in w-values across each of these n − t edges is at most 1, so their
total contribution is at most

Since for every vertex a ∈ T , w(a) ≤ n/B, and also every vertex b ∈
edges between T and

V is at most

t(n/B)2B = tn2/B.

b

b

V , w(b) ≤ 2/nB2, the contribution due to

We will see that this is the dominant term in the objective function. The next-most important term comes from the

edges in G.

(n − t)B.

28

✷

✷

Claim E.4 The contribution of the edges in G to the objective function is at least

and at most

δ(1 − 2n/B)

δ + (t2/2)(n/B)2

δ(1 − 2n/B) and δ.

(t2/2)(n/B)2.

Proof: Let (a, b) ∈ E. If neither a nor b is in T , then w(a) = w(b) = 1, and so this edge has no contribution. If
a ∈ T but b 6∈ T , then the difference in w-values on them is between (1 − n/B) and 1. So, the contribution of such
edges to the objective function is between

Finally, if a and b are in T , then the difference in w-values on them is at most n/B, and so the contribution of all such
edges to the objective function is at most

Claim E.5 The edges between pairs of vertices in

V contribute at most 2/B to the objective function.

Proof: As 0 ≤ w(a) ≤ 2/B2n for every a ∈

V , every edge between two vertices in

V can contribute at most

b

As there are fewer than n2/2 such edges, their total contribution to the objective function is at most

B3(2/B2n)2 = 4/Bn2.
b

b

(n2/2)(4/Bn2) = 2/B.

Lemma E.6 If n ≥ 4 and B = 2n3, the value of the objective function is at least

and at most

(n − t)B + δ − 1/2

(n − t)B + δ + 1/3.

Proof: Summing the contributions in the preceding three claims, we see that the value of the objective function is at
least

(n − t)B − 4/B + δ(1 − 2n/B) ≥ (n − t)B + δ − 4/B − 2nδ/B

≥ (n − t)B + δ − n3/B
≥ (n − t)B + δ − 1/2,

as δ ≤ (n/2)2.

Similarly, the objective function is at most

(n − t)B + tn2/B + δ + (t2/2)(n/B)2 + 2/B ≤ (n − t)B + n3/2B + δ + n4/8B2 + 2/B
≤ (n − t)B + n3/2B + δ + 1/32n2 + 1/n3
≤ (n − t)B + δ + 1/3.

Claim E.7 If n ≥ 2 and B = 2n3, then S ⊂ V .

Proof: The objective function is minimized by making t as large as possible, so t = n/2 and S ⊂ V .

29

✷

✷

✷

✷

Theorem E.8 The value of the objective function reveals the value of the minimum bisection in G.

Proof: The value of the objective function will be between

and

(n/2)B + δ − 1/2

(n/2)B + δ + 1/3.

So, the objective function will be smallest when δ is as small as possible.

✷

Theorem E.8 immediately implies Theorem 7.4.

30

5
1
0
2
 
n
u
J
 
0
3
 
 
]

G
L
.
s
c
[
 
 
2
v
0
9
2
0
0
.
5
0
5
1
:
v
i
X
r
a

Algorithms for Lipschitz Learning on Graphs ∗†

Rasmus Kyng
Yale University
rasmus.kyng@yale.edu

Anup Rao
Yale University
anup.rao@yale.edu

Sushant Sachdeva
Yale University
sachdeva@cs.yale.edu

Daniel A. Spielman
Yale University
spielman@cs.yale.edu

July 1, 2015

Abstract

We develop fast algorithms for solving regression problems on graphs where one is given the value of a function
at some vertices, and must ﬁnd its smoothest possible extension to all vertices. The extension we compute is the
absolutely minimal Lipschitz extension, and is the limit for large p of p-Laplacian regularization. We present an
algorithm that computes a minimal Lipschitz extension in expected linear time, and an algorithm that computes
an absolutely minimal Lipschitz extension in expected time eO(mn). The latter algorithm has variants that seem
to run much faster in practice. These extensions are particularly amenable to regularization: we can perform l0-
regularization on the given values in polynomial time and l1-regularization on the initial function values and on graph
edge weights in time eO(m3/2).

Our deﬁnitions and algorithms naturally extend to directed graphs.

1 Introduction

We consider a problem in which we are given a weighted undirected graph G = (V, E, ℓ) and values v0 : T → R
on a subset T of its vertices. We view the weights ℓ as indicating the lengths of edges, with shorter length indicating
greater similarity. Our goal it to assign values to every vertex v ∈ V \T so that the values assigned are as smooth as
possible across edges. A minimal Lipschitz extension of v0 is a vector v that minimizes

max
(x,y)∈E

(ℓ(x, y))−1

v(x) − v(y)

,

(cid:12)
(cid:12)

(cid:12)
(cid:12)

subject to v(x) = v0(x) for all x ∈ T . We call such a vector an inf-minimizer. Inf-minimizers are not unique. So,
among inf-minimizers we seek vectors that minimize the second-largest absolute value of ℓ(x, y)−1
v(x) − v(y)
across edges, and then the third-largest given that, and so on. We call such a vector v a lex-minimizer. It is also known
(cid:12)
as an absolutely minimal Lipschitz extension of v0.
(cid:12)
These are the limit of the solution to p-Laplacian minimization problems for large p, namely the vectors that solve

(cid:12)
(cid:12)

(1)

(2)

min
v∈Rn

v|T =v0|T X(x,y)∈E

(ℓ(x, y))−p|v(x) − v(y)|p.

The use of p = 2 was suggested in the foundational paper of Zhu et al. (2003), and is particularly nice because it can
be obtained by solving a system of linear equations in a symmetric diagonally dominant matrix, which can be done

∗This research was partially supported by AFOSR Award FA9550-12-1-0175, NSF grant CCF-1111257, a Simons Investigator Award to Daniel

Spielman, and a MacArthur Fellowship.

†Code used in this work is available at https://github.com/danspielman/YINSlex

1

very quickly (Cohen et al. (2014)). The use of larger values of p has been discussed by Alamgir and Luxburg (2011),
and by Bridle and Zhu (2013), but it is much more complicated to compute. The fastest algorithms we know for this
problem require convex programming, and then require very high accuracy to obtain the values at most vertices. By
taking the limit as p goes to inﬁnity, we recover the lex-minimizer, which we will show can be computed quickly.

The lex-minimization problem has a remarkable amount of structure. For example, in uniformly weighted graphs
the value of the lex-minimizer at every vertex not in T is equal to the average of the minimum and maximum of the
values at its neighbors. This is analogous to the property of the 2-Laplacian minimizer that the value at every vertex
not in T equals the average of the values at its neighbors.

1.1 Contributions

We ﬁrst present several important structural properties of lex-minimizers in Section 3.2. As we shall point out, some
of these were known from previous work, sometimes in restricted settings. We state them generally and prove them
for completeness. We also prove that the lex-minimizer is as stable as possible under perturbations of v0 (Section 3.1).
The structure of the lex-minimization problem has led us to develop elegant algorithms for its solution. Both the
algorithms and their analyses could be taught to undergraduates. We believe that these algorithms could be used in
place of 2-Laplacian minimization in many applications.

We present algorithms for the following problems. Throughout, m = |E| and n = |V |.

Inf-minimization: An algorithm that runs in expected time O(m + n log n) (Section 4.3).

Lex-minimization: An algorithm that runs in expected time O(n(m + n log n)) (Section 4), along with a variant that

runs quickly in practice (Section 4.4).

l1-regularization of edge lengths for inf-minimization: The problem of minimizing (1) given a limited budget with
O(m3/2)
which one can increase edge lengths is a linear programming problem. We show how to solve it in time
with an interior point method by using fast Laplacian solvers (Section 8). The same algorithm can accommodate
l1-regularization of the values given in v0.

e

l0-regularization of vertex values for inf-minimization: We give a polynomial time algorithm for l0-regularization
of the values at vertices. That is, we minimize (1) given a budget of a number of vertices that can be proclaimed
outliers and removed from T (Section 7.1). We solve this problem by reducing it to the problem of computing
minimum vertex covers on transitively closed directed acyclic graphs, a special case of minimum vertex cover
that can be solved in polynomial time.

After any regularization for inf-minimization, we suggest computing the lex-minimizer. We ﬁnd the result for l0-
regularization of vertex values to be particularly surprising, especially because we prove that the analogous problem
for 2-Laplacian minimization is NP-Hard (Section 7.2).

All of our algorithms extend naturally to directed graphs (Section 5). This is in contrast with the problem of
minimizing 2-Laplacians on directed graphs, which corresponds to computing electrical ﬂows in networks of resistors
and diodes, for which fast algorithms are not presently known.

We present a few experiments on examples demonstrating that the lex-minimizer can overcome known deﬁcien-
cies of the 2-Laplacian minimizer (Section 1.2, Figures 1,2), as well as a demonstration of the performance of the
directed analog of our algorithms on the WebSpam dataset of Castillo et al. (2006) (Section 6). In the WebSpam prob-
lem we use the link structure of a collection of web sites to ﬂag some sites as spam, given a small number of labeled
sites known to be spam or normal.

1.2 Relation to Prior Work

We ﬁrst encountered the idea of using the minimizer of the 2-Laplacian given by (2) for regression and classiﬁca-
tion on graphs in the work of Zhu et al. (2003) and Belkin et al. (2004) on semi-supervised learning. These works
transformed learning problems on sets of vectors into problems on graphs by identifying vectors with vertices and
constructing graphs with edges between nearby vectors. One shortcoming of this approach (see Nadler et al. (2009),

2

e
g
a

t
l

 

o
V
d
e
r
r
e

f

n

I

1

0.8

0.6

0.4

0.2

0

-0.2

-0.4

-0.6

-0.8

-1

-4

50 lex
50 l2
100 lex
100 l2
500 lex
500 l2
1000 lex
1000 l2

0.25

0.2

r
o
r
r
e
 
1
l
 
n
a
e
M

0.15

0.1

0.05

0
5000 

lex
2-Lap
labels

-2

0

2
Vertex position on real line

4

6

8

Figure 1: Lex vs 2-Laplacian on 1D gaussian clus-
ters.

Figure 2: kNN graphs on samples from 4D cube.

10000

20000

40000

80000

Number of Vertices

Alamgir and Luxburg (2011), Bridle and Zhu (2013)) is that if the number of vectors grows while the number of la-
beled vectors remains ﬁxed, then almost all the values of the 2-Laplacian minimizer converge to the mean of the
labels on most natural examples. For example, Nadler et al. (2009) consider sampling points from two Gaussian
distributions centered at 0 and 4 on the real line. They place edges between every pair of points (x, y) with length
exp(|x − y|2 /2σ2) for σ = 0.4, and provide only the labels v0(0) = −1 and v0(4) = 1. Figure 1 shows the values
of the 2-Laplacian minimizer in red, which are all approximately zero. In contrast, the values of the lex-minimizer in
blue, which are smoothly distributed between the labeled points, are shown.

The “manifold hypothesis” (see Chapelle et al. (2010), Ma and Fu (2011)) holds that much natural data lies near a
low-dimensional manifold and that natural functions we would like to learn on this data are smooth functions on the
manifold. Under this assumption, one should expect lex-minimizers to interpolate well. In contrast, the 2-Laplacian
minimizers degrade (dotted lines) if the number of labeled points remains ﬁxed while the total number of points grows.
In Figure 2, we demonstrate this by sampling many points uniformly from the unit cube in 4 dimensions, form their
8-nearest neighbor graph, and consider the problem of regressing the ﬁrst coordinate. We performed 8 experiments,
varying the number of labeled points in {50, 100, 500, 1000}. Each data point is the mean average l1 error over 100
experiments. The plots for root mean squared error are similar. The standard deviation of the estimations of the mean
are within one pixel, and so are not displayed. The performance of the lex-minimizer (solid lines) does not degrade as
the number of unlabeled points grows.

Analogous to our inf-minimizers, minimal Lipschitz extensions of functions in Euclidean space and over more
general metric spaces have been studied extensively in Mathematics (Kirszbraun (1934), McShane (1934), Whitney
(1934)). von Luxburg and Bousquet (2003) employ Lipschitz extensions on metric spaces for classiﬁcation and relate
these to Support Vector Machines. Their work inspired improvements in classiﬁcation and regression in metric spaces
with low doubling dimension (Gottlieb et al. (2013), Gottlieb et al. (2013b)). Theoretically fast, although not actually
practical, algorithms have been given for constructing minimal Lipschitz extensions of functions on low-dimensional
Euclidean spaces (Fefferman (2009a), Fefferman and Klartag (2009), Fefferman (2009b)). Sinop and Grady (2007)
suggest using inf-minimizers for binary classiﬁcation problems on graphs. For this special case, where all of the
given values are either 0 or 1, they present an O(m + n log n) time algorithm for computing an inf-minimizer. The
case of general given values, which we solve in this paper, is much more complicated. To compensate for the non-
uniqueness of inf-minimizers, they suggest choosing the inf-minimizer that minimizes (2) with p = 2. We believe that
the lex-minimizer is a more natural choice.

The analog of our lex-minimizer over continuous spaces is called the absolutely minimal Lipschitz extension
(AMLE). Starting with the work of Aronsson (1967), there have been several characterizations and proofs of the ex-
istence and uniqueness of the AMLE (Jensen (1993), Crandall et al. (2001), Barles and Busca (2001), Aronsson et al.
(2004)). Many of these results were later extended to general metric spaces, including graphs (Milman (1999),
Peres et al. (2011), Naor and Shefﬁeld (2010), Shefﬁeld and Smart (2010)). However, to the best of our knowledge,
fast algorithms for computing lex-minimizers on graphs were not known. For the special case of undirected, un-
weighted graphs, Lazarus et al. (1999) presented both a polynomial-time algorithm and an iterative method. Oberman

3

(2011) suggested computing the AMLE in Euclidean space by ﬁrst discretizing the problem and then solving the cor-
responding graph problem by an iterative method. However, no run-time guarantees were obtained for either iterative
method.

2 Notation and Basic Deﬁnitions

Lexicographic Ordering. Given a vector r ∈ Rm, let πr denote a permutation that sorts r in non-increasing order
by absolute value, i.e., ∀i ∈ [m − 1], |r(πr(i))| ≥ |r(πr(i + 1))|. Given two vectors r, s ∈ Rm, we write r (cid:22) s to
indicate that r is smaller than s in the lexicographic ordering on sorted absolute values, i.e.

∃j ∈ [m],

r(πr(j))

<

s(πs(j))

and ∀i ∈ [j − 1],

r(πr(i))

=

s(πs(i))

or ∀i ∈ [m],

=

r(πr(i))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.
(cid:12)
(cid:12)

s(πs(i))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

Note that it is possible that r (cid:22) s and s (cid:22) r while r 6= s. It is a total relation: for every r and s at least one of r (cid:22) s
or s (cid:22) r is true.

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Graphs and Matrices. We will work with weighted graphs. Unless explicitly stated, we will assume that they are
undirected. For a graph G, we let VG be its set of vertices, EG be its set of edges, and ℓG : EG → R+ be the
assignment of positive lengths to the edges. We let |VG| = n, and |EG| = m. We assume ℓG is symmetric, i.e.,
ℓG(x, y) = ℓG(y, x). When G is clear from the context, we drop the subscript.

A path P in G is an ordered sequence of (not necessarily distinct) vertices P = (x0, x1, . . . , xk), such that
(xi−1, xi) ∈ E for i ∈ [k]. The endpoints of P are denoted by ∂0P = x0, ∂1P = xk. The set of interior vertices
of P is deﬁned to be int(P ) = {xi : 0 < i < k}. For 0 ≤ i < j ≤ k, we use the notation P [xi : xj] to denote the
k
subpath (xi, . . . , xj). The length of P is ℓ(P ) =
i=1 ℓ(xi−1, xi).
A function v0 : V → R ∪ {∗} is called a voltage assignment (to G). A vertex x ∈ V is a terminal with
respect to v0 iff v0(x) 6= ∗. The other vertices, for which v0(x) = ∗, are non-terminals. We let T (v0) denote the
set of terminals with respect to v0. If T (v0) = V, we call v0 a complete voltage assignment (to G). We say that an
assignment v : V → R ∪ {∗} extends v0 if v(x) = v0(x) for all x such that v0(x) 6= ∗.

Given an assignment v0 : V → R ∪ {∗}, and two terminals x, y ∈ T (v0) for which (x, y) ∈ E, we deﬁne the

P

gradient on (x, y) due to v0 to be

gradG[v0](x, y) =

v0(x) − v0(y)
ℓ(x, y)

.

It may be useful to view gradG[v0](x, y) as the current in the edge (x, y) induced by voltages v0. When v0 is a
complete voltage assignment, we interpret gradG[v0] as a vector in Rm, with one entry for each edge. However, for
convenience, we deﬁne gradG[v0](x, y) = −gradG[v0](y, x). When G is clear from the context, we drop the subscript.
A graph G along with a voltage assignment v to G is called a partially-labeled graph, denoted (G, v). We say
that a partially-labeled graph (G, v0) is a well-posed instance if for every maximal connected component H of G, we
have T (v0) ∩ VH 6= ∅.

A path P in a partially-labeled graph (G, v0) is called a terminal path if both endpoints are terminals. We deﬁne

∇P (v0) to be its gradient:

∇P (v0) =

v0(∂0P ) − v0(∂1P )
ℓ(P )

.

If P contains no terminal-terminal edges (and hence, contains at least one non-terminal), it is a free terminal path.

Lex-Minimization. An instance of the LEX-MINIMIZATION problem is described by a partially-labeled graph
(G, v0). The objective is to compute a complete voltage assignment v : VG → R extending v0 that lex-minimizes
grad[v].

Deﬁnition 2.1 (Lex-minimizer) Given a partially-labeled graph (G, v0), we deﬁne lexG[v0] to be a complete voltage
assignment to V that extends v0, and such that for every other complete assignment v′ : VG → R that extends v0, we
have gradG[lexG[v0]] (cid:22) gradG[v′]. That is, lexG[v0] achieves a lexicographically-minimal gradient assignment to the
edges.

We call lexG[v0] the lex-minimizer for (G, v0). Note that if T (v0) = VG, then trivially, lexG[v0] = v0.

4

3 Basic Properties of Lex-Minimizers

Lazarus et al. (1999) established that lex-minimizers in unweighted and undirected graphs exist, are unique, and may
be computed by an elementary meta-algorithm. We state and prove these facts for undirected weighted graphs, and
defer the discussion of the directed case to Section 5. We also state for directed and weighted graphs characterizations
of lex-minimizers that were established by Peres et al. (2011), Naor and Shefﬁeld (2010) and Shefﬁeld and Smart
(2010) for unweighted graphs. These results are essential for the analyses of our algorithms. We defer most proofs to
Appendix A.

Deﬁnition 3.1 A steepest ﬁxable path in an instance (G, v0) is a free terminal path P that has the largest gradient
∇P (v0) amongst such paths.

Observe that a steepest ﬁxable path with ∇P (v0) 6= 0 must be a simple path.
Deﬁnition 3.2 Given a steepest ﬁxable path P in an instance (G, v0), we deﬁne ﬁxG[v0, P ] : VG → R ∪ {∗} to be the
voltage assignment deﬁned as follows

ﬁxG[v0, P ](x) =

v0(∂0P ) − ∇P (v0) · ℓG(P [∂0P : x]) x ∈ int(P ) \ T (v0),
v0(x)

otherwise.

(

We say that the vertices x ∈ int(P ) are ﬁxed by the operation ﬁx[v0, P ]. If we deﬁne v1 = ﬁxG[v0, P ], where
P = (x0, . . . , xr) is the steepest ﬁxable path in (G, v0), then it is easy to argue that for every i ∈ [r], we have
grad[v1](xi−1, xi) = ∇P (see Lemma A.5). The meta-algorithm META-LEX, spelled out as Algorithm 1, entails
repeatedly ﬁxing steepest ﬁxable paths. While it is possible to have multiple steepest ﬁxable paths, the result of ﬁxing
all of them does not depend on the order in which they are ﬁxed.

Theorem 3.3 Given a well-posed instance (G, v0), the meta-algorithm META-LEX, which repeatedly ﬁxes steepest
ﬁxable paths, produces the unique lex-minimizer extending v0.

Corollary 3.4 Given a well-posed instance (G, v0) such that T (v0) 6= VG, let P be a steepest ﬁxable path in (G, v0).
Then, (G, ﬁx[v0, P ]) is also a well-posed instance, and lexG[ﬁx[v0, P ]] = lexG[v0].

Since a lex-minimal element must be an inf-minimizer, we also obtain the following corollary, that can also be

proved using LP duality.

Lemma 3.5 Suppose we have a well-posed instance (G, v0). Then, there exists a complete voltage assignment v
extending v0 such that

grad[v]

∞ ≤ α, iff every terminal path P in (G, v0) satisﬁes ∇P (v0) ≤ α.
(cid:13)
(cid:13)

3.1 Stability

(cid:13)
(cid:13)

The following theorem states that lexG[v0] is monotonic with respect to v0 and it respects scaling and translation of
v0.

Theorem 3.6 Let (G, v0) be a well-posed instance with T := T (v0) as the set of terminals. Then the following
statements hold.

1. For any c, d ∈ R, v1 a partial assignment with terminals T (v1) = T and v1(t) = cv0(t) + d for all t ∈ T .

Then, lexG[v1](i) = c · lexG[v0](i) + d for all i ∈ VG.

2. v1 a partial assignment with terminals T (v1) = T. Suppose further that v1(t) ≥ v0(t) for all t ∈ T. Then,

lexG[v1](i) ≥ lexG[v0](i) for all i ∈ VG.

As a corollary, the above theorem gives a nice stability property that lex-minimal elements satisfy.

Corollary 3.7 Given well-posed instances (G, v0), (G, v1) such that T := T (v0) = T (v1), let ǫ := maxt∈T |v0(t) −
v1(t)|. Then |lexG[v0](i) − lexG[v1](i)| ≤ ǫ for all i ∈ VG.

5

3.2 Alternate Characterizations

There are at least two other seemingly disparate deﬁnitions that are equivalent to lex-minimal voltages.

lp-norm Minimizers. As mentioned in the introduction, for a well-posed instance (G, v0) the lex-minimizer is also
the limit of lp minimizers. This follows from existing results about the limit of lp-minimizers (Egger and Huotari
(1990)) in afﬁne spaces, since {grad[v] | v is complete, v extends v0} forms an afﬁne subspace of Rm. Thus, we have
the following theorem:

Theorem 3.8 (Limit of lp-minimizers, follows from Egger and Huotari (1990)) For any p ∈ (1, ∞), given a well-
posed instance (G, v0) deﬁne vp to be the unique complete voltage assignment extending v0 and minimizing
p ,
i.e.

grad[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Then, limp→∞ vp = lexG[v0].

vp = arg min
v is complete
v extends v0 (cid:13)
(cid:13)

grad[v]

p .

(cid:13)
(cid:13)

Max-Min Gradient Averaging. Consider a well-posed instance (G, v0), and a complete voltage assignment v ex-
tending v0. If G is such that ℓ(e) = 1 for all e ∈ EG, it is easy to see that lex = lexG[v0] satisﬁes the following simple
condition for all x ∈ VG \ T (v0),

lex(x) =

1
2  

max
(x,y)∈EG

lex(y) + min

lex(z)

.

(x,z)∈EG

!

This condition should be contrasted to the optimality condition for l2-regularization on these instances, which gives
for all non-terminals x, the optimal voltage v satisﬁes v(x) = 1

y:(x,y)∈EG v(y).

deg(x)

To prove the above claim, consider locally changing lex at x and observe that the gradients of edges not incident
at x remain unchanged, and at least one of edges incident at x will have a strictly larger gradient, contradicting lex-
minimality. For general graphs, this condition of local optimality can still be characterized by a simple max-min
gradient averaging property as described below.

P

Deﬁnition 3.9 (Max-Min Gradient Averaging) Given a well-posed instance (G, v0), and a complete voltage as-
signment v extending v0, we say that v satisﬁes the max-min gradient averaging property (w.r.t. (G, v0)) if for every
x ∈ VG \ T (v0), we have

grad[v](x, y) = − min

grad[v](x, y).

max
y:(x,y)∈EG

y:(x,y)∈EG

As stated in the theorem below, lexG[v0] is the unique assignment satisfying max-min gradient averaging property.
Shefﬁeld and Smart (2010) proved a variant of this statement for weighted graphs. For completeness, we present a
proof in the appendix.

Theorem 3.10 Given a well-posed instance (G, v0), lexG[v0] satisﬁes max-min gradient averaging property. More-
over, it is the unique complete voltage assignment extending v0 that satisﬁes this property w.r.t. (G, v0).

An advantage of this characterization is that it can be veriﬁed quickly. This is particularly useful for implementations
for computing the lex-minimizer.

4 Algorithms

We now sketch the ideas behind our algorithms and give precise statements of our results. A full description of all the
algorithms is included in the appendix.

We deﬁne the pressure of a vertex to be the gradient of the steepest terminal path through it:

pressure[v0](x) = max{∇P (v0) | P is a terminal path in (G, v0) and x ∈ P }.

6

Observe that in a graph with no terminal-terminal edges, a free terminal path is a steepest ﬁxable path iff its gradient
is equal to the highest pressure amongst all vertices. Moreover, vertices that lie on steepest ﬁxable paths are exactly
the vertices with the highest pressure. For a given α > 0, in order to identify vertices with pressure exceeding α, we
compute vectors vHigh[α](x) and vLow[α](x) deﬁned as follows in terms of dist, the metric on V induced by ℓ:

vLow[α](x) = min

{v0(t) + α · dist(x, t)}

t∈T (v0)

vHigh[α](x) = max
t∈T (v0)

{v0(t) − α · dist(t, x)}.

4.1 Lex-minimization on Star Graphs

We ﬁrst consider the problem of computing the lex-minimizer on a star graph in which every vertex but the center is a
terminal. This special case is a subroutine in the general algorithm, and also motivates some of our techniques.

Let x be the center vertex, T be the set of terminals, and all edges be of the form (x, t) with t ∈ T . The initial
voltage assignment is given by v : T → R, and we abbreviate dist(x, t) by d(t) = ℓ(x, t). From Corollary 3.4 we know
that we can determine the value of the lex minimizer at x by ﬁnding a steepest ﬁxable path. By deﬁnition, we need to
ﬁnd t1, t2 ∈ T that maximize the gradient of the path from t1 to t2, ∇(t1, t2) = v(t1)−v(t2)
d(t2)+d(t2) . As observed above, this
is equivalent to ﬁnding a terminal with the highest pressure. We now present a simple randomized algorithm for this
problem that runs in expected linear time.

Given a terminal t1, we can compute its pressure α along with the terminal t2 such that |∇(t1, t2)| = α in time
O(|T |) by scanning over the terminals in T . Consider doing this for a random terminal t1. We will show that in linear
time one can then ﬁnd the subset of terminals T ′ ⊂ T whose pressure is greater than α. Assuming this, we complete
the analysis of the algorithm. If T ′ = ∅, t1 is a vertex with highest pressure. Hence the path from t1 to t2 is a steepest
ﬁxable path, and we return (t1, t2). If T ′ 6= ∅, the terminal with the highest pressure must be in T ′, and we recurse by
picking a new random t1 ∈ T ′. As the size of T ′ will halve in expectation at each iteration, the expected time of the
algorithm on the star is O(|T |).

To determine which terminals have pressure exceeding α, we observe that the condition ∃t2 : α < ∇(t1, t2) =
v(t1)−v(t2)
d(t1)+d(t2) , is equivalent to ∃t2 : v(t2)+αd(t2) < v(t1)−αd(t1). This, in turn, is equivalent to vLow[α](x) < v(t1)−
αd(t1). We can compute vLow[α](x) in deterministic O(|T |) time. Similarly, we can check if ∃t2 : α < ∇(t2, t1) by
checking if vHigh[α](x) > vt1 + αd(t1). Thus, in linear time, we can compute the set T ′ of terminals with pressure
exceeding α. The above algorithm is described in Algorithm 10.

Theorem 4.1 Given a set of terminals T, initial voltages v : T → R, and distances d : T → R+, STARSTEEPESTPATH(T, v, d)
returns (t1, t2) maximizing v(t1)−v(t2)

d(t1)+d(t2) , and runs in expected time O(|T |).

4.2 Lex-minimization on General Graphs

Theorem 3.3, tells us that META-LEX will compute lex-minimizers given an algorithm for ﬁnding a steepest ﬁxable
path in (G, v0). Recall that ﬁnding a steepest ﬁxable path is equivalent to ﬁnding a path with gradient equal to the
highest pressure amongst all vertices. In this section, we show how to do this in expected time O(m + n log n).

We describe an algorithm VERTEXSTEEPESTPATH that ﬁnds a terminal path P through any vertex x such that
∇P (v0) = pressure[v0](x) in expected O(m + n log n) time. Using Dijkstra’s algorithm, we compute dist(x, t) for
all t ∈ T. If x ∈ T (v0), then there must be a terminal path P that starts at x that has ∇P (v0) = pressure[v0](x). To
compute such a P we examine all t ∈ T (v0) in O(|T |) time to ﬁnd the t that maximizes |∇(x, t)| = |v(x)−v(t)|
, and
dist(x,t)
then return a shortest path between x and that t.

If x /∈ T (v0), then the steepest path through x between terminals t1 and t2 must consist of shortest paths between
x and t1 and between x and t2. Thus, we can reduce the problem to that of ﬁnding the steepest path in a star graph
where x is the only non-terminal and is connected to each terminal t by an edge of length dist(x, t). By Theorem 4.1,
we can ﬁnd this steepest path in O(|T |) expected time. The above algorithm is formally described as Algorithm 9.

Theorem 4.2 Given a well-posed instance (G, v0), and a vertex x ∈ VG, VERTEXSTEEPESTPATH(G, v0, x) returns
a terminal path P through x such that ∇P (v0) = pressure[v0](x), in O(m + n log n) expected time.

7

As in the algorithm for the star graph, we need to identify the vertices whose pressure exceeds a given α. For a ﬁxed
α, we can compute vLow[α](x) and vHigh[α](x) for all x ∈ VG using a simple modiﬁcation of Dijkstra’s algorithm in
O(m + n log n) time. We describe the algorithms COMPVHIGH, COMPVLOW for these tasks in Algorithms 3 and 4.
The following lemma encapsulates the usefulness of vLow and vHigh.

Lemma 4.3 For every x ∈ VG, pressure[v0](x) > α iff vHigh[α](x) > vLow[α](x).

It immediately follows that the algorithm COMPHIGHPRESSGRAPH(G, v0, α) described in Algorithm 6 computes

the vertex induced subgraph on the vertex set {x ∈ VG| pressure[v0](x) > α}.

We can combine these algorithms into an algorithm STEEPESTPATH that ﬁnds the steepest ﬁxable path in (G, v0)
in O(m + n log n) expected time. We may assume that there are no terminal-terminal edges in G. We sample an edge
(x1, x2) uniformly at random from EG, and a terminal x3 uniformly at random from VG. For i = 1, 2, 3, we compute
the steepest terminal path Pi containing xi. By Theorem 4.2, this can be done in O(m + n log n) expected time. Let α
be the largest gradient maxi ∇Pi. As mentioned above, we can identify G′, the induced subgraph on vertices x with
pressure exceeding α, in O(m + n log n) time. If G′ is empty, we know that the path Pi with largest gradient is a
steepest ﬁxable path. If not, a steepest ﬁxable path in (G, v0) must be in G′, and hence we can recurse on G′. Since
we picked a uniformly random edge, and a uniformly random vertex, the expected size of G′ is at most half that of G.
Thus, we obtain an expected running time of O(m + n log n). This algorithm is described in detail in Algorithm 7.

Theorem 4.4 Given a well-posed instance (G, v0) with EG ∩ (T (v0) × T (v0)) = ∅, STEEPESTPATH(G, v0) returns
a steepest ﬁxable path in (G, v0), and runs in O(m + n log n) expected time.

By using STEEPESTPATH in META-LEX, we get the COMPLEXMIN, shown in Algorithm 1. From Theorem 3.3 and
Theorem 4.4, we immediately get the following corollary.

Corollary 4.5 Given a well-posed instance (G, v0) as input, algorithm COMPLEXMIN computes a lex-minimizing
assignment that extends v0 in O(n(m + n log n)) expected time.

4.3 Linear-time Algorithm for Inf-minimization

Given the algorithms in the previous section, it is straightforward to construct an inﬁnity minimizer. Let α⋆ be the
gradient of the steepest terminal path. From Lemma 3.5, we know that the norm of the inf minimizer is α⋆. Considering
all trivial terminal paths (terminal-terminal edges), and using STEEPESTPATH, we can compute α⋆ in randomized
O(m+n log n) time. It is well known (McShane (1934); Whitney (1934)) that v1 = vLow[α⋆] and v2 = vHigh[α⋆] are
inf-minimizers. It is also known that 1
2 (v1 + v2) is the inf-minimizer that minimizes the maximum ℓ∞-norm distance
to all inf-minimizers. In the case of path graphs, this was observed by Gaffney and Powell (1976) and independently
by Micchelli et al. (1976). For completeness, the algorithm is presented as Algorithm 5, and we have the following
result.

Theorem 4.6 Given a well-posed instance (G, v0), COMPINFMIN(G, v0) returns a complete voltage assignment v
for G extending v0 that minimizes

∞ , and runs in randomized O(m + n log n) time.

grad[v]

4.4 Faster Algorithms for Lex-minimization

(cid:13)
(cid:13)

(cid:13)
(cid:13)

The lex-minimizer has additional structure that allows one to compute it by more efﬁcient algorithms. One observation
that leads to a faster implementation is that ﬁxing a steepest ﬁxable path does not increase the pressure at vertices,
provided that one appropriately ignores terminal-terminal edges. Thus, if G(α) is a subgraph that we identiﬁed with
pressure greater than α, we can iteratively ﬁx all steepest ﬁxable paths P in G(α) with ∇P > α. Another simple
observation is that if G(α) is disconnected, we can simply recurse on each of the connected components. A complete
description of an the algorithm COMPFASTLEXMIN based on these idea is given in Algorithm 11. The algorithm
provably computes lexG(v0), and it is possible to implement it so that the space requirement is only O(m + n).
Although, we are unable to prove theoretical bounds on the running time that are better than O(n(m + n log n)),
it runs extremely quickly in practice. We used it to perform the experiments in this paper. For random regular
graphs and Delaunay graphs, with n = 0.5 × 106 vertices and around 2 million edges m ∼ 1.5 − 2 × 106, it

8

takes a couple of minutes on a 2009 MacBook Pro. Similar times are observed for other model graphs of this
size such as random regular graphs and real world networks. An implementation of this algorithm may be found
at https://github.com/danspielman/YINSlex.

5 Directed Graphs

Our deﬁnitions and algorithms, including those for regularization, extend to directed graphs with only small modiﬁ-
cations. We view directed edges as diodes and only consider potential differences in the direction of the edge. For
a complete voltage assignment v on the vertices of a directed graph G, we deﬁne the directed gradient on (x, y) due
to v to be grad+
. Given a partially-labelled directed graph (G, v0), we say that a a
complete voltage assignment v is a lex-minimizer if it extends v0 and for other complete voltage assignment v′ that
extends v0 we have grad+
G[v′]. We say that a partially-labelled directed graph (G, v0) is a well-posed
directed instance if every free vertex appears in a directed path between two terminals.

G[v](x, y) = max

G[v] (cid:22) grad+

v(x)−v(y)
ℓ(x,y)

, 0

n

o

The main difference between the directed and undirected cases is that the directed lex-minimizer is not necessarily
unique. To maintain clarity of exposition, we chose to focus on undirected graphs so far. For directed graphs, we have
the following corresponding structural results.

Theorem 5.1 Given a well-posed instance (G, v0) on a directed graph G, there exists a lex-minimizer, and the set of
all lex-minimizers is a convex set. Moreover, for every two lex-minimizers v and v′, we have grad+

G[v] = grad+

G[v′].

However, note that in the case of directed graphs, the lex-minimizer need not be unique. We still have a weaker version
of Theorem 3.3 for directed graphs.

Theorem 5.2 Given a well-posed instance (G, v0) on a directed graph G, let v1 be the partial voltage assignment
extending v0 obtained by repeatedly ﬁxing steepest ﬁxable (directed) paths P with ∇P > 0. Then, any lex-minimizer
of (G, v0) must extend v1. Moreover, for every edge e ∈ EG \ (T (V1) × T (V1)), any lex-minimizer v of (G, v0) must
satisfy grad+[v](e) = 0.

When the value of the lex-minimizer at a vertex is not uniquely determined, it is constrained to an interval. In our
experiments, we pick the convention that when the voltage at a vertex is constrained to an interval (−∞, a] or [a, ∞),
we assign a to the terminal. When it is constrained to a ﬁnite interval, we assign a voltage closest to the median of the
original voltages.

6 Experiments on WebSpam

We demonstrate the performance of our lex-minimization algorithms on directed graphs by using them to detect spam
webpages as in Zhou et al. (2007). We use the dataset webspam-uk2006-2.0 described in Castillo et al. (2006).
This collection includes 11,402 hosts, out of which 7,473 (65.5 %) are labeled, either as spam or normal. Each host
corresponds to the collection of web pages it serves. Of the hosts, 1924 are labeled spam (25.7 % of all labels). We
consider the problem of ﬂagging some hosts as spam, given only a small fraction of the labels for training. We assign
a value of 1 to the spam hosts, and a value of 0 to the normal ones. We then compute a lex minimizer and examine the
effect of ﬂagging as spam all hosts with a value greater than some threshold.

Following Zhou et al. (2007), we create edges between hosts with lengths equal to the reciprocal of the number of
links from one to the other. We run our experiments only on the largest strongly connected component of the graph,
which contains 7945 hosts of which 5552 are labeled. 16 % of the nodes in this subgraph are labeled spam. To create
training and test data, for a given value p, we select a random subset of p % of the spam labels and a random subset
of p % of the normal labels to use for training. The remaining labels are used for testing. We report results for p = 5
and p = 20.

Again following Zhou et al. (2007), we plot the precision and recall of different choices of threshold for ﬂagging
pages as spam. Recall is the fraction of spam pages our algorithm ﬂags as spam, and precision is the fraction of pages
our algorithm ﬂags as spam that actually are spam. Amongst the algorithms studied by Zhou et al. (2007), the top

9

performer was their algorithm based on sampling according to a random-walk that follows in-links from other hosts.
We compare their algorithm with the classiﬁcation we get by directing edges in the opposite directions of links. This
has the effect that a link to a spam host is evidence of spamminess, and a link from a normal host is evidence of
normality.

Results are shown in Figure 3. While we are not able to reliably ﬂag all spam hosts, we see that in the range of
10-50 % recall, we are able to ﬂag spam with precision above 82 %. We see that the performance of directed lex-
minimization does not degrade rapidly when from the “large training set” regime of p = 20, to the “small training set”
regime of p = 5.

5 % labels for training

20 % labels for training

RandWalk
DirectedLex

RandWalk
DirectedLex

i

i

n
o
s
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
0.1

0.2

0.3

0.4

0.7

0.8

0.9

1

0.2

0.3

0.4

0.7

0.8

0.9

1

0.6
0.5
Recall

0.6
0.5
Recall

Figure 3: Recall and precision in the web spam classiﬁcation experiment. Each data point shown was computed as an average over
100 runs. The largest standard deviation of the mean precision across the plotted recall values was less than 1.3 %. The algorithm
of Zhou et al. (2007) appears as RANDWALK. Our directed lex-minimization algorithm appears as DIRECTEDLEX.

For comparison, in Appendix C, we show the performance of our algorithm and that of Zhou et al. (2007) both
with link directions reversed, as well as the performance of undirected lex-minimization and Laplacian inference, all
of which are signiﬁcantly worse.

7 l0-Regularization of Vertex Values

We now explain how we can accommodate noise in both the given voltages and in the given lengths of edges. We can
ﬁnd the minimum number of labels to ignore, or the minimum increase in edges lengths needed so that there exists an
extension whose gradients have l∞-norm lower than a given target. After determining which labels to ignore or the
needed increment in edge lengths, we recommend computing a lex minimizer.

The algorithms we present in this section are essentially the same for directed and undirected graphs.

7.1 l0-Vertex Regularization for Inf-minimization

The l0-regularization of vertex labels can be viewed as a problem of outlier removal: the vector we compute is allowed
to disagree with v0 on up to k terminals. Given a voltage assignment v and a subset T ⊂ V of the vertices, by v(T )
we mean the vector obtained by restricting v to T . We deﬁne the l0-Vertex Regularization for l∞ problem to be

where v(T ) is the vector of values of v on the terminals T .

min
v∈IRn

gradG[v]

∞

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

0 ≤ k,
(cid:13)
(cid:13)

subject to

v(T ) − v0(T )

(3)

In Appendix D, we describe an approximation algorithm APPROX-OUTLIER that approximately solves program (3).

The precise statement we prove in Appendix D is given in the following theorem.

i

i

n
o
s
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
0.1

10

Theorem 7.1 (Approximate l0-vertex regularization) The algorithm APPROX-OUTLIER takes a positive integer k
and a partially-labeled graph (G, v0), and outputs an assignment v with
0 ≤ 2k, and
∞ ≤
α∗, where α∗ is the optimum value of program (3). The algorithm runs in time O(k(m + n log n)).
(cid:13)
(cid:13)
(cid:13)
(cid:13)

In Appendix D, we also describe an algorithm OUTLIER that exactly solves program (3) in polynomial time, and we
prove its correctness.

v(T ) − v0(T )

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Theorem 7.2 (Exact l0-vertex regularization) The algorithm OUTLIER takes a positive integer k and a partially-
labeled graph (G, v0) solves program (3) exactly. The algorithm runs in polynomial time.

We give a proof of Theorem 7.2 in Appendix D. To do this, we reduce the program (3) to the problem of minimizing
the required l0-budget needed to achieve a ﬁxed gradient α using a binary search over a set of O(n2) gradients. This
latter problem we reduce in polynomial time to Minimum Vertex Cover (VC) on a transitively closed, directed acyclic
graph (a TC-DAG). VC on a TC-DAG can be solved exactly in polynomial time by a reduction to the Maximum
Bipartite Matching Problem (Fulkerson (1956)). The problem was phrased by Fulkerson as one of ﬁnding a maximum
antichain of a ﬁnite poset. Any transitively closed DAG corresponds directly to the comparability graph of a poset. A
maximum antichain of a poset is a maximum independent set of a the comparability graph of the poset, and hence its
complement is a minimum vertex cover of the comparability graph. We refer to the algorithm developed by Fulkerson
as KONIG-COVER.

Theorem 7.3 The algorithm KONIG-COVER computes a minimum vertex cover for any transitively closed DAG G in
polynomial time.

7.2 Hardness of l0 regularization for l2

The result that l0-regularized inf-minimization can be solved exactly in polynomial time is surprising, especially
because the analogous problem for 2-Laplacian minimization turns out to be NP-Hard.

We deﬁne the the l0 vertex regularization for l2 for a partially-labeled graph (G, v0) and an integer k by

min
v∈Rn:kv(T )−v0(T )k0

≤k

vT Lv,

where L is the Laplacian of G.

Theorem 7.4 l0 vertex regularization for l2 is NP-Hard.

In Appendix E we prove Theorem 7.4 by giving a polynomial time (Karp) reduction from the NP-Hard minimum
bisection problem to l0 vertex regularization for l2.

8 l1-Edge and Vertex Regularization of Inf-minimizers

Consider a partially-labeled graph (G, v0) and an α > 0. The set of voltage assignments given by

v : v extends v0 and

gradG[v]

∞ ≤ α

n

(cid:13)
(cid:13)

(cid:13)
(cid:13)

o

is convex. Going further, let us consider the edge lengths in a graph to be speciﬁed by a vector ℓ ∈ IRE. Now the set
of voltages v and and lengths ℓ which achieve kgradG(ℓ)[v]k∞ ≤ α is jointly convex in v and ℓ. To see this, observe
that

kgradG(ℓ)[v]k∞ ≤ α ⇔ ∀(u, v) ∈ E : −αℓ(u, v) ≤ v(u) − v(v) ≤ αℓ(u, v).
Furthermore, the condition “v extends v0” is a linear constraint on v, which we express as v(T ) = v0(T ). From
the above, it is clear that the gradient condition corresponds to a convex set, as it is an intersection of half-spaces.
These half-spaces are given by O(m) linear inequalities. We can leverage this to phrase many regularized variants of
inf-minimization as convex programs, and in some cases linear programs.

(4)

11

For example, we may consider a variant of inf-minimization combined with an l1-budget for changing lengths of
edges and values on terminals. Given a parameter γ > 0 which speciﬁes the relative cost of regularizing terminals to
regularizing edges, the problem is as follows

arg min
v∈IRn,s∈IRm,s≥0

ksk1 + γ

v(T ) − v0(T )

1

subject to

gradG(ℓ+s)[v]

≤ α.

(5)

(cid:13)
(cid:13)
From our observation (4), it follows that problem (5) may be expressed as a linear program with O(n) variables
and O(m) constraints. We can use ideas from Daitch and Spielman (2008) to solve the resulting linear program in
O(m1.5) by an interior point method with a special purpose linear equation solver. The reason is that the linear
time
equations the IPM must solve at each iteration may be reduced to linear equations in symmetric, diagonally dominant
matrices, and these may be solved in nearly-linear time (Cohen et al. (2014)).

(cid:13)
(cid:13)

e

(cid:13)
(cid:13)
(cid:13)

∞

(cid:13)
(cid:13)
(cid:13)

Conclusion. We propose the use of inf and lex minimizers for regression on graphs. We present simple algorithms
for computing them that are provably fast and correct, and can also be implemented efﬁciently. We also present a
framework and polynomial time algorithms for regularization in this setting. The initial experiments reported in the
paper indicate that these algorithms give pretty good results on real and synthetic datasets. The results seem to compare
quite favorably to other algorithms, particularly in the regime of tiny labeled sets. We are testing these algorithms on
several other graph learning questions, and plan to report on them in a forthcoming experimental paper. We believe
that inf and lex minimizers, and the associated ideas presented in the paper, should be useful primitives that can be
proﬁtably combined with other approaches to learning on graphs.

We thank anonymous reviewers for helpful comments. We thank Santosh Vempala and Bartosz Walczak for pointing
out that it was already known how to compute a minimum vertex cover of a transitively closed DAG in polynomial
time.

Acknowledgements

References

Morteza Alamgir
In Advances
Information Processing
http://books.nips.cc/papers/files/nips24/NIPS2011_0278.pdf.

and Ulrike V. Luxburg.

transition
24,

in
pages

in Neural

Systems

Phase

the

family
379–387.

of
2011.

p-resistances.
URL

Gunnar Aronsson. Extension of functions satisfying lipschitz conditions. Arkiv fr Matematik, 6(6):551–561, 1967.

ISSN 0004-2080. doi: 10.1007/BF02591928. URL http://dx.doi.org/10.1007/BF02591928.

Gunnar Aronsson, Michael G. Crandall, and Petri Juutinen. A tour of the theory of absolutely minimizing functions.
ISSN 0273-0979. doi: 10.1090/S0273-0979-04-01035-3.

Bull. Amer. Math. Soc. (N.S.), 41(4):439–505, 2004.
URL http://dx.doi.org/10.1090/S0273-0979-04-01035-3.

Guy Barles and J´erˆome Busca. Existence and comparison results for fully nonlinear degenerate elliptic equations

without zeroth-order term. Comm. Partial Differential Equations, 26:2323–2337, 2001.

Mikhail Belkin, Irina Matveeva, and Partha Niyogi.

Regularization and semi-supervised learning on large
In Learning Theory, volume 3120 of Lecture Notes in Computer Science, pages 624–638.
doi: 10.1007/978-3-540-27819-1 43. URL

graphs.
Springer Berlin Heidelberg, 2004.
http://dx.doi.org/10.1007/978-3-540-27819-1_43.

ISBN 978-3-540-22282-8.

Nick Bridle and Xiaojin Zhu. p-voltages: Laplacian regularization for semi-supervised learning on high-dimensional

data. In Eleventh Workshop on Mining and Learning with Graphs (MLG2013), 2013.

12

Carlos Castillo, Debora Donato, Luca Becchetti, Paolo Boldi, Stefano Leonardi, Massimo Santini, and Sebastiano
Vigna. A reference collection for web spam. SIGIR Forum, 40(2):11–24, December 2006. ISSN 0163-5840. doi:
10.1145/1189702.1189703. URL http://doi.acm.org/10.1145/1189702.1189703.

Olivier Chapelle, Bernhard Schlkopf, and Alexander Zien. Semi-Supervised Learning. The MIT Press, 1st edition,

2010. ISBN 0262514125, 9780262514125.

Michael B Cohen, Rasmus Kyng, Gary L Miller, Jakub W Pachocki, Richard Peng, Anup B Rao, and Shen Chen Xu.
Solving SDD linear systems in nearly m log1/2 n time. In Proceedings of the 46th Annual ACM Symposium on
Theory of Computing, pages 343–352. ACM, 2014.

M.G. Crandall, L.C. Evans, and R.F. Gariepy. Optimal lipschitz extensions and the inﬁnity laplacian. Calculus of Vari-
ations and Partial Differential Equations, 13(2):123–139, 2001. ISSN 0944-2669. doi: 10.1007/s005260000065.
URL http://dx.doi.org/10.1007/s005260000065.

Samuel I. Daitch and Daniel A. Spielman. Faster approximate lossy generalized ﬂow via interior point algo-
rithms.
In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing, STOC ’08, pages
451–460, New York, NY, USA, 2008. ACM. ISBN 978-1-60558-047-0. doi: 10.1145/1374376.1374441. URL
http://doi.acm.org/10.1145/1374376.1374441.

Alan Egger and Robert Huotari. Rate of convergence of the discrete polya algorithm. Journal of Approximation
ISSN 0021-9045. doi: http://dx.doi.org/10.1016/0021-9045(90)90070-7. URL

Theory, 60(1):24 – 30, 1990.
http://www.sciencedirect.com/science/article/pii/0021904590900707.

Charles Fefferman. Whitney’s extension problems and interpolation of data.

(N.S.), 46(2):207–220, 2009a.
http://dx.doi.org/10.1090/S0273-0979-08-01240-8.

ISSN 0273-0979.

doi:

10.1090/S0273-0979-08-01240-8.

Bull. Amer. Math. Soc.
URL

Charles Fefferman. Fitting a [image] -smooth function to data, iii. Annals of Mathematics, 170(1):pp. 427–441, 2009b.

ISSN 0003486X. URL http://www.jstor.org/stable/40345469.

Charles Fefferman and Bo’az Klartag. Fitting a cm -smooth function to data i. Annals of Mathematics, 169(1):pp.

315–346, 2009. ISSN 0003486X. URL http://www.jstor.org/stable/40345445.

D. R. Fulkerson. Note on dilworths decomposition theorem for partially ordered sets. Proc. Amer. Math. Soc, 1956.

P.W. Gaffney and M.J.D. Powell. Optimal interpolation. In Numerical Analysis, volume 506 of Lecture Notes in Math-
ematics, pages 90–99. Springer Berlin Heidelberg, 1976. ISBN 978-3-540-07610-0. doi: 10.1007/BFb0080117.
URL http://dx.doi.org/10.1007/BFb0080117.

L.-A. Gottlieb, A. Kontorovich, and R. Krauthgamer. Efﬁcient classiﬁcation for metric data. CoRR, abs/1306.2547,

2013. URL http://arxiv.org/abs/1306.2547.

L.-A. Gottlieb, A. Kontorovich, and R. Krauthgamer. Efﬁcient regression in metric spaces via approximate lipschitz
extension. In Similarity-Based Pattern Recognition, volume 7953 of Lecture Notes in Computer Science, pages
43–58. Springer Berlin Heidelberg, 2013b. ISBN 978-3-642-39139-2. doi: 10.1007/978-3-642-39140-8 3. URL
http://dx.doi.org/10.1007/978-3-642-39140-8_3.

Robert Jensen. Uniqueness of lipschitz extensions: Minimizing the sup norm of the gradient. Archive for Ra-
doi: 10.1007/BF00386368. URL

ISSN 0003-9527.

tional Mechanics and Analysis, 123(1):51–74, 1993.
http://dx.doi.org/10.1007/BF00386368.

M. Kirszbraun. ber die zusammenziehende und lipschitzsche transformationen. Fundamenta Mathematicae, 22(1):

77–108, 1934. URL http://eudml.org/doc/212681.

13

Andrew J. Lazarus, Daniel E. Loeb,

James G. Propp, Walter R. Stromquist,

Combinatorial games under

man.
229 – 264,
http://www.sciencedirect.com/science/article/pii/S0899825698906765.

http://dx.doi.org/10.1006/game.1998.0676.

and Economic Behavior,

ISSN 0899-8256.

auction play.

Games

1999.

doi:

and Daniel H. Ull-
27(2):
URL

Yunqian Ma and Yun Fu. Manifold Learning Theory and Applications. CRC Press, Inc., Boca Raton, FL, USA, 1st

edition, 2011. ISBN 1439871094, 9781439871096.

E. J. McShane. Extension of range of functions. Bull. Amer. Math. Soc., 40(12):837–842, 12 1934. URL

http://projecteuclid.org/euclid.bams/1183497871.

C.A. Micchelli, T.J. Rivlin,

and S. Winograd.

merische Mathematik, 26(2):191–200, 1976.
http://dx.doi.org/10.1007/BF01395972.

The optimal
ISSN 0029-599X.

recovery of
doi:

smooth functions.
10.1007/BF01395972.

Nu-
URL

V. A. Milman.

Absolutely minimal extensions of

functions on metric spaces.

1999.

URL

http://iopscience.iop.org/1064-5616/190/6/A05/pdf/MSB_190_6_A05.pdf.

Boaz Nadler, Nathan Srebro, and Xueyuan Zhou. Statistical analysis of semi-supervised learning: The limit of inﬁnite
unlabelled data. 2009. URL http://ttic.uchicago.edu/˜nati/Publications/NSZnips09.pdf.

A. Naor and S. Shefﬁeld. Absolutely minimal Lipschitz extension of tree-valued mappings. CoRR, abs/1005.2535,

May 2010. URL http://arxiv.org/abs/1005.2535.

A. M. Oberman. Finite difference methods for the Inﬁnity Laplace and p-Laplace equations. CoRR, abs/1107.5278,

July 2011. URL http://arxiv.org/abs/1107.5278.

Yuval Peres, Oded Schramm, Scott Shefﬁeld, and DavidB. Wilson.

Tug-of-war and the inﬁnity lapla-
In Selected Works of Oded Schramm, Selected Works in Probability and Statistics, pages 595–
doi: 10.1007/978-1-4419-9675-6 18. URL

cian.
638. Springer New York, 2011.
http://dx.doi.org/10.1007/978-1-4419-9675-6_18.

ISBN 978-1-4419-9674-9.

S. Shefﬁeld and C. K. Smart. Vector-valued optimal Lipschitz extensions. CoRR, abs/1006.1741, June 2010. URL

http://arxiv.org/abs/1006.1741.

Ali Kemal Sinop and Leo Grady. A seeded image segmentation framework unifying graph cuts and random walker
which yields a new algorithm. In Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on,
pages 1–8. IEEE, 2007.

Vijay V. Vazirani. Approximation Algorithms. Springer-Verlag New York, Inc., New York, NY, USA, 2001. ISBN

3-540-65367-8.

Ulrike von Luxburg and Olivier Bousquet. Distance-based classiﬁcation with lipschitz functions.

In Learn-
ing Theory and Kernel Machines, volume 2777 of Lecture Notes in Computer Science, pages 314–328.
doi: 10.1007/978-3-540-45167-9 24. URL
Springer Berlin Heidelberg, 2003.
http://dx.doi.org/10.1007/978-3-540-45167-9_24.

ISBN 978-3-540-40720-1.

Hassler Whitney.

Analytic extensions of differentiable functions deﬁned in closed sets.

tions of
http://www.jstor.org/stable/1989708.

the American Mathematical Society, 36(1):pp. 63–89, 1934.

ISSN 00029947.

Transac-
URL

Dengyong Zhou, Christopher J. C. Burges, and Tao Tao. Transductive link spam detection.

In Proceedings
of the 3rd International Workshop on Adversarial Information Retrieval on the Web, AIRWeb ’07, pages 21–
ISBN 978-1-59593-732-2. doi: 10.1145/1244408.1244413. URL
28, New York, NY, USA, 2007. ACM.
http://doi.acm.org/10.1145/1244408.1244413.

Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic

functions. In IN ICML, pages 912–919, 2003.

14

A Basic Properties of Lex-Minimizers

A.1 Meta Algorithm

Algorithm 1: Algorithm META-LEX: Given a well-posed instance (G, v0), outputs lexG[v0].
for i = 1, 2, . . . :

1. if T (vi−1) = VG, then return vi−1.
2. E′ = EG \ (T (vi−1) × T (vi−1)), G′ := (VG, E′).
3. Let P ⋆
4. vi ← ﬁx[vi−1, P ⋆
i ].

i be a steepest ﬁxable path in (G′, vi−1). Let α⋆

i ← ∇P ⋆(vi−1).

In this subsection, we prove the results that appeared in section 2. We start with a simple observation.

Proposition A.1 Given a well-posed instance (G, v0) such that T (v0) 6= V, let P be a steepest ﬁxable path in (G, v0).
Then, ﬁx[v0, P ] extends v0, and (G, ﬁx[v0, P ]) is also a well-posed instance.

The properties we prove below do not depend on the choice of the steepest ﬁxable path.

Proposition A.2 For any well-posed instance (G, v0), with |VG| = n, META-LEX(G, v0) terminates in at most n
iterations, and outputs a complete voltage assignment v that extends v0.

Proof of Proposition A.2: By Proposition A.1, at any iteration i, vi−1 extends v0 and (G′, vi−1) is a well-posed
instance. META-LEX only outputs vi−1 iff T (vi−1) = V, which means vi−1 is a complete voltage assignment. For
any vi−1 that is not complete, for any x ∈ V \T (vi−1), we must have a free terminal path in (G′, vi−1) that contains x.
i exists in (G′, vi−1). Since P ⋆
Hence, a steepest ﬁxable path P ⋆
i ] ﬁxes the voltage
i
✷
for at least one non-terminal. Thus, META-LEX(G, v0) must complete in at most n iterations.

is a free terminal path, ﬁx[vi−1, P ⋆

For the following lemmas, consider a run of META-LEX with well-posed instance (G, v0) as input. Let vout be the
complete voltage assignment output by META-LEX. Let Ei be the set of edges E′ and Gi be the graph G′ constructed
in iteration i of META-LEX.

Lemma A.3 For every edge e ∈ Ei−1 \ Ei, we have

grad[vout](e)

≤ α⋆

i . Moreover, α⋆

i is non-increasing with i.

Proof of Lemma A.3: Let P ⋆
i = (x0, . . . , xr) be a steepest ﬁxable path in iteration i (when we deal with instance
(Gi−1, vi−1)). Consider a terminal path Pi+1 in (Gi, vi) such that {∂0Pi+1, ∂1Pi+1} ∩ (T (vi) \ T (vi−1)) 6= ∅. We
i . On the contrary, assume that ∇Pi+1(vi) > α⋆
claim that ∇Pi+1(vi) ≤ α⋆
i . Consider the case ∂0Pi+1 ∈ T (vi) \
T (vi−1), ∂1P1 ∈ T (vi−1). By the deﬁnition of vi, we must have ∂0Pi+1 = xj for some j ∈ [r − 1]. Let P ′
i+1 be the
path formed by joining paths P ⋆

i+1 is a free terminal path in (Gi−1, vi−1). We have,

i [x0 : xj] and Pi+1. P ′

(cid:12)
(cid:12)

(cid:12)
(cid:12)

vi−1(x0) − vi−1(∂1Pi+1) = (vi(x0) − vi(xj )) + (vi(∂0Pi+1) − vi(∂1Pi+1))
i · ℓ(P ′

i · ℓ(Pi+1) = α⋆

i [x0 : xj]) + α⋆

i · ℓ(P ⋆

> α⋆

i+1),

giving ∇P ′
The other cases can be handled similarly.

i+1(vi) > α⋆

i , which is a contradiction since the steepest ﬁxable path P ⋆
i

in (Gi−1, vi−1) has gradient α⋆
i .

Applying the above claim to an edge e ∈ Ei−1 \ Ei, whose gradient is ﬁxed for the ﬁrst time in iteration i, we
i . If v is the complete voltage assignment output by META-LEX, since v extends vi+1,
i , implying

i . Applying the claim to the symmetric edge, we obtain −grad[vout](e) ≤ α⋆

obtain that grad[vi+1](e) ≤ α⋆
we get grad[vout](e) ≤ α⋆
|grad[vout](e)| ≤ α⋆
i .

Consider any free terminal path Pi+1 in (Gi, vi). If Pi+1 is also a terminal path in (Gi−1, vi−1), it is a free
terminal path in (Gi−1, vi−1). In addition, since a steepest ﬁxable path P ⋆
i , we get
i
∇Pi+1(vi) = ∇Pi+1(vi−1) ≤ α⋆
i . Otherwise, we must have {∂0Pi+1, ∂1Pi+1} ∩ (T (vi) \ T (vi−1)) 6= ∅, and we can
deduce ∇Pi+1(vi) ≤ α⋆
i using the above claim. Thus, all free terminal paths Pi+1 in (Gi, vi) satisfy ∇Pi+1(vi) ≤ α⋆
i .
✷
In particular, α⋆

in (Gi−1, vi−1) has ∇P ⋆

i = α⋆

i is non-increasing with i.

i+1(vi) ≤ α⋆

i+1 = ∇P ⋆

i . Thus, α⋆

15

Lemma A.4 For any complete voltage assignment v for G that extends v0, if v 6= vout, we have grad[v] 6(cid:22) grad[vout],
and hence grad[vout] (cid:22) grad[v].

Proof of Lemma A.4: Consider any complete voltage assignment v for G that extends v0, such that v 6= vout. Thus,
there exists a unique i such that v extends vi−1 but does not extend vi. We will argue that grad[v] 6(cid:22) grad[vout], and
hence grad[vout] (cid:22) grad[v]. For every edge e ∈ E \ Ei−1 that has been ﬁxed so far, grad[v](e) = grad[vi−1](e) =
grad[vout](e), and hence we can ignore these edges.

Since v extends vi−1 but not vi, there exists an x ∈ T (vi) \ T (vi−1) such that v(x) 6= vi(x) = vout(x). Assume
i picked

i = (x0, . . . , xr) is the steepest ﬁxable path with gradient α⋆

v(x) < vi(x) (the other case is symmetric). If P ⋆
in iteration i, we must have x = xj for some j ∈ [r − 1]. Thus,

j

j

(v(xk−1) − v(xk)) = v(x0) − v(xj ) > vi(x0) − vi(xj ) = α⋆

i · ℓ(P ⋆

i [x0 : xj ]) = α⋆
i ·

ℓ(xk−1, xk).

Xk=1

Xk=1
Thus, for some k ∈ [j], we must have grad[v](xk−1, xk) > α⋆
is a path in Gi−1, we have {xk−1, xk} 6⊆
T (vi−1). This gives (xk−1, xk) ∈ (Ei−1 \ Ei). But then, from Lemma A.3, it follows that for all e ∈ (Ei−1 \ Ei), we
✷
have |grad[vout](e)| ≤ α⋆

i . Thus, we have grad[v] 6(cid:22) grad[vout].

i . Since P ∗
i

Lemma A.5 Let P = (x0, . . . , xr) be a steepest ﬁxable path such that it does not have any edges in T (v0) × T (v0)
and v1 = ﬁxG[v0, P ]. Then for every i ∈ [r], we have grad[v1](xi−1, xi) = ∇P.

Proof of Lemma A.5: Suppose this is not true and let j ∈ [r] be the minimum number such that grad[v1](xj−1, xj) 6=
∇P. By deﬁnition of v1 we would necessarily have j < r and vj ∈ T (v0). Suppose grad[v1](xj−1, xj ) < ∇P. We
would then have v1(x0) − v1(xj ) < ∇P ∗ ℓ(P [x0 : xj]). Since P does not have any edges in T (v0) × T (v0),
P1 := (xj, ..., xr) would be a free terminal path with ∇P1 > ∇P. This is a contradiction. Other cases can be ruled
out similarly.

✷

Proof of Theorem 3.3: Consider an arbitrary run of META-LEX on (G, v0). Let vout be the complete voltage
assignment output by META-LEX. Proposition A.1 implies that vout extends v0. Lemma A.4 implies that for any
complete voltage assignment v 6= vout that extends v0, we have grad[vout] (cid:22) grad[v]. Thus, vout is a lex-minimizer.
Moreover, the lemma also gives that for any such v, grad[v] 6(cid:22) grad[vout]. and hence vout is a unique lex-minimizer.
Thus, vout is the unique voltage assignment satisfying Def. 2.1, and we denote it as lexG[v0]. Since we started with an
✷
arbitrary run of META-LEX, uniqueness implies that every run of META-LEX on (G, v0) must output lexG[v0].

Proof of Lemma 3.5: Suppose we have a complete voltage assignment v extending v0, such that
For any terminal path P = (x0, . . . , xr), we get,

grad[v]

∞ ≤ α.

∇P (v0) = v0(∂0P ) − v0(∂1P ) = v(∂0P ) − v(∂1P ) =

grad[v](xi−1, xi) ≤ α ·

ℓ(xi−1, xi) = α · ℓ(P ),

(cid:13)
(cid:13)

(cid:13)
(cid:13)

r

i=1
X

giving ∇P (v0) ≤ α.

On the other hand, suppose every terminal path P in (G, v0) satisﬁes ∇P (v0) ≤ α. Consider v = lexG[v0]. We
know that v extends v0. For every edge e ∈ EG ∩ T (v0) × T (v0), e is a (trivial) terminal path in (G, v0), and hence
has satisﬁes grad[v](e) = grad[v0](e) = ∇e(v0) ≤ α. Considering the reverse edge, we also obtain −grad[v](e) ≤ α.
Thus, |grad[v](e)| ≤ α. Moreover, using Lemma A.3, we know that for edge e ∈ EG \ T (v0) × T (v0), |grad[v](e)| ≤
1 = ∇P ⋆
α⋆
1 ≤ α since P1 is a terminal path in (G, v0). Thus, for every e ∈ EG, |grad[v](e)| ≤ α, and hence
✷
grad[v]
∞ ≤ α.
(cid:13)
(cid:13)
(cid:13)
(cid:13)
A.2 Stability

In this subsection, we sketch a proof of the monotonicity of lex-minimizers and show how it implies the stability
property claimed earlier.

For any well-posed (G, v0), there could be several possible executions of META-LEX, each characterized by the

sequence of paths P ⋆

i . We can apply Theorem 3.3 to deduce the following structural result about the lex-minimizer.

r

i=1
X

16

Corollary A.6 For any well-posed instance (G, v0), consider a sequence of paths (P1, . . . , Pr) and voltage assign-
ments (v1, . . . , vr) for some positive integer r such that:

1. P ⋆

i is a steepest ﬁxable path in (Gi−1, vi−1) for i = 1, . . . , r.

2. vi = ﬁx[vi−1, P ⋆

i ] for i = 1, . . . , r.

3. T (vr) = VG.

Then, we have vr = lexG[v0].

We call such a sequence of paths and voltages to be a decomposition of lexG[v0]. Again, note that lexG[v0] can
possibly have multiple decompositions. However, any two such decompositions are consistent in the sense that they
produce the same voltage assignment.

Proof of Corollary 3.7: We ﬁrst deﬁne some operations on partial assignments which simpliﬁes the notation. Let
v0, v1 be any two partial assignments with the same set of terminals T := T (v0) = T (v1) and c, d ∈ R. By cv0 + d
we mean a partial assignment v with T (v) = T satisfying v(t) = cv0(t) + d for all t ∈ T . Also, by v0 + v1 we
mean a partial assignment v with T (v) = T satisfying v(t) = v0(t) + v1(t) for all t ∈ T. Also, we say v1 ≥ v0 if
v1(t) ≥ v0(t) for all t ∈ T .

Now we can show how Corollary 3.7 follows from Theorem 3.6. Let v := v1 − v0, and kvk∞ = ǫ, for some ǫ > 0.
Therefore, v0 + ǫ ≥ v1 ≥ v0 − ǫ. Theorem 3.6 then implies that lexG[v0] + ǫ ≥ lex[v1] ≥ lex[v0] − ǫ, hence proving
✷
the corollary.

Proof sketch of Theorem 3.6:
It is easy to see that the ﬁrst statement holds. For the second statement, we ﬁrst
observe that if there is a sequence of paths P1, ..., Pr that is simultaneously a decomposition of both lex[v0] and
lex[v1], then this is easy to see. If such a path sequence doesn’t exist, then we look at vt := v0 + t(v1 − v0). We
state here without a proof (though the proof is elementary) that we can then split the interval [0, 1] into ﬁnitely many
subintervals [a0, a1], [a1, a2], .., [ak−1, ak], with a0 = 0, ak = 1, such that for any i, there is a path sequence P1, ..., Pr
which is a decomposition of lex[vt] for all t ∈ [ai, ai+1]. We then observe that v0 = va0 ≤ va1 ≤ ...vak = v1. Since
for every ai, ai+1, there is a path sequence which is simultaneously a decomposition of both lex[vai ] and lex[vai+1 ],
we immediately get

lex[v0] = lex[va0 ] ≤ lex[va1] ≤ ... ≤ lex[vak ] = lex[v1].

✷

A.3 Alternate Characterizations

Proof of Theorem 3.10: We know that lexG[v0] extends v0. We ﬁrst prove that v = lexG[v0] satisﬁes the max-min
gradient averaging property. Assume to the contrary. Thus, there exists x ∈ VG \ T (v0) such that

max
y:(x,y)∈EG

grad[v](x, y) 6= − min

grad[v](x, y).

y:(x,y)∈EG

Assume that max(x,y)∈EG grad[v](x, y) ≥ − min(x,y)∈EG grad[v](x, y). Then, consider v′ extending v0 that is iden-
tical to v except for v′(x) = v(x) − ǫ for ǫ > 0. For ǫ small enough, we get that

and

max
y:(x,y)∈EG

grad[v′](x, y) < max

grad[v](x, y)

y:(x,y)∈EG

− min

y:(x,y)∈EG

grad[v′](x, y) < max

grad[v](x, y).

y:(x,y)∈EG

The gradient of edges not incident on the vertex x is left unchanged. This implies that grad[v]

6(cid:22) grad[v′],

contradicting the assumption that v is the lex-minimizer. (The other case is similar).

17

For the other direction. Consider a complete voltage assignment v extending v0 that satisﬁes the max-min gradient

averaging property w.r.t. (G, v0). Let

α = max

grad[v](x, y) ≥ 0

(x,y)∈EG
x∈V \T (v0)

be the maximum edge gradient, and consider any edge (x0, x1) ∈ EG such that grad[v](x1, x0) = α, with x1 ∈
V \ T (v0). If α = 0, grad[v] is identically zero, and is trivially the lex-minimal gradient assignment. Thus, both v and
lexG[v0] are constant on each connected component. Since (G, v0) is well-posed, there is at least one terminal in each
component, and hence v and lexG[v0] must be identical.

Now assume α > 0. By the max-min gradient averaging property, ∃x2 ∈ VG such that (x1, x2) ∈ EG and

grad[v](x1, x2) =

min
y:(x1,y)∈EG

grad[v](x1, y) = − max

grad[v](x1, y)

y:(x1,y)∈EG

≤ −grad[v](x1, x0) = −α.

Thus, grad[v](x2, x1) ≥ α. Since α is the maximum edge gradient, we must have grad[v](x2, x1) = α. More-
over, v(x2) > v(x1) > v(x0), thus x2 6= x0. We can inductively apply this argument at x2 until we hit a ter-
minal. Similarly, if x0 /∈ T (v0) we can extend the path in the other direction. Consequently, we obtain a path
P = (xj , . . . , x2, x1, x0, x−1, . . . , xk) with all vertices as distinct, such that xj , xk ∈ T (v0), and xi ∈ V \ T (v0)
for all i ∈ [j + 1, k − 1]. Moreover, grad[v](xi, xi−1) = α for all j < i ≤ k. Thus, P is a free terminal path with
∇P [v0] = α.

Moreover, since v is a voltage assignment extending v0 with

∞ = α, using Lemma 3.5, we know that
every terminal path P ′ in (G, v0) must satisfy ∇P ′(v0) ≤ α. Thus, P is a steepest ﬁxable path in (G, v0). Thus,
letting v1 = ﬁx[v0, P ], using Corollary 3.4, we obtain that lexG[v1] = lexG[v0]. Moreover, since α = ∇P [v0] =
grad[v](xi, xi−1) for all i ∈ (j, k], we get v1(xi) = v(xi) for all i ∈ (j, k). Thus, v extends v1.

grad[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We can iterate this argument for r iterations until T (vr) = VG, giving v = vr and vr = lexG[vr] = lexG[v0].
(Since we are ﬁxing at least one terminal at each iteration, this procedure terminates). Thus, we get v = lexG[v0]. ✷

B Description of the Algorithms

Algorithm 2: MODDIJKSTRA(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs a complete
voltage assignment v for G, and an array parent : V → V ∪ {null}.

Add x to a ﬁbonacci heap, with key(x) = +∞.
ﬁnished(x) ← false

Decrease key(x) to v0(x).
parent(x) ← null.

1. for x ∈ VG,
2.
3.
4. for x ∈ T (v0)
5.
6.
7. while heap is not empty
8.
9.
10.
11.
12.
13.
14.
15. return (v, parent)

x ← pop element with minimum key from heap
v(x) ← key(x). ﬁnished(x) ← true .
for y : (x, y) ∈ EG

if ﬁnished(y) = false

if key(y) > v(x) + α · ℓ(x, y)

Decrease key(y) to v(x) + α · ℓ(x, y).
parent(y) ← x.

Theorem B.1 For a well-posed instance (G, V0) and a gradient value α ≥ 0, let (v, parent) ← MODDIJKSTRA(G, v0, α).
Then, v is a complete voltage assignment such that, ∀x ∈ VG, v(x) = mint∈T (v0){v0(t) + αdist(x, t)}. Moreover, the
pointer array parent satisﬁes ∀x /∈ T (v0), parent(x) 6= null and v(x) = v(parent(x)) + α · ℓ(x, parent(x)).

18

Algorithm 3: Algorithm COMPVLOW(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs
vLow, a complete voltage assignment for G, and an array LParent : V → V ∪ {null}.

1. (vLow, LParent) ← MODDIJKSTRA(G, v0, α)
2. return (vLow, LParent)

Algorithm 4: Algorithm COMPVHIGH(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs
vHigh, a complete voltage assignment for G, and an array HParent : V → V ∪ {null}.

if x ∈ T (v0) then v1(x) ← −v0(x) else v1(x) ← v1(x).

1. for x ∈ VG
2.
3. (temp, HParent) ← MODDIJKSTRA(G, v1, α)
4. for x ∈ VG : vHigh(x) ← −temp(x)
5. return (vHigh, HParent)

Corollary B.2 For a well-posed instance (G, V0) and a gradient value α ≥ 0, let (vLow[α], LParent) ← COMPVLOW(G, v0, α)
and (vHigh[α], HParent) ← COMPVHIGH(G, v0, α). Then, vLow[α], vHigh[α] are complete voltage assignments for
G such that, ∀x ∈ VG,

vLow[α](x) = min

{v0(t) + α · dist(x, t)}

t∈T (v0)

vHigh[α](x) = max
t∈T (v0)

{v0(t) − α · dist(t, x)}.

Moreover, the pointer arrays LParent, HParent satisfy ∀x /∈ T (v0), LParent(x), HParent(x) 6= null and

vLow[α](x) = vLow[α](LParent(x)) + α · ℓ(x, LParent(x)),
vHigh[α](x) = vHigh[α](HParent(x)) − α · ℓ(x, HParent(x)).

Algorithm 5: Algorithm COMPINFMIN(G, v0): Given a well-posed instance (G, v0), outputs a complete voltage assignment
v for G, extending v0 that minimizes (cid:13)

(cid:13)grad[v](cid:13)

(cid:13)∞.

1. α ← max{|grad[v0](e)| | e ∈ EG ∩ (T (v0) × T (v0))}.
2. EG ← EG \ (T (v0) × T (v0))
3. P ←STEEPESTPATH(G, v0).
4. α ← max{α, ∇P (v0)}
5. (vLow, LParent) ← COMPVLOW(G, v0, α)
6. (vHigh, HParent) ← COMPVHIGH(G, v0, α)
7. for x ∈ VG
8.
9.
10.
11. return v

then v(x) ← v0(x)
else v(x) ← 1

2 · (vLow(x) + vHigh(x)).

if x ∈ T (v0)

1. (vLow, LParent) ← COMPVLOW(G, v0, α)
2. (vHigh, HParent) ← COMPVHIGH(G, v0, α)
3. VG′ ← {x ∈ VG | vHigh(x) > vLow(x) }
4. EG′ ← {(x, y) ∈ EG | x, y ∈ VG′ }.

19

Algorithm 6: Algorithm COMPHIGHPRESSGRAPH(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0,
outputs a minimal induced subgraph G′ of G where every vertex has pressure[v0](·) > α.

5. G′ ← (V ′, E′, ℓ)
6. return G′

Proof of Lemma 4.3:

is equivalent to

vHigh[α](x) > vLow[α](x)

max
t∈T (v0)

{v0(t) − α · dist(t, x)} > min

{v0(t) + α · dist(x, t)},

t∈T (v0)

which implies that there exists terminals s, t ∈ T (v0) such that

thus,

Hence,

v0(t) − α · dist(t, x) > v0(s) + α · dist(x, s)

pressure[v0](x) ≥

v0(t) − v0(s)
dist(t, x) + dist(x, s)

> α.

v0(t) − v0(s)
dist(t, x) + dist(x, s)

= pressure[v0](x) > α.

v0(t) − α · dist(t, x) > v0(s) + α · dist(x, s)

So the inequality on vHigh and vLow implies that pressure is strictly greater than α. On the other hand, if pressure[v0](x) >
α, there exists terminals s, t ∈ T (v0) such that

which implies vHigh[α](x) > vLow[α](x).

✷

Algorithm 7: Algorithm STEEPESTPATH(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs a steepest
free terminal path P in (G, v0).

P ← VERTEXSTEEPESTPATH(G, v0, xi)

1. Sample uniformly random e ∈ EG. Let e = (x1, x2).
2. Sample uniformly random x3 ∈ VG.
3. for i = 1 to 3
4.
5. Let j ∈ arg maxj∈{1,2,3} ∇Pj (v0)
6. G′ ← COMPHIGHPRESSGRAPH(G, v0, ∇Pj (v0))
7. if EG′ = ∅,
8.
9.

then return Pj
else return STEEPESTPATH(G′, v0|VG′ )

1. while T (v0) 6= VG
2.
3.
4.
5. return v0

EG ← EG \ (T (v0) × T (v0))
P ← STEEPESTPATH(G, v0)
v0 ← ﬁx[v0, P ]

Algorithm 8: Algorithm COMPLEXMIN(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs lexG[v0].

Algorithm 9: Algorithm VERTEXSTEEPESTPATH(G,v0, x): Given a well-posed instance (G, v0), and a vertex x ∈ VG,
outputs a steepest terminal path in (G, v0) through x.

1. Using Dijkstra’s algorithm, compute dist(x, t) for all t ∈ T (v0)

20

y ← arg maxy∈T (v0)
if v0(x) ≥ v0(y)

|v0(x)−v0(y)|
dist(x,y)

then return a shortest path from x to y
else return a shortest path from y to x

2. if x ∈ T (v0)
3.
4.
5.
6.
7. else
8.
9.
10.
11.

for t /∈ T (v0), d(t) ← dist(x, t)
(t1, t2) ← STARSTEEPESTPATH(T (v0), v0|T (v0), d)
Let P1 be a shortest path from t1 to x. Let P2 be a shortest path from x to t2.
P ← (P1, P2). return P.

Algorithm 10: STARSTEEPESTPATH(T, v, d): Returns the steepest path in a star graph, with a single non-terminal connected
to terminals in T, with lengths given by d, and voltages given by v.

|v(t1)−v(t)|
d(t1)+d(t)

1. Sample t1 uniformly and randomly from T
2. Compute t2 ∈ arg maxt∈T
3. α ← |v(t2)−v(t1)|
d(t1)+d(t2)
4. Compute vlow ← mint∈T (v(t) + α · d(t))
5. Tlow ← {t ∈ T | v(t) > vlow + α · d(t)}
6. Compute vhigh ← maxt∈T (v(t) − α · d(t))
7. Thigh ← {t ∈ T | v(t) < vhigh − α · d(t)}
8. T ′ ← Tlow ∪ Thigh.
9. if T ′ = ∅
10.
11.

then if v(t1) ≥ v(t2) then return (t1, t2) else return (t2, t1)
else return STARSTEEPESTPATH(T ′, v|T ′, dT ′ )

B.1 Faster Lex-minimization

Algorithm 11: Algorithm COMPFASTLEXMIN(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs
lexG[v0].

1. while T (v0) 6= VG
2.
3. return v0

v0 ← FIXPATHSABOVEPRESS(G, v0, 0)

Algorithm 12: Algorithm FIXPATHSABOVEPRESS(G, v0, α): Given a well-posed instance (G, v0), with T (v0) 6= VG, and
a gradient value α, iteratively ﬁxes all paths with gradient > α.

EG ← EG \ (T (v0) × T (v0))
Sample uniformly random e ∈ EG. Let e = (x1, x2).
Sample uniformly random x3 ∈ VG.
for i = 1 to 3

Pi ← VERTEXSTEEPESTPATH(G, v0, xi)

Let j ∈ arg maxj∈{1,2,3} ∇Pj(v0)
G′ ← COMPHIGHPRESSGRAPH(G, v0, ∇Pj (v0))
if EG′ = ∅,

1. while T (v0) 6= VG
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.

then v0 ← ﬁx[v0, P ]
else Let G′

for i = 1, . . . , r

i, i = 1, . . . , r be the connected components of G′.

21

vi ← FIXPATHSABOVEPRESS(G′
for x ∈ VG′

i, set v0(x) ← vi(x)

i, v0|VG′

i

, ∇Pj (v0))

if α > 0 then G ←COMPHIGHPRESSGRAPH(G, v0, α)

13.
14.
15.
16. return v0

C Experiments on WebSpam: Testing More Algorithms

For completeness, in this appendix we show how a number of algorithms perform on the web spam experiment of
Section 6. We consider the following algorithms:

• RANDWALK along in-links. For a detailed description see Zhou et al. (2007). This algorithm essentially per-
forms a Personalized PageRank random walk from each vertex x and computes a spam-value for the vertex x by
taking a weighted average of the labels of the vertices where the random walk from x terminates. Also shown in
Section 6.

• DIRECTEDLEX, with edges in the opposite directions of links. This has the effect that a link to a spam host is

evidence of spam, and a link from a normal host is evidence of normality. Also shown in Section 6.

• RANDWALK along out-links.

• DIRECTEDLEX, with edges in the directions of links. This has the effect that a link from to a spam host is

evidence of spam, and a link to a normal host is evidence of normality.

• UNDIRECTEDLEX: Lex-minimization with links treated as undirected edges.

• LAPLACIAN: l2-regression with links treated as undirected edges.

• DIRECTED 1-NEAREST NEIGHBOR: Uses shortest distance along paths following out-links. Spam-ratio is
deﬁned distance from normal hosts, divided by distance to spam hosts. Sites are ﬂagged as spam when spam-
ratio exceeds some threshold. We also tried following paths along in-links instead, but that gave much worse
results.

We use the experimental setup described in Section 6. Results are shown in Figure 4. The alternative convention
for DIRECTEDLEX orients edges in the directions of links. This takes a link from a spam host to be evidence of
spam, and a link to a normal host to be evidence of normality. This approach performs signiﬁcantly worse than our
preferred convention, as one would intuitively expect. UNDIRECTEDLEX and LAPLACIAN approaches also perform
signiﬁcantly worse. DIRECTED 1-NEAREST NEIGHBOR performs poorly, demonstrating that DIRECTEDLEX is very
different from that approach. As observed by Zhou et al. (2007), sampling based on a random walk following out-links
performs worse than following in-links. Up to 60 % recall, DIRECTEDLEX performs best, both in the regime of 5 %
labels for training and in the regime of 20 % labels for training.

22

5 % labels for training

RandWalk along in-links
DirectedLex
RandWalk along out-links
DirectedLex, opposite link direction
UndirectedLex
Laplacian
Directed 1NN

0
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Recall

20 % labels for training

RandWalk along in-links
DirectedLex
RandWalk along out-links
DirectedLex, opposite link direction
UndirectedLex
Laplacian
Directed 1NN

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

i

i

n
o
s
c
e
r
P

i

i

n
o
s
c
e
r
P

0
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Recall

Figure 4: Recall and precision in the WebSpam classiﬁcation experiment. Each data point shown was computed as an average
over 100 runs. The largest standard deviation of the mean precision across the plotted recall values was less than 1.5 %. The
algorithm of Zhou et al. (2007) appears as RANDWALK (along in-links). We also show RANDWALK along out-links. Our directed
lex-minimization algorithm appears as DIRECTEDLEX. We also show DIRECTEDLEX with link directions reversed, along with
UNDIRECTEDLEX and LAPLACIAN.

D l0-Vertex Regularization Proofs

In this appendix, we prove Theorem 7.1 and Theorem 7.2. For the purposes of proving the second theorem, we intro-
duce an alternative version of problem (3). The optimization problem here requires us to minimize l0-regularization

23

budget required to obtain an inf-minimizer with gradient below a given threshold:

min
v∈IRn
subject to

(cid:13)
(cid:13)

v(T ) − v0(T )

0

gradG[v]

(cid:13)
∞ ≤ α.
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We will also need the following graph construction.

Deﬁnition D.1 The α-pressure terminal graph of a partially-labeled graph (G, v0) is a directed unweighted graph
Gα = (T (v0),

E if and only if there is a terminal path P from s to t in G with

E) such that (s, t) ∈

b

b

∇P (v0) > α.

Note that the α-pressure terminal graph has O(n) vertices but may be dense, even when G is not.

Algorithm 13: Algorithm TERM-PRESSURE: Given a well-posed instance (G, v0) and α ≥ 0, outputs α pressure terminal
graph Gα.
Initialize Gα with vertex set Vα = T (v0) and edge set
for each terminal s ∈ T (v0)

E = ∅.

1. Compute the distances to every other terminal t by running Dijktra’s algorithm, allowing shortest paths

b

2. Use the resulting distances to check for every other terminal t if there is a terminal path P from s to t with

that run through other terminals.

∇P (v0) > α. If there is, add edge (s, t) to

E.

Lemma D.2 The α-pressure terminal graph of a voltage problem (G, v0) can be computed in O((m + n log n)n) time
using algorithm TERM-PRESSURE (Algorithm 13).

b

Proof: The correctness of the algorithm follows from the fact that Dijkstra’s algorithm will identify all shortest
distances between the terminals, and the pressure check will ensure that terminal pairs (s, t) are added to
E if and
only if they are the endpoints of a terminal path P with ∇P (v0) > α. The running time is dominated by performing
Dijkstra’s algorithm once for each terminal. A single run of Dijkstra’s algorithm takes O(m + n log n) time, and this
✷
is performed at most n times, for a total running time of O((m + n log n)n).

b

We make three observations that will turn out to be crucial for proving Theorems 7.1 and 7.2.

Observation D.3 Gα is a subgraph of Gβ for α ≥ β.

Proof: Suppose edge (s, t) appears in Gα, then for some path P

∇P (v0) > α ≥ β,

so the edge also appears in Gβ.

Observation D.4 Gα is transitively closed.

Proof: Suppose edges (s, t) and (t, r) appear in Gα. Let P(s,t), P(t,r), P(s,r) be the respective shortest paths in G
between these terminal pairs. Then

∇P(s,r)(v0) =

v0(s) − v0(r)
ℓ(P(s,r))

≥

v0(s) − v0(r)
ℓ(P(s,t)) + ℓ(P(t,r))

=

v0(s) − v0(t) + v0(t) − v0(r)
ℓ(P(s,t)) + ℓ(P(t,r))

≥ min

v0(s) − v0(t)
ℓ(P(s,t))

,

 

v0(t) − v0(r)

ℓ(P(t,r)) !

> α.

So edge (s, r) also appears in Gα. This is sufﬁcient for Gα to be transitively closed.

24

(6)

✷

(7)

✷

Observation D.5 Gα is a directed acyclic graph.

Proof: Suppose for a contradiction that a directed cycle appears in Gα. Let s and t be two vertices in this cycle. Let
P(s,t) and P(t,s) be the respective shortest paths in G between these terminal pairs. Because Gα is transitively closed,
both edges (s, t) and (t, s) must appear in Gα. But (s, t) ∈

E implies

and similarly (t, s) ∈

E implies

b
This is a contradiction.

v0(s) − v0(t) > αℓ(P(s,t)) > 0,

b

v0(t) − v0(s) > αℓ(P(t,s)) > 0.

✷

The usefulness of the α-pressure terminal graph is captured in the following lemma. We deﬁne a vertex cover of a
directed graph to be a vertex set that constitutes a vertex cover in the same graph with all edges taken to be undirected.

Lemma D.6 Given a partially-labeled graph (G, v0) and a set U ⊆ V , there exists a voltage assignment v ∈ IRn that
satisﬁes

if and only if U is a vertex cover in the α-pressure terminal graph Gα of (G, v0).
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:8)

(cid:9)

t ∈ T (v0) : v(t) 6= v0(t)

⊆ U and

gradG[v]

∞ ≤ α,

Proof: We ﬁrst show the “only if” direction. Suppose for a contradiction that there exists a voltage assignment v for
which
∞ ≤ α, but U is not a vertex cover in Gα. Let (s, t) be an edge Gα which is not covered by U . The
presence of this edge in Gα implies that there exists a terminal path P from s to t in G for which

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∇P (v0) > α.

But, by Lemma 3.5 this means there is no assignment v for G which agrees with v0 on s and t and has
α. This contradicts our assumption.

∞ ≤
(cid:13)
Now we show the “if” direction. Consider an arbitrary vertex cover U of Gα. Suppose for a contradiction that
(cid:13)
⊆ U .

t ∈ T (v0) : v(t) 6= v0(t)

gradG[v]

(cid:13)
(cid:13)

gradG[v]

there does not exist a voltage assignment v for G with
Deﬁne a partial voltage assignment vU given by

∞ ≤ α and

(cid:8)

(cid:9)

vU (t) =

v0(t)
∗

(

(cid:13)
(cid:13)

(cid:13)
(cid:13)
if t ∈ T (v0) \ U
o.w.

∞ ≤ α. By
The preceding statement is equivalent to saying that there is no v that extends vU and has
Lemma 3.5, this means there is terminal path between s, t ∈ T (vU ) with gradient strictly larger than α. But this
means an edge (s, t) is present in Gα and is not covered. This contradicts our assumption that U is a vertex cover. ✷

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We are now ready to prove Theorem 7.2.

∞

(cid:13)
(cid:13)

Proof of Theorem 7.2: We describe and prove the algorithm OUTLIER. The algorithm will reduce problem (3)
to problem (6): Suppose v∗ is an optimal assignment for problem (3).
It achieves a maximum gradient α∗ =
gradG[v∗]
. Using Dijkstra’s algorithm we compute the pairwise shortest distances between all terminals in G.
From these distances and the terminal voltages, we compute the gradient on the shortest path between each terminal
(cid:13)
pair. By Lemma 3.5, α∗ must equal one of these gradients. So we can solve problem (3) by iterating over the set of
(cid:13)
gradients between terminals and solving problem (6) for each of these O(n2) gradients. Among the assignments with
v(T ) − v0(T )

0 ≤ k, we then pick the solution that minimizes
(cid:13)
(cid:13)

In fact, we can do better. By Observation D.3, Gα is a subgraph of Gβ for α ≥ β. This means a vertex cover
(cid:13)
of Gα is also a vertex cover of Gβ, and hence the minimum vertex cover for Gβ is at least as large as the minimum
(cid:13)
vertex cover for Gα. This means we can do a binary search on the set of O(n2) terminal gradients to ﬁnd the minimum
gradient for which there exists an assignment with
0 ≤ k. This way, we only make O(log n) calls to
v(T ) − v0(T )
problem (6), in order to solve problem (3).
(cid:13)
(cid:13)

We use the following algorithm to solve problem (6).

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∞

.

25

1. Compute the α-pressure terminal graph Gα of G using the algorithm TERM-PRESSURE.
2. Compute a minimum vertex cover U of Gα using the algorithm KONIG-COVER from Theorem 7.3.
3. Deﬁne a partial voltage assignment vU given by

vU (t) =

v0(t)
∗

(

if t ∈ T (v0) \ U,
otherwise.

4. Using Algorithm 5, compute voltages v that extend vU and output v.

From Lemma D.2, it follows that step 1 computes the α-pressure terminal graph in polynomial time. From The-
orem 7.3 it follows that step 2 computes the a minimum vertex cover of the α-pressure terminal graph in polynomial
time, because our observations D.4 and D.5 establish that the graph is a TC-DAG. From Lemma D.6 and Theorem 4.6,
it follows that the output voltages solve program (6).

✷

To prove Theorem 7.1, we use the standard greedy approximation algorithm for MIN-VC (Vazirani (2001)).

Theorem D.7 2-Approximation Algorithm for Vertex Cover. The following algorithm gives a 2-approximation to
the Minimum Vertex Cover problem on a graph G = (V, E).

0. Initialize U = ∅.
1. Pick an edge (u, v) ∈ E that is not covered by U .
2. Add u and v to the set U .
3. Repeat from step 1 if there are still edges not covered by U .
4. Output U .

We are now in a position to prove Theorem 7.1

Proof of Theorem 7.1: Given an arbitrary k and a partially-labeled graph (G, v0), let α∗ be the optimum value
of program (3). Observe that by Lemma D.6, this implies that Gα∗ has a vertex cover of size k. Given the partial
assignment v0, for every vertex set U , we deﬁne

vU (t) =

v0(t)
∗

(

if t ∈ T (v0) \ U
o.w.

We claim the following algorithm APPROX-OUTLIER outputs a voltage assignment v with

gradG[v]

∞ ≤ α∗

(cid:13)
(cid:13)

(cid:13)
(cid:13)

and

v(T ) − v0(T )

(cid:13)
(cid:13)

Algorithm APPROX-OUTLIER:

0 ≤ 2k.
(cid:13)
(cid:13)

0. Initialize U = ∅.
1. Using the algorithm STEEPESTPATH (Algorithm 7), ﬁnd a steepest terminal path in G w.r.t. vU . Denote
this path P and let s and t be its terminal endpoints. If there is no terminal path with positive gradient, skip
to step 4.

2. Add s and t to the set U .
3. If |U | ≤ 2k − 2 then repeat from step 1.
4. Using the algorithm COMPINFMIN (Algorithm 5), compute voltages v that extend vU and output v.

From the stopping conditions, it is clear that |U | ≤ 2k. If in step 1 we ever ﬁnd that no terminal paths have positive
∞ = 0 ≤ α∗, by Lemma 3.5. Similarly if we ﬁnd a steepest
gradient then our v that extends vU will have
(cid:13)
(cid:13)

gradG[v]

(cid:13)
(cid:13)

26

gradG[v]

∞ ≤ α∗.

∞ ≤ α∗.
path with gradient less than α∗ w.r.t. vU , then for this U there exists v that extends vU and has
This will continue to hold when if we add vertices to U . Therefore, for the ﬁnal U , there will exist an v that extends
vU and has

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

If we never ﬁnd a steepest terminal path P with ∇P (v0) ≤ α∗, then each steepest path we ﬁnd corresponds to an
edge in Gα∗ that is not yet covered by U and our algorithm in fact implements the greedy approximation algorithm
for vertex cover described in Theorem D.7. This implies that the ﬁnal U is a vertex cover of Gα∗ of size at most 2k.
∞ ≤ α∗. This
By Lemma D.6, this implies that there exists a voltage assignment u extending vU that has
implies by Theorem 4.6 that the v we output has
(cid:13)
(cid:13)
In all cases, the v we output extends vU , so

∞ ≤ α∗.

gradG[u]

(cid:13)
(cid:13)

✷

gradG[v]
v(T ) − v0(T )
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

0 ≤ |U | ≤ 2k.
(cid:13)
(cid:13)

E Proof of Hardness of l0 regularization for l2

We will prove Theorem 7.4, by a reduction from minimum bisection. To this end, let G = (V, E) be any graph. We
will reduce the minimum bisection problem on G to our regularization problem. Let n = |V |. The graph on which we
will perform regularization will have vertex set

V ∪

V ,

V is a set of n vertices that are in 1-to-1 correspondence with V . We assume that every edge in G has weight 1.
V to the corresponding vertex in V by an edge of weight B, for some large B to be
V to each other by edges of weight B3. So, we have a complete
V to V , and the original graph G on V .

where
We now connect every vertex in
determined later. We also connect all of the vertices in
graph of weight B3 edges on
b
The input potential function will be

V , a matching of weight B edges connecting

b

b

b

v(a) =

b
0 for a ∈
1 for a ∈ V .
b

(

V , and

b

Now set k = n/2. We claim that we will be able to determine the value of the minimum bisection from the solution
to the regularization problem.

If S is the set of vertices on which v and w differ, then we know that the w is harmonic on S: for every a ∈ S,

w(a) is the weighted average of the values at its neighbors. In the following, we exploit the fact that |S| ≤ n/2.

Claim E.1 For every a ∈ S ∩

V , w(a) ≤ 2/nB2.

Proof: Let a be the vertex in S ∩
w-value equal to 0 by edges of weight B3. On the other hand, a has only one neighbor that is not in
w-value at most 1, and it is connected to that vertex by an edge of weight B. Call that vertex c. We have

V that maximizes w(a). So, a is connected to at least n/2 neighbors in

V with
V , that vertex has

b

b

b

((n − 1)B3 + B)w(a) = Bw(c) +

B3w(b)

b

b
V ,b6=a
Xb∈

= Bw(c) +

B3w(b) +

B3w(b)

b
V ∩S,b6=a
Xb∈

B3w(a)

≤ B +

b
V ∩S,b6=a
Xb∈
≤ B + (n/2 − 1)B3w(a).

b
V −S
Xb∈

Subtracting (n/2 − 1)B3w(a) from both sides gives

((n/2)B3 + B)w(a) ≤ B,

which implies the claim.

Claim E.2 For a ∈ S ∩ V , w(a) ≤ n/B.

27

✷

V . Let’s call that neighbor c. We know that w(c) ≤ 2/B2n. On the
Proof: Vertex a has exactly one neighbor in
other hand, vertex a has fewer than n − 1 neighbors in V , and each of these have w-value at most 1. Let da denote the
degree of a in G. Then,

b

So,

Let

and

bisection.

and at most

(B + da)w(a) ≤ da + B

2
B2n

.

w(a) ≤

da + 2/Bn
da + B
n + (2/Bn)
B + n

≤

≤ n/B.

|S| = k = n/2.

T = S ∩ V,

t = |T | .

(n − t)B − 4/B
b

(n − t)B + tn2/B.

We now estimate the value of the regularized objective function. To this end, we assume that

We will prove that S ⊂ V and so S = T and t = n/2.

Let δ denote the number of edges on the boundary of T in V . Once we know that t = n/2, δ is the size of a

Claim E.3 The contribution of the edges between V and

V to the objective function is at least

Proof: For the lower bound, we just count the edges between vertices in V \ T and
edges, and each of them has weight B. The endpoint in V \ T has w-value 1, and the endpoint in
most 2/nB2. So, the contribution of these edges is at least

V . There are n − t of these
V has w-value at

b

(n − t)B(1 − 2/nB2)2 ≥ (n − t)B(1 − 4/nB2) ≥ (n − t)B − 4/B.

b

For the upper bound, we observe that the difference in w-values across each of these n − t edges is at most 1, so their
total contribution is at most

Since for every vertex a ∈ T , w(a) ≤ n/B, and also every vertex b ∈
edges between T and

V is at most

t(n/B)2B = tn2/B.

b

b

V , w(b) ≤ 2/nB2, the contribution due to

We will see that this is the dominant term in the objective function. The next-most important term comes from the

edges in G.

(n − t)B.

28

✷

✷

Claim E.4 The contribution of the edges in G to the objective function is at least

and at most

δ(1 − 2n/B)

δ + (t2/2)(n/B)2

δ(1 − 2n/B) and δ.

(t2/2)(n/B)2.

Proof: Let (a, b) ∈ E. If neither a nor b is in T , then w(a) = w(b) = 1, and so this edge has no contribution. If
a ∈ T but b 6∈ T , then the difference in w-values on them is between (1 − n/B) and 1. So, the contribution of such
edges to the objective function is between

Finally, if a and b are in T , then the difference in w-values on them is at most n/B, and so the contribution of all such
edges to the objective function is at most

Claim E.5 The edges between pairs of vertices in

V contribute at most 2/B to the objective function.

Proof: As 0 ≤ w(a) ≤ 2/B2n for every a ∈

V , every edge between two vertices in

V can contribute at most

b

As there are fewer than n2/2 such edges, their total contribution to the objective function is at most

B3(2/B2n)2 = 4/Bn2.
b

b

(n2/2)(4/Bn2) = 2/B.

Lemma E.6 If n ≥ 4 and B = 2n3, the value of the objective function is at least

and at most

(n − t)B + δ − 1/2

(n − t)B + δ + 1/3.

Proof: Summing the contributions in the preceding three claims, we see that the value of the objective function is at
least

(n − t)B − 4/B + δ(1 − 2n/B) ≥ (n − t)B + δ − 4/B − 2nδ/B

≥ (n − t)B + δ − n3/B
≥ (n − t)B + δ − 1/2,

as δ ≤ (n/2)2.

Similarly, the objective function is at most

(n − t)B + tn2/B + δ + (t2/2)(n/B)2 + 2/B ≤ (n − t)B + n3/2B + δ + n4/8B2 + 2/B
≤ (n − t)B + n3/2B + δ + 1/32n2 + 1/n3
≤ (n − t)B + δ + 1/3.

Claim E.7 If n ≥ 2 and B = 2n3, then S ⊂ V .

Proof: The objective function is minimized by making t as large as possible, so t = n/2 and S ⊂ V .

29

✷

✷

✷

✷

Theorem E.8 The value of the objective function reveals the value of the minimum bisection in G.

Proof: The value of the objective function will be between

and

(n/2)B + δ − 1/2

(n/2)B + δ + 1/3.

So, the objective function will be smallest when δ is as small as possible.

✷

Theorem E.8 immediately implies Theorem 7.4.

30

5
1
0
2
 
n
u
J
 
0
3
 
 
]

G
L
.
s
c
[
 
 
2
v
0
9
2
0
0
.
5
0
5
1
:
v
i
X
r
a

Algorithms for Lipschitz Learning on Graphs ∗†

Rasmus Kyng
Yale University
rasmus.kyng@yale.edu

Anup Rao
Yale University
anup.rao@yale.edu

Sushant Sachdeva
Yale University
sachdeva@cs.yale.edu

Daniel A. Spielman
Yale University
spielman@cs.yale.edu

July 1, 2015

Abstract

We develop fast algorithms for solving regression problems on graphs where one is given the value of a function
at some vertices, and must ﬁnd its smoothest possible extension to all vertices. The extension we compute is the
absolutely minimal Lipschitz extension, and is the limit for large p of p-Laplacian regularization. We present an
algorithm that computes a minimal Lipschitz extension in expected linear time, and an algorithm that computes
an absolutely minimal Lipschitz extension in expected time eO(mn). The latter algorithm has variants that seem
to run much faster in practice. These extensions are particularly amenable to regularization: we can perform l0-
regularization on the given values in polynomial time and l1-regularization on the initial function values and on graph
edge weights in time eO(m3/2).

Our deﬁnitions and algorithms naturally extend to directed graphs.

1 Introduction

We consider a problem in which we are given a weighted undirected graph G = (V, E, ℓ) and values v0 : T → R
on a subset T of its vertices. We view the weights ℓ as indicating the lengths of edges, with shorter length indicating
greater similarity. Our goal it to assign values to every vertex v ∈ V \T so that the values assigned are as smooth as
possible across edges. A minimal Lipschitz extension of v0 is a vector v that minimizes

max
(x,y)∈E

(ℓ(x, y))−1

v(x) − v(y)

,

(cid:12)
(cid:12)

(cid:12)
(cid:12)

subject to v(x) = v0(x) for all x ∈ T . We call such a vector an inf-minimizer. Inf-minimizers are not unique. So,
among inf-minimizers we seek vectors that minimize the second-largest absolute value of ℓ(x, y)−1
v(x) − v(y)
across edges, and then the third-largest given that, and so on. We call such a vector v a lex-minimizer. It is also known
(cid:12)
as an absolutely minimal Lipschitz extension of v0.
(cid:12)
These are the limit of the solution to p-Laplacian minimization problems for large p, namely the vectors that solve

(cid:12)
(cid:12)

(1)

(2)

min
v∈Rn

v|T =v0|T X(x,y)∈E

(ℓ(x, y))−p|v(x) − v(y)|p.

The use of p = 2 was suggested in the foundational paper of Zhu et al. (2003), and is particularly nice because it can
be obtained by solving a system of linear equations in a symmetric diagonally dominant matrix, which can be done

∗This research was partially supported by AFOSR Award FA9550-12-1-0175, NSF grant CCF-1111257, a Simons Investigator Award to Daniel

Spielman, and a MacArthur Fellowship.

†Code used in this work is available at https://github.com/danspielman/YINSlex

1

very quickly (Cohen et al. (2014)). The use of larger values of p has been discussed by Alamgir and Luxburg (2011),
and by Bridle and Zhu (2013), but it is much more complicated to compute. The fastest algorithms we know for this
problem require convex programming, and then require very high accuracy to obtain the values at most vertices. By
taking the limit as p goes to inﬁnity, we recover the lex-minimizer, which we will show can be computed quickly.

The lex-minimization problem has a remarkable amount of structure. For example, in uniformly weighted graphs
the value of the lex-minimizer at every vertex not in T is equal to the average of the minimum and maximum of the
values at its neighbors. This is analogous to the property of the 2-Laplacian minimizer that the value at every vertex
not in T equals the average of the values at its neighbors.

1.1 Contributions

We ﬁrst present several important structural properties of lex-minimizers in Section 3.2. As we shall point out, some
of these were known from previous work, sometimes in restricted settings. We state them generally and prove them
for completeness. We also prove that the lex-minimizer is as stable as possible under perturbations of v0 (Section 3.1).
The structure of the lex-minimization problem has led us to develop elegant algorithms for its solution. Both the
algorithms and their analyses could be taught to undergraduates. We believe that these algorithms could be used in
place of 2-Laplacian minimization in many applications.

We present algorithms for the following problems. Throughout, m = |E| and n = |V |.

Inf-minimization: An algorithm that runs in expected time O(m + n log n) (Section 4.3).

Lex-minimization: An algorithm that runs in expected time O(n(m + n log n)) (Section 4), along with a variant that

runs quickly in practice (Section 4.4).

l1-regularization of edge lengths for inf-minimization: The problem of minimizing (1) given a limited budget with
O(m3/2)
which one can increase edge lengths is a linear programming problem. We show how to solve it in time
with an interior point method by using fast Laplacian solvers (Section 8). The same algorithm can accommodate
l1-regularization of the values given in v0.

e

l0-regularization of vertex values for inf-minimization: We give a polynomial time algorithm for l0-regularization
of the values at vertices. That is, we minimize (1) given a budget of a number of vertices that can be proclaimed
outliers and removed from T (Section 7.1). We solve this problem by reducing it to the problem of computing
minimum vertex covers on transitively closed directed acyclic graphs, a special case of minimum vertex cover
that can be solved in polynomial time.

After any regularization for inf-minimization, we suggest computing the lex-minimizer. We ﬁnd the result for l0-
regularization of vertex values to be particularly surprising, especially because we prove that the analogous problem
for 2-Laplacian minimization is NP-Hard (Section 7.2).

All of our algorithms extend naturally to directed graphs (Section 5). This is in contrast with the problem of
minimizing 2-Laplacians on directed graphs, which corresponds to computing electrical ﬂows in networks of resistors
and diodes, for which fast algorithms are not presently known.

We present a few experiments on examples demonstrating that the lex-minimizer can overcome known deﬁcien-
cies of the 2-Laplacian minimizer (Section 1.2, Figures 1,2), as well as a demonstration of the performance of the
directed analog of our algorithms on the WebSpam dataset of Castillo et al. (2006) (Section 6). In the WebSpam prob-
lem we use the link structure of a collection of web sites to ﬂag some sites as spam, given a small number of labeled
sites known to be spam or normal.

1.2 Relation to Prior Work

We ﬁrst encountered the idea of using the minimizer of the 2-Laplacian given by (2) for regression and classiﬁca-
tion on graphs in the work of Zhu et al. (2003) and Belkin et al. (2004) on semi-supervised learning. These works
transformed learning problems on sets of vectors into problems on graphs by identifying vectors with vertices and
constructing graphs with edges between nearby vectors. One shortcoming of this approach (see Nadler et al. (2009),

2

e
g
a

t
l

 

o
V
d
e
r
r
e

f

n

I

1

0.8

0.6

0.4

0.2

0

-0.2

-0.4

-0.6

-0.8

-1

-4

50 lex
50 l2
100 lex
100 l2
500 lex
500 l2
1000 lex
1000 l2

0.25

0.2

r
o
r
r
e
 
1
l
 
n
a
e
M

0.15

0.1

0.05

0
5000 

lex
2-Lap
labels

-2

0

2
Vertex position on real line

4

6

8

Figure 1: Lex vs 2-Laplacian on 1D gaussian clus-
ters.

Figure 2: kNN graphs on samples from 4D cube.

10000

20000

40000

80000

Number of Vertices

Alamgir and Luxburg (2011), Bridle and Zhu (2013)) is that if the number of vectors grows while the number of la-
beled vectors remains ﬁxed, then almost all the values of the 2-Laplacian minimizer converge to the mean of the
labels on most natural examples. For example, Nadler et al. (2009) consider sampling points from two Gaussian
distributions centered at 0 and 4 on the real line. They place edges between every pair of points (x, y) with length
exp(|x − y|2 /2σ2) for σ = 0.4, and provide only the labels v0(0) = −1 and v0(4) = 1. Figure 1 shows the values
of the 2-Laplacian minimizer in red, which are all approximately zero. In contrast, the values of the lex-minimizer in
blue, which are smoothly distributed between the labeled points, are shown.

The “manifold hypothesis” (see Chapelle et al. (2010), Ma and Fu (2011)) holds that much natural data lies near a
low-dimensional manifold and that natural functions we would like to learn on this data are smooth functions on the
manifold. Under this assumption, one should expect lex-minimizers to interpolate well. In contrast, the 2-Laplacian
minimizers degrade (dotted lines) if the number of labeled points remains ﬁxed while the total number of points grows.
In Figure 2, we demonstrate this by sampling many points uniformly from the unit cube in 4 dimensions, form their
8-nearest neighbor graph, and consider the problem of regressing the ﬁrst coordinate. We performed 8 experiments,
varying the number of labeled points in {50, 100, 500, 1000}. Each data point is the mean average l1 error over 100
experiments. The plots for root mean squared error are similar. The standard deviation of the estimations of the mean
are within one pixel, and so are not displayed. The performance of the lex-minimizer (solid lines) does not degrade as
the number of unlabeled points grows.

Analogous to our inf-minimizers, minimal Lipschitz extensions of functions in Euclidean space and over more
general metric spaces have been studied extensively in Mathematics (Kirszbraun (1934), McShane (1934), Whitney
(1934)). von Luxburg and Bousquet (2003) employ Lipschitz extensions on metric spaces for classiﬁcation and relate
these to Support Vector Machines. Their work inspired improvements in classiﬁcation and regression in metric spaces
with low doubling dimension (Gottlieb et al. (2013), Gottlieb et al. (2013b)). Theoretically fast, although not actually
practical, algorithms have been given for constructing minimal Lipschitz extensions of functions on low-dimensional
Euclidean spaces (Fefferman (2009a), Fefferman and Klartag (2009), Fefferman (2009b)). Sinop and Grady (2007)
suggest using inf-minimizers for binary classiﬁcation problems on graphs. For this special case, where all of the
given values are either 0 or 1, they present an O(m + n log n) time algorithm for computing an inf-minimizer. The
case of general given values, which we solve in this paper, is much more complicated. To compensate for the non-
uniqueness of inf-minimizers, they suggest choosing the inf-minimizer that minimizes (2) with p = 2. We believe that
the lex-minimizer is a more natural choice.

The analog of our lex-minimizer over continuous spaces is called the absolutely minimal Lipschitz extension
(AMLE). Starting with the work of Aronsson (1967), there have been several characterizations and proofs of the ex-
istence and uniqueness of the AMLE (Jensen (1993), Crandall et al. (2001), Barles and Busca (2001), Aronsson et al.
(2004)). Many of these results were later extended to general metric spaces, including graphs (Milman (1999),
Peres et al. (2011), Naor and Shefﬁeld (2010), Shefﬁeld and Smart (2010)). However, to the best of our knowledge,
fast algorithms for computing lex-minimizers on graphs were not known. For the special case of undirected, un-
weighted graphs, Lazarus et al. (1999) presented both a polynomial-time algorithm and an iterative method. Oberman

3

(2011) suggested computing the AMLE in Euclidean space by ﬁrst discretizing the problem and then solving the cor-
responding graph problem by an iterative method. However, no run-time guarantees were obtained for either iterative
method.

2 Notation and Basic Deﬁnitions

Lexicographic Ordering. Given a vector r ∈ Rm, let πr denote a permutation that sorts r in non-increasing order
by absolute value, i.e., ∀i ∈ [m − 1], |r(πr(i))| ≥ |r(πr(i + 1))|. Given two vectors r, s ∈ Rm, we write r (cid:22) s to
indicate that r is smaller than s in the lexicographic ordering on sorted absolute values, i.e.

∃j ∈ [m],

r(πr(j))

<

s(πs(j))

and ∀i ∈ [j − 1],

r(πr(i))

=

s(πs(i))

or ∀i ∈ [m],

=

r(πr(i))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.
(cid:12)
(cid:12)

s(πs(i))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

Note that it is possible that r (cid:22) s and s (cid:22) r while r 6= s. It is a total relation: for every r and s at least one of r (cid:22) s
or s (cid:22) r is true.

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Graphs and Matrices. We will work with weighted graphs. Unless explicitly stated, we will assume that they are
undirected. For a graph G, we let VG be its set of vertices, EG be its set of edges, and ℓG : EG → R+ be the
assignment of positive lengths to the edges. We let |VG| = n, and |EG| = m. We assume ℓG is symmetric, i.e.,
ℓG(x, y) = ℓG(y, x). When G is clear from the context, we drop the subscript.

A path P in G is an ordered sequence of (not necessarily distinct) vertices P = (x0, x1, . . . , xk), such that
(xi−1, xi) ∈ E for i ∈ [k]. The endpoints of P are denoted by ∂0P = x0, ∂1P = xk. The set of interior vertices
of P is deﬁned to be int(P ) = {xi : 0 < i < k}. For 0 ≤ i < j ≤ k, we use the notation P [xi : xj] to denote the
k
subpath (xi, . . . , xj). The length of P is ℓ(P ) =
i=1 ℓ(xi−1, xi).
A function v0 : V → R ∪ {∗} is called a voltage assignment (to G). A vertex x ∈ V is a terminal with
respect to v0 iff v0(x) 6= ∗. The other vertices, for which v0(x) = ∗, are non-terminals. We let T (v0) denote the
set of terminals with respect to v0. If T (v0) = V, we call v0 a complete voltage assignment (to G). We say that an
assignment v : V → R ∪ {∗} extends v0 if v(x) = v0(x) for all x such that v0(x) 6= ∗.

Given an assignment v0 : V → R ∪ {∗}, and two terminals x, y ∈ T (v0) for which (x, y) ∈ E, we deﬁne the

P

gradient on (x, y) due to v0 to be

gradG[v0](x, y) =

v0(x) − v0(y)
ℓ(x, y)

.

It may be useful to view gradG[v0](x, y) as the current in the edge (x, y) induced by voltages v0. When v0 is a
complete voltage assignment, we interpret gradG[v0] as a vector in Rm, with one entry for each edge. However, for
convenience, we deﬁne gradG[v0](x, y) = −gradG[v0](y, x). When G is clear from the context, we drop the subscript.
A graph G along with a voltage assignment v to G is called a partially-labeled graph, denoted (G, v). We say
that a partially-labeled graph (G, v0) is a well-posed instance if for every maximal connected component H of G, we
have T (v0) ∩ VH 6= ∅.

A path P in a partially-labeled graph (G, v0) is called a terminal path if both endpoints are terminals. We deﬁne

∇P (v0) to be its gradient:

∇P (v0) =

v0(∂0P ) − v0(∂1P )
ℓ(P )

.

If P contains no terminal-terminal edges (and hence, contains at least one non-terminal), it is a free terminal path.

Lex-Minimization. An instance of the LEX-MINIMIZATION problem is described by a partially-labeled graph
(G, v0). The objective is to compute a complete voltage assignment v : VG → R extending v0 that lex-minimizes
grad[v].

Deﬁnition 2.1 (Lex-minimizer) Given a partially-labeled graph (G, v0), we deﬁne lexG[v0] to be a complete voltage
assignment to V that extends v0, and such that for every other complete assignment v′ : VG → R that extends v0, we
have gradG[lexG[v0]] (cid:22) gradG[v′]. That is, lexG[v0] achieves a lexicographically-minimal gradient assignment to the
edges.

We call lexG[v0] the lex-minimizer for (G, v0). Note that if T (v0) = VG, then trivially, lexG[v0] = v0.

4

3 Basic Properties of Lex-Minimizers

Lazarus et al. (1999) established that lex-minimizers in unweighted and undirected graphs exist, are unique, and may
be computed by an elementary meta-algorithm. We state and prove these facts for undirected weighted graphs, and
defer the discussion of the directed case to Section 5. We also state for directed and weighted graphs characterizations
of lex-minimizers that were established by Peres et al. (2011), Naor and Shefﬁeld (2010) and Shefﬁeld and Smart
(2010) for unweighted graphs. These results are essential for the analyses of our algorithms. We defer most proofs to
Appendix A.

Deﬁnition 3.1 A steepest ﬁxable path in an instance (G, v0) is a free terminal path P that has the largest gradient
∇P (v0) amongst such paths.

Observe that a steepest ﬁxable path with ∇P (v0) 6= 0 must be a simple path.
Deﬁnition 3.2 Given a steepest ﬁxable path P in an instance (G, v0), we deﬁne ﬁxG[v0, P ] : VG → R ∪ {∗} to be the
voltage assignment deﬁned as follows

ﬁxG[v0, P ](x) =

v0(∂0P ) − ∇P (v0) · ℓG(P [∂0P : x]) x ∈ int(P ) \ T (v0),
v0(x)

otherwise.

(

We say that the vertices x ∈ int(P ) are ﬁxed by the operation ﬁx[v0, P ]. If we deﬁne v1 = ﬁxG[v0, P ], where
P = (x0, . . . , xr) is the steepest ﬁxable path in (G, v0), then it is easy to argue that for every i ∈ [r], we have
grad[v1](xi−1, xi) = ∇P (see Lemma A.5). The meta-algorithm META-LEX, spelled out as Algorithm 1, entails
repeatedly ﬁxing steepest ﬁxable paths. While it is possible to have multiple steepest ﬁxable paths, the result of ﬁxing
all of them does not depend on the order in which they are ﬁxed.

Theorem 3.3 Given a well-posed instance (G, v0), the meta-algorithm META-LEX, which repeatedly ﬁxes steepest
ﬁxable paths, produces the unique lex-minimizer extending v0.

Corollary 3.4 Given a well-posed instance (G, v0) such that T (v0) 6= VG, let P be a steepest ﬁxable path in (G, v0).
Then, (G, ﬁx[v0, P ]) is also a well-posed instance, and lexG[ﬁx[v0, P ]] = lexG[v0].

Since a lex-minimal element must be an inf-minimizer, we also obtain the following corollary, that can also be

proved using LP duality.

Lemma 3.5 Suppose we have a well-posed instance (G, v0). Then, there exists a complete voltage assignment v
extending v0 such that

grad[v]

∞ ≤ α, iff every terminal path P in (G, v0) satisﬁes ∇P (v0) ≤ α.
(cid:13)
(cid:13)

3.1 Stability

(cid:13)
(cid:13)

The following theorem states that lexG[v0] is monotonic with respect to v0 and it respects scaling and translation of
v0.

Theorem 3.6 Let (G, v0) be a well-posed instance with T := T (v0) as the set of terminals. Then the following
statements hold.

1. For any c, d ∈ R, v1 a partial assignment with terminals T (v1) = T and v1(t) = cv0(t) + d for all t ∈ T .

Then, lexG[v1](i) = c · lexG[v0](i) + d for all i ∈ VG.

2. v1 a partial assignment with terminals T (v1) = T. Suppose further that v1(t) ≥ v0(t) for all t ∈ T. Then,

lexG[v1](i) ≥ lexG[v0](i) for all i ∈ VG.

As a corollary, the above theorem gives a nice stability property that lex-minimal elements satisfy.

Corollary 3.7 Given well-posed instances (G, v0), (G, v1) such that T := T (v0) = T (v1), let ǫ := maxt∈T |v0(t) −
v1(t)|. Then |lexG[v0](i) − lexG[v1](i)| ≤ ǫ for all i ∈ VG.

5

3.2 Alternate Characterizations

There are at least two other seemingly disparate deﬁnitions that are equivalent to lex-minimal voltages.

lp-norm Minimizers. As mentioned in the introduction, for a well-posed instance (G, v0) the lex-minimizer is also
the limit of lp minimizers. This follows from existing results about the limit of lp-minimizers (Egger and Huotari
(1990)) in afﬁne spaces, since {grad[v] | v is complete, v extends v0} forms an afﬁne subspace of Rm. Thus, we have
the following theorem:

Theorem 3.8 (Limit of lp-minimizers, follows from Egger and Huotari (1990)) For any p ∈ (1, ∞), given a well-
posed instance (G, v0) deﬁne vp to be the unique complete voltage assignment extending v0 and minimizing
p ,
i.e.

grad[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Then, limp→∞ vp = lexG[v0].

vp = arg min
v is complete
v extends v0 (cid:13)
(cid:13)

grad[v]

p .

(cid:13)
(cid:13)

Max-Min Gradient Averaging. Consider a well-posed instance (G, v0), and a complete voltage assignment v ex-
tending v0. If G is such that ℓ(e) = 1 for all e ∈ EG, it is easy to see that lex = lexG[v0] satisﬁes the following simple
condition for all x ∈ VG \ T (v0),

lex(x) =

1
2  

max
(x,y)∈EG

lex(y) + min

lex(z)

.

(x,z)∈EG

!

This condition should be contrasted to the optimality condition for l2-regularization on these instances, which gives
for all non-terminals x, the optimal voltage v satisﬁes v(x) = 1

y:(x,y)∈EG v(y).

deg(x)

To prove the above claim, consider locally changing lex at x and observe that the gradients of edges not incident
at x remain unchanged, and at least one of edges incident at x will have a strictly larger gradient, contradicting lex-
minimality. For general graphs, this condition of local optimality can still be characterized by a simple max-min
gradient averaging property as described below.

P

Deﬁnition 3.9 (Max-Min Gradient Averaging) Given a well-posed instance (G, v0), and a complete voltage as-
signment v extending v0, we say that v satisﬁes the max-min gradient averaging property (w.r.t. (G, v0)) if for every
x ∈ VG \ T (v0), we have

grad[v](x, y) = − min

grad[v](x, y).

max
y:(x,y)∈EG

y:(x,y)∈EG

As stated in the theorem below, lexG[v0] is the unique assignment satisfying max-min gradient averaging property.
Shefﬁeld and Smart (2010) proved a variant of this statement for weighted graphs. For completeness, we present a
proof in the appendix.

Theorem 3.10 Given a well-posed instance (G, v0), lexG[v0] satisﬁes max-min gradient averaging property. More-
over, it is the unique complete voltage assignment extending v0 that satisﬁes this property w.r.t. (G, v0).

An advantage of this characterization is that it can be veriﬁed quickly. This is particularly useful for implementations
for computing the lex-minimizer.

4 Algorithms

We now sketch the ideas behind our algorithms and give precise statements of our results. A full description of all the
algorithms is included in the appendix.

We deﬁne the pressure of a vertex to be the gradient of the steepest terminal path through it:

pressure[v0](x) = max{∇P (v0) | P is a terminal path in (G, v0) and x ∈ P }.

6

Observe that in a graph with no terminal-terminal edges, a free terminal path is a steepest ﬁxable path iff its gradient
is equal to the highest pressure amongst all vertices. Moreover, vertices that lie on steepest ﬁxable paths are exactly
the vertices with the highest pressure. For a given α > 0, in order to identify vertices with pressure exceeding α, we
compute vectors vHigh[α](x) and vLow[α](x) deﬁned as follows in terms of dist, the metric on V induced by ℓ:

vLow[α](x) = min

{v0(t) + α · dist(x, t)}

t∈T (v0)

vHigh[α](x) = max
t∈T (v0)

{v0(t) − α · dist(t, x)}.

4.1 Lex-minimization on Star Graphs

We ﬁrst consider the problem of computing the lex-minimizer on a star graph in which every vertex but the center is a
terminal. This special case is a subroutine in the general algorithm, and also motivates some of our techniques.

Let x be the center vertex, T be the set of terminals, and all edges be of the form (x, t) with t ∈ T . The initial
voltage assignment is given by v : T → R, and we abbreviate dist(x, t) by d(t) = ℓ(x, t). From Corollary 3.4 we know
that we can determine the value of the lex minimizer at x by ﬁnding a steepest ﬁxable path. By deﬁnition, we need to
ﬁnd t1, t2 ∈ T that maximize the gradient of the path from t1 to t2, ∇(t1, t2) = v(t1)−v(t2)
d(t2)+d(t2) . As observed above, this
is equivalent to ﬁnding a terminal with the highest pressure. We now present a simple randomized algorithm for this
problem that runs in expected linear time.

Given a terminal t1, we can compute its pressure α along with the terminal t2 such that |∇(t1, t2)| = α in time
O(|T |) by scanning over the terminals in T . Consider doing this for a random terminal t1. We will show that in linear
time one can then ﬁnd the subset of terminals T ′ ⊂ T whose pressure is greater than α. Assuming this, we complete
the analysis of the algorithm. If T ′ = ∅, t1 is a vertex with highest pressure. Hence the path from t1 to t2 is a steepest
ﬁxable path, and we return (t1, t2). If T ′ 6= ∅, the terminal with the highest pressure must be in T ′, and we recurse by
picking a new random t1 ∈ T ′. As the size of T ′ will halve in expectation at each iteration, the expected time of the
algorithm on the star is O(|T |).

To determine which terminals have pressure exceeding α, we observe that the condition ∃t2 : α < ∇(t1, t2) =
v(t1)−v(t2)
d(t1)+d(t2) , is equivalent to ∃t2 : v(t2)+αd(t2) < v(t1)−αd(t1). This, in turn, is equivalent to vLow[α](x) < v(t1)−
αd(t1). We can compute vLow[α](x) in deterministic O(|T |) time. Similarly, we can check if ∃t2 : α < ∇(t2, t1) by
checking if vHigh[α](x) > vt1 + αd(t1). Thus, in linear time, we can compute the set T ′ of terminals with pressure
exceeding α. The above algorithm is described in Algorithm 10.

Theorem 4.1 Given a set of terminals T, initial voltages v : T → R, and distances d : T → R+, STARSTEEPESTPATH(T, v, d)
returns (t1, t2) maximizing v(t1)−v(t2)

d(t1)+d(t2) , and runs in expected time O(|T |).

4.2 Lex-minimization on General Graphs

Theorem 3.3, tells us that META-LEX will compute lex-minimizers given an algorithm for ﬁnding a steepest ﬁxable
path in (G, v0). Recall that ﬁnding a steepest ﬁxable path is equivalent to ﬁnding a path with gradient equal to the
highest pressure amongst all vertices. In this section, we show how to do this in expected time O(m + n log n).

We describe an algorithm VERTEXSTEEPESTPATH that ﬁnds a terminal path P through any vertex x such that
∇P (v0) = pressure[v0](x) in expected O(m + n log n) time. Using Dijkstra’s algorithm, we compute dist(x, t) for
all t ∈ T. If x ∈ T (v0), then there must be a terminal path P that starts at x that has ∇P (v0) = pressure[v0](x). To
compute such a P we examine all t ∈ T (v0) in O(|T |) time to ﬁnd the t that maximizes |∇(x, t)| = |v(x)−v(t)|
, and
dist(x,t)
then return a shortest path between x and that t.

If x /∈ T (v0), then the steepest path through x between terminals t1 and t2 must consist of shortest paths between
x and t1 and between x and t2. Thus, we can reduce the problem to that of ﬁnding the steepest path in a star graph
where x is the only non-terminal and is connected to each terminal t by an edge of length dist(x, t). By Theorem 4.1,
we can ﬁnd this steepest path in O(|T |) expected time. The above algorithm is formally described as Algorithm 9.

Theorem 4.2 Given a well-posed instance (G, v0), and a vertex x ∈ VG, VERTEXSTEEPESTPATH(G, v0, x) returns
a terminal path P through x such that ∇P (v0) = pressure[v0](x), in O(m + n log n) expected time.

7

As in the algorithm for the star graph, we need to identify the vertices whose pressure exceeds a given α. For a ﬁxed
α, we can compute vLow[α](x) and vHigh[α](x) for all x ∈ VG using a simple modiﬁcation of Dijkstra’s algorithm in
O(m + n log n) time. We describe the algorithms COMPVHIGH, COMPVLOW for these tasks in Algorithms 3 and 4.
The following lemma encapsulates the usefulness of vLow and vHigh.

Lemma 4.3 For every x ∈ VG, pressure[v0](x) > α iff vHigh[α](x) > vLow[α](x).

It immediately follows that the algorithm COMPHIGHPRESSGRAPH(G, v0, α) described in Algorithm 6 computes

the vertex induced subgraph on the vertex set {x ∈ VG| pressure[v0](x) > α}.

We can combine these algorithms into an algorithm STEEPESTPATH that ﬁnds the steepest ﬁxable path in (G, v0)
in O(m + n log n) expected time. We may assume that there are no terminal-terminal edges in G. We sample an edge
(x1, x2) uniformly at random from EG, and a terminal x3 uniformly at random from VG. For i = 1, 2, 3, we compute
the steepest terminal path Pi containing xi. By Theorem 4.2, this can be done in O(m + n log n) expected time. Let α
be the largest gradient maxi ∇Pi. As mentioned above, we can identify G′, the induced subgraph on vertices x with
pressure exceeding α, in O(m + n log n) time. If G′ is empty, we know that the path Pi with largest gradient is a
steepest ﬁxable path. If not, a steepest ﬁxable path in (G, v0) must be in G′, and hence we can recurse on G′. Since
we picked a uniformly random edge, and a uniformly random vertex, the expected size of G′ is at most half that of G.
Thus, we obtain an expected running time of O(m + n log n). This algorithm is described in detail in Algorithm 7.

Theorem 4.4 Given a well-posed instance (G, v0) with EG ∩ (T (v0) × T (v0)) = ∅, STEEPESTPATH(G, v0) returns
a steepest ﬁxable path in (G, v0), and runs in O(m + n log n) expected time.

By using STEEPESTPATH in META-LEX, we get the COMPLEXMIN, shown in Algorithm 1. From Theorem 3.3 and
Theorem 4.4, we immediately get the following corollary.

Corollary 4.5 Given a well-posed instance (G, v0) as input, algorithm COMPLEXMIN computes a lex-minimizing
assignment that extends v0 in O(n(m + n log n)) expected time.

4.3 Linear-time Algorithm for Inf-minimization

Given the algorithms in the previous section, it is straightforward to construct an inﬁnity minimizer. Let α⋆ be the
gradient of the steepest terminal path. From Lemma 3.5, we know that the norm of the inf minimizer is α⋆. Considering
all trivial terminal paths (terminal-terminal edges), and using STEEPESTPATH, we can compute α⋆ in randomized
O(m+n log n) time. It is well known (McShane (1934); Whitney (1934)) that v1 = vLow[α⋆] and v2 = vHigh[α⋆] are
inf-minimizers. It is also known that 1
2 (v1 + v2) is the inf-minimizer that minimizes the maximum ℓ∞-norm distance
to all inf-minimizers. In the case of path graphs, this was observed by Gaffney and Powell (1976) and independently
by Micchelli et al. (1976). For completeness, the algorithm is presented as Algorithm 5, and we have the following
result.

Theorem 4.6 Given a well-posed instance (G, v0), COMPINFMIN(G, v0) returns a complete voltage assignment v
for G extending v0 that minimizes

∞ , and runs in randomized O(m + n log n) time.

grad[v]

4.4 Faster Algorithms for Lex-minimization

(cid:13)
(cid:13)

(cid:13)
(cid:13)

The lex-minimizer has additional structure that allows one to compute it by more efﬁcient algorithms. One observation
that leads to a faster implementation is that ﬁxing a steepest ﬁxable path does not increase the pressure at vertices,
provided that one appropriately ignores terminal-terminal edges. Thus, if G(α) is a subgraph that we identiﬁed with
pressure greater than α, we can iteratively ﬁx all steepest ﬁxable paths P in G(α) with ∇P > α. Another simple
observation is that if G(α) is disconnected, we can simply recurse on each of the connected components. A complete
description of an the algorithm COMPFASTLEXMIN based on these idea is given in Algorithm 11. The algorithm
provably computes lexG(v0), and it is possible to implement it so that the space requirement is only O(m + n).
Although, we are unable to prove theoretical bounds on the running time that are better than O(n(m + n log n)),
it runs extremely quickly in practice. We used it to perform the experiments in this paper. For random regular
graphs and Delaunay graphs, with n = 0.5 × 106 vertices and around 2 million edges m ∼ 1.5 − 2 × 106, it

8

takes a couple of minutes on a 2009 MacBook Pro. Similar times are observed for other model graphs of this
size such as random regular graphs and real world networks. An implementation of this algorithm may be found
at https://github.com/danspielman/YINSlex.

5 Directed Graphs

Our deﬁnitions and algorithms, including those for regularization, extend to directed graphs with only small modiﬁ-
cations. We view directed edges as diodes and only consider potential differences in the direction of the edge. For
a complete voltage assignment v on the vertices of a directed graph G, we deﬁne the directed gradient on (x, y) due
to v to be grad+
. Given a partially-labelled directed graph (G, v0), we say that a a
complete voltage assignment v is a lex-minimizer if it extends v0 and for other complete voltage assignment v′ that
extends v0 we have grad+
G[v′]. We say that a partially-labelled directed graph (G, v0) is a well-posed
directed instance if every free vertex appears in a directed path between two terminals.

G[v](x, y) = max

G[v] (cid:22) grad+

v(x)−v(y)
ℓ(x,y)

, 0

n

o

The main difference between the directed and undirected cases is that the directed lex-minimizer is not necessarily
unique. To maintain clarity of exposition, we chose to focus on undirected graphs so far. For directed graphs, we have
the following corresponding structural results.

Theorem 5.1 Given a well-posed instance (G, v0) on a directed graph G, there exists a lex-minimizer, and the set of
all lex-minimizers is a convex set. Moreover, for every two lex-minimizers v and v′, we have grad+

G[v] = grad+

G[v′].

However, note that in the case of directed graphs, the lex-minimizer need not be unique. We still have a weaker version
of Theorem 3.3 for directed graphs.

Theorem 5.2 Given a well-posed instance (G, v0) on a directed graph G, let v1 be the partial voltage assignment
extending v0 obtained by repeatedly ﬁxing steepest ﬁxable (directed) paths P with ∇P > 0. Then, any lex-minimizer
of (G, v0) must extend v1. Moreover, for every edge e ∈ EG \ (T (V1) × T (V1)), any lex-minimizer v of (G, v0) must
satisfy grad+[v](e) = 0.

When the value of the lex-minimizer at a vertex is not uniquely determined, it is constrained to an interval. In our
experiments, we pick the convention that when the voltage at a vertex is constrained to an interval (−∞, a] or [a, ∞),
we assign a to the terminal. When it is constrained to a ﬁnite interval, we assign a voltage closest to the median of the
original voltages.

6 Experiments on WebSpam

We demonstrate the performance of our lex-minimization algorithms on directed graphs by using them to detect spam
webpages as in Zhou et al. (2007). We use the dataset webspam-uk2006-2.0 described in Castillo et al. (2006).
This collection includes 11,402 hosts, out of which 7,473 (65.5 %) are labeled, either as spam or normal. Each host
corresponds to the collection of web pages it serves. Of the hosts, 1924 are labeled spam (25.7 % of all labels). We
consider the problem of ﬂagging some hosts as spam, given only a small fraction of the labels for training. We assign
a value of 1 to the spam hosts, and a value of 0 to the normal ones. We then compute a lex minimizer and examine the
effect of ﬂagging as spam all hosts with a value greater than some threshold.

Following Zhou et al. (2007), we create edges between hosts with lengths equal to the reciprocal of the number of
links from one to the other. We run our experiments only on the largest strongly connected component of the graph,
which contains 7945 hosts of which 5552 are labeled. 16 % of the nodes in this subgraph are labeled spam. To create
training and test data, for a given value p, we select a random subset of p % of the spam labels and a random subset
of p % of the normal labels to use for training. The remaining labels are used for testing. We report results for p = 5
and p = 20.

Again following Zhou et al. (2007), we plot the precision and recall of different choices of threshold for ﬂagging
pages as spam. Recall is the fraction of spam pages our algorithm ﬂags as spam, and precision is the fraction of pages
our algorithm ﬂags as spam that actually are spam. Amongst the algorithms studied by Zhou et al. (2007), the top

9

performer was their algorithm based on sampling according to a random-walk that follows in-links from other hosts.
We compare their algorithm with the classiﬁcation we get by directing edges in the opposite directions of links. This
has the effect that a link to a spam host is evidence of spamminess, and a link from a normal host is evidence of
normality.

Results are shown in Figure 3. While we are not able to reliably ﬂag all spam hosts, we see that in the range of
10-50 % recall, we are able to ﬂag spam with precision above 82 %. We see that the performance of directed lex-
minimization does not degrade rapidly when from the “large training set” regime of p = 20, to the “small training set”
regime of p = 5.

5 % labels for training

20 % labels for training

RandWalk
DirectedLex

RandWalk
DirectedLex

i

i

n
o
s
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
0.1

0.2

0.3

0.4

0.7

0.8

0.9

1

0.2

0.3

0.4

0.7

0.8

0.9

1

0.6
0.5
Recall

0.6
0.5
Recall

Figure 3: Recall and precision in the web spam classiﬁcation experiment. Each data point shown was computed as an average over
100 runs. The largest standard deviation of the mean precision across the plotted recall values was less than 1.3 %. The algorithm
of Zhou et al. (2007) appears as RANDWALK. Our directed lex-minimization algorithm appears as DIRECTEDLEX.

For comparison, in Appendix C, we show the performance of our algorithm and that of Zhou et al. (2007) both
with link directions reversed, as well as the performance of undirected lex-minimization and Laplacian inference, all
of which are signiﬁcantly worse.

7 l0-Regularization of Vertex Values

We now explain how we can accommodate noise in both the given voltages and in the given lengths of edges. We can
ﬁnd the minimum number of labels to ignore, or the minimum increase in edges lengths needed so that there exists an
extension whose gradients have l∞-norm lower than a given target. After determining which labels to ignore or the
needed increment in edge lengths, we recommend computing a lex minimizer.

The algorithms we present in this section are essentially the same for directed and undirected graphs.

7.1 l0-Vertex Regularization for Inf-minimization

The l0-regularization of vertex labels can be viewed as a problem of outlier removal: the vector we compute is allowed
to disagree with v0 on up to k terminals. Given a voltage assignment v and a subset T ⊂ V of the vertices, by v(T )
we mean the vector obtained by restricting v to T . We deﬁne the l0-Vertex Regularization for l∞ problem to be

where v(T ) is the vector of values of v on the terminals T .

min
v∈IRn

gradG[v]

∞

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

0 ≤ k,
(cid:13)
(cid:13)

subject to

v(T ) − v0(T )

(3)

In Appendix D, we describe an approximation algorithm APPROX-OUTLIER that approximately solves program (3).

The precise statement we prove in Appendix D is given in the following theorem.

i

i

n
o
s
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
0.1

10

Theorem 7.1 (Approximate l0-vertex regularization) The algorithm APPROX-OUTLIER takes a positive integer k
and a partially-labeled graph (G, v0), and outputs an assignment v with
0 ≤ 2k, and
∞ ≤
α∗, where α∗ is the optimum value of program (3). The algorithm runs in time O(k(m + n log n)).
(cid:13)
(cid:13)
(cid:13)
(cid:13)

In Appendix D, we also describe an algorithm OUTLIER that exactly solves program (3) in polynomial time, and we
prove its correctness.

v(T ) − v0(T )

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Theorem 7.2 (Exact l0-vertex regularization) The algorithm OUTLIER takes a positive integer k and a partially-
labeled graph (G, v0) solves program (3) exactly. The algorithm runs in polynomial time.

We give a proof of Theorem 7.2 in Appendix D. To do this, we reduce the program (3) to the problem of minimizing
the required l0-budget needed to achieve a ﬁxed gradient α using a binary search over a set of O(n2) gradients. This
latter problem we reduce in polynomial time to Minimum Vertex Cover (VC) on a transitively closed, directed acyclic
graph (a TC-DAG). VC on a TC-DAG can be solved exactly in polynomial time by a reduction to the Maximum
Bipartite Matching Problem (Fulkerson (1956)). The problem was phrased by Fulkerson as one of ﬁnding a maximum
antichain of a ﬁnite poset. Any transitively closed DAG corresponds directly to the comparability graph of a poset. A
maximum antichain of a poset is a maximum independent set of a the comparability graph of the poset, and hence its
complement is a minimum vertex cover of the comparability graph. We refer to the algorithm developed by Fulkerson
as KONIG-COVER.

Theorem 7.3 The algorithm KONIG-COVER computes a minimum vertex cover for any transitively closed DAG G in
polynomial time.

7.2 Hardness of l0 regularization for l2

The result that l0-regularized inf-minimization can be solved exactly in polynomial time is surprising, especially
because the analogous problem for 2-Laplacian minimization turns out to be NP-Hard.

We deﬁne the the l0 vertex regularization for l2 for a partially-labeled graph (G, v0) and an integer k by

min
v∈Rn:kv(T )−v0(T )k0

≤k

vT Lv,

where L is the Laplacian of G.

Theorem 7.4 l0 vertex regularization for l2 is NP-Hard.

In Appendix E we prove Theorem 7.4 by giving a polynomial time (Karp) reduction from the NP-Hard minimum
bisection problem to l0 vertex regularization for l2.

8 l1-Edge and Vertex Regularization of Inf-minimizers

Consider a partially-labeled graph (G, v0) and an α > 0. The set of voltage assignments given by

v : v extends v0 and

gradG[v]

∞ ≤ α

n

(cid:13)
(cid:13)

(cid:13)
(cid:13)

o

is convex. Going further, let us consider the edge lengths in a graph to be speciﬁed by a vector ℓ ∈ IRE. Now the set
of voltages v and and lengths ℓ which achieve kgradG(ℓ)[v]k∞ ≤ α is jointly convex in v and ℓ. To see this, observe
that

kgradG(ℓ)[v]k∞ ≤ α ⇔ ∀(u, v) ∈ E : −αℓ(u, v) ≤ v(u) − v(v) ≤ αℓ(u, v).
Furthermore, the condition “v extends v0” is a linear constraint on v, which we express as v(T ) = v0(T ). From
the above, it is clear that the gradient condition corresponds to a convex set, as it is an intersection of half-spaces.
These half-spaces are given by O(m) linear inequalities. We can leverage this to phrase many regularized variants of
inf-minimization as convex programs, and in some cases linear programs.

(4)

11

For example, we may consider a variant of inf-minimization combined with an l1-budget for changing lengths of
edges and values on terminals. Given a parameter γ > 0 which speciﬁes the relative cost of regularizing terminals to
regularizing edges, the problem is as follows

arg min
v∈IRn,s∈IRm,s≥0

ksk1 + γ

v(T ) − v0(T )

1

subject to

gradG(ℓ+s)[v]

≤ α.

(5)

(cid:13)
(cid:13)
From our observation (4), it follows that problem (5) may be expressed as a linear program with O(n) variables
and O(m) constraints. We can use ideas from Daitch and Spielman (2008) to solve the resulting linear program in
O(m1.5) by an interior point method with a special purpose linear equation solver. The reason is that the linear
time
equations the IPM must solve at each iteration may be reduced to linear equations in symmetric, diagonally dominant
matrices, and these may be solved in nearly-linear time (Cohen et al. (2014)).

(cid:13)
(cid:13)

e

(cid:13)
(cid:13)
(cid:13)

∞

(cid:13)
(cid:13)
(cid:13)

Conclusion. We propose the use of inf and lex minimizers for regression on graphs. We present simple algorithms
for computing them that are provably fast and correct, and can also be implemented efﬁciently. We also present a
framework and polynomial time algorithms for regularization in this setting. The initial experiments reported in the
paper indicate that these algorithms give pretty good results on real and synthetic datasets. The results seem to compare
quite favorably to other algorithms, particularly in the regime of tiny labeled sets. We are testing these algorithms on
several other graph learning questions, and plan to report on them in a forthcoming experimental paper. We believe
that inf and lex minimizers, and the associated ideas presented in the paper, should be useful primitives that can be
proﬁtably combined with other approaches to learning on graphs.

We thank anonymous reviewers for helpful comments. We thank Santosh Vempala and Bartosz Walczak for pointing
out that it was already known how to compute a minimum vertex cover of a transitively closed DAG in polynomial
time.

Acknowledgements

References

Morteza Alamgir
In Advances
Information Processing
http://books.nips.cc/papers/files/nips24/NIPS2011_0278.pdf.

and Ulrike V. Luxburg.

transition
24,

in
pages

in Neural

Systems

Phase

the

family
379–387.

of
2011.

p-resistances.
URL

Gunnar Aronsson. Extension of functions satisfying lipschitz conditions. Arkiv fr Matematik, 6(6):551–561, 1967.

ISSN 0004-2080. doi: 10.1007/BF02591928. URL http://dx.doi.org/10.1007/BF02591928.

Gunnar Aronsson, Michael G. Crandall, and Petri Juutinen. A tour of the theory of absolutely minimizing functions.
ISSN 0273-0979. doi: 10.1090/S0273-0979-04-01035-3.

Bull. Amer. Math. Soc. (N.S.), 41(4):439–505, 2004.
URL http://dx.doi.org/10.1090/S0273-0979-04-01035-3.

Guy Barles and J´erˆome Busca. Existence and comparison results for fully nonlinear degenerate elliptic equations

without zeroth-order term. Comm. Partial Differential Equations, 26:2323–2337, 2001.

Mikhail Belkin, Irina Matveeva, and Partha Niyogi.

Regularization and semi-supervised learning on large
In Learning Theory, volume 3120 of Lecture Notes in Computer Science, pages 624–638.
doi: 10.1007/978-3-540-27819-1 43. URL

graphs.
Springer Berlin Heidelberg, 2004.
http://dx.doi.org/10.1007/978-3-540-27819-1_43.

ISBN 978-3-540-22282-8.

Nick Bridle and Xiaojin Zhu. p-voltages: Laplacian regularization for semi-supervised learning on high-dimensional

data. In Eleventh Workshop on Mining and Learning with Graphs (MLG2013), 2013.

12

Carlos Castillo, Debora Donato, Luca Becchetti, Paolo Boldi, Stefano Leonardi, Massimo Santini, and Sebastiano
Vigna. A reference collection for web spam. SIGIR Forum, 40(2):11–24, December 2006. ISSN 0163-5840. doi:
10.1145/1189702.1189703. URL http://doi.acm.org/10.1145/1189702.1189703.

Olivier Chapelle, Bernhard Schlkopf, and Alexander Zien. Semi-Supervised Learning. The MIT Press, 1st edition,

2010. ISBN 0262514125, 9780262514125.

Michael B Cohen, Rasmus Kyng, Gary L Miller, Jakub W Pachocki, Richard Peng, Anup B Rao, and Shen Chen Xu.
Solving SDD linear systems in nearly m log1/2 n time. In Proceedings of the 46th Annual ACM Symposium on
Theory of Computing, pages 343–352. ACM, 2014.

M.G. Crandall, L.C. Evans, and R.F. Gariepy. Optimal lipschitz extensions and the inﬁnity laplacian. Calculus of Vari-
ations and Partial Differential Equations, 13(2):123–139, 2001. ISSN 0944-2669. doi: 10.1007/s005260000065.
URL http://dx.doi.org/10.1007/s005260000065.

Samuel I. Daitch and Daniel A. Spielman. Faster approximate lossy generalized ﬂow via interior point algo-
rithms.
In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing, STOC ’08, pages
451–460, New York, NY, USA, 2008. ACM. ISBN 978-1-60558-047-0. doi: 10.1145/1374376.1374441. URL
http://doi.acm.org/10.1145/1374376.1374441.

Alan Egger and Robert Huotari. Rate of convergence of the discrete polya algorithm. Journal of Approximation
ISSN 0021-9045. doi: http://dx.doi.org/10.1016/0021-9045(90)90070-7. URL

Theory, 60(1):24 – 30, 1990.
http://www.sciencedirect.com/science/article/pii/0021904590900707.

Charles Fefferman. Whitney’s extension problems and interpolation of data.

(N.S.), 46(2):207–220, 2009a.
http://dx.doi.org/10.1090/S0273-0979-08-01240-8.

ISSN 0273-0979.

doi:

10.1090/S0273-0979-08-01240-8.

Bull. Amer. Math. Soc.
URL

Charles Fefferman. Fitting a [image] -smooth function to data, iii. Annals of Mathematics, 170(1):pp. 427–441, 2009b.

ISSN 0003486X. URL http://www.jstor.org/stable/40345469.

Charles Fefferman and Bo’az Klartag. Fitting a cm -smooth function to data i. Annals of Mathematics, 169(1):pp.

315–346, 2009. ISSN 0003486X. URL http://www.jstor.org/stable/40345445.

D. R. Fulkerson. Note on dilworths decomposition theorem for partially ordered sets. Proc. Amer. Math. Soc, 1956.

P.W. Gaffney and M.J.D. Powell. Optimal interpolation. In Numerical Analysis, volume 506 of Lecture Notes in Math-
ematics, pages 90–99. Springer Berlin Heidelberg, 1976. ISBN 978-3-540-07610-0. doi: 10.1007/BFb0080117.
URL http://dx.doi.org/10.1007/BFb0080117.

L.-A. Gottlieb, A. Kontorovich, and R. Krauthgamer. Efﬁcient classiﬁcation for metric data. CoRR, abs/1306.2547,

2013. URL http://arxiv.org/abs/1306.2547.

L.-A. Gottlieb, A. Kontorovich, and R. Krauthgamer. Efﬁcient regression in metric spaces via approximate lipschitz
extension. In Similarity-Based Pattern Recognition, volume 7953 of Lecture Notes in Computer Science, pages
43–58. Springer Berlin Heidelberg, 2013b. ISBN 978-3-642-39139-2. doi: 10.1007/978-3-642-39140-8 3. URL
http://dx.doi.org/10.1007/978-3-642-39140-8_3.

Robert Jensen. Uniqueness of lipschitz extensions: Minimizing the sup norm of the gradient. Archive for Ra-
doi: 10.1007/BF00386368. URL

ISSN 0003-9527.

tional Mechanics and Analysis, 123(1):51–74, 1993.
http://dx.doi.org/10.1007/BF00386368.

M. Kirszbraun. ber die zusammenziehende und lipschitzsche transformationen. Fundamenta Mathematicae, 22(1):

77–108, 1934. URL http://eudml.org/doc/212681.

13

Andrew J. Lazarus, Daniel E. Loeb,

James G. Propp, Walter R. Stromquist,

Combinatorial games under

man.
229 – 264,
http://www.sciencedirect.com/science/article/pii/S0899825698906765.

http://dx.doi.org/10.1006/game.1998.0676.

and Economic Behavior,

ISSN 0899-8256.

auction play.

Games

1999.

doi:

and Daniel H. Ull-
27(2):
URL

Yunqian Ma and Yun Fu. Manifold Learning Theory and Applications. CRC Press, Inc., Boca Raton, FL, USA, 1st

edition, 2011. ISBN 1439871094, 9781439871096.

E. J. McShane. Extension of range of functions. Bull. Amer. Math. Soc., 40(12):837–842, 12 1934. URL

http://projecteuclid.org/euclid.bams/1183497871.

C.A. Micchelli, T.J. Rivlin,

and S. Winograd.

merische Mathematik, 26(2):191–200, 1976.
http://dx.doi.org/10.1007/BF01395972.

The optimal
ISSN 0029-599X.

recovery of
doi:

smooth functions.
10.1007/BF01395972.

Nu-
URL

V. A. Milman.

Absolutely minimal extensions of

functions on metric spaces.

1999.

URL

http://iopscience.iop.org/1064-5616/190/6/A05/pdf/MSB_190_6_A05.pdf.

Boaz Nadler, Nathan Srebro, and Xueyuan Zhou. Statistical analysis of semi-supervised learning: The limit of inﬁnite
unlabelled data. 2009. URL http://ttic.uchicago.edu/˜nati/Publications/NSZnips09.pdf.

A. Naor and S. Shefﬁeld. Absolutely minimal Lipschitz extension of tree-valued mappings. CoRR, abs/1005.2535,

May 2010. URL http://arxiv.org/abs/1005.2535.

A. M. Oberman. Finite difference methods for the Inﬁnity Laplace and p-Laplace equations. CoRR, abs/1107.5278,

July 2011. URL http://arxiv.org/abs/1107.5278.

Yuval Peres, Oded Schramm, Scott Shefﬁeld, and DavidB. Wilson.

Tug-of-war and the inﬁnity lapla-
In Selected Works of Oded Schramm, Selected Works in Probability and Statistics, pages 595–
doi: 10.1007/978-1-4419-9675-6 18. URL

cian.
638. Springer New York, 2011.
http://dx.doi.org/10.1007/978-1-4419-9675-6_18.

ISBN 978-1-4419-9674-9.

S. Shefﬁeld and C. K. Smart. Vector-valued optimal Lipschitz extensions. CoRR, abs/1006.1741, June 2010. URL

http://arxiv.org/abs/1006.1741.

Ali Kemal Sinop and Leo Grady. A seeded image segmentation framework unifying graph cuts and random walker
which yields a new algorithm. In Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on,
pages 1–8. IEEE, 2007.

Vijay V. Vazirani. Approximation Algorithms. Springer-Verlag New York, Inc., New York, NY, USA, 2001. ISBN

3-540-65367-8.

Ulrike von Luxburg and Olivier Bousquet. Distance-based classiﬁcation with lipschitz functions.

In Learn-
ing Theory and Kernel Machines, volume 2777 of Lecture Notes in Computer Science, pages 314–328.
doi: 10.1007/978-3-540-45167-9 24. URL
Springer Berlin Heidelberg, 2003.
http://dx.doi.org/10.1007/978-3-540-45167-9_24.

ISBN 978-3-540-40720-1.

Hassler Whitney.

Analytic extensions of differentiable functions deﬁned in closed sets.

tions of
http://www.jstor.org/stable/1989708.

the American Mathematical Society, 36(1):pp. 63–89, 1934.

ISSN 00029947.

Transac-
URL

Dengyong Zhou, Christopher J. C. Burges, and Tao Tao. Transductive link spam detection.

In Proceedings
of the 3rd International Workshop on Adversarial Information Retrieval on the Web, AIRWeb ’07, pages 21–
ISBN 978-1-59593-732-2. doi: 10.1145/1244408.1244413. URL
28, New York, NY, USA, 2007. ACM.
http://doi.acm.org/10.1145/1244408.1244413.

Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic

functions. In IN ICML, pages 912–919, 2003.

14

A Basic Properties of Lex-Minimizers

A.1 Meta Algorithm

Algorithm 1: Algorithm META-LEX: Given a well-posed instance (G, v0), outputs lexG[v0].
for i = 1, 2, . . . :

1. if T (vi−1) = VG, then return vi−1.
2. E′ = EG \ (T (vi−1) × T (vi−1)), G′ := (VG, E′).
3. Let P ⋆
4. vi ← ﬁx[vi−1, P ⋆
i ].

i be a steepest ﬁxable path in (G′, vi−1). Let α⋆

i ← ∇P ⋆(vi−1).

In this subsection, we prove the results that appeared in section 2. We start with a simple observation.

Proposition A.1 Given a well-posed instance (G, v0) such that T (v0) 6= V, let P be a steepest ﬁxable path in (G, v0).
Then, ﬁx[v0, P ] extends v0, and (G, ﬁx[v0, P ]) is also a well-posed instance.

The properties we prove below do not depend on the choice of the steepest ﬁxable path.

Proposition A.2 For any well-posed instance (G, v0), with |VG| = n, META-LEX(G, v0) terminates in at most n
iterations, and outputs a complete voltage assignment v that extends v0.

Proof of Proposition A.2: By Proposition A.1, at any iteration i, vi−1 extends v0 and (G′, vi−1) is a well-posed
instance. META-LEX only outputs vi−1 iff T (vi−1) = V, which means vi−1 is a complete voltage assignment. For
any vi−1 that is not complete, for any x ∈ V \T (vi−1), we must have a free terminal path in (G′, vi−1) that contains x.
i exists in (G′, vi−1). Since P ⋆
Hence, a steepest ﬁxable path P ⋆
i ] ﬁxes the voltage
i
✷
for at least one non-terminal. Thus, META-LEX(G, v0) must complete in at most n iterations.

is a free terminal path, ﬁx[vi−1, P ⋆

For the following lemmas, consider a run of META-LEX with well-posed instance (G, v0) as input. Let vout be the
complete voltage assignment output by META-LEX. Let Ei be the set of edges E′ and Gi be the graph G′ constructed
in iteration i of META-LEX.

Lemma A.3 For every edge e ∈ Ei−1 \ Ei, we have

grad[vout](e)

≤ α⋆

i . Moreover, α⋆

i is non-increasing with i.

Proof of Lemma A.3: Let P ⋆
i = (x0, . . . , xr) be a steepest ﬁxable path in iteration i (when we deal with instance
(Gi−1, vi−1)). Consider a terminal path Pi+1 in (Gi, vi) such that {∂0Pi+1, ∂1Pi+1} ∩ (T (vi) \ T (vi−1)) 6= ∅. We
i . On the contrary, assume that ∇Pi+1(vi) > α⋆
claim that ∇Pi+1(vi) ≤ α⋆
i . Consider the case ∂0Pi+1 ∈ T (vi) \
T (vi−1), ∂1P1 ∈ T (vi−1). By the deﬁnition of vi, we must have ∂0Pi+1 = xj for some j ∈ [r − 1]. Let P ′
i+1 be the
path formed by joining paths P ⋆

i+1 is a free terminal path in (Gi−1, vi−1). We have,

i [x0 : xj] and Pi+1. P ′

(cid:12)
(cid:12)

(cid:12)
(cid:12)

vi−1(x0) − vi−1(∂1Pi+1) = (vi(x0) − vi(xj )) + (vi(∂0Pi+1) − vi(∂1Pi+1))
i · ℓ(P ′

i · ℓ(Pi+1) = α⋆

i [x0 : xj]) + α⋆

i · ℓ(P ⋆

> α⋆

i+1),

giving ∇P ′
The other cases can be handled similarly.

i+1(vi) > α⋆

i , which is a contradiction since the steepest ﬁxable path P ⋆
i

in (Gi−1, vi−1) has gradient α⋆
i .

Applying the above claim to an edge e ∈ Ei−1 \ Ei, whose gradient is ﬁxed for the ﬁrst time in iteration i, we
i . If v is the complete voltage assignment output by META-LEX, since v extends vi+1,
i , implying

i . Applying the claim to the symmetric edge, we obtain −grad[vout](e) ≤ α⋆

obtain that grad[vi+1](e) ≤ α⋆
we get grad[vout](e) ≤ α⋆
|grad[vout](e)| ≤ α⋆
i .

Consider any free terminal path Pi+1 in (Gi, vi). If Pi+1 is also a terminal path in (Gi−1, vi−1), it is a free
terminal path in (Gi−1, vi−1). In addition, since a steepest ﬁxable path P ⋆
i , we get
i
∇Pi+1(vi) = ∇Pi+1(vi−1) ≤ α⋆
i . Otherwise, we must have {∂0Pi+1, ∂1Pi+1} ∩ (T (vi) \ T (vi−1)) 6= ∅, and we can
deduce ∇Pi+1(vi) ≤ α⋆
i using the above claim. Thus, all free terminal paths Pi+1 in (Gi, vi) satisfy ∇Pi+1(vi) ≤ α⋆
i .
✷
In particular, α⋆

in (Gi−1, vi−1) has ∇P ⋆

i = α⋆

i is non-increasing with i.

i+1(vi) ≤ α⋆

i+1 = ∇P ⋆

i . Thus, α⋆

15

Lemma A.4 For any complete voltage assignment v for G that extends v0, if v 6= vout, we have grad[v] 6(cid:22) grad[vout],
and hence grad[vout] (cid:22) grad[v].

Proof of Lemma A.4: Consider any complete voltage assignment v for G that extends v0, such that v 6= vout. Thus,
there exists a unique i such that v extends vi−1 but does not extend vi. We will argue that grad[v] 6(cid:22) grad[vout], and
hence grad[vout] (cid:22) grad[v]. For every edge e ∈ E \ Ei−1 that has been ﬁxed so far, grad[v](e) = grad[vi−1](e) =
grad[vout](e), and hence we can ignore these edges.

Since v extends vi−1 but not vi, there exists an x ∈ T (vi) \ T (vi−1) such that v(x) 6= vi(x) = vout(x). Assume
i picked

i = (x0, . . . , xr) is the steepest ﬁxable path with gradient α⋆

v(x) < vi(x) (the other case is symmetric). If P ⋆
in iteration i, we must have x = xj for some j ∈ [r − 1]. Thus,

j

j

(v(xk−1) − v(xk)) = v(x0) − v(xj ) > vi(x0) − vi(xj ) = α⋆

i · ℓ(P ⋆

i [x0 : xj ]) = α⋆
i ·

ℓ(xk−1, xk).

Xk=1

Xk=1
Thus, for some k ∈ [j], we must have grad[v](xk−1, xk) > α⋆
is a path in Gi−1, we have {xk−1, xk} 6⊆
T (vi−1). This gives (xk−1, xk) ∈ (Ei−1 \ Ei). But then, from Lemma A.3, it follows that for all e ∈ (Ei−1 \ Ei), we
✷
have |grad[vout](e)| ≤ α⋆

i . Thus, we have grad[v] 6(cid:22) grad[vout].

i . Since P ∗
i

Lemma A.5 Let P = (x0, . . . , xr) be a steepest ﬁxable path such that it does not have any edges in T (v0) × T (v0)
and v1 = ﬁxG[v0, P ]. Then for every i ∈ [r], we have grad[v1](xi−1, xi) = ∇P.

Proof of Lemma A.5: Suppose this is not true and let j ∈ [r] be the minimum number such that grad[v1](xj−1, xj) 6=
∇P. By deﬁnition of v1 we would necessarily have j < r and vj ∈ T (v0). Suppose grad[v1](xj−1, xj ) < ∇P. We
would then have v1(x0) − v1(xj ) < ∇P ∗ ℓ(P [x0 : xj]). Since P does not have any edges in T (v0) × T (v0),
P1 := (xj, ..., xr) would be a free terminal path with ∇P1 > ∇P. This is a contradiction. Other cases can be ruled
out similarly.

✷

Proof of Theorem 3.3: Consider an arbitrary run of META-LEX on (G, v0). Let vout be the complete voltage
assignment output by META-LEX. Proposition A.1 implies that vout extends v0. Lemma A.4 implies that for any
complete voltage assignment v 6= vout that extends v0, we have grad[vout] (cid:22) grad[v]. Thus, vout is a lex-minimizer.
Moreover, the lemma also gives that for any such v, grad[v] 6(cid:22) grad[vout]. and hence vout is a unique lex-minimizer.
Thus, vout is the unique voltage assignment satisfying Def. 2.1, and we denote it as lexG[v0]. Since we started with an
✷
arbitrary run of META-LEX, uniqueness implies that every run of META-LEX on (G, v0) must output lexG[v0].

Proof of Lemma 3.5: Suppose we have a complete voltage assignment v extending v0, such that
For any terminal path P = (x0, . . . , xr), we get,

grad[v]

∞ ≤ α.

∇P (v0) = v0(∂0P ) − v0(∂1P ) = v(∂0P ) − v(∂1P ) =

grad[v](xi−1, xi) ≤ α ·

ℓ(xi−1, xi) = α · ℓ(P ),

(cid:13)
(cid:13)

(cid:13)
(cid:13)

r

i=1
X

giving ∇P (v0) ≤ α.

On the other hand, suppose every terminal path P in (G, v0) satisﬁes ∇P (v0) ≤ α. Consider v = lexG[v0]. We
know that v extends v0. For every edge e ∈ EG ∩ T (v0) × T (v0), e is a (trivial) terminal path in (G, v0), and hence
has satisﬁes grad[v](e) = grad[v0](e) = ∇e(v0) ≤ α. Considering the reverse edge, we also obtain −grad[v](e) ≤ α.
Thus, |grad[v](e)| ≤ α. Moreover, using Lemma A.3, we know that for edge e ∈ EG \ T (v0) × T (v0), |grad[v](e)| ≤
1 = ∇P ⋆
α⋆
1 ≤ α since P1 is a terminal path in (G, v0). Thus, for every e ∈ EG, |grad[v](e)| ≤ α, and hence
✷
grad[v]
∞ ≤ α.
(cid:13)
(cid:13)
(cid:13)
(cid:13)
A.2 Stability

In this subsection, we sketch a proof of the monotonicity of lex-minimizers and show how it implies the stability
property claimed earlier.

For any well-posed (G, v0), there could be several possible executions of META-LEX, each characterized by the

sequence of paths P ⋆

i . We can apply Theorem 3.3 to deduce the following structural result about the lex-minimizer.

r

i=1
X

16

Corollary A.6 For any well-posed instance (G, v0), consider a sequence of paths (P1, . . . , Pr) and voltage assign-
ments (v1, . . . , vr) for some positive integer r such that:

1. P ⋆

i is a steepest ﬁxable path in (Gi−1, vi−1) for i = 1, . . . , r.

2. vi = ﬁx[vi−1, P ⋆

i ] for i = 1, . . . , r.

3. T (vr) = VG.

Then, we have vr = lexG[v0].

We call such a sequence of paths and voltages to be a decomposition of lexG[v0]. Again, note that lexG[v0] can
possibly have multiple decompositions. However, any two such decompositions are consistent in the sense that they
produce the same voltage assignment.

Proof of Corollary 3.7: We ﬁrst deﬁne some operations on partial assignments which simpliﬁes the notation. Let
v0, v1 be any two partial assignments with the same set of terminals T := T (v0) = T (v1) and c, d ∈ R. By cv0 + d
we mean a partial assignment v with T (v) = T satisfying v(t) = cv0(t) + d for all t ∈ T . Also, by v0 + v1 we
mean a partial assignment v with T (v) = T satisfying v(t) = v0(t) + v1(t) for all t ∈ T. Also, we say v1 ≥ v0 if
v1(t) ≥ v0(t) for all t ∈ T .

Now we can show how Corollary 3.7 follows from Theorem 3.6. Let v := v1 − v0, and kvk∞ = ǫ, for some ǫ > 0.
Therefore, v0 + ǫ ≥ v1 ≥ v0 − ǫ. Theorem 3.6 then implies that lexG[v0] + ǫ ≥ lex[v1] ≥ lex[v0] − ǫ, hence proving
✷
the corollary.

Proof sketch of Theorem 3.6:
It is easy to see that the ﬁrst statement holds. For the second statement, we ﬁrst
observe that if there is a sequence of paths P1, ..., Pr that is simultaneously a decomposition of both lex[v0] and
lex[v1], then this is easy to see. If such a path sequence doesn’t exist, then we look at vt := v0 + t(v1 − v0). We
state here without a proof (though the proof is elementary) that we can then split the interval [0, 1] into ﬁnitely many
subintervals [a0, a1], [a1, a2], .., [ak−1, ak], with a0 = 0, ak = 1, such that for any i, there is a path sequence P1, ..., Pr
which is a decomposition of lex[vt] for all t ∈ [ai, ai+1]. We then observe that v0 = va0 ≤ va1 ≤ ...vak = v1. Since
for every ai, ai+1, there is a path sequence which is simultaneously a decomposition of both lex[vai ] and lex[vai+1 ],
we immediately get

lex[v0] = lex[va0 ] ≤ lex[va1] ≤ ... ≤ lex[vak ] = lex[v1].

✷

A.3 Alternate Characterizations

Proof of Theorem 3.10: We know that lexG[v0] extends v0. We ﬁrst prove that v = lexG[v0] satisﬁes the max-min
gradient averaging property. Assume to the contrary. Thus, there exists x ∈ VG \ T (v0) such that

max
y:(x,y)∈EG

grad[v](x, y) 6= − min

grad[v](x, y).

y:(x,y)∈EG

Assume that max(x,y)∈EG grad[v](x, y) ≥ − min(x,y)∈EG grad[v](x, y). Then, consider v′ extending v0 that is iden-
tical to v except for v′(x) = v(x) − ǫ for ǫ > 0. For ǫ small enough, we get that

and

max
y:(x,y)∈EG

grad[v′](x, y) < max

grad[v](x, y)

y:(x,y)∈EG

− min

y:(x,y)∈EG

grad[v′](x, y) < max

grad[v](x, y).

y:(x,y)∈EG

The gradient of edges not incident on the vertex x is left unchanged. This implies that grad[v]

6(cid:22) grad[v′],

contradicting the assumption that v is the lex-minimizer. (The other case is similar).

17

For the other direction. Consider a complete voltage assignment v extending v0 that satisﬁes the max-min gradient

averaging property w.r.t. (G, v0). Let

α = max

grad[v](x, y) ≥ 0

(x,y)∈EG
x∈V \T (v0)

be the maximum edge gradient, and consider any edge (x0, x1) ∈ EG such that grad[v](x1, x0) = α, with x1 ∈
V \ T (v0). If α = 0, grad[v] is identically zero, and is trivially the lex-minimal gradient assignment. Thus, both v and
lexG[v0] are constant on each connected component. Since (G, v0) is well-posed, there is at least one terminal in each
component, and hence v and lexG[v0] must be identical.

Now assume α > 0. By the max-min gradient averaging property, ∃x2 ∈ VG such that (x1, x2) ∈ EG and

grad[v](x1, x2) =

min
y:(x1,y)∈EG

grad[v](x1, y) = − max

grad[v](x1, y)

y:(x1,y)∈EG

≤ −grad[v](x1, x0) = −α.

Thus, grad[v](x2, x1) ≥ α. Since α is the maximum edge gradient, we must have grad[v](x2, x1) = α. More-
over, v(x2) > v(x1) > v(x0), thus x2 6= x0. We can inductively apply this argument at x2 until we hit a ter-
minal. Similarly, if x0 /∈ T (v0) we can extend the path in the other direction. Consequently, we obtain a path
P = (xj , . . . , x2, x1, x0, x−1, . . . , xk) with all vertices as distinct, such that xj , xk ∈ T (v0), and xi ∈ V \ T (v0)
for all i ∈ [j + 1, k − 1]. Moreover, grad[v](xi, xi−1) = α for all j < i ≤ k. Thus, P is a free terminal path with
∇P [v0] = α.

Moreover, since v is a voltage assignment extending v0 with

∞ = α, using Lemma 3.5, we know that
every terminal path P ′ in (G, v0) must satisfy ∇P ′(v0) ≤ α. Thus, P is a steepest ﬁxable path in (G, v0). Thus,
letting v1 = ﬁx[v0, P ], using Corollary 3.4, we obtain that lexG[v1] = lexG[v0]. Moreover, since α = ∇P [v0] =
grad[v](xi, xi−1) for all i ∈ (j, k], we get v1(xi) = v(xi) for all i ∈ (j, k). Thus, v extends v1.

grad[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We can iterate this argument for r iterations until T (vr) = VG, giving v = vr and vr = lexG[vr] = lexG[v0].
(Since we are ﬁxing at least one terminal at each iteration, this procedure terminates). Thus, we get v = lexG[v0]. ✷

B Description of the Algorithms

Algorithm 2: MODDIJKSTRA(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs a complete
voltage assignment v for G, and an array parent : V → V ∪ {null}.

Add x to a ﬁbonacci heap, with key(x) = +∞.
ﬁnished(x) ← false

Decrease key(x) to v0(x).
parent(x) ← null.

1. for x ∈ VG,
2.
3.
4. for x ∈ T (v0)
5.
6.
7. while heap is not empty
8.
9.
10.
11.
12.
13.
14.
15. return (v, parent)

x ← pop element with minimum key from heap
v(x) ← key(x). ﬁnished(x) ← true .
for y : (x, y) ∈ EG

if ﬁnished(y) = false

if key(y) > v(x) + α · ℓ(x, y)

Decrease key(y) to v(x) + α · ℓ(x, y).
parent(y) ← x.

Theorem B.1 For a well-posed instance (G, V0) and a gradient value α ≥ 0, let (v, parent) ← MODDIJKSTRA(G, v0, α).
Then, v is a complete voltage assignment such that, ∀x ∈ VG, v(x) = mint∈T (v0){v0(t) + αdist(x, t)}. Moreover, the
pointer array parent satisﬁes ∀x /∈ T (v0), parent(x) 6= null and v(x) = v(parent(x)) + α · ℓ(x, parent(x)).

18

Algorithm 3: Algorithm COMPVLOW(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs
vLow, a complete voltage assignment for G, and an array LParent : V → V ∪ {null}.

1. (vLow, LParent) ← MODDIJKSTRA(G, v0, α)
2. return (vLow, LParent)

Algorithm 4: Algorithm COMPVHIGH(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs
vHigh, a complete voltage assignment for G, and an array HParent : V → V ∪ {null}.

if x ∈ T (v0) then v1(x) ← −v0(x) else v1(x) ← v1(x).

1. for x ∈ VG
2.
3. (temp, HParent) ← MODDIJKSTRA(G, v1, α)
4. for x ∈ VG : vHigh(x) ← −temp(x)
5. return (vHigh, HParent)

Corollary B.2 For a well-posed instance (G, V0) and a gradient value α ≥ 0, let (vLow[α], LParent) ← COMPVLOW(G, v0, α)
and (vHigh[α], HParent) ← COMPVHIGH(G, v0, α). Then, vLow[α], vHigh[α] are complete voltage assignments for
G such that, ∀x ∈ VG,

vLow[α](x) = min

{v0(t) + α · dist(x, t)}

t∈T (v0)

vHigh[α](x) = max
t∈T (v0)

{v0(t) − α · dist(t, x)}.

Moreover, the pointer arrays LParent, HParent satisfy ∀x /∈ T (v0), LParent(x), HParent(x) 6= null and

vLow[α](x) = vLow[α](LParent(x)) + α · ℓ(x, LParent(x)),
vHigh[α](x) = vHigh[α](HParent(x)) − α · ℓ(x, HParent(x)).

Algorithm 5: Algorithm COMPINFMIN(G, v0): Given a well-posed instance (G, v0), outputs a complete voltage assignment
v for G, extending v0 that minimizes (cid:13)

(cid:13)grad[v](cid:13)

(cid:13)∞.

1. α ← max{|grad[v0](e)| | e ∈ EG ∩ (T (v0) × T (v0))}.
2. EG ← EG \ (T (v0) × T (v0))
3. P ←STEEPESTPATH(G, v0).
4. α ← max{α, ∇P (v0)}
5. (vLow, LParent) ← COMPVLOW(G, v0, α)
6. (vHigh, HParent) ← COMPVHIGH(G, v0, α)
7. for x ∈ VG
8.
9.
10.
11. return v

then v(x) ← v0(x)
else v(x) ← 1

2 · (vLow(x) + vHigh(x)).

if x ∈ T (v0)

1. (vLow, LParent) ← COMPVLOW(G, v0, α)
2. (vHigh, HParent) ← COMPVHIGH(G, v0, α)
3. VG′ ← {x ∈ VG | vHigh(x) > vLow(x) }
4. EG′ ← {(x, y) ∈ EG | x, y ∈ VG′ }.

19

Algorithm 6: Algorithm COMPHIGHPRESSGRAPH(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0,
outputs a minimal induced subgraph G′ of G where every vertex has pressure[v0](·) > α.

5. G′ ← (V ′, E′, ℓ)
6. return G′

Proof of Lemma 4.3:

is equivalent to

vHigh[α](x) > vLow[α](x)

max
t∈T (v0)

{v0(t) − α · dist(t, x)} > min

{v0(t) + α · dist(x, t)},

t∈T (v0)

which implies that there exists terminals s, t ∈ T (v0) such that

thus,

Hence,

v0(t) − α · dist(t, x) > v0(s) + α · dist(x, s)

pressure[v0](x) ≥

v0(t) − v0(s)
dist(t, x) + dist(x, s)

> α.

v0(t) − v0(s)
dist(t, x) + dist(x, s)

= pressure[v0](x) > α.

v0(t) − α · dist(t, x) > v0(s) + α · dist(x, s)

So the inequality on vHigh and vLow implies that pressure is strictly greater than α. On the other hand, if pressure[v0](x) >
α, there exists terminals s, t ∈ T (v0) such that

which implies vHigh[α](x) > vLow[α](x).

✷

Algorithm 7: Algorithm STEEPESTPATH(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs a steepest
free terminal path P in (G, v0).

P ← VERTEXSTEEPESTPATH(G, v0, xi)

1. Sample uniformly random e ∈ EG. Let e = (x1, x2).
2. Sample uniformly random x3 ∈ VG.
3. for i = 1 to 3
4.
5. Let j ∈ arg maxj∈{1,2,3} ∇Pj (v0)
6. G′ ← COMPHIGHPRESSGRAPH(G, v0, ∇Pj (v0))
7. if EG′ = ∅,
8.
9.

then return Pj
else return STEEPESTPATH(G′, v0|VG′ )

1. while T (v0) 6= VG
2.
3.
4.
5. return v0

EG ← EG \ (T (v0) × T (v0))
P ← STEEPESTPATH(G, v0)
v0 ← ﬁx[v0, P ]

Algorithm 8: Algorithm COMPLEXMIN(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs lexG[v0].

Algorithm 9: Algorithm VERTEXSTEEPESTPATH(G,v0, x): Given a well-posed instance (G, v0), and a vertex x ∈ VG,
outputs a steepest terminal path in (G, v0) through x.

1. Using Dijkstra’s algorithm, compute dist(x, t) for all t ∈ T (v0)

20

y ← arg maxy∈T (v0)
if v0(x) ≥ v0(y)

|v0(x)−v0(y)|
dist(x,y)

then return a shortest path from x to y
else return a shortest path from y to x

2. if x ∈ T (v0)
3.
4.
5.
6.
7. else
8.
9.
10.
11.

for t /∈ T (v0), d(t) ← dist(x, t)
(t1, t2) ← STARSTEEPESTPATH(T (v0), v0|T (v0), d)
Let P1 be a shortest path from t1 to x. Let P2 be a shortest path from x to t2.
P ← (P1, P2). return P.

Algorithm 10: STARSTEEPESTPATH(T, v, d): Returns the steepest path in a star graph, with a single non-terminal connected
to terminals in T, with lengths given by d, and voltages given by v.

|v(t1)−v(t)|
d(t1)+d(t)

1. Sample t1 uniformly and randomly from T
2. Compute t2 ∈ arg maxt∈T
3. α ← |v(t2)−v(t1)|
d(t1)+d(t2)
4. Compute vlow ← mint∈T (v(t) + α · d(t))
5. Tlow ← {t ∈ T | v(t) > vlow + α · d(t)}
6. Compute vhigh ← maxt∈T (v(t) − α · d(t))
7. Thigh ← {t ∈ T | v(t) < vhigh − α · d(t)}
8. T ′ ← Tlow ∪ Thigh.
9. if T ′ = ∅
10.
11.

then if v(t1) ≥ v(t2) then return (t1, t2) else return (t2, t1)
else return STARSTEEPESTPATH(T ′, v|T ′, dT ′ )

B.1 Faster Lex-minimization

Algorithm 11: Algorithm COMPFASTLEXMIN(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs
lexG[v0].

1. while T (v0) 6= VG
2.
3. return v0

v0 ← FIXPATHSABOVEPRESS(G, v0, 0)

Algorithm 12: Algorithm FIXPATHSABOVEPRESS(G, v0, α): Given a well-posed instance (G, v0), with T (v0) 6= VG, and
a gradient value α, iteratively ﬁxes all paths with gradient > α.

EG ← EG \ (T (v0) × T (v0))
Sample uniformly random e ∈ EG. Let e = (x1, x2).
Sample uniformly random x3 ∈ VG.
for i = 1 to 3

Pi ← VERTEXSTEEPESTPATH(G, v0, xi)

Let j ∈ arg maxj∈{1,2,3} ∇Pj(v0)
G′ ← COMPHIGHPRESSGRAPH(G, v0, ∇Pj (v0))
if EG′ = ∅,

1. while T (v0) 6= VG
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.

then v0 ← ﬁx[v0, P ]
else Let G′

for i = 1, . . . , r

i, i = 1, . . . , r be the connected components of G′.

21

vi ← FIXPATHSABOVEPRESS(G′
for x ∈ VG′

i, set v0(x) ← vi(x)

i, v0|VG′

i

, ∇Pj (v0))

if α > 0 then G ←COMPHIGHPRESSGRAPH(G, v0, α)

13.
14.
15.
16. return v0

C Experiments on WebSpam: Testing More Algorithms

For completeness, in this appendix we show how a number of algorithms perform on the web spam experiment of
Section 6. We consider the following algorithms:

• RANDWALK along in-links. For a detailed description see Zhou et al. (2007). This algorithm essentially per-
forms a Personalized PageRank random walk from each vertex x and computes a spam-value for the vertex x by
taking a weighted average of the labels of the vertices where the random walk from x terminates. Also shown in
Section 6.

• DIRECTEDLEX, with edges in the opposite directions of links. This has the effect that a link to a spam host is

evidence of spam, and a link from a normal host is evidence of normality. Also shown in Section 6.

• RANDWALK along out-links.

• DIRECTEDLEX, with edges in the directions of links. This has the effect that a link from to a spam host is

evidence of spam, and a link to a normal host is evidence of normality.

• UNDIRECTEDLEX: Lex-minimization with links treated as undirected edges.

• LAPLACIAN: l2-regression with links treated as undirected edges.

• DIRECTED 1-NEAREST NEIGHBOR: Uses shortest distance along paths following out-links. Spam-ratio is
deﬁned distance from normal hosts, divided by distance to spam hosts. Sites are ﬂagged as spam when spam-
ratio exceeds some threshold. We also tried following paths along in-links instead, but that gave much worse
results.

We use the experimental setup described in Section 6. Results are shown in Figure 4. The alternative convention
for DIRECTEDLEX orients edges in the directions of links. This takes a link from a spam host to be evidence of
spam, and a link to a normal host to be evidence of normality. This approach performs signiﬁcantly worse than our
preferred convention, as one would intuitively expect. UNDIRECTEDLEX and LAPLACIAN approaches also perform
signiﬁcantly worse. DIRECTED 1-NEAREST NEIGHBOR performs poorly, demonstrating that DIRECTEDLEX is very
different from that approach. As observed by Zhou et al. (2007), sampling based on a random walk following out-links
performs worse than following in-links. Up to 60 % recall, DIRECTEDLEX performs best, both in the regime of 5 %
labels for training and in the regime of 20 % labels for training.

22

5 % labels for training

RandWalk along in-links
DirectedLex
RandWalk along out-links
DirectedLex, opposite link direction
UndirectedLex
Laplacian
Directed 1NN

0
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Recall

20 % labels for training

RandWalk along in-links
DirectedLex
RandWalk along out-links
DirectedLex, opposite link direction
UndirectedLex
Laplacian
Directed 1NN

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

i

i

n
o
s
c
e
r
P

i

i

n
o
s
c
e
r
P

0
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Recall

Figure 4: Recall and precision in the WebSpam classiﬁcation experiment. Each data point shown was computed as an average
over 100 runs. The largest standard deviation of the mean precision across the plotted recall values was less than 1.5 %. The
algorithm of Zhou et al. (2007) appears as RANDWALK (along in-links). We also show RANDWALK along out-links. Our directed
lex-minimization algorithm appears as DIRECTEDLEX. We also show DIRECTEDLEX with link directions reversed, along with
UNDIRECTEDLEX and LAPLACIAN.

D l0-Vertex Regularization Proofs

In this appendix, we prove Theorem 7.1 and Theorem 7.2. For the purposes of proving the second theorem, we intro-
duce an alternative version of problem (3). The optimization problem here requires us to minimize l0-regularization

23

budget required to obtain an inf-minimizer with gradient below a given threshold:

min
v∈IRn
subject to

(cid:13)
(cid:13)

v(T ) − v0(T )

0

gradG[v]

(cid:13)
∞ ≤ α.
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We will also need the following graph construction.

Deﬁnition D.1 The α-pressure terminal graph of a partially-labeled graph (G, v0) is a directed unweighted graph
Gα = (T (v0),

E if and only if there is a terminal path P from s to t in G with

E) such that (s, t) ∈

b

b

∇P (v0) > α.

Note that the α-pressure terminal graph has O(n) vertices but may be dense, even when G is not.

Algorithm 13: Algorithm TERM-PRESSURE: Given a well-posed instance (G, v0) and α ≥ 0, outputs α pressure terminal
graph Gα.
Initialize Gα with vertex set Vα = T (v0) and edge set
for each terminal s ∈ T (v0)

E = ∅.

1. Compute the distances to every other terminal t by running Dijktra’s algorithm, allowing shortest paths

b

2. Use the resulting distances to check for every other terminal t if there is a terminal path P from s to t with

that run through other terminals.

∇P (v0) > α. If there is, add edge (s, t) to

E.

Lemma D.2 The α-pressure terminal graph of a voltage problem (G, v0) can be computed in O((m + n log n)n) time
using algorithm TERM-PRESSURE (Algorithm 13).

b

Proof: The correctness of the algorithm follows from the fact that Dijkstra’s algorithm will identify all shortest
distances between the terminals, and the pressure check will ensure that terminal pairs (s, t) are added to
E if and
only if they are the endpoints of a terminal path P with ∇P (v0) > α. The running time is dominated by performing
Dijkstra’s algorithm once for each terminal. A single run of Dijkstra’s algorithm takes O(m + n log n) time, and this
✷
is performed at most n times, for a total running time of O((m + n log n)n).

b

We make three observations that will turn out to be crucial for proving Theorems 7.1 and 7.2.

Observation D.3 Gα is a subgraph of Gβ for α ≥ β.

Proof: Suppose edge (s, t) appears in Gα, then for some path P

∇P (v0) > α ≥ β,

so the edge also appears in Gβ.

Observation D.4 Gα is transitively closed.

Proof: Suppose edges (s, t) and (t, r) appear in Gα. Let P(s,t), P(t,r), P(s,r) be the respective shortest paths in G
between these terminal pairs. Then

∇P(s,r)(v0) =

v0(s) − v0(r)
ℓ(P(s,r))

≥

v0(s) − v0(r)
ℓ(P(s,t)) + ℓ(P(t,r))

=

v0(s) − v0(t) + v0(t) − v0(r)
ℓ(P(s,t)) + ℓ(P(t,r))

≥ min

v0(s) − v0(t)
ℓ(P(s,t))

,

 

v0(t) − v0(r)

ℓ(P(t,r)) !

> α.

So edge (s, r) also appears in Gα. This is sufﬁcient for Gα to be transitively closed.

24

(6)

✷

(7)

✷

Observation D.5 Gα is a directed acyclic graph.

Proof: Suppose for a contradiction that a directed cycle appears in Gα. Let s and t be two vertices in this cycle. Let
P(s,t) and P(t,s) be the respective shortest paths in G between these terminal pairs. Because Gα is transitively closed,
both edges (s, t) and (t, s) must appear in Gα. But (s, t) ∈

E implies

and similarly (t, s) ∈

E implies

b
This is a contradiction.

v0(s) − v0(t) > αℓ(P(s,t)) > 0,

b

v0(t) − v0(s) > αℓ(P(t,s)) > 0.

✷

The usefulness of the α-pressure terminal graph is captured in the following lemma. We deﬁne a vertex cover of a
directed graph to be a vertex set that constitutes a vertex cover in the same graph with all edges taken to be undirected.

Lemma D.6 Given a partially-labeled graph (G, v0) and a set U ⊆ V , there exists a voltage assignment v ∈ IRn that
satisﬁes

if and only if U is a vertex cover in the α-pressure terminal graph Gα of (G, v0).
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:9)

(cid:8)

t ∈ T (v0) : v(t) 6= v0(t)

⊆ U and

gradG[v]

∞ ≤ α,

Proof: We ﬁrst show the “only if” direction. Suppose for a contradiction that there exists a voltage assignment v for
which
∞ ≤ α, but U is not a vertex cover in Gα. Let (s, t) be an edge Gα which is not covered by U . The
presence of this edge in Gα implies that there exists a terminal path P from s to t in G for which

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∇P (v0) > α.

But, by Lemma 3.5 this means there is no assignment v for G which agrees with v0 on s and t and has
α. This contradicts our assumption.

∞ ≤
(cid:13)
Now we show the “if” direction. Consider an arbitrary vertex cover U of Gα. Suppose for a contradiction that
(cid:13)
⊆ U .

t ∈ T (v0) : v(t) 6= v0(t)

gradG[v]

(cid:13)
(cid:13)

gradG[v]

there does not exist a voltage assignment v for G with
Deﬁne a partial voltage assignment vU given by

∞ ≤ α and

(cid:8)

(cid:9)

vU (t) =

v0(t)
∗

(

(cid:13)
(cid:13)

(cid:13)
(cid:13)
if t ∈ T (v0) \ U
o.w.

∞ ≤ α. By
The preceding statement is equivalent to saying that there is no v that extends vU and has
Lemma 3.5, this means there is terminal path between s, t ∈ T (vU ) with gradient strictly larger than α. But this
means an edge (s, t) is present in Gα and is not covered. This contradicts our assumption that U is a vertex cover. ✷

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We are now ready to prove Theorem 7.2.

∞

(cid:13)
(cid:13)

Proof of Theorem 7.2: We describe and prove the algorithm OUTLIER. The algorithm will reduce problem (3)
to problem (6): Suppose v∗ is an optimal assignment for problem (3).
It achieves a maximum gradient α∗ =
gradG[v∗]
. Using Dijkstra’s algorithm we compute the pairwise shortest distances between all terminals in G.
From these distances and the terminal voltages, we compute the gradient on the shortest path between each terminal
(cid:13)
pair. By Lemma 3.5, α∗ must equal one of these gradients. So we can solve problem (3) by iterating over the set of
(cid:13)
gradients between terminals and solving problem (6) for each of these O(n2) gradients. Among the assignments with
v(T ) − v0(T )

0 ≤ k, we then pick the solution that minimizes
(cid:13)
(cid:13)

In fact, we can do better. By Observation D.3, Gα is a subgraph of Gβ for α ≥ β. This means a vertex cover
(cid:13)
of Gα is also a vertex cover of Gβ, and hence the minimum vertex cover for Gβ is at least as large as the minimum
(cid:13)
vertex cover for Gα. This means we can do a binary search on the set of O(n2) terminal gradients to ﬁnd the minimum
gradient for which there exists an assignment with
0 ≤ k. This way, we only make O(log n) calls to
v(T ) − v0(T )
problem (6), in order to solve problem (3).
(cid:13)
(cid:13)

We use the following algorithm to solve problem (6).

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∞

.

25

1. Compute the α-pressure terminal graph Gα of G using the algorithm TERM-PRESSURE.
2. Compute a minimum vertex cover U of Gα using the algorithm KONIG-COVER from Theorem 7.3.
3. Deﬁne a partial voltage assignment vU given by

vU (t) =

v0(t)
∗

(

if t ∈ T (v0) \ U,
otherwise.

4. Using Algorithm 5, compute voltages v that extend vU and output v.

From Lemma D.2, it follows that step 1 computes the α-pressure terminal graph in polynomial time. From The-
orem 7.3 it follows that step 2 computes the a minimum vertex cover of the α-pressure terminal graph in polynomial
time, because our observations D.4 and D.5 establish that the graph is a TC-DAG. From Lemma D.6 and Theorem 4.6,
it follows that the output voltages solve program (6).

✷

To prove Theorem 7.1, we use the standard greedy approximation algorithm for MIN-VC (Vazirani (2001)).

Theorem D.7 2-Approximation Algorithm for Vertex Cover. The following algorithm gives a 2-approximation to
the Minimum Vertex Cover problem on a graph G = (V, E).

0. Initialize U = ∅.
1. Pick an edge (u, v) ∈ E that is not covered by U .
2. Add u and v to the set U .
3. Repeat from step 1 if there are still edges not covered by U .
4. Output U .

We are now in a position to prove Theorem 7.1

Proof of Theorem 7.1: Given an arbitrary k and a partially-labeled graph (G, v0), let α∗ be the optimum value
of program (3). Observe that by Lemma D.6, this implies that Gα∗ has a vertex cover of size k. Given the partial
assignment v0, for every vertex set U , we deﬁne

vU (t) =

v0(t)
∗

(

if t ∈ T (v0) \ U
o.w.

We claim the following algorithm APPROX-OUTLIER outputs a voltage assignment v with

gradG[v]

∞ ≤ α∗

(cid:13)
(cid:13)

(cid:13)
(cid:13)

and

v(T ) − v0(T )

(cid:13)
(cid:13)

Algorithm APPROX-OUTLIER:

0 ≤ 2k.
(cid:13)
(cid:13)

0. Initialize U = ∅.
1. Using the algorithm STEEPESTPATH (Algorithm 7), ﬁnd a steepest terminal path in G w.r.t. vU . Denote
this path P and let s and t be its terminal endpoints. If there is no terminal path with positive gradient, skip
to step 4.

2. Add s and t to the set U .
3. If |U | ≤ 2k − 2 then repeat from step 1.
4. Using the algorithm COMPINFMIN (Algorithm 5), compute voltages v that extend vU and output v.

From the stopping conditions, it is clear that |U | ≤ 2k. If in step 1 we ever ﬁnd that no terminal paths have positive
∞ = 0 ≤ α∗, by Lemma 3.5. Similarly if we ﬁnd a steepest
gradient then our v that extends vU will have
(cid:13)
(cid:13)

gradG[v]

(cid:13)
(cid:13)

26

gradG[v]

∞ ≤ α∗.

∞ ≤ α∗.
path with gradient less than α∗ w.r.t. vU , then for this U there exists v that extends vU and has
This will continue to hold when if we add vertices to U . Therefore, for the ﬁnal U , there will exist an v that extends
vU and has

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

If we never ﬁnd a steepest terminal path P with ∇P (v0) ≤ α∗, then each steepest path we ﬁnd corresponds to an
edge in Gα∗ that is not yet covered by U and our algorithm in fact implements the greedy approximation algorithm
for vertex cover described in Theorem D.7. This implies that the ﬁnal U is a vertex cover of Gα∗ of size at most 2k.
∞ ≤ α∗. This
By Lemma D.6, this implies that there exists a voltage assignment u extending vU that has
implies by Theorem 4.6 that the v we output has
(cid:13)
(cid:13)
In all cases, the v we output extends vU , so

∞ ≤ α∗.

gradG[u]

(cid:13)
(cid:13)

✷

gradG[v]
v(T ) − v0(T )
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

0 ≤ |U | ≤ 2k.
(cid:13)
(cid:13)

E Proof of Hardness of l0 regularization for l2

We will prove Theorem 7.4, by a reduction from minimum bisection. To this end, let G = (V, E) be any graph. We
will reduce the minimum bisection problem on G to our regularization problem. Let n = |V |. The graph on which we
will perform regularization will have vertex set

V ∪

V ,

V is a set of n vertices that are in 1-to-1 correspondence with V . We assume that every edge in G has weight 1.
V to the corresponding vertex in V by an edge of weight B, for some large B to be
V to each other by edges of weight B3. So, we have a complete
V to V , and the original graph G on V .

where
We now connect every vertex in
determined later. We also connect all of the vertices in
graph of weight B3 edges on
b
The input potential function will be

V , a matching of weight B edges connecting

b

b

b

v(a) =

b
0 for a ∈
1 for a ∈ V .
b

(

V , and

b

Now set k = n/2. We claim that we will be able to determine the value of the minimum bisection from the solution
to the regularization problem.

If S is the set of vertices on which v and w differ, then we know that the w is harmonic on S: for every a ∈ S,

w(a) is the weighted average of the values at its neighbors. In the following, we exploit the fact that |S| ≤ n/2.

Claim E.1 For every a ∈ S ∩

V , w(a) ≤ 2/nB2.

Proof: Let a be the vertex in S ∩
w-value equal to 0 by edges of weight B3. On the other hand, a has only one neighbor that is not in
w-value at most 1, and it is connected to that vertex by an edge of weight B. Call that vertex c. We have

V that maximizes w(a). So, a is connected to at least n/2 neighbors in

V with
V , that vertex has

b

b

b

((n − 1)B3 + B)w(a) = Bw(c) +

B3w(b)

b

b
V ,b6=a
Xb∈

= Bw(c) +

B3w(b) +

B3w(b)

b
V ∩S,b6=a
Xb∈

B3w(a)

≤ B +

b
V ∩S,b6=a
Xb∈
≤ B + (n/2 − 1)B3w(a).

b
V −S
Xb∈

Subtracting (n/2 − 1)B3w(a) from both sides gives

((n/2)B3 + B)w(a) ≤ B,

which implies the claim.

Claim E.2 For a ∈ S ∩ V , w(a) ≤ n/B.

27

✷

V . Let’s call that neighbor c. We know that w(c) ≤ 2/B2n. On the
Proof: Vertex a has exactly one neighbor in
other hand, vertex a has fewer than n − 1 neighbors in V , and each of these have w-value at most 1. Let da denote the
degree of a in G. Then,

b

So,

Let

and

bisection.

and at most

(B + da)w(a) ≤ da + B

2
B2n

.

w(a) ≤

da + 2/Bn
da + B
n + (2/Bn)
B + n

≤

≤ n/B.

|S| = k = n/2.

T = S ∩ V,

t = |T | .

(n − t)B − 4/B
b

(n − t)B + tn2/B.

We now estimate the value of the regularized objective function. To this end, we assume that

We will prove that S ⊂ V and so S = T and t = n/2.

Let δ denote the number of edges on the boundary of T in V . Once we know that t = n/2, δ is the size of a

Claim E.3 The contribution of the edges between V and

V to the objective function is at least

Proof: For the lower bound, we just count the edges between vertices in V \ T and
edges, and each of them has weight B. The endpoint in V \ T has w-value 1, and the endpoint in
most 2/nB2. So, the contribution of these edges is at least

V . There are n − t of these
V has w-value at

b

(n − t)B(1 − 2/nB2)2 ≥ (n − t)B(1 − 4/nB2) ≥ (n − t)B − 4/B.

b

For the upper bound, we observe that the difference in w-values across each of these n − t edges is at most 1, so their
total contribution is at most

Since for every vertex a ∈ T , w(a) ≤ n/B, and also every vertex b ∈
edges between T and

V is at most

t(n/B)2B = tn2/B.

b

b

V , w(b) ≤ 2/nB2, the contribution due to

We will see that this is the dominant term in the objective function. The next-most important term comes from the

edges in G.

(n − t)B.

28

✷

✷

Claim E.4 The contribution of the edges in G to the objective function is at least

and at most

δ(1 − 2n/B)

δ + (t2/2)(n/B)2

δ(1 − 2n/B) and δ.

(t2/2)(n/B)2.

Proof: Let (a, b) ∈ E. If neither a nor b is in T , then w(a) = w(b) = 1, and so this edge has no contribution. If
a ∈ T but b 6∈ T , then the difference in w-values on them is between (1 − n/B) and 1. So, the contribution of such
edges to the objective function is between

Finally, if a and b are in T , then the difference in w-values on them is at most n/B, and so the contribution of all such
edges to the objective function is at most

Claim E.5 The edges between pairs of vertices in

V contribute at most 2/B to the objective function.

Proof: As 0 ≤ w(a) ≤ 2/B2n for every a ∈

V , every edge between two vertices in

V can contribute at most

b

As there are fewer than n2/2 such edges, their total contribution to the objective function is at most

B3(2/B2n)2 = 4/Bn2.
b

b

(n2/2)(4/Bn2) = 2/B.

Lemma E.6 If n ≥ 4 and B = 2n3, the value of the objective function is at least

and at most

(n − t)B + δ − 1/2

(n − t)B + δ + 1/3.

Proof: Summing the contributions in the preceding three claims, we see that the value of the objective function is at
least

(n − t)B − 4/B + δ(1 − 2n/B) ≥ (n − t)B + δ − 4/B − 2nδ/B

≥ (n − t)B + δ − n3/B
≥ (n − t)B + δ − 1/2,

as δ ≤ (n/2)2.

Similarly, the objective function is at most

(n − t)B + tn2/B + δ + (t2/2)(n/B)2 + 2/B ≤ (n − t)B + n3/2B + δ + n4/8B2 + 2/B
≤ (n − t)B + n3/2B + δ + 1/32n2 + 1/n3
≤ (n − t)B + δ + 1/3.

Claim E.7 If n ≥ 2 and B = 2n3, then S ⊂ V .

Proof: The objective function is minimized by making t as large as possible, so t = n/2 and S ⊂ V .

29

✷

✷

✷

✷

Theorem E.8 The value of the objective function reveals the value of the minimum bisection in G.

Proof: The value of the objective function will be between

and

(n/2)B + δ − 1/2

(n/2)B + δ + 1/3.

So, the objective function will be smallest when δ is as small as possible.

✷

Theorem E.8 immediately implies Theorem 7.4.

30

5
1
0
2
 
n
u
J
 
0
3
 
 
]

G
L
.
s
c
[
 
 
2
v
0
9
2
0
0
.
5
0
5
1
:
v
i
X
r
a

Algorithms for Lipschitz Learning on Graphs ∗†

Rasmus Kyng
Yale University
rasmus.kyng@yale.edu

Anup Rao
Yale University
anup.rao@yale.edu

Sushant Sachdeva
Yale University
sachdeva@cs.yale.edu

Daniel A. Spielman
Yale University
spielman@cs.yale.edu

July 1, 2015

Abstract

We develop fast algorithms for solving regression problems on graphs where one is given the value of a function
at some vertices, and must ﬁnd its smoothest possible extension to all vertices. The extension we compute is the
absolutely minimal Lipschitz extension, and is the limit for large p of p-Laplacian regularization. We present an
algorithm that computes a minimal Lipschitz extension in expected linear time, and an algorithm that computes
an absolutely minimal Lipschitz extension in expected time eO(mn). The latter algorithm has variants that seem
to run much faster in practice. These extensions are particularly amenable to regularization: we can perform l0-
regularization on the given values in polynomial time and l1-regularization on the initial function values and on graph
edge weights in time eO(m3/2).

Our deﬁnitions and algorithms naturally extend to directed graphs.

1 Introduction

We consider a problem in which we are given a weighted undirected graph G = (V, E, ℓ) and values v0 : T → R
on a subset T of its vertices. We view the weights ℓ as indicating the lengths of edges, with shorter length indicating
greater similarity. Our goal it to assign values to every vertex v ∈ V \T so that the values assigned are as smooth as
possible across edges. A minimal Lipschitz extension of v0 is a vector v that minimizes

max
(x,y)∈E

(ℓ(x, y))−1

v(x) − v(y)

,

(cid:12)
(cid:12)

(cid:12)
(cid:12)

subject to v(x) = v0(x) for all x ∈ T . We call such a vector an inf-minimizer. Inf-minimizers are not unique. So,
among inf-minimizers we seek vectors that minimize the second-largest absolute value of ℓ(x, y)−1
v(x) − v(y)
across edges, and then the third-largest given that, and so on. We call such a vector v a lex-minimizer. It is also known
(cid:12)
as an absolutely minimal Lipschitz extension of v0.
(cid:12)
These are the limit of the solution to p-Laplacian minimization problems for large p, namely the vectors that solve

(cid:12)
(cid:12)

(1)

(2)

min
v∈Rn

v|T =v0|T X(x,y)∈E

(ℓ(x, y))−p|v(x) − v(y)|p.

The use of p = 2 was suggested in the foundational paper of Zhu et al. (2003), and is particularly nice because it can
be obtained by solving a system of linear equations in a symmetric diagonally dominant matrix, which can be done

∗This research was partially supported by AFOSR Award FA9550-12-1-0175, NSF grant CCF-1111257, a Simons Investigator Award to Daniel

Spielman, and a MacArthur Fellowship.

†Code used in this work is available at https://github.com/danspielman/YINSlex

1

very quickly (Cohen et al. (2014)). The use of larger values of p has been discussed by Alamgir and Luxburg (2011),
and by Bridle and Zhu (2013), but it is much more complicated to compute. The fastest algorithms we know for this
problem require convex programming, and then require very high accuracy to obtain the values at most vertices. By
taking the limit as p goes to inﬁnity, we recover the lex-minimizer, which we will show can be computed quickly.

The lex-minimization problem has a remarkable amount of structure. For example, in uniformly weighted graphs
the value of the lex-minimizer at every vertex not in T is equal to the average of the minimum and maximum of the
values at its neighbors. This is analogous to the property of the 2-Laplacian minimizer that the value at every vertex
not in T equals the average of the values at its neighbors.

1.1 Contributions

We ﬁrst present several important structural properties of lex-minimizers in Section 3.2. As we shall point out, some
of these were known from previous work, sometimes in restricted settings. We state them generally and prove them
for completeness. We also prove that the lex-minimizer is as stable as possible under perturbations of v0 (Section 3.1).
The structure of the lex-minimization problem has led us to develop elegant algorithms for its solution. Both the
algorithms and their analyses could be taught to undergraduates. We believe that these algorithms could be used in
place of 2-Laplacian minimization in many applications.

We present algorithms for the following problems. Throughout, m = |E| and n = |V |.

Inf-minimization: An algorithm that runs in expected time O(m + n log n) (Section 4.3).

Lex-minimization: An algorithm that runs in expected time O(n(m + n log n)) (Section 4), along with a variant that

runs quickly in practice (Section 4.4).

l1-regularization of edge lengths for inf-minimization: The problem of minimizing (1) given a limited budget with
O(m3/2)
which one can increase edge lengths is a linear programming problem. We show how to solve it in time
with an interior point method by using fast Laplacian solvers (Section 8). The same algorithm can accommodate
l1-regularization of the values given in v0.

e

l0-regularization of vertex values for inf-minimization: We give a polynomial time algorithm for l0-regularization
of the values at vertices. That is, we minimize (1) given a budget of a number of vertices that can be proclaimed
outliers and removed from T (Section 7.1). We solve this problem by reducing it to the problem of computing
minimum vertex covers on transitively closed directed acyclic graphs, a special case of minimum vertex cover
that can be solved in polynomial time.

After any regularization for inf-minimization, we suggest computing the lex-minimizer. We ﬁnd the result for l0-
regularization of vertex values to be particularly surprising, especially because we prove that the analogous problem
for 2-Laplacian minimization is NP-Hard (Section 7.2).

All of our algorithms extend naturally to directed graphs (Section 5). This is in contrast with the problem of
minimizing 2-Laplacians on directed graphs, which corresponds to computing electrical ﬂows in networks of resistors
and diodes, for which fast algorithms are not presently known.

We present a few experiments on examples demonstrating that the lex-minimizer can overcome known deﬁcien-
cies of the 2-Laplacian minimizer (Section 1.2, Figures 1,2), as well as a demonstration of the performance of the
directed analog of our algorithms on the WebSpam dataset of Castillo et al. (2006) (Section 6). In the WebSpam prob-
lem we use the link structure of a collection of web sites to ﬂag some sites as spam, given a small number of labeled
sites known to be spam or normal.

1.2 Relation to Prior Work

We ﬁrst encountered the idea of using the minimizer of the 2-Laplacian given by (2) for regression and classiﬁca-
tion on graphs in the work of Zhu et al. (2003) and Belkin et al. (2004) on semi-supervised learning. These works
transformed learning problems on sets of vectors into problems on graphs by identifying vectors with vertices and
constructing graphs with edges between nearby vectors. One shortcoming of this approach (see Nadler et al. (2009),

2

e
g
a

t
l

 

o
V
d
e
r
r
e

f

n

I

1

0.8

0.6

0.4

0.2

0

-0.2

-0.4

-0.6

-0.8

-1

-4

50 lex
50 l2
100 lex
100 l2
500 lex
500 l2
1000 lex
1000 l2

0.25

0.2

r
o
r
r
e
 
1
l
 
n
a
e
M

0.15

0.1

0.05

0
5000 

lex
2-Lap
labels

-2

0

2
Vertex position on real line

4

6

8

Figure 1: Lex vs 2-Laplacian on 1D gaussian clus-
ters.

Figure 2: kNN graphs on samples from 4D cube.

10000

20000

40000

80000

Number of Vertices

Alamgir and Luxburg (2011), Bridle and Zhu (2013)) is that if the number of vectors grows while the number of la-
beled vectors remains ﬁxed, then almost all the values of the 2-Laplacian minimizer converge to the mean of the
labels on most natural examples. For example, Nadler et al. (2009) consider sampling points from two Gaussian
distributions centered at 0 and 4 on the real line. They place edges between every pair of points (x, y) with length
exp(|x − y|2 /2σ2) for σ = 0.4, and provide only the labels v0(0) = −1 and v0(4) = 1. Figure 1 shows the values
of the 2-Laplacian minimizer in red, which are all approximately zero. In contrast, the values of the lex-minimizer in
blue, which are smoothly distributed between the labeled points, are shown.

The “manifold hypothesis” (see Chapelle et al. (2010), Ma and Fu (2011)) holds that much natural data lies near a
low-dimensional manifold and that natural functions we would like to learn on this data are smooth functions on the
manifold. Under this assumption, one should expect lex-minimizers to interpolate well. In contrast, the 2-Laplacian
minimizers degrade (dotted lines) if the number of labeled points remains ﬁxed while the total number of points grows.
In Figure 2, we demonstrate this by sampling many points uniformly from the unit cube in 4 dimensions, form their
8-nearest neighbor graph, and consider the problem of regressing the ﬁrst coordinate. We performed 8 experiments,
varying the number of labeled points in {50, 100, 500, 1000}. Each data point is the mean average l1 error over 100
experiments. The plots for root mean squared error are similar. The standard deviation of the estimations of the mean
are within one pixel, and so are not displayed. The performance of the lex-minimizer (solid lines) does not degrade as
the number of unlabeled points grows.

Analogous to our inf-minimizers, minimal Lipschitz extensions of functions in Euclidean space and over more
general metric spaces have been studied extensively in Mathematics (Kirszbraun (1934), McShane (1934), Whitney
(1934)). von Luxburg and Bousquet (2003) employ Lipschitz extensions on metric spaces for classiﬁcation and relate
these to Support Vector Machines. Their work inspired improvements in classiﬁcation and regression in metric spaces
with low doubling dimension (Gottlieb et al. (2013), Gottlieb et al. (2013b)). Theoretically fast, although not actually
practical, algorithms have been given for constructing minimal Lipschitz extensions of functions on low-dimensional
Euclidean spaces (Fefferman (2009a), Fefferman and Klartag (2009), Fefferman (2009b)). Sinop and Grady (2007)
suggest using inf-minimizers for binary classiﬁcation problems on graphs. For this special case, where all of the
given values are either 0 or 1, they present an O(m + n log n) time algorithm for computing an inf-minimizer. The
case of general given values, which we solve in this paper, is much more complicated. To compensate for the non-
uniqueness of inf-minimizers, they suggest choosing the inf-minimizer that minimizes (2) with p = 2. We believe that
the lex-minimizer is a more natural choice.

The analog of our lex-minimizer over continuous spaces is called the absolutely minimal Lipschitz extension
(AMLE). Starting with the work of Aronsson (1967), there have been several characterizations and proofs of the ex-
istence and uniqueness of the AMLE (Jensen (1993), Crandall et al. (2001), Barles and Busca (2001), Aronsson et al.
(2004)). Many of these results were later extended to general metric spaces, including graphs (Milman (1999),
Peres et al. (2011), Naor and Shefﬁeld (2010), Shefﬁeld and Smart (2010)). However, to the best of our knowledge,
fast algorithms for computing lex-minimizers on graphs were not known. For the special case of undirected, un-
weighted graphs, Lazarus et al. (1999) presented both a polynomial-time algorithm and an iterative method. Oberman

3

(2011) suggested computing the AMLE in Euclidean space by ﬁrst discretizing the problem and then solving the cor-
responding graph problem by an iterative method. However, no run-time guarantees were obtained for either iterative
method.

2 Notation and Basic Deﬁnitions

Lexicographic Ordering. Given a vector r ∈ Rm, let πr denote a permutation that sorts r in non-increasing order
by absolute value, i.e., ∀i ∈ [m − 1], |r(πr(i))| ≥ |r(πr(i + 1))|. Given two vectors r, s ∈ Rm, we write r (cid:22) s to
indicate that r is smaller than s in the lexicographic ordering on sorted absolute values, i.e.

∃j ∈ [m],

r(πr(j))

<

s(πs(j))

and ∀i ∈ [j − 1],

r(πr(i))

=

s(πs(i))

or ∀i ∈ [m],

=

r(πr(i))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.
(cid:12)
(cid:12)

s(πs(i))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

Note that it is possible that r (cid:22) s and s (cid:22) r while r 6= s. It is a total relation: for every r and s at least one of r (cid:22) s
or s (cid:22) r is true.

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Graphs and Matrices. We will work with weighted graphs. Unless explicitly stated, we will assume that they are
undirected. For a graph G, we let VG be its set of vertices, EG be its set of edges, and ℓG : EG → R+ be the
assignment of positive lengths to the edges. We let |VG| = n, and |EG| = m. We assume ℓG is symmetric, i.e.,
ℓG(x, y) = ℓG(y, x). When G is clear from the context, we drop the subscript.

A path P in G is an ordered sequence of (not necessarily distinct) vertices P = (x0, x1, . . . , xk), such that
(xi−1, xi) ∈ E for i ∈ [k]. The endpoints of P are denoted by ∂0P = x0, ∂1P = xk. The set of interior vertices
of P is deﬁned to be int(P ) = {xi : 0 < i < k}. For 0 ≤ i < j ≤ k, we use the notation P [xi : xj] to denote the
k
subpath (xi, . . . , xj). The length of P is ℓ(P ) =
i=1 ℓ(xi−1, xi).
A function v0 : V → R ∪ {∗} is called a voltage assignment (to G). A vertex x ∈ V is a terminal with
respect to v0 iff v0(x) 6= ∗. The other vertices, for which v0(x) = ∗, are non-terminals. We let T (v0) denote the
set of terminals with respect to v0. If T (v0) = V, we call v0 a complete voltage assignment (to G). We say that an
assignment v : V → R ∪ {∗} extends v0 if v(x) = v0(x) for all x such that v0(x) 6= ∗.

Given an assignment v0 : V → R ∪ {∗}, and two terminals x, y ∈ T (v0) for which (x, y) ∈ E, we deﬁne the

P

gradient on (x, y) due to v0 to be

gradG[v0](x, y) =

v0(x) − v0(y)
ℓ(x, y)

.

It may be useful to view gradG[v0](x, y) as the current in the edge (x, y) induced by voltages v0. When v0 is a
complete voltage assignment, we interpret gradG[v0] as a vector in Rm, with one entry for each edge. However, for
convenience, we deﬁne gradG[v0](x, y) = −gradG[v0](y, x). When G is clear from the context, we drop the subscript.
A graph G along with a voltage assignment v to G is called a partially-labeled graph, denoted (G, v). We say
that a partially-labeled graph (G, v0) is a well-posed instance if for every maximal connected component H of G, we
have T (v0) ∩ VH 6= ∅.

A path P in a partially-labeled graph (G, v0) is called a terminal path if both endpoints are terminals. We deﬁne

∇P (v0) to be its gradient:

∇P (v0) =

v0(∂0P ) − v0(∂1P )
ℓ(P )

.

If P contains no terminal-terminal edges (and hence, contains at least one non-terminal), it is a free terminal path.

Lex-Minimization. An instance of the LEX-MINIMIZATION problem is described by a partially-labeled graph
(G, v0). The objective is to compute a complete voltage assignment v : VG → R extending v0 that lex-minimizes
grad[v].

Deﬁnition 2.1 (Lex-minimizer) Given a partially-labeled graph (G, v0), we deﬁne lexG[v0] to be a complete voltage
assignment to V that extends v0, and such that for every other complete assignment v′ : VG → R that extends v0, we
have gradG[lexG[v0]] (cid:22) gradG[v′]. That is, lexG[v0] achieves a lexicographically-minimal gradient assignment to the
edges.

We call lexG[v0] the lex-minimizer for (G, v0). Note that if T (v0) = VG, then trivially, lexG[v0] = v0.

4

3 Basic Properties of Lex-Minimizers

Lazarus et al. (1999) established that lex-minimizers in unweighted and undirected graphs exist, are unique, and may
be computed by an elementary meta-algorithm. We state and prove these facts for undirected weighted graphs, and
defer the discussion of the directed case to Section 5. We also state for directed and weighted graphs characterizations
of lex-minimizers that were established by Peres et al. (2011), Naor and Shefﬁeld (2010) and Shefﬁeld and Smart
(2010) for unweighted graphs. These results are essential for the analyses of our algorithms. We defer most proofs to
Appendix A.

Deﬁnition 3.1 A steepest ﬁxable path in an instance (G, v0) is a free terminal path P that has the largest gradient
∇P (v0) amongst such paths.

Observe that a steepest ﬁxable path with ∇P (v0) 6= 0 must be a simple path.
Deﬁnition 3.2 Given a steepest ﬁxable path P in an instance (G, v0), we deﬁne ﬁxG[v0, P ] : VG → R ∪ {∗} to be the
voltage assignment deﬁned as follows

ﬁxG[v0, P ](x) =

v0(∂0P ) − ∇P (v0) · ℓG(P [∂0P : x]) x ∈ int(P ) \ T (v0),
v0(x)

otherwise.

(

We say that the vertices x ∈ int(P ) are ﬁxed by the operation ﬁx[v0, P ]. If we deﬁne v1 = ﬁxG[v0, P ], where
P = (x0, . . . , xr) is the steepest ﬁxable path in (G, v0), then it is easy to argue that for every i ∈ [r], we have
grad[v1](xi−1, xi) = ∇P (see Lemma A.5). The meta-algorithm META-LEX, spelled out as Algorithm 1, entails
repeatedly ﬁxing steepest ﬁxable paths. While it is possible to have multiple steepest ﬁxable paths, the result of ﬁxing
all of them does not depend on the order in which they are ﬁxed.

Theorem 3.3 Given a well-posed instance (G, v0), the meta-algorithm META-LEX, which repeatedly ﬁxes steepest
ﬁxable paths, produces the unique lex-minimizer extending v0.

Corollary 3.4 Given a well-posed instance (G, v0) such that T (v0) 6= VG, let P be a steepest ﬁxable path in (G, v0).
Then, (G, ﬁx[v0, P ]) is also a well-posed instance, and lexG[ﬁx[v0, P ]] = lexG[v0].

Since a lex-minimal element must be an inf-minimizer, we also obtain the following corollary, that can also be

proved using LP duality.

Lemma 3.5 Suppose we have a well-posed instance (G, v0). Then, there exists a complete voltage assignment v
extending v0 such that

grad[v]

∞ ≤ α, iff every terminal path P in (G, v0) satisﬁes ∇P (v0) ≤ α.
(cid:13)
(cid:13)

3.1 Stability

(cid:13)
(cid:13)

The following theorem states that lexG[v0] is monotonic with respect to v0 and it respects scaling and translation of
v0.

Theorem 3.6 Let (G, v0) be a well-posed instance with T := T (v0) as the set of terminals. Then the following
statements hold.

1. For any c, d ∈ R, v1 a partial assignment with terminals T (v1) = T and v1(t) = cv0(t) + d for all t ∈ T .

Then, lexG[v1](i) = c · lexG[v0](i) + d for all i ∈ VG.

2. v1 a partial assignment with terminals T (v1) = T. Suppose further that v1(t) ≥ v0(t) for all t ∈ T. Then,

lexG[v1](i) ≥ lexG[v0](i) for all i ∈ VG.

As a corollary, the above theorem gives a nice stability property that lex-minimal elements satisfy.

Corollary 3.7 Given well-posed instances (G, v0), (G, v1) such that T := T (v0) = T (v1), let ǫ := maxt∈T |v0(t) −
v1(t)|. Then |lexG[v0](i) − lexG[v1](i)| ≤ ǫ for all i ∈ VG.

5

3.2 Alternate Characterizations

There are at least two other seemingly disparate deﬁnitions that are equivalent to lex-minimal voltages.

lp-norm Minimizers. As mentioned in the introduction, for a well-posed instance (G, v0) the lex-minimizer is also
the limit of lp minimizers. This follows from existing results about the limit of lp-minimizers (Egger and Huotari
(1990)) in afﬁne spaces, since {grad[v] | v is complete, v extends v0} forms an afﬁne subspace of Rm. Thus, we have
the following theorem:

Theorem 3.8 (Limit of lp-minimizers, follows from Egger and Huotari (1990)) For any p ∈ (1, ∞), given a well-
posed instance (G, v0) deﬁne vp to be the unique complete voltage assignment extending v0 and minimizing
p ,
i.e.

grad[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Then, limp→∞ vp = lexG[v0].

vp = arg min
v is complete
v extends v0 (cid:13)
(cid:13)

grad[v]

p .

(cid:13)
(cid:13)

Max-Min Gradient Averaging. Consider a well-posed instance (G, v0), and a complete voltage assignment v ex-
tending v0. If G is such that ℓ(e) = 1 for all e ∈ EG, it is easy to see that lex = lexG[v0] satisﬁes the following simple
condition for all x ∈ VG \ T (v0),

lex(x) =

1
2  

max
(x,y)∈EG

lex(y) + min

lex(z)

.

(x,z)∈EG

!

This condition should be contrasted to the optimality condition for l2-regularization on these instances, which gives
for all non-terminals x, the optimal voltage v satisﬁes v(x) = 1

y:(x,y)∈EG v(y).

deg(x)

To prove the above claim, consider locally changing lex at x and observe that the gradients of edges not incident
at x remain unchanged, and at least one of edges incident at x will have a strictly larger gradient, contradicting lex-
minimality. For general graphs, this condition of local optimality can still be characterized by a simple max-min
gradient averaging property as described below.

P

Deﬁnition 3.9 (Max-Min Gradient Averaging) Given a well-posed instance (G, v0), and a complete voltage as-
signment v extending v0, we say that v satisﬁes the max-min gradient averaging property (w.r.t. (G, v0)) if for every
x ∈ VG \ T (v0), we have

grad[v](x, y) = − min

grad[v](x, y).

max
y:(x,y)∈EG

y:(x,y)∈EG

As stated in the theorem below, lexG[v0] is the unique assignment satisfying max-min gradient averaging property.
Shefﬁeld and Smart (2010) proved a variant of this statement for weighted graphs. For completeness, we present a
proof in the appendix.

Theorem 3.10 Given a well-posed instance (G, v0), lexG[v0] satisﬁes max-min gradient averaging property. More-
over, it is the unique complete voltage assignment extending v0 that satisﬁes this property w.r.t. (G, v0).

An advantage of this characterization is that it can be veriﬁed quickly. This is particularly useful for implementations
for computing the lex-minimizer.

4 Algorithms

We now sketch the ideas behind our algorithms and give precise statements of our results. A full description of all the
algorithms is included in the appendix.

We deﬁne the pressure of a vertex to be the gradient of the steepest terminal path through it:

pressure[v0](x) = max{∇P (v0) | P is a terminal path in (G, v0) and x ∈ P }.

6

Observe that in a graph with no terminal-terminal edges, a free terminal path is a steepest ﬁxable path iff its gradient
is equal to the highest pressure amongst all vertices. Moreover, vertices that lie on steepest ﬁxable paths are exactly
the vertices with the highest pressure. For a given α > 0, in order to identify vertices with pressure exceeding α, we
compute vectors vHigh[α](x) and vLow[α](x) deﬁned as follows in terms of dist, the metric on V induced by ℓ:

vLow[α](x) = min

{v0(t) + α · dist(x, t)}

t∈T (v0)

vHigh[α](x) = max
t∈T (v0)

{v0(t) − α · dist(t, x)}.

4.1 Lex-minimization on Star Graphs

We ﬁrst consider the problem of computing the lex-minimizer on a star graph in which every vertex but the center is a
terminal. This special case is a subroutine in the general algorithm, and also motivates some of our techniques.

Let x be the center vertex, T be the set of terminals, and all edges be of the form (x, t) with t ∈ T . The initial
voltage assignment is given by v : T → R, and we abbreviate dist(x, t) by d(t) = ℓ(x, t). From Corollary 3.4 we know
that we can determine the value of the lex minimizer at x by ﬁnding a steepest ﬁxable path. By deﬁnition, we need to
ﬁnd t1, t2 ∈ T that maximize the gradient of the path from t1 to t2, ∇(t1, t2) = v(t1)−v(t2)
d(t2)+d(t2) . As observed above, this
is equivalent to ﬁnding a terminal with the highest pressure. We now present a simple randomized algorithm for this
problem that runs in expected linear time.

Given a terminal t1, we can compute its pressure α along with the terminal t2 such that |∇(t1, t2)| = α in time
O(|T |) by scanning over the terminals in T . Consider doing this for a random terminal t1. We will show that in linear
time one can then ﬁnd the subset of terminals T ′ ⊂ T whose pressure is greater than α. Assuming this, we complete
the analysis of the algorithm. If T ′ = ∅, t1 is a vertex with highest pressure. Hence the path from t1 to t2 is a steepest
ﬁxable path, and we return (t1, t2). If T ′ 6= ∅, the terminal with the highest pressure must be in T ′, and we recurse by
picking a new random t1 ∈ T ′. As the size of T ′ will halve in expectation at each iteration, the expected time of the
algorithm on the star is O(|T |).

To determine which terminals have pressure exceeding α, we observe that the condition ∃t2 : α < ∇(t1, t2) =
v(t1)−v(t2)
d(t1)+d(t2) , is equivalent to ∃t2 : v(t2)+αd(t2) < v(t1)−αd(t1). This, in turn, is equivalent to vLow[α](x) < v(t1)−
αd(t1). We can compute vLow[α](x) in deterministic O(|T |) time. Similarly, we can check if ∃t2 : α < ∇(t2, t1) by
checking if vHigh[α](x) > vt1 + αd(t1). Thus, in linear time, we can compute the set T ′ of terminals with pressure
exceeding α. The above algorithm is described in Algorithm 10.

Theorem 4.1 Given a set of terminals T, initial voltages v : T → R, and distances d : T → R+, STARSTEEPESTPATH(T, v, d)
returns (t1, t2) maximizing v(t1)−v(t2)

d(t1)+d(t2) , and runs in expected time O(|T |).

4.2 Lex-minimization on General Graphs

Theorem 3.3, tells us that META-LEX will compute lex-minimizers given an algorithm for ﬁnding a steepest ﬁxable
path in (G, v0). Recall that ﬁnding a steepest ﬁxable path is equivalent to ﬁnding a path with gradient equal to the
highest pressure amongst all vertices. In this section, we show how to do this in expected time O(m + n log n).

We describe an algorithm VERTEXSTEEPESTPATH that ﬁnds a terminal path P through any vertex x such that
∇P (v0) = pressure[v0](x) in expected O(m + n log n) time. Using Dijkstra’s algorithm, we compute dist(x, t) for
all t ∈ T. If x ∈ T (v0), then there must be a terminal path P that starts at x that has ∇P (v0) = pressure[v0](x). To
compute such a P we examine all t ∈ T (v0) in O(|T |) time to ﬁnd the t that maximizes |∇(x, t)| = |v(x)−v(t)|
, and
dist(x,t)
then return a shortest path between x and that t.

If x /∈ T (v0), then the steepest path through x between terminals t1 and t2 must consist of shortest paths between
x and t1 and between x and t2. Thus, we can reduce the problem to that of ﬁnding the steepest path in a star graph
where x is the only non-terminal and is connected to each terminal t by an edge of length dist(x, t). By Theorem 4.1,
we can ﬁnd this steepest path in O(|T |) expected time. The above algorithm is formally described as Algorithm 9.

Theorem 4.2 Given a well-posed instance (G, v0), and a vertex x ∈ VG, VERTEXSTEEPESTPATH(G, v0, x) returns
a terminal path P through x such that ∇P (v0) = pressure[v0](x), in O(m + n log n) expected time.

7

As in the algorithm for the star graph, we need to identify the vertices whose pressure exceeds a given α. For a ﬁxed
α, we can compute vLow[α](x) and vHigh[α](x) for all x ∈ VG using a simple modiﬁcation of Dijkstra’s algorithm in
O(m + n log n) time. We describe the algorithms COMPVHIGH, COMPVLOW for these tasks in Algorithms 3 and 4.
The following lemma encapsulates the usefulness of vLow and vHigh.

Lemma 4.3 For every x ∈ VG, pressure[v0](x) > α iff vHigh[α](x) > vLow[α](x).

It immediately follows that the algorithm COMPHIGHPRESSGRAPH(G, v0, α) described in Algorithm 6 computes

the vertex induced subgraph on the vertex set {x ∈ VG| pressure[v0](x) > α}.

We can combine these algorithms into an algorithm STEEPESTPATH that ﬁnds the steepest ﬁxable path in (G, v0)
in O(m + n log n) expected time. We may assume that there are no terminal-terminal edges in G. We sample an edge
(x1, x2) uniformly at random from EG, and a terminal x3 uniformly at random from VG. For i = 1, 2, 3, we compute
the steepest terminal path Pi containing xi. By Theorem 4.2, this can be done in O(m + n log n) expected time. Let α
be the largest gradient maxi ∇Pi. As mentioned above, we can identify G′, the induced subgraph on vertices x with
pressure exceeding α, in O(m + n log n) time. If G′ is empty, we know that the path Pi with largest gradient is a
steepest ﬁxable path. If not, a steepest ﬁxable path in (G, v0) must be in G′, and hence we can recurse on G′. Since
we picked a uniformly random edge, and a uniformly random vertex, the expected size of G′ is at most half that of G.
Thus, we obtain an expected running time of O(m + n log n). This algorithm is described in detail in Algorithm 7.

Theorem 4.4 Given a well-posed instance (G, v0) with EG ∩ (T (v0) × T (v0)) = ∅, STEEPESTPATH(G, v0) returns
a steepest ﬁxable path in (G, v0), and runs in O(m + n log n) expected time.

By using STEEPESTPATH in META-LEX, we get the COMPLEXMIN, shown in Algorithm 1. From Theorem 3.3 and
Theorem 4.4, we immediately get the following corollary.

Corollary 4.5 Given a well-posed instance (G, v0) as input, algorithm COMPLEXMIN computes a lex-minimizing
assignment that extends v0 in O(n(m + n log n)) expected time.

4.3 Linear-time Algorithm for Inf-minimization

Given the algorithms in the previous section, it is straightforward to construct an inﬁnity minimizer. Let α⋆ be the
gradient of the steepest terminal path. From Lemma 3.5, we know that the norm of the inf minimizer is α⋆. Considering
all trivial terminal paths (terminal-terminal edges), and using STEEPESTPATH, we can compute α⋆ in randomized
O(m+n log n) time. It is well known (McShane (1934); Whitney (1934)) that v1 = vLow[α⋆] and v2 = vHigh[α⋆] are
inf-minimizers. It is also known that 1
2 (v1 + v2) is the inf-minimizer that minimizes the maximum ℓ∞-norm distance
to all inf-minimizers. In the case of path graphs, this was observed by Gaffney and Powell (1976) and independently
by Micchelli et al. (1976). For completeness, the algorithm is presented as Algorithm 5, and we have the following
result.

Theorem 4.6 Given a well-posed instance (G, v0), COMPINFMIN(G, v0) returns a complete voltage assignment v
for G extending v0 that minimizes

∞ , and runs in randomized O(m + n log n) time.

grad[v]

4.4 Faster Algorithms for Lex-minimization

(cid:13)
(cid:13)

(cid:13)
(cid:13)

The lex-minimizer has additional structure that allows one to compute it by more efﬁcient algorithms. One observation
that leads to a faster implementation is that ﬁxing a steepest ﬁxable path does not increase the pressure at vertices,
provided that one appropriately ignores terminal-terminal edges. Thus, if G(α) is a subgraph that we identiﬁed with
pressure greater than α, we can iteratively ﬁx all steepest ﬁxable paths P in G(α) with ∇P > α. Another simple
observation is that if G(α) is disconnected, we can simply recurse on each of the connected components. A complete
description of an the algorithm COMPFASTLEXMIN based on these idea is given in Algorithm 11. The algorithm
provably computes lexG(v0), and it is possible to implement it so that the space requirement is only O(m + n).
Although, we are unable to prove theoretical bounds on the running time that are better than O(n(m + n log n)),
it runs extremely quickly in practice. We used it to perform the experiments in this paper. For random regular
graphs and Delaunay graphs, with n = 0.5 × 106 vertices and around 2 million edges m ∼ 1.5 − 2 × 106, it

8

takes a couple of minutes on a 2009 MacBook Pro. Similar times are observed for other model graphs of this
size such as random regular graphs and real world networks. An implementation of this algorithm may be found
at https://github.com/danspielman/YINSlex.

5 Directed Graphs

Our deﬁnitions and algorithms, including those for regularization, extend to directed graphs with only small modiﬁ-
cations. We view directed edges as diodes and only consider potential differences in the direction of the edge. For
a complete voltage assignment v on the vertices of a directed graph G, we deﬁne the directed gradient on (x, y) due
to v to be grad+
. Given a partially-labelled directed graph (G, v0), we say that a a
complete voltage assignment v is a lex-minimizer if it extends v0 and for other complete voltage assignment v′ that
extends v0 we have grad+
G[v′]. We say that a partially-labelled directed graph (G, v0) is a well-posed
directed instance if every free vertex appears in a directed path between two terminals.

G[v](x, y) = max

G[v] (cid:22) grad+

v(x)−v(y)
ℓ(x,y)

, 0

n

o

The main difference between the directed and undirected cases is that the directed lex-minimizer is not necessarily
unique. To maintain clarity of exposition, we chose to focus on undirected graphs so far. For directed graphs, we have
the following corresponding structural results.

Theorem 5.1 Given a well-posed instance (G, v0) on a directed graph G, there exists a lex-minimizer, and the set of
all lex-minimizers is a convex set. Moreover, for every two lex-minimizers v and v′, we have grad+

G[v] = grad+

G[v′].

However, note that in the case of directed graphs, the lex-minimizer need not be unique. We still have a weaker version
of Theorem 3.3 for directed graphs.

Theorem 5.2 Given a well-posed instance (G, v0) on a directed graph G, let v1 be the partial voltage assignment
extending v0 obtained by repeatedly ﬁxing steepest ﬁxable (directed) paths P with ∇P > 0. Then, any lex-minimizer
of (G, v0) must extend v1. Moreover, for every edge e ∈ EG \ (T (V1) × T (V1)), any lex-minimizer v of (G, v0) must
satisfy grad+[v](e) = 0.

When the value of the lex-minimizer at a vertex is not uniquely determined, it is constrained to an interval. In our
experiments, we pick the convention that when the voltage at a vertex is constrained to an interval (−∞, a] or [a, ∞),
we assign a to the terminal. When it is constrained to a ﬁnite interval, we assign a voltage closest to the median of the
original voltages.

6 Experiments on WebSpam

We demonstrate the performance of our lex-minimization algorithms on directed graphs by using them to detect spam
webpages as in Zhou et al. (2007). We use the dataset webspam-uk2006-2.0 described in Castillo et al. (2006).
This collection includes 11,402 hosts, out of which 7,473 (65.5 %) are labeled, either as spam or normal. Each host
corresponds to the collection of web pages it serves. Of the hosts, 1924 are labeled spam (25.7 % of all labels). We
consider the problem of ﬂagging some hosts as spam, given only a small fraction of the labels for training. We assign
a value of 1 to the spam hosts, and a value of 0 to the normal ones. We then compute a lex minimizer and examine the
effect of ﬂagging as spam all hosts with a value greater than some threshold.

Following Zhou et al. (2007), we create edges between hosts with lengths equal to the reciprocal of the number of
links from one to the other. We run our experiments only on the largest strongly connected component of the graph,
which contains 7945 hosts of which 5552 are labeled. 16 % of the nodes in this subgraph are labeled spam. To create
training and test data, for a given value p, we select a random subset of p % of the spam labels and a random subset
of p % of the normal labels to use for training. The remaining labels are used for testing. We report results for p = 5
and p = 20.

Again following Zhou et al. (2007), we plot the precision and recall of different choices of threshold for ﬂagging
pages as spam. Recall is the fraction of spam pages our algorithm ﬂags as spam, and precision is the fraction of pages
our algorithm ﬂags as spam that actually are spam. Amongst the algorithms studied by Zhou et al. (2007), the top

9

performer was their algorithm based on sampling according to a random-walk that follows in-links from other hosts.
We compare their algorithm with the classiﬁcation we get by directing edges in the opposite directions of links. This
has the effect that a link to a spam host is evidence of spamminess, and a link from a normal host is evidence of
normality.

Results are shown in Figure 3. While we are not able to reliably ﬂag all spam hosts, we see that in the range of
10-50 % recall, we are able to ﬂag spam with precision above 82 %. We see that the performance of directed lex-
minimization does not degrade rapidly when from the “large training set” regime of p = 20, to the “small training set”
regime of p = 5.

5 % labels for training

20 % labels for training

RandWalk
DirectedLex

RandWalk
DirectedLex

1

0.9

0.8

0.7

i

i

n
o
s
c
e
r
P

0.6

0.5

0.4

0.3

0.2

0.1

0
0.1

0.2

0.3

0.4

0.7

0.8

0.9

1

0.2

0.3

0.4

0.7

0.8

0.9

1

0.6
0.5
Recall

0.6
0.5
Recall

Figure 3: Recall and precision in the web spam classiﬁcation experiment. Each data point shown was computed as an average over
100 runs. The largest standard deviation of the mean precision across the plotted recall values was less than 1.3 %. The algorithm
of Zhou et al. (2007) appears as RANDWALK. Our directed lex-minimization algorithm appears as DIRECTEDLEX.

For comparison, in Appendix C, we show the performance of our algorithm and that of Zhou et al. (2007) both
with link directions reversed, as well as the performance of undirected lex-minimization and Laplacian inference, all
of which are signiﬁcantly worse.

7 l0-Regularization of Vertex Values

We now explain how we can accommodate noise in both the given voltages and in the given lengths of edges. We can
ﬁnd the minimum number of labels to ignore, or the minimum increase in edges lengths needed so that there exists an
extension whose gradients have l∞-norm lower than a given target. After determining which labels to ignore or the
needed increment in edge lengths, we recommend computing a lex minimizer.

The algorithms we present in this section are essentially the same for directed and undirected graphs.

7.1 l0-Vertex Regularization for Inf-minimization

The l0-regularization of vertex labels can be viewed as a problem of outlier removal: the vector we compute is allowed
to disagree with v0 on up to k terminals. Given a voltage assignment v and a subset T ⊂ V of the vertices, by v(T )
we mean the vector obtained by restricting v to T . We deﬁne the l0-Vertex Regularization for l∞ problem to be

where v(T ) is the vector of values of v on the terminals T .

min
v∈IRn

gradG[v]

∞

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

0 ≤ k,
(cid:13)
(cid:13)

subject to

v(T ) − v0(T )

(3)

In Appendix D, we describe an approximation algorithm APPROX-OUTLIER that approximately solves program (3).

The precise statement we prove in Appendix D is given in the following theorem.

1

0.9

0.8

0.7

i

i

n
o
s
c
e
r
P

0.6

0.5

0.4

0.3

0.2

0.1

0
0.1

10

Theorem 7.1 (Approximate l0-vertex regularization) The algorithm APPROX-OUTLIER takes a positive integer k
and a partially-labeled graph (G, v0), and outputs an assignment v with
0 ≤ 2k, and
∞ ≤
α∗, where α∗ is the optimum value of program (3). The algorithm runs in time O(k(m + n log n)).
(cid:13)
(cid:13)
(cid:13)
(cid:13)

In Appendix D, we also describe an algorithm OUTLIER that exactly solves program (3) in polynomial time, and we
prove its correctness.

v(T ) − v0(T )

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Theorem 7.2 (Exact l0-vertex regularization) The algorithm OUTLIER takes a positive integer k and a partially-
labeled graph (G, v0) solves program (3) exactly. The algorithm runs in polynomial time.

We give a proof of Theorem 7.2 in Appendix D. To do this, we reduce the program (3) to the problem of minimizing
the required l0-budget needed to achieve a ﬁxed gradient α using a binary search over a set of O(n2) gradients. This
latter problem we reduce in polynomial time to Minimum Vertex Cover (VC) on a transitively closed, directed acyclic
graph (a TC-DAG). VC on a TC-DAG can be solved exactly in polynomial time by a reduction to the Maximum
Bipartite Matching Problem (Fulkerson (1956)). The problem was phrased by Fulkerson as one of ﬁnding a maximum
antichain of a ﬁnite poset. Any transitively closed DAG corresponds directly to the comparability graph of a poset. A
maximum antichain of a poset is a maximum independent set of a the comparability graph of the poset, and hence its
complement is a minimum vertex cover of the comparability graph. We refer to the algorithm developed by Fulkerson
as KONIG-COVER.

Theorem 7.3 The algorithm KONIG-COVER computes a minimum vertex cover for any transitively closed DAG G in
polynomial time.

7.2 Hardness of l0 regularization for l2

The result that l0-regularized inf-minimization can be solved exactly in polynomial time is surprising, especially
because the analogous problem for 2-Laplacian minimization turns out to be NP-Hard.

We deﬁne the the l0 vertex regularization for l2 for a partially-labeled graph (G, v0) and an integer k by

min
v∈Rn:kv(T )−v0(T )k0

≤k

vT Lv,

where L is the Laplacian of G.

Theorem 7.4 l0 vertex regularization for l2 is NP-Hard.

In Appendix E we prove Theorem 7.4 by giving a polynomial time (Karp) reduction from the NP-Hard minimum
bisection problem to l0 vertex regularization for l2.

8 l1-Edge and Vertex Regularization of Inf-minimizers

Consider a partially-labeled graph (G, v0) and an α > 0. The set of voltage assignments given by

v : v extends v0 and

gradG[v]

∞ ≤ α

n

(cid:13)
(cid:13)

(cid:13)
(cid:13)

o

is convex. Going further, let us consider the edge lengths in a graph to be speciﬁed by a vector ℓ ∈ IRE. Now the set
of voltages v and and lengths ℓ which achieve kgradG(ℓ)[v]k∞ ≤ α is jointly convex in v and ℓ. To see this, observe
that

kgradG(ℓ)[v]k∞ ≤ α ⇔ ∀(u, v) ∈ E : −αℓ(u, v) ≤ v(u) − v(v) ≤ αℓ(u, v).
Furthermore, the condition “v extends v0” is a linear constraint on v, which we express as v(T ) = v0(T ). From
the above, it is clear that the gradient condition corresponds to a convex set, as it is an intersection of half-spaces.
These half-spaces are given by O(m) linear inequalities. We can leverage this to phrase many regularized variants of
inf-minimization as convex programs, and in some cases linear programs.

(4)

11

For example, we may consider a variant of inf-minimization combined with an l1-budget for changing lengths of
edges and values on terminals. Given a parameter γ > 0 which speciﬁes the relative cost of regularizing terminals to
regularizing edges, the problem is as follows

arg min
v∈IRn,s∈IRm,s≥0

ksk1 + γ

v(T ) − v0(T )

1

subject to

gradG(ℓ+s)[v]

≤ α.

(5)

(cid:13)
(cid:13)
From our observation (4), it follows that problem (5) may be expressed as a linear program with O(n) variables
and O(m) constraints. We can use ideas from Daitch and Spielman (2008) to solve the resulting linear program in
O(m1.5) by an interior point method with a special purpose linear equation solver. The reason is that the linear
time
equations the IPM must solve at each iteration may be reduced to linear equations in symmetric, diagonally dominant
matrices, and these may be solved in nearly-linear time (Cohen et al. (2014)).

(cid:13)
(cid:13)

e

(cid:13)
(cid:13)
(cid:13)

∞

(cid:13)
(cid:13)
(cid:13)

Conclusion. We propose the use of inf and lex minimizers for regression on graphs. We present simple algorithms
for computing them that are provably fast and correct, and can also be implemented efﬁciently. We also present a
framework and polynomial time algorithms for regularization in this setting. The initial experiments reported in the
paper indicate that these algorithms give pretty good results on real and synthetic datasets. The results seem to compare
quite favorably to other algorithms, particularly in the regime of tiny labeled sets. We are testing these algorithms on
several other graph learning questions, and plan to report on them in a forthcoming experimental paper. We believe
that inf and lex minimizers, and the associated ideas presented in the paper, should be useful primitives that can be
proﬁtably combined with other approaches to learning on graphs.

We thank anonymous reviewers for helpful comments. We thank Santosh Vempala and Bartosz Walczak for pointing
out that it was already known how to compute a minimum vertex cover of a transitively closed DAG in polynomial
time.

Acknowledgements

References

Morteza Alamgir
In Advances
Information Processing
http://books.nips.cc/papers/files/nips24/NIPS2011_0278.pdf.

and Ulrike V. Luxburg.

transition
24,

in
pages

in Neural

Systems

Phase

the

family
379–387.

of
2011.

p-resistances.
URL

Gunnar Aronsson. Extension of functions satisfying lipschitz conditions. Arkiv fr Matematik, 6(6):551–561, 1967.

ISSN 0004-2080. doi: 10.1007/BF02591928. URL http://dx.doi.org/10.1007/BF02591928.

Gunnar Aronsson, Michael G. Crandall, and Petri Juutinen. A tour of the theory of absolutely minimizing functions.
ISSN 0273-0979. doi: 10.1090/S0273-0979-04-01035-3.

Bull. Amer. Math. Soc. (N.S.), 41(4):439–505, 2004.
URL http://dx.doi.org/10.1090/S0273-0979-04-01035-3.

Guy Barles and J´erˆome Busca. Existence and comparison results for fully nonlinear degenerate elliptic equations

without zeroth-order term. Comm. Partial Differential Equations, 26:2323–2337, 2001.

Mikhail Belkin, Irina Matveeva, and Partha Niyogi.

Regularization and semi-supervised learning on large
In Learning Theory, volume 3120 of Lecture Notes in Computer Science, pages 624–638.
doi: 10.1007/978-3-540-27819-1 43. URL

graphs.
Springer Berlin Heidelberg, 2004.
http://dx.doi.org/10.1007/978-3-540-27819-1_43.

ISBN 978-3-540-22282-8.

Nick Bridle and Xiaojin Zhu. p-voltages: Laplacian regularization for semi-supervised learning on high-dimensional

data. In Eleventh Workshop on Mining and Learning with Graphs (MLG2013), 2013.

12

Carlos Castillo, Debora Donato, Luca Becchetti, Paolo Boldi, Stefano Leonardi, Massimo Santini, and Sebastiano
Vigna. A reference collection for web spam. SIGIR Forum, 40(2):11–24, December 2006. ISSN 0163-5840. doi:
10.1145/1189702.1189703. URL http://doi.acm.org/10.1145/1189702.1189703.

Olivier Chapelle, Bernhard Schlkopf, and Alexander Zien. Semi-Supervised Learning. The MIT Press, 1st edition,

2010. ISBN 0262514125, 9780262514125.

Michael B Cohen, Rasmus Kyng, Gary L Miller, Jakub W Pachocki, Richard Peng, Anup B Rao, and Shen Chen Xu.
Solving SDD linear systems in nearly m log1/2 n time. In Proceedings of the 46th Annual ACM Symposium on
Theory of Computing, pages 343–352. ACM, 2014.

M.G. Crandall, L.C. Evans, and R.F. Gariepy. Optimal lipschitz extensions and the inﬁnity laplacian. Calculus of Vari-
ations and Partial Differential Equations, 13(2):123–139, 2001. ISSN 0944-2669. doi: 10.1007/s005260000065.
URL http://dx.doi.org/10.1007/s005260000065.

Samuel I. Daitch and Daniel A. Spielman. Faster approximate lossy generalized ﬂow via interior point algo-
rithms.
In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing, STOC ’08, pages
451–460, New York, NY, USA, 2008. ACM. ISBN 978-1-60558-047-0. doi: 10.1145/1374376.1374441. URL
http://doi.acm.org/10.1145/1374376.1374441.

Alan Egger and Robert Huotari. Rate of convergence of the discrete polya algorithm. Journal of Approximation
ISSN 0021-9045. doi: http://dx.doi.org/10.1016/0021-9045(90)90070-7. URL

Theory, 60(1):24 – 30, 1990.
http://www.sciencedirect.com/science/article/pii/0021904590900707.

Charles Fefferman. Whitney’s extension problems and interpolation of data.

(N.S.), 46(2):207–220, 2009a.
http://dx.doi.org/10.1090/S0273-0979-08-01240-8.

ISSN 0273-0979.

doi:

10.1090/S0273-0979-08-01240-8.

Bull. Amer. Math. Soc.
URL

Charles Fefferman. Fitting a [image] -smooth function to data, iii. Annals of Mathematics, 170(1):pp. 427–441, 2009b.

ISSN 0003486X. URL http://www.jstor.org/stable/40345469.

Charles Fefferman and Bo’az Klartag. Fitting a cm -smooth function to data i. Annals of Mathematics, 169(1):pp.

315–346, 2009. ISSN 0003486X. URL http://www.jstor.org/stable/40345445.

D. R. Fulkerson. Note on dilworths decomposition theorem for partially ordered sets. Proc. Amer. Math. Soc, 1956.

P.W. Gaffney and M.J.D. Powell. Optimal interpolation. In Numerical Analysis, volume 506 of Lecture Notes in Math-
ematics, pages 90–99. Springer Berlin Heidelberg, 1976. ISBN 978-3-540-07610-0. doi: 10.1007/BFb0080117.
URL http://dx.doi.org/10.1007/BFb0080117.

L.-A. Gottlieb, A. Kontorovich, and R. Krauthgamer. Efﬁcient classiﬁcation for metric data. CoRR, abs/1306.2547,

2013. URL http://arxiv.org/abs/1306.2547.

L.-A. Gottlieb, A. Kontorovich, and R. Krauthgamer. Efﬁcient regression in metric spaces via approximate lipschitz
extension. In Similarity-Based Pattern Recognition, volume 7953 of Lecture Notes in Computer Science, pages
43–58. Springer Berlin Heidelberg, 2013b. ISBN 978-3-642-39139-2. doi: 10.1007/978-3-642-39140-8 3. URL
http://dx.doi.org/10.1007/978-3-642-39140-8_3.

Robert Jensen. Uniqueness of lipschitz extensions: Minimizing the sup norm of the gradient. Archive for Ra-
doi: 10.1007/BF00386368. URL

ISSN 0003-9527.

tional Mechanics and Analysis, 123(1):51–74, 1993.
http://dx.doi.org/10.1007/BF00386368.

M. Kirszbraun. ber die zusammenziehende und lipschitzsche transformationen. Fundamenta Mathematicae, 22(1):

77–108, 1934. URL http://eudml.org/doc/212681.

13

Andrew J. Lazarus, Daniel E. Loeb,

James G. Propp, Walter R. Stromquist,

Combinatorial games under

man.
229 – 264,
http://www.sciencedirect.com/science/article/pii/S0899825698906765.

http://dx.doi.org/10.1006/game.1998.0676.

and Economic Behavior,

ISSN 0899-8256.

auction play.

Games

1999.

doi:

and Daniel H. Ull-
27(2):
URL

Yunqian Ma and Yun Fu. Manifold Learning Theory and Applications. CRC Press, Inc., Boca Raton, FL, USA, 1st

edition, 2011. ISBN 1439871094, 9781439871096.

E. J. McShane. Extension of range of functions. Bull. Amer. Math. Soc., 40(12):837–842, 12 1934. URL

http://projecteuclid.org/euclid.bams/1183497871.

C.A. Micchelli, T.J. Rivlin,

and S. Winograd.

merische Mathematik, 26(2):191–200, 1976.
http://dx.doi.org/10.1007/BF01395972.

The optimal
ISSN 0029-599X.

recovery of
doi:

smooth functions.
10.1007/BF01395972.

Nu-
URL

V. A. Milman.

Absolutely minimal extensions of

functions on metric spaces.

1999.

URL

http://iopscience.iop.org/1064-5616/190/6/A05/pdf/MSB_190_6_A05.pdf.

Boaz Nadler, Nathan Srebro, and Xueyuan Zhou. Statistical analysis of semi-supervised learning: The limit of inﬁnite
unlabelled data. 2009. URL http://ttic.uchicago.edu/˜nati/Publications/NSZnips09.pdf.

A. Naor and S. Shefﬁeld. Absolutely minimal Lipschitz extension of tree-valued mappings. CoRR, abs/1005.2535,

May 2010. URL http://arxiv.org/abs/1005.2535.

A. M. Oberman. Finite difference methods for the Inﬁnity Laplace and p-Laplace equations. CoRR, abs/1107.5278,

July 2011. URL http://arxiv.org/abs/1107.5278.

Yuval Peres, Oded Schramm, Scott Shefﬁeld, and DavidB. Wilson.

Tug-of-war and the inﬁnity lapla-
In Selected Works of Oded Schramm, Selected Works in Probability and Statistics, pages 595–
doi: 10.1007/978-1-4419-9675-6 18. URL

cian.
638. Springer New York, 2011.
http://dx.doi.org/10.1007/978-1-4419-9675-6_18.

ISBN 978-1-4419-9674-9.

S. Shefﬁeld and C. K. Smart. Vector-valued optimal Lipschitz extensions. CoRR, abs/1006.1741, June 2010. URL

http://arxiv.org/abs/1006.1741.

Ali Kemal Sinop and Leo Grady. A seeded image segmentation framework unifying graph cuts and random walker
which yields a new algorithm. In Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on,
pages 1–8. IEEE, 2007.

Vijay V. Vazirani. Approximation Algorithms. Springer-Verlag New York, Inc., New York, NY, USA, 2001. ISBN

3-540-65367-8.

Ulrike von Luxburg and Olivier Bousquet. Distance-based classiﬁcation with lipschitz functions.

In Learn-
ing Theory and Kernel Machines, volume 2777 of Lecture Notes in Computer Science, pages 314–328.
doi: 10.1007/978-3-540-45167-9 24. URL
Springer Berlin Heidelberg, 2003.
http://dx.doi.org/10.1007/978-3-540-45167-9_24.

ISBN 978-3-540-40720-1.

Hassler Whitney.

Analytic extensions of differentiable functions deﬁned in closed sets.

tions of
http://www.jstor.org/stable/1989708.

the American Mathematical Society, 36(1):pp. 63–89, 1934.

ISSN 00029947.

Transac-
URL

Dengyong Zhou, Christopher J. C. Burges, and Tao Tao. Transductive link spam detection.

In Proceedings
of the 3rd International Workshop on Adversarial Information Retrieval on the Web, AIRWeb ’07, pages 21–
ISBN 978-1-59593-732-2. doi: 10.1145/1244408.1244413. URL
28, New York, NY, USA, 2007. ACM.
http://doi.acm.org/10.1145/1244408.1244413.

Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic

functions. In IN ICML, pages 912–919, 2003.

14

A Basic Properties of Lex-Minimizers

A.1 Meta Algorithm

Algorithm 1: Algorithm META-LEX: Given a well-posed instance (G, v0), outputs lexG[v0].
for i = 1, 2, . . . :

1. if T (vi−1) = VG, then return vi−1.
2. E′ = EG \ (T (vi−1) × T (vi−1)), G′ := (VG, E′).
3. Let P ⋆
4. vi ← ﬁx[vi−1, P ⋆
i ].

i be a steepest ﬁxable path in (G′, vi−1). Let α⋆

i ← ∇P ⋆(vi−1).

In this subsection, we prove the results that appeared in section 2. We start with a simple observation.

Proposition A.1 Given a well-posed instance (G, v0) such that T (v0) 6= V, let P be a steepest ﬁxable path in (G, v0).
Then, ﬁx[v0, P ] extends v0, and (G, ﬁx[v0, P ]) is also a well-posed instance.

The properties we prove below do not depend on the choice of the steepest ﬁxable path.

Proposition A.2 For any well-posed instance (G, v0), with |VG| = n, META-LEX(G, v0) terminates in at most n
iterations, and outputs a complete voltage assignment v that extends v0.

Proof of Proposition A.2: By Proposition A.1, at any iteration i, vi−1 extends v0 and (G′, vi−1) is a well-posed
instance. META-LEX only outputs vi−1 iff T (vi−1) = V, which means vi−1 is a complete voltage assignment. For
any vi−1 that is not complete, for any x ∈ V \T (vi−1), we must have a free terminal path in (G′, vi−1) that contains x.
i exists in (G′, vi−1). Since P ⋆
Hence, a steepest ﬁxable path P ⋆
i ] ﬁxes the voltage
i
✷
for at least one non-terminal. Thus, META-LEX(G, v0) must complete in at most n iterations.

is a free terminal path, ﬁx[vi−1, P ⋆

For the following lemmas, consider a run of META-LEX with well-posed instance (G, v0) as input. Let vout be the
complete voltage assignment output by META-LEX. Let Ei be the set of edges E′ and Gi be the graph G′ constructed
in iteration i of META-LEX.

Lemma A.3 For every edge e ∈ Ei−1 \ Ei, we have

grad[vout](e)

≤ α⋆

i . Moreover, α⋆

i is non-increasing with i.

Proof of Lemma A.3: Let P ⋆
i = (x0, . . . , xr) be a steepest ﬁxable path in iteration i (when we deal with instance
(Gi−1, vi−1)). Consider a terminal path Pi+1 in (Gi, vi) such that {∂0Pi+1, ∂1Pi+1} ∩ (T (vi) \ T (vi−1)) 6= ∅. We
i . On the contrary, assume that ∇Pi+1(vi) > α⋆
claim that ∇Pi+1(vi) ≤ α⋆
i . Consider the case ∂0Pi+1 ∈ T (vi) \
T (vi−1), ∂1P1 ∈ T (vi−1). By the deﬁnition of vi, we must have ∂0Pi+1 = xj for some j ∈ [r − 1]. Let P ′
i+1 be the
path formed by joining paths P ⋆

i+1 is a free terminal path in (Gi−1, vi−1). We have,

i [x0 : xj] and Pi+1. P ′

(cid:12)
(cid:12)

(cid:12)
(cid:12)

vi−1(x0) − vi−1(∂1Pi+1) = (vi(x0) − vi(xj )) + (vi(∂0Pi+1) − vi(∂1Pi+1))
i · ℓ(P ′

i · ℓ(Pi+1) = α⋆

i [x0 : xj]) + α⋆

i · ℓ(P ⋆

> α⋆

i+1),

giving ∇P ′
The other cases can be handled similarly.

i+1(vi) > α⋆

i , which is a contradiction since the steepest ﬁxable path P ⋆
i

in (Gi−1, vi−1) has gradient α⋆
i .

Applying the above claim to an edge e ∈ Ei−1 \ Ei, whose gradient is ﬁxed for the ﬁrst time in iteration i, we
i . If v is the complete voltage assignment output by META-LEX, since v extends vi+1,
i , implying

i . Applying the claim to the symmetric edge, we obtain −grad[vout](e) ≤ α⋆

obtain that grad[vi+1](e) ≤ α⋆
we get grad[vout](e) ≤ α⋆
|grad[vout](e)| ≤ α⋆
i .

Consider any free terminal path Pi+1 in (Gi, vi). If Pi+1 is also a terminal path in (Gi−1, vi−1), it is a free
terminal path in (Gi−1, vi−1). In addition, since a steepest ﬁxable path P ⋆
i , we get
i
∇Pi+1(vi) = ∇Pi+1(vi−1) ≤ α⋆
i . Otherwise, we must have {∂0Pi+1, ∂1Pi+1} ∩ (T (vi) \ T (vi−1)) 6= ∅, and we can
deduce ∇Pi+1(vi) ≤ α⋆
i using the above claim. Thus, all free terminal paths Pi+1 in (Gi, vi) satisfy ∇Pi+1(vi) ≤ α⋆
i .
✷
In particular, α⋆

in (Gi−1, vi−1) has ∇P ⋆

i = α⋆

i is non-increasing with i.

i+1(vi) ≤ α⋆

i+1 = ∇P ⋆

i . Thus, α⋆

15

Lemma A.4 For any complete voltage assignment v for G that extends v0, if v 6= vout, we have grad[v] 6(cid:22) grad[vout],
and hence grad[vout] (cid:22) grad[v].

Proof of Lemma A.4: Consider any complete voltage assignment v for G that extends v0, such that v 6= vout. Thus,
there exists a unique i such that v extends vi−1 but does not extend vi. We will argue that grad[v] 6(cid:22) grad[vout], and
hence grad[vout] (cid:22) grad[v]. For every edge e ∈ E \ Ei−1 that has been ﬁxed so far, grad[v](e) = grad[vi−1](e) =
grad[vout](e), and hence we can ignore these edges.

Since v extends vi−1 but not vi, there exists an x ∈ T (vi) \ T (vi−1) such that v(x) 6= vi(x) = vout(x). Assume
i picked

i = (x0, . . . , xr) is the steepest ﬁxable path with gradient α⋆

v(x) < vi(x) (the other case is symmetric). If P ⋆
in iteration i, we must have x = xj for some j ∈ [r − 1]. Thus,

j

j

(v(xk−1) − v(xk)) = v(x0) − v(xj ) > vi(x0) − vi(xj ) = α⋆

i · ℓ(P ⋆

i [x0 : xj ]) = α⋆
i ·

ℓ(xk−1, xk).

Xk=1

Xk=1
Thus, for some k ∈ [j], we must have grad[v](xk−1, xk) > α⋆
is a path in Gi−1, we have {xk−1, xk} 6⊆
T (vi−1). This gives (xk−1, xk) ∈ (Ei−1 \ Ei). But then, from Lemma A.3, it follows that for all e ∈ (Ei−1 \ Ei), we
✷
have |grad[vout](e)| ≤ α⋆

i . Thus, we have grad[v] 6(cid:22) grad[vout].

i . Since P ∗
i

Lemma A.5 Let P = (x0, . . . , xr) be a steepest ﬁxable path such that it does not have any edges in T (v0) × T (v0)
and v1 = ﬁxG[v0, P ]. Then for every i ∈ [r], we have grad[v1](xi−1, xi) = ∇P.

Proof of Lemma A.5: Suppose this is not true and let j ∈ [r] be the minimum number such that grad[v1](xj−1, xj) 6=
∇P. By deﬁnition of v1 we would necessarily have j < r and vj ∈ T (v0). Suppose grad[v1](xj−1, xj ) < ∇P. We
would then have v1(x0) − v1(xj ) < ∇P ∗ ℓ(P [x0 : xj]). Since P does not have any edges in T (v0) × T (v0),
P1 := (xj, ..., xr) would be a free terminal path with ∇P1 > ∇P. This is a contradiction. Other cases can be ruled
out similarly.

✷

Proof of Theorem 3.3: Consider an arbitrary run of META-LEX on (G, v0). Let vout be the complete voltage
assignment output by META-LEX. Proposition A.1 implies that vout extends v0. Lemma A.4 implies that for any
complete voltage assignment v 6= vout that extends v0, we have grad[vout] (cid:22) grad[v]. Thus, vout is a lex-minimizer.
Moreover, the lemma also gives that for any such v, grad[v] 6(cid:22) grad[vout]. and hence vout is a unique lex-minimizer.
Thus, vout is the unique voltage assignment satisfying Def. 2.1, and we denote it as lexG[v0]. Since we started with an
✷
arbitrary run of META-LEX, uniqueness implies that every run of META-LEX on (G, v0) must output lexG[v0].

Proof of Lemma 3.5: Suppose we have a complete voltage assignment v extending v0, such that
For any terminal path P = (x0, . . . , xr), we get,

grad[v]

∞ ≤ α.

∇P (v0) = v0(∂0P ) − v0(∂1P ) = v(∂0P ) − v(∂1P ) =

grad[v](xi−1, xi) ≤ α ·

ℓ(xi−1, xi) = α · ℓ(P ),

(cid:13)
(cid:13)

(cid:13)
(cid:13)

r

i=1
X

giving ∇P (v0) ≤ α.

On the other hand, suppose every terminal path P in (G, v0) satisﬁes ∇P (v0) ≤ α. Consider v = lexG[v0]. We
know that v extends v0. For every edge e ∈ EG ∩ T (v0) × T (v0), e is a (trivial) terminal path in (G, v0), and hence
has satisﬁes grad[v](e) = grad[v0](e) = ∇e(v0) ≤ α. Considering the reverse edge, we also obtain −grad[v](e) ≤ α.
Thus, |grad[v](e)| ≤ α. Moreover, using Lemma A.3, we know that for edge e ∈ EG \ T (v0) × T (v0), |grad[v](e)| ≤
1 = ∇P ⋆
α⋆
1 ≤ α since P1 is a terminal path in (G, v0). Thus, for every e ∈ EG, |grad[v](e)| ≤ α, and hence
✷
grad[v]
∞ ≤ α.
(cid:13)
(cid:13)
(cid:13)
(cid:13)
A.2 Stability

In this subsection, we sketch a proof of the monotonicity of lex-minimizers and show how it implies the stability
property claimed earlier.

For any well-posed (G, v0), there could be several possible executions of META-LEX, each characterized by the

sequence of paths P ⋆

i . We can apply Theorem 3.3 to deduce the following structural result about the lex-minimizer.

r

i=1
X

16

Corollary A.6 For any well-posed instance (G, v0), consider a sequence of paths (P1, . . . , Pr) and voltage assign-
ments (v1, . . . , vr) for some positive integer r such that:

1. P ⋆

i is a steepest ﬁxable path in (Gi−1, vi−1) for i = 1, . . . , r.

2. vi = ﬁx[vi−1, P ⋆

i ] for i = 1, . . . , r.

3. T (vr) = VG.

Then, we have vr = lexG[v0].

We call such a sequence of paths and voltages to be a decomposition of lexG[v0]. Again, note that lexG[v0] can
possibly have multiple decompositions. However, any two such decompositions are consistent in the sense that they
produce the same voltage assignment.

Proof of Corollary 3.7: We ﬁrst deﬁne some operations on partial assignments which simpliﬁes the notation. Let
v0, v1 be any two partial assignments with the same set of terminals T := T (v0) = T (v1) and c, d ∈ R. By cv0 + d
we mean a partial assignment v with T (v) = T satisfying v(t) = cv0(t) + d for all t ∈ T . Also, by v0 + v1 we
mean a partial assignment v with T (v) = T satisfying v(t) = v0(t) + v1(t) for all t ∈ T. Also, we say v1 ≥ v0 if
v1(t) ≥ v0(t) for all t ∈ T .

Now we can show how Corollary 3.7 follows from Theorem 3.6. Let v := v1 − v0, and kvk∞ = ǫ, for some ǫ > 0.
Therefore, v0 + ǫ ≥ v1 ≥ v0 − ǫ. Theorem 3.6 then implies that lexG[v0] + ǫ ≥ lex[v1] ≥ lex[v0] − ǫ, hence proving
✷
the corollary.

Proof sketch of Theorem 3.6:
It is easy to see that the ﬁrst statement holds. For the second statement, we ﬁrst
observe that if there is a sequence of paths P1, ..., Pr that is simultaneously a decomposition of both lex[v0] and
lex[v1], then this is easy to see. If such a path sequence doesn’t exist, then we look at vt := v0 + t(v1 − v0). We
state here without a proof (though the proof is elementary) that we can then split the interval [0, 1] into ﬁnitely many
subintervals [a0, a1], [a1, a2], .., [ak−1, ak], with a0 = 0, ak = 1, such that for any i, there is a path sequence P1, ..., Pr
which is a decomposition of lex[vt] for all t ∈ [ai, ai+1]. We then observe that v0 = va0 ≤ va1 ≤ ...vak = v1. Since
for every ai, ai+1, there is a path sequence which is simultaneously a decomposition of both lex[vai ] and lex[vai+1 ],
we immediately get

lex[v0] = lex[va0 ] ≤ lex[va1] ≤ ... ≤ lex[vak ] = lex[v1].

✷

A.3 Alternate Characterizations

Proof of Theorem 3.10: We know that lexG[v0] extends v0. We ﬁrst prove that v = lexG[v0] satisﬁes the max-min
gradient averaging property. Assume to the contrary. Thus, there exists x ∈ VG \ T (v0) such that

max
y:(x,y)∈EG

grad[v](x, y) 6= − min

grad[v](x, y).

y:(x,y)∈EG

Assume that max(x,y)∈EG grad[v](x, y) ≥ − min(x,y)∈EG grad[v](x, y). Then, consider v′ extending v0 that is iden-
tical to v except for v′(x) = v(x) − ǫ for ǫ > 0. For ǫ small enough, we get that

and

max
y:(x,y)∈EG

grad[v′](x, y) < max

grad[v](x, y)

y:(x,y)∈EG

− min

y:(x,y)∈EG

grad[v′](x, y) < max

grad[v](x, y).

y:(x,y)∈EG

The gradient of edges not incident on the vertex x is left unchanged. This implies that grad[v]

6(cid:22) grad[v′],

contradicting the assumption that v is the lex-minimizer. (The other case is similar).

17

For the other direction. Consider a complete voltage assignment v extending v0 that satisﬁes the max-min gradient

averaging property w.r.t. (G, v0). Let

α = max

grad[v](x, y) ≥ 0

(x,y)∈EG
x∈V \T (v0)

be the maximum edge gradient, and consider any edge (x0, x1) ∈ EG such that grad[v](x1, x0) = α, with x1 ∈
V \ T (v0). If α = 0, grad[v] is identically zero, and is trivially the lex-minimal gradient assignment. Thus, both v and
lexG[v0] are constant on each connected component. Since (G, v0) is well-posed, there is at least one terminal in each
component, and hence v and lexG[v0] must be identical.

Now assume α > 0. By the max-min gradient averaging property, ∃x2 ∈ VG such that (x1, x2) ∈ EG and

grad[v](x1, x2) =

min
y:(x1,y)∈EG

grad[v](x1, y) = − max

grad[v](x1, y)

y:(x1,y)∈EG

≤ −grad[v](x1, x0) = −α.

Thus, grad[v](x2, x1) ≥ α. Since α is the maximum edge gradient, we must have grad[v](x2, x1) = α. More-
over, v(x2) > v(x1) > v(x0), thus x2 6= x0. We can inductively apply this argument at x2 until we hit a ter-
minal. Similarly, if x0 /∈ T (v0) we can extend the path in the other direction. Consequently, we obtain a path
P = (xj , . . . , x2, x1, x0, x−1, . . . , xk) with all vertices as distinct, such that xj , xk ∈ T (v0), and xi ∈ V \ T (v0)
for all i ∈ [j + 1, k − 1]. Moreover, grad[v](xi, xi−1) = α for all j < i ≤ k. Thus, P is a free terminal path with
∇P [v0] = α.

Moreover, since v is a voltage assignment extending v0 with

∞ = α, using Lemma 3.5, we know that
every terminal path P ′ in (G, v0) must satisfy ∇P ′(v0) ≤ α. Thus, P is a steepest ﬁxable path in (G, v0). Thus,
letting v1 = ﬁx[v0, P ], using Corollary 3.4, we obtain that lexG[v1] = lexG[v0]. Moreover, since α = ∇P [v0] =
grad[v](xi, xi−1) for all i ∈ (j, k], we get v1(xi) = v(xi) for all i ∈ (j, k). Thus, v extends v1.

grad[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We can iterate this argument for r iterations until T (vr) = VG, giving v = vr and vr = lexG[vr] = lexG[v0].
(Since we are ﬁxing at least one terminal at each iteration, this procedure terminates). Thus, we get v = lexG[v0]. ✷

B Description of the Algorithms

Algorithm 2: MODDIJKSTRA(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs a complete
voltage assignment v for G, and an array parent : V → V ∪ {null}.

Add x to a ﬁbonacci heap, with key(x) = +∞.
ﬁnished(x) ← false

Decrease key(x) to v0(x).
parent(x) ← null.

1. for x ∈ VG,
2.
3.
4. for x ∈ T (v0)
5.
6.
7. while heap is not empty
8.
9.
10.
11.
12.
13.
14.
15. return (v, parent)

x ← pop element with minimum key from heap
v(x) ← key(x). ﬁnished(x) ← true .
for y : (x, y) ∈ EG

if ﬁnished(y) = false

if key(y) > v(x) + α · ℓ(x, y)

Decrease key(y) to v(x) + α · ℓ(x, y).
parent(y) ← x.

Theorem B.1 For a well-posed instance (G, V0) and a gradient value α ≥ 0, let (v, parent) ← MODDIJKSTRA(G, v0, α).
Then, v is a complete voltage assignment such that, ∀x ∈ VG, v(x) = mint∈T (v0){v0(t) + αdist(x, t)}. Moreover, the
pointer array parent satisﬁes ∀x /∈ T (v0), parent(x) 6= null and v(x) = v(parent(x)) + α · ℓ(x, parent(x)).

18

Algorithm 3: Algorithm COMPVLOW(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs
vLow, a complete voltage assignment for G, and an array LParent : V → V ∪ {null}.

1. (vLow, LParent) ← MODDIJKSTRA(G, v0, α)
2. return (vLow, LParent)

Algorithm 4: Algorithm COMPVHIGH(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs
vHigh, a complete voltage assignment for G, and an array HParent : V → V ∪ {null}.

if x ∈ T (v0) then v1(x) ← −v0(x) else v1(x) ← v1(x).

1. for x ∈ VG
2.
3. (temp, HParent) ← MODDIJKSTRA(G, v1, α)
4. for x ∈ VG : vHigh(x) ← −temp(x)
5. return (vHigh, HParent)

Corollary B.2 For a well-posed instance (G, V0) and a gradient value α ≥ 0, let (vLow[α], LParent) ← COMPVLOW(G, v0, α)
and (vHigh[α], HParent) ← COMPVHIGH(G, v0, α). Then, vLow[α], vHigh[α] are complete voltage assignments for
G such that, ∀x ∈ VG,

vLow[α](x) = min

{v0(t) + α · dist(x, t)}

t∈T (v0)

vHigh[α](x) = max
t∈T (v0)

{v0(t) − α · dist(t, x)}.

Moreover, the pointer arrays LParent, HParent satisfy ∀x /∈ T (v0), LParent(x), HParent(x) 6= null and

vLow[α](x) = vLow[α](LParent(x)) + α · ℓ(x, LParent(x)),
vHigh[α](x) = vHigh[α](HParent(x)) − α · ℓ(x, HParent(x)).

Algorithm 5: Algorithm COMPINFMIN(G, v0): Given a well-posed instance (G, v0), outputs a complete voltage assignment
v for G, extending v0 that minimizes (cid:13)

(cid:13)grad[v](cid:13)

(cid:13)∞.

1. α ← max{|grad[v0](e)| | e ∈ EG ∩ (T (v0) × T (v0))}.
2. EG ← EG \ (T (v0) × T (v0))
3. P ←STEEPESTPATH(G, v0).
4. α ← max{α, ∇P (v0)}
5. (vLow, LParent) ← COMPVLOW(G, v0, α)
6. (vHigh, HParent) ← COMPVHIGH(G, v0, α)
7. for x ∈ VG
8.
9.
10.
11. return v

then v(x) ← v0(x)
else v(x) ← 1

2 · (vLow(x) + vHigh(x)).

if x ∈ T (v0)

1. (vLow, LParent) ← COMPVLOW(G, v0, α)
2. (vHigh, HParent) ← COMPVHIGH(G, v0, α)
3. VG′ ← {x ∈ VG | vHigh(x) > vLow(x) }
4. EG′ ← {(x, y) ∈ EG | x, y ∈ VG′ }.

19

Algorithm 6: Algorithm COMPHIGHPRESSGRAPH(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0,
outputs a minimal induced subgraph G′ of G where every vertex has pressure[v0](·) > α.

5. G′ ← (V ′, E′, ℓ)
6. return G′

Proof of Lemma 4.3:

is equivalent to

vHigh[α](x) > vLow[α](x)

max
t∈T (v0)

{v0(t) − α · dist(t, x)} > min

{v0(t) + α · dist(x, t)},

t∈T (v0)

which implies that there exists terminals s, t ∈ T (v0) such that

thus,

Hence,

v0(t) − α · dist(t, x) > v0(s) + α · dist(x, s)

pressure[v0](x) ≥

v0(t) − v0(s)
dist(t, x) + dist(x, s)

> α.

v0(t) − v0(s)
dist(t, x) + dist(x, s)

= pressure[v0](x) > α.

v0(t) − α · dist(t, x) > v0(s) + α · dist(x, s)

So the inequality on vHigh and vLow implies that pressure is strictly greater than α. On the other hand, if pressure[v0](x) >
α, there exists terminals s, t ∈ T (v0) such that

which implies vHigh[α](x) > vLow[α](x).

✷

Algorithm 7: Algorithm STEEPESTPATH(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs a steepest
free terminal path P in (G, v0).

P ← VERTEXSTEEPESTPATH(G, v0, xi)

1. Sample uniformly random e ∈ EG. Let e = (x1, x2).
2. Sample uniformly random x3 ∈ VG.
3. for i = 1 to 3
4.
5. Let j ∈ arg maxj∈{1,2,3} ∇Pj (v0)
6. G′ ← COMPHIGHPRESSGRAPH(G, v0, ∇Pj (v0))
7. if EG′ = ∅,
8.
9.

then return Pj
else return STEEPESTPATH(G′, v0|VG′ )

1. while T (v0) 6= VG
2.
3.
4.
5. return v0

EG ← EG \ (T (v0) × T (v0))
P ← STEEPESTPATH(G, v0)
v0 ← ﬁx[v0, P ]

Algorithm 8: Algorithm COMPLEXMIN(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs lexG[v0].

Algorithm 9: Algorithm VERTEXSTEEPESTPATH(G,v0, x): Given a well-posed instance (G, v0), and a vertex x ∈ VG,
outputs a steepest terminal path in (G, v0) through x.

1. Using Dijkstra’s algorithm, compute dist(x, t) for all t ∈ T (v0)

20

y ← arg maxy∈T (v0)
if v0(x) ≥ v0(y)

|v0(x)−v0(y)|
dist(x,y)

then return a shortest path from x to y
else return a shortest path from y to x

2. if x ∈ T (v0)
3.
4.
5.
6.
7. else
8.
9.
10.
11.

for t /∈ T (v0), d(t) ← dist(x, t)
(t1, t2) ← STARSTEEPESTPATH(T (v0), v0|T (v0), d)
Let P1 be a shortest path from t1 to x. Let P2 be a shortest path from x to t2.
P ← (P1, P2). return P.

Algorithm 10: STARSTEEPESTPATH(T, v, d): Returns the steepest path in a star graph, with a single non-terminal connected
to terminals in T, with lengths given by d, and voltages given by v.

|v(t1)−v(t)|
d(t1)+d(t)

1. Sample t1 uniformly and randomly from T
2. Compute t2 ∈ arg maxt∈T
3. α ← |v(t2)−v(t1)|
d(t1)+d(t2)
4. Compute vlow ← mint∈T (v(t) + α · d(t))
5. Tlow ← {t ∈ T | v(t) > vlow + α · d(t)}
6. Compute vhigh ← maxt∈T (v(t) − α · d(t))
7. Thigh ← {t ∈ T | v(t) < vhigh − α · d(t)}
8. T ′ ← Tlow ∪ Thigh.
9. if T ′ = ∅
10.
11.

then if v(t1) ≥ v(t2) then return (t1, t2) else return (t2, t1)
else return STARSTEEPESTPATH(T ′, v|T ′, dT ′ )

B.1 Faster Lex-minimization

Algorithm 11: Algorithm COMPFASTLEXMIN(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs
lexG[v0].

1. while T (v0) 6= VG
2.
3. return v0

v0 ← FIXPATHSABOVEPRESS(G, v0, 0)

Algorithm 12: Algorithm FIXPATHSABOVEPRESS(G, v0, α): Given a well-posed instance (G, v0), with T (v0) 6= VG, and
a gradient value α, iteratively ﬁxes all paths with gradient > α.

EG ← EG \ (T (v0) × T (v0))
Sample uniformly random e ∈ EG. Let e = (x1, x2).
Sample uniformly random x3 ∈ VG.
for i = 1 to 3

Pi ← VERTEXSTEEPESTPATH(G, v0, xi)

Let j ∈ arg maxj∈{1,2,3} ∇Pj(v0)
G′ ← COMPHIGHPRESSGRAPH(G, v0, ∇Pj (v0))
if EG′ = ∅,

1. while T (v0) 6= VG
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.

then v0 ← ﬁx[v0, P ]
else Let G′

for i = 1, . . . , r

i, i = 1, . . . , r be the connected components of G′.

21

vi ← FIXPATHSABOVEPRESS(G′
for x ∈ VG′

i, set v0(x) ← vi(x)

i, v0|VG′

i

, ∇Pj (v0))

if α > 0 then G ←COMPHIGHPRESSGRAPH(G, v0, α)

13.
14.
15.
16. return v0

C Experiments on WebSpam: Testing More Algorithms

For completeness, in this appendix we show how a number of algorithms perform on the web spam experiment of
Section 6. We consider the following algorithms:

• RANDWALK along in-links. For a detailed description see Zhou et al. (2007). This algorithm essentially per-
forms a Personalized PageRank random walk from each vertex x and computes a spam-value for the vertex x by
taking a weighted average of the labels of the vertices where the random walk from x terminates. Also shown in
Section 6.

• DIRECTEDLEX, with edges in the opposite directions of links. This has the effect that a link to a spam host is

evidence of spam, and a link from a normal host is evidence of normality. Also shown in Section 6.

• RANDWALK along out-links.

• DIRECTEDLEX, with edges in the directions of links. This has the effect that a link from to a spam host is

evidence of spam, and a link to a normal host is evidence of normality.

• UNDIRECTEDLEX: Lex-minimization with links treated as undirected edges.

• LAPLACIAN: l2-regression with links treated as undirected edges.

• DIRECTED 1-NEAREST NEIGHBOR: Uses shortest distance along paths following out-links. Spam-ratio is
deﬁned distance from normal hosts, divided by distance to spam hosts. Sites are ﬂagged as spam when spam-
ratio exceeds some threshold. We also tried following paths along in-links instead, but that gave much worse
results.

We use the experimental setup described in Section 6. Results are shown in Figure 4. The alternative convention
for DIRECTEDLEX orients edges in the directions of links. This takes a link from a spam host to be evidence of
spam, and a link to a normal host to be evidence of normality. This approach performs signiﬁcantly worse than our
preferred convention, as one would intuitively expect. UNDIRECTEDLEX and LAPLACIAN approaches also perform
signiﬁcantly worse. DIRECTED 1-NEAREST NEIGHBOR performs poorly, demonstrating that DIRECTEDLEX is very
different from that approach. As observed by Zhou et al. (2007), sampling based on a random walk following out-links
performs worse than following in-links. Up to 60 % recall, DIRECTEDLEX performs best, both in the regime of 5 %
labels for training and in the regime of 20 % labels for training.

22

5 % labels for training

RandWalk along in-links
DirectedLex
RandWalk along out-links
DirectedLex, opposite link direction
UndirectedLex
Laplacian
Directed 1NN

0
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Recall

20 % labels for training

RandWalk along in-links
DirectedLex
RandWalk along out-links
DirectedLex, opposite link direction
UndirectedLex
Laplacian
Directed 1NN

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

i

i

n
o
s
c
e
r
P

i

i

n
o
s
c
e
r
P

0
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Recall

Figure 4: Recall and precision in the WebSpam classiﬁcation experiment. Each data point shown was computed as an average
over 100 runs. The largest standard deviation of the mean precision across the plotted recall values was less than 1.5 %. The
algorithm of Zhou et al. (2007) appears as RANDWALK (along in-links). We also show RANDWALK along out-links. Our directed
lex-minimization algorithm appears as DIRECTEDLEX. We also show DIRECTEDLEX with link directions reversed, along with
UNDIRECTEDLEX and LAPLACIAN.

D l0-Vertex Regularization Proofs

In this appendix, we prove Theorem 7.1 and Theorem 7.2. For the purposes of proving the second theorem, we intro-
duce an alternative version of problem (3). The optimization problem here requires us to minimize l0-regularization

23

budget required to obtain an inf-minimizer with gradient below a given threshold:

min
v∈IRn
subject to

(cid:13)
(cid:13)

v(T ) − v0(T )

0

gradG[v]

(cid:13)
∞ ≤ α.
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We will also need the following graph construction.

Deﬁnition D.1 The α-pressure terminal graph of a partially-labeled graph (G, v0) is a directed unweighted graph
Gα = (T (v0),

E if and only if there is a terminal path P from s to t in G with

E) such that (s, t) ∈

b

b

∇P (v0) > α.

Note that the α-pressure terminal graph has O(n) vertices but may be dense, even when G is not.

Algorithm 13: Algorithm TERM-PRESSURE: Given a well-posed instance (G, v0) and α ≥ 0, outputs α pressure terminal
graph Gα.
Initialize Gα with vertex set Vα = T (v0) and edge set
for each terminal s ∈ T (v0)

E = ∅.

1. Compute the distances to every other terminal t by running Dijktra’s algorithm, allowing shortest paths

b

2. Use the resulting distances to check for every other terminal t if there is a terminal path P from s to t with

that run through other terminals.

∇P (v0) > α. If there is, add edge (s, t) to

E.

Lemma D.2 The α-pressure terminal graph of a voltage problem (G, v0) can be computed in O((m + n log n)n) time
using algorithm TERM-PRESSURE (Algorithm 13).

b

Proof: The correctness of the algorithm follows from the fact that Dijkstra’s algorithm will identify all shortest
distances between the terminals, and the pressure check will ensure that terminal pairs (s, t) are added to
E if and
only if they are the endpoints of a terminal path P with ∇P (v0) > α. The running time is dominated by performing
Dijkstra’s algorithm once for each terminal. A single run of Dijkstra’s algorithm takes O(m + n log n) time, and this
✷
is performed at most n times, for a total running time of O((m + n log n)n).

b

We make three observations that will turn out to be crucial for proving Theorems 7.1 and 7.2.

Observation D.3 Gα is a subgraph of Gβ for α ≥ β.

Proof: Suppose edge (s, t) appears in Gα, then for some path P

∇P (v0) > α ≥ β,

so the edge also appears in Gβ.

Observation D.4 Gα is transitively closed.

Proof: Suppose edges (s, t) and (t, r) appear in Gα. Let P(s,t), P(t,r), P(s,r) be the respective shortest paths in G
between these terminal pairs. Then

∇P(s,r)(v0) =

v0(s) − v0(r)
ℓ(P(s,r))

≥

v0(s) − v0(r)
ℓ(P(s,t)) + ℓ(P(t,r))

=

v0(s) − v0(t) + v0(t) − v0(r)
ℓ(P(s,t)) + ℓ(P(t,r))

≥ min

v0(s) − v0(t)
ℓ(P(s,t))

,

 

v0(t) − v0(r)

ℓ(P(t,r)) !

> α.

So edge (s, r) also appears in Gα. This is sufﬁcient for Gα to be transitively closed.

24

(6)

✷

(7)

✷

Observation D.5 Gα is a directed acyclic graph.

Proof: Suppose for a contradiction that a directed cycle appears in Gα. Let s and t be two vertices in this cycle. Let
P(s,t) and P(t,s) be the respective shortest paths in G between these terminal pairs. Because Gα is transitively closed,
both edges (s, t) and (t, s) must appear in Gα. But (s, t) ∈

E implies

and similarly (t, s) ∈

E implies

b
This is a contradiction.

v0(s) − v0(t) > αℓ(P(s,t)) > 0,

b

v0(t) − v0(s) > αℓ(P(t,s)) > 0.

✷

The usefulness of the α-pressure terminal graph is captured in the following lemma. We deﬁne a vertex cover of a
directed graph to be a vertex set that constitutes a vertex cover in the same graph with all edges taken to be undirected.

Lemma D.6 Given a partially-labeled graph (G, v0) and a set U ⊆ V , there exists a voltage assignment v ∈ IRn that
satisﬁes

if and only if U is a vertex cover in the α-pressure terminal graph Gα of (G, v0).
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:8)

(cid:9)

t ∈ T (v0) : v(t) 6= v0(t)

⊆ U and

gradG[v]

∞ ≤ α,

Proof: We ﬁrst show the “only if” direction. Suppose for a contradiction that there exists a voltage assignment v for
which
∞ ≤ α, but U is not a vertex cover in Gα. Let (s, t) be an edge Gα which is not covered by U . The
presence of this edge in Gα implies that there exists a terminal path P from s to t in G for which

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∇P (v0) > α.

But, by Lemma 3.5 this means there is no assignment v for G which agrees with v0 on s and t and has
α. This contradicts our assumption.

∞ ≤
(cid:13)
Now we show the “if” direction. Consider an arbitrary vertex cover U of Gα. Suppose for a contradiction that
(cid:13)
⊆ U .

t ∈ T (v0) : v(t) 6= v0(t)

gradG[v]

(cid:13)
(cid:13)

gradG[v]

there does not exist a voltage assignment v for G with
Deﬁne a partial voltage assignment vU given by

∞ ≤ α and

(cid:8)

(cid:9)

vU (t) =

v0(t)
∗

(

(cid:13)
(cid:13)

(cid:13)
(cid:13)
if t ∈ T (v0) \ U
o.w.

∞ ≤ α. By
The preceding statement is equivalent to saying that there is no v that extends vU and has
Lemma 3.5, this means there is terminal path between s, t ∈ T (vU ) with gradient strictly larger than α. But this
means an edge (s, t) is present in Gα and is not covered. This contradicts our assumption that U is a vertex cover. ✷

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We are now ready to prove Theorem 7.2.

∞

(cid:13)
(cid:13)

Proof of Theorem 7.2: We describe and prove the algorithm OUTLIER. The algorithm will reduce problem (3)
to problem (6): Suppose v∗ is an optimal assignment for problem (3).
It achieves a maximum gradient α∗ =
gradG[v∗]
. Using Dijkstra’s algorithm we compute the pairwise shortest distances between all terminals in G.
From these distances and the terminal voltages, we compute the gradient on the shortest path between each terminal
(cid:13)
pair. By Lemma 3.5, α∗ must equal one of these gradients. So we can solve problem (3) by iterating over the set of
(cid:13)
gradients between terminals and solving problem (6) for each of these O(n2) gradients. Among the assignments with
v(T ) − v0(T )

0 ≤ k, we then pick the solution that minimizes
(cid:13)
(cid:13)

In fact, we can do better. By Observation D.3, Gα is a subgraph of Gβ for α ≥ β. This means a vertex cover
(cid:13)
of Gα is also a vertex cover of Gβ, and hence the minimum vertex cover for Gβ is at least as large as the minimum
(cid:13)
vertex cover for Gα. This means we can do a binary search on the set of O(n2) terminal gradients to ﬁnd the minimum
gradient for which there exists an assignment with
0 ≤ k. This way, we only make O(log n) calls to
v(T ) − v0(T )
problem (6), in order to solve problem (3).
(cid:13)
(cid:13)

We use the following algorithm to solve problem (6).

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∞

.

25

1. Compute the α-pressure terminal graph Gα of G using the algorithm TERM-PRESSURE.
2. Compute a minimum vertex cover U of Gα using the algorithm KONIG-COVER from Theorem 7.3.
3. Deﬁne a partial voltage assignment vU given by

vU (t) =

v0(t)
∗

(

if t ∈ T (v0) \ U,
otherwise.

4. Using Algorithm 5, compute voltages v that extend vU and output v.

From Lemma D.2, it follows that step 1 computes the α-pressure terminal graph in polynomial time. From The-
orem 7.3 it follows that step 2 computes the a minimum vertex cover of the α-pressure terminal graph in polynomial
time, because our observations D.4 and D.5 establish that the graph is a TC-DAG. From Lemma D.6 and Theorem 4.6,
it follows that the output voltages solve program (6).

✷

To prove Theorem 7.1, we use the standard greedy approximation algorithm for MIN-VC (Vazirani (2001)).

Theorem D.7 2-Approximation Algorithm for Vertex Cover. The following algorithm gives a 2-approximation to
the Minimum Vertex Cover problem on a graph G = (V, E).

0. Initialize U = ∅.
1. Pick an edge (u, v) ∈ E that is not covered by U .
2. Add u and v to the set U .
3. Repeat from step 1 if there are still edges not covered by U .
4. Output U .

We are now in a position to prove Theorem 7.1

Proof of Theorem 7.1: Given an arbitrary k and a partially-labeled graph (G, v0), let α∗ be the optimum value
of program (3). Observe that by Lemma D.6, this implies that Gα∗ has a vertex cover of size k. Given the partial
assignment v0, for every vertex set U , we deﬁne

vU (t) =

v0(t)
∗

(

if t ∈ T (v0) \ U
o.w.

We claim the following algorithm APPROX-OUTLIER outputs a voltage assignment v with

gradG[v]

∞ ≤ α∗

(cid:13)
(cid:13)

(cid:13)
(cid:13)

and

v(T ) − v0(T )

(cid:13)
(cid:13)

Algorithm APPROX-OUTLIER:

0 ≤ 2k.
(cid:13)
(cid:13)

0. Initialize U = ∅.
1. Using the algorithm STEEPESTPATH (Algorithm 7), ﬁnd a steepest terminal path in G w.r.t. vU . Denote
this path P and let s and t be its terminal endpoints. If there is no terminal path with positive gradient, skip
to step 4.

2. Add s and t to the set U .
3. If |U | ≤ 2k − 2 then repeat from step 1.
4. Using the algorithm COMPINFMIN (Algorithm 5), compute voltages v that extend vU and output v.

From the stopping conditions, it is clear that |U | ≤ 2k. If in step 1 we ever ﬁnd that no terminal paths have positive
∞ = 0 ≤ α∗, by Lemma 3.5. Similarly if we ﬁnd a steepest
gradient then our v that extends vU will have
(cid:13)
(cid:13)

gradG[v]

(cid:13)
(cid:13)

26

gradG[v]

∞ ≤ α∗.

∞ ≤ α∗.
path with gradient less than α∗ w.r.t. vU , then for this U there exists v that extends vU and has
This will continue to hold when if we add vertices to U . Therefore, for the ﬁnal U , there will exist an v that extends
vU and has

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

If we never ﬁnd a steepest terminal path P with ∇P (v0) ≤ α∗, then each steepest path we ﬁnd corresponds to an
edge in Gα∗ that is not yet covered by U and our algorithm in fact implements the greedy approximation algorithm
for vertex cover described in Theorem D.7. This implies that the ﬁnal U is a vertex cover of Gα∗ of size at most 2k.
∞ ≤ α∗. This
By Lemma D.6, this implies that there exists a voltage assignment u extending vU that has
implies by Theorem 4.6 that the v we output has
(cid:13)
(cid:13)
In all cases, the v we output extends vU , so

∞ ≤ α∗.

gradG[u]

(cid:13)
(cid:13)

✷

gradG[v]
v(T ) − v0(T )
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

0 ≤ |U | ≤ 2k.
(cid:13)
(cid:13)

E Proof of Hardness of l0 regularization for l2

We will prove Theorem 7.4, by a reduction from minimum bisection. To this end, let G = (V, E) be any graph. We
will reduce the minimum bisection problem on G to our regularization problem. Let n = |V |. The graph on which we
will perform regularization will have vertex set

V ∪

V ,

V is a set of n vertices that are in 1-to-1 correspondence with V . We assume that every edge in G has weight 1.
V to the corresponding vertex in V by an edge of weight B, for some large B to be
V to each other by edges of weight B3. So, we have a complete
V to V , and the original graph G on V .

where
We now connect every vertex in
determined later. We also connect all of the vertices in
graph of weight B3 edges on
b
The input potential function will be

V , a matching of weight B edges connecting

b

b

b

v(a) =

b
0 for a ∈
1 for a ∈ V .
b

(

V , and

b

Now set k = n/2. We claim that we will be able to determine the value of the minimum bisection from the solution
to the regularization problem.

If S is the set of vertices on which v and w differ, then we know that the w is harmonic on S: for every a ∈ S,

w(a) is the weighted average of the values at its neighbors. In the following, we exploit the fact that |S| ≤ n/2.

Claim E.1 For every a ∈ S ∩

V , w(a) ≤ 2/nB2.

Proof: Let a be the vertex in S ∩
w-value equal to 0 by edges of weight B3. On the other hand, a has only one neighbor that is not in
w-value at most 1, and it is connected to that vertex by an edge of weight B. Call that vertex c. We have

V that maximizes w(a). So, a is connected to at least n/2 neighbors in

V with
V , that vertex has

b

b

b

((n − 1)B3 + B)w(a) = Bw(c) +

B3w(b)

b

b
V ,b6=a
Xb∈

= Bw(c) +

B3w(b) +

B3w(b)

b
V ∩S,b6=a
Xb∈

B3w(a)

≤ B +

b
V ∩S,b6=a
Xb∈
≤ B + (n/2 − 1)B3w(a).

b
V −S
Xb∈

Subtracting (n/2 − 1)B3w(a) from both sides gives

((n/2)B3 + B)w(a) ≤ B,

which implies the claim.

Claim E.2 For a ∈ S ∩ V , w(a) ≤ n/B.

27

✷

V . Let’s call that neighbor c. We know that w(c) ≤ 2/B2n. On the
Proof: Vertex a has exactly one neighbor in
other hand, vertex a has fewer than n − 1 neighbors in V , and each of these have w-value at most 1. Let da denote the
degree of a in G. Then,

b

So,

Let

and

bisection.

and at most

(B + da)w(a) ≤ da + B

2
B2n

.

w(a) ≤

da + 2/Bn
da + B
n + (2/Bn)
B + n

≤

≤ n/B.

|S| = k = n/2.

T = S ∩ V,

t = |T | .

(n − t)B − 4/B
b

(n − t)B + tn2/B.

We now estimate the value of the regularized objective function. To this end, we assume that

We will prove that S ⊂ V and so S = T and t = n/2.

Let δ denote the number of edges on the boundary of T in V . Once we know that t = n/2, δ is the size of a

Claim E.3 The contribution of the edges between V and

V to the objective function is at least

Proof: For the lower bound, we just count the edges between vertices in V \ T and
edges, and each of them has weight B. The endpoint in V \ T has w-value 1, and the endpoint in
most 2/nB2. So, the contribution of these edges is at least

V . There are n − t of these
V has w-value at

b

(n − t)B(1 − 2/nB2)2 ≥ (n − t)B(1 − 4/nB2) ≥ (n − t)B − 4/B.

b

For the upper bound, we observe that the difference in w-values across each of these n − t edges is at most 1, so their
total contribution is at most

Since for every vertex a ∈ T , w(a) ≤ n/B, and also every vertex b ∈
edges between T and

V is at most

t(n/B)2B = tn2/B.

b

b

V , w(b) ≤ 2/nB2, the contribution due to

We will see that this is the dominant term in the objective function. The next-most important term comes from the

edges in G.

(n − t)B.

28

✷

✷

Claim E.4 The contribution of the edges in G to the objective function is at least

and at most

δ(1 − 2n/B)

δ + (t2/2)(n/B)2

δ(1 − 2n/B) and δ.

(t2/2)(n/B)2.

Proof: Let (a, b) ∈ E. If neither a nor b is in T , then w(a) = w(b) = 1, and so this edge has no contribution. If
a ∈ T but b 6∈ T , then the difference in w-values on them is between (1 − n/B) and 1. So, the contribution of such
edges to the objective function is between

Finally, if a and b are in T , then the difference in w-values on them is at most n/B, and so the contribution of all such
edges to the objective function is at most

Claim E.5 The edges between pairs of vertices in

V contribute at most 2/B to the objective function.

Proof: As 0 ≤ w(a) ≤ 2/B2n for every a ∈

V , every edge between two vertices in

V can contribute at most

b

As there are fewer than n2/2 such edges, their total contribution to the objective function is at most

B3(2/B2n)2 = 4/Bn2.
b

b

(n2/2)(4/Bn2) = 2/B.

Lemma E.6 If n ≥ 4 and B = 2n3, the value of the objective function is at least

and at most

(n − t)B + δ − 1/2

(n − t)B + δ + 1/3.

Proof: Summing the contributions in the preceding three claims, we see that the value of the objective function is at
least

(n − t)B − 4/B + δ(1 − 2n/B) ≥ (n − t)B + δ − 4/B − 2nδ/B

≥ (n − t)B + δ − n3/B
≥ (n − t)B + δ − 1/2,

as δ ≤ (n/2)2.

Similarly, the objective function is at most

(n − t)B + tn2/B + δ + (t2/2)(n/B)2 + 2/B ≤ (n − t)B + n3/2B + δ + n4/8B2 + 2/B
≤ (n − t)B + n3/2B + δ + 1/32n2 + 1/n3
≤ (n − t)B + δ + 1/3.

Claim E.7 If n ≥ 2 and B = 2n3, then S ⊂ V .

Proof: The objective function is minimized by making t as large as possible, so t = n/2 and S ⊂ V .

29

✷

✷

✷

✷

Theorem E.8 The value of the objective function reveals the value of the minimum bisection in G.

Proof: The value of the objective function will be between

and

(n/2)B + δ − 1/2

(n/2)B + δ + 1/3.

So, the objective function will be smallest when δ is as small as possible.

✷

Theorem E.8 immediately implies Theorem 7.4.

30

5
1
0
2
 
n
u
J
 
0
3
 
 
]

G
L
.
s
c
[
 
 
2
v
0
9
2
0
0
.
5
0
5
1
:
v
i
X
r
a

Algorithms for Lipschitz Learning on Graphs ∗†

Rasmus Kyng
Yale University
rasmus.kyng@yale.edu

Anup Rao
Yale University
anup.rao@yale.edu

Sushant Sachdeva
Yale University
sachdeva@cs.yale.edu

Daniel A. Spielman
Yale University
spielman@cs.yale.edu

July 1, 2015

Abstract

We develop fast algorithms for solving regression problems on graphs where one is given the value of a function
at some vertices, and must ﬁnd its smoothest possible extension to all vertices. The extension we compute is the
absolutely minimal Lipschitz extension, and is the limit for large p of p-Laplacian regularization. We present an
algorithm that computes a minimal Lipschitz extension in expected linear time, and an algorithm that computes
an absolutely minimal Lipschitz extension in expected time eO(mn). The latter algorithm has variants that seem
to run much faster in practice. These extensions are particularly amenable to regularization: we can perform l0-
regularization on the given values in polynomial time and l1-regularization on the initial function values and on graph
edge weights in time eO(m3/2).

Our deﬁnitions and algorithms naturally extend to directed graphs.

1 Introduction

We consider a problem in which we are given a weighted undirected graph G = (V, E, ℓ) and values v0 : T → R
on a subset T of its vertices. We view the weights ℓ as indicating the lengths of edges, with shorter length indicating
greater similarity. Our goal it to assign values to every vertex v ∈ V \T so that the values assigned are as smooth as
possible across edges. A minimal Lipschitz extension of v0 is a vector v that minimizes

max
(x,y)∈E

(ℓ(x, y))−1

v(x) − v(y)

,

(cid:12)
(cid:12)

(cid:12)
(cid:12)

subject to v(x) = v0(x) for all x ∈ T . We call such a vector an inf-minimizer. Inf-minimizers are not unique. So,
among inf-minimizers we seek vectors that minimize the second-largest absolute value of ℓ(x, y)−1
v(x) − v(y)
across edges, and then the third-largest given that, and so on. We call such a vector v a lex-minimizer. It is also known
(cid:12)
as an absolutely minimal Lipschitz extension of v0.
(cid:12)
These are the limit of the solution to p-Laplacian minimization problems for large p, namely the vectors that solve

(cid:12)
(cid:12)

(1)

(2)

min
v∈Rn

v|T =v0|T X(x,y)∈E

(ℓ(x, y))−p|v(x) − v(y)|p.

The use of p = 2 was suggested in the foundational paper of Zhu et al. (2003), and is particularly nice because it can
be obtained by solving a system of linear equations in a symmetric diagonally dominant matrix, which can be done

∗This research was partially supported by AFOSR Award FA9550-12-1-0175, NSF grant CCF-1111257, a Simons Investigator Award to Daniel

Spielman, and a MacArthur Fellowship.

†Code used in this work is available at https://github.com/danspielman/YINSlex

1

very quickly (Cohen et al. (2014)). The use of larger values of p has been discussed by Alamgir and Luxburg (2011),
and by Bridle and Zhu (2013), but it is much more complicated to compute. The fastest algorithms we know for this
problem require convex programming, and then require very high accuracy to obtain the values at most vertices. By
taking the limit as p goes to inﬁnity, we recover the lex-minimizer, which we will show can be computed quickly.

The lex-minimization problem has a remarkable amount of structure. For example, in uniformly weighted graphs
the value of the lex-minimizer at every vertex not in T is equal to the average of the minimum and maximum of the
values at its neighbors. This is analogous to the property of the 2-Laplacian minimizer that the value at every vertex
not in T equals the average of the values at its neighbors.

1.1 Contributions

We ﬁrst present several important structural properties of lex-minimizers in Section 3.2. As we shall point out, some
of these were known from previous work, sometimes in restricted settings. We state them generally and prove them
for completeness. We also prove that the lex-minimizer is as stable as possible under perturbations of v0 (Section 3.1).
The structure of the lex-minimization problem has led us to develop elegant algorithms for its solution. Both the
algorithms and their analyses could be taught to undergraduates. We believe that these algorithms could be used in
place of 2-Laplacian minimization in many applications.

We present algorithms for the following problems. Throughout, m = |E| and n = |V |.

Inf-minimization: An algorithm that runs in expected time O(m + n log n) (Section 4.3).

Lex-minimization: An algorithm that runs in expected time O(n(m + n log n)) (Section 4), along with a variant that

runs quickly in practice (Section 4.4).

l1-regularization of edge lengths for inf-minimization: The problem of minimizing (1) given a limited budget with
O(m3/2)
which one can increase edge lengths is a linear programming problem. We show how to solve it in time
with an interior point method by using fast Laplacian solvers (Section 8). The same algorithm can accommodate
l1-regularization of the values given in v0.

e

l0-regularization of vertex values for inf-minimization: We give a polynomial time algorithm for l0-regularization
of the values at vertices. That is, we minimize (1) given a budget of a number of vertices that can be proclaimed
outliers and removed from T (Section 7.1). We solve this problem by reducing it to the problem of computing
minimum vertex covers on transitively closed directed acyclic graphs, a special case of minimum vertex cover
that can be solved in polynomial time.

After any regularization for inf-minimization, we suggest computing the lex-minimizer. We ﬁnd the result for l0-
regularization of vertex values to be particularly surprising, especially because we prove that the analogous problem
for 2-Laplacian minimization is NP-Hard (Section 7.2).

All of our algorithms extend naturally to directed graphs (Section 5). This is in contrast with the problem of
minimizing 2-Laplacians on directed graphs, which corresponds to computing electrical ﬂows in networks of resistors
and diodes, for which fast algorithms are not presently known.

We present a few experiments on examples demonstrating that the lex-minimizer can overcome known deﬁcien-
cies of the 2-Laplacian minimizer (Section 1.2, Figures 1,2), as well as a demonstration of the performance of the
directed analog of our algorithms on the WebSpam dataset of Castillo et al. (2006) (Section 6). In the WebSpam prob-
lem we use the link structure of a collection of web sites to ﬂag some sites as spam, given a small number of labeled
sites known to be spam or normal.

1.2 Relation to Prior Work

We ﬁrst encountered the idea of using the minimizer of the 2-Laplacian given by (2) for regression and classiﬁca-
tion on graphs in the work of Zhu et al. (2003) and Belkin et al. (2004) on semi-supervised learning. These works
transformed learning problems on sets of vectors into problems on graphs by identifying vectors with vertices and
constructing graphs with edges between nearby vectors. One shortcoming of this approach (see Nadler et al. (2009),

2

e
g
a

t
l

 

o
V
d
e
r
r
e

f

n

I

1

0.8

0.6

0.4

0.2

0

-0.2

-0.4

-0.6

-0.8

-1

-4

50 lex
50 l2
100 lex
100 l2
500 lex
500 l2
1000 lex
1000 l2

0.25

0.2

r
o
r
r
e
 
1
l
 
n
a
e
M

0.15

0.1

0.05

0
5000 

lex
2-Lap
labels

-2

0

2
Vertex position on real line

4

6

8

Figure 1: Lex vs 2-Laplacian on 1D gaussian clus-
ters.

Figure 2: kNN graphs on samples from 4D cube.

10000

20000

40000

80000

Number of Vertices

Alamgir and Luxburg (2011), Bridle and Zhu (2013)) is that if the number of vectors grows while the number of la-
beled vectors remains ﬁxed, then almost all the values of the 2-Laplacian minimizer converge to the mean of the
labels on most natural examples. For example, Nadler et al. (2009) consider sampling points from two Gaussian
distributions centered at 0 and 4 on the real line. They place edges between every pair of points (x, y) with length
exp(|x − y|2 /2σ2) for σ = 0.4, and provide only the labels v0(0) = −1 and v0(4) = 1. Figure 1 shows the values
of the 2-Laplacian minimizer in red, which are all approximately zero. In contrast, the values of the lex-minimizer in
blue, which are smoothly distributed between the labeled points, are shown.

The “manifold hypothesis” (see Chapelle et al. (2010), Ma and Fu (2011)) holds that much natural data lies near a
low-dimensional manifold and that natural functions we would like to learn on this data are smooth functions on the
manifold. Under this assumption, one should expect lex-minimizers to interpolate well. In contrast, the 2-Laplacian
minimizers degrade (dotted lines) if the number of labeled points remains ﬁxed while the total number of points grows.
In Figure 2, we demonstrate this by sampling many points uniformly from the unit cube in 4 dimensions, form their
8-nearest neighbor graph, and consider the problem of regressing the ﬁrst coordinate. We performed 8 experiments,
varying the number of labeled points in {50, 100, 500, 1000}. Each data point is the mean average l1 error over 100
experiments. The plots for root mean squared error are similar. The standard deviation of the estimations of the mean
are within one pixel, and so are not displayed. The performance of the lex-minimizer (solid lines) does not degrade as
the number of unlabeled points grows.

Analogous to our inf-minimizers, minimal Lipschitz extensions of functions in Euclidean space and over more
general metric spaces have been studied extensively in Mathematics (Kirszbraun (1934), McShane (1934), Whitney
(1934)). von Luxburg and Bousquet (2003) employ Lipschitz extensions on metric spaces for classiﬁcation and relate
these to Support Vector Machines. Their work inspired improvements in classiﬁcation and regression in metric spaces
with low doubling dimension (Gottlieb et al. (2013), Gottlieb et al. (2013b)). Theoretically fast, although not actually
practical, algorithms have been given for constructing minimal Lipschitz extensions of functions on low-dimensional
Euclidean spaces (Fefferman (2009a), Fefferman and Klartag (2009), Fefferman (2009b)). Sinop and Grady (2007)
suggest using inf-minimizers for binary classiﬁcation problems on graphs. For this special case, where all of the
given values are either 0 or 1, they present an O(m + n log n) time algorithm for computing an inf-minimizer. The
case of general given values, which we solve in this paper, is much more complicated. To compensate for the non-
uniqueness of inf-minimizers, they suggest choosing the inf-minimizer that minimizes (2) with p = 2. We believe that
the lex-minimizer is a more natural choice.

The analog of our lex-minimizer over continuous spaces is called the absolutely minimal Lipschitz extension
(AMLE). Starting with the work of Aronsson (1967), there have been several characterizations and proofs of the ex-
istence and uniqueness of the AMLE (Jensen (1993), Crandall et al. (2001), Barles and Busca (2001), Aronsson et al.
(2004)). Many of these results were later extended to general metric spaces, including graphs (Milman (1999),
Peres et al. (2011), Naor and Shefﬁeld (2010), Shefﬁeld and Smart (2010)). However, to the best of our knowledge,
fast algorithms for computing lex-minimizers on graphs were not known. For the special case of undirected, un-
weighted graphs, Lazarus et al. (1999) presented both a polynomial-time algorithm and an iterative method. Oberman

3

(2011) suggested computing the AMLE in Euclidean space by ﬁrst discretizing the problem and then solving the cor-
responding graph problem by an iterative method. However, no run-time guarantees were obtained for either iterative
method.

2 Notation and Basic Deﬁnitions

Lexicographic Ordering. Given a vector r ∈ Rm, let πr denote a permutation that sorts r in non-increasing order
by absolute value, i.e., ∀i ∈ [m − 1], |r(πr(i))| ≥ |r(πr(i + 1))|. Given two vectors r, s ∈ Rm, we write r (cid:22) s to
indicate that r is smaller than s in the lexicographic ordering on sorted absolute values, i.e.

∃j ∈ [m],

r(πr(j))

<

s(πs(j))

and ∀i ∈ [j − 1],

r(πr(i))

=

s(πs(i))

or ∀i ∈ [m],

=

r(πr(i))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.
(cid:12)
(cid:12)

s(πs(i))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

Note that it is possible that r (cid:22) s and s (cid:22) r while r 6= s. It is a total relation: for every r and s at least one of r (cid:22) s
or s (cid:22) r is true.

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Graphs and Matrices. We will work with weighted graphs. Unless explicitly stated, we will assume that they are
undirected. For a graph G, we let VG be its set of vertices, EG be its set of edges, and ℓG : EG → R+ be the
assignment of positive lengths to the edges. We let |VG| = n, and |EG| = m. We assume ℓG is symmetric, i.e.,
ℓG(x, y) = ℓG(y, x). When G is clear from the context, we drop the subscript.

A path P in G is an ordered sequence of (not necessarily distinct) vertices P = (x0, x1, . . . , xk), such that
(xi−1, xi) ∈ E for i ∈ [k]. The endpoints of P are denoted by ∂0P = x0, ∂1P = xk. The set of interior vertices
of P is deﬁned to be int(P ) = {xi : 0 < i < k}. For 0 ≤ i < j ≤ k, we use the notation P [xi : xj] to denote the
k
subpath (xi, . . . , xj). The length of P is ℓ(P ) =
i=1 ℓ(xi−1, xi).
A function v0 : V → R ∪ {∗} is called a voltage assignment (to G). A vertex x ∈ V is a terminal with
respect to v0 iff v0(x) 6= ∗. The other vertices, for which v0(x) = ∗, are non-terminals. We let T (v0) denote the
set of terminals with respect to v0. If T (v0) = V, we call v0 a complete voltage assignment (to G). We say that an
assignment v : V → R ∪ {∗} extends v0 if v(x) = v0(x) for all x such that v0(x) 6= ∗.

Given an assignment v0 : V → R ∪ {∗}, and two terminals x, y ∈ T (v0) for which (x, y) ∈ E, we deﬁne the

P

gradient on (x, y) due to v0 to be

gradG[v0](x, y) =

v0(x) − v0(y)
ℓ(x, y)

.

It may be useful to view gradG[v0](x, y) as the current in the edge (x, y) induced by voltages v0. When v0 is a
complete voltage assignment, we interpret gradG[v0] as a vector in Rm, with one entry for each edge. However, for
convenience, we deﬁne gradG[v0](x, y) = −gradG[v0](y, x). When G is clear from the context, we drop the subscript.
A graph G along with a voltage assignment v to G is called a partially-labeled graph, denoted (G, v). We say
that a partially-labeled graph (G, v0) is a well-posed instance if for every maximal connected component H of G, we
have T (v0) ∩ VH 6= ∅.

A path P in a partially-labeled graph (G, v0) is called a terminal path if both endpoints are terminals. We deﬁne

∇P (v0) to be its gradient:

∇P (v0) =

v0(∂0P ) − v0(∂1P )
ℓ(P )

.

If P contains no terminal-terminal edges (and hence, contains at least one non-terminal), it is a free terminal path.

Lex-Minimization. An instance of the LEX-MINIMIZATION problem is described by a partially-labeled graph
(G, v0). The objective is to compute a complete voltage assignment v : VG → R extending v0 that lex-minimizes
grad[v].

Deﬁnition 2.1 (Lex-minimizer) Given a partially-labeled graph (G, v0), we deﬁne lexG[v0] to be a complete voltage
assignment to V that extends v0, and such that for every other complete assignment v′ : VG → R that extends v0, we
have gradG[lexG[v0]] (cid:22) gradG[v′]. That is, lexG[v0] achieves a lexicographically-minimal gradient assignment to the
edges.

We call lexG[v0] the lex-minimizer for (G, v0). Note that if T (v0) = VG, then trivially, lexG[v0] = v0.

4

3 Basic Properties of Lex-Minimizers

Lazarus et al. (1999) established that lex-minimizers in unweighted and undirected graphs exist, are unique, and may
be computed by an elementary meta-algorithm. We state and prove these facts for undirected weighted graphs, and
defer the discussion of the directed case to Section 5. We also state for directed and weighted graphs characterizations
of lex-minimizers that were established by Peres et al. (2011), Naor and Shefﬁeld (2010) and Shefﬁeld and Smart
(2010) for unweighted graphs. These results are essential for the analyses of our algorithms. We defer most proofs to
Appendix A.

Deﬁnition 3.1 A steepest ﬁxable path in an instance (G, v0) is a free terminal path P that has the largest gradient
∇P (v0) amongst such paths.

Observe that a steepest ﬁxable path with ∇P (v0) 6= 0 must be a simple path.
Deﬁnition 3.2 Given a steepest ﬁxable path P in an instance (G, v0), we deﬁne ﬁxG[v0, P ] : VG → R ∪ {∗} to be the
voltage assignment deﬁned as follows

ﬁxG[v0, P ](x) =

v0(∂0P ) − ∇P (v0) · ℓG(P [∂0P : x]) x ∈ int(P ) \ T (v0),
v0(x)

otherwise.

(

We say that the vertices x ∈ int(P ) are ﬁxed by the operation ﬁx[v0, P ]. If we deﬁne v1 = ﬁxG[v0, P ], where
P = (x0, . . . , xr) is the steepest ﬁxable path in (G, v0), then it is easy to argue that for every i ∈ [r], we have
grad[v1](xi−1, xi) = ∇P (see Lemma A.5). The meta-algorithm META-LEX, spelled out as Algorithm 1, entails
repeatedly ﬁxing steepest ﬁxable paths. While it is possible to have multiple steepest ﬁxable paths, the result of ﬁxing
all of them does not depend on the order in which they are ﬁxed.

Theorem 3.3 Given a well-posed instance (G, v0), the meta-algorithm META-LEX, which repeatedly ﬁxes steepest
ﬁxable paths, produces the unique lex-minimizer extending v0.

Corollary 3.4 Given a well-posed instance (G, v0) such that T (v0) 6= VG, let P be a steepest ﬁxable path in (G, v0).
Then, (G, ﬁx[v0, P ]) is also a well-posed instance, and lexG[ﬁx[v0, P ]] = lexG[v0].

Since a lex-minimal element must be an inf-minimizer, we also obtain the following corollary, that can also be

proved using LP duality.

Lemma 3.5 Suppose we have a well-posed instance (G, v0). Then, there exists a complete voltage assignment v
extending v0 such that

grad[v]

∞ ≤ α, iff every terminal path P in (G, v0) satisﬁes ∇P (v0) ≤ α.
(cid:13)
(cid:13)

3.1 Stability

(cid:13)
(cid:13)

The following theorem states that lexG[v0] is monotonic with respect to v0 and it respects scaling and translation of
v0.

Theorem 3.6 Let (G, v0) be a well-posed instance with T := T (v0) as the set of terminals. Then the following
statements hold.

1. For any c, d ∈ R, v1 a partial assignment with terminals T (v1) = T and v1(t) = cv0(t) + d for all t ∈ T .

Then, lexG[v1](i) = c · lexG[v0](i) + d for all i ∈ VG.

2. v1 a partial assignment with terminals T (v1) = T. Suppose further that v1(t) ≥ v0(t) for all t ∈ T. Then,

lexG[v1](i) ≥ lexG[v0](i) for all i ∈ VG.

As a corollary, the above theorem gives a nice stability property that lex-minimal elements satisfy.

Corollary 3.7 Given well-posed instances (G, v0), (G, v1) such that T := T (v0) = T (v1), let ǫ := maxt∈T |v0(t) −
v1(t)|. Then |lexG[v0](i) − lexG[v1](i)| ≤ ǫ for all i ∈ VG.

5

3.2 Alternate Characterizations

There are at least two other seemingly disparate deﬁnitions that are equivalent to lex-minimal voltages.

lp-norm Minimizers. As mentioned in the introduction, for a well-posed instance (G, v0) the lex-minimizer is also
the limit of lp minimizers. This follows from existing results about the limit of lp-minimizers (Egger and Huotari
(1990)) in afﬁne spaces, since {grad[v] | v is complete, v extends v0} forms an afﬁne subspace of Rm. Thus, we have
the following theorem:

Theorem 3.8 (Limit of lp-minimizers, follows from Egger and Huotari (1990)) For any p ∈ (1, ∞), given a well-
posed instance (G, v0) deﬁne vp to be the unique complete voltage assignment extending v0 and minimizing
p ,
i.e.

grad[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Then, limp→∞ vp = lexG[v0].

vp = arg min
v is complete
v extends v0 (cid:13)
(cid:13)

grad[v]

p .

(cid:13)
(cid:13)

Max-Min Gradient Averaging. Consider a well-posed instance (G, v0), and a complete voltage assignment v ex-
tending v0. If G is such that ℓ(e) = 1 for all e ∈ EG, it is easy to see that lex = lexG[v0] satisﬁes the following simple
condition for all x ∈ VG \ T (v0),

lex(x) =

1
2  

max
(x,y)∈EG

lex(y) + min

lex(z)

.

(x,z)∈EG

!

This condition should be contrasted to the optimality condition for l2-regularization on these instances, which gives
for all non-terminals x, the optimal voltage v satisﬁes v(x) = 1

y:(x,y)∈EG v(y).

deg(x)

To prove the above claim, consider locally changing lex at x and observe that the gradients of edges not incident
at x remain unchanged, and at least one of edges incident at x will have a strictly larger gradient, contradicting lex-
minimality. For general graphs, this condition of local optimality can still be characterized by a simple max-min
gradient averaging property as described below.

P

Deﬁnition 3.9 (Max-Min Gradient Averaging) Given a well-posed instance (G, v0), and a complete voltage as-
signment v extending v0, we say that v satisﬁes the max-min gradient averaging property (w.r.t. (G, v0)) if for every
x ∈ VG \ T (v0), we have

grad[v](x, y) = − min

grad[v](x, y).

max
y:(x,y)∈EG

y:(x,y)∈EG

As stated in the theorem below, lexG[v0] is the unique assignment satisfying max-min gradient averaging property.
Shefﬁeld and Smart (2010) proved a variant of this statement for weighted graphs. For completeness, we present a
proof in the appendix.

Theorem 3.10 Given a well-posed instance (G, v0), lexG[v0] satisﬁes max-min gradient averaging property. More-
over, it is the unique complete voltage assignment extending v0 that satisﬁes this property w.r.t. (G, v0).

An advantage of this characterization is that it can be veriﬁed quickly. This is particularly useful for implementations
for computing the lex-minimizer.

4 Algorithms

We now sketch the ideas behind our algorithms and give precise statements of our results. A full description of all the
algorithms is included in the appendix.

We deﬁne the pressure of a vertex to be the gradient of the steepest terminal path through it:

pressure[v0](x) = max{∇P (v0) | P is a terminal path in (G, v0) and x ∈ P }.

6

Observe that in a graph with no terminal-terminal edges, a free terminal path is a steepest ﬁxable path iff its gradient
is equal to the highest pressure amongst all vertices. Moreover, vertices that lie on steepest ﬁxable paths are exactly
the vertices with the highest pressure. For a given α > 0, in order to identify vertices with pressure exceeding α, we
compute vectors vHigh[α](x) and vLow[α](x) deﬁned as follows in terms of dist, the metric on V induced by ℓ:

vLow[α](x) = min

{v0(t) + α · dist(x, t)}

t∈T (v0)

vHigh[α](x) = max
t∈T (v0)

{v0(t) − α · dist(t, x)}.

4.1 Lex-minimization on Star Graphs

We ﬁrst consider the problem of computing the lex-minimizer on a star graph in which every vertex but the center is a
terminal. This special case is a subroutine in the general algorithm, and also motivates some of our techniques.

Let x be the center vertex, T be the set of terminals, and all edges be of the form (x, t) with t ∈ T . The initial
voltage assignment is given by v : T → R, and we abbreviate dist(x, t) by d(t) = ℓ(x, t). From Corollary 3.4 we know
that we can determine the value of the lex minimizer at x by ﬁnding a steepest ﬁxable path. By deﬁnition, we need to
ﬁnd t1, t2 ∈ T that maximize the gradient of the path from t1 to t2, ∇(t1, t2) = v(t1)−v(t2)
d(t2)+d(t2) . As observed above, this
is equivalent to ﬁnding a terminal with the highest pressure. We now present a simple randomized algorithm for this
problem that runs in expected linear time.

Given a terminal t1, we can compute its pressure α along with the terminal t2 such that |∇(t1, t2)| = α in time
O(|T |) by scanning over the terminals in T . Consider doing this for a random terminal t1. We will show that in linear
time one can then ﬁnd the subset of terminals T ′ ⊂ T whose pressure is greater than α. Assuming this, we complete
the analysis of the algorithm. If T ′ = ∅, t1 is a vertex with highest pressure. Hence the path from t1 to t2 is a steepest
ﬁxable path, and we return (t1, t2). If T ′ 6= ∅, the terminal with the highest pressure must be in T ′, and we recurse by
picking a new random t1 ∈ T ′. As the size of T ′ will halve in expectation at each iteration, the expected time of the
algorithm on the star is O(|T |).

To determine which terminals have pressure exceeding α, we observe that the condition ∃t2 : α < ∇(t1, t2) =
v(t1)−v(t2)
d(t1)+d(t2) , is equivalent to ∃t2 : v(t2)+αd(t2) < v(t1)−αd(t1). This, in turn, is equivalent to vLow[α](x) < v(t1)−
αd(t1). We can compute vLow[α](x) in deterministic O(|T |) time. Similarly, we can check if ∃t2 : α < ∇(t2, t1) by
checking if vHigh[α](x) > vt1 + αd(t1). Thus, in linear time, we can compute the set T ′ of terminals with pressure
exceeding α. The above algorithm is described in Algorithm 10.

Theorem 4.1 Given a set of terminals T, initial voltages v : T → R, and distances d : T → R+, STARSTEEPESTPATH(T, v, d)
returns (t1, t2) maximizing v(t1)−v(t2)

d(t1)+d(t2) , and runs in expected time O(|T |).

4.2 Lex-minimization on General Graphs

Theorem 3.3, tells us that META-LEX will compute lex-minimizers given an algorithm for ﬁnding a steepest ﬁxable
path in (G, v0). Recall that ﬁnding a steepest ﬁxable path is equivalent to ﬁnding a path with gradient equal to the
highest pressure amongst all vertices. In this section, we show how to do this in expected time O(m + n log n).

We describe an algorithm VERTEXSTEEPESTPATH that ﬁnds a terminal path P through any vertex x such that
∇P (v0) = pressure[v0](x) in expected O(m + n log n) time. Using Dijkstra’s algorithm, we compute dist(x, t) for
all t ∈ T. If x ∈ T (v0), then there must be a terminal path P that starts at x that has ∇P (v0) = pressure[v0](x). To
compute such a P we examine all t ∈ T (v0) in O(|T |) time to ﬁnd the t that maximizes |∇(x, t)| = |v(x)−v(t)|
, and
dist(x,t)
then return a shortest path between x and that t.

If x /∈ T (v0), then the steepest path through x between terminals t1 and t2 must consist of shortest paths between
x and t1 and between x and t2. Thus, we can reduce the problem to that of ﬁnding the steepest path in a star graph
where x is the only non-terminal and is connected to each terminal t by an edge of length dist(x, t). By Theorem 4.1,
we can ﬁnd this steepest path in O(|T |) expected time. The above algorithm is formally described as Algorithm 9.

Theorem 4.2 Given a well-posed instance (G, v0), and a vertex x ∈ VG, VERTEXSTEEPESTPATH(G, v0, x) returns
a terminal path P through x such that ∇P (v0) = pressure[v0](x), in O(m + n log n) expected time.

7

As in the algorithm for the star graph, we need to identify the vertices whose pressure exceeds a given α. For a ﬁxed
α, we can compute vLow[α](x) and vHigh[α](x) for all x ∈ VG using a simple modiﬁcation of Dijkstra’s algorithm in
O(m + n log n) time. We describe the algorithms COMPVHIGH, COMPVLOW for these tasks in Algorithms 3 and 4.
The following lemma encapsulates the usefulness of vLow and vHigh.

Lemma 4.3 For every x ∈ VG, pressure[v0](x) > α iff vHigh[α](x) > vLow[α](x).

It immediately follows that the algorithm COMPHIGHPRESSGRAPH(G, v0, α) described in Algorithm 6 computes

the vertex induced subgraph on the vertex set {x ∈ VG| pressure[v0](x) > α}.

We can combine these algorithms into an algorithm STEEPESTPATH that ﬁnds the steepest ﬁxable path in (G, v0)
in O(m + n log n) expected time. We may assume that there are no terminal-terminal edges in G. We sample an edge
(x1, x2) uniformly at random from EG, and a terminal x3 uniformly at random from VG. For i = 1, 2, 3, we compute
the steepest terminal path Pi containing xi. By Theorem 4.2, this can be done in O(m + n log n) expected time. Let α
be the largest gradient maxi ∇Pi. As mentioned above, we can identify G′, the induced subgraph on vertices x with
pressure exceeding α, in O(m + n log n) time. If G′ is empty, we know that the path Pi with largest gradient is a
steepest ﬁxable path. If not, a steepest ﬁxable path in (G, v0) must be in G′, and hence we can recurse on G′. Since
we picked a uniformly random edge, and a uniformly random vertex, the expected size of G′ is at most half that of G.
Thus, we obtain an expected running time of O(m + n log n). This algorithm is described in detail in Algorithm 7.

Theorem 4.4 Given a well-posed instance (G, v0) with EG ∩ (T (v0) × T (v0)) = ∅, STEEPESTPATH(G, v0) returns
a steepest ﬁxable path in (G, v0), and runs in O(m + n log n) expected time.

By using STEEPESTPATH in META-LEX, we get the COMPLEXMIN, shown in Algorithm 1. From Theorem 3.3 and
Theorem 4.4, we immediately get the following corollary.

Corollary 4.5 Given a well-posed instance (G, v0) as input, algorithm COMPLEXMIN computes a lex-minimizing
assignment that extends v0 in O(n(m + n log n)) expected time.

4.3 Linear-time Algorithm for Inf-minimization

Given the algorithms in the previous section, it is straightforward to construct an inﬁnity minimizer. Let α⋆ be the
gradient of the steepest terminal path. From Lemma 3.5, we know that the norm of the inf minimizer is α⋆. Considering
all trivial terminal paths (terminal-terminal edges), and using STEEPESTPATH, we can compute α⋆ in randomized
O(m+n log n) time. It is well known (McShane (1934); Whitney (1934)) that v1 = vLow[α⋆] and v2 = vHigh[α⋆] are
inf-minimizers. It is also known that 1
2 (v1 + v2) is the inf-minimizer that minimizes the maximum ℓ∞-norm distance
to all inf-minimizers. In the case of path graphs, this was observed by Gaffney and Powell (1976) and independently
by Micchelli et al. (1976). For completeness, the algorithm is presented as Algorithm 5, and we have the following
result.

Theorem 4.6 Given a well-posed instance (G, v0), COMPINFMIN(G, v0) returns a complete voltage assignment v
for G extending v0 that minimizes

∞ , and runs in randomized O(m + n log n) time.

grad[v]

4.4 Faster Algorithms for Lex-minimization

(cid:13)
(cid:13)

(cid:13)
(cid:13)

The lex-minimizer has additional structure that allows one to compute it by more efﬁcient algorithms. One observation
that leads to a faster implementation is that ﬁxing a steepest ﬁxable path does not increase the pressure at vertices,
provided that one appropriately ignores terminal-terminal edges. Thus, if G(α) is a subgraph that we identiﬁed with
pressure greater than α, we can iteratively ﬁx all steepest ﬁxable paths P in G(α) with ∇P > α. Another simple
observation is that if G(α) is disconnected, we can simply recurse on each of the connected components. A complete
description of an the algorithm COMPFASTLEXMIN based on these idea is given in Algorithm 11. The algorithm
provably computes lexG(v0), and it is possible to implement it so that the space requirement is only O(m + n).
Although, we are unable to prove theoretical bounds on the running time that are better than O(n(m + n log n)),
it runs extremely quickly in practice. We used it to perform the experiments in this paper. For random regular
graphs and Delaunay graphs, with n = 0.5 × 106 vertices and around 2 million edges m ∼ 1.5 − 2 × 106, it

8

takes a couple of minutes on a 2009 MacBook Pro. Similar times are observed for other model graphs of this
size such as random regular graphs and real world networks. An implementation of this algorithm may be found
at https://github.com/danspielman/YINSlex.

5 Directed Graphs

Our deﬁnitions and algorithms, including those for regularization, extend to directed graphs with only small modiﬁ-
cations. We view directed edges as diodes and only consider potential differences in the direction of the edge. For
a complete voltage assignment v on the vertices of a directed graph G, we deﬁne the directed gradient on (x, y) due
to v to be grad+
. Given a partially-labelled directed graph (G, v0), we say that a a
complete voltage assignment v is a lex-minimizer if it extends v0 and for other complete voltage assignment v′ that
extends v0 we have grad+
G[v′]. We say that a partially-labelled directed graph (G, v0) is a well-posed
directed instance if every free vertex appears in a directed path between two terminals.

G[v](x, y) = max

G[v] (cid:22) grad+

v(x)−v(y)
ℓ(x,y)

, 0

n

o

The main difference between the directed and undirected cases is that the directed lex-minimizer is not necessarily
unique. To maintain clarity of exposition, we chose to focus on undirected graphs so far. For directed graphs, we have
the following corresponding structural results.

Theorem 5.1 Given a well-posed instance (G, v0) on a directed graph G, there exists a lex-minimizer, and the set of
all lex-minimizers is a convex set. Moreover, for every two lex-minimizers v and v′, we have grad+

G[v] = grad+

G[v′].

However, note that in the case of directed graphs, the lex-minimizer need not be unique. We still have a weaker version
of Theorem 3.3 for directed graphs.

Theorem 5.2 Given a well-posed instance (G, v0) on a directed graph G, let v1 be the partial voltage assignment
extending v0 obtained by repeatedly ﬁxing steepest ﬁxable (directed) paths P with ∇P > 0. Then, any lex-minimizer
of (G, v0) must extend v1. Moreover, for every edge e ∈ EG \ (T (V1) × T (V1)), any lex-minimizer v of (G, v0) must
satisfy grad+[v](e) = 0.

When the value of the lex-minimizer at a vertex is not uniquely determined, it is constrained to an interval. In our
experiments, we pick the convention that when the voltage at a vertex is constrained to an interval (−∞, a] or [a, ∞),
we assign a to the terminal. When it is constrained to a ﬁnite interval, we assign a voltage closest to the median of the
original voltages.

6 Experiments on WebSpam

We demonstrate the performance of our lex-minimization algorithms on directed graphs by using them to detect spam
webpages as in Zhou et al. (2007). We use the dataset webspam-uk2006-2.0 described in Castillo et al. (2006).
This collection includes 11,402 hosts, out of which 7,473 (65.5 %) are labeled, either as spam or normal. Each host
corresponds to the collection of web pages it serves. Of the hosts, 1924 are labeled spam (25.7 % of all labels). We
consider the problem of ﬂagging some hosts as spam, given only a small fraction of the labels for training. We assign
a value of 1 to the spam hosts, and a value of 0 to the normal ones. We then compute a lex minimizer and examine the
effect of ﬂagging as spam all hosts with a value greater than some threshold.

Following Zhou et al. (2007), we create edges between hosts with lengths equal to the reciprocal of the number of
links from one to the other. We run our experiments only on the largest strongly connected component of the graph,
which contains 7945 hosts of which 5552 are labeled. 16 % of the nodes in this subgraph are labeled spam. To create
training and test data, for a given value p, we select a random subset of p % of the spam labels and a random subset
of p % of the normal labels to use for training. The remaining labels are used for testing. We report results for p = 5
and p = 20.

Again following Zhou et al. (2007), we plot the precision and recall of different choices of threshold for ﬂagging
pages as spam. Recall is the fraction of spam pages our algorithm ﬂags as spam, and precision is the fraction of pages
our algorithm ﬂags as spam that actually are spam. Amongst the algorithms studied by Zhou et al. (2007), the top

9

performer was their algorithm based on sampling according to a random-walk that follows in-links from other hosts.
We compare their algorithm with the classiﬁcation we get by directing edges in the opposite directions of links. This
has the effect that a link to a spam host is evidence of spamminess, and a link from a normal host is evidence of
normality.

Results are shown in Figure 3. While we are not able to reliably ﬂag all spam hosts, we see that in the range of
10-50 % recall, we are able to ﬂag spam with precision above 82 %. We see that the performance of directed lex-
minimization does not degrade rapidly when from the “large training set” regime of p = 20, to the “small training set”
regime of p = 5.

5 % labels for training

20 % labels for training

RandWalk
DirectedLex

RandWalk
DirectedLex

1

0.9

0.8

0.7

i

i

n
o
s
c
e
r
P

0.6

0.5

0.4

0.3

0.2

0.1

0
0.1

0.2

0.3

0.4

0.7

0.8

0.9

1

0.2

0.3

0.4

0.7

0.8

0.9

1

0.6
0.5
Recall

0.6
0.5
Recall

Figure 3: Recall and precision in the web spam classiﬁcation experiment. Each data point shown was computed as an average over
100 runs. The largest standard deviation of the mean precision across the plotted recall values was less than 1.3 %. The algorithm
of Zhou et al. (2007) appears as RANDWALK. Our directed lex-minimization algorithm appears as DIRECTEDLEX.

For comparison, in Appendix C, we show the performance of our algorithm and that of Zhou et al. (2007) both
with link directions reversed, as well as the performance of undirected lex-minimization and Laplacian inference, all
of which are signiﬁcantly worse.

7 l0-Regularization of Vertex Values

We now explain how we can accommodate noise in both the given voltages and in the given lengths of edges. We can
ﬁnd the minimum number of labels to ignore, or the minimum increase in edges lengths needed so that there exists an
extension whose gradients have l∞-norm lower than a given target. After determining which labels to ignore or the
needed increment in edge lengths, we recommend computing a lex minimizer.

The algorithms we present in this section are essentially the same for directed and undirected graphs.

7.1 l0-Vertex Regularization for Inf-minimization

The l0-regularization of vertex labels can be viewed as a problem of outlier removal: the vector we compute is allowed
to disagree with v0 on up to k terminals. Given a voltage assignment v and a subset T ⊂ V of the vertices, by v(T )
we mean the vector obtained by restricting v to T . We deﬁne the l0-Vertex Regularization for l∞ problem to be

where v(T ) is the vector of values of v on the terminals T .

min
v∈IRn

gradG[v]

∞

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

0 ≤ k,
(cid:13)
(cid:13)

subject to

v(T ) − v0(T )

(3)

In Appendix D, we describe an approximation algorithm APPROX-OUTLIER that approximately solves program (3).

The precise statement we prove in Appendix D is given in the following theorem.

1

0.9

0.8

0.7

i

i

n
o
s
c
e
r
P

0.6

0.5

0.4

0.3

0.2

0.1

0
0.1

10

Theorem 7.1 (Approximate l0-vertex regularization) The algorithm APPROX-OUTLIER takes a positive integer k
and a partially-labeled graph (G, v0), and outputs an assignment v with
0 ≤ 2k, and
∞ ≤
α∗, where α∗ is the optimum value of program (3). The algorithm runs in time O(k(m + n log n)).
(cid:13)
(cid:13)
(cid:13)
(cid:13)

In Appendix D, we also describe an algorithm OUTLIER that exactly solves program (3) in polynomial time, and we
prove its correctness.

v(T ) − v0(T )

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Theorem 7.2 (Exact l0-vertex regularization) The algorithm OUTLIER takes a positive integer k and a partially-
labeled graph (G, v0) solves program (3) exactly. The algorithm runs in polynomial time.

We give a proof of Theorem 7.2 in Appendix D. To do this, we reduce the program (3) to the problem of minimizing
the required l0-budget needed to achieve a ﬁxed gradient α using a binary search over a set of O(n2) gradients. This
latter problem we reduce in polynomial time to Minimum Vertex Cover (VC) on a transitively closed, directed acyclic
graph (a TC-DAG). VC on a TC-DAG can be solved exactly in polynomial time by a reduction to the Maximum
Bipartite Matching Problem (Fulkerson (1956)). The problem was phrased by Fulkerson as one of ﬁnding a maximum
antichain of a ﬁnite poset. Any transitively closed DAG corresponds directly to the comparability graph of a poset. A
maximum antichain of a poset is a maximum independent set of a the comparability graph of the poset, and hence its
complement is a minimum vertex cover of the comparability graph. We refer to the algorithm developed by Fulkerson
as KONIG-COVER.

Theorem 7.3 The algorithm KONIG-COVER computes a minimum vertex cover for any transitively closed DAG G in
polynomial time.

7.2 Hardness of l0 regularization for l2

The result that l0-regularized inf-minimization can be solved exactly in polynomial time is surprising, especially
because the analogous problem for 2-Laplacian minimization turns out to be NP-Hard.

We deﬁne the the l0 vertex regularization for l2 for a partially-labeled graph (G, v0) and an integer k by

min
v∈Rn:kv(T )−v0(T )k0

≤k

vT Lv,

where L is the Laplacian of G.

Theorem 7.4 l0 vertex regularization for l2 is NP-Hard.

In Appendix E we prove Theorem 7.4 by giving a polynomial time (Karp) reduction from the NP-Hard minimum
bisection problem to l0 vertex regularization for l2.

8 l1-Edge and Vertex Regularization of Inf-minimizers

Consider a partially-labeled graph (G, v0) and an α > 0. The set of voltage assignments given by

v : v extends v0 and

gradG[v]

∞ ≤ α

n

(cid:13)
(cid:13)

(cid:13)
(cid:13)

o

is convex. Going further, let us consider the edge lengths in a graph to be speciﬁed by a vector ℓ ∈ IRE. Now the set
of voltages v and and lengths ℓ which achieve kgradG(ℓ)[v]k∞ ≤ α is jointly convex in v and ℓ. To see this, observe
that

kgradG(ℓ)[v]k∞ ≤ α ⇔ ∀(u, v) ∈ E : −αℓ(u, v) ≤ v(u) − v(v) ≤ αℓ(u, v).
Furthermore, the condition “v extends v0” is a linear constraint on v, which we express as v(T ) = v0(T ). From
the above, it is clear that the gradient condition corresponds to a convex set, as it is an intersection of half-spaces.
These half-spaces are given by O(m) linear inequalities. We can leverage this to phrase many regularized variants of
inf-minimization as convex programs, and in some cases linear programs.

(4)

11

For example, we may consider a variant of inf-minimization combined with an l1-budget for changing lengths of
edges and values on terminals. Given a parameter γ > 0 which speciﬁes the relative cost of regularizing terminals to
regularizing edges, the problem is as follows

arg min
v∈IRn,s∈IRm,s≥0

ksk1 + γ

v(T ) − v0(T )

1

subject to

gradG(ℓ+s)[v]

≤ α.

(5)

(cid:13)
(cid:13)
From our observation (4), it follows that problem (5) may be expressed as a linear program with O(n) variables
and O(m) constraints. We can use ideas from Daitch and Spielman (2008) to solve the resulting linear program in
O(m1.5) by an interior point method with a special purpose linear equation solver. The reason is that the linear
time
equations the IPM must solve at each iteration may be reduced to linear equations in symmetric, diagonally dominant
matrices, and these may be solved in nearly-linear time (Cohen et al. (2014)).

(cid:13)
(cid:13)

e

(cid:13)
(cid:13)
(cid:13)

∞

(cid:13)
(cid:13)
(cid:13)

Conclusion. We propose the use of inf and lex minimizers for regression on graphs. We present simple algorithms
for computing them that are provably fast and correct, and can also be implemented efﬁciently. We also present a
framework and polynomial time algorithms for regularization in this setting. The initial experiments reported in the
paper indicate that these algorithms give pretty good results on real and synthetic datasets. The results seem to compare
quite favorably to other algorithms, particularly in the regime of tiny labeled sets. We are testing these algorithms on
several other graph learning questions, and plan to report on them in a forthcoming experimental paper. We believe
that inf and lex minimizers, and the associated ideas presented in the paper, should be useful primitives that can be
proﬁtably combined with other approaches to learning on graphs.

We thank anonymous reviewers for helpful comments. We thank Santosh Vempala and Bartosz Walczak for pointing
out that it was already known how to compute a minimum vertex cover of a transitively closed DAG in polynomial
time.

Acknowledgements

References

Morteza Alamgir
In Advances
Information Processing
http://books.nips.cc/papers/files/nips24/NIPS2011_0278.pdf.

and Ulrike V. Luxburg.

transition
24,

in
pages

in Neural

Systems

Phase

the

family
379–387.

of
2011.

p-resistances.
URL

Gunnar Aronsson. Extension of functions satisfying lipschitz conditions. Arkiv fr Matematik, 6(6):551–561, 1967.

ISSN 0004-2080. doi: 10.1007/BF02591928. URL http://dx.doi.org/10.1007/BF02591928.

Gunnar Aronsson, Michael G. Crandall, and Petri Juutinen. A tour of the theory of absolutely minimizing functions.
ISSN 0273-0979. doi: 10.1090/S0273-0979-04-01035-3.

Bull. Amer. Math. Soc. (N.S.), 41(4):439–505, 2004.
URL http://dx.doi.org/10.1090/S0273-0979-04-01035-3.

Guy Barles and J´erˆome Busca. Existence and comparison results for fully nonlinear degenerate elliptic equations

without zeroth-order term. Comm. Partial Differential Equations, 26:2323–2337, 2001.

Mikhail Belkin, Irina Matveeva, and Partha Niyogi.

Regularization and semi-supervised learning on large
In Learning Theory, volume 3120 of Lecture Notes in Computer Science, pages 624–638.
doi: 10.1007/978-3-540-27819-1 43. URL

graphs.
Springer Berlin Heidelberg, 2004.
http://dx.doi.org/10.1007/978-3-540-27819-1_43.

ISBN 978-3-540-22282-8.

Nick Bridle and Xiaojin Zhu. p-voltages: Laplacian regularization for semi-supervised learning on high-dimensional

data. In Eleventh Workshop on Mining and Learning with Graphs (MLG2013), 2013.

12

Carlos Castillo, Debora Donato, Luca Becchetti, Paolo Boldi, Stefano Leonardi, Massimo Santini, and Sebastiano
Vigna. A reference collection for web spam. SIGIR Forum, 40(2):11–24, December 2006. ISSN 0163-5840. doi:
10.1145/1189702.1189703. URL http://doi.acm.org/10.1145/1189702.1189703.

Olivier Chapelle, Bernhard Schlkopf, and Alexander Zien. Semi-Supervised Learning. The MIT Press, 1st edition,

2010. ISBN 0262514125, 9780262514125.

Michael B Cohen, Rasmus Kyng, Gary L Miller, Jakub W Pachocki, Richard Peng, Anup B Rao, and Shen Chen Xu.
Solving SDD linear systems in nearly m log1/2 n time. In Proceedings of the 46th Annual ACM Symposium on
Theory of Computing, pages 343–352. ACM, 2014.

M.G. Crandall, L.C. Evans, and R.F. Gariepy. Optimal lipschitz extensions and the inﬁnity laplacian. Calculus of Vari-
ations and Partial Differential Equations, 13(2):123–139, 2001. ISSN 0944-2669. doi: 10.1007/s005260000065.
URL http://dx.doi.org/10.1007/s005260000065.

Samuel I. Daitch and Daniel A. Spielman. Faster approximate lossy generalized ﬂow via interior point algo-
rithms.
In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing, STOC ’08, pages
451–460, New York, NY, USA, 2008. ACM. ISBN 978-1-60558-047-0. doi: 10.1145/1374376.1374441. URL
http://doi.acm.org/10.1145/1374376.1374441.

Alan Egger and Robert Huotari. Rate of convergence of the discrete polya algorithm. Journal of Approximation
ISSN 0021-9045. doi: http://dx.doi.org/10.1016/0021-9045(90)90070-7. URL

Theory, 60(1):24 – 30, 1990.
http://www.sciencedirect.com/science/article/pii/0021904590900707.

Charles Fefferman. Whitney’s extension problems and interpolation of data.

(N.S.), 46(2):207–220, 2009a.
http://dx.doi.org/10.1090/S0273-0979-08-01240-8.

ISSN 0273-0979.

doi:

10.1090/S0273-0979-08-01240-8.

Bull. Amer. Math. Soc.
URL

Charles Fefferman. Fitting a [image] -smooth function to data, iii. Annals of Mathematics, 170(1):pp. 427–441, 2009b.

ISSN 0003486X. URL http://www.jstor.org/stable/40345469.

Charles Fefferman and Bo’az Klartag. Fitting a cm -smooth function to data i. Annals of Mathematics, 169(1):pp.

315–346, 2009. ISSN 0003486X. URL http://www.jstor.org/stable/40345445.

D. R. Fulkerson. Note on dilworths decomposition theorem for partially ordered sets. Proc. Amer. Math. Soc, 1956.

P.W. Gaffney and M.J.D. Powell. Optimal interpolation. In Numerical Analysis, volume 506 of Lecture Notes in Math-
ematics, pages 90–99. Springer Berlin Heidelberg, 1976. ISBN 978-3-540-07610-0. doi: 10.1007/BFb0080117.
URL http://dx.doi.org/10.1007/BFb0080117.

L.-A. Gottlieb, A. Kontorovich, and R. Krauthgamer. Efﬁcient classiﬁcation for metric data. CoRR, abs/1306.2547,

2013. URL http://arxiv.org/abs/1306.2547.

L.-A. Gottlieb, A. Kontorovich, and R. Krauthgamer. Efﬁcient regression in metric spaces via approximate lipschitz
extension. In Similarity-Based Pattern Recognition, volume 7953 of Lecture Notes in Computer Science, pages
43–58. Springer Berlin Heidelberg, 2013b. ISBN 978-3-642-39139-2. doi: 10.1007/978-3-642-39140-8 3. URL
http://dx.doi.org/10.1007/978-3-642-39140-8_3.

Robert Jensen. Uniqueness of lipschitz extensions: Minimizing the sup norm of the gradient. Archive for Ra-
doi: 10.1007/BF00386368. URL

ISSN 0003-9527.

tional Mechanics and Analysis, 123(1):51–74, 1993.
http://dx.doi.org/10.1007/BF00386368.

M. Kirszbraun. ber die zusammenziehende und lipschitzsche transformationen. Fundamenta Mathematicae, 22(1):

77–108, 1934. URL http://eudml.org/doc/212681.

13

Andrew J. Lazarus, Daniel E. Loeb,

James G. Propp, Walter R. Stromquist,

Combinatorial games under

man.
229 – 264,
http://www.sciencedirect.com/science/article/pii/S0899825698906765.

http://dx.doi.org/10.1006/game.1998.0676.

and Economic Behavior,

ISSN 0899-8256.

auction play.

Games

1999.

doi:

and Daniel H. Ull-
27(2):
URL

Yunqian Ma and Yun Fu. Manifold Learning Theory and Applications. CRC Press, Inc., Boca Raton, FL, USA, 1st

edition, 2011. ISBN 1439871094, 9781439871096.

E. J. McShane. Extension of range of functions. Bull. Amer. Math. Soc., 40(12):837–842, 12 1934. URL

http://projecteuclid.org/euclid.bams/1183497871.

C.A. Micchelli, T.J. Rivlin,

and S. Winograd.

merische Mathematik, 26(2):191–200, 1976.
http://dx.doi.org/10.1007/BF01395972.

The optimal
ISSN 0029-599X.

recovery of
doi:

smooth functions.
10.1007/BF01395972.

Nu-
URL

V. A. Milman.

Absolutely minimal extensions of

functions on metric spaces.

1999.

URL

http://iopscience.iop.org/1064-5616/190/6/A05/pdf/MSB_190_6_A05.pdf.

Boaz Nadler, Nathan Srebro, and Xueyuan Zhou. Statistical analysis of semi-supervised learning: The limit of inﬁnite
unlabelled data. 2009. URL http://ttic.uchicago.edu/˜nati/Publications/NSZnips09.pdf.

A. Naor and S. Shefﬁeld. Absolutely minimal Lipschitz extension of tree-valued mappings. CoRR, abs/1005.2535,

May 2010. URL http://arxiv.org/abs/1005.2535.

A. M. Oberman. Finite difference methods for the Inﬁnity Laplace and p-Laplace equations. CoRR, abs/1107.5278,

July 2011. URL http://arxiv.org/abs/1107.5278.

Yuval Peres, Oded Schramm, Scott Shefﬁeld, and DavidB. Wilson.

Tug-of-war and the inﬁnity lapla-
In Selected Works of Oded Schramm, Selected Works in Probability and Statistics, pages 595–
doi: 10.1007/978-1-4419-9675-6 18. URL

cian.
638. Springer New York, 2011.
http://dx.doi.org/10.1007/978-1-4419-9675-6_18.

ISBN 978-1-4419-9674-9.

S. Shefﬁeld and C. K. Smart. Vector-valued optimal Lipschitz extensions. CoRR, abs/1006.1741, June 2010. URL

http://arxiv.org/abs/1006.1741.

Ali Kemal Sinop and Leo Grady. A seeded image segmentation framework unifying graph cuts and random walker
which yields a new algorithm. In Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on,
pages 1–8. IEEE, 2007.

Vijay V. Vazirani. Approximation Algorithms. Springer-Verlag New York, Inc., New York, NY, USA, 2001. ISBN

3-540-65367-8.

Ulrike von Luxburg and Olivier Bousquet. Distance-based classiﬁcation with lipschitz functions.

In Learn-
ing Theory and Kernel Machines, volume 2777 of Lecture Notes in Computer Science, pages 314–328.
doi: 10.1007/978-3-540-45167-9 24. URL
Springer Berlin Heidelberg, 2003.
http://dx.doi.org/10.1007/978-3-540-45167-9_24.

ISBN 978-3-540-40720-1.

Hassler Whitney.

Analytic extensions of differentiable functions deﬁned in closed sets.

tions of
http://www.jstor.org/stable/1989708.

the American Mathematical Society, 36(1):pp. 63–89, 1934.

ISSN 00029947.

Transac-
URL

Dengyong Zhou, Christopher J. C. Burges, and Tao Tao. Transductive link spam detection.

In Proceedings
of the 3rd International Workshop on Adversarial Information Retrieval on the Web, AIRWeb ’07, pages 21–
ISBN 978-1-59593-732-2. doi: 10.1145/1244408.1244413. URL
28, New York, NY, USA, 2007. ACM.
http://doi.acm.org/10.1145/1244408.1244413.

Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic

functions. In IN ICML, pages 912–919, 2003.

14

A Basic Properties of Lex-Minimizers

A.1 Meta Algorithm

Algorithm 1: Algorithm META-LEX: Given a well-posed instance (G, v0), outputs lexG[v0].
for i = 1, 2, . . . :

1. if T (vi−1) = VG, then return vi−1.
2. E′ = EG \ (T (vi−1) × T (vi−1)), G′ := (VG, E′).
3. Let P ⋆
4. vi ← ﬁx[vi−1, P ⋆
i ].

i be a steepest ﬁxable path in (G′, vi−1). Let α⋆

i ← ∇P ⋆(vi−1).

In this subsection, we prove the results that appeared in section 2. We start with a simple observation.

Proposition A.1 Given a well-posed instance (G, v0) such that T (v0) 6= V, let P be a steepest ﬁxable path in (G, v0).
Then, ﬁx[v0, P ] extends v0, and (G, ﬁx[v0, P ]) is also a well-posed instance.

The properties we prove below do not depend on the choice of the steepest ﬁxable path.

Proposition A.2 For any well-posed instance (G, v0), with |VG| = n, META-LEX(G, v0) terminates in at most n
iterations, and outputs a complete voltage assignment v that extends v0.

Proof of Proposition A.2: By Proposition A.1, at any iteration i, vi−1 extends v0 and (G′, vi−1) is a well-posed
instance. META-LEX only outputs vi−1 iff T (vi−1) = V, which means vi−1 is a complete voltage assignment. For
any vi−1 that is not complete, for any x ∈ V \T (vi−1), we must have a free terminal path in (G′, vi−1) that contains x.
i exists in (G′, vi−1). Since P ⋆
Hence, a steepest ﬁxable path P ⋆
i ] ﬁxes the voltage
i
✷
for at least one non-terminal. Thus, META-LEX(G, v0) must complete in at most n iterations.

is a free terminal path, ﬁx[vi−1, P ⋆

For the following lemmas, consider a run of META-LEX with well-posed instance (G, v0) as input. Let vout be the
complete voltage assignment output by META-LEX. Let Ei be the set of edges E′ and Gi be the graph G′ constructed
in iteration i of META-LEX.

Lemma A.3 For every edge e ∈ Ei−1 \ Ei, we have

grad[vout](e)

≤ α⋆

i . Moreover, α⋆

i is non-increasing with i.

Proof of Lemma A.3: Let P ⋆
i = (x0, . . . , xr) be a steepest ﬁxable path in iteration i (when we deal with instance
(Gi−1, vi−1)). Consider a terminal path Pi+1 in (Gi, vi) such that {∂0Pi+1, ∂1Pi+1} ∩ (T (vi) \ T (vi−1)) 6= ∅. We
i . On the contrary, assume that ∇Pi+1(vi) > α⋆
claim that ∇Pi+1(vi) ≤ α⋆
i . Consider the case ∂0Pi+1 ∈ T (vi) \
T (vi−1), ∂1P1 ∈ T (vi−1). By the deﬁnition of vi, we must have ∂0Pi+1 = xj for some j ∈ [r − 1]. Let P ′
i+1 be the
path formed by joining paths P ⋆

i+1 is a free terminal path in (Gi−1, vi−1). We have,

i [x0 : xj] and Pi+1. P ′

(cid:12)
(cid:12)

(cid:12)
(cid:12)

vi−1(x0) − vi−1(∂1Pi+1) = (vi(x0) − vi(xj )) + (vi(∂0Pi+1) − vi(∂1Pi+1))
i · ℓ(P ′

i · ℓ(Pi+1) = α⋆

i [x0 : xj]) + α⋆

i · ℓ(P ⋆

> α⋆

i+1),

giving ∇P ′
The other cases can be handled similarly.

i+1(vi) > α⋆

i , which is a contradiction since the steepest ﬁxable path P ⋆
i

in (Gi−1, vi−1) has gradient α⋆
i .

Applying the above claim to an edge e ∈ Ei−1 \ Ei, whose gradient is ﬁxed for the ﬁrst time in iteration i, we
i . If v is the complete voltage assignment output by META-LEX, since v extends vi+1,
i , implying

i . Applying the claim to the symmetric edge, we obtain −grad[vout](e) ≤ α⋆

obtain that grad[vi+1](e) ≤ α⋆
we get grad[vout](e) ≤ α⋆
|grad[vout](e)| ≤ α⋆
i .

Consider any free terminal path Pi+1 in (Gi, vi). If Pi+1 is also a terminal path in (Gi−1, vi−1), it is a free
terminal path in (Gi−1, vi−1). In addition, since a steepest ﬁxable path P ⋆
i , we get
i
∇Pi+1(vi) = ∇Pi+1(vi−1) ≤ α⋆
i . Otherwise, we must have {∂0Pi+1, ∂1Pi+1} ∩ (T (vi) \ T (vi−1)) 6= ∅, and we can
deduce ∇Pi+1(vi) ≤ α⋆
i using the above claim. Thus, all free terminal paths Pi+1 in (Gi, vi) satisfy ∇Pi+1(vi) ≤ α⋆
i .
✷
In particular, α⋆

in (Gi−1, vi−1) has ∇P ⋆

i = α⋆

i is non-increasing with i.

i+1(vi) ≤ α⋆

i+1 = ∇P ⋆

i . Thus, α⋆

15

Lemma A.4 For any complete voltage assignment v for G that extends v0, if v 6= vout, we have grad[v] 6(cid:22) grad[vout],
and hence grad[vout] (cid:22) grad[v].

Proof of Lemma A.4: Consider any complete voltage assignment v for G that extends v0, such that v 6= vout. Thus,
there exists a unique i such that v extends vi−1 but does not extend vi. We will argue that grad[v] 6(cid:22) grad[vout], and
hence grad[vout] (cid:22) grad[v]. For every edge e ∈ E \ Ei−1 that has been ﬁxed so far, grad[v](e) = grad[vi−1](e) =
grad[vout](e), and hence we can ignore these edges.

Since v extends vi−1 but not vi, there exists an x ∈ T (vi) \ T (vi−1) such that v(x) 6= vi(x) = vout(x). Assume
i picked

i = (x0, . . . , xr) is the steepest ﬁxable path with gradient α⋆

v(x) < vi(x) (the other case is symmetric). If P ⋆
in iteration i, we must have x = xj for some j ∈ [r − 1]. Thus,

j

j

(v(xk−1) − v(xk)) = v(x0) − v(xj ) > vi(x0) − vi(xj ) = α⋆

i · ℓ(P ⋆

i [x0 : xj ]) = α⋆
i ·

ℓ(xk−1, xk).

Xk=1

Xk=1
Thus, for some k ∈ [j], we must have grad[v](xk−1, xk) > α⋆
is a path in Gi−1, we have {xk−1, xk} 6⊆
T (vi−1). This gives (xk−1, xk) ∈ (Ei−1 \ Ei). But then, from Lemma A.3, it follows that for all e ∈ (Ei−1 \ Ei), we
✷
have |grad[vout](e)| ≤ α⋆

i . Thus, we have grad[v] 6(cid:22) grad[vout].

i . Since P ∗
i

Lemma A.5 Let P = (x0, . . . , xr) be a steepest ﬁxable path such that it does not have any edges in T (v0) × T (v0)
and v1 = ﬁxG[v0, P ]. Then for every i ∈ [r], we have grad[v1](xi−1, xi) = ∇P.

Proof of Lemma A.5: Suppose this is not true and let j ∈ [r] be the minimum number such that grad[v1](xj−1, xj) 6=
∇P. By deﬁnition of v1 we would necessarily have j < r and vj ∈ T (v0). Suppose grad[v1](xj−1, xj ) < ∇P. We
would then have v1(x0) − v1(xj ) < ∇P ∗ ℓ(P [x0 : xj]). Since P does not have any edges in T (v0) × T (v0),
P1 := (xj, ..., xr) would be a free terminal path with ∇P1 > ∇P. This is a contradiction. Other cases can be ruled
out similarly.

✷

Proof of Theorem 3.3: Consider an arbitrary run of META-LEX on (G, v0). Let vout be the complete voltage
assignment output by META-LEX. Proposition A.1 implies that vout extends v0. Lemma A.4 implies that for any
complete voltage assignment v 6= vout that extends v0, we have grad[vout] (cid:22) grad[v]. Thus, vout is a lex-minimizer.
Moreover, the lemma also gives that for any such v, grad[v] 6(cid:22) grad[vout]. and hence vout is a unique lex-minimizer.
Thus, vout is the unique voltage assignment satisfying Def. 2.1, and we denote it as lexG[v0]. Since we started with an
✷
arbitrary run of META-LEX, uniqueness implies that every run of META-LEX on (G, v0) must output lexG[v0].

Proof of Lemma 3.5: Suppose we have a complete voltage assignment v extending v0, such that
For any terminal path P = (x0, . . . , xr), we get,

grad[v]

∞ ≤ α.

∇P (v0) = v0(∂0P ) − v0(∂1P ) = v(∂0P ) − v(∂1P ) =

grad[v](xi−1, xi) ≤ α ·

ℓ(xi−1, xi) = α · ℓ(P ),

(cid:13)
(cid:13)

(cid:13)
(cid:13)

r

i=1
X

giving ∇P (v0) ≤ α.

On the other hand, suppose every terminal path P in (G, v0) satisﬁes ∇P (v0) ≤ α. Consider v = lexG[v0]. We
know that v extends v0. For every edge e ∈ EG ∩ T (v0) × T (v0), e is a (trivial) terminal path in (G, v0), and hence
has satisﬁes grad[v](e) = grad[v0](e) = ∇e(v0) ≤ α. Considering the reverse edge, we also obtain −grad[v](e) ≤ α.
Thus, |grad[v](e)| ≤ α. Moreover, using Lemma A.3, we know that for edge e ∈ EG \ T (v0) × T (v0), |grad[v](e)| ≤
1 = ∇P ⋆
α⋆
1 ≤ α since P1 is a terminal path in (G, v0). Thus, for every e ∈ EG, |grad[v](e)| ≤ α, and hence
✷
grad[v]
∞ ≤ α.
(cid:13)
(cid:13)
(cid:13)
(cid:13)
A.2 Stability

In this subsection, we sketch a proof of the monotonicity of lex-minimizers and show how it implies the stability
property claimed earlier.

For any well-posed (G, v0), there could be several possible executions of META-LEX, each characterized by the

sequence of paths P ⋆

i . We can apply Theorem 3.3 to deduce the following structural result about the lex-minimizer.

r

i=1
X

16

Corollary A.6 For any well-posed instance (G, v0), consider a sequence of paths (P1, . . . , Pr) and voltage assign-
ments (v1, . . . , vr) for some positive integer r such that:

1. P ⋆

i is a steepest ﬁxable path in (Gi−1, vi−1) for i = 1, . . . , r.

2. vi = ﬁx[vi−1, P ⋆

i ] for i = 1, . . . , r.

3. T (vr) = VG.

Then, we have vr = lexG[v0].

We call such a sequence of paths and voltages to be a decomposition of lexG[v0]. Again, note that lexG[v0] can
possibly have multiple decompositions. However, any two such decompositions are consistent in the sense that they
produce the same voltage assignment.

Proof of Corollary 3.7: We ﬁrst deﬁne some operations on partial assignments which simpliﬁes the notation. Let
v0, v1 be any two partial assignments with the same set of terminals T := T (v0) = T (v1) and c, d ∈ R. By cv0 + d
we mean a partial assignment v with T (v) = T satisfying v(t) = cv0(t) + d for all t ∈ T . Also, by v0 + v1 we
mean a partial assignment v with T (v) = T satisfying v(t) = v0(t) + v1(t) for all t ∈ T. Also, we say v1 ≥ v0 if
v1(t) ≥ v0(t) for all t ∈ T .

Now we can show how Corollary 3.7 follows from Theorem 3.6. Let v := v1 − v0, and kvk∞ = ǫ, for some ǫ > 0.
Therefore, v0 + ǫ ≥ v1 ≥ v0 − ǫ. Theorem 3.6 then implies that lexG[v0] + ǫ ≥ lex[v1] ≥ lex[v0] − ǫ, hence proving
✷
the corollary.

Proof sketch of Theorem 3.6:
It is easy to see that the ﬁrst statement holds. For the second statement, we ﬁrst
observe that if there is a sequence of paths P1, ..., Pr that is simultaneously a decomposition of both lex[v0] and
lex[v1], then this is easy to see. If such a path sequence doesn’t exist, then we look at vt := v0 + t(v1 − v0). We
state here without a proof (though the proof is elementary) that we can then split the interval [0, 1] into ﬁnitely many
subintervals [a0, a1], [a1, a2], .., [ak−1, ak], with a0 = 0, ak = 1, such that for any i, there is a path sequence P1, ..., Pr
which is a decomposition of lex[vt] for all t ∈ [ai, ai+1]. We then observe that v0 = va0 ≤ va1 ≤ ...vak = v1. Since
for every ai, ai+1, there is a path sequence which is simultaneously a decomposition of both lex[vai ] and lex[vai+1 ],
we immediately get

lex[v0] = lex[va0 ] ≤ lex[va1] ≤ ... ≤ lex[vak ] = lex[v1].

✷

A.3 Alternate Characterizations

Proof of Theorem 3.10: We know that lexG[v0] extends v0. We ﬁrst prove that v = lexG[v0] satisﬁes the max-min
gradient averaging property. Assume to the contrary. Thus, there exists x ∈ VG \ T (v0) such that

max
y:(x,y)∈EG

grad[v](x, y) 6= − min

grad[v](x, y).

y:(x,y)∈EG

Assume that max(x,y)∈EG grad[v](x, y) ≥ − min(x,y)∈EG grad[v](x, y). Then, consider v′ extending v0 that is iden-
tical to v except for v′(x) = v(x) − ǫ for ǫ > 0. For ǫ small enough, we get that

and

max
y:(x,y)∈EG

grad[v′](x, y) < max

grad[v](x, y)

y:(x,y)∈EG

− min

y:(x,y)∈EG

grad[v′](x, y) < max

grad[v](x, y).

y:(x,y)∈EG

The gradient of edges not incident on the vertex x is left unchanged. This implies that grad[v]

6(cid:22) grad[v′],

contradicting the assumption that v is the lex-minimizer. (The other case is similar).

17

For the other direction. Consider a complete voltage assignment v extending v0 that satisﬁes the max-min gradient

averaging property w.r.t. (G, v0). Let

α = max

grad[v](x, y) ≥ 0

(x,y)∈EG
x∈V \T (v0)

be the maximum edge gradient, and consider any edge (x0, x1) ∈ EG such that grad[v](x1, x0) = α, with x1 ∈
V \ T (v0). If α = 0, grad[v] is identically zero, and is trivially the lex-minimal gradient assignment. Thus, both v and
lexG[v0] are constant on each connected component. Since (G, v0) is well-posed, there is at least one terminal in each
component, and hence v and lexG[v0] must be identical.

Now assume α > 0. By the max-min gradient averaging property, ∃x2 ∈ VG such that (x1, x2) ∈ EG and

grad[v](x1, x2) =

min
y:(x1,y)∈EG

grad[v](x1, y) = − max

grad[v](x1, y)

y:(x1,y)∈EG

≤ −grad[v](x1, x0) = −α.

Thus, grad[v](x2, x1) ≥ α. Since α is the maximum edge gradient, we must have grad[v](x2, x1) = α. More-
over, v(x2) > v(x1) > v(x0), thus x2 6= x0. We can inductively apply this argument at x2 until we hit a ter-
minal. Similarly, if x0 /∈ T (v0) we can extend the path in the other direction. Consequently, we obtain a path
P = (xj , . . . , x2, x1, x0, x−1, . . . , xk) with all vertices as distinct, such that xj , xk ∈ T (v0), and xi ∈ V \ T (v0)
for all i ∈ [j + 1, k − 1]. Moreover, grad[v](xi, xi−1) = α for all j < i ≤ k. Thus, P is a free terminal path with
∇P [v0] = α.

Moreover, since v is a voltage assignment extending v0 with

∞ = α, using Lemma 3.5, we know that
every terminal path P ′ in (G, v0) must satisfy ∇P ′(v0) ≤ α. Thus, P is a steepest ﬁxable path in (G, v0). Thus,
letting v1 = ﬁx[v0, P ], using Corollary 3.4, we obtain that lexG[v1] = lexG[v0]. Moreover, since α = ∇P [v0] =
grad[v](xi, xi−1) for all i ∈ (j, k], we get v1(xi) = v(xi) for all i ∈ (j, k). Thus, v extends v1.

grad[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We can iterate this argument for r iterations until T (vr) = VG, giving v = vr and vr = lexG[vr] = lexG[v0].
(Since we are ﬁxing at least one terminal at each iteration, this procedure terminates). Thus, we get v = lexG[v0]. ✷

B Description of the Algorithms

Algorithm 2: MODDIJKSTRA(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs a complete
voltage assignment v for G, and an array parent : V → V ∪ {null}.

Add x to a ﬁbonacci heap, with key(x) = +∞.
ﬁnished(x) ← false

Decrease key(x) to v0(x).
parent(x) ← null.

1. for x ∈ VG,
2.
3.
4. for x ∈ T (v0)
5.
6.
7. while heap is not empty
8.
9.
10.
11.
12.
13.
14.
15. return (v, parent)

x ← pop element with minimum key from heap
v(x) ← key(x). ﬁnished(x) ← true .
for y : (x, y) ∈ EG

if ﬁnished(y) = false

if key(y) > v(x) + α · ℓ(x, y)

Decrease key(y) to v(x) + α · ℓ(x, y).
parent(y) ← x.

Theorem B.1 For a well-posed instance (G, V0) and a gradient value α ≥ 0, let (v, parent) ← MODDIJKSTRA(G, v0, α).
Then, v is a complete voltage assignment such that, ∀x ∈ VG, v(x) = mint∈T (v0){v0(t) + αdist(x, t)}. Moreover, the
pointer array parent satisﬁes ∀x /∈ T (v0), parent(x) 6= null and v(x) = v(parent(x)) + α · ℓ(x, parent(x)).

18

Algorithm 3: Algorithm COMPVLOW(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs
vLow, a complete voltage assignment for G, and an array LParent : V → V ∪ {null}.

1. (vLow, LParent) ← MODDIJKSTRA(G, v0, α)
2. return (vLow, LParent)

Algorithm 4: Algorithm COMPVHIGH(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs
vHigh, a complete voltage assignment for G, and an array HParent : V → V ∪ {null}.

if x ∈ T (v0) then v1(x) ← −v0(x) else v1(x) ← v1(x).

1. for x ∈ VG
2.
3. (temp, HParent) ← MODDIJKSTRA(G, v1, α)
4. for x ∈ VG : vHigh(x) ← −temp(x)
5. return (vHigh, HParent)

Corollary B.2 For a well-posed instance (G, V0) and a gradient value α ≥ 0, let (vLow[α], LParent) ← COMPVLOW(G, v0, α)
and (vHigh[α], HParent) ← COMPVHIGH(G, v0, α). Then, vLow[α], vHigh[α] are complete voltage assignments for
G such that, ∀x ∈ VG,

vLow[α](x) = min

{v0(t) + α · dist(x, t)}

t∈T (v0)

vHigh[α](x) = max
t∈T (v0)

{v0(t) − α · dist(t, x)}.

Moreover, the pointer arrays LParent, HParent satisfy ∀x /∈ T (v0), LParent(x), HParent(x) 6= null and

vLow[α](x) = vLow[α](LParent(x)) + α · ℓ(x, LParent(x)),
vHigh[α](x) = vHigh[α](HParent(x)) − α · ℓ(x, HParent(x)).

Algorithm 5: Algorithm COMPINFMIN(G, v0): Given a well-posed instance (G, v0), outputs a complete voltage assignment
v for G, extending v0 that minimizes (cid:13)

(cid:13)grad[v](cid:13)

(cid:13)∞.

1. α ← max{|grad[v0](e)| | e ∈ EG ∩ (T (v0) × T (v0))}.
2. EG ← EG \ (T (v0) × T (v0))
3. P ←STEEPESTPATH(G, v0).
4. α ← max{α, ∇P (v0)}
5. (vLow, LParent) ← COMPVLOW(G, v0, α)
6. (vHigh, HParent) ← COMPVHIGH(G, v0, α)
7. for x ∈ VG
8.
9.
10.
11. return v

then v(x) ← v0(x)
else v(x) ← 1

2 · (vLow(x) + vHigh(x)).

if x ∈ T (v0)

1. (vLow, LParent) ← COMPVLOW(G, v0, α)
2. (vHigh, HParent) ← COMPVHIGH(G, v0, α)
3. VG′ ← {x ∈ VG | vHigh(x) > vLow(x) }
4. EG′ ← {(x, y) ∈ EG | x, y ∈ VG′ }.

19

Algorithm 6: Algorithm COMPHIGHPRESSGRAPH(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0,
outputs a minimal induced subgraph G′ of G where every vertex has pressure[v0](·) > α.

5. G′ ← (V ′, E′, ℓ)
6. return G′

Proof of Lemma 4.3:

is equivalent to

vHigh[α](x) > vLow[α](x)

max
t∈T (v0)

{v0(t) − α · dist(t, x)} > min

{v0(t) + α · dist(x, t)},

t∈T (v0)

which implies that there exists terminals s, t ∈ T (v0) such that

thus,

Hence,

v0(t) − α · dist(t, x) > v0(s) + α · dist(x, s)

pressure[v0](x) ≥

v0(t) − v0(s)
dist(t, x) + dist(x, s)

> α.

v0(t) − v0(s)
dist(t, x) + dist(x, s)

= pressure[v0](x) > α.

v0(t) − α · dist(t, x) > v0(s) + α · dist(x, s)

So the inequality on vHigh and vLow implies that pressure is strictly greater than α. On the other hand, if pressure[v0](x) >
α, there exists terminals s, t ∈ T (v0) such that

which implies vHigh[α](x) > vLow[α](x).

✷

Algorithm 7: Algorithm STEEPESTPATH(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs a steepest
free terminal path P in (G, v0).

P ← VERTEXSTEEPESTPATH(G, v0, xi)

1. Sample uniformly random e ∈ EG. Let e = (x1, x2).
2. Sample uniformly random x3 ∈ VG.
3. for i = 1 to 3
4.
5. Let j ∈ arg maxj∈{1,2,3} ∇Pj (v0)
6. G′ ← COMPHIGHPRESSGRAPH(G, v0, ∇Pj (v0))
7. if EG′ = ∅,
8.
9.

then return Pj
else return STEEPESTPATH(G′, v0|VG′ )

1. while T (v0) 6= VG
2.
3.
4.
5. return v0

EG ← EG \ (T (v0) × T (v0))
P ← STEEPESTPATH(G, v0)
v0 ← ﬁx[v0, P ]

Algorithm 8: Algorithm COMPLEXMIN(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs lexG[v0].

Algorithm 9: Algorithm VERTEXSTEEPESTPATH(G,v0, x): Given a well-posed instance (G, v0), and a vertex x ∈ VG,
outputs a steepest terminal path in (G, v0) through x.

1. Using Dijkstra’s algorithm, compute dist(x, t) for all t ∈ T (v0)

20

y ← arg maxy∈T (v0)
if v0(x) ≥ v0(y)

|v0(x)−v0(y)|
dist(x,y)

then return a shortest path from x to y
else return a shortest path from y to x

2. if x ∈ T (v0)
3.
4.
5.
6.
7. else
8.
9.
10.
11.

for t /∈ T (v0), d(t) ← dist(x, t)
(t1, t2) ← STARSTEEPESTPATH(T (v0), v0|T (v0), d)
Let P1 be a shortest path from t1 to x. Let P2 be a shortest path from x to t2.
P ← (P1, P2). return P.

Algorithm 10: STARSTEEPESTPATH(T, v, d): Returns the steepest path in a star graph, with a single non-terminal connected
to terminals in T, with lengths given by d, and voltages given by v.

|v(t1)−v(t)|
d(t1)+d(t)

1. Sample t1 uniformly and randomly from T
2. Compute t2 ∈ arg maxt∈T
3. α ← |v(t2)−v(t1)|
d(t1)+d(t2)
4. Compute vlow ← mint∈T (v(t) + α · d(t))
5. Tlow ← {t ∈ T | v(t) > vlow + α · d(t)}
6. Compute vhigh ← maxt∈T (v(t) − α · d(t))
7. Thigh ← {t ∈ T | v(t) < vhigh − α · d(t)}
8. T ′ ← Tlow ∪ Thigh.
9. if T ′ = ∅
10.
11.

then if v(t1) ≥ v(t2) then return (t1, t2) else return (t2, t1)
else return STARSTEEPESTPATH(T ′, v|T ′, dT ′ )

B.1 Faster Lex-minimization

Algorithm 11: Algorithm COMPFASTLEXMIN(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs
lexG[v0].

1. while T (v0) 6= VG
2.
3. return v0

v0 ← FIXPATHSABOVEPRESS(G, v0, 0)

Algorithm 12: Algorithm FIXPATHSABOVEPRESS(G, v0, α): Given a well-posed instance (G, v0), with T (v0) 6= VG, and
a gradient value α, iteratively ﬁxes all paths with gradient > α.

EG ← EG \ (T (v0) × T (v0))
Sample uniformly random e ∈ EG. Let e = (x1, x2).
Sample uniformly random x3 ∈ VG.
for i = 1 to 3

Pi ← VERTEXSTEEPESTPATH(G, v0, xi)

Let j ∈ arg maxj∈{1,2,3} ∇Pj(v0)
G′ ← COMPHIGHPRESSGRAPH(G, v0, ∇Pj (v0))
if EG′ = ∅,

1. while T (v0) 6= VG
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.

then v0 ← ﬁx[v0, P ]
else Let G′

for i = 1, . . . , r

i, i = 1, . . . , r be the connected components of G′.

21

vi ← FIXPATHSABOVEPRESS(G′
for x ∈ VG′

i, set v0(x) ← vi(x)

i, v0|VG′

i

, ∇Pj (v0))

if α > 0 then G ←COMPHIGHPRESSGRAPH(G, v0, α)

13.
14.
15.
16. return v0

C Experiments on WebSpam: Testing More Algorithms

For completeness, in this appendix we show how a number of algorithms perform on the web spam experiment of
Section 6. We consider the following algorithms:

• RANDWALK along in-links. For a detailed description see Zhou et al. (2007). This algorithm essentially per-
forms a Personalized PageRank random walk from each vertex x and computes a spam-value for the vertex x by
taking a weighted average of the labels of the vertices where the random walk from x terminates. Also shown in
Section 6.

• DIRECTEDLEX, with edges in the opposite directions of links. This has the effect that a link to a spam host is

evidence of spam, and a link from a normal host is evidence of normality. Also shown in Section 6.

• RANDWALK along out-links.

• DIRECTEDLEX, with edges in the directions of links. This has the effect that a link from to a spam host is

evidence of spam, and a link to a normal host is evidence of normality.

• UNDIRECTEDLEX: Lex-minimization with links treated as undirected edges.

• LAPLACIAN: l2-regression with links treated as undirected edges.

• DIRECTED 1-NEAREST NEIGHBOR: Uses shortest distance along paths following out-links. Spam-ratio is
deﬁned distance from normal hosts, divided by distance to spam hosts. Sites are ﬂagged as spam when spam-
ratio exceeds some threshold. We also tried following paths along in-links instead, but that gave much worse
results.

We use the experimental setup described in Section 6. Results are shown in Figure 4. The alternative convention
for DIRECTEDLEX orients edges in the directions of links. This takes a link from a spam host to be evidence of
spam, and a link to a normal host to be evidence of normality. This approach performs signiﬁcantly worse than our
preferred convention, as one would intuitively expect. UNDIRECTEDLEX and LAPLACIAN approaches also perform
signiﬁcantly worse. DIRECTED 1-NEAREST NEIGHBOR performs poorly, demonstrating that DIRECTEDLEX is very
different from that approach. As observed by Zhou et al. (2007), sampling based on a random walk following out-links
performs worse than following in-links. Up to 60 % recall, DIRECTEDLEX performs best, both in the regime of 5 %
labels for training and in the regime of 20 % labels for training.

22

5 % labels for training

RandWalk along in-links
DirectedLex
RandWalk along out-links
DirectedLex, opposite link direction
UndirectedLex
Laplacian
Directed 1NN

0
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Recall

20 % labels for training

RandWalk along in-links
DirectedLex
RandWalk along out-links
DirectedLex, opposite link direction
UndirectedLex
Laplacian
Directed 1NN

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

i

i

n
o
s
c
e
r
P

i

i

n
o
s
c
e
r
P

0
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Recall

Figure 4: Recall and precision in the WebSpam classiﬁcation experiment. Each data point shown was computed as an average
over 100 runs. The largest standard deviation of the mean precision across the plotted recall values was less than 1.5 %. The
algorithm of Zhou et al. (2007) appears as RANDWALK (along in-links). We also show RANDWALK along out-links. Our directed
lex-minimization algorithm appears as DIRECTEDLEX. We also show DIRECTEDLEX with link directions reversed, along with
UNDIRECTEDLEX and LAPLACIAN.

D l0-Vertex Regularization Proofs

In this appendix, we prove Theorem 7.1 and Theorem 7.2. For the purposes of proving the second theorem, we intro-
duce an alternative version of problem (3). The optimization problem here requires us to minimize l0-regularization

23

budget required to obtain an inf-minimizer with gradient below a given threshold:

min
v∈IRn
subject to

(cid:13)
(cid:13)

v(T ) − v0(T )

0

gradG[v]

(cid:13)
∞ ≤ α.
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We will also need the following graph construction.

Deﬁnition D.1 The α-pressure terminal graph of a partially-labeled graph (G, v0) is a directed unweighted graph
Gα = (T (v0),

E if and only if there is a terminal path P from s to t in G with

E) such that (s, t) ∈

b

b

∇P (v0) > α.

Note that the α-pressure terminal graph has O(n) vertices but may be dense, even when G is not.

Algorithm 13: Algorithm TERM-PRESSURE: Given a well-posed instance (G, v0) and α ≥ 0, outputs α pressure terminal
graph Gα.
Initialize Gα with vertex set Vα = T (v0) and edge set
for each terminal s ∈ T (v0)

E = ∅.

1. Compute the distances to every other terminal t by running Dijktra’s algorithm, allowing shortest paths

b

2. Use the resulting distances to check for every other terminal t if there is a terminal path P from s to t with

that run through other terminals.

∇P (v0) > α. If there is, add edge (s, t) to

E.

Lemma D.2 The α-pressure terminal graph of a voltage problem (G, v0) can be computed in O((m + n log n)n) time
using algorithm TERM-PRESSURE (Algorithm 13).

b

Proof: The correctness of the algorithm follows from the fact that Dijkstra’s algorithm will identify all shortest
distances between the terminals, and the pressure check will ensure that terminal pairs (s, t) are added to
E if and
only if they are the endpoints of a terminal path P with ∇P (v0) > α. The running time is dominated by performing
Dijkstra’s algorithm once for each terminal. A single run of Dijkstra’s algorithm takes O(m + n log n) time, and this
✷
is performed at most n times, for a total running time of O((m + n log n)n).

b

We make three observations that will turn out to be crucial for proving Theorems 7.1 and 7.2.

Observation D.3 Gα is a subgraph of Gβ for α ≥ β.

Proof: Suppose edge (s, t) appears in Gα, then for some path P

∇P (v0) > α ≥ β,

so the edge also appears in Gβ.

Observation D.4 Gα is transitively closed.

Proof: Suppose edges (s, t) and (t, r) appear in Gα. Let P(s,t), P(t,r), P(s,r) be the respective shortest paths in G
between these terminal pairs. Then

∇P(s,r)(v0) =

v0(s) − v0(r)
ℓ(P(s,r))

≥

v0(s) − v0(r)
ℓ(P(s,t)) + ℓ(P(t,r))

=

v0(s) − v0(t) + v0(t) − v0(r)
ℓ(P(s,t)) + ℓ(P(t,r))

≥ min

v0(s) − v0(t)
ℓ(P(s,t))

,

 

v0(t) − v0(r)

ℓ(P(t,r)) !

> α.

So edge (s, r) also appears in Gα. This is sufﬁcient for Gα to be transitively closed.

24

(6)

✷

(7)

✷

Observation D.5 Gα is a directed acyclic graph.

Proof: Suppose for a contradiction that a directed cycle appears in Gα. Let s and t be two vertices in this cycle. Let
P(s,t) and P(t,s) be the respective shortest paths in G between these terminal pairs. Because Gα is transitively closed,
both edges (s, t) and (t, s) must appear in Gα. But (s, t) ∈

E implies

and similarly (t, s) ∈

E implies

b
This is a contradiction.

v0(s) − v0(t) > αℓ(P(s,t)) > 0,

b

v0(t) − v0(s) > αℓ(P(t,s)) > 0.

✷

The usefulness of the α-pressure terminal graph is captured in the following lemma. We deﬁne a vertex cover of a
directed graph to be a vertex set that constitutes a vertex cover in the same graph with all edges taken to be undirected.

Lemma D.6 Given a partially-labeled graph (G, v0) and a set U ⊆ V , there exists a voltage assignment v ∈ IRn that
satisﬁes

if and only if U is a vertex cover in the α-pressure terminal graph Gα of (G, v0).
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:8)

(cid:9)

t ∈ T (v0) : v(t) 6= v0(t)

⊆ U and

gradG[v]

∞ ≤ α,

Proof: We ﬁrst show the “only if” direction. Suppose for a contradiction that there exists a voltage assignment v for
which
∞ ≤ α, but U is not a vertex cover in Gα. Let (s, t) be an edge Gα which is not covered by U . The
presence of this edge in Gα implies that there exists a terminal path P from s to t in G for which

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∇P (v0) > α.

But, by Lemma 3.5 this means there is no assignment v for G which agrees with v0 on s and t and has
α. This contradicts our assumption.

∞ ≤
(cid:13)
Now we show the “if” direction. Consider an arbitrary vertex cover U of Gα. Suppose for a contradiction that
(cid:13)
⊆ U .

t ∈ T (v0) : v(t) 6= v0(t)

gradG[v]

(cid:13)
(cid:13)

gradG[v]

there does not exist a voltage assignment v for G with
Deﬁne a partial voltage assignment vU given by

∞ ≤ α and

(cid:8)

(cid:9)

vU (t) =

v0(t)
∗

(

(cid:13)
(cid:13)

(cid:13)
(cid:13)
if t ∈ T (v0) \ U
o.w.

∞ ≤ α. By
The preceding statement is equivalent to saying that there is no v that extends vU and has
Lemma 3.5, this means there is terminal path between s, t ∈ T (vU ) with gradient strictly larger than α. But this
means an edge (s, t) is present in Gα and is not covered. This contradicts our assumption that U is a vertex cover. ✷

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We are now ready to prove Theorem 7.2.

∞

(cid:13)
(cid:13)

Proof of Theorem 7.2: We describe and prove the algorithm OUTLIER. The algorithm will reduce problem (3)
to problem (6): Suppose v∗ is an optimal assignment for problem (3).
It achieves a maximum gradient α∗ =
gradG[v∗]
. Using Dijkstra’s algorithm we compute the pairwise shortest distances between all terminals in G.
From these distances and the terminal voltages, we compute the gradient on the shortest path between each terminal
(cid:13)
pair. By Lemma 3.5, α∗ must equal one of these gradients. So we can solve problem (3) by iterating over the set of
(cid:13)
gradients between terminals and solving problem (6) for each of these O(n2) gradients. Among the assignments with
v(T ) − v0(T )

0 ≤ k, we then pick the solution that minimizes
(cid:13)
(cid:13)

In fact, we can do better. By Observation D.3, Gα is a subgraph of Gβ for α ≥ β. This means a vertex cover
(cid:13)
of Gα is also a vertex cover of Gβ, and hence the minimum vertex cover for Gβ is at least as large as the minimum
(cid:13)
vertex cover for Gα. This means we can do a binary search on the set of O(n2) terminal gradients to ﬁnd the minimum
gradient for which there exists an assignment with
0 ≤ k. This way, we only make O(log n) calls to
v(T ) − v0(T )
problem (6), in order to solve problem (3).
(cid:13)
(cid:13)

We use the following algorithm to solve problem (6).

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∞

.

25

1. Compute the α-pressure terminal graph Gα of G using the algorithm TERM-PRESSURE.
2. Compute a minimum vertex cover U of Gα using the algorithm KONIG-COVER from Theorem 7.3.
3. Deﬁne a partial voltage assignment vU given by

vU (t) =

v0(t)
∗

(

if t ∈ T (v0) \ U,
otherwise.

4. Using Algorithm 5, compute voltages v that extend vU and output v.

From Lemma D.2, it follows that step 1 computes the α-pressure terminal graph in polynomial time. From The-
orem 7.3 it follows that step 2 computes the a minimum vertex cover of the α-pressure terminal graph in polynomial
time, because our observations D.4 and D.5 establish that the graph is a TC-DAG. From Lemma D.6 and Theorem 4.6,
it follows that the output voltages solve program (6).

✷

To prove Theorem 7.1, we use the standard greedy approximation algorithm for MIN-VC (Vazirani (2001)).

Theorem D.7 2-Approximation Algorithm for Vertex Cover. The following algorithm gives a 2-approximation to
the Minimum Vertex Cover problem on a graph G = (V, E).

0. Initialize U = ∅.
1. Pick an edge (u, v) ∈ E that is not covered by U .
2. Add u and v to the set U .
3. Repeat from step 1 if there are still edges not covered by U .
4. Output U .

We are now in a position to prove Theorem 7.1

Proof of Theorem 7.1: Given an arbitrary k and a partially-labeled graph (G, v0), let α∗ be the optimum value
of program (3). Observe that by Lemma D.6, this implies that Gα∗ has a vertex cover of size k. Given the partial
assignment v0, for every vertex set U , we deﬁne

vU (t) =

v0(t)
∗

(

if t ∈ T (v0) \ U
o.w.

We claim the following algorithm APPROX-OUTLIER outputs a voltage assignment v with

gradG[v]

∞ ≤ α∗

(cid:13)
(cid:13)

(cid:13)
(cid:13)

and

v(T ) − v0(T )

(cid:13)
(cid:13)

Algorithm APPROX-OUTLIER:

0 ≤ 2k.
(cid:13)
(cid:13)

0. Initialize U = ∅.
1. Using the algorithm STEEPESTPATH (Algorithm 7), ﬁnd a steepest terminal path in G w.r.t. vU . Denote
this path P and let s and t be its terminal endpoints. If there is no terminal path with positive gradient, skip
to step 4.

2. Add s and t to the set U .
3. If |U | ≤ 2k − 2 then repeat from step 1.
4. Using the algorithm COMPINFMIN (Algorithm 5), compute voltages v that extend vU and output v.

From the stopping conditions, it is clear that |U | ≤ 2k. If in step 1 we ever ﬁnd that no terminal paths have positive
∞ = 0 ≤ α∗, by Lemma 3.5. Similarly if we ﬁnd a steepest
gradient then our v that extends vU will have
(cid:13)
(cid:13)

gradG[v]

(cid:13)
(cid:13)

26

gradG[v]

∞ ≤ α∗.

∞ ≤ α∗.
path with gradient less than α∗ w.r.t. vU , then for this U there exists v that extends vU and has
This will continue to hold when if we add vertices to U . Therefore, for the ﬁnal U , there will exist an v that extends
vU and has

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

If we never ﬁnd a steepest terminal path P with ∇P (v0) ≤ α∗, then each steepest path we ﬁnd corresponds to an
edge in Gα∗ that is not yet covered by U and our algorithm in fact implements the greedy approximation algorithm
for vertex cover described in Theorem D.7. This implies that the ﬁnal U is a vertex cover of Gα∗ of size at most 2k.
∞ ≤ α∗. This
By Lemma D.6, this implies that there exists a voltage assignment u extending vU that has
implies by Theorem 4.6 that the v we output has
(cid:13)
(cid:13)
In all cases, the v we output extends vU , so

∞ ≤ α∗.

gradG[u]

(cid:13)
(cid:13)

✷

gradG[v]
v(T ) − v0(T )
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

0 ≤ |U | ≤ 2k.
(cid:13)
(cid:13)

E Proof of Hardness of l0 regularization for l2

We will prove Theorem 7.4, by a reduction from minimum bisection. To this end, let G = (V, E) be any graph. We
will reduce the minimum bisection problem on G to our regularization problem. Let n = |V |. The graph on which we
will perform regularization will have vertex set

V ∪

V ,

V is a set of n vertices that are in 1-to-1 correspondence with V . We assume that every edge in G has weight 1.
V to the corresponding vertex in V by an edge of weight B, for some large B to be
V to each other by edges of weight B3. So, we have a complete
V to V , and the original graph G on V .

where
We now connect every vertex in
determined later. We also connect all of the vertices in
graph of weight B3 edges on
b
The input potential function will be

V , a matching of weight B edges connecting

b

b

b

v(a) =

b
0 for a ∈
1 for a ∈ V .
b

(

V , and

b

Now set k = n/2. We claim that we will be able to determine the value of the minimum bisection from the solution
to the regularization problem.

If S is the set of vertices on which v and w differ, then we know that the w is harmonic on S: for every a ∈ S,

w(a) is the weighted average of the values at its neighbors. In the following, we exploit the fact that |S| ≤ n/2.

Claim E.1 For every a ∈ S ∩

V , w(a) ≤ 2/nB2.

Proof: Let a be the vertex in S ∩
w-value equal to 0 by edges of weight B3. On the other hand, a has only one neighbor that is not in
w-value at most 1, and it is connected to that vertex by an edge of weight B. Call that vertex c. We have

V that maximizes w(a). So, a is connected to at least n/2 neighbors in

V with
V , that vertex has

b

b

b

((n − 1)B3 + B)w(a) = Bw(c) +

B3w(b)

b

b
V ,b6=a
Xb∈

= Bw(c) +

B3w(b) +

B3w(b)

b
V ∩S,b6=a
Xb∈

B3w(a)

≤ B +

b
V ∩S,b6=a
Xb∈
≤ B + (n/2 − 1)B3w(a).

b
V −S
Xb∈

Subtracting (n/2 − 1)B3w(a) from both sides gives

((n/2)B3 + B)w(a) ≤ B,

which implies the claim.

Claim E.2 For a ∈ S ∩ V , w(a) ≤ n/B.

27

✷

V . Let’s call that neighbor c. We know that w(c) ≤ 2/B2n. On the
Proof: Vertex a has exactly one neighbor in
other hand, vertex a has fewer than n − 1 neighbors in V , and each of these have w-value at most 1. Let da denote the
degree of a in G. Then,

b

So,

Let

and

bisection.

and at most

(B + da)w(a) ≤ da + B

2
B2n

.

w(a) ≤

da + 2/Bn
da + B
n + (2/Bn)
B + n

≤

≤ n/B.

|S| = k = n/2.

T = S ∩ V,

t = |T | .

(n − t)B − 4/B
b

(n − t)B + tn2/B.

We now estimate the value of the regularized objective function. To this end, we assume that

We will prove that S ⊂ V and so S = T and t = n/2.

Let δ denote the number of edges on the boundary of T in V . Once we know that t = n/2, δ is the size of a

Claim E.3 The contribution of the edges between V and

V to the objective function is at least

Proof: For the lower bound, we just count the edges between vertices in V \ T and
edges, and each of them has weight B. The endpoint in V \ T has w-value 1, and the endpoint in
most 2/nB2. So, the contribution of these edges is at least

V . There are n − t of these
V has w-value at

b

(n − t)B(1 − 2/nB2)2 ≥ (n − t)B(1 − 4/nB2) ≥ (n − t)B − 4/B.

b

For the upper bound, we observe that the difference in w-values across each of these n − t edges is at most 1, so their
total contribution is at most

Since for every vertex a ∈ T , w(a) ≤ n/B, and also every vertex b ∈
edges between T and

V is at most

t(n/B)2B = tn2/B.

b

b

V , w(b) ≤ 2/nB2, the contribution due to

We will see that this is the dominant term in the objective function. The next-most important term comes from the

edges in G.

(n − t)B.

28

✷

✷

Claim E.4 The contribution of the edges in G to the objective function is at least

and at most

δ(1 − 2n/B)

δ + (t2/2)(n/B)2

δ(1 − 2n/B) and δ.

(t2/2)(n/B)2.

Proof: Let (a, b) ∈ E. If neither a nor b is in T , then w(a) = w(b) = 1, and so this edge has no contribution. If
a ∈ T but b 6∈ T , then the difference in w-values on them is between (1 − n/B) and 1. So, the contribution of such
edges to the objective function is between

Finally, if a and b are in T , then the difference in w-values on them is at most n/B, and so the contribution of all such
edges to the objective function is at most

Claim E.5 The edges between pairs of vertices in

V contribute at most 2/B to the objective function.

Proof: As 0 ≤ w(a) ≤ 2/B2n for every a ∈

V , every edge between two vertices in

V can contribute at most

b

As there are fewer than n2/2 such edges, their total contribution to the objective function is at most

B3(2/B2n)2 = 4/Bn2.
b

b

(n2/2)(4/Bn2) = 2/B.

Lemma E.6 If n ≥ 4 and B = 2n3, the value of the objective function is at least

and at most

(n − t)B + δ − 1/2

(n − t)B + δ + 1/3.

Proof: Summing the contributions in the preceding three claims, we see that the value of the objective function is at
least

(n − t)B − 4/B + δ(1 − 2n/B) ≥ (n − t)B + δ − 4/B − 2nδ/B

≥ (n − t)B + δ − n3/B
≥ (n − t)B + δ − 1/2,

as δ ≤ (n/2)2.

Similarly, the objective function is at most

(n − t)B + tn2/B + δ + (t2/2)(n/B)2 + 2/B ≤ (n − t)B + n3/2B + δ + n4/8B2 + 2/B
≤ (n − t)B + n3/2B + δ + 1/32n2 + 1/n3
≤ (n − t)B + δ + 1/3.

Claim E.7 If n ≥ 2 and B = 2n3, then S ⊂ V .

Proof: The objective function is minimized by making t as large as possible, so t = n/2 and S ⊂ V .

29

✷

✷

✷

✷

Theorem E.8 The value of the objective function reveals the value of the minimum bisection in G.

Proof: The value of the objective function will be between

and

(n/2)B + δ − 1/2

(n/2)B + δ + 1/3.

So, the objective function will be smallest when δ is as small as possible.

✷

Theorem E.8 immediately implies Theorem 7.4.

30

5
1
0
2
 
n
u
J
 
0
3
 
 
]

G
L
.
s
c
[
 
 
2
v
0
9
2
0
0
.
5
0
5
1
:
v
i
X
r
a

Algorithms for Lipschitz Learning on Graphs ∗†

Rasmus Kyng
Yale University
rasmus.kyng@yale.edu

Anup Rao
Yale University
anup.rao@yale.edu

Sushant Sachdeva
Yale University
sachdeva@cs.yale.edu

Daniel A. Spielman
Yale University
spielman@cs.yale.edu

July 1, 2015

Abstract

We develop fast algorithms for solving regression problems on graphs where one is given the value of a function
at some vertices, and must ﬁnd its smoothest possible extension to all vertices. The extension we compute is the
absolutely minimal Lipschitz extension, and is the limit for large p of p-Laplacian regularization. We present an
algorithm that computes a minimal Lipschitz extension in expected linear time, and an algorithm that computes
an absolutely minimal Lipschitz extension in expected time eO(mn). The latter algorithm has variants that seem
to run much faster in practice. These extensions are particularly amenable to regularization: we can perform l0-
regularization on the given values in polynomial time and l1-regularization on the initial function values and on graph
edge weights in time eO(m3/2).

Our deﬁnitions and algorithms naturally extend to directed graphs.

1 Introduction

We consider a problem in which we are given a weighted undirected graph G = (V, E, ℓ) and values v0 : T → R
on a subset T of its vertices. We view the weights ℓ as indicating the lengths of edges, with shorter length indicating
greater similarity. Our goal it to assign values to every vertex v ∈ V \T so that the values assigned are as smooth as
possible across edges. A minimal Lipschitz extension of v0 is a vector v that minimizes

max
(x,y)∈E

(ℓ(x, y))−1

v(x) − v(y)

,

(cid:12)
(cid:12)

(cid:12)
(cid:12)

subject to v(x) = v0(x) for all x ∈ T . We call such a vector an inf-minimizer. Inf-minimizers are not unique. So,
among inf-minimizers we seek vectors that minimize the second-largest absolute value of ℓ(x, y)−1
v(x) − v(y)
across edges, and then the third-largest given that, and so on. We call such a vector v a lex-minimizer. It is also known
(cid:12)
as an absolutely minimal Lipschitz extension of v0.
(cid:12)
These are the limit of the solution to p-Laplacian minimization problems for large p, namely the vectors that solve

(cid:12)
(cid:12)

(1)

(2)

min
v∈Rn

v|T =v0|T X(x,y)∈E

(ℓ(x, y))−p|v(x) − v(y)|p.

The use of p = 2 was suggested in the foundational paper of Zhu et al. (2003), and is particularly nice because it can
be obtained by solving a system of linear equations in a symmetric diagonally dominant matrix, which can be done

∗This research was partially supported by AFOSR Award FA9550-12-1-0175, NSF grant CCF-1111257, a Simons Investigator Award to Daniel

Spielman, and a MacArthur Fellowship.

†Code used in this work is available at https://github.com/danspielman/YINSlex

1

very quickly (Cohen et al. (2014)). The use of larger values of p has been discussed by Alamgir and Luxburg (2011),
and by Bridle and Zhu (2013), but it is much more complicated to compute. The fastest algorithms we know for this
problem require convex programming, and then require very high accuracy to obtain the values at most vertices. By
taking the limit as p goes to inﬁnity, we recover the lex-minimizer, which we will show can be computed quickly.

The lex-minimization problem has a remarkable amount of structure. For example, in uniformly weighted graphs
the value of the lex-minimizer at every vertex not in T is equal to the average of the minimum and maximum of the
values at its neighbors. This is analogous to the property of the 2-Laplacian minimizer that the value at every vertex
not in T equals the average of the values at its neighbors.

1.1 Contributions

We ﬁrst present several important structural properties of lex-minimizers in Section 3.2. As we shall point out, some
of these were known from previous work, sometimes in restricted settings. We state them generally and prove them
for completeness. We also prove that the lex-minimizer is as stable as possible under perturbations of v0 (Section 3.1).
The structure of the lex-minimization problem has led us to develop elegant algorithms for its solution. Both the
algorithms and their analyses could be taught to undergraduates. We believe that these algorithms could be used in
place of 2-Laplacian minimization in many applications.

We present algorithms for the following problems. Throughout, m = |E| and n = |V |.

Inf-minimization: An algorithm that runs in expected time O(m + n log n) (Section 4.3).

Lex-minimization: An algorithm that runs in expected time O(n(m + n log n)) (Section 4), along with a variant that

runs quickly in practice (Section 4.4).

l1-regularization of edge lengths for inf-minimization: The problem of minimizing (1) given a limited budget with
O(m3/2)
which one can increase edge lengths is a linear programming problem. We show how to solve it in time
with an interior point method by using fast Laplacian solvers (Section 8). The same algorithm can accommodate
l1-regularization of the values given in v0.

e

l0-regularization of vertex values for inf-minimization: We give a polynomial time algorithm for l0-regularization
of the values at vertices. That is, we minimize (1) given a budget of a number of vertices that can be proclaimed
outliers and removed from T (Section 7.1). We solve this problem by reducing it to the problem of computing
minimum vertex covers on transitively closed directed acyclic graphs, a special case of minimum vertex cover
that can be solved in polynomial time.

After any regularization for inf-minimization, we suggest computing the lex-minimizer. We ﬁnd the result for l0-
regularization of vertex values to be particularly surprising, especially because we prove that the analogous problem
for 2-Laplacian minimization is NP-Hard (Section 7.2).

All of our algorithms extend naturally to directed graphs (Section 5). This is in contrast with the problem of
minimizing 2-Laplacians on directed graphs, which corresponds to computing electrical ﬂows in networks of resistors
and diodes, for which fast algorithms are not presently known.

We present a few experiments on examples demonstrating that the lex-minimizer can overcome known deﬁcien-
cies of the 2-Laplacian minimizer (Section 1.2, Figures 1,2), as well as a demonstration of the performance of the
directed analog of our algorithms on the WebSpam dataset of Castillo et al. (2006) (Section 6). In the WebSpam prob-
lem we use the link structure of a collection of web sites to ﬂag some sites as spam, given a small number of labeled
sites known to be spam or normal.

1.2 Relation to Prior Work

We ﬁrst encountered the idea of using the minimizer of the 2-Laplacian given by (2) for regression and classiﬁca-
tion on graphs in the work of Zhu et al. (2003) and Belkin et al. (2004) on semi-supervised learning. These works
transformed learning problems on sets of vectors into problems on graphs by identifying vectors with vertices and
constructing graphs with edges between nearby vectors. One shortcoming of this approach (see Nadler et al. (2009),

2

e
g
a

t
l

 

o
V
d
e
r
r
e

f

n

I

1

0.8

0.6

0.4

0.2

0

-0.2

-0.4

-0.6

-0.8

-1

-4

50 lex
50 l2
100 lex
100 l2
500 lex
500 l2
1000 lex
1000 l2

0.25

0.2

r
o
r
r
e
 
1
l
 
n
a
e
M

0.15

0.1

0.05

0
5000 

lex
2-Lap
labels

-2

0

2
Vertex position on real line

4

6

8

Figure 1: Lex vs 2-Laplacian on 1D gaussian clus-
ters.

Figure 2: kNN graphs on samples from 4D cube.

10000

20000

40000

80000

Number of Vertices

Alamgir and Luxburg (2011), Bridle and Zhu (2013)) is that if the number of vectors grows while the number of la-
beled vectors remains ﬁxed, then almost all the values of the 2-Laplacian minimizer converge to the mean of the
labels on most natural examples. For example, Nadler et al. (2009) consider sampling points from two Gaussian
distributions centered at 0 and 4 on the real line. They place edges between every pair of points (x, y) with length
exp(|x − y|2 /2σ2) for σ = 0.4, and provide only the labels v0(0) = −1 and v0(4) = 1. Figure 1 shows the values
of the 2-Laplacian minimizer in red, which are all approximately zero. In contrast, the values of the lex-minimizer in
blue, which are smoothly distributed between the labeled points, are shown.

The “manifold hypothesis” (see Chapelle et al. (2010), Ma and Fu (2011)) holds that much natural data lies near a
low-dimensional manifold and that natural functions we would like to learn on this data are smooth functions on the
manifold. Under this assumption, one should expect lex-minimizers to interpolate well. In contrast, the 2-Laplacian
minimizers degrade (dotted lines) if the number of labeled points remains ﬁxed while the total number of points grows.
In Figure 2, we demonstrate this by sampling many points uniformly from the unit cube in 4 dimensions, form their
8-nearest neighbor graph, and consider the problem of regressing the ﬁrst coordinate. We performed 8 experiments,
varying the number of labeled points in {50, 100, 500, 1000}. Each data point is the mean average l1 error over 100
experiments. The plots for root mean squared error are similar. The standard deviation of the estimations of the mean
are within one pixel, and so are not displayed. The performance of the lex-minimizer (solid lines) does not degrade as
the number of unlabeled points grows.

Analogous to our inf-minimizers, minimal Lipschitz extensions of functions in Euclidean space and over more
general metric spaces have been studied extensively in Mathematics (Kirszbraun (1934), McShane (1934), Whitney
(1934)). von Luxburg and Bousquet (2003) employ Lipschitz extensions on metric spaces for classiﬁcation and relate
these to Support Vector Machines. Their work inspired improvements in classiﬁcation and regression in metric spaces
with low doubling dimension (Gottlieb et al. (2013), Gottlieb et al. (2013b)). Theoretically fast, although not actually
practical, algorithms have been given for constructing minimal Lipschitz extensions of functions on low-dimensional
Euclidean spaces (Fefferman (2009a), Fefferman and Klartag (2009), Fefferman (2009b)). Sinop and Grady (2007)
suggest using inf-minimizers for binary classiﬁcation problems on graphs. For this special case, where all of the
given values are either 0 or 1, they present an O(m + n log n) time algorithm for computing an inf-minimizer. The
case of general given values, which we solve in this paper, is much more complicated. To compensate for the non-
uniqueness of inf-minimizers, they suggest choosing the inf-minimizer that minimizes (2) with p = 2. We believe that
the lex-minimizer is a more natural choice.

The analog of our lex-minimizer over continuous spaces is called the absolutely minimal Lipschitz extension
(AMLE). Starting with the work of Aronsson (1967), there have been several characterizations and proofs of the ex-
istence and uniqueness of the AMLE (Jensen (1993), Crandall et al. (2001), Barles and Busca (2001), Aronsson et al.
(2004)). Many of these results were later extended to general metric spaces, including graphs (Milman (1999),
Peres et al. (2011), Naor and Shefﬁeld (2010), Shefﬁeld and Smart (2010)). However, to the best of our knowledge,
fast algorithms for computing lex-minimizers on graphs were not known. For the special case of undirected, un-
weighted graphs, Lazarus et al. (1999) presented both a polynomial-time algorithm and an iterative method. Oberman

3

(2011) suggested computing the AMLE in Euclidean space by ﬁrst discretizing the problem and then solving the cor-
responding graph problem by an iterative method. However, no run-time guarantees were obtained for either iterative
method.

2 Notation and Basic Deﬁnitions

Lexicographic Ordering. Given a vector r ∈ Rm, let πr denote a permutation that sorts r in non-increasing order
by absolute value, i.e., ∀i ∈ [m − 1], |r(πr(i))| ≥ |r(πr(i + 1))|. Given two vectors r, s ∈ Rm, we write r (cid:22) s to
indicate that r is smaller than s in the lexicographic ordering on sorted absolute values, i.e.

∃j ∈ [m],

r(πr(j))

<

s(πs(j))

and ∀i ∈ [j − 1],

r(πr(i))

=

s(πs(i))

or ∀i ∈ [m],

=

r(πr(i))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.
(cid:12)
(cid:12)

s(πs(i))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

Note that it is possible that r (cid:22) s and s (cid:22) r while r 6= s. It is a total relation: for every r and s at least one of r (cid:22) s
or s (cid:22) r is true.

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Graphs and Matrices. We will work with weighted graphs. Unless explicitly stated, we will assume that they are
undirected. For a graph G, we let VG be its set of vertices, EG be its set of edges, and ℓG : EG → R+ be the
assignment of positive lengths to the edges. We let |VG| = n, and |EG| = m. We assume ℓG is symmetric, i.e.,
ℓG(x, y) = ℓG(y, x). When G is clear from the context, we drop the subscript.

A path P in G is an ordered sequence of (not necessarily distinct) vertices P = (x0, x1, . . . , xk), such that
(xi−1, xi) ∈ E for i ∈ [k]. The endpoints of P are denoted by ∂0P = x0, ∂1P = xk. The set of interior vertices
of P is deﬁned to be int(P ) = {xi : 0 < i < k}. For 0 ≤ i < j ≤ k, we use the notation P [xi : xj] to denote the
k
subpath (xi, . . . , xj). The length of P is ℓ(P ) =
i=1 ℓ(xi−1, xi).
A function v0 : V → R ∪ {∗} is called a voltage assignment (to G). A vertex x ∈ V is a terminal with
respect to v0 iff v0(x) 6= ∗. The other vertices, for which v0(x) = ∗, are non-terminals. We let T (v0) denote the
set of terminals with respect to v0. If T (v0) = V, we call v0 a complete voltage assignment (to G). We say that an
assignment v : V → R ∪ {∗} extends v0 if v(x) = v0(x) for all x such that v0(x) 6= ∗.

Given an assignment v0 : V → R ∪ {∗}, and two terminals x, y ∈ T (v0) for which (x, y) ∈ E, we deﬁne the

P

gradient on (x, y) due to v0 to be

gradG[v0](x, y) =

v0(x) − v0(y)
ℓ(x, y)

.

It may be useful to view gradG[v0](x, y) as the current in the edge (x, y) induced by voltages v0. When v0 is a
complete voltage assignment, we interpret gradG[v0] as a vector in Rm, with one entry for each edge. However, for
convenience, we deﬁne gradG[v0](x, y) = −gradG[v0](y, x). When G is clear from the context, we drop the subscript.
A graph G along with a voltage assignment v to G is called a partially-labeled graph, denoted (G, v). We say
that a partially-labeled graph (G, v0) is a well-posed instance if for every maximal connected component H of G, we
have T (v0) ∩ VH 6= ∅.

A path P in a partially-labeled graph (G, v0) is called a terminal path if both endpoints are terminals. We deﬁne

∇P (v0) to be its gradient:

∇P (v0) =

v0(∂0P ) − v0(∂1P )
ℓ(P )

.

If P contains no terminal-terminal edges (and hence, contains at least one non-terminal), it is a free terminal path.

Lex-Minimization. An instance of the LEX-MINIMIZATION problem is described by a partially-labeled graph
(G, v0). The objective is to compute a complete voltage assignment v : VG → R extending v0 that lex-minimizes
grad[v].

Deﬁnition 2.1 (Lex-minimizer) Given a partially-labeled graph (G, v0), we deﬁne lexG[v0] to be a complete voltage
assignment to V that extends v0, and such that for every other complete assignment v′ : VG → R that extends v0, we
have gradG[lexG[v0]] (cid:22) gradG[v′]. That is, lexG[v0] achieves a lexicographically-minimal gradient assignment to the
edges.

We call lexG[v0] the lex-minimizer for (G, v0). Note that if T (v0) = VG, then trivially, lexG[v0] = v0.

4

3 Basic Properties of Lex-Minimizers

Lazarus et al. (1999) established that lex-minimizers in unweighted and undirected graphs exist, are unique, and may
be computed by an elementary meta-algorithm. We state and prove these facts for undirected weighted graphs, and
defer the discussion of the directed case to Section 5. We also state for directed and weighted graphs characterizations
of lex-minimizers that were established by Peres et al. (2011), Naor and Shefﬁeld (2010) and Shefﬁeld and Smart
(2010) for unweighted graphs. These results are essential for the analyses of our algorithms. We defer most proofs to
Appendix A.

Deﬁnition 3.1 A steepest ﬁxable path in an instance (G, v0) is a free terminal path P that has the largest gradient
∇P (v0) amongst such paths.

Observe that a steepest ﬁxable path with ∇P (v0) 6= 0 must be a simple path.
Deﬁnition 3.2 Given a steepest ﬁxable path P in an instance (G, v0), we deﬁne ﬁxG[v0, P ] : VG → R ∪ {∗} to be the
voltage assignment deﬁned as follows

ﬁxG[v0, P ](x) =

v0(∂0P ) − ∇P (v0) · ℓG(P [∂0P : x]) x ∈ int(P ) \ T (v0),
v0(x)

otherwise.

(

We say that the vertices x ∈ int(P ) are ﬁxed by the operation ﬁx[v0, P ]. If we deﬁne v1 = ﬁxG[v0, P ], where
P = (x0, . . . , xr) is the steepest ﬁxable path in (G, v0), then it is easy to argue that for every i ∈ [r], we have
grad[v1](xi−1, xi) = ∇P (see Lemma A.5). The meta-algorithm META-LEX, spelled out as Algorithm 1, entails
repeatedly ﬁxing steepest ﬁxable paths. While it is possible to have multiple steepest ﬁxable paths, the result of ﬁxing
all of them does not depend on the order in which they are ﬁxed.

Theorem 3.3 Given a well-posed instance (G, v0), the meta-algorithm META-LEX, which repeatedly ﬁxes steepest
ﬁxable paths, produces the unique lex-minimizer extending v0.

Corollary 3.4 Given a well-posed instance (G, v0) such that T (v0) 6= VG, let P be a steepest ﬁxable path in (G, v0).
Then, (G, ﬁx[v0, P ]) is also a well-posed instance, and lexG[ﬁx[v0, P ]] = lexG[v0].

Since a lex-minimal element must be an inf-minimizer, we also obtain the following corollary, that can also be

proved using LP duality.

Lemma 3.5 Suppose we have a well-posed instance (G, v0). Then, there exists a complete voltage assignment v
extending v0 such that

grad[v]

∞ ≤ α, iff every terminal path P in (G, v0) satisﬁes ∇P (v0) ≤ α.
(cid:13)
(cid:13)

3.1 Stability

(cid:13)
(cid:13)

The following theorem states that lexG[v0] is monotonic with respect to v0 and it respects scaling and translation of
v0.

Theorem 3.6 Let (G, v0) be a well-posed instance with T := T (v0) as the set of terminals. Then the following
statements hold.

1. For any c, d ∈ R, v1 a partial assignment with terminals T (v1) = T and v1(t) = cv0(t) + d for all t ∈ T .

Then, lexG[v1](i) = c · lexG[v0](i) + d for all i ∈ VG.

2. v1 a partial assignment with terminals T (v1) = T. Suppose further that v1(t) ≥ v0(t) for all t ∈ T. Then,

lexG[v1](i) ≥ lexG[v0](i) for all i ∈ VG.

As a corollary, the above theorem gives a nice stability property that lex-minimal elements satisfy.

Corollary 3.7 Given well-posed instances (G, v0), (G, v1) such that T := T (v0) = T (v1), let ǫ := maxt∈T |v0(t) −
v1(t)|. Then |lexG[v0](i) − lexG[v1](i)| ≤ ǫ for all i ∈ VG.

5

3.2 Alternate Characterizations

There are at least two other seemingly disparate deﬁnitions that are equivalent to lex-minimal voltages.

lp-norm Minimizers. As mentioned in the introduction, for a well-posed instance (G, v0) the lex-minimizer is also
the limit of lp minimizers. This follows from existing results about the limit of lp-minimizers (Egger and Huotari
(1990)) in afﬁne spaces, since {grad[v] | v is complete, v extends v0} forms an afﬁne subspace of Rm. Thus, we have
the following theorem:

Theorem 3.8 (Limit of lp-minimizers, follows from Egger and Huotari (1990)) For any p ∈ (1, ∞), given a well-
posed instance (G, v0) deﬁne vp to be the unique complete voltage assignment extending v0 and minimizing
p ,
i.e.

grad[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Then, limp→∞ vp = lexG[v0].

vp = arg min
v is complete
v extends v0 (cid:13)
(cid:13)

grad[v]

p .

(cid:13)
(cid:13)

Max-Min Gradient Averaging. Consider a well-posed instance (G, v0), and a complete voltage assignment v ex-
tending v0. If G is such that ℓ(e) = 1 for all e ∈ EG, it is easy to see that lex = lexG[v0] satisﬁes the following simple
condition for all x ∈ VG \ T (v0),

lex(x) =

1
2  

max
(x,y)∈EG

lex(y) + min

lex(z)

.

(x,z)∈EG

!

This condition should be contrasted to the optimality condition for l2-regularization on these instances, which gives
for all non-terminals x, the optimal voltage v satisﬁes v(x) = 1

y:(x,y)∈EG v(y).

deg(x)

To prove the above claim, consider locally changing lex at x and observe that the gradients of edges not incident
at x remain unchanged, and at least one of edges incident at x will have a strictly larger gradient, contradicting lex-
minimality. For general graphs, this condition of local optimality can still be characterized by a simple max-min
gradient averaging property as described below.

P

Deﬁnition 3.9 (Max-Min Gradient Averaging) Given a well-posed instance (G, v0), and a complete voltage as-
signment v extending v0, we say that v satisﬁes the max-min gradient averaging property (w.r.t. (G, v0)) if for every
x ∈ VG \ T (v0), we have

grad[v](x, y) = − min

grad[v](x, y).

max
y:(x,y)∈EG

y:(x,y)∈EG

As stated in the theorem below, lexG[v0] is the unique assignment satisfying max-min gradient averaging property.
Shefﬁeld and Smart (2010) proved a variant of this statement for weighted graphs. For completeness, we present a
proof in the appendix.

Theorem 3.10 Given a well-posed instance (G, v0), lexG[v0] satisﬁes max-min gradient averaging property. More-
over, it is the unique complete voltage assignment extending v0 that satisﬁes this property w.r.t. (G, v0).

An advantage of this characterization is that it can be veriﬁed quickly. This is particularly useful for implementations
for computing the lex-minimizer.

4 Algorithms

We now sketch the ideas behind our algorithms and give precise statements of our results. A full description of all the
algorithms is included in the appendix.

We deﬁne the pressure of a vertex to be the gradient of the steepest terminal path through it:

pressure[v0](x) = max{∇P (v0) | P is a terminal path in (G, v0) and x ∈ P }.

6

Observe that in a graph with no terminal-terminal edges, a free terminal path is a steepest ﬁxable path iff its gradient
is equal to the highest pressure amongst all vertices. Moreover, vertices that lie on steepest ﬁxable paths are exactly
the vertices with the highest pressure. For a given α > 0, in order to identify vertices with pressure exceeding α, we
compute vectors vHigh[α](x) and vLow[α](x) deﬁned as follows in terms of dist, the metric on V induced by ℓ:

vLow[α](x) = min

{v0(t) + α · dist(x, t)}

t∈T (v0)

vHigh[α](x) = max
t∈T (v0)

{v0(t) − α · dist(t, x)}.

4.1 Lex-minimization on Star Graphs

We ﬁrst consider the problem of computing the lex-minimizer on a star graph in which every vertex but the center is a
terminal. This special case is a subroutine in the general algorithm, and also motivates some of our techniques.

Let x be the center vertex, T be the set of terminals, and all edges be of the form (x, t) with t ∈ T . The initial
voltage assignment is given by v : T → R, and we abbreviate dist(x, t) by d(t) = ℓ(x, t). From Corollary 3.4 we know
that we can determine the value of the lex minimizer at x by ﬁnding a steepest ﬁxable path. By deﬁnition, we need to
ﬁnd t1, t2 ∈ T that maximize the gradient of the path from t1 to t2, ∇(t1, t2) = v(t1)−v(t2)
d(t2)+d(t2) . As observed above, this
is equivalent to ﬁnding a terminal with the highest pressure. We now present a simple randomized algorithm for this
problem that runs in expected linear time.

Given a terminal t1, we can compute its pressure α along with the terminal t2 such that |∇(t1, t2)| = α in time
O(|T |) by scanning over the terminals in T . Consider doing this for a random terminal t1. We will show that in linear
time one can then ﬁnd the subset of terminals T ′ ⊂ T whose pressure is greater than α. Assuming this, we complete
the analysis of the algorithm. If T ′ = ∅, t1 is a vertex with highest pressure. Hence the path from t1 to t2 is a steepest
ﬁxable path, and we return (t1, t2). If T ′ 6= ∅, the terminal with the highest pressure must be in T ′, and we recurse by
picking a new random t1 ∈ T ′. As the size of T ′ will halve in expectation at each iteration, the expected time of the
algorithm on the star is O(|T |).

To determine which terminals have pressure exceeding α, we observe that the condition ∃t2 : α < ∇(t1, t2) =
v(t1)−v(t2)
d(t1)+d(t2) , is equivalent to ∃t2 : v(t2)+αd(t2) < v(t1)−αd(t1). This, in turn, is equivalent to vLow[α](x) < v(t1)−
αd(t1). We can compute vLow[α](x) in deterministic O(|T |) time. Similarly, we can check if ∃t2 : α < ∇(t2, t1) by
checking if vHigh[α](x) > vt1 + αd(t1). Thus, in linear time, we can compute the set T ′ of terminals with pressure
exceeding α. The above algorithm is described in Algorithm 10.

Theorem 4.1 Given a set of terminals T, initial voltages v : T → R, and distances d : T → R+, STARSTEEPESTPATH(T, v, d)
returns (t1, t2) maximizing v(t1)−v(t2)

d(t1)+d(t2) , and runs in expected time O(|T |).

4.2 Lex-minimization on General Graphs

Theorem 3.3, tells us that META-LEX will compute lex-minimizers given an algorithm for ﬁnding a steepest ﬁxable
path in (G, v0). Recall that ﬁnding a steepest ﬁxable path is equivalent to ﬁnding a path with gradient equal to the
highest pressure amongst all vertices. In this section, we show how to do this in expected time O(m + n log n).

We describe an algorithm VERTEXSTEEPESTPATH that ﬁnds a terminal path P through any vertex x such that
∇P (v0) = pressure[v0](x) in expected O(m + n log n) time. Using Dijkstra’s algorithm, we compute dist(x, t) for
all t ∈ T. If x ∈ T (v0), then there must be a terminal path P that starts at x that has ∇P (v0) = pressure[v0](x). To
compute such a P we examine all t ∈ T (v0) in O(|T |) time to ﬁnd the t that maximizes |∇(x, t)| = |v(x)−v(t)|
, and
dist(x,t)
then return a shortest path between x and that t.

If x /∈ T (v0), then the steepest path through x between terminals t1 and t2 must consist of shortest paths between
x and t1 and between x and t2. Thus, we can reduce the problem to that of ﬁnding the steepest path in a star graph
where x is the only non-terminal and is connected to each terminal t by an edge of length dist(x, t). By Theorem 4.1,
we can ﬁnd this steepest path in O(|T |) expected time. The above algorithm is formally described as Algorithm 9.

Theorem 4.2 Given a well-posed instance (G, v0), and a vertex x ∈ VG, VERTEXSTEEPESTPATH(G, v0, x) returns
a terminal path P through x such that ∇P (v0) = pressure[v0](x), in O(m + n log n) expected time.

7

As in the algorithm for the star graph, we need to identify the vertices whose pressure exceeds a given α. For a ﬁxed
α, we can compute vLow[α](x) and vHigh[α](x) for all x ∈ VG using a simple modiﬁcation of Dijkstra’s algorithm in
O(m + n log n) time. We describe the algorithms COMPVHIGH, COMPVLOW for these tasks in Algorithms 3 and 4.
The following lemma encapsulates the usefulness of vLow and vHigh.

Lemma 4.3 For every x ∈ VG, pressure[v0](x) > α iff vHigh[α](x) > vLow[α](x).

It immediately follows that the algorithm COMPHIGHPRESSGRAPH(G, v0, α) described in Algorithm 6 computes

the vertex induced subgraph on the vertex set {x ∈ VG| pressure[v0](x) > α}.

We can combine these algorithms into an algorithm STEEPESTPATH that ﬁnds the steepest ﬁxable path in (G, v0)
in O(m + n log n) expected time. We may assume that there are no terminal-terminal edges in G. We sample an edge
(x1, x2) uniformly at random from EG, and a terminal x3 uniformly at random from VG. For i = 1, 2, 3, we compute
the steepest terminal path Pi containing xi. By Theorem 4.2, this can be done in O(m + n log n) expected time. Let α
be the largest gradient maxi ∇Pi. As mentioned above, we can identify G′, the induced subgraph on vertices x with
pressure exceeding α, in O(m + n log n) time. If G′ is empty, we know that the path Pi with largest gradient is a
steepest ﬁxable path. If not, a steepest ﬁxable path in (G, v0) must be in G′, and hence we can recurse on G′. Since
we picked a uniformly random edge, and a uniformly random vertex, the expected size of G′ is at most half that of G.
Thus, we obtain an expected running time of O(m + n log n). This algorithm is described in detail in Algorithm 7.

Theorem 4.4 Given a well-posed instance (G, v0) with EG ∩ (T (v0) × T (v0)) = ∅, STEEPESTPATH(G, v0) returns
a steepest ﬁxable path in (G, v0), and runs in O(m + n log n) expected time.

By using STEEPESTPATH in META-LEX, we get the COMPLEXMIN, shown in Algorithm 1. From Theorem 3.3 and
Theorem 4.4, we immediately get the following corollary.

Corollary 4.5 Given a well-posed instance (G, v0) as input, algorithm COMPLEXMIN computes a lex-minimizing
assignment that extends v0 in O(n(m + n log n)) expected time.

4.3 Linear-time Algorithm for Inf-minimization

Given the algorithms in the previous section, it is straightforward to construct an inﬁnity minimizer. Let α⋆ be the
gradient of the steepest terminal path. From Lemma 3.5, we know that the norm of the inf minimizer is α⋆. Considering
all trivial terminal paths (terminal-terminal edges), and using STEEPESTPATH, we can compute α⋆ in randomized
O(m+n log n) time. It is well known (McShane (1934); Whitney (1934)) that v1 = vLow[α⋆] and v2 = vHigh[α⋆] are
inf-minimizers. It is also known that 1
2 (v1 + v2) is the inf-minimizer that minimizes the maximum ℓ∞-norm distance
to all inf-minimizers. In the case of path graphs, this was observed by Gaffney and Powell (1976) and independently
by Micchelli et al. (1976). For completeness, the algorithm is presented as Algorithm 5, and we have the following
result.

Theorem 4.6 Given a well-posed instance (G, v0), COMPINFMIN(G, v0) returns a complete voltage assignment v
for G extending v0 that minimizes

∞ , and runs in randomized O(m + n log n) time.

grad[v]

4.4 Faster Algorithms for Lex-minimization

(cid:13)
(cid:13)

(cid:13)
(cid:13)

The lex-minimizer has additional structure that allows one to compute it by more efﬁcient algorithms. One observation
that leads to a faster implementation is that ﬁxing a steepest ﬁxable path does not increase the pressure at vertices,
provided that one appropriately ignores terminal-terminal edges. Thus, if G(α) is a subgraph that we identiﬁed with
pressure greater than α, we can iteratively ﬁx all steepest ﬁxable paths P in G(α) with ∇P > α. Another simple
observation is that if G(α) is disconnected, we can simply recurse on each of the connected components. A complete
description of an the algorithm COMPFASTLEXMIN based on these idea is given in Algorithm 11. The algorithm
provably computes lexG(v0), and it is possible to implement it so that the space requirement is only O(m + n).
Although, we are unable to prove theoretical bounds on the running time that are better than O(n(m + n log n)),
it runs extremely quickly in practice. We used it to perform the experiments in this paper. For random regular
graphs and Delaunay graphs, with n = 0.5 × 106 vertices and around 2 million edges m ∼ 1.5 − 2 × 106, it

8

takes a couple of minutes on a 2009 MacBook Pro. Similar times are observed for other model graphs of this
size such as random regular graphs and real world networks. An implementation of this algorithm may be found
at https://github.com/danspielman/YINSlex.

5 Directed Graphs

Our deﬁnitions and algorithms, including those for regularization, extend to directed graphs with only small modiﬁ-
cations. We view directed edges as diodes and only consider potential differences in the direction of the edge. For
a complete voltage assignment v on the vertices of a directed graph G, we deﬁne the directed gradient on (x, y) due
to v to be grad+
. Given a partially-labelled directed graph (G, v0), we say that a a
complete voltage assignment v is a lex-minimizer if it extends v0 and for other complete voltage assignment v′ that
extends v0 we have grad+
G[v′]. We say that a partially-labelled directed graph (G, v0) is a well-posed
directed instance if every free vertex appears in a directed path between two terminals.

G[v](x, y) = max

G[v] (cid:22) grad+

v(x)−v(y)
ℓ(x,y)

, 0

n

o

The main difference between the directed and undirected cases is that the directed lex-minimizer is not necessarily
unique. To maintain clarity of exposition, we chose to focus on undirected graphs so far. For directed graphs, we have
the following corresponding structural results.

Theorem 5.1 Given a well-posed instance (G, v0) on a directed graph G, there exists a lex-minimizer, and the set of
all lex-minimizers is a convex set. Moreover, for every two lex-minimizers v and v′, we have grad+

G[v] = grad+

G[v′].

However, note that in the case of directed graphs, the lex-minimizer need not be unique. We still have a weaker version
of Theorem 3.3 for directed graphs.

Theorem 5.2 Given a well-posed instance (G, v0) on a directed graph G, let v1 be the partial voltage assignment
extending v0 obtained by repeatedly ﬁxing steepest ﬁxable (directed) paths P with ∇P > 0. Then, any lex-minimizer
of (G, v0) must extend v1. Moreover, for every edge e ∈ EG \ (T (V1) × T (V1)), any lex-minimizer v of (G, v0) must
satisfy grad+[v](e) = 0.

When the value of the lex-minimizer at a vertex is not uniquely determined, it is constrained to an interval. In our
experiments, we pick the convention that when the voltage at a vertex is constrained to an interval (−∞, a] or [a, ∞),
we assign a to the terminal. When it is constrained to a ﬁnite interval, we assign a voltage closest to the median of the
original voltages.

6 Experiments on WebSpam

We demonstrate the performance of our lex-minimization algorithms on directed graphs by using them to detect spam
webpages as in Zhou et al. (2007). We use the dataset webspam-uk2006-2.0 described in Castillo et al. (2006).
This collection includes 11,402 hosts, out of which 7,473 (65.5 %) are labeled, either as spam or normal. Each host
corresponds to the collection of web pages it serves. Of the hosts, 1924 are labeled spam (25.7 % of all labels). We
consider the problem of ﬂagging some hosts as spam, given only a small fraction of the labels for training. We assign
a value of 1 to the spam hosts, and a value of 0 to the normal ones. We then compute a lex minimizer and examine the
effect of ﬂagging as spam all hosts with a value greater than some threshold.

Following Zhou et al. (2007), we create edges between hosts with lengths equal to the reciprocal of the number of
links from one to the other. We run our experiments only on the largest strongly connected component of the graph,
which contains 7945 hosts of which 5552 are labeled. 16 % of the nodes in this subgraph are labeled spam. To create
training and test data, for a given value p, we select a random subset of p % of the spam labels and a random subset
of p % of the normal labels to use for training. The remaining labels are used for testing. We report results for p = 5
and p = 20.

Again following Zhou et al. (2007), we plot the precision and recall of different choices of threshold for ﬂagging
pages as spam. Recall is the fraction of spam pages our algorithm ﬂags as spam, and precision is the fraction of pages
our algorithm ﬂags as spam that actually are spam. Amongst the algorithms studied by Zhou et al. (2007), the top

9

performer was their algorithm based on sampling according to a random-walk that follows in-links from other hosts.
We compare their algorithm with the classiﬁcation we get by directing edges in the opposite directions of links. This
has the effect that a link to a spam host is evidence of spamminess, and a link from a normal host is evidence of
normality.

Results are shown in Figure 3. While we are not able to reliably ﬂag all spam hosts, we see that in the range of
10-50 % recall, we are able to ﬂag spam with precision above 82 %. We see that the performance of directed lex-
minimization does not degrade rapidly when from the “large training set” regime of p = 20, to the “small training set”
regime of p = 5.

5 % labels for training

20 % labels for training

RandWalk
DirectedLex

RandWalk
DirectedLex

i

i

n
o
s
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
0.1

0.2

0.3

0.4

0.7

0.8

0.9

1

0.2

0.3

0.4

0.7

0.8

0.9

1

0.6
0.5
Recall

0.6
0.5
Recall

Figure 3: Recall and precision in the web spam classiﬁcation experiment. Each data point shown was computed as an average over
100 runs. The largest standard deviation of the mean precision across the plotted recall values was less than 1.3 %. The algorithm
of Zhou et al. (2007) appears as RANDWALK. Our directed lex-minimization algorithm appears as DIRECTEDLEX.

For comparison, in Appendix C, we show the performance of our algorithm and that of Zhou et al. (2007) both
with link directions reversed, as well as the performance of undirected lex-minimization and Laplacian inference, all
of which are signiﬁcantly worse.

7 l0-Regularization of Vertex Values

We now explain how we can accommodate noise in both the given voltages and in the given lengths of edges. We can
ﬁnd the minimum number of labels to ignore, or the minimum increase in edges lengths needed so that there exists an
extension whose gradients have l∞-norm lower than a given target. After determining which labels to ignore or the
needed increment in edge lengths, we recommend computing a lex minimizer.

The algorithms we present in this section are essentially the same for directed and undirected graphs.

7.1 l0-Vertex Regularization for Inf-minimization

The l0-regularization of vertex labels can be viewed as a problem of outlier removal: the vector we compute is allowed
to disagree with v0 on up to k terminals. Given a voltage assignment v and a subset T ⊂ V of the vertices, by v(T )
we mean the vector obtained by restricting v to T . We deﬁne the l0-Vertex Regularization for l∞ problem to be

where v(T ) is the vector of values of v on the terminals T .

min
v∈IRn

gradG[v]

∞

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

0 ≤ k,
(cid:13)
(cid:13)

subject to

v(T ) − v0(T )

(3)

In Appendix D, we describe an approximation algorithm APPROX-OUTLIER that approximately solves program (3).

The precise statement we prove in Appendix D is given in the following theorem.

1

0.9

0.8

0.7

i

i

n
o
s
c
e
r
P

0.6

0.5

0.4

0.3

0.2

0.1

0
0.1

10

Theorem 7.1 (Approximate l0-vertex regularization) The algorithm APPROX-OUTLIER takes a positive integer k
and a partially-labeled graph (G, v0), and outputs an assignment v with
0 ≤ 2k, and
∞ ≤
α∗, where α∗ is the optimum value of program (3). The algorithm runs in time O(k(m + n log n)).
(cid:13)
(cid:13)
(cid:13)
(cid:13)

In Appendix D, we also describe an algorithm OUTLIER that exactly solves program (3) in polynomial time, and we
prove its correctness.

v(T ) − v0(T )

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Theorem 7.2 (Exact l0-vertex regularization) The algorithm OUTLIER takes a positive integer k and a partially-
labeled graph (G, v0) solves program (3) exactly. The algorithm runs in polynomial time.

We give a proof of Theorem 7.2 in Appendix D. To do this, we reduce the program (3) to the problem of minimizing
the required l0-budget needed to achieve a ﬁxed gradient α using a binary search over a set of O(n2) gradients. This
latter problem we reduce in polynomial time to Minimum Vertex Cover (VC) on a transitively closed, directed acyclic
graph (a TC-DAG). VC on a TC-DAG can be solved exactly in polynomial time by a reduction to the Maximum
Bipartite Matching Problem (Fulkerson (1956)). The problem was phrased by Fulkerson as one of ﬁnding a maximum
antichain of a ﬁnite poset. Any transitively closed DAG corresponds directly to the comparability graph of a poset. A
maximum antichain of a poset is a maximum independent set of a the comparability graph of the poset, and hence its
complement is a minimum vertex cover of the comparability graph. We refer to the algorithm developed by Fulkerson
as KONIG-COVER.

Theorem 7.3 The algorithm KONIG-COVER computes a minimum vertex cover for any transitively closed DAG G in
polynomial time.

7.2 Hardness of l0 regularization for l2

The result that l0-regularized inf-minimization can be solved exactly in polynomial time is surprising, especially
because the analogous problem for 2-Laplacian minimization turns out to be NP-Hard.

We deﬁne the the l0 vertex regularization for l2 for a partially-labeled graph (G, v0) and an integer k by

min
v∈Rn:kv(T )−v0(T )k0

≤k

vT Lv,

where L is the Laplacian of G.

Theorem 7.4 l0 vertex regularization for l2 is NP-Hard.

In Appendix E we prove Theorem 7.4 by giving a polynomial time (Karp) reduction from the NP-Hard minimum
bisection problem to l0 vertex regularization for l2.

8 l1-Edge and Vertex Regularization of Inf-minimizers

Consider a partially-labeled graph (G, v0) and an α > 0. The set of voltage assignments given by

v : v extends v0 and

gradG[v]

∞ ≤ α

n

(cid:13)
(cid:13)

(cid:13)
(cid:13)

o

is convex. Going further, let us consider the edge lengths in a graph to be speciﬁed by a vector ℓ ∈ IRE. Now the set
of voltages v and and lengths ℓ which achieve kgradG(ℓ)[v]k∞ ≤ α is jointly convex in v and ℓ. To see this, observe
that

kgradG(ℓ)[v]k∞ ≤ α ⇔ ∀(u, v) ∈ E : −αℓ(u, v) ≤ v(u) − v(v) ≤ αℓ(u, v).
Furthermore, the condition “v extends v0” is a linear constraint on v, which we express as v(T ) = v0(T ). From
the above, it is clear that the gradient condition corresponds to a convex set, as it is an intersection of half-spaces.
These half-spaces are given by O(m) linear inequalities. We can leverage this to phrase many regularized variants of
inf-minimization as convex programs, and in some cases linear programs.

(4)

11

For example, we may consider a variant of inf-minimization combined with an l1-budget for changing lengths of
edges and values on terminals. Given a parameter γ > 0 which speciﬁes the relative cost of regularizing terminals to
regularizing edges, the problem is as follows

arg min
v∈IRn,s∈IRm,s≥0

ksk1 + γ

v(T ) − v0(T )

1

subject to

gradG(ℓ+s)[v]

≤ α.

(5)

(cid:13)
(cid:13)
From our observation (4), it follows that problem (5) may be expressed as a linear program with O(n) variables
and O(m) constraints. We can use ideas from Daitch and Spielman (2008) to solve the resulting linear program in
O(m1.5) by an interior point method with a special purpose linear equation solver. The reason is that the linear
time
equations the IPM must solve at each iteration may be reduced to linear equations in symmetric, diagonally dominant
matrices, and these may be solved in nearly-linear time (Cohen et al. (2014)).

(cid:13)
(cid:13)

e

(cid:13)
(cid:13)
(cid:13)

∞

(cid:13)
(cid:13)
(cid:13)

Conclusion. We propose the use of inf and lex minimizers for regression on graphs. We present simple algorithms
for computing them that are provably fast and correct, and can also be implemented efﬁciently. We also present a
framework and polynomial time algorithms for regularization in this setting. The initial experiments reported in the
paper indicate that these algorithms give pretty good results on real and synthetic datasets. The results seem to compare
quite favorably to other algorithms, particularly in the regime of tiny labeled sets. We are testing these algorithms on
several other graph learning questions, and plan to report on them in a forthcoming experimental paper. We believe
that inf and lex minimizers, and the associated ideas presented in the paper, should be useful primitives that can be
proﬁtably combined with other approaches to learning on graphs.

We thank anonymous reviewers for helpful comments. We thank Santosh Vempala and Bartosz Walczak for pointing
out that it was already known how to compute a minimum vertex cover of a transitively closed DAG in polynomial
time.

Acknowledgements

References

Morteza Alamgir
In Advances
Information Processing
http://books.nips.cc/papers/files/nips24/NIPS2011_0278.pdf.

and Ulrike V. Luxburg.

transition
24,

in
pages

in Neural

Systems

Phase

the

family
379–387.

of
2011.

p-resistances.
URL

Gunnar Aronsson. Extension of functions satisfying lipschitz conditions. Arkiv fr Matematik, 6(6):551–561, 1967.

ISSN 0004-2080. doi: 10.1007/BF02591928. URL http://dx.doi.org/10.1007/BF02591928.

Gunnar Aronsson, Michael G. Crandall, and Petri Juutinen. A tour of the theory of absolutely minimizing functions.
ISSN 0273-0979. doi: 10.1090/S0273-0979-04-01035-3.

Bull. Amer. Math. Soc. (N.S.), 41(4):439–505, 2004.
URL http://dx.doi.org/10.1090/S0273-0979-04-01035-3.

Guy Barles and J´erˆome Busca. Existence and comparison results for fully nonlinear degenerate elliptic equations

without zeroth-order term. Comm. Partial Differential Equations, 26:2323–2337, 2001.

Mikhail Belkin, Irina Matveeva, and Partha Niyogi.

Regularization and semi-supervised learning on large
In Learning Theory, volume 3120 of Lecture Notes in Computer Science, pages 624–638.
doi: 10.1007/978-3-540-27819-1 43. URL

graphs.
Springer Berlin Heidelberg, 2004.
http://dx.doi.org/10.1007/978-3-540-27819-1_43.

ISBN 978-3-540-22282-8.

Nick Bridle and Xiaojin Zhu. p-voltages: Laplacian regularization for semi-supervised learning on high-dimensional

data. In Eleventh Workshop on Mining and Learning with Graphs (MLG2013), 2013.

12

Carlos Castillo, Debora Donato, Luca Becchetti, Paolo Boldi, Stefano Leonardi, Massimo Santini, and Sebastiano
Vigna. A reference collection for web spam. SIGIR Forum, 40(2):11–24, December 2006. ISSN 0163-5840. doi:
10.1145/1189702.1189703. URL http://doi.acm.org/10.1145/1189702.1189703.

Olivier Chapelle, Bernhard Schlkopf, and Alexander Zien. Semi-Supervised Learning. The MIT Press, 1st edition,

2010. ISBN 0262514125, 9780262514125.

Michael B Cohen, Rasmus Kyng, Gary L Miller, Jakub W Pachocki, Richard Peng, Anup B Rao, and Shen Chen Xu.
Solving SDD linear systems in nearly m log1/2 n time. In Proceedings of the 46th Annual ACM Symposium on
Theory of Computing, pages 343–352. ACM, 2014.

M.G. Crandall, L.C. Evans, and R.F. Gariepy. Optimal lipschitz extensions and the inﬁnity laplacian. Calculus of Vari-
ations and Partial Differential Equations, 13(2):123–139, 2001. ISSN 0944-2669. doi: 10.1007/s005260000065.
URL http://dx.doi.org/10.1007/s005260000065.

Samuel I. Daitch and Daniel A. Spielman. Faster approximate lossy generalized ﬂow via interior point algo-
rithms.
In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing, STOC ’08, pages
451–460, New York, NY, USA, 2008. ACM. ISBN 978-1-60558-047-0. doi: 10.1145/1374376.1374441. URL
http://doi.acm.org/10.1145/1374376.1374441.

Alan Egger and Robert Huotari. Rate of convergence of the discrete polya algorithm. Journal of Approximation
ISSN 0021-9045. doi: http://dx.doi.org/10.1016/0021-9045(90)90070-7. URL

Theory, 60(1):24 – 30, 1990.
http://www.sciencedirect.com/science/article/pii/0021904590900707.

Charles Fefferman. Whitney’s extension problems and interpolation of data.

(N.S.), 46(2):207–220, 2009a.
http://dx.doi.org/10.1090/S0273-0979-08-01240-8.

ISSN 0273-0979.

doi:

10.1090/S0273-0979-08-01240-8.

Bull. Amer. Math. Soc.
URL

Charles Fefferman. Fitting a [image] -smooth function to data, iii. Annals of Mathematics, 170(1):pp. 427–441, 2009b.

ISSN 0003486X. URL http://www.jstor.org/stable/40345469.

Charles Fefferman and Bo’az Klartag. Fitting a cm -smooth function to data i. Annals of Mathematics, 169(1):pp.

315–346, 2009. ISSN 0003486X. URL http://www.jstor.org/stable/40345445.

D. R. Fulkerson. Note on dilworths decomposition theorem for partially ordered sets. Proc. Amer. Math. Soc, 1956.

P.W. Gaffney and M.J.D. Powell. Optimal interpolation. In Numerical Analysis, volume 506 of Lecture Notes in Math-
ematics, pages 90–99. Springer Berlin Heidelberg, 1976. ISBN 978-3-540-07610-0. doi: 10.1007/BFb0080117.
URL http://dx.doi.org/10.1007/BFb0080117.

L.-A. Gottlieb, A. Kontorovich, and R. Krauthgamer. Efﬁcient classiﬁcation for metric data. CoRR, abs/1306.2547,

2013. URL http://arxiv.org/abs/1306.2547.

L.-A. Gottlieb, A. Kontorovich, and R. Krauthgamer. Efﬁcient regression in metric spaces via approximate lipschitz
extension. In Similarity-Based Pattern Recognition, volume 7953 of Lecture Notes in Computer Science, pages
43–58. Springer Berlin Heidelberg, 2013b. ISBN 978-3-642-39139-2. doi: 10.1007/978-3-642-39140-8 3. URL
http://dx.doi.org/10.1007/978-3-642-39140-8_3.

Robert Jensen. Uniqueness of lipschitz extensions: Minimizing the sup norm of the gradient. Archive for Ra-
doi: 10.1007/BF00386368. URL

ISSN 0003-9527.

tional Mechanics and Analysis, 123(1):51–74, 1993.
http://dx.doi.org/10.1007/BF00386368.

M. Kirszbraun. ber die zusammenziehende und lipschitzsche transformationen. Fundamenta Mathematicae, 22(1):

77–108, 1934. URL http://eudml.org/doc/212681.

13

Andrew J. Lazarus, Daniel E. Loeb,

James G. Propp, Walter R. Stromquist,

Combinatorial games under

man.
229 – 264,
http://www.sciencedirect.com/science/article/pii/S0899825698906765.

http://dx.doi.org/10.1006/game.1998.0676.

and Economic Behavior,

ISSN 0899-8256.

auction play.

Games

1999.

doi:

and Daniel H. Ull-
27(2):
URL

Yunqian Ma and Yun Fu. Manifold Learning Theory and Applications. CRC Press, Inc., Boca Raton, FL, USA, 1st

edition, 2011. ISBN 1439871094, 9781439871096.

E. J. McShane. Extension of range of functions. Bull. Amer. Math. Soc., 40(12):837–842, 12 1934. URL

http://projecteuclid.org/euclid.bams/1183497871.

C.A. Micchelli, T.J. Rivlin,

and S. Winograd.

merische Mathematik, 26(2):191–200, 1976.
http://dx.doi.org/10.1007/BF01395972.

The optimal
ISSN 0029-599X.

recovery of
doi:

smooth functions.
10.1007/BF01395972.

Nu-
URL

V. A. Milman.

Absolutely minimal extensions of

functions on metric spaces.

1999.

URL

http://iopscience.iop.org/1064-5616/190/6/A05/pdf/MSB_190_6_A05.pdf.

Boaz Nadler, Nathan Srebro, and Xueyuan Zhou. Statistical analysis of semi-supervised learning: The limit of inﬁnite
unlabelled data. 2009. URL http://ttic.uchicago.edu/˜nati/Publications/NSZnips09.pdf.

A. Naor and S. Shefﬁeld. Absolutely minimal Lipschitz extension of tree-valued mappings. CoRR, abs/1005.2535,

May 2010. URL http://arxiv.org/abs/1005.2535.

A. M. Oberman. Finite difference methods for the Inﬁnity Laplace and p-Laplace equations. CoRR, abs/1107.5278,

July 2011. URL http://arxiv.org/abs/1107.5278.

Yuval Peres, Oded Schramm, Scott Shefﬁeld, and DavidB. Wilson.

Tug-of-war and the inﬁnity lapla-
In Selected Works of Oded Schramm, Selected Works in Probability and Statistics, pages 595–
doi: 10.1007/978-1-4419-9675-6 18. URL

cian.
638. Springer New York, 2011.
http://dx.doi.org/10.1007/978-1-4419-9675-6_18.

ISBN 978-1-4419-9674-9.

S. Shefﬁeld and C. K. Smart. Vector-valued optimal Lipschitz extensions. CoRR, abs/1006.1741, June 2010. URL

http://arxiv.org/abs/1006.1741.

Ali Kemal Sinop and Leo Grady. A seeded image segmentation framework unifying graph cuts and random walker
which yields a new algorithm. In Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on,
pages 1–8. IEEE, 2007.

Vijay V. Vazirani. Approximation Algorithms. Springer-Verlag New York, Inc., New York, NY, USA, 2001. ISBN

3-540-65367-8.

Ulrike von Luxburg and Olivier Bousquet. Distance-based classiﬁcation with lipschitz functions.

In Learn-
ing Theory and Kernel Machines, volume 2777 of Lecture Notes in Computer Science, pages 314–328.
doi: 10.1007/978-3-540-45167-9 24. URL
Springer Berlin Heidelberg, 2003.
http://dx.doi.org/10.1007/978-3-540-45167-9_24.

ISBN 978-3-540-40720-1.

Hassler Whitney.

Analytic extensions of differentiable functions deﬁned in closed sets.

tions of
http://www.jstor.org/stable/1989708.

the American Mathematical Society, 36(1):pp. 63–89, 1934.

ISSN 00029947.

Transac-
URL

Dengyong Zhou, Christopher J. C. Burges, and Tao Tao. Transductive link spam detection.

In Proceedings
of the 3rd International Workshop on Adversarial Information Retrieval on the Web, AIRWeb ’07, pages 21–
ISBN 978-1-59593-732-2. doi: 10.1145/1244408.1244413. URL
28, New York, NY, USA, 2007. ACM.
http://doi.acm.org/10.1145/1244408.1244413.

Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic

functions. In IN ICML, pages 912–919, 2003.

14

A Basic Properties of Lex-Minimizers

A.1 Meta Algorithm

Algorithm 1: Algorithm META-LEX: Given a well-posed instance (G, v0), outputs lexG[v0].
for i = 1, 2, . . . :

1. if T (vi−1) = VG, then return vi−1.
2. E′ = EG \ (T (vi−1) × T (vi−1)), G′ := (VG, E′).
3. Let P ⋆
4. vi ← ﬁx[vi−1, P ⋆
i ].

i be a steepest ﬁxable path in (G′, vi−1). Let α⋆

i ← ∇P ⋆(vi−1).

In this subsection, we prove the results that appeared in section 2. We start with a simple observation.

Proposition A.1 Given a well-posed instance (G, v0) such that T (v0) 6= V, let P be a steepest ﬁxable path in (G, v0).
Then, ﬁx[v0, P ] extends v0, and (G, ﬁx[v0, P ]) is also a well-posed instance.

The properties we prove below do not depend on the choice of the steepest ﬁxable path.

Proposition A.2 For any well-posed instance (G, v0), with |VG| = n, META-LEX(G, v0) terminates in at most n
iterations, and outputs a complete voltage assignment v that extends v0.

Proof of Proposition A.2: By Proposition A.1, at any iteration i, vi−1 extends v0 and (G′, vi−1) is a well-posed
instance. META-LEX only outputs vi−1 iff T (vi−1) = V, which means vi−1 is a complete voltage assignment. For
any vi−1 that is not complete, for any x ∈ V \T (vi−1), we must have a free terminal path in (G′, vi−1) that contains x.
i exists in (G′, vi−1). Since P ⋆
Hence, a steepest ﬁxable path P ⋆
i ] ﬁxes the voltage
i
✷
for at least one non-terminal. Thus, META-LEX(G, v0) must complete in at most n iterations.

is a free terminal path, ﬁx[vi−1, P ⋆

For the following lemmas, consider a run of META-LEX with well-posed instance (G, v0) as input. Let vout be the
complete voltage assignment output by META-LEX. Let Ei be the set of edges E′ and Gi be the graph G′ constructed
in iteration i of META-LEX.

Lemma A.3 For every edge e ∈ Ei−1 \ Ei, we have

grad[vout](e)

≤ α⋆

i . Moreover, α⋆

i is non-increasing with i.

Proof of Lemma A.3: Let P ⋆
i = (x0, . . . , xr) be a steepest ﬁxable path in iteration i (when we deal with instance
(Gi−1, vi−1)). Consider a terminal path Pi+1 in (Gi, vi) such that {∂0Pi+1, ∂1Pi+1} ∩ (T (vi) \ T (vi−1)) 6= ∅. We
i . On the contrary, assume that ∇Pi+1(vi) > α⋆
claim that ∇Pi+1(vi) ≤ α⋆
i . Consider the case ∂0Pi+1 ∈ T (vi) \
T (vi−1), ∂1P1 ∈ T (vi−1). By the deﬁnition of vi, we must have ∂0Pi+1 = xj for some j ∈ [r − 1]. Let P ′
i+1 be the
path formed by joining paths P ⋆

i+1 is a free terminal path in (Gi−1, vi−1). We have,

i [x0 : xj] and Pi+1. P ′

(cid:12)
(cid:12)

(cid:12)
(cid:12)

vi−1(x0) − vi−1(∂1Pi+1) = (vi(x0) − vi(xj )) + (vi(∂0Pi+1) − vi(∂1Pi+1))
i · ℓ(P ′

i · ℓ(Pi+1) = α⋆

i [x0 : xj]) + α⋆

i · ℓ(P ⋆

> α⋆

i+1),

giving ∇P ′
The other cases can be handled similarly.

i+1(vi) > α⋆

i , which is a contradiction since the steepest ﬁxable path P ⋆
i

in (Gi−1, vi−1) has gradient α⋆
i .

Applying the above claim to an edge e ∈ Ei−1 \ Ei, whose gradient is ﬁxed for the ﬁrst time in iteration i, we
i . If v is the complete voltage assignment output by META-LEX, since v extends vi+1,
i , implying

i . Applying the claim to the symmetric edge, we obtain −grad[vout](e) ≤ α⋆

obtain that grad[vi+1](e) ≤ α⋆
we get grad[vout](e) ≤ α⋆
|grad[vout](e)| ≤ α⋆
i .

Consider any free terminal path Pi+1 in (Gi, vi). If Pi+1 is also a terminal path in (Gi−1, vi−1), it is a free
terminal path in (Gi−1, vi−1). In addition, since a steepest ﬁxable path P ⋆
i , we get
i
∇Pi+1(vi) = ∇Pi+1(vi−1) ≤ α⋆
i . Otherwise, we must have {∂0Pi+1, ∂1Pi+1} ∩ (T (vi) \ T (vi−1)) 6= ∅, and we can
deduce ∇Pi+1(vi) ≤ α⋆
i using the above claim. Thus, all free terminal paths Pi+1 in (Gi, vi) satisfy ∇Pi+1(vi) ≤ α⋆
i .
✷
In particular, α⋆

in (Gi−1, vi−1) has ∇P ⋆

i = α⋆

i is non-increasing with i.

i+1(vi) ≤ α⋆

i+1 = ∇P ⋆

i . Thus, α⋆

15

Lemma A.4 For any complete voltage assignment v for G that extends v0, if v 6= vout, we have grad[v] 6(cid:22) grad[vout],
and hence grad[vout] (cid:22) grad[v].

Proof of Lemma A.4: Consider any complete voltage assignment v for G that extends v0, such that v 6= vout. Thus,
there exists a unique i such that v extends vi−1 but does not extend vi. We will argue that grad[v] 6(cid:22) grad[vout], and
hence grad[vout] (cid:22) grad[v]. For every edge e ∈ E \ Ei−1 that has been ﬁxed so far, grad[v](e) = grad[vi−1](e) =
grad[vout](e), and hence we can ignore these edges.

Since v extends vi−1 but not vi, there exists an x ∈ T (vi) \ T (vi−1) such that v(x) 6= vi(x) = vout(x). Assume
i picked

i = (x0, . . . , xr) is the steepest ﬁxable path with gradient α⋆

v(x) < vi(x) (the other case is symmetric). If P ⋆
in iteration i, we must have x = xj for some j ∈ [r − 1]. Thus,

j

j

(v(xk−1) − v(xk)) = v(x0) − v(xj ) > vi(x0) − vi(xj ) = α⋆

i · ℓ(P ⋆

i [x0 : xj ]) = α⋆
i ·

ℓ(xk−1, xk).

Xk=1

Xk=1
Thus, for some k ∈ [j], we must have grad[v](xk−1, xk) > α⋆
is a path in Gi−1, we have {xk−1, xk} 6⊆
T (vi−1). This gives (xk−1, xk) ∈ (Ei−1 \ Ei). But then, from Lemma A.3, it follows that for all e ∈ (Ei−1 \ Ei), we
✷
have |grad[vout](e)| ≤ α⋆

i . Thus, we have grad[v] 6(cid:22) grad[vout].

i . Since P ∗
i

Lemma A.5 Let P = (x0, . . . , xr) be a steepest ﬁxable path such that it does not have any edges in T (v0) × T (v0)
and v1 = ﬁxG[v0, P ]. Then for every i ∈ [r], we have grad[v1](xi−1, xi) = ∇P.

Proof of Lemma A.5: Suppose this is not true and let j ∈ [r] be the minimum number such that grad[v1](xj−1, xj) 6=
∇P. By deﬁnition of v1 we would necessarily have j < r and vj ∈ T (v0). Suppose grad[v1](xj−1, xj ) < ∇P. We
would then have v1(x0) − v1(xj ) < ∇P ∗ ℓ(P [x0 : xj]). Since P does not have any edges in T (v0) × T (v0),
P1 := (xj, ..., xr) would be a free terminal path with ∇P1 > ∇P. This is a contradiction. Other cases can be ruled
out similarly.

✷

Proof of Theorem 3.3: Consider an arbitrary run of META-LEX on (G, v0). Let vout be the complete voltage
assignment output by META-LEX. Proposition A.1 implies that vout extends v0. Lemma A.4 implies that for any
complete voltage assignment v 6= vout that extends v0, we have grad[vout] (cid:22) grad[v]. Thus, vout is a lex-minimizer.
Moreover, the lemma also gives that for any such v, grad[v] 6(cid:22) grad[vout]. and hence vout is a unique lex-minimizer.
Thus, vout is the unique voltage assignment satisfying Def. 2.1, and we denote it as lexG[v0]. Since we started with an
✷
arbitrary run of META-LEX, uniqueness implies that every run of META-LEX on (G, v0) must output lexG[v0].

Proof of Lemma 3.5: Suppose we have a complete voltage assignment v extending v0, such that
For any terminal path P = (x0, . . . , xr), we get,

grad[v]

∞ ≤ α.

∇P (v0) = v0(∂0P ) − v0(∂1P ) = v(∂0P ) − v(∂1P ) =

grad[v](xi−1, xi) ≤ α ·

ℓ(xi−1, xi) = α · ℓ(P ),

(cid:13)
(cid:13)

(cid:13)
(cid:13)

r

i=1
X

giving ∇P (v0) ≤ α.

On the other hand, suppose every terminal path P in (G, v0) satisﬁes ∇P (v0) ≤ α. Consider v = lexG[v0]. We
know that v extends v0. For every edge e ∈ EG ∩ T (v0) × T (v0), e is a (trivial) terminal path in (G, v0), and hence
has satisﬁes grad[v](e) = grad[v0](e) = ∇e(v0) ≤ α. Considering the reverse edge, we also obtain −grad[v](e) ≤ α.
Thus, |grad[v](e)| ≤ α. Moreover, using Lemma A.3, we know that for edge e ∈ EG \ T (v0) × T (v0), |grad[v](e)| ≤
1 = ∇P ⋆
α⋆
1 ≤ α since P1 is a terminal path in (G, v0). Thus, for every e ∈ EG, |grad[v](e)| ≤ α, and hence
✷
grad[v]
∞ ≤ α.
(cid:13)
(cid:13)
(cid:13)
(cid:13)
A.2 Stability

In this subsection, we sketch a proof of the monotonicity of lex-minimizers and show how it implies the stability
property claimed earlier.

For any well-posed (G, v0), there could be several possible executions of META-LEX, each characterized by the

sequence of paths P ⋆

i . We can apply Theorem 3.3 to deduce the following structural result about the lex-minimizer.

r

i=1
X

16

Corollary A.6 For any well-posed instance (G, v0), consider a sequence of paths (P1, . . . , Pr) and voltage assign-
ments (v1, . . . , vr) for some positive integer r such that:

1. P ⋆

i is a steepest ﬁxable path in (Gi−1, vi−1) for i = 1, . . . , r.

2. vi = ﬁx[vi−1, P ⋆

i ] for i = 1, . . . , r.

3. T (vr) = VG.

Then, we have vr = lexG[v0].

We call such a sequence of paths and voltages to be a decomposition of lexG[v0]. Again, note that lexG[v0] can
possibly have multiple decompositions. However, any two such decompositions are consistent in the sense that they
produce the same voltage assignment.

Proof of Corollary 3.7: We ﬁrst deﬁne some operations on partial assignments which simpliﬁes the notation. Let
v0, v1 be any two partial assignments with the same set of terminals T := T (v0) = T (v1) and c, d ∈ R. By cv0 + d
we mean a partial assignment v with T (v) = T satisfying v(t) = cv0(t) + d for all t ∈ T . Also, by v0 + v1 we
mean a partial assignment v with T (v) = T satisfying v(t) = v0(t) + v1(t) for all t ∈ T. Also, we say v1 ≥ v0 if
v1(t) ≥ v0(t) for all t ∈ T .

Now we can show how Corollary 3.7 follows from Theorem 3.6. Let v := v1 − v0, and kvk∞ = ǫ, for some ǫ > 0.
Therefore, v0 + ǫ ≥ v1 ≥ v0 − ǫ. Theorem 3.6 then implies that lexG[v0] + ǫ ≥ lex[v1] ≥ lex[v0] − ǫ, hence proving
✷
the corollary.

Proof sketch of Theorem 3.6:
It is easy to see that the ﬁrst statement holds. For the second statement, we ﬁrst
observe that if there is a sequence of paths P1, ..., Pr that is simultaneously a decomposition of both lex[v0] and
lex[v1], then this is easy to see. If such a path sequence doesn’t exist, then we look at vt := v0 + t(v1 − v0). We
state here without a proof (though the proof is elementary) that we can then split the interval [0, 1] into ﬁnitely many
subintervals [a0, a1], [a1, a2], .., [ak−1, ak], with a0 = 0, ak = 1, such that for any i, there is a path sequence P1, ..., Pr
which is a decomposition of lex[vt] for all t ∈ [ai, ai+1]. We then observe that v0 = va0 ≤ va1 ≤ ...vak = v1. Since
for every ai, ai+1, there is a path sequence which is simultaneously a decomposition of both lex[vai ] and lex[vai+1 ],
we immediately get

lex[v0] = lex[va0 ] ≤ lex[va1] ≤ ... ≤ lex[vak ] = lex[v1].

✷

A.3 Alternate Characterizations

Proof of Theorem 3.10: We know that lexG[v0] extends v0. We ﬁrst prove that v = lexG[v0] satisﬁes the max-min
gradient averaging property. Assume to the contrary. Thus, there exists x ∈ VG \ T (v0) such that

max
y:(x,y)∈EG

grad[v](x, y) 6= − min

grad[v](x, y).

y:(x,y)∈EG

Assume that max(x,y)∈EG grad[v](x, y) ≥ − min(x,y)∈EG grad[v](x, y). Then, consider v′ extending v0 that is iden-
tical to v except for v′(x) = v(x) − ǫ for ǫ > 0. For ǫ small enough, we get that

and

max
y:(x,y)∈EG

grad[v′](x, y) < max

grad[v](x, y)

y:(x,y)∈EG

− min

y:(x,y)∈EG

grad[v′](x, y) < max

grad[v](x, y).

y:(x,y)∈EG

The gradient of edges not incident on the vertex x is left unchanged. This implies that grad[v]

6(cid:22) grad[v′],

contradicting the assumption that v is the lex-minimizer. (The other case is similar).

17

For the other direction. Consider a complete voltage assignment v extending v0 that satisﬁes the max-min gradient

averaging property w.r.t. (G, v0). Let

α = max

grad[v](x, y) ≥ 0

(x,y)∈EG
x∈V \T (v0)

be the maximum edge gradient, and consider any edge (x0, x1) ∈ EG such that grad[v](x1, x0) = α, with x1 ∈
V \ T (v0). If α = 0, grad[v] is identically zero, and is trivially the lex-minimal gradient assignment. Thus, both v and
lexG[v0] are constant on each connected component. Since (G, v0) is well-posed, there is at least one terminal in each
component, and hence v and lexG[v0] must be identical.

Now assume α > 0. By the max-min gradient averaging property, ∃x2 ∈ VG such that (x1, x2) ∈ EG and

grad[v](x1, x2) =

min
y:(x1,y)∈EG

grad[v](x1, y) = − max

grad[v](x1, y)

y:(x1,y)∈EG

≤ −grad[v](x1, x0) = −α.

Thus, grad[v](x2, x1) ≥ α. Since α is the maximum edge gradient, we must have grad[v](x2, x1) = α. More-
over, v(x2) > v(x1) > v(x0), thus x2 6= x0. We can inductively apply this argument at x2 until we hit a ter-
minal. Similarly, if x0 /∈ T (v0) we can extend the path in the other direction. Consequently, we obtain a path
P = (xj , . . . , x2, x1, x0, x−1, . . . , xk) with all vertices as distinct, such that xj , xk ∈ T (v0), and xi ∈ V \ T (v0)
for all i ∈ [j + 1, k − 1]. Moreover, grad[v](xi, xi−1) = α for all j < i ≤ k. Thus, P is a free terminal path with
∇P [v0] = α.

Moreover, since v is a voltage assignment extending v0 with

∞ = α, using Lemma 3.5, we know that
every terminal path P ′ in (G, v0) must satisfy ∇P ′(v0) ≤ α. Thus, P is a steepest ﬁxable path in (G, v0). Thus,
letting v1 = ﬁx[v0, P ], using Corollary 3.4, we obtain that lexG[v1] = lexG[v0]. Moreover, since α = ∇P [v0] =
grad[v](xi, xi−1) for all i ∈ (j, k], we get v1(xi) = v(xi) for all i ∈ (j, k). Thus, v extends v1.

grad[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We can iterate this argument for r iterations until T (vr) = VG, giving v = vr and vr = lexG[vr] = lexG[v0].
(Since we are ﬁxing at least one terminal at each iteration, this procedure terminates). Thus, we get v = lexG[v0]. ✷

B Description of the Algorithms

Algorithm 2: MODDIJKSTRA(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs a complete
voltage assignment v for G, and an array parent : V → V ∪ {null}.

Add x to a ﬁbonacci heap, with key(x) = +∞.
ﬁnished(x) ← false

Decrease key(x) to v0(x).
parent(x) ← null.

1. for x ∈ VG,
2.
3.
4. for x ∈ T (v0)
5.
6.
7. while heap is not empty
8.
9.
10.
11.
12.
13.
14.
15. return (v, parent)

x ← pop element with minimum key from heap
v(x) ← key(x). ﬁnished(x) ← true .
for y : (x, y) ∈ EG

if ﬁnished(y) = false

if key(y) > v(x) + α · ℓ(x, y)

Decrease key(y) to v(x) + α · ℓ(x, y).
parent(y) ← x.

Theorem B.1 For a well-posed instance (G, V0) and a gradient value α ≥ 0, let (v, parent) ← MODDIJKSTRA(G, v0, α).
Then, v is a complete voltage assignment such that, ∀x ∈ VG, v(x) = mint∈T (v0){v0(t) + αdist(x, t)}. Moreover, the
pointer array parent satisﬁes ∀x /∈ T (v0), parent(x) 6= null and v(x) = v(parent(x)) + α · ℓ(x, parent(x)).

18

Algorithm 3: Algorithm COMPVLOW(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs
vLow, a complete voltage assignment for G, and an array LParent : V → V ∪ {null}.

1. (vLow, LParent) ← MODDIJKSTRA(G, v0, α)
2. return (vLow, LParent)

Algorithm 4: Algorithm COMPVHIGH(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs
vHigh, a complete voltage assignment for G, and an array HParent : V → V ∪ {null}.

if x ∈ T (v0) then v1(x) ← −v0(x) else v1(x) ← v1(x).

1. for x ∈ VG
2.
3. (temp, HParent) ← MODDIJKSTRA(G, v1, α)
4. for x ∈ VG : vHigh(x) ← −temp(x)
5. return (vHigh, HParent)

Corollary B.2 For a well-posed instance (G, V0) and a gradient value α ≥ 0, let (vLow[α], LParent) ← COMPVLOW(G, v0, α)
and (vHigh[α], HParent) ← COMPVHIGH(G, v0, α). Then, vLow[α], vHigh[α] are complete voltage assignments for
G such that, ∀x ∈ VG,

vLow[α](x) = min

{v0(t) + α · dist(x, t)}

t∈T (v0)

vHigh[α](x) = max
t∈T (v0)

{v0(t) − α · dist(t, x)}.

Moreover, the pointer arrays LParent, HParent satisfy ∀x /∈ T (v0), LParent(x), HParent(x) 6= null and

vLow[α](x) = vLow[α](LParent(x)) + α · ℓ(x, LParent(x)),
vHigh[α](x) = vHigh[α](HParent(x)) − α · ℓ(x, HParent(x)).

Algorithm 5: Algorithm COMPINFMIN(G, v0): Given a well-posed instance (G, v0), outputs a complete voltage assignment
v for G, extending v0 that minimizes (cid:13)

(cid:13)grad[v](cid:13)

(cid:13)∞.

1. α ← max{|grad[v0](e)| | e ∈ EG ∩ (T (v0) × T (v0))}.
2. EG ← EG \ (T (v0) × T (v0))
3. P ←STEEPESTPATH(G, v0).
4. α ← max{α, ∇P (v0)}
5. (vLow, LParent) ← COMPVLOW(G, v0, α)
6. (vHigh, HParent) ← COMPVHIGH(G, v0, α)
7. for x ∈ VG
8.
9.
10.
11. return v

then v(x) ← v0(x)
else v(x) ← 1

2 · (vLow(x) + vHigh(x)).

if x ∈ T (v0)

1. (vLow, LParent) ← COMPVLOW(G, v0, α)
2. (vHigh, HParent) ← COMPVHIGH(G, v0, α)
3. VG′ ← {x ∈ VG | vHigh(x) > vLow(x) }
4. EG′ ← {(x, y) ∈ EG | x, y ∈ VG′ }.

19

Algorithm 6: Algorithm COMPHIGHPRESSGRAPH(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0,
outputs a minimal induced subgraph G′ of G where every vertex has pressure[v0](·) > α.

5. G′ ← (V ′, E′, ℓ)
6. return G′

Proof of Lemma 4.3:

is equivalent to

vHigh[α](x) > vLow[α](x)

max
t∈T (v0)

{v0(t) − α · dist(t, x)} > min

{v0(t) + α · dist(x, t)},

t∈T (v0)

which implies that there exists terminals s, t ∈ T (v0) such that

thus,

Hence,

v0(t) − α · dist(t, x) > v0(s) + α · dist(x, s)

pressure[v0](x) ≥

v0(t) − v0(s)
dist(t, x) + dist(x, s)

> α.

v0(t) − v0(s)
dist(t, x) + dist(x, s)

= pressure[v0](x) > α.

v0(t) − α · dist(t, x) > v0(s) + α · dist(x, s)

So the inequality on vHigh and vLow implies that pressure is strictly greater than α. On the other hand, if pressure[v0](x) >
α, there exists terminals s, t ∈ T (v0) such that

which implies vHigh[α](x) > vLow[α](x).

✷

Algorithm 7: Algorithm STEEPESTPATH(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs a steepest
free terminal path P in (G, v0).

P ← VERTEXSTEEPESTPATH(G, v0, xi)

1. Sample uniformly random e ∈ EG. Let e = (x1, x2).
2. Sample uniformly random x3 ∈ VG.
3. for i = 1 to 3
4.
5. Let j ∈ arg maxj∈{1,2,3} ∇Pj (v0)
6. G′ ← COMPHIGHPRESSGRAPH(G, v0, ∇Pj (v0))
7. if EG′ = ∅,
8.
9.

then return Pj
else return STEEPESTPATH(G′, v0|VG′ )

1. while T (v0) 6= VG
2.
3.
4.
5. return v0

EG ← EG \ (T (v0) × T (v0))
P ← STEEPESTPATH(G, v0)
v0 ← ﬁx[v0, P ]

Algorithm 8: Algorithm COMPLEXMIN(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs lexG[v0].

Algorithm 9: Algorithm VERTEXSTEEPESTPATH(G,v0, x): Given a well-posed instance (G, v0), and a vertex x ∈ VG,
outputs a steepest terminal path in (G, v0) through x.

1. Using Dijkstra’s algorithm, compute dist(x, t) for all t ∈ T (v0)

20

y ← arg maxy∈T (v0)
if v0(x) ≥ v0(y)

|v0(x)−v0(y)|
dist(x,y)

then return a shortest path from x to y
else return a shortest path from y to x

2. if x ∈ T (v0)
3.
4.
5.
6.
7. else
8.
9.
10.
11.

for t /∈ T (v0), d(t) ← dist(x, t)
(t1, t2) ← STARSTEEPESTPATH(T (v0), v0|T (v0), d)
Let P1 be a shortest path from t1 to x. Let P2 be a shortest path from x to t2.
P ← (P1, P2). return P.

Algorithm 10: STARSTEEPESTPATH(T, v, d): Returns the steepest path in a star graph, with a single non-terminal connected
to terminals in T, with lengths given by d, and voltages given by v.

|v(t1)−v(t)|
d(t1)+d(t)

1. Sample t1 uniformly and randomly from T
2. Compute t2 ∈ arg maxt∈T
3. α ← |v(t2)−v(t1)|
d(t1)+d(t2)
4. Compute vlow ← mint∈T (v(t) + α · d(t))
5. Tlow ← {t ∈ T | v(t) > vlow + α · d(t)}
6. Compute vhigh ← maxt∈T (v(t) − α · d(t))
7. Thigh ← {t ∈ T | v(t) < vhigh − α · d(t)}
8. T ′ ← Tlow ∪ Thigh.
9. if T ′ = ∅
10.
11.

then if v(t1) ≥ v(t2) then return (t1, t2) else return (t2, t1)
else return STARSTEEPESTPATH(T ′, v|T ′, dT ′ )

B.1 Faster Lex-minimization

Algorithm 11: Algorithm COMPFASTLEXMIN(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs
lexG[v0].

1. while T (v0) 6= VG
2.
3. return v0

v0 ← FIXPATHSABOVEPRESS(G, v0, 0)

Algorithm 12: Algorithm FIXPATHSABOVEPRESS(G, v0, α): Given a well-posed instance (G, v0), with T (v0) 6= VG, and
a gradient value α, iteratively ﬁxes all paths with gradient > α.

EG ← EG \ (T (v0) × T (v0))
Sample uniformly random e ∈ EG. Let e = (x1, x2).
Sample uniformly random x3 ∈ VG.
for i = 1 to 3

Pi ← VERTEXSTEEPESTPATH(G, v0, xi)

Let j ∈ arg maxj∈{1,2,3} ∇Pj(v0)
G′ ← COMPHIGHPRESSGRAPH(G, v0, ∇Pj (v0))
if EG′ = ∅,

1. while T (v0) 6= VG
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.

then v0 ← ﬁx[v0, P ]
else Let G′

for i = 1, . . . , r

i, i = 1, . . . , r be the connected components of G′.

21

vi ← FIXPATHSABOVEPRESS(G′
for x ∈ VG′

i, set v0(x) ← vi(x)

i, v0|VG′

i

, ∇Pj (v0))

if α > 0 then G ←COMPHIGHPRESSGRAPH(G, v0, α)

13.
14.
15.
16. return v0

C Experiments on WebSpam: Testing More Algorithms

For completeness, in this appendix we show how a number of algorithms perform on the web spam experiment of
Section 6. We consider the following algorithms:

• RANDWALK along in-links. For a detailed description see Zhou et al. (2007). This algorithm essentially per-
forms a Personalized PageRank random walk from each vertex x and computes a spam-value for the vertex x by
taking a weighted average of the labels of the vertices where the random walk from x terminates. Also shown in
Section 6.

• DIRECTEDLEX, with edges in the opposite directions of links. This has the effect that a link to a spam host is

evidence of spam, and a link from a normal host is evidence of normality. Also shown in Section 6.

• RANDWALK along out-links.

• DIRECTEDLEX, with edges in the directions of links. This has the effect that a link from to a spam host is

evidence of spam, and a link to a normal host is evidence of normality.

• UNDIRECTEDLEX: Lex-minimization with links treated as undirected edges.

• LAPLACIAN: l2-regression with links treated as undirected edges.

• DIRECTED 1-NEAREST NEIGHBOR: Uses shortest distance along paths following out-links. Spam-ratio is
deﬁned distance from normal hosts, divided by distance to spam hosts. Sites are ﬂagged as spam when spam-
ratio exceeds some threshold. We also tried following paths along in-links instead, but that gave much worse
results.

We use the experimental setup described in Section 6. Results are shown in Figure 4. The alternative convention
for DIRECTEDLEX orients edges in the directions of links. This takes a link from a spam host to be evidence of
spam, and a link to a normal host to be evidence of normality. This approach performs signiﬁcantly worse than our
preferred convention, as one would intuitively expect. UNDIRECTEDLEX and LAPLACIAN approaches also perform
signiﬁcantly worse. DIRECTED 1-NEAREST NEIGHBOR performs poorly, demonstrating that DIRECTEDLEX is very
different from that approach. As observed by Zhou et al. (2007), sampling based on a random walk following out-links
performs worse than following in-links. Up to 60 % recall, DIRECTEDLEX performs best, both in the regime of 5 %
labels for training and in the regime of 20 % labels for training.

22

5 % labels for training

RandWalk along in-links
DirectedLex
RandWalk along out-links
DirectedLex, opposite link direction
UndirectedLex
Laplacian
Directed 1NN

0
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Recall

20 % labels for training

RandWalk along in-links
DirectedLex
RandWalk along out-links
DirectedLex, opposite link direction
UndirectedLex
Laplacian
Directed 1NN

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

i

i

n
o
s
c
e
r
P

i

i

n
o
s
c
e
r
P

0
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Recall

Figure 4: Recall and precision in the WebSpam classiﬁcation experiment. Each data point shown was computed as an average
over 100 runs. The largest standard deviation of the mean precision across the plotted recall values was less than 1.5 %. The
algorithm of Zhou et al. (2007) appears as RANDWALK (along in-links). We also show RANDWALK along out-links. Our directed
lex-minimization algorithm appears as DIRECTEDLEX. We also show DIRECTEDLEX with link directions reversed, along with
UNDIRECTEDLEX and LAPLACIAN.

D l0-Vertex Regularization Proofs

In this appendix, we prove Theorem 7.1 and Theorem 7.2. For the purposes of proving the second theorem, we intro-
duce an alternative version of problem (3). The optimization problem here requires us to minimize l0-regularization

23

budget required to obtain an inf-minimizer with gradient below a given threshold:

min
v∈IRn
subject to

(cid:13)
(cid:13)

v(T ) − v0(T )

0

gradG[v]

(cid:13)
∞ ≤ α.
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We will also need the following graph construction.

Deﬁnition D.1 The α-pressure terminal graph of a partially-labeled graph (G, v0) is a directed unweighted graph
Gα = (T (v0),

E if and only if there is a terminal path P from s to t in G with

E) such that (s, t) ∈

b

b

∇P (v0) > α.

Note that the α-pressure terminal graph has O(n) vertices but may be dense, even when G is not.

Algorithm 13: Algorithm TERM-PRESSURE: Given a well-posed instance (G, v0) and α ≥ 0, outputs α pressure terminal
graph Gα.
Initialize Gα with vertex set Vα = T (v0) and edge set
for each terminal s ∈ T (v0)

E = ∅.

1. Compute the distances to every other terminal t by running Dijktra’s algorithm, allowing shortest paths

b

2. Use the resulting distances to check for every other terminal t if there is a terminal path P from s to t with

that run through other terminals.

∇P (v0) > α. If there is, add edge (s, t) to

E.

Lemma D.2 The α-pressure terminal graph of a voltage problem (G, v0) can be computed in O((m + n log n)n) time
using algorithm TERM-PRESSURE (Algorithm 13).

b

Proof: The correctness of the algorithm follows from the fact that Dijkstra’s algorithm will identify all shortest
distances between the terminals, and the pressure check will ensure that terminal pairs (s, t) are added to
E if and
only if they are the endpoints of a terminal path P with ∇P (v0) > α. The running time is dominated by performing
Dijkstra’s algorithm once for each terminal. A single run of Dijkstra’s algorithm takes O(m + n log n) time, and this
✷
is performed at most n times, for a total running time of O((m + n log n)n).

b

We make three observations that will turn out to be crucial for proving Theorems 7.1 and 7.2.

Observation D.3 Gα is a subgraph of Gβ for α ≥ β.

Proof: Suppose edge (s, t) appears in Gα, then for some path P

∇P (v0) > α ≥ β,

so the edge also appears in Gβ.

Observation D.4 Gα is transitively closed.

Proof: Suppose edges (s, t) and (t, r) appear in Gα. Let P(s,t), P(t,r), P(s,r) be the respective shortest paths in G
between these terminal pairs. Then

∇P(s,r)(v0) =

v0(s) − v0(r)
ℓ(P(s,r))

≥

v0(s) − v0(r)
ℓ(P(s,t)) + ℓ(P(t,r))

=

v0(s) − v0(t) + v0(t) − v0(r)
ℓ(P(s,t)) + ℓ(P(t,r))

≥ min

v0(s) − v0(t)
ℓ(P(s,t))

,

 

v0(t) − v0(r)

ℓ(P(t,r)) !

> α.

So edge (s, r) also appears in Gα. This is sufﬁcient for Gα to be transitively closed.

24

(6)

✷

(7)

✷

Observation D.5 Gα is a directed acyclic graph.

Proof: Suppose for a contradiction that a directed cycle appears in Gα. Let s and t be two vertices in this cycle. Let
P(s,t) and P(t,s) be the respective shortest paths in G between these terminal pairs. Because Gα is transitively closed,
both edges (s, t) and (t, s) must appear in Gα. But (s, t) ∈

E implies

and similarly (t, s) ∈

E implies

b
This is a contradiction.

v0(s) − v0(t) > αℓ(P(s,t)) > 0,

b

v0(t) − v0(s) > αℓ(P(t,s)) > 0.

✷

The usefulness of the α-pressure terminal graph is captured in the following lemma. We deﬁne a vertex cover of a
directed graph to be a vertex set that constitutes a vertex cover in the same graph with all edges taken to be undirected.

Lemma D.6 Given a partially-labeled graph (G, v0) and a set U ⊆ V , there exists a voltage assignment v ∈ IRn that
satisﬁes

if and only if U is a vertex cover in the α-pressure terminal graph Gα of (G, v0).
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:8)

(cid:9)

t ∈ T (v0) : v(t) 6= v0(t)

⊆ U and

gradG[v]

∞ ≤ α,

Proof: We ﬁrst show the “only if” direction. Suppose for a contradiction that there exists a voltage assignment v for
which
∞ ≤ α, but U is not a vertex cover in Gα. Let (s, t) be an edge Gα which is not covered by U . The
presence of this edge in Gα implies that there exists a terminal path P from s to t in G for which

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∇P (v0) > α.

But, by Lemma 3.5 this means there is no assignment v for G which agrees with v0 on s and t and has
α. This contradicts our assumption.

∞ ≤
(cid:13)
Now we show the “if” direction. Consider an arbitrary vertex cover U of Gα. Suppose for a contradiction that
(cid:13)
⊆ U .

t ∈ T (v0) : v(t) 6= v0(t)

gradG[v]

(cid:13)
(cid:13)

gradG[v]

there does not exist a voltage assignment v for G with
Deﬁne a partial voltage assignment vU given by

∞ ≤ α and

(cid:8)

(cid:9)

vU (t) =

v0(t)
∗

(

(cid:13)
(cid:13)

(cid:13)
(cid:13)
if t ∈ T (v0) \ U
o.w.

∞ ≤ α. By
The preceding statement is equivalent to saying that there is no v that extends vU and has
Lemma 3.5, this means there is terminal path between s, t ∈ T (vU ) with gradient strictly larger than α. But this
means an edge (s, t) is present in Gα and is not covered. This contradicts our assumption that U is a vertex cover. ✷

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We are now ready to prove Theorem 7.2.

∞

(cid:13)
(cid:13)

Proof of Theorem 7.2: We describe and prove the algorithm OUTLIER. The algorithm will reduce problem (3)
to problem (6): Suppose v∗ is an optimal assignment for problem (3).
It achieves a maximum gradient α∗ =
gradG[v∗]
. Using Dijkstra’s algorithm we compute the pairwise shortest distances between all terminals in G.
From these distances and the terminal voltages, we compute the gradient on the shortest path between each terminal
(cid:13)
pair. By Lemma 3.5, α∗ must equal one of these gradients. So we can solve problem (3) by iterating over the set of
(cid:13)
gradients between terminals and solving problem (6) for each of these O(n2) gradients. Among the assignments with
v(T ) − v0(T )

0 ≤ k, we then pick the solution that minimizes
(cid:13)
(cid:13)

In fact, we can do better. By Observation D.3, Gα is a subgraph of Gβ for α ≥ β. This means a vertex cover
(cid:13)
of Gα is also a vertex cover of Gβ, and hence the minimum vertex cover for Gβ is at least as large as the minimum
(cid:13)
vertex cover for Gα. This means we can do a binary search on the set of O(n2) terminal gradients to ﬁnd the minimum
gradient for which there exists an assignment with
0 ≤ k. This way, we only make O(log n) calls to
v(T ) − v0(T )
problem (6), in order to solve problem (3).
(cid:13)
(cid:13)

We use the following algorithm to solve problem (6).

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∞

.

25

1. Compute the α-pressure terminal graph Gα of G using the algorithm TERM-PRESSURE.
2. Compute a minimum vertex cover U of Gα using the algorithm KONIG-COVER from Theorem 7.3.
3. Deﬁne a partial voltage assignment vU given by

vU (t) =

v0(t)
∗

(

if t ∈ T (v0) \ U,
otherwise.

4. Using Algorithm 5, compute voltages v that extend vU and output v.

From Lemma D.2, it follows that step 1 computes the α-pressure terminal graph in polynomial time. From The-
orem 7.3 it follows that step 2 computes the a minimum vertex cover of the α-pressure terminal graph in polynomial
time, because our observations D.4 and D.5 establish that the graph is a TC-DAG. From Lemma D.6 and Theorem 4.6,
it follows that the output voltages solve program (6).

✷

To prove Theorem 7.1, we use the standard greedy approximation algorithm for MIN-VC (Vazirani (2001)).

Theorem D.7 2-Approximation Algorithm for Vertex Cover. The following algorithm gives a 2-approximation to
the Minimum Vertex Cover problem on a graph G = (V, E).

0. Initialize U = ∅.
1. Pick an edge (u, v) ∈ E that is not covered by U .
2. Add u and v to the set U .
3. Repeat from step 1 if there are still edges not covered by U .
4. Output U .

We are now in a position to prove Theorem 7.1

Proof of Theorem 7.1: Given an arbitrary k and a partially-labeled graph (G, v0), let α∗ be the optimum value
of program (3). Observe that by Lemma D.6, this implies that Gα∗ has a vertex cover of size k. Given the partial
assignment v0, for every vertex set U , we deﬁne

vU (t) =

v0(t)
∗

(

if t ∈ T (v0) \ U
o.w.

We claim the following algorithm APPROX-OUTLIER outputs a voltage assignment v with

gradG[v]

∞ ≤ α∗

(cid:13)
(cid:13)

(cid:13)
(cid:13)

and

v(T ) − v0(T )

(cid:13)
(cid:13)

Algorithm APPROX-OUTLIER:

0 ≤ 2k.
(cid:13)
(cid:13)

0. Initialize U = ∅.
1. Using the algorithm STEEPESTPATH (Algorithm 7), ﬁnd a steepest terminal path in G w.r.t. vU . Denote
this path P and let s and t be its terminal endpoints. If there is no terminal path with positive gradient, skip
to step 4.

2. Add s and t to the set U .
3. If |U | ≤ 2k − 2 then repeat from step 1.
4. Using the algorithm COMPINFMIN (Algorithm 5), compute voltages v that extend vU and output v.

From the stopping conditions, it is clear that |U | ≤ 2k. If in step 1 we ever ﬁnd that no terminal paths have positive
∞ = 0 ≤ α∗, by Lemma 3.5. Similarly if we ﬁnd a steepest
gradient then our v that extends vU will have
(cid:13)
(cid:13)

gradG[v]

(cid:13)
(cid:13)

26

gradG[v]

∞ ≤ α∗.

∞ ≤ α∗.
path with gradient less than α∗ w.r.t. vU , then for this U there exists v that extends vU and has
This will continue to hold when if we add vertices to U . Therefore, for the ﬁnal U , there will exist an v that extends
vU and has

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

If we never ﬁnd a steepest terminal path P with ∇P (v0) ≤ α∗, then each steepest path we ﬁnd corresponds to an
edge in Gα∗ that is not yet covered by U and our algorithm in fact implements the greedy approximation algorithm
for vertex cover described in Theorem D.7. This implies that the ﬁnal U is a vertex cover of Gα∗ of size at most 2k.
∞ ≤ α∗. This
By Lemma D.6, this implies that there exists a voltage assignment u extending vU that has
implies by Theorem 4.6 that the v we output has
(cid:13)
(cid:13)
In all cases, the v we output extends vU , so

∞ ≤ α∗.

gradG[u]

(cid:13)
(cid:13)

✷

gradG[v]
v(T ) − v0(T )
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

0 ≤ |U | ≤ 2k.
(cid:13)
(cid:13)

E Proof of Hardness of l0 regularization for l2

We will prove Theorem 7.4, by a reduction from minimum bisection. To this end, let G = (V, E) be any graph. We
will reduce the minimum bisection problem on G to our regularization problem. Let n = |V |. The graph on which we
will perform regularization will have vertex set

V ∪

V ,

V is a set of n vertices that are in 1-to-1 correspondence with V . We assume that every edge in G has weight 1.
V to the corresponding vertex in V by an edge of weight B, for some large B to be
V to each other by edges of weight B3. So, we have a complete
V to V , and the original graph G on V .

where
We now connect every vertex in
determined later. We also connect all of the vertices in
graph of weight B3 edges on
b
The input potential function will be

V , a matching of weight B edges connecting

b

b

b

v(a) =

b
0 for a ∈
1 for a ∈ V .
b

(

V , and

b

Now set k = n/2. We claim that we will be able to determine the value of the minimum bisection from the solution
to the regularization problem.

If S is the set of vertices on which v and w differ, then we know that the w is harmonic on S: for every a ∈ S,

w(a) is the weighted average of the values at its neighbors. In the following, we exploit the fact that |S| ≤ n/2.

Claim E.1 For every a ∈ S ∩

V , w(a) ≤ 2/nB2.

Proof: Let a be the vertex in S ∩
w-value equal to 0 by edges of weight B3. On the other hand, a has only one neighbor that is not in
w-value at most 1, and it is connected to that vertex by an edge of weight B. Call that vertex c. We have

V that maximizes w(a). So, a is connected to at least n/2 neighbors in

V with
V , that vertex has

b

b

b

((n − 1)B3 + B)w(a) = Bw(c) +

B3w(b)

b

b
V ,b6=a
Xb∈

= Bw(c) +

B3w(b) +

B3w(b)

b
V ∩S,b6=a
Xb∈

B3w(a)

≤ B +

b
V ∩S,b6=a
Xb∈
≤ B + (n/2 − 1)B3w(a).

b
V −S
Xb∈

Subtracting (n/2 − 1)B3w(a) from both sides gives

((n/2)B3 + B)w(a) ≤ B,

which implies the claim.

Claim E.2 For a ∈ S ∩ V , w(a) ≤ n/B.

27

✷

V . Let’s call that neighbor c. We know that w(c) ≤ 2/B2n. On the
Proof: Vertex a has exactly one neighbor in
other hand, vertex a has fewer than n − 1 neighbors in V , and each of these have w-value at most 1. Let da denote the
degree of a in G. Then,

b

So,

Let

and

bisection.

and at most

(B + da)w(a) ≤ da + B

2
B2n

.

w(a) ≤

da + 2/Bn
da + B
n + (2/Bn)
B + n

≤

≤ n/B.

|S| = k = n/2.

T = S ∩ V,

t = |T | .

(n − t)B − 4/B
b

(n − t)B + tn2/B.

We now estimate the value of the regularized objective function. To this end, we assume that

We will prove that S ⊂ V and so S = T and t = n/2.

Let δ denote the number of edges on the boundary of T in V . Once we know that t = n/2, δ is the size of a

Claim E.3 The contribution of the edges between V and

V to the objective function is at least

Proof: For the lower bound, we just count the edges between vertices in V \ T and
edges, and each of them has weight B. The endpoint in V \ T has w-value 1, and the endpoint in
most 2/nB2. So, the contribution of these edges is at least

V . There are n − t of these
V has w-value at

b

(n − t)B(1 − 2/nB2)2 ≥ (n − t)B(1 − 4/nB2) ≥ (n − t)B − 4/B.

b

For the upper bound, we observe that the difference in w-values across each of these n − t edges is at most 1, so their
total contribution is at most

Since for every vertex a ∈ T , w(a) ≤ n/B, and also every vertex b ∈
edges between T and

V is at most

t(n/B)2B = tn2/B.

b

b

V , w(b) ≤ 2/nB2, the contribution due to

We will see that this is the dominant term in the objective function. The next-most important term comes from the

edges in G.

(n − t)B.

28

✷

✷

Claim E.4 The contribution of the edges in G to the objective function is at least

and at most

δ(1 − 2n/B)

δ + (t2/2)(n/B)2

δ(1 − 2n/B) and δ.

(t2/2)(n/B)2.

Proof: Let (a, b) ∈ E. If neither a nor b is in T , then w(a) = w(b) = 1, and so this edge has no contribution. If
a ∈ T but b 6∈ T , then the difference in w-values on them is between (1 − n/B) and 1. So, the contribution of such
edges to the objective function is between

Finally, if a and b are in T , then the difference in w-values on them is at most n/B, and so the contribution of all such
edges to the objective function is at most

Claim E.5 The edges between pairs of vertices in

V contribute at most 2/B to the objective function.

Proof: As 0 ≤ w(a) ≤ 2/B2n for every a ∈

V , every edge between two vertices in

V can contribute at most

b

As there are fewer than n2/2 such edges, their total contribution to the objective function is at most

B3(2/B2n)2 = 4/Bn2.
b

b

(n2/2)(4/Bn2) = 2/B.

Lemma E.6 If n ≥ 4 and B = 2n3, the value of the objective function is at least

and at most

(n − t)B + δ − 1/2

(n − t)B + δ + 1/3.

Proof: Summing the contributions in the preceding three claims, we see that the value of the objective function is at
least

(n − t)B − 4/B + δ(1 − 2n/B) ≥ (n − t)B + δ − 4/B − 2nδ/B

≥ (n − t)B + δ − n3/B
≥ (n − t)B + δ − 1/2,

as δ ≤ (n/2)2.

Similarly, the objective function is at most

(n − t)B + tn2/B + δ + (t2/2)(n/B)2 + 2/B ≤ (n − t)B + n3/2B + δ + n4/8B2 + 2/B
≤ (n − t)B + n3/2B + δ + 1/32n2 + 1/n3
≤ (n − t)B + δ + 1/3.

Claim E.7 If n ≥ 2 and B = 2n3, then S ⊂ V .

Proof: The objective function is minimized by making t as large as possible, so t = n/2 and S ⊂ V .

29

✷

✷

✷

✷

Theorem E.8 The value of the objective function reveals the value of the minimum bisection in G.

Proof: The value of the objective function will be between

and

(n/2)B + δ − 1/2

(n/2)B + δ + 1/3.

So, the objective function will be smallest when δ is as small as possible.

✷

Theorem E.8 immediately implies Theorem 7.4.

30

5
1
0
2
 
n
u
J
 
0
3
 
 
]

G
L
.
s
c
[
 
 
2
v
0
9
2
0
0
.
5
0
5
1
:
v
i
X
r
a

Algorithms for Lipschitz Learning on Graphs ∗†

Rasmus Kyng
Yale University
rasmus.kyng@yale.edu

Anup Rao
Yale University
anup.rao@yale.edu

Sushant Sachdeva
Yale University
sachdeva@cs.yale.edu

Daniel A. Spielman
Yale University
spielman@cs.yale.edu

July 1, 2015

Abstract

We develop fast algorithms for solving regression problems on graphs where one is given the value of a function
at some vertices, and must ﬁnd its smoothest possible extension to all vertices. The extension we compute is the
absolutely minimal Lipschitz extension, and is the limit for large p of p-Laplacian regularization. We present an
algorithm that computes a minimal Lipschitz extension in expected linear time, and an algorithm that computes
an absolutely minimal Lipschitz extension in expected time eO(mn). The latter algorithm has variants that seem
to run much faster in practice. These extensions are particularly amenable to regularization: we can perform l0-
regularization on the given values in polynomial time and l1-regularization on the initial function values and on graph
edge weights in time eO(m3/2).

Our deﬁnitions and algorithms naturally extend to directed graphs.

1 Introduction

We consider a problem in which we are given a weighted undirected graph G = (V, E, ℓ) and values v0 : T → R
on a subset T of its vertices. We view the weights ℓ as indicating the lengths of edges, with shorter length indicating
greater similarity. Our goal it to assign values to every vertex v ∈ V \T so that the values assigned are as smooth as
possible across edges. A minimal Lipschitz extension of v0 is a vector v that minimizes

max
(x,y)∈E

(ℓ(x, y))−1

v(x) − v(y)

,

(cid:12)
(cid:12)

(cid:12)
(cid:12)

subject to v(x) = v0(x) for all x ∈ T . We call such a vector an inf-minimizer. Inf-minimizers are not unique. So,
among inf-minimizers we seek vectors that minimize the second-largest absolute value of ℓ(x, y)−1
v(x) − v(y)
across edges, and then the third-largest given that, and so on. We call such a vector v a lex-minimizer. It is also known
(cid:12)
as an absolutely minimal Lipschitz extension of v0.
(cid:12)
These are the limit of the solution to p-Laplacian minimization problems for large p, namely the vectors that solve

(cid:12)
(cid:12)

(1)

(2)

min
v∈Rn

v|T =v0|T X(x,y)∈E

(ℓ(x, y))−p|v(x) − v(y)|p.

The use of p = 2 was suggested in the foundational paper of Zhu et al. (2003), and is particularly nice because it can
be obtained by solving a system of linear equations in a symmetric diagonally dominant matrix, which can be done

∗This research was partially supported by AFOSR Award FA9550-12-1-0175, NSF grant CCF-1111257, a Simons Investigator Award to Daniel

Spielman, and a MacArthur Fellowship.

†Code used in this work is available at https://github.com/danspielman/YINSlex

1

very quickly (Cohen et al. (2014)). The use of larger values of p has been discussed by Alamgir and Luxburg (2011),
and by Bridle and Zhu (2013), but it is much more complicated to compute. The fastest algorithms we know for this
problem require convex programming, and then require very high accuracy to obtain the values at most vertices. By
taking the limit as p goes to inﬁnity, we recover the lex-minimizer, which we will show can be computed quickly.

The lex-minimization problem has a remarkable amount of structure. For example, in uniformly weighted graphs
the value of the lex-minimizer at every vertex not in T is equal to the average of the minimum and maximum of the
values at its neighbors. This is analogous to the property of the 2-Laplacian minimizer that the value at every vertex
not in T equals the average of the values at its neighbors.

1.1 Contributions

We ﬁrst present several important structural properties of lex-minimizers in Section 3.2. As we shall point out, some
of these were known from previous work, sometimes in restricted settings. We state them generally and prove them
for completeness. We also prove that the lex-minimizer is as stable as possible under perturbations of v0 (Section 3.1).
The structure of the lex-minimization problem has led us to develop elegant algorithms for its solution. Both the
algorithms and their analyses could be taught to undergraduates. We believe that these algorithms could be used in
place of 2-Laplacian minimization in many applications.

We present algorithms for the following problems. Throughout, m = |E| and n = |V |.

Inf-minimization: An algorithm that runs in expected time O(m + n log n) (Section 4.3).

Lex-minimization: An algorithm that runs in expected time O(n(m + n log n)) (Section 4), along with a variant that

runs quickly in practice (Section 4.4).

l1-regularization of edge lengths for inf-minimization: The problem of minimizing (1) given a limited budget with
O(m3/2)
which one can increase edge lengths is a linear programming problem. We show how to solve it in time
with an interior point method by using fast Laplacian solvers (Section 8). The same algorithm can accommodate
l1-regularization of the values given in v0.

e

l0-regularization of vertex values for inf-minimization: We give a polynomial time algorithm for l0-regularization
of the values at vertices. That is, we minimize (1) given a budget of a number of vertices that can be proclaimed
outliers and removed from T (Section 7.1). We solve this problem by reducing it to the problem of computing
minimum vertex covers on transitively closed directed acyclic graphs, a special case of minimum vertex cover
that can be solved in polynomial time.

After any regularization for inf-minimization, we suggest computing the lex-minimizer. We ﬁnd the result for l0-
regularization of vertex values to be particularly surprising, especially because we prove that the analogous problem
for 2-Laplacian minimization is NP-Hard (Section 7.2).

All of our algorithms extend naturally to directed graphs (Section 5). This is in contrast with the problem of
minimizing 2-Laplacians on directed graphs, which corresponds to computing electrical ﬂows in networks of resistors
and diodes, for which fast algorithms are not presently known.

We present a few experiments on examples demonstrating that the lex-minimizer can overcome known deﬁcien-
cies of the 2-Laplacian minimizer (Section 1.2, Figures 1,2), as well as a demonstration of the performance of the
directed analog of our algorithms on the WebSpam dataset of Castillo et al. (2006) (Section 6). In the WebSpam prob-
lem we use the link structure of a collection of web sites to ﬂag some sites as spam, given a small number of labeled
sites known to be spam or normal.

1.2 Relation to Prior Work

We ﬁrst encountered the idea of using the minimizer of the 2-Laplacian given by (2) for regression and classiﬁca-
tion on graphs in the work of Zhu et al. (2003) and Belkin et al. (2004) on semi-supervised learning. These works
transformed learning problems on sets of vectors into problems on graphs by identifying vectors with vertices and
constructing graphs with edges between nearby vectors. One shortcoming of this approach (see Nadler et al. (2009),

2

e
g
a

t
l

 

o
V
d
e
r
r
e

f

n

I

1

0.8

0.6

0.4

0.2

0

-0.2

-0.4

-0.6

-0.8

-1

-4

50 lex
50 l2
100 lex
100 l2
500 lex
500 l2
1000 lex
1000 l2

0.25

0.2

r
o
r
r
e
 
1
l
 
n
a
e
M

0.15

0.1

0.05

0
5000 

lex
2-Lap
labels

-2

0

2
Vertex position on real line

4

6

8

Figure 1: Lex vs 2-Laplacian on 1D gaussian clus-
ters.

Figure 2: kNN graphs on samples from 4D cube.

10000

20000

40000

80000

Number of Vertices

Alamgir and Luxburg (2011), Bridle and Zhu (2013)) is that if the number of vectors grows while the number of la-
beled vectors remains ﬁxed, then almost all the values of the 2-Laplacian minimizer converge to the mean of the
labels on most natural examples. For example, Nadler et al. (2009) consider sampling points from two Gaussian
distributions centered at 0 and 4 on the real line. They place edges between every pair of points (x, y) with length
exp(|x − y|2 /2σ2) for σ = 0.4, and provide only the labels v0(0) = −1 and v0(4) = 1. Figure 1 shows the values
of the 2-Laplacian minimizer in red, which are all approximately zero. In contrast, the values of the lex-minimizer in
blue, which are smoothly distributed between the labeled points, are shown.

The “manifold hypothesis” (see Chapelle et al. (2010), Ma and Fu (2011)) holds that much natural data lies near a
low-dimensional manifold and that natural functions we would like to learn on this data are smooth functions on the
manifold. Under this assumption, one should expect lex-minimizers to interpolate well. In contrast, the 2-Laplacian
minimizers degrade (dotted lines) if the number of labeled points remains ﬁxed while the total number of points grows.
In Figure 2, we demonstrate this by sampling many points uniformly from the unit cube in 4 dimensions, form their
8-nearest neighbor graph, and consider the problem of regressing the ﬁrst coordinate. We performed 8 experiments,
varying the number of labeled points in {50, 100, 500, 1000}. Each data point is the mean average l1 error over 100
experiments. The plots for root mean squared error are similar. The standard deviation of the estimations of the mean
are within one pixel, and so are not displayed. The performance of the lex-minimizer (solid lines) does not degrade as
the number of unlabeled points grows.

Analogous to our inf-minimizers, minimal Lipschitz extensions of functions in Euclidean space and over more
general metric spaces have been studied extensively in Mathematics (Kirszbraun (1934), McShane (1934), Whitney
(1934)). von Luxburg and Bousquet (2003) employ Lipschitz extensions on metric spaces for classiﬁcation and relate
these to Support Vector Machines. Their work inspired improvements in classiﬁcation and regression in metric spaces
with low doubling dimension (Gottlieb et al. (2013), Gottlieb et al. (2013b)). Theoretically fast, although not actually
practical, algorithms have been given for constructing minimal Lipschitz extensions of functions on low-dimensional
Euclidean spaces (Fefferman (2009a), Fefferman and Klartag (2009), Fefferman (2009b)). Sinop and Grady (2007)
suggest using inf-minimizers for binary classiﬁcation problems on graphs. For this special case, where all of the
given values are either 0 or 1, they present an O(m + n log n) time algorithm for computing an inf-minimizer. The
case of general given values, which we solve in this paper, is much more complicated. To compensate for the non-
uniqueness of inf-minimizers, they suggest choosing the inf-minimizer that minimizes (2) with p = 2. We believe that
the lex-minimizer is a more natural choice.

The analog of our lex-minimizer over continuous spaces is called the absolutely minimal Lipschitz extension
(AMLE). Starting with the work of Aronsson (1967), there have been several characterizations and proofs of the ex-
istence and uniqueness of the AMLE (Jensen (1993), Crandall et al. (2001), Barles and Busca (2001), Aronsson et al.
(2004)). Many of these results were later extended to general metric spaces, including graphs (Milman (1999),
Peres et al. (2011), Naor and Shefﬁeld (2010), Shefﬁeld and Smart (2010)). However, to the best of our knowledge,
fast algorithms for computing lex-minimizers on graphs were not known. For the special case of undirected, un-
weighted graphs, Lazarus et al. (1999) presented both a polynomial-time algorithm and an iterative method. Oberman

3

(2011) suggested computing the AMLE in Euclidean space by ﬁrst discretizing the problem and then solving the cor-
responding graph problem by an iterative method. However, no run-time guarantees were obtained for either iterative
method.

2 Notation and Basic Deﬁnitions

Lexicographic Ordering. Given a vector r ∈ Rm, let πr denote a permutation that sorts r in non-increasing order
by absolute value, i.e., ∀i ∈ [m − 1], |r(πr(i))| ≥ |r(πr(i + 1))|. Given two vectors r, s ∈ Rm, we write r (cid:22) s to
indicate that r is smaller than s in the lexicographic ordering on sorted absolute values, i.e.

∃j ∈ [m],

r(πr(j))

<

s(πs(j))

and ∀i ∈ [j − 1],

r(πr(i))

=

s(πs(i))

or ∀i ∈ [m],

=

r(πr(i))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.
(cid:12)
(cid:12)

s(πs(i))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

Note that it is possible that r (cid:22) s and s (cid:22) r while r 6= s. It is a total relation: for every r and s at least one of r (cid:22) s
or s (cid:22) r is true.

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Graphs and Matrices. We will work with weighted graphs. Unless explicitly stated, we will assume that they are
undirected. For a graph G, we let VG be its set of vertices, EG be its set of edges, and ℓG : EG → R+ be the
assignment of positive lengths to the edges. We let |VG| = n, and |EG| = m. We assume ℓG is symmetric, i.e.,
ℓG(x, y) = ℓG(y, x). When G is clear from the context, we drop the subscript.

A path P in G is an ordered sequence of (not necessarily distinct) vertices P = (x0, x1, . . . , xk), such that
(xi−1, xi) ∈ E for i ∈ [k]. The endpoints of P are denoted by ∂0P = x0, ∂1P = xk. The set of interior vertices
of P is deﬁned to be int(P ) = {xi : 0 < i < k}. For 0 ≤ i < j ≤ k, we use the notation P [xi : xj] to denote the
k
subpath (xi, . . . , xj). The length of P is ℓ(P ) =
i=1 ℓ(xi−1, xi).
A function v0 : V → R ∪ {∗} is called a voltage assignment (to G). A vertex x ∈ V is a terminal with
respect to v0 iff v0(x) 6= ∗. The other vertices, for which v0(x) = ∗, are non-terminals. We let T (v0) denote the
set of terminals with respect to v0. If T (v0) = V, we call v0 a complete voltage assignment (to G). We say that an
assignment v : V → R ∪ {∗} extends v0 if v(x) = v0(x) for all x such that v0(x) 6= ∗.

Given an assignment v0 : V → R ∪ {∗}, and two terminals x, y ∈ T (v0) for which (x, y) ∈ E, we deﬁne the

P

gradient on (x, y) due to v0 to be

gradG[v0](x, y) =

v0(x) − v0(y)
ℓ(x, y)

.

It may be useful to view gradG[v0](x, y) as the current in the edge (x, y) induced by voltages v0. When v0 is a
complete voltage assignment, we interpret gradG[v0] as a vector in Rm, with one entry for each edge. However, for
convenience, we deﬁne gradG[v0](x, y) = −gradG[v0](y, x). When G is clear from the context, we drop the subscript.
A graph G along with a voltage assignment v to G is called a partially-labeled graph, denoted (G, v). We say
that a partially-labeled graph (G, v0) is a well-posed instance if for every maximal connected component H of G, we
have T (v0) ∩ VH 6= ∅.

A path P in a partially-labeled graph (G, v0) is called a terminal path if both endpoints are terminals. We deﬁne

∇P (v0) to be its gradient:

∇P (v0) =

v0(∂0P ) − v0(∂1P )
ℓ(P )

.

If P contains no terminal-terminal edges (and hence, contains at least one non-terminal), it is a free terminal path.

Lex-Minimization. An instance of the LEX-MINIMIZATION problem is described by a partially-labeled graph
(G, v0). The objective is to compute a complete voltage assignment v : VG → R extending v0 that lex-minimizes
grad[v].

Deﬁnition 2.1 (Lex-minimizer) Given a partially-labeled graph (G, v0), we deﬁne lexG[v0] to be a complete voltage
assignment to V that extends v0, and such that for every other complete assignment v′ : VG → R that extends v0, we
have gradG[lexG[v0]] (cid:22) gradG[v′]. That is, lexG[v0] achieves a lexicographically-minimal gradient assignment to the
edges.

We call lexG[v0] the lex-minimizer for (G, v0). Note that if T (v0) = VG, then trivially, lexG[v0] = v0.

4

3 Basic Properties of Lex-Minimizers

Lazarus et al. (1999) established that lex-minimizers in unweighted and undirected graphs exist, are unique, and may
be computed by an elementary meta-algorithm. We state and prove these facts for undirected weighted graphs, and
defer the discussion of the directed case to Section 5. We also state for directed and weighted graphs characterizations
of lex-minimizers that were established by Peres et al. (2011), Naor and Shefﬁeld (2010) and Shefﬁeld and Smart
(2010) for unweighted graphs. These results are essential for the analyses of our algorithms. We defer most proofs to
Appendix A.

Deﬁnition 3.1 A steepest ﬁxable path in an instance (G, v0) is a free terminal path P that has the largest gradient
∇P (v0) amongst such paths.

Observe that a steepest ﬁxable path with ∇P (v0) 6= 0 must be a simple path.
Deﬁnition 3.2 Given a steepest ﬁxable path P in an instance (G, v0), we deﬁne ﬁxG[v0, P ] : VG → R ∪ {∗} to be the
voltage assignment deﬁned as follows

ﬁxG[v0, P ](x) =

v0(∂0P ) − ∇P (v0) · ℓG(P [∂0P : x]) x ∈ int(P ) \ T (v0),
v0(x)

otherwise.

(

We say that the vertices x ∈ int(P ) are ﬁxed by the operation ﬁx[v0, P ]. If we deﬁne v1 = ﬁxG[v0, P ], where
P = (x0, . . . , xr) is the steepest ﬁxable path in (G, v0), then it is easy to argue that for every i ∈ [r], we have
grad[v1](xi−1, xi) = ∇P (see Lemma A.5). The meta-algorithm META-LEX, spelled out as Algorithm 1, entails
repeatedly ﬁxing steepest ﬁxable paths. While it is possible to have multiple steepest ﬁxable paths, the result of ﬁxing
all of them does not depend on the order in which they are ﬁxed.

Theorem 3.3 Given a well-posed instance (G, v0), the meta-algorithm META-LEX, which repeatedly ﬁxes steepest
ﬁxable paths, produces the unique lex-minimizer extending v0.

Corollary 3.4 Given a well-posed instance (G, v0) such that T (v0) 6= VG, let P be a steepest ﬁxable path in (G, v0).
Then, (G, ﬁx[v0, P ]) is also a well-posed instance, and lexG[ﬁx[v0, P ]] = lexG[v0].

Since a lex-minimal element must be an inf-minimizer, we also obtain the following corollary, that can also be

proved using LP duality.

Lemma 3.5 Suppose we have a well-posed instance (G, v0). Then, there exists a complete voltage assignment v
extending v0 such that

grad[v]

∞ ≤ α, iff every terminal path P in (G, v0) satisﬁes ∇P (v0) ≤ α.
(cid:13)
(cid:13)

3.1 Stability

(cid:13)
(cid:13)

The following theorem states that lexG[v0] is monotonic with respect to v0 and it respects scaling and translation of
v0.

Theorem 3.6 Let (G, v0) be a well-posed instance with T := T (v0) as the set of terminals. Then the following
statements hold.

1. For any c, d ∈ R, v1 a partial assignment with terminals T (v1) = T and v1(t) = cv0(t) + d for all t ∈ T .

Then, lexG[v1](i) = c · lexG[v0](i) + d for all i ∈ VG.

2. v1 a partial assignment with terminals T (v1) = T. Suppose further that v1(t) ≥ v0(t) for all t ∈ T. Then,

lexG[v1](i) ≥ lexG[v0](i) for all i ∈ VG.

As a corollary, the above theorem gives a nice stability property that lex-minimal elements satisfy.

Corollary 3.7 Given well-posed instances (G, v0), (G, v1) such that T := T (v0) = T (v1), let ǫ := maxt∈T |v0(t) −
v1(t)|. Then |lexG[v0](i) − lexG[v1](i)| ≤ ǫ for all i ∈ VG.

5

3.2 Alternate Characterizations

There are at least two other seemingly disparate deﬁnitions that are equivalent to lex-minimal voltages.

lp-norm Minimizers. As mentioned in the introduction, for a well-posed instance (G, v0) the lex-minimizer is also
the limit of lp minimizers. This follows from existing results about the limit of lp-minimizers (Egger and Huotari
(1990)) in afﬁne spaces, since {grad[v] | v is complete, v extends v0} forms an afﬁne subspace of Rm. Thus, we have
the following theorem:

Theorem 3.8 (Limit of lp-minimizers, follows from Egger and Huotari (1990)) For any p ∈ (1, ∞), given a well-
posed instance (G, v0) deﬁne vp to be the unique complete voltage assignment extending v0 and minimizing
p ,
i.e.

grad[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Then, limp→∞ vp = lexG[v0].

vp = arg min
v is complete
v extends v0 (cid:13)
(cid:13)

grad[v]

p .

(cid:13)
(cid:13)

Max-Min Gradient Averaging. Consider a well-posed instance (G, v0), and a complete voltage assignment v ex-
tending v0. If G is such that ℓ(e) = 1 for all e ∈ EG, it is easy to see that lex = lexG[v0] satisﬁes the following simple
condition for all x ∈ VG \ T (v0),

lex(x) =

1
2  

max
(x,y)∈EG

lex(y) + min

lex(z)

.

(x,z)∈EG

!

This condition should be contrasted to the optimality condition for l2-regularization on these instances, which gives
for all non-terminals x, the optimal voltage v satisﬁes v(x) = 1

y:(x,y)∈EG v(y).

deg(x)

To prove the above claim, consider locally changing lex at x and observe that the gradients of edges not incident
at x remain unchanged, and at least one of edges incident at x will have a strictly larger gradient, contradicting lex-
minimality. For general graphs, this condition of local optimality can still be characterized by a simple max-min
gradient averaging property as described below.

P

Deﬁnition 3.9 (Max-Min Gradient Averaging) Given a well-posed instance (G, v0), and a complete voltage as-
signment v extending v0, we say that v satisﬁes the max-min gradient averaging property (w.r.t. (G, v0)) if for every
x ∈ VG \ T (v0), we have

grad[v](x, y) = − min

grad[v](x, y).

max
y:(x,y)∈EG

y:(x,y)∈EG

As stated in the theorem below, lexG[v0] is the unique assignment satisfying max-min gradient averaging property.
Shefﬁeld and Smart (2010) proved a variant of this statement for weighted graphs. For completeness, we present a
proof in the appendix.

Theorem 3.10 Given a well-posed instance (G, v0), lexG[v0] satisﬁes max-min gradient averaging property. More-
over, it is the unique complete voltage assignment extending v0 that satisﬁes this property w.r.t. (G, v0).

An advantage of this characterization is that it can be veriﬁed quickly. This is particularly useful for implementations
for computing the lex-minimizer.

4 Algorithms

We now sketch the ideas behind our algorithms and give precise statements of our results. A full description of all the
algorithms is included in the appendix.

We deﬁne the pressure of a vertex to be the gradient of the steepest terminal path through it:

pressure[v0](x) = max{∇P (v0) | P is a terminal path in (G, v0) and x ∈ P }.

6

Observe that in a graph with no terminal-terminal edges, a free terminal path is a steepest ﬁxable path iff its gradient
is equal to the highest pressure amongst all vertices. Moreover, vertices that lie on steepest ﬁxable paths are exactly
the vertices with the highest pressure. For a given α > 0, in order to identify vertices with pressure exceeding α, we
compute vectors vHigh[α](x) and vLow[α](x) deﬁned as follows in terms of dist, the metric on V induced by ℓ:

vLow[α](x) = min

{v0(t) + α · dist(x, t)}

t∈T (v0)

vHigh[α](x) = max
t∈T (v0)

{v0(t) − α · dist(t, x)}.

4.1 Lex-minimization on Star Graphs

We ﬁrst consider the problem of computing the lex-minimizer on a star graph in which every vertex but the center is a
terminal. This special case is a subroutine in the general algorithm, and also motivates some of our techniques.

Let x be the center vertex, T be the set of terminals, and all edges be of the form (x, t) with t ∈ T . The initial
voltage assignment is given by v : T → R, and we abbreviate dist(x, t) by d(t) = ℓ(x, t). From Corollary 3.4 we know
that we can determine the value of the lex minimizer at x by ﬁnding a steepest ﬁxable path. By deﬁnition, we need to
ﬁnd t1, t2 ∈ T that maximize the gradient of the path from t1 to t2, ∇(t1, t2) = v(t1)−v(t2)
d(t2)+d(t2) . As observed above, this
is equivalent to ﬁnding a terminal with the highest pressure. We now present a simple randomized algorithm for this
problem that runs in expected linear time.

Given a terminal t1, we can compute its pressure α along with the terminal t2 such that |∇(t1, t2)| = α in time
O(|T |) by scanning over the terminals in T . Consider doing this for a random terminal t1. We will show that in linear
time one can then ﬁnd the subset of terminals T ′ ⊂ T whose pressure is greater than α. Assuming this, we complete
the analysis of the algorithm. If T ′ = ∅, t1 is a vertex with highest pressure. Hence the path from t1 to t2 is a steepest
ﬁxable path, and we return (t1, t2). If T ′ 6= ∅, the terminal with the highest pressure must be in T ′, and we recurse by
picking a new random t1 ∈ T ′. As the size of T ′ will halve in expectation at each iteration, the expected time of the
algorithm on the star is O(|T |).

To determine which terminals have pressure exceeding α, we observe that the condition ∃t2 : α < ∇(t1, t2) =
v(t1)−v(t2)
d(t1)+d(t2) , is equivalent to ∃t2 : v(t2)+αd(t2) < v(t1)−αd(t1). This, in turn, is equivalent to vLow[α](x) < v(t1)−
αd(t1). We can compute vLow[α](x) in deterministic O(|T |) time. Similarly, we can check if ∃t2 : α < ∇(t2, t1) by
checking if vHigh[α](x) > vt1 + αd(t1). Thus, in linear time, we can compute the set T ′ of terminals with pressure
exceeding α. The above algorithm is described in Algorithm 10.

Theorem 4.1 Given a set of terminals T, initial voltages v : T → R, and distances d : T → R+, STARSTEEPESTPATH(T, v, d)
returns (t1, t2) maximizing v(t1)−v(t2)

d(t1)+d(t2) , and runs in expected time O(|T |).

4.2 Lex-minimization on General Graphs

Theorem 3.3, tells us that META-LEX will compute lex-minimizers given an algorithm for ﬁnding a steepest ﬁxable
path in (G, v0). Recall that ﬁnding a steepest ﬁxable path is equivalent to ﬁnding a path with gradient equal to the
highest pressure amongst all vertices. In this section, we show how to do this in expected time O(m + n log n).

We describe an algorithm VERTEXSTEEPESTPATH that ﬁnds a terminal path P through any vertex x such that
∇P (v0) = pressure[v0](x) in expected O(m + n log n) time. Using Dijkstra’s algorithm, we compute dist(x, t) for
all t ∈ T. If x ∈ T (v0), then there must be a terminal path P that starts at x that has ∇P (v0) = pressure[v0](x). To
compute such a P we examine all t ∈ T (v0) in O(|T |) time to ﬁnd the t that maximizes |∇(x, t)| = |v(x)−v(t)|
, and
dist(x,t)
then return a shortest path between x and that t.

If x /∈ T (v0), then the steepest path through x between terminals t1 and t2 must consist of shortest paths between
x and t1 and between x and t2. Thus, we can reduce the problem to that of ﬁnding the steepest path in a star graph
where x is the only non-terminal and is connected to each terminal t by an edge of length dist(x, t). By Theorem 4.1,
we can ﬁnd this steepest path in O(|T |) expected time. The above algorithm is formally described as Algorithm 9.

Theorem 4.2 Given a well-posed instance (G, v0), and a vertex x ∈ VG, VERTEXSTEEPESTPATH(G, v0, x) returns
a terminal path P through x such that ∇P (v0) = pressure[v0](x), in O(m + n log n) expected time.

7

As in the algorithm for the star graph, we need to identify the vertices whose pressure exceeds a given α. For a ﬁxed
α, we can compute vLow[α](x) and vHigh[α](x) for all x ∈ VG using a simple modiﬁcation of Dijkstra’s algorithm in
O(m + n log n) time. We describe the algorithms COMPVHIGH, COMPVLOW for these tasks in Algorithms 3 and 4.
The following lemma encapsulates the usefulness of vLow and vHigh.

Lemma 4.3 For every x ∈ VG, pressure[v0](x) > α iff vHigh[α](x) > vLow[α](x).

It immediately follows that the algorithm COMPHIGHPRESSGRAPH(G, v0, α) described in Algorithm 6 computes

the vertex induced subgraph on the vertex set {x ∈ VG| pressure[v0](x) > α}.

We can combine these algorithms into an algorithm STEEPESTPATH that ﬁnds the steepest ﬁxable path in (G, v0)
in O(m + n log n) expected time. We may assume that there are no terminal-terminal edges in G. We sample an edge
(x1, x2) uniformly at random from EG, and a terminal x3 uniformly at random from VG. For i = 1, 2, 3, we compute
the steepest terminal path Pi containing xi. By Theorem 4.2, this can be done in O(m + n log n) expected time. Let α
be the largest gradient maxi ∇Pi. As mentioned above, we can identify G′, the induced subgraph on vertices x with
pressure exceeding α, in O(m + n log n) time. If G′ is empty, we know that the path Pi with largest gradient is a
steepest ﬁxable path. If not, a steepest ﬁxable path in (G, v0) must be in G′, and hence we can recurse on G′. Since
we picked a uniformly random edge, and a uniformly random vertex, the expected size of G′ is at most half that of G.
Thus, we obtain an expected running time of O(m + n log n). This algorithm is described in detail in Algorithm 7.

Theorem 4.4 Given a well-posed instance (G, v0) with EG ∩ (T (v0) × T (v0)) = ∅, STEEPESTPATH(G, v0) returns
a steepest ﬁxable path in (G, v0), and runs in O(m + n log n) expected time.

By using STEEPESTPATH in META-LEX, we get the COMPLEXMIN, shown in Algorithm 1. From Theorem 3.3 and
Theorem 4.4, we immediately get the following corollary.

Corollary 4.5 Given a well-posed instance (G, v0) as input, algorithm COMPLEXMIN computes a lex-minimizing
assignment that extends v0 in O(n(m + n log n)) expected time.

4.3 Linear-time Algorithm for Inf-minimization

Given the algorithms in the previous section, it is straightforward to construct an inﬁnity minimizer. Let α⋆ be the
gradient of the steepest terminal path. From Lemma 3.5, we know that the norm of the inf minimizer is α⋆. Considering
all trivial terminal paths (terminal-terminal edges), and using STEEPESTPATH, we can compute α⋆ in randomized
O(m+n log n) time. It is well known (McShane (1934); Whitney (1934)) that v1 = vLow[α⋆] and v2 = vHigh[α⋆] are
inf-minimizers. It is also known that 1
2 (v1 + v2) is the inf-minimizer that minimizes the maximum ℓ∞-norm distance
to all inf-minimizers. In the case of path graphs, this was observed by Gaffney and Powell (1976) and independently
by Micchelli et al. (1976). For completeness, the algorithm is presented as Algorithm 5, and we have the following
result.

Theorem 4.6 Given a well-posed instance (G, v0), COMPINFMIN(G, v0) returns a complete voltage assignment v
for G extending v0 that minimizes

∞ , and runs in randomized O(m + n log n) time.

grad[v]

4.4 Faster Algorithms for Lex-minimization

(cid:13)
(cid:13)

(cid:13)
(cid:13)

The lex-minimizer has additional structure that allows one to compute it by more efﬁcient algorithms. One observation
that leads to a faster implementation is that ﬁxing a steepest ﬁxable path does not increase the pressure at vertices,
provided that one appropriately ignores terminal-terminal edges. Thus, if G(α) is a subgraph that we identiﬁed with
pressure greater than α, we can iteratively ﬁx all steepest ﬁxable paths P in G(α) with ∇P > α. Another simple
observation is that if G(α) is disconnected, we can simply recurse on each of the connected components. A complete
description of an the algorithm COMPFASTLEXMIN based on these idea is given in Algorithm 11. The algorithm
provably computes lexG(v0), and it is possible to implement it so that the space requirement is only O(m + n).
Although, we are unable to prove theoretical bounds on the running time that are better than O(n(m + n log n)),
it runs extremely quickly in practice. We used it to perform the experiments in this paper. For random regular
graphs and Delaunay graphs, with n = 0.5 × 106 vertices and around 2 million edges m ∼ 1.5 − 2 × 106, it

8

takes a couple of minutes on a 2009 MacBook Pro. Similar times are observed for other model graphs of this
size such as random regular graphs and real world networks. An implementation of this algorithm may be found
at https://github.com/danspielman/YINSlex.

5 Directed Graphs

Our deﬁnitions and algorithms, including those for regularization, extend to directed graphs with only small modiﬁ-
cations. We view directed edges as diodes and only consider potential differences in the direction of the edge. For
a complete voltage assignment v on the vertices of a directed graph G, we deﬁne the directed gradient on (x, y) due
to v to be grad+
. Given a partially-labelled directed graph (G, v0), we say that a a
complete voltage assignment v is a lex-minimizer if it extends v0 and for other complete voltage assignment v′ that
extends v0 we have grad+
G[v′]. We say that a partially-labelled directed graph (G, v0) is a well-posed
directed instance if every free vertex appears in a directed path between two terminals.

G[v](x, y) = max

G[v] (cid:22) grad+

v(x)−v(y)
ℓ(x,y)

, 0

n

o

The main difference between the directed and undirected cases is that the directed lex-minimizer is not necessarily
unique. To maintain clarity of exposition, we chose to focus on undirected graphs so far. For directed graphs, we have
the following corresponding structural results.

Theorem 5.1 Given a well-posed instance (G, v0) on a directed graph G, there exists a lex-minimizer, and the set of
all lex-minimizers is a convex set. Moreover, for every two lex-minimizers v and v′, we have grad+

G[v] = grad+

G[v′].

However, note that in the case of directed graphs, the lex-minimizer need not be unique. We still have a weaker version
of Theorem 3.3 for directed graphs.

Theorem 5.2 Given a well-posed instance (G, v0) on a directed graph G, let v1 be the partial voltage assignment
extending v0 obtained by repeatedly ﬁxing steepest ﬁxable (directed) paths P with ∇P > 0. Then, any lex-minimizer
of (G, v0) must extend v1. Moreover, for every edge e ∈ EG \ (T (V1) × T (V1)), any lex-minimizer v of (G, v0) must
satisfy grad+[v](e) = 0.

When the value of the lex-minimizer at a vertex is not uniquely determined, it is constrained to an interval. In our
experiments, we pick the convention that when the voltage at a vertex is constrained to an interval (−∞, a] or [a, ∞),
we assign a to the terminal. When it is constrained to a ﬁnite interval, we assign a voltage closest to the median of the
original voltages.

6 Experiments on WebSpam

We demonstrate the performance of our lex-minimization algorithms on directed graphs by using them to detect spam
webpages as in Zhou et al. (2007). We use the dataset webspam-uk2006-2.0 described in Castillo et al. (2006).
This collection includes 11,402 hosts, out of which 7,473 (65.5 %) are labeled, either as spam or normal. Each host
corresponds to the collection of web pages it serves. Of the hosts, 1924 are labeled spam (25.7 % of all labels). We
consider the problem of ﬂagging some hosts as spam, given only a small fraction of the labels for training. We assign
a value of 1 to the spam hosts, and a value of 0 to the normal ones. We then compute a lex minimizer and examine the
effect of ﬂagging as spam all hosts with a value greater than some threshold.

Following Zhou et al. (2007), we create edges between hosts with lengths equal to the reciprocal of the number of
links from one to the other. We run our experiments only on the largest strongly connected component of the graph,
which contains 7945 hosts of which 5552 are labeled. 16 % of the nodes in this subgraph are labeled spam. To create
training and test data, for a given value p, we select a random subset of p % of the spam labels and a random subset
of p % of the normal labels to use for training. The remaining labels are used for testing. We report results for p = 5
and p = 20.

Again following Zhou et al. (2007), we plot the precision and recall of different choices of threshold for ﬂagging
pages as spam. Recall is the fraction of spam pages our algorithm ﬂags as spam, and precision is the fraction of pages
our algorithm ﬂags as spam that actually are spam. Amongst the algorithms studied by Zhou et al. (2007), the top

9

performer was their algorithm based on sampling according to a random-walk that follows in-links from other hosts.
We compare their algorithm with the classiﬁcation we get by directing edges in the opposite directions of links. This
has the effect that a link to a spam host is evidence of spamminess, and a link from a normal host is evidence of
normality.

Results are shown in Figure 3. While we are not able to reliably ﬂag all spam hosts, we see that in the range of
10-50 % recall, we are able to ﬂag spam with precision above 82 %. We see that the performance of directed lex-
minimization does not degrade rapidly when from the “large training set” regime of p = 20, to the “small training set”
regime of p = 5.

5 % labels for training

20 % labels for training

RandWalk
DirectedLex

RandWalk
DirectedLex

1

0.9

0.8

0.7

i

i

n
o
s
c
e
r
P

0.6

0.5

0.4

0.3

0.2

0.1

0
0.1

0.2

0.3

0.4

0.7

0.8

0.9

1

0.2

0.3

0.4

0.7

0.8

0.9

1

0.6
0.5
Recall

0.6
0.5
Recall

Figure 3: Recall and precision in the web spam classiﬁcation experiment. Each data point shown was computed as an average over
100 runs. The largest standard deviation of the mean precision across the plotted recall values was less than 1.3 %. The algorithm
of Zhou et al. (2007) appears as RANDWALK. Our directed lex-minimization algorithm appears as DIRECTEDLEX.

For comparison, in Appendix C, we show the performance of our algorithm and that of Zhou et al. (2007) both
with link directions reversed, as well as the performance of undirected lex-minimization and Laplacian inference, all
of which are signiﬁcantly worse.

7 l0-Regularization of Vertex Values

We now explain how we can accommodate noise in both the given voltages and in the given lengths of edges. We can
ﬁnd the minimum number of labels to ignore, or the minimum increase in edges lengths needed so that there exists an
extension whose gradients have l∞-norm lower than a given target. After determining which labels to ignore or the
needed increment in edge lengths, we recommend computing a lex minimizer.

The algorithms we present in this section are essentially the same for directed and undirected graphs.

7.1 l0-Vertex Regularization for Inf-minimization

The l0-regularization of vertex labels can be viewed as a problem of outlier removal: the vector we compute is allowed
to disagree with v0 on up to k terminals. Given a voltage assignment v and a subset T ⊂ V of the vertices, by v(T )
we mean the vector obtained by restricting v to T . We deﬁne the l0-Vertex Regularization for l∞ problem to be

where v(T ) is the vector of values of v on the terminals T .

min
v∈IRn

gradG[v]

∞

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

0 ≤ k,
(cid:13)
(cid:13)

subject to

v(T ) − v0(T )

(3)

In Appendix D, we describe an approximation algorithm APPROX-OUTLIER that approximately solves program (3).

The precise statement we prove in Appendix D is given in the following theorem.

i

i

n
o
s
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
0.1

10

Theorem 7.1 (Approximate l0-vertex regularization) The algorithm APPROX-OUTLIER takes a positive integer k
and a partially-labeled graph (G, v0), and outputs an assignment v with
0 ≤ 2k, and
∞ ≤
α∗, where α∗ is the optimum value of program (3). The algorithm runs in time O(k(m + n log n)).
(cid:13)
(cid:13)
(cid:13)
(cid:13)

In Appendix D, we also describe an algorithm OUTLIER that exactly solves program (3) in polynomial time, and we
prove its correctness.

v(T ) − v0(T )

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Theorem 7.2 (Exact l0-vertex regularization) The algorithm OUTLIER takes a positive integer k and a partially-
labeled graph (G, v0) solves program (3) exactly. The algorithm runs in polynomial time.

We give a proof of Theorem 7.2 in Appendix D. To do this, we reduce the program (3) to the problem of minimizing
the required l0-budget needed to achieve a ﬁxed gradient α using a binary search over a set of O(n2) gradients. This
latter problem we reduce in polynomial time to Minimum Vertex Cover (VC) on a transitively closed, directed acyclic
graph (a TC-DAG). VC on a TC-DAG can be solved exactly in polynomial time by a reduction to the Maximum
Bipartite Matching Problem (Fulkerson (1956)). The problem was phrased by Fulkerson as one of ﬁnding a maximum
antichain of a ﬁnite poset. Any transitively closed DAG corresponds directly to the comparability graph of a poset. A
maximum antichain of a poset is a maximum independent set of a the comparability graph of the poset, and hence its
complement is a minimum vertex cover of the comparability graph. We refer to the algorithm developed by Fulkerson
as KONIG-COVER.

Theorem 7.3 The algorithm KONIG-COVER computes a minimum vertex cover for any transitively closed DAG G in
polynomial time.

7.2 Hardness of l0 regularization for l2

The result that l0-regularized inf-minimization can be solved exactly in polynomial time is surprising, especially
because the analogous problem for 2-Laplacian minimization turns out to be NP-Hard.

We deﬁne the the l0 vertex regularization for l2 for a partially-labeled graph (G, v0) and an integer k by

min
v∈Rn:kv(T )−v0(T )k0

≤k

vT Lv,

where L is the Laplacian of G.

Theorem 7.4 l0 vertex regularization for l2 is NP-Hard.

In Appendix E we prove Theorem 7.4 by giving a polynomial time (Karp) reduction from the NP-Hard minimum
bisection problem to l0 vertex regularization for l2.

8 l1-Edge and Vertex Regularization of Inf-minimizers

Consider a partially-labeled graph (G, v0) and an α > 0. The set of voltage assignments given by

v : v extends v0 and

gradG[v]

∞ ≤ α

n

(cid:13)
(cid:13)

(cid:13)
(cid:13)

o

is convex. Going further, let us consider the edge lengths in a graph to be speciﬁed by a vector ℓ ∈ IRE. Now the set
of voltages v and and lengths ℓ which achieve kgradG(ℓ)[v]k∞ ≤ α is jointly convex in v and ℓ. To see this, observe
that

kgradG(ℓ)[v]k∞ ≤ α ⇔ ∀(u, v) ∈ E : −αℓ(u, v) ≤ v(u) − v(v) ≤ αℓ(u, v).
Furthermore, the condition “v extends v0” is a linear constraint on v, which we express as v(T ) = v0(T ). From
the above, it is clear that the gradient condition corresponds to a convex set, as it is an intersection of half-spaces.
These half-spaces are given by O(m) linear inequalities. We can leverage this to phrase many regularized variants of
inf-minimization as convex programs, and in some cases linear programs.

(4)

11

For example, we may consider a variant of inf-minimization combined with an l1-budget for changing lengths of
edges and values on terminals. Given a parameter γ > 0 which speciﬁes the relative cost of regularizing terminals to
regularizing edges, the problem is as follows

arg min
v∈IRn,s∈IRm,s≥0

ksk1 + γ

v(T ) − v0(T )

1

subject to

gradG(ℓ+s)[v]

≤ α.

(5)

(cid:13)
(cid:13)
From our observation (4), it follows that problem (5) may be expressed as a linear program with O(n) variables
and O(m) constraints. We can use ideas from Daitch and Spielman (2008) to solve the resulting linear program in
O(m1.5) by an interior point method with a special purpose linear equation solver. The reason is that the linear
time
equations the IPM must solve at each iteration may be reduced to linear equations in symmetric, diagonally dominant
matrices, and these may be solved in nearly-linear time (Cohen et al. (2014)).

(cid:13)
(cid:13)

e

(cid:13)
(cid:13)
(cid:13)

∞

(cid:13)
(cid:13)
(cid:13)

Conclusion. We propose the use of inf and lex minimizers for regression on graphs. We present simple algorithms
for computing them that are provably fast and correct, and can also be implemented efﬁciently. We also present a
framework and polynomial time algorithms for regularization in this setting. The initial experiments reported in the
paper indicate that these algorithms give pretty good results on real and synthetic datasets. The results seem to compare
quite favorably to other algorithms, particularly in the regime of tiny labeled sets. We are testing these algorithms on
several other graph learning questions, and plan to report on them in a forthcoming experimental paper. We believe
that inf and lex minimizers, and the associated ideas presented in the paper, should be useful primitives that can be
proﬁtably combined with other approaches to learning on graphs.

We thank anonymous reviewers for helpful comments. We thank Santosh Vempala and Bartosz Walczak for pointing
out that it was already known how to compute a minimum vertex cover of a transitively closed DAG in polynomial
time.

Acknowledgements

References

Morteza Alamgir
In Advances
Information Processing
http://books.nips.cc/papers/files/nips24/NIPS2011_0278.pdf.

and Ulrike V. Luxburg.

transition
24,

in
pages

in Neural

Systems

Phase

the

family
379–387.

of
2011.

p-resistances.
URL

Gunnar Aronsson. Extension of functions satisfying lipschitz conditions. Arkiv fr Matematik, 6(6):551–561, 1967.

ISSN 0004-2080. doi: 10.1007/BF02591928. URL http://dx.doi.org/10.1007/BF02591928.

Gunnar Aronsson, Michael G. Crandall, and Petri Juutinen. A tour of the theory of absolutely minimizing functions.
ISSN 0273-0979. doi: 10.1090/S0273-0979-04-01035-3.

Bull. Amer. Math. Soc. (N.S.), 41(4):439–505, 2004.
URL http://dx.doi.org/10.1090/S0273-0979-04-01035-3.

Guy Barles and J´erˆome Busca. Existence and comparison results for fully nonlinear degenerate elliptic equations

without zeroth-order term. Comm. Partial Differential Equations, 26:2323–2337, 2001.

Mikhail Belkin, Irina Matveeva, and Partha Niyogi.

Regularization and semi-supervised learning on large
In Learning Theory, volume 3120 of Lecture Notes in Computer Science, pages 624–638.
doi: 10.1007/978-3-540-27819-1 43. URL

graphs.
Springer Berlin Heidelberg, 2004.
http://dx.doi.org/10.1007/978-3-540-27819-1_43.

ISBN 978-3-540-22282-8.

Nick Bridle and Xiaojin Zhu. p-voltages: Laplacian regularization for semi-supervised learning on high-dimensional

data. In Eleventh Workshop on Mining and Learning with Graphs (MLG2013), 2013.

12

Carlos Castillo, Debora Donato, Luca Becchetti, Paolo Boldi, Stefano Leonardi, Massimo Santini, and Sebastiano
Vigna. A reference collection for web spam. SIGIR Forum, 40(2):11–24, December 2006. ISSN 0163-5840. doi:
10.1145/1189702.1189703. URL http://doi.acm.org/10.1145/1189702.1189703.

Olivier Chapelle, Bernhard Schlkopf, and Alexander Zien. Semi-Supervised Learning. The MIT Press, 1st edition,

2010. ISBN 0262514125, 9780262514125.

Michael B Cohen, Rasmus Kyng, Gary L Miller, Jakub W Pachocki, Richard Peng, Anup B Rao, and Shen Chen Xu.
Solving SDD linear systems in nearly m log1/2 n time. In Proceedings of the 46th Annual ACM Symposium on
Theory of Computing, pages 343–352. ACM, 2014.

M.G. Crandall, L.C. Evans, and R.F. Gariepy. Optimal lipschitz extensions and the inﬁnity laplacian. Calculus of Vari-
ations and Partial Differential Equations, 13(2):123–139, 2001. ISSN 0944-2669. doi: 10.1007/s005260000065.
URL http://dx.doi.org/10.1007/s005260000065.

Samuel I. Daitch and Daniel A. Spielman. Faster approximate lossy generalized ﬂow via interior point algo-
rithms.
In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing, STOC ’08, pages
451–460, New York, NY, USA, 2008. ACM. ISBN 978-1-60558-047-0. doi: 10.1145/1374376.1374441. URL
http://doi.acm.org/10.1145/1374376.1374441.

Alan Egger and Robert Huotari. Rate of convergence of the discrete polya algorithm. Journal of Approximation
ISSN 0021-9045. doi: http://dx.doi.org/10.1016/0021-9045(90)90070-7. URL

Theory, 60(1):24 – 30, 1990.
http://www.sciencedirect.com/science/article/pii/0021904590900707.

Charles Fefferman. Whitney’s extension problems and interpolation of data.

(N.S.), 46(2):207–220, 2009a.
http://dx.doi.org/10.1090/S0273-0979-08-01240-8.

ISSN 0273-0979.

doi:

10.1090/S0273-0979-08-01240-8.

Bull. Amer. Math. Soc.
URL

Charles Fefferman. Fitting a [image] -smooth function to data, iii. Annals of Mathematics, 170(1):pp. 427–441, 2009b.

ISSN 0003486X. URL http://www.jstor.org/stable/40345469.

Charles Fefferman and Bo’az Klartag. Fitting a cm -smooth function to data i. Annals of Mathematics, 169(1):pp.

315–346, 2009. ISSN 0003486X. URL http://www.jstor.org/stable/40345445.

D. R. Fulkerson. Note on dilworths decomposition theorem for partially ordered sets. Proc. Amer. Math. Soc, 1956.

P.W. Gaffney and M.J.D. Powell. Optimal interpolation. In Numerical Analysis, volume 506 of Lecture Notes in Math-
ematics, pages 90–99. Springer Berlin Heidelberg, 1976. ISBN 978-3-540-07610-0. doi: 10.1007/BFb0080117.
URL http://dx.doi.org/10.1007/BFb0080117.

L.-A. Gottlieb, A. Kontorovich, and R. Krauthgamer. Efﬁcient classiﬁcation for metric data. CoRR, abs/1306.2547,

2013. URL http://arxiv.org/abs/1306.2547.

L.-A. Gottlieb, A. Kontorovich, and R. Krauthgamer. Efﬁcient regression in metric spaces via approximate lipschitz
extension. In Similarity-Based Pattern Recognition, volume 7953 of Lecture Notes in Computer Science, pages
43–58. Springer Berlin Heidelberg, 2013b. ISBN 978-3-642-39139-2. doi: 10.1007/978-3-642-39140-8 3. URL
http://dx.doi.org/10.1007/978-3-642-39140-8_3.

Robert Jensen. Uniqueness of lipschitz extensions: Minimizing the sup norm of the gradient. Archive for Ra-
doi: 10.1007/BF00386368. URL

ISSN 0003-9527.

tional Mechanics and Analysis, 123(1):51–74, 1993.
http://dx.doi.org/10.1007/BF00386368.

M. Kirszbraun. ber die zusammenziehende und lipschitzsche transformationen. Fundamenta Mathematicae, 22(1):

77–108, 1934. URL http://eudml.org/doc/212681.

13

Andrew J. Lazarus, Daniel E. Loeb,

James G. Propp, Walter R. Stromquist,

Combinatorial games under

man.
229 – 264,
http://www.sciencedirect.com/science/article/pii/S0899825698906765.

http://dx.doi.org/10.1006/game.1998.0676.

and Economic Behavior,

ISSN 0899-8256.

auction play.

Games

1999.

doi:

and Daniel H. Ull-
27(2):
URL

Yunqian Ma and Yun Fu. Manifold Learning Theory and Applications. CRC Press, Inc., Boca Raton, FL, USA, 1st

edition, 2011. ISBN 1439871094, 9781439871096.

E. J. McShane. Extension of range of functions. Bull. Amer. Math. Soc., 40(12):837–842, 12 1934. URL

http://projecteuclid.org/euclid.bams/1183497871.

C.A. Micchelli, T.J. Rivlin,

and S. Winograd.

merische Mathematik, 26(2):191–200, 1976.
http://dx.doi.org/10.1007/BF01395972.

The optimal
ISSN 0029-599X.

recovery of
doi:

smooth functions.
10.1007/BF01395972.

Nu-
URL

V. A. Milman.

Absolutely minimal extensions of

functions on metric spaces.

1999.

URL

http://iopscience.iop.org/1064-5616/190/6/A05/pdf/MSB_190_6_A05.pdf.

Boaz Nadler, Nathan Srebro, and Xueyuan Zhou. Statistical analysis of semi-supervised learning: The limit of inﬁnite
unlabelled data. 2009. URL http://ttic.uchicago.edu/˜nati/Publications/NSZnips09.pdf.

A. Naor and S. Shefﬁeld. Absolutely minimal Lipschitz extension of tree-valued mappings. CoRR, abs/1005.2535,

May 2010. URL http://arxiv.org/abs/1005.2535.

A. M. Oberman. Finite difference methods for the Inﬁnity Laplace and p-Laplace equations. CoRR, abs/1107.5278,

July 2011. URL http://arxiv.org/abs/1107.5278.

Yuval Peres, Oded Schramm, Scott Shefﬁeld, and DavidB. Wilson.

Tug-of-war and the inﬁnity lapla-
In Selected Works of Oded Schramm, Selected Works in Probability and Statistics, pages 595–
doi: 10.1007/978-1-4419-9675-6 18. URL

cian.
638. Springer New York, 2011.
http://dx.doi.org/10.1007/978-1-4419-9675-6_18.

ISBN 978-1-4419-9674-9.

S. Shefﬁeld and C. K. Smart. Vector-valued optimal Lipschitz extensions. CoRR, abs/1006.1741, June 2010. URL

http://arxiv.org/abs/1006.1741.

Ali Kemal Sinop and Leo Grady. A seeded image segmentation framework unifying graph cuts and random walker
which yields a new algorithm. In Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on,
pages 1–8. IEEE, 2007.

Vijay V. Vazirani. Approximation Algorithms. Springer-Verlag New York, Inc., New York, NY, USA, 2001. ISBN

3-540-65367-8.

Ulrike von Luxburg and Olivier Bousquet. Distance-based classiﬁcation with lipschitz functions.

In Learn-
ing Theory and Kernel Machines, volume 2777 of Lecture Notes in Computer Science, pages 314–328.
doi: 10.1007/978-3-540-45167-9 24. URL
Springer Berlin Heidelberg, 2003.
http://dx.doi.org/10.1007/978-3-540-45167-9_24.

ISBN 978-3-540-40720-1.

Hassler Whitney.

Analytic extensions of differentiable functions deﬁned in closed sets.

tions of
http://www.jstor.org/stable/1989708.

the American Mathematical Society, 36(1):pp. 63–89, 1934.

ISSN 00029947.

Transac-
URL

Dengyong Zhou, Christopher J. C. Burges, and Tao Tao. Transductive link spam detection.

In Proceedings
of the 3rd International Workshop on Adversarial Information Retrieval on the Web, AIRWeb ’07, pages 21–
ISBN 978-1-59593-732-2. doi: 10.1145/1244408.1244413. URL
28, New York, NY, USA, 2007. ACM.
http://doi.acm.org/10.1145/1244408.1244413.

Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic

functions. In IN ICML, pages 912–919, 2003.

14

A Basic Properties of Lex-Minimizers

A.1 Meta Algorithm

Algorithm 1: Algorithm META-LEX: Given a well-posed instance (G, v0), outputs lexG[v0].
for i = 1, 2, . . . :

1. if T (vi−1) = VG, then return vi−1.
2. E′ = EG \ (T (vi−1) × T (vi−1)), G′ := (VG, E′).
3. Let P ⋆
4. vi ← ﬁx[vi−1, P ⋆
i ].

i be a steepest ﬁxable path in (G′, vi−1). Let α⋆

i ← ∇P ⋆(vi−1).

In this subsection, we prove the results that appeared in section 2. We start with a simple observation.

Proposition A.1 Given a well-posed instance (G, v0) such that T (v0) 6= V, let P be a steepest ﬁxable path in (G, v0).
Then, ﬁx[v0, P ] extends v0, and (G, ﬁx[v0, P ]) is also a well-posed instance.

The properties we prove below do not depend on the choice of the steepest ﬁxable path.

Proposition A.2 For any well-posed instance (G, v0), with |VG| = n, META-LEX(G, v0) terminates in at most n
iterations, and outputs a complete voltage assignment v that extends v0.

Proof of Proposition A.2: By Proposition A.1, at any iteration i, vi−1 extends v0 and (G′, vi−1) is a well-posed
instance. META-LEX only outputs vi−1 iff T (vi−1) = V, which means vi−1 is a complete voltage assignment. For
any vi−1 that is not complete, for any x ∈ V \T (vi−1), we must have a free terminal path in (G′, vi−1) that contains x.
i exists in (G′, vi−1). Since P ⋆
Hence, a steepest ﬁxable path P ⋆
i ] ﬁxes the voltage
i
✷
for at least one non-terminal. Thus, META-LEX(G, v0) must complete in at most n iterations.

is a free terminal path, ﬁx[vi−1, P ⋆

For the following lemmas, consider a run of META-LEX with well-posed instance (G, v0) as input. Let vout be the
complete voltage assignment output by META-LEX. Let Ei be the set of edges E′ and Gi be the graph G′ constructed
in iteration i of META-LEX.

Lemma A.3 For every edge e ∈ Ei−1 \ Ei, we have

grad[vout](e)

≤ α⋆

i . Moreover, α⋆

i is non-increasing with i.

Proof of Lemma A.3: Let P ⋆
i = (x0, . . . , xr) be a steepest ﬁxable path in iteration i (when we deal with instance
(Gi−1, vi−1)). Consider a terminal path Pi+1 in (Gi, vi) such that {∂0Pi+1, ∂1Pi+1} ∩ (T (vi) \ T (vi−1)) 6= ∅. We
i . On the contrary, assume that ∇Pi+1(vi) > α⋆
claim that ∇Pi+1(vi) ≤ α⋆
i . Consider the case ∂0Pi+1 ∈ T (vi) \
T (vi−1), ∂1P1 ∈ T (vi−1). By the deﬁnition of vi, we must have ∂0Pi+1 = xj for some j ∈ [r − 1]. Let P ′
i+1 be the
path formed by joining paths P ⋆

i+1 is a free terminal path in (Gi−1, vi−1). We have,

i [x0 : xj] and Pi+1. P ′

(cid:12)
(cid:12)

(cid:12)
(cid:12)

vi−1(x0) − vi−1(∂1Pi+1) = (vi(x0) − vi(xj )) + (vi(∂0Pi+1) − vi(∂1Pi+1))
i · ℓ(P ′

i · ℓ(Pi+1) = α⋆

i [x0 : xj]) + α⋆

i · ℓ(P ⋆

> α⋆

i+1),

giving ∇P ′
The other cases can be handled similarly.

i+1(vi) > α⋆

i , which is a contradiction since the steepest ﬁxable path P ⋆
i

in (Gi−1, vi−1) has gradient α⋆
i .

Applying the above claim to an edge e ∈ Ei−1 \ Ei, whose gradient is ﬁxed for the ﬁrst time in iteration i, we
i . If v is the complete voltage assignment output by META-LEX, since v extends vi+1,
i , implying

i . Applying the claim to the symmetric edge, we obtain −grad[vout](e) ≤ α⋆

obtain that grad[vi+1](e) ≤ α⋆
we get grad[vout](e) ≤ α⋆
|grad[vout](e)| ≤ α⋆
i .

Consider any free terminal path Pi+1 in (Gi, vi). If Pi+1 is also a terminal path in (Gi−1, vi−1), it is a free
terminal path in (Gi−1, vi−1). In addition, since a steepest ﬁxable path P ⋆
i , we get
i
∇Pi+1(vi) = ∇Pi+1(vi−1) ≤ α⋆
i . Otherwise, we must have {∂0Pi+1, ∂1Pi+1} ∩ (T (vi) \ T (vi−1)) 6= ∅, and we can
deduce ∇Pi+1(vi) ≤ α⋆
i using the above claim. Thus, all free terminal paths Pi+1 in (Gi, vi) satisfy ∇Pi+1(vi) ≤ α⋆
i .
✷
In particular, α⋆

in (Gi−1, vi−1) has ∇P ⋆

i = α⋆

i is non-increasing with i.

i+1(vi) ≤ α⋆

i+1 = ∇P ⋆

i . Thus, α⋆

15

Lemma A.4 For any complete voltage assignment v for G that extends v0, if v 6= vout, we have grad[v] 6(cid:22) grad[vout],
and hence grad[vout] (cid:22) grad[v].

Proof of Lemma A.4: Consider any complete voltage assignment v for G that extends v0, such that v 6= vout. Thus,
there exists a unique i such that v extends vi−1 but does not extend vi. We will argue that grad[v] 6(cid:22) grad[vout], and
hence grad[vout] (cid:22) grad[v]. For every edge e ∈ E \ Ei−1 that has been ﬁxed so far, grad[v](e) = grad[vi−1](e) =
grad[vout](e), and hence we can ignore these edges.

Since v extends vi−1 but not vi, there exists an x ∈ T (vi) \ T (vi−1) such that v(x) 6= vi(x) = vout(x). Assume
i picked

i = (x0, . . . , xr) is the steepest ﬁxable path with gradient α⋆

v(x) < vi(x) (the other case is symmetric). If P ⋆
in iteration i, we must have x = xj for some j ∈ [r − 1]. Thus,

j

j

(v(xk−1) − v(xk)) = v(x0) − v(xj ) > vi(x0) − vi(xj ) = α⋆

i · ℓ(P ⋆

i [x0 : xj ]) = α⋆
i ·

ℓ(xk−1, xk).

Xk=1

Xk=1
Thus, for some k ∈ [j], we must have grad[v](xk−1, xk) > α⋆
is a path in Gi−1, we have {xk−1, xk} 6⊆
T (vi−1). This gives (xk−1, xk) ∈ (Ei−1 \ Ei). But then, from Lemma A.3, it follows that for all e ∈ (Ei−1 \ Ei), we
✷
have |grad[vout](e)| ≤ α⋆

i . Thus, we have grad[v] 6(cid:22) grad[vout].

i . Since P ∗
i

Lemma A.5 Let P = (x0, . . . , xr) be a steepest ﬁxable path such that it does not have any edges in T (v0) × T (v0)
and v1 = ﬁxG[v0, P ]. Then for every i ∈ [r], we have grad[v1](xi−1, xi) = ∇P.

Proof of Lemma A.5: Suppose this is not true and let j ∈ [r] be the minimum number such that grad[v1](xj−1, xj) 6=
∇P. By deﬁnition of v1 we would necessarily have j < r and vj ∈ T (v0). Suppose grad[v1](xj−1, xj ) < ∇P. We
would then have v1(x0) − v1(xj ) < ∇P ∗ ℓ(P [x0 : xj]). Since P does not have any edges in T (v0) × T (v0),
P1 := (xj, ..., xr) would be a free terminal path with ∇P1 > ∇P. This is a contradiction. Other cases can be ruled
out similarly.

✷

Proof of Theorem 3.3: Consider an arbitrary run of META-LEX on (G, v0). Let vout be the complete voltage
assignment output by META-LEX. Proposition A.1 implies that vout extends v0. Lemma A.4 implies that for any
complete voltage assignment v 6= vout that extends v0, we have grad[vout] (cid:22) grad[v]. Thus, vout is a lex-minimizer.
Moreover, the lemma also gives that for any such v, grad[v] 6(cid:22) grad[vout]. and hence vout is a unique lex-minimizer.
Thus, vout is the unique voltage assignment satisfying Def. 2.1, and we denote it as lexG[v0]. Since we started with an
✷
arbitrary run of META-LEX, uniqueness implies that every run of META-LEX on (G, v0) must output lexG[v0].

Proof of Lemma 3.5: Suppose we have a complete voltage assignment v extending v0, such that
For any terminal path P = (x0, . . . , xr), we get,

grad[v]

∞ ≤ α.

∇P (v0) = v0(∂0P ) − v0(∂1P ) = v(∂0P ) − v(∂1P ) =

grad[v](xi−1, xi) ≤ α ·

ℓ(xi−1, xi) = α · ℓ(P ),

(cid:13)
(cid:13)

(cid:13)
(cid:13)

r

i=1
X

giving ∇P (v0) ≤ α.

On the other hand, suppose every terminal path P in (G, v0) satisﬁes ∇P (v0) ≤ α. Consider v = lexG[v0]. We
know that v extends v0. For every edge e ∈ EG ∩ T (v0) × T (v0), e is a (trivial) terminal path in (G, v0), and hence
has satisﬁes grad[v](e) = grad[v0](e) = ∇e(v0) ≤ α. Considering the reverse edge, we also obtain −grad[v](e) ≤ α.
Thus, |grad[v](e)| ≤ α. Moreover, using Lemma A.3, we know that for edge e ∈ EG \ T (v0) × T (v0), |grad[v](e)| ≤
1 = ∇P ⋆
α⋆
1 ≤ α since P1 is a terminal path in (G, v0). Thus, for every e ∈ EG, |grad[v](e)| ≤ α, and hence
✷
grad[v]
∞ ≤ α.
(cid:13)
(cid:13)
(cid:13)
(cid:13)
A.2 Stability

In this subsection, we sketch a proof of the monotonicity of lex-minimizers and show how it implies the stability
property claimed earlier.

For any well-posed (G, v0), there could be several possible executions of META-LEX, each characterized by the

sequence of paths P ⋆

i . We can apply Theorem 3.3 to deduce the following structural result about the lex-minimizer.

r

i=1
X

16

Corollary A.6 For any well-posed instance (G, v0), consider a sequence of paths (P1, . . . , Pr) and voltage assign-
ments (v1, . . . , vr) for some positive integer r such that:

1. P ⋆

i is a steepest ﬁxable path in (Gi−1, vi−1) for i = 1, . . . , r.

2. vi = ﬁx[vi−1, P ⋆

i ] for i = 1, . . . , r.

3. T (vr) = VG.

Then, we have vr = lexG[v0].

We call such a sequence of paths and voltages to be a decomposition of lexG[v0]. Again, note that lexG[v0] can
possibly have multiple decompositions. However, any two such decompositions are consistent in the sense that they
produce the same voltage assignment.

Proof of Corollary 3.7: We ﬁrst deﬁne some operations on partial assignments which simpliﬁes the notation. Let
v0, v1 be any two partial assignments with the same set of terminals T := T (v0) = T (v1) and c, d ∈ R. By cv0 + d
we mean a partial assignment v with T (v) = T satisfying v(t) = cv0(t) + d for all t ∈ T . Also, by v0 + v1 we
mean a partial assignment v with T (v) = T satisfying v(t) = v0(t) + v1(t) for all t ∈ T. Also, we say v1 ≥ v0 if
v1(t) ≥ v0(t) for all t ∈ T .

Now we can show how Corollary 3.7 follows from Theorem 3.6. Let v := v1 − v0, and kvk∞ = ǫ, for some ǫ > 0.
Therefore, v0 + ǫ ≥ v1 ≥ v0 − ǫ. Theorem 3.6 then implies that lexG[v0] + ǫ ≥ lex[v1] ≥ lex[v0] − ǫ, hence proving
✷
the corollary.

Proof sketch of Theorem 3.6:
It is easy to see that the ﬁrst statement holds. For the second statement, we ﬁrst
observe that if there is a sequence of paths P1, ..., Pr that is simultaneously a decomposition of both lex[v0] and
lex[v1], then this is easy to see. If such a path sequence doesn’t exist, then we look at vt := v0 + t(v1 − v0). We
state here without a proof (though the proof is elementary) that we can then split the interval [0, 1] into ﬁnitely many
subintervals [a0, a1], [a1, a2], .., [ak−1, ak], with a0 = 0, ak = 1, such that for any i, there is a path sequence P1, ..., Pr
which is a decomposition of lex[vt] for all t ∈ [ai, ai+1]. We then observe that v0 = va0 ≤ va1 ≤ ...vak = v1. Since
for every ai, ai+1, there is a path sequence which is simultaneously a decomposition of both lex[vai ] and lex[vai+1 ],
we immediately get

lex[v0] = lex[va0 ] ≤ lex[va1] ≤ ... ≤ lex[vak ] = lex[v1].

✷

A.3 Alternate Characterizations

Proof of Theorem 3.10: We know that lexG[v0] extends v0. We ﬁrst prove that v = lexG[v0] satisﬁes the max-min
gradient averaging property. Assume to the contrary. Thus, there exists x ∈ VG \ T (v0) such that

max
y:(x,y)∈EG

grad[v](x, y) 6= − min

grad[v](x, y).

y:(x,y)∈EG

Assume that max(x,y)∈EG grad[v](x, y) ≥ − min(x,y)∈EG grad[v](x, y). Then, consider v′ extending v0 that is iden-
tical to v except for v′(x) = v(x) − ǫ for ǫ > 0. For ǫ small enough, we get that

and

max
y:(x,y)∈EG

grad[v′](x, y) < max

grad[v](x, y)

y:(x,y)∈EG

− min

y:(x,y)∈EG

grad[v′](x, y) < max

grad[v](x, y).

y:(x,y)∈EG

The gradient of edges not incident on the vertex x is left unchanged. This implies that grad[v]

6(cid:22) grad[v′],

contradicting the assumption that v is the lex-minimizer. (The other case is similar).

17

For the other direction. Consider a complete voltage assignment v extending v0 that satisﬁes the max-min gradient

averaging property w.r.t. (G, v0). Let

α = max

grad[v](x, y) ≥ 0

(x,y)∈EG
x∈V \T (v0)

be the maximum edge gradient, and consider any edge (x0, x1) ∈ EG such that grad[v](x1, x0) = α, with x1 ∈
V \ T (v0). If α = 0, grad[v] is identically zero, and is trivially the lex-minimal gradient assignment. Thus, both v and
lexG[v0] are constant on each connected component. Since (G, v0) is well-posed, there is at least one terminal in each
component, and hence v and lexG[v0] must be identical.

Now assume α > 0. By the max-min gradient averaging property, ∃x2 ∈ VG such that (x1, x2) ∈ EG and

grad[v](x1, x2) =

min
y:(x1,y)∈EG

grad[v](x1, y) = − max

grad[v](x1, y)

y:(x1,y)∈EG

≤ −grad[v](x1, x0) = −α.

Thus, grad[v](x2, x1) ≥ α. Since α is the maximum edge gradient, we must have grad[v](x2, x1) = α. More-
over, v(x2) > v(x1) > v(x0), thus x2 6= x0. We can inductively apply this argument at x2 until we hit a ter-
minal. Similarly, if x0 /∈ T (v0) we can extend the path in the other direction. Consequently, we obtain a path
P = (xj , . . . , x2, x1, x0, x−1, . . . , xk) with all vertices as distinct, such that xj , xk ∈ T (v0), and xi ∈ V \ T (v0)
for all i ∈ [j + 1, k − 1]. Moreover, grad[v](xi, xi−1) = α for all j < i ≤ k. Thus, P is a free terminal path with
∇P [v0] = α.

Moreover, since v is a voltage assignment extending v0 with

∞ = α, using Lemma 3.5, we know that
every terminal path P ′ in (G, v0) must satisfy ∇P ′(v0) ≤ α. Thus, P is a steepest ﬁxable path in (G, v0). Thus,
letting v1 = ﬁx[v0, P ], using Corollary 3.4, we obtain that lexG[v1] = lexG[v0]. Moreover, since α = ∇P [v0] =
grad[v](xi, xi−1) for all i ∈ (j, k], we get v1(xi) = v(xi) for all i ∈ (j, k). Thus, v extends v1.

grad[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We can iterate this argument for r iterations until T (vr) = VG, giving v = vr and vr = lexG[vr] = lexG[v0].
(Since we are ﬁxing at least one terminal at each iteration, this procedure terminates). Thus, we get v = lexG[v0]. ✷

B Description of the Algorithms

Algorithm 2: MODDIJKSTRA(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs a complete
voltage assignment v for G, and an array parent : V → V ∪ {null}.

Add x to a ﬁbonacci heap, with key(x) = +∞.
ﬁnished(x) ← false

Decrease key(x) to v0(x).
parent(x) ← null.

1. for x ∈ VG,
2.
3.
4. for x ∈ T (v0)
5.
6.
7. while heap is not empty
8.
9.
10.
11.
12.
13.
14.
15. return (v, parent)

x ← pop element with minimum key from heap
v(x) ← key(x). ﬁnished(x) ← true .
for y : (x, y) ∈ EG

if ﬁnished(y) = false

if key(y) > v(x) + α · ℓ(x, y)

Decrease key(y) to v(x) + α · ℓ(x, y).
parent(y) ← x.

Theorem B.1 For a well-posed instance (G, V0) and a gradient value α ≥ 0, let (v, parent) ← MODDIJKSTRA(G, v0, α).
Then, v is a complete voltage assignment such that, ∀x ∈ VG, v(x) = mint∈T (v0){v0(t) + αdist(x, t)}. Moreover, the
pointer array parent satisﬁes ∀x /∈ T (v0), parent(x) 6= null and v(x) = v(parent(x)) + α · ℓ(x, parent(x)).

18

Algorithm 3: Algorithm COMPVLOW(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs
vLow, a complete voltage assignment for G, and an array LParent : V → V ∪ {null}.

1. (vLow, LParent) ← MODDIJKSTRA(G, v0, α)
2. return (vLow, LParent)

Algorithm 4: Algorithm COMPVHIGH(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs
vHigh, a complete voltage assignment for G, and an array HParent : V → V ∪ {null}.

if x ∈ T (v0) then v1(x) ← −v0(x) else v1(x) ← v1(x).

1. for x ∈ VG
2.
3. (temp, HParent) ← MODDIJKSTRA(G, v1, α)
4. for x ∈ VG : vHigh(x) ← −temp(x)
5. return (vHigh, HParent)

Corollary B.2 For a well-posed instance (G, V0) and a gradient value α ≥ 0, let (vLow[α], LParent) ← COMPVLOW(G, v0, α)
and (vHigh[α], HParent) ← COMPVHIGH(G, v0, α). Then, vLow[α], vHigh[α] are complete voltage assignments for
G such that, ∀x ∈ VG,

vLow[α](x) = min

{v0(t) + α · dist(x, t)}

t∈T (v0)

vHigh[α](x) = max
t∈T (v0)

{v0(t) − α · dist(t, x)}.

Moreover, the pointer arrays LParent, HParent satisfy ∀x /∈ T (v0), LParent(x), HParent(x) 6= null and

vLow[α](x) = vLow[α](LParent(x)) + α · ℓ(x, LParent(x)),
vHigh[α](x) = vHigh[α](HParent(x)) − α · ℓ(x, HParent(x)).

Algorithm 5: Algorithm COMPINFMIN(G, v0): Given a well-posed instance (G, v0), outputs a complete voltage assignment
v for G, extending v0 that minimizes (cid:13)

(cid:13)grad[v](cid:13)

(cid:13)∞.

1. α ← max{|grad[v0](e)| | e ∈ EG ∩ (T (v0) × T (v0))}.
2. EG ← EG \ (T (v0) × T (v0))
3. P ←STEEPESTPATH(G, v0).
4. α ← max{α, ∇P (v0)}
5. (vLow, LParent) ← COMPVLOW(G, v0, α)
6. (vHigh, HParent) ← COMPVHIGH(G, v0, α)
7. for x ∈ VG
8.
9.
10.
11. return v

then v(x) ← v0(x)
else v(x) ← 1

2 · (vLow(x) + vHigh(x)).

if x ∈ T (v0)

1. (vLow, LParent) ← COMPVLOW(G, v0, α)
2. (vHigh, HParent) ← COMPVHIGH(G, v0, α)
3. VG′ ← {x ∈ VG | vHigh(x) > vLow(x) }
4. EG′ ← {(x, y) ∈ EG | x, y ∈ VG′ }.

19

Algorithm 6: Algorithm COMPHIGHPRESSGRAPH(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0,
outputs a minimal induced subgraph G′ of G where every vertex has pressure[v0](·) > α.

5. G′ ← (V ′, E′, ℓ)
6. return G′

Proof of Lemma 4.3:

is equivalent to

vHigh[α](x) > vLow[α](x)

max
t∈T (v0)

{v0(t) − α · dist(t, x)} > min

{v0(t) + α · dist(x, t)},

t∈T (v0)

which implies that there exists terminals s, t ∈ T (v0) such that

thus,

Hence,

v0(t) − α · dist(t, x) > v0(s) + α · dist(x, s)

pressure[v0](x) ≥

v0(t) − v0(s)
dist(t, x) + dist(x, s)

> α.

v0(t) − v0(s)
dist(t, x) + dist(x, s)

= pressure[v0](x) > α.

v0(t) − α · dist(t, x) > v0(s) + α · dist(x, s)

So the inequality on vHigh and vLow implies that pressure is strictly greater than α. On the other hand, if pressure[v0](x) >
α, there exists terminals s, t ∈ T (v0) such that

which implies vHigh[α](x) > vLow[α](x).

✷

Algorithm 7: Algorithm STEEPESTPATH(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs a steepest
free terminal path P in (G, v0).

P ← VERTEXSTEEPESTPATH(G, v0, xi)

1. Sample uniformly random e ∈ EG. Let e = (x1, x2).
2. Sample uniformly random x3 ∈ VG.
3. for i = 1 to 3
4.
5. Let j ∈ arg maxj∈{1,2,3} ∇Pj (v0)
6. G′ ← COMPHIGHPRESSGRAPH(G, v0, ∇Pj (v0))
7. if EG′ = ∅,
8.
9.

then return Pj
else return STEEPESTPATH(G′, v0|VG′ )

1. while T (v0) 6= VG
2.
3.
4.
5. return v0

EG ← EG \ (T (v0) × T (v0))
P ← STEEPESTPATH(G, v0)
v0 ← ﬁx[v0, P ]

Algorithm 8: Algorithm COMPLEXMIN(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs lexG[v0].

Algorithm 9: Algorithm VERTEXSTEEPESTPATH(G,v0, x): Given a well-posed instance (G, v0), and a vertex x ∈ VG,
outputs a steepest terminal path in (G, v0) through x.

1. Using Dijkstra’s algorithm, compute dist(x, t) for all t ∈ T (v0)

20

y ← arg maxy∈T (v0)
if v0(x) ≥ v0(y)

|v0(x)−v0(y)|
dist(x,y)

then return a shortest path from x to y
else return a shortest path from y to x

2. if x ∈ T (v0)
3.
4.
5.
6.
7. else
8.
9.
10.
11.

for t /∈ T (v0), d(t) ← dist(x, t)
(t1, t2) ← STARSTEEPESTPATH(T (v0), v0|T (v0), d)
Let P1 be a shortest path from t1 to x. Let P2 be a shortest path from x to t2.
P ← (P1, P2). return P.

Algorithm 10: STARSTEEPESTPATH(T, v, d): Returns the steepest path in a star graph, with a single non-terminal connected
to terminals in T, with lengths given by d, and voltages given by v.

|v(t1)−v(t)|
d(t1)+d(t)

1. Sample t1 uniformly and randomly from T
2. Compute t2 ∈ arg maxt∈T
3. α ← |v(t2)−v(t1)|
d(t1)+d(t2)
4. Compute vlow ← mint∈T (v(t) + α · d(t))
5. Tlow ← {t ∈ T | v(t) > vlow + α · d(t)}
6. Compute vhigh ← maxt∈T (v(t) − α · d(t))
7. Thigh ← {t ∈ T | v(t) < vhigh − α · d(t)}
8. T ′ ← Tlow ∪ Thigh.
9. if T ′ = ∅
10.
11.

then if v(t1) ≥ v(t2) then return (t1, t2) else return (t2, t1)
else return STARSTEEPESTPATH(T ′, v|T ′, dT ′ )

B.1 Faster Lex-minimization

Algorithm 11: Algorithm COMPFASTLEXMIN(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs
lexG[v0].

1. while T (v0) 6= VG
2.
3. return v0

v0 ← FIXPATHSABOVEPRESS(G, v0, 0)

Algorithm 12: Algorithm FIXPATHSABOVEPRESS(G, v0, α): Given a well-posed instance (G, v0), with T (v0) 6= VG, and
a gradient value α, iteratively ﬁxes all paths with gradient > α.

EG ← EG \ (T (v0) × T (v0))
Sample uniformly random e ∈ EG. Let e = (x1, x2).
Sample uniformly random x3 ∈ VG.
for i = 1 to 3

Pi ← VERTEXSTEEPESTPATH(G, v0, xi)

Let j ∈ arg maxj∈{1,2,3} ∇Pj(v0)
G′ ← COMPHIGHPRESSGRAPH(G, v0, ∇Pj (v0))
if EG′ = ∅,

1. while T (v0) 6= VG
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.

then v0 ← ﬁx[v0, P ]
else Let G′

for i = 1, . . . , r

i, i = 1, . . . , r be the connected components of G′.

21

vi ← FIXPATHSABOVEPRESS(G′
for x ∈ VG′

i, set v0(x) ← vi(x)

i, v0|VG′

i

, ∇Pj (v0))

if α > 0 then G ←COMPHIGHPRESSGRAPH(G, v0, α)

13.
14.
15.
16. return v0

C Experiments on WebSpam: Testing More Algorithms

For completeness, in this appendix we show how a number of algorithms perform on the web spam experiment of
Section 6. We consider the following algorithms:

• RANDWALK along in-links. For a detailed description see Zhou et al. (2007). This algorithm essentially per-
forms a Personalized PageRank random walk from each vertex x and computes a spam-value for the vertex x by
taking a weighted average of the labels of the vertices where the random walk from x terminates. Also shown in
Section 6.

• DIRECTEDLEX, with edges in the opposite directions of links. This has the effect that a link to a spam host is

evidence of spam, and a link from a normal host is evidence of normality. Also shown in Section 6.

• RANDWALK along out-links.

• DIRECTEDLEX, with edges in the directions of links. This has the effect that a link from to a spam host is

evidence of spam, and a link to a normal host is evidence of normality.

• UNDIRECTEDLEX: Lex-minimization with links treated as undirected edges.

• LAPLACIAN: l2-regression with links treated as undirected edges.

• DIRECTED 1-NEAREST NEIGHBOR: Uses shortest distance along paths following out-links. Spam-ratio is
deﬁned distance from normal hosts, divided by distance to spam hosts. Sites are ﬂagged as spam when spam-
ratio exceeds some threshold. We also tried following paths along in-links instead, but that gave much worse
results.

We use the experimental setup described in Section 6. Results are shown in Figure 4. The alternative convention
for DIRECTEDLEX orients edges in the directions of links. This takes a link from a spam host to be evidence of
spam, and a link to a normal host to be evidence of normality. This approach performs signiﬁcantly worse than our
preferred convention, as one would intuitively expect. UNDIRECTEDLEX and LAPLACIAN approaches also perform
signiﬁcantly worse. DIRECTED 1-NEAREST NEIGHBOR performs poorly, demonstrating that DIRECTEDLEX is very
different from that approach. As observed by Zhou et al. (2007), sampling based on a random walk following out-links
performs worse than following in-links. Up to 60 % recall, DIRECTEDLEX performs best, both in the regime of 5 %
labels for training and in the regime of 20 % labels for training.

22

5 % labels for training

RandWalk along in-links
DirectedLex
RandWalk along out-links
DirectedLex, opposite link direction
UndirectedLex
Laplacian
Directed 1NN

0
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Recall

20 % labels for training

RandWalk along in-links
DirectedLex
RandWalk along out-links
DirectedLex, opposite link direction
UndirectedLex
Laplacian
Directed 1NN

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

i

i

n
o
s
c
e
r
P

i

i

n
o
s
c
e
r
P

0
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Recall

Figure 4: Recall and precision in the WebSpam classiﬁcation experiment. Each data point shown was computed as an average
over 100 runs. The largest standard deviation of the mean precision across the plotted recall values was less than 1.5 %. The
algorithm of Zhou et al. (2007) appears as RANDWALK (along in-links). We also show RANDWALK along out-links. Our directed
lex-minimization algorithm appears as DIRECTEDLEX. We also show DIRECTEDLEX with link directions reversed, along with
UNDIRECTEDLEX and LAPLACIAN.

D l0-Vertex Regularization Proofs

In this appendix, we prove Theorem 7.1 and Theorem 7.2. For the purposes of proving the second theorem, we intro-
duce an alternative version of problem (3). The optimization problem here requires us to minimize l0-regularization

23

budget required to obtain an inf-minimizer with gradient below a given threshold:

min
v∈IRn
subject to

(cid:13)
(cid:13)

v(T ) − v0(T )

0

gradG[v]

(cid:13)
∞ ≤ α.
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We will also need the following graph construction.

Deﬁnition D.1 The α-pressure terminal graph of a partially-labeled graph (G, v0) is a directed unweighted graph
Gα = (T (v0),

E if and only if there is a terminal path P from s to t in G with

E) such that (s, t) ∈

b

b

∇P (v0) > α.

Note that the α-pressure terminal graph has O(n) vertices but may be dense, even when G is not.

Algorithm 13: Algorithm TERM-PRESSURE: Given a well-posed instance (G, v0) and α ≥ 0, outputs α pressure terminal
graph Gα.
Initialize Gα with vertex set Vα = T (v0) and edge set
for each terminal s ∈ T (v0)

E = ∅.

1. Compute the distances to every other terminal t by running Dijktra’s algorithm, allowing shortest paths

b

2. Use the resulting distances to check for every other terminal t if there is a terminal path P from s to t with

that run through other terminals.

∇P (v0) > α. If there is, add edge (s, t) to

E.

Lemma D.2 The α-pressure terminal graph of a voltage problem (G, v0) can be computed in O((m + n log n)n) time
using algorithm TERM-PRESSURE (Algorithm 13).

b

Proof: The correctness of the algorithm follows from the fact that Dijkstra’s algorithm will identify all shortest
distances between the terminals, and the pressure check will ensure that terminal pairs (s, t) are added to
E if and
only if they are the endpoints of a terminal path P with ∇P (v0) > α. The running time is dominated by performing
Dijkstra’s algorithm once for each terminal. A single run of Dijkstra’s algorithm takes O(m + n log n) time, and this
✷
is performed at most n times, for a total running time of O((m + n log n)n).

b

We make three observations that will turn out to be crucial for proving Theorems 7.1 and 7.2.

Observation D.3 Gα is a subgraph of Gβ for α ≥ β.

Proof: Suppose edge (s, t) appears in Gα, then for some path P

∇P (v0) > α ≥ β,

so the edge also appears in Gβ.

Observation D.4 Gα is transitively closed.

Proof: Suppose edges (s, t) and (t, r) appear in Gα. Let P(s,t), P(t,r), P(s,r) be the respective shortest paths in G
between these terminal pairs. Then

∇P(s,r)(v0) =

v0(s) − v0(r)
ℓ(P(s,r))

≥

v0(s) − v0(r)
ℓ(P(s,t)) + ℓ(P(t,r))

=

v0(s) − v0(t) + v0(t) − v0(r)
ℓ(P(s,t)) + ℓ(P(t,r))

≥ min

v0(s) − v0(t)
ℓ(P(s,t))

,

 

v0(t) − v0(r)

ℓ(P(t,r)) !

> α.

So edge (s, r) also appears in Gα. This is sufﬁcient for Gα to be transitively closed.

24

(6)

✷

(7)

✷

Observation D.5 Gα is a directed acyclic graph.

Proof: Suppose for a contradiction that a directed cycle appears in Gα. Let s and t be two vertices in this cycle. Let
P(s,t) and P(t,s) be the respective shortest paths in G between these terminal pairs. Because Gα is transitively closed,
both edges (s, t) and (t, s) must appear in Gα. But (s, t) ∈

E implies

and similarly (t, s) ∈

E implies

b
This is a contradiction.

v0(s) − v0(t) > αℓ(P(s,t)) > 0,

b

v0(t) − v0(s) > αℓ(P(t,s)) > 0.

✷

The usefulness of the α-pressure terminal graph is captured in the following lemma. We deﬁne a vertex cover of a
directed graph to be a vertex set that constitutes a vertex cover in the same graph with all edges taken to be undirected.

Lemma D.6 Given a partially-labeled graph (G, v0) and a set U ⊆ V , there exists a voltage assignment v ∈ IRn that
satisﬁes

if and only if U is a vertex cover in the α-pressure terminal graph Gα of (G, v0).
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:8)

(cid:9)

t ∈ T (v0) : v(t) 6= v0(t)

⊆ U and

gradG[v]

∞ ≤ α,

Proof: We ﬁrst show the “only if” direction. Suppose for a contradiction that there exists a voltage assignment v for
which
∞ ≤ α, but U is not a vertex cover in Gα. Let (s, t) be an edge Gα which is not covered by U . The
presence of this edge in Gα implies that there exists a terminal path P from s to t in G for which

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∇P (v0) > α.

But, by Lemma 3.5 this means there is no assignment v for G which agrees with v0 on s and t and has
α. This contradicts our assumption.

∞ ≤
(cid:13)
Now we show the “if” direction. Consider an arbitrary vertex cover U of Gα. Suppose for a contradiction that
(cid:13)
⊆ U .

t ∈ T (v0) : v(t) 6= v0(t)

gradG[v]

(cid:13)
(cid:13)

gradG[v]

there does not exist a voltage assignment v for G with
Deﬁne a partial voltage assignment vU given by

∞ ≤ α and

(cid:8)

(cid:9)

vU (t) =

v0(t)
∗

(

(cid:13)
(cid:13)

(cid:13)
(cid:13)
if t ∈ T (v0) \ U
o.w.

∞ ≤ α. By
The preceding statement is equivalent to saying that there is no v that extends vU and has
Lemma 3.5, this means there is terminal path between s, t ∈ T (vU ) with gradient strictly larger than α. But this
means an edge (s, t) is present in Gα and is not covered. This contradicts our assumption that U is a vertex cover. ✷

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We are now ready to prove Theorem 7.2.

∞

(cid:13)
(cid:13)

Proof of Theorem 7.2: We describe and prove the algorithm OUTLIER. The algorithm will reduce problem (3)
to problem (6): Suppose v∗ is an optimal assignment for problem (3).
It achieves a maximum gradient α∗ =
gradG[v∗]
. Using Dijkstra’s algorithm we compute the pairwise shortest distances between all terminals in G.
From these distances and the terminal voltages, we compute the gradient on the shortest path between each terminal
(cid:13)
pair. By Lemma 3.5, α∗ must equal one of these gradients. So we can solve problem (3) by iterating over the set of
(cid:13)
gradients between terminals and solving problem (6) for each of these O(n2) gradients. Among the assignments with
v(T ) − v0(T )

0 ≤ k, we then pick the solution that minimizes
(cid:13)
(cid:13)

In fact, we can do better. By Observation D.3, Gα is a subgraph of Gβ for α ≥ β. This means a vertex cover
(cid:13)
of Gα is also a vertex cover of Gβ, and hence the minimum vertex cover for Gβ is at least as large as the minimum
(cid:13)
vertex cover for Gα. This means we can do a binary search on the set of O(n2) terminal gradients to ﬁnd the minimum
gradient for which there exists an assignment with
0 ≤ k. This way, we only make O(log n) calls to
v(T ) − v0(T )
problem (6), in order to solve problem (3).
(cid:13)
(cid:13)

We use the following algorithm to solve problem (6).

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∞

.

25

1. Compute the α-pressure terminal graph Gα of G using the algorithm TERM-PRESSURE.
2. Compute a minimum vertex cover U of Gα using the algorithm KONIG-COVER from Theorem 7.3.
3. Deﬁne a partial voltage assignment vU given by

vU (t) =

v0(t)
∗

(

if t ∈ T (v0) \ U,
otherwise.

4. Using Algorithm 5, compute voltages v that extend vU and output v.

From Lemma D.2, it follows that step 1 computes the α-pressure terminal graph in polynomial time. From The-
orem 7.3 it follows that step 2 computes the a minimum vertex cover of the α-pressure terminal graph in polynomial
time, because our observations D.4 and D.5 establish that the graph is a TC-DAG. From Lemma D.6 and Theorem 4.6,
it follows that the output voltages solve program (6).

✷

To prove Theorem 7.1, we use the standard greedy approximation algorithm for MIN-VC (Vazirani (2001)).

Theorem D.7 2-Approximation Algorithm for Vertex Cover. The following algorithm gives a 2-approximation to
the Minimum Vertex Cover problem on a graph G = (V, E).

0. Initialize U = ∅.
1. Pick an edge (u, v) ∈ E that is not covered by U .
2. Add u and v to the set U .
3. Repeat from step 1 if there are still edges not covered by U .
4. Output U .

We are now in a position to prove Theorem 7.1

Proof of Theorem 7.1: Given an arbitrary k and a partially-labeled graph (G, v0), let α∗ be the optimum value
of program (3). Observe that by Lemma D.6, this implies that Gα∗ has a vertex cover of size k. Given the partial
assignment v0, for every vertex set U , we deﬁne

vU (t) =

v0(t)
∗

(

if t ∈ T (v0) \ U
o.w.

We claim the following algorithm APPROX-OUTLIER outputs a voltage assignment v with

gradG[v]

∞ ≤ α∗

(cid:13)
(cid:13)

(cid:13)
(cid:13)

and

v(T ) − v0(T )

(cid:13)
(cid:13)

Algorithm APPROX-OUTLIER:

0 ≤ 2k.
(cid:13)
(cid:13)

0. Initialize U = ∅.
1. Using the algorithm STEEPESTPATH (Algorithm 7), ﬁnd a steepest terminal path in G w.r.t. vU . Denote
this path P and let s and t be its terminal endpoints. If there is no terminal path with positive gradient, skip
to step 4.

2. Add s and t to the set U .
3. If |U | ≤ 2k − 2 then repeat from step 1.
4. Using the algorithm COMPINFMIN (Algorithm 5), compute voltages v that extend vU and output v.

From the stopping conditions, it is clear that |U | ≤ 2k. If in step 1 we ever ﬁnd that no terminal paths have positive
∞ = 0 ≤ α∗, by Lemma 3.5. Similarly if we ﬁnd a steepest
gradient then our v that extends vU will have
(cid:13)
(cid:13)

gradG[v]

(cid:13)
(cid:13)

26

gradG[v]

∞ ≤ α∗.

∞ ≤ α∗.
path with gradient less than α∗ w.r.t. vU , then for this U there exists v that extends vU and has
This will continue to hold when if we add vertices to U . Therefore, for the ﬁnal U , there will exist an v that extends
vU and has

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

If we never ﬁnd a steepest terminal path P with ∇P (v0) ≤ α∗, then each steepest path we ﬁnd corresponds to an
edge in Gα∗ that is not yet covered by U and our algorithm in fact implements the greedy approximation algorithm
for vertex cover described in Theorem D.7. This implies that the ﬁnal U is a vertex cover of Gα∗ of size at most 2k.
∞ ≤ α∗. This
By Lemma D.6, this implies that there exists a voltage assignment u extending vU that has
implies by Theorem 4.6 that the v we output has
(cid:13)
(cid:13)
In all cases, the v we output extends vU , so

∞ ≤ α∗.

gradG[u]

(cid:13)
(cid:13)

✷

gradG[v]
v(T ) − v0(T )
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

0 ≤ |U | ≤ 2k.
(cid:13)
(cid:13)

E Proof of Hardness of l0 regularization for l2

We will prove Theorem 7.4, by a reduction from minimum bisection. To this end, let G = (V, E) be any graph. We
will reduce the minimum bisection problem on G to our regularization problem. Let n = |V |. The graph on which we
will perform regularization will have vertex set

V ∪

V ,

V is a set of n vertices that are in 1-to-1 correspondence with V . We assume that every edge in G has weight 1.
V to the corresponding vertex in V by an edge of weight B, for some large B to be
V to each other by edges of weight B3. So, we have a complete
V to V , and the original graph G on V .

where
We now connect every vertex in
determined later. We also connect all of the vertices in
graph of weight B3 edges on
b
The input potential function will be

V , a matching of weight B edges connecting

b

b

b

v(a) =

b
0 for a ∈
1 for a ∈ V .
b

(

V , and

b

Now set k = n/2. We claim that we will be able to determine the value of the minimum bisection from the solution
to the regularization problem.

If S is the set of vertices on which v and w differ, then we know that the w is harmonic on S: for every a ∈ S,

w(a) is the weighted average of the values at its neighbors. In the following, we exploit the fact that |S| ≤ n/2.

Claim E.1 For every a ∈ S ∩

V , w(a) ≤ 2/nB2.

Proof: Let a be the vertex in S ∩
w-value equal to 0 by edges of weight B3. On the other hand, a has only one neighbor that is not in
w-value at most 1, and it is connected to that vertex by an edge of weight B. Call that vertex c. We have

V that maximizes w(a). So, a is connected to at least n/2 neighbors in

V with
V , that vertex has

b

b

b

((n − 1)B3 + B)w(a) = Bw(c) +

B3w(b)

b

b
V ,b6=a
Xb∈

= Bw(c) +

B3w(b) +

B3w(b)

b
V ∩S,b6=a
Xb∈

B3w(a)

≤ B +

b
V ∩S,b6=a
Xb∈
≤ B + (n/2 − 1)B3w(a).

b
V −S
Xb∈

Subtracting (n/2 − 1)B3w(a) from both sides gives

((n/2)B3 + B)w(a) ≤ B,

which implies the claim.

Claim E.2 For a ∈ S ∩ V , w(a) ≤ n/B.

27

✷

V . Let’s call that neighbor c. We know that w(c) ≤ 2/B2n. On the
Proof: Vertex a has exactly one neighbor in
other hand, vertex a has fewer than n − 1 neighbors in V , and each of these have w-value at most 1. Let da denote the
degree of a in G. Then,

b

So,

Let

and

bisection.

and at most

(B + da)w(a) ≤ da + B

2
B2n

.

w(a) ≤

da + 2/Bn
da + B
n + (2/Bn)
B + n

≤

≤ n/B.

|S| = k = n/2.

T = S ∩ V,

t = |T | .

(n − t)B − 4/B
b

(n − t)B + tn2/B.

We now estimate the value of the regularized objective function. To this end, we assume that

We will prove that S ⊂ V and so S = T and t = n/2.

Let δ denote the number of edges on the boundary of T in V . Once we know that t = n/2, δ is the size of a

Claim E.3 The contribution of the edges between V and

V to the objective function is at least

Proof: For the lower bound, we just count the edges between vertices in V \ T and
edges, and each of them has weight B. The endpoint in V \ T has w-value 1, and the endpoint in
most 2/nB2. So, the contribution of these edges is at least

V . There are n − t of these
V has w-value at

b

(n − t)B(1 − 2/nB2)2 ≥ (n − t)B(1 − 4/nB2) ≥ (n − t)B − 4/B.

b

For the upper bound, we observe that the difference in w-values across each of these n − t edges is at most 1, so their
total contribution is at most

Since for every vertex a ∈ T , w(a) ≤ n/B, and also every vertex b ∈
edges between T and

V is at most

t(n/B)2B = tn2/B.

b

b

V , w(b) ≤ 2/nB2, the contribution due to

We will see that this is the dominant term in the objective function. The next-most important term comes from the

edges in G.

(n − t)B.

28

✷

✷

Claim E.4 The contribution of the edges in G to the objective function is at least

and at most

δ(1 − 2n/B)

δ + (t2/2)(n/B)2

δ(1 − 2n/B) and δ.

(t2/2)(n/B)2.

Proof: Let (a, b) ∈ E. If neither a nor b is in T , then w(a) = w(b) = 1, and so this edge has no contribution. If
a ∈ T but b 6∈ T , then the difference in w-values on them is between (1 − n/B) and 1. So, the contribution of such
edges to the objective function is between

Finally, if a and b are in T , then the difference in w-values on them is at most n/B, and so the contribution of all such
edges to the objective function is at most

Claim E.5 The edges between pairs of vertices in

V contribute at most 2/B to the objective function.

Proof: As 0 ≤ w(a) ≤ 2/B2n for every a ∈

V , every edge between two vertices in

V can contribute at most

b

As there are fewer than n2/2 such edges, their total contribution to the objective function is at most

B3(2/B2n)2 = 4/Bn2.
b

b

(n2/2)(4/Bn2) = 2/B.

Lemma E.6 If n ≥ 4 and B = 2n3, the value of the objective function is at least

and at most

(n − t)B + δ − 1/2

(n − t)B + δ + 1/3.

Proof: Summing the contributions in the preceding three claims, we see that the value of the objective function is at
least

(n − t)B − 4/B + δ(1 − 2n/B) ≥ (n − t)B + δ − 4/B − 2nδ/B

≥ (n − t)B + δ − n3/B
≥ (n − t)B + δ − 1/2,

as δ ≤ (n/2)2.

Similarly, the objective function is at most

(n − t)B + tn2/B + δ + (t2/2)(n/B)2 + 2/B ≤ (n − t)B + n3/2B + δ + n4/8B2 + 2/B
≤ (n − t)B + n3/2B + δ + 1/32n2 + 1/n3
≤ (n − t)B + δ + 1/3.

Claim E.7 If n ≥ 2 and B = 2n3, then S ⊂ V .

Proof: The objective function is minimized by making t as large as possible, so t = n/2 and S ⊂ V .

29

✷

✷

✷

✷

Theorem E.8 The value of the objective function reveals the value of the minimum bisection in G.

Proof: The value of the objective function will be between

and

(n/2)B + δ − 1/2

(n/2)B + δ + 1/3.

So, the objective function will be smallest when δ is as small as possible.

✷

Theorem E.8 immediately implies Theorem 7.4.

30

5
1
0
2
 
n
u
J
 
0
3
 
 
]

G
L
.
s
c
[
 
 
2
v
0
9
2
0
0
.
5
0
5
1
:
v
i
X
r
a

Algorithms for Lipschitz Learning on Graphs ∗†

Rasmus Kyng
Yale University
rasmus.kyng@yale.edu

Anup Rao
Yale University
anup.rao@yale.edu

Sushant Sachdeva
Yale University
sachdeva@cs.yale.edu

Daniel A. Spielman
Yale University
spielman@cs.yale.edu

July 1, 2015

Abstract

We develop fast algorithms for solving regression problems on graphs where one is given the value of a function
at some vertices, and must ﬁnd its smoothest possible extension to all vertices. The extension we compute is the
absolutely minimal Lipschitz extension, and is the limit for large p of p-Laplacian regularization. We present an
algorithm that computes a minimal Lipschitz extension in expected linear time, and an algorithm that computes
an absolutely minimal Lipschitz extension in expected time eO(mn). The latter algorithm has variants that seem
to run much faster in practice. These extensions are particularly amenable to regularization: we can perform l0-
regularization on the given values in polynomial time and l1-regularization on the initial function values and on graph
edge weights in time eO(m3/2).

Our deﬁnitions and algorithms naturally extend to directed graphs.

1 Introduction

We consider a problem in which we are given a weighted undirected graph G = (V, E, ℓ) and values v0 : T → R
on a subset T of its vertices. We view the weights ℓ as indicating the lengths of edges, with shorter length indicating
greater similarity. Our goal it to assign values to every vertex v ∈ V \T so that the values assigned are as smooth as
possible across edges. A minimal Lipschitz extension of v0 is a vector v that minimizes

max
(x,y)∈E

(ℓ(x, y))−1

v(x) − v(y)

,

(cid:12)
(cid:12)

(cid:12)
(cid:12)

subject to v(x) = v0(x) for all x ∈ T . We call such a vector an inf-minimizer. Inf-minimizers are not unique. So,
among inf-minimizers we seek vectors that minimize the second-largest absolute value of ℓ(x, y)−1
v(x) − v(y)
across edges, and then the third-largest given that, and so on. We call such a vector v a lex-minimizer. It is also known
(cid:12)
as an absolutely minimal Lipschitz extension of v0.
(cid:12)
These are the limit of the solution to p-Laplacian minimization problems for large p, namely the vectors that solve

(cid:12)
(cid:12)

(1)

(2)

min
v∈Rn

v|T =v0|T X(x,y)∈E

(ℓ(x, y))−p|v(x) − v(y)|p.

The use of p = 2 was suggested in the foundational paper of Zhu et al. (2003), and is particularly nice because it can
be obtained by solving a system of linear equations in a symmetric diagonally dominant matrix, which can be done

∗This research was partially supported by AFOSR Award FA9550-12-1-0175, NSF grant CCF-1111257, a Simons Investigator Award to Daniel

Spielman, and a MacArthur Fellowship.

†Code used in this work is available at https://github.com/danspielman/YINSlex

1

very quickly (Cohen et al. (2014)). The use of larger values of p has been discussed by Alamgir and Luxburg (2011),
and by Bridle and Zhu (2013), but it is much more complicated to compute. The fastest algorithms we know for this
problem require convex programming, and then require very high accuracy to obtain the values at most vertices. By
taking the limit as p goes to inﬁnity, we recover the lex-minimizer, which we will show can be computed quickly.

The lex-minimization problem has a remarkable amount of structure. For example, in uniformly weighted graphs
the value of the lex-minimizer at every vertex not in T is equal to the average of the minimum and maximum of the
values at its neighbors. This is analogous to the property of the 2-Laplacian minimizer that the value at every vertex
not in T equals the average of the values at its neighbors.

1.1 Contributions

We ﬁrst present several important structural properties of lex-minimizers in Section 3.2. As we shall point out, some
of these were known from previous work, sometimes in restricted settings. We state them generally and prove them
for completeness. We also prove that the lex-minimizer is as stable as possible under perturbations of v0 (Section 3.1).
The structure of the lex-minimization problem has led us to develop elegant algorithms for its solution. Both the
algorithms and their analyses could be taught to undergraduates. We believe that these algorithms could be used in
place of 2-Laplacian minimization in many applications.

We present algorithms for the following problems. Throughout, m = |E| and n = |V |.

Inf-minimization: An algorithm that runs in expected time O(m + n log n) (Section 4.3).

Lex-minimization: An algorithm that runs in expected time O(n(m + n log n)) (Section 4), along with a variant that

runs quickly in practice (Section 4.4).

l1-regularization of edge lengths for inf-minimization: The problem of minimizing (1) given a limited budget with
O(m3/2)
which one can increase edge lengths is a linear programming problem. We show how to solve it in time
with an interior point method by using fast Laplacian solvers (Section 8). The same algorithm can accommodate
l1-regularization of the values given in v0.

e

l0-regularization of vertex values for inf-minimization: We give a polynomial time algorithm for l0-regularization
of the values at vertices. That is, we minimize (1) given a budget of a number of vertices that can be proclaimed
outliers and removed from T (Section 7.1). We solve this problem by reducing it to the problem of computing
minimum vertex covers on transitively closed directed acyclic graphs, a special case of minimum vertex cover
that can be solved in polynomial time.

After any regularization for inf-minimization, we suggest computing the lex-minimizer. We ﬁnd the result for l0-
regularization of vertex values to be particularly surprising, especially because we prove that the analogous problem
for 2-Laplacian minimization is NP-Hard (Section 7.2).

All of our algorithms extend naturally to directed graphs (Section 5). This is in contrast with the problem of
minimizing 2-Laplacians on directed graphs, which corresponds to computing electrical ﬂows in networks of resistors
and diodes, for which fast algorithms are not presently known.

We present a few experiments on examples demonstrating that the lex-minimizer can overcome known deﬁcien-
cies of the 2-Laplacian minimizer (Section 1.2, Figures 1,2), as well as a demonstration of the performance of the
directed analog of our algorithms on the WebSpam dataset of Castillo et al. (2006) (Section 6). In the WebSpam prob-
lem we use the link structure of a collection of web sites to ﬂag some sites as spam, given a small number of labeled
sites known to be spam or normal.

1.2 Relation to Prior Work

We ﬁrst encountered the idea of using the minimizer of the 2-Laplacian given by (2) for regression and classiﬁca-
tion on graphs in the work of Zhu et al. (2003) and Belkin et al. (2004) on semi-supervised learning. These works
transformed learning problems on sets of vectors into problems on graphs by identifying vectors with vertices and
constructing graphs with edges between nearby vectors. One shortcoming of this approach (see Nadler et al. (2009),

2

e
g
a

t
l

 

o
V
d
e
r
r
e

f

n

I

1

0.8

0.6

0.4

0.2

0

-0.2

-0.4

-0.6

-0.8

-1

-4

50 lex
50 l2
100 lex
100 l2
500 lex
500 l2
1000 lex
1000 l2

0.25

0.2

r
o
r
r
e
 
1
l
 
n
a
e
M

0.15

0.1

0.05

0
5000 

lex
2-Lap
labels

-2

0

2
Vertex position on real line

4

6

8

Figure 1: Lex vs 2-Laplacian on 1D gaussian clus-
ters.

Figure 2: kNN graphs on samples from 4D cube.

10000

20000

40000

80000

Number of Vertices

Alamgir and Luxburg (2011), Bridle and Zhu (2013)) is that if the number of vectors grows while the number of la-
beled vectors remains ﬁxed, then almost all the values of the 2-Laplacian minimizer converge to the mean of the
labels on most natural examples. For example, Nadler et al. (2009) consider sampling points from two Gaussian
distributions centered at 0 and 4 on the real line. They place edges between every pair of points (x, y) with length
exp(|x − y|2 /2σ2) for σ = 0.4, and provide only the labels v0(0) = −1 and v0(4) = 1. Figure 1 shows the values
of the 2-Laplacian minimizer in red, which are all approximately zero. In contrast, the values of the lex-minimizer in
blue, which are smoothly distributed between the labeled points, are shown.

The “manifold hypothesis” (see Chapelle et al. (2010), Ma and Fu (2011)) holds that much natural data lies near a
low-dimensional manifold and that natural functions we would like to learn on this data are smooth functions on the
manifold. Under this assumption, one should expect lex-minimizers to interpolate well. In contrast, the 2-Laplacian
minimizers degrade (dotted lines) if the number of labeled points remains ﬁxed while the total number of points grows.
In Figure 2, we demonstrate this by sampling many points uniformly from the unit cube in 4 dimensions, form their
8-nearest neighbor graph, and consider the problem of regressing the ﬁrst coordinate. We performed 8 experiments,
varying the number of labeled points in {50, 100, 500, 1000}. Each data point is the mean average l1 error over 100
experiments. The plots for root mean squared error are similar. The standard deviation of the estimations of the mean
are within one pixel, and so are not displayed. The performance of the lex-minimizer (solid lines) does not degrade as
the number of unlabeled points grows.

Analogous to our inf-minimizers, minimal Lipschitz extensions of functions in Euclidean space and over more
general metric spaces have been studied extensively in Mathematics (Kirszbraun (1934), McShane (1934), Whitney
(1934)). von Luxburg and Bousquet (2003) employ Lipschitz extensions on metric spaces for classiﬁcation and relate
these to Support Vector Machines. Their work inspired improvements in classiﬁcation and regression in metric spaces
with low doubling dimension (Gottlieb et al. (2013), Gottlieb et al. (2013b)). Theoretically fast, although not actually
practical, algorithms have been given for constructing minimal Lipschitz extensions of functions on low-dimensional
Euclidean spaces (Fefferman (2009a), Fefferman and Klartag (2009), Fefferman (2009b)). Sinop and Grady (2007)
suggest using inf-minimizers for binary classiﬁcation problems on graphs. For this special case, where all of the
given values are either 0 or 1, they present an O(m + n log n) time algorithm for computing an inf-minimizer. The
case of general given values, which we solve in this paper, is much more complicated. To compensate for the non-
uniqueness of inf-minimizers, they suggest choosing the inf-minimizer that minimizes (2) with p = 2. We believe that
the lex-minimizer is a more natural choice.

The analog of our lex-minimizer over continuous spaces is called the absolutely minimal Lipschitz extension
(AMLE). Starting with the work of Aronsson (1967), there have been several characterizations and proofs of the ex-
istence and uniqueness of the AMLE (Jensen (1993), Crandall et al. (2001), Barles and Busca (2001), Aronsson et al.
(2004)). Many of these results were later extended to general metric spaces, including graphs (Milman (1999),
Peres et al. (2011), Naor and Shefﬁeld (2010), Shefﬁeld and Smart (2010)). However, to the best of our knowledge,
fast algorithms for computing lex-minimizers on graphs were not known. For the special case of undirected, un-
weighted graphs, Lazarus et al. (1999) presented both a polynomial-time algorithm and an iterative method. Oberman

3

(2011) suggested computing the AMLE in Euclidean space by ﬁrst discretizing the problem and then solving the cor-
responding graph problem by an iterative method. However, no run-time guarantees were obtained for either iterative
method.

2 Notation and Basic Deﬁnitions

Lexicographic Ordering. Given a vector r ∈ Rm, let πr denote a permutation that sorts r in non-increasing order
by absolute value, i.e., ∀i ∈ [m − 1], |r(πr(i))| ≥ |r(πr(i + 1))|. Given two vectors r, s ∈ Rm, we write r (cid:22) s to
indicate that r is smaller than s in the lexicographic ordering on sorted absolute values, i.e.

∃j ∈ [m],

r(πr(j))

<

s(πs(j))

and ∀i ∈ [j − 1],

r(πr(i))

=

s(πs(i))

or ∀i ∈ [m],

=

r(πr(i))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.
(cid:12)
(cid:12)

s(πs(i))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

Note that it is possible that r (cid:22) s and s (cid:22) r while r 6= s. It is a total relation: for every r and s at least one of r (cid:22) s
or s (cid:22) r is true.

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Graphs and Matrices. We will work with weighted graphs. Unless explicitly stated, we will assume that they are
undirected. For a graph G, we let VG be its set of vertices, EG be its set of edges, and ℓG : EG → R+ be the
assignment of positive lengths to the edges. We let |VG| = n, and |EG| = m. We assume ℓG is symmetric, i.e.,
ℓG(x, y) = ℓG(y, x). When G is clear from the context, we drop the subscript.

A path P in G is an ordered sequence of (not necessarily distinct) vertices P = (x0, x1, . . . , xk), such that
(xi−1, xi) ∈ E for i ∈ [k]. The endpoints of P are denoted by ∂0P = x0, ∂1P = xk. The set of interior vertices
of P is deﬁned to be int(P ) = {xi : 0 < i < k}. For 0 ≤ i < j ≤ k, we use the notation P [xi : xj] to denote the
k
subpath (xi, . . . , xj). The length of P is ℓ(P ) =
i=1 ℓ(xi−1, xi).
A function v0 : V → R ∪ {∗} is called a voltage assignment (to G). A vertex x ∈ V is a terminal with
respect to v0 iff v0(x) 6= ∗. The other vertices, for which v0(x) = ∗, are non-terminals. We let T (v0) denote the
set of terminals with respect to v0. If T (v0) = V, we call v0 a complete voltage assignment (to G). We say that an
assignment v : V → R ∪ {∗} extends v0 if v(x) = v0(x) for all x such that v0(x) 6= ∗.

Given an assignment v0 : V → R ∪ {∗}, and two terminals x, y ∈ T (v0) for which (x, y) ∈ E, we deﬁne the

P

gradient on (x, y) due to v0 to be

gradG[v0](x, y) =

v0(x) − v0(y)
ℓ(x, y)

.

It may be useful to view gradG[v0](x, y) as the current in the edge (x, y) induced by voltages v0. When v0 is a
complete voltage assignment, we interpret gradG[v0] as a vector in Rm, with one entry for each edge. However, for
convenience, we deﬁne gradG[v0](x, y) = −gradG[v0](y, x). When G is clear from the context, we drop the subscript.
A graph G along with a voltage assignment v to G is called a partially-labeled graph, denoted (G, v). We say
that a partially-labeled graph (G, v0) is a well-posed instance if for every maximal connected component H of G, we
have T (v0) ∩ VH 6= ∅.

A path P in a partially-labeled graph (G, v0) is called a terminal path if both endpoints are terminals. We deﬁne

∇P (v0) to be its gradient:

∇P (v0) =

v0(∂0P ) − v0(∂1P )
ℓ(P )

.

If P contains no terminal-terminal edges (and hence, contains at least one non-terminal), it is a free terminal path.

Lex-Minimization. An instance of the LEX-MINIMIZATION problem is described by a partially-labeled graph
(G, v0). The objective is to compute a complete voltage assignment v : VG → R extending v0 that lex-minimizes
grad[v].

Deﬁnition 2.1 (Lex-minimizer) Given a partially-labeled graph (G, v0), we deﬁne lexG[v0] to be a complete voltage
assignment to V that extends v0, and such that for every other complete assignment v′ : VG → R that extends v0, we
have gradG[lexG[v0]] (cid:22) gradG[v′]. That is, lexG[v0] achieves a lexicographically-minimal gradient assignment to the
edges.

We call lexG[v0] the lex-minimizer for (G, v0). Note that if T (v0) = VG, then trivially, lexG[v0] = v0.

4

3 Basic Properties of Lex-Minimizers

Lazarus et al. (1999) established that lex-minimizers in unweighted and undirected graphs exist, are unique, and may
be computed by an elementary meta-algorithm. We state and prove these facts for undirected weighted graphs, and
defer the discussion of the directed case to Section 5. We also state for directed and weighted graphs characterizations
of lex-minimizers that were established by Peres et al. (2011), Naor and Shefﬁeld (2010) and Shefﬁeld and Smart
(2010) for unweighted graphs. These results are essential for the analyses of our algorithms. We defer most proofs to
Appendix A.

Deﬁnition 3.1 A steepest ﬁxable path in an instance (G, v0) is a free terminal path P that has the largest gradient
∇P (v0) amongst such paths.

Observe that a steepest ﬁxable path with ∇P (v0) 6= 0 must be a simple path.
Deﬁnition 3.2 Given a steepest ﬁxable path P in an instance (G, v0), we deﬁne ﬁxG[v0, P ] : VG → R ∪ {∗} to be the
voltage assignment deﬁned as follows

ﬁxG[v0, P ](x) =

v0(∂0P ) − ∇P (v0) · ℓG(P [∂0P : x]) x ∈ int(P ) \ T (v0),
v0(x)

otherwise.

(

We say that the vertices x ∈ int(P ) are ﬁxed by the operation ﬁx[v0, P ]. If we deﬁne v1 = ﬁxG[v0, P ], where
P = (x0, . . . , xr) is the steepest ﬁxable path in (G, v0), then it is easy to argue that for every i ∈ [r], we have
grad[v1](xi−1, xi) = ∇P (see Lemma A.5). The meta-algorithm META-LEX, spelled out as Algorithm 1, entails
repeatedly ﬁxing steepest ﬁxable paths. While it is possible to have multiple steepest ﬁxable paths, the result of ﬁxing
all of them does not depend on the order in which they are ﬁxed.

Theorem 3.3 Given a well-posed instance (G, v0), the meta-algorithm META-LEX, which repeatedly ﬁxes steepest
ﬁxable paths, produces the unique lex-minimizer extending v0.

Corollary 3.4 Given a well-posed instance (G, v0) such that T (v0) 6= VG, let P be a steepest ﬁxable path in (G, v0).
Then, (G, ﬁx[v0, P ]) is also a well-posed instance, and lexG[ﬁx[v0, P ]] = lexG[v0].

Since a lex-minimal element must be an inf-minimizer, we also obtain the following corollary, that can also be

proved using LP duality.

Lemma 3.5 Suppose we have a well-posed instance (G, v0). Then, there exists a complete voltage assignment v
extending v0 such that

grad[v]

∞ ≤ α, iff every terminal path P in (G, v0) satisﬁes ∇P (v0) ≤ α.
(cid:13)
(cid:13)

3.1 Stability

(cid:13)
(cid:13)

The following theorem states that lexG[v0] is monotonic with respect to v0 and it respects scaling and translation of
v0.

Theorem 3.6 Let (G, v0) be a well-posed instance with T := T (v0) as the set of terminals. Then the following
statements hold.

1. For any c, d ∈ R, v1 a partial assignment with terminals T (v1) = T and v1(t) = cv0(t) + d for all t ∈ T .

Then, lexG[v1](i) = c · lexG[v0](i) + d for all i ∈ VG.

2. v1 a partial assignment with terminals T (v1) = T. Suppose further that v1(t) ≥ v0(t) for all t ∈ T. Then,

lexG[v1](i) ≥ lexG[v0](i) for all i ∈ VG.

As a corollary, the above theorem gives a nice stability property that lex-minimal elements satisfy.

Corollary 3.7 Given well-posed instances (G, v0), (G, v1) such that T := T (v0) = T (v1), let ǫ := maxt∈T |v0(t) −
v1(t)|. Then |lexG[v0](i) − lexG[v1](i)| ≤ ǫ for all i ∈ VG.

5

3.2 Alternate Characterizations

There are at least two other seemingly disparate deﬁnitions that are equivalent to lex-minimal voltages.

lp-norm Minimizers. As mentioned in the introduction, for a well-posed instance (G, v0) the lex-minimizer is also
the limit of lp minimizers. This follows from existing results about the limit of lp-minimizers (Egger and Huotari
(1990)) in afﬁne spaces, since {grad[v] | v is complete, v extends v0} forms an afﬁne subspace of Rm. Thus, we have
the following theorem:

Theorem 3.8 (Limit of lp-minimizers, follows from Egger and Huotari (1990)) For any p ∈ (1, ∞), given a well-
posed instance (G, v0) deﬁne vp to be the unique complete voltage assignment extending v0 and minimizing
p ,
i.e.

grad[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Then, limp→∞ vp = lexG[v0].

vp = arg min
v is complete
v extends v0 (cid:13)
(cid:13)

grad[v]

p .

(cid:13)
(cid:13)

Max-Min Gradient Averaging. Consider a well-posed instance (G, v0), and a complete voltage assignment v ex-
tending v0. If G is such that ℓ(e) = 1 for all e ∈ EG, it is easy to see that lex = lexG[v0] satisﬁes the following simple
condition for all x ∈ VG \ T (v0),

lex(x) =

1
2  

max
(x,y)∈EG

lex(y) + min

lex(z)

.

(x,z)∈EG

!

This condition should be contrasted to the optimality condition for l2-regularization on these instances, which gives
for all non-terminals x, the optimal voltage v satisﬁes v(x) = 1

y:(x,y)∈EG v(y).

deg(x)

To prove the above claim, consider locally changing lex at x and observe that the gradients of edges not incident
at x remain unchanged, and at least one of edges incident at x will have a strictly larger gradient, contradicting lex-
minimality. For general graphs, this condition of local optimality can still be characterized by a simple max-min
gradient averaging property as described below.

P

Deﬁnition 3.9 (Max-Min Gradient Averaging) Given a well-posed instance (G, v0), and a complete voltage as-
signment v extending v0, we say that v satisﬁes the max-min gradient averaging property (w.r.t. (G, v0)) if for every
x ∈ VG \ T (v0), we have

grad[v](x, y) = − min

grad[v](x, y).

max
y:(x,y)∈EG

y:(x,y)∈EG

As stated in the theorem below, lexG[v0] is the unique assignment satisfying max-min gradient averaging property.
Shefﬁeld and Smart (2010) proved a variant of this statement for weighted graphs. For completeness, we present a
proof in the appendix.

Theorem 3.10 Given a well-posed instance (G, v0), lexG[v0] satisﬁes max-min gradient averaging property. More-
over, it is the unique complete voltage assignment extending v0 that satisﬁes this property w.r.t. (G, v0).

An advantage of this characterization is that it can be veriﬁed quickly. This is particularly useful for implementations
for computing the lex-minimizer.

4 Algorithms

We now sketch the ideas behind our algorithms and give precise statements of our results. A full description of all the
algorithms is included in the appendix.

We deﬁne the pressure of a vertex to be the gradient of the steepest terminal path through it:

pressure[v0](x) = max{∇P (v0) | P is a terminal path in (G, v0) and x ∈ P }.

6

Observe that in a graph with no terminal-terminal edges, a free terminal path is a steepest ﬁxable path iff its gradient
is equal to the highest pressure amongst all vertices. Moreover, vertices that lie on steepest ﬁxable paths are exactly
the vertices with the highest pressure. For a given α > 0, in order to identify vertices with pressure exceeding α, we
compute vectors vHigh[α](x) and vLow[α](x) deﬁned as follows in terms of dist, the metric on V induced by ℓ:

vLow[α](x) = min

{v0(t) + α · dist(x, t)}

t∈T (v0)

vHigh[α](x) = max
t∈T (v0)

{v0(t) − α · dist(t, x)}.

4.1 Lex-minimization on Star Graphs

We ﬁrst consider the problem of computing the lex-minimizer on a star graph in which every vertex but the center is a
terminal. This special case is a subroutine in the general algorithm, and also motivates some of our techniques.

Let x be the center vertex, T be the set of terminals, and all edges be of the form (x, t) with t ∈ T . The initial
voltage assignment is given by v : T → R, and we abbreviate dist(x, t) by d(t) = ℓ(x, t). From Corollary 3.4 we know
that we can determine the value of the lex minimizer at x by ﬁnding a steepest ﬁxable path. By deﬁnition, we need to
ﬁnd t1, t2 ∈ T that maximize the gradient of the path from t1 to t2, ∇(t1, t2) = v(t1)−v(t2)
d(t2)+d(t2) . As observed above, this
is equivalent to ﬁnding a terminal with the highest pressure. We now present a simple randomized algorithm for this
problem that runs in expected linear time.

Given a terminal t1, we can compute its pressure α along with the terminal t2 such that |∇(t1, t2)| = α in time
O(|T |) by scanning over the terminals in T . Consider doing this for a random terminal t1. We will show that in linear
time one can then ﬁnd the subset of terminals T ′ ⊂ T whose pressure is greater than α. Assuming this, we complete
the analysis of the algorithm. If T ′ = ∅, t1 is a vertex with highest pressure. Hence the path from t1 to t2 is a steepest
ﬁxable path, and we return (t1, t2). If T ′ 6= ∅, the terminal with the highest pressure must be in T ′, and we recurse by
picking a new random t1 ∈ T ′. As the size of T ′ will halve in expectation at each iteration, the expected time of the
algorithm on the star is O(|T |).

To determine which terminals have pressure exceeding α, we observe that the condition ∃t2 : α < ∇(t1, t2) =
v(t1)−v(t2)
d(t1)+d(t2) , is equivalent to ∃t2 : v(t2)+αd(t2) < v(t1)−αd(t1). This, in turn, is equivalent to vLow[α](x) < v(t1)−
αd(t1). We can compute vLow[α](x) in deterministic O(|T |) time. Similarly, we can check if ∃t2 : α < ∇(t2, t1) by
checking if vHigh[α](x) > vt1 + αd(t1). Thus, in linear time, we can compute the set T ′ of terminals with pressure
exceeding α. The above algorithm is described in Algorithm 10.

Theorem 4.1 Given a set of terminals T, initial voltages v : T → R, and distances d : T → R+, STARSTEEPESTPATH(T, v, d)
returns (t1, t2) maximizing v(t1)−v(t2)

d(t1)+d(t2) , and runs in expected time O(|T |).

4.2 Lex-minimization on General Graphs

Theorem 3.3, tells us that META-LEX will compute lex-minimizers given an algorithm for ﬁnding a steepest ﬁxable
path in (G, v0). Recall that ﬁnding a steepest ﬁxable path is equivalent to ﬁnding a path with gradient equal to the
highest pressure amongst all vertices. In this section, we show how to do this in expected time O(m + n log n).

We describe an algorithm VERTEXSTEEPESTPATH that ﬁnds a terminal path P through any vertex x such that
∇P (v0) = pressure[v0](x) in expected O(m + n log n) time. Using Dijkstra’s algorithm, we compute dist(x, t) for
all t ∈ T. If x ∈ T (v0), then there must be a terminal path P that starts at x that has ∇P (v0) = pressure[v0](x). To
compute such a P we examine all t ∈ T (v0) in O(|T |) time to ﬁnd the t that maximizes |∇(x, t)| = |v(x)−v(t)|
, and
dist(x,t)
then return a shortest path between x and that t.

If x /∈ T (v0), then the steepest path through x between terminals t1 and t2 must consist of shortest paths between
x and t1 and between x and t2. Thus, we can reduce the problem to that of ﬁnding the steepest path in a star graph
where x is the only non-terminal and is connected to each terminal t by an edge of length dist(x, t). By Theorem 4.1,
we can ﬁnd this steepest path in O(|T |) expected time. The above algorithm is formally described as Algorithm 9.

Theorem 4.2 Given a well-posed instance (G, v0), and a vertex x ∈ VG, VERTEXSTEEPESTPATH(G, v0, x) returns
a terminal path P through x such that ∇P (v0) = pressure[v0](x), in O(m + n log n) expected time.

7

As in the algorithm for the star graph, we need to identify the vertices whose pressure exceeds a given α. For a ﬁxed
α, we can compute vLow[α](x) and vHigh[α](x) for all x ∈ VG using a simple modiﬁcation of Dijkstra’s algorithm in
O(m + n log n) time. We describe the algorithms COMPVHIGH, COMPVLOW for these tasks in Algorithms 3 and 4.
The following lemma encapsulates the usefulness of vLow and vHigh.

Lemma 4.3 For every x ∈ VG, pressure[v0](x) > α iff vHigh[α](x) > vLow[α](x).

It immediately follows that the algorithm COMPHIGHPRESSGRAPH(G, v0, α) described in Algorithm 6 computes

the vertex induced subgraph on the vertex set {x ∈ VG| pressure[v0](x) > α}.

We can combine these algorithms into an algorithm STEEPESTPATH that ﬁnds the steepest ﬁxable path in (G, v0)
in O(m + n log n) expected time. We may assume that there are no terminal-terminal edges in G. We sample an edge
(x1, x2) uniformly at random from EG, and a terminal x3 uniformly at random from VG. For i = 1, 2, 3, we compute
the steepest terminal path Pi containing xi. By Theorem 4.2, this can be done in O(m + n log n) expected time. Let α
be the largest gradient maxi ∇Pi. As mentioned above, we can identify G′, the induced subgraph on vertices x with
pressure exceeding α, in O(m + n log n) time. If G′ is empty, we know that the path Pi with largest gradient is a
steepest ﬁxable path. If not, a steepest ﬁxable path in (G, v0) must be in G′, and hence we can recurse on G′. Since
we picked a uniformly random edge, and a uniformly random vertex, the expected size of G′ is at most half that of G.
Thus, we obtain an expected running time of O(m + n log n). This algorithm is described in detail in Algorithm 7.

Theorem 4.4 Given a well-posed instance (G, v0) with EG ∩ (T (v0) × T (v0)) = ∅, STEEPESTPATH(G, v0) returns
a steepest ﬁxable path in (G, v0), and runs in O(m + n log n) expected time.

By using STEEPESTPATH in META-LEX, we get the COMPLEXMIN, shown in Algorithm 1. From Theorem 3.3 and
Theorem 4.4, we immediately get the following corollary.

Corollary 4.5 Given a well-posed instance (G, v0) as input, algorithm COMPLEXMIN computes a lex-minimizing
assignment that extends v0 in O(n(m + n log n)) expected time.

4.3 Linear-time Algorithm for Inf-minimization

Given the algorithms in the previous section, it is straightforward to construct an inﬁnity minimizer. Let α⋆ be the
gradient of the steepest terminal path. From Lemma 3.5, we know that the norm of the inf minimizer is α⋆. Considering
all trivial terminal paths (terminal-terminal edges), and using STEEPESTPATH, we can compute α⋆ in randomized
O(m+n log n) time. It is well known (McShane (1934); Whitney (1934)) that v1 = vLow[α⋆] and v2 = vHigh[α⋆] are
inf-minimizers. It is also known that 1
2 (v1 + v2) is the inf-minimizer that minimizes the maximum ℓ∞-norm distance
to all inf-minimizers. In the case of path graphs, this was observed by Gaffney and Powell (1976) and independently
by Micchelli et al. (1976). For completeness, the algorithm is presented as Algorithm 5, and we have the following
result.

Theorem 4.6 Given a well-posed instance (G, v0), COMPINFMIN(G, v0) returns a complete voltage assignment v
for G extending v0 that minimizes

∞ , and runs in randomized O(m + n log n) time.

grad[v]

4.4 Faster Algorithms for Lex-minimization

(cid:13)
(cid:13)

(cid:13)
(cid:13)

The lex-minimizer has additional structure that allows one to compute it by more efﬁcient algorithms. One observation
that leads to a faster implementation is that ﬁxing a steepest ﬁxable path does not increase the pressure at vertices,
provided that one appropriately ignores terminal-terminal edges. Thus, if G(α) is a subgraph that we identiﬁed with
pressure greater than α, we can iteratively ﬁx all steepest ﬁxable paths P in G(α) with ∇P > α. Another simple
observation is that if G(α) is disconnected, we can simply recurse on each of the connected components. A complete
description of an the algorithm COMPFASTLEXMIN based on these idea is given in Algorithm 11. The algorithm
provably computes lexG(v0), and it is possible to implement it so that the space requirement is only O(m + n).
Although, we are unable to prove theoretical bounds on the running time that are better than O(n(m + n log n)),
it runs extremely quickly in practice. We used it to perform the experiments in this paper. For random regular
graphs and Delaunay graphs, with n = 0.5 × 106 vertices and around 2 million edges m ∼ 1.5 − 2 × 106, it

8

takes a couple of minutes on a 2009 MacBook Pro. Similar times are observed for other model graphs of this
size such as random regular graphs and real world networks. An implementation of this algorithm may be found
at https://github.com/danspielman/YINSlex.

5 Directed Graphs

Our deﬁnitions and algorithms, including those for regularization, extend to directed graphs with only small modiﬁ-
cations. We view directed edges as diodes and only consider potential differences in the direction of the edge. For
a complete voltage assignment v on the vertices of a directed graph G, we deﬁne the directed gradient on (x, y) due
to v to be grad+
. Given a partially-labelled directed graph (G, v0), we say that a a
complete voltage assignment v is a lex-minimizer if it extends v0 and for other complete voltage assignment v′ that
extends v0 we have grad+
G[v′]. We say that a partially-labelled directed graph (G, v0) is a well-posed
directed instance if every free vertex appears in a directed path between two terminals.

G[v](x, y) = max

G[v] (cid:22) grad+

v(x)−v(y)
ℓ(x,y)

, 0

n

o

The main difference between the directed and undirected cases is that the directed lex-minimizer is not necessarily
unique. To maintain clarity of exposition, we chose to focus on undirected graphs so far. For directed graphs, we have
the following corresponding structural results.

Theorem 5.1 Given a well-posed instance (G, v0) on a directed graph G, there exists a lex-minimizer, and the set of
all lex-minimizers is a convex set. Moreover, for every two lex-minimizers v and v′, we have grad+

G[v] = grad+

G[v′].

However, note that in the case of directed graphs, the lex-minimizer need not be unique. We still have a weaker version
of Theorem 3.3 for directed graphs.

Theorem 5.2 Given a well-posed instance (G, v0) on a directed graph G, let v1 be the partial voltage assignment
extending v0 obtained by repeatedly ﬁxing steepest ﬁxable (directed) paths P with ∇P > 0. Then, any lex-minimizer
of (G, v0) must extend v1. Moreover, for every edge e ∈ EG \ (T (V1) × T (V1)), any lex-minimizer v of (G, v0) must
satisfy grad+[v](e) = 0.

When the value of the lex-minimizer at a vertex is not uniquely determined, it is constrained to an interval. In our
experiments, we pick the convention that when the voltage at a vertex is constrained to an interval (−∞, a] or [a, ∞),
we assign a to the terminal. When it is constrained to a ﬁnite interval, we assign a voltage closest to the median of the
original voltages.

6 Experiments on WebSpam

We demonstrate the performance of our lex-minimization algorithms on directed graphs by using them to detect spam
webpages as in Zhou et al. (2007). We use the dataset webspam-uk2006-2.0 described in Castillo et al. (2006).
This collection includes 11,402 hosts, out of which 7,473 (65.5 %) are labeled, either as spam or normal. Each host
corresponds to the collection of web pages it serves. Of the hosts, 1924 are labeled spam (25.7 % of all labels). We
consider the problem of ﬂagging some hosts as spam, given only a small fraction of the labels for training. We assign
a value of 1 to the spam hosts, and a value of 0 to the normal ones. We then compute a lex minimizer and examine the
effect of ﬂagging as spam all hosts with a value greater than some threshold.

Following Zhou et al. (2007), we create edges between hosts with lengths equal to the reciprocal of the number of
links from one to the other. We run our experiments only on the largest strongly connected component of the graph,
which contains 7945 hosts of which 5552 are labeled. 16 % of the nodes in this subgraph are labeled spam. To create
training and test data, for a given value p, we select a random subset of p % of the spam labels and a random subset
of p % of the normal labels to use for training. The remaining labels are used for testing. We report results for p = 5
and p = 20.

Again following Zhou et al. (2007), we plot the precision and recall of different choices of threshold for ﬂagging
pages as spam. Recall is the fraction of spam pages our algorithm ﬂags as spam, and precision is the fraction of pages
our algorithm ﬂags as spam that actually are spam. Amongst the algorithms studied by Zhou et al. (2007), the top

9

performer was their algorithm based on sampling according to a random-walk that follows in-links from other hosts.
We compare their algorithm with the classiﬁcation we get by directing edges in the opposite directions of links. This
has the effect that a link to a spam host is evidence of spamminess, and a link from a normal host is evidence of
normality.

Results are shown in Figure 3. While we are not able to reliably ﬂag all spam hosts, we see that in the range of
10-50 % recall, we are able to ﬂag spam with precision above 82 %. We see that the performance of directed lex-
minimization does not degrade rapidly when from the “large training set” regime of p = 20, to the “small training set”
regime of p = 5.

5 % labels for training

20 % labels for training

RandWalk
DirectedLex

RandWalk
DirectedLex

i

i

n
o
s
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
0.1

0.2

0.3

0.4

0.7

0.8

0.9

1

0.2

0.3

0.4

0.7

0.8

0.9

1

0.6
0.5
Recall

0.6
0.5
Recall

Figure 3: Recall and precision in the web spam classiﬁcation experiment. Each data point shown was computed as an average over
100 runs. The largest standard deviation of the mean precision across the plotted recall values was less than 1.3 %. The algorithm
of Zhou et al. (2007) appears as RANDWALK. Our directed lex-minimization algorithm appears as DIRECTEDLEX.

For comparison, in Appendix C, we show the performance of our algorithm and that of Zhou et al. (2007) both
with link directions reversed, as well as the performance of undirected lex-minimization and Laplacian inference, all
of which are signiﬁcantly worse.

7 l0-Regularization of Vertex Values

We now explain how we can accommodate noise in both the given voltages and in the given lengths of edges. We can
ﬁnd the minimum number of labels to ignore, or the minimum increase in edges lengths needed so that there exists an
extension whose gradients have l∞-norm lower than a given target. After determining which labels to ignore or the
needed increment in edge lengths, we recommend computing a lex minimizer.

The algorithms we present in this section are essentially the same for directed and undirected graphs.

7.1 l0-Vertex Regularization for Inf-minimization

The l0-regularization of vertex labels can be viewed as a problem of outlier removal: the vector we compute is allowed
to disagree with v0 on up to k terminals. Given a voltage assignment v and a subset T ⊂ V of the vertices, by v(T )
we mean the vector obtained by restricting v to T . We deﬁne the l0-Vertex Regularization for l∞ problem to be

where v(T ) is the vector of values of v on the terminals T .

min
v∈IRn

gradG[v]

∞

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

0 ≤ k,
(cid:13)
(cid:13)

subject to

v(T ) − v0(T )

(3)

In Appendix D, we describe an approximation algorithm APPROX-OUTLIER that approximately solves program (3).

The precise statement we prove in Appendix D is given in the following theorem.

i

i

n
o
s
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
0.1

10

Theorem 7.1 (Approximate l0-vertex regularization) The algorithm APPROX-OUTLIER takes a positive integer k
and a partially-labeled graph (G, v0), and outputs an assignment v with
0 ≤ 2k, and
∞ ≤
α∗, where α∗ is the optimum value of program (3). The algorithm runs in time O(k(m + n log n)).
(cid:13)
(cid:13)
(cid:13)
(cid:13)

In Appendix D, we also describe an algorithm OUTLIER that exactly solves program (3) in polynomial time, and we
prove its correctness.

v(T ) − v0(T )

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Theorem 7.2 (Exact l0-vertex regularization) The algorithm OUTLIER takes a positive integer k and a partially-
labeled graph (G, v0) solves program (3) exactly. The algorithm runs in polynomial time.

We give a proof of Theorem 7.2 in Appendix D. To do this, we reduce the program (3) to the problem of minimizing
the required l0-budget needed to achieve a ﬁxed gradient α using a binary search over a set of O(n2) gradients. This
latter problem we reduce in polynomial time to Minimum Vertex Cover (VC) on a transitively closed, directed acyclic
graph (a TC-DAG). VC on a TC-DAG can be solved exactly in polynomial time by a reduction to the Maximum
Bipartite Matching Problem (Fulkerson (1956)). The problem was phrased by Fulkerson as one of ﬁnding a maximum
antichain of a ﬁnite poset. Any transitively closed DAG corresponds directly to the comparability graph of a poset. A
maximum antichain of a poset is a maximum independent set of a the comparability graph of the poset, and hence its
complement is a minimum vertex cover of the comparability graph. We refer to the algorithm developed by Fulkerson
as KONIG-COVER.

Theorem 7.3 The algorithm KONIG-COVER computes a minimum vertex cover for any transitively closed DAG G in
polynomial time.

7.2 Hardness of l0 regularization for l2

The result that l0-regularized inf-minimization can be solved exactly in polynomial time is surprising, especially
because the analogous problem for 2-Laplacian minimization turns out to be NP-Hard.

We deﬁne the the l0 vertex regularization for l2 for a partially-labeled graph (G, v0) and an integer k by

min
v∈Rn:kv(T )−v0(T )k0

≤k

vT Lv,

where L is the Laplacian of G.

Theorem 7.4 l0 vertex regularization for l2 is NP-Hard.

In Appendix E we prove Theorem 7.4 by giving a polynomial time (Karp) reduction from the NP-Hard minimum
bisection problem to l0 vertex regularization for l2.

8 l1-Edge and Vertex Regularization of Inf-minimizers

Consider a partially-labeled graph (G, v0) and an α > 0. The set of voltage assignments given by

v : v extends v0 and

gradG[v]

∞ ≤ α

n

(cid:13)
(cid:13)

(cid:13)
(cid:13)

o

is convex. Going further, let us consider the edge lengths in a graph to be speciﬁed by a vector ℓ ∈ IRE. Now the set
of voltages v and and lengths ℓ which achieve kgradG(ℓ)[v]k∞ ≤ α is jointly convex in v and ℓ. To see this, observe
that

kgradG(ℓ)[v]k∞ ≤ α ⇔ ∀(u, v) ∈ E : −αℓ(u, v) ≤ v(u) − v(v) ≤ αℓ(u, v).
Furthermore, the condition “v extends v0” is a linear constraint on v, which we express as v(T ) = v0(T ). From
the above, it is clear that the gradient condition corresponds to a convex set, as it is an intersection of half-spaces.
These half-spaces are given by O(m) linear inequalities. We can leverage this to phrase many regularized variants of
inf-minimization as convex programs, and in some cases linear programs.

(4)

11

For example, we may consider a variant of inf-minimization combined with an l1-budget for changing lengths of
edges and values on terminals. Given a parameter γ > 0 which speciﬁes the relative cost of regularizing terminals to
regularizing edges, the problem is as follows

arg min
v∈IRn,s∈IRm,s≥0

ksk1 + γ

v(T ) − v0(T )

1

subject to

gradG(ℓ+s)[v]

≤ α.

(5)

(cid:13)
(cid:13)
From our observation (4), it follows that problem (5) may be expressed as a linear program with O(n) variables
and O(m) constraints. We can use ideas from Daitch and Spielman (2008) to solve the resulting linear program in
O(m1.5) by an interior point method with a special purpose linear equation solver. The reason is that the linear
time
equations the IPM must solve at each iteration may be reduced to linear equations in symmetric, diagonally dominant
matrices, and these may be solved in nearly-linear time (Cohen et al. (2014)).

(cid:13)
(cid:13)

e

(cid:13)
(cid:13)
(cid:13)

∞

(cid:13)
(cid:13)
(cid:13)

Conclusion. We propose the use of inf and lex minimizers for regression on graphs. We present simple algorithms
for computing them that are provably fast and correct, and can also be implemented efﬁciently. We also present a
framework and polynomial time algorithms for regularization in this setting. The initial experiments reported in the
paper indicate that these algorithms give pretty good results on real and synthetic datasets. The results seem to compare
quite favorably to other algorithms, particularly in the regime of tiny labeled sets. We are testing these algorithms on
several other graph learning questions, and plan to report on them in a forthcoming experimental paper. We believe
that inf and lex minimizers, and the associated ideas presented in the paper, should be useful primitives that can be
proﬁtably combined with other approaches to learning on graphs.

We thank anonymous reviewers for helpful comments. We thank Santosh Vempala and Bartosz Walczak for pointing
out that it was already known how to compute a minimum vertex cover of a transitively closed DAG in polynomial
time.

Acknowledgements

References

Morteza Alamgir
In Advances
Information Processing
http://books.nips.cc/papers/files/nips24/NIPS2011_0278.pdf.

and Ulrike V. Luxburg.

transition
24,

in
pages

in Neural

Systems

Phase

the

family
379–387.

of
2011.

p-resistances.
URL

Gunnar Aronsson. Extension of functions satisfying lipschitz conditions. Arkiv fr Matematik, 6(6):551–561, 1967.

ISSN 0004-2080. doi: 10.1007/BF02591928. URL http://dx.doi.org/10.1007/BF02591928.

Gunnar Aronsson, Michael G. Crandall, and Petri Juutinen. A tour of the theory of absolutely minimizing functions.
ISSN 0273-0979. doi: 10.1090/S0273-0979-04-01035-3.

Bull. Amer. Math. Soc. (N.S.), 41(4):439–505, 2004.
URL http://dx.doi.org/10.1090/S0273-0979-04-01035-3.

Guy Barles and J´erˆome Busca. Existence and comparison results for fully nonlinear degenerate elliptic equations

without zeroth-order term. Comm. Partial Differential Equations, 26:2323–2337, 2001.

Mikhail Belkin, Irina Matveeva, and Partha Niyogi.

Regularization and semi-supervised learning on large
In Learning Theory, volume 3120 of Lecture Notes in Computer Science, pages 624–638.
doi: 10.1007/978-3-540-27819-1 43. URL

graphs.
Springer Berlin Heidelberg, 2004.
http://dx.doi.org/10.1007/978-3-540-27819-1_43.

ISBN 978-3-540-22282-8.

Nick Bridle and Xiaojin Zhu. p-voltages: Laplacian regularization for semi-supervised learning on high-dimensional

data. In Eleventh Workshop on Mining and Learning with Graphs (MLG2013), 2013.

12

Carlos Castillo, Debora Donato, Luca Becchetti, Paolo Boldi, Stefano Leonardi, Massimo Santini, and Sebastiano
Vigna. A reference collection for web spam. SIGIR Forum, 40(2):11–24, December 2006. ISSN 0163-5840. doi:
10.1145/1189702.1189703. URL http://doi.acm.org/10.1145/1189702.1189703.

Olivier Chapelle, Bernhard Schlkopf, and Alexander Zien. Semi-Supervised Learning. The MIT Press, 1st edition,

2010. ISBN 0262514125, 9780262514125.

Michael B Cohen, Rasmus Kyng, Gary L Miller, Jakub W Pachocki, Richard Peng, Anup B Rao, and Shen Chen Xu.
Solving SDD linear systems in nearly m log1/2 n time. In Proceedings of the 46th Annual ACM Symposium on
Theory of Computing, pages 343–352. ACM, 2014.

M.G. Crandall, L.C. Evans, and R.F. Gariepy. Optimal lipschitz extensions and the inﬁnity laplacian. Calculus of Vari-
ations and Partial Differential Equations, 13(2):123–139, 2001. ISSN 0944-2669. doi: 10.1007/s005260000065.
URL http://dx.doi.org/10.1007/s005260000065.

Samuel I. Daitch and Daniel A. Spielman. Faster approximate lossy generalized ﬂow via interior point algo-
rithms.
In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing, STOC ’08, pages
451–460, New York, NY, USA, 2008. ACM. ISBN 978-1-60558-047-0. doi: 10.1145/1374376.1374441. URL
http://doi.acm.org/10.1145/1374376.1374441.

Alan Egger and Robert Huotari. Rate of convergence of the discrete polya algorithm. Journal of Approximation
ISSN 0021-9045. doi: http://dx.doi.org/10.1016/0021-9045(90)90070-7. URL

Theory, 60(1):24 – 30, 1990.
http://www.sciencedirect.com/science/article/pii/0021904590900707.

Charles Fefferman. Whitney’s extension problems and interpolation of data.

(N.S.), 46(2):207–220, 2009a.
http://dx.doi.org/10.1090/S0273-0979-08-01240-8.

ISSN 0273-0979.

doi:

10.1090/S0273-0979-08-01240-8.

Bull. Amer. Math. Soc.
URL

Charles Fefferman. Fitting a [image] -smooth function to data, iii. Annals of Mathematics, 170(1):pp. 427–441, 2009b.

ISSN 0003486X. URL http://www.jstor.org/stable/40345469.

Charles Fefferman and Bo’az Klartag. Fitting a cm -smooth function to data i. Annals of Mathematics, 169(1):pp.

315–346, 2009. ISSN 0003486X. URL http://www.jstor.org/stable/40345445.

D. R. Fulkerson. Note on dilworths decomposition theorem for partially ordered sets. Proc. Amer. Math. Soc, 1956.

P.W. Gaffney and M.J.D. Powell. Optimal interpolation. In Numerical Analysis, volume 506 of Lecture Notes in Math-
ematics, pages 90–99. Springer Berlin Heidelberg, 1976. ISBN 978-3-540-07610-0. doi: 10.1007/BFb0080117.
URL http://dx.doi.org/10.1007/BFb0080117.

L.-A. Gottlieb, A. Kontorovich, and R. Krauthgamer. Efﬁcient classiﬁcation for metric data. CoRR, abs/1306.2547,

2013. URL http://arxiv.org/abs/1306.2547.

L.-A. Gottlieb, A. Kontorovich, and R. Krauthgamer. Efﬁcient regression in metric spaces via approximate lipschitz
extension. In Similarity-Based Pattern Recognition, volume 7953 of Lecture Notes in Computer Science, pages
43–58. Springer Berlin Heidelberg, 2013b. ISBN 978-3-642-39139-2. doi: 10.1007/978-3-642-39140-8 3. URL
http://dx.doi.org/10.1007/978-3-642-39140-8_3.

Robert Jensen. Uniqueness of lipschitz extensions: Minimizing the sup norm of the gradient. Archive for Ra-
doi: 10.1007/BF00386368. URL

ISSN 0003-9527.

tional Mechanics and Analysis, 123(1):51–74, 1993.
http://dx.doi.org/10.1007/BF00386368.

M. Kirszbraun. ber die zusammenziehende und lipschitzsche transformationen. Fundamenta Mathematicae, 22(1):

77–108, 1934. URL http://eudml.org/doc/212681.

13

Andrew J. Lazarus, Daniel E. Loeb,

James G. Propp, Walter R. Stromquist,

Combinatorial games under

man.
229 – 264,
http://www.sciencedirect.com/science/article/pii/S0899825698906765.

http://dx.doi.org/10.1006/game.1998.0676.

and Economic Behavior,

ISSN 0899-8256.

auction play.

Games

1999.

doi:

and Daniel H. Ull-
27(2):
URL

Yunqian Ma and Yun Fu. Manifold Learning Theory and Applications. CRC Press, Inc., Boca Raton, FL, USA, 1st

edition, 2011. ISBN 1439871094, 9781439871096.

E. J. McShane. Extension of range of functions. Bull. Amer. Math. Soc., 40(12):837–842, 12 1934. URL

http://projecteuclid.org/euclid.bams/1183497871.

C.A. Micchelli, T.J. Rivlin,

and S. Winograd.

merische Mathematik, 26(2):191–200, 1976.
http://dx.doi.org/10.1007/BF01395972.

The optimal
ISSN 0029-599X.

recovery of
doi:

smooth functions.
10.1007/BF01395972.

Nu-
URL

V. A. Milman.

Absolutely minimal extensions of

functions on metric spaces.

1999.

URL

http://iopscience.iop.org/1064-5616/190/6/A05/pdf/MSB_190_6_A05.pdf.

Boaz Nadler, Nathan Srebro, and Xueyuan Zhou. Statistical analysis of semi-supervised learning: The limit of inﬁnite
unlabelled data. 2009. URL http://ttic.uchicago.edu/˜nati/Publications/NSZnips09.pdf.

A. Naor and S. Shefﬁeld. Absolutely minimal Lipschitz extension of tree-valued mappings. CoRR, abs/1005.2535,

May 2010. URL http://arxiv.org/abs/1005.2535.

A. M. Oberman. Finite difference methods for the Inﬁnity Laplace and p-Laplace equations. CoRR, abs/1107.5278,

July 2011. URL http://arxiv.org/abs/1107.5278.

Yuval Peres, Oded Schramm, Scott Shefﬁeld, and DavidB. Wilson.

Tug-of-war and the inﬁnity lapla-
In Selected Works of Oded Schramm, Selected Works in Probability and Statistics, pages 595–
doi: 10.1007/978-1-4419-9675-6 18. URL

cian.
638. Springer New York, 2011.
http://dx.doi.org/10.1007/978-1-4419-9675-6_18.

ISBN 978-1-4419-9674-9.

S. Shefﬁeld and C. K. Smart. Vector-valued optimal Lipschitz extensions. CoRR, abs/1006.1741, June 2010. URL

http://arxiv.org/abs/1006.1741.

Ali Kemal Sinop and Leo Grady. A seeded image segmentation framework unifying graph cuts and random walker
which yields a new algorithm. In Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on,
pages 1–8. IEEE, 2007.

Vijay V. Vazirani. Approximation Algorithms. Springer-Verlag New York, Inc., New York, NY, USA, 2001. ISBN

3-540-65367-8.

Ulrike von Luxburg and Olivier Bousquet. Distance-based classiﬁcation with lipschitz functions.

In Learn-
ing Theory and Kernel Machines, volume 2777 of Lecture Notes in Computer Science, pages 314–328.
doi: 10.1007/978-3-540-45167-9 24. URL
Springer Berlin Heidelberg, 2003.
http://dx.doi.org/10.1007/978-3-540-45167-9_24.

ISBN 978-3-540-40720-1.

Hassler Whitney.

Analytic extensions of differentiable functions deﬁned in closed sets.

tions of
http://www.jstor.org/stable/1989708.

the American Mathematical Society, 36(1):pp. 63–89, 1934.

ISSN 00029947.

Transac-
URL

Dengyong Zhou, Christopher J. C. Burges, and Tao Tao. Transductive link spam detection.

In Proceedings
of the 3rd International Workshop on Adversarial Information Retrieval on the Web, AIRWeb ’07, pages 21–
ISBN 978-1-59593-732-2. doi: 10.1145/1244408.1244413. URL
28, New York, NY, USA, 2007. ACM.
http://doi.acm.org/10.1145/1244408.1244413.

Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic

functions. In IN ICML, pages 912–919, 2003.

14

A Basic Properties of Lex-Minimizers

A.1 Meta Algorithm

Algorithm 1: Algorithm META-LEX: Given a well-posed instance (G, v0), outputs lexG[v0].
for i = 1, 2, . . . :

1. if T (vi−1) = VG, then return vi−1.
2. E′ = EG \ (T (vi−1) × T (vi−1)), G′ := (VG, E′).
3. Let P ⋆
4. vi ← ﬁx[vi−1, P ⋆
i ].

i be a steepest ﬁxable path in (G′, vi−1). Let α⋆

i ← ∇P ⋆(vi−1).

In this subsection, we prove the results that appeared in section 2. We start with a simple observation.

Proposition A.1 Given a well-posed instance (G, v0) such that T (v0) 6= V, let P be a steepest ﬁxable path in (G, v0).
Then, ﬁx[v0, P ] extends v0, and (G, ﬁx[v0, P ]) is also a well-posed instance.

The properties we prove below do not depend on the choice of the steepest ﬁxable path.

Proposition A.2 For any well-posed instance (G, v0), with |VG| = n, META-LEX(G, v0) terminates in at most n
iterations, and outputs a complete voltage assignment v that extends v0.

Proof of Proposition A.2: By Proposition A.1, at any iteration i, vi−1 extends v0 and (G′, vi−1) is a well-posed
instance. META-LEX only outputs vi−1 iff T (vi−1) = V, which means vi−1 is a complete voltage assignment. For
any vi−1 that is not complete, for any x ∈ V \T (vi−1), we must have a free terminal path in (G′, vi−1) that contains x.
i exists in (G′, vi−1). Since P ⋆
Hence, a steepest ﬁxable path P ⋆
i ] ﬁxes the voltage
i
✷
for at least one non-terminal. Thus, META-LEX(G, v0) must complete in at most n iterations.

is a free terminal path, ﬁx[vi−1, P ⋆

For the following lemmas, consider a run of META-LEX with well-posed instance (G, v0) as input. Let vout be the
complete voltage assignment output by META-LEX. Let Ei be the set of edges E′ and Gi be the graph G′ constructed
in iteration i of META-LEX.

Lemma A.3 For every edge e ∈ Ei−1 \ Ei, we have

grad[vout](e)

≤ α⋆

i . Moreover, α⋆

i is non-increasing with i.

Proof of Lemma A.3: Let P ⋆
i = (x0, . . . , xr) be a steepest ﬁxable path in iteration i (when we deal with instance
(Gi−1, vi−1)). Consider a terminal path Pi+1 in (Gi, vi) such that {∂0Pi+1, ∂1Pi+1} ∩ (T (vi) \ T (vi−1)) 6= ∅. We
i . On the contrary, assume that ∇Pi+1(vi) > α⋆
claim that ∇Pi+1(vi) ≤ α⋆
i . Consider the case ∂0Pi+1 ∈ T (vi) \
T (vi−1), ∂1P1 ∈ T (vi−1). By the deﬁnition of vi, we must have ∂0Pi+1 = xj for some j ∈ [r − 1]. Let P ′
i+1 be the
path formed by joining paths P ⋆

i+1 is a free terminal path in (Gi−1, vi−1). We have,

i [x0 : xj] and Pi+1. P ′

(cid:12)
(cid:12)

(cid:12)
(cid:12)

vi−1(x0) − vi−1(∂1Pi+1) = (vi(x0) − vi(xj )) + (vi(∂0Pi+1) − vi(∂1Pi+1))
i · ℓ(P ′

i · ℓ(Pi+1) = α⋆

i [x0 : xj]) + α⋆

i · ℓ(P ⋆

> α⋆

i+1),

giving ∇P ′
The other cases can be handled similarly.

i+1(vi) > α⋆

i , which is a contradiction since the steepest ﬁxable path P ⋆
i

in (Gi−1, vi−1) has gradient α⋆
i .

Applying the above claim to an edge e ∈ Ei−1 \ Ei, whose gradient is ﬁxed for the ﬁrst time in iteration i, we
i . If v is the complete voltage assignment output by META-LEX, since v extends vi+1,
i , implying

i . Applying the claim to the symmetric edge, we obtain −grad[vout](e) ≤ α⋆

obtain that grad[vi+1](e) ≤ α⋆
we get grad[vout](e) ≤ α⋆
|grad[vout](e)| ≤ α⋆
i .

Consider any free terminal path Pi+1 in (Gi, vi). If Pi+1 is also a terminal path in (Gi−1, vi−1), it is a free
terminal path in (Gi−1, vi−1). In addition, since a steepest ﬁxable path P ⋆
i , we get
i
∇Pi+1(vi) = ∇Pi+1(vi−1) ≤ α⋆
i . Otherwise, we must have {∂0Pi+1, ∂1Pi+1} ∩ (T (vi) \ T (vi−1)) 6= ∅, and we can
deduce ∇Pi+1(vi) ≤ α⋆
i using the above claim. Thus, all free terminal paths Pi+1 in (Gi, vi) satisfy ∇Pi+1(vi) ≤ α⋆
i .
✷
In particular, α⋆

in (Gi−1, vi−1) has ∇P ⋆

i = α⋆

i is non-increasing with i.

i+1(vi) ≤ α⋆

i+1 = ∇P ⋆

i . Thus, α⋆

15

Lemma A.4 For any complete voltage assignment v for G that extends v0, if v 6= vout, we have grad[v] 6(cid:22) grad[vout],
and hence grad[vout] (cid:22) grad[v].

Proof of Lemma A.4: Consider any complete voltage assignment v for G that extends v0, such that v 6= vout. Thus,
there exists a unique i such that v extends vi−1 but does not extend vi. We will argue that grad[v] 6(cid:22) grad[vout], and
hence grad[vout] (cid:22) grad[v]. For every edge e ∈ E \ Ei−1 that has been ﬁxed so far, grad[v](e) = grad[vi−1](e) =
grad[vout](e), and hence we can ignore these edges.

Since v extends vi−1 but not vi, there exists an x ∈ T (vi) \ T (vi−1) such that v(x) 6= vi(x) = vout(x). Assume
i picked

i = (x0, . . . , xr) is the steepest ﬁxable path with gradient α⋆

v(x) < vi(x) (the other case is symmetric). If P ⋆
in iteration i, we must have x = xj for some j ∈ [r − 1]. Thus,

j

j

(v(xk−1) − v(xk)) = v(x0) − v(xj ) > vi(x0) − vi(xj ) = α⋆

i · ℓ(P ⋆

i [x0 : xj ]) = α⋆
i ·

ℓ(xk−1, xk).

Xk=1

Xk=1
Thus, for some k ∈ [j], we must have grad[v](xk−1, xk) > α⋆
is a path in Gi−1, we have {xk−1, xk} 6⊆
T (vi−1). This gives (xk−1, xk) ∈ (Ei−1 \ Ei). But then, from Lemma A.3, it follows that for all e ∈ (Ei−1 \ Ei), we
✷
have |grad[vout](e)| ≤ α⋆

i . Thus, we have grad[v] 6(cid:22) grad[vout].

i . Since P ∗
i

Lemma A.5 Let P = (x0, . . . , xr) be a steepest ﬁxable path such that it does not have any edges in T (v0) × T (v0)
and v1 = ﬁxG[v0, P ]. Then for every i ∈ [r], we have grad[v1](xi−1, xi) = ∇P.

Proof of Lemma A.5: Suppose this is not true and let j ∈ [r] be the minimum number such that grad[v1](xj−1, xj) 6=
∇P. By deﬁnition of v1 we would necessarily have j < r and vj ∈ T (v0). Suppose grad[v1](xj−1, xj ) < ∇P. We
would then have v1(x0) − v1(xj ) < ∇P ∗ ℓ(P [x0 : xj]). Since P does not have any edges in T (v0) × T (v0),
P1 := (xj, ..., xr) would be a free terminal path with ∇P1 > ∇P. This is a contradiction. Other cases can be ruled
out similarly.

✷

Proof of Theorem 3.3: Consider an arbitrary run of META-LEX on (G, v0). Let vout be the complete voltage
assignment output by META-LEX. Proposition A.1 implies that vout extends v0. Lemma A.4 implies that for any
complete voltage assignment v 6= vout that extends v0, we have grad[vout] (cid:22) grad[v]. Thus, vout is a lex-minimizer.
Moreover, the lemma also gives that for any such v, grad[v] 6(cid:22) grad[vout]. and hence vout is a unique lex-minimizer.
Thus, vout is the unique voltage assignment satisfying Def. 2.1, and we denote it as lexG[v0]. Since we started with an
✷
arbitrary run of META-LEX, uniqueness implies that every run of META-LEX on (G, v0) must output lexG[v0].

Proof of Lemma 3.5: Suppose we have a complete voltage assignment v extending v0, such that
For any terminal path P = (x0, . . . , xr), we get,

grad[v]

∞ ≤ α.

∇P (v0) = v0(∂0P ) − v0(∂1P ) = v(∂0P ) − v(∂1P ) =

grad[v](xi−1, xi) ≤ α ·

ℓ(xi−1, xi) = α · ℓ(P ),

(cid:13)
(cid:13)

(cid:13)
(cid:13)

r

i=1
X

giving ∇P (v0) ≤ α.

On the other hand, suppose every terminal path P in (G, v0) satisﬁes ∇P (v0) ≤ α. Consider v = lexG[v0]. We
know that v extends v0. For every edge e ∈ EG ∩ T (v0) × T (v0), e is a (trivial) terminal path in (G, v0), and hence
has satisﬁes grad[v](e) = grad[v0](e) = ∇e(v0) ≤ α. Considering the reverse edge, we also obtain −grad[v](e) ≤ α.
Thus, |grad[v](e)| ≤ α. Moreover, using Lemma A.3, we know that for edge e ∈ EG \ T (v0) × T (v0), |grad[v](e)| ≤
1 = ∇P ⋆
α⋆
1 ≤ α since P1 is a terminal path in (G, v0). Thus, for every e ∈ EG, |grad[v](e)| ≤ α, and hence
✷
grad[v]
∞ ≤ α.
(cid:13)
(cid:13)
(cid:13)
(cid:13)
A.2 Stability

In this subsection, we sketch a proof of the monotonicity of lex-minimizers and show how it implies the stability
property claimed earlier.

For any well-posed (G, v0), there could be several possible executions of META-LEX, each characterized by the

sequence of paths P ⋆

i . We can apply Theorem 3.3 to deduce the following structural result about the lex-minimizer.

r

i=1
X

16

Corollary A.6 For any well-posed instance (G, v0), consider a sequence of paths (P1, . . . , Pr) and voltage assign-
ments (v1, . . . , vr) for some positive integer r such that:

1. P ⋆

i is a steepest ﬁxable path in (Gi−1, vi−1) for i = 1, . . . , r.

2. vi = ﬁx[vi−1, P ⋆

i ] for i = 1, . . . , r.

3. T (vr) = VG.

Then, we have vr = lexG[v0].

We call such a sequence of paths and voltages to be a decomposition of lexG[v0]. Again, note that lexG[v0] can
possibly have multiple decompositions. However, any two such decompositions are consistent in the sense that they
produce the same voltage assignment.

Proof of Corollary 3.7: We ﬁrst deﬁne some operations on partial assignments which simpliﬁes the notation. Let
v0, v1 be any two partial assignments with the same set of terminals T := T (v0) = T (v1) and c, d ∈ R. By cv0 + d
we mean a partial assignment v with T (v) = T satisfying v(t) = cv0(t) + d for all t ∈ T . Also, by v0 + v1 we
mean a partial assignment v with T (v) = T satisfying v(t) = v0(t) + v1(t) for all t ∈ T. Also, we say v1 ≥ v0 if
v1(t) ≥ v0(t) for all t ∈ T .

Now we can show how Corollary 3.7 follows from Theorem 3.6. Let v := v1 − v0, and kvk∞ = ǫ, for some ǫ > 0.
Therefore, v0 + ǫ ≥ v1 ≥ v0 − ǫ. Theorem 3.6 then implies that lexG[v0] + ǫ ≥ lex[v1] ≥ lex[v0] − ǫ, hence proving
✷
the corollary.

Proof sketch of Theorem 3.6:
It is easy to see that the ﬁrst statement holds. For the second statement, we ﬁrst
observe that if there is a sequence of paths P1, ..., Pr that is simultaneously a decomposition of both lex[v0] and
lex[v1], then this is easy to see. If such a path sequence doesn’t exist, then we look at vt := v0 + t(v1 − v0). We
state here without a proof (though the proof is elementary) that we can then split the interval [0, 1] into ﬁnitely many
subintervals [a0, a1], [a1, a2], .., [ak−1, ak], with a0 = 0, ak = 1, such that for any i, there is a path sequence P1, ..., Pr
which is a decomposition of lex[vt] for all t ∈ [ai, ai+1]. We then observe that v0 = va0 ≤ va1 ≤ ...vak = v1. Since
for every ai, ai+1, there is a path sequence which is simultaneously a decomposition of both lex[vai ] and lex[vai+1 ],
we immediately get

lex[v0] = lex[va0 ] ≤ lex[va1] ≤ ... ≤ lex[vak ] = lex[v1].

✷

A.3 Alternate Characterizations

Proof of Theorem 3.10: We know that lexG[v0] extends v0. We ﬁrst prove that v = lexG[v0] satisﬁes the max-min
gradient averaging property. Assume to the contrary. Thus, there exists x ∈ VG \ T (v0) such that

max
y:(x,y)∈EG

grad[v](x, y) 6= − min

grad[v](x, y).

y:(x,y)∈EG

Assume that max(x,y)∈EG grad[v](x, y) ≥ − min(x,y)∈EG grad[v](x, y). Then, consider v′ extending v0 that is iden-
tical to v except for v′(x) = v(x) − ǫ for ǫ > 0. For ǫ small enough, we get that

and

max
y:(x,y)∈EG

grad[v′](x, y) < max

grad[v](x, y)

y:(x,y)∈EG

− min

y:(x,y)∈EG

grad[v′](x, y) < max

grad[v](x, y).

y:(x,y)∈EG

The gradient of edges not incident on the vertex x is left unchanged. This implies that grad[v]

6(cid:22) grad[v′],

contradicting the assumption that v is the lex-minimizer. (The other case is similar).

17

For the other direction. Consider a complete voltage assignment v extending v0 that satisﬁes the max-min gradient

averaging property w.r.t. (G, v0). Let

α = max

grad[v](x, y) ≥ 0

(x,y)∈EG
x∈V \T (v0)

be the maximum edge gradient, and consider any edge (x0, x1) ∈ EG such that grad[v](x1, x0) = α, with x1 ∈
V \ T (v0). If α = 0, grad[v] is identically zero, and is trivially the lex-minimal gradient assignment. Thus, both v and
lexG[v0] are constant on each connected component. Since (G, v0) is well-posed, there is at least one terminal in each
component, and hence v and lexG[v0] must be identical.

Now assume α > 0. By the max-min gradient averaging property, ∃x2 ∈ VG such that (x1, x2) ∈ EG and

grad[v](x1, x2) =

min
y:(x1,y)∈EG

grad[v](x1, y) = − max

grad[v](x1, y)

y:(x1,y)∈EG

≤ −grad[v](x1, x0) = −α.

Thus, grad[v](x2, x1) ≥ α. Since α is the maximum edge gradient, we must have grad[v](x2, x1) = α. More-
over, v(x2) > v(x1) > v(x0), thus x2 6= x0. We can inductively apply this argument at x2 until we hit a ter-
minal. Similarly, if x0 /∈ T (v0) we can extend the path in the other direction. Consequently, we obtain a path
P = (xj , . . . , x2, x1, x0, x−1, . . . , xk) with all vertices as distinct, such that xj , xk ∈ T (v0), and xi ∈ V \ T (v0)
for all i ∈ [j + 1, k − 1]. Moreover, grad[v](xi, xi−1) = α for all j < i ≤ k. Thus, P is a free terminal path with
∇P [v0] = α.

Moreover, since v is a voltage assignment extending v0 with

∞ = α, using Lemma 3.5, we know that
every terminal path P ′ in (G, v0) must satisfy ∇P ′(v0) ≤ α. Thus, P is a steepest ﬁxable path in (G, v0). Thus,
letting v1 = ﬁx[v0, P ], using Corollary 3.4, we obtain that lexG[v1] = lexG[v0]. Moreover, since α = ∇P [v0] =
grad[v](xi, xi−1) for all i ∈ (j, k], we get v1(xi) = v(xi) for all i ∈ (j, k). Thus, v extends v1.

grad[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We can iterate this argument for r iterations until T (vr) = VG, giving v = vr and vr = lexG[vr] = lexG[v0].
(Since we are ﬁxing at least one terminal at each iteration, this procedure terminates). Thus, we get v = lexG[v0]. ✷

B Description of the Algorithms

Algorithm 2: MODDIJKSTRA(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs a complete
voltage assignment v for G, and an array parent : V → V ∪ {null}.

Add x to a ﬁbonacci heap, with key(x) = +∞.
ﬁnished(x) ← false

Decrease key(x) to v0(x).
parent(x) ← null.

1. for x ∈ VG,
2.
3.
4. for x ∈ T (v0)
5.
6.
7. while heap is not empty
8.
9.
10.
11.
12.
13.
14.
15. return (v, parent)

x ← pop element with minimum key from heap
v(x) ← key(x). ﬁnished(x) ← true .
for y : (x, y) ∈ EG

if ﬁnished(y) = false

if key(y) > v(x) + α · ℓ(x, y)

Decrease key(y) to v(x) + α · ℓ(x, y).
parent(y) ← x.

Theorem B.1 For a well-posed instance (G, V0) and a gradient value α ≥ 0, let (v, parent) ← MODDIJKSTRA(G, v0, α).
Then, v is a complete voltage assignment such that, ∀x ∈ VG, v(x) = mint∈T (v0){v0(t) + αdist(x, t)}. Moreover, the
pointer array parent satisﬁes ∀x /∈ T (v0), parent(x) 6= null and v(x) = v(parent(x)) + α · ℓ(x, parent(x)).

18

Algorithm 3: Algorithm COMPVLOW(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs
vLow, a complete voltage assignment for G, and an array LParent : V → V ∪ {null}.

1. (vLow, LParent) ← MODDIJKSTRA(G, v0, α)
2. return (vLow, LParent)

Algorithm 4: Algorithm COMPVHIGH(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs
vHigh, a complete voltage assignment for G, and an array HParent : V → V ∪ {null}.

if x ∈ T (v0) then v1(x) ← −v0(x) else v1(x) ← v1(x).

1. for x ∈ VG
2.
3. (temp, HParent) ← MODDIJKSTRA(G, v1, α)
4. for x ∈ VG : vHigh(x) ← −temp(x)
5. return (vHigh, HParent)

Corollary B.2 For a well-posed instance (G, V0) and a gradient value α ≥ 0, let (vLow[α], LParent) ← COMPVLOW(G, v0, α)
and (vHigh[α], HParent) ← COMPVHIGH(G, v0, α). Then, vLow[α], vHigh[α] are complete voltage assignments for
G such that, ∀x ∈ VG,

vLow[α](x) = min

{v0(t) + α · dist(x, t)}

t∈T (v0)

vHigh[α](x) = max
t∈T (v0)

{v0(t) − α · dist(t, x)}.

Moreover, the pointer arrays LParent, HParent satisfy ∀x /∈ T (v0), LParent(x), HParent(x) 6= null and

vLow[α](x) = vLow[α](LParent(x)) + α · ℓ(x, LParent(x)),
vHigh[α](x) = vHigh[α](HParent(x)) − α · ℓ(x, HParent(x)).

Algorithm 5: Algorithm COMPINFMIN(G, v0): Given a well-posed instance (G, v0), outputs a complete voltage assignment
v for G, extending v0 that minimizes (cid:13)

(cid:13)grad[v](cid:13)

(cid:13)∞.

1. α ← max{|grad[v0](e)| | e ∈ EG ∩ (T (v0) × T (v0))}.
2. EG ← EG \ (T (v0) × T (v0))
3. P ←STEEPESTPATH(G, v0).
4. α ← max{α, ∇P (v0)}
5. (vLow, LParent) ← COMPVLOW(G, v0, α)
6. (vHigh, HParent) ← COMPVHIGH(G, v0, α)
7. for x ∈ VG
8.
9.
10.
11. return v

then v(x) ← v0(x)
else v(x) ← 1

2 · (vLow(x) + vHigh(x)).

if x ∈ T (v0)

1. (vLow, LParent) ← COMPVLOW(G, v0, α)
2. (vHigh, HParent) ← COMPVHIGH(G, v0, α)
3. VG′ ← {x ∈ VG | vHigh(x) > vLow(x) }
4. EG′ ← {(x, y) ∈ EG | x, y ∈ VG′ }.

19

Algorithm 6: Algorithm COMPHIGHPRESSGRAPH(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0,
outputs a minimal induced subgraph G′ of G where every vertex has pressure[v0](·) > α.

5. G′ ← (V ′, E′, ℓ)
6. return G′

Proof of Lemma 4.3:

is equivalent to

vHigh[α](x) > vLow[α](x)

max
t∈T (v0)

{v0(t) − α · dist(t, x)} > min

{v0(t) + α · dist(x, t)},

t∈T (v0)

which implies that there exists terminals s, t ∈ T (v0) such that

thus,

Hence,

v0(t) − α · dist(t, x) > v0(s) + α · dist(x, s)

pressure[v0](x) ≥

v0(t) − v0(s)
dist(t, x) + dist(x, s)

> α.

v0(t) − v0(s)
dist(t, x) + dist(x, s)

= pressure[v0](x) > α.

v0(t) − α · dist(t, x) > v0(s) + α · dist(x, s)

So the inequality on vHigh and vLow implies that pressure is strictly greater than α. On the other hand, if pressure[v0](x) >
α, there exists terminals s, t ∈ T (v0) such that

which implies vHigh[α](x) > vLow[α](x).

✷

Algorithm 7: Algorithm STEEPESTPATH(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs a steepest
free terminal path P in (G, v0).

P ← VERTEXSTEEPESTPATH(G, v0, xi)

1. Sample uniformly random e ∈ EG. Let e = (x1, x2).
2. Sample uniformly random x3 ∈ VG.
3. for i = 1 to 3
4.
5. Let j ∈ arg maxj∈{1,2,3} ∇Pj (v0)
6. G′ ← COMPHIGHPRESSGRAPH(G, v0, ∇Pj (v0))
7. if EG′ = ∅,
8.
9.

then return Pj
else return STEEPESTPATH(G′, v0|VG′ )

1. while T (v0) 6= VG
2.
3.
4.
5. return v0

EG ← EG \ (T (v0) × T (v0))
P ← STEEPESTPATH(G, v0)
v0 ← ﬁx[v0, P ]

Algorithm 8: Algorithm COMPLEXMIN(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs lexG[v0].

Algorithm 9: Algorithm VERTEXSTEEPESTPATH(G,v0, x): Given a well-posed instance (G, v0), and a vertex x ∈ VG,
outputs a steepest terminal path in (G, v0) through x.

1. Using Dijkstra’s algorithm, compute dist(x, t) for all t ∈ T (v0)

20

y ← arg maxy∈T (v0)
if v0(x) ≥ v0(y)

|v0(x)−v0(y)|
dist(x,y)

then return a shortest path from x to y
else return a shortest path from y to x

2. if x ∈ T (v0)
3.
4.
5.
6.
7. else
8.
9.
10.
11.

for t /∈ T (v0), d(t) ← dist(x, t)
(t1, t2) ← STARSTEEPESTPATH(T (v0), v0|T (v0), d)
Let P1 be a shortest path from t1 to x. Let P2 be a shortest path from x to t2.
P ← (P1, P2). return P.

Algorithm 10: STARSTEEPESTPATH(T, v, d): Returns the steepest path in a star graph, with a single non-terminal connected
to terminals in T, with lengths given by d, and voltages given by v.

|v(t1)−v(t)|
d(t1)+d(t)

1. Sample t1 uniformly and randomly from T
2. Compute t2 ∈ arg maxt∈T
3. α ← |v(t2)−v(t1)|
d(t1)+d(t2)
4. Compute vlow ← mint∈T (v(t) + α · d(t))
5. Tlow ← {t ∈ T | v(t) > vlow + α · d(t)}
6. Compute vhigh ← maxt∈T (v(t) − α · d(t))
7. Thigh ← {t ∈ T | v(t) < vhigh − α · d(t)}
8. T ′ ← Tlow ∪ Thigh.
9. if T ′ = ∅
10.
11.

then if v(t1) ≥ v(t2) then return (t1, t2) else return (t2, t1)
else return STARSTEEPESTPATH(T ′, v|T ′, dT ′ )

B.1 Faster Lex-minimization

Algorithm 11: Algorithm COMPFASTLEXMIN(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs
lexG[v0].

1. while T (v0) 6= VG
2.
3. return v0

v0 ← FIXPATHSABOVEPRESS(G, v0, 0)

Algorithm 12: Algorithm FIXPATHSABOVEPRESS(G, v0, α): Given a well-posed instance (G, v0), with T (v0) 6= VG, and
a gradient value α, iteratively ﬁxes all paths with gradient > α.

EG ← EG \ (T (v0) × T (v0))
Sample uniformly random e ∈ EG. Let e = (x1, x2).
Sample uniformly random x3 ∈ VG.
for i = 1 to 3

Pi ← VERTEXSTEEPESTPATH(G, v0, xi)

Let j ∈ arg maxj∈{1,2,3} ∇Pj(v0)
G′ ← COMPHIGHPRESSGRAPH(G, v0, ∇Pj (v0))
if EG′ = ∅,

1. while T (v0) 6= VG
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.

then v0 ← ﬁx[v0, P ]
else Let G′

for i = 1, . . . , r

i, i = 1, . . . , r be the connected components of G′.

21

vi ← FIXPATHSABOVEPRESS(G′
for x ∈ VG′

i, set v0(x) ← vi(x)

i, v0|VG′

i

, ∇Pj (v0))

if α > 0 then G ←COMPHIGHPRESSGRAPH(G, v0, α)

13.
14.
15.
16. return v0

C Experiments on WebSpam: Testing More Algorithms

For completeness, in this appendix we show how a number of algorithms perform on the web spam experiment of
Section 6. We consider the following algorithms:

• RANDWALK along in-links. For a detailed description see Zhou et al. (2007). This algorithm essentially per-
forms a Personalized PageRank random walk from each vertex x and computes a spam-value for the vertex x by
taking a weighted average of the labels of the vertices where the random walk from x terminates. Also shown in
Section 6.

• DIRECTEDLEX, with edges in the opposite directions of links. This has the effect that a link to a spam host is

evidence of spam, and a link from a normal host is evidence of normality. Also shown in Section 6.

• RANDWALK along out-links.

• DIRECTEDLEX, with edges in the directions of links. This has the effect that a link from to a spam host is

evidence of spam, and a link to a normal host is evidence of normality.

• UNDIRECTEDLEX: Lex-minimization with links treated as undirected edges.

• LAPLACIAN: l2-regression with links treated as undirected edges.

• DIRECTED 1-NEAREST NEIGHBOR: Uses shortest distance along paths following out-links. Spam-ratio is
deﬁned distance from normal hosts, divided by distance to spam hosts. Sites are ﬂagged as spam when spam-
ratio exceeds some threshold. We also tried following paths along in-links instead, but that gave much worse
results.

We use the experimental setup described in Section 6. Results are shown in Figure 4. The alternative convention
for DIRECTEDLEX orients edges in the directions of links. This takes a link from a spam host to be evidence of
spam, and a link to a normal host to be evidence of normality. This approach performs signiﬁcantly worse than our
preferred convention, as one would intuitively expect. UNDIRECTEDLEX and LAPLACIAN approaches also perform
signiﬁcantly worse. DIRECTED 1-NEAREST NEIGHBOR performs poorly, demonstrating that DIRECTEDLEX is very
different from that approach. As observed by Zhou et al. (2007), sampling based on a random walk following out-links
performs worse than following in-links. Up to 60 % recall, DIRECTEDLEX performs best, both in the regime of 5 %
labels for training and in the regime of 20 % labels for training.

22

5 % labels for training

RandWalk along in-links
DirectedLex
RandWalk along out-links
DirectedLex, opposite link direction
UndirectedLex
Laplacian
Directed 1NN

0
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Recall

20 % labels for training

RandWalk along in-links
DirectedLex
RandWalk along out-links
DirectedLex, opposite link direction
UndirectedLex
Laplacian
Directed 1NN

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

i

i

n
o
s
c
e
r
P

i

i

n
o
s
c
e
r
P

0
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Recall

Figure 4: Recall and precision in the WebSpam classiﬁcation experiment. Each data point shown was computed as an average
over 100 runs. The largest standard deviation of the mean precision across the plotted recall values was less than 1.5 %. The
algorithm of Zhou et al. (2007) appears as RANDWALK (along in-links). We also show RANDWALK along out-links. Our directed
lex-minimization algorithm appears as DIRECTEDLEX. We also show DIRECTEDLEX with link directions reversed, along with
UNDIRECTEDLEX and LAPLACIAN.

D l0-Vertex Regularization Proofs

In this appendix, we prove Theorem 7.1 and Theorem 7.2. For the purposes of proving the second theorem, we intro-
duce an alternative version of problem (3). The optimization problem here requires us to minimize l0-regularization

23

budget required to obtain an inf-minimizer with gradient below a given threshold:

min
v∈IRn
subject to

(cid:13)
(cid:13)

v(T ) − v0(T )

0

gradG[v]

(cid:13)
∞ ≤ α.
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We will also need the following graph construction.

Deﬁnition D.1 The α-pressure terminal graph of a partially-labeled graph (G, v0) is a directed unweighted graph
Gα = (T (v0),

E if and only if there is a terminal path P from s to t in G with

E) such that (s, t) ∈

b

b

∇P (v0) > α.

Note that the α-pressure terminal graph has O(n) vertices but may be dense, even when G is not.

Algorithm 13: Algorithm TERM-PRESSURE: Given a well-posed instance (G, v0) and α ≥ 0, outputs α pressure terminal
graph Gα.
Initialize Gα with vertex set Vα = T (v0) and edge set
for each terminal s ∈ T (v0)

E = ∅.

1. Compute the distances to every other terminal t by running Dijktra’s algorithm, allowing shortest paths

b

2. Use the resulting distances to check for every other terminal t if there is a terminal path P from s to t with

that run through other terminals.

∇P (v0) > α. If there is, add edge (s, t) to

E.

Lemma D.2 The α-pressure terminal graph of a voltage problem (G, v0) can be computed in O((m + n log n)n) time
using algorithm TERM-PRESSURE (Algorithm 13).

b

Proof: The correctness of the algorithm follows from the fact that Dijkstra’s algorithm will identify all shortest
distances between the terminals, and the pressure check will ensure that terminal pairs (s, t) are added to
E if and
only if they are the endpoints of a terminal path P with ∇P (v0) > α. The running time is dominated by performing
Dijkstra’s algorithm once for each terminal. A single run of Dijkstra’s algorithm takes O(m + n log n) time, and this
✷
is performed at most n times, for a total running time of O((m + n log n)n).

b

We make three observations that will turn out to be crucial for proving Theorems 7.1 and 7.2.

Observation D.3 Gα is a subgraph of Gβ for α ≥ β.

Proof: Suppose edge (s, t) appears in Gα, then for some path P

∇P (v0) > α ≥ β,

so the edge also appears in Gβ.

Observation D.4 Gα is transitively closed.

Proof: Suppose edges (s, t) and (t, r) appear in Gα. Let P(s,t), P(t,r), P(s,r) be the respective shortest paths in G
between these terminal pairs. Then

∇P(s,r)(v0) =

v0(s) − v0(r)
ℓ(P(s,r))

≥

v0(s) − v0(r)
ℓ(P(s,t)) + ℓ(P(t,r))

=

v0(s) − v0(t) + v0(t) − v0(r)
ℓ(P(s,t)) + ℓ(P(t,r))

≥ min

v0(s) − v0(t)
ℓ(P(s,t))

,

 

v0(t) − v0(r)

ℓ(P(t,r)) !

> α.

So edge (s, r) also appears in Gα. This is sufﬁcient for Gα to be transitively closed.

24

(6)

✷

(7)

✷

Observation D.5 Gα is a directed acyclic graph.

Proof: Suppose for a contradiction that a directed cycle appears in Gα. Let s and t be two vertices in this cycle. Let
P(s,t) and P(t,s) be the respective shortest paths in G between these terminal pairs. Because Gα is transitively closed,
both edges (s, t) and (t, s) must appear in Gα. But (s, t) ∈

E implies

and similarly (t, s) ∈

E implies

b
This is a contradiction.

v0(s) − v0(t) > αℓ(P(s,t)) > 0,

b

v0(t) − v0(s) > αℓ(P(t,s)) > 0.

✷

The usefulness of the α-pressure terminal graph is captured in the following lemma. We deﬁne a vertex cover of a
directed graph to be a vertex set that constitutes a vertex cover in the same graph with all edges taken to be undirected.

Lemma D.6 Given a partially-labeled graph (G, v0) and a set U ⊆ V , there exists a voltage assignment v ∈ IRn that
satisﬁes

if and only if U is a vertex cover in the α-pressure terminal graph Gα of (G, v0).
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:9)

(cid:8)

t ∈ T (v0) : v(t) 6= v0(t)

⊆ U and

gradG[v]

∞ ≤ α,

Proof: We ﬁrst show the “only if” direction. Suppose for a contradiction that there exists a voltage assignment v for
which
∞ ≤ α, but U is not a vertex cover in Gα. Let (s, t) be an edge Gα which is not covered by U . The
presence of this edge in Gα implies that there exists a terminal path P from s to t in G for which

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∇P (v0) > α.

But, by Lemma 3.5 this means there is no assignment v for G which agrees with v0 on s and t and has
α. This contradicts our assumption.

∞ ≤
(cid:13)
Now we show the “if” direction. Consider an arbitrary vertex cover U of Gα. Suppose for a contradiction that
(cid:13)
⊆ U .

t ∈ T (v0) : v(t) 6= v0(t)

gradG[v]

(cid:13)
(cid:13)

gradG[v]

there does not exist a voltage assignment v for G with
Deﬁne a partial voltage assignment vU given by

∞ ≤ α and

(cid:8)

(cid:9)

vU (t) =

v0(t)
∗

(

(cid:13)
(cid:13)

(cid:13)
(cid:13)
if t ∈ T (v0) \ U
o.w.

∞ ≤ α. By
The preceding statement is equivalent to saying that there is no v that extends vU and has
Lemma 3.5, this means there is terminal path between s, t ∈ T (vU ) with gradient strictly larger than α. But this
means an edge (s, t) is present in Gα and is not covered. This contradicts our assumption that U is a vertex cover. ✷

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We are now ready to prove Theorem 7.2.

∞

(cid:13)
(cid:13)

Proof of Theorem 7.2: We describe and prove the algorithm OUTLIER. The algorithm will reduce problem (3)
to problem (6): Suppose v∗ is an optimal assignment for problem (3).
It achieves a maximum gradient α∗ =
gradG[v∗]
. Using Dijkstra’s algorithm we compute the pairwise shortest distances between all terminals in G.
From these distances and the terminal voltages, we compute the gradient on the shortest path between each terminal
(cid:13)
pair. By Lemma 3.5, α∗ must equal one of these gradients. So we can solve problem (3) by iterating over the set of
(cid:13)
gradients between terminals and solving problem (6) for each of these O(n2) gradients. Among the assignments with
v(T ) − v0(T )

0 ≤ k, we then pick the solution that minimizes
(cid:13)
(cid:13)

In fact, we can do better. By Observation D.3, Gα is a subgraph of Gβ for α ≥ β. This means a vertex cover
(cid:13)
of Gα is also a vertex cover of Gβ, and hence the minimum vertex cover for Gβ is at least as large as the minimum
(cid:13)
vertex cover for Gα. This means we can do a binary search on the set of O(n2) terminal gradients to ﬁnd the minimum
gradient for which there exists an assignment with
0 ≤ k. This way, we only make O(log n) calls to
v(T ) − v0(T )
problem (6), in order to solve problem (3).
(cid:13)
(cid:13)

We use the following algorithm to solve problem (6).

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∞

.

25

1. Compute the α-pressure terminal graph Gα of G using the algorithm TERM-PRESSURE.
2. Compute a minimum vertex cover U of Gα using the algorithm KONIG-COVER from Theorem 7.3.
3. Deﬁne a partial voltage assignment vU given by

vU (t) =

v0(t)
∗

(

if t ∈ T (v0) \ U,
otherwise.

4. Using Algorithm 5, compute voltages v that extend vU and output v.

From Lemma D.2, it follows that step 1 computes the α-pressure terminal graph in polynomial time. From The-
orem 7.3 it follows that step 2 computes the a minimum vertex cover of the α-pressure terminal graph in polynomial
time, because our observations D.4 and D.5 establish that the graph is a TC-DAG. From Lemma D.6 and Theorem 4.6,
it follows that the output voltages solve program (6).

✷

To prove Theorem 7.1, we use the standard greedy approximation algorithm for MIN-VC (Vazirani (2001)).

Theorem D.7 2-Approximation Algorithm for Vertex Cover. The following algorithm gives a 2-approximation to
the Minimum Vertex Cover problem on a graph G = (V, E).

0. Initialize U = ∅.
1. Pick an edge (u, v) ∈ E that is not covered by U .
2. Add u and v to the set U .
3. Repeat from step 1 if there are still edges not covered by U .
4. Output U .

We are now in a position to prove Theorem 7.1

Proof of Theorem 7.1: Given an arbitrary k and a partially-labeled graph (G, v0), let α∗ be the optimum value
of program (3). Observe that by Lemma D.6, this implies that Gα∗ has a vertex cover of size k. Given the partial
assignment v0, for every vertex set U , we deﬁne

vU (t) =

v0(t)
∗

(

if t ∈ T (v0) \ U
o.w.

We claim the following algorithm APPROX-OUTLIER outputs a voltage assignment v with

gradG[v]

∞ ≤ α∗

(cid:13)
(cid:13)

(cid:13)
(cid:13)

and

v(T ) − v0(T )

(cid:13)
(cid:13)

Algorithm APPROX-OUTLIER:

0 ≤ 2k.
(cid:13)
(cid:13)

0. Initialize U = ∅.
1. Using the algorithm STEEPESTPATH (Algorithm 7), ﬁnd a steepest terminal path in G w.r.t. vU . Denote
this path P and let s and t be its terminal endpoints. If there is no terminal path with positive gradient, skip
to step 4.

2. Add s and t to the set U .
3. If |U | ≤ 2k − 2 then repeat from step 1.
4. Using the algorithm COMPINFMIN (Algorithm 5), compute voltages v that extend vU and output v.

From the stopping conditions, it is clear that |U | ≤ 2k. If in step 1 we ever ﬁnd that no terminal paths have positive
∞ = 0 ≤ α∗, by Lemma 3.5. Similarly if we ﬁnd a steepest
gradient then our v that extends vU will have
(cid:13)
(cid:13)

gradG[v]

(cid:13)
(cid:13)

26

gradG[v]

∞ ≤ α∗.

∞ ≤ α∗.
path with gradient less than α∗ w.r.t. vU , then for this U there exists v that extends vU and has
This will continue to hold when if we add vertices to U . Therefore, for the ﬁnal U , there will exist an v that extends
vU and has

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

If we never ﬁnd a steepest terminal path P with ∇P (v0) ≤ α∗, then each steepest path we ﬁnd corresponds to an
edge in Gα∗ that is not yet covered by U and our algorithm in fact implements the greedy approximation algorithm
for vertex cover described in Theorem D.7. This implies that the ﬁnal U is a vertex cover of Gα∗ of size at most 2k.
∞ ≤ α∗. This
By Lemma D.6, this implies that there exists a voltage assignment u extending vU that has
implies by Theorem 4.6 that the v we output has
(cid:13)
(cid:13)
In all cases, the v we output extends vU , so

∞ ≤ α∗.

gradG[u]

(cid:13)
(cid:13)

✷

gradG[v]
v(T ) − v0(T )
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

0 ≤ |U | ≤ 2k.
(cid:13)
(cid:13)

E Proof of Hardness of l0 regularization for l2

We will prove Theorem 7.4, by a reduction from minimum bisection. To this end, let G = (V, E) be any graph. We
will reduce the minimum bisection problem on G to our regularization problem. Let n = |V |. The graph on which we
will perform regularization will have vertex set

V ∪

V ,

V is a set of n vertices that are in 1-to-1 correspondence with V . We assume that every edge in G has weight 1.
V to the corresponding vertex in V by an edge of weight B, for some large B to be
V to each other by edges of weight B3. So, we have a complete
V to V , and the original graph G on V .

where
We now connect every vertex in
determined later. We also connect all of the vertices in
graph of weight B3 edges on
b
The input potential function will be

V , a matching of weight B edges connecting

b

b

b

v(a) =

b
0 for a ∈
1 for a ∈ V .
b

(

V , and

b

Now set k = n/2. We claim that we will be able to determine the value of the minimum bisection from the solution
to the regularization problem.

If S is the set of vertices on which v and w differ, then we know that the w is harmonic on S: for every a ∈ S,

w(a) is the weighted average of the values at its neighbors. In the following, we exploit the fact that |S| ≤ n/2.

Claim E.1 For every a ∈ S ∩

V , w(a) ≤ 2/nB2.

Proof: Let a be the vertex in S ∩
w-value equal to 0 by edges of weight B3. On the other hand, a has only one neighbor that is not in
w-value at most 1, and it is connected to that vertex by an edge of weight B. Call that vertex c. We have

V that maximizes w(a). So, a is connected to at least n/2 neighbors in

V with
V , that vertex has

b

b

b

((n − 1)B3 + B)w(a) = Bw(c) +

B3w(b)

b

b
V ,b6=a
Xb∈

= Bw(c) +

B3w(b) +

B3w(b)

b
V ∩S,b6=a
Xb∈

B3w(a)

≤ B +

b
V ∩S,b6=a
Xb∈
≤ B + (n/2 − 1)B3w(a).

b
V −S
Xb∈

Subtracting (n/2 − 1)B3w(a) from both sides gives

((n/2)B3 + B)w(a) ≤ B,

which implies the claim.

Claim E.2 For a ∈ S ∩ V , w(a) ≤ n/B.

27

✷

V . Let’s call that neighbor c. We know that w(c) ≤ 2/B2n. On the
Proof: Vertex a has exactly one neighbor in
other hand, vertex a has fewer than n − 1 neighbors in V , and each of these have w-value at most 1. Let da denote the
degree of a in G. Then,

b

So,

Let

and

bisection.

and at most

(B + da)w(a) ≤ da + B

2
B2n

.

w(a) ≤

da + 2/Bn
da + B
n + (2/Bn)
B + n

≤

≤ n/B.

|S| = k = n/2.

T = S ∩ V,

t = |T | .

(n − t)B − 4/B
b

(n − t)B + tn2/B.

We now estimate the value of the regularized objective function. To this end, we assume that

We will prove that S ⊂ V and so S = T and t = n/2.

Let δ denote the number of edges on the boundary of T in V . Once we know that t = n/2, δ is the size of a

Claim E.3 The contribution of the edges between V and

V to the objective function is at least

Proof: For the lower bound, we just count the edges between vertices in V \ T and
edges, and each of them has weight B. The endpoint in V \ T has w-value 1, and the endpoint in
most 2/nB2. So, the contribution of these edges is at least

V . There are n − t of these
V has w-value at

b

(n − t)B(1 − 2/nB2)2 ≥ (n − t)B(1 − 4/nB2) ≥ (n − t)B − 4/B.

b

For the upper bound, we observe that the difference in w-values across each of these n − t edges is at most 1, so their
total contribution is at most

Since for every vertex a ∈ T , w(a) ≤ n/B, and also every vertex b ∈
edges between T and

V is at most

t(n/B)2B = tn2/B.

b

b

V , w(b) ≤ 2/nB2, the contribution due to

We will see that this is the dominant term in the objective function. The next-most important term comes from the

edges in G.

(n − t)B.

28

✷

✷

Claim E.4 The contribution of the edges in G to the objective function is at least

and at most

δ(1 − 2n/B)

δ + (t2/2)(n/B)2

δ(1 − 2n/B) and δ.

(t2/2)(n/B)2.

Proof: Let (a, b) ∈ E. If neither a nor b is in T , then w(a) = w(b) = 1, and so this edge has no contribution. If
a ∈ T but b 6∈ T , then the difference in w-values on them is between (1 − n/B) and 1. So, the contribution of such
edges to the objective function is between

Finally, if a and b are in T , then the difference in w-values on them is at most n/B, and so the contribution of all such
edges to the objective function is at most

Claim E.5 The edges between pairs of vertices in

V contribute at most 2/B to the objective function.

Proof: As 0 ≤ w(a) ≤ 2/B2n for every a ∈

V , every edge between two vertices in

V can contribute at most

b

As there are fewer than n2/2 such edges, their total contribution to the objective function is at most

B3(2/B2n)2 = 4/Bn2.
b

b

(n2/2)(4/Bn2) = 2/B.

Lemma E.6 If n ≥ 4 and B = 2n3, the value of the objective function is at least

and at most

(n − t)B + δ − 1/2

(n − t)B + δ + 1/3.

Proof: Summing the contributions in the preceding three claims, we see that the value of the objective function is at
least

(n − t)B − 4/B + δ(1 − 2n/B) ≥ (n − t)B + δ − 4/B − 2nδ/B

≥ (n − t)B + δ − n3/B
≥ (n − t)B + δ − 1/2,

as δ ≤ (n/2)2.

Similarly, the objective function is at most

(n − t)B + tn2/B + δ + (t2/2)(n/B)2 + 2/B ≤ (n − t)B + n3/2B + δ + n4/8B2 + 2/B
≤ (n − t)B + n3/2B + δ + 1/32n2 + 1/n3
≤ (n − t)B + δ + 1/3.

Claim E.7 If n ≥ 2 and B = 2n3, then S ⊂ V .

Proof: The objective function is minimized by making t as large as possible, so t = n/2 and S ⊂ V .

29

✷

✷

✷

✷

Theorem E.8 The value of the objective function reveals the value of the minimum bisection in G.

Proof: The value of the objective function will be between

and

(n/2)B + δ − 1/2

(n/2)B + δ + 1/3.

So, the objective function will be smallest when δ is as small as possible.

✷

Theorem E.8 immediately implies Theorem 7.4.

30

5
1
0
2
 
n
u
J
 
0
3
 
 
]

G
L
.
s
c
[
 
 
2
v
0
9
2
0
0
.
5
0
5
1
:
v
i
X
r
a

Algorithms for Lipschitz Learning on Graphs ∗†

Rasmus Kyng
Yale University
rasmus.kyng@yale.edu

Anup Rao
Yale University
anup.rao@yale.edu

Sushant Sachdeva
Yale University
sachdeva@cs.yale.edu

Daniel A. Spielman
Yale University
spielman@cs.yale.edu

July 1, 2015

Abstract

We develop fast algorithms for solving regression problems on graphs where one is given the value of a function
at some vertices, and must ﬁnd its smoothest possible extension to all vertices. The extension we compute is the
absolutely minimal Lipschitz extension, and is the limit for large p of p-Laplacian regularization. We present an
algorithm that computes a minimal Lipschitz extension in expected linear time, and an algorithm that computes
an absolutely minimal Lipschitz extension in expected time eO(mn). The latter algorithm has variants that seem
to run much faster in practice. These extensions are particularly amenable to regularization: we can perform l0-
regularization on the given values in polynomial time and l1-regularization on the initial function values and on graph
edge weights in time eO(m3/2).

Our deﬁnitions and algorithms naturally extend to directed graphs.

1 Introduction

We consider a problem in which we are given a weighted undirected graph G = (V, E, ℓ) and values v0 : T → R
on a subset T of its vertices. We view the weights ℓ as indicating the lengths of edges, with shorter length indicating
greater similarity. Our goal it to assign values to every vertex v ∈ V \T so that the values assigned are as smooth as
possible across edges. A minimal Lipschitz extension of v0 is a vector v that minimizes

max
(x,y)∈E

(ℓ(x, y))−1

v(x) − v(y)

,

(cid:12)
(cid:12)

(cid:12)
(cid:12)

subject to v(x) = v0(x) for all x ∈ T . We call such a vector an inf-minimizer. Inf-minimizers are not unique. So,
among inf-minimizers we seek vectors that minimize the second-largest absolute value of ℓ(x, y)−1
v(x) − v(y)
across edges, and then the third-largest given that, and so on. We call such a vector v a lex-minimizer. It is also known
(cid:12)
as an absolutely minimal Lipschitz extension of v0.
(cid:12)
These are the limit of the solution to p-Laplacian minimization problems for large p, namely the vectors that solve

(cid:12)
(cid:12)

(1)

(2)

min
v∈Rn

v|T =v0|T X(x,y)∈E

(ℓ(x, y))−p|v(x) − v(y)|p.

The use of p = 2 was suggested in the foundational paper of Zhu et al. (2003), and is particularly nice because it can
be obtained by solving a system of linear equations in a symmetric diagonally dominant matrix, which can be done

∗This research was partially supported by AFOSR Award FA9550-12-1-0175, NSF grant CCF-1111257, a Simons Investigator Award to Daniel

Spielman, and a MacArthur Fellowship.

†Code used in this work is available at https://github.com/danspielman/YINSlex

1

very quickly (Cohen et al. (2014)). The use of larger values of p has been discussed by Alamgir and Luxburg (2011),
and by Bridle and Zhu (2013), but it is much more complicated to compute. The fastest algorithms we know for this
problem require convex programming, and then require very high accuracy to obtain the values at most vertices. By
taking the limit as p goes to inﬁnity, we recover the lex-minimizer, which we will show can be computed quickly.

The lex-minimization problem has a remarkable amount of structure. For example, in uniformly weighted graphs
the value of the lex-minimizer at every vertex not in T is equal to the average of the minimum and maximum of the
values at its neighbors. This is analogous to the property of the 2-Laplacian minimizer that the value at every vertex
not in T equals the average of the values at its neighbors.

1.1 Contributions

We ﬁrst present several important structural properties of lex-minimizers in Section 3.2. As we shall point out, some
of these were known from previous work, sometimes in restricted settings. We state them generally and prove them
for completeness. We also prove that the lex-minimizer is as stable as possible under perturbations of v0 (Section 3.1).
The structure of the lex-minimization problem has led us to develop elegant algorithms for its solution. Both the
algorithms and their analyses could be taught to undergraduates. We believe that these algorithms could be used in
place of 2-Laplacian minimization in many applications.

We present algorithms for the following problems. Throughout, m = |E| and n = |V |.

Inf-minimization: An algorithm that runs in expected time O(m + n log n) (Section 4.3).

Lex-minimization: An algorithm that runs in expected time O(n(m + n log n)) (Section 4), along with a variant that

runs quickly in practice (Section 4.4).

l1-regularization of edge lengths for inf-minimization: The problem of minimizing (1) given a limited budget with
O(m3/2)
which one can increase edge lengths is a linear programming problem. We show how to solve it in time
with an interior point method by using fast Laplacian solvers (Section 8). The same algorithm can accommodate
l1-regularization of the values given in v0.

e

l0-regularization of vertex values for inf-minimization: We give a polynomial time algorithm for l0-regularization
of the values at vertices. That is, we minimize (1) given a budget of a number of vertices that can be proclaimed
outliers and removed from T (Section 7.1). We solve this problem by reducing it to the problem of computing
minimum vertex covers on transitively closed directed acyclic graphs, a special case of minimum vertex cover
that can be solved in polynomial time.

After any regularization for inf-minimization, we suggest computing the lex-minimizer. We ﬁnd the result for l0-
regularization of vertex values to be particularly surprising, especially because we prove that the analogous problem
for 2-Laplacian minimization is NP-Hard (Section 7.2).

All of our algorithms extend naturally to directed graphs (Section 5). This is in contrast with the problem of
minimizing 2-Laplacians on directed graphs, which corresponds to computing electrical ﬂows in networks of resistors
and diodes, for which fast algorithms are not presently known.

We present a few experiments on examples demonstrating that the lex-minimizer can overcome known deﬁcien-
cies of the 2-Laplacian minimizer (Section 1.2, Figures 1,2), as well as a demonstration of the performance of the
directed analog of our algorithms on the WebSpam dataset of Castillo et al. (2006) (Section 6). In the WebSpam prob-
lem we use the link structure of a collection of web sites to ﬂag some sites as spam, given a small number of labeled
sites known to be spam or normal.

1.2 Relation to Prior Work

We ﬁrst encountered the idea of using the minimizer of the 2-Laplacian given by (2) for regression and classiﬁca-
tion on graphs in the work of Zhu et al. (2003) and Belkin et al. (2004) on semi-supervised learning. These works
transformed learning problems on sets of vectors into problems on graphs by identifying vectors with vertices and
constructing graphs with edges between nearby vectors. One shortcoming of this approach (see Nadler et al. (2009),

2

e
g
a

t
l

 

o
V
d
e
r
r
e

f

n

I

1

0.8

0.6

0.4

0.2

0

-0.2

-0.4

-0.6

-0.8

-1

-4

50 lex
50 l2
100 lex
100 l2
500 lex
500 l2
1000 lex
1000 l2

0.25

0.2

r
o
r
r
e
 
1
l
 
n
a
e
M

0.15

0.1

0.05

0
5000 

lex
2-Lap
labels

-2

0

2
Vertex position on real line

4

6

8

Figure 1: Lex vs 2-Laplacian on 1D gaussian clus-
ters.

Figure 2: kNN graphs on samples from 4D cube.

10000

20000

40000

80000

Number of Vertices

Alamgir and Luxburg (2011), Bridle and Zhu (2013)) is that if the number of vectors grows while the number of la-
beled vectors remains ﬁxed, then almost all the values of the 2-Laplacian minimizer converge to the mean of the
labels on most natural examples. For example, Nadler et al. (2009) consider sampling points from two Gaussian
distributions centered at 0 and 4 on the real line. They place edges between every pair of points (x, y) with length
exp(|x − y|2 /2σ2) for σ = 0.4, and provide only the labels v0(0) = −1 and v0(4) = 1. Figure 1 shows the values
of the 2-Laplacian minimizer in red, which are all approximately zero. In contrast, the values of the lex-minimizer in
blue, which are smoothly distributed between the labeled points, are shown.

The “manifold hypothesis” (see Chapelle et al. (2010), Ma and Fu (2011)) holds that much natural data lies near a
low-dimensional manifold and that natural functions we would like to learn on this data are smooth functions on the
manifold. Under this assumption, one should expect lex-minimizers to interpolate well. In contrast, the 2-Laplacian
minimizers degrade (dotted lines) if the number of labeled points remains ﬁxed while the total number of points grows.
In Figure 2, we demonstrate this by sampling many points uniformly from the unit cube in 4 dimensions, form their
8-nearest neighbor graph, and consider the problem of regressing the ﬁrst coordinate. We performed 8 experiments,
varying the number of labeled points in {50, 100, 500, 1000}. Each data point is the mean average l1 error over 100
experiments. The plots for root mean squared error are similar. The standard deviation of the estimations of the mean
are within one pixel, and so are not displayed. The performance of the lex-minimizer (solid lines) does not degrade as
the number of unlabeled points grows.

Analogous to our inf-minimizers, minimal Lipschitz extensions of functions in Euclidean space and over more
general metric spaces have been studied extensively in Mathematics (Kirszbraun (1934), McShane (1934), Whitney
(1934)). von Luxburg and Bousquet (2003) employ Lipschitz extensions on metric spaces for classiﬁcation and relate
these to Support Vector Machines. Their work inspired improvements in classiﬁcation and regression in metric spaces
with low doubling dimension (Gottlieb et al. (2013), Gottlieb et al. (2013b)). Theoretically fast, although not actually
practical, algorithms have been given for constructing minimal Lipschitz extensions of functions on low-dimensional
Euclidean spaces (Fefferman (2009a), Fefferman and Klartag (2009), Fefferman (2009b)). Sinop and Grady (2007)
suggest using inf-minimizers for binary classiﬁcation problems on graphs. For this special case, where all of the
given values are either 0 or 1, they present an O(m + n log n) time algorithm for computing an inf-minimizer. The
case of general given values, which we solve in this paper, is much more complicated. To compensate for the non-
uniqueness of inf-minimizers, they suggest choosing the inf-minimizer that minimizes (2) with p = 2. We believe that
the lex-minimizer is a more natural choice.

The analog of our lex-minimizer over continuous spaces is called the absolutely minimal Lipschitz extension
(AMLE). Starting with the work of Aronsson (1967), there have been several characterizations and proofs of the ex-
istence and uniqueness of the AMLE (Jensen (1993), Crandall et al. (2001), Barles and Busca (2001), Aronsson et al.
(2004)). Many of these results were later extended to general metric spaces, including graphs (Milman (1999),
Peres et al. (2011), Naor and Shefﬁeld (2010), Shefﬁeld and Smart (2010)). However, to the best of our knowledge,
fast algorithms for computing lex-minimizers on graphs were not known. For the special case of undirected, un-
weighted graphs, Lazarus et al. (1999) presented both a polynomial-time algorithm and an iterative method. Oberman

3

(2011) suggested computing the AMLE in Euclidean space by ﬁrst discretizing the problem and then solving the cor-
responding graph problem by an iterative method. However, no run-time guarantees were obtained for either iterative
method.

2 Notation and Basic Deﬁnitions

Lexicographic Ordering. Given a vector r ∈ Rm, let πr denote a permutation that sorts r in non-increasing order
by absolute value, i.e., ∀i ∈ [m − 1], |r(πr(i))| ≥ |r(πr(i + 1))|. Given two vectors r, s ∈ Rm, we write r (cid:22) s to
indicate that r is smaller than s in the lexicographic ordering on sorted absolute values, i.e.

∃j ∈ [m],

r(πr(j))

<

s(πs(j))

and ∀i ∈ [j − 1],

r(πr(i))

=

s(πs(i))

or ∀i ∈ [m],

=

r(πr(i))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.
(cid:12)
(cid:12)

s(πs(i))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

Note that it is possible that r (cid:22) s and s (cid:22) r while r 6= s. It is a total relation: for every r and s at least one of r (cid:22) s
or s (cid:22) r is true.

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Graphs and Matrices. We will work with weighted graphs. Unless explicitly stated, we will assume that they are
undirected. For a graph G, we let VG be its set of vertices, EG be its set of edges, and ℓG : EG → R+ be the
assignment of positive lengths to the edges. We let |VG| = n, and |EG| = m. We assume ℓG is symmetric, i.e.,
ℓG(x, y) = ℓG(y, x). When G is clear from the context, we drop the subscript.

A path P in G is an ordered sequence of (not necessarily distinct) vertices P = (x0, x1, . . . , xk), such that
(xi−1, xi) ∈ E for i ∈ [k]. The endpoints of P are denoted by ∂0P = x0, ∂1P = xk. The set of interior vertices
of P is deﬁned to be int(P ) = {xi : 0 < i < k}. For 0 ≤ i < j ≤ k, we use the notation P [xi : xj] to denote the
k
subpath (xi, . . . , xj). The length of P is ℓ(P ) =
i=1 ℓ(xi−1, xi).
A function v0 : V → R ∪ {∗} is called a voltage assignment (to G). A vertex x ∈ V is a terminal with
respect to v0 iff v0(x) 6= ∗. The other vertices, for which v0(x) = ∗, are non-terminals. We let T (v0) denote the
set of terminals with respect to v0. If T (v0) = V, we call v0 a complete voltage assignment (to G). We say that an
assignment v : V → R ∪ {∗} extends v0 if v(x) = v0(x) for all x such that v0(x) 6= ∗.

Given an assignment v0 : V → R ∪ {∗}, and two terminals x, y ∈ T (v0) for which (x, y) ∈ E, we deﬁne the

P

gradient on (x, y) due to v0 to be

gradG[v0](x, y) =

v0(x) − v0(y)
ℓ(x, y)

.

It may be useful to view gradG[v0](x, y) as the current in the edge (x, y) induced by voltages v0. When v0 is a
complete voltage assignment, we interpret gradG[v0] as a vector in Rm, with one entry for each edge. However, for
convenience, we deﬁne gradG[v0](x, y) = −gradG[v0](y, x). When G is clear from the context, we drop the subscript.
A graph G along with a voltage assignment v to G is called a partially-labeled graph, denoted (G, v). We say
that a partially-labeled graph (G, v0) is a well-posed instance if for every maximal connected component H of G, we
have T (v0) ∩ VH 6= ∅.

A path P in a partially-labeled graph (G, v0) is called a terminal path if both endpoints are terminals. We deﬁne

∇P (v0) to be its gradient:

∇P (v0) =

v0(∂0P ) − v0(∂1P )
ℓ(P )

.

If P contains no terminal-terminal edges (and hence, contains at least one non-terminal), it is a free terminal path.

Lex-Minimization. An instance of the LEX-MINIMIZATION problem is described by a partially-labeled graph
(G, v0). The objective is to compute a complete voltage assignment v : VG → R extending v0 that lex-minimizes
grad[v].

Deﬁnition 2.1 (Lex-minimizer) Given a partially-labeled graph (G, v0), we deﬁne lexG[v0] to be a complete voltage
assignment to V that extends v0, and such that for every other complete assignment v′ : VG → R that extends v0, we
have gradG[lexG[v0]] (cid:22) gradG[v′]. That is, lexG[v0] achieves a lexicographically-minimal gradient assignment to the
edges.

We call lexG[v0] the lex-minimizer for (G, v0). Note that if T (v0) = VG, then trivially, lexG[v0] = v0.

4

3 Basic Properties of Lex-Minimizers

Lazarus et al. (1999) established that lex-minimizers in unweighted and undirected graphs exist, are unique, and may
be computed by an elementary meta-algorithm. We state and prove these facts for undirected weighted graphs, and
defer the discussion of the directed case to Section 5. We also state for directed and weighted graphs characterizations
of lex-minimizers that were established by Peres et al. (2011), Naor and Shefﬁeld (2010) and Shefﬁeld and Smart
(2010) for unweighted graphs. These results are essential for the analyses of our algorithms. We defer most proofs to
Appendix A.

Deﬁnition 3.1 A steepest ﬁxable path in an instance (G, v0) is a free terminal path P that has the largest gradient
∇P (v0) amongst such paths.

Observe that a steepest ﬁxable path with ∇P (v0) 6= 0 must be a simple path.
Deﬁnition 3.2 Given a steepest ﬁxable path P in an instance (G, v0), we deﬁne ﬁxG[v0, P ] : VG → R ∪ {∗} to be the
voltage assignment deﬁned as follows

ﬁxG[v0, P ](x) =

v0(∂0P ) − ∇P (v0) · ℓG(P [∂0P : x]) x ∈ int(P ) \ T (v0),
v0(x)

otherwise.

(

We say that the vertices x ∈ int(P ) are ﬁxed by the operation ﬁx[v0, P ]. If we deﬁne v1 = ﬁxG[v0, P ], where
P = (x0, . . . , xr) is the steepest ﬁxable path in (G, v0), then it is easy to argue that for every i ∈ [r], we have
grad[v1](xi−1, xi) = ∇P (see Lemma A.5). The meta-algorithm META-LEX, spelled out as Algorithm 1, entails
repeatedly ﬁxing steepest ﬁxable paths. While it is possible to have multiple steepest ﬁxable paths, the result of ﬁxing
all of them does not depend on the order in which they are ﬁxed.

Theorem 3.3 Given a well-posed instance (G, v0), the meta-algorithm META-LEX, which repeatedly ﬁxes steepest
ﬁxable paths, produces the unique lex-minimizer extending v0.

Corollary 3.4 Given a well-posed instance (G, v0) such that T (v0) 6= VG, let P be a steepest ﬁxable path in (G, v0).
Then, (G, ﬁx[v0, P ]) is also a well-posed instance, and lexG[ﬁx[v0, P ]] = lexG[v0].

Since a lex-minimal element must be an inf-minimizer, we also obtain the following corollary, that can also be

proved using LP duality.

Lemma 3.5 Suppose we have a well-posed instance (G, v0). Then, there exists a complete voltage assignment v
extending v0 such that

grad[v]

∞ ≤ α, iff every terminal path P in (G, v0) satisﬁes ∇P (v0) ≤ α.
(cid:13)
(cid:13)

3.1 Stability

(cid:13)
(cid:13)

The following theorem states that lexG[v0] is monotonic with respect to v0 and it respects scaling and translation of
v0.

Theorem 3.6 Let (G, v0) be a well-posed instance with T := T (v0) as the set of terminals. Then the following
statements hold.

1. For any c, d ∈ R, v1 a partial assignment with terminals T (v1) = T and v1(t) = cv0(t) + d for all t ∈ T .

Then, lexG[v1](i) = c · lexG[v0](i) + d for all i ∈ VG.

2. v1 a partial assignment with terminals T (v1) = T. Suppose further that v1(t) ≥ v0(t) for all t ∈ T. Then,

lexG[v1](i) ≥ lexG[v0](i) for all i ∈ VG.

As a corollary, the above theorem gives a nice stability property that lex-minimal elements satisfy.

Corollary 3.7 Given well-posed instances (G, v0), (G, v1) such that T := T (v0) = T (v1), let ǫ := maxt∈T |v0(t) −
v1(t)|. Then |lexG[v0](i) − lexG[v1](i)| ≤ ǫ for all i ∈ VG.

5

3.2 Alternate Characterizations

There are at least two other seemingly disparate deﬁnitions that are equivalent to lex-minimal voltages.

lp-norm Minimizers. As mentioned in the introduction, for a well-posed instance (G, v0) the lex-minimizer is also
the limit of lp minimizers. This follows from existing results about the limit of lp-minimizers (Egger and Huotari
(1990)) in afﬁne spaces, since {grad[v] | v is complete, v extends v0} forms an afﬁne subspace of Rm. Thus, we have
the following theorem:

Theorem 3.8 (Limit of lp-minimizers, follows from Egger and Huotari (1990)) For any p ∈ (1, ∞), given a well-
posed instance (G, v0) deﬁne vp to be the unique complete voltage assignment extending v0 and minimizing
p ,
i.e.

grad[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Then, limp→∞ vp = lexG[v0].

vp = arg min
v is complete
v extends v0 (cid:13)
(cid:13)

grad[v]

p .

(cid:13)
(cid:13)

Max-Min Gradient Averaging. Consider a well-posed instance (G, v0), and a complete voltage assignment v ex-
tending v0. If G is such that ℓ(e) = 1 for all e ∈ EG, it is easy to see that lex = lexG[v0] satisﬁes the following simple
condition for all x ∈ VG \ T (v0),

lex(x) =

1
2  

max
(x,y)∈EG

lex(y) + min

lex(z)

.

(x,z)∈EG

!

This condition should be contrasted to the optimality condition for l2-regularization on these instances, which gives
for all non-terminals x, the optimal voltage v satisﬁes v(x) = 1

y:(x,y)∈EG v(y).

deg(x)

To prove the above claim, consider locally changing lex at x and observe that the gradients of edges not incident
at x remain unchanged, and at least one of edges incident at x will have a strictly larger gradient, contradicting lex-
minimality. For general graphs, this condition of local optimality can still be characterized by a simple max-min
gradient averaging property as described below.

P

Deﬁnition 3.9 (Max-Min Gradient Averaging) Given a well-posed instance (G, v0), and a complete voltage as-
signment v extending v0, we say that v satisﬁes the max-min gradient averaging property (w.r.t. (G, v0)) if for every
x ∈ VG \ T (v0), we have

grad[v](x, y) = − min

grad[v](x, y).

max
y:(x,y)∈EG

y:(x,y)∈EG

As stated in the theorem below, lexG[v0] is the unique assignment satisfying max-min gradient averaging property.
Shefﬁeld and Smart (2010) proved a variant of this statement for weighted graphs. For completeness, we present a
proof in the appendix.

Theorem 3.10 Given a well-posed instance (G, v0), lexG[v0] satisﬁes max-min gradient averaging property. More-
over, it is the unique complete voltage assignment extending v0 that satisﬁes this property w.r.t. (G, v0).

An advantage of this characterization is that it can be veriﬁed quickly. This is particularly useful for implementations
for computing the lex-minimizer.

4 Algorithms

We now sketch the ideas behind our algorithms and give precise statements of our results. A full description of all the
algorithms is included in the appendix.

We deﬁne the pressure of a vertex to be the gradient of the steepest terminal path through it:

pressure[v0](x) = max{∇P (v0) | P is a terminal path in (G, v0) and x ∈ P }.

6

Observe that in a graph with no terminal-terminal edges, a free terminal path is a steepest ﬁxable path iff its gradient
is equal to the highest pressure amongst all vertices. Moreover, vertices that lie on steepest ﬁxable paths are exactly
the vertices with the highest pressure. For a given α > 0, in order to identify vertices with pressure exceeding α, we
compute vectors vHigh[α](x) and vLow[α](x) deﬁned as follows in terms of dist, the metric on V induced by ℓ:

vLow[α](x) = min

{v0(t) + α · dist(x, t)}

t∈T (v0)

vHigh[α](x) = max
t∈T (v0)

{v0(t) − α · dist(t, x)}.

4.1 Lex-minimization on Star Graphs

We ﬁrst consider the problem of computing the lex-minimizer on a star graph in which every vertex but the center is a
terminal. This special case is a subroutine in the general algorithm, and also motivates some of our techniques.

Let x be the center vertex, T be the set of terminals, and all edges be of the form (x, t) with t ∈ T . The initial
voltage assignment is given by v : T → R, and we abbreviate dist(x, t) by d(t) = ℓ(x, t). From Corollary 3.4 we know
that we can determine the value of the lex minimizer at x by ﬁnding a steepest ﬁxable path. By deﬁnition, we need to
ﬁnd t1, t2 ∈ T that maximize the gradient of the path from t1 to t2, ∇(t1, t2) = v(t1)−v(t2)
d(t2)+d(t2) . As observed above, this
is equivalent to ﬁnding a terminal with the highest pressure. We now present a simple randomized algorithm for this
problem that runs in expected linear time.

Given a terminal t1, we can compute its pressure α along with the terminal t2 such that |∇(t1, t2)| = α in time
O(|T |) by scanning over the terminals in T . Consider doing this for a random terminal t1. We will show that in linear
time one can then ﬁnd the subset of terminals T ′ ⊂ T whose pressure is greater than α. Assuming this, we complete
the analysis of the algorithm. If T ′ = ∅, t1 is a vertex with highest pressure. Hence the path from t1 to t2 is a steepest
ﬁxable path, and we return (t1, t2). If T ′ 6= ∅, the terminal with the highest pressure must be in T ′, and we recurse by
picking a new random t1 ∈ T ′. As the size of T ′ will halve in expectation at each iteration, the expected time of the
algorithm on the star is O(|T |).

To determine which terminals have pressure exceeding α, we observe that the condition ∃t2 : α < ∇(t1, t2) =
v(t1)−v(t2)
d(t1)+d(t2) , is equivalent to ∃t2 : v(t2)+αd(t2) < v(t1)−αd(t1). This, in turn, is equivalent to vLow[α](x) < v(t1)−
αd(t1). We can compute vLow[α](x) in deterministic O(|T |) time. Similarly, we can check if ∃t2 : α < ∇(t2, t1) by
checking if vHigh[α](x) > vt1 + αd(t1). Thus, in linear time, we can compute the set T ′ of terminals with pressure
exceeding α. The above algorithm is described in Algorithm 10.

Theorem 4.1 Given a set of terminals T, initial voltages v : T → R, and distances d : T → R+, STARSTEEPESTPATH(T, v, d)
returns (t1, t2) maximizing v(t1)−v(t2)

d(t1)+d(t2) , and runs in expected time O(|T |).

4.2 Lex-minimization on General Graphs

Theorem 3.3, tells us that META-LEX will compute lex-minimizers given an algorithm for ﬁnding a steepest ﬁxable
path in (G, v0). Recall that ﬁnding a steepest ﬁxable path is equivalent to ﬁnding a path with gradient equal to the
highest pressure amongst all vertices. In this section, we show how to do this in expected time O(m + n log n).

We describe an algorithm VERTEXSTEEPESTPATH that ﬁnds a terminal path P through any vertex x such that
∇P (v0) = pressure[v0](x) in expected O(m + n log n) time. Using Dijkstra’s algorithm, we compute dist(x, t) for
all t ∈ T. If x ∈ T (v0), then there must be a terminal path P that starts at x that has ∇P (v0) = pressure[v0](x). To
compute such a P we examine all t ∈ T (v0) in O(|T |) time to ﬁnd the t that maximizes |∇(x, t)| = |v(x)−v(t)|
, and
dist(x,t)
then return a shortest path between x and that t.

If x /∈ T (v0), then the steepest path through x between terminals t1 and t2 must consist of shortest paths between
x and t1 and between x and t2. Thus, we can reduce the problem to that of ﬁnding the steepest path in a star graph
where x is the only non-terminal and is connected to each terminal t by an edge of length dist(x, t). By Theorem 4.1,
we can ﬁnd this steepest path in O(|T |) expected time. The above algorithm is formally described as Algorithm 9.

Theorem 4.2 Given a well-posed instance (G, v0), and a vertex x ∈ VG, VERTEXSTEEPESTPATH(G, v0, x) returns
a terminal path P through x such that ∇P (v0) = pressure[v0](x), in O(m + n log n) expected time.

7

As in the algorithm for the star graph, we need to identify the vertices whose pressure exceeds a given α. For a ﬁxed
α, we can compute vLow[α](x) and vHigh[α](x) for all x ∈ VG using a simple modiﬁcation of Dijkstra’s algorithm in
O(m + n log n) time. We describe the algorithms COMPVHIGH, COMPVLOW for these tasks in Algorithms 3 and 4.
The following lemma encapsulates the usefulness of vLow and vHigh.

Lemma 4.3 For every x ∈ VG, pressure[v0](x) > α iff vHigh[α](x) > vLow[α](x).

It immediately follows that the algorithm COMPHIGHPRESSGRAPH(G, v0, α) described in Algorithm 6 computes

the vertex induced subgraph on the vertex set {x ∈ VG| pressure[v0](x) > α}.

We can combine these algorithms into an algorithm STEEPESTPATH that ﬁnds the steepest ﬁxable path in (G, v0)
in O(m + n log n) expected time. We may assume that there are no terminal-terminal edges in G. We sample an edge
(x1, x2) uniformly at random from EG, and a terminal x3 uniformly at random from VG. For i = 1, 2, 3, we compute
the steepest terminal path Pi containing xi. By Theorem 4.2, this can be done in O(m + n log n) expected time. Let α
be the largest gradient maxi ∇Pi. As mentioned above, we can identify G′, the induced subgraph on vertices x with
pressure exceeding α, in O(m + n log n) time. If G′ is empty, we know that the path Pi with largest gradient is a
steepest ﬁxable path. If not, a steepest ﬁxable path in (G, v0) must be in G′, and hence we can recurse on G′. Since
we picked a uniformly random edge, and a uniformly random vertex, the expected size of G′ is at most half that of G.
Thus, we obtain an expected running time of O(m + n log n). This algorithm is described in detail in Algorithm 7.

Theorem 4.4 Given a well-posed instance (G, v0) with EG ∩ (T (v0) × T (v0)) = ∅, STEEPESTPATH(G, v0) returns
a steepest ﬁxable path in (G, v0), and runs in O(m + n log n) expected time.

By using STEEPESTPATH in META-LEX, we get the COMPLEXMIN, shown in Algorithm 1. From Theorem 3.3 and
Theorem 4.4, we immediately get the following corollary.

Corollary 4.5 Given a well-posed instance (G, v0) as input, algorithm COMPLEXMIN computes a lex-minimizing
assignment that extends v0 in O(n(m + n log n)) expected time.

4.3 Linear-time Algorithm for Inf-minimization

Given the algorithms in the previous section, it is straightforward to construct an inﬁnity minimizer. Let α⋆ be the
gradient of the steepest terminal path. From Lemma 3.5, we know that the norm of the inf minimizer is α⋆. Considering
all trivial terminal paths (terminal-terminal edges), and using STEEPESTPATH, we can compute α⋆ in randomized
O(m+n log n) time. It is well known (McShane (1934); Whitney (1934)) that v1 = vLow[α⋆] and v2 = vHigh[α⋆] are
inf-minimizers. It is also known that 1
2 (v1 + v2) is the inf-minimizer that minimizes the maximum ℓ∞-norm distance
to all inf-minimizers. In the case of path graphs, this was observed by Gaffney and Powell (1976) and independently
by Micchelli et al. (1976). For completeness, the algorithm is presented as Algorithm 5, and we have the following
result.

Theorem 4.6 Given a well-posed instance (G, v0), COMPINFMIN(G, v0) returns a complete voltage assignment v
for G extending v0 that minimizes

∞ , and runs in randomized O(m + n log n) time.

grad[v]

4.4 Faster Algorithms for Lex-minimization

(cid:13)
(cid:13)

(cid:13)
(cid:13)

The lex-minimizer has additional structure that allows one to compute it by more efﬁcient algorithms. One observation
that leads to a faster implementation is that ﬁxing a steepest ﬁxable path does not increase the pressure at vertices,
provided that one appropriately ignores terminal-terminal edges. Thus, if G(α) is a subgraph that we identiﬁed with
pressure greater than α, we can iteratively ﬁx all steepest ﬁxable paths P in G(α) with ∇P > α. Another simple
observation is that if G(α) is disconnected, we can simply recurse on each of the connected components. A complete
description of an the algorithm COMPFASTLEXMIN based on these idea is given in Algorithm 11. The algorithm
provably computes lexG(v0), and it is possible to implement it so that the space requirement is only O(m + n).
Although, we are unable to prove theoretical bounds on the running time that are better than O(n(m + n log n)),
it runs extremely quickly in practice. We used it to perform the experiments in this paper. For random regular
graphs and Delaunay graphs, with n = 0.5 × 106 vertices and around 2 million edges m ∼ 1.5 − 2 × 106, it

8

takes a couple of minutes on a 2009 MacBook Pro. Similar times are observed for other model graphs of this
size such as random regular graphs and real world networks. An implementation of this algorithm may be found
at https://github.com/danspielman/YINSlex.

5 Directed Graphs

Our deﬁnitions and algorithms, including those for regularization, extend to directed graphs with only small modiﬁ-
cations. We view directed edges as diodes and only consider potential differences in the direction of the edge. For
a complete voltage assignment v on the vertices of a directed graph G, we deﬁne the directed gradient on (x, y) due
to v to be grad+
. Given a partially-labelled directed graph (G, v0), we say that a a
complete voltage assignment v is a lex-minimizer if it extends v0 and for other complete voltage assignment v′ that
extends v0 we have grad+
G[v′]. We say that a partially-labelled directed graph (G, v0) is a well-posed
directed instance if every free vertex appears in a directed path between two terminals.

G[v](x, y) = max

G[v] (cid:22) grad+

v(x)−v(y)
ℓ(x,y)

, 0

n

o

The main difference between the directed and undirected cases is that the directed lex-minimizer is not necessarily
unique. To maintain clarity of exposition, we chose to focus on undirected graphs so far. For directed graphs, we have
the following corresponding structural results.

Theorem 5.1 Given a well-posed instance (G, v0) on a directed graph G, there exists a lex-minimizer, and the set of
all lex-minimizers is a convex set. Moreover, for every two lex-minimizers v and v′, we have grad+

G[v] = grad+

G[v′].

However, note that in the case of directed graphs, the lex-minimizer need not be unique. We still have a weaker version
of Theorem 3.3 for directed graphs.

Theorem 5.2 Given a well-posed instance (G, v0) on a directed graph G, let v1 be the partial voltage assignment
extending v0 obtained by repeatedly ﬁxing steepest ﬁxable (directed) paths P with ∇P > 0. Then, any lex-minimizer
of (G, v0) must extend v1. Moreover, for every edge e ∈ EG \ (T (V1) × T (V1)), any lex-minimizer v of (G, v0) must
satisfy grad+[v](e) = 0.

When the value of the lex-minimizer at a vertex is not uniquely determined, it is constrained to an interval. In our
experiments, we pick the convention that when the voltage at a vertex is constrained to an interval (−∞, a] or [a, ∞),
we assign a to the terminal. When it is constrained to a ﬁnite interval, we assign a voltage closest to the median of the
original voltages.

6 Experiments on WebSpam

We demonstrate the performance of our lex-minimization algorithms on directed graphs by using them to detect spam
webpages as in Zhou et al. (2007). We use the dataset webspam-uk2006-2.0 described in Castillo et al. (2006).
This collection includes 11,402 hosts, out of which 7,473 (65.5 %) are labeled, either as spam or normal. Each host
corresponds to the collection of web pages it serves. Of the hosts, 1924 are labeled spam (25.7 % of all labels). We
consider the problem of ﬂagging some hosts as spam, given only a small fraction of the labels for training. We assign
a value of 1 to the spam hosts, and a value of 0 to the normal ones. We then compute a lex minimizer and examine the
effect of ﬂagging as spam all hosts with a value greater than some threshold.

Following Zhou et al. (2007), we create edges between hosts with lengths equal to the reciprocal of the number of
links from one to the other. We run our experiments only on the largest strongly connected component of the graph,
which contains 7945 hosts of which 5552 are labeled. 16 % of the nodes in this subgraph are labeled spam. To create
training and test data, for a given value p, we select a random subset of p % of the spam labels and a random subset
of p % of the normal labels to use for training. The remaining labels are used for testing. We report results for p = 5
and p = 20.

Again following Zhou et al. (2007), we plot the precision and recall of different choices of threshold for ﬂagging
pages as spam. Recall is the fraction of spam pages our algorithm ﬂags as spam, and precision is the fraction of pages
our algorithm ﬂags as spam that actually are spam. Amongst the algorithms studied by Zhou et al. (2007), the top

9

performer was their algorithm based on sampling according to a random-walk that follows in-links from other hosts.
We compare their algorithm with the classiﬁcation we get by directing edges in the opposite directions of links. This
has the effect that a link to a spam host is evidence of spamminess, and a link from a normal host is evidence of
normality.

Results are shown in Figure 3. While we are not able to reliably ﬂag all spam hosts, we see that in the range of
10-50 % recall, we are able to ﬂag spam with precision above 82 %. We see that the performance of directed lex-
minimization does not degrade rapidly when from the “large training set” regime of p = 20, to the “small training set”
regime of p = 5.

5 % labels for training

20 % labels for training

RandWalk
DirectedLex

RandWalk
DirectedLex

i

i

n
o
s
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
0.1

0.2

0.3

0.4

0.7

0.8

0.9

1

0.2

0.3

0.4

0.7

0.8

0.9

1

0.6
0.5
Recall

0.6
0.5
Recall

Figure 3: Recall and precision in the web spam classiﬁcation experiment. Each data point shown was computed as an average over
100 runs. The largest standard deviation of the mean precision across the plotted recall values was less than 1.3 %. The algorithm
of Zhou et al. (2007) appears as RANDWALK. Our directed lex-minimization algorithm appears as DIRECTEDLEX.

For comparison, in Appendix C, we show the performance of our algorithm and that of Zhou et al. (2007) both
with link directions reversed, as well as the performance of undirected lex-minimization and Laplacian inference, all
of which are signiﬁcantly worse.

7 l0-Regularization of Vertex Values

We now explain how we can accommodate noise in both the given voltages and in the given lengths of edges. We can
ﬁnd the minimum number of labels to ignore, or the minimum increase in edges lengths needed so that there exists an
extension whose gradients have l∞-norm lower than a given target. After determining which labels to ignore or the
needed increment in edge lengths, we recommend computing a lex minimizer.

The algorithms we present in this section are essentially the same for directed and undirected graphs.

7.1 l0-Vertex Regularization for Inf-minimization

The l0-regularization of vertex labels can be viewed as a problem of outlier removal: the vector we compute is allowed
to disagree with v0 on up to k terminals. Given a voltage assignment v and a subset T ⊂ V of the vertices, by v(T )
we mean the vector obtained by restricting v to T . We deﬁne the l0-Vertex Regularization for l∞ problem to be

where v(T ) is the vector of values of v on the terminals T .

min
v∈IRn

gradG[v]

∞

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

0 ≤ k,
(cid:13)
(cid:13)

subject to

v(T ) − v0(T )

(3)

In Appendix D, we describe an approximation algorithm APPROX-OUTLIER that approximately solves program (3).

The precise statement we prove in Appendix D is given in the following theorem.

i

i

n
o
s
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
0.1

10

Theorem 7.1 (Approximate l0-vertex regularization) The algorithm APPROX-OUTLIER takes a positive integer k
and a partially-labeled graph (G, v0), and outputs an assignment v with
0 ≤ 2k, and
∞ ≤
α∗, where α∗ is the optimum value of program (3). The algorithm runs in time O(k(m + n log n)).
(cid:13)
(cid:13)
(cid:13)
(cid:13)

In Appendix D, we also describe an algorithm OUTLIER that exactly solves program (3) in polynomial time, and we
prove its correctness.

v(T ) − v0(T )

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Theorem 7.2 (Exact l0-vertex regularization) The algorithm OUTLIER takes a positive integer k and a partially-
labeled graph (G, v0) solves program (3) exactly. The algorithm runs in polynomial time.

We give a proof of Theorem 7.2 in Appendix D. To do this, we reduce the program (3) to the problem of minimizing
the required l0-budget needed to achieve a ﬁxed gradient α using a binary search over a set of O(n2) gradients. This
latter problem we reduce in polynomial time to Minimum Vertex Cover (VC) on a transitively closed, directed acyclic
graph (a TC-DAG). VC on a TC-DAG can be solved exactly in polynomial time by a reduction to the Maximum
Bipartite Matching Problem (Fulkerson (1956)). The problem was phrased by Fulkerson as one of ﬁnding a maximum
antichain of a ﬁnite poset. Any transitively closed DAG corresponds directly to the comparability graph of a poset. A
maximum antichain of a poset is a maximum independent set of a the comparability graph of the poset, and hence its
complement is a minimum vertex cover of the comparability graph. We refer to the algorithm developed by Fulkerson
as KONIG-COVER.

Theorem 7.3 The algorithm KONIG-COVER computes a minimum vertex cover for any transitively closed DAG G in
polynomial time.

7.2 Hardness of l0 regularization for l2

The result that l0-regularized inf-minimization can be solved exactly in polynomial time is surprising, especially
because the analogous problem for 2-Laplacian minimization turns out to be NP-Hard.

We deﬁne the the l0 vertex regularization for l2 for a partially-labeled graph (G, v0) and an integer k by

min
v∈Rn:kv(T )−v0(T )k0

≤k

vT Lv,

where L is the Laplacian of G.

Theorem 7.4 l0 vertex regularization for l2 is NP-Hard.

In Appendix E we prove Theorem 7.4 by giving a polynomial time (Karp) reduction from the NP-Hard minimum
bisection problem to l0 vertex regularization for l2.

8 l1-Edge and Vertex Regularization of Inf-minimizers

Consider a partially-labeled graph (G, v0) and an α > 0. The set of voltage assignments given by

v : v extends v0 and

gradG[v]

∞ ≤ α

n

(cid:13)
(cid:13)

(cid:13)
(cid:13)

o

is convex. Going further, let us consider the edge lengths in a graph to be speciﬁed by a vector ℓ ∈ IRE. Now the set
of voltages v and and lengths ℓ which achieve kgradG(ℓ)[v]k∞ ≤ α is jointly convex in v and ℓ. To see this, observe
that

kgradG(ℓ)[v]k∞ ≤ α ⇔ ∀(u, v) ∈ E : −αℓ(u, v) ≤ v(u) − v(v) ≤ αℓ(u, v).
Furthermore, the condition “v extends v0” is a linear constraint on v, which we express as v(T ) = v0(T ). From
the above, it is clear that the gradient condition corresponds to a convex set, as it is an intersection of half-spaces.
These half-spaces are given by O(m) linear inequalities. We can leverage this to phrase many regularized variants of
inf-minimization as convex programs, and in some cases linear programs.

(4)

11

For example, we may consider a variant of inf-minimization combined with an l1-budget for changing lengths of
edges and values on terminals. Given a parameter γ > 0 which speciﬁes the relative cost of regularizing terminals to
regularizing edges, the problem is as follows

arg min
v∈IRn,s∈IRm,s≥0

ksk1 + γ

v(T ) − v0(T )

1

subject to

gradG(ℓ+s)[v]

≤ α.

(5)

(cid:13)
(cid:13)
From our observation (4), it follows that problem (5) may be expressed as a linear program with O(n) variables
and O(m) constraints. We can use ideas from Daitch and Spielman (2008) to solve the resulting linear program in
O(m1.5) by an interior point method with a special purpose linear equation solver. The reason is that the linear
time
equations the IPM must solve at each iteration may be reduced to linear equations in symmetric, diagonally dominant
matrices, and these may be solved in nearly-linear time (Cohen et al. (2014)).

(cid:13)
(cid:13)

e

(cid:13)
(cid:13)
(cid:13)

∞

(cid:13)
(cid:13)
(cid:13)

Conclusion. We propose the use of inf and lex minimizers for regression on graphs. We present simple algorithms
for computing them that are provably fast and correct, and can also be implemented efﬁciently. We also present a
framework and polynomial time algorithms for regularization in this setting. The initial experiments reported in the
paper indicate that these algorithms give pretty good results on real and synthetic datasets. The results seem to compare
quite favorably to other algorithms, particularly in the regime of tiny labeled sets. We are testing these algorithms on
several other graph learning questions, and plan to report on them in a forthcoming experimental paper. We believe
that inf and lex minimizers, and the associated ideas presented in the paper, should be useful primitives that can be
proﬁtably combined with other approaches to learning on graphs.

We thank anonymous reviewers for helpful comments. We thank Santosh Vempala and Bartosz Walczak for pointing
out that it was already known how to compute a minimum vertex cover of a transitively closed DAG in polynomial
time.

Acknowledgements

References

Morteza Alamgir
In Advances
Information Processing
http://books.nips.cc/papers/files/nips24/NIPS2011_0278.pdf.

and Ulrike V. Luxburg.

transition
24,

in
pages

in Neural

Systems

Phase

the

family
379–387.

of
2011.

p-resistances.
URL

Gunnar Aronsson. Extension of functions satisfying lipschitz conditions. Arkiv fr Matematik, 6(6):551–561, 1967.

ISSN 0004-2080. doi: 10.1007/BF02591928. URL http://dx.doi.org/10.1007/BF02591928.

Gunnar Aronsson, Michael G. Crandall, and Petri Juutinen. A tour of the theory of absolutely minimizing functions.
ISSN 0273-0979. doi: 10.1090/S0273-0979-04-01035-3.

Bull. Amer. Math. Soc. (N.S.), 41(4):439–505, 2004.
URL http://dx.doi.org/10.1090/S0273-0979-04-01035-3.

Guy Barles and J´erˆome Busca. Existence and comparison results for fully nonlinear degenerate elliptic equations

without zeroth-order term. Comm. Partial Differential Equations, 26:2323–2337, 2001.

Mikhail Belkin, Irina Matveeva, and Partha Niyogi.

Regularization and semi-supervised learning on large
In Learning Theory, volume 3120 of Lecture Notes in Computer Science, pages 624–638.
doi: 10.1007/978-3-540-27819-1 43. URL

graphs.
Springer Berlin Heidelberg, 2004.
http://dx.doi.org/10.1007/978-3-540-27819-1_43.

ISBN 978-3-540-22282-8.

Nick Bridle and Xiaojin Zhu. p-voltages: Laplacian regularization for semi-supervised learning on high-dimensional

data. In Eleventh Workshop on Mining and Learning with Graphs (MLG2013), 2013.

12

Carlos Castillo, Debora Donato, Luca Becchetti, Paolo Boldi, Stefano Leonardi, Massimo Santini, and Sebastiano
Vigna. A reference collection for web spam. SIGIR Forum, 40(2):11–24, December 2006. ISSN 0163-5840. doi:
10.1145/1189702.1189703. URL http://doi.acm.org/10.1145/1189702.1189703.

Olivier Chapelle, Bernhard Schlkopf, and Alexander Zien. Semi-Supervised Learning. The MIT Press, 1st edition,

2010. ISBN 0262514125, 9780262514125.

Michael B Cohen, Rasmus Kyng, Gary L Miller, Jakub W Pachocki, Richard Peng, Anup B Rao, and Shen Chen Xu.
Solving SDD linear systems in nearly m log1/2 n time. In Proceedings of the 46th Annual ACM Symposium on
Theory of Computing, pages 343–352. ACM, 2014.

M.G. Crandall, L.C. Evans, and R.F. Gariepy. Optimal lipschitz extensions and the inﬁnity laplacian. Calculus of Vari-
ations and Partial Differential Equations, 13(2):123–139, 2001. ISSN 0944-2669. doi: 10.1007/s005260000065.
URL http://dx.doi.org/10.1007/s005260000065.

Samuel I. Daitch and Daniel A. Spielman. Faster approximate lossy generalized ﬂow via interior point algo-
rithms.
In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing, STOC ’08, pages
451–460, New York, NY, USA, 2008. ACM. ISBN 978-1-60558-047-0. doi: 10.1145/1374376.1374441. URL
http://doi.acm.org/10.1145/1374376.1374441.

Alan Egger and Robert Huotari. Rate of convergence of the discrete polya algorithm. Journal of Approximation
ISSN 0021-9045. doi: http://dx.doi.org/10.1016/0021-9045(90)90070-7. URL

Theory, 60(1):24 – 30, 1990.
http://www.sciencedirect.com/science/article/pii/0021904590900707.

Charles Fefferman. Whitney’s extension problems and interpolation of data.

(N.S.), 46(2):207–220, 2009a.
http://dx.doi.org/10.1090/S0273-0979-08-01240-8.

ISSN 0273-0979.

doi:

10.1090/S0273-0979-08-01240-8.

Bull. Amer. Math. Soc.
URL

Charles Fefferman. Fitting a [image] -smooth function to data, iii. Annals of Mathematics, 170(1):pp. 427–441, 2009b.

ISSN 0003486X. URL http://www.jstor.org/stable/40345469.

Charles Fefferman and Bo’az Klartag. Fitting a cm -smooth function to data i. Annals of Mathematics, 169(1):pp.

315–346, 2009. ISSN 0003486X. URL http://www.jstor.org/stable/40345445.

D. R. Fulkerson. Note on dilworths decomposition theorem for partially ordered sets. Proc. Amer. Math. Soc, 1956.

P.W. Gaffney and M.J.D. Powell. Optimal interpolation. In Numerical Analysis, volume 506 of Lecture Notes in Math-
ematics, pages 90–99. Springer Berlin Heidelberg, 1976. ISBN 978-3-540-07610-0. doi: 10.1007/BFb0080117.
URL http://dx.doi.org/10.1007/BFb0080117.

L.-A. Gottlieb, A. Kontorovich, and R. Krauthgamer. Efﬁcient classiﬁcation for metric data. CoRR, abs/1306.2547,

2013. URL http://arxiv.org/abs/1306.2547.

L.-A. Gottlieb, A. Kontorovich, and R. Krauthgamer. Efﬁcient regression in metric spaces via approximate lipschitz
extension. In Similarity-Based Pattern Recognition, volume 7953 of Lecture Notes in Computer Science, pages
43–58. Springer Berlin Heidelberg, 2013b. ISBN 978-3-642-39139-2. doi: 10.1007/978-3-642-39140-8 3. URL
http://dx.doi.org/10.1007/978-3-642-39140-8_3.

Robert Jensen. Uniqueness of lipschitz extensions: Minimizing the sup norm of the gradient. Archive for Ra-
doi: 10.1007/BF00386368. URL

ISSN 0003-9527.

tional Mechanics and Analysis, 123(1):51–74, 1993.
http://dx.doi.org/10.1007/BF00386368.

M. Kirszbraun. ber die zusammenziehende und lipschitzsche transformationen. Fundamenta Mathematicae, 22(1):

77–108, 1934. URL http://eudml.org/doc/212681.

13

Andrew J. Lazarus, Daniel E. Loeb,

James G. Propp, Walter R. Stromquist,

Combinatorial games under

man.
229 – 264,
http://www.sciencedirect.com/science/article/pii/S0899825698906765.

http://dx.doi.org/10.1006/game.1998.0676.

and Economic Behavior,

ISSN 0899-8256.

auction play.

Games

1999.

doi:

and Daniel H. Ull-
27(2):
URL

Yunqian Ma and Yun Fu. Manifold Learning Theory and Applications. CRC Press, Inc., Boca Raton, FL, USA, 1st

edition, 2011. ISBN 1439871094, 9781439871096.

E. J. McShane. Extension of range of functions. Bull. Amer. Math. Soc., 40(12):837–842, 12 1934. URL

http://projecteuclid.org/euclid.bams/1183497871.

C.A. Micchelli, T.J. Rivlin,

and S. Winograd.

merische Mathematik, 26(2):191–200, 1976.
http://dx.doi.org/10.1007/BF01395972.

The optimal
ISSN 0029-599X.

recovery of
doi:

smooth functions.
10.1007/BF01395972.

Nu-
URL

V. A. Milman.

Absolutely minimal extensions of

functions on metric spaces.

1999.

URL

http://iopscience.iop.org/1064-5616/190/6/A05/pdf/MSB_190_6_A05.pdf.

Boaz Nadler, Nathan Srebro, and Xueyuan Zhou. Statistical analysis of semi-supervised learning: The limit of inﬁnite
unlabelled data. 2009. URL http://ttic.uchicago.edu/˜nati/Publications/NSZnips09.pdf.

A. Naor and S. Shefﬁeld. Absolutely minimal Lipschitz extension of tree-valued mappings. CoRR, abs/1005.2535,

May 2010. URL http://arxiv.org/abs/1005.2535.

A. M. Oberman. Finite difference methods for the Inﬁnity Laplace and p-Laplace equations. CoRR, abs/1107.5278,

July 2011. URL http://arxiv.org/abs/1107.5278.

Yuval Peres, Oded Schramm, Scott Shefﬁeld, and DavidB. Wilson.

Tug-of-war and the inﬁnity lapla-
In Selected Works of Oded Schramm, Selected Works in Probability and Statistics, pages 595–
doi: 10.1007/978-1-4419-9675-6 18. URL

cian.
638. Springer New York, 2011.
http://dx.doi.org/10.1007/978-1-4419-9675-6_18.

ISBN 978-1-4419-9674-9.

S. Shefﬁeld and C. K. Smart. Vector-valued optimal Lipschitz extensions. CoRR, abs/1006.1741, June 2010. URL

http://arxiv.org/abs/1006.1741.

Ali Kemal Sinop and Leo Grady. A seeded image segmentation framework unifying graph cuts and random walker
which yields a new algorithm. In Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on,
pages 1–8. IEEE, 2007.

Vijay V. Vazirani. Approximation Algorithms. Springer-Verlag New York, Inc., New York, NY, USA, 2001. ISBN

3-540-65367-8.

Ulrike von Luxburg and Olivier Bousquet. Distance-based classiﬁcation with lipschitz functions.

In Learn-
ing Theory and Kernel Machines, volume 2777 of Lecture Notes in Computer Science, pages 314–328.
doi: 10.1007/978-3-540-45167-9 24. URL
Springer Berlin Heidelberg, 2003.
http://dx.doi.org/10.1007/978-3-540-45167-9_24.

ISBN 978-3-540-40720-1.

Hassler Whitney.

Analytic extensions of differentiable functions deﬁned in closed sets.

tions of
http://www.jstor.org/stable/1989708.

the American Mathematical Society, 36(1):pp. 63–89, 1934.

ISSN 00029947.

Transac-
URL

Dengyong Zhou, Christopher J. C. Burges, and Tao Tao. Transductive link spam detection.

In Proceedings
of the 3rd International Workshop on Adversarial Information Retrieval on the Web, AIRWeb ’07, pages 21–
ISBN 978-1-59593-732-2. doi: 10.1145/1244408.1244413. URL
28, New York, NY, USA, 2007. ACM.
http://doi.acm.org/10.1145/1244408.1244413.

Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic

functions. In IN ICML, pages 912–919, 2003.

14

A Basic Properties of Lex-Minimizers

A.1 Meta Algorithm

Algorithm 1: Algorithm META-LEX: Given a well-posed instance (G, v0), outputs lexG[v0].
for i = 1, 2, . . . :

1. if T (vi−1) = VG, then return vi−1.
2. E′ = EG \ (T (vi−1) × T (vi−1)), G′ := (VG, E′).
3. Let P ⋆
4. vi ← ﬁx[vi−1, P ⋆
i ].

i be a steepest ﬁxable path in (G′, vi−1). Let α⋆

i ← ∇P ⋆(vi−1).

In this subsection, we prove the results that appeared in section 2. We start with a simple observation.

Proposition A.1 Given a well-posed instance (G, v0) such that T (v0) 6= V, let P be a steepest ﬁxable path in (G, v0).
Then, ﬁx[v0, P ] extends v0, and (G, ﬁx[v0, P ]) is also a well-posed instance.

The properties we prove below do not depend on the choice of the steepest ﬁxable path.

Proposition A.2 For any well-posed instance (G, v0), with |VG| = n, META-LEX(G, v0) terminates in at most n
iterations, and outputs a complete voltage assignment v that extends v0.

Proof of Proposition A.2: By Proposition A.1, at any iteration i, vi−1 extends v0 and (G′, vi−1) is a well-posed
instance. META-LEX only outputs vi−1 iff T (vi−1) = V, which means vi−1 is a complete voltage assignment. For
any vi−1 that is not complete, for any x ∈ V \T (vi−1), we must have a free terminal path in (G′, vi−1) that contains x.
i exists in (G′, vi−1). Since P ⋆
Hence, a steepest ﬁxable path P ⋆
i ] ﬁxes the voltage
i
✷
for at least one non-terminal. Thus, META-LEX(G, v0) must complete in at most n iterations.

is a free terminal path, ﬁx[vi−1, P ⋆

For the following lemmas, consider a run of META-LEX with well-posed instance (G, v0) as input. Let vout be the
complete voltage assignment output by META-LEX. Let Ei be the set of edges E′ and Gi be the graph G′ constructed
in iteration i of META-LEX.

Lemma A.3 For every edge e ∈ Ei−1 \ Ei, we have

grad[vout](e)

≤ α⋆

i . Moreover, α⋆

i is non-increasing with i.

Proof of Lemma A.3: Let P ⋆
i = (x0, . . . , xr) be a steepest ﬁxable path in iteration i (when we deal with instance
(Gi−1, vi−1)). Consider a terminal path Pi+1 in (Gi, vi) such that {∂0Pi+1, ∂1Pi+1} ∩ (T (vi) \ T (vi−1)) 6= ∅. We
i . On the contrary, assume that ∇Pi+1(vi) > α⋆
claim that ∇Pi+1(vi) ≤ α⋆
i . Consider the case ∂0Pi+1 ∈ T (vi) \
T (vi−1), ∂1P1 ∈ T (vi−1). By the deﬁnition of vi, we must have ∂0Pi+1 = xj for some j ∈ [r − 1]. Let P ′
i+1 be the
path formed by joining paths P ⋆

i+1 is a free terminal path in (Gi−1, vi−1). We have,

i [x0 : xj] and Pi+1. P ′

(cid:12)
(cid:12)

(cid:12)
(cid:12)

vi−1(x0) − vi−1(∂1Pi+1) = (vi(x0) − vi(xj )) + (vi(∂0Pi+1) − vi(∂1Pi+1))
i · ℓ(P ′

i · ℓ(Pi+1) = α⋆

i [x0 : xj]) + α⋆

i · ℓ(P ⋆

> α⋆

i+1),

giving ∇P ′
The other cases can be handled similarly.

i+1(vi) > α⋆

i , which is a contradiction since the steepest ﬁxable path P ⋆
i

in (Gi−1, vi−1) has gradient α⋆
i .

Applying the above claim to an edge e ∈ Ei−1 \ Ei, whose gradient is ﬁxed for the ﬁrst time in iteration i, we
i . If v is the complete voltage assignment output by META-LEX, since v extends vi+1,
i , implying

i . Applying the claim to the symmetric edge, we obtain −grad[vout](e) ≤ α⋆

obtain that grad[vi+1](e) ≤ α⋆
we get grad[vout](e) ≤ α⋆
|grad[vout](e)| ≤ α⋆
i .

Consider any free terminal path Pi+1 in (Gi, vi). If Pi+1 is also a terminal path in (Gi−1, vi−1), it is a free
terminal path in (Gi−1, vi−1). In addition, since a steepest ﬁxable path P ⋆
i , we get
i
∇Pi+1(vi) = ∇Pi+1(vi−1) ≤ α⋆
i . Otherwise, we must have {∂0Pi+1, ∂1Pi+1} ∩ (T (vi) \ T (vi−1)) 6= ∅, and we can
deduce ∇Pi+1(vi) ≤ α⋆
i using the above claim. Thus, all free terminal paths Pi+1 in (Gi, vi) satisfy ∇Pi+1(vi) ≤ α⋆
i .
✷
In particular, α⋆

in (Gi−1, vi−1) has ∇P ⋆

i = α⋆

i is non-increasing with i.

i+1(vi) ≤ α⋆

i+1 = ∇P ⋆

i . Thus, α⋆

15

Lemma A.4 For any complete voltage assignment v for G that extends v0, if v 6= vout, we have grad[v] 6(cid:22) grad[vout],
and hence grad[vout] (cid:22) grad[v].

Proof of Lemma A.4: Consider any complete voltage assignment v for G that extends v0, such that v 6= vout. Thus,
there exists a unique i such that v extends vi−1 but does not extend vi. We will argue that grad[v] 6(cid:22) grad[vout], and
hence grad[vout] (cid:22) grad[v]. For every edge e ∈ E \ Ei−1 that has been ﬁxed so far, grad[v](e) = grad[vi−1](e) =
grad[vout](e), and hence we can ignore these edges.

Since v extends vi−1 but not vi, there exists an x ∈ T (vi) \ T (vi−1) such that v(x) 6= vi(x) = vout(x). Assume
i picked

i = (x0, . . . , xr) is the steepest ﬁxable path with gradient α⋆

v(x) < vi(x) (the other case is symmetric). If P ⋆
in iteration i, we must have x = xj for some j ∈ [r − 1]. Thus,

j

j

(v(xk−1) − v(xk)) = v(x0) − v(xj ) > vi(x0) − vi(xj ) = α⋆

i · ℓ(P ⋆

i [x0 : xj ]) = α⋆
i ·

ℓ(xk−1, xk).

Xk=1

Xk=1
Thus, for some k ∈ [j], we must have grad[v](xk−1, xk) > α⋆
is a path in Gi−1, we have {xk−1, xk} 6⊆
T (vi−1). This gives (xk−1, xk) ∈ (Ei−1 \ Ei). But then, from Lemma A.3, it follows that for all e ∈ (Ei−1 \ Ei), we
✷
have |grad[vout](e)| ≤ α⋆

i . Thus, we have grad[v] 6(cid:22) grad[vout].

i . Since P ∗
i

Lemma A.5 Let P = (x0, . . . , xr) be a steepest ﬁxable path such that it does not have any edges in T (v0) × T (v0)
and v1 = ﬁxG[v0, P ]. Then for every i ∈ [r], we have grad[v1](xi−1, xi) = ∇P.

Proof of Lemma A.5: Suppose this is not true and let j ∈ [r] be the minimum number such that grad[v1](xj−1, xj) 6=
∇P. By deﬁnition of v1 we would necessarily have j < r and vj ∈ T (v0). Suppose grad[v1](xj−1, xj ) < ∇P. We
would then have v1(x0) − v1(xj ) < ∇P ∗ ℓ(P [x0 : xj]). Since P does not have any edges in T (v0) × T (v0),
P1 := (xj, ..., xr) would be a free terminal path with ∇P1 > ∇P. This is a contradiction. Other cases can be ruled
out similarly.

✷

Proof of Theorem 3.3: Consider an arbitrary run of META-LEX on (G, v0). Let vout be the complete voltage
assignment output by META-LEX. Proposition A.1 implies that vout extends v0. Lemma A.4 implies that for any
complete voltage assignment v 6= vout that extends v0, we have grad[vout] (cid:22) grad[v]. Thus, vout is a lex-minimizer.
Moreover, the lemma also gives that for any such v, grad[v] 6(cid:22) grad[vout]. and hence vout is a unique lex-minimizer.
Thus, vout is the unique voltage assignment satisfying Def. 2.1, and we denote it as lexG[v0]. Since we started with an
✷
arbitrary run of META-LEX, uniqueness implies that every run of META-LEX on (G, v0) must output lexG[v0].

Proof of Lemma 3.5: Suppose we have a complete voltage assignment v extending v0, such that
For any terminal path P = (x0, . . . , xr), we get,

grad[v]

∞ ≤ α.

∇P (v0) = v0(∂0P ) − v0(∂1P ) = v(∂0P ) − v(∂1P ) =

grad[v](xi−1, xi) ≤ α ·

ℓ(xi−1, xi) = α · ℓ(P ),

(cid:13)
(cid:13)

(cid:13)
(cid:13)

r

i=1
X

giving ∇P (v0) ≤ α.

On the other hand, suppose every terminal path P in (G, v0) satisﬁes ∇P (v0) ≤ α. Consider v = lexG[v0]. We
know that v extends v0. For every edge e ∈ EG ∩ T (v0) × T (v0), e is a (trivial) terminal path in (G, v0), and hence
has satisﬁes grad[v](e) = grad[v0](e) = ∇e(v0) ≤ α. Considering the reverse edge, we also obtain −grad[v](e) ≤ α.
Thus, |grad[v](e)| ≤ α. Moreover, using Lemma A.3, we know that for edge e ∈ EG \ T (v0) × T (v0), |grad[v](e)| ≤
1 = ∇P ⋆
α⋆
1 ≤ α since P1 is a terminal path in (G, v0). Thus, for every e ∈ EG, |grad[v](e)| ≤ α, and hence
✷
grad[v]
∞ ≤ α.
(cid:13)
(cid:13)
(cid:13)
(cid:13)
A.2 Stability

In this subsection, we sketch a proof of the monotonicity of lex-minimizers and show how it implies the stability
property claimed earlier.

For any well-posed (G, v0), there could be several possible executions of META-LEX, each characterized by the

sequence of paths P ⋆

i . We can apply Theorem 3.3 to deduce the following structural result about the lex-minimizer.

r

i=1
X

16

Corollary A.6 For any well-posed instance (G, v0), consider a sequence of paths (P1, . . . , Pr) and voltage assign-
ments (v1, . . . , vr) for some positive integer r such that:

1. P ⋆

i is a steepest ﬁxable path in (Gi−1, vi−1) for i = 1, . . . , r.

2. vi = ﬁx[vi−1, P ⋆

i ] for i = 1, . . . , r.

3. T (vr) = VG.

Then, we have vr = lexG[v0].

We call such a sequence of paths and voltages to be a decomposition of lexG[v0]. Again, note that lexG[v0] can
possibly have multiple decompositions. However, any two such decompositions are consistent in the sense that they
produce the same voltage assignment.

Proof of Corollary 3.7: We ﬁrst deﬁne some operations on partial assignments which simpliﬁes the notation. Let
v0, v1 be any two partial assignments with the same set of terminals T := T (v0) = T (v1) and c, d ∈ R. By cv0 + d
we mean a partial assignment v with T (v) = T satisfying v(t) = cv0(t) + d for all t ∈ T . Also, by v0 + v1 we
mean a partial assignment v with T (v) = T satisfying v(t) = v0(t) + v1(t) for all t ∈ T. Also, we say v1 ≥ v0 if
v1(t) ≥ v0(t) for all t ∈ T .

Now we can show how Corollary 3.7 follows from Theorem 3.6. Let v := v1 − v0, and kvk∞ = ǫ, for some ǫ > 0.
Therefore, v0 + ǫ ≥ v1 ≥ v0 − ǫ. Theorem 3.6 then implies that lexG[v0] + ǫ ≥ lex[v1] ≥ lex[v0] − ǫ, hence proving
✷
the corollary.

Proof sketch of Theorem 3.6:
It is easy to see that the ﬁrst statement holds. For the second statement, we ﬁrst
observe that if there is a sequence of paths P1, ..., Pr that is simultaneously a decomposition of both lex[v0] and
lex[v1], then this is easy to see. If such a path sequence doesn’t exist, then we look at vt := v0 + t(v1 − v0). We
state here without a proof (though the proof is elementary) that we can then split the interval [0, 1] into ﬁnitely many
subintervals [a0, a1], [a1, a2], .., [ak−1, ak], with a0 = 0, ak = 1, such that for any i, there is a path sequence P1, ..., Pr
which is a decomposition of lex[vt] for all t ∈ [ai, ai+1]. We then observe that v0 = va0 ≤ va1 ≤ ...vak = v1. Since
for every ai, ai+1, there is a path sequence which is simultaneously a decomposition of both lex[vai ] and lex[vai+1 ],
we immediately get

lex[v0] = lex[va0 ] ≤ lex[va1] ≤ ... ≤ lex[vak ] = lex[v1].

✷

A.3 Alternate Characterizations

Proof of Theorem 3.10: We know that lexG[v0] extends v0. We ﬁrst prove that v = lexG[v0] satisﬁes the max-min
gradient averaging property. Assume to the contrary. Thus, there exists x ∈ VG \ T (v0) such that

max
y:(x,y)∈EG

grad[v](x, y) 6= − min

grad[v](x, y).

y:(x,y)∈EG

Assume that max(x,y)∈EG grad[v](x, y) ≥ − min(x,y)∈EG grad[v](x, y). Then, consider v′ extending v0 that is iden-
tical to v except for v′(x) = v(x) − ǫ for ǫ > 0. For ǫ small enough, we get that

and

max
y:(x,y)∈EG

grad[v′](x, y) < max

grad[v](x, y)

y:(x,y)∈EG

− min

y:(x,y)∈EG

grad[v′](x, y) < max

grad[v](x, y).

y:(x,y)∈EG

The gradient of edges not incident on the vertex x is left unchanged. This implies that grad[v]

6(cid:22) grad[v′],

contradicting the assumption that v is the lex-minimizer. (The other case is similar).

17

For the other direction. Consider a complete voltage assignment v extending v0 that satisﬁes the max-min gradient

averaging property w.r.t. (G, v0). Let

α = max

grad[v](x, y) ≥ 0

(x,y)∈EG
x∈V \T (v0)

be the maximum edge gradient, and consider any edge (x0, x1) ∈ EG such that grad[v](x1, x0) = α, with x1 ∈
V \ T (v0). If α = 0, grad[v] is identically zero, and is trivially the lex-minimal gradient assignment. Thus, both v and
lexG[v0] are constant on each connected component. Since (G, v0) is well-posed, there is at least one terminal in each
component, and hence v and lexG[v0] must be identical.

Now assume α > 0. By the max-min gradient averaging property, ∃x2 ∈ VG such that (x1, x2) ∈ EG and

grad[v](x1, x2) =

min
y:(x1,y)∈EG

grad[v](x1, y) = − max

grad[v](x1, y)

y:(x1,y)∈EG

≤ −grad[v](x1, x0) = −α.

Thus, grad[v](x2, x1) ≥ α. Since α is the maximum edge gradient, we must have grad[v](x2, x1) = α. More-
over, v(x2) > v(x1) > v(x0), thus x2 6= x0. We can inductively apply this argument at x2 until we hit a ter-
minal. Similarly, if x0 /∈ T (v0) we can extend the path in the other direction. Consequently, we obtain a path
P = (xj , . . . , x2, x1, x0, x−1, . . . , xk) with all vertices as distinct, such that xj , xk ∈ T (v0), and xi ∈ V \ T (v0)
for all i ∈ [j + 1, k − 1]. Moreover, grad[v](xi, xi−1) = α for all j < i ≤ k. Thus, P is a free terminal path with
∇P [v0] = α.

Moreover, since v is a voltage assignment extending v0 with

∞ = α, using Lemma 3.5, we know that
every terminal path P ′ in (G, v0) must satisfy ∇P ′(v0) ≤ α. Thus, P is a steepest ﬁxable path in (G, v0). Thus,
letting v1 = ﬁx[v0, P ], using Corollary 3.4, we obtain that lexG[v1] = lexG[v0]. Moreover, since α = ∇P [v0] =
grad[v](xi, xi−1) for all i ∈ (j, k], we get v1(xi) = v(xi) for all i ∈ (j, k). Thus, v extends v1.

grad[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We can iterate this argument for r iterations until T (vr) = VG, giving v = vr and vr = lexG[vr] = lexG[v0].
(Since we are ﬁxing at least one terminal at each iteration, this procedure terminates). Thus, we get v = lexG[v0]. ✷

B Description of the Algorithms

Algorithm 2: MODDIJKSTRA(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs a complete
voltage assignment v for G, and an array parent : V → V ∪ {null}.

Add x to a ﬁbonacci heap, with key(x) = +∞.
ﬁnished(x) ← false

Decrease key(x) to v0(x).
parent(x) ← null.

1. for x ∈ VG,
2.
3.
4. for x ∈ T (v0)
5.
6.
7. while heap is not empty
8.
9.
10.
11.
12.
13.
14.
15. return (v, parent)

x ← pop element with minimum key from heap
v(x) ← key(x). ﬁnished(x) ← true .
for y : (x, y) ∈ EG

if ﬁnished(y) = false

if key(y) > v(x) + α · ℓ(x, y)

Decrease key(y) to v(x) + α · ℓ(x, y).
parent(y) ← x.

Theorem B.1 For a well-posed instance (G, V0) and a gradient value α ≥ 0, let (v, parent) ← MODDIJKSTRA(G, v0, α).
Then, v is a complete voltage assignment such that, ∀x ∈ VG, v(x) = mint∈T (v0){v0(t) + αdist(x, t)}. Moreover, the
pointer array parent satisﬁes ∀x /∈ T (v0), parent(x) 6= null and v(x) = v(parent(x)) + α · ℓ(x, parent(x)).

18

Algorithm 3: Algorithm COMPVLOW(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs
vLow, a complete voltage assignment for G, and an array LParent : V → V ∪ {null}.

1. (vLow, LParent) ← MODDIJKSTRA(G, v0, α)
2. return (vLow, LParent)

Algorithm 4: Algorithm COMPVHIGH(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs
vHigh, a complete voltage assignment for G, and an array HParent : V → V ∪ {null}.

if x ∈ T (v0) then v1(x) ← −v0(x) else v1(x) ← v1(x).

1. for x ∈ VG
2.
3. (temp, HParent) ← MODDIJKSTRA(G, v1, α)
4. for x ∈ VG : vHigh(x) ← −temp(x)
5. return (vHigh, HParent)

Corollary B.2 For a well-posed instance (G, V0) and a gradient value α ≥ 0, let (vLow[α], LParent) ← COMPVLOW(G, v0, α)
and (vHigh[α], HParent) ← COMPVHIGH(G, v0, α). Then, vLow[α], vHigh[α] are complete voltage assignments for
G such that, ∀x ∈ VG,

vLow[α](x) = min

{v0(t) + α · dist(x, t)}

t∈T (v0)

vHigh[α](x) = max
t∈T (v0)

{v0(t) − α · dist(t, x)}.

Moreover, the pointer arrays LParent, HParent satisfy ∀x /∈ T (v0), LParent(x), HParent(x) 6= null and

vLow[α](x) = vLow[α](LParent(x)) + α · ℓ(x, LParent(x)),
vHigh[α](x) = vHigh[α](HParent(x)) − α · ℓ(x, HParent(x)).

Algorithm 5: Algorithm COMPINFMIN(G, v0): Given a well-posed instance (G, v0), outputs a complete voltage assignment
v for G, extending v0 that minimizes (cid:13)

(cid:13)grad[v](cid:13)

(cid:13)∞.

1. α ← max{|grad[v0](e)| | e ∈ EG ∩ (T (v0) × T (v0))}.
2. EG ← EG \ (T (v0) × T (v0))
3. P ←STEEPESTPATH(G, v0).
4. α ← max{α, ∇P (v0)}
5. (vLow, LParent) ← COMPVLOW(G, v0, α)
6. (vHigh, HParent) ← COMPVHIGH(G, v0, α)
7. for x ∈ VG
8.
9.
10.
11. return v

then v(x) ← v0(x)
else v(x) ← 1

2 · (vLow(x) + vHigh(x)).

if x ∈ T (v0)

1. (vLow, LParent) ← COMPVLOW(G, v0, α)
2. (vHigh, HParent) ← COMPVHIGH(G, v0, α)
3. VG′ ← {x ∈ VG | vHigh(x) > vLow(x) }
4. EG′ ← {(x, y) ∈ EG | x, y ∈ VG′ }.

19

Algorithm 6: Algorithm COMPHIGHPRESSGRAPH(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0,
outputs a minimal induced subgraph G′ of G where every vertex has pressure[v0](·) > α.

5. G′ ← (V ′, E′, ℓ)
6. return G′

Proof of Lemma 4.3:

is equivalent to

vHigh[α](x) > vLow[α](x)

max
t∈T (v0)

{v0(t) − α · dist(t, x)} > min

{v0(t) + α · dist(x, t)},

t∈T (v0)

which implies that there exists terminals s, t ∈ T (v0) such that

thus,

Hence,

v0(t) − α · dist(t, x) > v0(s) + α · dist(x, s)

pressure[v0](x) ≥

v0(t) − v0(s)
dist(t, x) + dist(x, s)

> α.

v0(t) − v0(s)
dist(t, x) + dist(x, s)

= pressure[v0](x) > α.

v0(t) − α · dist(t, x) > v0(s) + α · dist(x, s)

So the inequality on vHigh and vLow implies that pressure is strictly greater than α. On the other hand, if pressure[v0](x) >
α, there exists terminals s, t ∈ T (v0) such that

which implies vHigh[α](x) > vLow[α](x).

✷

Algorithm 7: Algorithm STEEPESTPATH(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs a steepest
free terminal path P in (G, v0).

P ← VERTEXSTEEPESTPATH(G, v0, xi)

1. Sample uniformly random e ∈ EG. Let e = (x1, x2).
2. Sample uniformly random x3 ∈ VG.
3. for i = 1 to 3
4.
5. Let j ∈ arg maxj∈{1,2,3} ∇Pj (v0)
6. G′ ← COMPHIGHPRESSGRAPH(G, v0, ∇Pj (v0))
7. if EG′ = ∅,
8.
9.

then return Pj
else return STEEPESTPATH(G′, v0|VG′ )

1. while T (v0) 6= VG
2.
3.
4.
5. return v0

EG ← EG \ (T (v0) × T (v0))
P ← STEEPESTPATH(G, v0)
v0 ← ﬁx[v0, P ]

Algorithm 8: Algorithm COMPLEXMIN(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs lexG[v0].

Algorithm 9: Algorithm VERTEXSTEEPESTPATH(G,v0, x): Given a well-posed instance (G, v0), and a vertex x ∈ VG,
outputs a steepest terminal path in (G, v0) through x.

1. Using Dijkstra’s algorithm, compute dist(x, t) for all t ∈ T (v0)

20

y ← arg maxy∈T (v0)
if v0(x) ≥ v0(y)

|v0(x)−v0(y)|
dist(x,y)

then return a shortest path from x to y
else return a shortest path from y to x

2. if x ∈ T (v0)
3.
4.
5.
6.
7. else
8.
9.
10.
11.

for t /∈ T (v0), d(t) ← dist(x, t)
(t1, t2) ← STARSTEEPESTPATH(T (v0), v0|T (v0), d)
Let P1 be a shortest path from t1 to x. Let P2 be a shortest path from x to t2.
P ← (P1, P2). return P.

Algorithm 10: STARSTEEPESTPATH(T, v, d): Returns the steepest path in a star graph, with a single non-terminal connected
to terminals in T, with lengths given by d, and voltages given by v.

|v(t1)−v(t)|
d(t1)+d(t)

1. Sample t1 uniformly and randomly from T
2. Compute t2 ∈ arg maxt∈T
3. α ← |v(t2)−v(t1)|
d(t1)+d(t2)
4. Compute vlow ← mint∈T (v(t) + α · d(t))
5. Tlow ← {t ∈ T | v(t) > vlow + α · d(t)}
6. Compute vhigh ← maxt∈T (v(t) − α · d(t))
7. Thigh ← {t ∈ T | v(t) < vhigh − α · d(t)}
8. T ′ ← Tlow ∪ Thigh.
9. if T ′ = ∅
10.
11.

then if v(t1) ≥ v(t2) then return (t1, t2) else return (t2, t1)
else return STARSTEEPESTPATH(T ′, v|T ′, dT ′ )

B.1 Faster Lex-minimization

Algorithm 11: Algorithm COMPFASTLEXMIN(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs
lexG[v0].

1. while T (v0) 6= VG
2.
3. return v0

v0 ← FIXPATHSABOVEPRESS(G, v0, 0)

Algorithm 12: Algorithm FIXPATHSABOVEPRESS(G, v0, α): Given a well-posed instance (G, v0), with T (v0) 6= VG, and
a gradient value α, iteratively ﬁxes all paths with gradient > α.

EG ← EG \ (T (v0) × T (v0))
Sample uniformly random e ∈ EG. Let e = (x1, x2).
Sample uniformly random x3 ∈ VG.
for i = 1 to 3

Pi ← VERTEXSTEEPESTPATH(G, v0, xi)

Let j ∈ arg maxj∈{1,2,3} ∇Pj(v0)
G′ ← COMPHIGHPRESSGRAPH(G, v0, ∇Pj (v0))
if EG′ = ∅,

1. while T (v0) 6= VG
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.

then v0 ← ﬁx[v0, P ]
else Let G′

for i = 1, . . . , r

i, i = 1, . . . , r be the connected components of G′.

21

vi ← FIXPATHSABOVEPRESS(G′
for x ∈ VG′

i, set v0(x) ← vi(x)

i, v0|VG′

i

, ∇Pj (v0))

if α > 0 then G ←COMPHIGHPRESSGRAPH(G, v0, α)

13.
14.
15.
16. return v0

C Experiments on WebSpam: Testing More Algorithms

For completeness, in this appendix we show how a number of algorithms perform on the web spam experiment of
Section 6. We consider the following algorithms:

• RANDWALK along in-links. For a detailed description see Zhou et al. (2007). This algorithm essentially per-
forms a Personalized PageRank random walk from each vertex x and computes a spam-value for the vertex x by
taking a weighted average of the labels of the vertices where the random walk from x terminates. Also shown in
Section 6.

• DIRECTEDLEX, with edges in the opposite directions of links. This has the effect that a link to a spam host is

evidence of spam, and a link from a normal host is evidence of normality. Also shown in Section 6.

• RANDWALK along out-links.

• DIRECTEDLEX, with edges in the directions of links. This has the effect that a link from to a spam host is

evidence of spam, and a link to a normal host is evidence of normality.

• UNDIRECTEDLEX: Lex-minimization with links treated as undirected edges.

• LAPLACIAN: l2-regression with links treated as undirected edges.

• DIRECTED 1-NEAREST NEIGHBOR: Uses shortest distance along paths following out-links. Spam-ratio is
deﬁned distance from normal hosts, divided by distance to spam hosts. Sites are ﬂagged as spam when spam-
ratio exceeds some threshold. We also tried following paths along in-links instead, but that gave much worse
results.

We use the experimental setup described in Section 6. Results are shown in Figure 4. The alternative convention
for DIRECTEDLEX orients edges in the directions of links. This takes a link from a spam host to be evidence of
spam, and a link to a normal host to be evidence of normality. This approach performs signiﬁcantly worse than our
preferred convention, as one would intuitively expect. UNDIRECTEDLEX and LAPLACIAN approaches also perform
signiﬁcantly worse. DIRECTED 1-NEAREST NEIGHBOR performs poorly, demonstrating that DIRECTEDLEX is very
different from that approach. As observed by Zhou et al. (2007), sampling based on a random walk following out-links
performs worse than following in-links. Up to 60 % recall, DIRECTEDLEX performs best, both in the regime of 5 %
labels for training and in the regime of 20 % labels for training.

22

5 % labels for training

RandWalk along in-links
DirectedLex
RandWalk along out-links
DirectedLex, opposite link direction
UndirectedLex
Laplacian
Directed 1NN

0
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Recall

20 % labels for training

RandWalk along in-links
DirectedLex
RandWalk along out-links
DirectedLex, opposite link direction
UndirectedLex
Laplacian
Directed 1NN

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

i

i

n
o
s
c
e
r
P

i

i

n
o
s
c
e
r
P

0
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Recall

Figure 4: Recall and precision in the WebSpam classiﬁcation experiment. Each data point shown was computed as an average
over 100 runs. The largest standard deviation of the mean precision across the plotted recall values was less than 1.5 %. The
algorithm of Zhou et al. (2007) appears as RANDWALK (along in-links). We also show RANDWALK along out-links. Our directed
lex-minimization algorithm appears as DIRECTEDLEX. We also show DIRECTEDLEX with link directions reversed, along with
UNDIRECTEDLEX and LAPLACIAN.

D l0-Vertex Regularization Proofs

In this appendix, we prove Theorem 7.1 and Theorem 7.2. For the purposes of proving the second theorem, we intro-
duce an alternative version of problem (3). The optimization problem here requires us to minimize l0-regularization

23

budget required to obtain an inf-minimizer with gradient below a given threshold:

min
v∈IRn
subject to

(cid:13)
(cid:13)

v(T ) − v0(T )

0

gradG[v]

(cid:13)
∞ ≤ α.
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We will also need the following graph construction.

Deﬁnition D.1 The α-pressure terminal graph of a partially-labeled graph (G, v0) is a directed unweighted graph
Gα = (T (v0),

E if and only if there is a terminal path P from s to t in G with

E) such that (s, t) ∈

b

b

∇P (v0) > α.

Note that the α-pressure terminal graph has O(n) vertices but may be dense, even when G is not.

Algorithm 13: Algorithm TERM-PRESSURE: Given a well-posed instance (G, v0) and α ≥ 0, outputs α pressure terminal
graph Gα.
Initialize Gα with vertex set Vα = T (v0) and edge set
for each terminal s ∈ T (v0)

E = ∅.

1. Compute the distances to every other terminal t by running Dijktra’s algorithm, allowing shortest paths

b

2. Use the resulting distances to check for every other terminal t if there is a terminal path P from s to t with

that run through other terminals.

∇P (v0) > α. If there is, add edge (s, t) to

E.

Lemma D.2 The α-pressure terminal graph of a voltage problem (G, v0) can be computed in O((m + n log n)n) time
using algorithm TERM-PRESSURE (Algorithm 13).

b

Proof: The correctness of the algorithm follows from the fact that Dijkstra’s algorithm will identify all shortest
distances between the terminals, and the pressure check will ensure that terminal pairs (s, t) are added to
E if and
only if they are the endpoints of a terminal path P with ∇P (v0) > α. The running time is dominated by performing
Dijkstra’s algorithm once for each terminal. A single run of Dijkstra’s algorithm takes O(m + n log n) time, and this
✷
is performed at most n times, for a total running time of O((m + n log n)n).

b

We make three observations that will turn out to be crucial for proving Theorems 7.1 and 7.2.

Observation D.3 Gα is a subgraph of Gβ for α ≥ β.

Proof: Suppose edge (s, t) appears in Gα, then for some path P

∇P (v0) > α ≥ β,

so the edge also appears in Gβ.

Observation D.4 Gα is transitively closed.

Proof: Suppose edges (s, t) and (t, r) appear in Gα. Let P(s,t), P(t,r), P(s,r) be the respective shortest paths in G
between these terminal pairs. Then

∇P(s,r)(v0) =

v0(s) − v0(r)
ℓ(P(s,r))

≥

v0(s) − v0(r)
ℓ(P(s,t)) + ℓ(P(t,r))

=

v0(s) − v0(t) + v0(t) − v0(r)
ℓ(P(s,t)) + ℓ(P(t,r))

≥ min

v0(s) − v0(t)
ℓ(P(s,t))

,

 

v0(t) − v0(r)

ℓ(P(t,r)) !

> α.

So edge (s, r) also appears in Gα. This is sufﬁcient for Gα to be transitively closed.

24

(6)

✷

(7)

✷

Observation D.5 Gα is a directed acyclic graph.

Proof: Suppose for a contradiction that a directed cycle appears in Gα. Let s and t be two vertices in this cycle. Let
P(s,t) and P(t,s) be the respective shortest paths in G between these terminal pairs. Because Gα is transitively closed,
both edges (s, t) and (t, s) must appear in Gα. But (s, t) ∈

E implies

and similarly (t, s) ∈

E implies

b
This is a contradiction.

v0(s) − v0(t) > αℓ(P(s,t)) > 0,

b

v0(t) − v0(s) > αℓ(P(t,s)) > 0.

✷

The usefulness of the α-pressure terminal graph is captured in the following lemma. We deﬁne a vertex cover of a
directed graph to be a vertex set that constitutes a vertex cover in the same graph with all edges taken to be undirected.

Lemma D.6 Given a partially-labeled graph (G, v0) and a set U ⊆ V , there exists a voltage assignment v ∈ IRn that
satisﬁes

if and only if U is a vertex cover in the α-pressure terminal graph Gα of (G, v0).
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:8)

(cid:9)

t ∈ T (v0) : v(t) 6= v0(t)

⊆ U and

gradG[v]

∞ ≤ α,

Proof: We ﬁrst show the “only if” direction. Suppose for a contradiction that there exists a voltage assignment v for
which
∞ ≤ α, but U is not a vertex cover in Gα. Let (s, t) be an edge Gα which is not covered by U . The
presence of this edge in Gα implies that there exists a terminal path P from s to t in G for which

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∇P (v0) > α.

But, by Lemma 3.5 this means there is no assignment v for G which agrees with v0 on s and t and has
α. This contradicts our assumption.

∞ ≤
(cid:13)
Now we show the “if” direction. Consider an arbitrary vertex cover U of Gα. Suppose for a contradiction that
(cid:13)
⊆ U .

t ∈ T (v0) : v(t) 6= v0(t)

gradG[v]

(cid:13)
(cid:13)

gradG[v]

there does not exist a voltage assignment v for G with
Deﬁne a partial voltage assignment vU given by

∞ ≤ α and

(cid:8)

(cid:9)

vU (t) =

v0(t)
∗

(

(cid:13)
(cid:13)

(cid:13)
(cid:13)
if t ∈ T (v0) \ U
o.w.

∞ ≤ α. By
The preceding statement is equivalent to saying that there is no v that extends vU and has
Lemma 3.5, this means there is terminal path between s, t ∈ T (vU ) with gradient strictly larger than α. But this
means an edge (s, t) is present in Gα and is not covered. This contradicts our assumption that U is a vertex cover. ✷

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We are now ready to prove Theorem 7.2.

∞

(cid:13)
(cid:13)

Proof of Theorem 7.2: We describe and prove the algorithm OUTLIER. The algorithm will reduce problem (3)
to problem (6): Suppose v∗ is an optimal assignment for problem (3).
It achieves a maximum gradient α∗ =
gradG[v∗]
. Using Dijkstra’s algorithm we compute the pairwise shortest distances between all terminals in G.
From these distances and the terminal voltages, we compute the gradient on the shortest path between each terminal
(cid:13)
pair. By Lemma 3.5, α∗ must equal one of these gradients. So we can solve problem (3) by iterating over the set of
(cid:13)
gradients between terminals and solving problem (6) for each of these O(n2) gradients. Among the assignments with
v(T ) − v0(T )

0 ≤ k, we then pick the solution that minimizes
(cid:13)
(cid:13)

In fact, we can do better. By Observation D.3, Gα is a subgraph of Gβ for α ≥ β. This means a vertex cover
(cid:13)
of Gα is also a vertex cover of Gβ, and hence the minimum vertex cover for Gβ is at least as large as the minimum
(cid:13)
vertex cover for Gα. This means we can do a binary search on the set of O(n2) terminal gradients to ﬁnd the minimum
gradient for which there exists an assignment with
0 ≤ k. This way, we only make O(log n) calls to
v(T ) − v0(T )
problem (6), in order to solve problem (3).
(cid:13)
(cid:13)

We use the following algorithm to solve problem (6).

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∞

.

25

1. Compute the α-pressure terminal graph Gα of G using the algorithm TERM-PRESSURE.
2. Compute a minimum vertex cover U of Gα using the algorithm KONIG-COVER from Theorem 7.3.
3. Deﬁne a partial voltage assignment vU given by

vU (t) =

v0(t)
∗

(

if t ∈ T (v0) \ U,
otherwise.

4. Using Algorithm 5, compute voltages v that extend vU and output v.

From Lemma D.2, it follows that step 1 computes the α-pressure terminal graph in polynomial time. From The-
orem 7.3 it follows that step 2 computes the a minimum vertex cover of the α-pressure terminal graph in polynomial
time, because our observations D.4 and D.5 establish that the graph is a TC-DAG. From Lemma D.6 and Theorem 4.6,
it follows that the output voltages solve program (6).

✷

To prove Theorem 7.1, we use the standard greedy approximation algorithm for MIN-VC (Vazirani (2001)).

Theorem D.7 2-Approximation Algorithm for Vertex Cover. The following algorithm gives a 2-approximation to
the Minimum Vertex Cover problem on a graph G = (V, E).

0. Initialize U = ∅.
1. Pick an edge (u, v) ∈ E that is not covered by U .
2. Add u and v to the set U .
3. Repeat from step 1 if there are still edges not covered by U .
4. Output U .

We are now in a position to prove Theorem 7.1

Proof of Theorem 7.1: Given an arbitrary k and a partially-labeled graph (G, v0), let α∗ be the optimum value
of program (3). Observe that by Lemma D.6, this implies that Gα∗ has a vertex cover of size k. Given the partial
assignment v0, for every vertex set U , we deﬁne

vU (t) =

v0(t)
∗

(

if t ∈ T (v0) \ U
o.w.

We claim the following algorithm APPROX-OUTLIER outputs a voltage assignment v with

gradG[v]

∞ ≤ α∗

(cid:13)
(cid:13)

(cid:13)
(cid:13)

and

v(T ) − v0(T )

(cid:13)
(cid:13)

Algorithm APPROX-OUTLIER:

0 ≤ 2k.
(cid:13)
(cid:13)

0. Initialize U = ∅.
1. Using the algorithm STEEPESTPATH (Algorithm 7), ﬁnd a steepest terminal path in G w.r.t. vU . Denote
this path P and let s and t be its terminal endpoints. If there is no terminal path with positive gradient, skip
to step 4.

2. Add s and t to the set U .
3. If |U | ≤ 2k − 2 then repeat from step 1.
4. Using the algorithm COMPINFMIN (Algorithm 5), compute voltages v that extend vU and output v.

From the stopping conditions, it is clear that |U | ≤ 2k. If in step 1 we ever ﬁnd that no terminal paths have positive
∞ = 0 ≤ α∗, by Lemma 3.5. Similarly if we ﬁnd a steepest
gradient then our v that extends vU will have
(cid:13)
(cid:13)

gradG[v]

(cid:13)
(cid:13)

26

gradG[v]

∞ ≤ α∗.

∞ ≤ α∗.
path with gradient less than α∗ w.r.t. vU , then for this U there exists v that extends vU and has
This will continue to hold when if we add vertices to U . Therefore, for the ﬁnal U , there will exist an v that extends
vU and has

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

If we never ﬁnd a steepest terminal path P with ∇P (v0) ≤ α∗, then each steepest path we ﬁnd corresponds to an
edge in Gα∗ that is not yet covered by U and our algorithm in fact implements the greedy approximation algorithm
for vertex cover described in Theorem D.7. This implies that the ﬁnal U is a vertex cover of Gα∗ of size at most 2k.
∞ ≤ α∗. This
By Lemma D.6, this implies that there exists a voltage assignment u extending vU that has
implies by Theorem 4.6 that the v we output has
(cid:13)
(cid:13)
In all cases, the v we output extends vU , so

∞ ≤ α∗.

gradG[u]

(cid:13)
(cid:13)

✷

gradG[v]
v(T ) − v0(T )
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

0 ≤ |U | ≤ 2k.
(cid:13)
(cid:13)

E Proof of Hardness of l0 regularization for l2

We will prove Theorem 7.4, by a reduction from minimum bisection. To this end, let G = (V, E) be any graph. We
will reduce the minimum bisection problem on G to our regularization problem. Let n = |V |. The graph on which we
will perform regularization will have vertex set

V ∪

V ,

V is a set of n vertices that are in 1-to-1 correspondence with V . We assume that every edge in G has weight 1.
V to the corresponding vertex in V by an edge of weight B, for some large B to be
V to each other by edges of weight B3. So, we have a complete
V to V , and the original graph G on V .

where
We now connect every vertex in
determined later. We also connect all of the vertices in
graph of weight B3 edges on
b
The input potential function will be

V , a matching of weight B edges connecting

b

b

b

v(a) =

b
0 for a ∈
1 for a ∈ V .
b

(

V , and

b

Now set k = n/2. We claim that we will be able to determine the value of the minimum bisection from the solution
to the regularization problem.

If S is the set of vertices on which v and w differ, then we know that the w is harmonic on S: for every a ∈ S,

w(a) is the weighted average of the values at its neighbors. In the following, we exploit the fact that |S| ≤ n/2.

Claim E.1 For every a ∈ S ∩

V , w(a) ≤ 2/nB2.

Proof: Let a be the vertex in S ∩
w-value equal to 0 by edges of weight B3. On the other hand, a has only one neighbor that is not in
w-value at most 1, and it is connected to that vertex by an edge of weight B. Call that vertex c. We have

V that maximizes w(a). So, a is connected to at least n/2 neighbors in

V with
V , that vertex has

b

b

b

((n − 1)B3 + B)w(a) = Bw(c) +

B3w(b)

b

b
V ,b6=a
Xb∈

= Bw(c) +

B3w(b) +

B3w(b)

b
V ∩S,b6=a
Xb∈

B3w(a)

≤ B +

b
V ∩S,b6=a
Xb∈
≤ B + (n/2 − 1)B3w(a).

b
V −S
Xb∈

Subtracting (n/2 − 1)B3w(a) from both sides gives

((n/2)B3 + B)w(a) ≤ B,

which implies the claim.

Claim E.2 For a ∈ S ∩ V , w(a) ≤ n/B.

27

✷

V . Let’s call that neighbor c. We know that w(c) ≤ 2/B2n. On the
Proof: Vertex a has exactly one neighbor in
other hand, vertex a has fewer than n − 1 neighbors in V , and each of these have w-value at most 1. Let da denote the
degree of a in G. Then,

b

So,

Let

and

bisection.

and at most

(B + da)w(a) ≤ da + B

2
B2n

.

w(a) ≤

da + 2/Bn
da + B
n + (2/Bn)
B + n

≤

≤ n/B.

|S| = k = n/2.

T = S ∩ V,

t = |T | .

(n − t)B − 4/B
b

(n − t)B + tn2/B.

We now estimate the value of the regularized objective function. To this end, we assume that

We will prove that S ⊂ V and so S = T and t = n/2.

Let δ denote the number of edges on the boundary of T in V . Once we know that t = n/2, δ is the size of a

Claim E.3 The contribution of the edges between V and

V to the objective function is at least

Proof: For the lower bound, we just count the edges between vertices in V \ T and
edges, and each of them has weight B. The endpoint in V \ T has w-value 1, and the endpoint in
most 2/nB2. So, the contribution of these edges is at least

V . There are n − t of these
V has w-value at

b

(n − t)B(1 − 2/nB2)2 ≥ (n − t)B(1 − 4/nB2) ≥ (n − t)B − 4/B.

b

For the upper bound, we observe that the difference in w-values across each of these n − t edges is at most 1, so their
total contribution is at most

Since for every vertex a ∈ T , w(a) ≤ n/B, and also every vertex b ∈
edges between T and

V is at most

t(n/B)2B = tn2/B.

b

b

V , w(b) ≤ 2/nB2, the contribution due to

We will see that this is the dominant term in the objective function. The next-most important term comes from the

edges in G.

(n − t)B.

28

✷

✷

Claim E.4 The contribution of the edges in G to the objective function is at least

and at most

δ(1 − 2n/B)

δ + (t2/2)(n/B)2

δ(1 − 2n/B) and δ.

(t2/2)(n/B)2.

Proof: Let (a, b) ∈ E. If neither a nor b is in T , then w(a) = w(b) = 1, and so this edge has no contribution. If
a ∈ T but b 6∈ T , then the difference in w-values on them is between (1 − n/B) and 1. So, the contribution of such
edges to the objective function is between

Finally, if a and b are in T , then the difference in w-values on them is at most n/B, and so the contribution of all such
edges to the objective function is at most

Claim E.5 The edges between pairs of vertices in

V contribute at most 2/B to the objective function.

Proof: As 0 ≤ w(a) ≤ 2/B2n for every a ∈

V , every edge between two vertices in

V can contribute at most

b

As there are fewer than n2/2 such edges, their total contribution to the objective function is at most

B3(2/B2n)2 = 4/Bn2.
b

b

(n2/2)(4/Bn2) = 2/B.

Lemma E.6 If n ≥ 4 and B = 2n3, the value of the objective function is at least

and at most

(n − t)B + δ − 1/2

(n − t)B + δ + 1/3.

Proof: Summing the contributions in the preceding three claims, we see that the value of the objective function is at
least

(n − t)B − 4/B + δ(1 − 2n/B) ≥ (n − t)B + δ − 4/B − 2nδ/B

≥ (n − t)B + δ − n3/B
≥ (n − t)B + δ − 1/2,

as δ ≤ (n/2)2.

Similarly, the objective function is at most

(n − t)B + tn2/B + δ + (t2/2)(n/B)2 + 2/B ≤ (n − t)B + n3/2B + δ + n4/8B2 + 2/B
≤ (n − t)B + n3/2B + δ + 1/32n2 + 1/n3
≤ (n − t)B + δ + 1/3.

Claim E.7 If n ≥ 2 and B = 2n3, then S ⊂ V .

Proof: The objective function is minimized by making t as large as possible, so t = n/2 and S ⊂ V .

29

✷

✷

✷

✷

Theorem E.8 The value of the objective function reveals the value of the minimum bisection in G.

Proof: The value of the objective function will be between

and

(n/2)B + δ − 1/2

(n/2)B + δ + 1/3.

So, the objective function will be smallest when δ is as small as possible.

✷

Theorem E.8 immediately implies Theorem 7.4.

30

5
1
0
2
 
n
u
J
 
0
3
 
 
]

G
L
.
s
c
[
 
 
2
v
0
9
2
0
0
.
5
0
5
1
:
v
i
X
r
a

Algorithms for Lipschitz Learning on Graphs ∗†

Rasmus Kyng
Yale University
rasmus.kyng@yale.edu

Anup Rao
Yale University
anup.rao@yale.edu

Sushant Sachdeva
Yale University
sachdeva@cs.yale.edu

Daniel A. Spielman
Yale University
spielman@cs.yale.edu

July 1, 2015

Abstract

We develop fast algorithms for solving regression problems on graphs where one is given the value of a function
at some vertices, and must ﬁnd its smoothest possible extension to all vertices. The extension we compute is the
absolutely minimal Lipschitz extension, and is the limit for large p of p-Laplacian regularization. We present an
algorithm that computes a minimal Lipschitz extension in expected linear time, and an algorithm that computes
an absolutely minimal Lipschitz extension in expected time eO(mn). The latter algorithm has variants that seem
to run much faster in practice. These extensions are particularly amenable to regularization: we can perform l0-
regularization on the given values in polynomial time and l1-regularization on the initial function values and on graph
edge weights in time eO(m3/2).

Our deﬁnitions and algorithms naturally extend to directed graphs.

1 Introduction

We consider a problem in which we are given a weighted undirected graph G = (V, E, ℓ) and values v0 : T → R
on a subset T of its vertices. We view the weights ℓ as indicating the lengths of edges, with shorter length indicating
greater similarity. Our goal it to assign values to every vertex v ∈ V \T so that the values assigned are as smooth as
possible across edges. A minimal Lipschitz extension of v0 is a vector v that minimizes

max
(x,y)∈E

(ℓ(x, y))−1

v(x) − v(y)

,

(cid:12)
(cid:12)

(cid:12)
(cid:12)

subject to v(x) = v0(x) for all x ∈ T . We call such a vector an inf-minimizer. Inf-minimizers are not unique. So,
among inf-minimizers we seek vectors that minimize the second-largest absolute value of ℓ(x, y)−1
v(x) − v(y)
across edges, and then the third-largest given that, and so on. We call such a vector v a lex-minimizer. It is also known
(cid:12)
as an absolutely minimal Lipschitz extension of v0.
(cid:12)
These are the limit of the solution to p-Laplacian minimization problems for large p, namely the vectors that solve

(cid:12)
(cid:12)

(1)

(2)

min
v∈Rn

v|T =v0|T X(x,y)∈E

(ℓ(x, y))−p|v(x) − v(y)|p.

The use of p = 2 was suggested in the foundational paper of Zhu et al. (2003), and is particularly nice because it can
be obtained by solving a system of linear equations in a symmetric diagonally dominant matrix, which can be done

∗This research was partially supported by AFOSR Award FA9550-12-1-0175, NSF grant CCF-1111257, a Simons Investigator Award to Daniel

Spielman, and a MacArthur Fellowship.

†Code used in this work is available at https://github.com/danspielman/YINSlex

1

very quickly (Cohen et al. (2014)). The use of larger values of p has been discussed by Alamgir and Luxburg (2011),
and by Bridle and Zhu (2013), but it is much more complicated to compute. The fastest algorithms we know for this
problem require convex programming, and then require very high accuracy to obtain the values at most vertices. By
taking the limit as p goes to inﬁnity, we recover the lex-minimizer, which we will show can be computed quickly.

The lex-minimization problem has a remarkable amount of structure. For example, in uniformly weighted graphs
the value of the lex-minimizer at every vertex not in T is equal to the average of the minimum and maximum of the
values at its neighbors. This is analogous to the property of the 2-Laplacian minimizer that the value at every vertex
not in T equals the average of the values at its neighbors.

1.1 Contributions

We ﬁrst present several important structural properties of lex-minimizers in Section 3.2. As we shall point out, some
of these were known from previous work, sometimes in restricted settings. We state them generally and prove them
for completeness. We also prove that the lex-minimizer is as stable as possible under perturbations of v0 (Section 3.1).
The structure of the lex-minimization problem has led us to develop elegant algorithms for its solution. Both the
algorithms and their analyses could be taught to undergraduates. We believe that these algorithms could be used in
place of 2-Laplacian minimization in many applications.

We present algorithms for the following problems. Throughout, m = |E| and n = |V |.

Inf-minimization: An algorithm that runs in expected time O(m + n log n) (Section 4.3).

Lex-minimization: An algorithm that runs in expected time O(n(m + n log n)) (Section 4), along with a variant that

runs quickly in practice (Section 4.4).

l1-regularization of edge lengths for inf-minimization: The problem of minimizing (1) given a limited budget with
O(m3/2)
which one can increase edge lengths is a linear programming problem. We show how to solve it in time
with an interior point method by using fast Laplacian solvers (Section 8). The same algorithm can accommodate
l1-regularization of the values given in v0.

e

l0-regularization of vertex values for inf-minimization: We give a polynomial time algorithm for l0-regularization
of the values at vertices. That is, we minimize (1) given a budget of a number of vertices that can be proclaimed
outliers and removed from T (Section 7.1). We solve this problem by reducing it to the problem of computing
minimum vertex covers on transitively closed directed acyclic graphs, a special case of minimum vertex cover
that can be solved in polynomial time.

After any regularization for inf-minimization, we suggest computing the lex-minimizer. We ﬁnd the result for l0-
regularization of vertex values to be particularly surprising, especially because we prove that the analogous problem
for 2-Laplacian minimization is NP-Hard (Section 7.2).

All of our algorithms extend naturally to directed graphs (Section 5). This is in contrast with the problem of
minimizing 2-Laplacians on directed graphs, which corresponds to computing electrical ﬂows in networks of resistors
and diodes, for which fast algorithms are not presently known.

We present a few experiments on examples demonstrating that the lex-minimizer can overcome known deﬁcien-
cies of the 2-Laplacian minimizer (Section 1.2, Figures 1,2), as well as a demonstration of the performance of the
directed analog of our algorithms on the WebSpam dataset of Castillo et al. (2006) (Section 6). In the WebSpam prob-
lem we use the link structure of a collection of web sites to ﬂag some sites as spam, given a small number of labeled
sites known to be spam or normal.

1.2 Relation to Prior Work

We ﬁrst encountered the idea of using the minimizer of the 2-Laplacian given by (2) for regression and classiﬁca-
tion on graphs in the work of Zhu et al. (2003) and Belkin et al. (2004) on semi-supervised learning. These works
transformed learning problems on sets of vectors into problems on graphs by identifying vectors with vertices and
constructing graphs with edges between nearby vectors. One shortcoming of this approach (see Nadler et al. (2009),

2

e
g
a

t
l

 

o
V
d
e
r
r
e

f

n

I

1

0.8

0.6

0.4

0.2

0

-0.2

-0.4

-0.6

-0.8

-1

-4

50 lex
50 l2
100 lex
100 l2
500 lex
500 l2
1000 lex
1000 l2

0.25

0.2

r
o
r
r
e
 
1
l
 
n
a
e
M

0.15

0.1

0.05

0
5000 

lex
2-Lap
labels

-2

0

2
Vertex position on real line

4

6

8

Figure 1: Lex vs 2-Laplacian on 1D gaussian clus-
ters.

Figure 2: kNN graphs on samples from 4D cube.

10000

20000

40000

80000

Number of Vertices

Alamgir and Luxburg (2011), Bridle and Zhu (2013)) is that if the number of vectors grows while the number of la-
beled vectors remains ﬁxed, then almost all the values of the 2-Laplacian minimizer converge to the mean of the
labels on most natural examples. For example, Nadler et al. (2009) consider sampling points from two Gaussian
distributions centered at 0 and 4 on the real line. They place edges between every pair of points (x, y) with length
exp(|x − y|2 /2σ2) for σ = 0.4, and provide only the labels v0(0) = −1 and v0(4) = 1. Figure 1 shows the values
of the 2-Laplacian minimizer in red, which are all approximately zero. In contrast, the values of the lex-minimizer in
blue, which are smoothly distributed between the labeled points, are shown.

The “manifold hypothesis” (see Chapelle et al. (2010), Ma and Fu (2011)) holds that much natural data lies near a
low-dimensional manifold and that natural functions we would like to learn on this data are smooth functions on the
manifold. Under this assumption, one should expect lex-minimizers to interpolate well. In contrast, the 2-Laplacian
minimizers degrade (dotted lines) if the number of labeled points remains ﬁxed while the total number of points grows.
In Figure 2, we demonstrate this by sampling many points uniformly from the unit cube in 4 dimensions, form their
8-nearest neighbor graph, and consider the problem of regressing the ﬁrst coordinate. We performed 8 experiments,
varying the number of labeled points in {50, 100, 500, 1000}. Each data point is the mean average l1 error over 100
experiments. The plots for root mean squared error are similar. The standard deviation of the estimations of the mean
are within one pixel, and so are not displayed. The performance of the lex-minimizer (solid lines) does not degrade as
the number of unlabeled points grows.

Analogous to our inf-minimizers, minimal Lipschitz extensions of functions in Euclidean space and over more
general metric spaces have been studied extensively in Mathematics (Kirszbraun (1934), McShane (1934), Whitney
(1934)). von Luxburg and Bousquet (2003) employ Lipschitz extensions on metric spaces for classiﬁcation and relate
these to Support Vector Machines. Their work inspired improvements in classiﬁcation and regression in metric spaces
with low doubling dimension (Gottlieb et al. (2013), Gottlieb et al. (2013b)). Theoretically fast, although not actually
practical, algorithms have been given for constructing minimal Lipschitz extensions of functions on low-dimensional
Euclidean spaces (Fefferman (2009a), Fefferman and Klartag (2009), Fefferman (2009b)). Sinop and Grady (2007)
suggest using inf-minimizers for binary classiﬁcation problems on graphs. For this special case, where all of the
given values are either 0 or 1, they present an O(m + n log n) time algorithm for computing an inf-minimizer. The
case of general given values, which we solve in this paper, is much more complicated. To compensate for the non-
uniqueness of inf-minimizers, they suggest choosing the inf-minimizer that minimizes (2) with p = 2. We believe that
the lex-minimizer is a more natural choice.

The analog of our lex-minimizer over continuous spaces is called the absolutely minimal Lipschitz extension
(AMLE). Starting with the work of Aronsson (1967), there have been several characterizations and proofs of the ex-
istence and uniqueness of the AMLE (Jensen (1993), Crandall et al. (2001), Barles and Busca (2001), Aronsson et al.
(2004)). Many of these results were later extended to general metric spaces, including graphs (Milman (1999),
Peres et al. (2011), Naor and Shefﬁeld (2010), Shefﬁeld and Smart (2010)). However, to the best of our knowledge,
fast algorithms for computing lex-minimizers on graphs were not known. For the special case of undirected, un-
weighted graphs, Lazarus et al. (1999) presented both a polynomial-time algorithm and an iterative method. Oberman

3

(2011) suggested computing the AMLE in Euclidean space by ﬁrst discretizing the problem and then solving the cor-
responding graph problem by an iterative method. However, no run-time guarantees were obtained for either iterative
method.

2 Notation and Basic Deﬁnitions

Lexicographic Ordering. Given a vector r ∈ Rm, let πr denote a permutation that sorts r in non-increasing order
by absolute value, i.e., ∀i ∈ [m − 1], |r(πr(i))| ≥ |r(πr(i + 1))|. Given two vectors r, s ∈ Rm, we write r (cid:22) s to
indicate that r is smaller than s in the lexicographic ordering on sorted absolute values, i.e.

∃j ∈ [m],

r(πr(j))

<

s(πs(j))

and ∀i ∈ [j − 1],

r(πr(i))

=

s(πs(i))

or ∀i ∈ [m],

=

r(πr(i))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.
(cid:12)
(cid:12)

s(πs(i))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

Note that it is possible that r (cid:22) s and s (cid:22) r while r 6= s. It is a total relation: for every r and s at least one of r (cid:22) s
or s (cid:22) r is true.

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Graphs and Matrices. We will work with weighted graphs. Unless explicitly stated, we will assume that they are
undirected. For a graph G, we let VG be its set of vertices, EG be its set of edges, and ℓG : EG → R+ be the
assignment of positive lengths to the edges. We let |VG| = n, and |EG| = m. We assume ℓG is symmetric, i.e.,
ℓG(x, y) = ℓG(y, x). When G is clear from the context, we drop the subscript.

A path P in G is an ordered sequence of (not necessarily distinct) vertices P = (x0, x1, . . . , xk), such that
(xi−1, xi) ∈ E for i ∈ [k]. The endpoints of P are denoted by ∂0P = x0, ∂1P = xk. The set of interior vertices
of P is deﬁned to be int(P ) = {xi : 0 < i < k}. For 0 ≤ i < j ≤ k, we use the notation P [xi : xj] to denote the
k
subpath (xi, . . . , xj). The length of P is ℓ(P ) =
i=1 ℓ(xi−1, xi).
A function v0 : V → R ∪ {∗} is called a voltage assignment (to G). A vertex x ∈ V is a terminal with
respect to v0 iff v0(x) 6= ∗. The other vertices, for which v0(x) = ∗, are non-terminals. We let T (v0) denote the
set of terminals with respect to v0. If T (v0) = V, we call v0 a complete voltage assignment (to G). We say that an
assignment v : V → R ∪ {∗} extends v0 if v(x) = v0(x) for all x such that v0(x) 6= ∗.

Given an assignment v0 : V → R ∪ {∗}, and two terminals x, y ∈ T (v0) for which (x, y) ∈ E, we deﬁne the

P

gradient on (x, y) due to v0 to be

gradG[v0](x, y) =

v0(x) − v0(y)
ℓ(x, y)

.

It may be useful to view gradG[v0](x, y) as the current in the edge (x, y) induced by voltages v0. When v0 is a
complete voltage assignment, we interpret gradG[v0] as a vector in Rm, with one entry for each edge. However, for
convenience, we deﬁne gradG[v0](x, y) = −gradG[v0](y, x). When G is clear from the context, we drop the subscript.
A graph G along with a voltage assignment v to G is called a partially-labeled graph, denoted (G, v). We say
that a partially-labeled graph (G, v0) is a well-posed instance if for every maximal connected component H of G, we
have T (v0) ∩ VH 6= ∅.

A path P in a partially-labeled graph (G, v0) is called a terminal path if both endpoints are terminals. We deﬁne

∇P (v0) to be its gradient:

∇P (v0) =

v0(∂0P ) − v0(∂1P )
ℓ(P )

.

If P contains no terminal-terminal edges (and hence, contains at least one non-terminal), it is a free terminal path.

Lex-Minimization. An instance of the LEX-MINIMIZATION problem is described by a partially-labeled graph
(G, v0). The objective is to compute a complete voltage assignment v : VG → R extending v0 that lex-minimizes
grad[v].

Deﬁnition 2.1 (Lex-minimizer) Given a partially-labeled graph (G, v0), we deﬁne lexG[v0] to be a complete voltage
assignment to V that extends v0, and such that for every other complete assignment v′ : VG → R that extends v0, we
have gradG[lexG[v0]] (cid:22) gradG[v′]. That is, lexG[v0] achieves a lexicographically-minimal gradient assignment to the
edges.

We call lexG[v0] the lex-minimizer for (G, v0). Note that if T (v0) = VG, then trivially, lexG[v0] = v0.

4

3 Basic Properties of Lex-Minimizers

Lazarus et al. (1999) established that lex-minimizers in unweighted and undirected graphs exist, are unique, and may
be computed by an elementary meta-algorithm. We state and prove these facts for undirected weighted graphs, and
defer the discussion of the directed case to Section 5. We also state for directed and weighted graphs characterizations
of lex-minimizers that were established by Peres et al. (2011), Naor and Shefﬁeld (2010) and Shefﬁeld and Smart
(2010) for unweighted graphs. These results are essential for the analyses of our algorithms. We defer most proofs to
Appendix A.

Deﬁnition 3.1 A steepest ﬁxable path in an instance (G, v0) is a free terminal path P that has the largest gradient
∇P (v0) amongst such paths.

Observe that a steepest ﬁxable path with ∇P (v0) 6= 0 must be a simple path.
Deﬁnition 3.2 Given a steepest ﬁxable path P in an instance (G, v0), we deﬁne ﬁxG[v0, P ] : VG → R ∪ {∗} to be the
voltage assignment deﬁned as follows

ﬁxG[v0, P ](x) =

v0(∂0P ) − ∇P (v0) · ℓG(P [∂0P : x]) x ∈ int(P ) \ T (v0),
v0(x)

otherwise.

(

We say that the vertices x ∈ int(P ) are ﬁxed by the operation ﬁx[v0, P ]. If we deﬁne v1 = ﬁxG[v0, P ], where
P = (x0, . . . , xr) is the steepest ﬁxable path in (G, v0), then it is easy to argue that for every i ∈ [r], we have
grad[v1](xi−1, xi) = ∇P (see Lemma A.5). The meta-algorithm META-LEX, spelled out as Algorithm 1, entails
repeatedly ﬁxing steepest ﬁxable paths. While it is possible to have multiple steepest ﬁxable paths, the result of ﬁxing
all of them does not depend on the order in which they are ﬁxed.

Theorem 3.3 Given a well-posed instance (G, v0), the meta-algorithm META-LEX, which repeatedly ﬁxes steepest
ﬁxable paths, produces the unique lex-minimizer extending v0.

Corollary 3.4 Given a well-posed instance (G, v0) such that T (v0) 6= VG, let P be a steepest ﬁxable path in (G, v0).
Then, (G, ﬁx[v0, P ]) is also a well-posed instance, and lexG[ﬁx[v0, P ]] = lexG[v0].

Since a lex-minimal element must be an inf-minimizer, we also obtain the following corollary, that can also be

proved using LP duality.

Lemma 3.5 Suppose we have a well-posed instance (G, v0). Then, there exists a complete voltage assignment v
extending v0 such that

grad[v]

∞ ≤ α, iff every terminal path P in (G, v0) satisﬁes ∇P (v0) ≤ α.
(cid:13)
(cid:13)

3.1 Stability

(cid:13)
(cid:13)

The following theorem states that lexG[v0] is monotonic with respect to v0 and it respects scaling and translation of
v0.

Theorem 3.6 Let (G, v0) be a well-posed instance with T := T (v0) as the set of terminals. Then the following
statements hold.

1. For any c, d ∈ R, v1 a partial assignment with terminals T (v1) = T and v1(t) = cv0(t) + d for all t ∈ T .

Then, lexG[v1](i) = c · lexG[v0](i) + d for all i ∈ VG.

2. v1 a partial assignment with terminals T (v1) = T. Suppose further that v1(t) ≥ v0(t) for all t ∈ T. Then,

lexG[v1](i) ≥ lexG[v0](i) for all i ∈ VG.

As a corollary, the above theorem gives a nice stability property that lex-minimal elements satisfy.

Corollary 3.7 Given well-posed instances (G, v0), (G, v1) such that T := T (v0) = T (v1), let ǫ := maxt∈T |v0(t) −
v1(t)|. Then |lexG[v0](i) − lexG[v1](i)| ≤ ǫ for all i ∈ VG.

5

3.2 Alternate Characterizations

There are at least two other seemingly disparate deﬁnitions that are equivalent to lex-minimal voltages.

lp-norm Minimizers. As mentioned in the introduction, for a well-posed instance (G, v0) the lex-minimizer is also
the limit of lp minimizers. This follows from existing results about the limit of lp-minimizers (Egger and Huotari
(1990)) in afﬁne spaces, since {grad[v] | v is complete, v extends v0} forms an afﬁne subspace of Rm. Thus, we have
the following theorem:

Theorem 3.8 (Limit of lp-minimizers, follows from Egger and Huotari (1990)) For any p ∈ (1, ∞), given a well-
posed instance (G, v0) deﬁne vp to be the unique complete voltage assignment extending v0 and minimizing
p ,
i.e.

grad[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Then, limp→∞ vp = lexG[v0].

vp = arg min
v is complete
v extends v0 (cid:13)
(cid:13)

grad[v]

p .

(cid:13)
(cid:13)

Max-Min Gradient Averaging. Consider a well-posed instance (G, v0), and a complete voltage assignment v ex-
tending v0. If G is such that ℓ(e) = 1 for all e ∈ EG, it is easy to see that lex = lexG[v0] satisﬁes the following simple
condition for all x ∈ VG \ T (v0),

lex(x) =

1
2  

max
(x,y)∈EG

lex(y) + min

lex(z)

.

(x,z)∈EG

!

This condition should be contrasted to the optimality condition for l2-regularization on these instances, which gives
for all non-terminals x, the optimal voltage v satisﬁes v(x) = 1

y:(x,y)∈EG v(y).

deg(x)

To prove the above claim, consider locally changing lex at x and observe that the gradients of edges not incident
at x remain unchanged, and at least one of edges incident at x will have a strictly larger gradient, contradicting lex-
minimality. For general graphs, this condition of local optimality can still be characterized by a simple max-min
gradient averaging property as described below.

P

Deﬁnition 3.9 (Max-Min Gradient Averaging) Given a well-posed instance (G, v0), and a complete voltage as-
signment v extending v0, we say that v satisﬁes the max-min gradient averaging property (w.r.t. (G, v0)) if for every
x ∈ VG \ T (v0), we have

grad[v](x, y) = − min

grad[v](x, y).

max
y:(x,y)∈EG

y:(x,y)∈EG

As stated in the theorem below, lexG[v0] is the unique assignment satisfying max-min gradient averaging property.
Shefﬁeld and Smart (2010) proved a variant of this statement for weighted graphs. For completeness, we present a
proof in the appendix.

Theorem 3.10 Given a well-posed instance (G, v0), lexG[v0] satisﬁes max-min gradient averaging property. More-
over, it is the unique complete voltage assignment extending v0 that satisﬁes this property w.r.t. (G, v0).

An advantage of this characterization is that it can be veriﬁed quickly. This is particularly useful for implementations
for computing the lex-minimizer.

4 Algorithms

We now sketch the ideas behind our algorithms and give precise statements of our results. A full description of all the
algorithms is included in the appendix.

We deﬁne the pressure of a vertex to be the gradient of the steepest terminal path through it:

pressure[v0](x) = max{∇P (v0) | P is a terminal path in (G, v0) and x ∈ P }.

6

Observe that in a graph with no terminal-terminal edges, a free terminal path is a steepest ﬁxable path iff its gradient
is equal to the highest pressure amongst all vertices. Moreover, vertices that lie on steepest ﬁxable paths are exactly
the vertices with the highest pressure. For a given α > 0, in order to identify vertices with pressure exceeding α, we
compute vectors vHigh[α](x) and vLow[α](x) deﬁned as follows in terms of dist, the metric on V induced by ℓ:

vLow[α](x) = min

{v0(t) + α · dist(x, t)}

t∈T (v0)

vHigh[α](x) = max
t∈T (v0)

{v0(t) − α · dist(t, x)}.

4.1 Lex-minimization on Star Graphs

We ﬁrst consider the problem of computing the lex-minimizer on a star graph in which every vertex but the center is a
terminal. This special case is a subroutine in the general algorithm, and also motivates some of our techniques.

Let x be the center vertex, T be the set of terminals, and all edges be of the form (x, t) with t ∈ T . The initial
voltage assignment is given by v : T → R, and we abbreviate dist(x, t) by d(t) = ℓ(x, t). From Corollary 3.4 we know
that we can determine the value of the lex minimizer at x by ﬁnding a steepest ﬁxable path. By deﬁnition, we need to
ﬁnd t1, t2 ∈ T that maximize the gradient of the path from t1 to t2, ∇(t1, t2) = v(t1)−v(t2)
d(t2)+d(t2) . As observed above, this
is equivalent to ﬁnding a terminal with the highest pressure. We now present a simple randomized algorithm for this
problem that runs in expected linear time.

Given a terminal t1, we can compute its pressure α along with the terminal t2 such that |∇(t1, t2)| = α in time
O(|T |) by scanning over the terminals in T . Consider doing this for a random terminal t1. We will show that in linear
time one can then ﬁnd the subset of terminals T ′ ⊂ T whose pressure is greater than α. Assuming this, we complete
the analysis of the algorithm. If T ′ = ∅, t1 is a vertex with highest pressure. Hence the path from t1 to t2 is a steepest
ﬁxable path, and we return (t1, t2). If T ′ 6= ∅, the terminal with the highest pressure must be in T ′, and we recurse by
picking a new random t1 ∈ T ′. As the size of T ′ will halve in expectation at each iteration, the expected time of the
algorithm on the star is O(|T |).

To determine which terminals have pressure exceeding α, we observe that the condition ∃t2 : α < ∇(t1, t2) =
v(t1)−v(t2)
d(t1)+d(t2) , is equivalent to ∃t2 : v(t2)+αd(t2) < v(t1)−αd(t1). This, in turn, is equivalent to vLow[α](x) < v(t1)−
αd(t1). We can compute vLow[α](x) in deterministic O(|T |) time. Similarly, we can check if ∃t2 : α < ∇(t2, t1) by
checking if vHigh[α](x) > vt1 + αd(t1). Thus, in linear time, we can compute the set T ′ of terminals with pressure
exceeding α. The above algorithm is described in Algorithm 10.

Theorem 4.1 Given a set of terminals T, initial voltages v : T → R, and distances d : T → R+, STARSTEEPESTPATH(T, v, d)
returns (t1, t2) maximizing v(t1)−v(t2)

d(t1)+d(t2) , and runs in expected time O(|T |).

4.2 Lex-minimization on General Graphs

Theorem 3.3, tells us that META-LEX will compute lex-minimizers given an algorithm for ﬁnding a steepest ﬁxable
path in (G, v0). Recall that ﬁnding a steepest ﬁxable path is equivalent to ﬁnding a path with gradient equal to the
highest pressure amongst all vertices. In this section, we show how to do this in expected time O(m + n log n).

We describe an algorithm VERTEXSTEEPESTPATH that ﬁnds a terminal path P through any vertex x such that
∇P (v0) = pressure[v0](x) in expected O(m + n log n) time. Using Dijkstra’s algorithm, we compute dist(x, t) for
all t ∈ T. If x ∈ T (v0), then there must be a terminal path P that starts at x that has ∇P (v0) = pressure[v0](x). To
compute such a P we examine all t ∈ T (v0) in O(|T |) time to ﬁnd the t that maximizes |∇(x, t)| = |v(x)−v(t)|
, and
dist(x,t)
then return a shortest path between x and that t.

If x /∈ T (v0), then the steepest path through x between terminals t1 and t2 must consist of shortest paths between
x and t1 and between x and t2. Thus, we can reduce the problem to that of ﬁnding the steepest path in a star graph
where x is the only non-terminal and is connected to each terminal t by an edge of length dist(x, t). By Theorem 4.1,
we can ﬁnd this steepest path in O(|T |) expected time. The above algorithm is formally described as Algorithm 9.

Theorem 4.2 Given a well-posed instance (G, v0), and a vertex x ∈ VG, VERTEXSTEEPESTPATH(G, v0, x) returns
a terminal path P through x such that ∇P (v0) = pressure[v0](x), in O(m + n log n) expected time.

7

As in the algorithm for the star graph, we need to identify the vertices whose pressure exceeds a given α. For a ﬁxed
α, we can compute vLow[α](x) and vHigh[α](x) for all x ∈ VG using a simple modiﬁcation of Dijkstra’s algorithm in
O(m + n log n) time. We describe the algorithms COMPVHIGH, COMPVLOW for these tasks in Algorithms 3 and 4.
The following lemma encapsulates the usefulness of vLow and vHigh.

Lemma 4.3 For every x ∈ VG, pressure[v0](x) > α iff vHigh[α](x) > vLow[α](x).

It immediately follows that the algorithm COMPHIGHPRESSGRAPH(G, v0, α) described in Algorithm 6 computes

the vertex induced subgraph on the vertex set {x ∈ VG| pressure[v0](x) > α}.

We can combine these algorithms into an algorithm STEEPESTPATH that ﬁnds the steepest ﬁxable path in (G, v0)
in O(m + n log n) expected time. We may assume that there are no terminal-terminal edges in G. We sample an edge
(x1, x2) uniformly at random from EG, and a terminal x3 uniformly at random from VG. For i = 1, 2, 3, we compute
the steepest terminal path Pi containing xi. By Theorem 4.2, this can be done in O(m + n log n) expected time. Let α
be the largest gradient maxi ∇Pi. As mentioned above, we can identify G′, the induced subgraph on vertices x with
pressure exceeding α, in O(m + n log n) time. If G′ is empty, we know that the path Pi with largest gradient is a
steepest ﬁxable path. If not, a steepest ﬁxable path in (G, v0) must be in G′, and hence we can recurse on G′. Since
we picked a uniformly random edge, and a uniformly random vertex, the expected size of G′ is at most half that of G.
Thus, we obtain an expected running time of O(m + n log n). This algorithm is described in detail in Algorithm 7.

Theorem 4.4 Given a well-posed instance (G, v0) with EG ∩ (T (v0) × T (v0)) = ∅, STEEPESTPATH(G, v0) returns
a steepest ﬁxable path in (G, v0), and runs in O(m + n log n) expected time.

By using STEEPESTPATH in META-LEX, we get the COMPLEXMIN, shown in Algorithm 1. From Theorem 3.3 and
Theorem 4.4, we immediately get the following corollary.

Corollary 4.5 Given a well-posed instance (G, v0) as input, algorithm COMPLEXMIN computes a lex-minimizing
assignment that extends v0 in O(n(m + n log n)) expected time.

4.3 Linear-time Algorithm for Inf-minimization

Given the algorithms in the previous section, it is straightforward to construct an inﬁnity minimizer. Let α⋆ be the
gradient of the steepest terminal path. From Lemma 3.5, we know that the norm of the inf minimizer is α⋆. Considering
all trivial terminal paths (terminal-terminal edges), and using STEEPESTPATH, we can compute α⋆ in randomized
O(m+n log n) time. It is well known (McShane (1934); Whitney (1934)) that v1 = vLow[α⋆] and v2 = vHigh[α⋆] are
inf-minimizers. It is also known that 1
2 (v1 + v2) is the inf-minimizer that minimizes the maximum ℓ∞-norm distance
to all inf-minimizers. In the case of path graphs, this was observed by Gaffney and Powell (1976) and independently
by Micchelli et al. (1976). For completeness, the algorithm is presented as Algorithm 5, and we have the following
result.

Theorem 4.6 Given a well-posed instance (G, v0), COMPINFMIN(G, v0) returns a complete voltage assignment v
for G extending v0 that minimizes

∞ , and runs in randomized O(m + n log n) time.

grad[v]

4.4 Faster Algorithms for Lex-minimization

(cid:13)
(cid:13)

(cid:13)
(cid:13)

The lex-minimizer has additional structure that allows one to compute it by more efﬁcient algorithms. One observation
that leads to a faster implementation is that ﬁxing a steepest ﬁxable path does not increase the pressure at vertices,
provided that one appropriately ignores terminal-terminal edges. Thus, if G(α) is a subgraph that we identiﬁed with
pressure greater than α, we can iteratively ﬁx all steepest ﬁxable paths P in G(α) with ∇P > α. Another simple
observation is that if G(α) is disconnected, we can simply recurse on each of the connected components. A complete
description of an the algorithm COMPFASTLEXMIN based on these idea is given in Algorithm 11. The algorithm
provably computes lexG(v0), and it is possible to implement it so that the space requirement is only O(m + n).
Although, we are unable to prove theoretical bounds on the running time that are better than O(n(m + n log n)),
it runs extremely quickly in practice. We used it to perform the experiments in this paper. For random regular
graphs and Delaunay graphs, with n = 0.5 × 106 vertices and around 2 million edges m ∼ 1.5 − 2 × 106, it

8

takes a couple of minutes on a 2009 MacBook Pro. Similar times are observed for other model graphs of this
size such as random regular graphs and real world networks. An implementation of this algorithm may be found
at https://github.com/danspielman/YINSlex.

5 Directed Graphs

Our deﬁnitions and algorithms, including those for regularization, extend to directed graphs with only small modiﬁ-
cations. We view directed edges as diodes and only consider potential differences in the direction of the edge. For
a complete voltage assignment v on the vertices of a directed graph G, we deﬁne the directed gradient on (x, y) due
to v to be grad+
. Given a partially-labelled directed graph (G, v0), we say that a a
complete voltage assignment v is a lex-minimizer if it extends v0 and for other complete voltage assignment v′ that
extends v0 we have grad+
G[v′]. We say that a partially-labelled directed graph (G, v0) is a well-posed
directed instance if every free vertex appears in a directed path between two terminals.

G[v](x, y) = max

G[v] (cid:22) grad+

v(x)−v(y)
ℓ(x,y)

, 0

n

o

The main difference between the directed and undirected cases is that the directed lex-minimizer is not necessarily
unique. To maintain clarity of exposition, we chose to focus on undirected graphs so far. For directed graphs, we have
the following corresponding structural results.

Theorem 5.1 Given a well-posed instance (G, v0) on a directed graph G, there exists a lex-minimizer, and the set of
all lex-minimizers is a convex set. Moreover, for every two lex-minimizers v and v′, we have grad+

G[v] = grad+

G[v′].

However, note that in the case of directed graphs, the lex-minimizer need not be unique. We still have a weaker version
of Theorem 3.3 for directed graphs.

Theorem 5.2 Given a well-posed instance (G, v0) on a directed graph G, let v1 be the partial voltage assignment
extending v0 obtained by repeatedly ﬁxing steepest ﬁxable (directed) paths P with ∇P > 0. Then, any lex-minimizer
of (G, v0) must extend v1. Moreover, for every edge e ∈ EG \ (T (V1) × T (V1)), any lex-minimizer v of (G, v0) must
satisfy grad+[v](e) = 0.

When the value of the lex-minimizer at a vertex is not uniquely determined, it is constrained to an interval. In our
experiments, we pick the convention that when the voltage at a vertex is constrained to an interval (−∞, a] or [a, ∞),
we assign a to the terminal. When it is constrained to a ﬁnite interval, we assign a voltage closest to the median of the
original voltages.

6 Experiments on WebSpam

We demonstrate the performance of our lex-minimization algorithms on directed graphs by using them to detect spam
webpages as in Zhou et al. (2007). We use the dataset webspam-uk2006-2.0 described in Castillo et al. (2006).
This collection includes 11,402 hosts, out of which 7,473 (65.5 %) are labeled, either as spam or normal. Each host
corresponds to the collection of web pages it serves. Of the hosts, 1924 are labeled spam (25.7 % of all labels). We
consider the problem of ﬂagging some hosts as spam, given only a small fraction of the labels for training. We assign
a value of 1 to the spam hosts, and a value of 0 to the normal ones. We then compute a lex minimizer and examine the
effect of ﬂagging as spam all hosts with a value greater than some threshold.

Following Zhou et al. (2007), we create edges between hosts with lengths equal to the reciprocal of the number of
links from one to the other. We run our experiments only on the largest strongly connected component of the graph,
which contains 7945 hosts of which 5552 are labeled. 16 % of the nodes in this subgraph are labeled spam. To create
training and test data, for a given value p, we select a random subset of p % of the spam labels and a random subset
of p % of the normal labels to use for training. The remaining labels are used for testing. We report results for p = 5
and p = 20.

Again following Zhou et al. (2007), we plot the precision and recall of different choices of threshold for ﬂagging
pages as spam. Recall is the fraction of spam pages our algorithm ﬂags as spam, and precision is the fraction of pages
our algorithm ﬂags as spam that actually are spam. Amongst the algorithms studied by Zhou et al. (2007), the top

9

performer was their algorithm based on sampling according to a random-walk that follows in-links from other hosts.
We compare their algorithm with the classiﬁcation we get by directing edges in the opposite directions of links. This
has the effect that a link to a spam host is evidence of spamminess, and a link from a normal host is evidence of
normality.

Results are shown in Figure 3. While we are not able to reliably ﬂag all spam hosts, we see that in the range of
10-50 % recall, we are able to ﬂag spam with precision above 82 %. We see that the performance of directed lex-
minimization does not degrade rapidly when from the “large training set” regime of p = 20, to the “small training set”
regime of p = 5.

5 % labels for training

20 % labels for training

RandWalk
DirectedLex

RandWalk
DirectedLex

i

i

n
o
s
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
0.1

0.2

0.3

0.4

0.7

0.8

0.9

1

0.2

0.3

0.4

0.7

0.8

0.9

1

0.6
0.5
Recall

0.6
0.5
Recall

Figure 3: Recall and precision in the web spam classiﬁcation experiment. Each data point shown was computed as an average over
100 runs. The largest standard deviation of the mean precision across the plotted recall values was less than 1.3 %. The algorithm
of Zhou et al. (2007) appears as RANDWALK. Our directed lex-minimization algorithm appears as DIRECTEDLEX.

For comparison, in Appendix C, we show the performance of our algorithm and that of Zhou et al. (2007) both
with link directions reversed, as well as the performance of undirected lex-minimization and Laplacian inference, all
of which are signiﬁcantly worse.

7 l0-Regularization of Vertex Values

We now explain how we can accommodate noise in both the given voltages and in the given lengths of edges. We can
ﬁnd the minimum number of labels to ignore, or the minimum increase in edges lengths needed so that there exists an
extension whose gradients have l∞-norm lower than a given target. After determining which labels to ignore or the
needed increment in edge lengths, we recommend computing a lex minimizer.

The algorithms we present in this section are essentially the same for directed and undirected graphs.

7.1 l0-Vertex Regularization for Inf-minimization

The l0-regularization of vertex labels can be viewed as a problem of outlier removal: the vector we compute is allowed
to disagree with v0 on up to k terminals. Given a voltage assignment v and a subset T ⊂ V of the vertices, by v(T )
we mean the vector obtained by restricting v to T . We deﬁne the l0-Vertex Regularization for l∞ problem to be

where v(T ) is the vector of values of v on the terminals T .

min
v∈IRn

gradG[v]

∞

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

0 ≤ k,
(cid:13)
(cid:13)

subject to

v(T ) − v0(T )

(3)

In Appendix D, we describe an approximation algorithm APPROX-OUTLIER that approximately solves program (3).

The precise statement we prove in Appendix D is given in the following theorem.

1

0.9

0.8

0.7

i

i

n
o
s
c
e
r
P

0.6

0.5

0.4

0.3

0.2

0.1

0
0.1

10

Theorem 7.1 (Approximate l0-vertex regularization) The algorithm APPROX-OUTLIER takes a positive integer k
and a partially-labeled graph (G, v0), and outputs an assignment v with
0 ≤ 2k, and
∞ ≤
α∗, where α∗ is the optimum value of program (3). The algorithm runs in time O(k(m + n log n)).
(cid:13)
(cid:13)
(cid:13)
(cid:13)

In Appendix D, we also describe an algorithm OUTLIER that exactly solves program (3) in polynomial time, and we
prove its correctness.

v(T ) − v0(T )

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Theorem 7.2 (Exact l0-vertex regularization) The algorithm OUTLIER takes a positive integer k and a partially-
labeled graph (G, v0) solves program (3) exactly. The algorithm runs in polynomial time.

We give a proof of Theorem 7.2 in Appendix D. To do this, we reduce the program (3) to the problem of minimizing
the required l0-budget needed to achieve a ﬁxed gradient α using a binary search over a set of O(n2) gradients. This
latter problem we reduce in polynomial time to Minimum Vertex Cover (VC) on a transitively closed, directed acyclic
graph (a TC-DAG). VC on a TC-DAG can be solved exactly in polynomial time by a reduction to the Maximum
Bipartite Matching Problem (Fulkerson (1956)). The problem was phrased by Fulkerson as one of ﬁnding a maximum
antichain of a ﬁnite poset. Any transitively closed DAG corresponds directly to the comparability graph of a poset. A
maximum antichain of a poset is a maximum independent set of a the comparability graph of the poset, and hence its
complement is a minimum vertex cover of the comparability graph. We refer to the algorithm developed by Fulkerson
as KONIG-COVER.

Theorem 7.3 The algorithm KONIG-COVER computes a minimum vertex cover for any transitively closed DAG G in
polynomial time.

7.2 Hardness of l0 regularization for l2

The result that l0-regularized inf-minimization can be solved exactly in polynomial time is surprising, especially
because the analogous problem for 2-Laplacian minimization turns out to be NP-Hard.

We deﬁne the the l0 vertex regularization for l2 for a partially-labeled graph (G, v0) and an integer k by

min
v∈Rn:kv(T )−v0(T )k0

≤k

vT Lv,

where L is the Laplacian of G.

Theorem 7.4 l0 vertex regularization for l2 is NP-Hard.

In Appendix E we prove Theorem 7.4 by giving a polynomial time (Karp) reduction from the NP-Hard minimum
bisection problem to l0 vertex regularization for l2.

8 l1-Edge and Vertex Regularization of Inf-minimizers

Consider a partially-labeled graph (G, v0) and an α > 0. The set of voltage assignments given by

v : v extends v0 and

gradG[v]

∞ ≤ α

n

(cid:13)
(cid:13)

(cid:13)
(cid:13)

o

is convex. Going further, let us consider the edge lengths in a graph to be speciﬁed by a vector ℓ ∈ IRE. Now the set
of voltages v and and lengths ℓ which achieve kgradG(ℓ)[v]k∞ ≤ α is jointly convex in v and ℓ. To see this, observe
that

kgradG(ℓ)[v]k∞ ≤ α ⇔ ∀(u, v) ∈ E : −αℓ(u, v) ≤ v(u) − v(v) ≤ αℓ(u, v).
Furthermore, the condition “v extends v0” is a linear constraint on v, which we express as v(T ) = v0(T ). From
the above, it is clear that the gradient condition corresponds to a convex set, as it is an intersection of half-spaces.
These half-spaces are given by O(m) linear inequalities. We can leverage this to phrase many regularized variants of
inf-minimization as convex programs, and in some cases linear programs.

(4)

11

For example, we may consider a variant of inf-minimization combined with an l1-budget for changing lengths of
edges and values on terminals. Given a parameter γ > 0 which speciﬁes the relative cost of regularizing terminals to
regularizing edges, the problem is as follows

arg min
v∈IRn,s∈IRm,s≥0

ksk1 + γ

v(T ) − v0(T )

1

subject to

gradG(ℓ+s)[v]

≤ α.

(5)

(cid:13)
(cid:13)
From our observation (4), it follows that problem (5) may be expressed as a linear program with O(n) variables
and O(m) constraints. We can use ideas from Daitch and Spielman (2008) to solve the resulting linear program in
O(m1.5) by an interior point method with a special purpose linear equation solver. The reason is that the linear
time
equations the IPM must solve at each iteration may be reduced to linear equations in symmetric, diagonally dominant
matrices, and these may be solved in nearly-linear time (Cohen et al. (2014)).

(cid:13)
(cid:13)

e

(cid:13)
(cid:13)
(cid:13)

∞

(cid:13)
(cid:13)
(cid:13)

Conclusion. We propose the use of inf and lex minimizers for regression on graphs. We present simple algorithms
for computing them that are provably fast and correct, and can also be implemented efﬁciently. We also present a
framework and polynomial time algorithms for regularization in this setting. The initial experiments reported in the
paper indicate that these algorithms give pretty good results on real and synthetic datasets. The results seem to compare
quite favorably to other algorithms, particularly in the regime of tiny labeled sets. We are testing these algorithms on
several other graph learning questions, and plan to report on them in a forthcoming experimental paper. We believe
that inf and lex minimizers, and the associated ideas presented in the paper, should be useful primitives that can be
proﬁtably combined with other approaches to learning on graphs.

We thank anonymous reviewers for helpful comments. We thank Santosh Vempala and Bartosz Walczak for pointing
out that it was already known how to compute a minimum vertex cover of a transitively closed DAG in polynomial
time.

Acknowledgements

References

Morteza Alamgir
In Advances
Information Processing
http://books.nips.cc/papers/files/nips24/NIPS2011_0278.pdf.

and Ulrike V. Luxburg.

transition
24,

in
pages

in Neural

Systems

Phase

the

family
379–387.

of
2011.

p-resistances.
URL

Gunnar Aronsson. Extension of functions satisfying lipschitz conditions. Arkiv fr Matematik, 6(6):551–561, 1967.

ISSN 0004-2080. doi: 10.1007/BF02591928. URL http://dx.doi.org/10.1007/BF02591928.

Gunnar Aronsson, Michael G. Crandall, and Petri Juutinen. A tour of the theory of absolutely minimizing functions.
ISSN 0273-0979. doi: 10.1090/S0273-0979-04-01035-3.

Bull. Amer. Math. Soc. (N.S.), 41(4):439–505, 2004.
URL http://dx.doi.org/10.1090/S0273-0979-04-01035-3.

Guy Barles and J´erˆome Busca. Existence and comparison results for fully nonlinear degenerate elliptic equations

without zeroth-order term. Comm. Partial Differential Equations, 26:2323–2337, 2001.

Mikhail Belkin, Irina Matveeva, and Partha Niyogi.

Regularization and semi-supervised learning on large
In Learning Theory, volume 3120 of Lecture Notes in Computer Science, pages 624–638.
doi: 10.1007/978-3-540-27819-1 43. URL

graphs.
Springer Berlin Heidelberg, 2004.
http://dx.doi.org/10.1007/978-3-540-27819-1_43.

ISBN 978-3-540-22282-8.

Nick Bridle and Xiaojin Zhu. p-voltages: Laplacian regularization for semi-supervised learning on high-dimensional

data. In Eleventh Workshop on Mining and Learning with Graphs (MLG2013), 2013.

12

Carlos Castillo, Debora Donato, Luca Becchetti, Paolo Boldi, Stefano Leonardi, Massimo Santini, and Sebastiano
Vigna. A reference collection for web spam. SIGIR Forum, 40(2):11–24, December 2006. ISSN 0163-5840. doi:
10.1145/1189702.1189703. URL http://doi.acm.org/10.1145/1189702.1189703.

Olivier Chapelle, Bernhard Schlkopf, and Alexander Zien. Semi-Supervised Learning. The MIT Press, 1st edition,

2010. ISBN 0262514125, 9780262514125.

Michael B Cohen, Rasmus Kyng, Gary L Miller, Jakub W Pachocki, Richard Peng, Anup B Rao, and Shen Chen Xu.
Solving SDD linear systems in nearly m log1/2 n time. In Proceedings of the 46th Annual ACM Symposium on
Theory of Computing, pages 343–352. ACM, 2014.

M.G. Crandall, L.C. Evans, and R.F. Gariepy. Optimal lipschitz extensions and the inﬁnity laplacian. Calculus of Vari-
ations and Partial Differential Equations, 13(2):123–139, 2001. ISSN 0944-2669. doi: 10.1007/s005260000065.
URL http://dx.doi.org/10.1007/s005260000065.

Samuel I. Daitch and Daniel A. Spielman. Faster approximate lossy generalized ﬂow via interior point algo-
rithms.
In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing, STOC ’08, pages
451–460, New York, NY, USA, 2008. ACM. ISBN 978-1-60558-047-0. doi: 10.1145/1374376.1374441. URL
http://doi.acm.org/10.1145/1374376.1374441.

Alan Egger and Robert Huotari. Rate of convergence of the discrete polya algorithm. Journal of Approximation
ISSN 0021-9045. doi: http://dx.doi.org/10.1016/0021-9045(90)90070-7. URL

Theory, 60(1):24 – 30, 1990.
http://www.sciencedirect.com/science/article/pii/0021904590900707.

Charles Fefferman. Whitney’s extension problems and interpolation of data.

(N.S.), 46(2):207–220, 2009a.
http://dx.doi.org/10.1090/S0273-0979-08-01240-8.

ISSN 0273-0979.

doi:

10.1090/S0273-0979-08-01240-8.

Bull. Amer. Math. Soc.
URL

Charles Fefferman. Fitting a [image] -smooth function to data, iii. Annals of Mathematics, 170(1):pp. 427–441, 2009b.

ISSN 0003486X. URL http://www.jstor.org/stable/40345469.

Charles Fefferman and Bo’az Klartag. Fitting a cm -smooth function to data i. Annals of Mathematics, 169(1):pp.

315–346, 2009. ISSN 0003486X. URL http://www.jstor.org/stable/40345445.

D. R. Fulkerson. Note on dilworths decomposition theorem for partially ordered sets. Proc. Amer. Math. Soc, 1956.

P.W. Gaffney and M.J.D. Powell. Optimal interpolation. In Numerical Analysis, volume 506 of Lecture Notes in Math-
ematics, pages 90–99. Springer Berlin Heidelberg, 1976. ISBN 978-3-540-07610-0. doi: 10.1007/BFb0080117.
URL http://dx.doi.org/10.1007/BFb0080117.

L.-A. Gottlieb, A. Kontorovich, and R. Krauthgamer. Efﬁcient classiﬁcation for metric data. CoRR, abs/1306.2547,

2013. URL http://arxiv.org/abs/1306.2547.

L.-A. Gottlieb, A. Kontorovich, and R. Krauthgamer. Efﬁcient regression in metric spaces via approximate lipschitz
extension. In Similarity-Based Pattern Recognition, volume 7953 of Lecture Notes in Computer Science, pages
43–58. Springer Berlin Heidelberg, 2013b. ISBN 978-3-642-39139-2. doi: 10.1007/978-3-642-39140-8 3. URL
http://dx.doi.org/10.1007/978-3-642-39140-8_3.

Robert Jensen. Uniqueness of lipschitz extensions: Minimizing the sup norm of the gradient. Archive for Ra-
doi: 10.1007/BF00386368. URL

ISSN 0003-9527.

tional Mechanics and Analysis, 123(1):51–74, 1993.
http://dx.doi.org/10.1007/BF00386368.

M. Kirszbraun. ber die zusammenziehende und lipschitzsche transformationen. Fundamenta Mathematicae, 22(1):

77–108, 1934. URL http://eudml.org/doc/212681.

13

Andrew J. Lazarus, Daniel E. Loeb,

James G. Propp, Walter R. Stromquist,

Combinatorial games under

man.
229 – 264,
http://www.sciencedirect.com/science/article/pii/S0899825698906765.

http://dx.doi.org/10.1006/game.1998.0676.

and Economic Behavior,

ISSN 0899-8256.

auction play.

Games

1999.

doi:

and Daniel H. Ull-
27(2):
URL

Yunqian Ma and Yun Fu. Manifold Learning Theory and Applications. CRC Press, Inc., Boca Raton, FL, USA, 1st

edition, 2011. ISBN 1439871094, 9781439871096.

E. J. McShane. Extension of range of functions. Bull. Amer. Math. Soc., 40(12):837–842, 12 1934. URL

http://projecteuclid.org/euclid.bams/1183497871.

C.A. Micchelli, T.J. Rivlin,

and S. Winograd.

merische Mathematik, 26(2):191–200, 1976.
http://dx.doi.org/10.1007/BF01395972.

The optimal
ISSN 0029-599X.

recovery of
doi:

smooth functions.
10.1007/BF01395972.

Nu-
URL

V. A. Milman.

Absolutely minimal extensions of

functions on metric spaces.

1999.

URL

http://iopscience.iop.org/1064-5616/190/6/A05/pdf/MSB_190_6_A05.pdf.

Boaz Nadler, Nathan Srebro, and Xueyuan Zhou. Statistical analysis of semi-supervised learning: The limit of inﬁnite
unlabelled data. 2009. URL http://ttic.uchicago.edu/˜nati/Publications/NSZnips09.pdf.

A. Naor and S. Shefﬁeld. Absolutely minimal Lipschitz extension of tree-valued mappings. CoRR, abs/1005.2535,

May 2010. URL http://arxiv.org/abs/1005.2535.

A. M. Oberman. Finite difference methods for the Inﬁnity Laplace and p-Laplace equations. CoRR, abs/1107.5278,

July 2011. URL http://arxiv.org/abs/1107.5278.

Yuval Peres, Oded Schramm, Scott Shefﬁeld, and DavidB. Wilson.

Tug-of-war and the inﬁnity lapla-
In Selected Works of Oded Schramm, Selected Works in Probability and Statistics, pages 595–
doi: 10.1007/978-1-4419-9675-6 18. URL

cian.
638. Springer New York, 2011.
http://dx.doi.org/10.1007/978-1-4419-9675-6_18.

ISBN 978-1-4419-9674-9.

S. Shefﬁeld and C. K. Smart. Vector-valued optimal Lipschitz extensions. CoRR, abs/1006.1741, June 2010. URL

http://arxiv.org/abs/1006.1741.

Ali Kemal Sinop and Leo Grady. A seeded image segmentation framework unifying graph cuts and random walker
which yields a new algorithm. In Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on,
pages 1–8. IEEE, 2007.

Vijay V. Vazirani. Approximation Algorithms. Springer-Verlag New York, Inc., New York, NY, USA, 2001. ISBN

3-540-65367-8.

Ulrike von Luxburg and Olivier Bousquet. Distance-based classiﬁcation with lipschitz functions.

In Learn-
ing Theory and Kernel Machines, volume 2777 of Lecture Notes in Computer Science, pages 314–328.
doi: 10.1007/978-3-540-45167-9 24. URL
Springer Berlin Heidelberg, 2003.
http://dx.doi.org/10.1007/978-3-540-45167-9_24.

ISBN 978-3-540-40720-1.

Hassler Whitney.

Analytic extensions of differentiable functions deﬁned in closed sets.

tions of
http://www.jstor.org/stable/1989708.

the American Mathematical Society, 36(1):pp. 63–89, 1934.

ISSN 00029947.

Transac-
URL

Dengyong Zhou, Christopher J. C. Burges, and Tao Tao. Transductive link spam detection.

In Proceedings
of the 3rd International Workshop on Adversarial Information Retrieval on the Web, AIRWeb ’07, pages 21–
ISBN 978-1-59593-732-2. doi: 10.1145/1244408.1244413. URL
28, New York, NY, USA, 2007. ACM.
http://doi.acm.org/10.1145/1244408.1244413.

Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic

functions. In IN ICML, pages 912–919, 2003.

14

A Basic Properties of Lex-Minimizers

A.1 Meta Algorithm

Algorithm 1: Algorithm META-LEX: Given a well-posed instance (G, v0), outputs lexG[v0].
for i = 1, 2, . . . :

1. if T (vi−1) = VG, then return vi−1.
2. E′ = EG \ (T (vi−1) × T (vi−1)), G′ := (VG, E′).
3. Let P ⋆
4. vi ← ﬁx[vi−1, P ⋆
i ].

i be a steepest ﬁxable path in (G′, vi−1). Let α⋆

i ← ∇P ⋆(vi−1).

In this subsection, we prove the results that appeared in section 2. We start with a simple observation.

Proposition A.1 Given a well-posed instance (G, v0) such that T (v0) 6= V, let P be a steepest ﬁxable path in (G, v0).
Then, ﬁx[v0, P ] extends v0, and (G, ﬁx[v0, P ]) is also a well-posed instance.

The properties we prove below do not depend on the choice of the steepest ﬁxable path.

Proposition A.2 For any well-posed instance (G, v0), with |VG| = n, META-LEX(G, v0) terminates in at most n
iterations, and outputs a complete voltage assignment v that extends v0.

Proof of Proposition A.2: By Proposition A.1, at any iteration i, vi−1 extends v0 and (G′, vi−1) is a well-posed
instance. META-LEX only outputs vi−1 iff T (vi−1) = V, which means vi−1 is a complete voltage assignment. For
any vi−1 that is not complete, for any x ∈ V \T (vi−1), we must have a free terminal path in (G′, vi−1) that contains x.
i exists in (G′, vi−1). Since P ⋆
Hence, a steepest ﬁxable path P ⋆
i ] ﬁxes the voltage
i
✷
for at least one non-terminal. Thus, META-LEX(G, v0) must complete in at most n iterations.

is a free terminal path, ﬁx[vi−1, P ⋆

For the following lemmas, consider a run of META-LEX with well-posed instance (G, v0) as input. Let vout be the
complete voltage assignment output by META-LEX. Let Ei be the set of edges E′ and Gi be the graph G′ constructed
in iteration i of META-LEX.

Lemma A.3 For every edge e ∈ Ei−1 \ Ei, we have

grad[vout](e)

≤ α⋆

i . Moreover, α⋆

i is non-increasing with i.

Proof of Lemma A.3: Let P ⋆
i = (x0, . . . , xr) be a steepest ﬁxable path in iteration i (when we deal with instance
(Gi−1, vi−1)). Consider a terminal path Pi+1 in (Gi, vi) such that {∂0Pi+1, ∂1Pi+1} ∩ (T (vi) \ T (vi−1)) 6= ∅. We
i . On the contrary, assume that ∇Pi+1(vi) > α⋆
claim that ∇Pi+1(vi) ≤ α⋆
i . Consider the case ∂0Pi+1 ∈ T (vi) \
T (vi−1), ∂1P1 ∈ T (vi−1). By the deﬁnition of vi, we must have ∂0Pi+1 = xj for some j ∈ [r − 1]. Let P ′
i+1 be the
path formed by joining paths P ⋆

i+1 is a free terminal path in (Gi−1, vi−1). We have,

i [x0 : xj] and Pi+1. P ′

(cid:12)
(cid:12)

(cid:12)
(cid:12)

vi−1(x0) − vi−1(∂1Pi+1) = (vi(x0) − vi(xj )) + (vi(∂0Pi+1) − vi(∂1Pi+1))
i · ℓ(P ′

i · ℓ(Pi+1) = α⋆

i [x0 : xj]) + α⋆

i · ℓ(P ⋆

> α⋆

i+1),

giving ∇P ′
The other cases can be handled similarly.

i+1(vi) > α⋆

i , which is a contradiction since the steepest ﬁxable path P ⋆
i

in (Gi−1, vi−1) has gradient α⋆
i .

Applying the above claim to an edge e ∈ Ei−1 \ Ei, whose gradient is ﬁxed for the ﬁrst time in iteration i, we
i . If v is the complete voltage assignment output by META-LEX, since v extends vi+1,
i , implying

i . Applying the claim to the symmetric edge, we obtain −grad[vout](e) ≤ α⋆

obtain that grad[vi+1](e) ≤ α⋆
we get grad[vout](e) ≤ α⋆
|grad[vout](e)| ≤ α⋆
i .

Consider any free terminal path Pi+1 in (Gi, vi). If Pi+1 is also a terminal path in (Gi−1, vi−1), it is a free
terminal path in (Gi−1, vi−1). In addition, since a steepest ﬁxable path P ⋆
i , we get
i
∇Pi+1(vi) = ∇Pi+1(vi−1) ≤ α⋆
i . Otherwise, we must have {∂0Pi+1, ∂1Pi+1} ∩ (T (vi) \ T (vi−1)) 6= ∅, and we can
deduce ∇Pi+1(vi) ≤ α⋆
i using the above claim. Thus, all free terminal paths Pi+1 in (Gi, vi) satisfy ∇Pi+1(vi) ≤ α⋆
i .
✷
In particular, α⋆

in (Gi−1, vi−1) has ∇P ⋆

i = α⋆

i is non-increasing with i.

i+1(vi) ≤ α⋆

i+1 = ∇P ⋆

i . Thus, α⋆

15

Lemma A.4 For any complete voltage assignment v for G that extends v0, if v 6= vout, we have grad[v] 6(cid:22) grad[vout],
and hence grad[vout] (cid:22) grad[v].

Proof of Lemma A.4: Consider any complete voltage assignment v for G that extends v0, such that v 6= vout. Thus,
there exists a unique i such that v extends vi−1 but does not extend vi. We will argue that grad[v] 6(cid:22) grad[vout], and
hence grad[vout] (cid:22) grad[v]. For every edge e ∈ E \ Ei−1 that has been ﬁxed so far, grad[v](e) = grad[vi−1](e) =
grad[vout](e), and hence we can ignore these edges.

Since v extends vi−1 but not vi, there exists an x ∈ T (vi) \ T (vi−1) such that v(x) 6= vi(x) = vout(x). Assume
i picked

i = (x0, . . . , xr) is the steepest ﬁxable path with gradient α⋆

v(x) < vi(x) (the other case is symmetric). If P ⋆
in iteration i, we must have x = xj for some j ∈ [r − 1]. Thus,

j

j

(v(xk−1) − v(xk)) = v(x0) − v(xj ) > vi(x0) − vi(xj ) = α⋆

i · ℓ(P ⋆

i [x0 : xj ]) = α⋆
i ·

ℓ(xk−1, xk).

Xk=1

Xk=1
Thus, for some k ∈ [j], we must have grad[v](xk−1, xk) > α⋆
is a path in Gi−1, we have {xk−1, xk} 6⊆
T (vi−1). This gives (xk−1, xk) ∈ (Ei−1 \ Ei). But then, from Lemma A.3, it follows that for all e ∈ (Ei−1 \ Ei), we
✷
have |grad[vout](e)| ≤ α⋆

i . Thus, we have grad[v] 6(cid:22) grad[vout].

i . Since P ∗
i

Lemma A.5 Let P = (x0, . . . , xr) be a steepest ﬁxable path such that it does not have any edges in T (v0) × T (v0)
and v1 = ﬁxG[v0, P ]. Then for every i ∈ [r], we have grad[v1](xi−1, xi) = ∇P.

Proof of Lemma A.5: Suppose this is not true and let j ∈ [r] be the minimum number such that grad[v1](xj−1, xj) 6=
∇P. By deﬁnition of v1 we would necessarily have j < r and vj ∈ T (v0). Suppose grad[v1](xj−1, xj ) < ∇P. We
would then have v1(x0) − v1(xj ) < ∇P ∗ ℓ(P [x0 : xj]). Since P does not have any edges in T (v0) × T (v0),
P1 := (xj, ..., xr) would be a free terminal path with ∇P1 > ∇P. This is a contradiction. Other cases can be ruled
out similarly.

✷

Proof of Theorem 3.3: Consider an arbitrary run of META-LEX on (G, v0). Let vout be the complete voltage
assignment output by META-LEX. Proposition A.1 implies that vout extends v0. Lemma A.4 implies that for any
complete voltage assignment v 6= vout that extends v0, we have grad[vout] (cid:22) grad[v]. Thus, vout is a lex-minimizer.
Moreover, the lemma also gives that for any such v, grad[v] 6(cid:22) grad[vout]. and hence vout is a unique lex-minimizer.
Thus, vout is the unique voltage assignment satisfying Def. 2.1, and we denote it as lexG[v0]. Since we started with an
✷
arbitrary run of META-LEX, uniqueness implies that every run of META-LEX on (G, v0) must output lexG[v0].

Proof of Lemma 3.5: Suppose we have a complete voltage assignment v extending v0, such that
For any terminal path P = (x0, . . . , xr), we get,

grad[v]

∞ ≤ α.

∇P (v0) = v0(∂0P ) − v0(∂1P ) = v(∂0P ) − v(∂1P ) =

grad[v](xi−1, xi) ≤ α ·

ℓ(xi−1, xi) = α · ℓ(P ),

(cid:13)
(cid:13)

(cid:13)
(cid:13)

r

i=1
X

giving ∇P (v0) ≤ α.

On the other hand, suppose every terminal path P in (G, v0) satisﬁes ∇P (v0) ≤ α. Consider v = lexG[v0]. We
know that v extends v0. For every edge e ∈ EG ∩ T (v0) × T (v0), e is a (trivial) terminal path in (G, v0), and hence
has satisﬁes grad[v](e) = grad[v0](e) = ∇e(v0) ≤ α. Considering the reverse edge, we also obtain −grad[v](e) ≤ α.
Thus, |grad[v](e)| ≤ α. Moreover, using Lemma A.3, we know that for edge e ∈ EG \ T (v0) × T (v0), |grad[v](e)| ≤
1 = ∇P ⋆
α⋆
1 ≤ α since P1 is a terminal path in (G, v0). Thus, for every e ∈ EG, |grad[v](e)| ≤ α, and hence
✷
grad[v]
∞ ≤ α.
(cid:13)
(cid:13)
(cid:13)
(cid:13)
A.2 Stability

In this subsection, we sketch a proof of the monotonicity of lex-minimizers and show how it implies the stability
property claimed earlier.

For any well-posed (G, v0), there could be several possible executions of META-LEX, each characterized by the

sequence of paths P ⋆

i . We can apply Theorem 3.3 to deduce the following structural result about the lex-minimizer.

r

i=1
X

16

Corollary A.6 For any well-posed instance (G, v0), consider a sequence of paths (P1, . . . , Pr) and voltage assign-
ments (v1, . . . , vr) for some positive integer r such that:

1. P ⋆

i is a steepest ﬁxable path in (Gi−1, vi−1) for i = 1, . . . , r.

2. vi = ﬁx[vi−1, P ⋆

i ] for i = 1, . . . , r.

3. T (vr) = VG.

Then, we have vr = lexG[v0].

We call such a sequence of paths and voltages to be a decomposition of lexG[v0]. Again, note that lexG[v0] can
possibly have multiple decompositions. However, any two such decompositions are consistent in the sense that they
produce the same voltage assignment.

Proof of Corollary 3.7: We ﬁrst deﬁne some operations on partial assignments which simpliﬁes the notation. Let
v0, v1 be any two partial assignments with the same set of terminals T := T (v0) = T (v1) and c, d ∈ R. By cv0 + d
we mean a partial assignment v with T (v) = T satisfying v(t) = cv0(t) + d for all t ∈ T . Also, by v0 + v1 we
mean a partial assignment v with T (v) = T satisfying v(t) = v0(t) + v1(t) for all t ∈ T. Also, we say v1 ≥ v0 if
v1(t) ≥ v0(t) for all t ∈ T .

Now we can show how Corollary 3.7 follows from Theorem 3.6. Let v := v1 − v0, and kvk∞ = ǫ, for some ǫ > 0.
Therefore, v0 + ǫ ≥ v1 ≥ v0 − ǫ. Theorem 3.6 then implies that lexG[v0] + ǫ ≥ lex[v1] ≥ lex[v0] − ǫ, hence proving
✷
the corollary.

Proof sketch of Theorem 3.6:
It is easy to see that the ﬁrst statement holds. For the second statement, we ﬁrst
observe that if there is a sequence of paths P1, ..., Pr that is simultaneously a decomposition of both lex[v0] and
lex[v1], then this is easy to see. If such a path sequence doesn’t exist, then we look at vt := v0 + t(v1 − v0). We
state here without a proof (though the proof is elementary) that we can then split the interval [0, 1] into ﬁnitely many
subintervals [a0, a1], [a1, a2], .., [ak−1, ak], with a0 = 0, ak = 1, such that for any i, there is a path sequence P1, ..., Pr
which is a decomposition of lex[vt] for all t ∈ [ai, ai+1]. We then observe that v0 = va0 ≤ va1 ≤ ...vak = v1. Since
for every ai, ai+1, there is a path sequence which is simultaneously a decomposition of both lex[vai ] and lex[vai+1 ],
we immediately get

lex[v0] = lex[va0 ] ≤ lex[va1] ≤ ... ≤ lex[vak ] = lex[v1].

✷

A.3 Alternate Characterizations

Proof of Theorem 3.10: We know that lexG[v0] extends v0. We ﬁrst prove that v = lexG[v0] satisﬁes the max-min
gradient averaging property. Assume to the contrary. Thus, there exists x ∈ VG \ T (v0) such that

max
y:(x,y)∈EG

grad[v](x, y) 6= − min

grad[v](x, y).

y:(x,y)∈EG

Assume that max(x,y)∈EG grad[v](x, y) ≥ − min(x,y)∈EG grad[v](x, y). Then, consider v′ extending v0 that is iden-
tical to v except for v′(x) = v(x) − ǫ for ǫ > 0. For ǫ small enough, we get that

and

max
y:(x,y)∈EG

grad[v′](x, y) < max

grad[v](x, y)

y:(x,y)∈EG

− min

y:(x,y)∈EG

grad[v′](x, y) < max

grad[v](x, y).

y:(x,y)∈EG

The gradient of edges not incident on the vertex x is left unchanged. This implies that grad[v]

6(cid:22) grad[v′],

contradicting the assumption that v is the lex-minimizer. (The other case is similar).

17

For the other direction. Consider a complete voltage assignment v extending v0 that satisﬁes the max-min gradient

averaging property w.r.t. (G, v0). Let

α = max

grad[v](x, y) ≥ 0

(x,y)∈EG
x∈V \T (v0)

be the maximum edge gradient, and consider any edge (x0, x1) ∈ EG such that grad[v](x1, x0) = α, with x1 ∈
V \ T (v0). If α = 0, grad[v] is identically zero, and is trivially the lex-minimal gradient assignment. Thus, both v and
lexG[v0] are constant on each connected component. Since (G, v0) is well-posed, there is at least one terminal in each
component, and hence v and lexG[v0] must be identical.

Now assume α > 0. By the max-min gradient averaging property, ∃x2 ∈ VG such that (x1, x2) ∈ EG and

grad[v](x1, x2) =

min
y:(x1,y)∈EG

grad[v](x1, y) = − max

grad[v](x1, y)

y:(x1,y)∈EG

≤ −grad[v](x1, x0) = −α.

Thus, grad[v](x2, x1) ≥ α. Since α is the maximum edge gradient, we must have grad[v](x2, x1) = α. More-
over, v(x2) > v(x1) > v(x0), thus x2 6= x0. We can inductively apply this argument at x2 until we hit a ter-
minal. Similarly, if x0 /∈ T (v0) we can extend the path in the other direction. Consequently, we obtain a path
P = (xj , . . . , x2, x1, x0, x−1, . . . , xk) with all vertices as distinct, such that xj , xk ∈ T (v0), and xi ∈ V \ T (v0)
for all i ∈ [j + 1, k − 1]. Moreover, grad[v](xi, xi−1) = α for all j < i ≤ k. Thus, P is a free terminal path with
∇P [v0] = α.

Moreover, since v is a voltage assignment extending v0 with

∞ = α, using Lemma 3.5, we know that
every terminal path P ′ in (G, v0) must satisfy ∇P ′(v0) ≤ α. Thus, P is a steepest ﬁxable path in (G, v0). Thus,
letting v1 = ﬁx[v0, P ], using Corollary 3.4, we obtain that lexG[v1] = lexG[v0]. Moreover, since α = ∇P [v0] =
grad[v](xi, xi−1) for all i ∈ (j, k], we get v1(xi) = v(xi) for all i ∈ (j, k). Thus, v extends v1.

grad[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We can iterate this argument for r iterations until T (vr) = VG, giving v = vr and vr = lexG[vr] = lexG[v0].
(Since we are ﬁxing at least one terminal at each iteration, this procedure terminates). Thus, we get v = lexG[v0]. ✷

B Description of the Algorithms

Algorithm 2: MODDIJKSTRA(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs a complete
voltage assignment v for G, and an array parent : V → V ∪ {null}.

Add x to a ﬁbonacci heap, with key(x) = +∞.
ﬁnished(x) ← false

Decrease key(x) to v0(x).
parent(x) ← null.

1. for x ∈ VG,
2.
3.
4. for x ∈ T (v0)
5.
6.
7. while heap is not empty
8.
9.
10.
11.
12.
13.
14.
15. return (v, parent)

x ← pop element with minimum key from heap
v(x) ← key(x). ﬁnished(x) ← true .
for y : (x, y) ∈ EG

if ﬁnished(y) = false

if key(y) > v(x) + α · ℓ(x, y)

Decrease key(y) to v(x) + α · ℓ(x, y).
parent(y) ← x.

Theorem B.1 For a well-posed instance (G, V0) and a gradient value α ≥ 0, let (v, parent) ← MODDIJKSTRA(G, v0, α).
Then, v is a complete voltage assignment such that, ∀x ∈ VG, v(x) = mint∈T (v0){v0(t) + αdist(x, t)}. Moreover, the
pointer array parent satisﬁes ∀x /∈ T (v0), parent(x) 6= null and v(x) = v(parent(x)) + α · ℓ(x, parent(x)).

18

Algorithm 3: Algorithm COMPVLOW(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs
vLow, a complete voltage assignment for G, and an array LParent : V → V ∪ {null}.

1. (vLow, LParent) ← MODDIJKSTRA(G, v0, α)
2. return (vLow, LParent)

Algorithm 4: Algorithm COMPVHIGH(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs
vHigh, a complete voltage assignment for G, and an array HParent : V → V ∪ {null}.

if x ∈ T (v0) then v1(x) ← −v0(x) else v1(x) ← v1(x).

1. for x ∈ VG
2.
3. (temp, HParent) ← MODDIJKSTRA(G, v1, α)
4. for x ∈ VG : vHigh(x) ← −temp(x)
5. return (vHigh, HParent)

Corollary B.2 For a well-posed instance (G, V0) and a gradient value α ≥ 0, let (vLow[α], LParent) ← COMPVLOW(G, v0, α)
and (vHigh[α], HParent) ← COMPVHIGH(G, v0, α). Then, vLow[α], vHigh[α] are complete voltage assignments for
G such that, ∀x ∈ VG,

vLow[α](x) = min

{v0(t) + α · dist(x, t)}

t∈T (v0)

vHigh[α](x) = max
t∈T (v0)

{v0(t) − α · dist(t, x)}.

Moreover, the pointer arrays LParent, HParent satisfy ∀x /∈ T (v0), LParent(x), HParent(x) 6= null and

vLow[α](x) = vLow[α](LParent(x)) + α · ℓ(x, LParent(x)),
vHigh[α](x) = vHigh[α](HParent(x)) − α · ℓ(x, HParent(x)).

Algorithm 5: Algorithm COMPINFMIN(G, v0): Given a well-posed instance (G, v0), outputs a complete voltage assignment
v for G, extending v0 that minimizes (cid:13)

(cid:13)grad[v](cid:13)

(cid:13)∞.

1. α ← max{|grad[v0](e)| | e ∈ EG ∩ (T (v0) × T (v0))}.
2. EG ← EG \ (T (v0) × T (v0))
3. P ←STEEPESTPATH(G, v0).
4. α ← max{α, ∇P (v0)}
5. (vLow, LParent) ← COMPVLOW(G, v0, α)
6. (vHigh, HParent) ← COMPVHIGH(G, v0, α)
7. for x ∈ VG
8.
9.
10.
11. return v

then v(x) ← v0(x)
else v(x) ← 1

2 · (vLow(x) + vHigh(x)).

if x ∈ T (v0)

1. (vLow, LParent) ← COMPVLOW(G, v0, α)
2. (vHigh, HParent) ← COMPVHIGH(G, v0, α)
3. VG′ ← {x ∈ VG | vHigh(x) > vLow(x) }
4. EG′ ← {(x, y) ∈ EG | x, y ∈ VG′ }.

19

Algorithm 6: Algorithm COMPHIGHPRESSGRAPH(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0,
outputs a minimal induced subgraph G′ of G where every vertex has pressure[v0](·) > α.

5. G′ ← (V ′, E′, ℓ)
6. return G′

Proof of Lemma 4.3:

is equivalent to

vHigh[α](x) > vLow[α](x)

max
t∈T (v0)

{v0(t) − α · dist(t, x)} > min

{v0(t) + α · dist(x, t)},

t∈T (v0)

which implies that there exists terminals s, t ∈ T (v0) such that

thus,

Hence,

v0(t) − α · dist(t, x) > v0(s) + α · dist(x, s)

pressure[v0](x) ≥

v0(t) − v0(s)
dist(t, x) + dist(x, s)

> α.

v0(t) − v0(s)
dist(t, x) + dist(x, s)

= pressure[v0](x) > α.

v0(t) − α · dist(t, x) > v0(s) + α · dist(x, s)

So the inequality on vHigh and vLow implies that pressure is strictly greater than α. On the other hand, if pressure[v0](x) >
α, there exists terminals s, t ∈ T (v0) such that

which implies vHigh[α](x) > vLow[α](x).

✷

Algorithm 7: Algorithm STEEPESTPATH(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs a steepest
free terminal path P in (G, v0).

P ← VERTEXSTEEPESTPATH(G, v0, xi)

1. Sample uniformly random e ∈ EG. Let e = (x1, x2).
2. Sample uniformly random x3 ∈ VG.
3. for i = 1 to 3
4.
5. Let j ∈ arg maxj∈{1,2,3} ∇Pj (v0)
6. G′ ← COMPHIGHPRESSGRAPH(G, v0, ∇Pj (v0))
7. if EG′ = ∅,
8.
9.

then return Pj
else return STEEPESTPATH(G′, v0|VG′ )

1. while T (v0) 6= VG
2.
3.
4.
5. return v0

EG ← EG \ (T (v0) × T (v0))
P ← STEEPESTPATH(G, v0)
v0 ← ﬁx[v0, P ]

Algorithm 8: Algorithm COMPLEXMIN(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs lexG[v0].

Algorithm 9: Algorithm VERTEXSTEEPESTPATH(G,v0, x): Given a well-posed instance (G, v0), and a vertex x ∈ VG,
outputs a steepest terminal path in (G, v0) through x.

1. Using Dijkstra’s algorithm, compute dist(x, t) for all t ∈ T (v0)

20

y ← arg maxy∈T (v0)
if v0(x) ≥ v0(y)

|v0(x)−v0(y)|
dist(x,y)

then return a shortest path from x to y
else return a shortest path from y to x

2. if x ∈ T (v0)
3.
4.
5.
6.
7. else
8.
9.
10.
11.

for t /∈ T (v0), d(t) ← dist(x, t)
(t1, t2) ← STARSTEEPESTPATH(T (v0), v0|T (v0), d)
Let P1 be a shortest path from t1 to x. Let P2 be a shortest path from x to t2.
P ← (P1, P2). return P.

Algorithm 10: STARSTEEPESTPATH(T, v, d): Returns the steepest path in a star graph, with a single non-terminal connected
to terminals in T, with lengths given by d, and voltages given by v.

|v(t1)−v(t)|
d(t1)+d(t)

1. Sample t1 uniformly and randomly from T
2. Compute t2 ∈ arg maxt∈T
3. α ← |v(t2)−v(t1)|
d(t1)+d(t2)
4. Compute vlow ← mint∈T (v(t) + α · d(t))
5. Tlow ← {t ∈ T | v(t) > vlow + α · d(t)}
6. Compute vhigh ← maxt∈T (v(t) − α · d(t))
7. Thigh ← {t ∈ T | v(t) < vhigh − α · d(t)}
8. T ′ ← Tlow ∪ Thigh.
9. if T ′ = ∅
10.
11.

then if v(t1) ≥ v(t2) then return (t1, t2) else return (t2, t1)
else return STARSTEEPESTPATH(T ′, v|T ′, dT ′ )

B.1 Faster Lex-minimization

Algorithm 11: Algorithm COMPFASTLEXMIN(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs
lexG[v0].

1. while T (v0) 6= VG
2.
3. return v0

v0 ← FIXPATHSABOVEPRESS(G, v0, 0)

Algorithm 12: Algorithm FIXPATHSABOVEPRESS(G, v0, α): Given a well-posed instance (G, v0), with T (v0) 6= VG, and
a gradient value α, iteratively ﬁxes all paths with gradient > α.

EG ← EG \ (T (v0) × T (v0))
Sample uniformly random e ∈ EG. Let e = (x1, x2).
Sample uniformly random x3 ∈ VG.
for i = 1 to 3

Pi ← VERTEXSTEEPESTPATH(G, v0, xi)

Let j ∈ arg maxj∈{1,2,3} ∇Pj(v0)
G′ ← COMPHIGHPRESSGRAPH(G, v0, ∇Pj (v0))
if EG′ = ∅,

1. while T (v0) 6= VG
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.

then v0 ← ﬁx[v0, P ]
else Let G′

for i = 1, . . . , r

i, i = 1, . . . , r be the connected components of G′.

21

vi ← FIXPATHSABOVEPRESS(G′
for x ∈ VG′

i, set v0(x) ← vi(x)

i, v0|VG′

i

, ∇Pj (v0))

if α > 0 then G ←COMPHIGHPRESSGRAPH(G, v0, α)

13.
14.
15.
16. return v0

C Experiments on WebSpam: Testing More Algorithms

For completeness, in this appendix we show how a number of algorithms perform on the web spam experiment of
Section 6. We consider the following algorithms:

• RANDWALK along in-links. For a detailed description see Zhou et al. (2007). This algorithm essentially per-
forms a Personalized PageRank random walk from each vertex x and computes a spam-value for the vertex x by
taking a weighted average of the labels of the vertices where the random walk from x terminates. Also shown in
Section 6.

• DIRECTEDLEX, with edges in the opposite directions of links. This has the effect that a link to a spam host is

evidence of spam, and a link from a normal host is evidence of normality. Also shown in Section 6.

• RANDWALK along out-links.

• DIRECTEDLEX, with edges in the directions of links. This has the effect that a link from to a spam host is

evidence of spam, and a link to a normal host is evidence of normality.

• UNDIRECTEDLEX: Lex-minimization with links treated as undirected edges.

• LAPLACIAN: l2-regression with links treated as undirected edges.

• DIRECTED 1-NEAREST NEIGHBOR: Uses shortest distance along paths following out-links. Spam-ratio is
deﬁned distance from normal hosts, divided by distance to spam hosts. Sites are ﬂagged as spam when spam-
ratio exceeds some threshold. We also tried following paths along in-links instead, but that gave much worse
results.

We use the experimental setup described in Section 6. Results are shown in Figure 4. The alternative convention
for DIRECTEDLEX orients edges in the directions of links. This takes a link from a spam host to be evidence of
spam, and a link to a normal host to be evidence of normality. This approach performs signiﬁcantly worse than our
preferred convention, as one would intuitively expect. UNDIRECTEDLEX and LAPLACIAN approaches also perform
signiﬁcantly worse. DIRECTED 1-NEAREST NEIGHBOR performs poorly, demonstrating that DIRECTEDLEX is very
different from that approach. As observed by Zhou et al. (2007), sampling based on a random walk following out-links
performs worse than following in-links. Up to 60 % recall, DIRECTEDLEX performs best, both in the regime of 5 %
labels for training and in the regime of 20 % labels for training.

22

5 % labels for training

RandWalk along in-links
DirectedLex
RandWalk along out-links
DirectedLex, opposite link direction
UndirectedLex
Laplacian
Directed 1NN

0
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Recall

20 % labels for training

RandWalk along in-links
DirectedLex
RandWalk along out-links
DirectedLex, opposite link direction
UndirectedLex
Laplacian
Directed 1NN

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

i

i

n
o
s
c
e
r
P

i

i

n
o
s
c
e
r
P

0
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Recall

Figure 4: Recall and precision in the WebSpam classiﬁcation experiment. Each data point shown was computed as an average
over 100 runs. The largest standard deviation of the mean precision across the plotted recall values was less than 1.5 %. The
algorithm of Zhou et al. (2007) appears as RANDWALK (along in-links). We also show RANDWALK along out-links. Our directed
lex-minimization algorithm appears as DIRECTEDLEX. We also show DIRECTEDLEX with link directions reversed, along with
UNDIRECTEDLEX and LAPLACIAN.

D l0-Vertex Regularization Proofs

In this appendix, we prove Theorem 7.1 and Theorem 7.2. For the purposes of proving the second theorem, we intro-
duce an alternative version of problem (3). The optimization problem here requires us to minimize l0-regularization

23

budget required to obtain an inf-minimizer with gradient below a given threshold:

min
v∈IRn
subject to

(cid:13)
(cid:13)

v(T ) − v0(T )

0

gradG[v]

(cid:13)
∞ ≤ α.
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We will also need the following graph construction.

Deﬁnition D.1 The α-pressure terminal graph of a partially-labeled graph (G, v0) is a directed unweighted graph
Gα = (T (v0),

E if and only if there is a terminal path P from s to t in G with

E) such that (s, t) ∈

b

b

∇P (v0) > α.

Note that the α-pressure terminal graph has O(n) vertices but may be dense, even when G is not.

Algorithm 13: Algorithm TERM-PRESSURE: Given a well-posed instance (G, v0) and α ≥ 0, outputs α pressure terminal
graph Gα.
Initialize Gα with vertex set Vα = T (v0) and edge set
for each terminal s ∈ T (v0)

E = ∅.

1. Compute the distances to every other terminal t by running Dijktra’s algorithm, allowing shortest paths

b

2. Use the resulting distances to check for every other terminal t if there is a terminal path P from s to t with

that run through other terminals.

∇P (v0) > α. If there is, add edge (s, t) to

E.

Lemma D.2 The α-pressure terminal graph of a voltage problem (G, v0) can be computed in O((m + n log n)n) time
using algorithm TERM-PRESSURE (Algorithm 13).

b

Proof: The correctness of the algorithm follows from the fact that Dijkstra’s algorithm will identify all shortest
distances between the terminals, and the pressure check will ensure that terminal pairs (s, t) are added to
E if and
only if they are the endpoints of a terminal path P with ∇P (v0) > α. The running time is dominated by performing
Dijkstra’s algorithm once for each terminal. A single run of Dijkstra’s algorithm takes O(m + n log n) time, and this
✷
is performed at most n times, for a total running time of O((m + n log n)n).

b

We make three observations that will turn out to be crucial for proving Theorems 7.1 and 7.2.

Observation D.3 Gα is a subgraph of Gβ for α ≥ β.

Proof: Suppose edge (s, t) appears in Gα, then for some path P

∇P (v0) > α ≥ β,

so the edge also appears in Gβ.

Observation D.4 Gα is transitively closed.

Proof: Suppose edges (s, t) and (t, r) appear in Gα. Let P(s,t), P(t,r), P(s,r) be the respective shortest paths in G
between these terminal pairs. Then

∇P(s,r)(v0) =

v0(s) − v0(r)
ℓ(P(s,r))

≥

v0(s) − v0(r)
ℓ(P(s,t)) + ℓ(P(t,r))

=

v0(s) − v0(t) + v0(t) − v0(r)
ℓ(P(s,t)) + ℓ(P(t,r))

≥ min

v0(s) − v0(t)
ℓ(P(s,t))

,

 

v0(t) − v0(r)

ℓ(P(t,r)) !

> α.

So edge (s, r) also appears in Gα. This is sufﬁcient for Gα to be transitively closed.

24

(6)

✷

(7)

✷

Observation D.5 Gα is a directed acyclic graph.

Proof: Suppose for a contradiction that a directed cycle appears in Gα. Let s and t be two vertices in this cycle. Let
P(s,t) and P(t,s) be the respective shortest paths in G between these terminal pairs. Because Gα is transitively closed,
both edges (s, t) and (t, s) must appear in Gα. But (s, t) ∈

E implies

and similarly (t, s) ∈

E implies

b
This is a contradiction.

v0(s) − v0(t) > αℓ(P(s,t)) > 0,

b

v0(t) − v0(s) > αℓ(P(t,s)) > 0.

✷

The usefulness of the α-pressure terminal graph is captured in the following lemma. We deﬁne a vertex cover of a
directed graph to be a vertex set that constitutes a vertex cover in the same graph with all edges taken to be undirected.

Lemma D.6 Given a partially-labeled graph (G, v0) and a set U ⊆ V , there exists a voltage assignment v ∈ IRn that
satisﬁes

if and only if U is a vertex cover in the α-pressure terminal graph Gα of (G, v0).
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:8)

(cid:9)

t ∈ T (v0) : v(t) 6= v0(t)

⊆ U and

gradG[v]

∞ ≤ α,

Proof: We ﬁrst show the “only if” direction. Suppose for a contradiction that there exists a voltage assignment v for
which
∞ ≤ α, but U is not a vertex cover in Gα. Let (s, t) be an edge Gα which is not covered by U . The
presence of this edge in Gα implies that there exists a terminal path P from s to t in G for which

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∇P (v0) > α.

But, by Lemma 3.5 this means there is no assignment v for G which agrees with v0 on s and t and has
α. This contradicts our assumption.

∞ ≤
(cid:13)
Now we show the “if” direction. Consider an arbitrary vertex cover U of Gα. Suppose for a contradiction that
(cid:13)
⊆ U .

t ∈ T (v0) : v(t) 6= v0(t)

gradG[v]

(cid:13)
(cid:13)

gradG[v]

there does not exist a voltage assignment v for G with
Deﬁne a partial voltage assignment vU given by

∞ ≤ α and

(cid:8)

(cid:9)

vU (t) =

v0(t)
∗

(

(cid:13)
(cid:13)

(cid:13)
(cid:13)
if t ∈ T (v0) \ U
o.w.

∞ ≤ α. By
The preceding statement is equivalent to saying that there is no v that extends vU and has
Lemma 3.5, this means there is terminal path between s, t ∈ T (vU ) with gradient strictly larger than α. But this
means an edge (s, t) is present in Gα and is not covered. This contradicts our assumption that U is a vertex cover. ✷

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We are now ready to prove Theorem 7.2.

∞

(cid:13)
(cid:13)

Proof of Theorem 7.2: We describe and prove the algorithm OUTLIER. The algorithm will reduce problem (3)
to problem (6): Suppose v∗ is an optimal assignment for problem (3).
It achieves a maximum gradient α∗ =
gradG[v∗]
. Using Dijkstra’s algorithm we compute the pairwise shortest distances between all terminals in G.
From these distances and the terminal voltages, we compute the gradient on the shortest path between each terminal
(cid:13)
pair. By Lemma 3.5, α∗ must equal one of these gradients. So we can solve problem (3) by iterating over the set of
(cid:13)
gradients between terminals and solving problem (6) for each of these O(n2) gradients. Among the assignments with
v(T ) − v0(T )

0 ≤ k, we then pick the solution that minimizes
(cid:13)
(cid:13)

In fact, we can do better. By Observation D.3, Gα is a subgraph of Gβ for α ≥ β. This means a vertex cover
(cid:13)
of Gα is also a vertex cover of Gβ, and hence the minimum vertex cover for Gβ is at least as large as the minimum
(cid:13)
vertex cover for Gα. This means we can do a binary search on the set of O(n2) terminal gradients to ﬁnd the minimum
gradient for which there exists an assignment with
0 ≤ k. This way, we only make O(log n) calls to
v(T ) − v0(T )
problem (6), in order to solve problem (3).
(cid:13)
(cid:13)

We use the following algorithm to solve problem (6).

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∞

.

25

1. Compute the α-pressure terminal graph Gα of G using the algorithm TERM-PRESSURE.
2. Compute a minimum vertex cover U of Gα using the algorithm KONIG-COVER from Theorem 7.3.
3. Deﬁne a partial voltage assignment vU given by

vU (t) =

v0(t)
∗

(

if t ∈ T (v0) \ U,
otherwise.

4. Using Algorithm 5, compute voltages v that extend vU and output v.

From Lemma D.2, it follows that step 1 computes the α-pressure terminal graph in polynomial time. From The-
orem 7.3 it follows that step 2 computes the a minimum vertex cover of the α-pressure terminal graph in polynomial
time, because our observations D.4 and D.5 establish that the graph is a TC-DAG. From Lemma D.6 and Theorem 4.6,
it follows that the output voltages solve program (6).

✷

To prove Theorem 7.1, we use the standard greedy approximation algorithm for MIN-VC (Vazirani (2001)).

Theorem D.7 2-Approximation Algorithm for Vertex Cover. The following algorithm gives a 2-approximation to
the Minimum Vertex Cover problem on a graph G = (V, E).

0. Initialize U = ∅.
1. Pick an edge (u, v) ∈ E that is not covered by U .
2. Add u and v to the set U .
3. Repeat from step 1 if there are still edges not covered by U .
4. Output U .

We are now in a position to prove Theorem 7.1

Proof of Theorem 7.1: Given an arbitrary k and a partially-labeled graph (G, v0), let α∗ be the optimum value
of program (3). Observe that by Lemma D.6, this implies that Gα∗ has a vertex cover of size k. Given the partial
assignment v0, for every vertex set U , we deﬁne

vU (t) =

v0(t)
∗

(

if t ∈ T (v0) \ U
o.w.

We claim the following algorithm APPROX-OUTLIER outputs a voltage assignment v with

gradG[v]

∞ ≤ α∗

(cid:13)
(cid:13)

(cid:13)
(cid:13)

and

v(T ) − v0(T )

(cid:13)
(cid:13)

Algorithm APPROX-OUTLIER:

0 ≤ 2k.
(cid:13)
(cid:13)

0. Initialize U = ∅.
1. Using the algorithm STEEPESTPATH (Algorithm 7), ﬁnd a steepest terminal path in G w.r.t. vU . Denote
this path P and let s and t be its terminal endpoints. If there is no terminal path with positive gradient, skip
to step 4.

2. Add s and t to the set U .
3. If |U | ≤ 2k − 2 then repeat from step 1.
4. Using the algorithm COMPINFMIN (Algorithm 5), compute voltages v that extend vU and output v.

From the stopping conditions, it is clear that |U | ≤ 2k. If in step 1 we ever ﬁnd that no terminal paths have positive
∞ = 0 ≤ α∗, by Lemma 3.5. Similarly if we ﬁnd a steepest
gradient then our v that extends vU will have
(cid:13)
(cid:13)

gradG[v]

(cid:13)
(cid:13)

26

gradG[v]

∞ ≤ α∗.

∞ ≤ α∗.
path with gradient less than α∗ w.r.t. vU , then for this U there exists v that extends vU and has
This will continue to hold when if we add vertices to U . Therefore, for the ﬁnal U , there will exist an v that extends
vU and has

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

If we never ﬁnd a steepest terminal path P with ∇P (v0) ≤ α∗, then each steepest path we ﬁnd corresponds to an
edge in Gα∗ that is not yet covered by U and our algorithm in fact implements the greedy approximation algorithm
for vertex cover described in Theorem D.7. This implies that the ﬁnal U is a vertex cover of Gα∗ of size at most 2k.
∞ ≤ α∗. This
By Lemma D.6, this implies that there exists a voltage assignment u extending vU that has
implies by Theorem 4.6 that the v we output has
(cid:13)
(cid:13)
In all cases, the v we output extends vU , so

∞ ≤ α∗.

gradG[u]

(cid:13)
(cid:13)

✷

gradG[v]
v(T ) − v0(T )
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

0 ≤ |U | ≤ 2k.
(cid:13)
(cid:13)

E Proof of Hardness of l0 regularization for l2

We will prove Theorem 7.4, by a reduction from minimum bisection. To this end, let G = (V, E) be any graph. We
will reduce the minimum bisection problem on G to our regularization problem. Let n = |V |. The graph on which we
will perform regularization will have vertex set

V ∪

V ,

V is a set of n vertices that are in 1-to-1 correspondence with V . We assume that every edge in G has weight 1.
V to the corresponding vertex in V by an edge of weight B, for some large B to be
V to each other by edges of weight B3. So, we have a complete
V to V , and the original graph G on V .

where
We now connect every vertex in
determined later. We also connect all of the vertices in
graph of weight B3 edges on
b
The input potential function will be

V , a matching of weight B edges connecting

b

b

b

v(a) =

b
0 for a ∈
1 for a ∈ V .
b

(

V , and

b

Now set k = n/2. We claim that we will be able to determine the value of the minimum bisection from the solution
to the regularization problem.

If S is the set of vertices on which v and w differ, then we know that the w is harmonic on S: for every a ∈ S,

w(a) is the weighted average of the values at its neighbors. In the following, we exploit the fact that |S| ≤ n/2.

Claim E.1 For every a ∈ S ∩

V , w(a) ≤ 2/nB2.

Proof: Let a be the vertex in S ∩
w-value equal to 0 by edges of weight B3. On the other hand, a has only one neighbor that is not in
w-value at most 1, and it is connected to that vertex by an edge of weight B. Call that vertex c. We have

V that maximizes w(a). So, a is connected to at least n/2 neighbors in

V with
V , that vertex has

b

b

b

((n − 1)B3 + B)w(a) = Bw(c) +

B3w(b)

b

b
V ,b6=a
Xb∈

= Bw(c) +

B3w(b) +

B3w(b)

b
V ∩S,b6=a
Xb∈

B3w(a)

≤ B +

b
V ∩S,b6=a
Xb∈
≤ B + (n/2 − 1)B3w(a).

b
V −S
Xb∈

Subtracting (n/2 − 1)B3w(a) from both sides gives

((n/2)B3 + B)w(a) ≤ B,

which implies the claim.

Claim E.2 For a ∈ S ∩ V , w(a) ≤ n/B.

27

✷

V . Let’s call that neighbor c. We know that w(c) ≤ 2/B2n. On the
Proof: Vertex a has exactly one neighbor in
other hand, vertex a has fewer than n − 1 neighbors in V , and each of these have w-value at most 1. Let da denote the
degree of a in G. Then,

b

So,

Let

and

bisection.

and at most

(B + da)w(a) ≤ da + B

2
B2n

.

w(a) ≤

da + 2/Bn
da + B
n + (2/Bn)
B + n

≤

≤ n/B.

|S| = k = n/2.

T = S ∩ V,

t = |T | .

(n − t)B − 4/B
b

(n − t)B + tn2/B.

We now estimate the value of the regularized objective function. To this end, we assume that

We will prove that S ⊂ V and so S = T and t = n/2.

Let δ denote the number of edges on the boundary of T in V . Once we know that t = n/2, δ is the size of a

Claim E.3 The contribution of the edges between V and

V to the objective function is at least

Proof: For the lower bound, we just count the edges between vertices in V \ T and
edges, and each of them has weight B. The endpoint in V \ T has w-value 1, and the endpoint in
most 2/nB2. So, the contribution of these edges is at least

V . There are n − t of these
V has w-value at

b

(n − t)B(1 − 2/nB2)2 ≥ (n − t)B(1 − 4/nB2) ≥ (n − t)B − 4/B.

b

For the upper bound, we observe that the difference in w-values across each of these n − t edges is at most 1, so their
total contribution is at most

Since for every vertex a ∈ T , w(a) ≤ n/B, and also every vertex b ∈
edges between T and

V is at most

t(n/B)2B = tn2/B.

b

b

V , w(b) ≤ 2/nB2, the contribution due to

We will see that this is the dominant term in the objective function. The next-most important term comes from the

edges in G.

(n − t)B.

28

✷

✷

Claim E.4 The contribution of the edges in G to the objective function is at least

and at most

δ(1 − 2n/B)

δ + (t2/2)(n/B)2

δ(1 − 2n/B) and δ.

(t2/2)(n/B)2.

Proof: Let (a, b) ∈ E. If neither a nor b is in T , then w(a) = w(b) = 1, and so this edge has no contribution. If
a ∈ T but b 6∈ T , then the difference in w-values on them is between (1 − n/B) and 1. So, the contribution of such
edges to the objective function is between

Finally, if a and b are in T , then the difference in w-values on them is at most n/B, and so the contribution of all such
edges to the objective function is at most

Claim E.5 The edges between pairs of vertices in

V contribute at most 2/B to the objective function.

Proof: As 0 ≤ w(a) ≤ 2/B2n for every a ∈

V , every edge between two vertices in

V can contribute at most

b

As there are fewer than n2/2 such edges, their total contribution to the objective function is at most

B3(2/B2n)2 = 4/Bn2.
b

b

(n2/2)(4/Bn2) = 2/B.

Lemma E.6 If n ≥ 4 and B = 2n3, the value of the objective function is at least

and at most

(n − t)B + δ − 1/2

(n − t)B + δ + 1/3.

Proof: Summing the contributions in the preceding three claims, we see that the value of the objective function is at
least

(n − t)B − 4/B + δ(1 − 2n/B) ≥ (n − t)B + δ − 4/B − 2nδ/B

≥ (n − t)B + δ − n3/B
≥ (n − t)B + δ − 1/2,

as δ ≤ (n/2)2.

Similarly, the objective function is at most

(n − t)B + tn2/B + δ + (t2/2)(n/B)2 + 2/B ≤ (n − t)B + n3/2B + δ + n4/8B2 + 2/B
≤ (n − t)B + n3/2B + δ + 1/32n2 + 1/n3
≤ (n − t)B + δ + 1/3.

Claim E.7 If n ≥ 2 and B = 2n3, then S ⊂ V .

Proof: The objective function is minimized by making t as large as possible, so t = n/2 and S ⊂ V .

29

✷

✷

✷

✷

Theorem E.8 The value of the objective function reveals the value of the minimum bisection in G.

Proof: The value of the objective function will be between

and

(n/2)B + δ − 1/2

(n/2)B + δ + 1/3.

So, the objective function will be smallest when δ is as small as possible.

✷

Theorem E.8 immediately implies Theorem 7.4.

30

5
1
0
2
 
n
u
J
 
0
3
 
 
]

G
L
.
s
c
[
 
 
2
v
0
9
2
0
0
.
5
0
5
1
:
v
i
X
r
a

Algorithms for Lipschitz Learning on Graphs ∗†

Rasmus Kyng
Yale University
rasmus.kyng@yale.edu

Anup Rao
Yale University
anup.rao@yale.edu

Sushant Sachdeva
Yale University
sachdeva@cs.yale.edu

Daniel A. Spielman
Yale University
spielman@cs.yale.edu

July 1, 2015

Abstract

We develop fast algorithms for solving regression problems on graphs where one is given the value of a function
at some vertices, and must ﬁnd its smoothest possible extension to all vertices. The extension we compute is the
absolutely minimal Lipschitz extension, and is the limit for large p of p-Laplacian regularization. We present an
algorithm that computes a minimal Lipschitz extension in expected linear time, and an algorithm that computes
an absolutely minimal Lipschitz extension in expected time eO(mn). The latter algorithm has variants that seem
to run much faster in practice. These extensions are particularly amenable to regularization: we can perform l0-
regularization on the given values in polynomial time and l1-regularization on the initial function values and on graph
edge weights in time eO(m3/2).

Our deﬁnitions and algorithms naturally extend to directed graphs.

1 Introduction

We consider a problem in which we are given a weighted undirected graph G = (V, E, ℓ) and values v0 : T → R
on a subset T of its vertices. We view the weights ℓ as indicating the lengths of edges, with shorter length indicating
greater similarity. Our goal it to assign values to every vertex v ∈ V \T so that the values assigned are as smooth as
possible across edges. A minimal Lipschitz extension of v0 is a vector v that minimizes

max
(x,y)∈E

(ℓ(x, y))−1

v(x) − v(y)

,

(cid:12)
(cid:12)

(cid:12)
(cid:12)

subject to v(x) = v0(x) for all x ∈ T . We call such a vector an inf-minimizer. Inf-minimizers are not unique. So,
among inf-minimizers we seek vectors that minimize the second-largest absolute value of ℓ(x, y)−1
v(x) − v(y)
across edges, and then the third-largest given that, and so on. We call such a vector v a lex-minimizer. It is also known
(cid:12)
as an absolutely minimal Lipschitz extension of v0.
(cid:12)
These are the limit of the solution to p-Laplacian minimization problems for large p, namely the vectors that solve

(cid:12)
(cid:12)

(1)

(2)

min
v∈Rn

v|T =v0|T X(x,y)∈E

(ℓ(x, y))−p|v(x) − v(y)|p.

The use of p = 2 was suggested in the foundational paper of Zhu et al. (2003), and is particularly nice because it can
be obtained by solving a system of linear equations in a symmetric diagonally dominant matrix, which can be done

∗This research was partially supported by AFOSR Award FA9550-12-1-0175, NSF grant CCF-1111257, a Simons Investigator Award to Daniel

Spielman, and a MacArthur Fellowship.

†Code used in this work is available at https://github.com/danspielman/YINSlex

1

very quickly (Cohen et al. (2014)). The use of larger values of p has been discussed by Alamgir and Luxburg (2011),
and by Bridle and Zhu (2013), but it is much more complicated to compute. The fastest algorithms we know for this
problem require convex programming, and then require very high accuracy to obtain the values at most vertices. By
taking the limit as p goes to inﬁnity, we recover the lex-minimizer, which we will show can be computed quickly.

The lex-minimization problem has a remarkable amount of structure. For example, in uniformly weighted graphs
the value of the lex-minimizer at every vertex not in T is equal to the average of the minimum and maximum of the
values at its neighbors. This is analogous to the property of the 2-Laplacian minimizer that the value at every vertex
not in T equals the average of the values at its neighbors.

1.1 Contributions

We ﬁrst present several important structural properties of lex-minimizers in Section 3.2. As we shall point out, some
of these were known from previous work, sometimes in restricted settings. We state them generally and prove them
for completeness. We also prove that the lex-minimizer is as stable as possible under perturbations of v0 (Section 3.1).
The structure of the lex-minimization problem has led us to develop elegant algorithms for its solution. Both the
algorithms and their analyses could be taught to undergraduates. We believe that these algorithms could be used in
place of 2-Laplacian minimization in many applications.

We present algorithms for the following problems. Throughout, m = |E| and n = |V |.

Inf-minimization: An algorithm that runs in expected time O(m + n log n) (Section 4.3).

Lex-minimization: An algorithm that runs in expected time O(n(m + n log n)) (Section 4), along with a variant that

runs quickly in practice (Section 4.4).

l1-regularization of edge lengths for inf-minimization: The problem of minimizing (1) given a limited budget with
O(m3/2)
which one can increase edge lengths is a linear programming problem. We show how to solve it in time
with an interior point method by using fast Laplacian solvers (Section 8). The same algorithm can accommodate
l1-regularization of the values given in v0.

e

l0-regularization of vertex values for inf-minimization: We give a polynomial time algorithm for l0-regularization
of the values at vertices. That is, we minimize (1) given a budget of a number of vertices that can be proclaimed
outliers and removed from T (Section 7.1). We solve this problem by reducing it to the problem of computing
minimum vertex covers on transitively closed directed acyclic graphs, a special case of minimum vertex cover
that can be solved in polynomial time.

After any regularization for inf-minimization, we suggest computing the lex-minimizer. We ﬁnd the result for l0-
regularization of vertex values to be particularly surprising, especially because we prove that the analogous problem
for 2-Laplacian minimization is NP-Hard (Section 7.2).

All of our algorithms extend naturally to directed graphs (Section 5). This is in contrast with the problem of
minimizing 2-Laplacians on directed graphs, which corresponds to computing electrical ﬂows in networks of resistors
and diodes, for which fast algorithms are not presently known.

We present a few experiments on examples demonstrating that the lex-minimizer can overcome known deﬁcien-
cies of the 2-Laplacian minimizer (Section 1.2, Figures 1,2), as well as a demonstration of the performance of the
directed analog of our algorithms on the WebSpam dataset of Castillo et al. (2006) (Section 6). In the WebSpam prob-
lem we use the link structure of a collection of web sites to ﬂag some sites as spam, given a small number of labeled
sites known to be spam or normal.

1.2 Relation to Prior Work

We ﬁrst encountered the idea of using the minimizer of the 2-Laplacian given by (2) for regression and classiﬁca-
tion on graphs in the work of Zhu et al. (2003) and Belkin et al. (2004) on semi-supervised learning. These works
transformed learning problems on sets of vectors into problems on graphs by identifying vectors with vertices and
constructing graphs with edges between nearby vectors. One shortcoming of this approach (see Nadler et al. (2009),

2

e
g
a

t
l

 

o
V
d
e
r
r
e

f

n

I

1

0.8

0.6

0.4

0.2

0

-0.2

-0.4

-0.6

-0.8

-1

-4

50 lex
50 l2
100 lex
100 l2
500 lex
500 l2
1000 lex
1000 l2

0.25

0.2

r
o
r
r
e
 
1
l
 
n
a
e
M

0.15

0.1

0.05

0
5000 

lex
2-Lap
labels

-2

0

2
Vertex position on real line

4

6

8

Figure 1: Lex vs 2-Laplacian on 1D gaussian clus-
ters.

Figure 2: kNN graphs on samples from 4D cube.

10000

20000

40000

80000

Number of Vertices

Alamgir and Luxburg (2011), Bridle and Zhu (2013)) is that if the number of vectors grows while the number of la-
beled vectors remains ﬁxed, then almost all the values of the 2-Laplacian minimizer converge to the mean of the
labels on most natural examples. For example, Nadler et al. (2009) consider sampling points from two Gaussian
distributions centered at 0 and 4 on the real line. They place edges between every pair of points (x, y) with length
exp(|x − y|2 /2σ2) for σ = 0.4, and provide only the labels v0(0) = −1 and v0(4) = 1. Figure 1 shows the values
of the 2-Laplacian minimizer in red, which are all approximately zero. In contrast, the values of the lex-minimizer in
blue, which are smoothly distributed between the labeled points, are shown.

The “manifold hypothesis” (see Chapelle et al. (2010), Ma and Fu (2011)) holds that much natural data lies near a
low-dimensional manifold and that natural functions we would like to learn on this data are smooth functions on the
manifold. Under this assumption, one should expect lex-minimizers to interpolate well. In contrast, the 2-Laplacian
minimizers degrade (dotted lines) if the number of labeled points remains ﬁxed while the total number of points grows.
In Figure 2, we demonstrate this by sampling many points uniformly from the unit cube in 4 dimensions, form their
8-nearest neighbor graph, and consider the problem of regressing the ﬁrst coordinate. We performed 8 experiments,
varying the number of labeled points in {50, 100, 500, 1000}. Each data point is the mean average l1 error over 100
experiments. The plots for root mean squared error are similar. The standard deviation of the estimations of the mean
are within one pixel, and so are not displayed. The performance of the lex-minimizer (solid lines) does not degrade as
the number of unlabeled points grows.

Analogous to our inf-minimizers, minimal Lipschitz extensions of functions in Euclidean space and over more
general metric spaces have been studied extensively in Mathematics (Kirszbraun (1934), McShane (1934), Whitney
(1934)). von Luxburg and Bousquet (2003) employ Lipschitz extensions on metric spaces for classiﬁcation and relate
these to Support Vector Machines. Their work inspired improvements in classiﬁcation and regression in metric spaces
with low doubling dimension (Gottlieb et al. (2013), Gottlieb et al. (2013b)). Theoretically fast, although not actually
practical, algorithms have been given for constructing minimal Lipschitz extensions of functions on low-dimensional
Euclidean spaces (Fefferman (2009a), Fefferman and Klartag (2009), Fefferman (2009b)). Sinop and Grady (2007)
suggest using inf-minimizers for binary classiﬁcation problems on graphs. For this special case, where all of the
given values are either 0 or 1, they present an O(m + n log n) time algorithm for computing an inf-minimizer. The
case of general given values, which we solve in this paper, is much more complicated. To compensate for the non-
uniqueness of inf-minimizers, they suggest choosing the inf-minimizer that minimizes (2) with p = 2. We believe that
the lex-minimizer is a more natural choice.

The analog of our lex-minimizer over continuous spaces is called the absolutely minimal Lipschitz extension
(AMLE). Starting with the work of Aronsson (1967), there have been several characterizations and proofs of the ex-
istence and uniqueness of the AMLE (Jensen (1993), Crandall et al. (2001), Barles and Busca (2001), Aronsson et al.
(2004)). Many of these results were later extended to general metric spaces, including graphs (Milman (1999),
Peres et al. (2011), Naor and Shefﬁeld (2010), Shefﬁeld and Smart (2010)). However, to the best of our knowledge,
fast algorithms for computing lex-minimizers on graphs were not known. For the special case of undirected, un-
weighted graphs, Lazarus et al. (1999) presented both a polynomial-time algorithm and an iterative method. Oberman

3

(2011) suggested computing the AMLE in Euclidean space by ﬁrst discretizing the problem and then solving the cor-
responding graph problem by an iterative method. However, no run-time guarantees were obtained for either iterative
method.

2 Notation and Basic Deﬁnitions

Lexicographic Ordering. Given a vector r ∈ Rm, let πr denote a permutation that sorts r in non-increasing order
by absolute value, i.e., ∀i ∈ [m − 1], |r(πr(i))| ≥ |r(πr(i + 1))|. Given two vectors r, s ∈ Rm, we write r (cid:22) s to
indicate that r is smaller than s in the lexicographic ordering on sorted absolute values, i.e.

∃j ∈ [m],

r(πr(j))

<

s(πs(j))

and ∀i ∈ [j − 1],

r(πr(i))

=

s(πs(i))

or ∀i ∈ [m],

=

r(πr(i))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.
(cid:12)
(cid:12)

s(πs(i))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

Note that it is possible that r (cid:22) s and s (cid:22) r while r 6= s. It is a total relation: for every r and s at least one of r (cid:22) s
or s (cid:22) r is true.

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Graphs and Matrices. We will work with weighted graphs. Unless explicitly stated, we will assume that they are
undirected. For a graph G, we let VG be its set of vertices, EG be its set of edges, and ℓG : EG → R+ be the
assignment of positive lengths to the edges. We let |VG| = n, and |EG| = m. We assume ℓG is symmetric, i.e.,
ℓG(x, y) = ℓG(y, x). When G is clear from the context, we drop the subscript.

A path P in G is an ordered sequence of (not necessarily distinct) vertices P = (x0, x1, . . . , xk), such that
(xi−1, xi) ∈ E for i ∈ [k]. The endpoints of P are denoted by ∂0P = x0, ∂1P = xk. The set of interior vertices
of P is deﬁned to be int(P ) = {xi : 0 < i < k}. For 0 ≤ i < j ≤ k, we use the notation P [xi : xj] to denote the
k
subpath (xi, . . . , xj). The length of P is ℓ(P ) =
i=1 ℓ(xi−1, xi).
A function v0 : V → R ∪ {∗} is called a voltage assignment (to G). A vertex x ∈ V is a terminal with
respect to v0 iff v0(x) 6= ∗. The other vertices, for which v0(x) = ∗, are non-terminals. We let T (v0) denote the
set of terminals with respect to v0. If T (v0) = V, we call v0 a complete voltage assignment (to G). We say that an
assignment v : V → R ∪ {∗} extends v0 if v(x) = v0(x) for all x such that v0(x) 6= ∗.

Given an assignment v0 : V → R ∪ {∗}, and two terminals x, y ∈ T (v0) for which (x, y) ∈ E, we deﬁne the

P

gradient on (x, y) due to v0 to be

gradG[v0](x, y) =

v0(x) − v0(y)
ℓ(x, y)

.

It may be useful to view gradG[v0](x, y) as the current in the edge (x, y) induced by voltages v0. When v0 is a
complete voltage assignment, we interpret gradG[v0] as a vector in Rm, with one entry for each edge. However, for
convenience, we deﬁne gradG[v0](x, y) = −gradG[v0](y, x). When G is clear from the context, we drop the subscript.
A graph G along with a voltage assignment v to G is called a partially-labeled graph, denoted (G, v). We say
that a partially-labeled graph (G, v0) is a well-posed instance if for every maximal connected component H of G, we
have T (v0) ∩ VH 6= ∅.

A path P in a partially-labeled graph (G, v0) is called a terminal path if both endpoints are terminals. We deﬁne

∇P (v0) to be its gradient:

∇P (v0) =

v0(∂0P ) − v0(∂1P )
ℓ(P )

.

If P contains no terminal-terminal edges (and hence, contains at least one non-terminal), it is a free terminal path.

Lex-Minimization. An instance of the LEX-MINIMIZATION problem is described by a partially-labeled graph
(G, v0). The objective is to compute a complete voltage assignment v : VG → R extending v0 that lex-minimizes
grad[v].

Deﬁnition 2.1 (Lex-minimizer) Given a partially-labeled graph (G, v0), we deﬁne lexG[v0] to be a complete voltage
assignment to V that extends v0, and such that for every other complete assignment v′ : VG → R that extends v0, we
have gradG[lexG[v0]] (cid:22) gradG[v′]. That is, lexG[v0] achieves a lexicographically-minimal gradient assignment to the
edges.

We call lexG[v0] the lex-minimizer for (G, v0). Note that if T (v0) = VG, then trivially, lexG[v0] = v0.

4

3 Basic Properties of Lex-Minimizers

Lazarus et al. (1999) established that lex-minimizers in unweighted and undirected graphs exist, are unique, and may
be computed by an elementary meta-algorithm. We state and prove these facts for undirected weighted graphs, and
defer the discussion of the directed case to Section 5. We also state for directed and weighted graphs characterizations
of lex-minimizers that were established by Peres et al. (2011), Naor and Shefﬁeld (2010) and Shefﬁeld and Smart
(2010) for unweighted graphs. These results are essential for the analyses of our algorithms. We defer most proofs to
Appendix A.

Deﬁnition 3.1 A steepest ﬁxable path in an instance (G, v0) is a free terminal path P that has the largest gradient
∇P (v0) amongst such paths.

Observe that a steepest ﬁxable path with ∇P (v0) 6= 0 must be a simple path.
Deﬁnition 3.2 Given a steepest ﬁxable path P in an instance (G, v0), we deﬁne ﬁxG[v0, P ] : VG → R ∪ {∗} to be the
voltage assignment deﬁned as follows

ﬁxG[v0, P ](x) =

v0(∂0P ) − ∇P (v0) · ℓG(P [∂0P : x]) x ∈ int(P ) \ T (v0),
v0(x)

otherwise.

(

We say that the vertices x ∈ int(P ) are ﬁxed by the operation ﬁx[v0, P ]. If we deﬁne v1 = ﬁxG[v0, P ], where
P = (x0, . . . , xr) is the steepest ﬁxable path in (G, v0), then it is easy to argue that for every i ∈ [r], we have
grad[v1](xi−1, xi) = ∇P (see Lemma A.5). The meta-algorithm META-LEX, spelled out as Algorithm 1, entails
repeatedly ﬁxing steepest ﬁxable paths. While it is possible to have multiple steepest ﬁxable paths, the result of ﬁxing
all of them does not depend on the order in which they are ﬁxed.

Theorem 3.3 Given a well-posed instance (G, v0), the meta-algorithm META-LEX, which repeatedly ﬁxes steepest
ﬁxable paths, produces the unique lex-minimizer extending v0.

Corollary 3.4 Given a well-posed instance (G, v0) such that T (v0) 6= VG, let P be a steepest ﬁxable path in (G, v0).
Then, (G, ﬁx[v0, P ]) is also a well-posed instance, and lexG[ﬁx[v0, P ]] = lexG[v0].

Since a lex-minimal element must be an inf-minimizer, we also obtain the following corollary, that can also be

proved using LP duality.

Lemma 3.5 Suppose we have a well-posed instance (G, v0). Then, there exists a complete voltage assignment v
extending v0 such that

grad[v]

∞ ≤ α, iff every terminal path P in (G, v0) satisﬁes ∇P (v0) ≤ α.
(cid:13)
(cid:13)

3.1 Stability

(cid:13)
(cid:13)

The following theorem states that lexG[v0] is monotonic with respect to v0 and it respects scaling and translation of
v0.

Theorem 3.6 Let (G, v0) be a well-posed instance with T := T (v0) as the set of terminals. Then the following
statements hold.

1. For any c, d ∈ R, v1 a partial assignment with terminals T (v1) = T and v1(t) = cv0(t) + d for all t ∈ T .

Then, lexG[v1](i) = c · lexG[v0](i) + d for all i ∈ VG.

2. v1 a partial assignment with terminals T (v1) = T. Suppose further that v1(t) ≥ v0(t) for all t ∈ T. Then,

lexG[v1](i) ≥ lexG[v0](i) for all i ∈ VG.

As a corollary, the above theorem gives a nice stability property that lex-minimal elements satisfy.

Corollary 3.7 Given well-posed instances (G, v0), (G, v1) such that T := T (v0) = T (v1), let ǫ := maxt∈T |v0(t) −
v1(t)|. Then |lexG[v0](i) − lexG[v1](i)| ≤ ǫ for all i ∈ VG.

5

3.2 Alternate Characterizations

There are at least two other seemingly disparate deﬁnitions that are equivalent to lex-minimal voltages.

lp-norm Minimizers. As mentioned in the introduction, for a well-posed instance (G, v0) the lex-minimizer is also
the limit of lp minimizers. This follows from existing results about the limit of lp-minimizers (Egger and Huotari
(1990)) in afﬁne spaces, since {grad[v] | v is complete, v extends v0} forms an afﬁne subspace of Rm. Thus, we have
the following theorem:

Theorem 3.8 (Limit of lp-minimizers, follows from Egger and Huotari (1990)) For any p ∈ (1, ∞), given a well-
posed instance (G, v0) deﬁne vp to be the unique complete voltage assignment extending v0 and minimizing
p ,
i.e.

grad[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Then, limp→∞ vp = lexG[v0].

vp = arg min
v is complete
v extends v0 (cid:13)
(cid:13)

grad[v]

p .

(cid:13)
(cid:13)

Max-Min Gradient Averaging. Consider a well-posed instance (G, v0), and a complete voltage assignment v ex-
tending v0. If G is such that ℓ(e) = 1 for all e ∈ EG, it is easy to see that lex = lexG[v0] satisﬁes the following simple
condition for all x ∈ VG \ T (v0),

lex(x) =

1
2  

max
(x,y)∈EG

lex(y) + min

lex(z)

.

(x,z)∈EG

!

This condition should be contrasted to the optimality condition for l2-regularization on these instances, which gives
for all non-terminals x, the optimal voltage v satisﬁes v(x) = 1

y:(x,y)∈EG v(y).

deg(x)

To prove the above claim, consider locally changing lex at x and observe that the gradients of edges not incident
at x remain unchanged, and at least one of edges incident at x will have a strictly larger gradient, contradicting lex-
minimality. For general graphs, this condition of local optimality can still be characterized by a simple max-min
gradient averaging property as described below.

P

Deﬁnition 3.9 (Max-Min Gradient Averaging) Given a well-posed instance (G, v0), and a complete voltage as-
signment v extending v0, we say that v satisﬁes the max-min gradient averaging property (w.r.t. (G, v0)) if for every
x ∈ VG \ T (v0), we have

grad[v](x, y) = − min

grad[v](x, y).

max
y:(x,y)∈EG

y:(x,y)∈EG

As stated in the theorem below, lexG[v0] is the unique assignment satisfying max-min gradient averaging property.
Shefﬁeld and Smart (2010) proved a variant of this statement for weighted graphs. For completeness, we present a
proof in the appendix.

Theorem 3.10 Given a well-posed instance (G, v0), lexG[v0] satisﬁes max-min gradient averaging property. More-
over, it is the unique complete voltage assignment extending v0 that satisﬁes this property w.r.t. (G, v0).

An advantage of this characterization is that it can be veriﬁed quickly. This is particularly useful for implementations
for computing the lex-minimizer.

4 Algorithms

We now sketch the ideas behind our algorithms and give precise statements of our results. A full description of all the
algorithms is included in the appendix.

We deﬁne the pressure of a vertex to be the gradient of the steepest terminal path through it:

pressure[v0](x) = max{∇P (v0) | P is a terminal path in (G, v0) and x ∈ P }.

6

Observe that in a graph with no terminal-terminal edges, a free terminal path is a steepest ﬁxable path iff its gradient
is equal to the highest pressure amongst all vertices. Moreover, vertices that lie on steepest ﬁxable paths are exactly
the vertices with the highest pressure. For a given α > 0, in order to identify vertices with pressure exceeding α, we
compute vectors vHigh[α](x) and vLow[α](x) deﬁned as follows in terms of dist, the metric on V induced by ℓ:

vLow[α](x) = min

{v0(t) + α · dist(x, t)}

t∈T (v0)

vHigh[α](x) = max
t∈T (v0)

{v0(t) − α · dist(t, x)}.

4.1 Lex-minimization on Star Graphs

We ﬁrst consider the problem of computing the lex-minimizer on a star graph in which every vertex but the center is a
terminal. This special case is a subroutine in the general algorithm, and also motivates some of our techniques.

Let x be the center vertex, T be the set of terminals, and all edges be of the form (x, t) with t ∈ T . The initial
voltage assignment is given by v : T → R, and we abbreviate dist(x, t) by d(t) = ℓ(x, t). From Corollary 3.4 we know
that we can determine the value of the lex minimizer at x by ﬁnding a steepest ﬁxable path. By deﬁnition, we need to
ﬁnd t1, t2 ∈ T that maximize the gradient of the path from t1 to t2, ∇(t1, t2) = v(t1)−v(t2)
d(t2)+d(t2) . As observed above, this
is equivalent to ﬁnding a terminal with the highest pressure. We now present a simple randomized algorithm for this
problem that runs in expected linear time.

Given a terminal t1, we can compute its pressure α along with the terminal t2 such that |∇(t1, t2)| = α in time
O(|T |) by scanning over the terminals in T . Consider doing this for a random terminal t1. We will show that in linear
time one can then ﬁnd the subset of terminals T ′ ⊂ T whose pressure is greater than α. Assuming this, we complete
the analysis of the algorithm. If T ′ = ∅, t1 is a vertex with highest pressure. Hence the path from t1 to t2 is a steepest
ﬁxable path, and we return (t1, t2). If T ′ 6= ∅, the terminal with the highest pressure must be in T ′, and we recurse by
picking a new random t1 ∈ T ′. As the size of T ′ will halve in expectation at each iteration, the expected time of the
algorithm on the star is O(|T |).

To determine which terminals have pressure exceeding α, we observe that the condition ∃t2 : α < ∇(t1, t2) =
v(t1)−v(t2)
d(t1)+d(t2) , is equivalent to ∃t2 : v(t2)+αd(t2) < v(t1)−αd(t1). This, in turn, is equivalent to vLow[α](x) < v(t1)−
αd(t1). We can compute vLow[α](x) in deterministic O(|T |) time. Similarly, we can check if ∃t2 : α < ∇(t2, t1) by
checking if vHigh[α](x) > vt1 + αd(t1). Thus, in linear time, we can compute the set T ′ of terminals with pressure
exceeding α. The above algorithm is described in Algorithm 10.

Theorem 4.1 Given a set of terminals T, initial voltages v : T → R, and distances d : T → R+, STARSTEEPESTPATH(T, v, d)
returns (t1, t2) maximizing v(t1)−v(t2)

d(t1)+d(t2) , and runs in expected time O(|T |).

4.2 Lex-minimization on General Graphs

Theorem 3.3, tells us that META-LEX will compute lex-minimizers given an algorithm for ﬁnding a steepest ﬁxable
path in (G, v0). Recall that ﬁnding a steepest ﬁxable path is equivalent to ﬁnding a path with gradient equal to the
highest pressure amongst all vertices. In this section, we show how to do this in expected time O(m + n log n).

We describe an algorithm VERTEXSTEEPESTPATH that ﬁnds a terminal path P through any vertex x such that
∇P (v0) = pressure[v0](x) in expected O(m + n log n) time. Using Dijkstra’s algorithm, we compute dist(x, t) for
all t ∈ T. If x ∈ T (v0), then there must be a terminal path P that starts at x that has ∇P (v0) = pressure[v0](x). To
compute such a P we examine all t ∈ T (v0) in O(|T |) time to ﬁnd the t that maximizes |∇(x, t)| = |v(x)−v(t)|
, and
dist(x,t)
then return a shortest path between x and that t.

If x /∈ T (v0), then the steepest path through x between terminals t1 and t2 must consist of shortest paths between
x and t1 and between x and t2. Thus, we can reduce the problem to that of ﬁnding the steepest path in a star graph
where x is the only non-terminal and is connected to each terminal t by an edge of length dist(x, t). By Theorem 4.1,
we can ﬁnd this steepest path in O(|T |) expected time. The above algorithm is formally described as Algorithm 9.

Theorem 4.2 Given a well-posed instance (G, v0), and a vertex x ∈ VG, VERTEXSTEEPESTPATH(G, v0, x) returns
a terminal path P through x such that ∇P (v0) = pressure[v0](x), in O(m + n log n) expected time.

7

As in the algorithm for the star graph, we need to identify the vertices whose pressure exceeds a given α. For a ﬁxed
α, we can compute vLow[α](x) and vHigh[α](x) for all x ∈ VG using a simple modiﬁcation of Dijkstra’s algorithm in
O(m + n log n) time. We describe the algorithms COMPVHIGH, COMPVLOW for these tasks in Algorithms 3 and 4.
The following lemma encapsulates the usefulness of vLow and vHigh.

Lemma 4.3 For every x ∈ VG, pressure[v0](x) > α iff vHigh[α](x) > vLow[α](x).

It immediately follows that the algorithm COMPHIGHPRESSGRAPH(G, v0, α) described in Algorithm 6 computes

the vertex induced subgraph on the vertex set {x ∈ VG| pressure[v0](x) > α}.

We can combine these algorithms into an algorithm STEEPESTPATH that ﬁnds the steepest ﬁxable path in (G, v0)
in O(m + n log n) expected time. We may assume that there are no terminal-terminal edges in G. We sample an edge
(x1, x2) uniformly at random from EG, and a terminal x3 uniformly at random from VG. For i = 1, 2, 3, we compute
the steepest terminal path Pi containing xi. By Theorem 4.2, this can be done in O(m + n log n) expected time. Let α
be the largest gradient maxi ∇Pi. As mentioned above, we can identify G′, the induced subgraph on vertices x with
pressure exceeding α, in O(m + n log n) time. If G′ is empty, we know that the path Pi with largest gradient is a
steepest ﬁxable path. If not, a steepest ﬁxable path in (G, v0) must be in G′, and hence we can recurse on G′. Since
we picked a uniformly random edge, and a uniformly random vertex, the expected size of G′ is at most half that of G.
Thus, we obtain an expected running time of O(m + n log n). This algorithm is described in detail in Algorithm 7.

Theorem 4.4 Given a well-posed instance (G, v0) with EG ∩ (T (v0) × T (v0)) = ∅, STEEPESTPATH(G, v0) returns
a steepest ﬁxable path in (G, v0), and runs in O(m + n log n) expected time.

By using STEEPESTPATH in META-LEX, we get the COMPLEXMIN, shown in Algorithm 1. From Theorem 3.3 and
Theorem 4.4, we immediately get the following corollary.

Corollary 4.5 Given a well-posed instance (G, v0) as input, algorithm COMPLEXMIN computes a lex-minimizing
assignment that extends v0 in O(n(m + n log n)) expected time.

4.3 Linear-time Algorithm for Inf-minimization

Given the algorithms in the previous section, it is straightforward to construct an inﬁnity minimizer. Let α⋆ be the
gradient of the steepest terminal path. From Lemma 3.5, we know that the norm of the inf minimizer is α⋆. Considering
all trivial terminal paths (terminal-terminal edges), and using STEEPESTPATH, we can compute α⋆ in randomized
O(m+n log n) time. It is well known (McShane (1934); Whitney (1934)) that v1 = vLow[α⋆] and v2 = vHigh[α⋆] are
inf-minimizers. It is also known that 1
2 (v1 + v2) is the inf-minimizer that minimizes the maximum ℓ∞-norm distance
to all inf-minimizers. In the case of path graphs, this was observed by Gaffney and Powell (1976) and independently
by Micchelli et al. (1976). For completeness, the algorithm is presented as Algorithm 5, and we have the following
result.

Theorem 4.6 Given a well-posed instance (G, v0), COMPINFMIN(G, v0) returns a complete voltage assignment v
for G extending v0 that minimizes

∞ , and runs in randomized O(m + n log n) time.

grad[v]

4.4 Faster Algorithms for Lex-minimization

(cid:13)
(cid:13)

(cid:13)
(cid:13)

The lex-minimizer has additional structure that allows one to compute it by more efﬁcient algorithms. One observation
that leads to a faster implementation is that ﬁxing a steepest ﬁxable path does not increase the pressure at vertices,
provided that one appropriately ignores terminal-terminal edges. Thus, if G(α) is a subgraph that we identiﬁed with
pressure greater than α, we can iteratively ﬁx all steepest ﬁxable paths P in G(α) with ∇P > α. Another simple
observation is that if G(α) is disconnected, we can simply recurse on each of the connected components. A complete
description of an the algorithm COMPFASTLEXMIN based on these idea is given in Algorithm 11. The algorithm
provably computes lexG(v0), and it is possible to implement it so that the space requirement is only O(m + n).
Although, we are unable to prove theoretical bounds on the running time that are better than O(n(m + n log n)),
it runs extremely quickly in practice. We used it to perform the experiments in this paper. For random regular
graphs and Delaunay graphs, with n = 0.5 × 106 vertices and around 2 million edges m ∼ 1.5 − 2 × 106, it

8

takes a couple of minutes on a 2009 MacBook Pro. Similar times are observed for other model graphs of this
size such as random regular graphs and real world networks. An implementation of this algorithm may be found
at https://github.com/danspielman/YINSlex.

5 Directed Graphs

Our deﬁnitions and algorithms, including those for regularization, extend to directed graphs with only small modiﬁ-
cations. We view directed edges as diodes and only consider potential differences in the direction of the edge. For
a complete voltage assignment v on the vertices of a directed graph G, we deﬁne the directed gradient on (x, y) due
to v to be grad+
. Given a partially-labelled directed graph (G, v0), we say that a a
complete voltage assignment v is a lex-minimizer if it extends v0 and for other complete voltage assignment v′ that
extends v0 we have grad+
G[v′]. We say that a partially-labelled directed graph (G, v0) is a well-posed
directed instance if every free vertex appears in a directed path between two terminals.

G[v](x, y) = max

G[v] (cid:22) grad+

v(x)−v(y)
ℓ(x,y)

, 0

n

o

The main difference between the directed and undirected cases is that the directed lex-minimizer is not necessarily
unique. To maintain clarity of exposition, we chose to focus on undirected graphs so far. For directed graphs, we have
the following corresponding structural results.

Theorem 5.1 Given a well-posed instance (G, v0) on a directed graph G, there exists a lex-minimizer, and the set of
all lex-minimizers is a convex set. Moreover, for every two lex-minimizers v and v′, we have grad+

G[v] = grad+

G[v′].

However, note that in the case of directed graphs, the lex-minimizer need not be unique. We still have a weaker version
of Theorem 3.3 for directed graphs.

Theorem 5.2 Given a well-posed instance (G, v0) on a directed graph G, let v1 be the partial voltage assignment
extending v0 obtained by repeatedly ﬁxing steepest ﬁxable (directed) paths P with ∇P > 0. Then, any lex-minimizer
of (G, v0) must extend v1. Moreover, for every edge e ∈ EG \ (T (V1) × T (V1)), any lex-minimizer v of (G, v0) must
satisfy grad+[v](e) = 0.

When the value of the lex-minimizer at a vertex is not uniquely determined, it is constrained to an interval. In our
experiments, we pick the convention that when the voltage at a vertex is constrained to an interval (−∞, a] or [a, ∞),
we assign a to the terminal. When it is constrained to a ﬁnite interval, we assign a voltage closest to the median of the
original voltages.

6 Experiments on WebSpam

We demonstrate the performance of our lex-minimization algorithms on directed graphs by using them to detect spam
webpages as in Zhou et al. (2007). We use the dataset webspam-uk2006-2.0 described in Castillo et al. (2006).
This collection includes 11,402 hosts, out of which 7,473 (65.5 %) are labeled, either as spam or normal. Each host
corresponds to the collection of web pages it serves. Of the hosts, 1924 are labeled spam (25.7 % of all labels). We
consider the problem of ﬂagging some hosts as spam, given only a small fraction of the labels for training. We assign
a value of 1 to the spam hosts, and a value of 0 to the normal ones. We then compute a lex minimizer and examine the
effect of ﬂagging as spam all hosts with a value greater than some threshold.

Following Zhou et al. (2007), we create edges between hosts with lengths equal to the reciprocal of the number of
links from one to the other. We run our experiments only on the largest strongly connected component of the graph,
which contains 7945 hosts of which 5552 are labeled. 16 % of the nodes in this subgraph are labeled spam. To create
training and test data, for a given value p, we select a random subset of p % of the spam labels and a random subset
of p % of the normal labels to use for training. The remaining labels are used for testing. We report results for p = 5
and p = 20.

Again following Zhou et al. (2007), we plot the precision and recall of different choices of threshold for ﬂagging
pages as spam. Recall is the fraction of spam pages our algorithm ﬂags as spam, and precision is the fraction of pages
our algorithm ﬂags as spam that actually are spam. Amongst the algorithms studied by Zhou et al. (2007), the top

9

performer was their algorithm based on sampling according to a random-walk that follows in-links from other hosts.
We compare their algorithm with the classiﬁcation we get by directing edges in the opposite directions of links. This
has the effect that a link to a spam host is evidence of spamminess, and a link from a normal host is evidence of
normality.

Results are shown in Figure 3. While we are not able to reliably ﬂag all spam hosts, we see that in the range of
10-50 % recall, we are able to ﬂag spam with precision above 82 %. We see that the performance of directed lex-
minimization does not degrade rapidly when from the “large training set” regime of p = 20, to the “small training set”
regime of p = 5.

5 % labels for training

20 % labels for training

RandWalk
DirectedLex

RandWalk
DirectedLex

1

0.9

0.8

0.7

i

i

n
o
s
c
e
r
P

0.6

0.5

0.4

0.3

0.2

0.1

0
0.1

0.2

0.3

0.4

0.7

0.8

0.9

1

0.2

0.3

0.4

0.7

0.8

0.9

1

0.6
0.5
Recall

0.6
0.5
Recall

Figure 3: Recall and precision in the web spam classiﬁcation experiment. Each data point shown was computed as an average over
100 runs. The largest standard deviation of the mean precision across the plotted recall values was less than 1.3 %. The algorithm
of Zhou et al. (2007) appears as RANDWALK. Our directed lex-minimization algorithm appears as DIRECTEDLEX.

For comparison, in Appendix C, we show the performance of our algorithm and that of Zhou et al. (2007) both
with link directions reversed, as well as the performance of undirected lex-minimization and Laplacian inference, all
of which are signiﬁcantly worse.

7 l0-Regularization of Vertex Values

We now explain how we can accommodate noise in both the given voltages and in the given lengths of edges. We can
ﬁnd the minimum number of labels to ignore, or the minimum increase in edges lengths needed so that there exists an
extension whose gradients have l∞-norm lower than a given target. After determining which labels to ignore or the
needed increment in edge lengths, we recommend computing a lex minimizer.

The algorithms we present in this section are essentially the same for directed and undirected graphs.

7.1 l0-Vertex Regularization for Inf-minimization

The l0-regularization of vertex labels can be viewed as a problem of outlier removal: the vector we compute is allowed
to disagree with v0 on up to k terminals. Given a voltage assignment v and a subset T ⊂ V of the vertices, by v(T )
we mean the vector obtained by restricting v to T . We deﬁne the l0-Vertex Regularization for l∞ problem to be

where v(T ) is the vector of values of v on the terminals T .

min
v∈IRn

gradG[v]

∞

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

0 ≤ k,
(cid:13)
(cid:13)

subject to

v(T ) − v0(T )

(3)

In Appendix D, we describe an approximation algorithm APPROX-OUTLIER that approximately solves program (3).

The precise statement we prove in Appendix D is given in the following theorem.

i

i

n
o
s
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
0.1

10

Theorem 7.1 (Approximate l0-vertex regularization) The algorithm APPROX-OUTLIER takes a positive integer k
and a partially-labeled graph (G, v0), and outputs an assignment v with
0 ≤ 2k, and
∞ ≤
α∗, where α∗ is the optimum value of program (3). The algorithm runs in time O(k(m + n log n)).
(cid:13)
(cid:13)
(cid:13)
(cid:13)

In Appendix D, we also describe an algorithm OUTLIER that exactly solves program (3) in polynomial time, and we
prove its correctness.

v(T ) − v0(T )

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Theorem 7.2 (Exact l0-vertex regularization) The algorithm OUTLIER takes a positive integer k and a partially-
labeled graph (G, v0) solves program (3) exactly. The algorithm runs in polynomial time.

We give a proof of Theorem 7.2 in Appendix D. To do this, we reduce the program (3) to the problem of minimizing
the required l0-budget needed to achieve a ﬁxed gradient α using a binary search over a set of O(n2) gradients. This
latter problem we reduce in polynomial time to Minimum Vertex Cover (VC) on a transitively closed, directed acyclic
graph (a TC-DAG). VC on a TC-DAG can be solved exactly in polynomial time by a reduction to the Maximum
Bipartite Matching Problem (Fulkerson (1956)). The problem was phrased by Fulkerson as one of ﬁnding a maximum
antichain of a ﬁnite poset. Any transitively closed DAG corresponds directly to the comparability graph of a poset. A
maximum antichain of a poset is a maximum independent set of a the comparability graph of the poset, and hence its
complement is a minimum vertex cover of the comparability graph. We refer to the algorithm developed by Fulkerson
as KONIG-COVER.

Theorem 7.3 The algorithm KONIG-COVER computes a minimum vertex cover for any transitively closed DAG G in
polynomial time.

7.2 Hardness of l0 regularization for l2

The result that l0-regularized inf-minimization can be solved exactly in polynomial time is surprising, especially
because the analogous problem for 2-Laplacian minimization turns out to be NP-Hard.

We deﬁne the the l0 vertex regularization for l2 for a partially-labeled graph (G, v0) and an integer k by

min
v∈Rn:kv(T )−v0(T )k0

≤k

vT Lv,

where L is the Laplacian of G.

Theorem 7.4 l0 vertex regularization for l2 is NP-Hard.

In Appendix E we prove Theorem 7.4 by giving a polynomial time (Karp) reduction from the NP-Hard minimum
bisection problem to l0 vertex regularization for l2.

8 l1-Edge and Vertex Regularization of Inf-minimizers

Consider a partially-labeled graph (G, v0) and an α > 0. The set of voltage assignments given by

v : v extends v0 and

gradG[v]

∞ ≤ α

n

(cid:13)
(cid:13)

(cid:13)
(cid:13)

o

is convex. Going further, let us consider the edge lengths in a graph to be speciﬁed by a vector ℓ ∈ IRE. Now the set
of voltages v and and lengths ℓ which achieve kgradG(ℓ)[v]k∞ ≤ α is jointly convex in v and ℓ. To see this, observe
that

kgradG(ℓ)[v]k∞ ≤ α ⇔ ∀(u, v) ∈ E : −αℓ(u, v) ≤ v(u) − v(v) ≤ αℓ(u, v).
Furthermore, the condition “v extends v0” is a linear constraint on v, which we express as v(T ) = v0(T ). From
the above, it is clear that the gradient condition corresponds to a convex set, as it is an intersection of half-spaces.
These half-spaces are given by O(m) linear inequalities. We can leverage this to phrase many regularized variants of
inf-minimization as convex programs, and in some cases linear programs.

(4)

11

For example, we may consider a variant of inf-minimization combined with an l1-budget for changing lengths of
edges and values on terminals. Given a parameter γ > 0 which speciﬁes the relative cost of regularizing terminals to
regularizing edges, the problem is as follows

arg min
v∈IRn,s∈IRm,s≥0

ksk1 + γ

v(T ) − v0(T )

1

subject to

gradG(ℓ+s)[v]

≤ α.

(5)

(cid:13)
(cid:13)
From our observation (4), it follows that problem (5) may be expressed as a linear program with O(n) variables
and O(m) constraints. We can use ideas from Daitch and Spielman (2008) to solve the resulting linear program in
O(m1.5) by an interior point method with a special purpose linear equation solver. The reason is that the linear
time
equations the IPM must solve at each iteration may be reduced to linear equations in symmetric, diagonally dominant
matrices, and these may be solved in nearly-linear time (Cohen et al. (2014)).

(cid:13)
(cid:13)

e

(cid:13)
(cid:13)
(cid:13)

∞

(cid:13)
(cid:13)
(cid:13)

Conclusion. We propose the use of inf and lex minimizers for regression on graphs. We present simple algorithms
for computing them that are provably fast and correct, and can also be implemented efﬁciently. We also present a
framework and polynomial time algorithms for regularization in this setting. The initial experiments reported in the
paper indicate that these algorithms give pretty good results on real and synthetic datasets. The results seem to compare
quite favorably to other algorithms, particularly in the regime of tiny labeled sets. We are testing these algorithms on
several other graph learning questions, and plan to report on them in a forthcoming experimental paper. We believe
that inf and lex minimizers, and the associated ideas presented in the paper, should be useful primitives that can be
proﬁtably combined with other approaches to learning on graphs.

We thank anonymous reviewers for helpful comments. We thank Santosh Vempala and Bartosz Walczak for pointing
out that it was already known how to compute a minimum vertex cover of a transitively closed DAG in polynomial
time.

Acknowledgements

References

Morteza Alamgir
In Advances
Information Processing
http://books.nips.cc/papers/files/nips24/NIPS2011_0278.pdf.

and Ulrike V. Luxburg.

transition
24,

in
pages

in Neural

Systems

Phase

the

family
379–387.

of
2011.

p-resistances.
URL

Gunnar Aronsson. Extension of functions satisfying lipschitz conditions. Arkiv fr Matematik, 6(6):551–561, 1967.

ISSN 0004-2080. doi: 10.1007/BF02591928. URL http://dx.doi.org/10.1007/BF02591928.

Gunnar Aronsson, Michael G. Crandall, and Petri Juutinen. A tour of the theory of absolutely minimizing functions.
ISSN 0273-0979. doi: 10.1090/S0273-0979-04-01035-3.

Bull. Amer. Math. Soc. (N.S.), 41(4):439–505, 2004.
URL http://dx.doi.org/10.1090/S0273-0979-04-01035-3.

Guy Barles and J´erˆome Busca. Existence and comparison results for fully nonlinear degenerate elliptic equations

without zeroth-order term. Comm. Partial Differential Equations, 26:2323–2337, 2001.

Mikhail Belkin, Irina Matveeva, and Partha Niyogi.

Regularization and semi-supervised learning on large
In Learning Theory, volume 3120 of Lecture Notes in Computer Science, pages 624–638.
doi: 10.1007/978-3-540-27819-1 43. URL

graphs.
Springer Berlin Heidelberg, 2004.
http://dx.doi.org/10.1007/978-3-540-27819-1_43.

ISBN 978-3-540-22282-8.

Nick Bridle and Xiaojin Zhu. p-voltages: Laplacian regularization for semi-supervised learning on high-dimensional

data. In Eleventh Workshop on Mining and Learning with Graphs (MLG2013), 2013.

12

Carlos Castillo, Debora Donato, Luca Becchetti, Paolo Boldi, Stefano Leonardi, Massimo Santini, and Sebastiano
Vigna. A reference collection for web spam. SIGIR Forum, 40(2):11–24, December 2006. ISSN 0163-5840. doi:
10.1145/1189702.1189703. URL http://doi.acm.org/10.1145/1189702.1189703.

Olivier Chapelle, Bernhard Schlkopf, and Alexander Zien. Semi-Supervised Learning. The MIT Press, 1st edition,

2010. ISBN 0262514125, 9780262514125.

Michael B Cohen, Rasmus Kyng, Gary L Miller, Jakub W Pachocki, Richard Peng, Anup B Rao, and Shen Chen Xu.
Solving SDD linear systems in nearly m log1/2 n time. In Proceedings of the 46th Annual ACM Symposium on
Theory of Computing, pages 343–352. ACM, 2014.

M.G. Crandall, L.C. Evans, and R.F. Gariepy. Optimal lipschitz extensions and the inﬁnity laplacian. Calculus of Vari-
ations and Partial Differential Equations, 13(2):123–139, 2001. ISSN 0944-2669. doi: 10.1007/s005260000065.
URL http://dx.doi.org/10.1007/s005260000065.

Samuel I. Daitch and Daniel A. Spielman. Faster approximate lossy generalized ﬂow via interior point algo-
rithms.
In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing, STOC ’08, pages
451–460, New York, NY, USA, 2008. ACM. ISBN 978-1-60558-047-0. doi: 10.1145/1374376.1374441. URL
http://doi.acm.org/10.1145/1374376.1374441.

Alan Egger and Robert Huotari. Rate of convergence of the discrete polya algorithm. Journal of Approximation
ISSN 0021-9045. doi: http://dx.doi.org/10.1016/0021-9045(90)90070-7. URL

Theory, 60(1):24 – 30, 1990.
http://www.sciencedirect.com/science/article/pii/0021904590900707.

Charles Fefferman. Whitney’s extension problems and interpolation of data.

(N.S.), 46(2):207–220, 2009a.
http://dx.doi.org/10.1090/S0273-0979-08-01240-8.

ISSN 0273-0979.

doi:

10.1090/S0273-0979-08-01240-8.

Bull. Amer. Math. Soc.
URL

Charles Fefferman. Fitting a [image] -smooth function to data, iii. Annals of Mathematics, 170(1):pp. 427–441, 2009b.

ISSN 0003486X. URL http://www.jstor.org/stable/40345469.

Charles Fefferman and Bo’az Klartag. Fitting a cm -smooth function to data i. Annals of Mathematics, 169(1):pp.

315–346, 2009. ISSN 0003486X. URL http://www.jstor.org/stable/40345445.

D. R. Fulkerson. Note on dilworths decomposition theorem for partially ordered sets. Proc. Amer. Math. Soc, 1956.

P.W. Gaffney and M.J.D. Powell. Optimal interpolation. In Numerical Analysis, volume 506 of Lecture Notes in Math-
ematics, pages 90–99. Springer Berlin Heidelberg, 1976. ISBN 978-3-540-07610-0. doi: 10.1007/BFb0080117.
URL http://dx.doi.org/10.1007/BFb0080117.

L.-A. Gottlieb, A. Kontorovich, and R. Krauthgamer. Efﬁcient classiﬁcation for metric data. CoRR, abs/1306.2547,

2013. URL http://arxiv.org/abs/1306.2547.

L.-A. Gottlieb, A. Kontorovich, and R. Krauthgamer. Efﬁcient regression in metric spaces via approximate lipschitz
extension. In Similarity-Based Pattern Recognition, volume 7953 of Lecture Notes in Computer Science, pages
43–58. Springer Berlin Heidelberg, 2013b. ISBN 978-3-642-39139-2. doi: 10.1007/978-3-642-39140-8 3. URL
http://dx.doi.org/10.1007/978-3-642-39140-8_3.

Robert Jensen. Uniqueness of lipschitz extensions: Minimizing the sup norm of the gradient. Archive for Ra-
doi: 10.1007/BF00386368. URL

ISSN 0003-9527.

tional Mechanics and Analysis, 123(1):51–74, 1993.
http://dx.doi.org/10.1007/BF00386368.

M. Kirszbraun. ber die zusammenziehende und lipschitzsche transformationen. Fundamenta Mathematicae, 22(1):

77–108, 1934. URL http://eudml.org/doc/212681.

13

Andrew J. Lazarus, Daniel E. Loeb,

James G. Propp, Walter R. Stromquist,

Combinatorial games under

man.
229 – 264,
http://www.sciencedirect.com/science/article/pii/S0899825698906765.

http://dx.doi.org/10.1006/game.1998.0676.

and Economic Behavior,

ISSN 0899-8256.

auction play.

Games

1999.

doi:

and Daniel H. Ull-
27(2):
URL

Yunqian Ma and Yun Fu. Manifold Learning Theory and Applications. CRC Press, Inc., Boca Raton, FL, USA, 1st

edition, 2011. ISBN 1439871094, 9781439871096.

E. J. McShane. Extension of range of functions. Bull. Amer. Math. Soc., 40(12):837–842, 12 1934. URL

http://projecteuclid.org/euclid.bams/1183497871.

C.A. Micchelli, T.J. Rivlin,

and S. Winograd.

merische Mathematik, 26(2):191–200, 1976.
http://dx.doi.org/10.1007/BF01395972.

The optimal
ISSN 0029-599X.

recovery of
doi:

smooth functions.
10.1007/BF01395972.

Nu-
URL

V. A. Milman.

Absolutely minimal extensions of

functions on metric spaces.

1999.

URL

http://iopscience.iop.org/1064-5616/190/6/A05/pdf/MSB_190_6_A05.pdf.

Boaz Nadler, Nathan Srebro, and Xueyuan Zhou. Statistical analysis of semi-supervised learning: The limit of inﬁnite
unlabelled data. 2009. URL http://ttic.uchicago.edu/˜nati/Publications/NSZnips09.pdf.

A. Naor and S. Shefﬁeld. Absolutely minimal Lipschitz extension of tree-valued mappings. CoRR, abs/1005.2535,

May 2010. URL http://arxiv.org/abs/1005.2535.

A. M. Oberman. Finite difference methods for the Inﬁnity Laplace and p-Laplace equations. CoRR, abs/1107.5278,

July 2011. URL http://arxiv.org/abs/1107.5278.

Yuval Peres, Oded Schramm, Scott Shefﬁeld, and DavidB. Wilson.

Tug-of-war and the inﬁnity lapla-
In Selected Works of Oded Schramm, Selected Works in Probability and Statistics, pages 595–
doi: 10.1007/978-1-4419-9675-6 18. URL

cian.
638. Springer New York, 2011.
http://dx.doi.org/10.1007/978-1-4419-9675-6_18.

ISBN 978-1-4419-9674-9.

S. Shefﬁeld and C. K. Smart. Vector-valued optimal Lipschitz extensions. CoRR, abs/1006.1741, June 2010. URL

http://arxiv.org/abs/1006.1741.

Ali Kemal Sinop and Leo Grady. A seeded image segmentation framework unifying graph cuts and random walker
which yields a new algorithm. In Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on,
pages 1–8. IEEE, 2007.

Vijay V. Vazirani. Approximation Algorithms. Springer-Verlag New York, Inc., New York, NY, USA, 2001. ISBN

3-540-65367-8.

Ulrike von Luxburg and Olivier Bousquet. Distance-based classiﬁcation with lipschitz functions.

In Learn-
ing Theory and Kernel Machines, volume 2777 of Lecture Notes in Computer Science, pages 314–328.
doi: 10.1007/978-3-540-45167-9 24. URL
Springer Berlin Heidelberg, 2003.
http://dx.doi.org/10.1007/978-3-540-45167-9_24.

ISBN 978-3-540-40720-1.

Hassler Whitney.

Analytic extensions of differentiable functions deﬁned in closed sets.

tions of
http://www.jstor.org/stable/1989708.

the American Mathematical Society, 36(1):pp. 63–89, 1934.

ISSN 00029947.

Transac-
URL

Dengyong Zhou, Christopher J. C. Burges, and Tao Tao. Transductive link spam detection.

In Proceedings
of the 3rd International Workshop on Adversarial Information Retrieval on the Web, AIRWeb ’07, pages 21–
ISBN 978-1-59593-732-2. doi: 10.1145/1244408.1244413. URL
28, New York, NY, USA, 2007. ACM.
http://doi.acm.org/10.1145/1244408.1244413.

Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic

functions. In IN ICML, pages 912–919, 2003.

14

A Basic Properties of Lex-Minimizers

A.1 Meta Algorithm

Algorithm 1: Algorithm META-LEX: Given a well-posed instance (G, v0), outputs lexG[v0].
for i = 1, 2, . . . :

1. if T (vi−1) = VG, then return vi−1.
2. E′ = EG \ (T (vi−1) × T (vi−1)), G′ := (VG, E′).
3. Let P ⋆
4. vi ← ﬁx[vi−1, P ⋆
i ].

i be a steepest ﬁxable path in (G′, vi−1). Let α⋆

i ← ∇P ⋆(vi−1).

In this subsection, we prove the results that appeared in section 2. We start with a simple observation.

Proposition A.1 Given a well-posed instance (G, v0) such that T (v0) 6= V, let P be a steepest ﬁxable path in (G, v0).
Then, ﬁx[v0, P ] extends v0, and (G, ﬁx[v0, P ]) is also a well-posed instance.

The properties we prove below do not depend on the choice of the steepest ﬁxable path.

Proposition A.2 For any well-posed instance (G, v0), with |VG| = n, META-LEX(G, v0) terminates in at most n
iterations, and outputs a complete voltage assignment v that extends v0.

Proof of Proposition A.2: By Proposition A.1, at any iteration i, vi−1 extends v0 and (G′, vi−1) is a well-posed
instance. META-LEX only outputs vi−1 iff T (vi−1) = V, which means vi−1 is a complete voltage assignment. For
any vi−1 that is not complete, for any x ∈ V \T (vi−1), we must have a free terminal path in (G′, vi−1) that contains x.
i exists in (G′, vi−1). Since P ⋆
Hence, a steepest ﬁxable path P ⋆
i ] ﬁxes the voltage
i
✷
for at least one non-terminal. Thus, META-LEX(G, v0) must complete in at most n iterations.

is a free terminal path, ﬁx[vi−1, P ⋆

For the following lemmas, consider a run of META-LEX with well-posed instance (G, v0) as input. Let vout be the
complete voltage assignment output by META-LEX. Let Ei be the set of edges E′ and Gi be the graph G′ constructed
in iteration i of META-LEX.

Lemma A.3 For every edge e ∈ Ei−1 \ Ei, we have

grad[vout](e)

≤ α⋆

i . Moreover, α⋆

i is non-increasing with i.

Proof of Lemma A.3: Let P ⋆
i = (x0, . . . , xr) be a steepest ﬁxable path in iteration i (when we deal with instance
(Gi−1, vi−1)). Consider a terminal path Pi+1 in (Gi, vi) such that {∂0Pi+1, ∂1Pi+1} ∩ (T (vi) \ T (vi−1)) 6= ∅. We
i . On the contrary, assume that ∇Pi+1(vi) > α⋆
claim that ∇Pi+1(vi) ≤ α⋆
i . Consider the case ∂0Pi+1 ∈ T (vi) \
T (vi−1), ∂1P1 ∈ T (vi−1). By the deﬁnition of vi, we must have ∂0Pi+1 = xj for some j ∈ [r − 1]. Let P ′
i+1 be the
path formed by joining paths P ⋆

i+1 is a free terminal path in (Gi−1, vi−1). We have,

i [x0 : xj] and Pi+1. P ′

(cid:12)
(cid:12)

(cid:12)
(cid:12)

vi−1(x0) − vi−1(∂1Pi+1) = (vi(x0) − vi(xj )) + (vi(∂0Pi+1) − vi(∂1Pi+1))
i · ℓ(P ′

i · ℓ(Pi+1) = α⋆

i [x0 : xj]) + α⋆

i · ℓ(P ⋆

> α⋆

i+1),

giving ∇P ′
The other cases can be handled similarly.

i+1(vi) > α⋆

i , which is a contradiction since the steepest ﬁxable path P ⋆
i

in (Gi−1, vi−1) has gradient α⋆
i .

Applying the above claim to an edge e ∈ Ei−1 \ Ei, whose gradient is ﬁxed for the ﬁrst time in iteration i, we
i . If v is the complete voltage assignment output by META-LEX, since v extends vi+1,
i , implying

i . Applying the claim to the symmetric edge, we obtain −grad[vout](e) ≤ α⋆

obtain that grad[vi+1](e) ≤ α⋆
we get grad[vout](e) ≤ α⋆
|grad[vout](e)| ≤ α⋆
i .

Consider any free terminal path Pi+1 in (Gi, vi). If Pi+1 is also a terminal path in (Gi−1, vi−1), it is a free
terminal path in (Gi−1, vi−1). In addition, since a steepest ﬁxable path P ⋆
i , we get
i
∇Pi+1(vi) = ∇Pi+1(vi−1) ≤ α⋆
i . Otherwise, we must have {∂0Pi+1, ∂1Pi+1} ∩ (T (vi) \ T (vi−1)) 6= ∅, and we can
deduce ∇Pi+1(vi) ≤ α⋆
i using the above claim. Thus, all free terminal paths Pi+1 in (Gi, vi) satisfy ∇Pi+1(vi) ≤ α⋆
i .
✷
In particular, α⋆

in (Gi−1, vi−1) has ∇P ⋆

i = α⋆

i is non-increasing with i.

i+1(vi) ≤ α⋆

i+1 = ∇P ⋆

i . Thus, α⋆

15

Lemma A.4 For any complete voltage assignment v for G that extends v0, if v 6= vout, we have grad[v] 6(cid:22) grad[vout],
and hence grad[vout] (cid:22) grad[v].

Proof of Lemma A.4: Consider any complete voltage assignment v for G that extends v0, such that v 6= vout. Thus,
there exists a unique i such that v extends vi−1 but does not extend vi. We will argue that grad[v] 6(cid:22) grad[vout], and
hence grad[vout] (cid:22) grad[v]. For every edge e ∈ E \ Ei−1 that has been ﬁxed so far, grad[v](e) = grad[vi−1](e) =
grad[vout](e), and hence we can ignore these edges.

Since v extends vi−1 but not vi, there exists an x ∈ T (vi) \ T (vi−1) such that v(x) 6= vi(x) = vout(x). Assume
i picked

i = (x0, . . . , xr) is the steepest ﬁxable path with gradient α⋆

v(x) < vi(x) (the other case is symmetric). If P ⋆
in iteration i, we must have x = xj for some j ∈ [r − 1]. Thus,

j

j

(v(xk−1) − v(xk)) = v(x0) − v(xj ) > vi(x0) − vi(xj ) = α⋆

i · ℓ(P ⋆

i [x0 : xj ]) = α⋆
i ·

ℓ(xk−1, xk).

Xk=1

Xk=1
Thus, for some k ∈ [j], we must have grad[v](xk−1, xk) > α⋆
is a path in Gi−1, we have {xk−1, xk} 6⊆
T (vi−1). This gives (xk−1, xk) ∈ (Ei−1 \ Ei). But then, from Lemma A.3, it follows that for all e ∈ (Ei−1 \ Ei), we
✷
have |grad[vout](e)| ≤ α⋆

i . Thus, we have grad[v] 6(cid:22) grad[vout].

i . Since P ∗
i

Lemma A.5 Let P = (x0, . . . , xr) be a steepest ﬁxable path such that it does not have any edges in T (v0) × T (v0)
and v1 = ﬁxG[v0, P ]. Then for every i ∈ [r], we have grad[v1](xi−1, xi) = ∇P.

Proof of Lemma A.5: Suppose this is not true and let j ∈ [r] be the minimum number such that grad[v1](xj−1, xj) 6=
∇P. By deﬁnition of v1 we would necessarily have j < r and vj ∈ T (v0). Suppose grad[v1](xj−1, xj ) < ∇P. We
would then have v1(x0) − v1(xj ) < ∇P ∗ ℓ(P [x0 : xj]). Since P does not have any edges in T (v0) × T (v0),
P1 := (xj, ..., xr) would be a free terminal path with ∇P1 > ∇P. This is a contradiction. Other cases can be ruled
out similarly.

✷

Proof of Theorem 3.3: Consider an arbitrary run of META-LEX on (G, v0). Let vout be the complete voltage
assignment output by META-LEX. Proposition A.1 implies that vout extends v0. Lemma A.4 implies that for any
complete voltage assignment v 6= vout that extends v0, we have grad[vout] (cid:22) grad[v]. Thus, vout is a lex-minimizer.
Moreover, the lemma also gives that for any such v, grad[v] 6(cid:22) grad[vout]. and hence vout is a unique lex-minimizer.
Thus, vout is the unique voltage assignment satisfying Def. 2.1, and we denote it as lexG[v0]. Since we started with an
✷
arbitrary run of META-LEX, uniqueness implies that every run of META-LEX on (G, v0) must output lexG[v0].

Proof of Lemma 3.5: Suppose we have a complete voltage assignment v extending v0, such that
For any terminal path P = (x0, . . . , xr), we get,

grad[v]

∞ ≤ α.

∇P (v0) = v0(∂0P ) − v0(∂1P ) = v(∂0P ) − v(∂1P ) =

grad[v](xi−1, xi) ≤ α ·

ℓ(xi−1, xi) = α · ℓ(P ),

(cid:13)
(cid:13)

(cid:13)
(cid:13)

r

i=1
X

giving ∇P (v0) ≤ α.

On the other hand, suppose every terminal path P in (G, v0) satisﬁes ∇P (v0) ≤ α. Consider v = lexG[v0]. We
know that v extends v0. For every edge e ∈ EG ∩ T (v0) × T (v0), e is a (trivial) terminal path in (G, v0), and hence
has satisﬁes grad[v](e) = grad[v0](e) = ∇e(v0) ≤ α. Considering the reverse edge, we also obtain −grad[v](e) ≤ α.
Thus, |grad[v](e)| ≤ α. Moreover, using Lemma A.3, we know that for edge e ∈ EG \ T (v0) × T (v0), |grad[v](e)| ≤
1 = ∇P ⋆
α⋆
1 ≤ α since P1 is a terminal path in (G, v0). Thus, for every e ∈ EG, |grad[v](e)| ≤ α, and hence
✷
grad[v]
∞ ≤ α.
(cid:13)
(cid:13)
(cid:13)
(cid:13)
A.2 Stability

In this subsection, we sketch a proof of the monotonicity of lex-minimizers and show how it implies the stability
property claimed earlier.

For any well-posed (G, v0), there could be several possible executions of META-LEX, each characterized by the

sequence of paths P ⋆

i . We can apply Theorem 3.3 to deduce the following structural result about the lex-minimizer.

r

i=1
X

16

Corollary A.6 For any well-posed instance (G, v0), consider a sequence of paths (P1, . . . , Pr) and voltage assign-
ments (v1, . . . , vr) for some positive integer r such that:

1. P ⋆

i is a steepest ﬁxable path in (Gi−1, vi−1) for i = 1, . . . , r.

2. vi = ﬁx[vi−1, P ⋆

i ] for i = 1, . . . , r.

3. T (vr) = VG.

Then, we have vr = lexG[v0].

We call such a sequence of paths and voltages to be a decomposition of lexG[v0]. Again, note that lexG[v0] can
possibly have multiple decompositions. However, any two such decompositions are consistent in the sense that they
produce the same voltage assignment.

Proof of Corollary 3.7: We ﬁrst deﬁne some operations on partial assignments which simpliﬁes the notation. Let
v0, v1 be any two partial assignments with the same set of terminals T := T (v0) = T (v1) and c, d ∈ R. By cv0 + d
we mean a partial assignment v with T (v) = T satisfying v(t) = cv0(t) + d for all t ∈ T . Also, by v0 + v1 we
mean a partial assignment v with T (v) = T satisfying v(t) = v0(t) + v1(t) for all t ∈ T. Also, we say v1 ≥ v0 if
v1(t) ≥ v0(t) for all t ∈ T .

Now we can show how Corollary 3.7 follows from Theorem 3.6. Let v := v1 − v0, and kvk∞ = ǫ, for some ǫ > 0.
Therefore, v0 + ǫ ≥ v1 ≥ v0 − ǫ. Theorem 3.6 then implies that lexG[v0] + ǫ ≥ lex[v1] ≥ lex[v0] − ǫ, hence proving
✷
the corollary.

Proof sketch of Theorem 3.6:
It is easy to see that the ﬁrst statement holds. For the second statement, we ﬁrst
observe that if there is a sequence of paths P1, ..., Pr that is simultaneously a decomposition of both lex[v0] and
lex[v1], then this is easy to see. If such a path sequence doesn’t exist, then we look at vt := v0 + t(v1 − v0). We
state here without a proof (though the proof is elementary) that we can then split the interval [0, 1] into ﬁnitely many
subintervals [a0, a1], [a1, a2], .., [ak−1, ak], with a0 = 0, ak = 1, such that for any i, there is a path sequence P1, ..., Pr
which is a decomposition of lex[vt] for all t ∈ [ai, ai+1]. We then observe that v0 = va0 ≤ va1 ≤ ...vak = v1. Since
for every ai, ai+1, there is a path sequence which is simultaneously a decomposition of both lex[vai ] and lex[vai+1 ],
we immediately get

lex[v0] = lex[va0 ] ≤ lex[va1] ≤ ... ≤ lex[vak ] = lex[v1].

✷

A.3 Alternate Characterizations

Proof of Theorem 3.10: We know that lexG[v0] extends v0. We ﬁrst prove that v = lexG[v0] satisﬁes the max-min
gradient averaging property. Assume to the contrary. Thus, there exists x ∈ VG \ T (v0) such that

max
y:(x,y)∈EG

grad[v](x, y) 6= − min

grad[v](x, y).

y:(x,y)∈EG

Assume that max(x,y)∈EG grad[v](x, y) ≥ − min(x,y)∈EG grad[v](x, y). Then, consider v′ extending v0 that is iden-
tical to v except for v′(x) = v(x) − ǫ for ǫ > 0. For ǫ small enough, we get that

and

max
y:(x,y)∈EG

grad[v′](x, y) < max

grad[v](x, y)

y:(x,y)∈EG

− min

y:(x,y)∈EG

grad[v′](x, y) < max

grad[v](x, y).

y:(x,y)∈EG

The gradient of edges not incident on the vertex x is left unchanged. This implies that grad[v]

6(cid:22) grad[v′],

contradicting the assumption that v is the lex-minimizer. (The other case is similar).

17

For the other direction. Consider a complete voltage assignment v extending v0 that satisﬁes the max-min gradient

averaging property w.r.t. (G, v0). Let

α = max

grad[v](x, y) ≥ 0

(x,y)∈EG
x∈V \T (v0)

be the maximum edge gradient, and consider any edge (x0, x1) ∈ EG such that grad[v](x1, x0) = α, with x1 ∈
V \ T (v0). If α = 0, grad[v] is identically zero, and is trivially the lex-minimal gradient assignment. Thus, both v and
lexG[v0] are constant on each connected component. Since (G, v0) is well-posed, there is at least one terminal in each
component, and hence v and lexG[v0] must be identical.

Now assume α > 0. By the max-min gradient averaging property, ∃x2 ∈ VG such that (x1, x2) ∈ EG and

grad[v](x1, x2) =

min
y:(x1,y)∈EG

grad[v](x1, y) = − max

grad[v](x1, y)

y:(x1,y)∈EG

≤ −grad[v](x1, x0) = −α.

Thus, grad[v](x2, x1) ≥ α. Since α is the maximum edge gradient, we must have grad[v](x2, x1) = α. More-
over, v(x2) > v(x1) > v(x0), thus x2 6= x0. We can inductively apply this argument at x2 until we hit a ter-
minal. Similarly, if x0 /∈ T (v0) we can extend the path in the other direction. Consequently, we obtain a path
P = (xj , . . . , x2, x1, x0, x−1, . . . , xk) with all vertices as distinct, such that xj , xk ∈ T (v0), and xi ∈ V \ T (v0)
for all i ∈ [j + 1, k − 1]. Moreover, grad[v](xi, xi−1) = α for all j < i ≤ k. Thus, P is a free terminal path with
∇P [v0] = α.

Moreover, since v is a voltage assignment extending v0 with

∞ = α, using Lemma 3.5, we know that
every terminal path P ′ in (G, v0) must satisfy ∇P ′(v0) ≤ α. Thus, P is a steepest ﬁxable path in (G, v0). Thus,
letting v1 = ﬁx[v0, P ], using Corollary 3.4, we obtain that lexG[v1] = lexG[v0]. Moreover, since α = ∇P [v0] =
grad[v](xi, xi−1) for all i ∈ (j, k], we get v1(xi) = v(xi) for all i ∈ (j, k). Thus, v extends v1.

grad[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We can iterate this argument for r iterations until T (vr) = VG, giving v = vr and vr = lexG[vr] = lexG[v0].
(Since we are ﬁxing at least one terminal at each iteration, this procedure terminates). Thus, we get v = lexG[v0]. ✷

B Description of the Algorithms

Algorithm 2: MODDIJKSTRA(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs a complete
voltage assignment v for G, and an array parent : V → V ∪ {null}.

Add x to a ﬁbonacci heap, with key(x) = +∞.
ﬁnished(x) ← false

Decrease key(x) to v0(x).
parent(x) ← null.

1. for x ∈ VG,
2.
3.
4. for x ∈ T (v0)
5.
6.
7. while heap is not empty
8.
9.
10.
11.
12.
13.
14.
15. return (v, parent)

x ← pop element with minimum key from heap
v(x) ← key(x). ﬁnished(x) ← true .
for y : (x, y) ∈ EG

if ﬁnished(y) = false

if key(y) > v(x) + α · ℓ(x, y)

Decrease key(y) to v(x) + α · ℓ(x, y).
parent(y) ← x.

Theorem B.1 For a well-posed instance (G, V0) and a gradient value α ≥ 0, let (v, parent) ← MODDIJKSTRA(G, v0, α).
Then, v is a complete voltage assignment such that, ∀x ∈ VG, v(x) = mint∈T (v0){v0(t) + αdist(x, t)}. Moreover, the
pointer array parent satisﬁes ∀x /∈ T (v0), parent(x) 6= null and v(x) = v(parent(x)) + α · ℓ(x, parent(x)).

18

Algorithm 3: Algorithm COMPVLOW(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs
vLow, a complete voltage assignment for G, and an array LParent : V → V ∪ {null}.

1. (vLow, LParent) ← MODDIJKSTRA(G, v0, α)
2. return (vLow, LParent)

Algorithm 4: Algorithm COMPVHIGH(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0, outputs
vHigh, a complete voltage assignment for G, and an array HParent : V → V ∪ {null}.

if x ∈ T (v0) then v1(x) ← −v0(x) else v1(x) ← v1(x).

1. for x ∈ VG
2.
3. (temp, HParent) ← MODDIJKSTRA(G, v1, α)
4. for x ∈ VG : vHigh(x) ← −temp(x)
5. return (vHigh, HParent)

Corollary B.2 For a well-posed instance (G, V0) and a gradient value α ≥ 0, let (vLow[α], LParent) ← COMPVLOW(G, v0, α)
and (vHigh[α], HParent) ← COMPVHIGH(G, v0, α). Then, vLow[α], vHigh[α] are complete voltage assignments for
G such that, ∀x ∈ VG,

vLow[α](x) = min

{v0(t) + α · dist(x, t)}

t∈T (v0)

vHigh[α](x) = max
t∈T (v0)

{v0(t) − α · dist(t, x)}.

Moreover, the pointer arrays LParent, HParent satisfy ∀x /∈ T (v0), LParent(x), HParent(x) 6= null and

vLow[α](x) = vLow[α](LParent(x)) + α · ℓ(x, LParent(x)),
vHigh[α](x) = vHigh[α](HParent(x)) − α · ℓ(x, HParent(x)).

Algorithm 5: Algorithm COMPINFMIN(G, v0): Given a well-posed instance (G, v0), outputs a complete voltage assignment
v for G, extending v0 that minimizes (cid:13)

(cid:13)grad[v](cid:13)

(cid:13)∞.

1. α ← max{|grad[v0](e)| | e ∈ EG ∩ (T (v0) × T (v0))}.
2. EG ← EG \ (T (v0) × T (v0))
3. P ←STEEPESTPATH(G, v0).
4. α ← max{α, ∇P (v0)}
5. (vLow, LParent) ← COMPVLOW(G, v0, α)
6. (vHigh, HParent) ← COMPVHIGH(G, v0, α)
7. for x ∈ VG
8.
9.
10.
11. return v

then v(x) ← v0(x)
else v(x) ← 1

2 · (vLow(x) + vHigh(x)).

if x ∈ T (v0)

1. (vLow, LParent) ← COMPVLOW(G, v0, α)
2. (vHigh, HParent) ← COMPVHIGH(G, v0, α)
3. VG′ ← {x ∈ VG | vHigh(x) > vLow(x) }
4. EG′ ← {(x, y) ∈ EG | x, y ∈ VG′ }.

19

Algorithm 6: Algorithm COMPHIGHPRESSGRAPH(G, v0, α): Given a well-posed instance (G, v0), a gradient value α ≥ 0,
outputs a minimal induced subgraph G′ of G where every vertex has pressure[v0](·) > α.

5. G′ ← (V ′, E′, ℓ)
6. return G′

Proof of Lemma 4.3:

is equivalent to

vHigh[α](x) > vLow[α](x)

max
t∈T (v0)

{v0(t) − α · dist(t, x)} > min

{v0(t) + α · dist(x, t)},

t∈T (v0)

which implies that there exists terminals s, t ∈ T (v0) such that

thus,

Hence,

v0(t) − α · dist(t, x) > v0(s) + α · dist(x, s)

pressure[v0](x) ≥

v0(t) − v0(s)
dist(t, x) + dist(x, s)

> α.

v0(t) − v0(s)
dist(t, x) + dist(x, s)

= pressure[v0](x) > α.

v0(t) − α · dist(t, x) > v0(s) + α · dist(x, s)

So the inequality on vHigh and vLow implies that pressure is strictly greater than α. On the other hand, if pressure[v0](x) >
α, there exists terminals s, t ∈ T (v0) such that

which implies vHigh[α](x) > vLow[α](x).

✷

Algorithm 7: Algorithm STEEPESTPATH(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs a steepest
free terminal path P in (G, v0).

P ← VERTEXSTEEPESTPATH(G, v0, xi)

1. Sample uniformly random e ∈ EG. Let e = (x1, x2).
2. Sample uniformly random x3 ∈ VG.
3. for i = 1 to 3
4.
5. Let j ∈ arg maxj∈{1,2,3} ∇Pj (v0)
6. G′ ← COMPHIGHPRESSGRAPH(G, v0, ∇Pj (v0))
7. if EG′ = ∅,
8.
9.

then return Pj
else return STEEPESTPATH(G′, v0|VG′ )

1. while T (v0) 6= VG
2.
3.
4.
5. return v0

EG ← EG \ (T (v0) × T (v0))
P ← STEEPESTPATH(G, v0)
v0 ← ﬁx[v0, P ]

Algorithm 8: Algorithm COMPLEXMIN(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs lexG[v0].

Algorithm 9: Algorithm VERTEXSTEEPESTPATH(G,v0, x): Given a well-posed instance (G, v0), and a vertex x ∈ VG,
outputs a steepest terminal path in (G, v0) through x.

1. Using Dijkstra’s algorithm, compute dist(x, t) for all t ∈ T (v0)

20

y ← arg maxy∈T (v0)
if v0(x) ≥ v0(y)

|v0(x)−v0(y)|
dist(x,y)

then return a shortest path from x to y
else return a shortest path from y to x

2. if x ∈ T (v0)
3.
4.
5.
6.
7. else
8.
9.
10.
11.

for t /∈ T (v0), d(t) ← dist(x, t)
(t1, t2) ← STARSTEEPESTPATH(T (v0), v0|T (v0), d)
Let P1 be a shortest path from t1 to x. Let P2 be a shortest path from x to t2.
P ← (P1, P2). return P.

Algorithm 10: STARSTEEPESTPATH(T, v, d): Returns the steepest path in a star graph, with a single non-terminal connected
to terminals in T, with lengths given by d, and voltages given by v.

|v(t1)−v(t)|
d(t1)+d(t)

1. Sample t1 uniformly and randomly from T
2. Compute t2 ∈ arg maxt∈T
3. α ← |v(t2)−v(t1)|
d(t1)+d(t2)
4. Compute vlow ← mint∈T (v(t) + α · d(t))
5. Tlow ← {t ∈ T | v(t) > vlow + α · d(t)}
6. Compute vhigh ← maxt∈T (v(t) − α · d(t))
7. Thigh ← {t ∈ T | v(t) < vhigh − α · d(t)}
8. T ′ ← Tlow ∪ Thigh.
9. if T ′ = ∅
10.
11.

then if v(t1) ≥ v(t2) then return (t1, t2) else return (t2, t1)
else return STARSTEEPESTPATH(T ′, v|T ′, dT ′ )

B.1 Faster Lex-minimization

Algorithm 11: Algorithm COMPFASTLEXMIN(G, v0): Given a well-posed instance (G, v0), with T (v0) 6= VG, outputs
lexG[v0].

1. while T (v0) 6= VG
2.
3. return v0

v0 ← FIXPATHSABOVEPRESS(G, v0, 0)

Algorithm 12: Algorithm FIXPATHSABOVEPRESS(G, v0, α): Given a well-posed instance (G, v0), with T (v0) 6= VG, and
a gradient value α, iteratively ﬁxes all paths with gradient > α.

EG ← EG \ (T (v0) × T (v0))
Sample uniformly random e ∈ EG. Let e = (x1, x2).
Sample uniformly random x3 ∈ VG.
for i = 1 to 3

Pi ← VERTEXSTEEPESTPATH(G, v0, xi)

Let j ∈ arg maxj∈{1,2,3} ∇Pj(v0)
G′ ← COMPHIGHPRESSGRAPH(G, v0, ∇Pj (v0))
if EG′ = ∅,

1. while T (v0) 6= VG
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.

then v0 ← ﬁx[v0, P ]
else Let G′

for i = 1, . . . , r

i, i = 1, . . . , r be the connected components of G′.

21

vi ← FIXPATHSABOVEPRESS(G′
for x ∈ VG′

i, set v0(x) ← vi(x)

i, v0|VG′

i

, ∇Pj (v0))

if α > 0 then G ←COMPHIGHPRESSGRAPH(G, v0, α)

13.
14.
15.
16. return v0

C Experiments on WebSpam: Testing More Algorithms

For completeness, in this appendix we show how a number of algorithms perform on the web spam experiment of
Section 6. We consider the following algorithms:

• RANDWALK along in-links. For a detailed description see Zhou et al. (2007). This algorithm essentially per-
forms a Personalized PageRank random walk from each vertex x and computes a spam-value for the vertex x by
taking a weighted average of the labels of the vertices where the random walk from x terminates. Also shown in
Section 6.

• DIRECTEDLEX, with edges in the opposite directions of links. This has the effect that a link to a spam host is

evidence of spam, and a link from a normal host is evidence of normality. Also shown in Section 6.

• RANDWALK along out-links.

• DIRECTEDLEX, with edges in the directions of links. This has the effect that a link from to a spam host is

evidence of spam, and a link to a normal host is evidence of normality.

• UNDIRECTEDLEX: Lex-minimization with links treated as undirected edges.

• LAPLACIAN: l2-regression with links treated as undirected edges.

• DIRECTED 1-NEAREST NEIGHBOR: Uses shortest distance along paths following out-links. Spam-ratio is
deﬁned distance from normal hosts, divided by distance to spam hosts. Sites are ﬂagged as spam when spam-
ratio exceeds some threshold. We also tried following paths along in-links instead, but that gave much worse
results.

We use the experimental setup described in Section 6. Results are shown in Figure 4. The alternative convention
for DIRECTEDLEX orients edges in the directions of links. This takes a link from a spam host to be evidence of
spam, and a link to a normal host to be evidence of normality. This approach performs signiﬁcantly worse than our
preferred convention, as one would intuitively expect. UNDIRECTEDLEX and LAPLACIAN approaches also perform
signiﬁcantly worse. DIRECTED 1-NEAREST NEIGHBOR performs poorly, demonstrating that DIRECTEDLEX is very
different from that approach. As observed by Zhou et al. (2007), sampling based on a random walk following out-links
performs worse than following in-links. Up to 60 % recall, DIRECTEDLEX performs best, both in the regime of 5 %
labels for training and in the regime of 20 % labels for training.

22

5 % labels for training

RandWalk along in-links
DirectedLex
RandWalk along out-links
DirectedLex, opposite link direction
UndirectedLex
Laplacian
Directed 1NN

0
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Recall

20 % labels for training

RandWalk along in-links
DirectedLex
RandWalk along out-links
DirectedLex, opposite link direction
UndirectedLex
Laplacian
Directed 1NN

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

i

i

n
o
s
c
e
r
P

i

i

n
o
s
c
e
r
P

0
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Recall

Figure 4: Recall and precision in the WebSpam classiﬁcation experiment. Each data point shown was computed as an average
over 100 runs. The largest standard deviation of the mean precision across the plotted recall values was less than 1.5 %. The
algorithm of Zhou et al. (2007) appears as RANDWALK (along in-links). We also show RANDWALK along out-links. Our directed
lex-minimization algorithm appears as DIRECTEDLEX. We also show DIRECTEDLEX with link directions reversed, along with
UNDIRECTEDLEX and LAPLACIAN.

D l0-Vertex Regularization Proofs

In this appendix, we prove Theorem 7.1 and Theorem 7.2. For the purposes of proving the second theorem, we intro-
duce an alternative version of problem (3). The optimization problem here requires us to minimize l0-regularization

23

budget required to obtain an inf-minimizer with gradient below a given threshold:

min
v∈IRn
subject to

(cid:13)
(cid:13)

v(T ) − v0(T )

0

gradG[v]

(cid:13)
∞ ≤ α.
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We will also need the following graph construction.

Deﬁnition D.1 The α-pressure terminal graph of a partially-labeled graph (G, v0) is a directed unweighted graph
Gα = (T (v0),

E if and only if there is a terminal path P from s to t in G with

E) such that (s, t) ∈

b

b

∇P (v0) > α.

Note that the α-pressure terminal graph has O(n) vertices but may be dense, even when G is not.

Algorithm 13: Algorithm TERM-PRESSURE: Given a well-posed instance (G, v0) and α ≥ 0, outputs α pressure terminal
graph Gα.
Initialize Gα with vertex set Vα = T (v0) and edge set
for each terminal s ∈ T (v0)

E = ∅.

1. Compute the distances to every other terminal t by running Dijktra’s algorithm, allowing shortest paths

b

2. Use the resulting distances to check for every other terminal t if there is a terminal path P from s to t with

that run through other terminals.

∇P (v0) > α. If there is, add edge (s, t) to

E.

Lemma D.2 The α-pressure terminal graph of a voltage problem (G, v0) can be computed in O((m + n log n)n) time
using algorithm TERM-PRESSURE (Algorithm 13).

b

Proof: The correctness of the algorithm follows from the fact that Dijkstra’s algorithm will identify all shortest
distances between the terminals, and the pressure check will ensure that terminal pairs (s, t) are added to
E if and
only if they are the endpoints of a terminal path P with ∇P (v0) > α. The running time is dominated by performing
Dijkstra’s algorithm once for each terminal. A single run of Dijkstra’s algorithm takes O(m + n log n) time, and this
✷
is performed at most n times, for a total running time of O((m + n log n)n).

b

We make three observations that will turn out to be crucial for proving Theorems 7.1 and 7.2.

Observation D.3 Gα is a subgraph of Gβ for α ≥ β.

Proof: Suppose edge (s, t) appears in Gα, then for some path P

∇P (v0) > α ≥ β,

so the edge also appears in Gβ.

Observation D.4 Gα is transitively closed.

Proof: Suppose edges (s, t) and (t, r) appear in Gα. Let P(s,t), P(t,r), P(s,r) be the respective shortest paths in G
between these terminal pairs. Then

∇P(s,r)(v0) =

v0(s) − v0(r)
ℓ(P(s,r))

≥

v0(s) − v0(r)
ℓ(P(s,t)) + ℓ(P(t,r))

=

v0(s) − v0(t) + v0(t) − v0(r)
ℓ(P(s,t)) + ℓ(P(t,r))

≥ min

v0(s) − v0(t)
ℓ(P(s,t))

,

 

v0(t) − v0(r)

ℓ(P(t,r)) !

> α.

So edge (s, r) also appears in Gα. This is sufﬁcient for Gα to be transitively closed.

24

(6)

✷

(7)

✷

Observation D.5 Gα is a directed acyclic graph.

Proof: Suppose for a contradiction that a directed cycle appears in Gα. Let s and t be two vertices in this cycle. Let
P(s,t) and P(t,s) be the respective shortest paths in G between these terminal pairs. Because Gα is transitively closed,
both edges (s, t) and (t, s) must appear in Gα. But (s, t) ∈

E implies

and similarly (t, s) ∈

E implies

b
This is a contradiction.

v0(s) − v0(t) > αℓ(P(s,t)) > 0,

b

v0(t) − v0(s) > αℓ(P(t,s)) > 0.

✷

The usefulness of the α-pressure terminal graph is captured in the following lemma. We deﬁne a vertex cover of a
directed graph to be a vertex set that constitutes a vertex cover in the same graph with all edges taken to be undirected.

Lemma D.6 Given a partially-labeled graph (G, v0) and a set U ⊆ V , there exists a voltage assignment v ∈ IRn that
satisﬁes

if and only if U is a vertex cover in the α-pressure terminal graph Gα of (G, v0).
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:8)

(cid:9)

t ∈ T (v0) : v(t) 6= v0(t)

⊆ U and

gradG[v]

∞ ≤ α,

Proof: We ﬁrst show the “only if” direction. Suppose for a contradiction that there exists a voltage assignment v for
which
∞ ≤ α, but U is not a vertex cover in Gα. Let (s, t) be an edge Gα which is not covered by U . The
presence of this edge in Gα implies that there exists a terminal path P from s to t in G for which

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∇P (v0) > α.

But, by Lemma 3.5 this means there is no assignment v for G which agrees with v0 on s and t and has
α. This contradicts our assumption.

∞ ≤
(cid:13)
Now we show the “if” direction. Consider an arbitrary vertex cover U of Gα. Suppose for a contradiction that
(cid:13)
⊆ U .

t ∈ T (v0) : v(t) 6= v0(t)

gradG[v]

(cid:13)
(cid:13)

gradG[v]

there does not exist a voltage assignment v for G with
Deﬁne a partial voltage assignment vU given by

∞ ≤ α and

(cid:8)

(cid:9)

vU (t) =

v0(t)
∗

(

(cid:13)
(cid:13)

(cid:13)
(cid:13)
if t ∈ T (v0) \ U
o.w.

∞ ≤ α. By
The preceding statement is equivalent to saying that there is no v that extends vU and has
Lemma 3.5, this means there is terminal path between s, t ∈ T (vU ) with gradient strictly larger than α. But this
means an edge (s, t) is present in Gα and is not covered. This contradicts our assumption that U is a vertex cover. ✷

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We are now ready to prove Theorem 7.2.

∞

(cid:13)
(cid:13)

Proof of Theorem 7.2: We describe and prove the algorithm OUTLIER. The algorithm will reduce problem (3)
to problem (6): Suppose v∗ is an optimal assignment for problem (3).
It achieves a maximum gradient α∗ =
gradG[v∗]
. Using Dijkstra’s algorithm we compute the pairwise shortest distances between all terminals in G.
From these distances and the terminal voltages, we compute the gradient on the shortest path between each terminal
(cid:13)
pair. By Lemma 3.5, α∗ must equal one of these gradients. So we can solve problem (3) by iterating over the set of
(cid:13)
gradients between terminals and solving problem (6) for each of these O(n2) gradients. Among the assignments with
v(T ) − v0(T )

0 ≤ k, we then pick the solution that minimizes
(cid:13)
(cid:13)

In fact, we can do better. By Observation D.3, Gα is a subgraph of Gβ for α ≥ β. This means a vertex cover
(cid:13)
of Gα is also a vertex cover of Gβ, and hence the minimum vertex cover for Gβ is at least as large as the minimum
(cid:13)
vertex cover for Gα. This means we can do a binary search on the set of O(n2) terminal gradients to ﬁnd the minimum
gradient for which there exists an assignment with
0 ≤ k. This way, we only make O(log n) calls to
v(T ) − v0(T )
problem (6), in order to solve problem (3).
(cid:13)
(cid:13)

We use the following algorithm to solve problem (6).

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∞

.

25

1. Compute the α-pressure terminal graph Gα of G using the algorithm TERM-PRESSURE.
2. Compute a minimum vertex cover U of Gα using the algorithm KONIG-COVER from Theorem 7.3.
3. Deﬁne a partial voltage assignment vU given by

vU (t) =

v0(t)
∗

(

if t ∈ T (v0) \ U,
otherwise.

4. Using Algorithm 5, compute voltages v that extend vU and output v.

From Lemma D.2, it follows that step 1 computes the α-pressure terminal graph in polynomial time. From The-
orem 7.3 it follows that step 2 computes the a minimum vertex cover of the α-pressure terminal graph in polynomial
time, because our observations D.4 and D.5 establish that the graph is a TC-DAG. From Lemma D.6 and Theorem 4.6,
it follows that the output voltages solve program (6).

✷

To prove Theorem 7.1, we use the standard greedy approximation algorithm for MIN-VC (Vazirani (2001)).

Theorem D.7 2-Approximation Algorithm for Vertex Cover. The following algorithm gives a 2-approximation to
the Minimum Vertex Cover problem on a graph G = (V, E).

0. Initialize U = ∅.
1. Pick an edge (u, v) ∈ E that is not covered by U .
2. Add u and v to the set U .
3. Repeat from step 1 if there are still edges not covered by U .
4. Output U .

We are now in a position to prove Theorem 7.1

Proof of Theorem 7.1: Given an arbitrary k and a partially-labeled graph (G, v0), let α∗ be the optimum value
of program (3). Observe that by Lemma D.6, this implies that Gα∗ has a vertex cover of size k. Given the partial
assignment v0, for every vertex set U , we deﬁne

vU (t) =

v0(t)
∗

(

if t ∈ T (v0) \ U
o.w.

We claim the following algorithm APPROX-OUTLIER outputs a voltage assignment v with

gradG[v]

∞ ≤ α∗

(cid:13)
(cid:13)

(cid:13)
(cid:13)

and

v(T ) − v0(T )

(cid:13)
(cid:13)

Algorithm APPROX-OUTLIER:

0 ≤ 2k.
(cid:13)
(cid:13)

0. Initialize U = ∅.
1. Using the algorithm STEEPESTPATH (Algorithm 7), ﬁnd a steepest terminal path in G w.r.t. vU . Denote
this path P and let s and t be its terminal endpoints. If there is no terminal path with positive gradient, skip
to step 4.

2. Add s and t to the set U .
3. If |U | ≤ 2k − 2 then repeat from step 1.
4. Using the algorithm COMPINFMIN (Algorithm 5), compute voltages v that extend vU and output v.

From the stopping conditions, it is clear that |U | ≤ 2k. If in step 1 we ever ﬁnd that no terminal paths have positive
∞ = 0 ≤ α∗, by Lemma 3.5. Similarly if we ﬁnd a steepest
gradient then our v that extends vU will have
(cid:13)
(cid:13)

gradG[v]

(cid:13)
(cid:13)

26

gradG[v]

∞ ≤ α∗.

∞ ≤ α∗.
path with gradient less than α∗ w.r.t. vU , then for this U there exists v that extends vU and has
This will continue to hold when if we add vertices to U . Therefore, for the ﬁnal U , there will exist an v that extends
vU and has

gradG[v]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

If we never ﬁnd a steepest terminal path P with ∇P (v0) ≤ α∗, then each steepest path we ﬁnd corresponds to an
edge in Gα∗ that is not yet covered by U and our algorithm in fact implements the greedy approximation algorithm
for vertex cover described in Theorem D.7. This implies that the ﬁnal U is a vertex cover of Gα∗ of size at most 2k.
∞ ≤ α∗. This
By Lemma D.6, this implies that there exists a voltage assignment u extending vU that has
implies by Theorem 4.6 that the v we output has
(cid:13)
(cid:13)
In all cases, the v we output extends vU , so

∞ ≤ α∗.

gradG[u]

(cid:13)
(cid:13)

✷

gradG[v]
v(T ) − v0(T )
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

0 ≤ |U | ≤ 2k.
(cid:13)
(cid:13)

E Proof of Hardness of l0 regularization for l2

We will prove Theorem 7.4, by a reduction from minimum bisection. To this end, let G = (V, E) be any graph. We
will reduce the minimum bisection problem on G to our regularization problem. Let n = |V |. The graph on which we
will perform regularization will have vertex set

V ∪

V ,

V is a set of n vertices that are in 1-to-1 correspondence with V . We assume that every edge in G has weight 1.
V to the corresponding vertex in V by an edge of weight B, for some large B to be
V to each other by edges of weight B3. So, we have a complete
V to V , and the original graph G on V .

where
We now connect every vertex in
determined later. We also connect all of the vertices in
graph of weight B3 edges on
b
The input potential function will be

V , a matching of weight B edges connecting

b

b

b

v(a) =

b
0 for a ∈
1 for a ∈ V .
b

(

V , and

b

Now set k = n/2. We claim that we will be able to determine the value of the minimum bisection from the solution
to the regularization problem.

If S is the set of vertices on which v and w differ, then we know that the w is harmonic on S: for every a ∈ S,

w(a) is the weighted average of the values at its neighbors. In the following, we exploit the fact that |S| ≤ n/2.

Claim E.1 For every a ∈ S ∩

V , w(a) ≤ 2/nB2.

Proof: Let a be the vertex in S ∩
w-value equal to 0 by edges of weight B3. On the other hand, a has only one neighbor that is not in
w-value at most 1, and it is connected to that vertex by an edge of weight B. Call that vertex c. We have

V that maximizes w(a). So, a is connected to at least n/2 neighbors in

V with
V , that vertex has

b

b

b

((n − 1)B3 + B)w(a) = Bw(c) +

B3w(b)

b

b
V ,b6=a
Xb∈

= Bw(c) +

B3w(b) +

B3w(b)

b
V ∩S,b6=a
Xb∈

B3w(a)

≤ B +

b
V ∩S,b6=a
Xb∈
≤ B + (n/2 − 1)B3w(a).

b
V −S
Xb∈

Subtracting (n/2 − 1)B3w(a) from both sides gives

((n/2)B3 + B)w(a) ≤ B,

which implies the claim.

Claim E.2 For a ∈ S ∩ V , w(a) ≤ n/B.

27

✷

V . Let’s call that neighbor c. We know that w(c) ≤ 2/B2n. On the
Proof: Vertex a has exactly one neighbor in
other hand, vertex a has fewer than n − 1 neighbors in V , and each of these have w-value at most 1. Let da denote the
degree of a in G. Then,

b

So,

Let

and

bisection.

and at most

(B + da)w(a) ≤ da + B

2
B2n

.

w(a) ≤

da + 2/Bn
da + B
n + (2/Bn)
B + n

≤

≤ n/B.

|S| = k = n/2.

T = S ∩ V,

t = |T | .

(n − t)B − 4/B
b

(n − t)B + tn2/B.

We now estimate the value of the regularized objective function. To this end, we assume that

We will prove that S ⊂ V and so S = T and t = n/2.

Let δ denote the number of edges on the boundary of T in V . Once we know that t = n/2, δ is the size of a

Claim E.3 The contribution of the edges between V and

V to the objective function is at least

Proof: For the lower bound, we just count the edges between vertices in V \ T and
edges, and each of them has weight B. The endpoint in V \ T has w-value 1, and the endpoint in
most 2/nB2. So, the contribution of these edges is at least

V . There are n − t of these
V has w-value at

b

(n − t)B(1 − 2/nB2)2 ≥ (n − t)B(1 − 4/nB2) ≥ (n − t)B − 4/B.

b

For the upper bound, we observe that the difference in w-values across each of these n − t edges is at most 1, so their
total contribution is at most

Since for every vertex a ∈ T , w(a) ≤ n/B, and also every vertex b ∈
edges between T and

V is at most

t(n/B)2B = tn2/B.

b

b

V , w(b) ≤ 2/nB2, the contribution due to

We will see that this is the dominant term in the objective function. The next-most important term comes from the

edges in G.

(n − t)B.

28

✷

✷

Claim E.4 The contribution of the edges in G to the objective function is at least

and at most

δ(1 − 2n/B)

δ + (t2/2)(n/B)2

δ(1 − 2n/B) and δ.

(t2/2)(n/B)2.

Proof: Let (a, b) ∈ E. If neither a nor b is in T , then w(a) = w(b) = 1, and so this edge has no contribution. If
a ∈ T but b 6∈ T , then the difference in w-values on them is between (1 − n/B) and 1. So, the contribution of such
edges to the objective function is between

Finally, if a and b are in T , then the difference in w-values on them is at most n/B, and so the contribution of all such
edges to the objective function is at most

Claim E.5 The edges between pairs of vertices in

V contribute at most 2/B to the objective function.

Proof: As 0 ≤ w(a) ≤ 2/B2n for every a ∈

V , every edge between two vertices in

V can contribute at most

b

As there are fewer than n2/2 such edges, their total contribution to the objective function is at most

B3(2/B2n)2 = 4/Bn2.
b

b

(n2/2)(4/Bn2) = 2/B.

Lemma E.6 If n ≥ 4 and B = 2n3, the value of the objective function is at least

and at most

(n − t)B + δ − 1/2

(n − t)B + δ + 1/3.

Proof: Summing the contributions in the preceding three claims, we see that the value of the objective function is at
least

(n − t)B − 4/B + δ(1 − 2n/B) ≥ (n − t)B + δ − 4/B − 2nδ/B

≥ (n − t)B + δ − n3/B
≥ (n − t)B + δ − 1/2,

as δ ≤ (n/2)2.

Similarly, the objective function is at most

(n − t)B + tn2/B + δ + (t2/2)(n/B)2 + 2/B ≤ (n − t)B + n3/2B + δ + n4/8B2 + 2/B
≤ (n − t)B + n3/2B + δ + 1/32n2 + 1/n3
≤ (n − t)B + δ + 1/3.

Claim E.7 If n ≥ 2 and B = 2n3, then S ⊂ V .

Proof: The objective function is minimized by making t as large as possible, so t = n/2 and S ⊂ V .

29

✷

✷

✷

✷

Theorem E.8 The value of the objective function reveals the value of the minimum bisection in G.

Proof: The value of the objective function will be between

and

(n/2)B + δ − 1/2

(n/2)B + δ + 1/3.

So, the objective function will be smallest when δ is as small as possible.

✷

Theorem E.8 immediately implies Theorem 7.4.

30

