Tree-to-Sequence Attentional Neural Machine Translation

Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa Tsuruoka
The University of Tokyo, 3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan
{eriguchi, hassy, tsuruoka}@logos.t.u-tokyo.ac.jp

6
1
0
2
 
n
u
J
 
8
 
 
]
L
C
.
s
c
[
 
 
3
v
5
7
0
6
0
.
3
0
6
1
:
v
i
X
r
a

Abstract

Most of
the existing Neural Machine
Translation (NMT) models focus on the
conversion of sequential data and do
not directly use syntactic information.
We propose a novel end-to-end syntac-
tic NMT model, extending a sequence-
to-sequence model with the source-side
phrase structure. Our model has an at-
tention mechanism that enables the de-
coder to generate a translated word while
softly aligning it with phrases as well as
words of the source sentence. Experi-
mental results on the WAT’15 English-
to-Japanese dataset demonstrate that our
proposed model considerably outperforms
sequence-to-sequence attentional NMT
models and compares favorably with the
state-of-the-art tree-to-string SMT system.

1

Introduction

Machine Translation (MT) has traditionally been
one of the most complex language processing
problems, but recent advances of Neural Machine
Translation (NMT) make it possible to perform
translation using a simple end-to-end architecture.
In the Encoder-Decoder model (Cho et al., 2014b;
Sutskever et al., 2014), a Recurrent Neural Net-
work (RNN) called the encoder reads the whole
sequence of source words to produce a ﬁxed-
length vector, and then another RNN called the
decoder generates the target words from the vec-
tor. The Encoder-Decoder model has been ex-
tended with an attention mechanism (Bahdanau et
al., 2015; Luong et al., 2015a), which allows the
model to jointly learn the soft alignment between
the source language and the target language. NMT
models have achieved state-of-the-art results in
English-to-French and English-to-German trans-

Figure 1: Alignment between an English phrase
and a Japanese word.

lation tasks (Luong et al., 2015b; Luong et al.,
2015a). However, it is yet to be seen whether
NMT is competitive with traditional Statistical
Machine Translation (SMT) approaches in trans-
lation tasks for structurally distant language pairs
such as English-to-Japanese.

Figure 1 shows a pair of parallel sentences in
English and Japanese. English and Japanese are
linguistically distant in many respects; they have
different syntactic constructions, and words and
In
phrases are deﬁned in different lexical units.
this example, the Japanese word “緑茶” is aligned
with the English words “green” and “tea”, and
the English word sequence “a cup of” is aligned
with a special symbol “null”, which is not explic-
itly translated into any Japanese words. One way
to solve this mismatch problem is to consider the
phrase structure of the English sentence and align
the phrase “a cup of green tea” with “緑茶”. In
SMT, it is known that incorporating syntactic con-
stituents of the source language into the models
improves word alignment (Yamada and Knight,
2001) and translation accuracy (Liu et al., 2006;
Neubig and Duh, 2014). However, the existing
NMT models do not allow us to perform this kind
of alignment.

In this paper, we propose a novel attentional
NMT model to take advantage of syntactic infor-

mation. Following the phrase structure of a source
sentence, we encode the sentence recursively in a
bottom-up fashion to produce a vector representa-
tion of the sentence and decode it while aligning
the input phrases and words with the output. Our
experimental results on the WAT’15 English-to-
Japanese translation task show that our proposed
model achieves state-of-the-art translation accu-
racy.

2 Neural Machine Translation

2.1 Encoder-Decoder Model

NMT is an end-to-end approach to data-driven
machine translation (Kalchbrenner and Blunsom,
2013; Sutskever et al., 2014; Bahdanau et al.,
2015). In other words, the NMT models directly
estimate the conditional probability p(y|x) given
a large collection of source and target sentence
pairs (x, y). An NMT model consists of an en-
coder process and a decoder process, and hence
they are often called Encoder-Decoder models. In
the Encoder-Decoder models, a sentence is treated
In the encoder pro-
as a sequence of words.
cess, the encoder embeds each of the source words
x = (x1, x2, · · · , xn) into a d-dimensional vector
space. The decoder then outputs a word sequence
y = (y1, y2, · · · , ym) in the target language given
the information on the source sentence provided
by the encoder. Here, n and m are the lengths
of the source and target sentences, respectively.
RNNs allow one to effectively embed sequential
data into the vector space.

In the RNN encoder, the i-th hidden unit hi ∈
Rd×1 is calculated given the i-th input xi and the
previous hidden unit hi−1 ∈ Rd×1,

hi = fenc(xi, hi−1),

(1)

where fenc is a non-linear function, and the initial
hidden unit h0 is usually set to zeros. The encod-
ing function fenc is recursively applied until the n-
th hidden unit hn is obtained. The RNN Encoder-
Decoder models assume that hn represents a vec-
tor of the meaning of the input sequence up to the
n-th word.

After encoding the whole input sentence into
the vector space, we decode it in a similar way.
The initial decoder unit s1 is initialized with the
input sentence vector (s1 = hn). Given the pre-
vious target word and the j-th hidden unit of the
decoder, the conditional probability that the j-th

target word is generated is calculated as follows:

p(yj|y<j, x) = g(sj),

(2)

where g is a non-linear function. The j-th hidden
unit of the decoder is calculated by using another
non-linear function fdec as follows:

sj = fdec(yj−1, sj−1).

(3)

We employ Long Short-Term Memory (LSTM)
units (Hochreiter and Schmidhuber, 1997; Gers et
al., 2000) in place of vanilla RNN units. The t-
th LSTM unit consists of several gates and two
different types of states: a hidden unit ht ∈ Rd×1
and a memory cell ct ∈ Rd×1,

it = σ(W (i)xt + U (i)ht−1 + b(i)),
ft = σ(W (f )xt + U (f )ht−1 + b(f )),
ot = σ(W (o)xt + U (o)ht−1 + b(o)),
˜ct = tanh(W (˜c)xt + U (˜c)ht−1 + b(˜c)),
ct = it (cid:12) ˜ct + ft (cid:12) ct−1,
ht = ot (cid:12) tanh(ct),

(4)

where each of it, ft, ot and ˜ct ∈ Rd×1 denotes
an input gate, a forget gate, an output gate, and a
state for updating the memory cell, respectively.
W (·) ∈ Rd×d and U (·) ∈ Rd×d are weight matri-
ces, b(·) ∈ Rd×1 is a bias vector, and xt ∈ Rd×1
is the word embedding of the t-th input word. σ(·)
is the logistic function, and the operator (cid:12) denotes
element-wise multiplication between vectors.

2.2 Attentional Encoder-Decoder Model

The NMT models with an attention mecha-
nism (Bahdanau et al., 2015; Luong et al., 2015a)
have been proposed to softly align each decoder
state with the encoder states. The attention mech-
anism allows the NMT models to explicitly quan-
tify how much each encoder state contributes to
the word prediction at each time step.

In the attentional NMT model in Luong et al.
(2015a), at the j-th step of the decoder process,
the attention score αj(i) between the i-th source
hidden unit hi and the j-th target hidden unit sj is
calculated as follows:

αj(i) =

exp(hi · sj)
k=1 exp(hk · sj)

,

(cid:80)n

(5)

where hi · sj is the inner product of hi and sj,
which is used to directly calculate the similarity
score between hi and sj. The j-th context vector

Figure 2: Attentional Encoder-Decoder model.

dj is calculated as the summation vector weighted
by αj(i):

dj =

αj(i)hi.

(6)

n
(cid:88)

i=1

To incorporate the attention mechanism into the
decoding process, the context vector is used for the
the j-th word prediction by putting an additional
hidden layer ˜sj:

˜sj = tanh(Wd[sj; dj] + bd),

(7)

where [sj; dj] ∈ R2d×1 is the concatenation of sj
and dj, and Wd ∈ Rd×2d and bd ∈ Rd×1 are a
weight matrix and a bias vector, respectively. The
model predicts the j-th word by using the softmax
function:

p(yj|y<j, x) = softmax(Ws ˜sj + bs),

(8)

where Ws ∈ R|V |×d and bs ∈ R|V |×1 are a weight
matrix and a bias vector, respectively. |V | stands
for the size of the vocabulary of the target lan-
guage. Figure 2 shows an example of the NMT
model with the attention mechanism.

2.3 Objective Function of NMT Models

The objective function to train the NMT models
is the sum of the log-likelihoods of the translation
pairs in the training data:

J(θ) =

log p(y|x),

(9)

(cid:88)

1
|D|

(x,y)∈D

where D denotes a set of parallel sentence pairs.
The model parameters θ are learned through
Stochastic Gradient Descent (SGD).

3 Attentional Tree-to-Sequence Model

3.1 Tree-based Encoder + Sequential

Encoder

The exsiting NMT models treat a sentence as a
sequence of words and neglect the structure of

Figure 3: Proposed model: Tree-to-sequence at-
tentional NMT model.

a sentence inherent in language. We propose a
novel tree-based encoder in order to explicitly take
the syntactic structure into consideration in the
NMT model. We focus on the phrase structure of
a sentence and construct a sentence vector from
phrase vectors in a bottom-up fashion. The sen-
tence vector in the tree-based encoder is there-
fore composed of the structural information rather
than the sequential data. Figure 3 shows our pro-
posed model, which we call a tree-to-sequence at-
tentional NMT model.

In Head-driven Phrase Structure Grammar
(HPSG) (Sag et al., 2003), a sentence is composed
of multiple phrase units and represented as a bi-
nary tree as shown in Figure 1. Following the
structure of the sentence, we construct a tree-based
encoder on top of the standard sequential encoder.
The k-th parent hidden unit h(phr)
for the k-th
phrase is calculated using the left and right child
hidden units hl

k as follows:

k and hr

k

h(phr)
k

= ftree(hl

k, hr

k),

(10)

where ftree is a non-linear function.

We construct a tree-based encoder with LSTM
units, where each node in the binary tree is repre-
sented with an LSTM unit. When initializing the
leaf units of the tree-based encoder, we employ the
sequential LSTM units described in Section 2.1.
Each non-leaf node is also represented with an
LSTM unit, and we employ Tree-LSTM (Tai et
al., 2015) to calculate the LSTM unit of the par-
ent node which has two child LSTM units. The
hidden unit h(phr)
∈ Rd×1 and the memory cell
c(phr)
∈ Rd×1 for the k-th parent node are calcu-
k

k

lated as follows:

k + b(fl)),
k + b(fr)),

k + b(i)),

ik = σ(U (i)
l hl
k = σ(U (fl)
l hl
f l
k = σ(U (fr)
l hl
f r
ok = σ(U (o)
l hl
˜ck = tanh(U (˜c)

r hr
k + U (i)
(fl)
r hr
k + U
k + U (fr)
r hr
k + b(o)),
k + U (o)
r hr
r hr
k + U (˜c)
l hl
= ik (cid:12) ˜ck + f l
k + f r
k (cid:12) cl
= ok (cid:12) tanh(c(phr)
),

k + b(˜c)),
k (cid:12) cr
k,

k

c(phr)
k
h(phr)
k

(11)

k, f r

k , oj, ˜cj ∈ Rd×1 are an input
where ik, f l
gate, the forget gates for left and right child units,
an output gate, and a state for updating the mem-
ory cell, respectively. cl
k are the mem-
ory cells for the left and right child units, respec-
tively. U (·) ∈ Rd×d denotes a weight matrix, and
b(·) ∈ Rd×1 represents a bias vector.

k and cr

Our proposed tree-based encoder is a natural
extension of the conventional sequential encoder,
since Tree-LSTM is a generalization of chain-
structured LSTM (Tai et al., 2015). Our encoder
differs from the original Tree-LSTM in the cal-
culation of the LSTM units for the leaf nodes.
The motivation is to construct the phrase nodes in
a context-sensitive way, which, for example, al-
lows the model to compute different representa-
tions for multiple occurrences of the same word in
a sentence because the sequential LSTMs are cal-
culated in the context of the previous units. This
ability contrasts with the original Tree-LSTM, in
which the leaves are composed only of the word
embeddings without any contextual information.

3.2

Initial Decoder Setting

We now have two different sentence vectors: one
is from the sequence encoder and the other from
the tree-based encoder. As shown in Figure 3, we
provide another Tree-LSTM unit which has the ﬁ-
nal sequential encoder unit (hn) and the tree-based
encoder unit (h(phr)
root ) as two child units and set it
as the initial decoder s1 as follows:

where gtree is the same function as ftree with an-
other set of Tree-LSTM parameters. This initial-
ization allows the decoder to capture information
from both the sequential data and phrase struc-
tures. Zoph and Knight (2016) proposed a simi-
lar method using a Tree-LSTM for initializing the

decoder, with which they translate multiple source
languages to one target language. When the syn-
tactic parser fails to output a parse tree for a sen-
tence, we encode the sentence with the sequential
encoder by setting h(phr)
root = 0. Our proposed tree-
based encoder therefore works with any sentences.

3.3 Attention Mechanism in Our Model

We adopt the attention mechanism into our tree-
to-sequence model in a novel way. Our model
gives attention not only to sequential hidden units
but also to phrase hidden units. This attention
mechanism tells us which words or phrases in the
source sentence are important when the model de-
codes a target word. The j-th context vector dj
is composed of the sequential and phrase vectors
weighted by the attention score αj(i):

dj =

αj(i)hi +

αj(i)h(phr)
i

.

(13)

n
(cid:88)

i=1

2n−1
(cid:88)

i=n+1

In addition, we

Note that a binary tree has n − 1 phrase nodes if
the tree has n leaves. We set a ﬁnal decoder ˜sj in
the same way as Equation (7).
adopt

input-feeding
method (Luong et al., 2015a) in our model, which
is a method for feeding ˜sj−1, the previous unit
to predict the word yj−1, into the current target
hidden unit sj,

the

sj = fdec(yj−1, [sj−1; ˜sj−1]),

(14)

where [sj−1; ˜sj−1] is the concatenation of sj−1
and ˜sj−1. The input-feeding approach contributes
to the enrichment in the calculation of the decoder,
because ˜sj−1 is an informative unit which can be
used to predict the output word as well as to be
compacted with attentional context vectors. Lu-
ong et al. (2015a) showed that the input-feeding
approach improves BLEU scores. We also ob-
served the same improvement in our preliminary
experiments.

3.4 Sampling-Based Approximation to the

The biggest computational bottleneck of train-
ing the NMT models is in the calculation of the
softmax layer described in Equation (8), because
its computational cost increases linearly with the
size of the vocabulary. The speedup technique
with GPUs has proven useful for sequence-based
NMT models (Sutskever et al., 2014; Luong et al.,

s1 = gtree(hn, h(phr)

root ),

(12)

NMT Models

2015a) but it is not easily applicable when deal-
ing with tree-structured data. In order to reduce
the training cost of the NMT models at the soft-
max layer, we employ BlackOut (Ji et al., 2016), a
sampling-based approximation method. BlackOut
has been shown to be effective in RNN Language
Models (RNNLMs) and allows a model to run rea-
sonably fast even with a million word vocabulary
with CPUs.

At each word prediction step in the training,
BlackOut estimates the conditional probability in
Equation (2) for the target word and K neg-
ative samples using a weighted softmax func-
tion. The negative samples are drawn from the
unigram distribution raised to the power β ∈
[0, 1] (Mikolov et al., 2013). The unigram dis-
tribution is estimated using the training data and
β is a hyperparameter. BlackOut is closely re-
lated to Noise Contrastive Estimation (NCE) (Gut-
mann and Hyv¨arinen, 2012) and achieves better
perplexity than the original softmax and NCE in
RNNLMs. The advantages of Blackout over the
other methods are discussed in Ji et al. (2016).
Note that BlackOut can be used as the original
softmax once the training is ﬁnished.

4 Experiments

4.1 Training Data

We applied the proposed model to the English-to-
Japanese translation dataset of the ASPEC corpus
given in WAT’15.1 Following Zhu (2015), we ex-
tracted the ﬁrst 1.5 million translation pairs from
the training data. To obtain the phrase structures
of the source sentences, i.e., English, we used the
probabilistic HPSG parser Enju (Miyao and Tsu-
jii, 2008). We used Enju only to obtain a binary
phrase structure for each sentence and did not use
any HPSG speciﬁc information. For the target
language, i.e., Japanese, we used KyTea (Neubig
et al., 2011), a Japanese segmentation tool, and
performed the pre-processing steps recommended
in WAT’15.2 We then ﬁltered out the translation
pairs whose sentence lengths are longer than 50
and whose source sentences are not parsed suc-
cessfully. Table 1 shows the details of the datasets
used in our experiments. We carried out two ex-
periments on a small training dataset to investigate

1http://orchid.kuee.kyoto-u.ac.jp/WAT/

WAT2015/index.html

2http://orchid.kuee.kyoto-u.ac.jp/WAT/

WAT2015/baseline/dataPreparationJE.html

Train
Development
Test

Sentences Parsed successfully
1,346,946
1,346,946
1,789
1,790
1,811
1,812

Table 1: Dataset in ASPEC corpus.

sentence pairs
|V | in English
|V | in Japanese

Train (small) Train (large)
1,346,946
87,796
65,680

100,000
25,478
23,532

Table 2: Training dataset and the vocabulary sizes.

the effectiveness of our proposed model and on
a large training dataset to compare our proposed
methods with the other systems.

The vocabulary consists of words observed in
the training data more than or equal to N times.
We set N = 2 for the small training dataset and
N = 5 for the large training dataset. The out-of-
vocabulary words are mapped to the special token
“unk”. We added another special symbol “eos” for
both languages and inserted it at the end of all the
sentences. Table 2 shows the details of each train-
ing dataset and its corresponding vocabulary size.

4.2 Training Details

The biases,
softmax weights, and BlackOut
weights are initialized with zeros. The hyperpa-
rameter β of BlackOut is set to 0.4 as recom-
mended by Ji et al. (2016). Following J´ozefowicz
et al. (2015), we initialize the forget gate biases of
LSTM and Tree-LSTM with 1.0. The remaining
model parameters in the NMT models in our ex-
periments are uniformly initialized in [−0.1, 0.1].
The model parameters are optimized by plain SGD
with the mini-batch size of 128. The initial learn-
ing rate of SGD is 1.0. We halve the learning rate
when the development loss becomes worse. Gra-
dient norms are clipped to 3.0 to avoid exploding
gradient problems (Pascanu et al., 2012).

Small Training Dataset We conduct experi-
ments with our proposed model and the sequential
attentional NMT model with the input-feeding ap-
proach. Each model has 256-dimensional hidden
units and word embeddings. The number of nega-
tive samples K of BlackOut is set to 500 or 2000.

Large Training Dataset Our proposed model
has 512-dimensional word embeddings and d-
dimensional hidden units (d ∈ {512, 768, 1024}).
K is set to 2500.

Our code3 is implemented in C++ using the
Eigen library,4 a template library for linear alge-
bra, and we run all of the experiments on multi-
core CPUs.5 It takes about a week to train a model
on the large training dataset with d = 512.

4.3 Decoding process

We use beam search to decode a target sentence
for an input sentence x and calculate the sum
of the log-likelihoods of the target sentence y =
(y1, · · · , ym) as the beam score:

score(x, y) =

log p(yj|y<j, x).

(15)

m
(cid:88)

j=1

Decoding in the NMT models is a generative pro-
cess and depends on the target language model
given a source sentence. The score becomes
smaller as the target sentence becomes longer, and
thus the simple beam search does not work well
when decoding a long sentence (Cho et al., 2014a;
Pouget-Abadie et al., 2014).
In our preliminary
experiments, the beam search with the length nor-
malization in Cho et al. (2014a) was not effective
in English-to-Japanese translation. The method
in Pouget-Abadie et al. (2014) needs to estimate
the conditional probability p(x|y) using another
NMT model and thus is not suitable for our work.
In this paper, we use statistics on sentence
lengths in beam search. Assuming that the length
of a target sentence correlates with the length of
a source sentence, we redeﬁne the score of each
candidate as follows:

m
(cid:88)

j=1

score(x, y) = Lx,y +

log p(yj|y<j, x),(16)

Lx,y = log p(len(y)|len(x)),

(17)

where Lx,y is the penalty for the conditional prob-
ability of the target sentence length len(y) given
the source sentence length len(x).
It allows
the model to decode a sentence by considering
the length of the target sentence.
In our exper-
iments, we computed the conditional probability

3https://github.com/tempra28/tree2seq
4http://eigen.tuxfamily.org/index.php
516 threads on Intel(R) Xeon(R) CPU E5-2667 v3 @

3.20GHz

p(len(y)|len(x)) in advance following the statis-
tics collected in the ﬁrst one million pairs of the
training dataset. We allow the decoder to generate
up to 100 words.

4.4 Evaluation

We evaluated the models by two automatic eval-
uation metrics, RIBES (Isozaki et al., 2010) and
BLEU (Papineni et al., 2002) following WAT’15.
We used the KyTea-based evaluation script for the
translation results.6 The RIBES score is a metric
based on rank correlation coefﬁcients with word
precision, and the BLEU score is based on n-gram
word precision and a Brevity Penalty (BP) for out-
puts shorter than the references. RIBES is known
to have stronger correlation with human judge-
ments than BLEU in translation between English
and Japanese as discussed in Isozaki et al. (2010).

5 Results and Discussion

5.1 Small Training Dataset

Table 3 shows the perplexity, BLEU, RIBES, and
the training time on the development data with the
Attentional NMT (ANMT) models trained on the
small dataset. We conducted the experiments with
our proposed method using BlackOut and soft-
max. We decoded a translation by our proposed
beam search with a beam size of 20.

As shown in Table 3, the results of our proposed
model with BlackOut improve as the number of
negative samples K increases. Although the result
of softmax is better than those of BlackOut (K =
500, 2000), the training time of softmax per epoch
is about three times longer than that of BlackOut
even with the small dataset.

As to the results of the ANMT model, reversing
the word order in the input sentence decreases the
scores in English-to-Japanese translation, which
contrasts with the results of other language pairs
reported in previous work (Sutskever et al., 2014;
Luong et al., 2015a). By taking syntactic infor-
mation into consideration, our proposed model
improves the scores, compared to the sequential
attention-based approach.

We found that better perplexity does not always
lead to better translation scores with BlackOut as
shown in Table 3. One of the possible reasons is
that BlackOut distorts the target word distribution

6http://lotus.kuee.kyoto-u.ac.jp/WAT/
evaluation/automatic_evaluation_systems/
automaticEvaluationJA.html

Proposed model
Proposed model
Proposed model (Softmax)
ANMT (Luong et al., 2015a)
+ reverse input
ANMT (Luong et al., 2015a)
+ reverse input

K
500
2000
—
500
500
2000
2000

Perplexity RIBES BLEU Time/epoch (min.)
20.0
20.5
21.8
18.5
17.7
19.4
17.5

19.6
21.0
17.9
21.6
22.6
23.1
26.1

71.8
72.6
73.2
70.7
69.8
71.5
69.5

55
70
180
45
45
60
60

Table 3: Evaluation results on the development data using the small training data. The training time per
epoch is also shown, and K is the number of negative samples in BlackOut.

Simple BS

Proposed BS

Beam size RIBES BLEU (BP)
20.0 (90.1)
72.3
19.5 (85.1)
72.3
20.5 (91.7)
72.6

6
20
20

Without sequential LSTMs
With sequential LSTMs

RIBES BLEU
19.5
20.0

69.4
72.3

Table 4: Effects of the Beam Search (BS) on the
development data.

Table 5: Effects of the sequential LSTMs in our
proposed tree-based encoder on the development
data.

by the modiﬁed unigram-based negative sampling
where frequent words can be treated as the nega-
tive samples multiple times at each training step.

Effects of the proposed beam search Table 4
shows the results on the development data of pro-
posed method with BlackOut (K = 2000) by
the simple beam search and our proposed beam
search. The beam size is set to 6 or 20 in the sim-
ple beam search, and to 20 in our proposed search.
We can see that our proposed search outperforms
the simple beam search in both scores. Unlike
RIBES, the BLEU score is sensitive to the beam
size and becomes lower as the beam size increases.
We found that the BP had a relatively large im-
pact on the BLEU score in the simple beam search
as the beam size increased. Our search method
works better than the simple beam search by keep-
ing long sentences in the candidates with a large
beam size.

Effects of the sequential LSTM units We also
investigated the effects of the sequential LSTMs
at the leaf nodes in our proposed tree-based en-
coder. Table 5 shows the result on the develop-
ment data of our proposed encoder and that of an
attentional tree-based encoder without sequential
LSTMs with BlackOut (K = 2000).7 The results
show that our proposed encoder considerably out-

7For this evaluation, we used the 1,789 sentences that
were successfully parsed by Enju because the encoder with-
out sequential LSTMs always requires a parse tree.

performs the encoder without sequential LSTMs,
suggesting that the sequential LSTMs at the leaf
nodes contribute to the context-aware construction
of the phrase representations in the tree.

5.2 Large Training Dataset

Table 6 shows the experimental results of RIBES
and BLEU scores achieved by the trained models
on the large dataset. We decoded the target sen-
tences by our proposed beam search with the beam
size of 20.8 The results of the other systems are the
ones reported in Nakazawa et al. (2015).

All of our proposed models show similar per-
formance regardless of the value of d. Our ensem-
ble model is composed of the three models with
d = 512, 768, and 1024, and it shows the best
RIBES score among all systems.9

As for the time required for training, our im-
plementation needs about one day to perform one
epoch on the large training dataset with d = 512.
It would take about 11 days without using the
BlackOut sampling.

Comparison with the NMT models The model
of Zhu (2015) is an ANMT model (Bahdanau et
al., 2015) with a bi-directional LSTM encoder,
and uses 1024-dimensional hidden units and 1000-

8We found two sentences which ends without eos with
d = 512, and then we decoded it again with the beam size of
1000 following Zhu (2015).

9Our ensemble model yields a METEOR (Denkowski and

Lavie, 2014) score of 53.6 with language option “-l other”.

Model
Proposed model (d = 512)
Proposed model (d = 768)
Proposed model (d = 1024)
Ensemble of the above three models
ANMT with LSTMs (Zhu, 2015)
+ Ensemble, unk replacement
+ System combination,

3 pre-reordered ensembles

ANMT with GRUs (Lee et al., 2015)
+ character-based decoding,

Begin/Inside representation

RIBES BLEU
34.36
81.46
34.78
81.89
34.87
81.58
82.45
36.95
32.19
79.70
34.19
80.27

80.91

36.21

81.15

35.75

PB baseline
HPB baseline
T2S baseline
T2S model (Neubig and Duh, 2014)
+ ANMT Rerank (Neubig et al., 2015)

69.19
74.70
75.80
79.65
81.38

29.80
32.56
33.44
36.58
38.17

Table 6: Evaluation results on the test data.

dimensional word embeddings. The model of Lee
et al. (2015) is also an ANMT model with a bi-
directional Gated Recurrent Unit (GRU) encoder,
and uses 1000-dimensional hidden units and 200-
dimensional word embeddings. Both models are
sequential ANMT models. Our single proposed
model with d = 512 outperforms the best result of
Zhu (2015)’s end-to-end NMT model with ensem-
ble and unknown replacement by +1.19 RIBES
and by +0.17 BLEU scores. Our ensemble model
in both RIBES and
shows better performance,
BLEU scores, than that of Zhu (2015)’s best sys-
tem which is a hybrid of the ANMT and SMT
models by +1.54 RIBES and by +0.74 BLEU
scores and Lee et al. (2015)’s ANMT system
with special character-based decoding by +1.30
RIBES and +1.20 BLEU scores.

Comparison with the SMT models PB, HPB
and T2S are the baseline SMT systems in
WAT’15: a phrase-based model, a hierarchical
phrase-based model, and a tree-to-string model,
respectively (Nakazawa et al., 2015). The best
model in WAT’15 is Neubig et al. (2015)’s tree-
to-string SMT model enhanced with reranking by
ANMT using a bi-directional LSTM encoder. Our
proposed end-to-end NMT model compares favor-
ably with Neubig et al. (2015).

5.3 Qualitative Analysis

We illustrate the translations of test data by our
model with d = 512 and several attentional rela-
tions when decoding a sentence. In Figures 4 and
5, an English sentence represented as a binary tree
is translated into Japanese, and several attentional
relations between English words or phrases and

Figure 4: Translation example of a short sen-
tence and the attentional relations by our proposed
model.

Japanese word are shown with the highest atten-
tion score α. The additional attentional relations
are also illustrated for comparison. We can see the
target words softly aligned with source words and
phrases.

In Figure 4, the Japanese word “液晶” means
“liquid crystal”, and it has a high attention score
(α = 0.41) with the English phrase “liquid crystal
for active matrix”. This is because the j-th tar-
get hidden unit sj has the contextual information
about the previous words y<j including “活性 マ
トリックス の” (“for active matrix” in English).
The Japanese word “セル” is softly aligned with
the phrase “the cells” with the highest attention
score (α = 0.35). In Japanese, there is no deﬁ-
nite article like “the” in English, and it is usually
aligned with null described as Section 1.

In Figure 5, in the case of the Japanese word
“示” (“showed” in English), the attention score
with the English phrase “showed excellent perfor-
mance” (α = 0.25) is higher than that with the
English word “showed” (α = 0.01). The Japanese
word “の” (“of” in English) is softly aligned with
the phrase “of Si dot MOS capacitor” with the
highest attention score (α = 0.30). It is because
our attention mechanism takes each previous con-
text of the Japanese phrases “優れ た 性能” (“ex-
cellent performance” in English) and “Ｓｉ ドッ
ト ＭＯＳ コンデンサ” (“Si dot MOS capaci-
tor” in English) into account and softly aligned the
target words with the whole phrase when trans-
lating the English verb “showed” and the prepo-
sition “of”. Our proposed model can thus ﬂexibly
learn the attentional relations between English and
Japanese.

We observed that our model translated the word
“active” into “活性”, a synonym of the reference
word “アクティブ”. We also found similar ex-

Figure 5: Translation example of a long sentence and the attentional relations by our proposed model.

amples in other sentences, where our model out-
puts synonyms of the reference words, e.g. “女”
and “女 性” (“female” in English) and “NASA”
and “航 空 宇 宙 局” (“National Aeronautics and
Space Administration” in English). These trans-
lations are penalized in terms of BLEU scores, but
they do not necessarily mean that the translations
were wrong. This point may be supported by the
fact that the NMT models were highly evaluated
in WAT’15 by crowd sourcing (Nakazawa et al.,
2015).

enables the NMT models to translate while align-
ing the target with the source. Luong et al. (2015a)
reﬁned the attention model so that it can dynami-
cally focus on local windows rather than the entire
sentence. They also proposed a more effective at-
tentional path in the calculation of ANMT models.
Subsequently, several ANMT models have been
proposed (Cheng et al., 2016; Cohn et al., 2016);
however, each model is based on the existing se-
quential attentional models and does not focus on
a syntactic structure of languages.

6 Related Work

7 Conclusion

Kalchbrenner and Blunsom (2013) were the ﬁrst
to propose an end-to-end NMT model using Con-
volutional Neural Networks (CNNs) as the source
encoder and using RNNs as the target decoder.
The Encoder-Decoder model can be seen as an ex-
tension of their model, and it replaces the CNNs
with RNNs using GRUs (Cho et al., 2014b) or
LSTMs (Sutskever et al., 2014).

Sutskever et al. (2014) have shown that mak-
ing the input sequences reversed is effective in a
French-to-English translation task, and the tech-
nique has also proven effective in translation tasks
between other European language pairs (Luong et
al., 2015a). All of the NMT models mentioned
above are based on sequential encoders. To incor-
porate structural information into the NMT mod-
els, Cho et al. (2014a) proposed to jointly learn
structures inherent in source-side languages but
did not report improvement of translation perfor-
mance. These studies motivated us to investigate
the role of syntactic structures explicitly given by
existing syntactic parsers in the NMT models.

The attention mechanism (Bahdanau et al.,
2015) has promoted NMT onto the next stage. It

In this paper, we propose a novel syntactic ap-
proach that extends attentional NMT models. We
focus on the phrase structure of the input sen-
tence and build a tree-based encoder following
the parsed tree. Our proposed tree-based encoder
is a natural extension of the sequential encoder
model, where the leaf units of the tree-LSTM
in the encoder can work together with the origi-
nal sequential LSTM encoder. Moreover, the at-
tention mechanism allows the tree-based encoder
to align not only the input words but also input
phrases with the output words. Experimental re-
sults on the WAT’15 English-to-Japanese transla-
tion dataset demonstrate that our proposed model
achieves the best RIBES score and outperforms
the sequential attentional NMT model.

Acknowledgments

We thank the anonymous reviewers for their con-
structive comments and suggestions. This work
was supported by CREST, JST, and JSPS KAK-
ENHI Grant Number 15J12597.

References

[Bahdanau et al.2015] Dzmitry Bahdanau, Kyunghyun
Cho, and Yoshua Bengio. 2015. Neural Machine
Translation by Jointly Learning to Align and Trans-
late. In Proceedings of the 3rd International Con-
ference on Learning Representations.

[Cheng et al.2016] Yong Cheng, Shiqi Shen, Zhongjun
He, Wei He, Hua Wu, Maosong Sun, and Yang Liu.
2016. Agreement-based Joint Training for Bidirec-
tional Attention-based Neural Machine Translation.
In Proceedings of the 25th International Joint Con-
ference on Artiﬁcial Intelligence. to appear.

[Cho et al.2014a] KyungHyun Cho, Bart van Merrien-
boer, Dzmitry Bahdanau, and Yoshua Bengio.
2014a. On the Properties of Neural Machine Trans-
lation: Encoder-Decoder Approaches. In Proceed-
ings of Eighth Workshop on Syntax, Semantics and
Structure in Statistical Translation (SSST-8).

[Cho et al.2014b] Kyunghyun Cho, Bart van Merrien-
boer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi
Bougares, Holger Schwenk, and Yoshua Bengio.
2014b. Learning Phrase Representations using RNN
Encoder–Decoder for Statistical Machine Transla-
In Proceedings of the 2014 Conference on
tion.
Empirical Methods in Natural Language Process-
ing, pages 1724–1734.

[Cohn et al.2016] Trevor Cohn, Cong Duy Vu Hoang,
Ekaterina Vymolova, Kaisheng Yao, Chris Dyer,
and Gholamreza Haffari.
Incorporating
Structural Alignment Biases into an Attentional
In Proceedings of the
Neural Translation Model.
15th Annual Conference of
the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies. to appear.

2016.

[Denkowski and Lavie2014] Michael Denkowski and
Alon Lavie. 2014. Meteor universal: Language spe-
ciﬁc translation evaluation for any target language.
In Proceedings of the 14th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics 2014 Workshop on Statistical Machine
Translation.

[Gers et al.2000] Felix A. Gers, J¨urgen Schmidhuber,
and Fred A. Cummins. 2000. Learning to Forget:
Continual Prediction with LSTM. Neural Computa-
tion, 12(10):2451–2471.

[Gutmann and Hyv¨arinen2012] Michael U. Gutmann
and Aapo Hyv¨arinen. 2012. Noise-contrastive esti-
mation of unnormalized statistical models, with ap-
plications to natural image statistics. Journal of Ma-
chine Learning Research, 13(1):307–361.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter
and J¨urgen Schmidhuber. 1997. Long Short-Term
Memory. Neural Computation, 9(8):1735–1780.

[Isozaki et al.2010] Hideki

Isozaki, Tsutomu Hirao,
Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada.
2010. Automatic Evaluation of Translation Quality

In Proceedings of the
for Distant Language Pairs.
2010 Conference on Empirical Methods in Natural
Language Processing, pages 944–952.

[Ji et al.2016] Shihao Ji, S. V. N. Vishwanathan, Na-
dathur Satish, Michael J. Anderson, and Pradeep
Dubey. 2016. BlackOut: Speeding up Recurrent
Neural Network Language Models With Very Large
In Proceedings of the 4th Interna-
Vocabularies.
tional Conference on Learning Representations.

[J´ozefowicz et al.2015] Rafal

J´ozefowicz, Wojciech
Zaremba, and Ilya Sutskever. 2015. An Empiri-
cal Exploration of Recurrent Network Architectures.
In Proceedings of the 32nd International Confer-
ence on Machine Learning, volume 37, pages 2342–
2350.

[Kalchbrenner and Blunsom2013] Nal Kalchbrenner
and Phil Blunsom. 2013. Recurrent continuous
In Proceedings of the 2013
translation models.
Conference on Empirical Methods
in Natural
Language Processing, pages 1700–1709.

[Lee et al.2015] Hyoung-Gyu Lee, JaeSong Lee, Jun-
Seok Kim, and Chang-Ki Lee. 2015. NAVER Ma-
In Pro-
chine Translation System for WAT 2015.
ceedings of the 2nd Workshop on Asian Translation
(WAT2015), pages 69–73.

[Liu et al.2006] Yang Liu, Qun Liu, and Shouxun Lin.
2006. Tree-to-string alignment template for statisti-
cal machine translation. In Proceedings of the 21st
International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 609–616.

[Luong et al.2015a] Thang Luong, Hieu Pham, and
2015a. Effective Ap-
Christopher D. Manning.
proaches to Attention-based Neural Machine Trans-
In Proceedings of the 2015 Conference on
lation.
Empirical Methods in Natural Language Process-
ing, pages 1412–1421.

[Luong et al.2015b] Thang Luong,

Ilya Sutskever,
Quoc Le, Oriol Vinyals, and Wojciech Zaremba.
2015b. Addressing the Rare Word Problem in Neu-
ral Machine Translation. In Proceedings of the 53rd
Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 11–19.

[Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases
In Advances in Neu-
and their compositionality.
ral Information Processing Systems 26, pages 3111–
3119.

[Miyao and Tsujii2008] Yusuke Miyao and Jun’ichi
Tsujii. 2008. Feature Forest Models for Proba-
bilistic HPSG Parsing. Computational Linguistics,
34(1):35–80.

Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 1556–1566.

[Yamada and Knight2001] Kenji Yamada and Kevin
Knight. 2001. A syntax-based statistical translation
model. In Proceedings of the 39th Annual Meeting
on Association for Computational Linguistics, pages
523–530.

[Zhu2015] Zhongyuan Zhu. 2015. Evaluating Neural
In
Machine Translation in English-Japanese Task.
Proceedings of the 2nd Workshop on Asian Transla-
tion (WAT2015), pages 61–68.

[Zoph and Knight2016] Barret Zoph and Kevin Knight.
In Pro-
2016. Multi-Source Neural Translation.
ceedings of the 15th Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
to appear.

[Nakazawa et al.2015] Toshiaki Nakazawa, Hideya
Mino, Isao Goto, Graham Neubig, Sadao Kuro-
hashi, and Eiichiro Sumita. 2015. Overview of
In Pro-
the 2nd Workshop on Asian Translation.
ceedings of the 2nd Workshop on Asian Translation
(WAT2015), pages 1–28.

[Neubig and Duh2014] Graham Neubig and Kevin
Duh. 2014. On the elements of an accurate tree-
In Proceed-
to-string machine translation system.
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 143–149.

[Neubig et al.2011] Graham Neubig, Yosuke Nakata,
and Shinsuke Mori. 2011. Pointwise Prediction for
Robust, Adaptable Japanese Morphological Analy-
sis. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 529–533.

[Neubig et al.2015] Graham Neubig, Makoto Mor-
ishita, and Satoshi Nakamura.
2015. Neural
Reranking Improves Subjective Quality of Machine
Translation: NAIST at WAT2015. In Proceedings of
the 2nd Workshop on Asian Translation (WAT2015),
pages 35–41.

[Papineni et al.2002] Kishore Papineni, Salim Roukos,
2002. BLEU:
Todd Ward, and Wei-Jing Zhu.
A Method for Automatic Evaluation of Machine
In Proceedings of the 40th Annual
Translation.
Meeting on Association for Computational Linguis-
tics, pages 311–318.

[Pascanu et al.2012] Razvan Pascanu, Tomas Mikolov,
and Yoshua Bengio. 2012. Understanding the ex-
ploding gradient problem. arXiv: 1211.5063.

[Pouget-Abadie et al.2014] Jean
Bart

Pouget-Abadie,
van Merrienboer,
Dzmitry Bahdanau,
Kyunghyun Cho, and Yoshua Bengio.
2014.
Overcoming the curse of sentence length for neural
machine translation using automatic segmentation.
In Proceedings of SSST-8, Eighth Workshop on
Syntax, Semantics and Structure in Statistical
Translation, pages 78–85.

[Sag et al.2003] Ivan A. Sag, Thomas Wasow, and
Emily Bender. 2003. Syntactic Theory: A Formal
Introduction. Center for the Study of Language and
Information, Stanford, 2nd edition.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals,
2014. Sequence to Sequence
and Quoc V. Le.
In Advances in
Learning with Neural Networks.
Neural Information Processing Systems 27, pages
3104–3112.

[Tai et al.2015] Kai Sheng Tai, Richard Socher, and
Christopher D. Manning. 2015. Improved Semantic
Representations From Tree-Structured Long Short-
In Proceedings of the
Term Memory Networks.
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint

Tree-to-Sequence Attentional Neural Machine Translation

Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa Tsuruoka
The University of Tokyo, 3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan
{eriguchi, hassy, tsuruoka}@logos.t.u-tokyo.ac.jp

6
1
0
2
 
n
u
J
 
8
 
 
]
L
C
.
s
c
[
 
 
3
v
5
7
0
6
0
.
3
0
6
1
:
v
i
X
r
a

Abstract

Most of
the existing Neural Machine
Translation (NMT) models focus on the
conversion of sequential data and do
not directly use syntactic information.
We propose a novel end-to-end syntac-
tic NMT model, extending a sequence-
to-sequence model with the source-side
phrase structure. Our model has an at-
tention mechanism that enables the de-
coder to generate a translated word while
softly aligning it with phrases as well as
words of the source sentence. Experi-
mental results on the WAT’15 English-
to-Japanese dataset demonstrate that our
proposed model considerably outperforms
sequence-to-sequence attentional NMT
models and compares favorably with the
state-of-the-art tree-to-string SMT system.

1

Introduction

Machine Translation (MT) has traditionally been
one of the most complex language processing
problems, but recent advances of Neural Machine
Translation (NMT) make it possible to perform
translation using a simple end-to-end architecture.
In the Encoder-Decoder model (Cho et al., 2014b;
Sutskever et al., 2014), a Recurrent Neural Net-
work (RNN) called the encoder reads the whole
sequence of source words to produce a ﬁxed-
length vector, and then another RNN called the
decoder generates the target words from the vec-
tor. The Encoder-Decoder model has been ex-
tended with an attention mechanism (Bahdanau et
al., 2015; Luong et al., 2015a), which allows the
model to jointly learn the soft alignment between
the source language and the target language. NMT
models have achieved state-of-the-art results in
English-to-French and English-to-German trans-

Figure 1: Alignment between an English phrase
and a Japanese word.

lation tasks (Luong et al., 2015b; Luong et al.,
2015a). However, it is yet to be seen whether
NMT is competitive with traditional Statistical
Machine Translation (SMT) approaches in trans-
lation tasks for structurally distant language pairs
such as English-to-Japanese.

Figure 1 shows a pair of parallel sentences in
English and Japanese. English and Japanese are
linguistically distant in many respects; they have
different syntactic constructions, and words and
In
phrases are deﬁned in different lexical units.
this example, the Japanese word “緑茶” is aligned
with the English words “green” and “tea”, and
the English word sequence “a cup of” is aligned
with a special symbol “null”, which is not explic-
itly translated into any Japanese words. One way
to solve this mismatch problem is to consider the
phrase structure of the English sentence and align
the phrase “a cup of green tea” with “緑茶”. In
SMT, it is known that incorporating syntactic con-
stituents of the source language into the models
improves word alignment (Yamada and Knight,
2001) and translation accuracy (Liu et al., 2006;
Neubig and Duh, 2014). However, the existing
NMT models do not allow us to perform this kind
of alignment.

In this paper, we propose a novel attentional
NMT model to take advantage of syntactic infor-

mation. Following the phrase structure of a source
sentence, we encode the sentence recursively in a
bottom-up fashion to produce a vector representa-
tion of the sentence and decode it while aligning
the input phrases and words with the output. Our
experimental results on the WAT’15 English-to-
Japanese translation task show that our proposed
model achieves state-of-the-art translation accu-
racy.

2 Neural Machine Translation

2.1 Encoder-Decoder Model

NMT is an end-to-end approach to data-driven
machine translation (Kalchbrenner and Blunsom,
2013; Sutskever et al., 2014; Bahdanau et al.,
2015). In other words, the NMT models directly
estimate the conditional probability p(y|x) given
a large collection of source and target sentence
pairs (x, y). An NMT model consists of an en-
coder process and a decoder process, and hence
they are often called Encoder-Decoder models. In
the Encoder-Decoder models, a sentence is treated
In the encoder pro-
as a sequence of words.
cess, the encoder embeds each of the source words
x = (x1, x2, · · · , xn) into a d-dimensional vector
space. The decoder then outputs a word sequence
y = (y1, y2, · · · , ym) in the target language given
the information on the source sentence provided
by the encoder. Here, n and m are the lengths
of the source and target sentences, respectively.
RNNs allow one to effectively embed sequential
data into the vector space.

In the RNN encoder, the i-th hidden unit hi ∈
Rd×1 is calculated given the i-th input xi and the
previous hidden unit hi−1 ∈ Rd×1,

hi = fenc(xi, hi−1),

(1)

where fenc is a non-linear function, and the initial
hidden unit h0 is usually set to zeros. The encod-
ing function fenc is recursively applied until the n-
th hidden unit hn is obtained. The RNN Encoder-
Decoder models assume that hn represents a vec-
tor of the meaning of the input sequence up to the
n-th word.

After encoding the whole input sentence into
the vector space, we decode it in a similar way.
The initial decoder unit s1 is initialized with the
input sentence vector (s1 = hn). Given the pre-
vious target word and the j-th hidden unit of the
decoder, the conditional probability that the j-th

target word is generated is calculated as follows:

p(yj|y<j, x) = g(sj),

(2)

where g is a non-linear function. The j-th hidden
unit of the decoder is calculated by using another
non-linear function fdec as follows:

sj = fdec(yj−1, sj−1).

(3)

We employ Long Short-Term Memory (LSTM)
units (Hochreiter and Schmidhuber, 1997; Gers et
al., 2000) in place of vanilla RNN units. The t-
th LSTM unit consists of several gates and two
different types of states: a hidden unit ht ∈ Rd×1
and a memory cell ct ∈ Rd×1,

it = σ(W (i)xt + U (i)ht−1 + b(i)),
ft = σ(W (f )xt + U (f )ht−1 + b(f )),
ot = σ(W (o)xt + U (o)ht−1 + b(o)),
˜ct = tanh(W (˜c)xt + U (˜c)ht−1 + b(˜c)),
ct = it (cid:12) ˜ct + ft (cid:12) ct−1,
ht = ot (cid:12) tanh(ct),

(4)

where each of it, ft, ot and ˜ct ∈ Rd×1 denotes
an input gate, a forget gate, an output gate, and a
state for updating the memory cell, respectively.
W (·) ∈ Rd×d and U (·) ∈ Rd×d are weight matri-
ces, b(·) ∈ Rd×1 is a bias vector, and xt ∈ Rd×1
is the word embedding of the t-th input word. σ(·)
is the logistic function, and the operator (cid:12) denotes
element-wise multiplication between vectors.

2.2 Attentional Encoder-Decoder Model

The NMT models with an attention mecha-
nism (Bahdanau et al., 2015; Luong et al., 2015a)
have been proposed to softly align each decoder
state with the encoder states. The attention mech-
anism allows the NMT models to explicitly quan-
tify how much each encoder state contributes to
the word prediction at each time step.

In the attentional NMT model in Luong et al.
(2015a), at the j-th step of the decoder process,
the attention score αj(i) between the i-th source
hidden unit hi and the j-th target hidden unit sj is
calculated as follows:

αj(i) =

exp(hi · sj)
k=1 exp(hk · sj)

,

(cid:80)n

(5)

where hi · sj is the inner product of hi and sj,
which is used to directly calculate the similarity
score between hi and sj. The j-th context vector

Figure 2: Attentional Encoder-Decoder model.

dj is calculated as the summation vector weighted
by αj(i):

dj =

αj(i)hi.

(6)

n
(cid:88)

i=1

To incorporate the attention mechanism into the
decoding process, the context vector is used for the
the j-th word prediction by putting an additional
hidden layer ˜sj:

˜sj = tanh(Wd[sj; dj] + bd),

(7)

where [sj; dj] ∈ R2d×1 is the concatenation of sj
and dj, and Wd ∈ Rd×2d and bd ∈ Rd×1 are a
weight matrix and a bias vector, respectively. The
model predicts the j-th word by using the softmax
function:

p(yj|y<j, x) = softmax(Ws ˜sj + bs),

(8)

where Ws ∈ R|V |×d and bs ∈ R|V |×1 are a weight
matrix and a bias vector, respectively. |V | stands
for the size of the vocabulary of the target lan-
guage. Figure 2 shows an example of the NMT
model with the attention mechanism.

2.3 Objective Function of NMT Models

The objective function to train the NMT models
is the sum of the log-likelihoods of the translation
pairs in the training data:

J(θ) =

log p(y|x),

(9)

(cid:88)

1
|D|

(x,y)∈D

where D denotes a set of parallel sentence pairs.
The model parameters θ are learned through
Stochastic Gradient Descent (SGD).

3 Attentional Tree-to-Sequence Model

3.1 Tree-based Encoder + Sequential

Encoder

The exsiting NMT models treat a sentence as a
sequence of words and neglect the structure of

Figure 3: Proposed model: Tree-to-sequence at-
tentional NMT model.

a sentence inherent in language. We propose a
novel tree-based encoder in order to explicitly take
the syntactic structure into consideration in the
NMT model. We focus on the phrase structure of
a sentence and construct a sentence vector from
phrase vectors in a bottom-up fashion. The sen-
tence vector in the tree-based encoder is there-
fore composed of the structural information rather
than the sequential data. Figure 3 shows our pro-
posed model, which we call a tree-to-sequence at-
tentional NMT model.

In Head-driven Phrase Structure Grammar
(HPSG) (Sag et al., 2003), a sentence is composed
of multiple phrase units and represented as a bi-
nary tree as shown in Figure 1. Following the
structure of the sentence, we construct a tree-based
encoder on top of the standard sequential encoder.
The k-th parent hidden unit h(phr)
for the k-th
phrase is calculated using the left and right child
hidden units hl

k as follows:

k and hr

k

h(phr)
k

= ftree(hl

k, hr

k),

(10)

where ftree is a non-linear function.

We construct a tree-based encoder with LSTM
units, where each node in the binary tree is repre-
sented with an LSTM unit. When initializing the
leaf units of the tree-based encoder, we employ the
sequential LSTM units described in Section 2.1.
Each non-leaf node is also represented with an
LSTM unit, and we employ Tree-LSTM (Tai et
al., 2015) to calculate the LSTM unit of the par-
ent node which has two child LSTM units. The
hidden unit h(phr)
∈ Rd×1 and the memory cell
c(phr)
∈ Rd×1 for the k-th parent node are calcu-
k

k

lated as follows:

k + b(fl)),
k + b(fr)),

k + b(i)),

ik = σ(U (i)
l hl
k = σ(U (fl)
l hl
f l
k = σ(U (fr)
l hl
f r
ok = σ(U (o)
l hl
˜ck = tanh(U (˜c)

r hr
k + U (i)
(fl)
r hr
k + U
k + U (fr)
r hr
k + b(o)),
k + U (o)
r hr
r hr
k + U (˜c)
l hl
= ik (cid:12) ˜ck + f l
k + f r
k (cid:12) cl
= ok (cid:12) tanh(c(phr)
),

k + b(˜c)),
k (cid:12) cr
k,

k

c(phr)
k
h(phr)
k

(11)

k, f r

k , oj, ˜cj ∈ Rd×1 are an input
where ik, f l
gate, the forget gates for left and right child units,
an output gate, and a state for updating the mem-
ory cell, respectively. cl
k are the mem-
ory cells for the left and right child units, respec-
tively. U (·) ∈ Rd×d denotes a weight matrix, and
b(·) ∈ Rd×1 represents a bias vector.

k and cr

Our proposed tree-based encoder is a natural
extension of the conventional sequential encoder,
since Tree-LSTM is a generalization of chain-
structured LSTM (Tai et al., 2015). Our encoder
differs from the original Tree-LSTM in the cal-
culation of the LSTM units for the leaf nodes.
The motivation is to construct the phrase nodes in
a context-sensitive way, which, for example, al-
lows the model to compute different representa-
tions for multiple occurrences of the same word in
a sentence because the sequential LSTMs are cal-
culated in the context of the previous units. This
ability contrasts with the original Tree-LSTM, in
which the leaves are composed only of the word
embeddings without any contextual information.

3.2

Initial Decoder Setting

We now have two different sentence vectors: one
is from the sequence encoder and the other from
the tree-based encoder. As shown in Figure 3, we
provide another Tree-LSTM unit which has the ﬁ-
nal sequential encoder unit (hn) and the tree-based
encoder unit (h(phr)
root ) as two child units and set it
as the initial decoder s1 as follows:

where gtree is the same function as ftree with an-
other set of Tree-LSTM parameters. This initial-
ization allows the decoder to capture information
from both the sequential data and phrase struc-
tures. Zoph and Knight (2016) proposed a simi-
lar method using a Tree-LSTM for initializing the

decoder, with which they translate multiple source
languages to one target language. When the syn-
tactic parser fails to output a parse tree for a sen-
tence, we encode the sentence with the sequential
encoder by setting h(phr)
root = 0. Our proposed tree-
based encoder therefore works with any sentences.

3.3 Attention Mechanism in Our Model

We adopt the attention mechanism into our tree-
to-sequence model in a novel way. Our model
gives attention not only to sequential hidden units
but also to phrase hidden units. This attention
mechanism tells us which words or phrases in the
source sentence are important when the model de-
codes a target word. The j-th context vector dj
is composed of the sequential and phrase vectors
weighted by the attention score αj(i):

dj =

αj(i)hi +

αj(i)h(phr)
i

.

(13)

n
(cid:88)

i=1

2n−1
(cid:88)

i=n+1

In addition, we

Note that a binary tree has n − 1 phrase nodes if
the tree has n leaves. We set a ﬁnal decoder ˜sj in
the same way as Equation (7).
adopt

input-feeding
method (Luong et al., 2015a) in our model, which
is a method for feeding ˜sj−1, the previous unit
to predict the word yj−1, into the current target
hidden unit sj,

the

sj = fdec(yj−1, [sj−1; ˜sj−1]),

(14)

where [sj−1; ˜sj−1] is the concatenation of sj−1
and ˜sj−1. The input-feeding approach contributes
to the enrichment in the calculation of the decoder,
because ˜sj−1 is an informative unit which can be
used to predict the output word as well as to be
compacted with attentional context vectors. Lu-
ong et al. (2015a) showed that the input-feeding
approach improves BLEU scores. We also ob-
served the same improvement in our preliminary
experiments.

3.4 Sampling-Based Approximation to the

The biggest computational bottleneck of train-
ing the NMT models is in the calculation of the
softmax layer described in Equation (8), because
its computational cost increases linearly with the
size of the vocabulary. The speedup technique
with GPUs has proven useful for sequence-based
NMT models (Sutskever et al., 2014; Luong et al.,

s1 = gtree(hn, h(phr)

root ),

(12)

NMT Models

2015a) but it is not easily applicable when deal-
ing with tree-structured data. In order to reduce
the training cost of the NMT models at the soft-
max layer, we employ BlackOut (Ji et al., 2016), a
sampling-based approximation method. BlackOut
has been shown to be effective in RNN Language
Models (RNNLMs) and allows a model to run rea-
sonably fast even with a million word vocabulary
with CPUs.

At each word prediction step in the training,
BlackOut estimates the conditional probability in
Equation (2) for the target word and K neg-
ative samples using a weighted softmax func-
tion. The negative samples are drawn from the
unigram distribution raised to the power β ∈
[0, 1] (Mikolov et al., 2013). The unigram dis-
tribution is estimated using the training data and
β is a hyperparameter. BlackOut is closely re-
lated to Noise Contrastive Estimation (NCE) (Gut-
mann and Hyv¨arinen, 2012) and achieves better
perplexity than the original softmax and NCE in
RNNLMs. The advantages of Blackout over the
other methods are discussed in Ji et al. (2016).
Note that BlackOut can be used as the original
softmax once the training is ﬁnished.

4 Experiments

4.1 Training Data

We applied the proposed model to the English-to-
Japanese translation dataset of the ASPEC corpus
given in WAT’15.1 Following Zhu (2015), we ex-
tracted the ﬁrst 1.5 million translation pairs from
the training data. To obtain the phrase structures
of the source sentences, i.e., English, we used the
probabilistic HPSG parser Enju (Miyao and Tsu-
jii, 2008). We used Enju only to obtain a binary
phrase structure for each sentence and did not use
any HPSG speciﬁc information. For the target
language, i.e., Japanese, we used KyTea (Neubig
et al., 2011), a Japanese segmentation tool, and
performed the pre-processing steps recommended
in WAT’15.2 We then ﬁltered out the translation
pairs whose sentence lengths are longer than 50
and whose source sentences are not parsed suc-
cessfully. Table 1 shows the details of the datasets
used in our experiments. We carried out two ex-
periments on a small training dataset to investigate

1http://orchid.kuee.kyoto-u.ac.jp/WAT/

WAT2015/index.html

2http://orchid.kuee.kyoto-u.ac.jp/WAT/

WAT2015/baseline/dataPreparationJE.html

Train
Development
Test

Sentences Parsed successfully
1,346,946
1,346,946
1,789
1,790
1,811
1,812

Table 1: Dataset in ASPEC corpus.

sentence pairs
|V | in English
|V | in Japanese

Train (small) Train (large)
1,346,946
87,796
65,680

100,000
25,478
23,532

Table 2: Training dataset and the vocabulary sizes.

the effectiveness of our proposed model and on
a large training dataset to compare our proposed
methods with the other systems.

The vocabulary consists of words observed in
the training data more than or equal to N times.
We set N = 2 for the small training dataset and
N = 5 for the large training dataset. The out-of-
vocabulary words are mapped to the special token
“unk”. We added another special symbol “eos” for
both languages and inserted it at the end of all the
sentences. Table 2 shows the details of each train-
ing dataset and its corresponding vocabulary size.

4.2 Training Details

The biases,
softmax weights, and BlackOut
weights are initialized with zeros. The hyperpa-
rameter β of BlackOut is set to 0.4 as recom-
mended by Ji et al. (2016). Following J´ozefowicz
et al. (2015), we initialize the forget gate biases of
LSTM and Tree-LSTM with 1.0. The remaining
model parameters in the NMT models in our ex-
periments are uniformly initialized in [−0.1, 0.1].
The model parameters are optimized by plain SGD
with the mini-batch size of 128. The initial learn-
ing rate of SGD is 1.0. We halve the learning rate
when the development loss becomes worse. Gra-
dient norms are clipped to 3.0 to avoid exploding
gradient problems (Pascanu et al., 2012).

Small Training Dataset We conduct experi-
ments with our proposed model and the sequential
attentional NMT model with the input-feeding ap-
proach. Each model has 256-dimensional hidden
units and word embeddings. The number of nega-
tive samples K of BlackOut is set to 500 or 2000.

Large Training Dataset Our proposed model
has 512-dimensional word embeddings and d-
dimensional hidden units (d ∈ {512, 768, 1024}).
K is set to 2500.

Our code3 is implemented in C++ using the
Eigen library,4 a template library for linear alge-
bra, and we run all of the experiments on multi-
core CPUs.5 It takes about a week to train a model
on the large training dataset with d = 512.

4.3 Decoding process

We use beam search to decode a target sentence
for an input sentence x and calculate the sum
of the log-likelihoods of the target sentence y =
(y1, · · · , ym) as the beam score:

score(x, y) =

log p(yj|y<j, x).

(15)

m
(cid:88)

j=1

Decoding in the NMT models is a generative pro-
cess and depends on the target language model
given a source sentence. The score becomes
smaller as the target sentence becomes longer, and
thus the simple beam search does not work well
when decoding a long sentence (Cho et al., 2014a;
Pouget-Abadie et al., 2014).
In our preliminary
experiments, the beam search with the length nor-
malization in Cho et al. (2014a) was not effective
in English-to-Japanese translation. The method
in Pouget-Abadie et al. (2014) needs to estimate
the conditional probability p(x|y) using another
NMT model and thus is not suitable for our work.
In this paper, we use statistics on sentence
lengths in beam search. Assuming that the length
of a target sentence correlates with the length of
a source sentence, we redeﬁne the score of each
candidate as follows:

m
(cid:88)

j=1

score(x, y) = Lx,y +

log p(yj|y<j, x),(16)

Lx,y = log p(len(y)|len(x)),

(17)

where Lx,y is the penalty for the conditional prob-
ability of the target sentence length len(y) given
the source sentence length len(x).
It allows
the model to decode a sentence by considering
the length of the target sentence.
In our exper-
iments, we computed the conditional probability

3https://github.com/tempra28/tree2seq
4http://eigen.tuxfamily.org/index.php
516 threads on Intel(R) Xeon(R) CPU E5-2667 v3 @

3.20GHz

p(len(y)|len(x)) in advance following the statis-
tics collected in the ﬁrst one million pairs of the
training dataset. We allow the decoder to generate
up to 100 words.

4.4 Evaluation

We evaluated the models by two automatic eval-
uation metrics, RIBES (Isozaki et al., 2010) and
BLEU (Papineni et al., 2002) following WAT’15.
We used the KyTea-based evaluation script for the
translation results.6 The RIBES score is a metric
based on rank correlation coefﬁcients with word
precision, and the BLEU score is based on n-gram
word precision and a Brevity Penalty (BP) for out-
puts shorter than the references. RIBES is known
to have stronger correlation with human judge-
ments than BLEU in translation between English
and Japanese as discussed in Isozaki et al. (2010).

5 Results and Discussion

5.1 Small Training Dataset

Table 3 shows the perplexity, BLEU, RIBES, and
the training time on the development data with the
Attentional NMT (ANMT) models trained on the
small dataset. We conducted the experiments with
our proposed method using BlackOut and soft-
max. We decoded a translation by our proposed
beam search with a beam size of 20.

As shown in Table 3, the results of our proposed
model with BlackOut improve as the number of
negative samples K increases. Although the result
of softmax is better than those of BlackOut (K =
500, 2000), the training time of softmax per epoch
is about three times longer than that of BlackOut
even with the small dataset.

As to the results of the ANMT model, reversing
the word order in the input sentence decreases the
scores in English-to-Japanese translation, which
contrasts with the results of other language pairs
reported in previous work (Sutskever et al., 2014;
Luong et al., 2015a). By taking syntactic infor-
mation into consideration, our proposed model
improves the scores, compared to the sequential
attention-based approach.

We found that better perplexity does not always
lead to better translation scores with BlackOut as
shown in Table 3. One of the possible reasons is
that BlackOut distorts the target word distribution

6http://lotus.kuee.kyoto-u.ac.jp/WAT/
evaluation/automatic_evaluation_systems/
automaticEvaluationJA.html

Proposed model
Proposed model
Proposed model (Softmax)
ANMT (Luong et al., 2015a)
+ reverse input
ANMT (Luong et al., 2015a)
+ reverse input

K
500
2000
—
500
500
2000
2000

Perplexity RIBES BLEU Time/epoch (min.)
20.0
20.5
21.8
18.5
17.7
19.4
17.5

19.6
21.0
17.9
21.6
22.6
23.1
26.1

71.8
72.6
73.2
70.7
69.8
71.5
69.5

55
70
180
45
45
60
60

Table 3: Evaluation results on the development data using the small training data. The training time per
epoch is also shown, and K is the number of negative samples in BlackOut.

Simple BS

Proposed BS

Beam size RIBES BLEU (BP)
20.0 (90.1)
72.3
19.5 (85.1)
72.3
20.5 (91.7)
72.6

6
20
20

Without sequential LSTMs
With sequential LSTMs

RIBES BLEU
19.5
20.0

69.4
72.3

Table 4: Effects of the Beam Search (BS) on the
development data.

Table 5: Effects of the sequential LSTMs in our
proposed tree-based encoder on the development
data.

by the modiﬁed unigram-based negative sampling
where frequent words can be treated as the nega-
tive samples multiple times at each training step.

Effects of the proposed beam search Table 4
shows the results on the development data of pro-
posed method with BlackOut (K = 2000) by
the simple beam search and our proposed beam
search. The beam size is set to 6 or 20 in the sim-
ple beam search, and to 20 in our proposed search.
We can see that our proposed search outperforms
the simple beam search in both scores. Unlike
RIBES, the BLEU score is sensitive to the beam
size and becomes lower as the beam size increases.
We found that the BP had a relatively large im-
pact on the BLEU score in the simple beam search
as the beam size increased. Our search method
works better than the simple beam search by keep-
ing long sentences in the candidates with a large
beam size.

Effects of the sequential LSTM units We also
investigated the effects of the sequential LSTMs
at the leaf nodes in our proposed tree-based en-
coder. Table 5 shows the result on the develop-
ment data of our proposed encoder and that of an
attentional tree-based encoder without sequential
LSTMs with BlackOut (K = 2000).7 The results
show that our proposed encoder considerably out-

7For this evaluation, we used the 1,789 sentences that
were successfully parsed by Enju because the encoder with-
out sequential LSTMs always requires a parse tree.

performs the encoder without sequential LSTMs,
suggesting that the sequential LSTMs at the leaf
nodes contribute to the context-aware construction
of the phrase representations in the tree.

5.2 Large Training Dataset

Table 6 shows the experimental results of RIBES
and BLEU scores achieved by the trained models
on the large dataset. We decoded the target sen-
tences by our proposed beam search with the beam
size of 20.8 The results of the other systems are the
ones reported in Nakazawa et al. (2015).

All of our proposed models show similar per-
formance regardless of the value of d. Our ensem-
ble model is composed of the three models with
d = 512, 768, and 1024, and it shows the best
RIBES score among all systems.9

As for the time required for training, our im-
plementation needs about one day to perform one
epoch on the large training dataset with d = 512.
It would take about 11 days without using the
BlackOut sampling.

Comparison with the NMT models The model
of Zhu (2015) is an ANMT model (Bahdanau et
al., 2015) with a bi-directional LSTM encoder,
and uses 1024-dimensional hidden units and 1000-

8We found two sentences which ends without eos with
d = 512, and then we decoded it again with the beam size of
1000 following Zhu (2015).

9Our ensemble model yields a METEOR (Denkowski and

Lavie, 2014) score of 53.6 with language option “-l other”.

Model
Proposed model (d = 512)
Proposed model (d = 768)
Proposed model (d = 1024)
Ensemble of the above three models
ANMT with LSTMs (Zhu, 2015)
+ Ensemble, unk replacement
+ System combination,

3 pre-reordered ensembles

ANMT with GRUs (Lee et al., 2015)
+ character-based decoding,

Begin/Inside representation

RIBES BLEU
34.36
81.46
34.78
81.89
34.87
81.58
82.45
36.95
32.19
79.70
34.19
80.27

80.91

36.21

81.15

35.75

PB baseline
HPB baseline
T2S baseline
T2S model (Neubig and Duh, 2014)
+ ANMT Rerank (Neubig et al., 2015)

69.19
74.70
75.80
79.65
81.38

29.80
32.56
33.44
36.58
38.17

Table 6: Evaluation results on the test data.

dimensional word embeddings. The model of Lee
et al. (2015) is also an ANMT model with a bi-
directional Gated Recurrent Unit (GRU) encoder,
and uses 1000-dimensional hidden units and 200-
dimensional word embeddings. Both models are
sequential ANMT models. Our single proposed
model with d = 512 outperforms the best result of
Zhu (2015)’s end-to-end NMT model with ensem-
ble and unknown replacement by +1.19 RIBES
and by +0.17 BLEU scores. Our ensemble model
in both RIBES and
shows better performance,
BLEU scores, than that of Zhu (2015)’s best sys-
tem which is a hybrid of the ANMT and SMT
models by +1.54 RIBES and by +0.74 BLEU
scores and Lee et al. (2015)’s ANMT system
with special character-based decoding by +1.30
RIBES and +1.20 BLEU scores.

Comparison with the SMT models PB, HPB
and T2S are the baseline SMT systems in
WAT’15: a phrase-based model, a hierarchical
phrase-based model, and a tree-to-string model,
respectively (Nakazawa et al., 2015). The best
model in WAT’15 is Neubig et al. (2015)’s tree-
to-string SMT model enhanced with reranking by
ANMT using a bi-directional LSTM encoder. Our
proposed end-to-end NMT model compares favor-
ably with Neubig et al. (2015).

5.3 Qualitative Analysis

We illustrate the translations of test data by our
model with d = 512 and several attentional rela-
tions when decoding a sentence. In Figures 4 and
5, an English sentence represented as a binary tree
is translated into Japanese, and several attentional
relations between English words or phrases and

Figure 4: Translation example of a short sen-
tence and the attentional relations by our proposed
model.

Japanese word are shown with the highest atten-
tion score α. The additional attentional relations
are also illustrated for comparison. We can see the
target words softly aligned with source words and
phrases.

In Figure 4, the Japanese word “液晶” means
“liquid crystal”, and it has a high attention score
(α = 0.41) with the English phrase “liquid crystal
for active matrix”. This is because the j-th tar-
get hidden unit sj has the contextual information
about the previous words y<j including “活性 マ
トリックス の” (“for active matrix” in English).
The Japanese word “セル” is softly aligned with
the phrase “the cells” with the highest attention
score (α = 0.35). In Japanese, there is no deﬁ-
nite article like “the” in English, and it is usually
aligned with null described as Section 1.

In Figure 5, in the case of the Japanese word
“示” (“showed” in English), the attention score
with the English phrase “showed excellent perfor-
mance” (α = 0.25) is higher than that with the
English word “showed” (α = 0.01). The Japanese
word “の” (“of” in English) is softly aligned with
the phrase “of Si dot MOS capacitor” with the
highest attention score (α = 0.30). It is because
our attention mechanism takes each previous con-
text of the Japanese phrases “優れ た 性能” (“ex-
cellent performance” in English) and “Ｓｉ ドッ
ト ＭＯＳ コンデンサ” (“Si dot MOS capaci-
tor” in English) into account and softly aligned the
target words with the whole phrase when trans-
lating the English verb “showed” and the prepo-
sition “of”. Our proposed model can thus ﬂexibly
learn the attentional relations between English and
Japanese.

We observed that our model translated the word
“active” into “活性”, a synonym of the reference
word “アクティブ”. We also found similar ex-

Figure 5: Translation example of a long sentence and the attentional relations by our proposed model.

amples in other sentences, where our model out-
puts synonyms of the reference words, e.g. “女”
and “女 性” (“female” in English) and “NASA”
and “航 空 宇 宙 局” (“National Aeronautics and
Space Administration” in English). These trans-
lations are penalized in terms of BLEU scores, but
they do not necessarily mean that the translations
were wrong. This point may be supported by the
fact that the NMT models were highly evaluated
in WAT’15 by crowd sourcing (Nakazawa et al.,
2015).

enables the NMT models to translate while align-
ing the target with the source. Luong et al. (2015a)
reﬁned the attention model so that it can dynami-
cally focus on local windows rather than the entire
sentence. They also proposed a more effective at-
tentional path in the calculation of ANMT models.
Subsequently, several ANMT models have been
proposed (Cheng et al., 2016; Cohn et al., 2016);
however, each model is based on the existing se-
quential attentional models and does not focus on
a syntactic structure of languages.

6 Related Work

7 Conclusion

Kalchbrenner and Blunsom (2013) were the ﬁrst
to propose an end-to-end NMT model using Con-
volutional Neural Networks (CNNs) as the source
encoder and using RNNs as the target decoder.
The Encoder-Decoder model can be seen as an ex-
tension of their model, and it replaces the CNNs
with RNNs using GRUs (Cho et al., 2014b) or
LSTMs (Sutskever et al., 2014).

Sutskever et al. (2014) have shown that mak-
ing the input sequences reversed is effective in a
French-to-English translation task, and the tech-
nique has also proven effective in translation tasks
between other European language pairs (Luong et
al., 2015a). All of the NMT models mentioned
above are based on sequential encoders. To incor-
porate structural information into the NMT mod-
els, Cho et al. (2014a) proposed to jointly learn
structures inherent in source-side languages but
did not report improvement of translation perfor-
mance. These studies motivated us to investigate
the role of syntactic structures explicitly given by
existing syntactic parsers in the NMT models.

The attention mechanism (Bahdanau et al.,
2015) has promoted NMT onto the next stage. It

In this paper, we propose a novel syntactic ap-
proach that extends attentional NMT models. We
focus on the phrase structure of the input sen-
tence and build a tree-based encoder following
the parsed tree. Our proposed tree-based encoder
is a natural extension of the sequential encoder
model, where the leaf units of the tree-LSTM
in the encoder can work together with the origi-
nal sequential LSTM encoder. Moreover, the at-
tention mechanism allows the tree-based encoder
to align not only the input words but also input
phrases with the output words. Experimental re-
sults on the WAT’15 English-to-Japanese transla-
tion dataset demonstrate that our proposed model
achieves the best RIBES score and outperforms
the sequential attentional NMT model.

Acknowledgments

We thank the anonymous reviewers for their con-
structive comments and suggestions. This work
was supported by CREST, JST, and JSPS KAK-
ENHI Grant Number 15J12597.

References

[Bahdanau et al.2015] Dzmitry Bahdanau, Kyunghyun
Cho, and Yoshua Bengio. 2015. Neural Machine
Translation by Jointly Learning to Align and Trans-
late. In Proceedings of the 3rd International Con-
ference on Learning Representations.

[Cheng et al.2016] Yong Cheng, Shiqi Shen, Zhongjun
He, Wei He, Hua Wu, Maosong Sun, and Yang Liu.
2016. Agreement-based Joint Training for Bidirec-
tional Attention-based Neural Machine Translation.
In Proceedings of the 25th International Joint Con-
ference on Artiﬁcial Intelligence. to appear.

[Cho et al.2014a] KyungHyun Cho, Bart van Merrien-
boer, Dzmitry Bahdanau, and Yoshua Bengio.
2014a. On the Properties of Neural Machine Trans-
lation: Encoder-Decoder Approaches. In Proceed-
ings of Eighth Workshop on Syntax, Semantics and
Structure in Statistical Translation (SSST-8).

[Cho et al.2014b] Kyunghyun Cho, Bart van Merrien-
boer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi
Bougares, Holger Schwenk, and Yoshua Bengio.
2014b. Learning Phrase Representations using RNN
Encoder–Decoder for Statistical Machine Transla-
In Proceedings of the 2014 Conference on
tion.
Empirical Methods in Natural Language Process-
ing, pages 1724–1734.

[Cohn et al.2016] Trevor Cohn, Cong Duy Vu Hoang,
Ekaterina Vymolova, Kaisheng Yao, Chris Dyer,
and Gholamreza Haffari.
Incorporating
Structural Alignment Biases into an Attentional
In Proceedings of the
Neural Translation Model.
15th Annual Conference of
the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies. to appear.

2016.

[Denkowski and Lavie2014] Michael Denkowski and
Alon Lavie. 2014. Meteor universal: Language spe-
ciﬁc translation evaluation for any target language.
In Proceedings of the 14th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics 2014 Workshop on Statistical Machine
Translation.

[Gers et al.2000] Felix A. Gers, J¨urgen Schmidhuber,
and Fred A. Cummins. 2000. Learning to Forget:
Continual Prediction with LSTM. Neural Computa-
tion, 12(10):2451–2471.

[Gutmann and Hyv¨arinen2012] Michael U. Gutmann
and Aapo Hyv¨arinen. 2012. Noise-contrastive esti-
mation of unnormalized statistical models, with ap-
plications to natural image statistics. Journal of Ma-
chine Learning Research, 13(1):307–361.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter
and J¨urgen Schmidhuber. 1997. Long Short-Term
Memory. Neural Computation, 9(8):1735–1780.

[Isozaki et al.2010] Hideki

Isozaki, Tsutomu Hirao,
Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada.
2010. Automatic Evaluation of Translation Quality

In Proceedings of the
for Distant Language Pairs.
2010 Conference on Empirical Methods in Natural
Language Processing, pages 944–952.

[Ji et al.2016] Shihao Ji, S. V. N. Vishwanathan, Na-
dathur Satish, Michael J. Anderson, and Pradeep
Dubey. 2016. BlackOut: Speeding up Recurrent
Neural Network Language Models With Very Large
In Proceedings of the 4th Interna-
Vocabularies.
tional Conference on Learning Representations.

[J´ozefowicz et al.2015] Rafal

J´ozefowicz, Wojciech
Zaremba, and Ilya Sutskever. 2015. An Empiri-
cal Exploration of Recurrent Network Architectures.
In Proceedings of the 32nd International Confer-
ence on Machine Learning, volume 37, pages 2342–
2350.

[Kalchbrenner and Blunsom2013] Nal Kalchbrenner
and Phil Blunsom. 2013. Recurrent continuous
In Proceedings of the 2013
translation models.
Conference on Empirical Methods
in Natural
Language Processing, pages 1700–1709.

[Lee et al.2015] Hyoung-Gyu Lee, JaeSong Lee, Jun-
Seok Kim, and Chang-Ki Lee. 2015. NAVER Ma-
In Pro-
chine Translation System for WAT 2015.
ceedings of the 2nd Workshop on Asian Translation
(WAT2015), pages 69–73.

[Liu et al.2006] Yang Liu, Qun Liu, and Shouxun Lin.
2006. Tree-to-string alignment template for statisti-
cal machine translation. In Proceedings of the 21st
International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 609–616.

[Luong et al.2015a] Thang Luong, Hieu Pham, and
2015a. Effective Ap-
Christopher D. Manning.
proaches to Attention-based Neural Machine Trans-
In Proceedings of the 2015 Conference on
lation.
Empirical Methods in Natural Language Process-
ing, pages 1412–1421.

[Luong et al.2015b] Thang Luong,

Ilya Sutskever,
Quoc Le, Oriol Vinyals, and Wojciech Zaremba.
2015b. Addressing the Rare Word Problem in Neu-
ral Machine Translation. In Proceedings of the 53rd
Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 11–19.

[Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases
In Advances in Neu-
and their compositionality.
ral Information Processing Systems 26, pages 3111–
3119.

[Miyao and Tsujii2008] Yusuke Miyao and Jun’ichi
Tsujii. 2008. Feature Forest Models for Proba-
bilistic HPSG Parsing. Computational Linguistics,
34(1):35–80.

Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 1556–1566.

[Yamada and Knight2001] Kenji Yamada and Kevin
Knight. 2001. A syntax-based statistical translation
model. In Proceedings of the 39th Annual Meeting
on Association for Computational Linguistics, pages
523–530.

[Zhu2015] Zhongyuan Zhu. 2015. Evaluating Neural
In
Machine Translation in English-Japanese Task.
Proceedings of the 2nd Workshop on Asian Transla-
tion (WAT2015), pages 61–68.

[Zoph and Knight2016] Barret Zoph and Kevin Knight.
In Pro-
2016. Multi-Source Neural Translation.
ceedings of the 15th Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
to appear.

[Nakazawa et al.2015] Toshiaki Nakazawa, Hideya
Mino, Isao Goto, Graham Neubig, Sadao Kuro-
hashi, and Eiichiro Sumita. 2015. Overview of
In Pro-
the 2nd Workshop on Asian Translation.
ceedings of the 2nd Workshop on Asian Translation
(WAT2015), pages 1–28.

[Neubig and Duh2014] Graham Neubig and Kevin
Duh. 2014. On the elements of an accurate tree-
In Proceed-
to-string machine translation system.
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 143–149.

[Neubig et al.2011] Graham Neubig, Yosuke Nakata,
and Shinsuke Mori. 2011. Pointwise Prediction for
Robust, Adaptable Japanese Morphological Analy-
sis. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 529–533.

[Neubig et al.2015] Graham Neubig, Makoto Mor-
ishita, and Satoshi Nakamura.
2015. Neural
Reranking Improves Subjective Quality of Machine
Translation: NAIST at WAT2015. In Proceedings of
the 2nd Workshop on Asian Translation (WAT2015),
pages 35–41.

[Papineni et al.2002] Kishore Papineni, Salim Roukos,
2002. BLEU:
Todd Ward, and Wei-Jing Zhu.
A Method for Automatic Evaluation of Machine
In Proceedings of the 40th Annual
Translation.
Meeting on Association for Computational Linguis-
tics, pages 311–318.

[Pascanu et al.2012] Razvan Pascanu, Tomas Mikolov,
and Yoshua Bengio. 2012. Understanding the ex-
ploding gradient problem. arXiv: 1211.5063.

[Pouget-Abadie et al.2014] Jean
Bart

Pouget-Abadie,
van Merrienboer,
Dzmitry Bahdanau,
Kyunghyun Cho, and Yoshua Bengio.
2014.
Overcoming the curse of sentence length for neural
machine translation using automatic segmentation.
In Proceedings of SSST-8, Eighth Workshop on
Syntax, Semantics and Structure in Statistical
Translation, pages 78–85.

[Sag et al.2003] Ivan A. Sag, Thomas Wasow, and
Emily Bender. 2003. Syntactic Theory: A Formal
Introduction. Center for the Study of Language and
Information, Stanford, 2nd edition.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals,
2014. Sequence to Sequence
and Quoc V. Le.
In Advances in
Learning with Neural Networks.
Neural Information Processing Systems 27, pages
3104–3112.

[Tai et al.2015] Kai Sheng Tai, Richard Socher, and
Christopher D. Manning. 2015. Improved Semantic
Representations From Tree-Structured Long Short-
In Proceedings of the
Term Memory Networks.
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint

Tree-to-Sequence Attentional Neural Machine Translation

Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa Tsuruoka
The University of Tokyo, 3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan
{eriguchi, hassy, tsuruoka}@logos.t.u-tokyo.ac.jp

6
1
0
2
 
n
u
J
 
8
 
 
]
L
C
.
s
c
[
 
 
3
v
5
7
0
6
0
.
3
0
6
1
:
v
i
X
r
a

Abstract

Most of
the existing Neural Machine
Translation (NMT) models focus on the
conversion of sequential data and do
not directly use syntactic information.
We propose a novel end-to-end syntac-
tic NMT model, extending a sequence-
to-sequence model with the source-side
phrase structure. Our model has an at-
tention mechanism that enables the de-
coder to generate a translated word while
softly aligning it with phrases as well as
words of the source sentence. Experi-
mental results on the WAT’15 English-
to-Japanese dataset demonstrate that our
proposed model considerably outperforms
sequence-to-sequence attentional NMT
models and compares favorably with the
state-of-the-art tree-to-string SMT system.

1

Introduction

Machine Translation (MT) has traditionally been
one of the most complex language processing
problems, but recent advances of Neural Machine
Translation (NMT) make it possible to perform
translation using a simple end-to-end architecture.
In the Encoder-Decoder model (Cho et al., 2014b;
Sutskever et al., 2014), a Recurrent Neural Net-
work (RNN) called the encoder reads the whole
sequence of source words to produce a ﬁxed-
length vector, and then another RNN called the
decoder generates the target words from the vec-
tor. The Encoder-Decoder model has been ex-
tended with an attention mechanism (Bahdanau et
al., 2015; Luong et al., 2015a), which allows the
model to jointly learn the soft alignment between
the source language and the target language. NMT
models have achieved state-of-the-art results in
English-to-French and English-to-German trans-

Figure 1: Alignment between an English phrase
and a Japanese word.

lation tasks (Luong et al., 2015b; Luong et al.,
2015a). However, it is yet to be seen whether
NMT is competitive with traditional Statistical
Machine Translation (SMT) approaches in trans-
lation tasks for structurally distant language pairs
such as English-to-Japanese.

Figure 1 shows a pair of parallel sentences in
English and Japanese. English and Japanese are
linguistically distant in many respects; they have
different syntactic constructions, and words and
In
phrases are deﬁned in different lexical units.
this example, the Japanese word “緑茶” is aligned
with the English words “green” and “tea”, and
the English word sequence “a cup of” is aligned
with a special symbol “null”, which is not explic-
itly translated into any Japanese words. One way
to solve this mismatch problem is to consider the
phrase structure of the English sentence and align
the phrase “a cup of green tea” with “緑茶”. In
SMT, it is known that incorporating syntactic con-
stituents of the source language into the models
improves word alignment (Yamada and Knight,
2001) and translation accuracy (Liu et al., 2006;
Neubig and Duh, 2014). However, the existing
NMT models do not allow us to perform this kind
of alignment.

In this paper, we propose a novel attentional
NMT model to take advantage of syntactic infor-

mation. Following the phrase structure of a source
sentence, we encode the sentence recursively in a
bottom-up fashion to produce a vector representa-
tion of the sentence and decode it while aligning
the input phrases and words with the output. Our
experimental results on the WAT’15 English-to-
Japanese translation task show that our proposed
model achieves state-of-the-art translation accu-
racy.

2 Neural Machine Translation

2.1 Encoder-Decoder Model

NMT is an end-to-end approach to data-driven
machine translation (Kalchbrenner and Blunsom,
2013; Sutskever et al., 2014; Bahdanau et al.,
2015). In other words, the NMT models directly
estimate the conditional probability p(y|x) given
a large collection of source and target sentence
pairs (x, y). An NMT model consists of an en-
coder process and a decoder process, and hence
they are often called Encoder-Decoder models. In
the Encoder-Decoder models, a sentence is treated
In the encoder pro-
as a sequence of words.
cess, the encoder embeds each of the source words
x = (x1, x2, · · · , xn) into a d-dimensional vector
space. The decoder then outputs a word sequence
y = (y1, y2, · · · , ym) in the target language given
the information on the source sentence provided
by the encoder. Here, n and m are the lengths
of the source and target sentences, respectively.
RNNs allow one to effectively embed sequential
data into the vector space.

In the RNN encoder, the i-th hidden unit hi ∈
Rd×1 is calculated given the i-th input xi and the
previous hidden unit hi−1 ∈ Rd×1,

hi = fenc(xi, hi−1),

(1)

where fenc is a non-linear function, and the initial
hidden unit h0 is usually set to zeros. The encod-
ing function fenc is recursively applied until the n-
th hidden unit hn is obtained. The RNN Encoder-
Decoder models assume that hn represents a vec-
tor of the meaning of the input sequence up to the
n-th word.

After encoding the whole input sentence into
the vector space, we decode it in a similar way.
The initial decoder unit s1 is initialized with the
input sentence vector (s1 = hn). Given the pre-
vious target word and the j-th hidden unit of the
decoder, the conditional probability that the j-th

target word is generated is calculated as follows:

p(yj|y<j, x) = g(sj),

(2)

where g is a non-linear function. The j-th hidden
unit of the decoder is calculated by using another
non-linear function fdec as follows:

sj = fdec(yj−1, sj−1).

(3)

We employ Long Short-Term Memory (LSTM)
units (Hochreiter and Schmidhuber, 1997; Gers et
al., 2000) in place of vanilla RNN units. The t-
th LSTM unit consists of several gates and two
different types of states: a hidden unit ht ∈ Rd×1
and a memory cell ct ∈ Rd×1,

it = σ(W (i)xt + U (i)ht−1 + b(i)),
ft = σ(W (f )xt + U (f )ht−1 + b(f )),
ot = σ(W (o)xt + U (o)ht−1 + b(o)),
˜ct = tanh(W (˜c)xt + U (˜c)ht−1 + b(˜c)),
ct = it (cid:12) ˜ct + ft (cid:12) ct−1,
ht = ot (cid:12) tanh(ct),

(4)

where each of it, ft, ot and ˜ct ∈ Rd×1 denotes
an input gate, a forget gate, an output gate, and a
state for updating the memory cell, respectively.
W (·) ∈ Rd×d and U (·) ∈ Rd×d are weight matri-
ces, b(·) ∈ Rd×1 is a bias vector, and xt ∈ Rd×1
is the word embedding of the t-th input word. σ(·)
is the logistic function, and the operator (cid:12) denotes
element-wise multiplication between vectors.

2.2 Attentional Encoder-Decoder Model

The NMT models with an attention mecha-
nism (Bahdanau et al., 2015; Luong et al., 2015a)
have been proposed to softly align each decoder
state with the encoder states. The attention mech-
anism allows the NMT models to explicitly quan-
tify how much each encoder state contributes to
the word prediction at each time step.

In the attentional NMT model in Luong et al.
(2015a), at the j-th step of the decoder process,
the attention score αj(i) between the i-th source
hidden unit hi and the j-th target hidden unit sj is
calculated as follows:

αj(i) =

exp(hi · sj)
k=1 exp(hk · sj)

,

(cid:80)n

(5)

where hi · sj is the inner product of hi and sj,
which is used to directly calculate the similarity
score between hi and sj. The j-th context vector

Figure 2: Attentional Encoder-Decoder model.

dj is calculated as the summation vector weighted
by αj(i):

dj =

αj(i)hi.

(6)

n
(cid:88)

i=1

To incorporate the attention mechanism into the
decoding process, the context vector is used for the
the j-th word prediction by putting an additional
hidden layer ˜sj:

˜sj = tanh(Wd[sj; dj] + bd),

(7)

where [sj; dj] ∈ R2d×1 is the concatenation of sj
and dj, and Wd ∈ Rd×2d and bd ∈ Rd×1 are a
weight matrix and a bias vector, respectively. The
model predicts the j-th word by using the softmax
function:

p(yj|y<j, x) = softmax(Ws ˜sj + bs),

(8)

where Ws ∈ R|V |×d and bs ∈ R|V |×1 are a weight
matrix and a bias vector, respectively. |V | stands
for the size of the vocabulary of the target lan-
guage. Figure 2 shows an example of the NMT
model with the attention mechanism.

2.3 Objective Function of NMT Models

The objective function to train the NMT models
is the sum of the log-likelihoods of the translation
pairs in the training data:

J(θ) =

log p(y|x),

(9)

(cid:88)

1
|D|

(x,y)∈D

where D denotes a set of parallel sentence pairs.
The model parameters θ are learned through
Stochastic Gradient Descent (SGD).

3 Attentional Tree-to-Sequence Model

3.1 Tree-based Encoder + Sequential

Encoder

The exsiting NMT models treat a sentence as a
sequence of words and neglect the structure of

Figure 3: Proposed model: Tree-to-sequence at-
tentional NMT model.

a sentence inherent in language. We propose a
novel tree-based encoder in order to explicitly take
the syntactic structure into consideration in the
NMT model. We focus on the phrase structure of
a sentence and construct a sentence vector from
phrase vectors in a bottom-up fashion. The sen-
tence vector in the tree-based encoder is there-
fore composed of the structural information rather
than the sequential data. Figure 3 shows our pro-
posed model, which we call a tree-to-sequence at-
tentional NMT model.

In Head-driven Phrase Structure Grammar
(HPSG) (Sag et al., 2003), a sentence is composed
of multiple phrase units and represented as a bi-
nary tree as shown in Figure 1. Following the
structure of the sentence, we construct a tree-based
encoder on top of the standard sequential encoder.
The k-th parent hidden unit h(phr)
for the k-th
phrase is calculated using the left and right child
hidden units hl

k as follows:

k and hr

k

h(phr)
k

= ftree(hl

k, hr

k),

(10)

where ftree is a non-linear function.

We construct a tree-based encoder with LSTM
units, where each node in the binary tree is repre-
sented with an LSTM unit. When initializing the
leaf units of the tree-based encoder, we employ the
sequential LSTM units described in Section 2.1.
Each non-leaf node is also represented with an
LSTM unit, and we employ Tree-LSTM (Tai et
al., 2015) to calculate the LSTM unit of the par-
ent node which has two child LSTM units. The
hidden unit h(phr)
∈ Rd×1 and the memory cell
c(phr)
∈ Rd×1 for the k-th parent node are calcu-
k

k

lated as follows:

k + b(fl)),
k + b(fr)),

k + b(i)),

ik = σ(U (i)
l hl
k = σ(U (fl)
l hl
f l
k = σ(U (fr)
l hl
f r
ok = σ(U (o)
l hl
˜ck = tanh(U (˜c)

r hr
k + U (i)
(fl)
r hr
k + U
k + U (fr)
r hr
k + b(o)),
k + U (o)
r hr
r hr
k + U (˜c)
l hl
= ik (cid:12) ˜ck + f l
k + f r
k (cid:12) cl
= ok (cid:12) tanh(c(phr)
),

k + b(˜c)),
k (cid:12) cr
k,

k

c(phr)
k
h(phr)
k

(11)

k, f r

k , oj, ˜cj ∈ Rd×1 are an input
where ik, f l
gate, the forget gates for left and right child units,
an output gate, and a state for updating the mem-
ory cell, respectively. cl
k are the mem-
ory cells for the left and right child units, respec-
tively. U (·) ∈ Rd×d denotes a weight matrix, and
b(·) ∈ Rd×1 represents a bias vector.

k and cr

Our proposed tree-based encoder is a natural
extension of the conventional sequential encoder,
since Tree-LSTM is a generalization of chain-
structured LSTM (Tai et al., 2015). Our encoder
differs from the original Tree-LSTM in the cal-
culation of the LSTM units for the leaf nodes.
The motivation is to construct the phrase nodes in
a context-sensitive way, which, for example, al-
lows the model to compute different representa-
tions for multiple occurrences of the same word in
a sentence because the sequential LSTMs are cal-
culated in the context of the previous units. This
ability contrasts with the original Tree-LSTM, in
which the leaves are composed only of the word
embeddings without any contextual information.

3.2

Initial Decoder Setting

We now have two different sentence vectors: one
is from the sequence encoder and the other from
the tree-based encoder. As shown in Figure 3, we
provide another Tree-LSTM unit which has the ﬁ-
nal sequential encoder unit (hn) and the tree-based
encoder unit (h(phr)
root ) as two child units and set it
as the initial decoder s1 as follows:

where gtree is the same function as ftree with an-
other set of Tree-LSTM parameters. This initial-
ization allows the decoder to capture information
from both the sequential data and phrase struc-
tures. Zoph and Knight (2016) proposed a simi-
lar method using a Tree-LSTM for initializing the

decoder, with which they translate multiple source
languages to one target language. When the syn-
tactic parser fails to output a parse tree for a sen-
tence, we encode the sentence with the sequential
encoder by setting h(phr)
root = 0. Our proposed tree-
based encoder therefore works with any sentences.

3.3 Attention Mechanism in Our Model

We adopt the attention mechanism into our tree-
to-sequence model in a novel way. Our model
gives attention not only to sequential hidden units
but also to phrase hidden units. This attention
mechanism tells us which words or phrases in the
source sentence are important when the model de-
codes a target word. The j-th context vector dj
is composed of the sequential and phrase vectors
weighted by the attention score αj(i):

dj =

αj(i)hi +

αj(i)h(phr)
i

.

(13)

n
(cid:88)

i=1

2n−1
(cid:88)

i=n+1

In addition, we

Note that a binary tree has n − 1 phrase nodes if
the tree has n leaves. We set a ﬁnal decoder ˜sj in
the same way as Equation (7).
adopt

input-feeding
method (Luong et al., 2015a) in our model, which
is a method for feeding ˜sj−1, the previous unit
to predict the word yj−1, into the current target
hidden unit sj,

the

sj = fdec(yj−1, [sj−1; ˜sj−1]),

(14)

where [sj−1; ˜sj−1] is the concatenation of sj−1
and ˜sj−1. The input-feeding approach contributes
to the enrichment in the calculation of the decoder,
because ˜sj−1 is an informative unit which can be
used to predict the output word as well as to be
compacted with attentional context vectors. Lu-
ong et al. (2015a) showed that the input-feeding
approach improves BLEU scores. We also ob-
served the same improvement in our preliminary
experiments.

3.4 Sampling-Based Approximation to the

The biggest computational bottleneck of train-
ing the NMT models is in the calculation of the
softmax layer described in Equation (8), because
its computational cost increases linearly with the
size of the vocabulary. The speedup technique
with GPUs has proven useful for sequence-based
NMT models (Sutskever et al., 2014; Luong et al.,

s1 = gtree(hn, h(phr)

root ),

(12)

NMT Models

2015a) but it is not easily applicable when deal-
ing with tree-structured data. In order to reduce
the training cost of the NMT models at the soft-
max layer, we employ BlackOut (Ji et al., 2016), a
sampling-based approximation method. BlackOut
has been shown to be effective in RNN Language
Models (RNNLMs) and allows a model to run rea-
sonably fast even with a million word vocabulary
with CPUs.

At each word prediction step in the training,
BlackOut estimates the conditional probability in
Equation (2) for the target word and K neg-
ative samples using a weighted softmax func-
tion. The negative samples are drawn from the
unigram distribution raised to the power β ∈
[0, 1] (Mikolov et al., 2013). The unigram dis-
tribution is estimated using the training data and
β is a hyperparameter. BlackOut is closely re-
lated to Noise Contrastive Estimation (NCE) (Gut-
mann and Hyv¨arinen, 2012) and achieves better
perplexity than the original softmax and NCE in
RNNLMs. The advantages of Blackout over the
other methods are discussed in Ji et al. (2016).
Note that BlackOut can be used as the original
softmax once the training is ﬁnished.

4 Experiments

4.1 Training Data

We applied the proposed model to the English-to-
Japanese translation dataset of the ASPEC corpus
given in WAT’15.1 Following Zhu (2015), we ex-
tracted the ﬁrst 1.5 million translation pairs from
the training data. To obtain the phrase structures
of the source sentences, i.e., English, we used the
probabilistic HPSG parser Enju (Miyao and Tsu-
jii, 2008). We used Enju only to obtain a binary
phrase structure for each sentence and did not use
any HPSG speciﬁc information. For the target
language, i.e., Japanese, we used KyTea (Neubig
et al., 2011), a Japanese segmentation tool, and
performed the pre-processing steps recommended
in WAT’15.2 We then ﬁltered out the translation
pairs whose sentence lengths are longer than 50
and whose source sentences are not parsed suc-
cessfully. Table 1 shows the details of the datasets
used in our experiments. We carried out two ex-
periments on a small training dataset to investigate

1http://orchid.kuee.kyoto-u.ac.jp/WAT/

WAT2015/index.html

2http://orchid.kuee.kyoto-u.ac.jp/WAT/

WAT2015/baseline/dataPreparationJE.html

Train
Development
Test

Sentences Parsed successfully
1,346,946
1,346,946
1,789
1,790
1,811
1,812

Table 1: Dataset in ASPEC corpus.

sentence pairs
|V | in English
|V | in Japanese

Train (small) Train (large)
1,346,946
87,796
65,680

100,000
25,478
23,532

Table 2: Training dataset and the vocabulary sizes.

the effectiveness of our proposed model and on
a large training dataset to compare our proposed
methods with the other systems.

The vocabulary consists of words observed in
the training data more than or equal to N times.
We set N = 2 for the small training dataset and
N = 5 for the large training dataset. The out-of-
vocabulary words are mapped to the special token
“unk”. We added another special symbol “eos” for
both languages and inserted it at the end of all the
sentences. Table 2 shows the details of each train-
ing dataset and its corresponding vocabulary size.

4.2 Training Details

The biases,
softmax weights, and BlackOut
weights are initialized with zeros. The hyperpa-
rameter β of BlackOut is set to 0.4 as recom-
mended by Ji et al. (2016). Following J´ozefowicz
et al. (2015), we initialize the forget gate biases of
LSTM and Tree-LSTM with 1.0. The remaining
model parameters in the NMT models in our ex-
periments are uniformly initialized in [−0.1, 0.1].
The model parameters are optimized by plain SGD
with the mini-batch size of 128. The initial learn-
ing rate of SGD is 1.0. We halve the learning rate
when the development loss becomes worse. Gra-
dient norms are clipped to 3.0 to avoid exploding
gradient problems (Pascanu et al., 2012).

Small Training Dataset We conduct experi-
ments with our proposed model and the sequential
attentional NMT model with the input-feeding ap-
proach. Each model has 256-dimensional hidden
units and word embeddings. The number of nega-
tive samples K of BlackOut is set to 500 or 2000.

Large Training Dataset Our proposed model
has 512-dimensional word embeddings and d-
dimensional hidden units (d ∈ {512, 768, 1024}).
K is set to 2500.

Our code3 is implemented in C++ using the
Eigen library,4 a template library for linear alge-
bra, and we run all of the experiments on multi-
core CPUs.5 It takes about a week to train a model
on the large training dataset with d = 512.

4.3 Decoding process

We use beam search to decode a target sentence
for an input sentence x and calculate the sum
of the log-likelihoods of the target sentence y =
(y1, · · · , ym) as the beam score:

score(x, y) =

log p(yj|y<j, x).

(15)

m
(cid:88)

j=1

Decoding in the NMT models is a generative pro-
cess and depends on the target language model
given a source sentence. The score becomes
smaller as the target sentence becomes longer, and
thus the simple beam search does not work well
when decoding a long sentence (Cho et al., 2014a;
Pouget-Abadie et al., 2014).
In our preliminary
experiments, the beam search with the length nor-
malization in Cho et al. (2014a) was not effective
in English-to-Japanese translation. The method
in Pouget-Abadie et al. (2014) needs to estimate
the conditional probability p(x|y) using another
NMT model and thus is not suitable for our work.
In this paper, we use statistics on sentence
lengths in beam search. Assuming that the length
of a target sentence correlates with the length of
a source sentence, we redeﬁne the score of each
candidate as follows:

m
(cid:88)

j=1

score(x, y) = Lx,y +

log p(yj|y<j, x),(16)

Lx,y = log p(len(y)|len(x)),

(17)

where Lx,y is the penalty for the conditional prob-
ability of the target sentence length len(y) given
the source sentence length len(x).
It allows
the model to decode a sentence by considering
the length of the target sentence.
In our exper-
iments, we computed the conditional probability

3https://github.com/tempra28/tree2seq
4http://eigen.tuxfamily.org/index.php
516 threads on Intel(R) Xeon(R) CPU E5-2667 v3 @

3.20GHz

p(len(y)|len(x)) in advance following the statis-
tics collected in the ﬁrst one million pairs of the
training dataset. We allow the decoder to generate
up to 100 words.

4.4 Evaluation

We evaluated the models by two automatic eval-
uation metrics, RIBES (Isozaki et al., 2010) and
BLEU (Papineni et al., 2002) following WAT’15.
We used the KyTea-based evaluation script for the
translation results.6 The RIBES score is a metric
based on rank correlation coefﬁcients with word
precision, and the BLEU score is based on n-gram
word precision and a Brevity Penalty (BP) for out-
puts shorter than the references. RIBES is known
to have stronger correlation with human judge-
ments than BLEU in translation between English
and Japanese as discussed in Isozaki et al. (2010).

5 Results and Discussion

5.1 Small Training Dataset

Table 3 shows the perplexity, BLEU, RIBES, and
the training time on the development data with the
Attentional NMT (ANMT) models trained on the
small dataset. We conducted the experiments with
our proposed method using BlackOut and soft-
max. We decoded a translation by our proposed
beam search with a beam size of 20.

As shown in Table 3, the results of our proposed
model with BlackOut improve as the number of
negative samples K increases. Although the result
of softmax is better than those of BlackOut (K =
500, 2000), the training time of softmax per epoch
is about three times longer than that of BlackOut
even with the small dataset.

As to the results of the ANMT model, reversing
the word order in the input sentence decreases the
scores in English-to-Japanese translation, which
contrasts with the results of other language pairs
reported in previous work (Sutskever et al., 2014;
Luong et al., 2015a). By taking syntactic infor-
mation into consideration, our proposed model
improves the scores, compared to the sequential
attention-based approach.

We found that better perplexity does not always
lead to better translation scores with BlackOut as
shown in Table 3. One of the possible reasons is
that BlackOut distorts the target word distribution

6http://lotus.kuee.kyoto-u.ac.jp/WAT/
evaluation/automatic_evaluation_systems/
automaticEvaluationJA.html

Proposed model
Proposed model
Proposed model (Softmax)
ANMT (Luong et al., 2015a)
+ reverse input
ANMT (Luong et al., 2015a)
+ reverse input

K
500
2000
—
500
500
2000
2000

Perplexity RIBES BLEU Time/epoch (min.)
20.0
20.5
21.8
18.5
17.7
19.4
17.5

19.6
21.0
17.9
21.6
22.6
23.1
26.1

71.8
72.6
73.2
70.7
69.8
71.5
69.5

55
70
180
45
45
60
60

Table 3: Evaluation results on the development data using the small training data. The training time per
epoch is also shown, and K is the number of negative samples in BlackOut.

Simple BS

Proposed BS

Beam size RIBES BLEU (BP)
20.0 (90.1)
72.3
19.5 (85.1)
72.3
20.5 (91.7)
72.6

6
20
20

Without sequential LSTMs
With sequential LSTMs

RIBES BLEU
19.5
20.0

69.4
72.3

Table 4: Effects of the Beam Search (BS) on the
development data.

Table 5: Effects of the sequential LSTMs in our
proposed tree-based encoder on the development
data.

by the modiﬁed unigram-based negative sampling
where frequent words can be treated as the nega-
tive samples multiple times at each training step.

Effects of the proposed beam search Table 4
shows the results on the development data of pro-
posed method with BlackOut (K = 2000) by
the simple beam search and our proposed beam
search. The beam size is set to 6 or 20 in the sim-
ple beam search, and to 20 in our proposed search.
We can see that our proposed search outperforms
the simple beam search in both scores. Unlike
RIBES, the BLEU score is sensitive to the beam
size and becomes lower as the beam size increases.
We found that the BP had a relatively large im-
pact on the BLEU score in the simple beam search
as the beam size increased. Our search method
works better than the simple beam search by keep-
ing long sentences in the candidates with a large
beam size.

Effects of the sequential LSTM units We also
investigated the effects of the sequential LSTMs
at the leaf nodes in our proposed tree-based en-
coder. Table 5 shows the result on the develop-
ment data of our proposed encoder and that of an
attentional tree-based encoder without sequential
LSTMs with BlackOut (K = 2000).7 The results
show that our proposed encoder considerably out-

7For this evaluation, we used the 1,789 sentences that
were successfully parsed by Enju because the encoder with-
out sequential LSTMs always requires a parse tree.

performs the encoder without sequential LSTMs,
suggesting that the sequential LSTMs at the leaf
nodes contribute to the context-aware construction
of the phrase representations in the tree.

5.2 Large Training Dataset

Table 6 shows the experimental results of RIBES
and BLEU scores achieved by the trained models
on the large dataset. We decoded the target sen-
tences by our proposed beam search with the beam
size of 20.8 The results of the other systems are the
ones reported in Nakazawa et al. (2015).

All of our proposed models show similar per-
formance regardless of the value of d. Our ensem-
ble model is composed of the three models with
d = 512, 768, and 1024, and it shows the best
RIBES score among all systems.9

As for the time required for training, our im-
plementation needs about one day to perform one
epoch on the large training dataset with d = 512.
It would take about 11 days without using the
BlackOut sampling.

Comparison with the NMT models The model
of Zhu (2015) is an ANMT model (Bahdanau et
al., 2015) with a bi-directional LSTM encoder,
and uses 1024-dimensional hidden units and 1000-

8We found two sentences which ends without eos with
d = 512, and then we decoded it again with the beam size of
1000 following Zhu (2015).

9Our ensemble model yields a METEOR (Denkowski and

Lavie, 2014) score of 53.6 with language option “-l other”.

Model
Proposed model (d = 512)
Proposed model (d = 768)
Proposed model (d = 1024)
Ensemble of the above three models
ANMT with LSTMs (Zhu, 2015)
+ Ensemble, unk replacement
+ System combination,

3 pre-reordered ensembles

ANMT with GRUs (Lee et al., 2015)
+ character-based decoding,

Begin/Inside representation

RIBES BLEU
34.36
81.46
34.78
81.89
34.87
81.58
82.45
36.95
32.19
79.70
34.19
80.27

80.91

36.21

81.15

35.75

PB baseline
HPB baseline
T2S baseline
T2S model (Neubig and Duh, 2014)
+ ANMT Rerank (Neubig et al., 2015)

69.19
74.70
75.80
79.65
81.38

29.80
32.56
33.44
36.58
38.17

Table 6: Evaluation results on the test data.

dimensional word embeddings. The model of Lee
et al. (2015) is also an ANMT model with a bi-
directional Gated Recurrent Unit (GRU) encoder,
and uses 1000-dimensional hidden units and 200-
dimensional word embeddings. Both models are
sequential ANMT models. Our single proposed
model with d = 512 outperforms the best result of
Zhu (2015)’s end-to-end NMT model with ensem-
ble and unknown replacement by +1.19 RIBES
and by +0.17 BLEU scores. Our ensemble model
in both RIBES and
shows better performance,
BLEU scores, than that of Zhu (2015)’s best sys-
tem which is a hybrid of the ANMT and SMT
models by +1.54 RIBES and by +0.74 BLEU
scores and Lee et al. (2015)’s ANMT system
with special character-based decoding by +1.30
RIBES and +1.20 BLEU scores.

Comparison with the SMT models PB, HPB
and T2S are the baseline SMT systems in
WAT’15: a phrase-based model, a hierarchical
phrase-based model, and a tree-to-string model,
respectively (Nakazawa et al., 2015). The best
model in WAT’15 is Neubig et al. (2015)’s tree-
to-string SMT model enhanced with reranking by
ANMT using a bi-directional LSTM encoder. Our
proposed end-to-end NMT model compares favor-
ably with Neubig et al. (2015).

5.3 Qualitative Analysis

We illustrate the translations of test data by our
model with d = 512 and several attentional rela-
tions when decoding a sentence. In Figures 4 and
5, an English sentence represented as a binary tree
is translated into Japanese, and several attentional
relations between English words or phrases and

Figure 4: Translation example of a short sen-
tence and the attentional relations by our proposed
model.

Japanese word are shown with the highest atten-
tion score α. The additional attentional relations
are also illustrated for comparison. We can see the
target words softly aligned with source words and
phrases.

In Figure 4, the Japanese word “液晶” means
“liquid crystal”, and it has a high attention score
(α = 0.41) with the English phrase “liquid crystal
for active matrix”. This is because the j-th tar-
get hidden unit sj has the contextual information
about the previous words y<j including “活性 マ
トリックス の” (“for active matrix” in English).
The Japanese word “セル” is softly aligned with
the phrase “the cells” with the highest attention
score (α = 0.35). In Japanese, there is no deﬁ-
nite article like “the” in English, and it is usually
aligned with null described as Section 1.

In Figure 5, in the case of the Japanese word
“示” (“showed” in English), the attention score
with the English phrase “showed excellent perfor-
mance” (α = 0.25) is higher than that with the
English word “showed” (α = 0.01). The Japanese
word “の” (“of” in English) is softly aligned with
the phrase “of Si dot MOS capacitor” with the
highest attention score (α = 0.30). It is because
our attention mechanism takes each previous con-
text of the Japanese phrases “優れ た 性能” (“ex-
cellent performance” in English) and “Ｓｉ ドッ
ト ＭＯＳ コンデンサ” (“Si dot MOS capaci-
tor” in English) into account and softly aligned the
target words with the whole phrase when trans-
lating the English verb “showed” and the prepo-
sition “of”. Our proposed model can thus ﬂexibly
learn the attentional relations between English and
Japanese.

We observed that our model translated the word
“active” into “活性”, a synonym of the reference
word “アクティブ”. We also found similar ex-

Figure 5: Translation example of a long sentence and the attentional relations by our proposed model.

amples in other sentences, where our model out-
puts synonyms of the reference words, e.g. “女”
and “女 性” (“female” in English) and “NASA”
and “航 空 宇 宙 局” (“National Aeronautics and
Space Administration” in English). These trans-
lations are penalized in terms of BLEU scores, but
they do not necessarily mean that the translations
were wrong. This point may be supported by the
fact that the NMT models were highly evaluated
in WAT’15 by crowd sourcing (Nakazawa et al.,
2015).

enables the NMT models to translate while align-
ing the target with the source. Luong et al. (2015a)
reﬁned the attention model so that it can dynami-
cally focus on local windows rather than the entire
sentence. They also proposed a more effective at-
tentional path in the calculation of ANMT models.
Subsequently, several ANMT models have been
proposed (Cheng et al., 2016; Cohn et al., 2016);
however, each model is based on the existing se-
quential attentional models and does not focus on
a syntactic structure of languages.

6 Related Work

7 Conclusion

Kalchbrenner and Blunsom (2013) were the ﬁrst
to propose an end-to-end NMT model using Con-
volutional Neural Networks (CNNs) as the source
encoder and using RNNs as the target decoder.
The Encoder-Decoder model can be seen as an ex-
tension of their model, and it replaces the CNNs
with RNNs using GRUs (Cho et al., 2014b) or
LSTMs (Sutskever et al., 2014).

Sutskever et al. (2014) have shown that mak-
ing the input sequences reversed is effective in a
French-to-English translation task, and the tech-
nique has also proven effective in translation tasks
between other European language pairs (Luong et
al., 2015a). All of the NMT models mentioned
above are based on sequential encoders. To incor-
porate structural information into the NMT mod-
els, Cho et al. (2014a) proposed to jointly learn
structures inherent in source-side languages but
did not report improvement of translation perfor-
mance. These studies motivated us to investigate
the role of syntactic structures explicitly given by
existing syntactic parsers in the NMT models.

The attention mechanism (Bahdanau et al.,
2015) has promoted NMT onto the next stage. It

In this paper, we propose a novel syntactic ap-
proach that extends attentional NMT models. We
focus on the phrase structure of the input sen-
tence and build a tree-based encoder following
the parsed tree. Our proposed tree-based encoder
is a natural extension of the sequential encoder
model, where the leaf units of the tree-LSTM
in the encoder can work together with the origi-
nal sequential LSTM encoder. Moreover, the at-
tention mechanism allows the tree-based encoder
to align not only the input words but also input
phrases with the output words. Experimental re-
sults on the WAT’15 English-to-Japanese transla-
tion dataset demonstrate that our proposed model
achieves the best RIBES score and outperforms
the sequential attentional NMT model.

Acknowledgments

We thank the anonymous reviewers for their con-
structive comments and suggestions. This work
was supported by CREST, JST, and JSPS KAK-
ENHI Grant Number 15J12597.

References

[Bahdanau et al.2015] Dzmitry Bahdanau, Kyunghyun
Cho, and Yoshua Bengio. 2015. Neural Machine
Translation by Jointly Learning to Align and Trans-
late. In Proceedings of the 3rd International Con-
ference on Learning Representations.

[Cheng et al.2016] Yong Cheng, Shiqi Shen, Zhongjun
He, Wei He, Hua Wu, Maosong Sun, and Yang Liu.
2016. Agreement-based Joint Training for Bidirec-
tional Attention-based Neural Machine Translation.
In Proceedings of the 25th International Joint Con-
ference on Artiﬁcial Intelligence. to appear.

[Cho et al.2014a] KyungHyun Cho, Bart van Merrien-
boer, Dzmitry Bahdanau, and Yoshua Bengio.
2014a. On the Properties of Neural Machine Trans-
lation: Encoder-Decoder Approaches. In Proceed-
ings of Eighth Workshop on Syntax, Semantics and
Structure in Statistical Translation (SSST-8).

[Cho et al.2014b] Kyunghyun Cho, Bart van Merrien-
boer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi
Bougares, Holger Schwenk, and Yoshua Bengio.
2014b. Learning Phrase Representations using RNN
Encoder–Decoder for Statistical Machine Transla-
In Proceedings of the 2014 Conference on
tion.
Empirical Methods in Natural Language Process-
ing, pages 1724–1734.

[Cohn et al.2016] Trevor Cohn, Cong Duy Vu Hoang,
Ekaterina Vymolova, Kaisheng Yao, Chris Dyer,
and Gholamreza Haffari.
Incorporating
Structural Alignment Biases into an Attentional
In Proceedings of the
Neural Translation Model.
15th Annual Conference of
the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies. to appear.

2016.

[Denkowski and Lavie2014] Michael Denkowski and
Alon Lavie. 2014. Meteor universal: Language spe-
ciﬁc translation evaluation for any target language.
In Proceedings of the 14th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics 2014 Workshop on Statistical Machine
Translation.

[Gers et al.2000] Felix A. Gers, J¨urgen Schmidhuber,
and Fred A. Cummins. 2000. Learning to Forget:
Continual Prediction with LSTM. Neural Computa-
tion, 12(10):2451–2471.

[Gutmann and Hyv¨arinen2012] Michael U. Gutmann
and Aapo Hyv¨arinen. 2012. Noise-contrastive esti-
mation of unnormalized statistical models, with ap-
plications to natural image statistics. Journal of Ma-
chine Learning Research, 13(1):307–361.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter
and J¨urgen Schmidhuber. 1997. Long Short-Term
Memory. Neural Computation, 9(8):1735–1780.

[Isozaki et al.2010] Hideki

Isozaki, Tsutomu Hirao,
Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada.
2010. Automatic Evaluation of Translation Quality

In Proceedings of the
for Distant Language Pairs.
2010 Conference on Empirical Methods in Natural
Language Processing, pages 944–952.

[Ji et al.2016] Shihao Ji, S. V. N. Vishwanathan, Na-
dathur Satish, Michael J. Anderson, and Pradeep
Dubey. 2016. BlackOut: Speeding up Recurrent
Neural Network Language Models With Very Large
In Proceedings of the 4th Interna-
Vocabularies.
tional Conference on Learning Representations.

[J´ozefowicz et al.2015] Rafal

J´ozefowicz, Wojciech
Zaremba, and Ilya Sutskever. 2015. An Empiri-
cal Exploration of Recurrent Network Architectures.
In Proceedings of the 32nd International Confer-
ence on Machine Learning, volume 37, pages 2342–
2350.

[Kalchbrenner and Blunsom2013] Nal Kalchbrenner
and Phil Blunsom. 2013. Recurrent continuous
In Proceedings of the 2013
translation models.
Conference on Empirical Methods
in Natural
Language Processing, pages 1700–1709.

[Lee et al.2015] Hyoung-Gyu Lee, JaeSong Lee, Jun-
Seok Kim, and Chang-Ki Lee. 2015. NAVER Ma-
In Pro-
chine Translation System for WAT 2015.
ceedings of the 2nd Workshop on Asian Translation
(WAT2015), pages 69–73.

[Liu et al.2006] Yang Liu, Qun Liu, and Shouxun Lin.
2006. Tree-to-string alignment template for statisti-
cal machine translation. In Proceedings of the 21st
International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 609–616.

[Luong et al.2015a] Thang Luong, Hieu Pham, and
2015a. Effective Ap-
Christopher D. Manning.
proaches to Attention-based Neural Machine Trans-
In Proceedings of the 2015 Conference on
lation.
Empirical Methods in Natural Language Process-
ing, pages 1412–1421.

[Luong et al.2015b] Thang Luong,

Ilya Sutskever,
Quoc Le, Oriol Vinyals, and Wojciech Zaremba.
2015b. Addressing the Rare Word Problem in Neu-
ral Machine Translation. In Proceedings of the 53rd
Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 11–19.

[Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases
In Advances in Neu-
and their compositionality.
ral Information Processing Systems 26, pages 3111–
3119.

[Miyao and Tsujii2008] Yusuke Miyao and Jun’ichi
Tsujii. 2008. Feature Forest Models for Proba-
bilistic HPSG Parsing. Computational Linguistics,
34(1):35–80.

Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 1556–1566.

[Yamada and Knight2001] Kenji Yamada and Kevin
Knight. 2001. A syntax-based statistical translation
model. In Proceedings of the 39th Annual Meeting
on Association for Computational Linguistics, pages
523–530.

[Zhu2015] Zhongyuan Zhu. 2015. Evaluating Neural
In
Machine Translation in English-Japanese Task.
Proceedings of the 2nd Workshop on Asian Transla-
tion (WAT2015), pages 61–68.

[Zoph and Knight2016] Barret Zoph and Kevin Knight.
In Pro-
2016. Multi-Source Neural Translation.
ceedings of the 15th Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
to appear.

[Nakazawa et al.2015] Toshiaki Nakazawa, Hideya
Mino, Isao Goto, Graham Neubig, Sadao Kuro-
hashi, and Eiichiro Sumita. 2015. Overview of
In Pro-
the 2nd Workshop on Asian Translation.
ceedings of the 2nd Workshop on Asian Translation
(WAT2015), pages 1–28.

[Neubig and Duh2014] Graham Neubig and Kevin
Duh. 2014. On the elements of an accurate tree-
In Proceed-
to-string machine translation system.
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 143–149.

[Neubig et al.2011] Graham Neubig, Yosuke Nakata,
and Shinsuke Mori. 2011. Pointwise Prediction for
Robust, Adaptable Japanese Morphological Analy-
sis. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 529–533.

[Neubig et al.2015] Graham Neubig, Makoto Mor-
ishita, and Satoshi Nakamura.
2015. Neural
Reranking Improves Subjective Quality of Machine
Translation: NAIST at WAT2015. In Proceedings of
the 2nd Workshop on Asian Translation (WAT2015),
pages 35–41.

[Papineni et al.2002] Kishore Papineni, Salim Roukos,
2002. BLEU:
Todd Ward, and Wei-Jing Zhu.
A Method for Automatic Evaluation of Machine
In Proceedings of the 40th Annual
Translation.
Meeting on Association for Computational Linguis-
tics, pages 311–318.

[Pascanu et al.2012] Razvan Pascanu, Tomas Mikolov,
and Yoshua Bengio. 2012. Understanding the ex-
ploding gradient problem. arXiv: 1211.5063.

[Pouget-Abadie et al.2014] Jean
Bart

Pouget-Abadie,
van Merrienboer,
Dzmitry Bahdanau,
Kyunghyun Cho, and Yoshua Bengio.
2014.
Overcoming the curse of sentence length for neural
machine translation using automatic segmentation.
In Proceedings of SSST-8, Eighth Workshop on
Syntax, Semantics and Structure in Statistical
Translation, pages 78–85.

[Sag et al.2003] Ivan A. Sag, Thomas Wasow, and
Emily Bender. 2003. Syntactic Theory: A Formal
Introduction. Center for the Study of Language and
Information, Stanford, 2nd edition.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals,
2014. Sequence to Sequence
and Quoc V. Le.
In Advances in
Learning with Neural Networks.
Neural Information Processing Systems 27, pages
3104–3112.

[Tai et al.2015] Kai Sheng Tai, Richard Socher, and
Christopher D. Manning. 2015. Improved Semantic
Representations From Tree-Structured Long Short-
In Proceedings of the
Term Memory Networks.
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint

Tree-to-Sequence Attentional Neural Machine Translation

Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa Tsuruoka
The University of Tokyo, 3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan
{eriguchi, hassy, tsuruoka}@logos.t.u-tokyo.ac.jp

6
1
0
2
 
n
u
J
 
8
 
 
]
L
C
.
s
c
[
 
 
3
v
5
7
0
6
0
.
3
0
6
1
:
v
i
X
r
a

Abstract

Most of
the existing Neural Machine
Translation (NMT) models focus on the
conversion of sequential data and do
not directly use syntactic information.
We propose a novel end-to-end syntac-
tic NMT model, extending a sequence-
to-sequence model with the source-side
phrase structure. Our model has an at-
tention mechanism that enables the de-
coder to generate a translated word while
softly aligning it with phrases as well as
words of the source sentence. Experi-
mental results on the WAT’15 English-
to-Japanese dataset demonstrate that our
proposed model considerably outperforms
sequence-to-sequence attentional NMT
models and compares favorably with the
state-of-the-art tree-to-string SMT system.

1

Introduction

Machine Translation (MT) has traditionally been
one of the most complex language processing
problems, but recent advances of Neural Machine
Translation (NMT) make it possible to perform
translation using a simple end-to-end architecture.
In the Encoder-Decoder model (Cho et al., 2014b;
Sutskever et al., 2014), a Recurrent Neural Net-
work (RNN) called the encoder reads the whole
sequence of source words to produce a ﬁxed-
length vector, and then another RNN called the
decoder generates the target words from the vec-
tor. The Encoder-Decoder model has been ex-
tended with an attention mechanism (Bahdanau et
al., 2015; Luong et al., 2015a), which allows the
model to jointly learn the soft alignment between
the source language and the target language. NMT
models have achieved state-of-the-art results in
English-to-French and English-to-German trans-

Figure 1: Alignment between an English phrase
and a Japanese word.

lation tasks (Luong et al., 2015b; Luong et al.,
2015a). However, it is yet to be seen whether
NMT is competitive with traditional Statistical
Machine Translation (SMT) approaches in trans-
lation tasks for structurally distant language pairs
such as English-to-Japanese.

Figure 1 shows a pair of parallel sentences in
English and Japanese. English and Japanese are
linguistically distant in many respects; they have
different syntactic constructions, and words and
In
phrases are deﬁned in different lexical units.
this example, the Japanese word “緑茶” is aligned
with the English words “green” and “tea”, and
the English word sequence “a cup of” is aligned
with a special symbol “null”, which is not explic-
itly translated into any Japanese words. One way
to solve this mismatch problem is to consider the
phrase structure of the English sentence and align
the phrase “a cup of green tea” with “緑茶”. In
SMT, it is known that incorporating syntactic con-
stituents of the source language into the models
improves word alignment (Yamada and Knight,
2001) and translation accuracy (Liu et al., 2006;
Neubig and Duh, 2014). However, the existing
NMT models do not allow us to perform this kind
of alignment.

In this paper, we propose a novel attentional
NMT model to take advantage of syntactic infor-

mation. Following the phrase structure of a source
sentence, we encode the sentence recursively in a
bottom-up fashion to produce a vector representa-
tion of the sentence and decode it while aligning
the input phrases and words with the output. Our
experimental results on the WAT’15 English-to-
Japanese translation task show that our proposed
model achieves state-of-the-art translation accu-
racy.

2 Neural Machine Translation

2.1 Encoder-Decoder Model

NMT is an end-to-end approach to data-driven
machine translation (Kalchbrenner and Blunsom,
2013; Sutskever et al., 2014; Bahdanau et al.,
2015). In other words, the NMT models directly
estimate the conditional probability p(y|x) given
a large collection of source and target sentence
pairs (x, y). An NMT model consists of an en-
coder process and a decoder process, and hence
they are often called Encoder-Decoder models. In
the Encoder-Decoder models, a sentence is treated
In the encoder pro-
as a sequence of words.
cess, the encoder embeds each of the source words
x = (x1, x2, · · · , xn) into a d-dimensional vector
space. The decoder then outputs a word sequence
y = (y1, y2, · · · , ym) in the target language given
the information on the source sentence provided
by the encoder. Here, n and m are the lengths
of the source and target sentences, respectively.
RNNs allow one to effectively embed sequential
data into the vector space.

In the RNN encoder, the i-th hidden unit hi ∈
Rd×1 is calculated given the i-th input xi and the
previous hidden unit hi−1 ∈ Rd×1,

hi = fenc(xi, hi−1),

(1)

where fenc is a non-linear function, and the initial
hidden unit h0 is usually set to zeros. The encod-
ing function fenc is recursively applied until the n-
th hidden unit hn is obtained. The RNN Encoder-
Decoder models assume that hn represents a vec-
tor of the meaning of the input sequence up to the
n-th word.

After encoding the whole input sentence into
the vector space, we decode it in a similar way.
The initial decoder unit s1 is initialized with the
input sentence vector (s1 = hn). Given the pre-
vious target word and the j-th hidden unit of the
decoder, the conditional probability that the j-th

target word is generated is calculated as follows:

p(yj|y<j, x) = g(sj),

(2)

where g is a non-linear function. The j-th hidden
unit of the decoder is calculated by using another
non-linear function fdec as follows:

sj = fdec(yj−1, sj−1).

(3)

We employ Long Short-Term Memory (LSTM)
units (Hochreiter and Schmidhuber, 1997; Gers et
al., 2000) in place of vanilla RNN units. The t-
th LSTM unit consists of several gates and two
different types of states: a hidden unit ht ∈ Rd×1
and a memory cell ct ∈ Rd×1,

it = σ(W (i)xt + U (i)ht−1 + b(i)),
ft = σ(W (f )xt + U (f )ht−1 + b(f )),
ot = σ(W (o)xt + U (o)ht−1 + b(o)),
˜ct = tanh(W (˜c)xt + U (˜c)ht−1 + b(˜c)),
ct = it (cid:12) ˜ct + ft (cid:12) ct−1,
ht = ot (cid:12) tanh(ct),

(4)

where each of it, ft, ot and ˜ct ∈ Rd×1 denotes
an input gate, a forget gate, an output gate, and a
state for updating the memory cell, respectively.
W (·) ∈ Rd×d and U (·) ∈ Rd×d are weight matri-
ces, b(·) ∈ Rd×1 is a bias vector, and xt ∈ Rd×1
is the word embedding of the t-th input word. σ(·)
is the logistic function, and the operator (cid:12) denotes
element-wise multiplication between vectors.

2.2 Attentional Encoder-Decoder Model

The NMT models with an attention mecha-
nism (Bahdanau et al., 2015; Luong et al., 2015a)
have been proposed to softly align each decoder
state with the encoder states. The attention mech-
anism allows the NMT models to explicitly quan-
tify how much each encoder state contributes to
the word prediction at each time step.

In the attentional NMT model in Luong et al.
(2015a), at the j-th step of the decoder process,
the attention score αj(i) between the i-th source
hidden unit hi and the j-th target hidden unit sj is
calculated as follows:

αj(i) =

exp(hi · sj)
k=1 exp(hk · sj)

,

(cid:80)n

(5)

where hi · sj is the inner product of hi and sj,
which is used to directly calculate the similarity
score between hi and sj. The j-th context vector

Figure 2: Attentional Encoder-Decoder model.

dj is calculated as the summation vector weighted
by αj(i):

dj =

αj(i)hi.

(6)

n
(cid:88)

i=1

To incorporate the attention mechanism into the
decoding process, the context vector is used for the
the j-th word prediction by putting an additional
hidden layer ˜sj:

˜sj = tanh(Wd[sj; dj] + bd),

(7)

where [sj; dj] ∈ R2d×1 is the concatenation of sj
and dj, and Wd ∈ Rd×2d and bd ∈ Rd×1 are a
weight matrix and a bias vector, respectively. The
model predicts the j-th word by using the softmax
function:

p(yj|y<j, x) = softmax(Ws ˜sj + bs),

(8)

where Ws ∈ R|V |×d and bs ∈ R|V |×1 are a weight
matrix and a bias vector, respectively. |V | stands
for the size of the vocabulary of the target lan-
guage. Figure 2 shows an example of the NMT
model with the attention mechanism.

2.3 Objective Function of NMT Models

The objective function to train the NMT models
is the sum of the log-likelihoods of the translation
pairs in the training data:

J(θ) =

log p(y|x),

(9)

(cid:88)

1
|D|

(x,y)∈D

where D denotes a set of parallel sentence pairs.
The model parameters θ are learned through
Stochastic Gradient Descent (SGD).

3 Attentional Tree-to-Sequence Model

3.1 Tree-based Encoder + Sequential

Encoder

The exsiting NMT models treat a sentence as a
sequence of words and neglect the structure of

Figure 3: Proposed model: Tree-to-sequence at-
tentional NMT model.

a sentence inherent in language. We propose a
novel tree-based encoder in order to explicitly take
the syntactic structure into consideration in the
NMT model. We focus on the phrase structure of
a sentence and construct a sentence vector from
phrase vectors in a bottom-up fashion. The sen-
tence vector in the tree-based encoder is there-
fore composed of the structural information rather
than the sequential data. Figure 3 shows our pro-
posed model, which we call a tree-to-sequence at-
tentional NMT model.

In Head-driven Phrase Structure Grammar
(HPSG) (Sag et al., 2003), a sentence is composed
of multiple phrase units and represented as a bi-
nary tree as shown in Figure 1. Following the
structure of the sentence, we construct a tree-based
encoder on top of the standard sequential encoder.
The k-th parent hidden unit h(phr)
for the k-th
phrase is calculated using the left and right child
hidden units hl

k as follows:

k and hr

k

h(phr)
k

= ftree(hl

k, hr

k),

(10)

where ftree is a non-linear function.

We construct a tree-based encoder with LSTM
units, where each node in the binary tree is repre-
sented with an LSTM unit. When initializing the
leaf units of the tree-based encoder, we employ the
sequential LSTM units described in Section 2.1.
Each non-leaf node is also represented with an
LSTM unit, and we employ Tree-LSTM (Tai et
al., 2015) to calculate the LSTM unit of the par-
ent node which has two child LSTM units. The
hidden unit h(phr)
∈ Rd×1 and the memory cell
c(phr)
∈ Rd×1 for the k-th parent node are calcu-
k

k

lated as follows:

k + b(fl)),
k + b(fr)),

k + b(i)),

ik = σ(U (i)
l hl
k = σ(U (fl)
l hl
f l
k = σ(U (fr)
l hl
f r
ok = σ(U (o)
l hl
˜ck = tanh(U (˜c)

r hr
k + U (i)
(fl)
r hr
k + U
k + U (fr)
r hr
k + b(o)),
k + U (o)
r hr
r hr
k + U (˜c)
l hl
= ik (cid:12) ˜ck + f l
k + f r
k (cid:12) cl
= ok (cid:12) tanh(c(phr)
),

k + b(˜c)),
k (cid:12) cr
k,

k

c(phr)
k
h(phr)
k

(11)

k, f r

k , oj, ˜cj ∈ Rd×1 are an input
where ik, f l
gate, the forget gates for left and right child units,
an output gate, and a state for updating the mem-
ory cell, respectively. cl
k are the mem-
ory cells for the left and right child units, respec-
tively. U (·) ∈ Rd×d denotes a weight matrix, and
b(·) ∈ Rd×1 represents a bias vector.

k and cr

Our proposed tree-based encoder is a natural
extension of the conventional sequential encoder,
since Tree-LSTM is a generalization of chain-
structured LSTM (Tai et al., 2015). Our encoder
differs from the original Tree-LSTM in the cal-
culation of the LSTM units for the leaf nodes.
The motivation is to construct the phrase nodes in
a context-sensitive way, which, for example, al-
lows the model to compute different representa-
tions for multiple occurrences of the same word in
a sentence because the sequential LSTMs are cal-
culated in the context of the previous units. This
ability contrasts with the original Tree-LSTM, in
which the leaves are composed only of the word
embeddings without any contextual information.

3.2

Initial Decoder Setting

We now have two different sentence vectors: one
is from the sequence encoder and the other from
the tree-based encoder. As shown in Figure 3, we
provide another Tree-LSTM unit which has the ﬁ-
nal sequential encoder unit (hn) and the tree-based
encoder unit (h(phr)
root ) as two child units and set it
as the initial decoder s1 as follows:

where gtree is the same function as ftree with an-
other set of Tree-LSTM parameters. This initial-
ization allows the decoder to capture information
from both the sequential data and phrase struc-
tures. Zoph and Knight (2016) proposed a simi-
lar method using a Tree-LSTM for initializing the

decoder, with which they translate multiple source
languages to one target language. When the syn-
tactic parser fails to output a parse tree for a sen-
tence, we encode the sentence with the sequential
encoder by setting h(phr)
root = 0. Our proposed tree-
based encoder therefore works with any sentences.

3.3 Attention Mechanism in Our Model

We adopt the attention mechanism into our tree-
to-sequence model in a novel way. Our model
gives attention not only to sequential hidden units
but also to phrase hidden units. This attention
mechanism tells us which words or phrases in the
source sentence are important when the model de-
codes a target word. The j-th context vector dj
is composed of the sequential and phrase vectors
weighted by the attention score αj(i):

dj =

αj(i)hi +

αj(i)h(phr)
i

.

(13)

n
(cid:88)

i=1

2n−1
(cid:88)

i=n+1

In addition, we

Note that a binary tree has n − 1 phrase nodes if
the tree has n leaves. We set a ﬁnal decoder ˜sj in
the same way as Equation (7).
adopt

input-feeding
method (Luong et al., 2015a) in our model, which
is a method for feeding ˜sj−1, the previous unit
to predict the word yj−1, into the current target
hidden unit sj,

the

sj = fdec(yj−1, [sj−1; ˜sj−1]),

(14)

where [sj−1; ˜sj−1] is the concatenation of sj−1
and ˜sj−1. The input-feeding approach contributes
to the enrichment in the calculation of the decoder,
because ˜sj−1 is an informative unit which can be
used to predict the output word as well as to be
compacted with attentional context vectors. Lu-
ong et al. (2015a) showed that the input-feeding
approach improves BLEU scores. We also ob-
served the same improvement in our preliminary
experiments.

3.4 Sampling-Based Approximation to the

The biggest computational bottleneck of train-
ing the NMT models is in the calculation of the
softmax layer described in Equation (8), because
its computational cost increases linearly with the
size of the vocabulary. The speedup technique
with GPUs has proven useful for sequence-based
NMT models (Sutskever et al., 2014; Luong et al.,

s1 = gtree(hn, h(phr)

root ),

(12)

NMT Models

2015a) but it is not easily applicable when deal-
ing with tree-structured data. In order to reduce
the training cost of the NMT models at the soft-
max layer, we employ BlackOut (Ji et al., 2016), a
sampling-based approximation method. BlackOut
has been shown to be effective in RNN Language
Models (RNNLMs) and allows a model to run rea-
sonably fast even with a million word vocabulary
with CPUs.

At each word prediction step in the training,
BlackOut estimates the conditional probability in
Equation (2) for the target word and K neg-
ative samples using a weighted softmax func-
tion. The negative samples are drawn from the
unigram distribution raised to the power β ∈
[0, 1] (Mikolov et al., 2013). The unigram dis-
tribution is estimated using the training data and
β is a hyperparameter. BlackOut is closely re-
lated to Noise Contrastive Estimation (NCE) (Gut-
mann and Hyv¨arinen, 2012) and achieves better
perplexity than the original softmax and NCE in
RNNLMs. The advantages of Blackout over the
other methods are discussed in Ji et al. (2016).
Note that BlackOut can be used as the original
softmax once the training is ﬁnished.

4 Experiments

4.1 Training Data

We applied the proposed model to the English-to-
Japanese translation dataset of the ASPEC corpus
given in WAT’15.1 Following Zhu (2015), we ex-
tracted the ﬁrst 1.5 million translation pairs from
the training data. To obtain the phrase structures
of the source sentences, i.e., English, we used the
probabilistic HPSG parser Enju (Miyao and Tsu-
jii, 2008). We used Enju only to obtain a binary
phrase structure for each sentence and did not use
any HPSG speciﬁc information. For the target
language, i.e., Japanese, we used KyTea (Neubig
et al., 2011), a Japanese segmentation tool, and
performed the pre-processing steps recommended
in WAT’15.2 We then ﬁltered out the translation
pairs whose sentence lengths are longer than 50
and whose source sentences are not parsed suc-
cessfully. Table 1 shows the details of the datasets
used in our experiments. We carried out two ex-
periments on a small training dataset to investigate

1http://orchid.kuee.kyoto-u.ac.jp/WAT/

WAT2015/index.html

2http://orchid.kuee.kyoto-u.ac.jp/WAT/

WAT2015/baseline/dataPreparationJE.html

Train
Development
Test

Sentences Parsed successfully
1,346,946
1,346,946
1,789
1,790
1,811
1,812

Table 1: Dataset in ASPEC corpus.

sentence pairs
|V | in English
|V | in Japanese

Train (small) Train (large)
1,346,946
87,796
65,680

100,000
25,478
23,532

Table 2: Training dataset and the vocabulary sizes.

the effectiveness of our proposed model and on
a large training dataset to compare our proposed
methods with the other systems.

The vocabulary consists of words observed in
the training data more than or equal to N times.
We set N = 2 for the small training dataset and
N = 5 for the large training dataset. The out-of-
vocabulary words are mapped to the special token
“unk”. We added another special symbol “eos” for
both languages and inserted it at the end of all the
sentences. Table 2 shows the details of each train-
ing dataset and its corresponding vocabulary size.

4.2 Training Details

The biases,
softmax weights, and BlackOut
weights are initialized with zeros. The hyperpa-
rameter β of BlackOut is set to 0.4 as recom-
mended by Ji et al. (2016). Following J´ozefowicz
et al. (2015), we initialize the forget gate biases of
LSTM and Tree-LSTM with 1.0. The remaining
model parameters in the NMT models in our ex-
periments are uniformly initialized in [−0.1, 0.1].
The model parameters are optimized by plain SGD
with the mini-batch size of 128. The initial learn-
ing rate of SGD is 1.0. We halve the learning rate
when the development loss becomes worse. Gra-
dient norms are clipped to 3.0 to avoid exploding
gradient problems (Pascanu et al., 2012).

Small Training Dataset We conduct experi-
ments with our proposed model and the sequential
attentional NMT model with the input-feeding ap-
proach. Each model has 256-dimensional hidden
units and word embeddings. The number of nega-
tive samples K of BlackOut is set to 500 or 2000.

Large Training Dataset Our proposed model
has 512-dimensional word embeddings and d-
dimensional hidden units (d ∈ {512, 768, 1024}).
K is set to 2500.

Our code3 is implemented in C++ using the
Eigen library,4 a template library for linear alge-
bra, and we run all of the experiments on multi-
core CPUs.5 It takes about a week to train a model
on the large training dataset with d = 512.

4.3 Decoding process

We use beam search to decode a target sentence
for an input sentence x and calculate the sum
of the log-likelihoods of the target sentence y =
(y1, · · · , ym) as the beam score:

score(x, y) =

log p(yj|y<j, x).

(15)

m
(cid:88)

j=1

Decoding in the NMT models is a generative pro-
cess and depends on the target language model
given a source sentence. The score becomes
smaller as the target sentence becomes longer, and
thus the simple beam search does not work well
when decoding a long sentence (Cho et al., 2014a;
Pouget-Abadie et al., 2014).
In our preliminary
experiments, the beam search with the length nor-
malization in Cho et al. (2014a) was not effective
in English-to-Japanese translation. The method
in Pouget-Abadie et al. (2014) needs to estimate
the conditional probability p(x|y) using another
NMT model and thus is not suitable for our work.
In this paper, we use statistics on sentence
lengths in beam search. Assuming that the length
of a target sentence correlates with the length of
a source sentence, we redeﬁne the score of each
candidate as follows:

m
(cid:88)

j=1

score(x, y) = Lx,y +

log p(yj|y<j, x),(16)

Lx,y = log p(len(y)|len(x)),

(17)

where Lx,y is the penalty for the conditional prob-
ability of the target sentence length len(y) given
the source sentence length len(x).
It allows
the model to decode a sentence by considering
the length of the target sentence.
In our exper-
iments, we computed the conditional probability

3https://github.com/tempra28/tree2seq
4http://eigen.tuxfamily.org/index.php
516 threads on Intel(R) Xeon(R) CPU E5-2667 v3 @

3.20GHz

p(len(y)|len(x)) in advance following the statis-
tics collected in the ﬁrst one million pairs of the
training dataset. We allow the decoder to generate
up to 100 words.

4.4 Evaluation

We evaluated the models by two automatic eval-
uation metrics, RIBES (Isozaki et al., 2010) and
BLEU (Papineni et al., 2002) following WAT’15.
We used the KyTea-based evaluation script for the
translation results.6 The RIBES score is a metric
based on rank correlation coefﬁcients with word
precision, and the BLEU score is based on n-gram
word precision and a Brevity Penalty (BP) for out-
puts shorter than the references. RIBES is known
to have stronger correlation with human judge-
ments than BLEU in translation between English
and Japanese as discussed in Isozaki et al. (2010).

5 Results and Discussion

5.1 Small Training Dataset

Table 3 shows the perplexity, BLEU, RIBES, and
the training time on the development data with the
Attentional NMT (ANMT) models trained on the
small dataset. We conducted the experiments with
our proposed method using BlackOut and soft-
max. We decoded a translation by our proposed
beam search with a beam size of 20.

As shown in Table 3, the results of our proposed
model with BlackOut improve as the number of
negative samples K increases. Although the result
of softmax is better than those of BlackOut (K =
500, 2000), the training time of softmax per epoch
is about three times longer than that of BlackOut
even with the small dataset.

As to the results of the ANMT model, reversing
the word order in the input sentence decreases the
scores in English-to-Japanese translation, which
contrasts with the results of other language pairs
reported in previous work (Sutskever et al., 2014;
Luong et al., 2015a). By taking syntactic infor-
mation into consideration, our proposed model
improves the scores, compared to the sequential
attention-based approach.

We found that better perplexity does not always
lead to better translation scores with BlackOut as
shown in Table 3. One of the possible reasons is
that BlackOut distorts the target word distribution

6http://lotus.kuee.kyoto-u.ac.jp/WAT/
evaluation/automatic_evaluation_systems/
automaticEvaluationJA.html

Proposed model
Proposed model
Proposed model (Softmax)
ANMT (Luong et al., 2015a)
+ reverse input
ANMT (Luong et al., 2015a)
+ reverse input

K
500
2000
—
500
500
2000
2000

Perplexity RIBES BLEU Time/epoch (min.)
20.0
20.5
21.8
18.5
17.7
19.4
17.5

19.6
21.0
17.9
21.6
22.6
23.1
26.1

71.8
72.6
73.2
70.7
69.8
71.5
69.5

55
70
180
45
45
60
60

Table 3: Evaluation results on the development data using the small training data. The training time per
epoch is also shown, and K is the number of negative samples in BlackOut.

Simple BS

Proposed BS

Beam size RIBES BLEU (BP)
20.0 (90.1)
72.3
19.5 (85.1)
72.3
20.5 (91.7)
72.6

6
20
20

Without sequential LSTMs
With sequential LSTMs

RIBES BLEU
19.5
20.0

69.4
72.3

Table 4: Effects of the Beam Search (BS) on the
development data.

Table 5: Effects of the sequential LSTMs in our
proposed tree-based encoder on the development
data.

by the modiﬁed unigram-based negative sampling
where frequent words can be treated as the nega-
tive samples multiple times at each training step.

Effects of the proposed beam search Table 4
shows the results on the development data of pro-
posed method with BlackOut (K = 2000) by
the simple beam search and our proposed beam
search. The beam size is set to 6 or 20 in the sim-
ple beam search, and to 20 in our proposed search.
We can see that our proposed search outperforms
the simple beam search in both scores. Unlike
RIBES, the BLEU score is sensitive to the beam
size and becomes lower as the beam size increases.
We found that the BP had a relatively large im-
pact on the BLEU score in the simple beam search
as the beam size increased. Our search method
works better than the simple beam search by keep-
ing long sentences in the candidates with a large
beam size.

Effects of the sequential LSTM units We also
investigated the effects of the sequential LSTMs
at the leaf nodes in our proposed tree-based en-
coder. Table 5 shows the result on the develop-
ment data of our proposed encoder and that of an
attentional tree-based encoder without sequential
LSTMs with BlackOut (K = 2000).7 The results
show that our proposed encoder considerably out-

7For this evaluation, we used the 1,789 sentences that
were successfully parsed by Enju because the encoder with-
out sequential LSTMs always requires a parse tree.

performs the encoder without sequential LSTMs,
suggesting that the sequential LSTMs at the leaf
nodes contribute to the context-aware construction
of the phrase representations in the tree.

5.2 Large Training Dataset

Table 6 shows the experimental results of RIBES
and BLEU scores achieved by the trained models
on the large dataset. We decoded the target sen-
tences by our proposed beam search with the beam
size of 20.8 The results of the other systems are the
ones reported in Nakazawa et al. (2015).

All of our proposed models show similar per-
formance regardless of the value of d. Our ensem-
ble model is composed of the three models with
d = 512, 768, and 1024, and it shows the best
RIBES score among all systems.9

As for the time required for training, our im-
plementation needs about one day to perform one
epoch on the large training dataset with d = 512.
It would take about 11 days without using the
BlackOut sampling.

Comparison with the NMT models The model
of Zhu (2015) is an ANMT model (Bahdanau et
al., 2015) with a bi-directional LSTM encoder,
and uses 1024-dimensional hidden units and 1000-

8We found two sentences which ends without eos with
d = 512, and then we decoded it again with the beam size of
1000 following Zhu (2015).

9Our ensemble model yields a METEOR (Denkowski and

Lavie, 2014) score of 53.6 with language option “-l other”.

Model
Proposed model (d = 512)
Proposed model (d = 768)
Proposed model (d = 1024)
Ensemble of the above three models
ANMT with LSTMs (Zhu, 2015)
+ Ensemble, unk replacement
+ System combination,

3 pre-reordered ensembles

ANMT with GRUs (Lee et al., 2015)
+ character-based decoding,

Begin/Inside representation

RIBES BLEU
34.36
81.46
34.78
81.89
34.87
81.58
82.45
36.95
32.19
79.70
34.19
80.27

80.91

36.21

81.15

35.75

PB baseline
HPB baseline
T2S baseline
T2S model (Neubig and Duh, 2014)
+ ANMT Rerank (Neubig et al., 2015)

69.19
74.70
75.80
79.65
81.38

29.80
32.56
33.44
36.58
38.17

Table 6: Evaluation results on the test data.

dimensional word embeddings. The model of Lee
et al. (2015) is also an ANMT model with a bi-
directional Gated Recurrent Unit (GRU) encoder,
and uses 1000-dimensional hidden units and 200-
dimensional word embeddings. Both models are
sequential ANMT models. Our single proposed
model with d = 512 outperforms the best result of
Zhu (2015)’s end-to-end NMT model with ensem-
ble and unknown replacement by +1.19 RIBES
and by +0.17 BLEU scores. Our ensemble model
in both RIBES and
shows better performance,
BLEU scores, than that of Zhu (2015)’s best sys-
tem which is a hybrid of the ANMT and SMT
models by +1.54 RIBES and by +0.74 BLEU
scores and Lee et al. (2015)’s ANMT system
with special character-based decoding by +1.30
RIBES and +1.20 BLEU scores.

Comparison with the SMT models PB, HPB
and T2S are the baseline SMT systems in
WAT’15: a phrase-based model, a hierarchical
phrase-based model, and a tree-to-string model,
respectively (Nakazawa et al., 2015). The best
model in WAT’15 is Neubig et al. (2015)’s tree-
to-string SMT model enhanced with reranking by
ANMT using a bi-directional LSTM encoder. Our
proposed end-to-end NMT model compares favor-
ably with Neubig et al. (2015).

5.3 Qualitative Analysis

We illustrate the translations of test data by our
model with d = 512 and several attentional rela-
tions when decoding a sentence. In Figures 4 and
5, an English sentence represented as a binary tree
is translated into Japanese, and several attentional
relations between English words or phrases and

Figure 4: Translation example of a short sen-
tence and the attentional relations by our proposed
model.

Japanese word are shown with the highest atten-
tion score α. The additional attentional relations
are also illustrated for comparison. We can see the
target words softly aligned with source words and
phrases.

In Figure 4, the Japanese word “液晶” means
“liquid crystal”, and it has a high attention score
(α = 0.41) with the English phrase “liquid crystal
for active matrix”. This is because the j-th tar-
get hidden unit sj has the contextual information
about the previous words y<j including “活性 マ
トリックス の” (“for active matrix” in English).
The Japanese word “セル” is softly aligned with
the phrase “the cells” with the highest attention
score (α = 0.35). In Japanese, there is no deﬁ-
nite article like “the” in English, and it is usually
aligned with null described as Section 1.

In Figure 5, in the case of the Japanese word
“示” (“showed” in English), the attention score
with the English phrase “showed excellent perfor-
mance” (α = 0.25) is higher than that with the
English word “showed” (α = 0.01). The Japanese
word “の” (“of” in English) is softly aligned with
the phrase “of Si dot MOS capacitor” with the
highest attention score (α = 0.30). It is because
our attention mechanism takes each previous con-
text of the Japanese phrases “優れ た 性能” (“ex-
cellent performance” in English) and “Ｓｉ ドッ
ト ＭＯＳ コンデンサ” (“Si dot MOS capaci-
tor” in English) into account and softly aligned the
target words with the whole phrase when trans-
lating the English verb “showed” and the prepo-
sition “of”. Our proposed model can thus ﬂexibly
learn the attentional relations between English and
Japanese.

We observed that our model translated the word
“active” into “活性”, a synonym of the reference
word “アクティブ”. We also found similar ex-

Figure 5: Translation example of a long sentence and the attentional relations by our proposed model.

amples in other sentences, where our model out-
puts synonyms of the reference words, e.g. “女”
and “女 性” (“female” in English) and “NASA”
and “航 空 宇 宙 局” (“National Aeronautics and
Space Administration” in English). These trans-
lations are penalized in terms of BLEU scores, but
they do not necessarily mean that the translations
were wrong. This point may be supported by the
fact that the NMT models were highly evaluated
in WAT’15 by crowd sourcing (Nakazawa et al.,
2015).

enables the NMT models to translate while align-
ing the target with the source. Luong et al. (2015a)
reﬁned the attention model so that it can dynami-
cally focus on local windows rather than the entire
sentence. They also proposed a more effective at-
tentional path in the calculation of ANMT models.
Subsequently, several ANMT models have been
proposed (Cheng et al., 2016; Cohn et al., 2016);
however, each model is based on the existing se-
quential attentional models and does not focus on
a syntactic structure of languages.

6 Related Work

7 Conclusion

Kalchbrenner and Blunsom (2013) were the ﬁrst
to propose an end-to-end NMT model using Con-
volutional Neural Networks (CNNs) as the source
encoder and using RNNs as the target decoder.
The Encoder-Decoder model can be seen as an ex-
tension of their model, and it replaces the CNNs
with RNNs using GRUs (Cho et al., 2014b) or
LSTMs (Sutskever et al., 2014).

Sutskever et al. (2014) have shown that mak-
ing the input sequences reversed is effective in a
French-to-English translation task, and the tech-
nique has also proven effective in translation tasks
between other European language pairs (Luong et
al., 2015a). All of the NMT models mentioned
above are based on sequential encoders. To incor-
porate structural information into the NMT mod-
els, Cho et al. (2014a) proposed to jointly learn
structures inherent in source-side languages but
did not report improvement of translation perfor-
mance. These studies motivated us to investigate
the role of syntactic structures explicitly given by
existing syntactic parsers in the NMT models.

The attention mechanism (Bahdanau et al.,
2015) has promoted NMT onto the next stage. It

In this paper, we propose a novel syntactic ap-
proach that extends attentional NMT models. We
focus on the phrase structure of the input sen-
tence and build a tree-based encoder following
the parsed tree. Our proposed tree-based encoder
is a natural extension of the sequential encoder
model, where the leaf units of the tree-LSTM
in the encoder can work together with the origi-
nal sequential LSTM encoder. Moreover, the at-
tention mechanism allows the tree-based encoder
to align not only the input words but also input
phrases with the output words. Experimental re-
sults on the WAT’15 English-to-Japanese transla-
tion dataset demonstrate that our proposed model
achieves the best RIBES score and outperforms
the sequential attentional NMT model.

Acknowledgments

We thank the anonymous reviewers for their con-
structive comments and suggestions. This work
was supported by CREST, JST, and JSPS KAK-
ENHI Grant Number 15J12597.

References

[Bahdanau et al.2015] Dzmitry Bahdanau, Kyunghyun
Cho, and Yoshua Bengio. 2015. Neural Machine
Translation by Jointly Learning to Align and Trans-
late. In Proceedings of the 3rd International Con-
ference on Learning Representations.

[Cheng et al.2016] Yong Cheng, Shiqi Shen, Zhongjun
He, Wei He, Hua Wu, Maosong Sun, and Yang Liu.
2016. Agreement-based Joint Training for Bidirec-
tional Attention-based Neural Machine Translation.
In Proceedings of the 25th International Joint Con-
ference on Artiﬁcial Intelligence. to appear.

[Cho et al.2014a] KyungHyun Cho, Bart van Merrien-
boer, Dzmitry Bahdanau, and Yoshua Bengio.
2014a. On the Properties of Neural Machine Trans-
lation: Encoder-Decoder Approaches. In Proceed-
ings of Eighth Workshop on Syntax, Semantics and
Structure in Statistical Translation (SSST-8).

[Cho et al.2014b] Kyunghyun Cho, Bart van Merrien-
boer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi
Bougares, Holger Schwenk, and Yoshua Bengio.
2014b. Learning Phrase Representations using RNN
Encoder–Decoder for Statistical Machine Transla-
In Proceedings of the 2014 Conference on
tion.
Empirical Methods in Natural Language Process-
ing, pages 1724–1734.

[Cohn et al.2016] Trevor Cohn, Cong Duy Vu Hoang,
Ekaterina Vymolova, Kaisheng Yao, Chris Dyer,
and Gholamreza Haffari.
Incorporating
Structural Alignment Biases into an Attentional
In Proceedings of the
Neural Translation Model.
15th Annual Conference of
the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies. to appear.

2016.

[Denkowski and Lavie2014] Michael Denkowski and
Alon Lavie. 2014. Meteor universal: Language spe-
ciﬁc translation evaluation for any target language.
In Proceedings of the 14th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics 2014 Workshop on Statistical Machine
Translation.

[Gers et al.2000] Felix A. Gers, J¨urgen Schmidhuber,
and Fred A. Cummins. 2000. Learning to Forget:
Continual Prediction with LSTM. Neural Computa-
tion, 12(10):2451–2471.

[Gutmann and Hyv¨arinen2012] Michael U. Gutmann
and Aapo Hyv¨arinen. 2012. Noise-contrastive esti-
mation of unnormalized statistical models, with ap-
plications to natural image statistics. Journal of Ma-
chine Learning Research, 13(1):307–361.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter
and J¨urgen Schmidhuber. 1997. Long Short-Term
Memory. Neural Computation, 9(8):1735–1780.

[Isozaki et al.2010] Hideki

Isozaki, Tsutomu Hirao,
Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada.
2010. Automatic Evaluation of Translation Quality

In Proceedings of the
for Distant Language Pairs.
2010 Conference on Empirical Methods in Natural
Language Processing, pages 944–952.

[Ji et al.2016] Shihao Ji, S. V. N. Vishwanathan, Na-
dathur Satish, Michael J. Anderson, and Pradeep
Dubey. 2016. BlackOut: Speeding up Recurrent
Neural Network Language Models With Very Large
In Proceedings of the 4th Interna-
Vocabularies.
tional Conference on Learning Representations.

[J´ozefowicz et al.2015] Rafal

J´ozefowicz, Wojciech
Zaremba, and Ilya Sutskever. 2015. An Empiri-
cal Exploration of Recurrent Network Architectures.
In Proceedings of the 32nd International Confer-
ence on Machine Learning, volume 37, pages 2342–
2350.

[Kalchbrenner and Blunsom2013] Nal Kalchbrenner
and Phil Blunsom. 2013. Recurrent continuous
In Proceedings of the 2013
translation models.
Conference on Empirical Methods
in Natural
Language Processing, pages 1700–1709.

[Lee et al.2015] Hyoung-Gyu Lee, JaeSong Lee, Jun-
Seok Kim, and Chang-Ki Lee. 2015. NAVER Ma-
In Pro-
chine Translation System for WAT 2015.
ceedings of the 2nd Workshop on Asian Translation
(WAT2015), pages 69–73.

[Liu et al.2006] Yang Liu, Qun Liu, and Shouxun Lin.
2006. Tree-to-string alignment template for statisti-
cal machine translation. In Proceedings of the 21st
International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 609–616.

[Luong et al.2015a] Thang Luong, Hieu Pham, and
2015a. Effective Ap-
Christopher D. Manning.
proaches to Attention-based Neural Machine Trans-
In Proceedings of the 2015 Conference on
lation.
Empirical Methods in Natural Language Process-
ing, pages 1412–1421.

[Luong et al.2015b] Thang Luong,

Ilya Sutskever,
Quoc Le, Oriol Vinyals, and Wojciech Zaremba.
2015b. Addressing the Rare Word Problem in Neu-
ral Machine Translation. In Proceedings of the 53rd
Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 11–19.

[Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases
In Advances in Neu-
and their compositionality.
ral Information Processing Systems 26, pages 3111–
3119.

[Miyao and Tsujii2008] Yusuke Miyao and Jun’ichi
Tsujii. 2008. Feature Forest Models for Proba-
bilistic HPSG Parsing. Computational Linguistics,
34(1):35–80.

Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 1556–1566.

[Yamada and Knight2001] Kenji Yamada and Kevin
Knight. 2001. A syntax-based statistical translation
model. In Proceedings of the 39th Annual Meeting
on Association for Computational Linguistics, pages
523–530.

[Zhu2015] Zhongyuan Zhu. 2015. Evaluating Neural
In
Machine Translation in English-Japanese Task.
Proceedings of the 2nd Workshop on Asian Transla-
tion (WAT2015), pages 61–68.

[Zoph and Knight2016] Barret Zoph and Kevin Knight.
In Pro-
2016. Multi-Source Neural Translation.
ceedings of the 15th Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
to appear.

[Nakazawa et al.2015] Toshiaki Nakazawa, Hideya
Mino, Isao Goto, Graham Neubig, Sadao Kuro-
hashi, and Eiichiro Sumita. 2015. Overview of
In Pro-
the 2nd Workshop on Asian Translation.
ceedings of the 2nd Workshop on Asian Translation
(WAT2015), pages 1–28.

[Neubig and Duh2014] Graham Neubig and Kevin
Duh. 2014. On the elements of an accurate tree-
In Proceed-
to-string machine translation system.
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 143–149.

[Neubig et al.2011] Graham Neubig, Yosuke Nakata,
and Shinsuke Mori. 2011. Pointwise Prediction for
Robust, Adaptable Japanese Morphological Analy-
sis. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 529–533.

[Neubig et al.2015] Graham Neubig, Makoto Mor-
ishita, and Satoshi Nakamura.
2015. Neural
Reranking Improves Subjective Quality of Machine
Translation: NAIST at WAT2015. In Proceedings of
the 2nd Workshop on Asian Translation (WAT2015),
pages 35–41.

[Papineni et al.2002] Kishore Papineni, Salim Roukos,
2002. BLEU:
Todd Ward, and Wei-Jing Zhu.
A Method for Automatic Evaluation of Machine
In Proceedings of the 40th Annual
Translation.
Meeting on Association for Computational Linguis-
tics, pages 311–318.

[Pascanu et al.2012] Razvan Pascanu, Tomas Mikolov,
and Yoshua Bengio. 2012. Understanding the ex-
ploding gradient problem. arXiv: 1211.5063.

[Pouget-Abadie et al.2014] Jean
Bart

Pouget-Abadie,
van Merrienboer,
Dzmitry Bahdanau,
Kyunghyun Cho, and Yoshua Bengio.
2014.
Overcoming the curse of sentence length for neural
machine translation using automatic segmentation.
In Proceedings of SSST-8, Eighth Workshop on
Syntax, Semantics and Structure in Statistical
Translation, pages 78–85.

[Sag et al.2003] Ivan A. Sag, Thomas Wasow, and
Emily Bender. 2003. Syntactic Theory: A Formal
Introduction. Center for the Study of Language and
Information, Stanford, 2nd edition.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals,
2014. Sequence to Sequence
and Quoc V. Le.
In Advances in
Learning with Neural Networks.
Neural Information Processing Systems 27, pages
3104–3112.

[Tai et al.2015] Kai Sheng Tai, Richard Socher, and
Christopher D. Manning. 2015. Improved Semantic
Representations From Tree-Structured Long Short-
In Proceedings of the
Term Memory Networks.
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint

Tree-to-Sequence Attentional Neural Machine Translation

Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa Tsuruoka
The University of Tokyo, 3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan
{eriguchi, hassy, tsuruoka}@logos.t.u-tokyo.ac.jp

6
1
0
2
 
n
u
J
 
8
 
 
]
L
C
.
s
c
[
 
 
3
v
5
7
0
6
0
.
3
0
6
1
:
v
i
X
r
a

Abstract

Most of
the existing Neural Machine
Translation (NMT) models focus on the
conversion of sequential data and do
not directly use syntactic information.
We propose a novel end-to-end syntac-
tic NMT model, extending a sequence-
to-sequence model with the source-side
phrase structure. Our model has an at-
tention mechanism that enables the de-
coder to generate a translated word while
softly aligning it with phrases as well as
words of the source sentence. Experi-
mental results on the WAT’15 English-
to-Japanese dataset demonstrate that our
proposed model considerably outperforms
sequence-to-sequence attentional NMT
models and compares favorably with the
state-of-the-art tree-to-string SMT system.

1

Introduction

Machine Translation (MT) has traditionally been
one of the most complex language processing
problems, but recent advances of Neural Machine
Translation (NMT) make it possible to perform
translation using a simple end-to-end architecture.
In the Encoder-Decoder model (Cho et al., 2014b;
Sutskever et al., 2014), a Recurrent Neural Net-
work (RNN) called the encoder reads the whole
sequence of source words to produce a ﬁxed-
length vector, and then another RNN called the
decoder generates the target words from the vec-
tor. The Encoder-Decoder model has been ex-
tended with an attention mechanism (Bahdanau et
al., 2015; Luong et al., 2015a), which allows the
model to jointly learn the soft alignment between
the source language and the target language. NMT
models have achieved state-of-the-art results in
English-to-French and English-to-German trans-

Figure 1: Alignment between an English phrase
and a Japanese word.

lation tasks (Luong et al., 2015b; Luong et al.,
2015a). However, it is yet to be seen whether
NMT is competitive with traditional Statistical
Machine Translation (SMT) approaches in trans-
lation tasks for structurally distant language pairs
such as English-to-Japanese.

Figure 1 shows a pair of parallel sentences in
English and Japanese. English and Japanese are
linguistically distant in many respects; they have
different syntactic constructions, and words and
In
phrases are deﬁned in different lexical units.
this example, the Japanese word “緑茶” is aligned
with the English words “green” and “tea”, and
the English word sequence “a cup of” is aligned
with a special symbol “null”, which is not explic-
itly translated into any Japanese words. One way
to solve this mismatch problem is to consider the
phrase structure of the English sentence and align
the phrase “a cup of green tea” with “緑茶”. In
SMT, it is known that incorporating syntactic con-
stituents of the source language into the models
improves word alignment (Yamada and Knight,
2001) and translation accuracy (Liu et al., 2006;
Neubig and Duh, 2014). However, the existing
NMT models do not allow us to perform this kind
of alignment.

In this paper, we propose a novel attentional
NMT model to take advantage of syntactic infor-

mation. Following the phrase structure of a source
sentence, we encode the sentence recursively in a
bottom-up fashion to produce a vector representa-
tion of the sentence and decode it while aligning
the input phrases and words with the output. Our
experimental results on the WAT’15 English-to-
Japanese translation task show that our proposed
model achieves state-of-the-art translation accu-
racy.

2 Neural Machine Translation

2.1 Encoder-Decoder Model

NMT is an end-to-end approach to data-driven
machine translation (Kalchbrenner and Blunsom,
2013; Sutskever et al., 2014; Bahdanau et al.,
2015). In other words, the NMT models directly
estimate the conditional probability p(y|x) given
a large collection of source and target sentence
pairs (x, y). An NMT model consists of an en-
coder process and a decoder process, and hence
they are often called Encoder-Decoder models. In
the Encoder-Decoder models, a sentence is treated
In the encoder pro-
as a sequence of words.
cess, the encoder embeds each of the source words
x = (x1, x2, · · · , xn) into a d-dimensional vector
space. The decoder then outputs a word sequence
y = (y1, y2, · · · , ym) in the target language given
the information on the source sentence provided
by the encoder. Here, n and m are the lengths
of the source and target sentences, respectively.
RNNs allow one to effectively embed sequential
data into the vector space.

In the RNN encoder, the i-th hidden unit hi ∈
Rd×1 is calculated given the i-th input xi and the
previous hidden unit hi−1 ∈ Rd×1,

hi = fenc(xi, hi−1),

(1)

where fenc is a non-linear function, and the initial
hidden unit h0 is usually set to zeros. The encod-
ing function fenc is recursively applied until the n-
th hidden unit hn is obtained. The RNN Encoder-
Decoder models assume that hn represents a vec-
tor of the meaning of the input sequence up to the
n-th word.

After encoding the whole input sentence into
the vector space, we decode it in a similar way.
The initial decoder unit s1 is initialized with the
input sentence vector (s1 = hn). Given the pre-
vious target word and the j-th hidden unit of the
decoder, the conditional probability that the j-th

target word is generated is calculated as follows:

p(yj|y<j, x) = g(sj),

(2)

where g is a non-linear function. The j-th hidden
unit of the decoder is calculated by using another
non-linear function fdec as follows:

sj = fdec(yj−1, sj−1).

(3)

We employ Long Short-Term Memory (LSTM)
units (Hochreiter and Schmidhuber, 1997; Gers et
al., 2000) in place of vanilla RNN units. The t-
th LSTM unit consists of several gates and two
different types of states: a hidden unit ht ∈ Rd×1
and a memory cell ct ∈ Rd×1,

it = σ(W (i)xt + U (i)ht−1 + b(i)),
ft = σ(W (f )xt + U (f )ht−1 + b(f )),
ot = σ(W (o)xt + U (o)ht−1 + b(o)),
˜ct = tanh(W (˜c)xt + U (˜c)ht−1 + b(˜c)),
ct = it (cid:12) ˜ct + ft (cid:12) ct−1,
ht = ot (cid:12) tanh(ct),

(4)

where each of it, ft, ot and ˜ct ∈ Rd×1 denotes
an input gate, a forget gate, an output gate, and a
state for updating the memory cell, respectively.
W (·) ∈ Rd×d and U (·) ∈ Rd×d are weight matri-
ces, b(·) ∈ Rd×1 is a bias vector, and xt ∈ Rd×1
is the word embedding of the t-th input word. σ(·)
is the logistic function, and the operator (cid:12) denotes
element-wise multiplication between vectors.

2.2 Attentional Encoder-Decoder Model

The NMT models with an attention mecha-
nism (Bahdanau et al., 2015; Luong et al., 2015a)
have been proposed to softly align each decoder
state with the encoder states. The attention mech-
anism allows the NMT models to explicitly quan-
tify how much each encoder state contributes to
the word prediction at each time step.

In the attentional NMT model in Luong et al.
(2015a), at the j-th step of the decoder process,
the attention score αj(i) between the i-th source
hidden unit hi and the j-th target hidden unit sj is
calculated as follows:

αj(i) =

exp(hi · sj)
k=1 exp(hk · sj)

,

(cid:80)n

(5)

where hi · sj is the inner product of hi and sj,
which is used to directly calculate the similarity
score between hi and sj. The j-th context vector

Figure 2: Attentional Encoder-Decoder model.

dj is calculated as the summation vector weighted
by αj(i):

dj =

αj(i)hi.

(6)

n
(cid:88)

i=1

To incorporate the attention mechanism into the
decoding process, the context vector is used for the
the j-th word prediction by putting an additional
hidden layer ˜sj:

˜sj = tanh(Wd[sj; dj] + bd),

(7)

where [sj; dj] ∈ R2d×1 is the concatenation of sj
and dj, and Wd ∈ Rd×2d and bd ∈ Rd×1 are a
weight matrix and a bias vector, respectively. The
model predicts the j-th word by using the softmax
function:

p(yj|y<j, x) = softmax(Ws ˜sj + bs),

(8)

where Ws ∈ R|V |×d and bs ∈ R|V |×1 are a weight
matrix and a bias vector, respectively. |V | stands
for the size of the vocabulary of the target lan-
guage. Figure 2 shows an example of the NMT
model with the attention mechanism.

2.3 Objective Function of NMT Models

The objective function to train the NMT models
is the sum of the log-likelihoods of the translation
pairs in the training data:

J(θ) =

log p(y|x),

(9)

(cid:88)

1
|D|

(x,y)∈D

where D denotes a set of parallel sentence pairs.
The model parameters θ are learned through
Stochastic Gradient Descent (SGD).

3 Attentional Tree-to-Sequence Model

3.1 Tree-based Encoder + Sequential

Encoder

The exsiting NMT models treat a sentence as a
sequence of words and neglect the structure of

Figure 3: Proposed model: Tree-to-sequence at-
tentional NMT model.

a sentence inherent in language. We propose a
novel tree-based encoder in order to explicitly take
the syntactic structure into consideration in the
NMT model. We focus on the phrase structure of
a sentence and construct a sentence vector from
phrase vectors in a bottom-up fashion. The sen-
tence vector in the tree-based encoder is there-
fore composed of the structural information rather
than the sequential data. Figure 3 shows our pro-
posed model, which we call a tree-to-sequence at-
tentional NMT model.

In Head-driven Phrase Structure Grammar
(HPSG) (Sag et al., 2003), a sentence is composed
of multiple phrase units and represented as a bi-
nary tree as shown in Figure 1. Following the
structure of the sentence, we construct a tree-based
encoder on top of the standard sequential encoder.
The k-th parent hidden unit h(phr)
for the k-th
phrase is calculated using the left and right child
hidden units hl

k as follows:

k and hr

k

h(phr)
k

= ftree(hl

k, hr

k),

(10)

where ftree is a non-linear function.

We construct a tree-based encoder with LSTM
units, where each node in the binary tree is repre-
sented with an LSTM unit. When initializing the
leaf units of the tree-based encoder, we employ the
sequential LSTM units described in Section 2.1.
Each non-leaf node is also represented with an
LSTM unit, and we employ Tree-LSTM (Tai et
al., 2015) to calculate the LSTM unit of the par-
ent node which has two child LSTM units. The
hidden unit h(phr)
∈ Rd×1 and the memory cell
c(phr)
∈ Rd×1 for the k-th parent node are calcu-
k

k

lated as follows:

k + b(fl)),
k + b(fr)),

k + b(i)),

ik = σ(U (i)
l hl
k = σ(U (fl)
l hl
f l
k = σ(U (fr)
l hl
f r
ok = σ(U (o)
l hl
˜ck = tanh(U (˜c)

r hr
k + U (i)
(fl)
r hr
k + U
k + U (fr)
r hr
k + b(o)),
k + U (o)
r hr
r hr
k + U (˜c)
l hl
= ik (cid:12) ˜ck + f l
k + f r
k (cid:12) cl
= ok (cid:12) tanh(c(phr)
),

k + b(˜c)),
k (cid:12) cr
k,

k

c(phr)
k
h(phr)
k

(11)

k, f r

k , oj, ˜cj ∈ Rd×1 are an input
where ik, f l
gate, the forget gates for left and right child units,
an output gate, and a state for updating the mem-
ory cell, respectively. cl
k are the mem-
ory cells for the left and right child units, respec-
tively. U (·) ∈ Rd×d denotes a weight matrix, and
b(·) ∈ Rd×1 represents a bias vector.

k and cr

Our proposed tree-based encoder is a natural
extension of the conventional sequential encoder,
since Tree-LSTM is a generalization of chain-
structured LSTM (Tai et al., 2015). Our encoder
differs from the original Tree-LSTM in the cal-
culation of the LSTM units for the leaf nodes.
The motivation is to construct the phrase nodes in
a context-sensitive way, which, for example, al-
lows the model to compute different representa-
tions for multiple occurrences of the same word in
a sentence because the sequential LSTMs are cal-
culated in the context of the previous units. This
ability contrasts with the original Tree-LSTM, in
which the leaves are composed only of the word
embeddings without any contextual information.

3.2

Initial Decoder Setting

We now have two different sentence vectors: one
is from the sequence encoder and the other from
the tree-based encoder. As shown in Figure 3, we
provide another Tree-LSTM unit which has the ﬁ-
nal sequential encoder unit (hn) and the tree-based
encoder unit (h(phr)
root ) as two child units and set it
as the initial decoder s1 as follows:

where gtree is the same function as ftree with an-
other set of Tree-LSTM parameters. This initial-
ization allows the decoder to capture information
from both the sequential data and phrase struc-
tures. Zoph and Knight (2016) proposed a simi-
lar method using a Tree-LSTM for initializing the

decoder, with which they translate multiple source
languages to one target language. When the syn-
tactic parser fails to output a parse tree for a sen-
tence, we encode the sentence with the sequential
encoder by setting h(phr)
root = 0. Our proposed tree-
based encoder therefore works with any sentences.

3.3 Attention Mechanism in Our Model

We adopt the attention mechanism into our tree-
to-sequence model in a novel way. Our model
gives attention not only to sequential hidden units
but also to phrase hidden units. This attention
mechanism tells us which words or phrases in the
source sentence are important when the model de-
codes a target word. The j-th context vector dj
is composed of the sequential and phrase vectors
weighted by the attention score αj(i):

dj =

αj(i)hi +

αj(i)h(phr)
i

.

(13)

n
(cid:88)

i=1

2n−1
(cid:88)

i=n+1

In addition, we

Note that a binary tree has n − 1 phrase nodes if
the tree has n leaves. We set a ﬁnal decoder ˜sj in
the same way as Equation (7).
adopt

input-feeding
method (Luong et al., 2015a) in our model, which
is a method for feeding ˜sj−1, the previous unit
to predict the word yj−1, into the current target
hidden unit sj,

the

sj = fdec(yj−1, [sj−1; ˜sj−1]),

(14)

where [sj−1; ˜sj−1] is the concatenation of sj−1
and ˜sj−1. The input-feeding approach contributes
to the enrichment in the calculation of the decoder,
because ˜sj−1 is an informative unit which can be
used to predict the output word as well as to be
compacted with attentional context vectors. Lu-
ong et al. (2015a) showed that the input-feeding
approach improves BLEU scores. We also ob-
served the same improvement in our preliminary
experiments.

3.4 Sampling-Based Approximation to the

The biggest computational bottleneck of train-
ing the NMT models is in the calculation of the
softmax layer described in Equation (8), because
its computational cost increases linearly with the
size of the vocabulary. The speedup technique
with GPUs has proven useful for sequence-based
NMT models (Sutskever et al., 2014; Luong et al.,

s1 = gtree(hn, h(phr)

root ),

(12)

NMT Models

2015a) but it is not easily applicable when deal-
ing with tree-structured data. In order to reduce
the training cost of the NMT models at the soft-
max layer, we employ BlackOut (Ji et al., 2016), a
sampling-based approximation method. BlackOut
has been shown to be effective in RNN Language
Models (RNNLMs) and allows a model to run rea-
sonably fast even with a million word vocabulary
with CPUs.

At each word prediction step in the training,
BlackOut estimates the conditional probability in
Equation (2) for the target word and K neg-
ative samples using a weighted softmax func-
tion. The negative samples are drawn from the
unigram distribution raised to the power β ∈
[0, 1] (Mikolov et al., 2013). The unigram dis-
tribution is estimated using the training data and
β is a hyperparameter. BlackOut is closely re-
lated to Noise Contrastive Estimation (NCE) (Gut-
mann and Hyv¨arinen, 2012) and achieves better
perplexity than the original softmax and NCE in
RNNLMs. The advantages of Blackout over the
other methods are discussed in Ji et al. (2016).
Note that BlackOut can be used as the original
softmax once the training is ﬁnished.

4 Experiments

4.1 Training Data

We applied the proposed model to the English-to-
Japanese translation dataset of the ASPEC corpus
given in WAT’15.1 Following Zhu (2015), we ex-
tracted the ﬁrst 1.5 million translation pairs from
the training data. To obtain the phrase structures
of the source sentences, i.e., English, we used the
probabilistic HPSG parser Enju (Miyao and Tsu-
jii, 2008). We used Enju only to obtain a binary
phrase structure for each sentence and did not use
any HPSG speciﬁc information. For the target
language, i.e., Japanese, we used KyTea (Neubig
et al., 2011), a Japanese segmentation tool, and
performed the pre-processing steps recommended
in WAT’15.2 We then ﬁltered out the translation
pairs whose sentence lengths are longer than 50
and whose source sentences are not parsed suc-
cessfully. Table 1 shows the details of the datasets
used in our experiments. We carried out two ex-
periments on a small training dataset to investigate

1http://orchid.kuee.kyoto-u.ac.jp/WAT/

WAT2015/index.html

2http://orchid.kuee.kyoto-u.ac.jp/WAT/

WAT2015/baseline/dataPreparationJE.html

Train
Development
Test

Sentences Parsed successfully
1,346,946
1,346,946
1,789
1,790
1,811
1,812

Table 1: Dataset in ASPEC corpus.

sentence pairs
|V | in English
|V | in Japanese

Train (small) Train (large)
1,346,946
87,796
65,680

100,000
25,478
23,532

Table 2: Training dataset and the vocabulary sizes.

the effectiveness of our proposed model and on
a large training dataset to compare our proposed
methods with the other systems.

The vocabulary consists of words observed in
the training data more than or equal to N times.
We set N = 2 for the small training dataset and
N = 5 for the large training dataset. The out-of-
vocabulary words are mapped to the special token
“unk”. We added another special symbol “eos” for
both languages and inserted it at the end of all the
sentences. Table 2 shows the details of each train-
ing dataset and its corresponding vocabulary size.

4.2 Training Details

The biases,
softmax weights, and BlackOut
weights are initialized with zeros. The hyperpa-
rameter β of BlackOut is set to 0.4 as recom-
mended by Ji et al. (2016). Following J´ozefowicz
et al. (2015), we initialize the forget gate biases of
LSTM and Tree-LSTM with 1.0. The remaining
model parameters in the NMT models in our ex-
periments are uniformly initialized in [−0.1, 0.1].
The model parameters are optimized by plain SGD
with the mini-batch size of 128. The initial learn-
ing rate of SGD is 1.0. We halve the learning rate
when the development loss becomes worse. Gra-
dient norms are clipped to 3.0 to avoid exploding
gradient problems (Pascanu et al., 2012).

Small Training Dataset We conduct experi-
ments with our proposed model and the sequential
attentional NMT model with the input-feeding ap-
proach. Each model has 256-dimensional hidden
units and word embeddings. The number of nega-
tive samples K of BlackOut is set to 500 or 2000.

Large Training Dataset Our proposed model
has 512-dimensional word embeddings and d-
dimensional hidden units (d ∈ {512, 768, 1024}).
K is set to 2500.

Our code3 is implemented in C++ using the
Eigen library,4 a template library for linear alge-
bra, and we run all of the experiments on multi-
core CPUs.5 It takes about a week to train a model
on the large training dataset with d = 512.

4.3 Decoding process

We use beam search to decode a target sentence
for an input sentence x and calculate the sum
of the log-likelihoods of the target sentence y =
(y1, · · · , ym) as the beam score:

score(x, y) =

log p(yj|y<j, x).

(15)

m
(cid:88)

j=1

Decoding in the NMT models is a generative pro-
cess and depends on the target language model
given a source sentence. The score becomes
smaller as the target sentence becomes longer, and
thus the simple beam search does not work well
when decoding a long sentence (Cho et al., 2014a;
Pouget-Abadie et al., 2014).
In our preliminary
experiments, the beam search with the length nor-
malization in Cho et al. (2014a) was not effective
in English-to-Japanese translation. The method
in Pouget-Abadie et al. (2014) needs to estimate
the conditional probability p(x|y) using another
NMT model and thus is not suitable for our work.
In this paper, we use statistics on sentence
lengths in beam search. Assuming that the length
of a target sentence correlates with the length of
a source sentence, we redeﬁne the score of each
candidate as follows:

m
(cid:88)

j=1

score(x, y) = Lx,y +

log p(yj|y<j, x),(16)

Lx,y = log p(len(y)|len(x)),

(17)

where Lx,y is the penalty for the conditional prob-
ability of the target sentence length len(y) given
the source sentence length len(x).
It allows
the model to decode a sentence by considering
the length of the target sentence.
In our exper-
iments, we computed the conditional probability

3https://github.com/tempra28/tree2seq
4http://eigen.tuxfamily.org/index.php
516 threads on Intel(R) Xeon(R) CPU E5-2667 v3 @

3.20GHz

p(len(y)|len(x)) in advance following the statis-
tics collected in the ﬁrst one million pairs of the
training dataset. We allow the decoder to generate
up to 100 words.

4.4 Evaluation

We evaluated the models by two automatic eval-
uation metrics, RIBES (Isozaki et al., 2010) and
BLEU (Papineni et al., 2002) following WAT’15.
We used the KyTea-based evaluation script for the
translation results.6 The RIBES score is a metric
based on rank correlation coefﬁcients with word
precision, and the BLEU score is based on n-gram
word precision and a Brevity Penalty (BP) for out-
puts shorter than the references. RIBES is known
to have stronger correlation with human judge-
ments than BLEU in translation between English
and Japanese as discussed in Isozaki et al. (2010).

5 Results and Discussion

5.1 Small Training Dataset

Table 3 shows the perplexity, BLEU, RIBES, and
the training time on the development data with the
Attentional NMT (ANMT) models trained on the
small dataset. We conducted the experiments with
our proposed method using BlackOut and soft-
max. We decoded a translation by our proposed
beam search with a beam size of 20.

As shown in Table 3, the results of our proposed
model with BlackOut improve as the number of
negative samples K increases. Although the result
of softmax is better than those of BlackOut (K =
500, 2000), the training time of softmax per epoch
is about three times longer than that of BlackOut
even with the small dataset.

As to the results of the ANMT model, reversing
the word order in the input sentence decreases the
scores in English-to-Japanese translation, which
contrasts with the results of other language pairs
reported in previous work (Sutskever et al., 2014;
Luong et al., 2015a). By taking syntactic infor-
mation into consideration, our proposed model
improves the scores, compared to the sequential
attention-based approach.

We found that better perplexity does not always
lead to better translation scores with BlackOut as
shown in Table 3. One of the possible reasons is
that BlackOut distorts the target word distribution

6http://lotus.kuee.kyoto-u.ac.jp/WAT/
evaluation/automatic_evaluation_systems/
automaticEvaluationJA.html

Proposed model
Proposed model
Proposed model (Softmax)
ANMT (Luong et al., 2015a)
+ reverse input
ANMT (Luong et al., 2015a)
+ reverse input

K
500
2000
—
500
500
2000
2000

Perplexity RIBES BLEU Time/epoch (min.)
20.0
20.5
21.8
18.5
17.7
19.4
17.5

19.6
21.0
17.9
21.6
22.6
23.1
26.1

71.8
72.6
73.2
70.7
69.8
71.5
69.5

55
70
180
45
45
60
60

Table 3: Evaluation results on the development data using the small training data. The training time per
epoch is also shown, and K is the number of negative samples in BlackOut.

Simple BS

Proposed BS

Beam size RIBES BLEU (BP)
20.0 (90.1)
72.3
19.5 (85.1)
72.3
20.5 (91.7)
72.6

6
20
20

Without sequential LSTMs
With sequential LSTMs

RIBES BLEU
19.5
20.0

69.4
72.3

Table 4: Effects of the Beam Search (BS) on the
development data.

Table 5: Effects of the sequential LSTMs in our
proposed tree-based encoder on the development
data.

by the modiﬁed unigram-based negative sampling
where frequent words can be treated as the nega-
tive samples multiple times at each training step.

Effects of the proposed beam search Table 4
shows the results on the development data of pro-
posed method with BlackOut (K = 2000) by
the simple beam search and our proposed beam
search. The beam size is set to 6 or 20 in the sim-
ple beam search, and to 20 in our proposed search.
We can see that our proposed search outperforms
the simple beam search in both scores. Unlike
RIBES, the BLEU score is sensitive to the beam
size and becomes lower as the beam size increases.
We found that the BP had a relatively large im-
pact on the BLEU score in the simple beam search
as the beam size increased. Our search method
works better than the simple beam search by keep-
ing long sentences in the candidates with a large
beam size.

Effects of the sequential LSTM units We also
investigated the effects of the sequential LSTMs
at the leaf nodes in our proposed tree-based en-
coder. Table 5 shows the result on the develop-
ment data of our proposed encoder and that of an
attentional tree-based encoder without sequential
LSTMs with BlackOut (K = 2000).7 The results
show that our proposed encoder considerably out-

7For this evaluation, we used the 1,789 sentences that
were successfully parsed by Enju because the encoder with-
out sequential LSTMs always requires a parse tree.

performs the encoder without sequential LSTMs,
suggesting that the sequential LSTMs at the leaf
nodes contribute to the context-aware construction
of the phrase representations in the tree.

5.2 Large Training Dataset

Table 6 shows the experimental results of RIBES
and BLEU scores achieved by the trained models
on the large dataset. We decoded the target sen-
tences by our proposed beam search with the beam
size of 20.8 The results of the other systems are the
ones reported in Nakazawa et al. (2015).

All of our proposed models show similar per-
formance regardless of the value of d. Our ensem-
ble model is composed of the three models with
d = 512, 768, and 1024, and it shows the best
RIBES score among all systems.9

As for the time required for training, our im-
plementation needs about one day to perform one
epoch on the large training dataset with d = 512.
It would take about 11 days without using the
BlackOut sampling.

Comparison with the NMT models The model
of Zhu (2015) is an ANMT model (Bahdanau et
al., 2015) with a bi-directional LSTM encoder,
and uses 1024-dimensional hidden units and 1000-

8We found two sentences which ends without eos with
d = 512, and then we decoded it again with the beam size of
1000 following Zhu (2015).

9Our ensemble model yields a METEOR (Denkowski and

Lavie, 2014) score of 53.6 with language option “-l other”.

Model
Proposed model (d = 512)
Proposed model (d = 768)
Proposed model (d = 1024)
Ensemble of the above three models
ANMT with LSTMs (Zhu, 2015)
+ Ensemble, unk replacement
+ System combination,

3 pre-reordered ensembles

ANMT with GRUs (Lee et al., 2015)
+ character-based decoding,

Begin/Inside representation

RIBES BLEU
34.36
81.46
34.78
81.89
34.87
81.58
82.45
36.95
32.19
79.70
34.19
80.27

80.91

36.21

81.15

35.75

PB baseline
HPB baseline
T2S baseline
T2S model (Neubig and Duh, 2014)
+ ANMT Rerank (Neubig et al., 2015)

69.19
74.70
75.80
79.65
81.38

29.80
32.56
33.44
36.58
38.17

Table 6: Evaluation results on the test data.

dimensional word embeddings. The model of Lee
et al. (2015) is also an ANMT model with a bi-
directional Gated Recurrent Unit (GRU) encoder,
and uses 1000-dimensional hidden units and 200-
dimensional word embeddings. Both models are
sequential ANMT models. Our single proposed
model with d = 512 outperforms the best result of
Zhu (2015)’s end-to-end NMT model with ensem-
ble and unknown replacement by +1.19 RIBES
and by +0.17 BLEU scores. Our ensemble model
in both RIBES and
shows better performance,
BLEU scores, than that of Zhu (2015)’s best sys-
tem which is a hybrid of the ANMT and SMT
models by +1.54 RIBES and by +0.74 BLEU
scores and Lee et al. (2015)’s ANMT system
with special character-based decoding by +1.30
RIBES and +1.20 BLEU scores.

Comparison with the SMT models PB, HPB
and T2S are the baseline SMT systems in
WAT’15: a phrase-based model, a hierarchical
phrase-based model, and a tree-to-string model,
respectively (Nakazawa et al., 2015). The best
model in WAT’15 is Neubig et al. (2015)’s tree-
to-string SMT model enhanced with reranking by
ANMT using a bi-directional LSTM encoder. Our
proposed end-to-end NMT model compares favor-
ably with Neubig et al. (2015).

5.3 Qualitative Analysis

We illustrate the translations of test data by our
model with d = 512 and several attentional rela-
tions when decoding a sentence. In Figures 4 and
5, an English sentence represented as a binary tree
is translated into Japanese, and several attentional
relations between English words or phrases and

Figure 4: Translation example of a short sen-
tence and the attentional relations by our proposed
model.

Japanese word are shown with the highest atten-
tion score α. The additional attentional relations
are also illustrated for comparison. We can see the
target words softly aligned with source words and
phrases.

In Figure 4, the Japanese word “液晶” means
“liquid crystal”, and it has a high attention score
(α = 0.41) with the English phrase “liquid crystal
for active matrix”. This is because the j-th tar-
get hidden unit sj has the contextual information
about the previous words y<j including “活性 マ
トリックス の” (“for active matrix” in English).
The Japanese word “セル” is softly aligned with
the phrase “the cells” with the highest attention
score (α = 0.35). In Japanese, there is no deﬁ-
nite article like “the” in English, and it is usually
aligned with null described as Section 1.

In Figure 5, in the case of the Japanese word
“示” (“showed” in English), the attention score
with the English phrase “showed excellent perfor-
mance” (α = 0.25) is higher than that with the
English word “showed” (α = 0.01). The Japanese
word “の” (“of” in English) is softly aligned with
the phrase “of Si dot MOS capacitor” with the
highest attention score (α = 0.30). It is because
our attention mechanism takes each previous con-
text of the Japanese phrases “優れ た 性能” (“ex-
cellent performance” in English) and “Ｓｉ ドッ
ト ＭＯＳ コンデンサ” (“Si dot MOS capaci-
tor” in English) into account and softly aligned the
target words with the whole phrase when trans-
lating the English verb “showed” and the prepo-
sition “of”. Our proposed model can thus ﬂexibly
learn the attentional relations between English and
Japanese.

We observed that our model translated the word
“active” into “活性”, a synonym of the reference
word “アクティブ”. We also found similar ex-

Figure 5: Translation example of a long sentence and the attentional relations by our proposed model.

amples in other sentences, where our model out-
puts synonyms of the reference words, e.g. “女”
and “女 性” (“female” in English) and “NASA”
and “航 空 宇 宙 局” (“National Aeronautics and
Space Administration” in English). These trans-
lations are penalized in terms of BLEU scores, but
they do not necessarily mean that the translations
were wrong. This point may be supported by the
fact that the NMT models were highly evaluated
in WAT’15 by crowd sourcing (Nakazawa et al.,
2015).

enables the NMT models to translate while align-
ing the target with the source. Luong et al. (2015a)
reﬁned the attention model so that it can dynami-
cally focus on local windows rather than the entire
sentence. They also proposed a more effective at-
tentional path in the calculation of ANMT models.
Subsequently, several ANMT models have been
proposed (Cheng et al., 2016; Cohn et al., 2016);
however, each model is based on the existing se-
quential attentional models and does not focus on
a syntactic structure of languages.

6 Related Work

7 Conclusion

Kalchbrenner and Blunsom (2013) were the ﬁrst
to propose an end-to-end NMT model using Con-
volutional Neural Networks (CNNs) as the source
encoder and using RNNs as the target decoder.
The Encoder-Decoder model can be seen as an ex-
tension of their model, and it replaces the CNNs
with RNNs using GRUs (Cho et al., 2014b) or
LSTMs (Sutskever et al., 2014).

Sutskever et al. (2014) have shown that mak-
ing the input sequences reversed is effective in a
French-to-English translation task, and the tech-
nique has also proven effective in translation tasks
between other European language pairs (Luong et
al., 2015a). All of the NMT models mentioned
above are based on sequential encoders. To incor-
porate structural information into the NMT mod-
els, Cho et al. (2014a) proposed to jointly learn
structures inherent in source-side languages but
did not report improvement of translation perfor-
mance. These studies motivated us to investigate
the role of syntactic structures explicitly given by
existing syntactic parsers in the NMT models.

The attention mechanism (Bahdanau et al.,
2015) has promoted NMT onto the next stage. It

In this paper, we propose a novel syntactic ap-
proach that extends attentional NMT models. We
focus on the phrase structure of the input sen-
tence and build a tree-based encoder following
the parsed tree. Our proposed tree-based encoder
is a natural extension of the sequential encoder
model, where the leaf units of the tree-LSTM
in the encoder can work together with the origi-
nal sequential LSTM encoder. Moreover, the at-
tention mechanism allows the tree-based encoder
to align not only the input words but also input
phrases with the output words. Experimental re-
sults on the WAT’15 English-to-Japanese transla-
tion dataset demonstrate that our proposed model
achieves the best RIBES score and outperforms
the sequential attentional NMT model.

Acknowledgments

We thank the anonymous reviewers for their con-
structive comments and suggestions. This work
was supported by CREST, JST, and JSPS KAK-
ENHI Grant Number 15J12597.

References

[Bahdanau et al.2015] Dzmitry Bahdanau, Kyunghyun
Cho, and Yoshua Bengio. 2015. Neural Machine
Translation by Jointly Learning to Align and Trans-
late. In Proceedings of the 3rd International Con-
ference on Learning Representations.

[Cheng et al.2016] Yong Cheng, Shiqi Shen, Zhongjun
He, Wei He, Hua Wu, Maosong Sun, and Yang Liu.
2016. Agreement-based Joint Training for Bidirec-
tional Attention-based Neural Machine Translation.
In Proceedings of the 25th International Joint Con-
ference on Artiﬁcial Intelligence. to appear.

[Cho et al.2014a] KyungHyun Cho, Bart van Merrien-
boer, Dzmitry Bahdanau, and Yoshua Bengio.
2014a. On the Properties of Neural Machine Trans-
lation: Encoder-Decoder Approaches. In Proceed-
ings of Eighth Workshop on Syntax, Semantics and
Structure in Statistical Translation (SSST-8).

[Cho et al.2014b] Kyunghyun Cho, Bart van Merrien-
boer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi
Bougares, Holger Schwenk, and Yoshua Bengio.
2014b. Learning Phrase Representations using RNN
Encoder–Decoder for Statistical Machine Transla-
In Proceedings of the 2014 Conference on
tion.
Empirical Methods in Natural Language Process-
ing, pages 1724–1734.

[Cohn et al.2016] Trevor Cohn, Cong Duy Vu Hoang,
Ekaterina Vymolova, Kaisheng Yao, Chris Dyer,
and Gholamreza Haffari.
Incorporating
Structural Alignment Biases into an Attentional
In Proceedings of the
Neural Translation Model.
15th Annual Conference of
the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies. to appear.

2016.

[Denkowski and Lavie2014] Michael Denkowski and
Alon Lavie. 2014. Meteor universal: Language spe-
ciﬁc translation evaluation for any target language.
In Proceedings of the 14th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics 2014 Workshop on Statistical Machine
Translation.

[Gers et al.2000] Felix A. Gers, J¨urgen Schmidhuber,
and Fred A. Cummins. 2000. Learning to Forget:
Continual Prediction with LSTM. Neural Computa-
tion, 12(10):2451–2471.

[Gutmann and Hyv¨arinen2012] Michael U. Gutmann
and Aapo Hyv¨arinen. 2012. Noise-contrastive esti-
mation of unnormalized statistical models, with ap-
plications to natural image statistics. Journal of Ma-
chine Learning Research, 13(1):307–361.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter
and J¨urgen Schmidhuber. 1997. Long Short-Term
Memory. Neural Computation, 9(8):1735–1780.

[Isozaki et al.2010] Hideki

Isozaki, Tsutomu Hirao,
Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada.
2010. Automatic Evaluation of Translation Quality

In Proceedings of the
for Distant Language Pairs.
2010 Conference on Empirical Methods in Natural
Language Processing, pages 944–952.

[Ji et al.2016] Shihao Ji, S. V. N. Vishwanathan, Na-
dathur Satish, Michael J. Anderson, and Pradeep
Dubey. 2016. BlackOut: Speeding up Recurrent
Neural Network Language Models With Very Large
In Proceedings of the 4th Interna-
Vocabularies.
tional Conference on Learning Representations.

[J´ozefowicz et al.2015] Rafal

J´ozefowicz, Wojciech
Zaremba, and Ilya Sutskever. 2015. An Empiri-
cal Exploration of Recurrent Network Architectures.
In Proceedings of the 32nd International Confer-
ence on Machine Learning, volume 37, pages 2342–
2350.

[Kalchbrenner and Blunsom2013] Nal Kalchbrenner
and Phil Blunsom. 2013. Recurrent continuous
In Proceedings of the 2013
translation models.
Conference on Empirical Methods
in Natural
Language Processing, pages 1700–1709.

[Lee et al.2015] Hyoung-Gyu Lee, JaeSong Lee, Jun-
Seok Kim, and Chang-Ki Lee. 2015. NAVER Ma-
In Pro-
chine Translation System for WAT 2015.
ceedings of the 2nd Workshop on Asian Translation
(WAT2015), pages 69–73.

[Liu et al.2006] Yang Liu, Qun Liu, and Shouxun Lin.
2006. Tree-to-string alignment template for statisti-
cal machine translation. In Proceedings of the 21st
International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 609–616.

[Luong et al.2015a] Thang Luong, Hieu Pham, and
2015a. Effective Ap-
Christopher D. Manning.
proaches to Attention-based Neural Machine Trans-
In Proceedings of the 2015 Conference on
lation.
Empirical Methods in Natural Language Process-
ing, pages 1412–1421.

[Luong et al.2015b] Thang Luong,

Ilya Sutskever,
Quoc Le, Oriol Vinyals, and Wojciech Zaremba.
2015b. Addressing the Rare Word Problem in Neu-
ral Machine Translation. In Proceedings of the 53rd
Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 11–19.

[Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases
In Advances in Neu-
and their compositionality.
ral Information Processing Systems 26, pages 3111–
3119.

[Miyao and Tsujii2008] Yusuke Miyao and Jun’ichi
Tsujii. 2008. Feature Forest Models for Proba-
bilistic HPSG Parsing. Computational Linguistics,
34(1):35–80.

Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 1556–1566.

[Yamada and Knight2001] Kenji Yamada and Kevin
Knight. 2001. A syntax-based statistical translation
model. In Proceedings of the 39th Annual Meeting
on Association for Computational Linguistics, pages
523–530.

[Zhu2015] Zhongyuan Zhu. 2015. Evaluating Neural
In
Machine Translation in English-Japanese Task.
Proceedings of the 2nd Workshop on Asian Transla-
tion (WAT2015), pages 61–68.

[Zoph and Knight2016] Barret Zoph and Kevin Knight.
In Pro-
2016. Multi-Source Neural Translation.
ceedings of the 15th Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
to appear.

[Nakazawa et al.2015] Toshiaki Nakazawa, Hideya
Mino, Isao Goto, Graham Neubig, Sadao Kuro-
hashi, and Eiichiro Sumita. 2015. Overview of
In Pro-
the 2nd Workshop on Asian Translation.
ceedings of the 2nd Workshop on Asian Translation
(WAT2015), pages 1–28.

[Neubig and Duh2014] Graham Neubig and Kevin
Duh. 2014. On the elements of an accurate tree-
In Proceed-
to-string machine translation system.
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 143–149.

[Neubig et al.2011] Graham Neubig, Yosuke Nakata,
and Shinsuke Mori. 2011. Pointwise Prediction for
Robust, Adaptable Japanese Morphological Analy-
sis. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 529–533.

[Neubig et al.2015] Graham Neubig, Makoto Mor-
ishita, and Satoshi Nakamura.
2015. Neural
Reranking Improves Subjective Quality of Machine
Translation: NAIST at WAT2015. In Proceedings of
the 2nd Workshop on Asian Translation (WAT2015),
pages 35–41.

[Papineni et al.2002] Kishore Papineni, Salim Roukos,
2002. BLEU:
Todd Ward, and Wei-Jing Zhu.
A Method for Automatic Evaluation of Machine
In Proceedings of the 40th Annual
Translation.
Meeting on Association for Computational Linguis-
tics, pages 311–318.

[Pascanu et al.2012] Razvan Pascanu, Tomas Mikolov,
and Yoshua Bengio. 2012. Understanding the ex-
ploding gradient problem. arXiv: 1211.5063.

[Pouget-Abadie et al.2014] Jean
Bart

Pouget-Abadie,
van Merrienboer,
Dzmitry Bahdanau,
Kyunghyun Cho, and Yoshua Bengio.
2014.
Overcoming the curse of sentence length for neural
machine translation using automatic segmentation.
In Proceedings of SSST-8, Eighth Workshop on
Syntax, Semantics and Structure in Statistical
Translation, pages 78–85.

[Sag et al.2003] Ivan A. Sag, Thomas Wasow, and
Emily Bender. 2003. Syntactic Theory: A Formal
Introduction. Center for the Study of Language and
Information, Stanford, 2nd edition.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals,
2014. Sequence to Sequence
and Quoc V. Le.
In Advances in
Learning with Neural Networks.
Neural Information Processing Systems 27, pages
3104–3112.

[Tai et al.2015] Kai Sheng Tai, Richard Socher, and
Christopher D. Manning. 2015. Improved Semantic
Representations From Tree-Structured Long Short-
In Proceedings of the
Term Memory Networks.
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint

