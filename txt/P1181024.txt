8
1
0
2
 
t
c
O
 
0
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
4
6
1
1
0
.
7
0
7
1
:
v
i
X
r
a

Kernel Feature Selection via
Conditional Covariance Minimization

Jianbo Chen∗
University of California, Berkeley
jianbochen@berkeley.edu

Mitchell Stern∗
University of California, Berkeley
mitchell@berkeley.edu

Martin J. Wainwright
University of California, Berkeley
wainwrig@berkeley.edu

Michael I. Jordan
University of California, Berkeley
jordan@berkeley.edu

Abstract

We propose a method for feature selection that employs kernel-based measures
of independence to ﬁnd a subset of covariates that is maximally predictive of the
response. Building on past work in kernel dimension reduction, we show how to
perform feature selection via a constrained optimization problem involving the
trace of the conditional covariance operator. We prove various consistency results
for this procedure, and also demonstrate that our method compares favorably with
other state-of-the-art algorithms on a variety of synthetic and real data sets.

1

Introduction

Feature selection is an important issue in statistical machine learning, leading to both computational
beneﬁts (lower storage and faster computation) and statistical beneﬁts, including increased model
interpretability. With large data sets becoming ever more prevalent, feature selection has seen
widespread usage across a variety of real-world tasks in recent years, including text classiﬁcation,
gene selection from microarray data, and face recognition [3, 14, 17]. In this work, we consider
the supervised variant of feature selection, which entails ﬁnding a subset of the input features that
explains the output well. This practice can reduce the computational expense of downstream learning
by removing features that are redundant or noisy, while simultaneously providing insight into the
data through the features that remain.

Feature selection algorithms can generally be divided into two groups: those which are agnostic
to the choice of learning algorithm, and those which attempt to ﬁnd features that optimize the
performance of a speciﬁc learning algorithm.1 Kernel methods have been successfully applied under
each of these paradigms in recent work; for instance, see the papers [1, 8, 16, 19, 23, 25, 26, 29].
Kernel feature selection methods have the advantage of capturing nonlinear relationships between the
features and the labels. Many previous approaches are ﬁlter methods based on the Hilbert-Schmidt
Independence Criterion (HSIC), as proposed by Gretton et al. [13] as a measure of dependence.
For instance, Song et al. [24, 25] proposed to optimize HSIC with greedy algorithms on features.
Masaeli et al. [19] proposed Hilbert-Schmidt Feature Selection (HSFS), which optimizes HSIC with
a continuous relaxation. In later work, Yamada et al. [29] proposed the HSIC-LASSO, in which the
dual augmented Lagrangian can be used to ﬁnd a global optimum. There are also wrapper methods

∗Equal contribution.
1Feature selection algorithms that operate independently of the choice of predictor are referred to as ﬁlter
methods. Algorithms tailored to speciﬁc predictors can be further divided into wrapper methods, which use
learning algorithms to evaluate features based on their predictive power, and embedded methods, which combine
feature selection and learning into a single problem [14].

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

and embedded methods using kernels. Most of the methods add weights to features and optimize the
original kernelized loss function together with a penalty on the weights [1, 5, 8, 11, 12, 27, 28]. For
example, Cao et al. [5] proposed margin-based algorithms for SVMs to select features in the kernel
space. Lastly, Allen [1] proposed an embedded method suitable for kernel SVMs and kernel ridge
regression.

In this paper, we propose to use the trace of the conditional covariance operator as a criterion for
feature selection. We offer theoretical motivation for this choice and show that our method can be
interpreted both as a ﬁlter method and as a wrapper method for a certain class of learning algorithms.
We also show that the empirical estimate of the criterion is consistent as the sample size increases.
Finally, we conclude with an empirical demonstration that our algorithm is comparable to or better
than several other popular feature selection algorithms on both synthetic and real-world tasks.

2 Formulating feature selection

Let X ⊂ Rd be the domain of covariates X, and let Y be the domain of responses Y . Given n
independent and identically distributed (i.i.d.) samples {(xi, yi), i = 1, 2, . . . , n} generated from an
unknown joint distribution PX,Y together with an integer m ≤ d, our goal is to select m of the d total
features X1, X2, . . . , Xd which best predict Y . Let S be the full set of features, and let T ⊆ S denote
a subset of features. For ease of notation, we identify S = {X1, . . . , Xd} with [d] = {1, . . . , d},
and also identify XT with T . We formulate the problem of supervised feature selection from two
perspectives below. The ﬁrst perspective motivates our algorithm as a ﬁlter method. The second
perspective offers an interpretation as a wrapper method.

2.1 From a dependence perspective

Viewing the problem from the perspective of dependence, we would ideally like to identify a subset
of features T of size m such that the remaining features S \ T are conditionally independent of the
responses given T . However, this may not be achievable when the number of allowable features
m is small. We therefore quantify the extent of the remaining conditional dependence using some
metric Q, and aim to minimize Q over all subsets T of the appropriate size. More formally, let
Q : 2[d] → [0, ∞) be a function mapping subsets of [d] to the non-negative reals that satisﬁes the
following properties:

• For a subset of features T , we have Q(T ) = 0 if and only if XS\T and Y are conditionally

independent given XT .

• The function Q is non-increasing, meaning that Q(T ) ≥ Q(S) whenever T ⊆ S. Hence, the

function Q achieves its minimum for the full feature set T = [d].

Given a ﬁxed integer m, the problem of supervised feature selection can then be posed as

This formulation can be taken as a ﬁlter method for feature selection.

min
T :|T |=m

Q(T ).

2.2 From a prediction perspective

An alternative perspective aims at characterizing how well XT can predict Y directly within the
context of a speciﬁc learning problem. Formally, we deﬁne the error of prediction as

EF (X) = inf
f ∈F

EX,Y L(Y, f (X)),

where F is a class of functions from X to Y, and L is a loss function speciﬁed by the user. For
example, in a univariate regression problem, the function class F might be the set of all linear
functions, and the loss function might be the squared error L(Y, f (X)) = (Y − f (X))2.

We then hope to solve the following problem:

min
T :|T |≤m

EF (XT ) = min

T :|T |≤m

inf
f ∈Fm

EX,Y L(Y, f (XT )),

where Fm is a class of functions supported on Rm. That is, we aim to ﬁnd the subset of m features
that minimizes the prediction error. This formulation thus falls within the scope of wrapper methods
for feature selection.

(1)

(2)

2

3 Conditional covariance operator

The conditional covariance operator provides a measure of conditional dependence for random
variables. It was ﬁrst proposed by Baker [2], and was further studied and used for sufﬁcient dimension
reduction by Fukumizu et al. [9, 10]. We provide a brief overview of this operator and some of its
key properties here.
Let (HX , kX ) and (HY , kY ) denote reproducing kernel Hilbert spaces (RKHSs) of functions on
spaces X and Y, respectively. Also let (X, Y ) be a random vector on X × Y with joint distribution
PX,Y . Assume the kernels kX and kY are bounded in expectation:

EX [kX (X, X)] < ∞ and EY [kY (Y, Y )] < ∞.
(3)
The cross-covariance operator associated with the pair (X, Y ) is the mapping ΣY X : HX → HY
deﬁned by the relations
(cid:104)g, ΣY X f (cid:105)HY = EX,Y [(f (X) − EX [f (X)])(g(Y ) − EY [g(Y )])]

for all f ∈ HX and g ∈ HY .
(4)

Baker [2] showed there exists a unique bounded operator VY X such that

The conditional covariance operator is then deﬁned as

ΣY X = Σ1/2

Y Y VY X Σ1/2
XX .

ΣY Y |X = ΣY Y − Σ1/2

Y Y VY X VXY Σ1/2
Y Y .

(5a)

(5b)

Among other results, Fukumizu et al. [9, 10] showed that the conditional covariance operator captures
the conditional variance of Y given X. More precisely, if the sum HX + R is dense in L2(PX ),
where L2(PX ) is the space of all square-integrable functions on X , then we have
for any g ∈ HY .
(cid:104)g, ΣY Y |X g(cid:105)HY = EX [varY |X [g(Y )|X]]

(6)

From Proposition 2 in the paper [10], we also know the residual error of g(Y ) with g ∈ HY can be
characterized by the conditional covariance operator. More formally, for any g ∈ HY , we have

(cid:104)g, ΣY Y |X g(cid:105)HY = inf
f ∈HX

EX,Y ((g(Y ) − EY [g(Y )]) − (f (X) − EX [f (X)]))2.

(7)

4 Proposed method

In this section, we describe our method for feature selection, which we call conditional covariance
minimization (CCM).
Let (H1, k1) denote an RKHS supported on X ⊂ Rd. Let T ⊆ [d] be a subset of features with
cardinality m ≤ d, and for all x ∈ Rd, take xT ∈ Rd to be the vector with components xT
i = xi if
1 (x, ˜x) = k1(xT , ˜xT ) for all x, ˜x ∈ X . Suppose
i ∈ T or 0 otherwise. We deﬁne the kernel kT
further that the kernel k1 is permutation-invariant. That is, for any x, ˜x ∈ X and permutation π,
denoting (xπ(1), . . . , xπ(d)) as xπ, we have k1(x, ˜x) = k1(xπ, ˜xπ). (Note that this property holds
for many common kernels, including the linear, polynomial, Gaussian, and Laplacian kernels.)
1 generates the same RKHS supported on Rm. We call this
Then for every T of cardinality m, kT
RKHS ( ˜H1, (cid:101)k1). We will show the trace of the conditional covariance operator trace(ΣY Y |X ) can
be interpreted as a dependence measure, as long as the RKHS H1 is large enough.
We say that an RKHS (H, k) is characteristic if the map P → EP [k(X, ·)] ∈ H is one-to-one. If k
is bounded, this is equivalent to saying that H + R is dense in L2(P ) for any probability measure
P [10]. We have the following lemma, whose proof is given in the appendix:

1 by kT

Lemma 1. If k1 is bounded and characteristic, then (cid:101)k1 is also characteristic.

Let (H2, k2) denote an RKHS supported on Y. Based on the above lemma, we have the following
theorem, which is a parallel version of Theorem 4 in [10]:
Theorem 2. If (H1, k1) and (H2, k2) are characteristic, we have ΣY Y |X (cid:22) ΣY Y |XT with equality
holding if and only if Y ⊥⊥ X|XT .

3

The proof is postponed to the appendix.

With this generic result in place, we now narrow our focus to problems with univariate responses,
including univariate regression, binary classiﬁcation and multi-class classiﬁcation. In the case of
regression, we assume H2 is supported on R, and we take k2 to be the linear kernel:

k2(y, ˜y) = y ˜y
(8)
for all y, ˜y ∈ R. For binary or multi-class classiﬁcation, we take k2 to be the Kronecker delta
function:

k2(y, ˜y) = δ(y, ˜y) =

(cid:26)1
0

if y = ˜y,
otherwise.

(9)

(10)

(11)

(12)

This can be equivalently interpreted as a linear kernel k(y, ˜y) = (cid:104)y, ˜y(cid:105) assuming a one-hot encoding
of Y , namely that Y = {y ∈ {0, 1}k : (cid:80)
i yi = 1} ⊂ Rk, where k is the number of classes.
When Y is R or {y ∈ {0, 1}k : (cid:80)
i yi = 1} ⊂ Rk, we obtain the following corollary of Theorem 2:
Corollary 3. If (H1, k1) is characteristic, Y is R or {y ∈ {0, 1}k : (cid:80)
i yi = 1} ⊂ Rk, and (H2, k2)
includes the identity function on Y, then we have Tr(ΣY Y |X ) ≤ Tr(ΣY Y |XT ) for any subset T of
features. Moreover, the equality Tr(ΣY Y |X ) = Tr(ΣY Y |XT ) holds if and only if Y ⊥⊥ X|XT .

Hence, in the univariate case, the problem of supervised feature selection reduces to minimizing the
trace of the conditional covariance operator over subsets of features with controlled cardinality:

min
T :|T |=m

Q(T ) := Tr(ΣY Y |XT ).

In the regression setting, Equation (7) implies the residual error of regression can also be characterized
by the trace of the conditional covariance operator when using the linear kernel on Y. More formally,
we have the following observation:
Corollary 4. Let ΣY Y |XT denote the conditional covariance operator of (XT , Y ) in ( ˜H1, (cid:101)k1).
Deﬁne the space of functions Fm from Rm to Y as

Fm = ˜H1 + R := {f + c : f ∈ ˜H1, c ∈ R}.

Then we have

Tr(ΣY Y |XT ) = EFm(XT ) = inf
f ∈Fm

EX,Y (Y − f (XT ))2.

Given the fact that the trace of the conditional covariance operator can characterize the dependence
and the prediction error in regression, we will use the empirical estimate of it as our objective. Given
n samples {(x1, y1), . . . , (xn, yn)}, the empirical estimate is given by [10]:
) := trace[ ˆΣ(n)

trace( ˆΣ(n)

+ εnI)−1 ˆΣ(n)

( ˆΣ(n)

XT Y ]

Y Y |XT

Y Y − ˆΣ(n)
Y XT
= εn trace[GY (GXT + nεnIn)−1],

XT XT

where ˆΣT (n)
Y Y are the covariance operators deﬁned with respect to the empirical
distribution and GXT and GY are the centralized kernel matrices, respectively. Concretely, we deﬁne

XT X and ˆΣ(n)

Y X , ˆΣT (n)

GXT : = (In −

11T )KXT (In −

11T )

and GY : = (In −

11T )KY (In −

11T ).

1
n

1
n

1
n

1
n

The (i, j)th entry of the kernel matrix KXT is ˜k1(xi
T denoting the ith sample with
only features in T . As the kernel k2 on the space of responses is linear, we have KY = YYT , where
Y is the n × k matrix with each row being a sample response. Without loss of generality, we assume
each column of Y is zero-mean, so that GY = KY = YYT . Our objective then becomes:
trace[GY (GXT + nεnIn)−1] = trace[YYT (GXT + nεnIn)−1] = trace[YT (GXT + nεnIn)−1Y].

T ), with xi

T , xj

(13)
For simplicity, we only consider univariate regression and binary classiﬁcation where k = 1, but our
discussion carries over to the multi-class setting with minimal modiﬁcation. The objective becomes

min
|T |=m

ˆQ(n)(T ) := yT (GXT + nεnIn)−1y,

(14)

where y = (y1, . . . , yn)T is an n-dimensional vector. We show the global optimal of the problem (14)
is consistent. More formally, we have the following theorem:

4

Theorem 5 (Feature Selection Consistency). Let the set A = argmin|T |≤mQ(T ) be the set of all the
optimal solutions to (12) and ˆT (n) ∈ argmin|T |≤m
ˆQ(n)(T ) be a global optimal of (14). If εn → 0
and εnn → ∞ as n → ∞, we have

P ( ˆT (n) ∈ A) → 1.

(15)

Our proof is provided in the appendix. A comparable result is given in Fukumizu et al. [10] for the
consistency of their dimension reduction estimator, but as our minimization takes place over a ﬁnite
set, our proof is considerably simpler.

5 Optimization

Finding a global optimum for (14) is NP-hard for generic kernels [28], and exhaustive search
is computationally intractable if the number of features is large. We therefore approximate the
problem of interest via continuous relaxation, as has previously been done in past work on feature
selection [4, 27, 28].

5.1

Initial relaxation

We begin by introducing a binary vector w ∈ {0, 1}d to indicate which features are active. This
allows us to rephrase the optimization problem from (14) as

where (cid:12) denotes the Hadamard product between two vectors and Gw(cid:12)X is the centralized version of
the kernel matrix Kw(cid:12)X with (Kw(cid:12)X )ij = k1(w (cid:12) xi, w (cid:12) xj).

We then approximate the problem (16) by relaxing the domain of w to the unit hypercube [0, 1]d and
replacing the equality constraint with an inequality constraint:

min
w

yT (Gw(cid:12)X + nεnIn)−1y

subject to wi ∈ {0, 1}, i = 1, . . . , d,

1T w = m,

yT (Gw(cid:12)X + nεnIn)−1y

min
w
subject to 0 ≤ wi ≤ 1, i = 1, . . . , d,

1T w ≤ m.

(16)

(17)

This objective can be optimized using projected gradient descent, and represents our ﬁrst tractable
approximation. A solution to the relaxed problem is converted back into a solution for the original
problem by setting the m largest values of w to 1 and remaining values to 0. We initialize w to the
uniform vector (m/d)[1, 1, . . . , 1]T in order to avoid the corners of the constraint set during the early
stages of optimization.

5.2 Computational issues

The optimization problem can be approximated and manipulated in a number of ways so as to reduce
computational complexity. We discuss a few such options below.

Removing the inequality constrant. The hard constraint 1T w ≤ m requires a nontrivial projec-
tion step, such as the one detailed in Duchi et al. [6]. We can instead replace it with a soft constraint
and move it to the objective. Letting λ1 ≥ 0 be a hyperparameter, this gives rise to the modiﬁed
problem

yT (Gw(cid:12)X + nεnIn)−1y + λ1(1T w − m)

min
w
subject to 0 ≤ wi ≤ 1, i = 1, . . . , d.

(18)

5

(19)

(20)

Removing the matrix inverse. The matrix inverse in the objective function is an expensive op-
eration. In light of this, we ﬁrst deﬁne an auxiliary variable α ∈ Rn, add the equality constraint
α = (Gw(cid:12)X +nεnIn)−1y, and rewrite the objective as αT y. We then note that we may multiply both
sides of the constraint by the centered kernel matrix to obtain the relation (Gw(cid:12)X + nεnIn)α = y.
Letting λ2 ≥ 0 be a hyperparameter, we ﬁnally replace this relation by a soft (cid:96)2 constraint to obtain

αT y + λ2(cid:107)(Gw(cid:12)X + nεnIn)α − y(cid:107)2
2

min
w,α
subject to 0 ≤ wi ≤ 1, i = 1, . . . , d,

1T w ≤ m.

Using a kernel approximation. Rahimi and Recht [22] propose a method for approximating kernel
evaluations by inner products of random feature vectors, so that k(x, ˜x) ≈ z(x)T z(˜x) for a random
map z depending on the choice of kernel k. Let Kw ≈ UwU T
w be such a decomposition, where
Uw ∈ Rn×D for some D < n. Then, deﬁning Vw = (I − 11T /n)Uw, we similarly have that the
centered kernel matrix can be written as Gw ≈ VwV T
w . By the Woodbury matrix identity, we may
write

(Gw(cid:12)X + nεnIn)−1 ≈

I −

1
nn2 Vw(ID +
ε2

1
εnn

w Vw)−1V T
V T
w

=

(I − Vw(V T

w Vw + εnnID)−1V T

w ).

1
εnn
1
εnn

Substituting this into our objective function, scaling by (cid:15)nn, and removing the constant term yT y
resulting from the identity matrix gives a new approximate optimization problem. This modiﬁcation
reduces the complexity of each optimization step from O(n2d + n3) to O(n2D + D3 + nDd).

Choice of formulation. We remark that each of the three approximations beyond the initial re-
laxation may be independently used or omitted, allowing for a number of possible objectives and
constraint sets. We explore some of these conﬁgurations in the experimental section below.

6 Experiments

In this section, we evaluate our approach (CCM) on both synthetic and real-world data sets. We
compare with several strong existing algorithms, including recursive feature elimination (RFE) [15],
Minimum Redundancy Maximum Relevance (mRMR) [21], BAHSIC [24, 25], and ﬁlter methods
using mutual information (MI) and Pearson’s correlation (PC). We use the author’s implementation for
BAHSIC2 and use the Scikit-learn [20] and Scikit-feature [17] packages for the rest of the algorithms.
The code for our approach is publicly available at https://github.com/Jianbo-Lab/CCM.

6.1 Synthetic data

We begin with experiments on the following synthetic data sets:

• Binary classiﬁcation (Friedman et al. [7]). Given Y = −1, (X1, . . . , X10) ∼ N (0, I10).
j ≤ 16, and

Given Y = 1, X1 through X4 are standard normal conditioned on 9 ≤ (cid:80)4
(X5, . . . , X10) ∼ N (0, I6).

j=1 X 2

• 3-dimensional XOR as 4-way classiﬁcation. Consider the 8 corners of the 3-dimensional
hypercube (v1, v2, v3) ∈ {−1, 1}3, and group them by the tuples (v1v3, v2v3), leaving 4 sets
of vectors paired with their negations {v(i), −v(i)}. Given a class i, a point is generated
from the mixture distribution (1/2)N (v(i), 0.5I3) + (1/2)N (−v(i), 0.5I3). Each example
additionally has 7 standard normal noise features for a total of 10 dimensions.

• Additive nonlinear regression: Y = −2 sin(2X1) + max(X2, 0) + X3 + exp(−X4) + ε, where

(X1, . . . , X10) ∼ N (0, I10) and ε ∼ N (0, 1).

2http://www.cc.gatech.edu/~lsong/code.html

6

Figure 1: The above plots show the median rank (y-axis) of the true features as a function of sample
size (x-axis) for the simulated data sets. Lower median ranks are better. The dotted line indicates the
optimal median rank.

The ﬁrst data set represents a standard nonlinear binary classiﬁcation task. The second data set is a
multi-class classiﬁcation task where each feature is independent of Y by itself but a combination of
three features has a joint effect on Y . The third data set arises from an additive model for nonlinear
regression.

Each data set has d = 10 dimensions in total, but only m = 3 or 4 true features. Since the identity
of these features is known, we can evaluate the performance of a feature selection algorithm by
computing the median rank it assigns to the real features, with lower median ranks indicating better
performance. Given enough samples, we would expect this value to come close to the optimal lower
bound of (m + 1)/2.

Our experimental setup is as follows. We generate 10 independent copies of each data set with
sample sizes ranging from 10 to 100, and record the median ranks assigned to the true features by
each algorithm. This process is repeated a total of 100 times, and the results are averaged across
trials. For kernel-based methods, we use a Gaussian kernel k(x, ˜x) = exp(−(cid:107)x − ˜x(cid:107)2/(2σ2)) on
X and a linear kernel k(y, ˜y) = yT ˜y on Y . We take σ to be the median pairwise distance between
samples scaled by 1/
2. Since the number of true features is known, we provide this as an input to
algorithms that require it.

√

Our initial experiments use the basic version of our algorithm from Section 5.1. When the number
of desired features m is ﬁxed, only the regularization parameter ε needs to be chosen. We use
ε = 0.001 for the classiﬁcation tasks and ε = 0.1 for the regression task, selecting these values from
{0.001, 0.01, 0.1} using cross-validation. Our results are shown in Figure 1.

On the binary and 4-way classiﬁcation tasks, our method outperforms all other algorithms, succeeding
in identifying the true features using fewer than 50 samples where others require close to 100 or even
fail to converge. On the additive nonlinear model, several algorithms perform well, and our method is
on par with the best of them across all sample sizes.

These experiments show that our algorithm is comparable to or better than several widely-used
feature selection techniques on a selection of synthetic tasks, and is adept at capturing several kinds
of nonlinear relationships between the covariates and the responses. When compared in particular
to its closest relative BAHSIC, a backward-elimination algorithm based on the Hilbert–Schmidt
independence criterion, we see that our algorithm often produces higher quality results with fewer
samples, and even succeeds in the non-additive problem where BAHSIC fails to converge.

We also rerun these experiments separately for each of the ﬁrst two approximations described
in Section 5.2 above, selecting λ1 from {0.001, 0.01, 0.1} and λ2 from {1, 10, 100} using cross-
validation. We ﬁnd that comparable results can be attained with either approximate objective, but
note that the algorithm is more robust to changes in λ1 than λ2.

6.2 Real-world data

In the previous section, we found that our method for feature selection excelled in identifying
nonlinear relationships on a variety of synthetic data sets. We now turn our attention to a collection

7

Samples
Features
Classes

ALLAML CLL-SUB-111 glass ORL orlraws10P pixraw10P TOX-171 vowel warpAR10P warpPIE10P wine Yale
165
1,024
15

111
11,340
3

100
10,304
10

100
10,000
10

72
7,129
2

400
1,024
40

171
5,784
4

130
2,400
10

210
2,420
10

214
10
6

990
10
11

178
13
3

Table 1: Summary of the benchmark data sets we use for our empirical evaluation.

Figure 2: The above plots show classiﬁcation accuracy (y-axis) versus number of selected features
(x-axis) for our real-world benchmark data sets. Higher accuracies are better.

of real-word tasks, studying the performance of our method and other nonlinear approaches when
used in conjunction with a kernel SVM for downstream classiﬁcation.

We carry out experiments on 12 standard benchmark tasks from the ASU feature selection website [17]
and the UCI repository [18]. A summary of our data sets is provided in Table 1. The data sets are
drawn from several domains including gene data, image data, and voice data, and span both the
low-dimensional and high-dimensional regimes.

For every task, we run each algorithm being evaluated to obtain ranks for all features. Performance is
then measured by training a kernel SVM on the top m features and computing the resulting accuracy
as measured by 5-fold cross-validation. This is done for m ∈ {5, 10, . . . , 100} if the total number of
features d is larger than 100, or m ∈ {1, 2, . . . , d} otherwise. In all cases we ﬁx the regularization
constant of the SVM to C = 1 and use a Gaussian kernel with σ set as in the previous section over
the selected features. For our own algorithm, we ﬁx ε = 0.001 across all experiments and set the
number of desired features to m = 100 if d > 100 or (cid:100)d/5(cid:101) otherwise. Our results are shown in
Figure 2.

Compared with three other popular methods for nonlinear feature selection, i.e. mRMR, BAHSIC,
and MI, we ﬁnd that our method is the strongest performer in the large majority of cases, sometimes
by a substantial margin as in the case of TOX-171. While our method is occasionally outperformed
in the beginning when the number of selected features is small, it either ties or overtakes the leading
method by the end in all but one instance. We remark that our method consistently improves upon
the performance of the related BAHSIC method, suggesting that our objective based on conditional
covariance may be more powerful than one based on the Hilbert-Schmidt independence criterion.

8

7 Conclusion

In this paper, we proposed an approach to feature selection based on minimizing the trace of the
conditional covariance operator. The idea is to select the features that maximally account for the
dependence of the response on the covariates. We do so by relaxing from an intractable discrete
formulation of the problem to a continuous approximation suitable for gradient-based optimization.
We demonstrate the effectiveness of our approach on multiple synthetic and real-world experiments,
ﬁnding that it often outperforms other state-of-the-art approaches, including another competitive
kernel feature selection method based on the Hilbert-Schmidt independence criterion.

References

[1] Genevera I Allen. Automatic feature selection via weighted kernels and regularization. Journal

of Computational and Graphical Statistics, 22(2):284–299, 2013.

[2] Charles R Baker. Joint measures and cross-covariance operators. Transactions of the American

Mathematical Society, 186:273–289, 1973.

[3] Verónica Bolón-Canedo, Noelia Sánchez-Maroño, and Amparo Alonso-Betanzos. Recent
advances and emerging challenges of feature selection in the context of big data. Knowledge-
Based Systems, 86:33–45, 2015.

[4] Paul S Bradley and Olvi L Mangasarian. Feature selection via concave minimization and
support vector machines. In Machine Learning Proceedings of the Fifteenth International
Conference(ICML ’98, pages 82–90. Morgan Kaufmann, 1998.

[5] Bin Cao, Dou Shen, Jian-Tao Sun, Qiang Yang, and Zheng Chen. Feature selection in a kernel
space. In Proceedings of the 24th international conference on Machine learning, pages 121–128.
ACM, 2007.

[6] John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Efﬁcient projections onto
the l1-ball for learning in high dimensions. In Proceedings of the 25th International Conference
on Machine Learning, ICML ’08, pages 272–279, New York, NY, USA, 2008. ACM. ISBN
978-1-60558-205-4. doi: 10.1145/1390156.1390191. URL http://doi.acm.org/10.1145/
1390156.1390191.

[7] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning,

volume 1. Springer series in statistics Springer, Berlin, 2001.

[8] Kenji Fukumizu and Chenlei Leng. Gradient-based kernel method for feature extraction and
variable selection. In Advances in Neural Information Processing Systems, pages 2114–2122,
2012.

[9] Kenji Fukumizu, Francis R Bach, and Michael I Jordan. Dimensionality reduction for supervised
learning with reproducing kernel hilbert spaces. Journal of Machine Learning Research, 5(Jan):
73–99, 2004.

[10] Kenji Fukumizu, Francis R Bach, and Michael I Jordan. Kernel dimension reduction in

regression. The Annals of Statistics, pages 1871–1905, 2009.

[11] Ran Gilad-Bachrach, Amir Navot, and Naftali Tishby. Margin based feature selection-theory
and algorithms. In Proceedings of the twenty-ﬁrst international conference on Machine learning,
page 43. ACM, 2004.

[12] Yves Grandvalet and Stéphane Canu. Adaptive scaling for feature selection in svms.

In
Proceedings of the 15th International Conference on Neural Information Processing Systems,
NIPS’02, pages 569–576, Cambridge, MA, USA, 2002. MIT Press. URL http://dl.acm.
org/citation.cfm?id=2968618.2968689.

[13] Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Schölkopf. Measuring statistical
dependence with hilbert-schmidt norms. In International conference on algorithmic learning
theory, pages 63–77. Springer, 2005.

9

[14] Isabelle Guyon and André Elisseeff. An introduction to variable and feature selection. Journal

of machine learning research, 3(Mar):1157–1182, 2003.

[15] Isabelle Guyon, Jason Weston, Stephen Barnhill, and Vladimir Vapnik. Gene selection for
cancer classiﬁcation using support vector machines. Machine learning, 46(1):389–422, 2002.

[16] P Jaganathan, N Rajkumar, and R Nagalakshmi. A kernel based feature selection method used in
the diagnosis of wisconsin breast cancer dataset. Advances in Computing and Communications,
pages 683–690, 2011.

[17] Jundong Li, Kewei Cheng, Suhang Wang, Fred Morstatter, Trevino Robert, Jiliang Tang, and

Huan Liu. Feature selection: A data perspective. arXiv:1601.07996, 2016.

[18] Moshe Lichman. UCI machine learning repository, 2013. URL http://archive.ics.uci.

edu/ml.

[19] Mahdokht Masaeli, Jennifer G Dy, and Glenn M Fung. From transformation-based dimension-
ality reduction to feature selection. In Proceedings of the 27th International Conference on
Machine Learning (ICML-10), pages 751–758, 2010.

[20] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,
M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine
Learning Research, 12:2825–2830, 2011.

[21] Hanchuan Peng, Fuhui Long, and Chris Ding. Feature selection based on mutual information
criteria of max-dependency, max-relevance, and min-redundancy. IEEE Transactions on pattern
analysis and machine intelligence, 27(8):1226–1238, 2005.

[22] Ali Rahimi and Ben Recht. Random features for large-scale kernel machines. In In Neural

Infomration Processing Systems, 2007.

[23] Shaogang Ren, Shuai Huang, John Onofrey, Xenios Papademetris, and Xiaoning Qian. A
Scalable Algorithm for Structured Kernel Feature Selection. In Guy Lebanon and S. V. N.
Vishwanathan, editors, Proceedings of the Eighteenth International Conference on Artiﬁcial
Intelligence and Statistics, volume 38 of Proceedings of Machine Learning Research, pages
781–789, San Diego, California, USA, 09–12 May 2015. PMLR.

[24] Le Song, Alex Smola, Arthur Gretton, Karsten M Borgwardt, and Justin Bedo. Supervised
feature selection via dependence estimation. In Proceedings of the 24th international conference
on Machine learning, pages 823–830. ACM, 2007.

[25] Le Song, Alex Smola, Arthur Gretton, Justin Bedo, and Karsten Borgwardt. Feature selection
via dependence maximization. Journal of Machine Learning Research, 13(May):1393–1434,
2012.

[26] Shiquan Sun, Qinke Peng, and Adnan Shakoor. A kernel-based multivariate feature selection

method for microarray data classiﬁcation. PloS one, 9(7):e102541, 2014.

[27] Jason Weston, Sayan Mukherjee, Olivier Chapelle, Massimiliano Pontil, Tomaso Poggio,
and Vladimir Vapnik. Feature selection for svms. In Proceedings of the 13th International
Conference on Neural Information Processing Systems, pages 647–653. MIT Press, 2000.

[28] Jason Weston, André Elisseeff, Bernhard Schölkopf, and Mike Tipping. Use of the zero-
norm with linear models and kernel methods. Journal of machine learning research, 3(Mar):
1439–1461, 2003.

[29] Makoto Yamada, Wittawat Jitkrittum, Leonid Sigal, Eric P Xing, and Masashi Sugiyama. High-
dimensional feature selection by feature-wise kernelized lasso. Neural computation, 26(1):
185–207, 2014.

10

A Appendix

A.1 Proof of Lemma 1

The appendix is devoted to proofs of various results from the main text.

Suppose there exist two distributions P, Q on Rm such that EP (X)[(cid:101)k(y, X)] = EQ(X)[(cid:101)k1(y, X)]
for any y ∈ Rm. Consider Rm as a subspace embedded in Rd. The probability distributions P
and Q can be extended to Rd by by setting the remaining components to zero. Then we also have
EP (X)[(cid:101)k1(y, X)] = EQ(X)[(cid:101)k1(y, X)] for any y ∈ Rd. As k1 is characteristic, P = Q.

A.2 Proof of Theorem 2 and Corollary 3

Theorem 2 and Corollary 3 can be proved simultaneously. The proof of Theorem 2 parallels the proof
of the corresponding theorem in the setting of dimension dimension reduction by Fukumizu et al.
[10].
We can interpret ˜H1 as a subset of H1, so Equation 7 implies ΣY Y |XT ≥ ΣY Y |X . In the univariate
case, this is equivalent to saying trace[ΣY Y |XT ] ≥ trace[ΣY Y |X ].
By the law of total variance, we have for any g ∈ H2,

EXT varY |XT [g(Y )|XT ] = EX [var[g(Y )|X]] + EXT varY |XT [EY |X [g(Y )|X]].
By Lemma 1, the kernel (cid:101)k1 is characteristic, so the conditional covariance operator characterizes the
conditional dependence, which reduces Equation 21 to

(21)

(cid:104)g, (ΣY Y |XT − ΣY Y |X )g(cid:105) = EXT varY |XT [EY |X [g(Y )|X]].
(22)
Hence ΣY Y |XT = ΣY Y |X if and only if given XT , EY |X [g(Y )|X] is almost surely determined.
Because k2 is characteristic, we have Y ⊥⊥ X|XT . Suppose Y is univariate and k2 is the linear
kernel. Then both ΣY Y |X and ΣY Y |XT can be equivalently interpreted as linear functions that map
real numbers to real numbers. When g = IdY , the identity function on Y , we have
(cid:104)IdY , (ΣY Y |XT − ΣY Y |X )IdY (cid:105) = EXT varY |XT [EY |X [Y |X]] = 0.

(23)

This implies Y ⊥⊥ X|XT directly.

A.3 Proof of Theorem 5

We provide a simpler proof than the one for Theorem 6 in the paper [10], where the consistency result
for dimension reduction was established.

For any subset of features T , we have

| trace[ ˆΣY Y |XT ] − trace[ΣY Y |XT ]|
≤ | trace[ΣY Y |XT ] − trace[ΣY Y − ΣY XT (ΣXT XT + εnI)−1ΣXT Y ]|+
+ | trace[ΣY Y − ΣY XT (ΣXT XT + εnI)−1ΣXT Y ] − trace[ ˆΣY Y |XT ]|,
where the second term converges to zero by the law of large numbers, whereas Fukumizu et al. [10]
proved that the second term can be upper bounded as

(cid:107)HS + (cid:107)ΣY XT (cid:107)HS)(cid:107) ˆΣY XT − ΣY XT (cid:107)HS

1
εn

T

{((cid:107) ˆΣY X (n)
+ (cid:107)ΣY Y (cid:107)trace(cid:107) ˆΣ(n)
+ | trace[ ˆΣY Y − ΣY Y ]|},

XT XT

− ΣXT XT (cid:107)HS

where (cid:107) · (cid:107)HS is the HSIC norm of an operator. By the Central Limit Theorem, both of the terms

(cid:107) ˆΣY XT − ΣY XT (cid:107)HS, (cid:107) ˆΣ(n)

− ΣXT XT (cid:107)HS
are guaranteed to be of order Op(n−1/2). Hence, the second term also converges to 0. This establishes
the convergence of trace[ ˆΣY Y |XT ] towards trace[ΣY Y |XT ], which yields the claim (15) by standard
ε–δ arguments.

| trace[ ˆΣY Y − ΣY Y ]|

XT XT

and

11

