BERT-ATTACK: Adversarial Attack Against BERT Using BERT

Linyang Li, Ruotian Ma,Qipeng Guo, Xiangyang Xue, Xipeng Qiu∗
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, China
{linyangli19,rtma19,qpguo16,xyxue,xpqiu}@fudan.edu.cn

0
2
0
2
 
r
p
A
 
1
2
 
 
]
L
C
.
s
c
[
 
 
1
v
4
8
9
9
0
.
4
0
0
2
:
v
i
X
r
a

Abstract

Adversarial attacks for discrete data (such as
text) has been proved signiﬁcantly more chal-
lenging than continuous data (such as image),
since it is difﬁcult to generate adversarial sam-
ples with gradient-based methods. Currently,
the successful attack methods for text usually
adopt heuristic replacement strategies on char-
acter or word level, which remains challeng-
ing to ﬁnd the optimal solution in the mas-
sive space of possible combination of replace-
ments, while preserving semantic consistency
In this paper, we pro-
and language ﬂuency.
pose BERT-Attack, a high-quality and effec-
tive method to generate adversarial samples
using pre-trained masked language models ex-
empliﬁed by BERT. We turn BERT against its
ﬁne-tuned models and other deep neural mod-
els for downstream tasks. Our method suc-
cessfully misleads the target models to pre-
dict incorrectly, outperforming state-of-the-art
attack strategies in both success rate and per-
turb percentage, while the generated adversar-
ial samples are ﬂuent and semantically pre-
served. Also, the cost of calculation is low,
thus possible for large-scale generations.

1

Introduction

Despite the success of deep learning, recent works
have found that these neural networks are vulnera-
ble to adversarial samples, which are crafted with
small perturbations to the original inputs (Goodfel-
low et al., 2014; Kurakin et al., 2016; Chakraborty
et al., 2018). That is, these adversarial samples are
imperceptible to human judges while they can mis-
lead the neural networks to incorrect predictions.
Therefore, it is essential to explore these adver-
sarial attack methods since the ultimate goal is to
make sure the neural networks are highly reliable
and robust. While in computer vision ﬁelds, both
attack strategies and their defense countermeasures

∗Corresponding author.

are well-explored (Chakraborty et al., 2018), the
adversarial attack for text is still challenging due
to the discrete nature of languages. Generating of
adversarial samples for text needs to possess such
qualities: (1) imperceptible to human judges yet
misleading to neural models; (2) ﬂuent in grammar
and semantically consistent with original inputs.

Previous methods craft adversarial samples
mainly based on speciﬁc rules (Li et al., 2018; Gao
et al., 2018; Yang et al., 2018; Jin et al., 2019).
Therefore, these methods are difﬁcult to guarantee
the ﬂuency and semantically preservation in the
generated adversarial samples. These manual craft
methods are rather complicated. They are designed
with multiple linguistic constraint like NER tag-
ging or POS tagging. Introducing contextualized
language models to serve as an automatic pertur-
bation generator could make these rules designing
much easier.

Recent rise of pre-trained language models, such
as BERT (Devlin et al., 2018), push the perfor-
mances of NLP tasks to a new level. On the one
hand, the powerful ability of a ﬁne-tuned BERT
on downstream tasks makes it more challenging to
be adversarial attacked (Jin et al., 2019). On the
other hand, BERT is a pre-trained masked language
model on extremely large-scale unsupervised data
and has learned general-purpose language knowl-
edge. Therefore, BERT has the potential to gener-
ate more ﬂuent and semantic-consistent substitu-
tions for an input text. Naturally, both the proper-
ties of BERT motivate us to explore the possibility
of attacking a ﬁne-tuned BERT with another BERT
as attacker.

In this paper, we propose an effective and
high-quality adversarial sample generation method:
BERT-Attack, using BERT as a language model to
generate adversarial samples. The core algorithm
of BERT-Attack is straightforward and consists of
two stages: ﬁnding the vulnerable words in one

given input sequence for the target model, then ap-
plying BERT to generate substitutes for the vulner-
able words. With the powerful ability of BERT, the
perturbations are generated considering the context
around. Therefore, the perturbations are ﬂuent and
reasonable. We uses the masked language model as
perturbation generator and ﬁnd perturbations that
maximize the risk of making wrong predictions
(Goodfellow et al., 2014). Differently from pre-
vious attacking strategies that requires traditional
single-direction language models as a constraint,
we only need to inference the language model once
as perturbation generator rather than repeatedly
using language models to score the generated ad-
versarial samples in a trail and error process.

Experimental results show that the proposed
BERT-Attack method successfully fooled its ﬁne-
tuned downstream model with the highest attack
success rate compared with previous methods.
Meanwhile, the perturb percentage is considerably
low, so does the query number, while the semantic
preservation is high.

To summarize our main contributions:

to generate ﬂuent

• We propose a simple and effective method,
BERT-Attack,
and
semantically-preserved adversarial samples
that can successfully mislead state-of-the-art
models in NLP, such as ﬁne-tuned BERT for
various downstream tasks.

• BERT-Attack has higher attacking success
rate and lower perturb percentage with less
access numbers to the target model compared
with previous attacking algorithms, while
does not require extra scoring models there-
fore extremely effective.

• We can generate adversarial samples with
BERT-Attack as a parallel dataset for further
research on the robustness of NLP models.

2 Related Work

To explore the robustness of neural networks, ad-
versarial attack has extensively studied for continu-
ous data (such as image) (Goodfellow et al., 2014;
Nguyen et al., 2015; Chakraborty et al., 2018). The
key idea is to ﬁnd a minimal perturbation that max-
imize the risk of making wrong predictions. This
minimax problem can be easily achieved by apply-
ing gradient descent over the continuous space of
images. However, adversarial attack for discrete
data such as text remains challenging.

Adversarial Attack for Text Current success-
ful attacks for text usually adopt heuristic rules
to modify the characters of a word (Jin et al.,
2019), and substituting words with synonyms (Ren
et al., 2019). Li et al. (2018); Gao et al. (2018)
apply perturbations based on word embeddings
such as Glove (Pennington et al., 2014), which
is not strictly semantically and grammatically coor-
dinated. Alzantot et al. (2018) adopts language
models to score the perturbations generated by
searching for close meaning words in the Glove
(Pennington et al., 2014) embeddings, using trail
and error to ﬁnd possible perturbations, yet the per-
turbations generated are still not context-aware and
heavily rely on cosine similarity measurement of
word embeddings. Glove embeddings do not guar-
antee similar vector space with cosine similarity
distance, therefore the perturbations are less seman-
tically consistent. Jin et al. (2019) apply a seman-
tically enhanced embedding (Mrkˇsi´c et al., 2016),
which is context unaware, thus less consistent with
the unperturbed inputs. Liang et al. (2017) use
phrase-level insertion and deletion, which produces
unnatural sentences inconsistent with the original
inputs, lacking ﬂuency control. To preserve se-
mantic information, Glockner et al. (2018) replace
words manually to break language inference sys-
tem (Bowman et al., 2015). Jia and Liang (2017)
propose manual craft methods to attack machine
reading comprehension systems. Lei et al. (2019)
introduce replacement strategies using embedding
transition.

Although the above approaches have achieved
good results, there is still much room for improve-
ment regarding the perturbed percentage, attacking
success rate, grammatical correctness and semantic
consistency, etc. Moreover, the substitution strate-
gies of these approaches are usually non-trivial,
resulting in that they are limited to speciﬁc tasks.

Adversarial Attack against BERT Pre-trained
language models, have become the mainstream for
many NLP tasks. Works such as (Wallace et al.,
2019; Jin et al., 2019; Pruthi et al., 2019) have ex-
plored these pre-trained language models in many
different angles. Wallace et al. (2019) explored The
possible ethical problems of learned knowledge in
pre-trained models.

From our perspective, we take the idea of turning
such language models against themselves. There-
fore, we introduce a novel BERT-Attack algorithm
to attack the ﬁne-tuned models.

3 BERT-Attack

Motivated by the interesting idea of turning BERT
against BERT, we propose BERT-Attack, using
the original BERT model to craft adversarial sam-
ples to fool the ﬁne-tuned BERT model.

Our method consists of two steps: (1) ﬁnding
the vulnerable words for the target model and then
(2) replacing them with the semantically similar
and grammatically correct words until a successful
attack.

The most-vulnerable words are the key words
that help the target model make judgements. Per-
turbations over these words can be most beneﬁ-
cial in crafting adversarial samples. After ﬁnding
which words that we are aimed to perturbate, we
use masked language models to generate pertur-
bations based on the top-K predictions from the
masked language model.

3.1 Finding Vulnerable Words

Under the black-box scenario, the logit output by
the target model (ﬁne-tuned BERT or other neural
models) is the only supervision we can get. We
ﬁrst select the words in the sequence which have a
high signiﬁcance inﬂuence on the ﬁnal output logit.
Let S = [w0, · · · , wi · · · ] denote the input sen-
tence, and oy(S) denote the logit output by the
target model for correct label y, the importance
score Iwi is deﬁned as

Iwi = oy(S) − oy(S\wi),

(1)

where S\wi = [w0, · · · , wi−1, [MASK], wi+1, · · · ]
is the sentence after replacing wi with [MASK].

Then we rank all the words according to the
ranking score Iwi in descending order to create
word list L. We only take (cid:15) percent of the most im-
portant words since we tend to keep perturbations
minimum.

This process maximize the risk of making wrong
predictions which is previously done by calculat-
ing gradients in image domains. The problem is
then formulated as replacing these most vulnerable
words with semantically consistent perturbations.

3.2 Word Replacement via BERT

After ﬁnding the vulnerable words, we iteratively
replace the words in list L one by one to ﬁnd pertur-
bations that can mislead target model. Previous ap-
proaches usually use multiple human-crafted rules
to ensure the generated example is semantically
consistent with the original one and grammatically

Figure 1: One step of our replacement strategy.

correct, such as synonym dictionary (Ren et al.,
2019), POS checker (Jin et al., 2019), semantic
similarity checker (Jin et al., 2019), etc. Alzantot
et al. (2018) applies traditional language model to
score the perturbated sentence at every attempt of
replacing a word.

These strategies of ﬁnding substitutes are un-
aware of the context between the perturb positions,
thus are insufﬁcient in ﬂuency control and seman-
tic consistency. More importantly, using language
models or POS checker in scoring the perturbated
samples is costly since this trail and error process
requires massive inference time.

To overcome the lack of ﬂuency control and se-
mantic preservation by using synonyms or similar
words in the embedding space, we leverage BERT
for word replacement. The genuine nature of the
masked language model makes sure that the gener-
ated sentences are relatively ﬂuent and grammar-
correct, also preserve most semantic information.
Further, compared with previous approaches us-
ing rule-based perturbation strategies, the masked
language model prediction is context-aware, thus
dynamically searches for perturbations rather than
simple synonyms replacing.

Different from previous methods using compli-
cated strategies to score and constrain the perturba-
tions, contextualized perturbation generator gener-
ates minimal perturbations with only one forward
pass. The time-consuming part is accessing target
model only without running models to score the
sentence, therefore extremely efﬁcient.

Thus, using the masked language model as a
contextualized perturbation generator can be one
possible solution to craft high-quality adversarial
samples efﬁciently.

Algorithm 1 BERT-Attack

1: procedure WORD IMPORTANCE RANKING
S = [w0, w1, · · · ] // tokenized sentence
2:
Y ← gold-label
for wi in S do

3:

4:

calculate importance score Iwi using Eq. 1

select word list L = [wtop−1, wtop−2, · · · ]
// sort S using Iwi in descending order and collect top − K words

8: procedure REPLACEMENT USING BERT
9:

H = [h0, · · · , hn] // sub-word tokenized sentence
generate top-K candidates for all sub-words using BERT and get P ∈n×K
for wj in L do

if wj is a whole word then

get candidate C = F ilter(P )
replace word wj

else

get candidate C using PPL ranking and Filter
replace sub-words [hj, · · · , hj+t]

Find Possible Adversarial Sample
for ck in C do

S (cid:48) = [w0, · · · , wj−1, ck, · · · ] // attempt
if argmax(o(H (cid:48)))! = Y then

return Sadv = S (cid:48)

// success attack

else

if oS(cid:48) < oSadv then

Sadv = [w0, · · · , wj−1, c, · · · ] // do one perturbation

return None

5:

6:

7:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

21:

22:

23:

24:

25:

26:

3.2.1 Word Replacement Strategy
As seen in Figure 1, given a chosen word w to
be replaced, we apply BERT to predict the pos-
sible words that are similar to w yet can mislead
the target model. Instead of following the masked
language model settings, we do not mask the cho-
sen word w and use the original sequence as input,
which can generate more semantic-consistent sub-
stitutes. For instance, given a sequence ”I like the
cat.”, if we mask the word cat, it would be very
hard for a masked language model to predict the
original word cat since it could be just as ﬂuent
if the sequence is ”I like the dog.”. Further, if we
mask out the given word w, for each iteration we
would have to rerun the masked language model
prediction process which is costly.

Since BERT uses Bytes-Pair-Encoding (BPE)
to tokenize the sequence S = [w0, · · · , wi, · · · ]
into sub-word tokens: H = [h0, h1, h2, · · · ], we
need algin the chosen word to its corresponding
sub-words in BERT.

Let M denote the BERT model, we feed the

tokenized sequence H into the BERT M to get
output prediction P = M(H). Instead of using
argmax prediction, we take the most possible K
predictions at each position, where K is a hyper-
parameter.

We iterate words that are sorted by word impor-
tance ranking process to ﬁnd perturbations. BERT
model uses BPE encoding to construct vocabular-
ies. While most words are still single words, rare
words are tokenized into sub-words. We treat sin-
gle words and sub-words separately to generate the
substitutes.

Single words For a single word wj, we make
attempts using the corresponding top-K predic-
tion candidates P j∈ K. We ﬁrst ﬁlter out stop
words collected from NLTK; for sentiment classi-
ﬁcation tasks we ﬁlter out antonyms using syn-
onym dictionaries (Mrkˇsi´c et al., 2016) since
BERT masked language model does not distin-
guish synonyms and antonyms. Then for given
candidate ck we construct a perturbed sequence

H (cid:48) = [h0, · · · , hj−1, ck, hj+1 · · · ]. If the target
model is already fooled to predict incorrectly, we
break the loop to obtain the ﬁnal adversarial sam-
ple H adv; otherwise, we select from the ﬁltered
candidates to pick one best perturbation and turn to
the next word in word list L.

Sub-words For word that is tokenized into sub-
words in BERT, we cannot obtain its substitutes
directly. Thus we use the perplexity of sub-word
combinations to craft word substitutes from pre-
dictions in sub-word level. Given sub-words
[h0, h1, · · · , ht] of word w, we list all possible
combinations from the prediction P ∈t×K from M,
which is Kt sub-word combinations, we can con-
vert them back to normal words by reversing the
BERT tokenization process. Then we use the per-
plexity of all combinations to get top-K combina-
tions; in this way, those combinations that are less
likely to be a natural word are ﬁltered out.

Then we replace the original word with the most
likely perturbation and repeat this process by iter-
ating the importance word ranking list to ﬁnd ﬁnal
adversarial sample. In this way, we acquire the
adversarial samples Sadv effectively since we only
iterate the masked language model once and do per-
turbations using masked language model without
other checking strategies.

We summarize the two-step BERT-Attack pro-

cess in Algorithm 1.

4 Experiments

4.1 Datasets

We apply our method to attack different types of
NLP tasks in the form of text classiﬁcation and
natural language inference. Following Jin et al.
(2019), we evaluate our method on 1k test samples
randomly selected from the test set of the given
task which are the same splits used by (Alzantot
et al., 2018; Jin et al., 2019).

Text Classiﬁcation We use different types of text
classiﬁcation tasks to study the effectiveness of our
method.

• Yelp Review classiﬁcation dataset, containing.
Following Zhang et al. (2015), we process the
dataset to construct a polarity classiﬁcation
task.

• IMDB Document-level movie review dataset,
where the average sequence length is longer

than Yelp dataset. We process the dataset into
a polarity classiﬁcation task 1.

• AG’s News Sentence level news-type classi-
ﬁcation dataset, containing 4 types of news:
World, Sports, Business, and Science.

• FAKE Fake News Classiﬁcation dataset, de-
tecting whether a news document is fake from
Kaggle Fake News Challenge 2.

Natural Language Inference

• SNLI Stanford language inference task (Bow-
man et al., 2015). Given one premise and one
hypothesis, and the goal is to predict if the hy-
pothesis is entailment, neural, or contradiction
of the premise.

• MNLI Language inference dataset on multi-
genre texts, covering transcribed speech, pop-
ular ﬁction, and government reports (Williams
et al., 2018), which is more complicated with
diversiﬁed written and spoken style texts, com-
pared with SNLI dataset, including eval data
matched with training domains and eval data
mismatched with training domains.

4.2 Automatic Evaluation Metrics

To measure the quality of the generated samples,
we set up various automatic evaluation metrics.
The success rate, which is the counter-part of after-
attack accuracy, is the core metric measuring the
success of the attacking method. Meanwhile, the
perturbed percentage is also crucial since, gen-
erally, less perturbation results in more semantic
consistency. Further, under the black-box setting,
queries of the target model are the only accessible
information. Constant queries for one sample is
less applicable. Thus query number per sample
is also a key metric. As used in TextFooler (Jin
et al., 2019), we also use Universal Sentence En-
coder (Cer et al., 2018) to measure the semantic
consistency between the adversarial sample and the
original sequence. To balance between semantic
preservation and attack success rate, we set up a
threshold of semantic similarity score to ﬁlter the
less similar examples.

4.3 Attacking Results

As shown in Table 1, BERT-Attack method suc-
cessfully fool its downstream ﬁne-tuned model. In

1 https://datasets.imdbws.com/
2 https://www.kaggle.com/c/fake-news/data

Dataset

Method

Original Acc Attacked Acc Perturb % Query Number Avg Len Semantic Sim

Fake

Yelp

IMDB

AG

SNLI

MNLI
matched

BERT-Attack(ours)

TextFooler(Jin et al., 2019)

BERT-Attack(ours)

TextFooler

BERT-Attack(ours)

TextFooler

BERT-Attack(ours)

TextFooler

BERT-Attack(ours)

TextFooler

BERT-Attack(ours)

TextFooler

MNLI
mismatched

BERT-Attack(ours)

TextFooler

97.8

95.6

90.9

94.2

7.2

19.3

5.1

6.6

11.4

13.6

10.6

12.5

1.1

11.7

4.1

12.8

4.4

6.1

15.4

22.0

89.4(H/P)

85.1(H/P)

82.1(H/P)

7.4/16.1

4.0/20.8

7.9/11.9

9.6/25.3

7/13.7

12.4/9.3

18.5/33.4

8.8/7.9

15.2/26.5

8.0/7.1

8.3/22.9

14.6/24.7

858

4403

273

743

454

1134

213

357

16/30

60/142

19/44

78/152

24/43

86/162

885

157

215

43

8/18

11/21

12/22

0.81

0.76

0.77

0.74

0.86

0.86

0.63

0.57

0.40/0.55

0.45/0.54

0.55/0.68

0.57/0.65

0.53/0.69

0.58/0.65

Table 1: Results of attacking against various ﬁne-tuned BERT models. TextFooler is the state-of-the-art baseline.
For MNLI task, we attack the hypothesis(H) or premises(P) separately.

both text classiﬁcation and natural language infer-
ence tasks, the ﬁne-tuned BERTs fail to classify
the generated adversarial samples correctly.

The average after-attack accuracy is lower than
10%, indicating that most samples are successfully
perturbated to fool the state-of-the-art classiﬁcation
models. Meanwhile, the perturb percentage is less
than 10 %, which is signiﬁcantly less than previous
works.

Further, BERT-Attack successfully attacked all
tasks listed, which are in diversiﬁed domains such
as News classiﬁcation, review classiﬁcation, lan-
guage inference in different domains. The results
indicate that the attacking method is robust in dif-
ferent tasks. Compared with the strong baseline
introduced by Jin et al. (2019), the BERT-Attack
method is more efﬁcient and more imperceptible.
The query number and the perturbation percentage
of our method are much less.

We can observe that it is generally easier to at-
tack the review classiﬁcation task since the perturb
percentage is incredibly low. BERT-Attack can
mislead the target model by replacing a handful of
words only. Since the average sequence length is
relatively long, the target model tends to make judg-
ments by only a few words in a sequence, which is
not the natural way of human prediction. Thus, the
perturbation of these keywords would result in in-
correct prediction from the target model, revealing
the vulnerability of it.

4.4 Human Evaluations

For further evaluation of the generated adversarial
samples, we set up human evaluations to measure
the quality of the generated samples in ﬂuency and
grammar as well as semantic preservation.

We ask human judges to score the grammar cor-
rectness of the mixed sentences of generated adver-
sarial samples and original sequences, scoring from
1-5 following (Jin et al., 2019). Then we ask hu-
man judges to make predictions for the generated
adversarial samples mixed with original samples.
We use IMDB dataset and MNLI dataset, and for
each task, we select 100 samples of both original
and adversarial samples for human judges.

Seen in Table 2, semantic score and label pre-
diction of adversarial samples are close to original
ones. MNLI task is a sentence pair prediction task
constructed by human crafted hypotheses based on
premises, therefore original pairs share a consider-
able amount of same words. Perturbations on these
words would make it difﬁcult for human judges to
predict correctly therefore the accuracy is lower
than simple sentence classiﬁcation task.

Dataset Accuracy Ori/Adv Semantic Ori/Adv

MNLI

IMDB

0.90/0.70

0.91/0.85

3.9/3.7

4.1/3.9

Table 2: Human-Evaluation Results

4.5 BERT-Attack against Other Models

Dataset Model Word-LSTM BERT-base BERT-large

The BERT-Attack method is also applicable in at-
tacking other target models, not limited to its ﬁne-
tuned model only. As seen in Table 3, the attack is
successful against LSTM-based models, indicating
that BERT-Attack is feasible for a wide range of
models. Under BERT-Attack, ESIM model is more
robust in MNLI dataset. We assume that encoding
two sentences separately gets higher robustness.
In attacking BERT-large models, the performance
is also excellent, indicating that BERT-Attack is
successful in attacking different pre-trained mod-
els not only against its own ﬁne-tuned downstream
models.

Dataset Model

ESIM

BERT-base BERT-large

IMDB

Word-LSTM

BERT-base

BERT-large

MNLI

ESIM

BERT-base

BERT-large

0.83

0.87

-

-

0.60

0.59

0.78

0.86

0.59

-

-

0.43

0.75

0.71

-

-

0.60

0.45

Table 4: Transferability analysis. The column is the
target model used in attack, and the row is the tested
model. All attacks are using BERT-base as masked lan-
guage model.

Dataset Model

Ori Acc Atk Acc Perturb %

5.2

Importance of Sequence Length

IMDB

Yelp

Word-LSTM 89.8

BERT-Large

98.2

Word-LSTM 96.0

BERT-Large

ESIM

MNLI
matched BERT-Large

97.9

76.2

86.4

10.2

12.4

1.1

8.2

9.6

13.2

2.7

2.9

4.7

4.1

21.7

7.4

Table 3: BERT-Attack against other models.

BERT-Attack method is based on the contextual-
ized masked language model. Thus the sequence
length plays an important role in high-quality per-
turbation process. As seen, instead of the previous
methods focusing on attacking the hypothesis of
NLI task, we aim at premises whose average length
is longer. This is because we believe that contex-
tual replacement would be less reasonable when
dealing with extremely short sequences. To avoid
such a problem, we believe that many word-level
synonym replacement strategies can be combined
with BERT-Attack, allowing BERT-Attack method
to be more applicable.

Dataset Method Ori Acc Atk Acc Perturb %

BERT-Atk

MNLI
matched +Adv Train

85.1

84.6

7.9

23.1

8.8

10.5

Table 5: Adversarial training.

5.3 Transferability and Adversarial Training

To test the transferability of the generated adver-
sarial samples, we take samples aimed at different
target models to attack other target models. Here,
we use BERT-base as masked language model for
all different target models. As seen in Table 4,
samples are transferable in NLI task while less
transferable in text classiﬁcation.

Meanwhile, we further ﬁne-tune the target model
using the generated adversarial samples from train
set and then test it on test set used before. As seen
in Table 5, generated samples used in ﬁne-tuning
help target model become more robust while ac-
curacy is close to the model trained with clean
datasets. The attack becomes more difﬁcult, in-

Figure 2: Different candidate number K.

5 Ablations and Discussions

5.1

Importance of Candidate Numbers

The candidate pool range is the major hyper-
parameter used in BERT-Attack algorithm. As seen
in Figure 2, the attack rate is rising along with the
candidate size increasing. Intuitively, a larger K
would result in less semantic similarity. However,
the semantic measure via Universal Sentence En-
coder is maintained in a stable range, (experiments
show that semantic similarities drop less than 2%),
indicating that the candidates are all reasonable and
semantically consistent with the original sentence.

Dataset

MNLI

IMDB

FAKE

Ori

Adv

Ori

Adv

Ori Some rooms have balconies .

Hypothesis

All of the rooms have balconies off of them .

Contradiction

Adv Many rooms have balconies .

Hypothesis

All of the rooms have balconies off of them .

Neutral

it is hard for a lover of the novel northanger abbey to sit through this bbc adaptation and to
keep from throwing objects at the tv screen... why are so many facts concerning the tilney family
and mrs . tilney ’ s death altered unnecessarily ? to make the story more ‘ horrible ? ’

it is hard for a lover of the novel northanger abbey to sit through this bbc adaptation and to
keep from throwing objects at the tv screen... why are so many facts concerning the tilney family
and mrs . tilney ’ s death altered unnecessarily ? to make the plot more ‘ horrible ? ’

the us may soon face an apocalyptic seismic event starkman today , ... earthquakes ..., as geologists Unreliable
say . via usualroutine the university of washington has already presented seismological ... charts
showing a gigantic geological rift that ... when scientists found a strange underground rupture ...

the us may soon face an apocalyptic seismic event starkman today , ... earthquakes ..., as geologists Reliable
say . en usualroutine the university of washington , already presented seismological ... charts
showing a gigantic geological rift that ... when scientists found a strange underground rupture ...

Label

Negative

Positive

Table 6: Some generated adversarial samples. Origin label is the correct prediction while label is adverse prediction.
Only red color parts are perturbed. We only attack premises in MNLI task. Text in FAKE dataset and IMDB dataset
is cut to ﬁt in the table. Original text contains more than 200 words.

dicating that the model is harder to be attacked.
Therefore, the generated dataset can be used as
additional data for further exploration of making
neural models more robust.

Dataset Method Atk Acc Perturb % Semantic

MNLI
matched

MIR

7.9

Random 20.2

LIR

27.2

8.8

12.2

15.0

0.68

0.60

0.60

Dataset Model

Atk Acc Perturb % Semantic

Yelp

MNLI

BERT-Atk

w/o sub-word

BERT-Atk

w/o sub-word

5.1

7.1

11.9

14.7

4.1

4.3

7.9

9.3

0.77

0.74

0.68

0.63

Table 7: Effects on sub-word level attack.

5.4 Effects on Sub-Word Level Attack

BPE method is currently the most efﬁcient way to
deal with a large number of words, as used in BERT.
We establish a comparative experiment where we
do not use sub-word level attack. That is we skip
those words that are tokenized with multiple sub-
words.

As seen in Table 7, using sub-word level attack
can achieve higher performances, not only in higher
attacking success rate but also in less perturbation
percentage.

5.5 Effects on Word Importance Ranking

Word importance ranking strategy is supposed to
ﬁnd keys are essential to NN models, which is very
much like calculating the maximum risk of wrong
predictions in FGSM algorithm (Goodfellow et al.,
2014). When not using word importance ranking,
attacking algorithm is less successful.

Table 8: Most Importance Ranking (MIR) vs Least Im-
portance Ranking (LIR)

5.6 Examples of Generated Adversarial

Sentences

As seen in Table 6, the generated adversarial sam-
ples are semantically consistent with its original
input, while the target model makes incorrect pre-
dictions. In both review classiﬁcation samples and
language inference samples, the perturbations do
not mislead human judges.

6 Conclusion

In this work, we propose a high-quality and effec-
tive method BERT-Attack to generate adversarial
samples using BERT masked language model. Ex-
periment results show that the proposed method
achieves a high success rate while maintaining a
minimum perturbation. Nevertheless, candidates
generated from the masked language model can
sometimes be antonyms or irrelevant to the original
words, causing a semantic loss. Thus, enhancing
language models to generate more semantically re-
lated perturbations can be one possible solution to
perfect BERT-Attack in the future.

Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian,
Deep
arXiv preprint

Xirong Li, and Wenchang Shi. 2017.
text classiﬁcation can be fooled.
arXiv:1704.08006.

Nikola Mrkˇsi´c, Diarmuid O S´eaghdha, Blaise Thom-
son, Milica Gaˇsi´c, Lina Rojas-Barahona, Pei-
Hao Su, David Vandyke, Tsung-Hsien Wen, and
Counter-ﬁtting word vec-
Steve Young. 2016.
arXiv preprint
tors to linguistic constraints.
arXiv:1603.00892.

Anh Nguyen, Jason Yosinski, and Jeff Clune. 2015.
Deep neural networks are easily fooled: High con-
ﬁdence predictions for unrecognizable images.
In
Proceedings of the IEEE conference on computer vi-
sion and pattern recognition, pages 427–436.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the conference on
empirical methods in natural language processing,
pages 1532–1543.

Danish Pruthi, Bhuwan Dhingra, and Zachary C Lip-
Combating adversarial misspellings
arXiv preprint

ton. 2019.
with robust word recognition.
arXiv:1905.11268.

Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che.
2019. Generating natural language adversarial ex-
amples through probability weighted word saliency.
In Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics, pages
1085–1097.

Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner,
and Sameer Singh. 2019. Universal adversarial trig-
gers for attacking and analyzing NLP. Empirical
Methods in Natural Language Processing.

Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 1112–1122.

Puyudi Yang, Jianbo Chen, Cho-Jui Hsieh, Jane-Ling
Wang, and Michael I Jordan. 2018. Greedy attack
and gumbel attack: Generating adversarial examples
for discrete data. arXiv preprint arXiv:1805.12316.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
In Advances in neural information pro-
siﬁcation.
cessing systems, pages 649–657.

References

Moustafa Alzantot, Yash Sharma, Ahmed Elgohary,
Bo-Jhang Ho, Mani B. Srivastava, and Kai-Wei
Chang. 2018. Generating natural language adversar-
ial examples. CoRR, abs/1804.07998.

Samuel R Bowman, Gabor Angeli, Christopher Potts,
and Christopher D Manning. 2015. A large anno-
tated corpus for learning natural language inference.
arXiv preprint arXiv:1508.05326.

Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,
Nicole Limtiaco, Rhomni St John, Noah Constant,
Mario Guajardo-Cespedes, Steve Yuan, Chris Tar,
arXiv
et al. 2018. Universal sentence encoder.
preprint arXiv:1803.11175.

Anirban Chakraborty, Manaar Alam, Vishal Dey, Anu-
pam Chattopadhyay, and Debdeep Mukhopadhyay.
2018. Adversarial attacks and defences: A survey.
arXiv preprint arXiv:1810.00069.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: pre-training of
deep bidirectional transformers for language under-
standing. CoRR, abs/1810.04805.

Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yan-
jun Qi. 2018. Black-box generation of adversarial
text sequences to evade deep learning classiﬁers. In
2018 IEEE Security and Privacy Workshops (SPW),
pages 50–56.

Max Glockner, Vered Shwartz, and Yoav Goldberg.
2018. Breaking nli systems with sentences that
arXiv preprint
require simple lexical inferences.
arXiv:1805.02266.

Ian J Goodfellow, Jonathon Shlens, and Christian
Szegedy. 2014. Explaining and harnessing adversar-
ial examples. arXiv preprint arXiv:1412.6572.

Robin Jia and Percy Liang. 2017. Adversarial exam-
ples for evaluating reading comprehension systems.
arXiv preprint arXiv:1707.07328.

Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter
Is BERT really robust? natural
Szolovits. 2019.
language attack on text classiﬁcation and entailment.
CoRR, abs/1907.11932.

Alexey Kurakin, Ian Goodfellow, and Samy Bengio.
2016. Adversarial examples in the physical world.
arXiv preprint arXiv:1607.02533.

Qi Lei, Lingfei Wu, Pin-Yu Chen, Alexandros G Di-
makis, Inderjit S Dhillon, and Michael Witbrock.
2019. Discrete adversarial attacks and submodular
optimization with applications to text classiﬁcation.
Systems and Machine Learning (SysML).

Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting
Wang. 2018. Textbugger: Generating adversarial
text against real-world applications. arXiv preprint
arXiv:1812.05271.

