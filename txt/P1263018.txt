8
1
0
2
 
t
c
O
 
0
2
 
 
]
L
C
.
s
c
[
 
 
2
v
0
5
9
9
0
.
7
0
8
1
:
v
i
X
r
a

Variational Memory Encoder-Decoder

Hung Le, Truyen Tran, Thin Nguyen and Svetha Venkatesh
Applied AI Institute, Deakin University, Geelong, Australia
{lethai,truyen.tran,thin.nguyen,svetha.venkatesh}@deakin.edu.au

Abstract

Introducing variability while maintaining coherence is a core task in learning to
generate utterances in conversation. Standard neural encoder-decoder models and
their extensions using conditional variational autoencoder often result in either
trivial or digressive responses. To overcome this, we explore a novel approach that
injects variability into neural encoder-decoder via the use of external memory as
a mixture model, namely Variational Memory Encoder-Decoder (VMED). By as-
sociating each memory read with a mode in the latent mixture distribution at each
timestep, our model can capture the variability observed in sequential data such
as natural conversations. We empirically compare the proposed model against
other recent approaches on various conversational datasets. The results show that
VMED consistently achieves signiﬁcant improvement over others in both metric-
based and qualitative evaluations.

1

Introduction

Recent advances in generative modeling have led to exploration of generative tasks. While gener-
ative models such as GAN [13] and VAE [21, 32] have been applied successfully for image gen-
eration, learning generative models for sequential discrete data is a long-standing problem. Early
attempts to generate sequences using RNNs [14] and neural encoder-decoder models [18, 38] gave
promising results, but the deterministic nature of these models proves to be inadequate in many re-
alistic settings. Tasks such as translation, question-answering and dialog generation would beneﬁt
from stochastic models that can produce a variety of outputs for an input. For example, there are
several ways to translate a sentence from one language to another, multiple answers to a question
and multiple responses for an utterance in conversation.

Another line of research that has captured attention recently is memory augmented neural networks
(MANNs). Such models have larger memory capacity and thus “remember” temporally distant
information in the input sequence and provide a RAM-like mechanism to support model execution.
MANNs have been successfully applied to long sequence prediction tasks [15, 36] demonstrating
great improvement when compared to other recurrent models. However, the role of memory in
sequence generation has not been well understood.

For tasks involving language understanding and production, handling intrinsic uncertainty and latent
variations is necessary. The choice of words and grammars may change erratically depending on
speaker intentions, moods and previous languages used. The underlying RNN in neural sequential
models ﬁnds it hard to capture the dynamics and their outputs are often trivial or too generic [25].
One way to overcome these problems is to introduce variability into these models. Unfortunately,
sequential data such as speech and natural language is a hard place to inject variability [33] since
they require a coherence of grammars and semantics yet allow freedom of word choice.

We propose a novel hybrid approach that integrates MANN and VAE, called Variational Memory
Encoder-Decoder (VMED), to model the sequential properties and inject variability in sequence
generation tasks. We introduce latent random variables to model the variability observed in the data

Preprint. Work in progress.

and capture dependencies between the latent variables across timesteps. Our assumption is that there
are latent variables governing an output at each timestep. In the conversation context, for instance,
the latent space may represent the speaker’s hidden intention and mood that dictate word choice and
grammars. For a rich latent multimodal space, we use a Mixture of Gaussians (MoG) because a
spoken word’s latent intention and mood can come from different modes, e.g., whether the speaker
is asking or answering, or she/he is happy or sad. By modeling the latent space as an MoG where
each mode associates with some memory slot, we aim to capture multiple modes of the speaker’s
intention and mood when producing a word in the response. Since the decoder in our model has
multiple read heads, the MoG can be computed directly from the content of chosen memory slots.
Our external memory plays a role as a mixture model distribution generating the latent variables that
are used to produce the output and take part in updating the memory for future generative steps.

To train our model, we adapt Stochastic Gradient Variational Bayes (SGVB) framework [21]. In-
stead of minimizing the KL divergence directly, we resort to using its variational approximation [16]
to accommodate the MoG in the latent space. We show that minimizing the approximation results
in KL divergence minimization. We further derive an upper bound on our total timestep-wise KL
divergence and demonstrate that minimizing the upper bound is equivalent to ﬁtting a continuous
function by a scaled MoG. We validate the proposed model on the task of conversational response
generation. This task serves as a nice testbed for the model because an utterance in a conversation
is conditioned on previous utterances, the intention and the mood of the speaker. Finally, we eval-
uate our model on two open-domain and two closed-domain conversational datasets. The results
demonstrate our proposed VMED gains signiﬁcant improvement over state-of-the-art alternatives.

2 Preliminaries

2.1 Memory-augmented Encoder-Decoder Architecture

A memory-augmented encoder-decoder (MAED) consists of two neural controllers linked via ex-
ternal memory. This is a natural extension to read-write MANNs to handle sequence-to-sequence
problems. In MAED, the memory serves as a compressor that encodes the input sequence to its
memory slots, capturing the most essential information. Then, a decoder will attend to these mem-
ory slots looking for the cues that help to predict the output sequence. MAED has recently demon-
strated promising results in machine translation [5, 40] and healthcare [22, 23, 31]. In this paper,
we advance a recent MAED known as DC-MANN described in [22] where the powerful DNC [15]
is chosen as the external memory.
In DNC, memory accesses and updates are executed via the
controller’s reading and writing heads at each timestep. Given current input xt and a set of K pre-
(cid:3), the controllers compute read-weight
vious read values from memory rt−1 = (cid:2)r1
vector wi,r
t for addressing the memory Mt. There are two versions of
t
decoding in DC-MANN: write-protected and writable memory. We prefer to allow writing to the
memory during inference because in this work, we focus on generating diverse output sequences,
which requires a dynamic memory for both encoding and decoding process.

and write-weight vector ww

t−1, ..., rK
t−1

t−1, r2

2.2 Conditional Variational Autoencoder (CVAE) for Conversation Generation

A dyadic conversation can be represented via three random variables: the conversation context x
(all the chat before the response utterance), the response utterance y and a latent variable z, which
is used to capture the latent distribution over the reasonable responses. A variational autoencoder
conditioned on x (CVAE) is trained to maximize the conditional log likelihood of y given x, which
involves an intractable marginalization over the latent variable z, i.e.,:

p (y | x) =

p (y, z | x) dz =

p (y | x, z) p (z | x) dz

(1)

(cid:90)

z

(cid:90)

z

Fortunately, CVAE can be efﬁciently trained with the Stochastic Gradient Variational Bayes (SGVB)
framework [21] by maximizing the variational lower bound of the conditional log likelihood. In
a typical CVAE work, z is assumed to follow multivariate Gaussian distribution with a diagonal
covariance matrix, which is conditioned on x as pφ (z | x) and a recognition network qθ(z | x, y) to
approximate the true posterior distribution p(z | x, y). The variational lower bound becomes:

2

Figure 1: Graphical Models of the vanilla CVAE (a) and our proposed VMED (b)

L (φ, θ; y, x) = − KL (qθ (z | x, y) (cid:107) pφ (z | x)) + Eqθ(z|x,y) [log p (y | x, z)] ≤ log p (y | x) (2)

With the introduction of the neural approximator qθ(z | x, y) and the reparameterization trick [20],
we can apply the standard back-propagation to compute the gradient of the variational lower bound.
Fig. 1(a) depicts elements of the graphical model for this approach in the case of using CVAE.

3 Methods

Built upon CVAE and partly inspired by VRNN [8], we introduce a novel memory-augmented vari-
ational recurrent network dubbed Variational Memory Encoder-Decoder (VMED). With an external
memory module, VMED explicitly models the dependencies between latent random variables across
subsequent timesteps. However, unlike the VRNN which uses hidden values of RNN to model the
latent distribution as a Gaussian, our VMED uses read values r from an external memory M as
a Mixture of Gaussians (MoG) to model the latent space. This choice of MoG also leads to new
formulation for the prior pφ and the posterior qθ mentioned in Eq. (2). The graphical representation
of our model is shown in Fig. 1(b).

3.1 Generative Process

t−1, r2

t−1, ..., rK
t−1

The VMED includes a CVAE at each time step of the decoder. These CVAEs are conditioned on
(cid:3) from the external memory.
the context sequence via K read values rt−1 = (cid:2)r1
Since the read values are conditioned on the previous state of the decoder hd
t−1, our model takes into
account the temporal structure of the output. Unlike other designs of CVAE where there is often
only one CVAE with a Gaussian prior for the whole decoding process, our model keeps reading
the external memory to produce the prior as a Mixture of Gaussians at every timestep. At the t-th
step of generating an utterance in the output sequence, the decoder will read from the memory K
read values, representing K modes of the MoG. This multi-modal prior reﬂects the fact that given a
context x, there are different modes of uttering the output word yt, which a single mode cannot fully
capture. The MoG prior distribution is modeled as:

gt = pφ (zt | x, rt−1) =

πi,x
t

(cid:0)x, ri

t−1

(cid:1) N

(cid:16)
zt; µi,x
t

(cid:0)x, ri

t−1

(cid:1) , σi,x

t

(cid:0)x, ri

t−1

(cid:1)2

(cid:17)
I

(3)

K
(cid:88)

i=1

t

t

and standard deviation (s.d.) σi,x

We treat the mean µi,x
of each Gaussian distribution in the prior
as neural functions of the context sequence x and read vectors from the memory. The context is
encoded into the memory by an LST M E encoder. In decoding, the decoder LST M D attends to
the memory and choose K read vectors. We split each read vector into two parts ri,µ and ri,σ , each
(cid:17)
of which is used to compute the mean and s.d., respectively: µi,x
.
Here we use the softplus function for computing s.d. to ensure the positiveness. The mode weight
πi,x
t−1 over memory slots. Since we use soft-
t
attention, a read value is computed from all slots yet the main contribution comes from the one

is chosen based on the read attention weights wi,r

t = sof tplus

t = ri,µ

t−1, σi,x

ri,σ
t−1

(cid:16)

3

(cid:3), hd

0, y∗
0

0, r2

Algorithm 1 VMED Generation
Require: Given pφ, (cid:2)r1
0, ..., rK
0
1: for t = 1, T do
2:
3:
4:
5:
6:

t , hd

t = LST M D (cid:0)[y∗

Sampling zt ∼ pφ (zt | x, rt−1) in Eq. (3)
Compute: od
Compute the conditional distribution: p (yt | x, z≤t) = sof tmax (cid:0)Woutod
Update memory and read [r1
Generate output y∗

t , ..., rK
t ] using hd
p (yt = y | x, z≤t)

t−1, zt] , hd

t as in DNC

t−1

(cid:1)

t

(cid:1)

t , r2
t = argmax
y∈V ocab

7: end for

with highest attention score. Thus, we pick the maximum attention score in each read weight and
t = max wi,r
normalize to become the mode weights: πi,x

max wi,r

t−1/

t−1.

i=K
(cid:80)
i=1

Armed with the prior, we follow a recurrent generative process by alternatively using the memory
to compute the MoG and using latent variable z sampled from the MoG to update the memory and
produce the output conditional distribution. The pseudo-algorithm of the generative process is given
in Algorithm 1.

3.2 Neural Posterior Approximation

At each step of the decoder, the true posterior p (zt | x, y) will be approximated by a neural function
of x, y and rt−1, denoted as qθ (zt | x, y, rt−1) . Here, we use a Gaussian distribution to approximate
the posterior. The unimodal posterior is chosen because given a response y, it is reasonable to
assume only one mode of latent space is responsible for this response. Also, choosing a unimodel
will allow the reparameterization trick during training and reduce the complexity of KL divergence
computation. The approximated posterior is computed by the following the equation:

ft = qθ (zt | x, y≤t, rt−1) = N

zt; µx,y
t

(x, y≤t, rt−1) , σx,y

(x, y≤t, rt−1)2 I

t

(4)

(cid:16)

(cid:17)

and s.d. σx,y

with mean µx,y
. We use an LST M U utterance encoder to model the ground truth
utterance sequence up to timestep t-th y≤t. The t-th hidden value of the LST M U is used to repre-
t = LST M U (cid:0)yt, hu
(cid:1). The neural posterior combines the
sent the given data in the posterior: hu

t−1

t

t

t−1 together with the ground truth data to produce the Gaussian posterior:

πi,x
t ri
t ], σx,y
t = sof tplus (Wσ [rt, hu

K
(cid:80)
read values rt =
i=1
µx,y
t = Wµ [rt, hu
t ]). In these equations, we use learnable matrix
weights Wµ and Wσ as a recognition network to compute the mean and s.d. of the posterior, ensur-
ing that the distribution has the same dimension as the prior. We apply the reparamterization trick to
calculate the random variable sampled from the posterior as z(cid:48)
t (cid:12)(cid:15), (cid:15) ∈ N (0, I). Intu-
itively, the reparameterization trick bridges the gap between the generation model and the inference
model during the training.

t +σx,y

t = µx,y

3.3 Learning

In the training phase, the neural posterior is used to produce the latent variable z(cid:48)
t. The read values
from memory are used directly as the MoG priors and the priors are trained to approximate the
posterior by reducing the KL divergence. During testing, the decoder uses the prior for generating
latent variable zt, from which the output is computed. The training and testing diagram is illustrated
in Fig. 2. The objective function becomes a timestep-wise variational lower bound by following
similar derivation presented in [8]:

L (θ, φ; y, x) = Eq∗

−KL (qθ (zt | x, y≤t, rt−1) (cid:107) pφ (zt | x, rt−1)) + log p (yt | x, z≤t)

(cid:35)

(5)

(cid:34) T

(cid:88)

t=1

4

Figure 2: Training and testing of VMED

where q∗ = qθ (z≤T | x, y≤T , r<T ). To maximize the objective function, we have to compute KL
divergence between ft = qθ (zt | x, y≤t, rt−1) and gt = pφ (zt | x, rt−1). Since there is no closed-
form for this KL (ft (cid:107) gt) between Gaussian ft and Mixture of Gaussians gt, we use a closed-
form approximation named Dvar [16] to replace the KL term in the objective function. For our
K
(cid:1) is the KL
(cid:80)
i=1
divergence between two Gaussians and πi is the mode weight of gt. The ﬁnal objective function is:

case: KL (ft (cid:107) gt) ≈ Dvar (ft (cid:107) gt) = − log

t). Here, KL (cid:0)ft (cid:107) gi

πie−KL(ft(cid:107)gi

t

T
(cid:88)

K
(cid:88)

(cid:104)

L =

log

πi,x
t

exp

−KL

N (cid:0)µx,y

t

, σx,y
t

2I(cid:1) (cid:107) N

µi,x
t

, σi,x
t

2I

(cid:16)

(cid:17)(cid:17)(cid:17)(cid:105)

(cid:16)

t=1

i=1

+

1
L

T
(cid:88)

L
(cid:88)

t=1

l=1

(cid:16)

log p

yt | x, z(l)
≤t

(cid:16)

(cid:17)

(6)

3.4 Theoretical Analysis

We now show that by modeling the prior as MoG and the posterior as Gaussian, minimizing the
approximation results in KL divergence minimization. Let deﬁne the log-likelihood Lf (g) =
Ef (x) [log g (x)], we have (see Supplementary material for full derivation):

Lf (g) ≥ log

πie−KL(f (cid:107)gi) + Lf (f ) = −Dvar + Lf (f )

K
(cid:88)

i=1

⇒ Dvar ≥Lf (f ) − Lf (g) = KL (f (cid:107) g)

Thus, minimizing Dvar results in KL divergence minimization. Next, we establish an upper bound
on the total timestep-wise KL divergence in Eq. (5) and show that minimizing this upper bound is
equivalent to ﬁtting a continuous function by a scaled MoG. The total timestep-wise KL divergence
reads:

T
(cid:88)

t=1

+∞
(cid:90)

T
(cid:88)

t=1

−∞

+∞
(cid:90)

T
(cid:88)

t=1

−∞

KL (ft (cid:107) gt) =

ft (x) log [ft (x)] dx −

ft (x) log [gt (x)] dx

5

Table 1: BLEU-1, 4 and A-Glove on testing datasets. B1, B4, AG are acronyms for BLEU-1,
BLEU-4, A-Glove metrics, respectively (higher is better).

Model

Seq2Seq
Seq2Seq-att
DNC
CVAE
VLSTM

VMED (K=1)
VMED (K=2)
VMED (K=3)
VMED (K=4)

Cornell Movies
B4
9.5
9.2
9.0
8.5
9.7

AG
0.52
0.54
0.51
0.56
0.59

B1
18.4
17.7
17.6
16.5
18.6

20.7
22.3
19.4
23.1

10.8
11.9
10.4
12.3

0.57
0.64
0.63
0.61

OpenSubtitle
B4
5.4
6.5
7.2
6.6
8.1

AG
0.29
0.42
0.47
0.45
0.43

B1
11.4
13.2
14.3
13.5
16.4

12.9
15.3
24.8
17.9

6.2
8.8
12.9
9.3

0.44
0.49
0.54
0.52

LJ users
B4
6.4
5.6
6.1
6.0
5.6

6.9
7.9
9.8
7.5

AG
0.45
0.49
0.47
0.48
0.46

0.47
0.51
0.49
0.47

B1
13.1
11.4
12.4
12.2
11.5

13.7
15.4
18.1
14.4

Reddit comments
AG
B4
B1
0.31
3.3
7.5
0.25
2.4
5.5
0.28
3.4
7.5
0.39
2.8
5.3
0.27
3.1
6.9

9.1
9.2
12.3
8.6

4.3
4.4
6.4
4.6

0.39
0.38
0.46
0.41

K
(cid:80)
i=1

tgi
πi

t and gi

where gt =

t is the i-th Gaussian in the MoG at timestep t-th. If at each decoding
step, minimizing Dvar results in adequate KL divergence such that the prior is optimized close to
the neural posterior, according to Chebyshev’s sum inequality, we can derive an upper bound on the
total timestep-wise KL divergence as (see Supplementary Materials for full derivation):

+∞
(cid:90)

T
(cid:88)

t=1

−∞

ft (x) log [ft (x)] dx −

ft (x) log

gt (x)

dx

(7)

+∞
(cid:90)

−∞

1
T

T
(cid:88)

t=1

(cid:35)

(cid:34) T
(cid:89)

t=1

The left term is sum of the entropies of ft (x), which does not depend on the training parameter
φ used to compute gt, so we can ignore that. Thus given f , minimizing the upper bound of the
total timestep-wise KL divergence is equivalent to maximizing the right term of Eq. (7). Since

gt is an MoG and products of MoG is proportional to an MoG,

gt (x) is a scaled MoG (see

Supplementary material for full proof). Maximizing the right term is equivalent to ﬁtting function
T
(cid:80)
t=1
possible regardless of the form of ft since MoG is a universal approximator [1, 27].

ft (x), which is sum of Gaussians and thus continuous, by a scaled MoG. This, in theory, is

T
(cid:81)
t=1

4 Results

Datasets and pre-processing: We perform experiments on two collections: The ﬁrst collection in-
cludes open-domain movie transcript datasets containing casual conversations: Cornell Movies1 and
OpenSubtitle2. They have been used commonly in evaluating conversational agents [26, 38]. The
second are closed-domain datasets crawled from speciﬁc domains, which are question-answering
of LiveJournal (LJ) users and Reddit comments on movie topics. For each dataset, we use 10,000
conversations for validating and 10,000 for testing.

Baselines, implementations and metrics: We compare our model with three deterministic base-
lines: the encoder-decoder neural conversational model (Seq2Seq) similar to [38] and its two vari-
ants equipped with attention mechanism [2] (Seq2Seq-att) and a DNC external memory [15] (DNC).
The vanilla CVAE is also included in the baselines. To build this CVAE, we follow similar archi-
tecture introduced in [43] without bag-of-word loss and dialog act features3. A variational recurrent
model without memory is also included in the baselines. The model termed VLSTM is implemented

1http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html
2http://opus.nlpl.eu/OpenSubtitles.php
3Another variant of non-memory CVAE with MoG prior is also examined. We produce a set of MoG
parameters by a feed forward network with the input as the last encoder hidden states. However, the model is
hard to train and fails to converge with these datasets.

6

Table 2: Examples of context-response pairs. /*/ denotes separations between stochastic responses.

Input context
Reddit comment:
What is your favorite
scene in ﬁlm history ?
Mine is the restaurant
scene in the
Godfather.

Reddit comment:
What actor will win
an Oscar in the next
10 years ?

LJ user: Your very
pretty and your is
very cute. May i add
ya to my friends list ?

Cornell movies:
Nobody can murder
someone with their
feelings . I mean that
is crazy ?

OpenSubtitle: I’m
obliged for your
hospitality. I
appreciate it and your
husband too.

Response
Seq2Seq: The scene in
Seq2Seq-att: The ﬁnal
DNC: The scene in
CVAE: Inception god! Not by a shark /*/ Amour great /*/ Pro thing you know 3
dead
VLSTM: The scene in /*/ The of a dead /*/ The sky in scene
VMED (K=3): The opening scene from history movie /*/ The scene in a shot
nights! Robin movie /*/ The psycho scene in ﬁght from
Seq2Seq: Colin
Seq2Seq-att: Liam Neeson
DNC: Tom Gyllenhaal
CVAE: Daryl and Aaron /*/ Carefully count Alfred Deniro /*/ Ponyo Joker
possible
VLSTM: Michael Bullock /*/ Michael /*/ Michael De
VMED (K=3): Edward or Leo Dicaprio goes on /*/ Dicaprio will /*/ Dicaprio
Tom has actually in jack on road
Seq2Seq: I! Add and too you back
Seq2Seq-att: I did may! Away back
DNC: Sure you added
CVAE: Sure ! I can add you /*/ Thank cool of surely /*/ Thank hi of sure!
VLSTM: Haha of do go /*/ Haha of sure! Yes lol /*/ Haha sure! Go
VMED (K=3): Thank cool for sure /*/ Add deﬁnitely! Sure surely friends /*/
Thank girl go ahead
Seq2Seq: Yes you are
Seq2Seq-att: Really not is it
DNC: Managing the truth
CVAE: Sure not to be in here /*/ Oh yes but i know! /*/ That to doing with here
and
VLSTM: I am dead . ! That is ... /*/ I did not what way . /*/ I am not . But his
things with ...
VMED (K=4): You are right! /*/ That is crazy /*/ You can’t know Jimmy
Seq2Seq: That is have got coming about these
Seq2Seq-att: May you not what nothing nobody
DNC: Yes i am taking this
CVAE: No . that for good! And okay /*/ All in the of two thing /*/ Sure. Is this!
VLSTM: I ... /*/ I understand /*/ I ! . ...
VMED (K=3): I know. I can afford /*/ I know nothing to store for you pass /*/ I
know. Doing anymore you father

based on LSTM instead of RNN as in VRNN framework [8]. We try our model VMED4 with dif-
ferent number of modes (K = 1, 2, 3, 4). It should be noted that, when K = 1, our model’s prior
is exactly a Gaussian and the KL term in Eq. (6) is no more an approximation. Details of dataset
descriptions and model implementations are included in Supplementary material.

We report results using two performance metrics in order to evaluate the system from various linguis-
tic points of view: (i) Smoothed Sentence-level BLEU [6]: BLEU is a popular metric that measures
the geometric mean of modiﬁed ngram precision with a length penalty. We use BLEU-1 to 4 as our
lexical similarity. (ii) Cosine Similarly of Sentence Embedding: a simple method to obtain sentence
embedding is to take the average of all the word embeddings in the sentences [11]. We follow [43]
and choose Glove [24] as the word embedding in measuring sentence similarly (A-Glove). To mea-
sure stochastic models, for each input, we generate output ten times. The metric between the ground
truth and the generated output is calculated and taken average over ten responses.

4Source code is available at https://github.com/thaihungle/VMED

7

Metric-based Analysis: We report results on four test datasets in Table 1. For BLEU scores, here
we only list results for BLEU-1 and 4. Other BLEUs show similar pattern and will be listed in
Supplementary material. As clearly seen, VMED models outperform other baselines over all metrics
across four datasets. In general, the performance of Seq2Seq is comparable with other deterministic
methods despite its simplicity. Surprisingly, CVAE or VLSTM does not show much advantage over
deterministic models. As we shall see, although CVAE and VLSTM responses are diverse, they are
often out of context. Among different modes of VMED, there is often one best ﬁt with the data and
thus shows superior performance. The optimal number of modes in our experiments often falls to
K = 3, indicating that increasing modes does not mean to improve accuracy.

It should be noted that there is inconsistency between BLEU scores and A-Glove metrics. This
is because BLEU measures lexicon matching while A-Glove evaluates semantic similarly in the
embedding space. For examples, two sentences having different words may share the same meaning
and lie close in the embedding space. In either case, compared to others, our optimal VMED always
achieves better performance.

Qualitative Analysis

Table 2 represents responses generated by experimental models in reply to different input sentences.
The replies listed are chosen randomly from 50 generated responses whose average of metric scores
over all models are highest. For stochastic models, we generate three times for each input, result-
ing in three different responses. In general, the stochastic models often yield longer and diverse
sequences as expected. For closed-domain cases, all models responses are fairly acceptable. Com-
pared to the rest, our VMED’s responds seem to relate more to the context and contain meaningful
information. In this experiment, the open-domain input seems nosier and harder than the closed-
domain ones, thus create a big challenge for all models. Despite that, the quality of VMED’s re-
sponses is superior to others. Among deterministic models, DNC’s generated responses look more
reasonable than Seq2Seq’s even though its BLEU scores are not always higher. Perhaps, the refer-
ence to external memory at every timestep enhances the coherence between output and input, making
the response more related to the context. VMED may inherit this feature from its external memory
and thus tends to produce reasonable responses. By contrast, although responses from CVAE and
VLSTM are not trivial, they have more grammatical errors and sometimes unrelated to the topic.

5 Related Work

With the recent revival of recurrent neural networks (RNNs), there has been much effort spent on
learning generative models of sequences. Early attempts include training RNN to generate the next
output given previous sequence, demonstrating RNNs’ ability to generate text and handwriting im-
ages [14]. Later, encoder-decoder architecture [37] enables generating a whole sequence in machine
translation [18], text summation [30] and conversation generation [38]. Although these models have
achieved signiﬁcant empirical successes, they fall short to capture the complexity and variability of
sequential processes.

These limitations have recently triggered a considerable effort on introducing variability into the
encoder-decoder architecture. Most of the methods focus on conditional VAE (CVAE) by con-
structing a variational lower bound conditioned on the context. The setting can be found in many
applications including machine translation [42] and dialog generation [4, 33, 34, 43]. A common
trick is to place a neural net between the encoder and the decoder to compute the Gaussian prior
and posterior of the CVAE. This design is further enhanced by the use of external memory [7] and
reinforcement learning [41]. In contrast to this design, our VMED uses recurrent latent variable
approach [8], that is, our model requires a CVAE for each step of generation. Besides, our external
memory is used for producing the latent distribution, which is different from the one proposed in [7]
where the memory is used only for holding long-term dependencies at sentence level. Compared to
variational addressing scheme mentioned in [3], our memory uses deterministic addressing scheme,
yet the memory content itself is used to introduce randomness to the architecture. More relevant to
our work is GTMM [12] where memory read-outs involve in constructing the prior and posterior at
every timesteps. However, this approach uses Gaussian prior without conditional context.

Using mixture of models instead of single Gaussian in VAE framework is not a new concept. Works
in [9, 17] and [29] proposed replacing the Gaussian prior and posterior in VAE by MoGs for cluster-
ing and generating image problems. Works in [35] and [39] applied MoG prior to model transitions

8

between video frames and caption generation, respectively. These methods use simple feed forward
network to produce Gaussian sub-distributions independently. In our model, on the contrary, mem-
ory slots are strongly correlated with each others, and thus modes in our MoG work together to
deﬁne the shape of the latent distributions at speciﬁc timestep. To the best of our knowledge, our
work is the ﬁrst attempt to use an external memory to induce mixture models for sequence generation
problems.

6 Conclusions

We propose a novel approach to sequence generation called Variational Memory Encoder-Decoder
(VMED) that introduces variability into encoder-decoder architecture via the use of external mem-
ory as mixture model. By modeling the latent temporal dependencies across timesteps, our VMED
produces a MoG representing the latent distribution. Each mode of the MoG associates with some
memory slot and thus captures some aspect of context supporting generation process. To accom-
modate the MoG, we employ a KL approximation and we demonstrate that minimizing this ap-
proximation is equivalent to minimizing the KL divergence. We derive an upper bound on our total
timestep-wise KL divergence and indicate that the optimization of this upper bound is equivalent
to ﬁtting a continuous function by an scaled MoG, which is in theory possible regardless of the
function form. This forms a theoretical basis for our model formulation using MoG prior for every
step of generation. We apply our proposed model to conversation generation problem. The results
demonstrate that VMED outperforms recent advances both quantitatively and qualitatively. Future
explorations may involve implementing a dynamic number of modes that enable learning of the op-
timal K for each timestep. Another aspect would be multi-person dialog setting, where our memory
as mixture model may be useful to capture more complex modes of speaking in the dialog.

References

[1] Athanassia Bacharoglou. Approximation of probability distributions by convex mixtures of
gaussian measures. Proceedings of the American Mathematical Society, 138(7):2619–2628,
2010.

[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by
jointly learning to align and translate. Proceedings of the International Conference on Learn-
ing Representations, 2015.

[3] Jörg Bornschein, Andriy Mnih, Daniel Zoran, and Danilo Jimenez Rezende. Variational mem-
ory addressing in generative models. In Advances in Neural Information Processing Systems,
pages 3923–3932, 2017.

[4] Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew Dai, Rafal Jozefowicz, and Samy
Bengio. Generating sentences from a continuous space. In Proceedings of The SIGNLL Con-
ference on Computational Natural Language Learning, pages 10–21, 2016.

[5] Denny Britz, Melody Guan, and Minh-Thang Luong. Efﬁcient attention using a ﬁxed-size
memory representation. In Proceedings of the Conference on Empirical Methods in Natural
Language Processing, pages 392–400, 2017.

[6] Boxing Chen and Colin Cherry. A systematic comparison of smoothing techniques for
sentence-level bleu. In Proceedings of the Ninth Workshop on Statistical Machine Transla-
tion, pages 362–367, 2014.

[7] Hongshen Chen, Zhaochun Ren, Jiliang Tang, Yihong Eric Zhao, and Dawei Yin. Hierarchical
variational memory network for dialogue generation. In Proceedings of the World Wide Web
Conference on World Wide Web, pages 1653–1662. International World Wide Web Conferences
Steering Committee, 2018.

[8] Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua
Bengio. A recurrent latent variable model for sequential data. In Advances in Neural Informa-
tion Processing Systems, pages 2980–2988, 2015.

9

[9] Nat Dilokthanakul, Pedro AM Mediano, Marta Garnelo, Matthew CH Lee, Hugh Salimbeni,
Kai Arulkumaran, and Murray Shanahan. Deep unsupervised clustering with gaussian mixture
variational autoencoders. arXiv preprint arXiv:1611.02648, 2016.

[10] J-L Durrieu, J-Ph Thiran, and Finnian Kelly. Lower and upper bounds for approximation
of the kullback-leibler divergence between gaussian mixture models. In IEEE International
Conference on Acoustics, Speech and Signal Processing., 2012.

[11] Gabriel Forgues, Joelle Pineau, Jean-Marie Larchevêque, and Réal Tremblay. Bootstrapping
dialog systems with word embeddings. In Nips, Modern Machine Learning and Natural Lan-
guage Processing Workshop, volume 2, 2014.

[12] Mevlana Gemici, Chia-Chun Hung, Adam Santoro, Greg Wayne, Shakir Mohamed, Danilo J
Rezende, David Amos, and Timothy Lillicrap. Generative temporal models with memory.
arXiv preprint arXiv:1702.04649, 2017.

[13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
In Advances in

Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets.
Neural Information Processing Systems, pages 2672–2680, 2014.

[14] Alex Graves. Generating sequences with recurrent neural networks.

arXiv preprint

arXiv:1308.0850, 2013.

[15] Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka
Grabska-Barwi´nska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John
Agapiou, et al. Hybrid computing using a neural network with dynamic external memory.
Nature, 538(7626):471–476, 2016.

[16] John R Hershey and Peder A Olsen. Approximating the kullback leibler divergence between
gaussian mixture models. In IEEE International Conference on Acoustics, Speech and Signal
Processing., 2007.

[17] Zhuxi Jiang, Yin Zheng, Huachun Tan, Bangsheng Tang, and Hanning Zhou. Variational deep
In Proceedings of the
embedding: An unsupervised and generative approach to clustering.
International Joint Conference on Artiﬁcial Intelligence, pages 1965–1972. International Joint
Conference on Artiﬁcial Intelligence, 2017.

[18] Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models. In Proceedings
of the Conference on Empirical Methods in Natural Language Processing, pages 1700–1709,
2013.

[19] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv

preprint arXiv:1412.6980, 2014.

[20] Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-
supervised learning with deep generative models. In Advances in Neural Information Process-
ing Systems, pages 3581–3589, 2014.

[21] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In Proceedings of the

International Conference on Learning Representations, 2014.

[22] Hung Le, Truyen Tran, and Svetha Venkatesh. Dual control memory augmented neural net-
works for treatment recommendations. In Advances in Knowledge Discovery and Data Mining,
pages 273–284, Cham, 2018. Springer International Publishing.

[23] Hung Le, Truyen Tran, and Svetha Venkatesh. Dual memory neural computer for asynchronous
two-view sequential learning. In Proceedings of the 24th ACM SIGKDD International Con-
ference on Knowledge Discovery; Data Mining, KDD ’18, pages 1637–1645, New York, NY,
USA, 2018. ACM.

[24] Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In

Advances in Neural Information Processing Systems, pages 2177–2185, 2014.

10

[25] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting
objective function for neural conversation models. In Proceedings of the Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, pages 110–119, 2016.

[26] Pierre Lison and Serge Bibauw. Not all dialogues are created equal: Instance weighting for
neural conversational models. In Proceedings of the Annual SIGdial Meeting on Discourse
and Dialogue, pages 384–394, 2017.

[27] Vladimir Maz’ya and Gunther Schmidt. On approximate approximations using gaussian ker-

nels. IMA Journal of Numerical Analysis, 16(1):13–29, 1996.

[28] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed repre-
sentations of words and phrases and their compositionality. In Advances in Neural Information
Processing Systems, pages 3111–3119, 2013.

[29] Eric Nalisnick, Lars Hertel, and Padhraic Smyth. Approximate inference for deep latent gaus-

sian mixtures. In NIPS Workshop on Bayesian Deep Learning, volume 2, 2016.

[30] Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar Gulcehre, and Bing Xiang. Ab-
stractive text summarization using sequence-to-sequence rnns and beyond. In Proceedings of
the SIGNLL Conference on Computational Natural Language Learning, pages 280–290, 2016.

[31] Aaditya Prakash, Siyuan Zhao, Sadid A Hasan, Vivek V Datla, Kathy Lee, Ashequl Qadir, Joey
Liu, and Oladimeji Farri. Condensed memory networks for clinical diagnostic inferencing. In
Proceedings of the AAAI Conference on Artiﬁcial Intelligence, pages 3274–3280, 2017.

[32] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation
In Proceedings of the International
and approximate inference in deep generative models.
Conference on International Conference on Machine Learning, pages II–1278. JMLR. org,
2014.

[33] Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron C
Courville, and Yoshua Bengio. A hierarchical latent variable encoder-decoder model for gen-
In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, pages
erating dialogues.
3295–3301, 2017.

[34] Xiaoyu Shen, Hui Su, Yanran Li, Wenjie Li, Shuzi Niu, Yang Zhao, Akiko Aizawa, and Guop-
ing Long. A conditional variational framework for dialog generation. In Proceedings of the
Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),
volume 2, pages 504–509, 2017.

[35] Rui Shu, James Brofos, Frank Zhang, Hung Hai Bui, Mohammad Ghavamzadeh, and Mykel
Kochenderfer. Stochastic video prediction with conditional density estimation. In ECCV Work-
shop on Action and Anticipation for Visual Learning, volume 2, 2016.

[36] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory net-
In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,

works.
Advances in Neural Information Processing Systems, pages 2440–2448. 2015.

[37] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural

networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.

[38] Oriol Vinyals and Quoc Le. A neural conversational model. arXiv preprint arXiv:1506.05869,

2015.

[39] Liwei Wang, Alexander Schwing, and Svetlana Lazebnik. Diverse and accurate image descrip-
tion using a variational auto-encoder with an additive gaussian encoding space. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,
Advances in Neural Information Processing Systems, pages 5756–5766. 2017.

[40] Mingxuan Wang, Zhengdong Lu, Hang Li, and Qun Liu. Memory-enhanced decoder for neu-
ral machine translation. In Proceedings of the Conference on Empirical Methods in Natural
Language Processing, pages 278–286, 2016.

11

[41] Tsung-Hsien Wen, Yishu Miao, Phil Blunsom, and Steve Young. Latent intention dialogue
models. In Proceedings of the International Conference on Machine Learning, pages 3732–
3741, 2017.

[42] Biao Zhang, Deyi Xiong, Hong Duan, Min Zhang, et al. Variational neural machine translation.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing,
pages 521–530, 2016.

[43] Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi. Learning discourse-level diversity for neu-
ral dialog models using conditional variational autoencoders. In Proceedings of the Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1,
pages 654–664, 2017.

12

Supplementary material

A Derivation of the Upper Bound on the KL divergence

Theorem 1. The KL divergence between a Gaussian and a Mixture of Gaussians has an upper
bound Dvar.

Proof. Dvar(f (cid:107) g) [16] is an approximation of KL divergence between two Mixture of Gaussians
(MoG), which is deﬁned as the following:

Dvar (f (cid:107) g) =

(cid:88)

πf
j log

j

j(cid:48)e−KL(fj (cid:107)fj(cid:48))
πf
πg
i e−KL(fj (cid:107)gi)

(cid:80)
j(cid:48)
(cid:80)
i

(8)

In our case, f is a Gaussian, a special case of MoG where the number of mode equals one. Then,
Eq. (8) becomes:

Dvar (f (cid:107) g) = log

1

K
(cid:80)
i=1

πg
i e−KL(f (cid:107)gi)

= − log

πie−KL(f (cid:107)gi)

K
(cid:88)

i=1

Let deﬁne the log-likelihood Lf (g) = Ef (x) [log g (x)], the lower bound for Lf (g) can be also be
derived, using variational parameters as follows:

where βi ≥ 0 and

βi = 1. According to [10], maximizing the RHS of the above inequality with

K
(cid:80)
i=1

respect to βi provides a lower bound for Lf (g):

Lf (g) =Ef

log

πigi (x)

(cid:34)

(cid:32) K
(cid:88)

i=1

(cid:33)(cid:35)

=

f (x) log

(cid:32) K
(cid:88)

i=1

βiπi gi (x)
βi

(cid:33)

dx

≥

βi

f (x) log

(cid:19)

(cid:18)
πi gi (x)
βi

dx

+∞
(cid:90)

−∞

K
(cid:88)

i=1

+∞
(cid:90)

−∞

K
(cid:88)

i=1

Lf (g) ≥ log

πie−KL(f (cid:107)gi) + Lf (f )

= − Dvar + Lf (f )

⇒ Dvar ≥Lf (f ) − Lf (g)

=KL (f (cid:107) g)

Therefore, the KL divergence has an upper bound: Dvar.

B Derivation of the Upper Bound on the Total Timestep-wise KL Divergence

Lemma 2. Chebyshev’s sum inequality:
if

and

a1 ≥ a2 ≥ ... ≥ an

13

Proof. Consider the sum:

then

whence:

b1 ≥ b2 ≥ ... ≥ bn

1
n

n
(cid:88)

k=1

akbk ≥

(cid:33) (cid:32)

(cid:32)

1
n

n
(cid:88)

k=1

ak

(cid:33)

bk

1
n

n
(cid:88)

k=1

S =

(aj − ak) (bj − bk)

n
(cid:88)

n
(cid:88)

j=1

k=1

0 ≤ 2n

ajbj − 2

aj

bk

n
(cid:88)

j=1

n
(cid:88)

n
(cid:88)

j=1

k=1

1
n

n
(cid:88)

j=1

ajbj ≥





1
n

n
(cid:88)

j=1



(cid:32)

aj



(cid:33)

1
n

n
(cid:88)

k=1

bk

The two sequences are non-increasing, therefore aj − ak and bj − bk have the same sign for any
j, k. Hence S ≥ 0. Opening the brackets, we deduce:

In our problem, ai = fi (x) and bi = log [gi (x)], i = 1, T . Under the assumption that at each step,
thanks to minimizing Dvar, the approximation between the MoG and the Gaussian is adequate to
preserve the order of these values, that is, if fi (x) ≤ fj (x), then gi (x) ≤ gj (x) and log [gi (x)] ≤
log [gj (x)]. Without loss of generality, we hypothesize that f1 (x) ≤ f2 (x) ≤ ... ≤ fT (x), then
we have log [g1 (x)] ≤ log [g2 (x)] ≤ ... ≤ log [gT (x)]. Thus, applying Lemma 2, we have:

ft (x) log [gt (x)] dx ≥

ft (x)

log [gt (x)] dx

ft (x) log [gt (x)] dx ≥

ft (x)

log [gt (x)] dx

1
T

T
(cid:88)

t=1

T
(cid:88)

t=1

ft (x) log [gt (x)] dx ≥

ft (x) log

gt (x)

dx

Thus, the upper bound on the total timestep-wise KL divergence reads:

ft (x) log [ft (x)] dx −

ft (x) log

gt (x)

dx

(cid:35)

(cid:35)

(cid:34) T
(cid:89)

t=1

(cid:34) T
(cid:89)

t=1

1
T

+∞
(cid:90)

−∞

+∞
(cid:90)

T
(cid:88)

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

−∞

⇒

⇒

+∞
(cid:90)

T
(cid:88)

t=1

−∞

C Proof

gt (x) =

tgi
πi

t (x) is a Scaled MoG

T
(cid:81)
t=1

T
(cid:81)
t=1

K
(cid:80)
i=1

Lemma 3. Product of two Gaussians is a scaled Gaussian.

1
T

T
(cid:88)

t=1

+∞
(cid:90)

−∞

+∞
(cid:90)

−∞

1
T

1
T

T
(cid:88)

t=1

T
(cid:88)

t=1

+∞
(cid:90)

−∞

1
T

T
(cid:88)

t=1

14

Proof. Let Nx (µ, Σ) denote a density of x, then

where:

Nx (µ1, Σ1) · Nx (µ2, Σ2) = ccNx (µc, Σc)

exp

−

(m1 − m2)T (Σ1 + Σ2)−1 (m1 − m2)

(cid:19)

(cid:18)

1
2

cc =

1
(cid:112)det (2π (Σ1 + Σ2))
(cid:1)−1 (cid:0)Σ−1
(cid:1)

1 + Σ−1
2
1 + Σ−1
2

mc = (cid:0)Σ−1
Σc = (cid:0)Σ−1

1 m1 + Σ−1

2 m2

(cid:1)

K2(cid:80)
j=1

K2(cid:88)

j=1

Lemma 4. Product of two MoGs is proportional to an MoG.

Proof. Let g1 (x) =

K1(cid:80)
i=1
of Gaussians. We have:

π1,iNx (µ1,i, Σ1,i) and g2 (x) =

π2,jNx (µ2,j, Σ2,j) are two Mixtures

K1(cid:88)

i=1

K1(cid:88)

K2(cid:88)

=

i=1

,j=1

By applying Lemma 3 to Eq. (9), we have

g1 (x) · g2 (x) =

π1,iNx (µ1,i, Σ1,i) ·

π2,jNx (µ2,j, Σ2,j)

π1,iπ2,jNx (µ1,i, Σ1,i) · Nx (µ2,j, Σ2,j)

(9)

g1 (x) · g2 (x) =

π1,iπ2,jcijNx (µij, Σij)

K1(cid:88)

K2(cid:88)

i=1

,j=1

= C

K1(cid:88)

K2(cid:88)

i=1

,j=1

π1,iπ2,jcij
C

Nx (µij, Σij)

(10)

where C =

π1,iπ2,jcij. Clearly, Eq. (10) is proportional to an MoG with K1 · K2 modes

Theorem 5.

gt (x) =

tgi
πi

t (x) is a scaled MoG.

Proof. By induction from Lemma 4, we can easily show that product of T MoGs is also proportional

to an MoG. That means

gt (x) equals to a scaled MoG.

K1(cid:80)
i=1

K2(cid:80)
,j=1

T
(cid:81)
t=1

T
(cid:81)
t=1

K
(cid:80)
i=1

T
(cid:81)
t=1

D Details of Data Descriptions and Model Implementations

Here we list all datasets used in our experiments:

• Open-domain datasets:

– Cornell movie dialog: This corpus contains a large metadata-rich collection of ﬁc-
tional conversations extracted from 617 raw movies with 220,579 conversational ex-
changes between 10,292 pairs of movie characters. For each dialog, we preprocess the
data by limiting the context length and the utterance output length to 20 and 10, re-
spectively. The vocabulary is kept to top 20,000 frequently-used words in the dataset.

15

– OpenSubtitles: This dataset consists of movie conversations in XML format. It also
contains sentences uttered by characters in movies, yet it is much bigger and noisier
than Cornell dataset. After preprocessing as above, there are more than 1.6 million
pairs of contexts and utterance with chosen vocabulary of 40,000 words.

• Closed-domain datasets::

– Live Journal (LJ) user question-answering dataset: question-answer dialog by LJ users
who are members of anxiety, arthritis, asthma, autism, depression, diabetes, and obe-
sity LJ communities5. After preprocessing as above, we get a dataset of more than
112,000 conversations. We limit the vocabulary size to 20,000 most common words.

– Reddit comments dataset: This dataset consists of posts and comments about movies
in Reddit website6. A single post may have multiple comments constituting a multi-
people dialog amongst the poster and commentors, which makes this dataset the most
challenging one. We crawl over four millions posts from Reddit website and after
preprocessing by retaining conversations whose utterance’s length are less than 20,
we have a dataset of nearly 200 thousand conversations with a vocabulary of more
than 16 thousand words.

We trained with the following hyperparameters (according to the performance on the validate
dataset): word embedding has size 96 and is shared across everywhere. We initialize the word
embedding from Google’s Word2Vec [28] pretrained word vectors. The hidden dimension of LSTM
in all controllers is set to 768 for all datasets except the big OpenSubtitles whose LSTM dimension
is 1024. The number of LSTM layers for every controllers is set to 3. All the initial weights are
sampled from a normal distribution with mean 0, standard deviation 0.1. The mini-batch size is
chosen as 256. The models are trained end-to-end using the Adam optimizer [19] with a learning
rate of 0.001 and gradient clipping at 10. For models using memory, we set the number and the size
of memory slots to 16 and 64, respectively. As indicated in [4], it is not trivial to optimize VAE
with RNN-like decoder due to the vanishing latent variable problem. Hence, to make the variational
models in our experiments converge we have to use the KL annealing trick by adding to the KL
loss term an annealing coefﬁcient α starts with a very small value and gradually increase up to 1.

E Full Reports on Model Performance

Model
Seq2Seq
Seq2Seq-att
DNC
CVAE
VLSTM
VMED (K=1)
VMED (K=2)
VMED (K=3)
VMED (K=4)

BLEU-1 BLEU-2 BLEU-3 BLEU-4

18.4
17.7
17.6
16.5
18.6
20.7
22.3
19.4
23.1
Table 3: Results on Cornell Movies

14.5
14.0
13.9
13.0
14.8
16.5
18.0
15.6
18.5

12.1
11.7
11.5
10.9
12.4
13.8
15.2
13.2
15.5

9.5
9.2
9.0
8.5
9.7
10.8
11.9
10.4
12.3

A-glove
0.52
0.54
0.51
0.56
0.59
0.57
0.64
0.63
0.61

5https://www.livejournal.com/
6https://www.reddit.com/r/movies/

16

Model
Seq2Seq
Seq2Seq-att
DNC
CVAE
VLSTM
VMED (K=1)
VMED (K=2)
VMED (K=3)
VMED (K=4)

Model
Seq2Seq
Seq2Seq-att
DNC
CVAE
VLSTM
VMED (K=1)
VMED (K=2)
VMED (K=3)
VMED (K=4)

Model
Seq2Seq
Seq2Seq-att
DNC
CVAE
VLSTM
VMED (K=1)
VMED (K=2)
VMED (K=3)
VMED (K=4)

BLEU-1 BLEU-2 BLEU-3 BLEU-4

11.4
13.2
14.3
13.5
16.4
12.9
15.3
24.8
17.9
Table 4: Results on OpenSubtitles

7.1
8.4
9.3
8.4
10.4
7.5
10.4
16.4
11.8

8.7
10.2
11.2
10.2
12.7
9.5
13.8
19.7
14.2

5.4
6.5
7.2
6.6
8.1
6.2
8.8
12.9
9.3

BLEU-1 BLEU-2 BLEU-3 BLEU-4

Table 5: Results on LJ users question-answering

BLEU-1 BLEU-2 BLEU-3 BLEU-4

13.1
11.4
12.4
12.2
11.5
13.7
15.4
18.1
14.4

7.5
5.5
7.5
5.3
6.9
9.1
9.2
12.3
8.6

10.1
8.7
9.6
9.4
8.8
10.7
12.2
14.8
11.4

5.5
4.0
5.6
4.3
5.1
6.8
7.0
9.7
6.9

8.3
7.1
7.8
7.7
7.3
8.9
10.1
12.4
9.5

4.4
3.1
4.5
3.6
4.1
5.5
5.7
8.1
5.9

6.4
5.6
6.1
6.0
5.6
6.9
7.9
9.8
7.5

3.3
2.4
3.4
2.8
3.1
4.3
4.4
6.4
4.6

Table 6: Results on Reddit comments

A-glove
0.29
0.42
0.47
0.45
0.43
0.44
0.49
0.54
0.52

A-glove
0.45
0.49
0.47
0.48
0.46
0.47
0.51
0.49
0.47

A-glove
0.31
0.25
0.28
0.39
0.27
0.39
0.38
0.46
0.41

17

