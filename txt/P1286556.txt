Top-N Recommendation on Graphs

Zhao Kang, Chong Peng, Ming Yang, Qiang Cheng
Department of Computer Science, Southern Illinois University, Carbondale, IL, USA
{Zhao.Kang, pchong, ming.yang, qcheng}@siu.edu

6
1
0
2
 
p
e
S
 
7
2
 
 
]

R

I
.
s
c
[
 
 
1
v
4
6
2
8
0
.
9
0
6
1
:
v
i
X
r
a

ABSTRACT
Recommender systems play an increasingly important role
in online applications to help users ﬁnd what they need or
prefer. Collaborative ﬁltering algorithms that generate pre-
dictions by analyzing the user-item rating matrix perform
poorly when the matrix is sparse. To alleviate this problem,
this paper proposes a simple recommendation algorithm that
fully exploits the similarity information among users and
items and intrinsic structural information of the user-item
matrix. The proposed method constructs a new represen-
tation which preserves aﬃnity and structure information in
the user-item rating matrix and then performs recommenda-
tion task. To capture proximity information about users and
items, two graphs are constructed. Manifold learning idea
is used to constrain the new representation to be smooth on
these graphs, so as to enforce users and item proximities.
Our model is formulated as a convex optimization problem,
for which we need to solve the well known Sylvester equa-
tion only. We carry out extensive empirical evaluations on
six benchmark datasets to show the eﬀectiveness of this ap-
proach.

Keywords
top-N recommendation; laplacian graph; collaborative ﬁlter-
ing

1.

INTRODUCTION

Recommender systems have become increasingly indis-
pensable in many applications [1]. Collaborative ﬁltering
(CF) based methods are a fundamental building block in
many recommender systems. CF based recommender sys-
tems predict the ratings of items to be given by a user based
on the ratings of the items previously rated by other users
who are most similar to the target user.

CF based methods can be classiﬁed into memory-based
methods [21] and model-based methods [14, 15]. The for-
mer includes two popular methods, user-oriented [9] and
item-oriented, e.g., ItemKNN [6], depending on whether the

ACM ISBN 978-1-4503-2138-9.
DOI: 10.1145/1235

neighborhood information is derived from similar users or
items. First they compute similarities between the active
item and other items, or between the active user and other
users. Then they predict the unknown rating by combining
the known rating of top k neighbors. Due to the simplicity
of memory-based CF, it has been successfully applied in in-
dustry. However, it suﬀers from several problems, including
data sparsity, cold start and data correlation [3], as users
typically rate only a small portion of the available items,
and they also tend to rate similar items closely. Therefore,
the similarities between users or items cannot be accurately
obtained with the existing similarity measures such as co-
sine and Pearson correlation, which in turn compromises the
recommendation accuracy.

To alleviate the problems of memory-based methods, many
model-based methods have been proposed, which use ob-
served ratings to learn a predictive model. Among them,
matrix factorization (MF) based models, e.g., PureSVD [5]
and weighted regularized MF (WRMF) [10], are very pop-
ular due to their capability of capturing the implicit rela-
tionships among items and their outstanding performance.
Nevertheless, it introduces high computational complexity
and also faces the problem of uninterpretable recommenda-
tions. Because the rating matrix is sparse, the factorization
of the user-item matrix may lead to inferior solutions [7].
By learning an aggregation coeﬃcient matrix [13], recently,
sparse linear method (SLIM) [17] has been proposed and
shown to be eﬀective. However, it just captures relations
between items that have been co-purchased/co-rated by at
least one user [12]. Moreover, it only explores the linear rela-
tions between items. Another class of methods use Bayesian
personalized ranking (BPR) criterion to measure the diﬀer-
ence between the rankings of user-purchased items and the
remaining items. For instance, BPRMF and BPRKNN [19]
have been demonstrated to be eﬀective for implicit feedback
datasets.

In this paper, we propose a novel Top-N recommendation
model based on graphs. This method not only takes into
account the neighborhood information, which is encoded by
our user graph and item graph, but also reveals hidden struc-
ture in the data by deploying graph regularization. In the
real world, data often reside on low-dimensional manifolds
embedded in a high-dimensional ambient space. Like the
Netﬂix Prize problem, where the size of the user-item matrix
can be huge, there exist relationships between users (such
as their age, hobbies, education, etc.) and movies (such as
their genre, release year, actors, origin country, etc.). More-
over, people sharing the same tastes for a class of movies

are likely to rate them similarly. As a result, the rows and
columns of the user-item matrix possess important struc-
tural information, which should be taken advantage of in
actual applications.

To preserve local geometric and discriminating structures
embedded in a high-dimensional space, numerous manifold
learning methods, such as locally linear embedding (LLE)
[20], locality preserving projection (LPP) [18], have been
proposed. In recent years, graph regularization based non-
negative matrix factorization [4] of data representation has
been developed to remedy the failure in representing geo-
metric structures in data. Inspired by this observation, to
comprehensively consider the associations between the users
and items and the local manifold structures of the user-item
data space, we propose to apply both user and item graph
regularizations. Unlike many existing recommendation al-
gorithms, we ﬁrst establish a new representation which is
infused with the above information. It turns out that this
new representation is not sparse anymore. Therefore, we
perform recommendation task with this novel representa-
tion.

2. DEFINITIONS AND NOTATIONS

Let U = {u1, u2, ..., um} and T = {t1, t2, ..., tn} represent
the sets of all users and all items, respectively. The whole
set of user-item purchases/ratings are represented by the
user-item matrix X of size m × n. Element xij is 1 or a
positive value if user ui has ever purchased/rated item tj,
otherwise it is marked as 0. The i-th row of X denotes the
purchase/rating history of user ui on all items. The j-th
column of X is the purchase/rating history of all users on
item tj. (cid:107)X(cid:107)2
ij is the squared Frobenius norm
of X. Tr(·) stands for the trace operator. I denotes the
identity matrix. (cid:12) is the Hadamard product.

F = (cid:80)

j x2

(cid:80)

i

3. PROPOSED MODEL

3.1 User and Item Graphs

User-item rating matrix is an overﬁt representation of user
tastes and item descriptions. This leads to problems of syn-
onymy, computational complexity, and potentially poorer
results. Therefore, a more compact representation of user
tastes and item descriptions is preferred. Graph regulariza-
tion is eﬀective in preserving local geometric and discrim-
inating structures embedded in a high-dimensional space.
It is based on the well known manifold assumption [18]: If
two data points such as xi and xj are close in the geodesic
distance on the data manifold, then their corresponding rep-
resentations yi and yj are also close to each other. In prac-
tice, it is diﬃcult to accurately estimate the global mani-
fold structure of the data due to the insuﬃcient number of
samples and the high dimensionality of the ambient space.
Therefore, many methods resort to local manifold struc-
tures. Much eﬀort on manifold learning [18] has shown that
local geometric structures of the data manifold can be eﬀec-
tively modeled through a nearest neighbor graph on sampled
data points.

We adopt graph regularization to incorporate user and
item proximities. In this paper, we construct two graphs:
the user graph and the item graph. We assume that users
having similar tastes for items form communities in the user
graph, while items having similar appeals to users form com-

munities in the item graph. Since “birds of a feather ﬂock
together”, this assumption is plausible and turns out to in-
deed beneﬁt recommender systems substantially in our ex-
periment results. As an example for movie recommendation,
the users are the vertices of a “social graph” whose edges rep-
resent relations induced by similar tastes.

More formally, we construct an undirected weighted graph
Gc = (Vc, Ec; Sc) on items, called the item graph. The ver-
tex set Vc corresponds to items {t1, · · · , tn} with each node
ti corresponding to a data point xi which is the i-th column
of X. Symmetric adjacency matrix Sc encodes the inter-item
information, in which sij is the weight of the edge joining
vertices ti and tj and represents how strong the relationship
or similarity items ti and tj have. Ec = {eij} is the edge set
with each edge eij between nodes ti and tj associated with
a weight sij. The graph regularization on the item graph is
formulated as

n
(cid:88)

1
2

(cid:107)yi − yj (cid:107)2

2sij =

yT

i yidii −

yT

i yj sij

n
(cid:88)

n
(cid:88)

(1)

i=1

i,j=1

i,j=1
=Tr(Y DcY T ) − Tr(Y ScY T ) = Tr(Y LcY T ),
where Dc is a diagonal matrix with dii = (cid:80)n
j=1 sij, and Lc =
Dc − Sc is the graph Laplacian. To preserve the structural
information of the manifold, we want (1) to be as small as
possible.
It is apparent that minimizing (1) imposes the
smoothness of the representation coeﬃcients; i.e., if items ti
and tj are similar (with a relatively bigger sij), their low-
dimensional representations yi and yj are also close to each
other. Therefore, optimizing (1) is an attempt to ensure the
manifold assumption.

The crucial part of graph regularization is the deﬁnition
of the adjacency matrix Sc. There exist a number of dif-
ferent similarity metrics in the literature [21], e.g., cosine
similarity, Pearson correlation coeﬃcient, and adjusted co-
sine similarity. For simplicity, in our experiment, we use
cosine similarity for explicit rating datasets and Jaccard
coeﬃcient for implicit feedback datesets. For binary vari-
ables, the Jaccard coeﬃcient is a more appropriate similar-
ity metric than cosine because it is insensitive to the am-
plitudes of ratings.
It measures the fraction of users who
have interactions with both items over the number of users
who have interacted either of them. Formally, according
to cosine deﬁnition, the similarity sij between two items
ti and tj is deﬁned as sij =
, where ‘·’ denotes
the vector dot-product operation. For Jaccard coeﬃcient,
sij = |xi∩xj |
|xi∪xj | , where ∩ and ∪ represent intersection and
union operations, respectively. Likewise, by deﬁning the
user graph Gr = (Vr, Er; Sr) whose vertex set Vr corre-
sponds to users {u1, · · · , um}, we get a corresponding ex-
pression Tr(Y T LrY ). Here Lr denotes the Laplacian of Gr,
which is similarly obtained from the data points correspond-
ing to the users, that is, the rows of X.

xi·xj
(cid:107)xi(cid:107)2(cid:107)xj (cid:107)2

3.2 Model

By exploiting both user and item graphs, our proposed

model can be written as

min
Y

(cid:107)X − Y (cid:107)2

F + αTr(Y LcY T ) + βTr(Y T LrY ).

(2)

The ﬁrst term of (2) penalizes large deviations of the pre-
dictions from the given ratings. The last two terms measure
the smoothness of the predicted ratings on the graph struc-
tures and encourage the ratings of nodes with aﬃnity to be
similar. They can alleviate the data sparsity issue to some
extent. When the item neighborhood information is not

available, user neighborhood information might exist, vice
versa. The parameters α and β adjust the balance between
the reconstruction error and graph regularizations.

By setting the derivative of the objective function of (2)

with respect to Y to zero, we have

(βLr + I)Y + αY Lc = X.

(3)

Equation (3) is the well known Sylvester equation, which
costs O(m3) or O(n3) with a general solver. But in our
situation, X is usually extremely sparse, and Lr and Lc can
also be sparse, especially for large m and n, so the cost can
be O(m2) or O(n2), or sometimes even as low as O(m) or
O(n) [2]. Many packages or programs are available to solve
(3).

To use the reconstructed matrix Y to make recommenda-
tions for user ui, we just sort ui’s non-purchased/non-rated
items based on their scores in non-increasing order and rec-
ommend the top N items.

4. CONNECTION TO EXISTING WORK

To the best of our knowledge, there are very few stud-
ies on graph Laplacian in the context of recommendation
task. Graph regularized weighted nonnegative matrix fac-
torization (GWNMF) [7] was proposed to incorporate the
neighborhood information in a MF approach. It solves the
following problem

min
U,V

(cid:107)M (cid:12) (X − U V T )(cid:107)2

F + αT r(U T LrU ) + βT r(V T LcV )

(4)

s.t. U ≥ 0, V ≥ 0,

where M is an indicator matrix. U and V T are in latent
spaces, whose dimensionality is usually speciﬁed with an
additional parameter. The latent factors are generally not
obvious and might not necessarily be interpretable or intu-
itively understandable. Here (4) has to learn both user and
item representations in the latent spaces. In our approach,
we just need to learn one representation and thus the learn-
ing process is simpliﬁed. On the other hand, U and V T
are supposed to be of low dimensionality, and thus useful
information can be lost during the low-rank approximation
of X from U and V T . The encoding of graph Laplacian on
U and V T might be not accurate any more. On the con-
trary, our method can better preserve the information in
X, so it can potentially give better recommendations than
GWNMF. Moreover, it is well known that several drawbacks
exist in the MF approach, e.g., low convergence rate, many
local optimums of U and V T due to the non-convexity of (4).
In contrast, our model (2) is strongly convex, admitting a
unique, globally optimal solution.

Table 1: The datasets used in evaluation

dataset #users #items #trns

rsize

csize

density

ratings

Delicious
lastfm
BX

1300
8813
4186

4516
6038
7733

17550
332486
182057

13.50
37.7
43.49

3.89
55.07
23.54

0.29%
0.62%
0.56%

-
-
-

2071
7026
5252

1508
6769
7635

23.54
17.21
27.87

17.14
16.59
40.51

35497
116537
212772

Filmtrust
Netﬂix
Yahoo
In this table, the “#users”, “#items”, “#trns” columns represent the num-
ber of users, number of items and number of transactions, respectively,
in each dataset. The “rsize” and “csize” columns show the average num-
ber of ratings of each user and of each item, respectively, in each dataset.
Column corresponding to “density” shows the density of each dataset (i.e.,
density=#trns/(#users×#items)). The “ratings” column is the rating range
of each dataset . The ratings in FilmTrust are real values with step 0.5, while
in the other datasets are integers.

1.14%
0.24%
0.53%

0.5-4
1-5
1-5

5. EXPERIMENTAL EVALUATION

5.1 Datasets

Table 1 shows the characteristics of the datasets. Deli-
cious, lastfm and BX have only implicit feedback. In par-
ticular, Delicious was from the bookmarking and tagging
information1, in which each URL was bookmarked by at
least 3 users. Lastfm represents music artist listening in-
formation2, in which each music artist was listened to by at
least 10 users and each user listened to at least 5 artists. BX
is derived from the Book-Crossing dataset3 such that only
implicit interactions were contained and each book was read
by at least 10 users.

FilmTrust, Netﬂix and Yahoo contain multi-value ratings.
Speciﬁcally, FilmTrust is a dataset crawled from the en-
tire FilmTrust website4. The Netﬂix is derived from Netﬂix
Prize dataset5 and each user rated at least 10 movies. The
Yahoo dataset is a subset obtained from Yahoo!Movies user
ratings6. In this dataset, each user rated at least 5 movies
and each movie was rated by at least 3 users.

5.2 Evaluation Methodology

For fair comparison, we follow the dataset preparation
approach used by SLIM [17] and adopt the 5-fold cross val-
idation. For each fold, a dataset is split into training and
test sets by randomly selecting one non-zero entry for each
user and putting it in the test set, while using the rest of the
data for training. Then a ranked list of size-N items for each
user is produced. We subsequently evaluate the method by
comparing the ranked list of recommended items with the
In the following results presented in
item in the test set.
this paper, N is equal to 10 by default.

For Top-N recommendation, the most direct and mean-
ingful metrics are hit-rate (HR) and the average reciprocal
hit-rank (ARHR) [6], since the users only care if a short
recommendation list contains the items of interest or not
rather than a very long recommendation list. HR is de-
ﬁned as HR = #hits
#users , where #hits is the number of users
whose item in the testing set is contained (i.e., hit) in the
size-N recommendation list, and #users is the total number
of users. ARHR is deﬁned as: ARHR = 1
1
,
pi
where pi is the position of the i-th hit in the ranked Top-N
list. In this metric, hits that occur earlier in the ranked list
are weighted higher than those occur later, and thus ARHR
indicates how strongly an item is recommended.

(cid:80)#hits
i=1

#users

6. EXPERIMENTAL RESULTS

6.1 Top-N Recommendation Performance

We use 5-fold cross-validation to choose parameters for
all competing methods and report their best performance in
Table 2. It can be seen that the HR improvements achieved
by our method against the next best performing scheme
(i.e., SLIM) are quite substantial on lastfm, Yahoo, BX,
FilmTrust datasets7. For Delicious and Netﬂix datasets, our

1http://www.delicious.com
2 http://www.last.fm
3http://www.informatik.uni-freiburg.de/ cziegler/BX/
4http://www.librec.net/datasets.html
5http://www.netﬂixprize.com/
6http://webscope.sandbox.yahoo.com/catalog.php?datatype=r
7Code is available at https://github.com/sckangz/CIKM16

Table 2: Comparison of Top-N recommendation algorithms

method
ItemKNN 300
PureSVD 1000
WRMF
250
BPRKNN 1e-4
300
BPRMF
10
GWNMF
10
SLIM
0.01
Our

method
ItemKNN 400
PureSVD 3000
WRMF
400
BPRKNN 1e-3
400
BPRMF
10
GWNMF
20
SLIM
0.01
Our

params
-
10
5
0.01
0.1
5
1
0.001

params
-
10
5
0.01
0.1
100
0.5
0.01

Delicious

HR
0.300
0.285
0.330
0.326
0.335
0.340
0.343
0.345

HR
0.045
0.043
0.047
0.047
0.048
0.049
0.050
0.060

-
-
-
-
-
10
-
-

-
-
-
-
-
-
-
-
BX

-
-
-
-
-
5
-
-

-
-
-
-
-
-
-
-
Netﬂix

ARHR
0.179
0.172
0.198
0.187
0.183
0.211
0.213
0.194

ARHR
0.026
0.023
0.027
0.028
0.027
0.027
0.029
0.030

100
200
100
1e-4
100
20
5
40

5
20
30
4e-3
100
15
10
1e-4

params
-
10
3
0.01
0.1
50
0.5
10

params
-
10
2
1e-3
0.5
5
15
1e-5

-
-
-
-
-
2
-
-

lastfm

HR
0.125
0.134
0.138
0.145
0.129
0.137
0.141
0.198

-
-
-
-
-
-
-
-
-
-
-
10
-
-
-
-
FilmTrust

HR
0.583
0.601
0.604
0.625
0.610
0.615
0.628
0.651

-
-
-
-
-
-
-
-
Yahoo

ARHR
0.075
0.078
0.078
0.083
0.073
0.077
0.082
0.085

ARHR
0.352
0.369
0.371
0.391
0.375
0.380
0.397
0.405

params
-
10
5
0.01
0.1
30
1.0
0.01

method
ARHR
0.085
ItemKNN 200
0.089
PureSVD 500
0.095
WRMF
300
0.090
BPRKNN 2e-3
0.072
300
BPRMF
0.080
5
GWNMF
0.098
5
SLIM
Our
0.078
0.02
1 The parameters for each method in the table are as follows: ItemKNN: the number of neighbors k; PureSVD: the number
of singular values and the number of SVD; WRMF: the latent space’s dimension and the weight on purchases; BPRKNN: its
learning rate and regularization parameter λ; BPRMF: the latent space’s dimension and learning rate; GWNMF: dimension of
the latent space, λ, and µ; SLIM: the l2-norm regularization coefficient β and the l1 -norm regularization parameter λ. Our:
item graph regularization parameter α and user graph regularization parameter β.

params
-
10
4
1e-3
0.1
200
1
5e-6

ARHR
0.185
0.118
0.128
0.182
0.180
0.183
0.187
0.194

HR
0.156
0.158
0.172
0.165
0.140
0.169
0.173
0.166

HR
0.318
0.210
0.250
0.310
0.308
0.313
0.320
0.379

300
2000
100
0.02
300
20
10
5e-3

-
-
-
-
-
50
-
-

-
-
-
-
-
50
-
-

-
-
-
-
-
-
-
-

-
-
-
-
-
-
-
-

(a) Delicious

(b) lastfm

(c) BX

(d) FilmTrust

(e) Netﬂix

(f) Yahoo

Figure 1: Performance versus Diﬀerent Values of N .

performance is close to the best performance of other meth-
ods. In most cases, there is no much diﬀerence among other

state-of-the-art methods in terms of HR. Figure 1 shows the
performance in HR of various methods for diﬀerent values of

N (i.e., 5, 10, 15, 20 and 25) on all six datasets. Our method
works the best in most cases.

6.2 Parameter Effects

Our model involves two trade-oﬀ parameters α and β,
which dictate how strongly item and user neighborhoods
and structure information contribute to the objective and
performance. In Figure 2, we depict the eﬀects of diﬀerent
α and β values on HR and ARHR for dataset FilmTrust
and Yahoo. The search for α ranges from 1e-6 to 1e-2 with
points from {1e-6, 1e-5, 1e-4, 1e-3, 1e-2}, the search points
for β are from {1e-6, 1e-4, 1e-2}. As can be seen from all
ﬁgures, our algorithm performs well over a wide range of α
and β values. HR and ARHR share the same trend with
varying α and β. Speciﬁcally, when α is small, HR and
ARHR both increase with α. After a certain point, they
begin to decrease. For FilmTrust, the performance with
β = 0.01 is very stable with respect to α. This suggests that
user-user similarity dominates the FilmTrust dataset.

6.3 Matrix Reconstruction

To show how our method reconstructs the user-item ma-
trix, we compare it with the method of next best perfor-
mance, SLIM, on FilmTrust. The density of FilmTrust is
1.14% and the mean for those non-zero elements is 2.998.
The reconstructed matrix ˆXSLIM from SLIM has a density
of 83.21%. For those 1.14% non-zero entries in X, ˆXSLIM
recovers 99.69% of them and their mean value is 1.686. In
contrast, the reconstructed matrix by our proposed algo-
rithm has a density of 91.7%. For those 1.14% non-zero en-
tries in X, our method recovers all of them with a mean of
2.975. These facts suggest that our method better recovers
X than SLIM. In other words, SLIM loses too much infor-
mation. This appears to explain the superior performance
of our method.

In fact, above analysis is equivalent to the two widely used
prediction accuracy metrics: Mean Absolute Error (MAE)
and Root Mean Squared Error (RMSE). Since our method
can recover the original ratings much better than SLIM, our
algorithm gives lower MAE and RMSE. This conclusion is
consistent with our HR and ARHR evaluation.

Table 3: Results with Diﬀerent Similarity Metrics

Datasets
Delicious
lastfm
BX

HR
Jaccard Cosine
0.330
0.345
0.204
0.198
0.057
0.060

ARHR
Jaccard Cosine
0.202
0.087
0.024

0.194
0.085
0.030

Table 4: Results by using diﬀerent graphs

FilmTrust
Yahoo

User Graph
0.638
0.303

0.625
0.379

Item Graph User-Item Graph

0.651
0.379

6.4 Graph Construction

Figure 3: Inﬂuence of neighborhood size k on
recommendation accuracy for FilmTrust dataset.

As we discussed previously, similarity is an important in-
In many recommendation
gredient of graph construction.
algorithms, the similarity computation is crucial to the rec-
ommendation quality [8]. To demonstrate the importance
of similarity metric, we use the cosine measure rather than
the Jaccard coeﬃcient to measure the similarity in binary
datasets. We compare the results in Table 3. As demon-
strated, for lastfm dataset, HR and ARHR increase after
we adopt the cosine similarity. However, for Delicious and
BX dataset, the Jaccard coeﬃcient works better. Therefore,
the diﬀerence of ﬁnal results can be big for certain datasets
with diﬀerent similarity measures. We expect that the ex-
perimental results in Table 2 can be further enhanced if one
performs a more careful analysis of Lr and Lc. For example,
it has been reported that normalizing the similarity scores
can improve the performance [6]. Also, a number of new
similarity metrics have recently been proposed, e.g., [16],
which may be also exploited.

Another important parameter is the neighborhood size
k, which cannot be known a priori [11]. For some small
datasets, setting a small k may not include all useful neigh-
bors and would infer incomplete relationships. In practice,
a large number of ratings from similar users or similar items
are not available, due to the sparsity inherent to rating data.
We just use the fully connected graph in our experiments.
To demonstrate this, we test the eﬀects of neighborhood size
k with values {10, 50, 200, 500, 800} on FilmTrust data. As
can be seen from Figure 3, the neighborhood size indeed in-
ﬂuences the performance of our proposed recommendation
method. Speciﬁcally, the performance keeps increasing as k
increases when k is small compared to the size of dataset,
then the performance keeps almost the same as the ﬁnal
accuracy obtained in Table 2 as k it becomes larger. This
conforms that a small neighborhood size can not capture all
similarity information.

6.5 Effects of User and Item Graphs

While the overall improvements are impressive, it would
be interesting to see more ﬁne-grained analysis of the im-
pact of user-user and item-item similarity graphs. We use
FilmTrust and Yahoo datasets as examples to show the ef-
fects of user and item graphs. Table 4 summarizes the HR
values obtained with user graph, item graph, and both user

[8] G. Guo, J. Zhang, and N. Yorke-Smith. A novel

bayesian similarity measure for recommender systems.
In IJCAI, pages 2619–2625. AAAI Press, 2013.
[9] J. L. Herlocker, J. A. Konstan, A. Borchers, and

J. Riedl. An algorithmic framework for performing
collaborative ﬁltering. In SIGIR, pages 230–237.
ACM, 1999.

[10] Y. Hu, Y. Koren, and C. Volinsky. Collaborative
ﬁltering for implicit feedback datasets. In ICDM,
pages 263–272. IEEE, 2008.

[11] J. Huang, F. Nie, and H. Huang. A new simplex

sparse learning model to measure data similarity for
clustering. In Proceedings of the 24th International
Conference on Artiﬁcial Intelligence, pages 3569–3575.
AAAI Press, 2015.

[12] Z. Kang and Q. Cheng. Top-n recommendation with

novel rank approximation. In SDM, pages 126–134.
SIAM, 2016.

[13] Z. Kang, C. Peng, and Q. Cheng. Robust subspace

clustering via tighter rank approximation. In CIKM,
pages 393–401. ACM, 2015.

[14] Z. Kang, C. Peng, and Q. Cheng. Top-n recommender
system via matrix completion. In Thirtieth AAAI
Conference on Artiﬁcial Intelligence, 2016.
[15] N. Koenigstein, P. Ram, and Y. Shavitt. Eﬃcient

retrieval of recommendations in a matrix factorization
framework. In CIKM, pages 535–544. ACM, 2012.
[16] H. Liu, Z. Hu, A. Mian, H. Tian, and X. Zhu. A new
user similarity model to improve the accuracy of
collaborative ﬁltering. Knowledge-Based Systems,
56:156–166, 2014.

[17] X. Ning and G. Karypis. Slim: Sparse linear methods
for top-n recommender systems. In ICDM, pages
497–506. IEEE, 2011.

[18] X. Niyogi. Locality preserving projections. In NIPS,

volume 16, page 153. MIT, 2004.

[19] S. Rendle, C. Freudenthaler, Z. Gantner, and

L. Schmidt-Thieme. Bpr: Bayesian personalized
ranking from implicit feedback. In UAI, pages
452–461. AUAI Press, 2009.

[20] S. T. Roweis and L. K. Saul. Nonlinear dimensionality

reduction by locally linear embedding. Science,
290(5500):2323–2326, 2000.

[21] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl.

Item-based collaborative ﬁltering recommendation
algorithms. In WWW, pages 285–295. ACM, 2001.

and item graph. It demonstrates that we are able to obtain
the best performance when we combine user and item graph.
Thus neighborhood information of users and items can al-
leviate the problem of data sparsity by taking advantage of
structural information more extensively, which in turn ben-
eﬁts the recommendation accuracy.

7. CONCLUSION

In this paper, we address the demands for high-quality rec-
ommendation on both implicit and explicit feedback datasets.
We reconstruct the user-item matrix by fully exploiting the
similarity information between users and items concurrently.
Moreover, the reconstructed data matrix also respects the
manifold structure of the user-item matrix. We conduct a
comprehensive set of experiments and compare our method
with other state-of-the-art Top-N recommendation algorithms.
The results demonstrate that the proposed algorithm works
eﬀectively. Due to the simplicity of our model, there is much
room to improve. For instance, our model can be easily ex-
tended to include side information (e.g., user demographic
information, item’s genre, social trust network) by utilizing
the graph representation. In some cases, external informa-
tion is more informative than the neighborhood information.

8. ACKNOWLEDGMENTS

This work is supported by the U.S. National Science Foun-
dation under Grant IIS 1218712, National Natural Science
Foundation of China under grant 11241005, and Shanxi Schol-
arship Council of China 2015-093. Q. Cheng is the corre-
sponding author.

9. REFERENCES
[1] G. Adomavicius and A. Tuzhilin. Toward the next

generation of recommender systems: A survey of the
state-of-the-art and possible extensions. Knowledge
and Data Engineering, IEEE Transactions on,
17(6):734–749, 2005.

[2] P. Benner, R.-C. Li, and N. Truhar. On the adi
method for sylvester equations. Journal of
Computational and Applied Mathematics,
233(4):1035–1045, 2009.

[3] F. Cacheda, V. Carneiro, D. Fern´andez, and

V. Formoso. Comparison of collaborative ﬁltering
algorithms: Limitations of current techniques and
proposals for scalable, high-performance recommender
systems. ACM Transactions on the Web, 5(1):2, 2011.

[4] D. Cai, X. He, J. Han, and T. S. Huang. Graph

regularized nonnegative matrix factorization for data
representation. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, 33(8):1548–1560,
2011.

[5] P. Cremonesi, Y. Koren, and R. Turrin. Performance
of recommender algorithms on top-n recommendation
tasks. In RecSys, pages 39–46. ACM, 2010.

[6] M. Deshpande and G. Karypis. Item-based top-n

recommendation algorithms. ACM Transactions on
Information Systems (TOIS), 22(1):143–177, 2004.

[7] Q. Gu, J. Zhou, and C. H. Ding. Collaborative

ﬁltering: Weighted nonnegative matrix factorization
incorporating user and item graphs. In SDM, pages
199–210. SIAM, 2010.

(a) FilmTrust

(b) FilmTrust

(c) Yahoo

(d) Yahoo

Figure 2: Inﬂuence of α and β on HR and ARHR.

Top-N Recommendation on Graphs

Zhao Kang, Chong Peng, Ming Yang, Qiang Cheng
Department of Computer Science, Southern Illinois University, Carbondale, IL, USA
{Zhao.Kang, pchong, ming.yang, qcheng}@siu.edu

6
1
0
2
 
p
e
S
 
7
2
 
 
]

R

I
.
s
c
[
 
 
1
v
4
6
2
8
0
.
9
0
6
1
:
v
i
X
r
a

ABSTRACT
Recommender systems play an increasingly important role
in online applications to help users ﬁnd what they need or
prefer. Collaborative ﬁltering algorithms that generate pre-
dictions by analyzing the user-item rating matrix perform
poorly when the matrix is sparse. To alleviate this problem,
this paper proposes a simple recommendation algorithm that
fully exploits the similarity information among users and
items and intrinsic structural information of the user-item
matrix. The proposed method constructs a new represen-
tation which preserves aﬃnity and structure information in
the user-item rating matrix and then performs recommenda-
tion task. To capture proximity information about users and
items, two graphs are constructed. Manifold learning idea
is used to constrain the new representation to be smooth on
these graphs, so as to enforce users and item proximities.
Our model is formulated as a convex optimization problem,
for which we need to solve the well known Sylvester equa-
tion only. We carry out extensive empirical evaluations on
six benchmark datasets to show the eﬀectiveness of this ap-
proach.

Keywords
top-N recommendation; laplacian graph; collaborative ﬁlter-
ing

1.

INTRODUCTION

Recommender systems have become increasingly indis-
pensable in many applications [1]. Collaborative ﬁltering
(CF) based methods are a fundamental building block in
many recommender systems. CF based recommender sys-
tems predict the ratings of items to be given by a user based
on the ratings of the items previously rated by other users
who are most similar to the target user.

CF based methods can be classiﬁed into memory-based
methods [21] and model-based methods [14, 15]. The for-
mer includes two popular methods, user-oriented [9] and
item-oriented, e.g., ItemKNN [6], depending on whether the

ACM ISBN 978-1-4503-2138-9.
DOI: 10.1145/1235

neighborhood information is derived from similar users or
items. First they compute similarities between the active
item and other items, or between the active user and other
users. Then they predict the unknown rating by combining
the known rating of top k neighbors. Due to the simplicity
of memory-based CF, it has been successfully applied in in-
dustry. However, it suﬀers from several problems, including
data sparsity, cold start and data correlation [3], as users
typically rate only a small portion of the available items,
and they also tend to rate similar items closely. Therefore,
the similarities between users or items cannot be accurately
obtained with the existing similarity measures such as co-
sine and Pearson correlation, which in turn compromises the
recommendation accuracy.

To alleviate the problems of memory-based methods, many
model-based methods have been proposed, which use ob-
served ratings to learn a predictive model. Among them,
matrix factorization (MF) based models, e.g., PureSVD [5]
and weighted regularized MF (WRMF) [10], are very pop-
ular due to their capability of capturing the implicit rela-
tionships among items and their outstanding performance.
Nevertheless, it introduces high computational complexity
and also faces the problem of uninterpretable recommenda-
tions. Because the rating matrix is sparse, the factorization
of the user-item matrix may lead to inferior solutions [7].
By learning an aggregation coeﬃcient matrix [13], recently,
sparse linear method (SLIM) [17] has been proposed and
shown to be eﬀective. However, it just captures relations
between items that have been co-purchased/co-rated by at
least one user [12]. Moreover, it only explores the linear rela-
tions between items. Another class of methods use Bayesian
personalized ranking (BPR) criterion to measure the diﬀer-
ence between the rankings of user-purchased items and the
remaining items. For instance, BPRMF and BPRKNN [19]
have been demonstrated to be eﬀective for implicit feedback
datasets.

In this paper, we propose a novel Top-N recommendation
model based on graphs. This method not only takes into
account the neighborhood information, which is encoded by
our user graph and item graph, but also reveals hidden struc-
ture in the data by deploying graph regularization. In the
real world, data often reside on low-dimensional manifolds
embedded in a high-dimensional ambient space. Like the
Netﬂix Prize problem, where the size of the user-item matrix
can be huge, there exist relationships between users (such
as their age, hobbies, education, etc.) and movies (such as
their genre, release year, actors, origin country, etc.). More-
over, people sharing the same tastes for a class of movies

are likely to rate them similarly. As a result, the rows and
columns of the user-item matrix possess important struc-
tural information, which should be taken advantage of in
actual applications.

To preserve local geometric and discriminating structures
embedded in a high-dimensional space, numerous manifold
learning methods, such as locally linear embedding (LLE)
[20], locality preserving projection (LPP) [18], have been
proposed. In recent years, graph regularization based non-
negative matrix factorization [4] of data representation has
been developed to remedy the failure in representing geo-
metric structures in data. Inspired by this observation, to
comprehensively consider the associations between the users
and items and the local manifold structures of the user-item
data space, we propose to apply both user and item graph
regularizations. Unlike many existing recommendation al-
gorithms, we ﬁrst establish a new representation which is
infused with the above information. It turns out that this
new representation is not sparse anymore. Therefore, we
perform recommendation task with this novel representa-
tion.

2. DEFINITIONS AND NOTATIONS

Let U = {u1, u2, ..., um} and T = {t1, t2, ..., tn} represent
the sets of all users and all items, respectively. The whole
set of user-item purchases/ratings are represented by the
user-item matrix X of size m × n. Element xij is 1 or a
positive value if user ui has ever purchased/rated item tj,
otherwise it is marked as 0. The i-th row of X denotes the
purchase/rating history of user ui on all items. The j-th
column of X is the purchase/rating history of all users on
item tj. (cid:107)X(cid:107)2
ij is the squared Frobenius norm
of X. Tr(·) stands for the trace operator. I denotes the
identity matrix. (cid:12) is the Hadamard product.

F = (cid:80)

j x2

(cid:80)

i

3. PROPOSED MODEL

3.1 User and Item Graphs

User-item rating matrix is an overﬁt representation of user
tastes and item descriptions. This leads to problems of syn-
onymy, computational complexity, and potentially poorer
results. Therefore, a more compact representation of user
tastes and item descriptions is preferred. Graph regulariza-
tion is eﬀective in preserving local geometric and discrim-
inating structures embedded in a high-dimensional space.
It is based on the well known manifold assumption [18]: If
two data points such as xi and xj are close in the geodesic
distance on the data manifold, then their corresponding rep-
resentations yi and yj are also close to each other. In prac-
tice, it is diﬃcult to accurately estimate the global mani-
fold structure of the data due to the insuﬃcient number of
samples and the high dimensionality of the ambient space.
Therefore, many methods resort to local manifold struc-
tures. Much eﬀort on manifold learning [18] has shown that
local geometric structures of the data manifold can be eﬀec-
tively modeled through a nearest neighbor graph on sampled
data points.

We adopt graph regularization to incorporate user and
item proximities. In this paper, we construct two graphs:
the user graph and the item graph. We assume that users
having similar tastes for items form communities in the user
graph, while items having similar appeals to users form com-

munities in the item graph. Since “birds of a feather ﬂock
together”, this assumption is plausible and turns out to in-
deed beneﬁt recommender systems substantially in our ex-
periment results. As an example for movie recommendation,
the users are the vertices of a “social graph” whose edges rep-
resent relations induced by similar tastes.

More formally, we construct an undirected weighted graph
Gc = (Vc, Ec; Sc) on items, called the item graph. The ver-
tex set Vc corresponds to items {t1, · · · , tn} with each node
ti corresponding to a data point xi which is the i-th column
of X. Symmetric adjacency matrix Sc encodes the inter-item
information, in which sij is the weight of the edge joining
vertices ti and tj and represents how strong the relationship
or similarity items ti and tj have. Ec = {eij} is the edge set
with each edge eij between nodes ti and tj associated with
a weight sij. The graph regularization on the item graph is
formulated as

n
(cid:88)

1
2

(cid:107)yi − yj (cid:107)2

2sij =

yT

i yidii −

yT

i yj sij

n
(cid:88)

n
(cid:88)

(1)

i=1

i,j=1

i,j=1
=Tr(Y DcY T ) − Tr(Y ScY T ) = Tr(Y LcY T ),
where Dc is a diagonal matrix with dii = (cid:80)n
j=1 sij, and Lc =
Dc − Sc is the graph Laplacian. To preserve the structural
information of the manifold, we want (1) to be as small as
possible.
It is apparent that minimizing (1) imposes the
smoothness of the representation coeﬃcients; i.e., if items ti
and tj are similar (with a relatively bigger sij), their low-
dimensional representations yi and yj are also close to each
other. Therefore, optimizing (1) is an attempt to ensure the
manifold assumption.

The crucial part of graph regularization is the deﬁnition
of the adjacency matrix Sc. There exist a number of dif-
ferent similarity metrics in the literature [21], e.g., cosine
similarity, Pearson correlation coeﬃcient, and adjusted co-
sine similarity. For simplicity, in our experiment, we use
cosine similarity for explicit rating datasets and Jaccard
coeﬃcient for implicit feedback datesets. For binary vari-
ables, the Jaccard coeﬃcient is a more appropriate similar-
ity metric than cosine because it is insensitive to the am-
plitudes of ratings.
It measures the fraction of users who
have interactions with both items over the number of users
who have interacted either of them. Formally, according
to cosine deﬁnition, the similarity sij between two items
ti and tj is deﬁned as sij =
, where ‘·’ denotes
the vector dot-product operation. For Jaccard coeﬃcient,
sij = |xi∩xj |
|xi∪xj | , where ∩ and ∪ represent intersection and
union operations, respectively. Likewise, by deﬁning the
user graph Gr = (Vr, Er; Sr) whose vertex set Vr corre-
sponds to users {u1, · · · , um}, we get a corresponding ex-
pression Tr(Y T LrY ). Here Lr denotes the Laplacian of Gr,
which is similarly obtained from the data points correspond-
ing to the users, that is, the rows of X.

xi·xj
(cid:107)xi(cid:107)2(cid:107)xj (cid:107)2

3.2 Model

By exploiting both user and item graphs, our proposed

model can be written as

min
Y

(cid:107)X − Y (cid:107)2

F + αTr(Y LcY T ) + βTr(Y T LrY ).

(2)

The ﬁrst term of (2) penalizes large deviations of the pre-
dictions from the given ratings. The last two terms measure
the smoothness of the predicted ratings on the graph struc-
tures and encourage the ratings of nodes with aﬃnity to be
similar. They can alleviate the data sparsity issue to some
extent. When the item neighborhood information is not

available, user neighborhood information might exist, vice
versa. The parameters α and β adjust the balance between
the reconstruction error and graph regularizations.

By setting the derivative of the objective function of (2)

with respect to Y to zero, we have

(βLr + I)Y + αY Lc = X.

(3)

Equation (3) is the well known Sylvester equation, which
costs O(m3) or O(n3) with a general solver. But in our
situation, X is usually extremely sparse, and Lr and Lc can
also be sparse, especially for large m and n, so the cost can
be O(m2) or O(n2), or sometimes even as low as O(m) or
O(n) [2]. Many packages or programs are available to solve
(3).

To use the reconstructed matrix Y to make recommenda-
tions for user ui, we just sort ui’s non-purchased/non-rated
items based on their scores in non-increasing order and rec-
ommend the top N items.

4. CONNECTION TO EXISTING WORK

To the best of our knowledge, there are very few stud-
ies on graph Laplacian in the context of recommendation
task. Graph regularized weighted nonnegative matrix fac-
torization (GWNMF) [7] was proposed to incorporate the
neighborhood information in a MF approach. It solves the
following problem

min
U,V

(cid:107)M (cid:12) (X − U V T )(cid:107)2

F + αT r(U T LrU ) + βT r(V T LcV )

(4)

s.t. U ≥ 0, V ≥ 0,

where M is an indicator matrix. U and V T are in latent
spaces, whose dimensionality is usually speciﬁed with an
additional parameter. The latent factors are generally not
obvious and might not necessarily be interpretable or intu-
itively understandable. Here (4) has to learn both user and
item representations in the latent spaces. In our approach,
we just need to learn one representation and thus the learn-
ing process is simpliﬁed. On the other hand, U and V T
are supposed to be of low dimensionality, and thus useful
information can be lost during the low-rank approximation
of X from U and V T . The encoding of graph Laplacian on
U and V T might be not accurate any more. On the con-
trary, our method can better preserve the information in
X, so it can potentially give better recommendations than
GWNMF. Moreover, it is well known that several drawbacks
exist in the MF approach, e.g., low convergence rate, many
local optimums of U and V T due to the non-convexity of (4).
In contrast, our model (2) is strongly convex, admitting a
unique, globally optimal solution.

Table 1: The datasets used in evaluation

dataset #users #items #trns

rsize

csize

density

ratings

Delicious
lastfm
BX

1300
8813
4186

4516
6038
7733

17550
332486
182057

13.50
37.7
43.49

3.89
55.07
23.54

0.29%
0.62%
0.56%

-
-
-

2071
7026
5252

1508
6769
7635

23.54
17.21
27.87

17.14
16.59
40.51

35497
116537
212772

Filmtrust
Netﬂix
Yahoo
In this table, the “#users”, “#items”, “#trns” columns represent the num-
ber of users, number of items and number of transactions, respectively,
in each dataset. The “rsize” and “csize” columns show the average num-
ber of ratings of each user and of each item, respectively, in each dataset.
Column corresponding to “density” shows the density of each dataset (i.e.,
density=#trns/(#users×#items)). The “ratings” column is the rating range
of each dataset . The ratings in FilmTrust are real values with step 0.5, while
in the other datasets are integers.

1.14%
0.24%
0.53%

0.5-4
1-5
1-5

5. EXPERIMENTAL EVALUATION

5.1 Datasets

Table 1 shows the characteristics of the datasets. Deli-
cious, lastfm and BX have only implicit feedback. In par-
ticular, Delicious was from the bookmarking and tagging
information1, in which each URL was bookmarked by at
least 3 users. Lastfm represents music artist listening in-
formation2, in which each music artist was listened to by at
least 10 users and each user listened to at least 5 artists. BX
is derived from the Book-Crossing dataset3 such that only
implicit interactions were contained and each book was read
by at least 10 users.

FilmTrust, Netﬂix and Yahoo contain multi-value ratings.
Speciﬁcally, FilmTrust is a dataset crawled from the en-
tire FilmTrust website4. The Netﬂix is derived from Netﬂix
Prize dataset5 and each user rated at least 10 movies. The
Yahoo dataset is a subset obtained from Yahoo!Movies user
ratings6. In this dataset, each user rated at least 5 movies
and each movie was rated by at least 3 users.

5.2 Evaluation Methodology

For fair comparison, we follow the dataset preparation
approach used by SLIM [17] and adopt the 5-fold cross val-
idation. For each fold, a dataset is split into training and
test sets by randomly selecting one non-zero entry for each
user and putting it in the test set, while using the rest of the
data for training. Then a ranked list of size-N items for each
user is produced. We subsequently evaluate the method by
comparing the ranked list of recommended items with the
In the following results presented in
item in the test set.
this paper, N is equal to 10 by default.

For Top-N recommendation, the most direct and mean-
ingful metrics are hit-rate (HR) and the average reciprocal
hit-rank (ARHR) [6], since the users only care if a short
recommendation list contains the items of interest or not
rather than a very long recommendation list. HR is de-
ﬁned as HR = #hits
#users , where #hits is the number of users
whose item in the testing set is contained (i.e., hit) in the
size-N recommendation list, and #users is the total number
of users. ARHR is deﬁned as: ARHR = 1
1
,
pi
where pi is the position of the i-th hit in the ranked Top-N
list. In this metric, hits that occur earlier in the ranked list
are weighted higher than those occur later, and thus ARHR
indicates how strongly an item is recommended.

(cid:80)#hits
i=1

#users

6. EXPERIMENTAL RESULTS

6.1 Top-N Recommendation Performance

We use 5-fold cross-validation to choose parameters for
all competing methods and report their best performance in
Table 2. It can be seen that the HR improvements achieved
by our method against the next best performing scheme
(i.e., SLIM) are quite substantial on lastfm, Yahoo, BX,
FilmTrust datasets7. For Delicious and Netﬂix datasets, our

1http://www.delicious.com
2 http://www.last.fm
3http://www.informatik.uni-freiburg.de/ cziegler/BX/
4http://www.librec.net/datasets.html
5http://www.netﬂixprize.com/
6http://webscope.sandbox.yahoo.com/catalog.php?datatype=r
7Code is available at https://github.com/sckangz/CIKM16

Table 2: Comparison of Top-N recommendation algorithms

method
ItemKNN 300
PureSVD 1000
WRMF
250
BPRKNN 1e-4
300
BPRMF
10
GWNMF
10
SLIM
0.01
Our

method
ItemKNN 400
PureSVD 3000
WRMF
400
BPRKNN 1e-3
400
BPRMF
10
GWNMF
20
SLIM
0.01
Our

params
-
10
5
0.01
0.1
5
1
0.001

params
-
10
5
0.01
0.1
100
0.5
0.01

Delicious

HR
0.300
0.285
0.330
0.326
0.335
0.340
0.343
0.345

HR
0.045
0.043
0.047
0.047
0.048
0.049
0.050
0.060

-
-
-
-
-
10
-
-

-
-
-
-
-
-
-
-
BX

-
-
-
-
-
5
-
-

-
-
-
-
-
-
-
-
Netﬂix

ARHR
0.179
0.172
0.198
0.187
0.183
0.211
0.213
0.194

ARHR
0.026
0.023
0.027
0.028
0.027
0.027
0.029
0.030

100
200
100
1e-4
100
20
5
40

5
20
30
4e-3
100
15
10
1e-4

params
-
10
3
0.01
0.1
50
0.5
10

params
-
10
2
1e-3
0.5
5
15
1e-5

-
-
-
-
-
2
-
-

lastfm

HR
0.125
0.134
0.138
0.145
0.129
0.137
0.141
0.198

-
-
-
-
-
-
-
-
-
-
-
10
-
-
-
-
FilmTrust

HR
0.583
0.601
0.604
0.625
0.610
0.615
0.628
0.651

-
-
-
-
-
-
-
-
Yahoo

ARHR
0.075
0.078
0.078
0.083
0.073
0.077
0.082
0.085

ARHR
0.352
0.369
0.371
0.391
0.375
0.380
0.397
0.405

params
-
10
5
0.01
0.1
30
1.0
0.01

method
ARHR
0.085
ItemKNN 200
0.089
PureSVD 500
0.095
WRMF
300
0.090
BPRKNN 2e-3
0.072
300
BPRMF
0.080
5
GWNMF
0.098
5
SLIM
Our
0.078
0.02
1 The parameters for each method in the table are as follows: ItemKNN: the number of neighbors k; PureSVD: the number
of singular values and the number of SVD; WRMF: the latent space’s dimension and the weight on purchases; BPRKNN: its
learning rate and regularization parameter λ; BPRMF: the latent space’s dimension and learning rate; GWNMF: dimension of
the latent space, λ, and µ; SLIM: the l2-norm regularization coefficient β and the l1 -norm regularization parameter λ. Our:
item graph regularization parameter α and user graph regularization parameter β.

params
-
10
4
1e-3
0.1
200
1
5e-6

ARHR
0.185
0.118
0.128
0.182
0.180
0.183
0.187
0.194

HR
0.156
0.158
0.172
0.165
0.140
0.169
0.173
0.166

HR
0.318
0.210
0.250
0.310
0.308
0.313
0.320
0.379

300
2000
100
0.02
300
20
10
5e-3

-
-
-
-
-
50
-
-

-
-
-
-
-
50
-
-

-
-
-
-
-
-
-
-

-
-
-
-
-
-
-
-

(a) Delicious

(b) lastfm

(c) BX

(d) FilmTrust

(e) Netﬂix

(f) Yahoo

Figure 1: Performance versus Diﬀerent Values of N .

performance is close to the best performance of other meth-
ods. In most cases, there is no much diﬀerence among other

state-of-the-art methods in terms of HR. Figure 1 shows the
performance in HR of various methods for diﬀerent values of

N (i.e., 5, 10, 15, 20 and 25) on all six datasets. Our method
works the best in most cases.

6.2 Parameter Effects

Our model involves two trade-oﬀ parameters α and β,
which dictate how strongly item and user neighborhoods
and structure information contribute to the objective and
performance. In Figure 2, we depict the eﬀects of diﬀerent
α and β values on HR and ARHR for dataset FilmTrust
and Yahoo. The search for α ranges from 1e-6 to 1e-2 with
points from {1e-6, 1e-5, 1e-4, 1e-3, 1e-2}, the search points
for β are from {1e-6, 1e-4, 1e-2}. As can be seen from all
ﬁgures, our algorithm performs well over a wide range of α
and β values. HR and ARHR share the same trend with
varying α and β. Speciﬁcally, when α is small, HR and
ARHR both increase with α. After a certain point, they
begin to decrease. For FilmTrust, the performance with
β = 0.01 is very stable with respect to α. This suggests that
user-user similarity dominates the FilmTrust dataset.

6.3 Matrix Reconstruction

To show how our method reconstructs the user-item ma-
trix, we compare it with the method of next best perfor-
mance, SLIM, on FilmTrust. The density of FilmTrust is
1.14% and the mean for those non-zero elements is 2.998.
The reconstructed matrix ˆXSLIM from SLIM has a density
of 83.21%. For those 1.14% non-zero entries in X, ˆXSLIM
recovers 99.69% of them and their mean value is 1.686. In
contrast, the reconstructed matrix by our proposed algo-
rithm has a density of 91.7%. For those 1.14% non-zero en-
tries in X, our method recovers all of them with a mean of
2.975. These facts suggest that our method better recovers
X than SLIM. In other words, SLIM loses too much infor-
mation. This appears to explain the superior performance
of our method.

In fact, above analysis is equivalent to the two widely used
prediction accuracy metrics: Mean Absolute Error (MAE)
and Root Mean Squared Error (RMSE). Since our method
can recover the original ratings much better than SLIM, our
algorithm gives lower MAE and RMSE. This conclusion is
consistent with our HR and ARHR evaluation.

Table 3: Results with Diﬀerent Similarity Metrics

Datasets
Delicious
lastfm
BX

HR
Jaccard Cosine
0.330
0.345
0.204
0.198
0.057
0.060

ARHR
Jaccard Cosine
0.202
0.087
0.024

0.194
0.085
0.030

Table 4: Results by using diﬀerent graphs

FilmTrust
Yahoo

User Graph
0.638
0.303

0.625
0.379

Item Graph User-Item Graph

0.651
0.379

6.4 Graph Construction

Figure 3: Inﬂuence of neighborhood size k on
recommendation accuracy for FilmTrust dataset.

As we discussed previously, similarity is an important in-
In many recommendation
gredient of graph construction.
algorithms, the similarity computation is crucial to the rec-
ommendation quality [8]. To demonstrate the importance
of similarity metric, we use the cosine measure rather than
the Jaccard coeﬃcient to measure the similarity in binary
datasets. We compare the results in Table 3. As demon-
strated, for lastfm dataset, HR and ARHR increase after
we adopt the cosine similarity. However, for Delicious and
BX dataset, the Jaccard coeﬃcient works better. Therefore,
the diﬀerence of ﬁnal results can be big for certain datasets
with diﬀerent similarity measures. We expect that the ex-
perimental results in Table 2 can be further enhanced if one
performs a more careful analysis of Lr and Lc. For example,
it has been reported that normalizing the similarity scores
can improve the performance [6]. Also, a number of new
similarity metrics have recently been proposed, e.g., [16],
which may be also exploited.

Another important parameter is the neighborhood size
k, which cannot be known a priori [11]. For some small
datasets, setting a small k may not include all useful neigh-
bors and would infer incomplete relationships. In practice,
a large number of ratings from similar users or similar items
are not available, due to the sparsity inherent to rating data.
We just use the fully connected graph in our experiments.
To demonstrate this, we test the eﬀects of neighborhood size
k with values {10, 50, 200, 500, 800} on FilmTrust data. As
can be seen from Figure 3, the neighborhood size indeed in-
ﬂuences the performance of our proposed recommendation
method. Speciﬁcally, the performance keeps increasing as k
increases when k is small compared to the size of dataset,
then the performance keeps almost the same as the ﬁnal
accuracy obtained in Table 2 as k it becomes larger. This
conforms that a small neighborhood size can not capture all
similarity information.

6.5 Effects of User and Item Graphs

While the overall improvements are impressive, it would
be interesting to see more ﬁne-grained analysis of the im-
pact of user-user and item-item similarity graphs. We use
FilmTrust and Yahoo datasets as examples to show the ef-
fects of user and item graphs. Table 4 summarizes the HR
values obtained with user graph, item graph, and both user

[8] G. Guo, J. Zhang, and N. Yorke-Smith. A novel

bayesian similarity measure for recommender systems.
In IJCAI, pages 2619–2625. AAAI Press, 2013.
[9] J. L. Herlocker, J. A. Konstan, A. Borchers, and

J. Riedl. An algorithmic framework for performing
collaborative ﬁltering. In SIGIR, pages 230–237.
ACM, 1999.

[10] Y. Hu, Y. Koren, and C. Volinsky. Collaborative
ﬁltering for implicit feedback datasets. In ICDM,
pages 263–272. IEEE, 2008.

[11] J. Huang, F. Nie, and H. Huang. A new simplex

sparse learning model to measure data similarity for
clustering. In Proceedings of the 24th International
Conference on Artiﬁcial Intelligence, pages 3569–3575.
AAAI Press, 2015.

[12] Z. Kang and Q. Cheng. Top-n recommendation with

novel rank approximation. In SDM, pages 126–134.
SIAM, 2016.

[13] Z. Kang, C. Peng, and Q. Cheng. Robust subspace

clustering via tighter rank approximation. In CIKM,
pages 393–401. ACM, 2015.

[14] Z. Kang, C. Peng, and Q. Cheng. Top-n recommender
system via matrix completion. In Thirtieth AAAI
Conference on Artiﬁcial Intelligence, 2016.
[15] N. Koenigstein, P. Ram, and Y. Shavitt. Eﬃcient

retrieval of recommendations in a matrix factorization
framework. In CIKM, pages 535–544. ACM, 2012.
[16] H. Liu, Z. Hu, A. Mian, H. Tian, and X. Zhu. A new
user similarity model to improve the accuracy of
collaborative ﬁltering. Knowledge-Based Systems,
56:156–166, 2014.

[17] X. Ning and G. Karypis. Slim: Sparse linear methods
for top-n recommender systems. In ICDM, pages
497–506. IEEE, 2011.

[18] X. Niyogi. Locality preserving projections. In NIPS,

volume 16, page 153. MIT, 2004.

[19] S. Rendle, C. Freudenthaler, Z. Gantner, and

L. Schmidt-Thieme. Bpr: Bayesian personalized
ranking from implicit feedback. In UAI, pages
452–461. AUAI Press, 2009.

[20] S. T. Roweis and L. K. Saul. Nonlinear dimensionality

reduction by locally linear embedding. Science,
290(5500):2323–2326, 2000.

[21] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl.

Item-based collaborative ﬁltering recommendation
algorithms. In WWW, pages 285–295. ACM, 2001.

and item graph. It demonstrates that we are able to obtain
the best performance when we combine user and item graph.
Thus neighborhood information of users and items can al-
leviate the problem of data sparsity by taking advantage of
structural information more extensively, which in turn ben-
eﬁts the recommendation accuracy.

7. CONCLUSION

In this paper, we address the demands for high-quality rec-
ommendation on both implicit and explicit feedback datasets.
We reconstruct the user-item matrix by fully exploiting the
similarity information between users and items concurrently.
Moreover, the reconstructed data matrix also respects the
manifold structure of the user-item matrix. We conduct a
comprehensive set of experiments and compare our method
with other state-of-the-art Top-N recommendation algorithms.
The results demonstrate that the proposed algorithm works
eﬀectively. Due to the simplicity of our model, there is much
room to improve. For instance, our model can be easily ex-
tended to include side information (e.g., user demographic
information, item’s genre, social trust network) by utilizing
the graph representation. In some cases, external informa-
tion is more informative than the neighborhood information.

8. ACKNOWLEDGMENTS

This work is supported by the U.S. National Science Foun-
dation under Grant IIS 1218712, National Natural Science
Foundation of China under grant 11241005, and Shanxi Schol-
arship Council of China 2015-093. Q. Cheng is the corre-
sponding author.

9. REFERENCES
[1] G. Adomavicius and A. Tuzhilin. Toward the next

generation of recommender systems: A survey of the
state-of-the-art and possible extensions. Knowledge
and Data Engineering, IEEE Transactions on,
17(6):734–749, 2005.

[2] P. Benner, R.-C. Li, and N. Truhar. On the adi
method for sylvester equations. Journal of
Computational and Applied Mathematics,
233(4):1035–1045, 2009.

[3] F. Cacheda, V. Carneiro, D. Fern´andez, and

V. Formoso. Comparison of collaborative ﬁltering
algorithms: Limitations of current techniques and
proposals for scalable, high-performance recommender
systems. ACM Transactions on the Web, 5(1):2, 2011.

[4] D. Cai, X. He, J. Han, and T. S. Huang. Graph

regularized nonnegative matrix factorization for data
representation. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, 33(8):1548–1560,
2011.

[5] P. Cremonesi, Y. Koren, and R. Turrin. Performance
of recommender algorithms on top-n recommendation
tasks. In RecSys, pages 39–46. ACM, 2010.

[6] M. Deshpande and G. Karypis. Item-based top-n

recommendation algorithms. ACM Transactions on
Information Systems (TOIS), 22(1):143–177, 2004.

[7] Q. Gu, J. Zhou, and C. H. Ding. Collaborative

ﬁltering: Weighted nonnegative matrix factorization
incorporating user and item graphs. In SDM, pages
199–210. SIAM, 2010.

(a) FilmTrust

(b) FilmTrust

(c) Yahoo

(d) Yahoo

Figure 2: Inﬂuence of α and β on HR and ARHR.

Top-N Recommendation on Graphs

Zhao Kang, Chong Peng, Ming Yang, Qiang Cheng
Department of Computer Science, Southern Illinois University, Carbondale, IL, USA
{Zhao.Kang, pchong, ming.yang, qcheng}@siu.edu

6
1
0
2
 
p
e
S
 
7
2
 
 
]

R

I
.
s
c
[
 
 
1
v
4
6
2
8
0
.
9
0
6
1
:
v
i
X
r
a

ABSTRACT
Recommender systems play an increasingly important role
in online applications to help users ﬁnd what they need or
prefer. Collaborative ﬁltering algorithms that generate pre-
dictions by analyzing the user-item rating matrix perform
poorly when the matrix is sparse. To alleviate this problem,
this paper proposes a simple recommendation algorithm that
fully exploits the similarity information among users and
items and intrinsic structural information of the user-item
matrix. The proposed method constructs a new represen-
tation which preserves aﬃnity and structure information in
the user-item rating matrix and then performs recommenda-
tion task. To capture proximity information about users and
items, two graphs are constructed. Manifold learning idea
is used to constrain the new representation to be smooth on
these graphs, so as to enforce users and item proximities.
Our model is formulated as a convex optimization problem,
for which we need to solve the well known Sylvester equa-
tion only. We carry out extensive empirical evaluations on
six benchmark datasets to show the eﬀectiveness of this ap-
proach.

Keywords
top-N recommendation; laplacian graph; collaborative ﬁlter-
ing

1.

INTRODUCTION

Recommender systems have become increasingly indis-
pensable in many applications [1]. Collaborative ﬁltering
(CF) based methods are a fundamental building block in
many recommender systems. CF based recommender sys-
tems predict the ratings of items to be given by a user based
on the ratings of the items previously rated by other users
who are most similar to the target user.

CF based methods can be classiﬁed into memory-based
methods [21] and model-based methods [14, 15]. The for-
mer includes two popular methods, user-oriented [9] and
item-oriented, e.g., ItemKNN [6], depending on whether the

ACM ISBN 978-1-4503-2138-9.
DOI: 10.1145/1235

neighborhood information is derived from similar users or
items. First they compute similarities between the active
item and other items, or between the active user and other
users. Then they predict the unknown rating by combining
the known rating of top k neighbors. Due to the simplicity
of memory-based CF, it has been successfully applied in in-
dustry. However, it suﬀers from several problems, including
data sparsity, cold start and data correlation [3], as users
typically rate only a small portion of the available items,
and they also tend to rate similar items closely. Therefore,
the similarities between users or items cannot be accurately
obtained with the existing similarity measures such as co-
sine and Pearson correlation, which in turn compromises the
recommendation accuracy.

To alleviate the problems of memory-based methods, many
model-based methods have been proposed, which use ob-
served ratings to learn a predictive model. Among them,
matrix factorization (MF) based models, e.g., PureSVD [5]
and weighted regularized MF (WRMF) [10], are very pop-
ular due to their capability of capturing the implicit rela-
tionships among items and their outstanding performance.
Nevertheless, it introduces high computational complexity
and also faces the problem of uninterpretable recommenda-
tions. Because the rating matrix is sparse, the factorization
of the user-item matrix may lead to inferior solutions [7].
By learning an aggregation coeﬃcient matrix [13], recently,
sparse linear method (SLIM) [17] has been proposed and
shown to be eﬀective. However, it just captures relations
between items that have been co-purchased/co-rated by at
least one user [12]. Moreover, it only explores the linear rela-
tions between items. Another class of methods use Bayesian
personalized ranking (BPR) criterion to measure the diﬀer-
ence between the rankings of user-purchased items and the
remaining items. For instance, BPRMF and BPRKNN [19]
have been demonstrated to be eﬀective for implicit feedback
datasets.

In this paper, we propose a novel Top-N recommendation
model based on graphs. This method not only takes into
account the neighborhood information, which is encoded by
our user graph and item graph, but also reveals hidden struc-
ture in the data by deploying graph regularization. In the
real world, data often reside on low-dimensional manifolds
embedded in a high-dimensional ambient space. Like the
Netﬂix Prize problem, where the size of the user-item matrix
can be huge, there exist relationships between users (such
as their age, hobbies, education, etc.) and movies (such as
their genre, release year, actors, origin country, etc.). More-
over, people sharing the same tastes for a class of movies

are likely to rate them similarly. As a result, the rows and
columns of the user-item matrix possess important struc-
tural information, which should be taken advantage of in
actual applications.

To preserve local geometric and discriminating structures
embedded in a high-dimensional space, numerous manifold
learning methods, such as locally linear embedding (LLE)
[20], locality preserving projection (LPP) [18], have been
proposed. In recent years, graph regularization based non-
negative matrix factorization [4] of data representation has
been developed to remedy the failure in representing geo-
metric structures in data. Inspired by this observation, to
comprehensively consider the associations between the users
and items and the local manifold structures of the user-item
data space, we propose to apply both user and item graph
regularizations. Unlike many existing recommendation al-
gorithms, we ﬁrst establish a new representation which is
infused with the above information. It turns out that this
new representation is not sparse anymore. Therefore, we
perform recommendation task with this novel representa-
tion.

2. DEFINITIONS AND NOTATIONS

Let U = {u1, u2, ..., um} and T = {t1, t2, ..., tn} represent
the sets of all users and all items, respectively. The whole
set of user-item purchases/ratings are represented by the
user-item matrix X of size m × n. Element xij is 1 or a
positive value if user ui has ever purchased/rated item tj,
otherwise it is marked as 0. The i-th row of X denotes the
purchase/rating history of user ui on all items. The j-th
column of X is the purchase/rating history of all users on
item tj. (cid:107)X(cid:107)2
ij is the squared Frobenius norm
of X. Tr(·) stands for the trace operator. I denotes the
identity matrix. (cid:12) is the Hadamard product.

F = (cid:80)

j x2

(cid:80)

i

3. PROPOSED MODEL

3.1 User and Item Graphs

User-item rating matrix is an overﬁt representation of user
tastes and item descriptions. This leads to problems of syn-
onymy, computational complexity, and potentially poorer
results. Therefore, a more compact representation of user
tastes and item descriptions is preferred. Graph regulariza-
tion is eﬀective in preserving local geometric and discrim-
inating structures embedded in a high-dimensional space.
It is based on the well known manifold assumption [18]: If
two data points such as xi and xj are close in the geodesic
distance on the data manifold, then their corresponding rep-
resentations yi and yj are also close to each other. In prac-
tice, it is diﬃcult to accurately estimate the global mani-
fold structure of the data due to the insuﬃcient number of
samples and the high dimensionality of the ambient space.
Therefore, many methods resort to local manifold struc-
tures. Much eﬀort on manifold learning [18] has shown that
local geometric structures of the data manifold can be eﬀec-
tively modeled through a nearest neighbor graph on sampled
data points.

We adopt graph regularization to incorporate user and
item proximities. In this paper, we construct two graphs:
the user graph and the item graph. We assume that users
having similar tastes for items form communities in the user
graph, while items having similar appeals to users form com-

munities in the item graph. Since “birds of a feather ﬂock
together”, this assumption is plausible and turns out to in-
deed beneﬁt recommender systems substantially in our ex-
periment results. As an example for movie recommendation,
the users are the vertices of a “social graph” whose edges rep-
resent relations induced by similar tastes.

More formally, we construct an undirected weighted graph
Gc = (Vc, Ec; Sc) on items, called the item graph. The ver-
tex set Vc corresponds to items {t1, · · · , tn} with each node
ti corresponding to a data point xi which is the i-th column
of X. Symmetric adjacency matrix Sc encodes the inter-item
information, in which sij is the weight of the edge joining
vertices ti and tj and represents how strong the relationship
or similarity items ti and tj have. Ec = {eij} is the edge set
with each edge eij between nodes ti and tj associated with
a weight sij. The graph regularization on the item graph is
formulated as

n
(cid:88)

1
2

(cid:107)yi − yj (cid:107)2

2sij =

yT

i yidii −

yT

i yj sij

n
(cid:88)

n
(cid:88)

(1)

i=1

i,j=1

i,j=1
=Tr(Y DcY T ) − Tr(Y ScY T ) = Tr(Y LcY T ),
where Dc is a diagonal matrix with dii = (cid:80)n
j=1 sij, and Lc =
Dc − Sc is the graph Laplacian. To preserve the structural
information of the manifold, we want (1) to be as small as
possible.
It is apparent that minimizing (1) imposes the
smoothness of the representation coeﬃcients; i.e., if items ti
and tj are similar (with a relatively bigger sij), their low-
dimensional representations yi and yj are also close to each
other. Therefore, optimizing (1) is an attempt to ensure the
manifold assumption.

The crucial part of graph regularization is the deﬁnition
of the adjacency matrix Sc. There exist a number of dif-
ferent similarity metrics in the literature [21], e.g., cosine
similarity, Pearson correlation coeﬃcient, and adjusted co-
sine similarity. For simplicity, in our experiment, we use
cosine similarity for explicit rating datasets and Jaccard
coeﬃcient for implicit feedback datesets. For binary vari-
ables, the Jaccard coeﬃcient is a more appropriate similar-
ity metric than cosine because it is insensitive to the am-
plitudes of ratings.
It measures the fraction of users who
have interactions with both items over the number of users
who have interacted either of them. Formally, according
to cosine deﬁnition, the similarity sij between two items
ti and tj is deﬁned as sij =
, where ‘·’ denotes
the vector dot-product operation. For Jaccard coeﬃcient,
sij = |xi∩xj |
|xi∪xj | , where ∩ and ∪ represent intersection and
union operations, respectively. Likewise, by deﬁning the
user graph Gr = (Vr, Er; Sr) whose vertex set Vr corre-
sponds to users {u1, · · · , um}, we get a corresponding ex-
pression Tr(Y T LrY ). Here Lr denotes the Laplacian of Gr,
which is similarly obtained from the data points correspond-
ing to the users, that is, the rows of X.

xi·xj
(cid:107)xi(cid:107)2(cid:107)xj (cid:107)2

3.2 Model

By exploiting both user and item graphs, our proposed

model can be written as

min
Y

(cid:107)X − Y (cid:107)2

F + αTr(Y LcY T ) + βTr(Y T LrY ).

(2)

The ﬁrst term of (2) penalizes large deviations of the pre-
dictions from the given ratings. The last two terms measure
the smoothness of the predicted ratings on the graph struc-
tures and encourage the ratings of nodes with aﬃnity to be
similar. They can alleviate the data sparsity issue to some
extent. When the item neighborhood information is not

available, user neighborhood information might exist, vice
versa. The parameters α and β adjust the balance between
the reconstruction error and graph regularizations.

By setting the derivative of the objective function of (2)

with respect to Y to zero, we have

(βLr + I)Y + αY Lc = X.

(3)

Equation (3) is the well known Sylvester equation, which
costs O(m3) or O(n3) with a general solver. But in our
situation, X is usually extremely sparse, and Lr and Lc can
also be sparse, especially for large m and n, so the cost can
be O(m2) or O(n2), or sometimes even as low as O(m) or
O(n) [2]. Many packages or programs are available to solve
(3).

To use the reconstructed matrix Y to make recommenda-
tions for user ui, we just sort ui’s non-purchased/non-rated
items based on their scores in non-increasing order and rec-
ommend the top N items.

4. CONNECTION TO EXISTING WORK

To the best of our knowledge, there are very few stud-
ies on graph Laplacian in the context of recommendation
task. Graph regularized weighted nonnegative matrix fac-
torization (GWNMF) [7] was proposed to incorporate the
neighborhood information in a MF approach. It solves the
following problem

min
U,V

(cid:107)M (cid:12) (X − U V T )(cid:107)2

F + αT r(U T LrU ) + βT r(V T LcV )

(4)

s.t. U ≥ 0, V ≥ 0,

where M is an indicator matrix. U and V T are in latent
spaces, whose dimensionality is usually speciﬁed with an
additional parameter. The latent factors are generally not
obvious and might not necessarily be interpretable or intu-
itively understandable. Here (4) has to learn both user and
item representations in the latent spaces. In our approach,
we just need to learn one representation and thus the learn-
ing process is simpliﬁed. On the other hand, U and V T
are supposed to be of low dimensionality, and thus useful
information can be lost during the low-rank approximation
of X from U and V T . The encoding of graph Laplacian on
U and V T might be not accurate any more. On the con-
trary, our method can better preserve the information in
X, so it can potentially give better recommendations than
GWNMF. Moreover, it is well known that several drawbacks
exist in the MF approach, e.g., low convergence rate, many
local optimums of U and V T due to the non-convexity of (4).
In contrast, our model (2) is strongly convex, admitting a
unique, globally optimal solution.

Table 1: The datasets used in evaluation

dataset #users #items #trns

rsize

csize

density

ratings

Delicious
lastfm
BX

1300
8813
4186

4516
6038
7733

17550
332486
182057

13.50
37.7
43.49

3.89
55.07
23.54

0.29%
0.62%
0.56%

-
-
-

2071
7026
5252

1508
6769
7635

23.54
17.21
27.87

17.14
16.59
40.51

35497
116537
212772

Filmtrust
Netﬂix
Yahoo
In this table, the “#users”, “#items”, “#trns” columns represent the num-
ber of users, number of items and number of transactions, respectively,
in each dataset. The “rsize” and “csize” columns show the average num-
ber of ratings of each user and of each item, respectively, in each dataset.
Column corresponding to “density” shows the density of each dataset (i.e.,
density=#trns/(#users×#items)). The “ratings” column is the rating range
of each dataset . The ratings in FilmTrust are real values with step 0.5, while
in the other datasets are integers.

1.14%
0.24%
0.53%

0.5-4
1-5
1-5

5. EXPERIMENTAL EVALUATION

5.1 Datasets

Table 1 shows the characteristics of the datasets. Deli-
cious, lastfm and BX have only implicit feedback. In par-
ticular, Delicious was from the bookmarking and tagging
information1, in which each URL was bookmarked by at
least 3 users. Lastfm represents music artist listening in-
formation2, in which each music artist was listened to by at
least 10 users and each user listened to at least 5 artists. BX
is derived from the Book-Crossing dataset3 such that only
implicit interactions were contained and each book was read
by at least 10 users.

FilmTrust, Netﬂix and Yahoo contain multi-value ratings.
Speciﬁcally, FilmTrust is a dataset crawled from the en-
tire FilmTrust website4. The Netﬂix is derived from Netﬂix
Prize dataset5 and each user rated at least 10 movies. The
Yahoo dataset is a subset obtained from Yahoo!Movies user
ratings6. In this dataset, each user rated at least 5 movies
and each movie was rated by at least 3 users.

5.2 Evaluation Methodology

For fair comparison, we follow the dataset preparation
approach used by SLIM [17] and adopt the 5-fold cross val-
idation. For each fold, a dataset is split into training and
test sets by randomly selecting one non-zero entry for each
user and putting it in the test set, while using the rest of the
data for training. Then a ranked list of size-N items for each
user is produced. We subsequently evaluate the method by
comparing the ranked list of recommended items with the
In the following results presented in
item in the test set.
this paper, N is equal to 10 by default.

For Top-N recommendation, the most direct and mean-
ingful metrics are hit-rate (HR) and the average reciprocal
hit-rank (ARHR) [6], since the users only care if a short
recommendation list contains the items of interest or not
rather than a very long recommendation list. HR is de-
ﬁned as HR = #hits
#users , where #hits is the number of users
whose item in the testing set is contained (i.e., hit) in the
size-N recommendation list, and #users is the total number
of users. ARHR is deﬁned as: ARHR = 1
1
,
pi
where pi is the position of the i-th hit in the ranked Top-N
list. In this metric, hits that occur earlier in the ranked list
are weighted higher than those occur later, and thus ARHR
indicates how strongly an item is recommended.

(cid:80)#hits
i=1

#users

6. EXPERIMENTAL RESULTS

6.1 Top-N Recommendation Performance

We use 5-fold cross-validation to choose parameters for
all competing methods and report their best performance in
Table 2. It can be seen that the HR improvements achieved
by our method against the next best performing scheme
(i.e., SLIM) are quite substantial on lastfm, Yahoo, BX,
FilmTrust datasets7. For Delicious and Netﬂix datasets, our

1http://www.delicious.com
2 http://www.last.fm
3http://www.informatik.uni-freiburg.de/ cziegler/BX/
4http://www.librec.net/datasets.html
5http://www.netﬂixprize.com/
6http://webscope.sandbox.yahoo.com/catalog.php?datatype=r
7Code is available at https://github.com/sckangz/CIKM16

Table 2: Comparison of Top-N recommendation algorithms

method
ItemKNN 300
PureSVD 1000
WRMF
250
BPRKNN 1e-4
300
BPRMF
10
GWNMF
10
SLIM
0.01
Our

method
ItemKNN 400
PureSVD 3000
WRMF
400
BPRKNN 1e-3
400
BPRMF
10
GWNMF
20
SLIM
0.01
Our

params
-
10
5
0.01
0.1
5
1
0.001

params
-
10
5
0.01
0.1
100
0.5
0.01

Delicious

HR
0.300
0.285
0.330
0.326
0.335
0.340
0.343
0.345

HR
0.045
0.043
0.047
0.047
0.048
0.049
0.050
0.060

-
-
-
-
-
10
-
-

-
-
-
-
-
-
-
-
BX

-
-
-
-
-
5
-
-

-
-
-
-
-
-
-
-
Netﬂix

ARHR
0.179
0.172
0.198
0.187
0.183
0.211
0.213
0.194

ARHR
0.026
0.023
0.027
0.028
0.027
0.027
0.029
0.030

100
200
100
1e-4
100
20
5
40

5
20
30
4e-3
100
15
10
1e-4

params
-
10
3
0.01
0.1
50
0.5
10

params
-
10
2
1e-3
0.5
5
15
1e-5

-
-
-
-
-
2
-
-

lastfm

HR
0.125
0.134
0.138
0.145
0.129
0.137
0.141
0.198

-
-
-
-
-
-
-
-
-
-
-
10
-
-
-
-
FilmTrust

HR
0.583
0.601
0.604
0.625
0.610
0.615
0.628
0.651

-
-
-
-
-
-
-
-
Yahoo

ARHR
0.075
0.078
0.078
0.083
0.073
0.077
0.082
0.085

ARHR
0.352
0.369
0.371
0.391
0.375
0.380
0.397
0.405

params
-
10
5
0.01
0.1
30
1.0
0.01

method
ARHR
0.085
ItemKNN 200
0.089
PureSVD 500
0.095
WRMF
300
0.090
BPRKNN 2e-3
0.072
300
BPRMF
0.080
5
GWNMF
0.098
5
SLIM
Our
0.078
0.02
1 The parameters for each method in the table are as follows: ItemKNN: the number of neighbors k; PureSVD: the number
of singular values and the number of SVD; WRMF: the latent space’s dimension and the weight on purchases; BPRKNN: its
learning rate and regularization parameter λ; BPRMF: the latent space’s dimension and learning rate; GWNMF: dimension of
the latent space, λ, and µ; SLIM: the l2-norm regularization coefficient β and the l1 -norm regularization parameter λ. Our:
item graph regularization parameter α and user graph regularization parameter β.

params
-
10
4
1e-3
0.1
200
1
5e-6

ARHR
0.185
0.118
0.128
0.182
0.180
0.183
0.187
0.194

HR
0.156
0.158
0.172
0.165
0.140
0.169
0.173
0.166

HR
0.318
0.210
0.250
0.310
0.308
0.313
0.320
0.379

300
2000
100
0.02
300
20
10
5e-3

-
-
-
-
-
50
-
-

-
-
-
-
-
50
-
-

-
-
-
-
-
-
-
-

-
-
-
-
-
-
-
-

(a) Delicious

(b) lastfm

(c) BX

(d) FilmTrust

(e) Netﬂix

(f) Yahoo

Figure 1: Performance versus Diﬀerent Values of N .

performance is close to the best performance of other meth-
ods. In most cases, there is no much diﬀerence among other

state-of-the-art methods in terms of HR. Figure 1 shows the
performance in HR of various methods for diﬀerent values of

N (i.e., 5, 10, 15, 20 and 25) on all six datasets. Our method
works the best in most cases.

6.2 Parameter Effects

Our model involves two trade-oﬀ parameters α and β,
which dictate how strongly item and user neighborhoods
and structure information contribute to the objective and
performance. In Figure 2, we depict the eﬀects of diﬀerent
α and β values on HR and ARHR for dataset FilmTrust
and Yahoo. The search for α ranges from 1e-6 to 1e-2 with
points from {1e-6, 1e-5, 1e-4, 1e-3, 1e-2}, the search points
for β are from {1e-6, 1e-4, 1e-2}. As can be seen from all
ﬁgures, our algorithm performs well over a wide range of α
and β values. HR and ARHR share the same trend with
varying α and β. Speciﬁcally, when α is small, HR and
ARHR both increase with α. After a certain point, they
begin to decrease. For FilmTrust, the performance with
β = 0.01 is very stable with respect to α. This suggests that
user-user similarity dominates the FilmTrust dataset.

6.3 Matrix Reconstruction

To show how our method reconstructs the user-item ma-
trix, we compare it with the method of next best perfor-
mance, SLIM, on FilmTrust. The density of FilmTrust is
1.14% and the mean for those non-zero elements is 2.998.
The reconstructed matrix ˆXSLIM from SLIM has a density
of 83.21%. For those 1.14% non-zero entries in X, ˆXSLIM
recovers 99.69% of them and their mean value is 1.686. In
contrast, the reconstructed matrix by our proposed algo-
rithm has a density of 91.7%. For those 1.14% non-zero en-
tries in X, our method recovers all of them with a mean of
2.975. These facts suggest that our method better recovers
X than SLIM. In other words, SLIM loses too much infor-
mation. This appears to explain the superior performance
of our method.

In fact, above analysis is equivalent to the two widely used
prediction accuracy metrics: Mean Absolute Error (MAE)
and Root Mean Squared Error (RMSE). Since our method
can recover the original ratings much better than SLIM, our
algorithm gives lower MAE and RMSE. This conclusion is
consistent with our HR and ARHR evaluation.

Table 3: Results with Diﬀerent Similarity Metrics

Datasets
Delicious
lastfm
BX

HR
Jaccard Cosine
0.330
0.345
0.204
0.198
0.057
0.060

ARHR
Jaccard Cosine
0.202
0.087
0.024

0.194
0.085
0.030

Table 4: Results by using diﬀerent graphs

FilmTrust
Yahoo

User Graph
0.638
0.303

0.625
0.379

Item Graph User-Item Graph

0.651
0.379

6.4 Graph Construction

Figure 3: Inﬂuence of neighborhood size k on
recommendation accuracy for FilmTrust dataset.

As we discussed previously, similarity is an important in-
In many recommendation
gredient of graph construction.
algorithms, the similarity computation is crucial to the rec-
ommendation quality [8]. To demonstrate the importance
of similarity metric, we use the cosine measure rather than
the Jaccard coeﬃcient to measure the similarity in binary
datasets. We compare the results in Table 3. As demon-
strated, for lastfm dataset, HR and ARHR increase after
we adopt the cosine similarity. However, for Delicious and
BX dataset, the Jaccard coeﬃcient works better. Therefore,
the diﬀerence of ﬁnal results can be big for certain datasets
with diﬀerent similarity measures. We expect that the ex-
perimental results in Table 2 can be further enhanced if one
performs a more careful analysis of Lr and Lc. For example,
it has been reported that normalizing the similarity scores
can improve the performance [6]. Also, a number of new
similarity metrics have recently been proposed, e.g., [16],
which may be also exploited.

Another important parameter is the neighborhood size
k, which cannot be known a priori [11]. For some small
datasets, setting a small k may not include all useful neigh-
bors and would infer incomplete relationships. In practice,
a large number of ratings from similar users or similar items
are not available, due to the sparsity inherent to rating data.
We just use the fully connected graph in our experiments.
To demonstrate this, we test the eﬀects of neighborhood size
k with values {10, 50, 200, 500, 800} on FilmTrust data. As
can be seen from Figure 3, the neighborhood size indeed in-
ﬂuences the performance of our proposed recommendation
method. Speciﬁcally, the performance keeps increasing as k
increases when k is small compared to the size of dataset,
then the performance keeps almost the same as the ﬁnal
accuracy obtained in Table 2 as k it becomes larger. This
conforms that a small neighborhood size can not capture all
similarity information.

6.5 Effects of User and Item Graphs

While the overall improvements are impressive, it would
be interesting to see more ﬁne-grained analysis of the im-
pact of user-user and item-item similarity graphs. We use
FilmTrust and Yahoo datasets as examples to show the ef-
fects of user and item graphs. Table 4 summarizes the HR
values obtained with user graph, item graph, and both user

[8] G. Guo, J. Zhang, and N. Yorke-Smith. A novel

bayesian similarity measure for recommender systems.
In IJCAI, pages 2619–2625. AAAI Press, 2013.
[9] J. L. Herlocker, J. A. Konstan, A. Borchers, and

J. Riedl. An algorithmic framework for performing
collaborative ﬁltering. In SIGIR, pages 230–237.
ACM, 1999.

[10] Y. Hu, Y. Koren, and C. Volinsky. Collaborative
ﬁltering for implicit feedback datasets. In ICDM,
pages 263–272. IEEE, 2008.

[11] J. Huang, F. Nie, and H. Huang. A new simplex

sparse learning model to measure data similarity for
clustering. In Proceedings of the 24th International
Conference on Artiﬁcial Intelligence, pages 3569–3575.
AAAI Press, 2015.

[12] Z. Kang and Q. Cheng. Top-n recommendation with

novel rank approximation. In SDM, pages 126–134.
SIAM, 2016.

[13] Z. Kang, C. Peng, and Q. Cheng. Robust subspace

clustering via tighter rank approximation. In CIKM,
pages 393–401. ACM, 2015.

[14] Z. Kang, C. Peng, and Q. Cheng. Top-n recommender
system via matrix completion. In Thirtieth AAAI
Conference on Artiﬁcial Intelligence, 2016.
[15] N. Koenigstein, P. Ram, and Y. Shavitt. Eﬃcient

retrieval of recommendations in a matrix factorization
framework. In CIKM, pages 535–544. ACM, 2012.
[16] H. Liu, Z. Hu, A. Mian, H. Tian, and X. Zhu. A new
user similarity model to improve the accuracy of
collaborative ﬁltering. Knowledge-Based Systems,
56:156–166, 2014.

[17] X. Ning and G. Karypis. Slim: Sparse linear methods
for top-n recommender systems. In ICDM, pages
497–506. IEEE, 2011.

[18] X. Niyogi. Locality preserving projections. In NIPS,

volume 16, page 153. MIT, 2004.

[19] S. Rendle, C. Freudenthaler, Z. Gantner, and

L. Schmidt-Thieme. Bpr: Bayesian personalized
ranking from implicit feedback. In UAI, pages
452–461. AUAI Press, 2009.

[20] S. T. Roweis and L. K. Saul. Nonlinear dimensionality

reduction by locally linear embedding. Science,
290(5500):2323–2326, 2000.

[21] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl.

Item-based collaborative ﬁltering recommendation
algorithms. In WWW, pages 285–295. ACM, 2001.

and item graph. It demonstrates that we are able to obtain
the best performance when we combine user and item graph.
Thus neighborhood information of users and items can al-
leviate the problem of data sparsity by taking advantage of
structural information more extensively, which in turn ben-
eﬁts the recommendation accuracy.

7. CONCLUSION

In this paper, we address the demands for high-quality rec-
ommendation on both implicit and explicit feedback datasets.
We reconstruct the user-item matrix by fully exploiting the
similarity information between users and items concurrently.
Moreover, the reconstructed data matrix also respects the
manifold structure of the user-item matrix. We conduct a
comprehensive set of experiments and compare our method
with other state-of-the-art Top-N recommendation algorithms.
The results demonstrate that the proposed algorithm works
eﬀectively. Due to the simplicity of our model, there is much
room to improve. For instance, our model can be easily ex-
tended to include side information (e.g., user demographic
information, item’s genre, social trust network) by utilizing
the graph representation. In some cases, external informa-
tion is more informative than the neighborhood information.

8. ACKNOWLEDGMENTS

This work is supported by the U.S. National Science Foun-
dation under Grant IIS 1218712, National Natural Science
Foundation of China under grant 11241005, and Shanxi Schol-
arship Council of China 2015-093. Q. Cheng is the corre-
sponding author.

9. REFERENCES
[1] G. Adomavicius and A. Tuzhilin. Toward the next

generation of recommender systems: A survey of the
state-of-the-art and possible extensions. Knowledge
and Data Engineering, IEEE Transactions on,
17(6):734–749, 2005.

[2] P. Benner, R.-C. Li, and N. Truhar. On the adi
method for sylvester equations. Journal of
Computational and Applied Mathematics,
233(4):1035–1045, 2009.

[3] F. Cacheda, V. Carneiro, D. Fern´andez, and

V. Formoso. Comparison of collaborative ﬁltering
algorithms: Limitations of current techniques and
proposals for scalable, high-performance recommender
systems. ACM Transactions on the Web, 5(1):2, 2011.

[4] D. Cai, X. He, J. Han, and T. S. Huang. Graph

regularized nonnegative matrix factorization for data
representation. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, 33(8):1548–1560,
2011.

[5] P. Cremonesi, Y. Koren, and R. Turrin. Performance
of recommender algorithms on top-n recommendation
tasks. In RecSys, pages 39–46. ACM, 2010.

[6] M. Deshpande and G. Karypis. Item-based top-n

recommendation algorithms. ACM Transactions on
Information Systems (TOIS), 22(1):143–177, 2004.

[7] Q. Gu, J. Zhou, and C. H. Ding. Collaborative

ﬁltering: Weighted nonnegative matrix factorization
incorporating user and item graphs. In SDM, pages
199–210. SIAM, 2010.

(a) FilmTrust

(b) FilmTrust

(c) Yahoo

(d) Yahoo

Figure 2: Inﬂuence of α and β on HR and ARHR.

Top-N Recommendation on Graphs

Zhao Kang, Chong Peng, Ming Yang, Qiang Cheng
Department of Computer Science, Southern Illinois University, Carbondale, IL, USA
{Zhao.Kang, pchong, ming.yang, qcheng}@siu.edu

6
1
0
2
 
p
e
S
 
7
2
 
 
]

R

I
.
s
c
[
 
 
1
v
4
6
2
8
0
.
9
0
6
1
:
v
i
X
r
a

ABSTRACT
Recommender systems play an increasingly important role
in online applications to help users ﬁnd what they need or
prefer. Collaborative ﬁltering algorithms that generate pre-
dictions by analyzing the user-item rating matrix perform
poorly when the matrix is sparse. To alleviate this problem,
this paper proposes a simple recommendation algorithm that
fully exploits the similarity information among users and
items and intrinsic structural information of the user-item
matrix. The proposed method constructs a new represen-
tation which preserves aﬃnity and structure information in
the user-item rating matrix and then performs recommenda-
tion task. To capture proximity information about users and
items, two graphs are constructed. Manifold learning idea
is used to constrain the new representation to be smooth on
these graphs, so as to enforce users and item proximities.
Our model is formulated as a convex optimization problem,
for which we need to solve the well known Sylvester equa-
tion only. We carry out extensive empirical evaluations on
six benchmark datasets to show the eﬀectiveness of this ap-
proach.

Keywords
top-N recommendation; laplacian graph; collaborative ﬁlter-
ing

1.

INTRODUCTION

Recommender systems have become increasingly indis-
pensable in many applications [1]. Collaborative ﬁltering
(CF) based methods are a fundamental building block in
many recommender systems. CF based recommender sys-
tems predict the ratings of items to be given by a user based
on the ratings of the items previously rated by other users
who are most similar to the target user.

CF based methods can be classiﬁed into memory-based
methods [21] and model-based methods [14, 15]. The for-
mer includes two popular methods, user-oriented [9] and
item-oriented, e.g., ItemKNN [6], depending on whether the

ACM ISBN 978-1-4503-2138-9.
DOI: 10.1145/1235

neighborhood information is derived from similar users or
items. First they compute similarities between the active
item and other items, or between the active user and other
users. Then they predict the unknown rating by combining
the known rating of top k neighbors. Due to the simplicity
of memory-based CF, it has been successfully applied in in-
dustry. However, it suﬀers from several problems, including
data sparsity, cold start and data correlation [3], as users
typically rate only a small portion of the available items,
and they also tend to rate similar items closely. Therefore,
the similarities between users or items cannot be accurately
obtained with the existing similarity measures such as co-
sine and Pearson correlation, which in turn compromises the
recommendation accuracy.

To alleviate the problems of memory-based methods, many
model-based methods have been proposed, which use ob-
served ratings to learn a predictive model. Among them,
matrix factorization (MF) based models, e.g., PureSVD [5]
and weighted regularized MF (WRMF) [10], are very pop-
ular due to their capability of capturing the implicit rela-
tionships among items and their outstanding performance.
Nevertheless, it introduces high computational complexity
and also faces the problem of uninterpretable recommenda-
tions. Because the rating matrix is sparse, the factorization
of the user-item matrix may lead to inferior solutions [7].
By learning an aggregation coeﬃcient matrix [13], recently,
sparse linear method (SLIM) [17] has been proposed and
shown to be eﬀective. However, it just captures relations
between items that have been co-purchased/co-rated by at
least one user [12]. Moreover, it only explores the linear rela-
tions between items. Another class of methods use Bayesian
personalized ranking (BPR) criterion to measure the diﬀer-
ence between the rankings of user-purchased items and the
remaining items. For instance, BPRMF and BPRKNN [19]
have been demonstrated to be eﬀective for implicit feedback
datasets.

In this paper, we propose a novel Top-N recommendation
model based on graphs. This method not only takes into
account the neighborhood information, which is encoded by
our user graph and item graph, but also reveals hidden struc-
ture in the data by deploying graph regularization. In the
real world, data often reside on low-dimensional manifolds
embedded in a high-dimensional ambient space. Like the
Netﬂix Prize problem, where the size of the user-item matrix
can be huge, there exist relationships between users (such
as their age, hobbies, education, etc.) and movies (such as
their genre, release year, actors, origin country, etc.). More-
over, people sharing the same tastes for a class of movies

are likely to rate them similarly. As a result, the rows and
columns of the user-item matrix possess important struc-
tural information, which should be taken advantage of in
actual applications.

To preserve local geometric and discriminating structures
embedded in a high-dimensional space, numerous manifold
learning methods, such as locally linear embedding (LLE)
[20], locality preserving projection (LPP) [18], have been
proposed. In recent years, graph regularization based non-
negative matrix factorization [4] of data representation has
been developed to remedy the failure in representing geo-
metric structures in data. Inspired by this observation, to
comprehensively consider the associations between the users
and items and the local manifold structures of the user-item
data space, we propose to apply both user and item graph
regularizations. Unlike many existing recommendation al-
gorithms, we ﬁrst establish a new representation which is
infused with the above information. It turns out that this
new representation is not sparse anymore. Therefore, we
perform recommendation task with this novel representa-
tion.

2. DEFINITIONS AND NOTATIONS

Let U = {u1, u2, ..., um} and T = {t1, t2, ..., tn} represent
the sets of all users and all items, respectively. The whole
set of user-item purchases/ratings are represented by the
user-item matrix X of size m × n. Element xij is 1 or a
positive value if user ui has ever purchased/rated item tj,
otherwise it is marked as 0. The i-th row of X denotes the
purchase/rating history of user ui on all items. The j-th
column of X is the purchase/rating history of all users on
item tj. (cid:107)X(cid:107)2
ij is the squared Frobenius norm
of X. Tr(·) stands for the trace operator. I denotes the
identity matrix. (cid:12) is the Hadamard product.

F = (cid:80)

j x2

(cid:80)

i

3. PROPOSED MODEL

3.1 User and Item Graphs

User-item rating matrix is an overﬁt representation of user
tastes and item descriptions. This leads to problems of syn-
onymy, computational complexity, and potentially poorer
results. Therefore, a more compact representation of user
tastes and item descriptions is preferred. Graph regulariza-
tion is eﬀective in preserving local geometric and discrim-
inating structures embedded in a high-dimensional space.
It is based on the well known manifold assumption [18]: If
two data points such as xi and xj are close in the geodesic
distance on the data manifold, then their corresponding rep-
resentations yi and yj are also close to each other. In prac-
tice, it is diﬃcult to accurately estimate the global mani-
fold structure of the data due to the insuﬃcient number of
samples and the high dimensionality of the ambient space.
Therefore, many methods resort to local manifold struc-
tures. Much eﬀort on manifold learning [18] has shown that
local geometric structures of the data manifold can be eﬀec-
tively modeled through a nearest neighbor graph on sampled
data points.

We adopt graph regularization to incorporate user and
item proximities. In this paper, we construct two graphs:
the user graph and the item graph. We assume that users
having similar tastes for items form communities in the user
graph, while items having similar appeals to users form com-

munities in the item graph. Since “birds of a feather ﬂock
together”, this assumption is plausible and turns out to in-
deed beneﬁt recommender systems substantially in our ex-
periment results. As an example for movie recommendation,
the users are the vertices of a “social graph” whose edges rep-
resent relations induced by similar tastes.

More formally, we construct an undirected weighted graph
Gc = (Vc, Ec; Sc) on items, called the item graph. The ver-
tex set Vc corresponds to items {t1, · · · , tn} with each node
ti corresponding to a data point xi which is the i-th column
of X. Symmetric adjacency matrix Sc encodes the inter-item
information, in which sij is the weight of the edge joining
vertices ti and tj and represents how strong the relationship
or similarity items ti and tj have. Ec = {eij} is the edge set
with each edge eij between nodes ti and tj associated with
a weight sij. The graph regularization on the item graph is
formulated as

n
(cid:88)

1
2

(cid:107)yi − yj (cid:107)2

2sij =

yT

i yidii −

yT

i yj sij

n
(cid:88)

n
(cid:88)

(1)

i=1

i,j=1

i,j=1
=Tr(Y DcY T ) − Tr(Y ScY T ) = Tr(Y LcY T ),
where Dc is a diagonal matrix with dii = (cid:80)n
j=1 sij, and Lc =
Dc − Sc is the graph Laplacian. To preserve the structural
information of the manifold, we want (1) to be as small as
possible.
It is apparent that minimizing (1) imposes the
smoothness of the representation coeﬃcients; i.e., if items ti
and tj are similar (with a relatively bigger sij), their low-
dimensional representations yi and yj are also close to each
other. Therefore, optimizing (1) is an attempt to ensure the
manifold assumption.

The crucial part of graph regularization is the deﬁnition
of the adjacency matrix Sc. There exist a number of dif-
ferent similarity metrics in the literature [21], e.g., cosine
similarity, Pearson correlation coeﬃcient, and adjusted co-
sine similarity. For simplicity, in our experiment, we use
cosine similarity for explicit rating datasets and Jaccard
coeﬃcient for implicit feedback datesets. For binary vari-
ables, the Jaccard coeﬃcient is a more appropriate similar-
ity metric than cosine because it is insensitive to the am-
plitudes of ratings.
It measures the fraction of users who
have interactions with both items over the number of users
who have interacted either of them. Formally, according
to cosine deﬁnition, the similarity sij between two items
ti and tj is deﬁned as sij =
, where ‘·’ denotes
the vector dot-product operation. For Jaccard coeﬃcient,
sij = |xi∩xj |
|xi∪xj | , where ∩ and ∪ represent intersection and
union operations, respectively. Likewise, by deﬁning the
user graph Gr = (Vr, Er; Sr) whose vertex set Vr corre-
sponds to users {u1, · · · , um}, we get a corresponding ex-
pression Tr(Y T LrY ). Here Lr denotes the Laplacian of Gr,
which is similarly obtained from the data points correspond-
ing to the users, that is, the rows of X.

xi·xj
(cid:107)xi(cid:107)2(cid:107)xj (cid:107)2

3.2 Model

By exploiting both user and item graphs, our proposed

model can be written as

min
Y

(cid:107)X − Y (cid:107)2

F + αTr(Y LcY T ) + βTr(Y T LrY ).

(2)

The ﬁrst term of (2) penalizes large deviations of the pre-
dictions from the given ratings. The last two terms measure
the smoothness of the predicted ratings on the graph struc-
tures and encourage the ratings of nodes with aﬃnity to be
similar. They can alleviate the data sparsity issue to some
extent. When the item neighborhood information is not

available, user neighborhood information might exist, vice
versa. The parameters α and β adjust the balance between
the reconstruction error and graph regularizations.

By setting the derivative of the objective function of (2)

with respect to Y to zero, we have

(βLr + I)Y + αY Lc = X.

(3)

Equation (3) is the well known Sylvester equation, which
costs O(m3) or O(n3) with a general solver. But in our
situation, X is usually extremely sparse, and Lr and Lc can
also be sparse, especially for large m and n, so the cost can
be O(m2) or O(n2), or sometimes even as low as O(m) or
O(n) [2]. Many packages or programs are available to solve
(3).

To use the reconstructed matrix Y to make recommenda-
tions for user ui, we just sort ui’s non-purchased/non-rated
items based on their scores in non-increasing order and rec-
ommend the top N items.

4. CONNECTION TO EXISTING WORK

To the best of our knowledge, there are very few stud-
ies on graph Laplacian in the context of recommendation
task. Graph regularized weighted nonnegative matrix fac-
torization (GWNMF) [7] was proposed to incorporate the
neighborhood information in a MF approach. It solves the
following problem

min
U,V

(cid:107)M (cid:12) (X − U V T )(cid:107)2

F + αT r(U T LrU ) + βT r(V T LcV )

(4)

s.t. U ≥ 0, V ≥ 0,

where M is an indicator matrix. U and V T are in latent
spaces, whose dimensionality is usually speciﬁed with an
additional parameter. The latent factors are generally not
obvious and might not necessarily be interpretable or intu-
itively understandable. Here (4) has to learn both user and
item representations in the latent spaces. In our approach,
we just need to learn one representation and thus the learn-
ing process is simpliﬁed. On the other hand, U and V T
are supposed to be of low dimensionality, and thus useful
information can be lost during the low-rank approximation
of X from U and V T . The encoding of graph Laplacian on
U and V T might be not accurate any more. On the con-
trary, our method can better preserve the information in
X, so it can potentially give better recommendations than
GWNMF. Moreover, it is well known that several drawbacks
exist in the MF approach, e.g., low convergence rate, many
local optimums of U and V T due to the non-convexity of (4).
In contrast, our model (2) is strongly convex, admitting a
unique, globally optimal solution.

Table 1: The datasets used in evaluation

dataset #users #items #trns

rsize

csize

density

ratings

Delicious
lastfm
BX

1300
8813
4186

4516
6038
7733

17550
332486
182057

13.50
37.7
43.49

3.89
55.07
23.54

0.29%
0.62%
0.56%

-
-
-

2071
7026
5252

1508
6769
7635

23.54
17.21
27.87

17.14
16.59
40.51

35497
116537
212772

Filmtrust
Netﬂix
Yahoo
In this table, the “#users”, “#items”, “#trns” columns represent the num-
ber of users, number of items and number of transactions, respectively,
in each dataset. The “rsize” and “csize” columns show the average num-
ber of ratings of each user and of each item, respectively, in each dataset.
Column corresponding to “density” shows the density of each dataset (i.e.,
density=#trns/(#users×#items)). The “ratings” column is the rating range
of each dataset . The ratings in FilmTrust are real values with step 0.5, while
in the other datasets are integers.

1.14%
0.24%
0.53%

0.5-4
1-5
1-5

5. EXPERIMENTAL EVALUATION

5.1 Datasets

Table 1 shows the characteristics of the datasets. Deli-
cious, lastfm and BX have only implicit feedback. In par-
ticular, Delicious was from the bookmarking and tagging
information1, in which each URL was bookmarked by at
least 3 users. Lastfm represents music artist listening in-
formation2, in which each music artist was listened to by at
least 10 users and each user listened to at least 5 artists. BX
is derived from the Book-Crossing dataset3 such that only
implicit interactions were contained and each book was read
by at least 10 users.

FilmTrust, Netﬂix and Yahoo contain multi-value ratings.
Speciﬁcally, FilmTrust is a dataset crawled from the en-
tire FilmTrust website4. The Netﬂix is derived from Netﬂix
Prize dataset5 and each user rated at least 10 movies. The
Yahoo dataset is a subset obtained from Yahoo!Movies user
ratings6. In this dataset, each user rated at least 5 movies
and each movie was rated by at least 3 users.

5.2 Evaluation Methodology

For fair comparison, we follow the dataset preparation
approach used by SLIM [17] and adopt the 5-fold cross val-
idation. For each fold, a dataset is split into training and
test sets by randomly selecting one non-zero entry for each
user and putting it in the test set, while using the rest of the
data for training. Then a ranked list of size-N items for each
user is produced. We subsequently evaluate the method by
comparing the ranked list of recommended items with the
In the following results presented in
item in the test set.
this paper, N is equal to 10 by default.

For Top-N recommendation, the most direct and mean-
ingful metrics are hit-rate (HR) and the average reciprocal
hit-rank (ARHR) [6], since the users only care if a short
recommendation list contains the items of interest or not
rather than a very long recommendation list. HR is de-
ﬁned as HR = #hits
#users , where #hits is the number of users
whose item in the testing set is contained (i.e., hit) in the
size-N recommendation list, and #users is the total number
of users. ARHR is deﬁned as: ARHR = 1
1
,
pi
where pi is the position of the i-th hit in the ranked Top-N
list. In this metric, hits that occur earlier in the ranked list
are weighted higher than those occur later, and thus ARHR
indicates how strongly an item is recommended.

(cid:80)#hits
i=1

#users

6. EXPERIMENTAL RESULTS

6.1 Top-N Recommendation Performance

We use 5-fold cross-validation to choose parameters for
all competing methods and report their best performance in
Table 2. It can be seen that the HR improvements achieved
by our method against the next best performing scheme
(i.e., SLIM) are quite substantial on lastfm, Yahoo, BX,
FilmTrust datasets7. For Delicious and Netﬂix datasets, our

1http://www.delicious.com
2 http://www.last.fm
3http://www.informatik.uni-freiburg.de/ cziegler/BX/
4http://www.librec.net/datasets.html
5http://www.netﬂixprize.com/
6http://webscope.sandbox.yahoo.com/catalog.php?datatype=r
7Code is available at https://github.com/sckangz/CIKM16

Table 2: Comparison of Top-N recommendation algorithms

method
ItemKNN 300
PureSVD 1000
WRMF
250
BPRKNN 1e-4
300
BPRMF
10
GWNMF
10
SLIM
0.01
Our

method
ItemKNN 400
PureSVD 3000
WRMF
400
BPRKNN 1e-3
400
BPRMF
10
GWNMF
20
SLIM
0.01
Our

params
-
10
5
0.01
0.1
5
1
0.001

params
-
10
5
0.01
0.1
100
0.5
0.01

Delicious

HR
0.300
0.285
0.330
0.326
0.335
0.340
0.343
0.345

HR
0.045
0.043
0.047
0.047
0.048
0.049
0.050
0.060

-
-
-
-
-
10
-
-

-
-
-
-
-
-
-
-
BX

-
-
-
-
-
5
-
-

-
-
-
-
-
-
-
-
Netﬂix

ARHR
0.179
0.172
0.198
0.187
0.183
0.211
0.213
0.194

ARHR
0.026
0.023
0.027
0.028
0.027
0.027
0.029
0.030

100
200
100
1e-4
100
20
5
40

5
20
30
4e-3
100
15
10
1e-4

params
-
10
3
0.01
0.1
50
0.5
10

params
-
10
2
1e-3
0.5
5
15
1e-5

-
-
-
-
-
2
-
-

lastfm

HR
0.125
0.134
0.138
0.145
0.129
0.137
0.141
0.198

-
-
-
-
-
-
-
-
-
-
-
10
-
-
-
-
FilmTrust

HR
0.583
0.601
0.604
0.625
0.610
0.615
0.628
0.651

-
-
-
-
-
-
-
-
Yahoo

ARHR
0.075
0.078
0.078
0.083
0.073
0.077
0.082
0.085

ARHR
0.352
0.369
0.371
0.391
0.375
0.380
0.397
0.405

params
-
10
5
0.01
0.1
30
1.0
0.01

method
ARHR
0.085
ItemKNN 200
0.089
PureSVD 500
0.095
WRMF
300
0.090
BPRKNN 2e-3
0.072
300
BPRMF
0.080
5
GWNMF
0.098
5
SLIM
Our
0.078
0.02
1 The parameters for each method in the table are as follows: ItemKNN: the number of neighbors k; PureSVD: the number
of singular values and the number of SVD; WRMF: the latent space’s dimension and the weight on purchases; BPRKNN: its
learning rate and regularization parameter λ; BPRMF: the latent space’s dimension and learning rate; GWNMF: dimension of
the latent space, λ, and µ; SLIM: the l2-norm regularization coefficient β and the l1 -norm regularization parameter λ. Our:
item graph regularization parameter α and user graph regularization parameter β.

params
-
10
4
1e-3
0.1
200
1
5e-6

ARHR
0.185
0.118
0.128
0.182
0.180
0.183
0.187
0.194

HR
0.156
0.158
0.172
0.165
0.140
0.169
0.173
0.166

HR
0.318
0.210
0.250
0.310
0.308
0.313
0.320
0.379

300
2000
100
0.02
300
20
10
5e-3

-
-
-
-
-
50
-
-

-
-
-
-
-
50
-
-

-
-
-
-
-
-
-
-

-
-
-
-
-
-
-
-

(a) Delicious

(b) lastfm

(c) BX

(d) FilmTrust

(e) Netﬂix

(f) Yahoo

Figure 1: Performance versus Diﬀerent Values of N .

performance is close to the best performance of other meth-
ods. In most cases, there is no much diﬀerence among other

state-of-the-art methods in terms of HR. Figure 1 shows the
performance in HR of various methods for diﬀerent values of

N (i.e., 5, 10, 15, 20 and 25) on all six datasets. Our method
works the best in most cases.

6.2 Parameter Effects

Our model involves two trade-oﬀ parameters α and β,
which dictate how strongly item and user neighborhoods
and structure information contribute to the objective and
performance. In Figure 2, we depict the eﬀects of diﬀerent
α and β values on HR and ARHR for dataset FilmTrust
and Yahoo. The search for α ranges from 1e-6 to 1e-2 with
points from {1e-6, 1e-5, 1e-4, 1e-3, 1e-2}, the search points
for β are from {1e-6, 1e-4, 1e-2}. As can be seen from all
ﬁgures, our algorithm performs well over a wide range of α
and β values. HR and ARHR share the same trend with
varying α and β. Speciﬁcally, when α is small, HR and
ARHR both increase with α. After a certain point, they
begin to decrease. For FilmTrust, the performance with
β = 0.01 is very stable with respect to α. This suggests that
user-user similarity dominates the FilmTrust dataset.

6.3 Matrix Reconstruction

To show how our method reconstructs the user-item ma-
trix, we compare it with the method of next best perfor-
mance, SLIM, on FilmTrust. The density of FilmTrust is
1.14% and the mean for those non-zero elements is 2.998.
The reconstructed matrix ˆXSLIM from SLIM has a density
of 83.21%. For those 1.14% non-zero entries in X, ˆXSLIM
recovers 99.69% of them and their mean value is 1.686. In
contrast, the reconstructed matrix by our proposed algo-
rithm has a density of 91.7%. For those 1.14% non-zero en-
tries in X, our method recovers all of them with a mean of
2.975. These facts suggest that our method better recovers
X than SLIM. In other words, SLIM loses too much infor-
mation. This appears to explain the superior performance
of our method.

In fact, above analysis is equivalent to the two widely used
prediction accuracy metrics: Mean Absolute Error (MAE)
and Root Mean Squared Error (RMSE). Since our method
can recover the original ratings much better than SLIM, our
algorithm gives lower MAE and RMSE. This conclusion is
consistent with our HR and ARHR evaluation.

Table 3: Results with Diﬀerent Similarity Metrics

Datasets
Delicious
lastfm
BX

HR
Jaccard Cosine
0.330
0.345
0.204
0.198
0.057
0.060

ARHR
Jaccard Cosine
0.202
0.087
0.024

0.194
0.085
0.030

Table 4: Results by using diﬀerent graphs

FilmTrust
Yahoo

User Graph
0.638
0.303

0.625
0.379

Item Graph User-Item Graph

0.651
0.379

6.4 Graph Construction

Figure 3: Inﬂuence of neighborhood size k on
recommendation accuracy for FilmTrust dataset.

As we discussed previously, similarity is an important in-
In many recommendation
gredient of graph construction.
algorithms, the similarity computation is crucial to the rec-
ommendation quality [8]. To demonstrate the importance
of similarity metric, we use the cosine measure rather than
the Jaccard coeﬃcient to measure the similarity in binary
datasets. We compare the results in Table 3. As demon-
strated, for lastfm dataset, HR and ARHR increase after
we adopt the cosine similarity. However, for Delicious and
BX dataset, the Jaccard coeﬃcient works better. Therefore,
the diﬀerence of ﬁnal results can be big for certain datasets
with diﬀerent similarity measures. We expect that the ex-
perimental results in Table 2 can be further enhanced if one
performs a more careful analysis of Lr and Lc. For example,
it has been reported that normalizing the similarity scores
can improve the performance [6]. Also, a number of new
similarity metrics have recently been proposed, e.g., [16],
which may be also exploited.

Another important parameter is the neighborhood size
k, which cannot be known a priori [11]. For some small
datasets, setting a small k may not include all useful neigh-
bors and would infer incomplete relationships. In practice,
a large number of ratings from similar users or similar items
are not available, due to the sparsity inherent to rating data.
We just use the fully connected graph in our experiments.
To demonstrate this, we test the eﬀects of neighborhood size
k with values {10, 50, 200, 500, 800} on FilmTrust data. As
can be seen from Figure 3, the neighborhood size indeed in-
ﬂuences the performance of our proposed recommendation
method. Speciﬁcally, the performance keeps increasing as k
increases when k is small compared to the size of dataset,
then the performance keeps almost the same as the ﬁnal
accuracy obtained in Table 2 as k it becomes larger. This
conforms that a small neighborhood size can not capture all
similarity information.

6.5 Effects of User and Item Graphs

While the overall improvements are impressive, it would
be interesting to see more ﬁne-grained analysis of the im-
pact of user-user and item-item similarity graphs. We use
FilmTrust and Yahoo datasets as examples to show the ef-
fects of user and item graphs. Table 4 summarizes the HR
values obtained with user graph, item graph, and both user

[8] G. Guo, J. Zhang, and N. Yorke-Smith. A novel

bayesian similarity measure for recommender systems.
In IJCAI, pages 2619–2625. AAAI Press, 2013.
[9] J. L. Herlocker, J. A. Konstan, A. Borchers, and

J. Riedl. An algorithmic framework for performing
collaborative ﬁltering. In SIGIR, pages 230–237.
ACM, 1999.

[10] Y. Hu, Y. Koren, and C. Volinsky. Collaborative
ﬁltering for implicit feedback datasets. In ICDM,
pages 263–272. IEEE, 2008.

[11] J. Huang, F. Nie, and H. Huang. A new simplex

sparse learning model to measure data similarity for
clustering. In Proceedings of the 24th International
Conference on Artiﬁcial Intelligence, pages 3569–3575.
AAAI Press, 2015.

[12] Z. Kang and Q. Cheng. Top-n recommendation with

novel rank approximation. In SDM, pages 126–134.
SIAM, 2016.

[13] Z. Kang, C. Peng, and Q. Cheng. Robust subspace

clustering via tighter rank approximation. In CIKM,
pages 393–401. ACM, 2015.

[14] Z. Kang, C. Peng, and Q. Cheng. Top-n recommender
system via matrix completion. In Thirtieth AAAI
Conference on Artiﬁcial Intelligence, 2016.
[15] N. Koenigstein, P. Ram, and Y. Shavitt. Eﬃcient

retrieval of recommendations in a matrix factorization
framework. In CIKM, pages 535–544. ACM, 2012.
[16] H. Liu, Z. Hu, A. Mian, H. Tian, and X. Zhu. A new
user similarity model to improve the accuracy of
collaborative ﬁltering. Knowledge-Based Systems,
56:156–166, 2014.

[17] X. Ning and G. Karypis. Slim: Sparse linear methods
for top-n recommender systems. In ICDM, pages
497–506. IEEE, 2011.

[18] X. Niyogi. Locality preserving projections. In NIPS,

volume 16, page 153. MIT, 2004.

[19] S. Rendle, C. Freudenthaler, Z. Gantner, and

L. Schmidt-Thieme. Bpr: Bayesian personalized
ranking from implicit feedback. In UAI, pages
452–461. AUAI Press, 2009.

[20] S. T. Roweis and L. K. Saul. Nonlinear dimensionality

reduction by locally linear embedding. Science,
290(5500):2323–2326, 2000.

[21] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl.

Item-based collaborative ﬁltering recommendation
algorithms. In WWW, pages 285–295. ACM, 2001.

and item graph. It demonstrates that we are able to obtain
the best performance when we combine user and item graph.
Thus neighborhood information of users and items can al-
leviate the problem of data sparsity by taking advantage of
structural information more extensively, which in turn ben-
eﬁts the recommendation accuracy.

7. CONCLUSION

In this paper, we address the demands for high-quality rec-
ommendation on both implicit and explicit feedback datasets.
We reconstruct the user-item matrix by fully exploiting the
similarity information between users and items concurrently.
Moreover, the reconstructed data matrix also respects the
manifold structure of the user-item matrix. We conduct a
comprehensive set of experiments and compare our method
with other state-of-the-art Top-N recommendation algorithms.
The results demonstrate that the proposed algorithm works
eﬀectively. Due to the simplicity of our model, there is much
room to improve. For instance, our model can be easily ex-
tended to include side information (e.g., user demographic
information, item’s genre, social trust network) by utilizing
the graph representation. In some cases, external informa-
tion is more informative than the neighborhood information.

8. ACKNOWLEDGMENTS

This work is supported by the U.S. National Science Foun-
dation under Grant IIS 1218712, National Natural Science
Foundation of China under grant 11241005, and Shanxi Schol-
arship Council of China 2015-093. Q. Cheng is the corre-
sponding author.

9. REFERENCES
[1] G. Adomavicius and A. Tuzhilin. Toward the next

generation of recommender systems: A survey of the
state-of-the-art and possible extensions. Knowledge
and Data Engineering, IEEE Transactions on,
17(6):734–749, 2005.

[2] P. Benner, R.-C. Li, and N. Truhar. On the adi
method for sylvester equations. Journal of
Computational and Applied Mathematics,
233(4):1035–1045, 2009.

[3] F. Cacheda, V. Carneiro, D. Fern´andez, and

V. Formoso. Comparison of collaborative ﬁltering
algorithms: Limitations of current techniques and
proposals for scalable, high-performance recommender
systems. ACM Transactions on the Web, 5(1):2, 2011.

[4] D. Cai, X. He, J. Han, and T. S. Huang. Graph

regularized nonnegative matrix factorization for data
representation. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, 33(8):1548–1560,
2011.

[5] P. Cremonesi, Y. Koren, and R. Turrin. Performance
of recommender algorithms on top-n recommendation
tasks. In RecSys, pages 39–46. ACM, 2010.

[6] M. Deshpande and G. Karypis. Item-based top-n

recommendation algorithms. ACM Transactions on
Information Systems (TOIS), 22(1):143–177, 2004.

[7] Q. Gu, J. Zhou, and C. H. Ding. Collaborative

ﬁltering: Weighted nonnegative matrix factorization
incorporating user and item graphs. In SDM, pages
199–210. SIAM, 2010.

(a) FilmTrust

(b) FilmTrust

(c) Yahoo

(d) Yahoo

Figure 2: Inﬂuence of α and β on HR and ARHR.

