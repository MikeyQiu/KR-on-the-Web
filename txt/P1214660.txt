DICOD: Distributed Convolutional Coordinate Descent for
Convolutional Sparse Coding

Moreau Thomas 1 Oudre Laurent 2 Vayatis Nicolas 1

Abstract

In this paper, we introduce DICOD, a convolu-
tional sparse coding algorithm which builds shift
invariant representations for long signals. This
algorithm is designed to run in a distributed set-
ting, with local message passing, making it com-
It is based on coordinate
munication efﬁcient.
descent and uses locally greedy updates which
accelerate the resolution compared to greedy co-
ordinate selection. We prove the convergence
of this algorithm and highlight its computational
speed-up which is super-linear in the number of
cores used. We also provide empirical evidence
for the acceleration properties of our algorithm
compared to state-of-the-art methods.

1. Convolutional Representation for Long

Signals

Sparse coding aims at building sparse linear representations
of a data set based on a dictionary of basic elements called
atoms.
It has proven to be useful in many applications,
ranging from EEG analysis to images and audio processing
(Adler et al., 2013; Kavukcuoglu et al., 2010; Mairal et al.,
2010; Grosse et al., 2007). Convolutional sparse coding is a
specialization of this approach, focused on building sparse,
shift-invariant representations of signals. Such representa-
tions present a major interest for applications like segmen-
tation or classiﬁcation as they separate the shape and the lo-
calization of patterns in a signal. This is typically the case
for physiological signals which can be composed of recur-
rent patterns linked to speciﬁc behavior in the human body
such as the characteristic heartbeat pattern in ECG record-
ings. Depending on the context, the dictionary can either
be ﬁxed analytically (e.g. wavelets, see Mallat 2008), or

1CMLA, ENS Cachan, CNRS, Universit´e Paris-Saclay,
94235 Cachan, France 2L2TI, Universit´e Paris 13, 93430
Villetaneuse, France. Correspondence to: Moreau Thomas
<thomas.moreau@cmla.ens-cachan.fr>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

learned from the data (Bristow et al., 2013; Mairal et al.,
2010).

Several algorithms have been proposed to solve the
convolutional sparse coding.
The Fast Iterative Soft-
Thresholding Algorithm (FISTA) was adapted for convo-
lutional problems in Chalasani et al. (2013) and uses prox-
imal gradient descent to compute the representation. The
Feature Sign Search (FSS), introduced in Grosse et al.
(2007), solves at each step a quadratic subproblem for an
active set of the estimated nonzero coordinates and the
Fast Convolutional Sparse Coding (FCSC) of Bristow et al.
(2013) is based on Alternating Direction Method of Multi-
pliers (ADMM). Finally, the coordinate descent (CD) has
been extended by Kavukcuoglu et al. (2010) to solve the
convolutional sparse coding. This method greedily opti-
mizes one coordinate at each iteration using fast local up-
dates. We refer the reader to Wohlberg (2016) for a detailed
presentation of these algorithms.

To our knowledge, there is no scalable version of these al-
gorithms for long signals. This is a typical situation, for
instance, in physiological signal processing where sensor
information can be collected for a few hours with sampling
frequencies ranging from 100 to 1000Hz. The existing
algorithms for generic (cid:96)1-regularized optimization can be
accelerated by improving the computational complexity of
each iteration. A ﬁrst approach to improve the complexity
of these algorithms is to estimate the non-zero coordinates
of the optimal solution to reduce the dimension of the op-
timization space, using either screening (El Ghaoui et al.,
2012; Fercoq et al., 2015) or active-set algorithms (John-
son & Guestrin, 2015). Another possibility is to develop
parallel algorithms which compute multiple updates simul-
taneously. Recent studies have considered distributing co-
ordinate descent algorithms for general (cid:96)1-regularized min-
imization (Scherrer et al., 2012a;b; Bradley et al., 2011; Yu
et al., 2012). These papers propose synchronous algorithms
using either locks or synchronizing steps to ensure the con-
vergence in general cases. You et al. (2016) derive an asyn-
chronous distributed algorithm for the projected coordinate
descent which uses centralized communication and ﬁnely
tuned step size to ensure the convergence of their method.

In the present paper, we design a novel distributed algo-

DICOD: Distributed Convolutional Sparse Coding

rithm tailored for the convolutional problem which is based
on coordinate descent, named Distributed Convolution Co-
ordinate Descent (DICOD). DICOD is asynchronous and
each process can run independently without locks or syn-
chronization steps. This algorithm uses a local communi-
cation scheme to reduce the number messages between the
processes and does not rely on external learning rates. We
also prove that this algorithm scales super-linearly with the
number of cores compared to the sequential CD, up to cer-
tain limitations.

In Section 2, we introduce the DICOD algorithm for the
resolution of convolutional sparse coding. Then, we prove
in Section 3 that DICOD converges to the optimal solution
for a wide range of settings and we analyze its complex-
ity. Finally, Section 4 presents numerical experiments that
illustrate the beneﬁts of the DICOD algorithm with respect
to other state-of-the-art algorithms and validate our theo-
retical analysis.

2. Distributed Convolutional Coordinate

Descent (DICOD)

Notations. The space of multivariate signals of length T
in RP is denoted by X P
T . For these signals, their value
is denoted by X[t] ∈ RP and for
at time t ∈
(cid:75)
, X[t] = 000P . The indicator function of
all t /∈
(cid:75)
t0 is denoted 111t0. For any signal X ∈ X P
T , the reversed
signal is deﬁned as X (cid:30)[t] = X[T − t]T and the d-norm

(cid:74)
0, T − 1
(cid:74)

0, T − 1

(cid:16)(cid:80)T −1

(cid:17)1/d

t=0 (cid:107)X[t](cid:107)d
d

is deﬁned as (cid:107)X(cid:107)d =
L, W ∈ N∗, the convolution between Z ∈ X 1
X P
such that for t ∈

W is a multivariate signal Z ∗DDD ∈ X P
0, T − 1
(cid:75)
(cid:74)
W −1
(cid:88)

. Finally, for
L and DDD ∈
T with T =L+W −1

,

(Z ∗ DDD)[t] ∆=

(cid:104)Z[t − τ ], DDD[τ ](cid:105) .

τ =0

This section reviews in Subsection 2.1 the convolutional
sparse coding as an (cid:96)1-regularized optimization problem
and the coordinate descent algorithm to solve it. Then, Sub-
section 2.2 and Subsection 2.3 respectively introduce the
Distributed Convolutional Coordinate Descent (DICOD)
and the Locally Greedy Coordinate Descent (LGCD) algo-
rithms to efﬁciently solve convolutional sparse coding for
long signals. Finally, Subsection 2.4 discusses related work
on (cid:96)1-regularized coordinate descent algorithms.

2.1. Coordinate Descent for Convolutional Sparse

Coding

Convolutional Sparse Coding. Consider the multivari-
ate signal X ∈ X P
K patterns with W (cid:28) T and Z = {Zk}K

W be a set of
L be a set

⊂ X P
k=1 ⊂ X 1

T . Let DDD =

DDDk

(cid:111)K

k=1

(cid:110)

of K activation signals with L=T −W +1. The convolu-
tional sparse representation models a multivariate signal X
as the sum of K convolutions between a local pattern DDDk
and an activation signal Zk such that:

K
(cid:88)

k=1

X[t] =

(Zk ∗ DDDk)[t] + E[t],

∀t ∈

0, T − 1
(cid:75)

(cid:74)

, (1)

with E ∈ X P
T representing an additive noise term. This
model also assumes that the coding signals Zk are sparse,
in the sense that only few entries are nonzero in each signal.
The sparsity property forces the representation to display
localized patterns in the signal. Note that this model can be
extended to higher order signals such as images by using
the proper convolution operator. In this study, we focus on
1D-convolution for the sake of simplicity.

Given a dictionary of patterns DDD, convolutional sparse cod-
ing aims to retrieve the sparse decomposition Z ∗ associated
to the signal X by solving the following (cid:96)1-regularized op-
timization problem

Z ∗ = argmin

E(Z) ,

Z=(Z1,...ZK )

E(Z) ∆=

X −

Zk ∗ DDDk

+ λ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
2

K
(cid:88)

k=1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

where

(2)

K
(cid:88)

k=1

(cid:13)
(cid:13)Zk

(cid:13)
(cid:13)1 ,

(3)

for a given regularization parameter λ > 0 . The problem
formulation (2) can be interpreted as a special case of the
LASSO problem with a band circulant matrix. Therefore,
classical optimization techniques designed for LASSO can
easily be applied to solve it with the same convergence
guarantees. Kavukcuoglu et al. (2010) adapted the coor-
dinate descent to efﬁciently solve the convolutional sparse
coding.

Convolutional Coordinate Descent. The coordinate de-
scent is a method which updates one coordinate at each it-
eration. This type of optimization algorithms is efﬁcient for
sparse optimization problem since few coordinates need to
be updated to ﬁnd the optimal solution and the greedy se-
lection of updated coordinates is a good strategy to achieve
fast convergence to the optimal point. Algorithm 1 summa-
rizes the greedy convolutional coordinate descent.

The method proposed by Kavukcuoglu et al. (2010) itera-
tively updates at each iteration one coordinate (k0, t0) of
the coding signal Z to its optimal value Z (cid:48)
[t0] when all
k0
other coordinates are ﬁxed. A closed form solution exists
to compute the value Z (cid:48)
k0

[t0] for the update,

Z (cid:48)
k0

[t0] =

1
(cid:107)DDDk0 (cid:107)2
2

Sh(βk0[t0], λ),

(4)

with the soft thresholding operator deﬁned as

Sh(u,λ)=sign(u)max(|u|−λ,0).

DICOD: Distributed Convolutional Sparse Coding

Algorithm 1 Greedy Coordinate Descent
1: Input: DDD, X, parameter (cid:15) > 0
2: C =
×
0, L − 1
(cid:75)
(cid:75)
3: Initialization: ∀(k, t) ∈ C,
(cid:16)
DDD(cid:30)

Zk[t] = 0, βk[t] =

1, K

[t]

(cid:17)

k ∗ X

(cid:74)

(cid:74)

4: repeat

5:

6:

k[t] =

∀(k, t) ∈ C, Z (cid:48)

1
(cid:107)DDDk(cid:107)2
2
Choose (k0, t0) = arg max
|∆Zk[t]|
(k,t)∈C
Update β using (5) and Zk0[t0] ← Z (cid:48)
k0

Sh(βk[t], λ) ,

[t0]

7:
8: until |∆Zk0 [t0]| < (cid:15)

Algorithm 2 DICODM
1: Input: DDD, X, parameter (cid:15) > 0
2: In parallel for m = 1 · · · M
3: For all (k, t) in Cm, initialize βk[t] and Zk[t]
4: repeat
5:
6:
7:

Receive messages and update β with (5)
∀(k, t) ∈ Cm, compute Z (cid:48)
Choose (k0, t0) = arg max
Update β with (5) and Zk0 [t0] ← Z (cid:48)
k0
if t0 − mLM < W then

8:
9:
10:
11:
12:
13: until global convergence (cid:107)∆Z(cid:107)∞ < (cid:15)

Send (k0, t0, ∆Zk0 [t0]) to core m − 1

Send (k0, t0, ∆Zk0 [t0]) to core m + 1

if (m + 1)LM − t0 < W then

k[t] with (4)

|∆Zk[t]|

(k,t)∈Cm

[t0]

and an auxiliary variable β ∈ X K

L deﬁned as

βk[t]=

Zk(cid:48)∗DDDk(cid:48)+Zk[t]et∗DDDk




DDD(cid:30)

k∗

X−

K
(cid:88)

k(cid:48)=1








[t] ,

where et is a dirac with value 1 in t and 0 elsewhere. Note
that βk[t] is simply the residual when Zk[t] is equal to 0.

The success of this algorithm highly depends on the efﬁ-
ciency in computing this coordinate update. For problem
(2), Kavukcuoglu et al. (2010) show that if at iteration q, the
coordinate (k0, t0) of Z (q) is updated to the value Z (cid:48)
[t0],
k0
then it is possible to compute β(q+1) from β(q) using

β(q+1)
k

[t] = β(q)

k [t] − (DDD(cid:30)

k ∗ DDDk0)[t − t0]∆Z (q)
k0

[t0],

(5)

for all (k, t) (cid:54)= (k0, t0). For all t /∈
−W + 1, W − 1
,
(DDD(cid:30)
(cid:75)
k ∗ DDDk0)[t] is zero. Thus, only O(KW ) operations are
needed to maintain β up-to-date with the current estimate
Z. In the following,

(cid:74)

∆Ek0 [t0] = E(Z (q)) − E(Z (q+1))

denotes the cost variation obtained when the coordinate
(k0, t0) is replaced by its optimal value Z (cid:48)
k0

[t0].

The selection of the updated coordinate (k0, t0) can follow
different strategies. Cyclic updates (Friedman et al., 2007)
and random updates (Shalev-Shwartz & Tewari, 2009) are
efﬁcient strategies as they have a O (cid:0)1(cid:1) computational
complexity. Osher & Li (2009) propose to select the co-
ordinate greedily to maximize the cost reduction of the up-
date. In this case, the coordinate is chosen as the one with
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)∆Zk[t]
the largest difference max(k,t)
(cid:12) between its current
value Zk[t] and the value Z (cid:48)

k[t] with

∆Zk[t] = Zk[t] − Z (cid:48)

k[t]

(6)

This strategy is computationally more expensive, with a
cost of O (cid:0)KT (cid:1) but it has a better convergence rate (Nu-
tini et al., 2015). In this paper, we focus on the greedy ap-
proach as it aims to get the largest gain from each update.
Moreover, as the updates in the greedy scheme are more
complex to compute, distributing them provides a larger
speedup compared to other strategies.

The procedure is run until maxk,t |∆Zk[t]| becomes
smaller than a speciﬁed tolerance parameter (cid:15).

2.2. Distributed Convolutional Coordinate Descent

(DICOD)

For convolutional sparse coding, the coordinate descent up-
dates are only weakly dependent as it is shown in (5). It is
thus natural to parallelize it for this problem.

DICOD. Algorithm 2 describes the steps of DICOD with
M workers. Each worker m ∈
is in charge
of updating the coordinates of a segment Cm of length
LM = L/M deﬁned by:

1, M

(cid:74)

(cid:75)

(cid:26)

Cm=

(k,t) ; k∈
(cid:74)

(cid:75)

1,K

, t∈(cid:114)(m−1)LM , mLM −1(cid:122)

.

(cid:27)

for all
The local updates are performed in parallel
the cores using the greedy coordinate descent
intro-
duced in Subsection 2.1. When a core m updates
the coefﬁent (k0, t0) such that t0 ∈
(m − 1)LM +
the updated coordinates of β are all
W, mLM − W
,
(cid:75)
contained in Cm and there is no need to update β on
the update is equiva-
the other cores.
mLM −W,mLM
lent to a sequential update. When t0∈
(cid:17)
(cid:16)
(cid:75)
(cid:74)
(m−1)LM ,(m−1)LM +W
resp. t0∈
, some of the co-
(cid:74)
ordinates of β in core m + 1 (resp. m − 1) need to be up-
dated and the update is not local anymore. This can be done

In these cases,

(cid:75)

(cid:74)

DICOD: Distributed Convolutional Sparse Coding

Cm updated in (k0, t0)

Cm+1 updated in (k1, t1)

∆Zk0 [t0], k0, t0

∆Zk0

[t0]

Z

β

t0−S

t0

No message

t0+S t1−S

t1

∆Zk1

[t1]

Z

β
t1+S

Figure 1. Communication process in DICOD for two cores Cm and Cm+1. (red) The process needs to send a message to its neighbor
as it updates a coordinate with t0 located near the border of the core’s segment, in the interference zone. (green) The update in t1 is
independent of other cores.

by sending the position of updated coordinate (k0, t0), and
the value of the update ∆Zk0[t0] to the neighboring core.
Figure 1 illustrates this communication process.
Inter-
processes communications are very limited in DICOD. One
node communicates with its neighbors only when it updates
coordinates close to the extremity of its segment. When the
size of the segment is reasonably large compared to the size
of the patterns, only a small part of the iterations needs to
send messages. We cannot apply the stopping criterion of
CD in each worker of DICOD, as this criterion might not
be reached globally. The updates in the neighbor cores can
break this criterion. To avoid this issue, the convergence is
considered to be reached once all the cores achieve this cri-
terion simultaneously. Workers that reach this state locally
are paused, waiting for incoming communication or for the
global convergence to be reached.

The key point that allows distributing the convolutional co-
ordinate descent algorithm is that the solutions on time seg-
ments that are not overlapping are only weakly dependent.
Equation (5) shows that a local change has impact on a seg-
ment of length 2W − 1 centered around the updated coor-
dinate. Thus, if two coordinates which are far enough were
updated simultaneously, the resulting point Z is the same
as if these two coordinates had been updated sequentially.
By splitting the signal into continuous segments over mul-
tiple cores, coordinates can be updated independently on
each core up to certain limits.

(k0, t0)

Interferences. When two coordinates
and
(k1, t1) are updated by two neighboring cores simultane-
ously, the updates might not be independent and cannot be
considered to be sequential. The local version of β used
for the second update does not account for the ﬁrst update.
We say that the updates are interfering. The cost reduction
resulting from these two updates is denoted ∆Ek0,k1 [t0, t1]
and simple computations, detailed in Proposition A.2,
show that

iterative steps
(cid:122)
(cid:123)
(cid:125)(cid:124)
∆Ek0 [t0] + ∆Ek1[t1]

∆Ek0,k1[t0, t1] =
− (DDD(cid:30)
k1
(cid:124)

∗ DDDk0 )[t1 − t0]∆Zk0[t0]∆Zk1 [t1]
,
(cid:125)

(cid:123)(cid:122)
interference

(7)

If |t1 − t0| ≥ W , then (DDD(cid:30)
∗DDDk0)[t1 − t0] = 0 and the up-
k1
dates can be considered to be sequential as the interference
term is zero. When |t1 − t0| < W , the interference term
does not vanish but Section 3 shows that under mild as-
sumption, this term can be controlled and it does not make
the algorithm diverge.

2.3. Locally Greedy Coordinate Descent (LGCD)

The theoretical analysis in Theorem 3 shows that DICOD
provides a super-linear acceleration compared to the greedy
coordinate descent. This result is supported with the nu-
merical experiment presented in Figure 4. The super-linear
speed up results from a double acceleration, provided by
the parallelization of the updates – we update M coordi-
nates at each iteration – and also by the reduction of the
complexity of each iteration. Indeed, each core computes
greedy updates with linear in complexity on 1/M -th of the
signal. This super-linear speed-up means that running DI-
COD sequentially will still provide a speed-up compared
to the greedy coordinate descent algorithm.

Algorithm 3 presents LGCD. This algorithm is similar to
a sequential version of DICOD. At each step, one segment
Cm is selected uniformly at random between the M seg-
ments. The greedy coordinate descent algorithm is applied
locally on this segment. This update is only locally greedy
and maximizes

(k0, t0) = argmax
(k,t)∈Cm

|∆Zk[t]|

This coordinate is then updated to its optimal value Z (cid:48)
[t0].
k0
In this case, there is no interference as the segments are not
updated simultaneously.

DICOD: Distributed Convolutional Sparse Coding

Algorithm 3 Locally greedy coordinate descent LGCDM
1: Input: DDD, X, parameter (cid:15) > 0, number of segments

& Guestrin, 2015). The evaluation of the performances of
our algorithm with these strategies is left for future work.

M

2: Initialize βk[t] and Zk[t] for all (k, t) in C
3: Initialize dZm = +∞ for m ∈
4: repeat
5:
6:
7:

Randomly select m ∈
∀(k, t) ∈ Cm, compute Z (cid:48)
Choose (k0, t0) = argmax
(k,t)∈Cm

(cid:75)
k[t] with (4)
|∆Zk[t]|

1, M

1, M

(cid:74)

(cid:75)

(cid:74)

8:
9:

Update β with (5)
Update estimate Zk0[t0](q+1) ← Z (cid:48)
[t0]
k0
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)Zk0 [t0](q+1) − Z (cid:48)
[t0]
Update dZm =
10:
(cid:12)
11: until (cid:107)dZ(cid:107)∞ < (cid:15) and (cid:107)∆Z(cid:107)∞ < (cid:15)

k0

Note that if M = T , this algorithm becomes very close
to the randomized coordinate descent. The coordinate is
selected greedily only between the K different channels of
the signal Z at the selected time. So the selection of M
depends on a tradeoff between the randomized coordinate
descent and the greedy coordinate descent.

2.4. Discussion

This algorithm differs from the existing paradigm to dis-
tribute CD (Scherrer et al., 2012a;b; Bradley et al., 2011;
Yu et al., 2012; You et al., 2016) as it does not rely on
centralized communication. Indeed, other parallel coordi-
nate descent algorithms rely on a parameter server, which
is an extra worker that holds the current value of Z. As
the size of the problem and the number of nodes grow, the
communication cost can rapidly become an issue with this
kind of centralized communication. The natural workload
split proposed with DICOD allows for more efﬁcient in-
teractions between the workers and reduces the need for
inter-node communications. Moreover, to prevent the in-
terferences breaking the convergence, existing algorithms
rely either on synchronous updates (Bradley et al., 2011;
Yu et al., 2012) or on reduced step size in the updates (You
et al., 2016; Scherrer et al., 2012a). In both case, they are
less efﬁcient than our asynchronous greedy algorithm that
can leverage the convolutional structure of the problem to
use both large updates and independent processes without
external parameters.

As seen in the introduction, another way to improve the
computational complexity of sparse coding algorithms is
to estimate the non-zero coordinates of the optimal solution
in order to reduce the dimension of the optimization space.
As this research direction is orthogonal to the paralleliza-
tion of the coordinate descent, it would be possible to com-
bine our algorithm with either screening (El Ghaoui et al.,
2012; Fercoq et al., 2015) or active-set methods (Johnson

3. Properties of DICOD

Convergence of DICOD. The magnitude of the interfer-
ence is related to the value of the cross-correlation between
dictionary elements, as shown in Proposition 1. Thus, when
the interferences have low probability and small magni-
tude, the distributed algorithm behaves as if the updates
were applied sequentially, resulting in a large acceleration
compared to the sequential CD algorithm.

Proposition 1. For concurrent updates for coordinates
(k0, t0) and (k1, t1) of a sparse code Z, the cost update
∆Ek0k1[t0, t1] is lower bounded by

∆Ek0,k1 [t0, t1] ≥ ∆Ek0[t0] + ∆Ek1[t1]
∗ DDDk1)[t0 − t1]

(DDD(cid:30)
k0
(cid:107)DDDk0(cid:107)2(cid:107)DDDk1 (cid:107)2

− 2

(cid:112)∆Ek0[t0]∆Ek1[t1].

(8)

The proof of this proposition is given in Appendix C.1. It
relies on the (cid:107)DDDk(cid:107)2
2-strong convexity of (4), which gives
for all Z. Using this inequality

2∆Ek[t](Z)

√

|∆Zk[t]| ≤
with (7) yields the result.

(cid:107)DDDk(cid:107)2

This proposition controls the magnitude of the interfer-
ence using the cost reduction associated to a single update.
When the correlations between the different elements of the
dictionary are small enough, the interfering update does not
increase the cost function. The updates are less efﬁcient but
do not worsen the current estimate. Using this control on
the interferences, we can prove the convergence of DICOD.

Theorem 2. Consider the following hypotheses,

H1. For all (k0,t0),(k1,t1) such that for all t0 (cid:54)= t1,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∗ DDDk1)[t0 − t1]

(DDD(cid:30)
k0
(cid:107)DDDk0(cid:107)2(cid:107)DDDk1(cid:107)2

< 1 .

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

H2. There exists A ∈ N∗ such that all cores m ∈
(cid:75)
are updated at least once between iteration i and i + A if
the solution is not locally optimal.

1, M
(cid:74)

H3. The delay in communication between the processes is
inferior to the update time.

Under (H1)-(H2)-(H3), the DICOD algorithm converges
to the optimal solution Z ∗ of (2).

Assumption (H1) is satisﬁed as long as the dictionary el-
ements are not replicated in shifted positions in the dictio-
nary. It ensures that the cost is updated in the right direction

DICOD: Distributed Convolutional Sparse Coding

at each step. This assumption can be linked to the shifted
mutual coherence introduced in Papyan et al. (2016).

Hypothesis (H2) ensures that all coordinates are updated
regularly if they are not already optimal. This analysis is
not valid when one of the cores fails. As only one core is
responsible for the update of a local segment, if a worker
fails, this segment cannot be updated anymore and thus the
algorithm will not converge to the optimal solution.

Finally, under (H3), an interference only results from one
update on each core. Multiple interferences occur when a
core updates multiple coordinates in the border of its seg-
ment before receiving the communication from other pro-
cesses border updates. When T (cid:29) W , the probability of
multiple interference is low and this hypothesis can be re-
laxed if the updates are not concentrated on the borders.

Proof sketch for Theorem 2. The full proof can be found
in Appendix C.2. The main argument in proving the con-
vergence is to show that most of the updates can be con-
sidered sequentially and that the remaining updates do
not increase the cost of the current point. By (H3), for
a given iteration, a core can interfere with at most one
other core. Thus, without loss of generality, we can
consider that at each step q,
the variation of the cost
E is either ∆Ek0[t0](Z (q)) or ∆Ek0k1[t0, t1](Z (q)), for
some (k0, t0), (k1, t1) ∈
1, K
. Proposi-
tion 1 and (H1) proves that ∆Ek0k1[t0, t1](Z (q)) ≥ 0.
For a single update ∆Ek0[t0](Z (q)), the update is equiva-
lent to a sequential update in CD, with the coordinate cho-
sen randomly between the best in each segments. Thus,
∆Ek0 [t0](Z (q)) > 0 and the convergence is eventually
proved using results from Osher & Li (2009).

0, T − 1
(cid:75)

×

(cid:75)

(cid:74)

(cid:74)

Speedup of DICOD. We denote Scd(M ) the speedup of
DICOD compared to the sequential greedy CD. This quan-
tiﬁes the number of iterations that can be run by DICOD
during one iteration of CD.
Theorem 3. Let α = W
4 and if
the non-zero coordinates of the sparse code are distributed
the expected speedup E[Scd(M )] is
uniformly in time,
lower bounded by

T and M ∈ N∗ . If αM < 1

E[Scd(M )] ≥ M 2(1 − 2α2M 2 (cid:16)

1 + 2α2M 2(cid:17) M

2 −1

) .

Proof sketch for Theorem 3. The full proof can be found
in Appendix D. There are two aspects involved in DI-
COD speedup: the computational complexity and the ac-
celeration due to the parallel updates. As stated in Subsec-
tion 2.1, the complexity of each iteration for CD is linear
with the length of the input signal T . In DICOD, each core
runs on a segment of size T
M . This accelerates the execu-
tion of individual updates by a factor M . Moreover, all
the cores compute their update simultaneously. The up-
dates without interference are equivalent to sequential up-
dates. Interfering updates happen with probability (cid:0)M α(cid:1)2
and do not increase the cost. Thus, one iteration of DI-
COD with Ni interferences provides a cost variation equiv-
alent to M − 2Ni iterations using sequential CD and, in
expectation, it is equivalent to M − 2E[Ni] iterations of
DICOD. The probability of interference depends on the ra-
tio between the length of the segments used for each core
and the size of the dictionary. If all the updates are spread
uniformly on each segment, the probability of interference

. The expected
between 2 neighboring cores is
number of interference E[Ni] can be upper bounded using
this probability and this yields the desired result.

(cid:17)2

(cid:16) MW
T

The overall speedup of DICOD is super-linear compared to
sequential greedy CD for the regime where αM (cid:28) 1. It
is almost quadratic for small M but as M grows, there is
a sharp transition that signiﬁcantly deteriorates the accel-
eration provided by DICOD. Section 4 empirically high-
lights this behavior. For a given α, it is possible to approxi-
mate the optimal number of cores M to solve convolutional
sparse coding problems.

Note that this super-linear speed up is due to the fact that
CD is inefﬁcient for long signals, as its iterations are com-
putationally too expensive to be competitive with the other
methods. The fact that we have a super-linear speed-up
means that running DICOD sequentially will provide an
acceleration compared to CD (see Subsection 2.3). For the
sequential run of DICOD, called LGCD, we have a linear
speed-up in comparison to CD, when M is small enough.
Indeed, the iteration cost is divided by M as we only need
to ﬁnd the maximal update on a local segment of size T
M .
When increasing M over T
W , the iteration cost does not de-
crease anymore as updating β costs O (cid:0)KW (cid:1) and ﬁnding
the best coordinate has the same complexity.

This result can be simpliﬁed when the interference proba-
bility (αM )2 is small.
Corollary 4. The expected speedup E[Scd(M )] when
M α → 0 is such that

E[Scd(M )] (cid:38)
α→0

M 2(1 − 2α2M 2 + O(α4M 4)) .

4. Numerical Results

All the numerical experiments are run on ﬁve Linux ma-
chines with 16 to 24 Intel Xeon 2.70 GHz processors and
at least 64 GB of RAM on local network. We use a combi-
nation of Python, C++ and the OpenMPI 1.6 for the algo-
rithms implementation. The code to reproduce the ﬁgures

DICOD: Distributed Convolutional Sparse Coding

Figure 2. Evolution of the loss function for DICOD, LGCD, CD,
FCSC and FISTA while solving sparse coding for a signal gen-
erated with default parameters relatively to the number of itera-
tions.

Figure 3. Evolution of the loss function for DICOD, LGCD, CD,
FCSC and FISTA while solving sparse coding for a signal gener-
ated with default parameters, relatively to time. This highlights
the speed of the algorithm on the given problem.

is available online1. The run time denotes the time for the
system to run the full algorithm pipeline, from cold start
and includes for instance the time to start the sub-processes.
The convergence refers to the variation of the cost with the
number of iterations and the speed to the variation of the
cost relative to time.

Long convolutional Sparse Coding Signals. To further
validate our algorithm, we generate signals and test the per-
formances of DICOD compared to state-of-the-art methods
proposed to solve convolutional sparse coding. We gen-
erate a signal X of length T in RP following the model
described in (1). The K dictionary atoms DDDk of length
W are drawn as a generic dictionary. First, each entry is
sampled from a Gaussian distribution. Then, each pattern
is normalized such that (cid:107)DDDk(cid:107)2 = 1. The sparse code en-
tries are drawn from a Bernoulli-Gaussian distribution with
Bernoulli parameter ρ = 0.007, mean 0 and standard vari-
ation σ = 10 . The noise term E is chosen as a Gaus-
sian white noise with variance 1. The default values for
the dimensions are set to W = 200, K = 25, P = 7,
T = 600 × W and we used λ = 1.

Algorithms Comparison. DICOD is compared to the
main state-of-the-art optimization algorithms for convo-
lutional sparse coding: Fast Convolutional Sparse Cod-
ing (FCSC) from Bristow et al. (2013), Fast Iterative Soft
Thresholding Algorithm (FISTA) using Fourier domain
computation as described in Wohlberg (2016), the greedy
convolutional coordinate descent (CD, Kavukcuoglu et al.
2010) and the randomized coordinate descent (RCD, Nes-
terov 2010). All the speciﬁc parameters for these algo-
rithms are ﬁxed based on the authors’ recommendations.

1Code available at github.com/tomMoral/dicod

DICODM denotes the DICOD algorithm run using M
cores. We also include LGCDM , for M ∈ {60, 600}, the
sequential run of the DICOD algorithm using M segments,
as described in Algorithm 3.

Figure 2 shows that the evolution of the performances of
LGCD relatively to the iterations are very close to the per-
formances of CD. The difference between these two algo-
rithms is that the updates are only locally greedy in LGCD.
As there is little difference visible between the two curves,
this means that in this case, the computed updates are es-
sentially the same. The differences are larger for LGCD600,
as the choice of coordinates are more localized in this case.
The performance of DICOD60 and DICOD30 are also close
to the iteration-wise performances of CD and LGCD. The
small differences between DICOD and LGCD result from
the iterations where there are interferences. Indeed, if two
iterations interfere, the cost does not decrease as much as
if the iterations were done sequentially. Thus, it requires
more steps to reach the same accuracy with DICOD60 than
with LGCD and with DICOD30, as there are more inter-
ferences when the number of cores M increases. This ex-
plains the discrepancy in the decrease of the cost around
the iteration 105. However, the number of extra steps re-
quired is quite low compared to the total number of steps
and the performances are mostly not affected by the inter-
ferences. The performances of RCD in terms of iterations
are much slower than the greedy methods. Indeed, as only a
few coordinates are useful, it takes many iterations to draw
them randomly.
In comparison, the greedy methods are
focused on the coordinates which largely divert from their
optimal value, and are thus most likely to be important. An-
other observation is that the performance in term of number
of iterations of the global methods FCSC and FISTA are
much better than the methods based on local updates. As

DICOD: Distributed Convolutional Sparse Coding

each iteration can update all the coordinates for FISTA, the
number of iterations needed to reach the optimal solution
is indeed smaller than for CD, where only one coordinate
is updated at a time.

In Figure 3, the speed of theses algorithms can be ob-
served. Even though it needs many more iterations to con-
verge, the randomized coordinate descent is faster than the
greedy coordinate descent. Indeed, for very long signals,
the iteration complexity of greedy CD is prohibitive. How-
ever, using the locally greedy updates, with LGCD60 and
LGCD600, the greedy algorithm can be made more efﬁ-
cient. LGCD600 is also faster than the other state-of-the-art
algorithms FISTA and FCSC. The choice of M = 600 is
a good tradeoff for LGCD as it means that the segments
are of the size of the dictionary W . With this choice for
M = T
W , the computational complexity of choosing a co-
ordinate is O (cid:0)KW (cid:1) and the complexity of maintaining β
is also O (cid:0)KW (cid:1). Thus, the iterations of this algorithm have
the same complexity as RCD but are more efﬁcient.

The distributed algorithm DICOD is faster compared to all
the other sequential algorithms and the speed up increases
with the number of cores. Also, DICOD has a shorter ini-
tialization time compared to the other algorithms. The ﬁrst
point in each curve indicates the time taken by the initial-
ization. For all the other methods, the computations for
constants – necessary to accelerate the iterations – have a
computational cost equivalent to the on of the gradient eval-
uation. As the segments of signal in DICOD are smaller,
the initialization time is also reduced. This shows that the
overhead of starting the cores is balanced by the reduction
of the initial computation for long signals. For shorter sig-
nals, we have observed that the initialization time is of the
same order as the other methods. The spawning overhead is
indeed constant whereas the constants are cheaper to com-
pute for small signals.

Speedup Evaluation. Figure 4 displays the speedup of
DICOD as a function of the number of cores. We used 10
generated problems for 2 signal lengths T = 150 · W and
T = 750 · W with W = 200 and we solved them using
DICODM with a number of cores M ranging from 1 to
75. The blue dots display the average running time for a
given number of workers. For both setups, the speedup is
super-linear up to the point where M α = 1
2 . For small
M the speedup is very close to quadratic and a sharp tran-
sition occurs as the number of cores grows. The verti-
cal solid green line indicates the approximate position of
the maximal speedup given in Corollary 4 and the dashed
lined is the expected theoretical run time derived from the
same expression. The transition after the maximum is very
sharp. This approximation of the speedup for small values
of M α is close to the experimental speedup observed with

Figure 4. Speedup of DICOD as a function of the number of pro-
cesses used, average over 10 run on different generated signals.
This highlights a sharp transition between a regime of quadratic
speedups and the regime where the interference are slowing down
drastically the convergence.

DICOD. The computed optimal value of M ∗ is close to the
optimal number of cores in these two examples.

5. Conclusion

In this work, we introduced an asynchronous distributed
algorithm that is able to speed up the resolution of the Con-
volutional Sparse Coding problem for long signals. This
algorithm is guaranteed to converge to the optimal solution
of (2) and scales superlinearly with the number of cores
used to distribute it. These claims are supported by numer-
ical experiments highlighting the performances of DICOD
compared to other state-of-the-art methods. Our proofs rely
extensively on the use of one dimensional convolutions. In
this setting, a process m only has two neighbors m − 1 and
m+1. This ensures that there is no high order interferences
between the updates. Our analysis does not apply straight-
forwardly to distributed computation using square patches
of images as the higher order interferences are more com-
plicated to handle. A way to apply our algorithm with these
guarantees to images is to split the signals along only one
direction, to avoid higher order interferences. The exten-
sion of our results to the grid splitting of images is an inter-
esting direction for future work.

References

Adler, A., Elad, M., Hel-Or, Y., and Rivlin, E. Sparse
Coding with Anomaly Detection. In IEEE International
Workshop on Machine Learning for Signal Processing
(MLSP), pp. 22 – 25, Southampton, United Kingdom,
2013.

DICOD: Distributed Convolutional Sparse Coding

Bradley, J. K., Kyrola, A., Bickson, D., and Guestrin,
C. Parallel Coordinate Descent for (cid:96)1-Regularized Loss
Minimization. In International Conference on Machine
Learning (ICML), pp. 321–328, Bellevue, WA, USA,
2011.

Nutini, J., Schmidt, M., Laradji, I. H., Friedlander, M. P.,
and Koepke, H. Coordinate Descent Converges Faster
with the Gauss-Southwell Rule Than Random Selec-
tion. In International Conference on Machine Learning
(ICML), pp. 1632–1641, Lille, France, 2015.

Bristow, H., Eriksson, A., and Lucey, S. Fast convolutional
sparse coding. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 391–398, Portland,
OR, USA, 2013.

Osher, S. and Li, Y. Coordinate descent optimization for (cid:96)1
minimization with application to compressed sensing; a
greedy algorithm. Inverse Problems and Imaging, 3(3):
487–503, 2009.

Papyan, V., Sulam, J., and Elad, M. Working Lo-
cally Thinking Globally - Part II: Theoretical Guaran-
tees for Convolutional Sparse Coding. arXiv preprint,
arXiv:1607(02009), 2016.

Scherrer, C., Halappanavar, M., Tewari, A., and Haglin,
D. Scaling Up Coordinate Descent Algorithms for Large
(cid:96)1 Regularization Problems. Technical report, Paciﬁc
Northwest National Laboratory (PNNL), 2012a.

Scherrer, C., Tewari, A., Halappanavar, M., and Haglin,
D. J. Feature Clustering for Accelerating Parallel Coor-
dinate Descent. In Advances in Neural Information Pro-
cessing Systems (NIPS), pp. 28–36, South Lake Tahoe,
United States, 2012b.

Shalev-Shwartz, S. and Tewari, A. Stochastic Methods
for (cid:96)1-regularized Loss Minimization. In International
Conference on Machine Learning (ICML), pp. 929–936,
Montreal, Canada, 2009.

Wohlberg, B.

Sparse Representations.
Processing, 25(1), 2016.

Efﬁcient Algorithms for Convolutional
IEEE Transactions on Image

You, Y., Lian, X., Liu, J., Yu, H.-F., Dhillon, I. S., Dem-
mel, J., and Hsieh, C.-J. Asynchronous Parallel Greedy
Coordinate Descent. In Advances in Neural Information
Processing Systems (NIPS), pp. 4682–4690, Barcelona,
Spain, 2016.

Yu, H. F., Hsieh, C. J., Si, S., and Dhillon, I. Scalable
coordinate descent approaches to parallel matrix factor-
In IEEE Interna-
ization for recommender systems.
tional Conference on Data Mining (ICDM), pp. 765–
774, Brussels, Belgium, 2012.

Chalasani, R., Principe, J. C., and Ramakrishnan, N. A
fast proximal method for convolutional sparse coding.
In International Joint Conference on Neural Networks
(IJCNN), pp. 1–5, Dallas, TX, USA, 2013.

El Ghaoui, L., Viallon, V., and Rabbani, T. Safe feature
elimination for the LASSO and sparse supervised learn-
ing problems. Journal of Machine Learning Research
(JMLR), 8(4):667–698, 2012.

Fercoq, O., Gramfort, A., and Salmon, J. Mind the duality
gap : safer rules for the Lasso. In International Confer-
ence on Machine Learning (ICML), pp. 333–342, Lille,
France, 2015.

Friedman, J., Hastie, T., H¨oﬂing, H., and Tibshirani, R.
Pathwise coordinate optimization. The Annals of Applied
Statistics, 1(2):302–332, 2007.

Grosse, R., Raina, R., Kwong, H., and Ng, A. Y. Shift-
Invariant Sparse Coding for Audio Classiﬁcation. Cor-
tex, 8:9, 2007.

Johnson, T. and Guestrin, C. Blitz: A Principled Meta-
In Inter-
Algorithm for Scaling Sparse Optimization.
national Conference on Machine Learning (ICML), pp.
1171–1179, Lille, France, 2015.

Kavukcuoglu, K., Sermanet, P., Boureau, Y.-l., Gregor, K.,
and Lecun, Y. Learning Convolutional Feature Hierar-
chies for Visual Recognition. In Advances in Neural In-
formation Processing Systems (NIPS), pp. 1090–1098,
Vancouver, Canada, 2010.

Mairal, J., Bach, F., Ponce, J., and Sapiro, G. Online Learn-
ing for Matrix Factorization and Sparse Coding. Jour-
nal of Machine Learning Research (JMLR), 11(1):19–
60, 2010.

Mallat, S. A Wavelet Tour of Signal Processing. Academic

press, 2008.

Nesterov, Y. Efﬁciency of coordinate descent methods on
huge-scale optimization problems. SIAM Journal on Op-
timization, 22(2):341–362, 2010.

DICOD: Distributed Convolutional Coordinate Descent for
Convolutional Sparse Coding

Moreau Thomas 1 Oudre Laurent 2 Vayatis Nicolas 1

Abstract

In this paper, we introduce DICOD, a convolu-
tional sparse coding algorithm which builds shift
invariant representations for long signals. This
algorithm is designed to run in a distributed set-
ting, with local message passing, making it com-
It is based on coordinate
munication efﬁcient.
descent and uses locally greedy updates which
accelerate the resolution compared to greedy co-
ordinate selection. We prove the convergence
of this algorithm and highlight its computational
speed-up which is super-linear in the number of
cores used. We also provide empirical evidence
for the acceleration properties of our algorithm
compared to state-of-the-art methods.

1. Convolutional Representation for Long

Signals

Sparse coding aims at building sparse linear representations
of a data set based on a dictionary of basic elements called
atoms.
It has proven to be useful in many applications,
ranging from EEG analysis to images and audio processing
(Adler et al., 2013; Kavukcuoglu et al., 2010; Mairal et al.,
2010; Grosse et al., 2007). Convolutional sparse coding is a
specialization of this approach, focused on building sparse,
shift-invariant representations of signals. Such representa-
tions present a major interest for applications like segmen-
tation or classiﬁcation as they separate the shape and the lo-
calization of patterns in a signal. This is typically the case
for physiological signals which can be composed of recur-
rent patterns linked to speciﬁc behavior in the human body
such as the characteristic heartbeat pattern in ECG record-
ings. Depending on the context, the dictionary can either
be ﬁxed analytically (e.g. wavelets, see Mallat 2008), or

1CMLA, ENS Cachan, CNRS, Universit´e Paris-Saclay,
94235 Cachan, France 2L2TI, Universit´e Paris 13, 93430
Villetaneuse, France. Correspondence to: Moreau Thomas
<thomas.moreau@cmla.ens-cachan.fr>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

learned from the data (Bristow et al., 2013; Mairal et al.,
2010).

Several algorithms have been proposed to solve the
convolutional sparse coding.
The Fast Iterative Soft-
Thresholding Algorithm (FISTA) was adapted for convo-
lutional problems in Chalasani et al. (2013) and uses prox-
imal gradient descent to compute the representation. The
Feature Sign Search (FSS), introduced in Grosse et al.
(2007), solves at each step a quadratic subproblem for an
active set of the estimated nonzero coordinates and the
Fast Convolutional Sparse Coding (FCSC) of Bristow et al.
(2013) is based on Alternating Direction Method of Multi-
pliers (ADMM). Finally, the coordinate descent (CD) has
been extended by Kavukcuoglu et al. (2010) to solve the
convolutional sparse coding. This method greedily opti-
mizes one coordinate at each iteration using fast local up-
dates. We refer the reader to Wohlberg (2016) for a detailed
presentation of these algorithms.

To our knowledge, there is no scalable version of these al-
gorithms for long signals. This is a typical situation, for
instance, in physiological signal processing where sensor
information can be collected for a few hours with sampling
frequencies ranging from 100 to 1000Hz. The existing
algorithms for generic (cid:96)1-regularized optimization can be
accelerated by improving the computational complexity of
each iteration. A ﬁrst approach to improve the complexity
of these algorithms is to estimate the non-zero coordinates
of the optimal solution to reduce the dimension of the op-
timization space, using either screening (El Ghaoui et al.,
2012; Fercoq et al., 2015) or active-set algorithms (John-
son & Guestrin, 2015). Another possibility is to develop
parallel algorithms which compute multiple updates simul-
taneously. Recent studies have considered distributing co-
ordinate descent algorithms for general (cid:96)1-regularized min-
imization (Scherrer et al., 2012a;b; Bradley et al., 2011; Yu
et al., 2012). These papers propose synchronous algorithms
using either locks or synchronizing steps to ensure the con-
vergence in general cases. You et al. (2016) derive an asyn-
chronous distributed algorithm for the projected coordinate
descent which uses centralized communication and ﬁnely
tuned step size to ensure the convergence of their method.

In the present paper, we design a novel distributed algo-

DICOD: Distributed Convolutional Sparse Coding

rithm tailored for the convolutional problem which is based
on coordinate descent, named Distributed Convolution Co-
ordinate Descent (DICOD). DICOD is asynchronous and
each process can run independently without locks or syn-
chronization steps. This algorithm uses a local communi-
cation scheme to reduce the number messages between the
processes and does not rely on external learning rates. We
also prove that this algorithm scales super-linearly with the
number of cores compared to the sequential CD, up to cer-
tain limitations.

In Section 2, we introduce the DICOD algorithm for the
resolution of convolutional sparse coding. Then, we prove
in Section 3 that DICOD converges to the optimal solution
for a wide range of settings and we analyze its complex-
ity. Finally, Section 4 presents numerical experiments that
illustrate the beneﬁts of the DICOD algorithm with respect
to other state-of-the-art algorithms and validate our theo-
retical analysis.

2. Distributed Convolutional Coordinate

Descent (DICOD)

Notations. The space of multivariate signals of length T
in RP is denoted by X P
T . For these signals, their value
is denoted by X[t] ∈ RP and for
at time t ∈
(cid:75)
, X[t] = 000P . The indicator function of
all t /∈
(cid:75)
t0 is denoted 111t0. For any signal X ∈ X P
T , the reversed
signal is deﬁned as X (cid:30)[t] = X[T − t]T and the d-norm

(cid:74)
0, T − 1
(cid:74)

0, T − 1

(cid:16)(cid:80)T −1

(cid:17)1/d

t=0 (cid:107)X[t](cid:107)d
d

is deﬁned as (cid:107)X(cid:107)d =
L, W ∈ N∗, the convolution between Z ∈ X 1
X P
such that for t ∈

W is a multivariate signal Z ∗DDD ∈ X P
0, T − 1
(cid:75)
(cid:74)
W −1
(cid:88)

. Finally, for
L and DDD ∈
T with T =L+W −1

,

(Z ∗ DDD)[t] ∆=

(cid:104)Z[t − τ ], DDD[τ ](cid:105) .

τ =0

This section reviews in Subsection 2.1 the convolutional
sparse coding as an (cid:96)1-regularized optimization problem
and the coordinate descent algorithm to solve it. Then, Sub-
section 2.2 and Subsection 2.3 respectively introduce the
Distributed Convolutional Coordinate Descent (DICOD)
and the Locally Greedy Coordinate Descent (LGCD) algo-
rithms to efﬁciently solve convolutional sparse coding for
long signals. Finally, Subsection 2.4 discusses related work
on (cid:96)1-regularized coordinate descent algorithms.

2.1. Coordinate Descent for Convolutional Sparse

Coding

Convolutional Sparse Coding. Consider the multivari-
ate signal X ∈ X P
K patterns with W (cid:28) T and Z = {Zk}K

W be a set of
L be a set

⊂ X P
k=1 ⊂ X 1

T . Let DDD =

DDDk

(cid:111)K

k=1

(cid:110)

of K activation signals with L=T −W +1. The convolu-
tional sparse representation models a multivariate signal X
as the sum of K convolutions between a local pattern DDDk
and an activation signal Zk such that:

K
(cid:88)

k=1

X[t] =

(Zk ∗ DDDk)[t] + E[t],

∀t ∈

0, T − 1
(cid:75)

(cid:74)

, (1)

with E ∈ X P
T representing an additive noise term. This
model also assumes that the coding signals Zk are sparse,
in the sense that only few entries are nonzero in each signal.
The sparsity property forces the representation to display
localized patterns in the signal. Note that this model can be
extended to higher order signals such as images by using
the proper convolution operator. In this study, we focus on
1D-convolution for the sake of simplicity.

Given a dictionary of patterns DDD, convolutional sparse cod-
ing aims to retrieve the sparse decomposition Z ∗ associated
to the signal X by solving the following (cid:96)1-regularized op-
timization problem

Z ∗ = argmin

E(Z) ,

Z=(Z1,...ZK )

E(Z) ∆=

X −

Zk ∗ DDDk

+ λ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
2

K
(cid:88)

k=1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

where

(2)

K
(cid:88)

k=1

(cid:13)
(cid:13)Zk

(cid:13)
(cid:13)1 ,

(3)

for a given regularization parameter λ > 0 . The problem
formulation (2) can be interpreted as a special case of the
LASSO problem with a band circulant matrix. Therefore,
classical optimization techniques designed for LASSO can
easily be applied to solve it with the same convergence
guarantees. Kavukcuoglu et al. (2010) adapted the coor-
dinate descent to efﬁciently solve the convolutional sparse
coding.

Convolutional Coordinate Descent. The coordinate de-
scent is a method which updates one coordinate at each it-
eration. This type of optimization algorithms is efﬁcient for
sparse optimization problem since few coordinates need to
be updated to ﬁnd the optimal solution and the greedy se-
lection of updated coordinates is a good strategy to achieve
fast convergence to the optimal point. Algorithm 1 summa-
rizes the greedy convolutional coordinate descent.

The method proposed by Kavukcuoglu et al. (2010) itera-
tively updates at each iteration one coordinate (k0, t0) of
the coding signal Z to its optimal value Z (cid:48)
[t0] when all
k0
other coordinates are ﬁxed. A closed form solution exists
to compute the value Z (cid:48)
k0

[t0] for the update,

Z (cid:48)
k0

[t0] =

1
(cid:107)DDDk0 (cid:107)2
2

Sh(βk0[t0], λ),

(4)

with the soft thresholding operator deﬁned as

Sh(u,λ)=sign(u)max(|u|−λ,0).

DICOD: Distributed Convolutional Sparse Coding

Algorithm 1 Greedy Coordinate Descent
1: Input: DDD, X, parameter (cid:15) > 0
2: C =
×
0, L − 1
(cid:75)
(cid:75)
3: Initialization: ∀(k, t) ∈ C,
(cid:16)
DDD(cid:30)

Zk[t] = 0, βk[t] =

1, K

[t]

(cid:17)

k ∗ X

(cid:74)

(cid:74)

4: repeat

5:

6:

k[t] =

∀(k, t) ∈ C, Z (cid:48)

1
(cid:107)DDDk(cid:107)2
2
Choose (k0, t0) = arg max
|∆Zk[t]|
(k,t)∈C
Update β using (5) and Zk0[t0] ← Z (cid:48)
k0

Sh(βk[t], λ) ,

[t0]

7:
8: until |∆Zk0 [t0]| < (cid:15)

Algorithm 2 DICODM
1: Input: DDD, X, parameter (cid:15) > 0
2: In parallel for m = 1 · · · M
3: For all (k, t) in Cm, initialize βk[t] and Zk[t]
4: repeat
5:
6:
7:

Receive messages and update β with (5)
∀(k, t) ∈ Cm, compute Z (cid:48)
Choose (k0, t0) = arg max
Update β with (5) and Zk0 [t0] ← Z (cid:48)
k0
if t0 − mLM < W then

8:
9:
10:
11:
12:
13: until global convergence (cid:107)∆Z(cid:107)∞ < (cid:15)

Send (k0, t0, ∆Zk0 [t0]) to core m − 1

Send (k0, t0, ∆Zk0 [t0]) to core m + 1

if (m + 1)LM − t0 < W then

k[t] with (4)

|∆Zk[t]|

(k,t)∈Cm

[t0]

and an auxiliary variable β ∈ X K

L deﬁned as

βk[t]=

Zk(cid:48)∗DDDk(cid:48)+Zk[t]et∗DDDk




DDD(cid:30)

k∗

X−

K
(cid:88)

k(cid:48)=1








[t] ,

where et is a dirac with value 1 in t and 0 elsewhere. Note
that βk[t] is simply the residual when Zk[t] is equal to 0.

The success of this algorithm highly depends on the efﬁ-
ciency in computing this coordinate update. For problem
(2), Kavukcuoglu et al. (2010) show that if at iteration q, the
coordinate (k0, t0) of Z (q) is updated to the value Z (cid:48)
[t0],
k0
then it is possible to compute β(q+1) from β(q) using

β(q+1)
k

[t] = β(q)

k [t] − (DDD(cid:30)

k ∗ DDDk0)[t − t0]∆Z (q)
k0

[t0],

(5)

for all (k, t) (cid:54)= (k0, t0). For all t /∈
−W + 1, W − 1
,
(DDD(cid:30)
(cid:75)
k ∗ DDDk0)[t] is zero. Thus, only O(KW ) operations are
needed to maintain β up-to-date with the current estimate
Z. In the following,

(cid:74)

∆Ek0 [t0] = E(Z (q)) − E(Z (q+1))

denotes the cost variation obtained when the coordinate
(k0, t0) is replaced by its optimal value Z (cid:48)
k0

[t0].

The selection of the updated coordinate (k0, t0) can follow
different strategies. Cyclic updates (Friedman et al., 2007)
and random updates (Shalev-Shwartz & Tewari, 2009) are
efﬁcient strategies as they have a O (cid:0)1(cid:1) computational
complexity. Osher & Li (2009) propose to select the co-
ordinate greedily to maximize the cost reduction of the up-
date. In this case, the coordinate is chosen as the one with
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)∆Zk[t]
the largest difference max(k,t)
(cid:12) between its current
value Zk[t] and the value Z (cid:48)

k[t] with

∆Zk[t] = Zk[t] − Z (cid:48)

k[t]

(6)

This strategy is computationally more expensive, with a
cost of O (cid:0)KT (cid:1) but it has a better convergence rate (Nu-
tini et al., 2015). In this paper, we focus on the greedy ap-
proach as it aims to get the largest gain from each update.
Moreover, as the updates in the greedy scheme are more
complex to compute, distributing them provides a larger
speedup compared to other strategies.

The procedure is run until maxk,t |∆Zk[t]| becomes
smaller than a speciﬁed tolerance parameter (cid:15).

2.2. Distributed Convolutional Coordinate Descent

(DICOD)

For convolutional sparse coding, the coordinate descent up-
dates are only weakly dependent as it is shown in (5). It is
thus natural to parallelize it for this problem.

DICOD. Algorithm 2 describes the steps of DICOD with
M workers. Each worker m ∈
is in charge
of updating the coordinates of a segment Cm of length
LM = L/M deﬁned by:

1, M

(cid:74)

(cid:75)

(cid:26)

Cm=

(k,t) ; k∈
(cid:74)

(cid:75)

1,K

, t∈(cid:114)(m−1)LM , mLM −1(cid:122)

.

(cid:27)

for all
The local updates are performed in parallel
the cores using the greedy coordinate descent
intro-
duced in Subsection 2.1. When a core m updates
the coefﬁent (k0, t0) such that t0 ∈
(m − 1)LM +
the updated coordinates of β are all
W, mLM − W
,
(cid:75)
contained in Cm and there is no need to update β on
the update is equiva-
the other cores.
mLM −W,mLM
lent to a sequential update. When t0∈
(cid:17)
(cid:16)
(cid:75)
(cid:74)
(m−1)LM ,(m−1)LM +W
resp. t0∈
, some of the co-
(cid:74)
ordinates of β in core m + 1 (resp. m − 1) need to be up-
dated and the update is not local anymore. This can be done

In these cases,

(cid:75)

(cid:74)

DICOD: Distributed Convolutional Sparse Coding

Cm updated in (k0, t0)

Cm+1 updated in (k1, t1)

∆Zk0 [t0], k0, t0

∆Zk0

[t0]

Z

β

t0−S

t0

No message

t0+S t1−S

t1

∆Zk1

[t1]

Z

β
t1+S

Figure 1. Communication process in DICOD for two cores Cm and Cm+1. (red) The process needs to send a message to its neighbor
as it updates a coordinate with t0 located near the border of the core’s segment, in the interference zone. (green) The update in t1 is
independent of other cores.

by sending the position of updated coordinate (k0, t0), and
the value of the update ∆Zk0[t0] to the neighboring core.
Figure 1 illustrates this communication process.
Inter-
processes communications are very limited in DICOD. One
node communicates with its neighbors only when it updates
coordinates close to the extremity of its segment. When the
size of the segment is reasonably large compared to the size
of the patterns, only a small part of the iterations needs to
send messages. We cannot apply the stopping criterion of
CD in each worker of DICOD, as this criterion might not
be reached globally. The updates in the neighbor cores can
break this criterion. To avoid this issue, the convergence is
considered to be reached once all the cores achieve this cri-
terion simultaneously. Workers that reach this state locally
are paused, waiting for incoming communication or for the
global convergence to be reached.

The key point that allows distributing the convolutional co-
ordinate descent algorithm is that the solutions on time seg-
ments that are not overlapping are only weakly dependent.
Equation (5) shows that a local change has impact on a seg-
ment of length 2W − 1 centered around the updated coor-
dinate. Thus, if two coordinates which are far enough were
updated simultaneously, the resulting point Z is the same
as if these two coordinates had been updated sequentially.
By splitting the signal into continuous segments over mul-
tiple cores, coordinates can be updated independently on
each core up to certain limits.

(k0, t0)

Interferences. When two coordinates
and
(k1, t1) are updated by two neighboring cores simultane-
ously, the updates might not be independent and cannot be
considered to be sequential. The local version of β used
for the second update does not account for the ﬁrst update.
We say that the updates are interfering. The cost reduction
resulting from these two updates is denoted ∆Ek0,k1 [t0, t1]
and simple computations, detailed in Proposition A.2,
show that

iterative steps
(cid:122)
(cid:123)
(cid:125)(cid:124)
∆Ek0 [t0] + ∆Ek1[t1]

∆Ek0,k1[t0, t1] =
− (DDD(cid:30)
k1
(cid:124)

∗ DDDk0 )[t1 − t0]∆Zk0[t0]∆Zk1 [t1]
,
(cid:125)

(cid:123)(cid:122)
interference

(7)

If |t1 − t0| ≥ W , then (DDD(cid:30)
∗DDDk0)[t1 − t0] = 0 and the up-
k1
dates can be considered to be sequential as the interference
term is zero. When |t1 − t0| < W , the interference term
does not vanish but Section 3 shows that under mild as-
sumption, this term can be controlled and it does not make
the algorithm diverge.

2.3. Locally Greedy Coordinate Descent (LGCD)

The theoretical analysis in Theorem 3 shows that DICOD
provides a super-linear acceleration compared to the greedy
coordinate descent. This result is supported with the nu-
merical experiment presented in Figure 4. The super-linear
speed up results from a double acceleration, provided by
the parallelization of the updates – we update M coordi-
nates at each iteration – and also by the reduction of the
complexity of each iteration. Indeed, each core computes
greedy updates with linear in complexity on 1/M -th of the
signal. This super-linear speed-up means that running DI-
COD sequentially will still provide a speed-up compared
to the greedy coordinate descent algorithm.

Algorithm 3 presents LGCD. This algorithm is similar to
a sequential version of DICOD. At each step, one segment
Cm is selected uniformly at random between the M seg-
ments. The greedy coordinate descent algorithm is applied
locally on this segment. This update is only locally greedy
and maximizes

(k0, t0) = argmax
(k,t)∈Cm

|∆Zk[t]|

This coordinate is then updated to its optimal value Z (cid:48)
[t0].
k0
In this case, there is no interference as the segments are not
updated simultaneously.

DICOD: Distributed Convolutional Sparse Coding

Algorithm 3 Locally greedy coordinate descent LGCDM
1: Input: DDD, X, parameter (cid:15) > 0, number of segments

& Guestrin, 2015). The evaluation of the performances of
our algorithm with these strategies is left for future work.

M

2: Initialize βk[t] and Zk[t] for all (k, t) in C
3: Initialize dZm = +∞ for m ∈
4: repeat
5:
6:
7:

Randomly select m ∈
∀(k, t) ∈ Cm, compute Z (cid:48)
Choose (k0, t0) = argmax
(k,t)∈Cm

(cid:75)
k[t] with (4)
|∆Zk[t]|

1, M

1, M

(cid:74)

(cid:74)

(cid:75)

8:
9:

Update β with (5)
Update estimate Zk0[t0](q+1) ← Z (cid:48)
[t0]
k0
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)Zk0 [t0](q+1) − Z (cid:48)
[t0]
Update dZm =
10:
(cid:12)
11: until (cid:107)dZ(cid:107)∞ < (cid:15) and (cid:107)∆Z(cid:107)∞ < (cid:15)

k0

Note that if M = T , this algorithm becomes very close
to the randomized coordinate descent. The coordinate is
selected greedily only between the K different channels of
the signal Z at the selected time. So the selection of M
depends on a tradeoff between the randomized coordinate
descent and the greedy coordinate descent.

2.4. Discussion

This algorithm differs from the existing paradigm to dis-
tribute CD (Scherrer et al., 2012a;b; Bradley et al., 2011;
Yu et al., 2012; You et al., 2016) as it does not rely on
centralized communication. Indeed, other parallel coordi-
nate descent algorithms rely on a parameter server, which
is an extra worker that holds the current value of Z. As
the size of the problem and the number of nodes grow, the
communication cost can rapidly become an issue with this
kind of centralized communication. The natural workload
split proposed with DICOD allows for more efﬁcient in-
teractions between the workers and reduces the need for
inter-node communications. Moreover, to prevent the in-
terferences breaking the convergence, existing algorithms
rely either on synchronous updates (Bradley et al., 2011;
Yu et al., 2012) or on reduced step size in the updates (You
et al., 2016; Scherrer et al., 2012a). In both case, they are
less efﬁcient than our asynchronous greedy algorithm that
can leverage the convolutional structure of the problem to
use both large updates and independent processes without
external parameters.

As seen in the introduction, another way to improve the
computational complexity of sparse coding algorithms is
to estimate the non-zero coordinates of the optimal solution
in order to reduce the dimension of the optimization space.
As this research direction is orthogonal to the paralleliza-
tion of the coordinate descent, it would be possible to com-
bine our algorithm with either screening (El Ghaoui et al.,
2012; Fercoq et al., 2015) or active-set methods (Johnson

3. Properties of DICOD

Convergence of DICOD. The magnitude of the interfer-
ence is related to the value of the cross-correlation between
dictionary elements, as shown in Proposition 1. Thus, when
the interferences have low probability and small magni-
tude, the distributed algorithm behaves as if the updates
were applied sequentially, resulting in a large acceleration
compared to the sequential CD algorithm.

Proposition 1. For concurrent updates for coordinates
(k0, t0) and (k1, t1) of a sparse code Z, the cost update
∆Ek0k1[t0, t1] is lower bounded by

∆Ek0,k1 [t0, t1] ≥ ∆Ek0[t0] + ∆Ek1[t1]
∗ DDDk1)[t0 − t1]

(DDD(cid:30)
k0
(cid:107)DDDk0(cid:107)2(cid:107)DDDk1 (cid:107)2

− 2

(cid:112)∆Ek0[t0]∆Ek1[t1].

(8)

The proof of this proposition is given in Appendix C.1. It
relies on the (cid:107)DDDk(cid:107)2
2-strong convexity of (4), which gives
for all Z. Using this inequality

2∆Ek[t](Z)

√

|∆Zk[t]| ≤
with (7) yields the result.

(cid:107)DDDk(cid:107)2

This proposition controls the magnitude of the interfer-
ence using the cost reduction associated to a single update.
When the correlations between the different elements of the
dictionary are small enough, the interfering update does not
increase the cost function. The updates are less efﬁcient but
do not worsen the current estimate. Using this control on
the interferences, we can prove the convergence of DICOD.

Theorem 2. Consider the following hypotheses,

H1. For all (k0,t0),(k1,t1) such that for all t0 (cid:54)= t1,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∗ DDDk1)[t0 − t1]

(DDD(cid:30)
k0
(cid:107)DDDk0(cid:107)2(cid:107)DDDk1(cid:107)2

< 1 .

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

H2. There exists A ∈ N∗ such that all cores m ∈
(cid:75)
are updated at least once between iteration i and i + A if
the solution is not locally optimal.

1, M
(cid:74)

H3. The delay in communication between the processes is
inferior to the update time.

Under (H1)-(H2)-(H3), the DICOD algorithm converges
to the optimal solution Z ∗ of (2).

Assumption (H1) is satisﬁed as long as the dictionary el-
ements are not replicated in shifted positions in the dictio-
nary. It ensures that the cost is updated in the right direction

DICOD: Distributed Convolutional Sparse Coding

at each step. This assumption can be linked to the shifted
mutual coherence introduced in Papyan et al. (2016).

Hypothesis (H2) ensures that all coordinates are updated
regularly if they are not already optimal. This analysis is
not valid when one of the cores fails. As only one core is
responsible for the update of a local segment, if a worker
fails, this segment cannot be updated anymore and thus the
algorithm will not converge to the optimal solution.

Finally, under (H3), an interference only results from one
update on each core. Multiple interferences occur when a
core updates multiple coordinates in the border of its seg-
ment before receiving the communication from other pro-
cesses border updates. When T (cid:29) W , the probability of
multiple interference is low and this hypothesis can be re-
laxed if the updates are not concentrated on the borders.

Proof sketch for Theorem 2. The full proof can be found
in Appendix C.2. The main argument in proving the con-
vergence is to show that most of the updates can be con-
sidered sequentially and that the remaining updates do
not increase the cost of the current point. By (H3), for
a given iteration, a core can interfere with at most one
other core. Thus, without loss of generality, we can
consider that at each step q,
the variation of the cost
E is either ∆Ek0[t0](Z (q)) or ∆Ek0k1[t0, t1](Z (q)), for
some (k0, t0), (k1, t1) ∈
1, K
. Proposi-
tion 1 and (H1) proves that ∆Ek0k1[t0, t1](Z (q)) ≥ 0.
For a single update ∆Ek0[t0](Z (q)), the update is equiva-
lent to a sequential update in CD, with the coordinate cho-
sen randomly between the best in each segments. Thus,
∆Ek0 [t0](Z (q)) > 0 and the convergence is eventually
proved using results from Osher & Li (2009).

0, T − 1
(cid:75)

×

(cid:75)

(cid:74)

(cid:74)

Speedup of DICOD. We denote Scd(M ) the speedup of
DICOD compared to the sequential greedy CD. This quan-
tiﬁes the number of iterations that can be run by DICOD
during one iteration of CD.
Theorem 3. Let α = W
4 and if
the non-zero coordinates of the sparse code are distributed
the expected speedup E[Scd(M )] is
uniformly in time,
lower bounded by

T and M ∈ N∗ . If αM < 1

E[Scd(M )] ≥ M 2(1 − 2α2M 2 (cid:16)

1 + 2α2M 2(cid:17) M

2 −1

) .

Proof sketch for Theorem 3. The full proof can be found
in Appendix D. There are two aspects involved in DI-
COD speedup: the computational complexity and the ac-
celeration due to the parallel updates. As stated in Subsec-
tion 2.1, the complexity of each iteration for CD is linear
with the length of the input signal T . In DICOD, each core
runs on a segment of size T
M . This accelerates the execu-
tion of individual updates by a factor M . Moreover, all
the cores compute their update simultaneously. The up-
dates without interference are equivalent to sequential up-
dates. Interfering updates happen with probability (cid:0)M α(cid:1)2
and do not increase the cost. Thus, one iteration of DI-
COD with Ni interferences provides a cost variation equiv-
alent to M − 2Ni iterations using sequential CD and, in
expectation, it is equivalent to M − 2E[Ni] iterations of
DICOD. The probability of interference depends on the ra-
tio between the length of the segments used for each core
and the size of the dictionary. If all the updates are spread
uniformly on each segment, the probability of interference

. The expected
between 2 neighboring cores is
number of interference E[Ni] can be upper bounded using
this probability and this yields the desired result.

(cid:17)2

(cid:16) MW
T

The overall speedup of DICOD is super-linear compared to
sequential greedy CD for the regime where αM (cid:28) 1. It
is almost quadratic for small M but as M grows, there is
a sharp transition that signiﬁcantly deteriorates the accel-
eration provided by DICOD. Section 4 empirically high-
lights this behavior. For a given α, it is possible to approxi-
mate the optimal number of cores M to solve convolutional
sparse coding problems.

Note that this super-linear speed up is due to the fact that
CD is inefﬁcient for long signals, as its iterations are com-
putationally too expensive to be competitive with the other
methods. The fact that we have a super-linear speed-up
means that running DICOD sequentially will provide an
acceleration compared to CD (see Subsection 2.3). For the
sequential run of DICOD, called LGCD, we have a linear
speed-up in comparison to CD, when M is small enough.
Indeed, the iteration cost is divided by M as we only need
to ﬁnd the maximal update on a local segment of size T
M .
When increasing M over T
W , the iteration cost does not de-
crease anymore as updating β costs O (cid:0)KW (cid:1) and ﬁnding
the best coordinate has the same complexity.

This result can be simpliﬁed when the interference proba-
bility (αM )2 is small.
Corollary 4. The expected speedup E[Scd(M )] when
M α → 0 is such that

E[Scd(M )] (cid:38)
α→0

M 2(1 − 2α2M 2 + O(α4M 4)) .

4. Numerical Results

All the numerical experiments are run on ﬁve Linux ma-
chines with 16 to 24 Intel Xeon 2.70 GHz processors and
at least 64 GB of RAM on local network. We use a combi-
nation of Python, C++ and the OpenMPI 1.6 for the algo-
rithms implementation. The code to reproduce the ﬁgures

DICOD: Distributed Convolutional Sparse Coding

Figure 2. Evolution of the loss function for DICOD, LGCD, CD,
FCSC and FISTA while solving sparse coding for a signal gen-
erated with default parameters relatively to the number of itera-
tions.

Figure 3. Evolution of the loss function for DICOD, LGCD, CD,
FCSC and FISTA while solving sparse coding for a signal gener-
ated with default parameters, relatively to time. This highlights
the speed of the algorithm on the given problem.

is available online1. The run time denotes the time for the
system to run the full algorithm pipeline, from cold start
and includes for instance the time to start the sub-processes.
The convergence refers to the variation of the cost with the
number of iterations and the speed to the variation of the
cost relative to time.

Long convolutional Sparse Coding Signals. To further
validate our algorithm, we generate signals and test the per-
formances of DICOD compared to state-of-the-art methods
proposed to solve convolutional sparse coding. We gen-
erate a signal X of length T in RP following the model
described in (1). The K dictionary atoms DDDk of length
W are drawn as a generic dictionary. First, each entry is
sampled from a Gaussian distribution. Then, each pattern
is normalized such that (cid:107)DDDk(cid:107)2 = 1. The sparse code en-
tries are drawn from a Bernoulli-Gaussian distribution with
Bernoulli parameter ρ = 0.007, mean 0 and standard vari-
ation σ = 10 . The noise term E is chosen as a Gaus-
sian white noise with variance 1. The default values for
the dimensions are set to W = 200, K = 25, P = 7,
T = 600 × W and we used λ = 1.

Algorithms Comparison. DICOD is compared to the
main state-of-the-art optimization algorithms for convo-
lutional sparse coding: Fast Convolutional Sparse Cod-
ing (FCSC) from Bristow et al. (2013), Fast Iterative Soft
Thresholding Algorithm (FISTA) using Fourier domain
computation as described in Wohlberg (2016), the greedy
convolutional coordinate descent (CD, Kavukcuoglu et al.
2010) and the randomized coordinate descent (RCD, Nes-
terov 2010). All the speciﬁc parameters for these algo-
rithms are ﬁxed based on the authors’ recommendations.

1Code available at github.com/tomMoral/dicod

DICODM denotes the DICOD algorithm run using M
cores. We also include LGCDM , for M ∈ {60, 600}, the
sequential run of the DICOD algorithm using M segments,
as described in Algorithm 3.

Figure 2 shows that the evolution of the performances of
LGCD relatively to the iterations are very close to the per-
formances of CD. The difference between these two algo-
rithms is that the updates are only locally greedy in LGCD.
As there is little difference visible between the two curves,
this means that in this case, the computed updates are es-
sentially the same. The differences are larger for LGCD600,
as the choice of coordinates are more localized in this case.
The performance of DICOD60 and DICOD30 are also close
to the iteration-wise performances of CD and LGCD. The
small differences between DICOD and LGCD result from
the iterations where there are interferences. Indeed, if two
iterations interfere, the cost does not decrease as much as
if the iterations were done sequentially. Thus, it requires
more steps to reach the same accuracy with DICOD60 than
with LGCD and with DICOD30, as there are more inter-
ferences when the number of cores M increases. This ex-
plains the discrepancy in the decrease of the cost around
the iteration 105. However, the number of extra steps re-
quired is quite low compared to the total number of steps
and the performances are mostly not affected by the inter-
ferences. The performances of RCD in terms of iterations
are much slower than the greedy methods. Indeed, as only a
few coordinates are useful, it takes many iterations to draw
them randomly.
In comparison, the greedy methods are
focused on the coordinates which largely divert from their
optimal value, and are thus most likely to be important. An-
other observation is that the performance in term of number
of iterations of the global methods FCSC and FISTA are
much better than the methods based on local updates. As

DICOD: Distributed Convolutional Sparse Coding

each iteration can update all the coordinates for FISTA, the
number of iterations needed to reach the optimal solution
is indeed smaller than for CD, where only one coordinate
is updated at a time.

In Figure 3, the speed of theses algorithms can be ob-
served. Even though it needs many more iterations to con-
verge, the randomized coordinate descent is faster than the
greedy coordinate descent. Indeed, for very long signals,
the iteration complexity of greedy CD is prohibitive. How-
ever, using the locally greedy updates, with LGCD60 and
LGCD600, the greedy algorithm can be made more efﬁ-
cient. LGCD600 is also faster than the other state-of-the-art
algorithms FISTA and FCSC. The choice of M = 600 is
a good tradeoff for LGCD as it means that the segments
are of the size of the dictionary W . With this choice for
M = T
W , the computational complexity of choosing a co-
ordinate is O (cid:0)KW (cid:1) and the complexity of maintaining β
is also O (cid:0)KW (cid:1). Thus, the iterations of this algorithm have
the same complexity as RCD but are more efﬁcient.

The distributed algorithm DICOD is faster compared to all
the other sequential algorithms and the speed up increases
with the number of cores. Also, DICOD has a shorter ini-
tialization time compared to the other algorithms. The ﬁrst
point in each curve indicates the time taken by the initial-
ization. For all the other methods, the computations for
constants – necessary to accelerate the iterations – have a
computational cost equivalent to the on of the gradient eval-
uation. As the segments of signal in DICOD are smaller,
the initialization time is also reduced. This shows that the
overhead of starting the cores is balanced by the reduction
of the initial computation for long signals. For shorter sig-
nals, we have observed that the initialization time is of the
same order as the other methods. The spawning overhead is
indeed constant whereas the constants are cheaper to com-
pute for small signals.

Speedup Evaluation. Figure 4 displays the speedup of
DICOD as a function of the number of cores. We used 10
generated problems for 2 signal lengths T = 150 · W and
T = 750 · W with W = 200 and we solved them using
DICODM with a number of cores M ranging from 1 to
75. The blue dots display the average running time for a
given number of workers. For both setups, the speedup is
super-linear up to the point where M α = 1
2 . For small
M the speedup is very close to quadratic and a sharp tran-
sition occurs as the number of cores grows. The verti-
cal solid green line indicates the approximate position of
the maximal speedup given in Corollary 4 and the dashed
lined is the expected theoretical run time derived from the
same expression. The transition after the maximum is very
sharp. This approximation of the speedup for small values
of M α is close to the experimental speedup observed with

Figure 4. Speedup of DICOD as a function of the number of pro-
cesses used, average over 10 run on different generated signals.
This highlights a sharp transition between a regime of quadratic
speedups and the regime where the interference are slowing down
drastically the convergence.

DICOD. The computed optimal value of M ∗ is close to the
optimal number of cores in these two examples.

5. Conclusion

In this work, we introduced an asynchronous distributed
algorithm that is able to speed up the resolution of the Con-
volutional Sparse Coding problem for long signals. This
algorithm is guaranteed to converge to the optimal solution
of (2) and scales superlinearly with the number of cores
used to distribute it. These claims are supported by numer-
ical experiments highlighting the performances of DICOD
compared to other state-of-the-art methods. Our proofs rely
extensively on the use of one dimensional convolutions. In
this setting, a process m only has two neighbors m − 1 and
m+1. This ensures that there is no high order interferences
between the updates. Our analysis does not apply straight-
forwardly to distributed computation using square patches
of images as the higher order interferences are more com-
plicated to handle. A way to apply our algorithm with these
guarantees to images is to split the signals along only one
direction, to avoid higher order interferences. The exten-
sion of our results to the grid splitting of images is an inter-
esting direction for future work.

References

Adler, A., Elad, M., Hel-Or, Y., and Rivlin, E. Sparse
Coding with Anomaly Detection. In IEEE International
Workshop on Machine Learning for Signal Processing
(MLSP), pp. 22 – 25, Southampton, United Kingdom,
2013.

DICOD: Distributed Convolutional Sparse Coding

Bradley, J. K., Kyrola, A., Bickson, D., and Guestrin,
C. Parallel Coordinate Descent for (cid:96)1-Regularized Loss
Minimization. In International Conference on Machine
Learning (ICML), pp. 321–328, Bellevue, WA, USA,
2011.

Nutini, J., Schmidt, M., Laradji, I. H., Friedlander, M. P.,
and Koepke, H. Coordinate Descent Converges Faster
with the Gauss-Southwell Rule Than Random Selec-
tion. In International Conference on Machine Learning
(ICML), pp. 1632–1641, Lille, France, 2015.

Bristow, H., Eriksson, A., and Lucey, S. Fast convolutional
sparse coding. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 391–398, Portland,
OR, USA, 2013.

Osher, S. and Li, Y. Coordinate descent optimization for (cid:96)1
minimization with application to compressed sensing; a
greedy algorithm. Inverse Problems and Imaging, 3(3):
487–503, 2009.

Papyan, V., Sulam, J., and Elad, M. Working Lo-
cally Thinking Globally - Part II: Theoretical Guaran-
tees for Convolutional Sparse Coding. arXiv preprint,
arXiv:1607(02009), 2016.

Scherrer, C., Halappanavar, M., Tewari, A., and Haglin,
D. Scaling Up Coordinate Descent Algorithms for Large
(cid:96)1 Regularization Problems. Technical report, Paciﬁc
Northwest National Laboratory (PNNL), 2012a.

Scherrer, C., Tewari, A., Halappanavar, M., and Haglin,
D. J. Feature Clustering for Accelerating Parallel Coor-
dinate Descent. In Advances in Neural Information Pro-
cessing Systems (NIPS), pp. 28–36, South Lake Tahoe,
United States, 2012b.

Shalev-Shwartz, S. and Tewari, A. Stochastic Methods
for (cid:96)1-regularized Loss Minimization. In International
Conference on Machine Learning (ICML), pp. 929–936,
Montreal, Canada, 2009.

Wohlberg, B.

Sparse Representations.
Processing, 25(1), 2016.

Efﬁcient Algorithms for Convolutional
IEEE Transactions on Image

You, Y., Lian, X., Liu, J., Yu, H.-F., Dhillon, I. S., Dem-
mel, J., and Hsieh, C.-J. Asynchronous Parallel Greedy
Coordinate Descent. In Advances in Neural Information
Processing Systems (NIPS), pp. 4682–4690, Barcelona,
Spain, 2016.

Yu, H. F., Hsieh, C. J., Si, S., and Dhillon, I. Scalable
coordinate descent approaches to parallel matrix factor-
In IEEE Interna-
ization for recommender systems.
tional Conference on Data Mining (ICDM), pp. 765–
774, Brussels, Belgium, 2012.

Chalasani, R., Principe, J. C., and Ramakrishnan, N. A
fast proximal method for convolutional sparse coding.
In International Joint Conference on Neural Networks
(IJCNN), pp. 1–5, Dallas, TX, USA, 2013.

El Ghaoui, L., Viallon, V., and Rabbani, T. Safe feature
elimination for the LASSO and sparse supervised learn-
ing problems. Journal of Machine Learning Research
(JMLR), 8(4):667–698, 2012.

Fercoq, O., Gramfort, A., and Salmon, J. Mind the duality
gap : safer rules for the Lasso. In International Confer-
ence on Machine Learning (ICML), pp. 333–342, Lille,
France, 2015.

Friedman, J., Hastie, T., H¨oﬂing, H., and Tibshirani, R.
Pathwise coordinate optimization. The Annals of Applied
Statistics, 1(2):302–332, 2007.

Grosse, R., Raina, R., Kwong, H., and Ng, A. Y. Shift-
Invariant Sparse Coding for Audio Classiﬁcation. Cor-
tex, 8:9, 2007.

Johnson, T. and Guestrin, C. Blitz: A Principled Meta-
In Inter-
Algorithm for Scaling Sparse Optimization.
national Conference on Machine Learning (ICML), pp.
1171–1179, Lille, France, 2015.

Kavukcuoglu, K., Sermanet, P., Boureau, Y.-l., Gregor, K.,
and Lecun, Y. Learning Convolutional Feature Hierar-
chies for Visual Recognition. In Advances in Neural In-
formation Processing Systems (NIPS), pp. 1090–1098,
Vancouver, Canada, 2010.

Mairal, J., Bach, F., Ponce, J., and Sapiro, G. Online Learn-
ing for Matrix Factorization and Sparse Coding. Jour-
nal of Machine Learning Research (JMLR), 11(1):19–
60, 2010.

Mallat, S. A Wavelet Tour of Signal Processing. Academic

press, 2008.

Nesterov, Y. Efﬁciency of coordinate descent methods on
huge-scale optimization problems. SIAM Journal on Op-
timization, 22(2):341–362, 2010.

