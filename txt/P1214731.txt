8
1
0
2
 
l
u
J
 
5
2
 
 
]

V
C
.
s
c
[
 
 
3
v
6
1
0
8
0
.
5
0
7
1
:
v
i
X
r
a

Pairwise Confusion
for Fine-Grained Visual Classiﬁcation

Abhimanyu Dubey1, Otkrist Gupta1, Pei Guo2, Ramesh Raskar1, Ryan Farrell2,
and Nikhil Naik1,3

1 Massachusetts Institute of Technology, Cambridge MA 02139, USA
{dubeya,otkrist,raskar,naik}@mit.edu
2 Brigham Young University, Provo UT 84602, USA
peiguo, farrell@cs.byu.edu
3 Harvard University, Cambridge MA 02139, USA
naik@fas.harvard.edu

Abstract. Fine-Grained Visual Classiﬁcation (FGVC) datasets contain
small sample sizes, along with signiﬁcant intra-class variation and inter-
class similarity. While prior work has addressed intra-class variation using
localization and segmentation techniques, inter-class similarity may also
aﬀect feature learning and reduce classiﬁcation performance. In this work,
we address this problem using a novel optimization procedure for the
end-to-end neural network training on FGVC tasks. Our procedure, called
Pairwise Confusion (PC) reduces overﬁtting by intentionally introducing
confusion in the activations. With PC regularization, we obtain state-of-
the-art performance on six of the most widely-used FGVC datasets and
demonstrate improved localization ability. PC is easy to implement, does
not need excessive hyperparameter tuning during training, and does not
add signiﬁcant overhead during test time.

1

Introduction

The Fine-Grained Visual Classiﬁcation (FGVC) task focuses on diﬀerentiating
between hard-to-distinguish object classes, such as species of birds, ﬂowers,
or animals; and identifying the makes or models of vehicles. FGVC datasets
depart from conventional image classiﬁcation in that they typically require expert
knowledge, rather than crowdsourcing, for gathering annotations. FGVC datasets
contain images with much higher visual similarity than those in large-scale visual
classiﬁcation (LSVC). Moreover, FGVC datasets have minute inter-class visual
diﬀerences in addition to the variations in pose, lighting and viewpoint found
in LSVC [1]. Additionally, FGVC datasets often exhibit long tails in the data
distribution, since the diﬃculty of obtaining examples of diﬀerent classes may
vary. This combination of small, non-uniform datasets and subtle inter-class
diﬀerences makes FGVC challenging even for powerful deep learning algorithms.
Most of the prior work in FGVC has focused on tackling the intra-class
variation in pose, lighting, and viewpoint using localization techniques [1,2,3,4,5],
and by augmenting training datasets with additional data from the Web [6,7].

2

A. Dubey, O. Gupta, P. Guo, R. Raskar, R. Farrell and N. Naik

However, we observe that prior work in FGVC does not pay much attention to
the problems that may arise due to the inter-class visual similarity in the feature
extraction pipeline. Similar to LSVC tasks, neural networks for FGVC tasks
are typically trained with cross-entropy loss [1,7,8,9]. In LSVC datasets such as
ImageNet [10], strongly discriminative learning using the cross-entropy loss is
successful in part due to the signiﬁcant inter-class variation (compared to intra-
class variation), which enables deep networks to learn generalized discriminatory
features with large amounts of data.

We posit that this formulation may not be ideal for FGVC, which shows
smaller visual diﬀerences between classes and larger diﬀerences within each class
than LSVC. For instance, if two samples in the training set have very similar
visual content but diﬀerent class labels, minimizing the cross-entropy loss will
force the neural network to learn features that distinguish these two images with
high conﬁdence—potentially forcing the network to learn sample-speciﬁc artifacts
for visually confusing classes in order to minimize training error. We suspect
that this eﬀect would be especially pronounced in FGVC, since there are fewer
samples from which the network can learn generalizable class-speciﬁc features.
Based on this hypothesis, we propose that introducing confusion in output
logit activations during training for an FGVC task will force the network to
learn slightly less discriminative features, thereby preventing it from overﬁtting
to sample-speciﬁc artifacts. Speciﬁcally, we aim to confuse the network, by
minimizing the distance between the predicted probability distributions for
random pairs of samples from the training set. To do so, we propose Pairwise
Confusion (PC)4, a pairwise algorithm for training convolutional neural networks
(CNNs) end-to-end for ﬁne-grained visual classiﬁcation.

In Pairwise Confusion, we construct a Siamese neural network trained with a
novel loss function that attempts to bring class conditional probability distribu-
tions closer to each other. Using Pairwise Confusion with a standard network
architecture like DenseNet [11] or ResNet [12] as a base network, we obtain
state-of-the-art performance on six of the most widely-used ﬁne-grained recogni-
tion datasets, improving over the previous-best published methods by 1.86% on
average. In addition, PC-trained networks show better localization performance
as compared to standard networks. Pairwise Confusion is simple to implement,
has no added overhead in training or prediction time, and provides performance
improvements both in FGVC tasks and other tasks that involve transfer learning
with small amounts of training data.

2 Related Work

Fine-Grained Visual Classiﬁcation: Early FGVC research focused on meth-
ods to train with limited labeled data and traditional image features. Yao et
al. [13] combined strongly discriminative image patches with randomization
techniques to prevent overﬁtting. Yao et al. [14] subsequently utilized template
matching to avoid the need for a large number of annotations.

4 Implementation available at https://github.com/abhimanyudubey/confusion.

Pairwise Confusion for Fine-Grained Visual Classiﬁcation

3

Table 1. A comparison of ﬁne-grained visual classiﬁcation (FGVC) datasets with large-
scale visual classiﬁcation (LSVC) datasets. FGVC datasets are signiﬁcantly smaller and
noisier than LSVC datasets.

Dataset

num.
samples
classes per class

Flowers-102 [32]
CUB-200-2011 [33]
Cars [34]
NABirds [35]
Aircrafts [36]
Stanford Dogs [37]

102
200
196
550
100
120

10
29.97
41.55
43.5
100
100

Dataset

num.
samples
classes per class

CIFAR-100 [38]
ImageNet [10]
CIFAR-10 [38]
SVHN [39]

100
1000
10
10

500
1200
5000
7325.7

Recently, improved localization of the target object in training images has
been shown to be useful for FGVC [1,15,16,17]. Zhang et al. [15] utilize part-based
Region-CNNs [18] to perform ﬁner localization. Spatial Transformer Networks [2]
show that learning a content-based aﬃne transformation layer improves FGVC
performance. Pose-normalized CNNs have also been shown to be eﬀective at
FGVC [19,20]. Model ensembling and boosting has also improved performance on
FGVC [21]. Lin et al. [1] introduced Bilinear Pooling, which combines pairwise
local feature sets and improves classiﬁcation performance. Bilinear Pooling has
been extended by Gao et al. [16] using a compact bilinear representation and
Cui et al. [9] using a general Kernel-based pooling framework that captures
higher-order interactions of features.

Pairwise Learning: Chopra et al. [22] introduced a Siamese neural network
for handwriting recognition. Parikh and Grauman [23] developed a pairwise
ranking scheme for relative attribute learning. Subsequently, pairwise neural
network models have become common for attribute modeling [24,25,26,27].

Learning from Label Confusion: Our method aims to improve classiﬁca-
tion performance by introducing confusion within the output labels. Prior work
in this area includes methods that utilize label noise (e.g., [28]) and data noise
(e.g., [29]) in training. Krause et al. [6] utilized noisy training data for FGVC.
Neelakantan et al. [30] added noise to the gradient during training to improve
generalization performance in very deep networks. Szegedy et al. [31] introduced
label-smoothing regularization for training deep Inception models.

In this paper, we bring together concepts from pairwise learning and label
confusion and take a step towards solving the problems of overﬁtting and sample-
speciﬁc artifacts when training neural networks for FGVC tasks.

3 Method

FGVC datasets in computer vision are orders of magnitude smaller than LSVC
datasets and contain greater imbalance across classes (see Table 1). Moreover,
the samples of a class are not accurately representative of the complete variation
in the visual class itself. The smaller dataset size can result in overﬁtting when

4

A. Dubey, O. Gupta, P. Guo, R. Raskar, R. Farrell and N. Naik

training deep neural architectures with large number of parameters—even with
preliminary layers being frozen. In addition, the training data may not be com-
pletely representative of the real-world data, with issues such as more abundant
sampling for certain classes. For example, in FGVC of birds, certain species from
geographically accessible areas may be overrepresented in the training dataset.
As a result, the neural network may learn to latch on to sample-speciﬁc artifacts
in the image, instead of learning a versatile representation for the target object.
We aim to solve both of these issues in FGVC (overﬁtting and sample-speciﬁc ar-
tifacts) by bringing the diﬀerent class-conditional probability distributions closer
together and confusing the deep network, subsequently reducing its prediction
over-conﬁdence, thus improving generalization performance.

Let us formalize the idea of “confusing” the conditional probability distribu-
tions. Consider the conditional probability distributions for two input images
x1 and x2, which can be given by pθ(y|x1) and pθ(y|x2) respectively. For a
classiﬁcation problem with N output classes, each of these distributions is an
N-dimensional vector, with each element i denoting the belief of the classiﬁer in
class yi given input x. If we wish to confuse the class outputs of the classiﬁer for
the pair x1 and x2, we should learn parameters θ that bring these conditional
probability distributions “closer” under some distance metric, that is, make the
predictions for x1 and x2 similar.

While KL-divergence might seem to be a reasonable choice to design a loss
function for optimizing the distance between conditional probability distributions,
in Section 3.1, we show that it is infeasible to train a neural network when
using KL-divergence as a regularizer. Therefore, we introduce the Euclidean
Distance between distributions as a metric for confusion in Sections 3.2 and 3.3
and describe neural network training with this metric in Section 3.4.

3.1 Symmetric KL-divergence or Jeﬀrey’s Divergence

The most prevalent method to measure dissimilarity of one probability distribution
from another is to use the Kullback-Liebler (KL) divergence. However, the
standard KL-divergence cannot serve our purpose owing to its asymmetric nature.
This could be remedied by using the symmetric KL-divergence, deﬁned for two
probability distributions P, Q with mass functions p(·), q(·) (for events u ∈ U):

DJ(P, Q) (cid:44) (cid:88)

(cid:104)
p(u) · log

p(u)
q(u)

u∈U

(cid:105)

q(u)
p(u)

+ q(u) · log

= DKL(P ||Q) + DKL(Q||P ) (1)

This symmetrized version of KL-divergence, known as Jeﬀrey’s divergence [40], is a
measure of the average relative entropy between two probability distributions [41].
For our model parameterized by θ, for samples x1 and x2, the Jeﬀrey’s divergence
can be written as:

DJ(pθ(y|x1), pθ(y|x2)) =

(pθ(yi|x1) − pθ(yi|x2)) · log

(cid:105)

pθ(yi|x1)
pθ(yi|x2)

(2)

N
(cid:88)

(cid:104)

i=1

Pairwise Confusion for Fine-Grained Visual Classiﬁcation

5

Jeﬀrey’s divergence satisﬁes all of our basic requirements of a symmetric diver-
gence metric between probability distributions, and therefore could be included
as a regularizing term while training with cross-entropy, to achieve our desired
confusion. However, when we learn model parameters using stochastic gradient
descent (SGD), it can be diﬃcult to train, especially if our distributions P, Q
have mass concentrated on diﬀerent events. This can be seen in Equation 2.
Consider Jeﬀrey’s divergence with N = 2 classes, and that x1 belongs to class 1,
and x2 belongs to class 2. If the model parameters θ are such that it correctly
identiﬁes both x1 and x2 by training using cross-entropy loss, pθ(y1|x1) = 1 − δ1
and pθ(y2|x2) = 1 − δ2, where 0 < δ1, δ2 < 1
2 (since the classiﬁer outputs correct
predictions for the input images), we can show:

DJ(pθ(y|x1), pθ(y|x2)) ≥ (1 − δ1 − δ2) · (2 log(1 − δ1 − δ2) − log(δ1δ2))

(3)

Please see the supplementary material for an expanded proof.

As training progresses with these labels, the cross-entropy loss will moti-
vate the values of δ1 and δ2 to become closer to zero (but never equaling zero,
since the probability outputs pθ(y|x1), pθ(y|x2) are the outputs from a soft-
max). As (δ1, δ2) → (0+, 0+), the second term − log(δ1δ2) on the R.H.S. of
inequality (4) typically grows whereas (1 − δ1 − δ2) approaches 1, which makes
DJ(pθ(y|x1), pθ(y|x2)) larger as the predictions get closer to the true labels. In
practice, we see that training with DJ(pθ(y|x1), pθ(y|x2)) as a regularizer term
diverges, unless a very small regularizing parameter is chosen, which removes the
eﬀect of regularization altogether.

A natural question that can arise from this analysis is that cross-entropy
training itself involves optimizing KL-divergence between the target label distri-
bution and the model’s predictions, however no such divergence occurs. This is
because cross-entropy involves only one direction of the KL-divergence, and the
target distribution has all the mass concentrated at one event (the correct label).
Since (x log x)|x=0 = 0, for predicted label vector y(cid:48) with correct label class c,
this simpliﬁes the cross-entropy error LCE(pθ(y|x), y(cid:48)) to be:

LCE(pθ(y|x), y(cid:48)) = −

y(cid:48)

i log(

) = − log(pθ(yc|x)) ≥ 0

(4)

N
(cid:88)

i=1

pθ(yi|x)
y(cid:48)
i

This formulation does not diverge as the model trains, i.e. pθ(yc|x) → 1. In
some cases where label noise is added to the label vector (such as label smooth-
ing [28,42]), the label noise is a ﬁxed constant and not approaching zero (as in
the case of Jeﬀery’s divergence between model predictions) and is hence feasible
to train. Thus, Jeﬀrey’s Divergence or symmetric KL-divergence, while a seem-
ingly natural choice, cannot be used to train a neural network with SGD. This
motivates us to look for an alternative metric to measure “confusion” between
conditional probability distributions.

3.2 Euclidean Distance as Confusion

Since the conditional probability distribution over N classes is an element within
RN on the unit simplex, we can consider the Euclidean distance to be a metric

6

A. Dubey, O. Gupta, P. Guo, R. Raskar, R. Farrell and N. Naik

of “confusion” between two conditional probability distributions. Analogous to
the previous setting, we deﬁne the Euclidean Confusion DEC(·, ·) for a pair of
inputs x1, x2 with model parameters θ as:

N
(cid:88)

i=1

DEC(pθ(y|x1), pθ(y|x2)) =

(pθ(yi|x1) − pθ(yi|x2))2 = (cid:107)pθ(y|x1) − pθ(y|x2)(cid:107)2
2

(5)
Unlike Jeﬀrey’s Divergence, Euclidean Confusion does not diverge when used as
a regularization term with cross-entropy. However, to verify this unconventional
choice for a distance metric between probability distributions, we prove some
properties that relate Euclidean Confusion to existing divergence measures.
Lemma 1. On a ﬁnite probability space, the Euclidean Confusion DEC(P, Q) is
a lower bound for the Jeﬀrey’s Divergence DJ(P, Q) for probability measures P, Q.

Proof. This follows from Pinsker’s Inequality and the relationship between (cid:96)1
and (cid:96)2 norms. Complete proof is provided in the supplementary material.

By Lemma 1, we can see that the Euclidean Confusion is a conservative estimate
for Jeﬀrey’s divergence, the earlier proposed divergence measure. For ﬁnite
probability spaces, the Total Variation Distance DTV(P, Q)2 = 1
2 (cid:107)P − Q(cid:107)1 is
also a measure of interest. However, due to its non-diﬀerentiable nature, it is
unsuitable for our case. Nevertheless, we can relate the Euclidean Confusion and
Total Variation Distance by the following result.
Lemma 2. On a ﬁnite probability space, the Euclidean Confusion DEC(P, Q) is
bounded by 4DTV(P, Q)2 for probability measures P, Q.

Proof. This follows directly from the relationship between (cid:96)1 and (cid:96)2 norms.
Complete proof is provided in the supplementary material.

3.3 Euclidean Confusion for Point Sets

In a standard classiﬁcation setting with N classes, we consider a training set
with m = (cid:80)N
i=1 mi training examples, where mi denotes the number of training
samples for class i. For this setting, we can write the total Euclidean Confusion
between points of classes i and j as the average of the Euclidean Confusion
between all pairs of points belonging to those two classes. For simplicity of
notation, let us denote the set of conditional probability distributions of all
training points belonging to class i for a model parameterized by θ as Si =
{pθ(y|xi
)}. Then, for a model parameterized by θ, the
mi
Euclidean Confusion is given by:

2), ..., pθ(y|xi

1), pθ(y|xi

We can simplify this equation by assuming an equal number of points n per class:

DEC(Si, Sj; θ) (cid:44) 1

mi,mj
(cid:88)

(cid:16)

mimj

u,v

DEC(pθ(y|xi

u), pθ(y|xj

v))

(cid:17)

DEC(Si, Sj; θ) =

(cid:107)pθ(y|xi

u) − pθ(y|xj

v)(cid:107)2
2

(cid:17)

1
n2

(cid:16)

n,n
(cid:88)

u,v

(6)

(7)

Pairwise Confusion for Fine-Grained Visual Classiﬁcation

7

This form of the Euclidean Confusion between the two sets of points gives us
an interesting connection with another popular distance metric over probability
distributions, known as the Energy Distance [43].

Introduced by Gabor Szekely [43], the Energy Distance DEN(F, G) between
two cumulative probability distribution functions F and G with random vectors
X and Y in RN can be given by

DEN(F, G)2 (cid:44) 2E(cid:107)X − Y (cid:107) − E(cid:107)X − X (cid:48)(cid:107) − E(cid:107)Y − Y (cid:48)(cid:107) ≥ 0

(8)

where (X, X (cid:48), Y, Y (cid:48)) are independent, and X ∼ F, X (cid:48) ∼ F, Y ∼ G, Y (cid:48) ∼ G. If we
consider the sets Si and Sj, with a uniform probability of selecting any of the n
points in each of these sets, then we obtain the following results.

Lemma 3. For sets Si, Sj and DEC(Si, Sj; θ) as deﬁned in Equation (14):

DEN(Si, Sj; θ)2 ≤ DEC(Si, Sj; θ)

1
2

where DEN(Si, Sj; θ) is the Energy Distance under Euclidean norm between Si and
Sj (parameterized by θ), and random vectors are selected with uniform probability
in both Si and Sj.

Proof. This follows from the deﬁnition of Energy Distance with uniform proba-
bility of sampling. Complete proof is provided in the supplementary material.

Corollary 1. For sets Si, Sj and DEC(Si, Sj; θ) as deﬁned in Equation (14), we
have:

DEC(Si, Si; θ) + DEC(Sj, Sj; θ) ≤ 2DEC(Si, Sj; θ)

with equality only when Si = Sj.

Proof. This follows from the fact that the Energy Distance DEN(Si, Sj; θ) is 0 only
when Si = Sj. The complete version of the proof is included in the supplement.

With these results, we restrict the behavior of Euclidean Confusion within two
well-deﬁned conventional probability distance measures, the Jeﬀrey’s divergence
and Energy Distance. One might consider optimizing the Energy Distance directly,
due to its similar formulation and the fact that we uniformly sample points during
training with SGD. However, the Energy Distance additionally includes the two
terms that account for the negative of the average all-pairs distances between
points in Si and Sj respectively, which we do not want to maximize, since we
do not wish to push points within the same class further apart. Therefore, we
proceed with our measure of Euclidean Confusion.

3.4 Learning with Gradient Descent

We proceed to learn parameters θ∗ for a neural network, with the following
learning objective function for a pair of input points, motivated by the formulation

8

A. Dubey, O. Gupta, P. Guo, R. Raskar, R. Farrell and N. Naik

Fig. 1. CNN training pipeline for Pairwise Confusion (PC). We employ a Siamese-like
architecture, with individual cross entropy calculations for each branch, followed by a
joint energy-distance minimization loss. We split each incoming batch of samples into
two mini-batches, and feed the network pairwise samples.

of Euclidean Confusion:

N,N
n,n
(cid:88)

(cid:104)

i=1,j(cid:54)=i
u,v

θ∗ = arg min

θ

LCE(pθ(y|xi

u), yi

u)+LCE(pθ(y|xj

v), yj

v)+

DEC(pθ(y|xj

v), pθ(y|xi

u))

(cid:105)

λ
n2

(9)
This objective function can be explained as: for each point in the training set, we
randomly select another point from a diﬀerent class and calculate the individual
cross-entropy losses and Euclidean Confusion until all pairs have been exhausted.
For each point in the training dataset, there are n·(N − 1) valid choices for the
other point, giving us a total of n2·N ·(N − 1) possible pairs. In practice, we
ﬁnd that we do not need to exhaust all combinations for eﬀective learning using
gradient descent, and in fact we observe that convergence is achieved far before
all observations are observed. We simplify our formulation instead by using the
following procedure described in Algorithm 1.

Training Procedure: As described in Algorithm 1, our learning procedure is a
slightly modiﬁed version of the standard SGD. We randomly permute the training
set twice, and then for each pair of points in the training set, add Euclidean
Confusion only if the samples belong to diﬀerent classes. This form of sampling
approximates the exhaustive Euclidean Confusion, with some points with regular
gradient descent, which in practice does not alter the performance. Moreover,
convergence is achieved after only a fraction of all the possible pairs are observed.
Formally, we wish to model the conditional probability distribution pθ(y|x) over
the p classes for function f (x; θ) = pθ(y|x) parameterized by model parameters
θ. Given our optimization procedure, we can rewrite the total loss for a pair of

Pairwise Confusion for Fine-Grained Visual Classiﬁcation

9

Algorithm 1 Training Using Euclidean Confusion

Training data D, Test data ˆD, parameters θ, hyperparameters ˆθ
for epoch ∈ [0,max epochs]) do

D1 ⇐ shuﬄe(D)
D2 ⇐ shuﬄe(D)
for i ∈ [0,num batches] do

Lbatch = 0
for (d1, d2) ∈ batch i of (D1, D2) do

γ ⇐ 1 if label(d1) (cid:54)= label(d2), 0 otherwise
Lpair ⇐ LCE(d1; θ) + LCE(d2; θ) + λ · γ · DEC(d1, d2; θ)
Lbatch ⇐ Lbatch + Lpair

end for
θ ⇐ Backprop(Lbatch, θ, ˆθ)

end for
ˆθ ⇐ ParameterUpdate(epoch, ˆθ)

end for

points x1, x2 with model parameters θ as:

2
(cid:88)

i=1

Lpair(x1, x2, y1, y2; θ) =

[LCE(pθ(y|xi), yi)] + λγ(y1, y2)DEC(pθ(y|x1), pθ(y|x2))

(10)
where, γ(y1, y2) = 1 when yi (cid:54)= yj, and 0 otherwise. We denote training with
this general architecture with the term Pairwise Confusion or PC for short.
Speciﬁcally, we train a Siamese-like neural network [22] with shared weights,
training each network individually using cross-entropy, and add the Euclidean
Confusion loss between the conditional probability distributions obtained from
each network (Figure 1). During training, we split an incoming batch of training
samples into two parts, and evaluating cross-entropy on each sub-batch identically,
followed by a pairwise loss term calculated for corresponding pairs of samples
across batches. During testing, only one branch of the network is active, and
generates output predictions for the input image. As a result, implementing
this method does not introduce any signiﬁcant computational overhead during
testing.

CNN Architectures We experiment with VGGNet [44], GoogLeNet [42],
ResNets [12], and DenseNets [11] as base architectures for the Siamese net-
work trained with PC to demonstrate that our method is insensitive to the
choice of source architecture.

4 Experimental Details

We perform all experiments using Caﬀe [45] or PyTorch [46] over a cluster of
NVIDIA Titan X, Tesla K40c and GTX 1080 GPUs. Our code and models are
available at github.com/abhimanyudubey/confusion. Next, we provide brief
descriptions of the various datasets used in our paper.

10

A. Dubey, O. Gupta, P. Guo, R. Raskar, R. Farrell and N. Naik

Table 2. Pairwise Confusion (PC) obtains state-of-the-art performance on six widely-
used ﬁne-grained visual classiﬁcation datasets (A-F). Improvement over the baseline
model is reported as (∆). All results averaged over 5 trials.

(A) CUB-200-2011

Method
Top-1 ∆
-
Gao et al. [16]
84.00
-
84.10
STN[2]
-
Zhang et al. [47] 84.50
-
85.80
Lin et al. [8]
86.20
Cui et al. [9]
-
78.15
ResNet-50
PC-ResNet-50
80.21
Bilinear CNN [1] 84.10
PC-BilinearCNN 85.58
DenseNet-161
84.21
PC-DenseNet-161 86.87

(2.06)

(1.48)

(2.66)

(B) Cars

(C) Aircrafts

Top-1 ∆
Method
-
85.70
Wang et al. [17]
-
86.80
Liu et al. [48]
-
92.00
Lin et al. [8]
92.40
-
Cui et al. [9]
91.71
ResNet-50
PC-ResNet-50
93.43
Bilinear CNN [1] 91.20
PC-Bilinear CNN 92.45
DenseNet-161
91.83
PC-DenseNet-161 92.86

(1.25)

(1.03)

(1.72)

Top-1 ∆
-
-
-
-

Method
Simon et al. [49] 85.50
86.90
Cui et al. [9]
87.30
LRBP [50]
88.50
Lin et al. [8]
81.19
ResNet-50
83.40
PC-ResNet-50
BilinearCNN [1]
84.10
PC-BilinearCNN 85.78
DenseNet-161
86.30
PC-DenseNet-161 89.24

(2.21)

(1.68)

(2.94)

(D) NABirds

(E) Flowers-102

(F) Stanford Dogs

Top-1 ∆
-
-

Method
Branson et al. [19] 35.70
75.00
Van et al. [35]
63.55
ResNet-50
68.15
PC-ResNet-50
BilinearCNN [1]
80.90
PC-BilinearCNN 82.01
DenseNet-161
79.35
PC-DenseNet-161 82.79

(4.60)

(1.11)

(3.44)

Top-1 ∆
Method
-
80.66
Det.+Seg. [51]
86.80
-
Overfeat[52]
92.46
ResNet-50
93.50
PC-ResNet-50
BilinearCNN [1]
92.52
PC-BilinearCNN 93.65
DenseNet-161
90.07
PC-DenseNet-161 91.39

(1.04)

(1.13)

(1.32)

Top-1 ∆
Method
-
80.43
Zhang et al. [3]
80.60
-
Krause et al. [6]
69.92
ResNet-50
73.35
PC-ResNet-50
BilinearCNN [1]
82.13
PC-BilinearCNN 83.04
DenseNet-161
81.18
PC-DenseNet-161 83.75

(0.91)

(2.57)

(3.43)

4.1 Fine-Grained Visual Classiﬁcation (FGVC) datasets

1. Wildlife Species Classiﬁcation: We experiment with several widely-used
FGVC datasets. The Caltech-UCSD Birds (CUB-200-2011) dataset [33] has
5,994 training and 5,794 test images across 200 species of North-American birds.
The NABirds dataset [35] contains 23,929 training and 24,633 test images
across over 550 visual categories, encompassing 400 species of birds, including
separate classes for male and female birds in some cases. The Stanford Dogs
dataset [37] has 20,580 images across 120 breeds of dogs around the world. Finally,
the Flowers-102 dataset [32] consists of 1,020 training, 1,020 validation and
6,149 test images over 102 ﬂower types.
2. Vehicle Make/Model Classiﬁcation: We experiment with two common
vehicle classiﬁcation datasets. The Stanford Cars dataset [34] contains 8,144
training and 8,041 test images across 196 car classes. The classes represent
variations in car make, model, and year. The Aircraft dataset is a set of 10,000
images across 100 classes denoting a ﬁne-grained set of airplanes of diﬀerent
varieties [36].

These datasets contain (i) large visual diversity in each class [32,33,37], (ii)
visually similar, often confusing samples belonging to diﬀerent classes, and (iii)
a large variation in the number of samples present per class, leading to greater
class imbalance than LSVC datasets like ImageNet [10]. Additionally, some of
these datasets have densely annotated part information available, which we do
not utilize in our experiments.

Pairwise Confusion for Fine-Grained Visual Classiﬁcation

11

Fig. 2. (left) Variation of test accuracy on CUB-200-2011 with logarithmic variation in
hyperparameter λ. (right) Convergence plot of GoogLeNet on CUB-200-2011.

5 Results

5.1 Fine-Grained Visual Classiﬁcation

We ﬁrst describe our results on the six FGVC datasets from Table 2. In all
experiments, we average results over 5 trials per experiment—after choosing the
best value of hyperparameter λ. Please see the supplementary material for mean
and standard deviation values for all experiments.

1. Fine-tuning from Baseline Models: We ﬁne-tune from three baseline mod-
els using the PC optimization procedure: ResNet-50 [12], Bilinear CNN [1], and
DenseNet-161 [11]. As Tables 2-(A-F) show, PC obtains substantial improvement
across all datasets and models. For instance, a baseline DenseNet-161 architecture
obtains an average accuracy of 84.21%, but PC-DenseNet-161 obtains an accu-
racy of 86.87%, an improvement of 2.66%. On NABirds, we obtain improvements
of 4.60% and 3.42% over baseline ResNet-50 and DenseNet-161 architectures.
2. Combining PC with Specialized FGVC models: Recent work in FGVC
has proposed several novel CNN designs that take part-localization into account,
such as bilinear pooling techniques [16,1,9] and spatial transformer networks [2].
We train a Bilinear CNN [1] with PC, and obtain an average improvement of
1.7% on the 6 datasets.

We note two important aspects of our analysis: (1) we do not compare with
ensembling and data augmentation techniques such as Boosted CNNs [21] and
Krause, et al. [6] since prior evidence indicates that these techniques invariably
improve performance, and (2) we evaluate a single-crop, single-model evaluation
without any part- or object-annotations, and perform competitively with methods
that use both augmentations.

Choice of Hyperparameter λ: Since our formulation requires the selection
of a hyperparameter λ, it is important to study the sensitivity of classiﬁcation
performance to the choice of λ. We conduct this experiment for four diﬀerent
models: GoogLeNet [42], ResNet-50 [12] and VGGNet-16 [44] and Bilinear-
CNN [1] on the CUB-200-2011 dataset. PC’s performance is not very sensitive
to the choice of λ (Figure 2 and Supplementary Tables S1-S5). For all six

12

A. Dubey, O. Gupta, P. Guo, R. Raskar, R. Farrell and N. Naik

datasets, the λ value is typically between the range [10,20]. On Bilinear CNN,
setting λ = 10 for all datasets gives average performance within 0.08% compared
to the reported values in Table 2. In general, PC obtains optimum performance
in the range of 0.05N and 0.15N , where N is the number of classes.

5.2 Additional Experiments

Since our method aims to improve classiﬁcation performance in FGVC tasks
by introducing confusion in output logit activations, we would expect to see a
larger improvement in datasets with higher inter-class similarity and intra-class
variation. To test this hypothesis, we conduct two additional experiments.

In the ﬁrst experiment, we construct two subsets of ImageNet-1K [10]. The
ﬁrst dataset, ImageNet-Dogs is a subset consisting only of species of dogs (117
classes and 116K images). The second dataset, ImageNet-Random contains
randomly selected classes from ImageNet-1K. Both datasets contain equal number
of classes (117) and images (116K), but ImageNet-Dogs has much higher inter-
class similarity and intra-class variation, as compared to ImageNet-Random. To
test repeatability, we construct 3 instances of Imagenet-Random, by randomly
choosing a diﬀerent subset of ImageNet with 117 classes each time. For both
experiments, we randomly construct a 80-20 train-val split from the training data
to ﬁnd optimal λ by cross-validation, and report the performance on the unseen
ImageNet validation set of the subset of chosen classes. In Table 3, we compare
the performance of training from scratch with- and without-PC across three
models: GoogLeNet, ResNet-50, and DenseNet-161. As expected, PC obtains a
larger gain in classiﬁcation accuracy (1.45%) on ImageNet-Dogs as compared to
the ImageNet-Random dataset(0.54% ± 0.28).

In the second experiment, we utilize the CIFAR-10 and CIFAR-100 datasets,
which contain the same number of total images. CIFAR-100 has 10× the number
of classes and 10% of images per class as CIFAR-10 and contains larger inter-class
similarity and intra-class variation. We train networks on both datasets from
scratch using default train-test splits (Table 3). As expected, we obtain larger
average gains of 1.77% on CIFAR-100, as compared to 0.20% on CIFAR-10.
Additionally, when training with λ = 10 on the entire ImageNet dataset, we
obtain a top-1 accuracy of 76.28% (compared to a baseline of 76.15%), which is a
smaller improvement, which is in line with what we would expect for a large-scale
image classiﬁcation problem with large inter-class variation.

Moreover, while training with PC, we observe that the rate of convergence is
always similar to or faster than training without PC. For example, a GoogLeNet
trained on CUB-200-2011 (Figure 2(right) above) shows that PC converges to
higher validation accuracy faster than normal training using identical learning
rate schedule and batch size. Note that the training accuracy is reduced when
training with PC, due to the regularization eﬀect. In sum, classiﬁcation problems
that have large intra-class variation and high inter-class similarity beneﬁt from
optimization with pairwise confusion. The improvement is even more prominent
when training data is limited.

Pairwise Confusion for Fine-Grained Visual Classiﬁcation

13

Table 3. Experiments with ImageNet and CIFAR show that datasets with large intra-
class variation and high inter-class similarity beneﬁt from optimization with Pairwise
Confusion. Only the mean accuracy over 3 Imagenet-Random experiments is shown.

Network

ImageNet-Random ImageNet-Dogs CIFAR-10
Baseline
71.85
GoogLeNet [42]
ResNet-50 [12]
82.01
DenseNet-161 [11] 78.34

CIFAR-100
Baseline PC Baseline PC Baseline PC
86.63 87.02 73.35 76.02
93.17 93.46 72.16 73.14
95.15 95.08 78.60 79.56

PC
72.09
82.65
79.10

64.17
75.92
71.44

62.35
73.81
70.15

Table 4. Pairwise Confusion (PC) improves localization performance in ﬁne-grained
visual classiﬁcation tasks. On the CUB-200-2011 dataset, PC obtains an average
improvement of 3.4% in Mean Intersection-over-Union (IoU) for Grad-CAM bounding
boxes for each of the ﬁve baseline models.

Method
Mean IoU (Baseline)
Mean IoU (PC) - Ours

GoogLeNet VGG-16 ResNet-50 DenseNet-161 Bilinear-CNN

0.29
0.35

0.31
0.34

0.32
0.35

0.34
0.37

0.37
0.39

5.3 Improvement in Localization Ability

Recent techniques for improving classiﬁcation performance in ﬁne-grained recog-
nition are based on summarizing and extracting dense localization information
in images [1,2]. Since our technique increases classiﬁcation accuracy, we wish
to understand if the improvement is a result of enhanced CNN localization
abilities due to PC. To measure the regions the CNN localizes on, we utilize
Gradient-Weighted Class Activation Mapping (Grad-CAM) [53], a method that
provides a heatmap of visual saliency as produced by the network. We perform
both quantitative and qualitative studies of localization ability of PC-trained
models.
Overlap in Localized Regions: To quantify the improvement in localization
due to PC, we construct bounding boxes around object regions obtained from
Grad-CAM, by thresholding the heatmap values at 0.5, and choosing the largest
box returned. We then calculate the mean IoU (intersection-over-union) of the
bounding box with the provided object bounding boxes for the CUB-200-2011
dataset. We compare the mean IoU across several models, with and without PC.
As summarized in Table 4, we observe an average 3.4% improvement across ﬁve
diﬀerent networks, implying better localization accuracy.
Change in Class-Activation Mapping: To qualitatively study the improve-
ment in localization due to PC, we obtain samples from the CUB-200-2011 dataset
and visualize the localization regions returned from Grad-CAM for both the base-
line and PC-trained VGG-16 model. As shown in Figure 3, PC models provide
tighter, more accurate localization around the target object, whereas sometimes
the baseline model has localization driven by image artifacts. Figure 3-(a) has
an example of the types of distractions that are often present in FGVC images
(the cartoon bird on the right). We see that the baseline VGG-16 network pays

14

A. Dubey, O. Gupta, P. Guo, R. Raskar, R. Farrell and N. Naik

Fig. 3. Pairwise Confusion (PC) obtains improved localization performance, as demon-
strated here with Grad-CAM heatmaps of the CUB-200-2011 dataset images (left) with
a VGGNet-16 model trained without PC (middle) and with PC (right). The objects
in (a) and (b) are correctly classiﬁed by both networks, and (c) and (d) are correctly
classiﬁed by PC, but not the baseline network (VGG-16). For all cases, we consistently
observe a tighter and more accurate localization with PC, whereas the baseline VGG-16
network often latches on to artifacts, even while making correct predictions.

signiﬁcant attention to the distraction, despite making the correct prediction.
With PC, we ﬁnd that the attention is limited almost exclusively to the correct
object, as desired. Similarly for Figure 3-(b), we see that the baseline method
latches on to the incorrect bird category, which is corrected by the addition of PC.
In Figures 3-(c-d), we see that the baseline classiﬁer makes incorrect decisions
due to poor localization, mistakes that are resolved by PC.

6 Conclusion

In this work, we introduce Pairwise Confusion (PC), an optimization procedure
to improve generalizability in ﬁne-grained visual classiﬁcation (FGVC) tasks by
encouraging confusion in output activations. PC improves FGVC performance
for a wide class of convolutional architectures while ﬁne-tuning. Our experiments
indicate that PC-trained networks show improved localization performance which
contributes to the gains in classiﬁcation accuracy. PC is easy to implement, does
not need excessive tuning during training, and does not add signiﬁcant overhead
during test time, in contrast to methods that introduce complex localization-
based pooling steps that are often diﬃcult to implement and train. Therefore,
our technique should be beneﬁcial to a wide variety of specialized neural network
models for applications that demand for ﬁne-grained visual classiﬁcation or
learning from limited labeled data.

Acknowledgements: We would like to thank Dr. Ashok Gupta for his
guidance on bird recognition, and Dr. Sumeet Agarwal, Spandan Madan and
Ishaan Grover for their feedback at various stages of this work.

Pairwise Confusion for Fine-Grained Visual
Classiﬁcation : Supplementary Material

Abhimanyu Dubey1, Otkrist Gupta1, Pei Guo2, Ramesh Raskar1, Ryan Farrell2,
and Nikhil Naik1,3

1 Massachusetts Institute of Technology, Cambridge MA 02139, USA
{dubeya,otkrist,raskar,naik}@mit.edu
2 Brigham Young University, Provo UT 84602, USA
peiguo, farrell@cs.byu.edu
3 Harvard University, Cambridge MA 02139, USA
naik@fas.harvard.edu

S1 Proofs for Lemmas from Section 3 in Main Text

S1.1 Equation 4 from Main Text (Behavior of Jeﬀrey’s Divergence)

Consider Jeﬀrey’s divergence with N = 2 classes, and that x1 belongs to class 1,
and x2 belongs to class 2. For a model with parameters θ that correctly identiﬁes
both x1 and x2 by training using cross-entropy loss, pθ(y1|x1) = 1 − δ1 and
pθ(y2|x2) = 1 − δ2, where 0 < δ1, δ2 < 1
2 (since the classiﬁer outputs correct
predictions for the input images), we get:

DJ(pθ(y|x1), pθ(y|x2)) = (1 − δ1 − δ2) · (log(

(1 − δ1)
δ2
+ (δ1 − 1 + δ2) · (log(

))

δ1
(1 − δ2)

))

= (1 − δ1 − δ2) · (log(

+ (1 − δ1 − δ2) · (log(

= (1 − δ1 − δ2) · (log

(1 − δ1)
δ2

))

(1 − δ2)
δ1

))

(1 − δ1)(1 − δ2)
δ1δ2

)

≥ (1 − δ1 − δ2) · (2 log(1 − δ1 − δ2) − log(δ1δ2))

(1)

(2)

(3)

(4)

S1.2 Lemmas 1 and 2 from Main Text (Euclidean Confusion

Bounds)

Lemma 1. On a ﬁnite probability space, for probability measures P, Q:

DEC(P, Q) ≤ DJ(P, Q)

where DJ(P, Q) is the Jeﬀrey’s Divergence between P and Q.

16

A. Dubey, O. Gupta, P. Guo, R. Raskar, R. Farrell and N. Naik

Proof. By the deﬁnition of Euclidean Confusion, we have:

For a ﬁnite-dimensional vector x, (cid:107)x(cid:107)2 ≤ (cid:107)x(cid:107)1, therefore:

DEC(P, Q) =

(p(u) − q(u))2

(cid:88)

u∈U

(cid:88)

u∈U

≤ (

|p(u) − q(u)|)2

= 4DTV(P, Q)2

Since DTV(P, Q) = 1

2 ((cid:80)

u∈U |p(u) − q(u)|) for ﬁnite alphabet U, we have:

Since Total Variation Distance is symmetric, we have:

= 2(DTV(P, Q)2 + DTV(Q, P )2)

(cid:113) 1
2

DKL(P ||Q), and similarly DTV(Q, P ) ≤

By Pinsker’s Inequality, DTV(P, Q) ≤
(cid:113) 1
2

DKL(Q||P ), therefore:

DKL(P ||Q) +

≤ 2

(cid:16) 1
2
= DJ(P, Q)

(cid:17)
DKL(Q||P )

1
2

Lemma 2. On a ﬁnite probability space, for probability measures P, Q:

DEC(P, Q) ≤ 4DTV(P, Q)2

where DTV denotes the total variation distance between P and Q.

Proof. By the deﬁnition of Euclidean Confusion, we have:

For a ﬁnite-dimensional vector x, (cid:107)x(cid:107)2 ≤ (cid:107)x(cid:107)1, therefore:

DEC(P, Q) =

(p(u) − q(u))2

(cid:88)

u∈U

(cid:88)

u∈U

≤ (

|p(u) − q(u)|)2

= 4DTV(P, Q)2

Since DTV(P, Q) = 1

2 ((cid:80)

u∈U |p(u) − q(u)|) for ﬁnite alphabet U, we have:

(5)

(6)

(7)

(8)

(9)

(10)

(11)

(12)

(13)

Pairwise Confusion : Supplementary Material

17

S1.3 Proofs for Lemma 3 and Corollary 1 from the Main Text

(Euclidean Confusion over Sets)

Deﬁnition 1. In a standard classiﬁcation setting with N classes, consider a
training set with m = (cid:80)N
i=1 mi training examples, where mi denotes the number
of training samples for class i. For simplicity of notation, let us denote the set of
conditional probability distributions of all training points belonging to class i for a
model parameterized by θ as Si = {pθ(y|xi
2), ..., pθ(y|xi
)}. Then, for
mi
a model parameterized by θ, the Euclidean Confusion is given by:

1), pθ(y|xi

DEC(Si, Sj; θ) (cid:44) 1

(cid:16)

mi,mj
(cid:88)

mimj

u,v

DEC(pθ(y|xi

u), pθ(y|xj

v))

(cid:17)

(14)

Lemma 3. For sets Si, Sj and DEC(Si, Sj; θ) as deﬁned in Equation (14):

DEN(Si, Sj; θ)2 ≤ DEC(Si, Sj; θ)

1
2

where DEN(Si, Sj; θ) is the Energy Distance under Euclidean norm between Si and
Sj (parameterized by θ), and random vectors are selected with uniform probability
in both Si and Sj.

Proof. From the deﬁnition of Euclidean Confusion, we have:

DEC(Si, Sj; θ) =

DEC(pθ(y|xi

u), pθ(y|xj

v))

1
mimj

=

1
mimj

(cid:16)

mi,mj
(cid:88)

u,v
mi,mj
(cid:88)

(cid:16)

u,v

(cid:17)

(cid:17)

(cid:107)pθ(y|xi

u) − pθ(y|xj

v)(cid:107)2
2

Considering Xi ∼ Uniform(Si), then we get:

=

1
mj

(cid:16)

mj
(cid:88)

v

E[(cid:107)Xi − pθ(y|xj

v)(cid:107)2
2]

(cid:17)

Considering Xj ∼ Uniform(Sj), we obtain:

= E[(cid:107)Xi − Xj(cid:107)2
2]

(15)

(16)

(17)

(18)

Under the squared Euclidean norm distance, the Energy Distance can be given
by:

DEN(Si, Sj; θ)2 = 2E[(cid:107)X − Y (cid:107)2

2] − E[(cid:107)X − X (cid:48)(cid:107)2

2] − E[(cid:107)Y − Y (cid:48)(cid:107)2
2]

(19)

Where random variables X, X (cid:48) ∼ P(Si) and Y, Y (cid:48) ∼ P(Sj). If P(Si) = Uniform(Si),
and P(Sj) = Uniform(Sj), we have by substitution of Equation (18):

1
2

DEN(Si, Sj; θ)2 = DEC(Si, Sj; θ) −

E[(cid:107)X − X (cid:48)(cid:107)2

2] + E[(cid:107)Y − Y (cid:48)(cid:107)2
2]

(20)

(cid:17)

(cid:16)

1
2

1
2

1
2

1
2

18

A. Dubey, O. Gupta, P. Guo, R. Raskar, R. Farrell and N. Naik

Since (cid:107)x − y(cid:107)2
Therefore, we have:

2 ≥ 0 ∀x ∈ X , y ∈ Y; Ex∼X ,y∼Y [(cid:107)x − y(cid:107)2

2] ≥ 0 ∀ ﬁnite sets X , Y.

DEN(Si, Sj; θ)2 ≤ DEC(Si, Sj; θ)

(21)

Corollary 1. For sets Si, Sj and DEC(Si, Sj; θ) as deﬁned in Equation (14), we
have:

DEC(Si, Si; θ) + DEC(Sj, Sj; θ) ≤ 2DEC(Si, Sj; θ)

with equality only when Si = Sj.
Proof. From Equation (20), we have:

(cid:16)

1
2

(cid:16)

1
2

DEN(Si, Sj; θ)2 = DEC(Si, Sj; θ) −

E[(cid:107)X − X (cid:48)(cid:107)2

2] + E[(cid:107)Y − Y (cid:48)(cid:107)2
2]

(22)

From Equation (18), we have:

DEC(Si, Sj; θ) = E[(cid:107)Xi − Xj(cid:107)2
2]

(23)

For Si = Sj, we have with Xi, Xj ∼ Uniform(Si):

DEC(Si, Si; θ) = E[(cid:107)Xi − Xj(cid:107)2
2]
(24)
Replacing this in Equation (20), we have with X, X (cid:48) ∼ Uniform(Si) and Y, Y (cid:48) ∼
Uniform(Sj):

DEN(Si, Sj; θ)2 = DEC(Si, Sj; θ) −

E[(cid:107)X − X (cid:48)(cid:107)2

2] + E[(cid:107)Y − Y (cid:48)(cid:107)2
2]

= DEC(Si, Sj; θ) −

(cid:17)
(cid:16)
DEC(Si, Si; θ) + DEC(Sj, Sj; θ)

1
2

From Szekely et al. [43], we know that the Energy Distance ≥ 0 with equality if
and only if Si = Sj. Thus, we have that:

DEC(Si, Si; θ) + DEC(Sj, Sj; θ) ≤ 2DEC(Si, Sj; θ)

(25)

(26)

(27)

(cid:17)

(cid:17)

With equality only when Si = Sj.

S2 Training Details

In this section, we describe the process for training with Pairwise Confusion
for diﬀerent base architectures, including the list of hyperparameters using for
diﬀerent datasets.

ResNet-50: In all experiments, we train for 40000 iterations with batch-size
8, with a linear decay of the learning rate from an initial value of 0.1. The
hyperparameter for the confusion term for each dataset is given in Table S1.

Bilinear and Compact Bilinear CNN: In all experiments, we use the
training procedure described by the authors4. In addition, we repeat the described
step 2 without the loss on confusion from the obtained weights after performing
Step 2 with the loss, and obtain an additional 0.5 percent gain in performance.
The hyperparameter for the confusion term for each dataset is given in Table S2.

4 https://github.com/gy20073/compact_bilinear_pooling/tree/master/

caffe-20160312/examples/compact_bilinear

Pairwise Confusion : Supplementary Material

19

Table S1. Regularization parameter λ for ResNet-50 experiments.

λ
Dataset
10
CUB2011
NABirds
15
Stanford Dogs 10
10
Cars
10
Flowers-102
15
Aircraft

λ
Dataset
20
CUB2011
NABirds
20
Stanford Dogs 10
10
Cars
10
Flowers-102
10
Aircraft

λ
Dataset
10
CUB2011
NABirds
15
Stanford Dogs 10
15
Cars
10
Flowers-102
15
Aircraft

Table S2. Regularization parameter λ for Bilinear CNN experiments.

DenseNet-161: In all experiments, we train for 40000 iterations with batch-
size 32, with a linear decay of the learning rate from an initial value of 0.1. The
hyperparameter for the confusion term for each dataset is given in Table S3.

Table S3. Regularization parameter λ for DenseNet-161 experiments.

GoogLeNet: In all experiments, we train for 300000 iterations with batch-size
32, with a step size of 30000, decreasing it by a ratio of 0.96. The hyperparameter
for the confusion term is given in Table S4.

VGGNet-16: In all experiments, we train for 40000 iterations with batch-
size 32, with a linear decay of the learning rate from an initial value of 0.1. The
hyperparameter for the confusion term is given in Table S5.

20

A. Dubey, O. Gupta, P. Guo, R. Raskar, R. Farrell and N. Naik

Table S4. Regularization parameter λ for GoogLeNet experiments.

Dataset
λ
CUB-200-2011 10
NABirds
20
Stanford Dogs 10
10
Cars
10
Flowers-102
15
Aircraft

λ
Dataset
15
CUB2011
NABirds
15
Stanford Dogs 10
10
Cars
10
Flowers-102
15
Aircraft

Table S5. Regularization parameter λ for VGGNet-16 experiments.

S3 Mean and Standard Deviation for FGVC Results

In Table S6, we provide the mean and standard deviation values over ﬁve
independent runs for training with Pairwise Confusion with diﬀerent baseline
models. These results correspond to Table 2 in the main text.

S4 Comparison with Regularization

We additionally compare the performance of our optimization technique with
other regularization methods as well. We ﬁrst compare Pairwise Comparison
with with Label-Smoothing Regularization (LSR) on all six FGVC datasets for
VGG-Net16, ResNet-50 and DenseNet-161. These results are summarized in
Table S7. Next, in Table S8, we compare the performance of Pairwise Confusion
(PC) with several additional regularization techniques on the CIFAR-10 and
CIFAR-100 datasets using two small architectures: CIFAR-10 Quick (C10Quick)
and CIFAR-10 Full (C10Full), which are standard models available in the Caﬀe
framework.

S5 Changes to Class-wise Prediction Accuracy

We ﬁnd that while the average and lowest per-class accuracy increase when
training with PC, there is a small decline in top-performing class accuracy (See
Table S9). Moreover, the standard deviation in per-class accuracy is reduced as
well. We also found that using PC slightly increased false positive errors while
obtaining a larger reduction in false negative errors. For example, on CUB-200-
2011 with ResNet-50, the average false positive error is increased by 0.06%, but

Pairwise Confusion : Supplementary Material

21

(A) CUB-200-2011

Top-1
Method
68.19 (0.39)
GoogLeNet
72.65 (0.47)
PC-GoogLeNet
78.15 (0.19)
ResNet-50
80.21 (0.21)
PC-ResNet-50
73.28 (0.41)
VGGNet16
PC-VGGNet16
76.48 (0.43)
Bilinear CNN [1] 84.10 (0.19)
PC-BilinearCNN 85.58 (0.28)
DenseNet-161
84.21 (0.27)
PC-DenseNet-161 86.87 (0.35)

(B) Cars

(C) Aircrafts

Top-1
Method
85.65 (0.14)
GoogLeNet
86.91 (0.16)
PC-GoogLeNet
91.71 (0.22)
ResNet-50
93.43 (0.24)
PC-ResNet-50
80.60 (0.39)
VGGNet16
PC-VGGNet16
83.16 (0.32)
Bilinear CNN [1] 91.20 (0.18)
PC-Bilinear CNN 92.45 (0.23)
DenseNet-161
91.83 (0.16)
PC-DenseNet-161 92.86 (0.18)

Top-1
Method
74.04 (0.51)
GoogLeNet
78.86 (0.37)
PC-GoogLeNet
81.19 (0.28)
ResNet-50
83.40 (0.25)
PC-ResNet-50
74.17 (0.21)
VGGNet16
PC-VGGNet16
77.20 (0.24)
Bilinear CNN [1] 84.10 (0.11)
PC-Bilinear CNN 85.78 (0.13)
DenseNet-161
86.30 (0.35)
PC-DenseNet-161 89.24 (0.32)

(D) NABirds

(E) Flowers-102

(E) Stanford Dogs

Top-1
Method
70.66 (0.17)
GoogLeNet
72.01 (0.14)
PC-GoogLeNet
63.55 (0.28)
ResNet-50
68.15 (0.31)
PC-ResNet-50
68.34 (0.19)
VGGNet16
72.25 (0.25)
PC-VGGNet16
Bilinear CNN [1] 80.90 (0.09)
PC-Bilinear CNN 82.01 (0.12)
DenseNet-161
79.35 (0.25)
PC-DenseNet-161 82.79 (0.20)

Top-1
Method
82.55 (0.11)
GoogLeNet
83.03 (0.15)
PC-GoogLeNet
92.46 (0.14)
ResNet-50
93.50 (0.12)
PC-ResNet-50
85.15 (0.08)
VGGNet16
86.19 (0.07)
PC-VGGNet16
Bilinear CNN [1] 92.52 (0.13)
PC-Bilinear CNN 93.65 (0.18)
DenseNet-161
90.07 (0.17)
PC-DenseNet-161 91.39 (0.15)

Top-1
Method
55.76 (0.36)
GoogLeNet
60.61 (0.29)
PC-GoogLeNet
69.92 (0.32)
ResNet-50
73.35 (0.33)
PC-ResNet-50
61.92 (0.40)
VGGNet16
65.51 (0.42)
PC-VGGNet16
Bilinear CNN [1] 82.13 (0.12)
PC-Bilinear CNN 83.04 (0.09)
DenseNet-161
81.18 (0.27)
PC-DenseNet-161 83.75(0.28)

Table S6. Pairwise Confusion (PC) obtains state-of-the-art performance on six widely-
used ﬁne-grained visual classiﬁcation datasets (A-F). Improvement over the baseline
model is reported as (∆). All results averaged over 5 trials with standard deviations
reported in parentheses.

Method

CUB-200-2011 Cars Aircrafts NABirds Flowers-102 Stanford Dogs

VGG-Net16

ResNet-50

DenseNet-161

PC
LSR
PC
LSR
PC
LSR

72.65
70.03
80.21
78.20
86.87
84.86

83.16
81.45
93.43
92.04
92.86
91.96

77.20
75.06
83.40
81.26
89.24
87.05

72.25
69.28
68.15
64.02
82.79
80.11

86.19
83.98
93.50
92.48
91.39
90.24

65.51
63.06
73.35
70.03
83.75
85.68

Table S7. Comparison with Label Smoothing Regularization (LSR) [42].

CIFAR-10 on C10Full
Test
100.00 (0.00) 75.54 (0.17) 24.46 (0.23) 95.15 (0.65) 81.45 (0.22) 14.65 (0.17) 100.00 (0.03) 42.41 (0.16) 57.59 (0.29)
Weight-decay [54] 100.00 (0.00) 75.61 (0.18) 24.51 (0.34) 95.18 (0.19) 81.53 (0.21) 14.73 (0.20) 100.00 (0.05) 42.87 (0.19) 57.13 (0.27)

CIFAR-100 on C10Quick
Test

CIFAR-10 on C10Quick
Test

Method
None

Train

Train

Train

∆

∆

∆

DeCov [55] 5
Dropout [56]
PC
PC + Dropout

-
88.78 (0.23) 79.75 (0.17) 8.04 (0.16)
99.5 (0.12) 79.41 (0.12) 20.09 (0.34) 92.15 (0.19) 82.40 (0.14) 9.81 (0.25) 75.00 (0.11) 45.89 (0.14) 29.11 (0.20)
92.25 (0.14) 80.51 (0.20) 10.74 (0.28) 93.88 (0.21) 82.67 (0.12) 11.21 (0.34) 72.12 (0.05) 46.72 (0.12) 25.50 (0.14)
93.04 (0.19) 81.13 (0.22) 11.01 (0.32) 93.85 (0.23) 83.57 (0.20) 10.28 (0.27) 71.15 (0.12) 49.22 (0.08) 21.93 (0.22)

72.53

45.10

27.43

-

-

Table S8. Image classiﬁcation performance and train-val gap (∆)) for Pairwise Confu-
sion (PC) and popular regularization methods. The standard deviation across trials is
mentioned in parentheses.

the average false negative error is reduced by 0.13%. So while some additional
mistakes are made in terms of false positives, we curb/reduce the problem of
classiﬁer overconﬁdence by a larger margin.

22

A. Dubey, O. Gupta, P. Guo, R. Raskar, R. Farrell and N. Naik

Class Accuracy Best Worst Mean Std. Dev.
Baseline
PC

91.14 68.34 78.15
90.67 70.95 80.21

5.12
4.22

Table S9. Class-wise Performance Comparison on CUB-200-211 for ResNet-50

References

1. Lin, T.Y., RoyChowdhury, A., Maji, S.: Bilinear cnn models for ﬁne-grained visual
recognition. IEEE International Conference on Computer Vision (2015) 1449–1457
2. Jaderberg, M., Simonyan, K., Zisserman, A., Kavukcuoglu, K.: Spatial transformer
networks. Advances in Neural Information Processing Systems (2015) 2017–2025
3. Zhang, Y., Wei, X.S., Wu, J., Cai, J., Lu, J., Nguyen, V.A., Do, M.N.: Weakly
supervised ﬁne-grained categorization with part-based image representation. IEEE
Transactions on Image Processing 25(4) (2016) 1713–1725

4. Krause, J., Jin, H., Yang, J., Fei-Fei, L.: Fine-grained recognition without part
annotations. IEEE Conference on Computer Vision and Pattern Recognition (2015)
5546–5555

5. Zhang, N., Shelhamer, E., Gao, Y., Darrell, T.: Fine-grained pose prediction, nor-
malization, and recognition. International Conference on Learning Representations
Workshops (2015)

6. Krause, J., Sapp, B., Howard, A., Zhou, H., Toshev, A., Duerig, T., Philbin, J.,
Fei-Fei, L.: The unreasonable eﬀectiveness of noisy data for ﬁne-grained recognition.
European Conference on Computer Vision (2016) 301–320

7. Cui, Y., Zhou, F., Lin, Y., Belongie, S.: Fine-grained categorization and dataset
bootstrapping using deep metric learning with humans in the loop. IEEE Conference
on Computer Vision and Pattern Recognition (2016)

8. Lin, T.Y., Maji, S.:

Improved bilinear pooling with cnns.

arXiv preprint

arXiv:1707.06772 (2017)

9. Cui, Y., Zhou, F., Wang, J., Liu, X., Lin, Y., Belongie, S.: Kernel pooling for
convolutional neural networks. IEEE Conference on Computer Vision and Pattern
Recognition (2017)

10. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale
hierarchical image database. IEEE Conference on Computer Vision and Pattern
Recognition (2009) 248–255

11. Huang, G., Liu, Z., van der Maaten, L., Weinberger, K.Q.: Densely connected convo-
lutional networks. IEEE Conference on Computer Vision and Pattern Recognition
(2017)

12. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
IEEE Conference on Computer Vision and Pattern Recognition (2016) 770–778
13. Yao, B., Khosla, A., Fei-Fei, L.: Combining randomization and discrimination
for ﬁne-grained image categorization. IEEE Conference on Computer Vision and
Pattern Recognition (2011) 1577–1584

14. Yao, B., Bradski, G., Fei-Fei, L.: A codebook-free and annotation-free approach
for ﬁne-grained image categorization. IEEE Conference on Computer Vision and
Pattern Recognition (2012) 3466–3473

5 Due to the lack of publicly available software implementations of DeCov, we are

unable to report the performance of DeCov on CIFAR-10 Full.

Pairwise Confusion : Supplementary Material

23

15. Zhang, N., Donahue, J., Girshick, R., Darrell, T.: Part-based r-cnns for ﬁne-grained
category detection. European Conference on Computer Vision (2014) 834–849
16. Gao, Y., Beijbom, O., Zhang, N., Darrell, T.: Compact bilinear pooling. IEEE

Conference on Computer Vision and Pattern Recognition (2016) 317–326

17. Wang, Y., Choi, J., Morariu, V., Davis, L.S.: Mining discriminative triplets of
patches for ﬁne-grained classiﬁcation. IEEE Conference on Computer Vision and
Pattern Recognition (June 2016)

18. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object
detection with region proposal networks. Advances in neural information processing
systems (2015) 91–99

19. Branson, S., Van Horn, G., Belongie, S., Perona, P.: Bird species categorization
using pose normalized deep convolutional nets. British Machine Vision Conference
(2014)

20. Zhang, N., Farrell, R., Darrell, T.: Pose pooling kernels for sub-category recognition.

IEEE Computer Vision and Pattern Recognition (2012) 3665–3672

21. Moghimi, M., Saberian, M., Yang, J., Li, L.J., Vasconcelos, N., Belongie, S.: Boosted

convolutional neural networks. (2016)

22. Chopra, S., Hadsell, R., LeCun, Y.: Learning a similarity metric discriminatively,
with application to face veriﬁcation. IEEE Conference on Computer Vision and
Pattern Recognition (2005) 539–546

23. Parikh, D., Grauman, K.: Relative attributes. IEEE International Conference on

Computer Vision (2011) 503–510

24. Dubey, A., Agarwal, S.: Modeling image virality with pairwise spatial transformer

networks. arXiv preprint arXiv:1709.07914 (2017)

25. Souri, Y., Noury, E., Adeli, E.: Deep relative attributes. Asian Conference on

Computer Vision (2016) 118–133

26. Dubey, A., Naik, N., Parikh, D., Raskar, R., Hidalgo, C.A.: Deep learning the city:
Quantifying urban perception at a global scale. European Conference on Computer
Vision (2016) 196–212

27. Singh, K.K., Lee, Y.J.: End-to-end localization and ranking for relative attributes.

European Conference on Computer Vision (2016) 753–769

28. Reed, S., Lee, H., Anguelov, D., Szegedy, C., Erhan, D., Rabinovich, A.: Train-
ing deep neural networks on noisy labels with bootstrapping. arXiv preprint
arXiv:1412.6596 (2014)

29. Xiao, T., Xia, T., Yang, Y., Huang, C., Wang, X.: Learning from massive noisy
labeled data for image classiﬁcation. IEEE Conference on Computer Vision and
Pattern Recognition (2015) 2691–2699

30. Neelakantan, A., Vilnis, L., Le, Q.V., Sutskever, I., Kaiser, L., Kurach, K., Martens,
J.: Adding gradient noise improves learning for very deep networks. arXiv preprint
arXiv:1511.06807 (2015)

31. Szegedy, C., Vanhoucke, V., Ioﬀe, S., Shlens, J., Wojna, Z.: Rethinking the inception
architecture for computer vision. IEEE Conference on Computer Vision and Pattern
Recognition (2016) 2818–2826

32. Nilsback, M.E., Zisserman, A.: Automated ﬂower classiﬁcation over a large number
of classes. Indian Conference on Computer Vision, Graphics & Image Processing
(2008) 722–729

33. Wah, C., Branson, S., Welinder, P., Perona, P., Belongie, S.: The caltech-ucsd

birds-200-2011 dataset. (2011)

34. Krause, J., Stark, M., Deng, J., Fei-Fei, L.: 3d object representations for ﬁne-grained
categorization. IEEE International Conference on Computer Vision Workshops
(2013) 554–561

24

A. Dubey, O. Gupta, P. Guo, R. Raskar, R. Farrell and N. Naik

35. Van Horn, G., Branson, S., Farrell, R., Haber, S., Barry, J., Ipeirotis, P., Perona, P.,
Belongie, S.: Building a bird recognition app and large scale dataset with citizen
scientists: The ﬁne print in ﬁne-grained dataset collection. IEEE Conference on
Computer Vision and Pattern Recognition (2015) 595–604

36. Maji, S., Rahtu, E., Kannala, J., Blaschko, M., Vedaldi, A.: Fine-grained visual

classiﬁcation of aircraft. arXiv preprint arXiv:1306.5151 (2013)

37. Khosla, A., Jayadevaprakash, N., Yao, B., Li, F.F.: Novel dataset for ﬁne-grained
image categorization: Stanford dogs. IEEE International Conference on Computer
Vision Workshops on Fine-Grained Visual Categorization (2011) 1
38. Krizhevsky, A., Nair, V., Hinton, G.: The cifar-10 dataset otkrist (2014)
39. Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., Ng, A.Y.: Reading digits
in natural images with unsupervised feature learning. NIPS workshop on deep
learning and unsupervised feature learning (2) (2011) 5
40. Jeﬀreys, H.: The theory of probability. OUP Oxford (1998)
41. Kullback, S., Leibler, R.A.: On information and suﬃciency. The annals of mathe-

matical statistics 22(1) (1951) 79–86

42. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,
Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. IEEE Conference
on Computer Vision and Pattern Recognition (2015) 1–9

43. Sz´ekely, G.J., Rizzo, M.L.: Energy statistics: A class of statistics based on distances.

Journal of statistical planning and inference 143(8) (2013) 1249–1272

44. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale

image recognition. arXiv preprint arXiv:1409.1556 (2014)

45. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama,
S., Darrell, T.: Caﬀe: Convolutional architecture for fast feature embedding. ACM
international conference on Multimedia (2014) 675–678

46. Paskze, A., Chintala, S.: Tensors and Dynamic neural networks in Python with
strong GPU acceleration. https://github.com/pytorch Accessed: [January 1,
2017].

47. Zhang, X., Xiong, H., Zhou, W., Lin, W., Tian, Q.: Picking deep ﬁlter responses for
ﬁne-grained image recognition. IEEE Conference on Computer Vision and Pattern
Recognition (2016) 1134–1142

48. Liu, M., Yu, C., Ling, H., Lei, J.: Hierarchical joint cnn-based models for ﬁne-grained
cars recognition. International Conference on Cloud Computing and Security (2016)
337–347

49. Simon, M., Gao, Y., Darrell, T., Denzler, J., Rodner, E.: Generalized orderless
pooling performs implicit salient matching. International Conference on Computer
Vision (ICCV) (2017)

50. Kong, S., Fowlkes, C.: Low-rank bilinear pooling for ﬁne-grained classiﬁcation.
IEEE Conference on Computer Vision and Pattern Recognition (2017) 7025–7034
51. Angelova, A., Zhu, S.: Eﬃcient object detection and segmentation for ﬁne-grained
recognition. IEEE Conference on Computer Vision and Pattern Recognition (2013)
811–818

52. Sharif Razavian, A., Azizpour, H., Sullivan, J., Carlsson, S.: CNN features oﬀ-
the-shelf: An astounding baseline for recognition. IEEE Conference on Computer
Vision and Pattern Recognition Workshops (June 2014)

53. Selvaraju, R.R., Das, A., Vedantam, R., Cogswell, M., Parikh, D., Batra, D.:
Grad-cam: Why did you say that? visual explanations from deep networks via
gradient-based localization. arXiv preprint arXiv:1610.02391 (2016)

54. Krogh, A., Hertz, J.A.: A simple weight decay can improve generalization. NIPS 4

(1991) 950–957

Pairwise Confusion : Supplementary Material

25

55. Cogswell, M., Ahmed, F., Girshick, R., Zitnick, L., Batra, D.: Reducing overﬁtting
in deep networks by decorrelating representations. arXiv preprint arXiv:1511.06068
(2015)

56. Srivastava, N., Hinton, G.E., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.:
Dropout: a simple way to prevent neural networks from overﬁtting. Journal of
Machine Learning Research 15(1) (2014) 1929–1958

