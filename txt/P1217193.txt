7
1
0
2
 
r
p
A
 
1
 
 
]

G
L
.
s
c
[
 
 
3
v
2
1
0
7
0
.
1
1
6
1
:
v
i
X
r
a

GRAM: Graph-based Attention Model for Healthcare
Representation Learning

Edward Choi∗, Mohammad Taha Bahadori∗, Le Song∗, Walter F. Stewart†, Jimeng Sun∗

∗ Georgia Institute of Technology

† Sutter Health

{mp2893,bahadori}@gatech.edu, lsong@cc.gatech.edu, stewarwf@sutterhealth.org, jsun@cc.gatech.edu

Abstract

Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two

important challenges remain:

• Data insuﬃciency: Often in healthcare predictive modeling, the sample size is insuﬃcient for deep

learning methods to achieve satisfactory results.

• Interpretation: The representations learned by deep learning methods should align with medical

knowledge.

To address these challenges, we propose a GRaph-based Attention Model, GRAM that supplements electronic
health records (EHR) with hierarchical information inherent to medical ontologies. Based on the data
volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in
the ontology via an attention mechanism.

We compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various
methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and
one heart failure prediction task. Compared to the basic RNN, GRAM achieved 10% higher accuracy for
predicting diseases rarely observed in the training data and 3% improved area under the ROC curve
for predicting heart failure using an order of magnitude less training data. Additionally, unlike other
methods, the medical concept representations learned by GRAM are well aligned with the medical ontology.
Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts
when facing data insuﬃciency at the lower level concepts.

1

Introduction

The rapid growth in volume and diversity of health care data from electronic health records (EHR) and other
sources is motivating the use of predictive modeling to improve care for individual patients. In particular,
novel applications are emerging that use deep learning methods such as word embedding (Choi et al., 2016c,d),
recurrent neural networks (RNN) (Che et al., 2016; Choi et al., 2016a,b; Lipton et al., 2016), convolutional
neural networks (CNN) (Nguyen et al., 2016) or stacked denoising autoencoders (SDA) (Che et al., 2015;
Miotto et al., 2016), demonstrating signiﬁcant performance enhancement for diverse prediction tasks. Deep
learning models appear to perform signiﬁcantly better than logistic regression or multilayer perceptron (MLP)
models that depend, to some degree, on expert feature construction (Lipton et al., 2015; Razavian et al.,
2016).

Training deep learning models typically requires large amounts of data that often cannot be met by a single
health system or provider organization. Sub-optimal model performance can be particularly challenging when
the focus of interest is predicting onset of a rare disease. For example, using Doctor AI (Choi et al., 2016a), we
discovered that RNN alone was ineﬀective to predict the onset of diseases such as cerebral degenerations (e.g.
Leukodystrophy, Cerebral lipidoses) or developmental disorders (e.g. autistic disorder, Heller’s syndrome),
partly because their rare occurrence in the training data provided little learning opportunity to the ﬂexible
models like RNN.

1

Figure 1: The illustration of GRAM. Leaf nodes (solid circles) represents a medical concept in the EHR, while
the non-leaf nodes (dotted circles) represent more general concepts. The ﬁnal representation gi of the leaf
concept ci is computed by combining the basic embeddings ei of ci and eg, ec and ea of its ancestors cg, cc
and ca via an attention mechanism. The ﬁnal representations form the embedding matrix G for all leaf
concepts. After that, we use G to embed patient visit vector xt to a visit representation vt, which is then fed
to a neural network model to make the ﬁnal prediction ˆyt.

The data requirement of deep learning models comes from having to assess exponential number of
combinations of input features. This can be alleviated by exploiting medical ontologies that encodes
hierarchical clinical constructs and relationships among medical concepts. Fortunately, there are many
well-organized ontologies in healthcare such as the International Classiﬁcation of Diseases (ICD), Clinical
Classiﬁcations Software (CCS) (Stearns et al., 2001) or Systematized Nomenclature of Medicine-Clinical
Terms (SNOMED-CT) (Project et al., 2010). Nodes (i.e. medical concepts) close to one another in medical
ontologies are likely to be associated with similar patients, allowing us to transfer knowledge among them.
Therefore, proper use of medical ontologies will be helpful when we lack enough data for the nodes in the
ontology to train deep learning models.

In this work, we propose GRAM, a method that infuses information from medical ontologies into deep
learning models via neural attention. Considering the frequency of a medical concept in the EHR data and
its ancestors in the ontology, GRAM decides the representation of the medical concept by adaptively combining
its ancestors via attention mechanism. This will not only support deep learning models to learn robust
representations without large amount of data, but also learn interpretable representations that align well
with the knowledge from the ontology. The attention mechanism is trained in an end-to-end fashion with
the neural network model that predicts the onset of disease(s). We also propose an eﬀective initialization
technique in addition to the ontological knowledge to better guide the representation learning process.

We compare predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various models
including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart
failure (HF) prediction task. We demonstrate that GRAM is up to 10% more accurate than the basic RNN
for predicting diseases less observed in the training data. After discussing GRAM’s scalability, we visualize
the representations learned from various models, where GRAM provides more intuitive representations by
grouping similar medical concepts close to one another. Finally, we show GRAM’s attention mechanism can
be interpreted to understand how it assigns the right amount of attention to the ancestors of each medical
concept by considering the data availability and the ontology structure.

2 Methodology

We ﬁrst deﬁne the notations describing EHR data and medical ontologies, followed by a description of GRAM
(Section 2.2), the end-to-end training of the attention generation and predictive modeling (Section 2.3), and
the eﬃcient initialization scheme (Section 2.4).

2

2.1 Basic Notation

We denote the set of entire medical codes from the EHR as c1, c2, . . . , c|C| ∈ C with the vocabulary size |C|.
The clinical record of each patient can be viewed as a sequence of visits V1, . . . , VT where each visit contains a
subset of medical codes Vt ⊆ C. Vt can be represented as a binary vector xt ∈ {0, 1}|C| where the i-th element
is 1 only if Vt contains the code ci. To avoid clutter, all algorithms will be presented for a single patient.

We assume that a given medical ontology G typically expresses the hierarchy of various medical concepts in
the form of a parent-child relationship, where the medical codes C form the leaf nodes. Ontology G is represented
as a directed acyclic graph (DAG) whose nodes form a set D = C +C(cid:48). The set C(cid:48) = {c|C|+1, c|C|+2, . . . , c|C|+|C(cid:48)|}
consists of all non-leaf nodes (i.e. ancestors of the leaf nodes), where |C(cid:48)| represents the number of all non-leaf
nodes. We use knowledge DAG to refer to G. A parent in the knowledge DAG G represents a related but
more general concept over its children. Therefore, G provides a multi-resolution view of medical concepts with
diﬀerent degrees of speciﬁcity. While some ontologies are exclusively expressed as parent-child hierarchies
(e.g. ICD-9, CCS), others are not. For example, in some instances SNOMED-CT also links medical concepts
to causal or treatment relationships, but the majority relationships in SNOMED-CT are still parent-child.
Therefore, we focus on the parent-child relationships in this work.

2.2 Knowledge DAG and the Attention Mechanism

GRAM leverages the parent-child relationship of G to learn robust representations when data volume is
constrained. GRAM balances the use of ontology information in relation to data volume in determining the
level of speciﬁcity for a medical concept. When a medical concept is less observed in the data, more weight is
given to its ancestors as they can be learned more accurately and oﬀer general (coarse-grained) information
about their children. The process of resorting to the parent concepts can be automated via the attention
mechanism and the end-to-end training as described in Figure 1.

In the knowledge DAG, each node ci is assigned a basic embedding vector ei ∈ Rm, where m represents the
dimensionality. Then e1, . . . , e|C| are the basic embeddings of the codes c1, . . . , c|C| while e|C|+1, . . . , e|C|+|C(cid:48)|
represent the basic embeddings of the internal nodes c|C|+1, . . . , c|C|+|C(cid:48)|. The initialization of these basic
embeddings is described in Section 2.4. We formulate a leaf node’s ﬁnal representation as a convex combination
of the basic embeddings of itself and its ancestors:

(cid:88)

gi =

αijej,

(cid:88)

j∈A(i)

j∈A(i)

αij = 1, αij ≥ 0 for j ∈ A(i),

where gi ∈ Rm denotes the ﬁnal representation of the code ci, A(i) the indices of the code ci and ci’s
ancestors, ej the basic embedding of the code cj and αij ∈ R the attention weight on the embedding ej when
calculating gi. The attention weight αij in Eq. (1) is calculated by the following Softmax function,

f (ei, ej) is a scalar value representing the compatibility between the basic embeddings of ei and ek. We
compute f (ei, ej) via the following feed-forward network with a single hidden layer (MLP),

αij =

(cid:80)

exp(f (ei, ej))
k∈A(i) exp(f (ei, ek))

f (ei, ej) = u(cid:62)

a tanh(Wa

(cid:21)

(cid:20) ei
ej

+ ba)

where Wa ∈ Rl×2m is the weight matrix for the concatenation of ei and ej, b ∈ Rl the bias vector, and
ua ∈ Rl the weight vector for generating the scalar value. The constant l represents the dimension size
of the hidden layer of f (·, ·). We always concatenate ei and ej in the child-ancestor order. Note that the
compatibility function f is an MLP, because MLP is well known to be a suﬃcient approximator for an
arbitrary function, and we empirically found that our formulation performed better in our use cases than
alternatives such as inner product and Bahdanau et al.’s (Bahdanau et al., 2014).

3

(1)

(2)

(3)

Remarks: The example in Figure 1 is derived based on a single path from ci to ca. However, the same
mechanism can be applicable to multiple paths as well. For example, code ck has two paths to the root ca,
containing ﬁve ancestors in total. Another scenario is where the EHR data contain both leaf codes and some
ancestor codes. We can move those ancestors present in EHR data from the set C(cid:48) to C and apply the same
process as Eq. (1) to obtain the ﬁnal representations for them.

2.3 End-to-End Training with a Predictive Model

We train the attention mechanism together with a predictive model such that the attention mechanism
improves the predictive performance. By concatenating ﬁnal representation g1, g2, . . . , g|C| of all medical
codes, we have the embedding matrix G ∈ Rm×|C| where gi is its i-th column of G. We can then convert
visit Vt to a visit representation vt by multiplying embedding matrix G with multi-hot vector xt indicating
the clinical events in visit Vt as shown in the right side of Figure 1. Finally the visit representation vt will
be used as an input to pass to a predictive model for predicting the target label yt using a neural network
(NN) model. In this work, we use RNN as the choice of the NN model as the task is to perform sequential
diagnoses prediction (Choi et al., 2016a,b) with the objective of predicting the disease codes of the next visit
Vt+1 given the visit records up to the current timestep V1, V2, . . . , Vt, which can be expressed as follows,

(cid:98)yt = (cid:98)xt+1 = Softmax(Wht + b), where
h1, h2, . . . , ht = RNN(v1, v2, . . . , vt; θr), where
v1, v2, . . . , vt = tanh(G[x1, x2, . . . , xt])

(4)

(5)

where xt ∈ R|C| denotes the t-th visit; vt ∈ Rm the t-th visit representation; ht ∈ Rr the RNN’s hidden layer
at t-th time step (i.e. t-th visit); θr RNN’s parameters; W ∈ R|C|×r and b ∈ R|C| the weight matrices and
the bias vector of the Softmax function; r denotes the dimension size of the hidden layer. We use “RNN” to
denote any recurrent neural network variants that can cope with the vanishing gradient problem (Bengio
et al., 1994), such as LSTM (Hochreiter and Schmidhuber, 1997), GRU (Cho et al., 2014), and IRNN (Le
et al., 2015), with any varying numbers of hidden layers. The prediction loss for all time steps is calculated
using the binary cross entropy as follows,

L(x1, x2 . . . , xT ) = −

1
T − 1

T −1
(cid:88)

(cid:16)

t=1

(cid:17)
(cid:62) log((cid:98)yt) + (1 − yt)(cid:62) log(1 − (cid:98)yt)

yt

where we sum the cross entropy errors from all timestamps of (cid:98)yt, T denotes the number of timestamps of
the visit sequence. Note that the above loss is deﬁned for a single patient. In actual implementation, we
will take the average of the individual loss for multiple patients. Algorithm 1 describes the overall training
procedure of GRAM, under the assumption that we are performing the sequential diagnoses prediction task
using an RNN. Note that Algorithm 1 describes stochastic gradient update to avoid clutter, but it can be
easily extended to other gradient based optimization such as mini-batch gradient update.

2.4

Initializing Basic Embeddings

The attention generation mechanism in Section 2.2 requires basic embeddings ei of each node in the knowledge
DAG. The basic embeddings of ancestors, however, pose a diﬃculty because they are often not observed in
the data. To properly initialize them, we use co-occurrence information to learn the basic embeddings of
medical codes and their ancestors. Co-occurrence has proven to be an important source of information when
learning representations of words or medical concepts (Mikolov et al., 2013; Choi et al., 2016c,d). To train
the basic embeddings, we employ GloVe (Pennington et al., 2014), which uses the global co-occurrence matrix
of words to learn their representations. In our case, the co-occurrence matrix of the codes and the ancestors
was generated by counting the co-occurrences within each visit Vt, where we augment each visit with the
ancestors of the codes in the visit. We describe the details of the initialization algorithm with an example.
We borrow the parent-child relationships from the knowledge DAG of Figure 1. Given a visit Vt,

4

Algorithm 1 GRAM Optimization

Randomly initialize basic embedding matrix E, attention parameters ua, Wa, ba, RNN parameter θr,
softmax parameters W, b.
repeat

Update E with GloVe objective function (see Section 2.4)

until convergence
repeat

X ← random patient from dataset
for visit Vt in X do

for code ci in Vt do

Refer G to ﬁnd ci’s ancestors C (cid:48)
for code cj in C (cid:48) do

Calculate attention weight αij using Eq. (2).

end for
Obtain ﬁnal representation gi using Eq. (1).

end for
vt ← tanh((cid:80)
i:ci∈Vt
Make prediction (cid:98)yt using Eq. (4)

gi)

end for
Calculate prediction loss L using Eq .(5)
Update parameters according to the gradient of L

until convergence

we augment it with the ancestors of all the codes to obtain the augmented visit V (cid:48)
t ,

V (cid:48)
t = {cd, cb, ca, ci, cg, cc, ca, ck, cj, cf , cc, cb, ca}

Vt = {cd, ci, ck}

where the augmented ancestors are underlined. Note that a single ancestor can appear multiple times in V (cid:48)
t .
In fact, the higher the ancestor is in the knowledge DAG, the more times it is likely to appear in V (cid:48)
t . We
count the co-occurrence of two codes in V (cid:48)

t as follows,

co-occurrence(ci, cj, V (cid:48)

t ) = count(ci, V (cid:48)

t ) × count(cj, V (cid:48)
t )

where count(ci, V (cid:48)
t ) is the number of times the code ci appears in the augmented visit V (cid:48)
t . For example, the
co-occurrence between the leaf code ci and the root ca is 3. However, the co-occurrence between the ancestor
cc and the root ca is 6. Therefore our algorithm will make the higher ancestor codes, which are more general
concepts, have more involvement in all medical events (i.e. visits), which is natural in healthcare application
as those general concepts are often reliable. We repeat this calculation for all pairs of codes in all augmented
visits of all patients to obtain the co-occurrence matrix M ∈ R|D|×|D| depicted by Figure 2. For training the
embedding vectors ei’s using M, we minimize the following loss function as described in Pennington et al.
(2014).

|D|
(cid:88)

i,j=1

(cid:40)

J =

f (Mij)(e(cid:62)

i ej + bi + bj − log Mij)2

where f (x) =

(x < xmax)α
1

if x < xmax
otherwise

where the hyperparameters xmax and α are respectively set to 100 and 0.75 as the original paper (Pennington
et al., 2014). Note that, after the initialization, the basic embeddings ei’s of both leaf nodes (i.e. medical
codes) and non-leaf nodes (i.e. ancestors) are ﬁne-tuned during model training via backpropagation.

5

Figure 2: Creating the co-occurrence matrix together with the ancestors. The n-th ancestors are the group of
nodes that are n hops away from any leaf node in G. Here we exclude the root node, which will be just a
single row (column).

Table 1: Basic statistics of Sutter PAMF, MIMIC-III and Sutter heart failure (HF) cohort.

Dataset

Sutter PAMF MIMIC-III

# of patients
# of visits
Avg. # of visits per patient
# of unique ICD9 codes
Avg. # of codes per visit
Max # of codes per visit

258,555†
13,920,759
53.8
10,437
1.98
54

7,499†
19,911
2.66
4,893
13.1
39

Sutter HF cohort
30,727† (3,408 cases)
572,551
38.38
5,689
2.06
29

† For all datasets, we chose patients who made at least two visits.

3 Experiments

We conduct three experiments to determine if GRAM oﬀered superior prediction performance when facing
data insuﬃciency. We ﬁrst describe the experimental setup followed by results comparing predictive
performance of GRAM with various baseline models. After discussing GRAM’s scalability, we qualitatively
evaluate the interpretability of the resulting representation. The source code of GRAM is publicly available at
https://github.com/mp2893/gram.

3.1 Experiment Setup

Prediction tasks and source of data: We conduct the sequential diagnoses prediction (SDP) tasks on
two datasets, which aim at predicting all diagnosis categories in the next visit, and a heart failure (HF)
prediction task on one dataset, which is a binary prediction task for predicting a future HF onset where the
prediction is made only once at the last visit xT .
Two sequential diagnoses predictions (SDP) are respectively conducted using two datasets: 1) Sutter Palo Alto
Medical Foundation (PAMF) dataset, which consists of 18-years longitudinal medical records of 258K patients
between age 50 and 90. This will determine GRAM’s performance for general adult population with long visit
records. 2) MIMIC-III dataset (Johnson et al., 2016; Goldberger et al., 2000), which is a publicly available
dataset consisting of medical records of 7.5K intensive care unit (ICU) patients over 11 years. This will
determine GRAM’s performance for high-risk patients with very short visit records. We utilize all the patients
with at least 2 visits. We prepared the true labels yt by grouping the ICD9 codes into 283 groups using CCS
single-level diagnosis grouper1. This is to improve the training speed and predictive performance for easier
analysis, while preserving suﬃcient granularity for each diagnosis. Each diagnosis code’s varying frequency in
the training data can be viewed as diﬀerent degrees of data insuﬃciency. We calculate Accuracy@k for each

1https://www.hcup-us.ahrq.gov/toolssoftware/ccs/AppendixASingleDX.txt

6

of CCS single-level diagnosis codes such that, given a visit Vt, we get 1 if the target diagnosis is in the top k
guesses and 0 otherwise.
We conduct HF prediction on Sutter heart failure (HF) cohort, which is a subset of Sutter PAMF data for a
heart failure onset prediction study with 3.4K HF cases chosen by a set of criteria described in Vijayakrishnan
et al. (2014); Gurwitz et al. (2013) and 27K matching controls chosen by a set of criteria described in Choi
et al. (2016e). This will determine GRAM’s performance for a diﬀerent prediction task where we predict the
onset of one speciﬁc condition. We randomly downsample the training data to create diﬀerent degrees of
data insuﬃciency. We use area under the ROC curve (AUC) to measure the performance.
A summary of the datasets are provided in Table 1.We used CCS multi-level diagnoses hierarchy2 as our
knowledge DAG G. We also tested the ICD9 code hierarchy3, but the performance was similar to using CCS
multi-level hierarchy. For all three tasks, we randomly divide the dataset into the training, validation and test
set by .75:.10:.15 ratio, and use the validation set to tune the hyper-parameters. Further details regarding
the hyper-parameter tuning are provided below. The test set performance is reported in the paper.
Implementation details: We implemented GRAM with Theano 0.8.2 (Team, 2016). For training models,
we used Adadelta (Zeiler, 2012) with a mini-batch of 100 patients, on a machine equipped with Intel Xeon
E5-2640, 256GB RAM, four Nvidia Titan X’s and CUDA 7.5.
Models for comparison are the following. The ﬁrst two GRAM+ and GRAM are the proposed methods and the
rest are baselines. Hyper-parameter tuning is conﬁgured so that the number of parameters for the baselines
would be comparable to GRAM’s. Further details are provided below.

• GRAM: Input sequence x1, . . . , xT is ﬁrst transformed by the embedding matrix G, then fed to the GRU
with a single hidden layer, which in turn makes the prediction, as described by Eq. (4). The basic
embeddings ei’s are randomly initialized.

• GRAM+: We use the same setup as GRAM, but the basic embeddings ei’s are initialized according to Section

2.4.

• RandomDAG: We use the same setup as GRAM, but each leaf concept has ﬁve randomly assigned

ancestors from the CCS multi-level hierarchy to test the eﬀect of correct domain knowledge.

• RNN: Input xt is transformed by an embedding matrix Wemb ∈ Rk×|C|, then fed to the GRU with a
single hidden layer. The embedding size k is a hyper-parameter. Wemb is randomly initialized and trained
together with the GRU.

• RNN+: We use the RNN model with the same setup as before, but we initialize the embedding matrix
Wemb with GloVe vectors trained only with the co-occurrence of leaf concepts. This is to compare GRAM
with a similar weight initialization technique.

• SimpleRollUp: We use the RNN model with the same setup as before. But for input xt, we replace
all diagnosis codes with their direct parent codes in the CCS multi-level hierarchy, giving us 578, 526 and
517 input codes respectively for Sutter data, MIMIC-III and Sutter HF cohort. This is to compare the
performance of GRAM with a common grouping technique.

• RollUpRare: We use the RNN model with the same setup as before, but we replace any diagnosis
code whose frequency is less than a certain threshold in the dataset with its direct parent. We set the
threshold to 100 for Sutter data and Sutter HF cohort, and 10 for MIMIC-III, giving us 4,408, 935 and
1,538 input codes respectively for Sutter data, MIMIC-III and Sutter HF cohort. This is an intuitive way
of dealing with infrequent medical codes.

Hyper-parameter Tuning: We deﬁne ﬁve hyper-parameters for GRAM:

• dimensionality m of the basic embedding ei: [100, 200, 300, 400, 500]

• dimensionality r of the RNN hidden layer ht from Eq. (4): [100, 200, 300, 400, 500]

2https://www.hcup-us.ahrq.gov/toolssoftware/ccs/AppendixCMultiDX.txt
3http://www.icd9data.com/2015/Volume1/default.htm

7

• dimensionality l of Wa and ba from Eq. (3): [100, 200, 300, 400, 500]

• L2 regularization coeﬃcient for all weights except RNN weights: [0.1, 0.01, 0.001, 0.0001]

• dropout rate for the dropout on the RNN hidden layer: [0.0, 0.2, 0.4, 0.6, 0.8]

We performed 100 iterations of the random search by using the above ranges for each of the three prediction
experiments. In order to fairly compare the model performances, we matched the number of model parameters
to be similar for all baseline methods. To facilitate reproducibility, ﬁnal hyper-parameter settings we
used for all models for each prediction experiments are described at the source code repository, https:
//github.com/mp2893/gram, along with the detailed steps we used to tune the hyper-parameters.

3.2 Prediction performance and scalability

Model

0-20

20-40

40-60

60-80

80-100

0.4238
0.4903
0.0150 0.3242 0.4325
GRAM+
0.4193
0.4895
0.4224
0.2987
0.0042
GRAM
0.4853
0.4059
0.4010
0.2700
RandomDAG 0.0050
0.4212 0.4959
0.4140
0.2742
0.0069
RNN+
0.2691
RNN
0.4951
0.4227
0.4134
0.0080
0.3078 0.4369 0.4330 0.4924
SimpleRollUp 0.0085
0.4956
0.2768
0.0062
RollUpRare

0.4176

0.4226

(a) Accuracy@5 of sequential diagnoses prediction on Sutter data

Model

0-20

20-40

40-60

60-80

80-100

0.0672 0.1787 0.2644 0.2490
GRAM+
0.6267
GRAM
0.1935
0.2296
0.0556
0.6363
0.1346
0.1512
RandomDAG 0.0329
0.4494
0.2080
0.2494
RNN+
0.0454
0.6239
0.6243
0.2371
0.1804
0.0454
RNN
0.2455 0.2667 0.6387
SimpleRollUp 0.0578
0.6277
0.2364
0.1843
0.0454
RollUpRare

0.1016
0.0708
0.0843
0.0731
0.1328
0.0653

(b) Accuracy@20 of sequential diagnoses prediction on MIMIC-III

Model

10%

20%

30%

40%

50%

60%

70%

80%

90%

100%

GRAM+
GRAM
RandomDAG 0.7644
0.7930
RNN+
RNN
0.7811
SimpleRollUp 0.7799
0.7830
RollUpRare

0.7970 0.8223 0.8307 0.8332 0.8389 0.8404 0.8452 0.8456 0.8447 0.8448
0.8447
0.7981 0.8217 0.8340 0.8332 0.8372
0.8226
0.8143
0.7986
0.8335
0.8261
0.8162
0.8314
0.8156
0.8066
0.8258
0.8177
0.8108
0.8291
0.8211
0.8064

0.8440
0.8274
0.8343
0.8258
0.8223
0.8262

0.8430
0.8254
0.8345
0.8297
0.8269
0.8307

0.8431
0.8312
0.8353
0.8278
0.8272
0.8296

0.8377
0.8185
0.8333
0.8207
0.8207
0.8202

0.7882
0.8117
0.7942
0.8022
0.8067

0.8070
0.8215
0.8111
0.8133
0.8119

(c) AUC of HF onset prediction on Sutter HF cohort

Table 2: Performance of three prediction tasks. The x-axis of (a) and (b) represents the labels grouped by the
percentile of their frequencies in the training data in non-decreasing order. 0-20 are the most rare diagnosese
while 80-100 are the most common ones. (b) uses Accuracy@20 because MIMIC-III has a large average
number of codes per visit (see Table 1). For (c), we vary the size of the training data to train the models.

8

Table 3: Scalablity result in per epoch training time in second (the number of epochs needed). SDP stands
for Sequential Diagnoses Prediction

Model

SDP
(Sutter data)

SDP
(MIMIC-III)

HF prediction
(Sutter HF cohort)

GRAM
RNN

525s (39 epochs)
352s (24 epochs)

2s (11 epochs)
1s (6 epochs)

12s (7 epochs)
8s (5 epochs)

Tables 2a and 2b show the sequential diagnoses prediction performance on Sutter data and MIMIC-
III. Both ﬁgures show that GRAM+ outperforms other models when predicting labels with signiﬁcant data
insuﬃciency (i.e. less observed in the training data).The performance gain is greater for MIMIC-III, where
GRAM+ outperforms the basic RNN by 10% in the 20th-40th percentile range. This seems to come from the fact
that MIMIC patients on average have signiﬁcantly shorter visit history than Sutter patients, with much more
codes received per visit. Such short sequences make it diﬃcult for the RNN to learn and predict diagnoses
sequence. The performance diﬀerence between GRAM+ and GRAM suggests that our proposed initialization
scheme of the basic embeddings ei is important for sequential diagnosis prediction.

Table 2c shows the HF prediction performance on Sutter HF cohort. GRAM and GRAM+ consistently
outperforms other baselines (except RNN+) by 3∼4% AUC, and RNN+ by maximum 1.8% AUC. These
diﬀerences are quite signiﬁcant given that the AUC is already in the mid-80s, a high value for HF prediction,
cf. (Choi et al., 2016e). Note that, for GRAM+ and RNN+, we used the downsampled training data to
initialize the basic embeddings ei’s and the embedding matrix Wemb with GloVe, respectively. The result
shows that the initialization scheme of the basic embeddings in GRAM+ gives limited improvement over GRAM.
This stems from the diﬀerent natures of the two prediction tasks. While the goal of HF prediction is to
predict a binary label for the entire visit sequence, the goal of sequential diagnosis prediction is to predict the
co-occurring diagnosis codes at every visit. Therefore the co-occurrence information infused by the initialized
embedding scheme is more beneﬁcial to sequential diagnosis prediction. Additionally, this beneﬁt is associated
with the natures of the two prediction tasks than the datasets used for the prediction tasks. Because the
initialized embedding shows diﬀerent degrees of improvement as shown by Tables 2a and 2c, when Sutter
HF cohort is a subset of Sutter PAMF, thus having similar characteristics. Overall, GRAM showed superior
predictive performance under data insuﬃciency in three diﬀerent experiments, demonstrating its general
applicability in clinical predictive modeling. Now we brieﬂy discuss the scalability of GRAM by comparing its
training time to RNN’s. Table 3 shows the number of seconds taken for the two models to train for a single
epoch for each predictive modeling task. GRAM+ and RNN+ showed the similar behavior as GRAM and RNN.
GRAM takes approximately 50% more time to train for a single epoch for all prediction tasks. This stems from
calculating attention weights and the ﬁnal representations gi for all medical codes. GRAM also generally takes
about 50% more epochs to reach to the model with the lowest validation loss. This is due to optimizing an
extra MLP model that generates the attention weights. Overall, use of GRAM adds a manageable amount of
overhead in training time to the plain RNN.

3.3 Qualitative evaluation of interpretable representations

To qualitatively assess the interpretability of the learned representations of the medical codes, we plot on
a 2-D space using t-SNE (Maaten and Hinton, 2008) the ﬁnal representations gi of 2,000 randomly chosen
diseases learned by GRAM+ for sequential diagnoses prediction on Sutter data4 (Figure 3a). The color of
the dots represents the highest disease categories and the text annotations represent the detailed disease
categories in CCS multi-level hierarchy. For comparison, we also show the t-SNE plots on the strongest
results from GRAM (Figure 3b), RNN+ (Figure 3c), RNN (Figure 3d) and RandomDAG (Figure 3e). GloVe
(Figure 3f) and Skip-gram (Figure 3g) were trained on the Sutter data, where a single visit Vt was used as
the context window to calculate the co-occurrence of codes.

Figures 3c and 3f conﬁrm that interpretable representations cannot simply be learned only by co-occurrence

or supervised prediction without medical knowledge. GRAM+ and GRAM learn interpretable disease

4The scatterplots of models trained for sequential diagnoses prediction on MIMIC-III and HF prediction for Sutter HF cohort

were similar but less structured due to smaller data size.

9

(a) Scatterplot of the ﬁnal representations gi’s of GRAM+

(b) Scatterplot of the ﬁnal representations gi’s of GRAM

(c) Scatterplot of the trained embedding matrix Wemb
of RNN+

(d) Scatterplot of the trained embedding matrix
Wemb of RNN

(e) Scatterplot of the ﬁnal representations gi’s of
RandomDAG

(f) Scatterplot of the disease representations trained
by GloVe

(g) Scatterplot of the basic embeddings ei’s trained
by Skip-gram

Figure 3: t-SNE scatterplots of medical concepts trained by GRAM+, GRAM, RNN+, RNN, RandomDAG,
GloVe and Skip-gram. The color of the dots represents the highest disease categories and the text annotations
represent the detailed disease categories in CCS multi-level hierarchy. It is clear that GRAM+ and GRAM exhibit
interpretable embedding that are well aligned with the medical ontology.

10

representations that are signiﬁcantly more consistent with the given knowledge DAG G. Based on the
prediction performance shown by Table 2, and the fact that the representations gi’s are the ﬁnal product
of GRAM, we can infer that such medically meaningful representations are necessary for predictive models to
cope with data insuﬃciency and make more accurate predictions. Figure 3b shows that the quality of the
ﬁnal representations gi of GRAM is quite similar to GRAM+. Compared to other baselines, GRAM demonstrates
signiﬁcantly more structured representations that align well with the given knowledge DAG. It is interesting
that Skip-gram shows the most structured representation among all baselines. We used GloVe to initialize
the basic embeddings ei in this work because it uses global co-occurrence information and its training time
is fast as it is only dependent only on the total number of unique concepts |C|. Skip-gram’s training time,
on the other hand, depends on both the number of patients and the number of visits each patient made,
which makes the algorithm generally slower than GloVe. An interactive visualization tool can be accessed at
http://www.sunlab.org/research/gram-graph-based-attention-model/.

3.4 Analysis of the attention behavior

Figure 4: GRAM’s attention behavior during HF prediction for four representative diseases (each column). In
each ﬁgure, the leaf node represents the disease and upper nodes are its ancestors. The size of the node shows
the amount of attention it receives, which is also shown by the bar charts. The number in the parenthesis
next to the disease is its frequency in the training data. We exclude the root of the knowledge DAG G from
all ﬁgures as it did not play a signiﬁcant role.

Next we show that GRAM’s attention can be explained intuitively based on the data availability and
knowledge DAG’s structure when performing a prediction task. Using Eq. (1), we can calculate the attention
weights of individual disease. Figure 4 shows the attention behaviors of four representative diseases when
performing HF prediction on Sutter HF cohort.

Other pneumothorax (ICD9 512.89) in Figure 4a is rarely observed in the data and has only ﬁve siblings.
In this case, most information is derived from the highest ancestor. Temporomandibular joint disorders &
articular disc disorder (ICD9 524.63) in Figure 4b is rarely observed but has 139 siblings. In this case, its
parent receives a stronger attention because it aggregates suﬃcient samples from all of its children to learn a
more accurate representation. Note that the disease itself also receives a stronger attention to facilitate easier
distinction from its large number of siblings.

Unspeciﬁed essential hypertension (ICD9 401.9) in Figure 4c is very frequently observed but has only two
siblings. In this case, GRAM assigns a very strong attention to the leaf, which is logical because the more you
observe a disease, the stronger your conﬁdence becomes. Need for prophylactic vaccination and inoculation
against inﬂuenza (ICD9 V04.81) in Figure 4d is quite frequently observed and also has 103 siblings. The
attention behavior in this case is quite similar to the case with fewer siblings (Figure 4b) with a slight
attention shift towards the leaf concept as more observations lead to higher conﬁdence.

4 Related Work

The attention mechanism is a general framework for neural network learning (Bahdanau et al., 2014), and
has been since used in many areas such as speech recognition (Chorowski et al., 2014), computer vision (Ba

11

et al., 2014; Xu et al., 2015) and healthcare (Choi et al., 2016b). However, no one has designed attention
model based on knowledge ontology, which is the focus of this work.

There are related works in learning the representations of graphs. Several studies focused on learning the
representations of graph vertices by using the neighbor information. DeepWalk (Perozzi et al., 2014) and
node2vec (Grover and Leskovec, 2016) use random walk while LINE (Tang et al., 2015) uses breadth-ﬁrst
search to ﬁnd the neighbors of a vertex and learn its representation based on the neighbor information.
Graph convolutional approaches (Yang et al., 2016; Kipf and Welling, 2016) also focus on learning the vertex
representations to mainly perform vertex classiﬁcation. All those works focus on solving the graph data
problems whereas GRAM focuses on solving clinical predictive modeling problems using the knowledge DAG as
supplementary information.

Several researchers tried to model the knowledge DAG such as WordNet (Miller, 1995) or Freebase
(Bollacker et al., 2008) where two entities are connected with various types of relation, forming a set of triples.
They aim to project entities and relations (Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014; Lin
et al., 2015) to the latent space based on the triples or additional information such as hierarchy of entities
(Xie et al., 2016). These works demonstrated tasks such as link prediction, triple classiﬁcation or entity
classiﬁcation using the learned representations. More recently, Li et al. (2016) learned the representations of
words and Wikipedia categories by utilizing the hierarchy of Wikipedia categories. GRAM is fundamentally
diﬀerent from the above studies in that it aims to design intuitive attention mechanism on the knowledge
DAG as a knowledge prior to cope with data insuﬃciency and learn medically interpretable representations
to make accurate predictions.

A classical approach for incorporating side information in the predictive models is to use graph Laplacian
regularization (Weinberger et al., 2006; Che et al., 2015). However, using this approach is not straightforward
as it relies on the appropriate deﬁnition of distance on graphs which is often unavailable.

5 Conclusion

Data insuﬃciency, either due to less common diseases or small datasets, is one of the key hurdles in healthcare
analytics, especially when we apply deep neural networks models. To overcome this challenge, we leverage the
knowledge DAG, which provides a multi-resolution view of medical concepts. We propose GRAM, a graph-based
attention model using both a knowledge DAG and EHR to learn an accurate and interpretable representations
for medical concepts. GRAM chooses a weighted average of ancestors of a medical concept and train the entire
process with a predictive model in an end-to-end fashion. We conducted three predictive modeling experiments
on real EHR datasets and showed signiﬁcant improvement in the prediction performance, especially on
low-frequency diseases and small datasets. Analysis of the attention behavior provided intuitive insight of
GRAM.

References

arXiv:1412.7755 (2014).

Jimmy Ba, Volodymyr Mnih, and Koray Kavukcuoglu. 2014. Multiple object recognition with visual attention.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural Machine Translation by Jointly

Learning to Align and Translate. arXiv:1409.0473 (2014).

Yoshua Bengio, Patrice Simard, and Paolo Frasconi. 1994. Learning long-term dependencies with gradient

descent is diﬃcult. IEEE Transactions on Neural Networks 5, 2 (1994).

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively

created graph database for structuring human knowledge. In SIGMOD.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013.

Translating embeddings for modeling multi-relational data. In NIPS.

12

Zhengping Che, David Kale, Wenzhe Li, Mohammad Taha Bahadori, and Yan Liu. 2015. Deep Computational

Phenotyping. In SIGKDD.

Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. 2016. Recurrent Neural

Networks for Multivariate Time Series with Missing Values. arXiv:1606.01865 (2016).

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for
statistical machine translation. In EMNLP.

Edward Choi, Mohammad Taha Bahadori, Andy Schuetz, Walter F. Stewart, and Jimeng Sun. 2016a. Doctor

AI: Predicting Clinical Events via Recurrent Neural Networks. In MLHC.

Edward Choi, Mohammad Taha Bahadori, Andy Schuetz, Walter F. Stewart, and Jimeng Sun. 2016b.
RETAIN: Interpretable Predictive Model in Healthcare using Reverse Time Attention Mechanism. In
NIPS.

Edward Choi, Mohammad Taha Bahadori, Elizabeth Searles, Catherine Coﬀey, Michael Thompson, James
Bost, Javier T Sojo, and Jimeng Sun. 2016c. Multi-layer Representation Learning for Medical Concepts. In
SIGKDD.

Edward Choi, Andy Schuetz, Walter F. Stewart, and Jimeng Sun. 2016e. Using Recurrent Neural Network
Models for Early Detection of Heart Failure Onset. Journal of the American Medical Informatics Association
(2016), ocw112.

Youngduck Choi, Chill Yi-I Chiu, and David Sontag. 2016d. Learning Low-Dimensional Representations of

Medical Concepts. (2016). AMIA CRI.

Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. End-to-end continuous

speech recognition using attention-based recurrent NN: First results. arXiv:1412.1602 (2014).

Ary Goldberger and others. 2000. Physiobank, physiotoolkit, and physionet components of a new research

resource for complex physiologic signals. Circulation (2000).

Aditya Grover and Jure Leskovec. 2016. Node2Vec: Scalable Feature Learning for Networks. In SIGKDD.

Jerry Gurwitz, David Magid, David Smith, Robert Goldberg, David McManus, Larry Allen, Jane Saczynski,
Micah Thorp, Grace Hsu, Sue Hee Sung, and others. 2013. Contemporary prevalence and correlates of
incident heart failure with preserved ejection fraction. The American journal of medicine 126, 5 (2013).

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural Computation 9, 8 (1997).

Alistair Johnson and others. 2016. MIMIC-III, a freely accessible critical care database. Scientiﬁc Data 3

Thomas N Kipf and Max Welling. 2016. Semi-supervised classiﬁcation with graph convolutional networks.

(2016).

arXiv:1609.02907 (2016).

Quoc V Le, Navdeep Jaitly, and Geoﬀrey E Hinton. 2015. A Simple Way to Initialize Recurrent Networks of

Rectiﬁed Linear Units. arXiv:1504.00941 (2015).

Yuezhang Li, Ronghuo Zheng, Tian Tian, Zhiting Hu, Rahul Iyer, and Katia Sycara. 2016. Joint Embedding
of Hierarchical Categories and Entities for Concept Categorization and Dataless Classiﬁcation. (2016).

Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning Entity and Relation

Embeddings for Knowledge Graph Completion. In AAAI.

13

Zachary C Lipton, David C Kale, Charles Elkan, and Randall Wetzell. 2015. Learning to Diagnose with

LSTM Recurrent Neural Networks. arXiv:1511.03677 (2015).

Zachary C Lipton, David C Kale, and Randall Wetzel. 2016. Modeling Missing Data in Clinical Time Series

with RNNs. In MLHC.

Laurens van der Maaten and Geoﬀrey Hinton. 2008. Visualizing data using t-SNE. JMLR 9, Nov (2008).

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeﬀ Dean. 2013. Distributed representations of

words and phrases and their compositionality. In NIPS.

George A Miller. 1995. WordNet: a lexical database for English. Commun. ACM 38, 11 (1995).

Riccardo Miotto, Li Li, Brian A Kidd, and Joel T Dudley. 2016. Deep Patient: An Unsupervised Representation

to Predict the Future of Patients from the Electronic Health Records. Scientiﬁc Reports 6 (2016).

Phuoc Nguyen, Truyen Tran, Nilmini Wickramasinghe, and Svetha Venkatesh. 2016. Deepr: A Convolutional

Net for Medical Records. arXiv:1607.07519 (2016).

Jeﬀrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global Vectors for Word

Representation. In EMNLP.

In SIGKDD.

Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online Learning of Social Representations.

Healthcare Cost & Utilization Project and others. 2010. Clinical classiﬁcations software (CCS) for ICD-9-CM.

Rockville, MD: Agency for Healthcare Research and Quality (2010).

Narges Razavian, Jake Marcus, and David Sontag. 2016. Multi-task Prediction of Disease Onsets from

Longitudinal Lab Tests. In MLHC.

Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. 2013. Reasoning with neural tensor

networks for knowledge base completion. In NIPS.

Michael Q Stearns, Colin Price, Kent A Spackman, and Amy Y Wang. 2001. SNOMED clinical terms:

overview of the development process and project status. In AMIA.

Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. LINE: Large-scale

Information Network Embedding. In WWW.

The Theano Development Team. 2016. Theano: A Python framework for fast computation of mathematical

expressions. arXiv:1605.02688 (2016).

Rajakrishnan Vijayakrishnan, Steven Steinhubl, Kenney Ng, Jimeng Sun, Roy Byrd, Zahra Daar, Brent
Williams, Shahram Ebadollahi, Walter Stewart, and others. 2014. Prevalence of heart failure signs and
symptoms in a large primary care population identiﬁed through the use of text and data mining of the
electronic health record. Journal of cardiac failure 20, 7 (2014).

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge Graph Embedding by Translating

on Hyperplanes. In AAAI.

Kilian Q Weinberger, Fei Sha, Qihui Zhu, and Lawrence K Saul. 2006. Graph Laplacian Regularization for

Large-Scale Semideﬁnite Programming. In NIPS.

Ruobing Xie, Zhiyuan Liu, and Maosong Sun. 2016. Representation Learning of Knowledge Graphs with

Hierarchical Types. In IJCAI.

14

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel,
and Yoshua Bengio. 2015. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention..
In ICML.

Zhilin Yang, William Cohen, and Ruslan Salakhutdinov. 2016. Revisiting Semi-Supervised Learning with

Graph Embeddings. arXiv:1603.08861 (2016).

Matthew D Zeiler. 2012. ADADELTA: an adaptive learning rate method. arXiv:1212.5701 (2012).

15

7
1
0
2
 
r
p
A
 
1
 
 
]

G
L
.
s
c
[
 
 
3
v
2
1
0
7
0
.
1
1
6
1
:
v
i
X
r
a

GRAM: Graph-based Attention Model for Healthcare
Representation Learning

Edward Choi∗, Mohammad Taha Bahadori∗, Le Song∗, Walter F. Stewart†, Jimeng Sun∗

∗ Georgia Institute of Technology

† Sutter Health

{mp2893,bahadori}@gatech.edu, lsong@cc.gatech.edu, stewarwf@sutterhealth.org, jsun@cc.gatech.edu

Abstract

Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two

important challenges remain:

• Data insuﬃciency: Often in healthcare predictive modeling, the sample size is insuﬃcient for deep

learning methods to achieve satisfactory results.

• Interpretation: The representations learned by deep learning methods should align with medical

knowledge.

To address these challenges, we propose a GRaph-based Attention Model, GRAM that supplements electronic
health records (EHR) with hierarchical information inherent to medical ontologies. Based on the data
volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in
the ontology via an attention mechanism.

We compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various
methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and
one heart failure prediction task. Compared to the basic RNN, GRAM achieved 10% higher accuracy for
predicting diseases rarely observed in the training data and 3% improved area under the ROC curve
for predicting heart failure using an order of magnitude less training data. Additionally, unlike other
methods, the medical concept representations learned by GRAM are well aligned with the medical ontology.
Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts
when facing data insuﬃciency at the lower level concepts.

1

Introduction

The rapid growth in volume and diversity of health care data from electronic health records (EHR) and other
sources is motivating the use of predictive modeling to improve care for individual patients. In particular,
novel applications are emerging that use deep learning methods such as word embedding (Choi et al., 2016c,d),
recurrent neural networks (RNN) (Che et al., 2016; Choi et al., 2016a,b; Lipton et al., 2016), convolutional
neural networks (CNN) (Nguyen et al., 2016) or stacked denoising autoencoders (SDA) (Che et al., 2015;
Miotto et al., 2016), demonstrating signiﬁcant performance enhancement for diverse prediction tasks. Deep
learning models appear to perform signiﬁcantly better than logistic regression or multilayer perceptron (MLP)
models that depend, to some degree, on expert feature construction (Lipton et al., 2015; Razavian et al.,
2016).

Training deep learning models typically requires large amounts of data that often cannot be met by a single
health system or provider organization. Sub-optimal model performance can be particularly challenging when
the focus of interest is predicting onset of a rare disease. For example, using Doctor AI (Choi et al., 2016a), we
discovered that RNN alone was ineﬀective to predict the onset of diseases such as cerebral degenerations (e.g.
Leukodystrophy, Cerebral lipidoses) or developmental disorders (e.g. autistic disorder, Heller’s syndrome),
partly because their rare occurrence in the training data provided little learning opportunity to the ﬂexible
models like RNN.

1

Figure 1: The illustration of GRAM. Leaf nodes (solid circles) represents a medical concept in the EHR, while
the non-leaf nodes (dotted circles) represent more general concepts. The ﬁnal representation gi of the leaf
concept ci is computed by combining the basic embeddings ei of ci and eg, ec and ea of its ancestors cg, cc
and ca via an attention mechanism. The ﬁnal representations form the embedding matrix G for all leaf
concepts. After that, we use G to embed patient visit vector xt to a visit representation vt, which is then fed
to a neural network model to make the ﬁnal prediction ˆyt.

The data requirement of deep learning models comes from having to assess exponential number of
combinations of input features. This can be alleviated by exploiting medical ontologies that encodes
hierarchical clinical constructs and relationships among medical concepts. Fortunately, there are many
well-organized ontologies in healthcare such as the International Classiﬁcation of Diseases (ICD), Clinical
Classiﬁcations Software (CCS) (Stearns et al., 2001) or Systematized Nomenclature of Medicine-Clinical
Terms (SNOMED-CT) (Project et al., 2010). Nodes (i.e. medical concepts) close to one another in medical
ontologies are likely to be associated with similar patients, allowing us to transfer knowledge among them.
Therefore, proper use of medical ontologies will be helpful when we lack enough data for the nodes in the
ontology to train deep learning models.

In this work, we propose GRAM, a method that infuses information from medical ontologies into deep
learning models via neural attention. Considering the frequency of a medical concept in the EHR data and
its ancestors in the ontology, GRAM decides the representation of the medical concept by adaptively combining
its ancestors via attention mechanism. This will not only support deep learning models to learn robust
representations without large amount of data, but also learn interpretable representations that align well
with the knowledge from the ontology. The attention mechanism is trained in an end-to-end fashion with
the neural network model that predicts the onset of disease(s). We also propose an eﬀective initialization
technique in addition to the ontological knowledge to better guide the representation learning process.

We compare predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various models
including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart
failure (HF) prediction task. We demonstrate that GRAM is up to 10% more accurate than the basic RNN
for predicting diseases less observed in the training data. After discussing GRAM’s scalability, we visualize
the representations learned from various models, where GRAM provides more intuitive representations by
grouping similar medical concepts close to one another. Finally, we show GRAM’s attention mechanism can
be interpreted to understand how it assigns the right amount of attention to the ancestors of each medical
concept by considering the data availability and the ontology structure.

2 Methodology

We ﬁrst deﬁne the notations describing EHR data and medical ontologies, followed by a description of GRAM
(Section 2.2), the end-to-end training of the attention generation and predictive modeling (Section 2.3), and
the eﬃcient initialization scheme (Section 2.4).

2

2.1 Basic Notation

We denote the set of entire medical codes from the EHR as c1, c2, . . . , c|C| ∈ C with the vocabulary size |C|.
The clinical record of each patient can be viewed as a sequence of visits V1, . . . , VT where each visit contains a
subset of medical codes Vt ⊆ C. Vt can be represented as a binary vector xt ∈ {0, 1}|C| where the i-th element
is 1 only if Vt contains the code ci. To avoid clutter, all algorithms will be presented for a single patient.

We assume that a given medical ontology G typically expresses the hierarchy of various medical concepts in
the form of a parent-child relationship, where the medical codes C form the leaf nodes. Ontology G is represented
as a directed acyclic graph (DAG) whose nodes form a set D = C +C(cid:48). The set C(cid:48) = {c|C|+1, c|C|+2, . . . , c|C|+|C(cid:48)|}
consists of all non-leaf nodes (i.e. ancestors of the leaf nodes), where |C(cid:48)| represents the number of all non-leaf
nodes. We use knowledge DAG to refer to G. A parent in the knowledge DAG G represents a related but
more general concept over its children. Therefore, G provides a multi-resolution view of medical concepts with
diﬀerent degrees of speciﬁcity. While some ontologies are exclusively expressed as parent-child hierarchies
(e.g. ICD-9, CCS), others are not. For example, in some instances SNOMED-CT also links medical concepts
to causal or treatment relationships, but the majority relationships in SNOMED-CT are still parent-child.
Therefore, we focus on the parent-child relationships in this work.

2.2 Knowledge DAG and the Attention Mechanism

GRAM leverages the parent-child relationship of G to learn robust representations when data volume is
constrained. GRAM balances the use of ontology information in relation to data volume in determining the
level of speciﬁcity for a medical concept. When a medical concept is less observed in the data, more weight is
given to its ancestors as they can be learned more accurately and oﬀer general (coarse-grained) information
about their children. The process of resorting to the parent concepts can be automated via the attention
mechanism and the end-to-end training as described in Figure 1.

In the knowledge DAG, each node ci is assigned a basic embedding vector ei ∈ Rm, where m represents the
dimensionality. Then e1, . . . , e|C| are the basic embeddings of the codes c1, . . . , c|C| while e|C|+1, . . . , e|C|+|C(cid:48)|
represent the basic embeddings of the internal nodes c|C|+1, . . . , c|C|+|C(cid:48)|. The initialization of these basic
embeddings is described in Section 2.4. We formulate a leaf node’s ﬁnal representation as a convex combination
of the basic embeddings of itself and its ancestors:

(cid:88)

gi =

αijej,

(cid:88)

j∈A(i)

j∈A(i)

αij = 1, αij ≥ 0 for j ∈ A(i),

where gi ∈ Rm denotes the ﬁnal representation of the code ci, A(i) the indices of the code ci and ci’s
ancestors, ej the basic embedding of the code cj and αij ∈ R the attention weight on the embedding ej when
calculating gi. The attention weight αij in Eq. (1) is calculated by the following Softmax function,

f (ei, ej) is a scalar value representing the compatibility between the basic embeddings of ei and ek. We
compute f (ei, ej) via the following feed-forward network with a single hidden layer (MLP),

αij =

(cid:80)

exp(f (ei, ej))
k∈A(i) exp(f (ei, ek))

f (ei, ej) = u(cid:62)

a tanh(Wa

(cid:21)

(cid:20) ei
ej

+ ba)

where Wa ∈ Rl×2m is the weight matrix for the concatenation of ei and ej, b ∈ Rl the bias vector, and
ua ∈ Rl the weight vector for generating the scalar value. The constant l represents the dimension size
of the hidden layer of f (·, ·). We always concatenate ei and ej in the child-ancestor order. Note that the
compatibility function f is an MLP, because MLP is well known to be a suﬃcient approximator for an
arbitrary function, and we empirically found that our formulation performed better in our use cases than
alternatives such as inner product and Bahdanau et al.’s (Bahdanau et al., 2014).

3

(1)

(2)

(3)

Remarks: The example in Figure 1 is derived based on a single path from ci to ca. However, the same
mechanism can be applicable to multiple paths as well. For example, code ck has two paths to the root ca,
containing ﬁve ancestors in total. Another scenario is where the EHR data contain both leaf codes and some
ancestor codes. We can move those ancestors present in EHR data from the set C(cid:48) to C and apply the same
process as Eq. (1) to obtain the ﬁnal representations for them.

2.3 End-to-End Training with a Predictive Model

We train the attention mechanism together with a predictive model such that the attention mechanism
improves the predictive performance. By concatenating ﬁnal representation g1, g2, . . . , g|C| of all medical
codes, we have the embedding matrix G ∈ Rm×|C| where gi is its i-th column of G. We can then convert
visit Vt to a visit representation vt by multiplying embedding matrix G with multi-hot vector xt indicating
the clinical events in visit Vt as shown in the right side of Figure 1. Finally the visit representation vt will
be used as an input to pass to a predictive model for predicting the target label yt using a neural network
(NN) model. In this work, we use RNN as the choice of the NN model as the task is to perform sequential
diagnoses prediction (Choi et al., 2016a,b) with the objective of predicting the disease codes of the next visit
Vt+1 given the visit records up to the current timestep V1, V2, . . . , Vt, which can be expressed as follows,

(cid:98)yt = (cid:98)xt+1 = Softmax(Wht + b), where
h1, h2, . . . , ht = RNN(v1, v2, . . . , vt; θr), where
v1, v2, . . . , vt = tanh(G[x1, x2, . . . , xt])

(4)

(5)

where xt ∈ R|C| denotes the t-th visit; vt ∈ Rm the t-th visit representation; ht ∈ Rr the RNN’s hidden layer
at t-th time step (i.e. t-th visit); θr RNN’s parameters; W ∈ R|C|×r and b ∈ R|C| the weight matrices and
the bias vector of the Softmax function; r denotes the dimension size of the hidden layer. We use “RNN” to
denote any recurrent neural network variants that can cope with the vanishing gradient problem (Bengio
et al., 1994), such as LSTM (Hochreiter and Schmidhuber, 1997), GRU (Cho et al., 2014), and IRNN (Le
et al., 2015), with any varying numbers of hidden layers. The prediction loss for all time steps is calculated
using the binary cross entropy as follows,

L(x1, x2 . . . , xT ) = −

1
T − 1

T −1
(cid:88)

(cid:16)

t=1

(cid:17)
(cid:62) log((cid:98)yt) + (1 − yt)(cid:62) log(1 − (cid:98)yt)

yt

where we sum the cross entropy errors from all timestamps of (cid:98)yt, T denotes the number of timestamps of
the visit sequence. Note that the above loss is deﬁned for a single patient. In actual implementation, we
will take the average of the individual loss for multiple patients. Algorithm 1 describes the overall training
procedure of GRAM, under the assumption that we are performing the sequential diagnoses prediction task
using an RNN. Note that Algorithm 1 describes stochastic gradient update to avoid clutter, but it can be
easily extended to other gradient based optimization such as mini-batch gradient update.

2.4

Initializing Basic Embeddings

The attention generation mechanism in Section 2.2 requires basic embeddings ei of each node in the knowledge
DAG. The basic embeddings of ancestors, however, pose a diﬃculty because they are often not observed in
the data. To properly initialize them, we use co-occurrence information to learn the basic embeddings of
medical codes and their ancestors. Co-occurrence has proven to be an important source of information when
learning representations of words or medical concepts (Mikolov et al., 2013; Choi et al., 2016c,d). To train
the basic embeddings, we employ GloVe (Pennington et al., 2014), which uses the global co-occurrence matrix
of words to learn their representations. In our case, the co-occurrence matrix of the codes and the ancestors
was generated by counting the co-occurrences within each visit Vt, where we augment each visit with the
ancestors of the codes in the visit. We describe the details of the initialization algorithm with an example.
We borrow the parent-child relationships from the knowledge DAG of Figure 1. Given a visit Vt,

4

Algorithm 1 GRAM Optimization

Randomly initialize basic embedding matrix E, attention parameters ua, Wa, ba, RNN parameter θr,
softmax parameters W, b.
repeat

Update E with GloVe objective function (see Section 2.4)

until convergence
repeat

X ← random patient from dataset
for visit Vt in X do

for code ci in Vt do

Refer G to ﬁnd ci’s ancestors C (cid:48)
for code cj in C (cid:48) do

Calculate attention weight αij using Eq. (2).

end for
Obtain ﬁnal representation gi using Eq. (1).

end for
vt ← tanh((cid:80)
i:ci∈Vt
Make prediction (cid:98)yt using Eq. (4)

gi)

end for
Calculate prediction loss L using Eq .(5)
Update parameters according to the gradient of L

until convergence

we augment it with the ancestors of all the codes to obtain the augmented visit V (cid:48)
t ,

V (cid:48)
t = {cd, cb, ca, ci, cg, cc, ca, ck, cj, cf , cc, cb, ca}

Vt = {cd, ci, ck}

where the augmented ancestors are underlined. Note that a single ancestor can appear multiple times in V (cid:48)
t .
In fact, the higher the ancestor is in the knowledge DAG, the more times it is likely to appear in V (cid:48)
t . We
count the co-occurrence of two codes in V (cid:48)

t as follows,

co-occurrence(ci, cj, V (cid:48)

t ) = count(ci, V (cid:48)

t ) × count(cj, V (cid:48)
t )

where count(ci, V (cid:48)
t ) is the number of times the code ci appears in the augmented visit V (cid:48)
t . For example, the
co-occurrence between the leaf code ci and the root ca is 3. However, the co-occurrence between the ancestor
cc and the root ca is 6. Therefore our algorithm will make the higher ancestor codes, which are more general
concepts, have more involvement in all medical events (i.e. visits), which is natural in healthcare application
as those general concepts are often reliable. We repeat this calculation for all pairs of codes in all augmented
visits of all patients to obtain the co-occurrence matrix M ∈ R|D|×|D| depicted by Figure 2. For training the
embedding vectors ei’s using M, we minimize the following loss function as described in Pennington et al.
(2014).

|D|
(cid:88)

i,j=1

(cid:40)

J =

f (Mij)(e(cid:62)

i ej + bi + bj − log Mij)2

where f (x) =

(x < xmax)α
1

if x < xmax
otherwise

where the hyperparameters xmax and α are respectively set to 100 and 0.75 as the original paper (Pennington
et al., 2014). Note that, after the initialization, the basic embeddings ei’s of both leaf nodes (i.e. medical
codes) and non-leaf nodes (i.e. ancestors) are ﬁne-tuned during model training via backpropagation.

5

Figure 2: Creating the co-occurrence matrix together with the ancestors. The n-th ancestors are the group of
nodes that are n hops away from any leaf node in G. Here we exclude the root node, which will be just a
single row (column).

Table 1: Basic statistics of Sutter PAMF, MIMIC-III and Sutter heart failure (HF) cohort.

Dataset

Sutter PAMF MIMIC-III

# of patients
# of visits
Avg. # of visits per patient
# of unique ICD9 codes
Avg. # of codes per visit
Max # of codes per visit

258,555†
13,920,759
53.8
10,437
1.98
54

7,499†
19,911
2.66
4,893
13.1
39

Sutter HF cohort
30,727† (3,408 cases)
572,551
38.38
5,689
2.06
29

† For all datasets, we chose patients who made at least two visits.

3 Experiments

We conduct three experiments to determine if GRAM oﬀered superior prediction performance when facing
data insuﬃciency. We ﬁrst describe the experimental setup followed by results comparing predictive
performance of GRAM with various baseline models. After discussing GRAM’s scalability, we qualitatively
evaluate the interpretability of the resulting representation. The source code of GRAM is publicly available at
https://github.com/mp2893/gram.

3.1 Experiment Setup

Prediction tasks and source of data: We conduct the sequential diagnoses prediction (SDP) tasks on
two datasets, which aim at predicting all diagnosis categories in the next visit, and a heart failure (HF)
prediction task on one dataset, which is a binary prediction task for predicting a future HF onset where the
prediction is made only once at the last visit xT .
Two sequential diagnoses predictions (SDP) are respectively conducted using two datasets: 1) Sutter Palo Alto
Medical Foundation (PAMF) dataset, which consists of 18-years longitudinal medical records of 258K patients
between age 50 and 90. This will determine GRAM’s performance for general adult population with long visit
records. 2) MIMIC-III dataset (Johnson et al., 2016; Goldberger et al., 2000), which is a publicly available
dataset consisting of medical records of 7.5K intensive care unit (ICU) patients over 11 years. This will
determine GRAM’s performance for high-risk patients with very short visit records. We utilize all the patients
with at least 2 visits. We prepared the true labels yt by grouping the ICD9 codes into 283 groups using CCS
single-level diagnosis grouper1. This is to improve the training speed and predictive performance for easier
analysis, while preserving suﬃcient granularity for each diagnosis. Each diagnosis code’s varying frequency in
the training data can be viewed as diﬀerent degrees of data insuﬃciency. We calculate Accuracy@k for each

1https://www.hcup-us.ahrq.gov/toolssoftware/ccs/AppendixASingleDX.txt

6

of CCS single-level diagnosis codes such that, given a visit Vt, we get 1 if the target diagnosis is in the top k
guesses and 0 otherwise.
We conduct HF prediction on Sutter heart failure (HF) cohort, which is a subset of Sutter PAMF data for a
heart failure onset prediction study with 3.4K HF cases chosen by a set of criteria described in Vijayakrishnan
et al. (2014); Gurwitz et al. (2013) and 27K matching controls chosen by a set of criteria described in Choi
et al. (2016e). This will determine GRAM’s performance for a diﬀerent prediction task where we predict the
onset of one speciﬁc condition. We randomly downsample the training data to create diﬀerent degrees of
data insuﬃciency. We use area under the ROC curve (AUC) to measure the performance.
A summary of the datasets are provided in Table 1.We used CCS multi-level diagnoses hierarchy2 as our
knowledge DAG G. We also tested the ICD9 code hierarchy3, but the performance was similar to using CCS
multi-level hierarchy. For all three tasks, we randomly divide the dataset into the training, validation and test
set by .75:.10:.15 ratio, and use the validation set to tune the hyper-parameters. Further details regarding
the hyper-parameter tuning are provided below. The test set performance is reported in the paper.
Implementation details: We implemented GRAM with Theano 0.8.2 (Team, 2016). For training models,
we used Adadelta (Zeiler, 2012) with a mini-batch of 100 patients, on a machine equipped with Intel Xeon
E5-2640, 256GB RAM, four Nvidia Titan X’s and CUDA 7.5.
Models for comparison are the following. The ﬁrst two GRAM+ and GRAM are the proposed methods and the
rest are baselines. Hyper-parameter tuning is conﬁgured so that the number of parameters for the baselines
would be comparable to GRAM’s. Further details are provided below.

• GRAM: Input sequence x1, . . . , xT is ﬁrst transformed by the embedding matrix G, then fed to the GRU
with a single hidden layer, which in turn makes the prediction, as described by Eq. (4). The basic
embeddings ei’s are randomly initialized.

• GRAM+: We use the same setup as GRAM, but the basic embeddings ei’s are initialized according to Section

2.4.

• RandomDAG: We use the same setup as GRAM, but each leaf concept has ﬁve randomly assigned

ancestors from the CCS multi-level hierarchy to test the eﬀect of correct domain knowledge.

• RNN: Input xt is transformed by an embedding matrix Wemb ∈ Rk×|C|, then fed to the GRU with a
single hidden layer. The embedding size k is a hyper-parameter. Wemb is randomly initialized and trained
together with the GRU.

• RNN+: We use the RNN model with the same setup as before, but we initialize the embedding matrix
Wemb with GloVe vectors trained only with the co-occurrence of leaf concepts. This is to compare GRAM
with a similar weight initialization technique.

• SimpleRollUp: We use the RNN model with the same setup as before. But for input xt, we replace
all diagnosis codes with their direct parent codes in the CCS multi-level hierarchy, giving us 578, 526 and
517 input codes respectively for Sutter data, MIMIC-III and Sutter HF cohort. This is to compare the
performance of GRAM with a common grouping technique.

• RollUpRare: We use the RNN model with the same setup as before, but we replace any diagnosis
code whose frequency is less than a certain threshold in the dataset with its direct parent. We set the
threshold to 100 for Sutter data and Sutter HF cohort, and 10 for MIMIC-III, giving us 4,408, 935 and
1,538 input codes respectively for Sutter data, MIMIC-III and Sutter HF cohort. This is an intuitive way
of dealing with infrequent medical codes.

Hyper-parameter Tuning: We deﬁne ﬁve hyper-parameters for GRAM:

• dimensionality m of the basic embedding ei: [100, 200, 300, 400, 500]

• dimensionality r of the RNN hidden layer ht from Eq. (4): [100, 200, 300, 400, 500]

2https://www.hcup-us.ahrq.gov/toolssoftware/ccs/AppendixCMultiDX.txt
3http://www.icd9data.com/2015/Volume1/default.htm

7

• dimensionality l of Wa and ba from Eq. (3): [100, 200, 300, 400, 500]

• L2 regularization coeﬃcient for all weights except RNN weights: [0.1, 0.01, 0.001, 0.0001]

• dropout rate for the dropout on the RNN hidden layer: [0.0, 0.2, 0.4, 0.6, 0.8]

We performed 100 iterations of the random search by using the above ranges for each of the three prediction
experiments. In order to fairly compare the model performances, we matched the number of model parameters
to be similar for all baseline methods. To facilitate reproducibility, ﬁnal hyper-parameter settings we
used for all models for each prediction experiments are described at the source code repository, https:
//github.com/mp2893/gram, along with the detailed steps we used to tune the hyper-parameters.

3.2 Prediction performance and scalability

Model

0-20

20-40

40-60

60-80

80-100

0.4238
0.4903
0.0150 0.3242 0.4325
GRAM+
0.4193
0.4895
0.4224
0.2987
0.0042
GRAM
0.4853
0.4059
0.4010
0.2700
RandomDAG 0.0050
0.4212 0.4959
0.4140
0.2742
0.0069
RNN+
0.2691
RNN
0.4951
0.4227
0.4134
0.0080
0.3078 0.4369 0.4330 0.4924
SimpleRollUp 0.0085
0.4956
0.2768
0.0062
RollUpRare

0.4176

0.4226

(a) Accuracy@5 of sequential diagnoses prediction on Sutter data

Model

0-20

20-40

40-60

60-80

80-100

0.0672 0.1787 0.2644 0.2490
GRAM+
0.6267
GRAM
0.1935
0.2296
0.0556
0.6363
0.1346
0.1512
RandomDAG 0.0329
0.4494
0.2080
0.2494
RNN+
0.0454
0.6239
0.6243
0.2371
0.1804
0.0454
RNN
0.2455 0.2667 0.6387
SimpleRollUp 0.0578
0.6277
0.2364
0.1843
0.0454
RollUpRare

0.1016
0.0708
0.0843
0.0731
0.1328
0.0653

(b) Accuracy@20 of sequential diagnoses prediction on MIMIC-III

Model

10%

20%

30%

40%

50%

60%

70%

80%

90%

100%

GRAM+
GRAM
RandomDAG 0.7644
0.7930
RNN+
RNN
0.7811
SimpleRollUp 0.7799
0.7830
RollUpRare

0.7970 0.8223 0.8307 0.8332 0.8389 0.8404 0.8452 0.8456 0.8447 0.8448
0.8447
0.7981 0.8217 0.8340 0.8332 0.8372
0.8226
0.8143
0.7986
0.8335
0.8261
0.8162
0.8314
0.8156
0.8066
0.8258
0.8177
0.8108
0.8291
0.8211
0.8064

0.8440
0.8274
0.8343
0.8258
0.8223
0.8262

0.8377
0.8185
0.8333
0.8207
0.8207
0.8202

0.8431
0.8312
0.8353
0.8278
0.8272
0.8296

0.8430
0.8254
0.8345
0.8297
0.8269
0.8307

0.7882
0.8117
0.7942
0.8022
0.8067

0.8070
0.8215
0.8111
0.8133
0.8119

(c) AUC of HF onset prediction on Sutter HF cohort

Table 2: Performance of three prediction tasks. The x-axis of (a) and (b) represents the labels grouped by the
percentile of their frequencies in the training data in non-decreasing order. 0-20 are the most rare diagnosese
while 80-100 are the most common ones. (b) uses Accuracy@20 because MIMIC-III has a large average
number of codes per visit (see Table 1). For (c), we vary the size of the training data to train the models.

8

Table 3: Scalablity result in per epoch training time in second (the number of epochs needed). SDP stands
for Sequential Diagnoses Prediction

Model

SDP
(Sutter data)

SDP
(MIMIC-III)

HF prediction
(Sutter HF cohort)

GRAM
RNN

525s (39 epochs)
352s (24 epochs)

2s (11 epochs)
1s (6 epochs)

12s (7 epochs)
8s (5 epochs)

Tables 2a and 2b show the sequential diagnoses prediction performance on Sutter data and MIMIC-
III. Both ﬁgures show that GRAM+ outperforms other models when predicting labels with signiﬁcant data
insuﬃciency (i.e. less observed in the training data).The performance gain is greater for MIMIC-III, where
GRAM+ outperforms the basic RNN by 10% in the 20th-40th percentile range. This seems to come from the fact
that MIMIC patients on average have signiﬁcantly shorter visit history than Sutter patients, with much more
codes received per visit. Such short sequences make it diﬃcult for the RNN to learn and predict diagnoses
sequence. The performance diﬀerence between GRAM+ and GRAM suggests that our proposed initialization
scheme of the basic embeddings ei is important for sequential diagnosis prediction.

Table 2c shows the HF prediction performance on Sutter HF cohort. GRAM and GRAM+ consistently
outperforms other baselines (except RNN+) by 3∼4% AUC, and RNN+ by maximum 1.8% AUC. These
diﬀerences are quite signiﬁcant given that the AUC is already in the mid-80s, a high value for HF prediction,
cf. (Choi et al., 2016e). Note that, for GRAM+ and RNN+, we used the downsampled training data to
initialize the basic embeddings ei’s and the embedding matrix Wemb with GloVe, respectively. The result
shows that the initialization scheme of the basic embeddings in GRAM+ gives limited improvement over GRAM.
This stems from the diﬀerent natures of the two prediction tasks. While the goal of HF prediction is to
predict a binary label for the entire visit sequence, the goal of sequential diagnosis prediction is to predict the
co-occurring diagnosis codes at every visit. Therefore the co-occurrence information infused by the initialized
embedding scheme is more beneﬁcial to sequential diagnosis prediction. Additionally, this beneﬁt is associated
with the natures of the two prediction tasks than the datasets used for the prediction tasks. Because the
initialized embedding shows diﬀerent degrees of improvement as shown by Tables 2a and 2c, when Sutter
HF cohort is a subset of Sutter PAMF, thus having similar characteristics. Overall, GRAM showed superior
predictive performance under data insuﬃciency in three diﬀerent experiments, demonstrating its general
applicability in clinical predictive modeling. Now we brieﬂy discuss the scalability of GRAM by comparing its
training time to RNN’s. Table 3 shows the number of seconds taken for the two models to train for a single
epoch for each predictive modeling task. GRAM+ and RNN+ showed the similar behavior as GRAM and RNN.
GRAM takes approximately 50% more time to train for a single epoch for all prediction tasks. This stems from
calculating attention weights and the ﬁnal representations gi for all medical codes. GRAM also generally takes
about 50% more epochs to reach to the model with the lowest validation loss. This is due to optimizing an
extra MLP model that generates the attention weights. Overall, use of GRAM adds a manageable amount of
overhead in training time to the plain RNN.

3.3 Qualitative evaluation of interpretable representations

To qualitatively assess the interpretability of the learned representations of the medical codes, we plot on
a 2-D space using t-SNE (Maaten and Hinton, 2008) the ﬁnal representations gi of 2,000 randomly chosen
diseases learned by GRAM+ for sequential diagnoses prediction on Sutter data4 (Figure 3a). The color of
the dots represents the highest disease categories and the text annotations represent the detailed disease
categories in CCS multi-level hierarchy. For comparison, we also show the t-SNE plots on the strongest
results from GRAM (Figure 3b), RNN+ (Figure 3c), RNN (Figure 3d) and RandomDAG (Figure 3e). GloVe
(Figure 3f) and Skip-gram (Figure 3g) were trained on the Sutter data, where a single visit Vt was used as
the context window to calculate the co-occurrence of codes.

Figures 3c and 3f conﬁrm that interpretable representations cannot simply be learned only by co-occurrence

or supervised prediction without medical knowledge. GRAM+ and GRAM learn interpretable disease

4The scatterplots of models trained for sequential diagnoses prediction on MIMIC-III and HF prediction for Sutter HF cohort

were similar but less structured due to smaller data size.

9

(a) Scatterplot of the ﬁnal representations gi’s of GRAM+

(b) Scatterplot of the ﬁnal representations gi’s of GRAM

(c) Scatterplot of the trained embedding matrix Wemb
of RNN+

(d) Scatterplot of the trained embedding matrix
Wemb of RNN

(e) Scatterplot of the ﬁnal representations gi’s of
RandomDAG

(f) Scatterplot of the disease representations trained
by GloVe

(g) Scatterplot of the basic embeddings ei’s trained
by Skip-gram

Figure 3: t-SNE scatterplots of medical concepts trained by GRAM+, GRAM, RNN+, RNN, RandomDAG,
GloVe and Skip-gram. The color of the dots represents the highest disease categories and the text annotations
represent the detailed disease categories in CCS multi-level hierarchy. It is clear that GRAM+ and GRAM exhibit
interpretable embedding that are well aligned with the medical ontology.

10

representations that are signiﬁcantly more consistent with the given knowledge DAG G. Based on the
prediction performance shown by Table 2, and the fact that the representations gi’s are the ﬁnal product
of GRAM, we can infer that such medically meaningful representations are necessary for predictive models to
cope with data insuﬃciency and make more accurate predictions. Figure 3b shows that the quality of the
ﬁnal representations gi of GRAM is quite similar to GRAM+. Compared to other baselines, GRAM demonstrates
signiﬁcantly more structured representations that align well with the given knowledge DAG. It is interesting
that Skip-gram shows the most structured representation among all baselines. We used GloVe to initialize
the basic embeddings ei in this work because it uses global co-occurrence information and its training time
is fast as it is only dependent only on the total number of unique concepts |C|. Skip-gram’s training time,
on the other hand, depends on both the number of patients and the number of visits each patient made,
which makes the algorithm generally slower than GloVe. An interactive visualization tool can be accessed at
http://www.sunlab.org/research/gram-graph-based-attention-model/.

3.4 Analysis of the attention behavior

Figure 4: GRAM’s attention behavior during HF prediction for four representative diseases (each column). In
each ﬁgure, the leaf node represents the disease and upper nodes are its ancestors. The size of the node shows
the amount of attention it receives, which is also shown by the bar charts. The number in the parenthesis
next to the disease is its frequency in the training data. We exclude the root of the knowledge DAG G from
all ﬁgures as it did not play a signiﬁcant role.

Next we show that GRAM’s attention can be explained intuitively based on the data availability and
knowledge DAG’s structure when performing a prediction task. Using Eq. (1), we can calculate the attention
weights of individual disease. Figure 4 shows the attention behaviors of four representative diseases when
performing HF prediction on Sutter HF cohort.

Other pneumothorax (ICD9 512.89) in Figure 4a is rarely observed in the data and has only ﬁve siblings.
In this case, most information is derived from the highest ancestor. Temporomandibular joint disorders &
articular disc disorder (ICD9 524.63) in Figure 4b is rarely observed but has 139 siblings. In this case, its
parent receives a stronger attention because it aggregates suﬃcient samples from all of its children to learn a
more accurate representation. Note that the disease itself also receives a stronger attention to facilitate easier
distinction from its large number of siblings.

Unspeciﬁed essential hypertension (ICD9 401.9) in Figure 4c is very frequently observed but has only two
siblings. In this case, GRAM assigns a very strong attention to the leaf, which is logical because the more you
observe a disease, the stronger your conﬁdence becomes. Need for prophylactic vaccination and inoculation
against inﬂuenza (ICD9 V04.81) in Figure 4d is quite frequently observed and also has 103 siblings. The
attention behavior in this case is quite similar to the case with fewer siblings (Figure 4b) with a slight
attention shift towards the leaf concept as more observations lead to higher conﬁdence.

4 Related Work

The attention mechanism is a general framework for neural network learning (Bahdanau et al., 2014), and
has been since used in many areas such as speech recognition (Chorowski et al., 2014), computer vision (Ba

11

et al., 2014; Xu et al., 2015) and healthcare (Choi et al., 2016b). However, no one has designed attention
model based on knowledge ontology, which is the focus of this work.

There are related works in learning the representations of graphs. Several studies focused on learning the
representations of graph vertices by using the neighbor information. DeepWalk (Perozzi et al., 2014) and
node2vec (Grover and Leskovec, 2016) use random walk while LINE (Tang et al., 2015) uses breadth-ﬁrst
search to ﬁnd the neighbors of a vertex and learn its representation based on the neighbor information.
Graph convolutional approaches (Yang et al., 2016; Kipf and Welling, 2016) also focus on learning the vertex
representations to mainly perform vertex classiﬁcation. All those works focus on solving the graph data
problems whereas GRAM focuses on solving clinical predictive modeling problems using the knowledge DAG as
supplementary information.

Several researchers tried to model the knowledge DAG such as WordNet (Miller, 1995) or Freebase
(Bollacker et al., 2008) where two entities are connected with various types of relation, forming a set of triples.
They aim to project entities and relations (Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014; Lin
et al., 2015) to the latent space based on the triples or additional information such as hierarchy of entities
(Xie et al., 2016). These works demonstrated tasks such as link prediction, triple classiﬁcation or entity
classiﬁcation using the learned representations. More recently, Li et al. (2016) learned the representations of
words and Wikipedia categories by utilizing the hierarchy of Wikipedia categories. GRAM is fundamentally
diﬀerent from the above studies in that it aims to design intuitive attention mechanism on the knowledge
DAG as a knowledge prior to cope with data insuﬃciency and learn medically interpretable representations
to make accurate predictions.

A classical approach for incorporating side information in the predictive models is to use graph Laplacian
regularization (Weinberger et al., 2006; Che et al., 2015). However, using this approach is not straightforward
as it relies on the appropriate deﬁnition of distance on graphs which is often unavailable.

5 Conclusion

Data insuﬃciency, either due to less common diseases or small datasets, is one of the key hurdles in healthcare
analytics, especially when we apply deep neural networks models. To overcome this challenge, we leverage the
knowledge DAG, which provides a multi-resolution view of medical concepts. We propose GRAM, a graph-based
attention model using both a knowledge DAG and EHR to learn an accurate and interpretable representations
for medical concepts. GRAM chooses a weighted average of ancestors of a medical concept and train the entire
process with a predictive model in an end-to-end fashion. We conducted three predictive modeling experiments
on real EHR datasets and showed signiﬁcant improvement in the prediction performance, especially on
low-frequency diseases and small datasets. Analysis of the attention behavior provided intuitive insight of
GRAM.

References

arXiv:1412.7755 (2014).

Jimmy Ba, Volodymyr Mnih, and Koray Kavukcuoglu. 2014. Multiple object recognition with visual attention.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural Machine Translation by Jointly

Learning to Align and Translate. arXiv:1409.0473 (2014).

Yoshua Bengio, Patrice Simard, and Paolo Frasconi. 1994. Learning long-term dependencies with gradient

descent is diﬃcult. IEEE Transactions on Neural Networks 5, 2 (1994).

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively

created graph database for structuring human knowledge. In SIGMOD.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013.

Translating embeddings for modeling multi-relational data. In NIPS.

12

Zhengping Che, David Kale, Wenzhe Li, Mohammad Taha Bahadori, and Yan Liu. 2015. Deep Computational

Phenotyping. In SIGKDD.

Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. 2016. Recurrent Neural

Networks for Multivariate Time Series with Missing Values. arXiv:1606.01865 (2016).

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for
statistical machine translation. In EMNLP.

Edward Choi, Mohammad Taha Bahadori, Andy Schuetz, Walter F. Stewart, and Jimeng Sun. 2016a. Doctor

AI: Predicting Clinical Events via Recurrent Neural Networks. In MLHC.

Edward Choi, Mohammad Taha Bahadori, Andy Schuetz, Walter F. Stewart, and Jimeng Sun. 2016b.
RETAIN: Interpretable Predictive Model in Healthcare using Reverse Time Attention Mechanism. In
NIPS.

Edward Choi, Mohammad Taha Bahadori, Elizabeth Searles, Catherine Coﬀey, Michael Thompson, James
Bost, Javier T Sojo, and Jimeng Sun. 2016c. Multi-layer Representation Learning for Medical Concepts. In
SIGKDD.

Edward Choi, Andy Schuetz, Walter F. Stewart, and Jimeng Sun. 2016e. Using Recurrent Neural Network
Models for Early Detection of Heart Failure Onset. Journal of the American Medical Informatics Association
(2016), ocw112.

Youngduck Choi, Chill Yi-I Chiu, and David Sontag. 2016d. Learning Low-Dimensional Representations of

Medical Concepts. (2016). AMIA CRI.

Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. End-to-end continuous

speech recognition using attention-based recurrent NN: First results. arXiv:1412.1602 (2014).

Ary Goldberger and others. 2000. Physiobank, physiotoolkit, and physionet components of a new research

resource for complex physiologic signals. Circulation (2000).

Aditya Grover and Jure Leskovec. 2016. Node2Vec: Scalable Feature Learning for Networks. In SIGKDD.

Jerry Gurwitz, David Magid, David Smith, Robert Goldberg, David McManus, Larry Allen, Jane Saczynski,
Micah Thorp, Grace Hsu, Sue Hee Sung, and others. 2013. Contemporary prevalence and correlates of
incident heart failure with preserved ejection fraction. The American journal of medicine 126, 5 (2013).

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural Computation 9, 8 (1997).

Alistair Johnson and others. 2016. MIMIC-III, a freely accessible critical care database. Scientiﬁc Data 3

Thomas N Kipf and Max Welling. 2016. Semi-supervised classiﬁcation with graph convolutional networks.

(2016).

arXiv:1609.02907 (2016).

Quoc V Le, Navdeep Jaitly, and Geoﬀrey E Hinton. 2015. A Simple Way to Initialize Recurrent Networks of

Rectiﬁed Linear Units. arXiv:1504.00941 (2015).

Yuezhang Li, Ronghuo Zheng, Tian Tian, Zhiting Hu, Rahul Iyer, and Katia Sycara. 2016. Joint Embedding
of Hierarchical Categories and Entities for Concept Categorization and Dataless Classiﬁcation. (2016).

Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning Entity and Relation

Embeddings for Knowledge Graph Completion. In AAAI.

13

Zachary C Lipton, David C Kale, Charles Elkan, and Randall Wetzell. 2015. Learning to Diagnose with

LSTM Recurrent Neural Networks. arXiv:1511.03677 (2015).

Zachary C Lipton, David C Kale, and Randall Wetzel. 2016. Modeling Missing Data in Clinical Time Series

with RNNs. In MLHC.

Laurens van der Maaten and Geoﬀrey Hinton. 2008. Visualizing data using t-SNE. JMLR 9, Nov (2008).

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeﬀ Dean. 2013. Distributed representations of

words and phrases and their compositionality. In NIPS.

George A Miller. 1995. WordNet: a lexical database for English. Commun. ACM 38, 11 (1995).

Riccardo Miotto, Li Li, Brian A Kidd, and Joel T Dudley. 2016. Deep Patient: An Unsupervised Representation

to Predict the Future of Patients from the Electronic Health Records. Scientiﬁc Reports 6 (2016).

Phuoc Nguyen, Truyen Tran, Nilmini Wickramasinghe, and Svetha Venkatesh. 2016. Deepr: A Convolutional

Net for Medical Records. arXiv:1607.07519 (2016).

Jeﬀrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global Vectors for Word

Representation. In EMNLP.

In SIGKDD.

Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online Learning of Social Representations.

Healthcare Cost & Utilization Project and others. 2010. Clinical classiﬁcations software (CCS) for ICD-9-CM.

Rockville, MD: Agency for Healthcare Research and Quality (2010).

Narges Razavian, Jake Marcus, and David Sontag. 2016. Multi-task Prediction of Disease Onsets from

Longitudinal Lab Tests. In MLHC.

Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. 2013. Reasoning with neural tensor

networks for knowledge base completion. In NIPS.

Michael Q Stearns, Colin Price, Kent A Spackman, and Amy Y Wang. 2001. SNOMED clinical terms:

overview of the development process and project status. In AMIA.

Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. LINE: Large-scale

Information Network Embedding. In WWW.

The Theano Development Team. 2016. Theano: A Python framework for fast computation of mathematical

expressions. arXiv:1605.02688 (2016).

Rajakrishnan Vijayakrishnan, Steven Steinhubl, Kenney Ng, Jimeng Sun, Roy Byrd, Zahra Daar, Brent
Williams, Shahram Ebadollahi, Walter Stewart, and others. 2014. Prevalence of heart failure signs and
symptoms in a large primary care population identiﬁed through the use of text and data mining of the
electronic health record. Journal of cardiac failure 20, 7 (2014).

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge Graph Embedding by Translating

on Hyperplanes. In AAAI.

Kilian Q Weinberger, Fei Sha, Qihui Zhu, and Lawrence K Saul. 2006. Graph Laplacian Regularization for

Large-Scale Semideﬁnite Programming. In NIPS.

Ruobing Xie, Zhiyuan Liu, and Maosong Sun. 2016. Representation Learning of Knowledge Graphs with

Hierarchical Types. In IJCAI.

14

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel,
and Yoshua Bengio. 2015. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention..
In ICML.

Zhilin Yang, William Cohen, and Ruslan Salakhutdinov. 2016. Revisiting Semi-Supervised Learning with

Graph Embeddings. arXiv:1603.08861 (2016).

Matthew D Zeiler. 2012. ADADELTA: an adaptive learning rate method. arXiv:1212.5701 (2012).

15

7
1
0
2
 
r
p
A
 
1
 
 
]

G
L
.
s
c
[
 
 
3
v
2
1
0
7
0
.
1
1
6
1
:
v
i
X
r
a

GRAM: Graph-based Attention Model for Healthcare
Representation Learning

Edward Choi∗, Mohammad Taha Bahadori∗, Le Song∗, Walter F. Stewart†, Jimeng Sun∗

∗ Georgia Institute of Technology

† Sutter Health

{mp2893,bahadori}@gatech.edu, lsong@cc.gatech.edu, stewarwf@sutterhealth.org, jsun@cc.gatech.edu

Abstract

Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two

important challenges remain:

• Data insuﬃciency: Often in healthcare predictive modeling, the sample size is insuﬃcient for deep

learning methods to achieve satisfactory results.

• Interpretation: The representations learned by deep learning methods should align with medical

knowledge.

To address these challenges, we propose a GRaph-based Attention Model, GRAM that supplements electronic
health records (EHR) with hierarchical information inherent to medical ontologies. Based on the data
volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in
the ontology via an attention mechanism.

We compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various
methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and
one heart failure prediction task. Compared to the basic RNN, GRAM achieved 10% higher accuracy for
predicting diseases rarely observed in the training data and 3% improved area under the ROC curve
for predicting heart failure using an order of magnitude less training data. Additionally, unlike other
methods, the medical concept representations learned by GRAM are well aligned with the medical ontology.
Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts
when facing data insuﬃciency at the lower level concepts.

1

Introduction

The rapid growth in volume and diversity of health care data from electronic health records (EHR) and other
sources is motivating the use of predictive modeling to improve care for individual patients. In particular,
novel applications are emerging that use deep learning methods such as word embedding (Choi et al., 2016c,d),
recurrent neural networks (RNN) (Che et al., 2016; Choi et al., 2016a,b; Lipton et al., 2016), convolutional
neural networks (CNN) (Nguyen et al., 2016) or stacked denoising autoencoders (SDA) (Che et al., 2015;
Miotto et al., 2016), demonstrating signiﬁcant performance enhancement for diverse prediction tasks. Deep
learning models appear to perform signiﬁcantly better than logistic regression or multilayer perceptron (MLP)
models that depend, to some degree, on expert feature construction (Lipton et al., 2015; Razavian et al.,
2016).

Training deep learning models typically requires large amounts of data that often cannot be met by a single
health system or provider organization. Sub-optimal model performance can be particularly challenging when
the focus of interest is predicting onset of a rare disease. For example, using Doctor AI (Choi et al., 2016a), we
discovered that RNN alone was ineﬀective to predict the onset of diseases such as cerebral degenerations (e.g.
Leukodystrophy, Cerebral lipidoses) or developmental disorders (e.g. autistic disorder, Heller’s syndrome),
partly because their rare occurrence in the training data provided little learning opportunity to the ﬂexible
models like RNN.

1

Figure 1: The illustration of GRAM. Leaf nodes (solid circles) represents a medical concept in the EHR, while
the non-leaf nodes (dotted circles) represent more general concepts. The ﬁnal representation gi of the leaf
concept ci is computed by combining the basic embeddings ei of ci and eg, ec and ea of its ancestors cg, cc
and ca via an attention mechanism. The ﬁnal representations form the embedding matrix G for all leaf
concepts. After that, we use G to embed patient visit vector xt to a visit representation vt, which is then fed
to a neural network model to make the ﬁnal prediction ˆyt.

The data requirement of deep learning models comes from having to assess exponential number of
combinations of input features. This can be alleviated by exploiting medical ontologies that encodes
hierarchical clinical constructs and relationships among medical concepts. Fortunately, there are many
well-organized ontologies in healthcare such as the International Classiﬁcation of Diseases (ICD), Clinical
Classiﬁcations Software (CCS) (Stearns et al., 2001) or Systematized Nomenclature of Medicine-Clinical
Terms (SNOMED-CT) (Project et al., 2010). Nodes (i.e. medical concepts) close to one another in medical
ontologies are likely to be associated with similar patients, allowing us to transfer knowledge among them.
Therefore, proper use of medical ontologies will be helpful when we lack enough data for the nodes in the
ontology to train deep learning models.

In this work, we propose GRAM, a method that infuses information from medical ontologies into deep
learning models via neural attention. Considering the frequency of a medical concept in the EHR data and
its ancestors in the ontology, GRAM decides the representation of the medical concept by adaptively combining
its ancestors via attention mechanism. This will not only support deep learning models to learn robust
representations without large amount of data, but also learn interpretable representations that align well
with the knowledge from the ontology. The attention mechanism is trained in an end-to-end fashion with
the neural network model that predicts the onset of disease(s). We also propose an eﬀective initialization
technique in addition to the ontological knowledge to better guide the representation learning process.

We compare predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various models
including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart
failure (HF) prediction task. We demonstrate that GRAM is up to 10% more accurate than the basic RNN
for predicting diseases less observed in the training data. After discussing GRAM’s scalability, we visualize
the representations learned from various models, where GRAM provides more intuitive representations by
grouping similar medical concepts close to one another. Finally, we show GRAM’s attention mechanism can
be interpreted to understand how it assigns the right amount of attention to the ancestors of each medical
concept by considering the data availability and the ontology structure.

2 Methodology

We ﬁrst deﬁne the notations describing EHR data and medical ontologies, followed by a description of GRAM
(Section 2.2), the end-to-end training of the attention generation and predictive modeling (Section 2.3), and
the eﬃcient initialization scheme (Section 2.4).

2

2.1 Basic Notation

We denote the set of entire medical codes from the EHR as c1, c2, . . . , c|C| ∈ C with the vocabulary size |C|.
The clinical record of each patient can be viewed as a sequence of visits V1, . . . , VT where each visit contains a
subset of medical codes Vt ⊆ C. Vt can be represented as a binary vector xt ∈ {0, 1}|C| where the i-th element
is 1 only if Vt contains the code ci. To avoid clutter, all algorithms will be presented for a single patient.

We assume that a given medical ontology G typically expresses the hierarchy of various medical concepts in
the form of a parent-child relationship, where the medical codes C form the leaf nodes. Ontology G is represented
as a directed acyclic graph (DAG) whose nodes form a set D = C +C(cid:48). The set C(cid:48) = {c|C|+1, c|C|+2, . . . , c|C|+|C(cid:48)|}
consists of all non-leaf nodes (i.e. ancestors of the leaf nodes), where |C(cid:48)| represents the number of all non-leaf
nodes. We use knowledge DAG to refer to G. A parent in the knowledge DAG G represents a related but
more general concept over its children. Therefore, G provides a multi-resolution view of medical concepts with
diﬀerent degrees of speciﬁcity. While some ontologies are exclusively expressed as parent-child hierarchies
(e.g. ICD-9, CCS), others are not. For example, in some instances SNOMED-CT also links medical concepts
to causal or treatment relationships, but the majority relationships in SNOMED-CT are still parent-child.
Therefore, we focus on the parent-child relationships in this work.

2.2 Knowledge DAG and the Attention Mechanism

GRAM leverages the parent-child relationship of G to learn robust representations when data volume is
constrained. GRAM balances the use of ontology information in relation to data volume in determining the
level of speciﬁcity for a medical concept. When a medical concept is less observed in the data, more weight is
given to its ancestors as they can be learned more accurately and oﬀer general (coarse-grained) information
about their children. The process of resorting to the parent concepts can be automated via the attention
mechanism and the end-to-end training as described in Figure 1.

In the knowledge DAG, each node ci is assigned a basic embedding vector ei ∈ Rm, where m represents the
dimensionality. Then e1, . . . , e|C| are the basic embeddings of the codes c1, . . . , c|C| while e|C|+1, . . . , e|C|+|C(cid:48)|
represent the basic embeddings of the internal nodes c|C|+1, . . . , c|C|+|C(cid:48)|. The initialization of these basic
embeddings is described in Section 2.4. We formulate a leaf node’s ﬁnal representation as a convex combination
of the basic embeddings of itself and its ancestors:

(cid:88)

gi =

αijej,

(cid:88)

j∈A(i)

j∈A(i)

αij = 1, αij ≥ 0 for j ∈ A(i),

where gi ∈ Rm denotes the ﬁnal representation of the code ci, A(i) the indices of the code ci and ci’s
ancestors, ej the basic embedding of the code cj and αij ∈ R the attention weight on the embedding ej when
calculating gi. The attention weight αij in Eq. (1) is calculated by the following Softmax function,

f (ei, ej) is a scalar value representing the compatibility between the basic embeddings of ei and ek. We
compute f (ei, ej) via the following feed-forward network with a single hidden layer (MLP),

αij =

(cid:80)

exp(f (ei, ej))
k∈A(i) exp(f (ei, ek))

f (ei, ej) = u(cid:62)

a tanh(Wa

(cid:21)

(cid:20) ei
ej

+ ba)

where Wa ∈ Rl×2m is the weight matrix for the concatenation of ei and ej, b ∈ Rl the bias vector, and
ua ∈ Rl the weight vector for generating the scalar value. The constant l represents the dimension size
of the hidden layer of f (·, ·). We always concatenate ei and ej in the child-ancestor order. Note that the
compatibility function f is an MLP, because MLP is well known to be a suﬃcient approximator for an
arbitrary function, and we empirically found that our formulation performed better in our use cases than
alternatives such as inner product and Bahdanau et al.’s (Bahdanau et al., 2014).

3

(1)

(2)

(3)

Remarks: The example in Figure 1 is derived based on a single path from ci to ca. However, the same
mechanism can be applicable to multiple paths as well. For example, code ck has two paths to the root ca,
containing ﬁve ancestors in total. Another scenario is where the EHR data contain both leaf codes and some
ancestor codes. We can move those ancestors present in EHR data from the set C(cid:48) to C and apply the same
process as Eq. (1) to obtain the ﬁnal representations for them.

2.3 End-to-End Training with a Predictive Model

We train the attention mechanism together with a predictive model such that the attention mechanism
improves the predictive performance. By concatenating ﬁnal representation g1, g2, . . . , g|C| of all medical
codes, we have the embedding matrix G ∈ Rm×|C| where gi is its i-th column of G. We can then convert
visit Vt to a visit representation vt by multiplying embedding matrix G with multi-hot vector xt indicating
the clinical events in visit Vt as shown in the right side of Figure 1. Finally the visit representation vt will
be used as an input to pass to a predictive model for predicting the target label yt using a neural network
(NN) model. In this work, we use RNN as the choice of the NN model as the task is to perform sequential
diagnoses prediction (Choi et al., 2016a,b) with the objective of predicting the disease codes of the next visit
Vt+1 given the visit records up to the current timestep V1, V2, . . . , Vt, which can be expressed as follows,

(cid:98)yt = (cid:98)xt+1 = Softmax(Wht + b), where
h1, h2, . . . , ht = RNN(v1, v2, . . . , vt; θr), where
v1, v2, . . . , vt = tanh(G[x1, x2, . . . , xt])

(4)

(5)

where xt ∈ R|C| denotes the t-th visit; vt ∈ Rm the t-th visit representation; ht ∈ Rr the RNN’s hidden layer
at t-th time step (i.e. t-th visit); θr RNN’s parameters; W ∈ R|C|×r and b ∈ R|C| the weight matrices and
the bias vector of the Softmax function; r denotes the dimension size of the hidden layer. We use “RNN” to
denote any recurrent neural network variants that can cope with the vanishing gradient problem (Bengio
et al., 1994), such as LSTM (Hochreiter and Schmidhuber, 1997), GRU (Cho et al., 2014), and IRNN (Le
et al., 2015), with any varying numbers of hidden layers. The prediction loss for all time steps is calculated
using the binary cross entropy as follows,

L(x1, x2 . . . , xT ) = −

1
T − 1

T −1
(cid:88)

(cid:16)

t=1

(cid:17)
(cid:62) log((cid:98)yt) + (1 − yt)(cid:62) log(1 − (cid:98)yt)

yt

where we sum the cross entropy errors from all timestamps of (cid:98)yt, T denotes the number of timestamps of
the visit sequence. Note that the above loss is deﬁned for a single patient. In actual implementation, we
will take the average of the individual loss for multiple patients. Algorithm 1 describes the overall training
procedure of GRAM, under the assumption that we are performing the sequential diagnoses prediction task
using an RNN. Note that Algorithm 1 describes stochastic gradient update to avoid clutter, but it can be
easily extended to other gradient based optimization such as mini-batch gradient update.

2.4

Initializing Basic Embeddings

The attention generation mechanism in Section 2.2 requires basic embeddings ei of each node in the knowledge
DAG. The basic embeddings of ancestors, however, pose a diﬃculty because they are often not observed in
the data. To properly initialize them, we use co-occurrence information to learn the basic embeddings of
medical codes and their ancestors. Co-occurrence has proven to be an important source of information when
learning representations of words or medical concepts (Mikolov et al., 2013; Choi et al., 2016c,d). To train
the basic embeddings, we employ GloVe (Pennington et al., 2014), which uses the global co-occurrence matrix
of words to learn their representations. In our case, the co-occurrence matrix of the codes and the ancestors
was generated by counting the co-occurrences within each visit Vt, where we augment each visit with the
ancestors of the codes in the visit. We describe the details of the initialization algorithm with an example.
We borrow the parent-child relationships from the knowledge DAG of Figure 1. Given a visit Vt,

4

Algorithm 1 GRAM Optimization

Randomly initialize basic embedding matrix E, attention parameters ua, Wa, ba, RNN parameter θr,
softmax parameters W, b.
repeat

Update E with GloVe objective function (see Section 2.4)

until convergence
repeat

X ← random patient from dataset
for visit Vt in X do

for code ci in Vt do

Refer G to ﬁnd ci’s ancestors C (cid:48)
for code cj in C (cid:48) do

Calculate attention weight αij using Eq. (2).

end for
Obtain ﬁnal representation gi using Eq. (1).

end for
vt ← tanh((cid:80)
i:ci∈Vt
Make prediction (cid:98)yt using Eq. (4)

gi)

end for
Calculate prediction loss L using Eq .(5)
Update parameters according to the gradient of L

until convergence

we augment it with the ancestors of all the codes to obtain the augmented visit V (cid:48)
t ,

V (cid:48)
t = {cd, cb, ca, ci, cg, cc, ca, ck, cj, cf , cc, cb, ca}

Vt = {cd, ci, ck}

where the augmented ancestors are underlined. Note that a single ancestor can appear multiple times in V (cid:48)
t .
In fact, the higher the ancestor is in the knowledge DAG, the more times it is likely to appear in V (cid:48)
t . We
count the co-occurrence of two codes in V (cid:48)

t as follows,

co-occurrence(ci, cj, V (cid:48)

t ) = count(ci, V (cid:48)

t ) × count(cj, V (cid:48)
t )

where count(ci, V (cid:48)
t ) is the number of times the code ci appears in the augmented visit V (cid:48)
t . For example, the
co-occurrence between the leaf code ci and the root ca is 3. However, the co-occurrence between the ancestor
cc and the root ca is 6. Therefore our algorithm will make the higher ancestor codes, which are more general
concepts, have more involvement in all medical events (i.e. visits), which is natural in healthcare application
as those general concepts are often reliable. We repeat this calculation for all pairs of codes in all augmented
visits of all patients to obtain the co-occurrence matrix M ∈ R|D|×|D| depicted by Figure 2. For training the
embedding vectors ei’s using M, we minimize the following loss function as described in Pennington et al.
(2014).

|D|
(cid:88)

i,j=1

(cid:40)

J =

f (Mij)(e(cid:62)

i ej + bi + bj − log Mij)2

where f (x) =

(x < xmax)α
1

if x < xmax
otherwise

where the hyperparameters xmax and α are respectively set to 100 and 0.75 as the original paper (Pennington
et al., 2014). Note that, after the initialization, the basic embeddings ei’s of both leaf nodes (i.e. medical
codes) and non-leaf nodes (i.e. ancestors) are ﬁne-tuned during model training via backpropagation.

5

Figure 2: Creating the co-occurrence matrix together with the ancestors. The n-th ancestors are the group of
nodes that are n hops away from any leaf node in G. Here we exclude the root node, which will be just a
single row (column).

Table 1: Basic statistics of Sutter PAMF, MIMIC-III and Sutter heart failure (HF) cohort.

Dataset

Sutter PAMF MIMIC-III

# of patients
# of visits
Avg. # of visits per patient
# of unique ICD9 codes
Avg. # of codes per visit
Max # of codes per visit

258,555†
13,920,759
53.8
10,437
1.98
54

7,499†
19,911
2.66
4,893
13.1
39

Sutter HF cohort
30,727† (3,408 cases)
572,551
38.38
5,689
2.06
29

† For all datasets, we chose patients who made at least two visits.

3 Experiments

We conduct three experiments to determine if GRAM oﬀered superior prediction performance when facing
data insuﬃciency. We ﬁrst describe the experimental setup followed by results comparing predictive
performance of GRAM with various baseline models. After discussing GRAM’s scalability, we qualitatively
evaluate the interpretability of the resulting representation. The source code of GRAM is publicly available at
https://github.com/mp2893/gram.

3.1 Experiment Setup

Prediction tasks and source of data: We conduct the sequential diagnoses prediction (SDP) tasks on
two datasets, which aim at predicting all diagnosis categories in the next visit, and a heart failure (HF)
prediction task on one dataset, which is a binary prediction task for predicting a future HF onset where the
prediction is made only once at the last visit xT .
Two sequential diagnoses predictions (SDP) are respectively conducted using two datasets: 1) Sutter Palo Alto
Medical Foundation (PAMF) dataset, which consists of 18-years longitudinal medical records of 258K patients
between age 50 and 90. This will determine GRAM’s performance for general adult population with long visit
records. 2) MIMIC-III dataset (Johnson et al., 2016; Goldberger et al., 2000), which is a publicly available
dataset consisting of medical records of 7.5K intensive care unit (ICU) patients over 11 years. This will
determine GRAM’s performance for high-risk patients with very short visit records. We utilize all the patients
with at least 2 visits. We prepared the true labels yt by grouping the ICD9 codes into 283 groups using CCS
single-level diagnosis grouper1. This is to improve the training speed and predictive performance for easier
analysis, while preserving suﬃcient granularity for each diagnosis. Each diagnosis code’s varying frequency in
the training data can be viewed as diﬀerent degrees of data insuﬃciency. We calculate Accuracy@k for each

1https://www.hcup-us.ahrq.gov/toolssoftware/ccs/AppendixASingleDX.txt

6

of CCS single-level diagnosis codes such that, given a visit Vt, we get 1 if the target diagnosis is in the top k
guesses and 0 otherwise.
We conduct HF prediction on Sutter heart failure (HF) cohort, which is a subset of Sutter PAMF data for a
heart failure onset prediction study with 3.4K HF cases chosen by a set of criteria described in Vijayakrishnan
et al. (2014); Gurwitz et al. (2013) and 27K matching controls chosen by a set of criteria described in Choi
et al. (2016e). This will determine GRAM’s performance for a diﬀerent prediction task where we predict the
onset of one speciﬁc condition. We randomly downsample the training data to create diﬀerent degrees of
data insuﬃciency. We use area under the ROC curve (AUC) to measure the performance.
A summary of the datasets are provided in Table 1.We used CCS multi-level diagnoses hierarchy2 as our
knowledge DAG G. We also tested the ICD9 code hierarchy3, but the performance was similar to using CCS
multi-level hierarchy. For all three tasks, we randomly divide the dataset into the training, validation and test
set by .75:.10:.15 ratio, and use the validation set to tune the hyper-parameters. Further details regarding
the hyper-parameter tuning are provided below. The test set performance is reported in the paper.
Implementation details: We implemented GRAM with Theano 0.8.2 (Team, 2016). For training models,
we used Adadelta (Zeiler, 2012) with a mini-batch of 100 patients, on a machine equipped with Intel Xeon
E5-2640, 256GB RAM, four Nvidia Titan X’s and CUDA 7.5.
Models for comparison are the following. The ﬁrst two GRAM+ and GRAM are the proposed methods and the
rest are baselines. Hyper-parameter tuning is conﬁgured so that the number of parameters for the baselines
would be comparable to GRAM’s. Further details are provided below.

• GRAM: Input sequence x1, . . . , xT is ﬁrst transformed by the embedding matrix G, then fed to the GRU
with a single hidden layer, which in turn makes the prediction, as described by Eq. (4). The basic
embeddings ei’s are randomly initialized.

• GRAM+: We use the same setup as GRAM, but the basic embeddings ei’s are initialized according to Section

2.4.

• RandomDAG: We use the same setup as GRAM, but each leaf concept has ﬁve randomly assigned

ancestors from the CCS multi-level hierarchy to test the eﬀect of correct domain knowledge.

• RNN: Input xt is transformed by an embedding matrix Wemb ∈ Rk×|C|, then fed to the GRU with a
single hidden layer. The embedding size k is a hyper-parameter. Wemb is randomly initialized and trained
together with the GRU.

• RNN+: We use the RNN model with the same setup as before, but we initialize the embedding matrix
Wemb with GloVe vectors trained only with the co-occurrence of leaf concepts. This is to compare GRAM
with a similar weight initialization technique.

• SimpleRollUp: We use the RNN model with the same setup as before. But for input xt, we replace
all diagnosis codes with their direct parent codes in the CCS multi-level hierarchy, giving us 578, 526 and
517 input codes respectively for Sutter data, MIMIC-III and Sutter HF cohort. This is to compare the
performance of GRAM with a common grouping technique.

• RollUpRare: We use the RNN model with the same setup as before, but we replace any diagnosis
code whose frequency is less than a certain threshold in the dataset with its direct parent. We set the
threshold to 100 for Sutter data and Sutter HF cohort, and 10 for MIMIC-III, giving us 4,408, 935 and
1,538 input codes respectively for Sutter data, MIMIC-III and Sutter HF cohort. This is an intuitive way
of dealing with infrequent medical codes.

Hyper-parameter Tuning: We deﬁne ﬁve hyper-parameters for GRAM:

• dimensionality m of the basic embedding ei: [100, 200, 300, 400, 500]

• dimensionality r of the RNN hidden layer ht from Eq. (4): [100, 200, 300, 400, 500]

2https://www.hcup-us.ahrq.gov/toolssoftware/ccs/AppendixCMultiDX.txt
3http://www.icd9data.com/2015/Volume1/default.htm

7

• dimensionality l of Wa and ba from Eq. (3): [100, 200, 300, 400, 500]

• L2 regularization coeﬃcient for all weights except RNN weights: [0.1, 0.01, 0.001, 0.0001]

• dropout rate for the dropout on the RNN hidden layer: [0.0, 0.2, 0.4, 0.6, 0.8]

We performed 100 iterations of the random search by using the above ranges for each of the three prediction
experiments. In order to fairly compare the model performances, we matched the number of model parameters
to be similar for all baseline methods. To facilitate reproducibility, ﬁnal hyper-parameter settings we
used for all models for each prediction experiments are described at the source code repository, https:
//github.com/mp2893/gram, along with the detailed steps we used to tune the hyper-parameters.

3.2 Prediction performance and scalability

Model

0-20

20-40

40-60

60-80

80-100

0.4238
0.4903
0.0150 0.3242 0.4325
GRAM+
0.4193
0.4895
0.4224
0.2987
0.0042
GRAM
0.4853
0.4059
0.4010
0.2700
RandomDAG 0.0050
0.4212 0.4959
0.4140
0.2742
0.0069
RNN+
0.2691
RNN
0.4951
0.4227
0.4134
0.0080
0.3078 0.4369 0.4330 0.4924
SimpleRollUp 0.0085
0.4956
0.2768
0.0062
RollUpRare

0.4176

0.4226

(a) Accuracy@5 of sequential diagnoses prediction on Sutter data

Model

0-20

20-40

40-60

60-80

80-100

0.0672 0.1787 0.2644 0.2490
GRAM+
0.6267
GRAM
0.1935
0.2296
0.0556
0.6363
0.1346
0.1512
RandomDAG 0.0329
0.4494
0.2080
0.2494
RNN+
0.0454
0.6239
0.6243
0.2371
0.1804
0.0454
RNN
0.2455 0.2667 0.6387
SimpleRollUp 0.0578
0.6277
0.2364
0.1843
0.0454
RollUpRare

0.1016
0.0708
0.0843
0.0731
0.1328
0.0653

(b) Accuracy@20 of sequential diagnoses prediction on MIMIC-III

Model

10%

20%

30%

40%

50%

60%

70%

80%

90%

100%

GRAM+
GRAM
RandomDAG 0.7644
0.7930
RNN+
RNN
0.7811
SimpleRollUp 0.7799
0.7830
RollUpRare

0.7970 0.8223 0.8307 0.8332 0.8389 0.8404 0.8452 0.8456 0.8447 0.8448
0.8447
0.7981 0.8217 0.8340 0.8332 0.8372
0.8226
0.8143
0.7986
0.8335
0.8261
0.8162
0.8314
0.8156
0.8066
0.8258
0.8177
0.8108
0.8291
0.8211
0.8064

0.8440
0.8274
0.8343
0.8258
0.8223
0.8262

0.8430
0.8254
0.8345
0.8297
0.8269
0.8307

0.8431
0.8312
0.8353
0.8278
0.8272
0.8296

0.8377
0.8185
0.8333
0.8207
0.8207
0.8202

0.7882
0.8117
0.7942
0.8022
0.8067

0.8070
0.8215
0.8111
0.8133
0.8119

(c) AUC of HF onset prediction on Sutter HF cohort

Table 2: Performance of three prediction tasks. The x-axis of (a) and (b) represents the labels grouped by the
percentile of their frequencies in the training data in non-decreasing order. 0-20 are the most rare diagnosese
while 80-100 are the most common ones. (b) uses Accuracy@20 because MIMIC-III has a large average
number of codes per visit (see Table 1). For (c), we vary the size of the training data to train the models.

8

Table 3: Scalablity result in per epoch training time in second (the number of epochs needed). SDP stands
for Sequential Diagnoses Prediction

Model

SDP
(Sutter data)

SDP
(MIMIC-III)

HF prediction
(Sutter HF cohort)

GRAM
RNN

525s (39 epochs)
352s (24 epochs)

2s (11 epochs)
1s (6 epochs)

12s (7 epochs)
8s (5 epochs)

Tables 2a and 2b show the sequential diagnoses prediction performance on Sutter data and MIMIC-
III. Both ﬁgures show that GRAM+ outperforms other models when predicting labels with signiﬁcant data
insuﬃciency (i.e. less observed in the training data).The performance gain is greater for MIMIC-III, where
GRAM+ outperforms the basic RNN by 10% in the 20th-40th percentile range. This seems to come from the fact
that MIMIC patients on average have signiﬁcantly shorter visit history than Sutter patients, with much more
codes received per visit. Such short sequences make it diﬃcult for the RNN to learn and predict diagnoses
sequence. The performance diﬀerence between GRAM+ and GRAM suggests that our proposed initialization
scheme of the basic embeddings ei is important for sequential diagnosis prediction.

Table 2c shows the HF prediction performance on Sutter HF cohort. GRAM and GRAM+ consistently
outperforms other baselines (except RNN+) by 3∼4% AUC, and RNN+ by maximum 1.8% AUC. These
diﬀerences are quite signiﬁcant given that the AUC is already in the mid-80s, a high value for HF prediction,
cf. (Choi et al., 2016e). Note that, for GRAM+ and RNN+, we used the downsampled training data to
initialize the basic embeddings ei’s and the embedding matrix Wemb with GloVe, respectively. The result
shows that the initialization scheme of the basic embeddings in GRAM+ gives limited improvement over GRAM.
This stems from the diﬀerent natures of the two prediction tasks. While the goal of HF prediction is to
predict a binary label for the entire visit sequence, the goal of sequential diagnosis prediction is to predict the
co-occurring diagnosis codes at every visit. Therefore the co-occurrence information infused by the initialized
embedding scheme is more beneﬁcial to sequential diagnosis prediction. Additionally, this beneﬁt is associated
with the natures of the two prediction tasks than the datasets used for the prediction tasks. Because the
initialized embedding shows diﬀerent degrees of improvement as shown by Tables 2a and 2c, when Sutter
HF cohort is a subset of Sutter PAMF, thus having similar characteristics. Overall, GRAM showed superior
predictive performance under data insuﬃciency in three diﬀerent experiments, demonstrating its general
applicability in clinical predictive modeling. Now we brieﬂy discuss the scalability of GRAM by comparing its
training time to RNN’s. Table 3 shows the number of seconds taken for the two models to train for a single
epoch for each predictive modeling task. GRAM+ and RNN+ showed the similar behavior as GRAM and RNN.
GRAM takes approximately 50% more time to train for a single epoch for all prediction tasks. This stems from
calculating attention weights and the ﬁnal representations gi for all medical codes. GRAM also generally takes
about 50% more epochs to reach to the model with the lowest validation loss. This is due to optimizing an
extra MLP model that generates the attention weights. Overall, use of GRAM adds a manageable amount of
overhead in training time to the plain RNN.

3.3 Qualitative evaluation of interpretable representations

To qualitatively assess the interpretability of the learned representations of the medical codes, we plot on
a 2-D space using t-SNE (Maaten and Hinton, 2008) the ﬁnal representations gi of 2,000 randomly chosen
diseases learned by GRAM+ for sequential diagnoses prediction on Sutter data4 (Figure 3a). The color of
the dots represents the highest disease categories and the text annotations represent the detailed disease
categories in CCS multi-level hierarchy. For comparison, we also show the t-SNE plots on the strongest
results from GRAM (Figure 3b), RNN+ (Figure 3c), RNN (Figure 3d) and RandomDAG (Figure 3e). GloVe
(Figure 3f) and Skip-gram (Figure 3g) were trained on the Sutter data, where a single visit Vt was used as
the context window to calculate the co-occurrence of codes.

Figures 3c and 3f conﬁrm that interpretable representations cannot simply be learned only by co-occurrence

or supervised prediction without medical knowledge. GRAM+ and GRAM learn interpretable disease

4The scatterplots of models trained for sequential diagnoses prediction on MIMIC-III and HF prediction for Sutter HF cohort

were similar but less structured due to smaller data size.

9

(a) Scatterplot of the ﬁnal representations gi’s of GRAM+

(b) Scatterplot of the ﬁnal representations gi’s of GRAM

(c) Scatterplot of the trained embedding matrix Wemb
of RNN+

(d) Scatterplot of the trained embedding matrix
Wemb of RNN

(e) Scatterplot of the ﬁnal representations gi’s of
RandomDAG

(f) Scatterplot of the disease representations trained
by GloVe

(g) Scatterplot of the basic embeddings ei’s trained
by Skip-gram

Figure 3: t-SNE scatterplots of medical concepts trained by GRAM+, GRAM, RNN+, RNN, RandomDAG,
GloVe and Skip-gram. The color of the dots represents the highest disease categories and the text annotations
represent the detailed disease categories in CCS multi-level hierarchy. It is clear that GRAM+ and GRAM exhibit
interpretable embedding that are well aligned with the medical ontology.

10

representations that are signiﬁcantly more consistent with the given knowledge DAG G. Based on the
prediction performance shown by Table 2, and the fact that the representations gi’s are the ﬁnal product
of GRAM, we can infer that such medically meaningful representations are necessary for predictive models to
cope with data insuﬃciency and make more accurate predictions. Figure 3b shows that the quality of the
ﬁnal representations gi of GRAM is quite similar to GRAM+. Compared to other baselines, GRAM demonstrates
signiﬁcantly more structured representations that align well with the given knowledge DAG. It is interesting
that Skip-gram shows the most structured representation among all baselines. We used GloVe to initialize
the basic embeddings ei in this work because it uses global co-occurrence information and its training time
is fast as it is only dependent only on the total number of unique concepts |C|. Skip-gram’s training time,
on the other hand, depends on both the number of patients and the number of visits each patient made,
which makes the algorithm generally slower than GloVe. An interactive visualization tool can be accessed at
http://www.sunlab.org/research/gram-graph-based-attention-model/.

3.4 Analysis of the attention behavior

Figure 4: GRAM’s attention behavior during HF prediction for four representative diseases (each column). In
each ﬁgure, the leaf node represents the disease and upper nodes are its ancestors. The size of the node shows
the amount of attention it receives, which is also shown by the bar charts. The number in the parenthesis
next to the disease is its frequency in the training data. We exclude the root of the knowledge DAG G from
all ﬁgures as it did not play a signiﬁcant role.

Next we show that GRAM’s attention can be explained intuitively based on the data availability and
knowledge DAG’s structure when performing a prediction task. Using Eq. (1), we can calculate the attention
weights of individual disease. Figure 4 shows the attention behaviors of four representative diseases when
performing HF prediction on Sutter HF cohort.

Other pneumothorax (ICD9 512.89) in Figure 4a is rarely observed in the data and has only ﬁve siblings.
In this case, most information is derived from the highest ancestor. Temporomandibular joint disorders &
articular disc disorder (ICD9 524.63) in Figure 4b is rarely observed but has 139 siblings. In this case, its
parent receives a stronger attention because it aggregates suﬃcient samples from all of its children to learn a
more accurate representation. Note that the disease itself also receives a stronger attention to facilitate easier
distinction from its large number of siblings.

Unspeciﬁed essential hypertension (ICD9 401.9) in Figure 4c is very frequently observed but has only two
siblings. In this case, GRAM assigns a very strong attention to the leaf, which is logical because the more you
observe a disease, the stronger your conﬁdence becomes. Need for prophylactic vaccination and inoculation
against inﬂuenza (ICD9 V04.81) in Figure 4d is quite frequently observed and also has 103 siblings. The
attention behavior in this case is quite similar to the case with fewer siblings (Figure 4b) with a slight
attention shift towards the leaf concept as more observations lead to higher conﬁdence.

4 Related Work

The attention mechanism is a general framework for neural network learning (Bahdanau et al., 2014), and
has been since used in many areas such as speech recognition (Chorowski et al., 2014), computer vision (Ba

11

et al., 2014; Xu et al., 2015) and healthcare (Choi et al., 2016b). However, no one has designed attention
model based on knowledge ontology, which is the focus of this work.

There are related works in learning the representations of graphs. Several studies focused on learning the
representations of graph vertices by using the neighbor information. DeepWalk (Perozzi et al., 2014) and
node2vec (Grover and Leskovec, 2016) use random walk while LINE (Tang et al., 2015) uses breadth-ﬁrst
search to ﬁnd the neighbors of a vertex and learn its representation based on the neighbor information.
Graph convolutional approaches (Yang et al., 2016; Kipf and Welling, 2016) also focus on learning the vertex
representations to mainly perform vertex classiﬁcation. All those works focus on solving the graph data
problems whereas GRAM focuses on solving clinical predictive modeling problems using the knowledge DAG as
supplementary information.

Several researchers tried to model the knowledge DAG such as WordNet (Miller, 1995) or Freebase
(Bollacker et al., 2008) where two entities are connected with various types of relation, forming a set of triples.
They aim to project entities and relations (Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014; Lin
et al., 2015) to the latent space based on the triples or additional information such as hierarchy of entities
(Xie et al., 2016). These works demonstrated tasks such as link prediction, triple classiﬁcation or entity
classiﬁcation using the learned representations. More recently, Li et al. (2016) learned the representations of
words and Wikipedia categories by utilizing the hierarchy of Wikipedia categories. GRAM is fundamentally
diﬀerent from the above studies in that it aims to design intuitive attention mechanism on the knowledge
DAG as a knowledge prior to cope with data insuﬃciency and learn medically interpretable representations
to make accurate predictions.

A classical approach for incorporating side information in the predictive models is to use graph Laplacian
regularization (Weinberger et al., 2006; Che et al., 2015). However, using this approach is not straightforward
as it relies on the appropriate deﬁnition of distance on graphs which is often unavailable.

5 Conclusion

Data insuﬃciency, either due to less common diseases or small datasets, is one of the key hurdles in healthcare
analytics, especially when we apply deep neural networks models. To overcome this challenge, we leverage the
knowledge DAG, which provides a multi-resolution view of medical concepts. We propose GRAM, a graph-based
attention model using both a knowledge DAG and EHR to learn an accurate and interpretable representations
for medical concepts. GRAM chooses a weighted average of ancestors of a medical concept and train the entire
process with a predictive model in an end-to-end fashion. We conducted three predictive modeling experiments
on real EHR datasets and showed signiﬁcant improvement in the prediction performance, especially on
low-frequency diseases and small datasets. Analysis of the attention behavior provided intuitive insight of
GRAM.

References

arXiv:1412.7755 (2014).

Jimmy Ba, Volodymyr Mnih, and Koray Kavukcuoglu. 2014. Multiple object recognition with visual attention.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural Machine Translation by Jointly

Learning to Align and Translate. arXiv:1409.0473 (2014).

Yoshua Bengio, Patrice Simard, and Paolo Frasconi. 1994. Learning long-term dependencies with gradient

descent is diﬃcult. IEEE Transactions on Neural Networks 5, 2 (1994).

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively

created graph database for structuring human knowledge. In SIGMOD.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013.

Translating embeddings for modeling multi-relational data. In NIPS.

12

Zhengping Che, David Kale, Wenzhe Li, Mohammad Taha Bahadori, and Yan Liu. 2015. Deep Computational

Phenotyping. In SIGKDD.

Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. 2016. Recurrent Neural

Networks for Multivariate Time Series with Missing Values. arXiv:1606.01865 (2016).

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for
statistical machine translation. In EMNLP.

Edward Choi, Mohammad Taha Bahadori, Andy Schuetz, Walter F. Stewart, and Jimeng Sun. 2016a. Doctor

AI: Predicting Clinical Events via Recurrent Neural Networks. In MLHC.

Edward Choi, Mohammad Taha Bahadori, Andy Schuetz, Walter F. Stewart, and Jimeng Sun. 2016b.
RETAIN: Interpretable Predictive Model in Healthcare using Reverse Time Attention Mechanism. In
NIPS.

Edward Choi, Mohammad Taha Bahadori, Elizabeth Searles, Catherine Coﬀey, Michael Thompson, James
Bost, Javier T Sojo, and Jimeng Sun. 2016c. Multi-layer Representation Learning for Medical Concepts. In
SIGKDD.

Edward Choi, Andy Schuetz, Walter F. Stewart, and Jimeng Sun. 2016e. Using Recurrent Neural Network
Models for Early Detection of Heart Failure Onset. Journal of the American Medical Informatics Association
(2016), ocw112.

Youngduck Choi, Chill Yi-I Chiu, and David Sontag. 2016d. Learning Low-Dimensional Representations of

Medical Concepts. (2016). AMIA CRI.

Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. End-to-end continuous

speech recognition using attention-based recurrent NN: First results. arXiv:1412.1602 (2014).

Ary Goldberger and others. 2000. Physiobank, physiotoolkit, and physionet components of a new research

resource for complex physiologic signals. Circulation (2000).

Aditya Grover and Jure Leskovec. 2016. Node2Vec: Scalable Feature Learning for Networks. In SIGKDD.

Jerry Gurwitz, David Magid, David Smith, Robert Goldberg, David McManus, Larry Allen, Jane Saczynski,
Micah Thorp, Grace Hsu, Sue Hee Sung, and others. 2013. Contemporary prevalence and correlates of
incident heart failure with preserved ejection fraction. The American journal of medicine 126, 5 (2013).

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural Computation 9, 8 (1997).

Alistair Johnson and others. 2016. MIMIC-III, a freely accessible critical care database. Scientiﬁc Data 3

Thomas N Kipf and Max Welling. 2016. Semi-supervised classiﬁcation with graph convolutional networks.

(2016).

arXiv:1609.02907 (2016).

Quoc V Le, Navdeep Jaitly, and Geoﬀrey E Hinton. 2015. A Simple Way to Initialize Recurrent Networks of

Rectiﬁed Linear Units. arXiv:1504.00941 (2015).

Yuezhang Li, Ronghuo Zheng, Tian Tian, Zhiting Hu, Rahul Iyer, and Katia Sycara. 2016. Joint Embedding
of Hierarchical Categories and Entities for Concept Categorization and Dataless Classiﬁcation. (2016).

Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning Entity and Relation

Embeddings for Knowledge Graph Completion. In AAAI.

13

Zachary C Lipton, David C Kale, Charles Elkan, and Randall Wetzell. 2015. Learning to Diagnose with

LSTM Recurrent Neural Networks. arXiv:1511.03677 (2015).

Zachary C Lipton, David C Kale, and Randall Wetzel. 2016. Modeling Missing Data in Clinical Time Series

with RNNs. In MLHC.

Laurens van der Maaten and Geoﬀrey Hinton. 2008. Visualizing data using t-SNE. JMLR 9, Nov (2008).

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeﬀ Dean. 2013. Distributed representations of

words and phrases and their compositionality. In NIPS.

George A Miller. 1995. WordNet: a lexical database for English. Commun. ACM 38, 11 (1995).

Riccardo Miotto, Li Li, Brian A Kidd, and Joel T Dudley. 2016. Deep Patient: An Unsupervised Representation

to Predict the Future of Patients from the Electronic Health Records. Scientiﬁc Reports 6 (2016).

Phuoc Nguyen, Truyen Tran, Nilmini Wickramasinghe, and Svetha Venkatesh. 2016. Deepr: A Convolutional

Net for Medical Records. arXiv:1607.07519 (2016).

Jeﬀrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global Vectors for Word

Representation. In EMNLP.

In SIGKDD.

Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online Learning of Social Representations.

Healthcare Cost & Utilization Project and others. 2010. Clinical classiﬁcations software (CCS) for ICD-9-CM.

Rockville, MD: Agency for Healthcare Research and Quality (2010).

Narges Razavian, Jake Marcus, and David Sontag. 2016. Multi-task Prediction of Disease Onsets from

Longitudinal Lab Tests. In MLHC.

Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. 2013. Reasoning with neural tensor

networks for knowledge base completion. In NIPS.

Michael Q Stearns, Colin Price, Kent A Spackman, and Amy Y Wang. 2001. SNOMED clinical terms:

overview of the development process and project status. In AMIA.

Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. LINE: Large-scale

Information Network Embedding. In WWW.

The Theano Development Team. 2016. Theano: A Python framework for fast computation of mathematical

expressions. arXiv:1605.02688 (2016).

Rajakrishnan Vijayakrishnan, Steven Steinhubl, Kenney Ng, Jimeng Sun, Roy Byrd, Zahra Daar, Brent
Williams, Shahram Ebadollahi, Walter Stewart, and others. 2014. Prevalence of heart failure signs and
symptoms in a large primary care population identiﬁed through the use of text and data mining of the
electronic health record. Journal of cardiac failure 20, 7 (2014).

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge Graph Embedding by Translating

on Hyperplanes. In AAAI.

Kilian Q Weinberger, Fei Sha, Qihui Zhu, and Lawrence K Saul. 2006. Graph Laplacian Regularization for

Large-Scale Semideﬁnite Programming. In NIPS.

Ruobing Xie, Zhiyuan Liu, and Maosong Sun. 2016. Representation Learning of Knowledge Graphs with

Hierarchical Types. In IJCAI.

14

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel,
and Yoshua Bengio. 2015. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention..
In ICML.

Zhilin Yang, William Cohen, and Ruslan Salakhutdinov. 2016. Revisiting Semi-Supervised Learning with

Graph Embeddings. arXiv:1603.08861 (2016).

Matthew D Zeiler. 2012. ADADELTA: an adaptive learning rate method. arXiv:1212.5701 (2012).

15

7
1
0
2
 
r
p
A
 
1
 
 
]

G
L
.
s
c
[
 
 
3
v
2
1
0
7
0
.
1
1
6
1
:
v
i
X
r
a

GRAM: Graph-based Attention Model for Healthcare
Representation Learning

Edward Choi∗, Mohammad Taha Bahadori∗, Le Song∗, Walter F. Stewart†, Jimeng Sun∗

∗ Georgia Institute of Technology

† Sutter Health

{mp2893,bahadori}@gatech.edu, lsong@cc.gatech.edu, stewarwf@sutterhealth.org, jsun@cc.gatech.edu

Abstract

Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two

important challenges remain:

• Data insuﬃciency: Often in healthcare predictive modeling, the sample size is insuﬃcient for deep

learning methods to achieve satisfactory results.

• Interpretation: The representations learned by deep learning methods should align with medical

knowledge.

To address these challenges, we propose a GRaph-based Attention Model, GRAM that supplements electronic
health records (EHR) with hierarchical information inherent to medical ontologies. Based on the data
volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in
the ontology via an attention mechanism.

We compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various
methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and
one heart failure prediction task. Compared to the basic RNN, GRAM achieved 10% higher accuracy for
predicting diseases rarely observed in the training data and 3% improved area under the ROC curve
for predicting heart failure using an order of magnitude less training data. Additionally, unlike other
methods, the medical concept representations learned by GRAM are well aligned with the medical ontology.
Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts
when facing data insuﬃciency at the lower level concepts.

1

Introduction

The rapid growth in volume and diversity of health care data from electronic health records (EHR) and other
sources is motivating the use of predictive modeling to improve care for individual patients. In particular,
novel applications are emerging that use deep learning methods such as word embedding (Choi et al., 2016c,d),
recurrent neural networks (RNN) (Che et al., 2016; Choi et al., 2016a,b; Lipton et al., 2016), convolutional
neural networks (CNN) (Nguyen et al., 2016) or stacked denoising autoencoders (SDA) (Che et al., 2015;
Miotto et al., 2016), demonstrating signiﬁcant performance enhancement for diverse prediction tasks. Deep
learning models appear to perform signiﬁcantly better than logistic regression or multilayer perceptron (MLP)
models that depend, to some degree, on expert feature construction (Lipton et al., 2015; Razavian et al.,
2016).

Training deep learning models typically requires large amounts of data that often cannot be met by a single
health system or provider organization. Sub-optimal model performance can be particularly challenging when
the focus of interest is predicting onset of a rare disease. For example, using Doctor AI (Choi et al., 2016a), we
discovered that RNN alone was ineﬀective to predict the onset of diseases such as cerebral degenerations (e.g.
Leukodystrophy, Cerebral lipidoses) or developmental disorders (e.g. autistic disorder, Heller’s syndrome),
partly because their rare occurrence in the training data provided little learning opportunity to the ﬂexible
models like RNN.

1

Figure 1: The illustration of GRAM. Leaf nodes (solid circles) represents a medical concept in the EHR, while
the non-leaf nodes (dotted circles) represent more general concepts. The ﬁnal representation gi of the leaf
concept ci is computed by combining the basic embeddings ei of ci and eg, ec and ea of its ancestors cg, cc
and ca via an attention mechanism. The ﬁnal representations form the embedding matrix G for all leaf
concepts. After that, we use G to embed patient visit vector xt to a visit representation vt, which is then fed
to a neural network model to make the ﬁnal prediction ˆyt.

The data requirement of deep learning models comes from having to assess exponential number of
combinations of input features. This can be alleviated by exploiting medical ontologies that encodes
hierarchical clinical constructs and relationships among medical concepts. Fortunately, there are many
well-organized ontologies in healthcare such as the International Classiﬁcation of Diseases (ICD), Clinical
Classiﬁcations Software (CCS) (Stearns et al., 2001) or Systematized Nomenclature of Medicine-Clinical
Terms (SNOMED-CT) (Project et al., 2010). Nodes (i.e. medical concepts) close to one another in medical
ontologies are likely to be associated with similar patients, allowing us to transfer knowledge among them.
Therefore, proper use of medical ontologies will be helpful when we lack enough data for the nodes in the
ontology to train deep learning models.

In this work, we propose GRAM, a method that infuses information from medical ontologies into deep
learning models via neural attention. Considering the frequency of a medical concept in the EHR data and
its ancestors in the ontology, GRAM decides the representation of the medical concept by adaptively combining
its ancestors via attention mechanism. This will not only support deep learning models to learn robust
representations without large amount of data, but also learn interpretable representations that align well
with the knowledge from the ontology. The attention mechanism is trained in an end-to-end fashion with
the neural network model that predicts the onset of disease(s). We also propose an eﬀective initialization
technique in addition to the ontological knowledge to better guide the representation learning process.

We compare predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various models
including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart
failure (HF) prediction task. We demonstrate that GRAM is up to 10% more accurate than the basic RNN
for predicting diseases less observed in the training data. After discussing GRAM’s scalability, we visualize
the representations learned from various models, where GRAM provides more intuitive representations by
grouping similar medical concepts close to one another. Finally, we show GRAM’s attention mechanism can
be interpreted to understand how it assigns the right amount of attention to the ancestors of each medical
concept by considering the data availability and the ontology structure.

2 Methodology

We ﬁrst deﬁne the notations describing EHR data and medical ontologies, followed by a description of GRAM
(Section 2.2), the end-to-end training of the attention generation and predictive modeling (Section 2.3), and
the eﬃcient initialization scheme (Section 2.4).

2

2.1 Basic Notation

We denote the set of entire medical codes from the EHR as c1, c2, . . . , c|C| ∈ C with the vocabulary size |C|.
The clinical record of each patient can be viewed as a sequence of visits V1, . . . , VT where each visit contains a
subset of medical codes Vt ⊆ C. Vt can be represented as a binary vector xt ∈ {0, 1}|C| where the i-th element
is 1 only if Vt contains the code ci. To avoid clutter, all algorithms will be presented for a single patient.

We assume that a given medical ontology G typically expresses the hierarchy of various medical concepts in
the form of a parent-child relationship, where the medical codes C form the leaf nodes. Ontology G is represented
as a directed acyclic graph (DAG) whose nodes form a set D = C +C(cid:48). The set C(cid:48) = {c|C|+1, c|C|+2, . . . , c|C|+|C(cid:48)|}
consists of all non-leaf nodes (i.e. ancestors of the leaf nodes), where |C(cid:48)| represents the number of all non-leaf
nodes. We use knowledge DAG to refer to G. A parent in the knowledge DAG G represents a related but
more general concept over its children. Therefore, G provides a multi-resolution view of medical concepts with
diﬀerent degrees of speciﬁcity. While some ontologies are exclusively expressed as parent-child hierarchies
(e.g. ICD-9, CCS), others are not. For example, in some instances SNOMED-CT also links medical concepts
to causal or treatment relationships, but the majority relationships in SNOMED-CT are still parent-child.
Therefore, we focus on the parent-child relationships in this work.

2.2 Knowledge DAG and the Attention Mechanism

GRAM leverages the parent-child relationship of G to learn robust representations when data volume is
constrained. GRAM balances the use of ontology information in relation to data volume in determining the
level of speciﬁcity for a medical concept. When a medical concept is less observed in the data, more weight is
given to its ancestors as they can be learned more accurately and oﬀer general (coarse-grained) information
about their children. The process of resorting to the parent concepts can be automated via the attention
mechanism and the end-to-end training as described in Figure 1.

In the knowledge DAG, each node ci is assigned a basic embedding vector ei ∈ Rm, where m represents the
dimensionality. Then e1, . . . , e|C| are the basic embeddings of the codes c1, . . . , c|C| while e|C|+1, . . . , e|C|+|C(cid:48)|
represent the basic embeddings of the internal nodes c|C|+1, . . . , c|C|+|C(cid:48)|. The initialization of these basic
embeddings is described in Section 2.4. We formulate a leaf node’s ﬁnal representation as a convex combination
of the basic embeddings of itself and its ancestors:

(cid:88)

gi =

αijej,

(cid:88)

j∈A(i)

j∈A(i)

αij = 1, αij ≥ 0 for j ∈ A(i),

where gi ∈ Rm denotes the ﬁnal representation of the code ci, A(i) the indices of the code ci and ci’s
ancestors, ej the basic embedding of the code cj and αij ∈ R the attention weight on the embedding ej when
calculating gi. The attention weight αij in Eq. (1) is calculated by the following Softmax function,

f (ei, ej) is a scalar value representing the compatibility between the basic embeddings of ei and ek. We
compute f (ei, ej) via the following feed-forward network with a single hidden layer (MLP),

αij =

(cid:80)

exp(f (ei, ej))
k∈A(i) exp(f (ei, ek))

f (ei, ej) = u(cid:62)

a tanh(Wa

(cid:21)

(cid:20) ei
ej

+ ba)

where Wa ∈ Rl×2m is the weight matrix for the concatenation of ei and ej, b ∈ Rl the bias vector, and
ua ∈ Rl the weight vector for generating the scalar value. The constant l represents the dimension size
of the hidden layer of f (·, ·). We always concatenate ei and ej in the child-ancestor order. Note that the
compatibility function f is an MLP, because MLP is well known to be a suﬃcient approximator for an
arbitrary function, and we empirically found that our formulation performed better in our use cases than
alternatives such as inner product and Bahdanau et al.’s (Bahdanau et al., 2014).

3

(1)

(2)

(3)

Remarks: The example in Figure 1 is derived based on a single path from ci to ca. However, the same
mechanism can be applicable to multiple paths as well. For example, code ck has two paths to the root ca,
containing ﬁve ancestors in total. Another scenario is where the EHR data contain both leaf codes and some
ancestor codes. We can move those ancestors present in EHR data from the set C(cid:48) to C and apply the same
process as Eq. (1) to obtain the ﬁnal representations for them.

2.3 End-to-End Training with a Predictive Model

We train the attention mechanism together with a predictive model such that the attention mechanism
improves the predictive performance. By concatenating ﬁnal representation g1, g2, . . . , g|C| of all medical
codes, we have the embedding matrix G ∈ Rm×|C| where gi is its i-th column of G. We can then convert
visit Vt to a visit representation vt by multiplying embedding matrix G with multi-hot vector xt indicating
the clinical events in visit Vt as shown in the right side of Figure 1. Finally the visit representation vt will
be used as an input to pass to a predictive model for predicting the target label yt using a neural network
(NN) model. In this work, we use RNN as the choice of the NN model as the task is to perform sequential
diagnoses prediction (Choi et al., 2016a,b) with the objective of predicting the disease codes of the next visit
Vt+1 given the visit records up to the current timestep V1, V2, . . . , Vt, which can be expressed as follows,

(cid:98)yt = (cid:98)xt+1 = Softmax(Wht + b), where
h1, h2, . . . , ht = RNN(v1, v2, . . . , vt; θr), where
v1, v2, . . . , vt = tanh(G[x1, x2, . . . , xt])

(4)

(5)

where xt ∈ R|C| denotes the t-th visit; vt ∈ Rm the t-th visit representation; ht ∈ Rr the RNN’s hidden layer
at t-th time step (i.e. t-th visit); θr RNN’s parameters; W ∈ R|C|×r and b ∈ R|C| the weight matrices and
the bias vector of the Softmax function; r denotes the dimension size of the hidden layer. We use “RNN” to
denote any recurrent neural network variants that can cope with the vanishing gradient problem (Bengio
et al., 1994), such as LSTM (Hochreiter and Schmidhuber, 1997), GRU (Cho et al., 2014), and IRNN (Le
et al., 2015), with any varying numbers of hidden layers. The prediction loss for all time steps is calculated
using the binary cross entropy as follows,

L(x1, x2 . . . , xT ) = −

1
T − 1

T −1
(cid:88)

(cid:16)

t=1

(cid:17)
(cid:62) log((cid:98)yt) + (1 − yt)(cid:62) log(1 − (cid:98)yt)

yt

where we sum the cross entropy errors from all timestamps of (cid:98)yt, T denotes the number of timestamps of
the visit sequence. Note that the above loss is deﬁned for a single patient. In actual implementation, we
will take the average of the individual loss for multiple patients. Algorithm 1 describes the overall training
procedure of GRAM, under the assumption that we are performing the sequential diagnoses prediction task
using an RNN. Note that Algorithm 1 describes stochastic gradient update to avoid clutter, but it can be
easily extended to other gradient based optimization such as mini-batch gradient update.

2.4

Initializing Basic Embeddings

The attention generation mechanism in Section 2.2 requires basic embeddings ei of each node in the knowledge
DAG. The basic embeddings of ancestors, however, pose a diﬃculty because they are often not observed in
the data. To properly initialize them, we use co-occurrence information to learn the basic embeddings of
medical codes and their ancestors. Co-occurrence has proven to be an important source of information when
learning representations of words or medical concepts (Mikolov et al., 2013; Choi et al., 2016c,d). To train
the basic embeddings, we employ GloVe (Pennington et al., 2014), which uses the global co-occurrence matrix
of words to learn their representations. In our case, the co-occurrence matrix of the codes and the ancestors
was generated by counting the co-occurrences within each visit Vt, where we augment each visit with the
ancestors of the codes in the visit. We describe the details of the initialization algorithm with an example.
We borrow the parent-child relationships from the knowledge DAG of Figure 1. Given a visit Vt,

4

Algorithm 1 GRAM Optimization

Randomly initialize basic embedding matrix E, attention parameters ua, Wa, ba, RNN parameter θr,
softmax parameters W, b.
repeat

Update E with GloVe objective function (see Section 2.4)

until convergence
repeat

X ← random patient from dataset
for visit Vt in X do

for code ci in Vt do

Refer G to ﬁnd ci’s ancestors C (cid:48)
for code cj in C (cid:48) do

Calculate attention weight αij using Eq. (2).

end for
Obtain ﬁnal representation gi using Eq. (1).

end for
vt ← tanh((cid:80)
i:ci∈Vt
Make prediction (cid:98)yt using Eq. (4)

gi)

end for
Calculate prediction loss L using Eq .(5)
Update parameters according to the gradient of L

until convergence

we augment it with the ancestors of all the codes to obtain the augmented visit V (cid:48)
t ,

V (cid:48)
t = {cd, cb, ca, ci, cg, cc, ca, ck, cj, cf , cc, cb, ca}

Vt = {cd, ci, ck}

where the augmented ancestors are underlined. Note that a single ancestor can appear multiple times in V (cid:48)
t .
In fact, the higher the ancestor is in the knowledge DAG, the more times it is likely to appear in V (cid:48)
t . We
count the co-occurrence of two codes in V (cid:48)

t as follows,

co-occurrence(ci, cj, V (cid:48)

t ) = count(ci, V (cid:48)

t ) × count(cj, V (cid:48)
t )

where count(ci, V (cid:48)
t ) is the number of times the code ci appears in the augmented visit V (cid:48)
t . For example, the
co-occurrence between the leaf code ci and the root ca is 3. However, the co-occurrence between the ancestor
cc and the root ca is 6. Therefore our algorithm will make the higher ancestor codes, which are more general
concepts, have more involvement in all medical events (i.e. visits), which is natural in healthcare application
as those general concepts are often reliable. We repeat this calculation for all pairs of codes in all augmented
visits of all patients to obtain the co-occurrence matrix M ∈ R|D|×|D| depicted by Figure 2. For training the
embedding vectors ei’s using M, we minimize the following loss function as described in Pennington et al.
(2014).

|D|
(cid:88)

i,j=1

(cid:40)

J =

f (Mij)(e(cid:62)

i ej + bi + bj − log Mij)2

where f (x) =

(x < xmax)α
1

if x < xmax
otherwise

where the hyperparameters xmax and α are respectively set to 100 and 0.75 as the original paper (Pennington
et al., 2014). Note that, after the initialization, the basic embeddings ei’s of both leaf nodes (i.e. medical
codes) and non-leaf nodes (i.e. ancestors) are ﬁne-tuned during model training via backpropagation.

5

Figure 2: Creating the co-occurrence matrix together with the ancestors. The n-th ancestors are the group of
nodes that are n hops away from any leaf node in G. Here we exclude the root node, which will be just a
single row (column).

Table 1: Basic statistics of Sutter PAMF, MIMIC-III and Sutter heart failure (HF) cohort.

Dataset

Sutter PAMF MIMIC-III

# of patients
# of visits
Avg. # of visits per patient
# of unique ICD9 codes
Avg. # of codes per visit
Max # of codes per visit

258,555†
13,920,759
53.8
10,437
1.98
54

7,499†
19,911
2.66
4,893
13.1
39

Sutter HF cohort
30,727† (3,408 cases)
572,551
38.38
5,689
2.06
29

† For all datasets, we chose patients who made at least two visits.

3 Experiments

We conduct three experiments to determine if GRAM oﬀered superior prediction performance when facing
data insuﬃciency. We ﬁrst describe the experimental setup followed by results comparing predictive
performance of GRAM with various baseline models. After discussing GRAM’s scalability, we qualitatively
evaluate the interpretability of the resulting representation. The source code of GRAM is publicly available at
https://github.com/mp2893/gram.

3.1 Experiment Setup

Prediction tasks and source of data: We conduct the sequential diagnoses prediction (SDP) tasks on
two datasets, which aim at predicting all diagnosis categories in the next visit, and a heart failure (HF)
prediction task on one dataset, which is a binary prediction task for predicting a future HF onset where the
prediction is made only once at the last visit xT .
Two sequential diagnoses predictions (SDP) are respectively conducted using two datasets: 1) Sutter Palo Alto
Medical Foundation (PAMF) dataset, which consists of 18-years longitudinal medical records of 258K patients
between age 50 and 90. This will determine GRAM’s performance for general adult population with long visit
records. 2) MIMIC-III dataset (Johnson et al., 2016; Goldberger et al., 2000), which is a publicly available
dataset consisting of medical records of 7.5K intensive care unit (ICU) patients over 11 years. This will
determine GRAM’s performance for high-risk patients with very short visit records. We utilize all the patients
with at least 2 visits. We prepared the true labels yt by grouping the ICD9 codes into 283 groups using CCS
single-level diagnosis grouper1. This is to improve the training speed and predictive performance for easier
analysis, while preserving suﬃcient granularity for each diagnosis. Each diagnosis code’s varying frequency in
the training data can be viewed as diﬀerent degrees of data insuﬃciency. We calculate Accuracy@k for each

1https://www.hcup-us.ahrq.gov/toolssoftware/ccs/AppendixASingleDX.txt

6

of CCS single-level diagnosis codes such that, given a visit Vt, we get 1 if the target diagnosis is in the top k
guesses and 0 otherwise.
We conduct HF prediction on Sutter heart failure (HF) cohort, which is a subset of Sutter PAMF data for a
heart failure onset prediction study with 3.4K HF cases chosen by a set of criteria described in Vijayakrishnan
et al. (2014); Gurwitz et al. (2013) and 27K matching controls chosen by a set of criteria described in Choi
et al. (2016e). This will determine GRAM’s performance for a diﬀerent prediction task where we predict the
onset of one speciﬁc condition. We randomly downsample the training data to create diﬀerent degrees of
data insuﬃciency. We use area under the ROC curve (AUC) to measure the performance.
A summary of the datasets are provided in Table 1.We used CCS multi-level diagnoses hierarchy2 as our
knowledge DAG G. We also tested the ICD9 code hierarchy3, but the performance was similar to using CCS
multi-level hierarchy. For all three tasks, we randomly divide the dataset into the training, validation and test
set by .75:.10:.15 ratio, and use the validation set to tune the hyper-parameters. Further details regarding
the hyper-parameter tuning are provided below. The test set performance is reported in the paper.
Implementation details: We implemented GRAM with Theano 0.8.2 (Team, 2016). For training models,
we used Adadelta (Zeiler, 2012) with a mini-batch of 100 patients, on a machine equipped with Intel Xeon
E5-2640, 256GB RAM, four Nvidia Titan X’s and CUDA 7.5.
Models for comparison are the following. The ﬁrst two GRAM+ and GRAM are the proposed methods and the
rest are baselines. Hyper-parameter tuning is conﬁgured so that the number of parameters for the baselines
would be comparable to GRAM’s. Further details are provided below.

• GRAM: Input sequence x1, . . . , xT is ﬁrst transformed by the embedding matrix G, then fed to the GRU
with a single hidden layer, which in turn makes the prediction, as described by Eq. (4). The basic
embeddings ei’s are randomly initialized.

• GRAM+: We use the same setup as GRAM, but the basic embeddings ei’s are initialized according to Section

2.4.

• RandomDAG: We use the same setup as GRAM, but each leaf concept has ﬁve randomly assigned

ancestors from the CCS multi-level hierarchy to test the eﬀect of correct domain knowledge.

• RNN: Input xt is transformed by an embedding matrix Wemb ∈ Rk×|C|, then fed to the GRU with a
single hidden layer. The embedding size k is a hyper-parameter. Wemb is randomly initialized and trained
together with the GRU.

• RNN+: We use the RNN model with the same setup as before, but we initialize the embedding matrix
Wemb with GloVe vectors trained only with the co-occurrence of leaf concepts. This is to compare GRAM
with a similar weight initialization technique.

• SimpleRollUp: We use the RNN model with the same setup as before. But for input xt, we replace
all diagnosis codes with their direct parent codes in the CCS multi-level hierarchy, giving us 578, 526 and
517 input codes respectively for Sutter data, MIMIC-III and Sutter HF cohort. This is to compare the
performance of GRAM with a common grouping technique.

• RollUpRare: We use the RNN model with the same setup as before, but we replace any diagnosis
code whose frequency is less than a certain threshold in the dataset with its direct parent. We set the
threshold to 100 for Sutter data and Sutter HF cohort, and 10 for MIMIC-III, giving us 4,408, 935 and
1,538 input codes respectively for Sutter data, MIMIC-III and Sutter HF cohort. This is an intuitive way
of dealing with infrequent medical codes.

Hyper-parameter Tuning: We deﬁne ﬁve hyper-parameters for GRAM:

• dimensionality m of the basic embedding ei: [100, 200, 300, 400, 500]

• dimensionality r of the RNN hidden layer ht from Eq. (4): [100, 200, 300, 400, 500]

2https://www.hcup-us.ahrq.gov/toolssoftware/ccs/AppendixCMultiDX.txt
3http://www.icd9data.com/2015/Volume1/default.htm

7

• dimensionality l of Wa and ba from Eq. (3): [100, 200, 300, 400, 500]

• L2 regularization coeﬃcient for all weights except RNN weights: [0.1, 0.01, 0.001, 0.0001]

• dropout rate for the dropout on the RNN hidden layer: [0.0, 0.2, 0.4, 0.6, 0.8]

We performed 100 iterations of the random search by using the above ranges for each of the three prediction
experiments. In order to fairly compare the model performances, we matched the number of model parameters
to be similar for all baseline methods. To facilitate reproducibility, ﬁnal hyper-parameter settings we
used for all models for each prediction experiments are described at the source code repository, https:
//github.com/mp2893/gram, along with the detailed steps we used to tune the hyper-parameters.

3.2 Prediction performance and scalability

Model

0-20

20-40

40-60

60-80

80-100

0.4238
0.4903
0.0150 0.3242 0.4325
GRAM+
0.4193
0.4895
0.4224
0.2987
0.0042
GRAM
0.4853
0.4059
0.4010
0.2700
RandomDAG 0.0050
0.4212 0.4959
0.4140
0.2742
0.0069
RNN+
0.2691
RNN
0.4951
0.4227
0.4134
0.0080
0.3078 0.4369 0.4330 0.4924
SimpleRollUp 0.0085
0.4956
0.2768
0.0062
RollUpRare

0.4176

0.4226

(a) Accuracy@5 of sequential diagnoses prediction on Sutter data

Model

0-20

20-40

40-60

60-80

80-100

0.0672 0.1787 0.2644 0.2490
GRAM+
0.6267
GRAM
0.1935
0.2296
0.0556
0.6363
0.1346
0.1512
RandomDAG 0.0329
0.4494
0.2080
0.2494
RNN+
0.0454
0.6239
0.6243
0.2371
0.1804
0.0454
RNN
0.2455 0.2667 0.6387
SimpleRollUp 0.0578
0.6277
0.2364
0.1843
0.0454
RollUpRare

0.1016
0.0708
0.0843
0.0731
0.1328
0.0653

(b) Accuracy@20 of sequential diagnoses prediction on MIMIC-III

Model

10%

20%

30%

40%

50%

60%

70%

80%

90%

100%

GRAM+
GRAM
RandomDAG 0.7644
0.7930
RNN+
RNN
0.7811
SimpleRollUp 0.7799
0.7830
RollUpRare

0.7970 0.8223 0.8307 0.8332 0.8389 0.8404 0.8452 0.8456 0.8447 0.8448
0.8447
0.7981 0.8217 0.8340 0.8332 0.8372
0.8226
0.8143
0.7986
0.8335
0.8261
0.8162
0.8314
0.8156
0.8066
0.8258
0.8177
0.8108
0.8291
0.8211
0.8064

0.8440
0.8274
0.8343
0.8258
0.8223
0.8262

0.8430
0.8254
0.8345
0.8297
0.8269
0.8307

0.8431
0.8312
0.8353
0.8278
0.8272
0.8296

0.8377
0.8185
0.8333
0.8207
0.8207
0.8202

0.7882
0.8117
0.7942
0.8022
0.8067

0.8070
0.8215
0.8111
0.8133
0.8119

(c) AUC of HF onset prediction on Sutter HF cohort

Table 2: Performance of three prediction tasks. The x-axis of (a) and (b) represents the labels grouped by the
percentile of their frequencies in the training data in non-decreasing order. 0-20 are the most rare diagnosese
while 80-100 are the most common ones. (b) uses Accuracy@20 because MIMIC-III has a large average
number of codes per visit (see Table 1). For (c), we vary the size of the training data to train the models.

8

Table 3: Scalablity result in per epoch training time in second (the number of epochs needed). SDP stands
for Sequential Diagnoses Prediction

Model

SDP
(Sutter data)

SDP
(MIMIC-III)

HF prediction
(Sutter HF cohort)

GRAM
RNN

525s (39 epochs)
352s (24 epochs)

2s (11 epochs)
1s (6 epochs)

12s (7 epochs)
8s (5 epochs)

Tables 2a and 2b show the sequential diagnoses prediction performance on Sutter data and MIMIC-
III. Both ﬁgures show that GRAM+ outperforms other models when predicting labels with signiﬁcant data
insuﬃciency (i.e. less observed in the training data).The performance gain is greater for MIMIC-III, where
GRAM+ outperforms the basic RNN by 10% in the 20th-40th percentile range. This seems to come from the fact
that MIMIC patients on average have signiﬁcantly shorter visit history than Sutter patients, with much more
codes received per visit. Such short sequences make it diﬃcult for the RNN to learn and predict diagnoses
sequence. The performance diﬀerence between GRAM+ and GRAM suggests that our proposed initialization
scheme of the basic embeddings ei is important for sequential diagnosis prediction.

Table 2c shows the HF prediction performance on Sutter HF cohort. GRAM and GRAM+ consistently
outperforms other baselines (except RNN+) by 3∼4% AUC, and RNN+ by maximum 1.8% AUC. These
diﬀerences are quite signiﬁcant given that the AUC is already in the mid-80s, a high value for HF prediction,
cf. (Choi et al., 2016e). Note that, for GRAM+ and RNN+, we used the downsampled training data to
initialize the basic embeddings ei’s and the embedding matrix Wemb with GloVe, respectively. The result
shows that the initialization scheme of the basic embeddings in GRAM+ gives limited improvement over GRAM.
This stems from the diﬀerent natures of the two prediction tasks. While the goal of HF prediction is to
predict a binary label for the entire visit sequence, the goal of sequential diagnosis prediction is to predict the
co-occurring diagnosis codes at every visit. Therefore the co-occurrence information infused by the initialized
embedding scheme is more beneﬁcial to sequential diagnosis prediction. Additionally, this beneﬁt is associated
with the natures of the two prediction tasks than the datasets used for the prediction tasks. Because the
initialized embedding shows diﬀerent degrees of improvement as shown by Tables 2a and 2c, when Sutter
HF cohort is a subset of Sutter PAMF, thus having similar characteristics. Overall, GRAM showed superior
predictive performance under data insuﬃciency in three diﬀerent experiments, demonstrating its general
applicability in clinical predictive modeling. Now we brieﬂy discuss the scalability of GRAM by comparing its
training time to RNN’s. Table 3 shows the number of seconds taken for the two models to train for a single
epoch for each predictive modeling task. GRAM+ and RNN+ showed the similar behavior as GRAM and RNN.
GRAM takes approximately 50% more time to train for a single epoch for all prediction tasks. This stems from
calculating attention weights and the ﬁnal representations gi for all medical codes. GRAM also generally takes
about 50% more epochs to reach to the model with the lowest validation loss. This is due to optimizing an
extra MLP model that generates the attention weights. Overall, use of GRAM adds a manageable amount of
overhead in training time to the plain RNN.

3.3 Qualitative evaluation of interpretable representations

To qualitatively assess the interpretability of the learned representations of the medical codes, we plot on
a 2-D space using t-SNE (Maaten and Hinton, 2008) the ﬁnal representations gi of 2,000 randomly chosen
diseases learned by GRAM+ for sequential diagnoses prediction on Sutter data4 (Figure 3a). The color of
the dots represents the highest disease categories and the text annotations represent the detailed disease
categories in CCS multi-level hierarchy. For comparison, we also show the t-SNE plots on the strongest
results from GRAM (Figure 3b), RNN+ (Figure 3c), RNN (Figure 3d) and RandomDAG (Figure 3e). GloVe
(Figure 3f) and Skip-gram (Figure 3g) were trained on the Sutter data, where a single visit Vt was used as
the context window to calculate the co-occurrence of codes.

Figures 3c and 3f conﬁrm that interpretable representations cannot simply be learned only by co-occurrence

or supervised prediction without medical knowledge. GRAM+ and GRAM learn interpretable disease

4The scatterplots of models trained for sequential diagnoses prediction on MIMIC-III and HF prediction for Sutter HF cohort

were similar but less structured due to smaller data size.

9

(a) Scatterplot of the ﬁnal representations gi’s of GRAM+

(b) Scatterplot of the ﬁnal representations gi’s of GRAM

(c) Scatterplot of the trained embedding matrix Wemb
of RNN+

(d) Scatterplot of the trained embedding matrix
Wemb of RNN

(e) Scatterplot of the ﬁnal representations gi’s of
RandomDAG

(f) Scatterplot of the disease representations trained
by GloVe

(g) Scatterplot of the basic embeddings ei’s trained
by Skip-gram

Figure 3: t-SNE scatterplots of medical concepts trained by GRAM+, GRAM, RNN+, RNN, RandomDAG,
GloVe and Skip-gram. The color of the dots represents the highest disease categories and the text annotations
represent the detailed disease categories in CCS multi-level hierarchy. It is clear that GRAM+ and GRAM exhibit
interpretable embedding that are well aligned with the medical ontology.

10

representations that are signiﬁcantly more consistent with the given knowledge DAG G. Based on the
prediction performance shown by Table 2, and the fact that the representations gi’s are the ﬁnal product
of GRAM, we can infer that such medically meaningful representations are necessary for predictive models to
cope with data insuﬃciency and make more accurate predictions. Figure 3b shows that the quality of the
ﬁnal representations gi of GRAM is quite similar to GRAM+. Compared to other baselines, GRAM demonstrates
signiﬁcantly more structured representations that align well with the given knowledge DAG. It is interesting
that Skip-gram shows the most structured representation among all baselines. We used GloVe to initialize
the basic embeddings ei in this work because it uses global co-occurrence information and its training time
is fast as it is only dependent only on the total number of unique concepts |C|. Skip-gram’s training time,
on the other hand, depends on both the number of patients and the number of visits each patient made,
which makes the algorithm generally slower than GloVe. An interactive visualization tool can be accessed at
http://www.sunlab.org/research/gram-graph-based-attention-model/.

3.4 Analysis of the attention behavior

Figure 4: GRAM’s attention behavior during HF prediction for four representative diseases (each column). In
each ﬁgure, the leaf node represents the disease and upper nodes are its ancestors. The size of the node shows
the amount of attention it receives, which is also shown by the bar charts. The number in the parenthesis
next to the disease is its frequency in the training data. We exclude the root of the knowledge DAG G from
all ﬁgures as it did not play a signiﬁcant role.

Next we show that GRAM’s attention can be explained intuitively based on the data availability and
knowledge DAG’s structure when performing a prediction task. Using Eq. (1), we can calculate the attention
weights of individual disease. Figure 4 shows the attention behaviors of four representative diseases when
performing HF prediction on Sutter HF cohort.

Other pneumothorax (ICD9 512.89) in Figure 4a is rarely observed in the data and has only ﬁve siblings.
In this case, most information is derived from the highest ancestor. Temporomandibular joint disorders &
articular disc disorder (ICD9 524.63) in Figure 4b is rarely observed but has 139 siblings. In this case, its
parent receives a stronger attention because it aggregates suﬃcient samples from all of its children to learn a
more accurate representation. Note that the disease itself also receives a stronger attention to facilitate easier
distinction from its large number of siblings.

Unspeciﬁed essential hypertension (ICD9 401.9) in Figure 4c is very frequently observed but has only two
siblings. In this case, GRAM assigns a very strong attention to the leaf, which is logical because the more you
observe a disease, the stronger your conﬁdence becomes. Need for prophylactic vaccination and inoculation
against inﬂuenza (ICD9 V04.81) in Figure 4d is quite frequently observed and also has 103 siblings. The
attention behavior in this case is quite similar to the case with fewer siblings (Figure 4b) with a slight
attention shift towards the leaf concept as more observations lead to higher conﬁdence.

4 Related Work

The attention mechanism is a general framework for neural network learning (Bahdanau et al., 2014), and
has been since used in many areas such as speech recognition (Chorowski et al., 2014), computer vision (Ba

11

et al., 2014; Xu et al., 2015) and healthcare (Choi et al., 2016b). However, no one has designed attention
model based on knowledge ontology, which is the focus of this work.

There are related works in learning the representations of graphs. Several studies focused on learning the
representations of graph vertices by using the neighbor information. DeepWalk (Perozzi et al., 2014) and
node2vec (Grover and Leskovec, 2016) use random walk while LINE (Tang et al., 2015) uses breadth-ﬁrst
search to ﬁnd the neighbors of a vertex and learn its representation based on the neighbor information.
Graph convolutional approaches (Yang et al., 2016; Kipf and Welling, 2016) also focus on learning the vertex
representations to mainly perform vertex classiﬁcation. All those works focus on solving the graph data
problems whereas GRAM focuses on solving clinical predictive modeling problems using the knowledge DAG as
supplementary information.

Several researchers tried to model the knowledge DAG such as WordNet (Miller, 1995) or Freebase
(Bollacker et al., 2008) where two entities are connected with various types of relation, forming a set of triples.
They aim to project entities and relations (Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014; Lin
et al., 2015) to the latent space based on the triples or additional information such as hierarchy of entities
(Xie et al., 2016). These works demonstrated tasks such as link prediction, triple classiﬁcation or entity
classiﬁcation using the learned representations. More recently, Li et al. (2016) learned the representations of
words and Wikipedia categories by utilizing the hierarchy of Wikipedia categories. GRAM is fundamentally
diﬀerent from the above studies in that it aims to design intuitive attention mechanism on the knowledge
DAG as a knowledge prior to cope with data insuﬃciency and learn medically interpretable representations
to make accurate predictions.

A classical approach for incorporating side information in the predictive models is to use graph Laplacian
regularization (Weinberger et al., 2006; Che et al., 2015). However, using this approach is not straightforward
as it relies on the appropriate deﬁnition of distance on graphs which is often unavailable.

5 Conclusion

Data insuﬃciency, either due to less common diseases or small datasets, is one of the key hurdles in healthcare
analytics, especially when we apply deep neural networks models. To overcome this challenge, we leverage the
knowledge DAG, which provides a multi-resolution view of medical concepts. We propose GRAM, a graph-based
attention model using both a knowledge DAG and EHR to learn an accurate and interpretable representations
for medical concepts. GRAM chooses a weighted average of ancestors of a medical concept and train the entire
process with a predictive model in an end-to-end fashion. We conducted three predictive modeling experiments
on real EHR datasets and showed signiﬁcant improvement in the prediction performance, especially on
low-frequency diseases and small datasets. Analysis of the attention behavior provided intuitive insight of
GRAM.

References

arXiv:1412.7755 (2014).

Jimmy Ba, Volodymyr Mnih, and Koray Kavukcuoglu. 2014. Multiple object recognition with visual attention.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural Machine Translation by Jointly

Learning to Align and Translate. arXiv:1409.0473 (2014).

Yoshua Bengio, Patrice Simard, and Paolo Frasconi. 1994. Learning long-term dependencies with gradient

descent is diﬃcult. IEEE Transactions on Neural Networks 5, 2 (1994).

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively

created graph database for structuring human knowledge. In SIGMOD.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013.

Translating embeddings for modeling multi-relational data. In NIPS.

12

Zhengping Che, David Kale, Wenzhe Li, Mohammad Taha Bahadori, and Yan Liu. 2015. Deep Computational

Phenotyping. In SIGKDD.

Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. 2016. Recurrent Neural

Networks for Multivariate Time Series with Missing Values. arXiv:1606.01865 (2016).

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for
statistical machine translation. In EMNLP.

Edward Choi, Mohammad Taha Bahadori, Andy Schuetz, Walter F. Stewart, and Jimeng Sun. 2016a. Doctor

AI: Predicting Clinical Events via Recurrent Neural Networks. In MLHC.

Edward Choi, Mohammad Taha Bahadori, Andy Schuetz, Walter F. Stewart, and Jimeng Sun. 2016b.
RETAIN: Interpretable Predictive Model in Healthcare using Reverse Time Attention Mechanism. In
NIPS.

Edward Choi, Mohammad Taha Bahadori, Elizabeth Searles, Catherine Coﬀey, Michael Thompson, James
Bost, Javier T Sojo, and Jimeng Sun. 2016c. Multi-layer Representation Learning for Medical Concepts. In
SIGKDD.

Edward Choi, Andy Schuetz, Walter F. Stewart, and Jimeng Sun. 2016e. Using Recurrent Neural Network
Models for Early Detection of Heart Failure Onset. Journal of the American Medical Informatics Association
(2016), ocw112.

Youngduck Choi, Chill Yi-I Chiu, and David Sontag. 2016d. Learning Low-Dimensional Representations of

Medical Concepts. (2016). AMIA CRI.

Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. End-to-end continuous

speech recognition using attention-based recurrent NN: First results. arXiv:1412.1602 (2014).

Ary Goldberger and others. 2000. Physiobank, physiotoolkit, and physionet components of a new research

resource for complex physiologic signals. Circulation (2000).

Aditya Grover and Jure Leskovec. 2016. Node2Vec: Scalable Feature Learning for Networks. In SIGKDD.

Jerry Gurwitz, David Magid, David Smith, Robert Goldberg, David McManus, Larry Allen, Jane Saczynski,
Micah Thorp, Grace Hsu, Sue Hee Sung, and others. 2013. Contemporary prevalence and correlates of
incident heart failure with preserved ejection fraction. The American journal of medicine 126, 5 (2013).

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural Computation 9, 8 (1997).

Alistair Johnson and others. 2016. MIMIC-III, a freely accessible critical care database. Scientiﬁc Data 3

Thomas N Kipf and Max Welling. 2016. Semi-supervised classiﬁcation with graph convolutional networks.

(2016).

arXiv:1609.02907 (2016).

Quoc V Le, Navdeep Jaitly, and Geoﬀrey E Hinton. 2015. A Simple Way to Initialize Recurrent Networks of

Rectiﬁed Linear Units. arXiv:1504.00941 (2015).

Yuezhang Li, Ronghuo Zheng, Tian Tian, Zhiting Hu, Rahul Iyer, and Katia Sycara. 2016. Joint Embedding
of Hierarchical Categories and Entities for Concept Categorization and Dataless Classiﬁcation. (2016).

Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning Entity and Relation

Embeddings for Knowledge Graph Completion. In AAAI.

13

Zachary C Lipton, David C Kale, Charles Elkan, and Randall Wetzell. 2015. Learning to Diagnose with

LSTM Recurrent Neural Networks. arXiv:1511.03677 (2015).

Zachary C Lipton, David C Kale, and Randall Wetzel. 2016. Modeling Missing Data in Clinical Time Series

with RNNs. In MLHC.

Laurens van der Maaten and Geoﬀrey Hinton. 2008. Visualizing data using t-SNE. JMLR 9, Nov (2008).

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeﬀ Dean. 2013. Distributed representations of

words and phrases and their compositionality. In NIPS.

George A Miller. 1995. WordNet: a lexical database for English. Commun. ACM 38, 11 (1995).

Riccardo Miotto, Li Li, Brian A Kidd, and Joel T Dudley. 2016. Deep Patient: An Unsupervised Representation

to Predict the Future of Patients from the Electronic Health Records. Scientiﬁc Reports 6 (2016).

Phuoc Nguyen, Truyen Tran, Nilmini Wickramasinghe, and Svetha Venkatesh. 2016. Deepr: A Convolutional

Net for Medical Records. arXiv:1607.07519 (2016).

Jeﬀrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global Vectors for Word

Representation. In EMNLP.

In SIGKDD.

Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online Learning of Social Representations.

Healthcare Cost & Utilization Project and others. 2010. Clinical classiﬁcations software (CCS) for ICD-9-CM.

Rockville, MD: Agency for Healthcare Research and Quality (2010).

Narges Razavian, Jake Marcus, and David Sontag. 2016. Multi-task Prediction of Disease Onsets from

Longitudinal Lab Tests. In MLHC.

Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. 2013. Reasoning with neural tensor

networks for knowledge base completion. In NIPS.

Michael Q Stearns, Colin Price, Kent A Spackman, and Amy Y Wang. 2001. SNOMED clinical terms:

overview of the development process and project status. In AMIA.

Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. LINE: Large-scale

Information Network Embedding. In WWW.

The Theano Development Team. 2016. Theano: A Python framework for fast computation of mathematical

expressions. arXiv:1605.02688 (2016).

Rajakrishnan Vijayakrishnan, Steven Steinhubl, Kenney Ng, Jimeng Sun, Roy Byrd, Zahra Daar, Brent
Williams, Shahram Ebadollahi, Walter Stewart, and others. 2014. Prevalence of heart failure signs and
symptoms in a large primary care population identiﬁed through the use of text and data mining of the
electronic health record. Journal of cardiac failure 20, 7 (2014).

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge Graph Embedding by Translating

on Hyperplanes. In AAAI.

Kilian Q Weinberger, Fei Sha, Qihui Zhu, and Lawrence K Saul. 2006. Graph Laplacian Regularization for

Large-Scale Semideﬁnite Programming. In NIPS.

Ruobing Xie, Zhiyuan Liu, and Maosong Sun. 2016. Representation Learning of Knowledge Graphs with

Hierarchical Types. In IJCAI.

14

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel,
and Yoshua Bengio. 2015. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention..
In ICML.

Zhilin Yang, William Cohen, and Ruslan Salakhutdinov. 2016. Revisiting Semi-Supervised Learning with

Graph Embeddings. arXiv:1603.08861 (2016).

Matthew D Zeiler. 2012. ADADELTA: an adaptive learning rate method. arXiv:1212.5701 (2012).

15

