ExpNet: Landmark-Free, Deep, 3D Facial Expressions

Feng-Ju Chang1, Anh Tuan Tran1, Tal Hassner2,3, Iacopo Masi1, Ram Nevatia1, Gerard Medioni1
1 Institute for Robotics and Intelligent Systems, USC, CA, USA
2 Information Sciences Institute, USC, CA, USA
3 The Open University of Israel, Israel
{fengjuch,anhttran,iacopoma,nevatia,medioni}@usc.edu, hassner@openu.ac.il

8
1
0
2
 
b
e
F
 
2
 
 
]

V
C
.
s
c
[
 
 
1
v
2
4
5
0
0
.
2
0
8
1
:
v
i
X
r
a

Abstract— We describe a deep learning based method for
estimating 3D facial expression coefﬁcients. Unlike previous
work, our process does not relay on facial landmark detection
methods as a proxy step. Recent methods have shown that a
CNN can be trained to regress accurate and discriminative
3D morphable model (3DMM) representations, directly from
image intensities. By foregoing facial landmark detection, these
methods were able to estimate shapes for occluded faces
appearing in unprecedented in-the-wild viewing conditions. We
build on those methods by showing that facial expressions can
also be estimated by a robust, deep, landmark-free approach.
Our ExpNet CNN is applied directly to the intensities of a face
image and regresses a 29D vector of 3D expression coefﬁcients.
We propose a unique method for collecting data to train
this network, leveraging on the robustness of deep networks
to training label noise. We further offer a novel means of
evaluating the accuracy of estimated expression coefﬁcients: by
measuring how well they capture facial emotions on the CK+
and EmotiW-17 emotion recognition benchmarks. We show
that our ExpNet produces expression coefﬁcients which better
discriminate between facial emotions than those obtained using
state of the art, facial landmark detection techniques. Moreover,
this advantage grows as image scales drop, demonstrating that
our ExpNet is more robust to scale changes than landmark
detection methods. Finally, at the same level of accuracy, our
ExpNet is orders of magnitude faster than its alternatives.

I. INTRODUCTION

Successful methods for single view 3D face shape model-
ing were proposed nearly two decades ago [4], [5], [32], [34].
These methods, and the many that followed, often claimed
high ﬁdelity reconstructions and offered parameterizations
for facial expressions besides the underlying 3D facial shape.
Despite their impressive results, they and others since [4],
[5], [8], [32], [34], [37], [42] suffered from prevailing prob-
lems when it came to processing face images taken under
unconstrained viewing conditions. Many of these methods
relied to some extent on facial landmark detection, performed
either prior to reconstruction or concurrently, as part of
the reconstruction process. By involving landmark detection,
these methods are sensitive to face pose and, aside from a few
recent exceptions (e.g., 3DDFA [49]), could not operate well
on faces viewed in extreme out of plane rotations (e.g., near
proﬁle). Scale and occlusions are also problems: Whether
because landmarks are too small to accurately localize or
altogether invisible due to occlusions, accurate detection and
consequent 3D reconstruction is not handled well.

In addition to these problems, many methods applied

978-1-5386-2335-0/18/$31.00 c(cid:13)2018 IEEE

Fig. 1: Deep 3D face modeling with expressions. We pro-
pose to regress 3DMM expression coefﬁcients without facial
landmark detection, directly from image intensities. We show
this approach to be highly robust to extreme appearance
variations, including out-of-plane head rotations (top row),
scale changes (middle), and even ages (bottom).

iterative steps of analysis-by-synthesis [3], [21], [35]. These
methods were not only computationally expensive, but also
hard to distribute and run in parallel on dedicated hardware
such as the ubiquitous graphical processing units (GPU).

Very recently, some of these problems were addressed
by two papers, which are both relevant to this work. First,
Tran et al. [38] proposed to use a deep CNN to estimate
the 3D shape and texture of faces appearing in uncon-
strained images. Their CNN regressed 3D morphable face
model (3DMM) parameters directly. To test the extent to
which their estimates were robust and discriminative, they
then used these 3DMM parameters as face representations
in challenging, unconstrained face recognition benchmarks,
including the Labeled Faces in the Wild (LFW) [20] and the
IARPA Janus Benchmark A (IJB-A) [24]. By doing so, they
showed that their estimated 3DMM parameters were nearly
as discriminative as opaque deep features extracted by deep
networks trained speciﬁcally for recognition.

Chang et al. [7] extended this work by showing that
6 degrees of freedom (6DoF) pose can also be estimated
using a similar deep, landmark free approach. Their proposed

FacePoseNet (FPN) essentially performed face alignment in
3D, directly from image intensities and without the need for
facial landmarks which are usually used for these purposes.
Our paper uses similar techniques to model 3D facial
expressions. Speciﬁcally, we show how facial expressions
can be modeled directly from image intensities using our
proposed deep neural network: ExpNet. To our knowledge,
this is the ﬁrst
time that a CNN is shown to estimate
3D expression coefﬁcients directly, without requiring or
involving facial landmark detection.

We provide a multitude of face reconstruction examples,
visualizing our estimated expressions on faces appearing
in challenging unconstrained conditions (see, e.g., Fig. 1).
We know of few previous method who offered this many
examples of their capabilities.

We go beyond previous work, however, by additionally
offering quantitative comparisons of our facial expression
estimates. To this end, we propose to measure how well dif-
ferent expression regression methods capture facial emotions
on the Extended Cohn-Kanade (CK+) [27] and EmotiW-
17 benchmarks [10]. Both benchmarks contain face images
labeled for emotion categories, allowing us to focus on how
well emotions are captured by our method and others. We
show that not only does our deep approach provide more
meaningful expression representations, it is more robust to
scale changes than methods which rely on landmarks for this
purpose. Finally, to promote reproduction of our results, our
code and deep models are publicly available.1

II. RELATED WORK

A. Expression Estimation

We ﬁrst emphasize the distinction between the related,
yet different tasks of emotion classiﬁcation vs. expression
regression. The former seeks to classify images or videos
into discrete sets of facial emotion classes [10], [27] or
action unites [14], [47]. This problem was often addressed
by considering the locations of facial landmarks. In recent
years a growing number of state of the art methods have
instead adopted deep networks [25], [26], [48], applying
them directly to image intensities rather than estimating
landmark positions as a proxy step.

Methods for expression regression attempt to extract pa-
rameters for face deformations. These parameters are of-
ten expressed in the form of active appearance models
(AAM) [27] and Blendshape model coefﬁcients [33], [49],
[50]. In this work we focus on estimating 3D expression
coefﬁcients, using the same representation described by
3DDFA [49]. Unlike 3DDFA, however, we completely de-
couple expression coefﬁcient regression from facial landmark
detection. Our tests demonstrate that by doing so, we obtain
a method which is more robust to changing image scales.

B. Facial Landmark Detection

There has been a great deal of work dedicated to accurately
detecting facial landmarks, and not only due to their role in

1Available: github.com/fengju514/Expression-Net

expression estimation. Face landmark detection is a general
problem which has applications in numerous face related
systems. Landmark detectors are very often used to align
face images by applying rigid [12], [13], [40] and non-rigid
transformations [16], [22], [49] transformations in 2D and
3D [17], [28], [29], [30], [31].

Generally speaking, landmark detectors can be divided
into two broad categories: Regression based [6], [23], [41]
and Model based [2], [46], [49] techniques. Regression based
methods estimate landmark locations directly from facial
appearance while model based methods explicitly model both
the shape and appearance of landmarks. Regardless of the
approach, landmark estimation can fail whenever faces are
viewed in extreme out-of-plane rotations (far from frontal),
low scale, or when the face bounding box differs signiﬁcantly
from the one used to develop the landmark detector.

To address the problem of varying 3D poses, the recent
3DDFA [49], related to our own, learns the parameters of
a 3DMM representations using a CNN. Unlike us, however,
they prescribe an iterative, analysis-by-synthesis approach.
Also related to us is the recent CE-CLM [46]. CE-CLM
introduces a convolution expert network to capture very com-
plex landmark appearance variations and thereby achieving
state-of-the-art landmark detection accuracy.

The exact locations of facial landmarks were once con-
sidered subject-speciﬁc information which can be used for
face recognition [11]. Today, however, such attempts are all
but abandoned. The reason for turning to other face repre-
sentations may be due to the real-word imaging conditions
typically assumed by modern face recognition systems [24]
where even state of the art landmark detection accuracy is
insufﬁcient to discriminate between individuals based solely
on the locations of their detected facial landmarks. In other
applications, however, facial landmarks prevail. This work
follows recent attempts, most notably Chang et al. [7], by
proposing landmark free alternatives for face understanding
tasks. This effort is intended to allow for accurate expression
estimation on images which defy landmark detection tech-
niques, in similar spirit to the abandonment of landmarks as
a means for representing identities. To our knowledge, such a
direct, landmark free, deep approach to expression modeling
was never previously attempted .

III. DEEP, 3D EXPRESSION MODELING

We propose to estimate facial expression coefﬁcients using
a CNN applied directly to image intensities. A chief concern
when training such deep networks is the availability of
labeled training data. For our purposes, training labels are
29D real-valued vectors of expression coefﬁcients. These
labels do not have a natural interpretations that can easily
be used by human operators to manually collect and label
training data. We next explain how 3D shapes and their
expressions are represented and how ample data may be
collected to effectively train a deep network for our purpose.

A. Representing 3D Faces and Expressions

We assume a standard 3DMM face representation [4],
[5], [8], [19], [32]. Given an input face photo I, standard
methods for estimating its 3DMM representation typically
detect facial feature points and then use those as constraints
when estimating the optimal 3DMM expression coefﬁcients
(see, for example, the recent 3DDFA method [49]). Instead,
we propose to estimate expression parameters by directly
regressing 3DMM expression coefﬁcients, decoupling shape
and texture from pose and from expression.

Speciﬁcally, we model a 3D face shape using the follow-
ing, standard, linear 3DMM representation (for now, ignoring
parameters representing facial texture and 6DoF pose):

(1)

S(cid:48) = (cid:98)s + Sα + Eη
where (cid:98)s represents the average 3D face shape. The second
term provides shape variations as a linear combination of
shape coefﬁcients α ∈ Rs with S ∈ R3n×s principal
components. 3D expression deformations are provided as
an additional linear combination of expression coefﬁcients
η ∈ Rm and expression components E ∈ R3n×m. Here,
3n represents the 3D coordinates for the n pixels in I. The
numbers of components, s, for shape and for expression, m,
provide the dimensionality of the 3DMM coefﬁcients. Our
representation uses the BFM 3DMM shape components [32],
where s = 99 and the expression components deﬁned by
3DDFA [49], with m = 29.

The vectors α and η control the intensity of deformations
provided by the principal components. Given estimates for
α and η, it is therefore possible to reconstruct the 3D face
shape of the face appearing in the input image using Eq. (1).

B. Generating 3D Expression Data

To our knowledge, there is no publicly available data set
containing sufﬁciently many face images labeled with their
29D expression coefﬁcients. Presumably, one way of miti-
gating this problem is to use a 3D facial expressions database
such as BU-4DFE [45] as a training data set. BU-4DFE
faces, however, are viewed under constrained conditions and
this would therefore limit application of the network to
constrained settings. Furthermore, BU-4DFE contains only
101 subjects and six facial expressions and can thus limit
the range of expression coefﬁcients our network predicts.

Another way of addressing the training data problem is
by utilizing a face landmark detection benchmark. That is,
taking the face images in existing landmark detection bench-
marks and computing their expression coefﬁcients using their
ground truth landmark annotations in order to obtain 29D
ground truth expression labels. Existing landmark detection
benchmarks, however, are limited in their sizes: The number
of images in the training and testing splits of the popular
300W landmark detection data set [36], for example,
is
3,026. This is far too small to train a deep CNN to regress
29D real valued vectors.

Given the absence of sufﬁciently large and rich 3D expres-
sion training sets, we propose a simple method for generating
ample examples of faces in thew wild, coupled with 29D

expression coefﬁcients labels. We begin by estimating 99D
3DMM coefﬁcients for the 0.5 million face images in the
CASIA WebFace collection [44]. 3DMM shape parameters
were estimated following the state of the art method of [38],
giving us, for every CASIA image, an estimate of its shape
coefﬁcients, α. We assume that all images belonging to
the same subject should have the same, single 3D shape.
We therefore apply the shape coefﬁcients pooling method
of [38] to average the 3DMM shape estimates for all images
belonging to the same subject, thereby obtaining a single
3DMM shape estimate per subject.

Poses were additionally estimated for each image using
FPN [7]. We then use standard techniques [15] to compute a
projection matrix Π from the 6DoF provided by that method.
Given a projection matrix Π that maps from the recovered
3D shape, S(cid:48), to the 2D points of an input image, we can
solve the following optimization problem to get expression
coefﬁcients:

η(cid:63) = arg min

||p − ΠS(cid:48)||2,

η

(2)

subject to |ηj| ≤ 3 δEj ,
where α in S(cid:48) (Eq. 1) is estimated by [38]. δEj is the standard
deviation of the j-th principal components of the 3DMM
expression; p is a set of 2D facial landmarks detected in the
input image by a standard facial landmark detection method,
in our experiments, CLNF [1]. We solve for η(cid:63) in Eq. (2)
via standard Gauss-Newton optimization.

C. Training ExpNet to Predict Expression Coefﬁcients

We use the expression coefﬁcients obtained from Eq. (2)
as ground truth labels when training our ExpNet. In prac-
tice, ExpNet employs a ResNet-101 deep network archi-
tecture [18]. We did not experiment with smaller network
structures, and so a more compact network may well work
just as well for our purposes. Our ExpNet is trained to regress
a parametric function f ({W, b}, I) (cid:55)→ η, where {W, b}
represent the parametric ﬁlters and weights of the CNN.
We use a standard (cid:96)2 reconstruction loss between ExpNet
predictions and its expression coefﬁcients training labels.

ExpNet

is trained using Stochastic Gradient Descent
(SGD) with a mini-batch size of 144, momentum of 0.9, and
weight decay of 5e-4. The network weights are updated with
learning rate set to 1e-3. When the validation loss saturates,
we decrease learning rates by an order of magnitude until
the validation loss stops decreasing. No data augmentation
is performed during training: that is, we use the plain images
in the CASIA set since they are already roughly aligned [44].
In order to make training easier, we removed the empirical
mean from all the input faces.

We note that our approach is similar to the one used by
Tran et al. [38], and in particular, we use the same network
architecture used in their work to regress 3DMM shape
and texture parameters. They, however, explicitly assume
a unique shape representations for all images of the same
subject. This assumption allowed them to better regularize
their network, by presenting it with multiple images with
varying nuisance but the same underlying label (i.e. shape

coefﬁcients do not vary within the images of a subject). Here,
this is not the case and expression parameters vary from one
image to the next, regardless of subject identity.

D. Estimating Expressions Coefﬁcients with ExpNet

Existing methods for expression estimation often take an
analysis-by-synthesis approach to optimizing facial landmark
locations. Contrary to them, our expressions are obtained in
a single forward pass of our CNN. To estimate an expression
coefﬁcients vector, ηt, we evaluate It f ({W, b}, It) for test
image, It. We preprocess test images using the face detector
of Yang et al. [43] and increasing its returned face bounding
box by a scale of ×1.25 of its size. This scaling was manually
determined to bring face bounding boxes to roughly the same
size as the loose bounding boxes of CASIA faces.

IV. EXPERIMENTAL RESULTS

We evaluated our method both qualitatively and quanti-
tatively. It is important to note that few previous methods
for 3D expression estimation performed quantitative tests;
instead, most offered only qualitative results. We provide
an extensive number of ﬁgures demonstrating the quality of
our expression estimation method (Sec. IV-B) In addition,
we offer quantitative tests, designed to capture the extent to
which our expressions reﬂect facial emotions (Sec. IV-A).

A. Quantitative Tests

Benchmark settings. Aside from 3DDFA [49], we know of
no previous method which directly estimates 29D expression
coefﬁcients vectors. Instead, previous work relied on facial
landmark detectors and used their detected landmarks to
estimate facial expressions. We therefore compare the expres-
sions estimated by our ExpNet to those obtained from state
of the art landmark detectors. Because no benchmark exists
with ground truth expression coefﬁcients, we compare these
methods on the related task of facial emotion classiﬁcation.
Our underlying assumption here is that better expression
estimation implies better emotion classiﬁcation.

We use benchmarks containing face images labeled for
discrete emotion classes. For each image we estimate its
expression coefﬁcients, either directly using our ExpNet and
3DDFA, or using detected landmarks by solving Eq. (2)
as described in Sec. III-B. We then attempt to classify the
emotions for test images using the exact same classiﬁcation
pipeline applied to these 29D expression representations.

Our

tests utilize the Extended Cohn-Kanade (CK+)
dataset [27] and the Emotion Recognition in the Wild
Challenge (EmotiW-17) dataset [10]. The CK+ dataset is a
constrained set, with frontal images taken in the lab, while
the EmotiW-17 dataset contains highly challening video
frames collected from 54 movie DVDs [9].

The CK+ dataset contains 327 face video clips labeled for
seven emotion classes: anger (An), contempt (Co), disgust
(Di), fear (Fe), happy (Ha), sadness (Sa), surprise (Su).
From each clip, we take the peak frame (the end of video)
the frame assigned with an emotion label
and use it for
classiﬁcation. The EmotiW-17 dataset, on the other hand,

Landmark-based

Deep, Direct

0.009

Time (s/img) DLIB CE-CLM OpenFace CLNF RCPR 3DDFA Us (ExpNet)
Landmarks
Pose Fitting
Expr. Fitting
Total

0.31
— 0.29 —
— 0.30 —
0.90

–
–
–
0.088

–
–
–
0.6

15.83

0.599

16.42

0.38

0.97

0.19

0.78

TABLE I: Expression estimation runtime. Comparing a num-
ber of alternative methods to our ExpNet. Landmark based
methods require several steps for landmark detection and
then expression optimization; whereas deep methods solve
for expression in a single step.

offers 383 face video clips labeled for 7 emotion classes:
anger (An), disgust (Di), fear (Fe), happy (Ha), neutral (Ne),
sadness (Sa), surprise (Su). We estimate 29D expression
representations for every frame and apply average pooling
of the per-frame estimates across all frames of each video.
Following the protocol used by [27], we ran a leave-
one clip-out test protocol to assess performance. We also
evaluate the robustness of different methods to scale changes.
Speciﬁcally, we tested all methods on multiple version of
the CK+ and EmotiW-17 benchmarks, each version with all
images scaled down to ×0.8, 0.6, 0.4, and 0.2 their sizes.

Emotion classiﬁcation pipeline. The same simple classi-
ﬁcation method was used for all methods in all our tests.
We preferred a simple classiﬁcation method rather than a
state of the art technique, in order to prevent obscuring the
quality of the landmark detector / emotion estimation by
using an elaborate classiﬁer. We therefore use a simple kNN
classiﬁer with K = 5. It is important to note that the results
obtained by all of the tested methods are far from the state
of the art on this set; our goal is not to outperform state of
the art emotion classiﬁcation methods, but only to compare
expression coefﬁcient estimation techniques.

Baseline methods. We compare our approach to widely
used, state-of-the-art face landmark detectors. These are
DLIB [23], CLNF [1], OpenFace [2], CE-CLM [46],
RCPR [6], and 3DDFA [49]. Note that CLNF is the method
used to produce our training labels.

Results. Fig. 2 and 3 report the emotion classiﬁcation confu-
sion matrices on the original CK+ and EmotiW-17 datasets
(unscaled) for our method (Fig. 2(c) and 3(c)), comparing
it to the other methods, 3DDFA (Fig. 2(b)) and CE-CLM /
CLNF (Fig. 2(a) / 3(a)).

On CK+, our expression coefﬁcients were able to capture
well surprise (Su), happy (Ha), and disgust (Di) emotions,
all emotions which are well deﬁned by facial expressions.
On EmotiW-17, our method performed well on neutral (Ne),
happy (Ha), sad (Sa), and angry (An), but less so on disgust
(Di), fear (Fe), and surprise (Su). From our observations,
these last emotions are visually similar to angry (An), which
could explain why they challenged our system. On the
whole, however, our representation was noticeably better at
capturing all emotion classes than its baselines.

Fig. 4 reports emotion classiﬁcation performances of all

(a)

(b)

(c)

Fig. 2: Confusion matrix for emotion recognition on the CK+ benchmark [27]. Confusion distributions across emotion classes
using the original input image resolution. Results provided for (a) the best performing landmark detector, CE-CLM [46],
(b) the recent, deep 3DDFA [49], (c) our ExpNet.

(a)

(b)

(c)

Fig. 3: Confusion matrix for emotion recognition on the EmotiW-17 benchmark [10]. Confusion distributions across emotion
classes using the original input image resolution. Results provided for (a) the best performing landmark detector, CLNF [1],
(b) the recent, deep 3DDFA [49], (c) our ExpNet.

(a)

(b)

Fig. 4: Emotion recognition accuracy across scales. Results provided for (a) the CK+ and (b) EmotiW-17 benchmarks. Each
curve corresponds to a different method. For each scale, the experiment resizes the input image accordingly. Lower scale
values indicate lower resolutions. Original resolutions were 640×490 (CK+) and 720×576 (EmotiW-17).

Fig. 5: Qualitative expression estimation on CK+. 3D head shapes estimated by a deep 3DMM ﬁtting method [38].
Expressions added using a number of baseline methods including our ExpNet. Our method is better able to model subtle
expressions than 3DDFA. The top-performing landmark detector, CE-CLM [46], does not perform as well on these images.

Fig. 6: Qualitative expression estimation on EmotiW-17. 3D head shapes estimated by a deep 3DMM ﬁtting method [38].
We add expressions using a number of baseline methods comparing them with our ExpNet. Our method and 3DDFA [49]
show consistent expression ﬁtting across scales. Our method additionally models subtle expressions better than 3DDFA. The
top-performing facial landmark detector, CLNF [1], does not perform as well on these images.

methods on scaled versions of the CK+ (Fig. 4(a)) and
EmotiW-17 sets (Fig. 4(b)). These results measure the sen-
sitivity of different methods to the input image resolution:
The x-axis reports the downsizing factor, proportional to the

original scale. A scale of 1 therefore represents the original
image sizes (640x490 for CK+; 720x576 for EmotiW-17),
scale of 0.2 implies 128x98 for CK+ and 144x115 for
EmotiW-17, and so fourth.

Results in Fig. 4 clearly show our approach to be the
most accurate in terms of emotion recognition accuracy. It is
additionally far more robust to scale changes compared than
the other landmark detection based methods. Note also the
difference in emotion recognition between deep methods—
ours and [49]—and landmark based approaches.

Importantly, our method outperforms CLNF [1] by a wide
margin in all tests. This result is signiﬁcant, as CLNF was the
method used to generate our expression labels in Sec. III-B.
Our improved performance suggests that the network learned
to generalize from its training data and thus performed better
on a wider range of viewing conditions and challenges.

Runtime. Tab. I reports runtimes for the methods tested.
All tests were performed on a machine with an NVIDIA,
GeForce GTX TITAN X and an Intel Xeon CPU E5-2640
v3 @ 2.60GHz. The only exception was 3DDFA [49], which
required a Windows system and was tested using an Intel
Core i7-4820K CPU @ 3.70GHz with 8 CPUs.

We compare landmark based approaches with deep, direct
method such as 3DDFA and our ExpNet. ExpNet is at least
one order of magnitude faster than any of its alternatives.
Note that, landmark based expression ﬁtting methods gener-
ally follow a three-step process: (i) facial landmark detection,
(ii) head pose estimation, and (iii) expression ﬁtting. Their
total processing time is therefore the sum of these steps.
Although some landmark detection methods (e.g. DLIB) are
extremely efﬁcient (0.009s), they are still required to solve
the optimization problem of Eq. (2), in order to translate
these detections to an expression coefﬁcients estimate. This
process is much slower than our proposed method.

As for deep methods for expression estimation, the soft-
ware package provided by 3DDFA [49] does not allow
testing on the GPU; in their paper, they report GPU runtime
to be 0.076 seconds, which is similar to our runtime, which
was measured on a GPU. Other facial landmarks detector
based methods, including the code used to solve Eq. (2), are
all intrinsically implemented on the CPU. Though they may
conceivably be expedited signiﬁcantly by porting them to the
GPU, we are unaware of any such implementation.

B. Qualitative Results

Fig. 5 and 6 provide qualitative renderings of the 3D
expressions estimated on CK+ and EmotiW-17 images. Each
result was obtained on the original, input image scale (scale
1) and also at our lowest resolution (scale 0.2). All the results
in these ﬁgures use the same 3D shape provided by 3DMM-
CNN [38]. Additional, mid level facial details can possibly
be added using, e.g., [39], but to emphasize expressions,
rather than details, we used only course facial shapes.

These ﬁgures visualize expressions estimated with our
ExpNet compared with the recent deep method for joint
estimation of shape and expression [49], and the top per-
forming landmark detectors CE-CLM [46], and CLNF [1].
For reference, we provide also the shape 3D face shape [38]
estimated before expressions were added.

Our expression estimates appear to be much better at
capturing expression nuances: This is clear from the subtle

Fig. 7: Expression estimation failures. Our method is less
able to handle extreme facial expressions. Other methods,
by comparison, appear to either exaggerate the expression
(3DDFA) or are inconsistent across scales (CE-CLM).

expressions, fear and anger, rendered in Fig. 5. This is
consistent with the improvement shown in the confusion
matrices in Fig.2. 3DDFA appears inconsistent across the
same expression (happy) and tends to either exaggerate the
expression or underestimate it. Both CE-CLM and CLNF
seem sensitive to input image resolutions: They both estimate
different expressions for the same input image offered at
different scales.

Finally, Fig. 5 demonstrates a weaknesses of our ExpNet
to strong intensity expressions such as surprise. 3DDFA, by
comparison, produces somewhat over-exaggerated estimates
on these images. Although CE-CLM produces visually suit-
able estimates, its predictions are inconsistent across scales.

V. CONCLUSIONS

We present a method for deep, 3D expression modeling
and show it to be far more robust than than facial landmark
detection methods widely used for this task. Our approach
estimates expressions without the use of facial landmarks,
suggesting that facial landmark detection methods may be
redundant for this task. This conclusion is consistent with
recent results demonstrating deep, landmark free 3D face
shape estimation [7] and 6DoF head alignment [38]. The sig-
niﬁcance of these results is that by avoiding facial landmark
detection, we can process face images obtained in extreme
viewing condition which can be challenging for landmark
detection methods.

REFERENCES

[1] T. Baltrusaitis, P. Robinson, and L.-P. Morency. Constrained local
neural ﬁelds for robust facial landmark detection in the wild. In Proc.
Conf. Comput. Vision Pattern Recognition Workshops, pages 354–361.
IEEE, 2013.

[2] T. Baltruˇsaitis, P. Robinson, and L.-P. Morency. Openface: an open
In Winter Conf. on App. of

source facial behavior analysis toolkit.
Comput. Vision, 2016.

[3] A. Bas, W. A. P. Smith, T. Bolkart, and S. Wuhrer. Fitting a 3D
morphable model to edges: A comparison between hard and soft
correspondences. arxiv preprint, abs/1602.01125, 2016.

[4] V. Blanz, S. Romdhani, and T. Vetter. Face identiﬁcation across
different poses and illuminations with a 3d morphable model. In Int.
Conf. on Automatic Face and Gesture Recognition, pages 192–197,
2002.

[5] V. Blanz and T. Vetter.

Face recognition based on ﬁtting a 3d
morphable model. Trans. Pattern Anal. Mach. Intell., 25(9):1063–
1074, 2003.

[6] X. P. Burgos-Artizzu, P. Perona, and P. Doll´ar. Robust face landmark
estimation under occlusion. In Proc. Int. Conf. Comput. Vision, pages
1513–1520. IEEE, 2013.

[7] F.-J. Chang, A. Tran, T. Hassner, I. Masi, R. Nevatia, and G. Medioni.
FacePoseNet: Making a case for landmark-free face alignment.
In
Proc. Int. Conf. Comput. Vision Workshops, 2017.

[8] B. Chu, S. Romdhani, and L. Chen. 3D-aided face recognition robust
In Proc. Conf. Comput. Vision

to expression and pose variations.
Pattern Recognition, 2014.

[9] A. Dhall et al. Collecting large, richly annotated facial-expression

databases from movies. 2012.

[10] A. Dhall, R. Goecke, S. Ghosh, J. Joshi, J. Hoey, and T. Gedeon.
From individual to group-level emotion recognition: Emotiw 5.0. In
Proceedings of the 19th ACM International Conference on Multimodal
Interaction, pages 524–528. ACM, 2017.

[11] G. J. Edwards, T. F. Cootes, and C. J. Taylor. Face recognition using
active appearance models. In European Conf. Comput. Vision, pages
581–595. Springer, 1998.

[12] E. Eidinger, R. Enbar, and T. Hassner. Age and gender estimation
of unﬁltered faces. Trans. on Inform. Forensics and Security, 9(12),
2014.

[13] M. Everingham, J. Sivic, and A. Zisserman. “Hello! My name is...
Buffy” – automatic naming of characters in TV video. In Proc. British
Mach. Vision Conf., 2006.

[14] C. Fabian Benitez-Quiroz, R. Srinivasan, and A. M. Martinez. Emo-
tionet: An accurate, real-time algorithm for the automatic annotation
of a million facial expressions in the wild. In Proc. Conf. Comput.
Vision Pattern Recognition, pages 5562–5570, 2016.

[15] R. Hartley and A. Zisserman. Multiple view geometry in computer

vision. Cambridge university press, 2003.
[16] T. Hassner. Viewing real-world faces in 3D.

In Proc. Int. Conf.
Comput. Vision, pages 3607–3614. IEEE, 2013. Available: www.
openu.ac.il/home/hassner/projects/poses.

[17] T. Hassner, S. Harel, E. Paz, and R. Enbar. Effective face frontalization
In Proc. Conf. Comput. Vision Pattern

in unconstrained images.
Recognition, 2015.

[18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image
recognition. In Proc. Conf. Comput. Vision Pattern Recognition, June
2016.

[19] G. Hu, F. Yan, C.-H. Chan, W. Deng, W. Christmas, J. Kittler, and
N. M. Robertson. Face recognition using a uniﬁed 3D morphable
model. In European Conf. Comput. Vision. Springer, 2016.

[20] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. Labeled
faces in the wild: A database for studying face recognition in un-
constrained environments. Technical Report 07-49, UMass, Amherst,
October 2007.

[21] P. Huber, G. Hu, R. Tena, P. Mortazavian, W. Koppen, W. Christmas,
M. Rtsch, and J. Kittler. A multiresolution 3D morphable face model
and ﬁtting framework. In Int. Conf. on Computer Vision Theory and
Applications, 2016.

[22] L. A. Jeni, J. F. Cohn, and T. Kanade. Dense 3D face alignment from
2D videos in real-time. In Int. Conf. on Automatic Face and Gesture
Recognition, volume 1. IEEE, 2015.

[23] D. E. King. Dlib-ml: A machine learning toolkit. J. Mach. Learning

Research, 10(Jul):1755–1758, 2009.

[24] B. F. Klare, B. Klein, E. Taborsky, A. Blanton, J. Cheney, K. Allen,
P. Grother, A. Mah, M. Burge, and A. K. Jain. Pushing the fron-
tiers of unconstrained face detection and recognition: IARPA Janus
In Proc. Conf. Comput. Vision Pattern Recognition,
Benchmark-A.
2015.

[25] R. Kosti, J. M. Alvarez, A. Recasens, and A. Lapedriza. Emotion
recognition in context. In Proc. Conf. Comput. Vision Pattern Recog-
nition, 2017.

[26] G. Levi and T. Hassner.

Emotion recognition in the wild via
In Int.

convolutional neural networks and mapped binary patterns.
Conf. on Multimodal Interaction, pages 503–510. ACM, 2015.
[27] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and
I. Matthews. The extended cohn-kanade dataset (ck+): A complete
In Proc.
dataset for action unit and emotion-speciﬁed expression.
Conf. Comput. Vision Pattern Recognition Workshops, pages 94–101.
IEEE, 2010.

[28] I. Masi, F. J. Chang, J. Choi, S. Harel, J. Kim, K. Kim, J. Leksut,
S. Rawls, Y. Wu, T. Hassner, W. AbdAlmageed, G. Medioni, L. P.
Morency, P. Natarajan, and R. Nevatia. Learning pose-aware models
for pose-invariant face recognition in the wild. Trans. Pattern Anal.
Mach. Intell., PP(99):1–1, 2018.

[29] I. Masi, C. Ferrari, A. Del Bimbo, and G. Medioni. Pose independent
face recognition by localizing local binary patterns via deformation
components. In Int. Conf. on Pattern Recognition, pages 4477–4482,
2014.

[30] I. Masi, T. Hassner, A. T. Tran, and G. Medioni. Rapid synthesis
In Int. Conf.
of massive face sets for improved face recognition.
on Automatic Face and Gesture Recognition, pages 604–611. IEEE,
2017.

[31] I. Masi, A. Tran, T. Hassner, J. T. Leksut, and G. Medioni. Do We Re-
ally Need to Collect Millions of Faces for Effective Face Recognition?
In European Conf. Comput. Vision, 2016. Available www.openu.
ac.il/home/hassner/projects/augmented_faces.
[32] P. Paysan, R. Knothe, B. Amberg, S. Romhani, and T. Vetter. A 3D
face model for pose and illumination invariant face recognition.
In
Int. Conf. on Advanced Video and Signal based Surveillance, 2009.

[33] E. Richardson, M. Sela, R. Or-El, and R. Kimmel.
detailed face reconstruction from a single image.
arXiv:1611.05053, 2016.

Learning
arXiv preprint

[34] S. Romdhani and T. Vetter. Efﬁcient, robust and accurate ﬁtting of a
3D morphable model. In Proc. Int. Conf. Comput. Vision, 2003.
[35] S. Romdhani and T. Vetter. Estimating 3D shape and texture using
pixel intensity, edges, specular highlights, texture constraints and a
prior. In Proc. Conf. Comput. Vision Pattern Recognition, volume 2,
pages 986–993, 2005.

[36] C. Sagonas, E. Antonakos, G. Tzimiropoulos, S. Zafeiriou, and
M. Pantic. 300 faces in-the-wild challenge: Database and results.
Image and Vision Computing, 2015.

[37] H. Tang, Y. Hu, Y. Fu, M. Hasegawa-Johnson, and T. S. Huang. Real-
time conversion from a single 2d face image to a 3D text-driven
emotive audio-visual avatar. In Int. Conf. on Multimedia and Expo,
pages 1205–1208. IEEE, 2008.

[38] A. Tran, T. Hassner, I. Masi, and G. Medioni. Regressing robust and
discriminative 3D morphable models with a very deep neural network.
In Proc. Conf. Comput. Vision Pattern Recognition, 2017.

[39] A. T. Tran, T. Hassner, I. Masi, E. Paz, Y. Nirkin, and G. Medioni.
arXiv

Extreme 3D face reconstruction: Looking past occlusions.
preprint arXiv:1712.05083, 2017.

[40] L. Wolf, T. Hassner, and I. Maoz. Face recognition in unconstrained
videos with matched background similarity. In Proc. Conf. Comput.
Vision Pattern Recognition, 2011.

[41] Y. Wu, T. Hassner, K. Kim, G. Medioni, and P. Natarajan. Facial
landmark detection with tweaked convolutional neural networks. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 2017.
[42] F. Yang, J. Wang, E. Shechtman, L. Bourdev, and D. Metaxas.
Expression ﬂow for 3D-aware face component transfer. ACM Trans.
on Graphics, 30(4):60, 2011.

[43] Z. Yang and R. Nevatia. A multi-scale cascade fully convolutional
In Int. Conf. on Pattern Recognition, pages

network face detector.
633–638, 2016.

[44] D. Yi, Z. Lei, S. Liao,
representation from scratch.
2014.
CASIA-WebFace-Database.html.

Learning face
arXiv preprint arXiv:1411.7923,
Available: http://www.cbsr.ia.ac.cn/english/

and S. Z. Li.

[45] L. Yin, X. Chen, Y. Sun, T. Worm, and M. Reale. A high-resolution
3d dynamic facial expression database. In Automatic Face & Gesture
Recognition, 2008. FG’08. 8th IEEE International Conference on,
pages 1–6. IEEE, 2008.

[46] A. Zadeh, T. Baltruˇsaitis, and L.-P. Morency. Convolutional experts
constrained local model for facial landmark detection. In Proc. Conf.
Comput. Vision Pattern Recognition Workshops, 2017.

[47] S. Zafeiriou, A. Papaioannou, I. Kotsia, M. Nicolaou, and G. Zhao.
In Proc. Conf. Comput. Vision Pattern

Facial affect“in-the-wild”.
Recognition Workshops, pages 36–47, 2016.

[48] K. Zhang, L. Tan, Z. Li, and Y. Qiao. Gender and smile classiﬁcation
In Proc. Conf. Comput.

using deep convolutional neural networks.
Vision Pattern Recognition Workshops, pages 34–38, 2016.

[49] X. Zhu, Z. Lei, X. Liu, H. Shi, and S. Li. Face alignment across
In Proc. Conf. Comput. Vision Pattern

large poses: A 3D solution.
Recognition, Las Vegas, NV, June 2016.

[50] X. Zhu, Z. Lei, J. Yan, D. Yi, and S. Z. Li. High-ﬁdelity pose and
In Proc.

expression normalization for face recognition in the wild.
Conf. Comput. Vision Pattern Recognition, pages 787–796, 2015.

ExpNet: Landmark-Free, Deep, 3D Facial Expressions

Feng-Ju Chang1, Anh Tuan Tran1, Tal Hassner2,3, Iacopo Masi1, Ram Nevatia1, Gerard Medioni1
1 Institute for Robotics and Intelligent Systems, USC, CA, USA
2 Information Sciences Institute, USC, CA, USA
3 The Open University of Israel, Israel
{fengjuch,anhttran,iacopoma,nevatia,medioni}@usc.edu, hassner@openu.ac.il

8
1
0
2
 
b
e
F
 
2
 
 
]

V
C
.
s
c
[
 
 
1
v
2
4
5
0
0
.
2
0
8
1
:
v
i
X
r
a

Abstract— We describe a deep learning based method for
estimating 3D facial expression coefﬁcients. Unlike previous
work, our process does not relay on facial landmark detection
methods as a proxy step. Recent methods have shown that a
CNN can be trained to regress accurate and discriminative
3D morphable model (3DMM) representations, directly from
image intensities. By foregoing facial landmark detection, these
methods were able to estimate shapes for occluded faces
appearing in unprecedented in-the-wild viewing conditions. We
build on those methods by showing that facial expressions can
also be estimated by a robust, deep, landmark-free approach.
Our ExpNet CNN is applied directly to the intensities of a face
image and regresses a 29D vector of 3D expression coefﬁcients.
We propose a unique method for collecting data to train
this network, leveraging on the robustness of deep networks
to training label noise. We further offer a novel means of
evaluating the accuracy of estimated expression coefﬁcients: by
measuring how well they capture facial emotions on the CK+
and EmotiW-17 emotion recognition benchmarks. We show
that our ExpNet produces expression coefﬁcients which better
discriminate between facial emotions than those obtained using
state of the art, facial landmark detection techniques. Moreover,
this advantage grows as image scales drop, demonstrating that
our ExpNet is more robust to scale changes than landmark
detection methods. Finally, at the same level of accuracy, our
ExpNet is orders of magnitude faster than its alternatives.

I. INTRODUCTION

Successful methods for single view 3D face shape model-
ing were proposed nearly two decades ago [4], [5], [32], [34].
These methods, and the many that followed, often claimed
high ﬁdelity reconstructions and offered parameterizations
for facial expressions besides the underlying 3D facial shape.
Despite their impressive results, they and others since [4],
[5], [8], [32], [34], [37], [42] suffered from prevailing prob-
lems when it came to processing face images taken under
unconstrained viewing conditions. Many of these methods
relied to some extent on facial landmark detection, performed
either prior to reconstruction or concurrently, as part of
the reconstruction process. By involving landmark detection,
these methods are sensitive to face pose and, aside from a few
recent exceptions (e.g., 3DDFA [49]), could not operate well
on faces viewed in extreme out of plane rotations (e.g., near
proﬁle). Scale and occlusions are also problems: Whether
because landmarks are too small to accurately localize or
altogether invisible due to occlusions, accurate detection and
consequent 3D reconstruction is not handled well.

In addition to these problems, many methods applied

978-1-5386-2335-0/18/$31.00 c(cid:13)2018 IEEE

Fig. 1: Deep 3D face modeling with expressions. We pro-
pose to regress 3DMM expression coefﬁcients without facial
landmark detection, directly from image intensities. We show
this approach to be highly robust to extreme appearance
variations, including out-of-plane head rotations (top row),
scale changes (middle), and even ages (bottom).

iterative steps of analysis-by-synthesis [3], [21], [35]. These
methods were not only computationally expensive, but also
hard to distribute and run in parallel on dedicated hardware
such as the ubiquitous graphical processing units (GPU).

Very recently, some of these problems were addressed
by two papers, which are both relevant to this work. First,
Tran et al. [38] proposed to use a deep CNN to estimate
the 3D shape and texture of faces appearing in uncon-
strained images. Their CNN regressed 3D morphable face
model (3DMM) parameters directly. To test the extent to
which their estimates were robust and discriminative, they
then used these 3DMM parameters as face representations
in challenging, unconstrained face recognition benchmarks,
including the Labeled Faces in the Wild (LFW) [20] and the
IARPA Janus Benchmark A (IJB-A) [24]. By doing so, they
showed that their estimated 3DMM parameters were nearly
as discriminative as opaque deep features extracted by deep
networks trained speciﬁcally for recognition.

Chang et al. [7] extended this work by showing that
6 degrees of freedom (6DoF) pose can also be estimated
using a similar deep, landmark free approach. Their proposed

FacePoseNet (FPN) essentially performed face alignment in
3D, directly from image intensities and without the need for
facial landmarks which are usually used for these purposes.
Our paper uses similar techniques to model 3D facial
expressions. Speciﬁcally, we show how facial expressions
can be modeled directly from image intensities using our
proposed deep neural network: ExpNet. To our knowledge,
this is the ﬁrst
time that a CNN is shown to estimate
3D expression coefﬁcients directly, without requiring or
involving facial landmark detection.

We provide a multitude of face reconstruction examples,
visualizing our estimated expressions on faces appearing
in challenging unconstrained conditions (see, e.g., Fig. 1).
We know of few previous method who offered this many
examples of their capabilities.

We go beyond previous work, however, by additionally
offering quantitative comparisons of our facial expression
estimates. To this end, we propose to measure how well dif-
ferent expression regression methods capture facial emotions
on the Extended Cohn-Kanade (CK+) [27] and EmotiW-
17 benchmarks [10]. Both benchmarks contain face images
labeled for emotion categories, allowing us to focus on how
well emotions are captured by our method and others. We
show that not only does our deep approach provide more
meaningful expression representations, it is more robust to
scale changes than methods which rely on landmarks for this
purpose. Finally, to promote reproduction of our results, our
code and deep models are publicly available.1

II. RELATED WORK

A. Expression Estimation

We ﬁrst emphasize the distinction between the related,
yet different tasks of emotion classiﬁcation vs. expression
regression. The former seeks to classify images or videos
into discrete sets of facial emotion classes [10], [27] or
action unites [14], [47]. This problem was often addressed
by considering the locations of facial landmarks. In recent
years a growing number of state of the art methods have
instead adopted deep networks [25], [26], [48], applying
them directly to image intensities rather than estimating
landmark positions as a proxy step.

Methods for expression regression attempt to extract pa-
rameters for face deformations. These parameters are of-
ten expressed in the form of active appearance models
(AAM) [27] and Blendshape model coefﬁcients [33], [49],
[50]. In this work we focus on estimating 3D expression
coefﬁcients, using the same representation described by
3DDFA [49]. Unlike 3DDFA, however, we completely de-
couple expression coefﬁcient regression from facial landmark
detection. Our tests demonstrate that by doing so, we obtain
a method which is more robust to changing image scales.

B. Facial Landmark Detection

There has been a great deal of work dedicated to accurately
detecting facial landmarks, and not only due to their role in

1Available: github.com/fengju514/Expression-Net

expression estimation. Face landmark detection is a general
problem which has applications in numerous face related
systems. Landmark detectors are very often used to align
face images by applying rigid [12], [13], [40] and non-rigid
transformations [16], [22], [49] transformations in 2D and
3D [17], [28], [29], [30], [31].

Generally speaking, landmark detectors can be divided
into two broad categories: Regression based [6], [23], [41]
and Model based [2], [46], [49] techniques. Regression based
methods estimate landmark locations directly from facial
appearance while model based methods explicitly model both
the shape and appearance of landmarks. Regardless of the
approach, landmark estimation can fail whenever faces are
viewed in extreme out-of-plane rotations (far from frontal),
low scale, or when the face bounding box differs signiﬁcantly
from the one used to develop the landmark detector.

To address the problem of varying 3D poses, the recent
3DDFA [49], related to our own, learns the parameters of
a 3DMM representations using a CNN. Unlike us, however,
they prescribe an iterative, analysis-by-synthesis approach.
Also related to us is the recent CE-CLM [46]. CE-CLM
introduces a convolution expert network to capture very com-
plex landmark appearance variations and thereby achieving
state-of-the-art landmark detection accuracy.

The exact locations of facial landmarks were once con-
sidered subject-speciﬁc information which can be used for
face recognition [11]. Today, however, such attempts are all
but abandoned. The reason for turning to other face repre-
sentations may be due to the real-word imaging conditions
typically assumed by modern face recognition systems [24]
where even state of the art landmark detection accuracy is
insufﬁcient to discriminate between individuals based solely
on the locations of their detected facial landmarks. In other
applications, however, facial landmarks prevail. This work
follows recent attempts, most notably Chang et al. [7], by
proposing landmark free alternatives for face understanding
tasks. This effort is intended to allow for accurate expression
estimation on images which defy landmark detection tech-
niques, in similar spirit to the abandonment of landmarks as
a means for representing identities. To our knowledge, such a
direct, landmark free, deep approach to expression modeling
was never previously attempted .

III. DEEP, 3D EXPRESSION MODELING

We propose to estimate facial expression coefﬁcients using
a CNN applied directly to image intensities. A chief concern
when training such deep networks is the availability of
labeled training data. For our purposes, training labels are
29D real-valued vectors of expression coefﬁcients. These
labels do not have a natural interpretations that can easily
be used by human operators to manually collect and label
training data. We next explain how 3D shapes and their
expressions are represented and how ample data may be
collected to effectively train a deep network for our purpose.

A. Representing 3D Faces and Expressions

We assume a standard 3DMM face representation [4],
[5], [8], [19], [32]. Given an input face photo I, standard
methods for estimating its 3DMM representation typically
detect facial feature points and then use those as constraints
when estimating the optimal 3DMM expression coefﬁcients
(see, for example, the recent 3DDFA method [49]). Instead,
we propose to estimate expression parameters by directly
regressing 3DMM expression coefﬁcients, decoupling shape
and texture from pose and from expression.

Speciﬁcally, we model a 3D face shape using the follow-
ing, standard, linear 3DMM representation (for now, ignoring
parameters representing facial texture and 6DoF pose):

(1)

S(cid:48) = (cid:98)s + Sα + Eη
where (cid:98)s represents the average 3D face shape. The second
term provides shape variations as a linear combination of
shape coefﬁcients α ∈ Rs with S ∈ R3n×s principal
components. 3D expression deformations are provided as
an additional linear combination of expression coefﬁcients
η ∈ Rm and expression components E ∈ R3n×m. Here,
3n represents the 3D coordinates for the n pixels in I. The
numbers of components, s, for shape and for expression, m,
provide the dimensionality of the 3DMM coefﬁcients. Our
representation uses the BFM 3DMM shape components [32],
where s = 99 and the expression components deﬁned by
3DDFA [49], with m = 29.

The vectors α and η control the intensity of deformations
provided by the principal components. Given estimates for
α and η, it is therefore possible to reconstruct the 3D face
shape of the face appearing in the input image using Eq. (1).

B. Generating 3D Expression Data

To our knowledge, there is no publicly available data set
containing sufﬁciently many face images labeled with their
29D expression coefﬁcients. Presumably, one way of miti-
gating this problem is to use a 3D facial expressions database
such as BU-4DFE [45] as a training data set. BU-4DFE
faces, however, are viewed under constrained conditions and
this would therefore limit application of the network to
constrained settings. Furthermore, BU-4DFE contains only
101 subjects and six facial expressions and can thus limit
the range of expression coefﬁcients our network predicts.

Another way of addressing the training data problem is
by utilizing a face landmark detection benchmark. That is,
taking the face images in existing landmark detection bench-
marks and computing their expression coefﬁcients using their
ground truth landmark annotations in order to obtain 29D
ground truth expression labels. Existing landmark detection
benchmarks, however, are limited in their sizes: The number
of images in the training and testing splits of the popular
300W landmark detection data set [36], for example,
is
3,026. This is far too small to train a deep CNN to regress
29D real valued vectors.

Given the absence of sufﬁciently large and rich 3D expres-
sion training sets, we propose a simple method for generating
ample examples of faces in thew wild, coupled with 29D

expression coefﬁcients labels. We begin by estimating 99D
3DMM coefﬁcients for the 0.5 million face images in the
CASIA WebFace collection [44]. 3DMM shape parameters
were estimated following the state of the art method of [38],
giving us, for every CASIA image, an estimate of its shape
coefﬁcients, α. We assume that all images belonging to
the same subject should have the same, single 3D shape.
We therefore apply the shape coefﬁcients pooling method
of [38] to average the 3DMM shape estimates for all images
belonging to the same subject, thereby obtaining a single
3DMM shape estimate per subject.

Poses were additionally estimated for each image using
FPN [7]. We then use standard techniques [15] to compute a
projection matrix Π from the 6DoF provided by that method.
Given a projection matrix Π that maps from the recovered
3D shape, S(cid:48), to the 2D points of an input image, we can
solve the following optimization problem to get expression
coefﬁcients:

η(cid:63) = arg min

||p − ΠS(cid:48)||2,

η

(2)

subject to |ηj| ≤ 3 δEj ,
where α in S(cid:48) (Eq. 1) is estimated by [38]. δEj is the standard
deviation of the j-th principal components of the 3DMM
expression; p is a set of 2D facial landmarks detected in the
input image by a standard facial landmark detection method,
in our experiments, CLNF [1]. We solve for η(cid:63) in Eq. (2)
via standard Gauss-Newton optimization.

C. Training ExpNet to Predict Expression Coefﬁcients

We use the expression coefﬁcients obtained from Eq. (2)
as ground truth labels when training our ExpNet. In prac-
tice, ExpNet employs a ResNet-101 deep network archi-
tecture [18]. We did not experiment with smaller network
structures, and so a more compact network may well work
just as well for our purposes. Our ExpNet is trained to regress
a parametric function f ({W, b}, I) (cid:55)→ η, where {W, b}
represent the parametric ﬁlters and weights of the CNN.
We use a standard (cid:96)2 reconstruction loss between ExpNet
predictions and its expression coefﬁcients training labels.

ExpNet

is trained using Stochastic Gradient Descent
(SGD) with a mini-batch size of 144, momentum of 0.9, and
weight decay of 5e-4. The network weights are updated with
learning rate set to 1e-3. When the validation loss saturates,
we decrease learning rates by an order of magnitude until
the validation loss stops decreasing. No data augmentation
is performed during training: that is, we use the plain images
in the CASIA set since they are already roughly aligned [44].
In order to make training easier, we removed the empirical
mean from all the input faces.

We note that our approach is similar to the one used by
Tran et al. [38], and in particular, we use the same network
architecture used in their work to regress 3DMM shape
and texture parameters. They, however, explicitly assume
a unique shape representations for all images of the same
subject. This assumption allowed them to better regularize
their network, by presenting it with multiple images with
varying nuisance but the same underlying label (i.e. shape

coefﬁcients do not vary within the images of a subject). Here,
this is not the case and expression parameters vary from one
image to the next, regardless of subject identity.

D. Estimating Expressions Coefﬁcients with ExpNet

Existing methods for expression estimation often take an
analysis-by-synthesis approach to optimizing facial landmark
locations. Contrary to them, our expressions are obtained in
a single forward pass of our CNN. To estimate an expression
coefﬁcients vector, ηt, we evaluate It f ({W, b}, It) for test
image, It. We preprocess test images using the face detector
of Yang et al. [43] and increasing its returned face bounding
box by a scale of ×1.25 of its size. This scaling was manually
determined to bring face bounding boxes to roughly the same
size as the loose bounding boxes of CASIA faces.

IV. EXPERIMENTAL RESULTS

We evaluated our method both qualitatively and quanti-
tatively. It is important to note that few previous methods
for 3D expression estimation performed quantitative tests;
instead, most offered only qualitative results. We provide
an extensive number of ﬁgures demonstrating the quality of
our expression estimation method (Sec. IV-B) In addition,
we offer quantitative tests, designed to capture the extent to
which our expressions reﬂect facial emotions (Sec. IV-A).

A. Quantitative Tests

Benchmark settings. Aside from 3DDFA [49], we know of
no previous method which directly estimates 29D expression
coefﬁcients vectors. Instead, previous work relied on facial
landmark detectors and used their detected landmarks to
estimate facial expressions. We therefore compare the expres-
sions estimated by our ExpNet to those obtained from state
of the art landmark detectors. Because no benchmark exists
with ground truth expression coefﬁcients, we compare these
methods on the related task of facial emotion classiﬁcation.
Our underlying assumption here is that better expression
estimation implies better emotion classiﬁcation.

We use benchmarks containing face images labeled for
discrete emotion classes. For each image we estimate its
expression coefﬁcients, either directly using our ExpNet and
3DDFA, or using detected landmarks by solving Eq. (2)
as described in Sec. III-B. We then attempt to classify the
emotions for test images using the exact same classiﬁcation
pipeline applied to these 29D expression representations.

Our

tests utilize the Extended Cohn-Kanade (CK+)
dataset [27] and the Emotion Recognition in the Wild
Challenge (EmotiW-17) dataset [10]. The CK+ dataset is a
constrained set, with frontal images taken in the lab, while
the EmotiW-17 dataset contains highly challening video
frames collected from 54 movie DVDs [9].

The CK+ dataset contains 327 face video clips labeled for
seven emotion classes: anger (An), contempt (Co), disgust
(Di), fear (Fe), happy (Ha), sadness (Sa), surprise (Su).
From each clip, we take the peak frame (the end of video)
the frame assigned with an emotion label
and use it for
classiﬁcation. The EmotiW-17 dataset, on the other hand,

Landmark-based

Deep, Direct

0.009

Time (s/img) DLIB CE-CLM OpenFace CLNF RCPR 3DDFA Us (ExpNet)
Landmarks
Pose Fitting
Expr. Fitting
Total

0.31
— 0.29 —
— 0.30 —
0.90

–
–
–
0.088

–
–
–
0.6

15.83

0.599

16.42

0.38

0.97

0.19

0.78

TABLE I: Expression estimation runtime. Comparing a num-
ber of alternative methods to our ExpNet. Landmark based
methods require several steps for landmark detection and
then expression optimization; whereas deep methods solve
for expression in a single step.

offers 383 face video clips labeled for 7 emotion classes:
anger (An), disgust (Di), fear (Fe), happy (Ha), neutral (Ne),
sadness (Sa), surprise (Su). We estimate 29D expression
representations for every frame and apply average pooling
of the per-frame estimates across all frames of each video.
Following the protocol used by [27], we ran a leave-
one clip-out test protocol to assess performance. We also
evaluate the robustness of different methods to scale changes.
Speciﬁcally, we tested all methods on multiple version of
the CK+ and EmotiW-17 benchmarks, each version with all
images scaled down to ×0.8, 0.6, 0.4, and 0.2 their sizes.

Emotion classiﬁcation pipeline. The same simple classi-
ﬁcation method was used for all methods in all our tests.
We preferred a simple classiﬁcation method rather than a
state of the art technique, in order to prevent obscuring the
quality of the landmark detector / emotion estimation by
using an elaborate classiﬁer. We therefore use a simple kNN
classiﬁer with K = 5. It is important to note that the results
obtained by all of the tested methods are far from the state
of the art on this set; our goal is not to outperform state of
the art emotion classiﬁcation methods, but only to compare
expression coefﬁcient estimation techniques.

Baseline methods. We compare our approach to widely
used, state-of-the-art face landmark detectors. These are
DLIB [23], CLNF [1], OpenFace [2], CE-CLM [46],
RCPR [6], and 3DDFA [49]. Note that CLNF is the method
used to produce our training labels.

Results. Fig. 2 and 3 report the emotion classiﬁcation confu-
sion matrices on the original CK+ and EmotiW-17 datasets
(unscaled) for our method (Fig. 2(c) and 3(c)), comparing
it to the other methods, 3DDFA (Fig. 2(b)) and CE-CLM /
CLNF (Fig. 2(a) / 3(a)).

On CK+, our expression coefﬁcients were able to capture
well surprise (Su), happy (Ha), and disgust (Di) emotions,
all emotions which are well deﬁned by facial expressions.
On EmotiW-17, our method performed well on neutral (Ne),
happy (Ha), sad (Sa), and angry (An), but less so on disgust
(Di), fear (Fe), and surprise (Su). From our observations,
these last emotions are visually similar to angry (An), which
could explain why they challenged our system. On the
whole, however, our representation was noticeably better at
capturing all emotion classes than its baselines.

Fig. 4 reports emotion classiﬁcation performances of all

(a)

(b)

(c)

Fig. 2: Confusion matrix for emotion recognition on the CK+ benchmark [27]. Confusion distributions across emotion classes
using the original input image resolution. Results provided for (a) the best performing landmark detector, CE-CLM [46],
(b) the recent, deep 3DDFA [49], (c) our ExpNet.

(a)

(b)

(c)

Fig. 3: Confusion matrix for emotion recognition on the EmotiW-17 benchmark [10]. Confusion distributions across emotion
classes using the original input image resolution. Results provided for (a) the best performing landmark detector, CLNF [1],
(b) the recent, deep 3DDFA [49], (c) our ExpNet.

(a)

(b)

Fig. 4: Emotion recognition accuracy across scales. Results provided for (a) the CK+ and (b) EmotiW-17 benchmarks. Each
curve corresponds to a different method. For each scale, the experiment resizes the input image accordingly. Lower scale
values indicate lower resolutions. Original resolutions were 640×490 (CK+) and 720×576 (EmotiW-17).

Fig. 5: Qualitative expression estimation on CK+. 3D head shapes estimated by a deep 3DMM ﬁtting method [38].
Expressions added using a number of baseline methods including our ExpNet. Our method is better able to model subtle
expressions than 3DDFA. The top-performing landmark detector, CE-CLM [46], does not perform as well on these images.

Fig. 6: Qualitative expression estimation on EmotiW-17. 3D head shapes estimated by a deep 3DMM ﬁtting method [38].
We add expressions using a number of baseline methods comparing them with our ExpNet. Our method and 3DDFA [49]
show consistent expression ﬁtting across scales. Our method additionally models subtle expressions better than 3DDFA. The
top-performing facial landmark detector, CLNF [1], does not perform as well on these images.

methods on scaled versions of the CK+ (Fig. 4(a)) and
EmotiW-17 sets (Fig. 4(b)). These results measure the sen-
sitivity of different methods to the input image resolution:
The x-axis reports the downsizing factor, proportional to the

original scale. A scale of 1 therefore represents the original
image sizes (640x490 for CK+; 720x576 for EmotiW-17),
scale of 0.2 implies 128x98 for CK+ and 144x115 for
EmotiW-17, and so fourth.

Results in Fig. 4 clearly show our approach to be the
most accurate in terms of emotion recognition accuracy. It is
additionally far more robust to scale changes compared than
the other landmark detection based methods. Note also the
difference in emotion recognition between deep methods—
ours and [49]—and landmark based approaches.

Importantly, our method outperforms CLNF [1] by a wide
margin in all tests. This result is signiﬁcant, as CLNF was the
method used to generate our expression labels in Sec. III-B.
Our improved performance suggests that the network learned
to generalize from its training data and thus performed better
on a wider range of viewing conditions and challenges.

Runtime. Tab. I reports runtimes for the methods tested.
All tests were performed on a machine with an NVIDIA,
GeForce GTX TITAN X and an Intel Xeon CPU E5-2640
v3 @ 2.60GHz. The only exception was 3DDFA [49], which
required a Windows system and was tested using an Intel
Core i7-4820K CPU @ 3.70GHz with 8 CPUs.

We compare landmark based approaches with deep, direct
method such as 3DDFA and our ExpNet. ExpNet is at least
one order of magnitude faster than any of its alternatives.
Note that, landmark based expression ﬁtting methods gener-
ally follow a three-step process: (i) facial landmark detection,
(ii) head pose estimation, and (iii) expression ﬁtting. Their
total processing time is therefore the sum of these steps.
Although some landmark detection methods (e.g. DLIB) are
extremely efﬁcient (0.009s), they are still required to solve
the optimization problem of Eq. (2), in order to translate
these detections to an expression coefﬁcients estimate. This
process is much slower than our proposed method.

As for deep methods for expression estimation, the soft-
ware package provided by 3DDFA [49] does not allow
testing on the GPU; in their paper, they report GPU runtime
to be 0.076 seconds, which is similar to our runtime, which
was measured on a GPU. Other facial landmarks detector
based methods, including the code used to solve Eq. (2), are
all intrinsically implemented on the CPU. Though they may
conceivably be expedited signiﬁcantly by porting them to the
GPU, we are unaware of any such implementation.

B. Qualitative Results

Fig. 5 and 6 provide qualitative renderings of the 3D
expressions estimated on CK+ and EmotiW-17 images. Each
result was obtained on the original, input image scale (scale
1) and also at our lowest resolution (scale 0.2). All the results
in these ﬁgures use the same 3D shape provided by 3DMM-
CNN [38]. Additional, mid level facial details can possibly
be added using, e.g., [39], but to emphasize expressions,
rather than details, we used only course facial shapes.

These ﬁgures visualize expressions estimated with our
ExpNet compared with the recent deep method for joint
estimation of shape and expression [49], and the top per-
forming landmark detectors CE-CLM [46], and CLNF [1].
For reference, we provide also the shape 3D face shape [38]
estimated before expressions were added.

Our expression estimates appear to be much better at
capturing expression nuances: This is clear from the subtle

Fig. 7: Expression estimation failures. Our method is less
able to handle extreme facial expressions. Other methods,
by comparison, appear to either exaggerate the expression
(3DDFA) or are inconsistent across scales (CE-CLM).

expressions, fear and anger, rendered in Fig. 5. This is
consistent with the improvement shown in the confusion
matrices in Fig.2. 3DDFA appears inconsistent across the
same expression (happy) and tends to either exaggerate the
expression or underestimate it. Both CE-CLM and CLNF
seem sensitive to input image resolutions: They both estimate
different expressions for the same input image offered at
different scales.

Finally, Fig. 5 demonstrates a weaknesses of our ExpNet
to strong intensity expressions such as surprise. 3DDFA, by
comparison, produces somewhat over-exaggerated estimates
on these images. Although CE-CLM produces visually suit-
able estimates, its predictions are inconsistent across scales.

V. CONCLUSIONS

We present a method for deep, 3D expression modeling
and show it to be far more robust than than facial landmark
detection methods widely used for this task. Our approach
estimates expressions without the use of facial landmarks,
suggesting that facial landmark detection methods may be
redundant for this task. This conclusion is consistent with
recent results demonstrating deep, landmark free 3D face
shape estimation [7] and 6DoF head alignment [38]. The sig-
niﬁcance of these results is that by avoiding facial landmark
detection, we can process face images obtained in extreme
viewing condition which can be challenging for landmark
detection methods.

REFERENCES

[1] T. Baltrusaitis, P. Robinson, and L.-P. Morency. Constrained local
neural ﬁelds for robust facial landmark detection in the wild. In Proc.
Conf. Comput. Vision Pattern Recognition Workshops, pages 354–361.
IEEE, 2013.

[2] T. Baltruˇsaitis, P. Robinson, and L.-P. Morency. Openface: an open
In Winter Conf. on App. of

source facial behavior analysis toolkit.
Comput. Vision, 2016.

[3] A. Bas, W. A. P. Smith, T. Bolkart, and S. Wuhrer. Fitting a 3D
morphable model to edges: A comparison between hard and soft
correspondences. arxiv preprint, abs/1602.01125, 2016.

[4] V. Blanz, S. Romdhani, and T. Vetter. Face identiﬁcation across
different poses and illuminations with a 3d morphable model. In Int.
Conf. on Automatic Face and Gesture Recognition, pages 192–197,
2002.

[5] V. Blanz and T. Vetter.

Face recognition based on ﬁtting a 3d
morphable model. Trans. Pattern Anal. Mach. Intell., 25(9):1063–
1074, 2003.

[6] X. P. Burgos-Artizzu, P. Perona, and P. Doll´ar. Robust face landmark
estimation under occlusion. In Proc. Int. Conf. Comput. Vision, pages
1513–1520. IEEE, 2013.

[7] F.-J. Chang, A. Tran, T. Hassner, I. Masi, R. Nevatia, and G. Medioni.
FacePoseNet: Making a case for landmark-free face alignment.
In
Proc. Int. Conf. Comput. Vision Workshops, 2017.

[8] B. Chu, S. Romdhani, and L. Chen. 3D-aided face recognition robust
In Proc. Conf. Comput. Vision

to expression and pose variations.
Pattern Recognition, 2014.

[9] A. Dhall et al. Collecting large, richly annotated facial-expression

databases from movies. 2012.

[10] A. Dhall, R. Goecke, S. Ghosh, J. Joshi, J. Hoey, and T. Gedeon.
From individual to group-level emotion recognition: Emotiw 5.0. In
Proceedings of the 19th ACM International Conference on Multimodal
Interaction, pages 524–528. ACM, 2017.

[11] G. J. Edwards, T. F. Cootes, and C. J. Taylor. Face recognition using
active appearance models. In European Conf. Comput. Vision, pages
581–595. Springer, 1998.

[12] E. Eidinger, R. Enbar, and T. Hassner. Age and gender estimation
of unﬁltered faces. Trans. on Inform. Forensics and Security, 9(12),
2014.

[13] M. Everingham, J. Sivic, and A. Zisserman. “Hello! My name is...
Buffy” – automatic naming of characters in TV video. In Proc. British
Mach. Vision Conf., 2006.

[14] C. Fabian Benitez-Quiroz, R. Srinivasan, and A. M. Martinez. Emo-
tionet: An accurate, real-time algorithm for the automatic annotation
of a million facial expressions in the wild. In Proc. Conf. Comput.
Vision Pattern Recognition, pages 5562–5570, 2016.

[15] R. Hartley and A. Zisserman. Multiple view geometry in computer

vision. Cambridge university press, 2003.
[16] T. Hassner. Viewing real-world faces in 3D.

In Proc. Int. Conf.
Comput. Vision, pages 3607–3614. IEEE, 2013. Available: www.
openu.ac.il/home/hassner/projects/poses.

[17] T. Hassner, S. Harel, E. Paz, and R. Enbar. Effective face frontalization
In Proc. Conf. Comput. Vision Pattern

in unconstrained images.
Recognition, 2015.

[18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image
recognition. In Proc. Conf. Comput. Vision Pattern Recognition, June
2016.

[19] G. Hu, F. Yan, C.-H. Chan, W. Deng, W. Christmas, J. Kittler, and
N. M. Robertson. Face recognition using a uniﬁed 3D morphable
model. In European Conf. Comput. Vision. Springer, 2016.

[20] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. Labeled
faces in the wild: A database for studying face recognition in un-
constrained environments. Technical Report 07-49, UMass, Amherst,
October 2007.

[21] P. Huber, G. Hu, R. Tena, P. Mortazavian, W. Koppen, W. Christmas,
M. Rtsch, and J. Kittler. A multiresolution 3D morphable face model
and ﬁtting framework. In Int. Conf. on Computer Vision Theory and
Applications, 2016.

[22] L. A. Jeni, J. F. Cohn, and T. Kanade. Dense 3D face alignment from
2D videos in real-time. In Int. Conf. on Automatic Face and Gesture
Recognition, volume 1. IEEE, 2015.

[23] D. E. King. Dlib-ml: A machine learning toolkit. J. Mach. Learning

Research, 10(Jul):1755–1758, 2009.

[24] B. F. Klare, B. Klein, E. Taborsky, A. Blanton, J. Cheney, K. Allen,
P. Grother, A. Mah, M. Burge, and A. K. Jain. Pushing the fron-
tiers of unconstrained face detection and recognition: IARPA Janus
In Proc. Conf. Comput. Vision Pattern Recognition,
Benchmark-A.
2015.

[25] R. Kosti, J. M. Alvarez, A. Recasens, and A. Lapedriza. Emotion
recognition in context. In Proc. Conf. Comput. Vision Pattern Recog-
nition, 2017.

[26] G. Levi and T. Hassner.

Emotion recognition in the wild via
In Int.

convolutional neural networks and mapped binary patterns.
Conf. on Multimodal Interaction, pages 503–510. ACM, 2015.
[27] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and
I. Matthews. The extended cohn-kanade dataset (ck+): A complete
In Proc.
dataset for action unit and emotion-speciﬁed expression.
Conf. Comput. Vision Pattern Recognition Workshops, pages 94–101.
IEEE, 2010.

[28] I. Masi, F. J. Chang, J. Choi, S. Harel, J. Kim, K. Kim, J. Leksut,
S. Rawls, Y. Wu, T. Hassner, W. AbdAlmageed, G. Medioni, L. P.
Morency, P. Natarajan, and R. Nevatia. Learning pose-aware models
for pose-invariant face recognition in the wild. Trans. Pattern Anal.
Mach. Intell., PP(99):1–1, 2018.

[29] I. Masi, C. Ferrari, A. Del Bimbo, and G. Medioni. Pose independent
face recognition by localizing local binary patterns via deformation
components. In Int. Conf. on Pattern Recognition, pages 4477–4482,
2014.

[30] I. Masi, T. Hassner, A. T. Tran, and G. Medioni. Rapid synthesis
In Int. Conf.
of massive face sets for improved face recognition.
on Automatic Face and Gesture Recognition, pages 604–611. IEEE,
2017.

[31] I. Masi, A. Tran, T. Hassner, J. T. Leksut, and G. Medioni. Do We Re-
ally Need to Collect Millions of Faces for Effective Face Recognition?
In European Conf. Comput. Vision, 2016. Available www.openu.
ac.il/home/hassner/projects/augmented_faces.
[32] P. Paysan, R. Knothe, B. Amberg, S. Romhani, and T. Vetter. A 3D
face model for pose and illumination invariant face recognition.
In
Int. Conf. on Advanced Video and Signal based Surveillance, 2009.

[33] E. Richardson, M. Sela, R. Or-El, and R. Kimmel.
detailed face reconstruction from a single image.
arXiv:1611.05053, 2016.

Learning
arXiv preprint

[34] S. Romdhani and T. Vetter. Efﬁcient, robust and accurate ﬁtting of a
3D morphable model. In Proc. Int. Conf. Comput. Vision, 2003.
[35] S. Romdhani and T. Vetter. Estimating 3D shape and texture using
pixel intensity, edges, specular highlights, texture constraints and a
prior. In Proc. Conf. Comput. Vision Pattern Recognition, volume 2,
pages 986–993, 2005.

[36] C. Sagonas, E. Antonakos, G. Tzimiropoulos, S. Zafeiriou, and
M. Pantic. 300 faces in-the-wild challenge: Database and results.
Image and Vision Computing, 2015.

[37] H. Tang, Y. Hu, Y. Fu, M. Hasegawa-Johnson, and T. S. Huang. Real-
time conversion from a single 2d face image to a 3D text-driven
emotive audio-visual avatar. In Int. Conf. on Multimedia and Expo,
pages 1205–1208. IEEE, 2008.

[38] A. Tran, T. Hassner, I. Masi, and G. Medioni. Regressing robust and
discriminative 3D morphable models with a very deep neural network.
In Proc. Conf. Comput. Vision Pattern Recognition, 2017.

[39] A. T. Tran, T. Hassner, I. Masi, E. Paz, Y. Nirkin, and G. Medioni.
arXiv

Extreme 3D face reconstruction: Looking past occlusions.
preprint arXiv:1712.05083, 2017.

[40] L. Wolf, T. Hassner, and I. Maoz. Face recognition in unconstrained
videos with matched background similarity. In Proc. Conf. Comput.
Vision Pattern Recognition, 2011.

[41] Y. Wu, T. Hassner, K. Kim, G. Medioni, and P. Natarajan. Facial
landmark detection with tweaked convolutional neural networks. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 2017.
[42] F. Yang, J. Wang, E. Shechtman, L. Bourdev, and D. Metaxas.
Expression ﬂow for 3D-aware face component transfer. ACM Trans.
on Graphics, 30(4):60, 2011.

[43] Z. Yang and R. Nevatia. A multi-scale cascade fully convolutional
In Int. Conf. on Pattern Recognition, pages

network face detector.
633–638, 2016.

[44] D. Yi, Z. Lei, S. Liao,
representation from scratch.
2014.
CASIA-WebFace-Database.html.

Learning face
arXiv preprint arXiv:1411.7923,
Available: http://www.cbsr.ia.ac.cn/english/

and S. Z. Li.

[45] L. Yin, X. Chen, Y. Sun, T. Worm, and M. Reale. A high-resolution
3d dynamic facial expression database. In Automatic Face & Gesture
Recognition, 2008. FG’08. 8th IEEE International Conference on,
pages 1–6. IEEE, 2008.

[46] A. Zadeh, T. Baltruˇsaitis, and L.-P. Morency. Convolutional experts
constrained local model for facial landmark detection. In Proc. Conf.
Comput. Vision Pattern Recognition Workshops, 2017.

[47] S. Zafeiriou, A. Papaioannou, I. Kotsia, M. Nicolaou, and G. Zhao.
In Proc. Conf. Comput. Vision Pattern

Facial affect“in-the-wild”.
Recognition Workshops, pages 36–47, 2016.

[48] K. Zhang, L. Tan, Z. Li, and Y. Qiao. Gender and smile classiﬁcation
In Proc. Conf. Comput.

using deep convolutional neural networks.
Vision Pattern Recognition Workshops, pages 34–38, 2016.

[49] X. Zhu, Z. Lei, X. Liu, H. Shi, and S. Li. Face alignment across
In Proc. Conf. Comput. Vision Pattern

large poses: A 3D solution.
Recognition, Las Vegas, NV, June 2016.

[50] X. Zhu, Z. Lei, J. Yan, D. Yi, and S. Z. Li. High-ﬁdelity pose and
In Proc.

expression normalization for face recognition in the wild.
Conf. Comput. Vision Pattern Recognition, pages 787–796, 2015.

