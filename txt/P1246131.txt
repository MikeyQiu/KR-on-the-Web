JSS

Journal of Statistical Software

MMMMMM YYYY, Volume VV, Issue II.

http://www.jstatsoft.org/

5
1
0
2
 
p
e
S
 
2
2
 
 
]

O
C

.
t
a
t
s
[
 
 
1
v
9
5
4
6
0
.
9
0
5
1
:
v
i
X
r
a

Stochastic gradient descent methods
for estimation with large data sets

Dustin Tran
Harvard University

Panos Toulis
Harvard University

Edoardo M. Airoldi
Harvard University

Abstract

We develop methods for parameter estimation in settings with large-scale data sets,
where traditional methods are no longer tenable. Our methods rely on stochastic approx-
imations, which are computationally eﬃcient as they maintain one iterate as a parameter
estimate, and successively update that iterate based on a single data point. When the
update is based on a noisy gradient, the stochastic approximation is known as standard
stochastic gradient descent, which has been fundamental in modern applications with large
data sets. Additionally, our methods are numerically stable because they employ implicit
updates of the iterates. Intuitively, an implicit update is a shrinked version of a stan-
dard one, where the shrinkage factor depends on the observed Fisher information at the
corresponding data point. This shrinkage prevents numerical divergence of the iterates,
which can be caused either by excess noise or outliers. Our sgd package in R oﬀers the
most extensive and robust implementation of stochastic gradient descent methods. We
demonstrate that sgd dominates alternative software in runtime for several estimation
problems with massive data sets. Our applications include the wide class of generalized
linear models as well as M-estimation for robust regression.

Keywords: stochastic gradient descent, implicit updates, massive data, exponential family,
generalized linear models, M-estimation.

1. Introduction

Massive data sets as well as streaming data, in which one observes only a group of data points
at a time, are becoming increasingly common in modern statistical analysis. Under the setting
of hundreds of millions of observations and hundreds or thousands of covariates (National
Research Council 2013), it becomes diﬃcult to estimate the parameters of a statistical model;
the three ideal properties are computational eﬃciency, statistical optimality, and numerical
stability, and it is challenging to address all three with a single estimation method.

2

Package sgd for estimation with large data sets

More formally, suppose there exists a vector of parameters θ(cid:63) ∈ Rp and that we observe
i.i.d. samples D = {xn, yn}, for n = 1, 2, . . . , N ; in the nth data point (xn, yn), the out-
come yn ∈ Rd is distributed conditional on covariates xn ∈ Rp according to a known den-
sity f (yn; xn, θ(cid:63)), and thus the log-likelihood function for the entire data set D is given by
(cid:96)(θ; D) = (cid:80)N
n=1 log f (yn; xn, θ). The task is to estimate the true parameter value θ(cid:63) when N
is inﬁnite (streaming setting), or to approximate some estimator of θ(cid:63), such as the maximum-
likelihood estimator θmle = arg maxθ∈Rp (cid:96)(θ; D), when N is ﬁnite.

Widely used methods for statistical estimation, such as Fisher scoring, the EM algorithm, and
iteratively reweighted least squares (Fisher 1925; Dempster, Laird, and Rubin 1977; Green
1984) are not feasible in such settings; either they strictly do not apply in the streaming
setting (inﬁnite N ), or they do not scale to large data (ﬁnite but large N ). Fisher scoring, for
example, requires at each iteration the inversion of a p × p matrix and evaluation of the log-
likelihood over the full data set D. This roughly yields O(N p2+(cid:15)) running time complexity,
which is prohibitive when N and p are large. In contrast, estimation with massive data sets
typically requires a running time complexity that is O(N p1−(cid:15)), i.e., that is linear in N but
sublinear in the parameter dimension p.

Such performance is achieved in general by the stochastic gradient descent (sgd) algorithm,
which was initially proposed by Sakrison (1965) as a modiﬁcation of the Robbins-Monro
It is deﬁned through the
procedure (Robbins and Monro 1951) for recursive estimation.
iteration

n = θsgd
θsgd

n−1 + γnCn∇ log f (yn; xn, θsgd

n−1).

(1)

n

We will refer to Equation 1 as sgd with explicit updates, or explicit sgd for short, because the
next iterate θsgd
can be computed immediately after the nth data point (xn, yn) is observed.
The sequence γn > 0 is the learning rate sequence, and is typically deﬁned such that nγn →
γ > 0 as n → ∞; the hyperparameter γ > 0 is ﬁxed and known as the learning rate parameter.
The sequence {Cn} is a sequence of positive-deﬁnite matrices, such that Cn → C with C
known, and is used to better condition the iteration; in the simplest case Cn = I, i.e., we
simply use the identity matrix, which results in ﬁrst-order explicit sgd.

From a computational perspective, explicit sgd is eﬃcient because it replaces the expensive
inversion of p × p matrices, as in Fisher scoring, by a scalar sequence γn > 0 and a matrix
Cn that is fast to manipulate numerically, by design. Furthermore, the log-likelihood is
evaluated at a single observation yn given xn, rather than the entire data set D, which saves
signiﬁcant computation time. From a theoretical perspective, explicit sgd is justiﬁed because
the theory of stochastic approximations (Robbins and Monro 1951, Theorem 1) implies that
θsgd
converges to a point θ∞ such that E (∇ log f (yn; xn, θ∞)) = 0. Under standard statistical
n
theory, E (∇ log f (yn; xn, θ(cid:63))) = 0, and this point is unique under typical regularity conditions
(Lehmann and Casella 1998, Theorem 5.1, p.463), such as concavity of log-likelihood; this
is true, for example, in the popular exponential family of statistical models (Brown 1986).
Therefore, θ∞ = θ(cid:63), i.e., explicit sgd converges to the true parameter value. In the ﬁnite
N setting, a similar condition holds where θsgd
approximates θmle if the nth data point in
n
Equation 1 is an unbiased sample from the total N data points; see also Toulis and Airoldi
(2015b) for a review of applications of sgd on modern machine learning applications.

Despite these theoretical guarantees, explicit sgd requires careful tuning of the hyperparam-
eter γ in the learning rate: small values of the parameter make the iteration (1) very slow

Journal of Statistical Software

3

to converge in practice, whereas large values can cause numerical divergence. Moreover, it is
known that explicit sgd is statistically ineﬃcient even when γ is correctly speciﬁed (Toulis,
Airoldi, and Rennie 2014). In particular, the amount of information loss from procedure (1)
depends on the spectral gap of the Fisher information matrix, I(θ) = −E (cid:0)∇2 log f (yn; xn, θ)(cid:1),
calculated at the true parameter value θ = θ(cid:63). A large spectral gap makes it hard, or even im-
possible, to make the learning rates large enough for fast convergence, and also small enough
for stability (Toulis and Airoldi 2015a, Section 3.5).

Motivated by these challenges, Toulis, Tran, and Airoldi (2015) introduced averaged implicit
stochastic gradient descent (ai-sgd), which is deﬁned by the procedure

(2)

(3)

(4)

n = θim
θim

n−1 + γnCn∇ log f (yn; xn, θim

n ),

¯θn = (1/n)

θim
i

.

n
(cid:88)

i=1

The ﬁrst key component of ai-sgd is the implicit update (2). Note that it is implicit because
the next iterate θim
n appears on both sides of the equation. This simple modiﬁcation of the
explicit sgd procedure oﬀers several statistical advantages. In particular, assuming a common
starting point θsgd
(cid:44) θ0, one can show through a Taylor approximation of (2) around
θ0 that the implicit update satisﬁes

n−1 = θim
n−1

∆θim

n = (I + γnCnˆI(θ0; xn, yn))−1∆θsgd

n + O(γ2

n),

where ∆θn = θn − θn−1 for both methods, I is the identity matrix, and ˆI(θ0; xn, yn) =
−∇2(cid:96)(θ0; xn, yn) is the observed Fisher information matrix at θ0 (equivalent to the Hessian of
the negative log-likelihood at θ0). Equation 4 implies that the implicit update (2) is a shrinked
version of the explicit update (1). This shrinkage makes the iterations signiﬁcantly more stable
in small-to-moderate samples, and also robust to misspeciﬁcations of the learning rate param-
eter γ (Toulis et al. 2014). The implicit update (2) also has a Bayesian interpretation, where
n is the posterior mode of a model with the standard multivariate normal N (θim
θim
n−1, γnCn)
as the prior, and f (θ; xn, yn) as the likelihood. Thus it provides an iterative form of regu-
larization. In optimization, update (2) is known as a proximal update, and corresponds to a
stochastic version of the proximal point algorithm (Rockafellar 1976). Krakowski, Mahony,
Williamson, and Warmuth (2007) and Nemirovski, Juditsky, Lan, and Shapiro (2009) have
shown that proximal methods ﬁt better in the geometry of the parameter space.

The second key component of ai-sgd is iterate averaging (3), which guarantees optimal sta-
tistical eﬃciency under fairly relaxed conditions. Ruppert (1988) and Polyak and Juditsky
(1992) ﬁrst proved that averaging of iterates can achieve statistical optimality in the stan-
dard context of stochastic approximation with explicit updates; Toulis et al. (2015) extended
this result to the implicit sgd update (2). Thus, ai-sgd is eﬀectively a recursive estimation
method that is both statistically optimal and numerically stable, while remaining applicable
to the setting of massive and/or streaming data.

In this paper we develop statistically eﬃcient sgd algorithms for generalized linear models—
extending Algorithm 1 of Toulis et al. (2014)—and also develop sgd algorithms to perform
high-dimensional M-estimation. This allows for scalable estimation of such models with mas-
sive and/or streaming data. We provide a publicly available package sgd (Tran, Toulis, and
Airoldi 2015) written in R, which implements ai-sgd, as well as other sgd variants. In Section

4

Package sgd for estimation with large data sets

2, we develop the algorithms. Section 3 contains experiments on simulated and real-world
data, in which we demonstrate the advantages of the sgd package compared to alternative
software. In Section 4, we describe the interface of sgd and implementation details for its use
in practice.

2. Algorithms

In this section we develop algorithms which implement implicit sgd and ai-sgd for gener-
alized linear models as well as M-estimation. We start by introducing an algorithm which
eﬃciently computes a generalization of implicit update (2), which is useful for the aforemen-
tioned applications.

2.1. Eﬃcient computation of implicit updates

The main diﬃculty in applying ai-sgd is the solution of the multidimensional ﬁxed point
equation for the implicit update (2). In the large class of models where the likelihood given
covariate x depends on the parameter θ only through the natural parameter η ≡ x(cid:62)θ, the
solution of the ﬁxed-point equation is computationally eﬃcient. The general result is given
in Theorem 2.1, whereas the assumption is made more precise below.
Assumption 2.1. The likelihood (cid:96)(θ; xn, yn) ≡ log f (yn; xn, θ) of parameter value θ given
data point (xn, yn) depends on θ only through the product x(cid:62)

n θ, i.e.,

(cid:96)(θ; xn, yn) ≡ (cid:96)(x(cid:62)

n θ; xn, yn).

A key implication of Assumption 2.1 is that the direction of the gradient of the log-likelihood
does not depend on the parameter value since ∇ log f (yn; xn, θ) = (cid:96)(cid:48)(x(cid:62)
n θ; xn, yn)xn, where
the latter derivative is with respect to the natural parameter x(cid:62)
n θ and with ﬁxed data xn, yn.
This property is crucial because it implies that the implicit update (2) can be performed once
a scalar value is found that will appropriately scale the gradient.

Theorem 2.1. Suppose Assumption 2.1 holds. Then the gradient for the implicit iterate θim
n
(2) is a scaled version of the gradient at the previous iterate, i.e.,

∇ log f (yn; xn, θim

n ) = sn∇ log f (yn; xn, θim

n−1).

The scalar sn ∈ R satisﬁes

snκn−1 = (cid:96)(cid:48) (cid:16)

n θim
x(cid:62)

n−1 + γnsnκn−1x(cid:62)

n Cnxn; xn, yn

(cid:17)

,

(5)

(6)

(7)

n θim

where κn−1 = (cid:96)(cid:48)(x(cid:62)
n−1; xn, yn).
Theorem 2.1 shows that the gradient ∇ log f (yn; xn, θim
n ) in the implicit update (2) is in fact
a scaled version of the gradient ∇ log f (yn; xn, θim
n−1) that would appear in update (2) if we
were applying explicit updates. Therefore, computing the implicit update reduces to ﬁnding
the scale factor sn ∈ R. See Toulis and Airoldi (2015a, Threorem 4.1) for a proof.

It is possible to regularize both explicit and implicit sgd by adding
Penalized likelihood.
a penalty to the log-likelihood. In particular, we consider the elastic net (Zou and Hastie

Journal of Statistical Software

5

2005), where for some ﬁxed α ∈ [0, 1] the penalty function is

1
2
Adding the elastic net with a regularization parameter λ ∈ R to explicit sgd is straightfor-
ward:

Pα(θ) = (1 − α)

2 + α(cid:107)θ(cid:107)1.

(cid:107)θ(cid:107)2

(8)

n = θsgd
θsgd

n−1 + γnCn(∇ log f (yn; xn, θsgd

n−1) − λ∇Pα(θsgd

n−1)),

where the gradient of the elastic net penalty is given by

∇Pα(θim

n−1) = (1 − α)θsgd

n−1 + α sign(θsgd

n−1).

Here, the operation sign(θ) is the element-wise sign operation, outputting 1 if θj > 0, −1 if
θj < 0, and 0 otherwise.
For implicit sgd the update would be

n = θim
θim

n−1 + γnCn(∇ log f (yn; xn, θim

n ) − λ∇Pα(θim

n )).

(11)

However, it is not generally possible to compute update (11). For example, Assumption
2.1 does not hold because the gradient of the log-likelihood and the gradient of the penalty
generally have two diﬀerent directions. This breaks the argument of Theorem 2.1, where the
direction of the update calculated at the next iterate θim
n is the same as the direction of the
update calculated at the previous iterate θim

n−1.

To circumvent this problem, we simply penalize the previous iterate instead of the current,
i.e., perform the update

n = θim
θim

n−1 + γnCn(∇ log f (yn; xn, θim

n ) − λ∇Pα(θim

n−1)).

Then update (12) is equivalent to

n = θim
θim

n−1 + γnCn(sn∇ log f (yn; xn, θim

n−1) − λ∇Pα(θim

n−1)),

where the scale factor sn satisﬁes
snκn−1 = (cid:96)(cid:48) (cid:16)

n θim
x(cid:62)

n−1 − γnλx(cid:62)

n Cn∇Pα(θim

n−1) + γnsnκn−1x(cid:62)

n Cnxn; xn, yn

(cid:17)

,

(14)

and where κn−1 = (cid:96)(cid:48)(x(cid:62)
identical to the proof of Theorem 2.1.

n θim

n−1; xn, yn). A proof for this case with penalized likelihoods is

Final algorithm for implicit updates. This analysis leads to Algorithm 1, which, for
models satisfying Assumption 2.1, implements the most general update (13) of implicit sgd
with conditioning matrices and penalty. This algorithm applies a root-ﬁnding procedure
solving Equation 14 at every iteration, which is fast because the equation is one-dimensional
and the search bounds for the solution are known, having a diminishing range O(γn). Indeed,
the one-dimensional search is computationally negligible in practice, as we see in Section
3.

We also note that because the implicit update (17) eﬀectively does regularization as a shrink-
age estimate (see Equation 4), the use of penalization is not as crucial in practice as it is for

(9)

(10)

(12)

(13)

6

Package sgd for estimation with large data sets

Algorithm 1 Eﬃcient implementation of implicit update (13)
1: function implicit update((cid:96)(cid:48)(·; ·), γn, θim
2:

n−1, xn, yn, Cn, Pα)

n θim

# Compute search bounds B
rn ← γn(cid:96)(cid:48) (cid:0)x(cid:62)
(cid:1)
n−1; xn, yn
B ← [0, rn]
if rn ≤ 0 then
B ← [rn, 0]

3:

4:

5:

6:

8:

7:

9:

end if
# Solve ﬁxed-point equation by a root-ﬁnding method
ξ = γn(cid:96)(cid:48)(x(cid:62)
sn ← ξ/rn
# Equivalent to implicit update (13)
return θim
n θim
12:
13: end function

n Cn∇Pα(θim

n−1 − γnλx(cid:62)

n−1) + ξx(cid:62)

n−1 + γnCn

(cid:0)sn(cid:96)(cid:48) (cid:0)x(cid:62)

n−1; xn, yn

n θim

10:

11:

n Cnxn; xn, yn), ξ ∈ B

(cid:1) xn − λ∇Pα(θim

n−1)(cid:1)

explicit updates. We make extensive experiments using Algorithm 2 and also examine this
eﬀect in Section 3.

2.2. Generalized linear models

In the family of generalized linear models (GLMs), the outcome yn ∈ R follows an exponential
family distribution conditional on xn,

yn | xn ∼ exp

(ηnyn − b(ηn))

c(yn, ψ),

ηn ≡ x(cid:62)

n θ(cid:63),

(15)

(cid:26) 1
ψ

(cid:27)

where the scalar ψ > 0 is the dispersion parameter which aﬀects the variance of the outcome,
c(·, ·) is the base measure, and b(·) is the log normalizer which ensures that the distribution
integrates to one.1 Additionally, in a GLM it is assumed that E (yn| xn) = h(x(cid:62)
n θ(cid:63)), where
h : R → R is known as the transfer function (Nelder and Wedderburn 1972; Dobson and
Barnett 2008). A simple property of GLMs is that the transfer function is the ﬁrst derivative
of the log normalizer, i.e., h(x(cid:62)

n θ), for all xn, θ.

n θ) = b(cid:48)(x(cid:62)

A straightforward implementation of explicit sgd for estimation with GLMs is

n = θsgd
θsgd

n−1 + γnCn[yn − h(x(cid:62)

n θsgd

n−1)]xn.

Similarly, the ai-sgd procedure can be written as

(16)

(17)

n = θim
θim

¯θn =

n−1 + γnCn[yn − h(x(cid:62)
n
1
(cid:88)
n

θim
n .

i=1

n θim

n )]xn,

By assumption, (cid:96)(θ; yn, xn) ∝ (x(cid:62)
n θ(cid:63)), and thus the log-likelihood depends on
parameter value θ(cid:63) only through its linear combination with covariate value xn. Additionally,

n θ(cid:63))yn − b(x(cid:62)

1We present one-dimensional outcomes for simplicity. However, our theory easily extends to multidimen-

sional outcomes. Such an extension is given, for example, in Section 2.3 on M-estimation.

Journal of Statistical Software

7

Algorithm 2 Estimation of generalized linear models with ai-sgd
1: Initialize θim
2: for n = 1, 2, . . . do
3:

n θ; xn, yn) ≡ yn − h(x(cid:62)

0 , ¯θ0

n θ)

Deﬁne (cid:96)(cid:48)(x(cid:62)
Calculate implicit update

4:

θim
n ← IMPLICIT UPDATE((cid:96)(cid:48)(·; ·), γn, θim

n−1, xn, yn, Cn, Pα)

¯θn ← n−1
n

¯θn−1 + 1

n θim
n

5:
6: end for

Var (yn|xn) = h(cid:48)(x(cid:62)
and concave, thus fulﬁlling Assumption 2.1.

n θ(cid:63))||xn||2, and thus h(cid:48) ≥ 0, which implies that (cid:96) is twice-diﬀerentiable

Penalized likelihood. As argued before, one can add the elastic penalty by applying it to
the previous estimate instead of the current. That is, for ﬁxed α ∈ [0, 1] and regularization
parameter λ ∈ R, the ai-sgd procedure for generalized linear models with elastic net is

n = θim
θim

n−1 + γnCn

[yn − h(x(cid:62)

n θim

n )]xn − λ∇Pα(θim

n−1)

(cid:16)

(cid:17)

,

¯θn =

1
n

n
(cid:88)

i=1

θim
n .

Algorithm 2 implements estimation of GLMs through ai-sgd based on updates (18).

2.3. M-Estimation

Given a data set of N observations D = {(xn, yn)} and a convex function ρ : R → R+, the
M-estimator is deﬁned as

ˆθm = arg min
θ∈Rp

ρ(yn − x(cid:62)

n θ),

N
(cid:88)

n=1

where it is assumed yn = x(cid:62)
n θ(cid:63)+(cid:15)n, and (cid:15)n are i.i.d. zero mean-valued noise. M-estimators are
especially useful in robust statistics (Huber 1964; Huber and Ronchetti 2009), as appropriate
choice of ρ can reduce the inﬂuence of outliers in data. Recently, there has been increased in-
terest in the literature for fast approximation of M-estimators due to their robustness (Donoho
and Montanari 2013; Jain, Tewari, and Kar 2014).

Typically in M-estimation, ρ is twice-diﬀerentiable around zero and

(cid:16)

E

ρ(cid:48)(yn − x(cid:62)
n

ˆθm)xn

= 0,

(cid:17)

where the expectation is over the empirical data distribution. Therefore sgd algorithms can
be applied to approximate the M-estimator ˆθm. Importantly, ρ is convex, which implies that
the conditions of Assumption 2.1 are met.

(18)

(19)

(20)

8

Package sgd for estimation with large data sets

Algorithm 3 M-estimation with ai-sgd
1: Initialize θim
2: for n = 1, 2, . . . do
3:

Deﬁne (cid:96)(cid:48)(x(cid:62)
Calculate implicit update

n θ; xn, yn) ≡ −ρ(cid:48)(yn − x(cid:62)

0 , ¯θ0

4:

n θ)

¯θn ← n−1
n

¯θn−1 + 1

n θim
n

5:
6: end for

θim
n ← IMPLICIT UPDATE((cid:96)(cid:48)(·; ·), γn, θim

n−1, xn, yn, Cn, Pα)

The ai-sgd procedure for approximating M-estimators is

n = θim
θim

¯θn =

n−1 + γnCn[ρ(cid:48)(yn − x(cid:62)
n
1
(cid:88)
n

θim
n .

i=1

n θim

n )]xn,

(21)

(22)

An outline of the procedure is given in Algorithm 3. As before, Algorithm 3 also includes the
optional use of a sequence of conditioning matrices Cn and a penalty function Pα. The use
of penalization has particularly been considered as a way to merge the robustness properties
given by a choice of ρ with sparsity, e.g, through lasso (Owen 2007; Lambert-Lacroix, Zwald
et al. 2011; Li, Peng, and Zhu 2011).

It is also typical to assume that the density of (cid:15)n is symmetric around zero. Therefore, it
also holds E (cid:0)ρ(cid:48)(yn − x(cid:62)
(cid:1) = 0, where the expectation is over the true data distribution.
Hence sgd procedures can be used to estimate θ(cid:63) in the case of an inﬁnite stream of obser-
vations (N = ∞). We write Algorithm 3 for the case of ﬁnite N , but it is trivial to adapt the
procedure to inﬁnite N .

n θ(cid:63))xn

3. Experiments

In this section, we compare the sgd methods implemented in the sgd package, such as ex-
plicit sgd and ai-sgd, with standard, deterministic optimization methods that are widely
used in statistical practice, such as glmnet, biglm, and speedglm. We demonstrate in both
massive and streaming data settings that standard methods are not applicable, and further-
more that sgd methods outperform such methods upon orders of magnitude in runtime and
convergence.

As standard methods are not competitive, we also compare the proposed sgd methods to
each other, e.g., comparing ai-sgd to explicit sgd, across a wide range of learning rate
speciﬁcations, including adaptive speciﬁcations such as AdaGrad (Duchi, Hazan, and Singer
2011) and RMSProp (Tieleman and Hinton 2012); more details on the speciﬁcations which
are available in sgd are given in Section 4.3.

All timings are carried out on a general-purpose 2.6 GHz Intel Core i5 processor, and are
reported for various algorithms which reach a thresholded L2 distance to the true parameter
value.

Journal of Statistical Software

9

3.1. Linear regression with the lasso

We follow an experiment used in benchmarking the glmnet package (Friedman, Hastie, and
Tibshirani 2010, Section 5.1), which ﬁts GLMs with the elastic net penalty over a regulariza-
tion path. As glmnet was shown to outperform related software such as elasticnet (Zou and
Hastie 2012) and lars (Hastie and Efron 2013), we compare sgd strictly to glmnet. The design
matrix X with N observations and p predictors is generated from a normal distribution such
that each pair of predictors Xj, Xj(cid:48) has the same correlation ρ. Each of the N outcomes yn,
n = 1, 2, . . . N , is deﬁned as

yn = x(cid:62)

n θ(cid:63) + k(cid:15)n,

(23)

where θ(cid:63)j = (−1)j exp(−2(j − 1)/20) so that the elements of the true parameter value θ(cid:63) have
alternating signs and are exponentially decreasing. The noise (cid:15)n is distributed as a standard
normal, (cid:15) ∼ N (0, 1), and k is chosen so that the signal-to-noise ratio is equal to 3.0. We run
glmnet with “covariance updates”, which takes advantage of sparse updates in the parameter
space to reduce the complexity of O(N p) calculations per iteration. It performs better in our
experiments than the “naive update” also considered in Friedman et al. (2010).

Table 1 outlines results for a combination of triplets (N, p, ρ), ranging from N = 1, 000 ob-
servations and scaling up to N = 10 million. glmnet is seen to be competitive with sgd
procedures under the setting of N = 1, 000 observations, and in fact glmnet slightly outper-
forms sgd algorithms for lower dimensions of N and p. It is in any higher dimensional setting
where sgd strictly dominates glmnet, as seen in the table where for example, with N = 50, 000
and p = 10, 000, sgd is orders of magnitude faster.

Furthermore, glmnet is restricted by the memory limitations of computer hardware. For
example, simulations with 100, 000 observations and 10, 000 features require 8 GB in memory
for simply storing the data, and more is required for parameter storage and computational
overhead. For the sgd package, we simply stream the data points using bigmemory, which
requires less than 500 MB of RAM for all our experiments, a 16-fold decrease in memory
requirements. This is not possible for glmnet in either the case of real streaming data, or
simply as a way to remove memory bottlenecks. In principle, gradient descent algorithms
such as glmnet can read and destroy data memory from disk as it loops over the full data
set; however, this is impractical as it requires such an expensive memory access at each
iteration.

We now compare the sgd algorithms. For small dimensional problems, explicit sgd achieves
faster runtime than ai-sgd as it does not require a one-dimensional search following Algorithm
1. However, in high dimensions and high correlations, it becomes extremely diﬃcult for
explicit sgd to even converge for this toy linear model. It is sensitive to the learning rate, and
any misspeciﬁcation can cause it to diverge numerically. Thus, we were not able to obtain a
proper timing for explicit sgd in settings of either high correlation (ρ > 0.9) or high dimension
with medium correlation (ρ > 0.5). In practice one must tune the hyperparameter for explicit
sgd—thus requiring signiﬁcant computational overhead and user input—while also closely
monitoring the stochastic gradients for consideration of other numerical issues. ai-sgd on the
other hand uses additional computation per iteration, which in high dimensions is negligible
compared to the cost of a stochastic gradient update. This additional computation leads to
signiﬁcantly more robust updates and faster convergence.

10

Package sgd for estimation with large data sets

0

0.1

0.9

0.95

Correlation
0.2

0.5

N = 1, 000 p = 100 (sec)

sgd(method="ai-sgd")
sgd(method="sgd")
glmnet

0.03
0.02
0.02

0.03
0.02
0.02

0.03
0.02
0.02

0.03
0.02
0.02

0.04
0.03
0.02

0.34
0.03
0.03

N = 10, 000 p = 1, 000 (sec)

sgd(method="ai-sgd")
sgd(method="sgd")
glmnet

1.81
2.78
6.60

1.65
2.90
7.76

1.78
2.93
8.00

1.50
2.81
7.83

1.85
–
6.50

1.83
–
6.70

N = 50, 000 p = 10, 000 (min)

sgd(method="ai-sgd")
sgd(method="sgd")
glmnet

3.12
4.83
14.58

3.51
4.86
15.28

3.43
5.23
16.29

3.26
–
15.58

3.40
–
16.54

3.38
–
16.41

N = 1, 000, 000 p = 50, 000 (min)

sgd(method="ai-sgd")
sgd(method="sgd")
glmnet

22.23
27.80
–

21.10
34.08
–

19.88
–
–

21.52
–
–

18.53
–
–

20.53
–
–

N = 10, 000, 000 p = 100, 000 (hr)

sgd(method="ai-sgd")
sgd(method="sgd")
glmnet

9.38
13.50
–

10.20
–
–

9.58
–
–

8.54
–
–

10.11
–
–

10.74
–
–

Table 1: Linear regression with the lasso. Timing (in various units) is displayed for 100
λ values, averaged over 10 runs. The ﬁrst line is sgd using ai-sgd and the second line is
sgd using explicit sgd. Omitted entries indicate failure of the algorithm; for explicit sgd it
numerically diverges, and for glmnet it could not run due to memory limitations.

3.2. Logistic regression with ridge penalty

Following benchmarks that are popular in the machine learning and optimization literature
(Xu 2011; Shamir and Zhang 2012; Bach and Moulines 2013; Schmidt, Le Roux, and Bach
2013), we perform large-scale logistic regression on four data sets:

• rcv1 (Lewis, Yang, Rose, and Li 2004): text data set in which the task is to classify
documents belonging to class ccat, where we apply preprocessing provided by Bottou
(2012).

• covtype (Blackard 1998): data set consisting of forest cover types in which the task is

to classify for one speciﬁc class among 7 forest cover types.

Journal of Statistical Software

11

description

type

covariates

training set

test set

λ

covtype
delta
rcv1
mnist

sparse
forest cover type
dense
synthetic data
text data
sparse
digit image features dense

54
500
47,152
784

464,809
450,000
781,265
60,000

116,203
50,000
23,149
10,000

10−6
10−2
10−5
10−3

Table 2: Summary of data sets and the L2 regularization parameter λ used.

• delta (Sonnenburg, Franc, Yom-Tov, and Sebag 2008): synthetic data oﬀered in the
PASCAL Large Scale Challenge. We apply the default processing oﬀered by the chal-
lenge organizers.

• mnist (Le Cun, Bottou, Bengio, and Haﬀner 1998): images of handwritten digits, where

the task is to classify digit 9 against all others.

A summary of the data sets is available in Table 3, where the number of observations are
typically on the order of several hundred thousand, and the covariates range from a few dozen
to tens of thousands. The regularization parameter λ for the ridge penalty are set according
to those used in Xu (2011).

We compare to the following three packages: biglm (Lumley 2013) and speedglm (Enea, Meiri,
and Kalimi 2015), both of which perform approximate updates using iteratively reweighted
least squares, and LiblineaR (Helleputte 2015), which is a simple wrapper to a C++ library
for regularized linear classiﬁcation. We use the stochastic dual coordinate ascent algorithm
(Shalev-Shwartz and Zhang 2013) in LiblineaR. In addition, we consider the mnlogit package
(Hasan, Zhiyu, and Mahani 2015), which implements multinomial logistic regression using the
classical technique of Newton-Raphson, and exploits iterations over intermediate data struc-
tures for fast Hessian calculations. For modest-sized problems, mnlogit is shown to be 10-50
times faster than mlogit (Croissant 2013), VGAM (Yee 2010), and the multinom function in
nnet (Venables and Ripley 2002). Finally, we also run the default function glm.fit as a base-
line. We note that mnlogit and glm.fit can be only employed for standard (unregularized)
multinomial regression, so we run them without the ridge penalty.

Table 3 outlines the runtimes for the considered packages. The two sgd algorithms are orders
of magnitude faster than its competitors on all data sets. Interestingly, biglm and speedglm
failed to run on the three real data sets when attempting to invert subsets of the data, and
only succeeded for the one synthetic data set delta. We also note that the largest data
set—rcv1—failed for the majority of algorithms: only the packages sgd and LiblineaR were
able to converge, both of which natively use stochastic gradients for computationally eﬃcient
updates. However, sgd is signiﬁcantly faster because less overhead seems to be involved in
passing data structures to perform computation in native C++.

Moreover, sgd requires O(p) memory, which is optimal in the sense that O(p) is the minimum
n . Both biglm and speedglm require O(p2) for the
required for simply storing the nth iterate θim
inversion of a p×p matrix, as do mnlogit and glm.fit. The mnlogit package also requires data
in the long format, which leads to a duplication of rows, as many entries display redundant
information. Moreover, while exploitation of the Hessian structure can help in practice (as it
outperforms glm.fit), we observe that the traditional technique of Newton-Raphson remains

12

Package sgd for estimation with large data sets

data set

sgd (ai-sgd)

sgd (sgd) biglm speedglm LiblineaR mnlogit

glm.fit

covtype
delta
rcv1
mnist

5.21
10.10
14.15
3.50

7.58
10.23
15.42
3.37

–
736.13
–
–

–
30.50
–
–

1444.78
2167.14
133.10
208.55

16.04
445.73
–
232.53

40.11
498.97
–
890.76

Table 3: Large-scale logistic regression on four data sets. Timing (in seconds) is displayed,
averaged over 10 runs. Omitted entries indicate failure of the algorithm; for biglm and
speedglm, it could not run due to inversions of singular matrices; for mnlogit it could not run
due to memory limitations.

Figure 1: Large scale logistic regression on four data sets. Each plot indicates the classiﬁca-
tion error on the test set for explicit sgd with AdaGrad, ai-sgd, averaged sgd, and explicit
sgd over a pass of the data.

Journal of Statistical Software

13

N

p

sgd (ai-sgd)

sgd (sgd) hqreg units

1,000
10,000
10,000
50,000
100,000
1,000,000
10,000,000
100,000

100
500
1,000
10,000
50,000
100,000
100,000
1,000,000

0.05
0.55
1.30
3.12
8.13
35.88
8.64
18.80

0.04
0.46
2.22
3.86
15.20
51.93
9.55
26.43

0.03
0.40
6.34
15.57
–
–
–
–

(sec)
(sec)
(sec)
(min)
(min)
(min)
(hr)
(hr)

Table 4: High-dimensional M-estimation with the Huber loss. Timing (in units given by the
last column) is displayed for 100 λ values, averaged over 10 runs. Omitted entries indicate
failure of the algorithm; for hqreg, it could not run due to memory limitations.

untenable because it still requires O(N p2) complexity per iteration in the worst case.

For demonstration, Figure 1 shows the progress of multiple sgd algorithms available in sgd (see
Section 4.2) over a pass of the data. We note that ai-sgd achieves the fastest or competitive
convergence rates, without requiring signiﬁcant tuning of parameters as the other algorithms
do; this includes popular adaptive learning rate speciﬁcations, such as explicit sgd with
AdaGrad.

3.3. M-estimation with the Huber loss

We follow an example for high-dimensional M-estimation in Donoho and Montanari (2013,
Section 2.4). Deﬁne the convex function ρ : R → R+ to be the Huber loss,

ρ(z; λ) =

(cid:40)

z2/2,
λ|z| − λ2/2,

if |z| ≤ λ,
otherwise.

Fix the thresholding parameter λ = 3, and generate the N ×p design matrix with i.i.d. entries
Xi,j ∼ N (0, 1
N ). We ﬁx the true set of parameters θ(cid:63) to be a vector randomly drawn with
ﬁxed norm (cid:107)θ(cid:63)(cid:107)2 = 6

p, and then generate outcome yn, n = 1, 2, . . . , N , as

√

yn = x(cid:62)

n θ(cid:63) + (cid:15)n.

(24)

For the distribution of errors (cid:15)n, we use Huber’s contaminated normal distribution CN(0.05, 10),
i.e., (cid:15)n ∼ 0.95z + 0.05h10, i.i.d., where z is standard normal and hx is a point mass at x.

Few alternative packages to sgd exist for high-dimensional robust estimation. We compare to
hqreg (Yi 2015), which ﬁts regularization paths for Huber loss regression with the elastic net
penalty. Note that hqreg is specialized to the Huber loss and cannot perform estimation for
the general setting of M-estimation problems considered here.

Table 4 outlines results for a combination of pairs (N, p), ranging from small problems of
N = 1, 000 observations to massive data settings of N = 10 million. We apply the elastic
net penalty with α = 0.5, which puts even weight on both the lasso and ridge components,

14

Package sgd for estimation with large data sets

Figure 2: High dimensional M-estimation with the Huber loss, for N = 100, 000 observations,
p = 10, 000 covariates, and a ﬁxed regularization parameter λ. The plots indicate the mean-
squared error across iterations (left) and time (right) for sgd algorithms. The horizontal line
displays the mean-squared error for the exact M -estimator (cid:98)θm.

and then compute a regularization path for both packages. We also include an example of
N = 100, 000 observations and p = 1, 000, 000 covariates, where there exist far more covariates
than data points; this occurs often in applications, e.g., in text analysis, bioinformatics, and
signal processing (Lustig, Donoho, Santos, and Pauly 2008; Blei 2012).

The sgd algorithms begin to outperform hqreg on the order of tens of thousands of obser-
vations, and signiﬁcantly so for larger data settings. Similar to the memory limitations of
glmnet, hqreg requires access to the full data set per iteration of its algorithm, which is in-
feasible when the data cannot be held in memory. Thus we were unable to obtain proper
timings for data sets of size greater than 50, 000 observations and 10, 000 covariates.

Figure 2 displays the progress of the sgd algorithms for the setting of N = 100, 000 observa-
tions and p = 10, 000 covariates, for a ﬁxed regularization parameter λ. For demonstration,
we run the algorithms over 10 passes of the data and thus over a total of 1 million iterations.
ai-sgd is seen to achieve a signiﬁcantly faster convergence rate than explicit sgd. We also
consider the use of adaptive schedules, here with RMSProp, as it performs the fastest among
other available learning rates (see Section 4.3). With RMSProp, the diﬀerence between the
two methods—sgd and ai-sgd—is noticeably smaller, and in fact sgd seems to converge
slightly faster. We note however that the use of sgd algorithms with RMSProp breaks sta-
tistical eﬃciency, and indeed we see this eﬀect as the mean-squared error oscillates around a
value higher than the MSE of the exact M-estimator (green line). Therefore we advocate the
use of ai-sgd with a one-dimensional learning rate, which still converges quite quickly.

Journal of Statistical Software

15

4. Interface and implementation

We now discuss the interface of sgd and various technical details that are important for its
use in practice.

4.1. Interface

The sgd package provides an intuitive and accessible set of methods for performing estimation
with large-scale data sets. At the core of the package is the function

sgd(formula, data, model, model.control, sgd.control)

The user provides a formula on the data frame data—similar to function primitives, such as
lm—and then speciﬁes the model. The model parameters are estimated using sgd methods,
which defaults to ai-sgd. The optional arguments model.control and sgd.control specify
attributes one can tweak about the model and the stochastic gradient method, respectively.
For example, given a data frame dat with response vector stored as the column y,

sgd(y ~ ., data=dat, model="lm")

ﬁts a linear model with the default speciﬁcations, e.g., ai-sgd with a one-dimensional learning
rate. Similarly,

sgd(y ~ ., data=dat, model="glm", model.control=list(family="binomial"))

ﬁts logistic regression with the default speciﬁcations. Numerous examples are available in the
package by running demo(package="sgd").

The sgd function also interfaces with data sets that are too large to ﬁt into memory or
are streaming (more details in Section 4.4), and can be run with a custom loss function if
desired.

The output of the sgd function is a sgd object, which is a light wrapper on a list which
collects quantities, such as the ﬁnal parameter estimates and convergence diagnostics. Custom
generic methods are also available for the sgd class, such as print, predict, and plot.

4.2. Stochastic gradient methods

While we describe the explicit sgd and ai-sgd algorithms in Section 2, the following stochastic
gradient methods are also implemented in sgd:

• implicit sgd: Proposed by Toulis et al. (2014) in the context of generalized linear models,
this algorithm uses the implicit update (2) and does not do any iterate averaging.

• averaged sgd: Proposed by Ruppert (1988) and Bather (1989) independently, this

algorithm uses the explicit update (1) followed by iterate averaging (3).

• classical momentum (cm): Proposed by Polyak (1964), this algorithm uses the update

vn = µvn−1 + an∇ log f (yn; xn, θn−1),
θn = θn−1 + vn,

(25)

(26)

16

Package sgd for estimation with large data sets

where µ ∈ [0, 1] is a ﬁxed momentum coeﬃcient. cm accelerates gradient descent with
a velocity vector which accumulates directions of large increase in the log-likelihood.

• Nesterov’s accelerated gradient (nag): Proposed by Nesterov (1983), this algorithm

uses the update

vn = µvn−1 + an∇ log f (yn; xn, θn−1 + µvn−1),
θn = θn−1 + vn,

(27)

(28)

where µ ∈ [0, 1] is a ﬁxed momentum coeﬃcient. nag is similar to cm but accumulates
velocity at a ”look-ahead” point θn−1 + µvn−1. This makes a partial update closer to
θn, allowing nag to change its velocity more quickly and responsively.

While all these methods are available, we recommend and apply ai-sgd as the default. It can
be seen as an eﬀective combination of the advantages from both implicit sgd and averaged
sgd (Toulis et al. 2015). The momentum-based methods cm and nag enjoy faster convergence
rates than the original explicit sgd, but oﬀer no theoretical beneﬁts against ai-sgd. Without
averaging techniques they also are statistically ineﬃcient, whereas iterate averaging can be
interpreted as an acceleration technique because larger learning rates are used. The velocity
update in nag is also a proxy for the implicit update, as its beneﬁt mostly relies on making
updates close to where the new estimate would lie.

4.3. Learning rates

We describe the available learning rates in more detail because they are critical for convergence
of SGD methods, in practice.
It is well-known (Sakrison 1965; Amari 1998; Toulis et al.
2014) that explicit sgd (1) and implicit sgd (2) have optimal statistical eﬃciency if the
learning rate sequence γn together, with the conditioning matrices Cn, approximate the inverse
Fisher information matrix I(θ(cid:63)) = −E (cid:0)∇2(cid:96)(θ(cid:63); xn, yn)(cid:1), i.e., γnCn → I(θ(cid:63))−1, in the limit.
Therefore in ﬁrst-order methods where Cn = I, the learning rate sequence acts as a scalar-
valued approximation to the optimal rescaling as it is used in Fisher scoring (Fisher 1925).
Based on this theory, the following learning rates are implemented in sgd:

• One-dimensional (Xu 2011): The learning rate is of the form

γn = γ0(1 + aγ0n)−c,

where γ0, a, c ∈ R are ﬁxed constants. For sgd algorithms without iterate averaging
and sgd algorithms with iterate averaging, Xu (2011) proved that setting c = 1 and
c = 2/3, respectively, leads to optimal statistical eﬃciency; a similar result holds for
ai-sgd (Toulis et al. 2015).

• AdaGrad (Duchi et al. 2011): Rather than specify a one-dimensional learning rate
γn ∈ R, Duchi et al. (2011) propose a diagonal conditioning matrix Cn ∈ Rp×p given by

In = In−1 + diag(∇(cid:96)(θn−1; xn, yn)∇(cid:96)(θn−1; xn, yn)(cid:62)),
Cn = η(In + (cid:15)I)−1/2,

where diag(·) extracts the diagonal entries of its matrix argument, η ∈ R is a constant, I
is the identity matrix, and (cid:15) is a ﬁxed value, typically 10−6, to prevent division by zero.

Journal of Statistical Software

17

In the limit, In is an unbiased estimate of the diagonal entries of the Fisher information,
and the proposed diagonal matrix Cn, which accumulates such curvature information,
is proven to be optimal for minimization of the regret bound.

• RMSProp (Tieleman and Hinton 2012): A learning rate which is popular in the deep
learning literature (Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov 2014;
Ranganath, Tang, Charlin, and Blei 2015; Rezende and Mohamed 2015), Tieleman and
Hinton (2012) propose the diagonal conditioning matrix Cn ∈ Rp×p given by

In = βIn−1 + (1 − β) diag(∇(cid:96)(θn−1; xn, yn)∇(cid:96)(θn−1; xn, yn)(cid:62)),
Cn = η(In + (cid:15)I)−1/2,

where β ∈ [0, 1] is the discount factor, η ∈ R is a constant, I is the identity matrix,
and (cid:15) is a ﬁxed value to prevent division by zero, as in AdaGrad. RMSProp uses a
decay in the estimate for the Fisher information by taking a weighted average, and
thus it gives more weight onto newer than older information. RMSProp aims to oﬀset
one problem AdaGrad often encounters in practice, where very large values occur for
initial estimates of In (e.g., due to poor initialization), thus slowing down the AdaGrad
procedure as it tries to accumulate enough curvature information to compensate for such
an error (Schaul, Antonoglou, and Silver 2014). RMSProp balances this by taking a
weighted average of previous and new information, and sees much empirical success. One
problem, however, is that RMSProp is no longer decaying suﬃciently quickly (Robbins
and Monro 1951; Duchi et al. 2011), and thus it has no guarantees on convergence.
Moreover, assuming convergence, the limit of the learning rate sequence is a constant,
which makes the iterates jitter around the true parameter value, ad inﬁnitum.

• Fisher: Following results on statistical eﬃciency and Fisher scoring, we propose a learn-

ing rate using a diagonal conditioning matrix Cn ∈ Rp×p given by

In = (1 − γn)In−1 + γndiag(∇(cid:96)(θn−1; xn, yn)∇(cid:96)(θn−1; xn, yn)(cid:62)),
Cn = (In + (cid:15)I)−1,

where γn ∝ 1/n, and (cid:15) is a small ﬁxed value to prevent division by zero, as in AdaGrad.
As before, In in the limit is an unbiased estimate of the diagonal Fisher information,
and Cn is adaptive to curvature information.

One critical but often unnoticed issue with AdaGrad, RMSProp, and similar adaptive sched-
ules is that they are statistically ineﬃcient: the speciﬁcation of the learning rates leads to
biased estimation of the inverse Fisher information matrix I(θ(cid:63))−1 that, as mentioned ear-
lier, is necessary for optimal statistical eﬃciency (an important exception is iterate averaging).
This leads to a suboptimal asymptotic variance for the sgd procedure. Thus we recommend
and apply the last proposed learning rate (“Fisher”) by default:
it takes advantage of the
curvature information such methods beneﬁt from, while still preserving as much statistical
eﬃciency as possible in diagonal conditioning matrices.

4.4. Software integration

For data sets that cannot be loaded into memory, we access subsets of the data using big-
memory (Kane, Emerson, and Weston 2013). This allows one to perform stochastic gradient

18

Package sgd for estimation with large data sets

descent by passing over the data loaded into RAM, and then to reload a new data set. This
naturally applies to both large data sets, e.g., on the order of dozens of gigabytes, and stream-
ing settings, in which one has access only to a subset of the (potentially inﬁnite) data at a
time.

In principle, with bigmemory the memory requirement for these stochastic gradient methods
is only a single data point and the current parameter estimate, which is the minimum O(p)
complexity for simply storing the estimate. In our implementation we use these savings to
try to load as much data into RAM as possible. This speeds up convergence in practice, as
it reduces the amount of I/O overhead; this especially becomes a signiﬁcant bottleneck when
reading many objects from disk.

For fast implementations we use Rcpp (Eddelbuettel and Fran¸cois 2011), where all algo-
rithms are written in C++ and only interface-level code is written in R. Aside from the major
computational gains, this also provides the opportunity to extend the library to other pro-
gramming languages. RcppArmadillo (Eddelbuettel and Sanderson 2014) is applied for access
to pre-optimized linear algebra routines, and BH (Eddelbuettel, Emerson, and Kane 2015) for
access to the Boost libraries. We apply template meta-programming and reusable classes in
an object-oriented framework, including concepts such as stochastic gradient methods, mod-
els, and learning rates. Such concepts make it easy for other users to develop new algorithms
and prototype them in their own research or practices.

The plotting routines adopt many features from ggplot2 (Wickham 2009), and are eﬀectively
templated ggplot objects. Our software is also robust through unit testing which follows the
paradigm from testthat (Wickham 2011).

5. Discussion

As explicit sgd has been used extensively in practice, particularly in the deep learning com-
munity, many heuristics have been proposed to solve issues that often occur. We describe
several of these issues and their proposed solutions in the literature, and compare to how our
sgd package handles them.

Overﬁtting. As sgd algorithms simply minimize a loss function evaluted over the training
data, overﬁtting is a prevalent problem as it is for all estimation methods. This is particularly
an issue in complex likelihood functions such as neural networks (see, e.g., Giles (2001); Bakker
and Heskes (2003)). Even with penalization terms that try to oﬀset the ﬁt of the parameters, it
is still diﬃcult for explicit sgd to ﬁnd the right set of hyperparameters for such regularization
without a computationally intensive search.

As a solution many practictioners adopt early stopping, which simply halts the optimization
routine before it converges. However, there is little theory on the estimates obtained from
early stopping. Most practically, it is diﬃcult to know when to stop the algorithm and how
to use it in combination with other regularization techniques, such as penalization.

Fortunately, one of the advantages of ai-sgd is that it requires less such tweaking: the implicit
update eﬀectively performs a regularization as seen from the Bayesian perspective, c.f., Section
1. We’ve also seen in practice that penalization terms do not aﬀect the ﬁnal estimates from
ai-sgd, which makes it less reliant on heuristics, such as early stopping.

Journal of Statistical Software

19

Vanishing or exploding gradients. The numerical instability of explicit sgd is a widespread
issue in practice (Bengio, Simard, and Frasconi 1994; Hochreiter 1998; Hochreiter, Bengio,
Frasconi, and Schmidhuber 2001; Toulis et al. 2014). The stochastic gradients can easily
be too large leading to divergence, and when chained through compositions of functions can
either vanish to zero, or even explode to numerically inﬁnite values; for example, Toulis et al.
(2014) demonstrate the instability of explicit sgd in a simple bivariate Poisson model, where
slight misspeciﬁcation of learning rate parameters lead to divergence.

Pascanu, Mikolov, and Bengio (2012) propose gradient clipping, which simply thresholds the
stochastic gradient if it is outside a bounded interval. Unfortunately, while it can work in
practice, it is a heuristic that breaks the key assumptions for convergence rate guarantees on
sgd algorithms. Similarly, there is no principled way to set the bounds. For ai-sgd algorithms
applied to the settings we consider in Section 2, such an issue never arises. Theoretical results
establish stability regardless of the speciﬁcation of the learning rate (Toulis and Airoldi 2015a,
Section 3), and perform well in practice, as seen in Section 3.

6. Concluding remarks

The sgd package is the most extensive implementation in R of stochastic gradient methods
for estimation with massive and/or streaming data sets. Thus, sgd broadens the capabilities
of R for estimation with modern large data sets—on the orders of hundreds of millions of
observations and hundreds of thousands of covariates—while retaining desirable statistical
properties. The software is based on solid theory of stochastic approximations, which help
guide the optimal selection of parameters, e.g., learning rates, in the underlying optimiza-
tion routines. In this paper, we show how sgd can be applied for estimation of generalized
linear models and M-estimation, which comprise a sizeable portion of estimation problems
encountered in statistical practice.

There are many software extensions that are currently in development. We are working
to interface with other high-performance computing packages, namely sqldf for faster I/O
applications with streaming data, doParallel (Analytics and Weston 2014) and Rmpi (Yu
2002) for parallel updates across environments, and gputools (Buckner, Seligman, and Wilson
2011) for eﬃcient computing with GPUs. The algorithms described here directly appeal
to asynchronous implementations, following Hogwild! (Nui, Recht, Re, and Wright 2011),
which allows for lock-free allocation of CPU cores. Sparse data structures would allow for
fast structured matrix and vector products, which occur, for example, when looping over
the covariates of a data point, and would signiﬁcantly speed up computation on sparse data
sets.

Finally, there has been little attention on, and in fact a pressing need for, model selection
and hypothesis testing in sgd procedures. We are pursuing this in light of the new statistical
challenges presented to us while developing the sgd package.

References

20

Package sgd for estimation with large data sets

Amari SI (1998). “Natural Gradient Works Eﬃciently in Learning.” Neural computation,

10(2), 251–276.

Analytics R, Weston S (2014). doParallel: Foreach Parallel Adaptor for the Parallel Pack-
age. R package version 1.0.8, URL http://CRAN.R-project.org/package=doParallel.

Bach F, Moulines E (2013). “Non-strongly-convex Smooth Stochastic Approximation with
Convergence Rate O(1/n).” In Advances in Neural Information Processing Systems, pp.
773–781.

Bakker B, Heskes T (2003). “Task Clustering and Gating for Bayesian Multitask Learning.”

The Journal of Machine Learning Research, 4, 83–99.

Bather J (1989). Stochastic Approximation: A Generalisation of the Robbins-Monro Proce-

dure, volume 89. Mathematical Sciences Institute, Cornell University.

Bengio Y, Simard P, Frasconi P (1994). “Learning Long-term Dependencies with Gradient

Descent is Diﬃcult.” Neural Networks, IEEE Transactions on, 5(2), 157–166.

Blackard J (1998). Comparison of Neural Networks and Discriminant Analysis in Predicting
Forest Cover Types. Ph.D. thesis, Department of Forest Sciences, Colorado State University.

Blei DM (2012). “Probabilistic Topic Models.” Communications of the ACM, 55(4), 77–84.

Bottou L (2012). “Stochastic Gradient Descent Tricks.” In Neural Networks: Tricks of the

Trade, volume 1, pp. 421–436.

Brown LD (1986). “Fundamentals of Statistical Exponential Families with Applications in

Statistical Decision Theory.” Lecture Notes-monograph series, pp. i–279.

Buckner J, Seligman M, Wilson J (2011). gputools: A Few GPU Enabled Functions. R

package version 0.26, URL http://CRAN.R-project.org/package=gputools.

Croissant Y (2013). mlogit: Multinomial Logit Model. R package version 0.2-4, URL http:

//CRAN.R-project.org/package=mlogit.

Dempster A, Laird N, Rubin D (1977). “Maximum Likelihood from Incomplete Data via the

EM Algorithm.” Journal of the Royal Statistical Society, Series B, 39, 1–38.

Dobson A, Barnett A (2008). An Introduction to Generalized Linear Models. Texts in Statis-

tical Science Series. CRC Press, London. ISBN 9781584889502.

Donoho D, Montanari A (2013). “High Dimensional Robust M-Estimation: Asymptotic Vari-

ance via Approximate Message Passing.” arXiv preprint arXiv:1310.7320v3.

Duchi J, Hazan E, Singer Y (2011). “Adaptive Subgradient Methods for Online Learning and
Stochastic Optimization.” The Journal of Machine Learning Research, 999999, 2121–2159.

Eddelbuettel D, Emerson JW, Kane MJ (2015). BH: Boost C++ Header Files. R package

version 1.58.0-1, URL http://CRAN.R-project.org/package=BH.

Eddelbuettel D, Fran¸cois R (2011). “Rcpp: Seamless R and C++ Integration.” Journal of

Statistical Software, 40(8), 1–18. URL http://www.jstatsoft.org/v40/i08/.

Journal of Statistical Software

21

Eddelbuettel D, Sanderson C (2014).

“RcppArmadillo: Accelerating R with hHgh-
performance C++ Linear Algebra.” Computational Statistics and Data Analysis, 71, 1054–
1063. URL http://dx.doi.org/10.1016/j.csda.2013.02.005.

Enea M, Meiri R, Kalimi T (2015). speedglm: Fitting Linear and Generalized Linear Models
to Large Data Sets. R package version 0.3, URL http://CRAN.R-project.org/package=
speedglm.

Fisher RA (1925). Statistical Methods for Research Workers. Oliver and Boyd, Edinburgh.

Friedman J, Hastie T, Tibshirani R (2010). “Regularization Paths for Generalized Linear

Models via Coordinate Descent.” Journal of Statistical Software, 27(6), 957–968.

Giles RCSLL (2001). “Overﬁtting in Neural Nets: Backpropagation, Conjugate Gradient, and

Early Stopping.” In Advances in Neural Information Processing Systems.

Green PJ (1984). “Iteratively Reweighted Least Squares for Maximum Likelihood Estimation,
and Some Robust and Resistant Alternatives.” Journal of the Royal Statistical Society.
Series B (Methodological), pp. 149–192.

Hasan A, Zhiyu W, Mahani AS (2015). mnlogit: Multinomial Logit Model. R package version

1.2.2, URL http://CRAN.R-project.org/package=mnlogit.

Hastie T, Efron B (2013).

lars: Least Angle Regression, Lasso and Forward Stagewise. R

package version 1.2, URL http://CRAN.R-project.org/package=lars.

Helleputte T (2015). LiblineaR: Linear Predictive Models Based on the LIBLINEAR C/C++

Library. R package version 1.94-2.

Hochreiter S (1998). “The Vanishing Gradient Problem During Learning Recurrent Neu-
ral Nets and Problem Solutions.” International Journal of Uncertainty, Fuzziness and
Knowledge-Based Systems, 6(02), 107–116.

Hochreiter S, Bengio Y, Frasconi P, Schmidhuber J (2001). “Gradient Flow in Recurrent Nets:

The Diﬃculty of Learning Long-term Dependencies.”

Huber P (1964). “Robust Estimation of a Location Parameter.” The Annals of Mathematical

Statistics, 35(1), 73–101.

Huber P, Ronchetti E (2009). Robust Statistics. Wiley Series in Probability and Statistics.

Jain P, Tewari A, Kar P (2014). “On Iterative Hard Thresholding Methods for High-
dimensional M-Estimation.” In Advances in Neural Information Processing Systems, pp.
685–693.

Kane MJ, Emerson J, Weston S (2013). “Scalable Strategies for Computing with Massive
Data.” Journal of Statistical Software, 55(14), 1–19. URL http://www.jstatsoft.org/
v55/i14/.

Krakowski KA, Mahony RE, Williamson RC, Warmuth MK (2007). “A Geometric View of

Non-Linear On-Line Stochastic Gradient Descent.” Author website.

22

Package sgd for estimation with large data sets

Lambert-Lacroix S, Zwald L, et al. (2011). “Robust Regression Through the Huberˆa ˘A´Zs
Criterion and Adaptive Lasso Penalty.” Electronic Journal of Statistics, 5, 1015–1053.

Le Cun Y, Bottou L, Bengio Y, Haﬀner P (1998). “Gradient-based Learning Applied to

Document Recognition.” Proceedings of IEEE, 86(11), 2278–2324.

Lehmann EL, Casella G (1998). Theory of Point Estimation, volume 31. Springer Science &

Business Media.

Lewis D, Yang Y, Rose T, Li F (2004). “RCV1: A New Benchmark Collection for Text

Categorization Research.” The Journal of Machine Learning Research, 5, 361–397.

Li G, Peng H, Zhu L (2011). “Nonconcave Penalized M-estimation with a Diverging Number

of Parameters.” Statistica Sinica, 21(1), 391.

Lumley T (2013). biglm: Bounded Memory Linear and Generalized Linear Models. R package

version 0.9-1, URL http://CRAN.R-project.org/package=biglm.

Lustig M, Donoho DL, Santos JM, Pauly JM (2008). “Compressed Sensing MRI.” Signal

Processing Magazine, IEEE, 25(2), 72–82.

National Research Council (2013). Frontiers in Massive Data Analysis. The National

Academies Press, Washington, DC.

Nelder J, Wedderburn R (1972). “Generalized Linear Models.” Journal of the Royal Statistical

Society. Series A (General), pp. 370–384.

Nemirovski A, Juditsky A, Lan G, Shapiro A (2009). “Robust Stochastic Approximation
Approach to Stochastic Programming.” SIAM Journal on Optimization, 19(4), 1574–1609.

Nesterov Y (1983). “A Method of Solving a Convex Programming Problem with Convergence

Rate O(1/k2).” In Soviet Mathematics Doklady, volume 27, pp. 372–376.

Nui F, Recht B, Re C, Wright SJ (2011). “Hogwild!: A Lock-Free Approach to Parallelizing
Stochastic Gradient Descent.” In Advances in Neural Information Processing Systems.

Owen AB (2007). “A Robust Hybrid of Lasso and Ridge Regression.” Contemporary Mathe-

matics, 443, 59–72.

Pascanu R, Mikolov T, Bengio Y (2012). “On the Diﬃculty of Training Recurrent Neural

Networks.” arXiv preprint arXiv:1211.5063.

Polyak BT (1964). “Some Methods of Speeding Up the Convergence of Iteration Methods.”

USSR Computational Mathematics and Mathematical Physics, 4(5), 1–17.

Polyak BT, Juditsky AB (1992). “Acceleration of Stochastic Approximation by Averaging.”

SIAM Journal on Control and Optimization, 30(4), 838–855.

Ranganath R, Tang L, Charlin L, Blei DM (2015). “Deep Exponential Families.” In Artiﬁcial

Intelligence and Statistics.

Rezende DJ, Mohamed S (2015). “Variational Inference with Normalizing Flows.” In Inter-

national Conference on Machine Learning.

Journal of Statistical Software

23

Robbins H, Monro S (1951). “A Stochastic Approximation Method.” The Annals of Mathe-

matical Statistics, pp. 400–407.

Rockafellar RT (1976). “Monotone Operators and the Proximal Point Algorithm.” SIAM

journal on control and optimization, 14(5), 877–898.

Ruppert D (1988). “Eﬃcient Estimations from a Slowly Convergent Robbins-Monro Process.”

Technical report, Cornell University Operations Research and Industrial Engineering.

Sakrison DJ (1965). “Eﬃcient Recursive Estimation; Application to Estimating the Parame-
ters of a Covariance Function.” International Journal of Engineering Science, 3(4), 461–483.

Schaul T, Antonoglou I, Silver D (2014). “Unit Tests for Stochastic Optimization.”

Schmidt M, Le Roux N, Bach F (2013). “Minimizing Finite Sums with the Stochastic Average

Gradient.” Technical report, HAL 00860051.

Shalev-Shwartz S, Zhang T (2013). “Stochastic Dual Coordinate Ascent Methods for Regu-

larized Loss.” The Journal of Machine Learning Research, 14(1), 567–599.

Shamir O, Zhang T (2012). “Stochastic Gradient Descent for Non-smooth Optimization:
Convergence Results and Optimal Averaging Schemes.” arXiv preprint arXiv:1212.1824.

Sonnenburg S, Franc V, Yom-Tov E, Sebag M (2008). “Pascal Large Scale Learning Chal-

lenge.” URL http://largescale.first.fraunhofer.de.

Srivastava N, Hinton G, Krizhevsky A, Sutskever I, Salakhutdinov R (2014). “Dropout:
A Simple Way to Prevent Neural Networks from Overﬁtting.” The Journal of Machine
Learning Research, 15(1), 1929–1958.

Tieleman T, Hinton G (2012). “Lecture 6.5—RmsProp: Divide the Gradient by a Running
Average of its Recent Magnitude.” COURSERA: Neural Networks for Machine Learning.

Toulis P, Airoldi E, Rennie J (2014). “Statistical Analysis of Stochastic Gradient Methods
for Generalized Linear Models.” In Proceedings of the 31st International Conference on
Machine Learning (ICML-14), pp. 667–675.

Toulis P, Airoldi EM (2015a). “Implicit Stochastic Gradient Descent.” arXiv preprint

arXiv:1408.2923.

Toulis P, Airoldi EM (2015b). “Scalable Estimation Strategies Based on Stochastic Approxi-
mations: Classical Results and New Insights.” Statistics and computing, 25(4), 781–795.

Toulis P, Tran D, Airoldi EM (2015). “Stability and Optimality in Stochastic Gradient De-

scent.” arXiv preprint arXiv:1505.02417v1.

Tran D, Toulis P, Airoldi EM (2015). sgd: Stochastic Gradient Descent for Scalable Estima-

tion. R package version 1.0, URL https://github.com/airoldilab/sgd.

Venables WN, Ripley BD (2002). Modern Applied Statistics with S. Fourth edition. Springer,

New York. ISBN 0-387-95457-0, URL http://www.stats.ox.ac.uk/pub/MASS4.

24

Package sgd for estimation with large data sets

Wickham H (2009). ggplot2: Elegant Graphics for Data Analysis. Springer New York. URL

http://had.co.nz/ggplot2/book.

Wickham H (2011). “testthat: Get Started with Testing.” The R Journal, 3, 5–10. URL
http://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf.

Xu W (2011). “Towards Optimal One Pass Large Scale Learning with Averaged Stochastic

Gradient Descent.” arXiv preprint arXiv:1107.2490.

Yee TW (2010). “The VGAM Package for Categorical Data Analysis.” Journal of Statistical

Software, 32(10), 1–34.

Yi C (2015). hqreg: Regularization Paths for Huber Loss Regression and Quantile Regression
Penalized by Lasso or Elastic-Net. R package version 1.0, URL http://CRAN.R-project.
org/package=hqreg.

Yu H (2002). “Rmpi: Parallel Statistical Computing in R.” R News, 2(2), 10–14. URL

http://cran.r-project.org/doc/Rnews/Rnews_2002-2.pdf.

Zou H, Hastie T (2005). “Regularization and Variable Selection via the Elastic Net.” Journal

of the Royal Statistical Society: Series B (Statistical Methodology), 67(2), 301–320.

Zou H, Hastie T (2012). elasticnet: Elastic-Net for Sparse Estimation and Sparse PCA. R

package version 1.1, URL http://CRAN.R-project.org/package=elasticnet.

Aﬃliation:

Dustin Tran, Panos Toulis, Edoardo M. Airoldi
Department of Statistics
Harvard University
1 Oxford Street, Cambridge, MA 02138, USA
E-mail: dtran@g.harvard.edu, ptoulis@fas.harvard.edu, airoldi@fas.harvard.edu

Journal of Statistical Software
published by the American Statistical Association

Volume VV, Issue II
MMMMMM YYYY

http://www.jstatsoft.org/
http://www.amstat.org/

Submitted: yyyy-mm-dd

Accepted: yyyy-mm-dd

JSS

Journal of Statistical Software

MMMMMM YYYY, Volume VV, Issue II.

http://www.jstatsoft.org/

5
1
0
2
 
p
e
S
 
2
2
 
 
]

O
C

.
t
a
t
s
[
 
 
1
v
9
5
4
6
0
.
9
0
5
1
:
v
i
X
r
a

Stochastic gradient descent methods
for estimation with large data sets

Dustin Tran
Harvard University

Panos Toulis
Harvard University

Edoardo M. Airoldi
Harvard University

Abstract

We develop methods for parameter estimation in settings with large-scale data sets,
where traditional methods are no longer tenable. Our methods rely on stochastic approx-
imations, which are computationally eﬃcient as they maintain one iterate as a parameter
estimate, and successively update that iterate based on a single data point. When the
update is based on a noisy gradient, the stochastic approximation is known as standard
stochastic gradient descent, which has been fundamental in modern applications with large
data sets. Additionally, our methods are numerically stable because they employ implicit
updates of the iterates. Intuitively, an implicit update is a shrinked version of a stan-
dard one, where the shrinkage factor depends on the observed Fisher information at the
corresponding data point. This shrinkage prevents numerical divergence of the iterates,
which can be caused either by excess noise or outliers. Our sgd package in R oﬀers the
most extensive and robust implementation of stochastic gradient descent methods. We
demonstrate that sgd dominates alternative software in runtime for several estimation
problems with massive data sets. Our applications include the wide class of generalized
linear models as well as M-estimation for robust regression.

Keywords: stochastic gradient descent, implicit updates, massive data, exponential family,
generalized linear models, M-estimation.

1. Introduction

Massive data sets as well as streaming data, in which one observes only a group of data points
at a time, are becoming increasingly common in modern statistical analysis. Under the setting
of hundreds of millions of observations and hundreds or thousands of covariates (National
Research Council 2013), it becomes diﬃcult to estimate the parameters of a statistical model;
the three ideal properties are computational eﬃciency, statistical optimality, and numerical
stability, and it is challenging to address all three with a single estimation method.

2

Package sgd for estimation with large data sets

More formally, suppose there exists a vector of parameters θ(cid:63) ∈ Rp and that we observe
i.i.d. samples D = {xn, yn}, for n = 1, 2, . . . , N ; in the nth data point (xn, yn), the out-
come yn ∈ Rd is distributed conditional on covariates xn ∈ Rp according to a known den-
sity f (yn; xn, θ(cid:63)), and thus the log-likelihood function for the entire data set D is given by
(cid:96)(θ; D) = (cid:80)N
n=1 log f (yn; xn, θ). The task is to estimate the true parameter value θ(cid:63) when N
is inﬁnite (streaming setting), or to approximate some estimator of θ(cid:63), such as the maximum-
likelihood estimator θmle = arg maxθ∈Rp (cid:96)(θ; D), when N is ﬁnite.

Widely used methods for statistical estimation, such as Fisher scoring, the EM algorithm, and
iteratively reweighted least squares (Fisher 1925; Dempster, Laird, and Rubin 1977; Green
1984) are not feasible in such settings; either they strictly do not apply in the streaming
setting (inﬁnite N ), or they do not scale to large data (ﬁnite but large N ). Fisher scoring, for
example, requires at each iteration the inversion of a p × p matrix and evaluation of the log-
likelihood over the full data set D. This roughly yields O(N p2+(cid:15)) running time complexity,
which is prohibitive when N and p are large. In contrast, estimation with massive data sets
typically requires a running time complexity that is O(N p1−(cid:15)), i.e., that is linear in N but
sublinear in the parameter dimension p.

Such performance is achieved in general by the stochastic gradient descent (sgd) algorithm,
which was initially proposed by Sakrison (1965) as a modiﬁcation of the Robbins-Monro
It is deﬁned through the
procedure (Robbins and Monro 1951) for recursive estimation.
iteration

n = θsgd
θsgd

n−1 + γnCn∇ log f (yn; xn, θsgd

n−1).

(1)

n

We will refer to Equation 1 as sgd with explicit updates, or explicit sgd for short, because the
next iterate θsgd
can be computed immediately after the nth data point (xn, yn) is observed.
The sequence γn > 0 is the learning rate sequence, and is typically deﬁned such that nγn →
γ > 0 as n → ∞; the hyperparameter γ > 0 is ﬁxed and known as the learning rate parameter.
The sequence {Cn} is a sequence of positive-deﬁnite matrices, such that Cn → C with C
known, and is used to better condition the iteration; in the simplest case Cn = I, i.e., we
simply use the identity matrix, which results in ﬁrst-order explicit sgd.

From a computational perspective, explicit sgd is eﬃcient because it replaces the expensive
inversion of p × p matrices, as in Fisher scoring, by a scalar sequence γn > 0 and a matrix
Cn that is fast to manipulate numerically, by design. Furthermore, the log-likelihood is
evaluated at a single observation yn given xn, rather than the entire data set D, which saves
signiﬁcant computation time. From a theoretical perspective, explicit sgd is justiﬁed because
the theory of stochastic approximations (Robbins and Monro 1951, Theorem 1) implies that
θsgd
converges to a point θ∞ such that E (∇ log f (yn; xn, θ∞)) = 0. Under standard statistical
n
theory, E (∇ log f (yn; xn, θ(cid:63))) = 0, and this point is unique under typical regularity conditions
(Lehmann and Casella 1998, Theorem 5.1, p.463), such as concavity of log-likelihood; this
is true, for example, in the popular exponential family of statistical models (Brown 1986).
Therefore, θ∞ = θ(cid:63), i.e., explicit sgd converges to the true parameter value. In the ﬁnite
N setting, a similar condition holds where θsgd
approximates θmle if the nth data point in
n
Equation 1 is an unbiased sample from the total N data points; see also Toulis and Airoldi
(2015b) for a review of applications of sgd on modern machine learning applications.

Despite these theoretical guarantees, explicit sgd requires careful tuning of the hyperparam-
eter γ in the learning rate: small values of the parameter make the iteration (1) very slow

Journal of Statistical Software

3

to converge in practice, whereas large values can cause numerical divergence. Moreover, it is
known that explicit sgd is statistically ineﬃcient even when γ is correctly speciﬁed (Toulis,
Airoldi, and Rennie 2014). In particular, the amount of information loss from procedure (1)
depends on the spectral gap of the Fisher information matrix, I(θ) = −E (cid:0)∇2 log f (yn; xn, θ)(cid:1),
calculated at the true parameter value θ = θ(cid:63). A large spectral gap makes it hard, or even im-
possible, to make the learning rates large enough for fast convergence, and also small enough
for stability (Toulis and Airoldi 2015a, Section 3.5).

Motivated by these challenges, Toulis, Tran, and Airoldi (2015) introduced averaged implicit
stochastic gradient descent (ai-sgd), which is deﬁned by the procedure

(2)

(3)

(4)

n = θim
θim

n−1 + γnCn∇ log f (yn; xn, θim

n ),

¯θn = (1/n)

θim
i

.

n
(cid:88)

i=1

The ﬁrst key component of ai-sgd is the implicit update (2). Note that it is implicit because
the next iterate θim
n appears on both sides of the equation. This simple modiﬁcation of the
explicit sgd procedure oﬀers several statistical advantages. In particular, assuming a common
starting point θsgd
(cid:44) θ0, one can show through a Taylor approximation of (2) around
θ0 that the implicit update satisﬁes

n−1 = θim
n−1

∆θim

n = (I + γnCnˆI(θ0; xn, yn))−1∆θsgd

n + O(γ2

n),

where ∆θn = θn − θn−1 for both methods, I is the identity matrix, and ˆI(θ0; xn, yn) =
−∇2(cid:96)(θ0; xn, yn) is the observed Fisher information matrix at θ0 (equivalent to the Hessian of
the negative log-likelihood at θ0). Equation 4 implies that the implicit update (2) is a shrinked
version of the explicit update (1). This shrinkage makes the iterations signiﬁcantly more stable
in small-to-moderate samples, and also robust to misspeciﬁcations of the learning rate param-
eter γ (Toulis et al. 2014). The implicit update (2) also has a Bayesian interpretation, where
n is the posterior mode of a model with the standard multivariate normal N (θim
θim
n−1, γnCn)
as the prior, and f (θ; xn, yn) as the likelihood. Thus it provides an iterative form of regu-
larization. In optimization, update (2) is known as a proximal update, and corresponds to a
stochastic version of the proximal point algorithm (Rockafellar 1976). Krakowski, Mahony,
Williamson, and Warmuth (2007) and Nemirovski, Juditsky, Lan, and Shapiro (2009) have
shown that proximal methods ﬁt better in the geometry of the parameter space.

The second key component of ai-sgd is iterate averaging (3), which guarantees optimal sta-
tistical eﬃciency under fairly relaxed conditions. Ruppert (1988) and Polyak and Juditsky
(1992) ﬁrst proved that averaging of iterates can achieve statistical optimality in the stan-
dard context of stochastic approximation with explicit updates; Toulis et al. (2015) extended
this result to the implicit sgd update (2). Thus, ai-sgd is eﬀectively a recursive estimation
method that is both statistically optimal and numerically stable, while remaining applicable
to the setting of massive and/or streaming data.

In this paper we develop statistically eﬃcient sgd algorithms for generalized linear models—
extending Algorithm 1 of Toulis et al. (2014)—and also develop sgd algorithms to perform
high-dimensional M-estimation. This allows for scalable estimation of such models with mas-
sive and/or streaming data. We provide a publicly available package sgd (Tran, Toulis, and
Airoldi 2015) written in R, which implements ai-sgd, as well as other sgd variants. In Section

4

Package sgd for estimation with large data sets

2, we develop the algorithms. Section 3 contains experiments on simulated and real-world
data, in which we demonstrate the advantages of the sgd package compared to alternative
software. In Section 4, we describe the interface of sgd and implementation details for its use
in practice.

2. Algorithms

In this section we develop algorithms which implement implicit sgd and ai-sgd for gener-
alized linear models as well as M-estimation. We start by introducing an algorithm which
eﬃciently computes a generalization of implicit update (2), which is useful for the aforemen-
tioned applications.

2.1. Eﬃcient computation of implicit updates

The main diﬃculty in applying ai-sgd is the solution of the multidimensional ﬁxed point
equation for the implicit update (2). In the large class of models where the likelihood given
covariate x depends on the parameter θ only through the natural parameter η ≡ x(cid:62)θ, the
solution of the ﬁxed-point equation is computationally eﬃcient. The general result is given
in Theorem 2.1, whereas the assumption is made more precise below.
Assumption 2.1. The likelihood (cid:96)(θ; xn, yn) ≡ log f (yn; xn, θ) of parameter value θ given
data point (xn, yn) depends on θ only through the product x(cid:62)

n θ, i.e.,

(cid:96)(θ; xn, yn) ≡ (cid:96)(x(cid:62)

n θ; xn, yn).

A key implication of Assumption 2.1 is that the direction of the gradient of the log-likelihood
does not depend on the parameter value since ∇ log f (yn; xn, θ) = (cid:96)(cid:48)(x(cid:62)
n θ; xn, yn)xn, where
the latter derivative is with respect to the natural parameter x(cid:62)
n θ and with ﬁxed data xn, yn.
This property is crucial because it implies that the implicit update (2) can be performed once
a scalar value is found that will appropriately scale the gradient.

Theorem 2.1. Suppose Assumption 2.1 holds. Then the gradient for the implicit iterate θim
n
(2) is a scaled version of the gradient at the previous iterate, i.e.,

∇ log f (yn; xn, θim

n ) = sn∇ log f (yn; xn, θim

n−1).

The scalar sn ∈ R satisﬁes

snκn−1 = (cid:96)(cid:48) (cid:16)

n θim
x(cid:62)

n−1 + γnsnκn−1x(cid:62)

n Cnxn; xn, yn

(cid:17)

,

(5)

(6)

(7)

n θim

where κn−1 = (cid:96)(cid:48)(x(cid:62)
n−1; xn, yn).
Theorem 2.1 shows that the gradient ∇ log f (yn; xn, θim
n ) in the implicit update (2) is in fact
a scaled version of the gradient ∇ log f (yn; xn, θim
n−1) that would appear in update (2) if we
were applying explicit updates. Therefore, computing the implicit update reduces to ﬁnding
the scale factor sn ∈ R. See Toulis and Airoldi (2015a, Threorem 4.1) for a proof.

It is possible to regularize both explicit and implicit sgd by adding
Penalized likelihood.
a penalty to the log-likelihood. In particular, we consider the elastic net (Zou and Hastie

Journal of Statistical Software

5

2005), where for some ﬁxed α ∈ [0, 1] the penalty function is

1
2
Adding the elastic net with a regularization parameter λ ∈ R to explicit sgd is straightfor-
ward:

Pα(θ) = (1 − α)

2 + α(cid:107)θ(cid:107)1.

(cid:107)θ(cid:107)2

(8)

n = θsgd
θsgd

n−1 + γnCn(∇ log f (yn; xn, θsgd

n−1) − λ∇Pα(θsgd

n−1)),

where the gradient of the elastic net penalty is given by

∇Pα(θim

n−1) = (1 − α)θsgd

n−1 + α sign(θsgd

n−1).

Here, the operation sign(θ) is the element-wise sign operation, outputting 1 if θj > 0, −1 if
θj < 0, and 0 otherwise.
For implicit sgd the update would be

n = θim
θim

n−1 + γnCn(∇ log f (yn; xn, θim

n ) − λ∇Pα(θim

n )).

(11)

However, it is not generally possible to compute update (11). For example, Assumption
2.1 does not hold because the gradient of the log-likelihood and the gradient of the penalty
generally have two diﬀerent directions. This breaks the argument of Theorem 2.1, where the
direction of the update calculated at the next iterate θim
n is the same as the direction of the
update calculated at the previous iterate θim

n−1.

To circumvent this problem, we simply penalize the previous iterate instead of the current,
i.e., perform the update

n = θim
θim

n−1 + γnCn(∇ log f (yn; xn, θim

n ) − λ∇Pα(θim

n−1)).

Then update (12) is equivalent to

n = θim
θim

n−1 + γnCn(sn∇ log f (yn; xn, θim

n−1) − λ∇Pα(θim

n−1)),

where the scale factor sn satisﬁes
snκn−1 = (cid:96)(cid:48) (cid:16)

n θim
x(cid:62)

n−1 − γnλx(cid:62)

n Cn∇Pα(θim

n−1) + γnsnκn−1x(cid:62)

n Cnxn; xn, yn

(cid:17)

,

(14)

and where κn−1 = (cid:96)(cid:48)(x(cid:62)
identical to the proof of Theorem 2.1.

n θim

n−1; xn, yn). A proof for this case with penalized likelihoods is

Final algorithm for implicit updates. This analysis leads to Algorithm 1, which, for
models satisfying Assumption 2.1, implements the most general update (13) of implicit sgd
with conditioning matrices and penalty. This algorithm applies a root-ﬁnding procedure
solving Equation 14 at every iteration, which is fast because the equation is one-dimensional
and the search bounds for the solution are known, having a diminishing range O(γn). Indeed,
the one-dimensional search is computationally negligible in practice, as we see in Section
3.

We also note that because the implicit update (17) eﬀectively does regularization as a shrink-
age estimate (see Equation 4), the use of penalization is not as crucial in practice as it is for

(9)

(10)

(12)

(13)

6

Package sgd for estimation with large data sets

Algorithm 1 Eﬃcient implementation of implicit update (13)
1: function implicit update((cid:96)(cid:48)(·; ·), γn, θim
2:

n−1, xn, yn, Cn, Pα)

n θim

# Compute search bounds B
rn ← γn(cid:96)(cid:48) (cid:0)x(cid:62)
(cid:1)
n−1; xn, yn
B ← [0, rn]
if rn ≤ 0 then
B ← [rn, 0]

3:

4:

5:

6:

7:

9:

8:

end if
# Solve ﬁxed-point equation by a root-ﬁnding method
ξ = γn(cid:96)(cid:48)(x(cid:62)
sn ← ξ/rn
# Equivalent to implicit update (13)
return θim
n θim
12:
13: end function

n Cn∇Pα(θim

n−1 − γnλx(cid:62)

n−1) + ξx(cid:62)

n−1 + γnCn

(cid:0)sn(cid:96)(cid:48) (cid:0)x(cid:62)

n−1; xn, yn

n θim

11:

10:

n Cnxn; xn, yn), ξ ∈ B

(cid:1) xn − λ∇Pα(θim

n−1)(cid:1)

explicit updates. We make extensive experiments using Algorithm 2 and also examine this
eﬀect in Section 3.

2.2. Generalized linear models

In the family of generalized linear models (GLMs), the outcome yn ∈ R follows an exponential
family distribution conditional on xn,

yn | xn ∼ exp

(ηnyn − b(ηn))

c(yn, ψ),

ηn ≡ x(cid:62)

n θ(cid:63),

(15)

(cid:26) 1
ψ

(cid:27)

where the scalar ψ > 0 is the dispersion parameter which aﬀects the variance of the outcome,
c(·, ·) is the base measure, and b(·) is the log normalizer which ensures that the distribution
integrates to one.1 Additionally, in a GLM it is assumed that E (yn| xn) = h(x(cid:62)
n θ(cid:63)), where
h : R → R is known as the transfer function (Nelder and Wedderburn 1972; Dobson and
Barnett 2008). A simple property of GLMs is that the transfer function is the ﬁrst derivative
of the log normalizer, i.e., h(x(cid:62)

n θ), for all xn, θ.

n θ) = b(cid:48)(x(cid:62)

A straightforward implementation of explicit sgd for estimation with GLMs is

n = θsgd
θsgd

n−1 + γnCn[yn − h(x(cid:62)

n θsgd

n−1)]xn.

Similarly, the ai-sgd procedure can be written as

(16)

(17)

n = θim
θim

¯θn =

n−1 + γnCn[yn − h(x(cid:62)
n
1
(cid:88)
n

θim
n .

i=1

n θim

n )]xn,

By assumption, (cid:96)(θ; yn, xn) ∝ (x(cid:62)
n θ(cid:63)), and thus the log-likelihood depends on
parameter value θ(cid:63) only through its linear combination with covariate value xn. Additionally,

n θ(cid:63))yn − b(x(cid:62)

1We present one-dimensional outcomes for simplicity. However, our theory easily extends to multidimen-

sional outcomes. Such an extension is given, for example, in Section 2.3 on M-estimation.

Journal of Statistical Software

7

Algorithm 2 Estimation of generalized linear models with ai-sgd
1: Initialize θim
2: for n = 1, 2, . . . do
3:

n θ; xn, yn) ≡ yn − h(x(cid:62)

0 , ¯θ0

n θ)

Deﬁne (cid:96)(cid:48)(x(cid:62)
Calculate implicit update

4:

θim
n ← IMPLICIT UPDATE((cid:96)(cid:48)(·; ·), γn, θim

n−1, xn, yn, Cn, Pα)

¯θn ← n−1
n

¯θn−1 + 1

n θim
n

5:
6: end for

Var (yn|xn) = h(cid:48)(x(cid:62)
and concave, thus fulﬁlling Assumption 2.1.

n θ(cid:63))||xn||2, and thus h(cid:48) ≥ 0, which implies that (cid:96) is twice-diﬀerentiable

Penalized likelihood. As argued before, one can add the elastic penalty by applying it to
the previous estimate instead of the current. That is, for ﬁxed α ∈ [0, 1] and regularization
parameter λ ∈ R, the ai-sgd procedure for generalized linear models with elastic net is

n = θim
θim

n−1 + γnCn

[yn − h(x(cid:62)

n θim

n )]xn − λ∇Pα(θim

n−1)

(cid:16)

(cid:17)

,

¯θn =

1
n

n
(cid:88)

i=1

θim
n .

Algorithm 2 implements estimation of GLMs through ai-sgd based on updates (18).

2.3. M-Estimation

Given a data set of N observations D = {(xn, yn)} and a convex function ρ : R → R+, the
M-estimator is deﬁned as

ˆθm = arg min
θ∈Rp

ρ(yn − x(cid:62)

n θ),

N
(cid:88)

n=1

where it is assumed yn = x(cid:62)
n θ(cid:63)+(cid:15)n, and (cid:15)n are i.i.d. zero mean-valued noise. M-estimators are
especially useful in robust statistics (Huber 1964; Huber and Ronchetti 2009), as appropriate
choice of ρ can reduce the inﬂuence of outliers in data. Recently, there has been increased in-
terest in the literature for fast approximation of M-estimators due to their robustness (Donoho
and Montanari 2013; Jain, Tewari, and Kar 2014).

Typically in M-estimation, ρ is twice-diﬀerentiable around zero and

(cid:16)

E

ρ(cid:48)(yn − x(cid:62)
n

ˆθm)xn

= 0,

(cid:17)

where the expectation is over the empirical data distribution. Therefore sgd algorithms can
be applied to approximate the M-estimator ˆθm. Importantly, ρ is convex, which implies that
the conditions of Assumption 2.1 are met.

(18)

(19)

(20)

8

Package sgd for estimation with large data sets

Algorithm 3 M-estimation with ai-sgd
1: Initialize θim
2: for n = 1, 2, . . . do
3:

Deﬁne (cid:96)(cid:48)(x(cid:62)
Calculate implicit update

n θ; xn, yn) ≡ −ρ(cid:48)(yn − x(cid:62)

0 , ¯θ0

4:

n θ)

¯θn ← n−1
n

¯θn−1 + 1

n θim
n

5:
6: end for

θim
n ← IMPLICIT UPDATE((cid:96)(cid:48)(·; ·), γn, θim

n−1, xn, yn, Cn, Pα)

The ai-sgd procedure for approximating M-estimators is

n = θim
θim

¯θn =

n−1 + γnCn[ρ(cid:48)(yn − x(cid:62)
n
1
(cid:88)
n

θim
n .

i=1

n θim

n )]xn,

(21)

(22)

An outline of the procedure is given in Algorithm 3. As before, Algorithm 3 also includes the
optional use of a sequence of conditioning matrices Cn and a penalty function Pα. The use
of penalization has particularly been considered as a way to merge the robustness properties
given by a choice of ρ with sparsity, e.g, through lasso (Owen 2007; Lambert-Lacroix, Zwald
et al. 2011; Li, Peng, and Zhu 2011).

It is also typical to assume that the density of (cid:15)n is symmetric around zero. Therefore, it
also holds E (cid:0)ρ(cid:48)(yn − x(cid:62)
(cid:1) = 0, where the expectation is over the true data distribution.
Hence sgd procedures can be used to estimate θ(cid:63) in the case of an inﬁnite stream of obser-
vations (N = ∞). We write Algorithm 3 for the case of ﬁnite N , but it is trivial to adapt the
procedure to inﬁnite N .

n θ(cid:63))xn

3. Experiments

In this section, we compare the sgd methods implemented in the sgd package, such as ex-
plicit sgd and ai-sgd, with standard, deterministic optimization methods that are widely
used in statistical practice, such as glmnet, biglm, and speedglm. We demonstrate in both
massive and streaming data settings that standard methods are not applicable, and further-
more that sgd methods outperform such methods upon orders of magnitude in runtime and
convergence.

As standard methods are not competitive, we also compare the proposed sgd methods to
each other, e.g., comparing ai-sgd to explicit sgd, across a wide range of learning rate
speciﬁcations, including adaptive speciﬁcations such as AdaGrad (Duchi, Hazan, and Singer
2011) and RMSProp (Tieleman and Hinton 2012); more details on the speciﬁcations which
are available in sgd are given in Section 4.3.

All timings are carried out on a general-purpose 2.6 GHz Intel Core i5 processor, and are
reported for various algorithms which reach a thresholded L2 distance to the true parameter
value.

Journal of Statistical Software

9

3.1. Linear regression with the lasso

We follow an experiment used in benchmarking the glmnet package (Friedman, Hastie, and
Tibshirani 2010, Section 5.1), which ﬁts GLMs with the elastic net penalty over a regulariza-
tion path. As glmnet was shown to outperform related software such as elasticnet (Zou and
Hastie 2012) and lars (Hastie and Efron 2013), we compare sgd strictly to glmnet. The design
matrix X with N observations and p predictors is generated from a normal distribution such
that each pair of predictors Xj, Xj(cid:48) has the same correlation ρ. Each of the N outcomes yn,
n = 1, 2, . . . N , is deﬁned as

yn = x(cid:62)

n θ(cid:63) + k(cid:15)n,

(23)

where θ(cid:63)j = (−1)j exp(−2(j − 1)/20) so that the elements of the true parameter value θ(cid:63) have
alternating signs and are exponentially decreasing. The noise (cid:15)n is distributed as a standard
normal, (cid:15) ∼ N (0, 1), and k is chosen so that the signal-to-noise ratio is equal to 3.0. We run
glmnet with “covariance updates”, which takes advantage of sparse updates in the parameter
space to reduce the complexity of O(N p) calculations per iteration. It performs better in our
experiments than the “naive update” also considered in Friedman et al. (2010).

Table 1 outlines results for a combination of triplets (N, p, ρ), ranging from N = 1, 000 ob-
servations and scaling up to N = 10 million. glmnet is seen to be competitive with sgd
procedures under the setting of N = 1, 000 observations, and in fact glmnet slightly outper-
forms sgd algorithms for lower dimensions of N and p. It is in any higher dimensional setting
where sgd strictly dominates glmnet, as seen in the table where for example, with N = 50, 000
and p = 10, 000, sgd is orders of magnitude faster.

Furthermore, glmnet is restricted by the memory limitations of computer hardware. For
example, simulations with 100, 000 observations and 10, 000 features require 8 GB in memory
for simply storing the data, and more is required for parameter storage and computational
overhead. For the sgd package, we simply stream the data points using bigmemory, which
requires less than 500 MB of RAM for all our experiments, a 16-fold decrease in memory
requirements. This is not possible for glmnet in either the case of real streaming data, or
simply as a way to remove memory bottlenecks. In principle, gradient descent algorithms
such as glmnet can read and destroy data memory from disk as it loops over the full data
set; however, this is impractical as it requires such an expensive memory access at each
iteration.

We now compare the sgd algorithms. For small dimensional problems, explicit sgd achieves
faster runtime than ai-sgd as it does not require a one-dimensional search following Algorithm
1. However, in high dimensions and high correlations, it becomes extremely diﬃcult for
explicit sgd to even converge for this toy linear model. It is sensitive to the learning rate, and
any misspeciﬁcation can cause it to diverge numerically. Thus, we were not able to obtain a
proper timing for explicit sgd in settings of either high correlation (ρ > 0.9) or high dimension
with medium correlation (ρ > 0.5). In practice one must tune the hyperparameter for explicit
sgd—thus requiring signiﬁcant computational overhead and user input—while also closely
monitoring the stochastic gradients for consideration of other numerical issues. ai-sgd on the
other hand uses additional computation per iteration, which in high dimensions is negligible
compared to the cost of a stochastic gradient update. This additional computation leads to
signiﬁcantly more robust updates and faster convergence.

10

Package sgd for estimation with large data sets

0

0.1

0.9

0.95

Correlation
0.2

0.5

N = 1, 000 p = 100 (sec)

sgd(method="ai-sgd")
sgd(method="sgd")
glmnet

0.03
0.02
0.02

0.03
0.02
0.02

0.03
0.02
0.02

0.03
0.02
0.02

0.04
0.03
0.02

0.34
0.03
0.03

N = 10, 000 p = 1, 000 (sec)

sgd(method="ai-sgd")
sgd(method="sgd")
glmnet

1.81
2.78
6.60

1.65
2.90
7.76

1.78
2.93
8.00

1.50
2.81
7.83

1.85
–
6.50

1.83
–
6.70

N = 50, 000 p = 10, 000 (min)

sgd(method="ai-sgd")
sgd(method="sgd")
glmnet

3.12
4.83
14.58

3.51
4.86
15.28

3.43
5.23
16.29

3.26
–
15.58

3.40
–
16.54

3.38
–
16.41

N = 1, 000, 000 p = 50, 000 (min)

sgd(method="ai-sgd")
sgd(method="sgd")
glmnet

22.23
27.80
–

21.10
34.08
–

19.88
–
–

21.52
–
–

18.53
–
–

20.53
–
–

N = 10, 000, 000 p = 100, 000 (hr)

sgd(method="ai-sgd")
sgd(method="sgd")
glmnet

9.38
13.50
–

10.20
–
–

9.58
–
–

8.54
–
–

10.11
–
–

10.74
–
–

Table 1: Linear regression with the lasso. Timing (in various units) is displayed for 100
λ values, averaged over 10 runs. The ﬁrst line is sgd using ai-sgd and the second line is
sgd using explicit sgd. Omitted entries indicate failure of the algorithm; for explicit sgd it
numerically diverges, and for glmnet it could not run due to memory limitations.

3.2. Logistic regression with ridge penalty

Following benchmarks that are popular in the machine learning and optimization literature
(Xu 2011; Shamir and Zhang 2012; Bach and Moulines 2013; Schmidt, Le Roux, and Bach
2013), we perform large-scale logistic regression on four data sets:

• rcv1 (Lewis, Yang, Rose, and Li 2004): text data set in which the task is to classify
documents belonging to class ccat, where we apply preprocessing provided by Bottou
(2012).

• covtype (Blackard 1998): data set consisting of forest cover types in which the task is

to classify for one speciﬁc class among 7 forest cover types.

Journal of Statistical Software

11

description

type

covariates

training set

test set

λ

covtype
delta
rcv1
mnist

sparse
forest cover type
dense
synthetic data
text data
sparse
digit image features dense

54
500
47,152
784

464,809
450,000
781,265
60,000

116,203
50,000
23,149
10,000

10−6
10−2
10−5
10−3

Table 2: Summary of data sets and the L2 regularization parameter λ used.

• delta (Sonnenburg, Franc, Yom-Tov, and Sebag 2008): synthetic data oﬀered in the
PASCAL Large Scale Challenge. We apply the default processing oﬀered by the chal-
lenge organizers.

• mnist (Le Cun, Bottou, Bengio, and Haﬀner 1998): images of handwritten digits, where

the task is to classify digit 9 against all others.

A summary of the data sets is available in Table 3, where the number of observations are
typically on the order of several hundred thousand, and the covariates range from a few dozen
to tens of thousands. The regularization parameter λ for the ridge penalty are set according
to those used in Xu (2011).

We compare to the following three packages: biglm (Lumley 2013) and speedglm (Enea, Meiri,
and Kalimi 2015), both of which perform approximate updates using iteratively reweighted
least squares, and LiblineaR (Helleputte 2015), which is a simple wrapper to a C++ library
for regularized linear classiﬁcation. We use the stochastic dual coordinate ascent algorithm
(Shalev-Shwartz and Zhang 2013) in LiblineaR. In addition, we consider the mnlogit package
(Hasan, Zhiyu, and Mahani 2015), which implements multinomial logistic regression using the
classical technique of Newton-Raphson, and exploits iterations over intermediate data struc-
tures for fast Hessian calculations. For modest-sized problems, mnlogit is shown to be 10-50
times faster than mlogit (Croissant 2013), VGAM (Yee 2010), and the multinom function in
nnet (Venables and Ripley 2002). Finally, we also run the default function glm.fit as a base-
line. We note that mnlogit and glm.fit can be only employed for standard (unregularized)
multinomial regression, so we run them without the ridge penalty.

Table 3 outlines the runtimes for the considered packages. The two sgd algorithms are orders
of magnitude faster than its competitors on all data sets. Interestingly, biglm and speedglm
failed to run on the three real data sets when attempting to invert subsets of the data, and
only succeeded for the one synthetic data set delta. We also note that the largest data
set—rcv1—failed for the majority of algorithms: only the packages sgd and LiblineaR were
able to converge, both of which natively use stochastic gradients for computationally eﬃcient
updates. However, sgd is signiﬁcantly faster because less overhead seems to be involved in
passing data structures to perform computation in native C++.

Moreover, sgd requires O(p) memory, which is optimal in the sense that O(p) is the minimum
n . Both biglm and speedglm require O(p2) for the
required for simply storing the nth iterate θim
inversion of a p×p matrix, as do mnlogit and glm.fit. The mnlogit package also requires data
in the long format, which leads to a duplication of rows, as many entries display redundant
information. Moreover, while exploitation of the Hessian structure can help in practice (as it
outperforms glm.fit), we observe that the traditional technique of Newton-Raphson remains

12

Package sgd for estimation with large data sets

data set

sgd (ai-sgd)

sgd (sgd) biglm speedglm LiblineaR mnlogit

glm.fit

covtype
delta
rcv1
mnist

5.21
10.10
14.15
3.50

7.58
10.23
15.42
3.37

–
736.13
–
–

–
30.50
–
–

1444.78
2167.14
133.10
208.55

16.04
445.73
–
232.53

40.11
498.97
–
890.76

Table 3: Large-scale logistic regression on four data sets. Timing (in seconds) is displayed,
averaged over 10 runs. Omitted entries indicate failure of the algorithm; for biglm and
speedglm, it could not run due to inversions of singular matrices; for mnlogit it could not run
due to memory limitations.

Figure 1: Large scale logistic regression on four data sets. Each plot indicates the classiﬁca-
tion error on the test set for explicit sgd with AdaGrad, ai-sgd, averaged sgd, and explicit
sgd over a pass of the data.

Journal of Statistical Software

13

N

p

sgd (ai-sgd)

sgd (sgd) hqreg units

1,000
10,000
10,000
50,000
100,000
1,000,000
10,000,000
100,000

100
500
1,000
10,000
50,000
100,000
100,000
1,000,000

0.05
0.55
1.30
3.12
8.13
35.88
8.64
18.80

0.04
0.46
2.22
3.86
15.20
51.93
9.55
26.43

0.03
0.40
6.34
15.57
–
–
–
–

(sec)
(sec)
(sec)
(min)
(min)
(min)
(hr)
(hr)

Table 4: High-dimensional M-estimation with the Huber loss. Timing (in units given by the
last column) is displayed for 100 λ values, averaged over 10 runs. Omitted entries indicate
failure of the algorithm; for hqreg, it could not run due to memory limitations.

untenable because it still requires O(N p2) complexity per iteration in the worst case.

For demonstration, Figure 1 shows the progress of multiple sgd algorithms available in sgd (see
Section 4.2) over a pass of the data. We note that ai-sgd achieves the fastest or competitive
convergence rates, without requiring signiﬁcant tuning of parameters as the other algorithms
do; this includes popular adaptive learning rate speciﬁcations, such as explicit sgd with
AdaGrad.

3.3. M-estimation with the Huber loss

We follow an example for high-dimensional M-estimation in Donoho and Montanari (2013,
Section 2.4). Deﬁne the convex function ρ : R → R+ to be the Huber loss,

ρ(z; λ) =

(cid:40)

z2/2,
λ|z| − λ2/2,

if |z| ≤ λ,
otherwise.

Fix the thresholding parameter λ = 3, and generate the N ×p design matrix with i.i.d. entries
Xi,j ∼ N (0, 1
N ). We ﬁx the true set of parameters θ(cid:63) to be a vector randomly drawn with
ﬁxed norm (cid:107)θ(cid:63)(cid:107)2 = 6

p, and then generate outcome yn, n = 1, 2, . . . , N , as

√

yn = x(cid:62)

n θ(cid:63) + (cid:15)n.

(24)

For the distribution of errors (cid:15)n, we use Huber’s contaminated normal distribution CN(0.05, 10),
i.e., (cid:15)n ∼ 0.95z + 0.05h10, i.i.d., where z is standard normal and hx is a point mass at x.

Few alternative packages to sgd exist for high-dimensional robust estimation. We compare to
hqreg (Yi 2015), which ﬁts regularization paths for Huber loss regression with the elastic net
penalty. Note that hqreg is specialized to the Huber loss and cannot perform estimation for
the general setting of M-estimation problems considered here.

Table 4 outlines results for a combination of pairs (N, p), ranging from small problems of
N = 1, 000 observations to massive data settings of N = 10 million. We apply the elastic
net penalty with α = 0.5, which puts even weight on both the lasso and ridge components,

14

Package sgd for estimation with large data sets

Figure 2: High dimensional M-estimation with the Huber loss, for N = 100, 000 observations,
p = 10, 000 covariates, and a ﬁxed regularization parameter λ. The plots indicate the mean-
squared error across iterations (left) and time (right) for sgd algorithms. The horizontal line
displays the mean-squared error for the exact M -estimator (cid:98)θm.

and then compute a regularization path for both packages. We also include an example of
N = 100, 000 observations and p = 1, 000, 000 covariates, where there exist far more covariates
than data points; this occurs often in applications, e.g., in text analysis, bioinformatics, and
signal processing (Lustig, Donoho, Santos, and Pauly 2008; Blei 2012).

The sgd algorithms begin to outperform hqreg on the order of tens of thousands of obser-
vations, and signiﬁcantly so for larger data settings. Similar to the memory limitations of
glmnet, hqreg requires access to the full data set per iteration of its algorithm, which is in-
feasible when the data cannot be held in memory. Thus we were unable to obtain proper
timings for data sets of size greater than 50, 000 observations and 10, 000 covariates.

Figure 2 displays the progress of the sgd algorithms for the setting of N = 100, 000 observa-
tions and p = 10, 000 covariates, for a ﬁxed regularization parameter λ. For demonstration,
we run the algorithms over 10 passes of the data and thus over a total of 1 million iterations.
ai-sgd is seen to achieve a signiﬁcantly faster convergence rate than explicit sgd. We also
consider the use of adaptive schedules, here with RMSProp, as it performs the fastest among
other available learning rates (see Section 4.3). With RMSProp, the diﬀerence between the
two methods—sgd and ai-sgd—is noticeably smaller, and in fact sgd seems to converge
slightly faster. We note however that the use of sgd algorithms with RMSProp breaks sta-
tistical eﬃciency, and indeed we see this eﬀect as the mean-squared error oscillates around a
value higher than the MSE of the exact M-estimator (green line). Therefore we advocate the
use of ai-sgd with a one-dimensional learning rate, which still converges quite quickly.

Journal of Statistical Software

15

4. Interface and implementation

We now discuss the interface of sgd and various technical details that are important for its
use in practice.

4.1. Interface

The sgd package provides an intuitive and accessible set of methods for performing estimation
with large-scale data sets. At the core of the package is the function

sgd(formula, data, model, model.control, sgd.control)

The user provides a formula on the data frame data—similar to function primitives, such as
lm—and then speciﬁes the model. The model parameters are estimated using sgd methods,
which defaults to ai-sgd. The optional arguments model.control and sgd.control specify
attributes one can tweak about the model and the stochastic gradient method, respectively.
For example, given a data frame dat with response vector stored as the column y,

sgd(y ~ ., data=dat, model="lm")

ﬁts a linear model with the default speciﬁcations, e.g., ai-sgd with a one-dimensional learning
rate. Similarly,

sgd(y ~ ., data=dat, model="glm", model.control=list(family="binomial"))

ﬁts logistic regression with the default speciﬁcations. Numerous examples are available in the
package by running demo(package="sgd").

The sgd function also interfaces with data sets that are too large to ﬁt into memory or
are streaming (more details in Section 4.4), and can be run with a custom loss function if
desired.

The output of the sgd function is a sgd object, which is a light wrapper on a list which
collects quantities, such as the ﬁnal parameter estimates and convergence diagnostics. Custom
generic methods are also available for the sgd class, such as print, predict, and plot.

4.2. Stochastic gradient methods

While we describe the explicit sgd and ai-sgd algorithms in Section 2, the following stochastic
gradient methods are also implemented in sgd:

• implicit sgd: Proposed by Toulis et al. (2014) in the context of generalized linear models,
this algorithm uses the implicit update (2) and does not do any iterate averaging.

• averaged sgd: Proposed by Ruppert (1988) and Bather (1989) independently, this

algorithm uses the explicit update (1) followed by iterate averaging (3).

• classical momentum (cm): Proposed by Polyak (1964), this algorithm uses the update

vn = µvn−1 + an∇ log f (yn; xn, θn−1),
θn = θn−1 + vn,

(25)

(26)

16

Package sgd for estimation with large data sets

where µ ∈ [0, 1] is a ﬁxed momentum coeﬃcient. cm accelerates gradient descent with
a velocity vector which accumulates directions of large increase in the log-likelihood.

• Nesterov’s accelerated gradient (nag): Proposed by Nesterov (1983), this algorithm

uses the update

vn = µvn−1 + an∇ log f (yn; xn, θn−1 + µvn−1),
θn = θn−1 + vn,

(27)

(28)

where µ ∈ [0, 1] is a ﬁxed momentum coeﬃcient. nag is similar to cm but accumulates
velocity at a ”look-ahead” point θn−1 + µvn−1. This makes a partial update closer to
θn, allowing nag to change its velocity more quickly and responsively.

While all these methods are available, we recommend and apply ai-sgd as the default. It can
be seen as an eﬀective combination of the advantages from both implicit sgd and averaged
sgd (Toulis et al. 2015). The momentum-based methods cm and nag enjoy faster convergence
rates than the original explicit sgd, but oﬀer no theoretical beneﬁts against ai-sgd. Without
averaging techniques they also are statistically ineﬃcient, whereas iterate averaging can be
interpreted as an acceleration technique because larger learning rates are used. The velocity
update in nag is also a proxy for the implicit update, as its beneﬁt mostly relies on making
updates close to where the new estimate would lie.

4.3. Learning rates

We describe the available learning rates in more detail because they are critical for convergence
of SGD methods, in practice.
It is well-known (Sakrison 1965; Amari 1998; Toulis et al.
2014) that explicit sgd (1) and implicit sgd (2) have optimal statistical eﬃciency if the
learning rate sequence γn together, with the conditioning matrices Cn, approximate the inverse
Fisher information matrix I(θ(cid:63)) = −E (cid:0)∇2(cid:96)(θ(cid:63); xn, yn)(cid:1), i.e., γnCn → I(θ(cid:63))−1, in the limit.
Therefore in ﬁrst-order methods where Cn = I, the learning rate sequence acts as a scalar-
valued approximation to the optimal rescaling as it is used in Fisher scoring (Fisher 1925).
Based on this theory, the following learning rates are implemented in sgd:

• One-dimensional (Xu 2011): The learning rate is of the form

γn = γ0(1 + aγ0n)−c,

where γ0, a, c ∈ R are ﬁxed constants. For sgd algorithms without iterate averaging
and sgd algorithms with iterate averaging, Xu (2011) proved that setting c = 1 and
c = 2/3, respectively, leads to optimal statistical eﬃciency; a similar result holds for
ai-sgd (Toulis et al. 2015).

• AdaGrad (Duchi et al. 2011): Rather than specify a one-dimensional learning rate
γn ∈ R, Duchi et al. (2011) propose a diagonal conditioning matrix Cn ∈ Rp×p given by

In = In−1 + diag(∇(cid:96)(θn−1; xn, yn)∇(cid:96)(θn−1; xn, yn)(cid:62)),
Cn = η(In + (cid:15)I)−1/2,

where diag(·) extracts the diagonal entries of its matrix argument, η ∈ R is a constant, I
is the identity matrix, and (cid:15) is a ﬁxed value, typically 10−6, to prevent division by zero.

Journal of Statistical Software

17

In the limit, In is an unbiased estimate of the diagonal entries of the Fisher information,
and the proposed diagonal matrix Cn, which accumulates such curvature information,
is proven to be optimal for minimization of the regret bound.

• RMSProp (Tieleman and Hinton 2012): A learning rate which is popular in the deep
learning literature (Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov 2014;
Ranganath, Tang, Charlin, and Blei 2015; Rezende and Mohamed 2015), Tieleman and
Hinton (2012) propose the diagonal conditioning matrix Cn ∈ Rp×p given by

In = βIn−1 + (1 − β) diag(∇(cid:96)(θn−1; xn, yn)∇(cid:96)(θn−1; xn, yn)(cid:62)),
Cn = η(In + (cid:15)I)−1/2,

where β ∈ [0, 1] is the discount factor, η ∈ R is a constant, I is the identity matrix,
and (cid:15) is a ﬁxed value to prevent division by zero, as in AdaGrad. RMSProp uses a
decay in the estimate for the Fisher information by taking a weighted average, and
thus it gives more weight onto newer than older information. RMSProp aims to oﬀset
one problem AdaGrad often encounters in practice, where very large values occur for
initial estimates of In (e.g., due to poor initialization), thus slowing down the AdaGrad
procedure as it tries to accumulate enough curvature information to compensate for such
an error (Schaul, Antonoglou, and Silver 2014). RMSProp balances this by taking a
weighted average of previous and new information, and sees much empirical success. One
problem, however, is that RMSProp is no longer decaying suﬃciently quickly (Robbins
and Monro 1951; Duchi et al. 2011), and thus it has no guarantees on convergence.
Moreover, assuming convergence, the limit of the learning rate sequence is a constant,
which makes the iterates jitter around the true parameter value, ad inﬁnitum.

• Fisher: Following results on statistical eﬃciency and Fisher scoring, we propose a learn-

ing rate using a diagonal conditioning matrix Cn ∈ Rp×p given by

In = (1 − γn)In−1 + γndiag(∇(cid:96)(θn−1; xn, yn)∇(cid:96)(θn−1; xn, yn)(cid:62)),
Cn = (In + (cid:15)I)−1,

where γn ∝ 1/n, and (cid:15) is a small ﬁxed value to prevent division by zero, as in AdaGrad.
As before, In in the limit is an unbiased estimate of the diagonal Fisher information,
and Cn is adaptive to curvature information.

One critical but often unnoticed issue with AdaGrad, RMSProp, and similar adaptive sched-
ules is that they are statistically ineﬃcient: the speciﬁcation of the learning rates leads to
biased estimation of the inverse Fisher information matrix I(θ(cid:63))−1 that, as mentioned ear-
lier, is necessary for optimal statistical eﬃciency (an important exception is iterate averaging).
This leads to a suboptimal asymptotic variance for the sgd procedure. Thus we recommend
and apply the last proposed learning rate (“Fisher”) by default:
it takes advantage of the
curvature information such methods beneﬁt from, while still preserving as much statistical
eﬃciency as possible in diagonal conditioning matrices.

4.4. Software integration

For data sets that cannot be loaded into memory, we access subsets of the data using big-
memory (Kane, Emerson, and Weston 2013). This allows one to perform stochastic gradient

18

Package sgd for estimation with large data sets

descent by passing over the data loaded into RAM, and then to reload a new data set. This
naturally applies to both large data sets, e.g., on the order of dozens of gigabytes, and stream-
ing settings, in which one has access only to a subset of the (potentially inﬁnite) data at a
time.

In principle, with bigmemory the memory requirement for these stochastic gradient methods
is only a single data point and the current parameter estimate, which is the minimum O(p)
complexity for simply storing the estimate. In our implementation we use these savings to
try to load as much data into RAM as possible. This speeds up convergence in practice, as
it reduces the amount of I/O overhead; this especially becomes a signiﬁcant bottleneck when
reading many objects from disk.

For fast implementations we use Rcpp (Eddelbuettel and Fran¸cois 2011), where all algo-
rithms are written in C++ and only interface-level code is written in R. Aside from the major
computational gains, this also provides the opportunity to extend the library to other pro-
gramming languages. RcppArmadillo (Eddelbuettel and Sanderson 2014) is applied for access
to pre-optimized linear algebra routines, and BH (Eddelbuettel, Emerson, and Kane 2015) for
access to the Boost libraries. We apply template meta-programming and reusable classes in
an object-oriented framework, including concepts such as stochastic gradient methods, mod-
els, and learning rates. Such concepts make it easy for other users to develop new algorithms
and prototype them in their own research or practices.

The plotting routines adopt many features from ggplot2 (Wickham 2009), and are eﬀectively
templated ggplot objects. Our software is also robust through unit testing which follows the
paradigm from testthat (Wickham 2011).

5. Discussion

As explicit sgd has been used extensively in practice, particularly in the deep learning com-
munity, many heuristics have been proposed to solve issues that often occur. We describe
several of these issues and their proposed solutions in the literature, and compare to how our
sgd package handles them.

Overﬁtting. As sgd algorithms simply minimize a loss function evaluted over the training
data, overﬁtting is a prevalent problem as it is for all estimation methods. This is particularly
an issue in complex likelihood functions such as neural networks (see, e.g., Giles (2001); Bakker
and Heskes (2003)). Even with penalization terms that try to oﬀset the ﬁt of the parameters, it
is still diﬃcult for explicit sgd to ﬁnd the right set of hyperparameters for such regularization
without a computationally intensive search.

As a solution many practictioners adopt early stopping, which simply halts the optimization
routine before it converges. However, there is little theory on the estimates obtained from
early stopping. Most practically, it is diﬃcult to know when to stop the algorithm and how
to use it in combination with other regularization techniques, such as penalization.

Fortunately, one of the advantages of ai-sgd is that it requires less such tweaking: the implicit
update eﬀectively performs a regularization as seen from the Bayesian perspective, c.f., Section
1. We’ve also seen in practice that penalization terms do not aﬀect the ﬁnal estimates from
ai-sgd, which makes it less reliant on heuristics, such as early stopping.

Journal of Statistical Software

19

Vanishing or exploding gradients. The numerical instability of explicit sgd is a widespread
issue in practice (Bengio, Simard, and Frasconi 1994; Hochreiter 1998; Hochreiter, Bengio,
Frasconi, and Schmidhuber 2001; Toulis et al. 2014). The stochastic gradients can easily
be too large leading to divergence, and when chained through compositions of functions can
either vanish to zero, or even explode to numerically inﬁnite values; for example, Toulis et al.
(2014) demonstrate the instability of explicit sgd in a simple bivariate Poisson model, where
slight misspeciﬁcation of learning rate parameters lead to divergence.

Pascanu, Mikolov, and Bengio (2012) propose gradient clipping, which simply thresholds the
stochastic gradient if it is outside a bounded interval. Unfortunately, while it can work in
practice, it is a heuristic that breaks the key assumptions for convergence rate guarantees on
sgd algorithms. Similarly, there is no principled way to set the bounds. For ai-sgd algorithms
applied to the settings we consider in Section 2, such an issue never arises. Theoretical results
establish stability regardless of the speciﬁcation of the learning rate (Toulis and Airoldi 2015a,
Section 3), and perform well in practice, as seen in Section 3.

6. Concluding remarks

The sgd package is the most extensive implementation in R of stochastic gradient methods
for estimation with massive and/or streaming data sets. Thus, sgd broadens the capabilities
of R for estimation with modern large data sets—on the orders of hundreds of millions of
observations and hundreds of thousands of covariates—while retaining desirable statistical
properties. The software is based on solid theory of stochastic approximations, which help
guide the optimal selection of parameters, e.g., learning rates, in the underlying optimiza-
tion routines. In this paper, we show how sgd can be applied for estimation of generalized
linear models and M-estimation, which comprise a sizeable portion of estimation problems
encountered in statistical practice.

There are many software extensions that are currently in development. We are working
to interface with other high-performance computing packages, namely sqldf for faster I/O
applications with streaming data, doParallel (Analytics and Weston 2014) and Rmpi (Yu
2002) for parallel updates across environments, and gputools (Buckner, Seligman, and Wilson
2011) for eﬃcient computing with GPUs. The algorithms described here directly appeal
to asynchronous implementations, following Hogwild! (Nui, Recht, Re, and Wright 2011),
which allows for lock-free allocation of CPU cores. Sparse data structures would allow for
fast structured matrix and vector products, which occur, for example, when looping over
the covariates of a data point, and would signiﬁcantly speed up computation on sparse data
sets.

Finally, there has been little attention on, and in fact a pressing need for, model selection
and hypothesis testing in sgd procedures. We are pursuing this in light of the new statistical
challenges presented to us while developing the sgd package.

References

20

Package sgd for estimation with large data sets

Amari SI (1998). “Natural Gradient Works Eﬃciently in Learning.” Neural computation,

10(2), 251–276.

Analytics R, Weston S (2014). doParallel: Foreach Parallel Adaptor for the Parallel Pack-
age. R package version 1.0.8, URL http://CRAN.R-project.org/package=doParallel.

Bach F, Moulines E (2013). “Non-strongly-convex Smooth Stochastic Approximation with
Convergence Rate O(1/n).” In Advances in Neural Information Processing Systems, pp.
773–781.

Bakker B, Heskes T (2003). “Task Clustering and Gating for Bayesian Multitask Learning.”

The Journal of Machine Learning Research, 4, 83–99.

Bather J (1989). Stochastic Approximation: A Generalisation of the Robbins-Monro Proce-

dure, volume 89. Mathematical Sciences Institute, Cornell University.

Bengio Y, Simard P, Frasconi P (1994). “Learning Long-term Dependencies with Gradient

Descent is Diﬃcult.” Neural Networks, IEEE Transactions on, 5(2), 157–166.

Blackard J (1998). Comparison of Neural Networks and Discriminant Analysis in Predicting
Forest Cover Types. Ph.D. thesis, Department of Forest Sciences, Colorado State University.

Blei DM (2012). “Probabilistic Topic Models.” Communications of the ACM, 55(4), 77–84.

Bottou L (2012). “Stochastic Gradient Descent Tricks.” In Neural Networks: Tricks of the

Trade, volume 1, pp. 421–436.

Brown LD (1986). “Fundamentals of Statistical Exponential Families with Applications in

Statistical Decision Theory.” Lecture Notes-monograph series, pp. i–279.

Buckner J, Seligman M, Wilson J (2011). gputools: A Few GPU Enabled Functions. R

package version 0.26, URL http://CRAN.R-project.org/package=gputools.

Croissant Y (2013). mlogit: Multinomial Logit Model. R package version 0.2-4, URL http:

//CRAN.R-project.org/package=mlogit.

Dempster A, Laird N, Rubin D (1977). “Maximum Likelihood from Incomplete Data via the

EM Algorithm.” Journal of the Royal Statistical Society, Series B, 39, 1–38.

Dobson A, Barnett A (2008). An Introduction to Generalized Linear Models. Texts in Statis-

tical Science Series. CRC Press, London. ISBN 9781584889502.

Donoho D, Montanari A (2013). “High Dimensional Robust M-Estimation: Asymptotic Vari-

ance via Approximate Message Passing.” arXiv preprint arXiv:1310.7320v3.

Duchi J, Hazan E, Singer Y (2011). “Adaptive Subgradient Methods for Online Learning and
Stochastic Optimization.” The Journal of Machine Learning Research, 999999, 2121–2159.

Eddelbuettel D, Emerson JW, Kane MJ (2015). BH: Boost C++ Header Files. R package

version 1.58.0-1, URL http://CRAN.R-project.org/package=BH.

Eddelbuettel D, Fran¸cois R (2011). “Rcpp: Seamless R and C++ Integration.” Journal of

Statistical Software, 40(8), 1–18. URL http://www.jstatsoft.org/v40/i08/.

Journal of Statistical Software

21

Eddelbuettel D, Sanderson C (2014).

“RcppArmadillo: Accelerating R with hHgh-
performance C++ Linear Algebra.” Computational Statistics and Data Analysis, 71, 1054–
1063. URL http://dx.doi.org/10.1016/j.csda.2013.02.005.

Enea M, Meiri R, Kalimi T (2015). speedglm: Fitting Linear and Generalized Linear Models
to Large Data Sets. R package version 0.3, URL http://CRAN.R-project.org/package=
speedglm.

Fisher RA (1925). Statistical Methods for Research Workers. Oliver and Boyd, Edinburgh.

Friedman J, Hastie T, Tibshirani R (2010). “Regularization Paths for Generalized Linear

Models via Coordinate Descent.” Journal of Statistical Software, 27(6), 957–968.

Giles RCSLL (2001). “Overﬁtting in Neural Nets: Backpropagation, Conjugate Gradient, and

Early Stopping.” In Advances in Neural Information Processing Systems.

Green PJ (1984). “Iteratively Reweighted Least Squares for Maximum Likelihood Estimation,
and Some Robust and Resistant Alternatives.” Journal of the Royal Statistical Society.
Series B (Methodological), pp. 149–192.

Hasan A, Zhiyu W, Mahani AS (2015). mnlogit: Multinomial Logit Model. R package version

1.2.2, URL http://CRAN.R-project.org/package=mnlogit.

Hastie T, Efron B (2013).

lars: Least Angle Regression, Lasso and Forward Stagewise. R

package version 1.2, URL http://CRAN.R-project.org/package=lars.

Helleputte T (2015). LiblineaR: Linear Predictive Models Based on the LIBLINEAR C/C++

Library. R package version 1.94-2.

Hochreiter S (1998). “The Vanishing Gradient Problem During Learning Recurrent Neu-
ral Nets and Problem Solutions.” International Journal of Uncertainty, Fuzziness and
Knowledge-Based Systems, 6(02), 107–116.

Hochreiter S, Bengio Y, Frasconi P, Schmidhuber J (2001). “Gradient Flow in Recurrent Nets:

The Diﬃculty of Learning Long-term Dependencies.”

Huber P (1964). “Robust Estimation of a Location Parameter.” The Annals of Mathematical

Statistics, 35(1), 73–101.

Huber P, Ronchetti E (2009). Robust Statistics. Wiley Series in Probability and Statistics.

Jain P, Tewari A, Kar P (2014). “On Iterative Hard Thresholding Methods for High-
dimensional M-Estimation.” In Advances in Neural Information Processing Systems, pp.
685–693.

Kane MJ, Emerson J, Weston S (2013). “Scalable Strategies for Computing with Massive
Data.” Journal of Statistical Software, 55(14), 1–19. URL http://www.jstatsoft.org/
v55/i14/.

Krakowski KA, Mahony RE, Williamson RC, Warmuth MK (2007). “A Geometric View of

Non-Linear On-Line Stochastic Gradient Descent.” Author website.

22

Package sgd for estimation with large data sets

Lambert-Lacroix S, Zwald L, et al. (2011). “Robust Regression Through the Huberˆa ˘A´Zs
Criterion and Adaptive Lasso Penalty.” Electronic Journal of Statistics, 5, 1015–1053.

Le Cun Y, Bottou L, Bengio Y, Haﬀner P (1998). “Gradient-based Learning Applied to

Document Recognition.” Proceedings of IEEE, 86(11), 2278–2324.

Lehmann EL, Casella G (1998). Theory of Point Estimation, volume 31. Springer Science &

Business Media.

Lewis D, Yang Y, Rose T, Li F (2004). “RCV1: A New Benchmark Collection for Text

Categorization Research.” The Journal of Machine Learning Research, 5, 361–397.

Li G, Peng H, Zhu L (2011). “Nonconcave Penalized M-estimation with a Diverging Number

of Parameters.” Statistica Sinica, 21(1), 391.

Lumley T (2013). biglm: Bounded Memory Linear and Generalized Linear Models. R package

version 0.9-1, URL http://CRAN.R-project.org/package=biglm.

Lustig M, Donoho DL, Santos JM, Pauly JM (2008). “Compressed Sensing MRI.” Signal

Processing Magazine, IEEE, 25(2), 72–82.

National Research Council (2013). Frontiers in Massive Data Analysis. The National

Academies Press, Washington, DC.

Nelder J, Wedderburn R (1972). “Generalized Linear Models.” Journal of the Royal Statistical

Society. Series A (General), pp. 370–384.

Nemirovski A, Juditsky A, Lan G, Shapiro A (2009). “Robust Stochastic Approximation
Approach to Stochastic Programming.” SIAM Journal on Optimization, 19(4), 1574–1609.

Nesterov Y (1983). “A Method of Solving a Convex Programming Problem with Convergence

Rate O(1/k2).” In Soviet Mathematics Doklady, volume 27, pp. 372–376.

Nui F, Recht B, Re C, Wright SJ (2011). “Hogwild!: A Lock-Free Approach to Parallelizing
Stochastic Gradient Descent.” In Advances in Neural Information Processing Systems.

Owen AB (2007). “A Robust Hybrid of Lasso and Ridge Regression.” Contemporary Mathe-

matics, 443, 59–72.

Pascanu R, Mikolov T, Bengio Y (2012). “On the Diﬃculty of Training Recurrent Neural

Networks.” arXiv preprint arXiv:1211.5063.

Polyak BT (1964). “Some Methods of Speeding Up the Convergence of Iteration Methods.”

USSR Computational Mathematics and Mathematical Physics, 4(5), 1–17.

Polyak BT, Juditsky AB (1992). “Acceleration of Stochastic Approximation by Averaging.”

SIAM Journal on Control and Optimization, 30(4), 838–855.

Ranganath R, Tang L, Charlin L, Blei DM (2015). “Deep Exponential Families.” In Artiﬁcial

Intelligence and Statistics.

Rezende DJ, Mohamed S (2015). “Variational Inference with Normalizing Flows.” In Inter-

national Conference on Machine Learning.

Journal of Statistical Software

23

Robbins H, Monro S (1951). “A Stochastic Approximation Method.” The Annals of Mathe-

matical Statistics, pp. 400–407.

Rockafellar RT (1976). “Monotone Operators and the Proximal Point Algorithm.” SIAM

journal on control and optimization, 14(5), 877–898.

Ruppert D (1988). “Eﬃcient Estimations from a Slowly Convergent Robbins-Monro Process.”

Technical report, Cornell University Operations Research and Industrial Engineering.

Sakrison DJ (1965). “Eﬃcient Recursive Estimation; Application to Estimating the Parame-
ters of a Covariance Function.” International Journal of Engineering Science, 3(4), 461–483.

Schaul T, Antonoglou I, Silver D (2014). “Unit Tests for Stochastic Optimization.”

Schmidt M, Le Roux N, Bach F (2013). “Minimizing Finite Sums with the Stochastic Average

Gradient.” Technical report, HAL 00860051.

Shalev-Shwartz S, Zhang T (2013). “Stochastic Dual Coordinate Ascent Methods for Regu-

larized Loss.” The Journal of Machine Learning Research, 14(1), 567–599.

Shamir O, Zhang T (2012). “Stochastic Gradient Descent for Non-smooth Optimization:
Convergence Results and Optimal Averaging Schemes.” arXiv preprint arXiv:1212.1824.

Sonnenburg S, Franc V, Yom-Tov E, Sebag M (2008). “Pascal Large Scale Learning Chal-

lenge.” URL http://largescale.first.fraunhofer.de.

Srivastava N, Hinton G, Krizhevsky A, Sutskever I, Salakhutdinov R (2014). “Dropout:
A Simple Way to Prevent Neural Networks from Overﬁtting.” The Journal of Machine
Learning Research, 15(1), 1929–1958.

Tieleman T, Hinton G (2012). “Lecture 6.5—RmsProp: Divide the Gradient by a Running
Average of its Recent Magnitude.” COURSERA: Neural Networks for Machine Learning.

Toulis P, Airoldi E, Rennie J (2014). “Statistical Analysis of Stochastic Gradient Methods
for Generalized Linear Models.” In Proceedings of the 31st International Conference on
Machine Learning (ICML-14), pp. 667–675.

Toulis P, Airoldi EM (2015a). “Implicit Stochastic Gradient Descent.” arXiv preprint

arXiv:1408.2923.

Toulis P, Airoldi EM (2015b). “Scalable Estimation Strategies Based on Stochastic Approxi-
mations: Classical Results and New Insights.” Statistics and computing, 25(4), 781–795.

Toulis P, Tran D, Airoldi EM (2015). “Stability and Optimality in Stochastic Gradient De-

scent.” arXiv preprint arXiv:1505.02417v1.

Tran D, Toulis P, Airoldi EM (2015). sgd: Stochastic Gradient Descent for Scalable Estima-

tion. R package version 1.0, URL https://github.com/airoldilab/sgd.

Venables WN, Ripley BD (2002). Modern Applied Statistics with S. Fourth edition. Springer,

New York. ISBN 0-387-95457-0, URL http://www.stats.ox.ac.uk/pub/MASS4.

24

Package sgd for estimation with large data sets

Wickham H (2009). ggplot2: Elegant Graphics for Data Analysis. Springer New York. URL

http://had.co.nz/ggplot2/book.

Wickham H (2011). “testthat: Get Started with Testing.” The R Journal, 3, 5–10. URL
http://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf.

Xu W (2011). “Towards Optimal One Pass Large Scale Learning with Averaged Stochastic

Gradient Descent.” arXiv preprint arXiv:1107.2490.

Yee TW (2010). “The VGAM Package for Categorical Data Analysis.” Journal of Statistical

Software, 32(10), 1–34.

Yi C (2015). hqreg: Regularization Paths for Huber Loss Regression and Quantile Regression
Penalized by Lasso or Elastic-Net. R package version 1.0, URL http://CRAN.R-project.
org/package=hqreg.

Yu H (2002). “Rmpi: Parallel Statistical Computing in R.” R News, 2(2), 10–14. URL

http://cran.r-project.org/doc/Rnews/Rnews_2002-2.pdf.

Zou H, Hastie T (2005). “Regularization and Variable Selection via the Elastic Net.” Journal

of the Royal Statistical Society: Series B (Statistical Methodology), 67(2), 301–320.

Zou H, Hastie T (2012). elasticnet: Elastic-Net for Sparse Estimation and Sparse PCA. R

package version 1.1, URL http://CRAN.R-project.org/package=elasticnet.

Aﬃliation:

Dustin Tran, Panos Toulis, Edoardo M. Airoldi
Department of Statistics
Harvard University
1 Oxford Street, Cambridge, MA 02138, USA
E-mail: dtran@g.harvard.edu, ptoulis@fas.harvard.edu, airoldi@fas.harvard.edu

Journal of Statistical Software
published by the American Statistical Association

Volume VV, Issue II
MMMMMM YYYY

http://www.jstatsoft.org/
http://www.amstat.org/

Submitted: yyyy-mm-dd

Accepted: yyyy-mm-dd

JSS

Journal of Statistical Software

MMMMMM YYYY, Volume VV, Issue II.

http://www.jstatsoft.org/

5
1
0
2
 
p
e
S
 
2
2
 
 
]

O
C

.
t
a
t
s
[
 
 
1
v
9
5
4
6
0
.
9
0
5
1
:
v
i
X
r
a

Stochastic gradient descent methods
for estimation with large data sets

Dustin Tran
Harvard University

Panos Toulis
Harvard University

Edoardo M. Airoldi
Harvard University

Abstract

We develop methods for parameter estimation in settings with large-scale data sets,
where traditional methods are no longer tenable. Our methods rely on stochastic approx-
imations, which are computationally eﬃcient as they maintain one iterate as a parameter
estimate, and successively update that iterate based on a single data point. When the
update is based on a noisy gradient, the stochastic approximation is known as standard
stochastic gradient descent, which has been fundamental in modern applications with large
data sets. Additionally, our methods are numerically stable because they employ implicit
updates of the iterates. Intuitively, an implicit update is a shrinked version of a stan-
dard one, where the shrinkage factor depends on the observed Fisher information at the
corresponding data point. This shrinkage prevents numerical divergence of the iterates,
which can be caused either by excess noise or outliers. Our sgd package in R oﬀers the
most extensive and robust implementation of stochastic gradient descent methods. We
demonstrate that sgd dominates alternative software in runtime for several estimation
problems with massive data sets. Our applications include the wide class of generalized
linear models as well as M-estimation for robust regression.

Keywords: stochastic gradient descent, implicit updates, massive data, exponential family,
generalized linear models, M-estimation.

1. Introduction

Massive data sets as well as streaming data, in which one observes only a group of data points
at a time, are becoming increasingly common in modern statistical analysis. Under the setting
of hundreds of millions of observations and hundreds or thousands of covariates (National
Research Council 2013), it becomes diﬃcult to estimate the parameters of a statistical model;
the three ideal properties are computational eﬃciency, statistical optimality, and numerical
stability, and it is challenging to address all three with a single estimation method.

2

Package sgd for estimation with large data sets

More formally, suppose there exists a vector of parameters θ(cid:63) ∈ Rp and that we observe
i.i.d. samples D = {xn, yn}, for n = 1, 2, . . . , N ; in the nth data point (xn, yn), the out-
come yn ∈ Rd is distributed conditional on covariates xn ∈ Rp according to a known den-
sity f (yn; xn, θ(cid:63)), and thus the log-likelihood function for the entire data set D is given by
(cid:96)(θ; D) = (cid:80)N
n=1 log f (yn; xn, θ). The task is to estimate the true parameter value θ(cid:63) when N
is inﬁnite (streaming setting), or to approximate some estimator of θ(cid:63), such as the maximum-
likelihood estimator θmle = arg maxθ∈Rp (cid:96)(θ; D), when N is ﬁnite.

Widely used methods for statistical estimation, such as Fisher scoring, the EM algorithm, and
iteratively reweighted least squares (Fisher 1925; Dempster, Laird, and Rubin 1977; Green
1984) are not feasible in such settings; either they strictly do not apply in the streaming
setting (inﬁnite N ), or they do not scale to large data (ﬁnite but large N ). Fisher scoring, for
example, requires at each iteration the inversion of a p × p matrix and evaluation of the log-
likelihood over the full data set D. This roughly yields O(N p2+(cid:15)) running time complexity,
which is prohibitive when N and p are large. In contrast, estimation with massive data sets
typically requires a running time complexity that is O(N p1−(cid:15)), i.e., that is linear in N but
sublinear in the parameter dimension p.

Such performance is achieved in general by the stochastic gradient descent (sgd) algorithm,
which was initially proposed by Sakrison (1965) as a modiﬁcation of the Robbins-Monro
It is deﬁned through the
procedure (Robbins and Monro 1951) for recursive estimation.
iteration

n = θsgd
θsgd

n−1 + γnCn∇ log f (yn; xn, θsgd

n−1).

(1)

n

We will refer to Equation 1 as sgd with explicit updates, or explicit sgd for short, because the
next iterate θsgd
can be computed immediately after the nth data point (xn, yn) is observed.
The sequence γn > 0 is the learning rate sequence, and is typically deﬁned such that nγn →
γ > 0 as n → ∞; the hyperparameter γ > 0 is ﬁxed and known as the learning rate parameter.
The sequence {Cn} is a sequence of positive-deﬁnite matrices, such that Cn → C with C
known, and is used to better condition the iteration; in the simplest case Cn = I, i.e., we
simply use the identity matrix, which results in ﬁrst-order explicit sgd.

From a computational perspective, explicit sgd is eﬃcient because it replaces the expensive
inversion of p × p matrices, as in Fisher scoring, by a scalar sequence γn > 0 and a matrix
Cn that is fast to manipulate numerically, by design. Furthermore, the log-likelihood is
evaluated at a single observation yn given xn, rather than the entire data set D, which saves
signiﬁcant computation time. From a theoretical perspective, explicit sgd is justiﬁed because
the theory of stochastic approximations (Robbins and Monro 1951, Theorem 1) implies that
θsgd
converges to a point θ∞ such that E (∇ log f (yn; xn, θ∞)) = 0. Under standard statistical
n
theory, E (∇ log f (yn; xn, θ(cid:63))) = 0, and this point is unique under typical regularity conditions
(Lehmann and Casella 1998, Theorem 5.1, p.463), such as concavity of log-likelihood; this
is true, for example, in the popular exponential family of statistical models (Brown 1986).
Therefore, θ∞ = θ(cid:63), i.e., explicit sgd converges to the true parameter value. In the ﬁnite
N setting, a similar condition holds where θsgd
approximates θmle if the nth data point in
n
Equation 1 is an unbiased sample from the total N data points; see also Toulis and Airoldi
(2015b) for a review of applications of sgd on modern machine learning applications.

Despite these theoretical guarantees, explicit sgd requires careful tuning of the hyperparam-
eter γ in the learning rate: small values of the parameter make the iteration (1) very slow

Journal of Statistical Software

3

to converge in practice, whereas large values can cause numerical divergence. Moreover, it is
known that explicit sgd is statistically ineﬃcient even when γ is correctly speciﬁed (Toulis,
Airoldi, and Rennie 2014). In particular, the amount of information loss from procedure (1)
depends on the spectral gap of the Fisher information matrix, I(θ) = −E (cid:0)∇2 log f (yn; xn, θ)(cid:1),
calculated at the true parameter value θ = θ(cid:63). A large spectral gap makes it hard, or even im-
possible, to make the learning rates large enough for fast convergence, and also small enough
for stability (Toulis and Airoldi 2015a, Section 3.5).

Motivated by these challenges, Toulis, Tran, and Airoldi (2015) introduced averaged implicit
stochastic gradient descent (ai-sgd), which is deﬁned by the procedure

(2)

(3)

(4)

n = θim
θim

n−1 + γnCn∇ log f (yn; xn, θim

n ),

¯θn = (1/n)

θim
i

.

n
(cid:88)

i=1

The ﬁrst key component of ai-sgd is the implicit update (2). Note that it is implicit because
the next iterate θim
n appears on both sides of the equation. This simple modiﬁcation of the
explicit sgd procedure oﬀers several statistical advantages. In particular, assuming a common
starting point θsgd
(cid:44) θ0, one can show through a Taylor approximation of (2) around
θ0 that the implicit update satisﬁes

n−1 = θim
n−1

∆θim

n = (I + γnCnˆI(θ0; xn, yn))−1∆θsgd

n + O(γ2

n),

where ∆θn = θn − θn−1 for both methods, I is the identity matrix, and ˆI(θ0; xn, yn) =
−∇2(cid:96)(θ0; xn, yn) is the observed Fisher information matrix at θ0 (equivalent to the Hessian of
the negative log-likelihood at θ0). Equation 4 implies that the implicit update (2) is a shrinked
version of the explicit update (1). This shrinkage makes the iterations signiﬁcantly more stable
in small-to-moderate samples, and also robust to misspeciﬁcations of the learning rate param-
eter γ (Toulis et al. 2014). The implicit update (2) also has a Bayesian interpretation, where
n is the posterior mode of a model with the standard multivariate normal N (θim
θim
n−1, γnCn)
as the prior, and f (θ; xn, yn) as the likelihood. Thus it provides an iterative form of regu-
larization. In optimization, update (2) is known as a proximal update, and corresponds to a
stochastic version of the proximal point algorithm (Rockafellar 1976). Krakowski, Mahony,
Williamson, and Warmuth (2007) and Nemirovski, Juditsky, Lan, and Shapiro (2009) have
shown that proximal methods ﬁt better in the geometry of the parameter space.

The second key component of ai-sgd is iterate averaging (3), which guarantees optimal sta-
tistical eﬃciency under fairly relaxed conditions. Ruppert (1988) and Polyak and Juditsky
(1992) ﬁrst proved that averaging of iterates can achieve statistical optimality in the stan-
dard context of stochastic approximation with explicit updates; Toulis et al. (2015) extended
this result to the implicit sgd update (2). Thus, ai-sgd is eﬀectively a recursive estimation
method that is both statistically optimal and numerically stable, while remaining applicable
to the setting of massive and/or streaming data.

In this paper we develop statistically eﬃcient sgd algorithms for generalized linear models—
extending Algorithm 1 of Toulis et al. (2014)—and also develop sgd algorithms to perform
high-dimensional M-estimation. This allows for scalable estimation of such models with mas-
sive and/or streaming data. We provide a publicly available package sgd (Tran, Toulis, and
Airoldi 2015) written in R, which implements ai-sgd, as well as other sgd variants. In Section

4

Package sgd for estimation with large data sets

2, we develop the algorithms. Section 3 contains experiments on simulated and real-world
data, in which we demonstrate the advantages of the sgd package compared to alternative
software. In Section 4, we describe the interface of sgd and implementation details for its use
in practice.

2. Algorithms

In this section we develop algorithms which implement implicit sgd and ai-sgd for gener-
alized linear models as well as M-estimation. We start by introducing an algorithm which
eﬃciently computes a generalization of implicit update (2), which is useful for the aforemen-
tioned applications.

2.1. Eﬃcient computation of implicit updates

The main diﬃculty in applying ai-sgd is the solution of the multidimensional ﬁxed point
equation for the implicit update (2). In the large class of models where the likelihood given
covariate x depends on the parameter θ only through the natural parameter η ≡ x(cid:62)θ, the
solution of the ﬁxed-point equation is computationally eﬃcient. The general result is given
in Theorem 2.1, whereas the assumption is made more precise below.
Assumption 2.1. The likelihood (cid:96)(θ; xn, yn) ≡ log f (yn; xn, θ) of parameter value θ given
data point (xn, yn) depends on θ only through the product x(cid:62)

n θ, i.e.,

(cid:96)(θ; xn, yn) ≡ (cid:96)(x(cid:62)

n θ; xn, yn).

A key implication of Assumption 2.1 is that the direction of the gradient of the log-likelihood
does not depend on the parameter value since ∇ log f (yn; xn, θ) = (cid:96)(cid:48)(x(cid:62)
n θ; xn, yn)xn, where
the latter derivative is with respect to the natural parameter x(cid:62)
n θ and with ﬁxed data xn, yn.
This property is crucial because it implies that the implicit update (2) can be performed once
a scalar value is found that will appropriately scale the gradient.

Theorem 2.1. Suppose Assumption 2.1 holds. Then the gradient for the implicit iterate θim
n
(2) is a scaled version of the gradient at the previous iterate, i.e.,

∇ log f (yn; xn, θim

n ) = sn∇ log f (yn; xn, θim

n−1).

The scalar sn ∈ R satisﬁes

snκn−1 = (cid:96)(cid:48) (cid:16)

n θim
x(cid:62)

n−1 + γnsnκn−1x(cid:62)

n Cnxn; xn, yn

(cid:17)

,

(5)

(6)

(7)

n θim

where κn−1 = (cid:96)(cid:48)(x(cid:62)
n−1; xn, yn).
Theorem 2.1 shows that the gradient ∇ log f (yn; xn, θim
n ) in the implicit update (2) is in fact
a scaled version of the gradient ∇ log f (yn; xn, θim
n−1) that would appear in update (2) if we
were applying explicit updates. Therefore, computing the implicit update reduces to ﬁnding
the scale factor sn ∈ R. See Toulis and Airoldi (2015a, Threorem 4.1) for a proof.

It is possible to regularize both explicit and implicit sgd by adding
Penalized likelihood.
a penalty to the log-likelihood. In particular, we consider the elastic net (Zou and Hastie

Journal of Statistical Software

5

2005), where for some ﬁxed α ∈ [0, 1] the penalty function is

1
2
Adding the elastic net with a regularization parameter λ ∈ R to explicit sgd is straightfor-
ward:

Pα(θ) = (1 − α)

2 + α(cid:107)θ(cid:107)1.

(cid:107)θ(cid:107)2

(8)

n = θsgd
θsgd

n−1 + γnCn(∇ log f (yn; xn, θsgd

n−1) − λ∇Pα(θsgd

n−1)),

where the gradient of the elastic net penalty is given by

∇Pα(θim

n−1) = (1 − α)θsgd

n−1 + α sign(θsgd

n−1).

Here, the operation sign(θ) is the element-wise sign operation, outputting 1 if θj > 0, −1 if
θj < 0, and 0 otherwise.
For implicit sgd the update would be

n = θim
θim

n−1 + γnCn(∇ log f (yn; xn, θim

n ) − λ∇Pα(θim

n )).

(11)

However, it is not generally possible to compute update (11). For example, Assumption
2.1 does not hold because the gradient of the log-likelihood and the gradient of the penalty
generally have two diﬀerent directions. This breaks the argument of Theorem 2.1, where the
direction of the update calculated at the next iterate θim
n is the same as the direction of the
update calculated at the previous iterate θim

n−1.

To circumvent this problem, we simply penalize the previous iterate instead of the current,
i.e., perform the update

n = θim
θim

n−1 + γnCn(∇ log f (yn; xn, θim

n ) − λ∇Pα(θim

n−1)).

Then update (12) is equivalent to

n = θim
θim

n−1 + γnCn(sn∇ log f (yn; xn, θim

n−1) − λ∇Pα(θim

n−1)),

where the scale factor sn satisﬁes
snκn−1 = (cid:96)(cid:48) (cid:16)

n θim
x(cid:62)

n−1 − γnλx(cid:62)

n Cn∇Pα(θim

n−1) + γnsnκn−1x(cid:62)

n Cnxn; xn, yn

(cid:17)

,

(14)

and where κn−1 = (cid:96)(cid:48)(x(cid:62)
identical to the proof of Theorem 2.1.

n θim

n−1; xn, yn). A proof for this case with penalized likelihoods is

Final algorithm for implicit updates. This analysis leads to Algorithm 1, which, for
models satisfying Assumption 2.1, implements the most general update (13) of implicit sgd
with conditioning matrices and penalty. This algorithm applies a root-ﬁnding procedure
solving Equation 14 at every iteration, which is fast because the equation is one-dimensional
and the search bounds for the solution are known, having a diminishing range O(γn). Indeed,
the one-dimensional search is computationally negligible in practice, as we see in Section
3.

We also note that because the implicit update (17) eﬀectively does regularization as a shrink-
age estimate (see Equation 4), the use of penalization is not as crucial in practice as it is for

(9)

(10)

(12)

(13)

6

Package sgd for estimation with large data sets

Algorithm 1 Eﬃcient implementation of implicit update (13)
1: function implicit update((cid:96)(cid:48)(·; ·), γn, θim
2:

n−1, xn, yn, Cn, Pα)

n θim

# Compute search bounds B
rn ← γn(cid:96)(cid:48) (cid:0)x(cid:62)
(cid:1)
n−1; xn, yn
B ← [0, rn]
if rn ≤ 0 then
B ← [rn, 0]

3:

4:

5:

6:

9:

7:

8:

end if
# Solve ﬁxed-point equation by a root-ﬁnding method
ξ = γn(cid:96)(cid:48)(x(cid:62)
sn ← ξ/rn
# Equivalent to implicit update (13)
return θim
n θim
12:
13: end function

n Cn∇Pα(θim

n−1 − γnλx(cid:62)

n−1) + ξx(cid:62)

n−1 + γnCn

(cid:0)sn(cid:96)(cid:48) (cid:0)x(cid:62)

n−1; xn, yn

n θim

11:

10:

n Cnxn; xn, yn), ξ ∈ B

(cid:1) xn − λ∇Pα(θim

n−1)(cid:1)

explicit updates. We make extensive experiments using Algorithm 2 and also examine this
eﬀect in Section 3.

2.2. Generalized linear models

In the family of generalized linear models (GLMs), the outcome yn ∈ R follows an exponential
family distribution conditional on xn,

yn | xn ∼ exp

(ηnyn − b(ηn))

c(yn, ψ),

ηn ≡ x(cid:62)

n θ(cid:63),

(15)

(cid:26) 1
ψ

(cid:27)

where the scalar ψ > 0 is the dispersion parameter which aﬀects the variance of the outcome,
c(·, ·) is the base measure, and b(·) is the log normalizer which ensures that the distribution
integrates to one.1 Additionally, in a GLM it is assumed that E (yn| xn) = h(x(cid:62)
n θ(cid:63)), where
h : R → R is known as the transfer function (Nelder and Wedderburn 1972; Dobson and
Barnett 2008). A simple property of GLMs is that the transfer function is the ﬁrst derivative
of the log normalizer, i.e., h(x(cid:62)

n θ), for all xn, θ.

n θ) = b(cid:48)(x(cid:62)

A straightforward implementation of explicit sgd for estimation with GLMs is

n = θsgd
θsgd

n−1 + γnCn[yn − h(x(cid:62)

n θsgd

n−1)]xn.

Similarly, the ai-sgd procedure can be written as

(16)

(17)

n = θim
θim

¯θn =

n−1 + γnCn[yn − h(x(cid:62)
n
1
(cid:88)
n

θim
n .

i=1

n θim

n )]xn,

By assumption, (cid:96)(θ; yn, xn) ∝ (x(cid:62)
n θ(cid:63)), and thus the log-likelihood depends on
parameter value θ(cid:63) only through its linear combination with covariate value xn. Additionally,

n θ(cid:63))yn − b(x(cid:62)

1We present one-dimensional outcomes for simplicity. However, our theory easily extends to multidimen-

sional outcomes. Such an extension is given, for example, in Section 2.3 on M-estimation.

Journal of Statistical Software

7

Algorithm 2 Estimation of generalized linear models with ai-sgd
1: Initialize θim
2: for n = 1, 2, . . . do
3:

n θ; xn, yn) ≡ yn − h(x(cid:62)

0 , ¯θ0

n θ)

Deﬁne (cid:96)(cid:48)(x(cid:62)
Calculate implicit update

4:

θim
n ← IMPLICIT UPDATE((cid:96)(cid:48)(·; ·), γn, θim

n−1, xn, yn, Cn, Pα)

¯θn ← n−1
n

¯θn−1 + 1

n θim
n

5:
6: end for

Var (yn|xn) = h(cid:48)(x(cid:62)
and concave, thus fulﬁlling Assumption 2.1.

n θ(cid:63))||xn||2, and thus h(cid:48) ≥ 0, which implies that (cid:96) is twice-diﬀerentiable

Penalized likelihood. As argued before, one can add the elastic penalty by applying it to
the previous estimate instead of the current. That is, for ﬁxed α ∈ [0, 1] and regularization
parameter λ ∈ R, the ai-sgd procedure for generalized linear models with elastic net is

n = θim
θim

n−1 + γnCn

[yn − h(x(cid:62)

n θim

n )]xn − λ∇Pα(θim

n−1)

(cid:16)

(cid:17)

,

¯θn =

1
n

n
(cid:88)

i=1

θim
n .

Algorithm 2 implements estimation of GLMs through ai-sgd based on updates (18).

2.3. M-Estimation

Given a data set of N observations D = {(xn, yn)} and a convex function ρ : R → R+, the
M-estimator is deﬁned as

ˆθm = arg min
θ∈Rp

ρ(yn − x(cid:62)

n θ),

N
(cid:88)

n=1

where it is assumed yn = x(cid:62)
n θ(cid:63)+(cid:15)n, and (cid:15)n are i.i.d. zero mean-valued noise. M-estimators are
especially useful in robust statistics (Huber 1964; Huber and Ronchetti 2009), as appropriate
choice of ρ can reduce the inﬂuence of outliers in data. Recently, there has been increased in-
terest in the literature for fast approximation of M-estimators due to their robustness (Donoho
and Montanari 2013; Jain, Tewari, and Kar 2014).

Typically in M-estimation, ρ is twice-diﬀerentiable around zero and

(cid:16)

E

ρ(cid:48)(yn − x(cid:62)
n

ˆθm)xn

= 0,

(cid:17)

where the expectation is over the empirical data distribution. Therefore sgd algorithms can
be applied to approximate the M-estimator ˆθm. Importantly, ρ is convex, which implies that
the conditions of Assumption 2.1 are met.

(18)

(19)

(20)

8

Package sgd for estimation with large data sets

Algorithm 3 M-estimation with ai-sgd
1: Initialize θim
2: for n = 1, 2, . . . do
3:

Deﬁne (cid:96)(cid:48)(x(cid:62)
Calculate implicit update

n θ; xn, yn) ≡ −ρ(cid:48)(yn − x(cid:62)

0 , ¯θ0

4:

n θ)

¯θn ← n−1
n

¯θn−1 + 1

n θim
n

5:
6: end for

θim
n ← IMPLICIT UPDATE((cid:96)(cid:48)(·; ·), γn, θim

n−1, xn, yn, Cn, Pα)

The ai-sgd procedure for approximating M-estimators is

n = θim
θim

¯θn =

n−1 + γnCn[ρ(cid:48)(yn − x(cid:62)
n
1
(cid:88)
n

θim
n .

i=1

n θim

n )]xn,

(21)

(22)

An outline of the procedure is given in Algorithm 3. As before, Algorithm 3 also includes the
optional use of a sequence of conditioning matrices Cn and a penalty function Pα. The use
of penalization has particularly been considered as a way to merge the robustness properties
given by a choice of ρ with sparsity, e.g, through lasso (Owen 2007; Lambert-Lacroix, Zwald
et al. 2011; Li, Peng, and Zhu 2011).

It is also typical to assume that the density of (cid:15)n is symmetric around zero. Therefore, it
also holds E (cid:0)ρ(cid:48)(yn − x(cid:62)
(cid:1) = 0, where the expectation is over the true data distribution.
Hence sgd procedures can be used to estimate θ(cid:63) in the case of an inﬁnite stream of obser-
vations (N = ∞). We write Algorithm 3 for the case of ﬁnite N , but it is trivial to adapt the
procedure to inﬁnite N .

n θ(cid:63))xn

3. Experiments

In this section, we compare the sgd methods implemented in the sgd package, such as ex-
plicit sgd and ai-sgd, with standard, deterministic optimization methods that are widely
used in statistical practice, such as glmnet, biglm, and speedglm. We demonstrate in both
massive and streaming data settings that standard methods are not applicable, and further-
more that sgd methods outperform such methods upon orders of magnitude in runtime and
convergence.

As standard methods are not competitive, we also compare the proposed sgd methods to
each other, e.g., comparing ai-sgd to explicit sgd, across a wide range of learning rate
speciﬁcations, including adaptive speciﬁcations such as AdaGrad (Duchi, Hazan, and Singer
2011) and RMSProp (Tieleman and Hinton 2012); more details on the speciﬁcations which
are available in sgd are given in Section 4.3.

All timings are carried out on a general-purpose 2.6 GHz Intel Core i5 processor, and are
reported for various algorithms which reach a thresholded L2 distance to the true parameter
value.

Journal of Statistical Software

9

3.1. Linear regression with the lasso

We follow an experiment used in benchmarking the glmnet package (Friedman, Hastie, and
Tibshirani 2010, Section 5.1), which ﬁts GLMs with the elastic net penalty over a regulariza-
tion path. As glmnet was shown to outperform related software such as elasticnet (Zou and
Hastie 2012) and lars (Hastie and Efron 2013), we compare sgd strictly to glmnet. The design
matrix X with N observations and p predictors is generated from a normal distribution such
that each pair of predictors Xj, Xj(cid:48) has the same correlation ρ. Each of the N outcomes yn,
n = 1, 2, . . . N , is deﬁned as

yn = x(cid:62)

n θ(cid:63) + k(cid:15)n,

(23)

where θ(cid:63)j = (−1)j exp(−2(j − 1)/20) so that the elements of the true parameter value θ(cid:63) have
alternating signs and are exponentially decreasing. The noise (cid:15)n is distributed as a standard
normal, (cid:15) ∼ N (0, 1), and k is chosen so that the signal-to-noise ratio is equal to 3.0. We run
glmnet with “covariance updates”, which takes advantage of sparse updates in the parameter
space to reduce the complexity of O(N p) calculations per iteration. It performs better in our
experiments than the “naive update” also considered in Friedman et al. (2010).

Table 1 outlines results for a combination of triplets (N, p, ρ), ranging from N = 1, 000 ob-
servations and scaling up to N = 10 million. glmnet is seen to be competitive with sgd
procedures under the setting of N = 1, 000 observations, and in fact glmnet slightly outper-
forms sgd algorithms for lower dimensions of N and p. It is in any higher dimensional setting
where sgd strictly dominates glmnet, as seen in the table where for example, with N = 50, 000
and p = 10, 000, sgd is orders of magnitude faster.

Furthermore, glmnet is restricted by the memory limitations of computer hardware. For
example, simulations with 100, 000 observations and 10, 000 features require 8 GB in memory
for simply storing the data, and more is required for parameter storage and computational
overhead. For the sgd package, we simply stream the data points using bigmemory, which
requires less than 500 MB of RAM for all our experiments, a 16-fold decrease in memory
requirements. This is not possible for glmnet in either the case of real streaming data, or
simply as a way to remove memory bottlenecks. In principle, gradient descent algorithms
such as glmnet can read and destroy data memory from disk as it loops over the full data
set; however, this is impractical as it requires such an expensive memory access at each
iteration.

We now compare the sgd algorithms. For small dimensional problems, explicit sgd achieves
faster runtime than ai-sgd as it does not require a one-dimensional search following Algorithm
1. However, in high dimensions and high correlations, it becomes extremely diﬃcult for
explicit sgd to even converge for this toy linear model. It is sensitive to the learning rate, and
any misspeciﬁcation can cause it to diverge numerically. Thus, we were not able to obtain a
proper timing for explicit sgd in settings of either high correlation (ρ > 0.9) or high dimension
with medium correlation (ρ > 0.5). In practice one must tune the hyperparameter for explicit
sgd—thus requiring signiﬁcant computational overhead and user input—while also closely
monitoring the stochastic gradients for consideration of other numerical issues. ai-sgd on the
other hand uses additional computation per iteration, which in high dimensions is negligible
compared to the cost of a stochastic gradient update. This additional computation leads to
signiﬁcantly more robust updates and faster convergence.

10

Package sgd for estimation with large data sets

0

0.1

0.9

0.95

Correlation
0.2

0.5

N = 1, 000 p = 100 (sec)

sgd(method="ai-sgd")
sgd(method="sgd")
glmnet

0.03
0.02
0.02

0.03
0.02
0.02

0.03
0.02
0.02

0.03
0.02
0.02

0.04
0.03
0.02

0.34
0.03
0.03

N = 10, 000 p = 1, 000 (sec)

sgd(method="ai-sgd")
sgd(method="sgd")
glmnet

1.81
2.78
6.60

1.65
2.90
7.76

1.78
2.93
8.00

1.50
2.81
7.83

1.85
–
6.50

1.83
–
6.70

N = 50, 000 p = 10, 000 (min)

sgd(method="ai-sgd")
sgd(method="sgd")
glmnet

3.12
4.83
14.58

3.51
4.86
15.28

3.43
5.23
16.29

3.26
–
15.58

3.40
–
16.54

3.38
–
16.41

N = 1, 000, 000 p = 50, 000 (min)

sgd(method="ai-sgd")
sgd(method="sgd")
glmnet

22.23
27.80
–

21.10
34.08
–

19.88
–
–

21.52
–
–

18.53
–
–

20.53
–
–

N = 10, 000, 000 p = 100, 000 (hr)

sgd(method="ai-sgd")
sgd(method="sgd")
glmnet

9.38
13.50
–

10.20
–
–

9.58
–
–

8.54
–
–

10.11
–
–

10.74
–
–

Table 1: Linear regression with the lasso. Timing (in various units) is displayed for 100
λ values, averaged over 10 runs. The ﬁrst line is sgd using ai-sgd and the second line is
sgd using explicit sgd. Omitted entries indicate failure of the algorithm; for explicit sgd it
numerically diverges, and for glmnet it could not run due to memory limitations.

3.2. Logistic regression with ridge penalty

Following benchmarks that are popular in the machine learning and optimization literature
(Xu 2011; Shamir and Zhang 2012; Bach and Moulines 2013; Schmidt, Le Roux, and Bach
2013), we perform large-scale logistic regression on four data sets:

• rcv1 (Lewis, Yang, Rose, and Li 2004): text data set in which the task is to classify
documents belonging to class ccat, where we apply preprocessing provided by Bottou
(2012).

• covtype (Blackard 1998): data set consisting of forest cover types in which the task is

to classify for one speciﬁc class among 7 forest cover types.

Journal of Statistical Software

11

description

type

covariates

training set

test set

λ

covtype
delta
rcv1
mnist

sparse
forest cover type
dense
synthetic data
text data
sparse
digit image features dense

54
500
47,152
784

464,809
450,000
781,265
60,000

116,203
50,000
23,149
10,000

10−6
10−2
10−5
10−3

Table 2: Summary of data sets and the L2 regularization parameter λ used.

• delta (Sonnenburg, Franc, Yom-Tov, and Sebag 2008): synthetic data oﬀered in the
PASCAL Large Scale Challenge. We apply the default processing oﬀered by the chal-
lenge organizers.

• mnist (Le Cun, Bottou, Bengio, and Haﬀner 1998): images of handwritten digits, where

the task is to classify digit 9 against all others.

A summary of the data sets is available in Table 3, where the number of observations are
typically on the order of several hundred thousand, and the covariates range from a few dozen
to tens of thousands. The regularization parameter λ for the ridge penalty are set according
to those used in Xu (2011).

We compare to the following three packages: biglm (Lumley 2013) and speedglm (Enea, Meiri,
and Kalimi 2015), both of which perform approximate updates using iteratively reweighted
least squares, and LiblineaR (Helleputte 2015), which is a simple wrapper to a C++ library
for regularized linear classiﬁcation. We use the stochastic dual coordinate ascent algorithm
(Shalev-Shwartz and Zhang 2013) in LiblineaR. In addition, we consider the mnlogit package
(Hasan, Zhiyu, and Mahani 2015), which implements multinomial logistic regression using the
classical technique of Newton-Raphson, and exploits iterations over intermediate data struc-
tures for fast Hessian calculations. For modest-sized problems, mnlogit is shown to be 10-50
times faster than mlogit (Croissant 2013), VGAM (Yee 2010), and the multinom function in
nnet (Venables and Ripley 2002). Finally, we also run the default function glm.fit as a base-
line. We note that mnlogit and glm.fit can be only employed for standard (unregularized)
multinomial regression, so we run them without the ridge penalty.

Table 3 outlines the runtimes for the considered packages. The two sgd algorithms are orders
of magnitude faster than its competitors on all data sets. Interestingly, biglm and speedglm
failed to run on the three real data sets when attempting to invert subsets of the data, and
only succeeded for the one synthetic data set delta. We also note that the largest data
set—rcv1—failed for the majority of algorithms: only the packages sgd and LiblineaR were
able to converge, both of which natively use stochastic gradients for computationally eﬃcient
updates. However, sgd is signiﬁcantly faster because less overhead seems to be involved in
passing data structures to perform computation in native C++.

Moreover, sgd requires O(p) memory, which is optimal in the sense that O(p) is the minimum
n . Both biglm and speedglm require O(p2) for the
required for simply storing the nth iterate θim
inversion of a p×p matrix, as do mnlogit and glm.fit. The mnlogit package also requires data
in the long format, which leads to a duplication of rows, as many entries display redundant
information. Moreover, while exploitation of the Hessian structure can help in practice (as it
outperforms glm.fit), we observe that the traditional technique of Newton-Raphson remains

12

Package sgd for estimation with large data sets

data set

sgd (ai-sgd)

sgd (sgd) biglm speedglm LiblineaR mnlogit

glm.fit

covtype
delta
rcv1
mnist

5.21
10.10
14.15
3.50

7.58
10.23
15.42
3.37

–
736.13
–
–

–
30.50
–
–

1444.78
2167.14
133.10
208.55

16.04
445.73
–
232.53

40.11
498.97
–
890.76

Table 3: Large-scale logistic regression on four data sets. Timing (in seconds) is displayed,
averaged over 10 runs. Omitted entries indicate failure of the algorithm; for biglm and
speedglm, it could not run due to inversions of singular matrices; for mnlogit it could not run
due to memory limitations.

Figure 1: Large scale logistic regression on four data sets. Each plot indicates the classiﬁca-
tion error on the test set for explicit sgd with AdaGrad, ai-sgd, averaged sgd, and explicit
sgd over a pass of the data.

Journal of Statistical Software

13

N

p

sgd (ai-sgd)

sgd (sgd) hqreg units

1,000
10,000
10,000
50,000
100,000
1,000,000
10,000,000
100,000

100
500
1,000
10,000
50,000
100,000
100,000
1,000,000

0.05
0.55
1.30
3.12
8.13
35.88
8.64
18.80

0.04
0.46
2.22
3.86
15.20
51.93
9.55
26.43

0.03
0.40
6.34
15.57
–
–
–
–

(sec)
(sec)
(sec)
(min)
(min)
(min)
(hr)
(hr)

Table 4: High-dimensional M-estimation with the Huber loss. Timing (in units given by the
last column) is displayed for 100 λ values, averaged over 10 runs. Omitted entries indicate
failure of the algorithm; for hqreg, it could not run due to memory limitations.

untenable because it still requires O(N p2) complexity per iteration in the worst case.

For demonstration, Figure 1 shows the progress of multiple sgd algorithms available in sgd (see
Section 4.2) over a pass of the data. We note that ai-sgd achieves the fastest or competitive
convergence rates, without requiring signiﬁcant tuning of parameters as the other algorithms
do; this includes popular adaptive learning rate speciﬁcations, such as explicit sgd with
AdaGrad.

3.3. M-estimation with the Huber loss

We follow an example for high-dimensional M-estimation in Donoho and Montanari (2013,
Section 2.4). Deﬁne the convex function ρ : R → R+ to be the Huber loss,

ρ(z; λ) =

(cid:40)

z2/2,
λ|z| − λ2/2,

if |z| ≤ λ,
otherwise.

Fix the thresholding parameter λ = 3, and generate the N ×p design matrix with i.i.d. entries
Xi,j ∼ N (0, 1
N ). We ﬁx the true set of parameters θ(cid:63) to be a vector randomly drawn with
ﬁxed norm (cid:107)θ(cid:63)(cid:107)2 = 6

p, and then generate outcome yn, n = 1, 2, . . . , N , as

√

yn = x(cid:62)

n θ(cid:63) + (cid:15)n.

(24)

For the distribution of errors (cid:15)n, we use Huber’s contaminated normal distribution CN(0.05, 10),
i.e., (cid:15)n ∼ 0.95z + 0.05h10, i.i.d., where z is standard normal and hx is a point mass at x.

Few alternative packages to sgd exist for high-dimensional robust estimation. We compare to
hqreg (Yi 2015), which ﬁts regularization paths for Huber loss regression with the elastic net
penalty. Note that hqreg is specialized to the Huber loss and cannot perform estimation for
the general setting of M-estimation problems considered here.

Table 4 outlines results for a combination of pairs (N, p), ranging from small problems of
N = 1, 000 observations to massive data settings of N = 10 million. We apply the elastic
net penalty with α = 0.5, which puts even weight on both the lasso and ridge components,

14

Package sgd for estimation with large data sets

Figure 2: High dimensional M-estimation with the Huber loss, for N = 100, 000 observations,
p = 10, 000 covariates, and a ﬁxed regularization parameter λ. The plots indicate the mean-
squared error across iterations (left) and time (right) for sgd algorithms. The horizontal line
displays the mean-squared error for the exact M -estimator (cid:98)θm.

and then compute a regularization path for both packages. We also include an example of
N = 100, 000 observations and p = 1, 000, 000 covariates, where there exist far more covariates
than data points; this occurs often in applications, e.g., in text analysis, bioinformatics, and
signal processing (Lustig, Donoho, Santos, and Pauly 2008; Blei 2012).

The sgd algorithms begin to outperform hqreg on the order of tens of thousands of obser-
vations, and signiﬁcantly so for larger data settings. Similar to the memory limitations of
glmnet, hqreg requires access to the full data set per iteration of its algorithm, which is in-
feasible when the data cannot be held in memory. Thus we were unable to obtain proper
timings for data sets of size greater than 50, 000 observations and 10, 000 covariates.

Figure 2 displays the progress of the sgd algorithms for the setting of N = 100, 000 observa-
tions and p = 10, 000 covariates, for a ﬁxed regularization parameter λ. For demonstration,
we run the algorithms over 10 passes of the data and thus over a total of 1 million iterations.
ai-sgd is seen to achieve a signiﬁcantly faster convergence rate than explicit sgd. We also
consider the use of adaptive schedules, here with RMSProp, as it performs the fastest among
other available learning rates (see Section 4.3). With RMSProp, the diﬀerence between the
two methods—sgd and ai-sgd—is noticeably smaller, and in fact sgd seems to converge
slightly faster. We note however that the use of sgd algorithms with RMSProp breaks sta-
tistical eﬃciency, and indeed we see this eﬀect as the mean-squared error oscillates around a
value higher than the MSE of the exact M-estimator (green line). Therefore we advocate the
use of ai-sgd with a one-dimensional learning rate, which still converges quite quickly.

Journal of Statistical Software

15

4. Interface and implementation

We now discuss the interface of sgd and various technical details that are important for its
use in practice.

4.1. Interface

The sgd package provides an intuitive and accessible set of methods for performing estimation
with large-scale data sets. At the core of the package is the function

sgd(formula, data, model, model.control, sgd.control)

The user provides a formula on the data frame data—similar to function primitives, such as
lm—and then speciﬁes the model. The model parameters are estimated using sgd methods,
which defaults to ai-sgd. The optional arguments model.control and sgd.control specify
attributes one can tweak about the model and the stochastic gradient method, respectively.
For example, given a data frame dat with response vector stored as the column y,

sgd(y ~ ., data=dat, model="lm")

ﬁts a linear model with the default speciﬁcations, e.g., ai-sgd with a one-dimensional learning
rate. Similarly,

sgd(y ~ ., data=dat, model="glm", model.control=list(family="binomial"))

ﬁts logistic regression with the default speciﬁcations. Numerous examples are available in the
package by running demo(package="sgd").

The sgd function also interfaces with data sets that are too large to ﬁt into memory or
are streaming (more details in Section 4.4), and can be run with a custom loss function if
desired.

The output of the sgd function is a sgd object, which is a light wrapper on a list which
collects quantities, such as the ﬁnal parameter estimates and convergence diagnostics. Custom
generic methods are also available for the sgd class, such as print, predict, and plot.

4.2. Stochastic gradient methods

While we describe the explicit sgd and ai-sgd algorithms in Section 2, the following stochastic
gradient methods are also implemented in sgd:

• implicit sgd: Proposed by Toulis et al. (2014) in the context of generalized linear models,
this algorithm uses the implicit update (2) and does not do any iterate averaging.

• averaged sgd: Proposed by Ruppert (1988) and Bather (1989) independently, this

algorithm uses the explicit update (1) followed by iterate averaging (3).

• classical momentum (cm): Proposed by Polyak (1964), this algorithm uses the update

vn = µvn−1 + an∇ log f (yn; xn, θn−1),
θn = θn−1 + vn,

(25)

(26)

16

Package sgd for estimation with large data sets

where µ ∈ [0, 1] is a ﬁxed momentum coeﬃcient. cm accelerates gradient descent with
a velocity vector which accumulates directions of large increase in the log-likelihood.

• Nesterov’s accelerated gradient (nag): Proposed by Nesterov (1983), this algorithm

uses the update

vn = µvn−1 + an∇ log f (yn; xn, θn−1 + µvn−1),
θn = θn−1 + vn,

(27)

(28)

where µ ∈ [0, 1] is a ﬁxed momentum coeﬃcient. nag is similar to cm but accumulates
velocity at a ”look-ahead” point θn−1 + µvn−1. This makes a partial update closer to
θn, allowing nag to change its velocity more quickly and responsively.

While all these methods are available, we recommend and apply ai-sgd as the default. It can
be seen as an eﬀective combination of the advantages from both implicit sgd and averaged
sgd (Toulis et al. 2015). The momentum-based methods cm and nag enjoy faster convergence
rates than the original explicit sgd, but oﬀer no theoretical beneﬁts against ai-sgd. Without
averaging techniques they also are statistically ineﬃcient, whereas iterate averaging can be
interpreted as an acceleration technique because larger learning rates are used. The velocity
update in nag is also a proxy for the implicit update, as its beneﬁt mostly relies on making
updates close to where the new estimate would lie.

4.3. Learning rates

We describe the available learning rates in more detail because they are critical for convergence
of SGD methods, in practice.
It is well-known (Sakrison 1965; Amari 1998; Toulis et al.
2014) that explicit sgd (1) and implicit sgd (2) have optimal statistical eﬃciency if the
learning rate sequence γn together, with the conditioning matrices Cn, approximate the inverse
Fisher information matrix I(θ(cid:63)) = −E (cid:0)∇2(cid:96)(θ(cid:63); xn, yn)(cid:1), i.e., γnCn → I(θ(cid:63))−1, in the limit.
Therefore in ﬁrst-order methods where Cn = I, the learning rate sequence acts as a scalar-
valued approximation to the optimal rescaling as it is used in Fisher scoring (Fisher 1925).
Based on this theory, the following learning rates are implemented in sgd:

• One-dimensional (Xu 2011): The learning rate is of the form

γn = γ0(1 + aγ0n)−c,

where γ0, a, c ∈ R are ﬁxed constants. For sgd algorithms without iterate averaging
and sgd algorithms with iterate averaging, Xu (2011) proved that setting c = 1 and
c = 2/3, respectively, leads to optimal statistical eﬃciency; a similar result holds for
ai-sgd (Toulis et al. 2015).

• AdaGrad (Duchi et al. 2011): Rather than specify a one-dimensional learning rate
γn ∈ R, Duchi et al. (2011) propose a diagonal conditioning matrix Cn ∈ Rp×p given by

In = In−1 + diag(∇(cid:96)(θn−1; xn, yn)∇(cid:96)(θn−1; xn, yn)(cid:62)),
Cn = η(In + (cid:15)I)−1/2,

where diag(·) extracts the diagonal entries of its matrix argument, η ∈ R is a constant, I
is the identity matrix, and (cid:15) is a ﬁxed value, typically 10−6, to prevent division by zero.

Journal of Statistical Software

17

In the limit, In is an unbiased estimate of the diagonal entries of the Fisher information,
and the proposed diagonal matrix Cn, which accumulates such curvature information,
is proven to be optimal for minimization of the regret bound.

• RMSProp (Tieleman and Hinton 2012): A learning rate which is popular in the deep
learning literature (Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov 2014;
Ranganath, Tang, Charlin, and Blei 2015; Rezende and Mohamed 2015), Tieleman and
Hinton (2012) propose the diagonal conditioning matrix Cn ∈ Rp×p given by

In = βIn−1 + (1 − β) diag(∇(cid:96)(θn−1; xn, yn)∇(cid:96)(θn−1; xn, yn)(cid:62)),
Cn = η(In + (cid:15)I)−1/2,

where β ∈ [0, 1] is the discount factor, η ∈ R is a constant, I is the identity matrix,
and (cid:15) is a ﬁxed value to prevent division by zero, as in AdaGrad. RMSProp uses a
decay in the estimate for the Fisher information by taking a weighted average, and
thus it gives more weight onto newer than older information. RMSProp aims to oﬀset
one problem AdaGrad often encounters in practice, where very large values occur for
initial estimates of In (e.g., due to poor initialization), thus slowing down the AdaGrad
procedure as it tries to accumulate enough curvature information to compensate for such
an error (Schaul, Antonoglou, and Silver 2014). RMSProp balances this by taking a
weighted average of previous and new information, and sees much empirical success. One
problem, however, is that RMSProp is no longer decaying suﬃciently quickly (Robbins
and Monro 1951; Duchi et al. 2011), and thus it has no guarantees on convergence.
Moreover, assuming convergence, the limit of the learning rate sequence is a constant,
which makes the iterates jitter around the true parameter value, ad inﬁnitum.

• Fisher: Following results on statistical eﬃciency and Fisher scoring, we propose a learn-

ing rate using a diagonal conditioning matrix Cn ∈ Rp×p given by

In = (1 − γn)In−1 + γndiag(∇(cid:96)(θn−1; xn, yn)∇(cid:96)(θn−1; xn, yn)(cid:62)),
Cn = (In + (cid:15)I)−1,

where γn ∝ 1/n, and (cid:15) is a small ﬁxed value to prevent division by zero, as in AdaGrad.
As before, In in the limit is an unbiased estimate of the diagonal Fisher information,
and Cn is adaptive to curvature information.

One critical but often unnoticed issue with AdaGrad, RMSProp, and similar adaptive sched-
ules is that they are statistically ineﬃcient: the speciﬁcation of the learning rates leads to
biased estimation of the inverse Fisher information matrix I(θ(cid:63))−1 that, as mentioned ear-
lier, is necessary for optimal statistical eﬃciency (an important exception is iterate averaging).
This leads to a suboptimal asymptotic variance for the sgd procedure. Thus we recommend
and apply the last proposed learning rate (“Fisher”) by default:
it takes advantage of the
curvature information such methods beneﬁt from, while still preserving as much statistical
eﬃciency as possible in diagonal conditioning matrices.

4.4. Software integration

For data sets that cannot be loaded into memory, we access subsets of the data using big-
memory (Kane, Emerson, and Weston 2013). This allows one to perform stochastic gradient

18

Package sgd for estimation with large data sets

descent by passing over the data loaded into RAM, and then to reload a new data set. This
naturally applies to both large data sets, e.g., on the order of dozens of gigabytes, and stream-
ing settings, in which one has access only to a subset of the (potentially inﬁnite) data at a
time.

In principle, with bigmemory the memory requirement for these stochastic gradient methods
is only a single data point and the current parameter estimate, which is the minimum O(p)
complexity for simply storing the estimate. In our implementation we use these savings to
try to load as much data into RAM as possible. This speeds up convergence in practice, as
it reduces the amount of I/O overhead; this especially becomes a signiﬁcant bottleneck when
reading many objects from disk.

For fast implementations we use Rcpp (Eddelbuettel and Fran¸cois 2011), where all algo-
rithms are written in C++ and only interface-level code is written in R. Aside from the major
computational gains, this also provides the opportunity to extend the library to other pro-
gramming languages. RcppArmadillo (Eddelbuettel and Sanderson 2014) is applied for access
to pre-optimized linear algebra routines, and BH (Eddelbuettel, Emerson, and Kane 2015) for
access to the Boost libraries. We apply template meta-programming and reusable classes in
an object-oriented framework, including concepts such as stochastic gradient methods, mod-
els, and learning rates. Such concepts make it easy for other users to develop new algorithms
and prototype them in their own research or practices.

The plotting routines adopt many features from ggplot2 (Wickham 2009), and are eﬀectively
templated ggplot objects. Our software is also robust through unit testing which follows the
paradigm from testthat (Wickham 2011).

5. Discussion

As explicit sgd has been used extensively in practice, particularly in the deep learning com-
munity, many heuristics have been proposed to solve issues that often occur. We describe
several of these issues and their proposed solutions in the literature, and compare to how our
sgd package handles them.

Overﬁtting. As sgd algorithms simply minimize a loss function evaluted over the training
data, overﬁtting is a prevalent problem as it is for all estimation methods. This is particularly
an issue in complex likelihood functions such as neural networks (see, e.g., Giles (2001); Bakker
and Heskes (2003)). Even with penalization terms that try to oﬀset the ﬁt of the parameters, it
is still diﬃcult for explicit sgd to ﬁnd the right set of hyperparameters for such regularization
without a computationally intensive search.

As a solution many practictioners adopt early stopping, which simply halts the optimization
routine before it converges. However, there is little theory on the estimates obtained from
early stopping. Most practically, it is diﬃcult to know when to stop the algorithm and how
to use it in combination with other regularization techniques, such as penalization.

Fortunately, one of the advantages of ai-sgd is that it requires less such tweaking: the implicit
update eﬀectively performs a regularization as seen from the Bayesian perspective, c.f., Section
1. We’ve also seen in practice that penalization terms do not aﬀect the ﬁnal estimates from
ai-sgd, which makes it less reliant on heuristics, such as early stopping.

Journal of Statistical Software

19

Vanishing or exploding gradients. The numerical instability of explicit sgd is a widespread
issue in practice (Bengio, Simard, and Frasconi 1994; Hochreiter 1998; Hochreiter, Bengio,
Frasconi, and Schmidhuber 2001; Toulis et al. 2014). The stochastic gradients can easily
be too large leading to divergence, and when chained through compositions of functions can
either vanish to zero, or even explode to numerically inﬁnite values; for example, Toulis et al.
(2014) demonstrate the instability of explicit sgd in a simple bivariate Poisson model, where
slight misspeciﬁcation of learning rate parameters lead to divergence.

Pascanu, Mikolov, and Bengio (2012) propose gradient clipping, which simply thresholds the
stochastic gradient if it is outside a bounded interval. Unfortunately, while it can work in
practice, it is a heuristic that breaks the key assumptions for convergence rate guarantees on
sgd algorithms. Similarly, there is no principled way to set the bounds. For ai-sgd algorithms
applied to the settings we consider in Section 2, such an issue never arises. Theoretical results
establish stability regardless of the speciﬁcation of the learning rate (Toulis and Airoldi 2015a,
Section 3), and perform well in practice, as seen in Section 3.

6. Concluding remarks

The sgd package is the most extensive implementation in R of stochastic gradient methods
for estimation with massive and/or streaming data sets. Thus, sgd broadens the capabilities
of R for estimation with modern large data sets—on the orders of hundreds of millions of
observations and hundreds of thousands of covariates—while retaining desirable statistical
properties. The software is based on solid theory of stochastic approximations, which help
guide the optimal selection of parameters, e.g., learning rates, in the underlying optimiza-
tion routines. In this paper, we show how sgd can be applied for estimation of generalized
linear models and M-estimation, which comprise a sizeable portion of estimation problems
encountered in statistical practice.

There are many software extensions that are currently in development. We are working
to interface with other high-performance computing packages, namely sqldf for faster I/O
applications with streaming data, doParallel (Analytics and Weston 2014) and Rmpi (Yu
2002) for parallel updates across environments, and gputools (Buckner, Seligman, and Wilson
2011) for eﬃcient computing with GPUs. The algorithms described here directly appeal
to asynchronous implementations, following Hogwild! (Nui, Recht, Re, and Wright 2011),
which allows for lock-free allocation of CPU cores. Sparse data structures would allow for
fast structured matrix and vector products, which occur, for example, when looping over
the covariates of a data point, and would signiﬁcantly speed up computation on sparse data
sets.

Finally, there has been little attention on, and in fact a pressing need for, model selection
and hypothesis testing in sgd procedures. We are pursuing this in light of the new statistical
challenges presented to us while developing the sgd package.

References

20

Package sgd for estimation with large data sets

Amari SI (1998). “Natural Gradient Works Eﬃciently in Learning.” Neural computation,

10(2), 251–276.

Analytics R, Weston S (2014). doParallel: Foreach Parallel Adaptor for the Parallel Pack-
age. R package version 1.0.8, URL http://CRAN.R-project.org/package=doParallel.

Bach F, Moulines E (2013). “Non-strongly-convex Smooth Stochastic Approximation with
Convergence Rate O(1/n).” In Advances in Neural Information Processing Systems, pp.
773–781.

Bakker B, Heskes T (2003). “Task Clustering and Gating for Bayesian Multitask Learning.”

The Journal of Machine Learning Research, 4, 83–99.

Bather J (1989). Stochastic Approximation: A Generalisation of the Robbins-Monro Proce-

dure, volume 89. Mathematical Sciences Institute, Cornell University.

Bengio Y, Simard P, Frasconi P (1994). “Learning Long-term Dependencies with Gradient

Descent is Diﬃcult.” Neural Networks, IEEE Transactions on, 5(2), 157–166.

Blackard J (1998). Comparison of Neural Networks and Discriminant Analysis in Predicting
Forest Cover Types. Ph.D. thesis, Department of Forest Sciences, Colorado State University.

Blei DM (2012). “Probabilistic Topic Models.” Communications of the ACM, 55(4), 77–84.

Bottou L (2012). “Stochastic Gradient Descent Tricks.” In Neural Networks: Tricks of the

Trade, volume 1, pp. 421–436.

Brown LD (1986). “Fundamentals of Statistical Exponential Families with Applications in

Statistical Decision Theory.” Lecture Notes-monograph series, pp. i–279.

Buckner J, Seligman M, Wilson J (2011). gputools: A Few GPU Enabled Functions. R

package version 0.26, URL http://CRAN.R-project.org/package=gputools.

Croissant Y (2013). mlogit: Multinomial Logit Model. R package version 0.2-4, URL http:

//CRAN.R-project.org/package=mlogit.

Dempster A, Laird N, Rubin D (1977). “Maximum Likelihood from Incomplete Data via the

EM Algorithm.” Journal of the Royal Statistical Society, Series B, 39, 1–38.

Dobson A, Barnett A (2008). An Introduction to Generalized Linear Models. Texts in Statis-

tical Science Series. CRC Press, London. ISBN 9781584889502.

Donoho D, Montanari A (2013). “High Dimensional Robust M-Estimation: Asymptotic Vari-

ance via Approximate Message Passing.” arXiv preprint arXiv:1310.7320v3.

Duchi J, Hazan E, Singer Y (2011). “Adaptive Subgradient Methods for Online Learning and
Stochastic Optimization.” The Journal of Machine Learning Research, 999999, 2121–2159.

Eddelbuettel D, Emerson JW, Kane MJ (2015). BH: Boost C++ Header Files. R package

version 1.58.0-1, URL http://CRAN.R-project.org/package=BH.

Eddelbuettel D, Fran¸cois R (2011). “Rcpp: Seamless R and C++ Integration.” Journal of

Statistical Software, 40(8), 1–18. URL http://www.jstatsoft.org/v40/i08/.

Journal of Statistical Software

21

Eddelbuettel D, Sanderson C (2014).

“RcppArmadillo: Accelerating R with hHgh-
performance C++ Linear Algebra.” Computational Statistics and Data Analysis, 71, 1054–
1063. URL http://dx.doi.org/10.1016/j.csda.2013.02.005.

Enea M, Meiri R, Kalimi T (2015). speedglm: Fitting Linear and Generalized Linear Models
to Large Data Sets. R package version 0.3, URL http://CRAN.R-project.org/package=
speedglm.

Fisher RA (1925). Statistical Methods for Research Workers. Oliver and Boyd, Edinburgh.

Friedman J, Hastie T, Tibshirani R (2010). “Regularization Paths for Generalized Linear

Models via Coordinate Descent.” Journal of Statistical Software, 27(6), 957–968.

Giles RCSLL (2001). “Overﬁtting in Neural Nets: Backpropagation, Conjugate Gradient, and

Early Stopping.” In Advances in Neural Information Processing Systems.

Green PJ (1984). “Iteratively Reweighted Least Squares for Maximum Likelihood Estimation,
and Some Robust and Resistant Alternatives.” Journal of the Royal Statistical Society.
Series B (Methodological), pp. 149–192.

Hasan A, Zhiyu W, Mahani AS (2015). mnlogit: Multinomial Logit Model. R package version

1.2.2, URL http://CRAN.R-project.org/package=mnlogit.

Hastie T, Efron B (2013).

lars: Least Angle Regression, Lasso and Forward Stagewise. R

package version 1.2, URL http://CRAN.R-project.org/package=lars.

Helleputte T (2015). LiblineaR: Linear Predictive Models Based on the LIBLINEAR C/C++

Library. R package version 1.94-2.

Hochreiter S (1998). “The Vanishing Gradient Problem During Learning Recurrent Neu-
ral Nets and Problem Solutions.” International Journal of Uncertainty, Fuzziness and
Knowledge-Based Systems, 6(02), 107–116.

Hochreiter S, Bengio Y, Frasconi P, Schmidhuber J (2001). “Gradient Flow in Recurrent Nets:

The Diﬃculty of Learning Long-term Dependencies.”

Huber P (1964). “Robust Estimation of a Location Parameter.” The Annals of Mathematical

Statistics, 35(1), 73–101.

Huber P, Ronchetti E (2009). Robust Statistics. Wiley Series in Probability and Statistics.

Jain P, Tewari A, Kar P (2014). “On Iterative Hard Thresholding Methods for High-
dimensional M-Estimation.” In Advances in Neural Information Processing Systems, pp.
685–693.

Kane MJ, Emerson J, Weston S (2013). “Scalable Strategies for Computing with Massive
Data.” Journal of Statistical Software, 55(14), 1–19. URL http://www.jstatsoft.org/
v55/i14/.

Krakowski KA, Mahony RE, Williamson RC, Warmuth MK (2007). “A Geometric View of

Non-Linear On-Line Stochastic Gradient Descent.” Author website.

22

Package sgd for estimation with large data sets

Lambert-Lacroix S, Zwald L, et al. (2011). “Robust Regression Through the Huberˆa ˘A´Zs
Criterion and Adaptive Lasso Penalty.” Electronic Journal of Statistics, 5, 1015–1053.

Le Cun Y, Bottou L, Bengio Y, Haﬀner P (1998). “Gradient-based Learning Applied to

Document Recognition.” Proceedings of IEEE, 86(11), 2278–2324.

Lehmann EL, Casella G (1998). Theory of Point Estimation, volume 31. Springer Science &

Business Media.

Lewis D, Yang Y, Rose T, Li F (2004). “RCV1: A New Benchmark Collection for Text

Categorization Research.” The Journal of Machine Learning Research, 5, 361–397.

Li G, Peng H, Zhu L (2011). “Nonconcave Penalized M-estimation with a Diverging Number

of Parameters.” Statistica Sinica, 21(1), 391.

Lumley T (2013). biglm: Bounded Memory Linear and Generalized Linear Models. R package

version 0.9-1, URL http://CRAN.R-project.org/package=biglm.

Lustig M, Donoho DL, Santos JM, Pauly JM (2008). “Compressed Sensing MRI.” Signal

Processing Magazine, IEEE, 25(2), 72–82.

National Research Council (2013). Frontiers in Massive Data Analysis. The National

Academies Press, Washington, DC.

Nelder J, Wedderburn R (1972). “Generalized Linear Models.” Journal of the Royal Statistical

Society. Series A (General), pp. 370–384.

Nemirovski A, Juditsky A, Lan G, Shapiro A (2009). “Robust Stochastic Approximation
Approach to Stochastic Programming.” SIAM Journal on Optimization, 19(4), 1574–1609.

Nesterov Y (1983). “A Method of Solving a Convex Programming Problem with Convergence

Rate O(1/k2).” In Soviet Mathematics Doklady, volume 27, pp. 372–376.

Nui F, Recht B, Re C, Wright SJ (2011). “Hogwild!: A Lock-Free Approach to Parallelizing
Stochastic Gradient Descent.” In Advances in Neural Information Processing Systems.

Owen AB (2007). “A Robust Hybrid of Lasso and Ridge Regression.” Contemporary Mathe-

matics, 443, 59–72.

Pascanu R, Mikolov T, Bengio Y (2012). “On the Diﬃculty of Training Recurrent Neural

Networks.” arXiv preprint arXiv:1211.5063.

Polyak BT (1964). “Some Methods of Speeding Up the Convergence of Iteration Methods.”

USSR Computational Mathematics and Mathematical Physics, 4(5), 1–17.

Polyak BT, Juditsky AB (1992). “Acceleration of Stochastic Approximation by Averaging.”

SIAM Journal on Control and Optimization, 30(4), 838–855.

Ranganath R, Tang L, Charlin L, Blei DM (2015). “Deep Exponential Families.” In Artiﬁcial

Intelligence and Statistics.

Rezende DJ, Mohamed S (2015). “Variational Inference with Normalizing Flows.” In Inter-

national Conference on Machine Learning.

Journal of Statistical Software

23

Robbins H, Monro S (1951). “A Stochastic Approximation Method.” The Annals of Mathe-

matical Statistics, pp. 400–407.

Rockafellar RT (1976). “Monotone Operators and the Proximal Point Algorithm.” SIAM

journal on control and optimization, 14(5), 877–898.

Ruppert D (1988). “Eﬃcient Estimations from a Slowly Convergent Robbins-Monro Process.”

Technical report, Cornell University Operations Research and Industrial Engineering.

Sakrison DJ (1965). “Eﬃcient Recursive Estimation; Application to Estimating the Parame-
ters of a Covariance Function.” International Journal of Engineering Science, 3(4), 461–483.

Schaul T, Antonoglou I, Silver D (2014). “Unit Tests for Stochastic Optimization.”

Schmidt M, Le Roux N, Bach F (2013). “Minimizing Finite Sums with the Stochastic Average

Gradient.” Technical report, HAL 00860051.

Shalev-Shwartz S, Zhang T (2013). “Stochastic Dual Coordinate Ascent Methods for Regu-

larized Loss.” The Journal of Machine Learning Research, 14(1), 567–599.

Shamir O, Zhang T (2012). “Stochastic Gradient Descent for Non-smooth Optimization:
Convergence Results and Optimal Averaging Schemes.” arXiv preprint arXiv:1212.1824.

Sonnenburg S, Franc V, Yom-Tov E, Sebag M (2008). “Pascal Large Scale Learning Chal-

lenge.” URL http://largescale.first.fraunhofer.de.

Srivastava N, Hinton G, Krizhevsky A, Sutskever I, Salakhutdinov R (2014). “Dropout:
A Simple Way to Prevent Neural Networks from Overﬁtting.” The Journal of Machine
Learning Research, 15(1), 1929–1958.

Tieleman T, Hinton G (2012). “Lecture 6.5—RmsProp: Divide the Gradient by a Running
Average of its Recent Magnitude.” COURSERA: Neural Networks for Machine Learning.

Toulis P, Airoldi E, Rennie J (2014). “Statistical Analysis of Stochastic Gradient Methods
for Generalized Linear Models.” In Proceedings of the 31st International Conference on
Machine Learning (ICML-14), pp. 667–675.

Toulis P, Airoldi EM (2015a). “Implicit Stochastic Gradient Descent.” arXiv preprint

arXiv:1408.2923.

Toulis P, Airoldi EM (2015b). “Scalable Estimation Strategies Based on Stochastic Approxi-
mations: Classical Results and New Insights.” Statistics and computing, 25(4), 781–795.

Toulis P, Tran D, Airoldi EM (2015). “Stability and Optimality in Stochastic Gradient De-

scent.” arXiv preprint arXiv:1505.02417v1.

Tran D, Toulis P, Airoldi EM (2015). sgd: Stochastic Gradient Descent for Scalable Estima-

tion. R package version 1.0, URL https://github.com/airoldilab/sgd.

Venables WN, Ripley BD (2002). Modern Applied Statistics with S. Fourth edition. Springer,

New York. ISBN 0-387-95457-0, URL http://www.stats.ox.ac.uk/pub/MASS4.

24

Package sgd for estimation with large data sets

Wickham H (2009). ggplot2: Elegant Graphics for Data Analysis. Springer New York. URL

http://had.co.nz/ggplot2/book.

Wickham H (2011). “testthat: Get Started with Testing.” The R Journal, 3, 5–10. URL
http://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf.

Xu W (2011). “Towards Optimal One Pass Large Scale Learning with Averaged Stochastic

Gradient Descent.” arXiv preprint arXiv:1107.2490.

Yee TW (2010). “The VGAM Package for Categorical Data Analysis.” Journal of Statistical

Software, 32(10), 1–34.

Yi C (2015). hqreg: Regularization Paths for Huber Loss Regression and Quantile Regression
Penalized by Lasso or Elastic-Net. R package version 1.0, URL http://CRAN.R-project.
org/package=hqreg.

Yu H (2002). “Rmpi: Parallel Statistical Computing in R.” R News, 2(2), 10–14. URL

http://cran.r-project.org/doc/Rnews/Rnews_2002-2.pdf.

Zou H, Hastie T (2005). “Regularization and Variable Selection via the Elastic Net.” Journal

of the Royal Statistical Society: Series B (Statistical Methodology), 67(2), 301–320.

Zou H, Hastie T (2012). elasticnet: Elastic-Net for Sparse Estimation and Sparse PCA. R

package version 1.1, URL http://CRAN.R-project.org/package=elasticnet.

Aﬃliation:

Dustin Tran, Panos Toulis, Edoardo M. Airoldi
Department of Statistics
Harvard University
1 Oxford Street, Cambridge, MA 02138, USA
E-mail: dtran@g.harvard.edu, ptoulis@fas.harvard.edu, airoldi@fas.harvard.edu

Journal of Statistical Software
published by the American Statistical Association

Volume VV, Issue II
MMMMMM YYYY

http://www.jstatsoft.org/
http://www.amstat.org/

Submitted: yyyy-mm-dd

Accepted: yyyy-mm-dd

JSS

Journal of Statistical Software

MMMMMM YYYY, Volume VV, Issue II.

http://www.jstatsoft.org/

5
1
0
2
 
p
e
S
 
2
2
 
 
]

O
C

.
t
a
t
s
[
 
 
1
v
9
5
4
6
0
.
9
0
5
1
:
v
i
X
r
a

Stochastic gradient descent methods
for estimation with large data sets

Dustin Tran
Harvard University

Panos Toulis
Harvard University

Edoardo M. Airoldi
Harvard University

Abstract

We develop methods for parameter estimation in settings with large-scale data sets,
where traditional methods are no longer tenable. Our methods rely on stochastic approx-
imations, which are computationally eﬃcient as they maintain one iterate as a parameter
estimate, and successively update that iterate based on a single data point. When the
update is based on a noisy gradient, the stochastic approximation is known as standard
stochastic gradient descent, which has been fundamental in modern applications with large
data sets. Additionally, our methods are numerically stable because they employ implicit
updates of the iterates. Intuitively, an implicit update is a shrinked version of a stan-
dard one, where the shrinkage factor depends on the observed Fisher information at the
corresponding data point. This shrinkage prevents numerical divergence of the iterates,
which can be caused either by excess noise or outliers. Our sgd package in R oﬀers the
most extensive and robust implementation of stochastic gradient descent methods. We
demonstrate that sgd dominates alternative software in runtime for several estimation
problems with massive data sets. Our applications include the wide class of generalized
linear models as well as M-estimation for robust regression.

Keywords: stochastic gradient descent, implicit updates, massive data, exponential family,
generalized linear models, M-estimation.

1. Introduction

Massive data sets as well as streaming data, in which one observes only a group of data points
at a time, are becoming increasingly common in modern statistical analysis. Under the setting
of hundreds of millions of observations and hundreds or thousands of covariates (National
Research Council 2013), it becomes diﬃcult to estimate the parameters of a statistical model;
the three ideal properties are computational eﬃciency, statistical optimality, and numerical
stability, and it is challenging to address all three with a single estimation method.

2

Package sgd for estimation with large data sets

More formally, suppose there exists a vector of parameters θ(cid:63) ∈ Rp and that we observe
i.i.d. samples D = {xn, yn}, for n = 1, 2, . . . , N ; in the nth data point (xn, yn), the out-
come yn ∈ Rd is distributed conditional on covariates xn ∈ Rp according to a known den-
sity f (yn; xn, θ(cid:63)), and thus the log-likelihood function for the entire data set D is given by
(cid:96)(θ; D) = (cid:80)N
n=1 log f (yn; xn, θ). The task is to estimate the true parameter value θ(cid:63) when N
is inﬁnite (streaming setting), or to approximate some estimator of θ(cid:63), such as the maximum-
likelihood estimator θmle = arg maxθ∈Rp (cid:96)(θ; D), when N is ﬁnite.

Widely used methods for statistical estimation, such as Fisher scoring, the EM algorithm, and
iteratively reweighted least squares (Fisher 1925; Dempster, Laird, and Rubin 1977; Green
1984) are not feasible in such settings; either they strictly do not apply in the streaming
setting (inﬁnite N ), or they do not scale to large data (ﬁnite but large N ). Fisher scoring, for
example, requires at each iteration the inversion of a p × p matrix and evaluation of the log-
likelihood over the full data set D. This roughly yields O(N p2+(cid:15)) running time complexity,
which is prohibitive when N and p are large. In contrast, estimation with massive data sets
typically requires a running time complexity that is O(N p1−(cid:15)), i.e., that is linear in N but
sublinear in the parameter dimension p.

Such performance is achieved in general by the stochastic gradient descent (sgd) algorithm,
which was initially proposed by Sakrison (1965) as a modiﬁcation of the Robbins-Monro
It is deﬁned through the
procedure (Robbins and Monro 1951) for recursive estimation.
iteration

n = θsgd
θsgd

n−1 + γnCn∇ log f (yn; xn, θsgd

n−1).

(1)

n

We will refer to Equation 1 as sgd with explicit updates, or explicit sgd for short, because the
next iterate θsgd
can be computed immediately after the nth data point (xn, yn) is observed.
The sequence γn > 0 is the learning rate sequence, and is typically deﬁned such that nγn →
γ > 0 as n → ∞; the hyperparameter γ > 0 is ﬁxed and known as the learning rate parameter.
The sequence {Cn} is a sequence of positive-deﬁnite matrices, such that Cn → C with C
known, and is used to better condition the iteration; in the simplest case Cn = I, i.e., we
simply use the identity matrix, which results in ﬁrst-order explicit sgd.

From a computational perspective, explicit sgd is eﬃcient because it replaces the expensive
inversion of p × p matrices, as in Fisher scoring, by a scalar sequence γn > 0 and a matrix
Cn that is fast to manipulate numerically, by design. Furthermore, the log-likelihood is
evaluated at a single observation yn given xn, rather than the entire data set D, which saves
signiﬁcant computation time. From a theoretical perspective, explicit sgd is justiﬁed because
the theory of stochastic approximations (Robbins and Monro 1951, Theorem 1) implies that
θsgd
converges to a point θ∞ such that E (∇ log f (yn; xn, θ∞)) = 0. Under standard statistical
n
theory, E (∇ log f (yn; xn, θ(cid:63))) = 0, and this point is unique under typical regularity conditions
(Lehmann and Casella 1998, Theorem 5.1, p.463), such as concavity of log-likelihood; this
is true, for example, in the popular exponential family of statistical models (Brown 1986).
Therefore, θ∞ = θ(cid:63), i.e., explicit sgd converges to the true parameter value. In the ﬁnite
N setting, a similar condition holds where θsgd
approximates θmle if the nth data point in
n
Equation 1 is an unbiased sample from the total N data points; see also Toulis and Airoldi
(2015b) for a review of applications of sgd on modern machine learning applications.

Despite these theoretical guarantees, explicit sgd requires careful tuning of the hyperparam-
eter γ in the learning rate: small values of the parameter make the iteration (1) very slow

Journal of Statistical Software

3

to converge in practice, whereas large values can cause numerical divergence. Moreover, it is
known that explicit sgd is statistically ineﬃcient even when γ is correctly speciﬁed (Toulis,
Airoldi, and Rennie 2014). In particular, the amount of information loss from procedure (1)
depends on the spectral gap of the Fisher information matrix, I(θ) = −E (cid:0)∇2 log f (yn; xn, θ)(cid:1),
calculated at the true parameter value θ = θ(cid:63). A large spectral gap makes it hard, or even im-
possible, to make the learning rates large enough for fast convergence, and also small enough
for stability (Toulis and Airoldi 2015a, Section 3.5).

Motivated by these challenges, Toulis, Tran, and Airoldi (2015) introduced averaged implicit
stochastic gradient descent (ai-sgd), which is deﬁned by the procedure

(2)

(3)

(4)

n = θim
θim

n−1 + γnCn∇ log f (yn; xn, θim

n ),

¯θn = (1/n)

θim
i

.

n
(cid:88)

i=1

The ﬁrst key component of ai-sgd is the implicit update (2). Note that it is implicit because
the next iterate θim
n appears on both sides of the equation. This simple modiﬁcation of the
explicit sgd procedure oﬀers several statistical advantages. In particular, assuming a common
starting point θsgd
(cid:44) θ0, one can show through a Taylor approximation of (2) around
θ0 that the implicit update satisﬁes

n−1 = θim
n−1

∆θim

n = (I + γnCnˆI(θ0; xn, yn))−1∆θsgd

n + O(γ2

n),

where ∆θn = θn − θn−1 for both methods, I is the identity matrix, and ˆI(θ0; xn, yn) =
−∇2(cid:96)(θ0; xn, yn) is the observed Fisher information matrix at θ0 (equivalent to the Hessian of
the negative log-likelihood at θ0). Equation 4 implies that the implicit update (2) is a shrinked
version of the explicit update (1). This shrinkage makes the iterations signiﬁcantly more stable
in small-to-moderate samples, and also robust to misspeciﬁcations of the learning rate param-
eter γ (Toulis et al. 2014). The implicit update (2) also has a Bayesian interpretation, where
n is the posterior mode of a model with the standard multivariate normal N (θim
θim
n−1, γnCn)
as the prior, and f (θ; xn, yn) as the likelihood. Thus it provides an iterative form of regu-
larization. In optimization, update (2) is known as a proximal update, and corresponds to a
stochastic version of the proximal point algorithm (Rockafellar 1976). Krakowski, Mahony,
Williamson, and Warmuth (2007) and Nemirovski, Juditsky, Lan, and Shapiro (2009) have
shown that proximal methods ﬁt better in the geometry of the parameter space.

The second key component of ai-sgd is iterate averaging (3), which guarantees optimal sta-
tistical eﬃciency under fairly relaxed conditions. Ruppert (1988) and Polyak and Juditsky
(1992) ﬁrst proved that averaging of iterates can achieve statistical optimality in the stan-
dard context of stochastic approximation with explicit updates; Toulis et al. (2015) extended
this result to the implicit sgd update (2). Thus, ai-sgd is eﬀectively a recursive estimation
method that is both statistically optimal and numerically stable, while remaining applicable
to the setting of massive and/or streaming data.

In this paper we develop statistically eﬃcient sgd algorithms for generalized linear models—
extending Algorithm 1 of Toulis et al. (2014)—and also develop sgd algorithms to perform
high-dimensional M-estimation. This allows for scalable estimation of such models with mas-
sive and/or streaming data. We provide a publicly available package sgd (Tran, Toulis, and
Airoldi 2015) written in R, which implements ai-sgd, as well as other sgd variants. In Section

4

Package sgd for estimation with large data sets

2, we develop the algorithms. Section 3 contains experiments on simulated and real-world
data, in which we demonstrate the advantages of the sgd package compared to alternative
software. In Section 4, we describe the interface of sgd and implementation details for its use
in practice.

2. Algorithms

In this section we develop algorithms which implement implicit sgd and ai-sgd for gener-
alized linear models as well as M-estimation. We start by introducing an algorithm which
eﬃciently computes a generalization of implicit update (2), which is useful for the aforemen-
tioned applications.

2.1. Eﬃcient computation of implicit updates

The main diﬃculty in applying ai-sgd is the solution of the multidimensional ﬁxed point
equation for the implicit update (2). In the large class of models where the likelihood given
covariate x depends on the parameter θ only through the natural parameter η ≡ x(cid:62)θ, the
solution of the ﬁxed-point equation is computationally eﬃcient. The general result is given
in Theorem 2.1, whereas the assumption is made more precise below.
Assumption 2.1. The likelihood (cid:96)(θ; xn, yn) ≡ log f (yn; xn, θ) of parameter value θ given
data point (xn, yn) depends on θ only through the product x(cid:62)

n θ, i.e.,

(cid:96)(θ; xn, yn) ≡ (cid:96)(x(cid:62)

n θ; xn, yn).

A key implication of Assumption 2.1 is that the direction of the gradient of the log-likelihood
does not depend on the parameter value since ∇ log f (yn; xn, θ) = (cid:96)(cid:48)(x(cid:62)
n θ; xn, yn)xn, where
the latter derivative is with respect to the natural parameter x(cid:62)
n θ and with ﬁxed data xn, yn.
This property is crucial because it implies that the implicit update (2) can be performed once
a scalar value is found that will appropriately scale the gradient.

Theorem 2.1. Suppose Assumption 2.1 holds. Then the gradient for the implicit iterate θim
n
(2) is a scaled version of the gradient at the previous iterate, i.e.,

∇ log f (yn; xn, θim

n ) = sn∇ log f (yn; xn, θim

n−1).

The scalar sn ∈ R satisﬁes

snκn−1 = (cid:96)(cid:48) (cid:16)

n θim
x(cid:62)

n−1 + γnsnκn−1x(cid:62)

n Cnxn; xn, yn

(cid:17)

,

(5)

(6)

(7)

n θim

where κn−1 = (cid:96)(cid:48)(x(cid:62)
n−1; xn, yn).
Theorem 2.1 shows that the gradient ∇ log f (yn; xn, θim
n ) in the implicit update (2) is in fact
a scaled version of the gradient ∇ log f (yn; xn, θim
n−1) that would appear in update (2) if we
were applying explicit updates. Therefore, computing the implicit update reduces to ﬁnding
the scale factor sn ∈ R. See Toulis and Airoldi (2015a, Threorem 4.1) for a proof.

It is possible to regularize both explicit and implicit sgd by adding
Penalized likelihood.
a penalty to the log-likelihood. In particular, we consider the elastic net (Zou and Hastie

Journal of Statistical Software

5

2005), where for some ﬁxed α ∈ [0, 1] the penalty function is

1
2
Adding the elastic net with a regularization parameter λ ∈ R to explicit sgd is straightfor-
ward:

Pα(θ) = (1 − α)

2 + α(cid:107)θ(cid:107)1.

(cid:107)θ(cid:107)2

(8)

n = θsgd
θsgd

n−1 + γnCn(∇ log f (yn; xn, θsgd

n−1) − λ∇Pα(θsgd

n−1)),

where the gradient of the elastic net penalty is given by

∇Pα(θim

n−1) = (1 − α)θsgd

n−1 + α sign(θsgd

n−1).

Here, the operation sign(θ) is the element-wise sign operation, outputting 1 if θj > 0, −1 if
θj < 0, and 0 otherwise.
For implicit sgd the update would be

n = θim
θim

n−1 + γnCn(∇ log f (yn; xn, θim

n ) − λ∇Pα(θim

n )).

(11)

However, it is not generally possible to compute update (11). For example, Assumption
2.1 does not hold because the gradient of the log-likelihood and the gradient of the penalty
generally have two diﬀerent directions. This breaks the argument of Theorem 2.1, where the
direction of the update calculated at the next iterate θim
n is the same as the direction of the
update calculated at the previous iterate θim

n−1.

To circumvent this problem, we simply penalize the previous iterate instead of the current,
i.e., perform the update

n = θim
θim

n−1 + γnCn(∇ log f (yn; xn, θim

n ) − λ∇Pα(θim

n−1)).

Then update (12) is equivalent to

n = θim
θim

n−1 + γnCn(sn∇ log f (yn; xn, θim

n−1) − λ∇Pα(θim

n−1)),

where the scale factor sn satisﬁes
snκn−1 = (cid:96)(cid:48) (cid:16)

n θim
x(cid:62)

n−1 − γnλx(cid:62)

n Cn∇Pα(θim

n−1) + γnsnκn−1x(cid:62)

n Cnxn; xn, yn

(cid:17)

,

(14)

and where κn−1 = (cid:96)(cid:48)(x(cid:62)
identical to the proof of Theorem 2.1.

n θim

n−1; xn, yn). A proof for this case with penalized likelihoods is

Final algorithm for implicit updates. This analysis leads to Algorithm 1, which, for
models satisfying Assumption 2.1, implements the most general update (13) of implicit sgd
with conditioning matrices and penalty. This algorithm applies a root-ﬁnding procedure
solving Equation 14 at every iteration, which is fast because the equation is one-dimensional
and the search bounds for the solution are known, having a diminishing range O(γn). Indeed,
the one-dimensional search is computationally negligible in practice, as we see in Section
3.

We also note that because the implicit update (17) eﬀectively does regularization as a shrink-
age estimate (see Equation 4), the use of penalization is not as crucial in practice as it is for

(9)

(10)

(12)

(13)

6

Package sgd for estimation with large data sets

Algorithm 1 Eﬃcient implementation of implicit update (13)
1: function implicit update((cid:96)(cid:48)(·; ·), γn, θim
2:

n−1, xn, yn, Cn, Pα)

n θim

# Compute search bounds B
rn ← γn(cid:96)(cid:48) (cid:0)x(cid:62)
(cid:1)
n−1; xn, yn
B ← [0, rn]
if rn ≤ 0 then
B ← [rn, 0]

3:

4:

5:

6:

8:

7:

9:

end if
# Solve ﬁxed-point equation by a root-ﬁnding method
ξ = γn(cid:96)(cid:48)(x(cid:62)
sn ← ξ/rn
# Equivalent to implicit update (13)
return θim
n θim
12:
13: end function

n Cn∇Pα(θim

n−1 − γnλx(cid:62)

n−1) + ξx(cid:62)

n−1 + γnCn

(cid:0)sn(cid:96)(cid:48) (cid:0)x(cid:62)

n−1; xn, yn

n θim

11:

10:

n Cnxn; xn, yn), ξ ∈ B

(cid:1) xn − λ∇Pα(θim

n−1)(cid:1)

explicit updates. We make extensive experiments using Algorithm 2 and also examine this
eﬀect in Section 3.

2.2. Generalized linear models

In the family of generalized linear models (GLMs), the outcome yn ∈ R follows an exponential
family distribution conditional on xn,

yn | xn ∼ exp

(ηnyn − b(ηn))

c(yn, ψ),

ηn ≡ x(cid:62)

n θ(cid:63),

(15)

(cid:26) 1
ψ

(cid:27)

where the scalar ψ > 0 is the dispersion parameter which aﬀects the variance of the outcome,
c(·, ·) is the base measure, and b(·) is the log normalizer which ensures that the distribution
integrates to one.1 Additionally, in a GLM it is assumed that E (yn| xn) = h(x(cid:62)
n θ(cid:63)), where
h : R → R is known as the transfer function (Nelder and Wedderburn 1972; Dobson and
Barnett 2008). A simple property of GLMs is that the transfer function is the ﬁrst derivative
of the log normalizer, i.e., h(x(cid:62)

n θ), for all xn, θ.

n θ) = b(cid:48)(x(cid:62)

A straightforward implementation of explicit sgd for estimation with GLMs is

n = θsgd
θsgd

n−1 + γnCn[yn − h(x(cid:62)

n θsgd

n−1)]xn.

Similarly, the ai-sgd procedure can be written as

(16)

(17)

n = θim
θim

¯θn =

n−1 + γnCn[yn − h(x(cid:62)
n
1
(cid:88)
n

θim
n .

i=1

n θim

n )]xn,

By assumption, (cid:96)(θ; yn, xn) ∝ (x(cid:62)
n θ(cid:63)), and thus the log-likelihood depends on
parameter value θ(cid:63) only through its linear combination with covariate value xn. Additionally,

n θ(cid:63))yn − b(x(cid:62)

1We present one-dimensional outcomes for simplicity. However, our theory easily extends to multidimen-

sional outcomes. Such an extension is given, for example, in Section 2.3 on M-estimation.

Journal of Statistical Software

7

Algorithm 2 Estimation of generalized linear models with ai-sgd
1: Initialize θim
2: for n = 1, 2, . . . do
3:

n θ; xn, yn) ≡ yn − h(x(cid:62)

0 , ¯θ0

n θ)

Deﬁne (cid:96)(cid:48)(x(cid:62)
Calculate implicit update

4:

θim
n ← IMPLICIT UPDATE((cid:96)(cid:48)(·; ·), γn, θim

n−1, xn, yn, Cn, Pα)

¯θn ← n−1
n

¯θn−1 + 1

n θim
n

5:
6: end for

Var (yn|xn) = h(cid:48)(x(cid:62)
and concave, thus fulﬁlling Assumption 2.1.

n θ(cid:63))||xn||2, and thus h(cid:48) ≥ 0, which implies that (cid:96) is twice-diﬀerentiable

Penalized likelihood. As argued before, one can add the elastic penalty by applying it to
the previous estimate instead of the current. That is, for ﬁxed α ∈ [0, 1] and regularization
parameter λ ∈ R, the ai-sgd procedure for generalized linear models with elastic net is

n = θim
θim

n−1 + γnCn

[yn − h(x(cid:62)

n θim

n )]xn − λ∇Pα(θim

n−1)

(cid:16)

(cid:17)

,

¯θn =

1
n

n
(cid:88)

i=1

θim
n .

Algorithm 2 implements estimation of GLMs through ai-sgd based on updates (18).

2.3. M-Estimation

Given a data set of N observations D = {(xn, yn)} and a convex function ρ : R → R+, the
M-estimator is deﬁned as

ˆθm = arg min
θ∈Rp

ρ(yn − x(cid:62)

n θ),

N
(cid:88)

n=1

where it is assumed yn = x(cid:62)
n θ(cid:63)+(cid:15)n, and (cid:15)n are i.i.d. zero mean-valued noise. M-estimators are
especially useful in robust statistics (Huber 1964; Huber and Ronchetti 2009), as appropriate
choice of ρ can reduce the inﬂuence of outliers in data. Recently, there has been increased in-
terest in the literature for fast approximation of M-estimators due to their robustness (Donoho
and Montanari 2013; Jain, Tewari, and Kar 2014).

Typically in M-estimation, ρ is twice-diﬀerentiable around zero and

(cid:16)

E

ρ(cid:48)(yn − x(cid:62)
n

ˆθm)xn

= 0,

(cid:17)

where the expectation is over the empirical data distribution. Therefore sgd algorithms can
be applied to approximate the M-estimator ˆθm. Importantly, ρ is convex, which implies that
the conditions of Assumption 2.1 are met.

(18)

(19)

(20)

8

Package sgd for estimation with large data sets

Algorithm 3 M-estimation with ai-sgd
1: Initialize θim
2: for n = 1, 2, . . . do
3:

Deﬁne (cid:96)(cid:48)(x(cid:62)
Calculate implicit update

n θ; xn, yn) ≡ −ρ(cid:48)(yn − x(cid:62)

0 , ¯θ0

4:

n θ)

¯θn ← n−1
n

¯θn−1 + 1

n θim
n

5:
6: end for

θim
n ← IMPLICIT UPDATE((cid:96)(cid:48)(·; ·), γn, θim

n−1, xn, yn, Cn, Pα)

The ai-sgd procedure for approximating M-estimators is

n = θim
θim

¯θn =

n−1 + γnCn[ρ(cid:48)(yn − x(cid:62)
n
1
(cid:88)
n

θim
n .

i=1

n θim

n )]xn,

(21)

(22)

An outline of the procedure is given in Algorithm 3. As before, Algorithm 3 also includes the
optional use of a sequence of conditioning matrices Cn and a penalty function Pα. The use
of penalization has particularly been considered as a way to merge the robustness properties
given by a choice of ρ with sparsity, e.g, through lasso (Owen 2007; Lambert-Lacroix, Zwald
et al. 2011; Li, Peng, and Zhu 2011).

It is also typical to assume that the density of (cid:15)n is symmetric around zero. Therefore, it
also holds E (cid:0)ρ(cid:48)(yn − x(cid:62)
(cid:1) = 0, where the expectation is over the true data distribution.
Hence sgd procedures can be used to estimate θ(cid:63) in the case of an inﬁnite stream of obser-
vations (N = ∞). We write Algorithm 3 for the case of ﬁnite N , but it is trivial to adapt the
procedure to inﬁnite N .

n θ(cid:63))xn

3. Experiments

In this section, we compare the sgd methods implemented in the sgd package, such as ex-
plicit sgd and ai-sgd, with standard, deterministic optimization methods that are widely
used in statistical practice, such as glmnet, biglm, and speedglm. We demonstrate in both
massive and streaming data settings that standard methods are not applicable, and further-
more that sgd methods outperform such methods upon orders of magnitude in runtime and
convergence.

As standard methods are not competitive, we also compare the proposed sgd methods to
each other, e.g., comparing ai-sgd to explicit sgd, across a wide range of learning rate
speciﬁcations, including adaptive speciﬁcations such as AdaGrad (Duchi, Hazan, and Singer
2011) and RMSProp (Tieleman and Hinton 2012); more details on the speciﬁcations which
are available in sgd are given in Section 4.3.

All timings are carried out on a general-purpose 2.6 GHz Intel Core i5 processor, and are
reported for various algorithms which reach a thresholded L2 distance to the true parameter
value.

Journal of Statistical Software

9

3.1. Linear regression with the lasso

We follow an experiment used in benchmarking the glmnet package (Friedman, Hastie, and
Tibshirani 2010, Section 5.1), which ﬁts GLMs with the elastic net penalty over a regulariza-
tion path. As glmnet was shown to outperform related software such as elasticnet (Zou and
Hastie 2012) and lars (Hastie and Efron 2013), we compare sgd strictly to glmnet. The design
matrix X with N observations and p predictors is generated from a normal distribution such
that each pair of predictors Xj, Xj(cid:48) has the same correlation ρ. Each of the N outcomes yn,
n = 1, 2, . . . N , is deﬁned as

yn = x(cid:62)

n θ(cid:63) + k(cid:15)n,

(23)

where θ(cid:63)j = (−1)j exp(−2(j − 1)/20) so that the elements of the true parameter value θ(cid:63) have
alternating signs and are exponentially decreasing. The noise (cid:15)n is distributed as a standard
normal, (cid:15) ∼ N (0, 1), and k is chosen so that the signal-to-noise ratio is equal to 3.0. We run
glmnet with “covariance updates”, which takes advantage of sparse updates in the parameter
space to reduce the complexity of O(N p) calculations per iteration. It performs better in our
experiments than the “naive update” also considered in Friedman et al. (2010).

Table 1 outlines results for a combination of triplets (N, p, ρ), ranging from N = 1, 000 ob-
servations and scaling up to N = 10 million. glmnet is seen to be competitive with sgd
procedures under the setting of N = 1, 000 observations, and in fact glmnet slightly outper-
forms sgd algorithms for lower dimensions of N and p. It is in any higher dimensional setting
where sgd strictly dominates glmnet, as seen in the table where for example, with N = 50, 000
and p = 10, 000, sgd is orders of magnitude faster.

Furthermore, glmnet is restricted by the memory limitations of computer hardware. For
example, simulations with 100, 000 observations and 10, 000 features require 8 GB in memory
for simply storing the data, and more is required for parameter storage and computational
overhead. For the sgd package, we simply stream the data points using bigmemory, which
requires less than 500 MB of RAM for all our experiments, a 16-fold decrease in memory
requirements. This is not possible for glmnet in either the case of real streaming data, or
simply as a way to remove memory bottlenecks. In principle, gradient descent algorithms
such as glmnet can read and destroy data memory from disk as it loops over the full data
set; however, this is impractical as it requires such an expensive memory access at each
iteration.

We now compare the sgd algorithms. For small dimensional problems, explicit sgd achieves
faster runtime than ai-sgd as it does not require a one-dimensional search following Algorithm
1. However, in high dimensions and high correlations, it becomes extremely diﬃcult for
explicit sgd to even converge for this toy linear model. It is sensitive to the learning rate, and
any misspeciﬁcation can cause it to diverge numerically. Thus, we were not able to obtain a
proper timing for explicit sgd in settings of either high correlation (ρ > 0.9) or high dimension
with medium correlation (ρ > 0.5). In practice one must tune the hyperparameter for explicit
sgd—thus requiring signiﬁcant computational overhead and user input—while also closely
monitoring the stochastic gradients for consideration of other numerical issues. ai-sgd on the
other hand uses additional computation per iteration, which in high dimensions is negligible
compared to the cost of a stochastic gradient update. This additional computation leads to
signiﬁcantly more robust updates and faster convergence.

10

Package sgd for estimation with large data sets

0

0.1

0.9

0.95

Correlation
0.2

0.5

N = 1, 000 p = 100 (sec)

sgd(method="ai-sgd")
sgd(method="sgd")
glmnet

0.03
0.02
0.02

0.03
0.02
0.02

0.03
0.02
0.02

0.03
0.02
0.02

0.04
0.03
0.02

0.34
0.03
0.03

N = 10, 000 p = 1, 000 (sec)

sgd(method="ai-sgd")
sgd(method="sgd")
glmnet

1.81
2.78
6.60

1.65
2.90
7.76

1.78
2.93
8.00

1.50
2.81
7.83

1.85
–
6.50

1.83
–
6.70

N = 50, 000 p = 10, 000 (min)

sgd(method="ai-sgd")
sgd(method="sgd")
glmnet

3.12
4.83
14.58

3.51
4.86
15.28

3.43
5.23
16.29

3.26
–
15.58

3.40
–
16.54

3.38
–
16.41

N = 1, 000, 000 p = 50, 000 (min)

sgd(method="ai-sgd")
sgd(method="sgd")
glmnet

22.23
27.80
–

21.10
34.08
–

19.88
–
–

21.52
–
–

18.53
–
–

20.53
–
–

N = 10, 000, 000 p = 100, 000 (hr)

sgd(method="ai-sgd")
sgd(method="sgd")
glmnet

9.38
13.50
–

10.20
–
–

9.58
–
–

8.54
–
–

10.11
–
–

10.74
–
–

Table 1: Linear regression with the lasso. Timing (in various units) is displayed for 100
λ values, averaged over 10 runs. The ﬁrst line is sgd using ai-sgd and the second line is
sgd using explicit sgd. Omitted entries indicate failure of the algorithm; for explicit sgd it
numerically diverges, and for glmnet it could not run due to memory limitations.

3.2. Logistic regression with ridge penalty

Following benchmarks that are popular in the machine learning and optimization literature
(Xu 2011; Shamir and Zhang 2012; Bach and Moulines 2013; Schmidt, Le Roux, and Bach
2013), we perform large-scale logistic regression on four data sets:

• rcv1 (Lewis, Yang, Rose, and Li 2004): text data set in which the task is to classify
documents belonging to class ccat, where we apply preprocessing provided by Bottou
(2012).

• covtype (Blackard 1998): data set consisting of forest cover types in which the task is

to classify for one speciﬁc class among 7 forest cover types.

Journal of Statistical Software

11

description

type

covariates

training set

test set

λ

covtype
delta
rcv1
mnist

sparse
forest cover type
dense
synthetic data
text data
sparse
digit image features dense

54
500
47,152
784

464,809
450,000
781,265
60,000

116,203
50,000
23,149
10,000

10−6
10−2
10−5
10−3

Table 2: Summary of data sets and the L2 regularization parameter λ used.

• delta (Sonnenburg, Franc, Yom-Tov, and Sebag 2008): synthetic data oﬀered in the
PASCAL Large Scale Challenge. We apply the default processing oﬀered by the chal-
lenge organizers.

• mnist (Le Cun, Bottou, Bengio, and Haﬀner 1998): images of handwritten digits, where

the task is to classify digit 9 against all others.

A summary of the data sets is available in Table 3, where the number of observations are
typically on the order of several hundred thousand, and the covariates range from a few dozen
to tens of thousands. The regularization parameter λ for the ridge penalty are set according
to those used in Xu (2011).

We compare to the following three packages: biglm (Lumley 2013) and speedglm (Enea, Meiri,
and Kalimi 2015), both of which perform approximate updates using iteratively reweighted
least squares, and LiblineaR (Helleputte 2015), which is a simple wrapper to a C++ library
for regularized linear classiﬁcation. We use the stochastic dual coordinate ascent algorithm
(Shalev-Shwartz and Zhang 2013) in LiblineaR. In addition, we consider the mnlogit package
(Hasan, Zhiyu, and Mahani 2015), which implements multinomial logistic regression using the
classical technique of Newton-Raphson, and exploits iterations over intermediate data struc-
tures for fast Hessian calculations. For modest-sized problems, mnlogit is shown to be 10-50
times faster than mlogit (Croissant 2013), VGAM (Yee 2010), and the multinom function in
nnet (Venables and Ripley 2002). Finally, we also run the default function glm.fit as a base-
line. We note that mnlogit and glm.fit can be only employed for standard (unregularized)
multinomial regression, so we run them without the ridge penalty.

Table 3 outlines the runtimes for the considered packages. The two sgd algorithms are orders
of magnitude faster than its competitors on all data sets. Interestingly, biglm and speedglm
failed to run on the three real data sets when attempting to invert subsets of the data, and
only succeeded for the one synthetic data set delta. We also note that the largest data
set—rcv1—failed for the majority of algorithms: only the packages sgd and LiblineaR were
able to converge, both of which natively use stochastic gradients for computationally eﬃcient
updates. However, sgd is signiﬁcantly faster because less overhead seems to be involved in
passing data structures to perform computation in native C++.

Moreover, sgd requires O(p) memory, which is optimal in the sense that O(p) is the minimum
n . Both biglm and speedglm require O(p2) for the
required for simply storing the nth iterate θim
inversion of a p×p matrix, as do mnlogit and glm.fit. The mnlogit package also requires data
in the long format, which leads to a duplication of rows, as many entries display redundant
information. Moreover, while exploitation of the Hessian structure can help in practice (as it
outperforms glm.fit), we observe that the traditional technique of Newton-Raphson remains

12

Package sgd for estimation with large data sets

data set

sgd (ai-sgd)

sgd (sgd) biglm speedglm LiblineaR mnlogit

glm.fit

covtype
delta
rcv1
mnist

5.21
10.10
14.15
3.50

7.58
10.23
15.42
3.37

–
736.13
–
–

–
30.50
–
–

1444.78
2167.14
133.10
208.55

16.04
445.73
–
232.53

40.11
498.97
–
890.76

Table 3: Large-scale logistic regression on four data sets. Timing (in seconds) is displayed,
averaged over 10 runs. Omitted entries indicate failure of the algorithm; for biglm and
speedglm, it could not run due to inversions of singular matrices; for mnlogit it could not run
due to memory limitations.

Figure 1: Large scale logistic regression on four data sets. Each plot indicates the classiﬁca-
tion error on the test set for explicit sgd with AdaGrad, ai-sgd, averaged sgd, and explicit
sgd over a pass of the data.

Journal of Statistical Software

13

N

p

sgd (ai-sgd)

sgd (sgd) hqreg units

1,000
10,000
10,000
50,000
100,000
1,000,000
10,000,000
100,000

100
500
1,000
10,000
50,000
100,000
100,000
1,000,000

0.05
0.55
1.30
3.12
8.13
35.88
8.64
18.80

0.04
0.46
2.22
3.86
15.20
51.93
9.55
26.43

0.03
0.40
6.34
15.57
–
–
–
–

(sec)
(sec)
(sec)
(min)
(min)
(min)
(hr)
(hr)

Table 4: High-dimensional M-estimation with the Huber loss. Timing (in units given by the
last column) is displayed for 100 λ values, averaged over 10 runs. Omitted entries indicate
failure of the algorithm; for hqreg, it could not run due to memory limitations.

untenable because it still requires O(N p2) complexity per iteration in the worst case.

For demonstration, Figure 1 shows the progress of multiple sgd algorithms available in sgd (see
Section 4.2) over a pass of the data. We note that ai-sgd achieves the fastest or competitive
convergence rates, without requiring signiﬁcant tuning of parameters as the other algorithms
do; this includes popular adaptive learning rate speciﬁcations, such as explicit sgd with
AdaGrad.

3.3. M-estimation with the Huber loss

We follow an example for high-dimensional M-estimation in Donoho and Montanari (2013,
Section 2.4). Deﬁne the convex function ρ : R → R+ to be the Huber loss,

ρ(z; λ) =

(cid:40)

z2/2,
λ|z| − λ2/2,

if |z| ≤ λ,
otherwise.

Fix the thresholding parameter λ = 3, and generate the N ×p design matrix with i.i.d. entries
Xi,j ∼ N (0, 1
N ). We ﬁx the true set of parameters θ(cid:63) to be a vector randomly drawn with
ﬁxed norm (cid:107)θ(cid:63)(cid:107)2 = 6

p, and then generate outcome yn, n = 1, 2, . . . , N , as

√

yn = x(cid:62)

n θ(cid:63) + (cid:15)n.

(24)

For the distribution of errors (cid:15)n, we use Huber’s contaminated normal distribution CN(0.05, 10),
i.e., (cid:15)n ∼ 0.95z + 0.05h10, i.i.d., where z is standard normal and hx is a point mass at x.

Few alternative packages to sgd exist for high-dimensional robust estimation. We compare to
hqreg (Yi 2015), which ﬁts regularization paths for Huber loss regression with the elastic net
penalty. Note that hqreg is specialized to the Huber loss and cannot perform estimation for
the general setting of M-estimation problems considered here.

Table 4 outlines results for a combination of pairs (N, p), ranging from small problems of
N = 1, 000 observations to massive data settings of N = 10 million. We apply the elastic
net penalty with α = 0.5, which puts even weight on both the lasso and ridge components,

14

Package sgd for estimation with large data sets

Figure 2: High dimensional M-estimation with the Huber loss, for N = 100, 000 observations,
p = 10, 000 covariates, and a ﬁxed regularization parameter λ. The plots indicate the mean-
squared error across iterations (left) and time (right) for sgd algorithms. The horizontal line
displays the mean-squared error for the exact M -estimator (cid:98)θm.

and then compute a regularization path for both packages. We also include an example of
N = 100, 000 observations and p = 1, 000, 000 covariates, where there exist far more covariates
than data points; this occurs often in applications, e.g., in text analysis, bioinformatics, and
signal processing (Lustig, Donoho, Santos, and Pauly 2008; Blei 2012).

The sgd algorithms begin to outperform hqreg on the order of tens of thousands of obser-
vations, and signiﬁcantly so for larger data settings. Similar to the memory limitations of
glmnet, hqreg requires access to the full data set per iteration of its algorithm, which is in-
feasible when the data cannot be held in memory. Thus we were unable to obtain proper
timings for data sets of size greater than 50, 000 observations and 10, 000 covariates.

Figure 2 displays the progress of the sgd algorithms for the setting of N = 100, 000 observa-
tions and p = 10, 000 covariates, for a ﬁxed regularization parameter λ. For demonstration,
we run the algorithms over 10 passes of the data and thus over a total of 1 million iterations.
ai-sgd is seen to achieve a signiﬁcantly faster convergence rate than explicit sgd. We also
consider the use of adaptive schedules, here with RMSProp, as it performs the fastest among
other available learning rates (see Section 4.3). With RMSProp, the diﬀerence between the
two methods—sgd and ai-sgd—is noticeably smaller, and in fact sgd seems to converge
slightly faster. We note however that the use of sgd algorithms with RMSProp breaks sta-
tistical eﬃciency, and indeed we see this eﬀect as the mean-squared error oscillates around a
value higher than the MSE of the exact M-estimator (green line). Therefore we advocate the
use of ai-sgd with a one-dimensional learning rate, which still converges quite quickly.

Journal of Statistical Software

15

4. Interface and implementation

We now discuss the interface of sgd and various technical details that are important for its
use in practice.

4.1. Interface

The sgd package provides an intuitive and accessible set of methods for performing estimation
with large-scale data sets. At the core of the package is the function

sgd(formula, data, model, model.control, sgd.control)

The user provides a formula on the data frame data—similar to function primitives, such as
lm—and then speciﬁes the model. The model parameters are estimated using sgd methods,
which defaults to ai-sgd. The optional arguments model.control and sgd.control specify
attributes one can tweak about the model and the stochastic gradient method, respectively.
For example, given a data frame dat with response vector stored as the column y,

sgd(y ~ ., data=dat, model="lm")

ﬁts a linear model with the default speciﬁcations, e.g., ai-sgd with a one-dimensional learning
rate. Similarly,

sgd(y ~ ., data=dat, model="glm", model.control=list(family="binomial"))

ﬁts logistic regression with the default speciﬁcations. Numerous examples are available in the
package by running demo(package="sgd").

The sgd function also interfaces with data sets that are too large to ﬁt into memory or
are streaming (more details in Section 4.4), and can be run with a custom loss function if
desired.

The output of the sgd function is a sgd object, which is a light wrapper on a list which
collects quantities, such as the ﬁnal parameter estimates and convergence diagnostics. Custom
generic methods are also available for the sgd class, such as print, predict, and plot.

4.2. Stochastic gradient methods

While we describe the explicit sgd and ai-sgd algorithms in Section 2, the following stochastic
gradient methods are also implemented in sgd:

• implicit sgd: Proposed by Toulis et al. (2014) in the context of generalized linear models,
this algorithm uses the implicit update (2) and does not do any iterate averaging.

• averaged sgd: Proposed by Ruppert (1988) and Bather (1989) independently, this

algorithm uses the explicit update (1) followed by iterate averaging (3).

• classical momentum (cm): Proposed by Polyak (1964), this algorithm uses the update

vn = µvn−1 + an∇ log f (yn; xn, θn−1),
θn = θn−1 + vn,

(25)

(26)

16

Package sgd for estimation with large data sets

where µ ∈ [0, 1] is a ﬁxed momentum coeﬃcient. cm accelerates gradient descent with
a velocity vector which accumulates directions of large increase in the log-likelihood.

• Nesterov’s accelerated gradient (nag): Proposed by Nesterov (1983), this algorithm

uses the update

vn = µvn−1 + an∇ log f (yn; xn, θn−1 + µvn−1),
θn = θn−1 + vn,

(27)

(28)

where µ ∈ [0, 1] is a ﬁxed momentum coeﬃcient. nag is similar to cm but accumulates
velocity at a ”look-ahead” point θn−1 + µvn−1. This makes a partial update closer to
θn, allowing nag to change its velocity more quickly and responsively.

While all these methods are available, we recommend and apply ai-sgd as the default. It can
be seen as an eﬀective combination of the advantages from both implicit sgd and averaged
sgd (Toulis et al. 2015). The momentum-based methods cm and nag enjoy faster convergence
rates than the original explicit sgd, but oﬀer no theoretical beneﬁts against ai-sgd. Without
averaging techniques they also are statistically ineﬃcient, whereas iterate averaging can be
interpreted as an acceleration technique because larger learning rates are used. The velocity
update in nag is also a proxy for the implicit update, as its beneﬁt mostly relies on making
updates close to where the new estimate would lie.

4.3. Learning rates

We describe the available learning rates in more detail because they are critical for convergence
of SGD methods, in practice.
It is well-known (Sakrison 1965; Amari 1998; Toulis et al.
2014) that explicit sgd (1) and implicit sgd (2) have optimal statistical eﬃciency if the
learning rate sequence γn together, with the conditioning matrices Cn, approximate the inverse
Fisher information matrix I(θ(cid:63)) = −E (cid:0)∇2(cid:96)(θ(cid:63); xn, yn)(cid:1), i.e., γnCn → I(θ(cid:63))−1, in the limit.
Therefore in ﬁrst-order methods where Cn = I, the learning rate sequence acts as a scalar-
valued approximation to the optimal rescaling as it is used in Fisher scoring (Fisher 1925).
Based on this theory, the following learning rates are implemented in sgd:

• One-dimensional (Xu 2011): The learning rate is of the form

γn = γ0(1 + aγ0n)−c,

where γ0, a, c ∈ R are ﬁxed constants. For sgd algorithms without iterate averaging
and sgd algorithms with iterate averaging, Xu (2011) proved that setting c = 1 and
c = 2/3, respectively, leads to optimal statistical eﬃciency; a similar result holds for
ai-sgd (Toulis et al. 2015).

• AdaGrad (Duchi et al. 2011): Rather than specify a one-dimensional learning rate
γn ∈ R, Duchi et al. (2011) propose a diagonal conditioning matrix Cn ∈ Rp×p given by

In = In−1 + diag(∇(cid:96)(θn−1; xn, yn)∇(cid:96)(θn−1; xn, yn)(cid:62)),
Cn = η(In + (cid:15)I)−1/2,

where diag(·) extracts the diagonal entries of its matrix argument, η ∈ R is a constant, I
is the identity matrix, and (cid:15) is a ﬁxed value, typically 10−6, to prevent division by zero.

Journal of Statistical Software

17

In the limit, In is an unbiased estimate of the diagonal entries of the Fisher information,
and the proposed diagonal matrix Cn, which accumulates such curvature information,
is proven to be optimal for minimization of the regret bound.

• RMSProp (Tieleman and Hinton 2012): A learning rate which is popular in the deep
learning literature (Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov 2014;
Ranganath, Tang, Charlin, and Blei 2015; Rezende and Mohamed 2015), Tieleman and
Hinton (2012) propose the diagonal conditioning matrix Cn ∈ Rp×p given by

In = βIn−1 + (1 − β) diag(∇(cid:96)(θn−1; xn, yn)∇(cid:96)(θn−1; xn, yn)(cid:62)),
Cn = η(In + (cid:15)I)−1/2,

where β ∈ [0, 1] is the discount factor, η ∈ R is a constant, I is the identity matrix,
and (cid:15) is a ﬁxed value to prevent division by zero, as in AdaGrad. RMSProp uses a
decay in the estimate for the Fisher information by taking a weighted average, and
thus it gives more weight onto newer than older information. RMSProp aims to oﬀset
one problem AdaGrad often encounters in practice, where very large values occur for
initial estimates of In (e.g., due to poor initialization), thus slowing down the AdaGrad
procedure as it tries to accumulate enough curvature information to compensate for such
an error (Schaul, Antonoglou, and Silver 2014). RMSProp balances this by taking a
weighted average of previous and new information, and sees much empirical success. One
problem, however, is that RMSProp is no longer decaying suﬃciently quickly (Robbins
and Monro 1951; Duchi et al. 2011), and thus it has no guarantees on convergence.
Moreover, assuming convergence, the limit of the learning rate sequence is a constant,
which makes the iterates jitter around the true parameter value, ad inﬁnitum.

• Fisher: Following results on statistical eﬃciency and Fisher scoring, we propose a learn-

ing rate using a diagonal conditioning matrix Cn ∈ Rp×p given by

In = (1 − γn)In−1 + γndiag(∇(cid:96)(θn−1; xn, yn)∇(cid:96)(θn−1; xn, yn)(cid:62)),
Cn = (In + (cid:15)I)−1,

where γn ∝ 1/n, and (cid:15) is a small ﬁxed value to prevent division by zero, as in AdaGrad.
As before, In in the limit is an unbiased estimate of the diagonal Fisher information,
and Cn is adaptive to curvature information.

One critical but often unnoticed issue with AdaGrad, RMSProp, and similar adaptive sched-
ules is that they are statistically ineﬃcient: the speciﬁcation of the learning rates leads to
biased estimation of the inverse Fisher information matrix I(θ(cid:63))−1 that, as mentioned ear-
lier, is necessary for optimal statistical eﬃciency (an important exception is iterate averaging).
This leads to a suboptimal asymptotic variance for the sgd procedure. Thus we recommend
and apply the last proposed learning rate (“Fisher”) by default:
it takes advantage of the
curvature information such methods beneﬁt from, while still preserving as much statistical
eﬃciency as possible in diagonal conditioning matrices.

4.4. Software integration

For data sets that cannot be loaded into memory, we access subsets of the data using big-
memory (Kane, Emerson, and Weston 2013). This allows one to perform stochastic gradient

18

Package sgd for estimation with large data sets

descent by passing over the data loaded into RAM, and then to reload a new data set. This
naturally applies to both large data sets, e.g., on the order of dozens of gigabytes, and stream-
ing settings, in which one has access only to a subset of the (potentially inﬁnite) data at a
time.

In principle, with bigmemory the memory requirement for these stochastic gradient methods
is only a single data point and the current parameter estimate, which is the minimum O(p)
complexity for simply storing the estimate. In our implementation we use these savings to
try to load as much data into RAM as possible. This speeds up convergence in practice, as
it reduces the amount of I/O overhead; this especially becomes a signiﬁcant bottleneck when
reading many objects from disk.

For fast implementations we use Rcpp (Eddelbuettel and Fran¸cois 2011), where all algo-
rithms are written in C++ and only interface-level code is written in R. Aside from the major
computational gains, this also provides the opportunity to extend the library to other pro-
gramming languages. RcppArmadillo (Eddelbuettel and Sanderson 2014) is applied for access
to pre-optimized linear algebra routines, and BH (Eddelbuettel, Emerson, and Kane 2015) for
access to the Boost libraries. We apply template meta-programming and reusable classes in
an object-oriented framework, including concepts such as stochastic gradient methods, mod-
els, and learning rates. Such concepts make it easy for other users to develop new algorithms
and prototype them in their own research or practices.

The plotting routines adopt many features from ggplot2 (Wickham 2009), and are eﬀectively
templated ggplot objects. Our software is also robust through unit testing which follows the
paradigm from testthat (Wickham 2011).

5. Discussion

As explicit sgd has been used extensively in practice, particularly in the deep learning com-
munity, many heuristics have been proposed to solve issues that often occur. We describe
several of these issues and their proposed solutions in the literature, and compare to how our
sgd package handles them.

Overﬁtting. As sgd algorithms simply minimize a loss function evaluted over the training
data, overﬁtting is a prevalent problem as it is for all estimation methods. This is particularly
an issue in complex likelihood functions such as neural networks (see, e.g., Giles (2001); Bakker
and Heskes (2003)). Even with penalization terms that try to oﬀset the ﬁt of the parameters, it
is still diﬃcult for explicit sgd to ﬁnd the right set of hyperparameters for such regularization
without a computationally intensive search.

As a solution many practictioners adopt early stopping, which simply halts the optimization
routine before it converges. However, there is little theory on the estimates obtained from
early stopping. Most practically, it is diﬃcult to know when to stop the algorithm and how
to use it in combination with other regularization techniques, such as penalization.

Fortunately, one of the advantages of ai-sgd is that it requires less such tweaking: the implicit
update eﬀectively performs a regularization as seen from the Bayesian perspective, c.f., Section
1. We’ve also seen in practice that penalization terms do not aﬀect the ﬁnal estimates from
ai-sgd, which makes it less reliant on heuristics, such as early stopping.

Journal of Statistical Software

19

Vanishing or exploding gradients. The numerical instability of explicit sgd is a widespread
issue in practice (Bengio, Simard, and Frasconi 1994; Hochreiter 1998; Hochreiter, Bengio,
Frasconi, and Schmidhuber 2001; Toulis et al. 2014). The stochastic gradients can easily
be too large leading to divergence, and when chained through compositions of functions can
either vanish to zero, or even explode to numerically inﬁnite values; for example, Toulis et al.
(2014) demonstrate the instability of explicit sgd in a simple bivariate Poisson model, where
slight misspeciﬁcation of learning rate parameters lead to divergence.

Pascanu, Mikolov, and Bengio (2012) propose gradient clipping, which simply thresholds the
stochastic gradient if it is outside a bounded interval. Unfortunately, while it can work in
practice, it is a heuristic that breaks the key assumptions for convergence rate guarantees on
sgd algorithms. Similarly, there is no principled way to set the bounds. For ai-sgd algorithms
applied to the settings we consider in Section 2, such an issue never arises. Theoretical results
establish stability regardless of the speciﬁcation of the learning rate (Toulis and Airoldi 2015a,
Section 3), and perform well in practice, as seen in Section 3.

6. Concluding remarks

The sgd package is the most extensive implementation in R of stochastic gradient methods
for estimation with massive and/or streaming data sets. Thus, sgd broadens the capabilities
of R for estimation with modern large data sets—on the orders of hundreds of millions of
observations and hundreds of thousands of covariates—while retaining desirable statistical
properties. The software is based on solid theory of stochastic approximations, which help
guide the optimal selection of parameters, e.g., learning rates, in the underlying optimiza-
tion routines. In this paper, we show how sgd can be applied for estimation of generalized
linear models and M-estimation, which comprise a sizeable portion of estimation problems
encountered in statistical practice.

There are many software extensions that are currently in development. We are working
to interface with other high-performance computing packages, namely sqldf for faster I/O
applications with streaming data, doParallel (Analytics and Weston 2014) and Rmpi (Yu
2002) for parallel updates across environments, and gputools (Buckner, Seligman, and Wilson
2011) for eﬃcient computing with GPUs. The algorithms described here directly appeal
to asynchronous implementations, following Hogwild! (Nui, Recht, Re, and Wright 2011),
which allows for lock-free allocation of CPU cores. Sparse data structures would allow for
fast structured matrix and vector products, which occur, for example, when looping over
the covariates of a data point, and would signiﬁcantly speed up computation on sparse data
sets.

Finally, there has been little attention on, and in fact a pressing need for, model selection
and hypothesis testing in sgd procedures. We are pursuing this in light of the new statistical
challenges presented to us while developing the sgd package.

References

20

Package sgd for estimation with large data sets

Amari SI (1998). “Natural Gradient Works Eﬃciently in Learning.” Neural computation,

10(2), 251–276.

Analytics R, Weston S (2014). doParallel: Foreach Parallel Adaptor for the Parallel Pack-
age. R package version 1.0.8, URL http://CRAN.R-project.org/package=doParallel.

Bach F, Moulines E (2013). “Non-strongly-convex Smooth Stochastic Approximation with
Convergence Rate O(1/n).” In Advances in Neural Information Processing Systems, pp.
773–781.

Bakker B, Heskes T (2003). “Task Clustering and Gating for Bayesian Multitask Learning.”

The Journal of Machine Learning Research, 4, 83–99.

Bather J (1989). Stochastic Approximation: A Generalisation of the Robbins-Monro Proce-

dure, volume 89. Mathematical Sciences Institute, Cornell University.

Bengio Y, Simard P, Frasconi P (1994). “Learning Long-term Dependencies with Gradient

Descent is Diﬃcult.” Neural Networks, IEEE Transactions on, 5(2), 157–166.

Blackard J (1998). Comparison of Neural Networks and Discriminant Analysis in Predicting
Forest Cover Types. Ph.D. thesis, Department of Forest Sciences, Colorado State University.

Blei DM (2012). “Probabilistic Topic Models.” Communications of the ACM, 55(4), 77–84.

Bottou L (2012). “Stochastic Gradient Descent Tricks.” In Neural Networks: Tricks of the

Trade, volume 1, pp. 421–436.

Brown LD (1986). “Fundamentals of Statistical Exponential Families with Applications in

Statistical Decision Theory.” Lecture Notes-monograph series, pp. i–279.

Buckner J, Seligman M, Wilson J (2011). gputools: A Few GPU Enabled Functions. R

package version 0.26, URL http://CRAN.R-project.org/package=gputools.

Croissant Y (2013). mlogit: Multinomial Logit Model. R package version 0.2-4, URL http:

//CRAN.R-project.org/package=mlogit.

Dempster A, Laird N, Rubin D (1977). “Maximum Likelihood from Incomplete Data via the

EM Algorithm.” Journal of the Royal Statistical Society, Series B, 39, 1–38.

Dobson A, Barnett A (2008). An Introduction to Generalized Linear Models. Texts in Statis-

tical Science Series. CRC Press, London. ISBN 9781584889502.

Donoho D, Montanari A (2013). “High Dimensional Robust M-Estimation: Asymptotic Vari-

ance via Approximate Message Passing.” arXiv preprint arXiv:1310.7320v3.

Duchi J, Hazan E, Singer Y (2011). “Adaptive Subgradient Methods for Online Learning and
Stochastic Optimization.” The Journal of Machine Learning Research, 999999, 2121–2159.

Eddelbuettel D, Emerson JW, Kane MJ (2015). BH: Boost C++ Header Files. R package

version 1.58.0-1, URL http://CRAN.R-project.org/package=BH.

Eddelbuettel D, Fran¸cois R (2011). “Rcpp: Seamless R and C++ Integration.” Journal of

Statistical Software, 40(8), 1–18. URL http://www.jstatsoft.org/v40/i08/.

Journal of Statistical Software

21

Eddelbuettel D, Sanderson C (2014).

“RcppArmadillo: Accelerating R with hHgh-
performance C++ Linear Algebra.” Computational Statistics and Data Analysis, 71, 1054–
1063. URL http://dx.doi.org/10.1016/j.csda.2013.02.005.

Enea M, Meiri R, Kalimi T (2015). speedglm: Fitting Linear and Generalized Linear Models
to Large Data Sets. R package version 0.3, URL http://CRAN.R-project.org/package=
speedglm.

Fisher RA (1925). Statistical Methods for Research Workers. Oliver and Boyd, Edinburgh.

Friedman J, Hastie T, Tibshirani R (2010). “Regularization Paths for Generalized Linear

Models via Coordinate Descent.” Journal of Statistical Software, 27(6), 957–968.

Giles RCSLL (2001). “Overﬁtting in Neural Nets: Backpropagation, Conjugate Gradient, and

Early Stopping.” In Advances in Neural Information Processing Systems.

Green PJ (1984). “Iteratively Reweighted Least Squares for Maximum Likelihood Estimation,
and Some Robust and Resistant Alternatives.” Journal of the Royal Statistical Society.
Series B (Methodological), pp. 149–192.

Hasan A, Zhiyu W, Mahani AS (2015). mnlogit: Multinomial Logit Model. R package version

1.2.2, URL http://CRAN.R-project.org/package=mnlogit.

Hastie T, Efron B (2013).

lars: Least Angle Regression, Lasso and Forward Stagewise. R

package version 1.2, URL http://CRAN.R-project.org/package=lars.

Helleputte T (2015). LiblineaR: Linear Predictive Models Based on the LIBLINEAR C/C++

Library. R package version 1.94-2.

Hochreiter S (1998). “The Vanishing Gradient Problem During Learning Recurrent Neu-
ral Nets and Problem Solutions.” International Journal of Uncertainty, Fuzziness and
Knowledge-Based Systems, 6(02), 107–116.

Hochreiter S, Bengio Y, Frasconi P, Schmidhuber J (2001). “Gradient Flow in Recurrent Nets:

The Diﬃculty of Learning Long-term Dependencies.”

Huber P (1964). “Robust Estimation of a Location Parameter.” The Annals of Mathematical

Statistics, 35(1), 73–101.

Huber P, Ronchetti E (2009). Robust Statistics. Wiley Series in Probability and Statistics.

Jain P, Tewari A, Kar P (2014). “On Iterative Hard Thresholding Methods for High-
dimensional M-Estimation.” In Advances in Neural Information Processing Systems, pp.
685–693.

Kane MJ, Emerson J, Weston S (2013). “Scalable Strategies for Computing with Massive
Data.” Journal of Statistical Software, 55(14), 1–19. URL http://www.jstatsoft.org/
v55/i14/.

Krakowski KA, Mahony RE, Williamson RC, Warmuth MK (2007). “A Geometric View of

Non-Linear On-Line Stochastic Gradient Descent.” Author website.

22

Package sgd for estimation with large data sets

Lambert-Lacroix S, Zwald L, et al. (2011). “Robust Regression Through the Huberˆa ˘A´Zs
Criterion and Adaptive Lasso Penalty.” Electronic Journal of Statistics, 5, 1015–1053.

Le Cun Y, Bottou L, Bengio Y, Haﬀner P (1998). “Gradient-based Learning Applied to

Document Recognition.” Proceedings of IEEE, 86(11), 2278–2324.

Lehmann EL, Casella G (1998). Theory of Point Estimation, volume 31. Springer Science &

Business Media.

Lewis D, Yang Y, Rose T, Li F (2004). “RCV1: A New Benchmark Collection for Text

Categorization Research.” The Journal of Machine Learning Research, 5, 361–397.

Li G, Peng H, Zhu L (2011). “Nonconcave Penalized M-estimation with a Diverging Number

of Parameters.” Statistica Sinica, 21(1), 391.

Lumley T (2013). biglm: Bounded Memory Linear and Generalized Linear Models. R package

version 0.9-1, URL http://CRAN.R-project.org/package=biglm.

Lustig M, Donoho DL, Santos JM, Pauly JM (2008). “Compressed Sensing MRI.” Signal

Processing Magazine, IEEE, 25(2), 72–82.

National Research Council (2013). Frontiers in Massive Data Analysis. The National

Academies Press, Washington, DC.

Nelder J, Wedderburn R (1972). “Generalized Linear Models.” Journal of the Royal Statistical

Society. Series A (General), pp. 370–384.

Nemirovski A, Juditsky A, Lan G, Shapiro A (2009). “Robust Stochastic Approximation
Approach to Stochastic Programming.” SIAM Journal on Optimization, 19(4), 1574–1609.

Nesterov Y (1983). “A Method of Solving a Convex Programming Problem with Convergence

Rate O(1/k2).” In Soviet Mathematics Doklady, volume 27, pp. 372–376.

Nui F, Recht B, Re C, Wright SJ (2011). “Hogwild!: A Lock-Free Approach to Parallelizing
Stochastic Gradient Descent.” In Advances in Neural Information Processing Systems.

Owen AB (2007). “A Robust Hybrid of Lasso and Ridge Regression.” Contemporary Mathe-

matics, 443, 59–72.

Pascanu R, Mikolov T, Bengio Y (2012). “On the Diﬃculty of Training Recurrent Neural

Networks.” arXiv preprint arXiv:1211.5063.

Polyak BT (1964). “Some Methods of Speeding Up the Convergence of Iteration Methods.”

USSR Computational Mathematics and Mathematical Physics, 4(5), 1–17.

Polyak BT, Juditsky AB (1992). “Acceleration of Stochastic Approximation by Averaging.”

SIAM Journal on Control and Optimization, 30(4), 838–855.

Ranganath R, Tang L, Charlin L, Blei DM (2015). “Deep Exponential Families.” In Artiﬁcial

Intelligence and Statistics.

Rezende DJ, Mohamed S (2015). “Variational Inference with Normalizing Flows.” In Inter-

national Conference on Machine Learning.

Journal of Statistical Software

23

Robbins H, Monro S (1951). “A Stochastic Approximation Method.” The Annals of Mathe-

matical Statistics, pp. 400–407.

Rockafellar RT (1976). “Monotone Operators and the Proximal Point Algorithm.” SIAM

journal on control and optimization, 14(5), 877–898.

Ruppert D (1988). “Eﬃcient Estimations from a Slowly Convergent Robbins-Monro Process.”

Technical report, Cornell University Operations Research and Industrial Engineering.

Sakrison DJ (1965). “Eﬃcient Recursive Estimation; Application to Estimating the Parame-
ters of a Covariance Function.” International Journal of Engineering Science, 3(4), 461–483.

Schaul T, Antonoglou I, Silver D (2014). “Unit Tests for Stochastic Optimization.”

Schmidt M, Le Roux N, Bach F (2013). “Minimizing Finite Sums with the Stochastic Average

Gradient.” Technical report, HAL 00860051.

Shalev-Shwartz S, Zhang T (2013). “Stochastic Dual Coordinate Ascent Methods for Regu-

larized Loss.” The Journal of Machine Learning Research, 14(1), 567–599.

Shamir O, Zhang T (2012). “Stochastic Gradient Descent for Non-smooth Optimization:
Convergence Results and Optimal Averaging Schemes.” arXiv preprint arXiv:1212.1824.

Sonnenburg S, Franc V, Yom-Tov E, Sebag M (2008). “Pascal Large Scale Learning Chal-

lenge.” URL http://largescale.first.fraunhofer.de.

Srivastava N, Hinton G, Krizhevsky A, Sutskever I, Salakhutdinov R (2014). “Dropout:
A Simple Way to Prevent Neural Networks from Overﬁtting.” The Journal of Machine
Learning Research, 15(1), 1929–1958.

Tieleman T, Hinton G (2012). “Lecture 6.5—RmsProp: Divide the Gradient by a Running
Average of its Recent Magnitude.” COURSERA: Neural Networks for Machine Learning.

Toulis P, Airoldi E, Rennie J (2014). “Statistical Analysis of Stochastic Gradient Methods
for Generalized Linear Models.” In Proceedings of the 31st International Conference on
Machine Learning (ICML-14), pp. 667–675.

Toulis P, Airoldi EM (2015a). “Implicit Stochastic Gradient Descent.” arXiv preprint

arXiv:1408.2923.

Toulis P, Airoldi EM (2015b). “Scalable Estimation Strategies Based on Stochastic Approxi-
mations: Classical Results and New Insights.” Statistics and computing, 25(4), 781–795.

Toulis P, Tran D, Airoldi EM (2015). “Stability and Optimality in Stochastic Gradient De-

scent.” arXiv preprint arXiv:1505.02417v1.

Tran D, Toulis P, Airoldi EM (2015). sgd: Stochastic Gradient Descent for Scalable Estima-

tion. R package version 1.0, URL https://github.com/airoldilab/sgd.

Venables WN, Ripley BD (2002). Modern Applied Statistics with S. Fourth edition. Springer,

New York. ISBN 0-387-95457-0, URL http://www.stats.ox.ac.uk/pub/MASS4.

24

Package sgd for estimation with large data sets

Wickham H (2009). ggplot2: Elegant Graphics for Data Analysis. Springer New York. URL

http://had.co.nz/ggplot2/book.

Wickham H (2011). “testthat: Get Started with Testing.” The R Journal, 3, 5–10. URL
http://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf.

Xu W (2011). “Towards Optimal One Pass Large Scale Learning with Averaged Stochastic

Gradient Descent.” arXiv preprint arXiv:1107.2490.

Yee TW (2010). “The VGAM Package for Categorical Data Analysis.” Journal of Statistical

Software, 32(10), 1–34.

Yi C (2015). hqreg: Regularization Paths for Huber Loss Regression and Quantile Regression
Penalized by Lasso or Elastic-Net. R package version 1.0, URL http://CRAN.R-project.
org/package=hqreg.

Yu H (2002). “Rmpi: Parallel Statistical Computing in R.” R News, 2(2), 10–14. URL

http://cran.r-project.org/doc/Rnews/Rnews_2002-2.pdf.

Zou H, Hastie T (2005). “Regularization and Variable Selection via the Elastic Net.” Journal

of the Royal Statistical Society: Series B (Statistical Methodology), 67(2), 301–320.

Zou H, Hastie T (2012). elasticnet: Elastic-Net for Sparse Estimation and Sparse PCA. R

package version 1.1, URL http://CRAN.R-project.org/package=elasticnet.

Aﬃliation:

Dustin Tran, Panos Toulis, Edoardo M. Airoldi
Department of Statistics
Harvard University
1 Oxford Street, Cambridge, MA 02138, USA
E-mail: dtran@g.harvard.edu, ptoulis@fas.harvard.edu, airoldi@fas.harvard.edu

Journal of Statistical Software
published by the American Statistical Association

Volume VV, Issue II
MMMMMM YYYY

http://www.jstatsoft.org/
http://www.amstat.org/

Submitted: yyyy-mm-dd

Accepted: yyyy-mm-dd

