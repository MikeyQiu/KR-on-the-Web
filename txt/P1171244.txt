7
1
0
2
 
n
a
J
 
1
1
 
 
]
L
C
.
s
c
[
 
 
1
v
3
6
1
3
0
.
1
0
7
1
:
v
i
X
r
a

Parsing Universal Dependencies without training

Héctor Martínez Alonso♠ Željko Agi´c♥ Barbara Plank♣ Anders Søgaard♦
♠Univ. Paris Diderot, Sorbonne Paris Cité – Alpage, INRIA, France
♥IT University of Copenhagen, Denmark
♣Center for Language and Cognition, University of Groningen, The Netherlands
♦University of Copenhagen, Denmark
hector.martinez-alonso@inria.fr

Abstract

We propose UDP, the ﬁrst training-free
parser for Universal Dependencies (UD).
Our algorithm is based on PageRank and a
small set of head attachment rules. It fea-
tures two-step decoding to guarantee that
function words are attached as leaf nodes.
The parser requires no training, and it is
competitive with a delexicalized transfer
system. UDP offers a linguistically sound
unsupervised alternative to cross-lingual
parsing for UD, which can be used as a
baseline for such systems. The parser has
very few parameters and is distinctly ro-
bust to domain change across languages.

1 Introduction

Grammar induction and unsupervised dependency
parsing are active ﬁelds of research in natural
language processing (Klein and Manning, 2004;
Gelling et al., 2012). However, many data-driven
approaches struggle with learning relations that
match the conventions of the test data, e.g., Klein
and Manning reported the tendency of their DMV
parser to make determiners the heads of German
nouns, which would not be an error if the test data
used a DP analysis (Abney, 1987). Even super-
vised transfer approaches (McDonald et al., 2011)
suffer from target adaptation problems when fac-
ing word order differences.

The Universal Dependencies

(UD) project
(Nivre et al., 2015; Nivre et al., 2016) offers a de-
pendency formalism that aims at providing a
consistent representation across languages, while
enforcing a few hard constraints.
The ar-
rival of such treebanks, expanded and improved
on a regular basis, provides a new milestone
for cross-lingual dependency parsing research
(McDonald et al., 2013).

Furthermore, given that UD rests on a series of
simple principles like the primacy of lexical heads,
cf.
Johannsen et al. (2015) for more details, we
expect that such a formalism lends itself more nat-
urally to a simple and linguistically sound rule-
based approach to cross-lingual parsing.
In this
paper we present such an approach.

Our system is a dependency parser that requires
no training, and relies solely on explicit part-of-
speech (POS) constraints that UD imposes. In par-
ticular, UD prescribes that trees are single-rooted,
and that function words like adpositions, auxil-
iaries, and determiners are always dependents of
content words, while other formalisms might treat
them as heads (De Marneffe et al., 2014). We as-
cribe our work to the viewpoints of Bender (2009)
about the incorporation of linguistic knowledge in
language-independent systems.

Contributions We introduce, to the best of our
knowledge, the ﬁrst unsupervised rule-based de-
pendency parser for Universal Dependencies.

Our method goes substantially beyond the exist-
ing work on rule-aided unsupervised dependency
parsing, speciﬁcally by:

i) adapting the dependency head rules to UD-

compliant POS relations,

ii) incorporating the UD restriction of function

words being leaves,

iii) applying personalized PageRank to improve

main predicate identiﬁcation, and by

iv) making the parsing entirely free of language-
speciﬁc parameters by estimating adposition
attachment direction at runtime.

We evaluate our system on 32 languages1 in three
setups, depending on the reliability of available
POS tags, and compare to a multi-source delexi-

1Out of 33 languages in UD v1.2. We exclude Japanese
because the treebank is distributed without word forms and
hence we can not provide results on predicted POS.

calized transfer system. In addition, we evaluate
the systems’ sensitivity to domain change for a
subset of UD languages for which domain infor-
mation was retrievable. The results expose a solid
and competitive system for all UD languages. Our
unsupervised parser compares favorably to delex-
icalized parsing, while being more robust to do-
main change.

2 Related work

Cross-lingual learning Recent years have seen
exciting developments in cross-lingual linguistic
structure prediction based on transfer or projection
of POS and dependencies (Das and Petrov, 2011;
McDonald et al., 2011). These works mainly use
supervised learning and domain adaptation tech-
niques for the target language.

The ﬁrst group of approaches deals with anno-
tation projection (Yarowsky et al., 2001), whereby
parallel corpora are used to transfer annotations
between resource-rich source languages and low-
resource target languages. Projection relies on
the availability and quality of parallel corpora,
source-side taggers and parsers, but also tok-
enizers, sentence aligners, and word aligners for
sources and targets. Hwa et al. (2005) were the
ﬁrst to project syntactic dependencies, and Tiede-
mann et al. (2014; 2016) improved on their pro-
jection algorithm. Current state of the art in
cross-lingual dependency parsing involves lever-
aging parallel corpora for annotation projection
(Ma and Xia, 2014; Rasooli and Collins, 2015).

The second group of approaches deals with
transferring source parsing models to target lan-
guages. Zeman and Resnik (2008) were the ﬁrst
to introduce the idea of delexicalization: remov-
ing lexical features by training and cross-lingually
solely on POS sequences.
applying parsers
Søgaard (2011) and McDonald et al. (2011) inde-
pendently extended the approach by using mul-
tiple sources, requiring uniform POS and depen-
dency representations (McDonald et al., 2013).

Both model transfer and annotation projection
rely on a large number of presumptions to derive
their competitive parsing models. By and large,
these presumptions are unrealistic and exclusive to
a group of very closely related, resource-rich Indo-
European languages. Agi´c et al. (2015; 2016)
exposed some of these biases in their proposal
for realistic cross-lingual tagging and parsing, as
they emphasized the lack of perfect sentence- and

word-splitting for truly low-resource languages.
Further, Johannsen et al. (2016) introduced joint
projection of POS and dependencies from multiple
sources while sharing the outlook on bias removal
in real-world multilingual processing.

Rule-based parsing Cross-lingual methods, re-
alistic or not, depend entirely on the availability
of data: for the sources, for the targets, or most
often for both sets of languages. Moreover, they
typically do not exploit constraints placed on lin-
guistic structures through a formalism, and they do
so by design.

With the emergence of UD as the practical stan-
dard for multilingual POS and syntactic depen-
dency annotation, we argue for an approach that
takes a fresh angle on both aspects. Speciﬁcally,
we propose a parser that i) requires no training
data, and in contrast ii) critically relies on exploit-
ing the UD constraints.

two

These

characteristics make

our
parser unsupervised.
Data-driven unsuper-
vised dependency parsing is now a well-
established discipline (Klein and Manning, 2004;
Spitkovsky et al., 2010a;
Spitkovsky et al., 2010b).
mance of
approaches involving any sort of supervision.

the perfor-
these parsers falls far behind the

Still,

unsupervised

rule-aided
by

Our work builds on the line of

research
dependency
on
and
parsing
Naseem et al. (2010), and also relates to Sø-
gaard’s (2012a; 2012b) work.
Our parser,
however, features two key differences:

Gillenwater et al. (2010)

i) the usage of PageRank personalization

(Lofgren, 2015), and of

ii) two-step decoding to treat content and func-
tion words differently according to the UD
formalism.

Through these differences, even without any train-
ing data, we parse nearly as well as a delexicalized
transfer parser, and with increased stability to do-
main change.

3 Method

Our approach does not use any training or unla-
beled data. We have used the English treebank
during development to assess the contribution of
individual head rules, and to tune PageRank pa-
rameters (Sec. 3.1) and function-word directional-
ity (Sec. 3.2). Adposition direction is calculated

on the ﬂy at runtime. We refer henceforth to our
UD parser as UDP.

3.1 PageRank setup

Our system uses the PageRank (PR) algorithm
(Page et al., 1999) to estimate the relevance of the
content words of a sentence. PR uses a random
walk to estimate which nodes in the graph are
more likely to be visited often, and thus, it gives
higher rank to nodes with more incoming edges, as
well as to nodes connected to those. Using PR to
score word relevance requires an effective graph-
building strategy. We have experimented with the
strategies by Søgaard (2012b), such as words be-
ing connected to adjacent words, but our system
fares best strictly using the dependency rules in
Table 1 to build the graph. UD trees are often very
ﬂat, and a highly connected graph yields a PR dis-
tribution that is closer to uniform, thereby remov-
ing some of the difference of word relevance.

We build a multigraph of all words in the sen-
tence covered by the head-dependent rules in Ta-
ble 1, giving each word an incoming edge for each
eligible dependent, i.e., ADV depends on ADJ and
VERB. This strategy does not always yield con-
nected graphs, and we use a teleport probability of
0.05 to ensure PR convergence.

Teleport probability is the probability that, in
any iteration of the PR calculation, the next active
node is randomly chosen, instead of being one of
the adjacent nodes of the current active node. See
Brin and Page (2012) for more details on teleport
probability, where the authors refer to one minus
teleport probability as damping factor.

We chose this value incrementally in intervals
of 0.01 during development until we found the
smallest value that guaranteed PR convergence. A
high teleport probability is undesirable, because
the resulting stationary distribution can be almost
uniform. We did not have to re-adjust this value
when running on the actual test data.

The main idea behind our personalized PR ap-
proach is the observation that ranking is only rele-
vant for content words.2 PR can incorporate a pri-
ori knowledge of the relevance of nodes by means
of personalization, namely giving more weight to
certain nodes.

Intuitively, the higher the rank of a word, the
closer it should be to the root node, i.e., the main
predicate of the sentence is the node that should

h =argminj∈H {γ(j, c) | δ(j, c) ∧ κ(j, c)}

else

if |H| = 0 then
h = root

1: H = ∅; D = ∅
2: C = hc1, ...cmi; F = hf1, ...fmi
3: for c ∈ C do
4:
5:
6:
7:
8:
9:
10:
11: end for
12: for f ∈ F do
13:
14:
15: end for
16: return D

end if
H = H ∪ {c}
D = D ∪ {(h, c)}

h =argminj∈H {γ(j, f ) | δ(j, f ) ∧ κ(j, f )}
D = D ∪ {(h, f )}

Figure 1: Two-step decoding algorithm for UDP.

have the highest PR, making it the dependent of
the root node (Fig. 1, lines 4-5). We use PR per-
sonalization to give 5 times more weight (over an
otherwise uniform distribution) to the node that is
estimated to be main predicate, i.e., the ﬁrst verb
or the ﬁrst content word if there are no verbs.

3.2 Head direction

Head direction is an important trait in dependency
syntax (Tesnière, 1959).
Indeed, the UD feature
inventory contains a trait to distinguish the general
adposition tag ADP in pre- and post-positions.

Instead of relying on this feature from the tree-
banks, which is not always provided, we estimate
the frequency of ADP-NOMINAL vs. NOMINAL-
ADP bigrams.3 We calculate this estimation di-
rectly on input data at runtime to keep the system
training-free. Moreover, it requires very few ex-
amples to converge (10-15 sentences).
If a lan-
guage has more ADP-NOMINAL bigrams, we con-
sider all its ADP to be prepositions (and thus de-
pendent of elements at their right). Otherwise, we
consider them postpositions.

For other function words, we have determined
on the English dev data whether to make them
strictly right- or left-attaching, or to allow ei-
ther direction. There, AUX, DET, and SCONJ
are right-attaching, while CONJ and PUNCT are
left-attaching. There are no direction constraints
for the rest. Punctuation is a common source of
parsing errors that has very little interest in this
setup. While we do evaluate on all tokens includ-
ing punctuation, we also apply a heuristic for the
last token in a sentence; if it is a punctuation, we
make it a dependent of the main predicate.

2 ADJ, NOUN, PROPN, and VERB mark content words.

3 NOMINAL= {NOUN, PROPN, PRON}

ADJ −→ ADV

NOUN −→ ADJ, NOUN, PROPN
NOUN −→ ADP, DET, NUM
PROPN −→ ADJ, NOUN, PROPN
PROPN −→ ADP, DET, NUM

VERB −→ ADV, AUX, NOUN
VERB −→ PROPN, PRON, SCONJ

then all function words (F ) in the second block
where H is not updated, thereby ensuring leafness
for all f ∈ F . The order of head attachment is not
monotonic wrt. PR between the ﬁrst and second
block, and can yield non-projectivities. Neverthe-
less, it still is a one-pass algorithm. Decoding runs
in less than O(n2), namely O(n × |C|). However,
running PR incurs the main computation cost.

Table 1: UD dependency rules.

4 Parser run example

3.3 Decoding

Fig. 1 shows the tree-decoding algorithm. It has
two blocks, namely a ﬁrst block (3-11) where we
assign the head of content words according to their
PageRank and the constraints of the dependency
rules, and a second block (12-15) where we assign
the head of function words according to their prox-
imity, direction of attachment, and dependency
rules. The algorithm requires:

1. The PR-sorted list of content words C.
2. The set of function words F , sorting is irrel-
evant because function-head assignations are
inter-independent.

3. A set H for the current possible heads, and
a set D for the dependencies assigned at
each iteration, which we represent as head-
dependent tuples (h, d).

4. A symbol root for the root node.
5. A function γ(n, m) that gives the linear dis-

tance between two nodes.

6. A function κ(h, d) that returns whether the
dependency (h, d) has a valid attachment di-
rection given the POS of the d (cf. Sec. 3.2).
7. A function δ(h, d) that determines whether
(h, d) is licensed by the rules in Table 1.

The head assignations in lines 7 and 13 read as
follow: the head h of a word (either c or f ) is the
closest element of the current list of heads (H) that
has the right direction (κ) and respects the POS-
dependency rules (δ). These assignations have a
back-off option to ensure the ﬁnal D is a tree. If
the conditions determined by κ and δ are too strict,
i.e., if the set of possible heads is empty, we drop
the δ head-rule constraint and recalculate the clos-
est possible head that respects the directionality
imposed by κ. If the set is empty again, we drop
both constraints and assign the closest head.

Lines 4 and 5 enforce the single-root constraint.
To enforce the leaf status of function nodes, the
algorithm ﬁrst attaches all content words (C), and

This section exempliﬁes a full run of UDP for
the example sentence from the English test data:
“They also had a special connection to some ex-
tremists”.

4.1 PageRank

Given an input sentence and its POS tags, we ob-
tain rank of each word by building a graph using
head rules and running PR on it. Table 2 provides
the sentence, the POS of each word, the number
of incoming edges for each word after building
the graph with the head rules from Sec. 3.1, and
the personalization vector for PR on this sentence.
Note that all nodes have the same personalization
weight, except the estimated main predicate, the
verb “had”.

Word: They
POS: PRON ADV VERB DET ADJ

also

had

special

a

connection
NOUN

to
some
ADP DET NOUN

extremists

Personalization:
Incoming edges:

1
0

1
0

5
4

1
0

1
1

1
5

1
0

1
0

1
5

Table 2: Words, POS, personalization, and incom-
ing edges for the example sentence.

Table 4 shows the directed multigraph used for
PR in detail. We can see, e.g., that the four in-
coming edges for the verb “had” from the two
nouns, plus from the adverb “also” and the pro-
noun “They”.

After running PR, we obtain the following rank-

ing for content words:
C = hhad,connection,extremists,speciali
Even though the verb has four incoming edges
and the nouns have ﬁve each, the personalization
makes the verb the highest-ranked word.

1

5

6

2

7

4

3

8

9

root They also had a special connection to some extremists

Figure 2: Example dependency tree predicted by
the algorithm.

4.2 Decoding

Once C is calculated, we can follow the algo-
rithm in Fig. 1 to obtain a dependency parse.
Table 3 shows a trace of the algorithm, with
C = hhad,connection,extremists,speciali and
F = {They,also,a,to,some}.

it
1
2
3
4

5
6
7
8
9

word
had
connection
extremists
special

They
also
a
to
some

h
root
had
had
connection

had
had
connection
extremists
extremists

H
∅
{had}
{had, connection}
{had, connection, extremists}

{had, connection, extremists, special}
...
...
...
...

Table 3: Algorithm trace for example sentence. it:
iteration number, word: current word, H: set of
possible heads.

The ﬁrst four iterations calculate the head of
content words following their PR, and the follow-
ing iterations attach the function words in F . Fi-
nally, Fig. 2 shows the resulting dependency tree.
Full lines are assigned in the ﬁrst block (content
dependents), dotted lines are assigned in the sec-
ond block (function dependents). The edge labels
indicate in which iteration the algorithm has as-
signed each dependency. Note that the algorithm
is deterministic for a certain input POS sequence.
Any 10-token sentence with the POS labels shown
in Table 2 would yield the same dependency tree.4

5 Experiments

This section describes the data, metrics and com-
parison systems used to assess the performance
of UDP. We evaluate on the test sections of the
UD1.2 treebanks (Nivre et al., 2015) that contain
If there is more than one treebank
word forms.
per language, we use the treebank that has the

4The resulting trees always pass the validation script in

github.com/UniversalDependencies/tools.

−→

They

also

had

a

special

connection

some

extremists

-

-

They
also
had
a
special
connection
to
some
extremists

•

-

-

•
•
-

•

•

to

•

-

-

•
•
-
•
•
•

•
•
•
•
•
-

Table 4: Matrix representation of the directed
graph for the words in the sentence.

canonical language name (e.g., Finnish instead of
Finnish-FTB). We use standard unlabeled attach-
ment score (UAS) and evaluate on all sentences of
the canonical UD test sets.

5.1 Baseline

We compare our UDP system with the perfor-
mance of a rule-based baseline that uses the head
rules in Table 5. The baseline identiﬁes the ﬁrst
verb (or ﬁrst content word if there are no verbs) as
the main predicate, and assigns heads to all words
according to the rules in Table 1. We have selected
the set of head rules to maximize precision on the
development set, and they do not provide full cov-
erage. The system makes any word not covered
by the rules (e.g., a word with a POS such as X or
SYM) either dependent of their left or right neigh-
bor, according to the estimated runtime parameter.
We report the best head direction and its score
This baseline
for each language in Table 5.
ﬁnds the head of each token based on its clos-
est possible head, or on its immediate left or
right neighbor if there is no head rule for the
POS at hand, which means that this system does
not necessarily yield well-formed tress. Each
token receives a head, and while the structures
are single-rooted,
they are not necessarily con-
nected. Note that we do not include results for
the DMV model by Klein and Manning (2004), as
it has been outperformed by a system similar to
ours (Søgaard, 2012b). The usual adjacency base-
line for unsupervised dependency parsing, where
all words depend on their left or right neighbor,
fares much worse than our baseline (20% UAS be-
low on average) even with an oracle pick for the
best per-language direction, and we do not report
those scores.

5.2 Evaluation setup

Our system relies solely on POS tags. To esti-
mate the quality degradation of our system un-
der non-gold POS scenarios, we evaluate UDP on
two alternative scenarios. The ﬁrst is predicted
POS (UDPP ), where we tag the respective test
set with TnT (Brants, 2000) trained on each lan-
guage’s training set. The second is a naive type-
constrained two-POS tag scenario (UDPN ), and
approximates a lower bound. We give each word
either CONTENT or FUNCTION tag, depending on
the word’s frequency. The 100 most frequent
words of the input test section receive the FUNC-
TION tag.

Finally, we compare our parser UDP to a su-
is a
pervised cross-lingual system (MSD). It
multi-source delexicalized transfer parser,
re-
ferred to as multi-dir in the original paper
For this baseline
by McDonald et al. (2011).
we train TurboParser (Martins et al., 2013) on a
delexicalized training set of 20k sentences, sam-
pled uniformly from the UD training data exclud-
ing the target language. MSD is a competitive and
realistic baseline in cross-lingual transfer parsing
work. This gives us an indication how our system
compares to standard cross-lingual parsers.

5.3 Results

Table 5 shows that UDP is a competitive system;
because UDPG is remarkably close to the super-
vised MSDG system, with an average difference
of 6.4%. Notably, UDP even outperforms MSD
on one language (Hindi).

More interestingly, on the evaluation scenario
with predicted POS we observe that our system
drops only marginally (2.2%) compared to MSD
In the least robust rule-based setup, the
(2.7%).
error propagation rate from POS to dependency
would be doubled, as either a wrongly tagged head
or dependent would break the dependency rules.
However, with an average POS accuracy by TnT
of 94.1%, the error propagation is 0.37, i.e, each
POS error causes 0.37 additional dependency er-
rors. In contrast, for MSD this error propagation
is 0.46, thus higher. 5

For the extreme POS scenario, content vs. func-
tion POS (CF), the drop in performance for UDP
is very large, but this might be too crude an eval-
uation setup. Nevertheless, UDP, the simple unsu-
pervised system with PageRank, outperforms the
adjacency baselines (BL) by ∼4% on average on
the two type-based naive POS tag scenario. This
difference indicates that even with very deﬁcient
POS tags, UDP can provide better structures.

6 Discussion

In this section we provide a further error analysis
of the UDP parser. We examine the contribution
to the overal results of using PageRank to score
content words, the behavior of the system across
different parts of speech, and we assess the robust-
ness of UDP on text from different domains.

5Err. prop. = (E(P arseP ) − E(P arseG))/E(P OSP ),

where E(x) = 1 − Accuracy(x).

Language BLG

UDPG MSDG MSDP UDPP UDPN

Ancient Greek
Arabic
Basque
Bulgarian
Church Slavonic
Croatian
Czech
Danish
Dutch
English
Estonian
Finnish
French
German
Gothic
Greek

42.2 L 43.4
34.8 R 47.8
47.8 R 45.0
54.9 R 70.5
53.8 L 59.2
41.6 L 56.7
46.5 R 61.0
47.3 R 57.9
36.1 L 49.5
46.2 R 53.0
73.2 R 70.0
43.8 R 45.1
47.1 R 64.5
48.2 R 60.6
50.2 L 57.5
45.7 R 58.5
Hebrew 41.8 R 55.4
43.9 R 46.3
53.1 R 56.7
44.6 L 60.6
47.5 R 56.6
50.6 R 69.4
49.4 L 56.2
49.1 R 61.7
37.8 L 55.7
60.8 R 68.4
45.8 R 65.7
52.7 R 63.7
50.6 R 63.6
48.2 R 63.9
52.4 R 62.8
41.4 R 34.2

Hindi
Hungarian
Indonesian
Irish
Italian
Latin
Norwegian
Persian
Polish
Portuguese
Romanian
Slovene
Spanish
Swedish
Tamil

Average

47.8

57.5

48.6
52.8
51.2
78.7
61.8
69.1
69.5
70.2
57.0
62.1
73.4
52.9
72.7
66.9
61.7
68.0
62.0
34.6
58.4
63.6
62.5
77.1
59.8
70.8
57.8
75.6
72.8
69.2
74.7
72.9
72.2
44.2

63.9

46.5
52.6
49.3
76.6
59.8
65.6
67.6
65.6
59.2
59.9
66.1
50.4
70.6
62.5
59.2
66.4
58.6
34.5
56.8
61.0
61.3
75.2
54.9
67.3
55.6
71.7
71.4
64.0
71.0
70.7
67.2
39.5

61.2

41.6
47.6
43.1
68.1
59.2
54.5
59.3
53.8
50.0
51.4
65.0
43.1
62.1
57.0
55.8
57.0
52.8
45.7
54.8
58.4
53.9
67.9
52.4
58.6
53.6
65.7
64.9
58.9
56.0
62.1
58.5
32.1

55.3

27.0
41.0
22.8
27.1
35.2
25.2
25.3
26.9
24.1
27.9
25.3
21.6
36.3
24.2
34.1
29.3
35.7
27.0
22.7
35.3
35.8
37.6
37.1
29.8
33.9
34.6
33.5
32.1
24.3
35.0
25.3
20.3

29.9

Table 5: UAS for baseline with gold POS (BLG)
with direction (L/R) for backoff attachments,
UDP with gold POS (UDPG) and predicted POS
(UDPP ), PR with naive content-function POS
(UDPN ), and multi-source delexicalized with gold
and predicted POS (MSDG and MSDP , respec-
tively). BL values higher than UDPG are under-
lined, and UDPG values higher than MSDG are in
boldface.

6.1 PageRank contribution

UDP depends on PageRank to score content
words, and on two-step decoding to ensure the
leaf status of function words. In this section we
isolate the constribution of both parts. We do so
by comparing the performance of BL, UDP, and
UDPN oP R, a version of UDP where we disable
PR and rank content words according to their read-
ing order, i.e., the ﬁrst word in the ranking is the
ﬁrst word to be read, regardless of the speciﬁc
language’s script direction. The baseline BL de-
scribed in 5.1 already ensures function words are
leaf nodes, because they have no listed dependent
POS in the head rules. The task of the decoding
steps is mainly to ensure the resulting structures
are well-formed dependency trees.

If we measure the difference between UDPN oP R
and BL, we see that UDPN oP R contributes with 4
UAS points on average over the baseline. Nev-
ertheless,
the baseline is oracle-informed about
the language’s best branching direction, a property
that UDP does not have. Instead, the decoding step
determines head direction as described in Section
3.2. Complementarily, we can measure the contri-
bution of PR by observing the difference between
regular UDP and UDPN oP R. The latter scores on
average 9 UAS points lower than UDP. These 9
points are caused by the difference attachment of
content words in the ﬁrst decoding step.

6.2 Breakdown by POS

UD is a constantly improving effort, and not all
v1.2 treebanks have the same level of formalism
compliance. Thus, the interpretation of, e.g., the
AUX–VERB or DET–PRON distinctions might dif-
fer across treebanks. However, we ignore these
differences in our analysis and consider all tree-
banks to be equally compliant.

The root accuracy scores oscillate around an av-
erage of 69%, with Arabic and Tamil (26%) and
Estonian (93%) as outliers. Given the PR per-
sonalization (Sec. 3.1), UDP has a strong bias for
choosing the ﬁrst verb as main predicate. With-
out personalization, performance drops 2% on av-
erage. This difference is consistent even for verb-
ﬁnal languages like Hindi, given that the main verb
of a simple clause will be its only verb, regardless
of where it appears. Moreover, using PR person-
alization makes the ranking calculations converge
a whole order of magnitude faster.

The bigram heuristic to determine adposition
direction succeeds at identifying the predominant
pre- or postposition preference for all languages
(average ADP UAS of 75%). The ﬁxed direc-
tion for the other functional POS is largely effec-
tive, with few exceptions, e.g., DET is consistently
right-attaching on all treebanks except Basque (av-
erage overall DET UAS of 84%, 32% for Basque).
These alternations could also be estimated from
the data in a manner similar to ADP. Our rules
do not make nouns eligible heads for verbs. As a
result, the system cannot infer relative clauses. We
have excluded the NOUN → VERB rule during de-
velopment because it makes the hierarchy between
verbs and nouns less conclusive.

We have not excluded punctuation from the
evaluation. Indeed, the UAS for the PUNCT is low

(an average of 21%, standard deviation of 9.6),
even lower than the otherwise problematic CONJ.
Even though conjunctions are pervasive and iden-
tifying their scope is one of the usual challenges
for parsers, the average UAS for CONJ is much
larger (an average of 38%, standard deviation of
13.5) than for PUNCT. Both POS show large stan-
dard deviations, which indicates great variability.
This variability can be caused by linguistic prop-
erties of the languages or evaluation datasets, but
also by differences in annotation convention.

6.3 Cross-domain consistency

Models with fewer parameters are less likely
to overﬁt for a certain dataset.
In our case,
a system with few, general rules is less likely
to make attachment decisions that are very
particular of a certain language or dataset.
Plank and van Noord (2010) have shown that rule-
based parsers can be more stable to domain shift.
We explore if their ﬁnding holds for UDP as well,
by testing on i) the UD development data as a read-
ily available proxy for domain shift, and ii) manu-
ally curated domain splits of select UD test sets.

Language Domain

BLG MSDG UDPG MSDP UDPP

Bulgarian bulletin

48.3
legal
47.9
literature 53.6
news
49.3
various
51.4

Croatian news
wiki

English answers

email
newsgroup
reviews
weblog
magazine†
bible†
questions†

legal
news
questions
various
wiki

Italian europarl

Serbian news
wiki

67.5
76.9
74.2
74.6
74.2

62.4
64.8

61.6
58.8
55.5
66.8
51.6
60.9
56.2
69.7

64.1
67.9
68.9
80.0
67.8
71.2

68.0
68.9

41.2
41.9

44.1
42.8
41.7
47.4
43.3
41.4
38.4
38.7

50.8
51.1
49.4
48.7
49.7
51.8

42.8
42.4

67.4
69.2
69.0
70.2
72.5

57.9
55.8

55.9
52.1
49.7
54.9
50.9
55.6
56.2
55.6

70.6
69.0
67.5
77.0
69.0
68.1

58.8
58.8

65.4
73.0
72.8
73.0
72.6

61.8
58.2

59.5
57.1
52.9
63.9
49.8
58.4
56.8
60.5

62.7
64.4
67.0
79.1
65.3
70.3

65.6
62.8

61.5
68.6
66.6
68.2
69.5

52.2
56.3

53.7
56.3
51.1
52.2
53.8
53.3
48.6
47.2

69.7
67.2
65.3
76.1
67.6
66.6

53.3
55.8

Table 6: Evaluation across domains. UAS for
baseline with gold POS (BLG), UDP with gold
POS (UDPG) and predicted POS (UDPP ), and
multi-source delexicalized with gold and pre-
dicted POS (MSDG and MSDP ). English datasets
marked with † are in-house annotated. Lowest re-
sults per language underlined. Bold: UDP outper-
forms MSD.

Development sets We have used the English de-
velopment data to choose which relations would
be included as head rules in the ﬁnal system (Ta-
ble 1). It would be possible that some of the rules
are indeed more beﬁtting for the English data or
for that particular section.

However, if we regard the results for UDPG in
Table 5, we can see that there are 24 languages
(out of 32) for which the parser performs better
than for English. This result indicates that the
head rules are general enough to provide reason-
able parses for languages other than the one cho-
sen for development. If we run UDPG on the de-
velopment sections for the other languages, we
ﬁnd the results are very consistent. Any language
scores on average ±1 UAS with regards to the test
section. There is no clear tendency for either sec-
tion being easier to parse with UDP.

Cross-domain test sets To further assess the
cross-domain robustness, we retrieved the domain
(genre) splits from the test sections of the UD tree-
banks where the domain information is available
as sentence metadata: from Bulgarian, Croatian,
and Italian. We also include a UD-compliant Ser-
bian dataset which is not included in the UD re-
lease but which is based on the same parallel cor-
pus as Croatian and has the same domain splits
(Agi´c and Ljubeši´c, 2015). When averaging we
pool Croatian and Serbian together as they come
from the same dataset.

For English, we have obtained the test data
splits matching the sentences from the original dis-
tribution of the English Web Treebank. In addition
to these already available datasets, we have an-
notated three different datasets to assess domain
variation more extensively, namely the ﬁrst 50
verses of the King James Bible, 50 sentences from
a magazine, and 75 sentences from the test split
in QuestionBank (Judge et al., 2006). We include
the third dataset to evaluate strictly on questions,
which we could do already in Italian. While the
answers domain in English is made up of text
from the Yahoo! Answers forum, only one fourth
of the sentences are questions. Note these three
small datasets are not included in the results on
the canonical test sections in Table 5.

Table 7 summarizes the per-language average
score and standard deviation, as well as the macro-
averaged standard deviation across languages.
UDP has a much lower standard deviation across
domains compared to MSD. This holds across lan-

Language

BLG

MSDG

UDPG

MSDP

UDPP

Bulgarian
Croatian+Serbian
English
Italian

50.1±2.4
42.1±0.7
42.2±2.8
50.3±1.2

73.5±3.5
66.0±3.0
60.1±6.2
70.0±5.4

69.7±1.8
57.8±1.4
53.9±2.5
70.1±3.3

71.3±3.3
62.1±3.0
57.3±4.3
68.1±6.0

66.9±3.2
54.4±2.0
52.0±3.3
68.7±3.9

Average Std.

1.8

4.5

2.5

4.2

3.1

Table 7: Average language-wise domain evalua-
tion. We report average UAS and standard devi-
ation per language. The bottom row provides the
average standard deviation for each system.

guages. We attribute this higher stability to UDP
being developed to satisfy a set of general prop-
erties of the UD syntactic formalism, instead of
being a data-driven method more sensitive to sam-
pling bias. This holds for both the gold-POS and
predicted-POS setup. The differences in standard
deviation are unsurprisingly smaller in the pre-
dicted POS setup. In general, the rule-based UPD
is less sensitive to domain shifts than the data-
driven MSD counterpart, conﬁrming earlier ﬁnd-
ings (Plank and van Noord, 2010).

Table 6 gives the detailed scores per language
and domain. From the scores we can see that
presidential bulletin, legal and weblogs
are amongst the hardest domains to parse. How-
ever, the systems often do not agree on which do-
main is hardest, with the exception of Bulgarian
bulletin. Interestingly, for the Italian data and
some of the hardest domains UDP outperforms
MSD, conﬁrming that it is a robust baseline.

6.4 Comparison to full supervision

In order to assess how much information the sim-
ple principles in UDP provide, we measure how
many gold-annotated sentences are necessary to
reach its performance, that is, after which size the
treebank provides enough information for training
that goes beyond the simple linguistic principles
outlined in Section 3.

For this comparison we use a ﬁrst-order non-
projective TurboParser (Martins et al., 2013) fol-
lowing the setup of Agi´c et al. (2016). The su-
pervised parsers require around 100 sentences to
reach UDP-comparable performance, namely a
mean of 300 sentences and a median of 100 sen-
tences, with Bulgarian (3k), Czech (1k), and Ger-
man (1.5k) as outliers. The difference between
mean and median shows there is great variance,
while UDP provides very constant results, also in
terms of POS and domain variation.

7 Conclusion

References

We have presented UDP, an unsupervised depen-
dency parser for Universal Dependencies (UD)
that makes use of personalized PageRank and a
small set of head-dependent rules. The parser re-
quires no training data and estimates adposition di-
rection directly from the input.

We achieve competitive performance on all but
two UD languages, and even beat a multi-source
delexicalized parser (MSD) on Hindi. We eval-
uated the parser on three POS setups and across
domains. Our results show that UDP is less af-
fected by deteriorating POS tags than MSD, and
is more resilient to domain changes. Given how
much of the overall dependency structure can be
explained by this fairly system, we propose UDP
as an additional UD parsing baseline. The parser,
the in-house annotated test sets, and the domain
data splits are made freely available.6

UD is a running project, and the guidelines are
bound to evolve overtime.
Indeed, the UD 2.0
guidelines have been recently released. UDP can
be augmented with edge labeling for some deter-
ministic labels like case or det. Some further
constrains can be incorporated in UDP. Moreover,
the parser makes no special treatment of multi-
word expression that would require a lexicon, co-
ordinations or proper names. All these three kinds
of structures have a ﬂat tree where all words de-
pend on the leftmost one. While coordination at-
tachment is a classical problem in parsing and out
of the scope of our work, a proper name sequence
can be straightforwardly identiﬁed from the part-
of-speech tags, and it falls thus in the area of struc-
tures predictable using simple heuristics. More-
over, our use of PageRank could be expanded to
directly score the potential dependency edges in-
stead of words, e.g., by means of edge reiﬁcation.

Acknowledgments

We thank the anonymous reviewers for their valu-
able feedback. Héctor Martínez Alonso is funded
by the French DGA project VerDi. Barbara Plank
thanks the Center for Information Technology of
the University of Groningen for the HPC cluster.
Željko Agi´c and Barbara Plank thank the Nvidia
Corporation for supporting their research. An-
ders Søgaard is funded by the ERC Starting Grant
LOWLANDS No. 313695.

[Abney1987] Steven Paul Abney. 1987. The English
noun phrase in its sentential aspect. Ph.D. thesis,
Massachusetts Institute of Technology.

[Agi´c et al.2016] Željko Agi´c, Anders Johannsen, Bar-
bara Plank, Héctor Alonso Martínez, Natalie
Schluter, and Anders Søgaard. 2016. Multilingual
Projection for Parsing Truly Low-Resource Lan-
guages. Transactions of the Association for Com-
putational Linguistics, 4.

[Agi´c and Ljubeši´c2015] Željko Agi´c
2015.

and Nikola
Universal Dependencies for

Ljubeši´c.
Croatian (that Work for Serbian, too). In BSNLP.

[Agi´c et al.2015] Željko Agi´c, Dirk Hovy, and Anders
Søgaard.
If All You Have is a Bit of
the Bible: Learning POS Taggers for Truly Low-
Resource Languages. In ACL.

2015.

[Bender2009] Emily M. Bender. 2009. Linguistically
naÏve != language independent: Why NLP needs
linguistic typology.
In Proceedings of the EACL
2009 Workshop on the Interaction Between Linguis-
tics and Computational Linguistics: Virtuous, Vi-
cious or Vacuous?

[Brants2000] Thorsten Brants. 2000. TnT – A Statisti-

cal Part-of-Speech Tagger. In ANLP.

[Brin and Page2012] Sergey Brin and Lawrence Page.
2012. Reprint of: The Anatomy of a Large-Scale
Hypertextual Web Search Engine. Computer net-
works, 56(18).

[Das and Petrov2011] Dipanjan Das and Slav Petrov.
2011. Unsupervised Part-of-Speech Tagging with
Bilingual Graph-Based Projections. In ACL.

[De Marneffe et al.2014] Marie-Catherine De Marn-
effe, Timothy Dozat, Natalia Silveira, Katri Haveri-
nen, Filip Ginter, Joakim Nivre, and Christopher D
Manning. 2014. Universal Stanford Dependencies:
A Cross-Linguistic Typology. In LREC.

[Gelling et al.2012] Douwe Gelling, Trevor Cohn, Phil
Blunsom, and Joao Graça. 2012. The Pascal Chal-
lenge on Grammar Induction. In Proceedings of the
NAACL-HLT Workshop on the Induction of Linguis-
tic Structure.

[Gillenwater et al.2010] Jennifer Gillenwater, Kuzman
Ganchev, Joao Graça, Fernando Pereira, and Ben
Taskar. 2010. Sparsity in Dependency Grammar
Induction. In ACL.

[Hwa et al.2005] Rebecca Hwa, Philip Resnik, Amy
Weinberg, Clara Cabezas, and Okan Kolak. 2005.
Bootstrapping Parsers via Syntactic Projection
Across Parallel Texts. Natural Language Engineer-
ing, 11(03).

6https://github.com/hectormartinez/ud_unsup_parser

[Johannsen et al.2015] Anders

Johannsen,

Héctor
Martínez Alonso, and Barbara Plank. 2015. Uni-
versal dependencies for Danish.
In International
Workshop on Treebanks and Linguistic Theories
(TLT14), page 157.

[Johannsen et al.2016] Anders Johannsen, Željko Agi´c,
and Anders Søgaard. 2016. Joint Part-of-Speech
and Dependency Projection from Multiple Sources.
In ACL.

[Judge et al.2006] John Judge, Aoife Cahill, and Josef
Van Genabith. 2006. Questionbank: Creating a
Corpus of Parse-Annotated Questions. In ACL.

[Klein and Manning2004] Dan Klein and Christopher
Manning. 2004. Corpus-Based Induction of Syn-
tactic Structure: Models of Dependency and Con-
stituency. In ACL.

[Lofgren2015] Peter Lofgren.

gorithms for Personalized PageRank.
abs/1512.04633.

2015. Efﬁcient Al-
CoRR,

[Ma and Xia2014] Xuezhe Ma and Fei Xia. 2014. Un-
supervised Dependency Parsing with Transferring
Distribution via Parallel Guidance and Entropy Reg-
ularization. In ACL.

[Martins et al.2013] André F. T. Martins, Miguel
Almeida, and Noah A. Smith. 2013. Turning on
the Turbo: Fast Third-Order Non-Projective Turbo
Parsers. In ACL.

[McDonald et al.2011] Ryan McDonald, Slav Petrov,
and Keith Hall. 2011. Multi-Source Transfer of
Delexicalized Dependency Parsers. In EMNLP.

[McDonald et al.2013] Ryan McDonald, Joakim Nivre,
Yvonne Quirmbach-Brundage, Yoav Goldberg, Di-
panjan Das, Kuzman Ganchev, Keith Hall, Slav
Petrov, Hao Zhang, Oscar Täckström, Claudia Be-
dini, Núria Bertomeu Castelló, and Jungmee Lee.
2013. Universal Dependency Annotation for Mul-
tilingual Parsing. In ACL.

[Naseem et al.2010] Tahira Naseem, Harr Chen, Regina
Barzilay, and Mark Johnson. 2010. Using Universal
Linguistic Knowledge to Guide Grammar Induction.
In EMNLP.

[Nivre et al.2015] Joakim Nivre, Željko Agi´c, Maria Je-
sus Aranzabe, Masayuki Asahara, Aitziber Atutxa,
Miguel Ballesteros, John Bauer, Kepa Bengoetxea,
Riyaz Ahmad Bhat, Cristina Bosco, Sam Bowman,
Giuseppe G. A. Celano, Miriam Connor, Marie-
Catherine de Marneffe, Arantza Diaz de Ilarraza,
Kaja Dobrovoljc, Timothy Dozat, Tomaž Erjavec,
Richárd Farkas, Jennifer Foster, Daniel Galbraith,
Filip Ginter, Iakes Goenaga, Koldo Gojenola, Yoav
Goldberg, Berta Gonzales, Bruno Guillaume, Jan
Hajiˇc, Dag Haug, Radu Ion, Elena Irimia, An-
ders Johannsen, Hiroshi Kanayama, Jenna Kan-
erva, Simon Krek, Veronika Laippala, Alessan-
dro Lenci, Nikola Ljubeši´c, Teresa Lynn, Christo-
pher Manning, C˘at˘alina M˘ar˘anduc, David Mareˇcek,

Héctor Martínez Alonso, Jan Mašek, Yuji Mat-
sumoto, Ryan McDonald, Anna Missilä, Verginica
Mititelu, Yusuke Miyao, Simonetta Montemagni,
Shunsuke Mori, Hanna Nurmi, Petya Osenova, Lilja
Øvrelid, Elena Pascual, Marco Passarotti, Cenel-
Augusto Perez, Slav Petrov, Jussi Piitulainen, Bar-
bara Plank, Martin Popel, Prokopis Prokopidis,
Sampo Pyysalo, Loganathan Ramasamy, Rudolf
Rosa, Shadi Saleh, Sebastian Schuster, Wolfgang
Seeker, Mojgan Seraji, Natalia Silveira, Maria Simi,
Radu Simionescu, Katalin Simkó, Kiril Simov,
Aaron Smith, Jan Štˇepánek, Alane Suhr, Zsolt
Szántó, Takaaki Tanaka, Reut Tsarfaty, Sumire Ue-
matsu, Larraitz Uria, Viktor Varga, Veronika Vincze,
Zdenˇek Žabokrtský, Daniel Zeman, and Hanzhi
Zhu. 2015. Universal Dependencies 1.2.

[Nivre et al.2016] Joakim Nivre, Marie-Catherine
de Marneffe, Filip Ginter, Yoav Goldberg, Jan
Hajic, Christopher D Manning, Ryan McDonald,
Slav Petrov, Sampo Pyysalo, Natalia Silveira, et al.
2016. Universal dependencies v1: A multilingual
treebank collection.
In Proceedings of the 10th
International Conference on Language Resources
and Evaluation (LREC 2016).

[Page et al.1999] Lawrence Page, Sergey Brin, Rajeev
Motwani, and Terry Winograd. 1999. The PageR-
ank Citation Ranking: Bringing Order to the Web.

[Plank and van Noord2010] Barbara Plank and Gertjan
van Noord. 2010. Grammar-Driven versus Data-
Driven: Which Parsing System Is More Affected by
Domain Shifts? In Proceedings of the 2010 Work-
shop on NLP and Linguistics: Finding the Common
Ground.

[Rasooli and Collins2015] Mohammad Sadegh Rasooli
and Michael Collins.
Density-Driven
Cross-Lingual Transfer of Dependency Parsers. In
EMNLP.

2015.

[Spitkovsky et al.2010a] Valentin Spitkovsky, Hiyan
Alshawi, and Daniel Jurafsky. 2010a. From Baby
Steps to Leapfrog: How Less is More in Unsuper-
vised Dependency Parsing. In NAACL.

[Spitkovsky et al.2010b] Valentin Spitkovsky, Hiyan
Alshawi, Daniel Jurafsky, and Christopher Manning.
2010b. Viterbi Training Improves Unsupervised De-
pendency Parsing. In CoNLL.

[Søgaard2011] Anders Søgaard. 2011. Data Point Se-
lection for Cross-Language Adaptation of Depen-
dency Parsers. In NAACL.

[Søgaard2012a] Anders Søgaard. 2012a. Two Base-
lines for Unsupervised Dependency Parsing. In Pro-
ceedings of the NAACL-HLT Workshop on the In-
duction of Linguistic Structure.

[Søgaard2012b] Anders Søgaard.

2012b. Unsuper-
vised dependency parsing without training. Natural
Language Engineering, 18(2).

[Tesnière1959] Lucien Tesnière. 1959. Eléments de

Syntaxe Structurale.

[Tiedemann and Agi´c2016] Jörg Tiedemann and Željko
Agi´c.
Synthetic Treebanking for Cross-
Lingual Dependency Parsing. Journal of Artiﬁcial
Intelligence Research, 55.

2016.

[Tiedemann2014] Jörg Tiedemann. 2014. Rediscover-
ing Annotation Projection for Cross-Lingual Parser
Induction. In COLING.

[Yarowsky et al.2001] David Yarowsky, Grace Ngai,
and Richard Wicentowski. 2001.
Inducing Mul-
tilingual Text Analysis Tools via Robust Projection
Across Aligned Corpora. In HLT.

[Zeman and Resnik2008] Daniel Zeman and Philip
Resnik. 2008. Cross-Language Parser Adaptation
Between Related Languages. In IJCNLP.

