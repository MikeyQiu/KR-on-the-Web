0
2
0
2
 
r
p
A
 
7
 
 
]

V
C
.
s
c
[
 
 
1
v
9
5
2
3
0
.
4
0
0
2
:
v
i
X
r
a

SC4D: A Sparse 4D Convolutional Network for
Skeleton-Based Action Recognition

Lei Shi1,2, Yifan Zhang1,2, Jian Cheng1,2,3 and Hanqing Lu1,2

1NLPR & AIRIA, Institute of Automation, Chinese Academy of Sciences
2School of Artiﬁcial Intelligence, University of Chinese Academy of Sciences, Beijing
100049, China
3CAS Center for Excellence in Brain Science and Intelligence Technology

Abstract. In this paper, a new perspective is presented for skeleton-
based action recognition. Speciﬁcally, we regard the skeletal sequence
as a spatial-temporal point cloud and voxelize it into a 4-dimensional
grid. A novel sparse 4D convolutional network (SC4D) is proposed to
directly process the generated 4D grid for high-level perceptions. With-
out manually designing the hand-crafted transformation rules, it makes
better use of the advantages of the convolutional network, resulting in
a more concise, general and robust framework for skeletal data. Besides,
by processing the space and time simultaneously, it largely keeps the
spatial-temporal consistency of the skeletal data, and thus brings better
expressiveness. Moreover, with the help of the sparse tensor, it can be
eﬃciently executed with less computations. To verify the superiority of
SC4D, extensive experiments are conducted on two challenging datasets,
namely, NTU-RGBD and SHREC, where SC4D achieves state-of-the-art
performance on both of them.

Keywords: Skeleton, Action Recognition, 4D Convolution.

1

Introduction

Action recognition is a popular research topic because it can be applied to
many practical ﬁelds such as human-computer interaction and video surveil-
lance [2,22,5,19]. In recent years, skeleton-based action recognition has drawn
considerable attentions due to its small amount of data, higher-level semantic
information and strong robustness for complicated environment [32,12,21]. In
detail, the skeletal data is generally a sequence of frames each contains the po-
sition information of the human body joints, which is expressed as the 2D/3D
coordinates of the camera coordinate system. It totally removes the background
information, and thus it focuses more on the human body itself. Recently, with
the success of the deep leaning, the data-driven methods have become the main-
stream for skeleton-based action recognition. In most of the existing neural-
network-based approaches, the joint coordinate is viewed as the attribute of
each element, and various hand-crafted strategies are designed to transform these
joints into various speciﬁc forms such as pseudo-images or graphs to feed them
into RNNs [8,26], CNNs [12,1] or GCNs [32,20] for feature extraction.

2

Lei Shi et al.

Generally speaking, the GCN-based method, which structures the skeletal
data as a spatiotemporal graph and feeds it into the graph convolutional net-
work, shows great eﬀectiveness and better performance. However, recent works
found that parameterizing the graph topology performs better than using the
ﬁxed human-body-based graph [21,9]. It means the prior-knowledge-based graph
structure is not the key of the success of the GCN-based method for skeletal data.
Its success may largely owes to the great generalizability and robustness of the
convolutional network. Thus, the GCN-based method is essentially the same
with the CNN-based method, which tries to design rules to transform the skele-
tal data into a CNN-suitable form. However, there are three limitations for these
methods: (1) Manually designed rules are not guaranteed to be an optimal choice
due to the human factor. Although some works propose to learn these rules in
the training process, it still more or less limits the ﬂexibility of the model. For ex-
ample, some works try to learn the graph topology, but the graph is still shifted
based on the human-body-based graphs and is limited by the hand-crafted graph
generation mechanism [21,9]. Besides, the raw skeletons exist in the metric space
(3D coordinate system) with a class-speciﬁc pattern, which are naturally suitable
for grid-based convolutional networks. It is unnecessary to still force a learning
of the transformation rules for convolutions in such a data-driven framework. (2)
Skeletal data is more ﬁxed compared with other graph-structured data such as
social networks or physical molecules. Its every element possesses a speciﬁc phys-
ical/semantic meaning, i.e., diﬀerent human joints such as feet or hands, which
is constant for various data samples. However, the convolution is weight-shared.
Thus, the ﬁxed rules force the convolutional model to mine constant interaction
patterns for diﬀerent joints and its neighbors of diﬀerent samples, which limits
the model’s ﬂexibility and capacity. There are also some works trying to avoid
constant contributions by permuting the arrangement of joints [1] or multiplying
attention weights for every joints [32], but they are treating symptoms and not
the root cause. (3) In previous works for skeletal data, the position information
is employed as features while the hand-crafted rules are used to organize con-
volutional operations. It is somewhat conﬂicted with the translation-invariant
character of the convolutional network. Since the input is position-variant, once
the skeleton is shifted a little bit, the extracted features also become completely
diﬀerent. This reduces the robustness and the generalizability of the model.

Instead, a new perspective is proposed in this work for skeleton-based action
recognition. We regard the skeletal sequence as a spatial-temporal point cloud
and voxelize it into a 4D grid. Similar to the RGB values of image pixels, each
voxel is attached with a feature vector that denotes whether there is a joint
in this position and which joint it is. Diﬀerent with previous works, it needs
no manual designs of transformation rules and can better utilize the power of
the data-driven mechanism. By performing convolution on 4D grid, the weight-
sharing mechanism is no longer worked for the ﬁxed relation structure, which
avoids constant contributions for diﬀerent joints and its neighbors of diﬀerent
samples. Besides, the position information is not included in the feature vector,
and thus it is more robust for position translation.

SC4D: A Sparse 4D Convolutional Network

3

Besides, diﬀerent with most existing methods that process the skeletal se-
quence frame by frame, we directly construct a 4D convolutional network to
process the generated 4D grid. It hierarchically extracts the spatial-temporal
features from low-levels to high-levels, which largely keeps the spatial-temporal
consistencies of the skeletal data. To construct 4D convolutional networks, a
naive method can be directly expanding the 3D convolutional kernels to 4D.
However, due to the additional temporal dimension of the kernel, it has a large
amount of computational cost. Since the skeletal data is very sparse, we propose
to employ the sparse tensor and the sparse convolution to reduce the computa-
tional cost inspired by [4]. It only tracks the non-empty voxels of the 4d grid,
along with their position information and associated feature vectors. The convo-
lutional operation is performed on these sparse voxels hierarchically, which gen-
erates the sparse output accordingly. It can generate similar results compared
with the dense convolution, but it is more eﬃcient and needs less computations.

Moreover, to further improve the performance, we propose three data aug-
mentation techniques. First, we suggest to interpolate points along bones, which
can utilize the prior knowledge of the human body and make it more distinctive.
The number of interpolations are determined based on the average length of
the bones in the human body. Second, since the skeletal joint is very sparse, a
dilation technique is used to enhance the spatial pattern to ease the recognition.
The values of the new added points are reduced proportionally to distinguish
them from the original joints. Third, as shown in previous methods [21], the bone
information and the motion information are eﬀective for skeleton-based action
recognition. To also exploit them for our voxelization-based methods, we propose
to transform the input from the coordinate space into the bone space and the
motion space. By modeling the information of the three spaces with multiple
streams and ﬁnally fusing the results, the performance is further improved.

The proposed method, namely, the sparse 4D convolutional network (SC4D),
is a general framework for skeletal data. To verify the eﬀectiveness of SC4D,
extensive experiments are conducted on two popular datasets for diﬀerent tasks,
i.e., NTU-RGBD for action recognition and SHREC for gesture recognition.
Although it is the ﬁrst try to voxelize skeletons into a 4D grid, SC4D achieves
state-of-the-art performance on both datasets, which illustrates that it is an
eﬀective method and a valuable-researched perspective.

Overall, our contributions lie in three aspects:

1. We propose a new perspective for skeleton-based action recognition, where
the skeletal data is voxelized into a 4d grid and is directly processed with
a sparse 4D convolutional network. The proposed framework is eﬀective,
concise, robust, and eﬃcient.

2. Two data enhancement strategies are proposed to augment the generated
skeletal grid to ease the recognition. We further transform the coordinate
data into two other spaces, i.e., the bone space and the motion space, to
utilize their complementarity to better recognize the human action.

4

Lei Shi et al.

3. The ﬁnal model achieves state-of-the-art performance on two challenging
datasets for diﬀerent tasks. The code will be released to facilitate future
works.

2 Related works

2.1 Skeleton-based action recognition

Skeleton-based action recognition has been studied for decades. Early-stage ap-
proaches concentrate on designing variable hand-crafted features [31]. With
the rise of deep learning, the mainstream approaches in recent years lie in
three aspect: (1) the RNN-based approaches where the skeleton sequence is
fed into the RNN models and the joints are modeled in a predeﬁned traver-
sal order [35,10,25,24]. (2) the CNN-based approaches where the skeleton se-
quence is transformed into a pseudo-image and are fed into the CNNs for recog-
nition [8,12,1]. (3) the GCN-based approaches where the skeleton sequence is
encoded into a spatiotemporal graph according to the physical structure of the
human body and is modeled with GCNs [32,28,21,20]. In contrast to previous
works, we project the sparse joints into a 4D grid and employ a sparse 4D con-
volutional network to extract features and make predictions.

2.2 Perception tasks based on 3D point cloud

Here, since we regard the skeletal data as point clouds, we introduce some recent
works for perception tasks based on the 3D point cloud. There have been many
works investigating how to model the point cloud with deep neural networks.
Some works exploit the metric space distance to aggregate features from local to
global in a hierarchical manner [17,23]. Some works transform the original data
into other forms such as surface [14], octree [18] or sparse lattice [27], which are
modeled with low-dimension neural networks. Some works perform the volumet-
ric representation for point cloud. For example, Tchapmi et al. [29] project the
raw point cloud data into 3D volumes and employ 3D convolutional networks
for 3D segmentation. It should be note that for many tasks, the point cloud is
video-based, and thus have an additional temporal dimension. For these tasks,
one strategy is to process the video frames sequentially and ﬁnally aggregate the
temporal features. For example, You et al. [34] represent multiple peoples ac-
tions captured by depth cameras as a sequence of point-cloud-based volume and
process the volumes frame by frame with a 3D convolutional network. Another
strategy is to directly model the videos in a 4D manner. For example, Choy et
al. [4] project the sequence of 3D scans into a 4D tesseract and introduce a 4D
sparse spatiotemporal convolutional network to extract features for 3D video
segmentation. This work follows the voxelization-based methods.

SC4D: A Sparse 4D Convolutional Network

5

Fig. 1. Pipeline overview.

3 Methods

3.1 Pipeline Overview

Fig. 1 shows the pipeline of our method. Skeletal data can be obtained by motion-
capture devices or pose estimation algorithms from videos. It is ﬁrst normalized
and voxelized into a 4D grid. Then the generated grid is directly processed
with a sparse 4D convolutional network, which hierarchically extracts semantics
from the 4D grid and outputs the high-level feature maps. Finally these feature
maps are global-average-pooled and classiﬁed with the SoftMax classiﬁer. The
following sections will go over these steps.

3.2 Voxelization

The raw skeletal data is a sequence of frames, each of which records the Carte-
sian coordinates of the human joints in the current frame. It is formulated as
t,m,n ∈ RCcoor , t = 1, 2, · · · , T, m =
t,m,n : Rraw
a set of joint coordinates as {Rraw
1, 2, · · · , M, n = 1, 2, · · · , N }, where T , M and N denote the number of frames,
persons and joints deﬁned in the data acquisition system, respectively. In the rest
of the paper, Ccoor is default to 3, i.e., the joints are obtained in a 3D coordinate
system.

Voxelizing these joint coordinates into a 4D grid is equivalent to calculat-
ing the new coordinates of these joints in the 4D-grid-based coordinate system.
t,m,n ∈ R3 →
First, we add the temporal coordinate for every joints, i.e., Rraw
t,m,n ∈ R4. The dimension order of the 4D coordinate system is default to
R4d
time-z-y-x, which means R4d
t,m,n(4) denote
the t-coordinate, z-coordinate, y-coordinate and x-coordinate, respectively. The
temporal coordinate is set as the index of frames, i.e., R4d

t,m,n(3) and R4d

t,m,n(2), R4d

t,m,n(1), R4d

t,m,n(1) = t.

Then, we normalize the coordinate into the range of zero to the grid size. In
detail, we use S ∈ R4 to denote the grid shape, e.g., S = [32, 32, 32, 32]. S(i)
denotes the grid size of the coordinate dimension i, where i = 1, 2, 3, 4. R4d

t,m,n

6

Lei Shi et al.

Fig. 2. Examples of two data enhancement techniques: bone interpolation and dilation.

is normalized to Rnor

t,m,n by:

Rnor

t,m,n(i) =

R4d

t,m,n(i) − min
∀t,m,n

{R4d

t,m,n(i)}

max
∀t,m,n

{R4d

t,m,n(i)} − min
∀t,m,n

{R4d

t,m,n(i)}

× S(i)

(1)

Finally, since the coordinate should be integer, the ﬁnal coordinate of the joints
in the 4d grid Rjpt
t,m,n + 0.5), where ﬂoor
means rounding down ﬂoat coordinates to integers.

t,m,n is obtained by Rjpt

t,m,n = f loor(Rnor

3.3 Feature Representation

t,m,n is attached with a feature vector Fjpt

After the voxelization process, we know the positions of all the joints in the
4D grid. Now we attach a feature vector for every joints to identify it. This
is similar with the word embedding process of the NLP ﬁeld, where words or
phrases from the vocabulary are mapped to vectors of real numbers. In detail,
Rjpt
t,m,n ∈ RCf ea . In this work, we propose
three strategies for embedding. The ﬁrst strategy is setting the feature of the
voxels that are occupied by joints as 1 and setting the feature of other voxels as
0. It means Cf ea = 1. However, this strategy cannot tell which joint occupies the
current voxel, and thus the information of the joint semantics is lost. The second
strategy is Cf ea = M + N . The ﬁrst M elements construct a one-hot vector that
indicates whether the current voxel is occupied by the joint of the mth person.
Similarly, the last N elements indicates whether it is the nth joint. In formulation,
Fjpt
t,m,n(i) = I{i == m||i == M + n}, where i = 1, 2, · · · , Cf ea. || denotes “or”.
If the expression is true, I{expression} = 1, otherwise I{expression} = 0.
However, it can not distinguish the situation that the joints of multiple persons
falling into a same voxel. The third strategy is Cf ea = M N . If the current voxel
is the nth joint of the mth person, Fjpt
t,m,n(i) = 0. In
formulation, Fjpt
t,m,n(i) = I{i == (n + m × N )}. The third strategy is competent
for more situations, but it needs more data volumes. In the following paper, we
use the third strategy.

t,m,n(i) = 1, otherwise Fjpt

3.4 Data Enhancement

The number of human joints are generally small. We propose two spatial feature
enhancement techniques to augment the local patterns as shown in Fig. 2. The

SC4D: A Sparse 4D Convolutional Network

7

ﬁrst strategy is interpolating points along the human bones, which are deﬁned
as the natural connections of the human body. Usually the human body can be
viewed as a tree structure [33,11]. Thus, the number of bones is one less than the
number of joints. For example, Fig. 4 shows the deﬁnition of the joints and bones
for the two datasets. First, we count the average length of every bones in the
dataset. The bone with the shortest length is deﬁned as the basic bone, which
means the number of interpolated points for this bone is one. Then, the number
of interpolated points for other bones are determined by the multiples of their
lengths to the length of the basic bone. The points are uniformly interpolated in
the segment between the two end joints of the bone. For example, if the length
of the basic bone is 2 and the length of the forearm is 10, we will uniformly
interpolate 5 points in the segment between the wrist and the elbow. As for
the features of the new interpolated points, they are calculated by the weighted
sum of the features of two end joints, where the weight is inversely proportional
to its distance from the corresponding end joint. In formulation, the number of
bones is B = N − 1. The number of the interpolated joints for every bones,
i.e., J ∈ RB, is calculated by above strategies. J(b) denotes the number of the
interpolated joints of the bth bone, where b = 1, 2, · · · , B denotes the bone index.
A map I ∈ RB×2 is deﬁned to record the indices of the two end joints of every
bones. I(b, 1) and I(b, 2) are the indices of two end joints of the bth bone, i.e.,
I(b, 1), I(b, 2) ∈ {1, 2, · · · , N }. The coordinate and the feature of the interpolated
t,m,b,j ∈ RCf ea , respectively, where
point are represented as Rinter
j = 1, 2, · · · , J(b) denotes the indices of the interpolated points of the bth bone.
Given Rjpt

t,m,b,j ∈ R4 and Finter

t,m,n, they are calculated as:

t,m,n and Fjpt

Rinter

t,m,b,j = Rjpt

t,m,I(b,1) + f loor(

Rjpt

t,m,I(b,2) − Rjpt

t,m,I(b,1)

J(b) + 1

× j + 0.5)

(2)

Finter

t,m,b,j = Fjpt

t,m,I(b,1) × (1 −

) + Fjpt

t,m,I(b,2) ×

j
J(b)

j
J(b)

The second strategy is the spatial dilation. We expand the coordinate of one
point along all its spatial dimensions according to the dilation number. Using the
1D data as an example, assume there is one point in position 2, after dilating it
with the dilation number 1, there will be three points in the position 1, 2 and 3.
For simple, when performing dilation on one dimension, the coordinates of other
dimensions are kept the same as before. The features of the new dilated points are
the same with the original points, but it is divided by a scale, which is inversely
proportional to its distance from the original point. In formulation, given the
dilation value Σ, the number of the new added points of one person in one
frame along the coordinate dimension c is Dc = 2ΣN (cid:48). c ∈ {2, 3, 4} because that
dilation is only performed along spatial dimensions. N (cid:48) denotes the number of the
original points, e.g., N (cid:48) = N + (cid:80)B
b J(b) if the bone interpolation is performed.
Then, the coordinate and the feature of the dilated point are represented as
t,m,c,d ∈ RCf ea , where d = 1, 2, · · · , Dc denotes the index
Rdil
of the new dilated points along the coordinate dimension c. Given Rjpt
t,m,n and

t,m,c,d ∈ R4 and Fdil

8

Lei Shi et al.

Fjpt

t,m,n, they are calculated as:

σ = ceil(

(d − 1)%(2N (cid:48)) + 1
2

)

φ = ceil(

d
2σ

)

Rdil

t,m,c,d(i) = Rjpt
t,m,c,d = Fjpt
Fdil

t,m,φ(i) + (−1)d × σ × I{i == c}
t,m,φ/σ

(3)

where i = 1, 2, 3, 4 denotes the index of the coordinate dimension of the dilated
points. σ ∈ {1, 2, . . . , Σ} denotes the distance from the new dilated points to the
original point. φ ∈ {1, 2, . . . , N } denotes the index of the corresponding original
point in the current dilation step. % denotes calculating remainder. Ceil means
rounding up the ﬂoat coordinates to integers.

3.5 Dense Convolution versus Sparse Convolution

After the voxelization and the data enhancement, we remove the duplicate points
that fall into the same voxel. In detail, we simply keep the ﬁrst one during the
traversal of points. Now the raw skeletal joint coordinate has been transformed
n ∈ R4 and a corresponding feature
into a coordinate vector of the 4d grid Rin
n ∈ RCf ea , where n = 1, 2, · · · , Nin, Nin denotes the total number
vector Fin
of points after performing the data enhancement techniques and removing du-
plications. Then, the problem is how to process them with neural networks.
The conventional method is updating the traditional 3D CNNs to 4D CNNs by
expanding the convolutional kernel dimension to 4, which we called dense con-
volution. Speciﬁcally, we create a 5D tensor Fden ∈ RS(1)×S(2)×S(3)×S(4)×Cf ea to
n . S ∈ R4 denotes the
represent a 4D grid and ﬁll its values based on Rin
grid shape. Fden can be directly fed into a 4D CNN.

n and Fin

However, because most of the elements in the generated 4D grid are 0, it is
unnecessary to perform convolutions for every elements, which in practice causes
the waste of computations and GPU memory. Instead, we consider to use the
sparse tensor to reduce the computations. We follow the method introduced in
[4]. To perform convolution or other operations sparsely, the key is to obtain a
mapping M to identify which input aﬀects which output according to the input
coordinates Rin
n and the operation deﬁnitions. It is deﬁned as pairs of lists of
input indices Iin ∈ RNin, output indices Iout ∈ RNout and weight indices Iwei ∈
RNwei (optional), i.e., M = {(Iin(i), Iout(j), Iwei(k))} where i ∈ {1, 2, · · · , Nin},
j ∈ {1, 2, · · · , Nout} and k ∈ {1, 2, · · · , Nwei}. Then the output is calculated ac-
cording to the inputs (Rin
n ), the weight (W(opt.)), the mapping (M) and
the deﬁnition of the operation (f ), i.e., Rout
n , W(opt.), M).
For convolutional operation, the input features are multiplied with the corre-
sponding weights, and then added to the corresponding output features based
on M. For pooling-based operations such as max-pooling and global-average-
pooling, weights are not needed. The input features are gathered and directly

n ←− f (Rin

n and Fin

n , Fout

n , Fin

SC4D: A Sparse 4D Convolutional Network

9

reduced based on M to get the output features. For non-spatial functions such
as Batch Normalization and ReLu, we can directly use the 1D dense operation
on the input features.

3.6 SC4D

Fig. 3. Architecture of the SC4D. Each convolution is appended with a Batch Normal-
ization layer and a ReLU layer. K denote the kernel size. C denote the basic number
of ﬁlters.

After deﬁning these operations, we can now build the network only with gen-
eralized sparse operations. With extensive experiments, the architecture of the
sparse 4D convolutional network (SC4D) for skeleton-based action recognition
is built as shown in Fig 3. It is inspired by the C3D network [30]. All layers
are built with sparse operations. There are totally 9 sparse convolutional layers.
Each sparse convolution is appended with a sparse Batch Normalization layer
and a sparse ReLU layer. The kernel size is 1 for the ﬁrst layer and K for others.
The number of ﬁlters are C, C, 2C, 4C, 4C, 8C, 8C, 8C, 8C, respectively. Both
K and C can be adjusted to balance the model size and the performance. The
ﬁrst sparse convolutional layer serves as an embedding layer to embed the one-
hot features into the feature space. The residual connections are added for every
convolutions except for the ﬁrst one to ease the gradient passing following [6].
Sparse max-pooling layer is added after the 2nd, 3rd, 5th and 7th convolutional
layers. The stride of the max-pooling is 2 for all dimensions. If the input grid size
is too small, the ﬁrst several pooling layers will be removed to save the resolu-
tion. A sparse global-average-pooling layer and a sparse fully-connected layer is
added in the end to make predictions. Dropout is used before the fully-connected
layer to avoid overﬁtting.

3.7 Multiple-Streams

Previous methods have shown that apart from the position information, the bone
information and the motion information of the skeletal data are also helpful for
action recognition [21]. Here, we transform the raw skeletal data from the co-
ordinate space into the bone space and the motion space to utilize these two

10

Lei Shi et al.

types of information. In detail, for bone information, given the raw joint coordi-
t,m,n ∈ R3 of the original space, we ﬁrst calculate the corresponding raw
nate Rraw
coordinate Rbraw

t,m,b ∈ R3 of the bone space by:

Rbraw

t,m,b = Rraw

t,m,I(b,1) − Rraw

t,m,I(b,2)

(4)

where b = 1, 2, · · · , B. The map I ∈ RB×2 records the indices of the two end
joints of every bones. Then, similar with the procedure of voxelizing Rraw
t,m,n intro-
duced in Sec. 3.2, Rbraw
t,m,b is voxelized into a bone-space-based 4D grid, resulting
in the sparse bone-space-based coordinate vector Rbone
t,m,b and the corresponding
feature vector Fbone

t,m,b. Fbone
Similarly, the raw coordinate Rmraw

t,m,b is same as Fjpt

t,m,n.

τ,m,n ∈ R3 of the motion space is obtained

by:

Rmraw

τ,m,n = Rraw

t+1,m,n − Rraw
t,m,n

(5)

τ,m,n and Fmotion

where τ = 1, 2, · · · , T −1. It is also voxelized into a motion-space-based 4D grid,
resulting in Rmotion
τ,m,n . Both the bone information and the motion in-
formation are modeled with two additional networks with the same architecture
as SC4D. The SoftMax scores of three streams are averaged to get the ﬁnal
prediction.

4 Experiments

4.1 Datasets and Training Details

We specially select two datasets, namely, NTU-RGBD and SHREC, with diﬀer-
ent tasks to show the generalizability of our model.

NTU-RGBD consists of 56,000 action clips in 60 action classes. Each action
is captured by 3 cameras. It provides 3D joint locations of 25 joints detected by
Kinect-V2 depth sensors as shown in Fig. 4, left. Each video has no more than 2
subjects. Because the accuracy of the cross-view benchmark is nearly saturated,
we conduct experiments on the cross-subject benchmark of the dataset, where
the training and testing sets are split based on diﬀerent subjects. Since the whole
dataset is large and the training is time-consuming, we extract a subset of NTU-
RGBD, namely, NTU-RGBD-SUB, for ablation studies. Speciﬁcally, because the
samples are captured by 3 camera, the samples captured by the ﬁrst camera are
used to form the NTU-RGBD-SUB.

SHREC contains 2800 gesture sequences performed by 28 subjects in two
ways: using one ﬁnger to perform gestures or using the whole hand to perform
gestures. It provides the 3D coordinates of 22 hand joints captured by Intel-
Real-Sense depth camera as shown in Fig. 4, right. This dataset has once been
used for the competition of SHREC’17 in conjunction with the Eurographics
3DOR’2017 Workshop, and thus it reﬂects the highest level in this ﬁeld.

All experiments are conducted on the PyTorch deep learning framework [15].
Stochastic gradient descent (SGD) with Nesterov momentum (0.9) is applied as

SC4D: A Sparse 4D Convolutional Network

11

Fig. 4. Illustration of the joint index and the bones for (a) NTU-RGBD dataset and
(b) SHREC dataset.

the optimization strategy. The batch size is set to 32. Cross-entropy is selected as
the loss function to back-propagate gradients. The weight decay is set to 0.0005.
Warm up is used with 5 epochs. Drop out rate is 0.2. To avoid overﬁtting, the
voxelized skeletons are randomly rotated and scaled for spatial dimensions. The
rotation range is [-10, 10]. The scale range is [0.8, 1.2].

Before the voxelization introduced in Sec. 3.2, the data is ﬁrst preprocessed
inspired by [21]. The center joint of each skeleton is set as the origin of the
coordinate. It is the joint #1 for both NTU-RGBD and SHREC as shown in
Fig. 4. Besides, for NTU-RGBD dataset, the skeletons are rotated to the same
viewpoint to reduce the intra-class variations. In detail, we keep the line between
the joint #9 and the joint #5 parallel with the x axis, and the line between the
joint #1 and the joint #2 parallel with the z axis.

4.2 Ablation Study

Ablation studies are conducted on NTU-RGBD-SUB. Firstly, we investigate the
strategies of the feature representation introduced in Sec. 3.3. We update the
C3D network [30] to C4D by simply expanding the dimension of the kernel from
3 to 4. Others are kept the same as before. Due to the GPU memory limitations,
we set the grid size to 16 (same for all dimensions) and the number of basic
channels to 16, i.e., the output channels of 8 convolutional layers in C4D is 16,
32, 64, 64, 128, 128, 128, 128. Tab. 1 shows the results of the three strategies,
where S3 performs better as expected.

Then, we replace the dense convolution with the sparse convolution to see the
diﬀerence. To keep the comparison fair, instead of using the ﬁnal SC4D showed in
Fig. 3, we use a C4D-comparable model named “Sparse C4D” for comparison,
whose architecture is the same with C4D except for the sparse operation. As

12

Lei Shi et al.

Table 1. Action recognition performance for diﬀerent feature representation strategies.
S1, S2 and S3 correspond to the strategies introduced in Sec. 3.3 sequentially.

shown in Tab 2, using dense C4D achieves only a little better than using sparse
C4D. But the sparse C4D largely saves the computations. For dense C4D, it
needs 4 TITANXP GPUs to train the model using the PyTorch framework. But
for sparse C4D, it needs only one-tenth memory of one GPU.

Table 2. Comparison of the dense convolution and the sparse convolution.

Strategy Acc. (%)
S1
S2
S3

80.6
81.7
82.3

Methods
Dense C4D
Sparse C4D

Acc. (%)
82.3
82.1

Now that we can save a large amount of GPU memories by using the sparse
convolution, we use the network architecture showed in Fig. 3, i.e., SC4D, for
experiments in the rest experiments. Compared with sparse C4D, residual con-
nections are added for every convolutional layers. A feature embedding layer is
added at the beginning. The basic number of ﬁlters is set to 32, i.e., C=32. The
basic kernel size is 3, i.e., K=3. With these designations, SC4D performs better
than sparse C4D. Then, we investigate the eﬀect of the grid size as shown in
Tab. 3. “Overlap” denotes the ratio of the number of points in sparse tensor to
the number of original points. It reﬂects the degree of diﬀerent joints falling into
the same voxel, which causes the loss of information. It is 100% when there are
no diﬀerent joints falling into the same voxel. The result shows that properly
increasing the grid size can improve the performance (Size=16 vs Size=32 vs
Size=64). We found that using larger grid can reduce the overlaps of the joints.
Thus, it can keep more useful information. However, it can not be increased
too large (Size=64 vs Size=128). It is because along with the increasing of the
grid size, the distance between the points also grows, which brings diﬃculties
for relation modeling with the ﬁx-size convolutional kernel.

The most important two factors that aﬀect the model performance is the
kernel size and the number of ﬁlters. Tab. 4 lists the performance of diﬀerent
conﬁgurations for the kernel size and the number of ﬁlters. The gird size is
ﬁxed to 32. It shows that increasing the kernel size improves the performance
especially when the original kernel size is small (K=3 vs K=4 vs K=5). We
believe it is because the large kernel size can help covering more points so that
capture more information in one convolutional step. The improvement decreases
or even becomes negative when the original kernel size is already large enough
(K=5 vs K=6). It is because the larger kernel size also brings more parameters

SC4D: A Sparse 4D Convolutional Network

13

Table 3. Action recognition performance for diﬀerent grid sizes. “Overlap” denotes
the ratio of the number of points in sparse tensor to the number of original points. It
is 100% when there no diﬀerent joints falling into the same voxel.

Grid size Acc. (%) Overlap (%)
Size=32
Size=16
Size=64
Size=128

98.6
93.1
99.7
99.9

87.9
86.6
88.8
88.5

that causes the diﬃculty for network training. The similar phenomenon is also
observed for the number of ﬁlters (C=32 vs C=64 vs C=128).

Table 4. Action recognition performance for diﬀerent kernel size (K) and the number
of ﬁlters (C). K and C are corresponded with Fig. 3.

Conﬁguration Acc. (%)
K=3, C=32
K=4, C=32
K=5, C=32
K=6, C=32
K=3, C=64
K=3, C=128

87.9
89.0
90.2
89.8
89.1
89.4

Then, we test the eﬀectiveness of two data enhancement strategies introduced
in Sec. 3.3. The grid size is increased to 64 to make the grid more sparse. Tab. 1
shows that both of the two strategies bring improvement. However, since the
number of points is also increased, the data becomes denser and it needs more
computations and memories.

Table 5. Action recognition performance for diﬀerent data enhancement strategies.

Strategy
no enhancement
+edge
+edge&dilate

Acc. (%)
88.8
88.9
89.7

Finally, we investigate the eﬀectiveness of the three streams introduced in
Sec. 3.7. They are processed by three networks with the same architecture, i.e.,
SC4D. The Sof tM ax scores are fused to get the ﬁnal prediction. It shows using
the bone information or the motion information performs worse than the original
data, but fusing three streams can largely improve the performance.

14

Lei Shi et al.

Table 6. Action recognition performance for diﬀerent streams.

Modality Acc. (%)
joint
bone
motion
fusion

88.8
86.9
81.5
90.7

4.3 Comparison with the SOTA

Although many strategies have been shown eﬀective for improving the perfor-
mance such as increasing the kernel size or the number of ﬁlters, due to the
GPU memory limitations, we have to made some trade-oﬀs. Finally, the kernel
size of the SC4D is set to 3 for the temporal dimension and 5 for the spatial
dimensions. The basic number of ﬁlters is 64, i.e., C=64. Grid size is 64 for
NTU-RGBD dataset and is 32 for SHREC dataset because the “overlap” (intro-
duced in Tab. 3) of the 32 × 32 × 32 × 32 grid already reaches 99.7% for SHREC.
The bone interpolation strategy is used for the joint stream. We test our ﬁnal
model on two datasets: NTU-RGBD for action recognition and SHREC for ges-
ture recognition. The results are showed in Tab. 7 and Tab. 8, where our SC4D
achieves state-of-the-art performance on two datasets. More comparisons are
showed in supplement materials. It veriﬁes the eﬀectiveness and generalizability
of our method.

Table 7. Comparison with the state-of-art-methods on NTU-RGBD dataset.

Method
AGC-LSTM [24]
2s-AGCN [21]
DGNN [20]
Bayesian-GCN [36]
NAS [16]
SC4D (ours)

Year Acc. (%)
2019
2019
2019
2020
2020
-

89.2
88.5
89.9
81.8
89.4
90.5

Table 8. Comparison with the state-of-art-methods on SHREC dataset.

Method
ST-GCN [32]
STA-Res-TCN [7]
ST-TS-HGR-NET [13]
DG-STA. [3]
SC4D (ours)

Year
2018
2018
2019
2019
-

14 gestures
92.7
93.6
94.3
94.4
95.8

28 gestures
87.7
90.7
89.4
90.7
93.6

SC4D: A Sparse 4D Convolutional Network

15

5 Conclusion

In this work, we propose a new perspective for skeleton-based action recogni-
tion, where the skeletal data is viewed as a point cloud and is voxelized into
a 4d grid for recognition. A novel sparse 4D convolutional network (SC4D) is
proposed to directly model the 4D grid, which largely keeps the spatial-temporal
consistencies of the skeletal data. The overall framework is concise, robust, and
eﬃcient due to the sparse operation. Besides, two data enhancement techniques
are introduced to augment the spatial pattern and ease the recognition. The data
is additionally projected into two other spaces to utilize the bone information
and the motion information for better performance. Our method achieves state-
of-the-art performance on two challenging datasets for diﬀerent tasks, which
conﬁrms its eﬀectiveness and generalizability.

16

Lei Shi et al.

References

1. Cao, C., Lan, C., Zhang, Y., Zeng, W., Lu, H., Zhang, Y.: Skeleton-Based Action
Recognition with Gated Convolutional Neural Networks. IEEE Transactions on
Circuits and Systems for Video Technology pp. 3247–3257 (2018)

2. Carreira, J., Zisserman, A.: Quo Vadis, Action Recognition? A New Model and
the Kinetics Dataset. In: The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR). pp. 6299–6308 (Jul 2017)

3. Chen, Y., Zhao, L., Peng, X., Yuan, J., Metaxas, D.N.: Construct Dynamic Graphs
for Hand Gesture Recognition via Spatial-Temporal Attention. In: BMVC (2019)
4. Choy, C., Gwak, J., Savarese, S.: 4d Spatio-Temporal ConvNets: Minkowski Con-

volutional Neural Networks. arXiv preprint arXiv:1904.08755 (2019)

5. Feichtenhofer, C., Fan, H., Malik, J., He, K.: Slowfast networks for video recogni-
tion. In: Proceedings of the IEEE International Conference on Computer Vision.
pp. 6202–6211 (2019)

6. He, K., Zhang, X., Ren, S., Sun, J.: Identity Mappings in Deep Residual Networks.
In: The European Conference on Computer Vision (ECCV). pp. 630–645 (2016)
7. Hou, J., Wang, G., Chen, X., Xue, J.H., Zhu, R., Yang, H.: Spatial-temporal at-
tention res-TCN for skeleton-based dynamic hand gesture recognition. In: The
European Conference on Computer Vision (ECCV). pp. 0–0 (2018)

8. Li, C., Zhong, Q., Xie, D., Pu, S.: Skeleton-based Action Recognition with Con-
volutional Neural Networks. In: IEEE International Conference on Multimedia &
Expo Workshops (ICMEW). pp. 597–600 (2017)

9. Li, M., Chen, S., Chen, X., Zhang, Y., Wang, Y., Tian, Q.: Actional-Structural
Graph Convolutional Networks for Skeleton-Based Action Recognition. pp. 3595–
3603 (2019)

10. Li, S., Li, W., Cook, C., Zhu, C., Gao, Y.: Independently recurrent neural net-
work (indrnn): Building A longer and deeper RNN. In: The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR). pp. 5457–5466 (2018)

11. Liu, J., Shahroudy, A., Xu, D., Wang, G., Liu, J., Shahroudy, A., Xu, D., Wang,
G.: Spatio-Temporal LSTM with Trust Gates for 3d Human Action Recognition.
In: The European Conference on Computer Vision (ECCV). vol. 9907, pp. 816–833
(2016)

12. Liu, M., Liu, H., Chen, C.: Enhanced skeleton visualization for view invariant

human action recognition. Pattern Recognition 68, 346–362 (2017)

13. Nguyen, X.S., Brun, L., Lzoray, O., Bougleux, S.: A neural network based on
SPD manifold learning for skeleton-based hand gesture recognition. In: The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) (Apr 2019)
14. Pan, H., Liu, S., Liu, Y., Tong, X.: Convolutional neural networks on 3d surfaces

using parallel frames. arXiv preprint arXiv:1808.04952 (2018)

15. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
Desmaison, A., Antiga, L., Lerer, A.: Automatic Diﬀerentiation in PyTorch. In:
Advancesin Neural Information Processing Systems Workshops (2017)

16. Peng, W., Hong, X., Chen, H., Zhao, G.: Learning Graph Convolutional Network
for Skeleton-based Human Action Recognition by Neural Searching. In: AAAI
(2020)

17. Qi, C.R., Yi, L., Su, H., Guibas, L.J.: Pointnet++: Deep hierarchical feature learn-
ing on point sets in a metric space. In: Advances in neural information processing
systems. pp. 5099–5108 (2017)

SC4D: A Sparse 4D Convolutional Network

17

18. Riegler, G., Osman Ulusoy, A., Geiger, A.: Octnet: Learning deep 3d representa-
tions at high resolutions. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. pp. 3577–3586 (2017)

19. Shi, L., Zhang, Y., Cheng, J., Lu, H.: Action Recognition via Pose-Based Graph
Convolutional Networks with Intermediate Dense Supervision. arXiv:1911.12509
(Nov 2019)

20. Shi, L., Zhang, Y., Cheng, J., Lu, H.: Skeleton-Based Action Recognition With
Directed Graph Neural Networks. In: The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR). pp. 7912–7921 (Jun 2019)

21. Shi, L., Zhang, Y., Cheng, J., Lu, H.: Two-Stream Adaptive Graph Convolutional
Networks for Skeleton-Based Action Recognition. In: The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR). pp. 12026–12035 (2019)
22. Shi, L., Zhang, Y., Jian, C., Hanqing, L.: Gesture Recognition using Spatiotemporal
Deformable Convolutional Representation. In: IEEE International Conference on
Image Processing (ICIP) (Oct 2019)

23. Shi, S., Wang, X., Li, H.: Pointrcnn: 3d object proposal generation and detection
from point cloud. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition. pp. 770–779 (2019)

24. Si, C., Chen, W., Wang, W., Wang, L., Tan, T.: An Attention Enhanced Graph
Convolutional LSTM Network for Skeleton-Based Action Recognition. In: The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 1227–
1236 (2019)

25. Si, C., Jing, Y., Wang, W., Wang, L., Tan, T.: Skeleton-Based Action Recognition
with Spatial Reasoning and Temporal Stack Learning. In: The European Confer-
ence on Computer Vision (ECCV). pp. 103–118 (Sep 2018)

26. Song, S., Lan, C., Xing, J., Zeng, W., Liu, J.: An End-to-End Spatio-Temporal
Attention Model for Human Action Recognition from Skeleton Data. In: AAAI.
pp. 4263–4270 (2017)

27. Su, H., Jampani, V., Sun, D., Maji, S., Kalogerakis, E., Yang, M.H., Kautz, J.:
Splatnet: Sparse lattice networks for point cloud processing. In: Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition. pp. 2530–2539
(2018)

28. Tang, Y., Tian, Y., Lu, J., Li, P., Zhou, J.: Deep Progressive Reinforcement Learn-
ing for Skeleton-Based Action Recognition. In: The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR). pp. 5323–5332 (2018)

29. Tchapmi, L., Choy, C., Armeni, I., Gwak, J., Savarese, S.: Segcloud: Semantic
segmentation of 3d point clouds. In: 2017 international conference on 3D vision
(3DV). pp. 537–547. IEEE (2017)

30. Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M.: Learning Spatiotem-
poral Features With 3d Convolutional Networks. In: The IEEE International Con-
ference on Computer Vision (ICCV). pp. 4489–4497 (Dec 2015)

31. Vemulapalli, R., Arrate, F., Chellappa, R.: Human action recognition by represent-
ing 3d skeletons as points in a lie group. In: The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR). pp. 588–595 (2014)

32. Yan, S., Xiong, Y., Lin, D.: Spatial Temporal Graph Convolutional Networks for

Skeleton-Based Action Recognition. In: AAAI (2018)

33. Yang, Y., Ramanan, D.: Articulated pose estimation with ﬂexible mixtures-of-
parts. In: The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR). pp. 1385–1392. IEEE (2011)

34. You, Q., Jiang, H.: Action4d: Online Action Recognition in the Crowd and Clutter.

pp. 11857–11866 (2019)

18

Lei Shi et al.

35. Zhang, P., Lan, C., Xing, J., Zeng, W., Xue, J., Zheng, N.: View Adaptive Re-
current Neural Networks for High Performance Human Action Recognition From
Skeleton Data. In: The IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR). pp. 2117–2126 (2017)

36. Zhao, R., Wang, K., Su, H., Ji, Q.: Bayesian Graph Convolution LSTM for Skeleton

Based Action Recognition. In: AAAI (2020)

