Sublabel-Accurate Convex Relaxation of

Vectorial Multilabel Energies

Emanuel Laude(cid:63)1, Thomas M¨ollenhoﬀ(cid:63)1, Michael Moeller1,
Jan Lellmann2, and Daniel Cremers1

1Technical University of Munich(cid:63)(cid:63)

2University of L¨ubeck

Abstract. Convex relaxations of multilabel problems have been demon-
strated to produce provably optimal or near-optimal solutions to a va-
riety of computer vision problems. Yet, they are of limited practical use
as they require a ﬁne discretization of the label space, entailing a huge
demand in memory and runtime. In this work, we propose the ﬁrst sub-
label accurate convex relaxation for vectorial multilabel problems. Our
key idea is to approximate the dataterm in a piecewise convex (rather
than piecewise linear) manner. As a result we have a more faithful ap-
proximation of the original cost function that provides a meaningful in-
terpretation for fractional solutions of the relaxed convex problem.

Keywords: Convex Relaxation, Optimization, Variational Methods

6
1
0
2
 
t
c
O
 
0
1
 
 
]

V
C
.
s
c
[
 
 
2
v
0
8
9
1
0
.
4
0
6
1
:
v
i
X
r
a

(a) Original dataterm

(b) Without lifting

(c) Classical lifting

(d) Proposed lifting

Fig. 1: In (a) we show a nonconvex dataterm. Convexiﬁcation without lifting
would result in the energy (b). Classical lifting methods [11] (c), approximate
the energy piecewise linearly between the labels, whereas the proposed method
results in an approximation that is convex on each triangle (d). Therefore, we
are able to capture the structure of the nonconvex energy much more accurately.

(cid:63) These authors contributed equally.
(cid:63)(cid:63) This work was supported by the ERC Starting Grant “Convex Vision”.

2

E. Laude, T. M¨ollenhoﬀ, M. Moeller, J. Lellmann, D. Cremers

1 Introduction

1.1 Nonconvex Vectorial Problems

In this paper, we derive a sublabel-accurate convex relaxation for vectorial op-
timization problems of the form

(cid:90)

Ω

min
Γ
→

u:Ω

ρ(cid:0)x, u(x)(cid:1) dx + λ T V (u),

R denotes a generally nonconvex
where Ω
Γ
pointwise dataterm. As regularization we focus on the total variation deﬁned as:

Rn and ρ : Ω

Rd, Γ

→

⊂

⊂

×

T V (u) =

C∞c (Ω,Rn

q

∈

q(x)

(cid:107)S∞ ≤

1

(cid:107)

sup
d),
×

(cid:90)

Ω(cid:104)

u, Div q

dx,

(cid:105)

where
For diﬀerentiable functions u we can integrate (2) by parts to ﬁnd

(cid:107) · (cid:107)S∞ is the Schatten-

∞

×

norm on Rn

d, i.e., the largest singular value.

(1)

(2)

(3)

(cid:90)

T V (u) =

u(x)

(cid:107)S1 dx,

Ω (cid:107)∇

(cid:107) · (cid:107)S1 penalizes the sum of the singular values of the
where the dual norm
Jacobian, which encourages the individual components of u to jump in the same
direction. This type of regularization is part of the framework of Sapiro and
Ringach [19].

1.2 Related Work

×

Due to its nonconvexity the optimization of (1) is challenging. For the scalar case
(n = 1), Ishikawa [9] proposed a pioneering technique to obtain globally optimal
solutions in a spatially discrete setting, given by the minimum s-t-cut of a graph
Γ . A continuous formulation was introduced by Pock
representing the space Ω
et al. [15] exhibiting several advantages such as less grid bias and parallelizability.
In a series of papers [16,14], connections of the above approaches were made
to the mathematical theory of cartesian currents [6] and the calibration method
for the Mumford-Shah functional [1], leading to a generalization of the convex
relaxation framework [15] to more general (in particular nonconvex) regularizers.
In the following, researchers have strived to generalize the concept of func-
tional lifting and convex relaxation to the vectorial setting (n > 1). If the
dataterm and the regularizer are both separable in the label dimension, one can
simply apply the above convex relaxation approach in a channel-wise manner

Sublabel-Accurate Convex Relaxation of Vectorial Multilabel Energies

3

to each component separately. But when either the dataterm or the regularizer
couple the label components, the situation becomes more complex [8,20].

The approach which is most closely related to our work, and which we con-
sider as a baseline method, is the one by Lellmann et al. [11]. They consider
coupled dataterms with coupled total variation regularization of the form (2).

A drawback shared by all mentioned papers is that ultimately one has to
discretize the label space. While Lellmann et al. [11] propose a sublabel-accurate
regularizer, we show that their dataterm leads to solutions which still have a
strong bias towards the label grid. For the scalar-valued setting, continuous label
spaces have been considered in the MRF community by Zach et al. [22] and Fix
et al. [5]. The paper [21] proposes a method for mixed continuous and discrete
vectorial label spaces, where everything is derived in the spatially discrete MRF
setting. M¨ollenhoﬀ et al. [12] recently proposed a novel formulation of the scalar-
valued case which retains fully continuous label spaces even after discretization.
The contribution of this work is to extend [12] to vectorial label spaces, thereby
complementing [11] with a sublabel-accurate dataterm.

1.3 Contribution

In this work we propose the ﬁrst sublabel-accurate convex formulation of vecto-
rial labeling problems. It generalizes the formulation for scalar-valued labeling
problems [12] and thus includes important applications such as optical ﬂow esti-
mation or color image denoising. We show that our method, derived in a spatially
continuous setting, has a variety of interesting theoretical properties as well as
practical advantages over the existing labeling approaches:

– We generalize existing functional lifting approaches (see Sec. 2.2).
– We show that our method is the best convex under-approximation (in a local

sense), see Prop. 1 and Prop. 2.

– Due to its sublabel-accuracy our method requires only a small amount of
labels to produce good results which leads to a drastic reduction in memory.
We believe that this is a vital step towards the real-time capability of lifting
and convex relaxation methods. Moreover, our method eliminates the label
bias, that previous lifting methods suﬀer from, even for many labels.

– In Sec. 2.3 we propose a regularizer that couples the diﬀerent label compo-
nents by enforcing a joint jump normal. This is in contrast to [8], where the
components are regularized separately.

– For convex dataterms, our method is equivalent to the unlifted problem –
see Prop. 4. Therefore, it allows a seamless transition between direct opti-
mization and convex relaxation approaches.

4

E. Laude, T. M¨ollenhoﬀ, M. Moeller, J. Lellmann, D. Cremers

1.4 Notation

x, y

= (cid:80)
We write
product if x, y are matrices. Similarly
usual Euclidean norm, respectively the Frobenius norm for matrices.

i xiyi for the standard inner product on Rn or the Frobenius
without any subscript denotes the

(cid:107) · (cid:107)

(cid:104)

(cid:105)

∈

Rn

y, x
(cid:104)

We denote the convex conjugate of a function f : Rn

by f ∗(y) =
supx
f (x). It is an important tool for devising convex relaxations,
as the biconjugate f ∗∗ is the largest lower-semicontinuous (lsc.) convex function
below f . For the indicator function of a set C we write δC, i.e., δC(x) = 0 if
x

Rn stands for the unit n-simplex.

otherwise. ∆U

∪ {∞}

C and

(cid:105) −

→

R

∈

∞

n ⊂

2 Convex Formulation

2.1 Lifted Representation

Motivated by Fig. 1, we construct an equivalent representation of (1) in a higher
dimensional space, before taking the convex envelope.

⊂

Let Γ

Rn be a compact and convex set. We partition Γ into a set

of
n-simplices ∆i so that Γ is a disjoint union of ∆i up to a set of measure zero.
Let tij be the j-th vertex of ∆i and denote by
the union of all
V
.
ij ≤ |V|
, 1
vertices, referred to as labels, with 1
Γ , we refer to u(x) as a sublabel. Any sublabel can be written
For u : Ω
as a convex combination of the vertices of a simplex ∆i with 1
for
appropriate barycentric coordinates α

t1, . . . , t|V|
{
j
≤

}
n + 1 and 1

≤ |T |

≤ |T |

→

≤

≤

≤

=

≤

T

i

i

∆U
n :

∈

u(x) = Tiα :=

αjtij , Ti := (ti1 , ti2, . . . , tin+1)

Rn

n+1.

×

(4)

By encoding the vertices tk
identify any u(x)
zeros and vice versa:

∈

using a one-of-
Γ with a sparse vector u(x) containing at least

representation ek we can
n many

∈ V

|V|

|V| −

u(x) = Eiα :=

αjeij , Ei := (ei1, ei2, . . . , ein+1)

R|V|×

n+1,

∈

∈

n+1
(cid:88)

j=1

n+1
(cid:88)

j=1

(5)

u(x) =

|V|(cid:88)

k=1

tkuk(x), α

∆U

n , 1

i

∈

≤

≤ |T |

.

The entries of the vector eij are zero except for the (ij)-th entry, which is equal
R|V| as the lifted representation of u. This one-
to one. We refer to u : Ω
to-one-correspondence between u(x) = Tiα and u(x) = Eiα is shown in Fig. 2.
Note that both, α and i depend on x. However, for notational convenience we
drop the dependence on x whenever we consider a ﬁxed point x

Ω.

→

∈

Sublabel-Accurate Convex Relaxation of Vectorial Multilabel Energies

5

Fig. 2: This ﬁgure illustrates our notation and the one-to-one correspondence
between u(x) = (0.3, 0.2)(cid:62) and the lifted u(x) containing the barycentric co-
ordinates α = (0.7, 0.1, 0.2)(cid:62) of the sublabel u(x)
. The
∆4 = conv
}
triangulation (
[0; 1] is visualized via the gray lines, cor-
) of Γ = [
responding to the triangles and the gray dots, corresponding to the vertices

t2, t3, t6
{

1; 1]

×

−

∈

V

T

,

=

(
−

{

V

1, 0)(cid:62), (0, 0)(cid:62), . . . , (1, 1)(cid:62)

, that we refer to as the labels.

}

2.2 Convexifying the Dataterm

Let for now the weight of the regularizer in (1) be zero. Then, at each point
x

Ω we minimize a generally nonconvex energy over a compact set Γ

Rn:

∈

⊂

(6)

ρ(u).

min
Γ
u
∈

We set up the lifted energy so that it attains ﬁnite values if and only if the
argument u is a sparse representation u = Eiα of a sublabel u

Γ :

ρ(u) = min

ρi(u),

ρi(u) =

1

i
≤|T |

≤

∈






,

∞

ρ(Tiα),

if u = Eiα, α

∆U
n ,

∈

(7)

otherwise.

Problems (6) and (7) are equivalent due to the one-to-one correspondence of
u = Tiα and u = Eiα. However, energy (7) is ﬁnite on a nonconvex set only. In
order to make optimization tractable, we minimize its convex envelope.

Proposition 1 The convex envelope of (7) is given as:

ρ∗∗(u) = sup
R

u, v

ρ∗i (v) =

|V|(cid:104)
∈
Eibi, v

v

(cid:104)

(cid:105)

ρ∗i (v),

1

≤|T |

(cid:105) −

max
i
≤
+ ρ∗i (A(cid:62)i E(cid:62)i v),
, Ai := (cid:0)M 1

ρi := ρ + δ∆i.

(8)

(cid:1), where M j

i are

bi and Ai are given as bi := M n+1
the columns of the matrix Mi := (T (cid:62)i , 1)−(cid:62)

i

i , M 2
i , . . . , M n
i
Rn+1
n+1.
×

∈

Proof. Follows from a calculation starting at the deﬁnition of ρ∗∗. See Ap-
pendix A for a detailed derivation.

6

E. Laude, T. M¨ollenhoﬀ, M. Moeller, J. Lellmann, D. Cremers

Standard lifting [11]

Proposed lifting

t1, t2, t3
{

Fig. 3: Geometrical intuition for the proposed lifting and standard lifting [11]
for the special case of 1-dimensional range Γ = [a, b] and 3 labels
.
}
The standard lifting correponds to a linear interpolation of the original cost in
between the locations t1, t2, t3, which are associated with the vertices e1, e2, e3
in the lifted energy (lower left). The proposed method extends the cost to the
relaxed set in a more precise way: The original cost is preserved on the connect-
ing lines between adjacent ei (black lines on the bottom right) up to concave
parts (red graphs and lower surface on the right). This information, which may
inﬂuence the exact location of the minimizer, is lost in the standard formula-
tion. If the solution of the lifted formulation u is in the interior (gray area) an
approximate solution to the original problem can still be obtained via Eq. (5).

n , i.e., ρ(u) = ρ(tk) if u = ek and +

The geometric intuition of this construction is depicted in Fig. 3. Note that if
one prescribes the value of ρi in (7) only on the vertices of the unit simplices
∆U
otherwise, one obtains the linear
, s = (ρ(ti), . . . , ρ(tL)) on the feasible set. This
biconjugate ρ∗∗(u) =
u, s
(cid:105)
(cid:104)
coincides with the standard relaxation of the dataterm used in [16,10,4,11]. In
that sense, our approach can be seen as a relaxing the dataterm in a more precise
way, by incorporating the true value of ρ not only on the ﬁnite set of labels
,
but also everywhere in between, i.e., on every sublabel.

∞

V

Sublabel-Accurate Convex Relaxation of Vectorial Multilabel Energies

7

2.3 Lifting the Vectorial Total Variation

We deﬁne the lifted vectorial total variation as

T V (u) =

Ψ (Du),

(9)

(cid:90)

Ω

where Du denotes the distributional derivative of u and Ψ is positively one-
homogeneous, i.e., Ψ (cu) = c Ψ (u), c (cid:62) 0. For such functions, the meaning of (9)
can be made fully precise using the polar decomposition of the Radon measure
Du [2, Cor. 1.29, Thm. 2.38]. However, in the following we restrict ourselves to
an intuitive motivation for the derivation of Ψ for smooth functions.

→

Our goal is to ﬁnd Ψ so that T V (u) = T V (u) whenever u : Ω

R|V|
corresponds to some u : Ω
Γ , in the sense that u(x) = Eiα whenever u(x) =
Tiα. In order for the equality to hold, it must in particular hold for all u that are
classically diﬀerentiable, i.e., Du =
u(x) is of rank
Rd. This rank 1 constraint
1, i.e.,
ν(x) for some ν(x)
enforces the diﬀerent components of u to have the same jump normal, which is
desirable in many applications. In that case, we observe

u, and whose Jacobian

u(x) = (Tiα

Tjβ)

→

∇

∇

∇

−

⊗

∈

(cid:90)

T V (u) =

Tiα

Tjβ

ν(x)
(cid:107)

(cid:107) · (cid:107)

−

dx.

Ω (cid:107)

(10)

For the corresponding lifted representation u, we have
ν(x). Therefore it is natural to require Ψ (
Tiα
(cid:107)
by these observations, we deﬁne

Ejβ)
⊗
ν(x)) :=
in order to achieve the goal T V (u) = T V (u). Motivated

u(x) = (Eiα
Ejβ)

u(x)) = Ψ ((Eiα

−
⊗

ν(x)

(cid:107) · (cid:107)

Tjβ

∇

∇

−

−

(cid:107)

Ψ (p) :=

Tiα
(cid:107)

−

Tjβ

ν

(cid:107) · (cid:107)

(cid:107)

if p = (Eiα

Ejβ)

ν,

−

⊗

(11)

otherwise,






∞

where α, β
≤
is intractable, we derive a “locally” tight convex underapproximation:

. Since the convex envelope of (9)

n+1, ν

≤ |T |

i, j

∈

∈

∆U

Rd and 1

R(u) =

sup
Rd

→

q:Ω

×|V|

(cid:90)

Ω(cid:104)

u, Div q

Ψ ∗(q) dx.

(cid:105) −

Proposition 2 The convex conjugate of Ψ is

Ψ ∗(q) = δ

(q)

K

with convex set

=

K

(cid:92)

(cid:110)

q

∈

Rd

×|V|

(cid:12)
(cid:12)

1

i,j

≤

≤|T |

Qiα
(cid:107)

−

Qjβ

(cid:107) ≤ (cid:107)

Tiα

Tjβ

, α, β
(cid:107)

∈

−

(cid:111)

∆U

n+1

, (14)

and Qi = (qi1, qi2, . . . , qin+1 )

Rd

n+1. qj

×

Rd are the columns of q.

∈

∈

(12)

(13)

8

E. Laude, T. M¨ollenhoﬀ, M. Moeller, J. Lellmann, D. Cremers

Proof. Follows from a calculation starting at the deﬁnition of the convex conju-
gate Ψ ∗. See Appendix A.

Interestingly, although in its original formulation (14) the set
many constraints, one can equivalently represent

by ﬁnitely many.

K

has inﬁnitely

K

Proposition 3 The set

in equation (14) is the same as

K

∈

(cid:110)

q

=

K

∈

Rd

×|V|

(cid:13)
(cid:13)Di
q

(cid:13)
(cid:13)S∞ ≤

|

1, 1

i

≤

≤ |T |

(cid:111)

, Di

q = QiD (TiD)−

1,

(15)

where the matrices QiD

Rd

n and TiD

×

Rn

×

n are given as

QiD := (cid:0)qi1

qin+1, . . . , qin

−

−

tin+1, . . . , tin

−

tin+1(cid:1) .

−

∈
qin+1(cid:1) , TiD := (cid:0)ti1

Proof. Similar to the analysis in [11], equation (14) basically states the Lipschitz
×|V|.
continuity of a piecewise linear function deﬁned by the matrices q
Therefore, one can expect that the Lipschitz constraint is equivalent to a bound
on the derivative. For the complete proof, see Appendix A.

Rd

∈

2.4 Lifting the Overall Optimization Problem

Combining dataterm and regularizer, the overall optimization problem is given

(cid:90)

min
R
→

u:Ω

sup

ρ∗∗(u) +

u, Div q

dx.

(cid:104)

(cid:105)

|V|

q:Ω

Ω

→K

(16)

A highly desirable property is that, opposed to any other vectorial lifting ap-
proach from the literature, our method with just one simplex applied to a convex
problem yields the same solution as the unlifted problem.

Proposition 4 If the triangulation contains only 1 simplex,

, i.e.,
∆
}
{
= n + 1, then the proposed optimization problem (16) is equivalent to

=

T

|V|

(cid:90)

Ω

min
→

u:Ω

∆

(ρ + δ∆)∗∗(x, u(x)) dx + λT V (u),

(17)

which is (1) with a globally convexiﬁed dataterm on ∆.

Proof. For u = tn+1 + T D˜u the substitution u =
ρ∗∗ and R yields the result. For a complete proof, see Appendix A.

˜u1, . . . , ˜un, 1

−

(cid:16)

(cid:17)

(cid:80)n

j=1 ˜uj

into

Sublabel-Accurate Convex Relaxation of Vectorial Multilabel Energies

9

3 Numerical Optimization

3.1 Discretization

For now assume that Ω
denote a ﬁnite-diﬀerence divergence operator with Div q : Ω
relaxed energy minimization problem becomes

Rd is a d-dimensional Cartesian grid and let Div
R|V|. Then the

→

⊂

min
R
→

u:Ω

|V|

max
q:Ω
→K

(cid:88)

x

Ω

∈

ρ∗∗(x, u(x)) +

Div q, u
(cid:104)

.
(cid:105)

(18)

In order to get rid of the pointwise maximum over ρ∗i (v) in Eq. (8), we introduce
additional variables w(x)
Ω
so that w(x) attains the value of the pointwise maximum:

R and additional constraints (v(x), w(x))

∈ C

, x

∈

∈

min
R
→

u:Ω

|V|

max
(v,w):Ω
q:Ω

(cid:88)

(cid:104)

→C

x

Ω

∈

→K

u(x), v(x)

w(x) +

(cid:105) −

Div q, u
(cid:104)

,
(cid:105)

(19)

where the set

is given as

C

=

C

1

i
≤

≤|T |

(cid:92)

Ci,

Ci :=

(cid:110)

(x, y)

R|V|

+1

∈

ρ∗i (x)

|

≤

(cid:111)

y

.

(20)

For numerical optimization we use a GPU-based implementation1 of a ﬁrst-order
primal-dual method [14]. The algorithm requires the orthogonal projections of
the dual variables onto the sets
in every iteration. However, the
respectively
K
+ 1 is diﬃcult for large values of
projection onto an epigraph of dimension
|V|
Ω as (n + 1)-
, x
∈ Ci, 1

|V|
≤ |T |
dimensional epigraph constraints introducing variables ri(x)

. We rewrite the constraints (v(x), w(x))

∈
Rn, si(x)

R:

≤

C

i

(cid:0)ri(x)(cid:1)

ρ∗i

≤

si(x),

ri(x) = A(cid:62)i E(cid:62)i v(x),

si(x) = w(x)

Eibi, v(x)

(21)

∈

− (cid:104)

∈

.
(cid:105)

These equality constraints can be implemented using Lagrange multipliers. For
the projection onto the set

we use an approach similar to [7, Figure 7].

K

3.2 Epigraphical Projections

is a central part
Computing the Euclidean projection onto the epigraph of ρ∗i
of the numerical implementation of the presented method. However, for n > 1
this is nontrivial. Therefore we provide a detailed explanation of the projection
methods used for diﬀerent classes of ρi. We will consider quadratic, truncated
quadratic and piecewise linear ρ.

1 https://github.com/tum-vision/sublabel_relax

10

E. Laude, T. M¨ollenhoﬀ, M. Moeller, J. Lellmann, D. Cremers

Quadratic case: Let ρ be of the form ρ(u) = a
2 u(cid:62)u + b(cid:62)u + c. A direct projection
onto the epigraph of ρ∗i = (ρ + δ∆i)∗ for n > 1 is diﬃcult. However, the epigraph
can be decomposed into separate epigraphs for which it is easier to project onto:
For proper, convex, lsc. functions f, g the epigraph of (f + g)∗ is the Minkowski
sum of the epigraphs of f ∗ and g∗ (cf. [17, Exercise 1.28, Theorem 11.23a]).
This means that it suﬃces to compute the projections onto the epigraphs of
a quadratic function f ∗ = ρ∗ and a convex, piecewise linear function g∗(v) =
max1

by rewriting constraint (21) as

tij , v

j
≤

≤

n+1(cid:104)

ρ∗(rf )

sf , δ∆i ∗(cg)

dg s.t. (r, s) = (rf , sf ) + (cg, dg).

(22)

≤

For the projection onto the epigraph of a n-dimensional quadratic function we
use the method described in [20, Appendix B.2]. The projection onto a piecewise
linear function is described in the last paragraph of this section.

(cid:105)

≤

ν, a

{

2 u(cid:62)u + b(cid:62)u + c

Truncated quadratic case: Let ρ be of the form ρ(u) = min
}
as it is the case for the nonconvex robust ROF with a truncated quadratic
dataterm in Sec. 4.2. Again, a direct projection onto the epigraph of ρ∗i is diﬃcult.
However, a decomposition of the epigraph into simpler epigraphs is possible
as the epigraph of min
∗ is the intersection of the epigraphs of f ∗ and
g∗. Hence, one can separately project onto the epigraphs of (ν + δ∆i)∗ and
( a
2 u(cid:62)u + b(cid:62)u + c + δ∆i)∗. Both of these projections can be handled using the
methods from the other paragraphs.

f, g
{

}

Piecewise linear case: In case ρ is piecewise linear on each ∆i, i.e., ρ attains
∆i and interpolates
ﬁnite values at a discrete set of sampled sublabels
linearly between them, we have that

Vi ⊂

(ρ + δ∆i)∗(v) = max
∈V

τ

i (cid:104)

τ, v

(cid:105) −

ρ(τ ).

(23)

Again this is a convex, piecewise linear function. For the projection onto the
epigraph of such a function, a quadratic program of the form

(x,y)

Rn+1

min
∈

1
2 (cid:107)

x

c

2 +
(cid:107)

−

1
2 (cid:107)

y

d
(cid:107)

−

2 s.t.

τ, x
(cid:104)

(cid:105) −

ρ(τ )

y,

τ
∀

≤

∈ Vi

(24)

needs to be solved. We implemented the primal active-set method described
in [13, Algorithm 16.3], and found it solves the program in a few (usually 2
10)
iterations for a moderate number of constraints.

−

Sublabel-Accurate Convex Relaxation of Vectorial Multilabel Energies

11

Naive, 81 labels.

[11], 81 labels.

Ours, 4 labels.

→

[
−

1, 1]2, discretized
Fig. 4: ROF denoising of a vector-valued signal f : [0, 1]
on 50 points (shown in red). We compare the proposed approach (right) with
two alternative techniques introduced in [11] (left and middle). The labels are
visualized by the gray grid. While the naive (standard) multilabel approach from
[11] (left) provides solutions that are constrained to the chosen set of labels, the
sublabel accurate regularizer from [11] (middle) does allow sublabel solutions,
yet – due to the dataterm bias – these still exhibit a strong preference for the grid
points. In contrast, the proposed approach does not exhibit any visible grid bias
providing fully sublabel-accurate solutions: With only 4 labels, the computed
solutions (shown in blue) coincide with the “unlifted” problem (green).

4 Experiments

4.1 Vectorial ROF Denoising

In order to validate experimentally, that our model is exact for convex dataterms,
we evaluate it on the Rudin-Osher-Fatemi [18] (ROF) model with vectorial
TV (2). In our model this corresponds to deﬁning ρ(x, u(x)) = 1
2.
As expected based on Prop. 4 the energy of the solution of the unlifted problem
is equal to the energy of the projected solution of our method for
= 4 up to
machine precision, as can be seen in Fig. 4 and Fig. 5. We point out, that the
sole purpose of this experiment is a proof of concept as our method introduces
an overhead and convex problems can be solved via direct optimization. It can
be seen in Fig. 4 and Fig. 5, that the baseline method [11] has a strong label
bias.

I(x)
(cid:107)

u(x)

|V|

2 (cid:107)

−

4.2 Denoising with Truncated Quadratic Dataterm

For images degraded with both, Gaussian and salt-and-pepper noise we deﬁne
2, ν(cid:9). We solve the problem
the dataterm as ρ(x, u(x)) = min (cid:8) 1
I(x)
2 (cid:107)
(cid:107)

u(x)

−

12

E. Laude, T. M¨ollenhoﬀ, M. Moeller, J. Lellmann, D. Cremers

Input image

Unlifted Problem,

Ours,

= 1,

E = 992.50

|T |
= 4,

|V|

E = 992.51

Ours,
= 2

= 6
|T |
2
2
×

|V|

×
E = 993.52

Baseline,
= 4

4

4,

|V|

×
×
E = 2255.81

Fig. 5: Convex ROF with vectorial TV. Direct optimization and proposed method
yield the same result. In contrast to the baseline method [11] the proposed ap-
proach has no discretization artefacts and yields a lower energy. The regulariza-
tion parameter is chosen as λ = 0.3.

Noisy input

Ours,

= 1,

|T |
= 4,

|V|

E = 2849.52

Ours,
= 2

= 6,
|T |
2,
2
×
×
E = 2806.18

|V|

Ours,

= 48,
3,
3

= 3

|T |
×
×
E = 2633.83

|V|

Baseline,
= 4

4

4,

|V|

×
×
E = 3151.80

Fig. 6: ROF with a truncated quadratic dataterm (λ = 0.03 and ν = 0.025).
Compared to the baseline method [11] the proposed approach yields much better
results, already with a very small number of 4 labels.

using the epigraph decomposition described in the second paragraph of Sec. 3.2.
leads to lower energies and
It can be seen, that increasing the number of labels
at the same time to a reduced eﬀect of the TV. This occurs as we always compute
a piecewise convex underapproximation of the original nonconvex dataterm, that
gets tighter with a growing number of labels. The baseline method [11] again
produces strong discretization artefacts even for a large number of labels
=
4

4 = 64.

|V|

|V|

4

×

×

Sublabel-Accurate Convex Relaxation of Vectorial Multilabel Energies

13

Image 1

|V|

= 5

5,
[8],
0.67 GB, 4 min
aep = 2.78

×

11,

[8],

|V|

= 11
2.1 GB, 12 min
aep = 1.97

×

17,

[8],

|V|

= 17
4.1 GB, 25 min
aep = 1.63

×

28,

[8],

|V|

= 28
9.3 GB, 60 min
aep = 1.39

×

Image 2

|V|

= 3

[11],
3,
0.67 GB, 0.35 min
aep = 5.44

×

|V|

= 5

[11],
5,
2.4 GB, 16 min
aep = 4.22

×

|V|

= 7

[11],
7,
5.2 GB, 33 min
aep = 2.65

×

[11],
9,
Out of memory.

= 9

|V|

×

Ground truth

|V|

= 2

Ours,
2,
0.63 GB, 17 min
aep = 1.28

×

Ours,

= 3

3,

Ours,

= 4

4,

|V|

×
1.9 GB, 34 min
aep = 1.07

|V|

×
4.1 GB, 41 min
aep = 0.97

|V|

= 6

Ours,
6,
10.1 GB, 56 min
aep = 0.9

×

Fig. 7: We compute the optical ﬂow using our method, the product space ap-
proach [8] and the baseline method [11] for a varying amount of labels and
compare the average endpoint error (aep). The product space method clearly
outperforms the baseline, but our approach ﬁnds the overall best result already
with 2
2 labels. To achieve a similarly precise result as the product space
method, we require 150 times fewer labels, 10 times less memory and 3 times
less time. For the same number of labels, the proposed approach requires more
memory as it has to store a convex approximation of the energy instead of a
linear one.

×

4.3 Optical Flow

We compute the optical ﬂow v : Ω
The label space Γ = [
displacement d
I1(x + v(x))

−

∈

R2 between two input images I1, I2.
d, d]2 is chosen according to the estimated maximum

→

R between the images. The dataterm is ρ(x, v(x)) =

, and λ(x) is based on the norm of the image gradient
(cid:107)
In Fig. 7 we compare the proposed method to the product space approach
[8]. Note that we implemented the product space dataterm using Lagrange mul-

∇

I2(x)
(cid:107)
I1(x).

−

14

E. Laude, T. M¨ollenhoﬀ, M. Moeller, J. Lellmann, D. Cremers

(a) Image 1 and 2

(b) Proposed,

= 2

2

×

|V|

(c) Baseline,

= 7

7

×

|V|

81
Fig. 8: Large displacement ﬂow between two 640
search window. The result of our method with 4 labels is shown in (b), the
baseline [11] in (c). Our method can correctly identify the large motion.

480 images (a) using a 81

×

×

×

−

tipliers, also referred to as the global approach in [8]. While this increases the
memory consumption, it comes with lower computation time and guaranteed
15, 15]2 on
convergence. For our method, we sample the label space Γ = [
150
150 sublabels and subsequently convexify the energy on each triangle us-
ing the quickhull algorithm [3]. For the product space approach we sample the
label space at equidistant labels, from 5
27. As the regularizer from the
×
product space approach is diﬀerent from the proposed one, we chose µ diﬀerently
for each method. For the proposed method, we set µ = 0.5 and for the product
space and baseline approach µ = 3. We can see in Fig. 7, our method outperforms
the product space approach w.r.t. the average end-point error. Our method out-
performs previous lifting approaches: In Fig. 8 we compare our method on large
displacement optical ﬂow to the baseline [11]. To obtain competitive results on
the Middlebury benchmark, one would need to engineer a better dataterm.

5 to 27

×

5 Conclusions

We proposed the ﬁrst sublabel-accurate convex relaxation of vectorial multil-
abel problems. To this end, we approximate the generally nonconvex dataterm
in a piecewise convex manner as opposed to the piecewise linear approxima-
tion done in the traditional functional lifting approaches. This assures a more
faithful approximation of the original cost function and provides a meaningful
interpretation for the non-integral solutions of the relaxed convex problem. In
experimental validations on large-displacement optical ﬂow estimation and color
image denoising, we show that the computed solutions have superior quality
to the traditional convex relaxation methods while requiring substantially less
memory and runtime.

Sublabel-Accurate Convex Relaxation of Vectorial Multilabel Energies

15

A Theory

Proof (Proof of Proposition 1). By deﬁnition the biconjugate of ρ is given as

We proceed computing the conjugate of ρi:

ρ∗∗(u) = sup
R

v

|V|(cid:104)

u, v

(cid:105) −

∈
= sup
R
v

|V|(cid:104)

∈

u, v

(cid:105) −

1

(cid:18)

≤|T |

1

min
i
≤
max
i
≤

≤|T |

(cid:19)∗

ρi(v)

ρ∗i (v).

ρ∗i (v) = sup
|V|(cid:104)
R

u

u, v

(cid:105) −

ρi(u)

∈
= sup
∆U
α

n+1

∈

Eiα, v
(cid:104)

(cid:105) −

ρ (Tiα) ,

We introduce the substitution r := Tiα

α = K −
i

1

, Ki :=

(cid:32)

(cid:33)

r
1

∈

∆i and obtain
(cid:32)

(cid:33)

Ti
1(cid:62)

∈

Rn+1

n+1,

×

since Ki is invertible for (
,
V
1. With this we can further rewrite the conjugate as

T

) being a non-degenerate triangulation and (cid:80)n+1

j=1 αj =

. . . = sup

Air + bi, E(cid:62)i v

ρ(r)

(cid:105) −

∆i(cid:104)

r

∈
Eibi, v
(cid:104)

=

+ sup
r

Rn(cid:104)

(cid:105)

∈

r, A(cid:62)i E(cid:62)i v

ρ(r)

δ∆i(r)

(cid:105) −

−

=

Eibi, v
(cid:104)

(cid:105)

+ ρ∗i (A(cid:62)i E(cid:62)i v).

Proof (Proof of Proposition 2). Deﬁne Ψ i,j as

Ψ i,j(p) :=

Tiα

Tjβ

−

ν

(cid:107) · (cid:107)

(cid:107)

if p = (Eiα

Ejβ)ν(cid:62), α, β

−

∆U

n+1, ν

∈

Rd,

∈

otherwise.



(cid:107)


∞

Then, Ψ can be rewritten as a pointwise minimum over the individual Ψ i,j

Ψ (p) = min

Ψ i,j(p).

1

i,j

≤

≤|T |

We begin computing the conjugate of Ψ i,j

Ψ ∗i,j(q) = sup
Rd

p

×|V|(cid:104)

p, q

(cid:105) −

Ψ i,j(p)

sup
ν

Rd(cid:104)

∈
Tiα
(
(cid:107)

∈
= sup
∆U
α,β

n+1

∈

= sup
∆U
α,β

n+1

∈
i,j (q),

= δ

K

Qiα

Qjβ, ν

−

Tiα

Tjβ

(cid:105) − (cid:107)

−

ν

(cid:107) · (cid:107)

(cid:107)

Tjβ

−

(cid:107) · (cid:107) · (cid:107)

)∗ (Qiα

Qjβ)

−

(25)

(26)

(27)

(28)

(29)

(30)

(31)

16

E. Laude, T. M¨ollenhoﬀ, M. Moeller, J. Lellmann, D. Cremers

Fig. 9: Figure illustrating the second direction of the proof of Proposition 4. The
). The line segment between
gray dots and lines visualize the triangulation (
Tiα and Tjβ is composed of shorter line segments which are fully contained in
one of the triangles. On each of the triangles the inequality (39) holds, which
allows to conclude that it holds for the whole line segment.

V

T

,

with the set Ki,j being deﬁned as

Ki,j :=

(cid:110)

q

∈

Rd

×|V|

(cid:12)
(cid:12)

(cid:107)

Qiα

Qjβ

−

(cid:107) ≤ (cid:107)

Tiα

Tjβ

, α, β
(cid:107)

∈

−

∆U

n+1

(cid:111)

.

(32)

Since the maximum over indicator functions of sets is equal to the indicator
function of the intersection of the sets we obtain for Ψ ∗

Ψ ∗(q) = max

Ψ ∗i,j(q)

1

i,j

≤

≤|T |

= δ

(q).

K

Proof (Proof of Proposition 3). Let q
for all α, β

n+1 and 1

∆U

i, j

≤

≤ |T |

∈

Rd

×|V| s.t.

∈
. For any 1

(cid:107)
i

Qiα

−
≤ |T |

≤

Qjβ
deﬁne

(cid:107) ≤ (cid:107)

Tiα

Tjβ

−

(cid:107)

fi : Rn

Rn,
n
(cid:88)

l=1

→

(cid:55)→

n
(cid:88)

l=1

−

(α1, ..., αn)

αltil + (1

αl)tin+1 = Tiα,

(33)

(34)

Sublabel-Accurate Convex Relaxation of Vectorial Multilabel Energies

17

and analogously

gi : Rn

R|V|
n
(cid:88)

l=1

→

(cid:55)→

n
(cid:88)

l=1

−

(α1, ..., αn)

αlqil + (1

αl)qin+1 = Qiα.

Let us choose an α
Tjβ
Tiα
(cid:107)

−

(cid:107)

for all α, β

∈

∈

Rn such that αi > 0, (cid:80)

∆U

n+1 and 1

i, j

≤

≤ |T |

l αl < 1. Then
implies that

Qiα
(cid:107)

−

Qjβ

(cid:107) ≤

gi(α)

gi(α

h)

−

−

(cid:107) ≤ (cid:107)

(cid:107)

fi(α)

fi(α

−

,

h)
(cid:107)

−

holds for all vectors h with suﬃciently small entries. Inserting the deﬁnitions of
gi and fi we ﬁnd that

(cid:107) ≤ (cid:107)
holds for all h with suﬃciently small entries. For a non-degenerate triangle, TiD
is invertible and a simple substitution yields that

QiDh
(cid:107)

TiDh
(cid:107)

QiD(TiD)−

(cid:107)

1˜h

˜h
(cid:107)2 ≤ (cid:107)
(cid:107)

,

holds for all ˜h with suﬃciently small entries. This means that the operator norm
of Di

q induced by the (cid:96)2 norm, i.e. the S∞ norm, is bounded by one.
Let us now show the other direction. For q

1, 1

×|V| s.t.

Rd

(cid:13)
(cid:13)Di
q

(cid:13)
(cid:13)S∞ ≤

, note that inverting the above computation immediately yields that

∈

i

≤ |T |

Qkα
(cid:107)

−

Qkβ

(cid:107) ≤ (cid:107)

Tkα

Tkβ

−

(cid:107)

k

≤

, α, β

n+1. Our goal is to show that having this
holds for all 1
inequality on each simplex is suﬃcient to extend it to arbitrary pairs of simplices.
The overall idea of this part of the proof is illustrated in Fig. 9.
(cid:80)

≤ |T |

∈

∆U

Rn with αl, βl ≥

∈

0, (cid:80)

l αl ≤

l βl ≤

Let 1

i, j
Consider the line segment

≤ |T |

≤

and α, β

1 be given.

(35)

(36)

(37)

(38)

≤

(39)

(40)

c(γ) : [0, 1]

Rd

→

(cid:55)→

γ

γ Tjβ + (1

γ) Tiα.

−

Since the triangulated domain is convex, there exist 0 = a0 < a1 < . . . < ar = 1
and functions αl(γ) such that for γ
1 one can write
−
T . The continuity
c(γ) = γ Tjβ + (1
of c(γ) implies that Tkl αl(al+1) = Tkl+1αl+1(al+1), i.e. these points correspond
to both simplices, kl and kl+1. Note that this also means that Qkl αl(al+1) =

[al, al+1], 0
γ) Tiα = Tkl αl(γ) for some 1

l
r
≤
kl ≤

≤
≤

−

∈

18

E. Laude, T. M¨ollenhoﬀ, M. Moeller, J. Lellmann, D. Cremers

Qkl+1αl+1(al+1). The intuition of this construction is that the c(al+1) are located
on the boundaries of adjacent simplices on the line segment. We ﬁnd

Tiα
(cid:107)

−

Tjβ

=

(cid:107)

(al+1 −

al)
(cid:107)

Tiα

−

Tjβ

(cid:107)

r
1
(cid:88)
−

l=0

1
r
(cid:88)
−

l=0

r
1
(cid:88)
−

l=0

r
1
(cid:88)
−

l=0

1
r
(cid:88)
−

l=0

l=0

r
1
(cid:88)
−

l=0

r
1
(cid:88)
−

l=0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:107)

=

=

=

=

≥

=

=

=

(al+1 −
(cid:107)

al)(Tiα

Tjβ)

−

(cid:107)

al+1Tiα
(cid:107)

−

−

alTiα

al+1Tjβ + alTjβ

(cid:107)

alTjβ + (1
(cid:107)

−

al)Tiα

(al+1Tjβ + (1

−

al+1)Tiα)
(cid:107)

−

(41)

Tkl αl(al)
(cid:107)

−

Tkl αl(al+1)
(cid:107)

(39)

r
1
(cid:88)
−

≥

(cid:107)

Qkl αl(al)

(Qkl αl(al)

−

Qkl αl(al+1)
(cid:107)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Qkl αl(al+1))

−

−

(Qkl αl(al)

Qkl+1αl+1(al+1))

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Qk0α0(a0)

Qiα

Qjβ

−
,

(cid:107)

Qkr αr(ar)
(cid:107)

−
which yields the assertion.

(cid:107)

t1, . . . , tn+1
Proof (Proof of Proposition 4). Let ∆ = conv
be given by aﬃnely
{
Rn. We show that our lifting approach applied to
independent vertices ti
the label space ∆ solves the convexiﬁed unlifted problem, where the dataterm
(n+1) and
was replaced by its convex hull on ∆. Let the matrices T
D

n be deﬁned through

R(n+1)

Rn

∈

∈

}

×

×

∈

(cid:16)

(cid:17)

T =

t1, . . . , tn+1

, D =

, T D =

tn+1, . . . , tn

(cid:16)

t1

−

(cid:17)

,

tn+1

−

1









. . .

1 . . .

−

−







1

1

The transformation x
(cid:55)→
Now consider the following lifted function u : Ω

tn+1 + T Dx maps ∆e = conv

(42)
Rn to ∆.
Rn+1 parametrized through

0, e1, . . . , en

} ⊂

{

→

Sublabel-Accurate Convex Relaxation of Vectorial Multilabel Energies

19

˜u : Ω

∆e:

→

Consider a ﬁxed x
of the lifted dataterm ρ yields:

∈

(cid:16)

u(x) =

˜u1(x), . . . , ˜un(x), 1

(cid:80)n

(cid:17)
j=1 ˜uj(x)

.

−

(43)

Ω. Plugging this lifted representation into the biconjugate

ρ∗∗(u) = sup

u, v

Rn+1 (cid:104)

(cid:105) −

v

∈

sup
∆U

n+1

α

∈

α, v
(cid:104)

(cid:105) −

ρ(T α)

(cid:42)

= sup
v

Rn+1

∈

˜u1(x), . . . , ˜un(x), 1

˜uj(x)

 , v

n
(cid:88)

j=1

−



(cid:43)

−

= sup
v

Rn+1 (cid:104)

∈

sup
∆U

n+1

α

∈

α, v

(cid:104)

(cid:105) −

ρ(T α)

+ vn+1−

(cid:105)

sup
∆U

α

n+1

∈
˜u, D(cid:62)v

(cid:42)



ρ



n
(cid:88)

j=1



αjtj +

1

n
(cid:88)

j=1

−

α1, . . . , αn, 1

αj

 , v

n
(cid:88)

−



(cid:43)

−

j=1

 tn+1

αj





= sup
v

Rn+1 (cid:104)

∈

˜u, D(cid:62)v

+ vn+1 −

(cid:105)

sup
∆U

n+1

α

∈

vn+1 +

α, D(cid:62)v
(cid:104)

(cid:105) −

ρ(tn+1 + T Dα)

(44)

(45)

Since D(cid:62) is surjective, we can apply the substitution ˜v = D(cid:62)v:

. . . = sup

˜u, ˜v

Rn (cid:104)

˜v

∈

(cid:105) −

α, ˜v

sup
∆U

n+1

(cid:104)

(cid:105) −

= sup
˜v

Rn (cid:104)

˜u, ˜v

(cid:105) −

∈

α

∈
sup
w

∈

∆ (cid:104)

(T D)−

1(w

tn+1), ˜v

ρ(w).

−

(cid:105) −

ρ(tn+1 + T Dα)

In the last step the substitution w = tn+1 + T Dα
performed. This can be further simpliﬁed to

⇔

α = (T D)−

1(w

tn+1) was

−

. . . = sup

Rn (cid:104)

˜v

˜u, ˜v
(cid:105)

+

(T D)−
(cid:104)

1tn+1, ˜v

(ρ + δ∆)∗((T D)−

T ˜v)

˜u + (T D)−

1tn+1, ˜v

(ρ + δ∆)∗((T D)−

T ˜v)

(46)

T D˜u + tn+1, (T D)−

(ρ + δ∆)∗((T D)−

T ˜v).

(cid:105) −

(cid:105) −
T ˜v

(cid:105) −

∈
= sup
˜v

∈
= sup
˜v

∈

Rn (cid:104)

Rn (cid:104)

Since T D is invertible we can perform another substitution v(cid:48) = (T D)−

T ˜v.

. . . = sup

T D˜u + tn+1, v(cid:48)

v(cid:48)∈

Rn (cid:104)

(cid:105) −
= (ρ + δ∆)∗∗(tn+1 + T D˜u).

(ρ + δ∆)∗(v(cid:48))

(47)

20

E. Laude, T. M¨ollenhoﬀ, M. Moeller, J. Lellmann, D. Cremers

The lifted regularizer is given as:

(cid:90)

R(u) =

sup
Rd

q:Ω

→

n+1

×

Ω(cid:104)

u, Div q

Ψ ∗(q) dx

(cid:105) −

Using the parametrization by ˜u, this can be equivalently written as

˜uj Div(qj −

qn+1) + Div qn+1 dx,

(cid:90)

n
(cid:88)

Ω

j=1

sup

q(x)

∈K

K ⊂

=

q

{

K

where the set

Rd

×

n+1 can be written as

Rd

×

n+1

D(cid:62)q(cid:62)(T D)−

1

(cid:107)S∞ ≤

.
1
}

Note that since qn+1 ∈
partial integration. With the substituion ˜q(x) = D(cid:62)q(x)(cid:62) we have

| (cid:107)

∈
C∞c (Ω, Rd), the last term Div qn+1 in (49) vanishes by

(50)

(cid:90)

sup
˜
˜q
K

∈

˜u, Div ˜q
(cid:105)

Ω(cid:104)

dx,

with set ˜

Rd

n:

×

K ⊂

Note that since qi ∈
With another substituion q(cid:48)(x) = ˜q(x)(T D)−

1

n

×

q

{

∈

=

| (cid:107)

Rd

q(T D)−

˜
(cid:107)S∞ ≤
K
C∞c (Ω, Rd), the same holds for the linearly transformed ˜q.
1 we have

.
1
}

(52)

· · ·

(cid:90)

Ω(cid:104)
(cid:90)

Ω(cid:104)

= sup
q(cid:48)∈K(cid:48)
= sup
q(cid:48)∈K(cid:48)

˜u, Div q(cid:48)T D

dx

T D˜u, Div q(cid:48)

dx

(cid:105)

(cid:105)

(cid:48) =

q
{

∈

K

Rd

n

×

q

| (cid:107)

(cid:107)S∞ ≤

1

,
}

where the set

Rd

n+1 is given as

×

(cid:48)

K

⊂

which is the usual unlifted deﬁnition of the total variation T V (tn+1 + T D˜u).

This shows that the lifting method solves

(cid:90)

Ω

min
→

˜u:Ω

∆e

(ρ(x,

) + δ∆)∗∗(tn+1 + T D˜u(x))dx + λT V (tn+1 + T D˜u),
·

(55)

which is equivalent to the original problem but with a convexiﬁed data term.

(48)

(49)

(51)

(53)

(54)

Sublabel-Accurate Convex Relaxation of Vectorial Multilabel Energies

21

Input image

Mean µ

Variance σ

Fig. 10: Joint estimation of mean and variance. Our formulation can optimize
diﬃcult nonconvex joint optimization problems with continuous label spaces.

B Additional Experiment: Adaptive Denoising

In this experiment we jointly estimate the mean µ and variance σ of an image
R according to a Gaussian model. The label space is chosen as Γ =
I : Ω
[1, 10] and the dataterm as proposed in [8]:
[0, 255]

→
×

ρ(x, µ(x), σ(x)) =

+

log(2πσ(x)2).

(56)

(µ(x)

I(x))2

−
2σ(x)2

1
2

As the projection onto the epigraph of (ρ + δ∆)∗ seems diﬃcult to compute,
we approximate ρ by a piecewise linear function using 29
29 sublabels and
convexify it using the quickhull algorithm [3]. In Fig. 10 we show the result of
minimizing (56) with total variation regularization.

×

References

1. Alberti, G., Bouchitt´e, G., Maso, G.D.: The calibration method for the Mumford-
Shah functional and free-discontinuity problems. Calc. Var. Partial Dif. 3(16), 299–
333 (2003)

2. Ambrosio, L., Fusco, N., Pallara, D.: Functions of bounded variation and free
discontinuity problems. Oxford Mathematical Monographs, The Clarendon Press
Oxford University Press, New York (2000)

3. Barber, C.B., Dobkin, D.P., Huhdanpaa, H.: The quickhull algorithm for convex
hulls. ACM Transactions on Mathematical Software (TOMS) 22(4), 469–483 (1996)
4. Chambolle, A., Cremers, D., Pock, T.: A convex approach to minimal partitions.

SIAM Journal on Imaging Sciences 5(4), 1113–1158 (2012)

5. Fix, A., Agarwal, S.: Duality and the continuous graphical model. In: Com-
puter Vision ECCV 2014, Lecture Notes in Computer Science, vol. 8691, pp.
266–281. Springer International Publishing (2014), http://dx.doi.org/10.1007/
978-3-319-10578-9_18

22

E. Laude, T. M¨ollenhoﬀ, M. Moeller, J. Lellmann, D. Cremers

6. Giaquinta, M., Modica, G., Souˇcek, J.: Cartesian currents in the calculus of vari-
ations I, II., Ergebnisse der Mathematik und ihrer Grenzgebiete. 3., vol. 37-38.
Springer-Verlag, Berlin (1998)

7. Goldluecke, B., Strekalovskiy, E., Cremers, D.: The natural total variation which
arises from geometric measure theory. SIAM Journal on Imaging Sciences 5(2),
537–563 (2012)

8. Goldluecke, B., Strekalovskiy, E., Cremers, D.: Tight convex relaxations for vector-

valued labeling. SIAM Journal on Imaging Sciences 6(3), 1626–1664 (2013)

9. Ishikawa, H.: Exact optimization for Markov random ﬁelds with convex priors.
IEEE Trans. Pattern Analysis and Machine Intelligence 25(10), 1333–1336 (2003)
10. Lellmann, J., Schn¨orr, C.: Continuous multiclass labeling approaches and algo-

rithms. SIAM Journal on Imaging Sciences 4(4), 1049–1096 (2011)

11. Lellmann, J., Strekalovskiy, E., Koetter, S., Cremers, D.: Total variation regular-

ization for functions with values in a manifold. In: ICCV (December 2013)

12. M¨ollenhoﬀ, T., Laude, E., Moeller, M., Lellmann, J., Cremers, D.: Sublabel-

accurate relaxation of nonconvex energies. In: CVPR (2016)

13. Nocedal, J., Wright, S.J.: Numerical Optimization. Springer, New York, 2nd edn.

(2006)

14. Pock, T., Cremers, D., Bischof, H., Chambolle, A.: An algorithm for minimizing

the piecewise smooth Mumford-Shah functional. In: ICCV (2009)

15. Pock, T., Schoenemann, T., Graber, G., Bischof, H., Cremers, D.: A convex formu-
lation of continuous multi-label problems. In: European Conference on Computer
Vision (ECCV). Marseille, France (October 2008)

16. Pock, T., Cremers, D., Bischof, H., Chambolle, A.: Global solutions of variational
models with convex regularization. SIAM J. Imaging Sci. 3(4), 1122–1145 (2010)

17. Rockafellar, R., Wets, R.B.: Variational Analysis. Springer (1998)
18. Rudin, L.I., Osher, S., Fatemi, E.: Nonlinear total variation based noise removal

algorithms. Physica D: Nonlinear Phenomena 60(1), 259–268 (1992)

19. Sapiro, G., Ringach, D.: Anisotropic diﬀusion of multivalued images with applica-

tions to color ﬁltering. IEEE Trans. Img. Proc. 5(11), 1582–1586 (1996)

20. Strekalovskiy, E., Chambolle, A., Cremers, D.: Convex relaxation of vectorial prob-
lems with coupled regularization. SIAM Journal on Imaging Sciences 7(1), 294–336
(2014)

21. Zach, C.: Dual decomposition for joint discrete-continuous optimization. In: AIS-

TATS. pp. 632–640 (2013)

22. Zach, C., Kohli, P.: A convex discrete-continuous approach for markov random
ﬁelds. In: ECCV, vol. 7577, pp. 386–399. Springer Berlin Heidelberg (2012)

Sublabel-Accurate Convex Relaxation of

Vectorial Multilabel Energies

Emanuel Laude(cid:63)1, Thomas M¨ollenhoﬀ(cid:63)1, Michael Moeller1,
Jan Lellmann2, and Daniel Cremers1

1Technical University of Munich(cid:63)(cid:63)

2University of L¨ubeck

Abstract. Convex relaxations of multilabel problems have been demon-
strated to produce provably optimal or near-optimal solutions to a va-
riety of computer vision problems. Yet, they are of limited practical use
as they require a ﬁne discretization of the label space, entailing a huge
demand in memory and runtime. In this work, we propose the ﬁrst sub-
label accurate convex relaxation for vectorial multilabel problems. Our
key idea is to approximate the dataterm in a piecewise convex (rather
than piecewise linear) manner. As a result we have a more faithful ap-
proximation of the original cost function that provides a meaningful in-
terpretation for fractional solutions of the relaxed convex problem.

Keywords: Convex Relaxation, Optimization, Variational Methods

6
1
0
2
 
t
c
O
 
0
1
 
 
]

V
C
.
s
c
[
 
 
2
v
0
8
9
1
0
.
4
0
6
1
:
v
i
X
r
a

(a) Original dataterm

(b) Without lifting

(c) Classical lifting

(d) Proposed lifting

Fig. 1: In (a) we show a nonconvex dataterm. Convexiﬁcation without lifting
would result in the energy (b). Classical lifting methods [11] (c), approximate
the energy piecewise linearly between the labels, whereas the proposed method
results in an approximation that is convex on each triangle (d). Therefore, we
are able to capture the structure of the nonconvex energy much more accurately.

(cid:63) These authors contributed equally.
(cid:63)(cid:63) This work was supported by the ERC Starting Grant “Convex Vision”.

2

E. Laude, T. M¨ollenhoﬀ, M. Moeller, J. Lellmann, D. Cremers

1 Introduction

1.1 Nonconvex Vectorial Problems

In this paper, we derive a sublabel-accurate convex relaxation for vectorial op-
timization problems of the form

(cid:90)

Ω

min
Γ
→

u:Ω

ρ(cid:0)x, u(x)(cid:1) dx + λ T V (u),

R denotes a generally nonconvex
where Ω
Γ
pointwise dataterm. As regularization we focus on the total variation deﬁned as:

Rn and ρ : Ω

Rd, Γ

→

⊂

⊂

×

T V (u) =

C∞c (Ω,Rn

q

∈

q(x)

(cid:107)S∞ ≤

1

(cid:107)

sup
d),
×

(cid:90)

Ω(cid:104)

u, Div q

dx,

(cid:105)

where
For diﬀerentiable functions u we can integrate (2) by parts to ﬁnd

(cid:107) · (cid:107)S∞ is the Schatten-

∞

×

norm on Rn

d, i.e., the largest singular value.

(1)

(2)

(3)

(cid:90)

T V (u) =

u(x)

(cid:107)S1 dx,

Ω (cid:107)∇

(cid:107) · (cid:107)S1 penalizes the sum of the singular values of the
where the dual norm
Jacobian, which encourages the individual components of u to jump in the same
direction. This type of regularization is part of the framework of Sapiro and
Ringach [19].

1.2 Related Work

×

Due to its nonconvexity the optimization of (1) is challenging. For the scalar case
(n = 1), Ishikawa [9] proposed a pioneering technique to obtain globally optimal
solutions in a spatially discrete setting, given by the minimum s-t-cut of a graph
Γ . A continuous formulation was introduced by Pock
representing the space Ω
et al. [15] exhibiting several advantages such as less grid bias and parallelizability.
In a series of papers [16,14], connections of the above approaches were made
to the mathematical theory of cartesian currents [6] and the calibration method
for the Mumford-Shah functional [1], leading to a generalization of the convex
relaxation framework [15] to more general (in particular nonconvex) regularizers.
In the following, researchers have strived to generalize the concept of func-
tional lifting and convex relaxation to the vectorial setting (n > 1). If the
dataterm and the regularizer are both separable in the label dimension, one can
simply apply the above convex relaxation approach in a channel-wise manner

Sublabel-Accurate Convex Relaxation of Vectorial Multilabel Energies

3

to each component separately. But when either the dataterm or the regularizer
couple the label components, the situation becomes more complex [8,20].

The approach which is most closely related to our work, and which we con-
sider as a baseline method, is the one by Lellmann et al. [11]. They consider
coupled dataterms with coupled total variation regularization of the form (2).

A drawback shared by all mentioned papers is that ultimately one has to
discretize the label space. While Lellmann et al. [11] propose a sublabel-accurate
regularizer, we show that their dataterm leads to solutions which still have a
strong bias towards the label grid. For the scalar-valued setting, continuous label
spaces have been considered in the MRF community by Zach et al. [22] and Fix
et al. [5]. The paper [21] proposes a method for mixed continuous and discrete
vectorial label spaces, where everything is derived in the spatially discrete MRF
setting. M¨ollenhoﬀ et al. [12] recently proposed a novel formulation of the scalar-
valued case which retains fully continuous label spaces even after discretization.
The contribution of this work is to extend [12] to vectorial label spaces, thereby
complementing [11] with a sublabel-accurate dataterm.

1.3 Contribution

In this work we propose the ﬁrst sublabel-accurate convex formulation of vecto-
rial labeling problems. It generalizes the formulation for scalar-valued labeling
problems [12] and thus includes important applications such as optical ﬂow esti-
mation or color image denoising. We show that our method, derived in a spatially
continuous setting, has a variety of interesting theoretical properties as well as
practical advantages over the existing labeling approaches:

– We generalize existing functional lifting approaches (see Sec. 2.2).
– We show that our method is the best convex under-approximation (in a local

sense), see Prop. 1 and Prop. 2.

– Due to its sublabel-accuracy our method requires only a small amount of
labels to produce good results which leads to a drastic reduction in memory.
We believe that this is a vital step towards the real-time capability of lifting
and convex relaxation methods. Moreover, our method eliminates the label
bias, that previous lifting methods suﬀer from, even for many labels.

– In Sec. 2.3 we propose a regularizer that couples the diﬀerent label compo-
nents by enforcing a joint jump normal. This is in contrast to [8], where the
components are regularized separately.

– For convex dataterms, our method is equivalent to the unlifted problem –
see Prop. 4. Therefore, it allows a seamless transition between direct opti-
mization and convex relaxation approaches.

4

E. Laude, T. M¨ollenhoﬀ, M. Moeller, J. Lellmann, D. Cremers

1.4 Notation

x, y

= (cid:80)
We write
product if x, y are matrices. Similarly
usual Euclidean norm, respectively the Frobenius norm for matrices.

i xiyi for the standard inner product on Rn or the Frobenius
without any subscript denotes the

(cid:107) · (cid:107)

(cid:104)

(cid:105)

∈

Rn

y, x
(cid:104)

We denote the convex conjugate of a function f : Rn

by f ∗(y) =
supx
f (x). It is an important tool for devising convex relaxations,
as the biconjugate f ∗∗ is the largest lower-semicontinuous (lsc.) convex function
below f . For the indicator function of a set C we write δC, i.e., δC(x) = 0 if
x

Rn stands for the unit n-simplex.

otherwise. ∆U

∪ {∞}

C and

(cid:105) −

→

R

∈

∞

n ⊂

2 Convex Formulation

2.1 Lifted Representation

Motivated by Fig. 1, we construct an equivalent representation of (1) in a higher
dimensional space, before taking the convex envelope.

⊂

Let Γ

Rn be a compact and convex set. We partition Γ into a set

of
n-simplices ∆i so that Γ is a disjoint union of ∆i up to a set of measure zero.
Let tij be the j-th vertex of ∆i and denote by
the union of all
V
.
ij ≤ |V|
, 1
vertices, referred to as labels, with 1
Γ , we refer to u(x) as a sublabel. Any sublabel can be written
For u : Ω
as a convex combination of the vertices of a simplex ∆i with 1
for
appropriate barycentric coordinates α

t1, . . . , t|V|
{
j
≤

}
n + 1 and 1

≤ |T |

≤ |T |

→

≤

≤

≤

=

≤

T

i

i

∆U
n :

∈

u(x) = Tiα :=

αjtij , Ti := (ti1 , ti2, . . . , tin+1)

Rn

n+1.

×

(4)

By encoding the vertices tk
identify any u(x)
zeros and vice versa:

∈

using a one-of-
Γ with a sparse vector u(x) containing at least

representation ek we can
n many

∈ V

|V|

|V| −

u(x) = Eiα :=

αjeij , Ei := (ei1, ei2, . . . , ein+1)

R|V|×

n+1,

∈

∈

n+1
(cid:88)

j=1

n+1
(cid:88)

j=1

(5)

u(x) =

|V|(cid:88)

k=1

tkuk(x), α

∆U

n , 1

i

∈

≤

≤ |T |

.

The entries of the vector eij are zero except for the (ij)-th entry, which is equal
R|V| as the lifted representation of u. This one-
to one. We refer to u : Ω
to-one-correspondence between u(x) = Tiα and u(x) = Eiα is shown in Fig. 2.
Note that both, α and i depend on x. However, for notational convenience we
drop the dependence on x whenever we consider a ﬁxed point x

Ω.

→

∈

Sublabel-Accurate Convex Relaxation of Vectorial Multilabel Energies

5

Fig. 2: This ﬁgure illustrates our notation and the one-to-one correspondence
between u(x) = (0.3, 0.2)(cid:62) and the lifted u(x) containing the barycentric co-
ordinates α = (0.7, 0.1, 0.2)(cid:62) of the sublabel u(x)
. The
∆4 = conv
}
triangulation (
[0; 1] is visualized via the gray lines, cor-
) of Γ = [
responding to the triangles and the gray dots, corresponding to the vertices

t2, t3, t6
{

1; 1]

−

×

∈

V

T

,

=

(
−

{

V

1, 0)(cid:62), (0, 0)(cid:62), . . . , (1, 1)(cid:62)

, that we refer to as the labels.

}

2.2 Convexifying the Dataterm

Let for now the weight of the regularizer in (1) be zero. Then, at each point
x

Ω we minimize a generally nonconvex energy over a compact set Γ

Rn:

∈

⊂

(6)

ρ(u).

min
Γ
u
∈

We set up the lifted energy so that it attains ﬁnite values if and only if the
argument u is a sparse representation u = Eiα of a sublabel u

Γ :

ρ(u) = min

ρi(u),

ρi(u) =

1

i
≤|T |

≤

∈






,

∞

ρ(Tiα),

if u = Eiα, α

∆U
n ,

∈

(7)

otherwise.

Problems (6) and (7) are equivalent due to the one-to-one correspondence of
u = Tiα and u = Eiα. However, energy (7) is ﬁnite on a nonconvex set only. In
order to make optimization tractable, we minimize its convex envelope.

Proposition 1 The convex envelope of (7) is given as:

ρ∗∗(u) = sup
R

u, v

ρ∗i (v) =

|V|(cid:104)
∈
Eibi, v

v

(cid:104)

(cid:105)

ρ∗i (v),

1

≤|T |

(cid:105) −

max
i
≤
+ ρ∗i (A(cid:62)i E(cid:62)i v),
, Ai := (cid:0)M 1

ρi := ρ + δ∆i.

(8)

(cid:1), where M j

i are

bi and Ai are given as bi := M n+1
the columns of the matrix Mi := (T (cid:62)i , 1)−(cid:62)

i

i , M 2
i , . . . , M n
i
Rn+1
n+1.
×

∈

Proof. Follows from a calculation starting at the deﬁnition of ρ∗∗. See Ap-
pendix A for a detailed derivation.

6

E. Laude, T. M¨ollenhoﬀ, M. Moeller, J. Lellmann, D. Cremers

Standard lifting [11]

Proposed lifting

t1, t2, t3
{

Fig. 3: Geometrical intuition for the proposed lifting and standard lifting [11]
for the special case of 1-dimensional range Γ = [a, b] and 3 labels
.
}
The standard lifting correponds to a linear interpolation of the original cost in
between the locations t1, t2, t3, which are associated with the vertices e1, e2, e3
in the lifted energy (lower left). The proposed method extends the cost to the
relaxed set in a more precise way: The original cost is preserved on the connect-
ing lines between adjacent ei (black lines on the bottom right) up to concave
parts (red graphs and lower surface on the right). This information, which may
inﬂuence the exact location of the minimizer, is lost in the standard formula-
tion. If the solution of the lifted formulation u is in the interior (gray area) an
approximate solution to the original problem can still be obtained via Eq. (5).

n , i.e., ρ(u) = ρ(tk) if u = ek and +

The geometric intuition of this construction is depicted in Fig. 3. Note that if
one prescribes the value of ρi in (7) only on the vertices of the unit simplices
∆U
otherwise, one obtains the linear
, s = (ρ(ti), . . . , ρ(tL)) on the feasible set. This
biconjugate ρ∗∗(u) =
u, s
(cid:105)
(cid:104)
coincides with the standard relaxation of the dataterm used in [16,10,4,11]. In
that sense, our approach can be seen as a relaxing the dataterm in a more precise
way, by incorporating the true value of ρ not only on the ﬁnite set of labels
,
but also everywhere in between, i.e., on every sublabel.

∞

V

Sublabel-Accurate Convex Relaxation of Vectorial Multilabel Energies

7

2.3 Lifting the Vectorial Total Variation

We deﬁne the lifted vectorial total variation as

T V (u) =

Ψ (Du),

(9)

(cid:90)

Ω

where Du denotes the distributional derivative of u and Ψ is positively one-
homogeneous, i.e., Ψ (cu) = c Ψ (u), c (cid:62) 0. For such functions, the meaning of (9)
can be made fully precise using the polar decomposition of the Radon measure
Du [2, Cor. 1.29, Thm. 2.38]. However, in the following we restrict ourselves to
an intuitive motivation for the derivation of Ψ for smooth functions.

→

Our goal is to ﬁnd Ψ so that T V (u) = T V (u) whenever u : Ω

R|V|
corresponds to some u : Ω
Γ , in the sense that u(x) = Eiα whenever u(x) =
Tiα. In order for the equality to hold, it must in particular hold for all u that are
classically diﬀerentiable, i.e., Du =
u(x) is of rank
Rd. This rank 1 constraint
1, i.e.,
ν(x) for some ν(x)
enforces the diﬀerent components of u to have the same jump normal, which is
desirable in many applications. In that case, we observe

u, and whose Jacobian

u(x) = (Tiα

Tjβ)

→

∇

∇

∇

−

⊗

∈

(cid:90)

T V (u) =

Tiα

Tjβ

ν(x)
(cid:107)

(cid:107) · (cid:107)

−

dx.

Ω (cid:107)

(10)

For the corresponding lifted representation u, we have
ν(x). Therefore it is natural to require Ψ (
Tiα
(cid:107)
by these observations, we deﬁne

Ejβ)
⊗
ν(x)) :=
in order to achieve the goal T V (u) = T V (u). Motivated

u(x) = (Eiα
Ejβ)

u(x)) = Ψ ((Eiα

−
⊗

ν(x)

(cid:107) · (cid:107)

Tjβ

∇

∇

−

−

(cid:107)

Ψ (p) :=

Tiα
(cid:107)

−

Tjβ

ν

(cid:107) · (cid:107)

(cid:107)

if p = (Eiα

Ejβ)

ν,

−

⊗

(11)

otherwise,






∞

where α, β
≤
is intractable, we derive a “locally” tight convex underapproximation:

. Since the convex envelope of (9)

n+1, ν

≤ |T |

i, j

∈

∈

∆U

Rd and 1

R(u) =

sup
Rd

→

q:Ω

×|V|

(cid:90)

Ω(cid:104)

u, Div q

Ψ ∗(q) dx.

(cid:105) −

Proposition 2 The convex conjugate of Ψ is

Ψ ∗(q) = δ

(q)

K

with convex set

=

K

(cid:92)

(cid:110)

q

∈

Rd

×|V|

(cid:12)
(cid:12)

1

i,j

≤

≤|T |

Qiα
(cid:107)

−

Qjβ

(cid:107) ≤ (cid:107)

Tiα

Tjβ

, α, β
(cid:107)

∈

−

(cid:111)

∆U

n+1

, (14)

and Qi = (qi1, qi2, . . . , qin+1 )

Rd

n+1. qj

×

Rd are the columns of q.

∈

∈

(12)

(13)

8

E. Laude, T. M¨ollenhoﬀ, M. Moeller, J. Lellmann, D. Cremers

Proof. Follows from a calculation starting at the deﬁnition of the convex conju-
gate Ψ ∗. See Appendix A.

Interestingly, although in its original formulation (14) the set
many constraints, one can equivalently represent

by ﬁnitely many.

K

has inﬁnitely

K

Proposition 3 The set

in equation (14) is the same as

K

∈

(cid:110)

q

=

K

∈

Rd

×|V|

(cid:13)
(cid:13)Di
q

(cid:13)
(cid:13)S∞ ≤

|

1, 1

i

≤

≤ |T |

(cid:111)

, Di

q = QiD (TiD)−

1,

(15)

where the matrices QiD

Rd

n and TiD

×

Rn

×

n are given as

QiD := (cid:0)qi1

qin+1, . . . , qin

−

−

tin+1, . . . , tin

−

tin+1(cid:1) .

−

∈
qin+1(cid:1) , TiD := (cid:0)ti1

Proof. Similar to the analysis in [11], equation (14) basically states the Lipschitz
×|V|.
continuity of a piecewise linear function deﬁned by the matrices q
Therefore, one can expect that the Lipschitz constraint is equivalent to a bound
on the derivative. For the complete proof, see Appendix A.

Rd

∈

2.4 Lifting the Overall Optimization Problem

Combining dataterm and regularizer, the overall optimization problem is given

(cid:90)

min
R
→

u:Ω

sup

ρ∗∗(u) +

u, Div q

dx.

(cid:104)

(cid:105)

|V|

q:Ω

Ω

→K

(16)

A highly desirable property is that, opposed to any other vectorial lifting ap-
proach from the literature, our method with just one simplex applied to a convex
problem yields the same solution as the unlifted problem.

Proposition 4 If the triangulation contains only 1 simplex,

, i.e.,
∆
}
{
= n + 1, then the proposed optimization problem (16) is equivalent to

=

T

|V|

(cid:90)

Ω

min
→

u:Ω

∆

(ρ + δ∆)∗∗(x, u(x)) dx + λT V (u),

(17)

which is (1) with a globally convexiﬁed dataterm on ∆.

Proof. For u = tn+1 + T D˜u the substitution u =
ρ∗∗ and R yields the result. For a complete proof, see Appendix A.

˜u1, . . . , ˜un, 1

−

(cid:16)

(cid:17)

(cid:80)n

j=1 ˜uj

into

Sublabel-Accurate Convex Relaxation of Vectorial Multilabel Energies

9

3 Numerical Optimization

3.1 Discretization

For now assume that Ω
denote a ﬁnite-diﬀerence divergence operator with Div q : Ω
relaxed energy minimization problem becomes

Rd is a d-dimensional Cartesian grid and let Div
R|V|. Then the

→

⊂

min
R
→

u:Ω

|V|

max
q:Ω
→K

(cid:88)

x

Ω

∈

ρ∗∗(x, u(x)) +

Div q, u
(cid:104)

.
(cid:105)

(18)

In order to get rid of the pointwise maximum over ρ∗i (v) in Eq. (8), we introduce
additional variables w(x)
Ω
so that w(x) attains the value of the pointwise maximum:

R and additional constraints (v(x), w(x))

∈ C

, x

∈

∈

min
R
→

u:Ω

|V|

max
(v,w):Ω
q:Ω

(cid:88)

(cid:104)

→C

x

Ω

∈

→K

u(x), v(x)

w(x) +

(cid:105) −

Div q, u
(cid:104)

,
(cid:105)

(19)

where the set

is given as

C

=

C

1

i
≤

≤|T |

(cid:92)

Ci,

Ci :=

(cid:110)

(x, y)

R|V|

+1

∈

ρ∗i (x)

|

≤

(cid:111)

y

.

(20)

For numerical optimization we use a GPU-based implementation1 of a ﬁrst-order
primal-dual method [14]. The algorithm requires the orthogonal projections of
the dual variables onto the sets
in every iteration. However, the
respectively
K
+ 1 is diﬃcult for large values of
projection onto an epigraph of dimension
|V|
Ω as (n + 1)-
, x
∈ Ci, 1

|V|
≤ |T |
dimensional epigraph constraints introducing variables ri(x)

. We rewrite the constraints (v(x), w(x))

∈
Rn, si(x)

R:

≤

C

i

(cid:0)ri(x)(cid:1)

ρ∗i

≤

si(x),

ri(x) = A(cid:62)i E(cid:62)i v(x),

si(x) = w(x)

Eibi, v(x)

(21)

∈

− (cid:104)

∈

.
(cid:105)

These equality constraints can be implemented using Lagrange multipliers. For
the projection onto the set

we use an approach similar to [7, Figure 7].

K

3.2 Epigraphical Projections

is a central part
Computing the Euclidean projection onto the epigraph of ρ∗i
of the numerical implementation of the presented method. However, for n > 1
this is nontrivial. Therefore we provide a detailed explanation of the projection
methods used for diﬀerent classes of ρi. We will consider quadratic, truncated
quadratic and piecewise linear ρ.

1 https://github.com/tum-vision/sublabel_relax

10

E. Laude, T. M¨ollenhoﬀ, M. Moeller, J. Lellmann, D. Cremers

Quadratic case: Let ρ be of the form ρ(u) = a
2 u(cid:62)u + b(cid:62)u + c. A direct projection
onto the epigraph of ρ∗i = (ρ + δ∆i)∗ for n > 1 is diﬃcult. However, the epigraph
can be decomposed into separate epigraphs for which it is easier to project onto:
For proper, convex, lsc. functions f, g the epigraph of (f + g)∗ is the Minkowski
sum of the epigraphs of f ∗ and g∗ (cf. [17, Exercise 1.28, Theorem 11.23a]).
This means that it suﬃces to compute the projections onto the epigraphs of
a quadratic function f ∗ = ρ∗ and a convex, piecewise linear function g∗(v) =
max1

by rewriting constraint (21) as

tij , v

j
≤

≤

n+1(cid:104)

ρ∗(rf )

sf , δ∆i ∗(cg)

dg s.t. (r, s) = (rf , sf ) + (cg, dg).

(22)

≤

For the projection onto the epigraph of a n-dimensional quadratic function we
use the method described in [20, Appendix B.2]. The projection onto a piecewise
linear function is described in the last paragraph of this section.

(cid:105)

≤

ν, a

{

2 u(cid:62)u + b(cid:62)u + c

Truncated quadratic case: Let ρ be of the form ρ(u) = min
}
as it is the case for the nonconvex robust ROF with a truncated quadratic
dataterm in Sec. 4.2. Again, a direct projection onto the epigraph of ρ∗i is diﬃcult.
However, a decomposition of the epigraph into simpler epigraphs is possible
as the epigraph of min
∗ is the intersection of the epigraphs of f ∗ and
g∗. Hence, one can separately project onto the epigraphs of (ν + δ∆i)∗ and
( a
2 u(cid:62)u + b(cid:62)u + c + δ∆i)∗. Both of these projections can be handled using the
methods from the other paragraphs.

f, g
{

}

Piecewise linear case: In case ρ is piecewise linear on each ∆i, i.e., ρ attains
∆i and interpolates
ﬁnite values at a discrete set of sampled sublabels
linearly between them, we have that

Vi ⊂

(ρ + δ∆i)∗(v) = max
∈V

τ

i (cid:104)

τ, v

(cid:105) −

ρ(τ ).

(23)

Again this is a convex, piecewise linear function. For the projection onto the
epigraph of such a function, a quadratic program of the form

(x,y)

Rn+1

min
∈

1
2 (cid:107)

x

c

2 +
(cid:107)

−

1
2 (cid:107)

y

d
(cid:107)

−

2 s.t.

τ, x
(cid:104)

(cid:105) −

ρ(τ )

y,

τ
∀

≤

∈ Vi

(24)

needs to be solved. We implemented the primal active-set method described
in [13, Algorithm 16.3], and found it solves the program in a few (usually 2
10)
iterations for a moderate number of constraints.

−

Sublabel-Accurate Convex Relaxation of Vectorial Multilabel Energies

11

Naive, 81 labels.

[11], 81 labels.

Ours, 4 labels.

→

[
−

1, 1]2, discretized
Fig. 4: ROF denoising of a vector-valued signal f : [0, 1]
on 50 points (shown in red). We compare the proposed approach (right) with
two alternative techniques introduced in [11] (left and middle). The labels are
visualized by the gray grid. While the naive (standard) multilabel approach from
[11] (left) provides solutions that are constrained to the chosen set of labels, the
sublabel accurate regularizer from [11] (middle) does allow sublabel solutions,
yet – due to the dataterm bias – these still exhibit a strong preference for the grid
points. In contrast, the proposed approach does not exhibit any visible grid bias
providing fully sublabel-accurate solutions: With only 4 labels, the computed
solutions (shown in blue) coincide with the “unlifted” problem (green).

4 Experiments

4.1 Vectorial ROF Denoising

In order to validate experimentally, that our model is exact for convex dataterms,
we evaluate it on the Rudin-Osher-Fatemi [18] (ROF) model with vectorial
TV (2). In our model this corresponds to deﬁning ρ(x, u(x)) = 1
2.
As expected based on Prop. 4 the energy of the solution of the unlifted problem
is equal to the energy of the projected solution of our method for
= 4 up to
machine precision, as can be seen in Fig. 4 and Fig. 5. We point out, that the
sole purpose of this experiment is a proof of concept as our method introduces
an overhead and convex problems can be solved via direct optimization. It can
be seen in Fig. 4 and Fig. 5, that the baseline method [11] has a strong label
bias.

I(x)
(cid:107)

u(x)

|V|

2 (cid:107)

−

4.2 Denoising with Truncated Quadratic Dataterm

For images degraded with both, Gaussian and salt-and-pepper noise we deﬁne
2, ν(cid:9). We solve the problem
the dataterm as ρ(x, u(x)) = min (cid:8) 1
I(x)
2 (cid:107)
(cid:107)

u(x)

−

12

E. Laude, T. M¨ollenhoﬀ, M. Moeller, J. Lellmann, D. Cremers

Input image

Unlifted Problem,

Ours,

= 1,

E = 992.50

|T |
= 4,

|V|

E = 992.51

Ours,
= 2

= 6
|T |
2
2
×

|V|

×
E = 993.52

Baseline,
= 4

4

4,

|V|

×
×
E = 2255.81

Fig. 5: Convex ROF with vectorial TV. Direct optimization and proposed method
yield the same result. In contrast to the baseline method [11] the proposed ap-
proach has no discretization artefacts and yields a lower energy. The regulariza-
tion parameter is chosen as λ = 0.3.

Noisy input

Ours,

= 1,

|T |
= 4,

|V|

E = 2849.52

Ours,
= 2

= 6,
|T |
2,
2
×
×
E = 2806.18

|V|

Ours,

= 48,
3,
3

= 3

|T |
×
×
E = 2633.83

|V|

Baseline,
= 4

4

4,

|V|

×
×
E = 3151.80

Fig. 6: ROF with a truncated quadratic dataterm (λ = 0.03 and ν = 0.025).
Compared to the baseline method [11] the proposed approach yields much better
results, already with a very small number of 4 labels.

using the epigraph decomposition described in the second paragraph of Sec. 3.2.
leads to lower energies and
It can be seen, that increasing the number of labels
at the same time to a reduced eﬀect of the TV. This occurs as we always compute
a piecewise convex underapproximation of the original nonconvex dataterm, that
gets tighter with a growing number of labels. The baseline method [11] again
produces strong discretization artefacts even for a large number of labels
=
4

4 = 64.

|V|

|V|

4

×

×

Sublabel-Accurate Convex Relaxation of Vectorial Multilabel Energies

13

Image 1

|V|

= 5

5,
[8],
0.67 GB, 4 min
aep = 2.78

×

11,

[8],

|V|

= 11
2.1 GB, 12 min
aep = 1.97

×

17,

[8],

|V|

= 17
4.1 GB, 25 min
aep = 1.63

×

28,

[8],

|V|

= 28
9.3 GB, 60 min
aep = 1.39

×

Image 2

|V|

= 3

[11],
3,
0.67 GB, 0.35 min
aep = 5.44

×

|V|

= 5

[11],
5,
2.4 GB, 16 min
aep = 4.22

×

|V|

= 7

[11],
7,
5.2 GB, 33 min
aep = 2.65

×

[11],
9,
Out of memory.

= 9

|V|

×

Ground truth

|V|

= 2

Ours,
2,
0.63 GB, 17 min
aep = 1.28

×

Ours,

= 3

3,

Ours,

= 4

4,

|V|

×
1.9 GB, 34 min
aep = 1.07

|V|

×
4.1 GB, 41 min
aep = 0.97

|V|

= 6

Ours,
6,
10.1 GB, 56 min
aep = 0.9

×

Fig. 7: We compute the optical ﬂow using our method, the product space ap-
proach [8] and the baseline method [11] for a varying amount of labels and
compare the average endpoint error (aep). The product space method clearly
outperforms the baseline, but our approach ﬁnds the overall best result already
with 2
2 labels. To achieve a similarly precise result as the product space
method, we require 150 times fewer labels, 10 times less memory and 3 times
less time. For the same number of labels, the proposed approach requires more
memory as it has to store a convex approximation of the energy instead of a
linear one.

×

4.3 Optical Flow

We compute the optical ﬂow v : Ω
The label space Γ = [
displacement d
I1(x + v(x))

−

∈

R2 between two input images I1, I2.
d, d]2 is chosen according to the estimated maximum

→

R between the images. The dataterm is ρ(x, v(x)) =

, and λ(x) is based on the norm of the image gradient
(cid:107)
In Fig. 7 we compare the proposed method to the product space approach
[8]. Note that we implemented the product space dataterm using Lagrange mul-

∇

I2(x)
(cid:107)
I1(x).

−

14

E. Laude, T. M¨ollenhoﬀ, M. Moeller, J. Lellmann, D. Cremers

(a) Image 1 and 2

(b) Proposed,

= 2

2

×

|V|

(c) Baseline,

= 7

7

×

|V|

81
Fig. 8: Large displacement ﬂow between two 640
search window. The result of our method with 4 labels is shown in (b), the
baseline [11] in (c). Our method can correctly identify the large motion.

480 images (a) using a 81

×

×

×

−

tipliers, also referred to as the global approach in [8]. While this increases the
memory consumption, it comes with lower computation time and guaranteed
15, 15]2 on
convergence. For our method, we sample the label space Γ = [
150
150 sublabels and subsequently convexify the energy on each triangle us-
ing the quickhull algorithm [3]. For the product space approach we sample the
label space at equidistant labels, from 5
27. As the regularizer from the
×
product space approach is diﬀerent from the proposed one, we chose µ diﬀerently
for each method. For the proposed method, we set µ = 0.5 and for the product
space and baseline approach µ = 3. We can see in Fig. 7, our method outperforms
the product space approach w.r.t. the average end-point error. Our method out-
performs previous lifting approaches: In Fig. 8 we compare our method on large
displacement optical ﬂow to the baseline [11]. To obtain competitive results on
the Middlebury benchmark, one would need to engineer a better dataterm.

5 to 27

×

5 Conclusions

We proposed the ﬁrst sublabel-accurate convex relaxation of vectorial multil-
abel problems. To this end, we approximate the generally nonconvex dataterm
in a piecewise convex manner as opposed to the piecewise linear approxima-
tion done in the traditional functional lifting approaches. This assures a more
faithful approximation of the original cost function and provides a meaningful
interpretation for the non-integral solutions of the relaxed convex problem. In
experimental validations on large-displacement optical ﬂow estimation and color
image denoising, we show that the computed solutions have superior quality
to the traditional convex relaxation methods while requiring substantially less
memory and runtime.

Sublabel-Accurate Convex Relaxation of Vectorial Multilabel Energies

15

A Theory

Proof (Proof of Proposition 1). By deﬁnition the biconjugate of ρ is given as

We proceed computing the conjugate of ρi:

ρ∗∗(u) = sup
R

v

|V|(cid:104)

u, v

(cid:105) −

∈
= sup
R
v

|V|(cid:104)

∈

u, v

(cid:105) −

1

(cid:18)

≤|T |

1

min
i
≤
max
i
≤

≤|T |

(cid:19)∗

ρi(v)

ρ∗i (v).

ρ∗i (v) = sup
|V|(cid:104)
R

u

u, v

(cid:105) −

ρi(u)

∈
= sup
∆U
α

n+1

∈

Eiα, v
(cid:104)

(cid:105) −

ρ (Tiα) ,

We introduce the substitution r := Tiα

α = K −
i

1

, Ki :=

(cid:32)

(cid:33)

r
1

∈

∆i and obtain
(cid:32)

(cid:33)

Ti
1(cid:62)

∈

Rn+1

n+1,

×

since Ki is invertible for (
,
V
1. With this we can further rewrite the conjugate as

T

) being a non-degenerate triangulation and (cid:80)n+1

j=1 αj =

. . . = sup

Air + bi, E(cid:62)i v

ρ(r)

(cid:105) −

∆i(cid:104)

r

∈
Eibi, v
(cid:104)

=

+ sup
r

Rn(cid:104)

(cid:105)

∈

r, A(cid:62)i E(cid:62)i v

ρ(r)

δ∆i(r)

(cid:105) −

−

=

Eibi, v
(cid:104)

(cid:105)

+ ρ∗i (A(cid:62)i E(cid:62)i v).

Proof (Proof of Proposition 2). Deﬁne Ψ i,j as

Ψ i,j(p) :=

Tiα

Tjβ

−

ν

(cid:107) · (cid:107)

(cid:107)

if p = (Eiα

Ejβ)ν(cid:62), α, β

−

∆U

n+1, ν

∈

Rd,

∈

otherwise.



(cid:107)


∞

Then, Ψ can be rewritten as a pointwise minimum over the individual Ψ i,j

Ψ (p) = min

Ψ i,j(p).

1

i,j

≤

≤|T |

We begin computing the conjugate of Ψ i,j

Ψ ∗i,j(q) = sup
Rd

p

×|V|(cid:104)

p, q

(cid:105) −

Ψ i,j(p)

sup
ν

Rd(cid:104)

∈
Tiα
(
(cid:107)

∈
= sup
∆U
α,β

n+1

∈

= sup
∆U
α,β

n+1

∈
i,j (q),

= δ

K

Qiα

Qjβ, ν

−

Tiα

Tjβ

(cid:105) − (cid:107)

−

ν

(cid:107) · (cid:107)

(cid:107)

Tjβ

−

(cid:107) · (cid:107) · (cid:107)

)∗ (Qiα

Qjβ)

−

(25)

(26)

(27)

(28)

(29)

(30)

(31)

16

E. Laude, T. M¨ollenhoﬀ, M. Moeller, J. Lellmann, D. Cremers

Fig. 9: Figure illustrating the second direction of the proof of Proposition 4. The
). The line segment between
gray dots and lines visualize the triangulation (
Tiα and Tjβ is composed of shorter line segments which are fully contained in
one of the triangles. On each of the triangles the inequality (39) holds, which
allows to conclude that it holds for the whole line segment.

V

T

,

with the set Ki,j being deﬁned as

Ki,j :=

(cid:110)

q

∈

Rd

×|V|

(cid:12)
(cid:12)

(cid:107)

Qiα

Qjβ

−

(cid:107) ≤ (cid:107)

Tiα

Tjβ

, α, β
(cid:107)

∈

−

∆U

n+1

(cid:111)

.

(32)

Since the maximum over indicator functions of sets is equal to the indicator
function of the intersection of the sets we obtain for Ψ ∗

Ψ ∗(q) = max

Ψ ∗i,j(q)

1

i,j

≤

≤|T |

= δ

(q).

K

Proof (Proof of Proposition 3). Let q
for all α, β

n+1 and 1

∆U

i, j

≤

≤ |T |

∈

Rd

×|V| s.t.

∈
. For any 1

(cid:107)
i

Qiα

−
≤ |T |

≤

Qjβ
deﬁne

(cid:107) ≤ (cid:107)

Tiα

Tjβ

−

(cid:107)

fi : Rn

Rn,
n
(cid:88)

l=1

→

(cid:55)→

n
(cid:88)

l=1

−

(α1, ..., αn)

αltil + (1

αl)tin+1 = Tiα,

(33)

(34)

Sublabel-Accurate Convex Relaxation of Vectorial Multilabel Energies

17

and analogously

gi : Rn

R|V|
n
(cid:88)

l=1

→

(cid:55)→

n
(cid:88)

l=1

−

(α1, ..., αn)

αlqil + (1

αl)qin+1 = Qiα.

Let us choose an α
Tjβ
Tiα
(cid:107)

−

(cid:107)

for all α, β

∈

∈

Rn such that αi > 0, (cid:80)

∆U

n+1 and 1

i, j

≤

≤ |T |

l αl < 1. Then
implies that

Qiα
(cid:107)

−

Qjβ

(cid:107) ≤

gi(α)

gi(α

h)

−

−

(cid:107) ≤ (cid:107)

(cid:107)

fi(α)

fi(α

−

,

h)
(cid:107)

−

holds for all vectors h with suﬃciently small entries. Inserting the deﬁnitions of
gi and fi we ﬁnd that

(cid:107) ≤ (cid:107)
holds for all h with suﬃciently small entries. For a non-degenerate triangle, TiD
is invertible and a simple substitution yields that

QiDh
(cid:107)

TiDh
(cid:107)

QiD(TiD)−

(cid:107)

1˜h

˜h
(cid:107)2 ≤ (cid:107)
(cid:107)

,

holds for all ˜h with suﬃciently small entries. This means that the operator norm
of Di

q induced by the (cid:96)2 norm, i.e. the S∞ norm, is bounded by one.
Let us now show the other direction. For q

1, 1

×|V| s.t.

Rd

(cid:13)
(cid:13)Di
q

(cid:13)
(cid:13)S∞ ≤

, note that inverting the above computation immediately yields that

∈

i

≤ |T |

Qkα
(cid:107)

−

Qkβ

(cid:107) ≤ (cid:107)

Tkα

Tkβ

−

(cid:107)

k

≤

, α, β

n+1. Our goal is to show that having this
holds for all 1
inequality on each simplex is suﬃcient to extend it to arbitrary pairs of simplices.
The overall idea of this part of the proof is illustrated in Fig. 9.
(cid:80)

≤ |T |

∈

∆U

Rn with αl, βl ≥

∈

0, (cid:80)

l αl ≤

l βl ≤

Let 1

i, j
Consider the line segment

≤ |T |

≤

and α, β

1 be given.

(35)

(36)

(37)

(38)

≤

(39)

(40)

c(γ) : [0, 1]

Rd

→

(cid:55)→

γ

γ Tjβ + (1

γ) Tiα.

−

Since the triangulated domain is convex, there exist 0 = a0 < a1 < . . . < ar = 1
and functions αl(γ) such that for γ
1 one can write
−
T . The continuity
c(γ) = γ Tjβ + (1
of c(γ) implies that Tkl αl(al+1) = Tkl+1αl+1(al+1), i.e. these points correspond
to both simplices, kl and kl+1. Note that this also means that Qkl αl(al+1) =

[al, al+1], 0
γ) Tiα = Tkl αl(γ) for some 1

l
r
≤
kl ≤

≤
≤

−

∈

18

E. Laude, T. M¨ollenhoﬀ, M. Moeller, J. Lellmann, D. Cremers

Qkl+1αl+1(al+1). The intuition of this construction is that the c(al+1) are located
on the boundaries of adjacent simplices on the line segment. We ﬁnd

Tiα
(cid:107)

−

Tjβ

=

(cid:107)

(al+1 −

al)
(cid:107)

Tiα

−

Tjβ

(cid:107)

r
1
(cid:88)
−

l=0

1
r
(cid:88)
−

l=0

r
1
(cid:88)
−

l=0

r
1
(cid:88)
−

l=0

1
r
(cid:88)
−

l=0

l=0

r
1
(cid:88)
−

l=0

r
1
(cid:88)
−

l=0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:107)

=

=

=

=

≥

=

=

=

(al+1 −
(cid:107)

al)(Tiα

Tjβ)

−

(cid:107)

al+1Tiα
(cid:107)

−

−

alTiα

al+1Tjβ + alTjβ

(cid:107)

alTjβ + (1
(cid:107)

−

al)Tiα

(al+1Tjβ + (1

−

al+1)Tiα)
(cid:107)

−

(41)

Tkl αl(al)
(cid:107)

−

Tkl αl(al+1)
(cid:107)

(39)

r
1
(cid:88)
−

≥

(cid:107)

Qkl αl(al)

(Qkl αl(al)

−

Qkl αl(al+1)
(cid:107)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Qkl αl(al+1))

−

−

(Qkl αl(al)

Qkl+1αl+1(al+1))

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Qk0α0(a0)

Qiα

Qjβ

−
,

(cid:107)

Qkr αr(ar)
(cid:107)

−
which yields the assertion.

(cid:107)

t1, . . . , tn+1
Proof (Proof of Proposition 4). Let ∆ = conv
be given by aﬃnely
{
Rn. We show that our lifting approach applied to
independent vertices ti
the label space ∆ solves the convexiﬁed unlifted problem, where the dataterm
(n+1) and
was replaced by its convex hull on ∆. Let the matrices T
D

n be deﬁned through

R(n+1)

Rn

∈

∈

}

×

×

∈

(cid:16)

(cid:17)

T =

t1, . . . , tn+1

, D =

, T D =

tn+1, . . . , tn

(cid:16)

t1

−

(cid:17)

,

tn+1

−

1









. . .

1 . . .

−

−







1

1

The transformation x
(cid:55)→
Now consider the following lifted function u : Ω

tn+1 + T Dx maps ∆e = conv

(42)
Rn to ∆.
Rn+1 parametrized through

0, e1, . . . , en

} ⊂

{

→

Sublabel-Accurate Convex Relaxation of Vectorial Multilabel Energies

19

˜u : Ω

∆e:

→

Consider a ﬁxed x
of the lifted dataterm ρ yields:

∈

(cid:16)

u(x) =

˜u1(x), . . . , ˜un(x), 1

(cid:80)n

(cid:17)
j=1 ˜uj(x)

.

−

(43)

Ω. Plugging this lifted representation into the biconjugate

ρ∗∗(u) = sup

u, v

Rn+1 (cid:104)

(cid:105) −

v

∈

sup
∆U

n+1

α

∈

α, v
(cid:104)

(cid:105) −

ρ(T α)

(cid:42)

= sup
v

Rn+1

∈

˜u1(x), . . . , ˜un(x), 1

˜uj(x)

 , v

n
(cid:88)

j=1

−



(cid:43)

−

= sup
v

Rn+1 (cid:104)

∈

sup
∆U

n+1

α

∈

α, v

(cid:104)

(cid:105) −

ρ(T α)

+ vn+1−

(cid:105)

sup
∆U

α

n+1

∈
˜u, D(cid:62)v

(cid:42)



ρ



n
(cid:88)

j=1



αjtj +

1

n
(cid:88)

j=1

−

α1, . . . , αn, 1

αj

 , v

n
(cid:88)

−



(cid:43)

−

j=1

 tn+1

αj





= sup
v

Rn+1 (cid:104)

∈

˜u, D(cid:62)v

+ vn+1 −

(cid:105)

sup
∆U

n+1

α

∈

vn+1 +

α, D(cid:62)v
(cid:104)

(cid:105) −

ρ(tn+1 + T Dα)

(44)

(45)

Since D(cid:62) is surjective, we can apply the substitution ˜v = D(cid:62)v:

. . . = sup

˜u, ˜v

Rn (cid:104)

˜v

∈

(cid:105) −

α, ˜v

sup
∆U

n+1

(cid:104)

(cid:105) −

= sup
˜v

Rn (cid:104)

˜u, ˜v

(cid:105) −

∈

α

∈
sup
w

∈

∆ (cid:104)

(T D)−

1(w

tn+1), ˜v

ρ(w).

−

(cid:105) −

ρ(tn+1 + T Dα)

In the last step the substitution w = tn+1 + T Dα
performed. This can be further simpliﬁed to

⇔

α = (T D)−

1(w

tn+1) was

−

. . . = sup

Rn (cid:104)

˜v

˜u, ˜v
(cid:105)

+

(T D)−
(cid:104)

1tn+1, ˜v

(ρ + δ∆)∗((T D)−

T ˜v)

˜u + (T D)−

1tn+1, ˜v

(ρ + δ∆)∗((T D)−

T ˜v)

(46)

T D˜u + tn+1, (T D)−

(ρ + δ∆)∗((T D)−

T ˜v).

(cid:105) −

(cid:105) −
T ˜v

(cid:105) −

∈
= sup
˜v

∈
= sup
˜v

∈

Rn (cid:104)

Rn (cid:104)

Since T D is invertible we can perform another substitution v(cid:48) = (T D)−

T ˜v.

. . . = sup

T D˜u + tn+1, v(cid:48)

v(cid:48)∈

Rn (cid:104)

(cid:105) −
= (ρ + δ∆)∗∗(tn+1 + T D˜u).

(ρ + δ∆)∗(v(cid:48))

(47)

20

E. Laude, T. M¨ollenhoﬀ, M. Moeller, J. Lellmann, D. Cremers

The lifted regularizer is given as:

(cid:90)

R(u) =

sup
Rd

q:Ω

→

n+1

×

Ω(cid:104)

u, Div q

Ψ ∗(q) dx

(cid:105) −

Using the parametrization by ˜u, this can be equivalently written as

˜uj Div(qj −

qn+1) + Div qn+1 dx,

(cid:90)

n
(cid:88)

Ω

j=1

sup

q(x)

∈K

K ⊂

=

q

{

K

where the set

Rd

×

n+1 can be written as

Rd

×

n+1

D(cid:62)q(cid:62)(T D)−

1

(cid:107)S∞ ≤

.
1
}

Note that since qn+1 ∈
partial integration. With the substituion ˜q(x) = D(cid:62)q(x)(cid:62) we have

| (cid:107)

∈
C∞c (Ω, Rd), the last term Div qn+1 in (49) vanishes by

(50)

(cid:90)

sup
˜
˜q
K

∈

˜u, Div ˜q
(cid:105)

Ω(cid:104)

dx,

with set ˜

Rd

n:

×

K ⊂

Note that since qi ∈
With another substituion q(cid:48)(x) = ˜q(x)(T D)−

1

n

×

q

{

∈

=

| (cid:107)

Rd

q(T D)−

˜
(cid:107)S∞ ≤
K
C∞c (Ω, Rd), the same holds for the linearly transformed ˜q.
1 we have

.
1
}

(52)

· · ·

(cid:90)

Ω(cid:104)
(cid:90)

Ω(cid:104)

= sup
q(cid:48)∈K(cid:48)
= sup
q(cid:48)∈K(cid:48)

˜u, Div q(cid:48)T D

dx

T D˜u, Div q(cid:48)

dx

(cid:105)

(cid:105)

(cid:48) =

q
{

∈

K

Rd

n

×

q

| (cid:107)

(cid:107)S∞ ≤

1

,
}

where the set

Rd

n+1 is given as

×

(cid:48)

K

⊂

which is the usual unlifted deﬁnition of the total variation T V (tn+1 + T D˜u).

This shows that the lifting method solves

(cid:90)

Ω

min
→

˜u:Ω

∆e

(ρ(x,

) + δ∆)∗∗(tn+1 + T D˜u(x))dx + λT V (tn+1 + T D˜u),
·

(55)

which is equivalent to the original problem but with a convexiﬁed data term.

(48)

(49)

(51)

(53)

(54)

Sublabel-Accurate Convex Relaxation of Vectorial Multilabel Energies

21

Input image

Mean µ

Variance σ

Fig. 10: Joint estimation of mean and variance. Our formulation can optimize
diﬃcult nonconvex joint optimization problems with continuous label spaces.

B Additional Experiment: Adaptive Denoising

In this experiment we jointly estimate the mean µ and variance σ of an image
R according to a Gaussian model. The label space is chosen as Γ =
I : Ω
[1, 10] and the dataterm as proposed in [8]:
[0, 255]

→
×

ρ(x, µ(x), σ(x)) =

+

log(2πσ(x)2).

(56)

(µ(x)

I(x))2

−
2σ(x)2

1
2

As the projection onto the epigraph of (ρ + δ∆)∗ seems diﬃcult to compute,
we approximate ρ by a piecewise linear function using 29
29 sublabels and
convexify it using the quickhull algorithm [3]. In Fig. 10 we show the result of
minimizing (56) with total variation regularization.

×

References

1. Alberti, G., Bouchitt´e, G., Maso, G.D.: The calibration method for the Mumford-
Shah functional and free-discontinuity problems. Calc. Var. Partial Dif. 3(16), 299–
333 (2003)

2. Ambrosio, L., Fusco, N., Pallara, D.: Functions of bounded variation and free
discontinuity problems. Oxford Mathematical Monographs, The Clarendon Press
Oxford University Press, New York (2000)

3. Barber, C.B., Dobkin, D.P., Huhdanpaa, H.: The quickhull algorithm for convex
hulls. ACM Transactions on Mathematical Software (TOMS) 22(4), 469–483 (1996)
4. Chambolle, A., Cremers, D., Pock, T.: A convex approach to minimal partitions.

SIAM Journal on Imaging Sciences 5(4), 1113–1158 (2012)

5. Fix, A., Agarwal, S.: Duality and the continuous graphical model. In: Com-
puter Vision ECCV 2014, Lecture Notes in Computer Science, vol. 8691, pp.
266–281. Springer International Publishing (2014), http://dx.doi.org/10.1007/
978-3-319-10578-9_18

22

E. Laude, T. M¨ollenhoﬀ, M. Moeller, J. Lellmann, D. Cremers

6. Giaquinta, M., Modica, G., Souˇcek, J.: Cartesian currents in the calculus of vari-
ations I, II., Ergebnisse der Mathematik und ihrer Grenzgebiete. 3., vol. 37-38.
Springer-Verlag, Berlin (1998)

7. Goldluecke, B., Strekalovskiy, E., Cremers, D.: The natural total variation which
arises from geometric measure theory. SIAM Journal on Imaging Sciences 5(2),
537–563 (2012)

8. Goldluecke, B., Strekalovskiy, E., Cremers, D.: Tight convex relaxations for vector-

valued labeling. SIAM Journal on Imaging Sciences 6(3), 1626–1664 (2013)

9. Ishikawa, H.: Exact optimization for Markov random ﬁelds with convex priors.
IEEE Trans. Pattern Analysis and Machine Intelligence 25(10), 1333–1336 (2003)
10. Lellmann, J., Schn¨orr, C.: Continuous multiclass labeling approaches and algo-

rithms. SIAM Journal on Imaging Sciences 4(4), 1049–1096 (2011)

11. Lellmann, J., Strekalovskiy, E., Koetter, S., Cremers, D.: Total variation regular-

ization for functions with values in a manifold. In: ICCV (December 2013)

12. M¨ollenhoﬀ, T., Laude, E., Moeller, M., Lellmann, J., Cremers, D.: Sublabel-

accurate relaxation of nonconvex energies. In: CVPR (2016)

13. Nocedal, J., Wright, S.J.: Numerical Optimization. Springer, New York, 2nd edn.

(2006)

14. Pock, T., Cremers, D., Bischof, H., Chambolle, A.: An algorithm for minimizing

the piecewise smooth Mumford-Shah functional. In: ICCV (2009)

15. Pock, T., Schoenemann, T., Graber, G., Bischof, H., Cremers, D.: A convex formu-
lation of continuous multi-label problems. In: European Conference on Computer
Vision (ECCV). Marseille, France (October 2008)

16. Pock, T., Cremers, D., Bischof, H., Chambolle, A.: Global solutions of variational
models with convex regularization. SIAM J. Imaging Sci. 3(4), 1122–1145 (2010)

17. Rockafellar, R., Wets, R.B.: Variational Analysis. Springer (1998)
18. Rudin, L.I., Osher, S., Fatemi, E.: Nonlinear total variation based noise removal

algorithms. Physica D: Nonlinear Phenomena 60(1), 259–268 (1992)

19. Sapiro, G., Ringach, D.: Anisotropic diﬀusion of multivalued images with applica-

tions to color ﬁltering. IEEE Trans. Img. Proc. 5(11), 1582–1586 (1996)

20. Strekalovskiy, E., Chambolle, A., Cremers, D.: Convex relaxation of vectorial prob-
lems with coupled regularization. SIAM Journal on Imaging Sciences 7(1), 294–336
(2014)

21. Zach, C.: Dual decomposition for joint discrete-continuous optimization. In: AIS-

TATS. pp. 632–640 (2013)

22. Zach, C., Kohli, P.: A convex discrete-continuous approach for markov random
ﬁelds. In: ECCV, vol. 7577, pp. 386–399. Springer Berlin Heidelberg (2012)

