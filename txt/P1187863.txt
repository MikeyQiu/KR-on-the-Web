Fast(er) Exact Decoding and Global Training for Transition-Based
Dependency Parsing via a Minimal Feature Set

Tianze Shi
Cornell University
tianze@cs.cornell.edu

Liang Huang
Oregon State University
liang.huang.sh@gmail.com

Lillian Lee
Cornell University
llee@cs.cornell.edu

Publication venue: Proceedings of EMNLP 2017

Abstract

We ﬁrst present a minimal feature set for
transition-based dependency parsing, con-
tinuing a recent trend started by Kiper-
wasser and Goldberg (2016a) and Cross
and Huang (2016a) of using bi-directional
LSTM features. We plug our minimal
feature set into the dynamic-programming
framework of Huang and Sagae (2010)
and Kuhlmann et al. (2011) to produce the
ﬁrst implementation of worst-case Opn3q
exact decoders for arc-hybrid and arc-
eager transition systems. With our mini-
mal features, we also present Opn3q global
training methods. Finally, using ensem-
bles including our new parsers, we achieve
the best unlabeled attachment score re-
ported (to our knowledge) on the Chinese
Treebank and the “second-best-in-class”
result on the English Penn Treebank.

1

Introduction

It used to be the case that the most accurate de-
pendency parsers made global decisions and em-
ployed exact decoding. But transition-based de-
pendency parsers (TBDPs) have recently achieved
state-of-the-art performance, despite the fact that
for efﬁciency reasons, they are usually trained to
make local, rather than global, decisions and the
decoding process is done approximately, rather
than exactly (Weiss et al., 2015; Dyer et al., 2015;
Andor et al., 2016). The key efﬁciency issue for
decoding is as follows. In order to make accurate
(local) attachment decisions, historically, TBDPs
have required a large set of features in order to ac-
cess rich information about particular positions in
the stack and buffer of the current parser conﬁgu-
ration. But consulting many positions means that
although polynomial-time exact-decoding algo-

rithms do exist, having been introduced by Huang
and Sagae (2010) and Kuhlmann et al. (2011), un-
fortunately, they are prohibitively costly in prac-
tice, since the number of positions considered can
factor into the exponent of the running time. For
instance, Huang and Sagae employ a fairly re-
duced set of nine positions, but the worst-case run-
ning time for the exact-decoding version of their
algorithm is Opn6q (originally reported as Opn7q)
for a length-n sentence. As an extreme case, Dyer
et al. (2015) use an LSTM to summarize arbitrary
information on the stack, which completely rules
out dynamic programming.

Recently, Kiperwasser and Goldberg (2016a)
and Cross and Huang (2016a) applied bi-
long short-term memory networks
directional
(Graves and Schmidhuber, 2005, bi-LSTMs) to
derive feature representations for parsing, because
these networks capture wide-window contextual
information well. Collectively, these two sets of
authors demonstrated that with bi-LSTMs, four
positional features sufﬁce for the arc-hybrid pars-
ing system (K&G), and three sufﬁce for arc-
standard (C&H).1

Inspired by their work, we arrive at a minimal
feature set for arc-hybrid and arc-eager:
it con-
tains only two positional bi-LSTM vectors, suf-
fers almost no loss in performance in comparison
to larger sets, and out-performs a single position.
(Details regarding the situation with arc-standard
can be found in §2.)

Our minimal feature set plugs into Huang and
Sagae’s and Kuhlmann et al.’s dynamic program-

1We note that K&G were not focused on minimizing posi-
tions, although they explicitly noted the implications of doing
so: “While not explored in this work, [fewer positions] re-
sults in very compact state signatures, [which is] very appeal-
ing for use in transition-based parsers that employ dynamic-
programming search” (pg. 319). C&H also noted in their
follow-up (Cross and Huang, 2016b) the possibility of future
work using dynamic programming thanks to simple features.

7
1
0
2
 
g
u
A
 
0
3
 
 
]
L
C
.
s
c
[
 
 
1
v
3
0
4
9
0
.
8
0
7
1
:
v
i
X
r
a

ming framework to produce the ﬁrst implementa-
tion of Opn3q exact decoders for arc-hybrid and
arc-eager parsers. We also enable and implement
Opn3q global training methods. Empirically, en-
sembles containing our minimal-feature, globally-
trained and exactly-decoded models produce the
best unlabeled attachment score (UAS) reported
(to our knowledge) on the Chinese Treebank and
the “second-best-in-class” result on the English
Penn Treebank.2

Additionally, we provide a slight update to
the theoretical connections previously drawn by
G´omez-Rodr´ıguez, Carroll, and Weir (2008, 2011)
between TBDPs and the graph-based dependency
parsing algorithms of Eisner (1996) and Eisner
and Satta (1999), including results regarding the
arc-eager parsing system.

2 A Minimal Feature Set

TBDPs incrementally process a sentence by mak-
ing transitions through search states representing
parser conﬁgurations. Three of the main transition
systems in use today (formal introduction in §3.1)
all maintain the following two data structures in
their conﬁgurations: (1) a stack of partially parsed
subtrees and (2) a buffer (mostly) of unprocessed
sentence tokens.

To featurize conﬁgurations for use in a scoring
function, it is common to have features that extract
information about the ﬁrst several elements on the
stack and the buffer, such as their word forms and
part-of-speech (POS) tags. We refer to these as po-
sitional features, as each feature relates to a partic-
ular position in the stack or buffer. Typically, mil-
lions of sparse indicator features (often developed
via manual engineering) are used.

In contrast, Chen and Manning (2014) intro-
duce a feature set consisting of dense word-, POS-,
and dependency-label embeddings. While dense,
these features are for the same 18 positions that
have been typically used in prior work. Re-
cently, Kiperwasser and Goldberg (2016a) and
Cross and Huang (2016a) adopt bi-directional
LSTMs, which have nice expressiveness and
context-sensitivity properties, to reduce the num-
ber of positions considered down to four and three,

2Our ideas were subsequently adapted to the labeled set-
ting by Shi, Wu, Chen, and Cheng (2017) in their submis-
sion to the CoNLL 2017 shared task on Universal Dependen-
cies parsing. Their team achieved the second-highest labeled
attachment score in general and had the top average perfor-
mance on the surprise languages.

Features Arc-standard Arc-hybrid Arc-eager

t

ÑÐ
ÑÐ
s 1,
s 2,
ÑÐ
s 1,
t
t

ÑÐ
s 0,
ÑÐ
s 0,
ÑÐ
s 0,
t

ÑÐ
b 0u
ÑÐ
b 0u
ÑÐ
b 0u
ÑÐ
b 0u

93.95˘0.12 94.08˘0.13 93.92˘0.04
94.13˘0.06 94.08˘0.05 93.91˘0.07
54.47˘0.36 94.03˘0.12 93.92˘0.07
47.11˘0.44 52.39˘0.23 79.15˘0.06

Min positions Arc-standard Arc-hybrid Arc-eager

K&G 2016a
C&H 2016a
our work

-
3
3

4
-
2

-
-
2

Table 1: Top: English PTB dev-set UAS% for
progressively smaller sets of positional features,
for greedy parsers with different transition sys-
tems. The “double-arrow” notation indicates vec-
tors produced by a bi-directional LSTM. Internal
lines highlight large performance drop-offs when
a feature is deleted. Bottom: sizes of the minimal
feature sets in Kiperwasser and Goldberg (2016a),
Cross and Huang (2016a), and our work.

for different transition systems, respectively.

This naturally begs the question, what is the
lower limit on the number of positional features
necessary for a parser to perform well? Kiper-
wasser and Goldberg (2016a) reason that for the
arc-hybrid system, the ﬁrst and second items on
the stack and the ﬁrst buffer item — denoted by s0,
s1, and b0, respectively — are required; they addi-
tionally include the third stack item, s2, because
it may not be adjacent to the others in the origi-
nal sentence. For arc-standard, Cross and Huang
(2016a) argue for the necessity of s0, s1, and b0.

We address the lower-limit question empiri-
cally, and ﬁnd that, surprisingly, two positions
sufﬁce for the greedy arc-eager and arc-hybrid
parsers. We also provide empirical support for
Cross and Huang’s argument for the necessity of
three features for arc-standard. In the rest of this
section, we explain our experiments, run only on
an English development set, that support this con-
clusion; the results are depicted in Table 1. We
later explore the implementation implications in
§3-4 and then test-set parsing-accuracy in §6.

We employ the same model architecture as
Kiperwasser and Goldberg (2016a). Speciﬁcally,
we ﬁrst use a bi-LSTM to encode an n-token sen-
tence, treated as a sequence of per-token concate-
nations of word- and POS-tag embeddings, into a
ÑÐ
ÑÐ
wi
w1, . . . ,
sequence of vectors r

ÑÐ
wns, where each

ÑÐ

ÑÐ
s j, deﬁned as

is the output of the bi-LSTM at time step i. (The
double-arrow notation for these vectors empha-
sizes the bi-directionality of their origin). Then,
for a given parser conﬁguration, stack positions
wipsj q where
are represented by
ipsjq gives the position in the sentence of the to-
ken that is the head of the tree in sj. Similarly,
ÑÐ
b j, deﬁned as
buffer positions are represented by
ÑÐ
wipbj q for the token at buffer position j. Finally,
as in Chen and Manning (2014), we use a multi-
layer perceptron to score possible transitions from
the given conﬁguration, where the input is the con-
ÑÐ
b ks.
catenation of some selection of the
We use greedy decoders, and train the models with
dynamic oracles (Goldberg and Nivre, 2013).

ÑÐ
s js and

Table 1 reports the parsing accuracy that re-
sults for feature sets of size four, three, two, and
one for three commonly-used transition systems.
The data is the development section of the English
Penn Treebank (PTB), and experimental settings
are as described in our other experimental section,
§6. We see that we can go down to three or, in the
arc-hybrid and arc-eager transition systems, even
two positions with very little loss in performance,
ÑÐ
b 0u our
but not further. We therefore call t
minimal feature set with respect to arc-hybrid and
arc-eager, and empirically conﬁrm that Cross and
ÑÐ
b 0u is minimal for arc-standard;
Huang’s t
see Table 1 for a summary.3

ÑÐ
s 0,

ÑÐ
s 1,

ÑÐ
s 0,

3 Dynamic Programming for TBDPs

As stated in the introduction, our minimal fea-
ture set from §2 plugs into Huang and Sagae and
Kuhlmann et al.’s dynamic programming (DP)
framework. To help explain the connection, this
section provides an overview of the DP frame-
work. We draw heavily from the presentation of
Kuhlmann et al. (2011).

3.1 Three Transition Systems

Transition-based parsing (Nivre, 2008; K¨ubler
et al., 2009) is an incremental parsing framework
based on transitions between parser conﬁgura-

ÑÐ
s 0,

3We tentatively conjecture that the following might ex-
plain the observed phenomena, but stress that we don’t cur-
rently see a concrete way to test the following hypothesis.
ÑÐ
b 0u, in the arc-standard case, situations can arise
With t
where there are multiple possible transitions with missing in-
formation. In contrast, in the arc-hybrid case, there is only
one possible transition with missing information (namely,
ÑÐ
reñ, introduced in §3.1); perhaps
s 1 is therefore not so cru-
cial for arc-hybrid in practice?

tions. For a sentence to be parsed, the system
starts from a corresponding initial conﬁguration,
and attempts to sequentially apply transitions un-
til a conﬁguration corresponding to a full parse is
produced. Formally, a transition system is deﬁned
as S “ pC, T, cs, Cτ q, where C is a nonempty set
of conﬁgurations, each t P T : C á C is a transi-
tion function between conﬁgurations, cs is an ini-
tialization function that maps an input sentence to
an initial conﬁguration, and Cτ Ď C is a set of
terminal conﬁgurations.

All systems we consider share a common tri-
partite representation for conﬁgurations: when we
write c “ pσ, β, Aq for some c P C, we are re-
ferring to a stack σ of partially parsed subtrees; a
buffer β of unprocessed tokens and, optionally, at
its beginning, a subtree with only left descendants;
and a set A of elements ph, mq, each of which is
an attachment (dependency arc) with head h and
modiﬁer m.4 We write mðh to indicate that m
left-modiﬁes h, and hñm to indicate that m right-
modiﬁes h. For a sentence w “ w1, ..., wn, the
initial conﬁguration is pσ0, β0, A0q, where σ0 and
A0 are empty and β0 “ rROOT|w1, ..., wns; ROOT
is a special node denoting the root of the parse
tree5 (vertical bars are a notational convenience
for indicating different parts of the buffer or stack;
our convention is to depict the buffer ﬁrst element
leftmost, and to depict the stack ﬁrst element right-
most). All terminal conﬁgurations have an empty
buffer and a stack containing only ROOT.

Arc-Standard The arc-standard system (Nivre,
2004) is motivated by bottom-up parsing: each de-
pendent has to be complete before being attached.
The three transitions, shift (sh, move a token from
the buffer to the stack), right-reduce (reñ, reduce
and attach a right modiﬁer), and left-reduce (reð,
reduce and attach a left modiﬁer), are deﬁned as:

shrpσ, b0|β, Aqs “ pσ|b0, β, Aq
reñrpσ|s1|s0, β, Aqs “ pσ|s1, β, A Y tps1, s0quq
reðrpσ|s1|s0, β, Aqs “ pσ|s0, β, A Y tps0, s1quq

Arc-Hybrid The arc-hybrid system (Yamada
and Matsumoto, 2003; G´omez-Rodr´ıguez et al.,
2008; Kuhlmann et al., 2011) has the same deﬁ-
nitions of sh and reñ as arc-standard, but forces

4For simplicity, we only present unlabeled parsing here.

See Shi et al. (2017) for labeled-parsing results.

5Other presentations place ROOT at the end of the buffer

or omit it entirely (Ballesteros and Nivre, 2013).

the collection of left modiﬁers before right modi-
ﬁers via its b0-modiﬁer reð transition. This con-
trasts with arc-standard, where the attachment of
left and right modiﬁers can be interleaved on the
stack.

shrpσ, b0|β, Aqs “ pσ|b0, β, Aq
reñrpσ|s1|s0, β, Aqs “ pσ|s1, β, A Y tps1, s0quq
reðrpσ|s0, b0|β, Aqs “ pσ, b0|β, A Y tpb0, s0quq

Arc-Eager
In contrast to the former two sys-
tems, the arc-eager system (Nivre, 2003) makes
attachments as early as possible — even if a modi-
ﬁer has not yet received all of its own modiﬁers.
This behavior is accomplished by decomposing
the right-reduce transition into two independent
transitions, one making the attachment (ra) and
one reducing the right-attached child (re).

shrpσ, b0|β, Aqs “ pσ|b0, β, Aq
reðrpσ|s0, b0|β, Aqs “ pσ, b0|β, A Y tpb0, s0quq
(precondition: s0 not attached to any word)
rarpσ|s0, b0|β, Aqs “ pσ|s0|b0, β, A Y tps0, b0quq
rerpσ|s0, β, Aqs “ pσ, β, Aq

(precondition: s0 has been attached to its head)

3.2 Deduction and Dynamic Programming

Kuhlmann et al. (2011) reformulate the three tran-
sition systems just discussed as deduction systems
(Pereira and Warren, 1983; Shieber et al., 1995),
wherein transitions serve as inference rules; these
are given as the lefthand sides of the ﬁrst three sub-
ﬁgures in Figure 1. For a given w “ w1, ..., wn,
assertions take the form ri, j, ks (or, when applica-
ble, a two-index shorthand to be discussed soon),
meaning that there exists a sequence of transi-
tions that, starting from a conﬁguration wherein
head ps0q “ wi, results in an ending conﬁgura-
tion wherein head ps0q “ wj and head pb0q “ wk.
If we deﬁne w0 as ROOT and wn`1 as an end-
of-sentence marker, then the goal theorem can be
stated as r0, 0, n ` 1s.

For arc-standard, we depict an assertion ri, h, ks
as a subtree whose root (head) is the token at h.
Assertions of the form ri, i, ks play an important
role for arc-hybrid and arc-eager, and we employ
the special shorthand ri, ks for them in Figure 1.
In that ﬁgure, we also graphically depict such sit-
uations as two consecutive half-trees with roots wi
and wk, where all tokens between i and k are al-
ready attached. The superscript b in an arc-eager

assertion rib, js is an indicator variable for whether
wi has been attached to its head (b “ 1) or not
(b “ 0) after the transition sequence is applied.

Kuhlmann et al. (2011) show that all three de-
duction systems can be directly “tabularized” and
dynamic programming (DP) can be applied, such
that, ignoring for the moment the issue of incor-
porating complex features (we return to this later),
time and space needs are low-order polynomial.
Speciﬁcally, as the two-index shorthand ri, js sug-
gests, arc-eager and arc-hybrid systems can be im-
plemented to take Opn2q space and Opn3q time;
the arc-standard system requires Opn3q space and
Opn4q time (if one applies the so-called hook trick
(Eisner and Satta, 1999)).

Since an Opn4q running time is not sufﬁciently
practical even in the simple-feature case, in the re-
mainder of this paper we consider only the arc-
hybrid and arc-eager systems, not arc-standard.

4 Practical Optimal Algorithms Enabled

By Our Minimal Feature Set

Until now, no one had suggested a set of positional
features that was both information-rich enough for
accurate parsing and small enough to obtain the
Opn3q running-time promised above. Fortunately,
ÑÐ
b 0u feature set qualiﬁes,
our bi-LSTM-based t
and enables the fast optimal procedures described
in this section.

ÑÐ
s 0,

4.1 Exact Decoding

Given an input sentence, a TBDP must choose
among a potentially exponential number of cor-
responding transition sequences. We assume ac-
cess to functions ft that score individual conﬁgu-
rations, where these functions are indexed by the
transition functions t P T . For a ﬁxed transition
sequence t “ t1, t2, . . ., we use ci to denote the
conﬁguration that results after applying ti.

Typically, for efﬁciency reasons, greedy left-to-
right decoding is employed: the next transition t˚
i
out of ci´1 is arg maxt ftpci´1q, so that past and
future decisions are not taken into account. The
score F ptq for the transition sequence is induced
by summing the relevant ftipci´1q values.

However, our use of minimal feature sets en-
ables direct computation of an argmax over the en-
tire space of transition sequences, arg maxt F ptq,
via dynamic programming, because our positions
don’t rely on any information “outside” the deduc-
tion rule indices, thus eliminating the need for ad-

h

j

j

j

j ` 1

h1

h2

k

k

h1

h2

0

0

1

i

i

i

i

i

0

0

j

j

j

j

j

j

j

j

Axiom r0, 0, 1s

Inference Rules

sh

ri, h, js
rj, j, j ` 1s

Axiom r0, 1s

Inference Rules

0

1

i

j

j ď n

sh

ri, js
rj, j ` 1s

j ď n

j j `1

reñ

ri, h1, ks

rk, h2, js

ri, h1, js

h1

hñ

1 h2

reñ

rk, is

ri, js

rk, js

reð

ri, h1, ks

rk, h2, js

ri, h2, js

hð

1 h2

h2

reð

rk, is

ri, js

rk, js

k

k

k

k

0

i i

i i

j

j

j

j

kñi

iðj

n ` 1

Goal

r0, 0, n ` 1s

n ` 1

Goal

r0, n ` 1s

(a) Arc-standard

00

1

(b) Arc-hybrid

i

i ` 1

j

j

0 ď i, j ď n

Axioms

Inference Rules

Axiom r00, 1s

Inference Rules

sh

ra

rib, js
rj0, j ` 1s

rib, js
rj1, j ` 1s

reð

rkb, is

ri0, js

rkb, js

re

rkb, is

ri1, js

rkb, js

j

j

i i0

i i1

ib

ib

kb

kb

kb

kb

00

j ď n

right-attach

j0 j `1

iñj
j ď n

j1 j `1

right-reduce

iñj

iðj

left-attach

left-reduce

k k

iðj

i i

k k

i i

j

j

j

j

j

j

j

j

k

k

i

i

k

k

i

i

0

Goal

r00, n ` 1s

Goal

n ` 1

(c) Arc-eager

(d) Edge-factored graph-based parsing.

n ` 1

Figure 1: 1a-1c: Kuhlmann et al.’s inference rules for three transition systems, together with CKY-style
visualizations of the local structures involved and, to their right, conditions for the rule to apply. 1d: the
edge-factored graph-based parsing algorithm (Eisner and Satta, 1999) discussed in §5.

ditional state-keeping.

We show how to integrate the scoring functions
for the arc-eager system; the arc-hybrid system is
handled similarly. The score-annotated rules are
as follows:
rib, js : v
rj0, j ` 1s : 0

rkb, is : v1
ri0, js : v2
rkb, js : v1 ` v2 ` ∆

preðq

pshq

ÑÐ
wi,

ÑÐ
wk,

ÑÐ
wiq ` freðp

ÑÐ
wjq — abus-
where ∆ “ fshp
ing notation by referring to conﬁgurations by their
features. The left-reduce rule says that we can ﬁrst
take the sequence of transitions asserted by rkb, is,
which has a score of v1, and then a shift transition
moving wi from b0 to s0. This means that the ini-
tial condition for ri0, js is met, so we can take the
sequence of transitions asserted by ri0, js — say it
has score v2 — and ﬁnally a left-reduce transition
to ﬁnish composing the larger transition sequence.
Notice that the scores for sh and ra are 0, as the
scoring of these transitions is accounted for by re-
duce rules elsewhere in the sequence.

4.2 Global Training

We employ large-margin training that considers
each transition sequence globally. Formally, for a
training sentence w “ w1, . . . , wn with gold tran-
sition sequence tgold, our loss function is

´

¯
F ptq ` costptgold, tq ´ F ptgoldq

max
t

where costptgold, tq is a custom margin for tak-
ing t instead of tgold — speciﬁcally, the number
of mis-attached nodes. Computing this max can
again be done efﬁciently with a slight modiﬁca-
tion to the scoring of reduce transitions:

rkb, is : v1 ri0, js : v2
rkb, js : v1 ` v2 ` ∆1 preðq

where ∆1 “ ∆ ` 1 phead pwiq ‰ wjq. This loss-
augmented inference or cost-augmented decoding
(Taskar et al., 2005; Smith, 2011) technique has
previously been applied to graph-based parsing by
Kiperwasser and Goldberg (2016a).

Efﬁciency Note The computation decomposes
into two parts: scoring all feature combinations,
and using DP to ﬁnd a proof for the goal theorem
in the deduction system. Time-complexity analy-
sis is usually given in terms of the latter, but the
former might have a large constant factor, such
as 104 or worse for neural-network-based scoring

functions. As a result, in practice, with a small
b 0u (Opn2q)
n, scoring with the feature set t
can be as time-consuming as the decoding steps
(Opn3q) for the arc-hybrid and arc-eager systems.

ÑÐ
s 0,

ÑÐ

5 Theoretical Connections

Our minimal feature set brings implementation of
practical optimal algorithms to TBDPs, whereas
previously only graph-based dependency parsers
(GBDPs) — a radically different, non-incremental
paradigm — enjoyed the ability to deploy them.
Interestingly, for both the transition- and graph-
based paradigms, the optimal algorithms build de-
pendency trees bottom-up from local structures. It
is thus natural to wonder if there are deeper, more
formal connections between the two.

In previous work, Kuhlmann et al. (2011) re-
lated the arc-standard system to the classic CKY
algorithm (Cocke, 1969; Kasami, 1965; Younger,
1967) in a manner clearly suggested by Figure 1a;
CKY can be viewed as a very simple graph-based
approach. G´omez-Rodr´ıguez et al. (2008, 2011)
formally prove that sequences of steps in the edge-
factored GBDP (Eisner, 1996) can be used to em-
ulate any individual step in the arc-hybrid system
(Yamada and Matsumoto, 2003) and the Eisner
and Satta (1999, Figure 1d) version. However,
they did not draw an explicitly direct connection
between Eisner and Satta (1999) and TBDPs.

ÑÐ
h,

Here, we provide an update to these previous
ﬁndings, stated in terms of the expressiveness of
scoring functions, considered as parameterization.
For the edge-factored GBDP, we write the score
ÑÐ
for an edge as fGp
mq, where h is the head and
m the modiﬁer. A tree’s score is the sum of its
edge scores. We say that a parameterized depen-
dency parsing model A contains model B if for ev-
ery instance of parameterization in model B, there
exists an instance of model A such that the two
models assign the same score to every parse tree.
We claim:

Lemma 1. The arc-eager model presented in §4.1
contains the edge-factored model.

Proof Sketch. Consider a given edge-factored
GBDP parameterized by fG. For any parse tree,
every edge iðj involves two deduction rules, and
their contribution to the score of the ﬁnal proof is
ÑÐ
ÑÐ
ÑÐ
ÑÐ
wi) “
wk,
wjq. We set fsh(
wk,
fsh(
ÑÐ
ÑÐ
0 and freðp
wiq. Similarly,
wj,
for edges kñi in the other direction, we set

ÑÐ
wi) ` freðp
ÑÐ
ÑÐ
wjq “ fGp
wi,

ÑÐ
wi,

Model

Training

Features

PTB
UAS (%) UEM (%)

CTB
UAS (%) UEM (%)

Arc-standard

Local

93.95˘0.12 52.29˘0.66

88.01˘0.26 36.87˘0.53

Arc-hybrid

Arc-eager

Local
Local
Global

Local
Local
Global

ÑÐ
s 2,

t

ÑÐ
s 1,

ÑÐ
s 0,

ÑÐ
b 0u

ÑÐ
s 2,

t

ÑÐ
b 0u

ÑÐ
s 2,

t

ÑÐ
b 0u

ÑÐ
s 1,
ÑÐ
s 0,
ÑÐ
s 0,

ÑÐ
s 0,
ÑÐ
b 0u
ÑÐ
b 0u

t

t

ÑÐ
s 1,
ÑÐ
s 0,
ÑÐ
s 0,

ÑÐ
s 0,
ÑÐ
b 0u
ÑÐ
b 0u

t

t

ÑÐ
h ,

t

ÑÐ
mu

93.89˘0.10 50.82˘0.75
93.80˘0.12 49.66˘0.43
94.43˘0.08 53.03˘0.71

87.87˘0.17 35.47˘0.48
87.78˘0.09 35.09˘0.40
88.38˘0.11 36.59˘0.27

93.80˘0.12 49.66˘0.43
93.77˘0.08 49.71˘0.24
94.53˘0.05 53.77˘0.46

87.49˘0.20 33.15˘0.72
87.33˘0.11 34.17˘0.41
88.62˘0.09 37.75˘0.87

Edge-factored

Global

94.50˘0.13 53.86˘0.78

88.25˘0.12 36.42˘0.52

Table 2: Test set performance for different training regimes and feature sets. The models use the same
decoders for testing and training. For each setting, the average and standard deviation across 5 runs with
different random initializations are reported. Boldface: best (averaged) result per dataset/measure.

ÑÐ
wk,

ÑÐ
wi) “ fGp

ÑÐ
fra(
wjq “ 0.
The parameterization we arrive at emulates ex-
actly the scoring model of fG.

ÑÐ
wiq and frep

ÑÐ
wk,

ÑÐ
wi,

We further claim that the arc-eager model is
more expressive than not only the edge-factored
GBDP, but also the arc-hybrid model in our paper.

Lemma 2. The arc-eager model contains the arc-
hybrid model.

Proof Sketch. We leverage the fact that the arc-
eager model divides the sh transition in the arc-
hybrid model into two separate transitions, sh and
ra. When we constrain the parameters fsh “ fra in
the arc-eager model, the model hypothesis space
becomes exactly the same as arc-hybrid’s.

The extra expressiveness of the arc-eager model
comes from the scoring functions fsh and fre
that capture structural contexts other than head-
modiﬁer relations. Unlike traditional higher-order
graph-based parsing that directly models relations
such as siblinghood (McDonald and Pereira, 2006)
or grandparenthood (Carreras, 2007), however, the
arguments in those two functions do not have any
ﬁxed type of structural interactions.

6 Experiments

Data and Evaluation We experimented with
English and Chinese. For English, we used the
Stanford Dependencies (de Marneffe and Man-
ning, 2008) conversion (via the Stanford parser
3.3.0) of the Penn Treebank (Marcus et al., 1993,
PTB). As is standard, we used §2-21 of the Wall
Street Journal for training, §22 for development,

and §23 for testing; POS tags were predicted using
10-way jackkniﬁng with the Stanford max entropy
tagger (Toutanova et al., 2003). For Chinese, we
used the Penn Chinese Treebank 5.1 (Xue et al.,
2002, CTB), with the same splits and head-ﬁnding
rules for conversion to dependencies as Zhang
and Clark (2008). We adopted the CTB’s gold-
standard tokenization and POS tags. We report
unlabeled attachment score (UAS) and sentence-
level unlabeled exact match (UEM). Following
prior work, all punctuation is excluded from eval-
uation. For each model, we initialized the network
parameters with 5 different random seeds and re-
port performance average and standard deviation.

Implementation Details Our model structures
reproduce those of Kiperwasser and Goldberg
(2016a). We use 2-layer bi-directional LSTMs
with 256 hidden cell units. Inputs are concatena-
tions of 28-dimensional randomly-initialized part-
of-speech embeddings and 100-dimensional word
vectors initialized from GloVe vectors (Penning-
ton et al., 2014) (English) and pre-trained skip-
gram-model vectors (Mikolov et al., 2013) (Chi-
nese). The concatenation of the bi-LSTM feature
vectors is passed through a multi-layer perceptron
(MLP) with 1 hidden layer which has 256 hid-
den units and activation function tanh. We set the
dropout rate for the bi-LSTM (Gal and Ghahra-
mani, 2016) and MLP (Srivastava et al., 2014) for
each model according to development-set perfor-
mance.6 All parameters except the word embed-

6For bi-LSTM input and recurrent connections, we con-

sider dropout rates in t0., 0.2u, and for MLP, t0., 0.4u.

Figure 2: Comparing our UAS results with results from the literature. x-axis: PTB; y-axis: CTB. Most
datapoint labels give author initials and publication year; citations are in the bibliography. Ensemble
datapoints are annotated with ensemble size. Weiss et al. (2015) and Andor et al. (2016) achieve UAS of
94.26 and 94.61 on PTB with beam search, but did not report CTB results, and are therefore omitted.

dings are initialized uniformly (Glorot and Ben-
gio, 2010). Approximately 1,000 tokens form a
mini-batch for sub-gradient computation. We train
each model for 20 epochs and perform model se-
lection based on development UAS. The proposed
structured loss function is optimized via Adam
(Kingma and Ba, 2015). The neural network com-
putation is based on the python interface to DyNet
(Neubig et al., 2017), and the exact decoding al-
gorithms are implemented in Cython.7

Main Results We implement exact decoders for
the arc-hybrid and arc-eager systems, and present
the test performance of different model conﬁgu-
rations in Table 2, comparing global models with
local models. All models use the same decoder
for testing as during the training process. Though
no global decoder for the arc-standard system has
been explored in this paper, its local models are
listed for comparison. We also include an edge-
factored graph-based model, which is convention-
ally trained globally. The edge-factored model
scores bi-LSTM features for each head-modiﬁer
pair; a maximum spanning tree algorithm is used
to ﬁnd the tree with the highest sum of edge
scores. For this model, we use Dozat and Man-

7See https://github.com/tzshi/dp-parser-emnlp17 .

ning’s (2017) biafﬁne scoring model, although in
our case the model size is smaller.8

Analogously to the dev-set results given in §2,
on the test data, the minimal feature sets perform
as well as larger ones in locally-trained models.
And there exists a clear trend of global models out-
performing local models for the two different tran-
sition systems on both datasets. This illustrates the
effectiveness of exact decoding and global train-
ing. Of the three types of global models, the arc-
eager arguably has the edge, an empirical ﬁnding
resonating with our theoretical comparison of their
model expressiveness.

Comparison with State-of-the-Art Models
Figure 2 compares our algorithms’ results with
those of the state-of-the-art.9 Our models are
competitive and an ensemble of 15 globally-
trained models (5 models each for arc-eager DP,
arc-hybrid DP and edge-factored) achieves 95.33
and 90.22 on PTB and CTB, respectively, reach-

8The same architecture and model size as other transition-

based global models is used for fair comparison.

9We exclude Choe and Charniak (2016), Kuncoro et al.
(2017) and Liu and Zhang (2017), which convert constituent-
based parses to dependency parses. They produce higher PTB
UAS, but access more training information and do not di-
rectly apply to datasets without constituency annotation.

ing the highest reported UAS on the CTB dataset,
and the second highest reported on the PTB
dataset among dependency-based approaches.

7 Related Work Not Yet Mentioned

Approximate Optimal Decoding/Training Be-
sides dynamic programming (Huang and Sagae,
2010; Kuhlmann et al., 2011), various other ap-
proaches have been proposed for approaching
global training and exact decoding. Best-ﬁrst
and A* search (Klein and Manning, 2003; Sagae
and Lavie, 2006; Sagae and Tsujii, 2007; Zhao
et al., 2013; Thang et al., 2015; Lee et al., 2016)
give optimality certiﬁcates when solutions are
found, but have the same worst-case time com-
plexity as the original search framework. Other
common approaches to search a larger space at
training or test time include beam search (Zhang
and Clark, 2011), dynamic oracles (Goldberg and
Nivre, 2012, 2013; Cross and Huang, 2016b) and
error states (Vaswani and Sagae, 2016). Beam
search records the k best-scoring transition pre-
ﬁxes to delay local hard decisions, while the lat-
ter two leverage conﬁgurations deviating from the
gold transition path during training to better simu-
late the test-time environment.

Neural Parsing Neural-network-based models
are widely used in state-of-the-art dependency
parsers (Henderson, 2003, 2004; Chen and Man-
ning, 2014; Weiss et al., 2015; Andor et al., 2016;
Dozat and Manning, 2017) because of their ex-
pressive representation power. Recently, Stern
et al. (2017) have proposed minimal span-based
features for constituency parsing.

Recurrent and recursive neural networks can be
used to build representations that encode complete
conﬁguration information or the entire parse tree
(Le and Zuidema, 2014; Dyer et al., 2015; Kiper-
wasser and Goldberg, 2016b), but these models
cannot be readily combined with DP approaches,
because their state spaces cannot be merged into
smaller sets and thus remain exponentially large.

8 Concluding Remarks

In this paper, we have shown the following.
ÑÐ
s 0,

• The bi-LSTM-powered feature set t

ÑÐ
b 0u
is minimal yet highly effective for arc-hybrid
and arc-eager transition-based parsing.

• Since DP algorithms for exact decoding
(Huang and Sagae, 2010; Kuhlmann et al.,

2011) have a run-time dependence on the
number of positional features, using our mere
two effective positional features results in a
running time of Opn3q, feasible for practice.

• Combining exact decoding with global train-
ing — which is also enabled by our minimal
feature set — with an ensemble of parsers
achieves 90.22 UAS on the Chinese Treebank
and 95.33 UAS on the Penn Treebank: these
are, to our knowledge, the best and second-
best results to date on these data sets among
“purely” dependency-based approaches.

There are many directions for further explo-
ration. Two possibilities are to create even better
training methods, and to ﬁnd some way to extend
our run-time improvements to other transition sys-
tems.
It would also be interesting to further in-
vestigate relationships between graph-based and
In §5 we have men-
dependency-based parsing.
tioned important earlier work in this regard, and
provided an update to those formal ﬁndings.

In our work, we have brought exact decoding,
which was formerly the province solely of graph-
based parsing, to the transition-based paradigm.
We hope that the future will bring more inspira-
tion from an integration of the two perspectives.

Acknowledgments:
an author-reviewer suc-
cess story We sincerely thank all the reviewers
for their extraordinarily careful and helpful com-
ments. Indeed, this paper originated as a short pa-
per submission by TS&LL to ACL 2017, where
an anonymous reviewer explained in the review
comments how, among other things, the DP run-
time could be improved from Opn4q to Opn3q. In
their author response, TS&LL invited the reviewer
to co-author, suggesting that they ask the confer-
ence organizers to make the connection between
anonymous reviewer and anonymous authors. All
three of us are truly grateful to PC co-chair Regina
Barzilay for implementing this idea, bringing us
together!

We also thank Kai Sun for help with Chi-
nese word vectors, and Xilun Chen, Yao Cheng,
Dezhong Deng, Juneki Hong, Jon Kleinberg,
Ryan McDonald, Ashudeep Singh, and Kai Zhao
for discussions and suggestions. TS and LL were
supported in part by a Google focused research
grant to Cornell University. LH was supported in
part by NSF IIS-1656051, DARPA N66001-17-2-
4030, and a Google Faculty Research Award.

References

Daniel Andor, Chris Alberti, David Weiss, Aliaksei
Severyn, Alessandro Presta, Kuzman Ganchev, Slav
Petrov, and Michael Collins. 2016. Globally nor-
In Pro-
malized transition-based neural networks.
ceedings of the Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 2442–2452, Berlin, Germany.

Miguel Ballesteros, Yoav Goldberg, Chris Dyer, and
Noah A. Smith. 2016. Training with exploration im-
proves a greedy stack LSTM parser. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 2005–2010.

Miguel Ballesteros and Joakim Nivre. 2013. Going
to the roots of dependency parsing. Computational
Linguistics, 39(1):5–13.

Xavier Carreras. 2007. Experiments with a higher-
In Proceed-
order projective dependency parser.
ings of the Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 957–961.

Danqi Chen and Christopher D. Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
740–750, Doha, Qatar.

Hao Cheng, Hao Fang, Xiaodong He, Jianfeng Gao,
and Li Deng. 2016. Bi-directional attention with
agreement for dependency parsing. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 2204–2214.

Do Kook Choe and Eugene Charniak. 2016. Parsing
as language modeling. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 2331–2336, Austin, Texas.

John Cocke. 1969. Programming languages and their
compilers: Preliminary notes. Technical report,
Courant Institute of Mathematical Sciences, New
York University.

James Cross and Liang Huang. 2016a.

Incremental
parsing with minimal features using bi-directional
In Proceedings of the 54th Annual Meet-
LSTM.
ing of the Association for Computational Linguistics
(Volume 2: Short Papers), pages 32–37.

James Cross and Liang Huang. 2016b. Span-based
constituency parsing with a structure-label system
and provably optimal dynamic oracles. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1–11.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
In Proceedings of the 53rd Annual
term memory.
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers), pages 334–343.

Jason Eisner. 1996. Three new probabilistic models for
In Proceed-
dependency parsing: An exploration.
ings of the 16th International Conference on Com-
putational Linguistics, pages 340–345.

Jason Eisner and Giorgio Satta. 1999. Efﬁcient pars-
ing for bilexical context-free grammars and head au-
tomaton grammars. In Proceedings of the 37th An-
nual Meeting of the Association for Computational
Linguistics, pages 457–464.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Advances in Neural Information
Processing Systems, pages 1019–1027.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difﬁculty of training deep feedforward neural
networks. In Proceedings of the 13th International
Conference on Artiﬁcial Intelligence and Statistics,
pages 249–256.

Yoav Goldberg and Joakim Nivre. 2012. A dynamic or-
acle for arc-eager dependency parsing. In Proceed-
ings of the 24th International Conference on Com-
putational Linguistics, pages 959–976.

Yoav Goldberg and Joakim Nivre. 2013. Training de-
terministic parsers with non-deterministic oracles.
Transactions of the Association for Computational
Linguistics, 1:403–414.

Carlos G´omez-Rodr´ıguez, John Carroll, and David
Weir. 2008. A deductive approach to dependency
In Proceedings of the 46th Annual Meet-
parsing.
ing of the Association for Computational Linguis-
tics: Human Language Technology, pages 968–976.

Carlos G´omez-Rodr´ıguez, John Carroll, and David
Weir. 2011. Dependency parsing schemata and
mildly non-projective dependency parsing. Compu-
tational Linguistics, 37(3):541–586.

Alex Graves and J¨urgen Schmidhuber. 2005. Frame-
wise phoneme classiﬁcation with bidirectional
LSTM and other neural network architectures. Neu-
ral Networks, 18(5-6):602–610.

James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Pro-
ceedings of the Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 24–31.

Timothy Dozat and Christopher D. Manning. 2017.
Deep biafﬁne attention for neural dependency pars-
ing. In Proceedings of the 5th International Confer-
ence on Learning Representations.

James Henderson. 2004. Discriminative training of a
In Proceedings
neural network statistical parser.
of the 42nd Annual Meeting of the Association for
Computational Linguistics, pages 95–102.

Liang Huang and Kenji Sagae. 2010. Dynamic pro-
In
gramming for linear-time incremental parsing.
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1077–
1086.

Kenton Lee, Mike Lewis, and Luke Zettlemoyer. 2016.
Global neural CCG parsing with optimality guaran-
tees. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
2366–2376.

Tadao Kasami. 1965.

recognition
and syntax-analysis algorithm for context-free lan-
guages. Technical report, Hawaii University Hon-
olulu Department of Electrical Engineering.

An efﬁcient

Jiangming Liu and Yue Zhang. 2017.

In-order
transition-based constituent parsing. Transactions
of the Association for Computational Linguistics.
To appear.

Diederik Kingma and Jimmy Ba. 2015. Adam: A
In Proceed-
method for stochastic optimization.
ings of the 4th International Conference on Learn-
ing Representations.

Mitchell Marcus, Beatrice Santorini,

and Mary
Ann Marcinkiewicz. 1993. Building a large anno-
tated corpus of English: The Penn Treebank. Com-
putational Linguistics, 19(2):313–330.

Eliyahu Kiperwasser and Yoav Goldberg. 2016a. Sim-
ple and accurate dependency parsing using bidirec-
tional LSTM feature representations. Transactions
of the Association for Computational Linguistics,
4:313–327.

Eliyahu Kiperwasser and Yoav Goldberg. 2016b. Easy-
ﬁrst dependency parsing with hierarchical
tree
LSTMs. Transactions of the Association for Com-
putational Linguistics, 4:445–461.

Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 423–430.

Sandra K¨ubler, Ryan McDonald, and Joakim Nivre.
2009. Dependency parsing, volume 2 of Synthesis
Lectures on Human Language Technologies. Mor-
gan & Claypool Publishers.

Marco Kuhlmann, Carlos G´omez-Rodr´ıguez, and Gior-
gio Satta. 2011. Dynamic programming algorithms
In Pro-
for transition-based dependency parsers.
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 673–682.

Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng
Kong, Chris Dyer, Graham Neubig, and Noah A.
Smith. 2017. What do recurrent neural network
In Proceedings of
grammars learn about syntax?
the 15th Conference of the European Chapter of the
Association for Computational Linguistics: Volume
1, Long Papers, pages 1249–1258, Valencia, Spain.

Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng
Kong, Chris Dyer, and Noah A. Smith. 2016. Dis-
tilling an ensemble of greedy dependency parsers
In Proceedings of the Con-
into one MST parser.
ference on Empirical Methods in Natural Language
Processing, pages 1744–1753.

Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies manual.
Technical report, Stanford University.

Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
In Proceedings of the 11th Conference of
rithms.
the European Chapter of the Association for Com-
putational Linguistics, pages 81–88.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.

Graham Neubig, Chris Dyer, Yoav Goldberg, Austin
Matthews, Waleed Ammar, Antonios Anastasopou-
los, Miguel Ballesteros, David Chiang, Daniel
Clothiaux, Trevor Cohn, Kevin Duh, Manaal
Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji,
Lingpeng Kong, Adhiguna Kuncoro, Gaurav Ku-
mar, Chaitanya Malaviya, Paul Michel, Yusuke
Oda, Matthew Richardson, Naomi Saphra, Swabha
Swayamdipta, and Pengcheng Yin. 2017. DyNet:
The dynamic neural network toolkit. arXiv preprint
arXiv:1701.03980.

Joakim Nivre. 2003. An efﬁcient algorithm for pro-
In Proceedings of the
jective dependency parsing.
8th International Workshop on Parsing Technolo-
gies, pages 149–160.

Joakim Nivre. 2004.

Incrementality in deterministic
In Proceedings of the Work-
dependency parsing.
shop on Incremental Parsing: Bringing Engineering
and Cognition Together, pages 50–57.

Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513–553.

Phong Le and Willem Zuidema. 2014. The inside-
outside recursive neural network model for depen-
dency parsing. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 729–739.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global vectors for
In Proceedings of the Con-
word representation.
ference on Empirical Methods in Natural Language
Processing, pages 1532–1543.

Fernando C. N. Pereira and David H. D. Warren. 1983.
Parsing as deduction. In Proceedings of the 21st An-
nual Meeting on Association for Computational Lin-
guistics, pages 137–144.

Kenji Sagae and Alon Lavie. 2006. A best-ﬁrst prob-
In Proceedings of
abilistic shift-reduce parser.
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 691–698.

Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency
parsing and domain adaptation with LR models and
parser ensembles. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 1044–1050.

Tianze Shi, Felix G. Wu, Xilun Chen, and Yao Cheng.
2017. Combining global models for parsing Uni-
versal Dependencies. In Proceedings of the CoNLL
2017 Shared Task: Multilingual Parsing from Raw
Text to Universal Dependencies, pages 31–39, Van-
couver, Canada.

Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of de-
ductive parsing. The Journal of Logic Program-
ming, 24(1):3–36.

Noah A. Smith. 2011. Linguistic Structure Prediction,
volume 13 of Synthesis Lectures on Human Lan-
guage Technologies. Morgan & Claypool Publish-
ers.

Nitish Srivastava, Geoffrey E. Hinton, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. 2014. Dropout: A simple way to prevent neural
Journal of Machine
networks from overﬁtting.
Learning Research, 15:1929–1958.

Mitchell Stern, Jacob Andreas, and Dan Klein. 2017.
A minimal span-based neural constituent parser. In
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics. To appear.

Ashish Vaswani and Kenji Sagae. 2016. Efﬁcient struc-
tured inference for transition-based parsing with
neural networks and error states. Transactions of the
Association for Computational Linguistics, 4:183–
196.

Wenhui Wang and Baobao Chang. 2016. Graph-based
dependency parsing with bidirectional LSTM.
In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 2306–2315.

David Weiss, Chris Alberti, Michael Collins, and Slav
Petrov. 2015. Structured training for neural net-
In Proceedings of
work transition-based parsing.
the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers), pages 323–333, Beijing,
China.

Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated Chinese cor-
pus. In Proceedings of the 19th International Con-
ference on Computational Linguistics.

Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
In Proceedings of the 8th International
chines.
Workshop on Parsing Technologies, pages 195–206.

Daniel H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189 – 208.

Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
In Pro-
and transition-based dependency parsing.
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 562–571.

Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105–151.

Ben Taskar, Vassil Chatalbashev, Daphne Koller, and
Carlos Guestrin. 2005. Learning structured predic-
tion models: A large margin approach. In Proceed-
ings of the 22nd International Conference on Ma-
chine Learning, pages 896–903.

Kai Zhao, James Cross, and Liang Huang. 2013. Opti-
mal incremental parsing via best-ﬁrst dynamic pro-
In Proceedings of the Conference on
gramming.
Empirical Methods in Natural Language Process-
ing, pages 758–768.

Quang Le Thang, Hiroshi Noji, and Yusuke Miyao.
2015. Optimal shift-reduce constituent parsing with
In Proceedings of the 53rd
structured perceptron.
Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 1534–1544.

Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
173–180.

Fast(er) Exact Decoding and Global Training for Transition-Based
Dependency Parsing via a Minimal Feature Set

Tianze Shi
Cornell University
tianze@cs.cornell.edu

Liang Huang
Oregon State University
liang.huang.sh@gmail.com

Lillian Lee
Cornell University
llee@cs.cornell.edu

Publication venue: Proceedings of EMNLP 2017

Abstract

We ﬁrst present a minimal feature set for
transition-based dependency parsing, con-
tinuing a recent trend started by Kiper-
wasser and Goldberg (2016a) and Cross
and Huang (2016a) of using bi-directional
LSTM features. We plug our minimal
feature set into the dynamic-programming
framework of Huang and Sagae (2010)
and Kuhlmann et al. (2011) to produce the
ﬁrst implementation of worst-case Opn3q
exact decoders for arc-hybrid and arc-
eager transition systems. With our mini-
mal features, we also present Opn3q global
training methods. Finally, using ensem-
bles including our new parsers, we achieve
the best unlabeled attachment score re-
ported (to our knowledge) on the Chinese
Treebank and the “second-best-in-class”
result on the English Penn Treebank.

1

Introduction

It used to be the case that the most accurate de-
pendency parsers made global decisions and em-
ployed exact decoding. But transition-based de-
pendency parsers (TBDPs) have recently achieved
state-of-the-art performance, despite the fact that
for efﬁciency reasons, they are usually trained to
make local, rather than global, decisions and the
decoding process is done approximately, rather
than exactly (Weiss et al., 2015; Dyer et al., 2015;
Andor et al., 2016). The key efﬁciency issue for
decoding is as follows. In order to make accurate
(local) attachment decisions, historically, TBDPs
have required a large set of features in order to ac-
cess rich information about particular positions in
the stack and buffer of the current parser conﬁgu-
ration. But consulting many positions means that
although polynomial-time exact-decoding algo-

rithms do exist, having been introduced by Huang
and Sagae (2010) and Kuhlmann et al. (2011), un-
fortunately, they are prohibitively costly in prac-
tice, since the number of positions considered can
factor into the exponent of the running time. For
instance, Huang and Sagae employ a fairly re-
duced set of nine positions, but the worst-case run-
ning time for the exact-decoding version of their
algorithm is Opn6q (originally reported as Opn7q)
for a length-n sentence. As an extreme case, Dyer
et al. (2015) use an LSTM to summarize arbitrary
information on the stack, which completely rules
out dynamic programming.

Recently, Kiperwasser and Goldberg (2016a)
and Cross and Huang (2016a) applied bi-
long short-term memory networks
directional
(Graves and Schmidhuber, 2005, bi-LSTMs) to
derive feature representations for parsing, because
these networks capture wide-window contextual
information well. Collectively, these two sets of
authors demonstrated that with bi-LSTMs, four
positional features sufﬁce for the arc-hybrid pars-
ing system (K&G), and three sufﬁce for arc-
standard (C&H).1

Inspired by their work, we arrive at a minimal
feature set for arc-hybrid and arc-eager:
it con-
tains only two positional bi-LSTM vectors, suf-
fers almost no loss in performance in comparison
to larger sets, and out-performs a single position.
(Details regarding the situation with arc-standard
can be found in §2.)

Our minimal feature set plugs into Huang and
Sagae’s and Kuhlmann et al.’s dynamic program-

1We note that K&G were not focused on minimizing posi-
tions, although they explicitly noted the implications of doing
so: “While not explored in this work, [fewer positions] re-
sults in very compact state signatures, [which is] very appeal-
ing for use in transition-based parsers that employ dynamic-
programming search” (pg. 319). C&H also noted in their
follow-up (Cross and Huang, 2016b) the possibility of future
work using dynamic programming thanks to simple features.

7
1
0
2
 
g
u
A
 
0
3
 
 
]
L
C
.
s
c
[
 
 
1
v
3
0
4
9
0
.
8
0
7
1
:
v
i
X
r
a

ming framework to produce the ﬁrst implementa-
tion of Opn3q exact decoders for arc-hybrid and
arc-eager parsers. We also enable and implement
Opn3q global training methods. Empirically, en-
sembles containing our minimal-feature, globally-
trained and exactly-decoded models produce the
best unlabeled attachment score (UAS) reported
(to our knowledge) on the Chinese Treebank and
the “second-best-in-class” result on the English
Penn Treebank.2

Additionally, we provide a slight update to
the theoretical connections previously drawn by
G´omez-Rodr´ıguez, Carroll, and Weir (2008, 2011)
between TBDPs and the graph-based dependency
parsing algorithms of Eisner (1996) and Eisner
and Satta (1999), including results regarding the
arc-eager parsing system.

2 A Minimal Feature Set

TBDPs incrementally process a sentence by mak-
ing transitions through search states representing
parser conﬁgurations. Three of the main transition
systems in use today (formal introduction in §3.1)
all maintain the following two data structures in
their conﬁgurations: (1) a stack of partially parsed
subtrees and (2) a buffer (mostly) of unprocessed
sentence tokens.

To featurize conﬁgurations for use in a scoring
function, it is common to have features that extract
information about the ﬁrst several elements on the
stack and the buffer, such as their word forms and
part-of-speech (POS) tags. We refer to these as po-
sitional features, as each feature relates to a partic-
ular position in the stack or buffer. Typically, mil-
lions of sparse indicator features (often developed
via manual engineering) are used.

In contrast, Chen and Manning (2014) intro-
duce a feature set consisting of dense word-, POS-,
and dependency-label embeddings. While dense,
these features are for the same 18 positions that
have been typically used in prior work. Re-
cently, Kiperwasser and Goldberg (2016a) and
Cross and Huang (2016a) adopt bi-directional
LSTMs, which have nice expressiveness and
context-sensitivity properties, to reduce the num-
ber of positions considered down to four and three,

2Our ideas were subsequently adapted to the labeled set-
ting by Shi, Wu, Chen, and Cheng (2017) in their submis-
sion to the CoNLL 2017 shared task on Universal Dependen-
cies parsing. Their team achieved the second-highest labeled
attachment score in general and had the top average perfor-
mance on the surprise languages.

Features Arc-standard Arc-hybrid Arc-eager

t

ÑÐ
ÑÐ
s 1,
s 2,
ÑÐ
s 1,
t
t

ÑÐ
s 0,
ÑÐ
s 0,
ÑÐ
s 0,
t

ÑÐ
b 0u
ÑÐ
b 0u
ÑÐ
b 0u
ÑÐ
b 0u

93.95˘0.12 94.08˘0.13 93.92˘0.04
94.13˘0.06 94.08˘0.05 93.91˘0.07
54.47˘0.36 94.03˘0.12 93.92˘0.07
47.11˘0.44 52.39˘0.23 79.15˘0.06

Min positions Arc-standard Arc-hybrid Arc-eager

K&G 2016a
C&H 2016a
our work

-
3
3

4
-
2

-
-
2

Table 1: Top: English PTB dev-set UAS% for
progressively smaller sets of positional features,
for greedy parsers with different transition sys-
tems. The “double-arrow” notation indicates vec-
tors produced by a bi-directional LSTM. Internal
lines highlight large performance drop-offs when
a feature is deleted. Bottom: sizes of the minimal
feature sets in Kiperwasser and Goldberg (2016a),
Cross and Huang (2016a), and our work.

for different transition systems, respectively.

This naturally begs the question, what is the
lower limit on the number of positional features
necessary for a parser to perform well? Kiper-
wasser and Goldberg (2016a) reason that for the
arc-hybrid system, the ﬁrst and second items on
the stack and the ﬁrst buffer item — denoted by s0,
s1, and b0, respectively — are required; they addi-
tionally include the third stack item, s2, because
it may not be adjacent to the others in the origi-
nal sentence. For arc-standard, Cross and Huang
(2016a) argue for the necessity of s0, s1, and b0.

We address the lower-limit question empiri-
cally, and ﬁnd that, surprisingly, two positions
sufﬁce for the greedy arc-eager and arc-hybrid
parsers. We also provide empirical support for
Cross and Huang’s argument for the necessity of
three features for arc-standard. In the rest of this
section, we explain our experiments, run only on
an English development set, that support this con-
clusion; the results are depicted in Table 1. We
later explore the implementation implications in
§3-4 and then test-set parsing-accuracy in §6.

We employ the same model architecture as
Kiperwasser and Goldberg (2016a). Speciﬁcally,
we ﬁrst use a bi-LSTM to encode an n-token sen-
tence, treated as a sequence of per-token concate-
nations of word- and POS-tag embeddings, into a
ÑÐ
ÑÐ
wi
w1, . . . ,
sequence of vectors r

ÑÐ
wns, where each

ÑÐ

ÑÐ
s j, deﬁned as

is the output of the bi-LSTM at time step i. (The
double-arrow notation for these vectors empha-
sizes the bi-directionality of their origin). Then,
for a given parser conﬁguration, stack positions
wipsj q where
are represented by
ipsjq gives the position in the sentence of the to-
ken that is the head of the tree in sj. Similarly,
ÑÐ
b j, deﬁned as
buffer positions are represented by
ÑÐ
wipbj q for the token at buffer position j. Finally,
as in Chen and Manning (2014), we use a multi-
layer perceptron to score possible transitions from
the given conﬁguration, where the input is the con-
ÑÐ
b ks.
catenation of some selection of the
We use greedy decoders, and train the models with
dynamic oracles (Goldberg and Nivre, 2013).

ÑÐ
s js and

Table 1 reports the parsing accuracy that re-
sults for feature sets of size four, three, two, and
one for three commonly-used transition systems.
The data is the development section of the English
Penn Treebank (PTB), and experimental settings
are as described in our other experimental section,
§6. We see that we can go down to three or, in the
arc-hybrid and arc-eager transition systems, even
two positions with very little loss in performance,
ÑÐ
b 0u our
but not further. We therefore call t
minimal feature set with respect to arc-hybrid and
arc-eager, and empirically conﬁrm that Cross and
ÑÐ
b 0u is minimal for arc-standard;
Huang’s t
see Table 1 for a summary.3

ÑÐ
s 0,

ÑÐ
s 1,

ÑÐ
s 0,

3 Dynamic Programming for TBDPs

As stated in the introduction, our minimal fea-
ture set from §2 plugs into Huang and Sagae and
Kuhlmann et al.’s dynamic programming (DP)
framework. To help explain the connection, this
section provides an overview of the DP frame-
work. We draw heavily from the presentation of
Kuhlmann et al. (2011).

3.1 Three Transition Systems

Transition-based parsing (Nivre, 2008; K¨ubler
et al., 2009) is an incremental parsing framework
based on transitions between parser conﬁgura-

ÑÐ
s 0,

3We tentatively conjecture that the following might ex-
plain the observed phenomena, but stress that we don’t cur-
rently see a concrete way to test the following hypothesis.
ÑÐ
b 0u, in the arc-standard case, situations can arise
With t
where there are multiple possible transitions with missing in-
formation. In contrast, in the arc-hybrid case, there is only
one possible transition with missing information (namely,
ÑÐ
reñ, introduced in §3.1); perhaps
s 1 is therefore not so cru-
cial for arc-hybrid in practice?

tions. For a sentence to be parsed, the system
starts from a corresponding initial conﬁguration,
and attempts to sequentially apply transitions un-
til a conﬁguration corresponding to a full parse is
produced. Formally, a transition system is deﬁned
as S “ pC, T, cs, Cτ q, where C is a nonempty set
of conﬁgurations, each t P T : C á C is a transi-
tion function between conﬁgurations, cs is an ini-
tialization function that maps an input sentence to
an initial conﬁguration, and Cτ Ď C is a set of
terminal conﬁgurations.

All systems we consider share a common tri-
partite representation for conﬁgurations: when we
write c “ pσ, β, Aq for some c P C, we are re-
ferring to a stack σ of partially parsed subtrees; a
buffer β of unprocessed tokens and, optionally, at
its beginning, a subtree with only left descendants;
and a set A of elements ph, mq, each of which is
an attachment (dependency arc) with head h and
modiﬁer m.4 We write mðh to indicate that m
left-modiﬁes h, and hñm to indicate that m right-
modiﬁes h. For a sentence w “ w1, ..., wn, the
initial conﬁguration is pσ0, β0, A0q, where σ0 and
A0 are empty and β0 “ rROOT|w1, ..., wns; ROOT
is a special node denoting the root of the parse
tree5 (vertical bars are a notational convenience
for indicating different parts of the buffer or stack;
our convention is to depict the buffer ﬁrst element
leftmost, and to depict the stack ﬁrst element right-
most). All terminal conﬁgurations have an empty
buffer and a stack containing only ROOT.

Arc-Standard The arc-standard system (Nivre,
2004) is motivated by bottom-up parsing: each de-
pendent has to be complete before being attached.
The three transitions, shift (sh, move a token from
the buffer to the stack), right-reduce (reñ, reduce
and attach a right modiﬁer), and left-reduce (reð,
reduce and attach a left modiﬁer), are deﬁned as:

shrpσ, b0|β, Aqs “ pσ|b0, β, Aq
reñrpσ|s1|s0, β, Aqs “ pσ|s1, β, A Y tps1, s0quq
reðrpσ|s1|s0, β, Aqs “ pσ|s0, β, A Y tps0, s1quq

Arc-Hybrid The arc-hybrid system (Yamada
and Matsumoto, 2003; G´omez-Rodr´ıguez et al.,
2008; Kuhlmann et al., 2011) has the same deﬁ-
nitions of sh and reñ as arc-standard, but forces

4For simplicity, we only present unlabeled parsing here.

See Shi et al. (2017) for labeled-parsing results.

5Other presentations place ROOT at the end of the buffer

or omit it entirely (Ballesteros and Nivre, 2013).

the collection of left modiﬁers before right modi-
ﬁers via its b0-modiﬁer reð transition. This con-
trasts with arc-standard, where the attachment of
left and right modiﬁers can be interleaved on the
stack.

shrpσ, b0|β, Aqs “ pσ|b0, β, Aq
reñrpσ|s1|s0, β, Aqs “ pσ|s1, β, A Y tps1, s0quq
reðrpσ|s0, b0|β, Aqs “ pσ, b0|β, A Y tpb0, s0quq

Arc-Eager
In contrast to the former two sys-
tems, the arc-eager system (Nivre, 2003) makes
attachments as early as possible — even if a modi-
ﬁer has not yet received all of its own modiﬁers.
This behavior is accomplished by decomposing
the right-reduce transition into two independent
transitions, one making the attachment (ra) and
one reducing the right-attached child (re).

shrpσ, b0|β, Aqs “ pσ|b0, β, Aq
reðrpσ|s0, b0|β, Aqs “ pσ, b0|β, A Y tpb0, s0quq
(precondition: s0 not attached to any word)
rarpσ|s0, b0|β, Aqs “ pσ|s0|b0, β, A Y tps0, b0quq
rerpσ|s0, β, Aqs “ pσ, β, Aq

(precondition: s0 has been attached to its head)

3.2 Deduction and Dynamic Programming

Kuhlmann et al. (2011) reformulate the three tran-
sition systems just discussed as deduction systems
(Pereira and Warren, 1983; Shieber et al., 1995),
wherein transitions serve as inference rules; these
are given as the lefthand sides of the ﬁrst three sub-
ﬁgures in Figure 1. For a given w “ w1, ..., wn,
assertions take the form ri, j, ks (or, when applica-
ble, a two-index shorthand to be discussed soon),
meaning that there exists a sequence of transi-
tions that, starting from a conﬁguration wherein
head ps0q “ wi, results in an ending conﬁgura-
tion wherein head ps0q “ wj and head pb0q “ wk.
If we deﬁne w0 as ROOT and wn`1 as an end-
of-sentence marker, then the goal theorem can be
stated as r0, 0, n ` 1s.

For arc-standard, we depict an assertion ri, h, ks
as a subtree whose root (head) is the token at h.
Assertions of the form ri, i, ks play an important
role for arc-hybrid and arc-eager, and we employ
the special shorthand ri, ks for them in Figure 1.
In that ﬁgure, we also graphically depict such sit-
uations as two consecutive half-trees with roots wi
and wk, where all tokens between i and k are al-
ready attached. The superscript b in an arc-eager

assertion rib, js is an indicator variable for whether
wi has been attached to its head (b “ 1) or not
(b “ 0) after the transition sequence is applied.

Kuhlmann et al. (2011) show that all three de-
duction systems can be directly “tabularized” and
dynamic programming (DP) can be applied, such
that, ignoring for the moment the issue of incor-
porating complex features (we return to this later),
time and space needs are low-order polynomial.
Speciﬁcally, as the two-index shorthand ri, js sug-
gests, arc-eager and arc-hybrid systems can be im-
plemented to take Opn2q space and Opn3q time;
the arc-standard system requires Opn3q space and
Opn4q time (if one applies the so-called hook trick
(Eisner and Satta, 1999)).

Since an Opn4q running time is not sufﬁciently
practical even in the simple-feature case, in the re-
mainder of this paper we consider only the arc-
hybrid and arc-eager systems, not arc-standard.

4 Practical Optimal Algorithms Enabled

By Our Minimal Feature Set

Until now, no one had suggested a set of positional
features that was both information-rich enough for
accurate parsing and small enough to obtain the
Opn3q running-time promised above. Fortunately,
ÑÐ
b 0u feature set qualiﬁes,
our bi-LSTM-based t
and enables the fast optimal procedures described
in this section.

ÑÐ
s 0,

4.1 Exact Decoding

Given an input sentence, a TBDP must choose
among a potentially exponential number of cor-
responding transition sequences. We assume ac-
cess to functions ft that score individual conﬁgu-
rations, where these functions are indexed by the
transition functions t P T . For a ﬁxed transition
sequence t “ t1, t2, . . ., we use ci to denote the
conﬁguration that results after applying ti.

Typically, for efﬁciency reasons, greedy left-to-
right decoding is employed: the next transition t˚
i
out of ci´1 is arg maxt ftpci´1q, so that past and
future decisions are not taken into account. The
score F ptq for the transition sequence is induced
by summing the relevant ftipci´1q values.

However, our use of minimal feature sets en-
ables direct computation of an argmax over the en-
tire space of transition sequences, arg maxt F ptq,
via dynamic programming, because our positions
don’t rely on any information “outside” the deduc-
tion rule indices, thus eliminating the need for ad-

h

j

j

j

j ` 1

h1

h2

k

k

h1

h2

0

0

1

i

i

i

i

i

0

0

j

j

j

j

j

j

j

j

Axiom r0, 0, 1s

Inference Rules

sh

ri, h, js
rj, j, j ` 1s

Axiom r0, 1s

Inference Rules

0

1

i

j

j ď n

sh

ri, js
rj, j ` 1s

j ď n

j j `1

reñ

ri, h1, ks

rk, h2, js

ri, h1, js

h1

hñ

1 h2

reñ

rk, is

ri, js

rk, js

reð

ri, h1, ks

rk, h2, js

ri, h2, js

hð

1 h2

h2

reð

rk, is

ri, js

rk, js

k

k

k

k

0

i i

i i

j

j

j

j

kñi

iðj

n ` 1

Goal

r0, 0, n ` 1s

n ` 1

Goal

r0, n ` 1s

(a) Arc-standard

00

1

(b) Arc-hybrid

i

i ` 1

j

j

0 ď i, j ď n

Axioms

Inference Rules

Axiom r00, 1s

Inference Rules

sh

ra

rib, js
rj0, j ` 1s

rib, js
rj1, j ` 1s

reð

rkb, is

ri0, js

rkb, js

re

rkb, is

ri1, js

rkb, js

j

j

i i0

i i1

ib

ib

kb

kb

kb

kb

00

j ď n

right-attach

j0 j `1

iñj
j ď n

j1 j `1

right-reduce

iñj

iðj

left-attach

left-reduce

k k

iðj

i i

k k

i i

j

j

j

j

j

j

j

j

k

k

i

i

k

k

i

i

0

Goal

r00, n ` 1s

Goal

n ` 1

(c) Arc-eager

(d) Edge-factored graph-based parsing.

n ` 1

Figure 1: 1a-1c: Kuhlmann et al.’s inference rules for three transition systems, together with CKY-style
visualizations of the local structures involved and, to their right, conditions for the rule to apply. 1d: the
edge-factored graph-based parsing algorithm (Eisner and Satta, 1999) discussed in §5.

ditional state-keeping.

We show how to integrate the scoring functions
for the arc-eager system; the arc-hybrid system is
handled similarly. The score-annotated rules are
as follows:
rib, js : v
rj0, j ` 1s : 0

rkb, is : v1
ri0, js : v2
rkb, js : v1 ` v2 ` ∆

preðq

pshq

ÑÐ
wi,

ÑÐ
wk,

ÑÐ
wiq ` freðp

ÑÐ
wjq — abus-
where ∆ “ fshp
ing notation by referring to conﬁgurations by their
features. The left-reduce rule says that we can ﬁrst
take the sequence of transitions asserted by rkb, is,
which has a score of v1, and then a shift transition
moving wi from b0 to s0. This means that the ini-
tial condition for ri0, js is met, so we can take the
sequence of transitions asserted by ri0, js — say it
has score v2 — and ﬁnally a left-reduce transition
to ﬁnish composing the larger transition sequence.
Notice that the scores for sh and ra are 0, as the
scoring of these transitions is accounted for by re-
duce rules elsewhere in the sequence.

4.2 Global Training

We employ large-margin training that considers
each transition sequence globally. Formally, for a
training sentence w “ w1, . . . , wn with gold tran-
sition sequence tgold, our loss function is

´

¯
F ptq ` costptgold, tq ´ F ptgoldq

max
t

where costptgold, tq is a custom margin for tak-
ing t instead of tgold — speciﬁcally, the number
of mis-attached nodes. Computing this max can
again be done efﬁciently with a slight modiﬁca-
tion to the scoring of reduce transitions:

rkb, is : v1 ri0, js : v2
rkb, js : v1 ` v2 ` ∆1 preðq

where ∆1 “ ∆ ` 1 phead pwiq ‰ wjq. This loss-
augmented inference or cost-augmented decoding
(Taskar et al., 2005; Smith, 2011) technique has
previously been applied to graph-based parsing by
Kiperwasser and Goldberg (2016a).

Efﬁciency Note The computation decomposes
into two parts: scoring all feature combinations,
and using DP to ﬁnd a proof for the goal theorem
in the deduction system. Time-complexity analy-
sis is usually given in terms of the latter, but the
former might have a large constant factor, such
as 104 or worse for neural-network-based scoring

functions. As a result, in practice, with a small
b 0u (Opn2q)
n, scoring with the feature set t
can be as time-consuming as the decoding steps
(Opn3q) for the arc-hybrid and arc-eager systems.

ÑÐ
s 0,

ÑÐ

5 Theoretical Connections

Our minimal feature set brings implementation of
practical optimal algorithms to TBDPs, whereas
previously only graph-based dependency parsers
(GBDPs) — a radically different, non-incremental
paradigm — enjoyed the ability to deploy them.
Interestingly, for both the transition- and graph-
based paradigms, the optimal algorithms build de-
pendency trees bottom-up from local structures. It
is thus natural to wonder if there are deeper, more
formal connections between the two.

In previous work, Kuhlmann et al. (2011) re-
lated the arc-standard system to the classic CKY
algorithm (Cocke, 1969; Kasami, 1965; Younger,
1967) in a manner clearly suggested by Figure 1a;
CKY can be viewed as a very simple graph-based
approach. G´omez-Rodr´ıguez et al. (2008, 2011)
formally prove that sequences of steps in the edge-
factored GBDP (Eisner, 1996) can be used to em-
ulate any individual step in the arc-hybrid system
(Yamada and Matsumoto, 2003) and the Eisner
and Satta (1999, Figure 1d) version. However,
they did not draw an explicitly direct connection
between Eisner and Satta (1999) and TBDPs.

ÑÐ
h,

Here, we provide an update to these previous
ﬁndings, stated in terms of the expressiveness of
scoring functions, considered as parameterization.
For the edge-factored GBDP, we write the score
ÑÐ
for an edge as fGp
mq, where h is the head and
m the modiﬁer. A tree’s score is the sum of its
edge scores. We say that a parameterized depen-
dency parsing model A contains model B if for ev-
ery instance of parameterization in model B, there
exists an instance of model A such that the two
models assign the same score to every parse tree.
We claim:

Lemma 1. The arc-eager model presented in §4.1
contains the edge-factored model.

Proof Sketch. Consider a given edge-factored
GBDP parameterized by fG. For any parse tree,
every edge iðj involves two deduction rules, and
their contribution to the score of the ﬁnal proof is
ÑÐ
ÑÐ
ÑÐ
ÑÐ
wi) “
wk,
wjq. We set fsh(
wk,
fsh(
ÑÐ
ÑÐ
0 and freðp
wiq. Similarly,
wj,
for edges kñi in the other direction, we set

ÑÐ
wi) ` freðp
ÑÐ
ÑÐ
wjq “ fGp
wi,

ÑÐ
wi,

Model

Training

Features

PTB
UAS (%) UEM (%)

CTB
UAS (%) UEM (%)

Arc-standard

Local

93.95˘0.12 52.29˘0.66

88.01˘0.26 36.87˘0.53

Arc-hybrid

Arc-eager

Local
Local
Global

Local
Local
Global

ÑÐ
s 2,

t

ÑÐ
s 1,

ÑÐ
s 0,

ÑÐ
b 0u

ÑÐ
s 2,

t

ÑÐ
b 0u

ÑÐ
s 2,

t

ÑÐ
b 0u

ÑÐ
s 1,
ÑÐ
s 0,
ÑÐ
s 0,

ÑÐ
s 0,
ÑÐ
b 0u
ÑÐ
b 0u

t

t

ÑÐ
s 1,
ÑÐ
s 0,
ÑÐ
s 0,

ÑÐ
s 0,
ÑÐ
b 0u
ÑÐ
b 0u

t

t

ÑÐ
h ,

t

ÑÐ
mu

93.89˘0.10 50.82˘0.75
93.80˘0.12 49.66˘0.43
94.43˘0.08 53.03˘0.71

87.87˘0.17 35.47˘0.48
87.78˘0.09 35.09˘0.40
88.38˘0.11 36.59˘0.27

93.80˘0.12 49.66˘0.43
93.77˘0.08 49.71˘0.24
94.53˘0.05 53.77˘0.46

87.49˘0.20 33.15˘0.72
87.33˘0.11 34.17˘0.41
88.62˘0.09 37.75˘0.87

Edge-factored

Global

94.50˘0.13 53.86˘0.78

88.25˘0.12 36.42˘0.52

Table 2: Test set performance for different training regimes and feature sets. The models use the same
decoders for testing and training. For each setting, the average and standard deviation across 5 runs with
different random initializations are reported. Boldface: best (averaged) result per dataset/measure.

ÑÐ
wk,

ÑÐ
wi) “ fGp

ÑÐ
fra(
wjq “ 0.
The parameterization we arrive at emulates ex-
actly the scoring model of fG.

ÑÐ
wiq and frep

ÑÐ
wk,

ÑÐ
wi,

We further claim that the arc-eager model is
more expressive than not only the edge-factored
GBDP, but also the arc-hybrid model in our paper.

Lemma 2. The arc-eager model contains the arc-
hybrid model.

Proof Sketch. We leverage the fact that the arc-
eager model divides the sh transition in the arc-
hybrid model into two separate transitions, sh and
ra. When we constrain the parameters fsh “ fra in
the arc-eager model, the model hypothesis space
becomes exactly the same as arc-hybrid’s.

The extra expressiveness of the arc-eager model
comes from the scoring functions fsh and fre
that capture structural contexts other than head-
modiﬁer relations. Unlike traditional higher-order
graph-based parsing that directly models relations
such as siblinghood (McDonald and Pereira, 2006)
or grandparenthood (Carreras, 2007), however, the
arguments in those two functions do not have any
ﬁxed type of structural interactions.

6 Experiments

Data and Evaluation We experimented with
English and Chinese. For English, we used the
Stanford Dependencies (de Marneffe and Man-
ning, 2008) conversion (via the Stanford parser
3.3.0) of the Penn Treebank (Marcus et al., 1993,
PTB). As is standard, we used §2-21 of the Wall
Street Journal for training, §22 for development,

and §23 for testing; POS tags were predicted using
10-way jackkniﬁng with the Stanford max entropy
tagger (Toutanova et al., 2003). For Chinese, we
used the Penn Chinese Treebank 5.1 (Xue et al.,
2002, CTB), with the same splits and head-ﬁnding
rules for conversion to dependencies as Zhang
and Clark (2008). We adopted the CTB’s gold-
standard tokenization and POS tags. We report
unlabeled attachment score (UAS) and sentence-
level unlabeled exact match (UEM). Following
prior work, all punctuation is excluded from eval-
uation. For each model, we initialized the network
parameters with 5 different random seeds and re-
port performance average and standard deviation.

Implementation Details Our model structures
reproduce those of Kiperwasser and Goldberg
(2016a). We use 2-layer bi-directional LSTMs
with 256 hidden cell units. Inputs are concatena-
tions of 28-dimensional randomly-initialized part-
of-speech embeddings and 100-dimensional word
vectors initialized from GloVe vectors (Penning-
ton et al., 2014) (English) and pre-trained skip-
gram-model vectors (Mikolov et al., 2013) (Chi-
nese). The concatenation of the bi-LSTM feature
vectors is passed through a multi-layer perceptron
(MLP) with 1 hidden layer which has 256 hid-
den units and activation function tanh. We set the
dropout rate for the bi-LSTM (Gal and Ghahra-
mani, 2016) and MLP (Srivastava et al., 2014) for
each model according to development-set perfor-
mance.6 All parameters except the word embed-

6For bi-LSTM input and recurrent connections, we con-

sider dropout rates in t0., 0.2u, and for MLP, t0., 0.4u.

Figure 2: Comparing our UAS results with results from the literature. x-axis: PTB; y-axis: CTB. Most
datapoint labels give author initials and publication year; citations are in the bibliography. Ensemble
datapoints are annotated with ensemble size. Weiss et al. (2015) and Andor et al. (2016) achieve UAS of
94.26 and 94.61 on PTB with beam search, but did not report CTB results, and are therefore omitted.

dings are initialized uniformly (Glorot and Ben-
gio, 2010). Approximately 1,000 tokens form a
mini-batch for sub-gradient computation. We train
each model for 20 epochs and perform model se-
lection based on development UAS. The proposed
structured loss function is optimized via Adam
(Kingma and Ba, 2015). The neural network com-
putation is based on the python interface to DyNet
(Neubig et al., 2017), and the exact decoding al-
gorithms are implemented in Cython.7

Main Results We implement exact decoders for
the arc-hybrid and arc-eager systems, and present
the test performance of different model conﬁgu-
rations in Table 2, comparing global models with
local models. All models use the same decoder
for testing as during the training process. Though
no global decoder for the arc-standard system has
been explored in this paper, its local models are
listed for comparison. We also include an edge-
factored graph-based model, which is convention-
ally trained globally. The edge-factored model
scores bi-LSTM features for each head-modiﬁer
pair; a maximum spanning tree algorithm is used
to ﬁnd the tree with the highest sum of edge
scores. For this model, we use Dozat and Man-

7See https://github.com/tzshi/dp-parser-emnlp17 .

ning’s (2017) biafﬁne scoring model, although in
our case the model size is smaller.8

Analogously to the dev-set results given in §2,
on the test data, the minimal feature sets perform
as well as larger ones in locally-trained models.
And there exists a clear trend of global models out-
performing local models for the two different tran-
sition systems on both datasets. This illustrates the
effectiveness of exact decoding and global train-
ing. Of the three types of global models, the arc-
eager arguably has the edge, an empirical ﬁnding
resonating with our theoretical comparison of their
model expressiveness.

Comparison with State-of-the-Art Models
Figure 2 compares our algorithms’ results with
those of the state-of-the-art.9 Our models are
competitive and an ensemble of 15 globally-
trained models (5 models each for arc-eager DP,
arc-hybrid DP and edge-factored) achieves 95.33
and 90.22 on PTB and CTB, respectively, reach-

8The same architecture and model size as other transition-

based global models is used for fair comparison.

9We exclude Choe and Charniak (2016), Kuncoro et al.
(2017) and Liu and Zhang (2017), which convert constituent-
based parses to dependency parses. They produce higher PTB
UAS, but access more training information and do not di-
rectly apply to datasets without constituency annotation.

ing the highest reported UAS on the CTB dataset,
and the second highest reported on the PTB
dataset among dependency-based approaches.

7 Related Work Not Yet Mentioned

Approximate Optimal Decoding/Training Be-
sides dynamic programming (Huang and Sagae,
2010; Kuhlmann et al., 2011), various other ap-
proaches have been proposed for approaching
global training and exact decoding. Best-ﬁrst
and A* search (Klein and Manning, 2003; Sagae
and Lavie, 2006; Sagae and Tsujii, 2007; Zhao
et al., 2013; Thang et al., 2015; Lee et al., 2016)
give optimality certiﬁcates when solutions are
found, but have the same worst-case time com-
plexity as the original search framework. Other
common approaches to search a larger space at
training or test time include beam search (Zhang
and Clark, 2011), dynamic oracles (Goldberg and
Nivre, 2012, 2013; Cross and Huang, 2016b) and
error states (Vaswani and Sagae, 2016). Beam
search records the k best-scoring transition pre-
ﬁxes to delay local hard decisions, while the lat-
ter two leverage conﬁgurations deviating from the
gold transition path during training to better simu-
late the test-time environment.

Neural Parsing Neural-network-based models
are widely used in state-of-the-art dependency
parsers (Henderson, 2003, 2004; Chen and Man-
ning, 2014; Weiss et al., 2015; Andor et al., 2016;
Dozat and Manning, 2017) because of their ex-
pressive representation power. Recently, Stern
et al. (2017) have proposed minimal span-based
features for constituency parsing.

Recurrent and recursive neural networks can be
used to build representations that encode complete
conﬁguration information or the entire parse tree
(Le and Zuidema, 2014; Dyer et al., 2015; Kiper-
wasser and Goldberg, 2016b), but these models
cannot be readily combined with DP approaches,
because their state spaces cannot be merged into
smaller sets and thus remain exponentially large.

8 Concluding Remarks

In this paper, we have shown the following.
ÑÐ
s 0,

• The bi-LSTM-powered feature set t

ÑÐ
b 0u
is minimal yet highly effective for arc-hybrid
and arc-eager transition-based parsing.

• Since DP algorithms for exact decoding
(Huang and Sagae, 2010; Kuhlmann et al.,

2011) have a run-time dependence on the
number of positional features, using our mere
two effective positional features results in a
running time of Opn3q, feasible for practice.

• Combining exact decoding with global train-
ing — which is also enabled by our minimal
feature set — with an ensemble of parsers
achieves 90.22 UAS on the Chinese Treebank
and 95.33 UAS on the Penn Treebank: these
are, to our knowledge, the best and second-
best results to date on these data sets among
“purely” dependency-based approaches.

There are many directions for further explo-
ration. Two possibilities are to create even better
training methods, and to ﬁnd some way to extend
our run-time improvements to other transition sys-
tems.
It would also be interesting to further in-
vestigate relationships between graph-based and
In §5 we have men-
dependency-based parsing.
tioned important earlier work in this regard, and
provided an update to those formal ﬁndings.

In our work, we have brought exact decoding,
which was formerly the province solely of graph-
based parsing, to the transition-based paradigm.
We hope that the future will bring more inspira-
tion from an integration of the two perspectives.

Acknowledgments:
an author-reviewer suc-
cess story We sincerely thank all the reviewers
for their extraordinarily careful and helpful com-
ments. Indeed, this paper originated as a short pa-
per submission by TS&LL to ACL 2017, where
an anonymous reviewer explained in the review
comments how, among other things, the DP run-
time could be improved from Opn4q to Opn3q. In
their author response, TS&LL invited the reviewer
to co-author, suggesting that they ask the confer-
ence organizers to make the connection between
anonymous reviewer and anonymous authors. All
three of us are truly grateful to PC co-chair Regina
Barzilay for implementing this idea, bringing us
together!

We also thank Kai Sun for help with Chi-
nese word vectors, and Xilun Chen, Yao Cheng,
Dezhong Deng, Juneki Hong, Jon Kleinberg,
Ryan McDonald, Ashudeep Singh, and Kai Zhao
for discussions and suggestions. TS and LL were
supported in part by a Google focused research
grant to Cornell University. LH was supported in
part by NSF IIS-1656051, DARPA N66001-17-2-
4030, and a Google Faculty Research Award.

References

Daniel Andor, Chris Alberti, David Weiss, Aliaksei
Severyn, Alessandro Presta, Kuzman Ganchev, Slav
Petrov, and Michael Collins. 2016. Globally nor-
In Pro-
malized transition-based neural networks.
ceedings of the Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 2442–2452, Berlin, Germany.

Miguel Ballesteros, Yoav Goldberg, Chris Dyer, and
Noah A. Smith. 2016. Training with exploration im-
proves a greedy stack LSTM parser. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 2005–2010.

Miguel Ballesteros and Joakim Nivre. 2013. Going
to the roots of dependency parsing. Computational
Linguistics, 39(1):5–13.

Xavier Carreras. 2007. Experiments with a higher-
In Proceed-
order projective dependency parser.
ings of the Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 957–961.

Danqi Chen and Christopher D. Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
740–750, Doha, Qatar.

Hao Cheng, Hao Fang, Xiaodong He, Jianfeng Gao,
and Li Deng. 2016. Bi-directional attention with
agreement for dependency parsing. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 2204–2214.

Do Kook Choe and Eugene Charniak. 2016. Parsing
as language modeling. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 2331–2336, Austin, Texas.

John Cocke. 1969. Programming languages and their
compilers: Preliminary notes. Technical report,
Courant Institute of Mathematical Sciences, New
York University.

James Cross and Liang Huang. 2016a.

Incremental
parsing with minimal features using bi-directional
In Proceedings of the 54th Annual Meet-
LSTM.
ing of the Association for Computational Linguistics
(Volume 2: Short Papers), pages 32–37.

James Cross and Liang Huang. 2016b. Span-based
constituency parsing with a structure-label system
and provably optimal dynamic oracles. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1–11.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
In Proceedings of the 53rd Annual
term memory.
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers), pages 334–343.

Jason Eisner. 1996. Three new probabilistic models for
In Proceed-
dependency parsing: An exploration.
ings of the 16th International Conference on Com-
putational Linguistics, pages 340–345.

Jason Eisner and Giorgio Satta. 1999. Efﬁcient pars-
ing for bilexical context-free grammars and head au-
tomaton grammars. In Proceedings of the 37th An-
nual Meeting of the Association for Computational
Linguistics, pages 457–464.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Advances in Neural Information
Processing Systems, pages 1019–1027.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difﬁculty of training deep feedforward neural
networks. In Proceedings of the 13th International
Conference on Artiﬁcial Intelligence and Statistics,
pages 249–256.

Yoav Goldberg and Joakim Nivre. 2012. A dynamic or-
acle for arc-eager dependency parsing. In Proceed-
ings of the 24th International Conference on Com-
putational Linguistics, pages 959–976.

Yoav Goldberg and Joakim Nivre. 2013. Training de-
terministic parsers with non-deterministic oracles.
Transactions of the Association for Computational
Linguistics, 1:403–414.

Carlos G´omez-Rodr´ıguez, John Carroll, and David
Weir. 2008. A deductive approach to dependency
In Proceedings of the 46th Annual Meet-
parsing.
ing of the Association for Computational Linguis-
tics: Human Language Technology, pages 968–976.

Carlos G´omez-Rodr´ıguez, John Carroll, and David
Weir. 2011. Dependency parsing schemata and
mildly non-projective dependency parsing. Compu-
tational Linguistics, 37(3):541–586.

Alex Graves and J¨urgen Schmidhuber. 2005. Frame-
wise phoneme classiﬁcation with bidirectional
LSTM and other neural network architectures. Neu-
ral Networks, 18(5-6):602–610.

James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Pro-
ceedings of the Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 24–31.

Timothy Dozat and Christopher D. Manning. 2017.
Deep biafﬁne attention for neural dependency pars-
ing. In Proceedings of the 5th International Confer-
ence on Learning Representations.

James Henderson. 2004. Discriminative training of a
In Proceedings
neural network statistical parser.
of the 42nd Annual Meeting of the Association for
Computational Linguistics, pages 95–102.

Liang Huang and Kenji Sagae. 2010. Dynamic pro-
In
gramming for linear-time incremental parsing.
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1077–
1086.

Kenton Lee, Mike Lewis, and Luke Zettlemoyer. 2016.
Global neural CCG parsing with optimality guaran-
tees. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
2366–2376.

Tadao Kasami. 1965.

recognition
and syntax-analysis algorithm for context-free lan-
guages. Technical report, Hawaii University Hon-
olulu Department of Electrical Engineering.

An efﬁcient

Jiangming Liu and Yue Zhang. 2017.

In-order
transition-based constituent parsing. Transactions
of the Association for Computational Linguistics.
To appear.

Diederik Kingma and Jimmy Ba. 2015. Adam: A
In Proceed-
method for stochastic optimization.
ings of the 4th International Conference on Learn-
ing Representations.

Mitchell Marcus, Beatrice Santorini,

and Mary
Ann Marcinkiewicz. 1993. Building a large anno-
tated corpus of English: The Penn Treebank. Com-
putational Linguistics, 19(2):313–330.

Eliyahu Kiperwasser and Yoav Goldberg. 2016a. Sim-
ple and accurate dependency parsing using bidirec-
tional LSTM feature representations. Transactions
of the Association for Computational Linguistics,
4:313–327.

Eliyahu Kiperwasser and Yoav Goldberg. 2016b. Easy-
ﬁrst dependency parsing with hierarchical
tree
LSTMs. Transactions of the Association for Com-
putational Linguistics, 4:445–461.

Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 423–430.

Sandra K¨ubler, Ryan McDonald, and Joakim Nivre.
2009. Dependency parsing, volume 2 of Synthesis
Lectures on Human Language Technologies. Mor-
gan & Claypool Publishers.

Marco Kuhlmann, Carlos G´omez-Rodr´ıguez, and Gior-
gio Satta. 2011. Dynamic programming algorithms
In Pro-
for transition-based dependency parsers.
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 673–682.

Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng
Kong, Chris Dyer, Graham Neubig, and Noah A.
Smith. 2017. What do recurrent neural network
In Proceedings of
grammars learn about syntax?
the 15th Conference of the European Chapter of the
Association for Computational Linguistics: Volume
1, Long Papers, pages 1249–1258, Valencia, Spain.

Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng
Kong, Chris Dyer, and Noah A. Smith. 2016. Dis-
tilling an ensemble of greedy dependency parsers
In Proceedings of the Con-
into one MST parser.
ference on Empirical Methods in Natural Language
Processing, pages 1744–1753.

Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies manual.
Technical report, Stanford University.

Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
In Proceedings of the 11th Conference of
rithms.
the European Chapter of the Association for Com-
putational Linguistics, pages 81–88.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.

Graham Neubig, Chris Dyer, Yoav Goldberg, Austin
Matthews, Waleed Ammar, Antonios Anastasopou-
los, Miguel Ballesteros, David Chiang, Daniel
Clothiaux, Trevor Cohn, Kevin Duh, Manaal
Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji,
Lingpeng Kong, Adhiguna Kuncoro, Gaurav Ku-
mar, Chaitanya Malaviya, Paul Michel, Yusuke
Oda, Matthew Richardson, Naomi Saphra, Swabha
Swayamdipta, and Pengcheng Yin. 2017. DyNet:
The dynamic neural network toolkit. arXiv preprint
arXiv:1701.03980.

Joakim Nivre. 2003. An efﬁcient algorithm for pro-
In Proceedings of the
jective dependency parsing.
8th International Workshop on Parsing Technolo-
gies, pages 149–160.

Joakim Nivre. 2004.

Incrementality in deterministic
In Proceedings of the Work-
dependency parsing.
shop on Incremental Parsing: Bringing Engineering
and Cognition Together, pages 50–57.

Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513–553.

Phong Le and Willem Zuidema. 2014. The inside-
outside recursive neural network model for depen-
dency parsing. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 729–739.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global vectors for
In Proceedings of the Con-
word representation.
ference on Empirical Methods in Natural Language
Processing, pages 1532–1543.

Fernando C. N. Pereira and David H. D. Warren. 1983.
Parsing as deduction. In Proceedings of the 21st An-
nual Meeting on Association for Computational Lin-
guistics, pages 137–144.

Kenji Sagae and Alon Lavie. 2006. A best-ﬁrst prob-
In Proceedings of
abilistic shift-reduce parser.
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 691–698.

Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency
parsing and domain adaptation with LR models and
parser ensembles. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 1044–1050.

Tianze Shi, Felix G. Wu, Xilun Chen, and Yao Cheng.
2017. Combining global models for parsing Uni-
versal Dependencies. In Proceedings of the CoNLL
2017 Shared Task: Multilingual Parsing from Raw
Text to Universal Dependencies, pages 31–39, Van-
couver, Canada.

Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of de-
ductive parsing. The Journal of Logic Program-
ming, 24(1):3–36.

Noah A. Smith. 2011. Linguistic Structure Prediction,
volume 13 of Synthesis Lectures on Human Lan-
guage Technologies. Morgan & Claypool Publish-
ers.

Nitish Srivastava, Geoffrey E. Hinton, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. 2014. Dropout: A simple way to prevent neural
Journal of Machine
networks from overﬁtting.
Learning Research, 15:1929–1958.

Mitchell Stern, Jacob Andreas, and Dan Klein. 2017.
A minimal span-based neural constituent parser. In
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics. To appear.

Ashish Vaswani and Kenji Sagae. 2016. Efﬁcient struc-
tured inference for transition-based parsing with
neural networks and error states. Transactions of the
Association for Computational Linguistics, 4:183–
196.

Wenhui Wang and Baobao Chang. 2016. Graph-based
dependency parsing with bidirectional LSTM.
In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 2306–2315.

David Weiss, Chris Alberti, Michael Collins, and Slav
Petrov. 2015. Structured training for neural net-
In Proceedings of
work transition-based parsing.
the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers), pages 323–333, Beijing,
China.

Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated Chinese cor-
pus. In Proceedings of the 19th International Con-
ference on Computational Linguistics.

Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
In Proceedings of the 8th International
chines.
Workshop on Parsing Technologies, pages 195–206.

Daniel H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189 – 208.

Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
In Pro-
and transition-based dependency parsing.
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 562–571.

Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105–151.

Ben Taskar, Vassil Chatalbashev, Daphne Koller, and
Carlos Guestrin. 2005. Learning structured predic-
tion models: A large margin approach. In Proceed-
ings of the 22nd International Conference on Ma-
chine Learning, pages 896–903.

Kai Zhao, James Cross, and Liang Huang. 2013. Opti-
mal incremental parsing via best-ﬁrst dynamic pro-
In Proceedings of the Conference on
gramming.
Empirical Methods in Natural Language Process-
ing, pages 758–768.

Quang Le Thang, Hiroshi Noji, and Yusuke Miyao.
2015. Optimal shift-reduce constituent parsing with
In Proceedings of the 53rd
structured perceptron.
Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 1534–1544.

Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
173–180.

Fast(er) Exact Decoding and Global Training for Transition-Based
Dependency Parsing via a Minimal Feature Set

Tianze Shi
Cornell University
tianze@cs.cornell.edu

Liang Huang
Oregon State University
liang.huang.sh@gmail.com

Lillian Lee
Cornell University
llee@cs.cornell.edu

Publication venue: Proceedings of EMNLP 2017

Abstract

We ﬁrst present a minimal feature set for
transition-based dependency parsing, con-
tinuing a recent trend started by Kiper-
wasser and Goldberg (2016a) and Cross
and Huang (2016a) of using bi-directional
LSTM features. We plug our minimal
feature set into the dynamic-programming
framework of Huang and Sagae (2010)
and Kuhlmann et al. (2011) to produce the
ﬁrst implementation of worst-case Opn3q
exact decoders for arc-hybrid and arc-
eager transition systems. With our mini-
mal features, we also present Opn3q global
training methods. Finally, using ensem-
bles including our new parsers, we achieve
the best unlabeled attachment score re-
ported (to our knowledge) on the Chinese
Treebank and the “second-best-in-class”
result on the English Penn Treebank.

1

Introduction

It used to be the case that the most accurate de-
pendency parsers made global decisions and em-
ployed exact decoding. But transition-based de-
pendency parsers (TBDPs) have recently achieved
state-of-the-art performance, despite the fact that
for efﬁciency reasons, they are usually trained to
make local, rather than global, decisions and the
decoding process is done approximately, rather
than exactly (Weiss et al., 2015; Dyer et al., 2015;
Andor et al., 2016). The key efﬁciency issue for
decoding is as follows. In order to make accurate
(local) attachment decisions, historically, TBDPs
have required a large set of features in order to ac-
cess rich information about particular positions in
the stack and buffer of the current parser conﬁgu-
ration. But consulting many positions means that
although polynomial-time exact-decoding algo-

rithms do exist, having been introduced by Huang
and Sagae (2010) and Kuhlmann et al. (2011), un-
fortunately, they are prohibitively costly in prac-
tice, since the number of positions considered can
factor into the exponent of the running time. For
instance, Huang and Sagae employ a fairly re-
duced set of nine positions, but the worst-case run-
ning time for the exact-decoding version of their
algorithm is Opn6q (originally reported as Opn7q)
for a length-n sentence. As an extreme case, Dyer
et al. (2015) use an LSTM to summarize arbitrary
information on the stack, which completely rules
out dynamic programming.

Recently, Kiperwasser and Goldberg (2016a)
and Cross and Huang (2016a) applied bi-
long short-term memory networks
directional
(Graves and Schmidhuber, 2005, bi-LSTMs) to
derive feature representations for parsing, because
these networks capture wide-window contextual
information well. Collectively, these two sets of
authors demonstrated that with bi-LSTMs, four
positional features sufﬁce for the arc-hybrid pars-
ing system (K&G), and three sufﬁce for arc-
standard (C&H).1

Inspired by their work, we arrive at a minimal
feature set for arc-hybrid and arc-eager:
it con-
tains only two positional bi-LSTM vectors, suf-
fers almost no loss in performance in comparison
to larger sets, and out-performs a single position.
(Details regarding the situation with arc-standard
can be found in §2.)

Our minimal feature set plugs into Huang and
Sagae’s and Kuhlmann et al.’s dynamic program-

1We note that K&G were not focused on minimizing posi-
tions, although they explicitly noted the implications of doing
so: “While not explored in this work, [fewer positions] re-
sults in very compact state signatures, [which is] very appeal-
ing for use in transition-based parsers that employ dynamic-
programming search” (pg. 319). C&H also noted in their
follow-up (Cross and Huang, 2016b) the possibility of future
work using dynamic programming thanks to simple features.

7
1
0
2
 
g
u
A
 
0
3
 
 
]
L
C
.
s
c
[
 
 
1
v
3
0
4
9
0
.
8
0
7
1
:
v
i
X
r
a

ming framework to produce the ﬁrst implementa-
tion of Opn3q exact decoders for arc-hybrid and
arc-eager parsers. We also enable and implement
Opn3q global training methods. Empirically, en-
sembles containing our minimal-feature, globally-
trained and exactly-decoded models produce the
best unlabeled attachment score (UAS) reported
(to our knowledge) on the Chinese Treebank and
the “second-best-in-class” result on the English
Penn Treebank.2

Additionally, we provide a slight update to
the theoretical connections previously drawn by
G´omez-Rodr´ıguez, Carroll, and Weir (2008, 2011)
between TBDPs and the graph-based dependency
parsing algorithms of Eisner (1996) and Eisner
and Satta (1999), including results regarding the
arc-eager parsing system.

2 A Minimal Feature Set

TBDPs incrementally process a sentence by mak-
ing transitions through search states representing
parser conﬁgurations. Three of the main transition
systems in use today (formal introduction in §3.1)
all maintain the following two data structures in
their conﬁgurations: (1) a stack of partially parsed
subtrees and (2) a buffer (mostly) of unprocessed
sentence tokens.

To featurize conﬁgurations for use in a scoring
function, it is common to have features that extract
information about the ﬁrst several elements on the
stack and the buffer, such as their word forms and
part-of-speech (POS) tags. We refer to these as po-
sitional features, as each feature relates to a partic-
ular position in the stack or buffer. Typically, mil-
lions of sparse indicator features (often developed
via manual engineering) are used.

In contrast, Chen and Manning (2014) intro-
duce a feature set consisting of dense word-, POS-,
and dependency-label embeddings. While dense,
these features are for the same 18 positions that
have been typically used in prior work. Re-
cently, Kiperwasser and Goldberg (2016a) and
Cross and Huang (2016a) adopt bi-directional
LSTMs, which have nice expressiveness and
context-sensitivity properties, to reduce the num-
ber of positions considered down to four and three,

2Our ideas were subsequently adapted to the labeled set-
ting by Shi, Wu, Chen, and Cheng (2017) in their submis-
sion to the CoNLL 2017 shared task on Universal Dependen-
cies parsing. Their team achieved the second-highest labeled
attachment score in general and had the top average perfor-
mance on the surprise languages.

Features Arc-standard Arc-hybrid Arc-eager

t

ÑÐ
ÑÐ
s 1,
s 2,
ÑÐ
s 1,
t
t

ÑÐ
s 0,
ÑÐ
s 0,
ÑÐ
s 0,
t

ÑÐ
b 0u
ÑÐ
b 0u
ÑÐ
b 0u
ÑÐ
b 0u

93.95˘0.12 94.08˘0.13 93.92˘0.04
94.13˘0.06 94.08˘0.05 93.91˘0.07
54.47˘0.36 94.03˘0.12 93.92˘0.07
47.11˘0.44 52.39˘0.23 79.15˘0.06

Min positions Arc-standard Arc-hybrid Arc-eager

K&G 2016a
C&H 2016a
our work

-
3
3

4
-
2

-
-
2

Table 1: Top: English PTB dev-set UAS% for
progressively smaller sets of positional features,
for greedy parsers with different transition sys-
tems. The “double-arrow” notation indicates vec-
tors produced by a bi-directional LSTM. Internal
lines highlight large performance drop-offs when
a feature is deleted. Bottom: sizes of the minimal
feature sets in Kiperwasser and Goldberg (2016a),
Cross and Huang (2016a), and our work.

for different transition systems, respectively.

This naturally begs the question, what is the
lower limit on the number of positional features
necessary for a parser to perform well? Kiper-
wasser and Goldberg (2016a) reason that for the
arc-hybrid system, the ﬁrst and second items on
the stack and the ﬁrst buffer item — denoted by s0,
s1, and b0, respectively — are required; they addi-
tionally include the third stack item, s2, because
it may not be adjacent to the others in the origi-
nal sentence. For arc-standard, Cross and Huang
(2016a) argue for the necessity of s0, s1, and b0.

We address the lower-limit question empiri-
cally, and ﬁnd that, surprisingly, two positions
sufﬁce for the greedy arc-eager and arc-hybrid
parsers. We also provide empirical support for
Cross and Huang’s argument for the necessity of
three features for arc-standard. In the rest of this
section, we explain our experiments, run only on
an English development set, that support this con-
clusion; the results are depicted in Table 1. We
later explore the implementation implications in
§3-4 and then test-set parsing-accuracy in §6.

We employ the same model architecture as
Kiperwasser and Goldberg (2016a). Speciﬁcally,
we ﬁrst use a bi-LSTM to encode an n-token sen-
tence, treated as a sequence of per-token concate-
nations of word- and POS-tag embeddings, into a
ÑÐ
ÑÐ
wi
w1, . . . ,
sequence of vectors r

ÑÐ
wns, where each

ÑÐ

ÑÐ
s j, deﬁned as

is the output of the bi-LSTM at time step i. (The
double-arrow notation for these vectors empha-
sizes the bi-directionality of their origin). Then,
for a given parser conﬁguration, stack positions
wipsj q where
are represented by
ipsjq gives the position in the sentence of the to-
ken that is the head of the tree in sj. Similarly,
ÑÐ
b j, deﬁned as
buffer positions are represented by
ÑÐ
wipbj q for the token at buffer position j. Finally,
as in Chen and Manning (2014), we use a multi-
layer perceptron to score possible transitions from
the given conﬁguration, where the input is the con-
ÑÐ
b ks.
catenation of some selection of the
We use greedy decoders, and train the models with
dynamic oracles (Goldberg and Nivre, 2013).

ÑÐ
s js and

Table 1 reports the parsing accuracy that re-
sults for feature sets of size four, three, two, and
one for three commonly-used transition systems.
The data is the development section of the English
Penn Treebank (PTB), and experimental settings
are as described in our other experimental section,
§6. We see that we can go down to three or, in the
arc-hybrid and arc-eager transition systems, even
two positions with very little loss in performance,
ÑÐ
b 0u our
but not further. We therefore call t
minimal feature set with respect to arc-hybrid and
arc-eager, and empirically conﬁrm that Cross and
ÑÐ
b 0u is minimal for arc-standard;
Huang’s t
see Table 1 for a summary.3

ÑÐ
s 0,

ÑÐ
s 1,

ÑÐ
s 0,

3 Dynamic Programming for TBDPs

As stated in the introduction, our minimal fea-
ture set from §2 plugs into Huang and Sagae and
Kuhlmann et al.’s dynamic programming (DP)
framework. To help explain the connection, this
section provides an overview of the DP frame-
work. We draw heavily from the presentation of
Kuhlmann et al. (2011).

3.1 Three Transition Systems

Transition-based parsing (Nivre, 2008; K¨ubler
et al., 2009) is an incremental parsing framework
based on transitions between parser conﬁgura-

ÑÐ
s 0,

3We tentatively conjecture that the following might ex-
plain the observed phenomena, but stress that we don’t cur-
rently see a concrete way to test the following hypothesis.
ÑÐ
b 0u, in the arc-standard case, situations can arise
With t
where there are multiple possible transitions with missing in-
formation. In contrast, in the arc-hybrid case, there is only
one possible transition with missing information (namely,
ÑÐ
reñ, introduced in §3.1); perhaps
s 1 is therefore not so cru-
cial for arc-hybrid in practice?

tions. For a sentence to be parsed, the system
starts from a corresponding initial conﬁguration,
and attempts to sequentially apply transitions un-
til a conﬁguration corresponding to a full parse is
produced. Formally, a transition system is deﬁned
as S “ pC, T, cs, Cτ q, where C is a nonempty set
of conﬁgurations, each t P T : C á C is a transi-
tion function between conﬁgurations, cs is an ini-
tialization function that maps an input sentence to
an initial conﬁguration, and Cτ Ď C is a set of
terminal conﬁgurations.

All systems we consider share a common tri-
partite representation for conﬁgurations: when we
write c “ pσ, β, Aq for some c P C, we are re-
ferring to a stack σ of partially parsed subtrees; a
buffer β of unprocessed tokens and, optionally, at
its beginning, a subtree with only left descendants;
and a set A of elements ph, mq, each of which is
an attachment (dependency arc) with head h and
modiﬁer m.4 We write mðh to indicate that m
left-modiﬁes h, and hñm to indicate that m right-
modiﬁes h. For a sentence w “ w1, ..., wn, the
initial conﬁguration is pσ0, β0, A0q, where σ0 and
A0 are empty and β0 “ rROOT|w1, ..., wns; ROOT
is a special node denoting the root of the parse
tree5 (vertical bars are a notational convenience
for indicating different parts of the buffer or stack;
our convention is to depict the buffer ﬁrst element
leftmost, and to depict the stack ﬁrst element right-
most). All terminal conﬁgurations have an empty
buffer and a stack containing only ROOT.

Arc-Standard The arc-standard system (Nivre,
2004) is motivated by bottom-up parsing: each de-
pendent has to be complete before being attached.
The three transitions, shift (sh, move a token from
the buffer to the stack), right-reduce (reñ, reduce
and attach a right modiﬁer), and left-reduce (reð,
reduce and attach a left modiﬁer), are deﬁned as:

shrpσ, b0|β, Aqs “ pσ|b0, β, Aq
reñrpσ|s1|s0, β, Aqs “ pσ|s1, β, A Y tps1, s0quq
reðrpσ|s1|s0, β, Aqs “ pσ|s0, β, A Y tps0, s1quq

Arc-Hybrid The arc-hybrid system (Yamada
and Matsumoto, 2003; G´omez-Rodr´ıguez et al.,
2008; Kuhlmann et al., 2011) has the same deﬁ-
nitions of sh and reñ as arc-standard, but forces

4For simplicity, we only present unlabeled parsing here.

See Shi et al. (2017) for labeled-parsing results.

5Other presentations place ROOT at the end of the buffer

or omit it entirely (Ballesteros and Nivre, 2013).

the collection of left modiﬁers before right modi-
ﬁers via its b0-modiﬁer reð transition. This con-
trasts with arc-standard, where the attachment of
left and right modiﬁers can be interleaved on the
stack.

shrpσ, b0|β, Aqs “ pσ|b0, β, Aq
reñrpσ|s1|s0, β, Aqs “ pσ|s1, β, A Y tps1, s0quq
reðrpσ|s0, b0|β, Aqs “ pσ, b0|β, A Y tpb0, s0quq

Arc-Eager
In contrast to the former two sys-
tems, the arc-eager system (Nivre, 2003) makes
attachments as early as possible — even if a modi-
ﬁer has not yet received all of its own modiﬁers.
This behavior is accomplished by decomposing
the right-reduce transition into two independent
transitions, one making the attachment (ra) and
one reducing the right-attached child (re).

shrpσ, b0|β, Aqs “ pσ|b0, β, Aq
reðrpσ|s0, b0|β, Aqs “ pσ, b0|β, A Y tpb0, s0quq
(precondition: s0 not attached to any word)
rarpσ|s0, b0|β, Aqs “ pσ|s0|b0, β, A Y tps0, b0quq
rerpσ|s0, β, Aqs “ pσ, β, Aq

(precondition: s0 has been attached to its head)

3.2 Deduction and Dynamic Programming

Kuhlmann et al. (2011) reformulate the three tran-
sition systems just discussed as deduction systems
(Pereira and Warren, 1983; Shieber et al., 1995),
wherein transitions serve as inference rules; these
are given as the lefthand sides of the ﬁrst three sub-
ﬁgures in Figure 1. For a given w “ w1, ..., wn,
assertions take the form ri, j, ks (or, when applica-
ble, a two-index shorthand to be discussed soon),
meaning that there exists a sequence of transi-
tions that, starting from a conﬁguration wherein
head ps0q “ wi, results in an ending conﬁgura-
tion wherein head ps0q “ wj and head pb0q “ wk.
If we deﬁne w0 as ROOT and wn`1 as an end-
of-sentence marker, then the goal theorem can be
stated as r0, 0, n ` 1s.

For arc-standard, we depict an assertion ri, h, ks
as a subtree whose root (head) is the token at h.
Assertions of the form ri, i, ks play an important
role for arc-hybrid and arc-eager, and we employ
the special shorthand ri, ks for them in Figure 1.
In that ﬁgure, we also graphically depict such sit-
uations as two consecutive half-trees with roots wi
and wk, where all tokens between i and k are al-
ready attached. The superscript b in an arc-eager

assertion rib, js is an indicator variable for whether
wi has been attached to its head (b “ 1) or not
(b “ 0) after the transition sequence is applied.

Kuhlmann et al. (2011) show that all three de-
duction systems can be directly “tabularized” and
dynamic programming (DP) can be applied, such
that, ignoring for the moment the issue of incor-
porating complex features (we return to this later),
time and space needs are low-order polynomial.
Speciﬁcally, as the two-index shorthand ri, js sug-
gests, arc-eager and arc-hybrid systems can be im-
plemented to take Opn2q space and Opn3q time;
the arc-standard system requires Opn3q space and
Opn4q time (if one applies the so-called hook trick
(Eisner and Satta, 1999)).

Since an Opn4q running time is not sufﬁciently
practical even in the simple-feature case, in the re-
mainder of this paper we consider only the arc-
hybrid and arc-eager systems, not arc-standard.

4 Practical Optimal Algorithms Enabled

By Our Minimal Feature Set

Until now, no one had suggested a set of positional
features that was both information-rich enough for
accurate parsing and small enough to obtain the
Opn3q running-time promised above. Fortunately,
ÑÐ
b 0u feature set qualiﬁes,
our bi-LSTM-based t
and enables the fast optimal procedures described
in this section.

ÑÐ
s 0,

4.1 Exact Decoding

Given an input sentence, a TBDP must choose
among a potentially exponential number of cor-
responding transition sequences. We assume ac-
cess to functions ft that score individual conﬁgu-
rations, where these functions are indexed by the
transition functions t P T . For a ﬁxed transition
sequence t “ t1, t2, . . ., we use ci to denote the
conﬁguration that results after applying ti.

Typically, for efﬁciency reasons, greedy left-to-
right decoding is employed: the next transition t˚
i
out of ci´1 is arg maxt ftpci´1q, so that past and
future decisions are not taken into account. The
score F ptq for the transition sequence is induced
by summing the relevant ftipci´1q values.

However, our use of minimal feature sets en-
ables direct computation of an argmax over the en-
tire space of transition sequences, arg maxt F ptq,
via dynamic programming, because our positions
don’t rely on any information “outside” the deduc-
tion rule indices, thus eliminating the need for ad-

h

j

j

j

j ` 1

h1

h2

k

k

h1

h2

0

0

1

i

i

i

i

i

0

0

j

j

j

j

j

j

j

j

Axiom r0, 0, 1s

Inference Rules

sh

ri, h, js
rj, j, j ` 1s

Axiom r0, 1s

Inference Rules

0

1

i

j

j ď n

sh

ri, js
rj, j ` 1s

j ď n

j j `1

reñ

ri, h1, ks

rk, h2, js

ri, h1, js

h1

hñ

1 h2

reñ

rk, is

ri, js

rk, js

reð

ri, h1, ks

rk, h2, js

ri, h2, js

hð

1 h2

h2

reð

rk, is

ri, js

rk, js

k

k

k

k

0

i i

i i

j

j

j

j

kñi

iðj

n ` 1

Goal

r0, 0, n ` 1s

n ` 1

Goal

r0, n ` 1s

(a) Arc-standard

00

1

(b) Arc-hybrid

i

i ` 1

j

j

0 ď i, j ď n

Axioms

Inference Rules

Axiom r00, 1s

Inference Rules

sh

ra

rib, js
rj0, j ` 1s

rib, js
rj1, j ` 1s

reð

rkb, is

ri0, js

rkb, js

re

rkb, is

ri1, js

rkb, js

j

j

i i0

i i1

ib

ib

kb

kb

kb

kb

00

j ď n

right-attach

j0 j `1

iñj
j ď n

j1 j `1

right-reduce

iñj

iðj

left-attach

left-reduce

k k

iðj

i i

k k

i i

j

j

j

j

j

j

j

j

k

k

i

i

k

k

i

i

0

Goal

r00, n ` 1s

Goal

n ` 1

(c) Arc-eager

(d) Edge-factored graph-based parsing.

n ` 1

Figure 1: 1a-1c: Kuhlmann et al.’s inference rules for three transition systems, together with CKY-style
visualizations of the local structures involved and, to their right, conditions for the rule to apply. 1d: the
edge-factored graph-based parsing algorithm (Eisner and Satta, 1999) discussed in §5.

ditional state-keeping.

We show how to integrate the scoring functions
for the arc-eager system; the arc-hybrid system is
handled similarly. The score-annotated rules are
as follows:
rib, js : v
rj0, j ` 1s : 0

rkb, is : v1
ri0, js : v2
rkb, js : v1 ` v2 ` ∆

preðq

pshq

ÑÐ
wi,

ÑÐ
wk,

ÑÐ
wiq ` freðp

ÑÐ
wjq — abus-
where ∆ “ fshp
ing notation by referring to conﬁgurations by their
features. The left-reduce rule says that we can ﬁrst
take the sequence of transitions asserted by rkb, is,
which has a score of v1, and then a shift transition
moving wi from b0 to s0. This means that the ini-
tial condition for ri0, js is met, so we can take the
sequence of transitions asserted by ri0, js — say it
has score v2 — and ﬁnally a left-reduce transition
to ﬁnish composing the larger transition sequence.
Notice that the scores for sh and ra are 0, as the
scoring of these transitions is accounted for by re-
duce rules elsewhere in the sequence.

4.2 Global Training

We employ large-margin training that considers
each transition sequence globally. Formally, for a
training sentence w “ w1, . . . , wn with gold tran-
sition sequence tgold, our loss function is

´

¯
F ptq ` costptgold, tq ´ F ptgoldq

max
t

where costptgold, tq is a custom margin for tak-
ing t instead of tgold — speciﬁcally, the number
of mis-attached nodes. Computing this max can
again be done efﬁciently with a slight modiﬁca-
tion to the scoring of reduce transitions:

rkb, is : v1 ri0, js : v2
rkb, js : v1 ` v2 ` ∆1 preðq

where ∆1 “ ∆ ` 1 phead pwiq ‰ wjq. This loss-
augmented inference or cost-augmented decoding
(Taskar et al., 2005; Smith, 2011) technique has
previously been applied to graph-based parsing by
Kiperwasser and Goldberg (2016a).

Efﬁciency Note The computation decomposes
into two parts: scoring all feature combinations,
and using DP to ﬁnd a proof for the goal theorem
in the deduction system. Time-complexity analy-
sis is usually given in terms of the latter, but the
former might have a large constant factor, such
as 104 or worse for neural-network-based scoring

functions. As a result, in practice, with a small
b 0u (Opn2q)
n, scoring with the feature set t
can be as time-consuming as the decoding steps
(Opn3q) for the arc-hybrid and arc-eager systems.

ÑÐ
s 0,

ÑÐ

5 Theoretical Connections

Our minimal feature set brings implementation of
practical optimal algorithms to TBDPs, whereas
previously only graph-based dependency parsers
(GBDPs) — a radically different, non-incremental
paradigm — enjoyed the ability to deploy them.
Interestingly, for both the transition- and graph-
based paradigms, the optimal algorithms build de-
pendency trees bottom-up from local structures. It
is thus natural to wonder if there are deeper, more
formal connections between the two.

In previous work, Kuhlmann et al. (2011) re-
lated the arc-standard system to the classic CKY
algorithm (Cocke, 1969; Kasami, 1965; Younger,
1967) in a manner clearly suggested by Figure 1a;
CKY can be viewed as a very simple graph-based
approach. G´omez-Rodr´ıguez et al. (2008, 2011)
formally prove that sequences of steps in the edge-
factored GBDP (Eisner, 1996) can be used to em-
ulate any individual step in the arc-hybrid system
(Yamada and Matsumoto, 2003) and the Eisner
and Satta (1999, Figure 1d) version. However,
they did not draw an explicitly direct connection
between Eisner and Satta (1999) and TBDPs.

ÑÐ
h,

Here, we provide an update to these previous
ﬁndings, stated in terms of the expressiveness of
scoring functions, considered as parameterization.
For the edge-factored GBDP, we write the score
ÑÐ
for an edge as fGp
mq, where h is the head and
m the modiﬁer. A tree’s score is the sum of its
edge scores. We say that a parameterized depen-
dency parsing model A contains model B if for ev-
ery instance of parameterization in model B, there
exists an instance of model A such that the two
models assign the same score to every parse tree.
We claim:

Lemma 1. The arc-eager model presented in §4.1
contains the edge-factored model.

Proof Sketch. Consider a given edge-factored
GBDP parameterized by fG. For any parse tree,
every edge iðj involves two deduction rules, and
their contribution to the score of the ﬁnal proof is
ÑÐ
ÑÐ
ÑÐ
ÑÐ
wi) “
wk,
wjq. We set fsh(
wk,
fsh(
ÑÐ
ÑÐ
0 and freðp
wiq. Similarly,
wj,
for edges kñi in the other direction, we set

ÑÐ
wi) ` freðp
ÑÐ
ÑÐ
wjq “ fGp
wi,

ÑÐ
wi,

Model

Training

Features

PTB
UAS (%) UEM (%)

CTB
UAS (%) UEM (%)

Arc-standard

Local

93.95˘0.12 52.29˘0.66

88.01˘0.26 36.87˘0.53

Arc-hybrid

Arc-eager

Local
Local
Global

Local
Local
Global

ÑÐ
s 2,

t

ÑÐ
s 1,

ÑÐ
s 0,

ÑÐ
b 0u

ÑÐ
s 2,

t

ÑÐ
b 0u

ÑÐ
s 2,

t

ÑÐ
b 0u

ÑÐ
s 1,
ÑÐ
s 0,
ÑÐ
s 0,

ÑÐ
s 0,
ÑÐ
b 0u
ÑÐ
b 0u

t

t

ÑÐ
s 1,
ÑÐ
s 0,
ÑÐ
s 0,

ÑÐ
s 0,
ÑÐ
b 0u
ÑÐ
b 0u

t

t

ÑÐ
h ,

t

ÑÐ
mu

93.89˘0.10 50.82˘0.75
93.80˘0.12 49.66˘0.43
94.43˘0.08 53.03˘0.71

87.87˘0.17 35.47˘0.48
87.78˘0.09 35.09˘0.40
88.38˘0.11 36.59˘0.27

93.80˘0.12 49.66˘0.43
93.77˘0.08 49.71˘0.24
94.53˘0.05 53.77˘0.46

87.49˘0.20 33.15˘0.72
87.33˘0.11 34.17˘0.41
88.62˘0.09 37.75˘0.87

Edge-factored

Global

94.50˘0.13 53.86˘0.78

88.25˘0.12 36.42˘0.52

Table 2: Test set performance for different training regimes and feature sets. The models use the same
decoders for testing and training. For each setting, the average and standard deviation across 5 runs with
different random initializations are reported. Boldface: best (averaged) result per dataset/measure.

ÑÐ
wk,

ÑÐ
wi) “ fGp

ÑÐ
fra(
wjq “ 0.
The parameterization we arrive at emulates ex-
actly the scoring model of fG.

ÑÐ
wiq and frep

ÑÐ
wk,

ÑÐ
wi,

We further claim that the arc-eager model is
more expressive than not only the edge-factored
GBDP, but also the arc-hybrid model in our paper.

Lemma 2. The arc-eager model contains the arc-
hybrid model.

Proof Sketch. We leverage the fact that the arc-
eager model divides the sh transition in the arc-
hybrid model into two separate transitions, sh and
ra. When we constrain the parameters fsh “ fra in
the arc-eager model, the model hypothesis space
becomes exactly the same as arc-hybrid’s.

The extra expressiveness of the arc-eager model
comes from the scoring functions fsh and fre
that capture structural contexts other than head-
modiﬁer relations. Unlike traditional higher-order
graph-based parsing that directly models relations
such as siblinghood (McDonald and Pereira, 2006)
or grandparenthood (Carreras, 2007), however, the
arguments in those two functions do not have any
ﬁxed type of structural interactions.

6 Experiments

Data and Evaluation We experimented with
English and Chinese. For English, we used the
Stanford Dependencies (de Marneffe and Man-
ning, 2008) conversion (via the Stanford parser
3.3.0) of the Penn Treebank (Marcus et al., 1993,
PTB). As is standard, we used §2-21 of the Wall
Street Journal for training, §22 for development,

and §23 for testing; POS tags were predicted using
10-way jackkniﬁng with the Stanford max entropy
tagger (Toutanova et al., 2003). For Chinese, we
used the Penn Chinese Treebank 5.1 (Xue et al.,
2002, CTB), with the same splits and head-ﬁnding
rules for conversion to dependencies as Zhang
and Clark (2008). We adopted the CTB’s gold-
standard tokenization and POS tags. We report
unlabeled attachment score (UAS) and sentence-
level unlabeled exact match (UEM). Following
prior work, all punctuation is excluded from eval-
uation. For each model, we initialized the network
parameters with 5 different random seeds and re-
port performance average and standard deviation.

Implementation Details Our model structures
reproduce those of Kiperwasser and Goldberg
(2016a). We use 2-layer bi-directional LSTMs
with 256 hidden cell units. Inputs are concatena-
tions of 28-dimensional randomly-initialized part-
of-speech embeddings and 100-dimensional word
vectors initialized from GloVe vectors (Penning-
ton et al., 2014) (English) and pre-trained skip-
gram-model vectors (Mikolov et al., 2013) (Chi-
nese). The concatenation of the bi-LSTM feature
vectors is passed through a multi-layer perceptron
(MLP) with 1 hidden layer which has 256 hid-
den units and activation function tanh. We set the
dropout rate for the bi-LSTM (Gal and Ghahra-
mani, 2016) and MLP (Srivastava et al., 2014) for
each model according to development-set perfor-
mance.6 All parameters except the word embed-

6For bi-LSTM input and recurrent connections, we con-

sider dropout rates in t0., 0.2u, and for MLP, t0., 0.4u.

Figure 2: Comparing our UAS results with results from the literature. x-axis: PTB; y-axis: CTB. Most
datapoint labels give author initials and publication year; citations are in the bibliography. Ensemble
datapoints are annotated with ensemble size. Weiss et al. (2015) and Andor et al. (2016) achieve UAS of
94.26 and 94.61 on PTB with beam search, but did not report CTB results, and are therefore omitted.

dings are initialized uniformly (Glorot and Ben-
gio, 2010). Approximately 1,000 tokens form a
mini-batch for sub-gradient computation. We train
each model for 20 epochs and perform model se-
lection based on development UAS. The proposed
structured loss function is optimized via Adam
(Kingma and Ba, 2015). The neural network com-
putation is based on the python interface to DyNet
(Neubig et al., 2017), and the exact decoding al-
gorithms are implemented in Cython.7

Main Results We implement exact decoders for
the arc-hybrid and arc-eager systems, and present
the test performance of different model conﬁgu-
rations in Table 2, comparing global models with
local models. All models use the same decoder
for testing as during the training process. Though
no global decoder for the arc-standard system has
been explored in this paper, its local models are
listed for comparison. We also include an edge-
factored graph-based model, which is convention-
ally trained globally. The edge-factored model
scores bi-LSTM features for each head-modiﬁer
pair; a maximum spanning tree algorithm is used
to ﬁnd the tree with the highest sum of edge
scores. For this model, we use Dozat and Man-

7See https://github.com/tzshi/dp-parser-emnlp17 .

ning’s (2017) biafﬁne scoring model, although in
our case the model size is smaller.8

Analogously to the dev-set results given in §2,
on the test data, the minimal feature sets perform
as well as larger ones in locally-trained models.
And there exists a clear trend of global models out-
performing local models for the two different tran-
sition systems on both datasets. This illustrates the
effectiveness of exact decoding and global train-
ing. Of the three types of global models, the arc-
eager arguably has the edge, an empirical ﬁnding
resonating with our theoretical comparison of their
model expressiveness.

Comparison with State-of-the-Art Models
Figure 2 compares our algorithms’ results with
those of the state-of-the-art.9 Our models are
competitive and an ensemble of 15 globally-
trained models (5 models each for arc-eager DP,
arc-hybrid DP and edge-factored) achieves 95.33
and 90.22 on PTB and CTB, respectively, reach-

8The same architecture and model size as other transition-

based global models is used for fair comparison.

9We exclude Choe and Charniak (2016), Kuncoro et al.
(2017) and Liu and Zhang (2017), which convert constituent-
based parses to dependency parses. They produce higher PTB
UAS, but access more training information and do not di-
rectly apply to datasets without constituency annotation.

ing the highest reported UAS on the CTB dataset,
and the second highest reported on the PTB
dataset among dependency-based approaches.

7 Related Work Not Yet Mentioned

Approximate Optimal Decoding/Training Be-
sides dynamic programming (Huang and Sagae,
2010; Kuhlmann et al., 2011), various other ap-
proaches have been proposed for approaching
global training and exact decoding. Best-ﬁrst
and A* search (Klein and Manning, 2003; Sagae
and Lavie, 2006; Sagae and Tsujii, 2007; Zhao
et al., 2013; Thang et al., 2015; Lee et al., 2016)
give optimality certiﬁcates when solutions are
found, but have the same worst-case time com-
plexity as the original search framework. Other
common approaches to search a larger space at
training or test time include beam search (Zhang
and Clark, 2011), dynamic oracles (Goldberg and
Nivre, 2012, 2013; Cross and Huang, 2016b) and
error states (Vaswani and Sagae, 2016). Beam
search records the k best-scoring transition pre-
ﬁxes to delay local hard decisions, while the lat-
ter two leverage conﬁgurations deviating from the
gold transition path during training to better simu-
late the test-time environment.

Neural Parsing Neural-network-based models
are widely used in state-of-the-art dependency
parsers (Henderson, 2003, 2004; Chen and Man-
ning, 2014; Weiss et al., 2015; Andor et al., 2016;
Dozat and Manning, 2017) because of their ex-
pressive representation power. Recently, Stern
et al. (2017) have proposed minimal span-based
features for constituency parsing.

Recurrent and recursive neural networks can be
used to build representations that encode complete
conﬁguration information or the entire parse tree
(Le and Zuidema, 2014; Dyer et al., 2015; Kiper-
wasser and Goldberg, 2016b), but these models
cannot be readily combined with DP approaches,
because their state spaces cannot be merged into
smaller sets and thus remain exponentially large.

8 Concluding Remarks

In this paper, we have shown the following.
ÑÐ
s 0,

• The bi-LSTM-powered feature set t

ÑÐ
b 0u
is minimal yet highly effective for arc-hybrid
and arc-eager transition-based parsing.

• Since DP algorithms for exact decoding
(Huang and Sagae, 2010; Kuhlmann et al.,

2011) have a run-time dependence on the
number of positional features, using our mere
two effective positional features results in a
running time of Opn3q, feasible for practice.

• Combining exact decoding with global train-
ing — which is also enabled by our minimal
feature set — with an ensemble of parsers
achieves 90.22 UAS on the Chinese Treebank
and 95.33 UAS on the Penn Treebank: these
are, to our knowledge, the best and second-
best results to date on these data sets among
“purely” dependency-based approaches.

There are many directions for further explo-
ration. Two possibilities are to create even better
training methods, and to ﬁnd some way to extend
our run-time improvements to other transition sys-
tems.
It would also be interesting to further in-
vestigate relationships between graph-based and
In §5 we have men-
dependency-based parsing.
tioned important earlier work in this regard, and
provided an update to those formal ﬁndings.

In our work, we have brought exact decoding,
which was formerly the province solely of graph-
based parsing, to the transition-based paradigm.
We hope that the future will bring more inspira-
tion from an integration of the two perspectives.

Acknowledgments:
an author-reviewer suc-
cess story We sincerely thank all the reviewers
for their extraordinarily careful and helpful com-
ments. Indeed, this paper originated as a short pa-
per submission by TS&LL to ACL 2017, where
an anonymous reviewer explained in the review
comments how, among other things, the DP run-
time could be improved from Opn4q to Opn3q. In
their author response, TS&LL invited the reviewer
to co-author, suggesting that they ask the confer-
ence organizers to make the connection between
anonymous reviewer and anonymous authors. All
three of us are truly grateful to PC co-chair Regina
Barzilay for implementing this idea, bringing us
together!

We also thank Kai Sun for help with Chi-
nese word vectors, and Xilun Chen, Yao Cheng,
Dezhong Deng, Juneki Hong, Jon Kleinberg,
Ryan McDonald, Ashudeep Singh, and Kai Zhao
for discussions and suggestions. TS and LL were
supported in part by a Google focused research
grant to Cornell University. LH was supported in
part by NSF IIS-1656051, DARPA N66001-17-2-
4030, and a Google Faculty Research Award.

References

Daniel Andor, Chris Alberti, David Weiss, Aliaksei
Severyn, Alessandro Presta, Kuzman Ganchev, Slav
Petrov, and Michael Collins. 2016. Globally nor-
In Pro-
malized transition-based neural networks.
ceedings of the Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 2442–2452, Berlin, Germany.

Miguel Ballesteros, Yoav Goldberg, Chris Dyer, and
Noah A. Smith. 2016. Training with exploration im-
proves a greedy stack LSTM parser. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 2005–2010.

Miguel Ballesteros and Joakim Nivre. 2013. Going
to the roots of dependency parsing. Computational
Linguistics, 39(1):5–13.

Xavier Carreras. 2007. Experiments with a higher-
In Proceed-
order projective dependency parser.
ings of the Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 957–961.

Danqi Chen and Christopher D. Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
740–750, Doha, Qatar.

Hao Cheng, Hao Fang, Xiaodong He, Jianfeng Gao,
and Li Deng. 2016. Bi-directional attention with
agreement for dependency parsing. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 2204–2214.

Do Kook Choe and Eugene Charniak. 2016. Parsing
as language modeling. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 2331–2336, Austin, Texas.

John Cocke. 1969. Programming languages and their
compilers: Preliminary notes. Technical report,
Courant Institute of Mathematical Sciences, New
York University.

James Cross and Liang Huang. 2016a.

Incremental
parsing with minimal features using bi-directional
In Proceedings of the 54th Annual Meet-
LSTM.
ing of the Association for Computational Linguistics
(Volume 2: Short Papers), pages 32–37.

James Cross and Liang Huang. 2016b. Span-based
constituency parsing with a structure-label system
and provably optimal dynamic oracles. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1–11.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
In Proceedings of the 53rd Annual
term memory.
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers), pages 334–343.

Jason Eisner. 1996. Three new probabilistic models for
In Proceed-
dependency parsing: An exploration.
ings of the 16th International Conference on Com-
putational Linguistics, pages 340–345.

Jason Eisner and Giorgio Satta. 1999. Efﬁcient pars-
ing for bilexical context-free grammars and head au-
tomaton grammars. In Proceedings of the 37th An-
nual Meeting of the Association for Computational
Linguistics, pages 457–464.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Advances in Neural Information
Processing Systems, pages 1019–1027.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difﬁculty of training deep feedforward neural
networks. In Proceedings of the 13th International
Conference on Artiﬁcial Intelligence and Statistics,
pages 249–256.

Yoav Goldberg and Joakim Nivre. 2012. A dynamic or-
acle for arc-eager dependency parsing. In Proceed-
ings of the 24th International Conference on Com-
putational Linguistics, pages 959–976.

Yoav Goldberg and Joakim Nivre. 2013. Training de-
terministic parsers with non-deterministic oracles.
Transactions of the Association for Computational
Linguistics, 1:403–414.

Carlos G´omez-Rodr´ıguez, John Carroll, and David
Weir. 2008. A deductive approach to dependency
In Proceedings of the 46th Annual Meet-
parsing.
ing of the Association for Computational Linguis-
tics: Human Language Technology, pages 968–976.

Carlos G´omez-Rodr´ıguez, John Carroll, and David
Weir. 2011. Dependency parsing schemata and
mildly non-projective dependency parsing. Compu-
tational Linguistics, 37(3):541–586.

Alex Graves and J¨urgen Schmidhuber. 2005. Frame-
wise phoneme classiﬁcation with bidirectional
LSTM and other neural network architectures. Neu-
ral Networks, 18(5-6):602–610.

James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Pro-
ceedings of the Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 24–31.

Timothy Dozat and Christopher D. Manning. 2017.
Deep biafﬁne attention for neural dependency pars-
ing. In Proceedings of the 5th International Confer-
ence on Learning Representations.

James Henderson. 2004. Discriminative training of a
In Proceedings
neural network statistical parser.
of the 42nd Annual Meeting of the Association for
Computational Linguistics, pages 95–102.

Liang Huang and Kenji Sagae. 2010. Dynamic pro-
In
gramming for linear-time incremental parsing.
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1077–
1086.

Kenton Lee, Mike Lewis, and Luke Zettlemoyer. 2016.
Global neural CCG parsing with optimality guaran-
tees. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
2366–2376.

Tadao Kasami. 1965.

recognition
and syntax-analysis algorithm for context-free lan-
guages. Technical report, Hawaii University Hon-
olulu Department of Electrical Engineering.

An efﬁcient

Jiangming Liu and Yue Zhang. 2017.

In-order
transition-based constituent parsing. Transactions
of the Association for Computational Linguistics.
To appear.

Diederik Kingma and Jimmy Ba. 2015. Adam: A
In Proceed-
method for stochastic optimization.
ings of the 4th International Conference on Learn-
ing Representations.

Mitchell Marcus, Beatrice Santorini,

and Mary
Ann Marcinkiewicz. 1993. Building a large anno-
tated corpus of English: The Penn Treebank. Com-
putational Linguistics, 19(2):313–330.

Eliyahu Kiperwasser and Yoav Goldberg. 2016a. Sim-
ple and accurate dependency parsing using bidirec-
tional LSTM feature representations. Transactions
of the Association for Computational Linguistics,
4:313–327.

Eliyahu Kiperwasser and Yoav Goldberg. 2016b. Easy-
ﬁrst dependency parsing with hierarchical
tree
LSTMs. Transactions of the Association for Com-
putational Linguistics, 4:445–461.

Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 423–430.

Sandra K¨ubler, Ryan McDonald, and Joakim Nivre.
2009. Dependency parsing, volume 2 of Synthesis
Lectures on Human Language Technologies. Mor-
gan & Claypool Publishers.

Marco Kuhlmann, Carlos G´omez-Rodr´ıguez, and Gior-
gio Satta. 2011. Dynamic programming algorithms
In Pro-
for transition-based dependency parsers.
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 673–682.

Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng
Kong, Chris Dyer, Graham Neubig, and Noah A.
Smith. 2017. What do recurrent neural network
In Proceedings of
grammars learn about syntax?
the 15th Conference of the European Chapter of the
Association for Computational Linguistics: Volume
1, Long Papers, pages 1249–1258, Valencia, Spain.

Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng
Kong, Chris Dyer, and Noah A. Smith. 2016. Dis-
tilling an ensemble of greedy dependency parsers
In Proceedings of the Con-
into one MST parser.
ference on Empirical Methods in Natural Language
Processing, pages 1744–1753.

Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies manual.
Technical report, Stanford University.

Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
In Proceedings of the 11th Conference of
rithms.
the European Chapter of the Association for Com-
putational Linguistics, pages 81–88.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.

Graham Neubig, Chris Dyer, Yoav Goldberg, Austin
Matthews, Waleed Ammar, Antonios Anastasopou-
los, Miguel Ballesteros, David Chiang, Daniel
Clothiaux, Trevor Cohn, Kevin Duh, Manaal
Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji,
Lingpeng Kong, Adhiguna Kuncoro, Gaurav Ku-
mar, Chaitanya Malaviya, Paul Michel, Yusuke
Oda, Matthew Richardson, Naomi Saphra, Swabha
Swayamdipta, and Pengcheng Yin. 2017. DyNet:
The dynamic neural network toolkit. arXiv preprint
arXiv:1701.03980.

Joakim Nivre. 2003. An efﬁcient algorithm for pro-
In Proceedings of the
jective dependency parsing.
8th International Workshop on Parsing Technolo-
gies, pages 149–160.

Joakim Nivre. 2004.

Incrementality in deterministic
In Proceedings of the Work-
dependency parsing.
shop on Incremental Parsing: Bringing Engineering
and Cognition Together, pages 50–57.

Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513–553.

Phong Le and Willem Zuidema. 2014. The inside-
outside recursive neural network model for depen-
dency parsing. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 729–739.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global vectors for
In Proceedings of the Con-
word representation.
ference on Empirical Methods in Natural Language
Processing, pages 1532–1543.

Fernando C. N. Pereira and David H. D. Warren. 1983.
Parsing as deduction. In Proceedings of the 21st An-
nual Meeting on Association for Computational Lin-
guistics, pages 137–144.

Kenji Sagae and Alon Lavie. 2006. A best-ﬁrst prob-
In Proceedings of
abilistic shift-reduce parser.
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 691–698.

Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency
parsing and domain adaptation with LR models and
parser ensembles. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 1044–1050.

Tianze Shi, Felix G. Wu, Xilun Chen, and Yao Cheng.
2017. Combining global models for parsing Uni-
versal Dependencies. In Proceedings of the CoNLL
2017 Shared Task: Multilingual Parsing from Raw
Text to Universal Dependencies, pages 31–39, Van-
couver, Canada.

Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of de-
ductive parsing. The Journal of Logic Program-
ming, 24(1):3–36.

Noah A. Smith. 2011. Linguistic Structure Prediction,
volume 13 of Synthesis Lectures on Human Lan-
guage Technologies. Morgan & Claypool Publish-
ers.

Nitish Srivastava, Geoffrey E. Hinton, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. 2014. Dropout: A simple way to prevent neural
Journal of Machine
networks from overﬁtting.
Learning Research, 15:1929–1958.

Mitchell Stern, Jacob Andreas, and Dan Klein. 2017.
A minimal span-based neural constituent parser. In
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics. To appear.

Ashish Vaswani and Kenji Sagae. 2016. Efﬁcient struc-
tured inference for transition-based parsing with
neural networks and error states. Transactions of the
Association for Computational Linguistics, 4:183–
196.

Wenhui Wang and Baobao Chang. 2016. Graph-based
dependency parsing with bidirectional LSTM.
In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 2306–2315.

David Weiss, Chris Alberti, Michael Collins, and Slav
Petrov. 2015. Structured training for neural net-
In Proceedings of
work transition-based parsing.
the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers), pages 323–333, Beijing,
China.

Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated Chinese cor-
pus. In Proceedings of the 19th International Con-
ference on Computational Linguistics.

Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
In Proceedings of the 8th International
chines.
Workshop on Parsing Technologies, pages 195–206.

Daniel H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189 – 208.

Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
In Pro-
and transition-based dependency parsing.
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 562–571.

Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105–151.

Ben Taskar, Vassil Chatalbashev, Daphne Koller, and
Carlos Guestrin. 2005. Learning structured predic-
tion models: A large margin approach. In Proceed-
ings of the 22nd International Conference on Ma-
chine Learning, pages 896–903.

Kai Zhao, James Cross, and Liang Huang. 2013. Opti-
mal incremental parsing via best-ﬁrst dynamic pro-
In Proceedings of the Conference on
gramming.
Empirical Methods in Natural Language Process-
ing, pages 758–768.

Quang Le Thang, Hiroshi Noji, and Yusuke Miyao.
2015. Optimal shift-reduce constituent parsing with
In Proceedings of the 53rd
structured perceptron.
Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 1534–1544.

Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
173–180.

