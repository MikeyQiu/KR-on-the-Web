6
1
0
2
 
c
e
D
 
4
2
 
 
]

V
C
.
s
c
[
 
 
1
v
0
7
1
8
0
.
2
1
6
1
:
v
i
X
r
a

Joint denoising and distortion correction of atomic scale scanning
transmission electron microscopy images

Benjamin Berkels1 and Benedikt Wirth2
1 AICES Graduate School, RWTH Aachen University, Germany
2 Applied Mathematics, University of M¨unster, Germany

Abstract. Nowadays, modern electron microscopes deliver images at atomic scale. The precise atomic
structure encodes information about material properties. Thus, an important ingredient in the image analysis
is to locate the centers of the atoms shown in micrographs as precisely as possible. Here, we consider scanning
transmission electron microscopy (STEM), which acquires data in a rastering pattern, pixel by pixel. Due
to this rastering combined with the magniﬁcation to atomic scale, movements of the specimen even at the
nanometer scale lead to random image distortions that make precise atom localization diﬃcult. Given a
series of STEM images, we derive a Bayesian method that jointly estimates the distortion in each image
and reconstructs the underlying atomic grid of the material by ﬁtting the atom bumps with suitable bump
functions. The resulting highly non-convex minimization problems are solved numerically with a trust
region approach. Well-posedness of the reconstruction method and the model behavior for faster and faster
rastering are investigated using variational techniques. The performance of the method is ﬁnally evaluated
on both synthetic and real experimental data.

Nowadays, imaging techniques like Transmission Electron Microscopy (TEM) allow to acquire images of
materials at atomic resolution. The precise atomic conﬁguration that can be identiﬁed from those images
allows to infer information about material properties. In this article, we concentrate on a variant of TEM,
described next.

1. Introduction

1.1. STEM

An important variant of TEM is the so-called Scanning Transmission Electron Microscopy (STEM). STEM
acquires images by moving a focused electron probe over a sample in line-by-line, pixel-by-pixel manner. In
this paper, we consider high-angle annular dark-ﬁeld (HAADF) STEM, which essentially counts at each pixel
the electrons from the beam that are deﬂected by the sample and arrive at a circular annulus centered around
the sampling position. A reason for the popularity of HAADF-STEM [2] is that the number of electrons
counted at a pixel is proportional to the atomic number of the material at the corresponding position,
which allows for a direct interpretation of the measured intensities. This property comes at a price. The
sequential pixel-by-pixel rastering process combined with the magniﬁcation to atomic scale makes movements
of the sample at the nanometer scale (e. g. induced by environmental and instrumental disturbances) cause
distortions in STEM images (see Figure 3 for an example). Most visibly, there are characteristic discontinuous
horizontal distortions of the depicted atoms [10]. Furthermore, low frequency sample drift induces smooth,
but possibly non-rigid deformations of the depicted atomic lattice. The longer the exposure (time spent at a
pixel), the bigger the distortions. Thus, a natural way to reduce the distortions is to decrease the exposure
time. Unfortunately, the signal-to-noise ratio (SNR) is inversely proportional to the exposure time. To get
both small distortions and a good SNR, an often employed strategy is to acquire several images with short
exposure instead of a single image with long exposure. Of course, then one has to reconstruct the underlying
atom conﬁguration from a series of images. Due to its special nature, STEM imaging calls for tailored
image processing techniques. Finally, in many STEM applications, the reconstructed high quality image is
no end in itself, but just an intermediate step towards a characterization of the depicted material. This

characterization is a description of the atomic grid of the material, i. e. the position of the atoms (which are
actually atomic columns, since STEM shows a 2D projection of the 3D material structure) and their atomic
number.

1.2. The proposed reconstruction model

We now brieﬂy describe our ansatz for the inverse problem of identifying atom positions and other parameters
from a STEM measurement. STEM aims to sample the material by obtaining a measurement gi (essentially
the number of electrons deﬂected from the electron beam) at a discrete set of locations xi ∈ R2, i = 1, . . . , N .
To this end, the beam is positioned at xi at time ti, however, it will be slightly displaced relative to the
sample by a stochastic motion wi ∈ R2, for instance due to temperature ﬂuctuations. Given the sequence of
measurements gi, we try to reconstruct the underlying material distribution of the sample (essentially the
atom numbers and positions, encoded by a vector p from a set A of reasonable parameters) as well as the
stochastic perturbations wi of the measurement locations by minimizing the functional

E[w, p] =

d(gi, ∆tu[p](xi + wi)) +

(1)

N
(cid:88)

i=1

1
2D

N
(cid:88)

i=1

|wi − wi−1|2
(ti − ti−1)

over w = (w1, . . . , wN ) ∈ (R2)N and p ∈ A. This variational approach will be rigorously derived from a
Bayesian model in the ﬁrst part of this article, however, the rough intuition behind it is easily explained.
The expected number of deﬂected electrons at any position x ∈ R2 depends on the atom conﬁguration p in
the sample and can easily be calculated as a function ∆tu[p] : R2 → [0, ∞) (the factor ∆t > 0 indicates the
resting time over position xi, which the total number of deﬂected electrons should be proportional to). A
deviation of a measurement gi at time ti from the expected number ∆tu[p](xi + wi) at the (perturbed, true)
location xi + wi is penalized by a dissimilarity d, for which we will essentially consider the Kullback-Leibler
In addition, the second sum penalizes a change between consecutive perturbations wi−1, wi
divergence.
with the idea that the stochastic perturbation cannot change too rapidly. The constant D just weights the
importance of that regularization.

Apart from applying this model to artiﬁcial and real STEM data, we also analyze how our reconstruction
behaves as the electron beam samples the material at a faster and faster rate, thereby allowing more
measurement locations during a ﬁxed measurement time. This requires an understanding of how the
stochastic signal of deﬂected electrons behaves as more and more measurement locations are scanned. Such
an understanding can be obtained using the same tools as in stochastic homogenization. The reconstruction
model in the limit of inﬁnite sampling speed (which corresponds to a time-continuous or even inﬁnitely fast
electron beam motion) can then be derived via a Γ-convergence analysis. The result depends on the scan
path; using space-ﬁlling curves one can obtain a reconstruction model for scans sampling the whole material,
while for instance row-wise scans with more and more rows lead to a tomographic-type model, in which only
the average signal per row is used.

Due to the nonconvexity and relatively high complexity of the variational model, a very good
initialization is vital for accurate reconstructions. For this purpose, the model is reduced to a less accurate but
much simpler convex optimization problem, which allows a fast and globally optimal solution. Essentially, the
reduced model represents a deconvolution problem similar to the ones considered and analyzed for instance
recently in [3, 7].

In numerical experiments, our proposed method robustly identiﬁes all atom positions with high accuracy.
Furthermore, the conducted limit analysis gives indications as to what are reasonable scan paths and how
measurement parameters should be chosen in relation to the stochastic noise present during the measurement.

1.3. Related work

The development of special processing methods tailored to the characteristic STEM properties is an active
research topic mostly in the ﬁeld of electron microscopy, but to some degree also in mathematics. In [10],
Jones and Nellist study sources of STEM distortion and propose an algorithm to correct these distortions
on individual images. Kimoto et al. [12] propose to use averaging and rigid registration on a series of STEM

2

images to obtain a high quality image of the underlying material, which achieves considerably better precision
than previous methods working on just a single image. Note that we use the term precision here in the way
it is used in the electron microscopy community. There, it is a measure how precisely atom centers can be
located (for a detailed deﬁnition see Section 6). The quality of the average image of a series can be further
improved signiﬁcantly with non-rigid registration [1]. As of now, to the best of our knowledge, this method
still achieves the best reported precision on STEM images in the literature [19]. Since it uses a smooth
deformation model, this method cannot fully correct the horizontal STEM distortions. Recently, Jones et
al. [11] proposed a non-rigid, non-smooth registration algorithm that also aims at correcting the horizontal
distortions.
In [6], De Backer et al. propose a model-based estimation of STEM images with Gaussian
bumps that is speciﬁcally designed to analyze a large ﬁeld of view. A key concept of this approach is the
segmentation of the image into smaller sections, which avoids the simultaneous estimation of the non-linear
ﬁt parameters to reduce the computational complexitity.

The outline of the article is as follows.

In Section 2, we derive the variational model (1) and in
particular analyze its well-posedness and further properties in Section 2.8. Section 3 then examines the limit
models for a continuum of sampling locations with main results Theorem 2 as well as Theorem 3 including
the subsequent Remark 3. Section 4 considers a reduced reconstruction model necessary to initialize the
numerical optimization, while Sections 5 and 6 describe the numerical implementation and results.

Finally, for the reader’s convenience, below we provide a reference list of the most important symbols

used throughout.

(x1, . . . , xN ), (t1, . . . , tN )

∆t, ∆T

Ω = [0, a]2

U, u

G = (G1, . . . , GN ), g = (g1, . . . , gN )

W = (WDt1, . . . , WDtN ), w = (w1, . . . , wN )

P, p = (y1, . . . , yL, c1 . . . , cL, ω, o)

L, J

A

N1, N2

fX

Wt

D

E

K

P, N

α, µ, σ2

W, G, E

STEM measurement locations and times

dwell time (at a measurement location) and waiting time

scanned material region

random variable and realization of STEM measurements at
locations (x1, . . . , xN )
random variable and realization of material density

random variable and realization of the accumulated sample
motion up to times t1, . . . , tN
random variable and realization of the vector of ﬁtting parameters
for u, consisting of atom positions yl, atom heights and widths cl
and ω, and background o
number of atoms and of ﬁtting parameters

domain of ﬁtting parameters

number of pixels in horizontal and vertical direction

probability density function of random variable X

Brownian motion

diﬀusion coeﬃcient of stochastic motion

Poisson and normal distribution

gain factor, noise mean and noise variance of electron detector

proposed energy functional

number of input images or measurements

time-continuum versions of w, g, E

3

2. A Bayesian model for removal of spatial and image noise

In this section, we describe the basic concept of STEM applied to an atomic crystal, and we model the
conditional probability of acquiring a particular image given the underlying atom distribution and (random)
motion of the material sample. This will then lead to a variational Bayesian model for recovering atom
distribution and motion from STEM measurements. Note that even though identifying the random sample
motion is not of primary interest, the model actually simpliﬁes by explicitly including that motion as an
unknown to be determined.

2.1. Noise-free STEM model

Let Ω = [0, a]2 denote the scanned material region, and let u : Ω → [0, ∞) denote the idealized STEM signal
that one would obtain without any noise in the image acquisition (in particular no motion of the sample),
if each point was scanned for a time interval of length 1 (u represents the intensity of electron deﬂection at
each point, but for simplicity we shall think of it as a material density).

HAADF-STEM acquires a measurement at a sequence x1, . . . , xN ∈ Ω of points in the sample by moving
to each single point xi and resting there for a ﬁxed time interval ∆t, the so-called dwell time, during which
the number of electrons is counted that are deﬂected from the electron beam at a certain angle. The points xi
are typically arranged along horizontal lines covering Ω and are traversed row-wise, but other scanning paths
are possible as well. Typically, the STEM dwell time is of the order of 10 microseconds. The instrument may
also spend some additional time in between the signal acquisition at two consecutive measurement positions:
Even though the time for moving the electron probe from one measurement position to the next lies at least
two orders of magnitude below the dwell time (about 10 to 100 nanoseconds), large movements (such as
from the end of one row to the beginning of the next) cause the electron probe to wobble a little so that the
instrument waits a time ∆T of around 60 microseconds before starting the scan of the next position.

By ti ∈ R, i = 1, . . . , N , we shall denote the time points at which the signal acquisition at location xi is
ﬁnished (note that, setting t0 = 0, we necessarily have ti − ti−1 ≥ ∆t for all i = 1, . . . , N ). Then, the motion
of the electron beam over the sample can be described by the piecewise constant function

x(t) = xi

for t ∈ [ti−1, ti) .

The signal recorded by the instrument at position xi is denoted gi. Without noise, we thus would have
gi = ∆tu(xi) for i = 1, . . . , N .

However, there are multiple noise sources. In particular, the sample is not stationary, but undergoes
a random drift that can be modeled by Brownian motion. Furthermore, the number of deﬂected electrons
is not deterministic, but obeys a stochastic law. As a result, the measured signal is a random variable,
depending on the stochastic motion and electron deﬂection of the sample.

In the following, we denote random variables by capital letters and their realizations by the corresponding
lower-case letters. Vectors will be denoted by boldfont letters. The probability density function of a random
variable X will be denoted fX .

2.2. A Bayesian probability model for observed material density and motion

Let us abbreviate G = (G1, . . . , GN ) to be the random variable of the STEM measurements at locations
(x1, . . . , xN ). Given actual measurements g = (g1, . . . , gN ), we aim to recover the underlying true
material density u and the accumulated sample motion wi at time points ti, i = 1, . . . , N . Here, u and
w = (w1, . . . , wN ) are just realizations of random variables U and W to be described later. Note that w can
be viewed as an auxiliary variable that will simplify the modeling, but it may also be viewed as a quantity of
its own interest. For instance, w may contain information about systematic motion artifacts that occurred
during the image acquisition and allow the microscopist to identify and eliminate the respective error sources.
To set up a corresponding variational model we would like to describe the conditional probability of the
measurements g being produced with motion W = w and density U = u. By Bayes’ theorem (see e. g. [15,

4

Sec. 1.1.6]), the corresponding probability density function can be expressed as

fW,U (w, u | G = g) =

fG(g | W = w, U = u)fW,U (w, u)
fG(g)

.

As our estimate of u and w for a given image g, we will later use the so-called maximum a posteriori estimate,
which is the pair (w, u) maximizing the above conditional probability.

Since the Brownian motion and the material density are independent, we have

The following paragraphs derive expressions for fW(w), fU (u), and fG(g | W = w, U = u).

fW,U (w, u) = fW(w)fU (u) .

2.3. Brownian motion of the sample

The sample motion is mainly due to random temperature ﬂuctuations and thus can be modeled using two-
dimensional Brownian motion, denoted by Wt ∈ R2 for t ∈ R. The actual random variable describing the
sample motion is WDt with D encoding the diﬀusion time scale of the motion. An actual path, that is, a
realization of WDt is denoted wt. Hence, the random variable of accumulated motion up to times t1, . . . , tN
is

W = (WDt1, . . . , WDtN ) with realizations w = (w1, . . . , wN ) .

By deﬁnition of Brownian motion, WDti − WDti−1 is distributed according to the normal distribution of
mean 0 and covariance D(ti − ti−1)I2 with the 2 × 2 identity matrix I2, for which we use the notation
WDti − WDti−1 ∼ N (0, D(ti − ti−1)I2). Denoting the accumulated random motion up to the begin of the
measurement by w0 (by a coordinate shift we may also simply deﬁne w0 to be zero), we thus have the
probability density function

fW(w) =

f(WDti −WDti−1 )(wi − wi−1) =

N
(cid:89)

i=1

1
i=1 2πD(ti − ti−1)

(cid:81)N

(cid:32)

exp

−

N
(cid:88)

i=1

|wi − wi−1|2
2D(ti − ti−1)

(cid:33)

.

2.4. Model for the sample density

Computer simulations based on forward models of STEM show [14] that the material density for a given
sample has the form

u(x) = u[p](x) =

b[cl](x − yl) + o ,

L
(cid:88)

l=1

where o is a constant (or slowly varying) background gray level, L is the number of atom locations visible in
Ω, yl ∈ Ω is the lth atom location, p = (y1, . . . , yL, c1 . . . , cL, o) is a vector of parameters, and b[cl] : R2 → R
is the signal response of a single atom (or of multiple atoms stacked above one another in the atom lattice),
parameterized by some coeﬃcient cl ∈ Rm. Since the number of atoms can be readily identiﬁed from a
STEM measurement, we assume L to be ﬁxed. A good model for the response of a single atom seems to
be a Gaussian bell function of height cl,1 (depending on the atom type and the number of stacked atoms)
and width cl,2 (which only depends on the employed magniﬁcation of the microscope), that is, m = 2,
cl = (cl,1, cl,2) and

b[cl](x) = cl,1 exp

(cid:32)

(cid:33)

.

−|x|2
2c2
l,2

We will use this particular b throughout the article, but other functions are possible as well. As noted above,
the width cl,2 only depends on the employed magniﬁcation and the instrument. In particular, it does not
depend on the type of the corresponding atom. Thus, it is suﬃcient to treat the width as a single scalar
unknown instead of having a separate width for each atom. Hence, the vector of parameters changes to
p = (y1, . . . , yL, c1 . . . , cL, ω, o) with cl ∈ R and ω ∈ R.

5

The above implies that instead of working with the random variable U , describing the distribution of
material densities u, we may just as well work with the random variable P, describing the distribution of
parameters p, and we have

fU (u) =

fP(p)
0

if u = u[p]
else

(cid:40)

(cid:40)

and

fG(g | W = w, U = u) =

fG(g | W = w, P = p)
0

if u = u[p]
else.

For the entries of P, we assume (in lack of a more appropriate description) a uniform distribution over a
compact set A ⊂ RJ (for instance A = [a, b]J with 0 < a < b), hence

fP(p) =

(cid:40) 1
|A|
0

if p ∈ A

else,

where |A| denotes the volume of A, and J = (2 + m)L + 1 for the general model or J = 3L + 2 for the
Gaussian bump model with a single scalar unknown for the width of the bumps.

2.5. Model for the intensity noise

HAADF-STEM essentially counts the electrons from the beam that are deﬂected by the sample and arrive
at a circular annulus centered around the sampling position. Since the electron deﬂection events occur
independently, this electron count must clearly be Poisson distributed, however, it is typically multiplied by
some gain factor α > 0 and perturbed by an additive Gaussian noise of distribution N (µ, σ2) modeling the
(device-speciﬁc) background noise of the sensor. While the noise model is usually not studied in detail in
the microscopy literature, mixed Poisson-Gaussian noise is suitable in general for CMOS sensors [8] and thus
also applicable to STEM. Thus, given a ﬁxed and constant material density u, the measured (stochastic)
STEM signal G during a time interval of length ∆t is the sum of a Poisson and a Gaussian distributed
random variable,

G = αGP + GG

with GP ∼ P(∆tu) and GG ∼ N (µ, σ2) ,

where P(λ) denotes the Poisson distribution with mean λ. Abbreviating ∆tu = z, the corresponding
probability density function is thus given by

fG(g) =

∞
(cid:88)

k=0

zke−z
k!

√

1
2πσ

(cid:18)

exp

−

(g − αk − µ)2
2σ2

(cid:19)

=: f data(g; z) .

For very small σ (cid:28) αz the Gaussian component is negligible unless αk ≈ g − µ, while for very large σ (cid:29) αz
the Poisson component ensures a concentration at k ≈ z so that the distribution can be approximated by

f data(g; z) ≈




z(cid:74)

g−µ
α (cid:75)e−z
g−µ
α

(cid:74)
C 1√



2πσ

!
(cid:75)
exp

(cid:16)

− (g−αz−µ)2
2σ2

(cid:17)

if σ (cid:28) αz

if σ (cid:29) αz

(cid:74)

·
(cid:75)

(where
denotes rounding to the nearest nonnegative integer), however, one can also stick with a numerical
approximation of f data(g; z). Note that above we integrated out the actual electron count k and the actual
realization g − αk of the background noise, since neither quantity is of interest to us. This is conceptually
diﬀerent from keeping one of them as an auxiliary variable as we have done for w. The resulting maximum a
posteriri estimate for u will diﬀer slightly between both approaches, where the approach without additional
variables takes into account the stochastic behavior in a larger region of the probability space.

6

2.6. Inﬂuence of sample motion on signal

Unfortunately, during a measurement interval of time ∆t at a position xi the sample is not stationary,
but undergoes a stochastic translation according to two-dimensional Brownian motion. Recall that the
(continuous) path of the sample as a function of time t ∈ R is described by wt ∈ R2. Thus, the material
density during the measurement interval [ti − ∆t, ti) at position xi is not u(xi), but rather changes over time
and is given by u(xi + wt). Consequently, the electron count is not distributed according to P(∆tu(xi)), but
rather according to

GP ∼ P

u(xi + wt) dt

.

(cid:19)

(cid:18)(cid:90) ti

ti−∆t

[ti − ∆t, ti) into M subintervals I1, . . . , IM of length h = ∆t

M and let
Indeed, partition the interval
uij = mint∈Ij u(xi + wt), uij = maxt∈Ij u(xi + wt). Then, the electron count GP j during the jth subinterval
is distributed according to a distribution Pj between P j = P(huij) and P j = P(huij) in the sense that the
cumulative distribution function of Pj lies everywhere between those of P j and P j. Likewise, letting GP j
and GP j denote random variables with distribution P j and P j, respectively, the distribution of the sum
GP = (cid:80)M
j=1 GP j, which are well-known to be
Poisson distributed according to

j=1 GP j lies between the distributions of (cid:80)M

j=1 GP j and (cid:80)M

GP j ∼ P



huij

 ,

GP j ∼ P

huij

 .





M
(cid:88)

j=1

M
(cid:88)

j=1







M
(cid:88)

j=1

M
(cid:88)

j=1

As M → ∞ and h → 0, both distributions uniformly converge against P
summarizing, the measured signal at location xi satisﬁes

(cid:16)(cid:82) ti

(cid:17)
ti−∆t u(xi + wt) dt

so that

Gi = αGP + GG

with GP ∼ P

u(xi + wt) dt

and GG ∼ N (µ, σ2) .

(cid:18)(cid:90) ti

ti−∆t

(cid:19)

2.7. Signal distribution for stochastic motion
Since wt is not known, but just a realization of WDt, the parameter (cid:82) ti
of Gi is itself just a realization of the random variable Λ = (cid:82) ti
component GP of the signal is actually distributed according to a probability distribution

ti−∆t u(xi +wt) dt inside the distribution
ti−∆t u(xi + WDt) dt. Thus, the Poisson

P (k) =

fΛ(λ) dλ ,

(cid:90) ∞

0

λke−λ
k!

where fΛ denotes the probability density of Λ, still to be determined.

Let us assume u is analytic (this is true for our model of u) and thus can be expanded into a Taylor

series about any position xi + wi,

u(x) =

∞
(cid:88)

j=0

uij(x − xi − wi, . . . , x − xi − wi
(cid:125)

(cid:124)

)

(cid:123)(cid:122)
j times

with uij = 1

j! Dju(xi + wi) .

As a consequence,

Λ = ∆tu(xi + wi) + ˜Λ

with ˜Λ =

WDt dt − ∆twi, . . . ,

WDt dt − ∆twi

.

(cid:18)(cid:90) ti

∞
(cid:88)

j=1

uij

ti−∆t

(cid:90) ti

ti−∆t

(cid:19)

It is readily veriﬁed that as long as the uij decrease fast enough in j (for instance, if the derivatives
|Dju(xi + wi)| increase at most exponentially in j), then ˜Λ is a random variable with moments (in particular
mean and standard deviation) that are small compared to ∆t (for instance, we show further below that the

7

highest order term in ˜Λ is of order ∆t(wi − wi−1)). Since the dwell time ∆t is small, we shall in our model
approximate Λ ≈ ui := ∆tu(xi + wi) and fΛ = δui, where δui denotes the Dirac measure centered at ui.
Thus, we obtain P (k) = uk

and

i e−ui
k!

Gi = αGP + GG

with GP ∼ P (ui) and GG ∼ N (µ, σ2)

and therefore

fGi (gi | WDti−1 = wi−1, WDti = wi, U = u) = f data(gi; ui) .

Improved approximation.
by incorporating terms of higher order in ∆t. As above we have

If desired, the distribution of Λ can be more accurately approximated, for instance

Λ = ∆tu(xi + wi) + Du(xi + wi)

WDt dt − ∆twi

(cid:18)(cid:90) ti

ti−∆t

(cid:19)

+ ˆΛ ,

where ˆΛ is a random variable with negligible moments if the uij decrease fast enough. We now examine the
distribution of Xi = (cid:82) ti
ti−∆t WDt dt under the conditions WDti−1 = wi−1 and WDti = wi. For the time being
let us assume ti−1 < ti − ∆t strictly. Approximating the integral by a Riemann sum with interval width
h = ∆t

i = ti − ∆t + jh for j = 0, . . . , M yields

M and tj

X h

i =

(tj

i − tj−1

i

)WDtj

i

= (tM

i − t0

i )WDtM

+

i

(tj

i − t0

i )(WDtj

− WDtj+1

)

i

i

M −1
(cid:88)

j=1

M
(cid:88)

j=1

= ∆twi +

(tj

i − t0

i )(WDtj

− WDtj+1

) .

i

i

M −1
(cid:88)

j=1

i = ti−1, ∆T = ti − ∆t − ti−1 and Yj = WDtj

Setting t−1
for j = 0, . . . , M we ﬁrst note that Y =
(Y0, . . . , YM )T is normally distributed with mean 0 ∈ RM +1 and diagonal variance V = Ddiag(∆T, h, . . . , h),
i )T and e = (1, . . . , 1)T we
that is, Y ∼ N (0, V ). Introducing the vectors a = (0, 0, t1
obtain

i , . . . , tM −1

− WDtj−1

i − t0

− t0

i

i

i

X h

i = ∆twi − YT a = ∆twi − eT V a

eT V e YT e − YT (a − eT V a
eT V e e) = ∆twi + X h
i,2 are normally distributed. Furthermore, since e and (a − eT V a

i,1 and X h
i,2 are independent. Using eT V a = Dh2 M (M −1)

= D M −1

eT V e e) are V -orthogonal,
2M ∆t2 and eT V e = D(M h + ∆T ) =

i,1 + X h

i,2 .

2

Obviously, X h
X h
i,1 and X h
D(∆t + ∆T ) we have

X h

i,1 ∼ N

(cid:16)

(cid:104)

0,



∆t

∆t
∆t+∆T

( M −1
M

(∆T + h)( M −1
M

2 )2 [∆T + M h]

(cid:105)

(cid:17)

(cid:16)

(cid:104)

DI2

= N

0,

( M −1

M )2( ∆t

2 )2 ∆t2

(cid:105)

(cid:17)

,

DI2


∆t+∆T


X h

i,2 ∼ N

0,

∆t
∆t+∆T

∆t

2 )2 + h

(tj−1

i − t0

i − M −1
M

∆t
∆t+∆T

∆t

2 )2

 DI2

 .

Now the condition Wti = wi and Wti−1 = wi−1 is equivalent to M −1
M
−X h

i,1, thus under this condition we have

∆t
∆t+∆T

∆t

2 (wi −wi−1) = eT V a

eT V e

(cid:80)M

j=0 Yj =

X h

i = ∆twi − M −1
M

∆t
∆t+∆T

∆t

2 (wi − wi−1) + X h

i,2 ∼ N

∆twi − M −1
M

∆t
∆t+∆T

∆t
2 (wi − wi−1),

(cid:104)
(∆T + h)( M −1

M

∆t
∆t+∆T

∆t

2 )2 + h (cid:80)M

j=2(tj−1

i − t0

i − M −1
M

∆t
∆t+∆T

(cid:33)

∆t

2 )2(cid:105)

DI2

.

M
(cid:88)

j=2

(cid:32)

8

As h = 1

M → 0 this converges against

(cid:18)

Xi ∼ N

( ∆T +∆t/2

∆T +∆t wi + ∆t/2
(cid:16)

∆T +∆t wi−1)∆t,

(cid:20)
( ∆t/2
∆T +∆t )2∆T ∆t2 +
(cid:104)

(cid:90) ti

(cid:21)

(cid:19)

(t − t0

i − ∆t/2

∆T +∆t ∆t)2 dt

ti−∆t
(cid:104)
( ∆T +∆t/2

∆T +∆t )3 + ( ∆t/2

DI2
∆T +∆t )3(cid:105)(cid:105)

(cid:17)

DI2

.

= N

( ∆T +∆t/2

∆T +∆t wi + ∆t/2

∆T +∆t wi−1)∆t,

∆T +∆t )2∆T ∆t2 + ∆t3
( ∆t/2

3

In the case ∆T = 0, an analogous argument leads to

Xi ∼ N (∆t wi+wi−1

2

, ∆t3

12 DI2) .

Λ − ˆΛ ∼ N (cid:0)m, s2(cid:1)

As a result,

for the mean and variance
(cid:104)

m = ∆t
(cid:104)

s2 = D

u(xi + wi) − ∆t/2

(cid:105)
∆T +∆t Du(xi + wi)(wi − wi−1)
∆T +∆t )3 + ( ∆t/2

(cid:104)
( ∆T +∆t/2

,
∆T +∆t )3(cid:105)(cid:105)

3

∆T +∆t )2∆T ∆t2 + ∆t3
( ∆t/2

|Du(xi + wi)|2 .

With this approximation for Λ we obtain

fGi(gi | WDti−1 = wi−1, WDti = wi, U = u) =

(cid:90)

R

f data(gi; z)

1√

2πs

exp(− (z−m)2

2s2

) dz ,

which only depends on gi, wi−1, wi, u(xi + wi), Du(xi + wi), µ, σ, and α.

2.8. The variational model

Since Gi only depends on U , WDti−1 , and WDti, we may write

fG(g | W = w, U = u) =

fGi(gi | WDti−1 = wi−1, WDti = wi, U = u) .

N
(cid:89)

i=1

As explained previously, we shall be looking for the widely used maximum a posteriori (MAP) estimate of
w and u (or equivalently p). To this end, we shall minimize the negative logarithm of fW,P(w, p | G = g),
which by Section 2.2 and the subsequent sections can be expressed as

E[w, p] = − log fW,P(w, p | G = g)

= const. −

log fGi(gi | WDti−1 = wi−1, WDti = wi, U = u[p]) − log fW(w) − log fP(p)

N
(cid:88)

i=1

N
(cid:88)

i=1

= const. +

d(gi, ∆tu[p](xi + wi)) +

1
2D

N
(cid:88)

i=1

|wi − wi−1|2
(ti − ti−1)

+ ιA(p) ,

where ιA denotes the indicator function of the set A and

d(g, z) = − log f data(g; z)

is a data dissimilarity. The minimizers (w, p) of the energy E serve as estimates of the true sample motion
and atom conﬁguration, where the well-posedness of the minimization is shown in the following theorem.
Theorem 1 (Existence of minimizers). Let p (cid:55)→ u[p] be a continuous mapping from A into C( ¯Ω). Then,
E[w, p] possesses a minimizer in R2N × A.

9

Proof. The energy is lower semi-continuous in all variables (note in particular that due to the continuous
dependence of u[p] on p the evaluation at xi + wi is continuous). Furthermore, the enery is coercive in the
sense that if any component of the variables diverges, then so does the energy. Finally, the choice w = 0 and
p ∈ A arbitrary yields ﬁnite energy, and all energy terms are globally bounded from below by a constant
only depending on g. Existence of minimizers thus follows by the standard direct method of the calculus of
variations.

For later reference, let us here also state a property of the data dissimilarity d. The property essentially
means that for small enough Gaussian sensor noise, d approximates the Kullback–Leibler divergence. This
is not surprising, since for small σ the measured signals will almost follow the Poisson distribution, whose
logarithm is well-known to lead to the Kullback–Leibler divergence. In the following, o and O denote the
Landau symbols. Also recall the notation
Lemma 1. Let σ
some ﬁxed 0 < z < z, then

for rounding to the nearest nonnegative integer.
g−µ
α ∈ o(exp(( α
α

α → 0. Furthermore, let g−µ

2 and z ∈ [z, z] for

(cid:12)
(cid:12) < c < 1

σ )2)), (cid:12)
(cid:12)

− g−µ
α

·
(cid:75)

(cid:74)

(cid:74)

(cid:75)

where the constant ˜C(g − µ, α, σ) only depends on g − µ, α, and σ, but not on z.

d(g, z) = z −

g−µ
α

(cid:74)

(cid:75)

log z + ˜C(g − µ, α, σ) + O( σ

α ) ,

Proof. Let us abbreviate K =

g−µ
α

(cid:74)

(cid:75)

, then log K ∈ o( α2

σ2 ). We have

f data(g; z) =

√

1
2πσ

e−zzK
K!

(cid:18)

exp

−

(g − µ − αK)2
2σ2



(cid:19)


1 +



rk




∞
(cid:88)

k=0
k(cid:54)=K

for

rk =

zk−K exp

K!
k!

(cid:18) (g − µ − αK)2 − (g − µ − αk)2
2σ2

(cid:19)

= exp

log K! − log k! + (k − K) log z +

(cid:32)

(cid:18)

(cid:18)

≤ exp

log K! − log k! + |K − k|C +

≤ exp

log K! − log k! + |K − k|C −

(cid:19)2

(cid:34)(cid:18) g − µ
α

α2
2σ2
c2 − (1 − c)2 (K − k)2(cid:105)(cid:19)

− K

−

(cid:18) g − µ
α

(cid:19)2(cid:35)(cid:33)

− k

(cid:104)

α2
2σ2
α2
2σ2 [1 − 2c] (K − k)2
(cid:17)2

(cid:16)

(cid:19)

for C = max(| log z|, | log z|), where we used ( g−µ
if k > K, then

α − k)2 =

1 +

g−µ
α −K
K−k

(K − k)2 ≥ (1 − c

1 )2(K − k)2. Now,

rk ≤ exp

(cid:16)

C(k − K) − α2

2σ2 [1 − 2c] (K − k)2(cid:17)

(cid:16)(cid:104)

≤ exp

C − α2

2σ2 [1 − 2c]

(cid:105)

(K − k)2(cid:17)

,

while for k < K,

rk ≤ exp

(cid:16)

2σ2 [1 − 2c] (K − k)2(cid:17)
log K + log(K − 1) + . . . + log(k + 1) + C(K − k) − α2
(cid:16)(cid:104)
(cid:16)
C + log K − α2

(log K + C)(K − k) − α2

2σ2 [1 − 2c] (K − k)2(cid:17)

≤ exp

(cid:105)
2σ2 [1 − 2c]

≤ exp

(K − k)2(cid:17)

.

Thus, for α

σ large enough, we obtain in both cases rk ≤ exp

(cid:16)

4σ2 [1 − 2c] (K − k)2(cid:17)
− α2

so that

0 ≤

rk ≤

∞
(cid:88)

k=0
k(cid:54)=K

∞
(cid:88)

k=0
k(cid:54)=K

(cid:18)

exp

−

α2
4σ2 [1 − 2c] (K − k)2

(cid:19)

(cid:90) ∞

(cid:18)

≤

exp

−

α2
4σ2 [1 − 2c] (K − k)2

(cid:19)

dk = 2

(cid:114) π

1 − 2c

σ
α

.

−∞

10

Therefore,

d(g, z) = − log f data(g; z) = z − K log z + log(2πσ2)

+ log K! +

2

(g − µ − αK)2
2σ2

(cid:16)

− log

1 + O

(cid:17)(cid:17)

.

(cid:16) σ
α

2.9. Scan mode and redundant image acquisition

Here, we specify the scan mode with which the experimental data of the later sections is obtained. In those
experiments, the instrument measures pixels row by row along N2 rows of N1 pixels each, that is, there are
horizontal and vertical pixel distances ∆x1, ∆x2 with N1∆x1 = N2∆x2 = a, N = N1N2, and

xi = xml = (m∆x1, l∆x2) ,
ti = (lN1 + m)∆t + l∆T

(cid:41)

for m = (i − 1)modN1 + 1 and l = (cid:98) i−1
N1

(cid:99) + 1 ,

(2)

(cid:98)·(cid:99) representing the integer part.

Finally, to obtain a better resolution, the same sample is sometimes imaged multiple times, say K times,
potentially after sample rotations Rk ∈ SO(2), k = 1, . . . , K, yielding measurements gk, k = 1, . . . , K. In
that case, we have Brownian motions wk for each image acquisition, and an analogous derivation as before
yields the objective functional

EK[w1, . . . , wK, p] =

d(gk

i , ∆tu[p](Rk(xi + wk

i ))) +

+ ιA(p) ,

(3)

K
(cid:88)

(cid:34) N
(cid:88)

k=1

i=1

1
2D

N
(cid:88)

i=1

|wk

i − wk
i−1|2
(ti − ti−1)

(cid:35)

to be minimized for w1, . . . , wK, p, where as usual, we abbreviated wk
Theorem 1 shows the well-posedness of this slightly extended energy.

0 = 0. An analogous argument as in

3. The corresponding continuous limit models

STEM essentially samples the probe at a discrete set of points. This may be viewed as a discretization of
a continuous measurement, and one may ask the question what happens as measurements are acquired at
more and more locations, thereby increasing the scan resolution until in the limit the instrument measures
the sample in a continuous manner. Of course, in parallel, the dwell time ∆t has to decrease at the same
rate in order to keep the total image acquisition time bounded. The examination of this limit process may
give some hints as to how to choose the scanning parameters. For ease of exposition, we just consider the
case of a single image acquisition; the case of K acquisitions follows in exactly the same way.

3.1. Limit model for measurements via stochastic coupling

In order to study the limit as the resolution of the scanning path and the number of measurement locations
tend to inﬁnity, we ﬁrst have to comprehend how the measurement vector g behaves for ﬁner and ﬁner
resolution. Note, for instance, that the dimension of g will increase with the number of measurement
locations. Unfortunately, g cannot be determined deterministically for a given resolution, since it is just one
possible realization of a random variable. Therefore, we must rather understand how the distribution of g
varies as the resolution gets ﬁner. This is possible using standard models of Poisson point processes.

Consider a time-continuous measurement (in our case the electron count) during the time interval [0, T ]
(for instance the total time interval to acquire a full STEM image). The local material density underneath
the electron beam at time t ∈ [0, T ] shall be λ(t) (in our case, λ(t) = u(x(t) + w(t)), where x denotes the
desired beam position and w the Brownian motion of the sample). The events of an electron being detected
can mathematically be formulated as a measure

q =

δτj ∈ M([0, T ]) ,

Nq
(cid:88)

j=1

11

where each Dirac mass δτj describes the detection of an eletron at time τj ∈ [0, T ] and M([0, T ]) denotes the
space of Radon measures on [0, T ]. The times τj are distributed over [0, T ] according to an inhomogeneous
Poisson point process with intensity λ(t) (for an introduction to Poisson processes, their existence, and their
interpretation as empirical processes as exploited here see for instance [13, Sec. 2.1 and 2.5], [16, Sec. 1.2]),
thus each q is just a realization of a random variable Q representing the Poisson point process. The actual
measurement gi is then obtained as

gi = αq([ti − ∆t, ti)) + gG ,

where gG is the realization of the sensor noise GG at measurement position i. Note that in our model
derivation we only had to exploit the fact that gP = q([ti − ∆t, ti)) is a realization of a Poisson distributed
random variable, but we did not have to resolve the single electron counting events temporally.

Now consider a particular scan of a particular sample with material density u. We perform the thought
experiment that we acquire measurements, indexed by superscript n, of this particular physical situation
(including the ﬁxed realization w of the Brownian motion) at ﬁner and ﬁner resolution, that is, with a dwell
time ∆tn → 0 as n → ∞ and the number of measurement locations N n → ∞. Note that the corresponding
density λn(t) changes as n → ∞, since the scanning path changes slightly. Consequently, also the distribution
Qn of electron detections changes.

The change of this distribution can be understood via the following standard stochastic coupling (an

extension coupling in the terminology of [17, Chp. 3, Sec. 3.1]). Let

B = {ˆq = (cid:80)∞

i=1 δτi,λi : τi ∈ [0, T ], λi ∈ [0, ∞) ∀i, ˆq([0, T ] × [0, λ]) < ∞ ∀λ ≥ 0} .

On B we impose the natural σ-algebra F which is generated by the maps ˆq (cid:55)→ ˆq(S) for all Borel measurable
S ⊂ [0, T ] × [0, ∞). Finally, let ρ be the probability measure on B such that the support sptˆq of ˆq is
distributed according to a Poisson point process on [0, T ] × [0, ∞) with intensity 1. For given ˆq ∈ B and
λ : [0, T ] → [0, ∞) we now introduce

ˆqλ =

(cid:88)

δτj .

(τj ,λj )∈sptˆq
λj ≤λ(τj )

It is known that ˆqλ is distributed according to a Poisson point process with intensity λ (this is a direct
consequence of the mapping theorem [13, p. 18 and example (2.30)-(2.31)], [16, Lem. 1.1.3]) and therefore
exactly like the electron count q. Thus, the above thought experiment may be performed at ﬁxed realization
ˆq (setting qn = ˆqλn ), which describes appropriately how the physical measurements or their distributions
change as n → ∞.

We also have to specify how to deal with the second component of the signal, the background sensor
noise, as n → ∞. For reasons to become clear later, we assume the variance σ2 of the Gaussian background
noise to change with n. Let R be independent Gaussian white noise on [0, T ], and denote the times at which
the electron beam moves from one to the next measurement location by tn
N n , where the superscript
n refers to the sequence of thought experiments with increasing resolution. For a realization r of R deﬁne

1 , . . . , tn

rn = (r(tn

1 ), . . . , r(tn

N n )) ,

G = µ + σnrn is distributed in the same way as the background noise of the measurement. Therefore

then gn
we may perform the above thought experiment for ﬁxed realization r.

The following lemma now analyses how the signal in our thought experiment behaves as the material

density λn(t) under the electron beam changes for n → ∞.

Lemma 2. For given ﬁxed ˆq ∈ B and white noise realization r, deﬁne the electron counting events, the
electron counts, the background noise, and the full measurement as

qn = ˆqλn ,
gn
P = (qn([tn
G = µ + σn(r(tn
gn
P + gn
gn = αgn
G ,

0 , tn

1 )), . . . , qn([tn
1 ), . . . , r(tn

N n−1, tn
N n )) ,

N n ))) ,

12

where 0 = tn

0 < . . . < tn

N n = T . We further introduce continuum versions of the signals as

Gn

P (t) =

(gn
P )i
i − tn
tn

i−1

,

Gn

G(t) =

(gn
α(tn

G)i − µ
i − tn
i−1)

,

Gn(t) =

(gn)i − µ
i − tn
α(tn
i−1)

,

each for t ∈ [tn

i−1, tn

i ) .

Finally, we shall assume supi=1,...,N n (tn
surely with respect to the distribution of ˆq and r.

i − tn

i−1) → 0 as n → ∞. The following statements hold true almost

(i) If λn → λ∞ uniformly, then qn → q∞ strongly in M([0, T ]).
(ii) If λn ∗(cid:42) λ∞ in L∞((0, T )), then there exists a subsequence with qn → q∞ strongly in M([0, T ]). If

λn (cid:54)→ λ∞ strongly in L1((0, T )), then with positive probability qn (cid:54)→ q∞ for the entire sequence.

(iii) We have

(cid:107)Gn

G(cid:107)M([0,T ]) →

(cid:40)
if σnN n
0
∞ if σnN n

α → 0 ,
α → ∞ ,

and Gn
G

∗(cid:42) 0 in M([0, T ]) if limn→∞

σnN n

α ∈ (0, ∞).

(iv) If limn→∞

σnN n

α < ∞ and λn → λ∞ uniformly on [0, T ], then Gn ∗(cid:42) q∞ in M([0, T ]). If only λn ∗(cid:42) λ∞

in L∞((0, T )), then Gn ∗(cid:42) q∞ only for a subsequence.
α = 0, letting ˜Gn(t) =

σnN n

(v) If limn→∞

(tn

(cid:0)

1
i −tn
tn

i−1

we have (cid:107) ˜Gn(cid:107)M([0,T ]) → 0.

i − tn

i−1)Gn(t)

− (tn

i − tn

i−1)Gn(t)(cid:1) for t ∈ [tn

i−1, tn

i ),

(cid:74)

(cid:75)

Note that whenever we use a function, such as Gn
when interpreting the function as density with respect to the Lebesgue measure.

G, in the sense of a measure, we refer to the measure induced

Proof.

(i) Let εn = supt∈[0,T ] |λn(t) − λ∞(t)|, then (cid:107)qn − q∞(cid:107)M([0,T ]) ≤ ˆq(Sn) for Sn = {(t, λ) ∈

[0, T ] × [0, ∞) : λ∞(t) − εn ≤ λ ≤ λ∞(t) + εn}. Thus

ρ(cid:0)(cid:8)ˆq ∈ B :

lim
n→∞

(cid:107)qn − q∞(cid:107)M([0,T ]) = 0(cid:9)(cid:1) ≥ ρ(cid:0)(cid:8)ˆq ∈ B :

ˆq(Sn) = 0(cid:9)(cid:1)

lim
n→∞

≥ ρ(cid:0)(cid:8)ˆq ∈ B : ˆq(Sn) = 0(cid:9)(cid:1) = e−|Sn| →

1 ,

n→∞

where |Sn| denotes the volume of Sn.

(ii) Abbreviate M = supn=0,1,... (cid:107)λn(cid:107)L∞((0,T )) < ∞ and denote by V ⊂ B the set of Poisson processes ˆq
such that qn (cid:54)→ q∞ for any subsequence. Let ˆq ∈ V with m points (τ1, λ1), . . . , (τm, λm) in [0, T ] × [0, M ]
and introduce ζ ˆq = (sgn(λ1 − λ∞(τ1)), . . . , sgn(λm − λ∞(τm))). Obviously, ˆq ∈ V implies that for every
large enough n there is a point (τi, λi) ∈ sptˆq ⊂ [0, T ] × [0, ∞) such that λi ∈ (λn(τi), λ∞(τi)] if ζ ˆq
i <
0 and λi ∈ (λ∞(τi), λn(τi)] else. Choosing δ = 1
2 min(|λ1 − λ∞(τ1)|, . . . , |λm − λ∞(τm)|) we thus have
(τ1, . . . , τm) ∈ S(λ1, . . . , λm, ζ ˆq) ⊂ Sm

δ (ζ ˆq) for

S(λ1, . . . , λm, ζ ˆq) = {(τ1, . . . , τm) ∈ [0, T ]m : ∃N > 0 ∀n > N ∃i ∈ {1, . . . , m} :

λi ∈ (λn(τi), λ∞(τi)] if ζ ˆq

i < 0 and λi ∈ (λ∞(τi), λn(τi)] else} ,

δ (ζ ˆq) = {(τ1, . . . , τm) ∈ [0, T ]m : ∃N > 0 ∀n > N ∃i ∈ {1, . . . , m} : ζ ˆq
Sm

i (λn(τi) − λ∞(τi)) ≥ δ} .

13

V ⊂

{ˆq ∈ B : ˆq([0, T ] × [0, M ]) = m, spt(ˆq) = {(τ1, λ1), . . . , (τm, λm)}, (τ1, . . . , τm) ∈ S(λ1, . . . , λm, ζ ˆq)}

ˆq ∈ B : spt(ˆq) ∩ [0, T ] × [0, M ] = {(τ1, λ1), . . . , (τm, λm)}, (τ1, . . . , τm) ∈

(cid:91)

(cid:91)

(cid:27)

Sm

δ (ζ)

ζ∈{−1,1}m

δ>0

{ˆq ∈ B : spt(ˆq) ∩ [0, T ] × [0, M ] = {(τ1, λ1), . . . , (τm, λm)}, (τ1, . . . , τm) ∈ Sm

δ (ζ)}

Therefore, we obtain

∞
(cid:91)

m=1
∞
(cid:91)

(cid:26)

⊂

⊂

=

m=1

∞
(cid:91)

∞
(cid:91)

(cid:91)

(cid:91)

m=1

ζ∈{−1,1}m

δ>0

(cid:91)

(cid:91)

V m
δ (ζ) .

m=1

ζ∈{−1,1}m

δ>0

Thus, we get





∞
(cid:91)

ρ(V ) ≤ ρ



(cid:91)

(cid:91)

V m
δ (ζ)

 .

l=1

ζ∈{−1,1}m

i∈N

δ (ζ) ⊂ V m
ˆδ

δ (ζ) is monotone in δ, that is, V m

Since V m
with a countably inﬁnite union. Hence, if we can show ρ(V m
m ≥ 0, we get ρ(V ) = 0, proving the desired claim.
It remains to show ρ(V m
(a) show that Sm
(b) imply that ρ(V m
As for the ﬁrst item, note that Sm
intersections of measurable sets,

δ (ζ) is a nullset and

δ (ζ)) = 0.

δ (ζ)) = 0 for ﬁxed δ > 0, ζ ∈ {−1, 1}m, m ≥ 0. To this end, we

(ζ) for any ˆδ < δ, we can replace the union over δ
δ (ζ)) = 0 for all δ > 0, ζ ∈ {−1, 1}m, and

δ (ζ) is measurable since it is the countable union of countable

Sm

δ (ζ) =

∞
(cid:91)

∞
(cid:92)

N =1

n=N

(cid:2){τ ∈ [0, T ] : ζ1(λn(τ ) − λ∞(τ )) ≥ δ} × Rm−1 ∪

R × {τ ∈ [0, T ] : ζ2(λn(τ ) − λ∞(τ )) ≥ δ} × Rm−2 ∪ . . . ∪

Rm−1 × {τ ∈ [0, T ] : ζm(λn(τ ) − λ∞(τ )) ≥ δ}(cid:3) .

Furthermore, the set

˜Sm
δ = Sm

δ (ζ) \ (R × Sm−1

δ

((ζ2, . . . , ζm)))

is a nullset, which follows from [18, Thm. B] and the fact that ˜Sm
δ
(cid:126)τ ∈ [0, T ]m−1 the set ˆSm
for (cid:126)τ ∈ Sm−1
subsequence n1, n2, . . . with ζi(λnj ((cid:126)τi) − λ∞((cid:126)τi)) < δ for all i = 2, . . . , m and j ∈ N so that

is measurable and that for any
Indeed,
((ζ2, . . . , ζm)) there is an inﬁnite

: (τ2, . . . , τm) = (cid:126)τ } is an L1-nullset.

δ = ∅, while for (cid:126)τ /∈ Sm−1

δ = {(τ1, . . . , τm) ∈ ˜Sm

((ζ2, . . . , ζm)) we have ˆSm

δ

δ

δ

ˆSm
δ = {(τ1, . . . , τm) ∈ Sm

δ (ζ) : (τ2, . . . , τm) = (cid:126)τ }

= {τ ∈ [0, T ] : ζ1(λnj (τ ) − λ∞(τ ) ≥ δ for all j} × {(cid:126)τ } = ¯S1

δ (ζ1) × {(cid:126)τ } .

0 ζ1χ ¯S1

δ (ζ1)(λnj − λ∞) dτ ≥ δ| ¯S1

δ (ζ1) must be a nullset since otherwise λnj − λ∞ tested with the characteristic function of
δ (ζ1)| for all j, contradicting the weak-* convergence
δ (±1) = {t ∈ [0, T ] : ∃N ≥ 0 ∀n > N :
δ (ζ) is

Now the set ¯S1
δ (ζ1) would yield (cid:82) T
¯S1
λn ∗(cid:42) λ∞ (here | · | denotes the Lebesgue measure). Finally, S1
±(λn(t) − λ∞(t)) ≥ δ} is a nullset for the same reason. By induction in m it thus follows that Sm
a nullset.
Now let us derive ρ(V m
δ (ζ)) = 0. For ˆq ∈ B with ˆq([0, T ] × [0, M ]) = m denote the points in its support
by spt(ˆq) ∩ [0, T ] × [0, M ] = {(τ1, λ1), . . . , (τm, λm)}. Obviously, τ1, . . . , τm ∈ [0, T ] are independently

14

identically distributed random variables obeying the uniform distribution on [0, T ]. Thus the vector
(τ1, . . . , τm) is uniformly distributed on [0, T ]m with constant probability density T −m so that

ρ(V m

δ (ζ)) = ρ({ˆq ∈ B : ˆq([0, T ] × [0, M ]) = m})T −m|Sm

δ (ζ)| = 0 .

This ends the proof of the ﬁrst part.
For the second part, assume λn (cid:54)→ λ∞ in L1((0, T )), that is, there exists a subsequence (indexed again
by n) such that Sn = {(τ, λ) ∈ [0, T ] × [0, ∞) : λ ∈ (λn(τ ), λ∞(τ )) or λ ∈ (λ∞(τ ), λn(τ ))} satisﬁes
|Sn| ≥ δ for some δ > 0 and all n. The probability that (cid:107)qn − q∞(cid:107)M([0,T ]) ≥ 1 is thus given by

ρ({ˆq ∈ B : (cid:107)qn − q∞(cid:107)M([0,T ]) ≥ 1}) = ρ({ˆq ∈ B : ˆq(Sn) ≥ 1}) = 1 − e−|Sn| ≥ 1 − e−δ .

If we introduce Bn = {ˆq ∈ B : (cid:107)qj − q∞(cid:107)M([0,T ]) ≥ 1 for some j ≥ n}, then this set is monotone,
Bn ⊃ Bn+1, and has probability ρ(Bn) ≥ 1 − e−δ due to the above. Thus,
(cid:33)

(cid:16)(cid:110)

ρ

j→∞

ˆq ∈ B : lim sup

(cid:107)qj − q∞(cid:107)M([0,T ]) ≥ 1

= ρ

Bn

≥ 1 − e−δ > 0

(cid:111)(cid:17)

(cid:32) ∞
(cid:92)

n=1

due to the monotone convergence theorem.
Note that there is a subset of B with positive probability such that we still have qn → q∞, for instance
all those ˆq that do not contain any mass in [0, T ] × [0, M ].

(iii) We have

By the strong law of large numbers, the right-hand side converges almost surely against the expected
value of the |r(tn
i )| has expected
value (cid:112)2/π so that almost surely

i ) is normally distributed with mean 0 and variance 1, |r(tn

i )|. Since r(tn

α

σnN n (cid:107)Gn

G(cid:107)M([0,T ]) = 1
N n

|r(tn

i )| .

N n
(cid:88)

i=1

α

σnN n (cid:107)Gn

G(cid:107)M([0,T ]) →

(cid:113) 2
π ,

∈ (0, ∞) so that we
which direcly implies the ﬁrst two statements. Next assume limn→∞
have almost sure boundedness of (cid:107)Gn
G(cid:107)M([0,T ]). Let ϕ ∈ C([0, T ]). For ε > 0 let δ > 0 such that
|ϕ(t) − ϕ(ˆt)| < ε for all |t − ˆt| < δ, and let 0 = τ0 < τ1 < . . . < τK = T with τj+1 − τj < δ for all j. For
simplicity let us assume that for each n we have {τ0, . . . , τK} ⊂ {t1
N n } (the argument can easily
be adapted if this is not the case). We have

0, . . . , tn

σnN n
α

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90) T

0

Gn

Gϕ dt

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

K
(cid:88)

(cid:90) τj

j=1

τj−1

Gn

(cid:12)
(cid:12)
(cid:12)
Gϕ dt
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

|ϕ(τj)|

K
(cid:88)

j=1

(cid:32)

K
(cid:88)

≤

j=1

(cid:90) τj

τj−1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

|ϕ(τj)|

Gn

G dt

+ ε

|Gn

G| dt

(cid:12)
(cid:90) τj
(cid:12)
(cid:12)
(cid:12)
(cid:12)

τj−1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90) τj

τj−1

σn
α

K
(cid:88)

j=1

(cid:33)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= ε(cid:107)Gn

G(cid:107)M([0,T ]) +

Gn

G dt

= ε(cid:107)Gn

G(cid:107)M([0,T ]) +

|ϕ(τj)|

(cid:88)

r(tn
i )

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ ε(cid:107)Gn

G(cid:107)M([0,T ]) + (cid:107)ϕ(cid:107)C([0,T ])

σn
α

K
(cid:88)

j=1

K n
j

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
K n
j

≤ ε(cid:107)Gn

G(cid:107)M([0,T ]) + (cid:107)ϕ(cid:107)C([0,T ])

(cid:88)

r(tn
i )

tn
i ∈(τj−1,τj ]

N nσn
α

sup
j

1
K n
j

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

tn
i ∈(τj−1,τj ]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

tn
i ∈(τj−1,τj ]

(cid:12)
(cid:12)
(cid:12)
r(tn
i )
(cid:12)
(cid:12)
(cid:12)

,

where K n
j denotes the number of summands in the interior sum. Now by the law of large numbers, each
of the absolute values on the right-hand side converges to zero almost surely so that the desired result
follows by the arbitrariness of ε.

15

(iv) By parts i and ii we almost surely have qn → q∞ (only for a subsequence in the case λn ∗(cid:42) λ∞), which

∗(cid:42) q∞. Furthermore, part iii implies Gn
G

∗(cid:42) 0 so that Gn = Gn

P + Gn
G

∗(cid:42) q∞.

implies Gn
P
i−1, tn
(v) For t ∈ [tn

i ) we have

| ˜Gn(t)| =

(cid:12)
(cid:12)

(cid:74)

(tn

i − tn

i−1)Gn(t)
(cid:75)
i − tn
tn

i−1

− (tn

i − tn

i−1)Gn(t)(cid:12)
(cid:12)

(tn

i −tn

i−1)Gn

i −tn

i−1)Gn

G(t)|

|
(cid:74)
|(tn






≤

i −tn
i−1)Gn
i −tn
tn

i−1

−(tn
G(t)
(cid:75)
i −tn
tn
G(t)|

i−1

if Gn

G(t) ≥ 0

else






≤ |Gn

G(t)| ,

thus (cid:107) ˜Gn(cid:107)M([0,T ]) ≤ (cid:107)Gn

G(cid:107)M([0,T ]) → 0 almost surely by part iii.

Statements i and ii of the lemma show that as our scanning path and thus the temporally changing
material density under the electron beam converge, the events of electron detections will converge exactly to
the events detected if a time-continuous scanning path were used (at least up to a subsequence, if the material
density under the beam only converges weakly). Statement iii analyses the behavior of the background sensor
noise and shows that measurements become useless if σnN n
α → ∞, since in that case the noisy background
signal becomes inﬁnitely large, swallowing up the electron count. Thus, increasing the scan resolution and
decreasing the dwell time can only be feasible if at the same time the variance of the background noise
decreases suﬃciently fast; for given σ one should choose a dwell time of at least ∆t ∼ σ. Statement iv
then shows that the combined measured signal (the accumulated electron count and background noise at
each measurement location) indeed converges (in the weak sense) against the electron count during a time-
continuous scan. Finally, the last statement implies that for suﬃciently decreasing background noise variance
the electron counting signal becomes clean in the sense that it deviates only little from integer values.

For later reference we shall here also prove the following statement about points of a Poisson process.

Lemma 3. Let ˆN n be a sequence with ˆN n → ∞ as n → ∞ and q = (cid:80)m
process on [0, T ] with positive intensity and m points. Then almost surely

j=1 δτj describe a Poisson point

{( ˆN n τ1

T mod 1, . . . , ˆN n τm

T mod 1) : n ∈ N} is dense on [0, 1]m .

Proof. The times τ1

T , . . . , τm

T are independently identically distributed on [0, 1]. Deﬁne

Bε(θ) = (θ1 − ε, θ1 + ε) × . . . × (θm − ε, θm + ε) ⊂ [0, 1]m ,
ε(θ) = {t ∈ [0, 1]m : ( ˆN nt1 mod 1, . . . , ˆN ntm mod 1) /∈ Bε(θ) ∀n ≥ l} .
Sl

Thus, we have to show that Sl
as (compare Figure 1)

ε(θ) is a nullset for all ε > 0, l ∈ N, and θ ∈ (0, 1)m. Now Sl

ε(θ) can be written

Sl

ε(θ) =

T k
ε (θ)

for T k

ε (θ) = {t ∈ [0, 1]m : ( ˆN kt1 mod 1, . . . , ˆN ktm mod 1) /∈ Bε(θ)}

∞
(cid:92)

k=l

= [0, 1]m \

B ε
ˆN k

(cid:0) θj
ˆN k

(cid:1) ∪ B ε

ˆN k

(cid:0) θj
ˆN k + 1
ˆN k

(cid:1) ∪ . . . ∪ B ε

(cid:0) θj
ˆN k + ˆN k−1
ˆN k

ˆN k

(cid:1)(cid:105)

.

m
(cid:89)

(cid:104)

j=1

Note that for every set S ⊂ Rm with ﬁnite perimeter there is some k such that |T k
Indeed, for any k ∈ N, the hypercube [0, 1]m is tiled by ( ˆN k)m hypercubes of sidelength 1
volume fraction 1 − (2ε)m inside T k
also tile S with an arbitrarily small error at the boundary ∂S so that the volume fraction of S inside T k
approaches 1 − (2ε)m as well. Therefore, we may for j ∈ N recursively deﬁne

ε (θ) ∩ S|/|S| < 1 − (2ε)m/2.
ˆN k , each having a
ε (θ) (see Figure 1). By choosing k suﬃciently large, the little hypercubes
ε (θ)

S1 = T l

ε(θ) , Sj+1 = T kj

ε (θ) ∩ Sj , where kj satisﬁes |T

kj
ε (θ)∩Sj |
|Sj |

< 1 − (2ε)m

.

2

16

(cid:8)

T k
ε (θ)

(cid:8)(cid:8)(cid:25)

2ε

2ε

•θ

1

1
ˆNk

2ε
ˆNk

2ε
ˆNk

1/ ˆN k
1

Figure 1. The set T k

ε (θ) from the proof of Lemma 3, consisting of copies of [0, 1]m \ Bε(θ), scaled by 1

ˆN k .

Then, Sl

ε(θ) ⊂ (cid:84)∞

j=1 Sj so that (letting | · | denote Lebesgue measure)

|Sl

ε(θ)| ≤ lim
j→∞

|Sj| = lim
j→∞

|S1|

|S2|
|S1|

· · ·

|Sj|
|Sj−1|

≤ lim
j→∞

(1 − (2ε)m/2)j = 0 .

3.2. Time-continuous scanning paths

Here, we shall consider the case in which the piecewise constant scanning path xn(t) approximates a time-
continuous scanning path as n → ∞, that is,

xn → x∞ uniformly on [0, T ] ,

where, for a ﬁxed total acquisition time T , we assume the number of measurement locations N n, the dwell
time ∆tn, the signal acquisition times tn
i , the path xn, and the background noise standard deviation σn to
satisfy

N n → ∞ , ∆tn = T

N n ,

i = i∆tn for i = 1, . . . , N n ,
tn

xn(t) = xn

i for t ∈ [tn

i−1, tn

i ) ,

σnN n

α → 0 .

Consequently, assuming a smooth (for instance Lipschitz) true underlying material density u, the material
density λn under the electron beam satisﬁes

λn(t) = u(xn(t) + w(t)) → u(x∞(t) + w(t)) = λ∞(t) uniformly on [0, T ] .

Statement iv of Lemma 2 thus implies weak convergence of the measured signal,

where q∞ represents the signal belonging to λ∞. In addition, by Statement v of Lemma 2 we have

Gn ∗(cid:42) q∞ with Gn(t) =

(gn)i − µ
i − tn
α(tn
i−1)

for t ∈ [tn

i−1, tn

i ) ,

1
∆tn (cid:107)

(∆tn)Gn
(cid:74)

(cid:75)

− (∆tn)Gn(cid:107)M([0,T ]) → 0 .

In that case, the energy, whose minimizers yield estimates for the sample motion and material density,
becomes

En[w, p] =

dn((gn)i, ∆tnu[p](xn

i + wi)) +

N n
(cid:88)

i=1

1
2D

N n
(cid:88)

i=1

|wi − wi−1|2
∆tn

+ ιA(p)

17

(note that the data dissimilarity dn depends on σn and thus has a superscript as well). In this formulation,
it is still inconvenient to analyze the energy convergence as n → ∞, since the dimension of the argument w
changes with n. Therefore, we reformulate the energy based on time-continuous representations of w as

E n[W, p] =

En[w, p]






∞

else,

if W : [0, T ] → R2 is the piecewise aﬃne interpolation
of the values w = (w0, . . . , wN n ) at positions (tn

0 , . . . , tn

N n )

where consistently with our previous notation, we use calligraphic capitals for functions derived from discrete
vectors. We shall show that this energy Γ-converges (up to constants) against the energy

(cid:40)

E ∞[W, p] =

dKL(q∞, u[p](x∞(·) + W(·))) + 1
∞

2D |W|2

H 1((0,T ))2 + ιA(p) ,

if W(0) = w0
else,

where | · |H 1((0,T ))2 is the H 1-seminorm and dKL is the Kullback–Leibler divergence

dKL(q, u) =

u(t) dt −

log u(t) dq(t) .

(cid:90) T

0

(cid:90) T

0

Thus, in the limit, the motion estimate W is regularized in H 1, and the material density is compared to the
electron detections via the Kullback–Leibler divergence between measures.

Theorem 2 (Limit reconstruction for time-continuous scanning paths). Let p (cid:55)→ u[p] be continuous from
A to C 0,1(Ω) with u[p] > 0 bounded away from zero for any p (this holds true for our particular choice of
parameterization). There exists a sequence of constants C n = C n(Gn, σn) such that with respect to weak
convergence in H 1((0, T ))2 and (strong) convergence in RJ we have

Γ − lim
n→∞

E n − C n = E ∞ .

Furthermore, minimizers of E n converge in the same topology against minimizers of E ∞.

Proof. lim inf-inequality: Let W n (cid:42) W in H 1((0, T ))2 and pn → p. Without loss of generality we may
assume E n[W n, pn] − C n < C < ∞ for all n (else we may either restrict to a subsequence or there is nothing
to show). Due to the compact embedding of H 1((0, T )) in C 0([0, T ]) combined with W n(0) = w0 for all n,
we get W(0) = w0.

Furthermore, note that

E n[W n, pn] =

0

∆tn

(cid:90) T

dn(∆tnαGn(t) + µ, ∆tnu[pn](xn(t) + W n(∆tn(cid:100) t

∆tn (cid:101))))

dt +

|W n|2

H 1((0,T ))2 + ιA(pn) .

1
2D

Since | · |2
convergence in RJ , respectively, it suﬃces to show that the integral minus

H 1((0,T ))2 and ιA are sequentially lower semi-continuous under weak convergence in H 1((0, w))2 and

C n =

1
∆tn

(cid:90) T

0

˜C(α∆tnGn(t), α, σn) −

∆tnGn(t)
(cid:75)

(cid:74)

log ∆tn dt

(with ˜C from Lemma 1) converges up to a subsequence against dKL(q∞, u[p](x∞(·) + W(·))).

We have u[pn] → u[p] in C 0,1(Ω) as well as xn → x∞ in L∞((0, T )) and W n → W in C 0((0, T )) (upon

extracting a subsequence) so that also

un(t) = u[pn](xn(t) + W n(∆tn(cid:100) t

∆tn (cid:101))) → u[p](x∞(t) + W(t)) = u∞(t)

uniformly. Furthermore, for n large enough Lemma 1 implies

dn(∆tnαGn(t) + µ, ∆tnun(t))
∆tn

−

˜C(α∆tnGn(t), α, σn)
∆tn

+ (cid:74)

log ∆tn

(cid:75)

∆tnGn(t)
∆tn
= un(t) − (cid:74)

∆tnGn(t)
∆tn

(cid:75)

log un(t) + O( σn

α∆tn )

18

since z ≤ u[pn] ≤ z for some 0 < z < z, ∆tnGn(t) ≤ (cid:107)Gn(cid:107)M([0,T ])
∆tnGn
|
(cid:74)
(cid:74)
of un and the weak-* convergence of Gn (which implies (cid:74)

−∆tnGn(cid:107)M([0,T ])
(cid:75)

−∆tnGn(t)| ≤ (cid:107)

∆tnGn(t)

∗(cid:42) q∞) we obtain

∆tn

(cid:75)

∆tnGn
(cid:75)∆tn

→ 0 uniformly in t. Thus, due to the uniform convergence

is uniformly bounded, and

(cid:90) T

0

dn(∆tnαGn(t) + µ, ∆tnun(t))
∆tn

dt − C n =

∆tnGn(t)
(cid:75)
∆tn

log un(t) dt + O( T σn

α∆tn )

(cid:90) T

0

→

un(t) − (cid:74)
(cid:90) T

0

(cid:90) T

0

u∞(t) dt −

log u∞(t) dq∞(t) = dKL(q∞, u∞) ,

as desired. Indeed, for any gn ∗(cid:42) g we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90) T

0

(cid:90) T

0

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:90) T
(cid:12)
(cid:12)
(cid:12)
(cid:12)

0

log un dgn −

log u∞ dg

≤

log un − log u∞ dgn

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:90) T
(cid:12)
(cid:12)
(cid:12)
(cid:12)

0

(cid:12)
(cid:12)
log u∞ d(g − gn)
(cid:12)
(cid:12)
(cid:12)

≤ (cid:107) log un − log u∞(cid:107)L∞((0,T ))(cid:107)gn(cid:107)M([0,T ]) +

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90) T

0

(cid:12)
(cid:12)
log u∞ d(g − gn)
(cid:12)
(cid:12)
(cid:12)

→ 0 .

lim sup-inequality: Let W ∈ H 1((0, T ))2 and p ∈ RJ be given with E ∞[W, p] < ∞. As recovery sequence
i )), i = 0, . . . , N n, which

we choose pn = p and W n the piecewise aﬃne interpolation of the points (tn
even converges strongly in H 1((0, T ))2. With this choice, we obtain

i , W(tn

E n[W n, pn] =

0

∆tn

(cid:90) T

dn(∆tnαGn(t) + µ, ∆tnu[p](xn(t) + W n(∆tn(cid:100) t

∆tn (cid:101))))

dt +

|W n|2

H 1((0,T ))2 + ιA(p) .

1
2D

H 1((0,T ))2 + ιA(p), while the integral minus C n converges to
The latter two terms converge against
dKL(q∞, u[p](x∞(·) + W(·))) as in the proof of the lim inf-inequality so that E n[W n, pn] − C n → E ∞[W, p]
as desired.

2D |W|2

1

Convergence of minimizers: For any ˆp ∈ A, we have minW,p E n[W, p] ≤ E n[0, ˆp], and the right-hand
side is uniformly bounded in n. Since 1
H 1((0,T ))2 + ιA(p) forms part of each E n, this implies (using
W n(0) = 0 and the Poincar´e inequality) that the set of minimizers of the E n is uniformly bounded (and thus
sequentially compact with respect to our chosen topology) in H 1((0, T ))2 × RJ , which in turn is well-known
to result in any sequence of minimizers having a subsequence converging to a minimizer of E ∞.

2D |W|2

That the limit energy only stays ﬁnite for motions W of H 1-regularity may seem a little counterintuitive
since Brownian motion almost surely has no weak derivative, however, in the MAP estimate, we retrieve
very special realizations which may indeed have additional regularity.

Remark 1 (Full coverage via Peano curve). In some contexts, for instance for biological samples that can
only be scanned at much lower magniﬁcation, it might be advantageous to fully cover the whole sample Ω
during the image acquisition (that is, to traverse every point in Ω at least once along the scan path), since
otherwise small objects might be overlooked in between the scan locations (for instance between the rows for
row-wise scans). This can be achieved using a space-ﬁlling curve such as a Peano curve x∞ as scan path.

To this end, consider a standard sequence of piecewise aﬃne curves yn : [0, T ] → Ω with constant
absolute velocity such that yn converges uniformly to the Peano curve x∞ : [0, T ] → Ω. Let N n denote the
number of corners of yn and take these as measurement locations xn
N n .
Then the corresponding beam scanning path xn also converges uniformly against x∞ so that our previous
Γ-convergence result applies. In particular, in the limit n → ∞ the data term dKL(q∞, u[p](x∞(·) + W(·)))
compares the measurement to the estimated material density at every point in Ω (up to the Brownian sample
motion).

N n with dwell time ∆tn = T

1 , . . . , xn

Note that since each point in Ω is covered one might be tempted to rewrite the data term and the H 1-
regularization of W as a space integral over Ω, however, this is not possible since a space-ﬁlling curve x∞
can never be injective. Thus, a sampling of the entire domain can only be expressed as a time-like variational
problem, but not a space-like one.

19

Remark 2 (Constant number of continuous rows). Typically, the electron beam scans the sample row-wise
as described in Section 2.9. We may consider the case in which the number N2 of scanned rows as well
as the scan time Trow per row and the waiting time ∆T between two rows stay the same, but the number
∆tn = a
N n
of acquired pixels per row converges to inﬁnity. In this case, our Γ-convergence result
∆xn
1
applies to each single row scan.

1 = Trow

Setting T = Trow we may split each function f (t) of time up into the time intervals corresponding to the

diﬀerent scan rows according to

fj : [0, T ] → R2 ,

fj(t) = f (t + (j − 1)(Trow + ∆T )) ,

j = 1, . . . , N2 .

In particular, we apply this notation to the motion estimate f = W, the scan path f = xn, and the signal
f = Gn. Then, our model energy can be rewritten as

En[w, p] = E n[W, p] =

E n
j [Wj, p] +

1
2D

|Wj(0) − Wj−1(T )|2
∆T

,

N2(cid:88)

j=1

where we simply set W0 = 0 and where E n
for the jth energy E n
j
j and (cid:80)N2
E ∞

|Wj (0)−Wj−1(T )|2
∆T

1
2D

j=1

the initial accumulated motion is w0 = Wj(0)). Since the E n

j represents the energy for scan path xn

j and signal Gn

j (note that
j Γ-converge against the

represents a continuous perturbation, the Γ-limit of the full energy is

E ∞[W, p] =

E ∞
j [Wj, p] +

1
2D

|Wj(0) − Wj−1(T )|2
∆T

.

N2(cid:88)

j=1

Thus, the motion estimate is H 1-regularized separately for each row, and the diﬀerence between the last and
the ﬁrst shift of each row are penalized quadratically in addition.

3.3. Finer row and column resolution

We now consider the case in which there are not only more and more pixels per row, but in which the number
of rows N n
2 also increases to inﬁnity. Of course, to perform such measurements in ﬁnite time, not only the
dwell time ∆tn has to decrease to zero, but also the waiting time ∆T n between consecutive rows. At ﬁrst
sight, this may seem unrealistic since a suﬃcient waiting time is necessary after any large motion, however,
if instead of scanning each row from left to right one scans the rows in alternating directions, obtaining a
snake-like scanning path, then the motion between consecutive rows actually is of the same order as the
motion between two pixels so that no waiting time is necessary. For simplicity we shall thus set the waiting
time ∆T between consecutive rows to zero; its inclusion would not lead to a qualitatively diﬀerent analysis.
Thus, we choose

N n
1 = a
N n
1

∆xn

1 , N n

2 → ∞ , N n = N n

, ∆xn

2 = a
N n
2

,

1 N n
xn(t) = xn

2 , ∆tn = T
N n ,
ml = (m∆xn

i = i∆tn for i = 1, . . . , N n ,
tn
i−1, tn
2 ) for t ∈ [tn
1 , l∆xn

i ) and m, l according to (2) .

σnN n

α → 0 ,

We will see that—for our chosen stochastic coupling—the variational model does not converge in this case.
Again we have to identify how the measured signal behaves as n → ∞. We ﬁrst note that the material
density λn(t) = u(xn(t) + w(t)) under the electron beam converges weakly against some row-wise average
density.
Lemma 4. Denoting by v = a

T the average vertical speed of the electron beam, we have

λn(t) = u(xn(t) + w(t)) ∗(cid:42) λ∞(t) =

u((s, vt) + w(t)) ds

in L∞((0, T )) .

Proof. It suﬃces to show (cid:82) β
α λ∞(t) dt = 1
[vα,vβ]×[0,a] u(x + w(x2/v)) dx as n → ∞
for arbitrary α, β ∈ [0, T ], since linear combinations of characteristic functions are dense in L1((0, T )).

α λn(t) dt → (cid:82) β

av

(cid:82)

1
a

(cid:90) a

0

20

Now let Lu be the Lipschitz constant of u and let ε : [0, ∞] → [0, ∞) with limδ→0 ε(δ) = 0 such that
|w(t) − w(ˆt)| ≤ ε(δ) for all |t − ˆt| ≤ δ. Then we have

u(xn(t) + w(t)) dt

(cid:99)

(cid:98) vβ
∆xn
2
(cid:88)

N n
1(cid:88)

l=(cid:98) vα
∆xn
2

(cid:99)

m=1

(cid:99)

(cid:98) vβ
∆xn
2
(cid:88)

(cid:90) a

l=(cid:98) vα
∆xn
2

(cid:99)

0

∆tn
∆xn
1

(cid:90) β

α

=

=

∆tn
1 ∆xn
2

∆xn

(cid:90) (cid:98) vβ
∆xn
2

(cid:99)∆xn
2

(cid:90) a

(cid:98) vα
∆xn
2

(cid:99)∆xn
2

0

= ∆tn

u(xn

ml + w((lN n

1 + m)∆tn)) + O(N n

1 ∆tn)

u(cid:0)(s, l∆xn

2 ) + w(cid:0) l∆xn

2

v

(cid:1)(cid:1) ds + O(N n

1 ∆tn + Lu(∆xn

1 + ε(N n

1 ∆tn)))

u(x + w(x2/v)) dx + O(cid:0)N n

1 ∆tn + Lu

(cid:0)∆xn

1 + ∆xn

2 + ε(cid:0)N n

1 ∆tn + ∆xn

2

(cid:1)(cid:1)(cid:1) ,

v

which converges against the desired limit.

Thus, by Statement ii of Lemma 2 a subsequence of the electron detection events (still indexed by n)
converges, qn → q∞, where q∞ represents the signal belonging to λ∞. Since all qn and q∞ are sums of Dirac
measures, this actually implies qn = q∞ for n large enough. Therefore, restricting to the subsequence and
large enough n, the measured signal satisﬁes
0 , tn

G = µ + σn(r(tn
gn

P = (q∞([tn

1 ), . . . , r(tn

N n−1, tn

N n ))) ,

for gn

N n )) ,

gn = αgn

1 )), . . . , q∞([tn
where, due to Statements iii and v of Lemma 2, we have

P + gn
G ,

N n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)

(gn

G)i−µ
α

(cid:12)
(cid:12) = (cid:107)Gn
(cid:12)

G(cid:107)M([0,T ]) → 0 ,

(cid:114) (gn)i−µ
α

(cid:122) − (gn)i−µ

α

(cid:12)
(cid:12) = (cid:107) ˜Gn(cid:107)M([0,T ]) → 0 .
(cid:12)

N n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)

To compare scan paths of diﬀerent resolution we shall again extend the vector of estimated Brownian

motions to a continuous function, which this time shall be deﬁned on Ω. Thus, we set

E n[W, p] =

En[w, p]






∞

else.

if W : Ω → R2 is the piecewise bilinear interpolation of the
1 1, xn
21, . . . , xn
N n

values w = (w1, . . . , wN n ) at positions (xn

11, xn

12, . . . , xn
N n

1 N n
2

)

As the following result shows, this variational model does not have a limit model in general.

Note that, for the sake of simplicity, we dropped the condition W(0) = w0, when deﬁning W on Ω
instead of [0, T ]. This leads to some invariance that causes E n to have inﬁnitely many minimizers (indeed,
a constant shift in W can be compensated for by the applying the same shift to the atom positions), but is
not the reason why there is no limit model.
Theorem 3 (Nonexistence of limit model). Again, let p (cid:55)→ u[p] be continuous from A to C 0,1(Ω) with
u[p] > 0 bounded away from zero for any p, and let C n be the sequence of constants from Theorem 2.
Almost surely, with respect to weak convergence in H 1(Ω)2 and (strong) convergence in RJ , we have

Γ − lim inf
n→∞

E n[W, p] − C n =ιA(p) +

|∂x2 W(a, x2)|2 dx2 + ι∂x1 W=0(W)

u[p](x + W(x)) dx −

log max

x∈[0,a]×{s}

u[p](x + W(x)) dq∞( T

a s) ,

Γ − lim sup
n→∞

E n[W, p] − C n =ιA(p) +

|∂x2 W(a, x2)|2 dx2 + ι∂x1 W=0(W)

(cid:90) a

a
2DT

(cid:90) a

a
2DT

0

0

+

T
|Ω|

(cid:90)

Ω

+

T
|Ω|

(cid:90)

Ω

(cid:90) a

0

0

(cid:90) a

21

u[p](x + W(x)) dx −

log min

x∈[0,a]×{s}

u[p](x + W(x)) dq∞( T

a s) ,

a ·) is to be interpreted as the pushforward (image measure) of q∞ under s (cid:55)→ a

where ι∂x1 W=0(W) = 0 if W(·, x2) is constant for almost all x2 ∈ [0, a] and ι∂x1 W=0(W) = ∞ else and where
q∞( T
Proof. Without loss of generality let q∞ = ˆqλ∞ = (cid:80)m
such that each measurement interval [tn
electron arrives at time τj ∈ [0, T ] and is detected at the cn
row,

j=1 δτj . Furthermore, we only consider n large enough,
i ) only contains a single electron detection τj. Thus, the jth
(cid:109)th

(cid:109)th pixel in the rn

(cid:108) τj mod (N n
∆tn

i−1, tn

τj
1 ∆tn

1 ∆tn)

j =

j =

T s.

N n

(cid:108)

(cid:40)

(gn)i =

(gn
(gn

G)i + α if there exists a j with i = in
G)i

else.

j = rn

j N n

1 + cn
j

Now let W n (cid:42) W in H 1(Ω)2 and pn → p. Using Lemma 1 and the constant C n from Theorem 2

depending on the data, α, and σn, the data term is given by

Dn

data =

dn((gn)i, ∆tnu[pn](xn

i + W n(xn

i ))

N n
(cid:88)

i=1

= C n + O( N nσn

α ) −

N n
(cid:88)

i=1

(cid:115) (gn)i − µ
α

The limit as n → ∞ of the last sum is obtained as

(cid:123) log u[pn](xn

i + W n(xn

i )) +

∆tnu[pn](xn

i + W n(xn

i )) .

N n
(cid:88)

i=1

(cid:90)

T
|Ω|

Ω

N n
(cid:88)

i=1
N n
(cid:88)

i=1

(gn)i − µ
α

(gn

G)i − µ
α

T
a2

N n
1(cid:88)

N n
2(cid:88)

c=1

r=1

m
(cid:88)

j=1

∆tnu[pn](xn

i + W n(xn

i )) =

∆xn

1 ∆xn

2 u[pn](xn

cr + W n(xn

cr)) →

u[p](x + W(x)) dx

due to the uniform convergence of u[pn] and the convergence W n → W in L1(Ω) because of the compact
embedding H 1(Ω) (cid:44)→ L1(Ω), while the second sum can be written as

(cid:123) log u[pn](xn

i + W n(xn

i )) = O((cid:107) ˜Gn(cid:107)M([0,T ])) +

log u[pn](xn

i + W n(xn

i ))

N n
(cid:88)

i=1

N n
(cid:88)

i=1

(cid:115) (gn)i − µ
α

= O((cid:107) ˜Gn(cid:107)M([0,T ])) +

log u[pn](xn
in
j

+ W n(xn
in
j

)) +

log u[pn](xn

i + W n(xn

i ))

= O((cid:107) ˜Gn(cid:107)M([0,T ]) + (cid:107)Gn

G(cid:107)M([0,T ])) +

log u[pn]((cn

j ∆xn

1 , rn

j ∆xn

2 ) + W n(cn

j ∆xn

1 , rn

j ∆xn

2 )) .

m
(cid:88)

j=1

Now almost surely (with respect to the distribution of ˆq or equivalently (τ1, . . . , τm))

∆xn
∆xn

2 (rn
1 (cn

1 , . . . , rn
1 , . . . , cn

m) → a
m) = a
N n
1

T (τ1, . . . , τm) as n → ∞ , while

((cid:100)N n

1 (N n
2

T mod 1)(cid:101), . . . , (cid:100)N n
τ1

1 (N n
2

The latter statement follows from Lemma 3 noting a
N n
1
Therefore,

T mod 1)(cid:101)) , n ∈ N , is dense on [0, a]m .
τm
τj
T mod 1) + O( a
N n
1

τj
T mod 1)(cid:101) = a(N n

2

).

(cid:100)N n

1 (N n
2






lim sup
n→∞
lim inf
n→∞






m
(cid:88)

j=1

log u[pn]

(cid:19)

(cid:32)(cid:18)cn
j ∆xn
1
j ∆xn
rn
2

+ W n

(cid:18)cn
j ∆xn
1
j ∆xn
rn
2

(cid:19)(cid:33)

=






m
(cid:88)

j=1

sup
x1∈[0,a]

inf
x1∈[0,a]






log u[p]

+ W

(cid:19)

(cid:18)(cid:18) x1
aτj
T

(cid:19)(cid:19)

(cid:18) x1
aτj
T

so that with the notation q∞( T

j=1 δaτj /T (s) we obtain

lim inf
n→∞

Dn

data − C n =

u[p](x + W(x)) dx −

log max

x∈[0,a]×{s}

u[p](x + W(x)) dq∞( T

a s) ,

lim sup
n→∞

Dn

data − C n =

u[p](x + W(x)) dx −

log min

x∈[0,a]×{s}

u[p](x + W(x)) dq∞( T

a s) .

(cid:90)

a s) = (cid:80)m
T
|Ω|
T
|Ω|

(cid:90)

Ω

Ω

(cid:90) a

0
(cid:90) a

0

22

Setting W n(xn

1 1), the remaining terms of E n[W n, pn] can be expressed as
1 0) := W n(xn
N n
N n

˜E n = ιA(pn) +

1r) − W n(xn
N n

1 (r−1))|2 +

|W n(xn

cr) − W n(xn

(c−1)r)|2

N n
2(cid:88)

(cid:34)
|W n(xn

(cid:34)
|W n(xn

1
2D∆tn

1
2D∆tn

r=1
N n
2(cid:88)

r=1

N n
1(cid:88)

c=2

(cid:90) a

∆xn
1

(cid:35)

(cid:35)

= ιA(pn) +

1r) − W n(xn
N n

1 (r−1))|2 + ∆xn

1

|∂x1W n(x1, r∆xn

2 )|2 dx2

so that lim inf n→∞ ˜E n = ∞ unless

(cid:90) a

sup
x2∈[∆xn

2 ,a]

∆xn
1

|∂x1W n(x1, x2)|2 dx2 = sup

|∂x1W n(x1, r∆xn

2 )|2 dx2 → 0 ,

(cid:90) a

r=1,...,N n
2

∆xn
1

which due to W n (cid:42) W implies ∂x1W = 0 almost everywhere. Furthermore,

˜E n = ιA(pn) +

≥ ιA(pn) +

N n
1
2D∆tn

N n
1
2D∆tn

(cid:34)

N n
2(cid:88)

r=1
N n
2(cid:88)

r=1

= ιA(pn) +

= ιA(pn) +

∆xn
2
2D∆tnN n
1

∆xn
2
2D∆tnN n
1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
N n
2(cid:88)

r=1
(cid:90) a

0

1
N n
1

(cid:12)
(cid:12)W n(xn
(cid:12)

2

(cid:12)
1r) − W n(xn
(cid:12)
1 (r−1))
N n
(cid:12)
1 (r−1)) + (cid:80)N n

1

N n
1(cid:88)

+

1
N n
1

c=2
c=2 W n(xn

W n(xn

1r) − W n(xn
N n

N n
1

cr) − W n(xn

(c−1)r)

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

|W n(xn

cr) − W n(xn

(c−1)r)|2

(cid:35)

|W n(xn

1 r) − W n(xn
N n
N n
∆xn
2

1 (r−1))|2

|∂x2 W n(a, x2)|2 dx2 ,

where we used Jensen’s inequality. Due to the weak lower semi-continuity of the H 1-norm, we thus obtain
(cid:82) a
lim inf n→∞ ˜E n ≥ ιA(p) + a
0 |∂x2 W(a, x2)|2 dx2. In addition, this limit can even be achieved by choosing
2DT

pn = p , W n(xn

cr) = W(xn
1 (r−1)) +
N n

[W(xn

1 r) − W(xn
1 (r−1))] ,
N n
N n

c = 1, . . . , N n

1 , r = 1, . . . , N n
2 ,

c
N n
1

so that in above Jensen’s inequality we actually have an equality. Thus the non-data terms coincide in the
Γ − lim inf and the Γ − lim sup.

The disparity between Γ − lim inf and Γ − lim sup implies that the functional does not have a Γ-
limit as n → ∞ and thus the density and motion reconstruction problems do not converge. This does not
necessarily imply that our thought experiment with faster and faster row scanning is completely unreasonable
or unphysical; instead a reﬁnement of our stochastic coupling might be necessary to make sense of the limit.
Intuitively, in the limit each row is swept out in zero time so that the electron counts belonging to a row
In other words, q∞ can only capture
cannot be ascribed a particular horizontal position along the row.
information on the vertical, but not on the horizontal location of the detected electrons. Resolving such
additional information requires the use of a diﬀerent, more involved stochastic coupling. Instead of pursuing
that route, we shall propose a slight model change below in which a measurement only reﬂects the average
of the material density along each row.

Remark 3 (Tomographic scanning models). One may adapt our reconstruction model and in particular the
data term by only estimating the average material density per row. The new energy then reads

En[w, p] =

dn

(gn)(r−1)N n

1 +c,

∆tnu[p](xn

cr + w(r−1)N n

1 +c)

 +

N n
2(cid:88)

r=1





N n
1(cid:88)

c=1

N n
1(cid:88)

c=1



1
2D

N n
(cid:88)

i=1

|wi − wi−1|2
∆tn

+ ιA(p) ,

23

and for its continuous version

E n[W, p] =

En[w, p]






∞

else

if W : Ω → R2 is the piecewise bilinear interpolation of the
1 1, xn
21, . . . , xn
N n

values w = (w1, . . . , wN n ) at positions (xn

11, xn

12, . . . , xn
N n

1 N n
2

)

one expects to obtain by a similar proof as above that Γ − limn→∞ E n − C n = E ∞ for

E ∞[W, p] = dKL

q∞, t (cid:55)→

u[p]((x1, vt) + W(x1, vt)) dx1

(cid:18)

1
a

(cid:90) a

0

(cid:19)

+

a
2DT

(cid:90) a

0

|∂x2W(a, x2)|2 dx2 + ι∂x1 W=0(W) + ιA(p) .

From the physical viewpoint, the limit problem would only allow to extract information of the average material
density u along each horizontal line. However, if such information were obtained for many diﬀerent rotations
of the sample, then this would correspond to sampling the Radon transform of the material density at a number
of angles. The sought material density could thus be reconstructed by standard tomographic techniques as for
instance used in computerized tomography.

4. An auxiliary convex model for atom identiﬁcation

To solve our variational problem numerically, we require the number of atoms and their approximate positions
as initialization. To this end, we reduce our complex model to a smaller auxiliary optimization problem,
which is desigend to be convex so that the global minimizer can be found. This identiﬁcation of the global
optimum is important, since otherwise the non-convex main model would easily get stuck in local minima,
compromising the accuracy of our material density reconstruction.

4.1. Lifting atom positions in measure space

To specify an appropriate model, note that it does not have to accurately describe all parts of the image
acquisition, but rather should be as simple as possible. Thus, we shall here assume that a single atom at
position y is described by a response function

cb(· − y) ,

b ﬁxed, e. g. b(x) = exp

, and c > 0.

(cid:19)

(cid:18) −|x|2
2ω2

In contrast to before, each atom now just is represented by two parameters, position y and height c, while
the atom width ω is ﬁxed a priori by the user to a reasonable value. Representing the distribution of atoms
with positions yl and heights cl via a sum of weighted Dirac measures,

the corresponding image or material density can be expressed as the convolution

We now introduce two further simpliﬁcations compared to our main model from Section 2.

• We will ignore the Brownian motion leading to misplaced pixels.

Instead, we interpret the slightly
changed pixel intensities as noise rather than pixel displacements. Since the smooth material density
u can locally be approximated by an aﬃne function, the change in pixel intensity due to Brownian
motion-induced pixel displacement becomes visible as Gaussian noise (whose variance actually depends
on the local slope of u, but will in the following be assumed ﬁxed for simplicity). The additional mixed

h =

clδyl ,

L
(cid:88)

l=1

u = h ∗ b .

24

Poisson-Gaussian noise inherent in the signal detection (described in Section 2.5) is typically of a much
smaller size and thus will be ignored. Therefore we will use the for Gaussian noise appropriate quadratic
data term

N
(cid:88)

i=1

d(gi, u(xi)) with d(g, z) = |z − g|2 .

• Instead of optimizing over the atom positions yl, which would be a highly non-convex optimization,
we optimize directly for the measure h. In other words, we lift the vector of unknowns to a measure,
thereby allowing a convex optimization. Since this way the discrete nature of the atom positions is no
longer strictly enforced, we have to add a regularization to our model that promotes spatial sparseness
of the measure h. L1-type norms are widely used for this purpose, so we shall additionally penalize the
total mass of h.

Summarizing, we shall solve the variational model

min
h∈M+(Ω)

F(h)

for F(h) =

|(h ∗ b)(xi) − gi|2 + η(cid:107)h(cid:107)M+(Ω) ,

N
(cid:88)

i=1

where M+(Ω) denotes the set of nonnegative Radon measures and η is some positive weight.
It is
straightforward to prove the existence of a minimizer h via the direct method of the calculus of variations.

4.2. Extracting atoms from the lifting

The resulting minimizer h will only approximately represent a linear combination of Dirac measures so that
some postprocessing is required to extract the atom positions and heights. In detail, we identify all connected
components Cl ⊂ Ω, l = 1, . . . , L, of the support of h (which is readily done with a computational complexity
proportional to the number of discretization points) and deﬁne the atom positions as

Furthermore, we set the atom heights to

yl =

x dh(x)

dh(x) ,

l = 1, . . . , L .

(cid:90)

Cl

(cid:30) (cid:90)

Cl

cl =

dh(x) +

b(yl − xi)2 ,

l = 1, . . . , L .

(cid:90)

Cl

η
2

(cid:30) N
(cid:88)

i=1

The latter equation is motivated by the simple fact that for a single atom ground truth, gi = cb(y − xi), if
atom position y is known, then h = (c − η
i=1 b(y − xi)2)δy minimizes the energy F among all multiples of
δy. Finally, in order to eliminate atoms that were just introduced by the minimization in order to reproduce
background variations of the image, we simply remove all atoms with height below a manually speciﬁed
threshold.

2 / (cid:80)N

4.3. Numerical optimization by a semi-smooth Newton method

According to the ﬁrst order optimality conditions for minimizing the convex functional F, the subdiﬀerential
∂F(h) must contain 0. Denoting by ιM+(Ω) the indicator function of M+(Ω), we thus obtain

0 ∈

2((h ∗ b)(xi) − gi)b(xi − ·) + η + ∂ιM+(Ω)(h)

N
(cid:88)

i=1

or equivalently

−β ∈ ∂ιM+(Ω)(h)

for β(x) =

2h((h ∗ b)(xi) − gi)b(xi − x) + η

N
(cid:88)

i=1

25

and thus (denoting by (cid:104)·, ·(cid:105) the dual pairing between M(Ω) and its dual, and interpreting β as element of
M(Ω)(cid:48) via (cid:104)β, ˜h(cid:105) = (cid:82) βd˜h)

(cid:104)β, ˜h − h(cid:105) ≥ 0

for all ˜h ∈ M+(Ω) .

Since β is smooth, this is equivalent to β ≥ 0 and β = 0 on spt(h). Thus, for any γ > 0 we have

0 = h − max(0, h − γβ) = h − max

0, h − γ

((h ∗ b)(xi) − gi)b(xi − ·) + η

,

(cid:32)

(cid:34)
2

N
(cid:88)

i=1

(cid:35)(cid:33)

to be solved for the measure h, where max(0, ·) denotes the positive part of a measure according to the Hahn
decomposition theorem.

The aim is to solve the above equation via Newton’s method, however, the equation is well-known to
lack the necessary semi-smoothness on the space M(Ω) of Radon measures (see e. g. [9, Lem. 2.7] for the
corresponding argument in the Lq-setting). Note, though, that we may very coarsly discretize the measure h
as we do not require high accuracy for the solution of the auxiliary problem. In fact, we only have to resolve
approximately half an atom width in order to still be able to identify all atoms. Thus, we choose a sqaure
grid on Ω of grid width ∆x and deﬁne χkl to be the characteristic function on the square in the kth row and
lth column. Let

h =

hklξkl

for ξkl =

χkl
∆x2

and denote the vector of coeﬃcients hkl by h. Again deriving the optimality conditions for h we arrive at

0 = hkl−max

0, hkl − γ

2

hmn

ξmn(x)b(xi − x) dx − gi

ξkl(x)b(xi − x) dx + η

=: F (h)kl

(cid:33) (cid:90)

R2

(cid:35)(cid:33)

(cid:32)

(cid:34)

(cid:32)

N
(cid:88)

(cid:88)

i=1

m,n

for all k, l. This is now solved for h using a semi-smooth Newton method, where the Newton equation

(cid:88)

k,l

(cid:90)

R2

DF (hold)(hold − hnew) = F (hold)

in each iteration is solved by a few steps of the GMRES method. Here, the generalized diﬀerential of F is
given by

D(F (h)kl)mn =

(cid:40)

δkl,mn
2γ (cid:80)N
i=1

(cid:82)

R2 ξmn(x)b(xi − x) dx (cid:82)

R2 ξkl(x)b(xi − x) dx else.

if F (h)kl = hkl

Note that in case of a scanning path with scan positions at points xij of the same rectangular grid, the

necessary computations can eﬃciently be performed using the fast Fourier transform (FFT). Indeed, let

b−i,−j =

ξij(x)b(x00 − x) dx

for all i, j ∈ Z , b = (bij)i,j∈Z ,

where x00 uses the canonical of our grid indexing, i. e. x00 := x11 − (∆x, ∆x). Then, we have

hmn

ξmn(x)b(xij − x) dx =

hmnbi−m,j−n = (h ∗ b)ij ,

(cid:88)

m,n

and this discrete convolution can be computed eﬃciently using FFT. Furthermore, F becomes

F (h)kl = hkl − max (cid:0)0, hkl − γ (cid:2)2((h ∗ b − g) ∗ ¯b)kl + η(cid:3)(cid:1)

for g = (gij)i,j∈N ,

¯bij = (b−i,−j)i,j∈N ,

with

(cid:40)

D(F (h)kl)mn =

δkl,mn
2γ(b ∗ ¯b)k−m,l−n

if F (h)kl = hkl
else.

In practice, the method robustly identiﬁes all atoms within an image with a computation time far below
that of the full model (tens of seconds). An example is provided in Figure 2, where the atom locations are
extracted from a STEM image of GaN.

(cid:90)

R2

(cid:90)

R2

(cid:88)

m,n

26

Figure 2. From left to right: STEM image of GaN (courtesy of Paul M. Voyles), computed atom distribution
h, resulting initial guess for the material density u.

5. Numerical implementation

While in the previous sections, for notational simplicity we have mainly considered the situation of a single
input image g, we shall from now on consider the practically more relevant case of using the model from
Section 2.9 with multiple input measurements g1, . . . , gK.

5.1. Necessary preprocessing of input images

1 , . . . , yini

In case of multiple input measurements, the images g1, . . . , gK typically do not all show the same region of
the material sample. This needs to be corrected for via a data preprocessing step. Applying the approach
L , ωini, oini) for the parameters
from Section 4 on g1, we get an initial guess pini = (yini
of u[p]. Then, the input images g2, . . . , gK are aligned with g1 using standard image alignment techniques,
thus bringing all images into the coordinate system of g1.
In our experiments we do so by determining
the optimal translation (as integer pixel shift) using the series registration strategy from [1] but with a
deformation model that just allows translations. Since the shifted images are not deﬁned at those pixels that
have no correspondence in their original coordinate system, the aligned images are cropped to their common
support. For the sake of simplicity, the aligned, cropped images are again denoted by g1, . . . , gK and their
size by N1 × N2. The initial center positions yini
L are adjusted accordingly. Furthermore, we remove
all atoms from pini which lie far enough outside the new, cropped image domain Ω such that they at most
contribute a value of 10−8 to u[pini] inside Ω (those are exactly the atoms with center further away from

1 , . . . , yini

1 . . . , cini

L , cini

(cid:114)

(cid:16) 10−8
||g1||∞

(cid:17)

Ω than

−2(ωini)2 log

). In diﬀerent words, we keep atoms at positions that are very close to but

slightly outside of the domain. This way, we avoid that atoms inside Ω try to compensate for atoms outside
Ω that are neglected (since they are not visible in Ω) but still account for a few deﬂected electron counts
measured inside Ω.

5.2. Additional, heuristic model modiﬁcations

Since the atoms in pini but outside Ω are not visible in the cropped images, we will penalize them to stay
close to their initially estimated position using the penalty
(cid:2)(cl − cini

l )2 + |yl − yini
l

P [p] =

|2(cid:3) ,

(cid:88)

νpen
2

yini
l /∈Ω

where νpen > 0 is a constant weighting factor.

Using the STEM rastering pattern (2) and deﬁning the initial accumulated Brownian motion as zero,

the Brownian motion term in (3) for w ∈ (R2)N1×N2 becomes


R1[w] =

1
2D

|w11|2
∆t



+

(cid:32)

N2(cid:88)

j=2

|w1j − wN1(j−1)|2
∆T

+

N1(cid:88)

i=2

|wij − w(i−1)j|2
∆t

(cid:33)
 .

27

During the numerical experiments, we found that penalizing the squared norm of the random motion wij
with a small weight noticeably improves the convergence speed without inﬂuencing the results much. The
reason lies in the slightly stronger local convexity of the resulting energy functional. Indeed, without this
penalization a constant shift of all wij by some z ∈ R2 could be compensated for via shifting all atoms by z
as well, indicating a lack of local positive deﬁniteness of the energy Hessian. Thus, we add the regularizer

R2[w] =

(cid:0)νhor|(wij)1|2 + νvert|(wij)2|2(cid:1) ,

1
2

N1(cid:88)

N2(cid:88)

i=1

j=1

where (wij)1 and (wij)2 denote the horizontal and vertical component of wij ∈ R2 and νhor, νvert > 0 are
constant weighting factors. Depending on the input data, it may even be beneﬁcial to use a noticeably larger
value for one of the weights. For instance, the data shown in Figure 3 has numerous pixel rows that contain
almost no signal. Hence, the energy landscape is very ﬂat with respect to the vertical shift of these rows so
that during optimization this vertical shift often ends up ﬁtting the noise (note that this is not a problem
of our underlying model, but rather a general problem of MAP estimates in regions of relatively constant
probability density). This can be prevented by an increased value of νvert.

In summary, for numerical experiments we employ the following heuristically improved version of (3),

EK[w1, . . . , wK, p] =

K
(cid:88)

(cid:20) N1(cid:88)

N2(cid:88)

k=1

i=1

j=1

d(u[p](xij + wk

(cid:21)
ij), (gk)ij) + R1[wk] + R2[wk]

+ P [p] .

5.3. Numerical optimization

Note that P [p], R1[w] and R2[w] are quadratic in their arguments, so the ﬁrst and second derivatives can be
computed easily. Using z = xij + wk
ij to shorten the notation, and calling the data term D, the derivatives
of D are as follows:

∇pD[w1, . . . , wK, p] =

∂1d(u[p](z), (gk)ij)∇pu[p](z)

K
(cid:88)

N1(cid:88)

N2(cid:88)

k=1

i=1

j=1

D[w1, . . . , wK, p] = ∂1d(u[p](z), (gk)ij)∇xu[p](z)

∇wk

ij

∇2
wk
ij

D[w1, . . . , wK, p] = ∂2

1 d(u[p](z), (gk)ij)∇xu[p](z) ⊗ ∇xu[p](z)
+ ∂1d(u[p](z), (gk)ij)∇2

xu[p](z)

1 d(u[p](z), (gk)ij)∇xu[p](z) ⊗ ∇pu[p](z)
+ ∂1d(u[p](z), (gk)ij)∇x∇pu[p](z)

∇wk

ij

∇pD[w1, . . . , wK, p] = ∂2

∇2

pD[w1, . . . , wK, p] =

K
(cid:88)

N1(cid:88)

N2(cid:88)

(cid:20)
1 d(u[p](z), (gk)ij)∇pu[p](z) ⊗ ∇pu[p](z)
∂2

k=1

i=1

j=1

+ ∂1d(u[p](z), (gk)ij)∇2

(cid:21)
pu[p](z)

The functional EK is minimized using the trust region Newton method from [5]. In particular, for the trust
region subproblem, we chose to implement the algorithm proposed in [5, Algorithm 7.3.4], making use of a
Cholesky factorization of the energy Hessian and an eigendirection-based approach to bypass saddle points,
where the Cholesky factorization is performed using the CHOLMOD package from Davis et al. [4]. To start
this method with an initial guess p slightly improved over pini, we proceed as follows.

• Find p by minimizing (cid:80)N1
i=1

j=1 d(u[p](xij), (g1)ij) + P [p] with respect to p using pini as initial value,
but keeping the shared bump width ω ﬁxed. To this end we use the computationally rather cheap BFGS
quasi-Newton method.

(cid:80)N2

28

• Further reﬁne p by minimizing (cid:80)K

(cid:80)N1
i=1

(cid:80)N2

j=1 d(u[p](xij), (gk)ij)

k=1

+ P [p] with respect to p using

(cid:20)

(cid:21)

the trust region Newton method.

• Improve the initial guess w1 = . . . = wK = 0 and reﬁne p by minimizing EK[w1, . . . , wK, p] using the

trust region Newton method while enforcing that wk are constant in each pixel row.

Finally, we can minimize the whole functional EK using the trust region Newton method starting from the
initial guess obtained above.

6. Experiments on synthetic and real data

All numerical experiments shown in this sections were conducted with the following reconstruction
parameters: D = 0.1, ∆t = 1/ max(N1 − 1, N2 − 1), ∆T = 1000∆t, νpen = 0.05, νhor = 0.1 and νvert = 10.
Note that the relatively large value of ∆T is chosen to compensate for the so-called ﬂyback error: when the
electron probe is instructed to jump back from the end of one scan line to the beginning of the next line,
the probe will not be positioned exactly where it is supposed to be. The position error from this eﬀect is
larger than the error caused by the Brownian motion during the time between ending one line and beginning
the scan in the next line. However, it can be modeled as Brownian motion during a larger, ﬁcticious time
interval ∆T for which reason we did not explicitly include the ﬂyback error in our forward model.

6.1. Synthetic data generation and experiments

To test the eﬀectiveness of the proposed method, we ﬁrst apply it to synthetic data for which the ground
truth is known. To this end, we consider a ground truth image u with Gaussian bump functions of height 45
and standard deviation of 3 pixel units, arranged in a hexagonal lattice of 19.37 pixel units lattice spacing.
We subsequently discretize the image into 256 × 256 pixels, however, the position of the ith pixel (counting
row-wise) is displaced by a vector wi = (cid:80)i
ˆ∆j. Here, ∆1, ∆2, . . . are random numbers drawn
j=1
from a normal distribution with mean 0 and standard deviation of 0.05 pixel units, while ˆ∆1, ˆ∆2, . . . represent
the Brownian motion happening between two pixel rows and thus are drawn from a normal distribution with
mean 0 and a larger standard deviation of 1 pixel unit. Finally, a constant background of o = 40 is added, and
the image is corrupted by Poisson noise. In this way, a series of 128 randomly corrupted images g1, . . . , g128
is generated. Figure 3 displays the ﬁrst synthetic image g1 next to a real image acquired by STEM, showing
good qualitative agreement.

j=1 ∆j + (cid:80)i/256

Figure 4 shows the reconstruction results of our algorithm for four synthetic input images (left column);
in addition to u[p] (bottom row), we display a color coding of the computed random pixel displacements wk
(middle column) as well as those displacements applied to u[p] (right column), which should represent the
input images without the Poisson noise in each pixel. While the detected random displacement may vary
considerably between consecutive pixel rows, it seems to be relatively constant along each row. However, the
plot of the displacement along a few selected pixel rows in Figure 5 reveals a Brownian-motion-like variation
along a row as well.

Using the synthetic data, we now perform the following series of experiments: For each k = 0, . . . , 6
we split the set of 128 synthetic input images into 27−k groups of each 2k images. To each such group we
apply our reconstruction algorithm. The quality of the result is measured via the so-called horizontal and
vertical precision, which is the standard deviation of the horizontal and vertical atom distances and which
should ideally be zero. The bottom right image of Figure 4 illustrates which distances are used to compute
the precision. To avoid confusion let us emphasize that in this terminology low precision values correspond
to high accuracy (so that the term “precision” may be a bit misleading). We choose the precision as quality
measure, since it is already well-established [19] and also applicable to measurements without ground truth.
Figure 6 shows the horizontal and vertical precision, averaged over the performed experiments, as a function
of the number K of input images. The precision decreases roughly like K −1/2, which is the expected rate
if the atom positions in each input image are displaced by Gaussian noise. Note that the vertical precision
almost is up to three times worse than the horizontal one. This is expected, since the hexagonal grid of our
synthetic images is aligned such that each pixel row traverses several atoms. Thus, the random displacement

29

Figure 3. Synthetic image created via our proposed image formation model (left) and an experimental
STEM image of GaN (right, courtesy of Paul M. Voyles).

changes much less between horizontally neighboring atoms than between vertically neighboring atoms (which
lie in diﬀerent pixel rows and thus have a much stronger random displacement between them). Moreover,
the absolute vertical distance of atom pairs is more than twice as big as the horizontal distance in this grid
(cf. bottom right of Figure 4), which means that the absolute precisions should not be compared directly.
Nevertheless, even the vertical precision reaches values of 0.1 pixels accuracy at 64 input images.

6.2. Experiments on real data

Figures 7 to 9 show the same as Figures 4 to 6, only this time for real data acquired by STEM from a GaN
material sample (experimental data courtesy of Paul M. Voyles). For this data, the precision reaches a
slightly higher value of 0.15 pixels for 64 input images, which at the used STEM resolution corresponds to
about 2.25 pm (pixel size in this case is about 15 pm). Interestingly, the rate of precision decrease is lower
than the expected K −1/2 from the synthetic experiments. Preliminary tests with diﬀerent experimental data
sets suggest that the origin of this inferior rate lies in the input data rather than the reconstruction method,
since it turns out that the better rate can be restored if the input images g1, . . . , gK are taken from diﬀerent
material regions (which nevertheless show the exactly same hexagonal atomic grid). This fact actually speaks
in favor of the reconstruction method, since it is apparently even able to identify hidden deviations of the
measurements from a perfectly regular atomic grid.

7. Conclusions

Using a Bayesian approach, we have derived a variational method to extract atom positions and random
pixel displacement from STEM images. With the help of tools from stochastic homogenization it turns
out that one can also make sense of this reconstruction method if the electron beam traverses the scanned
material in a time-continuous manner, constantly detecting deﬂected electrons. Depending on the scanning
path this may yield reconstructions based on space-ﬁlling measurements (that is, measurements at every
single location in the 2D scanning domain Ω) or on tomography-type measurements. To our knowledge,
such approaches and scanning paths are not yet investigated in the STEM community, and it would be
interesting to test how well they could be realized and what applications might beneﬁt from them.

For a numerical implementation, we ﬁrst derived a reduced, convex model that was used to provide an

30

gk

wk

u[p](xij + wk

ij)

k = 1

k = 2

k = 3

k = 4

u[p] − o

u[p]

(cid:102)

31

Figure 4. Given four synthetic input images g1, . . . , g4 (left column), our algorithm reconstructs the correct
atom distribution u[p] (bottom row) as well as the random pixel displacement due to Brownian motion
(middle column, color-coded according to the color wheel). The displacements applied to the reconstruction
are shown as well (right column).

Figure 5. Plots of the horizontal (black) and vertical (gray) component of w1 from Figure 4 for selected
pixel rows. The displacement is shown in pixels.

Figure 6. Average horizontal and vertical precision (in pixels) of the reconstructed atom positions as a
function of the number of synthetic input images.

initialization for the full reconstruction method. In our eyes, this (not new) idea is useful in inverse problems
beyond STEM reconstruction: Since accurate forward models often are complex and nonlinear, variational
reconstructions may easily get stuck in suboptimal local minima; eﬃcient convex models can help out by
providing a good, globally optimized initial guess.

Despite using a relatively accurate model of STEM imaging, choosing model parameters for a particular
type of experimental images still requires a little tuning (which is not problematic from the application
viewpoint, since once the parameters are chosen one can use them for a large series of physical experiments).
In particular, some additional heuristic regularization seems to be beneﬁcial. This may be a manifestation of
one of the well-known deﬁciencies of the MAP estimate for inverse problems: If the energy landscape is very
ﬂat, the MAP estimate may tend to overﬁt noise, which can be counteracted by additional regularization.

Acknowledgements

B. Berkels was funded in part by the Excellence Initiative of the German Federal and State Governments.
B. Wirth’s research was supported by the Alfried Krupp Prize for Young University Teachers awarded
by the Alfried Krupp von Bohlen und Halbach-Stiftung. The work was also supported by the Deutsche
Forschungsgemeinschaft (DFG), Cells-in-Motion Cluster of Excellence (EXC1003 – CiM), University of
M¨unster, Germany.
[1] Benjamin Berkels, Peter Binev, Douglas A. Blom, Wolfgang Dahmen, Robert Sharpley, and Thomas Vogt. Optimized

imaging using non-rigid registration. Ultramicroscopy, 138:46–56, March 2014.

[2] Peter Binev, Francisco Blanco-Silva, Douglas Blom, Wolfgang Dahmen, Philipp Lamby, Robert Sharpley, and Thomas Vogt.
High quality image formation by nonlocal means applied to high-angle annular darkeld scanning transmission electron

32

gk

wk

u[p](xij + wk

ij)

k = 1

k = 2

k = 3

k = 4

u[p] − o

u[p]

(cid:102)

33

Figure 7. Given four STEM images g1, . . . , g4 of GaN (left column), our algorithm reconstructs the correct
atom distribution u[p] (bottom row) as well as the random pixel displacement due to Brownian motion
(middle column, color-coded according to the color wheel). The displacements applied to the reconstruction
are shown as well (right column).

Figure 8. Plots of the horizontal (black) and vertical (gray) component of w1 from Figure 7 for selected
pixel rows. The displacement is shown in pixels.

Figure 9. Average horizontal and vertical precision (in pixels) of the reconstructed atom positions as a
function of the number of STEM input images.

microscopy (HAADF-STEM).
Imaging in Electron Microscopy, pages 127–145. Springer, 2012.

In Thomas Vogt, Peter Binev, and Wolfgang Dahmen, editors, Modeling Nanoscale

[3] Kristian Bredies and Hanna Katriina Pikkarainen. Inverse problems in spaces of measures. ESAIM Control Optim. Calc.

Var., 19(1):190–218, 2013.

[4] Y. Chen, T. A. Davis, W. W. Hager, and S. Rajamanickam. Algorithm 887: CHOLMOD, supernodal sparse Cholesky

factorization and update/downdate. ACM Transactions on Mathematical Software, 35(3):22:1–22:14, 2009.

[5] A. R. Conn, N. I. M Gould, and P. L. Toint. Trust-Region Methods. SIAM, 2000.
[6] A. De Backer, K. H. W. van den Bos, W. Van den Broek, J. Sijbers, and S. Van Aert. StatSTEM: An eﬃcient approach
for accurate and precise model-based quantiﬁcation of atomic resolution electron microscopy images. Ultramicroscopy,
2016.

[7] Vincent Duval and Gabriel Peyr´e. Exact support recovery for sparse spikes deconvolution. Found. Comput. Math.,

15(5):1315–1355, 2015.

[8] Alessandro Foi, Mejdi Trimeche, Vladimir Katkovnik, and Karen Egiazarian. Practical poissonian-gaussian noise modeling

and ﬁtting for single-image raw-data. IEEE Transactions on Image Processing, 17(10):1737–1754, 2008.

[9] M. Hinze, R. Pinnau, M. Ulbrich, and S. Ulbrich. Optimization with PDE constraints, volume 23 of Mathematical

Modelling: Theory and Applications. Springer, New York, 2009.

[10] Lewys Jones and Peter D. Nellist.

Identifying and correcting scan noise and drift in the scanning transmission electron

microscope. Microscopy and Microanalysis, 19:1050–1060, August 2013.

[11] Lewys Jones, Hao Yang, Timothy J. Pennycook, Matthew S. J. Marshall, Sandra Van Aert, Nigel D. Browning, Martin R.
Castell, and Peter D. Nellist. Smart align - a new tool for robust non-rigid registration of scanning microscope data.
Advanced Structural and Chemical Imaging, 1, 2015.

[12] Koji Kimoto, Toru Asaka, Xiuzhen Yu, Takuro Nagai, Yoshio Matsui, and Kazuo Ishizuka. Local crystal structure analysis
with several picometer precision using scanning transmission electron microscopy. Ultramicroscopy, 110(7):778–82, June
2010.

[13] J. F. C. Kingman. Poisson processes, volume 3 of Oxford Studies in Probability. The Clarendon Press, Oxford University

Press, New York, 1993. Oxford Science Publications.

[14] Earl J. Kirkland. Advanced Computing in Electron Microscopy. Springer, 2010.

34

[15] Peter M. Lee. Bayesian Statistics: An Introduction. Wiley, 4 edition, 2012.
[16] R.-D. Reiss. A course on point processes. Springer Series in Statistics. Springer-Verlag, New York, 1993.
[17] Hermann Thorisson. Coupling, stationarity, and regeneration. Probability and its Applications (New York). Springer-

Verlag, New York, 2000.

[18] Eric K. van Douwen. Fubini’s theorem for null sets. Amer. Math. Monthly, 96(8):718–721, 1989.
[19] Andrew B. Yankovich, Benjamin Berkels, Wolfgang Dahmen, Peter Binev, Sergio I. Sanchez, Steven A. Bradley, Ao Li,
Izabela Szlufarska, and Paul M. Voyles. Picometre-precision analysis of scanning transmission electron microscopy
images of platinum nanocatalysts. Nature Communications, 5, June 2014.

35

