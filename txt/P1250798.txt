Deep Back-Projection Networks For Super-Resolution

Muhammad Haris1, Greg Shakhnarovich2, and Norimichi Ukita1,
1Toyota Technological Institute, Japan 2Toyota Technological Institute at Chicago, United States
{mharis, ukita}@toyota-ti.ac.jp, greg@ttic.edu

8
1
0
2
 
r
a

M
 
7
 
 
]

V
C
.
s
c
[
 
 
1
v
5
3
7
2
0
.
3
0
8
1
:
v
i
X
r
a

Abstract

The feed-forward architectures of recently proposed
deep super-resolution networks learn representations of
low-resolution inputs, and the non-linear mapping from
those to high-resolution output. However, this approach
does not fully address the mutual dependencies of low- and
high-resolution images. We propose Deep Back-Projection
Networks (DBPN), that exploit iterative up- and down-
sampling layers, providing an error feedback mechanism
for projection errors at each stage. We construct mutually-
connected up- and down-sampling stages each of which
represents different types of image degradation and high-
resolution components. We show that extending this idea
to allow concatenation of features across up- and down-
sampling stages (Dense DBPN) allows us to reconstruct
further improve super-resolution, yielding superior results
and in particular establishing new state of the art results
for large scaling factors such as 8× across multiple data
sets.

1. Introduction

Signiﬁcant progress in deep learning for vision [15, 13,
5, 40, 27, 34, 17] has recently been propagating to the ﬁeld
of super-resolution (SR) [20, 30, 6, 12, 21, 22, 25, 43].

Single image SR is an ill-posed inverse problem where
the aim is to recover a high-resolution (HR) image from
a low-resolution (LR) image. A currently typical ap-
proach is to construct an HR image by learning non-linear
LR-to-HR mapping, implemented as a deep neural net-
work [6, 7, 38, 25, 22, 23, 43]. These networks compute
a sequence of feature maps from the LR image, culminat-
ing with one or more upsampling layers to increase reso-
In contrast to
lution and ﬁnally construct the HR image.
this purely feed-forward approach, human visual system is
believed to use a feedback connection to simply guide the
task for the relevant results [9, 24, 26]. Perhaps hampered
by lack of such feedback, the current SR networks with only
feed-forward connections have difﬁculty in representing the
LR to HR relation, especially for large scaling factors.

Figure 1. Super-resolution result on 8× enlargement. PSNR: Lap-
SRN [25] (15.25 dB), EDSR [31] (15.33 dB), and Ours (16.63 dB)

On the other hand, feedback connections were used ef-
fectively by one of the early SR algorithms, the iterative
back-projection [18]. It iteratively computes the reconstruc-
tion error then fuses it back to tune the HR image intensity.
Although it has been proven to improve the image quality,
the result still suffers from ringing effect and chessboard ef-
fect [4]. Moreover, this method is sensitive to choices of
parameters such as the number of iterations and the blur op-
erator, leading to variability in results.

Inspired by [18], we construct an end-to-end trainable
architecture based on the idea of iterative up- and down-
sampling: Deep Back-Projection Networks (DBPN). Our
networks successfully perform large scaling factors, as
shown in Fig. 1. Our work provides the following contri-
butions:
(1) Error feedback. We propose an iterative error-
correcting feedback mechanism for SR, which calculates
both up- and down-projection errors to guide the reconstruc-
tion for obtaining better results. Here, the projection errors
are used to characterize or constraint the features in early
layers. Detailed explanation can be seen in Section 3.
(2) Mutually connected up- and down-sampling stages.
Feed-forward architectures, which is considered as a one-
way mapping, only map rich representations of the input to

1

Figure 2. Comparisons of Deep Network SR. (a) Predeﬁned upsampling (e.g., SRCNN [6], VDSR [22], DRRN [43]) commonly uses
the conventional interpolation, such as Bicubic, to upscale LR input images before entering the network. (b) Single upsampling (e.g.,
FSRCNN [7], ESPCN [38]) propagates the LR features, then construct the SR image at the last step. (c) Progressive upsampling uses
a Laplacian pyramid network which gradually predicts SR images [25]. (d) Iterative up and downsampling approach is proposed by our
DBPN which exploit the mutually connected up- (blue box) and down-sampling (gold box) stages to obtain numerous HR features in
different depths.

the output space. This approach is unsuccessful to map LR
and HR image, especially in large scaling factors, due to
limited features available in the LR spaces. Therefore, our
networks focus not only generating variants of the HR fea-
tures using upsampling layers but also projecting it back to
the LR spaces using downsampling layers. This connection
is shown in Fig. 2 (d), alternating between up- (blue box)
and down-sampling (gold box) stages, which represent the
mutual relation of LR and HR image.
(3) Deep concatenation. Our networks represent different
types of image degradation and HR components. This abil-
ity enables the networks to reconstruct the HR image using
deep concatenation of the HR feature maps from all of the
up-sampling steps. Unlike other networks, our reconstruc-
tion directly utilizes different types of LR-to-HR features
without propagating them through the sampling layers as
shown by the red arrow in Fig. 2 (d).
(4) Improvement with dense connection. We improve the
accuracy of our network by densely connected [15] each up-
and down-sampling stage to encourage feature reuse.

2. Related Work

2.1. Image super-resolution using deep networks

Deep Networks SR can be primarily divided into four

types as shown in Fig. 2.

(a) Predeﬁned upsampling commonly uses interpola-
tion as the upsampling operator to produce middle resolu-
tion (MR) image. This schema was ﬁrstly proposed by SR-
CNN [6] to learn MR-to-HR non-linear mapping with sim-
ple convolutional layers. Later, the improved networks ex-
ploited residual learning [22, 43] and recursive layers [23].
However, this approach might produce new noise from the

MR image.

(b) Single upsampling offers simple yet effective way
to increase the spatial resolution. This approach was pro-
posed by FSRCNN [7] and ESPCN [38]. These methods
have been proven effective to increase the spatial resolu-
tion and replace predeﬁned operators. However, they fail
to learn complicated mapping due to limited capacity of the
networks. EDSR [31], the winner of NTIRE2017 [44], be-
longs to this type. However, it requires a large number of
ﬁlters in each layer and lengthy training time, around eight
days as stated by the authors. These problems open the op-
portunities to propose lighter networks that can preserve HR
components better.

(c) Progressive upsampling was recently proposed in
LapSRN [25]. It progressively reconstructs the multiple SR
images with different scales in one feed-forward network.
For the sake of simpliﬁcation, we can say that this network
is the stacked of single upsampling networks which only
relies on limited LR features. Due to this fact, LapSRN is
outperformed even by our shallow networks especially for
large scaling factors such as 8× in experimental results.

(d) Iterative up and downsampling is proposed by our
networks. We focus on increasing the sampling rate of SR
features in different depths and distribute the tasks to calcu-
late the reconstruction error to each stage. This schema en-
ables the networks to preserve the HR components by learn-
ing various up- and down-sampling operators while gener-
ating deeper features.

2.2. Feedback networks

Rather than learning a non-linear mapping of input-
to-target space in one step, the feedback networks com-
pose the prediction process into multiple steps which al-

low the model to have a self-correcting procedure. Feed-
back procedure has been implemented in various computing
tasks [3, 35, 47, 29, 49, 39, 32].

In the context of human pose estimation, Carreira et
al. [3] proposed an iterative error feedback by iteratively
estimating and applying a correction to the current estima-
tion. PredNet [32] is an unsupervised recurrent network to
predictively code the future frames by recursively feeding
the predictions back into the model. For image segmenta-
tion, Li et al. [29] learn implicit shape priors and use them to
improve the prediction. However, to our knowledge, feed-
back procedures have not been implemented to SR.

2.3. Adversarial training

Adversarial training, such as with Generative Adversar-
ial Networks (GANs) [10] has been applied to various im-
age reconstruction problems [28, 37, 34, 5, 20]. For the
SR task, Johnson et al. [20] introduced perceptual losses
based on high-level features extracted from pre-trained net-
works. Ledig et al. [28] proposed SRGAN which is consid-
ered as a single upsampling method. It proposed the natu-
ral image manifold that is able to create photo-realistic im-
ages by speciﬁcally formulating a loss function based on
the euclidian distance between feature maps extracted from
VGG19 [41] and SRResNet.

Our networks can be extended with the adversarial loss
as generator network. However, we optimize our network
only using an objective function such as mean square root
error (MSE). Therefore, instead of training DBPN with the
adversarial loss, we can compare DBPN with SRResNet
which is also optimized by MSE.

2.4. Back-projection

Back-projection [18] is well known as the efﬁcient it-
erative procedure to minimize the reconstruction error.
Previous studies have proven the effectivity of back-
projection [51, 11, 8, 46]. Originally, back-projection is
designed for the case with multiple LR inputs. However,
given only one LR input image, the updating procedure can
be obtained by upsampling the LR image using multiple
upsampling operators and calculate the reconstruction er-
ror iteratively [4]. Timofte et al. [46] mentioned that back-
projection can improve the quality of SR image. Zhao et
al. [51] proposed a method to reﬁne high-frequency texture
details with an iterative projection process. However, the
initialization which leads to an optimal solution remains un-
known. Most of the previous studies involve constant and
unlearnable predeﬁned parameters such as blur operator and
number of iteration.

To extend this algorithm, we develop an end-to-end
trainable architecture which focuses to guide the SR task
using mutually connected up- and down-sampling stages to
learn non-linear relation of LR and HR image. The mu-

tual relation between HR and LR image is constructed by
creating iterative up and down-projection unit where the
up-projection unit generates HR features, then the down-
projection unit projects it back to the LR spaces as shown
in Fig. 2 (d). This schema enables the networks to pre-
serve the HR components by learned various up- and down-
sampling operators and generates deeper features to con-
struct numerous LR and HR features.

3. Deep Back-Projection Networks

(cid:48)

(cid:48)

(cid:48)

×N

), respectively, where M

Let I h and I l be HR and LR image with (M × N ) and
(M
< N . The
main building block of our proposed DBPN architecture is
the projection unit, which is trained (as part of the end-to-
end training of the SR system) to map either an LR feature
map to an HR map (up-projection), or an HR map to an LR
map (down-projection).

< M and N

(cid:48)

3.1. Projection units

The up-projection unit is deﬁned as follows:

scale up:

scale down:

residual:

scale residual up:

output feature map:

0 = (Lt−1 ∗ pt) ↑s,
H t
0 = (H t
Lt
t = Lt
el
H t
1 = (el
H t = H t

0 ∗ gt) ↓s,
0 − Lt−1,
t ∗ qt) ↑s,
0 + H t
1

(1)

(2)

(3)

(4)

(5)

where * is the spatial convolution operator, ↑s and ↓s are, re-
spectively, the up- and down-sampling operator with scaling
factor s, and pt, gt, qt are (de)convolutional layers at stage
t.

This projection unit takes the previously computed LR
feature map Lt−1 as input, and maps it to an (intermediate)
0; then it attempts to map it back to LR map Lt
HR map H t
0
(“back-project”). The residual (difference) el
t between the
observed LR map Lt−1 and the reconstructed Lt
0 is mapped
to HR again, producing a new intermediate (residual) map
1; the ﬁnal output of the unit, the HR map H t, is obtained
H t
by summing the two intermediate HR maps. This step is
illustrated in the upper part of Fig. 3.

The down-projection unit is deﬁned very similarly, but
now its job is to map its input HR map H t to the LR map
Lt as illustrated in the lower part of Fig. 3.

scale down:

scale up:

residual:

scale residual down:

output feature map:

t) ↓s,
t) ↑s,

0 = (H t ∗ g(cid:48)
Lt
0 ∗ p(cid:48)
0 = (Lt
H t
0 − H t,
t = H t
eh
t ∗ g(cid:48)
Lt
1 = (eh
0 + Lt
Lt = Lt
1

t) ↓s,

(6)

(7)

(8)

(9)

(10)

Figure 3. Proposed up- and down-projection unit in the DBPN.

We organize projection units in a series of stages, alter-
nating between H and L. These projection units can be un-
derstood as a self-correcting procedure which feeds a pro-
jection error to the sampling layer and iteratively changes
the solution by feeding back the projection error.

The projection unit uses large sized ﬁlters such as 8 × 8
and 12 × 12. In other existing networks, the use of large-
sized ﬁlter is avoided because it slows down the conver-
gence speed and might produce sub-optimal results. How-
ever, iterative utilization of our projection units enables the
network to suppress this limitation and to perform better
performance on large scaling factor even with shallow net-
works.

3.2. Dense projection units

The

dense

inter-layer

connectivity

in
DenseNets [15] has been shown to alleviate the vanishing-
gradient problem, produce improved feature, and encour-
age feature reuse. Inspired by this we propose to improve
DBPN, by introducing dense connections in the projection
units called, yielding Dense DBPN (D-DBPN).

pattern

Unlike the original DenseNets, we avoid dropout and
batch norm, which are not suitable for SR, because they
remove the range ﬂexibility of the features [31]. Instead,
we use 1 × 1 convolution layer as feature pooling and di-
mensional reduction [42, 12] before entering the projection
unit.

In D-DBPN, the input for each unit is the concatenation
of the outputs from all previous units. Let the L˜t and H ˜t
be the input for dense up- and down-projection unit, re-
spectively. They are generated using conv(1, nR) which is
used to merge all previous outputs from each unit as shown
in Fig. 4. This improvement enables us to generate the fea-
ture maps effectively, as shown in the experimental results.

3.3. Network architecture

The proposed D-DBPN is illustrated in Fig. 5.

It can
be divided into three parts: initial feature extraction, pro-
jection, and reconstruction, as described below. Here, let
conv(f, n) be a convolutional layer, where f is the ﬁlter
size and n is the number of ﬁlters.

Figure 4. Proposed up- and down-projection unit in the D-DBPN.
The feature maps of all preceding units (i.e., [L1, ..., Lt−1] and
[H 1, ..., H t] in up- and down-projections units, respectively) are
concatenated and used as inputs, and its own feature maps are used
as inputs into all subsequent units.

feature-maps L0 from the input using conv(3, n0).
Then conv(1, nR) is used to reduce the dimension
from n0 to nR before entering projection step where
n0 is the number of ﬁlters used in the initial LR fea-
tures extraction and nR is the number of ﬁlters used in
each projection unit.

2. Back-projection stages. Following initial feature ex-
traction is a sequence of projection units, alternating
between construction of LR and HR feature maps H t,
Lt; each unit has access to the outputs of all previous
units.

3. Reconstruction.

Finally,

the target HR image
is reconstructed as I sr = fRec([H 1, H 2, ..., H t]),
where fRec use conv(3, 3) as reconstruction and
[H 1, H 2, ..., H t] refers to the concatenation of the
feature-maps produced in each up-projection unit.

Due to the deﬁnitions of these building blocks, our net-
work architecture is modular. We can easily deﬁne and train
networks with different numbers of stages, controlling the
depth. For a network with T stages, we have the initial ex-
traction stage (2 layers), and then T up-projection units and
T − 1 down-projection units, each with 3 layers, followed
by the reconstruction (one more layer). However, for the
dense network, we add conv(1, nR) in each projection unit,
except the ﬁrst three units.

4. Experimental Results

4.1. Implementation and training details

In the proposed networks, the ﬁlter size in the projec-
tion unit is various with respect to the scaling factor. For
2× enlargement, we use 6 × 6 convolutional layer with two
striding and two padding. Then, 4× enlargement use 8 × 8
convolutional layer with four striding and two padding. Fi-
nally, the 8× enlargement use 12 × 12 convolutional layer
with eight striding and two padding.1

1We found these settings to work well based on general intuition and

1. Initial feature extraction. We construct initial LR

preliminary experiments.

Figure 5. An implementation of D-DBPN for super-resolution. Unlike the original DBPN, D-DBPN exploits densely connected projection
unit to encourage feature reuse.

We initialize the weights based on [14]. Here, std is com-
puted by ((cid:112)2/nl) where nl = f 2
t nt, ft is the ﬁlter size,
and nt is the number of ﬁlters. For example, with ft = 3
and nt = 8, the std is 0.111. All convolutional and decon-
volutional layers are followed by parametric rectiﬁed linear
units (PReLUs).

We trained all networks using images from DIV2K [44],
Flickr [31], and ImageNet dataset [36] without augmenta-
tion.2 To produce LR images, we downscale the HR images
on particular scaling factors using Bicubic. We use batch
size of 20 with size 32 × 32 for LR image, while HR image
size corresponds to the scaling factors. The learning rate is
initialized to 1e − 4 for all layers and decrease by a factor
of 10 for every 5 × 105 iterations for total 106 iterations.
For optimization, we use Adam with momentum to 0.9 and
weight decay to 1e−4. All experiments were conducted us-
ing Caffe, MATLAB R2017a on NVIDIA TITAN X GPUs.

4.2. Model analysis

Depth analysis. To demonstrate the capability of our
projection unit, we construct multiple networks S (T = 2),
M (T = 4), and L (T = 6) from the original DBPN.
In the feature extraction, we use conv(3, 128) followed by
conv(1, 32). Then, we use conv(1, 1) for the reconstruc-
tion. The input and output image are luminance only.

The results on 4× enlargement are shown in Fig. 6.
DBPN outperforms the state-of-the-art methods. Starting
from our shallow network, the S network gives the higher
PSNR than VDSR, DRCN, and LapSRN. The S network
uses only 12 convolutional layers with smaller number of
ﬁlters than VDSR, DRCN, and LapSRN. At the best per-
formance, S networks can achieve 31.59 dB which better
0.24 dB, 0.06 dB, 0.05 dB than VDSR, DRCN, and Lap-
SRN, respectively. The M network shows performance
improvement which better than all four existing state-of-

2The comparison on DIV2K only are available in the supplementary

material.

Figure 6. The depth analysis of DBPNs compare to other networks
(VDSR [22], DRCN [23], DRRN [43], LapSRN [25]) on Set5
dataset for 4× enlargement.

the-art methods (VDSR, DRCN, LapSRN, and DRRN). At
the best performance, the M network can achieve 31.74
dB which better 0.39 dB, 0.21 dB, 0.20 dB, 0.06 dB than
VDSR, DRCN, LapSRN, and DRRN respectively. In to-
tal, the M network use 24 convolutional layers which has
the same depth as LapSRN. Compare to DRRN (up to 52
convolutional layers), the M network undeniable shows the
effectiveness of our projection unit. Finally, the L network
outperforms all methods with 31.86 dB which better 0.51
dB, 0.33 dB, 0.32 dB, 0.18 dB than VDSR, DRCN, Lap-
SRN, and DRRN, respectively.

The results of 8× enlargement are shown in Fig. 7. The
S, M, L networks outperform the current state-of-the-art for
8× enlargement which clearly show the effectiveness of our
proposed networks on large scaling factors. However, we
found that there is no signiﬁcant performance gain from
each proposed network especially for L and M networks
where the difference only 0.04 dB.

Number of parameters. We show the tradeoff between
performance and number of network parameters from our
networks and existing deep network SR in Fig. 8 and 9.

Figure 7. The depth analysis of DBPN on Set5 dataset for 8× en-
largement. S (T = 2), M (T = 4), and L (T = 6)

For the sake of low computation for real-time processing,
we construct SS network which is the lighter version of the
S network, (T = 2). We only use conv(3, 64) followed
by conv(1, 18) for the initial feature extraction. However,
the results outperform SRCNN, FSRCNN, and VDSR on
both 4× and 8× enlargement. Moreover, our SS network
performs better than VDSR with 72% and 37% fewer pa-
rameters on 4× and 8× enlargement, respectively.

Our S network has about 27% fewer parameters and
higher PSNR than LapSRN on 4× enlargement. Finally, D-
DBPN has about 76% fewer parameters, and approximately
the same PSNR, compared to EDSR on 4× enlargement.
On the 8× enlargement, D-DBPN has about 47% fewer
parameters with better PSNR compare to EDSR. This evi-
dence show that our networks has the best trade-off between
performance and number of parameter.

Deep concatenation. Each projection unit is used to
distribute the reconstruction step by constructing features
which represent different details of the HR components.
Deep concatenation is also well-related with the number of
T (back-projection stage), which shows more detailed fea-
tures generated from the projection units will also increase
the quality of the results. In Fig. 10, it is shown that each
stage successfully generates diverse features to reconstruct
SR image.

Dense connection. We implement D-DBPN-L which is
a dense connection of the L network to show how dense
connection can improve the network’s performance in all
cases as shown in Table 1. On 4× enlargement, the dense
network, D-DBPN-L, gains 0.13 dB and 0.05 dB higher
than DBPN-L on the Set5 and Set14, respectively. On 8×,
the gaps are even larger. The D-DBPN-L has 0.23 dB and
0.19 dB higher that DBPN-L on the Set5 and Set14, respec-
tively.

4.3. Comparison with the-state-of-the-arts

To conﬁrm the ability of the proposed network, we
performed several experiments and analysis. We com-
pare our network with eight state-of-the-art SR algo-
rithms: A+ [45], SRCNN [6], FSRCNN [7], VDSR [22],

Figure 8. Performance vs number of parameters. The results are
evaluated with Set5 dataset for 4× enlargement.

Figure 9. Performance vs number of parameters. The results are
evaluated with Set5 dataset for 8× enlargement.

Figure 10. Sample of activation maps from up-projection units in
D-DBPN where t = 7. Each feature has been enhanced using the
same grayscale colormap for visibility.

Table 1. Comparison of the DBPN-L and D-DBPN-L on 4× and
8× enlargement. Red indicates the best performance.

Set5

Set14

Algorithm

Scale

PSNR

SSIM PSNR

SSIM

DBPN-L
D-DBPN-L

DBPN-L
D-DBPN-L

4
4

8
8

31.86
31.99

26.63
26.86

0.891
0.893

0.761
0.773

28.47
28.52

24.73
24.92

0.777
0.778

0.631
0.638

DRCN [23], DRRN [43], LapSRN [25], and EDSR [31].
We carry out extensive experiments using 5 datasets:
Set5 [2], Set14 [50], BSDS100 [1], Urban100 [16] and
Manga109 [33]. Each dataset has different characteris-
tics. Set5, Set14 and BSDS100 consist of natural scenes;
Urban100 contains urban scenes with details in different
frequency bands; and Manga109 is a dataset of Japanese

Figure 11. Qualitative comparison of our models with other works on 4× super-resolution.

manga. Due to computation limit of Caffe, we have to di-
vide each image in Urban100 and Manga109 into four parts
and then calculate PSNR separately.

Our ﬁnal network, D-DBPN, uses conv(3, 256) then
conv(1, 64) for the initial feature extraction and t = 7 for
the back-projection stages.
In the reconstruction, we use
conv(3, 3). RGB color channels are used for input and out-
put image. It takes less than four days to train.

PSNR [19] and structural similarity (SSIM) [48] were
used to quantitatively evaluate the proposed method. Note
that higher PSNR and SSIM values indicate better quality.
As used by existing networks, all measurements used only
the luminance channel (Y). For SR by factor s, we crop s
pixels near image boundary before evaluation as in [31, 7].
Some of the existing networks such as SRCNN, FSRCNN,
VDSR, and EDSR did not perform 8× enlargement. To this
end, we retrained the existing networks by using author’s
code with the recommended parameters.

We show the quantitative results in the Table 2. Our D-
DBPN outperforms the existing methods by a large margin
in all scales except EDSR. For the 2× and 4× enlargement,
we have comparable PSNR with EDSR. However, the result
of EDSR tends to generate stronger edge than the ground
truth and lead to misleading information in several cases.
The result of EDSR for eyelashes in Fig. 11 shows that it
was interpreted as a stripe pattern. On the other hand, our
result generates softer patterns which subjectively closer to
the ground truth. On the butterﬂy image, EDSR separates
the white pattern which shows that EDSR tends to construct
regular pattern such ac circle and stripe, while D-DBPN
constructs the same pattern as the ground truth. The pre-
vious statement is strengthened by the results from the Ur-
ban100 dataset which consist of many regular patterns from
buildings. In Urban100, EDSR has 0.54 dB higher than D-

DBPN.

Our network shows it’s effectiveness in the 8× enlarge-
ment. The D-DBPN outperforms all of the existing meth-
ods by a large margin.
Interesting results are shown on
Manga109 dataset where D-DBPN obtains 25.50 dB which
is 0.61 dB better than EDSR. While on the Urban100
dataset, D-DBPN achieves 23.25 which is only 0.13 dB bet-
ter than EDSR. The results show that our networks perform
better on ﬁne-structures images such as manga characters,
even though we do not use any animation images in the
training.

The results of 8× enlargement are visually shown
in Fig. 12. Qualitatively, D-DBPN is able to preserve the
HR components better than other networks. It shows that
our networks can extract not only features but also create
contextual information from the LR input to generate HR
components in the case of large scaling factors, such as 8×
enlargement.

5. Conclusion

We have proposed Deep Back-Projection Networks for
Single Image Super-resolution. Unlike the previous meth-
ods which predict the SR image in a feed-forward manner,
our proposed networks focus to directly increase the SR fea-
tures using multiple up- and down-sampling stages and feed
the error predictions on each depth in the networks to revise
the sampling results, then, accumulates the self-correcting
features from each upsampling stage to create SR image.
We use error feedbacks from the up- and down-scaling steps
to guide the network to achieve a better result. The results
show the effectiveness of the proposed network compares to
other state-of-the-art methods. Moreover, our proposed net-
work successfully outperforms other state-of-the-art meth-
ods on large scaling factors such as 8× enlargement.

Figure 12. Qualitative comparison of our models with other works on 8× super-resolution. 1st line: LapSRN [25] (19.77 dB), EDSR [31]
(19.79 dB), and Ours (19.82 dB). 2nd line: LapSRN [25] (16.45 dB), EDSR [31] (19.1 dB), and Ours (23.1 dB). 3rd line: LapSRN [25]
(24.34 dB), EDSR [31] (25.29 dB), and Ours (28.84 dB)

Table 2. Quantitative evaluation of state-of-the-art SR algorithms: average PSNR/SSIM for scale factors 2×, 4× and 8×. Red indicates
the best and blue indicates the second best performance. (* indicates that the input is divided into four parts and calculated separately due
to computation limitation of Caffe)

Algorithm

Scale

PSNR

SSIM

PSNR

SSIM

PSNR

SSIM

PSNR

Set5

Set14

BSDS100

Urban100

Bicubic
A+ [45]
SRCNN [6]
FSRCNN [7]
VDSR [22]
DRCN [23]
DRRN [43]
LapSRN [25]
EDSR [31]
D-DBPN

Bicubic
A+ [45]
SRCNN [6]
FSRCNN [7]
VDSR [22]
DRCN [23]
DRRN [43]
LapSRN [25]
EDSR [31]
D-DBPN

Bicubic
A+ [45]
SRCNN [6]
FSRCNN [7]
VDSR [22]
LapSRN [25]
EDSR [31]
D-DBPN

2
2
2
2
2
2
2
2
2
2

4
4
4
4
4
4
4
4
4
4

8
8
8
8
8
8
8
8

33.65
36.54
36.65
36.99
37.53
37.63
37.74
37.52
38.11
38.09

28.42
30.30
30.49
30.71
31.35
31.53
31.68
31.54
32.46
32.47

24.39
25.52
25.33
25.41
25.72
26.14
26.97
27.21

0.930
0.954
0.954
0.955
0.958
0.959
0.959
0.959
0.960
0.960

0.810
0.859
0.862
0.865
0.882
0.884
0.888
0.885
0.897
0.898

0.657
0.692
0.689
0.682
0.711
0.738
0.775
0.784

30.34
32.40
32.29
32.73
32.97
32.98
33.23
33.08
33.92
33.85

26.10
27.43
27.61
27.70
28.03
28.04
28.21
28.19
28.80
28.82

23.19
23.98
23.85
23.93
24.21
24.44
24.94
25.13

0.870
0.906
0.903
0.909
0.913
0.913
0.913
0.913
0.919
0.919

0.704
0.752
0.754
0.756
0.770
0.770
0.772
0.772
0.788
0.786

0.568
0.597
0.593
0.592
0.609
0.623
0.640
0.648

29.56
31.22
31.36
31.51
31.90
31.85
32.05
31.80
32.32
32.27

25.96
26.82
26.91
26.97
27.29
27.24
27.38
27.32
27.71
27.72

23.67
24.20
24.13
24.21
24.37
24.54
24.80
24.88

0.844
0.887
0.888
0.891
0.896
0.894
0.897
0.895
0.901
0.900

0.669
0.710
0.712
0.714
0.726
0.724
0.728
0.728
0.742
0.740

0.547
0.568
0.565
0.567
0.576
0.586
0.596
0.601

26.88 (27.39∗)
29.23
29.52
29.87
30.77
30.76
31.23
30.41 (31.05∗)
32.93 (33.56∗)
− (33.02∗)

23.15 (23.64∗)
24.34
24.53
24.61
25.18
25.14
25.44
25.21 (25.87∗)
26.64 (27.30∗)
− (27.08∗)

20.74 (21.24∗)
21.37
21.29
21.32
21.54
21.81 (22.42∗)
22.47 (23.12∗)
− (23.25∗)

Manga109

PSNR

30.84 (31.05∗)
35.33
35.72
36.62
37.16
37.57
37.92
37.27 (37.53∗)
39.10 (39.33∗)
− (39.32∗)

24.92 (25.15∗)
27.02
27.66
27.89
28.82
28.97
29.46
29.09 (29.44∗)
31.02 (31.41∗)
− (31.50∗)

21.47 (21.68∗)
22.39
22.37
22.39
22.83
23.39 (23.67∗)
24.58 (24.89∗)
− (25.50∗)

SSIM

0.935
0.967
0.968
0.971
0.974
0.973
0.976
0.974
0.977
0.978

0.789
0.850
0.858
0.859
0.886
0.886
0.896
0.890
0.915
0.914

0.647
0.680
0.682
0.672
0.707
0.735
0.778
0.799

SSIM

0.841
0.894
0.895
0.901
0.914
0.913
0.919
0.910
0.935
0.931

0.659
0.720
0.724
0.727
0.753
0.752
0.764
0.756
0.803
0.795

0.516
0.545
0.543
0.537
0.560
0.582
0.620
0.622

References

[1] P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik. Con-
IEEE
tour detection and hierarchical image segmentation.
transactions on pattern analysis and machine intelligence,
33(5):898–916, 2011. 6

[2] M. Bevilacqua, A. Roumy, C. Guillemot, and M.-L. A.
Morel. Low-complexity single-image super-resolution based
on nonnegative neighbor embedding. In British Machine Vi-
sion Conference (BMVC), 2012. 6

[3] J. Carreira, P. Agrawal, K. Fragkiadaki, and J. Malik. Hu-
man pose estimation with iterative error feedback. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 4733–4742, 2016. 3

[4] S. Dai, M. Han, Y. Wu, and Y. Gong. Bilateral back-
projection for single image super resolution. In Multimedia
and Expo, 2007 IEEE International Conference on, pages
1039–1042. IEEE, 2007. 1, 3

[5] E. L. Denton, S. Chintala, R. Fergus, et al. Deep genera-
tive image models using a laplacian pyramid of adversarial
networks. In Advances in neural information processing sys-
tems, pages 1486–1494, 2015. 1, 3

[6] C. Dong, C. C. Loy, K. He, and X. Tang.

Image
IEEE
super-resolution using deep convolutional networks.
transactions on pattern analysis and machine intelligence,
38(2):295–307, 2016. 1, 2, 6, 8

[7] C. Dong, C. C. Loy, and X. Tang. Accelerating the super-
resolution convolutional neural network. In European Con-
ference on Computer Vision, pages 391–407. Springer, 2016.
1, 2, 6, 7, 8

[8] W. Dong, L. Zhang, G. Shi, and X. Wu. Nonlocal back-
In Image Pro-
projection for adaptive image enlargement.
cessing (ICIP), 2009 16th IEEE International Conference
on, pages 349–352. IEEE, 2009. 3

[9] D. J. Felleman and D. C. Van Essen. Distributed hierarchical
processing in the primate cerebral cortex. Cerebral cortex
(New York, NY: 1991), 1(1):1–47, 1991. 1

[10] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in neural information
processing systems, pages 2672–2680, 2014. 3

[11] M. Haris, M. R. Widyanto, and H. Nobuhara. First-order
derivative-based super-resolution. Signal, Image and Video
Processing, 11(1):1–8, 2017. 3

[12] M. Haris, M. R. Widyanto, and H. Nobuhara.

Inception
learning super-resolution. Appl. Opt., 56(22):6043–6048,
Aug 2017. 1, 4

[13] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. arXiv preprint arXiv:1512.03385,
2015. 1

[14] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation. In Proceedings of the IEEE International Con-
ference on Computer Vision, pages 1026–1034, 2015. 4
[15] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger.
Densely connected convolutional networks. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, 2017. 1, 2, 4

[16] J.-B. Huang, A. Singh, and N. Ahuja. Single image super-
resolution from transformed self-exemplars. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 5197–5206, 2015. 6

[17] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and
T. Brox. Flownet 2.0: Evolution of optical ﬂow estimation
with deep networks. In IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR), Jul 2017. 1

[18] M. Irani and S. Peleg. Improving resolution by image reg-
istration. CVGIP: Graphical models and image processing,
53(3):231–239, 1991. 1, 3

[19] M. Irani and S. Peleg. Motion analysis for image enhance-
ment: Resolution, occlusion, and transparency. Journal of
Visual Communication and Image Representation, 4(4):324–
335, 1993. 7

[20] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for
In European
real-time style transfer and super-resolution.
Conference on Computer Vision, pages 694–711. Springer,
2016. 1, 3

[21] A. Kappeler, S. Yoo, Q. Dai, and A. K. Katsaggelos.
Video super-resolution with convolutional neural networks.
IEEE Transactions on Computational Imaging, 2(2):109–
122, 2016. 1

[22] J. Kim, J. Kwon Lee, and K. Mu Lee. Accurate image super-
resolution using very deep convolutional networks. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 1646–1654, June 2016. 1, 2, 5,
6, 8

[23] J. Kim, J. Kwon Lee, and K. Mu Lee. Deeply-recursive con-
volutional network for image super-resolution. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 1637–1645, 2016. 1, 2, 5, 6, 8

[24] D. J. Kravitz, K. S. Saleem, C. I. Baker, L. G. Ungerleider,
and M. Mishkin. The ventral visual pathway: an expanded
neural framework for the processing of object quality. Trends
in cognitive sciences, 17(1):26–49, 2013. 1

[25] W.-S. Lai, J.-B. Huang, N. Ahuja, and M.-H. Yang. Deep
laplacian pyramid networks for fast and accurate super-
resolution. In IEEE Conferene on Computer Vision and Pat-
tern Recognition, 2017. 1, 2, 5, 6, 8

[26] V. A. Lamme and P. R. Roelfsema. The distinct modes of vi-
sion offered by feedforward and recurrent processing. Trends
in neurosciences, 23(11):571–579, 2000. 1

[27] G. Larsson, M. Maire, and G. Shakhnarovich. Fractalnet:
Ultra-deep neural networks without residuals. arXiv preprint
arXiv:1605.07648, 2016. 1

[28] C. Ledig, L. Theis, F. Husz´ar, J. Caballero, A. Cunningham,
A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, et al.
Photo-realistic single image super-resolution using a genera-
tive adversarial network. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), Jul 2017. 3
[29] K. Li, B. Hariharan, and J. Malik. Iterative instance segmen-
tation. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 3659–3667, 2016. 3

[30] R. Liao, X. Tao, R. Li, Z. Ma, and J. Jia. Video super-
resolution via deep draft-ensemble learning. In Proceedings
of the IEEE International Conference on Computer Vision,
pages 531–539, 2015. 1

Methods and results. In Computer Vision and Pattern Recog-
nition Workshops (CVPRW), 2017 IEEE Conference on,
pages 1110–1121. IEEE, 2017. 2, 5

[45] R. Timofte, V. De Smet, and L. Van Gool. A+: Adjusted
anchored neighborhood regression for fast super-resolution.
In Asian Conference on Computer Vision, pages 111–126.
Springer, 2014. 6, 8

[46] R. Timofte, R. Rothe, and L. Van Gool. Seven ways to im-
prove example-based single image super resolution. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 1865–1873, 2016. 3

[47] Z. Tu and X. Bai. Auto-context and its application to high-
IEEE
level vision tasks and 3d brain image segmentation.
Transactions on Pattern Analysis and Machine Intelligence,
32(10):1744–1757, 2010. 3

[48] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli.
from error visibility to struc-
Image Processing, IEEE Transactions on,

Image quality assessment:
tural similarity.
13(4):600–612, 2004. 7

[49] A. R. Zamir, T.-L. Wu, L. Sun, W. Shen, J. Malik,
arXiv preprint

Feedback networks.

and S. Savarese.
arXiv:1612.09508, 2016. 3

[50] R. Zeyde, M. Elad, and M. Protter. On single image scale-up
using sparse-representations. In Curves and Surfaces, pages
711–730. Springer, 2012. 6

[51] Y. Zhao, R.-G. Wang, W. Jia, W.-M. Wang, and W. Gao. It-
erative projection reconstruction for fast and efﬁcient image
upsampling. Neurocomputing, 226:200–211, 2017. 3

[31] B. Lim, S. Son, H. Kim, S. Nah, and K. M. Lee. Enhanced
deep residual networks for single image super-resolution.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) Workshops, July 2017. 1, 2, 4, 5, 6, 7,
8

[32] W. Lotter, G. Kreiman, and D. Cox. Deep predictive cod-
ing networks for video prediction and unsupervised learning.
arXiv preprint arXiv:1605.08104, 2016. 3

[33] Y. Matsui, K. Ito, Y. Aramaki, A. Fujimoto, T. Ogawa, T. Ya-
masaki, and K. Aizawa. Sketch-based manga retrieval us-
ing manga109 dataset. Multimedia Tools and Applications,
pages 1–28, 2016. 6

[34] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-
sentation learning with deep convolutional generative adver-
sarial networks. arXiv preprint arXiv:1511.06434, 2015. 1,
3

[35] S. Ross, D. Munoz, M. Hebert, and J. A. Bagnell. Learning
message-passing inference machines for structured predic-
tion. In Computer Vision and Pattern Recognition (CVPR),
2011 IEEE Conference on, pages 2737–2744. IEEE, 2011. 3
[36] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
et al.
Imagenet large scale visual recognition challenge.
International Journal of Computer Vision, 115(3):211–252,
2015. 5

[37] M. S. Sajjadi, B. Sch¨olkopf, and M. Hirsch. Enhancenet:
Single image super-resolution through automated texture
synthesis. arXiv preprint arXiv:1612.07919, 2016. 3
[38] W. Shi, J. Caballero, F. Husz´ar, J. Totz, A. P. Aitken,
R. Bishop, D. Rueckert, and Z. Wang. Real-time single im-
age and video super-resolution using an efﬁcient sub-pixel
convolutional neural network. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 1874–1883, 2016. 1, 2

[39] A. Shrivastava and A. Gupta. Contextual priming and feed-
back for faster r-cnn. In European Conference on Computer
Vision, pages 330–348. Springer, 2016. 3

[40] A. Shrivastava, T. Pﬁster, O. Tuzel, J. Susskind, W. Wang,
and R. Webb. Learning from simulated and unsupervised
images through adversarial training. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, 2017. 1

[41] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 3

[42] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 1–9, 2015. 4

[43] Y. Tai, J. Yang, and X. Liu. Image super-resolution via deep
recursive residual network. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, 2017.
1, 2, 5, 6, 8

[44] R. Timofte, E. Agustsson, L. Van Gool, M.-H. Yang,
L. Zhang, B. Lim, S. Son, H. Kim, S. Nah, K. M. Lee,
et al. Ntire 2017 challenge on single image super-resolution:

