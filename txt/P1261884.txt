Stochastic blockmodel approximation of a graphon:
Theory and consistent estimation

Edoardo M. Airoldi
Dept. Statistics
Harvard University

Thiago B. Costa
SEAS, and Dept. Statistics
Harvard University

Stanley H. Chan
SEAS, and Dept. Statistics
Harvard University

Abstract

Non-parametric approaches for analyzing network data based on exchangeable
graph models (ExGM) have recently gained interest. The key object that deﬁnes
an ExGM is often referred to as a graphon. This non-parametric perspective on
network modeling poses challenging questions on how to make inference on the
graphon underlying observed network data. In this paper, we propose a computa-
tionally efﬁcient procedure to estimate a graphon from a set of observed networks
generated from it. This procedure is based on a stochastic blockmodel approxi-
mation (SBA) of the graphon. We show that, by approximating the graphon with
a stochastic block model, the graphon can be consistently estimated, that is, the
estimation error vanishes as the size of the graph approaches inﬁnity.

1 Introduction

Revealing hidden structures of a graph is the heart of many data analysis problems. From the well-
known small-world network to the recent large-scale data collected from online service providers
such as Wikipedia, Twitter and Facebook, there is always a momentum in seeking better and more
informative representations of the graphs [1, 14, 29, 3, 26, 12]. In this paper, we develop a new com-
putational tool to study one type of non-parametric representations which recently draws signiﬁcant
attentions from the community [4, 19, 5, 30, 23].

The root of the non-parametric model discussed in this paper is in the theory of exchangeable ran-
dom arrays [2, 15, 16], and it is presented in [11] as a link connecting de Finetti’s work on partial
exchangeability and graph limits [20, 6]. In a nutshell, the theory predicts that every convergent
sequence of graphs (Gn) has a limit object that preserves many local and global properties of the
graphs in the sequence. This limit object, which is called a graphon, can be represented by mea-
surable functions w : [0, 1]2
[0, 1], in a way that any w′ obtained from measure preserving
transformations of w describes the same graphon.

→

Graphons are usually seen as kernel functions for random network models [18]. To construct an
n-vertex random graph
Uniform[0, 1] to
each vertex i

(n, w) for a given w, we ﬁrst assign a random label ui ∼
, and connect any two vertices i and j with probability w(ui, uj), i.e.,
}

G
1, . . . , n

∈ {

Pr (G[i, j] = 1

ui, uj) = w(ui, uj),

i, j = 1, . . . , n,

(1)

|

where G[i, j] denotes the (i, j)th entry of the adjacency matrix representing a particular realization
of
(n, w) (See Figure 1). As an example, we note that the stochastic block-model is the case where
w(x, y) is a piecewise constant function.

G

The problem of interest is deﬁned as follows: Given a sequence of 2T observed directed graphs
G1, . . . , G2T , can we make an estimate
?
w of w, such that
→ ∞
This question has been loosely attempted in the literature, but none of which has a complete solution.
For example, Lloyd et al. [19] proposed a Bayesian estimator without a consistency proof; Choi and

w with high probability as n

→

w

b

b

1

w

w(ui, uj)

×

uj

G2T

ui

(ui, uj)

G1

[Left] Given a graphon w :

[0, 1]2
samples ui, uj from
Figure 1:
Uniform[0,1] and assign Gt[i, j] = 1 with probability w(ui, uj), for t = 1, . . . , 2T .
[Middle]
Heat map of a graphon w. [Right] A random graph generated by the graphon shown in the middle.
Rows and columns of the graph are ordered by increasing ui, instead of i for better visualization.

[0, 1], we draw i.i.d.

→

Wolfe [9] studied the consistency properties, but did not provide algorithms to estimate the graphon.
To the best of our knowledge, the only method that estimates graphons consistently, besides ours, is
USVT [8]. However, our algorithm has better complexity and outperforms USVT in our simulations.
More recently, other groups have begun exploring approaches related to ours [28, 24].

The proposed approximation procedure requires w to be piecewise Lipschitz. The basic idea is to
approximate w by a two-dimensional step function
w with diminishing intervals as n increases.The
proposed method is called the stochastic blockmodel approximation (SBA) algorithm, as the idea of
using a two-dimensional step function for approximation is equivalent to using the stochastic block
models [10, 22, 13, 7, 25]. The SBA algorithm is deﬁned up to permutations of the nodes, so the
estimated graphon is not canonical. However, this does not affect the consistency properties of the
SBA algorithm, as the consistency is measured w.r.t. the graphon that generates the graphs.

b

2 Stochastic blockmodel approximation: Procedure

In this section we present the proposed SBA algorithm and discuss its basic properties.

2.1 Assumptions on graphons

We assume that w is piecewise Lipschitz, i.e., there exists a sequence of non-overlaping intervals
1, αk] deﬁned by 0 = α0 < . . . < αK = 1, and a constant L > 0 such that, for any
Ik = [αk
−
(x1, y1) and (x2, y2)

Ij ,

x1 −
For generality we assume w to be asymmetric i.e., w(u, v)
can be considered as a special case. Consequently, a random graph
directed, i.e., G[i, j]

x2|
= w(v, u), so that symmetric graphons
(n, w) generated by w is

w(x2, y2)

= G[j, i].

y1 −

L (
|

y2|

| ≤

) .

+

G

|

|

∈

Iij = Ii ×
w(x1, y1)
−

2.2 Similarity of graphon slices

The intuition of the proposed SBA algorithm is that if the graphon is smooth, neighboring cross-
sections of the graphon should be similar. In other words, if two labels ui and uj are close i.e.,
and the column slices
ui −
w(ui,
|
should also be small. To measure the similarity between two labels using the
w(
|
·
graphon slices, we deﬁne the following distance

0, then the difference between the row slices
w(
·

uj| ≈
, ui)
−

, uj)
|

w(uj,

)
|

−

)

·

|

·

dij =

1
2

1

0
(cid:18)Z

−

[w(x, ui)

w(x, uj )]2 dx +

[w(ui, y)

w(uj , y)]2 dy

.

(2)

−

(cid:19)

1

0
Z

2

Thus, dij is small only if both row and column slices of the graphon are similar.

The usage of dij for graphon estimation will be discussed in the next subsection. But before
we proceed, it should be noted that in practice dij has to be estimated from the observed graphs
dij of dij, it is helpful to express dij in a way that the estima-
G1, . . . , G2T . To derive an estimator
tors can be easily obtained. To this end, we let

cij =

w(x, ui)w(x, uj )dx

and

rij =

w(ui, y)w(uj , y)dy,

b

1

0
Z

and express dij as dij = 1
(cii −
2
we consider the following estimators for cij and rij :
h

cji +cjj )+(rii −

cij −

rij −

. Inspecting this expression,

1

0
Z
rji +rjj )
i

(3)

(4)

(5)

ck
ij =

b
rk
ij =

1
T 2 



1
T 2 

Gt1 [k, i]

Gt2 [k, j]

X1
t1≤
≤

T

XT <t2≤

2T

Gt1 [i, k]

Gt2 [j, k]

X1
t1≤
≤

T



XT <t2≤

2T

















,

.









Here, the superscript k can be interpreted as the dummy variables x and y in deﬁning cij and rij ,
respectively. Summing all possible k’s yields an estimator

dij that looks similar to dij :

b

dij =

1
2 "

1
S

rk
ii −

rk
ij −

rk
ji +

rk
jj

+

b
ck
ii −

ck
ij −

ck
ji +

ck
jj

Xk
∈S (cid:8)(cid:0)

(cid:0)
b
b
is the set of summation indices.

b

b

b

(cid:1)

b

b

b

,

#

(cid:1)(cid:9)

where

=

b

1, . . . , n

S

{

i, j

}\{

}

]

1

≤

t1≤

is w(ui,

T Gt1 [i,

The motivation of deﬁning the estimators in (3) and (4) is that a row of the adjacency matrix G[i,
]
·
is fully characterized by the corresponding row of the graphon w(ui,
). Thus the expected value of
1
rk
ij is an estimator for rij. To theoretically
k
T
∈S
dij is indeed a good estimator: it is not only
justify this intuition, we will show in Section 3 that
P
unbiased, but is also concentrated round dij for large n. Furthermore, we will show that it is possible
to use a random subset of
to achieve the same asymptotic behavior.
}\{
As a result, the estimation of dij can be performed locally in a neighborhood of i and j, instead of
all n vertices.

), and hence 1
S

instead of

1, . . . , n

(cid:16)P

i, j
b

S

(cid:17)

b

}

{

·

·

·

2.3 Blocking the vertices

The similarity metric
wise constant function
(unknown) labels
u1, . . . , un}
b
BK are deﬁned, we can then determine
the blocks
b
b
Bj:
frequency of edges that are present across blocks

dij discussed above suggests one simple method to approximate w by a piece-
w (i.e., a stochastic block-model). Given G1, . . . , G2T , we can cluster the
BK using a procedure described below. Once
w(ui, uj) by computing the empirical

into K blocks

{
B1, . . . ,

b
Bi and

B1, . . . ,

b
w(ui, uj) =

b

1
Bi| |
b

Bj| Xix∈
where
b
estimate of the expected number of edges linking block

b
Bi is the block containing ui so that summing Gt[x, y] over x

b
b
Bi Xjy∈
Bj

|

Bi and

Bj.

(G1[ix, jy] + G2[ix, jy] + . . . + G2T [ix, jy]) ,

b

b

(6)

b

1
2T

Bi and y

Bj yields an

∈

b

∈

b

To cluster the unknown labels
1. Starting with Ω =
other vertices iv ∈
Ω
precision parameter ∆ > 0. If
after scanning through Ω once, a block
Ω

{
u1, . . . , un}
{
ip}
\{

B1, the process repeats until Ω =
b

←

Ω

b

\

B1 =
.

∅

u1, . . . , un}
, we compute the distance

we propose a greedy approach as shown in Algorithm
, we randomly pick a node ip and call it the pivot. Then for all
dip,iv < ∆2 for some
dip,iv and check whether
dip,iv < ∆2, then we assign iv to the same block as ip. Therefore,
will be deﬁned. By updating Ω as

b

b

b

b

ip, iv1, iv2 , . . .
}

{

b

b

3

The proposed greedy algorithm is only a local solution in a sense that it does not return the globally
optimal clusters. However, as will be shown in Section 3, although the clustering algorithm is not
globally optimal, the estimated graphon
w is still guaranteed to be a consistent estimate of the true
graphon w as n
. Since the greedy algorithm is numerically efﬁcient, it serves as a practical
computational tool to estimate w.

→ ∞

b

2.4 Main algorithm

Algorithm 1 Stochastic blockmodel approximation

Input: A set of observed graphs G1, . . . , G2T and the precision parameter ∆.
Output: Estimated stochastic blocks
Initialize: Ω =
{
do
while Ω

, and k = 1.
b

B1, . . . ,

1, . . . , n

BK.

=

b

}

Randomly choose a vertex ip from Ω and assign it as the pivot for
for Every other vertices iv ∈

Ω

∅

do
ip}
dip,iv .

ip.

Bk:

b

Bk ←
b

∆2, then assign iv as a member of

Bk:

\{
Compute the distance estimate
If
dip,iv ≤
end for
Update Ω: Ω
Ω
b
Update counter: k

k + 1.

Bk.

←

b

\
←
b

end while

iv.

Bk ←
b

b

Algorithm 1 illustrates the pseudo-code for the proposed stochastic block-model approximation.
The complexity of this algorithm is
(T SKn), where T is half the number of observations, S is
the size of the neighborhood, K is the number of blocks and n is number of vertices of the graph.

O

3 Stochastic blockmodel approximation: Theory of estimation

In this section we present the theoretical aspects of the proposed SBA algorithm. We will ﬁrst
discuss the properties of the estimator
dij, and then show the consistency of the estimated graphon
w. Details of the proofs can be found in the supplementary material.

3.1 Concentration analysis of
b

dij

b

Our ﬁrst theorem below shows that the proposed estimator
around its expected value dij.

b

dij is both unbiased, and is concentrated

Theorem 1. The estimator

dij for dij is unbiased, i.e., E[

dij ] = dij . Further, for any ǫ > 0,

b

dij −
h(cid:12)
where S is the size of the neighborhood
(cid:12)
(cid:12) b

Pr

b

S

(cid:12)
(cid:12)
(cid:12)

≤

i

, and 2T is the number of observations.

dij

> ǫ

Sǫ2
32/T +8ǫ/3 ,
8e−
b

(7)

Proof. Here we only highlight the important steps to present the intuition. The basic idea of the
rk
proof is to zoom-in a microscopic term of
ij and show that it is unbiased. To this end, we use the
fact that Gt1 [i, k] and Gt2 [j, k] are conditionally independent on uk to show

E[Gt1 [i, k]Gt2[j, k]

uk] = Pr[Gt1 [i, k] = 1, Gt2[j, k] = 1

uk]

b

|

|

(a)
= Pr[Gt1 [i, k] = 1
= w(ui, uk)w(uj , uk),

|

uk] Pr[Gt2 [j, k] = 1

uk]

|

rk
ij ] =
uk]] = rij . The concentration inequality follows from a similar idea to bound the variance

uk] = w(ui, uk)w(uj , uk), and by iterated expectation we have E[

rk
ij |

which then implies E[
E[E[
rk
ij |
rk
ij and apply Bernstein’s inequality.
of
b
b

b

4

b

That Gt1[i, k] and Gt2[j, k] are conditionally independent on uk is a critical fact for the success of
the proposed algorithm. It also explains why at least 2 independently observed graphs are necessary,
for otherwise we cannot separate the probability in the second equality above marked with (a).

3.2 Choosing the number of blocks

The performance of the Algorithm 1 is sensitive to the number of blocks it deﬁnes. On the one hand,
it is desirable to have more blocks so that the graphon can be ﬁnely approximated. But on the other
hand, if the number of blocks is too large then each block will contain only few vertices. This is bad
because in order to estimate the value on each block, a sufﬁcient number of vertices in each block is
required. The trade-off between these two cases is controlled by the precision parameter ∆: a large
∆ generates few large clusters, while small ∆ generates many small clusters. A precise relationship
between the ∆ and K, the number of blocks generated the algorithm, is given in Theorem 2.
Theorem 2. Let ∆ be the accuracy parameter and K be the number of blocks estimated by Algo-
rithm 1, then

Pr

K >

"

QL√2

∆ # ≤

8n2e−

S∆4
128/T +16∆2/3 ,

where L is the Lipschitz constant and Q is the number of Lipschitz blocks in w.

In practice, we estimate ∆ using a cross-validation scheme to ﬁnd the optimal 2D histogram bin
width [27]. The idea is to test a sequence of potential values of ∆ and seek the one that minimizes
the cross validation risk, deﬁned as

J(∆) =

b

2

−

h(n

1) −

h(n

1)

n + 1

K

−

j=1
X

p2
j ,

b

/n and h = 1/K. Algorithm 2 details the proposed cross-validation scheme.

(8)

(9)

|

where

pj =

Bj|
Algorithm 2 Cross Validation
b
Input: Graphs G1, . . . , G2T .
Output: Blocks
B1, . . . ,
for a sequence of ∆’s do

b

Estimate blocks
B1, . . . ,
b
b
Compute
Bj|
pj =
|
b
J(∆) = 2
Compute
h(n
−
b
end for
b
Pick the ∆ with minimum
b

3.3 Consistency of

w

BK, and optimal ∆.

BK from G1, . . . , G2T . [Algorithm 1]

/n, for j = 1, . . . , K.
K
j=1

n+1

h(n

1)

b
1) −

−

p2
j , with h = 1/K.

P
J(∆), and the corresponding

b

B1, . . . ,

BK.

b

b

b

The goal of our next theorem is to show that
b
To begin with, let us ﬁrst recall two commonly used metric:
Deﬁnition 1. The mean squared error (MSE) and mean absolute error (MAE) are deﬁned as
b
n

w is a consistent estimate of w, i.e.,

w as n

→

w

b

n

.
→ ∞

Theorem 3. If S

Θ(n) and ∆

ω

MSE(

w) =

MAE(

b
w) =

1
n2

1
n2

b

iv =1
X
n

jv =1
X
n

|

1
4

iv =1
X

jv =1
X

log(n)
n

∈
E[MAE(

(cid:18)(cid:16)
w)] = 0

∩

(cid:19)
(cid:17)
and

∈

lim
n
→∞

b

5

(w(uiv , ujv )

w(uiv , ujv ))2

−

w(uiv , ujv )

b
w(uiv , ujv )
|

.

−

b
o(1), then

E[MSE(

w)] = 0.

lim
n
→∞

b

(G1[ix, jy] + G2[ix, jy] + . . . + G2T [ix, jy]) ,

Proof. The details of the proof can be found in the supplementary material . Here we only outline
the key steps to present the intuition of the theorem. The goal of Theorem 3 is to show convergence
of

. The idea is to consider the following two quantities:

w(uix , ujx),

|

|

b

−

w(ui, uj)

w(ui, uj) =

w(ui, uj)
|
1
Bi| |
1
b
Bi| |
so that if we can bound
b
w(ui, uj)
w(ui, uj)
|

w(ui, uj) =

b

|

b
b
Bj
Bi Xjx∈

Bj| Xix∈
b
Bj| Xix∈
w(ui, uj)
b
|
can also be bounded.

b
b
Bj
Bi Xjy∈

1
2T

−

w(ui, uj)
|

and

w(ui, uj)

|

w(ui, uj)
|

−

, then consequently

|

−

w(ui, uj)

w(ui, uj)
|

|
The bound for the ﬁrst term
b
vertex iv ∈
average over

Bj, by Theorem 1 a probability bound involving ∆ can be obtained.

−
Bi is guaranteed to be within a distance ∆ from the pivot of
Bi and
b
The bound for the second term
is shown in Lemma 2. Different from Lemma
b
1, here we need to consider two possible situations: either the intermediate estimate w(ui, uj) is
close to the ground truth w(ui, uj), or w(ui, uj) is far from the ground truth w(ui, uj). This ac-
counts for the sum in Lemma 2. Individual bounds are derived based on Lemma 1 and Theorem 1.

is shown in Lemma 1: By Algorithm 1, any
Bi. Since w(ui, uj) is an

w(ui, uj)
|

w(ui, uj)

−

b

b

b

b

|

Combining Lemma 1 and Lemma 2, we can then bound the error and show convergence.

Lemma 1. For any iv ∈
Pr

Bi and jv ∈
w(ui, uj)
b

Bj,

|
h

−

b

w(uiv , ujv )
|
Bj,

> 8∆1/2L1/4

32

S∆4
32/T +8∆2/3 .

e−

(10)

≤

i

|

Bi| |
b

Bj|
b

Pr

wij|

Bi and jv ∈
> 8∆1/2L1/4
b

Lemma 2. For any iv ∈
wij −
|
h
Θ(n) is necessary to make Theorem 3 valid, because if S is independent of n,
The condition S
the right hand sides of (10) and (11) cannot approach 0 even if n
. The condition on ∆ is also
important as it forces the numerators and denominators in the exponentials of (10) and (11) to be
well behaved.

S∆4
32/T +8∆2/3) .

√L∆) + 32

Bj|
b

Bi|
b

b
Bi| |

→ ∞

2e−
b

b
Bj |

2e−

(11)

256(T

≤

∈

b

i

2

|

|

|

4 Experiments

In this section we evaluate the proposed SBA algorithm by showing some empirical results. For the
purpose of comparison, we consider (i) the universal singular value thresholding (USVT) [8]; (ii)
the largest-gap algorithm (LG) [7]; (iii) matrix completion from few entries (OptSpace) [17].

4.1 Estimating stochastic blockmodels

Accuracy as a function of growing graph size. Our ﬁrst experiment is to evaluate the proposed
SBA algorithm for estimating stochastic blockmodels. For this purpose, we generate (arbitrarily) a
graphon

0.8 0.9 0.4 0.5
0.1 0.6 0.3 0.2
0.3 0.2 0.8 0.3
0.4 0.1 0.2 0.9

,






w = 




×

(12)

which represents a piecewise constant function with 4

4 equi-space blocks.

Since USVT and LG use only one observed graph whereas the proposed SBA require at least 2
observations, in order to make the comparison fair, we use half of the nodes for SBA by generating
n
two independent n
2 observed graphs. For USVT and LG, we use one n
Figure 2(a) shows the asymptotic behavior of the algorithms when n grows. Figure 2(b) shows the
estimation error of SBA algorithm as T grows for graphs of size 200 vertices.

n observed graph.

2 ×

×

6

)
E
A
M

(
0
1
g
o
l

−0.5

−1

−1.5

−2

−2.5

 
−3
0

−0.7

−0.8

−0.9

−1

−1.1

−1.2

−1.3

)
E
A
M

(
0
1
g
o
l

Proposed
Largest Gap
OptSpace
USVT

200

400

600

n

800

1000

5

10

15

25

30

35

40

20
2T

(a) Growing graph size, n

(b) Growing no. observations, 2T

Figure 2: (a) MAE reduces as graph size grows. For the fairness of the amount of data that can be
used, we use n
1 observation for USVT [8] and LG
[7]. (b) MAE of the proposed SBA algorithm reduces when more observations T is available. Both
plots are averaged over 100 independent trials.

2 observations for SBA, and n

n
2 ×

2 ×

×

×

n

Accuracy as a function of growing number of blocks. Our second experiment is to evaluate the
performance of the algorithms as K, the number of blocks, increases. To this end, we consider a
sequence of K, and for each K we generate a graphon w of K
K blocks. Each entry of the
block is a random number generated from Uniform[0, 1]. Same as the previous experiment, we ﬁx
n = 200 and T = 1. The experiment is repeated over 100 trials so that in every trial a different
graphon is generated. The result shown in Figure 3(a) indicates that while estimation error increases
as K grows, the proposed SBA algorithm still attains the lowest MAE for all K.

×

 

Proposed

 

 

 

)
E
A
M

(
0
1
g
o
l

−2

−2.1

−2.2

−2.3

−2.4

−2.5

−2.6

−2.7

−2.8

−2.9

 
−3
0

)
E
A
M

(
0
1
g
o
l

−0.6

−0.7

−0.8

−0.9

−1

−1.1

−1.2

−1.3

−1.4

−1.5

 
−1.6
0

Proposed
Largest Gap
USVT

 
−1.4
0

5

10
K

15

20

(a) Growing no. blocks, K

Proposed
Largest Gap
OptSpace
USVT

5

10
% missing links

15

20

(b) Missing links

Figure 3: (a) As K increases, MAE of all three algorithm increases but SBA still attains the lowest
MAE. Here, we use n
1 observation for USVT [8] and
LG [7]. (b) Estimation of graphon in the presence of missing links: As the amount of missing links
increases, estimation error also increases.

2 observations for SBA, and n

n
2 ×

2 ×

×

×

n

4.2 Estimation with missing edges

Our next experiment is to evaluate the performance of proposed SBA algorithm when there are
n binary matrix
missing edges in the observed graph. To model missing edges, we construct an n
×
M with probability Pr[M [i, j] = 0] = ξ, where 0
1 deﬁnes the percentage of missing
edges. Given ξ, 2T matrices are generated with missing edges, and the observed graphs are deﬁned
denotes the element-wise multiplication. The goal is to
as M1 ⊙
study how well SBA can reconstruct the graphon

w in the presence of missing links.

G1, . . . , M2T ⊙

G2T , where

⊙

≤

≤

ξ

b
7

The modiﬁcation of the proposed SBA algorithm for the case missing links is minimal: when com-
Bi and jy ∈
puting (6), instead of averaging over all ix ∈
Bj, we only average ix ∈
Bj
that are not masked out by all M ′s. Figure 3(b) shows the result of average over 100 independent
trials. Here, we consider the graphon given in (12), with n = 200 and T = 1. It is evident that SBA
b
b
b
outperforms its counterparts at a lower rate of missing links.

Bi and jy ∈
b

4.3 Estimating continuous graphons

Our ﬁnal experiment is to evaluate the proposed SBA algorithm in estimating continuous graphons.
Here, we consider two of the graphons reported in [8]:

w1(u, v) =

1 + exp

1
50(u2 + v2)
}

,

{−

and w2(u, v) = uv,

[0, 1]. Here, w2 can be considered as a special case of the Eigenmodel [13] or latent

where u, v
feature relational model [21].

∈

The results in Figure 4 shows that while both algorithms have improved estimates when n grows, the
performance depends on which of w1 and w2 that we are studying. This suggests that in practice the
choice of the algorithm should depend on the expected structure of the graphon to be estimated: If the
graph generated by the graphon demonstrates some low-rank properties, then USVT is likely to be
a better option. For more structured or complex graphons the proposed procedure is recommended.

 

Proposed
USVT

 

Proposed
USVT

)
E
A
M

(
0
1
g
o
l

−2.9

−2.95

−3

−3.05

−3.1

−3.15

 
−3.2
0

)
E
A
M

(
0
1
g
o
l

−0.6

−0.8

−1

−1.2

−1.4

−1.6

−1.8

 
−2
0

200

400

600

n

800

1000

200

400

600

n

800

1000

(a) graphon w1

(b) graphon w2

Figure 4: Comparison between SBA and USVT in estimating two continuous graphons w1 and w2.
Evidently, SBA performs better for w1 (high-rank) and worse for w2 (low-rank).

5 Concluding remarks

We presented a new computational tool for estimating graphons. The proposed algorithm approx-
imates the continuous graphon by a stochastic block-model, in which the ﬁrst step is to cluster
the unknown vertex labels into blocks by using an empirical estimate of the distance between two
graphon slices, and the second step is to build an empirical histogram to estimate the graphon. Com-
plete consistency analysis of the algorithm is derived. The algorithm was evaluated experimentally,
and we found that the algorithm is effective in estimating block structured graphons.

Implementation of the SBA algorithm is available online at https://github.com/airoldilab/SBA.

Acknowledgments. EMA is partially supported by NSF CAREER award IIS-1149662, ARO MURI
award W911NF-11-1-0036, and an Alfred P. Sloan Research Fellowship. SHC is partially supported
by a Croucher Foundation Post-Doctoral Research Fellowship.

References

[1] E.M. Airoldi, D.M. Blei, S.E. Fienberg, and E.P. Xing. Mixed-membership stochastic blockmodels.

Journal of Machine Learning Research, 9:1981–2014, 2008.

8

Advanced Study, Princeton, NJ, 1979.

30(1):137–154, 1989.

Theory, 56:2980–2998, Jun. 2010.

[2] D.J. Aldous. Representations for partially exchangeable arrays of random variables. Journal of Multi-

[3] H. Azari and E. M. Airoldi. Graphlet decomposition of a weighted network. Journal of Machine Learning

variate Analysis, 11:581–598, 1981.

Research, W&CP, 22:54–63, 2012.

[4] P.J. Bickel and A. Chen. A nonparametric view of network models and Newman-Girvan and other mod-

ularities. Proc. Natl. Acad. Sci. USA, 106:21068–21073, 2009.

[5] P.J. Bickel, A. Chen, and E. Levina. The method of moments and degree distributions for network models.

Annals of Statistics, 39(5):2280–2301, 2011.

[6] C. Borgs, J. Chayes, L. Lov´asz, V. T. S´os, B. Szegedy, and K. Vesztergombi. Graph limits and parameter

testing. In Proc. ACM Symposium on Theory of Computing, pages 261–270, 2006.

[7] A. Channarond, J. Daudin, and S. Robin. Classiﬁcation and estimation in the Stochastic Blockmodel

based on the empirical degrees. Electronic Journal of Statistics, 6:2574–2601, 2012.

[8] S. Chatterjee. Matrix estimation by universal singular value thresholding. ArXiv:1212.1247. 2012.
[9] D.S. Choi and P.J. Wolfe. Co-clustering separately exchangeable network data. ArXiv:1212.4093. 2012.
[10] D.S. Choi, P.J. Wolfe, and E.M. Airoldi. Stochastic blockmodels with a growing number of classes.

Biometrika, 99:273–284, 2012.

[11] P. Diaconis and S. Janson. Graph limits and exchangeable random graphs. Rendiconti di Matematica e

delle sue Applicazioni, Series VII, pages 33–61, 2008.

[12] A. Goldenberg, A.X. Zheng, S.E. Fienberg, and E.M. Airoldi. A survey of statistical network models.

Foundations and Trends in Machine Learning, 2:129–233, 2009.

[13] P.D. Hoff. Modeling homophily and stochastic equivalence in symmetric relational data.

In Neural

Information Processing Systems (NIPS), volume 20, pages 657–664, 2008.

[14] P.D. Hoff, A.E. Raftery, and M.S. Handcock. Latent space approaches to social network analysis. Journal

of the American Statistical Association, 97(460):1090–1098, 2002.

[15] D.N. Hoover. Relations on probability spaces and arrays of random variables. Preprint, Institute for

[16] O. Kallenberg. On the representation theorem for exchangeable arrays. Journal of Multivariate Analysis,

[17] R.H. Keshavan, A. Montanari, and S. Oh. Matrix completion from a few entries. IEEE Trans. Information

[18] N.D. Lawrence. Probabilistic non-linear principal component analysis with Gaussian process latent vari-

able models. Journal of Machine Learning Research, 6:1783–1816, 2005.

[19] J.R. Lloyd, P. Orbanz, Z. Ghahramani, and D.M. Roy. Random function priors for exchangeable arrays
with applications to graphs and relational data. In Neural Information Processing Systems (NIPS), 2012.
[20] L. Lov´asz and B. Szegedy. Limits of dense graph sequences. Journal of Combinatorial Theory, Series B,

96:933–957, 2006.

[21] K.T. Miller, T.L. Grifﬁths, and M.I. Jordan. Nonparametric latent fature models for link prediction. In

Neural Information Processing Systems (NIPS), 2009.

[22] K. Nowicki and T.A. Snijders. Estimation and prediction of stochastic block structures. Journal of

American Statistical Association, 96:1077–1087, 2001.

[23] P. Orbanz and D.M. Roy. Bayesian models of graphs, arrays and other exchangeable random structures,

2013. Unpublished manuscript.

[24] P.Latouche and S. Robin. Bayesian model averaging of stochastic block models to estimate the graphon
function and motif frequencies in a w-graph model. ArXiv:1310.6150, October 2013. Unpublished
manuscript.

[25] K. Rohe, S. Chatterjee, and B. Yu. Spectral clustering and the high-dimensional stochastic blockmodel.

[26] M. Tang, D.L. Sussman, and C.E. Priebe. Universally consistent vertex classiﬁcation for latent positions

Annals of Statistics, 39(4):1878–1915, 2011.

graphs. Annals of Statistics, 2013. In press.

[27] L. Wasserman. All of Nonparametric Statistics. Springer, 2005.
[28] P.J. Wolfe and S.C. Olhede. Nonparametric graphon estimation. ArXiv:1309.5936, September 2013.

Unpublished manuscript.

[29] Z. Xu, F. Yan, and Y. Qi. Inﬁnite Tucker decomposition: nonparametric Bayesian models for multiway

data analysis. In Proc. Intl. Conf. Machine Learning (ICML), 2012.

[30] Y. Zhao, E. Levina, and J. Zhu. Community extraction for social networks. In Proc. Natl. Acad. Sci. USA,

volume 108, pages 7321–7326, 2011.

9

Stochastic blockmodel approximation of a graphon:
Theory and consistent estimation

Edoardo M. Airoldi
Dept. Statistics
Harvard University

Thiago B. Costa
SEAS, and Dept. Statistics
Harvard University

Stanley H. Chan
SEAS, and Dept. Statistics
Harvard University

Abstract

Non-parametric approaches for analyzing network data based on exchangeable
graph models (ExGM) have recently gained interest. The key object that deﬁnes
an ExGM is often referred to as a graphon. This non-parametric perspective on
network modeling poses challenging questions on how to make inference on the
graphon underlying observed network data. In this paper, we propose a computa-
tionally efﬁcient procedure to estimate a graphon from a set of observed networks
generated from it. This procedure is based on a stochastic blockmodel approxi-
mation (SBA) of the graphon. We show that, by approximating the graphon with
a stochastic block model, the graphon can be consistently estimated, that is, the
estimation error vanishes as the size of the graph approaches inﬁnity.

1 Introduction

Revealing hidden structures of a graph is the heart of many data analysis problems. From the well-
known small-world network to the recent large-scale data collected from online service providers
such as Wikipedia, Twitter and Facebook, there is always a momentum in seeking better and more
informative representations of the graphs [1, 14, 29, 3, 26, 12]. In this paper, we develop a new com-
putational tool to study one type of non-parametric representations which recently draws signiﬁcant
attentions from the community [4, 19, 5, 30, 23].

The root of the non-parametric model discussed in this paper is in the theory of exchangeable ran-
dom arrays [2, 15, 16], and it is presented in [11] as a link connecting de Finetti’s work on partial
exchangeability and graph limits [20, 6]. In a nutshell, the theory predicts that every convergent
sequence of graphs (Gn) has a limit object that preserves many local and global properties of the
graphs in the sequence. This limit object, which is called a graphon, can be represented by mea-
surable functions w : [0, 1]2
[0, 1], in a way that any w′ obtained from measure preserving
transformations of w describes the same graphon.

→

Graphons are usually seen as kernel functions for random network models [18]. To construct an
n-vertex random graph
Uniform[0, 1] to
each vertex i

(n, w) for a given w, we ﬁrst assign a random label ui ∼
, and connect any two vertices i and j with probability w(ui, uj), i.e.,
}

G
1, . . . , n

∈ {

Pr (G[i, j] = 1

ui, uj) = w(ui, uj),

i, j = 1, . . . , n,

(1)

|

where G[i, j] denotes the (i, j)th entry of the adjacency matrix representing a particular realization
of
(n, w) (See Figure 1). As an example, we note that the stochastic block-model is the case where
w(x, y) is a piecewise constant function.

G

The problem of interest is deﬁned as follows: Given a sequence of 2T observed directed graphs
G1, . . . , G2T , can we make an estimate
?
w of w, such that
→ ∞
This question has been loosely attempted in the literature, but none of which has a complete solution.
For example, Lloyd et al. [19] proposed a Bayesian estimator without a consistency proof; Choi and

w with high probability as n

→

w

b

b

1

w

w(ui, uj)

×

uj

G2T

ui

(ui, uj)

G1

[Left] Given a graphon w :

[0, 1]2
samples ui, uj from
Figure 1:
Uniform[0,1] and assign Gt[i, j] = 1 with probability w(ui, uj), for t = 1, . . . , 2T .
[Middle]
Heat map of a graphon w. [Right] A random graph generated by the graphon shown in the middle.
Rows and columns of the graph are ordered by increasing ui, instead of i for better visualization.

[0, 1], we draw i.i.d.

→

Wolfe [9] studied the consistency properties, but did not provide algorithms to estimate the graphon.
To the best of our knowledge, the only method that estimates graphons consistently, besides ours, is
USVT [8]. However, our algorithm has better complexity and outperforms USVT in our simulations.
More recently, other groups have begun exploring approaches related to ours [28, 24].

The proposed approximation procedure requires w to be piecewise Lipschitz. The basic idea is to
approximate w by a two-dimensional step function
w with diminishing intervals as n increases.The
proposed method is called the stochastic blockmodel approximation (SBA) algorithm, as the idea of
using a two-dimensional step function for approximation is equivalent to using the stochastic block
models [10, 22, 13, 7, 25]. The SBA algorithm is deﬁned up to permutations of the nodes, so the
estimated graphon is not canonical. However, this does not affect the consistency properties of the
SBA algorithm, as the consistency is measured w.r.t. the graphon that generates the graphs.

b

2 Stochastic blockmodel approximation: Procedure

In this section we present the proposed SBA algorithm and discuss its basic properties.

2.1 Assumptions on graphons

We assume that w is piecewise Lipschitz, i.e., there exists a sequence of non-overlaping intervals
1, αk] deﬁned by 0 = α0 < . . . < αK = 1, and a constant L > 0 such that, for any
Ik = [αk
−
(x1, y1) and (x2, y2)

Ij ,

x1 −
For generality we assume w to be asymmetric i.e., w(u, v)
can be considered as a special case. Consequently, a random graph
directed, i.e., G[i, j]

x2|
= w(v, u), so that symmetric graphons
(n, w) generated by w is

w(x2, y2)

= G[j, i].

y1 −

L (
|

y2|

| ≤

) .

+

G

|

|

∈

Iij = Ii ×
w(x1, y1)
−

2.2 Similarity of graphon slices

The intuition of the proposed SBA algorithm is that if the graphon is smooth, neighboring cross-
sections of the graphon should be similar. In other words, if two labels ui and uj are close i.e.,
and the column slices
ui −
w(ui,
|
should also be small. To measure the similarity between two labels using the
w(
|
·
graphon slices, we deﬁne the following distance

0, then the difference between the row slices
w(
·

uj| ≈
, ui)
−

, uj)
|

w(uj,

)
|

−

)

·

·

|

dij =

1
2

1

0
(cid:18)Z

−

[w(x, ui)

w(x, uj )]2 dx +

[w(ui, y)

w(uj , y)]2 dy

.

(2)

−

(cid:19)

1

0
Z

2

Thus, dij is small only if both row and column slices of the graphon are similar.

The usage of dij for graphon estimation will be discussed in the next subsection. But before
we proceed, it should be noted that in practice dij has to be estimated from the observed graphs
dij of dij, it is helpful to express dij in a way that the estima-
G1, . . . , G2T . To derive an estimator
tors can be easily obtained. To this end, we let

cij =

w(x, ui)w(x, uj )dx

and

rij =

w(ui, y)w(uj , y)dy,

b

1

0
Z

and express dij as dij = 1
(cii −
2
we consider the following estimators for cij and rij :
h

cji +cjj )+(rii −

cij −

rij −

. Inspecting this expression,

1

0
Z
rji +rjj )
i

(3)

(4)

(5)

ck
ij =

b
rk
ij =

1
T 2 



1
T 2 

Gt1 [k, i]

Gt2 [k, j]

X1
t1≤
≤

T

XT <t2≤

2T

Gt1 [i, k]

Gt2 [j, k]

X1
t1≤
≤

T



XT <t2≤

2T

















,

.









Here, the superscript k can be interpreted as the dummy variables x and y in deﬁning cij and rij ,
respectively. Summing all possible k’s yields an estimator

dij that looks similar to dij :

b

dij =

1
2 "

1
S

rk
ii −

rk
ij −

rk
ji +

rk
jj

+

b
ck
ii −

ck
ij −

ck
ji +

ck
jj

Xk
∈S (cid:8)(cid:0)

(cid:0)
b
b
is the set of summation indices.

b

b

b

(cid:1)

b

b

b

,

#

(cid:1)(cid:9)

where

=

b

1, . . . , n

S

{

i, j

}\{

}

]

1

≤

t1≤

is w(ui,

T Gt1 [i,

The motivation of deﬁning the estimators in (3) and (4) is that a row of the adjacency matrix G[i,
]
·
is fully characterized by the corresponding row of the graphon w(ui,
). Thus the expected value of
1
rk
ij is an estimator for rij. To theoretically
k
T
∈S
dij is indeed a good estimator: it is not only
justify this intuition, we will show in Section 3 that
P
unbiased, but is also concentrated round dij for large n. Furthermore, we will show that it is possible
to use a random subset of
to achieve the same asymptotic behavior.
}\{
As a result, the estimation of dij can be performed locally in a neighborhood of i and j, instead of
all n vertices.

), and hence 1
S

instead of

1, . . . , n

(cid:16)P

i, j
b

S

(cid:17)

b

}

{

·

·

·

2.3 Blocking the vertices

The similarity metric
wise constant function
(unknown) labels
u1, . . . , un}
b
BK are deﬁned, we can then determine
the blocks
b
b
Bj:
frequency of edges that are present across blocks

dij discussed above suggests one simple method to approximate w by a piece-
w (i.e., a stochastic block-model). Given G1, . . . , G2T , we can cluster the
BK using a procedure described below. Once
w(ui, uj) by computing the empirical

into K blocks

{
B1, . . . ,

b
Bi and

B1, . . . ,

b
w(ui, uj) =

b

1
Bi| |
b

Bj| Xix∈
where
b
estimate of the expected number of edges linking block

b
Bi is the block containing ui so that summing Gt[x, y] over x

b
b
Bi Xjy∈
Bj

|

Bi and

Bj.

(G1[ix, jy] + G2[ix, jy] + . . . + G2T [ix, jy]) ,

b

b

(6)

b

1
2T

Bi and y

Bj yields an

∈

b

∈

b

To cluster the unknown labels
1. Starting with Ω =
other vertices iv ∈
Ω
precision parameter ∆ > 0. If
after scanning through Ω once, a block
Ω

{
u1, . . . , un}
{
ip}
\{

B1, the process repeats until Ω =
b

←

Ω

b

\

B1 =
.

∅

u1, . . . , un}
, we compute the distance

we propose a greedy approach as shown in Algorithm
, we randomly pick a node ip and call it the pivot. Then for all
dip,iv < ∆2 for some
dip,iv and check whether
dip,iv < ∆2, then we assign iv to the same block as ip. Therefore,
will be deﬁned. By updating Ω as

b

b

b

b

ip, iv1, iv2 , . . .
}

{

b

b

3

The proposed greedy algorithm is only a local solution in a sense that it does not return the globally
optimal clusters. However, as will be shown in Section 3, although the clustering algorithm is not
globally optimal, the estimated graphon
w is still guaranteed to be a consistent estimate of the true
graphon w as n
. Since the greedy algorithm is numerically efﬁcient, it serves as a practical
computational tool to estimate w.

→ ∞

b

2.4 Main algorithm

Algorithm 1 Stochastic blockmodel approximation

Input: A set of observed graphs G1, . . . , G2T and the precision parameter ∆.
Output: Estimated stochastic blocks
Initialize: Ω =
{
do
while Ω

, and k = 1.
b

B1, . . . ,

1, . . . , n

BK.

=

b

}

Randomly choose a vertex ip from Ω and assign it as the pivot for
for Every other vertices iv ∈

Ω

∅

do
ip}
dip,iv .

ip.

Bk:

b

Bk ←
b

∆2, then assign iv as a member of

Bk:

\{
Compute the distance estimate
If
dip,iv ≤
end for
Update Ω: Ω
Ω
b
Update counter: k

k + 1.

Bk.

←

b

\
←
b

end while

iv.

Bk ←
b

b

Algorithm 1 illustrates the pseudo-code for the proposed stochastic block-model approximation.
The complexity of this algorithm is
(T SKn), where T is half the number of observations, S is
the size of the neighborhood, K is the number of blocks and n is number of vertices of the graph.

O

3 Stochastic blockmodel approximation: Theory of estimation

In this section we present the theoretical aspects of the proposed SBA algorithm. We will ﬁrst
discuss the properties of the estimator
dij, and then show the consistency of the estimated graphon
w. Details of the proofs can be found in the supplementary material.

3.1 Concentration analysis of
b

dij

b

Our ﬁrst theorem below shows that the proposed estimator
around its expected value dij.

b

dij is both unbiased, and is concentrated

Theorem 1. The estimator

dij for dij is unbiased, i.e., E[

dij ] = dij . Further, for any ǫ > 0,

b

dij −
h(cid:12)
where S is the size of the neighborhood
(cid:12)
(cid:12) b

Pr

b

S

(cid:12)
(cid:12)
(cid:12)

≤

i

, and 2T is the number of observations.

dij

> ǫ

Sǫ2
32/T +8ǫ/3 ,
8e−
b

(7)

Proof. Here we only highlight the important steps to present the intuition. The basic idea of the
rk
proof is to zoom-in a microscopic term of
ij and show that it is unbiased. To this end, we use the
fact that Gt1 [i, k] and Gt2 [j, k] are conditionally independent on uk to show

E[Gt1 [i, k]Gt2[j, k]

uk] = Pr[Gt1 [i, k] = 1, Gt2[j, k] = 1

uk]

b

|

|

(a)
= Pr[Gt1 [i, k] = 1
= w(ui, uk)w(uj , uk),

|

uk] Pr[Gt2 [j, k] = 1

uk]

|

rk
ij ] =
uk]] = rij . The concentration inequality follows from a similar idea to bound the variance

uk] = w(ui, uk)w(uj , uk), and by iterated expectation we have E[

rk
ij |

which then implies E[
E[E[
rk
ij |
rk
ij and apply Bernstein’s inequality.
of
b
b

b

4

b

That Gt1[i, k] and Gt2[j, k] are conditionally independent on uk is a critical fact for the success of
the proposed algorithm. It also explains why at least 2 independently observed graphs are necessary,
for otherwise we cannot separate the probability in the second equality above marked with (a).

3.2 Choosing the number of blocks

The performance of the Algorithm 1 is sensitive to the number of blocks it deﬁnes. On the one hand,
it is desirable to have more blocks so that the graphon can be ﬁnely approximated. But on the other
hand, if the number of blocks is too large then each block will contain only few vertices. This is bad
because in order to estimate the value on each block, a sufﬁcient number of vertices in each block is
required. The trade-off between these two cases is controlled by the precision parameter ∆: a large
∆ generates few large clusters, while small ∆ generates many small clusters. A precise relationship
between the ∆ and K, the number of blocks generated the algorithm, is given in Theorem 2.
Theorem 2. Let ∆ be the accuracy parameter and K be the number of blocks estimated by Algo-
rithm 1, then

Pr

K >

"

QL√2

∆ # ≤

8n2e−

S∆4
128/T +16∆2/3 ,

where L is the Lipschitz constant and Q is the number of Lipschitz blocks in w.

In practice, we estimate ∆ using a cross-validation scheme to ﬁnd the optimal 2D histogram bin
width [27]. The idea is to test a sequence of potential values of ∆ and seek the one that minimizes
the cross validation risk, deﬁned as

J(∆) =

b

2

−

h(n

1) −

h(n

1)

n + 1

K

−

j=1
X

p2
j ,

b

/n and h = 1/K. Algorithm 2 details the proposed cross-validation scheme.

(8)

(9)

|

where

pj =

Bj|
Algorithm 2 Cross Validation
b
Input: Graphs G1, . . . , G2T .
Output: Blocks
B1, . . . ,
for a sequence of ∆’s do

b

Estimate blocks
B1, . . . ,
b
b
Compute
Bj|
pj =
|
b
J(∆) = 2
Compute
h(n
−
b
end for
b
Pick the ∆ with minimum
b

3.3 Consistency of

w

BK, and optimal ∆.

BK from G1, . . . , G2T . [Algorithm 1]

/n, for j = 1, . . . , K.
K
j=1

n+1

h(n

1)

b
1) −

−

p2
j , with h = 1/K.

P
J(∆), and the corresponding

b

B1, . . . ,

BK.

b

b

b

The goal of our next theorem is to show that
b
To begin with, let us ﬁrst recall two commonly used metric:
Deﬁnition 1. The mean squared error (MSE) and mean absolute error (MAE) are deﬁned as
b
n

w is a consistent estimate of w, i.e.,

w as n

→

w

b

n

.
→ ∞

Theorem 3. If S

Θ(n) and ∆

ω

MSE(

w) =

MAE(

b
w) =

1
n2

1
n2

b

iv =1
X
n

jv =1
X
n

|

1
4

iv =1
X

jv =1
X

log(n)
n

∈
E[MAE(

(cid:18)(cid:16)
w)] = 0

∩

(cid:19)
(cid:17)
and

∈

lim
n
→∞

b

5

(w(uiv , ujv )

w(uiv , ujv ))2

−

w(uiv , ujv )

b
w(uiv , ujv )
|

.

−

b
o(1), then

E[MSE(

w)] = 0.

lim
n
→∞

b

(G1[ix, jy] + G2[ix, jy] + . . . + G2T [ix, jy]) ,

Proof. The details of the proof can be found in the supplementary material . Here we only outline
the key steps to present the intuition of the theorem. The goal of Theorem 3 is to show convergence
of

. The idea is to consider the following two quantities:

w(uix , ujx),

|

|

b

−

w(ui, uj)

w(ui, uj) =

w(ui, uj)
|
1
Bi| |
1
b
Bi| |
so that if we can bound
b
w(ui, uj)
w(ui, uj)
|

w(ui, uj) =

b

|

b
b
Bj
Bi Xjx∈

Bj| Xix∈
b
Bj| Xix∈
w(ui, uj)
b
|
can also be bounded.

b
b
Bj
Bi Xjy∈

1
2T

−

w(ui, uj)
|

and

w(ui, uj)

|

w(ui, uj)
|

−

, then consequently

|

−

w(ui, uj)

w(ui, uj)
|

|
The bound for the ﬁrst term
b
vertex iv ∈
average over

Bj, by Theorem 1 a probability bound involving ∆ can be obtained.

−
Bi is guaranteed to be within a distance ∆ from the pivot of
Bi and
b
The bound for the second term
is shown in Lemma 2. Different from Lemma
b
1, here we need to consider two possible situations: either the intermediate estimate w(ui, uj) is
close to the ground truth w(ui, uj), or w(ui, uj) is far from the ground truth w(ui, uj). This ac-
counts for the sum in Lemma 2. Individual bounds are derived based on Lemma 1 and Theorem 1.

is shown in Lemma 1: By Algorithm 1, any
Bi. Since w(ui, uj) is an

w(ui, uj)
|

w(ui, uj)

−

b

b

b

b

|

Combining Lemma 1 and Lemma 2, we can then bound the error and show convergence.

Lemma 1. For any iv ∈
Pr

Bi and jv ∈
w(ui, uj)
b

Bj,

|
h

−

b

w(uiv , ujv )
|
Bj,

> 8∆1/2L1/4

32

S∆4
32/T +8∆2/3 .

e−

(10)

≤

i

|

Bi| |
b

Bj|
b

Pr

wij|

Bi and jv ∈
> 8∆1/2L1/4
b

Lemma 2. For any iv ∈
wij −
|
h
Θ(n) is necessary to make Theorem 3 valid, because if S is independent of n,
The condition S
the right hand sides of (10) and (11) cannot approach 0 even if n
. The condition on ∆ is also
important as it forces the numerators and denominators in the exponentials of (10) and (11) to be
well behaved.

S∆4
32/T +8∆2/3) .

√L∆) + 32

Bj|
b

Bi|
b

b
Bi| |

→ ∞

2e−
b

b
Bj |

2e−

(11)

256(T

≤

∈

b

i

2

|

|

|

4 Experiments

In this section we evaluate the proposed SBA algorithm by showing some empirical results. For the
purpose of comparison, we consider (i) the universal singular value thresholding (USVT) [8]; (ii)
the largest-gap algorithm (LG) [7]; (iii) matrix completion from few entries (OptSpace) [17].

4.1 Estimating stochastic blockmodels

Accuracy as a function of growing graph size. Our ﬁrst experiment is to evaluate the proposed
SBA algorithm for estimating stochastic blockmodels. For this purpose, we generate (arbitrarily) a
graphon

0.8 0.9 0.4 0.5
0.1 0.6 0.3 0.2
0.3 0.2 0.8 0.3
0.4 0.1 0.2 0.9

,






w = 




×

(12)

which represents a piecewise constant function with 4

4 equi-space blocks.

Since USVT and LG use only one observed graph whereas the proposed SBA require at least 2
observations, in order to make the comparison fair, we use half of the nodes for SBA by generating
n
two independent n
2 observed graphs. For USVT and LG, we use one n
Figure 2(a) shows the asymptotic behavior of the algorithms when n grows. Figure 2(b) shows the
estimation error of SBA algorithm as T grows for graphs of size 200 vertices.

n observed graph.

2 ×

×

6

−0.5

−1

)
E
A
M

(
0
1
g
o
l

−1.5

−2

−2.5

 
−3
0

−0.7

−0.8

−0.9

−1

−1.1

−1.2

−1.3

)
E
A
M

(
0
1
g
o
l

Proposed
Largest Gap
OptSpace
USVT

200

400

600

n

800

1000

5

10

15

25

30

35

40

20
2T

(a) Growing graph size, n

(b) Growing no. observations, 2T

Figure 2: (a) MAE reduces as graph size grows. For the fairness of the amount of data that can be
used, we use n
1 observation for USVT [8] and LG
[7]. (b) MAE of the proposed SBA algorithm reduces when more observations T is available. Both
plots are averaged over 100 independent trials.

2 observations for SBA, and n

n
2 ×

2 ×

×

×

n

Accuracy as a function of growing number of blocks. Our second experiment is to evaluate the
performance of the algorithms as K, the number of blocks, increases. To this end, we consider a
sequence of K, and for each K we generate a graphon w of K
K blocks. Each entry of the
block is a random number generated from Uniform[0, 1]. Same as the previous experiment, we ﬁx
n = 200 and T = 1. The experiment is repeated over 100 trials so that in every trial a different
graphon is generated. The result shown in Figure 3(a) indicates that while estimation error increases
as K grows, the proposed SBA algorithm still attains the lowest MAE for all K.

×

 

Proposed

 

 

 

)
E
A
M

(
0
1
g
o
l

−2

−2.1

−2.2

−2.3

−2.4

−2.5

−2.6

−2.7

−2.8

−2.9

 
−3
0

)
E
A
M

(
0
1
g
o
l

−0.6

−0.7

−0.8

−0.9

−1

−1.1

−1.2

−1.3

−1.4

−1.5

 
−1.6
0

Proposed
Largest Gap
USVT

 
−1.4
0

5

10
K

15

20

(a) Growing no. blocks, K

Proposed
Largest Gap
OptSpace
USVT

5

10
% missing links

15

20

(b) Missing links

Figure 3: (a) As K increases, MAE of all three algorithm increases but SBA still attains the lowest
MAE. Here, we use n
1 observation for USVT [8] and
LG [7]. (b) Estimation of graphon in the presence of missing links: As the amount of missing links
increases, estimation error also increases.

2 observations for SBA, and n

n
2 ×

2 ×

×

×

n

4.2 Estimation with missing edges

Our next experiment is to evaluate the performance of proposed SBA algorithm when there are
n binary matrix
missing edges in the observed graph. To model missing edges, we construct an n
×
M with probability Pr[M [i, j] = 0] = ξ, where 0
1 deﬁnes the percentage of missing
edges. Given ξ, 2T matrices are generated with missing edges, and the observed graphs are deﬁned
denotes the element-wise multiplication. The goal is to
as M1 ⊙
study how well SBA can reconstruct the graphon

w in the presence of missing links.

G1, . . . , M2T ⊙

G2T , where

⊙

≤

≤

ξ

b
7

The modiﬁcation of the proposed SBA algorithm for the case missing links is minimal: when com-
Bi and jy ∈
puting (6), instead of averaging over all ix ∈
Bj, we only average ix ∈
Bj
that are not masked out by all M ′s. Figure 3(b) shows the result of average over 100 independent
trials. Here, we consider the graphon given in (12), with n = 200 and T = 1. It is evident that SBA
b
b
b
outperforms its counterparts at a lower rate of missing links.

Bi and jy ∈
b

4.3 Estimating continuous graphons

Our ﬁnal experiment is to evaluate the proposed SBA algorithm in estimating continuous graphons.
Here, we consider two of the graphons reported in [8]:

w1(u, v) =

1 + exp

1
50(u2 + v2)
}

,

{−

and w2(u, v) = uv,

[0, 1]. Here, w2 can be considered as a special case of the Eigenmodel [13] or latent

where u, v
feature relational model [21].

∈

The results in Figure 4 shows that while both algorithms have improved estimates when n grows, the
performance depends on which of w1 and w2 that we are studying. This suggests that in practice the
choice of the algorithm should depend on the expected structure of the graphon to be estimated: If the
graph generated by the graphon demonstrates some low-rank properties, then USVT is likely to be
a better option. For more structured or complex graphons the proposed procedure is recommended.

 

Proposed
USVT

 

Proposed
USVT

)
E
A
M

(
0
1
g
o
l

−2.9

−2.95

−3

−3.05

−3.1

−3.15

 
−3.2
0

)
E
A
M

(
0
1
g
o
l

−0.6

−0.8

−1

−1.2

−1.4

−1.6

−1.8

 
−2
0

200

400

600

800

1000

n

200

400

600

n

800

1000

(a) graphon w1

(b) graphon w2

Figure 4: Comparison between SBA and USVT in estimating two continuous graphons w1 and w2.
Evidently, SBA performs better for w1 (high-rank) and worse for w2 (low-rank).

5 Concluding remarks

We presented a new computational tool for estimating graphons. The proposed algorithm approx-
imates the continuous graphon by a stochastic block-model, in which the ﬁrst step is to cluster
the unknown vertex labels into blocks by using an empirical estimate of the distance between two
graphon slices, and the second step is to build an empirical histogram to estimate the graphon. Com-
plete consistency analysis of the algorithm is derived. The algorithm was evaluated experimentally,
and we found that the algorithm is effective in estimating block structured graphons.

Implementation of the SBA algorithm is available online at https://github.com/airoldilab/SBA.

Acknowledgments. EMA is partially supported by NSF CAREER award IIS-1149662, ARO MURI
award W911NF-11-1-0036, and an Alfred P. Sloan Research Fellowship. SHC is partially supported
by a Croucher Foundation Post-Doctoral Research Fellowship.

References

[1] E.M. Airoldi, D.M. Blei, S.E. Fienberg, and E.P. Xing. Mixed-membership stochastic blockmodels.

Journal of Machine Learning Research, 9:1981–2014, 2008.

8

Advanced Study, Princeton, NJ, 1979.

30(1):137–154, 1989.

Theory, 56:2980–2998, Jun. 2010.

[2] D.J. Aldous. Representations for partially exchangeable arrays of random variables. Journal of Multi-

[3] H. Azari and E. M. Airoldi. Graphlet decomposition of a weighted network. Journal of Machine Learning

variate Analysis, 11:581–598, 1981.

Research, W&CP, 22:54–63, 2012.

[4] P.J. Bickel and A. Chen. A nonparametric view of network models and Newman-Girvan and other mod-

ularities. Proc. Natl. Acad. Sci. USA, 106:21068–21073, 2009.

[5] P.J. Bickel, A. Chen, and E. Levina. The method of moments and degree distributions for network models.

Annals of Statistics, 39(5):2280–2301, 2011.

[6] C. Borgs, J. Chayes, L. Lov´asz, V. T. S´os, B. Szegedy, and K. Vesztergombi. Graph limits and parameter

testing. In Proc. ACM Symposium on Theory of Computing, pages 261–270, 2006.

[7] A. Channarond, J. Daudin, and S. Robin. Classiﬁcation and estimation in the Stochastic Blockmodel

based on the empirical degrees. Electronic Journal of Statistics, 6:2574–2601, 2012.

[8] S. Chatterjee. Matrix estimation by universal singular value thresholding. ArXiv:1212.1247. 2012.
[9] D.S. Choi and P.J. Wolfe. Co-clustering separately exchangeable network data. ArXiv:1212.4093. 2012.
[10] D.S. Choi, P.J. Wolfe, and E.M. Airoldi. Stochastic blockmodels with a growing number of classes.

Biometrika, 99:273–284, 2012.

[11] P. Diaconis and S. Janson. Graph limits and exchangeable random graphs. Rendiconti di Matematica e

delle sue Applicazioni, Series VII, pages 33–61, 2008.

[12] A. Goldenberg, A.X. Zheng, S.E. Fienberg, and E.M. Airoldi. A survey of statistical network models.

Foundations and Trends in Machine Learning, 2:129–233, 2009.

[13] P.D. Hoff. Modeling homophily and stochastic equivalence in symmetric relational data.

In Neural

Information Processing Systems (NIPS), volume 20, pages 657–664, 2008.

[14] P.D. Hoff, A.E. Raftery, and M.S. Handcock. Latent space approaches to social network analysis. Journal

of the American Statistical Association, 97(460):1090–1098, 2002.

[15] D.N. Hoover. Relations on probability spaces and arrays of random variables. Preprint, Institute for

[16] O. Kallenberg. On the representation theorem for exchangeable arrays. Journal of Multivariate Analysis,

[17] R.H. Keshavan, A. Montanari, and S. Oh. Matrix completion from a few entries. IEEE Trans. Information

[18] N.D. Lawrence. Probabilistic non-linear principal component analysis with Gaussian process latent vari-

able models. Journal of Machine Learning Research, 6:1783–1816, 2005.

[19] J.R. Lloyd, P. Orbanz, Z. Ghahramani, and D.M. Roy. Random function priors for exchangeable arrays
with applications to graphs and relational data. In Neural Information Processing Systems (NIPS), 2012.
[20] L. Lov´asz and B. Szegedy. Limits of dense graph sequences. Journal of Combinatorial Theory, Series B,

96:933–957, 2006.

[21] K.T. Miller, T.L. Grifﬁths, and M.I. Jordan. Nonparametric latent fature models for link prediction. In

Neural Information Processing Systems (NIPS), 2009.

[22] K. Nowicki and T.A. Snijders. Estimation and prediction of stochastic block structures. Journal of

American Statistical Association, 96:1077–1087, 2001.

[23] P. Orbanz and D.M. Roy. Bayesian models of graphs, arrays and other exchangeable random structures,

2013. Unpublished manuscript.

[24] P.Latouche and S. Robin. Bayesian model averaging of stochastic block models to estimate the graphon
function and motif frequencies in a w-graph model. ArXiv:1310.6150, October 2013. Unpublished
manuscript.

[25] K. Rohe, S. Chatterjee, and B. Yu. Spectral clustering and the high-dimensional stochastic blockmodel.

[26] M. Tang, D.L. Sussman, and C.E. Priebe. Universally consistent vertex classiﬁcation for latent positions

Annals of Statistics, 39(4):1878–1915, 2011.

graphs. Annals of Statistics, 2013. In press.

[27] L. Wasserman. All of Nonparametric Statistics. Springer, 2005.
[28] P.J. Wolfe and S.C. Olhede. Nonparametric graphon estimation. ArXiv:1309.5936, September 2013.

Unpublished manuscript.

[29] Z. Xu, F. Yan, and Y. Qi. Inﬁnite Tucker decomposition: nonparametric Bayesian models for multiway

data analysis. In Proc. Intl. Conf. Machine Learning (ICML), 2012.

[30] Y. Zhao, E. Levina, and J. Zhu. Community extraction for social networks. In Proc. Natl. Acad. Sci. USA,

volume 108, pages 7321–7326, 2011.

9

