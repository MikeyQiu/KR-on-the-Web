8
1
0
2
 
y
a
M
 
4
2
 
 
]

V
C
.
s
c
[
 
 
2
v
6
1
1
6
0
.
2
1
7
1
:
v
i
X
r
a

Learning a Single Convolutional Super-Resolution Network for
Multiple Degradations

Kai Zhang1,2,3, Wangmeng Zuo1, Lei Zhang2
1School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China
2Dept. of Computing, The Hong Kong Polytechnic University, Hong Kong, China
3DAMO Academy, Alibaba Group
cskaizhang@gmail.com, wmzuo@hit.edu.cn, cslzhang@comp.polyu.edu.hk

Abstract

Recent years have witnessed the unprecedented success
of deep convolutional neural networks (CNNs) in single im-
age super-resolution (SISR). However, existing CNN-based
SISR methods mostly assume that a low-resolution (LR) im-
age is bicubicly downsampled from a high-resolution (HR)
image, thus inevitably giving rise to poor performance when
the true degradation does not follow this assumption. More-
over, they lack scalability in learning a single model to non-
blindly deal with multiple degradations. To address these
issues, we propose a general framework with dimensional-
ity stretching strategy that enables a single convolutional
super-resolution network to take two key factors of the SISR
degradation process, i.e., blur kernel and noise level, as in-
put. Consequently, the super-resolver can handle multiple
and even spatially variant degradations, which signiﬁcantly
improves the practicability. Extensive experimental results
on synthetic and real LR images show that the proposed
convolutional super-resolution network not only can pro-
duce favorable results on multiple degradations but also is
computationally efﬁcient, providing a highly effective and
scalable solution to practical SISR applications.

1. Introduction

Single image super-resolution (SISR) aims to recover a
high-resolution (HR) version of a low-resolution (LR) in-
put. As a classical problem, SISR is still an active yet chal-
lenging research topic in the ﬁeld of computer vision due to
its ill-poseness nature and high practical values [2]. In the
typical SISR framework, an LR image y is modeled as the
output of the following degradation process:

y = (x ⊗ k) ↓s + n,

(1)

where x ⊗ k represents the convolution between a blur ker-
nel k and a latent HR image x, ↓s is a subsequent downsam-
pling operation with scale factor s, and n usually is addi-
tive white Gaussian noise (AWGN) with standard deviation
(noise level) σ.

SISR methods can be broadly classiﬁed into three cate-
gories, i.e., interpolation-based methods, model-based op-
timization methods and discriminative learning methods.
Interpolation-based methods such as nearest-neighbor, bi-
linear and bicubic interpolators are simple and efﬁcient but
have very limited performance. By exploiting powerful im-
age priors (e.g., the non-local self-similarity prior [11, 32],
sparsity prior [52] and denoiser prior [4, 13, 57]), model-
based optimization methods are ﬂexible to reconstruct rel-
ative high-quality HR images, but they usually involve a
time-consuming optimization procedure. Although the in-
tegration of convolutional neural network (CNN) denoiser
prior and model-based optimization can improve the efﬁ-
ciency to some extent, it still suffers from the typical draw-
backs of model-based optimization methods, e.g., it is not in
an end-to-end learning manner and involves hand-designed
parameters [57]. As an alternative, discriminative learning
methods have attracted considerable attentions due to their
favorable SISR performance in terms of effectiveness and
efﬁciency. Notably, recent years have witnessed a dramatic
upsurge of using CNN for SISR.

In this paper, we focus on discriminative CNN methods
for SISR so as to exploit the merits of CNN, such as the fast
speed by parallel computing, high accuracy by end-to-end
training, and tremendous advances in training and design-
ing networks [16, 18, 21, 28]. While several SISR mod-
els based on discriminative CNN have reported impressive
results, they suffer from a common drawback: their mod-
els are specialized for a single simpliﬁed degradation (e.g.,
bicubic degradation) and lack scalability to handle multiple
degradations by using a single model. Because the practical
degradation of SISR is much more complex [40, 51], the
performance of learned CNN models may deteriorate seri-
ously when the assumed degradation deviates from the true
one, making them less effective in practical scenarios. It has
been pointed out that the blur kernel plays a vital role for the
success of SISR methods and the mismatch of blur kernels
will largely deteriorate the ﬁnal SISR results [12]. How-
ever, little work has been done on how to design a CNN to
address this crucial issue.

Given the facts above, it is natural to raise the follow-
ing questions, which are the focus of our paper: (i) Can we
learn a single model to effectively handle multiple and even
spatially variant degradations? (ii) Is it possible to use syn-
thetic data to train a model with high practicability? This
work aims to make one of the ﬁrst attempts towards answer-
ing these two questions.

To answer the ﬁrst question, we revisit and analyze the
general model-based SISR methods under the maximum a
posteriori (MAP) framework. Then we argue that one may
tackle this issue by taking LR input, blur kernel and noise
level as input to CNN but their dimensionality mismatch
makes it difﬁcult to design a single convolutional super-
resolution network. In view of this, we introduce a dimen-
sionality stretching strategy which facilitates the network to
handle multiple and even spatially variant degradations with
respect to blur kernel and noise. To the best of our knowl-
edge, there is no attempt to consider both the blur kernel
and noise for SISR via training a single CNN model.

For the second question, we will show that it is possible
to learn a practical super-resolver using synthetic data. To
this end, a large variety of degradations with different com-
binations of blur kernels and noise levels are sampled to
cover the degradation space. In a practical scenario, even
the degradation is more complex (e.g., the noise is non-
AWGN), we can select the best ﬁtted degradation model
rather than the bicubic degradation to produce a better re-
sult. It turns out that, by choosing a proper degradation, the
learned SISR model can yield perceptually convincing re-
sults on real LR images. It should be noted that we make no
effort to use specialized network architectures but use the
plain CNN as in [9, 41].

The main contributions of this paper are summarized in

the following:

• We propose a simple yet effective and scalable deep
CNN framework for SISR. The proposed model goes
beyond the widely-used bicubic degradation assump-
tion and works for multiple and even spatially variant
degradations, thus making a substantial step towards
developing a practical CNN-based super-resolver for
real applications.

• We propose a novel dimensionality stretching strategy
to address the dimensionality mismatch between LR
input image, blur kernel and noise level. Although this
strategy is proposed for SISR, it is general and can be
extended to other tasks such as deblurring.

• We show that

the proposed convolutional super-
resolution network learned from synthetic training data
can not only produce competitive results against state-
of-the-art SISR methods on synthetic LR images but
also give rise to visually plausible results on real LR
images.

2. Related Work

The ﬁrst work of using CNN to solve SISR can be
traced back to [8] where a three-layer super-resolution net-
work (SRCNN) was proposed. In the extended work [9],
the authors investigated the impact of depth on super-
resolution and empirically showed that the difﬁculty of
training deeper model hinders the performance improve-
ment of CNN super-resolvers. To overcome the training
difﬁculty, Kim et al. [24] proposed a very deep super-
resolution (VDSR) method with residual learning strategy.
Interestingly, they showed that VDSR can handle multiple
scales super-resolution. By analyzing the relation between
CNN and MAP inference, Zhang et al. [56] pointed out that
CNN mainly model the prior information and they empir-
ically demonstrated that a single model can handle mul-
tiple scales super-resolution, image deblocking and image
denoising. While achieving good performance, the above
methods take the bicubicly interpolated LR image as in-
put, which not only suffers from high computational cost
but also hinders the effective expansion of receptive ﬁeld.

To improve the efﬁciency, some researchers resort to di-
rectly manipulating the LR input and adopting an upscal-
ing operation at the end of the network. Shi et al. [41]
introduced an efﬁcient sub-pixel convolution layer to up-
scale the LR feature maps into HR images. Dong et al. [10]
adopted a deconvolution layer at the end of the network to
perform upsampling. Lai et al. [27] proposed a Laplacian
pyramid super-resolution network (LapSRN) that takes an
LR image as input and progressively predicts the sub-band
residuals with transposed convolutions in a coarse-to-ﬁne
manner. To improve the perceptual quality at a large scale
factor, Ledig et al. [29] proposed a generative adversarial
network [16] based super-resolution (SRGAN) method. In
the generator network of SRGAN, two sub-pixel convolu-
tion layers are used to efﬁciently upscale the LR input by a
factor of 4.

Although various techniques have been proposed for
SISR, the above CNN-based methods are tailored to the
widely-used settings of bicubic degradation, neglecting
their limited applicability for practical scenarios. An in-
teresting line of CNN-based methods which can go be-
yond bicubic degradation adopt a CNN denoiser to solve
SISR via model-based optimization framework [4, 34, 57].
For example,
the method proposed in [57] can handle
the widely-used Gaussian degradation as in [11]. How-
ever, manually selecting the hyper-parameters for different
degradations is not a trivial task [39]. As a result, it is desir-
able to learn a single SISR model which can handle multiple
degradations with high practicability. This paper attempts
to give a positive answer.

Due to the limited space, we can only discuss some of
the related works here. Other CNN-based SISR methods
can be found in [6, 22, 23, 30, 37, 42, 44, 45, 46, 53, 58].

3. Method

3.1. Degradation Model

Before solving the problem of SISR, it is important to
have a clear understanding of the degradation model which
is not limited to Eqn. (1). Another practical degradation
model can be given by

y = (x ↓s) ⊗ k + n.

(2)

e
s
i
o
N

When ↓ is the bicubic downsampler, Eqn. (2) corresponds
to a deblurring problem followed by a SISR problem with
bicubic degradation. Thus, it can beneﬁt from existing
deblurring methods and bicubic degradation based SISR
methods. Due to limited space, we only consider the more
widely assumed degradation model given in Eqn. (1). Nev-
ertheless, our method is general and can be easily extended
to handle Eqn. (2). In the following, we make a short dis-
cussion on blur kernel k, noise n and downsampler ↓.

Blur kernel. Different from image deblurring, the blur
kernel setting of SISR is usually simple. The most popu-
lar choice is isotropic Gaussian blur kernel parameterized
by standard deviation or kernel width [11, 51].
In [38],
anisotropic Gaussian blur kernels are also used. In practice,
more complex blur kernel models used in deblurring task,
such as motion blur [5], can be further considered. Empiri-
cal and theoretical analyses have revealed that the inﬂuence
of an accurate blur kernel is much larger than that of sophis-
ticated image priors [12]. Speciﬁcally, when the assumed
kernel is smoother than the true kernel, the recovered image
is over-smoothed. Most of SISR methods actually favor for
such case. On the other hand, when the assumed kernel is
sharper than the true kernel, high frequency ringing artifacts
will appear.

Noise. While being of low-resolution, the LR images are
usually also noisy. Directly super-resolving the noisy input
without noise removal would amplify the unwanted noise,
resulting in visually unpleasant results. To address this
problem, the straightforward way is to perform denoising
ﬁrst and then enhance the resolution. However, the denois-
ing pre-processing step tends to lose detail information and
would deteriorate the subsequent super-resolution perfor-
mance [43]. Thus, it would be highly desirable to jointly
perform denoising and super-resolution.

Downsampler. Existing literatures have considered two
types of downsamplers, including direct downsampler [11,
17, 36, 51, 55] and bicubic downsampler [7, 12, 14, 15,
47, 52]. In this paper, we consider the bicubic downsam-
pler since when k is delta kernel and the noise level is
zero, Eqn. (1) turns into the widely-used bicubic degrada-
It should be pointed out that, different from
tion model.
blur kernel and noise which vary in a general degradation
model, downsampler is assumed to be ﬁxed.

Blur Kernel
Figure 1. An illustration of different degradations for SISR. The
scale factor is 2. The general degradation models of Eqns. (1)
and (2) assume an HR image actually can degrade into many LR
images, whereas bicubic degradation model assumes an HR image
corresponds to a single LR image.

Though blur kernel and noise have been recognized as
key factors for the success of SISR and several methods
have been proposed to consider those two factors, there has
been little effort towards simultaneously considering blur
kernel and noise in a single CNN framework. It is a chal-
lenging task since the degradation space with respect to
blur kernel and noise is rather large (see Figure 1 as an ex-
ample). One relevant work is done by Zhang et al. [57];
nonetheless, their method is essentially a model-based op-
timization method and thus suffers from several drawbacks
as mentioned previously. In another related work, Riegler et
al. [38] exploited the blur kernel information into the SISR
model. Our method differs from [38] on two major as-
pects. First, our method considers a more general degra-
dation model. Second, our method exploits a more effective
way to parameterize the degradation model.

3.2. A Perspective from MAP Framework

Though existing CNN-based SISR methods are not nec-
essarily derived under the traditional MAP framework, they
have the same goal. We revisit and analyze the general
MAP framework of SISR, aiming to ﬁnd the intrinsic con-
nections between the MAP principle and the working mech-
anism of CNN. Consequently, more insights on CNN archi-
tecture design can be obtained.

Due to the ill-posed nature of SISR, regularization needs
to be imposed to constrain the solution. Mathematically,
the HR counterpart of an LR image y can be estimated by
solving the following MAP problem

ˆx = arg min x

1
2σ2 (cid:107)(x ⊗ k) ↓s −y(cid:107)2 + λΦ(x)

(3)

where 1
2σ2 (cid:107)(x⊗k) ↓s −y(cid:107)2 is the data ﬁdelity term, Φ(x) is
the regularization term (or prior term) and λ is the trade-off
parameter. Simply speaking, Eqn. (3) conveys two points:
(i) the estimated solution should not only accord with the
degradation process but also have the desired property of
clean HR images; (ii) ˆx is a function of LR image y, blur
kernel k, noise level σ, and trade-off parameter λ. There-
fore, the MAP solution of (non-blind) SISR can be formu-
lated as

ˆx = F(y, k, σ, λ; Θ)

(4)

where Θ denotes the parameters of the MAP inference.

By treating CNN as a discriminative learning solution to

Eqn. (4), we can have the following insights.

• Because the data ﬁdelity term corresponds to the degra-
dation process, accurate modeling of the degradation plays
a key role for the success of SISR. However, existing CNN-
based SISR methods with bicubic degradation actually aim
to solve the following problem

ˆx = arg min x(cid:107)x ↓s −y(cid:107)2 + Φ(x).

(5)

Inevitably, their practicability is very limited.

• To design a more practical SISR model, it is preferable to
learn a mapping function like Eqn. (4), which covers more
extensive degradations. It should be stressed that, since λ
can be absorbed into σ, Eqn. (4) can be reformulated as

• Considering that the MAP framework (Eqn. (3)) can per-
form generic image super-resolution with the same image
prior, it is intuitive to jointly perform denoising and SISR
in a uniﬁed CNN framework. Moreover, the work [56] in-
dicates that the parameters of the MAP inference mainly
model the prior; therefore, CNN has the capacity to deal
with multiple degradations via a single model.

From the viewpoint of MAP framework, one can see
that the goal of SISR is to learn a mapping function ˆx =
F(y, k, σ; Θ) rather than ˆx = F(y; Θ). However, it is not
an easy task to directly model ˆx = F(y, k, σ; Θ) via CNN.
The reason lies in the fact that the three inputs y, k and σ
have different dimensions. In the next subsection, we will
propose a simple dimensionality stretching strategy to re-
solve this problem.

3.3. Dimensionality Stretching

The proposed dimensionality stretching strategy is
schematically illustrated in Figure 2. Suppose the inputs
consist of a blur kernel of size p×p, a noise level σ and an
LR image of size W × H × C, where C denotes the number
of channels. The blur kernel is ﬁrst vectorized into a vector

Vectorization

PCA

Stretching

H

Blur Kernel

Noise Level

W

t+1

Degradation Maps

Figure 2. Schematic illustration of the dimensionality stretching
strategy. For an LR image of size W × H, the vectorized blur ker-
nel is ﬁrst projected onto a space of dimension t and then stretched
into a tensor M of size W × H × (t + 1) with the noise level.

of size p2 × 1 and then projected onto t-dimensional lin-
ear space by the PCA (Principal Component Analysis) tech-
nique. After that, the concatenated low dimensional vector
and the noise level, denoted by v, are stretched into degra-
dation maps M of size W × H × (t + 1), where all the
elements of i-th map are vi. By doing so, the degradation
maps then can be concatenated with the LR image, mak-
ing CNN possible to handle the three inputs. Such a simple
strategy can be easily exploited to deal with spatially vari-
ant degradations by considering the fact that the degradation
maps can be non-uniform.

The proposed super-resolution network for multiple
degradations, denoted by SRMD, is illustrated in Figure 3.
As one can see, the distinctive feature of SRMD is that
it takes the concatenated LR image and degradation maps
as input. To show the effectiveness of the dimensionality
stretching strategy, we resort to plain CNN without com-
plex architectural engineering. Typically, to super-resolve
an LR image with a scale factor of s, SRMD ﬁrst takes
the concatenated LR image and degradation maps of size
W × H × (C + t + 1) as input. Then, similar to [24], a
cascade of 3 × 3 convolutional layers are applied to perform
the non-linear mapping. Each layer is composed of three
types of operations, including Convolution (Conv), Recti-
ﬁed Linear Units (ReLU) [26], and Batch Normalization
(BN) [20]. Speciﬁcally, “Conv + BN + ReLU” is adopted
for each convolutional layer except the last convolutional
layer which consists of a single “Conv” operation. Finally, a
sub-pixel convolution layer [41] is followed by the last con-
volutional layer to convert multiple HR subimages of size
W × H × s2C to a single HR image of size sW × sH × C.
For all scale factors 2, 3 and 4, the number of convolu-
tional layers is set to 12, and the number of feature maps
in each layer is set to 128. We separately learn models for
each scale factor. In particular, we also learn the models for

ˆx = F(y, k, σ; Θ).

(6)

3.4. Proposed Network

LR Image & Degradation Maps

Nonlinear Mapping

HR Subimages

HR Image

Figure 3. The architecture of the proposed convolutional super-resolution network. In contrast to other CNN-based SISR methods which
only take the LR image as input and lack scalability to handle other degradations, the proposed network takes the concatenated LR image
and degradation maps as input, thus allowing a single model to manipulate multiple and even spatially variant degradations.

noise-free degradation, namely SRMDNF, by removing the
connection of the noise level map in the ﬁrst convolutional
ﬁlter and ﬁne-tuning with new training data.

It is worth pointing out that neither residual learning nor
bicubicly interpolated LR image is used for the network de-
sign due to the following reasons. First, with a moderate
network depth and advanced CNN training and design such
as ReLU [26], BN [20] and Adam [25], it is easy to train
the network without the residual learning strategy. Second,
since the degradation involves noise, bicubicly interpolated
LR image would aggravate the complexity of noise which
in turn will increase the difﬁculty of training.

3.5. Why not Learn a Blind Model?

To enhance the practicability of CNN for SISR, it seems
the most straightforward way is to learn a blind model with
synthesized training data by different degradations. How-
ever, such blind model does not perform as well as ex-
pected. First, the performance deteriorates seriously when
the blur kernel model is complex, e.g., motion blur. This
phenomenon can be explained by the following example.
Given an HR image, a blur kernel and corresponding LR
image, shifting the HR image to left by one pixel and shift-
ing the blur kernel to right by one pixel would result in the
same LR image. Thus, an LR image may correspond to dif-
ferent HR images with pixel shift. This in turn would aggra-
vate the pixel-wise average problem [29], typically leading
to over-smoothed results. Second, the blind model without
specially designed architecture design has inferior general-
ization ability and performs poorly in real applications.

In contrast, non-blind model for multiple degradations
suffers little from the pixel-wise average problem and has
better generalization ability. First, the degradation maps
contain the warping information and thus can enable the
network to have spatial transformation capability. For clar-
ity, one can treat the degradation maps induced by blur ker-
nel and noise level as the output of a spatial transformer
as in [21]. Second, by anchoring the model with degrada-
tion maps, the non-blind model generalizes easily to unseen
degradations and has the ability to control the tradeoff be-
tween data ﬁdelity term and regularization term.

4. Experiments

4.1. Training Data Synthesis and Network Training

Before synthesizing LR images according to Eqn. (1), it
is necessary to deﬁne the blur kernels and noise level range,
as well as providing a large-scale clean HR image set.

For the blur kernels, we follow the kernel model of
isotropic Gaussian with a ﬁxed kernel width which has been
proved practically feasible in SISR applications. Speciﬁ-
cally, the kernel width ranges are set to [0.2, 2], [0.2, 3] and
[0.2, 4] for scale factors 2, 3 and 4, respectively. We sample
the kernel width by a stride of 0.1. The kernel size is ﬁxed
to 15×15. To further expand the degradation space, we also
consider a more general kernel assumption, i.e., anisotropic
Gaussian, which is characterized by a Gaussian probabil-
ity density function N (0, Σ) with zero mean and varying
covariance matrix Σ [38]. The space of such Gaussian ker-
nel is determined by rotation angle of the eigenvectors of Σ
and scaling of corresponding eigenvalues. We set the rota-
tion angle range to [0, π]. For the scaling of eigenvalues, it
is set from 0.5 to 6, 8 and 10 for scale factors 2, 3 and 4,
respectively.

Although we adopt the bicubic downsampler through-
out the paper, it is straightforward to train a model with di-
rect downsampler. Alternatively, we can also include the
degradations with direct downsampler by approximating it.
Speciﬁcally, given a blur kernel kd under direct downsam-
pler ↓d, we can ﬁnd the corresponding blur kernel kb under
bicubic downsampler ↓b by solving the following problem
with a data-driven method

kb = arg min kb (cid:107)(x ⊗ kb) ↓b

s −(x ⊗ kd) ↓d

s (cid:107)2,

∀ x. (7)

In this paper, we also include such degradations for scale
factor 3.

Once the blur kernels are well-deﬁned or learned, we
then uniformly sample substantial kernels and aggregate
them to learn the PCA projection matrix. By preserving
about 99.8% of the energy, the kernels are projected onto a
space of dimension 15 (i.e., t = 15). The visualization of
some typical blur kernels for scale factor 3 and some PCA
eigenvectors is shown in Figure 4.

Figure 4. Visualization of six typical blur kernels (ﬁst row) of
isotropic Gaussian (ﬁrst two), anisotropic Gaussian (middle two)
and estimated ones for direct downsampler (last two) for scale fac-
tor 3 and PCA eigenvectors (second row) for the ﬁrst six largest
eigenvalues.

For the noise level range, we set it as [0, 75]. Because the
proposed method operates on RGB channels rather than Y
channel in YCbCr color space, we collect a large-scale color
images for training, including 400 BSD [33] images, 800
training images from DIV2K dataset [1] and 4, 744 images
from WED dataset [31].

Then, given an HR image, we synthesize LR image by
blurring it with a blur kernel k and bicubicly downsampling
it with a scale factor s, followed by an addition of AWGN
with noise level σ. The LR patch size is set to 40×40 which
means the corresponding HR patch sizes for scale factors 2,
3, and 4 are 80 × 80, 120 × 120 and 160 × 160, respectively.
In the training phase, we randomly select a blur ker-
nel and a noise level to synthesize an LR image and crop
N = 128×3, 000 LR/HR patch pairs (along with the degra-
dation maps) for each epoch. We optimize the following
loss function using Adam [25]

L(Θ) =

(cid:107)F(yi, Mi; Θ) − xi(cid:107)2.

(8)

1
2N

N
(cid:88)

i=1

The mini-batch size is set to 128. The learning rate starts
from 10−3 and reduces to 10−4 when the training error
stops decreasing. When the training error keeps unchanged
in ﬁve sequential epochs, we merge the parameters of each
batch normalization into the adjacent convolution ﬁlters.
Then, a small learning rate of 10−5 is used for additional
100 epochs to ﬁne-tune the model. Since SRMDNF is ob-
tained by ﬁne-tuning SRMD, its learning rate is ﬁxed to
10−5 for 200 epochs.

We train the models in Matlab (R2015b) environment
with MatConvNet package [48] and an Nvidia Titan X Pas-
cal GPU. The training of a single SRMD model can be done
in about two days. The source code can be downloaded at
https://github.com/cszn/SRMD.

4.2. Experiments on Bicubic Degradation

As mentioned above, instead of handling the bicubic
degradation only, our aim is to learn a single network to
handle multiple degradations. However, in order to show
the advantage of the dimensionality stretching strategy, the

proposed method is also compared with other CNN-based
methods speciﬁcally designed for bicubic degradation.

Table 1 shows the PSNR and SSIM [50] results of
state-of-the-art CNN-based SISR methods on four widely-
used datasets. As one can see, SRMD achieves compa-
rable results with VDSR at small scale factor and outper-
forms VDSR at large scale factor. In particular, SRMDNF
achieves the best overall quantitative results. Using Ima-
geNet dataset [26] to train the speciﬁc model with bicubic
degradation, SRResNet performs slightly better than SR-
MDNF on scale factor 4. To further compare with other
methods such as VDSR, we also have trained a SRMDNF
model (for scale factor 3) which operates on Y channel with
291 training images. The learned model achieves 33.97dB,
29.96dB, 28.95dB and 27.42dB on Set5, Set14, BSD100
and Urban100, respectively. As a result, it can still outper-
form other competing methods. The possible reason lies
in that the SRMDNF with multiple degradations shares the
same prior in the MAP framework which facilitates the im-
plicit prior learning and thus beneﬁts to PSNR improve-
ment. This also can explain why VDSR with multiple scales
improves the performance.

For the GPU run time, SRMD spends 0.084, 0.042 and
0.027 seconds to reconstruct an HR image of size 1, 024 ×
1, 024 for scale factors 2, 3 and 4, respectively. As a com-
parison, the run time of VDSR is 0.174 second for all scale
factors. Figure 5 shows the visual results of different meth-
ods. One can see that our proposed method yields very com-
petitive performance against other methods.

4.3. Experiments on General Degradations

In this subsection, we evaluate the performance of the
proposed method on general degradations. The degrada-
tion settings are given in Table 2. We only consider the
isotropic Gaussian blur kernel for an easy comparison. To
further show the scalability of the proposed method, another
widely-used degradation [11] which involves 7×7 Gaussian
kernel with width 1.6 and direct downsampler with scale
factor 3 is also included. We compare the proposed method
with VDSR, two model-based methods (i.e., NCSR [11]
and IRCNN [57]), and a cascaded denoising-SISR method
(i.e., DnCNN [56]+SRMDNF).

The quantitative results of different methods with dif-
ferent degradations on Set5 are provided in Table 2, from
which we have observations and analyses as follows. First,
the performance of VDSR deteriorates seriously when the
assumed bicubic degradation deviates from the true one.
Second, SRMD produces much better results than NCSR
and IRCNN, and outperforms DnCNN+SRMDNF. In par-
ticular, the PSNR gain of SRMD over DnCNN+SRMDNF
increases with the kernel width which veriﬁes the advantage
of joint denoising and super-resolution. Third, by setting
proper blur kernel, the proposed method delivers good per-

Table 1. Average PSNR and SSIM results for bicubic degradation on datasets Set5 [3], Set14 [54], BSD100 [33] and Urban100 [19]. The
best two results are highlighted in red and blue colors, respectively.

Dataset

Scale
Factor

Bicubic

SRCNN [9]

VDSR [24]

SRResNet [29]

DRRN [44]

LapSRN [27]

SRMD

SRMDNF

PSNR / SSIM

Set5

Set14

BSD100

Urban100

×2
×3
×4
×2
×3
×4
×2
×3
×4
×2
×3
×4

33.64 / 0.929
30.39 / 0.868
28.42 / 0.810
30.22 / 0.868
27.53 / 0.774
25.99 / 0.702
29.55 / 0.843
27.20 / 0.738
25.96 / 0.667
26.66 / 0.841
24.46 / 0.737
23.14 / 0.657

36.62 / 0.953
32.74 / 0.908
30.48 / 0.863
32.42 / 0.906
29.27 / 0.821
27.48 / 0.751
31.34 / 0.887
28.40 / 0.786
26.90 / 0.710
29.53 / 0.897
26.25 / 0.801
24.52 / 0.722

37.56 / 0.959
33.67 / 0.922
31.35 / 0.885
33.02 / 0.913
29.77 / 0.832
27.99 / 0.766
31.89 / 0.896
28.82 / 0.798
27.28 / 0.726
30.76 / 0.914
27.13 / 0.828
25.17 / 0.753

–
–
32.05 / 0.891
–
–
28.49 / 0.780
–
–
27.58 / 0.735
–
–
–

37.66 / 0.959
33.93 / 0.923
31.58 / 0.886
33.19 / 0.913
29.94 / 0.834
28.18 / 0.770
32.01 / 0.897
28.91 / 0.799
27.35 / 0.726
31.02 / 0.916
27.38 / 0.833
25.35 / 0.758

37.52 / 0.959
33.82 / 0.922
31.54 / 0.885
33.08 / 0.913
29.89 / 0.834
28.19 / 0.772
31.80 / 0.895
28.82 / 0.798
27.32 / 0.727
30.82 / 0.915
27.07 / 0.828
25.21 / 0.756

37.53 / 0.959
33.86 / 0.923
31.59 / 0.887
33.12 / 0.914
29.84 / 0.833
28.15 / 0.772
31.90 / 0.896
28.87 / 0.799
27.34 / 0.728
30.89 / 0.916
27.27 / 0.833
25.34 / 0.761

37.79 / 0.960
34.12 / 0.925
31.96 / 0.893
33.32 / 0.915
30.04 / 0.837
28.35 / 0.777
32.05 / 0.898
28.97 / 0.803
27.49 / 0.734
31.33 / 0.920
27.57 / 0.840
25.68 / 0.773

(a) SRCNN (23.78dB)

(b) VDSR (24.20dB)

(c) DRRN (25.11dB)

(d) LapSR (24.47dB)

(e) SRMD (25.09dB)

(f)SRMDNF (25.74dB)

Figure 5. SISR performance comparison of different methods with scale factor 4 on image “Img 099” from Urban100.

Table 2. Average PSNR and SSIM results of different methods with different degradations on Set5. The best results are highlighted in red
color. The results highlighted in gray color indicate unfair comparison due to mismatched degradation assumption.

Degradation Settings
Down-
sampler

Kernel
Width

Noise
Level

VDSR [24]

NCSR [11]

IRCNN [57]

DnCNN [56]+SRMDNF

SRMD

SRMDNF

PSNR (×2/×3/×4)

0.2
0.2
0.2
1.3
1.3
1.3
2.6
2.6
2.6

1.6

Bicubic
Bicubic
Bicubic
Bicubic
Bicubic
Bicubic
Bicubic
Bicubic
Bicubic

Direct

0
15
50
0
15
50
0
15
50

0

37.56/33.67/31.35
26.02/25.40/24.70
16.02/15.72/15.46
30.57/30.24/29.72
24.82/24.70/24.30
15.89/15.68/15.43
26.37/26.31/26.28
23.09/23.07/22.98
15.58/15.43/15.23

– /23.82/–
–
–
– /21.81/–
–
–
– /21.46/–
–
–

37.43/33.39/31.02
32.60/30.08/28.35
28.20/26.25/24.95
36.01/33.33/31.01
29.96/28.68/27.71
26.69/25.20/24.42
32.07/31.09/30.06
26.44/25.67/24.36
22.98/22.16/21.43

–
32.47/30.07/28.31
28.20/26.27/24.93
–
27.68/28.78/27.71
24.35/25.19/24.39
–

– /21.33/23.85
– /19.03/21.15

37.53/33.86/31.59
32.76/30.43/28.79
28.51/26.48/25.18
37.04/33.77/31.56
30.98/29.43/28.21
27.43/25.82/24.77
33.24/32.59/31.20
28.48/27.55/26.82
25.85/24.75/23.98

37.79/34.12/31.96
–
–
37.45/34.16/31.99
–
–
34.12/33.02/31.77
–
–

– /30.54/ –

– /33.02/ –

– /33.38/ –

–

– /33.74/ –

– /34.01/ –

(a) Ground-truth

(f) SRMDNF (30.34dB)
Figure 6. SISR performance comparison on image “Butterﬂy” from Set5. The degradation involves 7×7 Gaussian kernel with width 1.6
and direct downsampler with scale factor 3. Note that the comparison with VDSR is unfair because of degradation mismatch.

(d) IRCNN (29.32dB)

(e) SRMD (29.79dB)

(b) VDSR (24.73dB)

(c) NCSR (28.01dB)

formance in handling the degradation with direct downsam-
pler. The visual comparison is given in Figure 6. One can
see that NCSR and IRCNN produce more visually pleasant
results than VDSR since their assumed degradation matches
the true one. However, they cannot recover edges as sharper
as SRMD and SRMDNF.

4.4. Experiments on Spatially Variant Degradation

To demonstrate the effectiveness of SRMD for spatially
variant degradation, we synthesize an LR images with spa-
tially variant blur kernels and noise levels. Figure 7 shows
the visual result of the proposed SRMD for the spatially

(a)

(b)

(c)

Figure 7. An example of SRMD on dealing with spatially variant
degradation. (a) Noise level and Gaussian blur kernel width maps.
(b) Zoomed LR image. (c) Results of SRMD with scale factor 2.

(a) LR image

(b) VDSR [24]

(c) SelfEx [19]

(d) SRMD

Figure 9. SISR results on real image “Chip” with scale factor 4.

ure 8 that the performance of VDSR is severely affected by
the compression artifacts. While Waifu2x can successfully
remove the compression artifacts, it fails to recover sharp
edges. In comparison, SRMD can not only remove the un-
satisfying artifacts but also produce sharp edges. From Fig-
ure 9, we can see that VDSR and SelfEx both tend to pro-
duce over-smoothed results, whereas SRMD can recover
sharp image with better intensity and gradient statistics of
clean images [35].

5. Conclusion

In this paper, we proposed an effective super-resolution
network with high scalability of handling multiple degra-
dations via a single model. Different from existing CNN-
based SISR methods,
the proposed super-resolver takes
both LR image and its degradation maps as input. Specif-
ically, degradation maps are obtained by a simple dimen-
sionality stretching of the degradation parameters (i.e., blur
kernel and noise level). The results on synthetic LR images
demonstrated that the proposed super-resolver can not only
produce state-of-the-art results on bicubic degradation but
also perform favorably on other degradations and even spa-
tially variant degradations. Moreover, the results on real
LR images showed that the proposed method can recon-
struct visually plausible HR images. In summary, the pro-
posed super-resolver offers a feasible solution toward prac-
tical CNN-based SISR applications.

6. Acknowledgements

This work is supported by National Natural Science
Foundation of China (grant no. 61671182, 61471146),
HK RGC General Research Fund (PolyU 152240/15E)
and PolyU-Alibaba Collaborative Research Project “Qual-
ity Enhancement of Surveillance Images and Videos”. We
gratefully acknowledge the support from NVIDIA Corpo-
ration for providing us the Titan Xp GPU used in this re-
search.

(a) LR image

(b) VDSR [24]

(c) Waifu2x [49]

(d) SRMD

Figure 8. SISR results on image “Cat” with scale factor 2.

variant degradations. One can see that the proposed SRMD
is effective in recovering the latent HR image. Note that the
blur kernel is assumed to be isotropic Gaussian.

4.5. Experiments on Real Images

Besides the above experiments on LR images syntheti-
cally downsampled from HR images with known blur ker-
nels and corrupted by AWGN with known noise levels, we
also do experiments on real LR images to demonstrate the
effectiveness of the proposed SRMD. Since there are no
ground-truth HR images, we only provide the visual com-
parison.

As aforementioned, while we also use anisotropic Gaus-
sian kernels in training,
it is generally feasible to use
isotropic Gaussian for most of the real LR images in test-
ing. To ﬁnd the degradation parameters with good visual
quality, we use a grid search strategy rather than adopting
any blur kernel or noise level estimation methods. Speciﬁ-
cally, the kernel width is uniformly sampled from 0.1 to 2.4
with a stride of 0.1, and the noise level is from 0 to 75 with
stride 5.

Figures 8 and 9 illustrate the SISR results on two real LR
images “Cat” and “Chip”, respectively. The VDSR [24] is
used as one of the representative CNN-based methods for
comparison. For image “Cat” which is corrupted by com-
pression artifacts, Waifu2x [49] is also used for compari-
son. For image “Chip” which contains repetitive structures,
a self-similarity based method SelfEx [19] is also included
for comparison.

It can be observed from the visual results that SRMD
can produce much more visually plausible HR images than
the competing methods. Speciﬁcally, one can see from Fig-

References

[1] E. Agustsson and R. Timofte. Ntire 2017 challenge on sin-
gle image super-resolution: Dataset and study. In IEEE Con-
ference on Computer Vision and Pattern Recognition Work-
shops, volume 3, pages 126–135, July 2017.

[2] S. Baker and T. Kanade. Limits on super-resolution and how
to break them. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 24(9):1167–1183, 2002.

[3] M. Bevilacqua, A. Roumy, C. Guillemot, and M.-L. A.
Morel. Low-complexity single-image super-resolution based
on nonnegative neighbor embedding. In British Machine Vi-
sion Conference, 2012.

[4] S. A. Bigdeli, M. Jin, P. Favaro, and M. Zwicker. Deep mean-
shift priors for image restoration. In Advances in Neural In-
formation Processing Systems, 2017.

[5] G. Boracchi and A. Foi. Modeling the performance of image
restoration from motion blur. IEEE Transactions on Image
Processing, 21(8):3502–3517, Aug 2012.

[6] Y. Chen, W. Yu, and T. Pock. On learning optimized reac-
tion diffusion processes for effective image restoration.
In
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 5261–5269, 2015.

[7] Z. Cui, H. Chang, S. Shan, B. Zhong, and X. Chen. Deep
In European

network cascade for image super-resolution.
Conference on Computer Vision, pages 49–64, 2014.

[9] C. Dong, C. C. Loy, K. He, and X. Tang.

[8] C. Dong, C. C. Loy, K. He, and X. Tang. Learning a deep
convolutional network for image super-resolution. In Euro-
pean Conference on Computer Vision, pages 184–199, 2014.
Image
IEEE
super-resolution using deep convolutional networks.
Transactions on Pattern Analysis and Machine Intelligence,
38(2):295–307, 2016.

[10] C. Dong, C. C. Loy, and X. Tang. Accelerating the super-
resolution convolutional neural network. In European Con-
ference on Computer Vision, pages 391–407, 2016.

[11] W. Dong, L. Zhang, G. Shi, and X. Li. Nonlocally central-
ized sparse representation for image restoration. IEEE Trans-
actions on Image Processing, 22(4):1620–1630, 2013.
[12] N. Efrat, D. Glasner, A. Apartsin, B. Nadler, and A. Levin.
Accurate blur models vs. image priors in single image super-
resolution. In IEEE International Conference on Computer
Vision, pages 2832–2839, 2013.
[13] K. Egiazarian and V. Katkovnik.

Single image super-
In European Signal

resolution via BM3D sparse coding.
Processing Conference, pages 2849–2853, 2015.

[14] W. Freeman and C. Liu. Markov random ﬁelds for super-
resolution and texture synthesis. Advances in Markov Ran-
dom Fields for Vision and Image Processing, 1:155–165,
2011.

[15] D. Glasner, S. Bagon, and M. Irani. Super-resolution from
a single image. In IEEE International Conference on Com-
puter Vision, pages 349–356, 2009.

[16] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in neural information
processing systems, pages 2672–2680, 2014.

[17] H. He and W.-C. Siu. Single image super-resolution using
Gaussian process regression. In IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 449–456, 2011.

[18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
In IEEE Conference on Computer

for image recognition.
Vision and Pattern Recognition, pages 770–778, 2016.
[19] J.-B. Huang, A. Singh, and N. Ahuja. Single image super-
resolution from transformed self-exemplars. In IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
5197–5206, 2015.

[20] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
International Conference on Machine Learning, pages 448–
456, 2015.

[21] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial
In Advances in Neural Information

transformer networks.
Processing Systems, pages 2017–2025, 2015.

[22] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for
In European

real-time style transfer and super-resolution.
Conference on Computer Vision, pages 694–711, 2016.
[23] J. Kim, J. Kwon Lee, and K. Mu Lee. Deeply-recursive con-
volutional network for image super-resolution. In IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
1637–1645, 2016.

[24] J. Kim, J. K. Lee, and K. M. Lee. Accurate image super-
resolution using very deep convolutional networks. In IEEE
Conference on Computer Vision and Pattern Recognition,
pages 1646–1654, 2016.

[25] D. Kingma and J. Ba. Adam: A method for stochastic op-
timization. In International Conference for Learning Repre-
sentations, 2015.

[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in Neural Information Processing Systems, pages
1097–1105, 2012.

[27] W.-S. Lai, J.-B. Huang, N. Ahuja, and M.-H. Yang. Deep
laplacian pyramid networks for fast and accurate super-
In IEEE Conference on Computer Vision and
resolution.
Pattern Recognition, pages 624–632, July 2017.

[28] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature,

521(7553):436–444, 2015.

[29] C. Ledig, L. Theis, F. Husz´ar, J. Caballero, A. Cunningham,
A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, et al.
Photo-realistic single image super-resolution using a gen-
In IEEE Conference on Com-
erative adversarial network.
puter Vision and Pattern Recognition, pages 4681–4690,
July 2017.

[30] B. Lim, S. Son, H. Kim, S. Nah, and K. M. Lee. Enhanced
deep residual networks for single image super-resolution. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion Workshops, pages 136–144, July 2017.

[31] K. Ma, Z. Duanmu, Q. Wu, Z. Wang, H. Yong, H. Li, and
L. Zhang. Waterloo exploration database: New challenges
for image quality assessment models. IEEE Transactions on
Image Processing, 26(2):1004–1016, 2017.

[32] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman.
Non-local sparse models for image restoration. In IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
2272–2279, 2009.

[33] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database
of human segmented natural images and its application to
evaluating segmentation algorithms and measuring ecologi-

Conference, pages 689–692, 2015.

[49] Waifu2x.

Image super-resolution for anime-style art using
deep convolutional neural networks. http://waifu2x.
udp.jp/.

[50] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli.
from error visibility to struc-
IEEE Transactions on Image Processing,

Image quality assessment:
tural similarity.
13(4):600–612, 2004.

[51] C.-Y. Yang, C. Ma, and M.-H. Yang. Single-image super-
resolution: A benchmark. In European Conference on Com-
puter Vision, pages 372–386, 2014.

[52] J. Yang, J. Wright, T. S. Huang, and Y. Ma. Image super-
resolution via sparse representation. IEEE Transactions on
Image Processing, 19(11):2861–2873, 2010.

[53] W. Yang, J. Feng, J. Yang, F. Zhao, J. Liu, Z. Guo, and
S. Yan. Deep edge guided recurrent residual learning for
image super-resolution. IEEE Transactions on Image Pro-
cessing, 26(12):5895–5907, 2017.

[54] R. Zeyde, M. Elad, and M. Protter. On single image scale-up
using sparse-representations. In International conference on
curves and surfaces, pages 711–730, 2010.

[55] K. Zhang, X. Zhou, H. Zhang, and W. Zuo. Revisiting sin-
gle image super-resolution under internet environment: blur
kernels and reconstruction algorithms. In Paciﬁc Rim Con-
ference on Multimedia, pages 677–687, 2015.

[56] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang. Be-
yond a gaussian denoiser: Residual learning of deep CNN
for image denoising. IEEE Transactions on Image Process-
ing, pages 3142–3155, 2017.

[57] K. Zhang, W. Zuo, S. Gu, and L. Zhang. Learning deep CNN
In IEEE Conference
denoiser prior for image restoration.
on Computer Vision and Pattern Recognition, pages 3929–
3938, July 2017.

[58] Y. Zhang, Y. Tian, Y. Kong, B. Zhong, and Y. Fu. Residual
dense network for image super-resolution. In IEEE Confer-
ence on Computer Vision and Pattern Recognition, 2018.

cal statistics. In IEEE International Conference on Computer
Vision, volume 2, pages 416–423, July 2001.

[34] T. Meinhardt, M. M¨oller, C. Hazirbas, and D. Cremers.
Learning proximal operators: Using denoising networks for
In IEEE Interna-
regularizing inverse imaging problems.
tional Conference on Computer Vision, pages 1781–1790,
2017.

[35] J. Pan, Z. Hu, Z. Su, and M.-H. Yang. Deblurring text images
via L0-regularized intensity and gradient prior. In IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
2901–2908, 2014.

[36] T. Peleg and M. Elad. A statistical prediction model based
on sparse representations for single image super-resolution.
IEEE transactions on Image Processing, 23(6):2569–2582,
2014.

[37] J. S. Ren, L. Xu, Q. Yan, and W. Sun. Shepard convolu-
tional neural networks. In Advances in Neural Information
Processing Systems, pages 901–909, 2015.

[38] G. Riegler, S. Schulter, M. Ruther, and H. Bischof. Condi-
tioned regression models for non-blind single image super-
resolution. In IEEE International Conference on Computer
Vision, pages 522–530, 2015.

[39] Y. Romano, M. Elad, and P. Milanfar. The little engine that
could: Regularization by denoising (red). SIAM Journal on
Imaging Sciences, 10(4):1804–1844, 2017.

[40] Y. Romano, J. Isidoro, and P. Milanfar. RAISR: rapid and ac-
curate image super resolution. IEEE Transactions on Com-
putational Imaging, 3(1):110–125, 2017.

[41] W. Shi, J. Caballero, F. Husz´ar, J. Totz, A. P. Aitken,
R. Bishop, D. Rueckert, and Z. Wang. Real-time single im-
age and video super-resolution using an efﬁcient sub-pixel
convolutional neural network. In IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 1874–1883,
2016.

[42] Y. Shi, K. Wang, C. Chen, L. Xu, and L. Lin. Structure-
preserving image super-resolution via contextualized multi-
task learning. IEEE Transactions on Multimedia, 2017.
[43] A. Singh, F. Porikli, and N. Ahuja. Super-resolving noisy
images. In IEEE Conference on Computer Vision and Pattern
Recognition, pages 2846–2853, 2014.

[44] Y. Tai, J. Yang, and X. Liu. Image super-resolution via deep
In IEEE Conference on Com-
recursive residual network.
puter Vision and Pattern Recognition, pages 3147–3155,
2017.

[45] Y. Tai, J. Yang, X. Liu, and C. Xu. Memnet: A persistent
In IEEE Interna-
memory network for image restoration.
tional Conference on Computer Vision, pages 4539–4547,
2017.

[46] R. Timofte, E. Agustsson, L. Van Gool, M.-H. Yang, and
L. Zhang. Ntire 2017 challenge on single image super-
In IEEE Conference on
resolution: Methods and results.
Computer Vision and Pattern Recognition Workshops, pages
114–125, July 2017.

[47] R. Timofte, V. De Smet, and L. Van Gool. A+: Adjusted
anchored neighborhood regression for fast super-resolution.
In Asian Conference on Computer Vision, pages 111–126,
2014.

[48] A. Vedaldi and K. Lenc. MatConvNet: Convolutional neu-
ral networks for matlab. In ACM Conference on Multimedia

8
1
0
2
 
y
a
M
 
4
2
 
 
]

V
C
.
s
c
[
 
 
2
v
6
1
1
6
0
.
2
1
7
1
:
v
i
X
r
a

Learning a Single Convolutional Super-Resolution Network for
Multiple Degradations

Kai Zhang1,2,3, Wangmeng Zuo1, Lei Zhang2
1School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China
2Dept. of Computing, The Hong Kong Polytechnic University, Hong Kong, China
3DAMO Academy, Alibaba Group
cskaizhang@gmail.com, wmzuo@hit.edu.cn, cslzhang@comp.polyu.edu.hk

Abstract

Recent years have witnessed the unprecedented success
of deep convolutional neural networks (CNNs) in single im-
age super-resolution (SISR). However, existing CNN-based
SISR methods mostly assume that a low-resolution (LR) im-
age is bicubicly downsampled from a high-resolution (HR)
image, thus inevitably giving rise to poor performance when
the true degradation does not follow this assumption. More-
over, they lack scalability in learning a single model to non-
blindly deal with multiple degradations. To address these
issues, we propose a general framework with dimensional-
ity stretching strategy that enables a single convolutional
super-resolution network to take two key factors of the SISR
degradation process, i.e., blur kernel and noise level, as in-
put. Consequently, the super-resolver can handle multiple
and even spatially variant degradations, which signiﬁcantly
improves the practicability. Extensive experimental results
on synthetic and real LR images show that the proposed
convolutional super-resolution network not only can pro-
duce favorable results on multiple degradations but also is
computationally efﬁcient, providing a highly effective and
scalable solution to practical SISR applications.

1. Introduction

Single image super-resolution (SISR) aims to recover a
high-resolution (HR) version of a low-resolution (LR) in-
put. As a classical problem, SISR is still an active yet chal-
lenging research topic in the ﬁeld of computer vision due to
its ill-poseness nature and high practical values [2]. In the
typical SISR framework, an LR image y is modeled as the
output of the following degradation process:

y = (x ⊗ k) ↓s + n,

(1)

where x ⊗ k represents the convolution between a blur ker-
nel k and a latent HR image x, ↓s is a subsequent downsam-
pling operation with scale factor s, and n usually is addi-
tive white Gaussian noise (AWGN) with standard deviation
(noise level) σ.

SISR methods can be broadly classiﬁed into three cate-
gories, i.e., interpolation-based methods, model-based op-
timization methods and discriminative learning methods.
Interpolation-based methods such as nearest-neighbor, bi-
linear and bicubic interpolators are simple and efﬁcient but
have very limited performance. By exploiting powerful im-
age priors (e.g., the non-local self-similarity prior [11, 32],
sparsity prior [52] and denoiser prior [4, 13, 57]), model-
based optimization methods are ﬂexible to reconstruct rel-
ative high-quality HR images, but they usually involve a
time-consuming optimization procedure. Although the in-
tegration of convolutional neural network (CNN) denoiser
prior and model-based optimization can improve the efﬁ-
ciency to some extent, it still suffers from the typical draw-
backs of model-based optimization methods, e.g., it is not in
an end-to-end learning manner and involves hand-designed
parameters [57]. As an alternative, discriminative learning
methods have attracted considerable attentions due to their
favorable SISR performance in terms of effectiveness and
efﬁciency. Notably, recent years have witnessed a dramatic
upsurge of using CNN for SISR.

In this paper, we focus on discriminative CNN methods
for SISR so as to exploit the merits of CNN, such as the fast
speed by parallel computing, high accuracy by end-to-end
training, and tremendous advances in training and design-
ing networks [16, 18, 21, 28]. While several SISR mod-
els based on discriminative CNN have reported impressive
results, they suffer from a common drawback: their mod-
els are specialized for a single simpliﬁed degradation (e.g.,
bicubic degradation) and lack scalability to handle multiple
degradations by using a single model. Because the practical
degradation of SISR is much more complex [40, 51], the
performance of learned CNN models may deteriorate seri-
ously when the assumed degradation deviates from the true
one, making them less effective in practical scenarios. It has
been pointed out that the blur kernel plays a vital role for the
success of SISR methods and the mismatch of blur kernels
will largely deteriorate the ﬁnal SISR results [12]. How-
ever, little work has been done on how to design a CNN to
address this crucial issue.

Given the facts above, it is natural to raise the follow-
ing questions, which are the focus of our paper: (i) Can we
learn a single model to effectively handle multiple and even
spatially variant degradations? (ii) Is it possible to use syn-
thetic data to train a model with high practicability? This
work aims to make one of the ﬁrst attempts towards answer-
ing these two questions.

To answer the ﬁrst question, we revisit and analyze the
general model-based SISR methods under the maximum a
posteriori (MAP) framework. Then we argue that one may
tackle this issue by taking LR input, blur kernel and noise
level as input to CNN but their dimensionality mismatch
makes it difﬁcult to design a single convolutional super-
resolution network. In view of this, we introduce a dimen-
sionality stretching strategy which facilitates the network to
handle multiple and even spatially variant degradations with
respect to blur kernel and noise. To the best of our knowl-
edge, there is no attempt to consider both the blur kernel
and noise for SISR via training a single CNN model.

For the second question, we will show that it is possible
to learn a practical super-resolver using synthetic data. To
this end, a large variety of degradations with different com-
binations of blur kernels and noise levels are sampled to
cover the degradation space. In a practical scenario, even
the degradation is more complex (e.g., the noise is non-
AWGN), we can select the best ﬁtted degradation model
rather than the bicubic degradation to produce a better re-
sult. It turns out that, by choosing a proper degradation, the
learned SISR model can yield perceptually convincing re-
sults on real LR images. It should be noted that we make no
effort to use specialized network architectures but use the
plain CNN as in [9, 41].

The main contributions of this paper are summarized in

the following:

• We propose a simple yet effective and scalable deep
CNN framework for SISR. The proposed model goes
beyond the widely-used bicubic degradation assump-
tion and works for multiple and even spatially variant
degradations, thus making a substantial step towards
developing a practical CNN-based super-resolver for
real applications.

• We propose a novel dimensionality stretching strategy
to address the dimensionality mismatch between LR
input image, blur kernel and noise level. Although this
strategy is proposed for SISR, it is general and can be
extended to other tasks such as deblurring.

• We show that

the proposed convolutional super-
resolution network learned from synthetic training data
can not only produce competitive results against state-
of-the-art SISR methods on synthetic LR images but
also give rise to visually plausible results on real LR
images.

2. Related Work

The ﬁrst work of using CNN to solve SISR can be
traced back to [8] where a three-layer super-resolution net-
work (SRCNN) was proposed. In the extended work [9],
the authors investigated the impact of depth on super-
resolution and empirically showed that the difﬁculty of
training deeper model hinders the performance improve-
ment of CNN super-resolvers. To overcome the training
difﬁculty, Kim et al. [24] proposed a very deep super-
resolution (VDSR) method with residual learning strategy.
Interestingly, they showed that VDSR can handle multiple
scales super-resolution. By analyzing the relation between
CNN and MAP inference, Zhang et al. [56] pointed out that
CNN mainly model the prior information and they empir-
ically demonstrated that a single model can handle mul-
tiple scales super-resolution, image deblocking and image
denoising. While achieving good performance, the above
methods take the bicubicly interpolated LR image as in-
put, which not only suffers from high computational cost
but also hinders the effective expansion of receptive ﬁeld.

To improve the efﬁciency, some researchers resort to di-
rectly manipulating the LR input and adopting an upscal-
ing operation at the end of the network. Shi et al. [41]
introduced an efﬁcient sub-pixel convolution layer to up-
scale the LR feature maps into HR images. Dong et al. [10]
adopted a deconvolution layer at the end of the network to
perform upsampling. Lai et al. [27] proposed a Laplacian
pyramid super-resolution network (LapSRN) that takes an
LR image as input and progressively predicts the sub-band
residuals with transposed convolutions in a coarse-to-ﬁne
manner. To improve the perceptual quality at a large scale
factor, Ledig et al. [29] proposed a generative adversarial
network [16] based super-resolution (SRGAN) method. In
the generator network of SRGAN, two sub-pixel convolu-
tion layers are used to efﬁciently upscale the LR input by a
factor of 4.

Although various techniques have been proposed for
SISR, the above CNN-based methods are tailored to the
widely-used settings of bicubic degradation, neglecting
their limited applicability for practical scenarios. An in-
teresting line of CNN-based methods which can go be-
yond bicubic degradation adopt a CNN denoiser to solve
SISR via model-based optimization framework [4, 34, 57].
For example,
the method proposed in [57] can handle
the widely-used Gaussian degradation as in [11]. How-
ever, manually selecting the hyper-parameters for different
degradations is not a trivial task [39]. As a result, it is desir-
able to learn a single SISR model which can handle multiple
degradations with high practicability. This paper attempts
to give a positive answer.

Due to the limited space, we can only discuss some of
the related works here. Other CNN-based SISR methods
can be found in [6, 22, 23, 30, 37, 42, 44, 45, 46, 53, 58].

3. Method

3.1. Degradation Model

Before solving the problem of SISR, it is important to
have a clear understanding of the degradation model which
is not limited to Eqn. (1). Another practical degradation
model can be given by

y = (x ↓s) ⊗ k + n.

(2)

e
s
i
o
N

When ↓ is the bicubic downsampler, Eqn. (2) corresponds
to a deblurring problem followed by a SISR problem with
bicubic degradation. Thus, it can beneﬁt from existing
deblurring methods and bicubic degradation based SISR
methods. Due to limited space, we only consider the more
widely assumed degradation model given in Eqn. (1). Nev-
ertheless, our method is general and can be easily extended
to handle Eqn. (2). In the following, we make a short dis-
cussion on blur kernel k, noise n and downsampler ↓.

Blur kernel. Different from image deblurring, the blur
kernel setting of SISR is usually simple. The most popu-
lar choice is isotropic Gaussian blur kernel parameterized
by standard deviation or kernel width [11, 51].
In [38],
anisotropic Gaussian blur kernels are also used. In practice,
more complex blur kernel models used in deblurring task,
such as motion blur [5], can be further considered. Empiri-
cal and theoretical analyses have revealed that the inﬂuence
of an accurate blur kernel is much larger than that of sophis-
ticated image priors [12]. Speciﬁcally, when the assumed
kernel is smoother than the true kernel, the recovered image
is over-smoothed. Most of SISR methods actually favor for
such case. On the other hand, when the assumed kernel is
sharper than the true kernel, high frequency ringing artifacts
will appear.

Noise. While being of low-resolution, the LR images are
usually also noisy. Directly super-resolving the noisy input
without noise removal would amplify the unwanted noise,
resulting in visually unpleasant results. To address this
problem, the straightforward way is to perform denoising
ﬁrst and then enhance the resolution. However, the denois-
ing pre-processing step tends to lose detail information and
would deteriorate the subsequent super-resolution perfor-
mance [43]. Thus, it would be highly desirable to jointly
perform denoising and super-resolution.

Downsampler. Existing literatures have considered two
types of downsamplers, including direct downsampler [11,
17, 36, 51, 55] and bicubic downsampler [7, 12, 14, 15,
47, 52]. In this paper, we consider the bicubic downsam-
pler since when k is delta kernel and the noise level is
zero, Eqn. (1) turns into the widely-used bicubic degrada-
It should be pointed out that, different from
tion model.
blur kernel and noise which vary in a general degradation
model, downsampler is assumed to be ﬁxed.

Blur Kernel
Figure 1. An illustration of different degradations for SISR. The
scale factor is 2. The general degradation models of Eqns. (1)
and (2) assume an HR image actually can degrade into many LR
images, whereas bicubic degradation model assumes an HR image
corresponds to a single LR image.

Though blur kernel and noise have been recognized as
key factors for the success of SISR and several methods
have been proposed to consider those two factors, there has
been little effort towards simultaneously considering blur
kernel and noise in a single CNN framework. It is a chal-
lenging task since the degradation space with respect to
blur kernel and noise is rather large (see Figure 1 as an ex-
ample). One relevant work is done by Zhang et al. [57];
nonetheless, their method is essentially a model-based op-
timization method and thus suffers from several drawbacks
as mentioned previously. In another related work, Riegler et
al. [38] exploited the blur kernel information into the SISR
model. Our method differs from [38] on two major as-
pects. First, our method considers a more general degra-
dation model. Second, our method exploits a more effective
way to parameterize the degradation model.

3.2. A Perspective from MAP Framework

Though existing CNN-based SISR methods are not nec-
essarily derived under the traditional MAP framework, they
have the same goal. We revisit and analyze the general
MAP framework of SISR, aiming to ﬁnd the intrinsic con-
nections between the MAP principle and the working mech-
anism of CNN. Consequently, more insights on CNN archi-
tecture design can be obtained.

Due to the ill-posed nature of SISR, regularization needs
to be imposed to constrain the solution. Mathematically,
the HR counterpart of an LR image y can be estimated by
solving the following MAP problem

ˆx = arg min x

1
2σ2 (cid:107)(x ⊗ k) ↓s −y(cid:107)2 + λΦ(x)

(3)

where 1
2σ2 (cid:107)(x⊗k) ↓s −y(cid:107)2 is the data ﬁdelity term, Φ(x) is
the regularization term (or prior term) and λ is the trade-off
parameter. Simply speaking, Eqn. (3) conveys two points:
(i) the estimated solution should not only accord with the
degradation process but also have the desired property of
clean HR images; (ii) ˆx is a function of LR image y, blur
kernel k, noise level σ, and trade-off parameter λ. There-
fore, the MAP solution of (non-blind) SISR can be formu-
lated as

ˆx = F(y, k, σ, λ; Θ)

(4)

where Θ denotes the parameters of the MAP inference.

By treating CNN as a discriminative learning solution to

Eqn. (4), we can have the following insights.

• Because the data ﬁdelity term corresponds to the degra-
dation process, accurate modeling of the degradation plays
a key role for the success of SISR. However, existing CNN-
based SISR methods with bicubic degradation actually aim
to solve the following problem

ˆx = arg min x(cid:107)x ↓s −y(cid:107)2 + Φ(x).

(5)

Inevitably, their practicability is very limited.

• To design a more practical SISR model, it is preferable to
learn a mapping function like Eqn. (4), which covers more
extensive degradations. It should be stressed that, since λ
can be absorbed into σ, Eqn. (4) can be reformulated as

• Considering that the MAP framework (Eqn. (3)) can per-
form generic image super-resolution with the same image
prior, it is intuitive to jointly perform denoising and SISR
in a uniﬁed CNN framework. Moreover, the work [56] in-
dicates that the parameters of the MAP inference mainly
model the prior; therefore, CNN has the capacity to deal
with multiple degradations via a single model.

From the viewpoint of MAP framework, one can see
that the goal of SISR is to learn a mapping function ˆx =
F(y, k, σ; Θ) rather than ˆx = F(y; Θ). However, it is not
an easy task to directly model ˆx = F(y, k, σ; Θ) via CNN.
The reason lies in the fact that the three inputs y, k and σ
have different dimensions. In the next subsection, we will
propose a simple dimensionality stretching strategy to re-
solve this problem.

3.3. Dimensionality Stretching

The proposed dimensionality stretching strategy is
schematically illustrated in Figure 2. Suppose the inputs
consist of a blur kernel of size p×p, a noise level σ and an
LR image of size W × H × C, where C denotes the number
of channels. The blur kernel is ﬁrst vectorized into a vector

Vectorization

PCA

Stretching

H

Blur Kernel

Noise Level

W

t+1

Degradation Maps

Figure 2. Schematic illustration of the dimensionality stretching
strategy. For an LR image of size W × H, the vectorized blur ker-
nel is ﬁrst projected onto a space of dimension t and then stretched
into a tensor M of size W × H × (t + 1) with the noise level.

of size p2 × 1 and then projected onto t-dimensional lin-
ear space by the PCA (Principal Component Analysis) tech-
nique. After that, the concatenated low dimensional vector
and the noise level, denoted by v, are stretched into degra-
dation maps M of size W × H × (t + 1), where all the
elements of i-th map are vi. By doing so, the degradation
maps then can be concatenated with the LR image, mak-
ing CNN possible to handle the three inputs. Such a simple
strategy can be easily exploited to deal with spatially vari-
ant degradations by considering the fact that the degradation
maps can be non-uniform.

The proposed super-resolution network for multiple
degradations, denoted by SRMD, is illustrated in Figure 3.
As one can see, the distinctive feature of SRMD is that
it takes the concatenated LR image and degradation maps
as input. To show the effectiveness of the dimensionality
stretching strategy, we resort to plain CNN without com-
plex architectural engineering. Typically, to super-resolve
an LR image with a scale factor of s, SRMD ﬁrst takes
the concatenated LR image and degradation maps of size
W × H × (C + t + 1) as input. Then, similar to [24], a
cascade of 3 × 3 convolutional layers are applied to perform
the non-linear mapping. Each layer is composed of three
types of operations, including Convolution (Conv), Recti-
ﬁed Linear Units (ReLU) [26], and Batch Normalization
(BN) [20]. Speciﬁcally, “Conv + BN + ReLU” is adopted
for each convolutional layer except the last convolutional
layer which consists of a single “Conv” operation. Finally, a
sub-pixel convolution layer [41] is followed by the last con-
volutional layer to convert multiple HR subimages of size
W × H × s2C to a single HR image of size sW × sH × C.
For all scale factors 2, 3 and 4, the number of convolu-
tional layers is set to 12, and the number of feature maps
in each layer is set to 128. We separately learn models for
each scale factor. In particular, we also learn the models for

ˆx = F(y, k, σ; Θ).

(6)

3.4. Proposed Network

LR Image & Degradation Maps

Nonlinear Mapping

HR Subimages

HR Image

Figure 3. The architecture of the proposed convolutional super-resolution network. In contrast to other CNN-based SISR methods which
only take the LR image as input and lack scalability to handle other degradations, the proposed network takes the concatenated LR image
and degradation maps as input, thus allowing a single model to manipulate multiple and even spatially variant degradations.

noise-free degradation, namely SRMDNF, by removing the
connection of the noise level map in the ﬁrst convolutional
ﬁlter and ﬁne-tuning with new training data.

It is worth pointing out that neither residual learning nor
bicubicly interpolated LR image is used for the network de-
sign due to the following reasons. First, with a moderate
network depth and advanced CNN training and design such
as ReLU [26], BN [20] and Adam [25], it is easy to train
the network without the residual learning strategy. Second,
since the degradation involves noise, bicubicly interpolated
LR image would aggravate the complexity of noise which
in turn will increase the difﬁculty of training.

3.5. Why not Learn a Blind Model?

To enhance the practicability of CNN for SISR, it seems
the most straightforward way is to learn a blind model with
synthesized training data by different degradations. How-
ever, such blind model does not perform as well as ex-
pected. First, the performance deteriorates seriously when
the blur kernel model is complex, e.g., motion blur. This
phenomenon can be explained by the following example.
Given an HR image, a blur kernel and corresponding LR
image, shifting the HR image to left by one pixel and shift-
ing the blur kernel to right by one pixel would result in the
same LR image. Thus, an LR image may correspond to dif-
ferent HR images with pixel shift. This in turn would aggra-
vate the pixel-wise average problem [29], typically leading
to over-smoothed results. Second, the blind model without
specially designed architecture design has inferior general-
ization ability and performs poorly in real applications.

In contrast, non-blind model for multiple degradations
suffers little from the pixel-wise average problem and has
better generalization ability. First, the degradation maps
contain the warping information and thus can enable the
network to have spatial transformation capability. For clar-
ity, one can treat the degradation maps induced by blur ker-
nel and noise level as the output of a spatial transformer
as in [21]. Second, by anchoring the model with degrada-
tion maps, the non-blind model generalizes easily to unseen
degradations and has the ability to control the tradeoff be-
tween data ﬁdelity term and regularization term.

4. Experiments

4.1. Training Data Synthesis and Network Training

Before synthesizing LR images according to Eqn. (1), it
is necessary to deﬁne the blur kernels and noise level range,
as well as providing a large-scale clean HR image set.

For the blur kernels, we follow the kernel model of
isotropic Gaussian with a ﬁxed kernel width which has been
proved practically feasible in SISR applications. Speciﬁ-
cally, the kernel width ranges are set to [0.2, 2], [0.2, 3] and
[0.2, 4] for scale factors 2, 3 and 4, respectively. We sample
the kernel width by a stride of 0.1. The kernel size is ﬁxed
to 15×15. To further expand the degradation space, we also
consider a more general kernel assumption, i.e., anisotropic
Gaussian, which is characterized by a Gaussian probabil-
ity density function N (0, Σ) with zero mean and varying
covariance matrix Σ [38]. The space of such Gaussian ker-
nel is determined by rotation angle of the eigenvectors of Σ
and scaling of corresponding eigenvalues. We set the rota-
tion angle range to [0, π]. For the scaling of eigenvalues, it
is set from 0.5 to 6, 8 and 10 for scale factors 2, 3 and 4,
respectively.

Although we adopt the bicubic downsampler through-
out the paper, it is straightforward to train a model with di-
rect downsampler. Alternatively, we can also include the
degradations with direct downsampler by approximating it.
Speciﬁcally, given a blur kernel kd under direct downsam-
pler ↓d, we can ﬁnd the corresponding blur kernel kb under
bicubic downsampler ↓b by solving the following problem
with a data-driven method

kb = arg min kb (cid:107)(x ⊗ kb) ↓b

s −(x ⊗ kd) ↓d

s (cid:107)2,

∀ x. (7)

In this paper, we also include such degradations for scale
factor 3.

Once the blur kernels are well-deﬁned or learned, we
then uniformly sample substantial kernels and aggregate
them to learn the PCA projection matrix. By preserving
about 99.8% of the energy, the kernels are projected onto a
space of dimension 15 (i.e., t = 15). The visualization of
some typical blur kernels for scale factor 3 and some PCA
eigenvectors is shown in Figure 4.

Figure 4. Visualization of six typical blur kernels (ﬁst row) of
isotropic Gaussian (ﬁrst two), anisotropic Gaussian (middle two)
and estimated ones for direct downsampler (last two) for scale fac-
tor 3 and PCA eigenvectors (second row) for the ﬁrst six largest
eigenvalues.

For the noise level range, we set it as [0, 75]. Because the
proposed method operates on RGB channels rather than Y
channel in YCbCr color space, we collect a large-scale color
images for training, including 400 BSD [33] images, 800
training images from DIV2K dataset [1] and 4, 744 images
from WED dataset [31].

Then, given an HR image, we synthesize LR image by
blurring it with a blur kernel k and bicubicly downsampling
it with a scale factor s, followed by an addition of AWGN
with noise level σ. The LR patch size is set to 40×40 which
means the corresponding HR patch sizes for scale factors 2,
3, and 4 are 80 × 80, 120 × 120 and 160 × 160, respectively.
In the training phase, we randomly select a blur ker-
nel and a noise level to synthesize an LR image and crop
N = 128×3, 000 LR/HR patch pairs (along with the degra-
dation maps) for each epoch. We optimize the following
loss function using Adam [25]

L(Θ) =

(cid:107)F(yi, Mi; Θ) − xi(cid:107)2.

(8)

1
2N

N
(cid:88)

i=1

The mini-batch size is set to 128. The learning rate starts
from 10−3 and reduces to 10−4 when the training error
stops decreasing. When the training error keeps unchanged
in ﬁve sequential epochs, we merge the parameters of each
batch normalization into the adjacent convolution ﬁlters.
Then, a small learning rate of 10−5 is used for additional
100 epochs to ﬁne-tune the model. Since SRMDNF is ob-
tained by ﬁne-tuning SRMD, its learning rate is ﬁxed to
10−5 for 200 epochs.

We train the models in Matlab (R2015b) environment
with MatConvNet package [48] and an Nvidia Titan X Pas-
cal GPU. The training of a single SRMD model can be done
in about two days. The source code can be downloaded at
https://github.com/cszn/SRMD.

4.2. Experiments on Bicubic Degradation

As mentioned above, instead of handling the bicubic
degradation only, our aim is to learn a single network to
handle multiple degradations. However, in order to show
the advantage of the dimensionality stretching strategy, the

proposed method is also compared with other CNN-based
methods speciﬁcally designed for bicubic degradation.

Table 1 shows the PSNR and SSIM [50] results of
state-of-the-art CNN-based SISR methods on four widely-
used datasets. As one can see, SRMD achieves compa-
rable results with VDSR at small scale factor and outper-
forms VDSR at large scale factor. In particular, SRMDNF
achieves the best overall quantitative results. Using Ima-
geNet dataset [26] to train the speciﬁc model with bicubic
degradation, SRResNet performs slightly better than SR-
MDNF on scale factor 4. To further compare with other
methods such as VDSR, we also have trained a SRMDNF
model (for scale factor 3) which operates on Y channel with
291 training images. The learned model achieves 33.97dB,
29.96dB, 28.95dB and 27.42dB on Set5, Set14, BSD100
and Urban100, respectively. As a result, it can still outper-
form other competing methods. The possible reason lies
in that the SRMDNF with multiple degradations shares the
same prior in the MAP framework which facilitates the im-
plicit prior learning and thus beneﬁts to PSNR improve-
ment. This also can explain why VDSR with multiple scales
improves the performance.

For the GPU run time, SRMD spends 0.084, 0.042 and
0.027 seconds to reconstruct an HR image of size 1, 024 ×
1, 024 for scale factors 2, 3 and 4, respectively. As a com-
parison, the run time of VDSR is 0.174 second for all scale
factors. Figure 5 shows the visual results of different meth-
ods. One can see that our proposed method yields very com-
petitive performance against other methods.

4.3. Experiments on General Degradations

In this subsection, we evaluate the performance of the
proposed method on general degradations. The degrada-
tion settings are given in Table 2. We only consider the
isotropic Gaussian blur kernel for an easy comparison. To
further show the scalability of the proposed method, another
widely-used degradation [11] which involves 7×7 Gaussian
kernel with width 1.6 and direct downsampler with scale
factor 3 is also included. We compare the proposed method
with VDSR, two model-based methods (i.e., NCSR [11]
and IRCNN [57]), and a cascaded denoising-SISR method
(i.e., DnCNN [56]+SRMDNF).

The quantitative results of different methods with dif-
ferent degradations on Set5 are provided in Table 2, from
which we have observations and analyses as follows. First,
the performance of VDSR deteriorates seriously when the
assumed bicubic degradation deviates from the true one.
Second, SRMD produces much better results than NCSR
and IRCNN, and outperforms DnCNN+SRMDNF. In par-
ticular, the PSNR gain of SRMD over DnCNN+SRMDNF
increases with the kernel width which veriﬁes the advantage
of joint denoising and super-resolution. Third, by setting
proper blur kernel, the proposed method delivers good per-

Table 1. Average PSNR and SSIM results for bicubic degradation on datasets Set5 [3], Set14 [54], BSD100 [33] and Urban100 [19]. The
best two results are highlighted in red and blue colors, respectively.

Dataset

Scale
Factor

Bicubic

SRCNN [9]

VDSR [24]

SRResNet [29]

DRRN [44]

LapSRN [27]

SRMD

SRMDNF

PSNR / SSIM

Set5

Set14

BSD100

Urban100

×2
×3
×4
×2
×3
×4
×2
×3
×4
×2
×3
×4

33.64 / 0.929
30.39 / 0.868
28.42 / 0.810
30.22 / 0.868
27.53 / 0.774
25.99 / 0.702
29.55 / 0.843
27.20 / 0.738
25.96 / 0.667
26.66 / 0.841
24.46 / 0.737
23.14 / 0.657

36.62 / 0.953
32.74 / 0.908
30.48 / 0.863
32.42 / 0.906
29.27 / 0.821
27.48 / 0.751
31.34 / 0.887
28.40 / 0.786
26.90 / 0.710
29.53 / 0.897
26.25 / 0.801
24.52 / 0.722

37.56 / 0.959
33.67 / 0.922
31.35 / 0.885
33.02 / 0.913
29.77 / 0.832
27.99 / 0.766
31.89 / 0.896
28.82 / 0.798
27.28 / 0.726
30.76 / 0.914
27.13 / 0.828
25.17 / 0.753

–
–
32.05 / 0.891
–
–
28.49 / 0.780
–
–
27.58 / 0.735
–
–
–

37.66 / 0.959
33.93 / 0.923
31.58 / 0.886
33.19 / 0.913
29.94 / 0.834
28.18 / 0.770
32.01 / 0.897
28.91 / 0.799
27.35 / 0.726
31.02 / 0.916
27.38 / 0.833
25.35 / 0.758

37.52 / 0.959
33.82 / 0.922
31.54 / 0.885
33.08 / 0.913
29.89 / 0.834
28.19 / 0.772
31.80 / 0.895
28.82 / 0.798
27.32 / 0.727
30.82 / 0.915
27.07 / 0.828
25.21 / 0.756

37.53 / 0.959
33.86 / 0.923
31.59 / 0.887
33.12 / 0.914
29.84 / 0.833
28.15 / 0.772
31.90 / 0.896
28.87 / 0.799
27.34 / 0.728
30.89 / 0.916
27.27 / 0.833
25.34 / 0.761

37.79 / 0.960
34.12 / 0.925
31.96 / 0.893
33.32 / 0.915
30.04 / 0.837
28.35 / 0.777
32.05 / 0.898
28.97 / 0.803
27.49 / 0.734
31.33 / 0.920
27.57 / 0.840
25.68 / 0.773

(a) SRCNN (23.78dB)

(b) VDSR (24.20dB)

(c) DRRN (25.11dB)

(d) LapSR (24.47dB)

(e) SRMD (25.09dB)

(f)SRMDNF (25.74dB)

Figure 5. SISR performance comparison of different methods with scale factor 4 on image “Img 099” from Urban100.

Table 2. Average PSNR and SSIM results of different methods with different degradations on Set5. The best results are highlighted in red
color. The results highlighted in gray color indicate unfair comparison due to mismatched degradation assumption.

Degradation Settings
Down-
sampler

Kernel
Width

Noise
Level

VDSR [24]

NCSR [11]

IRCNN [57]

DnCNN [56]+SRMDNF

SRMD

SRMDNF

PSNR (×2/×3/×4)

0.2
0.2
0.2
1.3
1.3
1.3
2.6
2.6
2.6

1.6

Bicubic
Bicubic
Bicubic
Bicubic
Bicubic
Bicubic
Bicubic
Bicubic
Bicubic

Direct

0
15
50
0
15
50
0
15
50

0

37.56/33.67/31.35
26.02/25.40/24.70
16.02/15.72/15.46
30.57/30.24/29.72
24.82/24.70/24.30
15.89/15.68/15.43
26.37/26.31/26.28
23.09/23.07/22.98
15.58/15.43/15.23

– /23.82/–
–
–
– /21.81/–
–
–
– /21.46/–
–
–

37.43/33.39/31.02
32.60/30.08/28.35
28.20/26.25/24.95
36.01/33.33/31.01
29.96/28.68/27.71
26.69/25.20/24.42
32.07/31.09/30.06
26.44/25.67/24.36
22.98/22.16/21.43

–
32.47/30.07/28.31
28.20/26.27/24.93
–
27.68/28.78/27.71
24.35/25.19/24.39
–

– /21.33/23.85
– /19.03/21.15

37.53/33.86/31.59
32.76/30.43/28.79
28.51/26.48/25.18
37.04/33.77/31.56
30.98/29.43/28.21
27.43/25.82/24.77
33.24/32.59/31.20
28.48/27.55/26.82
25.85/24.75/23.98

37.79/34.12/31.96
–
–
37.45/34.16/31.99
–
–
34.12/33.02/31.77
–
–

– /30.54/ –

– /33.02/ –

– /33.38/ –

–

– /33.74/ –

– /34.01/ –

(a) Ground-truth

(f) SRMDNF (30.34dB)
Figure 6. SISR performance comparison on image “Butterﬂy” from Set5. The degradation involves 7×7 Gaussian kernel with width 1.6
and direct downsampler with scale factor 3. Note that the comparison with VDSR is unfair because of degradation mismatch.

(d) IRCNN (29.32dB)

(e) SRMD (29.79dB)

(b) VDSR (24.73dB)

(c) NCSR (28.01dB)

formance in handling the degradation with direct downsam-
pler. The visual comparison is given in Figure 6. One can
see that NCSR and IRCNN produce more visually pleasant
results than VDSR since their assumed degradation matches
the true one. However, they cannot recover edges as sharper
as SRMD and SRMDNF.

4.4. Experiments on Spatially Variant Degradation

To demonstrate the effectiveness of SRMD for spatially
variant degradation, we synthesize an LR images with spa-
tially variant blur kernels and noise levels. Figure 7 shows
the visual result of the proposed SRMD for the spatially

(a)

(b)

(c)

Figure 7. An example of SRMD on dealing with spatially variant
degradation. (a) Noise level and Gaussian blur kernel width maps.
(b) Zoomed LR image. (c) Results of SRMD with scale factor 2.

(a) LR image

(b) VDSR [24]

(c) SelfEx [19]

(d) SRMD

Figure 9. SISR results on real image “Chip” with scale factor 4.

ure 8 that the performance of VDSR is severely affected by
the compression artifacts. While Waifu2x can successfully
remove the compression artifacts, it fails to recover sharp
edges. In comparison, SRMD can not only remove the un-
satisfying artifacts but also produce sharp edges. From Fig-
ure 9, we can see that VDSR and SelfEx both tend to pro-
duce over-smoothed results, whereas SRMD can recover
sharp image with better intensity and gradient statistics of
clean images [35].

5. Conclusion

In this paper, we proposed an effective super-resolution
network with high scalability of handling multiple degra-
dations via a single model. Different from existing CNN-
based SISR methods,
the proposed super-resolver takes
both LR image and its degradation maps as input. Specif-
ically, degradation maps are obtained by a simple dimen-
sionality stretching of the degradation parameters (i.e., blur
kernel and noise level). The results on synthetic LR images
demonstrated that the proposed super-resolver can not only
produce state-of-the-art results on bicubic degradation but
also perform favorably on other degradations and even spa-
tially variant degradations. Moreover, the results on real
LR images showed that the proposed method can recon-
struct visually plausible HR images. In summary, the pro-
posed super-resolver offers a feasible solution toward prac-
tical CNN-based SISR applications.

6. Acknowledgements

This work is supported by National Natural Science
Foundation of China (grant no. 61671182, 61471146),
HK RGC General Research Fund (PolyU 152240/15E)
and PolyU-Alibaba Collaborative Research Project “Qual-
ity Enhancement of Surveillance Images and Videos”. We
gratefully acknowledge the support from NVIDIA Corpo-
ration for providing us the Titan Xp GPU used in this re-
search.

(a) LR image

(b) VDSR [24]

(c) Waifu2x [49]

(d) SRMD

Figure 8. SISR results on image “Cat” with scale factor 2.

variant degradations. One can see that the proposed SRMD
is effective in recovering the latent HR image. Note that the
blur kernel is assumed to be isotropic Gaussian.

4.5. Experiments on Real Images

Besides the above experiments on LR images syntheti-
cally downsampled from HR images with known blur ker-
nels and corrupted by AWGN with known noise levels, we
also do experiments on real LR images to demonstrate the
effectiveness of the proposed SRMD. Since there are no
ground-truth HR images, we only provide the visual com-
parison.

As aforementioned, while we also use anisotropic Gaus-
sian kernels in training,
it is generally feasible to use
isotropic Gaussian for most of the real LR images in test-
ing. To ﬁnd the degradation parameters with good visual
quality, we use a grid search strategy rather than adopting
any blur kernel or noise level estimation methods. Speciﬁ-
cally, the kernel width is uniformly sampled from 0.1 to 2.4
with a stride of 0.1, and the noise level is from 0 to 75 with
stride 5.

Figures 8 and 9 illustrate the SISR results on two real LR
images “Cat” and “Chip”, respectively. The VDSR [24] is
used as one of the representative CNN-based methods for
comparison. For image “Cat” which is corrupted by com-
pression artifacts, Waifu2x [49] is also used for compari-
son. For image “Chip” which contains repetitive structures,
a self-similarity based method SelfEx [19] is also included
for comparison.

It can be observed from the visual results that SRMD
can produce much more visually plausible HR images than
the competing methods. Speciﬁcally, one can see from Fig-

References

[1] E. Agustsson and R. Timofte. Ntire 2017 challenge on sin-
gle image super-resolution: Dataset and study. In IEEE Con-
ference on Computer Vision and Pattern Recognition Work-
shops, volume 3, pages 126–135, July 2017.

[2] S. Baker and T. Kanade. Limits on super-resolution and how
to break them. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 24(9):1167–1183, 2002.

[3] M. Bevilacqua, A. Roumy, C. Guillemot, and M.-L. A.
Morel. Low-complexity single-image super-resolution based
on nonnegative neighbor embedding. In British Machine Vi-
sion Conference, 2012.

[4] S. A. Bigdeli, M. Jin, P. Favaro, and M. Zwicker. Deep mean-
shift priors for image restoration. In Advances in Neural In-
formation Processing Systems, 2017.

[5] G. Boracchi and A. Foi. Modeling the performance of image
restoration from motion blur. IEEE Transactions on Image
Processing, 21(8):3502–3517, Aug 2012.

[6] Y. Chen, W. Yu, and T. Pock. On learning optimized reac-
tion diffusion processes for effective image restoration.
In
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 5261–5269, 2015.

[7] Z. Cui, H. Chang, S. Shan, B. Zhong, and X. Chen. Deep
In European

network cascade for image super-resolution.
Conference on Computer Vision, pages 49–64, 2014.

[9] C. Dong, C. C. Loy, K. He, and X. Tang.

[8] C. Dong, C. C. Loy, K. He, and X. Tang. Learning a deep
convolutional network for image super-resolution. In Euro-
pean Conference on Computer Vision, pages 184–199, 2014.
Image
IEEE
super-resolution using deep convolutional networks.
Transactions on Pattern Analysis and Machine Intelligence,
38(2):295–307, 2016.

[10] C. Dong, C. C. Loy, and X. Tang. Accelerating the super-
resolution convolutional neural network. In European Con-
ference on Computer Vision, pages 391–407, 2016.

[11] W. Dong, L. Zhang, G. Shi, and X. Li. Nonlocally central-
ized sparse representation for image restoration. IEEE Trans-
actions on Image Processing, 22(4):1620–1630, 2013.
[12] N. Efrat, D. Glasner, A. Apartsin, B. Nadler, and A. Levin.
Accurate blur models vs. image priors in single image super-
resolution. In IEEE International Conference on Computer
Vision, pages 2832–2839, 2013.
[13] K. Egiazarian and V. Katkovnik.

Single image super-
In European Signal

resolution via BM3D sparse coding.
Processing Conference, pages 2849–2853, 2015.

[14] W. Freeman and C. Liu. Markov random ﬁelds for super-
resolution and texture synthesis. Advances in Markov Ran-
dom Fields for Vision and Image Processing, 1:155–165,
2011.

[15] D. Glasner, S. Bagon, and M. Irani. Super-resolution from
a single image. In IEEE International Conference on Com-
puter Vision, pages 349–356, 2009.

[16] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in neural information
processing systems, pages 2672–2680, 2014.

[17] H. He and W.-C. Siu. Single image super-resolution using
Gaussian process regression. In IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 449–456, 2011.

[18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
In IEEE Conference on Computer

for image recognition.
Vision and Pattern Recognition, pages 770–778, 2016.
[19] J.-B. Huang, A. Singh, and N. Ahuja. Single image super-
resolution from transformed self-exemplars. In IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
5197–5206, 2015.

[20] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
International Conference on Machine Learning, pages 448–
456, 2015.

[21] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial
In Advances in Neural Information

transformer networks.
Processing Systems, pages 2017–2025, 2015.

[22] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for
In European

real-time style transfer and super-resolution.
Conference on Computer Vision, pages 694–711, 2016.
[23] J. Kim, J. Kwon Lee, and K. Mu Lee. Deeply-recursive con-
volutional network for image super-resolution. In IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
1637–1645, 2016.

[24] J. Kim, J. K. Lee, and K. M. Lee. Accurate image super-
resolution using very deep convolutional networks. In IEEE
Conference on Computer Vision and Pattern Recognition,
pages 1646–1654, 2016.

[25] D. Kingma and J. Ba. Adam: A method for stochastic op-
timization. In International Conference for Learning Repre-
sentations, 2015.

[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in Neural Information Processing Systems, pages
1097–1105, 2012.

[27] W.-S. Lai, J.-B. Huang, N. Ahuja, and M.-H. Yang. Deep
laplacian pyramid networks for fast and accurate super-
In IEEE Conference on Computer Vision and
resolution.
Pattern Recognition, pages 624–632, July 2017.

[28] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature,

521(7553):436–444, 2015.

[29] C. Ledig, L. Theis, F. Husz´ar, J. Caballero, A. Cunningham,
A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, et al.
Photo-realistic single image super-resolution using a gen-
In IEEE Conference on Com-
erative adversarial network.
puter Vision and Pattern Recognition, pages 4681–4690,
July 2017.

[30] B. Lim, S. Son, H. Kim, S. Nah, and K. M. Lee. Enhanced
deep residual networks for single image super-resolution. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion Workshops, pages 136–144, July 2017.

[31] K. Ma, Z. Duanmu, Q. Wu, Z. Wang, H. Yong, H. Li, and
L. Zhang. Waterloo exploration database: New challenges
for image quality assessment models. IEEE Transactions on
Image Processing, 26(2):1004–1016, 2017.

[32] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman.
Non-local sparse models for image restoration. In IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
2272–2279, 2009.

[33] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database
of human segmented natural images and its application to
evaluating segmentation algorithms and measuring ecologi-

Conference, pages 689–692, 2015.

[49] Waifu2x.

Image super-resolution for anime-style art using
deep convolutional neural networks. http://waifu2x.
udp.jp/.

[50] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli.
from error visibility to struc-
IEEE Transactions on Image Processing,

Image quality assessment:
tural similarity.
13(4):600–612, 2004.

[51] C.-Y. Yang, C. Ma, and M.-H. Yang. Single-image super-
resolution: A benchmark. In European Conference on Com-
puter Vision, pages 372–386, 2014.

[52] J. Yang, J. Wright, T. S. Huang, and Y. Ma. Image super-
resolution via sparse representation. IEEE Transactions on
Image Processing, 19(11):2861–2873, 2010.

[53] W. Yang, J. Feng, J. Yang, F. Zhao, J. Liu, Z. Guo, and
S. Yan. Deep edge guided recurrent residual learning for
image super-resolution. IEEE Transactions on Image Pro-
cessing, 26(12):5895–5907, 2017.

[54] R. Zeyde, M. Elad, and M. Protter. On single image scale-up
using sparse-representations. In International conference on
curves and surfaces, pages 711–730, 2010.

[55] K. Zhang, X. Zhou, H. Zhang, and W. Zuo. Revisiting sin-
gle image super-resolution under internet environment: blur
kernels and reconstruction algorithms. In Paciﬁc Rim Con-
ference on Multimedia, pages 677–687, 2015.

[56] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang. Be-
yond a gaussian denoiser: Residual learning of deep CNN
for image denoising. IEEE Transactions on Image Process-
ing, pages 3142–3155, 2017.

[57] K. Zhang, W. Zuo, S. Gu, and L. Zhang. Learning deep CNN
In IEEE Conference
denoiser prior for image restoration.
on Computer Vision and Pattern Recognition, pages 3929–
3938, July 2017.

[58] Y. Zhang, Y. Tian, Y. Kong, B. Zhong, and Y. Fu. Residual
dense network for image super-resolution. In IEEE Confer-
ence on Computer Vision and Pattern Recognition, 2018.

cal statistics. In IEEE International Conference on Computer
Vision, volume 2, pages 416–423, July 2001.

[34] T. Meinhardt, M. M¨oller, C. Hazirbas, and D. Cremers.
Learning proximal operators: Using denoising networks for
In IEEE Interna-
regularizing inverse imaging problems.
tional Conference on Computer Vision, pages 1781–1790,
2017.

[35] J. Pan, Z. Hu, Z. Su, and M.-H. Yang. Deblurring text images
via L0-regularized intensity and gradient prior. In IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
2901–2908, 2014.

[36] T. Peleg and M. Elad. A statistical prediction model based
on sparse representations for single image super-resolution.
IEEE transactions on Image Processing, 23(6):2569–2582,
2014.

[37] J. S. Ren, L. Xu, Q. Yan, and W. Sun. Shepard convolu-
tional neural networks. In Advances in Neural Information
Processing Systems, pages 901–909, 2015.

[38] G. Riegler, S. Schulter, M. Ruther, and H. Bischof. Condi-
tioned regression models for non-blind single image super-
resolution. In IEEE International Conference on Computer
Vision, pages 522–530, 2015.

[39] Y. Romano, M. Elad, and P. Milanfar. The little engine that
could: Regularization by denoising (red). SIAM Journal on
Imaging Sciences, 10(4):1804–1844, 2017.

[40] Y. Romano, J. Isidoro, and P. Milanfar. RAISR: rapid and ac-
curate image super resolution. IEEE Transactions on Com-
putational Imaging, 3(1):110–125, 2017.

[41] W. Shi, J. Caballero, F. Husz´ar, J. Totz, A. P. Aitken,
R. Bishop, D. Rueckert, and Z. Wang. Real-time single im-
age and video super-resolution using an efﬁcient sub-pixel
convolutional neural network. In IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 1874–1883,
2016.

[42] Y. Shi, K. Wang, C. Chen, L. Xu, and L. Lin. Structure-
preserving image super-resolution via contextualized multi-
task learning. IEEE Transactions on Multimedia, 2017.
[43] A. Singh, F. Porikli, and N. Ahuja. Super-resolving noisy
images. In IEEE Conference on Computer Vision and Pattern
Recognition, pages 2846–2853, 2014.

[44] Y. Tai, J. Yang, and X. Liu. Image super-resolution via deep
In IEEE Conference on Com-
recursive residual network.
puter Vision and Pattern Recognition, pages 3147–3155,
2017.

[45] Y. Tai, J. Yang, X. Liu, and C. Xu. Memnet: A persistent
In IEEE Interna-
memory network for image restoration.
tional Conference on Computer Vision, pages 4539–4547,
2017.

[46] R. Timofte, E. Agustsson, L. Van Gool, M.-H. Yang, and
L. Zhang. Ntire 2017 challenge on single image super-
In IEEE Conference on
resolution: Methods and results.
Computer Vision and Pattern Recognition Workshops, pages
114–125, July 2017.

[47] R. Timofte, V. De Smet, and L. Van Gool. A+: Adjusted
anchored neighborhood regression for fast super-resolution.
In Asian Conference on Computer Vision, pages 111–126,
2014.

[48] A. Vedaldi and K. Lenc. MatConvNet: Convolutional neu-
ral networks for matlab. In ACM Conference on Multimedia

8
1
0
2
 
y
a
M
 
4
2
 
 
]

V
C
.
s
c
[
 
 
2
v
6
1
1
6
0
.
2
1
7
1
:
v
i
X
r
a

Learning a Single Convolutional Super-Resolution Network for
Multiple Degradations

Kai Zhang1,2,3, Wangmeng Zuo1, Lei Zhang2
1School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China
2Dept. of Computing, The Hong Kong Polytechnic University, Hong Kong, China
3DAMO Academy, Alibaba Group
cskaizhang@gmail.com, wmzuo@hit.edu.cn, cslzhang@comp.polyu.edu.hk

Abstract

Recent years have witnessed the unprecedented success
of deep convolutional neural networks (CNNs) in single im-
age super-resolution (SISR). However, existing CNN-based
SISR methods mostly assume that a low-resolution (LR) im-
age is bicubicly downsampled from a high-resolution (HR)
image, thus inevitably giving rise to poor performance when
the true degradation does not follow this assumption. More-
over, they lack scalability in learning a single model to non-
blindly deal with multiple degradations. To address these
issues, we propose a general framework with dimensional-
ity stretching strategy that enables a single convolutional
super-resolution network to take two key factors of the SISR
degradation process, i.e., blur kernel and noise level, as in-
put. Consequently, the super-resolver can handle multiple
and even spatially variant degradations, which signiﬁcantly
improves the practicability. Extensive experimental results
on synthetic and real LR images show that the proposed
convolutional super-resolution network not only can pro-
duce favorable results on multiple degradations but also is
computationally efﬁcient, providing a highly effective and
scalable solution to practical SISR applications.

1. Introduction

Single image super-resolution (SISR) aims to recover a
high-resolution (HR) version of a low-resolution (LR) in-
put. As a classical problem, SISR is still an active yet chal-
lenging research topic in the ﬁeld of computer vision due to
its ill-poseness nature and high practical values [2]. In the
typical SISR framework, an LR image y is modeled as the
output of the following degradation process:

y = (x ⊗ k) ↓s + n,

(1)

where x ⊗ k represents the convolution between a blur ker-
nel k and a latent HR image x, ↓s is a subsequent downsam-
pling operation with scale factor s, and n usually is addi-
tive white Gaussian noise (AWGN) with standard deviation
(noise level) σ.

SISR methods can be broadly classiﬁed into three cate-
gories, i.e., interpolation-based methods, model-based op-
timization methods and discriminative learning methods.
Interpolation-based methods such as nearest-neighbor, bi-
linear and bicubic interpolators are simple and efﬁcient but
have very limited performance. By exploiting powerful im-
age priors (e.g., the non-local self-similarity prior [11, 32],
sparsity prior [52] and denoiser prior [4, 13, 57]), model-
based optimization methods are ﬂexible to reconstruct rel-
ative high-quality HR images, but they usually involve a
time-consuming optimization procedure. Although the in-
tegration of convolutional neural network (CNN) denoiser
prior and model-based optimization can improve the efﬁ-
ciency to some extent, it still suffers from the typical draw-
backs of model-based optimization methods, e.g., it is not in
an end-to-end learning manner and involves hand-designed
parameters [57]. As an alternative, discriminative learning
methods have attracted considerable attentions due to their
favorable SISR performance in terms of effectiveness and
efﬁciency. Notably, recent years have witnessed a dramatic
upsurge of using CNN for SISR.

In this paper, we focus on discriminative CNN methods
for SISR so as to exploit the merits of CNN, such as the fast
speed by parallel computing, high accuracy by end-to-end
training, and tremendous advances in training and design-
ing networks [16, 18, 21, 28]. While several SISR mod-
els based on discriminative CNN have reported impressive
results, they suffer from a common drawback: their mod-
els are specialized for a single simpliﬁed degradation (e.g.,
bicubic degradation) and lack scalability to handle multiple
degradations by using a single model. Because the practical
degradation of SISR is much more complex [40, 51], the
performance of learned CNN models may deteriorate seri-
ously when the assumed degradation deviates from the true
one, making them less effective in practical scenarios. It has
been pointed out that the blur kernel plays a vital role for the
success of SISR methods and the mismatch of blur kernels
will largely deteriorate the ﬁnal SISR results [12]. How-
ever, little work has been done on how to design a CNN to
address this crucial issue.

Given the facts above, it is natural to raise the follow-
ing questions, which are the focus of our paper: (i) Can we
learn a single model to effectively handle multiple and even
spatially variant degradations? (ii) Is it possible to use syn-
thetic data to train a model with high practicability? This
work aims to make one of the ﬁrst attempts towards answer-
ing these two questions.

To answer the ﬁrst question, we revisit and analyze the
general model-based SISR methods under the maximum a
posteriori (MAP) framework. Then we argue that one may
tackle this issue by taking LR input, blur kernel and noise
level as input to CNN but their dimensionality mismatch
makes it difﬁcult to design a single convolutional super-
resolution network. In view of this, we introduce a dimen-
sionality stretching strategy which facilitates the network to
handle multiple and even spatially variant degradations with
respect to blur kernel and noise. To the best of our knowl-
edge, there is no attempt to consider both the blur kernel
and noise for SISR via training a single CNN model.

For the second question, we will show that it is possible
to learn a practical super-resolver using synthetic data. To
this end, a large variety of degradations with different com-
binations of blur kernels and noise levels are sampled to
cover the degradation space. In a practical scenario, even
the degradation is more complex (e.g., the noise is non-
AWGN), we can select the best ﬁtted degradation model
rather than the bicubic degradation to produce a better re-
sult. It turns out that, by choosing a proper degradation, the
learned SISR model can yield perceptually convincing re-
sults on real LR images. It should be noted that we make no
effort to use specialized network architectures but use the
plain CNN as in [9, 41].

The main contributions of this paper are summarized in

the following:

• We propose a simple yet effective and scalable deep
CNN framework for SISR. The proposed model goes
beyond the widely-used bicubic degradation assump-
tion and works for multiple and even spatially variant
degradations, thus making a substantial step towards
developing a practical CNN-based super-resolver for
real applications.

• We propose a novel dimensionality stretching strategy
to address the dimensionality mismatch between LR
input image, blur kernel and noise level. Although this
strategy is proposed for SISR, it is general and can be
extended to other tasks such as deblurring.

• We show that

the proposed convolutional super-
resolution network learned from synthetic training data
can not only produce competitive results against state-
of-the-art SISR methods on synthetic LR images but
also give rise to visually plausible results on real LR
images.

2. Related Work

The ﬁrst work of using CNN to solve SISR can be
traced back to [8] where a three-layer super-resolution net-
work (SRCNN) was proposed. In the extended work [9],
the authors investigated the impact of depth on super-
resolution and empirically showed that the difﬁculty of
training deeper model hinders the performance improve-
ment of CNN super-resolvers. To overcome the training
difﬁculty, Kim et al. [24] proposed a very deep super-
resolution (VDSR) method with residual learning strategy.
Interestingly, they showed that VDSR can handle multiple
scales super-resolution. By analyzing the relation between
CNN and MAP inference, Zhang et al. [56] pointed out that
CNN mainly model the prior information and they empir-
ically demonstrated that a single model can handle mul-
tiple scales super-resolution, image deblocking and image
denoising. While achieving good performance, the above
methods take the bicubicly interpolated LR image as in-
put, which not only suffers from high computational cost
but also hinders the effective expansion of receptive ﬁeld.

To improve the efﬁciency, some researchers resort to di-
rectly manipulating the LR input and adopting an upscal-
ing operation at the end of the network. Shi et al. [41]
introduced an efﬁcient sub-pixel convolution layer to up-
scale the LR feature maps into HR images. Dong et al. [10]
adopted a deconvolution layer at the end of the network to
perform upsampling. Lai et al. [27] proposed a Laplacian
pyramid super-resolution network (LapSRN) that takes an
LR image as input and progressively predicts the sub-band
residuals with transposed convolutions in a coarse-to-ﬁne
manner. To improve the perceptual quality at a large scale
factor, Ledig et al. [29] proposed a generative adversarial
network [16] based super-resolution (SRGAN) method. In
the generator network of SRGAN, two sub-pixel convolu-
tion layers are used to efﬁciently upscale the LR input by a
factor of 4.

Although various techniques have been proposed for
SISR, the above CNN-based methods are tailored to the
widely-used settings of bicubic degradation, neglecting
their limited applicability for practical scenarios. An in-
teresting line of CNN-based methods which can go be-
yond bicubic degradation adopt a CNN denoiser to solve
SISR via model-based optimization framework [4, 34, 57].
For example,
the method proposed in [57] can handle
the widely-used Gaussian degradation as in [11]. How-
ever, manually selecting the hyper-parameters for different
degradations is not a trivial task [39]. As a result, it is desir-
able to learn a single SISR model which can handle multiple
degradations with high practicability. This paper attempts
to give a positive answer.

Due to the limited space, we can only discuss some of
the related works here. Other CNN-based SISR methods
can be found in [6, 22, 23, 30, 37, 42, 44, 45, 46, 53, 58].

3. Method

3.1. Degradation Model

Before solving the problem of SISR, it is important to
have a clear understanding of the degradation model which
is not limited to Eqn. (1). Another practical degradation
model can be given by

y = (x ↓s) ⊗ k + n.

(2)

e
s
i
o
N

When ↓ is the bicubic downsampler, Eqn. (2) corresponds
to a deblurring problem followed by a SISR problem with
bicubic degradation. Thus, it can beneﬁt from existing
deblurring methods and bicubic degradation based SISR
methods. Due to limited space, we only consider the more
widely assumed degradation model given in Eqn. (1). Nev-
ertheless, our method is general and can be easily extended
to handle Eqn. (2). In the following, we make a short dis-
cussion on blur kernel k, noise n and downsampler ↓.

Blur kernel. Different from image deblurring, the blur
kernel setting of SISR is usually simple. The most popu-
lar choice is isotropic Gaussian blur kernel parameterized
by standard deviation or kernel width [11, 51].
In [38],
anisotropic Gaussian blur kernels are also used. In practice,
more complex blur kernel models used in deblurring task,
such as motion blur [5], can be further considered. Empiri-
cal and theoretical analyses have revealed that the inﬂuence
of an accurate blur kernel is much larger than that of sophis-
ticated image priors [12]. Speciﬁcally, when the assumed
kernel is smoother than the true kernel, the recovered image
is over-smoothed. Most of SISR methods actually favor for
such case. On the other hand, when the assumed kernel is
sharper than the true kernel, high frequency ringing artifacts
will appear.

Noise. While being of low-resolution, the LR images are
usually also noisy. Directly super-resolving the noisy input
without noise removal would amplify the unwanted noise,
resulting in visually unpleasant results. To address this
problem, the straightforward way is to perform denoising
ﬁrst and then enhance the resolution. However, the denois-
ing pre-processing step tends to lose detail information and
would deteriorate the subsequent super-resolution perfor-
mance [43]. Thus, it would be highly desirable to jointly
perform denoising and super-resolution.

Downsampler. Existing literatures have considered two
types of downsamplers, including direct downsampler [11,
17, 36, 51, 55] and bicubic downsampler [7, 12, 14, 15,
47, 52]. In this paper, we consider the bicubic downsam-
pler since when k is delta kernel and the noise level is
zero, Eqn. (1) turns into the widely-used bicubic degrada-
It should be pointed out that, different from
tion model.
blur kernel and noise which vary in a general degradation
model, downsampler is assumed to be ﬁxed.

Blur Kernel
Figure 1. An illustration of different degradations for SISR. The
scale factor is 2. The general degradation models of Eqns. (1)
and (2) assume an HR image actually can degrade into many LR
images, whereas bicubic degradation model assumes an HR image
corresponds to a single LR image.

Though blur kernel and noise have been recognized as
key factors for the success of SISR and several methods
have been proposed to consider those two factors, there has
been little effort towards simultaneously considering blur
kernel and noise in a single CNN framework. It is a chal-
lenging task since the degradation space with respect to
blur kernel and noise is rather large (see Figure 1 as an ex-
ample). One relevant work is done by Zhang et al. [57];
nonetheless, their method is essentially a model-based op-
timization method and thus suffers from several drawbacks
as mentioned previously. In another related work, Riegler et
al. [38] exploited the blur kernel information into the SISR
model. Our method differs from [38] on two major as-
pects. First, our method considers a more general degra-
dation model. Second, our method exploits a more effective
way to parameterize the degradation model.

3.2. A Perspective from MAP Framework

Though existing CNN-based SISR methods are not nec-
essarily derived under the traditional MAP framework, they
have the same goal. We revisit and analyze the general
MAP framework of SISR, aiming to ﬁnd the intrinsic con-
nections between the MAP principle and the working mech-
anism of CNN. Consequently, more insights on CNN archi-
tecture design can be obtained.

Due to the ill-posed nature of SISR, regularization needs
to be imposed to constrain the solution. Mathematically,
the HR counterpart of an LR image y can be estimated by
solving the following MAP problem

ˆx = arg min x

1
2σ2 (cid:107)(x ⊗ k) ↓s −y(cid:107)2 + λΦ(x)

(3)

where 1
2σ2 (cid:107)(x⊗k) ↓s −y(cid:107)2 is the data ﬁdelity term, Φ(x) is
the regularization term (or prior term) and λ is the trade-off
parameter. Simply speaking, Eqn. (3) conveys two points:
(i) the estimated solution should not only accord with the
degradation process but also have the desired property of
clean HR images; (ii) ˆx is a function of LR image y, blur
kernel k, noise level σ, and trade-off parameter λ. There-
fore, the MAP solution of (non-blind) SISR can be formu-
lated as

ˆx = F(y, k, σ, λ; Θ)

(4)

where Θ denotes the parameters of the MAP inference.

By treating CNN as a discriminative learning solution to

Eqn. (4), we can have the following insights.

• Because the data ﬁdelity term corresponds to the degra-
dation process, accurate modeling of the degradation plays
a key role for the success of SISR. However, existing CNN-
based SISR methods with bicubic degradation actually aim
to solve the following problem

ˆx = arg min x(cid:107)x ↓s −y(cid:107)2 + Φ(x).

(5)

Inevitably, their practicability is very limited.

• To design a more practical SISR model, it is preferable to
learn a mapping function like Eqn. (4), which covers more
extensive degradations. It should be stressed that, since λ
can be absorbed into σ, Eqn. (4) can be reformulated as

• Considering that the MAP framework (Eqn. (3)) can per-
form generic image super-resolution with the same image
prior, it is intuitive to jointly perform denoising and SISR
in a uniﬁed CNN framework. Moreover, the work [56] in-
dicates that the parameters of the MAP inference mainly
model the prior; therefore, CNN has the capacity to deal
with multiple degradations via a single model.

From the viewpoint of MAP framework, one can see
that the goal of SISR is to learn a mapping function ˆx =
F(y, k, σ; Θ) rather than ˆx = F(y; Θ). However, it is not
an easy task to directly model ˆx = F(y, k, σ; Θ) via CNN.
The reason lies in the fact that the three inputs y, k and σ
have different dimensions. In the next subsection, we will
propose a simple dimensionality stretching strategy to re-
solve this problem.

3.3. Dimensionality Stretching

The proposed dimensionality stretching strategy is
schematically illustrated in Figure 2. Suppose the inputs
consist of a blur kernel of size p×p, a noise level σ and an
LR image of size W × H × C, where C denotes the number
of channels. The blur kernel is ﬁrst vectorized into a vector

Vectorization

PCA

Stretching

H

Blur Kernel

Noise Level

W

t+1

Degradation Maps

Figure 2. Schematic illustration of the dimensionality stretching
strategy. For an LR image of size W × H, the vectorized blur ker-
nel is ﬁrst projected onto a space of dimension t and then stretched
into a tensor M of size W × H × (t + 1) with the noise level.

of size p2 × 1 and then projected onto t-dimensional lin-
ear space by the PCA (Principal Component Analysis) tech-
nique. After that, the concatenated low dimensional vector
and the noise level, denoted by v, are stretched into degra-
dation maps M of size W × H × (t + 1), where all the
elements of i-th map are vi. By doing so, the degradation
maps then can be concatenated with the LR image, mak-
ing CNN possible to handle the three inputs. Such a simple
strategy can be easily exploited to deal with spatially vari-
ant degradations by considering the fact that the degradation
maps can be non-uniform.

The proposed super-resolution network for multiple
degradations, denoted by SRMD, is illustrated in Figure 3.
As one can see, the distinctive feature of SRMD is that
it takes the concatenated LR image and degradation maps
as input. To show the effectiveness of the dimensionality
stretching strategy, we resort to plain CNN without com-
plex architectural engineering. Typically, to super-resolve
an LR image with a scale factor of s, SRMD ﬁrst takes
the concatenated LR image and degradation maps of size
W × H × (C + t + 1) as input. Then, similar to [24], a
cascade of 3 × 3 convolutional layers are applied to perform
the non-linear mapping. Each layer is composed of three
types of operations, including Convolution (Conv), Recti-
ﬁed Linear Units (ReLU) [26], and Batch Normalization
(BN) [20]. Speciﬁcally, “Conv + BN + ReLU” is adopted
for each convolutional layer except the last convolutional
layer which consists of a single “Conv” operation. Finally, a
sub-pixel convolution layer [41] is followed by the last con-
volutional layer to convert multiple HR subimages of size
W × H × s2C to a single HR image of size sW × sH × C.
For all scale factors 2, 3 and 4, the number of convolu-
tional layers is set to 12, and the number of feature maps
in each layer is set to 128. We separately learn models for
each scale factor. In particular, we also learn the models for

ˆx = F(y, k, σ; Θ).

(6)

3.4. Proposed Network

LR Image & Degradation Maps

Nonlinear Mapping

HR Subimages

HR Image

Figure 3. The architecture of the proposed convolutional super-resolution network. In contrast to other CNN-based SISR methods which
only take the LR image as input and lack scalability to handle other degradations, the proposed network takes the concatenated LR image
and degradation maps as input, thus allowing a single model to manipulate multiple and even spatially variant degradations.

noise-free degradation, namely SRMDNF, by removing the
connection of the noise level map in the ﬁrst convolutional
ﬁlter and ﬁne-tuning with new training data.

It is worth pointing out that neither residual learning nor
bicubicly interpolated LR image is used for the network de-
sign due to the following reasons. First, with a moderate
network depth and advanced CNN training and design such
as ReLU [26], BN [20] and Adam [25], it is easy to train
the network without the residual learning strategy. Second,
since the degradation involves noise, bicubicly interpolated
LR image would aggravate the complexity of noise which
in turn will increase the difﬁculty of training.

3.5. Why not Learn a Blind Model?

To enhance the practicability of CNN for SISR, it seems
the most straightforward way is to learn a blind model with
synthesized training data by different degradations. How-
ever, such blind model does not perform as well as ex-
pected. First, the performance deteriorates seriously when
the blur kernel model is complex, e.g., motion blur. This
phenomenon can be explained by the following example.
Given an HR image, a blur kernel and corresponding LR
image, shifting the HR image to left by one pixel and shift-
ing the blur kernel to right by one pixel would result in the
same LR image. Thus, an LR image may correspond to dif-
ferent HR images with pixel shift. This in turn would aggra-
vate the pixel-wise average problem [29], typically leading
to over-smoothed results. Second, the blind model without
specially designed architecture design has inferior general-
ization ability and performs poorly in real applications.

In contrast, non-blind model for multiple degradations
suffers little from the pixel-wise average problem and has
better generalization ability. First, the degradation maps
contain the warping information and thus can enable the
network to have spatial transformation capability. For clar-
ity, one can treat the degradation maps induced by blur ker-
nel and noise level as the output of a spatial transformer
as in [21]. Second, by anchoring the model with degrada-
tion maps, the non-blind model generalizes easily to unseen
degradations and has the ability to control the tradeoff be-
tween data ﬁdelity term and regularization term.

4. Experiments

4.1. Training Data Synthesis and Network Training

Before synthesizing LR images according to Eqn. (1), it
is necessary to deﬁne the blur kernels and noise level range,
as well as providing a large-scale clean HR image set.

For the blur kernels, we follow the kernel model of
isotropic Gaussian with a ﬁxed kernel width which has been
proved practically feasible in SISR applications. Speciﬁ-
cally, the kernel width ranges are set to [0.2, 2], [0.2, 3] and
[0.2, 4] for scale factors 2, 3 and 4, respectively. We sample
the kernel width by a stride of 0.1. The kernel size is ﬁxed
to 15×15. To further expand the degradation space, we also
consider a more general kernel assumption, i.e., anisotropic
Gaussian, which is characterized by a Gaussian probabil-
ity density function N (0, Σ) with zero mean and varying
covariance matrix Σ [38]. The space of such Gaussian ker-
nel is determined by rotation angle of the eigenvectors of Σ
and scaling of corresponding eigenvalues. We set the rota-
tion angle range to [0, π]. For the scaling of eigenvalues, it
is set from 0.5 to 6, 8 and 10 for scale factors 2, 3 and 4,
respectively.

Although we adopt the bicubic downsampler through-
out the paper, it is straightforward to train a model with di-
rect downsampler. Alternatively, we can also include the
degradations with direct downsampler by approximating it.
Speciﬁcally, given a blur kernel kd under direct downsam-
pler ↓d, we can ﬁnd the corresponding blur kernel kb under
bicubic downsampler ↓b by solving the following problem
with a data-driven method

kb = arg min kb (cid:107)(x ⊗ kb) ↓b

s −(x ⊗ kd) ↓d

s (cid:107)2,

∀ x. (7)

In this paper, we also include such degradations for scale
factor 3.

Once the blur kernels are well-deﬁned or learned, we
then uniformly sample substantial kernels and aggregate
them to learn the PCA projection matrix. By preserving
about 99.8% of the energy, the kernels are projected onto a
space of dimension 15 (i.e., t = 15). The visualization of
some typical blur kernels for scale factor 3 and some PCA
eigenvectors is shown in Figure 4.

Figure 4. Visualization of six typical blur kernels (ﬁst row) of
isotropic Gaussian (ﬁrst two), anisotropic Gaussian (middle two)
and estimated ones for direct downsampler (last two) for scale fac-
tor 3 and PCA eigenvectors (second row) for the ﬁrst six largest
eigenvalues.

For the noise level range, we set it as [0, 75]. Because the
proposed method operates on RGB channels rather than Y
channel in YCbCr color space, we collect a large-scale color
images for training, including 400 BSD [33] images, 800
training images from DIV2K dataset [1] and 4, 744 images
from WED dataset [31].

Then, given an HR image, we synthesize LR image by
blurring it with a blur kernel k and bicubicly downsampling
it with a scale factor s, followed by an addition of AWGN
with noise level σ. The LR patch size is set to 40×40 which
means the corresponding HR patch sizes for scale factors 2,
3, and 4 are 80 × 80, 120 × 120 and 160 × 160, respectively.
In the training phase, we randomly select a blur ker-
nel and a noise level to synthesize an LR image and crop
N = 128×3, 000 LR/HR patch pairs (along with the degra-
dation maps) for each epoch. We optimize the following
loss function using Adam [25]

L(Θ) =

(cid:107)F(yi, Mi; Θ) − xi(cid:107)2.

(8)

1
2N

N
(cid:88)

i=1

The mini-batch size is set to 128. The learning rate starts
from 10−3 and reduces to 10−4 when the training error
stops decreasing. When the training error keeps unchanged
in ﬁve sequential epochs, we merge the parameters of each
batch normalization into the adjacent convolution ﬁlters.
Then, a small learning rate of 10−5 is used for additional
100 epochs to ﬁne-tune the model. Since SRMDNF is ob-
tained by ﬁne-tuning SRMD, its learning rate is ﬁxed to
10−5 for 200 epochs.

We train the models in Matlab (R2015b) environment
with MatConvNet package [48] and an Nvidia Titan X Pas-
cal GPU. The training of a single SRMD model can be done
in about two days. The source code can be downloaded at
https://github.com/cszn/SRMD.

4.2. Experiments on Bicubic Degradation

As mentioned above, instead of handling the bicubic
degradation only, our aim is to learn a single network to
handle multiple degradations. However, in order to show
the advantage of the dimensionality stretching strategy, the

proposed method is also compared with other CNN-based
methods speciﬁcally designed for bicubic degradation.

Table 1 shows the PSNR and SSIM [50] results of
state-of-the-art CNN-based SISR methods on four widely-
used datasets. As one can see, SRMD achieves compa-
rable results with VDSR at small scale factor and outper-
forms VDSR at large scale factor. In particular, SRMDNF
achieves the best overall quantitative results. Using Ima-
geNet dataset [26] to train the speciﬁc model with bicubic
degradation, SRResNet performs slightly better than SR-
MDNF on scale factor 4. To further compare with other
methods such as VDSR, we also have trained a SRMDNF
model (for scale factor 3) which operates on Y channel with
291 training images. The learned model achieves 33.97dB,
29.96dB, 28.95dB and 27.42dB on Set5, Set14, BSD100
and Urban100, respectively. As a result, it can still outper-
form other competing methods. The possible reason lies
in that the SRMDNF with multiple degradations shares the
same prior in the MAP framework which facilitates the im-
plicit prior learning and thus beneﬁts to PSNR improve-
ment. This also can explain why VDSR with multiple scales
improves the performance.

For the GPU run time, SRMD spends 0.084, 0.042 and
0.027 seconds to reconstruct an HR image of size 1, 024 ×
1, 024 for scale factors 2, 3 and 4, respectively. As a com-
parison, the run time of VDSR is 0.174 second for all scale
factors. Figure 5 shows the visual results of different meth-
ods. One can see that our proposed method yields very com-
petitive performance against other methods.

4.3. Experiments on General Degradations

In this subsection, we evaluate the performance of the
proposed method on general degradations. The degrada-
tion settings are given in Table 2. We only consider the
isotropic Gaussian blur kernel for an easy comparison. To
further show the scalability of the proposed method, another
widely-used degradation [11] which involves 7×7 Gaussian
kernel with width 1.6 and direct downsampler with scale
factor 3 is also included. We compare the proposed method
with VDSR, two model-based methods (i.e., NCSR [11]
and IRCNN [57]), and a cascaded denoising-SISR method
(i.e., DnCNN [56]+SRMDNF).

The quantitative results of different methods with dif-
ferent degradations on Set5 are provided in Table 2, from
which we have observations and analyses as follows. First,
the performance of VDSR deteriorates seriously when the
assumed bicubic degradation deviates from the true one.
Second, SRMD produces much better results than NCSR
and IRCNN, and outperforms DnCNN+SRMDNF. In par-
ticular, the PSNR gain of SRMD over DnCNN+SRMDNF
increases with the kernel width which veriﬁes the advantage
of joint denoising and super-resolution. Third, by setting
proper blur kernel, the proposed method delivers good per-

Table 1. Average PSNR and SSIM results for bicubic degradation on datasets Set5 [3], Set14 [54], BSD100 [33] and Urban100 [19]. The
best two results are highlighted in red and blue colors, respectively.

Dataset

Scale
Factor

Bicubic

SRCNN [9]

VDSR [24]

SRResNet [29]

DRRN [44]

LapSRN [27]

SRMD

SRMDNF

PSNR / SSIM

Set5

Set14

BSD100

Urban100

×2
×3
×4
×2
×3
×4
×2
×3
×4
×2
×3
×4

33.64 / 0.929
30.39 / 0.868
28.42 / 0.810
30.22 / 0.868
27.53 / 0.774
25.99 / 0.702
29.55 / 0.843
27.20 / 0.738
25.96 / 0.667
26.66 / 0.841
24.46 / 0.737
23.14 / 0.657

36.62 / 0.953
32.74 / 0.908
30.48 / 0.863
32.42 / 0.906
29.27 / 0.821
27.48 / 0.751
31.34 / 0.887
28.40 / 0.786
26.90 / 0.710
29.53 / 0.897
26.25 / 0.801
24.52 / 0.722

37.56 / 0.959
33.67 / 0.922
31.35 / 0.885
33.02 / 0.913
29.77 / 0.832
27.99 / 0.766
31.89 / 0.896
28.82 / 0.798
27.28 / 0.726
30.76 / 0.914
27.13 / 0.828
25.17 / 0.753

–
–
32.05 / 0.891
–
–
28.49 / 0.780
–
–
27.58 / 0.735
–
–
–

37.66 / 0.959
33.93 / 0.923
31.58 / 0.886
33.19 / 0.913
29.94 / 0.834
28.18 / 0.770
32.01 / 0.897
28.91 / 0.799
27.35 / 0.726
31.02 / 0.916
27.38 / 0.833
25.35 / 0.758

37.52 / 0.959
33.82 / 0.922
31.54 / 0.885
33.08 / 0.913
29.89 / 0.834
28.19 / 0.772
31.80 / 0.895
28.82 / 0.798
27.32 / 0.727
30.82 / 0.915
27.07 / 0.828
25.21 / 0.756

37.53 / 0.959
33.86 / 0.923
31.59 / 0.887
33.12 / 0.914
29.84 / 0.833
28.15 / 0.772
31.90 / 0.896
28.87 / 0.799
27.34 / 0.728
30.89 / 0.916
27.27 / 0.833
25.34 / 0.761

37.79 / 0.960
34.12 / 0.925
31.96 / 0.893
33.32 / 0.915
30.04 / 0.837
28.35 / 0.777
32.05 / 0.898
28.97 / 0.803
27.49 / 0.734
31.33 / 0.920
27.57 / 0.840
25.68 / 0.773

(a) SRCNN (23.78dB)

(b) VDSR (24.20dB)

(c) DRRN (25.11dB)

(d) LapSR (24.47dB)

(e) SRMD (25.09dB)

(f)SRMDNF (25.74dB)

Figure 5. SISR performance comparison of different methods with scale factor 4 on image “Img 099” from Urban100.

Table 2. Average PSNR and SSIM results of different methods with different degradations on Set5. The best results are highlighted in red
color. The results highlighted in gray color indicate unfair comparison due to mismatched degradation assumption.

Degradation Settings
Down-
sampler

Kernel
Width

Noise
Level

VDSR [24]

NCSR [11]

IRCNN [57]

DnCNN [56]+SRMDNF

SRMD

SRMDNF

PSNR (×2/×3/×4)

0.2
0.2
0.2
1.3
1.3
1.3
2.6
2.6
2.6

1.6

Bicubic
Bicubic
Bicubic
Bicubic
Bicubic
Bicubic
Bicubic
Bicubic
Bicubic

Direct

0
15
50
0
15
50
0
15
50

0

37.56/33.67/31.35
26.02/25.40/24.70
16.02/15.72/15.46
30.57/30.24/29.72
24.82/24.70/24.30
15.89/15.68/15.43
26.37/26.31/26.28
23.09/23.07/22.98
15.58/15.43/15.23

– /23.82/–
–
–
– /21.81/–
–
–
– /21.46/–
–
–

37.43/33.39/31.02
32.60/30.08/28.35
28.20/26.25/24.95
36.01/33.33/31.01
29.96/28.68/27.71
26.69/25.20/24.42
32.07/31.09/30.06
26.44/25.67/24.36
22.98/22.16/21.43

–
32.47/30.07/28.31
28.20/26.27/24.93
–
27.68/28.78/27.71
24.35/25.19/24.39
–

– /21.33/23.85
– /19.03/21.15

37.53/33.86/31.59
32.76/30.43/28.79
28.51/26.48/25.18
37.04/33.77/31.56
30.98/29.43/28.21
27.43/25.82/24.77
33.24/32.59/31.20
28.48/27.55/26.82
25.85/24.75/23.98

37.79/34.12/31.96
–
–
37.45/34.16/31.99
–
–
34.12/33.02/31.77
–
–

– /30.54/ –

– /33.02/ –

– /33.38/ –

–

– /33.74/ –

– /34.01/ –

(a) Ground-truth

(f) SRMDNF (30.34dB)
Figure 6. SISR performance comparison on image “Butterﬂy” from Set5. The degradation involves 7×7 Gaussian kernel with width 1.6
and direct downsampler with scale factor 3. Note that the comparison with VDSR is unfair because of degradation mismatch.

(d) IRCNN (29.32dB)

(e) SRMD (29.79dB)

(b) VDSR (24.73dB)

(c) NCSR (28.01dB)

formance in handling the degradation with direct downsam-
pler. The visual comparison is given in Figure 6. One can
see that NCSR and IRCNN produce more visually pleasant
results than VDSR since their assumed degradation matches
the true one. However, they cannot recover edges as sharper
as SRMD and SRMDNF.

4.4. Experiments on Spatially Variant Degradation

To demonstrate the effectiveness of SRMD for spatially
variant degradation, we synthesize an LR images with spa-
tially variant blur kernels and noise levels. Figure 7 shows
the visual result of the proposed SRMD for the spatially

(a)

(b)

(c)

Figure 7. An example of SRMD on dealing with spatially variant
degradation. (a) Noise level and Gaussian blur kernel width maps.
(b) Zoomed LR image. (c) Results of SRMD with scale factor 2.

(a) LR image

(b) VDSR [24]

(c) SelfEx [19]

(d) SRMD

Figure 9. SISR results on real image “Chip” with scale factor 4.

ure 8 that the performance of VDSR is severely affected by
the compression artifacts. While Waifu2x can successfully
remove the compression artifacts, it fails to recover sharp
edges. In comparison, SRMD can not only remove the un-
satisfying artifacts but also produce sharp edges. From Fig-
ure 9, we can see that VDSR and SelfEx both tend to pro-
duce over-smoothed results, whereas SRMD can recover
sharp image with better intensity and gradient statistics of
clean images [35].

5. Conclusion

In this paper, we proposed an effective super-resolution
network with high scalability of handling multiple degra-
dations via a single model. Different from existing CNN-
based SISR methods,
the proposed super-resolver takes
both LR image and its degradation maps as input. Specif-
ically, degradation maps are obtained by a simple dimen-
sionality stretching of the degradation parameters (i.e., blur
kernel and noise level). The results on synthetic LR images
demonstrated that the proposed super-resolver can not only
produce state-of-the-art results on bicubic degradation but
also perform favorably on other degradations and even spa-
tially variant degradations. Moreover, the results on real
LR images showed that the proposed method can recon-
struct visually plausible HR images. In summary, the pro-
posed super-resolver offers a feasible solution toward prac-
tical CNN-based SISR applications.

6. Acknowledgements

This work is supported by National Natural Science
Foundation of China (grant no. 61671182, 61471146),
HK RGC General Research Fund (PolyU 152240/15E)
and PolyU-Alibaba Collaborative Research Project “Qual-
ity Enhancement of Surveillance Images and Videos”. We
gratefully acknowledge the support from NVIDIA Corpo-
ration for providing us the Titan Xp GPU used in this re-
search.

(a) LR image

(b) VDSR [24]

(c) Waifu2x [49]

(d) SRMD

Figure 8. SISR results on image “Cat” with scale factor 2.

variant degradations. One can see that the proposed SRMD
is effective in recovering the latent HR image. Note that the
blur kernel is assumed to be isotropic Gaussian.

4.5. Experiments on Real Images

Besides the above experiments on LR images syntheti-
cally downsampled from HR images with known blur ker-
nels and corrupted by AWGN with known noise levels, we
also do experiments on real LR images to demonstrate the
effectiveness of the proposed SRMD. Since there are no
ground-truth HR images, we only provide the visual com-
parison.

As aforementioned, while we also use anisotropic Gaus-
sian kernels in training,
it is generally feasible to use
isotropic Gaussian for most of the real LR images in test-
ing. To ﬁnd the degradation parameters with good visual
quality, we use a grid search strategy rather than adopting
any blur kernel or noise level estimation methods. Speciﬁ-
cally, the kernel width is uniformly sampled from 0.1 to 2.4
with a stride of 0.1, and the noise level is from 0 to 75 with
stride 5.

Figures 8 and 9 illustrate the SISR results on two real LR
images “Cat” and “Chip”, respectively. The VDSR [24] is
used as one of the representative CNN-based methods for
comparison. For image “Cat” which is corrupted by com-
pression artifacts, Waifu2x [49] is also used for compari-
son. For image “Chip” which contains repetitive structures,
a self-similarity based method SelfEx [19] is also included
for comparison.

It can be observed from the visual results that SRMD
can produce much more visually plausible HR images than
the competing methods. Speciﬁcally, one can see from Fig-

References

[1] E. Agustsson and R. Timofte. Ntire 2017 challenge on sin-
gle image super-resolution: Dataset and study. In IEEE Con-
ference on Computer Vision and Pattern Recognition Work-
shops, volume 3, pages 126–135, July 2017.

[2] S. Baker and T. Kanade. Limits on super-resolution and how
to break them. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 24(9):1167–1183, 2002.

[3] M. Bevilacqua, A. Roumy, C. Guillemot, and M.-L. A.
Morel. Low-complexity single-image super-resolution based
on nonnegative neighbor embedding. In British Machine Vi-
sion Conference, 2012.

[4] S. A. Bigdeli, M. Jin, P. Favaro, and M. Zwicker. Deep mean-
shift priors for image restoration. In Advances in Neural In-
formation Processing Systems, 2017.

[5] G. Boracchi and A. Foi. Modeling the performance of image
restoration from motion blur. IEEE Transactions on Image
Processing, 21(8):3502–3517, Aug 2012.

[6] Y. Chen, W. Yu, and T. Pock. On learning optimized reac-
tion diffusion processes for effective image restoration.
In
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 5261–5269, 2015.

[7] Z. Cui, H. Chang, S. Shan, B. Zhong, and X. Chen. Deep
In European

network cascade for image super-resolution.
Conference on Computer Vision, pages 49–64, 2014.

[9] C. Dong, C. C. Loy, K. He, and X. Tang.

[8] C. Dong, C. C. Loy, K. He, and X. Tang. Learning a deep
convolutional network for image super-resolution. In Euro-
pean Conference on Computer Vision, pages 184–199, 2014.
Image
IEEE
super-resolution using deep convolutional networks.
Transactions on Pattern Analysis and Machine Intelligence,
38(2):295–307, 2016.

[10] C. Dong, C. C. Loy, and X. Tang. Accelerating the super-
resolution convolutional neural network. In European Con-
ference on Computer Vision, pages 391–407, 2016.

[11] W. Dong, L. Zhang, G. Shi, and X. Li. Nonlocally central-
ized sparse representation for image restoration. IEEE Trans-
actions on Image Processing, 22(4):1620–1630, 2013.
[12] N. Efrat, D. Glasner, A. Apartsin, B. Nadler, and A. Levin.
Accurate blur models vs. image priors in single image super-
resolution. In IEEE International Conference on Computer
Vision, pages 2832–2839, 2013.
[13] K. Egiazarian and V. Katkovnik.

Single image super-
In European Signal

resolution via BM3D sparse coding.
Processing Conference, pages 2849–2853, 2015.

[14] W. Freeman and C. Liu. Markov random ﬁelds for super-
resolution and texture synthesis. Advances in Markov Ran-
dom Fields for Vision and Image Processing, 1:155–165,
2011.

[15] D. Glasner, S. Bagon, and M. Irani. Super-resolution from
a single image. In IEEE International Conference on Com-
puter Vision, pages 349–356, 2009.

[16] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in neural information
processing systems, pages 2672–2680, 2014.

[17] H. He and W.-C. Siu. Single image super-resolution using
Gaussian process regression. In IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 449–456, 2011.

[18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
In IEEE Conference on Computer

for image recognition.
Vision and Pattern Recognition, pages 770–778, 2016.
[19] J.-B. Huang, A. Singh, and N. Ahuja. Single image super-
resolution from transformed self-exemplars. In IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
5197–5206, 2015.

[20] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
International Conference on Machine Learning, pages 448–
456, 2015.

[21] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial
In Advances in Neural Information

transformer networks.
Processing Systems, pages 2017–2025, 2015.

[22] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for
In European

real-time style transfer and super-resolution.
Conference on Computer Vision, pages 694–711, 2016.
[23] J. Kim, J. Kwon Lee, and K. Mu Lee. Deeply-recursive con-
volutional network for image super-resolution. In IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
1637–1645, 2016.

[24] J. Kim, J. K. Lee, and K. M. Lee. Accurate image super-
resolution using very deep convolutional networks. In IEEE
Conference on Computer Vision and Pattern Recognition,
pages 1646–1654, 2016.

[25] D. Kingma and J. Ba. Adam: A method for stochastic op-
timization. In International Conference for Learning Repre-
sentations, 2015.

[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in Neural Information Processing Systems, pages
1097–1105, 2012.

[27] W.-S. Lai, J.-B. Huang, N. Ahuja, and M.-H. Yang. Deep
laplacian pyramid networks for fast and accurate super-
In IEEE Conference on Computer Vision and
resolution.
Pattern Recognition, pages 624–632, July 2017.

[28] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature,

521(7553):436–444, 2015.

[29] C. Ledig, L. Theis, F. Husz´ar, J. Caballero, A. Cunningham,
A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, et al.
Photo-realistic single image super-resolution using a gen-
In IEEE Conference on Com-
erative adversarial network.
puter Vision and Pattern Recognition, pages 4681–4690,
July 2017.

[30] B. Lim, S. Son, H. Kim, S. Nah, and K. M. Lee. Enhanced
deep residual networks for single image super-resolution. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion Workshops, pages 136–144, July 2017.

[31] K. Ma, Z. Duanmu, Q. Wu, Z. Wang, H. Yong, H. Li, and
L. Zhang. Waterloo exploration database: New challenges
for image quality assessment models. IEEE Transactions on
Image Processing, 26(2):1004–1016, 2017.

[32] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman.
Non-local sparse models for image restoration. In IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
2272–2279, 2009.

[33] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database
of human segmented natural images and its application to
evaluating segmentation algorithms and measuring ecologi-

Conference, pages 689–692, 2015.

[49] Waifu2x.

Image super-resolution for anime-style art using
deep convolutional neural networks. http://waifu2x.
udp.jp/.

[50] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli.
from error visibility to struc-
IEEE Transactions on Image Processing,

Image quality assessment:
tural similarity.
13(4):600–612, 2004.

[51] C.-Y. Yang, C. Ma, and M.-H. Yang. Single-image super-
resolution: A benchmark. In European Conference on Com-
puter Vision, pages 372–386, 2014.

[52] J. Yang, J. Wright, T. S. Huang, and Y. Ma. Image super-
resolution via sparse representation. IEEE Transactions on
Image Processing, 19(11):2861–2873, 2010.

[53] W. Yang, J. Feng, J. Yang, F. Zhao, J. Liu, Z. Guo, and
S. Yan. Deep edge guided recurrent residual learning for
image super-resolution. IEEE Transactions on Image Pro-
cessing, 26(12):5895–5907, 2017.

[54] R. Zeyde, M. Elad, and M. Protter. On single image scale-up
using sparse-representations. In International conference on
curves and surfaces, pages 711–730, 2010.

[55] K. Zhang, X. Zhou, H. Zhang, and W. Zuo. Revisiting sin-
gle image super-resolution under internet environment: blur
kernels and reconstruction algorithms. In Paciﬁc Rim Con-
ference on Multimedia, pages 677–687, 2015.

[56] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang. Be-
yond a gaussian denoiser: Residual learning of deep CNN
for image denoising. IEEE Transactions on Image Process-
ing, pages 3142–3155, 2017.

[57] K. Zhang, W. Zuo, S. Gu, and L. Zhang. Learning deep CNN
In IEEE Conference
denoiser prior for image restoration.
on Computer Vision and Pattern Recognition, pages 3929–
3938, July 2017.

[58] Y. Zhang, Y. Tian, Y. Kong, B. Zhong, and Y. Fu. Residual
dense network for image super-resolution. In IEEE Confer-
ence on Computer Vision and Pattern Recognition, 2018.

cal statistics. In IEEE International Conference on Computer
Vision, volume 2, pages 416–423, July 2001.

[34] T. Meinhardt, M. M¨oller, C. Hazirbas, and D. Cremers.
Learning proximal operators: Using denoising networks for
In IEEE Interna-
regularizing inverse imaging problems.
tional Conference on Computer Vision, pages 1781–1790,
2017.

[35] J. Pan, Z. Hu, Z. Su, and M.-H. Yang. Deblurring text images
via L0-regularized intensity and gradient prior. In IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
2901–2908, 2014.

[36] T. Peleg and M. Elad. A statistical prediction model based
on sparse representations for single image super-resolution.
IEEE transactions on Image Processing, 23(6):2569–2582,
2014.

[37] J. S. Ren, L. Xu, Q. Yan, and W. Sun. Shepard convolu-
tional neural networks. In Advances in Neural Information
Processing Systems, pages 901–909, 2015.

[38] G. Riegler, S. Schulter, M. Ruther, and H. Bischof. Condi-
tioned regression models for non-blind single image super-
resolution. In IEEE International Conference on Computer
Vision, pages 522–530, 2015.

[39] Y. Romano, M. Elad, and P. Milanfar. The little engine that
could: Regularization by denoising (red). SIAM Journal on
Imaging Sciences, 10(4):1804–1844, 2017.

[40] Y. Romano, J. Isidoro, and P. Milanfar. RAISR: rapid and ac-
curate image super resolution. IEEE Transactions on Com-
putational Imaging, 3(1):110–125, 2017.

[41] W. Shi, J. Caballero, F. Husz´ar, J. Totz, A. P. Aitken,
R. Bishop, D. Rueckert, and Z. Wang. Real-time single im-
age and video super-resolution using an efﬁcient sub-pixel
convolutional neural network. In IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 1874–1883,
2016.

[42] Y. Shi, K. Wang, C. Chen, L. Xu, and L. Lin. Structure-
preserving image super-resolution via contextualized multi-
task learning. IEEE Transactions on Multimedia, 2017.
[43] A. Singh, F. Porikli, and N. Ahuja. Super-resolving noisy
images. In IEEE Conference on Computer Vision and Pattern
Recognition, pages 2846–2853, 2014.

[44] Y. Tai, J. Yang, and X. Liu. Image super-resolution via deep
In IEEE Conference on Com-
recursive residual network.
puter Vision and Pattern Recognition, pages 3147–3155,
2017.

[45] Y. Tai, J. Yang, X. Liu, and C. Xu. Memnet: A persistent
In IEEE Interna-
memory network for image restoration.
tional Conference on Computer Vision, pages 4539–4547,
2017.

[46] R. Timofte, E. Agustsson, L. Van Gool, M.-H. Yang, and
L. Zhang. Ntire 2017 challenge on single image super-
In IEEE Conference on
resolution: Methods and results.
Computer Vision and Pattern Recognition Workshops, pages
114–125, July 2017.

[47] R. Timofte, V. De Smet, and L. Van Gool. A+: Adjusted
anchored neighborhood regression for fast super-resolution.
In Asian Conference on Computer Vision, pages 111–126,
2014.

[48] A. Vedaldi and K. Lenc. MatConvNet: Convolutional neu-
ral networks for matlab. In ACM Conference on Multimedia

8
1
0
2
 
y
a
M
 
4
2
 
 
]

V
C
.
s
c
[
 
 
2
v
6
1
1
6
0
.
2
1
7
1
:
v
i
X
r
a

Learning a Single Convolutional Super-Resolution Network for
Multiple Degradations

Kai Zhang1,2,3, Wangmeng Zuo1, Lei Zhang2
1School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China
2Dept. of Computing, The Hong Kong Polytechnic University, Hong Kong, China
3DAMO Academy, Alibaba Group
cskaizhang@gmail.com, wmzuo@hit.edu.cn, cslzhang@comp.polyu.edu.hk

Abstract

Recent years have witnessed the unprecedented success
of deep convolutional neural networks (CNNs) in single im-
age super-resolution (SISR). However, existing CNN-based
SISR methods mostly assume that a low-resolution (LR) im-
age is bicubicly downsampled from a high-resolution (HR)
image, thus inevitably giving rise to poor performance when
the true degradation does not follow this assumption. More-
over, they lack scalability in learning a single model to non-
blindly deal with multiple degradations. To address these
issues, we propose a general framework with dimensional-
ity stretching strategy that enables a single convolutional
super-resolution network to take two key factors of the SISR
degradation process, i.e., blur kernel and noise level, as in-
put. Consequently, the super-resolver can handle multiple
and even spatially variant degradations, which signiﬁcantly
improves the practicability. Extensive experimental results
on synthetic and real LR images show that the proposed
convolutional super-resolution network not only can pro-
duce favorable results on multiple degradations but also is
computationally efﬁcient, providing a highly effective and
scalable solution to practical SISR applications.

1. Introduction

Single image super-resolution (SISR) aims to recover a
high-resolution (HR) version of a low-resolution (LR) in-
put. As a classical problem, SISR is still an active yet chal-
lenging research topic in the ﬁeld of computer vision due to
its ill-poseness nature and high practical values [2]. In the
typical SISR framework, an LR image y is modeled as the
output of the following degradation process:

y = (x ⊗ k) ↓s + n,

(1)

where x ⊗ k represents the convolution between a blur ker-
nel k and a latent HR image x, ↓s is a subsequent downsam-
pling operation with scale factor s, and n usually is addi-
tive white Gaussian noise (AWGN) with standard deviation
(noise level) σ.

SISR methods can be broadly classiﬁed into three cate-
gories, i.e., interpolation-based methods, model-based op-
timization methods and discriminative learning methods.
Interpolation-based methods such as nearest-neighbor, bi-
linear and bicubic interpolators are simple and efﬁcient but
have very limited performance. By exploiting powerful im-
age priors (e.g., the non-local self-similarity prior [11, 32],
sparsity prior [52] and denoiser prior [4, 13, 57]), model-
based optimization methods are ﬂexible to reconstruct rel-
ative high-quality HR images, but they usually involve a
time-consuming optimization procedure. Although the in-
tegration of convolutional neural network (CNN) denoiser
prior and model-based optimization can improve the efﬁ-
ciency to some extent, it still suffers from the typical draw-
backs of model-based optimization methods, e.g., it is not in
an end-to-end learning manner and involves hand-designed
parameters [57]. As an alternative, discriminative learning
methods have attracted considerable attentions due to their
favorable SISR performance in terms of effectiveness and
efﬁciency. Notably, recent years have witnessed a dramatic
upsurge of using CNN for SISR.

In this paper, we focus on discriminative CNN methods
for SISR so as to exploit the merits of CNN, such as the fast
speed by parallel computing, high accuracy by end-to-end
training, and tremendous advances in training and design-
ing networks [16, 18, 21, 28]. While several SISR mod-
els based on discriminative CNN have reported impressive
results, they suffer from a common drawback: their mod-
els are specialized for a single simpliﬁed degradation (e.g.,
bicubic degradation) and lack scalability to handle multiple
degradations by using a single model. Because the practical
degradation of SISR is much more complex [40, 51], the
performance of learned CNN models may deteriorate seri-
ously when the assumed degradation deviates from the true
one, making them less effective in practical scenarios. It has
been pointed out that the blur kernel plays a vital role for the
success of SISR methods and the mismatch of blur kernels
will largely deteriorate the ﬁnal SISR results [12]. How-
ever, little work has been done on how to design a CNN to
address this crucial issue.

Given the facts above, it is natural to raise the follow-
ing questions, which are the focus of our paper: (i) Can we
learn a single model to effectively handle multiple and even
spatially variant degradations? (ii) Is it possible to use syn-
thetic data to train a model with high practicability? This
work aims to make one of the ﬁrst attempts towards answer-
ing these two questions.

To answer the ﬁrst question, we revisit and analyze the
general model-based SISR methods under the maximum a
posteriori (MAP) framework. Then we argue that one may
tackle this issue by taking LR input, blur kernel and noise
level as input to CNN but their dimensionality mismatch
makes it difﬁcult to design a single convolutional super-
resolution network. In view of this, we introduce a dimen-
sionality stretching strategy which facilitates the network to
handle multiple and even spatially variant degradations with
respect to blur kernel and noise. To the best of our knowl-
edge, there is no attempt to consider both the blur kernel
and noise for SISR via training a single CNN model.

For the second question, we will show that it is possible
to learn a practical super-resolver using synthetic data. To
this end, a large variety of degradations with different com-
binations of blur kernels and noise levels are sampled to
cover the degradation space. In a practical scenario, even
the degradation is more complex (e.g., the noise is non-
AWGN), we can select the best ﬁtted degradation model
rather than the bicubic degradation to produce a better re-
sult. It turns out that, by choosing a proper degradation, the
learned SISR model can yield perceptually convincing re-
sults on real LR images. It should be noted that we make no
effort to use specialized network architectures but use the
plain CNN as in [9, 41].

The main contributions of this paper are summarized in

the following:

• We propose a simple yet effective and scalable deep
CNN framework for SISR. The proposed model goes
beyond the widely-used bicubic degradation assump-
tion and works for multiple and even spatially variant
degradations, thus making a substantial step towards
developing a practical CNN-based super-resolver for
real applications.

• We propose a novel dimensionality stretching strategy
to address the dimensionality mismatch between LR
input image, blur kernel and noise level. Although this
strategy is proposed for SISR, it is general and can be
extended to other tasks such as deblurring.

• We show that

the proposed convolutional super-
resolution network learned from synthetic training data
can not only produce competitive results against state-
of-the-art SISR methods on synthetic LR images but
also give rise to visually plausible results on real LR
images.

2. Related Work

The ﬁrst work of using CNN to solve SISR can be
traced back to [8] where a three-layer super-resolution net-
work (SRCNN) was proposed. In the extended work [9],
the authors investigated the impact of depth on super-
resolution and empirically showed that the difﬁculty of
training deeper model hinders the performance improve-
ment of CNN super-resolvers. To overcome the training
difﬁculty, Kim et al. [24] proposed a very deep super-
resolution (VDSR) method with residual learning strategy.
Interestingly, they showed that VDSR can handle multiple
scales super-resolution. By analyzing the relation between
CNN and MAP inference, Zhang et al. [56] pointed out that
CNN mainly model the prior information and they empir-
ically demonstrated that a single model can handle mul-
tiple scales super-resolution, image deblocking and image
denoising. While achieving good performance, the above
methods take the bicubicly interpolated LR image as in-
put, which not only suffers from high computational cost
but also hinders the effective expansion of receptive ﬁeld.

To improve the efﬁciency, some researchers resort to di-
rectly manipulating the LR input and adopting an upscal-
ing operation at the end of the network. Shi et al. [41]
introduced an efﬁcient sub-pixel convolution layer to up-
scale the LR feature maps into HR images. Dong et al. [10]
adopted a deconvolution layer at the end of the network to
perform upsampling. Lai et al. [27] proposed a Laplacian
pyramid super-resolution network (LapSRN) that takes an
LR image as input and progressively predicts the sub-band
residuals with transposed convolutions in a coarse-to-ﬁne
manner. To improve the perceptual quality at a large scale
factor, Ledig et al. [29] proposed a generative adversarial
network [16] based super-resolution (SRGAN) method. In
the generator network of SRGAN, two sub-pixel convolu-
tion layers are used to efﬁciently upscale the LR input by a
factor of 4.

Although various techniques have been proposed for
SISR, the above CNN-based methods are tailored to the
widely-used settings of bicubic degradation, neglecting
their limited applicability for practical scenarios. An in-
teresting line of CNN-based methods which can go be-
yond bicubic degradation adopt a CNN denoiser to solve
SISR via model-based optimization framework [4, 34, 57].
For example,
the method proposed in [57] can handle
the widely-used Gaussian degradation as in [11]. How-
ever, manually selecting the hyper-parameters for different
degradations is not a trivial task [39]. As a result, it is desir-
able to learn a single SISR model which can handle multiple
degradations with high practicability. This paper attempts
to give a positive answer.

Due to the limited space, we can only discuss some of
the related works here. Other CNN-based SISR methods
can be found in [6, 22, 23, 30, 37, 42, 44, 45, 46, 53, 58].

3. Method

3.1. Degradation Model

Before solving the problem of SISR, it is important to
have a clear understanding of the degradation model which
is not limited to Eqn. (1). Another practical degradation
model can be given by

y = (x ↓s) ⊗ k + n.

(2)

e
s
i
o
N

When ↓ is the bicubic downsampler, Eqn. (2) corresponds
to a deblurring problem followed by a SISR problem with
bicubic degradation. Thus, it can beneﬁt from existing
deblurring methods and bicubic degradation based SISR
methods. Due to limited space, we only consider the more
widely assumed degradation model given in Eqn. (1). Nev-
ertheless, our method is general and can be easily extended
to handle Eqn. (2). In the following, we make a short dis-
cussion on blur kernel k, noise n and downsampler ↓.

Blur kernel. Different from image deblurring, the blur
kernel setting of SISR is usually simple. The most popu-
lar choice is isotropic Gaussian blur kernel parameterized
by standard deviation or kernel width [11, 51].
In [38],
anisotropic Gaussian blur kernels are also used. In practice,
more complex blur kernel models used in deblurring task,
such as motion blur [5], can be further considered. Empiri-
cal and theoretical analyses have revealed that the inﬂuence
of an accurate blur kernel is much larger than that of sophis-
ticated image priors [12]. Speciﬁcally, when the assumed
kernel is smoother than the true kernel, the recovered image
is over-smoothed. Most of SISR methods actually favor for
such case. On the other hand, when the assumed kernel is
sharper than the true kernel, high frequency ringing artifacts
will appear.

Noise. While being of low-resolution, the LR images are
usually also noisy. Directly super-resolving the noisy input
without noise removal would amplify the unwanted noise,
resulting in visually unpleasant results. To address this
problem, the straightforward way is to perform denoising
ﬁrst and then enhance the resolution. However, the denois-
ing pre-processing step tends to lose detail information and
would deteriorate the subsequent super-resolution perfor-
mance [43]. Thus, it would be highly desirable to jointly
perform denoising and super-resolution.

Downsampler. Existing literatures have considered two
types of downsamplers, including direct downsampler [11,
17, 36, 51, 55] and bicubic downsampler [7, 12, 14, 15,
47, 52]. In this paper, we consider the bicubic downsam-
pler since when k is delta kernel and the noise level is
zero, Eqn. (1) turns into the widely-used bicubic degrada-
It should be pointed out that, different from
tion model.
blur kernel and noise which vary in a general degradation
model, downsampler is assumed to be ﬁxed.

Blur Kernel
Figure 1. An illustration of different degradations for SISR. The
scale factor is 2. The general degradation models of Eqns. (1)
and (2) assume an HR image actually can degrade into many LR
images, whereas bicubic degradation model assumes an HR image
corresponds to a single LR image.

Though blur kernel and noise have been recognized as
key factors for the success of SISR and several methods
have been proposed to consider those two factors, there has
been little effort towards simultaneously considering blur
kernel and noise in a single CNN framework. It is a chal-
lenging task since the degradation space with respect to
blur kernel and noise is rather large (see Figure 1 as an ex-
ample). One relevant work is done by Zhang et al. [57];
nonetheless, their method is essentially a model-based op-
timization method and thus suffers from several drawbacks
as mentioned previously. In another related work, Riegler et
al. [38] exploited the blur kernel information into the SISR
model. Our method differs from [38] on two major as-
pects. First, our method considers a more general degra-
dation model. Second, our method exploits a more effective
way to parameterize the degradation model.

3.2. A Perspective from MAP Framework

Though existing CNN-based SISR methods are not nec-
essarily derived under the traditional MAP framework, they
have the same goal. We revisit and analyze the general
MAP framework of SISR, aiming to ﬁnd the intrinsic con-
nections between the MAP principle and the working mech-
anism of CNN. Consequently, more insights on CNN archi-
tecture design can be obtained.

Due to the ill-posed nature of SISR, regularization needs
to be imposed to constrain the solution. Mathematically,
the HR counterpart of an LR image y can be estimated by
solving the following MAP problem

ˆx = arg min x

1
2σ2 (cid:107)(x ⊗ k) ↓s −y(cid:107)2 + λΦ(x)

(3)

where 1
2σ2 (cid:107)(x⊗k) ↓s −y(cid:107)2 is the data ﬁdelity term, Φ(x) is
the regularization term (or prior term) and λ is the trade-off
parameter. Simply speaking, Eqn. (3) conveys two points:
(i) the estimated solution should not only accord with the
degradation process but also have the desired property of
clean HR images; (ii) ˆx is a function of LR image y, blur
kernel k, noise level σ, and trade-off parameter λ. There-
fore, the MAP solution of (non-blind) SISR can be formu-
lated as

ˆx = F(y, k, σ, λ; Θ)

(4)

where Θ denotes the parameters of the MAP inference.

By treating CNN as a discriminative learning solution to

Eqn. (4), we can have the following insights.

• Because the data ﬁdelity term corresponds to the degra-
dation process, accurate modeling of the degradation plays
a key role for the success of SISR. However, existing CNN-
based SISR methods with bicubic degradation actually aim
to solve the following problem

ˆx = arg min x(cid:107)x ↓s −y(cid:107)2 + Φ(x).

(5)

Inevitably, their practicability is very limited.

• To design a more practical SISR model, it is preferable to
learn a mapping function like Eqn. (4), which covers more
extensive degradations. It should be stressed that, since λ
can be absorbed into σ, Eqn. (4) can be reformulated as

• Considering that the MAP framework (Eqn. (3)) can per-
form generic image super-resolution with the same image
prior, it is intuitive to jointly perform denoising and SISR
in a uniﬁed CNN framework. Moreover, the work [56] in-
dicates that the parameters of the MAP inference mainly
model the prior; therefore, CNN has the capacity to deal
with multiple degradations via a single model.

From the viewpoint of MAP framework, one can see
that the goal of SISR is to learn a mapping function ˆx =
F(y, k, σ; Θ) rather than ˆx = F(y; Θ). However, it is not
an easy task to directly model ˆx = F(y, k, σ; Θ) via CNN.
The reason lies in the fact that the three inputs y, k and σ
have different dimensions. In the next subsection, we will
propose a simple dimensionality stretching strategy to re-
solve this problem.

3.3. Dimensionality Stretching

The proposed dimensionality stretching strategy is
schematically illustrated in Figure 2. Suppose the inputs
consist of a blur kernel of size p×p, a noise level σ and an
LR image of size W × H × C, where C denotes the number
of channels. The blur kernel is ﬁrst vectorized into a vector

Vectorization

PCA

Stretching

H

Blur Kernel

Noise Level

W

t+1

Degradation Maps

Figure 2. Schematic illustration of the dimensionality stretching
strategy. For an LR image of size W × H, the vectorized blur ker-
nel is ﬁrst projected onto a space of dimension t and then stretched
into a tensor M of size W × H × (t + 1) with the noise level.

of size p2 × 1 and then projected onto t-dimensional lin-
ear space by the PCA (Principal Component Analysis) tech-
nique. After that, the concatenated low dimensional vector
and the noise level, denoted by v, are stretched into degra-
dation maps M of size W × H × (t + 1), where all the
elements of i-th map are vi. By doing so, the degradation
maps then can be concatenated with the LR image, mak-
ing CNN possible to handle the three inputs. Such a simple
strategy can be easily exploited to deal with spatially vari-
ant degradations by considering the fact that the degradation
maps can be non-uniform.

The proposed super-resolution network for multiple
degradations, denoted by SRMD, is illustrated in Figure 3.
As one can see, the distinctive feature of SRMD is that
it takes the concatenated LR image and degradation maps
as input. To show the effectiveness of the dimensionality
stretching strategy, we resort to plain CNN without com-
plex architectural engineering. Typically, to super-resolve
an LR image with a scale factor of s, SRMD ﬁrst takes
the concatenated LR image and degradation maps of size
W × H × (C + t + 1) as input. Then, similar to [24], a
cascade of 3 × 3 convolutional layers are applied to perform
the non-linear mapping. Each layer is composed of three
types of operations, including Convolution (Conv), Recti-
ﬁed Linear Units (ReLU) [26], and Batch Normalization
(BN) [20]. Speciﬁcally, “Conv + BN + ReLU” is adopted
for each convolutional layer except the last convolutional
layer which consists of a single “Conv” operation. Finally, a
sub-pixel convolution layer [41] is followed by the last con-
volutional layer to convert multiple HR subimages of size
W × H × s2C to a single HR image of size sW × sH × C.
For all scale factors 2, 3 and 4, the number of convolu-
tional layers is set to 12, and the number of feature maps
in each layer is set to 128. We separately learn models for
each scale factor. In particular, we also learn the models for

ˆx = F(y, k, σ; Θ).

(6)

3.4. Proposed Network

LR Image & Degradation Maps

Nonlinear Mapping

HR Subimages

HR Image

Figure 3. The architecture of the proposed convolutional super-resolution network. In contrast to other CNN-based SISR methods which
only take the LR image as input and lack scalability to handle other degradations, the proposed network takes the concatenated LR image
and degradation maps as input, thus allowing a single model to manipulate multiple and even spatially variant degradations.

noise-free degradation, namely SRMDNF, by removing the
connection of the noise level map in the ﬁrst convolutional
ﬁlter and ﬁne-tuning with new training data.

It is worth pointing out that neither residual learning nor
bicubicly interpolated LR image is used for the network de-
sign due to the following reasons. First, with a moderate
network depth and advanced CNN training and design such
as ReLU [26], BN [20] and Adam [25], it is easy to train
the network without the residual learning strategy. Second,
since the degradation involves noise, bicubicly interpolated
LR image would aggravate the complexity of noise which
in turn will increase the difﬁculty of training.

3.5. Why not Learn a Blind Model?

To enhance the practicability of CNN for SISR, it seems
the most straightforward way is to learn a blind model with
synthesized training data by different degradations. How-
ever, such blind model does not perform as well as ex-
pected. First, the performance deteriorates seriously when
the blur kernel model is complex, e.g., motion blur. This
phenomenon can be explained by the following example.
Given an HR image, a blur kernel and corresponding LR
image, shifting the HR image to left by one pixel and shift-
ing the blur kernel to right by one pixel would result in the
same LR image. Thus, an LR image may correspond to dif-
ferent HR images with pixel shift. This in turn would aggra-
vate the pixel-wise average problem [29], typically leading
to over-smoothed results. Second, the blind model without
specially designed architecture design has inferior general-
ization ability and performs poorly in real applications.

In contrast, non-blind model for multiple degradations
suffers little from the pixel-wise average problem and has
better generalization ability. First, the degradation maps
contain the warping information and thus can enable the
network to have spatial transformation capability. For clar-
ity, one can treat the degradation maps induced by blur ker-
nel and noise level as the output of a spatial transformer
as in [21]. Second, by anchoring the model with degrada-
tion maps, the non-blind model generalizes easily to unseen
degradations and has the ability to control the tradeoff be-
tween data ﬁdelity term and regularization term.

4. Experiments

4.1. Training Data Synthesis and Network Training

Before synthesizing LR images according to Eqn. (1), it
is necessary to deﬁne the blur kernels and noise level range,
as well as providing a large-scale clean HR image set.

For the blur kernels, we follow the kernel model of
isotropic Gaussian with a ﬁxed kernel width which has been
proved practically feasible in SISR applications. Speciﬁ-
cally, the kernel width ranges are set to [0.2, 2], [0.2, 3] and
[0.2, 4] for scale factors 2, 3 and 4, respectively. We sample
the kernel width by a stride of 0.1. The kernel size is ﬁxed
to 15×15. To further expand the degradation space, we also
consider a more general kernel assumption, i.e., anisotropic
Gaussian, which is characterized by a Gaussian probabil-
ity density function N (0, Σ) with zero mean and varying
covariance matrix Σ [38]. The space of such Gaussian ker-
nel is determined by rotation angle of the eigenvectors of Σ
and scaling of corresponding eigenvalues. We set the rota-
tion angle range to [0, π]. For the scaling of eigenvalues, it
is set from 0.5 to 6, 8 and 10 for scale factors 2, 3 and 4,
respectively.

Although we adopt the bicubic downsampler through-
out the paper, it is straightforward to train a model with di-
rect downsampler. Alternatively, we can also include the
degradations with direct downsampler by approximating it.
Speciﬁcally, given a blur kernel kd under direct downsam-
pler ↓d, we can ﬁnd the corresponding blur kernel kb under
bicubic downsampler ↓b by solving the following problem
with a data-driven method

kb = arg min kb (cid:107)(x ⊗ kb) ↓b

s −(x ⊗ kd) ↓d

s (cid:107)2,

∀ x. (7)

In this paper, we also include such degradations for scale
factor 3.

Once the blur kernels are well-deﬁned or learned, we
then uniformly sample substantial kernels and aggregate
them to learn the PCA projection matrix. By preserving
about 99.8% of the energy, the kernels are projected onto a
space of dimension 15 (i.e., t = 15). The visualization of
some typical blur kernels for scale factor 3 and some PCA
eigenvectors is shown in Figure 4.

Figure 4. Visualization of six typical blur kernels (ﬁst row) of
isotropic Gaussian (ﬁrst two), anisotropic Gaussian (middle two)
and estimated ones for direct downsampler (last two) for scale fac-
tor 3 and PCA eigenvectors (second row) for the ﬁrst six largest
eigenvalues.

For the noise level range, we set it as [0, 75]. Because the
proposed method operates on RGB channels rather than Y
channel in YCbCr color space, we collect a large-scale color
images for training, including 400 BSD [33] images, 800
training images from DIV2K dataset [1] and 4, 744 images
from WED dataset [31].

Then, given an HR image, we synthesize LR image by
blurring it with a blur kernel k and bicubicly downsampling
it with a scale factor s, followed by an addition of AWGN
with noise level σ. The LR patch size is set to 40×40 which
means the corresponding HR patch sizes for scale factors 2,
3, and 4 are 80 × 80, 120 × 120 and 160 × 160, respectively.
In the training phase, we randomly select a blur ker-
nel and a noise level to synthesize an LR image and crop
N = 128×3, 000 LR/HR patch pairs (along with the degra-
dation maps) for each epoch. We optimize the following
loss function using Adam [25]

L(Θ) =

(cid:107)F(yi, Mi; Θ) − xi(cid:107)2.

(8)

1
2N

N
(cid:88)

i=1

The mini-batch size is set to 128. The learning rate starts
from 10−3 and reduces to 10−4 when the training error
stops decreasing. When the training error keeps unchanged
in ﬁve sequential epochs, we merge the parameters of each
batch normalization into the adjacent convolution ﬁlters.
Then, a small learning rate of 10−5 is used for additional
100 epochs to ﬁne-tune the model. Since SRMDNF is ob-
tained by ﬁne-tuning SRMD, its learning rate is ﬁxed to
10−5 for 200 epochs.

We train the models in Matlab (R2015b) environment
with MatConvNet package [48] and an Nvidia Titan X Pas-
cal GPU. The training of a single SRMD model can be done
in about two days. The source code can be downloaded at
https://github.com/cszn/SRMD.

4.2. Experiments on Bicubic Degradation

As mentioned above, instead of handling the bicubic
degradation only, our aim is to learn a single network to
handle multiple degradations. However, in order to show
the advantage of the dimensionality stretching strategy, the

proposed method is also compared with other CNN-based
methods speciﬁcally designed for bicubic degradation.

Table 1 shows the PSNR and SSIM [50] results of
state-of-the-art CNN-based SISR methods on four widely-
used datasets. As one can see, SRMD achieves compa-
rable results with VDSR at small scale factor and outper-
forms VDSR at large scale factor. In particular, SRMDNF
achieves the best overall quantitative results. Using Ima-
geNet dataset [26] to train the speciﬁc model with bicubic
degradation, SRResNet performs slightly better than SR-
MDNF on scale factor 4. To further compare with other
methods such as VDSR, we also have trained a SRMDNF
model (for scale factor 3) which operates on Y channel with
291 training images. The learned model achieves 33.97dB,
29.96dB, 28.95dB and 27.42dB on Set5, Set14, BSD100
and Urban100, respectively. As a result, it can still outper-
form other competing methods. The possible reason lies
in that the SRMDNF with multiple degradations shares the
same prior in the MAP framework which facilitates the im-
plicit prior learning and thus beneﬁts to PSNR improve-
ment. This also can explain why VDSR with multiple scales
improves the performance.

For the GPU run time, SRMD spends 0.084, 0.042 and
0.027 seconds to reconstruct an HR image of size 1, 024 ×
1, 024 for scale factors 2, 3 and 4, respectively. As a com-
parison, the run time of VDSR is 0.174 second for all scale
factors. Figure 5 shows the visual results of different meth-
ods. One can see that our proposed method yields very com-
petitive performance against other methods.

4.3. Experiments on General Degradations

In this subsection, we evaluate the performance of the
proposed method on general degradations. The degrada-
tion settings are given in Table 2. We only consider the
isotropic Gaussian blur kernel for an easy comparison. To
further show the scalability of the proposed method, another
widely-used degradation [11] which involves 7×7 Gaussian
kernel with width 1.6 and direct downsampler with scale
factor 3 is also included. We compare the proposed method
with VDSR, two model-based methods (i.e., NCSR [11]
and IRCNN [57]), and a cascaded denoising-SISR method
(i.e., DnCNN [56]+SRMDNF).

The quantitative results of different methods with dif-
ferent degradations on Set5 are provided in Table 2, from
which we have observations and analyses as follows. First,
the performance of VDSR deteriorates seriously when the
assumed bicubic degradation deviates from the true one.
Second, SRMD produces much better results than NCSR
and IRCNN, and outperforms DnCNN+SRMDNF. In par-
ticular, the PSNR gain of SRMD over DnCNN+SRMDNF
increases with the kernel width which veriﬁes the advantage
of joint denoising and super-resolution. Third, by setting
proper blur kernel, the proposed method delivers good per-

Table 1. Average PSNR and SSIM results for bicubic degradation on datasets Set5 [3], Set14 [54], BSD100 [33] and Urban100 [19]. The
best two results are highlighted in red and blue colors, respectively.

Dataset

Scale
Factor

Bicubic

SRCNN [9]

VDSR [24]

SRResNet [29]

DRRN [44]

LapSRN [27]

SRMD

SRMDNF

PSNR / SSIM

Set5

Set14

BSD100

Urban100

×2
×3
×4
×2
×3
×4
×2
×3
×4
×2
×3
×4

33.64 / 0.929
30.39 / 0.868
28.42 / 0.810
30.22 / 0.868
27.53 / 0.774
25.99 / 0.702
29.55 / 0.843
27.20 / 0.738
25.96 / 0.667
26.66 / 0.841
24.46 / 0.737
23.14 / 0.657

36.62 / 0.953
32.74 / 0.908
30.48 / 0.863
32.42 / 0.906
29.27 / 0.821
27.48 / 0.751
31.34 / 0.887
28.40 / 0.786
26.90 / 0.710
29.53 / 0.897
26.25 / 0.801
24.52 / 0.722

37.56 / 0.959
33.67 / 0.922
31.35 / 0.885
33.02 / 0.913
29.77 / 0.832
27.99 / 0.766
31.89 / 0.896
28.82 / 0.798
27.28 / 0.726
30.76 / 0.914
27.13 / 0.828
25.17 / 0.753

–
–
32.05 / 0.891
–
–
28.49 / 0.780
–
–
27.58 / 0.735
–
–
–

37.66 / 0.959
33.93 / 0.923
31.58 / 0.886
33.19 / 0.913
29.94 / 0.834
28.18 / 0.770
32.01 / 0.897
28.91 / 0.799
27.35 / 0.726
31.02 / 0.916
27.38 / 0.833
25.35 / 0.758

37.52 / 0.959
33.82 / 0.922
31.54 / 0.885
33.08 / 0.913
29.89 / 0.834
28.19 / 0.772
31.80 / 0.895
28.82 / 0.798
27.32 / 0.727
30.82 / 0.915
27.07 / 0.828
25.21 / 0.756

37.53 / 0.959
33.86 / 0.923
31.59 / 0.887
33.12 / 0.914
29.84 / 0.833
28.15 / 0.772
31.90 / 0.896
28.87 / 0.799
27.34 / 0.728
30.89 / 0.916
27.27 / 0.833
25.34 / 0.761

37.79 / 0.960
34.12 / 0.925
31.96 / 0.893
33.32 / 0.915
30.04 / 0.837
28.35 / 0.777
32.05 / 0.898
28.97 / 0.803
27.49 / 0.734
31.33 / 0.920
27.57 / 0.840
25.68 / 0.773

(a) SRCNN (23.78dB)

(b) VDSR (24.20dB)

(c) DRRN (25.11dB)

(d) LapSR (24.47dB)

(e) SRMD (25.09dB)

(f)SRMDNF (25.74dB)

Figure 5. SISR performance comparison of different methods with scale factor 4 on image “Img 099” from Urban100.

Table 2. Average PSNR and SSIM results of different methods with different degradations on Set5. The best results are highlighted in red
color. The results highlighted in gray color indicate unfair comparison due to mismatched degradation assumption.

Degradation Settings
Down-
sampler

Kernel
Width

Noise
Level

VDSR [24]

NCSR [11]

IRCNN [57]

DnCNN [56]+SRMDNF

SRMD

SRMDNF

PSNR (×2/×3/×4)

0.2
0.2
0.2
1.3
1.3
1.3
2.6
2.6
2.6

1.6

Bicubic
Bicubic
Bicubic
Bicubic
Bicubic
Bicubic
Bicubic
Bicubic
Bicubic

Direct

0
15
50
0
15
50
0
15
50

0

37.56/33.67/31.35
26.02/25.40/24.70
16.02/15.72/15.46
30.57/30.24/29.72
24.82/24.70/24.30
15.89/15.68/15.43
26.37/26.31/26.28
23.09/23.07/22.98
15.58/15.43/15.23

– /23.82/–
–
–
– /21.81/–
–
–
– /21.46/–
–
–

37.43/33.39/31.02
32.60/30.08/28.35
28.20/26.25/24.95
36.01/33.33/31.01
29.96/28.68/27.71
26.69/25.20/24.42
32.07/31.09/30.06
26.44/25.67/24.36
22.98/22.16/21.43

–
32.47/30.07/28.31
28.20/26.27/24.93
–
27.68/28.78/27.71
24.35/25.19/24.39
–

– /21.33/23.85
– /19.03/21.15

37.53/33.86/31.59
32.76/30.43/28.79
28.51/26.48/25.18
37.04/33.77/31.56
30.98/29.43/28.21
27.43/25.82/24.77
33.24/32.59/31.20
28.48/27.55/26.82
25.85/24.75/23.98

37.79/34.12/31.96
–
–
37.45/34.16/31.99
–
–
34.12/33.02/31.77
–
–

– /30.54/ –

– /33.02/ –

– /33.38/ –

–

– /33.74/ –

– /34.01/ –

(a) Ground-truth

(f) SRMDNF (30.34dB)
Figure 6. SISR performance comparison on image “Butterﬂy” from Set5. The degradation involves 7×7 Gaussian kernel with width 1.6
and direct downsampler with scale factor 3. Note that the comparison with VDSR is unfair because of degradation mismatch.

(d) IRCNN (29.32dB)

(e) SRMD (29.79dB)

(b) VDSR (24.73dB)

(c) NCSR (28.01dB)

formance in handling the degradation with direct downsam-
pler. The visual comparison is given in Figure 6. One can
see that NCSR and IRCNN produce more visually pleasant
results than VDSR since their assumed degradation matches
the true one. However, they cannot recover edges as sharper
as SRMD and SRMDNF.

4.4. Experiments on Spatially Variant Degradation

To demonstrate the effectiveness of SRMD for spatially
variant degradation, we synthesize an LR images with spa-
tially variant blur kernels and noise levels. Figure 7 shows
the visual result of the proposed SRMD for the spatially

(a)

(b)

(c)

Figure 7. An example of SRMD on dealing with spatially variant
degradation. (a) Noise level and Gaussian blur kernel width maps.
(b) Zoomed LR image. (c) Results of SRMD with scale factor 2.

(a) LR image

(b) VDSR [24]

(c) SelfEx [19]

(d) SRMD

Figure 9. SISR results on real image “Chip” with scale factor 4.

ure 8 that the performance of VDSR is severely affected by
the compression artifacts. While Waifu2x can successfully
remove the compression artifacts, it fails to recover sharp
edges. In comparison, SRMD can not only remove the un-
satisfying artifacts but also produce sharp edges. From Fig-
ure 9, we can see that VDSR and SelfEx both tend to pro-
duce over-smoothed results, whereas SRMD can recover
sharp image with better intensity and gradient statistics of
clean images [35].

5. Conclusion

In this paper, we proposed an effective super-resolution
network with high scalability of handling multiple degra-
dations via a single model. Different from existing CNN-
based SISR methods,
the proposed super-resolver takes
both LR image and its degradation maps as input. Specif-
ically, degradation maps are obtained by a simple dimen-
sionality stretching of the degradation parameters (i.e., blur
kernel and noise level). The results on synthetic LR images
demonstrated that the proposed super-resolver can not only
produce state-of-the-art results on bicubic degradation but
also perform favorably on other degradations and even spa-
tially variant degradations. Moreover, the results on real
LR images showed that the proposed method can recon-
struct visually plausible HR images. In summary, the pro-
posed super-resolver offers a feasible solution toward prac-
tical CNN-based SISR applications.

6. Acknowledgements

This work is supported by National Natural Science
Foundation of China (grant no. 61671182, 61471146),
HK RGC General Research Fund (PolyU 152240/15E)
and PolyU-Alibaba Collaborative Research Project “Qual-
ity Enhancement of Surveillance Images and Videos”. We
gratefully acknowledge the support from NVIDIA Corpo-
ration for providing us the Titan Xp GPU used in this re-
search.

(a) LR image

(b) VDSR [24]

(c) Waifu2x [49]

(d) SRMD

Figure 8. SISR results on image “Cat” with scale factor 2.

variant degradations. One can see that the proposed SRMD
is effective in recovering the latent HR image. Note that the
blur kernel is assumed to be isotropic Gaussian.

4.5. Experiments on Real Images

Besides the above experiments on LR images syntheti-
cally downsampled from HR images with known blur ker-
nels and corrupted by AWGN with known noise levels, we
also do experiments on real LR images to demonstrate the
effectiveness of the proposed SRMD. Since there are no
ground-truth HR images, we only provide the visual com-
parison.

As aforementioned, while we also use anisotropic Gaus-
sian kernels in training,
it is generally feasible to use
isotropic Gaussian for most of the real LR images in test-
ing. To ﬁnd the degradation parameters with good visual
quality, we use a grid search strategy rather than adopting
any blur kernel or noise level estimation methods. Speciﬁ-
cally, the kernel width is uniformly sampled from 0.1 to 2.4
with a stride of 0.1, and the noise level is from 0 to 75 with
stride 5.

Figures 8 and 9 illustrate the SISR results on two real LR
images “Cat” and “Chip”, respectively. The VDSR [24] is
used as one of the representative CNN-based methods for
comparison. For image “Cat” which is corrupted by com-
pression artifacts, Waifu2x [49] is also used for compari-
son. For image “Chip” which contains repetitive structures,
a self-similarity based method SelfEx [19] is also included
for comparison.

It can be observed from the visual results that SRMD
can produce much more visually plausible HR images than
the competing methods. Speciﬁcally, one can see from Fig-

References

[1] E. Agustsson and R. Timofte. Ntire 2017 challenge on sin-
gle image super-resolution: Dataset and study. In IEEE Con-
ference on Computer Vision and Pattern Recognition Work-
shops, volume 3, pages 126–135, July 2017.

[2] S. Baker and T. Kanade. Limits on super-resolution and how
to break them. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 24(9):1167–1183, 2002.

[3] M. Bevilacqua, A. Roumy, C. Guillemot, and M.-L. A.
Morel. Low-complexity single-image super-resolution based
on nonnegative neighbor embedding. In British Machine Vi-
sion Conference, 2012.

[4] S. A. Bigdeli, M. Jin, P. Favaro, and M. Zwicker. Deep mean-
shift priors for image restoration. In Advances in Neural In-
formation Processing Systems, 2017.

[5] G. Boracchi and A. Foi. Modeling the performance of image
restoration from motion blur. IEEE Transactions on Image
Processing, 21(8):3502–3517, Aug 2012.

[6] Y. Chen, W. Yu, and T. Pock. On learning optimized reac-
tion diffusion processes for effective image restoration.
In
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 5261–5269, 2015.

[7] Z. Cui, H. Chang, S. Shan, B. Zhong, and X. Chen. Deep
In European

network cascade for image super-resolution.
Conference on Computer Vision, pages 49–64, 2014.

[9] C. Dong, C. C. Loy, K. He, and X. Tang.

[8] C. Dong, C. C. Loy, K. He, and X. Tang. Learning a deep
convolutional network for image super-resolution. In Euro-
pean Conference on Computer Vision, pages 184–199, 2014.
Image
IEEE
super-resolution using deep convolutional networks.
Transactions on Pattern Analysis and Machine Intelligence,
38(2):295–307, 2016.

[10] C. Dong, C. C. Loy, and X. Tang. Accelerating the super-
resolution convolutional neural network. In European Con-
ference on Computer Vision, pages 391–407, 2016.

[11] W. Dong, L. Zhang, G. Shi, and X. Li. Nonlocally central-
ized sparse representation for image restoration. IEEE Trans-
actions on Image Processing, 22(4):1620–1630, 2013.
[12] N. Efrat, D. Glasner, A. Apartsin, B. Nadler, and A. Levin.
Accurate blur models vs. image priors in single image super-
resolution. In IEEE International Conference on Computer
Vision, pages 2832–2839, 2013.
[13] K. Egiazarian and V. Katkovnik.

Single image super-
In European Signal

resolution via BM3D sparse coding.
Processing Conference, pages 2849–2853, 2015.

[14] W. Freeman and C. Liu. Markov random ﬁelds for super-
resolution and texture synthesis. Advances in Markov Ran-
dom Fields for Vision and Image Processing, 1:155–165,
2011.

[15] D. Glasner, S. Bagon, and M. Irani. Super-resolution from
a single image. In IEEE International Conference on Com-
puter Vision, pages 349–356, 2009.

[16] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in neural information
processing systems, pages 2672–2680, 2014.

[17] H. He and W.-C. Siu. Single image super-resolution using
Gaussian process regression. In IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 449–456, 2011.

[18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
In IEEE Conference on Computer

for image recognition.
Vision and Pattern Recognition, pages 770–778, 2016.
[19] J.-B. Huang, A. Singh, and N. Ahuja. Single image super-
resolution from transformed self-exemplars. In IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
5197–5206, 2015.

[20] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
International Conference on Machine Learning, pages 448–
456, 2015.

[21] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial
In Advances in Neural Information

transformer networks.
Processing Systems, pages 2017–2025, 2015.

[22] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for
In European

real-time style transfer and super-resolution.
Conference on Computer Vision, pages 694–711, 2016.
[23] J. Kim, J. Kwon Lee, and K. Mu Lee. Deeply-recursive con-
volutional network for image super-resolution. In IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
1637–1645, 2016.

[24] J. Kim, J. K. Lee, and K. M. Lee. Accurate image super-
resolution using very deep convolutional networks. In IEEE
Conference on Computer Vision and Pattern Recognition,
pages 1646–1654, 2016.

[25] D. Kingma and J. Ba. Adam: A method for stochastic op-
timization. In International Conference for Learning Repre-
sentations, 2015.

[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in Neural Information Processing Systems, pages
1097–1105, 2012.

[27] W.-S. Lai, J.-B. Huang, N. Ahuja, and M.-H. Yang. Deep
laplacian pyramid networks for fast and accurate super-
In IEEE Conference on Computer Vision and
resolution.
Pattern Recognition, pages 624–632, July 2017.

[28] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature,

521(7553):436–444, 2015.

[29] C. Ledig, L. Theis, F. Husz´ar, J. Caballero, A. Cunningham,
A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, et al.
Photo-realistic single image super-resolution using a gen-
In IEEE Conference on Com-
erative adversarial network.
puter Vision and Pattern Recognition, pages 4681–4690,
July 2017.

[30] B. Lim, S. Son, H. Kim, S. Nah, and K. M. Lee. Enhanced
deep residual networks for single image super-resolution. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion Workshops, pages 136–144, July 2017.

[31] K. Ma, Z. Duanmu, Q. Wu, Z. Wang, H. Yong, H. Li, and
L. Zhang. Waterloo exploration database: New challenges
for image quality assessment models. IEEE Transactions on
Image Processing, 26(2):1004–1016, 2017.

[32] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman.
Non-local sparse models for image restoration. In IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
2272–2279, 2009.

[33] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database
of human segmented natural images and its application to
evaluating segmentation algorithms and measuring ecologi-

Conference, pages 689–692, 2015.

[49] Waifu2x.

Image super-resolution for anime-style art using
deep convolutional neural networks. http://waifu2x.
udp.jp/.

[50] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli.
from error visibility to struc-
IEEE Transactions on Image Processing,

Image quality assessment:
tural similarity.
13(4):600–612, 2004.

[51] C.-Y. Yang, C. Ma, and M.-H. Yang. Single-image super-
resolution: A benchmark. In European Conference on Com-
puter Vision, pages 372–386, 2014.

[52] J. Yang, J. Wright, T. S. Huang, and Y. Ma. Image super-
resolution via sparse representation. IEEE Transactions on
Image Processing, 19(11):2861–2873, 2010.

[53] W. Yang, J. Feng, J. Yang, F. Zhao, J. Liu, Z. Guo, and
S. Yan. Deep edge guided recurrent residual learning for
image super-resolution. IEEE Transactions on Image Pro-
cessing, 26(12):5895–5907, 2017.

[54] R. Zeyde, M. Elad, and M. Protter. On single image scale-up
using sparse-representations. In International conference on
curves and surfaces, pages 711–730, 2010.

[55] K. Zhang, X. Zhou, H. Zhang, and W. Zuo. Revisiting sin-
gle image super-resolution under internet environment: blur
kernels and reconstruction algorithms. In Paciﬁc Rim Con-
ference on Multimedia, pages 677–687, 2015.

[56] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang. Be-
yond a gaussian denoiser: Residual learning of deep CNN
for image denoising. IEEE Transactions on Image Process-
ing, pages 3142–3155, 2017.

[57] K. Zhang, W. Zuo, S. Gu, and L. Zhang. Learning deep CNN
In IEEE Conference
denoiser prior for image restoration.
on Computer Vision and Pattern Recognition, pages 3929–
3938, July 2017.

[58] Y. Zhang, Y. Tian, Y. Kong, B. Zhong, and Y. Fu. Residual
dense network for image super-resolution. In IEEE Confer-
ence on Computer Vision and Pattern Recognition, 2018.

cal statistics. In IEEE International Conference on Computer
Vision, volume 2, pages 416–423, July 2001.

[34] T. Meinhardt, M. M¨oller, C. Hazirbas, and D. Cremers.
Learning proximal operators: Using denoising networks for
In IEEE Interna-
regularizing inverse imaging problems.
tional Conference on Computer Vision, pages 1781–1790,
2017.

[35] J. Pan, Z. Hu, Z. Su, and M.-H. Yang. Deblurring text images
via L0-regularized intensity and gradient prior. In IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
2901–2908, 2014.

[36] T. Peleg and M. Elad. A statistical prediction model based
on sparse representations for single image super-resolution.
IEEE transactions on Image Processing, 23(6):2569–2582,
2014.

[37] J. S. Ren, L. Xu, Q. Yan, and W. Sun. Shepard convolu-
tional neural networks. In Advances in Neural Information
Processing Systems, pages 901–909, 2015.

[38] G. Riegler, S. Schulter, M. Ruther, and H. Bischof. Condi-
tioned regression models for non-blind single image super-
resolution. In IEEE International Conference on Computer
Vision, pages 522–530, 2015.

[39] Y. Romano, M. Elad, and P. Milanfar. The little engine that
could: Regularization by denoising (red). SIAM Journal on
Imaging Sciences, 10(4):1804–1844, 2017.

[40] Y. Romano, J. Isidoro, and P. Milanfar. RAISR: rapid and ac-
curate image super resolution. IEEE Transactions on Com-
putational Imaging, 3(1):110–125, 2017.

[41] W. Shi, J. Caballero, F. Husz´ar, J. Totz, A. P. Aitken,
R. Bishop, D. Rueckert, and Z. Wang. Real-time single im-
age and video super-resolution using an efﬁcient sub-pixel
convolutional neural network. In IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 1874–1883,
2016.

[42] Y. Shi, K. Wang, C. Chen, L. Xu, and L. Lin. Structure-
preserving image super-resolution via contextualized multi-
task learning. IEEE Transactions on Multimedia, 2017.
[43] A. Singh, F. Porikli, and N. Ahuja. Super-resolving noisy
images. In IEEE Conference on Computer Vision and Pattern
Recognition, pages 2846–2853, 2014.

[44] Y. Tai, J. Yang, and X. Liu. Image super-resolution via deep
In IEEE Conference on Com-
recursive residual network.
puter Vision and Pattern Recognition, pages 3147–3155,
2017.

[45] Y. Tai, J. Yang, X. Liu, and C. Xu. Memnet: A persistent
In IEEE Interna-
memory network for image restoration.
tional Conference on Computer Vision, pages 4539–4547,
2017.

[46] R. Timofte, E. Agustsson, L. Van Gool, M.-H. Yang, and
L. Zhang. Ntire 2017 challenge on single image super-
In IEEE Conference on
resolution: Methods and results.
Computer Vision and Pattern Recognition Workshops, pages
114–125, July 2017.

[47] R. Timofte, V. De Smet, and L. Van Gool. A+: Adjusted
anchored neighborhood regression for fast super-resolution.
In Asian Conference on Computer Vision, pages 111–126,
2014.

[48] A. Vedaldi and K. Lenc. MatConvNet: Convolutional neu-
ral networks for matlab. In ACM Conference on Multimedia

